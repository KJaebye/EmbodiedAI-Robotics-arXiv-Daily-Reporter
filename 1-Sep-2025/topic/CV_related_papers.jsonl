{'arxiv_id': 'arXiv:2508.21271', 'title': 'Mini Autonomous Car Driving based on 3D Convolutional Neural Networks', 'authors': 'Pablo Moraes, Monica Rodriguez, Kristofer S. Kappel, Hiago Sodre, Santiago Fernandez, Igor Nunes, Bruna Guterres, Ricardo Grando', 'link': 'https://arxiv.org/abs/2508.21271', 'abstract': "Autonomous driving applications have become increasingly relevant in the automotive industry due to their potential to enhance vehicle safety, efficiency, and user experience, thereby meeting the growing demand for sophisticated driving assistance features. However, the development of reliable and trustworthy autonomous systems poses challenges such as high complexity, prolonged training periods, and intrinsic levels of uncertainty. Mini Autonomous Cars (MACs) are used as a practical testbed, enabling validation of autonomous control methodologies on small-scale setups. This simplified and cost-effective environment facilitates rapid evaluation and comparison of machine learning models, which is particularly useful for algorithms requiring online training. To address these challenges, this work presents a methodology based on RGB-D information and three-dimensional convolutional neural networks (3D CNNs) for MAC autonomous driving in simulated environments. We evaluate the proposed approach against recurrent neural networks (RNNs), with architectures trained and tested on two simulated tracks with distinct environmental features. Performance was assessed using task completion success, lap-time metrics, and driving consistency. Results highlight how architectural modifications and track complexity influence the models' generalization capability and vehicle control performance. The proposed 3D CNN demonstrated promising results when compared with RNNs.", 'abstract_zh': '自主驾驶应用由于其在提升车辆安全、效率和用户体验方面的潜力，在汽车行业中越来越受到关注，从而满足了对复杂驾驶辅助功能的日益增长的需求。然而，可靠且可信赖的自主系统的开发面临着高复杂性、漫长的训练周期以及固有的不确定性等挑战。微型自主车（MACs）被用作实际的试验平台，用于在小型设置中验证自主控制方法。这种简化且经济高效的环境促进了机器学习模型的快速评估和比较，尤其适用于需要在线训练的算法。为应对这些挑战，本研究提出了一种基于RGB-D信息和三维卷积神经网络（3D CNN）的方法，用于仿真实验环境中的MAC自主驾驶。我们用两种具有不同环境特征的仿真实 Tracks 来训练和测试基于RNN的方法，并评估了提出的3D CNN方法的表现，通过任务完成成功率、圈速指标和驾驶一致性进行评估。研究结果表明，架构修改和赛道复杂性影响模型的泛化能力和车辆控制性能。与RNN相比，提出的3D CNN方法显示出有前景的结果。', 'title_zh': '基于3D卷积神经网络的微型自主驾车技术'}
{'arxiv_id': 'arXiv:2508.21163', 'title': 'Observer Design for Optical Flow-Based Visual-Inertial Odometry with Almost-Global Convergence', 'authors': 'Tarek Bouazza, Soulaimane Berkane, Minh-Duc Hua, Tarek Hamel', 'link': 'https://arxiv.org/abs/2508.21163', 'abstract': 'This paper presents a novel cascaded observer architecture that combines optical flow and IMU measurements to perform continuous monocular visual-inertial odometry (VIO). The proposed solution estimates body-frame velocity and gravity direction simultaneously by fusing velocity direction information from optical flow measurements with gyro and accelerometer data. This fusion is achieved using a globally exponentially stable Riccati observer, which operates under persistently exciting translational motion conditions. The estimated gravity direction in the body frame is then employed, along with an optional magnetometer measurement, to design a complementary observer on $\\mathbf{SO}(3)$ for attitude estimation. The resulting interconnected observer architecture is shown to be almost globally asymptotically stable. To extract the velocity direction from sparse optical flow data, a gradient descent algorithm is developed to solve a constrained minimization problem on the unit sphere. The effectiveness of the proposed algorithms is validated through simulation results.', 'abstract_zh': '这篇论文提出了一种新颖的级联观测器架构，结合了光学流和IMU测量来执行单目视觉惯性里程计（VIO）。所提出的方法通过融合光学流测量的速度方向信息与陀螺仪和加速度计数据，同时估计体-frame速度和重力方向。这种融合是通过在持续激发的平移运动条件下使用全局指数稳定的Riccati观测器实现的。在体-frame中估计的重力方向随后被用于与可选的磁力计测量结合，以在$\\mathbf{SO}(3)$上设计一个补充观测器进行姿态估计。所得的互联观测器架构被证明是几乎全局渐近稳定的。为了从稀疏的光学流数据中提取速度方向，开发了一种梯度下降算法来求解单位球上的约束最小化问题。所提出算法的有效性通过仿真结果得到了验证。', 'title_zh': '基于光学流动的视觉-惯性里程计的观察者设计与几乎全局收敛分析'}
{'arxiv_id': 'arXiv:2508.21542', 'title': 'Complete Gaussian Splats from a Single Image with Denoising Diffusion Models', 'authors': 'Ziwei Liao, Mohamed Sayed, Steven L. Waslander, Sara Vicente, Daniyar Turmukhambetov, Michael Firman', 'link': 'https://arxiv.org/abs/2508.21542', 'abstract': 'Gaussian splatting typically requires dense observations of the scene and can fail to reconstruct occluded and unobserved areas. We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. Completing the unobserved surfaces of a scene is challenging due to the ambiguity of the plausible surfaces. Conventional methods use a regression-based formulation to predict a single "mode" for occluded and out-of-frustum surfaces, leading to blurriness, implausibility, and failure to capture multiple possible explanations. Thus, they often address this problem partially, focusing either on objects isolated from the background, reconstructing only visible surfaces, or failing to extrapolate far from the input views. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings.', 'abstract_zh': '基于拉普拉斯扩散模型的单图全场景高保真重建', 'title_zh': '使用去噪扩散模型从单张图像生成完整高斯点云'}
{'arxiv_id': 'arXiv:2508.21102', 'title': 'GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions', 'authors': 'Kei Katsumata, Yui Iioka, Naoki Hosomi, Teruhisa Misu, Kentaro Yamada, Komei Sugiura', 'link': 'https://arxiv.org/abs/2508.21102', 'abstract': 'We focus on the task of identifying the location of target regions from a natural language instruction and a front camera image captured by a mobility. This task is challenging because it requires both existence prediction and segmentation, particularly for stuff-type target regions with ambiguous boundaries. Existing methods often underperform in handling stuff-type target regions, in addition to absent or multiple targets. To overcome these limitations, we propose GENNAV, which predicts target existence and generates segmentation masks for multiple stuff-type target regions. To evaluate GENNAV, we constructed a novel benchmark called GRiN-Drive, which includes three distinct types of samples: no-target, single-target, and multi-target. GENNAV achieved superior performance over baseline methods on standard evaluation metrics. Furthermore, we conducted real-world experiments with four automobiles operated in five geographically distinct urban areas to validate its zero-shot transfer performance. In these experiments, GENNAV outperformed baseline methods and demonstrated its robustness across diverse real-world environments. The project page is available at this https URL.', 'abstract_zh': '我们专注于从自然语言指令和由移动设备捕捉的前视摄像头图像中识别目标区域的位置。这项任务具有挑战性，因为它需要同时进行存在预测和分割，特别是对于边界模糊的目标区域。现有方法在处理此类目标区域时常常表现不佳，尤其是在处理不存在或多个目标的情况下。为了克服这些限制，我们提出了GENNAV，它预测目标的存在并为多个类型的目标区域生成分割掩码。为了评估GENNAV，我们构建了一个名为GRiN-Drive的新基准，其中包括三种不同的样本类型：无目标、单目标和多目标。GENNAV在标准评估指标上优于基线方法。此外，我们在五个城市地区进行的四辆汽车的实际实验中验证了其零样本迁移性能，在这些实验中，GENNAV优于基线方法并展示了其在不同现实环境中的鲁棒性。项目页面可访问此链接。', 'title_zh': 'GENNAV: 通用引用可导航区域的多边形掩码生成'}
{'arxiv_id': 'arXiv:2508.21080', 'title': '2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving', 'authors': 'Ali K. AlShami, Ryan Rabinowitz, Maged Shoman, Jianwu Fang, Lukas Picek, Shao-Yuan Lo, Steve Cruz, Khang Nhut Lam, Nachiket Kamod, Lei-Lei Li, Jugal Kalita, Terrance E. Boult', 'link': 'https://arxiv.org/abs/2508.21080', 'abstract': "As the computer vision community advances autonomous driving algorithms, integrating vision-based insights with sensor data remains essential for improving perception, decision making, planning, prediction, simulation, and control. Yet we must ask: Why don't we have entirely safe self-driving cars yet? A key part of the answer lies in addressing novel scenarios, one of the most critical barriers to real-world deployment. Our 2COOOL workshop provides a dedicated forum for researchers and industry experts to push the state of the art in novelty handling, including out-of-distribution hazard detection, vision-language models for hazard understanding, new benchmarking and methodologies, and safe autonomous driving practices. The 2nd Workshop on the Challenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held at the International Conference on Computer Vision (ICCV) 2025 in Honolulu, Hawaii, on October 19, 2025. We aim to inspire the development of new algorithms and systems for hazard avoidance, drawing on ideas from anomaly detection, open-set recognition, open-vocabulary modeling, domain adaptation, and related fields. Building on the success of its inaugural edition at the Winter Conference on Applications of Computer Vision (WACV) 2025, the workshop will feature a mix of academic and industry participation.", 'abstract_zh': '面向自主驾驶领域的标签外危害应对挑战研讨会（2COOOL）', 'title_zh': '2COOOL: 第二届关于自主驾驶领域未标注风险挑战研讨会'}
{'arxiv_id': 'arXiv:2508.21816', 'title': 'The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning', 'authors': 'Yiming Lin, Yuchen Niu, Shang Wang, Kaizhu Huang, Qiufeng Wang, Xiao-Bo Jin', 'link': 'https://arxiv.org/abs/2508.21816', 'abstract': 'Context recognition (SR) is a fundamental task in computer vision that aims to extract structured semantic summaries from images by identifying key events and their associated entities. Specifically, given an input image, the model must first classify the main visual events (verb classification), then identify the participating entities and their semantic roles (semantic role labeling), and finally localize these entities in the image (semantic role localization). Existing methods treat verb classification as a single-label problem, but we show through a comprehensive analysis that this formulation fails to address the inherent ambiguity in visual event recognition, as multiple verb categories may reasonably describe the same image. This paper makes three key contributions: First, we reveal through empirical analysis that verb classification is inherently a multi-label problem due to the ubiquitous semantic overlap between verb categories. Second, given the impracticality of fully annotating large-scale datasets with multiple labels, we propose to reformulate verb classification as a single positive multi-label learning (SPMLL) problem - a novel perspective in SR research. Third, we design a comprehensive multi-label evaluation benchmark for SR that is carefully designed to fairly evaluate model performance in a multi-label setting. To address the challenges of SPMLL, we futher develop the Graph Enhanced Verb Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to capture label correlations and adversarial training to optimize decision boundaries. Extensive experiments on real-world datasets show that our approach achieves more than 3\\% MAP improvement while remaining competitive on traditional top-1 and top-5 accuracy metrics.', 'abstract_zh': '基于上下文识别的动词分类（多标签问题）：一种新的研究视角', 'title_zh': 'demon存在于不确定性中：重新审视基于单一正样本多标签学习的情境识别'}
{'arxiv_id': 'arXiv:2508.21732', 'title': 'CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models', 'authors': 'João Valente, Atabak Dehban, Rodrigo Ventura', 'link': 'https://arxiv.org/abs/2508.21732', 'abstract': "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities across various multimodal tasks. They continue, however, to struggle with trivial scenarios such as reading values from Digital Measurement Devices (DMDs), particularly in real-world conditions involving clutter, occlusions, extreme viewpoints, and motion blur; common in head-mounted cameras and Augmented Reality (AR) applications. Motivated by these limitations, this work introduces CAD2DMD-SET, a synthetic data generation tool designed to support visual question answering (VQA) tasks involving DMDs. By leveraging 3D CAD models, advanced rendering, and high-fidelity image composition, our tool produces diverse, VQA-labelled synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present DMDBench, a curated validation set of 1,000 annotated real-world images designed to evaluate model performance under practical constraints. Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein Similarity (ANLS) and further fine-tuning LoRA's of these models with CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL showcasing a score increase of 200% without degrading on other tasks. This demonstrates that the CAD2DMD-SET training dataset substantially improves the robustness and performance of LVLMs when operating under the previously stated challenging conditions. The CAD2DMD-SET tool is expected to be released as open-source once the final version of this manuscript is prepared, allowing the community to add different measurement devices and generate their own datasets.", 'abstract_zh': '最近在大型视觉-语言模型（LVLMs）方面取得的进展展示了其在多种多模态任务中的出色能力。然而，这些模型在处理数字测量设备（DMDs）读取数值等简单场景时仍存在挑战，特别是在包含杂乱环境、遮挡、极端视角和运动模糊等现实条件下，这些情况在头戴式相机和增强现实（AR）应用中尤为常见。受这些限制的启发，本文提出了CAD2DMD-SET，一种合成数据生成工具，旨在支持涉及DMDs的视觉问答（VQA）任务。通过利用3D CAD模型、高级渲染和高保真图像合成，我们的工具生成了多样化的、带有VQA标签的合成DMD数据集，适用于LVLMs的微调。此外，我们还介绍了DMDBench，一个包含1,000张带有标注的现实世界图像的精炼验证集，用于在实际约束条件下评估模型性能。使用平均归一化莱文相似度（ANLS）对三种最先进的LVLMs进行基准测试，并进一步使用CAD2DMD-SET生成的数据集微调这些模型的LoRA，结果显示显著的性能提升，其中InternVL的得分提高了200%，且未在其他任务上退步。这表明CAD2DMD-SET训练数据集在上述挑战条件下大大提高了LVLMs的鲁棒性和性能。CAD2DMD-SET工具计划在最终论文版本准备好后作为开源发布，允许社区添加不同的测量设备并生成自己的数据集。', 'title_zh': 'CAD2DMD-SET：合成生成数字测量设备CAD模型数据集的工具，用于大型视觉-语言模型的微调'}
{'arxiv_id': 'arXiv:2508.21715', 'title': 'Entropy-Based Non-Invasive Reliability Monitoring of Convolutional Neural Networks', 'authors': 'Amirhossein Nazeri, Wael Hafez', 'link': 'https://arxiv.org/abs/2508.21715', 'abstract': 'Convolutional Neural Networks (CNNs) have become the foundation of modern computer vision, achieving unprecedented accuracy across diverse image recognition tasks. While these networks excel on in-distribution data, they remain vulnerable to adversarial perturbations imperceptible input modifications that cause misclassification with high confidence. However, existing detection methods either require expensive retraining, modify network architecture, or degrade performance on clean inputs. Here we show that adversarial perturbations create immediate, detectable entropy signatures in CNN activations that can be monitored without any model modification. Using parallel entropy monitoring on VGG-16, we demonstrate that adversarial inputs consistently shift activation entropy by 7% in early convolutional layers, enabling 90% detection accuracy with false positives and false negative rates below 20%. The complete separation between clean and adversarial entropy distributions reveals that CNNs inherently encode distribution shifts in their activation patterns. This work establishes that CNN reliability can be assessed through activation entropy alone, enabling practical deployment of self-diagnostic vision systems that detect adversarial inputs in real-time without compromising original model performance.', 'abstract_zh': '卷积神经网络（CNNs）已成为现代计算机视觉的基石，实现了各种图像识别任务前所未有的准确性。虽然这些网络在内部数据分布上表现出色，但它们仍然容易受到不可感知的输入修改（即对抗性扰动）的影响，这些扰动会导致模型以高置信度错误分类。然而，现有的检测方法要么需要昂贵的重新训练，要么修改网络架构，要么在干净输入上降低性能。在这里，我们展示了对抗性扰动会在CNN激活中立即产生可检测的熵特征签名，这些特征可以通过不修改任何模型的情况下进行监控。通过对VGG-16进行并行熵监控，我们证明了对抗性输入始终在早期卷积层中将激活熵偏移7%，从而实现90%的检测准确率，并保持假正例和假负例率低于20%。干净和对抗性熵分布的完全分离揭示了CNNs在其激活模式中固有地编码分布偏移。这项工作表明，仅通过激活熵就可以评估CNN的可靠性，从而实现实时检测对抗性输入的自诊断视觉系统的实际部署，而不影响原始模型的性能。', 'title_zh': '基于熵的非侵入式卷积神经网络可靠性监控'}
{'arxiv_id': 'arXiv:2508.21550', 'title': 'EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting', 'authors': 'Yujin Park, Haejun Chung, Ikbeom Jang', 'link': 'https://arxiv.org/abs/2508.21550', 'abstract': 'Pairwise comparison is often favored over absolute rating or ordinal classification in subjective or difficult annotation tasks due to its improved reliability. However, exhaustive comparisons require a massive number of annotations (O(n^2)). Recent work has greatly reduced the annotation burden (O(n log n)) by actively sampling pairwise comparisons using a sorting algorithm. We further improve annotation efficiency by (1) roughly pre-ordering items using the Contrastive Language-Image Pre-training (CLIP) model hierarchically without training, and (2) replacing easy, obvious human comparisons with automated comparisons. The proposed EZ-Sort first produces a CLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores, and finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation was conducted using various datasets: face-age estimation (FGNET), historical image chronology (DHCI), and retinal image quality assessment (EyePACS). It showed that EZ-Sort reduced human annotation cost by 90.5% compared to exhaustive pairwise comparisons and by 19.8% compared to prior work (when n = 100), while improving or maintaining inter-rater reliability. These results demonstrate that combining CLIP-based priors with uncertainty-aware sampling yields an efficient and scalable solution for pairwise ranking.', 'abstract_zh': '基于CLIP的对比排序：一种高效可扩展的成对排序方法', 'title_zh': 'EZ-Sort: 高效的零样本CLIP指导的先序对比与人力参与排序'}
{'arxiv_id': 'arXiv:2508.21496', 'title': 'ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding', 'authors': 'Hao Lu, Jiahao Wang, Yaolun Zhang, Ruohui Wang, Xuanyu Zheng, Yepeng Tang, Dahua Lin, Lewei Lu', 'link': 'https://arxiv.org/abs/2508.21496', 'abstract': "Video multimodal large language models (Video-MLLMs) have achieved remarkable progress in video understanding. However, they remain vulnerable to hallucination-producing content inconsistent with or unrelated to video inputs. Previous video hallucination benchmarks primarily focus on short-videos. They attribute hallucinations to factors such as strong language priors, missing frames, or vision-language biases introduced by the visual encoder. While these causes indeed account for most hallucinations in short videos, they still oversimplify the cause of hallucinations. Sometimes, models generate incorrect outputs but with correct frame-level semantics. We refer to this type of hallucination as Semantic Aggregation Hallucination (SAH), which arises during the process of aggregating frame-level semantics into event-level semantic groups. Given that SAH becomes particularly critical in long videos due to increased semantic complexity across multiple events, it is essential to separate and thoroughly investigate the causes of this type of hallucination. To address the above issues, we introduce ELV-Halluc, the first benchmark dedicated to long-video hallucination, enabling a systematic investigation of SAH. Our experiments confirm the existence of SAH and show that it increases with semantic complexity. Additionally, we find that models are more prone to SAH on rapidly changing semantics. Moreover, we discuss potential approaches to mitigate SAH. We demonstrate that positional encoding strategy contributes to alleviating SAH, and further adopt DPO strategy to enhance the model's ability to distinguish semantics within and across events. To support this, we curate a dataset of 8K adversarial data pairs and achieve improvements on both ELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.", 'abstract_zh': 'Video多模态大型语言模型（Video-MLLMs）在视频理解领域取得了显著进展，然而，它们仍然容易产生与视频输入内容不一致或无关的幻觉。现有的视频幻觉基准主要关注短视频，将幻觉归因于强大的语言先验、缺失的帧或由视觉编码器引入的视觉-语言偏见。虽然这些因素确实解释了短视频中大部分的幻觉，但它们仍然过于简化了幻觉的原因。有时，模型生成错误的输出，但帧级语义是正确的。我们将这种类型的幻觉称为语义聚合幻觉（SAH），它发生在将帧级语义聚合到事件级语义组的过程中。由于长视频中多事件间的语义复杂性增加，SAH变得尤为重要。因此，有必要分离和深入研究这种幻觉的原因。为了解决上述问题，我们引入了ELV-Halluc，这是第一个专门关注长视频幻觉的基准，使得系统地研究SAH成为可能。我们的实验验证了SAH的存在，并表明其随语义复杂性的增加而增加。此外，我们发现模型在快速变化的语义上更容易产生SAH。我们还讨论了减轻SAH的潜在方法。我们证明了位置编码策略有助于缓解SAH，并进一步采用了DPO策略以增强模型在事件内外区分语义的能力。为此，我们构建了一个包含8K对抗数据对的数据集，并在ELV-Halluc和Video-MME上取得了改进，包括SAH比显著减少了27.7%。', 'title_zh': 'ELV-Halluc: 长视频理解中语义聚合幻象的基准测试'}
{'arxiv_id': 'arXiv:2508.21263', 'title': 'Deep Active Learning for Lung Disease Severity Classification from Chest X-rays: Learning with Less Data in the Presence of Class Imbalance', 'authors': 'Roy M. Gabriel, Mohammadreza Zandehshahvar, Marly van Assen, Nattakorn Kittisut, Kyle Peters, Carlo N. De Cecco, Ali Adibi', 'link': 'https://arxiv.org/abs/2508.21263', 'abstract': 'To reduce the amount of required labeled data for lung disease severity classification from chest X-rays (CXRs) under class imbalance, this study applied deep active learning with a Bayesian Neural Network (BNN) approximation and weighted loss function. This retrospective study collected 2,319 CXRs from 963 patients (mean age, 59.2 $\\pm$ 16.6 years; 481 female) at Emory Healthcare affiliated hospitals between January and November 2020. All patients had clinically confirmed COVID-19. Each CXR was independently labeled by 3 to 6 board-certified radiologists as normal, moderate, or severe. A deep neural network with Monte Carlo Dropout was trained using active learning to classify disease severity. Various acquisition functions were used to iteratively select the most informative samples from an unlabeled pool. Performance was evaluated using accuracy, area under the receiver operating characteristic curve (AU ROC), and area under the precision-recall curve (AU PRC). Training time and acquisition time were recorded. Statistical analysis included descriptive metrics and performance comparisons across acquisition strategies. Entropy Sampling achieved 93.7% accuracy (AU ROC, 0.91) in binary classification (normal vs. diseased) using 15.4% of the training data. In the multi-class setting, Mean STD sampling achieved 70.3% accuracy (AU ROC, 0.86) using 23.1% of the labeled data. These methods outperformed more complex and computationally expensive acquisition functions and significantly reduced labeling needs. Deep active learning with BNN approximation and weighted loss effectively reduces labeled data requirements while addressing class imbalance, maintaining or exceeding diagnostic performance.', 'abstract_zh': '基于贝叶斯神经网络近似和加权损失函数的深度主动学习在缓解类不平衡下减少胸片肺疾病严重程度分类所需标注数据量的研究', 'title_zh': '基于胸片的肺部疾病严重程度分类的深度主动学习：面向类别不平衡情况下的少数据学习'}
