{'arxiv_id': 'arXiv:2508.21378', 'title': 'RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation', 'authors': 'Chenduo Ying, Linkang Du, Peng Cheng, Yuanchao Shu', 'link': 'https://arxiv.org/abs/2508.21378', 'abstract': 'Large language models (LLMs) demonstrate remarkable capabilities in reasoning and code generation, enabling robotic manipulation to be initiated with just a single instruction. The LLM carries out various tasks by generating policy code required to control the robot. Despite advances in LLMs, achieving reliable policy code generation remains a significant challenge due to the diverse requirements of real-world tasks and the inherent complexity of user instructions. In practice, different users may provide distinct instructions to drive the robot for the same task, which may cause the unreliability of policy code generation. To bridge this gap, we design RoboInspector, a pipeline to unveil and characterize the unreliability of the policy code for LLM-enabled robotic manipulation from two perspectives: the complexity of the manipulation task and the granularity of the instruction. We perform comprehensive experiments with 168 distinct combinations of tasks, instructions, and LLMs in two prominent frameworks. The RoboInspector identifies four main unreliable behaviors that lead to manipulation failure. We provide a detailed characterization of these behaviors and their underlying causes, giving insight for practical development to reduce unreliability. Furthermore, we introduce a refinement approach guided by failure policy code feedback that improves the reliability of policy code generation by up to 35% in LLM-enabled robotic manipulation, evaluated in both simulation and real-world environments.', 'abstract_zh': '大型语言模型在机器人 manipulation 中的政策代码生成可靠性研究：从任务复杂性和指令粒度视角探索不确定行为及其改进方法', 'title_zh': 'RoboInspector: 揭示基于LLM的机器人操作中策略代码的不可靠性'}
{'arxiv_id': 'arXiv:2508.21803', 'title': 'Automated Clinical Problem Detection from SOAP Notes using a Collaborative Multi-Agent LLM Architecture', 'authors': 'Yeawon Lee, Xiaoyang Wang, Christopher C. Yang', 'link': 'https://arxiv.org/abs/2508.21803', 'abstract': "Accurate interpretation of clinical narratives is critical for patient care, but the complexity of these notes makes automation challenging. While Large Language Models (LLMs) show promise, single-model approaches can lack the robustness required for high-stakes clinical tasks. We introduce a collaborative multi-agent system (MAS) that models a clinical consultation team to address this gap. The system is tasked with identifying clinical problems by analyzing only the Subjective (S) and Objective (O) sections of SOAP notes, simulating the diagnostic reasoning process of synthesizing raw data into an assessment. A Manager agent orchestrates a dynamically assigned team of specialist agents who engage in a hierarchical, iterative debate to reach a consensus. We evaluated our MAS against a single-agent baseline on a curated dataset of 420 MIMIC-III notes. The dynamic multi-agent configuration demonstrated consistently improved performance in identifying congestive heart failure, acute kidney injury, and sepsis. Qualitative analysis of the agent debates reveals that this structure effectively surfaces and weighs conflicting evidence, though it can occasionally be susceptible to groupthink. By modeling a clinical team's reasoning process, our system offers a promising path toward more accurate, robust, and interpretable clinical decision support tools.", 'abstract_zh': '准确解释临床叙事对于患者护理至关重要，但这些笔记的复杂性使自动化任务面临挑战。虽然大规模语言模型(Large Language Models, LLMs)前景广阔，但单一模型的方法可能无法满足高风险临床任务所需的稳健性。我们提出了一种协作多智能体系统(collaborative multi-agent system, MAS)，该系统旨在通过分析SOAP笔记中仅有的主诉(S)和体征(O)部分来模拟诊断推理过程，从而识别临床问题。管理(agent Manager)智能体协调一个动态分配的专业智能体团队，进行层次化的迭代辩论，以达成共识。我们在一个包含420份MIMIC-III笔记的精心策划数据集上将我们的MAS与单一智能体基线进行评估。动态多智能体配置在识别充血性心力衰竭、急性肾损伤和败血症方面表现出一致的性能提升。对智能体辩论的定性分析表明，这种结构有效地突显并权衡了相冲突的证据，尽管偶尔会受到群体思维的影响。通过模拟临床团队的推理过程，我们的系统为更加准确、稳健和可解释的临床决策支持工具开辟了有前景的道路。', 'title_zh': '基于协作多智能体LLM架构的SOAP笔记临床问题自动检测'}
{'arxiv_id': 'arXiv:2508.21648', 'title': 'Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI', 'authors': 'Farhad Abtahi, Mehdi Astaraki, Fernando Seoane', 'link': 'https://arxiv.org/abs/2508.21648', 'abstract': 'Bias in medical artificial intelligence is conventionally viewed as a defect requiring elimination. However, human reasoning inherently incorporates biases shaped by education, culture, and experience, suggesting their presence may be inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble Diagnostic system with Leveraged diversitY), a conceptual framework that orchestrates multiple AI models while preserving their diverse outputs rather than collapsing them into a consensus. Unlike traditional approaches that suppress disagreement, MEDLEY documents model-specific biases as potential strengths and treats hallucinations as provisional hypotheses for clinician verification. A proof-of-concept demonstrator was developed using over 30 large language models, creating a minimum viable product that preserved both consensus and minority views in synthetic cases, making diagnostic uncertainty and latent biases transparent for clinical oversight. While not yet a validated clinical tool, the demonstration illustrates how structured diversity can enhance medical reasoning under clinician supervision. By reframing AI imperfection as a resource, MEDLEY offers a paradigm shift that opens new regulatory, ethical, and innovation pathways for developing trustworthy medical AI systems.', 'abstract_zh': '医学人工智能中的偏差通常被视为需要消除的缺陷。然而，人类推理本质上融合了由教育、文化及经验形成的偏差，这表明其存在可能是不可避免且可能有价值的。我们提出了一种MEDLEY（Medical Ensemble Diagnostic system with Leveraged diversitY）概念框架，该框架协调多个AI模型并保留其多样化的输出，而非将其压缩为一致意见。与传统的抑制分歧的方法不同，MEDLEY记录模型特定的偏差作为潜在的优势，并将幻觉视为供临床医生验证的暂定假设。该概念验证演示器使用了超过30个大型语言模型，开发了一个最小可行产品，该产品在合成案例中既保留了共识意见也保留了少数派观点，使诊断不确定性及潜在偏差透明化，便于临床监督。尽管尚未被验证为临床工具，该演示展示了在临床监督下结构化多样性的增强效果。通过将AI不完美之处重新定义为资源，MEDLEY提供了一种范式转变，开启了开发可信的医疗AI系统的新型监管、伦理和创新途径。', 'title_zh': '利用 Imperfection 机遇：一种利用医疗AI偏见的多模型方法（MEDLEY）'}
{'arxiv_id': 'arXiv:2508.21622', 'title': 'Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study', 'authors': 'Saravanan Venkatachalam', 'link': 'https://arxiv.org/abs/2508.21622', 'abstract': 'This paper presents an integrated framework that combines traditional network optimization models with large language models (LLMs) to deliver interactive, explainable, and role-aware decision support for supply chain planning. The proposed system bridges the gap between complex operations research outputs and business stakeholder understanding by generating natural language summaries, contextual visualizations, and tailored key performance indicators (KPIs). The core optimization model addresses tactical inventory redistribution across a network of distribution centers for multi-period and multi-item, using a mixed-integer formulation. The technical architecture incorporates AI agents, RESTful APIs, and a dynamic user interface to support real-time interaction, configuration updates, and simulation-based insights. A case study demonstrates how the system improves planning outcomes by preventing stockouts, reducing costs, and maintaining service levels. Future extensions include integrating private LLMs, transfer learning, reinforcement learning, and Bayesian neural networks to enhance explainability, adaptability, and real-time decision-making.', 'abstract_zh': '本文提出了一种结合传统网络优化模型与大规模语言模型（LLMs）的集成框架，以提供供应链计划中的互动、可解释和角色感知的决策支持。该提出的系统通过生成自然语言摘要、情境可视化和定制的关键绩效指标（KPIs），弥合了复杂运作研究输出与业务相关方理解之间的差距。核心优化模型采用混合整数规划形式，针对多个时期和多项目的分配中心网络，解决战略库存再分配问题。技术架构包括AI代理、RESTful API和动态用户界面，以支持实时交互、配置更新和基于模拟的洞察。案例研究展示了该系统通过防止缺货、降低费用和维持服务级别来改善规划结果。未来扩展包括集成私人LLMs、迁移学习、强化学习和贝叶斯神经网络，以提高可解释性、适应性和实时决策能力。', 'title_zh': '将大规模语言模型与网络优化集成以实现交互式和可解释的供应链规划：一个实际案例研究'}
{'arxiv_id': 'arXiv:2508.21540', 'title': 'HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining', 'authors': 'Eduardo Illueca-Fernandez, Kaile Chen, Fernando Seoane, Farhad Abtahi', 'link': 'https://arxiv.org/abs/2508.21540', 'abstract': 'Process mining has emerged as a powerful analytical technique for understanding complex healthcare workflows. However, its application faces significant barriers, including technical complexity, a lack of standardized approaches, and limited access to practical training resources. We introduce HealthProcessAI, a GenAI framework designed to simplify process mining applications in healthcare and epidemiology by providing a comprehensive wrapper around existing Python (PM4PY) and R (bupaR) libraries. To address unfamiliarity and improve accessibility, the framework integrates multiple Large Language Models (LLMs) for automated process map interpretation and report generation, helping translate technical analyses into outputs that diverse users can readily understand. We validated the framework using sepsis progression data as a proof-of-concept example and compared the outputs of five state-of-the-art LLM models through the OpenRouter platform. To test its functionality, the framework successfully processed sepsis data across four proof-of-concept scenarios, demonstrating robust technical performance and its capability to generate reports through automated LLM analysis. LLM evaluation using five independent LLMs as automated evaluators revealed distinct model strengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency scores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By integrating multiple Large Language Models (LLMs) for automated interpretation and report generation, the framework addresses widespread unfamiliarity with process mining outputs, making them more accessible to clinicians, data scientists, and researchers. This structured analytics and AI-driven interpretation combination represents a novel methodological advance in translating complex process mining results into potentially actionable insights for healthcare applications.', 'abstract_zh': 'HealthProcessAI：一种集成大型语言模型的GenAI框架，用于简化医疗和流行病学中的过程挖掘应用', 'title_zh': '健康过程AI：增强型LLM辅助医疗过程挖掘的技术框架与概念验证'}
{'arxiv_id': 'arXiv:2508.21376', 'title': 'AHELM: A Holistic Evaluation of Audio-Language Models', 'authors': 'Tony Lee, Haoqin Tu, Chi Heem Wong, Zijun Wang, Siwei Yang, Yifan Mai, Yuyin Zhou, Cihang Xie, Percy Liang', 'link': 'https://arxiv.org/abs/2508.21376', 'abstract': 'Evaluations of audio-language models (ALMs) -- multimodal models that take interleaved audio and text as input and output text -- are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets -- including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering -- to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with one ranking 5th overall despite having only speech-to-text capabilities. For transparency, all raw prompts, model generations, and outputs are available on our website at this https URL. AHELM is intended to be a living benchmark and new datasets and models will be added over time.', 'abstract_zh': '音语言模型（ALMs）的评估——接受交错音频和文本作为输入并输出文本的多模态模型——受到缺乏标准化基准的阻碍；大多数基准仅衡量一两种能力，并省略了诸如公平性或安全性之类的评估方面。此外，模型之间的比较困难，因为单独的评估测试的模型有限且使用不同的提示方法和推理参数。为了解决这些不足，我们引入了AHELM基准，整合了多种数据集，包括两个新的合成音频-文本数据集PARADE，用于评估ALMs避免刻板印象的能力，以及CoRe-Bench，用于通过推理多轮问答来衡量对话音频上的推理能力，以全面衡量ALMs在我们识别的十个重要方面（音频感知、知识、推理、情感检测、偏差、公平性、多语言性、鲁棒性、有毒内容和安全性）上的性能。我们还标准化了提示、推理参数和评估指标，以确保模型之间的公平比较。我们测试了来自3个开发者的14个开放权重和闭合API音语言模型，以及3个附加的简单基线系统，每个系统由自动语音识别器和语言模型组成。结果显示，虽然Gemini 2.5 Pro在10个方面中的5个方面排名第一，但在ASR任务中表现出组不公平性（$p=0.01$），而大多数其他模型则没有。我们还发现，基线系统在AHELM上的表现相当不错，其中一个系统排名第五，尽管只有语音到文本的能力。为透明起见，所有原始提示、模型生成和输出均在我们的网站上提供。AHELM旨在成为一种活基准，将来会不断添加新的数据集和模型。', 'title_zh': 'AHELM：音频-语言模型的整体评估'}
{'arxiv_id': 'arXiv:2508.21365', 'title': 'Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models', 'authors': 'Yi Liao, Yu Gu, Yuan Sui, Zining Zhu, Yifan Lu, Guohua Tang, Zhongqian Sun, Wei Yang', 'link': 'https://arxiv.org/abs/2508.21365', 'abstract': 'Large language models (LLMs) excel at complex reasoning tasks such as mathematics and coding, yet they frequently struggle with simple interactive tasks that young children perform effortlessly. This discrepancy highlights a critical gap between declarative knowledge (knowing about something) and procedural knowledge (knowing how to do something). Although traditional reinforcement learning (RL) agents can acquire procedural knowledge through environmental interaction, they often operate as black boxes and require substantial training data. In contrast, LLMs possess extensive world knowledge and reasoning capabilities, but are unable to effectively convert this static knowledge into dynamic decision-making in interactive settings. To address this challenge, we propose Think in Games (TiG), a novel framework that empowers LLMs to develop procedural understanding through direct interaction with game environments, while retaining their inherent reasoning and explanatory abilities. Specifically, TiG reformulates RL-based decision-making as a language modeling task: LLMs generate language-guided policies, which are refined iteratively through online reinforcement learning based on environmental feedback. Our experimental results show that TiG successfully bridges the gap between declarative and procedural knowledge, achieving competitive performance with dramatically lower data and computational demands compared to conventional RL methods. Moreover, TiG provides step-by-step natural language explanations for its decisions, greatly improving transparency and interpretability in complex interactive tasks.', 'abstract_zh': '大型语言模型通过游戏增强程序性理解：Think in Games框架', 'title_zh': '在游戏中思考：通过大规模语言模型强化学习进行游戏推理学习'}
{'arxiv_id': 'arXiv:2508.21320', 'title': 'Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation', 'authors': 'Mohsen Nayebi Kerdabadi, Arya Hadizadeh Moghaddam, Dongjie Wang, Zijun Yao', 'link': 'https://arxiv.org/abs/2508.21320', 'abstract': 'Medical ontology graphs map external knowledge to medical codes in electronic health records via structured relationships. By leveraging domain-approved connections (e.g., parent-child), predictive models can generate richer medical concept representations by incorporating contextual information from related concepts. However, existing literature primarily focuses on incorporating domain knowledge from a single ontology system, or from multiple ontology systems (e.g., diseases, drugs, and procedures) in isolation, without integrating them into a unified learning structure. Consequently, concept representation learning often remains limited to intra-ontology relationships, overlooking cross-ontology connections. In this paper, we propose LINKO, a large language model (LLM)-augmented integrative ontology learning framework that leverages multiple ontology graphs simultaneously by enabling dual-axis knowledge propagation both within and across heterogeneous ontology systems to enhance medical concept representation learning. Specifically, LINKO first employs LLMs to provide a graph-retrieval-augmented initialization for ontology concept embedding, through an engineered prompt that includes concept descriptions, and is further augmented with ontology context. Second, our method jointly learns the medical concepts in diverse ontology graphs by performing knowledge propagation in two axes: (1) intra-ontology vertical propagation across hierarchical ontology levels and (2) inter-ontology horizontal propagation within every level in parallel. Last, through extensive experiments on two public datasets, we validate the superior performance of LINKO over state-of-the-art baselines. As a plug-in encoder compatible with existing EHR predictive models, LINKO further demonstrates enhanced robustness in scenarios involving limited data availability and rare disease prediction.', 'abstract_zh': '多 ontology 图辅助的大型语言模型增强集成本体学习框架：提升医疗概念表示学习', 'title_zh': '基于双轴传播的多本体集成与医学概念表示'}
{'arxiv_id': 'arXiv:2508.21238', 'title': "Addressing accuracy and hallucination of LLMs in Alzheimer's disease research through knowledge graphs", 'authors': 'Tingxuan Xu, Jiarui Feng, Justin Melendez, Kaleigh Roberts, Donghong Cai, Mingfang Zhu, Donald Elbert, Yixin Chen, Randall J. Bateman', 'link': 'https://arxiv.org/abs/2508.21238', 'abstract': "In the past two years, large language model (LLM)-based chatbots, such as ChatGPT, have revolutionized various domains by enabling diverse task completion and question-answering capabilities. However, their application in scientific research remains constrained by challenges such as hallucinations, limited domain-specific knowledge, and lack of explainability or traceability for the response. Graph-based Retrieval-Augmented Generation (GraphRAG) has emerged as a promising approach to improving chatbot reliability by integrating domain-specific contextual information before response generation, addressing some limitations of standard LLMs. Despite its potential, there are only limited studies that evaluate GraphRAG on specific domains that require intensive knowledge, like Alzheimer's disease or other biomedical domains. In this paper, we assess the quality and traceability of two popular GraphRAG systems. We compile a database of 50 papers and 70 expert questions related to Alzheimer's disease, construct a GraphRAG knowledge base, and employ GPT-4o as the LLM for answering queries. We then compare the quality of responses generated by GraphRAG with those from a standard GPT-4o model. Additionally, we discuss and evaluate the traceability of several Retrieval-Augmented Generation (RAG) and GraphRAG systems. Finally, we provide an easy-to-use interface with a pre-built Alzheimer's disease database for researchers to test the performance of both standard RAG and GraphRAG.", 'abstract_zh': '过去两年，基于大型语言模型（LLM）的聊天机器人，如ChatGPT，通过实现多样化的任务完成和问答能力，颠覆了多个领域。然而，它们在科学研究中的应用仍受到幻觉、领域特定知识有限以及响应缺乏可解释性和可追溯性等挑战的制约。基于图的检索增强生成（GraphRAG）作为一种方法，通过在响应生成前整合领域特定的上下文信息，有望改善聊天机器人的可靠性，从而解决标准LLM的一些局限性。尽管具有潜力，但在需要密集专业知识的具体领域（如阿尔茨海默病或其它生物医药领域）中，仅有限的研究对GraphRAG进行了评估。在本文中，我们评估了两种流行GraphRAG系统的质量和可追溯性。我们收集了50篇与阿尔茨海默病相关的论文和70个专家问题，构建了一个GraphRAG知识库，并使用GPT-4o作为LLM以回答查询。随后，我们将GraphRAG生成的响应质量与标准GPT-4o模型的响应质量进行了比较。此外，我们讨论并评估了几种检索增强生成（RAG）和GraphRAG系统的可追溯性。最后，我们提供了一个易于使用的界面，内置了阿尔茨海默病数据库，以供研究人员测试标准RAG和GraphRAG的性能。', 'title_zh': '通过知识图谱解决阿尔茨海默病研究中大语言模型的准确性和幻觉问题'}
{'arxiv_id': 'arXiv:2508.21204', 'title': 'Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding', 'authors': 'Vanessa Figueiredo', 'link': 'https://arxiv.org/abs/2508.21204', 'abstract': 'We study how architectural inductive biases influence the cognitive behavior of large language models (LLMs) in instructional dialogue. We introduce a symbolic scaffolding mechanism paired with a short-term memory schema designed to promote adaptive, structured reasoning in Socratic tutoring. Using controlled ablation across five system variants, we evaluate model outputs via expert-designed rubrics covering scaffolding, responsiveness, symbolic reasoning, and conversational memory. We present preliminary results using an LLM-based evaluation framework aligned to a cognitively grounded rubric. This enables scalable, systematic comparisons across architectural variants in early-stage experimentation. The preliminary results show that our full system consistently outperforms baseline variants. Analysis reveals that removing memory or symbolic structure degrades key cognitive behaviors, including abstraction, adaptive probing, and conceptual continuity. These findings support a processing-level account in which architectural scaffolds can reliably shape emergent instructional strategies in LLMs.', 'abstract_zh': '我们研究了建筑学诱导偏见如何影响大型语言模型（LLM）在教学对话中的认知行为。我们引入了一种符号支撑机制，结合一种短期记忆架构，旨在促进苏格拉底式辅导中的适应性和结构化推理。通过跨越五种系统变体的受控剥离分析，我们使用由专家设计的标准来评估模型输出，涵盖支撑、响应性、符号推理和对话记忆。我们采用与认知基础标准相一致的LLM评估框架呈现初步结果。这使得在早期实验中能够进行可扩展和系统的架构变体比较。初步结果表明，我们的完整系统始终优于基线变体。分析表明，移除记忆或符号结构会降低抽象、适应性探查和概念连续性等关键认知行为。这些发现支持一个处理层面的解释，即架构支撑可以可靠地塑造LLM中的 emergent 教学策略。', 'title_zh': '模糊性、符号性与情境性：通过认知支架增强大语言模型指令训练'}
{'arxiv_id': 'arXiv:2508.21788', 'title': 'Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval', 'authors': 'Inés Altemir Marinas, Anastasiia Kucherenko, Andrei Kucharavy', 'link': 'https://arxiv.org/abs/2508.21788', 'abstract': "Large language models (LLMs) rely heavily on web-scale datasets like Common Crawl, which provides over 80\\% of training data for some modern models. However, the indiscriminate nature of web crawling raises challenges in data quality, safety, and ethics. Despite the critical importance of training data quality, prior research on harmful content has been limited to small samples due to computational constraints. This project presents a framework for indexing and analyzing LLM training datasets using an ElasticSearch-based pipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages), achieving fast query performance--most searches in milliseconds, all under 2 seconds. Our work demonstrates real-time dataset analysis, offering practical tools for safer, more accountable AI systems.", 'abstract_zh': '大规模语言模型（LLMs） heavily依赖像Common Crawl这样的Web规模数据集，为某些现代模型提供了超过80%的训练数据。然而，无差别的Web抓取性质在数据质量、安全性和伦理方面提出了挑战。尽管训练数据质量至关重要，但由于计算限制，前期研究针对有害内容的样本有限。本项目提出了一种使用基于ElasticSearch的管道进行索引和分析LLMs训练数据集的框架。我们将该框架应用于SwissAI的FineWeb-2语料库（1.5TB，四种语言），实现了快速查询性能——大多数查询在毫秒级，所有查询在2秒以内。我们的工作展示了实时数据集分析，提供了更安全、更负责任的AI系统的实用工具。', 'title_zh': '细梳详查细网：问题内容搜索与检索的索引技术报告'}
{'arxiv_id': 'arXiv:2508.21787', 'title': 'PiCSAR: Probabilistic Confidence Selection And Ranking', 'authors': 'Joshua Ong Jun Leang, Zheng Zhao, Aryo Pradipta Gema, Sohee Yang, Wai-Chung Kwan, Xuanli He, Wenda Li, Pasquale Minervini, Eleonora Giunchiglia, Shay B. Cohen', 'link': 'https://arxiv.org/abs/2508.21787', 'abstract': 'Best-of-n sampling improves the accuracy of large language models (LLMs) and large reasoning models (LRMs) by generating multiple candidate solutions and selecting the one with the highest reward. The key challenge for reasoning tasks is designing a scoring function that can identify correct reasoning chains without access to ground-truth answers. We propose Probabilistic Confidence Selection And Ranking (PiCSAR): a simple, training-free method that scores each candidate generation using the joint log-likelihood of the reasoning and final answer. The joint log-likelihood of the reasoning and final answer naturally decomposes into reasoning confidence and answer confidence. PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500, +9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in 16 out of 20 comparisons. Our analysis reveals that correct reasoning chains exhibit significantly higher reasoning and answer confidence, justifying the effectiveness of PiCSAR.', 'abstract_zh': 'Best-of-n 抽样通过生成多个候选解决方案并选择得分最高的一个，提高了大规模语言模型（LLMs）和大规模推理模型（LRMs）的准确性。对于推理任务的关键挑战是设计一个评分函数，能够在不访问真实答案的情况下识别正确的推理链。我们提出了一种无需训练的简单方法 Probabilistic Confidence Selection And Ranking (PiCSAR)：该方法使用推理和最终答案的联合对数似然性对每个候选生成进行评分。推理和最终答案的联合对数似然性自然地分解为推理置信度和答案置信度。PiCSAR 在多种基准测试中取得了显著提升（MATH500 上提升了 10.18，AIME2025 上提升了 9.81），在 20 次比较中有 16 次超过了基线模型，且使用样本的数量至少减少了 2 倍。我们的分析表明，正确的推理链显示出显著更高的推理和答案置信度，这证明了 PiCSAR 的有效性。', 'title_zh': 'PiCSAR: 概率置信度选择和排序'}
{'arxiv_id': 'arXiv:2508.21777', 'title': 'Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but Persistent Need for Expert Oversight', 'authors': 'Ugur Dinc, Jibak Sarkar, Philipp Schubert, Sabine Semrau, Thomas Weissmann, Andre Karius, Johann Brand, Bernd-Niklas Axer, Ahmed Gomaa, Pluvio Stephan, Ishita Sheth, Sogand Beirami, Annette Schwarz, Udo Gaipl, Benjamin Frey, Christoph Bert, Stefanie Corradini, Rainer Fietkau, Florian Putz', 'link': 'https://arxiv.org/abs/2508.21777', 'abstract': "Introduction: Large language models (LLM) have shown great potential in clinical decision support. GPT-5 is a novel LLM system that has been specifically marketed towards oncology use.\nMethods: Performance was assessed using two complementary benchmarks: (i) the ACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300 multiple-choice items, and (ii) a curated set of 60 authentic radiation oncologic vignettes representing diverse disease sites and treatment indications. For the vignette evaluation, GPT-5 was instructed to generate concise therapeutic plans. Four board-certified radiation oncologists rated correctness, comprehensiveness, and hallucinations. Inter-rater reliability was quantified using Fleiss' \\k{appa}.\nResults: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%, outperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were most pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's treatment recommendations were rated highly for correctness (mean 3.24/4, 95% CI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69). Hallucinations were rare with no case reaching majority consensus for their presence. Inter-rater agreement was low (Fleiss' \\k{appa} 0.083 for correctness), reflecting inherent variability in clinical judgment. Errors clustered in complex scenarios requiring precise trial knowledge or detailed clinical adaptation.\nDiscussion: GPT-5 clearly outperformed prior model variants on the radiation oncology multiple-choice benchmark. Although GPT-5 exhibited favorable performance in generating real-world radiation oncology treatment recommendations, correctness ratings indicate room for further improvement. While hallucinations were infrequent, the presence of substantive errors underscores that GPT-5-generated recommendations require rigorous expert oversight before clinical implementation.", 'abstract_zh': 'Introduction: 大型语言模型(LLM)在临床决策支持方面显示出巨大潜力。GPT-5是一种专门针对肿瘤学使用的新颖LLM系统。\n\nMethods: 性能评估使用了两个互补的标准：(i) ACR放射肿瘤学在职考试(TXIT, 2021)，包含300个多选题；(ii) 一个精心挑选的包含60个真实放射肿瘤学案例集，涵盖了多种疾病部位和治疗适应症。对于案例评估，GPT-5被指示生成简明的治疗方案。四位放射肿瘤学专科医生评估了治疗方案的正确性、全面性和虚构内容。信度使用Fleiss的\\kappa进行量化。\n\nResults: 在TXIT基准测试中，GPT-5的平均正确率为92.8%，Performance优于GPT-4（78.8%）和GPT-3.5（62.1%）。在专业领域内，收益最显著的是剂量和诊断。在案例评估中，GPT-5的治疗建议在正确性（平均3.24/4，95% CI: 3.11-3.38）和全面性（3.59/4，95% CI: 3.49-3.69）方面得到了高度评价。虚构内容很少见，没有一例达到了多数共识。正确性评定的信度较低（Fleiss的\\kappa为0.083），反映出临床判断的固有变异性。错误多出现在需要精准实验知识或详细临床适应的复杂场景中。\n\nDiscussion: GPT-5在放射肿瘤学多项选择基准测试中的表现明显优于先前的模型版本。尽管GPT-5在生成真实世界放射肿瘤学治疗建议方面的表现优异，但正确性评分表明有进一步改进的空间。虽然虚构内容很少见，实质性错误的存在表明，在临床实施前，GPT-5生成的建议需要严格的专家监督。', 'title_zh': '在放射肿瘤学中基准测试GPT-5：可测量的收益，但仍需持续的专业监督'}
{'arxiv_id': 'arXiv:2508.21762', 'title': 'Reasoning-Intensive Regression', 'authors': 'Diane Tchuindjo, Omar Khattab', 'link': 'https://arxiv.org/abs/2508.21762', 'abstract': 'AI researchers and practitioners increasingly apply large language models (LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing subtle numerical properties from text. Unlike standard language regression tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc problems like rubric-based scoring or domain-specific retrieval, where much deeper analysis of text is required while only limited task-specific training data and computation are available. We cast three realistic problems as RiR tasks to establish an initial benchmark, and use that to test our hypothesis that prompting frozen LLMs and finetuning Transformer encoders via gradient descent will both often struggle in RiR. We then propose MENTAT, a simple and lightweight method that combines batch-reflective prompt optimization with neural ensemble learning. MENTAT achieves up to 65% improvement over both baselines, though substantial room remains for future advances in RiR.', 'abstract_zh': 'AI研究人员和 practitioners 逐渐将大规模语言模型 (LLMs) 应用于我们称之为推理密集型回归 (RiR) 的任务中，即从文本中推导出细微的数值属性。与标准语言回归任务，如情感分析或相似度分析不同，RiR 经常出现在如评分表评分或领域特定检索等即兴问题中，这些任务需要更深入的文本分析，但仅有有限的任务特定训练数据和计算资源可用。我们将三个现实问题作为 RiR 任务以建立初始基准，并通过此基准测试我们的假设：冻结的 LLMs 在提示调优和通过梯度下降微调 Transformer 编码器时通常都会在 RiR 中遇到困难。随后，我们提出了 MENTAT，一种简单且轻量的方法，结合了批量反思提示优化与神经集成学习。尽管如此，RiR 仍然有巨大的未来进步空间，MENTAT 相较于基准方法实现了高达 65% 的性能提升。', 'title_zh': '密集推理回归'}
{'arxiv_id': 'arXiv:2508.21589', 'title': 'Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning', 'authors': 'Zinan Tang, Xin Gao, Qizhi Pei, Zhuoshi Pan, Mengzhang Cai, Jiang Wu, Conghui He, Lijun Wu', 'link': 'https://arxiv.org/abs/2508.21589', 'abstract': "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely on high-quality training data. While data selection and data synthesis are two common strategies to improve data quality, existing approaches often face limitations in static dataset curation that fail to adapt to evolving model capabilities. In this paper, we introduce Middo, a self-evolving Model-informed dynamic data optimization framework that uses model-aware data selection and context-preserving data refinement. Unlike conventional one-off filtering/synthesis methods, our framework establishes a closed-loop optimization system: (1) A self-referential diagnostic module proactively identifies suboptimal samples through tri-axial model signals - loss patterns (complexity), embedding cluster dynamics (diversity), and self-alignment scores (quality); (2) An adaptive optimization engine then transforms suboptimal samples into pedagogically valuable training points while preserving semantic integrity; (3) This optimization process continuously evolves with model capability through dynamic learning principles. Experiments on multiple benchmarks demonstrate that our \\method consistently enhances the quality of seed data and boosts LLM's performance with improving accuracy by 7.15% on average while maintaining the original dataset scale. This work establishes a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models. Our datasets, models, and code are coming soon.", 'abstract_zh': '监督微调（SFT）大型语言模型（LLM）从根本上依赖高质量的训练数据。Middo：一种基于模型的自适应动态数据优化框架，通过模型意识的数据选择和语境保留的数据精炼实现自我演进。', 'title_zh': 'Middo: 基于模型的动态数据优化以通过闭环学习增强LLM微调'}
{'arxiv_id': 'arXiv:2508.21476', 'title': 'Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards', 'authors': 'Xiaolong Wei, Bo Lu, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin', 'link': 'https://arxiv.org/abs/2508.21476', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable creative writing capabilities, yet their substantial computational demands hinder widespread use. Enhancing Small Language Models (SLMs) offers a promising alternative, but current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and Reinforcement Learning from Human Feedback (RLHF) is costly. This paper explores two distinct AI-driven reward strategies within a Reinforcement Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a 7B-parameter SLM, specifically for generating Chinese greetings. The first strategy employs a RM trained on high-quality preference data curated by a novel multi-agent rejection sampling framework designed for creative tasks. The second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose reward function is optimized via an adversarial training scheme with a reflection mechanism, to directly provide reward signals. Comprehensive experiments reveal that while both approaches significantly enhance creative output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields superior generation quality. Furthermore, it offers notable advantages in training efficiency and reduced dependency on human-annotated data, presenting a more scalable and effective path towards creative SLMs. Our automated evaluation methods also exhibit strong alignment with human judgments. Our code and data are publicly available at this https URL.', 'abstract_zh': '大规模语言模型（LLMs）展示了卓越的创造性写作能力，但其巨大的计算需求阻碍了其广泛应用。增强小型语言模型（SLMs）提供了一种有希望的替代方案，但当前的方法如监督微调（SFT）在新颖性方面存在不足，强化学习从人类反馈（RLHF）则成本较高。本文在AI反馈强化学习（RLAIF）框架内探索了两种独特的AI驱动的奖励策略，以激发一个包含70亿参数的小型语言模型的创造性写作能力，特别用于生成中文问候语。第一种策略采用了一种基于新设计的多智能体拒绝采样框架训练的偏好模型，该框架旨在针对创造任务。第二种更为新颖的策略则利用了一个原理引导下的LLM作为裁判，其奖励函数通过与反向训练方案结合的反思机制进行优化，直接提供奖励信号。全面的实验表明，虽然两种方法在基线之上显著提升了创造性输出，但原理引导下的LLM作为裁判明显产生了更高的生成质量。此外，其在训练效率和减少对人工标注数据的依赖方面具有显著优势，为创造性SLM的发展提供了更加可扩展和有效的方法。我们的自动评估方法也与人类判断表现出较强的一致性。我们的代码和数据已在此网页公开。', 'title_zh': '在小型语言模型中激发创造性写作：LLM作为裁判与多智能体精炼奖励的对比'}
{'arxiv_id': 'arXiv:2508.21433', 'title': 'The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management', 'authors': 'Tobias Lindenbauer, Igor Slinko, Ludwig Felder, Egor Bogomolov, Yaroslav Zharov', 'link': 'https://arxiv.org/abs/2508.21433', 'abstract': 'Large Language Model (LLM)-based agents solve complex tasks through iterative reasoning, exploration, and tool-use, a process that can result in long, expensive context histories. While state-of-the-art Software Engineering ( SE) agents like OpenHands or Cursor use LLM-based summarization to tackle this issue, it is unclear whether the increased complexity offers tangible performance benefits compared to simply omitting older observations. We present a systematic comparison of these strategies within SWE-agent on SWE-bench Verified across five diverse model configurations. We find that a simple observation-masking strategy halves cost relative to a raw agent while matching, and sometimes slightly exceeding, the solve rate of LLM summarization. For example, with Qwen3-Coder 480B, masking improves solve rate from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization at a lower cost. These results suggest that, at least within SWE-agent on SWE-bench Verified, the most effective and efficient context management can be the simplest. We release code and data for reproducibility', 'abstract_zh': '基于大型语言模型的代理通过迭代推理、探索和工具使用来解决复杂任务，这一过程可能会产生长且昂贵的历史上下文。虽然像OpenHands或Cursor这样的最新软件工程代理使用基于大型语言模型的总结来应对这一问题，但尚不清楚增加的复杂性是否能提供实际的性能优势，相比直接省略较旧的观察结果。在SWE-agent上，我们在SWE-bench Verified的五个不同模型配置中系统比较了这些策略。我们发现，简单的观察屏蔽策略使成本减半，同时与基于大型语言模型的总结匹配，有时甚至稍微超过总结的解题率。例如，使用Qwen3-Coder 480B时，屏蔽提高了解题率从53.8%（原始代理）到54.8%，同时以较低成本保持与总结的竞争力。这些结果表明，在SWE-agent上，SWE-bench Verified至少在最有效和高效的上下文管理上，最简单的做法可能是最好的。我们公布代码和数据以确保可再现性。', 'title_zh': '简单观察掩蔽与大规模语言模型总结一样高效，用于代理情境管理的复杂性陷阱'}
{'arxiv_id': 'arXiv:2508.21393', 'title': 'zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs', 'authors': 'Guofu Liao, Taotao Wang, Shengli Zhang, Jiqun Zhang, Shi Long, Dacheng Tao', 'link': 'https://arxiv.org/abs/2508.21393', 'abstract': 'Fine-tuning large language models (LLMs) is crucial for adapting them to specific tasks, yet it remains computationally demanding and raises concerns about correctness and privacy, particularly in untrusted environments. Although parameter-efficient methods like Low-Rank Adaptation (LoRA) significantly reduce resource requirements, ensuring the security and verifiability of fine-tuning under zero-knowledge constraints remains an unresolved challenge. To address this, we introduce zkLoRA, the first framework to integrate LoRA fine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and correctness. zkLoRA employs advanced cryptographic techniques -- such as lookup arguments, sumcheck protocols, and polynomial commitments -- to verify both arithmetic and non-arithmetic operations in Transformer-based architectures. The framework provides end-to-end verifiability for forward propagation, backward propagation, and parameter updates during LoRA fine-tuning, while safeguarding the privacy of model parameters and training data. Leveraging GPU-based implementations, zkLoRA demonstrates practicality and efficiency through experimental validation on open-source LLMs like LLaMA, scaling up to 13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs, zkLoRA bridges a critical gap, enabling secure and trustworthy deployment of LLMs in sensitive or untrusted environments.', 'abstract_zh': '基于零知识证明的LoRA细调框架：实现可验证的安全与正确性', 'title_zh': 'zkLoRA: 通过零知识证明实现可验证安全的大规模语言模型微调'}
{'arxiv_id': 'arXiv:2508.21377', 'title': 'Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models', 'authors': 'Shubham Sharma, Sneha Tuli, Narendra Badam', 'link': 'https://arxiv.org/abs/2508.21377', 'abstract': "Large Language Models (LLMs) are transforming AI across industries, but their development and deployment remain complex. This survey reviews 16 key challenges in building and using LLMs and examines how these challenges are addressed by two state-of-the-art models with unique approaches: OpenAI's closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a large open source Mixture-of-Experts model. Through this comparison, we showcase the trade-offs between closed source models (robust safety, fine-tuned reliability) and open source models (efficiency, adaptability). We also explore LLM applications across different domains (from chatbots and coding tools to healthcare and education), highlighting which model attributes are best suited for each use case. This article aims to guide AI researchers, developers, and decision-makers in understanding current LLM capabilities, limitations, and best practices.", 'abstract_zh': '大规模语言模型（LLMs）正在跨行业transform AI，但其研发和部署仍然复杂。本文回顾了构建和使用LLMs的16个关键挑战，并探讨了两个采用独特方法的前沿模型OpenAI的GPT-4o（2024年5月更新）和DeepSeek-V3-0324（2025年3月）是如何应对这些挑战的。通过比较这两种模型，我们展示了闭源模型（稳健的安全性，微调的可靠性）与开源模型（效率，适应性）之间的权衡。同时，我们探讨了LLMs在不同领域的应用（从聊天机器人和编码工具到医疗和教育），指出了哪些模型特性最适合每种应用场景。本文旨在指导AI研究人员、开发人员和决策者了解当前LLM的能力、局限性和最佳实践。', 'title_zh': '大型语言模型的挑战与应用：GPT与DeepSeek家族模型的比较'}
{'arxiv_id': 'arXiv:2508.21368', 'title': 'EconAgentic in DePIN Markets: A Large Language Model Approach to the Sharing Economy of Decentralized Physical Infrastructure', 'authors': 'Yulin Liu, Mocca Schweitzer', 'link': 'https://arxiv.org/abs/2508.21368', 'abstract': "The Decentralized Physical Infrastructure (DePIN) market is revolutionizing the sharing economy through token-based economics and smart contracts that govern decentralized operations. By 2024, DePIN projects have exceeded \\$10 billion in market capitalization, underscoring their rapid growth. However, the unregulated nature of these markets, coupled with the autonomous deployment of AI agents in smart contracts, introduces risks such as inefficiencies and potential misalignment with human values. To address these concerns, we introduce EconAgentic, a Large Language Model (LLM)-powered framework designed to mitigate these challenges. Our research focuses on three key areas: 1) modeling the dynamic evolution of DePIN markets, 2) evaluating stakeholders' actions and their economic impacts, and 3) analyzing macroeconomic indicators to align market outcomes with societal goals. Through EconAgentic, we simulate how AI agents respond to token incentives, invest in infrastructure, and adapt to market conditions, comparing AI-driven decisions with human heuristic benchmarks. Our results show that EconAgentic provides valuable insights into the efficiency, inclusion, and stability of DePIN markets, contributing to both academic understanding and practical improvements in the design and governance of decentralized, tokenized economies.", 'abstract_zh': "基于代币经济和智能合约的去中心化物理基础设施（DePIN）市场正在通过去中心化运营革新共享经济。到2024年，DePIN项目市场资本已超过100亿美元，凸显其迅猛增长。然而，这些市场的无监管性质以及智能合约中自主部署的人工智能代理带来的潜在风险，如低效率和与人类价值取向的偏差，需要加以应对。为此，我们提出了EconAgentic框架，这是一种以大规模语言模型为动力的框架，旨在缓解这些挑战。我们的研究集中在三个方面：1） modeling the dynamic evolution of DePIN markets（建模DePIN市场的动态演变），2） evaluating stakeholders' actions and their economic impacts（评估相关方行为及其经济影响），3） analyzing macroeconomic indicators to align market outcomes with societal goals（分析宏观经济指标以使市场结果与社会目标保持一致）。通过EconAgentic，我们模拟了人工智能代理如何响应代币激励、投资基础设施并适应市场条件，并将AI驱动的决策与人类启发式基准进行了对比。结果表明，EconAgentic为理解和改进去中心化代币化经济的设计与治理提供了有价值的见解。", 'title_zh': '基于大语言模型的DePIN市场经济代理研究：去中心化物理基础设施共享经济探索'}
{'arxiv_id': 'arXiv:2508.21294', 'title': 'BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning', 'authors': 'João Guilherme Alves Santos, Giovana Kerche Bonás, Thales Sales Almeida', 'link': 'https://arxiv.org/abs/2508.21294', 'abstract': 'With the growing capabilities of Large Language Models (LLMs), there is an increasing need for robust evaluation methods, especially in multilingual and non-English contexts. We present an updated version of the BLUEX dataset, now including 2024-2025 exams and automatically generated image captions using state-of-the-art models, enhancing its relevance for data contamination studies in LLM pretraining. Captioning strategies increase accessibility to text-only models by more than 40%, producing 1,422 usable questions, more than doubling the number in the original BLUEX. We evaluated commercial and open-source LLMs and their ability to leverage visual context through captions.', 'abstract_zh': '随着大型语言模型（LLMs）能力的不断增强，尤其是在多语言和非英语环境中，对 robust 评估方法的需求也在增加。我们更新了 BLUEX 数据集，其中包括 2024-2025 年的考试和使用先进模型自动生成的图像描述，增强了其在 LLM 预训练数据污染研究方面的相关性。图像描述策略将仅文本模型的访问性提高了超过 40%，生成了 1,422 个可用问题，数量是原始 BLUEX 的两倍多。我们评估了商业和开源 LLM 以及它们通过图像描述利用视觉上下文的能力。', 'title_zh': 'BLUEX 重访：通过自动字幕增强基准覆盖范围'}
{'arxiv_id': 'arXiv:2508.21285', 'title': 'A Financial Brain Scan of the LLM', 'authors': 'Hui Chen, Antoine Didisheim, Luciano Somoza, Hanqing Tian', 'link': 'https://arxiv.org/abs/2508.21285', 'abstract': 'Emerging techniques in computer science make it possible to "brain scan" large language models (LLMs), identify the plain-English concepts that guide their reasoning, and steer them while holding other factors constant. We show that this approach can map LLM-generated economic forecasts to concepts such as sentiment, technical analysis, and timing, and compute their relative importance without reducing performance. We also show that models can be steered to be more or less risk-averse, optimistic, or pessimistic, which allows researchers to correct or simulate biases. The method is transparent, lightweight, and replicable for empirical research in the social sciences.', 'abstract_zh': '新兴的计算机科学技术使人们能够“扫描”大型语言模型（LLMs），识别指导其推理的简单英语概念，并在保持其他因素不变的情况下引导它们。我们展示了这种方法可以将LLM生成的经济预测映射到情感、技术分析和时间性等概念，并在不降低性能的情况下计算它们的相对重要性。我们还展示了可以通过引导模型变得更加或不那么风险averse、乐观或悲观，从而使研究人员能够纠正或模拟偏见。该方法具有透明性、轻量级和可重复性，适用于社会科学中的实证研究。', 'title_zh': 'LLM的财务脑扫描'}
{'arxiv_id': 'arXiv:2508.21228', 'title': 'Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection', 'authors': 'Weizhi Gao, Xiaorui Liu, Feiyi Wang, Dan Lu, Junqi Yin', 'link': 'https://arxiv.org/abs/2508.21228', 'abstract': 'Large language models (LLMs) have demonstrated impressive performance in both research and real-world applications, but they still struggle with hallucination. Existing hallucination detection methods often perform poorly on sentence-level generation or rely heavily on domain-specific knowledge. While self-consistency approaches help address these limitations, they incur high computational costs due to repeated generation. In this paper, we conduct the first study on identifying redundancy in self-consistency methods, manifested as shared prefix tokens across generations, and observe that non-exact-answer tokens contribute minimally to the semantic content. Based on these insights, we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation through selective inference and annealed decoding. Being orthogonal to the model, dataset, decoding strategy, and self-consistency baseline, our DMP consistently improves the efficiency of multi-response generation and holds promise for extension to alignment and reasoning tasks. Extensive experiments show that our method achieves up to a 3x speedup without sacrificing AUROC performance.', 'abstract_zh': '大型语言模型在研究和实际应用中展现出了令人印象深刻的性能，但仍存在幻觉问题。现有的幻觉检测方法在句子级生成上表现不佳，或严重依赖领域特定知识。虽然自一致性方法有助于解决这些问题，但由于重复生成引起的高计算成本，限制了其应用。本文首次研究了自一致性方法中的冗余性，表现为生成过程中的共享前缀令牌，并观察到非精确答案令牌对语义内容的贡献极小。基于这些洞察，我们提出了一种新型解码记忆管道（DMP），通过选择性推理和退火解码加速生成过程。DMP与模型、数据集、解码策略和自一致性基线无关，能一致地提高多响应生成的效率，并有望扩展到对齐和推理任务中。广泛实验证明，我们的方法在不牺牲AUROC性能的情况下最多可实现3倍的加速。', 'title_zh': '解码记忆：一种高效的一致性幻觉检测管道'}
{'arxiv_id': 'arXiv:2508.21186', 'title': 'Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to Softmax Equilibrium', 'authors': 'Christopher R. Lee-Jenkins', 'link': 'https://arxiv.org/abs/2508.21186', 'abstract': "Decoding in large language models is often described as scoring tokens and normalizing with softmax. We give a minimal, self-contained account of this step as a constrained variational principle on the probability simplex. The discrete, normalization-respecting ascent is the classical multiplicative-weights (entropic mirror) update; its continuous-time limit is the replicator flow. From these ingredients we prove that, for a fixed context and temperature, the next-token distribution follows a smooth trajectory inside the simplex and converges to the softmax equilibrium. This formalizes the common ``manifold traversal'' intuition at the output-distribution level. The analysis yields precise, practice-facing consequences: temperature acts as an exact rescaling of time along the same trajectory, while top-k and nucleus sampling restrict the flow to a face with identical guarantees. We also outline a controlled account of path-dependent score adjustments and their connection to loop-like, hallucination-style behavior. We make no claims about training dynamics or internal representations; those are deferred to future work.", 'abstract_zh': '大型语言模型中的解码通常被描述为对令牌进行评分并使用softmax进行规范化。我们以约束变分原理在概率单纯形上的形式给出这一步骤的 minimalist、自包含说明。离散的、遵守规范化准则的上升过程是经典的乘子权重（对数镜像）更新；其连续时间极限是复制流。从这些成分出发，我们证明，在固定上下文和温度的情况下，下一个令牌的分布沿单纯形内的平滑轨迹移动，并最终收敛到softmax稳态。这在输出分布层面上正式化了常见的“流形遍历”直觉。分析给出了精确的、面向实践的结果：温度作为时间沿相同轨迹的精确缩放，而top-k和nucleus采样将流限制在具有相同保证的面上。我们还概述了路径依赖得分调整的受控说明及其与环形幻觉行为的联系。我们不对训练动力学或内部表示发表任何声明；这些内容将留待未来工作。', 'title_zh': '流形轨迹在下一个词预测中的建模：从复制动态到Softmax平衡'}
{'arxiv_id': 'arXiv:2508.21184', 'title': 'BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design', 'authors': 'Deepro Choudhury, Sinead Williamson, Adam Goliński, Ning Miao, Freddie Bickford Smith, Michael Kirchhof, Yizhe Zhang, Tom Rainforth', 'link': 'https://arxiv.org/abs/2508.21184', 'abstract': "We propose a general-purpose approach for improving the ability of Large Language Models (LLMs) to intelligently and adaptively gather information from a user or other external source using the framework of sequential Bayesian experimental design (BED). This enables LLMs to act as effective multi-turn conversational agents and interactively interface with external environments. Our approach, which we call BED-LLM (Bayesian Experimental Design with Large Language Models), is based on iteratively choosing questions or queries that maximize the expected information gain (EIG) about the task of interest given the responses gathered previously. We show how this EIG can be formulated in a principled way using a probabilistic model derived from the LLM's belief distribution and provide detailed insights into key decisions in its construction. Further key to the success of BED-LLM are a number of specific innovations, such as a carefully designed estimator for the EIG, not solely relying on in-context updates for conditioning on previous responses, and a targeted strategy for proposing candidate queries. We find that BED-LLM achieves substantial gains in performance across a wide range of tests based on the 20-questions game and using the LLM to actively infer user preferences, compared to direct prompting of the LLM and other adaptive design strategies.", 'abstract_zh': '基于贝叶斯实验设计的大语言模型通用增强方法', 'title_zh': 'BED-LLM：基于LLM和贝叶斯实验设计的智能信息收集方法'}
{'arxiv_id': 'arXiv:2508.21164', 'title': 'Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations', 'authors': 'Muskan Saraf, Sajjad Rezvani Boroujeni, Justin Beaudry, Hossein Abedi, Tom Bush', 'link': 'https://arxiv.org/abs/2508.21164', 'abstract': 'Large language models (LLMs) are increasingly used to evaluate outputs, yet their judgments may be influenced. This study examines bias in self- and cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions: no labels, true labels, and two false-label scenarios. Blog posts authored by each model were evaluated by all three using both overall preference voting and quality ratings for Coherence, Informativeness, and Conciseness, with all scores expressed as percentages for direct comparison. Results reveal striking asymmetries: the "Claude" label consistently boosts scores, while the "Gemini" label consistently depresses them, regardless of actual content. False labels frequently reversed rankings, producing shifts of up to 50 percentage points in preference votes and up to 12 percentage points in converted quality ratings. Gemini\'s self-scores collapsed under true labels, while Claude\'s self-preference intensified. These findings show that perceived model identity can heavily distort high-level judgments and subtly influence detailed quality ratings, underscoring the need for blind or multimodel evaluation protocols to ensure fairness in LLM benchmarking.', 'abstract_zh': '大型语言模型（LLMs）越来越多地用于评估输出，但其判断可能会受到偏见的影响。本研究考察了ChatGPT、Gemini和Claude在四种条件下的自我评估和相互评估中的偏见：无标签、真实标签以及两种虚假标签情景。每种模型撰写的博客帖子分别由这三种模型使用整体偏好投票和一致性、信息性和简洁性评分进行评估，所有评分均以百分比形式表示，以便直接比较。结果显示存在明显的不对称性：“Claude”标签始终提升评分，而“Gemini”标签始终降低评分，与实际内容无关。虚假标签经常导致排名反转，评分偏好投票可上下波动50个百分点，转换后的质量评分可上下波动12个百分点。Gemini的自我评分在真实标签下崩溃，而Claude的自我偏好则加剧。这些发现表明，模型的身份感知可以严重失真高层次判断，并微妙地影响详细的质量评分，突显了在LLM基准测试中需要采用盲评估或多种模型评估协议以确保公平性的必要性。', 'title_zh': '量化标签诱导偏见在大规模语言模型自我评估和交叉评估中的影响'}
{'arxiv_id': 'arXiv:2508.21148', 'title': 'A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers', 'authors': 'Ming Hu, Chenglong Ma, Wei Li, Wanghan Xu, Jiamin Wu, Jucheng Hu, Tianbin Li, Guohang Zhuang, Jiaqi Liu, Yingzhou Lu, Ying Chen, Chaoyang Zhang, Cheng Tan, Jie Ying, Guocheng Wu, Shujian Gao, Pengcheng Chen, Jiashi Lin, Haitao Wu, Lulu Chen, Fengxiang Wang, Yuanyuan Zhang, Xiangyu Zhao, Feilong Tang, Encheng Su, Junzhi Ning, Xinyao Liu, Ye Du, Changkai Ji, Cheng Tang, Huihui Xu, Ziyang Chen, Ziyan Huang, Jiyao Liu, Pengfei Jiang, Yizhou Wang, Chen Tang, Jianyu Wu, Yuchen Ren, Siyuan Yan, Zhonghua Wang, Zhongxing Xu, Shiyan Su, Shangquan Sun, Runkai Zhao, Zhisheng Zhang, Yu Liu, Fudi Wang, Yuanfeng Ji, Yanzhou Su, Hongming Shan, Chunmei Feng, Jiahao Xu, Jiangtao Yan, Wenhao Tang, Diping Song, Lihao Liu, Yanyan Huang, Lequan Yu, Bin Fu, Shujun Wang, Xiaomeng Li, Xiaowei Hu, Yun Gu, Ben Fei, Zhongying Deng, Benyou Wang, Yuewen Cao, Minjie Shen, Haodong Duan, Jie Xu, Yirong Chen, Fang Yan, Hongxia Hao, Jielan Li, Jiajun Du, Yanbo Wang, Imran Razzak, Chi Zhang, Lijun Wu, Conghui He, Zhaohui Lu, Jinhai Huang, Yihao Liu, Fenghua Ling, Yuqiang Li, Aoran Wang, Qihao Zheng, Nanqing Dong, Tianfan Fu, Dongzhan Zhou, Yan Lu, Wenlong Zhang, Jin Ye, Jianfei Cai, Wanli Ouyang, Yu Qiao, Zongyuan Ge, Shixiang Tang, Junjun He', 'link': 'https://arxiv.org/abs/2508.21148', 'abstract': 'Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.', 'abstract_zh': '科学大型语言模型（Sci-LLMs）正在改变知识在科学研究中的表示、整合和应用方式，但其进步受到科学数据复杂性的制约。本文综述从数据-centric 角度出发，重新构架 Sci-LLMs 的发展为模型与其底层数据基础的共生进化。我们制定了统一的科学数据分类体系和层次化的科学知识模型，强调了多模态、跨尺度和领域特定的挑战，这些挑战将科学语料库区分为一般的自然语言处理数据集。系统回顾了从通用基础模型到跨多个科学学科的专业化模型，并对超过270个预训练和后训练数据集进行了详尽分析，展示了Sci-LLMs独有的需求——异质性、多尺度、充满不确定性的语料库，这些语料库需要能够保持领域不变性和支持跨模态推理的表示。在评估中，我们检验了超过190个基准数据集，并跟踪了从静态考试转向过程导向和发现导向评估的转变，同时采用先进的评估协议。这些数据-centric 分析突出了科学数据发展中持续存在的问题，并讨论了涉及半自动化注释流水线和专家验证的新兴解决方案。最终，我们概述了一种范式转变，即基于Sci-LLMs的自主代理能够积极实验、验证，并为持续演进的知识库贡献内容。本文共同提供了一条路线图，用于构建可信赖的、持续演进的人工智能系统，使其作为真正合作伙伴加速科学发现。', 'title_zh': '科学大型语言模型综述：从数据基础到代理前沿'}
{'arxiv_id': 'arXiv:2508.21113', 'title': 'R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning', 'authors': 'Jie Jiang, Qi Yang, Bolin Ni, Shiming Xiang, Han Hu, Houwen Peng', 'link': 'https://arxiv.org/abs/2508.21113', 'abstract': "Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost.", 'abstract_zh': '具有自适应思考能力的Multimodal Large Language模型R-4B：基于问题复杂度的自动思考机制', 'title_zh': 'R-4B: 通过双模式退火和强化学习激励通用自思考能力在MLLMs中的应用'}
{'arxiv_id': 'arXiv:2508.21111', 'title': 'Automating the Deep Space Network Data Systems; A Case Study in Adaptive Anomaly Detection through Agentic AI', 'authors': 'Evan J. Chou, Lisa S. Locke, Harvey M. Soldan', 'link': 'https://arxiv.org/abs/2508.21111', 'abstract': "The Deep Space Network (DSN) is NASA's largest network of antenna facilities that generate a large volume of multivariate time-series data. These facilities contain DSN antennas and transmitters that undergo degradation over long periods of time, which may cause costly disruptions to the data flow and threaten the earth-connection of dozens of spacecraft that rely on the Deep Space Network for their lifeline. The purpose of this study was to experiment with different methods that would be able to assist JPL engineers with directly pinpointing anomalies and equipment degradation through collected data, and continue conducting maintenance and operations of the DSN for future space missions around our universe. As such, we have researched various machine learning techniques that can fully reconstruct data through predictive analysis, and determine anomalous data entries within real-time datasets through statistical computations and thresholds. On top of the fully trained and tested machine learning models, we have also integrated the use of a reinforcement learning subsystem that classifies identified anomalies based on severity level and a Large Language Model that labels an explanation for each anomalous data entry, all of which can be improved and fine-tuned over time through human feedback/input. Specifically, for the DSN transmitters, we have also implemented a full data pipeline system that connects the data extraction, parsing, and processing workflow all together as there was no coherent program or script for performing these tasks before. Using this data pipeline system, we were able to then also connect the models trained from DSN antenna data, completing the data workflow for DSN anomaly detection. This was all wrapped around and further connected by an agentic AI system, where complex reasoning was utilized to determine the classifications and predictions of anomalous data.", 'abstract_zh': '深空网络（DSN）是NASA最大的天线设施网络，生成大量多变量时间序列数据。这些设施中的DSN天线和发射机在长时间内会逐渐退化，可能导致数据流中断并威胁到依赖DSN的数十个航天器的地面连接。本研究旨在通过采用不同的方法，帮助JPL工程师直接定位异常和设备退化，并继续进行DSN的维护和操作，为未来围绕我们宇宙的空间任务提供保障。为此，我们研究了各种可用于完全重建数据并通过预测分析确定实时数据集中异常数据条目的机器学习技术，并结合使用基于严重程度分类识别出的异常的强化学习子系统和为每个异常数据条目提供解释的大型语言模型，这些模型可以通过人类反馈/输入进行改进和微调。特别地，对于DSN发射机，我们还实现了一个完整的数据管道系统，将数据提取、解析和处理工作流程整合在一起，此前没有相应的程序或脚本执行这些任务。使用此数据管道系统，我们能够连接从DSN天线数据训练的模型，完成DSN异常检测的数据工作流程。所有这些都由一个代理AI系统进一步整合，利用复杂推理来确定异常数据的分类和预测。', 'title_zh': '基于代理人工智能的自适应异常检测案研究：自动化深空网络数据系统'}
{'arxiv_id': 'arXiv:2508.21107', 'title': 'Learning to Generate Unit Test via Adversarial Reinforcement Learning', 'authors': 'Dongjun Lee, Changho Hwang, Kimin Lee', 'link': 'https://arxiv.org/abs/2508.21107', 'abstract': "Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate test generation, yet methods for training LLMs to produce high-quality tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning framework that trains an LLM to generate high-quality unit tests given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via reinforcement learning. The unit test generator is trained to maximize a discrimination reward, which reflects its ability to produce tests that expose faults in the code generator's solutions, and the code generator is trained to maximize a code reward, which reflects its ability to produce solutions that pass the unit tests generated by the test generator. In our experiments, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on human-written ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models such as GPT-4.1 in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for this task.", 'abstract_zh': '基于强化学习的UTRL框架：训练大型语言模型生成高质量单元测试', 'title_zh': '基于对抗强化学习的单元测试生成学习'}
{'arxiv_id': 'arXiv:2508.21097', 'title': 'Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation', 'authors': 'Nazanin Siavash, Armin Moin', 'link': 'https://arxiv.org/abs/2508.21097', 'abstract': "This paper introduces a novel research direction for model-to-text/code transformations by leveraging Large Language Models (LLMs) that can be enhanced with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum and hybrid quantum-classical software systems, where model-driven approaches can help reduce the costs and mitigate the risks associated with the heterogeneous platform landscape and lack of developers' skills. We validate one of the proposed ideas regarding generating code out of UML model instances of software systems. This Python code uses a well-established library, called Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG pipeline that we deploy incorporates sample Qiskit code from public GitHub repositories. Experimental results show that well-engineered prompts can improve CodeBLEU scores by up to a factor of four, yielding more accurate and consistent quantum code. However, the proposed research direction can go beyond this through further investigation in the future by conducting experiments to address our other research questions and ideas proposed here, such as deploying software system model instances as the source of information in the RAG pipelines, or deploying LLMs for code-to-code transformations, for instance, for transpilation use cases.", 'abstract_zh': '本文通过利用增强检索增强生成（RAG）管道的大语言模型（LLMs），提出了模型到文本/代码转换的一种新颖研究方向，重点关注量子和混合量子- classical软件系统，其中模型驱动的方法可以帮助减少异构平台landscape带来的成本和风险，以及缺乏开发者技能的问题。我们验证了将软件系统的UML模型实例生成代码的其中一个提案。该Python代码使用了名为Qiskit的成熟库，在基于门的或量子电路的量子计算机上执行。我们部署的RAG管道包含来自公共GitHub仓库的Qiskit代码样例。实验结果显示，精心设计的提示可以将CodeBLEU得分提高多达四倍，从而生成更准确和一致的量子代码。然而，通过在未来进行实验来进一步研究我们的其他研究问题和提出的其他想法，该研究方向可以取得更进一步的成果，例如将软件系统模型实例用作RAG管道的信息源，或部署LLMs进行代码到代码的转换，例如移植使用案例。', 'title_zh': '基于大型语言模型和检索增强生成的模型驱动量子代码生成'}
