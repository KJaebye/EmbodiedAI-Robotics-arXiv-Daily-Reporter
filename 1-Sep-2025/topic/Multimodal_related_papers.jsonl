{'arxiv_id': 'arXiv:2508.21635', 'title': 'The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics', 'authors': 'Nicolas Soncini, Javier Cremona, Erica Vidal, Maximiliano García, Gastón Castro, Taihú Pire', 'link': 'https://arxiv.org/abs/2508.21635', 'abstract': 'We present a multi-modal dataset collected in a soybean crop field, comprising over two hours of recorded data from sensors such as stereo infrared camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel odometry. This dataset captures key challenges inherent to robotics in agricultural environments, including variations in natural lighting, motion blur, rough terrain, and long, perceptually aliased sequences. By addressing these complexities, the dataset aims to support the development and benchmarking of advanced algorithms for localization, mapping, perception, and navigation in agricultural robotics. The platform and data collection system is designed to meet the key requirements for evaluating multi-modal SLAM systems, including hardware synchronization of sensors, 6-DOF ground truth and loops on long trajectories.\nWe run multimodal state-of-the art SLAM methods on the dataset, showcasing the existing limitations in their application on agricultural settings. The dataset and utilities to work with it are released on this https URL.', 'abstract_zh': '我们呈现了一个在大豆田地中收集的多模态数据集，包含超过两小时的传感器记录数据，这些传感器包括立体红外相机、彩色相机、加速度计、陀螺仪、磁力计、GNSS（单点定位、实时动态和后处理动态）、以及车轮里程计。该数据集捕捉了农业环境中机器人技术固有的关键挑战，包括自然光照变化、运动模糊、崎岖地形以及长时间的知觉 alias 序列。通过解决这些复杂性，该数据集旨在支持用于定位、制图、感知和导航的农业机器人高级算法的研发和基准测试。该平台和数据采集系统设计用于评估多模态 SLAM 系统的关键需求，包括传感器的硬件同步、6-DOF 地面真实值和长轨迹上的闭环。\n\n我们在该数据集上运行了多模态最先进的 SLAM 方法，展示了其在农业环境应用中的现有局限性。数据集及其相关工具在此 https://链接 释放。', 'title_zh': 'Rosario 数据集 v2：农业机器人多模态数据集'}
{'arxiv_id': 'arXiv:2508.21475', 'title': 'MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents', 'authors': 'Xijia Tao, Yihua Teng, Xinxing Su, Xinyu Fu, Jihao Wu, Chaofan Tao, Ziru Liu, Haoli Bai, Rui Liu, Lingpeng Kong', 'link': 'https://arxiv.org/abs/2508.21475', 'abstract': 'Large multimodal language models (MLLMs) are increasingly deployed as web agents, yet many multimodal browsing benchmarks can be solved by shallow, fixed workflows that lean on high-recall image search and nearby text-masking the genuinely multimodal challenges of fine-grained visual reasoning, provenance verification, and long-horizon tool use. We introduce MMSearch-Plus, a benchmark of 311 tasks that highly demand multimodal understanding while preserving the difficulty profile of strong text-only browsing suites. Each item is constructed to contain multiple weak, localized visual signals that must be extracted, propagated through iterative text-image search, and cross-validated under retrieval noise before answering. Our curation procedure, Spatial-Temporal Extrapolation, seeds questions whose answers require extrapolating from spatial cues (micro-text, part-level appearance, layouts, signage) and temporal traces (broadcast overlays, seasonal context) to out-of-image facts such as events, dates, and venues. We provide a model-agnostic agent framework with browsing tools and evaluate a range of closed and open MLLMs. The strongest agent (o3) attains 15.1% without search and 36.0% accuracy with rollout under our framework, while a strong open-source model (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20 rounds of search. Beyond answer accuracy, we assess bounding-box production and cropped-image search, and conduct an error analysis that surfaces failures in source verification, part-based reasoning, and long-horizon planning.', 'abstract_zh': '大规模多模态语言模型（MLLMs）正越来越多地作为网络代理部署，然而许多多模态浏览基准可以通过依赖高召回率图像搜索和附近文本遮罩的方法来解决，这些方法忽视了真正需要精细视觉推理、来源验证和长期工具使用等多模态挑战。我们引入了MMSearch-Plus，这是一个包含311项任务的基准测试，这些任务高度要求多模态理解同时保持强大的文本-only 浏览套件的难度特征。每项任务都构建为包含多个弱局部视觉信号，这些信号必须被提取并通过迭代的文本-图像搜索传播，在检索噪声下进行交叉验证后才能回答。我们的编纂程序，时空外推，通过从空间线索（微观文本、部件级外观、布局、标识）和时间踪迹（广播覆盖、季节背景）推断出图像外的事实（事件、日期、场地）来提出问题。我们提供了一个模型无关的代理框架，配备了浏览工具，并评估了多种闭源和开源MMLMs。在我们的框架下，最强的代理（o3）在无搜索情况下达到15.1%的准确率，并在进行展开后达到36.0%的准确率，而在无搜索情况下，一个强大的开源模型（Qwen-2.5-VL-72B-Instruct）达到0.0%的准确率，并在20轮搜索后达到6.9%的准确率。除了答案准确性，我们还评估了边界框生成和裁剪图像搜索，并进行了一项错误分析，揭示了来源验证、部件级推理和长期规划等方面的失败。', 'title_zh': 'MMSearch-Plus: 一个简单而具有挑战性的多模态浏览代理基准'}
{'arxiv_id': 'arXiv:2508.21793', 'title': 'MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction', 'authors': 'Xiaoyang Wang, Christopher C. Yang', 'link': 'https://arxiv.org/abs/2508.21793', 'abstract': 'Healthcare systems generate diverse multimodal data, including Electronic Health Records (EHR), clinical notes, and medical images. Effectively leveraging this data for clinical prediction is challenging, particularly as real-world samples often present with varied or incomplete modalities. Existing approaches typically require complete modality data or rely on manual selection strategies, limiting their applicability in real-world clinical settings where data availability varies across patients and institutions. To address these limitations, we propose MoE-Health, a novel Mixture of Experts framework designed for robust multimodal fusion in healthcare prediction. MoE-Health architecture is specifically developed to handle samples with differing modalities and improve performance on critical clinical tasks. By leveraging specialized expert networks and a dynamic gating mechanism, our approach dynamically selects and combines relevant experts based on available data modalities, enabling flexible adaptation to varying data availability scenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical clinical prediction tasks: in-hospital mortality prediction, long length of stay, and hospital readmission prediction. Experimental results demonstrate that MoE-Health achieves superior performance compared to existing multimodal fusion methods while maintaining robustness across different modality availability patterns. The framework effectively integrates multimodal information, offering improved predictive performance and robustness in handling heterogeneous and incomplete healthcare data, making it particularly suitable for deployment in diverse healthcare environments with heterogeneous data availability.', 'abstract_zh': 'MoE-Health: 一种用于医疗预测的新型专家混合框架', 'title_zh': 'MoE-Health：一种混合专家框架以实现稳健的多模态医疗预测'}
{'arxiv_id': 'arXiv:2508.21460', 'title': 'Diffusion-based Multi-modal Synergy Interest Network for Click-through Rate Prediction', 'authors': 'Xiaoxi Cui, Weihai Lu, Yu Tong, Yiheng Li, Zhejun Zhao', 'link': 'https://arxiv.org/abs/2508.21460', 'abstract': "In click-through rate prediction, click-through rate prediction is used to model users' interests. However, most of the existing CTR prediction methods are mainly based on the ID modality. As a result, they are unable to comprehensively model users' multi-modal preferences. Therefore, it is necessary to introduce multi-modal CTR prediction. Although it seems appealing to directly apply the existing multi-modal fusion methods to click-through rate prediction models, these methods (1) fail to effectively disentangle commonalities and specificities across different modalities; (2) fail to consider the synergistic effects between modalities and model the complex interactions between modalities.\nTo address the above issues, this paper proposes the Diffusion-based Multi-modal Synergy Interest Network (Diff-MSIN) framework for click-through prediction. This framework introduces three innovative modules: the Multi-modal Feature Enhancement (MFE) Module Synergistic Relationship Capture (SRC) Module, and the Feature Dynamic Adaptive Fusion (FDAF) Module. The MFE Module and SRC Module extract synergistic, common, and special information among different modalities. They effectively enhances the representation of the modalities, improving the overall quality of the fusion. To encourage distinctiveness among different features, we design a Knowledge Decoupling method. Additionally, the FDAF Module focuses on capturing user preferences and reducing fusion noise. To validate the effectiveness of the Diff-MSIN framework, we conducted extensive experiments using the Rec-Tmall and three Amazon datasets. The results demonstrate that our approach yields a significant improvement of at least 1.67% compared to the baseline, highlighting its potential for enhancing multi-modal recommendation systems. Our code is available at the following link: this https URL.", 'abstract_zh': '基于扩散过程的多模态协同兴趣网络在点击率预测中的应用', 'title_zh': '基于扩散的多模态协同兴趣网络点击率预测'}
{'arxiv_id': 'arXiv:2508.21135', 'title': 'HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection', 'authors': 'Harris Song, Tuan-Anh Vu, Sanjith Menon, Sriram Narasimhan, M. Khalid Jawed', 'link': 'https://arxiv.org/abs/2508.21135', 'abstract': 'Detecting hidden or partially concealed objects remains a fundamental challenge in multimodal environments, where factors like occlusion, camouflage, and lighting variations significantly hinder performance. Traditional RGB-based detection methods often fail under such adverse conditions, motivating the need for more robust, modality-agnostic approaches. In this work, we present HiddenObject, a fusion framework that integrates RGB, thermal, and depth data using a Mamba-based fusion mechanism. Our method captures complementary signals across modalities, enabling enhanced detection of obscured or camouflaged targets. Specifically, the proposed approach identifies modality-specific features and fuses them in a unified representation that generalizes well across challenging scenarios. We validate HiddenObject across multiple benchmark datasets, demonstrating state-of-the-art or competitive performance compared to existing methods. These results highlight the efficacy of our fusion design and expose key limitations in current unimodal and naïve fusion strategies. More broadly, our findings suggest that Mamba-based fusion architectures can significantly advance the field of multimodal object detection, especially under visually degraded or complex conditions.', 'abstract_zh': '检测隐藏或部分遮蔽的物体仍然是多模态环境中的一项基本挑战，遮挡、迷彩和光照变化等因素显著阻碍了性能的提升。传统的基于RGB的检测方法在这些不利条件下往往失效，推动了对更为 robust、模态无关的方法的需求。在本文中，我们提出了一种融合框架HiddenObject，该框架使用Mamba机制融合RGB、热成像和深度数据。我们的方法在不同模态中捕捉互补信号，增强了对被遮挡或迷彩目标的检测能力。具体来说，所提出的方法识别出特定模态的特征，并在统一的表示中融合这些特征，以在复杂场景中泛化良好。我们在多个基准数据集中验证了HiddenObject，结果显示其性能与现有方法相当或优于现有方法。这些结果突显了我们融合设计的有效性，并揭示了当前单模态和简单的融合策略的关键局限性。更广泛地说，我们的发现表明，基于Mamba的融合架构能够显著推动多模态目标检测领域的进展，尤其是在视觉退化或复杂条件下。', 'title_zh': '隐含对象检测：模态无关的多模态隐含对象融合'}
