{'arxiv_id': 'arXiv:2511.05234', 'title': 'Context-aware Learned Mesh-based Simulation via Trajectory-Level Meta-Learning', 'authors': 'Philipp Dahlinger, Niklas Freymuth, Tai Hoang, Tobias Würth, Michael Volpp, Luise Kärger, Gerhard Neumann', 'link': 'https://arxiv.org/abs/2511.05234', 'abstract': 'Simulating object deformations is a critical challenge across many scientific domains, including robotics, manufacturing, and structural mechanics. Learned Graph Network Simulators (GNSs) offer a promising alternative to traditional mesh-based physics simulators. Their speed and inherent differentiability make them particularly well suited for applications that require fast and accurate simulations, such as robotic manipulation or manufacturing optimization. However, existing learned simulators typically rely on single-step observations, which limits their ability to exploit temporal context. Without this information, these models fail to infer, e.g., material properties. Further, they rely on auto-regressive rollouts, which quickly accumulate error for long trajectories. We instead frame mesh-based simulation as a trajectory-level meta-learning problem. Using Conditional Neural Processes, our method enables rapid adaptation to new simulation scenarios from limited initial data while capturing their latent simulation properties. We utilize movement primitives to directly predict fast, stable and accurate simulations from a single model call. The resulting approach, Movement-primitive Meta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of the runtime cost compared to state-of-the-art GNSs across several tasks.', 'abstract_zh': '基于网格的仿真对象变形模拟是许多科学领域的一项关键挑战，包括机器人学、制造和结构力学。学习图网络仿真器（GNSs）为传统基于网格的物理仿真器提供了一种有前途的替代方案。它们的速度和内在可微性使它们特别适合需要快速和准确仿真的应用，例如机器人操作或制造优化。然而，现有的学习仿真器通常依赖于单步观察，这限制了它们利用时间上下文的能力。没有这些信息，这些模型难以推断，例如材料属性。此外，它们依赖于自回归滚动预测，这会导致长时间轨迹迅速累积误差。相反，我们将基于网格的仿真框架为一个轨迹级元学习问题。利用条件神经过程，我们的方法可以在有限的初始数据下快速适应新的仿真场景，同时捕获其潜在的仿真特性。我们使用运动基元直接从单次模型调用预测快速、稳定和准确的仿真。由此产生的方法，运动基元元网格图网络（M3GN），在多个任务上与最先进的GNSs相比，提供了更高的仿真精度，且运行时间成本仅为其中的一小部分。', 'title_zh': '基于轨迹级元学习的上下文感知学习网格模拟'}
{'arxiv_id': 'arXiv:2511.05489', 'title': 'TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning', 'authors': 'Junwen Pan, Qizhe Zhang, Rui Zhang, Ming Lu, Xin Wan, Yuan Zhang, Chang Liu, Qi She', 'link': 'https://arxiv.org/abs/2511.05489', 'abstract': 'Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at this https URL.', 'abstract_zh': '基于时间的搜索旨在根据给定的查询从成千上万的帧中识别出一个相关的最小帧集，为准确理解长视频奠定基础。现有工作试图逐步缩小搜索空间。然而，这些方法通常依赖于手工设计的搜索过程，缺乏端到端优化以学习最优的搜索策略。本文提出了TimeSearch-R，将其时间搜索重新表述为交替的文本-视频思考过程，并通过强化学习（RL）无缝地将视频片段的搜索过程融入推理过程。然而，将如Group Relative Policy Optimization (GRPO)等RL训练方法应用于视频推理可能会导致未监督的中间搜索决策，这将导致对视频内容探索不足和逻辑推理不一致。为此，我们引入了带有完整性自我验证的GRPO (GRPO-CSV)，它是通过强化学习交替推理过程中收集搜索到的视频帧，并利用相同的策略模型验证所搜索帧的充分性，从而提高视频推理的完整性。此外，我们构建了专门用于SFT冷启动和GRPO-CSV的RL训练的数据集，过滤掉时间依赖性较弱的样本，以增强任务难度并改进时间搜索能力。大量实验表明，TimeSearch-R在Haystack-LVBench、Haystack-Ego4D等时间搜索基准测试和VideoMME、MLVU等长视频理解基准测试中取得了显著改进。值得注意的是，TimeSearch-R在LongVideoBench上建立了新的state-of-the-art，分别比基线模型Qwen2.5-VL提高了4.1%，比先进的视频推理模型Video-R1提高了2.0%。代码已发布在https://。', 'title_zh': 'TimeSearch-R：自验证强化学习驱动的长视频适应性 temporal 搜索理解'}
{'arxiv_id': 'arXiv:2511.05394', 'title': 'AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly', 'authors': 'Alexander Htet Kyaw, Haotian Ma, Sasa Zivkovic, Jenny Sabin', 'link': 'https://arxiv.org/abs/2511.05394', 'abstract': 'We present an AI-assisted Augmented Reality assembly workflow that uses deep learning-based object recognition to identify different assembly components and display step-by-step instructions. For each assembly step, the system displays a bounding box around the corresponding components in the physical space, and where the component should be placed. By connecting assembly instructions with the real-time location of relevant components, the system eliminates the need for manual searching, sorting, or labeling of different components before each assembly. To demonstrate the feasibility of using object recognition for AR-assisted assembly, we highlight a case study involving the assembly of LEGO sculptures.', 'abstract_zh': '基于深度学习的物体识别的AI辅助增强现实装配工作流：以LEGO雕塑装配案例研究为例', 'title_zh': 'AI辅助AR装配：物体识别与增强现实装配中的计算机视觉'}
{'arxiv_id': 'arXiv:2511.05308', 'title': 'Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation', 'authors': 'Matteo Bastico, David Ryckelynck, Laurent Corté, Yannick Tillier, Etienne Decencière', 'link': 'https://arxiv.org/abs/2511.05308', 'abstract': 'As 3D point clouds become a cornerstone of modern technology, the need for sophisticated generative models and reliable evaluation metrics has grown exponentially. In this work, we first expose that some commonly used metrics for evaluating generated point clouds, particularly those based on Chamfer Distance (CD), lack robustness against defects and fail to capture geometric fidelity and local shape consistency when used as quality indicators. We further show that introducing samples alignment prior to distance calculation and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet essential steps to ensure the consistency and robustness of point cloud generative model evaluation metrics. While existing metrics primarily focus on directly comparing 3D Euclidean coordinates, we present a novel metric, named Surface Normal Concordance (SNC), which approximates surface similarity by comparing estimated point normals. This new metric, when combined with traditional ones, provides a more comprehensive evaluation of the quality of generated samples. Finally, leveraging recent advancements in transformer-based models for point cloud analysis, such as serialized patch attention , we propose a new architecture for generating high-fidelity 3D structures, the Diffusion Point Transformer. We perform extensive experiments and comparisons on the ShapeNet dataset, showing that our model outperforms previous solutions, particularly in terms of quality of generated point clouds, achieving new state-of-the-art. Code available at this https URL.', 'abstract_zh': '随着3D点云成为现代技术的基石，对复杂的生成模型和可靠的评估指标的需求呈指数级增长。在本文中，我们首先揭示了某些常用用于评估生成点云的指标，特别是基于Chamfer距离（CD）的指标，在面对缺陷时缺乏鲁棒性，并且在作为质量指标时无法捕捉几何保真度和局部形状一致性。我们进一步表明，在进行距离计算之前引入样本对齐以及用密度感知Chamfer距离（DCD）取代CD是确保点云生成模型评估指标一致性和鲁棒性的简单但至关重要的步骤。虽然现有的指标主要侧重于直接比较3D欧几里得坐标，我们提出了一种新的指标，名为表面法线一致性（SNC），它通过比较估计的点法线来近似表面相似性。该新指标与传统指标结合使用，为生成样本的质量提供了更为全面的评估。最后，利用基于变换器的点云分析的最新进展，如序贯块注意力，我们提出了一种生成高保真3D结构的新架构，即扩散点变换器（Diffusion Point Transformer）。我们在Shapenet数据集上进行了广泛的实验和比较，结果显示我们的模型在生成点云的质量方面优于之前的解决方案，达到了新的最先进水平。代码可在如下链接获取：this https URL。', 'title_zh': '重思用于3D点云生成的度量标准和扩散架构'}
{'arxiv_id': 'arXiv:2511.05299', 'title': 'LiveStar: Live Streaming Assistant for Real-World Online Video Understanding', 'authors': 'Zhenyu Yang, Kairui Zhang, Yuhang Hu, Bing Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Weiming Dong, Changsheng Xu', 'link': 'https://arxiv.org/abs/2511.05299', 'abstract': "Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at this https URL.", 'abstract_zh': '尽管在离线视频理解方面取得了显著进展，现有的在线Video Large Language Models（Video-LLMs）通常难以同时处理连续的逐帧输入并确定最优响应时机，常常牺牲实时响应性和叙事连贯性。为了应对这些限制，我们引入了LiveStar，这是一种创新的直播助手，通过自适应流解码实现始终如一的主动响应。具体来说，LiveStar融合了：（1）一种训练策略，实现变长视频流的增量视频语言对齐，保持动态演变帧序列间的时序一致性；（2）一种响应静默解码框架，通过单次前向验证确定最优主动响应时机；（3）一种基于峰值尾部记忆压缩的内存感知加速技术，结合流式键值缓存，实现10分钟以上视频的在线推理加速1.53倍。我们还构建了一个全方位的OmniStar数据集，这是一个包含15种多样的真实场景和5项评估任务的全面数据集，用于在线视频理解的训练和基准测试。在三个基准上的广泛实验表明，LiveStar实现了最先进的性能，与现有的在线Video-LLMs相比，语义准确性平均提高19.5%，响应时间差减少18.1%，同时在所有五个OmniStar任务中的FPS提高12.0%。我们的模型和数据集可以在以下链接访问。', 'title_zh': 'LiveStar: 实时直播助手 for 实际应用场景中的在线视频理解'}
{'arxiv_id': 'arXiv:2511.05250', 'title': 'Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks', 'authors': 'Mohamed Sanim Akremi, Rim Slama, Hedi Tabia', 'link': 'https://arxiv.org/abs/2511.05250', 'abstract': 'Online continuous motion recognition is a hot topic of research since it is more practical in real life application cases. Recently, Skeleton-based approaches have become increasingly popular, demonstrating the power of using such 3D temporal data. However, most of these works have focused on segment-based recognition and are not suitable for the online scenarios. In this paper, we propose an online recognition system for skeleton sequence streaming composed from two main components: a detector and a classifier, which use a Semi-Positive Definite (SPD) matrix representation and a Siamese network. The powerful statistical representations for the skeletal data given by the SPD matrices and the learning of their semantic similarity by the Siamese network enable the detector to predict time intervals of the motions throughout an unsegmented sequence. In addition, they ensure the classifier capability to recognize the motion in each predicted interval. The proposed detector is flexible and able to identify the kinetic state continuously. We conduct extensive experiments on both hand gesture and body action recognition benchmarks to prove the accuracy of our online recognition system which in most cases outperforms state-of-the-art performances.', 'abstract_zh': '在线连续动作识别是研究的热点话题，因为它在实际应用场景中更具实用性。近年来，基于骨架的方法越来越受欢迎，展示了利用这种3D时序数据的强大力量。然而，大多数这些工作主要集中在基于段的识别上，并不适用于在线场景。本文提出了一种基于骨架序列流的数据在线识别系统，由两个主要组件组成：一个检测器和一个分类器，它们使用半正定矩阵（SPD矩阵）表示和Siamese网络。由半正定矩阵提供的强大统计表示和Siamese网络学习的语义相似性，使检测器能够预测未分段序列中的动作时间间隔。此外，这保证了分类器能够在每个预测间隔内识别动作。所提出的检测器灵活且能够连续识别运动状态。我们在手部手势和人体动作识别基准数据集上进行了广泛实验，以证明我们提出的在线识别系统的准确性，在大多数情况下超越了现有的最先进的性能。', 'title_zh': '基于检测器和Deep SPD Siamese网络的准确在线动作和手势识别系统'}
{'arxiv_id': 'arXiv:2511.05229', 'title': '4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos', 'authors': 'Mengqi Guo, Bo Xu, Yanyan Li, Gim Hee Lee', 'link': 'https://arxiv.org/abs/2511.05229', 'abstract': 'Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.', 'abstract_zh': '单目动态场景中具有未知摄像机姿态的新型视角合成仍然是计算机视觉和图形学中的一个基本挑战。我们提出了4D3R，这是一种无需摄像机姿态的动态神经渲染框架，通过两阶段方法解耦静态和动态组件。该方法首先利用3D基础模型进行初始姿态和几何估计，然后进行运动感知细化。4D3R引入了两项关键技术创新：(1) 运动感知束调整（MA-BA）模块，结合基于变压器的学习先验与SAM2，实现稳健的动态物体分割，从而提高摄像机姿态优化的准确性；(2) 高效的运动感知高斯点聚合（MA-GS）表示，利用带有变形场MLP和线性混合皮肤的控制点，来建模动态运动，显著降低计算成本同时保持高质量的重建。在真实世界的动态数据集上的大量实验表明，与现有方法相比，我们的方法在复杂场景下特别是大型动态物体中实现了高达1.8dB PSNR的性能提升，并且计算需求降低了5倍。', 'title_zh': '4D3R：基于单目视频的动态场景运动感知神经重建与渲染'}
{'arxiv_id': 'arXiv:2511.05055', 'title': 'No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation', 'authors': 'Mingyu Sung, Hyeonmin Choe, Il-Min Kim, Sangseok Yun, Jae Mo Kang', 'link': 'https://arxiv.org/abs/2511.05055', 'abstract': 'Monocular depth estimation (MDE), inferring pixel-level depths in single RGB images from a monocular camera, plays a crucial and pivotal role in a variety of AI applications demanding a three-dimensional (3D) topographical scene. In the real-world scenarios, MDE models often need to be deployed in environments with different conditions from those for training. Test-time (domain) adaptation (TTA) is one of the compelling and practical approaches to address the issue. Although there have been notable advancements in TTA for MDE, particularly in a self-supervised manner, existing methods are still ineffective and problematic when applied to diverse and dynamic environments. To break through this challenge, we propose a novel and high-performing TTA framework for MDE, named PITTA. Our approach incorporates two key innovative strategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware image masking. Specifically, PITTA enables highly effective TTA on a pretrained MDE network in a pose-agnostic manner without resorting to any camera pose information. Besides, our instance-aware masking strategy extracts instance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.) from a segmentation mask produced by a pretrained panoptic segmentation network, by removing static objects including background components. To further boost performance, we also present a simple yet effective edge extraction methodology for the input image (i.e., a single monocular image) and depth map. Extensive experimental evaluations on DrivingStereo and Waymo datasets with varying environmental conditions demonstrate that our proposed framework, PITTA, surpasses the existing state-of-the-art techniques with remarkable performance improvements in MDE during TTA.', 'abstract_zh': '单目深度估计的测试时域适应框架：PITTA', 'title_zh': '无需姿态估计？不成问题：单目深度估计的姿势无关且实例感知的测试时适应'}
{'arxiv_id': 'arXiv:2511.05034', 'title': 'Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation', 'authors': 'Jing Jin, Xu Liu, Te Gao, Zhihong Shi, Yixiong Liang, Ruiqing Zheng, Hulin Kuang, Min Zeng, Shichao Kan', 'link': 'https://arxiv.org/abs/2511.05034', 'abstract': 'Whole Slide Image (WSI) representation is critical for cancer subtyping, cancer recognition and mutation this http URL an end-to-end WSI representation model poses significant challenges, as a standard gigapixel slide can contain tens of thousands of image tiles, making it difficult to compute gradients of all tiles in a single mini-batch due to current GPU limitations. To address this challenge, we propose a method of dynamic residual encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI representation. Our approach utilizes a memory bank to store the features of tiles across all WSIs in the dataset. During training, a mini-batch usually contains multiple WSIs. For each WSI in the batch, a subset of tiles is randomly sampled and their features are computed using a tile encoder. Then, additional tile features from the same WSI are selected from the memory bank. The representation of each individual WSI is generated using a residual encoding technique that incorporates both the sampled features and those retrieved from the memory bank. Finally, the slide-level contrastive loss is computed based on the representations and histopathology reports ofthe WSIs within the mini-batch. Experiments conducted over cancer subtyping, cancer recognition, and mutation prediction tasks proved the effectiveness of the proposed DRE-SLCL method.', 'abstract_zh': 'Whole Slide Image (WSI) 表征对于癌症亚型划分、癌症识别和突变分析至关重要。由于标准的吉格拉像素切片中包含成千上万的图像块，当前GPU限制使得在一个小批量中计算所有块的梯度变得困难。为了解决这一挑战，我们提出了一种基于切片级对比学习的动态残差编码方法（DRE-SLCL），以实现端到端的WSI表征。我们的方法利用记忆库存储数据集中所有WSI图像块的特征。在训练过程中，一个批次通常包含多个WSI。对于批次中的每个WSI，随机采样一组图像块并使用图像块编码器计算它们的特征，然后从记忆库中选择相同WSI的额外图像块特征。每个WSI的表示使用残差编码技术生成，该技术结合了采样特征和从记忆库检索的特征。最后，基于批次中WSI的表示和组织病理报告计算切片级对比损失。在癌症亚型划分、癌症识别和突变预测任务上的实验证明了提出的方法DRE-SLCL的有效性。', 'title_zh': '滑块级对比学习引导的动态残差编码用于端到端全视野图像表示'}
{'arxiv_id': 'arXiv:2511.04970', 'title': 'Learning Fourier shapes to probe the geometric world of deep neural networks', 'authors': 'Jian Wang, Yixing Yong, Haixia Bi, Lijun He, Fan Li', 'link': 'https://arxiv.org/abs/2511.04970', 'abstract': "While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that optimized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model's salient regions; and third, that they constitute a new, generalizable adversarial paradigm capable of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance optimization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception.", 'abstract_zh': '尽管形状和纹理都是视觉识别的基础，深度神经网络（DNNs）的研究主要集中在纹理上，而对形状的几何理解探究不足。在这里，我们展示：首先，优化后的形状可以作为强大的语义载体，仅通过其几何定义即可生成高置信度的分类结果；其次，形状是高保真的可解释性工具，能够精确隔离模型的关键区域；第三，它们构成一种新的、可泛化的对抗范式，能够迷惑下游视觉任务。这是通过一个端到端可微框架实现的，该框架集成了强大的傅里叶级数来参数化任意形状、基于环绕数的映射将形状转换为DNNs所需的像素网格，并通过信号能量约束提高优化效率同时确保物理上合理的形状。我们的工作提供了一种灵活的框架来探究DNNs的几何世界，并为挑战和理解机器感知开辟了新的前沿。', 'title_zh': '学习傅里叶形状以探究深度神经网络的几何世界'}
{'arxiv_id': 'arXiv:2511.04811', 'title': 'An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention', 'authors': 'Shuo Zhao, Yu Zhou, Jianxu Chen', 'link': 'https://arxiv.org/abs/2511.04811', 'abstract': "Biomedical image segmentation is critical for precise structure delineation and downstream analysis. Traditional methods often struggle with noisy data, while deep learning models such as U-Net have set new benchmarks in segmentation performance. nnU-Net further automates model configuration, making it adaptable across datasets without extensive tuning. However, it requires a substantial amount of annotated data for cross-validation, posing a challenge when only raw images but no labels are available. Large foundation models offer zero-shot generalizability, but may underperform on specific datasets with unique characteristics, limiting their direct use for analysis. This work addresses these bottlenecks by proposing a data-centric AI workflow that leverages active learning and pseudo-labeling to combine the strengths of traditional neural networks and large foundation models while minimizing human intervention. The pipeline starts by generating pseudo-labels from a foundation model, which are then used for nnU-Net's self-configuration. Subsequently, a representative core-set is selected for minimal manual annotation, enabling effective fine-tuning of the nnU-Net model. This approach significantly reduces the need for manual annotations while maintaining competitive performance, providing an accessible solution for biomedical researchers to apply state-of-the-art AI techniques in their segmentation tasks. The code is available at this https URL.", 'abstract_zh': '生物医学图像分割对于精确的结构界定和后续分析至关重要。传统方法 often 常常难以处理噪声数据，而基于深度学习的模型如 U-Net 已在分割性能上设立了新的标杆。nnU-Net 进一步实现了模型配置的自动化，使其在无需大量调优的情况下适应不同数据集。然而，它需要大量的标注数据进行交叉验证，当仅有原始图像而无标签时，这构成了一个挑战。大模型提供零样本泛化能力，但在特定具有独特特性的数据集上可能表现不佳，限制了它们的直接应用。本文通过提出一种数据驱动的AI工作流来应对这些瓶颈，该工作流利用积极学习和伪标签来结合传统神经网络和大模型的优点，同时减少人工干预。该管道首先从基础模型生成伪标签，然后用于 nnU-Net 的自我配置。随后，选择一个具有代表性的核心集进行最少的手动注释，以实现 nnU-Net 模型的有效微调。这种方法显著减少了手动注释的需要，同时保持了竞争力的性能，为生物医学研究人员提供了一种易于应用的最新AI技术的方法。代码可在以下网址获取：this https URL。', 'title_zh': '基于最小人工干预的生物医学图像实例分割主动学习管道'}
{'arxiv_id': 'arXiv:2511.04803', 'title': 'Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose', 'authors': 'Shuo Zhao, Jianxu Chen', 'link': 'https://arxiv.org/abs/2511.04803', 'abstract': 'Generalist biomedical image segmentation models such as Cellpose are increasingly applied across diverse imaging modalities and cell types. However, two critical challenges remain underexplored: (1) the extent of training data redundancy and (2) the impact of cross domain transfer on model retention. In this study, we conduct a systematic empirical analysis of these challenges using Cellpose as a case study. First, to assess data redundancy, we propose a simple dataset quantization (DQ) strategy for constructing compact yet diverse training subsets. Experiments on the Cyto dataset show that image segmentation performance saturates with only 10% of the data, revealing substantial redundancy and potential for training with minimal annotations. Latent space analysis using MAE embeddings and t-SNE confirms that DQ selected patches capture greater feature diversity than random sampling. Second, to examine catastrophic forgetting, we perform cross domain finetuning experiments and observe significant degradation in source domain performance, particularly when adapting from generalist to specialist domains. We demonstrate that selective DQ based replay reintroducing just 5-10% of the source data effectively restores source performance, while full replay can hinder target adaptation. Additionally, we find that training domain sequencing improves generalization and reduces forgetting in multi stage transfer. Our findings highlight the importance of data centric design in biomedical image segmentation and suggest that efficient training requires not only compact subsets but also retention aware learning strategies and informed domain ordering. The code is available at this https URL.', 'abstract_zh': '通用型生物医学图像分割模型如Cellpose在多种成像模态和细胞类型中的应用越来越广泛。然而，两种关键挑战仍待深入研究：(1) 训练数据的冗余程度；(2) 跨域迁移对模型保持性的影响。在本研究中，我们以Cellpose为案例，系统地分析了这些挑战。首先，为评估数据冗余，我们提出了一种简单的数据集量化（DQ）策略，以构建紧凑且多样的训练子集。在Cyto数据集上的实验表明，仅使用数据的10%即可达到性能饱和，揭示了显著的冗余和潜在的少量标注训练机会。MAE嵌入和t-SNE的潜在空间分析也证实，DQ选择的补丁能捕获更多的特征多样性，超过随机采样。其次，为考察灾难性遗忘，我们进行了跨域微调实验，并观察到源域性能显著下降，特别是在从通用型向专业型领域适应时。我们展示了基于选择性DQ的重播放，仅重新引入5-10%的源数据即可有效恢复源性能，而完整重播放可能会阻碍目标适应。另外，我们发现，训练领域顺序可以提高多阶段转移中的泛化能力和减少遗忘。我们的研究结果强调了在生物医学图像分割中以数据为中心的设计的重要性，并建议高效的训练不仅需要紧凑的子集，还需要具备保持性的学习策略和有见地的领域排序。相关代码可在以下链接获取。', 'title_zh': '生物医学图像分割中的数据效率与传输稳健性：基于Cellpose的冗余与遗忘研究'}
{'arxiv_id': 'arXiv:2511.04753', 'title': 'CPO: Condition Preference Optimization for Controllable Image Generation', 'authors': 'Zonglin Lyu, Ming Li, Xinxin Liu, Chen Chen', 'link': 'https://arxiv.org/abs/2511.04753', 'abstract': 'To enhance controllability in text-to-image generation, ControlNet introduces image-based control signals, while ControlNet++ improves pixel-level cycle consistency between generated images and the input control signal. To avoid the prohibitive cost of back-propagating through the sampling process, ControlNet++ optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step approximation, which not only ignores the contribution of high-noise timesteps but also introduces additional approximation errors. A straightforward alternative for optimizing controllability across all timesteps is Direct Preference Optimization (DPO), a fine-tuning method that increases model preference for more controllable images ($I^{w}$) over less controllable ones ($I^{l}$). However, due to uncertainty in generative models, it is difficult to ensure that win--lose image pairs differ only in controllability while keeping other factors, such as image quality, fixed. To address this, we propose performing preference learning over control conditions rather than generated images. Specifically, we construct winning and losing control signals, $\\mathbf{c}^{w}$ and $\\mathbf{c}^{l}$, and train the model to prefer $\\mathbf{c}^{w}$. This method, which we term \\textit{Condition Preference Optimization} (CPO), eliminates confounding factors and yields a low-variance training objective. Our approach theoretically exhibits lower contrastive loss variance than DPO and empirically achieves superior results. Moreover, CPO requires less computation and storage for dataset curation. Extensive experiments show that CPO significantly improves controllability over the state-of-the-art ControlNet++ across multiple control types: over $10\\%$ error rate reduction in segmentation, $70$--$80\\%$ in human pose, and consistent $2$--$5\\%$ reductions in edge and depth maps.', 'abstract_zh': '基于图像的控制信号增强文本到图像生成的可控性：ControlNet的引入与ControlNet++的改进', 'title_zh': '条件偏好优化以实现可控图像生成'}
