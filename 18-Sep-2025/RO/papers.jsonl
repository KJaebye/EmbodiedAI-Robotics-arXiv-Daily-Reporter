{'arxiv_id': 'arXiv:2509.14228', 'title': 'Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models', 'authors': 'Benjamin Shaffer, Victoria Edwards, Brooks Kinch, Nathaniel Trask, M. Ani Hsieh', 'link': 'https://arxiv.org/abs/2509.14228', 'abstract': "Source localization in a complex flow poses a significant challenge for multi-robot teams tasked with localizing the source of chemical leaks or tracking the dispersion of an oil spill. The flow dynamics can be time-varying and chaotic, resulting in sporadic and intermittent sensor readings, and complex environmental geometries further complicate a team's ability to model and predict the dispersion. To accurately account for the physical processes that drive the dispersion dynamics, robots must have access to computationally intensive numerical models, which can be difficult when onboard computation is limited. We present a distributed mobile sensing framework for source localization in which each robot carries a machine-learned, finite element model of its environment to guide information-based sampling. The models are used to evaluate an approximate mutual information criterion to drive an infotaxis control strategy, which selects sensing regions that are expected to maximize informativeness for the source localization objective. Our approach achieves faster error reduction compared to baseline sensing strategies and results in more accurate source localization compared to baseline machine learning approaches.", 'abstract_zh': '复杂流场中源定位为多机器人团队在化学泄漏源定位或溢油扩散追踪任务中带来了显著挑战。流场动力学可能具有时变性和混沌性，导致传感器读数出现间歇性和不连续性，复杂环境几何形状进一步增加了团队建模和预测扩散的难度。为了准确反映驱动扩散动力学的物理过程，机器人需要访问计算密集型的数值模型，但在计算资源有限的情况下这可能很困难。我们提出了一种分布式移动传感框架，其中每个机器人携带一个环境的机器学习有限元模型以指导基于信息的采样。这些模型用于评估近似互信息准则，以驱动信息素战略，选择预期能最大化源定位信息量的传感区域。与基线传感策略相比，我们的方法实现了更快的误差减少，与基线机器学习方法相比，源定位结果更为准确。', 'title_zh': '复杂流场中基于物理保真环境模型的多机器人多源定位'}
{'arxiv_id': 'arXiv:2509.14210', 'title': 'GLIDE: A Coordinated Aerial-Ground Framework for Search and Rescue in Unknown Environments', 'authors': 'Seth Farrell, Chenghao Li, Hongzhan Yu, Hesam Mojtahedi, Sicun Gao, Henrik I. Christensen', 'link': 'https://arxiv.org/abs/2509.14210', 'abstract': "We present a cooperative aerial-ground search-and-rescue (SAR) framework that pairs two unmanned aerial vehicles (UAVs) with an unmanned ground vehicle (UGV) to achieve rapid victim localization and obstacle-aware navigation in unknown environments. We dub this framework Guided Long-horizon Integrated Drone Escort (GLIDE), highlighting the UGV's reliance on UAV guidance for long-horizon planning. In our framework, a goal-searching UAV executes real-time onboard victim detection and georeferencing to nominate goals for the ground platform, while a terrain-scouting UAV flies ahead of the UGV's planned route to provide mid-level traversability updates. The UGV fuses aerial cues with local sensing to perform time-efficient A* planning and continuous replanning as information arrives. Additionally, we present a hardware demonstration (using a GEM e6 golf cart as the UGV and two X500 UAVs) to evaluate end-to-end SAR mission performance and include simulation ablations to assess the planning stack in isolation from detection. Empirical results demonstrate that explicit role separation across UAVs, coupled with terrain scouting and guided planning, improves reach time and navigation safety in time-critical SAR missions.", 'abstract_zh': '我们提出了一种协作的空地搜救框架，该框架将两架无人机与一辆无人地面车辆配对，以实现快速受害者定位和未知环境中的障碍物感知导航。我们称此框架为引导远见集成无人机护航（GLIDE），突出了无人地面车辆对无人机长时间规划的依赖。在该框架中，一架目标搜索无人机执行实时机载受害者检测和地理定位，为地面平台提名目标；一架地形侦察无人机则在无人地面车辆规划路线上方飞行，提供中等水平的通行性更新。无人地面车辆融合空中线索与局部感知，进行高效的时间敏感型A*规划，并在信息到达时进行持续规划。此外，我们使用GEM e6高尔夫车作为无人地面车辆和两架X500无人机展示了硬件演示，以评估端到端的搜救任务性能，并通过仿真消融实验评估规划栈在与检测隔离情况下的表现。实验结果表明，无人机之间的明确角色分离、结合地形侦察及引导规划，在时间敏感的搜救任务中能够提高到达时间和导航安全性。', 'title_zh': 'GLIDE：未知环境中协同空地搜索与救援框架'}
{'arxiv_id': 'arXiv:2509.14191', 'title': 'MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping', 'authors': 'Zhihao Cao, Hanyu Wu, Li Wa Tang, Zizhou Luo, Zihan Zhu, Wei Zhang, Marc Pollefeys, Martin R. Oswald', 'link': 'https://arxiv.org/abs/2509.14191', 'abstract': 'Recent progress in dense SLAM has primarily targeted monocular setups, often at the expense of robustness and geometric coverage. We present MCGS-SLAM, the first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting (3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM fuses dense RGB inputs from multiple viewpoints into a unified, continuously optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines poses and depths via dense photometric and geometric residuals, while a scale consistency module enforces metric alignment across views using low-rank priors. The system supports RGB input and maintains real-time performance at large scale. Experiments on synthetic and real-world datasets show that MCGS-SLAM consistently yields accurate trajectories and photorealistic reconstructions, usually outperforming monocular baselines. Notably, the wide field of view from multi-camera input enables reconstruction of side-view regions that monocular setups miss, critical for safe autonomous operation. These results highlight the promise of multi-camera Gaussian Splatting SLAM for high-fidelity mapping in robotics and autonomous driving.', 'abstract_zh': 'Recent进展中的密集SLAM主要集中在单目设置上，通常会牺牲鲁棒性和几何覆盖率。我们提出了MCGS-SLAM，这是第一个基于RGB的多相机SLAM系统，构建于3D高斯点绘（3DGS）之上。不同于依赖稀疏地图或惯性数据的先前方法，MCGS-SLAM 将多视角的密集RGB输入融合为一个统一的、连续优化的高斯地图。多相机束调整（MCBA）通过密集的光度和几何残差联合优化姿态和深度，同时，尺度一致性模块使用低秩先验在不同视图之间强制实现度量对齐。该系统支持RGB输入，并在大规模应用中保持实时性能。实验结果表明，MCGS-SLAM 通常能够提供准确的轨迹和逼真的重建，通常优于单目基准。值得注意的是，多相机输入的宽视场能够重建单目设置遗漏的侧视区域，这对于自主操作的安全至关重要。这些结果突显了多相机高斯点绘SLAM在机器人和自动驾驶领域进行高保真地图构建的潜力。', 'title_zh': 'MCGS-SLAM：基于高斯点云的多相机SLAM框架，实现高保真映射'}
{'arxiv_id': 'arXiv:2509.14178', 'title': '\\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video', 'authors': 'Kai Ye, Yuhang Wu, Shuyuan Hu, Junliang Li, Meng Liu, Yongquan Chen, Rui Huang', 'link': 'https://arxiv.org/abs/2509.14178', 'abstract': 'Dexterous manipulation remains a challenging robotics problem, largely due to the difficulty of collecting extensive human demonstrations for learning. In this paper, we introduce \\textsc{Gen2Real}, which replaces costly human demos with one generated video and drives robot skill from it: it combines demonstration generation that leverages video generation with pose and depth estimation to yield hand-object trajectories, trajectory optimization that uses Physics-aware Interaction Optimization Model (PIOM) to impose physics consistency, and demonstration learning that retargets human motions to a robot hand and stabilizes control with an anchor-based residual Proximal Policy Optimization (PPO) policy. Using only generated videos, the learned policy achieves a 77.3\\% success rate on grasping tasks in simulation and demonstrates coherent executions on a real robot. We also conduct ablation studies to validate the contribution of each component and demonstrate the ability to directly specify tasks using natural language, highlighting the flexibility and robustness of \\textsc{Gen2Real} in generalizing grasping skills from imagined videos to real-world execution.', 'abstract_zh': '灵巧操作仍然是一个具有挑战性的机器人问题，主要是因为难以收集大量的手工演示用于学习。在本文中，我们介绍了\\textsc{Gen2Real}，它用一个生成的视频取代了昂贵的手工演示，并从中驱动机器人的技能：它结合了利用视频生成、姿态和深度估计生成示范，以产生手-物体轨迹；使用物理感知交互优化模型（PIOM）进行路径优化，以确保物理一致性；以及通过基于锚点的残差Proximal Policy Optimization（PPO）策略将人类动作重新定向到机器人手中并稳定控制以实现示范学习。仅使用生成的视频，所学习的策略在仿真中的夹取任务中达到了77.3%的成功率，并在实际机器人上展示了连贯的执行。我们还进行了消融研究以验证每个组件的贡献，并展示了可以直接使用自然语言指定任务的能力，突显了\\textsc{Gen2Real}在将想象中的视频技能泛化到实际执行中的灵活性和鲁棒性。', 'title_zh': 'Gen2Real: 通过利用生成视频迈向无需演示的灵巧 manipulation'}
{'arxiv_id': 'arXiv:2509.14159', 'title': 'MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies', 'authors': 'Dayi Dong, Maulik Bhatt, Seoyeon Choi, Negar Mehr', 'link': 'https://arxiv.org/abs/2509.14159', 'abstract': 'As robots become more integrated in society, their ability to coordinate with other robots and humans on multi-modal tasks (those with multiple valid solutions) is crucial. We propose to learn such behaviors from expert demonstrations via imitation learning (IL). However, when expert demonstrations are multi-modal, standard IL approaches can struggle to capture the diverse strategies, hindering effective coordination. Diffusion models are known to be effective at handling complex multi-modal trajectory distributions in single-agent systems. Diffusion models have also excelled in multi-agent scenarios where multi-modality is more common and crucial to learning coordinated behaviors. Typically, diffusion-based approaches require a centralized planner or explicit communication among agents, but this assumption can fail in real-world scenarios where robots must operate independently or with agents like humans that they cannot directly communicate with. Therefore, we propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE) paradigm for multi-modal multi-agent imitation learning using diffusion policies. Agents are trained jointly with full information, but execute policies using only local information to achieve implicit coordination. We demonstrate in both simulation and hardware experiments that our method recovers multi-modal coordination behavior among agents in a variety of tasks and environments, while improving upon state-of-the-art baselines.', 'abstract_zh': '随着机器人在社会中的集成程度提高，它们在多模态任务（具有多个有效解决方案的任务）中与其他机器人和人类协调的能力变得至关重要。我们提出通过模仿学习（IL）从专家示范中学习此类行为。然而，当专家示范是多模态时，标准的IL方法可能难以捕捉到多样化的策略，阻碍了有效的协调。扩散模型在处理单智能体系统的复杂多模态轨迹分布方面表现出色。扩散模型在多智能体场景中也表现出色，这些场景中的多模态现象更为普遍且对学习协调行为至关重要。通常，基于扩散的方法需要中心化规划或智能体间的显式通信，但在机器人必须独立操作或与无法直接沟通的智能体（如人类）交互的实际场景中，这一假设可能失效。因此，我们提出了一种集中式训练、去中心化执行（CTDE）范式，用于多模态多智能体模仿学习，采用扩散策略。智能体在训练期间共享完整信息，但在执行策略时仅使用局部信息以实现隐式的协调。我们在仿真和硬件实验中展示了该方法在多种任务和环境中恢复智能体的多模态协调行为，并优于最先进的基线方法。', 'title_zh': 'MIMIC-D：多模态模仿在去中心化扩散策略下的多智能体协调'}
{'arxiv_id': 'arXiv:2509.14147', 'title': 'StableTracker: Learning to Stably Track Target via Differentiable Simulation', 'authors': 'Fanxing Li, Shengyang Wang, Fangyu Sun, Shuyu Wu, Dexin Zuo, Wenxian Yu, Danping Zou', 'link': 'https://arxiv.org/abs/2509.14147', 'abstract': 'FPV object tracking methods heavily rely on handcraft modular designs, resulting in hardware overload and cumulative error, which seriously degrades the tracking performance, especially for rapidly accelerating or decelerating targets. To address these challenges, we present \\textbf{StableTracker}, a learning-based control policy that enables quadrotors to robustly follow the moving target from arbitrary perspectives. The policy is trained using backpropagation-through-time via differentiable simulation, allowing the quadrotor to maintain the target at the center of the visual field in both horizontal and vertical directions, while keeping a fixed relative distance, thereby functioning as an autonomous aerial camera. We compare StableTracker against both state-of-the-art traditional algorithms and learning baselines. Simulation experiments demonstrate that our policy achieves superior accuracy, stability and generalization across varying safe distances, trajectories, and target velocities. Furthermore, a real-world experiment on a quadrotor with an onboard computer validated practicality of the proposed approach.', 'abstract_zh': '基于学习的控制策略StableTracker使四旋翼能够在任意视角下稳健地跟随移动目标，解决了手动模块设计导致的硬件过载和累积误差问题，特别是对于快速加速或减速的目标，显著提升了跟踪性能。', 'title_zh': 'StableTracker：通过可微模拟学习稳定跟踪目标'}
{'arxiv_id': 'arXiv:2509.14143', 'title': 'CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping', 'authors': 'Zijian An, Ran Yang, Yiming Feng, Lifeng Zhou', 'link': 'https://arxiv.org/abs/2509.14143', 'abstract': 'Vision-language-action (VLA) models have recently emerged as a promising paradigm for robotic control, enabling end-to-end policies that ground natural language instructions into visuomotor actions. However, current VLAs often struggle to satisfy precise task constraints, such as stopping based on numeric thresholds, since their observation-to-action mappings are implicitly shaped by training data and lack explicit mechanisms for condition monitoring. In this work, we propose CLAW (CLIP-Language-Action for Weight), a framework that decouples condition evaluation from action generation. CLAW leverages a fine-tuned CLIP model as a lightweight prompt generator, which continuously monitors the digital readout of a scale and produces discrete directives based on task-specific weight thresholds. These prompts are then consumed by $\\pi_0$, a flow-based VLA policy, which integrates the prompts with multi-view camera observations to produce continuous robot actions. This design enables CLAW to combine symbolic weight reasoning with high-frequency visuomotor control. We validate CLAW on three experimental setups: single-object grasping and mixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW reliably executes weight-aware behaviors and outperforms both raw-$\\pi_0$ and fine-tuned $\\pi_0$ models. We have uploaded the videos as supplementary materials.', 'abstract_zh': '基于CLIP-Language-Action的条件监控框架', 'title_zh': 'CLAW：一种考虑重量的视觉-语言-动作框架用于机器人抓取'}
{'arxiv_id': 'arXiv:2509.14138', 'title': 'SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model', 'authors': 'Ran Yang, Zijian An, Lifeng ZHou, Yiming Feng', 'link': 'https://arxiv.org/abs/2509.14138', 'abstract': 'Long-horizon robotic manipulation tasks require executing multiple interdependent subtasks in strict sequence, where errors in detecting subtask completion can cascade into downstream failures. Existing Vision-Language-Action (VLA) models such as $\\pi_0$ excel at continuous low-level control but lack an internal signal for identifying when a subtask has finished, making them brittle in sequential settings. We propose SeqVLA, a completion-aware extension of $\\pi_0$ that augments the base architecture with a lightweight detection head perceiving whether the current subtask is complete. This dual-head design enables SeqVLA not only to generate manipulation actions but also to autonomously trigger transitions between subtasks. We investigate four finetuning strategies that vary in how the action and detection heads are optimized (joint vs. sequential finetuning) and how pretrained knowledge is preserved (full finetuning vs. frozen backbone). Experiments are performed on two multi-stage tasks: salad packing with seven distinct subtasks and candy packing with four distinct subtasks. Results show that SeqVLA significantly outperforms the baseline $\\pi_0$ and other strong baselines in overall success rate. In particular, joint finetuning with an unfrozen backbone yields the most decisive and statistically reliable completion predictions, eliminating sequence-related failures and enabling robust long-horizon execution. Our results highlight the importance of coupling action generation with subtask-aware detection for scalable sequential manipulation.', 'abstract_zh': '长时机器人操作任务需要严格顺序执行多个相互依赖的子任务，其中子任务完成检测错误会连锁导致下游任务失败。现有的视觉-语言-行动（VLA）模型如$\\pi_0$在连续的低级控制方面表现出色，但缺乏识别子任务完成的内部信号，使其在序列设置中较为脆弱。我们提出了SeqVLA，这是一种针对子任务完成情况进行感知的$\\pi_0$的扩展，通过增加一个小而轻的检测头来判断当前子任务是否已完成。这种双头设计使SeqVLA不仅能生成操作动作，还能自主触发子任务之间的转换。我们探讨了四种不同的微调策略，这些策略在操作头和检测头的优化方式（联合微调 vs. 顺序微调）以及预训练知识的保留方式（完全微调 vs. 固定骨干网）上有所不同。在两个多阶段任务（七种不同子任务的沙拉包装和四种不同子任务的糖果包装）上进行了实验。结果显示，SeqVLA在总体成功率上显著优于基线$\\pi_0$和其他强基线模型。特别是联合微调且不冻结骨干网的方法提供了最可靠且统计上显著的完成预测，消除了序列相关失败，使执行更加稳健。我们的结果强调了将动作生成与子任务感知检测相结合的重要性，以实现可扩展的序列化操作。', 'title_zh': 'SeqVLA：面向完成意识视觉-语言-动作模型的长期 horizon 操作序列任务执行'}
{'arxiv_id': 'arXiv:2509.14127', 'title': 'Energy Efficient Multi Robot Package Delivery under Capacity-Constraints via Voronoi-Constrained Networks', 'authors': 'Alkesh K. Srivastava, Jared Michael Levin, Philip Dames', 'link': 'https://arxiv.org/abs/2509.14127', 'abstract': 'We consider the problem of delivering multiple packages from a single pickup depot to distinct goal locations using a homogeneous fleet of robots with limited carrying capacity. We propose VCST-RCP, a Voronoi-Constrained Steiner Tree Relay Coordination Planning framework that constructs sparse relay trunks using Steiner tree optimization and then synthesizes robot-level pickup, relay, and delivery schedules. This framework reframes relays from incidental byproducts into central elements of coordination, offering a contrast with traditional delivery methods that rely on direct source-to-destination transport. Extensive experiments show consistent improvements of up to 34% compared to conventional baselines, underscoring the benefits of incorporating relays into the delivery process. These improvements translate directly to enhanced energy efficiency in multi-robot delivery under capacity constraints, providing a scalable framework for real-world logistics.', 'abstract_zh': '我们考虑使用具有有限承载能力的 homogenous 车队机器人，从单一取货点向不同的目标地点递送多个包裹的问题。我们提出了一种 Voronoi-Constrained Steiner Tree Relay Coordination Planning (VCST-RCP) 框架，该框架通过 Steiner 树优化构建稀疏接力骨干网络，然后合成机器人级别的取货、接力和配送时间表。该框架将接力重新定义为中心协调要素，与依赖于直接源地到目的地运输的传统递送方法形成对比。广泛的经验研究表明，与传统基准相比，改进幅度高达 34%，突显了将接力整合进递送过程中的益处。这些改进直接转化为在承载能力受限条件下多机器人递送的能源效率提升，提供了一种适用于实际物流的可扩展框架。', 'title_zh': '基于Voronoi约束网络的容量限制多机器人包裹配送能效优化'}
{'arxiv_id': 'arXiv:2509.14126', 'title': 'CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads', 'authors': 'Viktor Lorentz, Khaled Wahba, Sayantan Auddy, Marc Toussaint, Wolfgang Hönig', 'link': 'https://arxiv.org/abs/2509.14126', 'abstract': 'Collaborative transportation of cable-suspended payloads by teams of Unmanned Aerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to different payload shapes, and provide built-in compliance, making it attractive for applications ranging from disaster relief to precision logistics. However, multi-UAV coordination under disturbances, nonlinear payload dynamics, and slack--taut cable modes remains a challenging control problem. To our knowledge, no prior work has addressed these cable mode transitions in the multi-UAV context, instead relying on simplifying rigid-link assumptions. We propose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for multi-UAV cable-suspended payload transport. Simulation results demonstrate that the learned policies can outperform classical decentralized controllers in terms of disturbance rejection and tracking precision, achieving an 80% recovery rate from harsh conditions compared to 44% for the baseline method. We also achieve successful zero-shot sim-to-real transfer and demonstrate that our policies are highly robust under harsh conditions, including wind, random external disturbances, and transitions between slack and taut cable dynamics. This work paves the way for autonomous, resilient UAV teams capable of executing complex payload missions in unstructured environments.', 'abstract_zh': '多旋翼无人机团队协作吊运电缆悬挂载荷具有增强载荷容量、适应不同载荷形状以及提供内置顺应性等潜力，使其在从灾害救援到精密物流等多个应用领域具有吸引力。然而，在扰动下多旋翼无人机协调、非线性载荷动力学以及松弛和紧绷电缆模式之间切换仍是一个具有挑战性的控制问题。据我们所知，目前尚没有研究在多旋翼无人机上下考虑这些电缆模式的切换，而是依赖于刚性连接的简化假设。我们提出了一种去中心化的强化学习（RL）框架CrazyMARL，用于多旋翼无人机电缆悬挂载荷运输。模拟结果表明，所学习的策略在干扰抑制和跟踪精度方面可优于经典去中心化控制器，相较于基线方法，达到80%的恶劣条件恢复率，而基线方法为44%。我们还实现了成功的无监督模拟到现实世界的转移，并展示了我们的策略在恶劣条件（包括风、随机外部干扰以及松弛和紧绷电缆动力学之间的切换）下具有高度的鲁棒性。这一工作为能够在非结构化环境中执行复杂载荷任务的自主、鲁棒无人机团队铺平了道路。', 'title_zh': '疯狂MARL：协作悬挂吊载空中运输的去中心化直接电机控制策略'}
{'arxiv_id': 'arXiv:2509.14117', 'title': 'GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model', 'authors': 'Ali Abouzeid, Malak Mansour, Zezhou Sun, Dezhen Song', 'link': 'https://arxiv.org/abs/2509.14117', 'abstract': 'Vision-Language-Action (VLA) models often fail to generalize to novel camera viewpoints, a limitation stemming from their difficulty in inferring robust 3D geometry from 2D images. We introduce GeoAware-VLA, a simple yet effective approach that enhances viewpoint invariance by integrating strong geometric priors into the vision backbone. Instead of training a visual encoder or relying on explicit 3D data, we leverage a frozen, pretrained geometric vision model as a feature extractor. A trainable projection layer then adapts these geometrically-rich features for the policy decoder, relieving it of the burden of learning 3D consistency from scratch. Through extensive evaluations on LIBERO benchmark subsets, we show GeoAware-VLA achieves substantial improvements in zero-shot generalization to novel camera poses, boosting success rates by over 2x in simulation. Crucially, these benefits translate to the physical world; our model shows a significant performance gain on a real robot, especially when evaluated from unseen camera angles. Our approach proves effective across both continuous and discrete action spaces, highlighting that robust geometric grounding is a key component for creating more generalizable robotic agents.', 'abstract_zh': 'GeoAware-VLA:通过集成强几何先验增强视点不变性以提升通用能力', 'title_zh': 'GeoAware-VLA: 隐式几何意识的视觉-语言-动作模型'}
{'arxiv_id': 'arXiv:2509.14082', 'title': 'FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video', 'authors': 'Valerii Serpiva, Artem Lykov, Faryal Batool, Vladislav Kozlovskiy, Miguel Altamirano Cabrera, Dzmitry Tsetserukou', 'link': 'https://arxiv.org/abs/2509.14082', 'abstract': 'We present FlightDiffusion, a diffusion-model-based framework for training autonomous drones from first-person view (FPV) video. Our model generates realistic video sequences from a single frame, enriched with corresponding action spaces to enable reasoning-driven navigation in dynamic environments. Beyond direct policy learning, FlightDiffusion leverages its generative capabilities to synthesize diverse FPV trajectories and state-action pairs, facilitating the creation of large-scale training datasets without the high cost of real-world data collection. Our evaluation demonstrates that the generated trajectories are physically plausible and executable, with a mean position error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad (RMSE 0.24 rad). This approach enables improved policy learning and dataset scalability, leading to superior performance in downstream navigation tasks. Results in simulated environments highlight enhanced robustness, smoother trajectory planning, and adaptability to unseen conditions. An ANOVA revealed no statistically significant difference between performance in simulation and reality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD = 0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real transfer. The generated datasets provide a valuable resource for future UAV research. This work introduces diffusion-based reasoning as a promising paradigm for unifying navigation, action generation, and data synthesis in aerial robotics.', 'abstract_zh': '基于扩散模型的从第一人称视角训练自主无人机的FlightDiffusion框架', 'title_zh': 'FlightDiffusion：通过扩散模型生成FPV视频革新中国自主无人机训练'}
{'arxiv_id': 'arXiv:2509.14075', 'title': 'Constraint-Consistent Control of Task-Based and Kinematic RCM Constraints for Surgical Robots', 'authors': 'Yu Li, Hamid Sadeghian, Zewen Yang, Valentin Le Mesle, Sami Haddadin', 'link': 'https://arxiv.org/abs/2509.14075', 'abstract': 'Robotic-assisted minimally invasive surgery (RAMIS) requires precise enforcement of the remote center of motion (RCM) constraint to ensure safe tool manipulation through a trocar. Achieving this constraint under dynamic and interactive conditions remains challenging, as existing control methods either lack robustness at the torque level or do not guarantee consistent RCM constraint satisfaction. This paper proposes a constraint-consistent torque controller that treats the RCM as a rheonomic holonomic constraint and embeds it into a projection-based inverse-dynamics framework. The method unifies task-level and kinematic formulations, enabling accurate tool-tip tracking while maintaining smooth and efficient torque behavior. The controller is validated both in simulation and on a RAMIS training platform, and is benchmarked against state-of-the-art approaches. Results show improved RCM constraint satisfaction, reduced required torque, and robust performance by improving joint torque smoothness through the consistency formulation under clinically relevant scenarios, including spiral trajectories, variable insertion depths, moving trocars, and human interaction. These findings demonstrate the potential of constraint-consistent torque control to enhance safety and reliability in surgical robotics. The project page is available at: this https URL', 'abstract_zh': '机器人辅助微创手术（RAMIS）需要精确执行远程中心运动（RCM）约束，以确保通过Trocar安全操作手术工具。在动态和交互条件下实现这一约束仍然具有挑战性，因为现有控制方法要么在扭力水平上缺乏鲁棒性，要么不能保证一致的RCM约束满足。本文提出了一种约束一致的扭力控制器，将RCM视为一种动力学 holonomic 约束，并将其嵌入到投影为基础的逆动力学框架中。该方法统一了任务级和运动学公式，实现了精确的工具尖端跟踪，同时保持平滑和高效的扭力行为。该控制器在仿真和RAMIS培训平台上进行了验证，并与最先进的方法进行了基准测试。结果显示，在包括螺旋轨迹、变插入深度、移动Trocar和与人类交互的临床相关场景下，通过一致性公式提高了关节扭力平滑性，从而提高了RCM约束满足度、减少了所需扭力并增强了鲁棒性能。这些发现展示了约束一致的扭力控制在增强手术机器人安全性和可靠性方面的潜力。项目页面可用于此：this https URL', 'title_zh': '基于任务和运动RCM约束的一致性控制约束管理'}
{'arxiv_id': 'arXiv:2509.14063', 'title': 'Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace', 'authors': 'Sundhar Vinodh Sangeetha, Chih-Yuan Chiu, Sarah H.Q. Li, Shreyas Kousik', 'link': 'https://arxiv.org/abs/2509.14063', 'abstract': 'Autonomous aircraft must safely operate in untowered airspace, where coordination relies on voice-based communication among human pilots. Safe operation requires an aircraft to predict the intent, and corresponding goal location, of other aircraft. This paper introduces a multimodal framework for aircraft goal prediction that integrates natural language understanding with spatial reasoning to improve autonomous decision-making in such environments. We leverage automatic speech recognition and large language models to transcribe and interpret pilot radio calls, identify aircraft, and extract discrete intent labels. These intent labels are fused with observed trajectories to condition a temporal convolutional network and Gaussian mixture model for probabilistic goal prediction. Our method significantly reduces goal prediction error compared to baselines that rely solely on motion history, demonstrating that language-conditioned prediction increases prediction accuracy. Experiments on a real-world dataset from an untowered airport validate the approach and highlight its potential to enable socially aware, language-conditioned robotic motion planning.', 'abstract_zh': '自主飞行器必须在无控制塔台的空域安全运行，其中协调依赖于人类飞行员之间的语音通信。安全运行需要飞行器预测其他飞行器的意图及其相应的目标位置。本文引入了一种多模态框架，该框架结合自然语言理解和空间推理，以改善此类环境中的自主决策。我们利用自动语音识别和大型语言模型来转录和解释飞行员的无线电呼叫、识别飞行器并提取离散的意图标签。这些意图标签与观测轨迹融合，以条件化时序卷积网络和高斯混合模型，进行概率性目标预测。与仅依赖运动历史的基线方法相比，我们的方法显著减少了目标预测误差，证明了语言条件下的预测可以提高预测准确性。实验结果验证了该方法，并突显了其在实现社交意识的语言条件下飞行器运动规划方面的潜力。', 'title_zh': '语言约束提高无塔台 airspace 中航空器目标预测的准确性'}
{'arxiv_id': 'arXiv:2509.14040', 'title': 'Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning', 'authors': 'Zewen Yang, Xiaobing Dai, Dongfa Zhang, Yu Li, Ziyang Meng, Bingkun Huang, Hamid Sadeghian, Sami Haddadin', 'link': 'https://arxiv.org/abs/2509.14040', 'abstract': "Learning from demonstration allows robots to acquire complex skills from human demonstrations, but conventional approaches often require large datasets and fail to generalize across coordinate transformations. In this paper, we propose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP) learning framework that enables robots to perform human-guided automated control from a single motion prompt. A dataset-construction strategy based on coordinate transformations is introduced that enforces invariance to translation, rotation, and scaling, while supporting multi-step predictions. Moreover, GeoGP is robust to variations in the user's motion prompt and supports multi-skill autonomy. We validate the proposed approach through numerical simulations with the designed user graphical interface and two real-world robotic experiments, which demonstrate that the proposed method is effective, generalizes across tasks, and significantly reduces the demonstration burden. Project page is available at: this https URL", 'abstract_zh': '从演示学习使机器人能够从人类示范中获得复杂技能，但传统方法通常需要大量数据集并在坐标变换跨任务泛化方面存在困难。本文提出了Prompt2Auto，这是一种几何不变的单次学习高斯过程（GeoGP）框架，使机器人能够从单一运动提示进行人引导的自动化控制。提出了一种基于坐标变换的数据集构建策略，确保在平移、旋转和缩放方面的不变性，同时支持多步预测。此外，GeoGP对用户运动提示的变化具有鲁棒性，支持多技能自主。通过设计的用户图形界面的数值仿真和两个实际机器人实验验证了所提出的方法，证明了该方法的有效性、跨任务泛化能力以及显著降低了演示负担。项目页面见：this https URL。', 'title_zh': 'Prompt2Auto: 从运动提示到基于几何不变的一次性高斯过程学习自动控制'}
{'arxiv_id': 'arXiv:2509.14025', 'title': 'TransforMARS: Fault-Tolerant Self-Reconfiguration for Arbitrarily Shaped Modular Aerial Robot Systems', 'authors': 'Rui Huang, Zhiyu Gao, Siyu Tang, Jialin Zhang, Lei He, Ziqian Zhang, Lin Zhao', 'link': 'https://arxiv.org/abs/2509.14025', 'abstract': 'Modular Aerial Robot Systems (MARS) consist of multiple drone modules that are physically bound together to form a single structure for flight. Exploiting structural redundancy, MARS can be reconfigured into different formations to mitigate unit or rotor failures and maintain stable flight. Prior work on MARS self-reconfiguration has solely focused on maximizing controllability margins to tolerate a single rotor or unit fault for rectangular-shaped MARS. We propose TransforMARS, a general fault-tolerant reconfiguration framework that transforms arbitrarily shaped MARS under multiple rotor and unit faults while ensuring continuous in-air stability. Specifically, we develop algorithms to first identify and construct minimum controllable assemblies containing faulty units. We then plan feasible disassembly-assembly sequences to transport MARS units or subassemblies to form target configuration. Our approach enables more flexible and practical feasible reconfiguration. We validate TransforMARS in challenging arbitrarily shaped MARS configurations, demonstrating substantial improvements over prior works in both the capacity of handling diverse configurations and the number of faults tolerated. The videos and source code of this work are available at the anonymous repository: this https URL', 'abstract_zh': '模块化飞行器系统（MARS）由多个物理连接的无人机模块组成，形成单一结构进行飞行。利用结构冗余，MARS可以重新配置成不同的编队以缓解单一旋翼或模块故障并维持稳定飞行。先前关于MARS自重构的研究仅集中在最大化控制裕度以容忍矩形MARS的单一旋翼或模块故障。我们提出了一种通用的故障 tolerant 重构框架 TransforMARS，能够在多种旋翼和模块故障情况下转换任意形状的MARS，同时确保飞行中的连续稳定性。具体地，我们开发了算法以首先识别并构建包含故障模块的最小可控组合。然后规划可行的拆卸-重构序列以运输MARS模块或子组合以形成目标配置。我们的方法能够实现更灵活和实用的可行重构。我们通过挑战性的任意形状MARS配置验证了TransforMARS，证明了与先前研究相比，在处理多样配置能力和容忍故障数量方面均取得了显著改进。相关视频和源代码可在匿名仓库中获得：this https URL。', 'title_zh': 'TransformMARS：任意形状模块化空中机器人系统的容错自重构'}
{'arxiv_id': 'arXiv:2509.14010', 'title': 'Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization', 'authors': 'Zong Chen, Shaoyang Li, Ben Liu, Min Li, Zhouping Yin, Yiqun Li', 'link': 'https://arxiv.org/abs/2509.14010', 'abstract': "Wheel-legged robots with integrated manipulators hold great promise for mobile manipulation in logistics, industrial automation, and human-robot collaboration. However, unified control of such systems remains challenging due to the redundancy in degrees of freedom, complex wheel-ground contact dynamics, and the need for seamless coordination between locomotion and manipulation. In this work, we present the design and whole-body motion control of an omnidirectional wheel-legged quadrupedal robot equipped with a dexterous manipulator. The proposed platform incorporates independently actuated steering modules and hub-driven wheels, enabling agile omnidirectional locomotion with high maneuverability in structured environments. To address the challenges of contact-rich interaction, we develop a contact-aware whole-body dynamic optimization framework that integrates point-contact modeling for manipulation with line-contact modeling for wheel-ground interactions. A warm-start strategy is introduced to accelerate online optimization, ensuring real-time feasibility for high-dimensional control. Furthermore, a unified kinematic model tailored for the robot's 4WIS-4WID actuation scheme eliminates the need for mode switching across different locomotion strategies, improving control consistency and robustness. Simulation and experimental results validate the effectiveness of the proposed framework, demonstrating agile terrain traversal, high-speed omnidirectional mobility, and precise manipulation under diverse scenarios, underscoring the system's potential for factory automation, urban logistics, and service robotics in semi-structured environments.", 'abstract_zh': '集成 manipulator 的轮-legged 机器人在物流、工业自动化和人机协作中的移动 manipulation 具有巨大潜力。然而，由于自由度的冗余性、复杂的轮-地面接触动力学以及运动与操作之间的无缝协调需求，这类系统的统一控制仍然具有挑战性。在本工作中，我们提出了一个配备灵巧 manipulator 的全向轮-legged 四足机器人设计及其全身运动控制方法。该提出的平台集成了独立驱动的转向模块和 Hub 驱动的轮子，能够在结构化环境中实现高效灵活的全向运动。为应对接触丰富的交互挑战，我们开发了一种接触感知的全身动态优化框架，将点接触建模与线接触建模结合应用于操作和轮-地面交互。引入预热策略以加速在线优化，确保高维度控制的实时可行性。此外，针对该机器人 4WIS-4WID 驱动方案定制的统一运动学模型消除了不同运动策略之间的模式切换需求，提高了控制的一致性和鲁棒性。仿真和实验结果验证了所提框架的有效性，展示了在多种场景下该系统灵活的地形穿越、高速全向移动和精确操作的能力，突显了其在半结构化环境中的工厂自动化、城市物流和服务机器人应用的潜力。', 'title_zh': '全身体动控制的 omnidirectional 轮腿式移动 manipulator 通过接触感知动力学优化'}
{'arxiv_id': 'arXiv:2509.13998', 'title': 'Flexible and Foldable: Workspace Analysis and Object Manipulation Using a Soft, Interconnected, Origami-Inspired Actuator Array', 'authors': 'Bailey Dacre, Rodrigo Moreno, Serhat Demirtas, Ziqiao Wang, Yuhao Jiang, Jamie Paik, Kasper Stoy, Andrés Faíña', 'link': 'https://arxiv.org/abs/2509.13998', 'abstract': 'Object manipulation is a fundamental challenge in robotics, where systems must balance trade-offs among manipulation capabilities, system complexity, and throughput. Distributed manipulator systems (DMS) use the coordinated motion of actuator arrays to perform complex object manipulation tasks, seeing widespread exploration within the literature and in industry. However, existing DMS designs typically rely on high actuator densities and impose constraints on object-to-actuator scale ratios, limiting their adaptability. We present a novel DMS design utilizing an array of 3-DoF, origami-inspired robotic tiles interconnected by a compliant surface layer. Unlike conventional DMS, our approach enables manipulation not only at the actuator end effectors but also across a flexible surface connecting all actuators; creating a continuous, controllable manipulation surface. We analyse the combined workspace of such a system, derive simple motion primitives, and demonstrate its capabilities to translate simple geometric objects across an array of tiles. By leveraging the inter-tile connective material, our approach significantly reduces actuator density, increasing the area over which an object can be manipulated by x1.84 without an increase in the number of actuators. This design offers a lower cost and complexity alternative to traditional high-density arrays, and introduces new opportunities for manipulation strategies that leverage the flexibility of the interconnected surface.', 'abstract_zh': '基于折纸灵感的柔性表面连接三自由度 robotic 块分布式 manipulator 系统设计', 'title_zh': '灵活可折叠：基于 Origami 灵感的软互联执行器阵列的工作空间分析与物体操作'}
{'arxiv_id': 'arXiv:2509.13972', 'title': 'BIM Informed Visual SLAM for Construction Monitoring', 'authors': 'Asier Bikandi, Miguel Fernandez-Cortizas, Muhammad Shaheer, Ali Tourani, Holger Voos, Jose Luis Sanchez-Lopez', 'link': 'https://arxiv.org/abs/2509.13972', 'abstract': 'Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring construction sites, where aligning the evolving as-built state with the as-planned design enables early error detection and reduces costly rework. LiDAR-based SLAM achieves high geometric precision, but its sensors are typically large and power-demanding, limiting their use on portable platforms. Visual SLAM offers a practical alternative with lightweight cameras already embedded in most mobile devices. however, visually mapping construction environments remains challenging: repetitive layouts, occlusions, and incomplete or low-texture structures often cause drift in the trajectory map. To mitigate this, we propose an RGB-D SLAM system that incorporates the Building Information Model (BIM) as structural prior knowledge. Instead of relying solely on visual cues, our system continuously establishes correspondences between detected wall and their BIM counterparts, which are then introduced as constraints in the back-end optimization. The proposed method operates in real time and has been validated on real construction sites, reducing trajectory error by an average of 23.71% and map RMSE by 7.14% compared to visual SLAM baselines. These results demonstrate that BIM constraints enable reliable alignment of the digital plan with the as-built scene, even under partially constructed conditions.', 'abstract_zh': '基于RGB-D的BIM约束SLAM系统在施工场地监测中的应用', 'title_zh': 'BIM驱动的视觉SLAM在施工监测中的应用'}
{'arxiv_id': 'arXiv:2509.13965', 'title': 'MetricNet: Recovering Metric Scale in Generative Navigation Policies', 'authors': 'Abhijeet Nayak, Débora N.P. Oliveira, Samiran Gode, Cordelia Schmid, Wolfram Burgard', 'link': 'https://arxiv.org/abs/2509.13965', 'abstract': 'Generative navigation policies have made rapid progress in improving end-to-end learned navigation. Despite their promising results, this paradigm has two structural problems. First, the sampled trajectories exist in an abstract, unscaled space without metric grounding. Second, the control strategy discards the full path, instead moving directly towards a single waypoint. This leads to short-sighted and unsafe actions, moving the robot towards obstacles that a complete and correctly scaled path would circumvent. To address these issues, we propose MetricNet, an effective add-on for generative navigation that predicts the metric distance between waypoints, grounding policy outputs in real-world coordinates. We evaluate our method in simulation with a new benchmarking framework and show that executing MetricNet-scaled waypoints significantly improves both navigation and exploration performance. Beyond simulation, we further validate our approach in real-world experiments. Finally, we propose MetricNav, which integrates MetricNet into a navigation policy to guide the robot away from obstacles while still moving towards the goal.', 'abstract_zh': '基于度量的生成导航政策已在端到端学习导航方面取得了迅速进展。尽管成果显著，该范式存在两个结构问题。首先，采样的轨迹存在于抽象且无尺度的空間中，缺乏度量基准。其次，控制策略只关注单个航点，丢弃了完整的路径信息。这会导致目光短浅且不安全的行为，使机器人朝前方障碍移动，而完整的正确尺度路径会避开这些障碍。为解决这些问题，我们提出MetricNet，这是一种有效的生成导航补充工具，能够预测航点间的度量距离，将策略输出锚定在现实世界的坐标系中。我们在一个新的基准测试框架下对我们的方法进行了模拟评估，并展示了执行MetricNet缩放后的航点显著提高了导航和探索性能。此外，我们在真实世界实验中进一步验证了我们的方法。最后，我们提出MetricNav，将MetricNet整合到导航策略中，引导机器人远离障碍物同时仍朝目标移动。', 'title_zh': 'MetricNet: 恢复生成导航策略中的度量尺度'}
{'arxiv_id': 'arXiv:2509.13956', 'title': 'SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning', 'authors': 'Zewei Yang, Zengqi Peng, Jun Ma', 'link': 'https://arxiv.org/abs/2509.13956', 'abstract': 'Autonomous parking is a critical component for achieving safe and efficient urban autonomous driving. However, unstructured environments and dynamic interactions pose significant challenges to autonomous parking tasks. To address this problem, we propose SEG-Parking, a novel end-to-end offline reinforcement learning (RL) framework to achieve interaction-aware autonomous parking. Notably, a specialized parking dataset is constructed for parking scenarios, which include those without interference from the opposite vehicle (OV) and complex ones involving interactions with the OV. Based on this dataset, a goal-conditioned state encoder is pretrained to map the fused perception information into the latent space. Then, an offline RL policy is optimized with a conservative regularizer that penalizes out-of-distribution actions. Extensive closed-loop experiments are conducted in the high-fidelity CARLA simulator. Comparative results demonstrate the superior performance of our framework with the highest success rate and robust generalization to out-of-distribution parking scenarios. The related dataset and source code will be made publicly available after the paper is accepted.', 'abstract_zh': '自主泊车是实现安全高效的城市自动驾驶的关键组成部分。然而，非结构化环境和动态交互给自主泊车任务带来了重大挑战。为了解决这一问题，我们提出了一种新颖的端到端离线强化学习(RL)框架SEG-Parking，以实现交互感知的自主泊车。特别地，我们构建了一个专门用于泊车场景的数据集，其中包括无对向车辆干扰的场景和涉及对向车辆交互的复杂场景。基于此数据集，我们预先训练了一个目标条件状态编码器，将融合感知信息映射到潜在空间。然后，该离线RL策略使用保守正则化器进行优化，该正则化器惩罚异常分布的动作。我们在高度保真的CARLA模拟器中进行了广泛的闭环实验。比较结果表明，我们的框架具有最高的成功率，并且在异常分布的泊车场景中具有强大的泛化能力。论文被接受后，相关数据集和源代码将公开发布。', 'title_zh': 'SEG-Parking:向着安全、高效且泛化的自主泊车的端到端 Offline 强化学习方法'}
{'arxiv_id': 'arXiv:2509.13949', 'title': 'SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks', 'authors': 'Jannick Stranghöner, Philipp Hartmann, Marco Braun, Sebastian Wrede, Klaus Neumann', 'link': 'https://arxiv.org/abs/2509.13949', 'abstract': 'High-mix low-volume (HMLV) industrial assembly, common in small and medium-sized enterprises (SMEs), requires the same precision, safety, and reliability as high-volume automation while remaining flexible to product variation and environmental uncertainty. Current robotic systems struggle to meet these demands. Manual programming is brittle and costly to adapt, while learning-based methods suffer from poor sample efficiency and unsafe exploration in contact-rich tasks. To address this, we present SHaRe-RL, a reinforcement learning framework that leverages multiple sources of prior knowledge. By (i) structuring skills into manipulation primitives, (ii) incorporating human demonstrations and online corrections, and (iii) bounding interaction forces with per-axis compliance, SHaRe-RL enables efficient and safe online learning for long-horizon, contact-rich industrial assembly tasks. Experiments on the insertion of industrial Harting connector modules with 0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance within practical time budgets. Our results show that process expertise, without requiring robotics or RL knowledge, can meaningfully contribute to learning, enabling safer, more robust, and more economically viable deployment of RL for industrial assembly.', 'abstract_zh': '面向小批量多样生产的高精度机器人学习框架：SHaRe-RL', 'title_zh': 'SHaRe-RL: 结构化、交互式强化学习在接触丰富的工业装配任务中的应用'}
{'arxiv_id': 'arXiv:2509.13948', 'title': 'The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot', 'authors': 'Benedict Barrow, Roger K. Moore', 'link': 'https://arxiv.org/abs/2509.13948', 'abstract': "Trust and the perception of trustworthiness play an important role in decision-making and our behaviour towards others, and this is true not only of human-human interactions but also of human-robot interactions. While significant advances have been made in recent years in the field of social robotics, there is still some way to go before we fully understand the factors that influence human trust in robots. This paper presents the results of a study into the first impressions created by a social robot's facial features, based on the hypothesis that a `babyface' engenders trust. By manipulating the back-projected face of a Furhat robot, the study confirms that eye shape and size have a significant impact on the perception of trustworthiness. The work thus contributes to an understanding of the design choices that need to be made when developing social robots so as to optimise the effectiveness of human-robot interaction.", 'abstract_zh': '信任与信任感知在人类决策和对他人行为中扮演重要角色，这不仅体现在人与人之间的互动中，也体现在人与机器人之间的互动中。尽管近年来社会机器人领域取得了显著进展，但在完全理解影响人类对机器人信任的因素方面仍有很多工作要做。本文基于“婴儿脸”能够引发信任的假设，研究了社会机器人面部特征给人的第一印象，并通过操纵Furhat机器人的投影面部证实了眼睛的形状和大小对信任感知有显著影响。这项工作有助于理解在开发社会机器人时需要做出的设计选择，以优化人机互动的有效性。', 'title_zh': '面部特征对社交机器人可信度感知的影响'}
{'arxiv_id': 'arXiv:2509.13943', 'title': 'Reinforcement Learning for Autonomous Point-to-Point UAV Navigation', 'authors': 'Salim Oyinlola, Nitesh Subedi, Soumik Sarkar', 'link': 'https://arxiv.org/abs/2509.13943', 'abstract': 'Unmanned Aerial Vehicles (UAVs) are increasingly used in automated inspection, delivery, and navigation tasks that require reliable autonomy. This project develops a reinforcement learning (RL) approach to enable a single UAV to autonomously navigate between predefined points without manual intervention. The drone learns navigation policies through trial-and-error interaction, using a custom reward function that encourages goal-reaching efficiency while penalizing collisions and unsafe behavior. The control system integrates ROS with a Gym-compatible training environment, enabling flexible deployment and testing. After training, the learned policy is deployed on a real UAV platform and evaluated under practical conditions. Results show that the UAV can successfully perform autonomous navigation with minimal human oversight, demonstrating the viability of RL-based control for point-to-point drone operations in real-world scenarios.', 'abstract_zh': '无人驾驶航空器（UAV）越来越多地用于自动化检测、递送和导航等任务，需要可靠的自主能力。该项目开发了一种强化学习（RL）方法，使单个UAV能够在无需人工干预的情况下自主在预先定义的点之间导航。无人机遇通过试错交互学习导航策略，使用自定义奖励函数鼓励高效的目标达成，同时对碰撞和不安全行为进行惩罚。控制系统将ROS与兼容Gym的训练环境相结合，实现灵活的部署和测试。训练完成后，所学习的策略部署到实际的UAV平台上，并在实际条件下进行评估。结果表明，该UAV能够在最少的人为监督下成功执行自主导航，展示了基于RL的控制在实际场景中进行点到点无人机操作的可行性。', 'title_zh': '自主点到点无人机导航的强化学习方法'}
{'arxiv_id': 'arXiv:2509.13926', 'title': 'MAP: End-to-End Autonomous Driving with Map-Assisted Planning', 'authors': 'Huilin Yin, Yiming Kan, Daniel Watzenig', 'link': 'https://arxiv.org/abs/2509.13926', 'abstract': 'In recent years, end-to-end autonomous driving has attracted increasing attention for its ability to jointly model perception, prediction, and planning within a unified framework. However, most existing approaches underutilize the online mapping module, leaving its potential to enhance trajectory planning largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel map-assisted end-to-end trajectory planning framework. MAP explicitly integrates segmentation-based map features and the current ego status through a Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and a Weight Adapter based on current ego status. Experiments conducted on the DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6% reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a 44.5% improvement in overall score compared to the UniV2X baseline, even without post-processing. Furthermore, it achieves top ranking in Track 2 of the End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of overall score. These results highlight the effectiveness of explicitly leveraging semantic map features in planning and suggest new directions for improving structure design in end-to-end autonomous driving systems. Our code is available at this https URL', 'abstract_zh': '基于地图辅助的端到端轨迹规划框架MAP', 'title_zh': 'MAP:基于地图辅助规划的端到端自动驾驶'}
{'arxiv_id': 'arXiv:2509.13903', 'title': 'PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models', 'authors': 'Artem Lykov, Jeffrin Sam, Hung Khang Nguyen, Vladislav Kozlovskiy, Yara Mahmoud, Valerii Serpiva, Miguel Altamirano Cabrera, Mikhail Konenkov, Dzmitry Tsetserukou', 'link': 'https://arxiv.org/abs/2509.13903', 'abstract': 'We introduce PhysicalAgent, an agentic framework for robotic manipulation that integrates iterative reasoning, diffusion-based video generation, and closed-loop execution. Given a textual instruction, our method generates short video demonstrations of candidate trajectories, executes them on the robot, and iteratively re-plans in response to failures. This approach enables robust recovery from execution errors. We evaluate PhysicalAgent across multiple perceptual modalities (egocentric, third-person, and simulated) and robotic embodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing against state-of-the-art task-specific baselines. Experiments demonstrate that our method consistently outperforms prior approaches, achieving up to 83% success on human-familiar tasks. Physical trials reveal that first-attempt success is limited (20-30%), yet iterative correction increases overall success to 80% across platforms. These results highlight the potential of video-based generative reasoning for general-purpose robotic manipulation and underscore the importance of iterative execution for recovering from initial failures. Our framework paves the way for scalable, adaptable, and robust robot control.', 'abstract_zh': '物理代理：一种集成迭代推理、基于扩散的视频生成和闭环执行的机器人操作框架', 'title_zh': '物理智能体：面向基础世界模型的通用认知机器人研究'}
{'arxiv_id': 'arXiv:2509.13882', 'title': 'Repulsive Trajectory Modification and Conflict Resolution for Efficient Multi-Manipulator Motion Planning', 'authors': 'Junhwa Hong, Beomjoon Lee, Woojin Lee, Changjoo Nam', 'link': 'https://arxiv.org/abs/2509.13882', 'abstract': 'We propose an efficient motion planning method designed to efficiently find collision-free trajectories for multiple manipulators. While multi-manipulator systems offer significant advantages, coordinating their motions is computationally challenging owing to the high dimensionality of their composite configuration space. Conflict-Based Search (CBS) addresses this by decoupling motion planning, but suffers from subsequent conflicts incurred by resolving existing conflicts, leading to an exponentially growing constraint tree of CBS. Our proposed method is based on repulsive trajectory modification within the two-level structure of CBS. Unlike conventional CBS variants, the low-level planner applies a gradient descent approach using an Artificial Potential Field. This field generates repulsive forces that guide the trajectory of the conflicting manipulator away from those of other robots. As a result, subsequent conflicts are less likely to occur. Additionally, we develop a strategy that, under a specific condition, directly attempts to find a conflict-free solution in a single step without growing the constraint tree. Through extensive tests including physical robot experiments, we demonstrate that our method consistently reduces the number of expanded nodes in the constraint tree, achieves a higher success rate, and finds a solution faster compared to Enhanced CBS and other state-of-the-art algorithms.', 'abstract_zh': '我们提出了一种高效运动规划方法，旨在高效地为多个操作器找到无碰撞轨迹。尽管多操作器系统具有显著优势，但由于其复合配置空间的高维度，协调其运动在计算上具有挑战性。冲突基搜索（CBS）通过解耦运动规划来解决这一问题，但会在解决现有冲突时产生附加冲突，导致CBS的约束树呈指数增长。我们提出的方法基于CBS的两层结构内的排斥轨迹修改。与传统的CBS变体不同，低层规划器采用梯度下降方法结合人工势场。该场产生排斥力，引导冲突操作器的轨迹远离其他机器人。这样一来，后续冲突的可能性较小。此外，我们开发了一种策略，在特定条件下，直接尝试在单步骤中找到无冲突解决方案，而不增长约束树。通过包括物理机器人实验在内的广泛测试，我们展示了该方法在约束树中展开的节点数量上具有一致性减少，成功率更高，并且比增强的CBS和其他最先进的算法更快找到解决方案。', 'title_zh': '排斥轨迹修改与冲突分辨率的高效多 manipulator 运动规划'}
{'arxiv_id': 'arXiv:2509.13861', 'title': 'Using Petri Nets for Context-Adaptive Robot Explanations', 'authors': 'Görkem Kılınç Soylu, Neziha Akalin, Maria Riveiro', 'link': 'https://arxiv.org/abs/2509.13861', 'abstract': 'In human-robot interaction, robots must communicate in a natural and transparent manner to foster trust, which requires adapting their communication to the context. In this paper, we propose using Petri nets (PNs) to model contextual information for adaptive robot explanations. PNs provide a formal, graphical method for representing concurrent actions, causal dependencies, and system states, making them suitable for analyzing dynamic interactions between humans and robots. We demonstrate this approach through a scenario involving a robot that provides explanations based on contextual cues such as user attention and presence. Model analysis confirms key properties, including deadlock-freeness, context-sensitive reachability, boundedness, and liveness, showing the robustness and flexibility of PNs for designing and verifying context-adaptive explanations in human-robot interactions.', 'abstract_zh': '在人类-机器人交互中，机器人必须以自然和透明的方式进行沟通以培养信任，这需要根据上下文适应其沟通方式。本文提出使用 Petri 网（PNs）来建模上下文信息以适应机器人解释。PNs 提供了一种形式化的图形方法来表示并发动作、因果依赖关系和系统状态，使其适用于分析人类与机器人之间的动态交互。我们通过一个场景来展示这种方法，该场景涉及一个根据用户注意力和在场等上下文线索提供解释的机器人。模型分析确认了关键属性，包括无死锁性、上下文感知可达性、有界性和活化性，展示了 PNs 在设计和验证人类-机器人交互中的上下文适应性解释方面的稳健性和灵活性。', 'title_zh': '基于Petri网的上下文自适应机器人解释'}
{'arxiv_id': 'arXiv:2509.13857', 'title': 'InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap', 'authors': 'Nguyen Hoang Khoi Tran, Julie Stephany Berrio, Mao Shan, Stewart Worrall', 'link': 'https://arxiv.org/abs/2509.13857', 'abstract': 'Reliable global localization is critical for autonomous vehicles, especially in environments where GNSS is degraded or unavailable, such as urban canyons and tunnels. Although high-definition (HD) maps provide accurate priors, the cost of data collection, map construction, and maintenance limits scalability. OpenStreetMap (OSM) offers a free and globally available alternative, but its coarse abstraction poses challenges for matching with sensor data. We propose InterKey, a cross-modal framework that leverages road intersections as distinctive landmarks for global localization. Our method constructs compact binary descriptors by jointly encoding road and building imprints from point clouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation, orientation determination, and area-equalized sampling strategies, enabling robust cross-modal matching. Experiments on the KITTI dataset demonstrate that InterKey achieves state-of-the-art accuracy, outperforming recent baselines by a large margin. The framework generalizes to sensors that can produce dense structural point clouds, offering a scalable and cost-effective solution for robust vehicle localization.', 'abstract_zh': '可靠的全局定位对于自动驾驶车辆至关重要，尤其是在全球导航卫星系统（GNSS）受到降级或不可用影响的城市峡谷和隧道等环境中。尽管高精度地图（HD Maps）提供了准确的先验信息，但其数据采集、地图构建和维护的成本限制了其扩展性。OpenStreetMap（OSM）提供了免费且全球可用的替代方案，但其粗粒度的抽象化为与传感器数据匹配带来了挑战。我们提出了一种跨模态框架InterKey，该框架利用道路交汇点作为独特的地标进行全局定位。我们的方法通过联合编码点云和OSM中的道路及建筑印记来构建紧凑的二进制描述子。为了弥合模态差异，我们引入了差异缓解、方向确定和区域均衡采样策略，从而实现稳健的跨模态匹配。在KITTI数据集上的实验表明，InterKey 达到了最先进的准确性，显著优于近期的基线方法。该框架适用于能够生成密集结构点云的传感器，提供了稳健车辆定位的可扩展且经济高效解决方案。', 'title_zh': 'InterKey：跨模态交义关键点在OpenStreetMap中的全局定位'}
{'arxiv_id': 'arXiv:2509.13839', 'title': 'Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models', 'authors': 'Motonari Kambara, Komei Sugiura', 'link': 'https://arxiv.org/abs/2509.13839', 'abstract': 'In this work, we address the problem of predicting the future success of open-vocabulary object manipulation tasks. Conventional approaches typically determine success or failure after the action has been carried out. However, they make it difficult to prevent potential hazards and rely on failures to trigger replanning, thereby reducing the efficiency of object manipulation sequences. To overcome these challenges, we propose a model, which predicts the alignment between a pre-manipulation egocentric image with the planned trajectory and a given natural language instruction. We introduce a Multi-Level Trajectory Fusion module, which employs a state-of-the-art deep state-space model and a transformer encoder in parallel to capture multi-level time-series self-correlation within the end effector trajectory. Our experimental results indicate that the proposed method outperformed existing methods, including foundation models.', 'abstract_zh': '本研究解决了预测开放词汇对象操作任务未来成功的问题。传统方法通常在动作执行后才确定成功或失败。这使得难以预防潜在的危害，并且依赖失败来触发重新规划，从而降低了对象操作序列的效率。为克服这些挑战，我们提出了一种模型，该模型预测预操作第一人称视角图像与计划轨迹和给定自然语言指令之间的对齐。我们引入了多级轨迹融合模块，该模块并行使用最先进的深度状态空间模型和变换器编码器来捕捉末端执行器轨迹内的多级时间序列自相关性。实验结果表明，提出的方法优于现有方法，包括基础模型。', 'title_zh': '预操纵对齐预测的平行深度状态空间与变换器模型方法'}
{'arxiv_id': 'arXiv:2509.13833', 'title': 'Track Any Motions under Any Disturbances', 'authors': 'Zhikai Zhang, Jun Guo, Chao Chen, Jilong Wang, Chenghuai Lin, Yunrui Lian, Han Xue, Zhenrong Wang, Maoqi Liu, Huaping Liu, He Wang, Li Yi', 'link': 'https://arxiv.org/abs/2509.13833', 'abstract': 'A foundational humanoid motion tracker is expected to be able to track diverse, highly dynamic, and contact-rich motions. More importantly, it needs to operate stably in real-world scenarios against various dynamics disturbances, including terrains, external forces, and physical property changes for general practical use. To achieve this goal, we propose Any2Track (Track Any motions under Any disturbances), a two-stage RL framework to track various motions under multiple disturbances in the real world. Any2Track reformulates dynamics adaptability as an additional capability on top of basic action execution and consists of two key components: AnyTracker and AnyAdapter. AnyTracker is a general motion tracker with a series of careful designs to track various motions within a single policy. AnyAdapter is a history-informed adaptation module that endows the tracker with online dynamics adaptability to overcome the sim2real gap and multiple real-world disturbances. We deploy Any2Track on Unitree G1 hardware and achieve a successful sim2real transfer in a zero-shot manner. Any2Track performs exceptionally well in tracking various motions under multiple real-world disturbances.', 'abstract_zh': '一种基础的人形运动跟踪器应能够跟踪多样、高度动态和接触丰富的运动。更重要的是，它需要在各种动力学干扰下，包括地形、外力和物理属性变化的现实场景中稳定运行。为了实现这一目标，我们提出了一种名为Any2Track（在任意干扰下跟踪任意运动）的两阶段强化学习框架，该框架能够在多种干扰下实时跟踪各种运动。Any2Track将动力学适应性重新表述为基本动作执行之外的一种附加能力，并包含两个关键组件：AnyTracker和AnyAdapter。AnyTracker是一种通用的运动跟踪器，经过精心设计，能够在单一策略内跟踪各种运动。AnyAdapter是一种基于历史的信息适应模块，赋予跟踪器在线动力学适应能力，以克服模拟到现实的差距和多种现实世界的干扰。我们将在Unitree G1硬件上部署Any2Track，并以零样本的方式实现成功的模拟到现实的转换。在多种现实世界的干扰下，Any2Track在跟踪各种运动方面表现出色。', 'title_zh': '在任意干扰下的任意运动跟踪'}
{'arxiv_id': 'arXiv:2509.13832', 'title': 'UltraHiT: A Hierarchical Transformer Architecture for Generalizable Internal Carotid Artery Robotic Ultrasonography', 'authors': 'Teng Wang, Haojun Jiang, Yuxuan Wang, Zhenguo Sun, Xiangjie Yan, Xiang Li, Gao Huang', 'link': 'https://arxiv.org/abs/2509.13832', 'abstract': 'Carotid ultrasound is crucial for the assessment of cerebrovascular health, particularly the internal carotid artery (ICA). While previous research has explored automating carotid ultrasound, none has tackled the challenging ICA. This is primarily due to its deep location, tortuous course, and significant individual variations, which greatly increase scanning complexity. To address this, we propose a Hierarchical Transformer-based decision architecture, namely UltraHiT, that integrates high-level variation assessment with low-level action decision. Our motivation stems from conceptualizing individual vascular structures as morphological variations derived from a standard vascular model. The high-level module identifies variation and switches between two low-level modules: an adaptive corrector for variations, or a standard executor for normal cases. Specifically, both the high-level module and the adaptive corrector are implemented as causal transformers that generate predictions based on the historical scanning sequence. To ensure generalizability, we collected the first large-scale ICA scanning dataset comprising 164 trajectories and 72K samples from 28 subjects of both genders. Based on the above innovations, our approach achieves a 95% success rate in locating the ICA on unseen individuals, outperforming baselines and demonstrating its effectiveness. Our code will be released after acceptance.', 'abstract_zh': '颈动脉超声对于评估脑血管健康至关重要，尤其是内颈动脉（ICA）。尽管以往研究已探索自动化颈动脉超声技术，但均未解决具有挑战性的ICA问题。这主要归因于ICA的深部位置、蜿蜒路径以及显著的个体差异，极大地增加了扫描的复杂性。为解决这一问题，我们提出了一种基于分层变换器的决策架构，即UltraHiT，该架构将高层次的变异性评估与低层次的动作决策相结合。我们的动机源于将个体血管结构视为源自标准血管模型的形态变异。高层次模块识别变异并切换到两个低层次模块之一：适应性校正器以处理变异，或标准执行器以处理正常情况。具体而言，高低层次模块均通过因果变换器实现，基于历史扫描序列生成预测。为了确保泛化能力，我们收集了包含164条轨迹和72,000个样本的大型ICA扫描数据集，涉及28名性别各异的受试者。在这些创新的基础上，我们的方法在未见个体中识别ICA的成功率达到95%，优于基线方法并展示了其有效性。我们的代码将在接受后发布。', 'title_zh': 'UltraHiT: 一种用于可泛化的颈内动脉机器人超声成像的分层Transformer架构'}
{'arxiv_id': 'arXiv:2509.13827', 'title': 'How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots', 'authors': 'Renyuan Liu, Haoting Zhou, Chuankai Fang, Qinbing Fu', 'link': 'https://arxiv.org/abs/2509.13827', 'abstract': 'Anyone who has tried to swat a fly has likely been frustrated by its remarkable this http URL ability stems from its visual neural perception system, particularly the collision-selective neurons within its small this http URL autonomous robots operating in complex and unfamiliar environments, achieving similar agility is highly desirable but often constrained by the trade-off between computational cost and this http URL this context, insect-inspired intelligence offers a parsimonious route to low-power, computationally efficient this http URL this paper, we propose an attention-driven visuomotor control strategy inspired by a specific class of fly visual projection neurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated escape this http URL our knowledge, this represents the first embodiment of an LPLC2 neural model in the embedded vision of a physical mobile robot, enabling collision perception and reactive this http URL model was simplified and optimized at 70KB in memory to suit the computational constraints of a vision-based micro robot, the Colias, while preserving key neural perception this http URL further incorporated multi-attention mechanisms to emulate the distributed nature of LPLC2 responses, allowing the robot to detect and react to approaching targets both rapidly and this http URL systematically evaluated the proposed method against a state-of-the-art locust-inspired collision detection this http URL showed that the fly-inspired visuomotor model achieved comparable robustness, at success rate of 96.1% in collision detection while producing more adaptive and elegant evasive this http URL demonstrating an effective collision-avoidance strategy, this work highlights the potential of fly-inspired neural models for advancing research into collective behaviors in insect intelligence.', 'abstract_zh': '飞蠊启发的低功耗计算视觉运动控制策略及其应用：一种适用于复杂环境微型自主机器人的碰撞感知与规避方法', 'title_zh': '飞神经感知机制增强微机器人视运动控制'}
{'arxiv_id': 'arXiv:2509.13816', 'title': 'Agile in the Face of Delay: Asynchronous End-to-End Learning for Real-World Aerial Navigation', 'authors': 'Yude Li, Zhexuan Zhou, Huizhe Li, Youmin Gong, Jie Mei', 'link': 'https://arxiv.org/abs/2509.13816', 'abstract': 'Robust autonomous navigation for Autonomous Aerial Vehicles (AAVs) in complex environments is a critical capability. However, modern end-to-end navigation faces a key challenge: the high-frequency control loop needed for agile flight conflicts with low-frequency perception streams, which are limited by sensor update rates and significant computational cost. This mismatch forces conventional synchronous models into undesirably low control rates. To resolve this, we propose an asynchronous reinforcement learning framework that decouples perception and control, enabling a high-frequency policy to act on the latest IMU state for immediate reactivity, while incorporating perception features asynchronously. To manage the resulting data staleness, we introduce a theoretically-grounded Temporal Encoding Module (TEM) that explicitly conditions the policy on perception delays, a strategy complemented by a two-stage curriculum to ensure stable and efficient training. Validated in extensive simulations, our method was successfully deployed in zero-shot sim-to-real transfer on an onboard NUC, where it sustains a 100~Hz control rate and demonstrates robust, agile navigation in cluttered real-world environments. Our source code will be released for community reference.', 'abstract_zh': '自主航空车辆在复杂环境中的鲁棒自主导航是一项关键能力。然而，现代端到端导航面临一个关键挑战：敏捷飞行所需的高频控制环与受限于传感器更新率和显著计算成本的低频感知流之间的不匹配。这种不匹配迫使传统的同步模型进入不理想的低控制率。为了解决这一问题，我们提出了一种异步强化学习框架，解耦感知与控制，使高频策略能够基于最新的IMU状态进行即时反应，同时异步整合感知特征。为管理由此产生的数据陈旧性，我们引入了一个基于理论的时序编码模块（TEM），明确地将策略条件在感知延迟上，这一策略通过两阶段课程来确保稳定和高效的训练。该方法在广泛的仿真中得到验证，并成功在机载NUC上实现了零样本仿真实际转换，维持了100~Hz的控制率，并在杂乱的实际环境中展示了鲁棒性和灵活的导航能力。源代码将向社区公开。', 'title_zh': '临危不乱：面向实际空域导航的异步端到端学习'}
{'arxiv_id': 'arXiv:2509.13815', 'title': 'Soft Regrasping Tool Inspired by Jamming Gripper', 'authors': 'Takuya Kiyokawa, Zhengtao Hu, Weiwei Wan, Kensuke Harada', 'link': 'https://arxiv.org/abs/2509.13815', 'abstract': 'Regrasping on fixtures is a promising approach to reduce pose uncertainty in robotic assembly, but conventional rigid fixtures lack adaptability and require dedicated designs for each part. To overcome this limitation, we propose a soft jig inspired by the jamming transition phenomenon, which can be continuously deformed to accommodate diverse object geometries. By pressing a triangular-pyramid-shaped tool into the membrane and evacuating the enclosed air, a stable cavity is formed as a placement space. We further optimize the stamping depth to balance placement stability and gripper accessibility. In soft-jig-based regrasping, the key challenge lies in optimizing the cavity size to achieve precise dropping; once the part is reliably placed, subsequent grasping can be performed with reduced uncertainty. Accordingly, we conducted drop experiments on ten mechanical parts of varying shapes, which achieved placement success rates exceeding 80% for most objects and above 90% for cylindrical ones, while failures were mainly caused by geometric constraints and membrane properties. These results demonstrate that the proposed jig enables general-purpose, accurate, and repeatable regrasping, while also clarifying its current limitations and future potential as a practical alternative to rigid fixtures in assembly automation.', 'abstract_zh': '基于软工装的重新抓取方法：减少机器人装配中姿态不确定性的一种有前途的 approach', 'title_zh': '受阻尼夹持器启发的软重新抓取工具'}
{'arxiv_id': 'arXiv:2509.13802', 'title': 'Shell-Type Soft Jig for Holding Objects during Disassembly', 'authors': 'Takuya Kiyokawa, Ryunosuke Takebayashi, Kensuke Harada', 'link': 'https://arxiv.org/abs/2509.13802', 'abstract': "This study addresses a flexible holding tool for robotic disassembly. We propose a shell-type soft jig that securely and universally holds objects, mitigating the risk of component damage and adapting to diverse shapes while enabling soft fixation that is robust to recognition, planning, and control errors. The balloon-based holding mechanism ensures proper alignment and stable holding performance, thereby reducing the need for dedicated jig design, highly accurate perception, precise grasping, and finely tuned trajectory planning that are typically required with conventional fixtures. Our experimental results demonstrate the practical feasibility of the proposed jig through performance comparisons with a vise and a jamming-gripper-inspired soft jig. Tests on ten different objects further showed representative successes and failures, clarifying the jig's limitations and outlook.", 'abstract_zh': '一种用于机器人拆卸的柔性持物工具的研究', 'title_zh': '用于拆卸过程中保持物体的壳型软夹具'}
{'arxiv_id': 'arXiv:2509.13780', 'title': 'Behavior Foundation Model for Humanoid Robots', 'authors': 'Weishuai Zeng, Shunlin Lu, Kangning Yin, Xiaojie Niu, Minyue Dai, Jingbo Wang, Jiangmiao Pang', 'link': 'https://arxiv.org/abs/2509.13780', 'abstract': 'Whole-body control (WBC) of humanoid robots has witnessed remarkable progress in skill versatility, enabling a wide range of applications such as locomotion, teleoperation, and motion tracking. Despite these achievements, existing WBC frameworks remain largely task-specific, relying heavily on labor-intensive reward engineering and demonstrating limited generalization across tasks and skills. These limitations hinder their response to arbitrary control modes and restrict their deployment in complex, real-world scenarios. To address these challenges, we revisit existing WBC systems and identify a shared objective across diverse tasks: the generation of appropriate behaviors that guide the robot toward desired goal states. Building on this insight, we propose the Behavior Foundation Model (BFM), a generative model pretrained on large-scale behavioral datasets to capture broad, reusable behavioral knowledge for humanoid robots. BFM integrates a masked online distillation framework with a Conditional Variational Autoencoder (CVAE) to model behavioral distributions, thereby enabling flexible operation across diverse control modes and efficient acquisition of novel behaviors without retraining from scratch. Extensive experiments in both simulation and on a physical humanoid platform demonstrate that BFM generalizes robustly across diverse WBC tasks while rapidly adapting to new behaviors. These results establish BFM as a promising step toward a foundation model for general-purpose humanoid control.', 'abstract_zh': '全身体操控制（WBC）在类人机器人中的进展显著提高了技能的多样性，使其在行走、远程操控和运动跟踪等领域拥有广泛应用。尽管取得了这些成就，现有的WBC框架仍主要针对特定任务，依赖于劳动密集型的奖励工程，并在跨任务和技能的一般化方面表现有限。这些限制阻碍了其对任意控制模式的响应，并限制了其在复杂真实世界场景中的部署。为解决这些挑战，我们回顾了现有的WBC系统，并发现了一个贯穿不同任务的共同目标：生成适当的行為以引导机器人达到期望的目标状态。基于这一洞察，我们提出行为基础模型（BFM），这是一种基于大规模行为数据集预训练的生成模型，用于捕捉类人机器人中广泛且可重用的行为知识。BFM结合了带掩码的在线蒸馏框架和条件变分自编码器（CVAE），以建模行为分布，从而实现跨不同控制模式的灵活操作，并在无需从零重新训练的情况下高效获取新行为。在模拟和物理类人平台上的广泛实验表明，BFM在不同WBC任务中表现出强大的泛化能力，并能快速适应新行为。这些结果使BFM成为通用类人控制基础模型的一个有前景的步骤。', 'title_zh': '类人机器人行为基础模型'}
{'arxiv_id': 'arXiv:2509.13774', 'title': 'Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach', 'authors': 'Piaopiao Jin, Qi Wang, Guokang Sun, Ziwen Cai, Pinjia He, Yangwei You', 'link': 'https://arxiv.org/abs/2509.13774', 'abstract': 'Vision-language-action (VLA) models demonstrate strong generalization in robotic manipulation but face challenges in complex, real-world tasks. While supervised fine-tuning with demonstrations is constrained by data quality, reinforcement learning (RL) offers a promising alternative. We propose a human-in-the-loop dual-actor fine-tuning framework grounded in RL. The framework integrates a primary actor for robust multi-task performance with a refinement actor for latent-space adaptation. Beyond standard physical interventions, we introduce a lightweight talk-and-tweak scheme that converts human corrections into semantically grounded language commands, thereby generating a new dataset for policy learning. In real-world multi-task experiments, our approach achieves 100% success across three tasks within 101 minutes of online fine-tuning. For long-horizon tasks, it sustains a 50% success rate over 12 consecutive operations. Furthermore, the framework scales effectively to multi-robot training, achieving up to a 2 times improvement in efficiency when using dual robots. The experiment videos are available at this https URL.', 'abstract_zh': '基于RL的人机环路双actor细调框架：Vision-Language-Action (VLA)模型在机器人操作中表现出强大的泛化能力，但在复杂的真实世界任务中面临挑战。虽然监督细调受限于数据质量，强化学习(RL)提供了一种有前景的替代方案。我们提出了一种基于RL的人机环路双actor细调框架，该框架结合了主actor以实现稳健的多任务性能，并集成了一个精炼actor以适应潜在空间。在标准物理干预之外，我们引入了一种轻量级的“谈和调整”方案，将人类修正转化为语义上连贯的语言命令，从而生成用于策略学习的新数据集。在实际多任务实验中，我们的方法在101分钟的在线细调时间内实现三任务100%的成功率。对于长时间任务，它在12次连续操作中维持50%的成功率。此外，该框架在多机器人训练中能够有效扩展，使用双机器人时效率提高最多可达2倍。实验视频可在以下链接查看。', 'title_zh': 'VLA模型的双演员微调：一种谈练结合的人在回路方法'}
{'arxiv_id': 'arXiv:2509.13771', 'title': 'CDFlow: Generative Gradient Flows for Configuration Space Distance Fields via Neural ODEs', 'authors': 'Mengzhu Li, Yunyu Zhou, He Ying, F. Richard Yu', 'link': 'https://arxiv.org/abs/2509.13771', 'abstract': 'Signed Distance Fields (SDFs) are a fundamental representation in robot motion planning. Their configuration-space counterpart, the Configuration Space Distance Field (CDF), directly encodes distances in joint space, offering a unified representation for optimization and control. However, existing CDF formulations face two major challenges in high-degree-of-freedom (DoF) robots: (1) they effectively return only a single nearest collision configuration, neglecting the multi-modal nature of minimal-distance collision configurations and leading to gradient ambiguity; and (2) they rely on sparse sampling of the collision boundary, which often fails to identify the true closest configurations, producing oversmoothed approximations and geometric distortion in high-dimensional spaces. We propose CDFlow, a novel framework that addresses these limitations by learning a continuous flow in configuration space via Neural Ordinary Differential Equations (Neural ODEs). We redefine the problem from finding a single nearest point to modeling the distribution of minimal-distance collision configurations. We also introduce an adaptive refinement sampling strategy to generate high-fidelity training data for this distribution. The resulting Neural ODE implicitly models this multi-modal distribution and produces a smooth, consistent gradient field-derived as the expected direction towards the distribution-that mitigates gradient ambiguity and preserves sharp geometric features. Extensive experiments on high-DoF motion planning tasks demonstrate that CDFlow significantly improves planning efficiency, trajectory quality, and robustness compared to existing CDF-based methods, enabling more robust and efficient planning for collision-aware robots in complex environments.', 'abstract_zh': '基于神经常微分方程的Configuration Space Distance Flow (CDFlow)在高自由度机器人运动规划中的应用', 'title_zh': 'CDFlow: 生成梯度流方法在神经ODEs中的配置空间距离场建模'}
{'arxiv_id': 'arXiv:2509.13737', 'title': 'Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control', 'authors': 'Renjie Wang, Shangke Lyu, Donglin Wang', 'link': 'https://arxiv.org/abs/2509.13737', 'abstract': 'While Reinforcement Learning (RL) has achieved remarkable progress in legged locomotion control, it often suffers from performance degradation in out-of-distribution (OOD) conditions and discrepancies between the simulation and the real environments. Instead of mainly relying on domain randomization (DR) to best cover the real environments and thereby close the sim-to-real gap and enhance robustness, this work proposes an emerging decoupled framework that acquires fast online adaptation ability and mitigates the sim-to-real problems in unfamiliar environments by isolating stance-leg control and swing-leg control. Various simulation and real-world experiments demonstrate its effectiveness against horizontal force disturbances, uneven terrains, heavy and biased payloads, and sim-to-real gap.', 'abstract_zh': '基于腿足运动控制的强化学习在外域条件下的在线解耦框架：隔离支撑腿控制与摆动腿控制以缓解仿真实验与真实环境的差异', 'title_zh': '基于解耦反应力控制与步态控制的动态适应性腿式运动策略'}
{'arxiv_id': 'arXiv:2509.13736', 'title': 'Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning', 'authors': 'Muyuan Ma, Long Cheng, Lijun Han, Xiuze Xia, Houcheng Li', 'link': 'https://arxiv.org/abs/2509.13736', 'abstract': 'Wearable exoskeletons can augment human strength and reduce muscle fatigue during specific tasks. However, developing personalized and task-generalizable assistance algorithms remains a critical challenge. To address this, a meta-imitation learning approach is proposed. This approach leverages a task-specific neural network to predict human elbow joint movements, enabling effective assistance while enhancing generalization to new scenarios. To accelerate data collection, full-body keypoint motions are extracted from publicly available RGB video and motion-capture datasets across multiple tasks, and subsequently retargeted in simulation. Elbow flexion trajectories generated in simulation are then used to train the task-specific neural network within the model-agnostic meta-learning (MAML) framework, which allows the network to rapidly adapt to novel tasks and unseen users with only a few gradient updates. The adapted network outputs personalized references tracked by a gravity-compensated PD controller to ensure stable assistance. Experimental results demonstrate that the exoskeleton significantly reduces both muscle activation and metabolic cost for new users performing untrained tasks, compared to performing without exoskeleton assistance. These findings suggest that the proposed framework effectively improves task generalization and user adaptability for wearable exoskeleton systems.', 'abstract_zh': '可穿戴外骨骼可以在特定任务中增强人类力量并减少肌肉疲劳。然而，开发个性化且适用于多种任务的辅助算法仍是一个关键挑战。为此，提出了一种元模仿学习方法。该方法利用特定任务的神经网络预测人类肘关节运动，从而实现有效辅助并增强对新场景的泛化能力。为加速数据收集，在多个任务中从公开的RGB视频和动作捕捉数据集中提取全身关键点运动，并在模拟中重新目标对齐。模拟中生成的肘关节屈曲轨迹用于在模型无关的元学习（MAML）框架内训练特定任务的神经网络，从而使网络仅通过少量梯度更新就能快速适应新任务和未见过的用户。调整后的网络输出由重力补偿PD控制器追踪的个性化参考轨迹，以确保稳定辅助。实验结果表明，与未使用外骨骼辅助相比，外骨骼显著减少了新用户在执行未训练任务时的肌肉激活和代谢成本。这些发现表明，所提出的方法有效地提高了可穿戴外骨骼系统的任务泛化能力和用户适应性。', 'title_zh': '基于元学习的用户和任务跨适应外骨骼运动调整'}
{'arxiv_id': 'arXiv:2509.13733', 'title': 'FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph', 'authors': 'Xiaolin Zhou, Tingyang Xiao, Liu Liu, Yucheng Wang, Maiyue Chen, Xinrui Meng, Xinjie Wang, Wei Feng, Wei Sui, Zhizhong Su', 'link': 'https://arxiv.org/abs/2509.13733', 'abstract': 'Visual-Language Navigation (VLN) is a fundamental challenge in robotic systems, with broad applications for the deployment of embodied agents in real-world environments. Despite recent advances, existing approaches are limited in long-range spatial reasoning, often exhibiting low success rates and high inference latency, particularly in long-range navigation tasks. To address these limitations, we propose FSR-VLN, a vision-language navigation system that combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation supporting progressive retrieval, from coarse room-level localization to fine-grained goal view and object identification. Building on HMSG, FSR first performs fast matching to efficiently select candidate rooms, views, and objects, then applies VLM-driven refinement for final goal selection. We evaluated FSR-VLN across four comprehensive indoor datasets collected by humanoid robots, utilizing 87 instructions that encompass a diverse range of object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all datasets, measured by the retrieval success rate (RSR), while reducing the response time by 82% compared to VLM-based methods on tour videos by activating slow reasoning only when fast intuition fails. Furthermore, we integrate FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1 humanoid robot, enabling natural language interaction and real-time navigation.', 'abstract_zh': '视觉-语言导航（VLN）是机器人系统中的一个基本挑战，广泛应用于现实环境中嵌入式代理的部署。尽管近期有所进展，现有方法在长距离空间推理方面仍存在局限性，通常成功率低且推理延迟高，尤其是在长距离导航任务中。为解决这些问题，我们提出了FSR-VLN，这是一种结合了层次多模态场景图（HMSG）和快速到慢速导航推理（FSR）的视觉-语言导航系统。HMSG提供了一种支持逐步检索的多模态地图表示，从粗略的房间级定位到精细的目标视角和物体识别。基于HMSG，FSR首先执行快速匹配以高效地选择候选房间、视角和物体，然后采用VLM驱动的细化以进行最终目标选择。我们在四种综合室内数据集中评估了FSR-VLN，这些数据集由类人机器人收集，并涵盖了87条包含多种物体类别的指示。FSR-VLN在所有数据集中均达到了最先进的性能，通过RSR指标衡量，并与基于VLM的方法相比将响应时间降低了82%，仅在快速直觉失败时激活慢速推理。此外，我们将FSR-VLN与Unitree-G1类人机器人上的语音交互、规划和控制模块集成，实现了自然语言交互和实时导航。', 'title_zh': 'FSR-VLN：具有层次多模态场景图的快速和慢速推理视觉-语言导航'}
{'arxiv_id': 'arXiv:2509.13731', 'title': 'Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings', 'authors': 'Jeongwoo Park, Seabin Lee, Changmin Park, Wonjong Lee, Changjoo Nam', 'link': 'https://arxiv.org/abs/2509.13731', 'abstract': 'The industrial insertion of flexible flat cables (FFCs) into receptacles presents a significant challenge owing to the need for submillimeter precision when handling the deformable cables. In manufacturing processes, FFC insertion with robotic manipulators often requires laborious human-guided trajectory generation. While Reinforcement Learning (RL) offers a solution to automate this task without modeling complex properties of FFCs, the nondeterminism caused by the deformability of FFCs requires significant efforts and time on training. Moreover, training directly in a real environment is dangerous as industrial robots move fast and possess no safety measure. We propose an RL algorithm for FFC insertion that leverages a foundation model-based real-to-sim approach to reduce the training time and eliminate the risk of physical damages to robots and surroundings. Training is done entirely in simulation, allowing for random exploration without the risk of physical damages. Sim-to-real transfer is achieved through semantic segmentation masks which leave only those visual features relevant to the insertion tasks such as the geometric and spatial information of the cables and receptacles. To enhance generality, we use a foundation model, Segment Anything Model 2 (SAM2). To eleminate human intervention, we employ a Vision-Language Model (VLM) to automate the initial prompting of SAM2 to find segmentation masks. In the experiments, our method exhibits zero-shot capabilities, which enable direct deployments to real environments without fine-tuning.', 'abstract_zh': '基于基础模型的模拟到现实转换的柔性扁平电缆插入强化学习算法', 'title_zh': '工业环境中柔性电缆插入的强化学习方法'}
{'arxiv_id': 'arXiv:2509.13720', 'title': 'EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility', 'authors': 'Tianle Zeng, Jianwei Peng, Hanjing Ye, Guangcheng Chen, Senzi Luo, Hong Zhang', 'link': 'https://arxiv.org/abs/2509.13720', 'abstract': 'Zero-shot object navigation (ZSON) in large-scale outdoor environments faces many challenges; we specifically address a coupled one: long-range targets that reduce to tiny projections and intermittent visibility due to partial or complete occlusion. We present a unified, lightweight closed-loop system built on an aligned multi-scale image tile hierarchy. Through hierarchical target-saliency fusion, it summarizes localized semantic contrast into a stable coarse-layer regional saliency that provides the target direction and indicates target visibility. This regional saliency supports visibility-aware heading maintenance through keyframe memory, saliency-weighted fusion of historical headings, and active search during temporary invisibility. The system avoids whole-image rescaling, enables deterministic bottom-up aggregation, supports zero-shot navigation, and runs efficiently on a mobile robot. Across simulation and real-world outdoor trials, the system detects semantic targets beyond 150m, maintains a correct heading through visibility changes with 82.6% probability, and improves overall task success by 17.5% compared with the SOTA methods, demonstrating robust ZSON toward distant and intermittently observable targets.', 'abstract_zh': '面向大规模室外环境的零样本物体导航（ZSON）面临众多挑战；我们具体解决了其中一个耦合挑战：由于部分或完全遮挡导致的长距离目标减小为微小投影以及间歇性可见性。我们提出了一种基于对齐多尺度图像瓷砖层次结构的统一轻量级闭环系统。通过层次化目标显著性融合，将局部语义对比总结为稳定的大尺度区域显著性，提供目标方向并指示目标可见性。该区域显著性通过关键帧记忆、显著性加权的历史方向融合以及临时不可见时的主动搜索，支持可见性感知的方向维持。该系统避免了整个图像的缩放，实现了确定性的自底向上聚合，支持零样本导航，并能在移动机器人上高效运行。在仿真和实际室外试验中，该系统检测到超出150米的语义目标，通过可见性变化保持正确的方向概率为82.6%，相比当前先进方法提高整体任务成功率17.5%，展示了对远距离和间歇可见目标的强大零样本导航能力。', 'title_zh': 'EZREAL: 提升恶劣能见度下户外机器人远距离目标导航的零样本能力'}
{'arxiv_id': 'arXiv:2509.13692', 'title': 'HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion', 'authors': 'Yadan Zeng, Jiadong Zhou, Xiaohan Li, I-Ming Chen', 'link': 'https://arxiv.org/abs/2509.13692', 'abstract': 'Point cloud completion is essential for robotic perception, object reconstruction and supporting downstream tasks like grasp planning, obstacle avoidance, and manipulation. However, incomplete geometry caused by self-occlusion and sensor limitations can significantly degrade downstream reasoning and interaction. To address these challenges, we propose HGACNet, a novel framework that reconstructs complete point clouds of individual objects by hierarchically encoding 3D geometric features and fusing them with image-guided priors from a single-view RGB image. At the core of our approach, the Hierarchical Graph Attention (HGA) encoder adaptively selects critical local points through graph attention-based downsampling and progressively refines hierarchical geometric features to better capture structural continuity and spatial relationships. To strengthen cross-modal interaction, we further design a Multi-Scale Cross-Modal Fusion (MSCF) module that performs attention-based feature alignment between hierarchical geometric features and structured visual representations, enabling fine-grained semantic guidance for completion. In addition, we proposed the contrastive loss (C-Loss) to explicitly align the feature distributions across modalities, improving completion fidelity under modality discrepancy. Finally, extensive experiments conducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset confirm the effectiveness of HGACNet, demonstrating state-of-the-art performance as well as strong applicability in real-world robotic manipulation tasks.', 'abstract_zh': '点云完成对于机器人感知、对象重建以及抓取规划、障碍物避让和操作等下游任务至关重要。然而，由自我遮挡和传感器限制引起的不完整几何结构会严重降低下游推理和交互的性能。为应对这些挑战，我们提出了一种新的框架HGACNet，通过分层编码3D几何特征并结合单视角RGB图像的图像引导先验信息来重建个体对象的完整点云。我们的方法的核心是分层图注意力（HGA）编码器，它通过图注意力机制下的下采样自适应地选择关键局部点，并逐步细化分层几何特征以更好地捕捉结构连续性和空间关系。为了增强跨模态交互，我们设计了一个多尺度跨模态融合（MSCF）模块，在分层几何特征和结构化视觉表示之间进行注意力辅助特征对齐，从而为完成任务提供精细的语义指导。此外，我们提出了对比损失（C-Loss）以明确对齐不同模态下的特征分布，提高模态差异下的完成精度。最后，通过ShapeNet-ViPC基准和YCB-Complete数据集上的广泛实验验证了HGACNet的有效性，展示了其在机器人操作任务中的先进性能和强大适用性。', 'title_zh': 'HGACNet：分层图注意网络在跨模态点云完成中的应用'}
{'arxiv_id': 'arXiv:2509.13691', 'title': 'SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics', 'authors': 'Songhao Huang, Yuwei Wu, Guangyao Shi, Gaurav S. Sukhatme, Vijay Kumar', 'link': 'https://arxiv.org/abs/2509.13691', 'abstract': 'We investigate the problem of automatic domain generation for the Planning Domain Definition Language (PDDL) using Large Language Models (LLMs), with a particular focus on unmanned aerial vehicle (UAV) tasks. Although PDDL is a widely adopted standard in robotic planning, manually designing domains for diverse applications such as surveillance, delivery, and inspection is labor-intensive and error-prone, which hinders adoption and real-world deployment. To address these challenges, we propose SPAR, a framework that leverages the generative capabilities of LLMs to automatically produce valid, diverse, and semantically accurate PDDL domains from natural language input. To this end, we first introduce a systematically formulated and validated UAV planning dataset, consisting of ground-truth PDDL domains and associated problems, each paired with detailed domain and action descriptions. Building on this dataset, we design a prompting framework that generates high-quality PDDL domains from language input. The generated domains are evaluated through syntax validation, executability, feasibility, and interpretability. Overall, this work demonstrates that LLMs can substantially accelerate the creation of complex planning domains, providing a reproducible dataset and evaluation pipeline that enables application experts without prior experience to leverage it for practical tasks and advance future research in aerial robotics and automated planning.', 'abstract_zh': '使用大型语言模型自动生成Planning Domain Definition Language (PDDL)领域：面向无人驾驶航空器任务的研究', 'title_zh': 'SPAR: 基于大规模语言模型的可扩展PDDL领域生成方法在无人机机器人中的应用'}
{'arxiv_id': 'arXiv:2509.13666', 'title': 'DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring', 'authors': 'Zhenqi Wu, Abhinav Modi, Angelos Mavrogiannis, Kaustubh Joshi, Nikhil Chopra, Yiannis Aloimonos, Nare Karapetyan, Ioannis Rekleitis, Xiaomin Lin', 'link': 'https://arxiv.org/abs/2509.13666', 'abstract': 'The ocean is warming and acidifying, increasing the risk of mass mortality events for temperature-sensitive shellfish such as oysters. This motivates the development of long-term monitoring systems. However, human labor is costly and long-duration underwater work is highly hazardous, thus favoring robotic solutions as a safer and more efficient option. To enable underwater robots to make real-time, environment-aware decisions without human intervention, we must equip them with an intelligent "brain." This highlights the need for persistent,wide-area, and low-cost benthic monitoring. To this end, we present DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term underwater exploration and habitat monitoring. The results show that our framework is highly efficient in finding and exploring target objects (e.g., oysters, shipwrecks) without prior location information. In the oyster-monitoring task, our framework takes 31.5% less time than the previous baseline with the same amount of oysters. Compared to the vanilla VLM, it uses 23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our framework successfully explores and maps the wreck without collisions, requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage, while the vanilla model achieves 60.23% average coverage in our shipwreck environments.', 'abstract_zh': '海洋正在变暖和酸化，增加了对温度敏感的壳类生物如牡蛎大规模死亡事件的风险。这促进了长时间监测系统的开发。然而，人类劳动力成本高且长时间水下工作风险极高，因此机器人解决方案更安全且效率更高。为了使水下机器人能够在没有人类干预的情况下进行实时、环境感知的决策，我们需要为它们配备一个智能化的“大脑”。这突显了持久、广域和低成本底栖监测的必要性。为此，我们提出DREAM，一种由视觉语言模型（VLM）引导的长期水下探索和栖息地监测自主框架。结果显示，我们框架在没有先验位置信息的情况下能够高效地寻找和探索目标物体（如牡蛎、沉船）。在牡蛎监测任务中，与之前的基线相比，我们的框架在相同数量的牡蛎情况下节省了31.5%的时间。与标准VLM相比，它使用了23% fewer步骤，却覆盖了8.88%更多的牡蛎。在沉船场景中，我们的框架成功地探索和映射了沉船，且未发生碰撞，所需的步骤比标准模型少了27.5%，并且实现了100%的覆盖，而在我们的沉船环境中，标准模型的平均覆盖率为60.23%。', 'title_zh': 'DREAM: 具有领域意识的推理以实现高效自主水下监测'}
{'arxiv_id': 'arXiv:2509.13649', 'title': 'Barometer-Aided Attitude Estimation', 'authors': 'Méloné Nyoba Tchonkeu, Soulaimane Berkane, Tarek Hamel', 'link': 'https://arxiv.org/abs/2509.13649', 'abstract': 'Accurate and robust attitude estimation is a central challenge for autonomous vehicles operating in GNSS-denied or highly dynamic environments. In such cases, Inertial Measurement Units (IMUs) alone are insufficient for reliable tilt estimation due to the ambiguity between gravitational and inertial accelerations. While auxiliary velocity sensors, such as GNSS, Pitot tubes, Doppler radar, or visual odometry, are often used, they can be unavailable, intermittent, or costly. This work introduces a barometer-aided attitude estimation architecture that leverages barometric altitude measurements to infer vertical velocity and attitude within a nonlinear observer on SO(3). The design cascades a deterministic Riccati observer with a complementary filter, ensuring Almost Global Asymptotic Stability (AGAS) under a uniform observability condition while maintaining geometric consistency. The analysis highlights barometer-aided estimation as a lightweight and effective complementary modality.', 'abstract_zh': '自主车辆在GNSS-denied或高动态环境下精确且鲁棒的姿态估计是一个核心挑战。为此，本工作提出了一种气压辅助姿态估计架构，利用气压高度测量来推断垂直速度和姿态，在SO(3)上的非线性观测器中进行估计。该设计采用定性的里卡提观测器与互补滤波器级联，确保在均匀可观测性条件下几乎全域渐近稳定（AGAS），同时保持几何一致性。分析表明，气压辅助估计是一种轻量且有效的补充模态。', 'title_zh': '气压助力姿态估计'}
{'arxiv_id': 'arXiv:2509.13595', 'title': 'Leg-Arm Coordinated Operation for Curtain Wall Installation', 'authors': 'Xiao Liu, Weijun Wang, Tianlun Huang, Zhiyong Wang, Wei Feng', 'link': 'https://arxiv.org/abs/2509.13595', 'abstract': 'With the acceleration of urbanization, the number of high-rise buildings and large public facilities is increasing, making curtain walls an essential component of modern architecture with widespread applications. Traditional curtain wall installation methods face challenges such as variable on-site terrain, high labor intensity, low construction efficiency, and significant safety risks. Large panels often require multiple workers to complete installation. To address these issues, based on a hexapod curtain wall installation robot, we design a hierarchical optimization-based whole-body control framework for coordinated arm-leg planning tailored to three key tasks: wall installation, ceiling installation, and floor laying. This framework integrates the motion of the hexapod legs with the operation of the folding arm and the serial-parallel manipulator. We conduct experiments on the hexapod curtain wall installation robot to validate the proposed control method, demonstrating its capability in performing curtain wall installation tasks. Our results confirm the effectiveness of the hierarchical optimization-based arm-leg coordination framework for the hexapod robot, laying the foundation for its further application in complex construction site environments.', 'abstract_zh': '随着城市化的加速，高层建筑和大型公共设施的数量增加，使幕墙成为现代建筑中广泛应用的重要组成部分。传统的幕墙安装方法面临现场地形变化、高劳动强度、低施工效率和显著的安全风险等挑战。大型面板的安装往往需要多名工人共同完成。为解决这些问题，基于六足幕墙安装机器人，我们设计了一种基于分层优化的整体运动控制框架，适用于墙面安装、天花板安装和楼面铺设三大关键任务的协调臂腿规划。该框架将六足机器人腿的运动与折叠臂和串联并联 manipulator 的操作集成在一起。我们对六足幕墙安装机器人进行了实验，验证了所提出控制方法的有效性，展示了其在执行幕墙安装任务方面的能力。实验结果证实了基于分层优化的臂腿协调控制框架在六足机器人中的有效性，为其实现复杂施工环境应用奠定了基础。', 'title_zh': '幕wall安装的腿臂协调操作'}
{'arxiv_id': 'arXiv:2509.13591', 'title': 'Object Pose Estimation through Dexterous Touch', 'authors': 'Amir-Hossein Shahidzadeh, Jiyue Zhu, Kezhou Chen, Sha Yi, Cornelia Fermüller, Yiannis Aloimonos, Xiaolong Wang', 'link': 'https://arxiv.org/abs/2509.13591', 'abstract': "Robust object pose estimation is essential for manipulation and interaction tasks in robotics, particularly in scenarios where visual data is limited or sensitive to lighting, occlusions, and appearances. Tactile sensors often offer limited and local contact information, making it challenging to reconstruct the pose from partial data. Our approach uses sensorimotor exploration to actively control a robot hand to interact with the object. We train with Reinforcement Learning (RL) to explore and collect tactile data. The collected 3D point clouds are used to iteratively refine the object's shape and pose. In our setup, one hand holds the object steady while the other performs active exploration. We show that our method can actively explore an object's surface to identify critical pose features without prior knowledge of the object's geometry. Supplementary material and more demonstrations will be provided at this https URL .", 'abstract_zh': '鲁棒的对象姿态估计对于机器人中的操作和交互任务至关重要，特别是在视觉数据受限或对光照、遮挡和外观敏感的情景中。触觉传感器通常只能提供有限的局部接触信息，这使得从部分数据重建姿态变得具有挑战性。我们的方法通过传感器运动探索主动控制机器人手与物体互动。我们使用强化学习（RL）进行探索并收集触觉数据。收集到的3D点云用于迭代细化物体的形状和姿态。在我们的设置中，一只手稳持物体，另一只手进行主动探索。我们展示了我们的方法可以在没有物体几何先验知识的情况下，主动探索物体表面以识别关键的姿态特征。更多信息和演示将在以下网址提供：this https URL。', 'title_zh': '通过灵巧触觉进行物体姿态估计'}
{'arxiv_id': 'arXiv:2509.13579', 'title': 'TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning', 'authors': 'Momchil S. Tomov, Sang Uk Lee, Hansford Hendrago, Jinwook Huh, Teawon Han, Forbes Howington, Rafael da Silva, Gianmarco Bernasconi, Marc Heim, Samuel Findler, Xiaonan Ji, Alexander Boule, Michael Napoli, Kuo Chen, Jesse Miller, Boaz Floor, Yunqing Hu', 'link': 'https://arxiv.org/abs/2509.13579', 'abstract': 'We present TreeIRL, a novel planner for autonomous driving that combines Monte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to achieve state-of-the-art performance in simulation and in real-world driving. The core idea is to use MCTS to find a promising set of safe candidate trajectories and a deep IRL scoring function to select the most human-like among them. We evaluate TreeIRL against both classical and state-of-the-art planners in large-scale simulations and on 500+ miles of real-world autonomous driving in the Las Vegas metropolitan area. Test scenarios include dense urban traffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves the best overall performance, striking a balance between safety, progress, comfort, and human-likeness. To our knowledge, our work is the first demonstration of MCTS-based planning on public roads and underscores the importance of evaluating planners across a diverse set of metrics and in real-world environments. TreeIRL is highly extensible and could be further improved with reinforcement learning and imitation learning, providing a framework for exploring different combinations of classical and learning-based approaches to solve the planning bottleneck in autonomous driving.', 'abstract_zh': 'TreeIRL：一种结合蒙特卡洛树搜索和逆强化学习的新型自主驾驶规划器', 'title_zh': 'TreeIRL: 基于树搜索和逆强化学习的安全城市驾驶'}
{'arxiv_id': 'arXiv:2509.13574', 'title': 'Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic Policies: Mitigating Multi-Step Inference Degradation', 'authors': 'Zidong Chen, Zihao Guo, Peng Wang, ThankGod Itua Egbe, Yan Lyu, Chenghao Qian', 'link': 'https://arxiv.org/abs/2509.13574', 'abstract': 'Flow matching has emerged as a competitive framework for learning high-quality generative policies in robotics; however, we find that generalisation arises and saturates early along the flow trajectory, in accordance with recent findings in the literature. We further observe that increasing the number of Euler integration steps during inference counter-intuitively and universally degrades policy performance. We attribute this to (i) additional, uniformly spaced integration steps oversample the late-time region, thereby constraining actions towards the training trajectories and reducing generalisation; and (ii) the learned velocity field becoming non-Lipschitz as integration time approaches 1, causing instability. To address these issues, we propose a novel policy that utilises non-uniform time scheduling (e.g., U-shaped) during training, which emphasises both early and late temporal stages to regularise policy training, and a dense-jump integration schedule at inference, which uses a single-step integration to replace the multi-step integration beyond a jump point, to avoid unstable areas around 1. Essentially, our policy is an efficient one-step learner that still pushes forward performance through multi-step integration, yielding up to 23.7% performance gains over state-of-the-art baselines across diverse robotic tasks.', 'abstract_zh': '流匹配作为一种在机器人领域学习高质量生成策略的竞争力框架已崭露头角；然而，我们发现泛化在流轨迹早期阶段就出现并达到饱和，这与最近文献中的发现一致。我们在推断过程中进一步观察到，增加欧拉积分步骤非但没有提升，反而普遍降低了策略性能。我们将其归因于：(i) 额外的均匀间隔积分步骤过度采样晚期区域，从而限制行动范围，使之朝着训练轨迹收敛，减少了泛化能力；(ii) 随着积分时间接近1，学习到的速度场变得非Lipschitz，导致不稳定性。为解决这些问题，我们提出一种新型策略，在训练过程中采用非均匀时间调度（例如U形），强调早期和晚期的时间阶段来规范策略培训，并在推断过程中采用密集跳跃积分安排，通过在跳跃点之后使用单步积分来替代多步积分，以避免1附近不稳定的区域。本质上，我们的策略是高效的单步学习者，但仍通过多步积分推动性能提升，与当前最先进的基线相比，在多种机器人任务中取得了高达23.7%的性能提升。', 'title_zh': '基于非均匀时间调度的密集-跳跃流匹配：缓解多步推理退化'}
{'arxiv_id': 'arXiv:2509.13572', 'title': 'Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference', 'authors': 'Ozan Karaali, Hossam Farag, Strahinja Dosen, Cedomir Stefanovic', 'link': 'https://arxiv.org/abs/2509.13572', 'abstract': 'This study examines the potential of utilizing Vision Language Models (VLMs) to improve the perceptual capabilities of semi-autonomous prosthetic hands. We introduce a unified benchmark for end-to-end perception and grasp inference, evaluating a single VLM to perform tasks that traditionally require complex pipelines with separate modules for object detection, pose estimation, and grasp planning. To establish the feasibility and current limitations of this approach, we benchmark eight contemporary VLMs on their ability to perform a unified task essential for bionic grasping. From a single static image, they should (1) identify common objects and their key properties (name, shape, orientation, and dimensions), and (2) infer appropriate grasp parameters (grasp type, wrist rotation, hand aperture, and number of fingers). A corresponding prompt requesting a structured JSON output was employed with a dataset of 34 snapshots of common objects. Key performance metrics, including accuracy for categorical attributes (e.g., object name, shape) and errors in numerical estimates (e.g., dimensions, hand aperture), along with latency and cost, were analyzed. The results demonstrated that most models exhibited high performance in object identification and shape recognition, while accuracy in estimating dimensions and inferring optimal grasp parameters, particularly hand rotation and aperture, varied more significantly. This work highlights the current capabilities and limitations of VLMs as advanced perceptual modules for semi-autonomous control of bionic limbs, demonstrating their potential for effective prosthetic applications.', 'abstract_zh': '本研究探讨了利用视觉语言模型（VLMs）提高半自主假手感知能力的潜力。我们引入了一个统一的基准，用于评估单一VLM进行传统的复杂流水线所要求的任务，该流水线由物体检测、姿态估计和抓取规划等多个模块组成。为了验证该方法的可行性和当前的局限性，我们在34个常见物体的快照数据集上对八种当代VLM进行基准测试，评估它们执行一个对仿生抓取至关重要的统一任务的能力。从单一静态图像中，它们应（1）识别常见物体及其关键属性（名称、形状、朝向和尺寸），以及（2）推断合适的抓取参数（抓取类型、腕部旋转、手指张开度和手指数量）。采用请求结构化JSON输出的提示进行评估。关键性能指标包括类别属性的准确性（如对象名称、形状）和数值估计的误差（如尺寸、手指张开度），以及延迟和成本。研究结果表明，大多数模型在物体识别和形状识别方面表现出了高效率，但在估计尺寸和推断最优抓取参数的准确性上，尤其是在手部旋转和张开度方面，差异较大。本工作突显了VLMs作为半自主控制仿生肢体高级感知模块的当前能力和局限性，展示了其在假肢应用中的潜在有效性。', 'title_zh': '使用视觉语言模型控制仿生手：物体感知和抓取推理评估'}
{'arxiv_id': 'arXiv:2509.13541', 'title': 'Semantic 3D Reconstructions with SLAM for Central Airway Obstruction', 'authors': "Ayberk Acar, Fangjie Li, Hao Li, Lidia Al-Zogbi, Kanyifeechukwu Jane Oguine, Susheela Sharma Stern, Jesse F. d'Almeida, Robert J. Webster III, Ipek Oguz, Jie Ying Wu", 'link': 'https://arxiv.org/abs/2509.13541', 'abstract': 'Central airway obstruction (CAO) is a life-threatening condition with increasing incidence, caused by tumors in and outside of the airway. Traditional treatment methods such as bronchoscopy and electrocautery can be used to remove the tumor completely; however, these methods carry a high risk of complications. Recent advances allow robotic interventions with lesser risk. The combination of robot interventions with scene understanding and mapping also opens up the possibilities for automation. We present a novel pipeline that enables real-time, semantically informed 3D reconstructions of the central airway using monocular endoscopic video.\nOur approach combines DROID-SLAM with a segmentation model trained to identify obstructive tissues. The SLAM module reconstructs the 3D geometry of the airway in real time, while the segmentation masks guide the annotation of obstruction regions within the reconstructed point cloud. To validate our pipeline, we evaluate the reconstruction quality using ex vivo models.\nQualitative and quantitative results show high similarity between ground truth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By integrating segmentation directly into the SLAM workflow, our system produces annotated 3D maps that highlight clinically relevant regions in real time. High-speed capabilities of the pipeline allows quicker reconstructions compared to previous work, reflecting the surgical scene more accurately.\nTo the best of our knowledge, this is the first work to integrate semantic segmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our framework is modular and can generalize to other anatomies or procedures with minimal changes, offering a promising step toward autonomous robotic interventions.', 'abstract_zh': '基于单目内窥镜视频的实时语义驱动中央气道三维重建方法', 'title_zh': '基于SLAM的中央气道阻塞的语义3D重建'}
{'arxiv_id': 'arXiv:2509.13534', 'title': 'Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning', 'authors': 'Chunxin Zheng, Kai Chen, Zhihai Bi, Yulin Li, Liang Pan, Jinni Zhou, Haoang Li, Jun Ma', 'link': 'https://arxiv.org/abs/2509.13534', 'abstract': 'Whole-body manipulation (WBM) for humanoid robots presents a promising approach for executing embracing tasks involving bulky objects, where traditional grasping relying on end-effectors only remains limited in such scenarios due to inherent stability and payload constraints. This paper introduces a reinforcement learning framework that integrates a pre-trained human motion prior with a neural signed distance field (NSDF) representation to achieve robust whole-body embracing. Our method leverages a teacher-student architecture to distill large-scale human motion data, generating kinematically natural and physically feasible whole-body motion patterns. This facilitates coordinated control across the arms and torso, enabling stable multi-contact interactions that enhance the robustness in manipulation and also the load capacity. The embedded NSDF further provides accurate and continuous geometric perception, improving contact awareness throughout long-horizon tasks. We thoroughly evaluate the approach through comprehensive simulations and real-world experiments. The results demonstrate improved adaptability to diverse shapes and sizes of objects and also successful sim-to-real transfer. These indicate that the proposed framework offers an effective and practical solution for multi-contact and long-horizon WBM tasks of humanoid robots.', 'abstract_zh': '全身心 manip 操作（Whole-body Manipulation, WBM）中的人形机器人拥抱任务：基于预训练人类运动先验与神经-signed距离场表示的强化学习框架', 'title_zh': '拥抱 bulky 物体的人形机器人：基于强化学习的全身 manipulation'}
{'arxiv_id': 'arXiv:2509.13501', 'title': 'Trajectory Tracking with Reachability-Guided Quadratic Programming and Freeze-Resume', 'authors': 'Hossein Gholampour, Logan E. Beaver', 'link': 'https://arxiv.org/abs/2509.13501', 'abstract': "Many robotic systems must follow planned paths yet pause safely and resume when people or objects intervene. We present an output-space method for systems whose tracked output can be feedback-linearized to a double integrator (e.g., manipulators). The approach has two parts. Offline, we perform a pre-run reachability check to verify that the motion plan respects speed and acceleration magnitude limits. Online, we apply a quadratic program to track the motion plan under the same limits. We use a one-step reachability test to bound the maximum disturbance the system is capable of rejecting. When the state coincides with the reference path we recover perfect tracking in the deterministic case, and we correct errors using a KKT-inspired weight. We demonstrate that safety stops and unplanned deviations are handled efficiently, and the system returns to the motion plan without replanning. We demonstrate our system's improved performance over pure pursuit in simulation.", 'abstract_zh': '许多机器人系统需沿规划路径行进，并在遇到人员或物体时安全暂停并在后续恢复。我们提出了一种输出空间方法，适用于其跟踪输出可反馈线性化为双积分器（例如， manipulator）的系统。该方法分为两个部分。在离线阶段，我们执行预运行可达性检查以验证运动计划是否满足速度和加速度幅度限制。在在线阶段，我们应用二次规划来在相同的限制下跟踪运动计划。我们使用一步可达性测试来限制系统能够拒绝的最大干扰。当状态与参考路径重合时，在确定性情况下可恢复完美跟踪，并使用KKT启发式的权重来纠正错误。我们证明了该方法高效地处理了安全停止和未计划的偏离，并且系统能无需重新规划即可返回至运动计划。我们通过仿真演示了该系统在纯追求算法上的改进性能。', 'title_zh': '轨迹跟踪与可达性引导的二次规划及冻结-恢复方法'}
{'arxiv_id': 'arXiv:2509.13434', 'title': 'A Convex Formulation of Compliant Contact between Filaments and Rigid Bodies', 'authors': 'Wei-Chen Li, Glen Chou', 'link': 'https://arxiv.org/abs/2509.13434', 'abstract': 'We present a computational framework for simulating filaments interacting with rigid bodies through contact. Filaments are challenging to simulate due to their codimensionality, i.e., they are one-dimensional structures embedded in three-dimensional space. Existing methods often assume that filaments remain permanently attached to rigid bodies. Our framework unifies discrete elastic rod (DER) modeling, a pressure field patch contact model, and a convex contact formulation to accurately simulate frictional interactions between slender filaments and rigid bodies - capabilities not previously achievable. Owing to the convex formulation of contact, each time step can be solved to global optimality, guaranteeing complementarity between contact velocity and impulse. We validate the framework by assessing the accuracy of frictional forces and comparing its physical fidelity against baseline methods. Finally, we demonstrate its applicability in both soft robotics, such as a stochastic filament-based gripper, and deformable object manipulation, such as shoelace tying, providing a versatile simulator for systems involving complex filament-filament and filament-rigid body interactions.', 'abstract_zh': '一种用于模拟纤丝与刚体通过接触相互作用的计算框架', 'title_zh': '纤索单元与刚体柔顺接触的凸规划表述'}
{'arxiv_id': 'arXiv:2509.13386', 'title': 'VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization', 'authors': 'Hansol Lim, Minhyeok Im, Jonathan Boyack, Jee Won Lee, Jongseong Brad Choi', 'link': 'https://arxiv.org/abs/2509.13386', 'abstract': "Demands for software-defined vehicles (SDV) are rising and electric vehicles (EVs) are increasingly being equipped with powerful computers. This enables onboard AI systems to optimize charge-aware path optimization customized to reflect vehicle's current condition and environment. We present VEGA, a charge-aware EV navigation agent that plans over a charger-annotated road graph using Proximal Policy Optimization (PPO) with budgeted A* teacher-student guidance under state-of-charge (SoC) feasibility. VEGA consists of two modules. First, a physics-informed neural operator (PINO), trained on real vehicle speed and battery-power logs, uses recent vehicle speed logs to estimate aerodynamic drag, rolling resistance, mass, motor and regenerative-braking efficiencies, and auxiliary load by learning a vehicle-custom dynamics. Second, a Reinforcement Learning (RL) agent uses these dynamics to optimize a path with optimal charging stops and dwell times under SoC constraints. VEGA requires no additional sensors and uses only vehicle speed signals. It may serve as a virtual sensor for power and efficiency to potentially reduce EV cost. In evaluation on long routes like San Francisco to New York, VEGA's stops, dwell times, SoC management, and total travel time closely track Tesla Trip Planner while being slightly more conservative, presumably due to real vehicle conditions such as vehicle parameter drift due to deterioration. Although trained only in U.S. regions, VEGA was able to compute optimal charge-aware paths in France and Japan, demonstrating generalizability. It achieves practical integration of physics-informed learning and RL for EV eco-routing.", 'abstract_zh': '基于充电意识的软件定义车辆导航代理VEGA：结合proximal策略优化与预算A*教师-学生指导的电动车辆路径优化', 'title_zh': 'VEGA：基于物理知情神经算子和近端策略优化的电动汽车导航代理'}
{'arxiv_id': 'arXiv:2509.13381', 'title': 'Cooperative Target Detection with AUVs: A Dual-Timescale Hierarchical MARDL Approach', 'authors': 'Zhang Xueyao, Yang Bo, Yu Zhiwen, Cao Xuelin, George C. Alexandropoulos, Merouane Debbah, Chau Yuen', 'link': 'https://arxiv.org/abs/2509.13381', 'abstract': 'Autonomous Underwater Vehicles (AUVs) have shown great potential for cooperative detection and reconnaissance. However, collaborative AUV communications introduce risks of exposure. In adversarial environments, achieving efficient collaboration while ensuring covert operations becomes a key challenge for underwater cooperative missions. In this paper, we propose a novel dual time-scale Hierarchical Multi-Agent Proximal Policy Optimization (H-MAPPO) framework. The high-level component determines the individuals participating in the task based on a central AUV, while the low-level component reduces exposure probabilities through power and trajectory control by the participating AUVs. Simulation results show that the proposed framework achieves rapid convergence, outperforms benchmark algorithms in terms of performance, and maximizes long-term cooperative efficiency while ensuring covert operations.', 'abstract_zh': '自主水下车辆（AUVs）在协同探测和侦察方面展现了巨大的潜力。然而，协作的AUV通信增加了暴露的风险。在对抗环境中，实现高效的协同工作同时确保隐蔽操作成为水下协同任务的关键挑战。本文提出了一种新颖的双时间尺度层次多代理接近策略优化（H-MAPPO）框架。高层次组件基于中央AUV确定参与任务的个体，而低层次组件通过参与AUVs的功率和轨迹控制来降低暴露概率。仿真结果表明，所提出框架实现了快速收敛，性能优于基准算法，同时最大化长期协同效率并确保隐蔽操作。', 'title_zh': '基于AUV的协同目标检测：一种双时间尺度分层MARDL方法'}
{'arxiv_id': 'arXiv:2509.13380', 'title': 'ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy', 'authors': 'Alejandro D. Mousist', 'link': 'https://arxiv.org/abs/2509.13380', 'abstract': 'This paper presents ASTREA, the first agentic system deployed on flight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using thermal control as a representative use case, we integrate a resource-constrained Large Language Model (LLM) agent with a reinforcement learning controller in an asynchronous architecture tailored for space-qualified platforms. Ground experiments show that LLM-guided supervision improves thermal stability and reduces violations, confirming the feasibility of combining semantic reasoning with adaptive control under hardware constraints. However, on-orbit validation aboard the International Space Station (ISS) reveals performance degradation caused by inference latency mismatched with the rapid thermal cycles characteristic of Low Earth Orbit (LEO) satellites. These results highlight both the opportunities and current limitations of agentic LLM-based systems in real flight environments, providing practical design guidelines for future space autonomy.', 'abstract_zh': 'ASTREA：部署在飞行 heritage 硬件上的首个自主航天器操作智能系统', 'title_zh': 'ASTREA: 引入智能自主orbital热管理技术'}
{'arxiv_id': 'arXiv:2509.13378', 'title': 'Using role-play and Hierarchical Task Analysis for designing human-robot interaction', 'authors': 'Mattias Wingren, Sören Andersson, Sara Rosenberg, Malin Andtfolk, Susanne Hägglund, Prashani Jayasingha Arachchige, Linda Nyholm', 'link': 'https://arxiv.org/abs/2509.13378', 'abstract': "We present the use of two methods we believe warrant more use than they currently have in the field of human-robot interaction: role-play and Hierarchical Task Analysis. Some of its potential is showcased through our use of them in an ongoing research project which entails developing a robot application meant to assist at a community pharmacy. The two methods have provided us with several advantages. The role-playing provided a controlled and adjustable environment for understanding the customers' needs where pharmacists could act as models for the robot's behavior; and the Hierarchical Task Analysis ensured the behavior displayed was modelled correctly and aided development through facilitating co-design. Future research could focus on developing task analysis methods especially suited for social robot interaction.", 'abstract_zh': '我们介绍了在人机交互领域应用两种我们认为应更多使用的 методs：角色扮演和层次任务分析。通过在一项旨在辅助社区药房的机器人应用开发的研究项目中使用这两种方法，展示了它们的一些潜在价值。这两种方法为我们的研究提供了多个优势。角色扮演提供了一个可控且可调的环境，使药剂师能够作为机器人行为的模型，帮助理解客户的需求；而层次任务分析确保了机器人行为的正确建模，并通过促进协同设计来辅助开发。未来的研究可以重点关注开发特别适用于社会机器人交互的任务分析方法。', 'title_zh': '基于角色扮演和层级任务分析的设计人类-机器人交互方法'}
{'arxiv_id': 'arXiv:2509.13349', 'title': 'Label-Efficient Grasp Joint Prediction with Point-JEPA', 'authors': 'Jed Guzelkabaagac, Boris Petrović', 'link': 'https://arxiv.org/abs/2509.13349', 'abstract': 'We investigate whether 3D self-supervised pretraining with a Joint-Embedding Predictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle prediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained Point-JEPA encoder, we train a lightweight multi-hypothesis head with winner-takes-all and evaluate by top-logit selection. On DLR-Hand II with object-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes and reaches parity with full supervision. These results suggest JEPA-style pretraining is a practical approach for data-efficient grasp learning.', 'abstract_zh': '基于Joint-Embedding Predictive Architecture的3D自监督预训练是否能实现标签高效的手抓取关节角预测', 'title_zh': '标签高效指尖抓取联合预测ewith点-JEPA'}
{'arxiv_id': 'arXiv:2509.13342', 'title': 'Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments', 'authors': 'Isaac Ronald Ward', 'link': 'https://arxiv.org/abs/2509.13342', 'abstract': "In this work, an existing deep neural network approach for determining a robot's pose from visual information (RGB images) is modified, improving its localization performance without impacting its ease of training. Explicitly, the network's loss function is extended in a manner which intuitively combines the positional and rotational error in order to increase robustness to perceptual aliasing. An improvement in the localization accuracy for indoor scenes is observed: with decreases of up to 9.64% and 2.99% in the median positional and rotational error respectively, when compared to the unmodified network.\nAdditionally, photogrammetry data is used to produce a pose-labelled dataset which allows the above model to be trained on a local environment, resulting in localization accuracies of 0.11m & 0.89 degrees. This trained model forms the basis of a navigation algorithm, which is tested in real-time on a TurtleBot (a wheeled robotic device). As such, this work introduces a full pipeline for creating a robust navigational algorithm for any given real world indoor scene; the only requirement being a collection of images from the scene, which can be captured in as little as 330 seconds of", 'abstract_zh': '基于视觉信息改进的机器人姿态识别方法及其在室内导航中的应用', 'title_zh': '基于仿真实现的深度神经网络的现实世界机器人探索'}
{'arxiv_id': 'arXiv:2509.13336', 'title': 'Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning', 'authors': 'Mehran Behjati, Rosdiadee Nordin, Nor Fadzilah Abdullah', 'link': 'https://arxiv.org/abs/2509.13336', 'abstract': 'This paper presents a reinforcement learning (RL) based approach for path planning of cellular connected unmanned aerial vehicles (UAVs) operating beyond visual line of sight (BVLoS). The objective is to minimize travel distance while maximizing the quality of cellular link connectivity by considering real world aerial coverage constraints and employing an empirical aerial channel model. The proposed solution employs RL techniques to train an agent, using the quality of communication links between the UAV and base stations (BSs) as the reward function. Simulation results demonstrate the effectiveness of the proposed method in training the agent and generating feasible UAV path plans. The proposed approach addresses the challenges due to limitations in UAV cellular communications, highlighting the need for investigations and considerations in this area. The RL algorithm efficiently identifies optimal paths, ensuring maximum connectivity with ground BSs to ensure safe and reliable BVLoS flight operation. Moreover, the solution can be deployed as an offline path planning module that can be integrated into future ground control systems (GCS) for UAV operations, enhancing their capabilities and safety. The method holds potential for complex long range UAV applications, advancing the technology in the field of cellular connected UAV path planning.', 'abstract_zh': '基于强化学习的越视距操作蜂窝连接无人机路径规划方法', 'title_zh': '基于 reinforcement learning 的 BVLoS 航迹规划以最大化 UAV 无线蜂窝连接性'}
{'arxiv_id': 'arXiv:2509.13605', 'title': 'A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms', 'authors': 'Ruochen Hou, Gabriel I. Fernandez, Alex Xu, Dennis W. Hong', 'link': 'https://arxiv.org/abs/2509.13605', 'abstract': 'In previous work, we introduced a 2D localization algorithm called CLAP, Clustering to Localize Across $n$ Possibilities, which was used during our championship win in RoboCup 2024, an international autonomous humanoid soccer competition. CLAP is particularly recognized for its robustness against outliers, where clustering is employed to suppress noise and mitigate against erroneous feature matches. This clustering-based strategy provides an alternative to traditional outlier rejection schemes such as RANSAC, in which candidates are validated by reprojection error across all data points. In this paper, CLAP is extended to a more general framework beyond 2D localization, specifically to 3D localization and image stitching. We also show how CLAP, RANSAC, and Hough transforms are related. The generalization of CLAP is widely applicable to many different fields and can be a useful tool to deal with noise and uncertainty.', 'abstract_zh': '先前工作中，我们提出了一种名为CLAP的2D定位算法，即Clustering to Localize Across $n$ Possibilities，在2024年国际自主 humanoid 足球锦标赛RoboCup中获得冠军。CLAP特别因其对离群值的鲁棒性而受到认可，其中聚类方法被用于抑制噪声并减轻错误特征匹配的影响。基于聚类的策略为传统的离群值剔除方案（如RANSAC）提供了一种替代方案，RANSAC方案通过重现投影误差来验证候选者。在本文中，我们将CLAP扩展到了更通用的框架中，具体应用到了3D定位和图像拼接。我们还展示了CLAP、RANSAC和霍夫变换之间的关系。CLAP的一般化方法在许多不同的领域都有广泛应用，并可作为应对噪声和不确定性的一种有用工具。', 'title_zh': 'CLAP从三维定位的一般化到图像处理与RANSAC及霍夫变换的联系'}
{'arxiv_id': 'arXiv:2509.13577', 'title': 'Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles', 'authors': 'Tongfei Guo, Lili Su', 'link': 'https://arxiv.org/abs/2509.13577', 'abstract': 'Trajectory prediction is central to the safe and seamless operation of autonomous vehicles (AVs). In deployment, however, prediction models inevitably face distribution shifts between training data and real-world conditions, where rare or underrepresented traffic scenarios induce out-of-distribution (OOD) cases. While most prior OOD detection research in AVs has concentrated on computer vision tasks such as object detection and segmentation, trajectory-level OOD detection remains largely underexplored. A recent study formulated this problem as a quickest change detection (QCD) task, providing formal guarantees on the trade-off between detection delay and false alarms [1]. Building on this foundation, we propose a new framework that introduces adaptive mechanisms to achieve robust detection in complex driving environments. Empirical analysis across multiple real-world datasets reveals that prediction errors--even on in-distribution samples--exhibit mode-dependent distributions that evolve over time with dataset-specific dynamics. By explicitly modeling these error modes, our method achieves substantial improvements in both detection delay and false alarm rates. Comprehensive experiments on established trajectory prediction benchmarks show that our framework significantly outperforms prior UQ- and vision-based OOD approaches in both accuracy and computational efficiency, offering a practical path toward reliable, driving-aware autonomy.', 'abstract_zh': '自主车辆中轨迹预测的分布偏移检测对于安全无缝的操作至关重要。然而，在部署过程中，预测模型不可避免地会面临训练数据与实际条件之间的分布偏移，其中罕见或未充分代表的交通场景会导致分布外(OOD)情况。虽然以往关于自主车辆中分布外检测的研究主要集中在目标检测和分割等计算机视觉任务上，但轨迹级别的分布外检测仍待深入探索。近期的研究将该问题表述为快速变化检测(QCD)任务，并对检测延迟和误报之间的权衡提供了形式化的保证[1]。在此基础上，我们提出了一种新的框架，引入适应性机制以在复杂的驾驶环境中实现稳健的检测。跨多个实际数据集的经验分析表明，即使是在分布内样本上，预测误差也表现出受模式依赖的分布，并随数据集特定的动力学而演变。通过明确建模这些误差模式，我们的方法在检测延迟和误报率方面取得了显著改进。在已建立的轨迹预测基准上的综合实验表明，与先前的不确定性量化(UQ)和视觉基线的分布外方法相比，我们的框架在准确性和计算效率上均表现优异，提供了实现可靠、驾驶感知自主性的现实之路。', 'title_zh': '动态感知：面向自主车辆轨迹预测的自适应多模式离分布检测'}
{'arxiv_id': 'arXiv:2509.13564', 'title': 'Multi-Attacker Single-Defender Target Defense in Conical Environments', 'authors': 'Arman Pourghorban, Dipankar Maity', 'link': 'https://arxiv.org/abs/2509.13564', 'abstract': "We consider a variant of the target defense problem in a planar conical environment where a single defender is tasked to capture a sequence of incoming attackers. The attackers' objective is to breach the target boundary without being captured by the defender. As soon as the current attacker breaches the target or gets captured by the defender, the next attacker appears at the boundary of the environment and moves radially toward the target with maximum speed. Therefore, the defender's final location at the end of the current game becomes its initial location for the next game. The attackers pick strategies that are advantageous for the current as well as for future engagements between the defender and the remaining attackers. The attackers have their own sensors with limited range, using which they can perfectly detect if the defender is within their sensing range. We derive equilibrium strategies for all the players to optimize the capture percentage using the notions of capture distribution. Finally, the theoretical results are verified through numerical examples using Monte Carlo type random trials of experiments.", 'abstract_zh': '我们在平面圆锥环境下的目标防御问题变种中考虑了一个单个防守者捕获序列入侵攻击者的问题。攻击者的目的是在不被防守者捕获的情况下突破目标边界。一旦当前攻击者突破目标或被防守者捕获，下一个攻击者就会出现在环境边界上，并以最大速度径向向目标移动。因此，防守者在当前游戏结束时的最终位置将成为下一个游戏的初始位置。攻击者选择策略，这些策略不仅对当前，而且对未来防御者与剩余攻击者的交战都有利。攻击者拥有有限范围的传感器，可以完美检测防守者是否在其检测范围内。我们利用捕获分布的概念推导出所有参与者的均衡策略，以优化捕获百分比。最后，通过蒙特卡罗类型随机试验的数值例子验证了理论结果。', 'title_zh': '圆锥环境下多攻击者单防卫者目标防御'}
{'arxiv_id': 'arXiv:2509.13414', 'title': 'MapAnything: Universal Feed-Forward Metric 3D Reconstruction', 'authors': 'Nikhil Keetha, Norman Müller, Johannes Schönberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, Jonathon Luiten, Manuel Lopez-Antequera, Samuel Rota Bulò, Christian Richardt, Deva Ramanan, Sebastian Scherer, Peter Kontschieder', 'link': 'https://arxiv.org/abs/2509.13414', 'abstract': 'We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras. MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame. Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more. We provide extensive experimental analyses and model ablations demonstrating that MapAnything outperforms or matches specialist feed-forward models while offering more efficient joint training behavior, thus paving the way toward a universal 3D reconstruction backbone.', 'abstract_zh': 'MapAnything: 一种统一的基于Transformer的前后端模型及其在3D场景几何和相机直接回归中的应用', 'title_zh': 'MapAnything：通用端到端度量三维重建'}
{'arxiv_id': 'arXiv:2509.13352', 'title': 'Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning', 'authors': 'Anis Koubaa, Khaled Gabr', 'link': 'https://arxiv.org/abs/2509.13352', 'abstract': 'Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense, surveillance, and disaster response, yet most systems remain confined to SAE Level 2--3 autonomy. Their reliance on rule-based control and narrow AI restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks lack context-aware reasoning, autonomous decision-making, and ecosystem-level integration; critically, none leverage Large Language Model (LLM) agents with tool-calling for real-time knowledge access. This paper introduces the Agentic UAVs framework, a five-layer architecture (Perception, Reasoning, Action, Integration, Learning) that augments UAVs with LLM-driven reasoning, database querying, and third-party system interaction. A ROS2 and Gazebo-based prototype integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3 deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved higher detection confidence (0.79 vs. 0.72), improved person detection rates (91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%). These results confirm that modest computational overhead enables qualitatively new levels of autonomy and ecosystem integration.', 'abstract_zh': '基于大语言模型的自主无人机框架', 'title_zh': '代理型无人机：由LLM驱动的集成工具调用与认知推理自主性'}
{'arxiv_id': 'arXiv:2509.13331', 'title': 'Explainable AI-Enhanced Supervisory Control for High-Precision Spacecraft Formation', 'authors': 'Reza Pirayeshshirazinezhad', 'link': 'https://arxiv.org/abs/2509.13331', 'abstract': 'We use artificial intelligence (AI) and supervisory adaptive control systems to plan and optimize the mission of precise spacecraft formation. Machine learning and robust control enhance the efficiency of spacecraft precision formation of the Virtual Telescope for X-ray Observation (VTXO) space mission. VTXO is a precise formation of two separate spacecraft making a virtual telescope with a one-kilometer focal length. One spacecraft carries the lens and the other spacecraft holds the camera to observe high-energy space objects in the X-ray domain with 55 milli-arcsecond angular resolution accuracy. Timed automata for supervisory control, Monte Carlo simulations for stability and robustness evaluation, and integration of deep neural networks for optimal estimation of mission parameters, satisfy the high precision mission criteria. We integrate deep neural networks with a constrained, non-convex dynamic optimization pipeline to predict optimal mission parameters, ensuring precision mission criteria are met. AI framework provides explainability by predicting the resulting energy consumption and mission error for a given set of mission parameters. It allows for transparent, justifiable, and real-time trade-offs, a capability not present in traditional adaptive controllers. The results show reductions in energy consumption and improved mission accuracy, demonstrating the capability of the system to address dynamic uncertainties and disturbances.', 'abstract_zh': '我们使用人工智能（AI）和监督自适应控制系统来规划和优化精确航天器编队的任务。机器学习和鲁棒控制提高了虚拟X射线观测望远镜（VTXO）航天任务中航天器精确编队的效率。VTXO是由两颗分离的航天器组成的一个虚拟望远镜，焦距为一公里，其中一颗航天器携带镜片，另一颗航天器携带相机，以55毫角秒的角分辨率精度观测高能太空目标。基于时态自动机的监督控制、蒙特卡洛模拟的稳定性和鲁棒性评估，以及深度神经网络的最优估计整合，满足了高精度任务标准。我们将深度神经网络与受限的非凸动态优化管道集成，以预测最优任务参数，确保满足精确任务标准。AI框架通过预测给定任务参数的结果能量消耗和任务误差提供了可解释性，实现了透明、可辩护和实时的权衡，这是传统自适应控制器不具备的能力。结果显示，能量消耗减少和任务精度提高，证明了该系统的动态不确定性及干扰的解决能力。', 'title_zh': '可解释AI增强的监督控制以实现高精度航天器编队飞行'}
