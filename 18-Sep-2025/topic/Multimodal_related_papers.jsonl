{'arxiv_id': 'arXiv:2509.13615', 'title': 'See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles', 'authors': 'Zongru Wu, Rui Mao, Zhiyuan Tian, Pengzhou Cheng, Tianjie Ju, Zheng Wu, Lingzhong Dong, Haiyue Sheng, Zhuosheng Zhang, Gongshen Liu', 'link': 'https://arxiv.org/abs/2509.13615', 'abstract': 'The advent of multimodal agents facilitates effective interaction within graphical user interface (GUI), especially in ubiquitous GUI control. However, their inability to reliably execute toggle control instructions remains a key bottleneck. To investigate this, we construct a state control benchmark with binary toggle instructions from public datasets. Evaluations of existing agents demonstrate their unreliability, particularly when the current toggle state already matches the desired state. To address the challenge, we propose State-aware Reasoning (StaR), a training method that teaches agents to perceive the current toggle state, analyze the desired state from the instruction, and act accordingly. Experiments on three multimodal agents demonstrate that StaR can improve toggle instruction execution accuracy by over 30\\%. Further evaluations on three public benchmarks show that StaR also enhances general task performance. Finally, evaluations on a dynamic environment highlight the potential of StaR for real-world applications. Code, benchmark, and StaR-enhanced agents are available at this https URL.', 'abstract_zh': '多模态代理的出现促进了图形用户界面（GUI）内的有效交互，尤其是在泛在GUI控制方面。然而，它们在可靠执行切换控制指令方面的能力不足仍然是一个关键瓶颈。为探究此问题，我们从公开数据集中构建了一个基于二元切换指令的状态控制基准。对于现有代理的评估表明，它们在当前切换状态已与目标状态匹配时表现尤其不可靠。为应对这一挑战，我们提出了状态感知推理（StaR），这是一种培训方法，旨在教导代理感知当前切换状态、从指令中分析目标状态并据此行动。在三个多模态代理上的实验表明，StaR 可以将切换指令执行的准确性提高超过30%。进一步在三个公开基准上的评估表明，StaR 也提升了总体任务表现。最后，对动态环境的评估突显了StaR 在实际应用中的潜力。代码、基准和StaR增强的代理可在以下链接获得。', 'title_zh': '看见、思考、行动：通过识别开关以教会多模态代理有效与GUI交互'}
{'arxiv_id': 'arXiv:2509.14001', 'title': 'MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment', 'authors': 'Elena Camuffo, Francesco Barbato, Mete Ozay, Simone Milani, Umberto Michieli', 'link': 'https://arxiv.org/abs/2509.14001', 'abstract': 'We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment), a knowledge distillation approach that transfers region-level multimodal semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight vision-only object detector student (e.g., YOLO). A translation module maps student features into a joint space, where the training of the student and translator is guided by a dual-objective loss that enforces both local alignment and global relational consistency. Unlike prior approaches focused on dense or global alignment, MOCHA operates at the object level, enabling efficient transfer of semantics without modifying the teacher or requiring textual input at inference. We validate our method across four personalized detection benchmarks under few-shot regimes. Results show consistent gains over baselines, with a +10.1 average score improvement. Despite its compact architecture, MOCHA reaches performance on par with larger multimodal models, proving its suitability for real-world deployment.', 'abstract_zh': 'MOCHA（多模态对象感知跨架构对齐），一种知识蒸馏方法，将大型视觉-语言教师（如LLaVA）的区域级多模态语义转移到轻量级的纯视觉对象检测学生（如YOLO）中。', 'title_zh': 'MOCHA: 多模态对象 Awareness 的跨架构对齐'}
{'arxiv_id': 'arXiv:2509.13676', 'title': 'Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation', 'authors': 'Xiaobo Yang, Xiaojin Gong', 'link': 'https://arxiv.org/abs/2509.13676', 'abstract': 'Recently, Referring Image Segmentation (RIS) frameworks that pair the Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM) have achieved impressive results. However, adapting MLLM to segmentation is computationally intensive, primarily due to visual token redundancy. We observe that traditional patch-wise visual projectors struggle to strike a balance between reducing the number of visual tokens and preserving semantic clarity, often retaining overly long token sequences to avoid performance drops. Inspired by text tokenizers, we propose a novel semantic visual projector that leverages semantic superpixels generated by SAM to identify "visual words" in an image. By compressing and projecting semantic superpixels as visual tokens, our approach adaptively shortens the token sequence according to scene complexity while minimizing semantic loss in compression. To mitigate loss of information, we propose a semantic superpixel positional embedding to strengthen MLLM\'s awareness of superpixel geometry and position, alongside a semantic superpixel aggregator to preserve both fine-grained details inside superpixels and global context outside. Experiments show that our method cuts visual tokens by 93% without compromising performance, notably speeding up MLLM training and inference, and outperforming existing compressive visual projectors on RIS.', 'abstract_zh': '近期，将多模态大规模语言模型（MLLM）与段Anything模型（SAM）结合以实现引用图像分割（RIS）框架的研究取得了显著成果。然而，将MLLM适应于分割任务在计算上非常密集，主要由于视觉标记冗余。我们观察到，传统的基于patches的视觉 projector在减少视觉标记数量和保持语义清晰度之间难以取得平衡，往往保留过长的标记序列以避免性能下降。受文本标记化启发，我们提出了一种新的语义视觉 projector，利用SAM生成的语义超像素来识别图像中的“视觉单词”。通过压缩和投影语义超像素为视觉标记，我们的方法能够根据场景复杂性自适应地缩短标记序列，同时最小化压缩过程中的语义损失。为减轻信息损失，我们提出了一种语义超像素位置嵌入，以增强MLLM对超像素几何和位置的感知，同时提出了一种语义超像素聚合器，以保留超像素内部的精细细节和超像素外部的全局上下文。实验表明，我们的方法在不牺牲性能的情况下，将视觉标记数量减少了93%，显著加速了MLLM的训练和推理，并在引用图像分割任务上优于现有的压缩视觉投影器。', 'title_zh': '重新利用SAM以实现高效视觉投影器，用于基于MLLM的指针图像分割'}
{'arxiv_id': 'arXiv:2509.13590', 'title': 'Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation', 'authors': 'Samer Al-Hamadani', 'link': 'https://arxiv.org/abs/2509.13590', 'abstract': 'The rapid advancement of artificial intelligence (AI) in healthcare imaging has revolutionized diagnostic medicine and clinical decision-making processes. This work presents an intelligent multimodal framework for medical image analysis that leverages Vision-Language Models (VLMs) in healthcare diagnostics. The framework integrates Google Gemini 2.5 Flash for automated tumor detection and clinical report generation across multiple imaging modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual feature extraction with natural language processing to enable contextual image interpretation, incorporating coordinate verification mechanisms and probabilistic Gaussian modeling for anomaly distribution. Multi-layered visualization techniques generate detailed medical illustrations, overlay comparisons, and statistical representations to enhance clinical confidence, with location measurement achieving 80 pixels average deviation. Result processing utilizes precise prompt engineering and textual analysis to extract structured clinical information while maintaining interpretability. Experimental evaluations demonstrated high performance in anomaly detection across multiple modalities. The system features a user-friendly Gradio interface for clinical workflow integration and demonstrates zero-shot learning capabilities to reduce dependence on large datasets. This framework represents a significant advancement in automated diagnostic support and radiological workflow efficiency, though clinical validation and multi-center evaluation are necessary prior to widespread adoption.', 'abstract_zh': '医疗影像领域人工智能的飞速发展颠覆了诊断医学和临床决策过程。本文提出了一种智能多模态医疗影像分析框架，利用视觉-语言模型（VLMs）进行医疗诊断。该框架结合了Google Gemini 2.5 Flash实现多模态（CT、MRI、X-ray、超声）影像中的自动化肿瘤检测和临床报告生成。系统结合了视觉特征提取和自然语言处理技术，通过坐标验证机制和概率高斯建模进行异常分布的解释性分析。多层级可视化技术生成详细的医疗图像、叠加比较及统计表示，以增强临床信心，位置测量平均偏差为80像素。结果处理通过精确的提示工程和文本分析提取结构化的临床信息，保持可解释性。实验评估在多模态异常检测方面展现了高绩效。该系统具备用户友好的Gradio界面，集成临床工作流程，并展示零样本学习能力，减少对大规模数据集的依赖。该框架代表了自动诊断支持和放射学工作流程效率的重要进步，但在广泛采用之前需要临床上的验证和多中心评估。', 'title_zh': '基于VLM的自动化医学图像分析与临床报告生成智能医疗影像平台'}
