{'arxiv_id': 'arXiv:2509.14228', 'title': 'Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models', 'authors': 'Benjamin Shaffer, Victoria Edwards, Brooks Kinch, Nathaniel Trask, M. Ani Hsieh', 'link': 'https://arxiv.org/abs/2509.14228', 'abstract': "Source localization in a complex flow poses a significant challenge for multi-robot teams tasked with localizing the source of chemical leaks or tracking the dispersion of an oil spill. The flow dynamics can be time-varying and chaotic, resulting in sporadic and intermittent sensor readings, and complex environmental geometries further complicate a team's ability to model and predict the dispersion. To accurately account for the physical processes that drive the dispersion dynamics, robots must have access to computationally intensive numerical models, which can be difficult when onboard computation is limited. We present a distributed mobile sensing framework for source localization in which each robot carries a machine-learned, finite element model of its environment to guide information-based sampling. The models are used to evaluate an approximate mutual information criterion to drive an infotaxis control strategy, which selects sensing regions that are expected to maximize informativeness for the source localization objective. Our approach achieves faster error reduction compared to baseline sensing strategies and results in more accurate source localization compared to baseline machine learning approaches.", 'abstract_zh': '复杂流场中源定位为多机器人团队在化学泄漏源定位或溢油扩散追踪任务中带来了显著挑战。流场动力学可能具有时变性和混沌性，导致传感器读数出现间歇性和不连续性，复杂环境几何形状进一步增加了团队建模和预测扩散的难度。为了准确反映驱动扩散动力学的物理过程，机器人需要访问计算密集型的数值模型，但在计算资源有限的情况下这可能很困难。我们提出了一种分布式移动传感框架，其中每个机器人携带一个环境的机器学习有限元模型以指导基于信息的采样。这些模型用于评估近似互信息准则，以驱动信息素战略，选择预期能最大化源定位信息量的传感区域。与基线传感策略相比，我们的方法实现了更快的误差减少，与基线机器学习方法相比，源定位结果更为准确。', 'title_zh': '复杂流场中基于物理保真环境模型的多机器人多源定位'}
{'arxiv_id': 'arXiv:2509.14159', 'title': 'MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies', 'authors': 'Dayi Dong, Maulik Bhatt, Seoyeon Choi, Negar Mehr', 'link': 'https://arxiv.org/abs/2509.14159', 'abstract': 'As robots become more integrated in society, their ability to coordinate with other robots and humans on multi-modal tasks (those with multiple valid solutions) is crucial. We propose to learn such behaviors from expert demonstrations via imitation learning (IL). However, when expert demonstrations are multi-modal, standard IL approaches can struggle to capture the diverse strategies, hindering effective coordination. Diffusion models are known to be effective at handling complex multi-modal trajectory distributions in single-agent systems. Diffusion models have also excelled in multi-agent scenarios where multi-modality is more common and crucial to learning coordinated behaviors. Typically, diffusion-based approaches require a centralized planner or explicit communication among agents, but this assumption can fail in real-world scenarios where robots must operate independently or with agents like humans that they cannot directly communicate with. Therefore, we propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE) paradigm for multi-modal multi-agent imitation learning using diffusion policies. Agents are trained jointly with full information, but execute policies using only local information to achieve implicit coordination. We demonstrate in both simulation and hardware experiments that our method recovers multi-modal coordination behavior among agents in a variety of tasks and environments, while improving upon state-of-the-art baselines.', 'abstract_zh': '随着机器人在社会中的集成程度提高，它们在多模态任务（具有多个有效解决方案的任务）中与其他机器人和人类协调的能力变得至关重要。我们提出通过模仿学习（IL）从专家示范中学习此类行为。然而，当专家示范是多模态时，标准的IL方法可能难以捕捉到多样化的策略，阻碍了有效的协调。扩散模型在处理单智能体系统的复杂多模态轨迹分布方面表现出色。扩散模型在多智能体场景中也表现出色，这些场景中的多模态现象更为普遍且对学习协调行为至关重要。通常，基于扩散的方法需要中心化规划或智能体间的显式通信，但在机器人必须独立操作或与无法直接沟通的智能体（如人类）交互的实际场景中，这一假设可能失效。因此，我们提出了一种集中式训练、去中心化执行（CTDE）范式，用于多模态多智能体模仿学习，采用扩散策略。智能体在训练期间共享完整信息，但在执行策略时仅使用局部信息以实现隐式的协调。我们在仿真和硬件实验中展示了该方法在多种任务和环境中恢复智能体的多模态协调行为，并优于最先进的基线方法。', 'title_zh': 'MIMIC-D：多模态模仿在去中心化扩散策略下的多智能体协调'}
{'arxiv_id': 'arXiv:2509.14143', 'title': 'CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping', 'authors': 'Zijian An, Ran Yang, Yiming Feng, Lifeng Zhou', 'link': 'https://arxiv.org/abs/2509.14143', 'abstract': 'Vision-language-action (VLA) models have recently emerged as a promising paradigm for robotic control, enabling end-to-end policies that ground natural language instructions into visuomotor actions. However, current VLAs often struggle to satisfy precise task constraints, such as stopping based on numeric thresholds, since their observation-to-action mappings are implicitly shaped by training data and lack explicit mechanisms for condition monitoring. In this work, we propose CLAW (CLIP-Language-Action for Weight), a framework that decouples condition evaluation from action generation. CLAW leverages a fine-tuned CLIP model as a lightweight prompt generator, which continuously monitors the digital readout of a scale and produces discrete directives based on task-specific weight thresholds. These prompts are then consumed by $\\pi_0$, a flow-based VLA policy, which integrates the prompts with multi-view camera observations to produce continuous robot actions. This design enables CLAW to combine symbolic weight reasoning with high-frequency visuomotor control. We validate CLAW on three experimental setups: single-object grasping and mixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW reliably executes weight-aware behaviors and outperforms both raw-$\\pi_0$ and fine-tuned $\\pi_0$ models. We have uploaded the videos as supplementary materials.', 'abstract_zh': '基于CLIP-Language-Action的条件监控框架', 'title_zh': 'CLAW：一种考虑重量的视觉-语言-动作框架用于机器人抓取'}
{'arxiv_id': 'arXiv:2509.14138', 'title': 'SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model', 'authors': 'Ran Yang, Zijian An, Lifeng ZHou, Yiming Feng', 'link': 'https://arxiv.org/abs/2509.14138', 'abstract': 'Long-horizon robotic manipulation tasks require executing multiple interdependent subtasks in strict sequence, where errors in detecting subtask completion can cascade into downstream failures. Existing Vision-Language-Action (VLA) models such as $\\pi_0$ excel at continuous low-level control but lack an internal signal for identifying when a subtask has finished, making them brittle in sequential settings. We propose SeqVLA, a completion-aware extension of $\\pi_0$ that augments the base architecture with a lightweight detection head perceiving whether the current subtask is complete. This dual-head design enables SeqVLA not only to generate manipulation actions but also to autonomously trigger transitions between subtasks. We investigate four finetuning strategies that vary in how the action and detection heads are optimized (joint vs. sequential finetuning) and how pretrained knowledge is preserved (full finetuning vs. frozen backbone). Experiments are performed on two multi-stage tasks: salad packing with seven distinct subtasks and candy packing with four distinct subtasks. Results show that SeqVLA significantly outperforms the baseline $\\pi_0$ and other strong baselines in overall success rate. In particular, joint finetuning with an unfrozen backbone yields the most decisive and statistically reliable completion predictions, eliminating sequence-related failures and enabling robust long-horizon execution. Our results highlight the importance of coupling action generation with subtask-aware detection for scalable sequential manipulation.', 'abstract_zh': '长时机器人操作任务需要严格顺序执行多个相互依赖的子任务，其中子任务完成检测错误会连锁导致下游任务失败。现有的视觉-语言-行动（VLA）模型如$\\pi_0$在连续的低级控制方面表现出色，但缺乏识别子任务完成的内部信号，使其在序列设置中较为脆弱。我们提出了SeqVLA，这是一种针对子任务完成情况进行感知的$\\pi_0$的扩展，通过增加一个小而轻的检测头来判断当前子任务是否已完成。这种双头设计使SeqVLA不仅能生成操作动作，还能自主触发子任务之间的转换。我们探讨了四种不同的微调策略，这些策略在操作头和检测头的优化方式（联合微调 vs. 顺序微调）以及预训练知识的保留方式（完全微调 vs. 固定骨干网）上有所不同。在两个多阶段任务（七种不同子任务的沙拉包装和四种不同子任务的糖果包装）上进行了实验。结果显示，SeqVLA在总体成功率上显著优于基线$\\pi_0$和其他强基线模型。特别是联合微调且不冻结骨干网的方法提供了最可靠且统计上显著的完成预测，消除了序列相关失败，使执行更加稳健。我们的结果强调了将动作生成与子任务感知检测相结合的重要性，以实现可扩展的序列化操作。', 'title_zh': 'SeqVLA：面向完成意识视觉-语言-动作模型的长期 horizon 操作序列任务执行'}
{'arxiv_id': 'arXiv:2509.14126', 'title': 'CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads', 'authors': 'Viktor Lorentz, Khaled Wahba, Sayantan Auddy, Marc Toussaint, Wolfgang Hönig', 'link': 'https://arxiv.org/abs/2509.14126', 'abstract': 'Collaborative transportation of cable-suspended payloads by teams of Unmanned Aerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to different payload shapes, and provide built-in compliance, making it attractive for applications ranging from disaster relief to precision logistics. However, multi-UAV coordination under disturbances, nonlinear payload dynamics, and slack--taut cable modes remains a challenging control problem. To our knowledge, no prior work has addressed these cable mode transitions in the multi-UAV context, instead relying on simplifying rigid-link assumptions. We propose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for multi-UAV cable-suspended payload transport. Simulation results demonstrate that the learned policies can outperform classical decentralized controllers in terms of disturbance rejection and tracking precision, achieving an 80% recovery rate from harsh conditions compared to 44% for the baseline method. We also achieve successful zero-shot sim-to-real transfer and demonstrate that our policies are highly robust under harsh conditions, including wind, random external disturbances, and transitions between slack and taut cable dynamics. This work paves the way for autonomous, resilient UAV teams capable of executing complex payload missions in unstructured environments.', 'abstract_zh': '多旋翼无人机团队协作吊运电缆悬挂载荷具有增强载荷容量、适应不同载荷形状以及提供内置顺应性等潜力，使其在从灾害救援到精密物流等多个应用领域具有吸引力。然而，在扰动下多旋翼无人机协调、非线性载荷动力学以及松弛和紧绷电缆模式之间切换仍是一个具有挑战性的控制问题。据我们所知，目前尚没有研究在多旋翼无人机上下考虑这些电缆模式的切换，而是依赖于刚性连接的简化假设。我们提出了一种去中心化的强化学习（RL）框架CrazyMARL，用于多旋翼无人机电缆悬挂载荷运输。模拟结果表明，所学习的策略在干扰抑制和跟踪精度方面可优于经典去中心化控制器，相较于基线方法，达到80%的恶劣条件恢复率，而基线方法为44%。我们还实现了成功的无监督模拟到现实世界的转移，并展示了我们的策略在恶劣条件（包括风、随机外部干扰以及松弛和紧绷电缆动力学之间的切换）下具有高度的鲁棒性。这一工作为能够在非结构化环境中执行复杂载荷任务的自主、鲁棒无人机团队铺平了道路。', 'title_zh': '疯狂MARL：协作悬挂吊载空中运输的去中心化直接电机控制策略'}
{'arxiv_id': 'arXiv:2509.14117', 'title': 'GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model', 'authors': 'Ali Abouzeid, Malak Mansour, Zezhou Sun, Dezhen Song', 'link': 'https://arxiv.org/abs/2509.14117', 'abstract': 'Vision-Language-Action (VLA) models often fail to generalize to novel camera viewpoints, a limitation stemming from their difficulty in inferring robust 3D geometry from 2D images. We introduce GeoAware-VLA, a simple yet effective approach that enhances viewpoint invariance by integrating strong geometric priors into the vision backbone. Instead of training a visual encoder or relying on explicit 3D data, we leverage a frozen, pretrained geometric vision model as a feature extractor. A trainable projection layer then adapts these geometrically-rich features for the policy decoder, relieving it of the burden of learning 3D consistency from scratch. Through extensive evaluations on LIBERO benchmark subsets, we show GeoAware-VLA achieves substantial improvements in zero-shot generalization to novel camera poses, boosting success rates by over 2x in simulation. Crucially, these benefits translate to the physical world; our model shows a significant performance gain on a real robot, especially when evaluated from unseen camera angles. Our approach proves effective across both continuous and discrete action spaces, highlighting that robust geometric grounding is a key component for creating more generalizable robotic agents.', 'abstract_zh': 'GeoAware-VLA:通过集成强几何先验增强视点不变性以提升通用能力', 'title_zh': 'GeoAware-VLA: 隐式几何意识的视觉-语言-动作模型'}
{'arxiv_id': 'arXiv:2509.14063', 'title': 'Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace', 'authors': 'Sundhar Vinodh Sangeetha, Chih-Yuan Chiu, Sarah H.Q. Li, Shreyas Kousik', 'link': 'https://arxiv.org/abs/2509.14063', 'abstract': 'Autonomous aircraft must safely operate in untowered airspace, where coordination relies on voice-based communication among human pilots. Safe operation requires an aircraft to predict the intent, and corresponding goal location, of other aircraft. This paper introduces a multimodal framework for aircraft goal prediction that integrates natural language understanding with spatial reasoning to improve autonomous decision-making in such environments. We leverage automatic speech recognition and large language models to transcribe and interpret pilot radio calls, identify aircraft, and extract discrete intent labels. These intent labels are fused with observed trajectories to condition a temporal convolutional network and Gaussian mixture model for probabilistic goal prediction. Our method significantly reduces goal prediction error compared to baselines that rely solely on motion history, demonstrating that language-conditioned prediction increases prediction accuracy. Experiments on a real-world dataset from an untowered airport validate the approach and highlight its potential to enable socially aware, language-conditioned robotic motion planning.', 'abstract_zh': '自主飞行器必须在无控制塔台的空域安全运行，其中协调依赖于人类飞行员之间的语音通信。安全运行需要飞行器预测其他飞行器的意图及其相应的目标位置。本文引入了一种多模态框架，该框架结合自然语言理解和空间推理，以改善此类环境中的自主决策。我们利用自动语音识别和大型语言模型来转录和解释飞行员的无线电呼叫、识别飞行器并提取离散的意图标签。这些意图标签与观测轨迹融合，以条件化时序卷积网络和高斯混合模型，进行概率性目标预测。与仅依赖运动历史的基线方法相比，我们的方法显著减少了目标预测误差，证明了语言条件下的预测可以提高预测准确性。实验结果验证了该方法，并突显了其在实现社交意识的语言条件下飞行器运动规划方面的潜力。', 'title_zh': '语言约束提高无塔台 airspace 中航空器目标预测的准确性'}
{'arxiv_id': 'arXiv:2509.14040', 'title': 'Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning', 'authors': 'Zewen Yang, Xiaobing Dai, Dongfa Zhang, Yu Li, Ziyang Meng, Bingkun Huang, Hamid Sadeghian, Sami Haddadin', 'link': 'https://arxiv.org/abs/2509.14040', 'abstract': "Learning from demonstration allows robots to acquire complex skills from human demonstrations, but conventional approaches often require large datasets and fail to generalize across coordinate transformations. In this paper, we propose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP) learning framework that enables robots to perform human-guided automated control from a single motion prompt. A dataset-construction strategy based on coordinate transformations is introduced that enforces invariance to translation, rotation, and scaling, while supporting multi-step predictions. Moreover, GeoGP is robust to variations in the user's motion prompt and supports multi-skill autonomy. We validate the proposed approach through numerical simulations with the designed user graphical interface and two real-world robotic experiments, which demonstrate that the proposed method is effective, generalizes across tasks, and significantly reduces the demonstration burden. Project page is available at: this https URL", 'abstract_zh': 'Learning from Demonstration 使机器人能够通过人类示范获取复杂技能，但传统方法通常需要大量数据集并且无法跨坐标变换进行泛化。本文提出了一种几何不变的单次学习高斯过程（GeoGP）框架——Prompt2Auto，该框架使机器人能够从单一运动提示进行带有指导的人机自动控制。该论文介绍了一种基于坐标变换的数据集构建策略，该策略保证了平移、旋转和缩放的不变性，并支持多步预测。此外，GeoGP 对用户的运动提示变化具有鲁棒性，并支持多技能自主。通过设计的用户图形界面的数值仿真和两个实际机器人实验对该方法进行了验证，结果表明该方法有效、跨任务泛化能力强，并显著减轻了示范负担。更多内容请参见项目页面：this https URL', 'title_zh': 'Prompt2Auto: 从运动提示到基于几何不变的一次性高斯过程学习的自动化控制'}
{'arxiv_id': 'arXiv:2509.14010', 'title': 'Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization', 'authors': 'Zong Chen, Shaoyang Li, Ben Liu, Min Li, Zhouping Yin, Yiqun Li', 'link': 'https://arxiv.org/abs/2509.14010', 'abstract': "Wheel-legged robots with integrated manipulators hold great promise for mobile manipulation in logistics, industrial automation, and human-robot collaboration. However, unified control of such systems remains challenging due to the redundancy in degrees of freedom, complex wheel-ground contact dynamics, and the need for seamless coordination between locomotion and manipulation. In this work, we present the design and whole-body motion control of an omnidirectional wheel-legged quadrupedal robot equipped with a dexterous manipulator. The proposed platform incorporates independently actuated steering modules and hub-driven wheels, enabling agile omnidirectional locomotion with high maneuverability in structured environments. To address the challenges of contact-rich interaction, we develop a contact-aware whole-body dynamic optimization framework that integrates point-contact modeling for manipulation with line-contact modeling for wheel-ground interactions. A warm-start strategy is introduced to accelerate online optimization, ensuring real-time feasibility for high-dimensional control. Furthermore, a unified kinematic model tailored for the robot's 4WIS-4WID actuation scheme eliminates the need for mode switching across different locomotion strategies, improving control consistency and robustness. Simulation and experimental results validate the effectiveness of the proposed framework, demonstrating agile terrain traversal, high-speed omnidirectional mobility, and precise manipulation under diverse scenarios, underscoring the system's potential for factory automation, urban logistics, and service robotics in semi-structured environments.", 'abstract_zh': '集成 manipulator 的轮-legged 机器人在物流、工业自动化和人机协作中的移动 manipulation 具有巨大潜力。然而，由于自由度的冗余性、复杂的轮-地面接触动力学以及运动与操作之间的无缝协调需求，这类系统的统一控制仍然具有挑战性。在本工作中，我们提出了一个配备灵巧 manipulator 的全向轮-legged 四足机器人设计及其全身运动控制方法。该提出的平台集成了独立驱动的转向模块和 Hub 驱动的轮子，能够在结构化环境中实现高效灵活的全向运动。为应对接触丰富的交互挑战，我们开发了一种接触感知的全身动态优化框架，将点接触建模与线接触建模结合应用于操作和轮-地面交互。引入预热策略以加速在线优化，确保高维度控制的实时可行性。此外，针对该机器人 4WIS-4WID 驱动方案定制的统一运动学模型消除了不同运动策略之间的模式切换需求，提高了控制的一致性和鲁棒性。仿真和实验结果验证了所提框架的有效性，展示了在多种场景下该系统灵活的地形穿越、高速全向移动和精确操作的能力，突显了其在半结构化环境中的工厂自动化、城市物流和服务机器人应用的潜力。', 'title_zh': '全身体动控制的 omnidirectional 轮腿式移动 manipulator 通过接触感知动力学优化'}
{'arxiv_id': 'arXiv:2509.13956', 'title': 'SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning', 'authors': 'Zewei Yang, Zengqi Peng, Jun Ma', 'link': 'https://arxiv.org/abs/2509.13956', 'abstract': 'Autonomous parking is a critical component for achieving safe and efficient urban autonomous driving. However, unstructured environments and dynamic interactions pose significant challenges to autonomous parking tasks. To address this problem, we propose SEG-Parking, a novel end-to-end offline reinforcement learning (RL) framework to achieve interaction-aware autonomous parking. Notably, a specialized parking dataset is constructed for parking scenarios, which include those without interference from the opposite vehicle (OV) and complex ones involving interactions with the OV. Based on this dataset, a goal-conditioned state encoder is pretrained to map the fused perception information into the latent space. Then, an offline RL policy is optimized with a conservative regularizer that penalizes out-of-distribution actions. Extensive closed-loop experiments are conducted in the high-fidelity CARLA simulator. Comparative results demonstrate the superior performance of our framework with the highest success rate and robust generalization to out-of-distribution parking scenarios. The related dataset and source code will be made publicly available after the paper is accepted.', 'abstract_zh': '自主泊车是实现安全高效的城市自动驾驶的关键组成部分。然而，非结构化环境和动态交互给自主泊车任务带来了重大挑战。为了解决这一问题，我们提出了一种新颖的端到端离线强化学习(RL)框架SEG-Parking，以实现交互感知的自主泊车。特别地，我们构建了一个专门用于泊车场景的数据集，其中包括无对向车辆干扰的场景和涉及对向车辆交互的复杂场景。基于此数据集，我们预先训练了一个目标条件状态编码器，将融合感知信息映射到潜在空间。然后，该离线RL策略使用保守正则化器进行优化，该正则化器惩罚异常分布的动作。我们在高度保真的CARLA模拟器中进行了广泛的闭环实验。比较结果表明，我们的框架具有最高的成功率，并且在异常分布的泊车场景中具有强大的泛化能力。论文被接受后，相关数据集和源代码将公开发布。', 'title_zh': 'SEG-Parking:向着安全、高效且泛化的自主泊车的端到端 Offline 强化学习方法'}
{'arxiv_id': 'arXiv:2509.13949', 'title': 'SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks', 'authors': 'Jannick Stranghöner, Philipp Hartmann, Marco Braun, Sebastian Wrede, Klaus Neumann', 'link': 'https://arxiv.org/abs/2509.13949', 'abstract': 'High-mix low-volume (HMLV) industrial assembly, common in small and medium-sized enterprises (SMEs), requires the same precision, safety, and reliability as high-volume automation while remaining flexible to product variation and environmental uncertainty. Current robotic systems struggle to meet these demands. Manual programming is brittle and costly to adapt, while learning-based methods suffer from poor sample efficiency and unsafe exploration in contact-rich tasks. To address this, we present SHaRe-RL, a reinforcement learning framework that leverages multiple sources of prior knowledge. By (i) structuring skills into manipulation primitives, (ii) incorporating human demonstrations and online corrections, and (iii) bounding interaction forces with per-axis compliance, SHaRe-RL enables efficient and safe online learning for long-horizon, contact-rich industrial assembly tasks. Experiments on the insertion of industrial Harting connector modules with 0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance within practical time budgets. Our results show that process expertise, without requiring robotics or RL knowledge, can meaningfully contribute to learning, enabling safer, more robust, and more economically viable deployment of RL for industrial assembly.', 'abstract_zh': '面向小批量多样生产的高精度机器人学习框架：SHaRe-RL', 'title_zh': 'SHaRe-RL: 结构化、交互式强化学习在接触丰富的工业装配任务中的应用'}
{'arxiv_id': 'arXiv:2509.13948', 'title': 'The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot', 'authors': 'Benedict Barrow, Roger K. Moore', 'link': 'https://arxiv.org/abs/2509.13948', 'abstract': "Trust and the perception of trustworthiness play an important role in decision-making and our behaviour towards others, and this is true not only of human-human interactions but also of human-robot interactions. While significant advances have been made in recent years in the field of social robotics, there is still some way to go before we fully understand the factors that influence human trust in robots. This paper presents the results of a study into the first impressions created by a social robot's facial features, based on the hypothesis that a `babyface' engenders trust. By manipulating the back-projected face of a Furhat robot, the study confirms that eye shape and size have a significant impact on the perception of trustworthiness. The work thus contributes to an understanding of the design choices that need to be made when developing social robots so as to optimise the effectiveness of human-robot interaction.", 'abstract_zh': '信任与信任感知在人类决策和对他人行为中扮演重要角色，这不仅体现在人与人之间的互动中，也体现在人与机器人之间的互动中。尽管近年来社会机器人领域取得了显著进展，但在完全理解影响人类对机器人信任的因素方面仍有很多工作要做。本文基于“婴儿脸”能够引发信任的假设，研究了社会机器人面部特征给人的第一印象，并通过操纵Furhat机器人的投影面部证实了眼睛的形状和大小对信任感知有显著影响。这项工作有助于理解在开发社会机器人时需要做出的设计选择，以优化人机互动的有效性。', 'title_zh': '面部特征对社交机器人可信度感知的影响'}
{'arxiv_id': 'arXiv:2509.13926', 'title': 'MAP: End-to-End Autonomous Driving with Map-Assisted Planning', 'authors': 'Huilin Yin, Yiming Kan, Daniel Watzenig', 'link': 'https://arxiv.org/abs/2509.13926', 'abstract': 'In recent years, end-to-end autonomous driving has attracted increasing attention for its ability to jointly model perception, prediction, and planning within a unified framework. However, most existing approaches underutilize the online mapping module, leaving its potential to enhance trajectory planning largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel map-assisted end-to-end trajectory planning framework. MAP explicitly integrates segmentation-based map features and the current ego status through a Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and a Weight Adapter based on current ego status. Experiments conducted on the DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6% reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a 44.5% improvement in overall score compared to the UniV2X baseline, even without post-processing. Furthermore, it achieves top ranking in Track 2 of the End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of overall score. These results highlight the effectiveness of explicitly leveraging semantic map features in planning and suggest new directions for improving structure design in end-to-end autonomous driving systems. Our code is available at this https URL', 'abstract_zh': '基于地图辅助的端到端轨迹规划框架MAP', 'title_zh': 'MAP：带地图辅助规划的端到端自动驾驶'}
{'arxiv_id': 'arXiv:2509.13903', 'title': 'PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models', 'authors': 'Artem Lykov, Jeffrin Sam, Hung Khang Nguyen, Vladislav Kozlovskiy, Yara Mahmoud, Valerii Serpiva, Miguel Altamirano Cabrera, Mikhail Konenkov, Dzmitry Tsetserukou', 'link': 'https://arxiv.org/abs/2509.13903', 'abstract': 'We introduce PhysicalAgent, an agentic framework for robotic manipulation that integrates iterative reasoning, diffusion-based video generation, and closed-loop execution. Given a textual instruction, our method generates short video demonstrations of candidate trajectories, executes them on the robot, and iteratively re-plans in response to failures. This approach enables robust recovery from execution errors. We evaluate PhysicalAgent across multiple perceptual modalities (egocentric, third-person, and simulated) and robotic embodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing against state-of-the-art task-specific baselines. Experiments demonstrate that our method consistently outperforms prior approaches, achieving up to 83% success on human-familiar tasks. Physical trials reveal that first-attempt success is limited (20-30%), yet iterative correction increases overall success to 80% across platforms. These results highlight the potential of video-based generative reasoning for general-purpose robotic manipulation and underscore the importance of iterative execution for recovering from initial failures. Our framework paves the way for scalable, adaptable, and robust robot control.', 'abstract_zh': '物理代理：一种集成迭代推理、基于扩散的视频生成和闭环执行的机器人操作框架', 'title_zh': '物理智能体：面向基础世界模型的通用认知机器人研究'}
{'arxiv_id': 'arXiv:2509.13839', 'title': 'Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models', 'authors': 'Motonari Kambara, Komei Sugiura', 'link': 'https://arxiv.org/abs/2509.13839', 'abstract': 'In this work, we address the problem of predicting the future success of open-vocabulary object manipulation tasks. Conventional approaches typically determine success or failure after the action has been carried out. However, they make it difficult to prevent potential hazards and rely on failures to trigger replanning, thereby reducing the efficiency of object manipulation sequences. To overcome these challenges, we propose a model, which predicts the alignment between a pre-manipulation egocentric image with the planned trajectory and a given natural language instruction. We introduce a Multi-Level Trajectory Fusion module, which employs a state-of-the-art deep state-space model and a transformer encoder in parallel to capture multi-level time-series self-correlation within the end effector trajectory. Our experimental results indicate that the proposed method outperformed existing methods, including foundation models.', 'abstract_zh': '本研究解决了预测开放词汇对象操作任务未来成功的问题。传统方法通常在动作执行后才确定成功或失败。这使得难以预防潜在的危害，并且依赖失败来触发重新规划，从而降低了对象操作序列的效率。为克服这些挑战，我们提出了一种模型，该模型预测预操作第一人称视角图像与计划轨迹和给定自然语言指令之间的对齐。我们引入了多级轨迹融合模块，该模块并行使用最先进的深度状态空间模型和变换器编码器来捕捉末端执行器轨迹内的多级时间序列自相关性。实验结果表明，提出的方法优于现有方法，包括基础模型。', 'title_zh': '预操纵对齐预测的平行深度状态空间与变换器模型方法'}
{'arxiv_id': 'arXiv:2509.13833', 'title': 'Track Any Motions under Any Disturbances', 'authors': 'Zhikai Zhang, Jun Guo, Chao Chen, Jilong Wang, Chenghuai Lin, Yunrui Lian, Han Xue, Zhenrong Wang, Maoqi Liu, Huaping Liu, He Wang, Li Yi', 'link': 'https://arxiv.org/abs/2509.13833', 'abstract': 'A foundational humanoid motion tracker is expected to be able to track diverse, highly dynamic, and contact-rich motions. More importantly, it needs to operate stably in real-world scenarios against various dynamics disturbances, including terrains, external forces, and physical property changes for general practical use. To achieve this goal, we propose Any2Track (Track Any motions under Any disturbances), a two-stage RL framework to track various motions under multiple disturbances in the real world. Any2Track reformulates dynamics adaptability as an additional capability on top of basic action execution and consists of two key components: AnyTracker and AnyAdapter. AnyTracker is a general motion tracker with a series of careful designs to track various motions within a single policy. AnyAdapter is a history-informed adaptation module that endows the tracker with online dynamics adaptability to overcome the sim2real gap and multiple real-world disturbances. We deploy Any2Track on Unitree G1 hardware and achieve a successful sim2real transfer in a zero-shot manner. Any2Track performs exceptionally well in tracking various motions under multiple real-world disturbances.', 'abstract_zh': '一种基础的人形运动跟踪器应能够跟踪多样、高度动态和接触丰富的运动。更重要的是，它需要在各种动力学干扰下，包括地形、外力和物理属性变化的现实场景中稳定运行。为了实现这一目标，我们提出了一种名为Any2Track（在任意干扰下跟踪任意运动）的两阶段强化学习框架，该框架能够在多种干扰下实时跟踪各种运动。Any2Track将动力学适应性重新表述为基本动作执行之外的一种附加能力，并包含两个关键组件：AnyTracker和AnyAdapter。AnyTracker是一种通用的运动跟踪器，经过精心设计，能够在单一策略内跟踪各种运动。AnyAdapter是一种基于历史的信息适应模块，赋予跟踪器在线动力学适应能力，以克服模拟到现实的差距和多种现实世界的干扰。我们将在Unitree G1硬件上部署Any2Track，并以零样本的方式实现成功的模拟到现实的转换。在多种现实世界的干扰下，Any2Track在跟踪各种运动方面表现出色。', 'title_zh': '在任意干扰下的任意运动跟踪'}
{'arxiv_id': 'arXiv:2509.13827', 'title': 'How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots', 'authors': 'Renyuan Liu, Haoting Zhou, Chuankai Fang, Qinbing Fu', 'link': 'https://arxiv.org/abs/2509.13827', 'abstract': 'Anyone who has tried to swat a fly has likely been frustrated by its remarkable this http URL ability stems from its visual neural perception system, particularly the collision-selective neurons within its small this http URL autonomous robots operating in complex and unfamiliar environments, achieving similar agility is highly desirable but often constrained by the trade-off between computational cost and this http URL this context, insect-inspired intelligence offers a parsimonious route to low-power, computationally efficient this http URL this paper, we propose an attention-driven visuomotor control strategy inspired by a specific class of fly visual projection neurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated escape this http URL our knowledge, this represents the first embodiment of an LPLC2 neural model in the embedded vision of a physical mobile robot, enabling collision perception and reactive this http URL model was simplified and optimized at 70KB in memory to suit the computational constraints of a vision-based micro robot, the Colias, while preserving key neural perception this http URL further incorporated multi-attention mechanisms to emulate the distributed nature of LPLC2 responses, allowing the robot to detect and react to approaching targets both rapidly and this http URL systematically evaluated the proposed method against a state-of-the-art locust-inspired collision detection this http URL showed that the fly-inspired visuomotor model achieved comparable robustness, at success rate of 96.1% in collision detection while producing more adaptive and elegant evasive this http URL demonstrating an effective collision-avoidance strategy, this work highlights the potential of fly-inspired neural models for advancing research into collective behaviors in insect intelligence.', 'abstract_zh': '飞蠊启发的低功耗计算视觉运动控制策略及其应用：一种适用于复杂环境微型自主机器人的碰撞感知与规避方法', 'title_zh': '飞神经感知机制增强微机器人视运动控制'}
{'arxiv_id': 'arXiv:2509.13816', 'title': 'Agile in the Face of Delay: Asynchronous End-to-End Learning for Real-World Aerial Navigation', 'authors': 'Yude Li, Zhexuan Zhou, Huizhe Li, Youmin Gong, Jie Mei', 'link': 'https://arxiv.org/abs/2509.13816', 'abstract': 'Robust autonomous navigation for Autonomous Aerial Vehicles (AAVs) in complex environments is a critical capability. However, modern end-to-end navigation faces a key challenge: the high-frequency control loop needed for agile flight conflicts with low-frequency perception streams, which are limited by sensor update rates and significant computational cost. This mismatch forces conventional synchronous models into undesirably low control rates. To resolve this, we propose an asynchronous reinforcement learning framework that decouples perception and control, enabling a high-frequency policy to act on the latest IMU state for immediate reactivity, while incorporating perception features asynchronously. To manage the resulting data staleness, we introduce a theoretically-grounded Temporal Encoding Module (TEM) that explicitly conditions the policy on perception delays, a strategy complemented by a two-stage curriculum to ensure stable and efficient training. Validated in extensive simulations, our method was successfully deployed in zero-shot sim-to-real transfer on an onboard NUC, where it sustains a 100~Hz control rate and demonstrates robust, agile navigation in cluttered real-world environments. Our source code will be released for community reference.', 'abstract_zh': '自主航空车辆在复杂环境中的鲁棒自主导航是一项关键能力。然而，现代端到端导航面临一个关键挑战：敏捷飞行所需的高频控制环与受限于传感器更新率和显著计算成本的低频感知流之间的不匹配。这种不匹配迫使传统的同步模型进入不理想的低控制率。为了解决这一问题，我们提出了一种异步强化学习框架，解耦感知与控制，使高频策略能够基于最新的IMU状态进行即时反应，同时异步整合感知特征。为管理由此产生的数据陈旧性，我们引入了一个基于理论的时序编码模块（TEM），明确地将策略条件在感知延迟上，这一策略通过两阶段课程来确保稳定和高效的训练。该方法在广泛的仿真中得到验证，并成功在机载NUC上实现了零样本仿真实际转换，维持了100~Hz的控制率，并在杂乱的实际环境中展示了鲁棒性和灵活的导航能力。源代码将向社区公开。', 'title_zh': '临危不乱：面向实际空域导航的异步端到端学习'}
{'arxiv_id': 'arXiv:2509.13780', 'title': 'Behavior Foundation Model for Humanoid Robots', 'authors': 'Weishuai Zeng, Shunlin Lu, Kangning Yin, Xiaojie Niu, Minyue Dai, Jingbo Wang, Jiangmiao Pang', 'link': 'https://arxiv.org/abs/2509.13780', 'abstract': 'Whole-body control (WBC) of humanoid robots has witnessed remarkable progress in skill versatility, enabling a wide range of applications such as locomotion, teleoperation, and motion tracking. Despite these achievements, existing WBC frameworks remain largely task-specific, relying heavily on labor-intensive reward engineering and demonstrating limited generalization across tasks and skills. These limitations hinder their response to arbitrary control modes and restrict their deployment in complex, real-world scenarios. To address these challenges, we revisit existing WBC systems and identify a shared objective across diverse tasks: the generation of appropriate behaviors that guide the robot toward desired goal states. Building on this insight, we propose the Behavior Foundation Model (BFM), a generative model pretrained on large-scale behavioral datasets to capture broad, reusable behavioral knowledge for humanoid robots. BFM integrates a masked online distillation framework with a Conditional Variational Autoencoder (CVAE) to model behavioral distributions, thereby enabling flexible operation across diverse control modes and efficient acquisition of novel behaviors without retraining from scratch. Extensive experiments in both simulation and on a physical humanoid platform demonstrate that BFM generalizes robustly across diverse WBC tasks while rapidly adapting to new behaviors. These results establish BFM as a promising step toward a foundation model for general-purpose humanoid control.', 'abstract_zh': '全身体操控制（WBC）在类人机器人中的进展显著提高了技能的多样性，使其在行走、远程操控和运动跟踪等领域拥有广泛应用。尽管取得了这些成就，现有的WBC框架仍主要针对特定任务，依赖于劳动密集型的奖励工程，并在跨任务和技能的一般化方面表现有限。这些限制阻碍了其对任意控制模式的响应，并限制了其在复杂真实世界场景中的部署。为解决这些挑战，我们回顾了现有的WBC系统，并发现了一个贯穿不同任务的共同目标：生成适当的行為以引导机器人达到期望的目标状态。基于这一洞察，我们提出行为基础模型（BFM），这是一种基于大规模行为数据集预训练的生成模型，用于捕捉类人机器人中广泛且可重用的行为知识。BFM结合了带掩码的在线蒸馏框架和条件变分自编码器（CVAE），以建模行为分布，从而实现跨不同控制模式的灵活操作，并在无需从零重新训练的情况下高效获取新行为。在模拟和物理类人平台上的广泛实验表明，BFM在不同WBC任务中表现出强大的泛化能力，并能快速适应新行为。这些结果使BFM成为通用类人控制基础模型的一个有前景的步骤。', 'title_zh': '类人机器人行为基础模型'}
{'arxiv_id': 'arXiv:2509.13774', 'title': 'Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach', 'authors': 'Piaopiao Jin, Qi Wang, Guokang Sun, Ziwen Cai, Pinjia He, Yangwei You', 'link': 'https://arxiv.org/abs/2509.13774', 'abstract': 'Vision-language-action (VLA) models demonstrate strong generalization in robotic manipulation but face challenges in complex, real-world tasks. While supervised fine-tuning with demonstrations is constrained by data quality, reinforcement learning (RL) offers a promising alternative. We propose a human-in-the-loop dual-actor fine-tuning framework grounded in RL. The framework integrates a primary actor for robust multi-task performance with a refinement actor for latent-space adaptation. Beyond standard physical interventions, we introduce a lightweight talk-and-tweak scheme that converts human corrections into semantically grounded language commands, thereby generating a new dataset for policy learning. In real-world multi-task experiments, our approach achieves 100% success across three tasks within 101 minutes of online fine-tuning. For long-horizon tasks, it sustains a 50% success rate over 12 consecutive operations. Furthermore, the framework scales effectively to multi-robot training, achieving up to a 2 times improvement in efficiency when using dual robots. The experiment videos are available at this https URL.', 'abstract_zh': '基于RL的人机环路双actor细调框架：Vision-Language-Action (VLA)模型在机器人操作中表现出强大的泛化能力，但在复杂的真实世界任务中面临挑战。虽然监督细调受限于数据质量，强化学习(RL)提供了一种有前景的替代方案。我们提出了一种基于RL的人机环路双actor细调框架，该框架结合了主actor以实现稳健的多任务性能，并集成了一个精炼actor以适应潜在空间。在标准物理干预之外，我们引入了一种轻量级的“谈和调整”方案，将人类修正转化为语义上连贯的语言命令，从而生成用于策略学习的新数据集。在实际多任务实验中，我们的方法在101分钟的在线细调时间内实现三任务100%的成功率。对于长时间任务，它在12次连续操作中维持50%的成功率。此外，该框架在多机器人训练中能够有效扩展，使用双机器人时效率提高最多可达2倍。实验视频可在以下链接查看。', 'title_zh': 'VLA模型的双演员微调：一种谈练结合的人在回路方法'}
{'arxiv_id': 'arXiv:2509.13737', 'title': 'Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control', 'authors': 'Renjie Wang, Shangke Lyu, Donglin Wang', 'link': 'https://arxiv.org/abs/2509.13737', 'abstract': 'While Reinforcement Learning (RL) has achieved remarkable progress in legged locomotion control, it often suffers from performance degradation in out-of-distribution (OOD) conditions and discrepancies between the simulation and the real environments. Instead of mainly relying on domain randomization (DR) to best cover the real environments and thereby close the sim-to-real gap and enhance robustness, this work proposes an emerging decoupled framework that acquires fast online adaptation ability and mitigates the sim-to-real problems in unfamiliar environments by isolating stance-leg control and swing-leg control. Various simulation and real-world experiments demonstrate its effectiveness against horizontal force disturbances, uneven terrains, heavy and biased payloads, and sim-to-real gap.', 'abstract_zh': '基于腿足运动控制的强化学习在外域条件下的在线解耦框架：隔离支撑腿控制与摆动腿控制以缓解仿真实验与真实环境的差异', 'title_zh': '基于解耦反应力控制与步态控制的动态适应性腿式运动策略'}
{'arxiv_id': 'arXiv:2509.13733', 'title': 'FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph', 'authors': 'Xiaolin Zhou, Tingyang Xiao, Liu Liu, Yucheng Wang, Maiyue Chen, Xinrui Meng, Xinjie Wang, Wei Feng, Wei Sui, Zhizhong Su', 'link': 'https://arxiv.org/abs/2509.13733', 'abstract': 'Visual-Language Navigation (VLN) is a fundamental challenge in robotic systems, with broad applications for the deployment of embodied agents in real-world environments. Despite recent advances, existing approaches are limited in long-range spatial reasoning, often exhibiting low success rates and high inference latency, particularly in long-range navigation tasks. To address these limitations, we propose FSR-VLN, a vision-language navigation system that combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation supporting progressive retrieval, from coarse room-level localization to fine-grained goal view and object identification. Building on HMSG, FSR first performs fast matching to efficiently select candidate rooms, views, and objects, then applies VLM-driven refinement for final goal selection. We evaluated FSR-VLN across four comprehensive indoor datasets collected by humanoid robots, utilizing 87 instructions that encompass a diverse range of object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all datasets, measured by the retrieval success rate (RSR), while reducing the response time by 82% compared to VLM-based methods on tour videos by activating slow reasoning only when fast intuition fails. Furthermore, we integrate FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1 humanoid robot, enabling natural language interaction and real-time navigation.', 'abstract_zh': '视觉-语言导航（VLN）是机器人系统中的一个基本挑战，广泛应用于现实环境中嵌入式代理的部署。尽管近期有所进展，现有方法在长距离空间推理方面仍存在局限性，通常成功率低且推理延迟高，尤其是在长距离导航任务中。为解决这些问题，我们提出了FSR-VLN，这是一种结合了层次多模态场景图（HMSG）和快速到慢速导航推理（FSR）的视觉-语言导航系统。HMSG提供了一种支持逐步检索的多模态地图表示，从粗略的房间级定位到精细的目标视角和物体识别。基于HMSG，FSR首先执行快速匹配以高效地选择候选房间、视角和物体，然后采用VLM驱动的细化以进行最终目标选择。我们在四种综合室内数据集中评估了FSR-VLN，这些数据集由类人机器人收集，并涵盖了87条包含多种物体类别的指示。FSR-VLN在所有数据集中均达到了最先进的性能，通过RSR指标衡量，并与基于VLM的方法相比将响应时间降低了82%，仅在快速直觉失败时激活慢速推理。此外，我们将FSR-VLN与Unitree-G1类人机器人上的语音交互、规划和控制模块集成，实现了自然语言交互和实时导航。', 'title_zh': 'FSR-VLN：具有层次多模态场景图的快速和慢速推理视觉-语言导航'}
{'arxiv_id': 'arXiv:2509.13731', 'title': 'Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings', 'authors': 'Jeongwoo Park, Seabin Lee, Changmin Park, Wonjong Lee, Changjoo Nam', 'link': 'https://arxiv.org/abs/2509.13731', 'abstract': 'The industrial insertion of flexible flat cables (FFCs) into receptacles presents a significant challenge owing to the need for submillimeter precision when handling the deformable cables. In manufacturing processes, FFC insertion with robotic manipulators often requires laborious human-guided trajectory generation. While Reinforcement Learning (RL) offers a solution to automate this task without modeling complex properties of FFCs, the nondeterminism caused by the deformability of FFCs requires significant efforts and time on training. Moreover, training directly in a real environment is dangerous as industrial robots move fast and possess no safety measure. We propose an RL algorithm for FFC insertion that leverages a foundation model-based real-to-sim approach to reduce the training time and eliminate the risk of physical damages to robots and surroundings. Training is done entirely in simulation, allowing for random exploration without the risk of physical damages. Sim-to-real transfer is achieved through semantic segmentation masks which leave only those visual features relevant to the insertion tasks such as the geometric and spatial information of the cables and receptacles. To enhance generality, we use a foundation model, Segment Anything Model 2 (SAM2). To eleminate human intervention, we employ a Vision-Language Model (VLM) to automate the initial prompting of SAM2 to find segmentation masks. In the experiments, our method exhibits zero-shot capabilities, which enable direct deployments to real environments without fine-tuning.', 'abstract_zh': '基于基础模型的模拟到现实转换的柔性扁平电缆插入强化学习算法', 'title_zh': '工业环境中柔性电缆插入的强化学习方法'}
{'arxiv_id': 'arXiv:2509.13720', 'title': 'EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility', 'authors': 'Tianle Zeng, Jianwei Peng, Hanjing Ye, Guangcheng Chen, Senzi Luo, Hong Zhang', 'link': 'https://arxiv.org/abs/2509.13720', 'abstract': 'Zero-shot object navigation (ZSON) in large-scale outdoor environments faces many challenges; we specifically address a coupled one: long-range targets that reduce to tiny projections and intermittent visibility due to partial or complete occlusion. We present a unified, lightweight closed-loop system built on an aligned multi-scale image tile hierarchy. Through hierarchical target-saliency fusion, it summarizes localized semantic contrast into a stable coarse-layer regional saliency that provides the target direction and indicates target visibility. This regional saliency supports visibility-aware heading maintenance through keyframe memory, saliency-weighted fusion of historical headings, and active search during temporary invisibility. The system avoids whole-image rescaling, enables deterministic bottom-up aggregation, supports zero-shot navigation, and runs efficiently on a mobile robot. Across simulation and real-world outdoor trials, the system detects semantic targets beyond 150m, maintains a correct heading through visibility changes with 82.6% probability, and improves overall task success by 17.5% compared with the SOTA methods, demonstrating robust ZSON toward distant and intermittently observable targets.', 'abstract_zh': '面向大规模室外环境的零样本物体导航（ZSON）面临众多挑战；我们具体解决了其中一个耦合挑战：由于部分或完全遮挡导致的长距离目标减小为微小投影以及间歇性可见性。我们提出了一种基于对齐多尺度图像瓷砖层次结构的统一轻量级闭环系统。通过层次化目标显著性融合，将局部语义对比总结为稳定的大尺度区域显著性，提供目标方向并指示目标可见性。该区域显著性通过关键帧记忆、显著性加权的历史方向融合以及临时不可见时的主动搜索，支持可见性感知的方向维持。该系统避免了整个图像的缩放，实现了确定性的自底向上聚合，支持零样本导航，并能在移动机器人上高效运行。在仿真和实际室外试验中，该系统检测到超出150米的语义目标，通过可见性变化保持正确的方向概率为82.6%，相比当前先进方法提高整体任务成功率17.5%，展示了对远距离和间歇可见目标的强大零样本导航能力。', 'title_zh': 'EZREAL: 提升恶劣能见度下户外机器人远距离目标导航的零样本能力'}
{'arxiv_id': 'arXiv:2509.13666', 'title': 'DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring', 'authors': 'Zhenqi Wu, Abhinav Modi, Angelos Mavrogiannis, Kaustubh Joshi, Nikhil Chopra, Yiannis Aloimonos, Nare Karapetyan, Ioannis Rekleitis, Xiaomin Lin', 'link': 'https://arxiv.org/abs/2509.13666', 'abstract': 'The ocean is warming and acidifying, increasing the risk of mass mortality events for temperature-sensitive shellfish such as oysters. This motivates the development of long-term monitoring systems. However, human labor is costly and long-duration underwater work is highly hazardous, thus favoring robotic solutions as a safer and more efficient option. To enable underwater robots to make real-time, environment-aware decisions without human intervention, we must equip them with an intelligent "brain." This highlights the need for persistent,wide-area, and low-cost benthic monitoring. To this end, we present DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term underwater exploration and habitat monitoring. The results show that our framework is highly efficient in finding and exploring target objects (e.g., oysters, shipwrecks) without prior location information. In the oyster-monitoring task, our framework takes 31.5% less time than the previous baseline with the same amount of oysters. Compared to the vanilla VLM, it uses 23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our framework successfully explores and maps the wreck without collisions, requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage, while the vanilla model achieves 60.23% average coverage in our shipwreck environments.', 'abstract_zh': '基于视觉语言模型的长期水下自主探索与栖息地监测框架', 'title_zh': 'DREAM: 领域aware推理实现高效自主水下监控'}
{'arxiv_id': 'arXiv:2509.13595', 'title': 'Leg-Arm Coordinated Operation for Curtain Wall Installation', 'authors': 'Xiao Liu, Weijun Wang, Tianlun Huang, Zhiyong Wang, Wei Feng', 'link': 'https://arxiv.org/abs/2509.13595', 'abstract': 'With the acceleration of urbanization, the number of high-rise buildings and large public facilities is increasing, making curtain walls an essential component of modern architecture with widespread applications. Traditional curtain wall installation methods face challenges such as variable on-site terrain, high labor intensity, low construction efficiency, and significant safety risks. Large panels often require multiple workers to complete installation. To address these issues, based on a hexapod curtain wall installation robot, we design a hierarchical optimization-based whole-body control framework for coordinated arm-leg planning tailored to three key tasks: wall installation, ceiling installation, and floor laying. This framework integrates the motion of the hexapod legs with the operation of the folding arm and the serial-parallel manipulator. We conduct experiments on the hexapod curtain wall installation robot to validate the proposed control method, demonstrating its capability in performing curtain wall installation tasks. Our results confirm the effectiveness of the hierarchical optimization-based arm-leg coordination framework for the hexapod robot, laying the foundation for its further application in complex construction site environments.', 'abstract_zh': '随着城市化的加速，高层建筑和大型公共设施的数量增加，使幕墙成为现代建筑中广泛应用的重要组成部分。传统的幕墙安装方法面临现场地形变化、高劳动强度、低施工效率和显著的安全风险等挑战。大型面板的安装往往需要多名工人共同完成。为解决这些问题，基于六足幕墙安装机器人，我们设计了一种基于分层优化的整体运动控制框架，适用于墙面安装、天花板安装和楼面铺设三大关键任务的协调臂腿规划。该框架将六足机器人腿的运动与折叠臂和串联并联 manipulator 的操作集成在一起。我们对六足幕墙安装机器人进行了实验，验证了所提出控制方法的有效性，展示了其在执行幕墙安装任务方面的能力。实验结果证实了基于分层优化的臂腿协调控制框架在六足机器人中的有效性，为其实现复杂施工环境应用奠定了基础。', 'title_zh': '幕wall安装的腿臂协调操作'}
{'arxiv_id': 'arXiv:2509.13574', 'title': 'Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic Policies: Mitigating Multi-Step Inference Degradation', 'authors': 'Zidong Chen, Zihao Guo, Peng Wang, ThankGod Itua Egbe, Yan Lyu, Chenghao Qian', 'link': 'https://arxiv.org/abs/2509.13574', 'abstract': 'Flow matching has emerged as a competitive framework for learning high-quality generative policies in robotics; however, we find that generalisation arises and saturates early along the flow trajectory, in accordance with recent findings in the literature. We further observe that increasing the number of Euler integration steps during inference counter-intuitively and universally degrades policy performance. We attribute this to (i) additional, uniformly spaced integration steps oversample the late-time region, thereby constraining actions towards the training trajectories and reducing generalisation; and (ii) the learned velocity field becoming non-Lipschitz as integration time approaches 1, causing instability. To address these issues, we propose a novel policy that utilises non-uniform time scheduling (e.g., U-shaped) during training, which emphasises both early and late temporal stages to regularise policy training, and a dense-jump integration schedule at inference, which uses a single-step integration to replace the multi-step integration beyond a jump point, to avoid unstable areas around 1. Essentially, our policy is an efficient one-step learner that still pushes forward performance through multi-step integration, yielding up to 23.7% performance gains over state-of-the-art baselines across diverse robotic tasks.', 'abstract_zh': '流匹配技术已被证明是机器人领域学习高质量生成策略的竞争性框架；然而，我们发现泛化现象在流轨迹早期就已出现并趋于饱和，这与近期文献中的发现一致。我们进一步观察到，在推理过程中增加欧拉积分步骤反而普遍地降低了策略性能。我们将其归因于：（i）额外的均匀分布的积分步骤过度采样晚期区域，从而限制了动作向训练轨迹靠拢，降低了泛化能力；以及（ii）随着积分时间接近1，所学的速度场变得非Lipschitz，导致不稳定性。为了解决这些问题，我们提出了一种新的策略，该策略在训练过程中采用非均匀时间调度（例如U形），强调早期和晚期的时间阶段以规范策略训练，并在推理过程中采用密集跳跃积分调度，使用单步积分替代跳点之后的多步积分，以避免接近1的不稳定区域。本质上，我们的策略是一种高效的单步学习者，仍然通过多步积分促进性能提升，在多种机器人任务上相较于现有最佳基线获得高达23.7%的性能提升。', 'title_zh': '基于非均匀时间调度的密集-跳跃流匹配方法：缓解多步推理退化'}
{'arxiv_id': 'arXiv:2509.13572', 'title': 'Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference', 'authors': 'Ozan Karaali, Hossam Farag, Strahinja Dosen, Cedomir Stefanovic', 'link': 'https://arxiv.org/abs/2509.13572', 'abstract': 'This study examines the potential of utilizing Vision Language Models (VLMs) to improve the perceptual capabilities of semi-autonomous prosthetic hands. We introduce a unified benchmark for end-to-end perception and grasp inference, evaluating a single VLM to perform tasks that traditionally require complex pipelines with separate modules for object detection, pose estimation, and grasp planning. To establish the feasibility and current limitations of this approach, we benchmark eight contemporary VLMs on their ability to perform a unified task essential for bionic grasping. From a single static image, they should (1) identify common objects and their key properties (name, shape, orientation, and dimensions), and (2) infer appropriate grasp parameters (grasp type, wrist rotation, hand aperture, and number of fingers). A corresponding prompt requesting a structured JSON output was employed with a dataset of 34 snapshots of common objects. Key performance metrics, including accuracy for categorical attributes (e.g., object name, shape) and errors in numerical estimates (e.g., dimensions, hand aperture), along with latency and cost, were analyzed. The results demonstrated that most models exhibited high performance in object identification and shape recognition, while accuracy in estimating dimensions and inferring optimal grasp parameters, particularly hand rotation and aperture, varied more significantly. This work highlights the current capabilities and limitations of VLMs as advanced perceptual modules for semi-autonomous control of bionic limbs, demonstrating their potential for effective prosthetic applications.', 'abstract_zh': '本研究探讨了利用视觉语言模型（VLMs）提高半自主假手感知能力的潜力。我们引入了一个统一的基准，用于评估单一VLM进行传统的复杂流水线所要求的任务，该流水线由物体检测、姿态估计和抓取规划等多个模块组成。为了验证该方法的可行性和当前的局限性，我们在34个常见物体的快照数据集上对八种当代VLM进行基准测试，评估它们执行一个对仿生抓取至关重要的统一任务的能力。从单一静态图像中，它们应（1）识别常见物体及其关键属性（名称、形状、朝向和尺寸），以及（2）推断合适的抓取参数（抓取类型、腕部旋转、手指张开度和手指数量）。采用请求结构化JSON输出的提示进行评估。关键性能指标包括类别属性的准确性（如对象名称、形状）和数值估计的误差（如尺寸、手指张开度），以及延迟和成本。研究结果表明，大多数模型在物体识别和形状识别方面表现出了高效率，但在估计尺寸和推断最优抓取参数的准确性上，尤其是在手部旋转和张开度方面，差异较大。本工作突显了VLMs作为半自主控制仿生肢体高级感知模块的当前能力和局限性，展示了其在假肢应用中的潜在有效性。', 'title_zh': '使用视觉语言模型控制仿生手：物体感知和抓取推理评估'}
{'arxiv_id': 'arXiv:2509.13534', 'title': 'Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning', 'authors': 'Chunxin Zheng, Kai Chen, Zhihai Bi, Yulin Li, Liang Pan, Jinni Zhou, Haoang Li, Jun Ma', 'link': 'https://arxiv.org/abs/2509.13534', 'abstract': 'Whole-body manipulation (WBM) for humanoid robots presents a promising approach for executing embracing tasks involving bulky objects, where traditional grasping relying on end-effectors only remains limited in such scenarios due to inherent stability and payload constraints. This paper introduces a reinforcement learning framework that integrates a pre-trained human motion prior with a neural signed distance field (NSDF) representation to achieve robust whole-body embracing. Our method leverages a teacher-student architecture to distill large-scale human motion data, generating kinematically natural and physically feasible whole-body motion patterns. This facilitates coordinated control across the arms and torso, enabling stable multi-contact interactions that enhance the robustness in manipulation and also the load capacity. The embedded NSDF further provides accurate and continuous geometric perception, improving contact awareness throughout long-horizon tasks. We thoroughly evaluate the approach through comprehensive simulations and real-world experiments. The results demonstrate improved adaptability to diverse shapes and sizes of objects and also successful sim-to-real transfer. These indicate that the proposed framework offers an effective and practical solution for multi-contact and long-horizon WBM tasks of humanoid robots.', 'abstract_zh': '全身心 manip 操作（Whole-body Manipulation, WBM）中的人形机器人拥抱任务：基于预训练人类运动先验与神经-signed距离场表示的强化学习框架', 'title_zh': '拥抱 bulky 物体的人形机器人：基于强化学习的全身 manipulation'}
{'arxiv_id': 'arXiv:2509.13386', 'title': 'VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization', 'authors': 'Hansol Lim, Minhyeok Im, Jonathan Boyack, Jee Won Lee, Jongseong Brad Choi', 'link': 'https://arxiv.org/abs/2509.13386', 'abstract': "Demands for software-defined vehicles (SDV) are rising and electric vehicles (EVs) are increasingly being equipped with powerful computers. This enables onboard AI systems to optimize charge-aware path optimization customized to reflect vehicle's current condition and environment. We present VEGA, a charge-aware EV navigation agent that plans over a charger-annotated road graph using Proximal Policy Optimization (PPO) with budgeted A* teacher-student guidance under state-of-charge (SoC) feasibility. VEGA consists of two modules. First, a physics-informed neural operator (PINO), trained on real vehicle speed and battery-power logs, uses recent vehicle speed logs to estimate aerodynamic drag, rolling resistance, mass, motor and regenerative-braking efficiencies, and auxiliary load by learning a vehicle-custom dynamics. Second, a Reinforcement Learning (RL) agent uses these dynamics to optimize a path with optimal charging stops and dwell times under SoC constraints. VEGA requires no additional sensors and uses only vehicle speed signals. It may serve as a virtual sensor for power and efficiency to potentially reduce EV cost. In evaluation on long routes like San Francisco to New York, VEGA's stops, dwell times, SoC management, and total travel time closely track Tesla Trip Planner while being slightly more conservative, presumably due to real vehicle conditions such as vehicle parameter drift due to deterioration. Although trained only in U.S. regions, VEGA was able to compute optimal charge-aware paths in France and Japan, demonstrating generalizability. It achieves practical integration of physics-informed learning and RL for EV eco-routing.", 'abstract_zh': '基于充电意识的软件定义车辆导航代理VEGA：结合proximal策略优化与预算A*教师-学生指导的电动车辆路径优化', 'title_zh': 'VEGA：基于物理知情神经算子和近端策略优化的电动汽车导航代理'}
{'arxiv_id': 'arXiv:2509.13331', 'title': 'Explainable AI-Enhanced Supervisory Control for High-Precision Spacecraft Formation', 'authors': 'Reza Pirayeshshirazinezhad', 'link': 'https://arxiv.org/abs/2509.13331', 'abstract': 'We use artificial intelligence (AI) and supervisory adaptive control systems to plan and optimize the mission of precise spacecraft formation. Machine learning and robust control enhance the efficiency of spacecraft precision formation of the Virtual Telescope for X-ray Observation (VTXO) space mission. VTXO is a precise formation of two separate spacecraft making a virtual telescope with a one-kilometer focal length. One spacecraft carries the lens and the other spacecraft holds the camera to observe high-energy space objects in the X-ray domain with 55 milli-arcsecond angular resolution accuracy. Timed automata for supervisory control, Monte Carlo simulations for stability and robustness evaluation, and integration of deep neural networks for optimal estimation of mission parameters, satisfy the high precision mission criteria. We integrate deep neural networks with a constrained, non-convex dynamic optimization pipeline to predict optimal mission parameters, ensuring precision mission criteria are met. AI framework provides explainability by predicting the resulting energy consumption and mission error for a given set of mission parameters. It allows for transparent, justifiable, and real-time trade-offs, a capability not present in traditional adaptive controllers. The results show reductions in energy consumption and improved mission accuracy, demonstrating the capability of the system to address dynamic uncertainties and disturbances.', 'abstract_zh': '我们使用人工智能（AI）和监督自适应控制系统来规划和优化精准航天器编队的任务。机器学习和稳健控制提高了虚拟X射线观测望远镜（VTXO）空间任务中航天器精准编队的效率。VTXO是由两个分离的航天器组成，形成一个焦距为一千米的虚拟望远镜，其中一个航天器携带透镜，另一个航天器搭载相机，以55毫秒角分辨率精确观测高能空间目标。监督控制的时间自动机、蒙特卡洛模拟的稳定性和鲁棒性评估，以及深度神经网络的集成用于优化任务参数估计，满足高精度任务标准。我们将深度神经网络与约束的非凸动态优化管道结合使用，以预测最优任务参数，确保满足精度任务标准。AI框架通过预测给定任务参数集下的能量消耗和任务误差提供了可解释性，允许进行透明、可验证和实时的权衡，这是传统自适应控制器不具备的能力。结果显示，能量消耗减少且任务精确度提高，证明了系统的动态不确定性与干扰处理能力。', 'title_zh': '可解释的人工智能增强监督控制以实现高精度航天器编队飞行'}
{'arxiv_id': 'arXiv:2509.14195', 'title': 'Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning', 'authors': 'Shalima Binta Manir, Tim Oates', 'link': 'https://arxiv.org/abs/2509.14195', 'abstract': "Mental representation, characterized by structured internal models mirroring external environments, is fundamental to advanced cognition but remains challenging to investigate empirically. Existing theory hypothesizes that second-order learning -- learning mechanisms that adapt first-order learning (i.e., learning about the task/domain) -- promotes the emergence of such environment-cognition isomorphism. In this paper, we empirically validate this hypothesis by proposing a hierarchical architecture comprising a Graph Convolutional Network (GCN) as a first-order learner and an MLP controller as a second-order learner. The GCN directly maps node-level features to predictions of optimal navigation paths, while the MLP dynamically adapts the GCN's parameters when confronting structurally novel maze environments. We demonstrate that second-order learning is particularly effective when the cognitive system develops an internal mental map structurally isomorphic to the environment. Quantitative and qualitative results highlight significant performance improvements and robust generalization on unseen maze tasks, providing empirical support for the pivotal role of structured mental representations in maximizing the effectiveness of second-order learning.", 'abstract_zh': '结构化的内部模型促进了环境认知同构性的高级认知过程：通过层次架构验证第二级学习的有效性', 'title_zh': '层次学习在迷宫导航中的应用：通过二次学习涌现心理表征'}
{'arxiv_id': 'arXiv:2509.13968', 'title': 'Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks', 'authors': 'Konstantinos Voudouris, Andrew Barron, Marta Halina, Colin Klein, Matishalin Patel', 'link': 'https://arxiv.org/abs/2509.13968', 'abstract': 'Transitional accounts of evolution emphasise a few changes that shape what is evolvable, with dramatic consequences for derived lineages. More recently it has been proposed that cognition might also have evolved via a series of major transitions that manipulate the structure of biological neural networks, fundamentally changing the flow of information. We used idealised models of information flow, artificial neural networks (ANNs), to evaluate whether changes in information flow in a network can yield a transitional change in cognitive performance. We compared networks with feed-forward, recurrent and laminated topologies, and tested their performance learning artificial grammars that differed in complexity, controlling for network size and resources. We documented a qualitative expansion in the types of input that recurrent networks can process compared to feed-forward networks, and a related qualitative increase in performance for learning the most complex grammars. We also noted how the difficulty in training recurrent networks poses a form of transition barrier and contingent irreversibility -- other key features of evolutionary transitions. Not all changes in network topology confer a performance advantage in this task set. Laminated networks did not outperform non-laminated networks in grammar learning. Overall, our findings show how some changes in information flow can yield transitions in cognitive performance.', 'abstract_zh': '过渡演变的概括强调了少数变化如何塑造可演变性，并对衍生谱系产生了重大影响。最近有观点提出，认知也可能通过一系列主要过渡演变来发展，操纵生物神经网络的结构，从根本上改变信息流。我们使用理想的 INFORMATION FLOW 模型和人工神经网络（ANNs）来评估网络中信息流的变化是否能导致认知表现的过渡变化。我们比较了前馈、反馈和层状网络结构，并测试了它们在不同复杂度的人工文法学习中的表现，同时控制了网络规模和资源。我们记录了反馈网络相较于前馈网络在处理输入类型上的定性扩展，以及在学习最复杂文法方面相关的表现提升。我们还注意到，在训练反馈网络时出现的难度为过渡障碍和历史不可逆性提供了形式上的支持——这些都是演化过渡的关键特征。并非所有网络拓扑结构的变化都在此任务集中提供表现优势。层状网络在文法学习中并未优于非层状网络。总体而言，我们的发现显示了信息流某些变化如何导致认知表现的过渡变化。', 'title_zh': '探索生物认知进化中的重大转变人工神经网络研究'}
{'arxiv_id': 'arXiv:2509.13389', 'title': 'From Next Token Prediction to (STRIPS) World Models -- Preliminary Results', 'authors': 'Carlos Núñez-Molina, Vicenç Gómez, Hector Geffner', 'link': 'https://arxiv.org/abs/2509.13389', 'abstract': 'We consider the problem of learning propositional STRIPS world models from action traces alone, using a deep learning architecture (transformers) and gradient descent. The task is cast as a supervised next token prediction problem where the tokens are the actions, and an action $a$ may follow an action sequence if the hidden effects of the previous actions do not make an action precondition of $a$ false. We show that a suitable transformer architecture can faithfully represent propositional STRIPS world models, and that the models can be learned from sets of random valid (positive) and invalid (negative) action sequences alone. A number of experiments are reported.', 'abstract_zh': '仅从动作轨迹学习命题STRIPS世界模型：基于变压器和梯度下降的监督下一令牌预测问题', 'title_zh': '从下一个token预测到STRIPS世界模型——初步结果'}
{'arxiv_id': 'arXiv:2509.13347', 'title': 'OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft', 'authors': 'Zihao Wang, Muyao Li, Kaichen He, Xiangyu Wang, Zhancun Mu, Anji Liu, Yitao Liang', 'link': 'https://arxiv.org/abs/2509.13347', 'abstract': 'The choice of action spaces is a critical yet unresolved challenge in developing capable, end-to-end trainable agents. This paper first presents a large-scale, systematic comparison of prominent abstracted action spaces and tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the open-ended Minecraft. Our analysis reveals that no single action space is universally optimal; instead, the most effective abstraction is highly task-dependent, creating a dilemma for building generalist agents. To resolve this, we introduce Chain of Action (CoA), a novel framework that unifies high-level planning and low-level control within a single, monolithic VLA model. CoA treats an abstracted action not as a command for a separate policy, but as an intermediate reasoning step--akin to a chain of thought--that guides the generation of the final, executable action. Furthermore, we demonstrate that an All-in-One agent trained on a diverse mixture of action spaces using the CoA paradigm learns a more robust and generalizable policy. This unified agent achieves a new state-of-the-art, improving the overall task success rate over strong, specialized baselines. To foster reproducible research, we release the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive benchmark of over 800 distinct tasks, curated datasets, source code, and all pretrained model checkpoints at this https URL', 'abstract_zh': '行动空间的选择是开发具备端到端训练能力的智能代理的一个至关重要的但尚未解决的挑战。本文首先在开放世界Minecraft中系统比较了多种著名的抽象行动空间和标记化方法，涵盖视觉-语言-行动（VLA）或分层代理模型。我们的分析表明，并没有单一的行动空间是普适最优的；相反，最有效的抽象高度依赖于具体任务，这为构建通才代理带来了挑战。为了解决这一问题，我们引入了行动链（CoA）这一新颖框架，该框架在单一的、模块化的VLA模型中统一了高层规划和低层控制。CoA 将抽象动作视为一个逐步推理步骤——类似于思维链——引导生成最终可执行的动作。此外，我们证明，采用CoA框架在多种行动空间混合数据上进行训练的全功能代理学会了更加稳健和通用的策略。该统一代理在多个任务上的成功率超过专门基线，创造了新的研究前沿。为了促进可再现研究，我们发布了OpenHA（开放分层次代理）套件，包括超过800个独特任务的全面基准、精心策划的数据集、源代码和所有预训练模型检查点（https://openha.github.io）。', 'title_zh': 'OpenHA：Minecraft中的一系列开源分级代理模型'}
{'arxiv_id': 'arXiv:2509.13341', 'title': 'Imagined Autocurricula', 'authors': 'Ahmet H. Güzel, Matthew Thomas Jackson, Jarek Luca Liesen, Tim Rocktäschel, Jakob Nicolaus Foerster, Ilija Bogunovic, Jack Parker-Holder', 'link': 'https://arxiv.org/abs/2509.13341', 'abstract': 'Training agents to act in embodied environments typically requires vast training data or access to accurate simulation, neither of which exists for many cases in the real world. Instead, world models are emerging as an alternative leveraging offline, passively collected data, they make it possible to generate diverse worlds for training agents in simulation. In this work, we harness world models to generate imagined environments to train robust agents capable of generalizing to novel task variations. One of the challenges in doing this is ensuring the agent trains on useful generated data. We thus propose a novel approach, IMAC (Imagined Autocurricula), leveraging Unsupervised Environment Design (UED), which induces an automatic curriculum over generated worlds. In a series of challenging, procedurally generated environments, we show it is possible to achieve strong transfer performance on held-out environments, having trained only inside a world model learned from a narrower dataset. We believe this opens the path to utilizing larger-scale, foundation world models for generally capable agents.', 'abstract_zh': '利用世界模型生成想象环境训练 robust 代理以实现泛化的转移性能', 'title_zh': '想象自动生成课程'}
{'arxiv_id': 'arXiv:2509.13888', 'title': 'Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification', 'authors': 'Mariano Barone, Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato', 'link': 'https://arxiv.org/abs/2509.13888', 'abstract': 'Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: this https URL', 'abstract_zh': '医疗保健中的虚假信息，从疫苗犹豫到未证实的治疗方法，对公共卫生和对医疗系统的信任构成了风险。尽管机器学习和自然语言处理已经推动了自动事实核查的发展，但由于复杂的术语、领域专业知识的需要以及基于科学证据的至关重要性，验证 biomedical 声明仍然具有独特的挑战性。我们提出了一种名为 CER（Combining Evidence and Reasoning）的新颖框架，该框架结合了科学证据检索、通过大规模语言模型进行推理以及监督真伪预测。通过将大规模语言模型的文本生成能力与高质量 biomedical 科学证据的先进检索技术相结合，CER 有效地减轻了幻觉风险，确保生成的输出基于可核实、基于证据的来源。在专家标注数据集（HealthFC、BioASQ-7b、SciFact）上的评估显示了最先进的性能和跨数据集的有前景的一般化能力。完整代码和数据已公开以确保透明性和可再现性：this https URL。', 'title_zh': '通过多模态声明检测与基于证据的验证对抗 biomedical 领域的虚假信息'}
{'arxiv_id': 'arXiv:2509.13792', 'title': 'Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation', 'authors': 'Inder Pal Singh, Nidhal Eddine Chenni, Abd El Rahman Shabayek, Arunkumar Rathinam, Djamila Aouada', 'link': 'https://arxiv.org/abs/2509.13792', 'abstract': 'Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous space operations such as rendezvous, docking, and in-orbit servicing. Hybrid pipelines that combine object detection, keypoint regression, and Perspective-n-Point (PnP) solvers have recently achieved strong results on synthetic datasets, yet their performance deteriorates sharply on real or lab-generated imagery due to the persistent synthetic-to-real domain gap. Existing unsupervised domain adaptation approaches aim to mitigate this issue but often underperform when a modest number of labeled target samples are available. In this work, we propose the first Supervised Domain Adaptation (SDA) framework tailored for SPE keypoint regression. Building on the Learning Invariant Representation and Risk (LIRR) paradigm, our method jointly optimizes domain-invariant representations and task-specific risk using both labeled synthetic and limited labeled real data, thereby reducing generalization error under domain shift. Extensive experiments on the SPEED+ benchmark demonstrate that our approach consistently outperforms source-only, fine-tuning, and oracle baselines. Notably, with only 5% labeled target data, our method matches or surpasses oracle performance trained on larger fractions of labeled data. The framework is lightweight, backbone-agnostic, and computationally efficient, offering a practical pathway toward robust and deployable spacecraft pose estimation in real-world space environments.', 'abstract_zh': '空间探测器姿态估计的监督域自适应方法', 'title_zh': '合成与现实之间的鸿沟：监督领域适应算法在鲁棒航天器6自由度姿态估计中的应用'}
