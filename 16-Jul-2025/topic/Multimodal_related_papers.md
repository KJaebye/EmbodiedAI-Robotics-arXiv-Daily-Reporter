# MMOne: Representing Multiple Modalities in One Scene 

**Title (ZH)**: MMOne: 在一个场景中表示多种模态 

**Authors**: Zhifeng Gu, Bing Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.11129)  

**Abstract**: Humans perceive the world through multimodal cues to understand and interact with the environment. Learning a scene representation for multiple modalities enhances comprehension of the physical world. However, modality conflicts, arising from inherent distinctions among different modalities, present two critical challenges: property disparity and granularity disparity. To address these challenges, we propose a general framework, MMOne, to represent multiple modalities in one scene, which can be readily extended to additional modalities. Specifically, a modality modeling module with a novel modality indicator is proposed to capture the unique properties of each modality. Additionally, we design a multimodal decomposition mechanism to separate multi-modal Gaussians into single-modal Gaussians based on modality differences. We address the essential distinctions among modalities by disentangling multimodal information into shared and modality-specific components, resulting in a more compact and efficient multimodal scene representation. Extensive experiments demonstrate that our method consistently enhances the representation capability for each modality and is scalable to additional modalities. The code is available at this https URL. 

**Abstract (ZH)**: 人类通过多模态 cues 认知和交互环境。学习多模态场景表示增强对物理世界的理解。然而，不同模态固有差异导致的模态冲突——属性差异和粒度差异——构成了两个关键挑战。为应对这些挑战，我们提出了一种通用框架 MMOne，能够在一个场景中表示多种模态，并可便捷扩展至其他模态。具体而言，我们提出了一种包含新型模态指示器的模态建模模块来捕获每种模态的独特属性。此外，我们设计了一种多模态分解机制，根据模态差异将多模态高斯分布分解为单模态高斯分布。通过将多模态信息拆解为共享和模态特定组件，我们解决了模态间的基本差异，从而获得更为紧凑和高效的多模态场景表示。大量实验表明，我们的方法能够一致地提升每种模态的表示能力，并具备扩展到其他模态的潜力。代码已发布在该网址。 

---
