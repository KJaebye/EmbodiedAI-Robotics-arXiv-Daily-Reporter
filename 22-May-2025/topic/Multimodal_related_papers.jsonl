{'arxiv_id': 'arXiv:2505.15687', 'title': 'Discovering Pathology Rationale and Token Allocation for Efficient Multimodal Pathology Reasoning', 'authors': 'Zhe Xu, Cheng Jin, Yihui Wang, Ziyi Liu, Hao Chen', 'link': 'https://arxiv.org/abs/2505.15687', 'abstract': 'Multimodal pathological image understanding has garnered widespread interest due to its potential to improve diagnostic accuracy and enable personalized treatment through integrated visual and textual data. However, existing methods exhibit limited reasoning capabilities, which hamper their ability to handle complex diagnostic scenarios. Additionally, the enormous size of pathological images leads to severe computational burdens, further restricting their practical deployment. To address these limitations, we introduce a novel bilateral reinforcement learning framework comprising two synergistic branches. One reinforcement branch enhances the reasoning capability by enabling the model to learn task-specific decision processes, i.e., pathology rationales, directly from labels without explicit reasoning supervision. While the other branch dynamically allocates a tailored number of tokens to different images based on both their visual content and task context, thereby optimizing computational efficiency. We apply our method to various pathological tasks such as visual question answering, cancer subtyping, and lesion detection. Extensive experiments show an average +41.7 absolute performance improvement with 70.3% lower inference costs over the base models, achieving both reasoning accuracy and computational efficiency.', 'abstract_zh': '多模态病理图像理解因其实现诊断准确性和个性化治疗的潜力而引起了广泛关注，但由于现有方法的推理能力有限，无法有效处理复杂的诊断场景。此外，病理图像的巨大尺寸导致严重的计算负担，进一步限制了其实用部署。为解决这些限制，我们提出了一种新颖的双边强化学习框架，该框架包含两个协同的分支。一个强化分支通过使模型直接从标签中学习任务特定的决策过程，即病理推理，来增强推理能力，无需显式的推理监督。而另一个分支则根据图像的视觉内容和任务上下文动态分配定制数量的标记，从而优化计算效率。我们将该方法应用于各种病理任务，如视觉问答、癌症亚型分类和病灶检测。广泛的实验结果显示，与基线模型相比，我们的方法在平均绝对性能上提高了41.7%，同时将推理成本降低了70.3%，实现了推理准确性和计算效率的双重提升。', 'title_zh': '发现病理推理的病理学依据和令牌分配以实现高效多模态病理学推理'}
{'arxiv_id': 'arXiv:2505.15447', 'title': 'ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning', 'authors': 'Ziqiang Xu, Qi Dai, Tian Xie, Yifan Yang, Kai Qiu, DongDong Chen, Zuxuan Wu, Chong Luo', 'link': 'https://arxiv.org/abs/2505.15447', 'abstract': 'Video understanding is inherently intention-driven-humans naturally focus on relevant frames based on their goals. Recent advancements in multimodal large language models (MLLMs) have enabled flexible query-driven reasoning; however, video-based frameworks like Video Chain-of-Thought lack direct training signals to effectively identify relevant frames. Current approaches often rely on heuristic methods or pseudo-label supervised annotations, which are both costly and limited in scalability across diverse scenarios. To overcome these challenges, we introduce ViaRL, the first framework to leverage rule-based reinforcement learning (RL) for optimizing frame selection in intention-driven video understanding. An iterated amplification strategy is adopted to perform alternating cyclic training in the video CoT system, where each component undergoes iterative cycles of refinement to improve its capabilities. ViaRL utilizes the answer accuracy of a downstream model as a reward signal to train a frame selector through trial-and-error, eliminating the need for expensive annotations while closely aligning with human-like learning processes. Comprehensive experiments across multiple benchmarks, including VideoMME, LVBench, and MLVU, demonstrate that ViaRL consistently delivers superior temporal grounding performance and robust generalization across diverse video understanding tasks, highlighting its effectiveness and scalability. Notably, ViaRL achieves a nearly 15\\% improvement on Needle QA, a subset of MLVU, which is required to search a specific needle within a long video and regarded as one of the most suitable benchmarks for evaluating temporal grounding.', 'abstract_zh': '基于规则的强化学习驱动的视频理解中的片段选择框架', 'title_zh': 'ViaRL：基于视觉迭代放大强化学习的自适应时空定位'}
{'arxiv_id': 'arXiv:2505.15058', 'title': 'AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars', 'authors': 'Tianbao Zhang, Jian Zhao, Yuer Li, Zheng Zhu, Ping Hu, Zhaoxin Fan, Wenjun Wu, Xuelong Li', 'link': 'https://arxiv.org/abs/2505.15058', 'abstract': 'Whole-body audio-driven avatar pose and expression generation is a critical task for creating lifelike digital humans and enhancing the capabilities of interactive virtual agents, with wide-ranging applications in virtual reality, digital entertainment, and remote communication. Existing approaches often generate audio-driven facial expressions and gestures independently, which introduces a significant limitation: the lack of seamless coordination between facial and gestural elements, resulting in less natural and cohesive animations. To address this limitation, we propose AsynFusion, a novel framework that leverages diffusion transformers to achieve harmonious expression and gesture synthesis. The proposed method is built upon a dual-branch DiT architecture, which enables the parallel generation of facial expressions and gestures. Within the model, we introduce a Cooperative Synchronization Module to facilitate bidirectional feature interaction between the two modalities, and an Asynchronous LCM Sampling strategy to reduce computational overhead while maintaining high-quality outputs. Extensive experiments demonstrate that AsynFusion achieves state-of-the-art performance in generating real-time, synchronized whole-body animations, consistently outperforming existing methods in both quantitative and qualitative evaluations.', 'abstract_zh': '全身音频驱动的角色姿态与表情生成是创建真实感数字人类和增强交互式虚拟代理能力的关键任务，广泛应用于虚拟现实、数字娱乐和远程通信。现有的方法通常独立生成音频驱动的面部表情和手势，这引入了一个重要限制：面部和手势元素之间缺乏无缝协调，导致生成的动画不够自然和一致。为了解决这一限制，我们提出了一种新颖的框架AsynFusion，利用扩散变压器实现和谐的表情和手势合成。该方法基于双支路DiT架构，能够并行生成面部表情和手势。在模型中，我们引入了协同同步模块以促进两种模态之间的双向特征交互，并采用了异步LCM采样策略以在保持高质量输出的同时减少计算开销。广泛的实验表明，AsynFusion在生成实时同步的全身动画方面达到了最先进的性能，定量和定性评估均优于现有方法。', 'title_zh': 'AsynFusion: 向异步潜在一致性模型的方向发展，用于解耦的全身音driven动画角色'}
{'arxiv_id': 'arXiv:2505.14714', 'title': 'KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection', 'authors': 'Tuan-Vinh La, Minh-Hieu Nguyen, Minh-Son Dao', 'link': 'https://arxiv.org/abs/2505.14714', 'abstract': 'Fake news detection remains a challenging problem due to the complex interplay between textual misinformation, manipulated images, and external knowledge reasoning. While existing approaches have achieved notable results in verifying veracity and cross-modal consistency, two key challenges persist: (1) Existing methods often consider only the global image context while neglecting local object-level details, and (2) they fail to incorporate external knowledge and entity relationships for deeper semantic understanding. To address these challenges, we propose a novel multi-modal fake news detection framework that integrates visual, textual, and knowledge-based representations. Our approach leverages bottom-up attention to capture fine-grained object details, CLIP for global image semantics, and RoBERTa for context-aware text encoding. We further enhance knowledge utilization by retrieving and adaptively selecting relevant entities from a knowledge graph. The fused multi-modal features are processed through a Transformer-based classifier to predict news veracity. Experimental results demonstrate that our model outperforms recent approaches, showcasing the effectiveness of neighbor selection mechanism and multi-modal fusion for fake news detection. Our proposal introduces a new paradigm: knowledge-grounded multimodal reasoning. By integrating explicit entity-level selection and NLI-guided filtering, we shift fake news detection from feature fusion to semantically grounded verification. For reproducibility and further research, the source code is publicly at \\href{this https URL}{this http URL}.', 'abstract_zh': '虚假新闻检测仍然是一个具有挑战性的问题，由于文本误导信息、操纵图像和外部知识推理之间的复杂相互作用。虽然现有的方法在验证真实性和多模态一致性方面已取得显著成果，但仍然存在两个关键挑战：（1）现有方法通常只考虑全局图像上下文，而忽略局部对象级别的细节；（2）它们未能整合外部知识和实体关系以实现更深入的意义理解。为了解决这些挑战，我们提出了一种新的多模态虚假新闻检测框架，结合了视觉、文本和知识基表示。我们的方法利用自底向上的注意力机制捕捉细微的对象细节，使用CLIP提取全局图像语义，并使用RoBERTa进行上下文感知文本编码。为进一步利用知识，我们通过检索知识图谱中相关实体并进行适应性选择来增强知识的利用。融合的多模态特征通过基于变换器的分类器进行处理，以预测新闻真实性。实验结果表明，我们的模型优于近期的方法，证明了邻居选择机制和多模态融合在虚假新闻检测中的有效性。我们的提议引入了一个新的范式：基于知识的多模态推理。通过集成显式的实体级选择和基于自然语言推理的过滤机制，我们将虚假新闻检测从特征融合转向基于语义的验证。为了重现性与进一步研究，源代码公开于[this http URL]。', 'title_zh': 'KGAlign：联合语义-结构知识编码的多模态假新闻检测'}
{'arxiv_id': 'arXiv:2505.14707', 'title': 'CrypticBio: A Large Multimodal Dataset for Visually Confusing Biodiversity', 'authors': 'Georgiana Manolache, Gerard Schouten, Joaquin Vanschoren', 'link': 'https://arxiv.org/abs/2505.14707', 'abstract': 'We present CrypticBio, the largest publicly available multimodal dataset of visually confusing species, specifically curated to support the development of AI models in the context of biodiversity applications. Visually confusing or cryptic species are groups of two or more taxa that are nearly indistinguishable based on visual characteristics alone. While much existing work addresses taxonomic identification in a broad sense, datasets that directly address the morphological confusion of cryptic species are small, manually curated, and target only a single taxon. Thus, the challenge of identifying such subtle differences in a wide range of taxa remains unaddressed. Curated from real-world trends in species misidentification among community annotators of iNaturalist, CrypticBio contains 52K unique cryptic groups spanning 67K species, represented in 166 million images. Rich research-grade image annotations--including scientific, multicultural, and multilingual species terminology, hierarchical taxonomy, spatiotemporal context, and associated cryptic groups--address multimodal AI in biodiversity research. For easy dataset curation, we provide an open-source pipeline CrypticBio-Curate. The multimodal nature of the dataset beyond vision-language arises from the integration of geographical and temporal data as complementary cues to identifying cryptic species. To highlight the importance of the dataset, we benchmark a suite of state-of-the-art foundation models across CrypticBio subsets of common, unseen, endangered, and invasive species, and demonstrate the substantial impact of geographical context on vision-language zero-shot learning for cryptic species. By introducing CrypticBio, we aim to catalyze progress toward real-world-ready biodiversity AI models capable of handling the nuanced challenges of species ambiguity.', 'abstract_zh': 'CrypticBio：最大的公开多模态混淆物种数据集，专门支持生物多样性应用中的AI模型开发', 'title_zh': 'CrypticBio: 一个大型多模态数据集，用于视觉上难以区分的生物多样性'}
