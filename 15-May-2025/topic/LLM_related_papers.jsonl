{'arxiv_id': 'arXiv:2505.09427', 'title': 'SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation', 'authors': 'Achref Doula, Max Mühläuser, Alejandro Sanchez Guinea', 'link': 'https://arxiv.org/abs/2505.09427', 'abstract': 'Large Language Models (LLMs) show growing promise in autonomous driving by reasoning over complex traffic scenarios to generate path plans. However, their tendencies toward overconfidence, and hallucinations raise critical safety concerns. We introduce SafePath, a modular framework that augments LLM-based path planning with formal safety guarantees using conformal prediction. SafePath operates in three stages. In the first stage, we use an LLM that generates a set of diverse candidate paths, exploring possible trajectories based on agent behaviors and environmental cues. In the second stage, SafePath filters out high-risk trajectories while guaranteeing that at least one safe option is included with a user-defined probability, through a multiple-choice question-answering formulation that integrates conformal prediction. In the final stage, our approach selects the path with the lowest expected collision risk when uncertainty is low or delegates control to a human when uncertainty is high. We theoretically prove that SafePath guarantees a safe trajectory with a user-defined probability, and we show how its human delegation rate can be tuned to balance autonomy and safety. Extensive experiments on nuScenes and Highway-env show that SafePath reduces planning uncertainty by 77\\% and collision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven path planning more safer.', 'abstract_zh': 'SafePath：使用符合预测增强的LLM路径规划的正式安全保证', 'title_zh': 'SafePath: 依从性预测用于安全的LLM基于自主导航'}
{'arxiv_id': 'arXiv:2505.09614', 'title': 'Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?', 'authors': 'Anthony GX-Chen, Dongyan Lin, Mandana Samiei, Doina Precup, Blake A. Richards, Rob Fergus, Kenneth Marino', 'link': 'https://arxiv.org/abs/2505.09614', 'abstract': 'Language model (LM) agents are increasingly used as autonomous decision-makers who need to actively gather information to guide their decisions. A crucial cognitive skill for such agents is the efficient exploration and understanding of the causal structure of the world -- key to robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs possess this capability or exhibit systematic biases leading to erroneous conclusions. In this work, we examine LMs\' ability to explore and infer causal relationships, using the well-established "Blicket Test" paradigm from developmental psychology. We find that LMs reliably infer the common, intuitive disjunctive causal relationships but systematically struggle with the unusual, yet equally (or sometimes even more) evidenced conjunctive ones. This "disjunctive bias" persists across model families, sizes, and prompting strategies, and performance further declines as task complexity increases. Interestingly, an analogous bias appears in human adults, suggesting that LMs may have inherited deep-seated reasoning heuristics from their training data. To this end, we quantify similarities between LMs and humans, finding that LMs exhibit adult-like inference profiles (but not children-like). Finally, we propose a test-time sampling method which explicitly samples and eliminates hypotheses about causal relationships from the LM. This scalable approach significantly reduces the disjunctive bias and moves LMs closer to the goal of scientific, causally rigorous reasoning.', 'abstract_zh': '语言模型（LM）代理日益用于担任自主决策者，需要主动搜集信息来指导其决策。此类代理高效探索和理解世界因果结构的关键认知技能是其进行稳健、科学依据充分推理的基础。然而，目前仍不清楚语言模型是否具备这种能力，还是倾向于表现出导致错误结论的系统性偏差。在本研究中，我们利用发展心理学中广泛认可的“Blicket 测试”范式，考察语言模型探索和推断因果关系的能力。我们发现，语言模型可靠地推断出常见的直观析取因果关系，但在处理异常但仍具有同样（或有时甚至更多）证据的联言因果关系时表现出系统性困难。这种“析取偏差”在不同模型家族、规模和提示策略中普遍存在，并且随着任务复杂性的增加，性能进一步下降。有趣的是，成年人类也表现出类似的偏差，这表明语言模型可能继承了其训练数据中的深层次推理启发式。为了解决这一问题，我们量化了语言模型与人类之间的相似性，发现语言模型显示出类似成年人的推理特征（但不是类似儿童的）。最后，我们提出了一种在测试时采样的方法，该方法明确地从语言模型中采样和排除关于因果关系的假设。这种可扩展的方法显著减少了析取偏差，使语言模型更接近科学、因果严谨推理的目标。', 'title_zh': '语言代理镜像人类因果推理偏见。我们如何帮助它们像科学家一样思考？'}
{'arxiv_id': 'arXiv:2505.09396', 'title': 'The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners', 'authors': 'Vince Trencsenyi, Agnieszka Mensfelt, Kostas Stathis', 'link': 'https://arxiv.org/abs/2505.09396', 'abstract': "The rapid rise of large language models (LLMs) has shifted artificial intelligence (AI) research toward agentic systems, motivating the use of weaker and more flexible notions of agency. However, this shift raises key questions about the extent to which LLM-based agents replicate human strategic reasoning, particularly in game-theoretic settings. In this context, we examine the role of agentic sophistication in shaping artificial reasoners' performance by evaluating three agent designs: a simple game-theoretic model, an unstructured LLM-as-agent model, and an LLM integrated into a traditional agentic framework. Using guessing games as a testbed, we benchmarked these agents against human participants across general reasoning patterns and individual role-based objectives. Furthermore, we introduced obfuscated game scenarios to assess agents' ability to generalise beyond training distributions. Our analysis, covering over 2000 reasoning samples across 25 agent configurations, shows that human-inspired cognitive structures can enhance LLM agents' alignment with human strategic behaviour. Still, the relationship between agentic design complexity and human-likeness is non-linear, highlighting a critical dependence on underlying LLM capabilities and suggesting limits to simple architectural augmentation.", 'abstract_zh': '大型语言模型的迅速崛起促使人工智能研究转向自主系统，推动了更弱更灵活的自主概念的应用。然而，这种转变引发了关于基于大型语言模型的代理是否能复制人类战略推理能力的关键问题，尤其是在博弈论环境中。在此背景下，我们通过评估三种代理设计——一个简单的博弈论模型、一个无结构的大规模语言模型代理以及一个整合传统自主框架的大规模语言模型，探讨了自主复杂性在塑造人工推理性能中的作用。使用猜测游戏作为测试平台，我们评估了这些代理在一般推理模式和个体角色目标方面的表现，并引入了混淆的博弈场景来评估代理超越训练分布的泛化能力。我们的分析涵盖了25种代理配置超过2000个推理样本，结果显示，灵感来源于人类的认知结构可以增强大规模语言模型代理与人类战略行为的一致性。然而，自主设计复杂性与人类相似性的关系是非线性的，突显了其对底层大规模语言模型能力的强烈依赖，并表明简单的架构增强有限。', 'title_zh': '人类启发式能力在LLM驱动战略推理中的影响'}
{'arxiv_id': 'arXiv:2505.09289', 'title': 'Reproducibility Study of "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"', 'authors': 'Pedro M. P. Curvo, Mara Dragomir, Salvador Torpes, Mohammadmahdi Rahimi', 'link': 'https://arxiv.org/abs/2505.09289', 'abstract': 'This study evaluates and extends the findings made by Piatti et al., who introduced GovSim, a simulation framework designed to assess the cooperative decision-making capabilities of large language models (LLMs) in resource-sharing scenarios. By replicating key experiments, we validate claims regarding the performance of large models, such as GPT-4-turbo, compared to smaller models. The impact of the universalization principle is also examined, with results showing that large models can achieve sustainable cooperation, with or without the principle, while smaller models fail without it. In addition, we provide multiple extensions to explore the applicability of the framework to new settings. We evaluate additional models, such as DeepSeek-V3 and GPT-4o-mini, to test whether cooperative behavior generalizes across different architectures and model sizes. Furthermore, we introduce new settings: we create a heterogeneous multi-agent environment, study a scenario using Japanese instructions, and explore an "inverse environment" where agents must cooperate to mitigate harmful resource distributions. Our results confirm that the benchmark can be applied to new models, scenarios, and languages, offering valuable insights into the adaptability of LLMs in complex cooperative tasks. Moreover, the experiment involving heterogeneous multi-agent systems demonstrates that high-performing models can influence lower-performing ones to adopt similar behaviors. This finding has significant implications for other agent-based applications, potentially enabling more efficient use of computational resources and contributing to the development of more effective cooperative AI systems.', 'abstract_zh': '本研究评估并扩展了Piatti等人的研究成果，他们提出了GovSim，一种用于评估大型语言模型（LLMs）在资源分享场景中合作决策能力的仿真框架。通过复制关键实验，我们验证了关于大型模型（如GPT-4-turbo）与较小模型相比的表现声称。还研究了普遍化原则的影响，结果显示，大型模型可以在有或没有该原则的情况下实现可持续合作，而较小模型则需要该原则才能合作。此外，我们提供了多项扩展，以探讨该框架在新环境中的适用性。我们评估了其他模型（如DeepSeek-V3和GPT-4o-mini），以测试合作行为是否能跨不同架构和模型规模泛化。此外，我们引入了新环境：创建了一个异质多代理环境，研究了一个使用日语指令的场景，并探讨了一个“逆环境”，其中代理必须合作以缓解有害资源分配。研究结果证实，基准可以应用于新模型、场景和语言，为大型语言模型在复杂合作任务中的适应性提供了有价值的见解。此外，涉及异质多代理系统的实验表明，高性能模型可以影响低性能模型，使其采用类似行为。这一发现对其他基于代理的应用具有重要意义，可能有助于更有效地利用计算资源，并促进更有效的合作AI系统的发展。', 'title_zh': '"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"的再现性研究'}
{'arxiv_id': 'arXiv:2505.09031', 'title': 'Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification', 'authors': 'Adarsh Kumar, Hwiyoon Kim, Jawahar Sai Nathani, Neil Roy', 'link': 'https://arxiv.org/abs/2505.09031', 'abstract': 'Hallucination, where large language models (LLMs) generate confident but incorrect or irrelevant information, remains a key limitation in their application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has emerged as a promising method for improving multistep reasoning by guiding models through intermediate steps. However, CoT alone does not fully address the hallucination problem. In this work, we investigate how combining CoT with retrieval-augmented generation (RAG), as well as applying self-consistency and self-verification strategies, can reduce hallucinations and improve factual accuracy. By incorporating external knowledge sources during reasoning and enabling models to verify or revise their own outputs, we aim to generate more accurate and coherent responses. We present a comparative evaluation of baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification techniques. Our results highlight the effectiveness of each method and identify the most robust approach for minimizing hallucinations while preserving fluency and reasoning depth.', 'abstract_zh': '大语言模型（LLMs）生成的幻觉问题仍是其在处理复杂、开放式任务时的关键局限。思维链（CoT）提示作为一种改进多步推理的方法逐渐显示出潜力，通过引导模型完成中间步骤。然而，单独使用CoT并不能完全解决幻觉问题。在本研究中，我们探讨了将CoT与检索增强生成（RAG）相结合，以及应用自一致性与自我验证策略，如何减少幻觉并提高事实准确性。通过在推理过程中引入外部知识源，并使模型能够验证或修订自己的输出，我们旨在生成更准确和连贯的回应。我们对基线LLMs、CoT、CoT+RAG、自一致性及自我验证技术进行了比较评估。我们的结果突出了每种方法的有效性，并确定了在保留流畅性和推理深度的同时最大限度减少幻觉的最稳健方法。', 'title_zh': '提高大语言模型可靠性的方法：结合论理思维、基于检索的生成、自我一致性与自我验证'}
{'arxiv_id': 'arXiv:2505.09024', 'title': 'Automated Meta Prompt Engineering for Alignment with the Theory of Mind', 'authors': 'Aaron Baughman, Rahul Agarwal, Eduardo Morales, Gozde Akay', 'link': 'https://arxiv.org/abs/2505.09024', 'abstract': "We introduce a method of meta-prompting that jointly produces fluent text for complex tasks while optimizing the similarity of neural states between a human's mental expectation and a Large Language Model's (LLM) neural processing. A technique of agentic reinforcement learning is applied, in which an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning, how to produce content by interpreting the intended and unintended generated text traits. To measure human mental beliefs around content production, users modify long form AI-generated text articles before publication at the US Open 2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM) alignment problem by anticipating and including human edits within the creation of text from an LLM. Throughout experimentation and by interpreting the results of a live production system, the expectations of human content reviewers had 100% of alignment with AI 53.8% of the time with an average iteration count of 4.38. The geometric interpretation of content traits such as factualness, novelty, repetitiveness, and relevancy over a Hilbert vector space combines spatial volume (all trait importance) with vertices alignment (individual trait relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an increase in content quality by extending the coverage of tennis action. Our work that was deployed at the US Open 2024 has been used across other live events within sports and entertainment.", 'abstract_zh': '一种促进大型语言模型和人类思维一致性的元提示方法及其应用', 'title_zh': '基于理论心智的自动化元提示工程'}
{'arxiv_id': 'arXiv:2505.08905', 'title': 'Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora', 'authors': 'Michael Majurski, Cynthia Matuszek', 'link': 'https://arxiv.org/abs/2505.08905', 'abstract': 'Language Models (LMs) continue to advance, improving response quality and coherence. Given Internet-scale training datasets, LMs have likely encountered much of what users might ask them to generate in some form during their training. A plethora of evaluation benchmarks have been constructed to assess model quality, response appropriateness, and reasoning capabilities. However, the human effort required for benchmark construction is limited and being rapidly outpaced by the size and scope of the models under evaluation. Additionally, having humans build a benchmark for every possible domain of interest is impractical. Therefore, we propose a methodology for automating the construction of fact-based synthetic data model evaluations grounded in document populations. This work leverages those very same LMs to evaluate domain-specific knowledge automatically, using only grounding documents (e.g., a textbook) as input. This synthetic data benchmarking approach corresponds well with human curated questions with a Spearman ranking correlation of 0.96 and a benchmark evaluation Pearson accuracy correlation of 0.79. This novel tool supports generating both multiple choice and open-ended synthetic data questions to gain diagnostic insight of LM capability. We apply this methodology to evaluate model performance on a recent relevant arXiv preprint, discovering a surprisingly strong performance from Gemma3 models.', 'abstract_zh': '语言模型继续进步，提升响应质量和连贯性。鉴于互联网规模的训练数据集，语言模型在训练过程中很可能已经遇到了用户可能请求生成的各种内容。已经构建了许多评估基准来评估模型质量、响应适宜性和推理能力。然而，基准构建所需的人力投入受到限制，并且正在被评估模型的规模和范围迅速超越。此外，为每一个感兴趣的领域手工构建基准是不切实际的。因此，我们提出了一种方法，通过基于文档集合自动构建事实基础的合成数据模型评估方法，从而实现自动化。该方法利用那些语言模型本身，仅使用锚定文档（例如，教科书）作为输入，自动评估领域的特定知识。这种合成数据基准方法与人工策划的问题具有Spearman排名相关性为0.96和基准评估Pearson准确性相关性为0.79。这一新颖工具支持生成选择题和开放题目的合成数据问题，以诊断语言模型的能力。我们将此方法应用于评估一个近期相关的arXiv预印本文本的表现，发现Gemma3模型取得了令人惊讶的强大性能。', 'title_zh': '基于未监督文档语料库的语言模型合成数据评估'}
{'arxiv_id': 'arXiv:2505.09610', 'title': 'Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors', 'authors': 'Nicolas Dupuis, Ravi Nair, Shyam Ramji, Sean McClintock, Nishant Chauhan, Priyanka Nagpal, Bart Blaner, Ken Valk, Leon Stok, Ruchir Puri', 'link': 'https://arxiv.org/abs/2505.09610', 'abstract': 'The use of Large Language Models (LLMs) in hardware design has taken off in recent years, principally through its incorporation in tools that increase chip designer productivity. There has been considerable discussion about the use of LLMs in RTL specifications of chip designs, for which the two most popular languages are Verilog and VHDL. LLMs and their use in Verilog design has received significant attention due to the higher popularity of the language, but little attention so far has been given to VHDL despite its continued popularity in the industry. There has also been little discussion about the unique needs of organizations that engage in high-performance processor design, and techniques to deploy AI solutions in these settings. In this paper, we describe our journey in developing a Large Language Model (LLM) specifically for the purpose of explaining VHDL code, a task that has particular importance in an organization with decades of experience and assets in high-performance processor design. We show how we developed test sets specific to our needs and used them for evaluating models as we performed extended pretraining (EPT) of a base LLM. Expert evaluation of the code explanations produced by the EPT model increased to 69% compared to a base model rating of 43%. We further show how we developed an LLM-as-a-judge to gauge models similar to expert evaluators. This led us to deriving and evaluating a host of new models, including an instruction-tuned version of the EPT model with an expected expert evaluator rating of 71%. Our experiments also indicate that with the potential use of newer base models, this rating can be pushed to 85% and beyond. We conclude with a discussion on further improving the quality of hardware design LLMs using exciting new developments in the Generative AI world.', 'abstract_zh': '大型语言模型在硬件设计中的应用：以VHDL代码解释器开发为例', 'title_zh': '自定义大型语言模型以用于高性能微处理器的VHDL设计'}
{'arxiv_id': 'arXiv:2505.09598', 'title': 'How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference', 'authors': 'Nidhal Jegham, Marwen Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi', 'link': 'https://arxiv.org/abs/2505.09598', 'abstract': "As large language models (LLMs) spread across industries, understanding their environmental footprint at the inference level is no longer optional; it is essential. However, most existing studies exclude proprietary models, overlook infrastructural variability and overhead, or focus solely on training, even as inference increasingly dominates AI's environmental impact. To bridge this gap, this paper introduces a novel infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models as deployed in commercial data centers. Our framework combines public API performance data with region-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost. Our results show that o3 and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33 Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results in substantial annual environmental impacts. These include electricity use comparable to 35,000 U.S. homes, freshwater evaporation matching the annual drinking needs of 1.2 million people, and carbon emissions requiring a Chicago-sized forest to offset. These findings illustrate a growing paradox: although individual queries are efficient, their global scale drives disproportionate resource consumption. Our study provides a standardized, empirically grounded methodology for benchmarking the sustainability of LLM deployments, laying a foundation for future environmental accountability in AI development and sustainability standards.", 'abstract_zh': '随着大型语言模型（LLMs）在各行业的普及，理解其推理阶段的环境足迹已不再是一种选择，而是必要条件。然而，现有的大多数研究排除了私有模型，忽视了 infrastructural 变异性和开销，或者仅专注于训练，即使推理越来越多地影响着人工智能的环境影响。为填补这一空白，本文提出了一个基于基础设施的新基准测试框架，用于量化部署在商业数据中心的30个最先进的LLM推理的环境足迹。我们的框架结合了公共API性能数据、地区特定的环境多重因素以及硬件配置的统计推断。我们还利用交叉效率数据包络分析（DEA）来按相对于环境成本的性能对模型进行排名。我们的结果显示，o3和DeepSeek-R1 是最耗能的模型，每次长提示消耗超过33 Wh，比GPT-4.1 nano的消耗多70多倍。而Claude-3.7 Sonnet 在生态效率方面排名最高。单独一个较短的GPT-4o查询消耗0.43 Wh，将其放大到每天7亿次查询的结果会导致显著的年度环境影响。包括相当于35,000个美国家庭的用电量、符合120万人一年饮用水量的淡水蒸发以及需要相当于芝加哥大小森林的碳排放量来抵消。这些发现揭示了一个日益增长的悖论：尽管单个查询本身是高效的，但其全球规模导致了不成比例的资源消耗。我们的研究提供了一个标准化、实证为基础的方法来基准测试LLM部署的可持续性，为未来在人工智能开发和可持续性标准中的环境问责制奠定了基础。', 'title_zh': 'AI的饥饿程度如何？大规模语言模型推理的能耗、水资源消耗及碳足迹基准研究'}
{'arxiv_id': 'arXiv:2505.09595', 'title': 'WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models', 'authors': 'Abdullah Mushtaq, Imran Taj, Rafay Naeem, Ibrahim Ghaznavi, Junaid Qadir', 'link': 'https://arxiv.org/abs/2505.09595', 'abstract': 'Large Language Models (LLMs) are predominantly trained and aligned in ways that reinforce Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality. Existing benchmarking frameworks fail to adequately capture this bias, as they rely on rigid, closed-form assessments that overlook the complexity of cultural inclusivity. To address this, we introduce WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity (GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our approach is grounded in the Multiplex Worldview proposed by Senturk et al., which distinguishes between Uniplex models, reinforcing cultural homogenization, and Multiplex models, which integrate diverse perspectives. WorldView-Bench measures Cultural Polarization, the exclusion of alternative perspectives, through free-form generative evaluation rather than conventional categorical benchmarks. We implement applied multiplexity through two intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where system prompts embed multiplexity principles, and (2) Multi-Agent System (MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing distinct cultural perspectives collaboratively generate responses. Our results demonstrate a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance. These findings highlight the potential of multiplex-aware AI evaluation in mitigating cultural bias in LLMs, paving the way for more inclusive and ethically aligned AI systems.', 'abstract_zh': 'WorldView-Bench：评估大型语言模型全球文化包容性的新基准', 'title_zh': 'WorldView-Bench：评估大规模语言模型全球文化视角的基准'}
{'arxiv_id': 'arXiv:2505.09576', 'title': 'Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach', 'authors': 'Shannon Lodoen, Alexi Orchard', 'link': 'https://arxiv.org/abs/2505.09576', 'abstract': 'Since 2022, versions of generative AI chatbots such as ChatGPT and Claude have been trained using a specialized technique called Reinforcement Learning from Human Feedback (RLHF) to fine-tune language model output using feedback from human annotators. As a result, the integration of RLHF has greatly enhanced the outputs of these large language models (LLMs) and made the interactions and responses appear more "human-like" than those of previous versions using only supervised learning. The increasing convergence of human and machine-written text has potentially severe ethical, sociotechnical, and pedagogical implications relating to transparency, trust, bias, and interpersonal relations. To highlight these implications, this paper presents a rhetorical analysis of some of the central procedures and processes currently being reshaped by RLHF-enhanced generative AI chatbots: upholding language conventions, information seeking practices, and expectations for social relationships. Rhetorical investigations of generative AI and LLMs have, to this point, focused largely on the persuasiveness of the content generated. Using Ian Bogost\'s concept of procedural rhetoric, this paper shifts the site of rhetorical investigation from content analysis to the underlying mechanisms of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical investigation opens a new direction for further inquiry in AI ethics that considers how procedures rerouted through AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships. It will therefore be of interest to educators, researchers, scholars, and the growing number of users of generative AI chatbots.', 'abstract_zh': '自2022年以来，如ChatGPT和Claude等生成式AI聊天机器人版本通过一种专门的技术——基于人类反馈的强化学习（RLHF）进行训练，使用人类注释者的反馈来微调语言模型的输出。RLHF的整合显著提升了这些大型语言模型（LLMs）的输出效果，使其互动和回应显得比仅使用监督学习的版本更加“人性化”。人类和机器写作文本日益融合可能对透明度、信任、偏见和人际关系等方面产生严重的伦理、社会技术和教育的影响。为了突显这些影响，本文对RLHF增强的生成式AI聊天机器人目前正重新塑造的一些核心程序和过程进行了修辞分析：维护语言规范、信息寻求行为以及社交关系的期望。迄今为止，关于生成式AI和LLMs的修辞研究主要集中在生成内容的说服力上。通过使用Ian Bogost的程序修辞概念，本文将修辞研究的焦点从内容分析转移到嵌入RLHF增强的LLMs中的说服机制。这一理论探讨为AI伦理研究开辟了新的方向，考虑了经由AI驱动技术重新导向的程序如何巩固霸权语言使用、延续偏见、脱离情境地学习以及侵犯人类关系的可能性。因此，这将对教育工作者、研究人员、学者以及日益增长的生成式AI聊天机器人用户群体产生兴趣。', 'title_zh': '基于程序修辞的方法：从人类反馈中学习的强化学习中的伦理与说服研究'}
{'arxiv_id': 'arXiv:2505.09438', 'title': 'Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment', 'authors': 'Paul Tschisgale, Holger Maus, Fabian Kieser, Ben Kroehs, Stefan Petersen, Peter Wulff', 'link': 'https://arxiv.org/abs/2505.09438', 'abstract': "Large language models (LLMs) are now widely accessible, reaching learners at all educational levels. This development has raised concerns that their use may circumvent essential learning processes and compromise the integrity of established assessment formats. In physics education, where problem solving plays a central role in instruction and assessment, it is therefore essential to understand the physics-specific problem-solving capabilities of LLMs. Such understanding is key to informing responsible and pedagogically sound approaches to integrating LLMs into instruction and assessment. This study therefore compares the problem-solving performance of a general-purpose LLM (GPT-4o, using varying prompting techniques) and a reasoning-optimized model (o1-preview) with that of participants of the German Physics Olympiad, based on a set of well-defined Olympiad problems. In addition to evaluating the correctness of the generated solutions, the study analyzes characteristic strengths and limitations of LLM-generated solutions. The findings of this study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate advanced problem-solving capabilities on Olympiad-type physics problems, on average outperforming the human participants. Prompting techniques had little effect on GPT-4o's performance, while o1-preview almost consistently outperformed both GPT-4o and the human benchmark. Based on these findings, the study discusses implications for the design of summative and formative assessment in physics education, including how to uphold assessment integrity and support students in critically engaging with LLMs.", 'abstract_zh': '大型语言模型（LLMs）现在广泛可用，已触及各个教育层次的学习者。这一发展引发了对其使用可能绕过关键学习过程并损害既定评估格式完整性的担忧。在物理教育中，由于解决问题在教学和评估中扮演着核心角色，因此理解LLMs的物理专用问题解决能力变得至关重要。这种理解对于指导负责任且教育上合理的将LLMs整合到教学和评估中的方法具有重要意义。因此，本研究比较了一般用途LLM（使用不同提示技术的GPT-4o）和推理优化模型（o1-preview）与德国物理奥林匹克参赛者在一组定义明确的物理奥林匹克问题上的问题解决表现。除了评估生成解的正确性外，本研究还分析了LLM生成解的特征优势和局限性。研究结果表明，两种测试的LLMs（GPT-4o和o1-preview）在物理奥林匹克类型的问题上表现出高级问题解决能力，平均而言优于人类参与者。提示技术对GPT-4o的性能影响甚微，而o1-preview几乎总是优于GPT-4o和人类基准。基于这些发现，本研究讨论了物理教育中总结性和形成性评估设计的含义，包括如何维护评估完整性以及支持学生批判性地与LLMs互动。', 'title_zh': '基于GPT和推理的大语言模型在物理奥林匹克问题上的评估：超越人类性能及对教育评估的影响'}
{'arxiv_id': 'arXiv:2505.09436', 'title': 'CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios', 'authors': 'Raghav Garg, Kapil Sharma, Karan Gupta', 'link': 'https://arxiv.org/abs/2505.09436', 'abstract': "Large Language Models (LLMs) hold immense potential for revolutionizing Customer Experience Management (CXM), particularly in contact center operations. However, evaluating their practical utility in complex operational environments is hindered by data scarcity (due to privacy concerns) and the limitations of current benchmarks. Existing benchmarks often lack realism, failing to incorporate deep knowledge base (KB) integration, real-world noise, or critical operational tasks beyond conversational fluency. To bridge this gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational CXM contexts. Given the diversity in possible contact center features, we have developed a scalable LLM-powered pipeline that simulates the brand's CXM entities that form the foundation of our datasets-such as knowledge articles including product specifications, issue taxonomies, and contact center conversations. The entities closely represent real-world distribution because of controlled noise injection (informed by domain experts) and rigorous automated validation. Building on this, we release CXMArena, which provides dedicated benchmarks targeting five important operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools. Our baseline experiments underscore the benchmark's difficulty: even state of the art embedding and generation models achieve only 68% accuracy on article search, while standard embedding methods yield a low F1 score of 0.3 for knowledge base refinement, highlighting significant challenges for current models necessitating complex pipelines and solutions over conventional techniques.", 'abstract_zh': '大型语言模型（LLMs）在客户体验管理（CXM）特别是在接触中心运营中的革命潜力巨大。然而，由于隐私问题导致的数据稀缺性和现有基准的局限性，评估其在复杂运营环境中的实际效用受到阻碍。现有的基准往往缺乏现实性，未能整合深度知识库（KB）、现实世界噪音或超出对话流畅性的重要运营任务。为填补这一缺口，我们提出了CXMArena，这是一个新型的大规模合成基准数据集，专门用于评估AI在运营CXM环境中的性能。鉴于接触中心特征的多样性，我们开发了一个可扩展的LLM驱动的流程，模拟品牌的CXM实体，这些实体构成了我们数据集的基础，如包含产品规格、问题分类和接触中心对话的知识文章。实体由于受控的噪音注入（由领域专家指导）和严格的自动化验证，接近真实世界的应用分布。在此基础上，我们发布了CXMArena，提供了针对五个重要运营任务的专用基准：知识库精炼、意图预测、代理质量遵从性、文章搜索以及集成工具的多轮检索与生成。基线实验突显了基准的难度：即使最先进的嵌入和生成模型在文章搜索任务上的准确率也只有68%，标准嵌入方法在知识库精炼任务上的F1分数仅为0.3，这表明当前模型面临巨大挑战，需要复杂的流程和解决方案才能超越传统技术。', 'title_zh': 'CXMArena: 统一数据集以在真实的客户关系管理场景中评估性能'}
{'arxiv_id': 'arXiv:2505.09407', 'title': 'Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits', 'authors': 'Subrit Dikshit, Ritu Tiwari, Priyank Jain', 'link': 'https://arxiv.org/abs/2505.09407', 'abstract': 'Cloud-based multilingual translation services like Google Translate and Microsoft Translator achieve state-of-the-art translation capabilities. These services inherently use large multilingual language models such as GRU, LSTM, BERT, GPT, T5, or similar encoder-decoder architectures with attention mechanisms as the backbone. Also, new age natural language systems, for instance ChatGPT and DeepSeek, have established huge potential in multiple tasks in natural language processing. At the same time, they also possess outstanding multilingual translation capabilities. However, these models use the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) is an alternate solution that explores the quantum computing realm instead of the classical computing realm to study and demonstrate multilingual machine translation. QEDACVC introduces the quantum encoder-decoder architecture that simulates and runs on quantum computing hardware via quantum convolution, quantum pooling, quantum variational circuit, and quantum attention as software alterations. QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.', 'abstract_zh': '基于云的多语言翻译服务如Google Translate和Microsoft Translator实现了最先进的翻译能力。这些服务本质上使用了如GRU、LSTM、BERT、GPT、T5或类似的关注机制下编码解码架构的大规模多语言语言模型。同时，新一代自然语言系统，例如ChatGPT和DeepSeek，在多项自然语言处理任务中展示了巨大的潜力，同时也具备出色的多语言翻译能力。然而，这些模型使用经典计算域作为后端。QEDACVC（Quantum Encoder Decoder Attention-based Convolutional Variational Circuits）是一种替代方案，它探索量子计算领域而非经典计算领域来研究和展示多语言机器翻译。QEDACVC引入了一种量子编码解码架构，通过量子卷积、量子池化、量子变分电路和量子注意力等软件修改在量子计算硬件上进行模拟和运行。当在OPUS数据集的英语、法语、德语和印地语语料上训练时，QEDACVC达到了82%的准确率。', 'title_zh': '基于量子编码解码注意机制卷积变分电路的多语言机器翻译'}
{'arxiv_id': 'arXiv:2505.09343', 'title': 'Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures', 'authors': 'Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Huazuo Gao, Jiashi Li, Liyue Zhang, Panpan Huang, Shangyan Zhou, Shirong Ma, Wenfeng Liang, Ying He, Yuqing Wang, Yuxuan Liu, Y.X. Wei', 'link': 'https://arxiv.org/abs/2505.09343', 'abstract': "The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.", 'abstract_zh': '大语言模型的快速扩展揭示了当前硬件架构的关键限制，包括内存容量、计算效率和 interconnection 带宽的约束。DeepSeek-V3 在 2048 块 NVIDIA H800 GPU 上训练，展示了基于硬件的模型协同设计如何有效地应对这些挑战，从而实现大规模的成本效益训练和推理。本文深入分析了 DeepSeek-V3/R1 模型架构及其 AI 基础设施，强调了多项关键创新，包括多头潜在注意力（MLA）以提高内存效率、专家混合架构（MoE）以优化计算-通信权衡、使用 FP8 混合精度训练以充分利用硬件能力，以及多平面网络拓扑以最小化集群级网络开销。基于 DeepSeek-V3 开发过程中遇到的硬件瓶颈，本文与学术界和工业界同行就未来硬件方向进行了更广泛讨论，包括精确的低精度计算单元、扩展性和分发性的收敛以及低延迟通信网络的创新。这些见解突显了硬件与模型协同设计在满足 AI 工作负载不断上升的需求中的关键作用，为下一代 AI 系统的创新提供了实用蓝图。', 'title_zh': 'DeepSeek-V3的深度学习扩展挑战及硬件架构反思'}
{'arxiv_id': 'arXiv:2505.09142', 'title': 'ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor', 'authors': 'Seungbeom Choi, Jeonghoe Goo, Eunjoo Jeon, Mingyu Yang, Minsung Jang', 'link': 'https://arxiv.org/abs/2505.09142', 'abstract': 'We propose ELIS, a serving system for Large Language Models (LLMs) featuring an Iterative Shortest Remaining Time First (ISRTF) scheduler designed to efficiently manage inference tasks with the shortest remaining tokens. Current LLM serving systems often employ a first-come-first-served scheduling strategy, which can lead to the "head-of-line blocking" problem. To overcome this limitation, it is necessary to predict LLM inference times and apply a shortest job first scheduling strategy. However, due to the auto-regressive nature of LLMs, predicting the inference latency is challenging. ELIS addresses this challenge by training a response length predictor for LLMs using the BGE model, an encoder-based state-of-the-art model. Additionally, we have devised the ISRTF scheduling strategy, an optimization of shortest remaining time first tailored to existing LLM iteration batching. To evaluate our work in an industrial setting, we simulate streams of requests based on our study of real-world user LLM serving trace records. Furthermore, we implemented ELIS as a cloud-native scheduler system on Kubernetes to evaluate its performance in production environments. Our experimental results demonstrate that ISRTF reduces the average job completion time by up to 19.6%.', 'abstract_zh': '我们提出ELIS，一种大型语言模型服务系统，具备迭代最短剩余时间优先(ISRTF)调度器，旨在高效管理剩余最短 tokens 的推理任务。当前大型语言模型服务系统常采用先到先服务的调度策略，这可能导致“线路头阻塞”问题。为克服此局限，需预测大型语言模型的推理时间并采用最短作业优先调度策略。但由于大型语言模型具有自回归特性，预测推理延迟颇具挑战性。ELIS通过使用基于编码器的前沿模型BGE训练响应长度预测器来应对这一挑战。此外，我们还设计了ISRTF调度策略，这是针对现有大型语言模型迭代批处理的最短剩余时间优先调度的优化。为了在工业环境中评估我们的工作，我们根据实际用户大型语言模型服务追踪记录模拟了请求流。进一步地，我们在Kubernetes上实现了ELIS，作为云原生调度系统，以评估其在生产环境中的性能。实验结果表明，ISRTF可将平均任务完成时间降低19.6%。', 'title_zh': 'ELIS: 效率高的LLM迭代调度系统，带有响应长度预测器'}
{'arxiv_id': 'arXiv:2505.09108', 'title': 'Air-Ground Collaboration for Language-Specified Missions in Unknown Environments', 'authors': 'Fernando Cladera, Zachary Ravichandran, Jason Hughes, Varun Murali, Carlos Nieto-Granda, M. Ani Hsieh, George J. Pappas, Camillo J. Taylor, Vijay Kumar', 'link': 'https://arxiv.org/abs/2505.09108', 'abstract': 'As autonomous robotic systems become increasingly mature, users will want to specify missions at the level of intent rather than in low-level detail. Language is an expressive and intuitive medium for such mission specification. However, realizing language-guided robotic teams requires overcoming significant technical hurdles. Interpreting and realizing language-specified missions requires advanced semantic reasoning. Successful heterogeneous robots must effectively coordinate actions and share information across varying viewpoints. Additionally, communication between robots is typically intermittent, necessitating robust strategies that leverage communication opportunities to maintain coordination and achieve mission objectives. In this work, we present a first-of-its-kind system where an unmanned aerial vehicle (UAV) and an unmanned ground vehicle (UGV) are able to collaboratively accomplish missions specified in natural language while reacting to changes in specification on the fly. We leverage a Large Language Model (LLM)-enabled planner to reason over semantic-metric maps that are built online and opportunistically shared between an aerial and a ground robot. We consider task-driven navigation in urban and rural areas. Our system must infer mission-relevant semantics and actively acquire information via semantic mapping. In both ground and air-ground teaming experiments, we demonstrate our system on seven different natural-language specifications at up to kilometer-scale navigation.', 'abstract_zh': '自主机器人系统日趋成熟后，用户将希望以意图而非低级细节来指定任务。自然语言是一个表达性和直观性的任务指定媒介。然而，实现语言引导的机器人团队需要克服重大的技术障碍。解释和实现语言指定的任务需要高级语义推理。成功的异构机器人必须有效地协调动作并分享信息，跨越不同视角。此外，机器人之间的通信通常断断续续，需要采用强大的策略，利用通信机会来维持协调并达成任务目标。在本工作中，我们提出了一种开创性系统，在该系统中，无人机(UAV)和地面机器人(UGV)能够协同完成自然语言指定的任务，并且能够在指定内容发生变化时实时响应。我们利用一个增强连通性的大规模语言模型(LLL)-启用规划器，在在线构建和机会性共享的语义-度量地图上进行推理。我们考虑城市和农村地区的任务导向导航。我们的系统必须推断与任务相关的信息，并通过语义映射主动获取信息。在地面和空地团队实验中，我们在多达千米尺度的导航中演示了七个不同自然语言规定的内容。', 'title_zh': '未知环境中的语言指定任务空地协作'}
{'arxiv_id': 'arXiv:2505.09082', 'title': 'CEC-Zero: Chinese Error Correction Solution Based on LLM', 'authors': 'Sophie Zhang, Zhiming Lin', 'link': 'https://arxiv.org/abs/2505.09082', 'abstract': "Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC). While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization. This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision. By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models. Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications. This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models.", 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）在中国文本处理能力，尤其是在中文拼写纠错（CSC）方面展现了卓越的能力。尽管LLMs在准确性和稳健性上优于传统的BERT基模型，但在可靠性和泛化能力方面仍存在挑战。本文提出了一种名为CEC-Zero的新型强化学习（RL）框架，该框架使LLMs能够通过自主错误策略学习自我纠正，而无需外部监督。通过将RL与LLMs的生成能力结合，该方法消除了对外标注数据或辅助模型的依赖。实验结果显示，增强RL的LLMs实现了行业可行的准确度和更好的跨域泛化能力，为中文自然语言处理应用中的可靠性优化提供了可扩展的解决方案。这一突破促进了LLMs在实际中文文本纠错场景中的部署，并为自我提升的语言模型建立了新的范式。', 'title_zh': 'CEC-Zero: 基于LLM的中文错误修正解决方案'}
{'arxiv_id': 'arXiv:2505.09081', 'title': 'SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation', 'authors': 'Gaurav Koley', 'link': 'https://arxiv.org/abs/2505.09081', 'abstract': 'Contemporary approaches to agent-based modeling (ABM) of social systems have traditionally emphasized rule-based behaviors, limiting their ability to capture nuanced dynamics by moving beyond predefined rules and leveraging contextual understanding from LMs of human social interaction. This paper presents SALM (Social Agent LM Framework), a novel approach for integrating language models (LMs) into social network simulation that achieves unprecedented temporal stability in multi-agent scenarios. Our primary contributions include: (1) a hierarchical prompting architecture enabling stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2) an attention-based memory system achieving 80% cache hit rates (95% CI [78%, 82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on personality stability. Through extensive validation against SNAP ego networks, we demonstrate the first LLM-based framework capable of modeling long-term social phenomena while maintaining empirically validated behavioral fidelity.', 'abstract_zh': '基于代理的建模（ABM）在社会系统中的当代方法传统上强调基于规则的行为，限制了它们通过超越预定义规则并利用人类社会互动语言模型的上下文理解来捕捉细腻动态的能力。本文提出了SALM（社会代理LM框架），这是一种将语言模型（LMs）集成到社会网络模拟中的新颖方法，实现了多代理场景中前所未有的时间稳定性。我们的主要贡献包括：（1）分层提示架构，使其能够在超过4000个时间步长的同时降低73%的令牌使用量，实现稳定模拟；（2）基于注意力的记忆系统，实现了80%的缓存命中率（95%置信区间为78%至82%），并具有亚线性内存增长，增长率为9.5%；以及（3）人格稳定性的形式边界。通过广泛验证against SNAP自网络，我们展示了第一个基于LLM的框架，能够在保持经验验证的行为保真度的同时建模长期社会现象。', 'title_zh': 'SALM：一种由语言模型驱动的社交网络仿真多Agent框架'}
{'arxiv_id': 'arXiv:2505.09062', 'title': 'Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models', 'authors': 'Junda Zhao, Yuliang Song, Eldan Cohen', 'link': 'https://arxiv.org/abs/2505.09062', 'abstract': "Recent advancements in source code summarization have leveraged transformer-based pre-trained models, including Large Language Models of Code (LLMCs), to automate and improve the generation of code summaries. However, existing methods often focus on generating a single high-quality summary for a given source code, neglecting scenarios where the generated summary might be inadequate and alternative options are needed. In this paper, we introduce Variational Prefix Tuning (VPT), a novel approach that enhances pre-trained models' ability to generate diverse yet accurate sets of summaries, allowing the user to choose the most suitable one for the given source code. Our method integrates a Conditional Variational Autoencoder (CVAE) framework as a modular component into pre-trained models, enabling us to model the distribution of observed target summaries and sample continuous embeddings to be used as prefixes to steer the generation of diverse outputs during decoding. Importantly, we construct our method in a parameter-efficient manner, eliminating the need for expensive model retraining, especially when using LLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset of generated summaries, optimizing both the diversity and the accuracy of the options presented to users. We present extensive experimental evaluations using widely used datasets and current state-of-the-art pre-trained code summarization models to demonstrate the effectiveness of our approach and its adaptability across models.", 'abstract_zh': 'Recent advancements in源代码摘要生成采用基于变换器的预训练模型，包括代码大型语言模型（LLMCs），以自动化并提高代码摘要的生成质量。然而，现有方法往往专注于生成给定源代码的单个高质量摘要，忽视了生成摘要可能不足且需要替代选项的情形。在本文中，我们介绍了变分前缀调谐（VPT），这是一种新型方法，能够增强预训练模型生成多样且准确的摘要集的能力，允许用户选择最适合给定源代码的摘要。我们的方法通过将条件变分自编码器（CVAE）框架作为一个模块化组件集成到预训练模型中，能够建模观察到的目标摘要的分布，并采样连续嵌入作为前缀，在解码过程中引导生成多种多样的输出。重要的是，我们以参数高效的方式构建了该方法，避免了在使用LLMCs时昂贵的模型重新训练的需求。此外，我们采用双标准重排序方法从生成的摘要中选择子集，优化提供给用户的选项的多样性和准确性。我们使用广泛使用的数据集和当前最先进的预训练代码摘要模型进行了详尽的实验评估，以展示我们方法的有效性和跨模型的适应性。', 'title_zh': '基于变分前缀调优的多样性和准确的代码摘要预训练语言模型方法'}
{'arxiv_id': 'arXiv:2505.09027', 'title': 'Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation', 'authors': 'Yi Cui', 'link': 'https://arxiv.org/abs/2505.09027', 'abstract': 'We introduce WebApp1K, a novel benchmark for evaluating large language models (LLMs) in test-driven development (TDD) tasks, where test cases serve as both prompt and verification for code generation. Unlike traditional approaches relying on natural language prompts, our benchmark emphasizes the ability of LLMs to interpret and implement functionality directly from test cases, reflecting real-world software development practices. Comprising 1000 diverse challenges across 20 application domains, the benchmark evaluates LLMs on their ability to generate compact, functional code under the constraints of context length and multi-feature complexity. Our findings highlight instruction following and in-context learning as critical capabilities for TDD success, surpassing the importance of general coding proficiency or pretraining knowledge. Through comprehensive evaluation of 19 frontier models, we reveal performance bottlenecks, such as instruction loss in long prompts, and provide a detailed error analysis spanning multiple root causes. This work underscores the practical value of TDD-specific benchmarks and lays the foundation for advancing LLM capabilities in rigorous, application-driven coding scenarios.', 'abstract_zh': '我们介绍了一个新的基准WebApp1K，用于评估大型语言模型（LLMs）在测试驱动开发（TDD）任务中的性能，其中测试用例既作为代码生成的提示也作为验证依据。与依赖自然语言提示的传统方法不同，我们的基准测试强调LLMs直接从测试用例中解释和实现功能的能力，反映了实际软件开发实践。该基准包含来自20个应用领域的1000个多样化的挑战，评估LLMs在上下文长度和多特征复杂性约束下生成紧凑功能性代码的能力。我们的研究结果突显了指令遵循和上下文学习对于TDD成功至关重要，超过了一般编程能力和预训练知识的重要性。通过对19个前沿模型进行全面评估，我们揭示了性能瓶颈，如长提示中的指令损失，并提供了涵盖多个根本原因的详细错误分析。这项工作强调了TDD特定基准的实际价值，并为在严格的应用驱动编码场景中推进LLM能力奠定了基础。', 'title_zh': '测试作为提示：一种面向测试驱动开发的LLM代码生成基准'}
{'arxiv_id': 'arXiv:2505.09022', 'title': 'Block-Biased Mamba for Long-Range Sequence Processing', 'authors': 'Annan Yu, N. Benjamin Erichson', 'link': 'https://arxiv.org/abs/2505.09022', 'abstract': "Mamba extends earlier state space models (SSMs) by introducing input-dependent dynamics, and has demonstrated strong empirical performance across a range of domains, including language modeling, computer vision, and foundation models. However, a surprising weakness remains: despite being built on architectures designed for long-range dependencies, Mamba performs poorly on long-range sequential tasks. Understanding and addressing this gap is important for improving Mamba's universality and versatility. In this work, we analyze Mamba's limitations through three perspectives: expressiveness, inductive bias, and training stability. Our theoretical results show how Mamba falls short in each of these aspects compared to earlier SSMs such as S4D. To address these issues, we propose $\\text{B}_2\\text{S}_6$, a simple extension of Mamba's S6 unit that combines block-wise selective dynamics with a channel-specific bias. We prove that these changes equip the model with a better-suited inductive bias and improve its expressiveness and stability. Empirically, $\\text{B}_2\\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks while maintaining Mamba's performance on language modeling benchmarks.", 'abstract_zh': 'Mamba通过引入输入依赖的动力学扩展了早期的状态空间模型（SSMs），并在语言建模、计算机视觉和基础模型等多种领域中表现出强大的实际性能。然而，一个令人惊讶的弱点仍然存在：尽管基于设计长范围依赖性的架构，Mamba在长范围序列任务上的表现不佳。理解并解决这一差距对于提高Mamba的通用性和灵活性至关重要。在本文中，我们从三个角度分析Mamba的局限性：表现力、归纳偏置、训练稳定性。我们的理论结果展示了与早期的SSMs如S4D相比，Mamba在这三个方面存在不足。为了应对这些问题，我们提出了一种B2S6模型，它是Mamba S6单元的简单扩展，结合了块级选择性动力学和通道特定偏置。我们证明这些改变使模型具有更适宜的归纳偏置，并提高了其表现力和稳定性。实验结果表明，B2S6在Long-Range Arena（LRA）任务中优于S4和S4D，在语言建模基准测试中保持了Mamba的性能。', 'title_zh': '块偏好黄鼠狼模型：长距序列处理'}
{'arxiv_id': 'arXiv:2505.08902', 'title': 'Performance Gains of LLMs With Humans in a World of LLMs Versus Humans', 'authors': 'Lucas McCullum, Pelagie Ami Agassi, Leo Anthony Celi, Daniel K. Ebner, Chrystinne Oliveira Fernandes, Rachel S. Hicklen, Mkliwa Koumbia, Lisa Soleymani Lehmann, David Restrepo', 'link': 'https://arxiv.org/abs/2505.08902', 'abstract': 'Currently, a considerable research effort is devoted to comparing LLMs to a group of human experts, where the term "expert" is often ill-defined or variable, at best, in a state of constantly updating LLM releases. Without proper safeguards in place, LLMs will threaten to cause harm to the established structure of safe delivery of patient care which has been carefully developed throughout history to keep the safety of the patient at the forefront. A key driver of LLM innovation is founded on community research efforts which, if continuing to operate under "humans versus LLMs" principles, will expedite this trend. Therefore, research efforts moving forward must focus on effectively characterizing the safe use of LLMs in clinical settings that persist across the rapid development of novel LLM models. In this communication, we demonstrate that rather than comparing LLMs to humans, there is a need to develop strategies enabling efficient work of humans with LLMs in an almost symbiotic manner.', 'abstract_zh': '当前，相当一部分研究致力于将大型语言模型（LLM）与一群人类专家进行比较，而“专家”这一术语在持续更新的LLM版本中往往定义模糊或变化不定。若缺乏适当的保障措施，LLM将威胁到精心发展、旨在确保患者安全的传统临床护理结构。LLM创新的主要驱动力来自于社区研究，如果继续在“人类与LLM对抗”的框架下进行，将进一步加剧这一趋势。因此，未来的研究必须侧重于如何有效表征LLM在临床环境中的安全应用，这涵盖新型LLM模型的快速发展中长期保持有效。在本文中，我们证明了与其将LLM与人类进行比较，不如开发促进人类与LLM高效协作的战略，几乎是一种共生关系。', 'title_zh': 'LLMs与人类在充满LLMs的世界中相比，人类参与下的性能提升'}
{'arxiv_id': 'arXiv:2505.08894', 'title': 'WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp', 'authors': 'Hiba Eltigani, Rukhshan Haroon, Asli Kocak, Abdullah Bin Faisal, Noah Martin, Fahad Dogar', 'link': 'https://arxiv.org/abs/2505.08894', 'abstract': 'Recent advances in generative AI, such as ChatGPT, have transformed access to information in education, knowledge-seeking, and everyday decision-making. However, in many developing regions, access remains a challenge due to the persistent digital divide. To help bridge this gap, we developed WaLLM - a custom AI chatbot over WhatsApp, a widely used communication platform in developing regions. Beyond answering queries, WaLLM offers several features to enhance user engagement: a daily top question, suggested follow-up questions, trending and recent queries, and a leaderboard-based reward system. Our service has been operational for over 6 months, amassing over 14.7K queries from approximately 100 users. In this paper, we present WaLLM\'s design and a systematic analysis of logs to understand user interactions. Our results show that 55% of user queries seek factual information. "Health and well-being" was the most popular topic (28%), including queries about nutrition and disease, suggesting users view WaLLM as a reliable source. Two-thirds of users\' activity occurred within 24 hours of the daily top question. Users who accessed the "Leaderboard" interacted with WaLLM 3x as those who did not. We conclude by discussing implications for culture-based customization, user interface design, and appropriate calibration of users\' trust in AI systems for developing regions.', 'abstract_zh': 'Recent Advances in Generative AI, such as ChatGPT, have transformed access to information in education, knowledge-seeking, and everyday decision-making. However, in many developing regions, access remains a challenge due to the persistent digital divide. To help bridge this gap, we developed WaLLM - a custom AI chatbot over WhatsApp, a widely used communication platform in developing regions. Beyond answering queries, WaLLM offers several features to enhance user engagement: a daily top question, suggested follow-up questions, trending and recent queries, and a leaderboard-based reward system. Our service has been operational for over 6 months, amassing over 14.7K queries from approximately 100 users. In this paper, we present WaLLM\'s design and a systematic analysis of logs to understand user interactions. Our results show that 55% of user queries seek factual information. "Health and well-being" was the most popular topic (28%), including queries about nutrition and disease, suggesting users view WaLLM as a reliable source. Two-thirds of users\' activity occurred within 24 hours of the daily top question. Users who accessed the "Leaderboard" interacted with WaLLM 3x as those who did not. We conclude by discussing implications for culture-based customization, user interface design, and appropriate calibration of users\' trust in AI systems for developing regions.', 'title_zh': 'WaLLM —— 一个基于WhatsApp的LLM驱动聊天机器人的见解'}
{'arxiv_id': 'arXiv:2505.08878', 'title': 'Optimized Couplings for Watermarking Large Language Models', 'authors': 'Dor Tsur, Carol Xuan Long, Claudio Mayrink Verdun, Hsiang Hsu, Haim Permuter, Flavio P. Calmon', 'link': 'https://arxiv.org/abs/2505.08878', 'abstract': "Large-language models (LLMs) are now able to produce text that is, in many cases, seemingly indistinguishable from human-generated content. This has fueled the development of watermarks that imprint a ``signal'' in LLM-generated text with minimal perturbation of an LLM's output. This paper provides an analysis of text watermarking in a one-shot setting. Through the lens of hypothesis testing with side information, we formulate and analyze the fundamental trade-off between watermark detection power and distortion in generated textual quality. We argue that a key component in watermark design is generating a coupling between the side information shared with the watermark detector and a random partition of the LLM vocabulary. Our analysis identifies the optimal coupling and randomization strategy under the worst-case LLM next-token distribution that satisfies a min-entropy constraint. We provide a closed-form expression of the resulting detection rate under the proposed scheme and quantify the cost in a max-min sense. Finally, we provide an array of numerical results, comparing the proposed scheme with the theoretical optimum and existing schemes, in both synthetic data and LLM watermarking. Our code is available at this https URL", 'abstract_zh': '大型语言模型（LLMs）现在能够生成在许多情况下难以与人类生成的内容区分开来的文本。这推动了生成水印的发展，这些水印能够在最小程度上干扰LLM输出的情况下，在LLM生成的文本中嵌入“信号”。本文提供了在单次设置下对文本水印的分析。通过假设检验与辅助信息的视角，我们提出了并分析了水印检测能力和生成文本质量失真的基本权衡。我们认为水印设计的关键组件是在水印检测器与共享的辅助信息之间以及LLM词汇表的随机划分之间建立耦合。我们的分析确定了在满足最小熵约束的最坏情况LLM下一个词分布下的最优耦合和随机化策略。我们提供了所提出方案下检测率的闭式表达式，并从最大最小意义量化成本。最后，我们在合成数据和LLM水印中比较了所提出的方案与理论最优值和现有方案的性能。我们的代码可在以下网址获取：这个 https URL', 'title_zh': '优化耦合用于大型语言模型水印刻印'}
{'arxiv_id': 'arXiv:2505.08849', 'title': 'Improved Algorithms for Differentially Private Language Model Alignment', 'authors': 'Keyu Chen, Hao Tang, Qinglin Liu, Yizhao Xu', 'link': 'https://arxiv.org/abs/2505.08849', 'abstract': 'Language model alignment is crucial for ensuring that large language models (LLMs) align with human preferences, yet it often involves sensitive user data, raising significant privacy concerns. While prior work has integrated differential privacy (DP) with alignment techniques, their performance remains limited. In this paper, we propose novel algorithms for privacy-preserving alignment and rigorously analyze their effectiveness across varying privacy budgets and models. Our framework can be deployed on two celebrated alignment techniques, namely direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF). Through systematic experiments on large-scale language models, we demonstrate that our approach achieves state-of-the-art performance. Notably, one of our algorithms, DP-AdamW, combined with DPO, surpasses existing methods, improving alignment quality by up to 15% under moderate privacy budgets ({\\epsilon}=2-5). We further investigate the interplay between privacy guarantees, alignment efficacy, and computational demands, providing practical guidelines for optimizing these trade-offs.', 'abstract_zh': '语言模型对齐对于确保大型语言模型（LLMs）与人类偏好一致至关重要，但通常涉及敏感用户数据，从而引发重大隐私问题。虽然已有工作将差分隐私（DP）与对齐技术结合使用，但其性能仍然有限。在本文中，我们提出了一种新型的隐私保护对齐算法，并严格分析了其在不同隐私预算和模型下的有效性。我们的框架可以在两种著名的对齐技术，即直接偏好优化（DPO）和人类反馈强化学习（RLHF）上部署。通过大规模语言模型的系统实验，我们证明了我们的方法达到了最先进的性能。值得注意的是，我们的算法之一DP-AdamW，与DPO结合使用，在中等隐私预算（ε=2-5）下，对齐质量提高了高达15%，超过了现有方法。我们进一步探讨了隐私保证、对齐效果和计算需求之间的相互作用，提供了优化这些权衡的实际指南。', 'title_zh': '改进的差分隐私语言模型对齐算法'}
{'arxiv_id': 'arXiv:2505.08844', 'title': 'CellTypeAgent: Trustworthy cell type annotation with Large Language Models', 'authors': 'Jiawen Chen, Jianghao Zhang, Huaxiu Yao, Yun Li', 'link': 'https://arxiv.org/abs/2505.08844', 'abstract': 'Cell type annotation is a critical yet laborious step in single-cell RNA sequencing analysis. We present a trustworthy large language model (LLM)-agent, CellTypeAgent, which integrates LLMs with verification from relevant databases. CellTypeAgent achieves higher accuracy than existing methods while mitigating hallucinations. We evaluated CellTypeAgent across nine real datasets involving 303 cell types from 36 tissues. This combined approach holds promise for more efficient and reliable cell type annotation.', 'abstract_zh': '细胞类型注释是单细胞RNA测序分析中的一个关键但耗时的步骤。我们提出了一种可信赖的大语言模型（LLM）代理CellTypeAgent，它将LLM与相关数据库的验证相结合。CellTypeAgent在准确性上优于现有方法，同时减少了幻觉现象。我们使用涉及36个组织303种细胞类型的九个真实数据集评估了CellTypeAgent。这种结合方法有望提高细胞类型注释的效率和可靠性。', 'title_zh': 'CellTypeAgent: 用大型语言模型进行可信赖的细胞类型注释'}
{'arxiv_id': 'arXiv:2505.08830', 'title': 'Federated Large Language Models: Feasibility, Robustness, Security and Future Directions', 'authors': 'Wenhao Jiang, Yuchuan Luo, Guilin Deng, Silong Chen, Xu Yang, Shihong Wu, Xinwen Gao, Lin Liu, Shaojing Fu', 'link': 'https://arxiv.org/abs/2505.08830', 'abstract': 'The integration of Large Language Models (LLMs) and Federated Learning (FL) presents a promising solution for joint training on distributed data while preserving privacy and addressing data silo issues. However, this emerging field, known as Federated Large Language Models (FLLM), faces significant challenges, including communication and computation overheads, heterogeneity, privacy and security concerns. Current research has primarily focused on the feasibility of FLLM, but future trends are expected to emphasize enhancing system robustness and security. This paper provides a comprehensive review of the latest advancements in FLLM, examining challenges from four critical perspectives: feasibility, robustness, security, and future directions. We present an exhaustive survey of existing studies on FLLM feasibility, introduce methods to enhance robustness in the face of resource, data, and task heterogeneity, and analyze novel risks associated with this integration, including privacy threats and security challenges. We also review the latest developments in defense mechanisms and explore promising future research directions, such as few-shot learning, machine unlearning, and IP protection. This survey highlights the pressing need for further research to enhance system robustness and security while addressing the unique challenges posed by the integration of FL and LLM.', 'abstract_zh': '大型语言模型与联邦学习的整合：挑战与未来方向', 'title_zh': '联邦大型语言模型：可行性、稳健性、安全性及未来发展方向'}
{'arxiv_id': 'arXiv:2505.08827', 'title': 'Self Rewarding Self Improving', 'authors': 'Toby Simonds, Kevin Lopez, Akira Yoshiyama, Dominique Garmier', 'link': 'https://arxiv.org/abs/2505.08827', 'abstract': 'We demonstrate that large language models can effectively self-improve through self-judging without requiring reference solutions, leveraging the inherent asymmetry between generating and verifying solutions. Our experiments on Countdown puzzles and MIT Integration Bee problems show that models can provide reliable reward signals without ground truth answers, enabling reinforcement learning in domains previously not possible. By implementing self-judging, we achieve significant performance gains maintaining alignment with formal verification. When combined with synthetic question generation, we establish a complete self-improvement loop where models generate practice problems, solve them, and evaluate their own performance-achieving an 8% improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on integration tasks. Our findings demonstrate that LLM judges can provide effective reward signals for training models, unlocking many reinforcement learning environments previously limited by the difficulty of creating programmatic rewards. This suggests a potential paradigm shift toward AI systems that continuously improve through self-directed learning rather than human-guided training, potentially accelerating progress in domains with scarce training data or complex evaluation requirements.', 'abstract_zh': '我们展示了大型语言模型可以通过自我评判有效地自我改进，无需参考答案，利用生成和验证解决方案之间的固有不对称性。我们在 Countdown 数独和 MIT Integration Bee 问题上的实验表明，模型可以在没有真实答案的情况下提供可靠的奖励信号，从而在先前不可能的应用领域实现强化学习。通过实施自我评判，我们在保持与形式验证一致性的同时实现了显著的性能提升。结合合成问题生成后，我们建立了一个完整的自我改进循环，其中模型生成练习问题、解决这些问题并评估自己的表现——Qwen 2.5 7B 较基线模型实现了 8% 的提升，并在积分任务上超过了 GPT-4。我们的研究结果显示，语言模型裁判可以为训练模型提供有效的奖励信号，解锁了许多以前因难以创建程序化奖励而受限的强化学习环境。这表明一种潜在的范式转变，即通过自主学习而非人工指导训练实现 AI 系统的持续改进，有可能加速在稀缺训练数据或复杂评估要求领域中的进展。', 'title_zh': '自我奖励自我改进'}
{'arxiv_id': 'arXiv:2505.08823', 'title': 'An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits', 'authors': 'Cody Steinmetz, Gavin Childress, Aaron Herbst, Gavin Jones, Jasdeep Singh, Eli Vang, Keagan Weinstock', 'link': 'https://arxiv.org/abs/2505.08823', 'abstract': 'Large language models (LLMs) have transformed natural-language processing, yet their scale makes real-world deployment costly. Post-training quantization reduces memory and computation but often degrades accuracy, while quantization-aware training can recover performance at the cost of extra training. Pushing quantization to the ternary (2-bit) regime yields even larger savings but is notoriously unstable. Building on recent work showing that a bias-free, RMS-normalized Transformer with straight-through estimation can reach 1.58-bit precision, we demonstrate that simply inserting RMS normalization before every linear projection and applying a gradual, layer-wise quantization schedule stably fine-tunes full-precision checkpoints into ternary LLMs. Our approach matches or surpasses more elaborate knowledge-distillation pipelines on standard language-modeling benchmarks without adding model complexity. These results indicate that careful normalization alone can close much of the accuracy gap between ternary and full-precision LLMs, making ultra-low-bit inference practical.', 'abstract_zh': '基于 RMS 归一化和逐层量化调度的稳定三值大语言模型精调', 'title_zh': '额外的RMSNorm即可用于将微调精度提升至1.58比特'}
