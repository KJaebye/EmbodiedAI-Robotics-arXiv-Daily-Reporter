{'arxiv_id': 'arXiv:2508.11524', 'title': 'Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models', 'authors': 'Wenkai Yu, Jianhang Tang, Yang Zhang, Shanjiang Tang, Kebing Jin, Hankz Hankui Zhuo', 'link': 'https://arxiv.org/abs/2508.11524', 'abstract': 'Addressing large-scale planning problems has become one of the central challenges in the planning community, deriving from the state-space explosion caused by growing objects and actions. Recently, researchers have explored the effectiveness of leveraging Large Language Models (LLMs) to generate helpful actions and states to prune the search space. However, prior works have largely overlooked integrating LLMs with domain-specific knowledge to ensure valid plans. In this paper, we propose a novel LLM-assisted planner integrated with problem decomposition, which first decomposes large planning problems into multiple simpler sub-tasks. Then we explore two novel paradigms to utilize LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where LLM4Inspire provides heuristic guidance according to general knowledge and LLM4Predict employs domain-specific knowledge to infer intermediate conditions. We empirically validate the effectiveness of our planner across multiple domains, demonstrating the ability of search space partition when solving large-scale planning problems. The experimental results show that LLMs effectively locate feasible solutions when pruning the search space, where infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds particular promise compared with LLM4Inspire, which offers general knowledge within LLMs.', 'abstract_zh': '地址大规模规划问题已成为规划社区的核心挑战之一，源于对象和动作增多引起的状态空间爆炸。最近，研究人员探索了利用大型语言模型（LLMs）生成有助于裁剪搜索空间的有益动作和状态的有效性。然而，先前的工作大多忽视了将LLMs与领域特定知识集成以确保有效计划的重要性。在本文中，我们提出了一种结合问题分解的新型LLM辅助规划器，首先将大规模规划问题分解为多个更简单的子任务。然后，我们探索了利用LLMs的两种新颖范式，即LLM4Inspire和LLM4Predict，以协助问题分解，其中LLM4Inspire根据通用知识提供启发式指导，而LLM4Predict利用领域特定知识推断中间条件。我们跨多个领域 empirically 验证了我们规划器的有效性，展示了在解决大规模规划问题时对搜索空间分区的能力。实验结果表明，当裁剪搜索空间时，LLMs能够有效定位可行解，其中将领域特定知识融入LLMs，即LLM4Predict，相较于提供通用知识的LLM4Inspire更具潜力。', 'title_zh': '启发还是预测？探索大型语言模型辅助经典规划器的新范式'}
{'arxiv_id': 'arXiv:2508.11452', 'title': 'Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps', 'authors': 'Kangyu Wang, Hongliang He, Lin Liu, Ruiqi Liang, Zhenzhong Lan, Jianguo Li', 'link': 'https://arxiv.org/abs/2508.11452', 'abstract': 'Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have ushered in a new era of AI capabilities, demonstrating near-human-level performance across diverse scenarios. While numerous benchmarks (e.g., MMLU) and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the development of LLMs and MLLMs, most rely on static datasets or crowdsourced general-domain prompts, often falling short of reflecting performance in real-world applications. To bridge this critical gap, we present Inclusion Arena, a live leaderboard that ranks models based on human feedback collected directly from AI-powered applications. Our platform integrates pairwise model comparisons into natural user interactions, ensuring evaluations reflect practical usage scenarios. For robust model ranking, we employ the Bradley-Terry model augmented with two key innovations: (1) Placement Matches, a cold-start mechanism to quickly estimate initial ratings for newly integrated models, and (2) Proximity Sampling, an intelligent comparison strategy that prioritizes battles between models of similar capabilities to maximize information gain and enhance rating stability. Extensive empirical analyses and simulations demonstrate that Inclusion Arena yields reliable and stable rankings, exhibits higher data transitivity compared to general crowdsourced datasets, and significantly mitigates the risk of malicious manipulation. By fostering an open alliance between foundation models and real-world applications, Inclusion Arena aims to accelerate the development of LLMs and MLLMs truly optimized for practical, user-centric deployments. The platform is publicly accessible at this https URL.', 'abstract_zh': '包容竞技场：一种基于直接用户反馈的实时排行榜，用于评估大型语言模型和多模态大型语言模型', 'title_zh': '包容性竞技场：一个评估大型基础模型与实际应用的开源平台'}
{'arxiv_id': 'arXiv:2508.11416', 'title': 'AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager', 'authors': 'Xuhua Zhao, Yuxuan Xie, Caihua Chen, Yuxiang Sun', 'link': 'https://arxiv.org/abs/2508.11416', 'abstract': 'Recent advances in mathematical reasoning and the long-term planning capabilities of large language models (LLMs) have precipitated the development of agents, which are being increasingly leveraged in business operations processes. Decision models to optimize inventory levels are one of the core elements of operations management. However, the capabilities of the LLM agent in making inventory decisions in uncertain contexts, as well as the decision-making biases (e.g. framing effect, etc.) of the agent, remain largely unexplored. This prompts concerns regarding the capacity of LLM agents to effectively address real-world problems, as well as the potential implications of biases that may be present. To address this gap, we introduce AIM-Bench, a novel benchmark designed to assess the decision-making behaviour of LLM agents in uncertain supply chain management scenarios through a diverse series of inventory replenishment experiments. Our results reveal that different LLMs typically exhibit varying degrees of decision bias that are similar to those observed in human beings. In addition, we explored strategies to mitigate the pull-to-centre effect and the bullwhip effect, namely cognitive reflection and implementation of information sharing. These findings underscore the need for careful consideration of the potential biases in deploying LLMs in Inventory decision-making scenarios. We hope that these insights will pave the way for mitigating human decision bias and developing human-centred decision support systems for supply chains.', 'abstract_zh': 'Recent advances in数学推理和大规模语言模型（LLMs）的长期规划能力推动了代理的开发，这些代理在业务操作流程中日益受到重视。库存优化决策模型是运营管理的核心要素之一。然而，LLM代理在不确定环境下进行库存决策的能力以及代理的决策偏见（如框架效应等）仍鲜有研究。这引发对其能否有效解决实际问题及潜在偏见影响的担忧。为应对这一空白，我们引入了AIM-Bench，这是一种新型基准，旨在通过一系列多样的库存补充实验评估LLM代理在不确定供应链管理场景中的决策行为。我们的结果显示，不同LLM通常表现出不同程度与人类相似的决策偏见。此外，我们探索了减轻中心效应和牛鞭效应的策略，如认知反思和信息共享实施。这些发现强调了在库存决策场景中部署LLM时谨慎考虑潜在偏见的重要性。我们希望这些见解能为减轻人类决策偏见并为供应链开发以人为中心的决策支持系统铺平道路。', 'title_zh': 'AIM-Bench: 评估行动者LLM决策偏见的库存管理者评估benchmark'}
{'arxiv_id': 'arXiv:2508.11252', 'title': 'Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information', 'authors': 'Youcheng Huang, Bowen Qin, Chen Huang, Duanyu Feng, Xi Yang, Wenqiang Lei', 'link': 'https://arxiv.org/abs/2508.11252', 'abstract': "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving abilities in mathematics, as evaluated by existing benchmarks exclusively on well-defined problems. However, such evaluation setup constitutes a critical gap, since a genuine intelligent agent should not only solve problems (as a math quiz solver), but also be able~to ask for information when the problems lack sufficient information, enabling proactivity in responding users' requests. To bridge such gap, we proposes a new dataset consisting of two types of incomplete problems with diverse contexts. Based on the dataset, our systematical evaluation of LRMs reveals their inability in proactively asking for information. In addition, we uncover the behaviors related to overthinking and hallucination of LRMs, and highlight the potential and challenges of supervised fine-tuning in learning such ability. We hope to provide new insights in developing LRMs with genuine intelligence, rather than just solving problems.", 'abstract_zh': '大推理模型（LRMs）在数学问题求解方面的卓越能力已在现有的基准测试中得到评估，这些基准测试仅针对清晰定义的问题。然而，这种评估方式存在一个关键缺陷，因为真正的智能代理不仅应该能够解决问题（如数学测验解答器），还应该能够在问题缺乏足够信息时主动请求信息，以增强对用户请求的主动响应。为了弥合这一差距，我们提出了一种新的数据集，包含两类具有多样化背景的不完整问题。基于该数据集，我们系统性地评估了大推理模型的能力，揭示了它们在主动请求信息方面的能力不足。此外，我们还发现了大推理模型在过度思考和妄想方面的行为，并强调了在监督微调中学习这种能力的潜力和挑战。我们希望为开发具备真正智能的大推理模型提供新的洞察，而不仅仅是解决问题。', 'title_zh': '超越解决数学测题：评估大型 reasoning 模型请求信息的能力lóg\nuser\nBeyond Human-Level: Understanding the General Competence of Large Language Models Through Mathematical Reasoning超出人类水平：通过数学推理理解大型语言模型的通用能力'}
{'arxiv_id': 'arXiv:2508.11628', 'title': 'Is ChatGPT-5 Ready for Mammogram VQA?', 'authors': 'Qiang Li, Shansong Wang, Mingzhe Hu, Mojtaba Safari, Zachary Eidex, Xiaofeng Yang', 'link': 'https://arxiv.org/abs/2508.11628', 'abstract': 'Mammogram visual question answering (VQA) integrates image interpretation with clinical reasoning and has potential to support breast cancer screening. We systematically evaluated the GPT-5 family and GPT-4o model on four public mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment, abnormality detection, and malignancy classification tasks. GPT-5 consistently was the best performing model but lagged behind both human experts and domain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores among GPT variants in density (56.8%), distortion (52.5%), mass (64.5%), calcification (63.5%), and malignancy (52.8%) classification. On InBreast, it attained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0% malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection and 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared with human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and specificity (52.3%). While GPT-5 exhibits promising capabilities for screening tasks, its performance remains insufficient for high-stakes clinical imaging applications without targeted domain adaptation and optimization. However, the tremendous improvements in performance from GPT-4o to GPT-5 show a promising trend in the potential for general large language models (LLMs) to assist with mammography VQA tasks.', 'abstract_zh': '乳腺X线摄影视觉问答（VQA）将图像解释与临床推理相结合，有望支持乳腺癌筛查。我们系统性地评估了GPT-5家族和GPT-4o模型在EMBED、InBreast、CMMD和CBIS-DDSM四个公开的乳腺X线图像数据集上的BI-RADS评估、异常检测和恶性分类任务。GPT-5在这些任务上表现稳定，但均逊于人类专家和领域特定微调模型。在EMBED数据集上，GPT-5在密度、扭曲、肿块、钙化和恶性分类上的表现最佳，分别为56.8%、52.5%、64.5%、63.5%和52.8%。在InBreast数据集上，GPT-5获得了36.9%的BI-RADS准确率、45.9%的异常检测准确率和35.0%的恶性分类准确率。在CMMD数据集上，GPT-5实现了32.3%的异常检测准确率和55.0%的恶性分类准确率。在CBIS-DDSM数据集上，GPT-5实现了69.3%的BI-RADS准确率、66.0%的异常检测准确率和58.2%的恶性分类准确率。与人类专家估计相比，GPT-5的灵敏度为63.5%，特异性为52.3%。虽然GPT-5在筛查任务上表现有潜力，但在未经目标领域适应和优化的情况下，其性能仍不足以支持高风险临床成像应用。然而，从GPT-4o到GPT-5在性能上的巨大提升显示了通用大型语言模型（LLM）在辅助乳腺X线摄影VQA任务上的潜在前景。', 'title_zh': 'ChatGPT-5准备好应对乳腺X光片VQA任务了吗？'}
{'arxiv_id': 'arXiv:2508.11616', 'title': 'Controlling Multimodal LLMs via Reward-guided Decoding', 'authors': "Oscar Mañas, Pierluca D'Oro, Koustuv Sinha, Adriana Romero-Soriano, Michal Drozdzal, Aishwarya Agrawal", 'link': 'https://arxiv.org/abs/2508.11616', 'abstract': "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it is becoming increasingly desirable to adapt them for diverse user needs. In this paper, we study the adaptation of MLLMs through controlled decoding. To achieve this, we introduce the first method for reward-guided decoding of MLLMs and demonstrate its application in improving their visual grounding. Our method involves building reward models for visual grounding and using them to guide the MLLM's decoding process. Concretely, we build two separate reward models to independently control the degree of object precision and recall in the model's output. Our approach enables on-the-fly controllability of an MLLM's inference process in two ways: first, by giving control over the relative importance of each reward function during decoding, allowing a user to dynamically trade off object precision for recall in image captioning tasks; second, by giving control over the breadth of the search during decoding, allowing the user to control the trade-off between the amount of test-time compute and the degree of visual grounding. We evaluate our method on standard object hallucination benchmarks, showing that it provides significant controllability over MLLM inference, while consistently outperforming existing hallucination mitigation methods.", 'abstract_zh': '多模态大型语言模型的适应性研究：基于奖励引导的解码方法及其在视觉定位改进中的应用', 'title_zh': '通过奖励引导解码控制多模态大规模语言模型'}
{'arxiv_id': 'arXiv:2508.11599', 'title': 'CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection', 'authors': 'Zhihao Li, Zimo Ji, Tao Zheng, Hao Ren, Xiao Lan', 'link': 'https://arxiv.org/abs/2508.11599', 'abstract': 'Cryptographic algorithms are fundamental to modern security, yet their implementations frequently harbor subtle logic flaws that are hard to detect. We introduce CryptoScope, a novel framework for automated cryptographic vulnerability detection powered by Large Language Models (LLMs). CryptoScope combines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation (RAG), guided by a curated cryptographic knowledge base containing over 12,000 entries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily derived from real-world CVE vulnerabilities, complemented by cryptographic challenges from major Capture The Flag (CTF) competitions and synthetic examples across 11 programming languages. CryptoScope consistently improves performance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%, GPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9 previously undisclosed flaws in widely used open-source cryptographic projects.', 'abstract_zh': '加密算法是现代安全的基础，但其实现中常常隐藏难以检测的细微逻辑漏洞。我们介绍了一种新型框架CryptoScope，该框架利用大型语言模型（LLMs）自动检测加密漏洞。CryptoScope 结合了思维链（CoT）提示与检索增强生成（RAG），并辅以一个包含超过12,000条条目的定制化加密知识库。我们对CryptoScope进行了评估，使用LLM-CLVA基准测试，该测试包含92个案例，主要源自实际世界中的CVE漏洞，同时辅以来自主要Capture The Flag（CTF）竞标大赛的加密挑战和跨11种编程语言的合成示例。CryptoScope在性能上持续超越强大的LLM基线，分别提高了DeepSeek-V3的11.62%、GPT-4o-mini的20.28%和GLM-4-Flash的28.69%，并发现9个广泛使用的开源加密项目中的未披露漏洞。', 'title_zh': 'CryptoScope: 利用大规模语言模型进行自动化密码逻辑漏洞检测'}
{'arxiv_id': 'arXiv:2508.11582', 'title': 'Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models', 'authors': 'Qiguang Chen, Dengyun Peng, Jinhao Liu, HuiKang Su, Jiannan Guan, Libo Qin, Wanxiang Che', 'link': 'https://arxiv.org/abs/2508.11582', 'abstract': "Recent advancements in large language models (LLMs) have greatly improved their capabilities on complex reasoning tasks through Long Chain-of-Thought (CoT). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. To improve the efficiency, current methods often rely on human-defined difficulty priors, which do not align with the LLM's self-awared difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to dynamically assess and adjust their reasoning depth in response to problem complexity. DR. SAF integrates three key components: Boundary Self-Awareness Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism. These components allow models to optimize their reasoning processes, balancing efficiency and accuracy without compromising performance. Our experimental results demonstrate that DR. SAF achieves a 49.27% reduction in total response tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain in token efficiency and a 5x reduction in training time, making it well-suited to resource-limited settings. During extreme training, DR. SAF can even surpass traditional instruction-based models in token efficiency with more than 16% accuracy improvement.", 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）通过长链推理（Long Chain-of-Thought，Long CoT）大幅提高了其在复杂推理任务上的能力。然而，这种方法通常会导致显著冗余，损害计算效率并造成实时应用中的重大延迟。为了提高效率，当前方法往往依赖于人工定义的难度先验，这些先验不与LLM自身的难度感知相匹配，导致效率低下。在本文中，我们引入了动态推理边界自意识框架（DR.SAF），使模型能够动态评估和调整其推理深度以应对问题的复杂性。DR.SAF集成了三个关键组成部分：边界自意识对齐、自适应奖励管理以及边界保持机制。这些组件允许模型优化其推理过程，在提高效率的同时保持准确性。实验结果表明，DR.SAF在保持最小准确性损失的情况下，将总响应 tokens减少了49.27%。此外，该框架还实现了6.59倍的 token效率提升和5倍的训练时间减少，使其适合资源受限的环境。在极端训练过程中，DR.SAF甚至在准确率提高超过16%的情况下，在token效率方面超过了传统的基于指令的模型。', 'title_zh': '感知先思考少些：动态边界自我意识推动大规模语言模型极端高效推理'}
{'arxiv_id': 'arXiv:2508.11551', 'title': 'ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization', 'authors': 'Shengzhuang Chen, Xu Ouyang, Michael Arthur Leopold Pearce, Thomas Hartvigsen, Jonathan Richard Schwarz', 'link': 'https://arxiv.org/abs/2508.11551', 'abstract': 'Determining the optimal data mixture for large language model training remains a challenging problem with an outsized impact on performance. In practice, language model developers continue to rely on heuristic exploration since no learning-based approach has emerged as a reliable solution. In this work, we propose to view the selection of training data mixtures as a black-box hyperparameter optimization problem, for which Bayesian Optimization is a well-established class of appropriate algorithms. Firstly, we cast data mixture learning as a sequential decision-making problem, in which we aim to find a suitable trade-off between the computational cost of training exploratory (proxy-) models and final mixture performance. Secondly, we systematically explore the properties of transferring mixtures learned at a small scale to larger-scale experiments, providing insights and highlighting opportunities for research at a modest scale. By proposing Multi-fidelity Bayesian Optimization as a suitable method in this common scenario, we introduce a natural framework to balance experiment cost with model fit, avoiding the risks of overfitting to smaller scales while minimizing the number of experiments at high cost. We present results for pre-training and instruction finetuning across models ranging from 1 million to 7 billion parameters, varying from simple architectures to state-of-the-art models and benchmarks spanning dozens of datasets. We demonstrate consistently strong results relative to a wide range of benchmarks, showingspeed-ups of over 500% in determining the best data mixture on our largest experiments relative to recent baselines. In addition, we broaden access to research by sharing ADMIRE IFT Runs, a dataset of 460 full training & evaluation runs across various model sizes worth over 13,000 GPU hours, greatly reducing the cost of conducting research in this area.', 'abstract_zh': '确定大规模语言模型训练的最佳数据混合比例仍然是一个具有显著影响但尚未解决的难题。实际上，语言模型开发者继续依赖启发式探索，因为尚未出现可靠的学习方法。在本文中，我们将训练数据混合的选择视为黑盒超参数优化问题，并提出使用贝叶斯优化作为合适的算法类别。首先，我们将数据混合学习视为一个顺序决策问题，旨在寻找训练探索性（代理）模型的计算成本与最终混合性能之间的合适权衡。其次，系统地探讨了从小规模实验转移混合数据到大规模实验的特性，为适度规模的研究提供了见解并揭示了研究机会。通过提议多精度贝叶斯优化作为适合该常见场景的方法，我们介绍了一个自然框架，用于平衡实验成本与模型拟合度，避免对小规模过度拟合的风险，同时将高成本实验次数降到最低。我们报告了涵盖从100万到70亿参数模型、从简单架构到最新模型和基准数据集的广泛范围的预训练和指令调优结果。在我们的最大规模实验中，确定最佳数据混合的比例比最近的基准方法快超过500%。此外，我们通过共享包含460次不同模型规模的完整训练与评估运行的ADMIRE IFT Runs数据集（超过13,000个GPU小时），扩大了对该领域的研究访问。', 'title_zh': 'ADMIRE-BayesOpt: 加速数据混合重新加权的语言模型中的贝叶斯优化'}
{'arxiv_id': 'arXiv:2508.11454', 'title': 'Reference Points in LLM Sentiment Analysis: The Role of Structured Context', 'authors': 'Junichiro Niimi', 'link': 'https://arxiv.org/abs/2508.11454', 'abstract': 'Large language models (LLMs) are now widely used across many fields, including marketing research. Sentiment analysis, in particular, helps firms understand consumer preferences. While most NLP studies classify sentiment from review text alone, marketing theories, such as prospect theory and expectation--disconfirmation theory, point out that customer evaluations are shaped not only by the actual experience but also by additional reference points. This study therefore investigates how the content and format of such supplementary information affect sentiment analysis using LLMs. We compare natural language (NL) and JSON-formatted prompts using a lightweight 3B parameter model suitable for practical marketing applications. Experiments on two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with additional information outperforms all baselines without fine-tuning: Macro-F1 rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it deployable in resource-constrained edge devices. Furthermore, a follow-up analysis confirms that performance gains stem from genuine contextual reasoning rather than label proxying. This work demonstrates that structured prompting can enable smaller models to achieve competitive performance, offering a practical alternative to large-scale model deployment.', 'abstract_zh': '大型语言模型中的补充信息如何影响基于LLM的情感分析：以Yelp类别为例', 'title_zh': 'LLM情感分析中的参考点：结构化上下文的作用'}
{'arxiv_id': 'arXiv:2508.11408', 'title': 'On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting', 'authors': 'Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, Jingren Zhou', 'link': 'https://arxiv.org/abs/2508.11408', 'abstract': "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two prominent post-training paradigms for refining the capabilities and aligning the behavior of Large Language Models (LLMs). Existing approaches that integrate SFT and RL often face the risk of disrupting established model patterns and inducing overfitting to expert data. To address this, we present a novel investigation into the unified view of SFT and RL through an off-policy versus on-policy lens. We propose CHORD, a framework for the Controllable Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic Weighting, which reframes SFT not as a separate stage but as a dynamically weighted auxiliary objective within the on-policy RL process. Based on an analysis of off-policy expert data's influence at both holistic and granular levels, we incorporate a dual-control mechanism in CHORD. Specifically, the framework first employs a global coefficient to holistically guide the transition from off-policy imitation to on-policy exploration, and then applies a token-wise weighting function that enables granular learning from expert tokens, which preserves on-policy exploration and mitigates disruption from off-policy data. We conduct extensive experiments on widely used benchmarks, providing empirical evidence that CHORD achieves a stable and efficient learning process. By effectively harmonizing off-policy expert data with on-policy exploration, CHORD demonstrates significant improvements over baselines. We release the implementation at this https URL to inspire further research.", 'abstract_zh': '监督微调（SFT）和强化学习（RL）是两种重要的后训练范式，用于 refinement 大型语言模型（LLMs）的能力和行为对齐。现有将 SFT 和 RL 结合的方法往往面临破坏模型稳定性和过度拟合专家数据的风险。为了解决这一问题，我们提出了一种通过 off-policy 和 on-policy 视角统一 SFT 和 RL 的新颖研究。我们提出了 CHORD 框架，通过动态加权实现可控的 off- 和 on-policy 强化学习谐调，将 SFT 不视为单独的阶段，而是作为 on-policy RL 过程中的动态加权辅助目标。基于 off-policy 专家数据在整体和细粒度层面的影响分析，我们在 CHORD 中引入了一种双控制机制。具体地，框架首先使用全局系数在整体上引导 off-policy 仿真的过渡到 on-policy 探索，然后应用一个按词加权函数，使模型能够从专家词中进行细粒度学习，从而保留 on-policy 探索并降低 off-policy 数据的干扰。我们在广泛使用的基准上进行了广泛的实验，提供了实证证据表明 CHORD 能够实现稳定和高效的训练过程。通过有效调和 off-policy 专家数据与 on-policy 探索，CHORD 在基线上显示出显著的改进。我们在此 https:// 放开源代码以启发 further 研究。', 'title_zh': '基于策略 RL 结合离策略专家：动态权重调控的监督微调与强化学习和谐融合'}
{'arxiv_id': 'arXiv:2508.11398', 'title': 'Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis', 'authors': 'Mithat Can Ozgun, Jiahuan Pei, Koen Hindriks, Lucia Donatelli, Qingzhi Liu, Xin Sun, Junxiao Wang', 'link': 'https://arxiv.org/abs/2508.11398', 'abstract': "LLM-based agents have emerged as transformative tools capable of executing complex tasks through iterative planning and action, achieving significant advancements in understanding and addressing user needs. Yet, their effectiveness remains limited in specialized domains such as mental health diagnosis, where they underperform compared to general applications. Current approaches to integrating diagnostic capabilities into LLMs rely on scarce, highly sensitive mental health datasets, which are challenging to acquire. These methods also fail to emulate clinicians' proactive inquiry skills, lack multi-turn conversational comprehension, and struggle to align outputs with expert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the first LLM-based agent workflow designed to autonomously generate DSM-5 Level-1 diagnostic questionnaires. By simulating therapist-client dialogues with specific client profiles, the framework delivers transparent, step-by-step disorder predictions, producing explainable and trustworthy results. This workflow serves as a complementary tool for mental health diagnosis, ensuring adherence to ethical and legal standards. Through comprehensive experiments, we evaluate leading LLMs across three critical dimensions: conversational realism, diagnostic accuracy, and explainability. Our datasets and implementations are fully open-sourced.", 'abstract_zh': "基于LLM的代理在通过迭代规划和行动执行复杂任务方面 emerged as transformative tools capable of achieving significant advancements in understanding and addressing user needs. Yet, their effectiveness remains limited in specialized domains such as mental health diagnosis, where they underperform compared to general applications. Current approaches to integrating diagnostic capabilities into LLMs rely on scarce, highly sensitive mental health datasets, which are challenging to acquire. These methods also fail to emulate clinicians' proactive inquiry skills, lack multi-turn conversational comprehension, and struggle to align outputs with expert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the first LLM-based agent workflow designed to autonomously generate DSM-5 Level-1 diagnostic questionnaires. By simulating therapist-client dialogues with specific client profiles, the framework delivers transparent, step-by-step disorder predictions, producing explainable and trustworthy results. This workflow serves as a complementary tool for mental health diagnosis, ensuring adherence to ethical and legal standards. Through comprehensive experiments, we evaluate leading LLMs across three critical dimensions: conversational realism, diagnostic accuracy, and explainability. Our datasets and implementations are fully open-sourced.\n\n基于LLM的代理在通过迭代规划和行动执行复杂任务方面取得了革命性进展，实现了对用户需求的理解和解决的显著进步。然而，在心理健康诊断等专业领域，它们的效果逊于通用应用。目前将诊断能力集成到LLM中的方法依赖于稀缺、高度敏感的心理健康数据集，获取这些数据极具挑战性。这些方法也未能模拟临床医生的主动探索技能，缺乏多轮对话理解能力，难以使输出与专家临床推理对齐。为了弥补这些差距，我们提出了DSM5AgentFlow，这是第一个基于LLM的代理工作流，旨在自主生成DSM-5 Level-1诊断问卷。通过模拟具有特定客户档案的治疗师-客户对话，该框架提供了透明的、逐步的障碍预测，生成可解释和可信赖的结果。该工作流作为心理健康诊断的一种辅助工具，确保遵守伦理和法律标准。通过全面的实验，我们从三个关键维度评估了领先的人工智能模型：对话逼真性、诊断准确性以及可解释性。我们的数据集和实现均已完全开源。", 'title_zh': '可信AI心理治疗：多Agent大型语言模型工作流在咨询与可解释的精神障碍诊断中的应用'}
{'arxiv_id': 'arXiv:2508.11386', 'title': 'Retrieval-augmented reasoning with lean language models', 'authors': 'Ryan Sze-Yin Chan, Federico Nanni, Tomas Lazauskas, Rosie Wood, Penelope Yong, Lionel Tarassenko, Mark Girolami, James Geddes, Andrew Duncan', 'link': 'https://arxiv.org/abs/2508.11386', 'abstract': 'This technical report details a novel approach to combining reasoning and retrieval augmented generation (RAG) within a single, lean language model architecture. While existing RAG systems typically rely on large-scale models and external APIs, our work addresses the increasing demand for performant and privacy-preserving solutions deployable in resource-constrained or secure environments. Building on recent developments in test-time scaling and small-scale reasoning models, we develop a retrieval augmented conversational agent capable of interpreting complex, domain-specific queries using a lightweight backbone model. Our system integrates a dense retriever with fine-tuned Qwen2.5-Instruct models, using synthetic query generation and reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a curated corpus, in this case, the NHS A-to-Z condition pages. We explore the impact of summarisation-based document compression, synthetic data design, and reasoning-aware fine-tuning on model performance. Evaluation against both non-reasoning and general-purpose lean models demonstrates that our domain-specific fine-tuning approach yields substantial gains in answer accuracy and consistency, approaching frontier-level performance while remaining feasible for local deployment. All implementation details and code are publicly released to support reproducibility and adaptation across domains.', 'abstract_zh': '本技术报告详细介绍了在一个精简的语言模型架构中结合推理和检索增强生成（RAG）的新方法。尽管现有的RAG系统通常依赖大型模型和外部API，我们的工作致力于满足对高性能且保护隐私的解决方案的需求，这些解决方案可以在资源受限或安全环境中部署。基于最近在测试时扩展和小型推理模型发展方面的进展，我们开发了一个检索增强的对话代理，能够使用轻量级的基础模型解释复杂的、领域特定的查询。我们的系统将稠密检索器与细调的Qwen2.5-Instruct模型结合使用，利用来自前沿模型（如DeepSeek-R1）的合成查询生成和推理轨迹对精选语料库（例如，NHS A-to-Z病症页面）进行加权。我们探讨了基于总结的文档压缩、合成数据设计以及推理意识微调对模型性能的影响。与其他非推理和通用精简模型的评估表明，我们针对特定领域的微调方法在答案的准确性和一致性方面取得了显著提升，同时保持了在本地部署方面的可行性。所有实施细节和代码均已公开发布，以支持可重复性和跨领域的适应性。', 'title_zh': '基于精简语言模型的检索增强推理'}
{'arxiv_id': 'arXiv:2508.11383', 'title': 'When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs', 'authors': 'Mikhail Seleznyov, Mikhail Chaichuk, Gleb Ershov, Alexander Panchenko, Elena Tutubalina, Oleg Somov', 'link': 'https://arxiv.org/abs/2508.11383', 'abstract': "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic variations in prompt phrasing and formatting. In this work, we present the first systematic evaluation of 5 methods for improving prompt robustness within a unified experimental framework. We benchmark these techniques on 8 models from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions dataset. Our evaluation covers robustness methods from both fine-tuned and in-context learning paradigms, and tests their generalization against multiple types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and DeepSeek V3 to assess frontier models' current robustness to format perturbations. Our findings offer actionable insights into the relative effectiveness of these robustness methods, enabling practitioners to make informed decisions when aiming for stable and reliable LLM performance in real-world applications. Code: this https URL.", 'abstract_zh': '大规模语言模型（LLMs）对提示语句措辞和格式中的细微、非语义变化极为敏感。在本文中，我们提出了一种统一实验框架下的首次系统性评估，评测了5种提高提示鲁棒性的方法。我们在来自Llama、Qwen和Gemma家族的8个模型上，使用来自Natural Instructions数据集的52个任务进行了基准测试。我们的评估涵盖了调优和上下文学习范式下的鲁棒性方法，并测试了它们在多种分布偏移类型下的泛化能力。最后，我们将分析扩展到GPT-4.1和DeepSeek V3，以评估前沿模型对格式扰动的当前鲁棒性。我们的研究提供了这些鲁棒性方法相对有效性的实用见解，帮助实践者在实际应用中实现稳定可靠的LLM性能。代码：this https URL。', 'title_zh': '标点符号 Matters：大规模比较 LLMs 的提示稳健性方法'}
{'arxiv_id': 'arXiv:2508.11356', 'title': 'ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism', 'authors': 'Jia Liu, ChangYi He, YingQiao Lin, MingMin Yang, FeiYang Shen, ShaoGuo Liu, TingTing Gao', 'link': 'https://arxiv.org/abs/2508.11356', 'abstract': "Recent advancements in Large Language Models have yielded significant improvements in complex reasoning tasks such as mathematics and programming. However, these models remain heavily dependent on annotated data and exhibit limited adaptability in unsupervised scenarios. To address these limitations, test-time reinforcement learning (TTRL) has been proposed, which enables self-optimization by leveraging model-generated pseudo-labels. Despite its promise, TTRL faces several key challenges, including high inference costs due to parallel rollouts and early-stage estimation bias that fosters overconfidence, reducing output diversity and causing performance plateaus. To address these challenges, we introduce an entropy-based mechanism to enhance the exploration-exploitation balance in test-time reinforcement learning through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our approach enables Llama3.1-8B to achieve a 68 percent relative improvement in Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of the rollout tokens budget. This highlights our method's ability to effectively optimize the trade-off between inference efficiency, diversity, and estimation robustness, thereby advancing unsupervised reinforcement learning for open-domain reasoning tasks.", 'abstract_zh': '近年来，大型语言模型的进展在复杂推理任务（如数学和编程等方面取得了显著的改进。然而，这些模型仍然高度依赖于标注数据，并在无监督场景中表现出局限性。为了应对这些限制，，我们提出了在训练时强化学习（建议-time reinforcement learning，）中的混合策略策略方法（（-Time Reinforcement Learning with Reset-Based Mechanism,）（简称TTRL），该方法通过利用生成的伪标签增强了了优化过程。尽管TTRL展现出前景，，仍面临几项挑战，如包括推理成本由于平行并并的并行阶段的偏差增加使得自信度增加以及输出多样性减少和性能 plateau呈现等。为了应对这些挑战，我们引入了一种基于熵的机制来增强在训练时强化学习中的探索--exploitation利用平衡通过两个机制：熵门限树众数机制（Entropy-F Tree Majority Mechanism，，简称ETMR）和基于熵的优势估计机制（Entropy-Based Advantage Ententin Mechanon，，）简称EAR）。与基于熵的Llama3-8ient--8B相比，我们的方法在PASet on--entity测量上的AIME on2-4基准上上实现了68%ent的推理令牌预算上改进了68 onentonent的Ponentent- eonanententon。这突显了我们an on的能力来有效地在推理效率、多样性以及稳定性之间实现 ean opte enton，从而推进无监督强化学习在跨域任务上 on域任务方面的应用情应用。', 'title_zh': 'ETTRL：通过熵机制在LLM测试时强化学习中平衡探索与利用'}
{'arxiv_id': 'arXiv:2508.11291', 'title': 'Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks', 'authors': 'Rui Bao, Nan Xue, Yaping Sun, Zhiyong Chen', 'link': 'https://arxiv.org/abs/2508.11291', 'abstract': 'The integration of wireless communications and Large Language Models (LLMs) is poised to unlock ubiquitous intelligent services, yet deploying them in wireless edge-device collaborative environments presents a critical trade-off between inference quality and end-to-end latency. A fundamental mismatch exists between task complexity and resource allocation: offloading simple queries invites prohibitive latency, while on-device models lack the capacity for demanding computations. To address this challenge, we propose a dynamic, quality-latency aware routing framework that orchestrates inference between a lightweight model on the mobile device and a powerful model on the edge server. Our framework employs two distinct cost models: for single-turn queries, it fuses a BERT-predicted semantic score with communication and computation overheads; for multi-turn dialogues, it further quantifies context-aware costs arising from model switching and KV-cache management. While maintaining full inference quality, extensive experiments demonstrate that our framework cuts average response latency by 5-15% and reduces large model invocations by 10-20% against competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.', 'abstract_zh': '无线通信与大规模语言模型的集成有望解锁泛在智能服务，但在无线边缘设备协作环境中部署它们带来了推理质量与端到端延迟之间的关键权衡。任务复杂性和资源分配之间存在根本性的 mismatch：将简单查询卸载会导致不可接受的延迟，而设备上的模型无法处理复杂的计算任务。为了解决这一挑战，我们提出了一种动态的、注重推理质量与延迟的路由框架，该框架在移动设备上的轻量级模型和边缘服务器上的强大模型之间协调推理任务。我们的框架采用两种不同的成本模型：对于单轮查询，它将 BERT 预测的语义分数与通信和计算开销融合；对于多轮对话，它进一步量化从模型切换和 KV 缓存管理中产生的上下文感知成本。通过维护完整的推理质量，实验结果表明，相对于竞争对手基准（如 MMLU、GSM8K 和 MT-Bench-101），我们的框架将平均响应延迟降低 5-15%，并减少大规模模型调用 10-20%。', 'title_zh': '面向无线边缘设备网络中大语言模型推理的动态质量-延迟感知路由'}
{'arxiv_id': 'arXiv:2508.11287', 'title': 'CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems', 'authors': 'Xuran Liu, Nan Xue, Rui Bao, Yaping Sun, Zhiyong Chen, Meixia Tao, Xiaodong Xu, Shuguang Cui', 'link': 'https://arxiv.org/abs/2508.11287', 'abstract': 'While deploying large language models on edge devices promises low-latency and privacy-preserving AI services, it is hindered by limited device resources. Although pipeline parallelism facilitates distributed inference, existing approaches often ignore the cold-start latency caused by on-demand model loading. In this paper, we propose a latency-aware scheduling framework that overlaps model loading with computation and communication to minimize total inference latency. Based on device and model parameters, the framework dynamically adjusts layer partitioning and allocation to effectively hide loading time, thereby eliminating as many idle periods as possible. We formulate the problem as a Mixed-Integer Non-Linear Program and design an efficient dynamic programming algorithm to optimize model partitioning and device assignment. Experimental results show that the proposed method significantly reduces cold-start latency compared to baseline strategies.', 'abstract_zh': '基于延迟感知的模型加载与计算通信Overlap调度框架：提高边缘设备上大型语言模型推理的冷启动效率', 'title_zh': 'CSGO：无线协作边缘LLM系统中冷启动的通用优化方法'}
{'arxiv_id': 'arXiv:2508.11281', 'title': 'ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection', 'authors': 'Axel Delaval, Shujian Yang, Haicheng Wang, Han Qiu, Jialiang Lu', 'link': 'https://arxiv.org/abs/2508.11281', 'abstract': "Detecting toxic content using language models is crucial yet challenging. While substantial progress has been made in English, toxicity detection in French remains underdeveloped, primarily due to the lack of culturally relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new public benchmark of 53,622 French online comments, constructed via a semi-automated annotation pipeline that reduces manual labeling to only 10% through high-confidence LLM-based pre-annotation and human verification. Then, we benchmark a broad range of models and uncover a counterintuitive insight: Small Language Models (SLMs) outperform many larger models in robustness and generalization under the toxicity detection task. Motivated by this finding, we propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic weighted loss that progressively emphasizes the model's final decision, significantly improving faithfulness. Our fine-tuned 4B model achieves state-of-the-art performance, improving its F1 score by 13% over its baseline and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a cross-lingual toxicity benchmark demonstrates strong multilingual ability, suggesting that our methodology can be effectively extended to other languages and safety-critical classification tasks.", 'abstract_zh': '检测毒元素材的语言模型至关重要但极具挑战性 French toxicity detection, substantial progress has been made remains largely under-developed primarily due由于缺乏文化相关的大规模数据集 on this this basis on we we on we we introduce introduce introduce introduce introduce on introduce on on on we on on on on on on introducing To on on To To on on To To on To on on on To on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on entitled To on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on onENCH chain on on on on on on on on on on on.on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on Chain-of-Thought-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th on-Th', 'title_zh': 'ToxiFrench：通过CoT微调提升法语毒性检测的语言模型基准与增强'}
{'arxiv_id': 'arXiv:2508.11280', 'title': 'LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought', 'authors': 'Ruiyan Qi, Congding Wen, Weibo Zhou, Shangsong Liang, Lingbo Li', 'link': 'https://arxiv.org/abs/2508.11280', 'abstract': "Evaluating large language models (LLMs) in specific domain like tourism remains challenging due to the prohibitive cost of annotated benchmarks and persistent issues like hallucinations. We propose $\\textbf{L}$able-Free $\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert $\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that leverages expert-derived reasoning structures-instead of labeled data-to access LLMs in tourism. First, we iteratively refine and validate hierarchical ToT components through alignment with generic quality dimensions and expert feedback. Results demonstrate the effectiveness of our systematically optimized expert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we apply LETToT's optimized expert ToT to evaluate models of varying scales (32B-671B parameters), revealing: (1) Scaling laws persist in specialized domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g., DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit reasoning architectures outperform counterparts in accuracy and conciseness ($p<0.05$). Our work established a scalable, label-free paradigm for domain-specific LLM evaluation, offering a robust alternative to conventional annotated benchmarks.", 'abstract_zh': 'Lable-Free Evaluation of LLM on Tourism Using Expert Tree-of-Thought (LETToT)', 'title_zh': 'LETToT: 无需标签评估大型语言模型在旅游领域的专家思维树评价方法'}
{'arxiv_id': 'arXiv:2508.11257', 'title': 'Hallucination in LLM-Based Code Generation: An Automotive Case Study', 'authors': 'Marc Pavel, Nenad Petrovic, Lukasz Mazur, Vahid Zolfaghari, Fengjunjie Pan, Alois Knoll', 'link': 'https://arxiv.org/abs/2508.11257', 'abstract': 'Large Language Models (LLMs) have shown significant potential in automating code generation tasks offering new opportunities across software engineering domains. However, their practical application remains limited due to hallucinations - outputs that appear plausible but are factually incorrect, unverifiable or nonsensical. This paper investigates hallucination phenomena in the context of code generation with a specific focus on the automotive domain. A case study is presented that evaluates multiple code LLMs for three different prompting complexities ranging from a minimal one-liner prompt to a prompt with Covesa Vehicle Signal Specifications (VSS) as additional context and finally to a prompt with an additional code skeleton. The evaluation reveals a high frequency of syntax violations, invalid reference errors and API knowledge conflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the evaluated models, only GPT-4.1 and GPT-4o were able to produce a correct solution when given the most context-rich prompt. Simpler prompting strategies failed to yield a working result, even after multiple refinement iterations. These findings highlight the need for effective mitigation techniques to ensure the safe and reliable use of LLM generated code, especially in safety-critical domains such as automotive software systems.', 'abstract_zh': '大型语言模型（LLMs）在自动化代码生成任务方面展示了显著潜力，为软件工程领域带来了新的机遇。然而，由于其可能出现事实错误、无法验证或无意义的幻觉输出，其实际应用仍受到限制。本文在代码生成的背景下探讨幻觉现象，特别关注汽车领域。文中通过案例研究评估了多种代码LLM，针对三种不同复杂度的提示，从简单的单行提示到包含Covesa车辆信号规范（VSS）作为额外上下文的提示，再到包含附加代码框架的提示。评估结果显示，最先进模型GPT-4.1、Codex和GPT-4o中频繁出现语法规则违反、无效引用错误和API知识冲突。在最丰富的提示下，只有GPT-4.1和GPT-4o能够生成正确结果。简单提示策略即使经过多次优化迭代也无法产出可工作的结果。这些发现突显了在包括汽车软件系统在内的安全关键领域确保安全可靠使用LLM生成代码的有效缓解技术的必要性。', 'title_zh': '基于LLM的代码生成中的幻觉：一项汽车案例研究'}
{'arxiv_id': 'arXiv:2508.11222', 'title': 'ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal', 'authors': 'Haonan Zhang, Dongxia Wang, Yi Liu, Kexin Chen, Jiashui Wang, Xinlei Ying, Long Liu, Wenhai Wang', 'link': 'https://arxiv.org/abs/2508.11222', 'abstract': "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously rejecting benign queries due to overly conservative safety measures - a critical functional flaw that undermines their reliability and usability. Current methods for testing this behavior are demonstrably inadequate, suffering from flawed benchmarks and limited test generation capabilities, as highlighted by our empirical user study. To the best of our knowledge, this paper introduces the first evolutionary testing framework, ORFuzz, for the systematic detection and analysis of LLM over-refusals. ORFuzz uniquely integrates three core components: (1) safety category-aware seed selection for comprehensive test coverage, (2) adaptive mutator optimization using reasoning LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge model validated to accurately reflect user perception of toxicity and refusal. Our extensive evaluations demonstrate that ORFuzz generates diverse, validated over-refusal instances at a rate (6.98% average) more than double that of leading baselines, effectively uncovering vulnerabilities. Furthermore, ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly transferable test cases that achieves a superior 63.56% average over-refusal rate across 10 diverse LLMs, significantly outperforming existing datasets. ORFuzz and ORFuzzSet provide a robust automated testing framework and a valuable community resource, paving the way for developing more reliable and trustworthy LLM-based software systems.", 'abstract_zh': 'Large Language Models (LLMs) 增加了过度拒绝的现象——由于过度保守的安全措施错误地拒绝了一些 Innocuous 查询，这是一种关键的功能缺陷，削弱了它们的可靠性和实用性。现有的测试方法明显不足，受到有缺陷的基准和有限的测试生成能力的限制，这一点在我们的实证用户研究中得到了体现。据我们所知，本文首次介绍了一种进化测试框架 ORFuzz，用于系统地检测和分析 LLM 的过度拒绝现象。ORFuzz 独特地集成了三大核心组件：(1) 意识到安全类别种子选择以实现全面的测试覆盖率，(2) 通过使用推理 LLM 进行自适应突变优化以生成有效的测试案例，以及 (3) OR-Judge，一种经过验证并在毒性和拒绝方面反映出用户感知的人类对齐裁判模型。我们的广泛评估表明，ORFuzz 以 6.98% 的平均生成多样化的验证过度拒绝实例的速率，超过领先基线的两倍，有效地发现了漏洞。此外，ORFuzz 的输出构成了一个新的基准 ORFuzzSet，包含 1,855 个高可转移性的测试案例，这些案例在 10 种不同的 LLM 上实现了 63.56% 的平均过度拒绝率，显著优于现有数据集。ORFuzz 和 ORFuzzSet 为开发更可靠和值得信赖的基于 LLM 的软件系统提供了强大的自动测试框架和有价值的社区资源。', 'title_zh': 'ORFuzz: 测试过拒绝行为保障大语言模型安全的“另一面” fuzzing'}
{'arxiv_id': 'arXiv:2508.11158', 'title': 'Role-Augmented Intent-Driven Generative Search Engine Optimization', 'authors': 'Xiaolu Chen, Haojie Wu, Jie Bao, Zhen Chen, Yong Liao, Hu Huang', 'link': 'https://arxiv.org/abs/2508.11158', 'abstract': 'Generative Search Engines (GSEs), powered by Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), are reshaping information retrieval. While commercial systems (e.g., BingChat, this http URL) demonstrate impressive semantic synthesis capabilities, their black-box nature fundamentally undermines established Search Engine Optimization (SEO) practices. Content creators face a critical challenge: their optimization strategies, effective in traditional search engines, are misaligned with generative retrieval contexts, resulting in diminished visibility. To bridge this gap, we propose a Role-Augmented Intent-Driven Generative Search Engine Optimization (G-SEO) method, providing a structured optimization pathway tailored for GSE scenarios. Our method models search intent through reflective refinement across diverse informational roles, enabling targeted content enhancement. To better evaluate the method under realistic settings, we address the benchmarking limitations of prior work by: (1) extending the GEO dataset with diversified query variations reflecting real-world search scenarios and (2) introducing G-Eval 2.0, a 6-level LLM-augmented evaluation rubric for fine-grained human-aligned assessment. Experimental results demonstrate that search intent serves as an effective signal for guiding content optimization, yielding significant improvements over single-aspect baseline approaches in both subjective impressions and objective content visibility within GSE responses.', 'abstract_zh': '基于大型语言模型和检索增强生成的生成性搜索引擎（GSEs）正在重塑信息检索。虽然商业系统（例如BingChat）展示了令人印象深刻的语义合成能力，但其黑箱性质从根本上削弱了传统搜索引擎优化（SEO）惯例。内容创作者面临一项关键挑战：他们在传统搜索引擎中的优化策略与生成检索情境不匹配，导致其内容可见度降低。为弥补这一差距，我们提出了一种角色增强意图驱动的生成性搜索引擎优化（G-SEO）方法，为生成性搜索场景提供了一种结构化的优化途径。该方法通过跨多种信息角色的反思性细化来建模搜索意图，从而实现目标内容增强。为更好地在现实环境中评估该方法，我们通过以下两种方式弥补了先前工作的基准测试限制：（1）扩展GEO数据集，利用多样化的真实查询变化反映实际搜索场景；（2）引入G-Eval 2.0，一种六级增强的人工智能评估标准，用于细粒度的人机一致性评估。实验结果表明，搜索意图作为指导内容优化的有效信号，相较于单一维度基准方法，在生成性搜索引擎响应中的主观感受和客观内容可见性均取得了显著改进。', 'title_zh': '角色增强意图驱动生成型搜索引擎优化'}
{'arxiv_id': 'arXiv:2508.11152', 'title': 'AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions', 'authors': 'Tianjiao Zhao, Jingrao Lyu, Stokes Jones, Harrison Garber, Stefano Pasquali, Dhagash Mehta', 'link': 'https://arxiv.org/abs/2508.11152', 'abstract': 'The field of artificial intelligence (AI) agents is evolving rapidly, driven by the capabilities of Large Language Models (LLMs) to autonomously perform and refine tasks with human-like efficiency and adaptability. In this context, multi-agent collaboration has emerged as a promising approach, enabling multiple AI agents to work together to solve complex challenges. This study investigates the application of role-based multi-agent systems to support stock selection in equity research and portfolio management. We present a comprehensive analysis performed by a team of specialized agents and evaluate their stock-picking performance against established benchmarks under varying levels of risk tolerance. Furthermore, we examine the advantages and limitations of employing multi-agent frameworks in equity analysis, offering critical insights into their practical efficacy and implementation challenges.', 'abstract_zh': '基于大型语言模型的AI代理领域正在快速发展，多代理协作作为一项有前途的方法逐渐兴起，使多个AI代理能够合作解决复杂挑战。本研究探讨了基于角色的多代理系统在股权研究和投资组合管理中支持股票筛选的应用。我们呈现了一支专业代理团队进行的全面分析，并评估了他们在不同风险容忍度水平下的股票选取表现。此外，我们还考察了在股权分析中采用多代理框架的优势和局限性，提供了对其实际效用和实施挑战的宝贵见解。', 'title_zh': 'AlphaAgents：基于大型语言模型的多智能体股票组合构建'}
{'arxiv_id': 'arXiv:2508.11133', 'title': 'MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents', 'authors': 'Tomer Wolfson, Harsh Trivedi, Mor Geva, Yoav Goldberg, Dan Roth, Tushar Khot, Ashish Sabharwal, Reut Tsarfaty', 'link': 'https://arxiv.org/abs/2508.11133', 'abstract': 'Large language models (LLMs) are emerging as a go-to tool for querying information. However, current LLM benchmarks rarely feature natural questions that are both information-seeking as well as genuinely time-consuming for humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural and complex questions that require dozens, and at times hundreds, of intermediate steps to solve -- far more than any existing QA benchmark. To build MoNaCo, we developed a decomposed annotation pipeline to elicit and manually answer natural time-consuming questions at scale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and hallucinations. Our results underscore the need for reasoning models that better handle the complexity and sheer breadth of real-world information-seeking questions -- with MoNaCo providing an effective resource for tracking such progress. The MONACO benchmark, codebase, prompts and models predictions are publicly available at: this https URL', 'abstract_zh': '大型语言模型（LLMs）正逐渐成为查询信息的首选工具。然而，当前的LLM基准数据集很少包含既寻求信息又对人类来说真正耗时的自然问题。为解决这一问题，我们引入了MoNaCo基准，该基准包含1,315个自然且复杂的需要几十步乃至上百步中间步骤才能解决的问题——远超现有任何问答基准的数据复杂度。为构建MoNaCo，我们开发了一个分解注释流水线，以大规模生成和手动回答自然耗时的问题。在MoNaCo上进行评估的前沿LLMs最多只实现了61.2%的F1分数，受到召回率低和幻觉的影响。我们的结果强调了处理真实世界信息寻求问题的复杂性和广泛性所需的推理模型的需求——MoNaCo提供了跟踪这种进展的有效资源。MoNaCo基准、代码库、提示词和模型预测已在以下网址公开：this https URL。', 'title_zh': 'MoNaCo: 更自然且复杂的跨文档推理问题生成'}
{'arxiv_id': 'arXiv:2508.11017', 'title': 'Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics', 'authors': 'Carter Blum, Katja Filipova, Ann Yuan, Asma Ghandeharioun, Julian Zimmert, Fred Zhang, Jessica Hoffmann, Tal Linzen, Martin Wattenberg, Lucas Dixon, Mor Geva', 'link': 'https://arxiv.org/abs/2508.11017', 'abstract': 'Large language models (LLMs) struggle with cross-lingual knowledge transfer: they hallucinate when asked in one language about facts expressed in a different language during training. This work introduces a controlled setting to study the causes and dynamics of this phenomenon by training small Transformer models from scratch on synthetic multilingual datasets. We identify a learning phase wherein a model develops either separate or unified representations of the same facts across languages, and show that unification is essential for cross-lingual transfer. We also show that the degree of unification depends on mutual information between facts and training data language, and on how easy it is to extract that language. Based on these insights, we develop methods to modulate the level of cross-lingual transfer by manipulating data distribution and tokenization, and we introduce metrics and visualizations to formally characterize their effects on unification. Our work shows how controlled settings can shed light on pre-training dynamics and suggests new directions for improving cross-lingual transfer in LLMs.', 'abstract_zh': '大规模语言模型在跨语言知识迁移中表现挣扎：它们在用一种语言询问不同语言中表述的事实时会产生错觉。本研究引入了一个受控环境，通过从零训练小的Transformer模型来研究这一现象的原因和动态。我们识别了一个学习阶段，在此阶段，模型会发展出针对同一事实的独立或统一的语言表示，并表明统一是跨语言迁移的关键。我们还表明，统一的程度取决于事实与训练数据语言之间的互信息，以及提取该语言的难易程度。基于这些洞见，我们开发了方法来通过调整数据分布和分词来调控跨语言迁移的程度，并引入了度量和可视化方法以正式表征这些对统一的影响。我们的研究显示了如何通过受控环境揭示预训练动态，并指出改进大规模语言模型跨语言迁移的新方向。', 'title_zh': '超越罗塞塔石碑：统一力在泛化动态中的作用'}
{'arxiv_id': 'arXiv:2508.11016', 'title': 'CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention', 'authors': 'Qingbin Li, Rongkun Xue, Jie Wang, Ming Zhou, Zhi Li, Xiaofeng Ji, Yongqi Wang, Miao Liu, Zheming Yang, Minghui Qiu, Jing Yang', 'link': 'https://arxiv.org/abs/2508.11016', 'abstract': 'Recent advances in Reinforcement Learning with Verified Reward (RLVR) have driven the emergence of more sophisticated cognitive behaviors in large language models (LLMs), thereby enhancing their reasoning capabilities. However, in prior RLVR pipelines, the repeated use of static initial-state sampling drawn exactly from the dataset distribution during each sampling phase produced overly deterministic, low diversity model behavior, which manifested as rapid entropy collapse and hindered sustained performance gains during prolonged training. To address this issue, we introduce CURE (Critical-token-gUided Re concatenation for Entropy-collapse prevention), a two-stage framework that balances exploration and exploitation. Specifically, in the first stage, to deliberately steer the model toward novel yet coherent contexts, we re-generate at high-entropy critical tokens and jointly optimize the original and the branched trajectories. The further comparison with vanilla DAPO shows that the regeneration process achieves a better performance on math reasoning tasks while sustaining a high-level entropy degree for exploration. In the second stage, we continue training with static initial-state sampling by DAPO, intentionally placing the model in a familiar state to gradually strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that, compared to other RLVR methods, CURE achieves a 5% performance gain across six math benchmarks, establishing state-of-the-art performance in both entropy and accuracy. A series of experiments further validate the effectiveness of our approach. Code is available at this https URL.', 'abstract_zh': '最近在验证奖励强化学习（RLVR）方面的进展推动了大型语言模型（LLMs）更复杂认知行为的出现，从而增强了其推理能力。然而，在先前的RLVR管道中，每次采样阶段都反复使用精确来自数据集分布的静态初始状态采样，导致了过度确定性和低多样性模型行为，表现为快速熵坍缩，并阻碍了长时间训练期间的持续性能提升。为解决这一问题，我们引入了CURE（关键令牌引导的重新组合以防止熵坍缩），这是一种平衡探索与利用的两阶段框架。具体而言，在第一阶段，为了引导模型朝新颖但连贯的上下文前进，我们重新生成高熵的关键令牌，并联合优化原始轨迹和分支轨迹。与vanilla DAPO的进一步对比显示，重新生成过程在数学推理任务上取得了更好的性能，同时保持了高水平的熵度以促进探索。在第二阶段，我们继续使用DAPO的静态初始状态采样进行训练，有意将模型置于熟悉的狀態，逐步强化利用。在Qwen-2.5-Math-7B上进行的广泛实验表明，与其它RLVR方法相比，CURE在六个数学基准上实现了5%的性能提升，同时在熵和准确性上均达到了最先进的性能。一系列实验进一步验证了我们方法的有效性。代码可在以下链接获取。', 'title_zh': 'CURE: 关键词引导的重新拼接以防止熵坍缩'}
{'arxiv_id': 'arXiv:2508.11009', 'title': 'SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth', 'authors': 'Wenpeng Xing, Lanyi Wei, Haixiao Hu, Rongchang Li, Mohan Li, Changting Lin, Meng Han', 'link': 'https://arxiv.org/abs/2508.11009', 'abstract': 'The rapid proliferation of large language models (LLMs) in applications targeting children and adolescents necessitates a fundamental reassessment of prevailing AI safety frameworks, which are largely tailored to adult users and neglect the distinct developmental vulnerabilities of minors. This paper highlights key deficiencies in existing LLM safety benchmarks, including their inadequate coverage of age-specific cognitive, emotional, and social risks spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence (13--18). To bridge these gaps, we introduce SproutBench, an innovative evaluation suite comprising 1,283 developmentally grounded adversarial prompts designed to probe risks such as emotional dependency, privacy violations, and imitation of hazardous behaviors. Through rigorous empirical evaluation of 47 diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by robust inter-dimensional correlations (e.g., between Safety and Risk Prevention) and a notable inverse relationship between Interactivity and Age Appropriateness. These insights yield practical guidelines for advancing child-centric AI design and deployment.', 'abstract_zh': '面向儿童和青少年的应用激增促使我们对现有的AI安全框架进行根本性的重新评估，这些框架主要针对成人用户，忽视了未成年人的独特发育脆弱性。本文指出现有大语言模型安全基准的关键缺陷，包括其对早期童年（0-6岁）、学龄期（7-12岁）和青春期（13-18岁）年龄特异性认知、情感和社会风险的不足覆盖。为弥补这些差距，我们提出了SproutBench，这是一个创新的评估套件，包括1,283个基于发育阶段的对抗性提示，旨在探究情感依赖、隐私侵犯和模仿危险行为等风险。通过47种不同LLM的严格实证评估，我们发现了一系列重大的安全漏洞，这些漏洞通过跨维度相关性（例如，安全性和风险预防之间）得到了验证，并且交互性和年龄适宜性之间存在值得注意的负相关关系。这些洞察提供了面向儿童的AI设计和部署的实际指导。', 'title_zh': 'SproutBench: 青少年安全且伦理合规的大语言模型基准'}
{'arxiv_id': 'arXiv:2508.10991', 'title': 'MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications', 'authors': 'Wenpeng Xing, Zhonghao Qi, Yupeng Qin, Yilin Li, Caini Chang, Jiahui Yu, Changting Lin, Zhenzhen Xie, Meng Han', 'link': 'https://arxiv.org/abs/2508.10991', 'abstract': 'The integration of Large Language Models (LLMs) with external tools via protocols such as the Model Context Protocol (MCP) introduces critical security vulnerabilities, including prompt injection, data exfiltration, and other threats. To counter these challenges, we propose MCP-Guard, a robust, layered defense architecture designed for LLM--tool interactions. MCP-Guard employs a three-stage detection pipeline that balances efficiency with accuracy: it progresses from lightweight static scanning for overt threats and a deep neural detector for semantic attacks, to our fine-tuned E5-based model achieves (96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM arbitrator synthesizes these signals to deliver the final decision while minimizing false positives. To facilitate rigorous training and evaluation, we also introduce MCP-AttackBench, a comprehensive benchmark of over 70,000 samples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench simulates diverse, real-world attack vectors in the MCP format, providing a foundation for future research into securing LLM-tool ecosystems.', 'abstract_zh': 'LLMs与外部工具通过协议（如MRectified Context Protocol）集成的关键安全性挑战及CP-MCPuard：一种针对LLMs工具交互的稳健分分层次化防御架构', 'title_zh': 'MCP-Guard: 大型语言模型应用中模型上下文协议完整性的防御框架'}
{'arxiv_id': 'arXiv:2508.10971', 'title': 'Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules', 'authors': 'Nasim Shirvani-Mahdavi, Chengkai Li', 'link': 'https://arxiv.org/abs/2508.10971', 'abstract': "Knowledge graphs (KGs) can be enhanced through rule mining; however, the resulting logical rules are often difficult for humans to interpret due to their inherent complexity and the idiosyncratic labeling conventions of individual KGs. This work presents Rule2Text, a comprehensive framework that leverages large language models (LLMs) to generate natural language explanations for mined logical rules, thereby improving KG accessibility and usability. We conduct extensive experiments using multiple datasets, including Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically evaluate several LLMs across a comprehensive range of prompting strategies, including zero-shot, few-shot, variable type incorporation, and Chain-of-Thought reasoning. To systematically assess models' performance, we conduct a human evaluation of generated explanations on correctness and clarity. To address evaluation scalability, we develop and validate an LLM-as-a-judge framework that demonstrates strong agreement with human evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge, and human-in-the-loop feedback, we construct high-quality ground truth datasets, which we use to fine-tune the open-source Zephyr model. Our results demonstrate significant improvements in explanation quality after fine-tuning, with particularly strong gains in the domain-specific dataset. Additionally, we integrate a type inference module to support KGs lacking explicit type information. All code and data are publicly available at this https URL.", 'abstract_zh': '知识图谱中的规则可以通过规则挖掘进行增强，然而，生成的逻辑规则往往由于其固有的复杂性和个体知识图谱的特殊标记惯例而难以供人类解读。本文提出了一个综合框架Rule2Text，该框架利用大规模语言模型（LLMs）自动生成挖掘出的逻辑规则的自然语言解释，从而提高知识图谱的可访问性和可用性。我们使用多种数据集进行了广泛的实验，包括Freebase变体（FB-CVT-REV、FB+CVT-REV和FB15k-237）以及ogbl-biokg数据集，使用AMIE 3.5.1进行规则挖掘。我们系统地评估了多种大规模语言模型在广泛策略下的表现，包括零样本、少样本、类型信息变体整合和chain-of-thought推理。为系统性评估模型性能，我们进行了生成解释的正确性和清晰度的人类评估。为解决评估量化的挑战，我们开发并验证了一个大规模语言模型作为评委的框架，该框架与人类评估者表现出强烈的一致性。利用性能最佳的模型（Gemini 2.0 Flash）、LLM评委和人工反馈，我们构建了高质量的基准数据集，用于微调开源Zephyr模型。我们的结果显示，在微调后解释质量有显著提升，特别是在特定领域数据集中的提升尤为明显。此外，我们整合了一个类型推断模块以支持缺乏显式类型信息的知识图谱。所有代码和数据均在此网址公开。', 'title_zh': 'Rule2Text：知识图规则自然语言解释生成与评估框架'}
{'arxiv_id': 'arXiv:2508.10948', 'title': 'Apriel-Nemotron-15B-Thinker', 'authors': 'Shruthan Radhakrishna, Soham Parikh, Gopal Sarda, Anil Turkkan, Quaizar Vohra, Raymond Li, Dhruv Jhamb, Kelechi Ogueji, Aanjaneya Shukla, Oluwanifemi Bamgbose, Toby Liang, Luke Kumar, Oleksiy Ostapenko, Shiva Krishna Reddy Malay, Aman Tiwari, Tara Bogavelli, Vikas Yadav, Jash Mehta, Saloni Mittal, Akshay Kalkunte, Pulkit Pattnaik, Khalil Slimi, Anirudh Sreeram, Jishnu Nair, Akintunde Oladipo, Shashank Maiya, Khyati Mahajan, Rishabh Maheshwary, Masoud Hashemi, Sai Rajeswar Mudumba, Sathwik Tejaswi Madhusudhan, Torsten Scholak, Sebastien Paquet, Sagar Davasam, Srinivas Sunkara', 'link': 'https://arxiv.org/abs/2508.10948', 'abstract': 'While large language models (LLMs) have achieved remarkable reasoning capabilities across domains like code, math and other enterprise tasks, their significant memory and computational costs often preclude their use in practical enterprise settings. To this end, we introduce Apriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow Apriel SLM series that achieves performance against medium sized state-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while maintaining only half the memory footprint of those alternatives. Apriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline including 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised Fine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive evaluations across a diverse suite of benchmarks consistently demonstrate that our Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its 32-billion parameter counterparts, despite being less than half their size.', 'abstract_zh': '虽然大型语言模型（LLMs）在代码、数学和其他企业任务等领域实现了卓越的推理能力，但其显著的内存和计算成本往往使其难以在实际企业环境中使用。为解决这一问题，我们引入了ServiceNow Apriel SLM系列中的Apriel-Nemotron-15B-Thinker模型，该模型拥有150亿参数，在基准测试中表现出色，与320亿参数的竞争模型o1-mini、QWQ32B和EXAONE-Deep-32B相比，其内存占用仅为后者的一半。Apriel-Nemotron-15B-Thinker模型的训练管道包括四阶段流程：1）基础模型扩展、2）持续预训练、3）监督微调（SFT）以及4）基于GRPO的强化学习。跨多种基准测试的全面评估一致表明，尽管Apriel-Nemotron-15B-Thinker模型的规模仅为后者的一半，但其性能却与之相当或更优。', 'title_zh': 'April-Nemotron-15B-Thinker'}
{'arxiv_id': 'arXiv:2508.10925', 'title': 'gpt-oss-120b & gpt-oss-20b Model Card', 'authors': 'OpenAI, Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul K. Arora, Yu Bai, Bowen Baker, Haiming Bao, Boaz Barak, Ally Bennett, Tyler Bertao, Nivedita Brett, Eugene Brevdo, Greg Brockman, Sebastien Bubeck, Che Chang, Kai Chen, Mark Chen, Enoch Cheung, Aidan Clark, Dan Cook, Marat Dukhan, Casey Dvorak, Kevin Fives, Vlad Fomenko, Timur Garipov, Kristian Georgiev, Mia Glaese, Tarun Gogineni, Adam Goucher, Lukas Gross, Katia Gil Guzman, John Hallman, Jackie Hehir, Johannes Heidecke, Alec Helyar, Haitang Hu, Romain Huet, Jacob Huh, Saachi Jain, Zach Johnson, Chris Koch, Irina Kofman, Dominik Kundel, Jason Kwon, Volodymyr Kyrylov, Elaine Ya Le, Guillaume Leclerc, James Park Lennon, Scott Lessans, Mario Lezcano-Casado, Yuanzhi Li, Zhuohan Li, Ji Lin, Jordan Liss, Lily, Jiancheng Liu, Kevin Lu, Chris Lu, Zoran Martinovic, Lindsay McCallum, Josh McGrath, Scott McKinney, Aidan McLaughlin, Song Mei, Steve Mostovoy, Tong Mu, Gideon Myles, Alexander Neitz, Alex Nichol, Jakub Pachocki, Alex Paino, Dana Palmie, Ashley Pantuliano, Giambattista Parascandolo, Jongsoo Park, Leher Pathak, Carolina Paz, Ludovic Peran, Dmitry Pimenov, Michelle Pokrass, Elizabeth Proehl, Huida Qiu, Gaby Raila, Filippo Raso, Hongyu Ren, Kimmy Richardson, David Robinson, Bob Rotsted, Hadi Salman, Suvansh Sanjeev, Max Schwarzer, D. Sculley, Harshit Sikchi, Kendal Simon, Karan Singhal, Yang Song', 'link': 'https://arxiv.org/abs/2508.10925', 'abstract': 'We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models that push the frontier of accuracy and inference cost. The models use an efficient mixture-of-expert transformer architecture and are trained using large-scale distillation and reinforcement learning. We optimize the models to have strong agentic capabilities (deep research browsing, python tool use, and support for developer-provided functions), all while using a rendered chat format that enables clear instruction following and role delineation. Both models achieve strong results on benchmarks ranging from mathematics, coding, and safety. We release the model weights, inference implementations, tool environments, and tokenizers under an Apache 2.0 license to enable broad use and further research.', 'abstract_zh': '我们展示了两个开放重量推理模型GPT-OSS-120B和GPT-OSS-20B，它们推动了准确性和推理成本的边界。这些模型采用高效的专家混合变压器架构，并通过大规模的知识蒸馏和强化学习进行训练。我们优化模型以具备强大的代理能力（深度研究浏览、Python工具使用及开发者提供的功能支持），同时使用渲染的对话格式，以实现清晰的指令跟随和角色划分。这两种模型在涵盖数学、编程和安全性等多个基准测试中均取得了优异成果。我们以Apache 2.0许可证发布模型权重、推理实现、工具环境和分词器，以促进广泛使用和进一步研究。', 'title_zh': 'GPT-oss-120B & GPT-oss-20B 模型卡片'}
{'arxiv_id': 'arXiv:2508.10919', 'title': 'Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?', 'authors': 'Mohammed Saqr, Kamila Misiejuk, Sonsoles López-Pernas', 'link': 'https://arxiv.org/abs/2508.10919', 'abstract': 'While research on human-AI collaboration exists, it mainly examined language learning and used traditional counting methods with little attention to evolution and dynamics of collaboration on cognitively demanding tasks. This study examines human-AI interactions while solving a complex problem. Student-AI interactions were qualitatively coded and analyzed with transition network analysis, sequence analysis and partial correlation networks as well as comparison of frequencies using chi-square and Person-residual shaded Mosaic plots to map interaction patterns, their evolution, and their relationship to problem complexity and student performance. Findings reveal a dominant Instructive pattern with interactions characterized by iterative ordering rather than collaborative negotiation. Oftentimes, students engaged in long threads that showed misalignment between their prompts and AI output that exemplified a lack of synergy that challenges the prevailing assumptions about LLMs as collaborative partners. We also found no significant correlations between assignment complexity, prompt length, and student grades suggesting a lack of cognitive depth, or effect of problem difficulty. Our study indicates that the current LLMs, optimized for instruction-following rather than cognitive partnership, compound their capability to act as cognitively stimulating or aligned collaborators. Implications for designing AI systems that prioritize cognitive alignment and collaboration are discussed.', 'abstract_zh': '人类与AI协作研究：复杂问题解决中的交互模式及其演变', 'title_zh': '人类与人工智能的合作，还是在指令、服务、重复动态中盲目服从的人工智能？'}
