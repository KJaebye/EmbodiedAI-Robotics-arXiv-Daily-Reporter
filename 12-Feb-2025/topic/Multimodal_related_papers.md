# SensPS: Sensing Personal Space Comfortable Distance between Human-Human Using Multimodal Sensors 

**Title (ZH)**: SensPS：基于多模态传感器的人与人之间的舒适间距感知 

**Authors**: Ko Watanabe, Nico Förster, Shoya Ishimaru  

**Link**: [PDF](https://arxiv.org/pdf/2502.07441)  

**Abstract**: Personal space, also known as peripersonal space, is crucial in human social interaction, influencing comfort, communication, and social stress. Estimating and respecting personal space is essential for enhancing human-computer interaction (HCI) and smart environments. Personal space preferences vary due to individual traits, cultural background, and contextual factors. Advanced multimodal sensing technologies, including eye-tracking and wristband sensors, offer opportunities to develop adaptive systems that dynamically adjust to user comfort levels. Integrating physiological and behavioral data enables a deeper understanding of spatial interactions. This study develops a sensor-based model to estimate comfortable personal space and identifies key features influencing spatial preferences. Our findings show that multimodal sensors, particularly eye-tracking and physiological wristband data, can effectively predict personal space preferences, with eye-tracking data playing a more significant role. An experimental study involving controlled human interactions demonstrates that a Transformer-based model achieves the highest predictive accuracy (F1 score: 0.87) for estimating personal space. Eye-tracking features, such as gaze point and pupil diameter, emerge as the most significant predictors, while physiological signals from wristband sensors contribute marginally. These results highlight the potential for AI-driven personalization of social space in adaptive environments, suggesting that multimodal sensing can be leveraged to develop intelligent systems that optimize spatial arrangements in workplaces, educational institutions, and public settings. Future work should explore larger datasets, real-world applications, and additional physiological markers to enhance model robustness. 

**Abstract (ZH)**: 基于传感器的个人空间估算及关键影响因素分析 

---
# Enhancing Higher Education with Generative AI: A Multimodal Approach for Personalised Learning 

**Title (ZH)**: 增强高等教育：一种多模态个性化学习的方法基于生成型人工智能 

**Authors**: Johnny Chan, Yuming Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.07401)  

**Abstract**: This research explores the opportunities of Generative AI (GenAI) in the realm of higher education through the design and development of a multimodal chatbot for an undergraduate course. Leveraging the ChatGPT API for nuanced text-based interactions and Google Bard for advanced image analysis and diagram-to-code conversions, we showcase the potential of GenAI in addressing a broad spectrum of educational queries. Additionally, the chatbot presents a file-based analyser designed for educators, offering deep insights into student feedback via sentiment and emotion analysis, and summarising course evaluations with key metrics. These combinations highlight the crucial role of multimodal conversational AI in enhancing teaching and learning processes, promising significant advancements in educational adaptability, engagement, and feedback analysis. By demonstrating a practical web application, this research underlines the imperative for integrating GenAI technologies to foster more dynamic and responsive educational environments, ultimately contributing to improved educational outcomes and pedagogical strategies. 

**Abstract (ZH)**: 这项研究通过设计和开发一个针对本科课程的多模态聊天机器人，探索生成式人工智能（GenAI）在高等教育领域的机会。利用ChatGPT API进行细腻的文字交互，以及利用Google Bard进行高级图像分析和图表到代码的转换，我们展示了GenAI在解决广泛教育查询方面的潜力。此外，聊天机器人还提供了一个基于文件的分析器，为教育者提供学生反馈的深入见解，通过情感和情绪分析，并以关键指标总结课程评价。这些组合突显了多模态对话AI在增强教学和学习过程中的关键作用，有望在教育适应性、参与度和反馈分析方面取得重大进展。通过展示一个实际的网络应用程序，这项研究强调了集成GenAI技术以培养更具动态性和响应性的教育环境的紧迫性，最终有助于提高教育成果和教学策略。 

---
# Generative Distribution Prediction: A Unified Approach to Multimodal Learning 

**Title (ZH)**: 生成分布预测：多模态学习的统一方法 

**Authors**: Xinyu Tian, Xiaotong Shen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07090)  

**Abstract**: Accurate prediction with multimodal data-encompassing tabular, textual, and visual inputs or outputs-is fundamental to advancing analytics in diverse application domains. Traditional approaches often struggle to integrate heterogeneous data types while maintaining high predictive accuracy. We introduce Generative Distribution Prediction (GDP), a novel framework that leverages multimodal synthetic data generation-such as conditional diffusion models-to enhance predictive performance across structured and unstructured modalities. GDP is model-agnostic, compatible with any high-fidelity generative model, and supports transfer learning for domain adaptation. We establish a rigorous theoretical foundation for GDP, providing statistical guarantees on its predictive accuracy when using diffusion models as the generative backbone. By estimating the data-generating distribution and adapting to various loss functions for risk minimization, GDP enables accurate point predictions across multimodal settings. We empirically validate GDP on four supervised learning tasks-tabular data prediction, question answering, image captioning, and adaptive quantile regression-demonstrating its versatility and effectiveness across diverse domains. 

**Abstract (ZH)**: 多模态数据（包含表格、文本和视觉输入或输出）的准确预测是推动不同应用领域数据分析发展的基础。传统方法往往难以在保持高预测精度的同时整合异构数据类型。我们提出了生成分布预测（GDP），这是一种利用条件扩散模型等多模态合成数据生成方法以提高结构化和非结构化模态预测性能的新框架。GDP 是模型无关的，可以与任何高保真生成模型兼容，并支持领域适应的迁移学习。我们为 GDP 建立了坚实理论基础，在使用扩散模型作为生成基础时提供了其预测精度的统计保证。通过估计数据生成分布并适应各种损失函数以实现风险最小化，GDP 使在多模态环境中能够进行准确的点预测。我们通过四个监督学习任务（表格数据预测、问答、图像字幕和自适应分位数回归）的经验验证，展示了其在不同领域的多样性和有效性。 

---
# Synthetic Audio Helps for Cognitive State Tasks 

**Title (ZH)**: 合成音频有助于认知状态任务 

**Authors**: Adil Soubki, John Murzaku, Peter Zeng, Owen Rambow  

**Link**: [PDF](https://arxiv.org/pdf/2502.06922)  

**Abstract**: The NLP community has broadly focused on text-only approaches of cognitive state tasks, but audio can provide vital missing cues through prosody. We posit that text-to-speech models learn to track aspects of cognitive state in order to produce naturalistic audio, and that the signal audio models implicitly identify is orthogonal to the information that language models exploit. We present Synthetic Audio Data fine-tuning (SAD), a framework where we show that 7 tasks related to cognitive state modeling benefit from multimodal training on both text and zero-shot synthetic audio data from an off-the-shelf TTS system. We show an improvement over the text-only modality when adding synthetic audio data to text-only corpora. Furthermore, on tasks and corpora that do contain gold audio, we show our SAD framework achieves competitive performance with text and synthetic audio compared to text and gold audio. 

**Abstract (ZH)**: 自然语言处理社区广泛聚焦于认知状态任务的文本方法，但声音可以通过语调提供重要的缺失提示。我们提出，文本到语音模型为了生成自然声音而学会了追踪认知状态的某些方面，并且音频模型隐含识别的信号与语言模型利用的信息是正交的。我们呈现了一种合成音频数据微调（SAD）框架，表明7个与认知状态建模相关的任务通过在文本和现成TTS系统的零样本合成音频数据的多模态训练中受益。我们展示了在仅文本语料库中加入合成音频数据时性能的提升。此外，在包含真实音频的任务和语料库中，我们的SAD框架在文本和合成音频与文本和真实音频的对比中实现了具有竞争力的性能。 

---
# Multimodal Cognitive Reframing Therapy via Multi-hop Psychotherapeutic Reasoning 

**Title (ZH)**: 多模态认知重框疗法：多跳心理治疗推理 

**Authors**: Subin Kim, Hoonrae Kim, Heejin Do, Gary Geunbae Lee  

**Link**: [PDF](https://arxiv.org/pdf/2502.06873)  

**Abstract**: Previous research has revealed the potential of large language models (LLMs) to support cognitive reframing therapy; however, their focus was primarily on text-based methods, often overlooking the importance of non-verbal evidence crucial in real-life therapy. To alleviate this gap, we extend the textual cognitive reframing to multimodality, incorporating visual clues. Specifically, we present a new dataset called Multi Modal-Cognitive Support Conversation (M2CoSC), which pairs each GPT-4-generated dialogue with an image that reflects the virtual client's facial expressions. To better mirror real psychotherapy, where facial expressions lead to interpreting implicit emotional evidence, we propose a multi-hop psychotherapeutic reasoning approach that explicitly identifies and incorporates subtle evidence. Our comprehensive experiments with both LLMs and vision-language models (VLMs) demonstrate that the VLMs' performance as psychotherapists is significantly improved with the M2CoSC dataset. Furthermore, the multi-hop psychotherapeutic reasoning method enables VLMs to provide more thoughtful and empathetic suggestions, outperforming standard prompting methods. 

**Abstract (ZH)**: Previous research has revealed the potential of large language models (LLMs) to support cognitive reframing therapy; however, their focus was primarily on text-based methods, often overlooking the importance of non-verbal evidence crucial in real-life therapy. To alleviate this gap, we extend the textual cognitive reframing to multimodality, incorporating visual clues. Specifically, we present a new dataset called Multi Modal-Cognitive Support Conversation (M2CoSC), which pairs each GPT-4-generated dialogue with an image that reflects the virtual client's facial expressions. To better mirror real psychotherapy, where facial expressions lead to interpreting implicit emotional evidence, we propose a multi-hop psychotherapeutic reasoning approach that explicitly identifies and incorporates subtle evidence. Our comprehensive experiments with both LLMs and vision-language models (VLMs) demonstrate that the VLMs' performance as psychotherapists is significantly improved with the M2CoSC dataset. Furthermore, the multi-hop psychotherapeutic reasoning method enables VLMs to provide more thoughtful and empathetic suggestions, outperforming standard prompting methods.

翻译标题为：

前期研究揭示了大语言模型（LLMs）在支持认知重塑疗法方面的潜力；然而，它们主要关注基于文本的方法，往往会忽略现实生活中疗法中至关重要的非言语证据的重要性。为缓解这一差距，我们扩展了基于文本的认知重塑方法到多模态，结合了视觉线索。具体地，我们提出了一种新的数据集，称为多模态认知支持对话（M2CoSC），该数据集将每个GPT-4生成的对话与反映虚拟客户面部表情的图像配对。为了更好地模拟实际心理治疗，其中面部表情导致对隐含情感证据的解释，我们提出了一种多跳心理治疗推理方法，明确地识别和整合细微的证据。我们对大语言模型（LLMs）和视图-语言模型（VLMs）的全面实验表明，使用M2CoSC数据集可以显著提高VLMs作为心理治疗师的性能。此外，多跳心理治疗推理方法使VLMs能够提供更具思考性和同情心的建议，优于标准提示方法。 

---
# Survey on Vision-Language-Action Models 

**Title (ZH)**: 视觉-语言-行动模型综述 

**Authors**: Adilzhan Adilkhanov, Amir Yelenov, Assylkhan Seitzhanov, Ayan Mazhitov, Azamat Abdikarimov, Danissa Sandykbayeva, Daryn Kenzhebek, Daulet Baimukashev, Dinmukhammed Mukashev, Ilyas Umurbekov, Jabrail Chumakov, Kamila Spanova, Karina Burunchina, Rasul Yermagambet, Rustam Chibar, Saltanat Seitzhan, Soibkhon Khajikhanov, Tasbolat Taunyazov, Temirlan Galimzhanov, Temirlan Kaiyrbay, Tleukhan Mussin, Togzhan Syrymova, Valeriya Kostyukova, Yermakhan Kassym, Madina Yergibay, Margulan Issa, Moldir Zabirova, Nurdaulet Zhuzbay, Nurlan Kabdyshev, Nurlan Zhaniyar, Yerkebulan Massalim, Zerde Nurbayeva, Zhanat Kappassov  

**Link**: [PDF](https://arxiv.org/pdf/2502.06851)  

**Abstract**: This paper presents an AI-generated review of Vision-Language-Action (VLA) models, summarizing key methodologies, findings, and future directions. The content is produced using large language models (LLMs) and is intended only for demonstration purposes. This work does not represent original research, but highlights how AI can help automate literature reviews. As AI-generated content becomes more prevalent, ensuring accuracy, reliability, and proper synthesis remains a challenge. Future research will focus on developing a structured framework for AI-assisted literature reviews, exploring techniques to enhance citation accuracy, source credibility, and contextual understanding. By examining the potential and limitations of LLM in academic writing, this study aims to contribute to the broader discussion of integrating AI into research workflows. This work serves as a preliminary step toward establishing systematic approaches for leveraging AI in literature review generation, making academic knowledge synthesis more efficient and scalable. 

**Abstract (ZH)**: 这篇论文呈现了由AI生成的Vision-Language-Action (VLA)模型综述，总结了关键方法、发现和未来方向。内容由大规模语言模型生成，仅用于示范目的。此工作不代表原始研究，但突显了AI如何帮助自动化文献综述。随着AI生成内容的日益普及，确保准确性和可靠性以及妥善综述依然是一个挑战。未来的研究将侧重于开发AI辅助文献综述的结构化框架，探索提高引文准确性、来源可信度和背景理解的技术。通过考察大规模语言模型在学术写作中的潜力和限制，该项研究旨在促进AI整合到研究工作流中的更广泛讨论。此工作为系统化应用AI进行文献综述生成奠定了初步步骤，使学术知识综合更加高效和可扩展。 

---
# CAST: Cross Attention based multimodal fusion of Structure and Text for materials property prediction 

**Title (ZH)**: CAST：基于跨注意力机制的结构与文本多模态融合方法在材料性质预测中的应用 

**Authors**: Jaewan Lee, Changyoung Park, Hongjun Yang, Sungbin Lim, Sehui Han  

**Link**: [PDF](https://arxiv.org/pdf/2502.06836)  

**Abstract**: Recent advancements in AI have revolutionized property prediction in materials science and accelerating material discovery. Graph neural networks (GNNs) stand out due to their ability to represent crystal structures as graphs, effectively capturing local interactions and delivering superior predictions. However, these methods often lose critical global information, such as crystal systems and repetitive unit connectivity. To address this, we propose CAST, a cross-attention-based multimodal fusion model that integrates graph and text modalities to preserve essential material information. CAST combines node- and token-level features using cross-attention mechanisms, surpassing previous approaches reliant on material-level embeddings like graph mean-pooling or [CLS] tokens. A masked node prediction pretraining strategy further enhances atomic-level information integration. Our method achieved up to 22.9\% improvement in property prediction across four crystal properties including band gap compared to methods like CrysMMNet and MultiMat. Pretraining was key to aligning node and text embeddings, with attention maps confirming its effectiveness in capturing relationships between nodes and tokens. This study highlights the potential of multimodal learning in materials science, paving the way for more robust predictive models that incorporate both local and global information. 

**Abstract (ZH)**: 近期人工智能的发展 revolutionized 材料科学中的属性预测并加速了材料发现。CAST：一种基于跨注意力的多模态融合模型，通过整合图和文本模态保留关键材料信息。 

---
# Emotion Recognition and Generation: A Comprehensive Review of Face, Speech, and Text Modalities 

**Title (ZH)**: 情感识别与生成：面向、语音和文本模态的综述 

**Authors**: Rebecca Mobbs, Dimitrios Makris, Vasileios Argyriou  

**Link**: [PDF](https://arxiv.org/pdf/2502.06803)  

**Abstract**: Emotion recognition and generation have emerged as crucial topics in Artificial Intelligence research, playing a significant role in enhancing human-computer interaction within healthcare, customer service, and other fields. Although several reviews have been conducted on emotion recognition and generation as separate entities, many of these works are either fragmented or limited to specific methodologies, lacking a comprehensive overview of recent developments and trends across different modalities. In this survey, we provide a holistic review aimed at researchers beginning their exploration in emotion recognition and generation. We introduce the fundamental principles underlying emotion recognition and generation across facial, vocal, and textual modalities. This work categorises recent state-of-the-art research into distinct technical approaches and explains the theoretical foundations and motivations behind these methodologies, offering a clearer understanding of their application. Moreover, we discuss evaluation metrics, comparative analyses, and current limitations, shedding light on the challenges faced by researchers in the field. Finally, we propose future research directions to address these challenges and encourage further exploration into developing robust, effective, and ethically responsible emotion recognition and generation systems. 

**Abstract (ZH)**: 情绪识别与生成已成为人工智能研究中的关键话题，在 healthcare、客户服务中心及其他领域通过增强人机交互发挥着重要作用。虽然已有针对情绪识别和生成分别进行的综述，但这些工作的内容往往是零碎的或局限于特定的方法论，缺乏对不同模态领域内最新进展和趋势的全面概述。本文提供了一个全面的综述，旨在帮助刚开始探索情绪识别与生成的研究人员。我们介绍了跨面部、语音和文本模态的情绪识别与生成的基本原理。本文将最新的前沿研究归类为不同的技术方法，并解释这些方法背后的理论基础和动机，为应用提供更清晰的理解。此外，我们讨论了评估指标、对比分析以及当前的局限性，揭示了该领域研究人员面临的挑战。最后，我们提出未来的研究方向以应对这些挑战，并鼓励进一步探索开发强大、有效且负责任的情绪识别与生成系统。 

---
