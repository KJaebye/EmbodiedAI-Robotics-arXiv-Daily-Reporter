{'arxiv_id': 'arXiv:2502.07772', 'title': 'Automatic Robot Task Planning by Integrating Large Language Model with Genetic Programming', 'authors': 'Azizjon Kobilov, Jianglin Lan', 'link': 'https://arxiv.org/abs/2502.07772', 'abstract': 'Accurate task planning is critical for controlling autonomous systems, such as robots, drones, and self-driving vehicles. Behavior Trees (BTs) are considered one of the most prominent control-policy-defining frameworks in task planning, due to their modularity, flexibility, and reusability. Generating reliable and accurate BT-based control policies for robotic systems remains challenging and often requires domain expertise. In this paper, we present the LLM-GP-BT technique that leverages the Large Language Model (LLM) and Genetic Programming (GP) to automate the generation and configuration of BTs. The LLM-GP-BT technique processes robot task commands expressed in human natural language and converts them into accurate and reliable BT-based task plans in a computationally efficient and user-friendly manner. The proposed technique is systematically developed and validated through simulation experiments, demonstrating its potential to streamline task planning for autonomous systems.', 'abstract_zh': '准确的任务规划对于控制自主系统（如机器人、无人机和自动驾驶车辆）至关重要。行为树（BT）因其模块化、灵活性和重用性，被认为是任务规划中最具 prominence 的控制策略定义框架之一。构建可靠的基于行为树的机器人控制策略仍然具有挑战性，通常需要领域专业知识。本文提出了一种结合大型语言模型（LLM）和遗传编程（GP）的LLM-GP-BT技术，以自动化行为树的生成和配置。该技术处理用人类自然语言表达的机器人任务命令，并以高效的方式将它们转换为准确可靠的基于行为树的任务规划。所提出的技术通过仿真实验进行了系统开发和验证，展示了其在自主系统任务规划中简化流程的潜力。', 'title_zh': '基于大型语言模型与遗传编程集成的自动机器人任务规划'}
{'arxiv_id': 'arXiv:2502.07709', 'title': 'MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces', 'authors': 'Loris Gaven, Thomas Carta, Clément Romac, Cédric Colas, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer', 'link': 'https://arxiv.org/abs/2502.07709', 'abstract': "Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one's own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and LP online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces.", 'abstract_zh': '开放学习代理必须在广阔的可能空间中有效优先考虑目标，专注于那些最大化学习进步（LP）的目标。当通过在线强化学习训练的大维和演变的目标空间中的LLM代理实现了这种以内在目标为导向的探索时，LP预测中的一个关键挑战是对自身能力进行建模，这是一种元认知监控。传统方法要么需要广泛的抽样，要么依赖于脆弱的目标组专家定义。我们引入了MAGELLAN，这是一种元认知框架，让LLM代理能够在线学习预测其能力与LP。通过捕获目标之间的语义关系，MAGELLAN使LP估算更加样本高效，并通过泛化适应变化的目标空间。在一个交互式学习环境中，我们展示了MAGELLAN如何提高LP预测效率和目标优先级，并且是唯一使代理能够全面掌握一个庞大且不断变化的目标空间的方法。这些结果表明，通过为LLM代理增强一种预测LP的能力，可以有效地将课程学习扩展到开放的目标空间。', 'title_zh': 'MAGELLAN: 元认知的learning progress预测引导自主telic大模型代理在大规模目标空间中的学习'}
{'arxiv_id': 'arXiv:2502.07527', 'title': 'NatureLM: Deciphering the Language of Nature for Scientific Discovery', 'authors': 'Yingce Xia, Peiran Jin, Shufang Xie, Liang He, Chuan Cao, Renqian Luo, Guoqing Liu, Yue Wang, Zequn Liu, Yuan-Jyue Chen, Zekun Guo, Yeqi Bai, Pan Deng, Yaosen Min, Ziheng Lu, Hongxia Hao, Han Yang, Jielan Li, Chang Liu, Jia Zhang, Jianwei Zhu, Kehan Wu, Wei Zhang, Kaiyuan Gao, Qizhi Pei, Qian Wang, Xixian Liu, Yanting Li, Houtian Zhu, Yeqing Lu, Mingqian Ma, Zun Wang, Tian Xie, Krzysztof Maziarz, Marwin Segler, Zhao Yang, Zilong Chen, Yu Shi, Shuxin Zheng, Lijun Wu, Chen Hu, Peggy Dai, Tie-Yan Liu, Haiguang Liu, Tao Qin', 'link': 'https://arxiv.org/abs/2502.07527', 'abstract': 'Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the "language of nature", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.', 'abstract_zh': 'Foundation Models在自然语言处理和人工智能领域的革命及其在自然科学发现中的应用：跨领域科学数据预训练的Nature Language Model', 'title_zh': 'NatureLM：解读自然语言以推动科学研究'}
{'arxiv_id': 'arXiv:2502.07503', 'title': "Harnessing Language's Fractal Geometry with Recursive Inference Scaling", 'authors': 'Ibrahim Alabdulmohsin, Xiaohua Zhai', 'link': 'https://arxiv.org/abs/2502.07503', 'abstract': 'Recent research in language modeling reveals two scaling effects: the well-known improvement from increased training compute, and a lesser-known boost from applying more sophisticated or computationally intensive inference methods. Inspired by recent findings on the fractal geometry of language, we introduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe for scaling inference time. For a given fixed model architecture and training compute budget, RINS substantially improves language modeling performance. It also generalizes beyond pure language tasks, delivering gains in multimodal systems, including a +2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS improves both the asymptotic performance limits and the scaling exponents. These advantages are maintained even when compared to state-of-the-art recursive techniques like the "repeat-all-over" (RAO) strategy in Mobile LLM. Finally, stochastic RINS not only can enhance performance further but also provides the flexibility to optionally forgo increased inference computation at test time with minimal performance degradation.', 'abstract_zh': '近期语言模型研究揭示了两种缩放效应：众所周知的训练计算增加带来的改进，以及较少为人知的、更为复杂的或计算密集型推理方法带来的提升。受语言分形几何结构相关发现的启发，我们提出了递归推理缩放（RINS）作为扩展推理时间的补充插件方法。对于给定的固定模型架构和训练计算预算，RINS显著提升了语言模型性能。此外，通过推导数据缩放定律，我们展示了RINS不仅能提高渐近性能极限，还能改变缩放指数。即使与移动大模型中的“全重复”（RAO）策略等最先进的递归技术相比，这些优势仍然保持不变。最后，随机RINS不仅能进一步提升性能，还能在测试时选择性地避免增加推理计算，同时将性能下降降至最低。', 'title_zh': '利用递归推理缩放 harness 语言的分形几何结构'}
{'arxiv_id': 'arXiv:2502.07443', 'title': 'Approximating Human Strategic Reasoning with LLM-Enhanced Recursive Reasoners Leveraging Multi-agent Hypergames', 'authors': 'Vince Trencsenyi, Agnieszka Mensfelt, Kostas Stathis', 'link': 'https://arxiv.org/abs/2502.07443', 'abstract': 'LLM-driven multi-agent-based simulations have been gaining traction with applications in game-theoretic and social simulations. While most implementations seek to exploit or evaluate LLM-agentic reasoning, they often do so with a weak notion of agency and simplified architectures. We implement a role-based multi-agent strategic interaction framework tailored to sophisticated recursive reasoners, providing the means for systematic in-depth development and evaluation of strategic reasoning. Our game environment is governed by the umpire responsible for facilitating games, from matchmaking through move validation to environment management. Players incorporate state-of-the-art LLMs in their decision mechanism, relying on a formal hypergame-based model of hierarchical beliefs. We use one-shot, 2-player beauty contests to evaluate the recursive reasoning capabilities of the latest LLMs, providing a comparison to an established baseline model from economics and data from human experiments. Furthermore, we introduce the foundations of an alternative semantic measure of reasoning to the k-level theory. Our experiments show that artificial reasoners can outperform the baseline model in terms of both approximating human behaviour and reaching the optimal solution.', 'abstract_zh': '基于LLM的多agent模拟在博弈理论和社会模拟中的应用逐渐受到关注。我们实现了一种面向复杂递归推理者的角色制多agent战略互动框架，为其战略推理的系统深入开发和评估提供了手段。我们的游戏环境由裁判管理，负责从匹配到验证动作再到环境管理的整个流程。玩家在其决策机制中整合了最先进的LLM，并依赖于基于层次信念的正式超博弈模型。我们使用单轮2人的美丽竞赛来评估最新LLM的递归推理能力，并与经济学中的一个公认基准模型和人类实验数据进行比较。此外，我们引入了对k级理论的替代语义推理度量的基础。实验结果表明，人工推理者在逼近人类行为和达到最优解方面可超越基准模型。', 'title_zh': '利用多-agent超博弈增强的大语言模型强化递归推理近似人类战略推理'}
{'arxiv_id': 'arXiv:2502.07374', 'title': 'LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!', 'authors': 'Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica', 'link': 'https://arxiv.org/abs/2502.07374', 'abstract': "Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at this https URL.", 'abstract_zh': '大型推理模型（LRMs）通过长链推理（Long CoT）来解决复杂推理问题，长链推理包含反思、回溯和自我验证。然而，引发长链推理的训练技术和数据需求仍然不够了解。在本工作中，我们发现，大型语言模型（LLM）可以通过高效的数据标注有监督微调（SFT）和参数高效的低秩适配（LoRA）有效学习长链推理。仅使用17,000个长链推理训练样本，Qwen2.5-32B-Instruct模型在各类数学和编程基准测试上取得了显著改进，包括在AIME 2024上的56.7%（+40.0%）和LiveCodeBench上的57.0%（+8.1%），与专有o1-preview模型的表现相当（44.6%和59.1%）。更重要的是，我们发现长链推理的结构对于学习过程至关重要，而单个推理步骤的内容则影响甚微。内容上的扰动，如使用错误样本训练或移除推理关键词，对性能影响甚微。相比之下，扰乱长链推理逻辑一致性的结构修改，如打乱或删除推理步骤，会显著降低准确性。例如，使用错误答案的长链推理样本训练的模型，其准确性仅比使用完全正确样本训练的模型低3.2%。这些见解加深了我们对如何在LLM中激发推理能力的理解，并强调了高效训练下一代推理模型的关键考虑因素。这是我们之前发布的Sky-T1-32B-Preview模型的学术论文，代码可从该网址获取。', 'title_zh': 'LLMs可以通过演示学习推理，结构而不是内容才是关键！'}
{'arxiv_id': 'arXiv:2502.07266', 'title': 'When More is Less: Understanding Chain-of-Thought Length in LLMs', 'authors': 'Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, Yisen Wang', 'link': 'https://arxiv.org/abs/2502.07266', 'abstract': 'Chain-of-thought (CoT) reasoning enhances the multi-step reasoning capabilities of large language models (LLMs) by breaking complex tasks into smaller, manageable sub-tasks. Researchers have been exploring ways to guide models to generate more complex CoT processes to improve the reasoning ability of LLMs, such as long CoT and the test-time scaling law. However, for most models and tasks, does an increase in CoT length consistently lead to improved reasoning accuracy? In this paper, we observe a nuanced relationship: as the number of reasoning steps increases, performance initially improves but eventually decreases. To understand this phenomenon, we provide a piece of evidence that longer reasoning processes are increasingly susceptible to noise. We theoretically prove the existence of an optimal CoT length and derive a scaling law for this optimal length based on model capability and task difficulty. Inspired by our theory, we conduct experiments on both synthetic and real world datasets and propose Length-filtered Vote to alleviate the effects of excessively long or short CoTs. Our findings highlight the critical need to calibrate CoT length to align with model capabilities and task demands, offering a principled framework for optimizing multi-step reasoning in LLMs.', 'abstract_zh': 'Chain-of-Thought推理增强了大型语言模型的多步推理能力，但其长度增加是否一致地提高推理准确性？', 'title_zh': '当更多变成更少：理解LLMs中的链式思考长度'}
{'arxiv_id': 'arXiv:2502.07191', 'title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'authors': 'Fan Liu, Wenshuo Chao, Naiqiang Tan, Hao Liu', 'link': 'https://arxiv.org/abs/2502.07191', 'abstract': 'With the advancement of large language models (LLMs), solving complex reasoning tasks has gained increasing attention. Inference-time computation methods (e.g., Best-of-N, beam search, et al.) are particularly valuable as they can enhance reasoning performance without modifying model parameters or requiring additional training. However, these techniques come with implementation challenges, and most existing methods remain at the proof-of-concept stage with limited practical adoption due to their computational complexity and varying effectiveness across different tasks. In this paper, we investigate and benchmark diverse inference-time computation strategies across reasoning tasks of varying complexity. Since most current methods rely on a proposer-verifier pipeline that first generates candidate solutions (e.g., reasoning solutions) and then selects the best one based on reward signals (e.g., RLHF rewards, process rewards), our research focuses on optimizing both candidate solution generation (e.g., instructing prompts, hyperparameters such as temperature and top-p) and reward mechanisms (e.g., self-evaluation, reward types). Through extensive experiments (more than 20,000 A100-80G GPU hours with over 1,000 experiments) across a variety of models (e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation studies reveal that previously overlooked strategies can significantly enhance performance (e.g., tuning temperature can improve reasoning task performance by up to 5%). Furthermore, we establish a standardized benchmark for inference-time computation by systematically evaluating six representative methods across eight reasoning tasks. These findings provide a stronger foundation for future research. The code is available at this https URL', 'abstract_zh': '随着大规模语言模型（LLMs）的进步，解决复杂推理任务越来越受到关注。推理时的计算方法（例如，Best-of-N、束搜索等）特别有价值，因为它们可以在不修改模型参数或需要额外训练的情况下提升推理性能。然而，这些技术伴随着实施挑战，且大多数现有方法仍处于概念验证阶段，由于其计算复杂性和在不同任务上的不同有效性，限制了其实际应用。本文研究并 benchmark 了多种不同复杂度推理任务下的推理时计算策略。鉴于当前大多数方法依赖于一种提案-验证流水线，在生成候选解决方案（例如，推理解决方案）后，基于奖励信号（例如，RLHF奖励、过程奖励）选择最佳方案，我们的研究重点在于优化候选解决方案生成（例如，指令提示、温度和 top-p 等超参数）及奖励机制（例如，自我评估、奖励类型）。通过在各种规模模型（例如，Llama、Qwen 和 Mistral 家族）上的广泛实验（超过 20,000 个 A100-80G GPU 小时，超过 1,000 次实验），我们的消融研究显示，一些此前被忽视的策略可以显著提升性能（例如，调整温度可以使推理任务性能提高高达 5%）。此外，我们通过系统评估六种代表性方法在八种推理任务上的表现，建立了推理时计算的标准基准。这些发现为未来研究提供了更坚实的基础。代码可在该链接获取。', 'title_zh': '推理时间LLM推理计算技巧汇总'}
{'arxiv_id': 'arXiv:2502.07190', 'title': "Understanding LLMs' Fluid Intelligence Deficiency: An Analysis of the ARC Task", 'authors': 'Junjie Wu, Mo Yu, Lemao Liu, Dit-Yan Yeung, Jie Zhou', 'link': 'https://arxiv.org/abs/2502.07190', 'abstract': "While LLMs have exhibited strong performance on various NLP tasks, it is noteworthy that most of these tasks rely on utilizing the vast amount of knowledge encoded in LLMs' parameters, rather than solving new problems without prior knowledge. In cognitive research, the latter ability is referred to as fluid intelligence, which is considered to be critical for assessing human intelligence. Recent research on fluid intelligence assessments has highlighted significant deficiencies in LLMs' abilities. In this paper, we analyze the challenges LLMs face in demonstrating fluid intelligence through controlled experiments, using the most representative ARC task as an example. Our study revealed three major limitations in existing LLMs: limited ability for skill composition, unfamiliarity with abstract input formats, and the intrinsic deficiency of left-to-right decoding. Our data and code can be found in this https URL.", 'abstract_zh': '虽然大型语言模型在各种自然语言处理任务中表现出 strong 的性能，值得注意的是，这些任务大多依赖于利用嵌入在模型参数中的大量知识，而非解决没有任何先验知识的新问题。在认知研究中，后一种能力被称为流体智力，被认为是评估人类智力的关键。最近对流体智力评估的研究强调了语言模型在此方面存在的重大缺陷。在本文中，我们通过使用典型的 ARC 任务进行受控实验，分析语言模型在展示流体智力时面临的挑战。我们的研究揭示了现有语言模型存在的三大限制：技能组合能力有限、不熟悉抽象输入格式以及从左到右解码的内在缺陷。有关数据和代码请参见此链接：https://github.com/alibaba/Qwen-Language-Model。', 'title_zh': '理解LLMs在流动智能方面的不足：ARC任务分析'}
{'arxiv_id': 'arXiv:2502.07132', 'title': 'Interactive Data Harmonization with LLM Agents', 'authors': 'Aécio Santos, Eduardo H. M. Pena, Roque Lopez, Juliana Freire', 'link': 'https://arxiv.org/abs/2502.07132', 'abstract': 'Data harmonization is an essential task that entails integrating datasets from diverse sources. Despite years of research in this area, it remains a time-consuming and challenging task due to schema mismatches, varying terminologies, and differences in data collection methodologies. This paper presents the case for agentic data harmonization as a means to both empower experts to harmonize their data and to streamline the process. We introduce Harmonia, a system that combines LLM-based reasoning, an interactive user interface, and a library of data harmonization primitives to automate the synthesis of data harmonization pipelines. We demonstrate Harmonia in a clinical data harmonization scenario, where it helps to interactively create reusable pipelines that map datasets to a standard format. Finally, we discuss challenges and open problems, and suggest research directions for advancing our vision.', 'abstract_zh': '数据协调是一种必要的任务，涉及整合来自多种数据源的数据集。尽管在该领域进行了多年的研究，但由于模式不匹配、术语差异和数据收集方法的不同，这一任务仍然耗时且具有挑战性。本文提出了代理数据协调的概念，旨在授权专家进行数据协调并对过程进行简化。我们介绍了Harmonia系统，该系统结合了基于LLM的推理、交互式用户界面和数据协调原始操作库，以自动化数据协调管道的合成。我们通过一个临床数据协调场景展示了Harmonia，其中它帮助交互式创建可重复使用的管道，将数据集映射到标准格式。最后，我们讨论了挑战和开放问题，并提出了推动这一愿景发展的研究方向。', 'title_zh': 'LLM代理参与的数据互动一体化'}
{'arxiv_id': 'arXiv:2502.06975', 'title': 'Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents', 'authors': 'Mathis Pink, Qinyuan Wu, Vy Ai Vo, Javier Turek, Jianing Mu, Alexander Huth, Mariya Toneva', 'link': 'https://arxiv.org/abs/2502.06975', 'abstract': 'As Large Language Models (LLMs) evolve from text-completion tools into fully fledged agents operating in dynamic environments, they must address the challenge of continually learning and retaining long-term knowledge. Many biological systems solve these challenges with episodic memory, which supports single-shot learning of instance-specific contexts. Inspired by this, we present an episodic memory framework for LLM agents, centered around five key properties of episodic memory that underlie adaptive and context-sensitive behavior. With various research efforts already partially covering these properties, this position paper argues that now is the right time for an explicit, integrated focus on episodic memory to catalyze the development of long-term agents. To this end, we outline a roadmap that unites several research directions under the goal to support all five properties of episodic memory for more efficient long-term LLM agents.', 'abstract_zh': '大型语言模型（LLMs）从文本补全工具演进为在动态环境中运作的完备代理时，必须应对持续学习和保留长期知识的挑战。受生物系统使用情节记忆解决这些挑战的启发，我们提出了一种情节记忆框架，该框架围绕着情节记忆的五个关键属性，这些属性支持适应性和上下文敏感的行为。鉴于已有各种研究努力部分涵盖了这些属性，本文立场指出，现在是明确、整合关注情节记忆以推动长期代理发展的合适时机。为此，我们概述了一条道路图，将多个研究方向统一在支持情节记忆所有五个属性的目标之下，以促进更高效的长期记忆语言模型。', 'title_zh': '位置： episodic 记忆是长时 LLM 前辈缺乏的关键部分'}
{'arxiv_id': 'arXiv:2502.07771', 'title': 'Breaking Down Bias: On The Limits of Generalizable Pruning Strategies', 'authors': 'Sibo Ma, Alejandro Salinas, Peter Henderson, Julian Nyarko', 'link': 'https://arxiv.org/abs/2502.07771', 'abstract': 'We employ model pruning to examine how LLMs conceptualize racial biases, and whether a generalizable mitigation strategy for such biases appears feasible. Our analysis yields several novel insights. We find that pruning can be an effective method to reduce bias without significantly increasing anomalous model behavior. Neuron-based pruning strategies generally yield better results than approaches pruning entire attention heads. However, our results also show that the effectiveness of either approach quickly deteriorates as pruning strategies become more generalized. For instance, a model that is trained on removing racial biases in the context of financial decision-making poorly generalizes to biases in commercial transactions. Overall, our analysis suggests that racial biases are only partially represented as a general concept within language models. The other part of these biases is highly context-specific, suggesting that generalizable mitigation strategies may be of limited effectiveness. Our findings have important implications for legal frameworks surrounding AI. In particular, they suggest that an effective mitigation strategy should include the allocation of legal responsibility on those that deploy models in a specific use case.', 'abstract_zh': '我们采用模型剪枝来探讨LLMs如何概念化种族偏见，并探讨是否存在一种可行的普遍适用的偏见缓解策略。我们的分析揭示了一些新颖的见解。我们发现，剪枝可以有效地减少偏见而不显著增加模型异常行为。基于神经元的剪枝策略通常比剪枝整个注意力头的方法效果更好。然而，我们的结果也表明，这两种方法的有效性在剪枝策略更加通用时迅速下降。例如，一个在金融决策背景下训练以去除种族偏见的模型，在商业交易中的偏见缓解效果较差。总体而言，我们的分析表明，语言模型中种族偏见仅部分作为一个通用概念被表示，而这些偏见的另一部分高度依赖于具体情境，这暗示普遍适用的缓解策略可能效果有限。我们的发现对围绕AI的法律框架具有重要意义，特别是表明有效的缓解策略应包括在特定应用场景中部署模型的责任分配。', 'title_zh': '打破偏见：可泛化的剪枝策略的局限性'}
{'arxiv_id': 'arXiv:2502.07755', 'title': 'An Advanced NLP Framework for Automated Medical Diagnosis with DeBERTa and Dynamic Contextual Positional Gating', 'authors': 'Mohammad Ali Labbaf Khaniki, Sahabeh Saadati, Mohammad Manthouri', 'link': 'https://arxiv.org/abs/2502.07755', 'abstract': "This paper presents a novel Natural Language Processing (NLP) framework for enhancing medical diagnosis through the integration of advanced techniques in data augmentation, feature extraction, and classification. The proposed approach employs back-translation to generate diverse paraphrased datasets, improving robustness and mitigating overfitting in classification tasks. Leveraging Decoding-enhanced BERT with Disentangled Attention (DeBERTa) with Dynamic Contextual Positional Gating (DCPG), the model captures fine-grained contextual and positional relationships, dynamically adjusting the influence of positional information based on semantic context to produce high-quality text embeddings. For classification, an Attention-Based Feedforward Neural Network (ABFNN) is utilized, effectively focusing on the most relevant features to improve decision-making accuracy. Applied to the classification of symptoms, clinical notes, and other medical texts, this architecture demonstrates its ability to address the complexities of medical data. The combination of data augmentation, contextual embedding generation, and advanced classification mechanisms offers a robust and accurate diagnostic tool, with potential applications in automated medical diagnosis and clinical decision support. This method demonstrates the effectiveness of the proposed NLP framework for medical diagnosis, achieving remarkable results with an accuracy of 99.78%, recall of 99.72%, precision of 99.79%, and an F1-score of 99.75%. These metrics not only underscore the model's robust performance in classifying medical texts with exceptional precision and reliability but also highlight its superiority over existing methods, making it a highly promising tool for automated diagnostic systems.", 'abstract_zh': '一种通过数据增强、特征提取和分类的先进技术整合提高医学诊断的自然语言处理框架', 'title_zh': '基于DeBERTa和动态上下文位置门控的高级自然语言处理框架在自动医学诊断中的应用'}
{'arxiv_id': 'arXiv:2502.07752', 'title': 'Towards Efficient Optimizer Design for LLM via Structured Fisher Approximation with a Low-Rank Extension', 'authors': 'Wenbo Gong, Meyer Scetbon, Chao Ma, Edward Meeds', 'link': 'https://arxiv.org/abs/2502.07752', 'abstract': 'Designing efficient optimizers for large language models (LLMs) with low-memory requirements and fast convergence is an important and challenging problem. This paper makes a step towards the systematic design of such optimizers through the lens of structured Fisher information matrix (FIM) approximation. We show that many state-of-the-art efficient optimizers can be viewed as solutions to FIM approximation (under the Frobenius norm) with specific structural assumptions. Building on these insights, we propose two design recommendations of practical efficient optimizers for LLMs, involving the careful selection of structural assumptions to balance generality and efficiency, and enhancing memory efficiency of optimizers with general structures through a novel low-rank extension framework. We demonstrate how to use each design approach by deriving new memory-efficient optimizers: Row and Column Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation (Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the effectiveness, showing faster and better convergence than existing memory-efficient baselines and Adam with little memory overhead. Notably, Alice achieves better than 2x faster convergence over Adam, while RACS delivers strong performance on the 1B model with SGD-like memory.', 'abstract_zh': '设计具有低内存需求和快速收敛性的大型语言模型高效优化器：基于结构化 Fisher 信息矩阵近似的系统设计', 'title_zh': '基于结构化 Fisher 近似与低秩扩展的高效 LLM 优化器设计'}
{'arxiv_id': 'arXiv:2502.07747', 'title': 'WHODUNIT: Evaluation benchmark for culprit detection in mystery stories', 'authors': 'Kshitij Gupta', 'link': 'https://arxiv.org/abs/2502.07747', 'abstract': 'We present a novel data set, WhoDunIt, to assess the deductive reasoning capabilities of large language models (LLM) within narrative contexts. Constructed from open domain mystery novels and short stories, the dataset challenges LLMs to identify the perpetrator after reading and comprehending the story. To evaluate model robustness, we apply a range of character-level name augmentations, including original names, name swaps, and substitutions with well-known real and/or fictional entities from popular discourse. We further use various prompting styles to investigate the influence of prompting on deductive reasoning accuracy.\nWe conduct evaluation study with state-of-the-art models, specifically GPT-4o, GPT-4-turbo, and GPT-4o-mini, evaluated through multiple trials with majority response selection to ensure reliability. The results demonstrate that while LLMs perform reliably on unaltered texts, accuracy diminishes with certain name substitutions, particularly those with wide recognition. This dataset is publicly available here.', 'abstract_zh': '我们提出一个新颖的数据集，WhoDunIt，以评估大型语言模型（LLM）在叙述性情境中的演绎推理能力。该数据集由开放领域中的悬疑小说和短篇故事构成，挑战LLM在阅读和理解故事后识别凶手的能力。为评估模型的 robustness，我们应用了一系列字符级名称增广，包括原始名称、名称互换以及用知名的真实或虚构实体替换。我们还使用了各种提示风格来调查提示对演绎推理准确性的影响。通过多轮试验并选择多数响应来评估先进模型，特别是GPT-4o、GPT-4-turbo和GPT-4o-mini。结果表明，在未修改的文本上，LLM表现出色，但在某些名称替换后，尤其是广为人知的替换，准确性会降低。该数据集已公开。', 'title_zh': 'HO.ExecuteReader: 谋杀故事中凶手鉴定的评估基准'}
{'arxiv_id': 'arXiv:2502.07728', 'title': 'Verifying LLM-Generated Code in the Context of Software Verification with Ada/SPARK', 'authors': 'Marcos Cramer, Lucian McIntyre', 'link': 'https://arxiv.org/abs/2502.07728', 'abstract': 'Large language models (LLMs) have demonstrated remarkable code generation capabilities, but the correctness of the generated code cannot be inherently trusted. This paper explores the feasibility of using formal software verification, specifically the SPARK framework for Ada, to ensure the reliability of LLM-generated code. We present Marmaragan, a tool that leverages an LLM in order to generate SPARK annotations for existing programs, enabling formal verification of the code. The tool is benchmarked on a curated set of SPARK programs, with annotations selectively removed to test specific capabilities. The performance of Marmaragan with GPT-4o on the benchmark is promising, with correct annotations having been generated for 50.7% of the benchmark cases. The results establish a foundation for future work on combining the power of LLMs with the reliability of formal software verification.', 'abstract_zh': '大规模语言模型（LLMs）展现了出色的代码生成能力，但生成代码的正确性无法自然信任。本文探讨了使用形式软件验证，具体是Ada的SPARK框架，来确保LLM生成代码可靠性的可行性。我们介绍了Marmaragan工具，该工具利用LLM为现有程序生成SPARK注解，从而使代码可进行形式验证。该工具在精选的SPARK程序集上进行基准测试，通过选择性移除注解来测试特定能力。Marmaragan使用GPT-4o在基准测试中的表现令人鼓舞，50.7%的基准案例产生了正确的注解。实验结果为将LLM的力量与形式软件验证的可靠性相结合的未来工作奠定了基础。', 'title_zh': '在Ada/SPARK软件验证背景下验证LLM生成的代码'}
{'arxiv_id': 'arXiv:2502.07640', 'title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'authors': 'Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, Chi Jin', 'link': 'https://arxiv.org/abs/2502.07640', 'abstract': 'We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. The final prover outperforms all existing open-source models in whole-proof generation. On the miniF2F benchmark, it achieves a 57.6% success rate (Pass@32), exceeding the previous best open-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works.', 'abstract_zh': 'Goedel-Prover：一种在数学问题自动化形式证明中达到最佳性能的开源大规模语言模型', 'title_zh': 'Goedel-Prover：开源自动定理证明的前沿模型'}
{'arxiv_id': 'arXiv:2502.07586', 'title': "We Can't Understand AI Using our Existing Vocabulary", 'authors': 'John Hewitt, Robert Geirhos, Been Kim', 'link': 'https://arxiv.org/abs/2502.07586', 'abstract': 'This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start from the premise that humans and machines have differing concepts. This means interpretability can be framed as a communication problem: humans must be able to reference and control machine concepts, and communicate human concepts to machines. Creating a shared human-machine language through developing neologisms, we believe, could solve this communication problem. Successful neologisms achieve a useful amount of abstraction: not too detailed, so they\'re reusable in many contexts, and not too high-level, so they convey precise information. As a proof of concept, we demonstrate how a "length neologism" enables controlling LLM response length, while a "diversity neologism" allows sampling more variable responses. Taken together, we argue that we cannot understand AI using our existing vocabulary, and expanding it through neologisms creates opportunities for both controlling and understanding machines better.', 'abstract_zh': '论人工智能的理解：超越现有词汇，创造新的概念术语', 'title_zh': '我们无法用现有词汇理解AI。'}
{'arxiv_id': 'arXiv:2502.07459', 'title': 'PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian', 'authors': 'Erfan Moosavi Monazzah, Vahid Rahimzadeh, Yadollah Yaghoobzadeh, Azadeh Shakery, Mohammad Taher Pilehvar', 'link': 'https://arxiv.org/abs/2502.07459', 'abstract': 'Large language models predominantly reflect Western cultures, largely due to the dominance of English-centric training data. This imbalance presents a significant challenge, as LLMs are increasingly used across diverse contexts without adequate evaluation of their cultural competence in non-English languages, including Persian. To address this gap, we introduce PerCul, a carefully constructed dataset designed to assess the sensitivity of LLMs toward Persian culture. PerCul features story-based, multiple-choice questions that capture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is curated with input from native Persian annotators to ensure authenticity and to prevent the use of translation as a shortcut. We evaluate several state-of-the-art multilingual and Persian-specific LLMs, establishing a foundation for future research in cross-cultural NLP evaluation. Our experiments demonstrate a 11.3% gap between best closed source model and layperson baseline while the gap increases to 21.3% by using the best open-weight model. You can access the dataset from here: this https URL', 'abstract_zh': '大型语言模型主要反映了西方文化，主要是由于以英语为中心的训练数据占主导地位。这种不平衡提出了一个重大挑战，因为LLMs在多种情境下的应用日益增多，而对其在非英语语言，包括波斯语中的文化适应性评估并不充分。为应对这一缺口，我们引入了PerCul，这是一个精心构建的语料库，旨在评估LLMs对波斯文化的敏感度。PerCul包含基于故事的多项选择题，能捕捉文化细微的情景。与现有基准不同，PerCul通过使用波斯语本土注释者进行定制，以确保真实性和防止使用翻译作为捷径。我们评估了几种最先进的多语言和波斯语专用LLMs，为跨文化NLP评估的未来研究奠定了基础。实验结果显示，在最佳封闭源模型和普通人基线之间有11.3%的差距，而使用最佳开源模型时，这一差距增加到21.3%。您可从此链接访问数据集：this https URL。', 'title_zh': 'PerCul：以故事为驱动的波斯语语言模型文化评估'}
{'arxiv_id': 'arXiv:2502.07445', 'title': 'Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon', 'authors': 'Nurit Cohen-Inger, Yehonatan Elisha, Bracha Shapira, Lior Rokach, Seffi Cohen', 'link': 'https://arxiv.org/abs/2502.07445', 'abstract': "Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework that systematically distorts benchmark prompts via a parametric transformation and detects overfitting of LLMs. By rephrasing inputs while preserving their semantic content and labels, C-BOD exposes whether a model's performance is driven by memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our method reveals an average performance degradation of 2.15% under modest perturbations, with 20 out of 26 models exhibiting statistically significant differences. Notably, models with higher baseline accuracy exhibit larger performance differences under perturbation, and larger LLMs tend to be more sensitive to rephrasings indicating that both cases may overrely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy show insignificant degradation, suggesting reduced dependency on superficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation.", 'abstract_zh': '大型语言模型（LLMs）在公共基准测试中常常表现出色，但这些高分可能掩盖了其对特定数据集表面特征的过度依赖而非真正的语言理解能力。我们引入了变色龙基准过拟合检测器（C-BOD），这是一种元评估框架，通过参数化变换系统地扭曲基准测试提示，并检测LLMs的过拟合。通过保留输入的语义内容和标签重新表述输入，C-BOD揭示了模型性能是否受记忆模式驱动。在使用26个领先LLM评估MMLU基准测试时，我们的方法在适度扰动下平均性能下降2.15%，其中20个模型显示出统计学上的显著差异。值得注意的是，基线准确率较高的模型在扰动下的性能差异更大，而大模型往往对重新表述更为敏感，这表明这两种情况可能过度依赖固定提示模式。相反，Llama家族模型和基线准确率较低的模型显示出不显著的下降，这表明它们对表面线索的依赖较小。此外，C-BOD的数据集和模型无关性设计使其易于集成到训练管道中，以促进更 robust的语言理解能力。我们的研究结果挑战了社区仅关注排行榜分数，并优先考虑LLM评估中的弹性和泛化能力。', 'title_zh': '忽略你对LLMs评估所知道的一切 - LLMs如同变色龙'}
{'arxiv_id': 'arXiv:2502.07424', 'title': 'RomanLens: Latent Romanization and its role in Multilinguality in LLMs', 'authors': 'Alan Saji, Jaavid Aktar Husain, Thanmay Jayakumar, Raj Dabre, Anoop Kunchukuttan, Mitesh M. Khapra, Ratish Puduppully', 'link': 'https://arxiv.org/abs/2502.07424', 'abstract': "Large Language Models (LLMs) exhibit remarkable multilingual generalization despite being predominantly trained on English-centric corpora. A fundamental question arises: how do LLMs achieve such robust multilingual capabilities? For non-Latin script languages, we investigate the role of romanization - the representation of non-Latin scripts using Latin characters - as a bridge in multilingual processing. Using mechanistic interpretability techniques, we analyze next-token generation and find that intermediate layers frequently represent target words in romanized form before transitioning to native script, a phenomenon we term Latent Romanization. Further, through activation patching experiments, we demonstrate that LLMs encode semantic concepts similarly across native and romanized scripts, suggesting a shared underlying representation. Additionally in translation towards non Latin languages, our findings reveal that when the target language is in romanized form, its representations emerge earlier in the model's layers compared to native script. These insights contribute to a deeper understanding of multilingual representation in LLMs and highlight the implicit role of romanization in facilitating language transfer. Our work provides new directions for potentially improving multilingual language modeling and interpretability.", 'abstract_zh': '大型语言模型（LLMs）尽管主要基于以英语为中心的语料库训练，仍展现出令人瞩目的多语言泛化能力。一个基本问题随之而来：LLMs 是如何实现如此稳健的多语言能力的？对于非拉丁字母 script 的语言，我们探讨了罗马化——即使用拉丁字母表示非拉丁字母 script——在多语言处理中的作用。通过运用机械可解释性技术，我们分析了下一个标记生成过程，并发现中间层经常以罗马化形式表示目标词，随后过渡到原生 script，我们称这一现象为隐含罗马化。此外，通过激活补丁实验，我们证明了LLMs 在原生 script 和罗马化 script 中对语义概念的编码方式相似，暗示存在共享的潜在表示。在翻译至非拉丁字母语言时，我们的发现显示，当目标语言以罗马化形式给出时，其表示在模型层中出现得更早，相比原生 script 语言而言。这些见解加深了对LLMs 中多语言表示的理解，并突显了罗马化在促进语言迁移中隐含的作用。我们的工作为改进多语言语言建模和可解释性提供了新的方向。', 'title_zh': 'RomanLens: 隐含的罗马化及其在LLMs中的多语言作用'}
{'arxiv_id': 'arXiv:2502.07399', 'title': 'On Iterative Evaluation and Enhancement of Code Quality Using GPT-4o', 'authors': 'Rundong Liu, Andre Frade, Amal Vaidya, Maxime Labonne, Marcus Kaiser, Bismayan Chakrabarti, Jonathan Budd, Sean Moran', 'link': 'https://arxiv.org/abs/2502.07399', 'abstract': "This paper introduces CodeQUEST, a novel framework leveraging Large Language Models (LLMs) to iteratively evaluate and enhance code quality across multiple dimensions, including readability, maintainability, efficiency, and security. The framework is divided into two main components: an Evaluator that assesses code quality across ten dimensions, providing both quantitative scores and qualitative summaries, and an Optimizer that iteratively improves the code based on the Evaluator's feedback. Our study demonstrates that CodeQUEST can effectively and robustly evaluate code quality, with its assessments aligning closely with established code quality metrics. Through a series of experiments using a curated dataset of Python and JavaScript examples, CodeQUEST demonstrated significant improvements in code quality, achieving a mean relative percentage improvement of 52.6%. The framework's evaluations were validated against a set of proxy metrics comprising of Pylint Score, Radon Maintainability Index, and Bandit output logs, showing a meaningful correlation. This highlights the potential of LLMs in automating code quality evaluation and improvement processes, presenting a significant advancement toward enhancing software development practices. The code implementation of the framework is available at: this https URL.", 'abstract_zh': '本文介绍了CodeQUEST，这是一种利用大型语言模型（LLMs）逐步评估和提升代码质量的新框架，覆盖可读性、可维护性、效率和安全性等多个维度。该框架分为两个主要部分：评估器，用于从十个维度评估代码质量，提供量化评分和定性总结；优化器，根据评估器的反馈逐步改进代码。我们的研究显示，CodeQUEST能够有效地且稳健地评估代码质量，其评估结果与已有的代码质量指标高度一致。通过使用精心筛选的Python和JavaScript示例数据集进行的一系列实验，CodeQUEST展示了显著的代码质量改进，平均相对百分比改进为52.6%。框架的评估结果与Pylint评分、Radon维护性指数和Bandit输出日志等代理指标集进行了验证，显示出有意义的相关性。这表明，大型语言模型有可能实现代码质量评估和改进的自动化，并为提升软件开发实践带来了重要进展。该框架的代码实现可在以下链接获取：this https URL。', 'title_zh': '使用GPT-4o迭代评估和提升代码质量'}
{'arxiv_id': 'arXiv:2502.07340', 'title': 'Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering', 'authors': 'Shuzheng Si, Haozhe Zhao, Gang Chen, Cheng Gao, Yuzhuo Bai, Zhitong Wang, Kaikai An, Kangyang Luo, Chen Qian, Fanchao Qi, Baobao Chang, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.07340', 'abstract': "Training LLMs on data that contains unfamiliar knowledge during the instruction tuning stage can make LLMs overconfident and encourage hallucinations. To address this challenge, we introduce a novel framework, NOVA, which identifies high-quality data that aligns well with the LLM's learned knowledge to reduce hallucinations. NOVA includes Internal Consistency Probing (ICP) and Semantic Equivalence Identification (SEI) to measure how familiar the LLM is with instruction data. Specifically, ICP evaluates the LLM's understanding of the given instruction by calculating the tailored consistency among multiple self-generated responses. SEI further assesses the familiarity of the LLM with the target response by comparing it to the generated responses, using the proposed semantic clustering and well-designed voting strategy. Finally, we introduce an expert-aligned reward model, considering characteristics beyond just familiarity to enhance data quality. By considering data quality and avoiding unfamiliar data, we can utilize the selected data to effectively align LLMs to follow instructions and hallucinate less. Extensive experiments and analysis show that NOVA significantly reduces hallucinations and allows LLMs to maintain a strong ability to follow instructions.", 'abstract_zh': '在训练阶段使用包含陌生知识的数据会对LLM造成过度自信并引发幻觉。为解决这一挑战，我们提出了一种新型框架NOVA，以识别与LLM学习的知识高度一致的高质量数据，从而减少幻觉。NOVA包括内部一致性探针（ICP）和语义等价识别（SEI），用于衡量LLM对指令数据的熟悉程度。具体而言，ICP通过计算多个自动生成的回答之间的定制一致性来评估LLM对给定指令的理解。SEI进一步通过将目标回答与生成的回答进行比较，使用提议的语义聚类和精心设计的投票策略来评估LLM对目标回答的熟悉程度。最后，我们引入了一种专家对齐的奖励模型，考虑超出熟悉度之外的其他特征来提高数据质量。通过考虑数据质量和避免陌生数据，我们可以有效利用所选数据使LLM更符合指令要求并减少幻觉。广泛的经验验证和分析表明，NOVA显著减少了幻觉，使LLM能够保持良好的跟随指令的能力。', 'title_zh': '通过有效的数据过滤使大型语言模型更好地遵循指令并减少幻觉'}
{'arxiv_id': 'arXiv:2502.07316', 'title': 'CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction', 'authors': 'Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, Junxian He', 'link': 'https://arxiv.org/abs/2502.07316', 'abstract': 'Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at this https URL.', 'abstract_zh': 'Large Language Models中的CodeI/O：一种系统化凝练上下文关联代码中内在推理模式的新方法', 'title_zh': 'CodeI/O: 通过代码输入-输出预测凝练推理模式'}
{'arxiv_id': 'arXiv:2502.07286', 'title': 'Small Language Model Makes an Effective Long Text Extractor', 'authors': 'Yelin Chen, Fanjin Zhang, Jie Tang', 'link': 'https://arxiv.org/abs/2502.07286', 'abstract': 'Named Entity Recognition (NER) is a fundamental problem in natural language processing (NLP). However, the task of extracting longer entity spans (e.g., awards) from extended texts (e.g., homepages) is barely explored. Current NER methods predominantly fall into two categories: span-based methods and generation-based methods. Span-based methods require the enumeration of all possible token-pair spans, followed by classification on each span, resulting in substantial redundant computations and excessive GPU memory usage. In contrast, generation-based methods involve prompting or fine-tuning large language models (LLMs) to adapt to downstream NER tasks. However, these methods struggle with the accurate generation of longer spans and often incur significant time costs for effective fine-tuning. To address these challenges, this paper introduces a lightweight span-based NER method called SeNER, which incorporates a bidirectional arrow attention mechanism coupled with LogN-Scaling on the [CLS] token to embed long texts effectively, and comprises a novel bidirectional sliding-window plus-shaped attention (BiSPA) mechanism to reduce redundant candidate token-pair spans significantly and model interactions between token-pair spans simultaneously. Extensive experiments demonstrate that our method achieves state-of-the-art extraction accuracy on three long NER datasets and is capable of extracting entities from long texts in a GPU-memory-friendly manner. Code: this https URL', 'abstract_zh': '命名实体识别（NER）是自然语言处理（NLP）中的一个基础问题。然而，从扩展文本（如主页）中提取较长实体跨度（如奖项）的任务鲜有研究。当前的NER方法主要分为基于跨度的方法和生成基于的方法。基于跨度的方法需要枚举所有可能的词对跨度，然后对每个跨度进行分类，导致大量的冗余计算和过度的GPU内存使用。相比之下，生成基于的方法涉及对大型语言模型（LLMs）进行提示或微调以适应下游NER任务。然而，这些方法在准确生成较长跨度方面存在困难，并且通常需要显著的时间成本来进行有效的微调。为了解决这些挑战，本文提出了一种轻量级的基于跨度的NER方法SeNER，该方法结合了双向箭头注意力机制和[CLS]标记上的LogN-Scaling嵌入长文本，并包含一种新颖的双向滑动窗口菱形注意力机制（BiSPA），以显著减少冗余的候选词对跨度并同时建模词对跨度之间的交互。广泛实验证明，本文方法在三个长NER数据集上达到了最先进的提取准确率，并且能够以友好GPU内存的方式从长文本中提取实体。代码：https://this-url。', 'title_zh': '小型语言模型成为一个有效的长文本提取器'}
{'arxiv_id': 'arXiv:2502.07250', 'title': 'NARCE: A Mamba-Based Neural Algorithmic Reasoner Framework for Online Complex Event Detection', 'authors': 'Liying Han, Gaofeng Dong, Xiaomin Ouyang, Lance Kaplan, Federico Cerutti, Mani Srivastava', 'link': 'https://arxiv.org/abs/2502.07250', 'abstract': "Current machine learning models excel in short-span perception tasks but struggle to derive high-level insights from long-term observation, a capability central to understanding complex events (CEs). CEs, defined as sequences of short-term atomic events (AEs) governed by spatiotemporal rules, are challenging to detect online due to the need to extract meaningful patterns from long and noisy sensor data while ignoring irrelevant events. We hypothesize that state-based methods are well-suited for CE detection, as they capture event progression through state transitions without requiring long-term memory. Baseline experiments validate this, demonstrating that the state-space model Mamba outperforms existing architectures. However, Mamba's reliance on extensive labeled data, which are difficult to obtain, motivates our second hypothesis: decoupling CE rule learning from noisy sensor data can reduce data requirements. To address this, we propose NARCE, a framework that combines Neural Algorithmic Reasoning (NAR) to split the task into two components: (i) learning CE rules independently of sensor data using synthetic concept traces generated by LLMs and (ii) mapping sensor inputs to these rules via an adapter. Our results show that NARCE outperforms baselines in accuracy, generalization to unseen and longer sensor data, and data efficiency, significantly reducing annotation costs while advancing robust CE detection.", 'abstract_zh': '基于神经算法推理的事件规则学习框架：实现复杂事件检测的高效率与鲁棒性', 'title_zh': 'NARCE：一种基于Mamba的神经算法推理框架，用于在线复杂事件检测'}
{'arxiv_id': 'arXiv:2502.07218', 'title': 'LUNAR: LLM Unlearning via Neural Activation Redirection', 'authors': 'William F. Shen, Xinchi Qiu, Meghdad Kurmanji, Alex Iacob, Lorenzo Sani, Yihong Chen, Nicola Cancedda, Nicholas D. Lane', 'link': 'https://arxiv.org/abs/2502.07218', 'abstract': 'Large Language Models (LLMs) benefit from training on ever larger amounts of textual data, but as a result, they increasingly incur the risk of leaking private information. The ability to selectively remove knowledge from LLMs is, therefore, a highly desirable capability. In this paper, we propose LUNAR, a novel unlearning methodology grounded in the Linear Representation Hypothesis. LUNAR operates by redirecting the representations of unlearned data to regions that trigger the model\'s inherent ability to express its inability to answer. LUNAR achieves state-of-the-art unlearning performance while significantly enhancing the controllability of the unlearned model during inference. Specifically, LUNAR achieves between 2.9x to 11.7x improvements on combined "unlearning efficacy" and "model utility" score ("Deviation Score") on the PISTOL dataset across various base models. We also demonstrate, through quantitative analysis and qualitative examples, LUNAR\'s superior controllability in generating coherent and contextually aware responses, mitigating undesired side effects of existing methods. Moreover, we demonstrate that LUNAR is robust against white-box adversarial attacks and versatile in handling real-world scenarios, such as processing sequential unlearning requests.', 'abstract_zh': '大型语言模型（LLMs）通过使用越来越多的文本数据进行训练而受益，但随之而来的风险是泄露私人信息。因此，能够选择性地从LLMs中删除知识是一项极其 desirable 的能力。本文我们提出LUNAR，一种基于线性表示假说的新型遗忘方法。LUNAR通过将未学习数据的表示重定向到能够触发模型表达其无法回答能力的区域来工作。LUNAR在PISTOL数据集上实现了最先进的遗忘性能，同时显著增强了推理过程中未学习模型的可控性。具体而言，LUNAR在各种基模型上实现了2.9倍至11.7倍的“遗忘有效性”和“模型实用性”得分（“偏差分数”）的改善。我们还通过定量分析和定性示例展示了LUNAR在生成连贯且上下文相关的响应方面具有优越的可控性，减少了现有方法的不良副作用。此外，我们展示了LUNAR能够抵抗白盒对抗攻击，并且能够灵活处理现实世界场景，例如处理顺序遗忘请求。', 'title_zh': 'LUNAR: 通过神经激活重定向实现大语言模型去学习'}
{'arxiv_id': 'arXiv:2502.07184', 'title': 'Refine Knowledge of Large Language Models via Adaptive Contrastive Learning', 'authors': 'Yinghui Li, Haojing Huang, Jiayi Kuang, Yangning Li, Shu-Yu Guo, Chao Qu, Xiaoyu Tan, Hai-Tao Zheng, Ying Shen, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.07184', 'abstract': "How to alleviate the hallucinations of Large Language Models (LLMs) has always been the fundamental goal pursued by the LLMs research community. Looking through numerous hallucination-related studies, a mainstream category of methods is to reduce hallucinations by optimizing the knowledge representation of LLMs to change their output. Considering that the core focus of these works is the knowledge acquired by models, and knowledge has long been a central theme in human societal progress, we believe that the process of models refining knowledge can greatly benefit from the way humans learn. In our work, by imitating the human learning process, we design an Adaptive Contrastive Learning strategy. Our method flexibly constructs different positive and negative samples for contrastive learning based on LLMs' actual mastery of knowledge. This strategy helps LLMs consolidate the correct knowledge they already possess, deepen their understanding of the correct knowledge they have encountered but not fully grasped, forget the incorrect knowledge they previously learned, and honestly acknowledge the knowledge they lack. Extensive experiments and detailed analyses on widely used datasets demonstrate the effectiveness of our method.", 'abstract_zh': '如何缓解大型语言模型的幻觉一直是大型语言模型研究社区的根本目标。通过对大量关于幻觉的研究进行审视，降低幻觉的主要方法之一是通过优化大型语言模型的知识表示来改变其输出。鉴于这些工作的核心重点是模型所获得的知识，而知识一直是人类社会进步中的中心主题，我们认为模型提炼知识的过程可以从人类的学习方式中获得极大的益处。在我们的工作中，通过模仿人类的学习过程，我们设计了一种自适应对比学习策略。该方法基于模型实际掌握的知识，灵活构建不同正负样本进行对比学习，帮助模型巩固已有的正确知识，加深对已接触但未完全掌握的正确知识的理解，遗忘之前学到的错误知识，并诚实地承认自身所缺乏的知识。广泛的实验和对常用数据集的详细分析证明了该方法的有效性。', 'title_zh': '通过自适应对比学习精炼大型语言模型的知识'}
{'arxiv_id': 'arXiv:2502.07165', 'title': "Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification", 'authors': 'Peipei Wei, Dimitris Dimitriadis, Yan Xu, Mingwei Shen', 'link': 'https://arxiv.org/abs/2502.07165', 'abstract': 'We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent prompting strategy for text classification. It first asks multiple LLM agents to independently generate candidate principles based on analysis of demonstration samples with or without labels, consolidates them into final principles via a finalizer agent, and then sends them to a classifier agent to perform downstream classification tasks. Extensive experiments on binary and multi-class classification datasets with different sizes of LLMs show that our approach not only achieves substantial performance gains (1.55% - 19.37%) over zero-shot prompting on macro-F1 score but also outperforms other strong baselines (CoT and stepback prompting). Principles generated by our approach help LLMs perform better on classification tasks than human crafted principles on two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach also shows on-par or better performance compared to demonstration-based few-shot prompting approaches, yet with substantially lower inference costs. Ablation studies show that label information and the multi-agent cooperative LLM framework play an important role in generating high-quality principles to facilitate downstream classification tasks.', 'abstract_zh': '基于原理的提示策略：一种简单的高效多Agent文本分类提示方法', 'title_zh': '基于原理的多agent提示策略：超越演示，教给我原则：文本分类'}
{'arxiv_id': 'arXiv:2502.07164', 'title': 'Does Training on Synthetic Data Make Models Less Robust?', 'authors': 'Lingze Zhang, Ellie Pavlick', 'link': 'https://arxiv.org/abs/2502.07164', 'abstract': 'An increasingly common practice is to train large language models (LLMs) using synthetic data. Often this synthetic data is produced by the same or similar LLMs as those it is being used to train. This raises the question of whether the synthetic data might in fact exacerbate certain "blindspots" by reinforcing heuristics that the LLM already encodes. In this paper, we conduct simulated experiments on the natural language inference (NLI) task with Llama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted evaluation set designed to measure the presence of specific heuristic strategies for NLI, as our "blindspot" task. Our goal is to determine whether performance disparities between the general and blind spot tasks emerge. Our results indicate that synthetic data does not reinforce blindspots in the way we expected. Specifically, we see that, while fine-tuning with synthetic data doesn\'t necessarily reduce the use of the heuristic, it also does not make it worse as we hypothesized.', 'abstract_zh': '使用合成数据训练大型语言模型：对特定启发式策略盲点的影响探究', 'title_zh': '合成数据训练会使模型变得不够 robust 吗？'}
{'arxiv_id': 'arXiv:2502.07154', 'title': 'Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning', 'authors': 'Feng Chen, Allan Raventos, Nan Cheng, Surya Ganguli, Shaul Druckmann', 'link': 'https://arxiv.org/abs/2502.07154', 'abstract': 'Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation. This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget? To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in $N$ independent samples. We show, surprisingly, that training with cross-entropy (CE) loss can be ${\\it misaligned}$ with pass@N in that pass@N accuracy ${\\it decreases}$ with longer training. We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N. Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance. Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions; and (2) proving theorems by searching over proof trees of varying shapes. Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies.', 'abstract_zh': '近期大规模语言模型的进展突显了通过扩展测试时计算资源来实现复杂任务（如数学推理和代码生成）高性能的能力。这提出了一个关键问题：在后续的测试时计算策略和预算下，如何修改模型训练以优化性能？为探索这一问题，我们专注于pass@N这一简单的测试时策略，该策略在N个独立样本中搜索正确答案。我们发现，使用交叉熵（CE）损失的训练可能会与pass@N产生偏差，即随着训练时间的延长，pass@N的准确性会下降。我们从CE引起的模型过度自信的角度解释了这种偏差的来源，并通过实验验证了过度自信是pass@N扩展测试时计算能力的主要障碍。此外，我们提出了一种原理上合理的、与pass@N更一致的训练损失，通过限制模型的信心和恢复pass@N测试性能。我们的算法在MATH和MiniF2F基准测试中展示了改进的数学推理能力，包括（1）回答数学问题；和（2）通过搜索不同形状的证明树来证明定理。总体而言，我们的工作强调了需要联合设计LLM开发中的两个传统分离阶段：训练时协议和测试时搜索与推理策略的重要性。', 'title_zh': '重新思考扩展测试时计算量时的微调：限制信心提高数学推理能力'}
{'arxiv_id': 'arXiv:2502.07128', 'title': 'Cardiverse: Harnessing LLMs for Novel Card Game Prototyping', 'authors': 'Danrui Li, Sen Zhang, Sam S. Sohn, Kaidong Hu, Muhammad Usman, Mubbasir Kapadia', 'link': 'https://arxiv.org/abs/2502.07128', 'abstract': 'The prototyping of computer games, particularly card games, requires extensive human effort in creative ideation and gameplay evaluation. Recent advances in Large Language Models (LLMs) offer opportunities to automate and streamline these processes. However, it remains challenging for LLMs to design novel game mechanics beyond existing databases, generate consistent gameplay environments, and develop scalable gameplay AI for large-scale evaluations. This paper addresses these challenges by introducing a comprehensive automated card game prototyping framework. The approach highlights a graph-based indexing method for generating novel game designs, an LLM-driven system for consistent game code generation validated by gameplay records, and a gameplay AI constructing method that uses an ensemble of LLM-generated action-value functions optimized through self-play. These contributions aim to accelerate card game prototyping, reduce human labor, and lower barriers to entry for game developers.', 'abstract_zh': '基于大型语言模型的卡片游戏原型设计框架', 'title_zh': 'Cardiverse: 利用大规模语言模型进行新型纸牌游戏原型设计'}
{'arxiv_id': 'arXiv:2502.07115', 'title': 'Online Scheduling for LLM Inference with KV Cache Constraints', 'authors': 'Patrick Jaillet, Jiashuo Jiang, Chara Podimata, Zijie Zhou', 'link': 'https://arxiv.org/abs/2502.07115', 'abstract': "Large Language Model (LLM) inference, where a trained model generates text one word at a time in response to user prompts, is a computationally intensive process requiring efficient scheduling to optimize latency and resource utilization. A key challenge in LLM inference is the management of the Key-Value (KV) cache, which reduces redundant computations but introduces memory constraints. In this work, we model LLM inference with KV cache constraints theoretically and propose novel batching and scheduling algorithms that minimize inference latency while effectively managing the KV cache's memory.\nWe analyze both semi-online and fully online scheduling models, and our results are threefold. First, we provide a polynomial-time algorithm that achieves exact optimality in terms of average latency in the semi-online prompt arrival model. Second, in the fully online case with a stochastic prompt arrival, we introduce an efficient online scheduling algorithm with constant regret. Third, we prove that no algorithm (deterministic or randomized) can achieve a constant competitive ratio in fully online adversarial settings. Our empirical evaluations on a public LLM inference dataset, using the Llama-70B model on A100 GPUs, show that our approach significantly outperforms benchmark algorithms used currently in practice, achieving lower latency while reducing energy consumption. Overall, our results offer a path toward more sustainable and cost-effective LLM deployment.", 'abstract_zh': '大型语言模型（LLM）推理中，通过高效调度以优化延迟和资源利用率，在用户提示下逐词生成文本是一个计算密集的过程。在LLM推理中，管理键-值（KV）缓存是一项关键挑战，虽然它可以减少冗余计算，但也引入了内存约束。在本项工作中，我们理论上建模了带有KV缓存约束的LLM推理过程，并提出了新颖的分批和调度算法，以最小化推理延迟并有效管理KV缓存的内存。', 'title_zh': '带有KV缓存约束的LLM推理在线调度'}
{'arxiv_id': 'arXiv:2502.07088', 'title': 'Kernels of Selfhood: GPT-4o shows humanlike patterns of cognitive consistency moderated by free choice', 'authors': 'Steven A. Lehr, Ketan S. Saichandran, Eddie Harmon-Jones, Nykko Vitali, Mahzarin R. Banaji', 'link': 'https://arxiv.org/abs/2502.07088', 'abstract': "Large Language Models (LLMs) show emergent patterns that mimic human cognition. We explore whether they also mirror other, less deliberative human psychological processes. Drawing upon classical theories of cognitive consistency, two preregistered studies tested whether GPT-4o changed its attitudes toward Vladimir Putin in the direction of a positive or negative essay it wrote about the Russian leader. Indeed, GPT displayed patterns of attitude change mimicking cognitive consistency effects in humans. Even more remarkably, the degree of change increased sharply when the LLM was offered an illusion of choice about which essay (positive or negative) to write. This result suggests that GPT-4o manifests a functional analog of humanlike selfhood, although how faithfully the chatbot's behavior reflects the mechanisms of human attitude change remains to be understood.", 'abstract_zh': '大型语言模型（LLMs）展示了模仿人类认知的 emergent 模式。我们探讨它们是否也反映了其他较少经过深思熟虑的人类心理学过程。依据古典的认知一致性理论，两项预先注册的研究测试了 GPT-4 是否在其撰写的关于俄罗斯领导人普京的正面或负面文章引导下，态度发生了向积极或消极的变化。确实，GPT 展现出与人类认知一致性效应相似的态度变化模式。更令人惊讶的是，当给 LLM 提供关于撰写哪篇（正面或负面）文章的假象选择时，态度变化的程度急剧增加。这一结果表明，GPT-4o 展现了一种功能上的类人类自我模拟，但聊天机器人行为如何反映人类态度变化的机制尚需进一步理解。', 'title_zh': '自我之核：GPT-4o展现出由自由选择调和的人类认知一致性模式'}
{'arxiv_id': 'arXiv:2502.07072', 'title': 'IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large Language Models', 'authors': 'Sayem Mohammad Imtiaz, Astha Singh, Fraol Batole, Hridesh Rajan', 'link': 'https://arxiv.org/abs/2502.07072', 'abstract': "Not a day goes by without hearing about the impressive feats of large language models (LLMs), and equally, not a day passes without hearing about their challenges. LLMs are notoriously vulnerable to biases in their dataset, leading to issues such as toxicity. While domain-adaptive training has been employed to mitigate these issues, these techniques often address all model parameters indiscriminately during the repair process, resulting in poor repair quality and reduced model versatility. In this paper, we introduce a novel dynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach selectively targets the most error-prone sections of the model for repair. Specifically, we propose dynamically slicing the model's most sensitive layers that require immediate attention, concentrating repair efforts on those areas. This method enables more effective repairs with potentially less impact on the model's overall performance by altering a smaller portion of the model. We evaluated our technique on three models from the GPT2 and GPT-Neo families, with parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our results show that IRepair repairs errors 43.6% more effectively while causing 46% less disruption to general performance compared to the closest baseline, direct preference optimization. Our empirical analysis also reveals that errors are more concentrated in a smaller section of the model, with the top 20% of layers exhibiting 773% more error density than the remaining 80\\%. This highlights the need for selective repair. Additionally, we demonstrate that a dynamic selection approach is essential for addressing errors dispersed throughout the model, ensuring a robust and efficient repair.", 'abstract_zh': '基于动态切片的意图感知大型语言模型修复策略：IRepair', 'title_zh': 'IRepair: 一种基于意图的数据驱动错误修复方法在大规模语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.07049', 'title': 'Large Language Models in Software Security: A Survey of Vulnerability Detection Techniques and Insights', 'authors': 'Ze Sheng, Zhicheng Chen, Shuning Gu, Heqing Huang, Guofei Gu, Jeff Huang', 'link': 'https://arxiv.org/abs/2502.07049', 'abstract': 'Large Language Models (LLMs) are emerging as transformative tools for software vulnerability detection, addressing critical challenges in the security domain. Traditional methods, such as static and dynamic analysis, often falter due to inefficiencies, high false positive rates, and the growing complexity of modern software systems. By leveraging their ability to analyze code structures, identify patterns, and generate repair sugges- tions, LLMs, exemplified by models like GPT, BERT, and CodeBERT, present a novel and scalable approach to mitigating vulnerabilities. This paper provides a detailed survey of LLMs in vulnerability detection. It examines key aspects, including model architectures, application methods, target languages, fine-tuning strategies, datasets, and evaluation metrics. We also analyze the scope of current research problems, highlighting the strengths and weaknesses of existing approaches. Further, we address challenges such as cross-language vulnerability detection, multimodal data integration, and repository-level analysis. Based on these findings, we propose solutions for issues like dataset scalability, model interpretability, and applications in low-resource scenarios. Our contributions are threefold: (1) a systematic review of how LLMs are applied in vulnerability detection; (2) an analysis of shared patterns and differences across studies, with a unified framework for understanding the field; and (3) a summary of key challenges and future research directions. This work provides valuable insights for advancing LLM-based vulnerability detection. We also maintain and regularly update latest selected paper on this https URL', 'abstract_zh': '大型语言模型（LLMs）在软件漏洞检测领域的新兴应用及其在安全领域的突破，克服传统方法的不足，包括静态和动态分析的低效性、高误报率以及现代软件系统的复杂性。通过利用它们分析代码结构、识别模式和生成修复建议的能力，LLMs，如GPT、BERT和CodeBERT，提供了应对软件漏洞的一种新颖且可扩展的方法。本文详细总结了LLMs在漏洞检测中的应用，探讨了模型架构、应用方法、目标语言、微调策略、数据集和评估指标等方面的关键方面。我们还分析了当前研究中的问题范围，强调现有方法的优势和局限性。进一步，我们探讨了跨语言漏洞检测、多模态数据集成和仓库级分析等方面的挑战。基于这些发现，我们提出了提高数据集可扩展性、增强模型可解释性和在资源有限场景下应用解决方案。本文的贡献主要包括以下三个方面：（1）系统总结了LLMs在漏洞检测中的应用；（2）分析了各研究中的共性和差异，并提供了一个统一框架以理解该领域；（3）总结了关键挑战和未来的研究方向。本工作为基于LLM的漏洞检测技术的发展提供了宝贵的见解。我们也会不断更新在此处的最新精选文章：https://www.example.com', 'title_zh': '大型语言模型在软件安全中的应用：漏洞检测技术综述与见解'}
{'arxiv_id': 'arXiv:2502.07046', 'title': 'SnipGen: A Mining Repository Framework for Evaluating LLMs for Code', 'authors': 'Daniel Rodriguez-Cardenas, Alejandro Velasco, Denys Poshyvany', 'link': 'https://arxiv.org/abs/2502.07046', 'abstract': "Language Models (LLMs), such as transformer-based neural networks trained on billions of parameters, have become increasingly prevalent in software engineering (SE). These models, trained on extensive datasets that include code repositories, exhibit remarkable capabilities for SE tasks. However, evaluating their effectiveness poses significant challenges, primarily due to the potential overlap between the datasets used for training and those employed for evaluation. To address this issue, we introduce SnipGen, a comprehensive repository mining framework designed to leverage prompt engineering across various downstream tasks for code generation. SnipGen aims to mitigate data contamination by generating robust testbeds and crafting tailored data points to assist researchers and practitioners in evaluating LLMs for code-related tasks. In our exploratory study, SnipGen mined approximately 227K data points from 338K recent code changes in GitHub commits, focusing on method-level granularity. SnipGen features a collection of prompt templates that can be combined to create a Chain-of-Thought-like sequence of prompts, enabling a nuanced assessment of LLMs' code generation quality. By providing the mining tool, the methodology, and the dataset, SnipGen empowers researchers and practitioners to rigorously evaluate and interpret LLMs' performance in software engineering contexts.", 'abstract_zh': '语言模型（LLMs），如基于转换器的神经网络，已经广泛应用于软件工程（SE）。这些模型在包含代码仓库的大规模数据集上训练，显示出在SE任务中的非凡能力。然而，评估其有效性面临重大挑战，主要是由于训练数据集和评估数据集之间的潜在重叠。为了解决这一问题，我们引入了SnipGen，这是一种综合的代码库挖掘框架，旨在利用提示工程跨多种下游任务进行代码生成。SnipGen旨在通过生成稳健的测试环境和设计针对特定任务的数据点，来减轻数据污染，帮助研究人员和从业者评估LLMs在代码相关任务上的效果。在我们的探索性研究中，SnipGen从GitHub提交的338,000次最近代码更改中挖掘了约227,000个数据点，重点关注方法级别细粒度。SnipGen具有提示模板集合，可以组合生成类似于逐步推理序列的提示，便于对LLMs的代码生成质量进行细致评估。通过提供挖掘工具、方法论和数据集，SnipGen使研究人员和从业者能够在软件工程背景下严格评估和解读LLMs的表现。', 'title_zh': 'SnipGen: 一个代码生成大语言模型评估的挖掘仓库框架'}
{'arxiv_id': 'arXiv:2502.07045', 'title': 'Scalable and Ethical Insider Threat Detection through Data Synthesis and Analysis by LLMs', 'authors': 'Haywood Gelman, John D. Hastings', 'link': 'https://arxiv.org/abs/2502.07045', 'abstract': 'Insider threats wield an outsized influence on organizations, disproportionate to their small numbers. This is due to the internal access insiders have to systems, information, and infrastructure. %One example of this influence is where anonymous respondents submit web-based job search site reviews, an insider threat risk to organizations. Signals for such risks may be found in anonymous submissions to public web-based job search site reviews. This research studies the potential for large language models (LLMs) to analyze and detect insider threat sentiment within job site reviews. Addressing ethical data collection concerns, this research utilizes synthetic data generation using LLMs alongside existing job review datasets. A comparative analysis of sentiment scores generated by LLMs is benchmarked against expert human scoring. Findings reveal that LLMs demonstrate alignment with human evaluations in most cases, thus effectively identifying nuanced indicators of threat sentiment. The performance is lower on human-generated data than synthetic data, suggesting areas for improvement in evaluating real-world data. Text diversity analysis found differences between human-generated and LLM-generated datasets, with synthetic data exhibiting somewhat lower diversity. Overall, the results demonstrate the applicability of LLMs to insider threat detection, and a scalable solution for insider sentiment testing by overcoming ethical and logistical barriers tied to data acquisition.', 'abstract_zh': '内部威胁在组织中施加的影响与其较小的数量不成比例，这归因于他们对系统、信息和基础设施的内部访问权限。这种影响的一个例子是匿名受访者提交基于网络的求职网站评论，这是组织内的一个内部威胁风险。此类风险的信号可能会出现在公共求职网站评论中的匿名提交中。本研究探讨了大规模语言模型（LLMs）在分析和检测求职网站评论中的内部威胁情绪方面的潜力。为了解决伦理数据收集问题，本研究利用LLMs生成合成数据，并结合现有的求职评论数据集。通过将LLMs生成的情感分数与专家人工评分进行比较分析，本研究提供了一个基准。结果表明，LLMs在大多数情况下与人类评估保持一致，从而有效识别威胁情绪的细微指标。在人造数据上的表现低于合成数据，这表明在评估真实世界数据方面存在改进空间。文本多样性的分析发现，人造数据和人造生成的数据集之间存在差异，人造数据表现出较低的多样性。总体而言，结果表明LLMs在内部威胁检测中的适用性，并提供了一种克服与数据获取相关的伦理和物流障碍的可扩展解决方案，以测试内部情绪。', 'title_zh': '通过LLM数据合成与分析实现可扩展且负责任的内部威胁检测'}
{'arxiv_id': 'arXiv:2502.07036', 'title': 'Automated Consistency Analysis of LLMs', 'authors': 'Aditya Patwardhan, Vivek Vaidya, Ashish Kundu', 'link': 'https://arxiv.org/abs/2502.07036', 'abstract': 'Generative AI (Gen AI) with large language models (LLMs) are being widely adopted across the industry, academia and government. Cybersecurity is one of the key sectors where LLMs can be and/or are already being used. There are a number of problems that inhibit the adoption of trustworthy Gen AI and LLMs in cybersecurity and such other critical areas. One of the key challenge to the trustworthiness and reliability of LLMs is: how consistent an LLM is in its responses?\nIn this paper, we have analyzed and developed a formal definition of consistency of responses of LLMs. We have formally defined what is consistency of responses and then develop a framework for consistency evaluation. The paper proposes two approaches to validate consistency: self-validation, and validation across multiple LLMs. We have carried out extensive experiments for several LLMs such as GPT4oMini, GPT3.5, Gemini, Cohere, and Llama3, on a security benchmark consisting of several cybersecurity questions: informational and situational. Our experiments corroborate the fact that even though these LLMs are being considered and/or already being used for several cybersecurity tasks today, they are often inconsistent in their responses, and thus are untrustworthy and unreliable for cybersecurity.', 'abstract_zh': '基于大型语言模型的生成型AI在网络安全中的响应一致性分析与评估', 'title_zh': '自动一致性分析 of LLMs'}
{'arxiv_id': 'arXiv:2502.07022', 'title': 'AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in Corporate Statements', 'authors': 'Adriana Eufrosiana Bora, Pierre-Luc St-Charles, Mirko Bronzi, Arsène Fansi Tchango, Bruno Rousseau, Kerrie Mengersen', 'link': 'https://arxiv.org/abs/2502.07022', 'abstract': "Despite over a decade of legislative efforts to address modern slavery in the supply chains of large corporations, the effectiveness of government oversight remains hampered by the challenge of scrutinizing thousands of statements annually. While Large Language Models (LLMs) can be considered a well established solution for the automatic analysis and summarization of documents, recognizing concrete modern slavery countermeasures taken by companies and differentiating those from vague claims remains a challenging task. To help evaluate and fine-tune LLMs for the assessment of corporate statements, we introduce a dataset composed of 5,731 modern slavery statements taken from the Australian Modern Slavery Register and annotated at the sentence level. This paper details the construction steps for the dataset that include the careful design of annotation specifications, the selection and preprocessing of statements, and the creation of high-quality annotation subsets for effective model evaluations. To demonstrate our dataset's utility, we propose a machine learning methodology for the detection of sentences relevant to mandatory reporting requirements set by the Australian Modern Slavery Act. We then follow this methodology to benchmark modern language models under zero-shot and supervised learning settings.", 'abstract_zh': '尽管在过去十年中，立法努力已经致力于解决大型企业供应链中的现代奴隶制问题，但政府监督的有效性仍然受到每年审查数千份声明的挑战。虽然大型语言模型（LLMs）可以被视为自动分析和总结文档的一个成熟解决方案，但在识别公司实际采取的现代奴隶制防范措施并将其与模糊声明区分开来方面仍面临挑战。为了帮助评估和微调LLMs以评估企业声明，我们引入了一个数据集，该数据集包含从澳大利亚现代奴隶制登记册中提取的5,731份现代奴隶制声明，并在句子级别进行了标注。本文详细介绍了数据集的构建步骤，包括精心设计的注释规范、声明的选择和预处理以及用于有效模型评估的高质量注释子集的创建。为展示数据集的实用性，我们提出了一种机器学习方法，用于检测与澳大利亚现代奴隶制法案规定的强制报告要求相关的句子。然后，我们按照该方法在零样本和有监督学习设置下对现代语言模型进行了基准测试。', 'title_zh': 'AIMS.au: 企业声明中现代奴隶制对策分析的数据集'}
{'arxiv_id': 'arXiv:2502.07017', 'title': 'Finding Words Associated with DIF: Predicting Differential Item Functioning using LLMs and Explainable AI', 'authors': 'Hotaka Maeda, Yikai Lu', 'link': 'https://arxiv.org/abs/2502.07017', 'abstract': 'We fine-tuned and compared several encoder-based Transformer large language models (LLM) to predict differential item functioning (DIF) from the item text. We then applied explainable artificial intelligence (XAI) methods to these models to identify specific words associated with DIF. The data included 42,180 items designed for English language arts and mathematics summative state assessments among students in grades 3 to 11. Prediction $R^2$ ranged from .04 to .32 among eight focal and reference group pairs. Our findings suggest that many words associated with DIF reflect minor sub-domains included in the test blueprint by design, rather than construct-irrelevant item content that should be removed from assessments. This may explain why qualitative reviews of DIF items often yield confusing or inconclusive results. Our approach can be used to screen words associated with DIF during the item-writing process for immediate revision, or help review traditional DIF analysis results by highlighting key words in the text. Extensions of this research can enhance the fairness of assessment programs, especially those that lack resources to build high-quality items, and among smaller subpopulations where we do not have sufficient sample sizes for traditional DIF analyses.', 'abstract_zh': '我们对几种基于编码器的Transformer大型语言模型进行了微调和比较，以预测项目功能差异(DIF)。然后，我们应用可解释的人工智能(XAI)方法来识别与DIF相关的特定词语。数据包括供3至11年级学生进行英语语言艺术和数学总结性州评估的42,180个项目。预测$R^2$值在八个核心组和参照组对之间从0.04到0.32不等。我们的研究发现表明，与DIF相关的许多词语反映了测试蓝图设计中包含的较小子领域，而不是与构建不相关的项目内容，这些内容应从评估中删除。这可能解释了为什么对DIF项目的定性审查往往导致困惑或无法得出明确结果。我们的方法可以在项目编写过程中筛选与DIF相关的词语以进行即时修订，或通过突出显示文本中的关键词语来审查传统的DIF分析结果。这项研究的扩展可以增强评估计划的公平性，特别是对于那些缺乏资源建立高质量项目的计划，以及那些样本量不足以进行传统DIF分析的小子群体。', 'title_zh': '基于LLM和可解释人工智能预测项目功能差异的相关词汇查找'}
{'arxiv_id': 'arXiv:2502.06994', 'title': 'SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering', 'authors': 'Xuehang Guo, Xingyao Wang, Yangyi Chen, Sha Li, Chi Han, Manling Li, Heng Ji', 'link': 'https://arxiv.org/abs/2502.06994', 'abstract': "Software engineering (SE) is increasingly collaborative, with developers working together on shared complex codebases. Effective collaboration in shared environments requires participants -- whether humans or AI agents -- to stay on the same page as their environment evolves. When a collaborator's understanding diverges from the current state -- what we term the out-of-sync challenge -- the collaborator's actions may fail, leading to integration issues. In this work, we introduce SyncMind, a framework that systematically defines the out-of-sync problem faced by large language model (LLM) agents in collaborative software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE derived from 21 popular GitHub repositories with executable verification tests. Experiments on SyncBench uncover critical insights into existing LLM agents' capabilities and limitations. Besides substantial performance gaps among agents (from Llama-3.1 agent <= 3.33% to Claude-3.5-Sonnet >= 28.18%), their consistently low collaboration willingness (<= 4.86%) suggests fundamental limitations of existing LLM in CSE. However, when collaboration occurs, it positively correlates with out-of-sync recovery success. Minimal performance differences in agents' resource-aware out-of-sync recoveries further reveal their significant lack of resource awareness and adaptability, shedding light on future resource-efficient collaborative systems. Code and data are openly available on our project website: this https URL.", 'abstract_zh': '软件工程中的大语言模型代理同步问题及其解决框架：SyncMind与SyncBench探索', 'title_zh': 'SyncMind: 测量协作软件工程中代理脱节恢复能力'}
{'arxiv_id': 'arXiv:2502.06921', 'title': 'GraNNite: Enabling High-Performance Execution of Graph Neural Networks on Resource-Constrained Neural Processing Units', 'authors': 'Arghadip Das, Shamik Kundu, Arnab Raha, Soumendu Ghosh, Deepak Mathaikutty, Vijay Raghunathan', 'link': 'https://arxiv.org/abs/2502.06921', 'abstract': 'Graph Neural Networks (GNNs) are vital for learning from graph-structured data, enabling applications in network analysis, recommendation systems, and speech analytics. Deploying them on edge devices like client PCs and laptops enhances real-time processing, privacy, and cloud independence. GNNs aid Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and enable event-based vision tasks. However, irregular memory access, sparsity, and dynamic structures cause high latency and energy overhead on resource-constrained devices. While modern edge processors integrate CPUs, GPUs, and NPUs, NPUs designed for data-parallel tasks struggle with irregular GNN computations. We introduce GraNNite, the first hardware-aware framework optimizing GNN execution on commercial-off-the-shelf (COTS) SOTA DNN accelerators via a structured three-step methodology: (1) enabling NPU execution, (2) optimizing performance, and (3) trading accuracy for efficiency gains. Step 1 employs GraphSplit for workload distribution and StaGr for static aggregation, while GrAd and NodePad handle dynamic graphs. Step 2 boosts performance using EffOp for control-heavy tasks and GraSp for sparsity exploitation. Graph Convolution optimizations PreG, SymG, and CacheG reduce redundancy and memory transfers. Step 3 balances quality versus efficiency, where QuantGr applies INT8 quantization, and GrAx1, GrAx2, and GrAx3 accelerate attention, broadcast-add, and SAGE-max aggregation. On Intel Core Ultra AI PCs, GraNNite achieves 2.6X to 7.6X speedups over default NPU mappings and up to 8.6X energy gains over CPUs and GPUs, delivering 10.8X and 6.7X higher performance than CPUs and GPUs, respectively, across GNN models.', 'abstract_zh': '基于图形神经网络的硬件感知优化框架GraNNite', 'title_zh': 'GraNNite：在资源受限的神经处理单元上实现图神经网络高性能执行'}
{'arxiv_id': 'arXiv:2502.06918', 'title': 'Leveraging GPT-4o Efficiency for Detecting Rework Anomaly in Business Processes', 'authors': 'Mohammad Derakhshan, Paolo Ceravolo, Fatemeh Mohammadi', 'link': 'https://arxiv.org/abs/2502.06918', 'abstract': "This paper investigates the effectiveness of GPT-4o-2024-08-06, one of the Large Language Models (LLM) from OpenAI, in detecting business process anomalies, with a focus on rework anomalies. In our study, we developed a GPT-4o-based tool capable of transforming event logs into a structured format and identifying reworked activities within business event logs. The analysis was performed on a synthetic dataset designed to contain rework anomalies but free of loops. To evaluate the anomaly detection capabilities of GPT 4o-2024-08-06, we used three prompting techniques: zero-shot, one-shot, and few-shot. These techniques were tested on different anomaly distributions, namely normal, uniform, and exponential, to identify the most effective approach for each case. The results demonstrate the strong performance of GPT-4o-2024-08-06. On our dataset, the model achieved 96.14% accuracy with one-shot prompting for the normal distribution, 97.94% accuracy with few-shot prompting for the uniform distribution, and 74.21% accuracy with few-shot prompting for the exponential distribution. These results highlight the model's potential as a reliable tool for detecting rework anomalies in event logs and how anomaly distribution and prompting strategy influence the model's performance.", 'abstract_zh': '这篇论文探讨了OpenAI的大语言模型GPT-4o-2024-08-06在检测业务流程异常（重点关注返工异常）方面的有效性。在该项研究中，我们开发了一个基于GPT-4o的工具，能够将事件日志转化为结构化格式，并在业务事件日志中识别返工活动。分析是在一个包含返工异常但无循环的合成数据集上进行的。为了评估GPT-4o-2024-08-06的异常检测能力，我们使用了三种提示技术：零-shot、one-shot和few-shot。这些技术在不同的异常分布（分别是正常、均匀和指数分布）上进行了测试，以确定每种情况下最有效的策略。结果显示，GPT-4o-2024-08-06具有很强的性能。在我们的数据集上，使用one-shot提示时，正常分布的准确率为96.14%；使用few-shot提示时，均匀分布的准确率为97.94%；而当使用few-shot提示时，指数分布的准确率为74.21%。这些结果突显了该模型作为检测事件日志中返工异常的可靠工具的潜力，并显示了异常分布和提示策略如何影响模型的性能。', 'title_zh': '利用GPT-4o效率检测业务流程中的返工异常'}
{'arxiv_id': 'arXiv:2502.06916', 'title': 'Hyper Compressed Fine-Tuning of Large Foundation Models with Quantum Inspired Adapters', 'authors': 'Snehal Raj, Brian Coyle', 'link': 'https://arxiv.org/abs/2502.06916', 'abstract': 'Fine-tuning pre-trained large foundation models for specific tasks has become increasingly challenging due to the computational and storage demands associated with full parameter updates. Parameter-Efficient Fine-Tuning (PEFT) methods address this issue by updating only a small subset of model parameters using adapter modules. In this work, we propose \\emph{Quantum-Inspired Adapters}, a PEFT approach inspired by Hamming-weight preserving quantum circuits from quantum machine learning literature. These models can be both expressive and parameter-efficient by operating in a combinatorially large space while simultaneously preserving orthogonality in weight parameters. We test our proposed adapters by adapting large language models and large vision transformers on benchmark datasets. Our method can achieve 99.2\\% of the performance of existing fine-tuning methods such LoRA with a 44x parameter compression on language understanding datasets like GLUE and VTAB. Compared to existing orthogonal fine-tuning methods such as OFT or BOFT, we achieve 98\\% relative performance with 25x fewer parameters. This demonstrates competitive performance paired with a significant reduction in trainable parameters. Through ablation studies, we determine that combining multiple Hamming-weight orders with orthogonality and matrix compounding are essential for performant fine-tuning. Our findings suggest that Quantum-Inspired Adapters offer a promising direction for efficient adaptation of language and vision models in resource-constrained environments.', 'abstract_zh': '基于量子启发的参数高效微调方法：Hamming权重保持适配器在特定任务中的高效微调', 'title_zh': '基于量子启发适配器的超压缩大型基础模型细调'}
{'arxiv_id': 'arXiv:2502.06907', 'title': "Can ChatGPT Diagnose Alzheimer's Disease?", 'authors': 'Quoc-Toan Nguyen, Linh Le, Xuan-The Tran, Thomas Do, Chin-Teng Lin', 'link': 'https://arxiv.org/abs/2502.06907', 'abstract': "Can ChatGPT diagnose Alzheimer's Disease (AD)? AD is a devastating neurodegenerative condition that affects approximately 1 in 9 individuals aged 65 and older, profoundly impairing memory and cognitive function. This paper utilises 9300 electronic health records (EHRs) with data from Magnetic Resonance Imaging (MRI) and cognitive tests to address an intriguing question: As a general-purpose task solver, can ChatGPT accurately detect AD using EHRs? We present an in-depth evaluation of ChatGPT using a black-box approach with zero-shot and multi-shot methods. This study unlocks ChatGPT's capability to analyse MRI and cognitive test results, as well as its potential as a diagnostic tool for AD. By automating aspects of the diagnostic process, this research opens a transformative approach for the healthcare system, particularly in addressing disparities in resource-limited regions where AD specialists are scarce. Hence, it offers a foundation for a promising method for early detection, supporting individuals with timely interventions, which is paramount for Quality of Life (QoL).", 'abstract_zh': 'ChatGPT能诊断阿尔茨海默病（AD）吗？AD是一种严重影响记忆和认知功能的毁灭性神经退行性疾病，约每9名65岁及以上的人中就有1人患病。本文利用包含磁共振成像（MRI）和认知测试数据的9300份电子健康记录（EHRs），探讨了一个有趣的问题：作为通用任务解决者，ChatGPT能否准确地利用EHRs检测AD？我们采用黑盒方法，通过零样本和多样本方法对ChatGPT进行了深入评估。本研究揭示了ChatGPT分析MRI和认知测试结果的能力，并探讨了其作为AD诊断工具的潜力。通过自动化诊断过程的某些方面，该项研究为医疗保健系统开启了一种变革性的方法，特别是在资源受限区域AD专家稀缺的情况下，它为早期检测提供了一种有希望的方法，支持及时干预，这对于提高生活质量（QoL）至关重要。', 'title_zh': 'ChatGPT能诊断阿尔茨海默病吗？'}
{'arxiv_id': 'arXiv:2502.06901', 'title': 'Enabling Autoregressive Models to Fill In Masked Tokens', 'authors': 'Daniel Israel, Aditya Grover, Guy Van den Broeck', 'link': 'https://arxiv.org/abs/2502.06901', 'abstract': 'Historically, LLMs have been trained using either autoregressive (AR) or masked language modeling (MLM) objectives, with AR models gaining dominance in recent years. However, AR models are inherently incapable of masked infilling, which is the ability to predict masked tokens between past and future context. In contrast, MLM models suffer from intrinsic computational inefficiencies during both training and inference that hinder their scalability. This work introduces MARIA (Masked and Autoregressive Infilling Architecture), a novel approach that leverages the strengths of both paradigms to achieve state-of-the-art masked infilling performance. MARIA combines a pre-trained MLM and AR model by training a linear decoder that takes their concatenated hidden states as input. This minimal modification enables the AR model to perform infilling while retaining its inherent advantages in terms of faster inference with KV caching. Our results demonstrate that MARIA significantly outperforms existing methods, namely discrete diffusion models, on masked infilling tasks.', 'abstract_zh': '历史上传统上，大型语言模型（LLM）要么使用自回归（AR）目标，要么使用掩蔽语言模型（MLM）目标，近期自回归模型在其中占据了主导地位。然而，自回归模型本质上无法进行掩蔽填充，即预测过去和未来上下文之间的掩蔽词。相比之下，掩蔽语言模型在训练和推理过程中固有的计算效率低问题阻碍了其扩展性。本文提出了MARIA（Masked and Autoregressive Infilling Architecture）架构，这是一种结合了两种范式优势的新方法，以实现最先进的掩蔽填充性能。MARIA通过训练一个线性解码器来结合预训练的MLM和AR模型，该解码器将它们的拼接隐藏状态作为输入。这一最小的修改使AR模型能够进行填充操作，同时保留其固有的基于KV缓存的更快推理优势。我们的实验结果表明，MARIA在掩蔽填充任务中显著优于现有方法，特别是离散扩散模型。', 'title_zh': '使自回归模型填充掩蔽令牌'}
{'arxiv_id': 'arXiv:2502.06898', 'title': 'Large Language Models for In-File Vulnerability Localization Can Be "Lost in the End"', 'authors': 'Francesco Sovrano, Adam Bauer, Alberto Bacchelli', 'link': 'https://arxiv.org/abs/2502.06898', 'abstract': "Recent advancements in artificial intelligence have enabled processing of larger inputs, leading everyday software developers to increasingly rely on chat-based large language models (LLMs) like GPT-3.5 and GPT-4 to detect vulnerabilities across entire files, not just within functions. This new development practice requires researchers to urgently investigate whether commonly used LLMs can effectively analyze large file-sized inputs, in order to provide timely insights for software developers and engineers about the pros and cons of this emerging technological trend. Hence, the goal of this paper is to evaluate the effectiveness of several state-of-the-art chat-based LLMs, including the GPT models, in detecting in-file vulnerabilities. We conducted a costly investigation into how the performance of LLMs varies based on vulnerability type, input size, and vulnerability location within the file. To give enough statistical power to our study, we could only focus on the three most common (as well as dangerous) vulnerabilities: XSS, SQL injection, and path traversal. Our findings indicate that the effectiveness of LLMs in detecting these vulnerabilities is strongly influenced by both the location of the vulnerability and the overall size of the input. Specifically, regardless of the vulnerability type, LLMs tend to significantly (p < .05) underperform when detecting vulnerabilities located toward the end of larger files, a pattern we call the 'lost-in-the-end' effect. Finally, to further support software developers and practitioners, we also explored the optimal input size for these LLMs and presented a simple strategy for identifying it, which can be applied to other models and vulnerability types. Eventually, we show how adjusting the input size can lead to significant improvements in LLM-based vulnerability detection, with an average recall increase of over 37% across all models.", 'abstract_zh': '近年来，人工智能的最新进展使得处理更大输入成为可能，促使日常软件开发者越来越多地依赖基于聊天的大型语言模型（LLMs），如GPT-3.5和GPT-4来检测整个文件中的漏洞，而不仅仅是函数内的漏洞。这一新的开发实践要求研究人员迫切调查常用LLMs能否有效分析大型文件输入，以及时为软件开发者和工程师提供关于这一新兴技术趋势的优缺点见解。因此，本文旨在评估几种最先进的基于聊天的LLMs（包括GPT模型）检测文件内漏洞的有效性。我们进行了成本高昂的研究，探讨了LLMs的性能如何受漏洞类型、输入大小和文件内漏洞位置的影响。为了确保研究有足够的统计效力，我们只能关注三种最常见的（也是最危险的）漏洞：XSS、SQL注入和路径遍历。我们的研究结果表明，LLMs在检测这些漏洞方面的有效性强烈受漏洞位置和输入总体大小的影响。具体而言，无论漏洞类型如何，当检测位于较大文件末尾的漏洞时，LLMs往往会显著（p < .05）表现不佳，我们称之为“末尾迷失”效应。最后，为了进一步支持软件开发者和实践者，我们还探讨了这些LLMs的最佳输入大小，并提出了一个简单策略来确定它，该策略可应用于其他模型和漏洞类型。最终，我们展示了调整输入大小如何在LLM基漏洞检测中带来显著改进，所有模型的平均召回率平均增加了超过37%。', 'title_zh': '大型语言模型在文件中定位漏洞可能会“迷失在结尾处”'}
{'arxiv_id': 'arXiv:2502.06892', 'title': 'Certifying Language Model Robustness with Fuzzed Randomized Smoothing: An Efficient Defense Against Backdoor Attacks', 'authors': 'Bowei He, Lihao Yin, Hui-Ling Zhen, Jianping Zhang, Lanqing Hong, Mingxuan Yuan, Chen Ma', 'link': 'https://arxiv.org/abs/2502.06892', 'abstract': "The widespread deployment of pre-trained language models (PLMs) has exposed them to textual backdoor attacks, particularly those planted during the pre-training stage. These attacks pose significant risks to high-reliability applications, as they can stealthily affect multiple downstream tasks. While certifying robustness against such threats is crucial, existing defenses struggle with the high-dimensional, interdependent nature of textual data and the lack of access to original poisoned pre-training data. To address these challenges, we introduce \\textbf{F}uzzed \\textbf{R}andomized \\textbf{S}moothing (\\textbf{FRS}), a novel approach for efficiently certifying language model robustness against backdoor attacks. FRS integrates software robustness certification techniques with biphased model parameter smoothing, employing Monte Carlo tree search for proactive fuzzing to identify vulnerable textual segments within the Damerau-Levenshtein space. This allows for targeted and efficient text randomization, while eliminating the need for access to poisoned training data during model smoothing. Our theoretical analysis demonstrates that FRS achieves a broader certified robustness radius compared to existing methods. Extensive experiments across various datasets, model configurations, and attack strategies validate FRS's superiority in terms of defense efficiency, accuracy, and robustness.", 'abstract_zh': '预训练语言模型广泛部署后面临文本后门攻击的挑战，特别是在预训练阶段植入的攻击。这些攻击对高可靠性应用构成重大风险，因为它们能在不被察觉的情况下影响多个下游任务。虽然对抗这类威胁的鲁棒性验证至关重要，但现有防御措施难以处理文本数据的高维性和相互依赖性，以及无法获取原始受污染的预训练数据。为应对这些挑战，我们提出了 fuzzed randomized smoothing (FRS)——一种用于高效验证语言模型在后门攻击下的鲁棒性的新颖方法。FRS 融合了软件鲁棒性验证技术与双阶段模型参数平滑，使用蒙特卡洛树搜索进行前瞻性的模糊测试，以在 Damerau-Levenshtein 空间内识别易受攻击的文本片段。这使得文本随机化更具有针对性和效率，同时在模型平滑过程中无需访问受污染的训练数据。理论分析表明，FRS 在鲁棒性验证范围上优于现有方法。广泛的实验结果验证了 FRS 在防御效率、准确性和鲁棒性方面的优越性。', 'title_zh': '使用 fuzzed 随机化平滑认证语言模型健壮性：一种有效对抗后门攻击的方法'}
{'arxiv_id': 'arXiv:2502.06890', 'title': 'LLMs for Drug-Drug Interaction Prediction: A Comprehensive Comparison', 'authors': 'Gabriele De Vito, Filomena Ferrucci, Athanasios Angelakis', 'link': 'https://arxiv.org/abs/2502.06890', 'abstract': "The increasing volume of drug combinations in modern therapeutic regimens needs reliable methods for predicting drug-drug interactions (DDIs). While Large Language Models (LLMs) have revolutionized various domains, their potential in pharmaceutical research, particularly in DDI prediction, remains largely unexplored. This study thoroughly investigates LLMs' capabilities in predicting DDIs by uniquely processing molecular structures (SMILES), target organisms, and gene interaction data as raw text input from the latest DrugBank dataset. We evaluated 18 different LLMs, including proprietary models (GPT-4, Claude, Gemini) and open-source variants (from 1.5B to 72B parameters), first assessing their zero-shot capabilities in DDI prediction. We then fine-tuned selected models (GPT-4, Phi-3.5 2.7B, Qwen-2.5 3B, Gemma-2 9B, and Deepseek R1 distilled Qwen 1.5B) to optimize their performance. Our comprehensive evaluation framework included validation across 13 external DDI datasets, comparing against traditional approaches such as l2-regularized logistic regression. Fine-tuned LLMs demonstrated superior performance, with Phi-3.5 2.7B achieving a sensitivity of 0.978 in DDI prediction, with an accuracy of 0.919 on balanced datasets (50% positive, 50% negative cases). This result represents an improvement over both zero-shot predictions and state-of-the-art machine-learning methods used for DDI prediction. Our analysis reveals that LLMs can effectively capture complex molecular interaction patterns and cases where drug pairs target common genes, making them valuable tools for practical applications in pharmaceutical research and clinical settings.", 'abstract_zh': '现代治疗方案中药物组合不断增加，需要可靠的方法来预测药物-药物相互作用（DDIs）。虽然大型语言模型（LLMs）已颠覆多个领域，但在制药研究，特别是在DDI预测方面的潜力尚未得到充分探索。本研究通过独特处理分子结构（SMILES）、目标生物体和基因交互数据作为最新DrugBank数据集的原始文本输入，全面考察了LLMs在预测DDIs方面的能力。我们评估了18种不同的LLMs，包括专有模型（GPT-4、Claude、Gemini）和开源变体（参数量从1.5B到72B），首先评估了它们在DDI预测方面的零样本能力。然后，我们对选定的模型（GPT-4、Phi-3.5 2.7B、Qwen-2.5 3B、Gemma-2 9B和Deepseek R1精简版Qwen 1.5B）进行微调以优化其性能。我们的综合评估框架包括在13个外部DDI数据集上的验证，与传统的似然回归等传统方法进行比较。微调后的LLMs表现出色，Phi-3.5 2.7B在DDI预测中的敏感性为0.978，在平衡数据集（50%阳性，50%阴性案例）上的准确率为0.919。这一结果优于零样本预测和用于DDI预测的最先进的机器学习方法。我们的分析表明，LLMs能够有效捕捉复杂的分子相互作用模式，以及药物对共同靶向基因的情况，使其成为制药研究和临床环境中实际应用的有价值的工具。', 'title_zh': '药物-药物相互作用预测中的大规模语言模型：一项综合比较'}
{'arxiv_id': 'arXiv:2502.06884', 'title': 'Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models', 'authors': 'Sina Tayebati, Divake Kumar, Nastaran Darabi, Dinithi Jayasuriya, Ranganath Krishnan, Amit Ranjan Trivedi', 'link': 'https://arxiv.org/abs/2502.06884', 'abstract': 'Large Language and Vision-Language Models (LLMs/VLMs) are increasingly used in safety-critical applications, yet their opaque decision-making complicates risk assessment and reliability. Uncertainty quantification (UQ) helps assess prediction confidence and enables abstention when uncertainty is high. Conformal prediction (CP), a leading UQ method, provides statistical guarantees but relies on static thresholds, which fail to adapt to task complexity and evolving data distributions, leading to suboptimal trade-offs in accuracy, coverage, and informativeness. To address this, we propose learnable conformal abstention, integrating reinforcement learning (RL) with CP to optimize abstention thresholds dynamically. By treating CP thresholds as adaptive actions, our approach balances multiple objectives, minimizing prediction set size while maintaining reliable coverage. Extensive evaluations across diverse LLM/VLM benchmarks show our method outperforms Least Ambiguous Classifiers (LAC) and Adaptive Prediction Sets (APS), improving accuracy by up to 3.2%, boosting AUROC for hallucination detection by 22.19%, enhancing uncertainty-guided selective generation (AUARC) by 21.17%, and reducing calibration error by 70%-85%. These improvements hold across multiple models and datasets while consistently meeting the 90% coverage target, establishing our approach as a more effective and flexible solution for reliable decision-making in safety-critical applications. The code is available at: {this https URL}.', 'abstract_zh': '大型语言模型和多模态语言-视觉模型（LLMs/VLMs）在安全关键应用中的使用越来越普遍，但其不透明的决策机制复杂化了风险评估和可靠性分析。不确定性量化（UQ）有助于评估预测的置信度，并在不确定性高时允许规避决策。校准预测（CP），作为领先的UQ方法，提供了统计保证，但依赖于静态阈值，无法适应任务复杂性和数据分布的变化，导致准确度、覆盖率和信息量之间的次优权衡。为了解决这一问题，我们提出了可学习的校准规避，将强化学习（RL）与CP结合，以动态优化规避阈值。通过将CP阈值视为可适应的动作，我们的方法在满足可靠覆盖率的同时，最小化预测集合的大小。在多个LLM/VLM基准测试中的广泛评估表明，我们的方法优于最少含糊分类器（LAC）和自适应预测集合（APS），在准确度上提高了最多3.2%，在幻觉检测的AUROC上提高了22.19%，在不确定性引导的选择性生成（AUARC）上提高了21.17%，并且将校准误差降低了70%-85%。这些改进在多个模型和数据集上保持一致，并始终满足90%的覆盖率目标，确立了我们的方法在安全关键应用中实现可靠决策的有效性和灵活性。代码可在以下链接获取：{this https URL}。', 'title_zh': '学习符合性放弃策略以实现大型语言和多模态模型中的自适应风险管理'}
{'arxiv_id': 'arXiv:2502.06882', 'title': 'Multi-Agent Simulator Drives Language Models for Legal Intensive Interaction', 'authors': 'Shengbin Yue, Ting Huang, Zheng Jia, Siyuan Wang, Shujun Liu, Yun Song, Xuanjing Huang, Zhongyu Wei', 'link': 'https://arxiv.org/abs/2502.06882', 'abstract': "Large Language Models (LLMs) have significantly advanced legal intelligence, but the scarcity of scenario data impedes the progress toward interactive legal scenarios. This paper introduces a Multi-agent Legal Simulation Driver (MASER) to scalably generate synthetic data by simulating interactive legal scenarios. Leveraging real-legal case sources, MASER ensures the consistency of legal attributes between participants and introduces a supervisory mechanism to align participants' characters and behaviors as well as addressing distractions. A Multi-stage Interactive Legal Evaluation (MILE) benchmark is further constructed to evaluate LLMs' performance in dynamic legal scenarios. Extensive experiments confirm the effectiveness of our framework.", 'abstract_zh': '大规模语言模型（LLMs）显著推动了法律智能的发展，但场景数据的稀缺阻碍了交互式法律场景的进展。本文介绍了一种多-agent 法律模拟驱动器（MASER），以通过模拟交互式法律场景来scalably生成合成数据。利用真实的法律案例源，MASER 确保了参与者之间法律属性的一致性，并引入了一种监督机制来对齐参与者的角色和行为，并解决干扰问题。进一步构建了一个多阶段交互式法律评估（MILE）基准，以评估LLMs在动态法律场景中的性能。广泛的实验验证了我们框架的有效性。', 'title_zh': '多智能体模拟器驱动语言模型进行法律密集型交互'}
{'arxiv_id': 'arXiv:2502.06876', 'title': 'Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging', 'authors': 'Jinluan Yang, Dingnan Jin, Anke Tang, Li Shen, Didi Zhu, Zhengyu Chen, Daixin Wang, Qing Cui, Zhiqiang Zhang, Jun Zhou, Fei Wu, Kun Kuang', 'link': 'https://arxiv.org/abs/2502.06876', 'abstract': 'Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI, with existing methods like data mixture strategies facing limitations including reliance on expert knowledge and conflicting optimization signals. While model merging offers a promising alternative by integrating specialized models, its potential for 3H optimization remains underexplored. This paper establishes the first comprehensive benchmark for model merging in 3H-aligned LLMs, systematically evaluating 15 methods (12 training-free merging and 3 data mixture techniques) across 10 datasets associated with 5 annotation dimensions, 2 LLM families, and 2 training paradigms. Our analysis reveals three pivotal insights: (i) previously overlooked collaborative/conflicting relationships among 3H dimensions, (ii) the consistent superiority of model merging over data mixture approaches in balancing alignment trade-offs, and (iii) the critical role of parameter-level conflict resolution through redundant component pruning and outlier mitigation. Building on these findings, we propose R-TSVM, a Reweighting-enhanced Task Singular Vector Merging method that incorporates outlier-aware parameter weighting and sparsity-adaptive rank selection strategies adapted to the heavy-tailed parameter distribution and sparsity for LLMs, further improving LLM alignment across multiple evaluations. Our models will be available at this https URL.', 'abstract_zh': '实现大型语言模型（LLMs）在帮助性、诚信性和无害性（3H优化）方面的平衡对齐构成了负责任AI的基石，现有方法如数据混合策略存在依赖专家知识和优化信号冲突等局限性。通过集成专门模型的模型合并提供了一种有前景的替代方案，但其在3H优化方面的潜力尚未得到充分探索。本文建立了第一个针对3H对齐LLMs的全面模型合并基准，系统性评估了15种方法（12种不依赖训练的合并方法和3种数据混合技术）在与5个注释维度、2个LLM系列和2个训练范式相关的10个数据集上的表现。我们的分析揭示了三个关键洞察：（i）3H维度之间之前未被重视的合作/冲突关系；（ii）模型合并方法在平衡对齐权衡方面始终优于数据混合方法；（iii）参数级别冲突解决的關鍵作用，通过冗余组件剪枝和异常值缓解策略。基于以上发现，我们提出了R-TSVM，这是一种增强的重新加权任务奇异向量合并方法，结合了异常值感知参数加权和针对LLM的重尾参数分布和稀疏性的自适应秩选择策略，进一步提高LLM对齐性。我们的模型将在该网址提供：https://。', 'title_zh': 'Mix数据还是合并模型？通过模型合并平衡大型语言模型的有用性、诚实性和无害性'}
{'arxiv_id': 'arXiv:2502.06875', 'title': 'Beyond Vision: How Large Language Models Interpret Facial Expressions from Valence-Arousal Values', 'authors': 'Vaibhav Mehra, Guy Laban, Hatice Gunes', 'link': 'https://arxiv.org/abs/2502.06875', 'abstract': 'Large Language Models primarily operate through text-based inputs and outputs, yet human emotion is communicated through both verbal and non-verbal cues, including facial expressions. While Vision-Language Models analyze facial expressions from images, they are resource-intensive and may depend more on linguistic priors than visual understanding. To address this, this study investigates whether LLMs can infer affective meaning from dimensions of facial expressions-Valence and Arousal values, structured numerical representations, rather than using raw visual input. VA values were extracted using Facechannel from images of facial expressions and provided to LLMs in two tasks: (1) categorizing facial expressions into basic (on the IIMI dataset) and complex emotions (on the Emotic dataset) and (2) generating semantic descriptions of facial expressions (on the Emotic dataset). Results from the categorization task indicate that LLMs struggle to classify VA values into discrete emotion categories, particularly for emotions beyond basic polarities (e.g., happiness, sadness). However, in the semantic description task, LLMs produced textual descriptions that align closely with human-generated interpretations, demonstrating a stronger capacity for free text affective inference of facial expressions.', 'abstract_zh': '大型语言模型主要通过文本形式的输入和输出进行操作，然而人类情感的传达既包括口头也包括非口头线索，如面部表情。尽管视觉语言模型能够分析图像中的面部表情，但它们资源消耗大，并且可能更加依赖于语言先验而非视觉理解。为解决这一问题，本研究探讨了大型语言模型是否能够从面部表情维度（愉悦度和唤醒度）的结构化数值表示中推断情感意义，而不是直接使用原始视觉输入。愉悦度和唤醒度值使用Facechannel从面部表情图像中提取，并在两类任务中提供给大型语言模型：（1）在IIMI数据集中将面部表情分类为基本情绪，在Emotic数据集中分类为复杂情绪；（2）在Emotic数据集中生成面部表情的语义描述。分类任务的结果表明，大型语言模型在将VA值分类为离散情绪类别方面存在困难，特别是在基本极性情绪之外的情感（如快乐、悲伤）分类上。然而，在语义描述任务中，大型语言模型生成的文本描述与人类生成的解释高度一致，展现了更强的情感推断能力。', 'title_zh': '超越视觉：大型语言模型如何通过唤起-唤醒值解释面部表情'}
{'arxiv_id': 'arXiv:2502.06874', 'title': 'Group Reasoning Emission Estimation Networks', 'authors': 'Yanming Guo, Xiao Qian, Kevin Credit, Jin Ma', 'link': 'https://arxiv.org/abs/2502.06874', 'abstract': 'Accurate greenhouse gas (GHG) emission reporting is critical for governments, businesses, and investors. However, adoption remains limited particularly among small and medium enterprises due to high implementation costs, fragmented emission factor databases, and a lack of robust sector classification methods. To address these challenges, we introduce Group Reasoning Emission Estimation Networks (GREEN), an AI-driven carbon accounting framework that standardizes enterprise-level emission estimation, constructs a large-scale benchmark dataset, and leverages a novel reasoning approach with large language models (LLMs). Specifically, we compile textual descriptions for 20,850 companies with validated North American Industry Classification System (NAICS) labels and align these with an economic model of carbon intensity factors. By reframing sector classification as an information retrieval task, we fine-tune Sentence-BERT models using a contrastive learning loss. To overcome the limitations of single-stage models in handling thousands of hierarchical categories, we propose a Group Reasoning method that ensembles LLM classifiers based on the natural NAICS ontology, decomposing the task into multiple sub-classification steps. We theoretically prove that this approach reduces classification uncertainty and computational complexity. Experiments on 1,114 NAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47% Top-10 accuracy), and case studies on 20 companies report a mean absolute percentage error (MAPE) of 45.88%. The project is available at: this https URL.', 'abstract_zh': '准确报告温室气体排放对于政府、企业和投资者至关重要。然而，由于实施成本高、排放因子数据库碎片化以及缺乏稳健的行业分类方法，中小企业采用仍受限。为应对这些挑战，我们引入了基于组推理的排放估计算法网络（GREEN），这是一种AI驱动的碳核算框架，用于标准化企业级排放估计算法，构建大规模基准数据集，并利用大型语言模型（LLMs）的新型推理方法。具体来说，我们为20,850家具有验证过的北美行业分类系统（NAICS）标签的公司编纂了文本描述，并将其与碳强度因素的经济模型对齐。通过将行业分类重新构想为信息检索任务，我们使用对比学习损失微调Sentence-BERT模型。为了解决单一模型在处理数千个层级类别时的局限性，我们提出了一种基于自然NAICS本体论的组推理方法，将任务分解为多个子分类步骤。我们理论证明了这种方法能降低分类不确定性并减少计算复杂性。在1,114个NAICS类别上的实验中取得了最先进的性能（Top-1准确率为83.68%，Top-10准确率为91.47%），20家公司的案例研究报告的平均绝对百分比误差（MAPE）为45.88%。该项目可在以下链接获取：this https URL。', 'title_zh': '群体推理排放估计网络'}
{'arxiv_id': 'arXiv:2502.06872', 'title': 'Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey', 'authors': 'Bo Ni, Zheyuan Liu, Leyao Wang, Yongjia Lei, Yuying Zhao, Xueqi Cheng, Qingkai Zeng, Luna Dong, Yinglong Xia, Krishnaram Kenthapadi, Ryan Rossi, Franck Dernoncourt, Md Mehrab Tanjim, Nesreen Ahmed, Xiaorui Liu, Wenqi Fan, Erik Blasch, Yu Wang, Meng Jiang, Tyler Derr', 'link': 'https://arxiv.org/abs/2502.06872', 'abstract': "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact.", 'abstract_zh': 'Retrieval-Augmented Generation系统的可信性增强：可靠性、隐私、安全性、公平性、可解释性和问责制综合框架', 'title_zh': '可信检索增强生成的大语言模型：一种综述'}
{'arxiv_id': 'arXiv:2502.06868', 'title': 'Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject', 'authors': 'Zenghao Duan, Wenbin Duan, Zhiyi Yin, Yinghan Shen, Shaoling Jing, Jie Zhang, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.06868', 'abstract': 'Knowledge editing has become a promising approach for efficiently and precisely updating knowledge embedded in large language models (LLMs). In this work, we focus on Same-Subject Editing, which involves modifying multiple attributes of a single entity to ensure comprehensive and consistent updates to entity-centric knowledge. Through preliminary observation, we identify a significant challenge: Current state-of-the-art editing methods struggle when tasked with editing multiple related knowledge pieces for the same subject. To address the lack of relevant editing data for identical subjects in traditional benchmarks, we introduce the $\\text{S}^2\\text{RKE}$(Same-Subject Related Knowledge Editing) benchmark. Our extensive experiments reveal that only mainstream locate-then-edit methods, such as ROME and MEMIT, exhibit "related knowledge perturbation," where subsequent edits interfere with earlier ones. Further analysis reveals that these methods over-rely on subject information, neglecting other critical factors, resulting in reduced editing effectiveness.', 'abstract_zh': 'Same-Subject Related Knowledge Editing', 'title_zh': '相关知识扰动 matters: 同主题多知识编辑再思考'}
{'arxiv_id': 'arXiv:2502.06867', 'title': 'Forbidden Science: Dual-Use AI Challenge Benchmark and Scientific Refusal Tests', 'authors': 'David Noever, Forrest McKee', 'link': 'https://arxiv.org/abs/2502.06867', 'abstract': "The development of robust safety benchmarks for large language models requires open, reproducible datasets that can measure both appropriate refusal of harmful content and potential over-restriction of legitimate scientific discourse. We present an open-source dataset and testing framework for evaluating LLM safety mechanisms across mainly controlled substance queries, analyzing four major models' responses to systematically varied prompts. Our results reveal distinct safety profiles: Claude-3.5-sonnet demonstrated the most conservative approach with 73% refusals and 27% allowances, while Mistral attempted to answer 100% of queries. GPT-3.5-turbo showed moderate restriction with 10% refusals and 90% allowances, and Grok-2 registered 20% refusals and 80% allowances. Testing prompt variation strategies revealed decreasing response consistency, from 85% with single prompts to 65% with five variations. This publicly available benchmark enables systematic evaluation of the critical balance between necessary safety restrictions and potential over-censorship of legitimate scientific inquiry, while providing a foundation for measuring progress in AI safety implementation. Chain-of-thought analysis reveals potential vulnerabilities in safety mechanisms, highlighting the complexity of implementing robust safeguards without unduly restricting desirable and valid scientific discourse.", 'abstract_zh': '大型语言模型稳健安全基准的发展需要开放可复现的数据集，以衡量适宜拒绝有害内容和潜在过度限制合法科学讨论的能力。我们提出了一个开源数据集和测试框架，用于评估主要涉及管控物质查询的LLM安全机制响应，分析四种主要模型对系统变化提示的响应。我们的结果揭示了不同的安全特性：Claude-3.5-sonnet展示了最为保守的方法，拒绝了73%的内容而允许了27%，而Mistral试图回答100%的查询。GPT-3.5-turbo展示了中等程度的限制，拒绝了10%的内容而允许了90%，Grok-2拒绝了20%的内容而允许了80%。测试提示变化策略揭示了响应一致性逐渐下降，从单个提示的85%下降到五个变化的65%。这个公开可用的基准提供了系统评估必要的安全限制与潜在过度审查合法科学探讨之间平衡的基础，同时也为测量AI安全实施的进步提供了基础。链式思考分析揭示了安全机制的潜在脆弱性，强调了在不过度限制可取和有效的科学讨论的情况下实施强大保障措施的复杂性。', 'title_zh': '禁止的科学：双重用途AI挑战基准和科学拒绝测试'}
{'arxiv_id': 'arXiv:2502.06864', 'title': 'Knowledge Graph-Guided Retrieval Augmented Generation', 'authors': 'Xiangrong Zhu, Yuexiang Xie, Yi Liu, Yaliang Li, Wei Hu', 'link': 'https://arxiv.org/abs/2502.06864', 'abstract': 'Retrieval-augmented generation (RAG) has emerged as a promising technology for addressing hallucination issues in the responses generated by large language models (LLMs). Existing studies on RAG primarily focus on applying semantic-based approaches to retrieve isolated relevant chunks, which ignore their intrinsic relationships. In this paper, we propose a novel Knowledge Graph-Guided Retrieval Augmented Generation (KG$^2$RAG) framework that utilizes knowledge graphs (KGs) to provide fact-level relationships between chunks, improving the diversity and coherence of the retrieved results. Specifically, after performing a semantic-based retrieval to provide seed chunks, KG$^2$RAG employs a KG-guided chunk expansion process and a KG-based chunk organization process to deliver relevant and important knowledge in well-organized paragraphs. Extensive experiments conducted on the HotpotQA dataset and its variants demonstrate the advantages of KG$^2$RAG compared to existing RAG-based approaches, in terms of both response quality and retrieval quality.', 'abstract_zh': 'Knowledge Graph-Guided Retrieval-Augmented Generation (KG²RAG): Enhancing Diversity and Coherence in Responses', 'title_zh': '知识图谱引导的检索增强生成'}
{'arxiv_id': 'arXiv:2502.06858', 'title': 'LLM-Supported Natural Language to Bash Translation', 'authors': "Finnian Westenfelder, Erik Hemberg, Miguel Tulla, Stephen Moskal, Una-May O'Reilly, Silviu Chiricescu", 'link': 'https://arxiv.org/abs/2502.06858', 'abstract': 'The Bourne-Again Shell (Bash) command-line interface for Linux systems has complex syntax and requires extensive specialized knowledge. Using the natural language to Bash command (NL2SH) translation capabilities of large language models (LLMs) for command composition circumvents these issues. However, the NL2SH performance of LLMs is difficult to assess due to inaccurate test data and unreliable heuristics for determining the functional equivalence of Bash commands. We present a manually verified test dataset of 600 instruction-command pairs and a training dataset of 40,939 pairs, increasing the size of previous datasets by 441% and 135%, respectively. Further, we present a novel functional equivalence heuristic that combines command execution with LLM evaluation of command outputs. Our heuristic can determine the functional equivalence of two Bash commands with 95% confidence, a 16% increase over previous heuristics. Evaluation of popular LLMs using our test dataset and heuristic demonstrates that parsing, in-context learning, in-weight learning, and constrained decoding can improve NL2SH accuracy by up to 32%. Our findings emphasize the importance of dataset quality, execution-based evaluation and translation method for advancing NL2SH translation. Our code is available at this https URL', 'abstract_zh': 'Linux系统中的Bourne-Again Shell (Bash) 命令行界面具有复杂的语法结构，需要广泛的专业知识。利用大规模语言模型（LLMs）的自然语言到Bash命令（NL2SH）转换能力进行命令组合可以规避这些问题。然而，LLMs的NL2SH性能难以评估，因为测试数据不准确且确定Bash命令功能等价性的启发式方法不可靠。我们提供了一个包含600个指令-命令对的手动验证测试数据集和一个包含40,939个对的训练数据集，分别将先前数据集的大小扩大了441%和135%。此外，我们提出了一个结合命令执行和LLM评估命令输出的功能等价性启发式方法。该启发式方法可以以95%的信心确定两个Bash命令的功能等价性，相比之前的启发式方法提高了16%。使用我们的测试数据集和启发式方法评估流行的LLMs表明，语法分析、上下文学习、加权学习和约束解码可以将NL2SH准确性提高多达32%。我们的研究强调了数据集质量、基于执行的评估和转换方法对于提高NL2SH翻译的重要性。我们的代码可在此处访问。', 'title_zh': 'LLM支持的自然语言到Bash脚本转换'}
{'arxiv_id': 'arXiv:2502.06855', 'title': 'Self-Supervised Prompt Optimization', 'authors': 'Jinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng, Jinhao Tu, Xinbing Liang, Sirui Hong, Chenglin Wu, Yuyu Luo', 'link': 'https://arxiv.org/abs/2502.06855', 'abstract': "Well-designed prompts are crucial for enhancing Large language models' (LLMs) reasoning capabilities while aligning their outputs with task requirements across diverse domains. However, manually designed prompts require expertise and iterative experimentation. While existing prompt optimization methods aim to automate this process, they rely heavily on external references such as ground truth or by humans, limiting their applicability in real-world scenarios where such data is unavailable or costly to obtain. To address this, we propose Self-Supervised Prompt Optimization (SPO), a cost-efficient framework that discovers effective prompts for both closed and open-ended tasks without requiring external reference. Motivated by the observations that prompt quality manifests directly in LLM outputs and LLMs can effectively assess adherence to task requirements, we derive evaluation and optimization signals purely from output comparisons. Specifically, SPO selects superior prompts through pairwise output comparisons evaluated by an LLM evaluator, followed by an LLM optimizer that aligns outputs with task requirements. Extensive experiments demonstrate that SPO outperforms state-of-the-art prompt optimization methods, achieving comparable or superior results with significantly lower costs (e.g., 1.1% to 5.6% of existing methods) and fewer samples (e.g., three samples). The code is available at this https URL.", 'abstract_zh': '自监督提示优化：一种无需外部参考的高效提示优化框架', 'title_zh': '自我监督提示优化'}
{'arxiv_id': 'arXiv:2502.06854', 'title': 'Can Large Language Models Understand Intermediate Representations?', 'authors': 'Hailong Jiang, Jianfeng Zhu, Yao Wan, Bo Fang, Hongyu Zhang, Ruoming Jin, Qiang Guan', 'link': 'https://arxiv.org/abs/2502.06854', 'abstract': 'Intermediate Representations (IRs) are essential in compiler design and program analysis, yet their comprehension by Large Language Models (LLMs) remains underexplored. This paper presents a pioneering empirical study to investigate the capabilities of LLMs, including GPT-4, GPT-3, Gemma 2, LLaMA 3.1, and Code Llama, in understanding IRs. We analyze their performance across four tasks: Control Flow Graph (CFG) reconstruction, decompilation, code summarization, and execution reasoning. Our results indicate that while LLMs demonstrate competence in parsing IR syntax and recognizing high-level structures, they struggle with control flow reasoning, execution semantics, and loop handling. Specifically, they often misinterpret branching instructions, omit critical IR operations, and rely on heuristic-based reasoning, leading to errors in CFG reconstruction, IR decompilation, and execution reasoning. The study underscores the necessity for IR-specific enhancements in LLMs, recommending fine-tuning on structured IR datasets and integration of explicit control flow models to augment their comprehension and handling of IR-related tasks.', 'abstract_zh': '大型语言模型在中间表示理解中的能力：一项开创性的实证研究', 'title_zh': '大型语言模型能够理解中间表示吗？'}
{'arxiv_id': 'arXiv:2502.06846', 'title': 'Prot2Chat: Protein LLM with Early Fusion of Sequence and Structure', 'authors': 'Zhicong Wang, Zicheng Ma, Ziqiang Cao, Changlong Zhou, Jun Zhang, Yiqin Gao', 'link': 'https://arxiv.org/abs/2502.06846', 'abstract': 'Proteins play a pivotal role in living organisms, yet understanding their functions presents significant challenges, including the limited flexibility of classification-based methods, the inability to effectively leverage spatial structural information, and the lack of systematic evaluation metrics for protein Q&A systems. To address these limitations, we propose Prot2Chat, a novel framework that integrates multimodal protein representations with natural language through a unified module, enabling large language model (LLM)-driven answer generation. Our model incorporates a modified ProteinMPNN encoder, which encodes protein sequence and structural information in a unified manner, a protein-text adapter with cross-attention mechanisms, and a LLaMA3 decoder. To optimize training efficiency, we freeze the encoder and employ LoRA techniques for the decoder. We conducted experiments on two datasets, both automated metrics and expert evaluations demonstrate the superior performance of our model. Furthermore, zero-shot prediction results highlight its strong generalization capabilities. This framework offers a promising solution for bridging protein domain knowledge with natural language understanding, paving the way for transformative advancements in protein-related research.', 'abstract_zh': '蛋白质在生物体中扮演着关键角色，然而理解其功能面临显著挑战，包括基于分类方法的有限灵活性、无法有效利用空间结构信息以及缺乏系统性的蛋白质问答评估指标。为了解决这些局限性，我们提出Prot2Chat，这是一种新颖的框架，通过统一模块将多模态蛋白质表示与自然语言相结合，实现大型语言模型（LLM）驱动的答案生成。该模型包含一个修改后的ProteinMPNN编码器，能够统一编码蛋白质序列和结构信息，一个蛋白质-文本适配器，配有跨注意力机制，以及一个LLaMA3解码器。为了优化训练效率，我们固定编码器，并对解码器使用LoRA技术。我们在两个数据集上进行了实验，自动评估指标和专家评价均显示了该模型的优越性能。此外，零-shot预测结果突显了其强大的泛化能力。该框架为桥梁蛋白质领域知识与自然语言理解提供了有前景的解决方案，为蛋白质相关研究的变革性进展铺平了道路。', 'title_zh': 'Prot2Chat: 基于序列和结构早期融合的蛋白质LLM'}
{'arxiv_id': 'arXiv:2502.06844', 'title': 'Exploring Model Invariance with Discrete Search for Ultra-Low-Bit Quantization', 'authors': 'Yuqiao Wen, Yanshuai Cao, Lili Mou', 'link': 'https://arxiv.org/abs/2502.06844', 'abstract': 'Large language models have been increasing in size due to their success in a wide range of applications. This calls for a pressing need to reduce memory usage to make them more accessible. Post-training quantization is a popular technique which uses fewer bits (e.g., 4--8 bits) to represent the model without retraining it. However, it remains a challenging task to perform quantization in an ultra-low-bit setup (e.g., 2 bits). In this paper, we propose InvarExplore, a unified framework that systematically explores different model invariance at the same time, allowing us to take advantage of the synergy between each type of invariance. Importantly, InvarExplore features a discrete search algorithm that enables us to explore permutation invariance, which is under-studied as it cannot be optimized with gradient-based methods. Results show that InvarExplore is compatible with existing state-of-the-art methods, achieving an add-on performance improvement over strong competing methods.', 'abstract_zh': '大规模语言模型由于在其广泛的应用中取得成功而变得越来越大，这迫切需要减少内存使用以提高其可访问性。后训练量化是一种常用的技术，它使用较少的位数（例如4-8位）来表示模型而不重新训练它。然而，在超低位数设置（例如2位）下进行量化仍然是一个具有挑战性的任务。在此论文中，我们提出了一种统一框架InvarExplore，该框架系统地探索不同的模型不变性，从而使我们能够利用每种不变性之间的协同作用。重要的是，InvarExplore具有一种离散搜索算法，使我们能够探索对基于梯度优化方法难以优化的排列不变性进行探索。结果表明，InvarExplore与现有最佳方法兼容，并且相对于强大的竞争对手方法实现了额外的性能提升。', 'title_zh': '探索超低比特量化中的模型不变性寻优方法'}
{'arxiv_id': 'arXiv:2502.06842', 'title': 'Integrating Generative Artificial Intelligence in ADRD: A Framework for Streamlining Diagnosis and Care in Neurodegenerative Diseases', 'authors': 'Andrew G. Breithaupt, Alice Tang, Bruce L. Miller, Pedro Pinheiro-Chagas', 'link': 'https://arxiv.org/abs/2502.06842', 'abstract': "Healthcare systems are struggling to meet the growing demand for neurological care, with challenges particularly acute in Alzheimer's disease and related dementias (ADRD). While artificial intelligence research has often focused on identifying patterns beyond human perception, implementing such predictive capabilities remains challenging as clinicians cannot readily verify insights they cannot themselves detect. We propose that large language models (LLMs) offer more immediately practical applications by enhancing clinicians' capabilities in three critical areas: comprehensive data collection, interpretation of complex clinical information, and timely application of relevant medical knowledge. These challenges stem from limited time for proper diagnosis, growing data complexity, and an overwhelming volume of medical literature that exceeds any clinician's capacity to fully master. We present a framework for responsible AI integration that leverages LLMs' ability to communicate effectively with both patients and providers while maintaining human oversight. This approach prioritizes standardized, high-quality data collection to enable a system that learns from every patient encounter while incorporating the latest clinical evidence, continuously improving care delivery. We begin to address implementation challenges and initiate important discussions around ethical considerations and governance needs. While developed for ADRD, this roadmap provides principles for responsible AI integration across neurology and other medical specialties, with potential to improve diagnostic accuracy, reduce care disparities, and advance clinical knowledge through a learning healthcare system.", 'abstract_zh': '卫生系统正 struggles 以满足不断增长的神经科护理需求，特别是在阿尔茨海默病及相关痴呆症（ADRD）领域，面临的挑战尤为严峻。尽管人工智能研究往往侧重于识别超越人类感知的模式，但将这些预测能力付诸实践仍然极具挑战性，因为临床医生无法轻易验证他们自己无法察觉的洞察。我们提出，大型语言模型（LLMs）可以在三个方面立即增强临床医生的能力，提供更为实际的应用：全面的数据收集、复杂临床信息的解释以及相关医学知识的及时应用。这些挑战源自于有限的诊断时间、日益复杂的数据以及超出了任何临床医生能力范围的庞大医学文献。我们提出了一种负责任的人工智能集成框架，利用LLMs的有效沟通能力，同时保持人类监督。这种做法优先考虑标准化、高质量的数据收集，使系统能够从每一次患者接触中学习，并结合最新的临床证据，持续改进照护服务。我们开始解决实施挑战，并开展关于伦理考量和治理需求的重要讨论。虽然该框架最初是为ADRD设计的，但提供了神经学和其他医学专科负责任的人工智能集成原则，有望提高诊断准确性、减少照护差异，并通过学习卫生系统推动临床知识的发展。', 'title_zh': '将生成型人工智能集成到神经退行性疾病管理中：一种 streamlined 诊断和护理框架'}
{'arxiv_id': 'arXiv:2502.06833', 'title': 'Entropy Adaptive Decoding: Dynamic Model Switching for Efficient Inference', 'authors': 'Toby Simonds', 'link': 'https://arxiv.org/abs/2502.06833', 'abstract': "We present Entropy Adaptive Decoding (EAD), a novel approach for efficient language model inference that dynamically switches between different-sized models based on prediction uncertainty. By monitoring rolling entropy in model logit distributions, our method identifies text regions where a smaller model suffices and switches to a larger model only when prediction uncertainty exceeds a threshold. Unlike speculative decoding approaches that maintain perfect output fidelity through verification, EAD accepts controlled output divergence in exchange for computational efficiency. Our experiments on the MATH benchmark demonstrate remarkable efficiency gains across different model families. Using the LLaMA family, we maintain 96.7\\% of the 11B model's performance (50.4\\% vs 52.1\\%) while using it for only 43\\% of tokens, decreasing computational cost by 41.5\\%. These gains become more pronounced with larger size differentials in the Qwen family, where we achieve 92.9\\% of the 14B model's performance (74.3\\% vs 80.0\\%) while using it for just 25\\% of tokens, decreasing computational cost by 67\\%. The consistency of these results across model pairs suggests that language model computation can be significantly optimized by selectively deploying model capacity based on local generation complexity. Our findings indicate that current approaches to model inference may be unnecessarily conservative in their pursuit of perfect output fidelity, and that accepting minor performance trade-offs can enable dramatic reductions in computational costs.", 'abstract_zh': '基于熵自适应解码的高效语言模型推断方法', 'title_zh': '熵自适应解码：动态模型切换以实现高效推理'}
{'arxiv_id': 'arXiv:2502.06820', 'title': 'LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning', 'authors': 'Zhekai Du, Yinjie Min, Jingjing Li, Ke Lu, Changliang Zou, Liuhua Peng, Tingjin Chu, Mingming Gong', 'link': 'https://arxiv.org/abs/2502.06820', 'abstract': 'Low-rank adaptation (LoRA) has become a prevalent method for adapting pre-trained large language models to downstream tasks. However, the simple low-rank decomposition form may constrain the hypothesis space. To address this limitation, we introduce Location-aware Cosine Adaptation (LoCA), a novel frequency-domain parameter-efficient fine-tuning method based on inverse Discrete Cosine Transform (iDCT) with selective locations of learnable components. We begin with a comprehensive theoretical comparison between frequency-domain and low-rank decompositions for fine-tuning pre-trained large models. Our analysis reveals that frequency-domain approximation with carefully selected frequency components can surpass the expressivity of traditional low-rank-based methods. Furthermore, we demonstrate that iDCT offers a more efficient implementation compared to inverse Discrete Fourier Transform (iDFT), allowing for better selection and tuning of frequency components while maintaining equivalent expressivity to the optimal iDFT-based adaptation. By employing finite-difference approximation to estimate gradients for discrete locations of learnable coefficients on the DCT spectrum, LoCA dynamically selects the most informative frequency components during training. Experiments on diverse language and vision fine-tuning tasks demonstrate that LoCA offers enhanced parameter efficiency while maintains computational feasibility comparable to low-rank-based methods.', 'abstract_zh': '基于逆离散余弦变换的注意力位置感知余弦适应（LoCA）：一种频率域参数高效微调方法', 'title_zh': 'LoCA: 基于位置的余弦适配参数高效微调'}
{'arxiv_id': 'arXiv:2502.06813', 'title': 'Policy Guided Tree Search for Enhanced LLM Reasoning', 'authors': 'Yang Li', 'link': 'https://arxiv.org/abs/2502.06813', 'abstract': 'Despite their remarkable capabilities, large language models often struggle with tasks requiring complex reasoning and planning. While existing approaches like Chain-of-Thought prompting and tree search techniques show promise, they are limited by their reliance on predefined heuristics and computationally expensive exploration strategies. We propose Policy-Guided Tree Search (PGTS), a framework that combines reinforcement learning with structured tree exploration to efficiently navigate reasoning paths. Our key innovation is a learned policy that dynamically decides between expanding, branching, backtracking, or terminating exploration, eliminating the need for manual heuristics or exhaustive search. Experiments across mathematical reasoning, logical deduction, and planning benchmarks demonstrate that PGTS achieves superior reasoning performance while significantly reducing computational costs compared to existing methods. These results establish PGTS as a scalable and effective solution for tackling complex reasoning tasks with LLMs.', 'abstract_zh': '尽管大型语言模型具有 remarkable 的能力，但在需要复杂推理和规划的任务上往往表现不佳。虽然现有的方法如 Chain-of-Thought 提示和树搜索技术显示出潜力，但它们受限于对预定义启发式的依赖以及计算成本高昂的探索策略。我们提出了一种 Policy-Guided Tree Search (PGTS) 框架，该框架结合了强化学习与结构化树探索，以高效地导航推理路径。我们的核心创新是一种学习得到的策略，它能够动态地决定是否扩展、分支、回溯或终止探索，从而消除手动启发式或 exhaustive 搜索的需要。跨数学推理、逻辑演绎和规划基准的实验结果显示，PGTS 在提高推理性能的同时，相比现有方法显著降低了计算成本。这些结果将 PGTS 确立为一个适用于利用大规模语言模型解决复杂推理任务的可扩展且有效的解决方案。', 'title_zh': '政策引导树搜索以增强大规模语言模型推理'}
{'arxiv_id': 'arXiv:2502.06809', 'title': 'Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution', 'authors': 'Muhammad Umair Haider, Hammad Rizwan, Hassan Sajjad, Peizhong Ju, A.B. Siddique', 'link': 'https://arxiv.org/abs/2502.06809', 'abstract': 'Interpreting and controlling the internal mechanisms of large language models (LLMs) is crucial for improving their trustworthiness and utility. Recent efforts have primarily focused on identifying and manipulating neurons by establishing discrete mappings between neurons and semantic concepts. However, such mappings struggle to handle the inherent polysemanticity in LLMs, where individual neurons encode multiple, distinct concepts. This makes precise control challenging and complicates downstream interventions. Through an in-depth analysis of both encoder and decoder-based LLMs across multiple text classification datasets, we uncover that while individual neurons encode multiple concepts, their activation magnitudes vary across concepts in distinct, Gaussian-like patterns. Building on this insight, we introduce NeuronLens, a novel range-based interpretation and manipulation framework that provides a finer view of neuron activation distributions to localize concept attribution within a neuron. Extensive empirical evaluations demonstrate that NeuronLens significantly reduces unintended interference, while maintaining precise control for manipulation of targeted concepts, outperforming existing methods.', 'abstract_zh': '解释和控制大型语言模型（LLMs）的内部机制对于提高其可靠性和实用性至关重要。近期的努力主要集中在通过建立神经元与语义概念之间的离散映射来识别和操控神经元。然而，这种映射难以处理LLMs固有的多义性，即单个神经元编码多个不同的概念。这使得精确控制变得困难，并复杂化了下游干预。通过对多个文本分类数据集上的编码器和解码器基于的LLMs进行深入分析，我们发现尽管单个神经元编码多个概念，但在不同概念上的激活强度呈现出明显的、类似高斯分布的模式。基于这一洞见，我们引入了NeuronLens，这是一种新型的范围基解释和操控框架，提供对神经元激活分布的更精细视角，以定位神经元中的概念归因。广泛的实证评估表明，NeuronLens显著减少了无意的干扰，同时保持了对目标概念操控的精确控制，优于现有方法。', 'title_zh': '神经元以范围说话：突破离散神经元归因的束缚'}
{'arxiv_id': 'arXiv:2502.06807', 'title': 'Competitive Programming with Large Reasoning Models', 'authors': 'OpenAI, Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 contributors, Oleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, Wenda Zhou', 'link': 'https://arxiv.org/abs/2502.06807', 'abstract': 'We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.', 'abstract_zh': '我们将强化学习应用于大型语言模型（LLMs）显著提升了复杂编码和推理任务的表现。此外，我们对比了两种通用推理模型——OpenAI o1和o3的早期检查点版本，以及一个针对2024国际信息学奥林匹克竞赛（IOI）设计的手工工程化推理策略的领域特定系统o1-ioi。我们参加了2024年的IOI竞赛，并使用手工设计的测试时策略，o1-ioi位于第49百分位。在较宽松的竞赛约束下，o1-ioi获得了金牌。然而，当我们评估后续模型o3时，发现即使不使用手工设计的领域特定策略或较宽松的约束，o3也能获得金牌。我们的研究发现，尽管专门的流水线如o1-ioi带来了明显的改进，但扩展后的通用o3模型在推理领域仍能超越这些结果，而且无需依赖手工设计的推理启发式方法。值得注意的是，o3在2024年的IOI竞赛中获得了金牌，并且其Codeforces排名与顶级的人类竞争对手相当。总体而言，这些结果表明，在推理领域，如编程竞赛，通过扩展通用的强化学习而非依赖领域特定技术，提供了一条稳健的道路，以实现先进的人工智能。', 'title_zh': '大规模推理模型下的编程竞赛'}
{'arxiv_id': 'arXiv:2502.06806', 'title': 'Logits are All We Need to Adapt Closed Models', 'authors': 'Gaurush Hiranandani, Haolun Wu, Subhojyoti Mukherjee, Sanmi Koyejo', 'link': 'https://arxiv.org/abs/2502.06806', 'abstract': 'Many commercial Large Language Models (LLMs) are often closed-source, limiting developers to prompt tuning for aligning content generation with specific applications. While these models currently do not provide access to token logits, we argue that if such access were available, it would enable more powerful adaptation techniques beyond prompt engineering. In this paper, we propose a token-level probability reweighting framework that, given access to logits and a small amount of task-specific data, can effectively steer black-box LLMs toward application-specific content generation. Our approach views next-token prediction through the lens of supervised classification. We show that aligning black-box LLMs with task-specific data can be formulated as a label noise correction problem, leading to \\emph{Plugin} model -- an autoregressive probability reweighting model that operates solely on logits. We provide theoretical justification for why reweighting logits alone is sufficient for task adaptation. Extensive experiments with multiple datasets, LLMs, and reweighting models demonstrate the effectiveness of our method, advocating for broader access to token logits in closed-source models.', 'abstract_zh': '许多商用大型语言模型（LLMs）往往是闭源的，限制了开发者仅能通过提示调优来使内容生成与特定应用对齐。尽管这些模型当前不提供对标记概率的访问，我们认为如果能够访问这些信息，将能够使模型适应技术超出提示工程的更强大的适应技术。本文提出了一种标记级别概率重加权框架，该框架在获得标记概率和少量任务特定数据的情况下，可以有效地引导黑盒LLMs生成特定应用的内容。我们的方法将下一个标记的预测视为监督分类的问题。我们展示了如何将黑盒LLMs与任务特定数据对齐可以形成为一个标签噪声纠正问题，并由此提出了一种仅基于标记概率进行自回归概率重加权的“Plug-in”模型。我们提供了理论上的理由说明仅重加权标记概率就足以进行任务适应。通过多种数据集、LLMs和重加权模型的广泛实验，证明了我们方法的有效性，并倡导在闭源模型中提供对标记概率的更广泛访问。', 'title_zh': '_logits都是我们所需要适应封闭模型的要素_'}
{'arxiv_id': 'arXiv:2502.06802', 'title': 'Solving the Content Gap in Roblox Game Recommendations: LLM-Based Profile Generation and Reranking', 'authors': 'Chen Wang, Xiaokai Wei, Yexi Jiang, Frank Ong, Kevin Gao, Xiao Yu, Zheng Hui, Se-eun Yoon, Philip Yu, Michelle Gong', 'link': 'https://arxiv.org/abs/2502.06802', 'abstract': 'With the vast and dynamic user-generated content on Roblox, creating effective game recommendations requires a deep understanding of game content. Traditional recommendation models struggle with the inconsistent and sparse nature of game text features such as titles and descriptions. Recent advancements in large language models (LLMs) offer opportunities to enhance recommendation systems by analyzing in-game text data. This paper addresses two challenges: generating high-quality, structured text features for games without extensive human annotation, and validating these features to ensure they improve recommendation relevance. We propose an approach that extracts in-game text and uses LLMs to infer attributes such as genre and gameplay objectives from raw player interactions. Additionally, we introduce an LLM-based re-ranking mechanism to assess the effectiveness of the generated text features, enhancing personalization and user satisfaction. Beyond recommendations, our approach supports applications such as user engagement-based integrity detection, already deployed in production. This scalable framework demonstrates the potential of in-game text understanding to improve recommendation quality on Roblox and adapt recommendations to its unique, user-generated ecosystem.', 'abstract_zh': '利用大规模语言模型提升Roblox游戏中游戏推荐的有效性', 'title_zh': '解决Roblox游戏推荐中的内容缺口：基于LLM的用户画像生成与重排序'}
