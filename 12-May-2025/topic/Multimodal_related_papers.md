# MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks 

**Title (ZH)**: MM-Skin: 通过源自教科书的图像-文本数据集增强皮肤病视力语言模型 

**Authors**: Wenqi Zeng, Yuqi Sun, Chenxi Ma, Weimin Tan, Bo Yan  

**Link**: [PDF](https://arxiv.org/pdf/2505.06152)  

**Abstract**: Medical vision-language models (VLMs) have shown promise as clinical assistants across various medical fields. However, specialized dermatology VLM capable of delivering professional and detailed diagnostic analysis remains underdeveloped, primarily due to less specialized text descriptions in current dermatology multimodal datasets. To address this issue, we propose MM-Skin, the first large-scale multimodal dermatology dataset that encompasses 3 imaging modalities, including clinical, dermoscopic, and pathological and nearly 10k high-quality image-text pairs collected from professional textbooks. In addition, we generate over 27k diverse, instruction-following vision question answering (VQA) samples (9 times the size of current largest dermatology VQA dataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a dermatology-specific VLM designed for precise and nuanced skin disease interpretation. Comprehensive benchmark evaluations of SkinVL on VQA, supervised fine-tuning (SFT) and zero-shot classification tasks across 8 datasets, reveal its exceptional performance for skin diseases in comparison to both general and medical VLM models. The introduction of MM-Skin and SkinVL offers a meaningful contribution to advancing the development of clinical dermatology VLM assistants. MM-Skin is available at this https URL 

**Abstract (ZH)**: 医学视觉语言模型（VLMs）在 various 医疗领域展现出作为临床助手的前景。然而，专门用于皮肤科且能够提供专业详细诊断分析的视觉语言模型仍处于初步发展阶段，主要原因是当前皮肤科多模态数据集中文本描述不够专门化。为了解决这一问题，我们提出了 MM-Skin，这是首个包含 3 种成像模态（临床、皮肤镜和病理）的大规模多模态皮肤科数据集，积累了近 10,000 个高质量图像-文本对，来自专业教科书。此外，我们还生成了超过 27,000 个多样且遵循指令的视觉问答（VQA）样本（比当前最大的皮肤科 VQA 数据集大 9 倍）。利用公开数据集和 MM-Skin，我们开发了 SkinVL，这是一种针对皮肤疾病的专属性视觉语言模型，旨在进行精确和细腻的皮肤疾病解释。在 8 个数据集上的视觉问答、监督微调和零样本分类基准评估中，SkinVL 在皮肤疾病任务上的表现明显优于通用和医学视觉语言模型。MM-Skin 的引入和 SkinVL 的开发对推动临床皮肤科视觉语言模型助手的发展具有重要意义。MM-Skin 可在以下链接获取：this https URL。 

---
# Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models 

**Title (ZH)**: 基于Transformer模型的CMU-MOSEI多模态情感分析 

**Authors**: Jugal Gajjar, Kaustik Ranaware  

**Link**: [PDF](https://arxiv.org/pdf/2505.06110)  

**Abstract**: This project performs multimodal sentiment analysis using the CMU-MOSEI dataset, using transformer-based models with early fusion to integrate text, audio, and visual modalities. We employ BERT-based encoders for each modality, extracting embeddings that are concatenated before classification. The model achieves strong performance, with 97.87\% 7-class accuracy and a 0.9682 F1-score on the test set, demonstrating the effectiveness of early fusion in capturing cross-modal interactions. The training utilized Adam optimization (lr=1e-4), dropout (0.3), and early stopping to ensure generalization and robustness. Results highlight the superiority of transformer architectures in modeling multimodal sentiment, with a low MAE (0.1060) indicating precise sentiment intensity prediction. Future work may compare fusion strategies or enhance interpretability. This approach utilizes multimodal learning by effectively combining linguistic, acoustic, and visual cues for sentiment analysis. 

**Abstract (ZH)**: 本项目使用CMU-MOSEI数据集进行多模态情感分析，采用基于Transformer的模型并采用早期融合策略整合文本、音频和视觉模态。我们为每个模态使用BERT编码器提取嵌入，然后在分类前进行拼接。模型在测试集上达到97.87%的7类准确率和0.9682的F1分数，证明了早期融合在捕捉跨模态交互方面的有效性。训练过程中采用Adam优化（学习率1e-4）、dropout（0.3）和早停策略以确保泛化能力和稳健性。结果表明，Transformer架构在建模多模态情感方面具有优势，较低的MAE（0.1060）表明情感强度预测的精确性。未来工作可能会比较不同的融合策略或增强可解释性。本方法通过有效结合语言、声学和视觉线索来进行情感分析。 

---
# Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI 

**Title (ZH)**: 利用视觉语言模型进行汽车UI的视觉定位与分析 

**Authors**: Benjamin Raphael Ernhofer, Daniil Prokhorov, Jannica Langner, Dominik Bollmann  

**Link**: [PDF](https://arxiv.org/pdf/2505.05895)  

**Abstract**: Modern automotive infotainment systems require intelligent and adaptive solutions to handle frequent User Interface (UI) updates and diverse design variations. We introduce a vision-language framework for understanding and interacting with automotive infotainment systems, enabling seamless adaptation across different UI designs. To further support research in this field, we release AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208 annotations. Additionally, we present a synthetic data pipeline to generate training data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation (LoRa) and incorporating reasoning generated by our pipeline, along with visual grounding and evaluation capabilities. The fine-tuned Evaluative Large Action Model (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and dataset are available on Hugging Face) and demonstrating strong cross-domain generalization, including a +5.2% improvement on ScreenSpot over the baseline model. Notably, our approach achieves 80.4% average accuracy on ScreenSpot, closely matching or even surpassing specialized models for desktop, mobile, and web, such as ShowUI, despite being trained for the infotainment domain. This research investigates how data collection and subsequent fine-tuning can lead to AI-driven progress within automotive UI understanding and interaction. The applied method is cost-efficient and fine-tuned models can be deployed on consumer-grade GPUs. 

**Abstract (ZH)**: 现代汽车娱乐系统需要智能且适应性强的解决方案来处理频繁的用户界面更新和多样化的设计变化。我们介绍了一种vision-language框架，用于理解和与汽车娱乐系统交互， enabling无缝跨不同UI设计的适应性。为了进一步支持该领域的研究，我们发布了AutomotiveUI-Bench-4K，这是一个包含998张图像和4,208个注释的开源数据集。此外，我们呈现了一种合成数据管道来生成训练数据。我们使用低秩适应（LoRa）并结合由我们的管道生成的推理、视觉定位和评估能力，对基于Molmo-7B的模型进行了微调。微调后的Evaluative Large Action Model (ELAM)在AutomotiveUI-Bench-4K上表现出较强的效果（模型和数据集可在Hugging Face上获得），并在跨域泛化方面表现出色，包括在ScreenSpot上比基线模型提高了5.2%。值得注意的是，我们的方法在ScreenSpot上的平均准确率为80.4%，接近或甚至超过了专门为桌面、移动和网络设计的ShowUI等专用模型，尽管它是为娱乐系统领域训练的。本研究探讨了数据收集和后续微调如何推动汽车UI理解与交互的AI驱动进步。应用的方法经济高效，微调后的模型可以部署在消费级GPU上。 

---
# Multi-Modal Molecular Representation Learning via Structure Awareness 

**Title (ZH)**: 基于结构意识的多模态分子表示学习 

**Authors**: Rong Yin, Ruyue Liu, Xiaoshuai Hao, Xingrui Zhou, Yong Liu, Can Ma, Weiping Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.05877)  

**Abstract**: Accurate extraction of molecular representations is a critical step in the drug discovery process. In recent years, significant progress has been made in molecular representation learning methods, among which multi-modal molecular representation methods based on images, and 2D/3D topologies have become increasingly mainstream. However, existing these multi-modal approaches often directly fuse information from different modalities, overlooking the potential of intermodal interactions and failing to adequately capture the complex higher-order relationships and invariant features between molecules. To overcome these challenges, we propose a structure-awareness-based multi-modal self-supervised molecular representation pre-training framework (MMSA) designed to enhance molecular graph representations by leveraging invariant knowledge between molecules. The framework consists of two main modules: the multi-modal molecular representation learning module and the structure-awareness module. The multi-modal molecular representation learning module collaboratively processes information from different modalities of the same molecule to overcome intermodal differences and generate a unified molecular embedding. Subsequently, the structure-awareness module enhances the molecular representation by constructing a hypergraph structure to model higher-order correlations between molecules. This module also introduces a memory mechanism for storing typical molecular representations, aligning them with memory anchors in the memory bank to integrate invariant knowledge, thereby improving the model generalization ability. Extensive experiments have demonstrated the effectiveness of MMSA, which achieves state-of-the-art performance on the MoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to 9.6% over baseline methods. 

**Abstract (ZH)**: 准确提取分子表示是药物发现过程中一个关键步骤。近年来，在分子表示学习方法方面取得了显著进展，其中基于图像和2D/3D拓扑的多模态分子表示方法变得日益主流。然而，现有的多模态方法通常直接融合不同模态的信息，忽视了模态间相互作用的潜力，并未能充分捕捉分子之间的复杂高阶关系和不变特征。为克服这些挑战，我们提出了一种基于结构意识的多模态自监督分子表示预训练框架（MMSA），旨在通过利用分子间的不变知识来增强分子图表示。该框架由两个主要模块组成：多模态分子表示学习模块和结构意识模块。多模态分子表示学习模块协作处理同一分子不同模态的信息，以克服模态间差异并生成统一的分子嵌入。随后，结构意识模块通过构建超图结构来建模分子间的高阶相关性，从而增强分子表示。该模块还引入了记忆机制，用于存储典型分子表示，并将它们与记忆库中的记忆锚进行对齐，以整合不变知识，从而提高模型的泛化能力。广泛实验表明，MMSA 的有效性，在MoleculeNet基准测试上的性能显著优于基线方法，AUC改善范围从1.8%到9.6%。 

---
# Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models 

**Title (ZH)**: 超越语言先验：增强多模态模型中的视觉理解与注意力 

**Authors**: Aarti Ghatkesar, Uddeshya Upadhyay, Ganesh Venkatesh  

**Link**: [PDF](https://arxiv.org/pdf/2505.05626)  

**Abstract**: Achieving deep alignment between vision and language remains a central challenge for Multimodal Large Language Models (MLLMs). These models often fail to fully leverage visual input, defaulting to strong language priors. Our approach first provides insights into how MLLMs internally build visual understanding of image regions and then introduces techniques to amplify this capability. Specifically, we explore techniques designed both to deepen the model's understanding of visual content and to ensure that these visual insights actively guide language generation. We demonstrate the superior multimodal understanding of our resultant model through a detailed upstream analysis quantifying its ability to predict visually-dependent tokens as well as 10 pt boost on visually challenging tasks. 

**Abstract (ZH)**: 实现视觉与语言的深层对齐仍然是多模态大规模语言模型（MLLMs）面临的核心挑战。我们的方法首先深入探讨MLLMs如何内部构建对图像区域的视觉理解，然后引入技术以增强这一能力。具体来说，我们探索了既加深模型对视觉内容理解又能确保这些视觉洞察积极引导语言生成的技术。我们通过详细的上游分析展示了所得模型的优越多模态理解能力，该分析定量评估了模型预测视觉依赖性标记的能力，并在视觉挑战性任务上获得了10个百分点的提升。 

---
# PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models 

**Title (ZH)**: PyTDC：面向生物医学基础模型的多模态机器学习训练、评估和推理平台 

**Authors**: Alejandro Velez-Arce, Marinka Zitnik  

**Link**: [PDF](https://arxiv.org/pdf/2505.05577)  

**Abstract**: Existing biomedical benchmarks do not provide end-to-end infrastructure for training, evaluation, and inference of models that integrate multimodal biological data and a broad range of machine learning tasks in therapeutics. We present PyTDC, an open-source machine-learning platform providing streamlined training, evaluation, and inference software for multimodal biological AI models. PyTDC unifies distributed, heterogeneous, continuously updated data sources and model weights and standardizes benchmarking and inference endpoints. This paper discusses the components of PyTDC's architecture and, to our knowledge, the first-of-its-kind case study on the introduced single-cell drug-target nomination ML task. We find state-of-the-art methods in graph representation learning and domain-specific methods from graph theory perform poorly on this task. Though we find a context-aware geometric deep learning method that outperforms the evaluated SoTA and domain-specific baseline methods, the model is unable to generalize to unseen cell types or incorporate additional modalities, highlighting PyTDC's capacity to facilitate an exciting avenue of research developing multimodal, context-aware, foundation models for open problems in biomedical AI. 

**Abstract (ZH)**: 现有的生物医药基准尚未提供从训练、评估到推理的端到端基础设施，用于整合多模态生物数据和广泛药物治疗机器学习任务的模型。我们提出PyTDC，一个开源机器学习平台，提供多模态生物AI模型的简化训练、评估和推理软件。PyTDC 统一了分布式、异构的并持续更新的数据源和模型权重，并标准化了基准测试和推理端点。本文讨论了PyTDC 架构的组件，并提供了这项工作中介绍的第一个案例研究，即单细胞药物靶点提名的机器学习任务。我们发现，图表示学习的最新方法和特定领域的图理论方法在这项任务上表现不佳。尽管我们发现一种基于语境的几何深度学习方法在评估的最新方法和领域特定基线方法中表现出色，但该模型无法泛化到未见的细胞类型或整合额外的模态，突显了PyTDC 在促进为生物医药AI 开放问题开发多模态、基于语境的基石模型的研究方面的能力。 

---
