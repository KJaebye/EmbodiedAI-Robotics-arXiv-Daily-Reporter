{'arxiv_id': 'arXiv:2503.23130', 'title': 'Can DeepSeek-V3 Reason Like a Surgeon? An Empirical Evaluation for Vision-Language Understanding in Robotic-Assisted Surgery', 'authors': 'Boyi Ma, Yanguang Zhao, Jie Wang, Guankun Wang, Kun Yuan, Tong Chen, Long Bai, Hongliang Ren', 'link': 'https://arxiv.org/abs/2503.23130', 'abstract': 'DeepSeek-V3, a recently emerging Large Language Model (LLM), demonstrates outstanding performance in general scene understanding, question-answering (QA), and text generation tasks, owing to its efficient training paradigm and strong reasoning capabilities. In this study, we investigate the dialogue capabilities of DeepSeek-V3 in robotic surgery scenarios, focusing on tasks such as Single Phrase QA, Visual QA, and Detailed Description. The Single Phrase QA tasks further include sub-tasks such as surgical instrument recognition, action understanding, and spatial position analysis. We conduct extensive evaluations using publicly available datasets, including EndoVis18 and CholecT50, along with their corresponding dialogue data. Our comprehensive evaluation results indicate that, when provided with specific prompts, DeepSeek-V3 performs well in surgical instrument and tissue recognition tasks However, DeepSeek-V3 exhibits significant limitations in spatial position analysis and struggles to understand surgical actions accurately. Additionally, our findings reveal that, under general prompts, DeepSeek-V3 lacks the ability to effectively analyze global surgical concepts and fails to provide detailed insights into surgical scenarios. Based on our observations, we argue that the DeepSeek-V3 is not ready for vision-language tasks in surgical contexts without fine-tuning on surgery-specific datasets.', 'abstract_zh': 'DeepSeek-V3：一种新兴的大语言模型在机器人手术对话能力研究', 'title_zh': 'Can DeepSeek-V3 模拟外科医生的推理能力？一种基于视觉-语言理解的机器人辅助手术实证评估'}
{'arxiv_id': 'arXiv:2503.24228', 'title': 'PAARS: Persona Aligned Agentic Retail Shoppers', 'authors': "Saab Mansour, Leonardo Perelli, Lorenzo Mainetti, George Davidson, Stefano D'Amato", 'link': 'https://arxiv.org/abs/2503.24228', 'abstract': 'In e-commerce, behavioral data is collected for decision making which can be costly and slow. Simulation with LLM powered agents is emerging as a promising alternative for representing human population behavior. However, LLMs are known to exhibit certain biases, such as brand bias, review rating bias and limited representation of certain groups in the population, hence they need to be carefully benchmarked and aligned to user behavior. Ultimately, our goal is to synthesise an agent population and verify that it collectively approximates a real sample of humans. To this end, we propose a framework that: (i) creates synthetic shopping agents by automatically mining personas from anonymised historical shopping data, (ii) equips agents with retail-specific tools to synthesise shopping sessions and (iii) introduces a novel alignment suite measuring distributional differences between humans and shopping agents at the group (i.e. population) level rather than the traditional "individual" level. Experimental results demonstrate that using personas improves performance on the alignment suite, though a gap remains to human behaviour. We showcase an initial application of our framework for automated agentic A/B testing and compare the findings to human results. Finally, we discuss applications, limitations and challenges setting the stage for impactful future work.', 'abstract_zh': '基于LLM的代理模拟在电子商务中的行为数据合成与对齐：一个框架及其应用', 'title_zh': 'PAARS：个性导向的行动者零售消费者'}
{'arxiv_id': 'arXiv:2503.24047', 'title': 'Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents', 'authors': 'Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, Jiajun Zhang', 'link': 'https://arxiv.org/abs/2503.24047', 'abstract': 'As scientific research becomes increasingly complex, innovative tools are needed to manage vast data, facilitate interdisciplinary collaboration, and accelerate discovery. Large language models (LLMs) are now evolving into LLM-based scientific agents that automate critical tasks, ranging from hypothesis generation and experiment design to data analysis and simulation. Unlike general-purpose LLMs, these specialized agents integrate domain-specific knowledge, advanced tool sets, and robust validation mechanisms, enabling them to handle complex data types, ensure reproducibility, and drive scientific breakthroughs. This survey provides a focused review of the architectures, design, benchmarks, applications, and ethical considerations surrounding LLM-based scientific agents. We highlight why they differ from general agents and the ways in which they advance research across various scientific fields. By examining their development and challenges, this survey offers a comprehensive roadmap for researchers and practitioners to harness these agents for more efficient, reliable, and ethically sound scientific discovery.', 'abstract_zh': '随着科学研究的日益复杂，需要创新工具来管理大量数据、促进跨学科合作并加速发现过程。大型语言模型（LLMs）现在正在演变成基于LLM的科学代理，自动化从假设生成和实验设计到数据分析和模拟的各种关键任务。与通用型LLMs不同，这些专门的代理整合了领域特定知识、高级工具集和 robust 验证机制，使其能够处理复杂数据类型、确保可再现性并推动科学突破。本文综述了基于LLM的科学代理的架构、设计、基准测试、应用和伦理考虑。我们强调了它们与通用代理的区别，并展示了它们如何在各个科学领域推进研究。通过对它们的发展和挑战的分析，本文为研究人员和从业者提供了一条综合路线图，以便更高效、可靠且伦理地利用这些代理进行科学研究。', 'title_zh': '面向科学智能：基于LLM的科学代理综述'}
{'arxiv_id': 'arXiv:2503.24028', 'title': 'Pay More Attention to the Robustness of Prompt for Instruction Data Mining', 'authors': 'Qiang Wang, Dawei Feng, Xu Zhang, Ao Shen, Yang Xu, Bo Ding, Huaimin Wang', 'link': 'https://arxiv.org/abs/2503.24028', 'abstract': "Instruction tuning has emerged as a paramount method for tailoring the behaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve high performance through fine-tuning with a limited quantity of high-quality instruction data. Building upon this approach, we further explore the impact of prompt's robustness on the selection of high-quality instruction data. This paper proposes a pioneering framework of high-quality online instruction data mining for instruction tuning, focusing on the impact of prompt's robustness on the data mining process. Our notable innovation, is to generate the adversarial instruction data by conducting the attack for the prompt of online instruction data. Then, we introduce an Adversarial Instruction-Following Difficulty metric to measure how much help the adversarial instruction data can provide to the generation of the corresponding response. Apart from it, we propose a novel Adversarial Instruction Output Embedding Consistency approach to select high-quality online instruction data. We conduct extensive experiments on two benchmark datasets to assess the performance. The experimental results serve to underscore the effectiveness of our proposed two methods. Moreover, the results underscore the critical practical significance of considering prompt's robustness.", 'abstract_zh': '基于提示鲁棒性的高质量在线指令数据挖掘框架', 'title_zh': '更多关注指令数据挖掘中提示的鲁棒性'}
{'arxiv_id': 'arXiv:2503.23781', 'title': 'DebFlow: Automating Agent Creation via Agent Debate', 'authors': 'Jinwei Su, Yinghui Xia, Ronghua Shi, Jianhui Wang, Jianuo Huang, Yijin Wang, Tianyu Shi, Yang Jingsong, Lewei He', 'link': 'https://arxiv.org/abs/2503.23781', 'abstract': 'Large language models (LLMs) have demonstrated strong potential and impressive performance in automating the generation and optimization of workflows. However, existing approaches are marked by limited reasoning capabilities, high computational demands, and significant resource requirements. To address these issues, we propose DebFlow, a framework that employs a debate mechanism to optimize workflows and integrates reflexion to improve based on previous experiences. We evaluated our method across six benchmark datasets, including HotpotQA, MATH, and ALFWorld. Our approach achieved a 3\\% average performance improvement over the latest baselines, demonstrating its effectiveness in diverse problem domains. In particular, during training, our framework reduces resource consumption by 37\\% compared to the state-of-the-art baselines. Additionally, we performed ablation studies. Removing the Debate component resulted in a 4\\% performance drop across two benchmark datasets, significantly greater than the 2\\% drop observed when the Reflection component was removed. These findings strongly demonstrate the critical role of Debate in enhancing framework performance, while also highlighting the auxiliary contribution of reflexion to overall optimization.', 'abstract_zh': '大型语言模型（LLMs）在自动化工作流的生成和优化方面展现了强大的潜力和令人印象深刻的性能。然而，现有方法存在推理能力有限、高计算需求和显著的资源要求等问题。为解决这些问题，我们提出了DebFlow框架，该框架采用了辩论机制来优化工作流，并结合反省机制以根据以往经验改进。我们通过对HotpotQA、MATH和ALFWorld等六个基准数据集进行评估，我们的方法在最新基线方法上实现了3%的平均性能提升，证明了其在多样化的问题领域中的有效性。特别是在训练过程中，与最先进的基线方法相比，我们的框架将资源消耗降低了37%。此外，我们还进行了消融研究。去除辩论组件导致两个基准数据集的性能下降了4%，这一下降幅度远大于去除反省组件时观察到的2%的下降幅度。这些发现强烈证明了辩论在增强框架性能中的关键作用，同时突显了反省对整体优化的辅助贡献。', 'title_zh': 'DebFlow: 通过代理辩论自动化代理创建'}
{'arxiv_id': 'arXiv:2503.23487', 'title': 'Benchmarking Systematic Relational Reasoning with Large Language and Reasoning Models', 'authors': 'Irtaza Khalid, Amir Masoud Nourollah, Steven Schockaert', 'link': 'https://arxiv.org/abs/2503.23487', 'abstract': "Large Language Models (LLMs) have been found to struggle with systematic reasoning. Even on tasks where they appear to perform well, their performance often depends on shortcuts, rather than on genuine reasoning abilities, leading them to collapse on out-of-distribution examples. Post-training strategies based on reinforcement learning and chain-of-thought prompting have recently been hailed as a step change. However, little is still known about the potential of the resulting ``Large Reasoning Models'' (LRMs) beyond problem solving in mathematics and programming, where finding genuine out-of-distribution problems can be difficult. In this paper, we focus on tasks that require systematic reasoning about relational compositions, especially for qualitative spatial and temporal reasoning. These tasks allow us to control the difficulty of problem instances, and measure in a precise way to what extent models can generalise. We find that that the considered LLMs and LRMs overall perform poorly overall, albeit better than random chance.", 'abstract_zh': '大型语言模型（LLMs）在系统性推理方面表现不佳。即使在它们表现似乎很好的任务中，它们的表现往往依赖于捷径，而不是真正的推理能力，导致它们在分布外示例中失效。基于强化学习和推理链的后训练策略最近被认为是一个重大突破。然而，除了在数学和编程问题解决领域之外，关于由此产生的“大型推理模型”（LRMs）的潜力知之甚少，在这些领域中生成具有挑战性的分布外问题可能较为困难。在本文中，我们专注于那些需要对关系组成进行系统性推理的任务，特别是在定性空间和时间推理方面。这些任务使我们能够控制问题实例的难度，并精确测量模型的泛化能力。我们发现，所考虑的LLMs和LRMs整体表现不佳，尽管比随机猜测要好。', 'title_zh': '大规模语言和推理模型中系统关系推理的基准测试'}
{'arxiv_id': 'arXiv:2503.23363', 'title': 'Large Language Models Are Better Logical Fallacy Reasoners with Counterargument, Explanation, and Goal-Aware Prompt Formulation', 'authors': 'Jiwon Jeong, Hyeju Jang, Hogun Park', 'link': 'https://arxiv.org/abs/2503.23363', 'abstract': 'The advancement of Large Language Models (LLMs) has greatly improved our ability to process complex language. However, accurately detecting logical fallacies remains a significant challenge. This study presents a novel and effective prompt formulation approach for logical fallacy detection, applicable in both supervised (fine-tuned) and unsupervised (zero-shot) settings. Our method enriches input text incorporating implicit contextual information -- counterarguments, explanations, and goals -- which we query for validity within the context of the argument. We then rank these queries based on confidence scores to inform classification. We evaluate our approach across multiple datasets from 5 domains, covering 29 distinct fallacy types, using models from the GPT and LLaMA series. The results show substantial improvements over state-of-the-art models, with F1 score increases of up to 0.60 in zero-shot settings and up to 0.45 in fine-tuned models. Extensive analyses further illustrate why and how our method excels.', 'abstract_zh': '大型语言模型的进步大大提高了我们处理复杂语言的能力。然而，准确检测逻辑谬误仍是一项重大挑战。本研究提出了一种新颖而有效的提示构建方法，适用于监督（微调）和无监督（零样本）设置中的逻辑谬误检测。我们的方法通过整合输入文本中的隐含上下文信息——反论、解释和目标——来增强输入文本，并在论点的上下文中查询这些信息的有效性。然后，我们根据置信度评分对这些查询进行排序，以指导分类。我们在涵盖29种不同谬误类型的5个领域多个数据集中评估了我们的方法，使用了GPT和LLaMA系列模型。结果表明，在零样本设置下，我们的方法显著优于最新模型，F1分数提高了0.60，在微调模型中提高了0.45。进一步的详尽分析还阐明了我们的方法为何以及如何优越。', 'title_zh': '大规模语言模型在反论、解释和目标导向提示 formulation 下是更好的逻辑谬误推理器。'}
{'arxiv_id': 'arXiv:2503.23350', 'title': 'A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation Models', 'authors': 'Liangbo Ning, Ziran Liang, Zhuohang Jiang, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao-yong Wei, Shanru Lin, Hui Liu, Philip S. Yu, Qing Li', 'link': 'https://arxiv.org/abs/2503.23350', 'abstract': "With the advancement of web techniques, they have significantly revolutionized various aspects of people's lives. Despite the importance of the web, many tasks performed on it are repetitive and time-consuming, negatively impacting overall quality of life. To efficiently handle these tedious daily tasks, one of the most promising approaches is to advance autonomous agents based on Artificial Intelligence (AI) techniques, referred to as AI Agents, as they can operate continuously without fatigue or performance degradation. In the context of the web, leveraging AI Agents -- termed WebAgents -- to automatically assist people in handling tedious daily tasks can dramatically enhance productivity and efficiency. Recently, Large Foundation Models (LFMs) containing billions of parameters have exhibited human-like language understanding and reasoning capabilities, showing proficiency in performing various complex tasks. This naturally raises the question: `Can LFMs be utilized to develop powerful AI Agents that automatically handle web tasks, providing significant convenience to users?' To fully explore the potential of LFMs, extensive research has emerged on WebAgents designed to complete daily web tasks according to user instructions, significantly enhancing the convenience of daily human life. In this survey, we comprehensively review existing research studies on WebAgents across three key aspects: architectures, training, and trustworthiness. Additionally, several promising directions for future research are explored to provide deeper insights.", 'abstract_zh': '随着网络技术的进步，它们在多个方面显著地革新了人们的生活。尽管网络很重要，但其中许多任务是重复性和耗时的，负面影响了整体生活质量。为了高效地处理这些繁琐的日常任务，最 promising 的方法之一是基于人工智能（AI）技术的进步自主代理，称为AI代理，因为它们可以在不感到疲劳或性能下降的情况下连续工作。在Web的背景下，利用被称为WebAgents的AI代理自动协助人们处理繁琐的日常任务，可以大幅度提高生产力和效率。最近，包含数十亿参数的大规模基础模型（LFMs）展示了类似人类的语言理解和推理能力，并且在执行各种复杂任务方面表现出色。这自然引出了一个问题：`LFMs能否用于开发强大的AI代理，自动处理网络任务，为用户提供显著便利？`为了全面探索LFMs的潜力，已经开展了大量研究，旨在设计能够根据用户指令完成日常网络任务的WebAgents，极大地提升了日常生活的便利性。在这篇综述中，我们从架构、训练和可信度三个方面全面回顾了现有WebAgents的研究，并探讨了若干具有前景的研究方向，以提供更深入的见解。', 'title_zh': 'Web代理综述：面向基于大规模基础模型的下一代网络自动化AI代理'}
{'arxiv_id': 'arXiv:2503.23339', 'title': 'A Scalable Framework for Evaluating Health Language Models', 'authors': 'Neil Mallinar, A. Ali Heydari, Xin Liu, Anthony Z. Faranesh, Brent Winslow, Nova Hammerquist, Benjamin Graef, Cathy Speed, Mark Malhotra, Shwetak Patel, Javier L. Prieto, Daniel McDuff, Ahmed A. Metwally', 'link': 'https://arxiv.org/abs/2503.23339', 'abstract': 'Large language models (LLMs) have emerged as powerful tools for analyzing complex datasets. Recent studies demonstrate their potential to generate useful, personalized responses when provided with patient-specific health information that encompasses lifestyle, biomarkers, and context. As LLM-driven health applications are increasingly adopted, rigorous and efficient one-sided evaluation methodologies are crucial to ensure response quality across multiple dimensions, including accuracy, personalization and safety. Current evaluation practices for open-ended text responses heavily rely on human experts. This approach introduces human factors and is often cost-prohibitive, labor-intensive, and hinders scalability, especially in complex domains like healthcare where response assessment necessitates domain expertise and considers multifaceted patient data. In this work, we introduce Adaptive Precise Boolean rubrics: an evaluation framework that streamlines human and automated evaluation of open-ended questions by identifying gaps in model responses using a minimal set of targeted rubrics questions. Our approach is based on recent work in more general evaluation settings that contrasts a smaller set of complex evaluation targets with a larger set of more precise, granular targets answerable with simple boolean responses. We validate this approach in metabolic health, a domain encompassing diabetes, cardiovascular disease, and obesity. Our results demonstrate that Adaptive Precise Boolean rubrics yield higher inter-rater agreement among expert and non-expert human evaluators, and in automated assessments, compared to traditional Likert scales, while requiring approximately half the evaluation time of Likert-based methods. This enhanced efficiency, particularly in automated evaluation and non-expert contributions, paves the way for more extensive and cost-effective evaluation of LLMs in health.', 'abstract_zh': '大型语言模型（LLMs）已成为分析复杂数据集的强大工具。最近的研究表明，当提供包括生活方式、生物标志物和上下文在内的患者特定健康信息时，LLM有可能生成有用且个性化的回应。随着LLM驱动的健康应用的日益普及，需要严格且高效的一方评价方法来确保在多个维度上（包括准确性、个性化和安全性）的回应质量。当前对开放文本回应的评价方法高度依赖于人类专家。这种方法引入了人为因素，通常成本高昂、劳动密集且阻碍可扩展性，尤其是在需要领域专业知识来评估多方面患者数据的复杂领域如医疗健康。在这项工作中，我们引入了自适应精确布尔评判标准：一种评价框架，通过使用一组靶向评判标准问题来识别模型回应中的缺口，从而简化人类和自动化的评价过程。我们的方法基于近年来在更通用的评价环境中对比一组复杂的评价目标与一组更精确、可使用简单布尔响应回答的细粒度目标的工作。我们通过代谢健康这一涵盖糖尿病、心血管疾病和肥胖的领域验证了该方法。结果表明，自适应精确布尔评判标准在专家和非专家人类评价者之间以及自动评估中相较于传统的李克特量表具有更高的评价者间一致性，且所需评价时间大约仅为李克特量表方法的一半。这种增强的效率，尤其是在自动化评价和非专家贡献中的效率，为在医疗健康领域更广泛且成本效益更高的LLM评价铺平了道路。', 'title_zh': '可扩展的健康语言模型评估框架'}
{'arxiv_id': 'arXiv:2503.23329', 'title': 'A Multi-Agent Framework with Automated Decision Rule Optimization for Cross-Domain Misinformation Detection', 'authors': 'Hui Li, Ante Wang, kunquan li, Zhihao Wang, Liang Zhang, Delai Qiu, Qingsong Liu, Jinsong Su', 'link': 'https://arxiv.org/abs/2503.23329', 'abstract': 'Misinformation spans various domains, but detection methods trained on specific domains often perform poorly when applied to others. With the rapid development of Large Language Models (LLMs), researchers have begun to utilize LLMs for cross-domain misinformation detection. However, existing LLM-based methods often fail to adequately analyze news in the target domain, limiting their detection capabilities. More importantly, these methods typically rely on manually designed decision rules, which are limited by domain knowledge and expert experience, thus limiting the generalizability of decision rules to different domains. To address these issues, we propose a MultiAgent Framework for cross-domain misinformation detection with Automated Decision Rule Optimization (MARO). Under this framework, we first employs multiple expert agents to analyze target-domain news. Subsequently, we introduce a question-reflection mechanism that guides expert agents to facilitate higherquality analysis. Furthermore, we propose a decision rule optimization approach based on carefully-designed cross-domain validation tasks to iteratively enhance the effectiveness of decision rules in different domains. Experimental results and in-depth analysis on commonlyused datasets demonstrate that MARO achieves significant improvements over existing methods.', 'abstract_zh': '一种用于跨域 misinformation 检测的多智能体框架及自动决策规则优化（MARO）', 'title_zh': '跨域 misinformation 检测的自动决策规则优化多Agent框架'}
{'arxiv_id': 'arXiv:2503.23314', 'title': 'SPIO: Ensemble and Selective Strategies via LLM-Based Multi-Agent Planning in Automated Data Science', 'authors': 'Wonduk Seo, Juhyeon Lee, Yi Bu', 'link': 'https://arxiv.org/abs/2503.23314', 'abstract': 'Large Language Models (LLMs) have revolutionized automated data analytics and machine learning by enabling dynamic reasoning and adaptability. While recent approaches have advanced multi-stage pipelines through multi-agent systems, they typically rely on rigid, single-path workflows that limit the exploration and integration of diverse strategies, often resulting in suboptimal predictions. To address these challenges, we propose SPIO (Sequential Plan Integration and Optimization), a novel framework that leverages LLM-driven decision-making to orchestrate multi-agent planning across four key modules: data preprocessing, feature engineering, modeling, and hyperparameter tuning. In each module, dedicated planning agents independently generate candidate strategies that cascade into subsequent stages, fostering comprehensive exploration. A plan optimization agent refines these strategies by suggesting several optimized plans. We further introduce two variants: SPIO-S, which selects a single best solution path as determined by the LLM, and SPIO-E, which selects the top k candidate plans and ensembles them to maximize predictive performance. Extensive experiments on Kaggle and OpenML datasets demonstrate that SPIO significantly outperforms state-of-the-art methods, providing a robust and scalable solution for automated data science task.', 'abstract_zh': '大规模语言模型（LLMs）通过实现动态推理和适应性，已经革新了自动化数据分析师和机器学习。尽管近期方法通过多智能体系统促进了多阶段管道的发展，但它们通常依赖于僵化的单路径工作流程，限制了不同策略的探索和集成，经常导致次优预测。为了解决这些挑战，我们提出了一种名为SPIO（Sequential Plan Integration and Optimization）的新框架，该框架利用LLM驱动的决策来协调四个关键模块——数据预处理、特征工程、建模和超参数调整中的多智能体规划。在每个模块中，专门的规划智能体独立生成候选策略，这些策略逐步传递到后续阶段，促进全面的探索。一个计划优化智能体通过建议多种优化计划来改进这些策略。我们还介绍了两个变体：SPIO-S，它选择由LLM确定的最佳解决方案路径；SPIO-E，它选择前k个候选计划并将其集成以最大化预测性能。在Kaggle和OpenML数据集上的广泛实验表明，SPIO显著优于现有方法，提供了自动化数据科学任务的稳健且可扩展的解决方案。', 'title_zh': 'SPIO：基于LLM的多Agent规划的集成与选择策略在自动化数据科学中的应用'}
{'arxiv_id': 'arXiv:2503.23299', 'title': 'GRASP: Municipal Budget AI Chatbots for Enhancing Civic Engagement', 'authors': 'Jerry Xu, Justin Wang, Joley Leung, Jasmine Gu', 'link': 'https://arxiv.org/abs/2503.23299', 'abstract': 'There are a growing number of AI applications, but none tailored specifically to help residents answer their questions about municipal budget, a topic most are interested in but few have a solid comprehension of. In this research paper, we propose GRASP, a custom AI chatbot framework which stands for Generation with Retrieval and Action System for Prompts. GRASP provides more truthful and grounded responses to user budget queries than traditional information retrieval systems like general Large Language Models (LLMs) or web searches. These improvements come from the novel combination of a Retrieval-Augmented Generation (RAG) framework ("Generation with Retrieval") and an agentic workflow ("Action System"), as well as prompt engineering techniques, the incorporation of municipal budget domain knowledge, and collaboration with local town officials to ensure response truthfulness. During testing, we found that our GRASP chatbot provided precise and accurate responses for local municipal budget queries 78% of the time, while GPT-4o and Gemini were only accurate 60% and 35% of the time, respectively. GRASP chatbots greatly reduce the time and effort needed for the general public to get an intuitive and correct understanding of their town\'s budget, thus fostering greater communal discourse, improving government transparency, and allowing citizens to make more informed decisions.', 'abstract_zh': '面向市政预算查询的定制AI聊天机器人框架：GRASP', 'title_zh': 'GRASP: 市级预算AI聊天机器人以增强公民参与度'}
{'arxiv_id': 'arXiv:2503.23190', 'title': 'Ethereum Price Prediction Employing Large Language Models for Short-term and Few-shot Forecasting', 'authors': 'Eftychia Makri, Georgios Palaiokrassas, Sarah Bouraga, Antigoni Polychroniadou, Leandros Tassiulas', 'link': 'https://arxiv.org/abs/2503.23190', 'abstract': 'Cryptocurrencies have transformed financial markets with their innovative blockchain technology and volatile price movements, presenting both challenges and opportunities for predictive analytics. Ethereum, being one of the leading cryptocurrencies, has experienced significant market fluctuations, making its price prediction an attractive yet complex problem. This paper presents a comprehensive study on the effectiveness of Large Language Models (LLMs) in predicting Ethereum prices for short-term and few-shot forecasting scenarios. The main challenge in training models for time series analysis is the lack of data. We address this by leveraging a novel approach that adapts existing pre-trained LLMs on natural language or images from billions of tokens to the unique characteristics of Ethereum price time series data. Through thorough experimentation and comparison with traditional and contemporary models, our results demonstrate that selectively freezing certain layers of pre-trained LLMs achieves state-of-the-art performance in this domain. This approach consistently surpasses benchmarks across multiple metrics, including Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE), demonstrating its effectiveness and robustness. Our research not only contributes to the existing body of knowledge on LLMs but also provides practical insights in the cryptocurrency prediction domain. The adaptability of pre-trained LLMs to handle the nature of Ethereum prices suggests a promising direction for future research, potentially including the integration of sentiment analysis to further refine forecasting accuracy.', 'abstract_zh': '加密货币通过其创新的区块链技术和波动的价格变化，已经改变了金融市场，为预测分析带来了挑战与机会。以以太币为例，作为一种领先的加密货币，其市场波动显著，使价格预测成为一个既吸引人又复杂的难题。本文对大规模语言模型（LLMs）在预测以太币价格方面的有效性进行了综合性研究，特别是针对短期和少样本预测场景。时间序列分析模型训练的主要挑战在于数据不足。我们通过一种新颖的方法，将现有的预训练LLMs从数亿个令牌中的自然语言或图像数据改编为以太币价格时间序列数据的独特特征来解决这一问题。通过彻底的实验和与传统及当代模型的比较，我们的结果表明，选择性地冻结预训练LLMs中的某些层可以在此领域实现最先进的性能。这种方法在多个指标上，包括均方误差（MSE）、平均绝对误差（MAE）和均方根误差（RMSE）上都超过了基准模型，展示了其有效性和稳健性。我们的研究不仅丰富了现有的大规模语言模型知识库，还为加密货币预测领域提供了实用的见解。预训练LLMs对处理以太币价格性质的高度适应性显示了一个有前景的研究方向，未来的研究可能包括整合情感分析以进一步提高预测准确性。', 'title_zh': '使用大型语言模型进行以太坊短期和少样本价格预测'}
{'arxiv_id': 'arXiv:2503.23037', 'title': 'Agentic Large Language Models, a survey', 'authors': 'Aske Plaat, Max van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten, Kees Joost Batenburg', 'link': 'https://arxiv.org/abs/2503.23037', 'abstract': 'There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories. The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories. We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs may provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world, while agentic LLMs are also likely to benefit society.', 'abstract_zh': '基于代理的大规模语言模型：研究进展与展望', 'title_zh': '代理型大型语言模型：一项综述'}
{'arxiv_id': 'arXiv:2503.22941', 'title': 'Identifying Multi-modal Knowledge Neurons in Pretrained Transformers via Two-stage Filtering', 'authors': 'Yugen Sato, Tomohiro Takagi', 'link': 'https://arxiv.org/abs/2503.22941', 'abstract': 'Recent advances in large language models (LLMs) have led to the development of multimodal LLMs (MLLMs) in the fields of natural language processing (NLP) and computer vision. Although these models allow for integrated visual and language understanding, they present challenges such as opaque internal processing and the generation of hallucinations and misinformation. Therefore, there is a need for a method to clarify the location of knowledge in MLLMs.\nIn this study, we propose a method to identify neurons associated with specific knowledge using MiniGPT-4, a Transformer-based MLLM. Specifically, we extract knowledge neurons through two stages: activation differences filtering using inpainting and gradient-based filtering using GradCAM. Experiments on the image caption generation task using the MS COCO 2017 dataset, BLEU, ROUGE, and BERTScore quantitative evaluation, and qualitative evaluation using an activation heatmap showed that our method is able to locate knowledge with higher accuracy than existing methods.\nThis study contributes to the visualization and explainability of knowledge in MLLMs and shows the potential for future knowledge editing and control.', 'abstract_zh': '近期大规模语言模型的发展推动了多模态大语言模型（多模态LLMs）在自然语言处理（NLP）和计算机视觉领域的研究。尽管这些模型能够集成视觉和语言理解，但它们也带来了内部处理不透明及幻觉和 misinformation产生的挑战。因此，需要一种方法来澄清多模态LLMs中的知识位置。\n\n在本研究中，我们提出了一种使用基于Transformer的多模态LLM MiniGPT-4的方法来识别与特定知识相关的神经元。具体而言，我们通过两个阶段提取知识神经元：使用 inpainting 的激活差异筛选和使用 GradCAM 的梯度筛选。通过在MS COCO 2017数据集上的图像字幕生成任务、以及基于BLEU、ROUGE和BERTScore的定量评估和基于激活热图的定性评估，我们证明了该方法能够比现有方法更准确地定位知识。', 'title_zh': '基于两阶段过滤的预训练变换器多模态知识神经元识别'}
{'arxiv_id': 'arXiv:2503.22931', 'title': 'Factored Agents: Decoupling In-Context Learning and Memorization for Robust Tool Use', 'authors': 'Nicholas Roth, Christopher Hidey, Lucas Spangher, William F. Arnold, Chang Ye, Nick Masiewicki, Jinoo Baek, Peter Grabowski, Eugene Ie', 'link': 'https://arxiv.org/abs/2503.22931', 'abstract': 'In this paper, we propose a novel factored agent architecture designed to overcome the limitations of traditional single-agent systems in agentic AI. Our approach decomposes the agent into two specialized components: (1) a large language model (LLM) that serves as a high level planner and in-context learner, which may use dynamically available information in user prompts, (2) a smaller language model which acts as a memorizer of tool format and output. This decoupling addresses prevalent issues in monolithic designs, including malformed, missing, and hallucinated API fields, as well as suboptimal planning in dynamic environments. Empirical evaluations demonstrate that our factored architecture significantly improves planning accuracy and error resilience, while elucidating the inherent trade-off between in-context learning and static memorization. These findings suggest that a factored approach is a promising pathway for developing more robust and adaptable agentic AI systems.', 'abstract_zh': '本文提出了一种新颖的事实性智能体架构，旨在克服传统单一智能体系统在智能体AI领域的局限性。该方法将智能体分解为两个专门化的组件：（1）一个大型语言模型（LLM），作为高层规划者和上下文学习者，可以利用用户提示中动态可用的信息；（2）一个较小的语言模型，作为工具格式和输出的记忆器。这种分解解决了单一庞大设计中存在的诸多问题，包括错误构造、缺失和臆想的API字段，以及在动态环境下的次优规划。实证评估表明，我们的分解架构显著提高了规划准确性和错误鲁棒性，同时阐明了上下文学习与静态记忆之间固有的权衡关系。这些发现表明，分解方法是开发更 robust 和适应性强的智能体AI系统的一个有前景的途径。', 'title_zh': '因子智能体：分离上下文学习和记忆以实现稳健的工具使用'}
{'arxiv_id': 'arXiv:2503.22719', 'title': 'LLM-based Agent Simulation for Maternal Health Interventions: Uncertainty Estimation and Decision-focused Evaluation', 'authors': 'Sarah Martinson, Lingkai Kong, Cheol Woo Kim, Aparna Taneja, Milind Tambe', 'link': 'https://arxiv.org/abs/2503.22719', 'abstract': "Agent-based simulation is crucial for modeling complex human behavior, yet traditional approaches require extensive domain knowledge and large datasets. In data-scarce healthcare settings where historic and counterfactual data are limited, large language models (LLMs) offer a promising alternative by leveraging broad world knowledge. This study examines an LLM-driven simulation of a maternal mobile health program, predicting beneficiaries' listening behavior when they receive health information via automated messages (control) or live representatives (intervention). Since uncertainty quantification is critical for decision-making in health interventions, we propose an LLM epistemic uncertainty estimation method based on binary entropy across multiple samples. We enhance model robustness through ensemble approaches, improving F1 score and model calibration compared to individual models. Beyond direct evaluation, we take a decision-focused approach, demonstrating how LLM predictions inform intervention feasibility and trial implementation in data-limited settings. The proposed method extends to public health, disaster response, and other domains requiring rapid intervention assessment under severe data constraints. All code and prompts used for this work can be found at this https URL.", 'abstract_zh': '基于代理的仿真是模拟复杂人类行为的关键，但传统方法需要广泛的领域知识和大量数据集。在历史和反事实数据有限的数据稀缺医疗环境中，大型语言模型（LLMs）通过利用广泛的背景知识提供了一种有希望的替代方案。本研究探讨了由LLM驱动的母婴移动健康项目仿真，预测受试者在接受自动化消息（对照组）或现场代表（干预组）健康信息时的倾听行为。由于不确定性量化对健康干预决策至关重要，我们提出了一种基于多样本二进制熵的LLM认知不确定性估计方法。通过集成方法增强模型的鲁棒性，与单个模型相比，提高了F1分数和模型校准。除了直接评估，我们采用以决策为导向的方法，展示了LLM预测如何在数据有限的环境中指导干预可行性和试验实施。提出的方法扩展到公共卫生、灾害响应及其他需要在严重数据限制下快速评估干预措施的领域。所有为此工作使用的代码和提示均可在此网址访问。', 'title_zh': '基于LLM的代理模拟在孕产妇健康干预中的应用：不确定性估计与决策导向评估'}
{'arxiv_id': 'arXiv:2503.24379', 'title': 'Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation', 'authors': 'Shengqiong Wu, Weicai Ye, Jiahao Wang, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Shuicheng Yan, Hao Fei, Tat-Seng Chua', 'link': 'https://arxiv.org/abs/2503.24379', 'abstract': 'To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: this https URL', 'abstract_zh': '为了应对当前视频生成社区中准确用户意图解释的瓶颈，我们提出了Any2Caption，这是一种在任何条件下可控视频生成的新框架。其关键思想是将各种条件解释步骤与视频合成步骤解耦。通过利用现代多模态大型语言模型（MLLMs），Any2Caption 将多样化的输入——文本、图像、视频以及区域、运动和相机姿态等专业提示——解释为密集且结构化的字幕，为骨干视频生成器提供更好的指导。我们还介绍了包含337,000个实例和407,000种条件的大规模数据集Any2CapIns，用于任何条件到字幕指令调优。全面的评估证明，我们的系统在多种现有视频生成模型方面显著提高了可控性和视频质量。项目页面：this https URL。', 'title_zh': 'Any2Caption：任意条件到字幕的解释以实现可控视频生成'}
{'arxiv_id': 'arXiv:2503.24377', 'title': 'Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models', 'authors': 'Rui Wang, Hongru Wang, Boyang Xue, Jianhui Pang, Shudong Liu, Yi Chen, Jiahao Qiu, Derek Fai Wong, Heng Ji, Kam-Fai Wong', 'link': 'https://arxiv.org/abs/2503.24377', 'abstract': 'Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.', 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）的最新进展显著增强了其执行复杂推理任务的能力，从快速直观思考（System 1）过渡到慢速深入推理（System 2）。虽然System 2推理可以提高任务准确性，但由于其慢速思考的性质和低效或不必要的推理行为，往往会带来巨大的计算成本。相比之下，System 1推理虽然计算高效，但会导致性能不理想。因此，在性能（效益）和计算成本（预算）之间取得平衡至关重要，从而引出了推理经济学的概念。在这篇综述中，我们对大型语言模型训练后和测试时推理阶段的推理经济学进行了全面分析，涵盖i) 推理低效的原因，ii) 不同推理模式的行为分析，和iii) 实现推理经济学的潜在解决方案。通过提供可操作的见解并突出显示开放挑战，我们旨在揭示提高大型语言模型推理经济学的策略，从而为推进这一不断发展的领域的研究提供有价值资源。我们还提供了一个公开的存储库以持续跟踪这一快速发展的领域的进展。', 'title_zh': '挖掘推理经济效益：大规模语言模型高效推理综述'}
{'arxiv_id': 'arXiv:2503.24376', 'title': 'Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1', 'authors': 'Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, Xihui Liu', 'link': 'https://arxiv.org/abs/2503.24376', 'abstract': "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.", 'abstract_zh': '近期在Chain of Thought (COT) 生成方面的进展显著提高了大型语言模型（LLMs）的推理能力，强化学习（RL）作为后训练方法被证明是有效的方法。多模态大型语言模型（MLLMs）继承了这种推理潜力，但在要求感知和逻辑推理结合的任务中尚未得到充分探索。为解决这一问题，我们提出SEED-Bench-R1基准，旨在系统性地评估MLLMs在视频理解中的后训练方法。该基准包含复杂的实际视频和复杂的日常规划任务，以多项选择题的形式呈现，需要复杂的感知和推理能力。SEED-Bench-R1通过三层层级结构评估泛化能力：同分布、跨环境和跨环境任务场景，并配备了大规模训练数据集和易于验证的答案。使用Qwen2-VL-Instruct-7B作为基础模型，我们比较了RL与监督微调（SFT）的效果， Demonstrating RL的数据效率和在同分布和异分布任务中的优越性能，甚至在长视频基准（LongVideoBench）等一般视频理解基准测试中优于SFT。我们的详细分析表明，RL增强了视觉感知，但往往产生了不那么逻辑连贯的推理链。我们识别了关键限制，包括推理不一致和忽略视觉线索，并建议未来在基础模型推理、奖励建模和RL对抗噪声信号的鲁棒性方面的改进。', 'title_zh': '探究强化学习对视频理解的影响：SEED-Bench-R1的见解'}
{'arxiv_id': 'arXiv:2503.24370', 'title': 'Effectively Controlling Reasoning Models through Thinking Intervention', 'authors': 'Tong Wu, Chong Xiang, Jiachen T. Wang, Prateek Mittal', 'link': 'https://arxiv.org/abs/2503.24370', 'abstract': 'Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.', 'abstract_zh': '增强推理的大语言模型（LLMs）在生成最终答案之前显式地生成中间推理步骤，有助于模型在复杂问题解决方面表现出色。在本文中，我们展示了这种新兴的生成框架为更精细地控制模型行为提供了独特机会。我们提出了思考干预，这是一种新的范式，通过战略性地插入或修订特定的思考令牌来显式地引导LLMs的内部推理过程。我们在多个任务上进行了全面评估，包括IFEval上的指令跟随、SEP上的指令层次结构以及XSTest和SORRY-Bench上的安全性对齐。我们的结果表明，思考干预显著优于基线提示方法，在指令跟随场景中实现了高达6.7%的准确率提升，在关于指令层次结构的推理中实现了15.4%的改进，并在使用开源DeepSeek R1模型处理不确定提示时拒绝率提高了40.0%。总体而言，我们的工作为控制推理LLMs打开了一个有希望的新研究方向。', 'title_zh': '通过思考干预有效控制推理模型'}
{'arxiv_id': 'arXiv:2503.24354', 'title': 'ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion', 'authors': 'Rana Muhammad Shahroz Khan, Dongwen Tang, Pingzhi Li, Kai Wang, Tianlong Chen', 'link': 'https://arxiv.org/abs/2503.24354', 'abstract': 'Parameter generation has emerged as a novel paradigm for neural network development, offering an alternative to traditional neural network training by synthesizing high-quality model weights directly. In the context of Low-Rank Adaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large language models (LLMs), this approach promises efficient adaptation without costly retraining. However, existing methods face critical limitations in simultaneously achieving scalability and controllability. In this paper, we introduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$ framework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel conditioning mechanism that integrates model architecture and textual task specifications, enabling the generation of task-specific LoRA parameters that can seamlessly transfer across evolving foundation models. Our approach successfully scales to billions-of-parameter LLMs and maintains controllability. Through extensive experiments across seven language tasks, four vision tasks, and three multimodal tasks using five pre-trained LLMs, we demonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that achieve comparable or superior performance to vanilla trained counterparts.', 'abstract_zh': '基于条件递归扩散的Low-Rank Adaptation ($\\textit{i.e.}$, 持续更新的大语言模型)参数生成方法', 'title_zh': 'ORAL: 通过条件递归扩散提示您的大规模LoRAs'}
{'arxiv_id': 'arXiv:2503.24310', 'title': 'BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models', 'authors': 'Alok Abhishek, Lisa Erickson, Tushar Bandopadhyay', 'link': 'https://arxiv.org/abs/2503.24310', 'abstract': 'In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models.', 'abstract_zh': '本研究引入了BEATS框架，这是一种评估大型语言模型（LLMs）中的偏见、伦理、公平性和事实性的新颖框架。基于BEATS框架，我们提出了一种针对LLMs的偏见基准，该基准涵盖了29个不同的指标。这些指标涵盖了包括人口统计学、认知和社会偏见在内的广泛特征，以及伦理推理、群体公平性和与虚假信息相关的事实性测量。这些指标使得定量评估LLM生成的响应可能延续社会偏见，从而加剧系统不公平性成为可能。为了在该基准上获得高分，LLM必须在响应中展示出非常公平的行为，使其成为负责任AI评估的严格标准。基于我们实验数据的实证结果表明，37.65%的由行业领先模型生成的输出包含某种形式的偏见，这突显了在关键决策系统中使用这些模型的重大风险。BEATS框架和基准提供了一种可扩展且统计上严谨的方法来评估LLMs，诊断推动偏见的因素，并开发缓解策略。通过BEATS框架，我们的目标是帮助开发更具社会责任心和伦理对齐的AI模型。', 'title_zh': 'BEATS：大型语言模型偏差评估测试套件'}
{'arxiv_id': 'arXiv:2503.24307', 'title': 'A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. RAG', 'authors': 'Arshia Kermani, Veronica Perez-Rosas, Vangelis Metsis', 'link': 'https://arxiv.org/abs/2503.24307', 'abstract': 'This study presents a systematic comparison of three approaches for the analysis of mental health text using large language models (LLMs): prompt engineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA 3, we evaluate these approaches on emotion classification and mental health condition detection tasks across two datasets. Fine-tuning achieves the highest accuracy (91% for emotion classification, 80% for mental health conditions) but requires substantial computational resources and large training sets, while prompt engineering and RAG offer more flexible deployment with moderate performance (40-68% accuracy). Our findings provide practical insights for implementing LLM-based solutions in mental health applications, highlighting the trade-offs between accuracy, computational requirements, and deployment flexibility.', 'abstract_zh': '本研究系统比较了三种使用大规模语言模型（LLMs）分析心理健康文本的方法：提示工程、检索增强生成（RAG）和微调，并使用LLaMA 3在两个数据集中评估了这些方法在情绪分类和心理健康状况检测任务中的性能。微调在准确性方面最高（情绪分类91%，心理健康状况80%），但需要大量的计算资源和大规模的训练集，而提示工程和RAG提供了更具灵活性的部署方案，性能中等（情绪分类40%-68%的准确性）。本研究的结果为在心理健康应用中实施基于LLM的解决方案提供了实用见解，突显了准确性和计算需求与部署灵活性之间的权衡。', 'title_zh': 'LLM策略系统评估：微调 vs. 提示工程 vs. RAG 对于心理健康文本分析'}
{'arxiv_id': 'arXiv:2503.24277', 'title': 'Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality', 'authors': 'Sewoong Lee, Adam Davies, Marc E. Canby, Julia Hockenmaier', 'link': 'https://arxiv.org/abs/2503.24277', 'abstract': 'Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic interpretability, but leading SAE approaches with top-$k$ style activation functions lack theoretical grounding for selecting the hyperparameter $k$. SAEs are based on the linear representation hypothesis (LRH), which assumes that the representations of large language models (LLMs) are linearly encoded, and the superposition hypothesis (SH), which states that there can be more features in the model than its dimensionality. We show that, based on the formal definitions of the LRH and SH, the magnitude of sparse feature vectors (the latent representations learned by SAEs of the dense embeddings of LLMs) can be approximated using their corresponding dense vector with a closed-form error bound. To visualize this, we propose the ZF plot, which reveals a previously unknown relationship between LLM hidden embeddings and SAE feature vectors, allowing us to make the first empirical measurement of the extent to which feature vectors of pre-trained SAEs are over- or under-activated for a given input. Correspondingly, we introduce Approximate Feature Activation (AFA), which approximates the magnitude of the ground-truth sparse feature vector, and propose a new evaluation metric derived from AFA to assess the alignment between inputs and activations. We also leverage AFA to introduce a novel SAE architecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with theoretical justifications; and (b) obviate the need to tune SAE sparsity hyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve reconstruction loss comparable to that of state-of-the-art top-k SAEs, without requiring the hyperparameter $k$ to be tuned. Our code is available at: this https URL.', 'abstract_zh': '稀疏自编码器（SAEs）已成为现代机理可解释性的主力工具，但主流的SAE方法使用顶级-$k$样式激活函数，缺乏选择超参数$k$的理论依据。SAEs基于线性表示假设（LRH），该假设认为大规模语言模型（LLMs）的表示形式是线性编码的，以及超定假设（SH），该假设表明模型中的特征数可以多于其维度。我们基于LRH和SH的形式定义，展示了稀疏特征向量（由SAEs学习的大语言模型密集嵌入的隐式表示）的幅度可以用相应的密集向量进行近似，并带有封闭形式的误差界。为了可视化这一点，我们提出了ZF图，揭示了大规模语言模型隐藏嵌入和SAE特征向量之间的一种未知关系，允许我们首次对预训练SAE的特征向量在给定输入下的过度激活或欠激活程度进行实证测量。相应地，我们引入了近似特征激活（AFA），用于近似地面真值稀疏特征向量的幅度，并提出了一种新的评估指标，该指标源自AFA，用于评估输入和激活之间的对齐情况。我们还利用AFA引入了一种新的SAE架构——顶级-AFA SAE，使得SAE：（a）更加符合理论依据；（b）取消了调整SAE稀疏度超参数的需求。最后，我们实验证明，顶级-AFA SAE在重构损失方面与最先进的顶级-$k$ SAE相当，无需调整超参数$k$。代码可在以下链接获取：this https URL。', 'title_zh': '评估和设计稀疏自编码器：近似准正交性方法'}
{'arxiv_id': 'arXiv:2503.24235', 'title': 'What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models', 'authors': 'Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, Chen Ma', 'link': 'https://arxiv.org/abs/2503.24235', 'abstract': "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.", 'abstract_zh': '随着预训练时代对扩大计算（数据和参数）的热情逐渐减弱，“测试时缩放”（TTS），也称为“测试时计算”已 emerges as a prominent research focus. 近期研究展示了 TTS 能进一步激发大型语言模型（LLMs）的问题解决能力，不仅在数学和编码等专门推理任务中取得了重大突破，还在开放式问答等通用任务中取得了显著成果。然而，尽管该领域的努力有了爆炸式的增长，仍迫切需要一个全面的综述来提供系统性的理解。为填补这一空白，我们提出一个统一的多维度框架，沿着四个核心维度组织 TTS 研究：什么进行缩放、如何进行缩放、在哪里进行缩放以及缩放效果如何。基于这一分类，我们对方法、应用场景和评估方面进行了广泛回顾，并呈现了一个有组织的分解，突出了个体技术在更广泛 TTS 地形中的独特功能角色。通过这一分析，我们提炼了 TTS 到目前为止的主要发展轨迹，并提供了实践部署的手册指南。此外，我们确定了一些开放性的挑战，并提出了有希望的未来方向，包括进一步缩放、澄清技术的功能本质、泛化到更多任务以及更多归因。', 'title_zh': '什么是、如何进行、在哪里以及进行得如何？大规模语言模型测试时缩放综述'}
{'arxiv_id': 'arXiv:2503.24191', 'title': 'Output Constraints as Attack Surface: Exploiting Structured Generation to Bypass LLM Safety Mechanisms', 'authors': 'Shuoming Zhang, Jiacheng Zhao, Ruiyuan Xu, Xiaobing Feng, Huimin Cui', 'link': 'https://arxiv.org/abs/2503.24191', 'abstract': 'Content Warning: This paper may contain unsafe or harmful content generated by LLMs that may be offensive to readers. Large Language Models (LLMs) are extensively used as tooling platforms through structured output APIs to ensure syntax compliance so that robust integration with existing softwares like agent systems, could be achieved. However, the feature enabling functionality of grammar-guided structured output presents significant security vulnerabilities. In this work, we reveal a critical control-plane attack surface orthogonal to traditional data-plane vulnerabilities. We introduce Constrained Decoding Attack (CDA), a novel jailbreak class that weaponizes structured output constraints to bypass safety mechanisms. Unlike prior attacks focused on input prompts, CDA operates by embedding malicious intent in schema-level grammar rules (control-plane) while maintaining benign surface prompts (data-plane). We instantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2% attack success rates across proprietary and open-weight LLMs on five safety benchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our findings identify a critical security blind spot in current LLM architectures and urge a paradigm shift in LLM safety to address control-plane vulnerabilities, as current mechanisms focused solely on data-plane threats leave critical systems exposed.', 'abstract_zh': '内容警告：本论文可能包含由大型语言模型生成的不安全或有害内容，可能令读者感到不适。大型语言模型通过结构化输出API广泛用作工具平台，以确保语法合规性，从而实现与现有软件（如代理系统）的稳健集成。然而，由语法指导的结构化输出功能使安全性面临重大威胁。在本工作中，我们揭示了一个与传统数据平面漏洞正交的关键控制平面攻击面。我们引入了受限解码攻击（CDA），这是一种新的 Jailbreak 类别，利用结构化输出约束绕过安全机制。与先前主要针对输入提示的攻击不同，CDA 通过在方案级语法规则中嵌入恶意意图（控制平面）来运作，同时保持看似 benign 的表面提示（数据平面）。我们通过一个概念证明的链枚举攻击实例化了这一点，该攻击在五个安全基准测试中实现了96.2%的攻击成功率，包括针对专有和开源大型语言模型（如GPT-4o和Gemini-2.0-flash）的单个查询。我们的研究结果揭示了当前大型语言模型架构中的关键安全盲点，并呼吁在大型语言模型安全性方面进行范式转变，以解决控制平面漏洞，因为当前机制仅专注于数据平面威胁，使关键系统面临风险。', 'title_zh': '输出约束作为攻击面：利用结构化生成绕过LLM安全机制'}
{'arxiv_id': 'arXiv:2503.24062', 'title': 'Artificial Conversations, Real Results: Fostering Language Detection with Synthetic Data', 'authors': 'Fatemeh Mohammadi, Tommaso Romano, Samira Maghool, Paolo Ceravolo', 'link': 'https://arxiv.org/abs/2503.24062', 'abstract': 'Collecting high-quality training data is essential for fine-tuning Large Language Models (LLMs). However, acquiring such data is often costly and time-consuming, especially for non-English languages such as Italian. Recently, researchers have begun to explore the use of LLMs to generate synthetic datasets as a viable alternative. This study proposes a pipeline for generating synthetic data and a comprehensive approach for investigating the factors that influence the validity of synthetic data generated by LLMs by examining how model performance is affected by metrics such as prompt strategy, text length and target position in a specific task, i.e. inclusive language detection in Italian job advertisements. Our results show that, in most cases and across different metrics, the fine-tuned models trained on synthetic data consistently outperformed other models on both real and synthetic test datasets. The study discusses the practical implications and limitations of using synthetic data for language detection tasks with LLMs.', 'abstract_zh': '高质量训练数据的收集对于Fine-tuning大型语言模型（LLMs）至关重要。然而，获取此类数据往往成本高昂且耗时，特别是在意大利语等非英语语言领域。最近，研究人员开始探索使用LLMs生成合成数据作为可行的替代方案。本研究提出了一种生成合成数据的管道，并通过评估模型性能受提示策略、文本长度和特定任务中的目标位置等因素的影响，对由LLMs生成的合成数据有效性进行了全面研究，具体任务是在意大利求职广告中检测包容性语言。研究结果表明，在大多数情况下且在不同指标下，使用合成数据训练的Fine-tuned模型在真实和合成测试数据集上的一般表现均优于其他模型。本研究讨论了使用合成数据进行语言检测任务的实用意义及其限制。', 'title_zh': '人工对话，真实成效：利用合成数据促进语言检测'}
{'arxiv_id': 'arXiv:2503.24000', 'title': 'Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving', 'authors': 'Wei Gao, Xinyu Zhou, Peng Sun, Tianwei Zhang, Yonggang Wen', 'link': 'https://arxiv.org/abs/2503.24000', 'abstract': 'Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a promising technique to optimize Large Language Model (LLM) serving. It primarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to reduce the computation cost. Despite the development of many compression algorithms, their applications in production environments are still not prevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache} compression solutions from a practical perspective. Our contributions are three-fold. First, we comprehensively review existing algorithmic designs and benchmark studies for \\texttt{KV} \\texttt{cache} compression and identify missing pieces in their performance measurement, which could hinder their adoption in practice. Second, we empirically evaluate representative \\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that affect the computational efficiency: (1) while compressing \\texttt{KV} \\texttt{cache} can reduce memory consumption, current implementations (e.g., FlashAttention, PagedAttention) do not optimize for production-level LLM serving, resulting in suboptimal throughput performance; (2) compressing \\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased end-to-end latency. We further investigate the accuracy performance of individual samples rather than the overall performance, revealing the intrinsic limitations in \\texttt{KV} \\texttt{cache} compression when handling specific LLM tasks. Third, we provide tools to shed light on future \\texttt{KV} \\texttt{cache} compression studies and facilitate their practical deployment in production. They are open-sourced in \\href{this https URL}{this https URL}.', 'abstract_zh': 'Key-Value 缓存（\\texttt{KV} 缓存）压缩已成为优化大型语言模型（LLM）服务的一种有前途的技术。它主要通过减少\\texttt{KV} 缓存的内存消耗来降低计算成本。尽管已经开发出了许多压缩算法，但它们在生产环境中的应用仍然不够广泛。在本文中，我们从实用的角度回顾了主流的\\texttt{KV} 缓存压缩解决方案。我们的贡献主要体现在三个方面。首先，我们全面回顾了现有的算法设计和基准研究，并识别出它们在性能测量中存在的不足之处，这可能阻碍其实现。其次，我们实证评估了代表性的\\texttt{KV} 缓存压缩方法，发现了影响计算效率的两个关键问题：（1）虽然压缩\\texttt{KV} 缓存可以减少内存消耗，但当前实现（如FlashAttention、PagedAttention）未针对生产级别的LLM服务进行优化，导致吞吐量性能欠佳；（2）压缩\\texttt{KV} 缓存可能会导致输出时间延长，从而增加端到端延迟。我们进一步研究了单个样本的准确性性能，揭示了特定LLM任务处理中\\texttt{KV} 缓存压缩的内在限制。第三，我们提供了工具，以促进未来\\texttt{KV} 缓存压缩研究，并使其在生产环境中得以实际应用。这些工具已在\\href{this https URL}{this https URL}开源。', 'title_zh': '重新思考大型语言模型服务中键值缓存压缩技术的方法'}
{'arxiv_id': 'arXiv:2503.23989', 'title': 'Rubric Is All You Need: Enhancing LLM-based Code Evaluation With Question-Specific Rubrics', 'authors': 'Aditya Pathak, Rachit Gandhi, Vaibhav Uttam, Devansh, Yashwanth Nakka, Aaryan Raj Jindal, Pratyush Ghosh, Arnav Ramamoorthy, Shreyash Verma, Aditya Mittal, Aashna Ased, Chirag Khatri, Jagat Sesh Challa, Dhruv Kumar', 'link': 'https://arxiv.org/abs/2503.23989', 'abstract': "Since the disruption in LLM technology brought about by the release of GPT-3 and ChatGPT, LLMs have shown remarkable promise in programming-related tasks. While code generation remains a popular field of research, code evaluation using LLMs remains a problem with no conclusive solution. In this paper, we focus on LLM-based code evaluation and attempt to fill in the existing gaps. We propose multi-agentic novel approaches using question-specific rubrics tailored to the problem statement, arguing that these perform better for logical assessment than the existing approaches that use question-agnostic rubrics. To address the lack of suitable evaluation datasets, we introduce two datasets: a Data Structures and Algorithms dataset containing 150 student submissions from a popular Data Structures and Algorithms practice website, and an Object Oriented Programming dataset comprising 80 student submissions from undergraduate computer science courses. In addition to using standard metrics (Spearman Correlation, Cohen's Kappa), we additionally propose a new metric called as Leniency, which quantifies evaluation strictness relative to expert assessment. Our comprehensive analysis demonstrates that question-specific rubrics significantly enhance logical assessment of code in educational settings, providing better feedback aligned with instructional goals beyond mere syntactic correctness.", 'abstract_zh': '自GPT-3和ChatGPT发布以来，LLM技术在编程相关任务中显示出显著的潜力。尽管代码生成仍然是一个热门的研究领域，但使用LLM进行代码评估仍然是一个没有明确解决方案的问题。本文聚焦于基于LLM的代码评估，试图填补现有空白。我们提出了基于多代理的新颖方法，并针对问题陈述量身定制评分标准，认为这些方法在逻辑评估方面优于现有使用通用评分标准的方法。为了解决合适的评估数据集缺乏的问题，我们引入了两个数据集：一个包含来自流行的数据结构和算法练习网站的150份学生提交的数据结构和算法数据集，以及一个包含来自本科计算机科学课程的80份学生提交的对象导向编程数据集。除使用标准指标（Spearman相关系数和Cohen’s Kappa系数）外，我们还提出了一种新的指标称为宽容度（Leniency），用于量化评估的严格程度与专家评估的差异。我们的综合分析表明，针对问题的评分标准在教育环境中显著增强了代码的逻辑评估，提供了与教学目标更一致的反馈，远超单纯的语法正确性。', 'title_zh': '评分标准即所需：基于问答特定评分标准提升大语言模型代码评价能力'}
{'arxiv_id': 'arXiv:2503.23895', 'title': 'Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement', 'authors': 'Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu', 'link': 'https://arxiv.org/abs/2503.23895', 'abstract': 'Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at this https URL.', 'abstract_zh': '动态参数增强检索生成（DyPRAG）：一种轻量级参数转换模型驱动的知识增强框架', 'title_zh': '更有智慧胜过财富：动态参数检索增强生成在测试时知识增强'}
{'arxiv_id': 'arXiv:2503.23886', 'title': 'SchemaAgent: A Multi-Agents Framework for Generating Relational Database Schema', 'authors': 'Qin Wang, Youhuan Li, Yansong Feng, Si Chen, Ziming Li, Pan Zhang, Zhichao Shi, Yuequn Dou, chuchu Gao, Zebin Huang, Zihui Si, Yixuan Chen, Zhaohai Sun, Ke Tang, Wenqiang Jin', 'link': 'https://arxiv.org/abs/2503.23886', 'abstract': "The relational database design would output a schema based on user's requirements, which defines table structures and their interrelated relations. Translating requirements into accurate schema involves several non-trivial subtasks demanding both database expertise and domain-specific knowledge. This poses unique challenges for automated design of relational databases. Existing efforts are mostly based on customized rules or conventional deep learning models, often producing suboptimal schema. Recently, large language models (LLMs) have significantly advanced intelligent application development across various domains. In this paper, we propose SchemaAgent, a unified LLM-based multi-agent framework for the automated generation of high-quality database schema. SchemaAgent is the first to apply LLMs for schema generation, which emulates the workflow of manual schema design by assigning specialized roles to agents and enabling effective collaboration to refine their respective subtasks. Schema generation is a streamlined workflow, where directly applying the multi-agent framework may cause compounding impact of errors. To address this, we incorporate dedicated roles for reflection and inspection, alongside an innovative error detection and correction mechanism to identify and rectify issues across various phases. For evaluation, we present a benchmark named \\textit{RSchema}, which contains more than 500 pairs of requirement description and schema. Experimental results on this benchmark demonstrate the superiority of our approach over mainstream LLMs for relational database schema generation.", 'abstract_zh': '基于关系数据库的设计将根据用户需求输出一个模式，定义表结构及其相互关系。将需求转换为准确的模式涉及多个非平凡的子任务，既需要数据库专业知识，也需特定领域的知识。这为关系数据库的自动化设计带来了独特的挑战。现有的努力大多基于定制规则或传统的深度学习模型，常常生成次优化的模式。最近，大型语言模型（LLMs）在各领域智能应用开发中取得了显著进展。在本文中，我们提出SchemaAgent，一个统一的基于LLM的多智能体框架，用于自动化生成高质量的数据库模式。SchemaAgent是首次将LLM应用于模式生成，通过为智能体分配特定角色并实现有效的协作来模拟手动模式设计的流程，精炼各自子任务。模式生成是一个简化的工作流，直接应用多智能体框架可能导致错误累积的影响。为此，我们引入了专门的角色进行反思和检查，并结合一种创新的错误检测和校正机制，以跨各个阶段识别和修正问题。为了评估，我们提出了一个名为“RSchema”的基准，包含超过500对需求描述和模式。在此基准上的实验结果表明，我们的方法在关系数据库模式生成方面优于主流的LLM。', 'title_zh': 'SchemaAgent：一个生成关系数据库模式的多智能体框架'}
{'arxiv_id': 'arXiv:2503.23830', 'title': 'OrchMLLM: Orchestrate Multimodal Data with Batch Post-Balancing to Accelerate Multimodal Large Language Model Training', 'authors': 'Yijie Zheng, Bangjun Xiao, Lei Shi, Xiaoyang Li, Faming Wu, Tianyu Li, Xuefeng Xiao, Yang Zhang, Yuxuan Wang, Shouda Liu', 'link': 'https://arxiv.org/abs/2503.23830', 'abstract': 'Multimodal large language models (MLLMs), such as GPT-4o, are garnering significant attention. During the exploration of MLLM training, we identified Modality Composition Incoherence, a phenomenon that the proportion of a certain modality varies dramatically across different examples. It exacerbates the challenges of addressing mini-batch imbalances, which lead to uneven GPU utilization between Data Parallel (DP) instances and severely degrades the efficiency and scalability of MLLM training, ultimately affecting training speed and hindering further research on MLLMs.\nTo address these challenges, we introduce OrchMLLM, a comprehensive framework designed to mitigate the inefficiencies in MLLM training caused by Modality Composition Incoherence. First, we propose Batch Post-Balancing Dispatcher, a technique that efficiently eliminates mini-batch imbalances in sequential data. Additionally, we integrate MLLM Global Orchestrator into the training framework to orchestrate multimodal data and tackle the issues arising from Modality Composition Incoherence. We evaluate OrchMLLM across various MLLM sizes, demonstrating its efficiency and scalability. Experimental results reveal that OrchMLLM achieves a Model FLOPs Utilization (MFU) of $41.6\\%$ when training an 84B MLLM with three modalities on $2560$ H100 GPUs, outperforming Megatron-LM by up to $3.1\\times$ in throughput.', 'abstract_zh': '多模态大型语言模型（MLLMs）如GPT-4o正引起广泛关注。在探索MLLM训练过程中，我们识别出模态组成不一致性现象，即某些模态的比例在不同示例中急剧变化。这加剧了小批次不平衡的挑战，导致数据并行（DP）实例间的GPU利用率不均衡，严重降低了MLLM训练的效率和可扩展性，最终影响训练速度并阻碍进一步的MLLM研究。\n\n为应对这些挑战，我们提出了OrchMLLM，这是一种全面的框架，旨在缓解由模态组成不一致性引起的MLLM训练效率低下问题。首先，我们提出批后不平衡调度器，一种高效消除序列数据中小批次不平衡的技术。此外，我们将MLLM全局协调器集成到训练框架中，协调多模态数据并解决由模态组成不一致性引起的问题。我们在多种MLLM规模上评估了OrchMLLM，展示了其效率和可扩展性。实验结果表明，当使用2560个H100 GPU训练一个包含三种模态的84B MLLM时，OrchMLLM的模型FLOPs利用率（MFU）达到41.6%，吞吐量最高可比Megatron-LM提高3.1倍。', 'title_zh': 'OrchMLLM: 采用批量后均衡技术 orchestrating 多模态数据以加速多模态大型语言模型训练'}
{'arxiv_id': 'arXiv:2503.23803', 'title': 'Thinking Longer, Not Larger: Enhancing Software Engineering Agents via Scaling Test-Time Compute', 'authors': 'Yingwei Ma, Binhua Li, Yihong Dong, Xue Jiang, Rongyu Cao, Jue Chen, Fei Huang, Yongbin Li', 'link': 'https://arxiv.org/abs/2503.23803', 'abstract': 'Recent advancements in software engineering agents have demonstrated promising capabilities in automating program improvements. However, their reliance on closed-source or resource-intensive models introduces significant deployment challenges in private environments, prompting a critical question: \\textit{How can personally deployable open-source LLMs achieve comparable code reasoning performance?}\nTo this end, we propose a unified Test-Time Compute scaling framework that leverages increased inference-time computation instead of larger models. Our framework incorporates two complementary strategies: internal TTC and external TTC. Internally, we introduce a \\textit{development-contextualized trajectory synthesis} method leveraging real-world software repositories to bootstrap multi-stage reasoning processes, such as fault localization and patch generation. We further enhance trajectory quality through rejection sampling, rigorously evaluating trajectories along accuracy and complexity. Externally, we propose a novel \\textit{development-process-based search} strategy guided by reward models and execution verification. This approach enables targeted computational allocation at critical development decision points, overcoming limitations of existing "end-point only" verification methods.\nEvaluations on SWE-bench Verified demonstrate our \\textbf{32B model achieves a 46\\% issue resolution rate}, surpassing significantly larger models such as DeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical validation of the test-time scaling phenomenon within SWE agents, revealing that \\textbf{models dynamically allocate more tokens to increasingly challenging problems}, effectively enhancing reasoning capabilities. We publicly release all training data, models, and code to facilitate future research. this https URL', 'abstract_zh': '近年来，软件工程代理在自动化程序改进方面展示了令人瞩目的能力。然而，它们对闭源或资源密集型模型的依赖性在私有环境中引发了重要的部署挑战，促使我们提出一个关键问题：\\textit{如何实现可个人部署的开源大语言模型以获得相当的代码推理性能？}\n\n为此，我们提出一种统一的测试时计算扩展框架，该框架利用增加的推理时计算量而不是更大的模型。我们的框架包含两种互补策略：内部 TTC 和外部 TTC。内部，我们引入一种基于实际软件仓库的开发上下文轨迹合成方法，以启动多阶段推理过程，如故障定位和补丁生成。我们进一步通过拒绝采样提高轨迹质量，并严格评估其准确性和复杂性。外部，我们提出了一种基于开发过程的新型搜索策略，该策略受到奖励模型和执行验证的引导。这种方法能够在关键的开发决策点进行有针对性的计算分配，克服了现有“端点验证”方法的局限性。\n\n在 SWE-bench 验证上，我们的 \\textbf{32B 模型实现了46%的问题解决率}，显著超过了如 DeepSeek R1 671B 和 OpenAI o1 这样的更大模型。此外，我们还在 SWE 代理中提供了测试时计算扩展现象的实证验证，揭示了模型如何动态地将更多令牌分配给越来越具有挑战性的问题，从而有效提升推理能力。我们公开发布所有训练数据、模型和代码，以促进未来的研究。更多详情请参见：此链接。', 'title_zh': '思考更久，而不是更大：通过扩展测试时计算提升软件工程代理'}
{'arxiv_id': 'arXiv:2503.23798', 'title': 'Adaptive Layer-skipping in Pre-trained LLMs', 'authors': 'Xuan Luo, Weizhi Wang, Xifeng Yan', 'link': 'https://arxiv.org/abs/2503.23798', 'abstract': "Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration.", 'abstract_zh': 'FlexiDepth: 动态调整Transformer层数以适应不同tokens的计算需求', 'title_zh': '预训练大规模语言模型中的自适应层跳过技术支持'}
{'arxiv_id': 'arXiv:2503.23740', 'title': 'LANID: LLM-assisted New Intent Discovery', 'authors': 'Lu Fan, Jiashu Pu, Rongsheng Zhang, Xiao-Ming Wu', 'link': 'https://arxiv.org/abs/2503.23740', 'abstract': 'Task-oriented Dialogue Systems (TODS) often face the challenge of encountering new intents. New Intent Discovery (NID) is a crucial task that aims to identify these novel intents while maintaining the capability to recognize existing ones. Previous efforts to adapt TODS to new intents have struggled with inadequate semantic representation or have depended on external knowledge, which is often not scalable or flexible. Recently, Large Language Models (LLMs) have demonstrated strong zero-shot capabilities; however, their scale can be impractical for real-world applications that involve extensive queries. To address the limitations of existing NID methods by leveraging LLMs, we propose LANID, a framework that enhances the semantic representation of lightweight NID encoders with the guidance of LLMs. Specifically, LANID employs the $K$-nearest neighbors and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithms to sample selective utterance pairs from the training set. It then queries an LLM to ascertain the relationships between these pairs. The data produced from this process is utilized to design a contrastive fine-tuning task, which is then used to train a small encoder with a contrastive triplet loss. Our experimental results demonstrate the efficacy of the proposed method across three distinct NID datasets, surpassing strong baselines in both unsupervised and semi-supervised settings. Our code is available at this https URL.', 'abstract_zh': '基于任务的对话系统中新意图发现（LANID）', 'title_zh': 'LANID: LLM-assisted新型意图发现'}
{'arxiv_id': 'arXiv:2503.23573', 'title': 'DASH: Detection and Assessment of Systematic Hallucinations of VLMs', 'authors': 'Maximilian Augustin, Yannic Neuhaus, Matthias Hein', 'link': 'https://arxiv.org/abs/2503.23573', 'abstract': "Vision-language models (VLMs) are prone to object hallucinations, where they erroneously indicate the presenceof certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assessment of Systematic Hallucinations), an automatic, large-scale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. A key component is DASH-OPT for image-based retrieval, where we optimize over the ''natural image manifold'' to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations. Code and data are available at this https URL.", 'abstract_zh': 'Vision-language模型中的系统性幻觉检测与评估（DASH）', 'title_zh': 'DASH: 系统幻觉检测与评估'}
{'arxiv_id': 'arXiv:2503.23514', 'title': 'If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong Learning in LLMs', 'authors': 'Siqi Fan, Xiusheng Huang, Yiqun Yao, Xuezhi Fang, Kang Liu, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang', 'link': 'https://arxiv.org/abs/2503.23514', 'abstract': "Large language models (LLMs) can carry out human-like dialogue, but unlike humans, they are stateless due to the superposition property. However, during multi-turn, multi-agent interactions, LLMs begin to exhibit consistent, character-like behaviors, hinting at a form of emergent lifelong learning. Despite this, existing benchmarks often fail to capture these dynamics, primarily focusing on static, open-ended evaluations. To address this gap, we introduce LIFESTATE-BENCH, a benchmark designed to assess lifelong learning in LLMs. It features two episodic datasets: Hamlet and a synthetic script collection, rich in narrative structure and character interactions. Our fact checking evaluation probes models' self-awareness, episodic memory retrieval, and relationship tracking, across both parametric and non-parametric approaches. Experiments on models like Llama3.1-8B, GPT-4-turbo, and DeepSeek R1, we demonstrate that nonparametric methods significantly outperform parametric ones in managing stateful learning. However, all models exhibit challenges with catastrophic forgetting as interactions extend, highlighting the need for further advancements in lifelong learning.", 'abstract_zh': 'Large Language Models (LLMs) 的长生命周期学习评估基准：LIFESTATE-BENCH', 'title_zh': '如果语言模型是一个角色，它会知道自己自己的故事吗？评估语言模型的终身学习能力'}
{'arxiv_id': 'arXiv:2503.23483', 'title': 'Order Independence With Finetuning', 'authors': 'Katrina Brown, Reid McIlroy', 'link': 'https://arxiv.org/abs/2503.23483', 'abstract': 'Large language models (LLMs) demonstrate remarkable performance on many NLP tasks, yet often exhibit order dependence: simply reordering semantically identical tokens (e.g., answer choices in multiple-choice questions) can lead to inconsistent predictions. Recent work proposes Set-Based Prompting (SBP) as a way to remove order information from designated token subsets, thereby mitigating positional biases. However, applying SBP on base models induces an out-of-distribution input format, which can degrade in-distribution performance. We introduce a fine-tuning strategy that integrates SBP into the training process, "pulling" these set-formatted prompts closer to the model\'s training manifold. We show that SBP can be incorporated into a model via fine-tuning. Our experiments on in-distribution (MMLU) and out-of-distribution (CSQA, ARC Challenge) multiple-choice tasks show that SBP fine-tuning significantly improves accuracy and robustness to answer-order permutations, all while preserving broader language modeling capabilities. We discuss the broader implications of order-invariant modeling and outline future directions for building fairer, more consistent LLMs.', 'abstract_zh': '基于集的提示微调：提高大型语言模型的顺序不变性能', 'title_zh': '微调的顺序无关性'}
{'arxiv_id': 'arXiv:2503.23448', 'title': 'Semantic-Preserving Transformations as Mutation Operators: A Study on Their Effectiveness in Defect Detection', 'authors': 'Max Hort, Linas Vidziunas, Leon Moonen', 'link': 'https://arxiv.org/abs/2503.23448', 'abstract': "Recent advances in defect detection use language models. Existing works enhanced the training data to improve the models' robustness when applied to semantically identical code (i.e., predictions should be the same). However, the use of semantically identical code has not been considered for improving the tools during their application - a concept closely related to metamorphic testing.\nThe goal of our study is to determine whether we can use semantic-preserving transformations, analogue to mutation operators, to improve the performance of defect detection tools in the testing stage. We first collect existing publications which implemented semantic-preserving transformations and share their implementation, such that we can reuse them. We empirically study the effectiveness of three different ensemble strategies for enhancing defect detection tools. We apply the collected transformations on the Devign dataset, considering vulnerabilities as a type of defect, and two fine-tuned large language models for defect detection (VulBERTa, PLBART). We found 28 publications with 94 different transformations.\nWe choose to implement 39 transformations from four of the publications, but a manual check revealed that 23 out 39 transformations change code semantics. Using the 16 remaining, correct transformations and three ensemble strategies, we were not able to increase the accuracy of the defect detection models. Our results show that reusing shared semantic-preserving transformation is difficult, sometimes even causing wrongful changes to the semantics.\nKeywords: defect detection, language model, semantic-preserving transformation, ensemble", 'abstract_zh': 'Recent advances in 缺陷检测使用语言模型：基于语义保留变换的性能提升研究', 'title_zh': '语义保留变换作为变异操作符：其在缺陷检测中的有效性研究'}
{'arxiv_id': 'arXiv:2503.23439', 'title': 'Speculative End-Turn Detector for Efficient Speech Chatbot Assistant', 'authors': 'Hyunjong Ok, Suho Yoo, Jaeho Lee', 'link': 'https://arxiv.org/abs/2503.23439', 'abstract': 'Spoken dialogue systems powered by large language models have demonstrated remarkable abilities in understanding human speech and generating appropriate spoken responses. However, these systems struggle with end-turn detection (ETD) -- the ability to distinguish between user turn completion and hesitation. This limitation often leads to premature or delayed responses, disrupting the flow of spoken conversations. In this paper, we introduce the ETD Dataset, the first public dataset for end-turn detection. The ETD dataset consists of both synthetic speech data generated with text-to-speech models and real-world speech data collected from web sources. We also propose SpeculativeETD, a novel collaborative inference framework that balances efficiency and accuracy to improve real-time ETD in resource-constrained environments. Our approach jointly employs a lightweight GRU-based model, which rapidly detects the non-speaking units in real-time on local devices, and a high-performance Wav2vec-based model running on the server to make a more challenging classification of distinguishing turn ends from mere pauses. Experiments demonstrate that the proposed SpeculativeETD significantly improves ETD accuracy while keeping the required computations low. Datasets and code will be available after the review.', 'abstract_zh': '由大规模语言模型驱动的对话系统在理解人类语音和生成适当语音响应方面展现了非凡的能力。然而，这些系统在结束轮检测（ETD）——区分用户发言结束和犹豫的能力——方面存在局限性。这一局限常常导致提前或延迟的应答，破坏了对话的流畅性。本文介绍了ETD数据集，这是首个公开的结束轮检测数据集。ETD数据集包含由文本到语音模型生成的合成语音数据和从网络来源收集的真实语音数据。同时，我们提出了SpeculativeETD，这是一种新颖的协作推理框架，通过平衡效率与准确性来提高受限资源环境下实时结束轮检测的性能。我们的方法结合使用了一个轻量级的基于GRU的模型，在本地设备上实时快速检测非说话单位，以及一个高性能的Wav2vec模型在服务器上运行，进行更具有挑战性的区分发言结束和简单停顿的分类。实验表明，提出的SpeculativeETD在保持所需计算量低的同时，显著提高了结束轮检测的准确性。数据集和代码将在审稿通过后提供。', 'title_zh': 'speculation-end-turn 侦测器：高效的语音聊天机器人助手'}
{'arxiv_id': 'arXiv:2503.23415', 'title': 'An Analysis of Decoding Methods for LLM-based Agents for Faithful Multi-Hop Question Answering', 'authors': 'Alexander Murphy, Mohd Sanad Zaki Rizvi, Aden Haussmann, Ping Nie, Guifu Liu, Aryo Pradipta Gema, Pasquale Minervini', 'link': 'https://arxiv.org/abs/2503.23415', 'abstract': 'Large Language Models (LLMs) frequently produce factually inaccurate outputs - a phenomenon known as hallucination - which limits their accuracy in knowledge-intensive NLP tasks. Retrieval-augmented generation and agentic frameworks such as Reasoning and Acting (ReAct) can address this issue by giving the model access to external knowledge. However, LLMs often fail to remain faithful to retrieved information. Mitigating this is critical, especially if LLMs are required to reason about the retrieved information. Recent research has explored training-free decoding strategies to improve the faithfulness of model generations. We present a systematic analysis of how the combination of the ReAct framework and decoding strategies (i.e., DeCoRe, DoLa, and CAD) can influence the faithfulness of LLM-generated answers. Our results show that combining an agentic framework for knowledge retrieval with decoding methods that enhance faithfulness can increase accuracy on the downstream Multi-Hop Question Answering tasks. For example, we observe an F1 increase from 19.5 to 32.6 on HotpotQA when using ReAct and DoLa.', 'abstract_zh': '大型语言模型（LLMs）经常生成事实不准确的输出——这一现象被称为幻觉，这限制了它们在知识密集型NLP任务中的准确性。检索增强生成和Reasoning and Acting（ReAct）等有能动性框架可以通过给予模型访问外部知识的能力来解决这一问题。然而，LLMs往往未能忠实地保留检索到的信息。减轻这一问题至关重要，尤其是在LLMs需要推理检索到的信息时。最近的研究探索了无需训练的解码策略以提高模型生成的忠实性。我们对结合ReAct框架与增强忠实性的解码方法（即DeCoRe、DoLa和CAD）如何影响LLM生成答案的忠实性进行了系统分析。结果显示，结合知识检索的有能动性框架与增强忠实性的解码方法可以在下游多跳问答任务中提高准确性。例如，我们观察到在使用ReAct和DoLa时，HotpotQA的F1分数从19.5提高到32.6。', 'title_zh': '基于LLM的代理多跳问答忠实解码方法分析'}
{'arxiv_id': 'arXiv:2503.23395', 'title': 'Scaling Auditory Cognition via Test-Time Compute in Audio Language Models', 'authors': 'Ting Dang, Yan Gao, Hong Jia', 'link': 'https://arxiv.org/abs/2503.23395', 'abstract': 'Large language models (LLMs) have shown exceptional versatility in natural language processing, prompting recent efforts to extend their multimodal capabilities to speech processing through the development of audio large language models (Audio LLMs). While Audio LLMs excel in tasks such as speech recognition and synthesis, it remains unclear how they perform when faced with the auditory cognitive challenges posed by real-world environments, such as audio comprehension and listening recall, particularly in the presence of background noise or overlapping speech. Unlike text-based LLMs, which have access to vast amounts of text data for pre-training, retraining Audio LLMs with diverse auditory cognitive scenes is difficult due to the limited datasets that simulate real-world auditory cognitive scenarios and the challenge of acquiring auditory cognitive labels for training. While test-time compute (TTC) methods have been shown to enhance the capabilities of text-based LLMs during inference, a key challenge lies in designing these TTC methods to improve the auditory capabilities of Audio LLMs. This study aims to address these two research gaps by: i) exploring the auditory cognitive capabilities of Audio LLMs, and ii) enhancing their capabilities using TTC approaches. We have investigated five different Audio LLMs for auditory cognition using a \\textit{self-collected} database and have proposed five TTC approaches to enhance auditory cognitive capabilities during inference. Our findings reveal that Audio LLMs performance decreases in more challenging auditory cognitive tasks. The proposed TTC approaches significantly enhance cognitive auditory capabilities, advancing the development of more adaptable and resilient Audio LLMs for practical applications such as assistive listening devices, voice-based AI assistants, and communication technologies.', 'abstract_zh': '大型语言模型（LLMs）在自然语言处理中展示了非凡的灵活性，推动了通过开发音频大型语言模型（Audio LLMs）来扩展其多模态能力以应用于语音处理的努力。尽管Audio LLMs在语音识别和合成任务中表现出色，但在面临真实世界环境中的听觉认知挑战，如音频理解和听觉记忆，尤其是存在背景噪声或重叠语音的情况下，它们的性能尚不清楚。与具备大量文本数据用于预训练和重新训练的文本基于LLMs不同，由于模拟真实世界听觉认知场景的数据集有限以及获取训练所需的听觉认知标签的挑战，重新训练Audio LLMs具有多样性听觉认知场景非常困难。虽然测试时计算（TTC）方法已经被证明可以在推理过程中增强文本基于LLMs的能力，但关键的挑战在于设计这些TTC方法以改善Audio LLMs的听觉能力。本研究旨在通过以下方式解决这两个研究空白：i) 探索Audio LLMs的听觉认知能力，ii) 使用TTC方法增强其能力。我们使用一个自收集的数据库研究了五种不同的Audio LLMs在听觉认知中的应用，并提出了五种TTC方法以在推理过程中增强听觉认知能力。我们的研究发现表明，Audio LLMs在更复杂的听觉认知任务中的表现下降。所提出的方法显著提高了听觉认知能力，推动了适用于辅助听力设备、基于语音的人工智能助手和通信技术等实际应用的更适应性和鲁棒性Audio LLMs的发展。', 'title_zh': '通过测试时计算提升音频语言模型中的听觉认知'}
{'arxiv_id': 'arXiv:2503.23371', 'title': 'FeRG-LLM : Feature Engineering by Reason Generation Large Language Models', 'authors': 'Jeonghyun Ko, Gyeongyun Park, Donghoon Lee, Kyunam Lee', 'link': 'https://arxiv.org/abs/2503.23371', 'abstract': "One of the key tasks in machine learning for tabular data is feature engineering. Although it is vital for improving the performance of models, it demands considerable human expertise and deep domain knowledge, making it labor-intensive endeavor. To address this issue, we propose a novel framework, \\textbf{FeRG-LLM} (\\textbf{Fe}ature engineering by \\textbf{R}eason \\textbf{G}eneration \\textbf{L}arge \\textbf{L}anguage \\textbf{M}odels), a large language model designed to automatically perform feature engineering at an 8-billion-parameter scale. We have constructed two-stage conversational dialogues that enable language models to analyze machine learning tasks and discovering new features, exhibiting their Chain-of-Thought (CoT) capabilities. We use these dialogues to fine-tune Llama 3.1 8B model and integrate Direct Preference Optimization (DPO) to receive feedback improving quality of new features and the model's performance. Our experiments show that FeRG-LLM performs comparably to or better than Llama 3.1 70B on most datasets, while using fewer resources and achieving reduced inference time. It outperforms other studies in classification tasks and performs well in regression tasks. Moreover, since it does not rely on cloud-hosted LLMs like GPT-4 with extra API costs when generating features, it can be deployed locally, addressing security concerns.", 'abstract_zh': '一种用于表格数据的机器学习的关键任务是特征工程。尽管特征工程对于提高模型性能至关重要，但它需要大量的专业知识和深入的领域知识，使得这一过程耗时且劳动密集。为了解决这一问题，我们提出了一种新的框架——FeRG-LLM（Feature engineering by Reason Generation Large Language Models），这是一种大型语言模型，旨在以80亿参数规模自动进行特征工程。我们构建了两阶段的对话流程，使语言模型能够分析机器学习任务并发现新特征，展示了其链式推理（Chain-of-Thought, CoT）能力。我们使用这些对话流程对Llama 3.1 8B模型进行微调，并集成直接偏好优化（DPO）以获得反馈，提高新特征的质量和模型的性能。实验结果显示，FeRG-LLM在大多数数据集上的性能与Llama 3.1 70B相当或更好，同时使用较少资源并缩短了推理时间。在分类任务中，FeRG-LLM优于其他研究，在回归任务中表现良好。此外，由于它不需要依赖于像GPT-4这样的云托管大型语言模型来生成特征（后者会产生额外的API成本），因此它可以本地部署，解决了安全性问题。', 'title_zh': 'FeRG-LLM：基于推理生成的特征工程大型语言模型'}
{'arxiv_id': 'arXiv:2503.23362', 'title': 'Mixture of Routers', 'authors': 'Jia-Chen Zhang, Yu-Jie Xiong, Xi-He Qiu, Chun-Ming Xia, Fei Dai', 'link': 'https://arxiv.org/abs/2503.23362', 'abstract': 'Supervised fine-tuning (SFT) is a milestone in aligning large language models with human instructions and adapting them to downstream tasks. In particular, Low-Rank Adaptation (LoRA) has gained widespread attention due to its parameter efficiency. However, its impact on improving the performance of large models remains limited. Recent studies suggest that combining LoRA with Mixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE adapts to the diversity and complexity of datasets by dynamically selecting the most suitable experts, thereby improving task accuracy and efficiency. Despite impressive results, recent studies reveal issues in the MoE routing mechanism, such as incorrect assignments and imbalanced expert allocation. Inspired by the principles of Redundancy and Fault Tolerance Theory. We innovatively integrate the concept of Mixture of Experts into the routing mechanism and propose an efficient fine-tuning method called Mixture of Routers (MoR). It employs multiple sub-routers for joint selection and uses a learnable main router to determine the weights of the sub-routers. The results show that MoR outperforms baseline models on most tasks, achieving an average performance improvement of 1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method suitable for a wide range of applications. Our code is available here: this https URL.', 'abstract_zh': '监督微调（SFT）是将大规模语言模型与人类指令对齐并适应下游任务的重要里程碑。特别地，低秩适应（LoRA）因其参数效率赢得了广泛关注，但其对提高大规模模型性能的影响仍然有限。最新研究表明，将LoRA与Mixture-of-Experts（MoE）结合可以显著提升微调性能。MoE通过动态选择最合适的专家来适应数据集的多样性和复杂性，从而提高任务准确性和效率。尽管取得了令人印象深刻的成果，但最近的研究揭示了MoE路由机制中的问题，如不正确的分配和专家分配不平衡。受冗余和容错理论原则的启发，我们创新地将专家混合的概念融入路由机制，提出了一种高效的微调方法——Router混合（MoR）。该方法使用多个子路由器进行联合选择，并通过一个可学习的主路由器来确定子路由器的权重。结果表明，MoR在大多数任务上优于基线模型，平均性能提升1%。MoR可以作为插拔式、参数高效的微调方法，适用于广泛的应用场景。我们的代码可在以下链接获取：this https URL。', 'title_zh': '路由器混合体'}
{'arxiv_id': 'arXiv:2503.23281', 'title': 'Extracting Patient History from Clinical Text: A Comparative Study of Clinical Large Language Models', 'authors': 'Hieu Nghiem, Tuan-Dung Le, Suhao Chen, Thanh Thieu, Andrew Gin, Ellie Phuong Nguyen, Dursun Delen, Johnson Thomas, Jivan Lamichhane, Zhuqi Miao', 'link': 'https://arxiv.org/abs/2503.23281', 'abstract': "Extracting medical history entities (MHEs) related to a patient's chief complaint (CC), history of present illness (HPI), and past, family, and social history (PFSH) helps structure free-text clinical notes into standardized EHRs, streamlining downstream tasks like continuity of care, medical coding, and quality metrics. Fine-tuned clinical large language models (cLLMs) can assist in this process while ensuring the protection of sensitive data via on-premises deployment. This study evaluates the performance of cLLMs in recognizing CC/HPI/PFSH-related MHEs and examines how note characteristics impact model accuracy. We annotated 1,449 MHEs across 61 outpatient-related clinical notes from the MTSamples repository. To recognize these entities, we fine-tuned seven state-of-the-art cLLMs. Additionally, we assessed the models' performance when enhanced by integrating, problems, tests, treatments, and other basic medical entities (BMEs). We compared the performance of these models against GPT-4o in a zero-shot setting. To further understand the textual characteristics affecting model accuracy, we conducted an error analysis focused on note length, entity length, and segmentation. The cLLMs showed potential in reducing the time required for extracting MHEs by over 20%. However, detecting many types of MHEs remained challenging due to their polysemous nature and the frequent involvement of non-medical vocabulary. Fine-tuned GatorTron and GatorTronS, two of the most extensively trained cLLMs, demonstrated the highest performance. Integrating pre-identified BME information improved model performance for certain entities. Regarding the impact of textual characteristics on model performance, we found that longer entities were harder to identify, note length did not correlate with a higher error rate, and well-organized segments with headings are beneficial for the extraction.", 'abstract_zh': '提取与患者主诉(CC)、现病史(HPI)及既往史、家族史和社会史(PFSH)相关的医疗历史实体(MHEs)，有助于将自由文本临床笔记结构化为标准化电子病历(EHRs)，简化诸如延续护理、医疗编码和质量指标等下游任务。针对保护敏感数据，通过本地部署使用的细调临床大型语言模型(cLLMs)可以在这一过程中提供帮助。本研究评估了cLLMs在识别CC/HPI/PFSH相关的MHEs方面的能力，并检查了笔记特征如何影响模型准确性。我们对来自MTSamples数据仓库的61份与门诊相关的1,449个MHEs进行了标注。为了识别这些实体，我们细调了七个最先进的cLLMs。此外，我们还评估了通过结合问题、检查、治疗和其他基本医疗实体(BMEs)增强模型的表现。我们在零样本设置下将这些模型的性能与GPT-4o进行了比较。为了进一步理解影响模型准确性的文本特征，我们对笔记长度、实体长度和分段进行了错误分析。结果显示，cLLMs有可能通过超过20%的时间减少提取MHEs所需的时间。然而，由于MHEs的多义性和频繁涉及非医学词汇，检测许多类型的MHEs仍然具有挑战性。细调后的GatorTron和GatorTronS两种最广泛训练的cLLMs表现出最高的性能。结合预识别的BME信息可以提高某些实体的模型性能。关于文本特征对模型性能的影响，我们发现较长的实体更难识别，笔记长度与更高的错误率无关，而结构良好且带有标题的分段对提取有益。', 'title_zh': '从临床文本中提取患者病史：临床大型语言模型的比较研究'}
{'arxiv_id': 'arXiv:2503.23250', 'title': 'Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions', 'authors': 'Shih-Han Chan', 'link': 'https://arxiv.org/abs/2503.23250', 'abstract': "Security threats like prompt injection attacks pose significant risks to applications that integrate Large Language Models (LLMs), potentially leading to unauthorized actions such as API misuse. Unlike previous approaches that aim to detect these attacks on a best-effort basis, this paper introduces a novel method that appends an Encrypted Prompt to each user prompt, embedding current permissions. These permissions are verified before executing any actions (such as API calls) generated by the LLM. If the permissions are insufficient, the LLM's actions will not be executed, ensuring safety. This approach guarantees that only actions within the scope of the current permissions from the LLM can proceed. In scenarios where adversarial prompts are introduced to mislead the LLM, this method ensures that any unauthorized actions from LLM wouldn't be executed by verifying permissions in Encrypted Prompt. Thus, threats like prompt injection attacks that trigger LLM to generate harmful actions can be effectively mitigated.", 'abstract_zh': '像提示注入攻击这样的安全威胁对集成大型语言模型（LLMs）的应用程序构成了重大风险，可能导致未经授权的操作如API滥用。不同于以往需要尽力检测这些攻击的方法，本文提出了一种新型方法，即在每个用户提示后添加加密提示，并嵌入当前权限。这些权限在执行任何由LLM生成的操作（如API调用）之前进行验证。如果权限不足，LLM的操作将不会被执行，确保安全性。该方法保证只有当前LLM权限范围内的操作才能继续进行。在对抗性提示引入以误导LLM的情况下，通过验证加密提示中的权限可以防止未经授权的操作被执行，从而有效缓解提示注入攻击等威胁，防止LLM生成有害操作。', 'title_zh': '加密提示：保障LLM应用程序免受未授权操作的风险'}
{'arxiv_id': 'arXiv:2503.23243', 'title': 'Evaluating how LLM annotations represent diverse views on contentious topics', 'authors': 'Megan A. Brown, Shubham Atreja, Libby Hemphill, Patrick Y. Wu', 'link': 'https://arxiv.org/abs/2503.23243', 'abstract': 'Researchers have proposed the use of generative large language models (LLMs) to label data for both research and applied settings. This literature emphasizes the improved performance of LLMs relative to other natural language models, noting that LLMs typically outperform other models on standard metrics such as accuracy, precision, recall, and F1 score. However, previous literature has also highlighted the bias embedded in language models, particularly around contentious topics such as potentially toxic content. This bias could result in labels applied by LLMs that disproportionately align with majority groups over a more diverse set of viewpoints. In this paper, we evaluate how LLMs represent diverse viewpoints on these contentious tasks. Across four annotation tasks on four datasets, we show that LLMs do not show substantial disagreement with annotators on the basis of demographics. Instead, the model, prompt, and disagreement between human annotators on the labeling task are far more predictive of LLM agreement. Our findings suggest that when using LLMs to annotate data, under-representing the views of particular groups is not a substantial concern. We conclude with a discussion of the implications for researchers and practitioners.', 'abstract_zh': '研究人员提出使用生成性大型语言模型（LLMs）来为研究和应用场景标注数据。该文献强调了LLMs相对于其他自然语言模型的性能改进，指出LLMs通常在准确率、精确率、召回率和F1分数等标准指标上表现更优。然而，先前的研究也指出语言模型中存在的偏见，特别是在涉及潜在有毒内容等争议性话题时更为明显。这种偏见可能导致LLMs应用的标签过度偏向主流群体，而不是各种不同的观点。本文评估了LLMs在处理这些争议性任务时如何代表多样的观点。通过对四个数据集上的四项标注任务的研究，我们表明，基于人口统计学因素，LLMs与标注者之间没有实质性的分歧。相反，模型、提示以及人类标注者之间的标注分歧更能预测LLMs的一致性。我们的研究结果表明，在使用LLMs标注数据时，代表性不足的具体群体的观点不是主要关切。最后，本文讨论了研究工作者和实践者的相关影响。', 'title_zh': '评估大规模语言模型标注如何代表对争议性话题的多样观点'}
{'arxiv_id': 'arXiv:2503.23242', 'title': 'Beyond speculation: Measuring the growing presence of LLM-generated texts in multilingual disinformation', 'authors': 'Dominik Macko, Aashish Anantha Ramakrishnan, Jason Samuel Lucas, Robert Moro, Ivan Srba, Adaku Uchendu, Dongwon Lee', 'link': 'https://arxiv.org/abs/2503.23242', 'abstract': 'Increased sophistication of large language models (LLMs) and the consequent quality of generated multilingual text raises concerns about potential disinformation misuse. While humans struggle to distinguish LLM-generated content from human-written texts, the scholarly debate about their impact remains divided. Some argue that heightened fears are overblown due to natural ecosystem limitations, while others contend that specific "longtail" contexts face overlooked risks. Our study bridges this debate by providing the first empirical evidence of LLM presence in the latest real-world disinformation datasets, documenting the increase of machine-generated content following ChatGPT\'s release, and revealing crucial patterns across languages, platforms, and time periods.', 'abstract_zh': '大型语言模型复杂性的提高及其生成的多语言文本质量升高引发了关于潜在虚假信息滥用的担忧。尽管人类难以区分大型语言模型生成的内容与人类撰写的文本，关于其影响的学术辩论仍存在分歧。有人认为夸大了这些担忧，因为自然生态系统有限，而另一些人则认为特定的“长尾”情境面临着未被忽视的风险。我们的研究通过提供最新真实世界虚假信息数据集中的大型语言模型存在的首个实证证据，记录了ChatGPT发布后机器生成内容的增加，并揭示了跨语言、平台和时间段的关键模式。', 'title_zh': '超越猜测：测量大规模语言模型生成文本在多语言虚假信息中的日益增长存在'}
{'arxiv_id': 'arXiv:2503.23231', 'title': 'CCCI: Code Completion with Contextual Information for Complex Data Transfer Tasks Using Large Language Models', 'authors': 'Hangzhan Jin, Mohammad Hamdaqa', 'link': 'https://arxiv.org/abs/2503.23231', 'abstract': 'Unlike code generation, which involves creating code from scratch, code completion focuses on integrating new lines or blocks of code into an existing codebase. This process requires a deep understanding of the surrounding context, such as variable scope, object models, API calls, and database relations, to produce accurate results. These complex contextual dependencies make code completion a particularly challenging problem. Current models and approaches often fail to effectively incorporate such context, leading to inaccurate completions with low acceptance rates (around 30\\%). For tasks like data transfer, which rely heavily on specific relationships and data structures, acceptance rates drop even further. This study introduces CCCI, a novel method for generating context-aware code completions specifically designed to address data transfer tasks. By integrating contextual information, such as database table relationships, object models, and library details into Large Language Models (LLMs), CCCI improves the accuracy of code completions. We evaluate CCCI using 289 Java snippets, extracted from over 819 operational scripts in an industrial setting. The results demonstrate that CCCI achieved a 49.1\\% Build Pass rate and a 41.0\\% CodeBLEU score, comparable to state-of-the-art methods that often struggle with complex task completion.', 'abstract_zh': '不同于代码生成从头创建代码，代码补全关注于将新的代码行或代码块集成到现有的代码库中。这一过程需要对周围环境有深刻的理解，如变量作用域、对象模型、API调用和数据库关系，以生成准确的结果。这些复杂的上下文依赖关系使得代码补全成为一个特别具有挑战性的问题。当前的模型和方法往往难以有效地融合这些上下文，导致完成度低且接受率低（约为30%）。对于像数据传输这样的任务，它严重依赖特定的关系和数据结构，接受率甚至会更低。本研究引入了CCCI，这是一种生成上下文感知代码补全的新方法，专门针对数据传输任务。通过将数据库表关系、对象模型和库细节等上下文信息集成到大型语言模型中，CCCI提高了代码补全的准确性。我们使用289个从超过819个工业运行脚本中提取的Java片段对CCCI进行了评估。结果显示，CCCI实现了49.1%的构建通过率和41.0%的CodeBLEU得分，与在复杂任务完成上常常力不从心的先进方法相当。', 'title_zh': 'CCCI：使用大型语言模型结合上下文信息进行复杂数据传输任务的代码完成'}
{'arxiv_id': 'arXiv:2503.23219', 'title': 'Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs', 'authors': 'Sanjoy Chowdhury, Hanan Gani, Nishit Anand, Sayan Nag, Ruohan Gao, Mohamed Elhoseiny, Salman Khan, Dinesh Manocha', 'link': 'https://arxiv.org/abs/2503.23219', 'abstract': 'Recent advancements in reasoning optimization have greatly enhanced the performance of large language models (LLMs). However, existing work fails to address the complexities of audio-visual scenarios, underscoring the need for further research. In this paper, we introduce AURELIA, a novel actor-critic based audio-visual (AV) reasoning framework that distills structured, step-by-step reasoning into AVLLMs at test time, improving their ability to process complex multi-modal inputs without additional training or fine-tuning. To further advance AVLLM reasoning skills, we present AVReasonBench, a challenging benchmark comprising 4500 audio-visual questions, each paired with detailed step-by-step reasoning. Our benchmark spans six distinct tasks, including AV-GeoIQ, which evaluates AV reasoning combined with geographical and cultural knowledge. Evaluating 18 AVLLMs on AVReasonBench reveals significant limitations in their multi-modal reasoning capabilities. Using AURELIA, we achieve up to a 100% relative improvement, demonstrating its effectiveness. This performance gain highlights the potential of reasoning-enhanced data generation for advancing AVLLMs in real-world applications. Our code and data will be publicly released at: https: //github.com/schowdhury671/aurelia.', 'abstract_zh': '近期在推理优化方面的进展显著提升了大型语言模型（LLMs）的表现。然而，现有工作仍未解决音频-视觉场景的复杂性，强调了进一步研究的必要性。本文介绍了一种新颖的基于actor-critic的音频-视觉（AV）推理框架AURELIA，在测试时将结构化的逐步推理提炼为AVLLMs，从而提高它们处理复杂多模态输入的能力，无需额外的训练或微调。为了进一步提升AVLLM的推理能力，我们提出了AVReasonBench这一具有挑战性的基准，包含4500个音频-视觉问题，每个问题都配以详细的逐步推理。该基准覆盖六个不同的任务，包括AV-GeoIQ，该任务评估结合地理和文化知识的音频-视觉推理。在AVReasonBench上对18个AVLLMs的评估揭示了它们在多模态推理能力上的显著局限性。使用AURELIA，我们达到了高达100%的相对改进，展示了其有效性。此性能提升突显了推理增强数据生成在推动音频-视觉语言模型在实际应用中的潜力。我们的代码和数据将在此公开发布：https://github.com/schowdhury671/aurelia。', 'title_zh': 'Aurelia: 视听LLM中测试时推理知识蒸馏'}
{'arxiv_id': 'arXiv:2503.23175', 'title': 'Large Language Models are Unreliable for Cyber Threat Intelligence', 'authors': 'Emanuele Mezzi, Fabio Massacci, Katja Tuma', 'link': 'https://arxiv.org/abs/2503.23175', 'abstract': 'Several recent works have argued that Large Language Models (LLMs) can be used to tame the data deluge in the cybersecurity field, by improving the automation of Cyber Threat Intelligence (CTI) tasks. This work presents an evaluation methodology that other than allowing to test LLMs on CTI tasks when using zero-shot learning, few-shot learning and fine-tuning, also allows to quantify their consistency and their confidence level. We run experiments with three state-of-the-art LLMs and a dataset of 350 threat intelligence reports and present new evidence of potential security risks in relying on LLMs for CTI. We show how LLMs cannot guarantee sufficient performance on real-size reports while also being inconsistent and overconfident. Few-shot learning and fine-tuning only partially improve the results, thus posing doubts about the possibility of using LLMs for CTI scenarios, where labelled datasets are lacking and where confidence is a fundamental factor.', 'abstract_zh': '几种近期的研究表明，大规模语言模型（LLMs）可以用于缓解网络安全领域的数据洪流，通过改进网络安全威胁情报（CTI）任务的自动化。本文提出了一种评估方法，不仅可以在零样本学习、少样本学习和微调的情况下测试LLMs在CTI任务上的表现，还能够量化其一致性和置信水平。我们使用三种最先进的LLMs并对350份威胁情报报告进行了实验，展示了依赖LLMs进行CTI的潜在安全风险。研究表明，LLMs在处理实际大小的报告时无法保证足够的性能，同时表现出不一致和过于自信的特点。少样本学习和微调只能部分改善结果，因此对在缺乏标注数据集和置信度至关重要的CTI情景中使用LLMs的可能性提出了疑问。', 'title_zh': '大型语言模型在网络安全威胁情报方面不可靠。'}
{'arxiv_id': 'arXiv:2503.23157', 'title': 'Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL', 'authors': 'Mohammadreza Pourreza, Shayan Talaei, Ruoxi Sun, Xingchen Wan, Hailong Li, Azalia Mirhoseini, Amin Saberi, Sercan "O. Arik', 'link': 'https://arxiv.org/abs/2503.23157', 'abstract': 'Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks.', 'abstract_zh': '文本到SQL转换是一项涉及多个推理密集型子任务的挑战性任务，包括自然语言理解、数据库模式理解以及精确的SQL查询公式化。现有的方法通常依赖于手工设计的推理路径，这些路径可能带有诱导偏置，从而限制了它们的整体效果。受最近增强推理模型如DeepSeek R1和OpenAI o1的成功启发，这些模型通过奖励驱动的自我探索有效地提升了推理能力和泛化能力，我们提出了一种针对文本到SQL任务的新颖部分奖励集。我们的奖励集包括模式链接、AI反馈、n-克gram相似性和语法检查，明确设计以解决强化学习（RL）中普遍存在的奖励稀疏问题。利用组相对策略优化（GRPO），我们的方法明确鼓励大型语言模型（LLMs）发展必要的内在推理技巧以生成准确的SQL查询。通过不同规模的模型，我们证明，使用我们提出的奖励进行仅强化学习训练的一致上实现了比监督微调（SFT）更高的准确性和更好的泛化能力。值得注意的是，我们训练的14B参数量模型在BIRD基准上分别比 proprietary 模型o3-mini和Gemini-1.5-Pro-002高出4%和3%，这突显了我们提出的带有部分奖励的RL训练框架在提高文本到SQL任务的准确性和推理能力方面的有效性。', 'title_zh': 'Reasoning-SQL：增强推理的文本到SQL转换的SQL定制部分奖励强化学习'}
{'arxiv_id': 'arXiv:2503.23145', 'title': 'CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis', 'authors': 'Anjiang Wei, Tarun Suresh, Jiannan Cao, Naveen Kannan, Yuheng Wu, Kai Yan, Thiago S. F. X. Teixeira, Ke Wang, Alex Aiken', 'link': 'https://arxiv.org/abs/2503.23145', 'abstract': 'Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.', 'abstract_zh': 'CodeARC：代码抽象与推理挑战', 'title_zh': 'CodeARC: 评估LLM代理归纳程序合成推理能力的基准测试'}
{'arxiv_id': 'arXiv:2503.23128', 'title': 'CrossMuSim: A Cross-Modal Framework for Music Similarity Retrieval with LLM-Powered Text Description Sourcing and Mining', 'authors': 'Tristan Tsoi, Jiajun Deng, Yaolong Ju, Benno Weck, Holger Kirchhoff, Simon Lui', 'link': 'https://arxiv.org/abs/2503.23128', 'abstract': "Music similarity retrieval is fundamental for managing and exploring relevant content from large collections in streaming platforms. This paper presents a novel cross-modal contrastive learning framework that leverages the open-ended nature of text descriptions to guide music similarity modeling, addressing the limitations of traditional uni-modal approaches in capturing complex musical relationships. To overcome the scarcity of high-quality text-music paired data, this paper introduces a dual-source data acquisition approach combining online scraping and LLM-based prompting, where carefully designed prompts leverage LLMs' comprehensive music knowledge to generate contextually rich descriptions. Exten1sive experiments demonstrate that the proposed framework achieves significant performance improvements over existing benchmarks through objective metrics, subjective evaluations, and real-world A/B testing on the Huawei Music streaming platform.", 'abstract_zh': '音乐相似性检索是管理并探索大型流媒体平台内容相关性的基础。本文提出了一种新颖的跨模态对比学习框架，利用开放式的文本描述指导音乐相似性建模，解决了传统单模态方法在捕捉复杂音乐关系方面的局限性。为了克服高质量文本-音乐配对数据的稀缺性，本文引入了一种结合在线抓取和基于LLM的提示的双重数据源获取方法，其中精心设计的提示利用了LLM全面的音乐知识生成语境丰富的描述。 extensive experiments demonstrate that the proposed framework achieves significant performance improvements over existing benchmarks through objective metrics, subjective evaluations, and real-world A/B testing on the Huawei Music streaming platform.', 'title_zh': 'CrossMuSim：一种基于LLM驱动文本描述的跨模态音乐相似性检索框架'}
{'arxiv_id': 'arXiv:2503.23084', 'title': 'The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction', 'authors': 'Yihuai Hong, Dian Zhou, Meng Cao, Lei Yu, Zhijing Jin', 'link': 'https://arxiv.org/abs/2503.23084', 'abstract': "Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model's residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems.", 'abstract_zh': '大型语言模型在多种推理基准测试中表现出色，但先前的研究表明，它们有时难以将学到的知识应用于未见过的问题，这可能是因为过度依赖记忆中的训练示例。然而，大型语言模型在文本生成过程中何时切换至推理或记忆的具体条件尚不明确。本研究通过识别模型残差流中的一组线性特征，提供了对大型语言模型推理-记忆动态机制的机制性理解，这些特征不仅能够区分推理任务与记忆密集型任务，还可以因果性地影响模型在推理任务上的性能。此外，我们还展示了干预这些推理特征有助于模型在答案生成过程中更准确地激活最相关的解决问题能力。我们的研究为理解大型语言模型中的推理和记忆机制提供了新的见解，并为开发更为 robust 和可解释的生成式 AI 系统奠定了基础。', 'title_zh': '语言模型中的推理-记忆互动由单向机制调节'}
{'arxiv_id': 'arXiv:2503.22973', 'title': 'XL-Instruct: Synthetic Data for Cross-Lingual Open-Ended Generation', 'authors': 'Vivek Iyer, Ricardo Rei, Pinzhen Chen, Alexandra Birch', 'link': 'https://arxiv.org/abs/2503.22973', 'abstract': "Cross-lingual open-ended generation -- i.e. generating responses in a desired language different from that of the user's query -- is an important yet understudied problem. We introduce XL-AlpacaEval, a new benchmark for evaluating cross-lingual generation capabilities in Large Language Models (LLMs), and propose XL-Instruct, a high-quality synthetic data generation method. Fine-tuning with just 8K XL-Instruct-generated instructions significantly improves model performance, increasing the win rate against GPT-4o-Mini from 7.4% to 21.5%, and improving on several fine-grained quality metrics. Additionally, models fine-tuned on XL-Instruct exhibit strong zero-shot transfer to both English-only and multilingual generation tasks. Given its consistent gains across the board, we strongly recommend incorporating XL-Instruct in the post-training pipeline of future multilingual LLMs. To facilitate further research, we will publicly and freely release the XL-Instruct and XL-AlpacaEval datasets, which constitute two of the few cross-lingual resources currently available in the literature.", 'abstract_zh': '跨语言开放生成——即生成与用户查询语言不同的语言的响应——是一个重要但研究不足的问题。我们引入了XL-AlpacaEval，一个用于评估大型语言模型跨语言生成能力的新基准，提出了XL-Instruct，一种高质量合成数据生成方法。仅使用8K XL-Instruct生成的指令进行微调显著提高了模型性能，使模型对阵GPT-4o-Mini的胜率从7.4%提高到21.5%，并在多个细粒度质量指标上表现出改进。此外，基于XL-Instruct微调的模型在英语文本生成和多语言生成任务上表现出强大的零样本迁移能力。鉴于其在各个方面的稳定改进，我们强烈建议在未来多语言大型语言模型的后训练流程中纳入XL-Instruct。为了促进进一步研究，我们将公开和免费发布XL-Instruct和XL-AlpacaEval数据集，这构成了当前文献中为数不多的跨语言资源之二。', 'title_zh': 'XL-Instruct: 合成数据用于跨语言开放式生成'}
{'arxiv_id': 'arXiv:2503.22968', 'title': 'HRET: A Self-Evolving LLM Evaluation Toolkit for Korean', 'authors': 'Hanwool Lee, Soo Yong Kim, Dasol Choi, SangWon Baek, Seunghyeok Hong, Ilgyun Jeong, Inseon Hwang, Naeun Lee, Guijin Son', 'link': 'https://arxiv.org/abs/2503.22968', 'abstract': 'Recent advancements in Korean large language models (LLMs) have spurred numerous benchmarks and evaluation methodologies, yet the lack of a standardized evaluation framework has led to inconsistent results and limited comparability. To address this, we introduce HRET Haerae Evaluation Toolkit, an open-source, self-evolving evaluation framework tailored specifically for Korean LLMs. HRET unifies diverse evaluation methods, including logit-based scoring, exact-match, language-inconsistency penalization, and LLM-as-a-Judge assessments. Its modular, registry-based architecture integrates major benchmarks (HAE-RAE Bench, KMMLU, KUDGE, HRM8K) and multiple inference backends (vLLM, HuggingFace, OpenAI-compatible endpoints). With automated pipelines for continuous evolution, HRET provides a robust foundation for reproducible, fair, and transparent Korean NLP research.', 'abstract_zh': 'Recent advancements in Korean大型语言模型（LLMs）尽管激发了众多基准测试和评价方法，但缺乏标准化的评价框架仍导致了结果不一致和可比性有限。为解决这一问题，我们引入了HRET Haerae评价工具包，这是一个专为韩语LLMs设计的开源、自我进化的评价框架。HRET统一了多种评价方法，包括logit评分、精确匹配、语言一致性惩罚和LLM作为裁判的评估。其模块化、注册表为基础的架构集成了主要基准测试（HAE-RAE Bench、KMMLU、KUDGE、HRM8K）和多个推理后端（vLLM、HuggingFace、OpenAI兼容端点）。通过自动化管道进行持续进化，HRET为可重复、公平和透明的韩语NLP研究提供了坚实的基础。', 'title_zh': 'HRET：一种自我进化的韩语大型语言模型评价工具-kit'}
{'arxiv_id': 'arXiv:2503.22954', 'title': 'Can LLMs Support Medical Knowledge Imputation? An Evaluation-Based Perspective', 'authors': 'Xinyu Yao, Aditya Sannabhadti, Holly Wiberg, Karmel S. Shehadeh, Rema Padman', 'link': 'https://arxiv.org/abs/2503.22954', 'abstract': 'Medical knowledge graphs (KGs) are essential for clinical decision support and biomedical research, yet they often exhibit incompleteness due to knowledge gaps and structural limitations in medical coding systems. This issue is particularly evident in treatment mapping, where coding systems such as ICD, Mondo, and ATC lack comprehensive coverage, resulting in missing or inconsistent associations between diseases and their potential treatments. To address this issue, we have explored the use of Large Language Models (LLMs) for imputing missing treatment relationships. Although LLMs offer promising capabilities in knowledge augmentation, their application in medical knowledge imputation presents significant risks, including factual inaccuracies, hallucinated associations, and instability between and within LLMs. In this study, we systematically evaluate LLM-driven treatment mapping, assessing its reliability through benchmark comparisons. Our findings highlight critical limitations, including inconsistencies with established clinical guidelines and potential risks to patient safety. This study serves as a cautionary guide for researchers and practitioners, underscoring the importance of critical evaluation and hybrid approaches when leveraging LLMs to enhance treatment mappings on medical knowledge graphs.', 'abstract_zh': '医学知识图谱（KGs）对于临床决策支持和生物医学研究至关重要，但由于医学编码系统的知识缺口和结构限制，它们常常表现出不完整性。在治疗映射方面，这种情况尤为明显，如ICD、Mondo和ATC等编码系统缺乏全面的覆盖范围，导致疾病与潜在治疗之间的关联存在缺失或不一致。为了解决这一问题，我们探索了使用大型语言模型（LLMs）来补充缺失的治疗关系。尽管LLMs在知识增补方面具有潜力，但将其应用于医学知识图谱填充却存在显著风险，包括事实不准确、虚构的关联以及LLMs之间和内部的不稳定。在本研究中，我们系统评估了LLMs驱动的治疗映射，并通过基准比较评估其可靠性。我们的研究结果强调了关键限制，包括与既有临床指南的一致性问题以及对患者安全的风险。本研究为研究人员和实践者提供了一种警戒指南，强调了在利用LLMs增强医学知识图谱中的治疗映射时进行批判性评估和混合方法的重要性。', 'title_zh': 'LLMs在医疗知识补全中提供支持的能力：基于评估的角度'}
{'arxiv_id': 'arXiv:2503.22948', 'title': 'SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning', 'authors': 'Tianyang Xu, Xiaoze Liu, Feijie Wu, Xiaoqian Wang, Jing Gao', 'link': 'https://arxiv.org/abs/2503.22948', 'abstract': "Large Language Models (LLMs) have transformed natural language processing by learning from massive datasets, yet this rapid progress has also drawn legal scrutiny, as the ability to unintentionally generate copyrighted content has already prompted several prominent lawsuits. In this work, we introduce SUV (Selective Unlearning for Verbatim data), a selective unlearning framework designed to prevent LLM from memorizing copyrighted content while preserving its overall utility. In detail, the proposed method constructs a dataset that captures instances of copyrighted infringement cases by the targeted LLM. With the dataset, we unlearn the content from the LLM by means of Direct Preference Optimization (DPO), which replaces the verbatim copyrighted content with plausible and coherent alternatives. Since DPO may hinder the LLM's performance in other unrelated tasks, we integrate gradient projection and Fisher information regularization to mitigate the degradation. We validate our approach using a large-scale dataset of 500 famous books (predominantly copyrighted works) and demonstrate that SUV significantly reduces verbatim memorization with negligible impact on the performance on unrelated tasks. Extensive experiments on both our dataset and public benchmarks confirm the scalability and efficacy of our approach, offering a promising solution for mitigating copyright risks in real-world LLM applications.", 'abstract_zh': '大型语言模型（LLMs）通过学习大规模数据集已改变自然语言处理领域，但这种快速进展也引起了法律关注，因为无意生成受版权保护内容的能力已引发多起重要诉讼。在此项工作中，我们提出了一种选择性遗忘框架SUV（Selective Unlearning for Verbatim data），旨在防止LLM记忆受版权保护的内容，同时保持其整体效用。具体而言，所提出的方法构建了一个数据集，该数据集捕获目标LLM的版权侵权实例。借助该数据集，我们通过直接偏好优化（DPO）移除LLM中的内容，即将确切的受版权保护的内容替换为合理且连贯的替代内容。由于DPO可能阻碍LLM在其他无关任务上的性能，我们整合了梯度投影和费舍尔信息正则化以减轻性能退化。我们使用包含500部著名书籍（主要为受版权保护的作品）的大规模数据集验证了该方法，并展示了SUV显著减少了受版权保护内容的精确记忆，对无关任务的性能影响微乎其微。广泛的实验不仅证实了所提出方法的可扩展性和有效性，还提供了在实际应用中减轻LLM版权风险的有希望的解决方案。', 'title_zh': 'SUV: 可扩展的大语言模型版权合规与正则化选择性遗忘'}
{'arxiv_id': 'arXiv:2503.22877', 'title': 'Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models', 'authors': 'Bruno Coelho, Shujaat Mirza, Yuyuan Cui, Christina Pöpper, Damon McCoy', 'link': 'https://arxiv.org/abs/2503.22877', 'abstract': 'Fact-checking is a potentially useful application of Large Language Models (LLMs) to combat the growing dissemination of disinformation. However, the performance of LLMs varies across geographic regions. In this paper, we evaluate the factual accuracy of open and private models across a diverse set of regions and scenarios.\nUsing a dataset containing 600 fact-checked statements balanced across six global regions we examine three experimental setups of fact-checking a statement: (1) when just the statement is available, (2) when an LLM-based agent with Wikipedia access is utilized, and (3) as a best case scenario when a Retrieval-Augmented Generation (RAG) system provided with the official fact check is employed. Our findings reveal that regardless of the scenario and LLM used, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global North perform substantially better than those from the Global South. Furthermore, this gap is broadened for the more realistic case of a Wikipedia agent-based system, highlighting that overly general knowledge bases have a limited ability to address region-specific nuances. These results underscore the urgent need for better dataset balancing and robust retrieval strategies to enhance LLM fact-checking capabilities, particularly in geographically diverse contexts.', 'abstract_zh': '大型语言模型在事实核查中的应用：地理区域差异及其对策', 'title_zh': '地理区域之间大规模语言模型事实检查不平等性研究：基于代理和检索模型方法'}
{'arxiv_id': 'arXiv:2503.22853', 'title': 'Teaching LLMs Music Theory with In-Context Learning and Chain-of-Thought Prompting: Pedagogical Strategies for Machines', 'authors': 'Liam Pond, Ichiro Fujinaga', 'link': 'https://arxiv.org/abs/2503.22853', 'abstract': 'This study evaluates the baseline capabilities of Large Language Models (LLMs) like ChatGPT, Claude, and Gemini to learn concepts in music theory through in-context learning and chain-of-thought prompting. Using carefully designed prompts (in-context learning) and step-by-step worked examples (chain-of-thought prompting), we explore how LLMs can be taught increasingly complex material and how pedagogical strategies for human learners translate to educating machines. Performance is evaluated using questions from an official Canadian Royal Conservatory of Music (RCM) Level 6 examination, which covers a comprehensive range of topics, including interval and chord identification, key detection, cadence classification, and metrical analysis. Additionally, we evaluate the suitability of various music encoding formats for these tasks (ABC, Humdrum, MEI, MusicXML). All experiments were run both with and without contextual prompts. Results indicate that without context, ChatGPT with MEI performs the best at 52%, while with context, Claude with MEI performs the best at 75%. Future work will further refine prompts and expand to cover more advanced music theory concepts. This research contributes to the broader understanding of teaching LLMs and has applications for educators, students, and developers of AI music tools alike.', 'abstract_zh': '本研究评估了像ChatGPT、Claude和Gemini这样的大型语言模型通过在上下文学习和链式思考提示方法学习音乐理论基本能力。利用精心设计的提示（在上下文学习）和逐步示例（链式思考提示），我们探究了如何逐渐教授LLMs复杂材料，以及人类学习者的教学策略如何应用于机器教育。性能评估使用加拿大皇家音乐学院（RCM）级6考试中的官方问题，涵盖了包括音程和和弦识别、调性检测、终止式分类和节奏分析在内的广泛主题。此外，我们还评估了各种音乐编码格式（ABC、Humdrum、MEI、MusicXML）在这些任务中的适用性。所有实验均在有无上下文提示的情况下进行。结果显示，在无上下文的情况下，使用MEI的ChatGPT表现最佳，得分为52%，而在有上下文的情况下，使用MEI的Claude表现最佳，得分为75%。未来的工作将进一步精炼提示，并扩展到涵盖更高级的音乐理论概念。本研究为更广泛理解教授LLMs提供了贡献，并对教育者、学生和AI音乐工具开发者都有实际应用价值。', 'title_zh': '用上下文学习和链式思考提示教授大语言模型音乐理论：面向机器的教学策略'}
{'arxiv_id': 'arXiv:2503.22851', 'title': 'RobuNFR: Evaluating the Robustness of Large Language Models on Non-Functional Requirements Aware Code Generation', 'authors': 'Feng Lin, Dong Jae Kim, Zhenhao Li, Jinqiu Yang, Tse-Husn, Chen', 'link': 'https://arxiv.org/abs/2503.22851', 'abstract': 'When using LLMs to address Non-Functional Requirements (NFRs), developers may behave differently (e.g., expressing the same NFR in different words). Robust LLMs should output consistent results across these variations; however, this aspect remains underexplored. We propose RobuNFR for evaluating the robustness of LLMs in NFR-aware code generation across four NFR dimensions: design, readability, reliability, and performance, using three methodologies: prompt variation, regression testing, and diverse workflows. Our experiments show that RobuNFR reveals robustness issues in the tested LLMs when considering NFRs in code generation. Specifically, under prompt variation, including NFRs leads to a decrease in Pass@1 by up to 39 percent and an increase in the standard deviation from 0.48 to 2.48 compared to the baseline without NFRs (i.e., Function-Only). While incorporating NFRs generally improves overall NFR metrics, it also results in higher prompt sensitivity. In regression settings, some LLMs exhibit differences across versions, with improvements in one aspect (e.g., reduced code smells) often accompanied by regressions in another (e.g., decreased correctness), revealing inconsistencies that challenge their robustness. When varying workflows, the tested LLMs show significantly different NFR-aware code generation capabilities between two workflows: (1) integrating NFRs and functional requirements into the initial prompt and (2) enhancing Function-Only-generated code with the same NFR.', 'abstract_zh': '使用RobuNFR评估LLM在代码生成中对非功能性需求（NFRs）感知的鲁棒性：在设计、可读性、可靠性和性能四个维度上的评估，采用三种方法：提示变异、回归测试和多样化的工作流', 'title_zh': 'RobuNFR: 非功能需求 awareness 代码生成中的大型语言模型稳健性评估'}
{'arxiv_id': 'arXiv:2503.22776', 'title': 'Post-Incorporating Code Structural Knowledge into LLMs via In-Context Learning for Code Translation', 'authors': 'Yali Du, Hui Sun, Ming Li', 'link': 'https://arxiv.org/abs/2503.22776', 'abstract': 'Code translation migrates codebases across programming languages. Recently, large language models (LLMs) have achieved significant advancements in software mining. However, handling the syntactic structure of source code remains a challenge. Classic syntax-aware methods depend on intricate model architectures and loss functions, rendering their integration into LLM training resource-intensive. This paper employs in-context learning (ICL), which directly integrates task exemplars into the input context, to post-incorporate code structural knowledge into pre-trained LLMs. We revisit exemplar selection in ICL from an information-theoretic perspective, proposing that list-wise selection based on information coverage is more precise and general objective than traditional methods based on combining similarity and diversity. To address the challenges of quantifying information coverage, we introduce a surrogate measure, Coverage of Abstract Syntax Tree (CAST). Furthermore, we formulate the NP-hard CAST maximization for exemplar selection and prove that it is a standard submodular maximization problem. Therefore, we propose a greedy algorithm for CAST submodular maximization, which theoretically guarantees a (1-1/e)-approximate solution in polynomial time complexity. Our method is the first training-free and model-agnostic approach to post-incorporate code structural knowledge into existing LLMs at test time. Experimental results show that our method significantly improves LLMs performance and reveals two meaningful insights: 1) Code structural knowledge can be effectively post-incorporated into pre-trained LLMs during inference, despite being overlooked during training; 2) Scaling up model size or training data does not lead to the emergence of code structural knowledge, underscoring the necessity of explicitly considering code syntactic structure.', 'abstract_zh': '代码翻译跨越编程语言。通过大型语言模型（LLMs）在软件挖掘方面的最新进展，代码迁移成为了可能。然而，处理源代码的语法结构仍然是一个挑战。经典的方法依赖于复杂的模型架构和损失函数，使得它们难以整合到LLM训练中。本文采用上下文相关学习（ICL），直接将任务示例集成到输入上下文中，以在预训练的LLMs中后嵌入代码结构知识。我们从信息论的角度回顾了ICL中的示例选择，提出基于信息覆盖的列表式选择比基于相似性和多样性的传统方法更加精确和通用。为了解决量化信息覆盖的挑战，我们引入了一个代理指标，抽象语法树（AST）覆盖度（CAST）。此外，我们为CAST的最大化形式化了NP-hard问题，并证明其是一个标准的次模函数最大化问题。因此，我们提出了一种贪婪算法以实现CAST的次模函数最大化，该算法在多项式时间复杂度内理论上保证了（1-1/e）近似解。我们的方法是第一个在测试时后嵌入代码结构知识的无需训练和模型无关的方法。实验结果表明，我们的方法显著提升了LLMs的性能，并揭示了两个重要的见解：1）在训练过程中被忽视的代码结构知识可以在推断过程中有效地后嵌入到预训练的LLMs中；2）增加模型大小或训练数据量不会导致代码结构知识的出现，强调了显式考虑代码语法结构的必要性。', 'title_zh': '通过上下文学习将代码结构知识后嵌入到大规模语言模型中以进行代码翻译'}
{'arxiv_id': 'arXiv:2503.22769', 'title': 'MediTools -- Medical Education Powered by LLMs', 'authors': 'Amr Alshatnawi, Remi Sampaleanu, David Liebovitz', 'link': 'https://arxiv.org/abs/2503.22769', 'abstract': 'Artificial Intelligence (AI) has been advancing rapidly and with the advent of large language models (LLMs) in late 2022, numerous opportunities have emerged for adopting this technology across various domains, including medicine. These innovations hold immense potential to revolutionize and modernize medical education. Our research project leverages large language models to enhance medical education and address workflow challenges through the development of MediTools - AI Medical Education. This prototype application focuses on developing interactive tools that simulate real-life clinical scenarios, provide access to medical literature, and keep users updated with the latest medical news. Our first tool is a dermatology case simulation tool that uses real patient images depicting various dermatological conditions and enables interaction with LLMs acting as virtual patients. This platform allows users to practice their diagnostic skills and enhance their clinical decision-making abilities. The application also features two additional tools: an AI-enhanced PubMed tool for engaging with LLMs to gain deeper insights into research papers, and a Google News tool that offers LLM generated summaries of articles for various medical specialties. A comprehensive survey has been conducted among medical professionals and students to gather initial feedback on the effectiveness and user satisfaction of MediTools, providing insights for further development and refinement of the application. This research demonstrates the potential of AI-driven tools in transforming and revolutionizing medical education, offering a scalable and interactive platform for continuous learning and skill development.', 'abstract_zh': '人工智能（AI）的发展日益迅速，特别是2022年底大型语言模型（LLMs）的出现，为医疗等多个领域 adopting这一技术带来了众多机会。这些创新拥有巨大潜力，能够革新和现代化医学教育。我们的研究项目利用大型语言模型增强医学教育，并通过开发MediTools - AI医学教育解决工作流程挑战。该原型应用程序侧重于开发模拟现实临床场景的交互工具，提供医学文献访问，并使用户能够获取最新医学新闻。我们的第一个工具是使用实际患者图像模拟各种皮肤病条件的皮肤病案例模拟工具，允许与作为虚拟患者的LLMs进行交互。该平台允许用户练习诊断技能并提高临床决策能力。该应用程序还包含两个附加工具：一个增强的PubMed工具，用于与LLMs交互以深入了解研究论文，以及一个Google新闻工具，提供由LLMs生成的文章摘要，适用于各种医学专科。我们对医学专业人员和学生进行了全面调查，收集了对MediTools的有效性和用户满意度的初步反馈，为应用程序的进一步开发和改进提供了见解。这项研究展示了AI驱动工具在革新和现代化医学教育方面的潜力，提供了一个可扩展且交互式的平台，支持持续学习和技能发展。', 'title_zh': 'MediTools —— 由大规模语言模型驱动的医学教育'}
{'arxiv_id': 'arXiv:2503.22764', 'title': 'Boosting Large Language Models with Mask Fine-Tuning', 'authors': 'Mingyuan Zhang, Yue Bai, Huan Wang, Yizhou Wang, Qihua Dong, Yun Fu', 'link': 'https://arxiv.org/abs/2503.22764', 'abstract': 'The model is usually kept integral in the mainstream large language model (LLM) fine-tuning protocols. No works have questioned whether maintaining the integrity of the model is indispensable for performance. In this work, we introduce Mask Fine-Tuning (MFT), a brand-new LLM fine-tuning paradigm to show that properly breaking the integrity of the model can surprisingly lead to improved performance. Specifically, MFT learns a set of binary masks supervised by the typical LLM fine-tuning objective. Extensive experiments show that MFT gains a consistent performance boost across various domains and backbones (e.g., 1.95%/1.88% average gain in coding with LLaMA2-7B/3.1-8B). Detailed procedures are provided to study the proposed MFT from different hyperparameter perspectives for better insight. In particular, MFT naturally updates the current LLM training protocol by deploying it on a complete well-trained model. This study extends the functionality of mask learning from its conventional network pruning context for model compression to a more general scope.', 'abstract_zh': '模型在主流大语言模型（LLM）fine-tuning范式中通常保持整体性。没有任何研究质疑保持模型完整性的必要性对性能的影响。在这项工作中，我们引入了Mask Fine-Tuning (MFT)，这是一种全新的LLM fine-tuning范式，以展示适当破坏模型的完整性能意外地提高性能。具体来说，MFT通过监督典型的LLM fine-tuning目标学习一组二进制掩码。大量的实验表明，MFT在各种领域和骨干模型（例如，在使用LLaMA2-7B/3.1-8B时分别获得1.95%/1.88%的平均性能提升）中获得了持续的性能提升。我们提供了详细的过程，从不同的超参数视角研究提出的MFT，以获得更好的见解。特别是，MFT自然地更新了当前的LLM训练范式，将其应用于一个完整的训练良好的模型。这项研究将掩码学习的功能从其传统的网络剪枝上下文扩展到了更广泛的范围。', 'title_zh': '使用掩码微调提升大型语言模型'}
{'arxiv_id': 'arXiv:2503.22748', 'title': 'Ignite Forecasting with SPARK: An Efficient Generative Framework for Refining LLMs in Temporal Knowledge Graph Forecasting', 'authors': 'Gongzhu Yin, Hongli Zhang, Yi Luo, Yuchen Yang, Kun Lu, Chao Meng', 'link': 'https://arxiv.org/abs/2503.22748', 'abstract': "Temporal Knowledge Graph (TKG) forecasting is crucial for predicting future events using historical data. With the surge of Large Language Models (LLMs), recent studies have begun exploring their integration into TKG forecasting and achieved some success. However, they still face limitations such as limited input length, inefficient output generation, and resource-intensive refinement, which undermine their performance and practical applicability. To address these limitations, we introduce SPARK, a Sequence-level Proxy-Adapting framework for Refining LLMs in TKG forecasting. Inspired by inference-time algorithms adopted in controlling generation, SPARK offers a cost-effective, plug-and-play solution through two key innovations: (1) Beam Sequence-Level Generation, which reframes TKG forecasting as a top-K sequence-level generation task, using beam search for efficiently generating next-entity distribution in a single forward pass. (2) TKG Adapter for Refinement, which employs traditional TKG models as trainable proxy adapters to leverage global graph information and refine LLM outputs, overcoming both the input length and the resource-intensive fine-tuning problems. Experiments across diverse datasets validate SPARK's forecasting performance, robust generalization capabilities, and high efficiency. We release source codes at this https URL.", 'abstract_zh': '基于序列级代理适应的TKG预测中LLM精炼框架SPARK', 'title_zh': '用SPARK点燃预测：一种高效的生成框架，用于细化时间知识图谱forecasting中的LLMs'}
{'arxiv_id': 'arXiv:2503.22746', 'title': 'Susceptibility of Large Language Models to User-Driven Factors in Medical Queries', 'authors': 'Kyung Ho Lim, Ujin Kang, Xiang Li, Jin Sung Kim, Young-Chul Jung, Sangjoon Park, Byung-Hoon Kim', 'link': 'https://arxiv.org/abs/2503.22746', 'abstract': 'Large language models (LLMs) are increasingly used in healthcare, but their reliability is heavily influenced by user-driven factors such as question phrasing and the completeness of clinical information. In this study, we examined how misinformation framing, source authority, model persona, and omission of key clinical details affect the diagnostic accuracy and reliability of LLM outputs. We conducted two experiments: one introducing misleading external opinions with varying assertiveness (perturbation test), and another removing specific categories of patient information (ablation test). Using public datasets (MedQA and Medbullets), we evaluated proprietary models (GPT-4o, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash) and open-source models (LLaMA 3 8B, LLaMA 3 Med42 8B, DeepSeek R1 8B). All models were vulnerable to user-driven misinformation, with proprietary models especially affected by definitive and authoritative language. Assertive tone had the greatest negative impact on accuracy. In the ablation test, omitting physical exam findings and lab results caused the most significant performance drop. Although proprietary models had higher baseline accuracy, their performance declined sharply under misinformation. These results highlight the need for well-structured prompts and complete clinical context. Users should avoid authoritative framing of misinformation and provide full clinical details, especially for complex cases.', 'abstract_zh': '大型语言模型在医疗领域的可靠性和影响因素探究：错误信息框架、信息源权威性、模型人设和关键临床细节缺失对诊断准确性和可靠性的影响', 'title_zh': '大型语言模型在医疗查询中的易感性对用户驱动因素的反应'}
{'arxiv_id': 'arXiv:2503.22723', 'title': 'Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping', 'authors': 'Mohammad Saif Nazir, Chayan Banerjee', 'link': 'https://arxiv.org/abs/2503.22723', 'abstract': 'Reinforcement learning often faces challenges with reward misalignment, where agents optimize for given rewards but fail to exhibit the desired behaviors. This occurs when the reward function incentivizes proxy behaviors that diverge from the true objective. While human-in-the-loop (HIL) methods can help, they may exacerbate the problem, as humans are prone to biases that lead to inconsistent, subjective, or misaligned feedback, complicating the learning process. To address these issues, we propose two key contributions. First, we extend the use of zero-shot, off-the-shelf large language models (LLMs) for reward shaping beyond natural language processing (NLP) to continuous control tasks. By leveraging LLMs as direct feedback providers, we replace surrogate models trained on human feedback, which often suffer from the bias inherent in the feedback data it is trained on. Second, we introduce a hybrid framework (LLM-HFBF) that enables LLMs to identify and correct biases in human feedback while incorporating this feedback into the reward shaping process. The LLM-HFBF framework creates a more balanced and reliable system by addressing both the limitations of LLMs (e.g., lack of domain-specific knowledge) and human supervision (e.g., inherent biases). By enabling human feedback bias flagging and correction, our approach improves reinforcement learning performance and reduces reliance on potentially biased human guidance. Empirical experiments show that biased human feedback significantly reduces performance, with average episodic reward (AER) dropping from 28.472 in (unbiased approaches) to 7.039 (biased with conservative bias). In contrast, LLM-based approaches maintain a matching AER like unbiased feedback, even in custom edge case scenarios.', 'abstract_zh': '强化学习 Often 遇到奖励错配的挑战，其中智能体优化给定奖励但未能体现出预期行为。当奖励函数激励与真目标相悖的代理行为时，这种情况就会发生。虽然人类在环（Human-in-the-loop, HIL）方法可以帮助解决这个问题，但人类易受偏差影响，可能导致不一致、主观或错配的反馈，从而复杂化学习过程。为应对这些问题，我们提出了两项关键贡献。首先，我们扩展了零样本、即用型大型语言模型（LLM）在奖励塑形中的应用，从自然语言处理（NLP）领域扩展到连续控制任务。通过利用LLM直接提供反馈，我们替代了基于人类反馈训练的代理模型，后者往往受到反馈数据固有的偏差所影响。其次，我们引入了一种混合框架（LLM-HFBF），该框架使LLM能够识别和纠正人类反馈中的偏差，并将这些反馈纳入奖励塑形过程。LLM-HFBF框架通过同时解决LLM的局限性（如缺乏领域特定知识）和人类监督的固有偏差，创造了更为平衡和可靠的系统。通过允许人类反馈偏差标记和纠正，我们的方法提高了强化学习性能，并减少了对潜在偏差的人类指导的依赖。实验证明，带有偏差的人类反馈显著降低了性能，平均 episodic 奖励（AER）从（无偏差方法）的 28.472 下降到 7.039（带有保守偏差的有偏差）。相比之下，基于LLM的方法即使在定制的边缘情况下也能保持与无偏差反馈相匹配的AER。', 'title_zh': '零样本LLMs在人类在环RL中的应用：用奖励塑形取代人类反馈'}
{'arxiv_id': 'arXiv:2501.00363', 'title': 'SPDZCoder: Combining Expert Knowledge with LLMs for Generating Privacy-Computing Code', 'authors': 'Xiaoning Dong, Peilin Xin, Jia Li, Wei Xu', 'link': 'https://arxiv.org/abs/2501.00363', 'abstract': 'Privacy computing receives increasing attention but writing privacy computing code remains challenging for developers due to limited library functions, necessitating function implementation from scratch, and data-oblivious requirement, contradicting intuitive thinking and usual practices of programmers. Automating the generation of privacy computing code with Large Language Models can streamline development effort and lower the barrier to using privacy computing frameworks. However, existing LLMs still encounter challenges in code translation for privacy-preserving computation, such as translating Python to MP-SPDZ, due to the scarcity of MP-SPDZ data required for effective pre-training or fine-tuning. Moreover, the lack of a benchmark further complicates the evaluation of translation quality. To address the limitations, this work proposes SPDZCoder, a rule-based framework that combines LLMs with expert knowledge for generating privacy-computing code without requiring additional training data. Specifically, SPDZCoder employ a rigorous procedure for collecting high-quality expert knowledge to represent the semantic-expressing differences between Python and MP-SPDZ, and to derive transformation rules for translating Python to MP-SPDZ based on these knowledge. Then, SPDZCoder progressively converts Python code into MP-SPDZ code using transformation rules in a three stage pipeline. To evaluate SPDZCoder, we manually constructed a benchmark dataset, SPDZEval, which comprises six data splits, each representing a distinct class of challenging tasks in MP-SPDZ implementation. Extensive experiments show that SPDZCoder achieves superior performance, significantly surpassing baselines in pass@1 and pass@2. Specifically, SPDZCoder attains an overall correctness of 85.94% and 92.01% in pass@1 and pass@2, respectively, whereas the best-performing baseline achieves 63.58% and 76.36%, respectively.', 'abstract_zh': '基于大型语言模型的隐私计算代码自动生成研究：SPDZCoder框架', 'title_zh': 'SPDZCoder: 结合专家知识与LLMs生成隐私计算代码'}
