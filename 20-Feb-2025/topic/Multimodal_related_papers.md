# Transferring Textual Preferences to Vision-Language Understanding through Model Merging 

**Title (ZH)**: 通过模型合并将文本偏好转移到视觉-语言理解 

**Authors**: Chen-An Li, Tzu-Han Lin, Yun-Nung Chen, Hung-yi Lee  

**Link**: [PDF](https://arxiv.org/pdf/2502.13487)  

**Abstract**: Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is computationally expensive. This paper explores a training-free alternative by merging text-based reward models (RMs) with LVLMs to create VLRMs. Our approach shows that integrating these models leads to improved performance over LVLMs' scoring and text-based RMs, offering an efficient method for incorporating textual preferences into LVLMs. 

**Abstract (ZH)**: 大型多模态语言模型通过融合基于文本的奖励模型和LVLMs来构建VLRMs：无需训练的替代方案 

---
# HyperGCL: Multi-Modal Graph Contrastive Learning via Learnable Hypergraph Views 

**Title (ZH)**: HyperGCL：基于可学习超图视图的多模态图对比学习 

**Authors**: Khaled Mohammed Saifuddin, Jonathan Shihao Ji, Esra Akbas  

**Link**: [PDF](https://arxiv.org/pdf/2502.13277)  

**Abstract**: Recent advancements in Graph Contrastive Learning (GCL) have demonstrated remarkable effectiveness in improving graph representations. However, relying on predefined augmentations (e.g., node dropping, edge perturbation, attribute masking) may result in the loss of task-relevant information and a lack of adaptability to diverse input data. Furthermore, the selection of negative samples remains rarely explored. In this paper, we introduce HyperGCL, a novel multimodal GCL framework from a hypergraph perspective. HyperGCL constructs three distinct hypergraph views by jointly utilizing the input graph's structure and attributes, enabling a comprehensive integration of multiple modalities in contrastive learning. A learnable adaptive topology augmentation technique enhances these views by preserving important relations and filtering out noise. View-specific encoders capture essential characteristics from each view, while a network-aware contrastive loss leverages the underlying topology to define positive and negative samples effectively. Extensive experiments on benchmark datasets demonstrate that HyperGCL achieves state-of-the-art node classification performance. 

**Abstract (ZH)**: Recent Advances in Hypergraph Contrastive Learning for Graph Representation Improvement 

---
