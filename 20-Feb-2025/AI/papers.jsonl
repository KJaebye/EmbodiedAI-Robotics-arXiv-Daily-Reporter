{'arxiv_id': 'arXiv:2502.13953', 'title': 'Neurosymbolic artificial intelligence via large language models and coherence-driven inference', 'authors': 'Steve Huntsman, Jewell Thomas', 'link': 'https://arxiv.org/abs/2502.13953', 'abstract': 'We devise an algorithm to generate sets of propositions that objectively instantiate graphs that support coherence-driven inference. We then benchmark the ability of large language models (LLMs) to reconstruct coherence graphs from (a straightforward transformation of) propositions expressed in natural language, with promising results from a single prompt to models optimized for reasoning. Combining coherence-driven inference with consistency evaluations by neural models may advance the state of the art in machine cognition.', 'abstract_zh': '我们设计了一种算法以生成客观实例化支持共现推理的图的命题集。然后我们通过自然语言中表达的命题的直接变换测试了大语言模型（LLMs）重建共现图的能力，单个提示到推理优化模型都取得了令人鼓舞的结果。结合共现推理与神经模型的一致性评估可能推动机器认知领域的前沿。', 'title_zh': '通过大型语言模型和连贯驱动推理的神经符号人工智能'}
{'arxiv_id': 'arXiv:2502.13943', 'title': 'AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence', 'authors': 'Yuliang Liu, Junjie Lu, Zhaoling Chen, Chaofeng Qu, Jason Klein Liu, Chonghan Liu, Zefan Cai, Yunhui Xia, Li Zhao, Jiang Bian, Chuheng Zhang, Wei Shen, Zhouhan Lin', 'link': 'https://arxiv.org/abs/2502.13943', 'abstract': "Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities.", 'abstract_zh': '当前用于训练过程奖励模型（PRMs）的方法通常涉及使用基于规则的技术将响应拆分成多个推理步骤，例如使用预定义的占位符令牌或设定推理步骤长度为固定大小。这些方法忽视了特定单词通常不标记文本中的真正决策点这一事实。为解决这一问题，我们提出了一种称为AdaptiveStep的方法，该方法根据模型预测下一个单词的信心程度来划分推理步骤。这种划分方法在每一步提供了更多的决策信息，从而增强下游任务，如奖励模型学习。此外，我们的方法不需要人工标注。通过在数学推理和代码生成任务中使用AdaptiveStep训练的PRMs进行实验，展示了其有效性。实验结果表明，所得到的PRM实现了最先进的Best-of-N性能，优于基于标记级价值引导解码的贪婪搜索策略，并且与现有开源PRMs相比，构建成本降低了超过30%。此外，我们还对PRM的性能、转移能力和泛化能力进行了全面分析和案例研究。', 'title_zh': '自适应步长：通过模型信心自动分割推理步骤'}
{'arxiv_id': 'arXiv:2502.13834', 'title': 'Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning', 'authors': 'Zenan Li, Zhaoyu Li, Wen Tang, Xian Zhang, Yuan Yao, Xujie Si, Fan Yang, Kaiyu Yang, Xiaoxing Ma', 'link': 'https://arxiv.org/abs/2502.13834', 'abstract': 'Large language models (LLMs) can prove mathematical theorems formally by generating proof steps (\\textit{a.k.a.} tactics) within a proof system. However, the space of possible tactics is vast and complex, while the available training data for formal proofs is limited, posing a significant challenge to LLM-based tactic generation. To address this, we introduce a neuro-symbolic tactic generator that synergizes the mathematical intuition learned by LLMs with domain-specific insights encoded by symbolic methods. The key aspect of this integration is identifying which parts of mathematical reasoning are best suited to LLMs and which to symbolic methods. While the high-level idea of neuro-symbolic integration is broadly applicable to various mathematical problems, in this paper, we focus specifically on Olympiad inequalities (Figure~1). We analyze how humans solve these problems and distill the techniques into two types of tactics: (1) scaling, handled by symbolic methods, and (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with LLMs to prune and rank the proof goals for efficient proof search. We evaluate our framework on 161 challenging inequalities from multiple mathematics competitions, achieving state-of-the-art performance and significantly outperforming existing LLM and symbolic approaches without requiring additional training data.', 'abstract_zh': '大规模语言模型可以通过在证明系统内生成证明步骤（即窍门）来形式化地证明数学定理。然而，可能的窍门空间 vast 而复杂，可供训练的形式证明数据有限，这给基于大规模语言模型的窍门生成带来了显著挑战。为应对这一挑战，我们提出了一种神经符号窍门生成器，该生成器结合了大规模语言模型学到的数学直觉与通过符号方法编码的领域特定洞察。这种集成的关键在于识别哪些部分的数学推理最适合大规模语言模型，哪些最适合符号方法。虽然神经符号集成的基本思想适用于各种数学问题，但在本文中，我们专门集中在奥林匹克不等式（图1）上。我们分析了人类解决这些问题的方式，并提炼出两种类型的窍门：（1）缩放，由符号方法处理；（2）重写，由大规模语言模型处理。此外，我们结合符号工具和大规模语言模型来剪枝和排序证明目标，以实现有效的证明搜索。我们通过多个数学竞赛中的161个具有挑战性的不等式评估了该框架，取得了最先进的性能，并在无需额外训练数据的情况下显著优于现有大规模语言模型和符号方法。', 'title_zh': '利用LLMs与符号推理协同证明奥林匹克不等式'}
{'arxiv_id': 'arXiv:2502.13820', 'title': 'Scoring Verifiers: Evaluating Synthetic Verification in Code and Reasoning', 'authors': 'Aleksander Ficek, Somshubra Majumdar, Vahid Noroozi, Boris Ginsburg', 'link': 'https://arxiv.org/abs/2502.13820', 'abstract': 'Code verification has recently found great success as a critical component in training large scale reasoning models for coding. Synthetic techniques such as self-generated test cases and reward models provide a way to enhance code capabilities beyond predefined tests. Building on these advancements, we propose new benchmarks designed to systematically evaluate the impact of synthetic verification methods on assessing solution correctness. We introduce HE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks into scoring and ranking datasets to evaluate the effectiveness of synthetic verifiers. Using these benchmarks, we analyze synthetic verification methods in standard, reasoning-based, and reward-based LLMs. Our results show that recent reasoning models significantly improve test case generation and that scaling test cases enhances verification accuracy.', 'abstract_zh': '合成验证方法对评估解决方案正确性的系统影响研究：从预定义测试到增强代码能力', 'title_zh': '评分验证者：评估代码和推理中的合成验证'}
{'arxiv_id': 'arXiv:2502.13769', 'title': 'A consensus set for the aggregation of partial rankings: the case of the Optimal Set of Bucket Orders Problem', 'authors': 'Juan A. Aledo, José A. Gámez, Alejandro Rosete', 'link': 'https://arxiv.org/abs/2502.13769', 'abstract': 'In rank aggregation problems (RAP), the solution is usually a consensus ranking that generalizes a set of input orderings. There are different variants that differ not only in terms of the type of rankings that are used as input and output, but also in terms of the objective function employed to evaluate the quality of the desired output ranking. In contrast, in some machine learning tasks (e.g. subgroup discovery) or multimodal optimization tasks, attention is devoted to obtaining several models/results to account for the diversity in the input data or across the search landscape. Thus, in this paper we propose to provide, as the solution to an RAP, a set of rankings to better explain the preferences expressed in the input orderings. We exemplify our proposal through the Optimal Bucket Order Problem (OBOP), an RAP which consists in finding a single consensus ranking (with ties) that generalizes a set of input rankings codified as a precedence matrix. To address this, we introduce the Optimal Set of Bucket Orders Problem (OSBOP), a generalization of the OBOP that aims to produce not a single ranking as output but a set of consensus rankings. Experimental results are presented to illustrate this proposal, showing how, by providing a set of consensus rankings, the fitness of the solution significantly improves with respect to the one of the original OBOP, without losing comprehensibility.', 'abstract_zh': '基于排名聚合问题的最优桶序集问题及其应用', 'title_zh': '最优桶序集合用于部分排名聚合的问题'}
{'arxiv_id': 'arXiv:2502.13743', 'title': 'Inference of Abstraction for Grounded Predicate Logic', 'authors': 'Hiroyuki Kido', 'link': 'https://arxiv.org/abs/2502.13743', 'abstract': 'An important open question in AI is what simple and natural principle enables a machine to reason logically for meaningful abstraction with grounded symbols. This paper explores a conceptually new approach to combining probabilistic reasoning and predicative symbolic reasoning over data. We return to the era of reasoning with a full joint distribution before the advent of Bayesian networks. We then discuss that a full joint distribution over models of exponential size in propositional logic and of infinite size in predicate logic should be simply derived from a full joint distribution over data of linear size. We show that the same process is not only enough to generalise the logical consequence relation of predicate logic but also to provide a new perspective to rethink well-known limitations such as the undecidability of predicate logic, the symbol grounding problem and the principle of explosion. The reproducibility of this theoretical work is fully demonstrated by the included proofs.', 'abstract_zh': '人工智能领域的一个重要开放问题是，什么简单而自然的原则能让机器进行逻辑推理并实现基于具体符号的有意义抽象？本文探讨了一种结合概率推理和预测性符号推理的新概念性方法。我们回到贝叶斯网络出现之前的全联合分布推理时代。然后讨论在命题逻辑中，全联合分布在模型上的大小呈指数级，在谓词逻辑中则为无限大，这种全联合分布应该能从数据的线性大小的全联合分布中简单推导而出。我们证明，同样的过程不仅足以推广谓词逻辑的逻辑后承关系，还提供了重新思考谓词逻辑的不可判定性、符号接地问题以及矛盾原则的新视角。本文的理论工作由包含的证明完全展示了其可重现性。', 'title_zh': '基于Grounded谓词逻辑的抽象推理'}
{'arxiv_id': 'arXiv:2502.13731', 'title': 'Robust Counterfactual Inference in Markov Decision Processes', 'authors': 'Jessica Lally, Milad Kazemi, Nicola Paoletti', 'link': 'https://arxiv.org/abs/2502.13731', 'abstract': 'This paper addresses a key limitation in existing counterfactual inference methods for Markov Decision Processes (MDPs). Current approaches assume a specific causal model to make counterfactuals identifiable. However, there are usually many causal models that align with the observational and interventional distributions of an MDP, each yielding different counterfactual distributions, so fixing a particular causal model limits the validity (and usefulness) of counterfactual inference. We propose a novel non-parametric approach that computes tight bounds on counterfactual transition probabilities across all compatible causal models. Unlike previous methods that require solving prohibitively large optimisation problems (with variables that grow exponentially in the size of the MDP), our approach provides closed-form expressions for these bounds, making computation highly efficient and scalable for non-trivial MDPs. Once such an interval counterfactual MDP is constructed, our method identifies robust counterfactual policies that optimise the worst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate our method on various case studies, demonstrating improved robustness over existing methods.', 'abstract_zh': '本文解决了一类马尔可夫决策过程(MDP)反事实推理方法中存在的关键限制。当前的方法假设特定的因果模型以使反事实可识别。然而，通常存在许多与MDP的观察分布和干预分布相一致的因果模型，每个模型会产生不同的反事实分布，固定特定的因果模型限制了反事实推理的有效性（及其实用性）。我们提出了一种新颖的非参数方法，该方法在所有兼容因果模型上计算反事实转换概率的紧界。与先前需要求解难以承受规模的优化问题（变量随MDP规模呈指数增长）的方法不同，我们的方法提供了这些界的具体形式表达式，使得计算在非平凡的MDP上变得高度高效和可扩展。一旦构建了这样的区间反事实MDP，我们的方法就能识别出在不确定的区间MDP概率下优化最坏情况奖励的稳健反事实策略。我们通过各种案例研究评估了该方法，展示了其相比现有方法的鲁棒性改进。', 'title_zh': '马尔可夫决策过程中的鲁棒反事实推理'}
{'arxiv_id': 'arXiv:2502.13701', 'title': 'Causes and Strategies in Multiagent Systems', 'authors': 'Sylvia S. Kerkhove, Natasha Alechina, Mehdi Dastani', 'link': 'https://arxiv.org/abs/2502.13701', 'abstract': "Causality plays an important role in daily processes, human reasoning, and artificial intelligence. There has however not been much research on causality in multi-agent strategic settings. In this work, we introduce a systematic way to build a multi-agent system model, represented as a concurrent game structure, for a given structural causal model. In the obtained so-called causal concurrent game structure, transitions correspond to interventions on agent variables of the given causal model. The Halpern and Pearl framework of causality is used to determine the effects of a certain value for an agent variable on other variables. The causal concurrent game structure allows us to analyse and reason about causal effects of agents' strategic decisions. We formally investigate the relation between causal concurrent game structures and the original structural causal models.", 'abstract_zh': '因果关系在日常过程、人类推理和人工智能中起着重要作用。然而，在多智能体战略性设置中关于因果关系的研究较少。在本文中，我们提出了一个系统的方法，以给定的结构因果模型为基础构建一个表示为并发游戏结构的多智能体系统模型。在获得的所谓的因果并发游戏结构中，转换对应于对给定因果模型中的智能体变量进行干预。我们使用Halpern和Pearl的因果框架来确定特定智能体变量值对其他变量的影响。因果并发游戏结构使我们能够分析和推理智能体战略性决策的因果效果。我们正式地研究了因果并发游戏结构与原始结构因果模型之间的关系。', 'title_zh': '多Agent系统中的原因与策略研究'}
{'arxiv_id': 'arXiv:2502.13569', 'title': 'Model Evolution Framework with Genetic Algorithm for Multi-Task Reinforcement Learning', 'authors': 'Yan Yu, Wengang Zhou, Yaodong Yang, Wanxuan Lu, Yingyan Hou, Houqiang Li', 'link': 'https://arxiv.org/abs/2502.13569', 'abstract': "Multi-task reinforcement learning employs a single policy to complete various tasks, aiming to develop an agent with generalizability across different scenarios. Given the shared characteristics of tasks, the agent's learning efficiency can be enhanced through parameter sharing. Existing approaches typically use a routing network to generate specific routes for each task and reconstruct a set of modules into diverse models to complete multiple tasks simultaneously. However, due to the inherent difference between tasks, it is crucial to allocate resources based on task difficulty, which is constrained by the model's structure. To this end, we propose a Model Evolution framework with Genetic Algorithm (MEGA), which enables the model to evolve during training according to the difficulty of the tasks. When the current model is insufficient for certain tasks, the framework will automatically incorporate additional modules, enhancing the model's capabilities. Moreover, to adapt to our model evolution framework, we introduce a genotype module-level model, using binary sequences as genotype policies for model reconstruction, while leveraging a non-gradient genetic algorithm to optimize these genotype policies. Unlike routing networks with fixed output dimensions, our approach allows for the dynamic adjustment of the genotype policy length, enabling it to accommodate models with a varying number of modules. We conducted experiments on various robotics manipulation tasks in the Meta-World benchmark. Our state-of-the-art performance demonstrated the effectiveness of the MEGA framework. We will release our source code to the public.", 'abstract_zh': '基于遗传算法的模型进化框架（MEGA）在多任务强化学习中的应用', 'title_zh': '基于遗传算法的多任务 reinforcement learning 模型演化框架'}
{'arxiv_id': 'arXiv:2502.13516', 'title': 'SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin', 'authors': 'Hao Yi, Qingyang Li, Yulan Hu, Fuzheng Zhang, Di Zhang, Yong Liu', 'link': 'https://arxiv.org/abs/2502.13516', 'abstract': 'Recently, enhancing the numerical and logical reasoning capability of Large Language Models (LLMs) has emerged as a research hotspot. Existing methods face several limitations: inference-phase techniques (e.g., Chain of Thoughts) rely on prompt selection and the pretrained knowledge; sentence-level Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) struggle with step-wise mathematical correctness and depend on stronger models distillation or human annotations; while Reinforcement Learning (RL) approaches incur high GPU memory costs and unstable training. To address these, we propose \\textbf{S}elf-training framework integrating \\textbf{P}rocess \\textbf{P}reference learning using \\textbf{D}ynamic value margin (SPPD). SPPD leverages a process-based Markov Decision Process (MDP) and Bellman optimality equation to derive \\textbf{dynamic value margin} on step-level preference optimization, which employs tree-based self-sampling on model responses \\textbf{without any distillation} from other models. Furthermore, we theoretically prove that SPPD is \\textbf{equivalent to on-policy policy gradient methods} under reward constraints. Experiments on 7B-scale models demonstrate superior performance across in-domain and out-domain mathematical benchmarks. We open-source our code at \\href{this https URL}{this https URL}.', 'abstract_zh': '最近，增强大型语言模型的数值和逻辑推理能力已成为研究热点。我们提出了一种结合过程偏好学习和动态价值差距的自训练框架（SPPD）。SPPD 利用基于过程的马尔可夫决策过程（MDP）和贝尔曼最优方程来在步骤级偏好优化中得出动态价值差距，该方法通过树状结构自我采样模型响应，而不依赖其他模型的蒸馏。此外，我们理论证明在奖励约束条件下，SPPD 等同于在线策略策略梯度方法。在涵盖领域内外的数学基准测试中，7B规模模型展示了优越的表现。我们已开源代码，网址为：this https URL。', 'title_zh': 'SPPD：基于动态价值边际的进程偏好学习自训练'}
{'arxiv_id': 'arXiv:2502.13476', 'title': 'Integration of Agentic AI with 6G Networks for Mission-Critical Applications: Use-case and Challenges', 'authors': 'Sunder Ali Khowaja, Kapal Dev, Muhammad Salman Pathan, Engin Zeydan, Merouane Debbah', 'link': 'https://arxiv.org/abs/2502.13476', 'abstract': 'We are in a transformative era, and advances in Artificial Intelligence (AI), especially the foundational models, are constantly in the news. AI has been an integral part of many applications that rely on automation for service delivery, and one of them is mission-critical public safety applications. The problem with AI-oriented mission-critical applications is the humanin-the-loop system and the lack of adaptability to dynamic conditions while maintaining situational awareness. Agentic AI (AAI) has gained a lot of attention recently due to its ability to analyze textual data through a contextual lens while quickly adapting to conditions. In this context, this paper proposes an AAI framework for mission-critical applications. We propose a novel framework with a multi-layer architecture to realize the AAI. We also present a detailed implementation of AAI layer that bridges the gap between network infrastructure and missioncritical applications. Our preliminary analysis shows that the AAI reduces initial response time by 5.6 minutes on average, while alert generation time is reduced by 15.6 seconds on average and resource allocation is improved by up to 13.4%. We also show that the AAI methods improve the number of concurrent operations by 40, which reduces the recovery time by up to 5.2 minutes. Finally, we highlight some of the issues and challenges that need to be considered when implementing AAI frameworks.', 'abstract_zh': '面向关键任务应用的代理人工智能框架', 'title_zh': '将代理人工智能与6G网络集成以应用于关键任务应用：案例研究与挑战'}
{'arxiv_id': 'arXiv:2502.13430', 'title': 'Vision-Based Generic Potential Function for Policy Alignment in Multi-Agent Reinforcement Learning', 'authors': 'Hao Ma, Shijie Wang, Zhiqiang Pu, Siyao Zhao, Xiaolin Ai', 'link': 'https://arxiv.org/abs/2502.13430', 'abstract': 'Guiding the policy of multi-agent reinforcement learning to align with human common sense is a difficult problem, largely due to the complexity of modeling common sense as a reward, especially in complex and long-horizon multi-agent tasks. Recent works have shown the effectiveness of reward shaping, such as potential-based rewards, to enhance policy alignment. The existing works, however, primarily rely on experts to design rule-based rewards, which are often labor-intensive and lack a high-level semantic understanding of common sense. To solve this problem, we propose a hierarchical vision-based reward shaping method. At the bottom layer, a visual-language model (VLM) serves as a generic potential function, guiding the policy to align with human common sense through its intrinsic semantic understanding. To help the policy adapts to uncertainty and changes in long-horizon tasks, the top layer features an adaptive skill selection module based on a visual large language model (vLLM). The module uses instructions, video replays, and training records to dynamically select suitable potential function from a pre-designed pool. Besides, our method is theoretically proven to preserve the optimal policy. Extensive experiments conducted in the Google Research Football environment demonstrate that our method not only achieves a higher win rate but also effectively aligns the policy with human common sense.', 'abstract_zh': '基于视觉的分层奖励塑造方法引导多智能体强化学习政策与人类常识对齐', 'title_zh': '基于视觉的通用潜在函数在多智能体 reinforcement learning 中的策略对齐'}
{'arxiv_id': 'arXiv:2502.13392', 'title': 'Atomic Proximal Policy Optimization for Electric Robo-Taxi Dispatch and Charger Allocation', 'authors': 'Jim Dai, Manxi Wu, Zhanhao Zhang', 'link': 'https://arxiv.org/abs/2502.13392', 'abstract': 'Pioneering companies such as Waymo have deployed robo-taxi services in several U.S. cities. These robo-taxis are electric vehicles, and their operations require the joint optimization of ride matching, vehicle repositioning, and charging scheduling in a stochastic environment. We model the operations of the ride-hailing system with robo-taxis as a discrete-time, average reward Markov Decision Process with infinite horizon. As the fleet size grows, the dispatching is challenging as the set of system state and the fleet dispatching action set grow exponentially with the number of vehicles. To address this, we introduce a scalable deep reinforcement learning algorithm, called Atomic Proximal Policy Optimization (Atomic-PPO), that reduces the action space using atomic action decomposition. We evaluate our algorithm using real-world NYC for-hire vehicle data and we measure the performance using the long-run average reward achieved by the dispatching policy relative to a fluid-based reward upper bound. Our experiments demonstrate the superior performance of our Atomic-PPO compared to benchmarks. Furthermore, we conduct extensive numerical experiments to analyze the efficient allocation of charging facilities and assess the impact of vehicle range and charger speed on fleet performance.', 'abstract_zh': 'Waymo等先驱公司在多个美国城市部署了ロbo-taxi服务。这些ロbo-taxi是电动车辆，其运营需要在一个随机环境中对行程匹配、车辆重新定位和充电调度进行联合优化。我们将配备ロbo-taxi的叫车系统建模为一个离散时间的平均奖励马尔可夫决策过程（无穷 horizons）。随着车队规模的增加，调度变得具有挑战性，因为系统状态集和调度动作集会随车辆数量的增加呈指数级增长。为此，我们引入了一种可扩展的深度强化学习算法——原子近端策略优化（Atomic-PPO），该算法通过原子动作分解减少了动作空间。我们使用纽约市的实际情况数据评估了该算法，并使用调度策略相对于基于流的奖励上界的长期平均奖励来衡量性能。我们的实验表明，相比基准算法，我们的Atomic-PPO表现出更优的性能。此外，我们进行了广泛的数值实验，分析了充电设施的高效分配，并评估了车辆续航能力和充电速度对车队性能的影响。', 'title_zh': '原子近端策略优化在电动机器人出租车调度与充电站分配中的应用'}
{'arxiv_id': 'arXiv:2502.13389', 'title': 'Reasoning with Reinforced Functional Token Tuning', 'authors': 'Kongcheng Zhang, Qi Yao, Baisheng Lai, Jiaxing Huang, Wenkai Fang, Dacheng Tao, Mingli Song, Shunyu Liu', 'link': 'https://arxiv.org/abs/2502.13389', 'abstract': 'In this work, we propose Reinforced Functional Token Tuning (RFTT), a novel reinforced fine-tuning framework that empowers Large Language Models (LLMs) with self-play learn-to-reason capabilities. Unlike prior prompt-driven reasoning efforts, RFTT embeds a rich set of learnable functional tokens (e.g., <analyze>, <verify>, <refine>) directly into the model vocabulary, enabling chain-of-thought construction with diverse human-like reasoning behaviors. Specifically, RFTT comprises two phases: (1) supervised fine-tuning performs prompt-driven tree search to obtain self-generated training data annotated with functional tokens, which warms up the model to learn these tokens for reasoning; and (2) online reinforcement learning further allows the model to explore different reasoning pathways through functional token sampling without relying on prompts, thereby facilitating effective self-improvement for functional reasoning. Extensive experiments demonstrate the superiority of the proposed RFTT on mathematical benchmarks, significantly boosting Qwen-2.5-7B-Instruct (70.6% to 79.8%) and LLaMA-3.1-8B-Instruct (32.2% to 60.2%) on the MATH dataset. Moreover, the performance of RFTT consistently improves with more search rollouts at inference time. Our code is available at this https URL.', 'abstract_zh': '基于强化的学习可推理功能标记调优（RFTT）：一种增强的大语言模型推理能力的新框架', 'title_zh': '强化功能标记调谐推理'}
{'arxiv_id': 'arXiv:2502.13388', 'title': 'Reflection of Episodes: Learning to Play Game from Expert and Self Experiences', 'authors': 'Xiaojie Xu, Zongyuan Li, Chang Lu, Runnan Qi, Yanan Ni, Lumin Jiang, Xiangbei Liu, Xuebo Zhang, Yongchun Fang, Kuihua Huang, Xian Guo, Zhanghua Wu, Zhenya Li', 'link': 'https://arxiv.org/abs/2502.13388', 'abstract': 'StarCraft II is a complex and dynamic real-time strategy (RTS) game environment, which is very suitable for artificial intelligence and reinforcement learning research. To address the problem of Large Language Model(LLM) learning in complex environments through self-reflection, we propose a Reflection of Episodes(ROE) framework based on expert experience and self-experience. This framework first obtains key information in the game through a keyframe selection method, then makes decisions based on expert experience and self-experience. After a game is completed, it reflects on the previous experience to obtain new self-experience. Finally, in the experiment, our method beat the robot under the Very Hard difficulty in TextStarCraft II. We analyze the data of the LLM in the process of the game in detail, verified its effectiveness.', 'abstract_zh': 'StarCraft II是一个复杂多变的即时战略(RTS)游戏环境，非常适合人工智能和强化学习研究。为了解决大规模语言模型（LLM）在复杂环境中的学习问题，我们提出了一种基于专家经验和自我经验的episode反思（ROE）框架。该框架首先通过关键帧选择方法获取游戏中的关键信息，然后基于专家经验和自我经验进行决策。在一局游戏结束后，对其进行反思以获得新的自我经验。在实验中，我们的方法在TextStarCraft II的极难模式下战胜了机器人。我们详细分析了游戏过程中大规模语言模型的数据，验证了其有效性。', 'title_zh': '基于专家和自我经验的episode反映：学习玩游戏'}
{'arxiv_id': 'arXiv:2502.13373', 'title': 'Fighter Jet Navigation and Combat using Deep Reinforcement Learning with Explainable AI', 'authors': 'Swati Kar, Soumyabrata Dey, Mahesh K Banavar, Shahnewaz Karim Sakib', 'link': 'https://arxiv.org/abs/2502.13373', 'abstract': "This paper presents the development of an Artificial Intelligence (AI) based fighter jet agent within a customized Pygame simulation environment, designed to solve multi-objective tasks via deep reinforcement learning (DRL). The jet's primary objectives include efficiently navigating the environment, reaching a target, and selectively engaging or evading an enemy. A reward function balances these goals while optimized hyperparameters enhance learning efficiency. Results show more than 80\\% task completion rate, demonstrating effective decision-making. To enhance transparency, the jet's action choices are analyzed by comparing the rewards of the actual chosen action (factual action) with those of alternate actions (counterfactual actions), providing insights into the decision-making rationale. This study illustrates DRL's potential for multi-objective problem-solving with explainable AI. Project page is available at: \\href{this https URL}{Project GitHub Link}.", 'abstract_zh': '基于定制Pygame仿真环境的AI战斗机代理开发：通过深度强化学习解决多目标任务', 'title_zh': '基于可解释AI的战斗机导航与战斗深度强化学习方法'}
{'arxiv_id': 'arXiv:2502.13313', 'title': 'Revisiting Privacy, Utility, and Efficiency Trade-offs when Fine-Tuning Large Language Models', 'authors': 'Soumi Das, Camila Kolling, Mohammad Aflah Khan, Mahsa Amani, Bishwamittra Ghosh, Qinyuan Wu, Till Speicher, Krishna P. Gummadi', 'link': 'https://arxiv.org/abs/2502.13313', 'abstract': 'We study the inherent trade-offs in minimizing privacy risks and maximizing utility, while maintaining high computational efficiency, when fine-tuning large language models (LLMs). A number of recent works in privacy research have attempted to mitigate privacy risks posed by memorizing fine-tuning data by using differentially private training methods (e.g., DP), albeit at a significantly higher computational cost (inefficiency). In parallel, several works in systems research have focussed on developing (parameter) efficient fine-tuning methods (e.g., LoRA), but few works, if any, investigated whether such efficient methods enhance or diminish privacy risks. In this paper, we investigate this gap and arrive at a surprising conclusion: efficient fine-tuning methods like LoRA mitigate privacy risks similar to private fine-tuning methods like DP. Our empirical finding directly contradicts prevailing wisdom that privacy and efficiency objectives are at odds during fine-tuning. Our finding is established by (a) carefully defining measures of privacy and utility that distinguish between memorizing sensitive and non-sensitive tokens in training and test datasets used in fine-tuning and (b) extensive evaluations using multiple open-source language models from Pythia, Gemma, and Llama families and different domain-specific datasets.', 'abstract_zh': '我们研究在优化大型语言模型（LLM）微调时减少隐私风险和最大化实用性之间固有的权衡，同时保持高度的计算效率。尽管使用差分隐私训练方法（如DP）可以缓解由于记忆微调数据带来的隐私风险，但这显著增加了计算成本（低效率）。同时，系统研究中的一些工作聚焦于开发高效的微调方法（如LoRA），但很少有研究探讨这些高效的微调方法是增强还是减少了隐私风险。在本文中，我们探讨了这一差距，并得出一个令人惊讶的结论：类似于差分隐私微调方法（如DP），高效的微调方法（如LoRA）也减轻了隐私风险。我们的实证发现直接反驳了微调期间隐私和效率目标相冲突的普遍认识。这一发现通过（a）仔细定义区分训练和测试数据集中敏感和非敏感标记的隐私和实用性度量，以及（b）使用来自Pythia、Gemma和Llama家族的多个开源语言模型和不同领域特定数据集进行广泛的评估而得以确立。', 'title_zh': '重访微调大型语言模型时隐私、实用性和效率的权衡'}
{'arxiv_id': 'arXiv:2502.13295', 'title': 'Demonstrating specification gaming in reasoning models', 'authors': 'Alexander Bondarenko, Denis Volk, Dmitrii Volkov, Jeffrey Ladish', 'link': 'https://arxiv.org/abs/2502.13295', 'abstract': "We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like o1 preview and DeepSeek-R1 will often hack the benchmark by default, while language models like GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work to hack.\nWe improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber capabilities testing.", 'abstract_zh': '我们通过指示模型战胜国际象棋引擎来展示大规模语言模型代理规范游戏。我们发现，如o1 preview和DeepSeek-R1这类推理模型通常默认会作弊，而如GPT-4o和Claude 3.5 Sonnet这类语言模型需要明确被告知正常玩法无效才能作弊。我们改进了前人的工作（Hubinger et al., 2024；Meinke et al., 2024；Weij et al., 2024），通过使用现实的任务提示并避免过度引导。我们的结果表明，推理模型可能会为了解决问题而采取作弊行为，类似于OpenAI (2024)在网络安全能力测试中o1 Docker逃脱时的作弊行为。', 'title_zh': '证明推理模型中的规范游戏行为'}
{'arxiv_id': 'arXiv:2502.13170', 'title': 'Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment', 'authors': 'Yuze Zhao, Tianyun Ji, Wenjun Feng, Zhenya Huang, Qi Liu, Zhiding Liu, Yixiao Ma, Kai Zhang, Enhong Chen', 'link': 'https://arxiv.org/abs/2502.13170', 'abstract': 'The reasoning abilities are one of the most enigmatic and captivating aspects of large language models (LLMs). Numerous studies are dedicated to exploring and expanding the boundaries of this reasoning capability. However, tasks that embody both reasoning and recall characteristics are often overlooked. In this paper, we introduce such a novel task, code reasoning, to provide a new perspective for the reasoning abilities of LLMs. We summarize three meta-benchmarks based on established forms of logical reasoning, and instantiate these into eight specific benchmark tasks. Our testing on these benchmarks reveals that LLMs continue to struggle with identifying satisfactory reasoning pathways. Additionally, we present a new pathway exploration pipeline inspired by human intricate problem-solving methods. This Reflective Hypothesis Decomposition and Amendment (RHDA) pipeline consists of the following iterative steps: (1) Proposing potential hypotheses based on observations and decomposing them; (2) Utilizing tools to validate hypotheses and reflection outcomes; (3) Revising hypothesis in light of observations. Our approach effectively mitigates logical chain collapses arising from forgetting or hallucination issues in multi-step reasoning, resulting in performance gains of up to $3\\times$. Finally, we expanded this pipeline by applying it to simulate complex household tasks in real-world scenarios, specifically in VirtualHome, enhancing the handling of failure cases. We release our code and all of results at this https URL.', 'abstract_zh': '大型语言模型的推理能力是迄今最神秘和迷人的方面之一。许多研究致力于探索和扩展这一推理能力的边界。然而，同时包含推理和回忆特征的任务往往被忽视。在本文中，我们引入了一种新的任务——代码推理，以提供大型语言模型推理能力的新视角。我们基于已建立的逻辑推理形式总结了三个元基准，并将这些形式具体化为八个特定的基准任务。在这些基准上的测试表明，大型语言模型仍然难以识别满意的推理路径。此外，我们提出了一种新的路径探索管道，受到人类复杂问题解决方法的启发。该Reflective Hypothesis Decomposition and Amendment (RHDA)管道包含以下迭代步骤：（1）基于观察提出潜在假设并分解它们；（2）使用工具验证假设和反思结果；（3）根据观察结果修订假设。我们的方法有效地缓解了多步推理中由于忘记或幻觉问题导致的逻辑链条断裂问题，从而获得了高达$3\\times$的性能提升。最后，我们通过将该管道应用于模拟现实世界中的复杂家务任务（特别是在VirtualHome中）来扩展了这个管道，增强了对失败案例的处理能力。我们在此网址发布我们的代码和所有结果：https://。', 'title_zh': '通过假设分解与修正揭示代码推理的魔力'}
{'arxiv_id': 'arXiv:2502.13149', 'title': 'Bi-Fact: A Bidirectional Factorization-based Evaluation of Intent Extraction from UI Trajectories', 'authors': 'Sapir Caduri', 'link': 'https://arxiv.org/abs/2502.13149', 'abstract': 'Bi-Fact, a novel approach to automatic evaluation for Intent Understanding, is presented. Drawing inspiration from FactScore, Bi-Fact enables fine-grained intent comparison by splitting both gold and predicted intents into facts and calculating precision and recall, considering the UI trajectory. This paper outlines a comprehensive evaluation of Bi-Fact, assessing its performance and comparing it to existing metrics.', 'abstract_zh': 'Bi-Fact：一种新颖的意图理解自动评估方法', 'title_zh': '双向分解：基于双向分解的UI轨迹意图提取评价方法'}
{'arxiv_id': 'arXiv:2502.13965', 'title': 'Autellix: An Efficient Serving Engine for LLM Agents as General Programs', 'authors': 'Michael Luo, Xiaoxiang Shi, Colin Cai, Tianjun Zhang, Justin Wong, Yichuan Wang, Chi Wang, Yanping Huang, Zhifeng Chen, Joseph E. Gonzalez, Ion Stoica', 'link': 'https://arxiv.org/abs/2502.13965', 'abstract': "Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.", 'abstract_zh': '大型语言模型（LLM）应用正在从简单的聊天机器人演变成为动态的通用代理程序，这些程序扩展了LLM的调用和输出令牌以帮助AI代理进行推理、探索和解决复杂任务。然而，现有的LLM服务系统忽略了程序之间及其调用之间的依赖关系，错过了优化的重要机会。我们的分析表明，提交给LLM服务引擎的程序经历了长时间的累积等待时间，主要是由于个体LLM请求和程序级别的头部阻塞。为解决这一问题，我们引入了Autellix，这是一种将程序视为一等公民的LLM服务系统，以最小化其端到端延迟。Autellix拦截程序提交的LLM调用，通过为调度器提供程序级别的上下文来增强调度。我们提出了两种调度算法，分别针对单线程和分布式程序，根据程序之前完成的调用来预取和优先级排序LLM调用。我们的评估显示，与最先进的系统（如vLLM）相比，在相同的延迟条件下，Autellix将程序的吞吐量提高了4-15倍。', 'title_zh': 'Autellix: 一种高效的LLM代理通用程序服务引擎'}
{'arxiv_id': 'arXiv:2502.13964', 'title': 'A Training-Free Framework for Precise Mobile Manipulation of Small Everyday Objects', 'authors': 'Arjun Gupta, Rishik Sathua, Saurabh Gupta', 'link': 'https://arxiv.org/abs/2502.13964', 'abstract': 'Many everyday mobile manipulation tasks require precise interaction with small objects, such as grasping a knob to open a cabinet or pressing a light switch. In this paper, we develop Servoing with Vision Models (SVM), a closed-loop training-free framework that enables a mobile manipulator to tackle such precise tasks involving the manipulation of small objects. SVM employs an RGB-D wrist camera and uses visual servoing for control. Our novelty lies in the use of state-of-the-art vision models to reliably compute 3D targets from the wrist image for diverse tasks and under occlusion due to the end-effector. To mitigate occlusion artifacts, we employ vision models to out-paint the end-effector thereby significantly enhancing target localization. We demonstrate that aided by out-painting methods, open-vocabulary object detectors can serve as a drop-in module to identify semantic targets (e.g. knobs) and point tracking methods can reliably track interaction sites indicated by user clicks. This training-free method obtains an 85% zero-shot success rate on manipulating unseen objects in novel environments in the real world, outperforming an open-loop control method and an imitation learning baseline trained on 1000+ demonstrations by an absolute success rate of 50%.', 'abstract_zh': '基于视觉模型的服务机器人微观操作框架（Servoing with Vision Models for Precision Manipulation of Small Objects）', 'title_zh': '无需训练的框架以精确操控日常小型物体'}
{'arxiv_id': 'arXiv:2502.13957', 'title': 'RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision', 'authors': 'Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang', 'link': 'https://arxiv.org/abs/2502.13957', 'abstract': 'Retrieval-augmented generation (RAG) has shown great potential for knowledge-intensive tasks, but its traditional architectures rely on static retrieval, limiting their effectiveness for complex questions that require sequential information-seeking. While agentic reasoning and search offer a more adaptive approach, most existing methods depend heavily on prompt engineering. In this work, we introduce RAG-Gym, a unified optimization framework that enhances information-seeking agents through fine-grained process supervision at each search step. We also propose ReSearch, a novel agent architecture that synergizes answer reasoning and search query generation within the RAG-Gym framework. Experiments on four challenging datasets show that RAG-Gym improves performance by up to 25.6\\% across various agent architectures, with ReSearch consistently outperforming existing baselines. Further analysis highlights the effectiveness of advanced LLMs as process reward judges and the transferability of trained reward models as verifiers for different LLMs. Additionally, we examine the scaling properties of training and inference in agentic RAG. The project homepage is available at this https URL.', 'abstract_zh': '检索增强生成（RAG）在知识密集型任务中显示出巨大潜力，但其传统的架构依赖静态检索，限制了其对需要顺序信息查找的复杂问题的有效性。尽管代理式推理和搜索提供了更加适应的解决方案，但大多数现有方法仍然高度依赖于提示工程。在本工作中，我们引入了RAG-Gym，这是一种统一的优化框架，通过在每次搜索步骤中提供精细的过程监督来增强信息查找代理。我们还提出了一种名为ReSearch的新一代代理架构，在RAG-Gym框架中结合了答案推理和搜索查询生成。在四个具有挑战性的数据集上的实验表明，RAG-Gym在各种代理架构上将性能提高了最多25.6%，而ReSearch持续优于现有基线。进一步的分析表明高级LLM作为过程奖励裁判的有效性和训练的奖励模型对不同LLM作为验证者的可迁移性。此外，我们还研究了代理式RAG的训练和推理的扩展性。项目主页可通过此链接访问：https://xxxxxxx。', 'title_zh': 'RAG-Gym: 通过过程监督优化推理和搜索智能体'}
{'arxiv_id': 'arXiv:2502.13946', 'title': "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region", 'authors': 'Chak Tou Leong, Qingyu Yin, Jian Wang, Wenjie Li', 'link': 'https://arxiv.org/abs/2502.13946', 'abstract': "The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.", 'abstract_zh': '大型语言模型的安全对齐仍存在脆弱性，因为它们的初始行为可以通过相对简单的攻击轻易被破解。由于现有大型语言模型在输入指令与初始模型输出之间填充固定模板是一种常见做法，我们假设该模板是其脆弱性的关键因素：大型语言模型的安全相关决策过度依赖模板区域的综合信息，这大大影响了这些模型的安全行为。我们将这一问题称为模板锚定的安全对齐。在本文中，我们进行了广泛的实验，并验证了模板锚定的安全对齐在各种对齐的大型语言模型中普遍存在。我们的机制分析揭示了它如何导致模型在遇到推理时的破解攻击中变得易受攻击。此外，我们证明了将安全机制与模板区域分离是减轻破解攻击脆弱性的有希望的方法。我们鼓励未来的研究开发减少对模板区域依赖的更稳健的安全对齐技术。', 'title_zh': '为什么保护性船舶会搁浅？对齐的大型语言模型的安全机制倾向于集中在模板区域。'}
{'arxiv_id': 'arXiv:2502.13935', 'title': 'Continually Learning Structured Visual Representations via Network Refinement with Rerelation', 'authors': 'Zeki Doruk Erden, Boi Faltings', 'link': 'https://arxiv.org/abs/2502.13935', 'abstract': 'Current machine learning paradigm relies on continuous representations like neural networks, which iteratively adjust parameters to approximate outcomes rather than directly learning the structure of problem. This spreads information across the network, causing issues like information loss and incomprehensibility Building on prior work in environment dynamics modeling, we propose a method that learns visual space in a structured, continual manner. Our approach refines networks to capture the core structure of objects while representing significant subvariants in structure efficiently. We demonstrate this with 2D shape detection, showing incremental learning on MNIST without overwriting knowledge and creating compact, comprehensible representations. These results offer a promising step toward a transparent, continually learning alternative to traditional neural networks for visual processing.', 'abstract_zh': '当前的机器学习范式依赖于神经网络等连续表示，通过迭代调整参数来逼近结果，而不是直接学习问题的结构。这种方法在网络中传播信息，导致信息丢失和不可理解性等问题。基于先前在环境动力学建模方面的研究，我们提出了一种在结构化、连续方式下学习视觉空间的方法。我们的方法 refinements 网络以捕捉对象的核心结构，同时有效地表示结构上的显著亚变体。我们通过2D形状检测展示了这一点，展示了在MNIST上进行增量学习而不覆盖知识，并生成紧凑且易于理解的表示。这些结果为透明的、连续学习的传统神经网络替代方案提供了有希望的步骤，用于视觉处理。', 'title_zh': '持续学习结构化视觉表示通过关系网络精炼'}
{'arxiv_id': 'arXiv:2502.13928', 'title': 'Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images', 'authors': 'Shengguang Wu, Fan-Yun Sun, Kaiyue Wen, Nick Haber', 'link': 'https://arxiv.org/abs/2502.13928', 'abstract': "Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises because existing VLMs are not explicitly trained to generate texts that are accurately grounded in fine-grained image details. To enhance visual feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive Optimization), a novel finetuning objective that steers the model toward capturing important visual details and aligning them with corresponding text tokens. To further facilitate this detailed alignment, we introduce MVC, a paired image-text dataset built by automatically filtering and augmenting visual counterfactual data to challenge the model with hard contrastive cases involving Minimal Visual Contrasts. Experiments show that our method consistently improves VLM performance across diverse benchmarks covering various abilities and domains, achieving up to a 22% reduction in hallucinations, and significant gains in vision-centric and general tasks. Notably, these improvements become increasingly pronounced in benchmarks with higher visual dependency. In short, S-VCO offers a significant enhancement of VLM's visually-dependent task performance while retaining or even improving the model's general abilities. We opensource our code at this https URL", 'abstract_zh': "Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises because existing VLMs are not explicitly trained to generate texts that are accurately grounded in fine-grained image details. To enhance visual feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive Optimization), a novel finetuning objective that steers the model toward capturing important visual details and aligning them with corresponding text tokens. To further facilitate this detailed alignment, we introduce MVC, a paired image-text dataset built by automatically filtering and augmenting visual counterfactual data to challenge the model with hard contrastive cases involving Minimal Visual Contrasts. Experiments show that our method consistently improves VLM performance across diverse benchmarks covering various abilities and domains, achieving up to a 22% reduction in hallucinations, and significant gains in vision-centric and general tasks. Notably, these improvements become increasingly pronounced in benchmarks with higher visual dependency. In short, S-VCO offers a significant enhancement of VLM's visually-dependent task performance while retaining or even improving the model's general abilities. 我们开源了我们的代码，地址为：this https URL。", 'title_zh': '对称视觉对比优化：通过最少的对比图像对齐视觉语言模型'}
{'arxiv_id': 'arXiv:2502.13913', 'title': 'How Do LLMs Perform Two-Hop Reasoning in Context?', 'authors': 'Tianyu Guo, Hanlin Zhu, Ruiqi Zhang, Jiantao Jiao, Song Mei, Michael I. Jordan, Stuart Russell', 'link': 'https://arxiv.org/abs/2502.13913', 'abstract': '"Socrates is human. All humans are mortal. Therefore, Socrates is mortal." This classical example demonstrates two-hop reasoning, where a conclusion logically follows from two connected premises. While transformer-based Large Language Models (LLMs) can make two-hop reasoning, they tend to collapse to random guessing when faced with distracting premises. To understand the underlying mechanism, we train a three-layer transformer on synthetic two-hop reasoning tasks. The training dynamics show two stages: a slow learning phase, where the 3-layer transformer performs random guessing like LLMs, followed by an abrupt phase transitions, where the 3-layer transformer suddenly reaches $100%$ accuracy. Through reverse engineering, we explain the inner mechanisms for how models learn to randomly guess between distractions initially, and how they learn to ignore distractions eventually. We further propose a three-parameter model that supports the causal claims for the mechanisms to the training dynamics of the transformer. Finally, experiments on LLMs suggest that the discovered mechanisms generalize across scales. Our methodologies provide new perspectives for scientific understandings of LLMs and our findings provide new insights into how reasoning emerges during training.', 'abstract_zh': '苏格拉底是人。所有人都是 Mortal。因此，苏格拉底是 Mortal。这个经典的例子展示了两跳推理，其中结论从两个相连的前提中逻辑地得出。虽然基于Transformer的大语言模型（LLMs）可以进行两跳推理，但面对分散注意力的前提时，它们往往会退化为随机猜测。为了理解其内部机制，我们对三层Transformer进行了训练，以完成合成的两跳推理任务。训练动态显示了两个阶段：一个缓慢的学习阶段，在此期间三层Transformer像LLMs一样进行随机猜测，随后是一个突变的相变阶段，在此阶段三层Transformer突然达到100%的准确率。通过逆向工程，我们解释了模型如何在初始阶段随机猜测干扰信息以学习推理，以及如何最终学会忽略干扰信息。我们进一步提出了一个三参数模型，该模型支持对变压器训练动态中机制因果关系的说明。最后，对LLMs的实验表明，发现的机制在不同规模上具有泛化能力。我们的方法论为大语言模型的科学理解提供了新的视角，我们的发现为推理在训练期间如何产生提供了新的见解。', 'title_zh': 'LLMs在上下文中进行两_hop推理的表现如何？'}
{'arxiv_id': 'arXiv:2502.13909', 'title': 'Lost in Sequence: Do Large Language Models Understand Sequential Recommendation?', 'authors': 'Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park', 'link': 'https://arxiv.org/abs/2502.13909', 'abstract': "Large Language Models (LLMs) have recently emerged as promising tools for recommendation thanks to their advanced textual understanding ability and context-awareness. Despite the current practice of training and evaluating LLM-based recommendation (LLM4Rec) models under a sequential recommendation scenario, we found that whether these models understand the sequential information inherent in users' item interaction sequences has been largely overlooked. In this paper, we first demonstrate through a series of experiments that existing LLM4Rec models do not fully capture sequential information both during training and inference. Then, we propose a simple yet effective LLM-based sequential recommender, called LLM-SRec, a method that enhances the integration of sequential information into LLMs by distilling the user representations extracted from a pre-trained CF-SRec model into LLMs. Our extensive experiments show that LLM-SRec enhances LLMs' ability to understand users' item interaction sequences, ultimately leading to improved recommendation performance. Furthermore, unlike existing LLM4Rec models that require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by training only a few lightweight MLPs, highlighting its practicality in real-world applications. Our code is available at this https URL.", 'abstract_zh': '大规模语言模型(LLMs)近年来由于其先进的文本理解和上下文意识能力，已成为推荐领域的有前途的工具。尽管目前的实践是在序列推荐场景下训练和评估基于LLM的推荐(LLM4Rec)模型，我们发现这些模型是否能够充分理解用户项交互序列中固有的序列信息已经被很大程度地忽视。在本文中，我们首先通过一系列实验展示了现有的LLM4Rec模型在训练和推理过程中并未充分捕捉序列信息。然后，我们提出了一种简单而有效的基于LLM的序列推荐器，称为LLM-SRec，该方法通过从预训练的CF-SRec模型中提取用户表示并将其 distilled 到LLM中，增强序列信息在LLM中的整合。我们的实验表明，LLM-SRec提高了LLM理解用户项交互序列的能力，最终提升了推荐性能。此外，与现有的LLM4Rec模型需要对LLM进行微调不同，LLM-SRec仅通过训练少数轻量级的MLP即可实现最先进的性能，突显了其在实际应用中的实用性。代码可从此链接获取。', 'title_zh': '迷失在序列中：大型语言模型理解序列推荐吗？'}
{'arxiv_id': 'arXiv:2502.13905', 'title': 'Partially Observable Gaussian Process Network and Doubly Stochastic Variational Inference', 'authors': 'Saksham Kiroriwal, Julius Pfrommer, Jürgen Beyerer', 'link': 'https://arxiv.org/abs/2502.13905', 'abstract': 'To reduce the curse of dimensionality for Gaussian processes (GP), they can be decomposed into a Gaussian Process Network (GPN) of coupled subprocesses with lower dimensionality. In some cases, intermediate observations are available within the GPN. However, intermediate observations are often indirect, noisy, and incomplete in most real-world systems. This work introduces the Partially Observable Gaussian Process Network (POGPN) to model real-world process networks. We model a joint distribution of latent functions of subprocesses and make inferences using observations from all subprocesses. POGPN incorporates observation lenses (observation likelihoods) into the well-established inference method of deep Gaussian processes. We also introduce two training methods for POPGN to make inferences on the whole network using node observations. The application to benchmark problems demonstrates how incorporating partial observations during training and inference can improve the predictive performance of the overall network, offering a promising outlook for its practical application.', 'abstract_zh': '基于部分可观测性的高斯过程网络用于降低高维性的 curse', 'title_zh': '部分可观测高斯过程网络与双重随机变分推断'}
{'arxiv_id': 'arXiv:2502.13897', 'title': 'DataSciBench: An LLM Agent Benchmark for Data Science', 'authors': 'Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, Yisong Yue', 'link': 'https://arxiv.org/abs/2502.13897', 'abstract': 'This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at this https URL.', 'abstract_zh': 'DataSciBench：一种评估大型语言模型数据科学能力的综合性基准', 'title_zh': 'DataSciBench: 一个数据科学LLM代理基准测试'}
{'arxiv_id': 'arXiv:2502.13881', 'title': 'PSCon: Toward Conversational Product Search', 'authors': 'Jie Zou, Mohammad Aliannejadi, Evangelos Kanoulas, Shuxi Han, Heli Ma, Zheng Wang, Yang Yang, Heng Tao Shen', 'link': 'https://arxiv.org/abs/2502.13881', 'abstract': 'Conversational Product Search (CPS) is confined to simulated conversations due to the lack of real-world CPS datasets that reflect human-like language. Additionally, current conversational datasets are limited to support cross-market and multi-lingual usage. In this paper, we introduce a new CPS data collection protocol and present PSCon, a novel CPS dataset designed to assist product search via human-like conversations. The dataset is constructed using a coached human-to-human data collection protocol and supports two languages and dual markets. Also, the dataset enables thorough exploration of six subtasks of CPS: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation. Furthermore, we also offer an analysis of the dataset and propose a benchmark model on the proposed CPS dataset.', 'abstract_zh': '基于对话的产品搜索（CPS）受限于缺乏反映人类语言的现实世界CPS数据集，此外，现有的对话数据集也限制了跨市场和多语言的应用。本文介绍了一种新的CPS数据收集协议，并推出了PSCon这一新型CPS数据集，旨在通过类似人类的对话方式辅助产品搜索。该数据集采用引导的人际数据收集协议构建，支持两种语言和两个市场。此外，该数据集还能够全面探索CPS的六个子任务：用户意图检测、关键词提取、系统动作预测、问题选择、物品排名和响应生成。Furthermore，我们还对该数据集进行了分析，并在所提出的CPS数据集上提出了一个基准模型。', 'title_zh': 'PSCon: 向会话式产品搜索迈进'}
{'arxiv_id': 'arXiv:2502.13875', 'title': 'MEX: Memory-efficient Approach to Referring Multi-Object Tracking', 'authors': 'Huu-Thien Tran, Phuoc-Sang Pham, Thai-Son Tran, Khoa Luu', 'link': 'https://arxiv.org/abs/2502.13875', 'abstract': 'Referring Multi-Object Tracking (RMOT) is a relatively new concept that has rapidly gained traction as a promising research direction at the intersection of computer vision and natural language processing. Unlike traditional multi-object tracking, RMOT identifies and tracks objects and incorporates textual descriptions for object class names, making the approach more intuitive. Various techniques have been proposed to address this challenging problem; however, most require the training of the entire network due to their end-to-end nature. Among these methods, iKUN has emerged as a particularly promising solution. Therefore, we further explore its pipeline and enhance its performance. In this paper, we introduce a practical module dubbed Memory-Efficient Cross-modality -- MEX. This memory-efficient technique can be directly applied to off-the-shelf trackers like iKUN, resulting in significant architectural improvements. Our method proves effective during inference on a single GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI dataset, which offers diverse autonomous driving scenes with relevant language expressions, is particularly useful for studying this problem. Empirically, our method demonstrates effectiveness and efficiency regarding HOTA tracking scores, substantially improving memory allocation and processing speed.', 'abstract_zh': '基于参考的多对象跟踪（RMOT）是一种相对较新的概念，作为计算机视觉和自然语言处理交叉领域的有前途的研究方向，正迅速获得关注。RMOT不同于传统的多对象跟踪，它能够识别和跟踪对象，并结合文本描述以提高对象类别名称的可理解性。为了解决这一具有挑战性的问题，提出了多种技术；然而，大多数方法由于是端到端的，需要重新训练整个网络。在这之中，iKUN 成为了一个特别有前景的解决方案。因此，我们进一步探索其管道并改进其性能。在本文中，我们引入了一个名为Memory-Efficient Cross-modality -- MEX的实用模块。这项内存效率高的技术可以直接应用于现成的跟踪器，如iKUN，从而带来架构上的显著改进。我们的方法在单个带有4 GB内存的GPU上进行推断时证明是有效的。特别是，Refer-KITTI数据集因其提供多种多样的自动驾驶场景及相关的语言表达，对于研究这一问题特别有用。我们的方法在HOTA跟踪评分上表现出有效性与效率，并显著改善了内存分配和处理速度。', 'title_zh': 'MEX: MEMORY-EFFICIENT APPROACH TO REFERRING MULTI-OBJECT TRACKING'}
{'arxiv_id': 'arXiv:2502.13873', 'title': 'NVR: Vector Runahead on NPUs for Sparse Memory Access', 'authors': 'Hui Wang, Zhengpeng Zhao, Jing Wang, Yushu Du, Yuan Cheng, Bing Guo, He Xiao, Chenhao Ma, Xiaomeng Han, Dean You, Jiapeng Guan, Ran Wei, Dawei Yang, Zhe Jiang', 'link': 'https://arxiv.org/abs/2502.13873', 'abstract': 'Deep Neural Networks are increasingly leveraging sparsity to reduce the scaling up of model parameter size. However, reducing wall-clock time through sparsity and pruning remains challenging due to irregular memory access patterns, leading to frequent cache misses. In this paper, we present NPU Vector Runahead (NVR), a prefetching mechanism tailored for NPUs to address cache miss problems in sparse DNN workloads. Rather than optimising memory patterns with high overhead and poor portability, NVR adapts runahead execution to the unique architecture of NPUs. NVR provides a general micro-architectural solution for sparse DNN workloads without requiring compiler or algorithmic support, operating as a decoupled, speculative, lightweight hardware sub-thread alongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an average 90% reduction in cache misses compared to SOTA prefetching in general-purpose processors, delivering 4x average speedup on sparse workloads versus NPUs without prefetching. Moreover, we investigate the advantages of incorporating a small cache (16KB) into the NPU combined with NVR. Our evaluation shows that expanding this modest cache delivers 5x higher performance benefits than increasing the L2 cache size by the same amount.', 'abstract_zh': '深层神经网络正越来越多地利用稀疏性来减少模型参数规模的扩展。然而，通过稀疏性与剪枝减少实际运行时间仍然具有挑战性，因为不规则的内存访问模式导致频繁的缓存缺失。在本文中，我们提出了适用于NPUs的NPU向量前瞻机制（NPU Vector Runahead, NVR），该机制旨在解决稀疏DNN工作负载中的缓存缺失问题。NVR 不是通过具有高开销和较差移植性的优化内存模式来解决问题，而是根据NPU的独特架构进行前瞻执行的调整。NVR 提供了一种适用于稀疏DNN工作负载的一般微架构解决方案，无需编译器或算法支持，并在NPU旁边作为一个解耦、推测性的轻量级硬件子线程操作，硬件开销不到5%。NVR 在缓存缺失次数上比通用处理器的最新预取技术平均减少了90%，对于没有预取的NPUs，在稀疏工作负载上的平均加速比达到4倍。此外，我们研究了将小型缓存（16KB）集成到NPU中与NVR结合的优劣。我们的评估表明，扩大这一适度大小的缓存比增加相同量的L2缓存大小提供5倍以上的性能优势。', 'title_zh': 'NVR: Vector Runahead on NPUs for Sparse Memory Access'}
{'arxiv_id': 'arXiv:2502.13870', 'title': 'SPEX: Scaling Feature Interaction Explanations for LLMs', 'authors': 'Justin Singh Kang, Landon Butler, Abhineet Agarwal, Yigit Efe Erginbas, Ramtin Pedarsani, Kannan Ramchandran, Bin Yu', 'link': 'https://arxiv.org/abs/2502.13870', 'abstract': 'Large language models (LLMs) have revolutionized machine learning due to their ability to capture complex interactions between input features. Popular post-hoc explanation methods like SHAP provide marginal feature attributions, while their extensions to interaction importances only scale to small input lengths ($\\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic interaction attribution algorithm that efficiently scales to large input lengths ($\\approx 1000)$. SPEX exploits underlying natural sparsity among interactions -- common in real-world data -- and applies a sparse Fourier transform using a channel decoding algorithm to efficiently identify important interactions. We perform experiments across three difficult long-context datasets that require LLMs to utilize interactions between inputs to complete the task. For large inputs, SPEX outperforms marginal attribution methods by up to 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX successfully identifies key features and interactions that strongly influence model output. For one of our datasets, HotpotQA, SPEX provides interactions that align with human annotations. Finally, we use our model-agnostic approach to generate explanations to demonstrate abstract reasoning in closed-source LLMs (GPT-4o mini) and compositional reasoning in vision-language models.', 'abstract_zh': '大型语言模型（LLMs）通过捕获输入特征间的复杂交互，已经革新了机器学习。流行的事后解释方法如SHAP提供边缘特征归属，而它们的扩展方法仅适用于短输入长度（约20个）。我们提出了频谱解释器（SPEX），这是一种通用的交互归属算法，能够高效地扩展到长输入长度（约1000个）。SPEX 利用了交互中的潜在自然稀疏性——在现实世界数据中常见，并借助信道解码算法应用稀疏傅里叶变换，以高效地识别重要交互。我们在三个需要LLMs利用输入间交互才能完成任务的困难长上下文数据集上进行了实验。对于长输入，SPEX 在忠实重建LLM输出方面比边缘归属方法高出了最多20%。此外，SPEX 能够识别对模型输出有重要影响的关键特征和交互。对于我们的一个数据集HotpotQA，SPEX 提供的交互与人类注释一致。最后，我们利用通用方法为封闭源代码LLM（GPT-4o mini）生成解释，证明了其抽象推理能力，并展示了视觉-语言模型的组合推理能力。', 'title_zh': 'SPEX: 扩展大规模语言模型功能交互解释'}
{'arxiv_id': 'arXiv:2502.13847', 'title': 'DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue', 'authors': 'Feiyuan Zhang, Dezhi Zhu, James Ming, Yilun Jin, Di Chai, Liu Yang, Han Tian, Zhaoxin Fan, Kai Chen', 'link': 'https://arxiv.org/abs/2502.13847', 'abstract': 'Retrieval-Augmented Generation (RAG) systems have shown substantial benefits in applications such as question answering and multi-turn dialogue \\citep{lewis2020retrieval}. However, traditional RAG methods, while leveraging static knowledge bases, often overlook the potential of dynamic historical information in ongoing conversations. To bridge this gap, we introduce DH-RAG, a Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue. DH-RAG is inspired by human cognitive processes that utilize both long-term memory and immediate historical context in conversational responses \\citep{stafford1987conversational}. DH-RAG is structured around two principal components: a History-Learning based Query Reconstruction Module, designed to generate effective queries by synthesizing current and prior interactions, and a Dynamic History Information Updating Module, which continually refreshes historical context throughout the dialogue. The center of DH-RAG is a Dynamic Historical Information database, which is further refined by three strategies within the Query Reconstruction Module: Historical Query Clustering, Hierarchical Matching, and Chain of Thought Tracking. Experimental evaluations show that DH-RAG significantly surpasses conventional models on several benchmarks, enhancing response relevance, coherence, and dialogue quality.', 'abstract_zh': '动态历史 context 增强的检索增强生成方法 (DH-RAG) 用于多轮对话', 'title_zh': 'DH-RAG：一种基于动态历史语境的检索增强生成方法用于多轮对话'}
{'arxiv_id': 'arXiv:2502.13845', 'title': 'Enhancing LLM-Based Recommendations Through Personalized Reasoning', 'authors': 'Jiahao Liu, Xueshuo Yan, Dongsheng Li, Guangping Zhang, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu', 'link': 'https://arxiv.org/abs/2502.13845', 'abstract': "Current recommendation systems powered by large language models (LLMs) often underutilize their reasoning capabilities due to a lack of explicit logical structuring. To address this limitation, we introduce CoT-Rec, a framework that integrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by incorporating two crucial processes: user preference analysis and item perception evaluation. CoT-Rec operates in two key phases: (1) personalized data extraction, where user preferences and item perceptions are identified, and (2) personalized data application, where this information is leveraged to refine recommendations. Our experimental analysis demonstrates that CoT-Rec improves recommendation accuracy by making better use of LLMs' reasoning potential. The implementation is publicly available at this https URL.", 'abstract_zh': '基于大型语言模型的当前推荐系统往往由于缺乏明确的逻辑结构而未能充分利用其推理能力。为了克服这一局限性，我们引入了CoT-Rec框架，该框架通过整合用户偏好分析和物品感知评估，将Chain-of-Thought（CoT）推理融入到由大型语言模型驱动的推荐中。CoT-Rec分为两个关键阶段：(1) 个性化数据提取，识别用户偏好和物品感知，(2) 个性化数据应用，利用这些信息改进推荐。实验分析表明，CoT-Rec通过更有效地利用大型语言模型的推理潜力提高了推荐精度。该实施已在此处公开：this https URL。', 'title_zh': '通过个性化推理增强基于LLM的推荐'}
{'arxiv_id': 'arXiv:2502.13843', 'title': 'Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based User Agents', 'authors': 'Jiahao Liu, Shengkang Gu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu', 'link': 'https://arxiv.org/abs/2502.13843', 'abstract': 'Large Language Model (LLM)-based user agents have emerged as a powerful tool for improving recommender systems by simulating user interactions. However, existing methods struggle with cross-domain scenarios due to inefficient memory structures, leading to irrelevant information retention and failure to account for social influence factors such as popularity. To address these limitations, we introduce AgentCF++, a novel framework featuring a dual-layer memory architecture and a two-step fusion mechanism to filter domain-specific preferences effectively. Additionally, we propose interest groups with shared memory, allowing the model to capture the impact of popularity trends on users with similar interests. Through extensive experiments on multiple cross-domain datasets, AgentCF++ demonstrates superior performance over baseline models, highlighting its effectiveness in refining user behavior simulation for recommender systems. Our code is available at this https URL.', 'abstract_zh': '基于大型语言模型（LLM）的用户代理通过模拟用户交互提高了推荐系统的性能，但在跨域场景中由于不高效的内存结构而遇到挑战，导致无关信息的保留和社交影响因素如流行性考虑不周。为解决这些限制，我们提出了AgentCF++这一新型框架，该框架采用双层内存架构和两步融合机制，有效过滤领域特定的偏好。此外，我们还提出了共享内存的兴趣群组，使模型能够捕捉类似兴趣用户受到流行趋势影响的情况。通过在多个跨域数据集上的广泛实验，AgentCF++在基线模型上表现出更优的表现，突显了其在细化推荐系统中用户行为模拟方面的能力。我们的代码可在以下链接获取：这个 https URL。', 'title_zh': '基于内存优化的语言模型驱动用户代理以增强跨域推荐'}
{'arxiv_id': 'arXiv:2502.13840', 'title': 'Mitigating Popularity Bias in Collaborative Filtering through Fair Sampling', 'authors': 'Jiahao Liu, Dongsheng Li, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu', 'link': 'https://arxiv.org/abs/2502.13840', 'abstract': 'Recommender systems often suffer from popularity bias, where frequently interacted items are overrepresented in recommendations. This bias stems from propensity factors influencing training data, leading to imbalanced exposure. In this paper, we introduce a Fair Sampling (FS) approach to address this issue by ensuring that both users and items are selected with equal probability as positive and negative instances. Unlike traditional inverse propensity score (IPS) methods, FS does not require propensity estimation, eliminating errors associated with inaccurate calculations. Our theoretical analysis demonstrates that FS effectively neutralizes the influence of propensity factors, achieving unbiased learning. Experimental results validate that FS outperforms state-of-the-art methods in both point-wise and pair-wise recommendation tasks, enhancing recommendation fairness without sacrificing accuracy. The implementation is available at this https URL.', 'abstract_zh': '推荐系统常常遭受流行性偏差的影响，其中高频互动的项目在推荐中过度代表性。这种偏差源于影响训练数据的倾向性因素，导致曝光不均。在本文中，我们引入了一种公平采样（FS）方法，通过确保用户和项目以相等概率被选为正例和负例来解决这一问题。与传统的逆倾向性得分（IPS）方法不同，FS 不需要进行倾向性估计，从而消除了与不准确计算相关的错误。我们的理论分析表明，FS 有效地抵消了倾向性因素的影响，实现了无偏学习。实验结果验证了在点wise和pairwise推荐任务中，FS 在提高推荐公平性的同时优于最先进的方法。实现代码可在以下链接获取：this https URL。', 'title_zh': '通过公平采样缓解协同过滤中的流行性偏差'}
{'arxiv_id': 'arXiv:2502.13836', 'title': 'Quantifying Memorization and Retriever Performance in Retrieval-Augmented Vision-Language Models', 'authors': 'Peter Carragher, Abhinand Jha, R Raghav, Kathleen M. Carley', 'link': 'https://arxiv.org/abs/2502.13836', 'abstract': 'Large Language Models (LLMs) demonstrate remarkable capabilities in question answering (QA), but metrics for assessing their reliance on memorization versus retrieval remain underdeveloped. Moreover, while finetuned models are state-of-the-art on closed-domain tasks, general-purpose models like GPT-4o exhibit strong zero-shot performance. This raises questions about the trade-offs between memorization, generalization, and retrieval. In this work, we analyze the extent to which multimodal retrieval-augmented VLMs memorize training data compared to baseline VLMs. Using the WebQA benchmark, we contrast finetuned models with baseline VLMs on multihop retrieval and question answering, examining the impact of finetuning on data memorization. To quantify memorization in end-to-end retrieval and QA systems, we propose several proxy metrics by investigating instances where QA succeeds despite retrieval failing. Our results reveal the extent to which finetuned models rely on memorization. In contrast, retrieval-augmented VLMs have lower memorization scores, at the cost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a challenge for future work to reconcile memorization and generalization in both Open-Domain QA and joint Retrieval-QA tasks.', 'abstract_zh': '大型语言模型在问答任务中展现出显著的能力，但评估其依赖记忆而非检索的度量仍然发展不足。此外，虽然微调模型在封闭域任务中表现卓越，通用型模型如GPT-4在零样本情况下表现出强大性能。这引发了记忆、泛化和检索间权衡关系的疑问。在本文中，我们分析了模式增强的多模态视觉语言模型在记忆训练数据方面相较于基线模型的程度。通过使用WebQA基准，我们将微调模型与基线视觉语言模型在多跳检索和问答任务中进行对比，探讨微调对数据记忆的影响。为量化端到端检索和问答系统中的记忆程度，我们提出了几种代理度量，通过调查检索失败但问答仍成功的情况。我们的结果显示，微调模型在多大程度上依赖记忆。相比之下，增强检索的视觉语言模型的记忆分数较低，但准确率较低（WebQA测试集上的准确率分别为72%和52%）。因此，我们的指标对后续研究提出了挑战，要求在开放式领域问答和联合检索-问答任务中解决记忆和泛化之间的平衡。', 'title_zh': '量化 Retrieval-Augmented Vision-Language 模型中记忆化和检索器性能'}
{'arxiv_id': 'arXiv:2502.13805', 'title': 'AnDB: Breaking Boundaries with an AI-Native Database for Universal Semantic Analysis', 'authors': 'Tianqing Wang, Xun Xue, Guoliang Li, Yong Wang', 'link': 'https://arxiv.org/abs/2502.13805', 'abstract': 'In this demonstration, we present AnDB, an AI-native database that supports traditional OLTP workloads and innovative AI-driven tasks, enabling unified semantic analysis across structured and unstructured data. While structured data analytics is mature, challenges remain in bridging the semantic gap between user queries and unstructured data. AnDB addresses these issues by leveraging cutting-edge AI-native technologies, allowing users to perform semantic queries using intuitive SQL-like statements without requiring AI expertise. This approach eliminates the ambiguity of traditional text-to-SQL systems and provides a seamless end-to-end optimization for analyzing all data types. AnDB automates query processing by generating multiple execution plans and selecting the optimal one through its optimizer, which balances accuracy, execution time, and financial cost based on user policies and internal optimizing mechanisms. AnDB future-proofs data management infrastructure, empowering users to effectively and efficiently harness the full potential of all kinds of data without starting from scratch.', 'abstract_zh': 'AnDB：一种支持传统OLTP工作负载和创新AI驱动任务的AI原生数据库，实现结构化和非结构化数据的统一语义分析。', 'title_zh': 'AnDB：以AI原生数据库打破边界实现通用语义分析'}
{'arxiv_id': 'arXiv:2502.13794', 'title': 'LESA: Learnable LLM Layer Scaling-Up', 'authors': 'Yifei Yang, Zouying Cao, Xinbei Ma, Yao Yao, Libo Qin, Zhi Chen, Hai Zhao', 'link': 'https://arxiv.org/abs/2502.13794', 'abstract': 'Training Large Language Models (LLMs) from scratch requires immense computational resources, making it prohibitively expensive. Model scaling-up offers a promising solution by leveraging the parameters of smaller models to create larger ones. However, existing depth scaling-up methods rely on empirical heuristic rules for layer duplication, which result in poorer initialization and slower convergence during continual pre-training. We propose \\textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating parameters from each layer and applying Singular Value Decomposition, we uncover latent patterns between layers, suggesting that inter-layer parameters can be learned. LESA uses a neural network to predict the parameters inserted between adjacent layers, enabling better initialization and faster training. Experiments show that LESA outperforms existing baselines, achieving superior performance with less than half the computational cost during continual pre-training. Extensive analyses demonstrate its effectiveness across different model sizes and tasks.', 'abstract_zh': '一种可学习的深度扩展方法：LESA', 'title_zh': 'LESA: 学习可调整的大语言模型层规模增长'}
{'arxiv_id': 'arXiv:2502.13785', 'title': 'Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA Therapeutics', 'authors': 'Matthew Wood, Mathieu Klop, Maxime Allard', 'link': 'https://arxiv.org/abs/2502.13785', 'abstract': "mRNA-based vaccines have become a major focus in the pharmaceutical industry. The coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can strongly influence translation efficiency, stability, degradation, and other factors that collectively determine a vaccine's effectiveness. However, optimizing mRNA sequences for those properties remains a complex challenge. Existing deep learning models often focus solely on coding region optimization, overlooking the UTRs. We present Helix-mRNA, a structured state-space-based and attention hybrid model to address these challenges. In addition to a first pre-training, a second pre-training stage allows us to specialise the model with high-quality data. We employ single nucleotide tokenization of mRNA sequences with codon separation, ensuring prior biological and structural information from the original mRNA sequence is not lost. Our model, Helix-mRNA, outperforms existing methods in analysing both UTRs and coding region properties. It can process sequences 6x longer than current approaches while using only 10% of the parameters of existing foundation models. Its predictive capabilities extend to all mRNA regions. We open-source the model (this https URL) and model weights (this https URL).", 'abstract_zh': '基于mRNA的疫苗已成为制药行业的重点。mRNA的编码序列及其未翻译区（UTRs）可以强烈影响翻译效率、稳定性、降解以及其他决定疫苗有效性的因素。然而，对这些属性进行优化仍然是一个复杂的挑战。现有的深度学习模型往往仅专注于编码区域优化，忽视了UTRs。我们提出Helix-mRNA，这是一种结构化状态空间和注意力机制结合的模型，以解决这些挑战。除了初始预训练外，还有一个额外的预训练阶段，使模型能够使用高质量数据专门化。我们使用单碱基 token 化方式处理mRNA序列，并进行密码子分割，确保原始mRNA序列的先前生物学和结构信息不丢失。我们的模型Helix-mRNA在分析UTRs和编码区域属性方面优于现有方法。它可以处理是现有方法6倍长的序列，同时仅使用现有基础模型10%的参数量，其预测能力涵盖了所有mRNA区域。我们开源了该模型（请参见此处：https://），以及模型权重（请参见此处：https://）。', 'title_zh': '螺旋-mRNA：一种混合基础模型用于全长mRNA治疗剂'}
{'arxiv_id': 'arXiv:2502.13778', 'title': 'Poster: SpiderSim: Multi-Agent Driven Theoretical Cybersecurity Simulation for Industrial Digitalization', 'authors': 'Jiaqi Li, Xizhong Guo, Yang Zhao, Lvyang Zhang, Lidong Zhai', 'link': 'https://arxiv.org/abs/2502.13778', 'abstract': "Rapid industrial digitalization has created intricate cybersecurity demands that necessitate effective validation methods. While cyber ranges and simulation platforms are widely deployed, they frequently face limitations in scenario diversity and creation efficiency. In this paper, we present SpiderSim, a theoretical cybersecurity simulation platform enabling rapid and lightweight scenario generation for industrial digitalization security research. At its core, our platform introduces three key innovations: a structured framework for unified scenario modeling, a multi-agent collaboration mechanism for automated generation, and modular atomic security capabilities for flexible scenario composition. Extensive implementation trials across multiple industrial digitalization contexts, including marine ranch monitoring systems, validate our platform's capacity for broad scenario coverage with efficient generation processes. Built on solid theoretical foundations and released as open-source software, SpiderSim facilitates broader research and development in automated security testing for industrial digitalization.", 'abstract_zh': '快速工业数字化创造了复杂的网络安全需求， necessitate有效的验证方法。尽管广泛应用了网络范围和模拟平台，它们经常在场景多样性和生成效率方面面临限制。在本文中，我们介绍了SpiderSim，这是一种理论上的网络安全模拟平台，用于工业数字化安全研究中的快速和轻量级场景生成。该平台的核心包含三项关键创新：统一场景建模的结构化框架、自动生成的多代理协作机制以及模块化原子安全能力，以实现灵活的场景组合。在包括海洋牧场监测系统等多种工业数字化背景下广泛的实施试验表明，该平台能够通过高效的生成过程实现广泛的场景覆盖。基于坚实的理论基础并在作为开源软件发布后，SpiderSim促进了自动化安全测试在工业数字化领域的更广泛研究和开发。', 'title_zh': '海报：SpiderSim：基于多agent的工业数字化理论网络安全模拟系统'}
{'arxiv_id': 'arXiv:2502.13775', 'title': 'VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare', 'authors': 'Anudeex Shetty, Amin Beheshti, Mark Dras, Usman Naseem', 'link': 'https://arxiv.org/abs/2502.13775', 'abstract': 'Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or monolithic preference, failing to account for the diversity of perspectives across cultures, demographics, and communities. This limitation is particularly critical in health-related scenarios, where plurality is essential due to the influence of culture, religion, personal values, and conflicting opinions. Despite progress in pluralistic alignment, no prior work has focused on health, likely due to the unavailability of publicly available datasets. To address this gap, we introduce VITAL, a new benchmark dataset comprising 13.1K value-laden situations and 5.4K multiple-choice questions focused on health, designed to assess and benchmark pluralistic alignment methodologies. Through extensive evaluation of eight LLMs of varying sizes, we demonstrate that existing pluralistic alignment techniques fall short in effectively accommodating diverse healthcare beliefs, underscoring the need for tailored AI alignment in specific domains. This work highlights the limitations of current approaches and lays the groundwork for developing health-specific alignment solutions.', 'abstract_zh': '现有的对齐技术已成为确保大型语言模型（LLMs）生成符合人类价值观的输出的核心。然而，现有的对齐范式通常建模平均或统一的偏好，未能考虑到不同文化、人口统计和社区之间的视角多样性。这一局限性在与文化、宗教、个人价值观和不同意见相关的健康场景中尤为重要。尽管在多元对齐方面取得了进展，但此前没有研究关注健康领域，很可能是因为缺乏公开可用的数据集。为填补这一空白，我们引入了VITAL，这是一个新的基准数据集，包含13100个价值导向的情境和5400个多选题，专注于健康领域，旨在评估和基准测试多元对齐方法。通过对八种不同规模的LLM的广泛评估，我们证明现有的多元对齐技术在有效包容多样化的卫生保健信念方面存在不足，突显了在特定领域内为AI对齐量身定制方案的必要性。这项工作突显了当前方法的局限性，并为开发针对健康领域的对齐解决方案奠定了基础。', 'title_zh': 'VITAL：一个新的数据集，用于 healthcare 领域多样共识基准测试'}
{'arxiv_id': 'arXiv:2502.13767', 'title': 'AI Software Engineer: Programming with Trust', 'authors': 'Abhik Roychoudhury, Corina Pasareanu, Michael Pradel, Baishakhi Ray', 'link': 'https://arxiv.org/abs/2502.13767', 'abstract': 'Large Language Models (LLMs) have shown surprising proficiency in generating code snippets, promising to automate large parts of software engineering via artificial intelligence (AI). We argue that successfully deploying AI software engineers requires a level of trust equal to or even greater than the trust established by human-driven software engineering practices. The recent trend toward LLM agents offers a path toward integrating the power of LLMs to create new code with the power of analysis tools to increase trust in the code. This opinion piece comments on whether LLM agents could dominate software engineering workflows in the future and whether the focus of programming will shift from programming at scale to programming with trust.', 'abstract_zh': '大型语言模型在生成代码片段方面展示了惊人的能力，有望通过人工智能自动化软件工程的大部分工作。我们argue认为，成功部署AI软件工程师所需的信任程度，不应低于或甚至高于由人类驱动的软件工程实践所建立的信任。最近LLM代理的趋势为结合大型语言模型的生成能力与分析工 具的验证能力以提高代码信任度提供了可能。本文的观点讨论了未来LLM代理是否可能主导软件工程工作流程，以及编程焦点是否会从大规模编程转向基于信任的编程。', 'title_zh': 'AI软件工程师：编程以信任为基础'}
{'arxiv_id': 'arXiv:2502.13764', 'title': 'An Overall Real-Time Mechanism for Classification and Quality Evaluation of Rice', 'authors': 'Wanke Xia, Ruxin Peng, Haoqi Chu, Xinlei Zhu, Zhiyu Yang, Yaojun Wang', 'link': 'https://arxiv.org/abs/2502.13764', 'abstract': 'Rice is one of the most widely cultivated crops globally and has been developed into numerous varieties. The quality of rice during cultivation is primarily determined by its cultivar and characteristics. Traditionally, rice classification and quality assessment rely on manual visual inspection, a process that is both time-consuming and prone to errors. However, with advancements in machine vision technology, automating rice classification and quality evaluation based on its cultivar and characteristics has become increasingly feasible, enhancing both accuracy and efficiency. This study proposes a real-time evaluation mechanism for comprehensive rice grain assessment, integrating a one-stage object detection approach, a deep convolutional neural network, and traditional machine learning techniques. The proposed framework enables rice variety identification, grain completeness grading, and grain chalkiness evaluation. The rice grain dataset used in this study comprises approximately 20,000 images from six widely cultivated rice varieties in China. Experimental results demonstrate that the proposed mechanism achieves a mean average precision (mAP) of 99.14% in the object detection task and an accuracy of 97.89% in the classification task. Furthermore, the framework attains an average accuracy of 97.56% in grain completeness grading within the same rice variety, contributing to an effective quality evaluation system.', 'abstract_zh': '全球种植最为广泛的稻谷作物之一已发展出众多品种。稻谷在栽培期间的质量主要由其品种和特征决定。传统上，稻谷分类和质量评估依赖手工视觉检查，这一过程既耗时又容易出错。然而，随着机器视觉技术的进步，基于品种和特征自动进行稻谷分类和质量评价变得日益可行，提高了准确性和效率。本研究提出了一种实时的综合稻谷粒品质评价机制，结合了一阶段物体检测方法、深度卷积神经网络和传统的机器学习技术。所提出的框架能够实现稻谷品种识别、籽粒完整度分级和籽粒垩白评价。本研究使用的稻谷粒数据集包含来自中国广泛种植的六种水稻品种的约20,000张图像。实验结果表明，所提出机制在物体检测任务中的平均平均精确度（mAP）为99.14%，分类任务的准确率为97.89%。此外，该框架在同一水稻品种中的籽粒完整度分级的平均准确率为97.56%，有助于构建有效的质量评价系统。', 'title_zh': '实时分类与质量评估综合机制 for 米类'}
{'arxiv_id': 'arXiv:2502.13755', 'title': 'GPA: Grover Policy Agent for Generating Optimal Quantum Sensor Circuits', 'authors': 'Ahmad Alomari, Sathish A. P. Kumar', 'link': 'https://arxiv.org/abs/2502.13755', 'abstract': 'This study proposes a GPA for designing optimal Quantum Sensor Circuits (QSCs) to address complex quantum physics problems. The GPA consists of two parts: the Quantum Policy Evaluation (QPE) and the Quantum Policy Improvement (QPI). The QPE performs phase estimation to generate the search space, while the QPI utilizes Grover search and amplitude amplification techniques to efficiently identify an optimal policy that generates optimal QSCs. The GPA generates QSCs by selecting sequences of gates that maximize the Quantum Fisher Information (QFI) while minimizing the number of gates. The QSCs generated by the GPA are capable of producing entangled quantum states, specifically the squeezed states. High QFI indicates increased sensitivity to parameter changes, making the circuit useful for quantum state estimation and control tasks. Evaluation of the GPA on a QSC that consists of two qubits and a sequence of R_x, R_y, and S gates demonstrates its efficiency in generating optimal QSCs with a QFI of 1. Compared to existing quantum agents, the GPA achieves higher QFI with fewer gates, demonstrating a more efficient and scalable approach to the design of QSCs. This work illustrates the potential computational power of quantum agents for solving quantum physics problems', 'abstract_zh': '本研究提出了一种GPA，用于设计最优量子传感器电路（QSCs）以解决复杂量子物理学问题。GPA包括两部分：量子策略评估（QPE）和量子策略改进（QPI）。QPE执行相位估计以生成搜索空间，而QPI利用Grover搜索和振幅放大技术来高效地识别生成最优QSCs的最优策略。GPA通过选择最大化量子费雪信息（QFI）并最小化门的数量的门序列表来生成QSCs。由GPA生成的QSCs能够产生纠缠量子态，特别是压缩态。高QFI表明参数变化的敏感度增加，使电路适用于量子态估计和控制任务。对由两个量子位和一系列R_x、R_y和S门组成的QSC进行评估，展示了GPA在生成具有QFI为1的最优QSCs方面的高效性。与现有量子代理相比，GPA在更少的门数量下实现了更高的QFI，展示了设计QSCs更加高效和可扩展的方法。本工作展示了量子代理在解决量子物理学问题方面的潜在计算能力。', 'title_zh': 'GPA: 使用Grover算法的政策代理生成最优量子传感器电路'}
{'arxiv_id': 'arXiv:2502.13751', 'title': 'RobustX: Robust Counterfactual Explanations Made Easy', 'authors': 'Junqi Jiang, Luca Marzari, Aaryan Purohit, Francesco Leofante', 'link': 'https://arxiv.org/abs/2502.13751', 'abstract': "The increasing use of Machine Learning (ML) models to aid decision-making in high-stakes industries demands explainability to facilitate trust. Counterfactual Explanations (CEs) are ideally suited for this, as they can offer insights into the predictions of an ML model by illustrating how changes in its input data may lead to different outcomes. However, for CEs to realise their explanatory potential, significant challenges remain in ensuring their robustness under slight changes in the scenario being explained. Despite the widespread recognition of CEs' robustness as a fundamental requirement, a lack of standardised tools and benchmarks hinders a comprehensive and effective comparison of robust CE generation methods. In this paper, we introduce RobustX, an open-source Python library implementing a collection of CE generation and evaluation methods, with a focus on the robustness property. RobustX provides interfaces to several existing methods from the literature, enabling streamlined access to state-of-the-art techniques. The library is also easily extensible, allowing fast prototyping of novel robust CE generation and evaluation methods.", 'abstract_zh': '机器学习模型在高风险行业辅助决策中的应用越来越多，亟需提高透明度以增加信任。反事实解释（CEs）在这方面尤为适用，因为它们可以揭示 machine learning 模型的预测结果，并展示输入数据变化可能带来的不同结果。然而，要充分发挥 CE 的解释潜力，仍需解决在解释场景稍有变化时保证其鲁棒性的重要挑战。尽管鲁棒性被广泛认为是 CE 的基本原则之一，但由于缺乏标准化的工具和基准，使得全面且有效的鲁棒 CE 生成方法比较变得困难。本文引入了 RobustX，这是一个开源 Python 库，实现了多种 CE 生成和评估方法，并特别关注鲁棒性这一特性。RobustX 提供了对文献中多种现有方法的接口，使得最先进的技术可以一键访问。该库还易于扩展，便于快速原型设计新的鲁棒 CE 生成和评估方法。', 'title_zh': 'RobustX: 简化 robust 反事实解释'}
{'arxiv_id': 'arXiv:2502.13728', 'title': 'Secure Federated Data Distillation', 'authors': 'Marco Arazzi, Mert Cihangiroglu, Serena Nicolazzo, Antonino Nocera', 'link': 'https://arxiv.org/abs/2502.13728', 'abstract': "Dataset Distillation (DD) is a powerful technique for reducing large datasets into compact, representative synthetic datasets, accelerating Machine Learning training. However, traditional DD methods operate in a centralized manner, which poses significant privacy threats and reduces its applicability. To mitigate these risks, we propose a Secure Federated Data Distillation framework (SFDD) to decentralize the distillation process while preserving this http URL existing Federated Distillation techniques that focus on training global models with distilled knowledge, our approach aims to produce a distilled dataset without exposing local contributions. We leverage the gradient-matching-based distillation method, adapting it for a distributed setting where clients contribute to the distillation process without sharing raw data. The central aggregator iteratively refines a synthetic dataset by integrating client-side updates while ensuring data confidentiality. To make our approach resilient to inference attacks perpetrated by the server that could exploit gradient updates to reconstruct private data, we create an optimized Local Differential Privacy approach, called LDPO-RLD (Label Differential Privacy Obfuscation via Randomized Linear Dispersion). Furthermore, we assess the framework's resilience against malicious clients executing backdoor attacks and demonstrate robustness under the assumption of a sufficient number of participating clients. Our experimental results demonstrate the effectiveness of SFDD and that the proposed defense concretely mitigates the identified vulnerabilities, with minimal impact on the performance of the distilled dataset. By addressing the interplay between privacy and federation in dataset distillation, this work advances the field of privacy-preserving Machine Learning making our SFDD framework a viable solution for sensitive data-sharing applications.", 'abstract_zh': 'Secure Federated Data Distillation Framework (SFDD)', 'title_zh': '安全联邦数据蒸馏'}
{'arxiv_id': 'arXiv:2502.13723', 'title': 'Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with Refined Values', 'authors': 'Hongbo Zhang, Han Cui, Guangsheng Bao, Linyi Yang, Jun Wang, Yue Zhang', 'link': 'https://arxiv.org/abs/2502.13723', 'abstract': 'We introduce Direct Value Optimization (DVO), an innovative reinforcement learning framework for enhancing large language models in complex reasoning tasks. Unlike traditional methods relying on preference labels, DVO utilizes value signals at individual reasoning steps, optimizing models via a mean squared error loss. The key benefit of DVO lies in its fine-grained supervision, circumventing the need for labor-intensive human annotations. Target values within the DVO are estimated using either Monte Carlo Tree Search or an outcome value model. Our empirical analysis on both mathematical and commonsense reasoning tasks shows that DVO consistently outperforms existing offline preference optimization techniques, even with fewer training steps. These findings underscore the importance of value signals in advancing reasoning capabilities and highlight DVO as a superior methodology under scenarios lacking explicit human preference information.', 'abstract_zh': '直接价值优化(DVO):一种增强大规模语言模型在复杂推理任务中的创新强化学习框架', 'title_zh': '直接价值优化：通过细化价值改进大语言模型的链式推理'}
{'arxiv_id': 'arXiv:2502.13719', 'title': 'TrustRAG: An Information Assistant with Retrieval Augmented Generation', 'authors': 'Yixing Fan, Qiang Yan, Wenshan Wang, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.13719', 'abstract': '\\Ac{RAG} has emerged as a crucial technique for enhancing large models with real-time and domain-specific knowledge. While numerous improvements and open-source tools have been proposed to refine the \\ac{RAG} framework for accuracy, relatively little attention has been given to improving the trustworthiness of generated results. To address this gap, we introduce TrustRAG, a novel framework that enhances \\ac{RAG} from three perspectives: indexing, retrieval, and generation. Specifically, in the indexing stage, we propose a semantic-enhanced chunking strategy that incorporates hierarchical indexing to supplement each chunk with contextual information, ensuring semantic completeness. In the retrieval stage, we introduce a utility-based filtering mechanism to identify high-quality information, supporting answer generation while reducing input length. In the generation stage, we propose fine-grained citation enhancement, which detects opinion-bearing sentences in responses and infers citation relationships at the sentence-level, thereby improving citation accuracy. We open-source the TrustRAG framework and provide a demonstration studio designed for excerpt-based question answering tasks \\footnote{this https URL}. Based on these, we aim to help researchers: 1) systematically enhancing the trustworthiness of \\ac{RAG} systems and (2) developing their own \\ac{RAG} systems with more reliable outputs.', 'abstract_zh': '基于TrustRAG框架：从检索、检索和生成三个视角提升大模型的可信度', 'title_zh': 'TrustRAG: 一种检索增强生成的信息助手'}
{'arxiv_id': 'arXiv:2502.13685', 'title': 'MoM: Linear Sequence Modeling with Mixture-of-Memories', 'authors': 'Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, Yu Cheng', 'link': 'https://arxiv.org/abs/2502.13685', 'abstract': 'Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brain\'s ability to maintain robust long-term memory while mitigating "memory interference", we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at this https URL and is also released as a part of this https URL.', 'abstract_zh': '基于记忆混合的线性序列建模方法', 'title_zh': 'MoM：基于混合记忆的线性序列建模'}
{'arxiv_id': 'arXiv:2502.13681', 'title': 'An LLM-based Agent for Reliable Docker Environment Configuration', 'authors': 'Ruida Hu, Chao Peng, Xinchen Wang, Cuiyun Gao', 'link': 'https://arxiv.org/abs/2502.13681', 'abstract': 'Environment configuration is a critical yet time-consuming step in software development, especially when dealing with unfamiliar code repositories. While Large Language Models (LLMs) demonstrate the potential to accomplish software engineering tasks, existing methods for environment configuration often rely on manual efforts or fragile scripts, leading to inefficiencies and unreliable outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully automate environment configuration and generate executable Dockerfiles for arbitrary Python repositories. We address two major challenges: (1) enabling the LLM agent to configure environments within isolated Docker containers, and (2) ensuring the successful configuration process is recorded and accurately transferred to a Dockerfile without error. To achieve this, we propose atomic configuration synthesis, featuring a dual-environment architecture (internal and external environment) with a rollback mechanism to prevent environment "pollution" from failed commands, guaranteeing atomic execution (execute fully or not at all) and a Dockerfile generator to transfer successful configuration steps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark of 420 recent Python repositories with unit tests, where it achieves an 86.0% success rate, outperforming the best baseline by 63.9%.', 'abstract_zh': 'Repository2Run: 基于大语言模型的全面自动化环境配置代理', 'title_zh': '基于LLM的可靠Docker环境配置代理'}
{'arxiv_id': 'arXiv:2502.13668', 'title': 'PeerQA: A Scientific Question Answering Dataset from Peer Reviews', 'authors': 'Tim Baumgärtner, Ted Briscoe, Iryna Gurevych', 'link': 'https://arxiv.org/abs/2502.13668', 'abstract': 'We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset. PeerQA questions have been sourced from peer reviews, which contain questions that reviewers raised while thoroughly examining the scientific article. Answers have been annotated by the original authors of each paper. The dataset contains 579 QA pairs from 208 academic articles, with a majority from ML and NLP, as well as a subset of other scientific communities like Geoscience and Public Health. PeerQA supports three critical tasks for developing practical QA systems: Evidence retrieval, unanswerable question classification, and answer generation. We provide a detailed analysis of the collected dataset and conduct experiments establishing baseline systems for all three tasks. Our experiments and analyses reveal the need for decontextualization in document-level retrieval, where we find that even simple decontextualization approaches consistently improve retrieval performance across architectures. On answer generation, PeerQA serves as a challenging benchmark for long-context modeling, as the papers have an average size of 12k tokens. Our code and data is available at this https URL.', 'abstract_zh': 'PeerQA：一个基于实际应用的科学文献级别问答数据集', 'title_zh': 'PeerQA：来自同行评审的科学问答数据集'}
{'arxiv_id': 'arXiv:2502.13652', 'title': 'C2T: A Classifier-Based Tree Construction Method in Speculative Decoding', 'authors': 'Feiye Huo, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Shengli Sun', 'link': 'https://arxiv.org/abs/2502.13652', 'abstract': 'The growing scale of Large Language Models (LLMs) has exacerbated inference latency and computational costs. Speculative decoding methods, which aim to mitigate these issues, often face inefficiencies in the construction of token trees and the verification of candidate tokens. Existing strategies, including chain mode, static tree, and dynamic tree approaches, have limitations in accurately preparing candidate token trees for verification. We propose a novel method named C2T that adopts a lightweight classifier to generate and prune token trees dynamically. Our classifier considers additional feature variables beyond the commonly used joint probability to predict the confidence score for each draft token to determine whether it is the candidate token for verification. This method outperforms state-of-the-art (SOTA) methods such as EAGLE-2 on multiple benchmarks, by reducing the total number of candidate tokens by 25% while maintaining or even improving the acceptance length.', 'abstract_zh': '大规模语言模型（LLMs）规模的不断扩大加剧了推理延迟和计算成本。为缓解这些问题的推测解码方法在构建令牌树和验证候选令牌方面常面临效率低下。现有的策略，包括链模式、静态树和动态树方法，在精确准备待验证的候选令牌树方面存在局限性。我们提出了一种名为C2T的新方法，该方法采用轻量级分类器动态生成和修剪令牌树。我们的分类器除了考虑常用的联合概率外，还考虑其他特征变量来预测每个草稿令牌的信心分数，以确定它是否是待验证的候选令牌。该方法在多个基准上优于现有最先进方法（SOTA），例如EAGLE-2，在减少候选令牌总数25%的同时保持甚至提高了接受长度。', 'title_zh': 'C2T：推测解码中基于分类器的树构建方法'}
{'arxiv_id': 'arXiv:2502.13638', 'title': 'Integrating Inverse and Forward Modeling for Sparse Temporal Data from Sensor Networks', 'authors': 'Julian Vexler, Björn Vieten, Martin Nelke, Stefan Kramer', 'link': 'https://arxiv.org/abs/2502.13638', 'abstract': 'We present CavePerception, a framework for the analysis of sparse data from sensor networks that incorporates elements of inverse modeling and forward modeling. By integrating machine learning with physical modeling in a hypotheses space, we aim to improve the interpretability of sparse, noisy, and potentially incomplete sensor data. The framework assumes data from a two-dimensional sensor network laid out in a graph structure that detects certain objects, with certain motion patterns. Examples of such sensors are magnetometers. Given knowledge about the objects and the way they act on the sensors, one can develop a data generator that produces data from simulated motions of the objects across the sensor field. The framework uses the simulated data to infer object behaviors across the sensor network. The approach is experimentally tested on real-world data, where magnetometers are used on an airport to detect and identify aircraft motions. Experiments demonstrate the value of integrating inverse and forward modeling, enabling intelligent systems to better understand and predict complex, sensor-driven events.', 'abstract_zh': 'CavePerception：一种将逆向建模与前向建模结合分析稀疏传感器网络数据的框架', 'title_zh': '将逆向建模与正向建模集成用于传感器网络的稀疏时序数据'}
{'arxiv_id': 'arXiv:2502.13632', 'title': 'Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization', 'authors': 'Or Raphael Bidusa, Shaul Markovitch', 'link': 'https://arxiv.org/abs/2502.13632', 'abstract': "The opaque nature of Large Language Models (LLMs) has led to significant research efforts aimed at enhancing their interpretability, primarily through post-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck Models (CBMs), offer both interpretability and intervenability by incorporating explicit concept representations. However, these methods suffer from key limitations, including reliance on labeled concept datasets and significant architectural modifications that challenges re-integration into existing system pipelines. In this work, we introduce a new methodology for incorporating interpretability and intervenability into an existing model by integrating Concept Layers (CLs) into its architecture. Our approach projects the model's internal vector representations into a conceptual, explainable vector space before reconstructing and feeding them back into the model. Furthermore, we eliminate the need for a human-selected concept set by algorithmically searching an ontology for a set of concepts that can be either task-specific or task-agnostic. We evaluate CLs across multiple tasks, demonstrating that they maintain the original model's performance and agreement while enabling meaningful interventions. Additionally, we present a proof of concept showcasing an intervenability interface, allowing users to adjust model behavior dynamically, such as mitigating biases during inference.", 'abstract_zh': '大型语言模型的不透明性质导致了旨在增强其可解释性和干预性的显著研究努力，主要通过事后方法进行。近年来，概念瓶颈模型（CBMs）等内置方法不仅提供了可解释性，而且通过引入显式概念表示还提供了干预性，但这些方法存在一些关键局限性，包括依赖于标记的概念数据集以及对现有系统管线造成重大架构修改的挑战。在此项工作中，我们提出了一种新的方法，通过将概念层（CLs）整合到现有模型的架构中，以实现可解释性与干预性的结合。我们的方法将模型的内部向量表示投影到一个概念性的、可解释的向量空间中，然后再重构并反馈回模型中。此外，我们通过算法在本体中搜索概念集，从而消除人工选择概念集的需要，这些概念集可以是任务特定的，也可以是任务无关的。我们跨多个任务评估了CLs，结果显示它们保持了原始模型的性能和一致性，同时还允许有意义的干预。此外，我们展示了概念层的一个概念证明，展示了干预接口，允许用户动态调整模型行为，例如在推断过程中缓解偏见。', 'title_zh': '概念层：通过LLM概念化增强可解释性和干预性'}
{'arxiv_id': 'arXiv:2502.13622', 'title': 'REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models', 'authors': 'DongGeon Lee, Hwanjo Yu', 'link': 'https://arxiv.org/abs/2502.13622', 'abstract': 'Hallucinations in large language model (LLM) outputs severely limit their reliability in knowledge-intensive tasks such as question answering. To address this challenge, we introduce REFIND (Retrieval-augmented Factuality hallucINation Detection), a novel framework that detects hallucinated spans within LLM outputs by directly leveraging retrieved documents. As part of the REFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that quantifies the sensitivity of LLM outputs to retrieved evidence. This innovative approach enables REFIND to efficiently and accurately detect hallucinations, setting it apart from existing methods. In the evaluation, REFIND demonstrated robustness across nine languages, including low-resource settings, and significantly outperformed baseline models, achieving superior IoU scores in identifying hallucinated spans. This work highlights the effectiveness of quantifying context sensitivity for hallucination detection, thereby paving the way for more reliable and trustworthy LLM applications across diverse languages.', 'abstract_zh': '检索增强事实幻觉检测：在大型语言模型输出中的事实幻觉检测', 'title_zh': 'REFIND: 在大规模语言模型中基于检索的事实幻觉检测'}
{'arxiv_id': 'arXiv:2502.13621', 'title': 'Decentralized Planning Using Probabilistic Hyperproperties', 'authors': 'Francesco Pontiggia, Filip Macák, Roman Andriushchenko, Michele Chiari, Milan Češka', 'link': 'https://arxiv.org/abs/2502.13621', 'abstract': 'Multi-agent planning under stochastic dynamics is usually formalised using decentralized (partially observable) Markov decision processes ( MDPs) and reachability or expected reward specifications. In this paper, we propose a different approach: we use an MDP describing how a single agent operates in an environment and probabilistic hyperproperties to capture desired temporal objectives for a set of decentralized agents operating in the environment. We extend existing approaches for model checking probabilistic hyperproperties to handle temporal formulae relating paths of different agents, thus requiring the self-composition between multiple MDPs. Using several case studies, we demonstrate that our approach provides a flexible and expressive framework to broaden the specification capabilities with respect to existing planning techniques. Additionally, we establish a close connection between a subclass of probabilistic hyperproperties and planning for a particular type of Dec-MDPs, for both of which we show undecidability. This lays the ground for the use of existing decentralized planning tools in the field of probabilistic hyperproperty verification.', 'abstract_zh': '在随机动力学下的多Agent规划通常使用去中心化（部分可观测的）马尔科夫决策过程（MDP）和可达性或期望奖励规范进行形式化。本文提出了一种不同的方法：我们使用描述单个Agent在环境中的操作的MDP，并使用概率超性质来捕获一组在环境中操作的去中心化Agent的期望时间目标。我们将现有的概率超性质模型检查方法扩展为处理连接不同Agent路径的时序公式，从而需要处理多个MDP之间的自我组合。通过几个案例研究，我们证明了该方法提供了一个灵活且表达能力强的框架，可以扩展现有的规划技术的规范能力。此外，我们建立了概率超性质的一个子类与特定类型Dec-MDP规划之间的密切联系，对于这两种情况我们都证明了不可判定性。这为在概率超性质验证领域使用现有的去中心化规划工具奠定了基础。', 'title_zh': '基于概率超性质的分布式规划'}
{'arxiv_id': 'arXiv:2502.13619', 'title': 'Complex Ontology Matching with Large Language Model Embeddings', 'authors': 'Guilherme Sousa, Rinaldo Lima, Cassia Trojahn', 'link': 'https://arxiv.org/abs/2502.13619', 'abstract': 'Ontology, and more broadly, Knowledge Graph Matching is a challenging task in which expressiveness has not been fully addressed. Despite the increasing use of embeddings and language models for this task, approaches for generating expressive correspondences still do not take full advantage of these models, in particular, large language models (LLMs). This paper proposes to integrate LLMs into an approach for generating expressive correspondences based on alignment need and ABox-based relation discovery. The generation of correspondences is performed by matching similar surroundings of instance sub-graphs. The integration of LLMs results in different architectural modifications, including label similarity, sub-graph matching, and entity matching. The performance word embeddings, sentence embeddings, and LLM-based embeddings, was compared. The results demonstrate that integrating LLMs surpasses all other models, enhancing the baseline version of the approach with a 45\\% increase in F-measure.', 'abstract_zh': '本体论和更广泛的知识图谱匹配是一个表达性尚未完全解决的挑战性任务。尽管嵌入和语言模型在该任务中的使用不断增加，但生成表达性的对应关系的方法仍未充分利用这些模型，特别是大规模语言模型（LLMs）。本文提出将LLMs集成到基于对齐需求和ABox关系发现的生成表达性对应关系的方法中。通过匹配实例子图的相似环境来进行对应关系的生成。LLMs的集成导致了不同的架构修改，包括标签相似性、子图匹配和实体匹配。比较了词嵌入、句子嵌入和基于LLM的嵌入的性能。结果表明，集成LLMs超越了所有其他模型，使方法的基础版本在F-值上提高了45%。', 'title_zh': '大规模语言模型嵌入下的复杂本体匹配'}
{'arxiv_id': 'arXiv:2502.13606', 'title': 'LaVCa: LLM-assisted Visual Cortex Captioning', 'authors': 'Takuya Matsuyama, Shinji Nishimoto, Yu Takagi', 'link': 'https://arxiv.org/abs/2502.13606', 'abstract': 'Understanding the property of neural populations (or voxels) in the human brain can advance our comprehension of human perceptual and cognitive processing capabilities and contribute to developing brain-inspired computer models. Recent encoding models using deep neural networks (DNNs) have successfully predicted voxel-wise activity. However, interpreting the properties that explain voxel responses remains challenging because of the black-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex Captioning (LaVCa), a data-driven approach that uses large language models (LLMs) to generate natural-language captions for images to which voxels are selective. By applying LaVCa for image-evoked brain activity, we demonstrate that LaVCa generates captions that describe voxel selectivity more accurately than the previously proposed method. Furthermore, the captions generated by LaVCa quantitatively capture more detailed properties than the existing method at both the inter-voxel and intra-voxel levels. Furthermore, a more detailed analysis of the voxel-specific properties generated by LaVCa reveals fine-grained functional differentiation within regions of interest (ROIs) in the visual cortex and voxels that simultaneously represent multiple distinct concepts. These findings offer profound insights into human visual representations by assigning detailed captions throughout the visual cortex while highlighting the potential of LLM-based methods in understanding brain representations. Please check out our webpage at this https URL', 'abstract_zh': '理解人类大脑中神经群体（或体素）的性质可以增进我们对人类感知和认知处理能力的理解，并有助于开发类脑计算机模型。使用深度神经网络（DNNs）的最近编码模型已成功预测了体素级活动。然而，由于DNNs的黑箱性质，解释解释体素响应的属性仍然是一个挑战。为此，我们提出了基于大语言模型的视觉皮层图解（LaVCa）方法，该方法使用大语言模型生成与体素选择性反应的图像相关联的自然语言图解。通过应用LaVCa进行图像诱发的大脑活动分析，我们证明LaVCa生成的图解比之前的方法更准确地描述了体素的选择性。此外，LaVCa生成的图解在体素间和体素内层次上更详细地定量捕捉了现有方法的属性。进一步分析LaVCa生成的体素特异性属性揭示了感兴趣区（ROIs）内视觉皮层的微细化功能差异以及同时代表多个不同概念的体素。这些发现通过在整个视觉皮层提供详细的图解，提供了对人类视觉表示的深刻见解，并突显了基于大语言模型方法在理解大脑表示方面的潜力。请访问我们的网页：https://this.url', 'title_zh': 'LaVCa: LLM辅助的视觉皮层 captioning'}
{'arxiv_id': 'arXiv:2502.13603', 'title': 'Efficient Safety Retrofitting Against Jailbreaking for LLMs', 'authors': 'Dario Garcia-Gasulla, Anna Arias-Duart, Adrian Tormos, Daniel Hinjos, Oscar Molina-Sedano, Ashwin Kumar Gururajan, Maria Eugenia Cardello', 'link': 'https://arxiv.org/abs/2502.13603', 'abstract': "Direct Preference Optimization (DPO) is an efficient alignment technique that steers LLMs towards preferable outputs by training on preference data, bypassing the need for explicit reward models. Its simplicity enables easy adaptation to various domains and safety requirements. This paper examines DPO's effectiveness in model safety against jailbreaking attacks while minimizing data requirements and training costs. We introduce Egida, a dataset expanded from multiple sources, which includes 27 different safety topics and 18 different attack styles, complemented with synthetic and human labels. This data is used to boost the safety of state-of-the-art LLMs (Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack styles. In addition to safety evaluations, we assess their post-alignment performance degradation in general purpose tasks, and their tendency to over refusal. Following the proposed methodology, trained models reduce their Attack Success Rate by 10%-30%, using small training efforts (2,000 samples) with low computational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned models generalize to unseen topics and attack styles, with the most successful attack style reaching a success rate around 5%. Size and family are found to strongly influence model malleability towards safety, pointing at the importance of pre-training choices. To validate our findings, a large independent assessment of human preference agreement with Llama-Guard-3-8B is conducted by the authors and the associated dataset Egida-HSafe is released. Overall, this study illustrates how affordable and accessible it is to enhance LLM safety using DPO while outlining its current limitations. All datasets and models are released to enable reproducibility and further research.", 'abstract_zh': 'Direct Preference Optimization (DPO)在减少数据需求和训练成本的同时，评估其在防止Jailbreaking攻击方面的模型安全性效果。引入Egida数据集，涵盖27个不同的安全主题和18种不同的攻击样式，结合合成和人工标签，以提升最新LLM的安全性。此外，还评估了它们在通用任务中性能下降的趋势以及过度拒绝倾向。遵循提议的方法，训练后的模型将攻击成功率降低10%-30%，使用少量训练样本（2,000个样本）和低计算成本（8B模型3美元，72B模型20美元）。安全性对齐的模型可以迁移到未见过的主题和攻击样式，最成功的攻击样式成功率达到约5%。模型尺寸和家族对安全性趋向的可塑性影响显著，强调了预训练选择的重要性。为验证研究发现，作者进行了一项独立的人类偏好一致性评估，并发布了相关的数据集Egida-HSafe。总体而言，本研究展示了使用DPO提升LLM安全性是负担得起且易于获取的，同时指出了其当前的局限性。所有数据集和模型均已发布以实现可再现性和进一步研究。', 'title_zh': '面向 Jailbreaking 的高效 LLM 安全加固方法'}
{'arxiv_id': 'arXiv:2502.13595', 'title': 'MMTEB: Massive Multilingual Text Embedding Benchmark', 'authors': 'Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemiński, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, Ömer Çağatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafał Poświata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Björn Plüster, Jan Philipp Harries, Loïc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek Šuppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Günther, Mengzhou Xia, Weijia Shi, Xing Han Lù, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, Niklas Muennighoff', 'link': 'https://arxiv.org/abs/2502.13595', 'abstract': 'Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.', 'abstract_zh': '大规模多语言文本嵌入基准（MMTEB）：一个多语言评价任务的大型、社区驱动扩展', 'title_zh': '大规模多语言文本嵌入基准（MMTEB）'}
{'arxiv_id': 'arXiv:2502.13576', 'title': 'Beyond One-Size-Fits-All: Tailored Benchmarks for Efficient Evaluation', 'authors': 'Peiwen Yuan, Yueqi Zhang, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li', 'link': 'https://arxiv.org/abs/2502.13576', 'abstract': "Evaluating models on large benchmarks is very resource-intensive, especially during the period of rapid model evolution. Existing efficient evaluation methods estimate the performance of target models by testing them only on a small and static coreset of the benchmark, which is derived from the publicly available evaluation results of source models. These methods rely on the assumption that target models have high prediction consistency with source models. However, we demonstrate that it doesn't generalize well in practice. To alleviate the inconsistency issue, we present TailoredBench, a method that conducts customized evaluation tailored to each target model. Specifically, a Global-coreset is first constructed as a probe to identify the most consistent source models for each target model with an adaptive source model selection strategy. Afterwards, a scalable K-Medoids clustering algorithm is proposed to extend the Global-coreset to a tailored Native-coreset for each target model. According to the predictions on Native-coresets, we obtain the performance of target models on the whole benchmark with a calibrated estimation strategy. Comprehensive experiments on 5 benchmarks across over 300 models demonstrate that compared to best performing baselines, TailoredBench achieves an average reduction of 31.4% in MAE of accuracy estimates under the same inference budgets, showcasing strong effectiveness and generalizability.", 'abstract_zh': '基于定制化评估的TailoredBench：在大基准上评估模型的方法', 'title_zh': '超越一刀切：针对高效的评价定制基准'}
{'arxiv_id': 'arXiv:2502.13562', 'title': 'Are Large Language Models In-Context Graph Learners?', 'authors': 'Jintang Li, Ruofan Wu, Yuchang Zhu, Huizhe Zhang, Liang Chen, Zibin Zheng', 'link': 'https://arxiv.org/abs/2502.13562', 'abstract': 'Large language models (LLMs) have demonstrated remarkable in-context reasoning capabilities across a wide range of tasks, particularly with unstructured inputs such as language or images. However, LLMs struggle to handle structured data, such as graphs, due to their lack of understanding of non-Euclidean structures. As a result, without additional fine-tuning, their performance significantly lags behind that of graph neural networks (GNNs) in graph learning tasks. In this paper, we show that learning on graph data can be conceptualized as a retrieval-augmented generation (RAG) process, where specific instances (e.g., nodes or edges) act as queries, and the graph itself serves as the retrieved context. Building on this insight, we propose a series of RAG frameworks to enhance the in-context learning capabilities of LLMs for graph learning tasks. Comprehensive evaluations demonstrate that our proposed RAG frameworks significantly improve LLM performance on graph-based tasks, particularly in scenarios where a pretrained LLM must be used without modification or accessed via an API.', 'abstract_zh': '大型语言模型（LLMs）在多种任务中展示了卓越的上下文推理能力，特别是在语言或图像等非结构化输入上。然而，LLMs在处理图等结构化数据时表现不佳，因为它们不理解非欧几里得结构。因此，在不需要额外微调的情况下，它们在图学习任务上的表现远逊于图神经网络（GNNs）。在本文中，我们展示了图数据上的学习可以被视为检索增强生成（RAG）过程，其中特定实例（如节点或边）充当查询，而图本身作为检索上下文。基于这一洞察，我们提出了一系列RAG框架，以增强LLMs在图学习任务中的上下文学习能力。全面的评估表明，我们提出的一系列RAG框架显著提高了LLMs在图基任务上的表现，特别是在需要使用未修改的预训练LLM或通过API访问的情况下。', 'title_zh': '大型语言模型是图学习者吗？'}
{'arxiv_id': 'arXiv:2502.13555', 'title': 'Democratizing Large Language Model-Based Graph Data Augmentation via Latent Knowledge Graphs', 'authors': 'Yushi Feng, Tsai Hor Chan, Guosheng Yin, Lequan Yu', 'link': 'https://arxiv.org/abs/2502.13555', 'abstract': 'Data augmentation is necessary for graph representation learning due to the scarcity and noise present in graph data. Most of the existing augmentation methods overlook the context information inherited from the dataset as they rely solely on the graph structure for augmentation. Despite the success of some large language model-based (LLM) graph learning methods, they are mostly white-box which require access to the weights or latent features from the open-access LLMs, making them difficult to be democratized for everyone as existing LLMs are mostly closed-source for commercial considerations. To overcome these limitations, we propose a black-box context-driven graph data augmentation approach, with the guidance of LLMs -- DemoGraph. Leveraging the text prompt as context-related information, we task the LLM with generating knowledge graphs (KGs), which allow us to capture the structural interactions from the text outputs. We then design a dynamic merging schema to stochastically integrate the LLM-generated KGs into the original graph during training. To control the sparsity of the augmented graph, we further devise a granularity-aware prompting strategy and an instruction fine-tuning module, which seamlessly generates text prompts according to different granularity levels of the dataset. Extensive experiments on various graph learning tasks validate the effectiveness of our method over existing graph data augmentation methods. Notably, our approach excels in scenarios involving electronic health records (EHRs), which validates its maximal utilization of contextual knowledge, leading to enhanced predictive performance and interpretability.', 'abstract_zh': '基于LLM的黑盒上下文驱动图数据增强方法：DemoGraph', 'title_zh': '基于潜在知识图谱的大规模语言模型驱动的图数据增强民主化'}
{'arxiv_id': 'arXiv:2502.13544', 'title': 'From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MARKERGEN', 'authors': 'Peiwen Yuan, Chuyi Tan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Boyuan Pan, Yao Hu, Kan Li', 'link': 'https://arxiv.org/abs/2502.13544', 'abstract': 'Despite the rapid progress of large language models (LLMs), their length-controllable text generation (LCTG) ability remains below expectations, posing a major limitation for practical applications. Existing methods mainly focus on end-to-end training to reinforce adherence to length constraints. However, the lack of decomposition and targeted enhancement of LCTG sub-abilities restricts further this http URL bridge this gap, we conduct a bottom-up decomposition of LCTG sub-abilities with human patterns as reference and perform a detailed error this http URL this basis, we propose MarkerGen, a simple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental deficiencies via external tool integration;(2) conducts explicit length modeling with dynamically inserted markers;(3) employs a three-stage generation scheme to better align length constraints while maintaining content this http URL experiments demonstrate that MarkerGen significantly improves LCTG across various settings, exhibiting outstanding effectiveness and generalizability.', 'abstract_zh': '尽管大规模语言模型（LLM）取得了 rapid progress，其可调控长度的文本生成（LCTG）能力仍然不尽如人意，成为实际应用中的重大限制。现有方法主要集中在端到端训练以强化对长度约束的遵守。然而，LCTG子能力的缺乏分解和目标化增强限制了这一进程。为填补这一空白，我们以人类模式为参考进行了自底向上的LCTG子能力分解，并进行详细的错误分析。基于此，我们提出了一个简单有效的插件式解决方案MarkerGen，该方法通过以下方式工作：（1）通过外部工具集成来缓解LLM的基本缺陷；（2）通过动态插入标记来进行显式长度建模；（3）采用三阶段生成方案以更好地满足长度约束同时保持内容质量。实验表明，MarkerGen在各种场景中显著改善了LCTG，展示了出色的有效性和泛化能力。', 'title_zh': '从亚能力诊断到与人类一致的生成：通过MARKERGEN缩小文本长度控制的差距'}
{'arxiv_id': 'arXiv:2502.13542', 'title': 'Activation-aware Probe-Query: Effective Key-Value Retrieval for Long-Context LLMs Inference', 'authors': 'Qingfa Xiao, Jiachuan Wang, Haoyang Li, Cheng Deng, Jiaqi Tang, Shuangyin Li, Yongqi Zhang, Jun Wang, Lei Chen', 'link': 'https://arxiv.org/abs/2502.13542', 'abstract': 'Recent advances in large language models (LLMs) have showcased exceptional performance in long-context tasks, while facing significant inference efficiency challenges with limited GPU memory. Existing solutions first proposed the sliding-window approach to accumulate a set of historical \\textbf{key-value} (KV) pairs for reuse, then further improvements selectively retain its subsets at each step. However, due to the sparse attention distribution across a long context, it is hard to identify and recall relevant KV pairs, as the attention is distracted by massive candidate pairs. Additionally, we found it promising to select representative tokens as probe-Query in each sliding window to effectively represent the entire context, which is an approach overlooked by existing methods. Thus, we propose \\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that dynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the relevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a token-level indicator, Activation Bias, within each context window, enabling the proper construction of probe-Query for retrieval at pre-filling stage. To accurately recall the relevant KV pairs and minimize the irrelevant ones, we design a dynamic KV cut-off mechanism guided by information density across layers at the decoding stage. Experiments on the Long-Bench and $\\infty$ Benchmarks demonstrate its state-of-the-art performance with competitive inference quality and resource efficiency.', 'abstract_zh': 'Recent Advances in Large Language Models: An Activation-Aware Approach for Efficient Inference in Long-Context Tasks', 'title_zh': '激活感知探针查询：有效的长上下文LLMs推理中的键值检索'}
{'arxiv_id': 'arXiv:2502.13534', 'title': 'Solving the Encoding Bottleneck: Of the HHL Algorithm, By the HHL Algorithm', 'authors': 'Guang Ping He', 'link': 'https://arxiv.org/abs/2502.13534', 'abstract': 'The Harrow-Hassidim-Lloyd (HHL) algorithm offers exponential speedup for solving the quantum linear-system problem. But some caveats for the speedup could be hard to met. One of the difficulties is the encoding bottleneck, i.e., the efficient preparation of the initial quantum state. To prepare an arbitrary $N$-dimensional state exactly, existing state-preparation approaches generally require a runtime of $O(N)$, which will ruin the speedup of the HHL algorithm. Here we show that the states can be prepared approximately with a runtime of $O(poly(\\log N))$ by employing a slightly modified version of the HHL algorithm itself. Thus, applying this approach to prepare the initial state of the original HHL algorithm can preserve the exponential speedup advantage. It can also serve as a standalone solution for other applications demanding rapid state preparation.', 'abstract_zh': 'Harrow-Hassidim-Lloyd (HHL) 算法提供了求解量子线性系统问题的指数级加速，但加速的效果可能难以实现。一个主要的难点是编码瓶颈，即高效制备初始量子态。为了精确制备一个 $N$ 维状态，现有方法通常需要 $O(N)$ 的运行时间，这将破坏 HHL 算法的加速优势。我们通过使用 HHL 算法的略微修改版本，可以实现以 $O(\\text{poly}(\\log N))$ 的运行时间近似制备所需状态，从而可以应用于原始 HHL 算法的初始态制备，以保持其指数级加速的优势。此外，该方法也可作为其他需要快速制备状态的应用的独立解决方案。', 'title_zh': '解决编码瓶颈：借助HHL算法'}
{'arxiv_id': 'arXiv:2502.13533', 'title': 'Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models', 'authors': 'Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Yang You, Guiming Xie, Xuejian Gong, Kunlong Zhou', 'link': 'https://arxiv.org/abs/2502.13533', 'abstract': 'Large Language Models (LLMs) have significantly advanced natural language processing with exceptional task generalization capabilities. Low-Rank Adaption (LoRA) offers a cost-effective fine-tuning solution, freezing the original model parameters and training only lightweight, low-rank adapter matrices. However, the memory footprint of LoRA is largely dominated by the original model parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA training scheme founded on the intuition that many neurons in over-parameterized LLMs have low training utility but are essential for inference. LoRAM presents a unique twist: it trains on a pruned (small) model to obtain pruned low-rank matrices, which are then recovered and utilized with the original (large) model for inference. Additionally, minimal-cost continual pre-training, performed by the model publishers in advance, aligns the knowledge discrepancy between pruned and original models. Our extensive experiments demonstrate the efficacy of LoRAM across various pruning strategies and downstream tasks. For a model with 70 billion parameters, LoRAM enables training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA training and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by structured pruning combined with 4-bit quantization, for LLaMA-3.1-70B (LLaMA-2-70B), reduces the parameter storage cost that dominates the memory usage in low-rank matrix training by 15.81$\\times$ (16.95$\\times$), while achieving dominant performance gains over both the original LLaMA-3.1-70B (LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).', 'abstract_zh': '大语言模型（LLMs）在自然语言处理任务泛化能力方面取得了显著进展。LoRA低秩适应（Low-Rank Adaption）提供了一种经济有效的微调方案，固定原始模型参数并仅训练轻量级低秩适配器矩阵。然而，LoRA的记忆占用主要被原始模型参数所支配。为解决这一问题，我们提出LoRAM，这是一种基于过度参数化LLMs中许多神经元在训练中的低效用但对推理至关重要这一直觉的记忆高效LoRA训练方案。LoRAM的独创之处在于，在精简（小型）模型上进行训练以获得精简的低秩矩阵，然后用原始（大型）模型恢复和利用这些矩阵进行推理。此外，由模型提供商预先进行的低成本持续预训练可使精简模型和原始模型之间知识差距趋于一致。我们的广泛实验表明，LoRAM在各种剪枝策略和下游任务上均表现出色。对于一个具有700亿参数的模型，LoRAM使得在仅20G HBM的GPU上进行训练成为可能，替代了用于LoRA训练的A100-80G GPU和用于全微调的15个GPU。具体来说，通过结构化剪枝结合4位量化实现的QLoRAM，使得LLaMA-3.1-70B（LLaMA-2-70B）的参数存储成本在低秩矩阵训练中的主导部分减少了15.81×（16.95×），同时在性能上显著优于原始的LLaMA-3.1-70B（LLaMA-2-70B）和LoRA训练的LLaMA-3.1-8B（LLaMA-2-13B）。', 'title_zh': '训练小型模型，推理大型模型：大规模语言模型的内存高效LoRA训练'}
{'arxiv_id': 'arXiv:2502.13527', 'title': 'Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking', 'authors': 'Yanzeng Li, Yunfan Xiong, Jialun Zhong, Jinchao Zhang, Jie Zhou, Lei Zou', 'link': 'https://arxiv.org/abs/2502.13527', 'abstract': "The rise of Large Language Models (LLMs) has led to significant applications but also introduced serious security threats, particularly from jailbreak attacks that manipulate output generation. These attacks utilize prompt engineering and logit manipulation to steer models toward harmful content, prompting LLM providers to implement filtering and safety alignment strategies. We investigate LLMs' safety mechanisms and their recent applications, revealing a new threat model targeting structured output interfaces, which enable attackers to manipulate the inner logit during LLM generation, requiring only API access permissions. To demonstrate this threat model, we introduce a black-box attack framework called AttackPrefixTree (APT). APT exploits structured output interfaces to dynamically construct attack patterns. By leveraging prefixes of models' safety refusal response and latent harmful outputs, APT effectively bypasses safety measures. Experiments on benchmark datasets indicate that this approach achieves higher attack success rate than existing methods. This work highlights the urgent need for LLM providers to enhance security protocols to address vulnerabilities arising from the interaction between safety patterns and structured outputs.", 'abstract_zh': '大型语言模型（LLMs）的兴起带来了显著的应用，同时也引入了严重的安全威胁，特别是来自越狱攻击的威胁，这些攻击通过操控输出生成来操纵模型。这些攻击利用提示工程和logit操纵来引导模型产生有害内容，促使LLM提供商实施过滤和安全对齐策略。我们调查了LLMs的安全机制及其最近的应用，揭示了一种新的威胁模型，针对结构化输出接口，这种接口允许攻击者在LLM生成过程中操纵内部logit，仅需API访问权限。为了展示这一威胁模型，我们引入了一个名为AttackPrefixTree（APT）的黑盒攻击框架，APT利用结构化输出接口动态构建攻击模式。通过利用模型安全拒绝响应的前缀和潜在有害输出，APT有效地绕过了安全措施。基准数据集上的实验表明，这种方法在攻击成功率方面高于现有方法。这项工作强调了LLM提供商亟需增强安全协议，以应对安全模式和结构化输出交互引发的漏洞。', 'title_zh': '利用前缀树在结构化输出接口中增强Jailbreak攻击'}
{'arxiv_id': 'arXiv:2502.13524', 'title': 'MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis', 'authors': 'Wei Dai, Steven Wang, Jun Liu', 'link': 'https://arxiv.org/abs/2502.13524', 'abstract': "Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the ``Mamba'' model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mamba's potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models.", 'abstract_zh': '高效的三维医学图像评价对于医疗诊断和治疗至关重要。近年来，深度学习和计算机视觉在医学图像分析和解释中的应用取得了显著增长。传统的卷积神经网络(CNNs)和视觉变换器(ViTs)面临着显著的计算挑战，促使架构上的创新。最近的研究引入了新型架构“Mamba”模型作为传统CNNs或ViTs的替代方案。Mamba模型在处理一维数据方面表现出卓越的能力，并且具有较低的计算需求。然而，Mamba在三维医学图像分析中的潜力尚未被充分探索，随着维度的增加，可能会面临显著的计算挑战。本文提出了MobileViM，这是一种针对三维医学图像高效分割的精简架构。在MobileViM网络中，我们提出了一种新的维数独立机制和双向遍历方法，并结合了基于视觉-Mamba的框架。MobileViM还包含了一种跨尺度桥梁技术，以提高不同医学成像模态下的效率和准确性。通过这些增强，MobileViM在单个图形处理单元(NVIDIA RTX 4090)上实现了超过90帧每秒(FPS)的分割速度，比相同的计算资源下最先进的深度学习模型快24 FPS以上。此外，实验评估表明，MobileViM在PENGWIN、BraTS2024、ATLAS和Toothfairy2数据集中分别达到了92.72%、86.69%、80.46%和77.43%的Dice相似度分数，显著优于现有模型。', 'title_zh': 'MobileViM: 一种轻量级且维度无关的医学图像分析视觉章鱼'}
{'arxiv_id': 'arXiv:2502.13519', 'title': 'MILE: Model-based Intervention Learning', 'authors': 'Yigit Korkmaz, Erdem Bıyık', 'link': 'https://arxiv.org/abs/2502.13519', 'abstract': 'Imitation learning techniques have been shown to be highly effective in real-world control scenarios, such as robotics. However, these approaches not only suffer from compounding error issues but also require human experts to provide complete trajectories. Although there exist interactive methods where an expert oversees the robot and intervenes if needed, these extensions usually only utilize the data collected during intervention periods and ignore the feedback signal hidden in non-intervention timesteps. In this work, we create a model to formulate how the interventions occur in such cases, and show that it is possible to learn a policy with just a handful of expert interventions. Our key insight is that it is possible to get crucial information about the quality of the current state and the optimality of the chosen action from expert feedback, regardless of the presence or the absence of intervention. We evaluate our method on various discrete and continuous simulation environments, a real-world robotic manipulation task, as well as a human subject study. Videos and the code can be found at this https URL .', 'abstract_zh': '模仿学习技术在实际控制场景中，如机器人领域，已被证明非常有效。然而，这些方法不仅会遇到累积误差问题，还需要人类专家提供完整的轨迹。尽管存在一种交互方法，允许专家监督机器人并在必要时介入，但这些扩展通常仅利用干预期间收集的数据，并忽略了非干预时间段中隐含的反馈信号。本文中，我们创建了一个模型来描述在这种情况下干预的发生方式，并展示了只需少量专家干预就有可能学习到策略。我们的关键见解是，无论是否存在干预，都可以从专家反馈中获取有关当前状态质量和所选动作优化性的关键信息。我们先后在各种离散和连续模拟环境、一个真实的机器人操作任务以及一项人类受控试验中评估了我们的方法。相关视频和代码可在以下链接找到：this https URL。', 'title_zh': '基于模型的干预学习'}
{'arxiv_id': 'arXiv:2502.13509', 'title': 'Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion', 'authors': 'Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Wei Bi, Yida Xu, Guo Li, Xian Yang', 'link': 'https://arxiv.org/abs/2502.13509', 'abstract': 'Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data such as lab test results capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative embeddings. These embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.', 'abstract_zh': '大型语言模型（LLMs）在视觉语言任务中展现了卓越的表现，但在医疗领域的应用仍相对未被充分探索，特别是在将结构化时间序列数据与非结构化临床笔记整合方面。在临床实践中，动态时间序列数据，如实验室检测结果，捕捉关键的时间模式，而临床笔记则提供丰富的语义背景。由于连续信号与离散文本之间的固有差异，将这些模态数据融合极具挑战性。为填补这一空白，我们提出了ProMedTS，这是一种新颖的自监督多模态框架，利用提示引导学习来统一这些异构数据类型。我们的方法利用轻量级的异常检测生成异常描述作为提示，引导原始时间序列数据的编码为信息丰富的嵌入。这些嵌入在共享的潜在空间中与文本表示对齐，从而保留细微的时间细节与语义洞察。此外，我们的框架还整合了定制的自监督目标，以增强跨模态和模态内部的对齐。我们在实际数据集上评估了ProMedTS在疾病诊断任务中的表现，结果显示我们的方法在所有测试中均优于现有最先进的方法。', 'title_zh': '解锁多模态集成在EHR中的潜力：一种语言与时间序列融合的提示学习框架'}
{'arxiv_id': 'arXiv:2502.13502', 'title': 'PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own Deep Neural Net At Inference', 'authors': 'Burc Gokden', 'link': 'https://arxiv.org/abs/2502.13502', 'abstract': 'We show that Large Language Model from Power Law Decoder Representations (PLDR-LLM) is a foundational model whose deductive outputs are invariant tensors up to a small perturbation. PLDR-LLM learns a singularity condition for the deductive outputs that enable the once-inferred energy-curvature tensor $\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph attention (PLGA) generating the deductive outputs at inference. We demonstrate that a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in a straightforward manner to improve the inference time. The invariance and generalizable nature of deductive outputs is at a very high fidelity where deductive outputs have same RMSE and determinant values up to 15 decimal places after caching, and zero-shot benchmark scores remain unchanged. Ablation studies show that learned deductive outputs have distinct loss and accuracy characteristics from models pretrained with transferred, randomly initialized or identity tensors as a constant tensor operator and an LLM with scaled-dot product attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$ is predefined as identity. The observed invariance characteristic introduces a novel asymmetry between training and inference phases with caching. We outline observed common characteristics of the deductive outputs for the learned singularity condition. We provide an implementation of a training and inference framework for PLDR-LLM with KV-cache and G-cache.', 'abstract_zh': '我们展示了Power律解码表示（PLDR）大型语言模型（LLM）是一个基础模型，其演绎输出在轻微扰动下是不变张量。PLDR-LLM 学习演绎输出的奇异性条件，使得一次推断生成的能曲率张量 $\\mathbf{G}_{LM}$ 能够替代基于PLGA的深度神经网络。我们证明了 $\\mathbf{G}_{LM}$ 缓存（G-cache）和KV-cache 可以方便地实现以提高推理时间。经缓存后的演绎输出具有极高的保真度，其RMSE和行列式值在小数点后15位相同，零样本基准得分保持不变。消融研究表明，学习到的演绎输出与预制转移、随机初始化或恒等张量的模型相比具有不同的损失和准确率特性。当 $\\mathbf{G}_{LM}$ 预定义为恒等时，使用缩放点积注意机制（SDPA）的LLM是PLDR-LLM的特例。观察到的不变特性在训练和推理阶段引入了一种新颖的不对称性。我们列出了学习到的奇异性条件下演绎输出的常见特征，并提供了一个包含KV-cache和G-cache的PLDR-LLM训练和推理框架的实现。', 'title_zh': 'PLDR-LLMs 学习一个可泛化的张量运算符，该运算符可在推理时替换其自身的深度神经网络。'}
{'arxiv_id': 'arXiv:2502.13499', 'title': 'Hidden Darkness in LLM-Generated Designs: Exploring Dark Patterns in Ecommerce Web Components Generated by LLMs', 'authors': 'Ziwei Chen, Jiawen Shen, Luna, Kristen Vaccaro', 'link': 'https://arxiv.org/abs/2502.13499', 'abstract': "Recent work has highlighted the risks of LLM-generated content for a wide range of harmful behaviors, including incorrect and harmful code. In this work, we extend this by studying whether LLM-generated web design contains dark patterns. This work evaluated designs of ecommerce web components generated by four popular LLMs: Claude, GPT, Gemini, and Llama. We tested 13 commonly used ecommerce components (e.g., search, product reviews) and used them as prompts to generate a total of 312 components across all models. Over one-third of generated components contain at least one dark pattern. The majority of dark pattern strategies involve hiding crucial information, limiting users' actions, and manipulating them into making decisions through a sense of urgency. Dark patterns are also more frequently produced in components that are related to company interests. These findings highlight the need for interventions to prevent dark patterns during front-end code generation with LLMs and emphasize the importance of expanding ethical design education to a broader audience.", 'abstract_zh': '近期的研究强调了由大规模语言模型生成的内容在一系列有害行为中的风险，包括有害的代码。本研究进一步探讨了由大规模语言模型生成的网页设计是否包含暗模式。本研究评估了由四款流行的大型语言模型（Claude、GPT、Gemini和Llama）生成的电商网页组件的设计，共测试了13种常用的电商组件（例如搜索、产品评价），并使用这些组件作为提示生成了共计312个组件。超过三分之一的生成组件包含至少一个暗模式。大多数暗模式策略涉及隐藏关键信息、限制用户行动，并通过紧迫感促使用户做出决策。与公司利益相关的组件中，暗模式的产生更为频繁。这些发现突显了在使用大规模语言模型进行前端代码生成时需要采取干预措施以防止暗模式的必要性，并强调了扩大伦理设计教育的重要性。', 'title_zh': 'LLM生成设计中的隐性黑暗模式：探索由LLM生成的电商网页组件中的暗模式'}
{'arxiv_id': 'arXiv:2502.13497', 'title': 'Towards Geo-Culturally Grounded LLM Generations', 'authors': 'Piyawat Lertvittayakumjorn, David Kinney, Vinodkumar Prabhakaran, Donald Martin, Sunipa Dev', 'link': 'https://arxiv.org/abs/2502.13497', 'abstract': "Generative large language models (LLMs) have been demonstrated to have gaps in diverse, cultural knowledge across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on the ability of LLMs to display familiarity with a diverse range of national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on a series of cultural familiarity benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., the norms, artifacts, and institutions of national cultures), while KB grounding's effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models, while failing to improve evaluators' judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional knowledge about a culture and open-ended cultural fluency when it comes to evaluating the cultural familiarity of generative LLMs.", 'abstract_zh': '生成型大型语言模型在全球多元文化知识方面存在差距。我们调查了检索增强生成和搜索 grounding 技术对大型语言模型展示对多种国家文化熟悉程度能力的影响。具体而言，我们比较了标准大型语言模型、从定制知识库中检索信息增强的大型语言模型（即知识库 grounding）以及从网络搜索中检索信息增强的大型语言模型（即搜索 grounding）在一系列文化熟悉度基准测试中的性能。我们发现，搜索 grounding 显著提高了大型语言模型在测试命题知识（如国家文化的规范、器物和制度）的多项选择基准测试中的性能，而知识库 grounding 的有效性受到知识库覆盖不全和检索器次优的限制。然而，搜索 grounding 也会增加语言模型产生刻板印象判断的风险，而未能在具有足够统计效能的人类评估中改善评估者对文化熟悉度的判断。这些结果突出了在评估生成型大型语言模型的文化熟悉度时命题文化知识与开放式文化流利度之间的区别。', 'title_zh': '面向地理文化根基的语言模型生成'}
{'arxiv_id': 'arXiv:2502.13490', 'title': 'What are Models Thinking about? Understanding Large Language Model Hallucinations "Psychology" through Model Inner State Analysis', 'authors': 'Peiran Wang, Yang Liu, Yunfei Lu, Jue Hong, Ye Wu', 'link': 'https://arxiv.org/abs/2502.13490', 'abstract': "Large language model (LLM) systems suffer from the models' unstable ability to generate valid and factual content, resulting in hallucination generation. Current hallucination detection methods heavily rely on out-of-model information sources, such as RAG to assist the detection, thus bringing heavy additional latency. Recently, internal states of LLMs' inference have been widely used in numerous research works, such as prompt injection detection, etc. Considering the interpretability of LLM internal states and the fact that they do not require external information sources, we introduce such states into LLM hallucination detection. In this paper, we systematically analyze different internal states' revealing features during inference forward and comprehensively evaluate their ability in hallucination detection. Specifically, we cut the forward process of a large language model into three stages: understanding, query, generation, and extracting the internal state from these stages. By analyzing these states, we provide a deep understanding of why the hallucinated content is generated and what happened in the internal state of the models. Then, we introduce these internal states into hallucination detection and conduct comprehensive experiments to discuss the advantages and limitations.", 'abstract_zh': '大型语言模型（LLM）系统在生成有效和事实性内容方面表现出不稳定的模型能力，导致产生幻觉。当前的幻觉检测方法 heavily 依赖模型外部的信息源，如RAG来辅助检测，从而带来了额外的延迟。最近，大型语言模型推理过程中的内部状态已在众多研究工作中广泛使用，如提示注入检测等。鉴于大型语言模型内部状态的可解释性及其不需要外部信息源的特点，我们将这些状态引入到幻觉检测中。在本文中，我们系统地分析了推理过程中不同内部状态的揭示特征，并全面评估了它们在幻觉检测中的能力。具体地，我们将大型语言模型的前向过程划分为理解、查询、生成三个阶段，并从这些阶段中提取内部状态。通过对这些状态的分析，我们提供了对幻觉内容生成原因及其模型内部状态变化的深入理解。随后，我们将这些内部状态引入幻觉检测，并进行综合实验以讨论其优势和局限性。', 'title_zh': 'Large语言模型幻觉“心理学”——通过模型内部状态分析理解模型思维'}
{'arxiv_id': 'arXiv:2502.13487', 'title': 'Transferring Textual Preferences to Vision-Language Understanding through Model Merging', 'authors': 'Chen-An Li, Tzu-Han Lin, Yun-Nung Chen, Hung-yi Lee', 'link': 'https://arxiv.org/abs/2502.13487', 'abstract': "Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is computationally expensive. This paper explores a training-free alternative by merging text-based reward models (RMs) with LVLMs to create VLRMs. Our approach shows that integrating these models leads to improved performance over LVLMs' scoring and text-based RMs, offering an efficient method for incorporating textual preferences into LVLMs.", 'abstract_zh': '大型多模态语言模型通过融合基于文本的奖励模型和LVLMs来构建VLRMs：无需训练的替代方案', 'title_zh': '通过模型合并将文本偏好转移到视觉-语言理解'}
{'arxiv_id': 'arXiv:2502.13480', 'title': 'Astra: Efficient and Money-saving Automatic Parallel Strategies Search on Heterogeneous GPUs', 'authors': 'Peiran Wang, Haibing Li, Fu Haohan, Shiyong Li, Yanpeng Wang, Dou Shen', 'link': 'https://arxiv.org/abs/2502.13480', 'abstract': 'In this paper, we introduce an efficient and money-saving automatic parallel strategies search framework on heterogeneous GPUs: Astra. First, Astra searches for the efficiency-optimal parallel strategy in both GPU configurations search space (GPU types and GPU numbers) and parallel parameters search space. Then, Astra also provides the solution on heterogeneous GPUs by mathematically modeling the time consumption of heterogeneous training. At last, Astra is the first to propose the automatic parallel strategy search on money-saving. The experiment results demonstrate that Astra can achieve better throughput than expert-designed strategies. The search time cost for Astra can also be limited to 1.27 seconds in a single-GPU setting and less than 1.35 minutes in a heterogeneous-GPU setting on average with an accuracy of over 95%.', 'abstract_zh': '基于异构GPU的高效经济型自动并行策略搜索框架Astra', 'title_zh': 'Astra: 在异构GPU上高效且节省成本的自动并行策略搜索方法'}
{'arxiv_id': 'arXiv:2502.13475', 'title': 'LLM should think and action as a human', 'authors': 'Haun Leung, ZiNan Wang', 'link': 'https://arxiv.org/abs/2502.13475', 'abstract': 'It is popular lately to train large language models to be used as chat assistants, but in the conversation between the user and the chat assistant, there are prompts, require multi-turns between the chat assistant and the user. However, there are a number of issues with the multi-turns conversation: The response of the chat assistant is prone to errors and cannot help users achieve their goals; It is difficult for chat assistant to generate responses with different processes based on actual needs for the same command or request; Chat assistant require the use of tools, but the current approach is not elegant and efficient, and the number of tool calls that can be supported is limited. The main reason for these issues is that large language models do not have the thinking ability as a human, lack the reasoning ability and planning ability, and lack the ability to execute plans. To solve these issues, we propose a thinking method based on a built-in chain of thought: In the multi-turns conversation, for each user prompt, the large language model thinks based on elements such as chat history, thinking context, action calls, memory and knowledge, makes detailed reasoning and planning, and actions according to the plan. We also explored how the large language model enhances thinking ability through this thinking method: Collect training datasets according to the thinking method and fine tune the large language model through supervised learning; Train a consistency reward model and use it as a reward function to fine tune the large language model using reinforcement learning, and the reinforced large language model outputs according to this way of thinking. Our experimental results show that the reasoning ability and planning ability of the large language model are enhanced, and the issues in the multi-turns conversation are solved.', 'abstract_zh': '近年来，训练大型语言模型作为聊天助手变得流行，但在用户与聊天助手的对话中，聊天助手需要进行多轮交互。然而，多轮对话存在许多问题：聊天助手的回答容易出错，无法帮助用户达成目标；对于相同的命令或请求，聊天助手难以生成基于实际需求的不同步骤的响应；聊天助手需要使用工具，但当前的方法不够优雅高效，支持的工具调用数量有限。这些问题的主要原因是大型语言模型缺乏人类的思考能力，缺乏推理和规划能力，也缺乏执行计划的能力。为了解决这些问题，我们提出了一种基于内置推理链的方法：在多轮对话中，对于每个用户提示，大型语言模型基于聊天历史、推理上下文、操作调用、记忆和知识进行思考，进行详细的推理和规划，并根据计划执行动作。我们还探讨了这种思考方法如何增强大型语言模型的思考能力：根据这种思考方法收集训练数据集，并通过监督学习微调大型语言模型；训练一致性奖励模型，并使用它作为奖励函数，通过强化学习微调大型语言模型，增强后的大型语言模型根据这种方式输出。实验结果表明，大型语言模型的推理能力和规划能力得到了增强，多轮对话中的问题得到了解决。', 'title_zh': 'LLM 应当思考和行动如人类一般。'}
{'arxiv_id': 'arXiv:2502.13471', 'title': 'Some Insights of Construction of Feature Graph to Learn Pairwise Feature Interactions with Graph Neural Networks', 'authors': 'Phaphontee Yamchote, Saw Nay Htet Win, Chainarong Amornbunchornvej, Thanapon Noraset', 'link': 'https://arxiv.org/abs/2502.13471', 'abstract': "Feature interaction is crucial in predictive machine learning models, as it captures the relationships between features that influence model performance. In this work, we focus on pairwise interactions and investigate their importance in constructing feature graphs for Graph Neural Networks (GNNs). Rather than proposing new methods, we leverage existing GNN models and tools to explore the relationship between feature graph structures and their effectiveness in modeling interactions. Through experiments on synthesized datasets, we uncover that edges between interacting features are important for enabling GNNs to model feature interactions effectively. We also observe that including non-interaction edges can act as noise, degrading model performance. Furthermore, we provide theoretical support for sparse feature graph selection using the Minimum Description Length (MDL) principle. We prove that feature graphs retaining only necessary interaction edges yield a more efficient and interpretable representation than complete graphs, aligning with Occam's Razor.\nOur findings offer both theoretical insights and practical guidelines for designing feature graphs that improve the performance and interpretability of GNN models.", 'abstract_zh': '特征相互作用对于预测机器学习模型至关重要，因为它捕获了影响模型性能的特征之间的关系。在本工作中，我们关注于成对的相互作用，并探究它们在构建图神经网络（GNN）特征图结构中的重要性。我们没有提出新的方法，而是利用现有的GNN模型和工具来探索特征图结构与其在建模相互作用方面的有效性之间的关系。通过对合成数据集的实验，我们发现，相互作用特征之间的边对于使GNN能够有效地建模特征相互作用至关重要。我们还观察到包含非相互作用边可能会增加噪声，从而降低模型性能。此外，我们利用最小描述长度（MDL）原则为稀疏特征图选择提供理论支持。我们证明，保留仅必要相互作用边的特征图比完整图提供了更高效和可解释的表示，这与奥卡姆剃刀原则一致。我们的发现不仅提供了设计改进GNN模型性能和可解释性的特征图的理论洞察和实践指南。', 'title_zh': '基于图神经网络学习成对特征交互的特征图构建一些见解'}
{'arxiv_id': 'arXiv:2502.13465', 'title': 'HawkBench: Investigating Resilience of RAG Methods on Stratified Information-Seeking Tasks', 'authors': 'Hongjin Qian, Zheng Liu, Chao Gao, Yankai Wang, Defu Lian, Zhicheng Dou', 'link': 'https://arxiv.org/abs/2502.13465', 'abstract': 'In real-world information-seeking scenarios, users have dynamic and diverse needs, requiring RAG systems to demonstrate adaptable resilience. To comprehensively evaluate the resilience of current RAG methods, we introduce HawkBench, a human-labeled, multi-domain benchmark designed to rigorously assess RAG performance across categorized task types. By stratifying tasks based on information-seeking behaviors, HawkBench provides a systematic evaluation of how well RAG systems adapt to diverse user needs.\nUnlike existing benchmarks, which focus primarily on specific task types (mostly factoid queries) and rely on varying knowledge bases, HawkBench offers: (1) systematic task stratification to cover a broad range of query types, including both factoid and rationale queries, (2) integration of multi-domain corpora across all task types to mitigate corpus bias, and (3) rigorous annotation for high-quality evaluation.\nHawkBench includes 1,600 high-quality test samples, evenly distributed across domains and task types. Using this benchmark, we evaluate representative RAG methods, analyzing their performance in terms of answer quality and response latency. Our findings highlight the need for dynamic task strategies that integrate decision-making, query interpretation, and global knowledge understanding to improve RAG generalizability. We believe HawkBench serves as a pivotal benchmark for advancing the resilience of RAG methods and their ability to achieve general-purpose information seeking.', 'abstract_zh': 'HawkBench：一个多领域的人工标注基准，用于全面评估RAG系统的适应性韧性', 'title_zh': 'HawkBench: 探究RAG方法在分层信息检索任务中的鲁棒性'}
{'arxiv_id': 'arXiv:2502.13464', 'title': 'Estimating Commonsense Plausibility through Semantic Shifts', 'authors': 'Wanqing Cui, Keping Bi, Jiafeng Guo, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.13464', 'abstract': "Commonsense plausibility estimation is critical for evaluating language models (LMs), yet existing generative approaches--reliant on likelihoods or verbalized judgments--struggle with fine-grained discrimination. In this paper, we propose ComPaSS, a novel discriminative framework that quantifies commonsense plausibility by measuring semantic shifts when augmenting sentences with commonsense-related information. Plausible augmentations induce minimal shifts in semantics, while implausible ones result in substantial deviations. Evaluations on two types of fine-grained commonsense plausibility estimation tasks across different backbones, including LLMs and vision-language models (VLMs), show that ComPaSS consistently outperforms baselines. It demonstrates the advantage of discriminative approaches over generative methods in fine-grained commonsense plausibility evaluation. Experiments also show that (1) VLMs yield superior performance to LMs, when integrated with ComPaSS, on vision-grounded commonsense tasks. (2) contrastive pre-training sharpens backbone models' ability to capture semantic nuances, thereby further enhancing ComPaSS.", 'abstract_zh': '常识合理性估计对于评估语言模型至关重要，而现有的生成方法依赖于似然性或口头判断，在细微差别区分方面表现出困难。在这篇论文中，我们提出了ComPaSS，一种新颖的辨别框架，通过衡量添加与常识相关的信息时语义的变化来定量衡量常识合理性。合理的增强只会引起最小的语义变化，而不合理的增强会导致显著的偏差。对包括大型语言模型（LLMs）和视觉语言模型（VLMs）在内的不同架构的两种类型的细微常识合理性的评估任务进行了评估，结果显示ComPaSS始终优于基线方法。它展示了辨别方法在细微常识合理性评估中的优势，超越了生成方法。实验还表明，（1）当与ComPaSS结合使用时，VLMs在视觉 grounding 的常识任务中比LMs表现出更优的性能。（2）对比预训练进一步增强了基础模型捕捉语义细微差别的能力，从而进一步增强了ComPaSS。', 'title_zh': '通过语义转变估计常识合理性'}
{'arxiv_id': 'arXiv:2502.13458', 'title': 'ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails', 'authors': 'Xiaofei Wen, Wenxuan Zhou, Wenjie Jacky Mo, Muhao Chen', 'link': 'https://arxiv.org/abs/2502.13458', 'abstract': "Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose ThinkGuard, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail's cautiousness and interpretability. Evaluated on multiple safety benchmarks, ThinkGuard achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency.", 'abstract_zh': '确保大型语言模型的安全性是关键，因为它们已应用于实际应用场景。现有的防护措施依赖于基于规则的过滤或单次分类，这限制了它们处理复杂安全违规的能力。为了解决这一问题，我们提出了一种名为ThinkGuard的批判增强型防护模型，该模型通过生成结构化批判性反馈并结合安全标签来提炼高容量语言模型的知识。通过对增强批判性反馈的数据进行微调，捕获的审慎思考能力极大地提升了防护措施的谨慎性和可解释性。在多个安全性基准测试中，ThinkGuard达到了最高的平均F1和AUPRC，超越了所有基线模型。与LLaMA Guard 3相比，ThinkGuard的准确性提高了16.1%，宏观F1提高了27.0%。此外，ThinkGuard超越了仅依赖标签微调的模型，确认结构化批判性反馈能够同时提升分类精度和复杂安全性推理能力，同时保持计算效率。', 'title_zh': '思辨护卫: �审慎的慢思考建立谨慎的 guardrails'}
{'arxiv_id': 'arXiv:2502.13450', 'title': 'Interleaved Gibbs Diffusion for Constrained Generation', 'authors': 'Gautham Govind Anil, Sachin Yadav, Dheeraj Nagaraj, Karthikeyan Shanmugam, Prateek Jain', 'link': 'https://arxiv.org/abs/2502.13450', 'abstract': 'We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling framework for mixed continuous-discrete data, focusing on constrained generation problems. Prior works on discrete and continuous-discrete diffusion models assume factorized denoising distribution for fast generation, which can hinder the modeling of strong dependencies between random variables encountered in constrained generation. IGD moves beyond this by interleaving continuous and discrete denoising algorithms via a discrete time Gibbs sampling type Markov chain. IGD provides flexibility in the choice of denoisers, allows conditional generation via state-space doubling and inference time scaling via the ReDeNoise method. Empirical evaluations on three challenging tasks-solving 3-SAT, generating molecule structures, and generating layouts-demonstrate state-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT out of the box and achieves state-of-the-art results in molecule generation without relying on equivariant diffusion or domain-specific architectures. We explore a wide range of modeling, and interleaving strategies along with hyperparameters in each of these problems.', 'abstract_zh': '交错吉布斯扩散（IGD）：一种针对混合连续-离散数据的生成 modeling 框架及其应用', 'title_zh': '交替吉布斯扩散约束生成'}
{'arxiv_id': 'arXiv:2502.13442', 'title': 'TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation', 'authors': 'Jialin Ouyang', 'link': 'https://arxiv.org/abs/2502.13442', 'abstract': 'Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed. A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions. Experiments show TreeCut effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 61% and 42% in their respective worst-case scenarios. Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems.', 'abstract_zh': '大型语言模型（LLMs）现在在标准数学文字问题基准测试（如GSM8K）上实现了接近人类的性能，但其真正的推理能力仍存在争议。一个主要问题是，模型经常对无法回答的问题生成自信但缺乏依据的答案。我们引入了TreeCut，这是一个合成数据集，通过将每个问题表示为一棵树并移除选定的必要条件，系统地生成无限个无法回答的数学文字问题及其可回答的对应问题。实验显示，在最坏情况下，TreeCut有效诱导了GPT-4o和o3-mini等大型语言模型生成幻觉，比例分别为61%和42%。进一步分析表明，更深或更复杂的树、复合项目名称以及路径中间移除必要条件均增加幻觉的可能性，突显了LLMs在识别无法回答的数学问题方面持续面临的挑战。', 'title_zh': 'TreeCut: 一个合成的无法回答的数学文字题数据集，用于评估LLM的幻觉能力'}
{'arxiv_id': 'arXiv:2502.13441', 'title': 'The Self-Improvement Paradox: Can Language Models Bootstrap Reasoning Capabilities without External Scaffolding?', 'authors': 'Yutao Sun, Mingshuai Chen, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Jianwei Yin', 'link': 'https://arxiv.org/abs/2502.13441', 'abstract': 'Self-improving large language models (LLMs) -- i.e., to improve the performance of an LLM by fine-tuning it with synthetic data generated by itself -- is a promising way to advance the capabilities of LLMs while avoiding extensive supervision. Existing approaches to self-improvement often rely on external supervision signals in the form of seed data and/or assistance from third-party models. This paper presents Crescent -- a simple yet effective framework for generating high-quality synthetic question-answer data in a fully autonomous manner. Crescent first elicits the LLM to generate raw questions via a bait prompt, then diversifies these questions leveraging a rejection sampling-based self-deduplication, and finally feeds the questions to the LLM and collects the corresponding answers by means of majority voting. We show that Crescent sheds light on the potential of true self-improvement with zero external supervision signals for math reasoning; in particular, Crescent-generated question-answer pairs suffice to (i) improve the reasoning capabilities of an LLM while preserving its general performance (especially in the 0-shot setting); and (ii) distil LLM knowledge to weaker models more effectively than existing methods based on seed-dataset augmentation.', 'abstract_zh': '自我提升的大语言模型（LLMs）——即通过自身生成的合成数据对LLM进行微调以提高其性能——是一种避免大量监督而促进LLM能力发展的有前途的方法。这种自我提升的方法通常依赖外部监督信号，如种子数据和/或第三方模型的帮助。本文提出Crescent——一种简单而有效的全自主生成高质量合成问答数据的框架。Crescent首先通过诱饵提示促使LLM生成原始问题，然后利用基于拒绝采样的自我去重方法增加问题多样性，最后通过多数投票将问题输入LLM并收集相应的答案。我们展示，Crescent揭示了在零外部监督信号下进行真正自我提升的潜力，特别是在数学推理领域；特别是，Crescent生成的问答对足以（i）在保持LLM整体性能的情况下提高其推理能力（尤其是在零样本设置中）；（ii）更有效地将LLM知识传授给较弱的模型，优于基于种子数据集增强的方法。', 'title_zh': '自我改进悖论：语言模型能否在无需外部支撑的情况下自动生成推理能力？'}
{'arxiv_id': 'arXiv:2502.13440', 'title': 'Semi-supervised classification of bird vocalizations', 'authors': 'Simen Hexeberg, Mandar Chitre, Matthias Hoffmann-Kuhnt, Bing Wen Low', 'link': 'https://arxiv.org/abs/2502.13440', 'abstract': 'Changes in bird populations can indicate broader changes in ecosystems, making birds one of the most important animal groups to monitor. Combining machine learning and passive acoustics enables continuous monitoring over extended periods without direct human involvement. However, most existing techniques require extensive expert-labeled datasets for training and cannot easily detect time-overlapping calls in busy soundscapes. We propose a semi-supervised acoustic bird detector designed to allow both the detection of time-overlapping calls (when separated in frequency) and the use of few labeled training samples. The classifier is trained and evaluated on a combination of community-recorded open-source data and long-duration soundscape recordings from Singapore. It achieves a mean F0.5 score of 0.701 across 315 classes from 110 bird species on a hold-out test set, with an average of 11 labeled training samples per class. It outperforms the state-of-the-art BirdNET classifier on a test set of 103 bird species despite significantly fewer labeled training samples. The detector is further tested on 144 microphone-hours of continuous soundscape data. The rich soundscape in Singapore makes suppression of false positives a challenge on raw, continuous data streams. Nevertheless, we demonstrate that achieving high precision in such environments with minimal labeled training data is possible.', 'abstract_zh': '鸟类种群变化可以指示生态系统更为广泛的改变，使鸟类成为最重要的监测动物群之一。结合机器学习和被动声学能够在长时间内实现无需直接人类干预的连续监测。然而，大多数现有技术需要大量的专家标注数据集进行训练，并且难以在繁忙的声音景观中检测到时间重叠的声音。我们提出了一种半监督声学鸟类检测器，能够检测在频率上分离的时间重叠叫声，并且仅使用少量标注训练样本。分类器在组合了社区录制的开源数据和新加坡长达数小时的声音景观录音数据上进行训练和评估。该检测器在排除外部测试集的315个类别中（来自110种鸟类）实现了0.701的平均F0.5分数，平均每类有11个标注训练样本。尽管标注训练样本显著较少，但在103种鸟类的测试集上仍优于最先进的BirdNET分类器。该检测器进一步在连续声音景观数据上测试了144小时的麦克风录音。新加坡丰富的声音景观使得在原始连续数据流中抑制假阳性成为一个挑战。尽管如此，我们证明在这些环境中，使用最少的标注训练数据实现高精度是可能的。', 'title_zh': '半监督鸟类鸣声分类'}
{'arxiv_id': 'arXiv:2502.13428', 'title': 'MCTS-KBQA: Monte Carlo Tree Search for Knowledge Base Question Answering', 'authors': 'Guanming Xiong, Haochen Li, Wen Zhao', 'link': 'https://arxiv.org/abs/2502.13428', 'abstract': "This study explores how to enhance the reasoning capabilities of large language models (LLMs) in knowledge base question answering (KBQA) by leveraging Monte Carlo Tree Search (MCTS). Semantic parsing-based KBQA methods are particularly challenging as these approaches require locating elements from knowledge bases and generating logical forms, demanding not only extensive annotated data but also strong reasoning capabilities. Although recent approaches leveraging LLMs as agents have demonstrated considerable potential, these studies are inherently constrained by their linear decision-making processes. To address this limitation, we propose a MCTS-based framework that enhances LLMs' reasoning capabilities through tree search methodology. We design a carefully designed step-wise reward mechanism that requires only direct prompting of open-source instruction LLMs without additional fine-tuning. Experimental results demonstrate that our approach significantly outperforms linear decision-making methods, particularly in low-resource scenarios. Additionally, we contribute new data resources to the KBQA community by annotating intermediate reasoning processes for existing question-SPARQL datasets using distant supervision. Experimental results on the extended dataset demonstrate that our method achieves comparable performance to fully supervised models while using significantly less training data.", 'abstract_zh': '本研究通过利用蒙特卡洛树搜索（MCTS）探索如何增强大型语言模型（LLMs）在知识库问答（KBQA）中的推理能力。基于语义解析的KBQA方法尤其具有挑战性，因为这些方法要求从知识库中定位元素并生成逻辑形式，不仅需要大量的标注数据，还要求较强的推理能力。尽管最近利用LLMs作为代理的方法显示了巨大的潜力，但这些研究本质上受限于其线性的决策过程。为了解决这一限制，我们提出了一种基于MCTS的框架，通过树搜索方法增强LLMs的推理能力。我们设计了一种精心设计的逐步奖励机制，仅需直接提示开源指令LLMs，无需额外微调。实验结果表明，我们的方法在低资源场景下显著优于线性决策方法。此外，我们通过使用远程监督为现有的问答-SPARQL数据集标注中间推理过程，为KBQA社区贡献了新的数据资源。实验结果表明，我们的方法在使用显著较少训练数据的情况下实现了与完全监督模型相当的性能。', 'title_zh': 'MCTS-KBQA: 针对知识库问答的蒙特卡洛树搜索'}
{'arxiv_id': 'arXiv:2502.13422', 'title': 'TabSD: Large Free-Form Table Question Answering with SQL-Based Table Decomposition', 'authors': 'Yuxiang Wang, Junhao Gan, Jianzhong Qi', 'link': 'https://arxiv.org/abs/2502.13422', 'abstract': "Question answering on free-form tables (TableQA) is challenging due to the absence of predefined schemas and the presence of noise in large tables. While Large Language Models (LLMs) have shown promise in TableQA, they struggle with large free-form tables and noise sensitivity. To address these challenges, we propose TabSD, a SQL-based decomposition model that enhances LLMs' ability to process large free-form tables. TabSD generates SQL queries to guide the table decomposition, remove noise, and processes sub-tables for better answer generation. Additionally, SQL Verifier refines SQL outputs to enhance decomposition accuracy. We introduce two TableQA datasets with large free-form tables, SLQA and SEQA, which consist solely of large free-form tables and will be publicly available. Experimental results on four benchmark datasets demonstrate that TABSD outperforms the best-existing baseline models by 23.07%, 2.84%, 23.24% and 9.32% in accuracy, respectively, highlighting its effectiveness in handling large and noisy free-form tables.", 'abstract_zh': '基于SQL的表格分解模型TabSD在处理自由格式表格问答（TableQA）中的挑战', 'title_zh': 'TabSD: 基于SQL分解的大规模自由格式表格问答'}
{'arxiv_id': 'arXiv:2502.13417', 'title': 'RLTHF: Targeted Human Feedback for LLM Alignment', 'authors': 'Yifei Xu, Tusher Chakraborty, Emre Kıcıman, Bibek Aryal, Eduardo Rodrigues, Srinagesh Sharma, Roberto Estevao, Maria Angels de Luis Balaguer, Jessica Wolk, Rafael Padilha, Leonardo Nunes, Shobana Balakrishnan, Songwu Lu, Ranveer Chandra', 'link': 'https://arxiv.org/abs/2502.13417', 'abstract': "Fine-tuning large language models (LLMs) to align with user preferences is challenging due to the high cost of quality human annotations in Reinforcement Learning from Human Feedback (RLHF) and the generalizability limitations of AI Feedback. To address these challenges, we propose RLTHF, a human-AI hybrid framework that combines LLM-based initial alignment with selective human annotations to achieve full-human annotation alignment with minimal effort. RLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward model's reward distribution and iteratively enhances alignment by integrating strategic human corrections while leveraging LLM's correctly labeled samples. Evaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human annotation-level alignment with only 6-7% of the human annotation effort. Furthermore, models trained on RLTHF's curated datasets for downstream tasks outperform those trained on fully human-annotated datasets, underscoring the effectiveness of RLTHF's strategic data curation.", 'abstract_zh': '基于人类-AI混合框架的大型语言模型细调以实现用户偏好对齐', 'title_zh': 'RLTHF: 目标导向的人类反馈促进大模型对齐'}
{'arxiv_id': 'arXiv:2502.13412', 'title': 'Explore-Construct-Filter: An Automated Framework for Rich and Reliable API Knowledge Graph Construction', 'authors': 'Yanbang Sun, Qing Huang, Xiaoxue Ren, Zhenchang Xing, Xiaohong Li, Junjie Wang', 'link': 'https://arxiv.org/abs/2502.13412', 'abstract': "The API Knowledge Graph (API KG) is a structured network that models API entities and their relations, providing essential semantic insights for tasks such as API recommendation, code generation, and API misuse detection. However, constructing a knowledge-rich and reliable API KG presents several challenges. Existing schema-based methods rely heavily on manual annotations to design KG schemas, leading to excessive manual overhead. On the other hand, schema-free methods, due to the lack of schema guidance, are prone to introducing noise, reducing the KG's reliability. To address these issues, we propose the Explore-Construct-Filter framework, an automated approach for API KG construction based on large language models (LLMs). This framework consists of three key modules: 1) KG exploration: LLMs simulate the workflow of annotators to automatically design a schema with comprehensive type triples, minimizing human intervention; 2) KG construction: Guided by the schema, LLMs extract instance triples to construct a rich yet unreliable API KG; 3) KG filtering: Removing invalid type triples and suspicious instance triples to construct a rich and reliable API KG. Experimental results demonstrate that our method surpasses the state-of-the-art method, achieving a 25.2% improvement in F1 score. Moreover, the Explore-Construct-Filter framework proves effective, with the KG exploration module increasing KG richness by 133.6% and the KG filtering module improving reliability by 26.6%. Finally, cross-model experiments confirm the generalizability of our framework.", 'abstract_zh': 'API知识图谱（API KG）是建模API实体及其关系的结构化网络，为API推荐、代码生成和API滥用检测等任务提供重要的语义洞察。然而，构建丰富且可靠的API KG存在若干挑战。现有的基于模式的方法高度依赖手工标注来设计KG模式，导致手工劳动量过大。另一方面，无模式方法由于缺乏模式指导，容易引入噪声，降低KG的可靠性。为解决这些问题，我们提出了一种基于大规模语言模型（LLMs）的API KG自动构建框架——Explore-Construct-Filter框架。该框架包含三个关键模块：1）KG探索：LLMs模拟标注人员的工作流程，自动设计具有全面类型三元组的模式，最大限度减少人工干预；2）KG构建：根据模式指导，LLMs提取实例三元组以构建丰富但不可靠的API KG；3）KG过滤：去除无效类型三元组和可疑实例三元组，构建丰富且可靠的API KG。实验结果显示，我们的方法超越了现有最佳方法，F1分数提高25.2%。此外，Explore-Construct-Filter框架的有效性证明，KG探索模块使KG丰富度提高133.6%，KG过滤模块提高了26.6%的可靠性。最后，跨模型实验确认了该框架的普适性。', 'title_zh': '探索-构建-过滤：一种自动化的丰富可靠API知识图构建框架'}
{'arxiv_id': 'arXiv:2502.13410', 'title': 'Tell Me Why: Incentivizing Explanations', 'authors': 'Siddarth Srinivasan, Ezra Karger, Michiel Bakker, Yiling Chen', 'link': 'https://arxiv.org/abs/2502.13410', 'abstract': "Common sense suggests that when individuals explain why they believe something, we can arrive at more accurate conclusions than when they simply state what they believe. Yet, there is no known mechanism that provides incentives to elicit explanations for beliefs from agents. This likely stems from the fact that standard Bayesian models make assumptions (like conditional independence of signals) that preempt the need for explanations, in order to show efficient information aggregation. A natural justification for the value of explanations is that agents' beliefs tend to be drawn from overlapping sources of information, so agents' belief reports do not reveal all that needs to be known. Indeed, this work argues that rationales-explanations of an agent's private information-lead to more efficient aggregation by allowing agents to efficiently identify what information they share and what information is new. Building on this model of rationales, we present a novel 'deliberation mechanism' to elicit rationales from agents in which truthful reporting of beliefs and rationales is a perfect Bayesian equilibrium.", 'abstract_zh': '常识表明，当个体解释他们为什么相信某事时，我们比仅仅陈述他们的信念可以获得更准确的结论。然而，目前尚无机制激励个体提供信念的解释。这可能源于标准贝叶斯模型基于（如信号的条件独立性）假设来预先排除解释的需求，以便展示信息聚合的效率。解释的价值自然在于个体的信念往往源自重叠的信息来源，因此个体的信念报告并未揭示所有需要了解的信息。事实上，本文认为，解释——即个体的理性说明——通过使个体有效识别共享信息与新信息，促进了更高效的聚合。在此模型的基础上，我们提出了一种新颖的“反思机制”，以激励个体报告信念及其解释，诚实行事的信念和解释是完美的贝叶斯均衡。', 'title_zh': '告诉我原因：激励解释'}
{'arxiv_id': 'arXiv:2502.13407', 'title': 'JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework', 'authors': 'Ziyuan Liu, Ruifei Zhu, Long Gao, Yuanxiu Zhou, Jingyu Ma, Yuantao Gu', 'link': 'https://arxiv.org/abs/2502.13407', 'abstract': 'Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas. To address these issues, we introduce the JL1-CD dataset, which contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5 to 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation (MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD datasets demonstrate that the MTKD framework significantly improves the performance of CD models with various network architectures and parameter sizes, achieving new state-of-the-art results. The code is available at this https URL.', 'abstract_zh': '深学习在遥感图像变化检测领域的应用取得了显著成功，但仍面临两个主要挑战：亚米级全面的开源变化检测数据集稀缺，以及在变化区域变化的图像中实现一致且满意的检测结果的难度。为此，我们介绍了JL1-CD数据集，该数据集包含5,000对分辨率为0.5至0.75米的512×512像素图像。此外，我们提出了多师知识蒸馏（MTKD）框架用于变化检测。在JL1-CD和SYSU-CD数据集上的实验结果表明，MTKD框架显著提高了各种网络架构和参数量的变化检测模型性能，并取得了新的最佳结果。相关代码可访问此链接。', 'title_zh': 'JL1-CD：一个新的遥感变化检测基准以及一个稳健的多师知识蒸馏框架'}
{'arxiv_id': 'arXiv:2502.13406', 'title': 'Generative Predictive Control: Flow Matching Policies for Dynamic and Difficult-to-Demonstrate Tasks', 'authors': 'Vince Kurtz, Joel W. Burdick', 'link': 'https://arxiv.org/abs/2502.13406', 'abstract': 'Generative control policies have recently unlocked major progress in robotics. These methods produce action sequences via diffusion or flow matching, with training data provided by demonstrations. But despite enjoying considerable success on difficult manipulation problems, generative policies come with two key limitations. First, behavior cloning requires expert demonstrations, which can be time-consuming and expensive to obtain. Second, existing methods are limited to relatively slow, quasi-static tasks. In this paper, we leverage a tight connection between sampling-based predictive control and generative modeling to address each of these issues. In particular, we introduce generative predictive control, a supervised learning framework for tasks with fast dynamics that are easy to simulate but difficult to demonstrate. We then show how trained flow-matching policies can be warm-started at run-time, maintaining temporal consistency and enabling fast feedback rates. We believe that generative predictive control offers a complementary approach to existing behavior cloning methods, and hope that it paves the way toward generalist policies that extend beyond quasi-static demonstration-oriented tasks.', 'abstract_zh': '生成式预测控制 recently unlocked major progress in robotics.', 'title_zh': '生成预测控制：流匹配策略用于动态和难以演示的任务'}
{'arxiv_id': 'arXiv:2502.13398', 'title': '$\\mathtt{GeLLM^3O}$: Generalizing Large Language Models for Multi-property Molecule Optimization', 'authors': 'Vishal Dey, Xiao Hu, Xia Ning', 'link': 'https://arxiv.org/abs/2502.13398', 'abstract': "Despite recent advancements, most computational methods for molecule optimization are constrained to single- or double-property optimization tasks and suffer from poor scalability and generalizability to novel optimization tasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable out-of-domain generalizability to novel tasks. To demonstrate LLMs' potential for molecule optimization, we introduce $\\mathtt{MoMUInstruct}$, the first high-quality instruction-tuning dataset specifically focused on complex multi-property molecule optimization tasks. Leveraging $\\mathtt{MoMUInstruct}$, we develop $\\mathtt{GeLLM^3O}$s, a series of instruction-tuned LLMs for molecule optimization. Extensive evaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that $\\mathtt{GeLLM^3O}$s consistently outperform state-of-the-art baselines. $\\mathtt{GeLLM^3O}$s also exhibit outstanding zero-shot generalization to unseen tasks, significantly outperforming powerful closed-source LLMs. Such strong generalizability demonstrates the tremendous potential of $\\mathtt{GeLLM^3O}$s as foundational models for molecule optimization, thereby tackling novel optimization tasks without resource-intensive retraining. $\\mathtt{MoMUInstruct}$, models, and code are accessible through this https URL.", 'abstract_zh': '尽管近期取得了进展，大多数分子优化的计算方法仍然局限于单一或双性质优化任务，并且在扩展性和新型优化任务的一般化能力方面表现不佳。与此同时，大规模语言模型（LLMs）在新型任务上的跨域一般化表现出色。为了展示LLMs在分子优化领域的潜力，我们引入了$\\mathtt{MoMUInstruct}$，这是首个专注于复杂多性质分子优化任务的高质量指令调优数据集。基于$\\mathtt{MoMUInstruct}$，我们开发了$\\mathtt{GeLLM^3O}$系列指令调优的大规模语言模型，用于分子优化。广泛的任务评估表明，$\\mathtt{GeLLM^3O}$系列模型在5个领域内和5个领域外任务中都优于最先进的基线模型。$\\mathtt{GeLLM^3O}$模型还展现出在未见过的任务上的出色零样本泛化能力，显著优于强大的闭源大规模语言模型。这种强大的泛化能力表明$\\mathtt{GeLLM^3O}$模型作为分子优化领域的基础模型具有巨大的潜力，能够无需资源密集型的重新训练来应对新型优化任务。$\\mathtt{MoMUInstruct}$数据集、模型和代码可通过以下链接访问：https://。', 'title_zh': 'GeLLM^3O: 多性质分子优化的大语言模型泛化'}
{'arxiv_id': 'arXiv:2502.13376', 'title': 'Learning Symbolic Task Decompositions for Multi-Agent Teams', 'authors': 'Ameesh Shah, Niklas Lauffer, Thomas Chen, Nikhil Pitta, Sanjit A. Seshia', 'link': 'https://arxiv.org/abs/2502.13376', 'abstract': "One approach for improving sample efficiency in cooperative multi-agent learning is to decompose overall tasks into sub-tasks that can be assigned to individual agents. We study this problem in the context of reward machines: symbolic tasks that can be formally decomposed into sub-tasks. In order to handle settings without a priori knowledge of the environment, we introduce a framework that can learn the optimal decomposition from model-free interactions with the environment. Our method uses a task-conditioned architecture to simultaneously learn an optimal decomposition and the corresponding agents' policies for each sub-task. In doing so, we remove the need for a human to manually design the optimal decomposition while maintaining the sample-efficiency benefits of improved credit assignment. We provide experimental results in several deep reinforcement learning settings, demonstrating the efficacy of our approach. Our results indicate that our approach succeeds even in environments with codependent agent dynamics, enabling synchronous multi-agent learning not achievable in previous works.", 'abstract_zh': '改善合作多智能体学习样本效率的一种方法是将整体任务分解为可以分配给单个智能体的子任务：基于奖励机器的形式化可分解符号任务中任务分解问题的研究。为了处理先验环境知识未知的设置，我们引入了一种可以从无模型交互中学习最优分解的框架。我们的方法使用任务条件化架构同时学习最优分解及其对应的每个子任务的智能体策略，从而去除手动设计最优分解的需要，同时保持改进的奖励归因效益。我们在多个深度强化学习设置中提供实验结果，展示了我们方法的有效性。我们的结果表明，我们的方法即使在存在相互依赖智能体动力学的环境中也能成功，从而使得同步多智能体学习成为可能，这在之前的工作中是不可实现的。', 'title_zh': '学习符号化任务分解的多Agent团队策略'}
{'arxiv_id': 'arXiv:2502.13361', 'title': 'RGAR: Recurrence Generation-augmented Retrieval for Factual-aware Medical Question Answering', 'authors': 'Sichu Liang, Linhai Zhang, Hongyu Zhu, Wenwen Wang, Yulan He, Deyu Zhou', 'link': 'https://arxiv.org/abs/2502.13361', 'abstract': 'Medical question answering requires extensive access to specialized conceptual knowledge. The current paradigm, Retrieval-Augmented Generation (RAG), acquires expertise medical knowledge through large-scale corpus retrieval and uses this knowledge to guide a general-purpose large language model (LLM) for generating answers. However, existing retrieval approaches often overlook the importance of factual knowledge, which limits the relevance of retrieved conceptual knowledge and restricts its applicability in real-world scenarios, such as clinical decision-making based on Electronic Health Records (EHRs). This paper introduces RGAR, a recurrence generation-augmented retrieval framework that retrieves both relevant factual and conceptual knowledge from dual sources (i.e., EHRs and the corpus), allowing them to interact and refine each another. Through extensive evaluation across three factual-aware medical question answering benchmarks, RGAR establishes a new state-of-the-art performance among medical RAG systems. Notably, the Llama-3.1-8B-Instruct model with RGAR surpasses the considerably larger, RAG-enhanced GPT-3.5. Our findings demonstrate the benefit of extracting factual knowledge for retrieval, which consistently yields improved generation quality.', 'abstract_zh': '医疗问答需要广泛获取专门的概念知识。当前的范式，检索增强生成（RAG），通过大规模语料库检索获得医学专业知识，并利用这些知识引导通用大型语言模型（LLM）生成答案。然而，现有的检索方法往往忽略了事实知识的重要性，这限制了检索概念知识的相关性，并在诸如基于电子健康记录（EHRs）的临床决策制定等实际场景中限制了其适用性。本文介绍了一种名为RGAR的循环生成增强检索框架，该框架从双重来源（即EHRs和语料库）检索相关的事实知识和概念知识，允许它们相互作用和相互完善。通过在三个事实感知的医疗问答基准上的广泛评估，RGAR在医疗RAG系统中建立了新的最佳性能。值得注意的是，使用RGAR的Llama-3.1-8B-Instruct模型超越了显著更大的、增强了的RAG-GPT-3.5模型。我们的研究结果表明，提取事实知识对于检索的益处，持续地提高了生成质量。', 'title_zh': 'RGAR：基于 recurrence 生成增强检索的医学事实感知问答'}
{'arxiv_id': 'arXiv:2502.13345', 'title': 'Secure and Efficient Watermarking for Latent Diffusion Models in Model Distribution Scenarios', 'authors': 'Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu', 'link': 'https://arxiv.org/abs/2502.13345', 'abstract': 'Latent diffusion models have exhibited considerable potential in generative tasks. Watermarking is considered to be an alternative to safeguard the copyright of generative models and prevent their misuse. However, in the context of model distribution scenarios, the accessibility of models to large scale of model users brings new challenges to the security, efficiency and robustness of existing watermark solutions. To address these issues, we propose a secure and efficient watermarking solution. A new security mechanism is designed to prevent watermark leakage and watermark escape, which considers watermark randomness and watermark-model association as two constraints for mandatory watermark injection. To reduce the time cost of training the security module, watermark injection and the security mechanism are decoupled, ensuring that fine-tuning VAE only accomplishes the security mechanism without the burden of learning watermark patterns. A watermark distribution-based verification strategy is proposed to enhance the robustness against diverse attacks in the model distribution scenarios. Experimental results prove that our watermarking consistently outperforms existing six baselines on effectiveness and robustness against ten image processing attacks and adversarial attacks, while enhancing security in the distribution scenarios.', 'abstract_zh': '潜在扩散模型在生成任务中展现出显著潜力。水印技术被视为保护生成模型版权和防止其滥用的一种替代方案。然而，在模型分发场景下，模型对大规模用户群体的可访问性带来了新的挑战，对现有水印解决方案的安全性、效率和鲁棒性提出了新的要求。为应对这些挑战，我们提出了一种安全高效的水印解决方案。设计了一种新的安全机制，以防止水印泄露和水印逃逸，并将水印随机性和水印-模型关联作为强制性水印注入的两大约束条件。通过将水印注入和安全机制解耦，减少了训练安全模块所需的时间成本，确保仅通过微调VAE即可实现安全机制，而不必学习水印模式。提出了基于水印分发的验证策略，以增强模型分发场景下的鲁棒性。实验结果证明，我们的水印技术在有效性及对十种图像处理攻击和对抗攻击的鲁棒性方面，均优于现有六种基线方法，同时在分发场景中增强了安全性。', 'title_zh': '在模型分发场景中具有安全性和高效性的潜扩散模型水印技术'}
{'arxiv_id': 'arXiv:2502.13339', 'title': 'How Expressive are Knowledge Graph Foundation Models?', 'authors': 'Xingyue Huang, Pablo Barceló, Michael M. Bronstein, İsmail İlkan Ceylan, Mikhail Galkin, Juan L Reutter, Miguel Romero Orth', 'link': 'https://arxiv.org/abs/2502.13339', 'abstract': "Knowledge Graph Foundation Models (KGFMs) are at the frontier for deep learning on knowledge graphs (KGs), as they can generalize to completely novel knowledge graphs with different relational vocabularies. Despite their empirical success, our theoretical understanding of KGFMs remains very limited. In this paper, we conduct a rigorous study of the expressive power of KGFMs. Specifically, we show that the expressive power of KGFMs directly depends on the motifs that are used to learn the relation representations. We then observe that the most typical motifs used in the existing literature are binary, as the representations are learned based on how pairs of relations interact, which limits the model's expressiveness. As part of our study, we design more expressive KGFMs using richer motifs, which necessitate learning relation representations based on, e.g., how triples of relations interact with each other. Finally, we empirically validate our theoretical findings, showing that the use of richer motifs results in better performance on a wide range of datasets drawn from different domains.", 'abstract_zh': '知识图谱基础模型的知识图谱表示能力研究：从二元模式到更丰富的模式', 'title_zh': '知识图谱基础模型的表现力如何？'}
{'arxiv_id': 'arXiv:2502.13337', 'title': 'Language Models are Few-Shot Graders', 'authors': 'Chenyan Zhao, Mariana Silva, Seth Poulsen', 'link': 'https://arxiv.org/abs/2502.13337', 'abstract': 'Providing evaluations to student work is a critical component of effective student learning, and automating its process can significantly reduce the workload on human graders. Automatic Short Answer Grading (ASAG) systems, enabled by advancements in Large Language Models (LLMs), offer a promising solution for assessing and providing instant feedback for open-ended student responses. In this paper, we present an ASAG pipeline leveraging state-of-the-art LLMs. Our new LLM-based ASAG pipeline achieves better performances than existing custom-built models on the same datasets. We also compare the grading performance of three OpenAI models: GPT-4, GPT-4o, and o1-preview. Our results demonstrate that GPT-4o achieves the best balance between accuracy and cost-effectiveness. On the other hand, o1-preview, despite higher accuracy, exhibits a larger variance in error that makes it less practical for classroom use. We investigate the effects of incorporating instructor-graded examples into prompts using no examples, random selection, and Retrieval-Augmented Generation (RAG)-based selection strategies. Our findings indicate that providing graded examples enhances grading accuracy, with RAG-based selection outperforming random selection. Additionally, integrating grading rubrics improves accuracy by offering a structured standard for evaluation.', 'abstract_zh': '基于大型语言模型的自动短回答评分管道：性能分析与策略优化', 'title_zh': '语言模型是少-shot评分器'}
{'arxiv_id': 'arXiv:2502.13329', 'title': 'Language Models Can Predict Their Own Behavior', 'authors': 'Dhananjay Ashok, Jonathan May', 'link': 'https://arxiv.org/abs/2502.13329', 'abstract': 'Autoregressive Language Models output text by sequentially predicting the next token to generate, with modern methods like Chain-of-Thought (CoT) prompting achieving state-of-the-art reasoning capabilities by scaling the number of generated tokens. However, are there times when we can infer how the model will behave (e.g. abstain from answering a question) early in the computation, making generation unnecessary? We show that internal representation of input tokens alone can often precisely predict, not just the next token, but eventual behavior over the entire output sequence. We leverage this capacity and learn probes on internal states to create early warning (and exit) systems. Specifically, if the probes can confidently estimate the way the LM is going to behave, then the system will avoid generating tokens altogether and return the estimated behavior instead. On 27 text classification datasets spanning five different tasks, we apply this method to estimate the eventual answer of an LM under CoT prompting, reducing inference costs by 65% (average) while suffering an accuracy loss of no more than 1.4% (worst case). We demonstrate the potential of this method to pre-emptively identify when a model will abstain from answering a question, fail to follow output format specifications, or give a low-confidence response. We explore the limits of this capability, showing that probes generalize to unseen datasets, but perform worse when LM outputs are longer and struggle to predict properties that require access to knowledge that the models themselves lack. Encouragingly, performance scales with model size, suggesting applicability to the largest of models', 'abstract_zh': '自回归语言模型通过序贯预测下一个词令牌来生成文本，现代方法如Chain-of-Thought (CoT) 提示技术通过扩大生成的词令牌数量实现了最先进的推理能力。然而，在计算的早期我们是否可以推断出模型的行为（例如，避免回答某个问题），从而使得生成变得没有必要？我们表明，仅输入词令牌的内部表示通常可以精确预测最终生成序列的整体行为，而不仅仅是下一个词。我们利用这一能力，通过在内部状态上学习探针来创建早期预警（和退出）系统。具体而言，如果探针能够自信地估计自回归语言模型将如何表现，则系统将完全避免生成词令牌，而是返回估计的行为。在涵盖五个不同任务的27个文本分类数据集上，我们应用此方法以CoT提示方式估计自回归语言模型的最终答案，减少了65%的推理成本（平均值），同时准确性损失不超过1.4%（最坏情况）。我们展示了此方法预判模型何时避免回答问题、无法遵循输出格式规范或给出低置信度响应的潜力。我们探讨了这一能力的极限，表明探针能够泛化到未见过的数据集，但在自回归语言模型输出较长且难以预测需要模型自身所缺乏知识的属性时表现较差。令人鼓舞的是，性能随着模型规模的增加而提高，表明该方法适用于最大的模型。', 'title_zh': '语言模型可以预测自身的行为'}
{'arxiv_id': 'arXiv:2502.13321', 'title': 'Adjust for Trust: Mitigating Trust-Induced Inappropriate Reliance on AI Assistance', 'authors': 'Tejas Srinivasan, Jesse Thomason', 'link': 'https://arxiv.org/abs/2502.13321', 'abstract': "Trust biases how users rely on AI recommendations in AI-assisted decision-making tasks, with low and high levels of trust resulting in increased under- and over-reliance, respectively. We propose that AI assistants should adapt their behavior through trust-adaptive interventions to mitigate such inappropriate reliance. For instance, when user trust is low, providing an explanation can elicit more careful consideration of the assistant's advice by the user. In two decision-making scenarios -- laypeople answering science questions and doctors making medical diagnoses -- we find that providing supporting and counter-explanations during moments of low and high trust, respectively, yields up to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. We are similarly able to reduce over-reliance by adaptively inserting forced pauses to promote deliberation. Our results highlight how AI adaptation to user trust facilitates appropriate reliance, presenting exciting avenues for improving human-AI collaboration.", 'abstract_zh': '信任偏差：用户在AI辅助决策任务中依赖AI推荐的程度，低信任和高信任分别导致不当依赖的增加和减少。我们提出，AI助手应通过信任自适应干预来调整其行为，以减轻这种不当依赖。例如，当用户信任度较低时，提供解释可以促使用户更认真地考虑助手的建议。在两种决策场景——普通人在回答科学问题和医生进行医疗诊断中——我们发现，在低信任和高信任时刻分别提供支持性解释和反驳性解释，可将不当依赖减少最多38%，决策准确性提高20%。我们还能够通过适应性插入强制暂停来促进深思熟虑，从而减少过度依赖。我们的研究结果显示，AI对用户信任的适应性调整有助于实现适当的依赖，为改善人机协作提供了令人兴奋的新方向。', 'title_zh': '调整信任：减轻由信任引起的不当依赖人工智能辅助问题'}
{'arxiv_id': 'arXiv:2502.13311', 'title': 'Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors', 'authors': 'Jian Wang, Yinpei Dai, Yichi Zhang, Ziqiao Ma, Wenjie Li, Joyce Chai', 'link': 'https://arxiv.org/abs/2502.13311', 'abstract': "Intelligent tutoring agents powered by large language models (LLMs) have been increasingly explored to deliver personalized guidance in areas such as language learning and science education. However, their capabilities in guiding users to solve complex real-world tasks remain underexplored. To address this limitation, in this work, we focus on coding tutoring, a challenging problem that requires tutors to proactively guide students toward completing predefined coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER), which combines knowledge tracing to estimate a student's knowledge state and turn-by-turn verification to ensure effective guidance toward task completion. We introduce DICT, an automatic evaluation protocol that assesses tutor agents holistically using controlled student simulation and code generation tests. Extensive experiments reveal the challenges of coding tutoring and demonstrate that TRAVER achieves a significantly higher success rate. Although we use code tutoring as an example in this paper, our results and findings can be extended beyond coding, providing valuable insights into advancing tutoring agents for a variety of tasks.", 'abstract_zh': '由大型语言模型驱动的智能辅导代理在语言学习和科学教育等领域提供了个性化指导，然而它们在引导用户解决复杂实际任务方面的能力仍待探索。为解决这一局限，本文专注于编码辅导这一具有挑战性的问题，要求辅导代理积极引导学生完成预定义的编码任务。我们提出了一种新的代理工作流——追踪与验证（TRACE-AND-VERIFY，TRAVER），该工作流结合了知识追踪以估计学生知识状态，并通过逐一验证确保对任务完成的有效指导。我们介绍了DICT，一种自动评估协议，通过受控的学生模拟和代码生成测试全方位评估辅导代理。大量实验揭示了编码辅导的挑战，并证明了TRAVER获得了显著更高的成功率。尽管本文以代码辅导为例，但我们的结果和发现可以扩展到其他领域，为各类任务的辅导代理研发提供了宝贵的见解。', 'title_zh': '基于逐轮验证的对话辅导剂训练：大语言模型作为你的编程导师的有趣案例'}
{'arxiv_id': 'arXiv:2502.13297', 'title': 'Understanding and Tackling Label Errors in Individual-Level Nature Language Understanding', 'authors': 'Yunpeng Xiao, Youpeng Zhao, Kai Shu', 'link': 'https://arxiv.org/abs/2502.13297', 'abstract': 'Natural language understanding (NLU) is a task that enables machines to understand human language. Some tasks, such as stance detection and sentiment analysis, are closely related to individual subjective perspectives, thus termed individual-level NLU. Previously, these tasks are often simplified to text-level NLU tasks, ignoring individual factors. This not only makes inference difficult and unexplainable but often results in a large number of label errors when creating datasets. To address the above limitations, we propose a new NLU annotation guideline based on individual-level factors. Specifically, we incorporate other posts by the same individual and then annotate individual subjective perspectives after considering all individual posts. We use this guideline to expand and re-annotate the stance detection and topic-based sentiment analysis datasets. We find that error rates in the samples were as high as 31.7\\% and 23.3\\%. We further use large language models to conduct experiments on the re-annotation datasets and find that the large language models perform well on both datasets after adding individual factors. Both GPT-4o and Llama3-70B can achieve an accuracy greater than 87\\% on the re-annotation datasets. We also verify the effectiveness of individual factors through ablation studies. We call on future researchers to add individual factors when creating such datasets. Our re-annotation dataset can be found at this https URL', 'abstract_zh': '基于个体因素的自然语言理解标注指南：扩展与重注释立场检测和主题相关的情感分析数据集', 'title_zh': '理解并解决个体水平自然语言理解中的标签错误问题'}
{'arxiv_id': 'arXiv:2502.13290', 'title': 'Prediction of Clinical Complication Onset using Neural Point Processes', 'authors': 'Sachini Weerasekara, Sagar Kamarthi, Jacqueline Isaacs', 'link': 'https://arxiv.org/abs/2502.13290', 'abstract': 'Predicting medical events in advance within critical care settings is paramount for patient outcomes and resource management. Utilizing predictive models, healthcare providers can anticipate issues such as cardiac arrest, sepsis, or respiratory failure before they manifest. Recently, there has been a surge in research focusing on forecasting adverse medical event onsets prior to clinical manifestation using machine learning. However, while these models provide temporal prognostic predictions for the occurrence of a specific adverse event of interest within defined time intervals, their interpretability often remains a challenge. In this work, we explore the applicability of neural temporal point processes in the context of adverse event onset prediction, with the aim of explaining clinical pathways and providing interpretable insights. Our experiments span six state-of-the-art neural point processes and six critical care datasets, each focusing on the onset of distinct adverse events. This work represents a novel application class of neural temporal point processes in event prediction.', 'abstract_zh': '在重症监护环境中提前预测医疗事件对于患者的预后和资源管理至关重要。利用预测模型，医疗提供者可以在临床表现之前预见诸如心搏骤停、感染性休克或呼吸衰竭等问题。近年来，研究关注使用机器学习在临床表现之前预测不良医疗事件的出现，虽然这些模型可以在特定时间区间内提供关于特定不利事件发生的时序预估预测，但其可解释性常常仍是挑战。在这项研究中，我们探索了神经时间点过程在不良事件出现预测中的应用，旨在解释临床路径并提供可解释的见解。我们的实验涵盖了六种最先进的神经时间点过程和六种重症监护数据集，每个数据集都侧重于特定不良事件的出现。这项工作代表了神经时间点过程在事件预测中的一种新应用类别。', 'title_zh': '使用神经点过程预测临床并发症的发生'}
{'arxiv_id': 'arXiv:2502.13278', 'title': 'Performance Evaluation of Sentiment Analysis on Text and Emoji Data Using End-to-End, Transfer Learning, Distributed and Explainable AI Models', 'authors': 'Sirisha Velampalli, Chandrashekar Muniyappa, Ashutosh Saxena', 'link': 'https://arxiv.org/abs/2502.13278', 'abstract': 'Emojis are being frequently used in todays digital world to express from simple to complex thoughts more than ever before. Hence, they are also being used in sentiment analysis and targeted marketing campaigns. In this work, we performed sentiment analysis of Tweets as well as on emoji dataset from the Kaggle. Since tweets are sentences we have used Universal Sentence Encoder (USE) and Sentence Bidirectional Encoder Representations from Transformers (SBERT) end-to-end sentence embedding models to generate the embeddings which are used to train the Standard fully connected Neural Networks (NN), and LSTM NN models. We observe the text classification accuracy was almost the same for both the models around 98 percent. On the contrary, when the validation set was built using emojis that were not present in the training set then the accuracy of both the models reduced drastically to 70 percent. In addition, the models were also trained using the distributed training approach instead of a traditional singlethreaded model for better scalability. Using the distributed training approach, we were able to reduce the run-time by roughly 15% without compromising on accuracy. Finally, as part of explainable AI the Shap algorithm was used to explain the model behaviour and check for model biases for the given feature set.', 'abstract_zh': 'emojis在当今数字世界中被频繁用于表达从简单到复杂的thoughts，其使用频率超过以往。因此，它们也被用于情感分析和目标营销活动。本文对推文以及来自Kaggle的emoji数据集进行了情感分析。由于推文是句子，我们使用了全局句子编码器（USE）和双向Transformer句子表示（SBERT）端到端句子嵌入模型生成嵌入，并使用这些嵌入训练标准全连接神经网络（NN）和LSTM NN模型。我们观察到，两种模型的文本分类准确率几乎相同，约为98%。然而，当验证集使用训练集中未出现的emoji构建时，两种模型的准确率骤降至70%。此外，我们还使用分布式训练方法而不是传统的单线程模型来训练模型，以提高可扩展性。使用分布式训练方法，我们能够在不牺牲准确性的前提下将运行时间减少约15%。最后，作为可解释AI的一部分，我们使用Shap算法解释模型行为并检查给定特征集的模型偏差。', 'title_zh': '基于端到端、迁移学习、分布式和可解释AI模型的情感分析在文本和Emoji数据上的性能评估'}
{'arxiv_id': 'arXiv:2502.13277', 'title': 'HyperGCL: Multi-Modal Graph Contrastive Learning via Learnable Hypergraph Views', 'authors': 'Khaled Mohammed Saifuddin, Jonathan Shihao Ji, Esra Akbas', 'link': 'https://arxiv.org/abs/2502.13277', 'abstract': "Recent advancements in Graph Contrastive Learning (GCL) have demonstrated remarkable effectiveness in improving graph representations. However, relying on predefined augmentations (e.g., node dropping, edge perturbation, attribute masking) may result in the loss of task-relevant information and a lack of adaptability to diverse input data. Furthermore, the selection of negative samples remains rarely explored. In this paper, we introduce HyperGCL, a novel multimodal GCL framework from a hypergraph perspective. HyperGCL constructs three distinct hypergraph views by jointly utilizing the input graph's structure and attributes, enabling a comprehensive integration of multiple modalities in contrastive learning. A learnable adaptive topology augmentation technique enhances these views by preserving important relations and filtering out noise. View-specific encoders capture essential characteristics from each view, while a network-aware contrastive loss leverages the underlying topology to define positive and negative samples effectively. Extensive experiments on benchmark datasets demonstrate that HyperGCL achieves state-of-the-art node classification performance.", 'abstract_zh': 'Recent Advances in Hypergraph Contrastive Learning for Graph Representation Improvement', 'title_zh': 'HyperGCL：基于可学习超图视图的多模态图对比学习'}
{'arxiv_id': 'arXiv:2502.13260', 'title': 'Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models', 'authors': 'Yingqian Cui, Pengfei He, Jingying Zeng, Hui Liu, Xianfeng Tang, Zhenwei Dai, Yan Han, Chen Luo, Jing Huang, Zhen Li, Suhang Wang, Yue Xing, Jiliang Tang, Qi He', 'link': 'https://arxiv.org/abs/2502.13260', 'abstract': 'Chain-of-Thought (CoT) reasoning, which breaks down complex tasks into intermediate reasoning steps, has significantly enhanced the performance of large language models (LLMs) on challenging tasks. However, the detailed reasoning process in CoT often incurs long generation times and high computational costs, partly due to the inclusion of unnecessary steps. To address this, we propose a method to identify critical reasoning steps using perplexity as a measure of their importance: a step is deemed critical if its removal causes a significant increase in perplexity. Our method enables models to focus solely on generating these critical steps. This can be achieved through two approaches: refining demonstration examples in few-shot CoT or fine-tuning the model using selected examples that include only critical steps. Comprehensive experiments validate the effectiveness of our method, which achieves a better balance between the reasoning accuracy and efficiency of CoT.', 'abstract_zh': 'Chain-of-Thought (CoT)推理，通过将复杂任务分解为中间推理步骤，显著提升了大规模语言模型（LLMs）在挑战性任务上的性能。然而，CoT中的详细推理过程往往导致生成时间长和高计算成本，部分原因是包含了不必要的步骤。为解决这一问题，我们提出了一种使用困惑度作为重要性指标来识别关键推理步骤的方法：如果移除某步骤会导致困惑度显著增加，则该步骤被视为关键步骤。该方法使模型能够仅聚焦于生成这些关键步骤。这可以通过两种方式实现：改进少量示例CoT中的演示示例，或使用仅包含关键步骤的选定示例对模型进行微调。全面的实验验证了该方法的有效性，实现了CoT在推理准确性和效率上的更好平衡。', 'title_zh': '逐级困惑度导向细化以提高大型语言模型的高效链式推理'}
{'arxiv_id': 'arXiv:2502.13259', 'title': 'HumT DumT: Measuring and controlling human-like language in LLMs', 'authors': 'Myra Cheng, Sunny Yu, Dan Jurafsky', 'link': 'https://arxiv.org/abs/2502.13259', 'abstract': 'Should LLMs generate language that makes them seem human? Human-like language might improve user experience, but might also lead to overreliance and stereotyping. Assessing these potential impacts requires a systematic way to measure human-like tone in LLM outputs. We introduce HumT and SocioT, metrics for human-like tone and other dimensions of social perceptions in text data based on relative probabilities from an LLM. By measuring HumT across preference and usage datasets, we find that users prefer less human-like outputs from LLMs. HumT also offers insights into the impacts of anthropomorphism: human-like LLM outputs are highly correlated with warmth, social closeness, femininity, and low status, which are closely linked to the aforementioned harms. We introduce DumT, a method using HumT to systematically control and reduce the degree of human-like tone while preserving model performance. DumT offers a practical approach for mitigating risks associated with anthropomorphic language generation.', 'abstract_zh': 'LLMs生成类人类语言是否合适？类人类语言可能改善用户体验，但也可能导致过度依赖和刻板印象。评估这些潜在影响需要一种系统的方法来衡量LLM输出中的人类化语气。我们引入了HumT和SocioT，基于LLM相对概率的文本数据中人类化语气和社会感知的度量标准。通过对偏好和使用数据集中的HumT进行测量，我们发现用户更倾向于LLM生成的较不类人类的输出。HumT还提供了关于拟人化影响的见解：类人类的LLM输出与温暖、社交亲近、女性化和低地位高度相关，这些都与上述危害密切相关。我们引入了DumT，一种使用HumT系统地控制和减少人类化语气程度的方法，同时保持模型性能。DumT提供了一种缓解拟人化语言生成风险的实用方法。', 'title_zh': 'HumT DumT: 测量和控制LLM中的人类语言特性'}
{'arxiv_id': 'arXiv:2502.13256', 'title': 'A Survey of Anomaly Detection in Cyber-Physical Systems', 'authors': 'Danial Abshari, Meera Sridhar', 'link': 'https://arxiv.org/abs/2502.13256', 'abstract': 'In our increasingly interconnected world, Cyber-Physical Systems (CPS) play a crucial role in industries like healthcare, transportation, and manufacturing by combining physical processes with computing power. These systems, however, face many challenges, especially regarding security and system faults. Anomalies in CPS may indicate unexpected problems, from sensor malfunctions to cyber-attacks, and must be detected to prevent failures that can cause harm or disrupt services. This paper provides an overview of the different ways researchers have approached anomaly detection in CPS. We categorize and compare methods like machine learning, deep learning, mathematical models, invariant, and hybrid techniques. Our goal is to help readers understand the strengths and weaknesses of these methods and how they can be used to create safer, more reliable CPS. By identifying the gaps in current solutions, we aim to encourage future research that will make CPS more secure and adaptive in our increasingly automated world.', 'abstract_zh': '在日益互联的世界中， Cyber-Physical Systems (CPS) 通过结合物理过程与计算能力，在医疗、交通和制造等行业中发挥着关键作用。然而，这些系统面临许多挑战，尤其是在安全性和系统故障方面。CPS中的异常可能表明从传感器故障到网络攻击等各种意外问题，必须被检测以防止可能造成损害或中断服务的故障。本文概述了研究人员在CPS中采用的不同异常检测方法。我们将这些方法（包括机器学习、深度学习、数学模型、不变量以及混合技术）进行分类和比较，旨在帮助读者了解这些方法的优势和局限性，并指导其用于创建更安全、更可靠的CPS。通过识别当前解决方案的不足，我们希望鼓励未来的研究，使CPS在日益自动化的世界中更具安全性和适应性。', 'title_zh': '网络物理系统中的异常检测综述'}
{'arxiv_id': 'arXiv:2502.13251', 'title': 'Neural Attention Search', 'authors': 'Difan Deng, Marius Lindauer', 'link': 'https://arxiv.org/abs/2502.13251', 'abstract': "We present Neural Attention Search (NAtS), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. This approach can efficiently reduce the KV cache sizes required by transformer-based models during inference and thus reduce inference costs. In this paper, we design a search space that contains three token types: (i) Global Tokens will be preserved and queried by all the following tokens. (ii) Local Tokens survive until the next global token appears. (iii) Sliding Window Tokens have an impact on the inference of a fixed size of the next following tokens. Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that NAtS can efficiently reduce the KV cache size required for the models while maintaining the models' performance.", 'abstract_zh': '基于神经注意力的搜索框架（NAtS）：自动评估序列中每个令牌的重要性并确定是否可以在若干步后删除对应的令牌', 'title_zh': '神经注意力搜索'}
{'arxiv_id': 'arXiv:2502.13248', 'title': 'Communication Strategy on Macro-and-Micro Traffic State in Cooperative Deep Reinforcement Learning for Regional Traffic Signal Control', 'authors': 'Hankang Gu, Shangbo Wang, Dongyao Jia, Yuli Zhang, Yanrong Luo, Guoqiang Mao, Jianping Wang, Eng Gee Lim', 'link': 'https://arxiv.org/abs/2502.13248', 'abstract': 'Adaptive Traffic Signal Control (ATSC) has become a popular research topic in intelligent transportation systems. Regional Traffic Signal Control (RTSC) using the Multi-agent Deep Reinforcement Learning (MADRL) technique has become a promising approach for ATSC due to its ability to achieve the optimum trade-off between scalability and optimality. Most existing RTSC approaches partition a traffic network into several disjoint regions, followed by applying centralized reinforcement learning techniques to each region. However, the pursuit of cooperation among RTSC agents still remains an open issue and no communication strategy for RTSC agents has been investigated. In this paper, we propose communication strategies to capture the correlation of micro-traffic states among lanes and the correlation of macro-traffic states among intersections. We first justify the evolution equation of the RTSC process is Markovian via a system of store-and-forward queues. Next, based on the evolution equation, we propose two GAT-Aggregated (GA2) communication modules--GA2-Naive and GA2-Aug to extract both intra-region and inter-region correlations between macro and micro traffic states. While GA2-Naive only considers the movements at each intersection, GA2-Aug also considers the lane-changing behavior of vehicles. Two proposed communication modules are then aggregated into two existing novel RTSC frameworks--RegionLight and Regional-DRL. Experimental results demonstrate that both GA2-Naive and GA2-Aug effectively improve the performance of existing RTSC frameworks under both real and synthetic scenarios. Hyperparameter testing also reveals the robustness and potential of our communication modules in large-scale traffic networks.', 'abstract_zh': '自适应交通信号控制（ATSC）已成为智能交通运输系统中的一个热门研究课题。基于多Agent深度强化学习（MADRL）的区域交通信号控制（RTSC）已成为ATSC的一个有前途的方法，由于其在可扩展性和优化性之间的最优权衡能力。现有的大多数RTSC方法将交通网络划分为若干个不相交区域，并在同一区域内应用集中式强化学习技术。然而，RTSC代理之间的合作追求仍然是一个开放的问题，尚未研究RTSC代理的通信策略。在本文中，我们提出了通信策略来捕捉车道上的微观交通状态之间的相关性以及交叉口之间的宏观交通状态之间的相关性。我们首先通过具有存储转发队列的系统证明了RTSC过程的演化方程是马尔可夫的。基于演化方程，我们提出了一种GAT-聚合（GA2）通信模块——GA2-Naive和GA2-Aug，以提取区域内的和区域间的宏观和微观交通状态的相关性。而GA2-Naive仅考虑交叉口的移动，GA2-Aug还考虑了车辆的车道变换行为。随后，提出的两个通信模块被整合到两个现有的新RTSC框架——RegionLight和Regional-DRL中。实验结果表明，GA2-Naive和GA2-Aug在实际和合成场景中均能有效提高现有RTSC框架的性能。超参数测试还揭示了我们提出的通信模块在大规模交通网络中的稳健性和潜力。', 'title_zh': '宏微观交通状态的通信策略在合作深度强化学习区域交通信号控制中的应用'}
{'arxiv_id': 'arXiv:2502.13234', 'title': 'MotionMatcher: Motion Customization of Text-to-Video Diffusion Models via Motion Feature Matching', 'authors': 'Yen-Siang Wu, Chi-Pin Huang, Fu-En Yang, Yu-Chiang Frank Wang', 'link': 'https://arxiv.org/abs/2502.13234', 'abstract': 'Text-to-video (T2V) diffusion models have shown promising capabilities in synthesizing realistic videos from input text prompts. However, the input text description alone provides limited control over the precise objects movements and camera framing. In this work, we tackle the motion customization problem, where a reference video is provided as motion guidance. While most existing methods choose to fine-tune pre-trained diffusion models to reconstruct the frame differences of the reference video, we observe that such strategy suffer from content leakage from the reference video, and they cannot capture complex motion accurately. To address this issue, we propose MotionMatcher, a motion customization framework that fine-tunes the pre-trained T2V diffusion model at the feature level. Instead of using pixel-level objectives, MotionMatcher compares high-level, spatio-temporal motion features to fine-tune diffusion models, ensuring precise motion learning. For the sake of memory efficiency and accessibility, we utilize a pre-trained T2V diffusion model, which contains considerable prior knowledge about video motion, to compute these motion features. In our experiments, we demonstrate state-of-the-art motion customization performances, validating the design of our framework.', 'abstract_zh': '文本到视频（T2V）扩散模型展示了从输入文本提示合成逼真视频的潜力。然而，仅有的文本描述输入对精确对象运动和相机构图的控制有限。在本文中，我们解决了一种运动定制问题，提供了一个参考视频作为运动指导。尽管大多数现有方法选择微调预训练的扩散模型以重建参考视频的帧差异，但我们发现这种策略会从参考视频中泄露内容，并且无法准确捕捉复杂运动。为了解决这一问题，我们提出了一种运动匹配器（MotionMatcher）运动定制框架，该框架在特征级别微调预训练的T2V扩散模型。与其使用像素级目标，MotionMatcher比较高层次的空间-时间运动特征来微调扩散模型，确保精确的运动学习。为提高内存效率和可获得性，我们利用了一个含有大量视频运动先验知识的预训练T2V扩散模型来计算这些运动特征。在我们的实验中，我们展示了最先进的运动定制性能，验证了我们框架的设计。', 'title_zh': 'MotionMatcher：通过运动特征匹配的文本到视频扩散模型的运动自化'}
{'arxiv_id': 'arXiv:2502.13233', 'title': 'SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?', 'authors': 'Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, Ninghao Liu', 'link': 'https://arxiv.org/abs/2502.13233', 'abstract': "Large Language Models (LLMs) have shown remarkable capabilities in general domains but often struggle with tasks requiring specialized knowledge. Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve external information from static knowledge bases, which can be outdated or incomplete, missing fine-grained clinical details essential for accurate medical question answering. In this work, we propose SearchRAG, a novel framework that overcomes these limitations by leveraging real-time search engines. Our method employs synthetic query generation to convert complex medical questions into search-engine-friendly queries and utilizes uncertainty-based knowledge selection to filter and incorporate the most relevant and informative medical knowledge into the LLM's input. Experimental results demonstrate that our method significantly improves response accuracy in medical question answering tasks, particularly for complex questions requiring detailed and up-to-date knowledge.", 'abstract_zh': '大规模语言模型（LLMs）在通用领域展示了出色的能力，但在需要专业领域知识的任务上常常表现不佳。传统的检索增强生成（RAG）技术通常从静态知识库中检索外部信息，这些知识库可能过时或不完整，缺乏准确医疗问答所必需的细粒度临床细节。本文提出了一种名为SearchRAG的新框架，该框架通过利用实时搜索引擎来克服这些局限性。我们的方法采用合成查询生成将复杂的医疗问题转换为搜索引擎友好的查询，并利用基于不确定性的知识选择来筛选和整合最相关的、最有信息量的医疗知识以供LLM输入。实验结果表明，我们的方法在医疗问答任务中显著提高了响应准确性，特别是在需要详细和最新知识的复杂问题上。', 'title_zh': 'SearchRAG: 搜索引擎能助力基于大语言模型的医疗问答吗？'}
{'arxiv_id': 'arXiv:2502.13228', 'title': 'Conformal Prediction as Bayesian Quadrature', 'authors': 'Jake C. Snell, Thomas L. Griffiths', 'link': 'https://arxiv.org/abs/2502.13228', 'abstract': 'As machine learning-based prediction systems are increasingly used in high-stakes situations, it is important to understand how such predictive models will perform upon deployment. Distribution-free uncertainty quantification techniques such as conformal prediction provide guarantees about the loss black-box models will incur even when the details of the models are hidden. However, such methods are based on frequentist probability, which unduly limits their applicability. We revisit the central aspects of conformal prediction from a Bayesian perspective and thereby illuminate the shortcomings of frequentist guarantees. We propose a practical alternative based on Bayesian quadrature that provides interpretable guarantees and offers a richer representation of the likely range of losses to be observed at test time.', 'abstract_zh': '基于机器学习的预测系统在高风险情况下被广泛应用后，了解其部署后的性能至关重要。分布自由的不确定性量化技术，如 conformal prediction，可以在模型细节隐藏的情况下提供关于黑盒模型损失的保证。然而，这些方法基于频率主义概率，限制了其应用范围。我们从贝叶斯视角重访 conformal prediction 的核心方面，并由此阐明频率主义保证的不足之处。我们提出了一种基于贝叶斯 quadrature 的实用替代方案，该方案提供了可解释的保证，并能更好地表示测试时可能出现的损失范围。', 'title_zh': '齐性预测作为贝叶斯 quadrature'}
{'arxiv_id': 'arXiv:2502.13221', 'title': 'Two Tickets are Better than One: Fair and Accurate Hiring Under Strategic LLM Manipulations', 'authors': 'Lee Cohen, Jack Hsieh, Connie Hong, Judy Hanwen Shen', 'link': 'https://arxiv.org/abs/2502.13221', 'abstract': "In an era of increasingly capable foundation models, job seekers are turning to generative AI tools to enhance their application materials. However, unequal access to and knowledge about generative AI tools can harm both employers and candidates by reducing the accuracy of hiring decisions and giving some candidates an unfair advantage. To address these challenges, we introduce a new variant of the strategic classification framework tailored to manipulations performed using large language models, accommodating varying levels of manipulations and stochastic outcomes. We propose a ``two-ticket'' scheme, where the hiring algorithm applies an additional manipulation to each submitted resume and considers this manipulated version together with the original submitted resume. We establish theoretical guarantees for this scheme, showing improvements for both the fairness and accuracy of hiring decisions when the true positive rate is maximized subject to a no false positives constraint. We further generalize this approach to an $n$-ticket scheme and prove that hiring outcomes converge to a fixed, group-independent decision, eliminating disparities arising from differential LLM access. Finally, we empirically validate our framework and the performance of our two-ticket scheme on real resumes using an open-source resume screening tool.", 'abstract_zh': '在基础模型能力日益强大的时代，求职者开始利用生成式AI工具来增强他们的申请材料。然而，生成式AI工具获取的不平等和知识的不均衡可能导致雇主和求职者双方受损，降低招聘决策的准确性，并给予部分求职者不公平的优势。为应对这些挑战，我们提出了一种针对使用大规模语言模型进行操控的战略分类框架的新变体，该框架能够适应不同水平的操控和不确定性结果。我们提出了一种“双票”方案，其中招聘算法对每个提交的简历进行额外的操控，并将操控后的版本与原始提交简历一同考虑。我们为该方案建立了理论保证，证明在真阳性率最大化且无假阳性约束的情况下，该方案可以改进招聘决策的公平性和准确性。我们进一步将该方法推广到“n票”方案，并证明招聘结果将收敛到一个固定的、与群体无关的决策，消除由于不同访问大规模语言模型导致的差异。最后，我们使用一个开源简历筛选工具，在实际简历上 empirically 验证了我们框架和“双票”方案的性能。', 'title_zh': '两张票优于一张票：在战略性的LLM操纵下的公平和准确招聘'}
{'arxiv_id': 'arXiv:2502.13207', 'title': 'Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation', 'authors': 'Giorgio Franceschelli, Mirco Musolesi', 'link': 'https://arxiv.org/abs/2502.13207', 'abstract': 'Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results. Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We propose using our score as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments in poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.', 'abstract_zh': '尽管大型语言模型在创造性任务中的应用日益增多，但其输出往往缺乏多样性。常见的解决方案，如在更高温度下采样，可能会牺牲结果的质量。借鉴信息论原理，我们提出一种基于上下文的评分方法，以定量评估价值和原创性。该评分方法激励准确性并符合请求，同时促进远离已学习分布的差异。我们建议将我们的评分方法作为强化学习框架中的奖励，以优化大型语言模型的性能。通过诗歌生成和数学问题解决实验验证了我们的策略，结果显示该方法可以提升生成解决方案的价值和原创性。', 'title_zh': '跳出（灰色）框架：基于上下文的评分方法评估神经文本生成的价值与原创性'}
{'arxiv_id': 'arXiv:2502.13200', 'title': 'Learning To Explore With Predictive World Model Via Self-Supervised Learning', 'authors': 'Alana Santana, Paula P. Costa, Esther L. Colombini', 'link': 'https://arxiv.org/abs/2502.13200', 'abstract': 'Autonomous artificial agents must be able to learn behaviors in complex environments without humans to design tasks and rewards. Designing these functions for each environment is not feasible, thus, motivating the development of intrinsic reward functions. In this paper, we propose using several cognitive elements that have been neglected for a long time to build an internal world model for an intrinsically motivated agent. Our agent performs satisfactory iterations with the environment, learning complex behaviors without needing previously designed reward functions. We used 18 Atari games to evaluate what cognitive skills emerge in games that require reactive and deliberative behaviors. Our results show superior performance compared to the state-of-the-art in many test cases with dense and sparse rewards.', 'abstract_zh': '自主人工代理必须能够在没有人类设计任务和奖励的情况下，在复杂环境中学习行为。由于为每个环境设计这些功能是不可行的，因此推动了内在奖励函数的开发。本文我们提出使用长期被忽略的认知元素来为内在动机代理构建内部世界模型。我们的代理能够在与环境的满意交互中学习复杂行为，无需先前设计的奖励函数。我们使用18个雅达利游戏评估了所需反应性和深思熟虑行为游戏中涌现的认知技能。实验结果表明，与最先进的方法相比，在稠密和稀疏奖励的情况下，我们的方法在多种测试案例中表现出更优的性能。', 'title_zh': '基于自监督学习的预测世界模型探索学习'}
{'arxiv_id': 'arXiv:2502.13199', 'title': 'The Role of GitHub Copilot on Software Development: A Perspec-tive on Productivity, Security, Best Practices and Future Directions', 'authors': 'Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, Akhil Dusi', 'link': 'https://arxiv.org/abs/2502.13199', 'abstract': "GitHub Copilot is transforming software development by automating tasks and boosting productivity through AI-driven code generation. In this paper, we con-duct a literature survey to synthesize insights on Copilot's impact on productivity and security. We review academic journal databases, industry reports, and official docu-mentation to highlight key findings and challenges. While Copilot accelerates coding and prototyping, concerns over security vulnerabilities and intellectual property risks persist. Drawing from the literature, we provide a perspective on best practices and future directions for responsible AI adoption in software engineering, offering action-able insights for developers and organizations to integrate Copilot effectively while maintaining high standards of quality and security.", 'abstract_zh': 'GitHub Copilot正通过AI驱动的代码生成自动化任务并提升软件开发效率与安全性：文献综述与负责任AI采用的展望', 'title_zh': 'GitHub Copilot在软件开发中的作用：从生产率、安全性、最佳实践及未来方向的角度探讨'}
{'arxiv_id': 'arXiv:2502.13198', 'title': 'Enhancing Machine Learning Performance through Intelligent Data Quality Assessment: An Unsupervised Data-centric Framework', 'authors': 'Manal Rahal, Bestoun S. Ahmed, Gergely Szabados, Torgny Fornstedt, Jorgen Samuelsson', 'link': 'https://arxiv.org/abs/2502.13198', 'abstract': 'Poor data quality limits the advantageous power of Machine Learning (ML) and weakens high-performing ML software systems. Nowadays, data are more prone to the risk of poor quality due to their increasing volume and complexity. Therefore, tedious and time-consuming work goes into data preparation and improvement before moving further in the ML pipeline. To address this challenge, we propose an intelligent data-centric evaluation framework that can identify high-quality data and improve the performance of an ML system. The proposed framework combines the curation of quality measurements and unsupervised learning to distinguish high- and low-quality data. The framework is designed to integrate flexible and general-purpose methods so that it is deployed in various domains and applications. To validate the outcomes of the designed framework, we implemented it in a real-world use case from the field of analytical chemistry, where it is tested on three datasets of anti-sense oligonucleotides. A domain expert is consulted to identify the relevant quality measurements and evaluate the outcomes of the framework. The results show that the quality-centric data evaluation framework identifies the characteristics of high-quality data that guide the conduct of efficient laboratory experiments and consequently improve the performance of the ML system.', 'abstract_zh': '低质量数据限制了机器学习的优势并削弱了高性能机器学习软件系统的能力。随着数据量和复杂性的增加，数据更容易受到低质量风险的影响。因此，在进入机器学习管道之前需要进行繁琐且耗时的数据准备和改进工作。为应对这一挑战，我们提出了一种智能数据为中心的评价框架，以识别高质量数据并提升机器学习系统的性能。该框架结合了质量度量的策划和无监督学习，以区分高质量和低质量数据。该框架设计灵活且通用，以便在各种领域和应用中部署。为了验证所设计框架的结果，我们在分析化学领域实施了一个实际应用场景，并在三种反义寡核苷酸数据集中对其进行测试。领域专家参与识别相关质量度量并评估框架的结果。结果表明，质量为中心的数据评价框架识别出了高质量数据的特征，指导高效的实验室实验，并进而提升了机器学习系统的性能。', 'title_zh': '通过智能数据质量评估提升机器学习性能：一种无监督的数据为中心框架'}
{'arxiv_id': 'arXiv:2502.13194', 'title': 'Conditional Max-Sum for Asynchronous Multiagent Decision Making', 'authors': 'Dimitrios Troullinos, Georgios Chalkiadakis, Ioannis Papamichail, Markos Papageorgiou', 'link': 'https://arxiv.org/abs/2502.13194', 'abstract': 'In this paper we present a novel approach for multiagent decision making in dynamic environments based on Factor Graphs and the Max-Sum algorithm, considering asynchronous variable reassignments and distributed message-passing among agents. Motivated by the challenging domain of lane-free traffic where automated vehicles can communicate and coordinate as agents, we propose a more realistic communication framework for Factor Graph formulations that satisfies the above-mentioned restrictions, along with Conditional Max-Sum: an extension of Max-Sum with a revised message-passing process that is better suited for asynchronous settings. The overall application in lane-free traffic can be viewed as a hybrid system where the Factor Graph formulation undertakes the strategic decision making of vehicles, that of desired lateral alignment in a coordinated manner; and acts on top of a rule-based method we devise that provides a structured representation of the lane-free environment for the factors, while also handling the underlying control of vehicles regarding core operations and safety. Our experimental evaluation showcases the capabilities of the proposed framework in problems with intense coordination needs when compared to a domain-specific baseline without communication, and an increased adeptness of Conditional Max-Sum with respect to the standard algorithm.', 'abstract_zh': '基于因子图和Max-Sum算法的异步变量重赋值及分布式消息传递的多agent动态环境决策方法：适用于无车道交通的条件Max-Sum框架', 'title_zh': '异步多agent决策制定的条件最大化算法'}
{'arxiv_id': 'arXiv:2502.13191', 'title': 'On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis', 'authors': 'Junyi Guan, Abhijith Sharma, Chong Tian, Salem Lahlou', 'link': 'https://arxiv.org/abs/2502.13191', 'abstract': 'Spiking Neural Networks (SNNs) are increasingly explored for their energy efficiency and robustness in real-world applications, yet their privacy risks remain largely unexamined. In this work, we investigate the susceptibility of SNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an adversary attempts to determine whether a given sample was part of the training dataset. While prior work suggests that SNNs may offer inherent robustness due to their discrete, event-driven nature, we find that its resilience diminishes as latency (T) increases. Furthermore, we introduce an input dropout strategy under black box setting, that significantly enhances membership inference in SNNs. Our findings challenge the assumption that SNNs are inherently more secure, and even though they are expected to be better, our results reveal that SNNs exhibit privacy vulnerabilities that are equally comparable to Artificial Neural Networks (ANNs). Our code is available at this https URL.', 'abstract_zh': '脉冲神经网络（SNNs）在实际应用中因能效高和健壮性好而受到越来越多的关注，但其隐私风险尚未得到充分研究。本文探讨了SNNs对成员推理攻击（MIAs）的敏感性——一种主要的隐私威胁，攻击者试图确定给定样本是否属于训练数据集。虽然以往的研究表明，由于SNNs的离散和事件驱动的特性，它们可能具有内在的鲁棒性，但我们的研究发现，随着延迟（T）的增加，其抗攻击能力会减弱。此外，我们还在黑箱环境中引入了一种输入丢弃策略，显著增强了SNNs的成员推理能力。我们的研究挑战了SNNs本就更安全的假设，并揭示出尽管SNNs预期更好，但它们仍然存在与人工神经网络（ANNs）相当的隐私漏洞。我们的代码可在以下网址获取。', 'title_zh': '关于脉冲神经网络的隐私风险：一种成员推断分析'}
{'arxiv_id': 'arXiv:2502.13189', 'title': 'MoBA: Mixture of Block Attention for Long-Context LLMs', 'authors': 'Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, Jiezhong Qiu', 'link': 'https://arxiv.org/abs/2502.13189', 'abstract': "Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored.\nIn this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at this https URL.", 'abstract_zh': '扩展有效的上下文长度是推动大型语言模型（LLMs）向通用人工智能（AGI）发展的关键。然而，传统注意力机制中固有的计算复杂度平方级增长带来了难以承受的开销。现有方法要么引入强偏置结构，如sink或窗口注意力，这些结构具有任务特定性，要么从根本上将注意力机制修改为线性近似，这些方法在复杂推理任务中的性能尚未得到充分探索。\n在本文中，我们提出了一种遵循“少结构”原则的解决方案，允许模型自主决定注意力的关注点，而不是引入预定义的偏置。我们引入了块注意力混合（MoBA）这一创新方法，将专家混合（MoE）的原则应用于注意力机制。这一新的架构在处理长上下文任务时表现出色，并且提供了一个关键优势：能够无缝切换到全注意力和稀疏注意力之间，提高效率而不牺牲性能。MoBA已经在支持Kimi的长上下文请求中部署，并展示了在LLMs中高效注意力计算方面的显著进展。我们的代码可在以下链接获取：this https URL。', 'title_zh': 'MoBA：长上下文语言模型的块注意力混合模型'}
{'arxiv_id': 'arXiv:2502.13187', 'title': 'A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models', 'authors': 'Longchao Da, Justin Turnau, Thirulogasankar Pranav Kutralingam, Alvaro Velasquez, Paulo Shakarian, Hua Wei', 'link': 'https://arxiv.org/abs/2502.13187', 'abstract': 'Deep Reinforcement Learning (RL) has been explored and verified to be effective in solving decision-making tasks in various domains, such as robotics, transportation, recommender systems, etc. It learns from the interaction with environments and updates the policy using the collected experience. However, due to the limited real-world data and unbearable consequences of taking detrimental actions, the learning of RL policy is mainly restricted within the simulators. This practice guarantees safety in learning but introduces an inevitable sim-to-real gap in terms of deployment, thus causing degraded performance and risks in execution. There are attempts to solve the sim-to-real problems from different domains with various techniques, especially in the era with emerging techniques such as large foundations or language models that have cast light on the sim-to-real. This survey paper, to the best of our knowledge, is the first taxonomy that formally frames the sim-to-real techniques from key elements of the Markov Decision Process (State, Action, Transition, and Reward). Based on the framework, we cover comprehensive literature from the classic to the most advanced methods including the sim-to-real techniques empowered by foundation models, and we also discuss the specialties that are worth attention in different domains of sim-to-real problems. Then we summarize the formal evaluation process of sim-to-real performance with accessible code or benchmarks. The challenges and opportunities are also presented to encourage future exploration of this direction. We are actively maintaining a to include the most up-to-date sim-to-real research outcomes to help the researchers in their work.', 'abstract_zh': '深强化学习的仿真到现实问题：马尔可夫决策过程关键元素的分类综述', 'title_zh': '基础模型视角下的RL中从模拟到现实方法综述：进展、前景与挑战'}
{'arxiv_id': 'arXiv:2502.13185', 'title': 'CondensNet: Enabling stable long-term climate simulations via hybrid deep learning models with adaptive physical constraints', 'authors': 'Xin Wang, Juntao Yang, Jeff Adie, Simon See, Kalli Furtado, Chen Chen, Troy Arcomano, Romit Maulik, Gianmarco Mengaldo', 'link': 'https://arxiv.org/abs/2502.13185', 'abstract': "Accurate and efficient climate simulations are crucial for understanding Earth's evolving climate. However, current general circulation models (GCMs) face challenges in capturing unresolved physical processes, such as cloud and convection. A common solution is to adopt cloud resolving models, that provide more accurate results than the standard subgrid parametrisation schemes typically used in GCMs. However, cloud resolving models, also referred to as super paramtetrizations, remain computationally prohibitive. Hybrid modeling, which integrates deep learning with equation-based GCMs, offers a promising alternative but often struggles with long-term stability and accuracy issues. In this work, we find that water vapor oversaturation during condensation is a key factor compromising the stability of hybrid models. To address this, we introduce CondensNet, a novel neural network architecture that embeds a self-adaptive physical constraint to correct unphysical condensation processes. CondensNet effectively mitigates water vapor oversaturation, enhancing simulation stability while maintaining accuracy and improving computational efficiency compared to super parameterization schemes.\nWe integrate CondensNet into a GCM to form PCNN-GCM (Physics-Constrained Neural Network GCM), a hybrid deep learning framework designed for long-term stable climate simulations in real-world conditions, including ocean and land. PCNN-GCM represents a significant milestone in hybrid climate modeling, as it shows a novel way to incorporate physical constraints adaptively, paving the way for accurate, lightweight, and stable long-term climate simulations.", 'abstract_zh': '准确高效的气候模拟对于理解地球 evolving气候至关重要。然而，当前的地球系统模型（GCMs）在捕捉未解决的物理过程，如云和对流方面面临挑战。一种常见的解决方案是采用云解析模型，这些模型比通常在GCMs中使用的亚网格参数化方案提供更准确的结果。然而，云解析模型也被称为超参数化模型，在计算上仍具有挑战性。将深度学习与基于方程的GCMs相结合的综合模型提供了一种有前途的替代方案，但通常会遇到长期稳定性和准确性问题。在本研究中，我们发现凝结过程中水汽过饱和是综合模型不稳定的关键因素。为了解决这一问题，我们引入了CondensNet，这是一种新型神经网络架构，嵌入了自我适应的物理约束以纠正不物理的凝结过程。CondensNet有效地缓解了水汽过饱和问题，增强了模拟稳定性并保持了准确性，同时相比超参数化方案提高了计算效率。我们将CondensNet整合到GCM中，形成了PCNN-GCM（物理约束神经网络GCM），这是一种旨在适应现实世界条件（包括海洋和陆地）的长期稳定气候模拟的综合深度学习框架。PCNN-GCM代表了综合气候模型的一个重要里程碑，因为它展示了如何适应性地引入物理约束，为实现精确、轻量级和长期稳定的气候模拟铺平了道路。', 'title_zh': 'CondensNet：通过具有自适应物理约束的混合深度学习模型实现稳定的长期气候模拟'}
{'arxiv_id': 'arXiv:2502.13181', 'title': 'RingFormer: Rethinking Recurrent Transformer with Adaptive Level Signals', 'authors': 'Jaemu Heo, Eldor Fozilov, Hyunmin Song, Taehwan Kim', 'link': 'https://arxiv.org/abs/2502.13181', 'abstract': 'Transformers have achieved great success in effectively processing sequential data such as text. Their architecture consisting of several attention and feedforward blocks can model relations between elements of a sequence in parallel manner, which makes them very efficient to train and effective in sequence modeling. Even though they have shown strong performance in processing sequential data, the size of their parameters is considerably larger when compared to other architectures such as RNN and CNN based models. Therefore, several approaches have explored parameter sharing and recurrence in Transformer models to address their computational demands. However, such methods struggle to maintain high performance compared to the original transformer model. To address this challenge, we propose our novel approach, RingFormer, which employs one Transformer layer that processes input repeatedly in a circular, ring-like manner, while utilizing low-rank matrices to generate input-dependent level signals. This allows us to reduce the model parameters substantially while maintaining high performance in a variety of tasks such as translation and image classification, as validated in the experiments.', 'abstract_zh': 'RingFormer：一种循环处理的变压器模型', 'title_zh': '环形变换器：重新思考具有自适应层级信号的循环变换器'}
{'arxiv_id': 'arXiv:2502.13180', 'title': 'Uncertain Multi-Objective Recommendation via Orthogonal Meta-Learning Enhanced Bayesian Optimization', 'authors': 'Hongxu Wang, Zhu Sun, Yingpeng Du, Lu Zhang, Tiantian He, Yew-Soon Ong', 'link': 'https://arxiv.org/abs/2502.13180', 'abstract': 'Recommender systems (RSs) play a crucial role in shaping our digital interactions, influencing how we access and engage with information across various domains. Traditional research has predominantly centered on maximizing recommendation accuracy, often leading to unintended side effects such as echo chambers and constrained user experiences. Drawing inspiration from autonomous driving, we introduce a novel framework that categorizes RS autonomy into five distinct levels, ranging from basic rule-based accuracy-driven systems to behavior-aware, uncertain multi-objective RSs - where users may have varying needs, such as accuracy, diversity, and fairness. In response, we propose an approach that dynamically identifies and optimizes multiple objectives based on individual user preferences, fostering more ethical and intelligent user-centric recommendations. To navigate the uncertainty inherent in multi-objective RSs, we develop a Bayesian optimization (BO) framework that captures personalized trade-offs between different objectives while accounting for their uncertain interdependencies. Furthermore, we introduce an orthogonal meta-learning paradigm to enhance BO efficiency and effectiveness by leveraging shared knowledge across similar tasks and mitigating conflicts among objectives through the discovery of orthogonal information. Finally, extensive empirical evaluations demonstrate the effectiveness of our method in optimizing uncertain multi-objectives for individual users, paving the way for more adaptive and user-focused RSs.', 'abstract_zh': '推荐系统在塑造我们的数字互动方面发挥着关键作用，影响我们在各个领域获取和互动信息的方式。传统研究主要集中在最大化推荐准确性上，这往往导致回声室效应和用户使用体验受限等意外后果。受到自动驾驶的启发，我们提出了一种新的框架，将推荐系统的自主性分为五个不同的层级，从基于基本规则和准确性驱动的系统到关注用户行为、具有不确定性的多目标推荐系统——其中用户的需求可能不同，例如准确性、多样性和平等性。为响应这一挑战，我们提出了一种基于个体用户偏好的动态识别和优化多目标的方法，促进更具伦理性和智能性的以用户为中心的推荐。为应对多目标推荐系统中的不确定性，我们开发了一种贝叶斯优化（BO）框架，既能捕捉不同目标之间的个性化权衡，又能考虑它们之间的不确定关联性。此外，我们引入了一种正交元学习范式，通过利用相似任务中的共享知识来提高贝叶斯优化效率和效果，并通过发现正交信息来缓解目标间的冲突。最后，广泛的实证评估表明，我们的方法在优化个体用户的不确定多目标方面效果显著，为更具适应性和用户导向的推荐系统的开发铺平了道路。', 'title_zh': '不确定多目标推荐通过正交元学习增强的贝叶斯优化'}
{'arxiv_id': 'arXiv:2502.13179', 'title': 'PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models', 'authors': 'Jiaqi Zhao, Miao Zhang, Ming Wang, Yuzhang Shang, Kaihao Zhang, Weili Guan, Yaowei Wang, Min Zhang', 'link': 'https://arxiv.org/abs/2502.13179', 'abstract': 'Large Language Models (LLMs) suffer severe performance degradation when facing extremely low-bit (sub 2-bit) quantization. Several existing sub 2-bit post-training quantization (PTQ) methods utilize a mix-precision scheme by leveraging an unstructured fine-grained mask to explicitly distinguish salient weights, while which introduces an extra 1-bit or more per weight. To explore the real limit of PTQ, we propose an extremely low-bit PTQ method called PTQ1.61, which enables weight quantization to 1.61-bit for the first time. Specifically, we first introduce a one-dimensional structured mask with negligibly additional 0.0002-bit per weight based on input activations from the perspective of reducing the upper bound of quantization error to allocate corresponding salient weight channels to 4-bit. For non-salient channels binarization, an efficient block-wise scaling factors optimization framework is then presented to take implicit row-wise correlations and angular biases into account. Different from prior works that concentrate on adjusting quantization methodologies, we further propose a novel paradigm called quantization preprocessing, where we argue that transforming the weight distribution of the pretrained model before quantization can alleviate the difficulty in per-channel extremely low-bit PTQ. Extensive experiments indicate our PTQ1.61 achieves state-of-the-art performance in extremely low-bit quantization. Codes are available at this https URL.', 'abstract_zh': '极大低比特（低于2比特）量化下大语言模型（LLMs）的性能严重下降。现有的一些低于2比特后训练量化（PTQ）方法通过利用非结构化的细粒度掩码引入混精度方案，明确区分重要权重，但每个权重引入了额外的1比特或多比特。为了探索PTQ的真实极限，我们提出了一种称为PTQ1.61的极度低比特PTQ方法，这使得权重量化首次达到1.61比特。具体来说，我们首先从降低量化误差上限的角度出发，引入了一维结构化掩码，基于输入激活，每个权重仅附加忽略不计的0.0002比特，将重要权重通道分配给4比特。对于非重要通道的二值化，我们提出了一种高效的分块标量因子优化框架，考虑了行内隐式的相关性和角度偏差。不同于专注于调整量化方法的先前工作，我们进一步提出了一种新颖的预处理量化范式，即在量化前转换预训练模型的权重分布，以缓解单通道极度低比特量化中的困难。大量实验表明，我们的PTQ1.61在极度低比特量化中达到了最先进的性能。相关代码可在以下链接获取。', 'title_zh': 'PTQ1.61: 探索极低位宽后训练量化方法的真正极限以应用于大型语言模型'}
{'arxiv_id': 'arXiv:2502.13178', 'title': 'Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis', 'authors': 'Jiaqi Zhao, Ming Wang, Miao Zhang, Yuzhang Shang, Xuebo Liu, Yaowei Wang, Min Zhang, Liqiang Nie', 'link': 'https://arxiv.org/abs/2502.13178', 'abstract': 'Post-training Quantization (PTQ) technique has been extensively adopted for large language models (LLMs) compression owing to its efficiency and low resource requirement. However, current research lacks a in-depth analysis of the superior and applicable scenarios of each PTQ strategy. In addition, existing algorithms focus primarily on performance, overlooking the trade-off among model size, performance, and quantization bitwidth. To mitigate these confusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly, in order to support our benchmark, we propose a comprehensive taxonomy for existing mainstream methods by scrutinizing their computational strategies (e.g., optimization-based, compensation-based, etc.). Then, we conduct extensive experiments with the baseline within each class, covering models with various sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1), architectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and VILA1.5) on a wide range of evaluation this http URL comparative analysis on the results, we summarize the superior of each PTQ strategy and modelsize-bitwidth trade-off considering the performance. For example, our benchmark reveals that compensation-based technique demonstrates outstanding cross-architecture robustness and extremely low-bit PTQ for ultra large models should be reexamined. Finally, we further accordingly claim that a practical combination of compensation and other PTQ strategy can achieve SOTA various robustness. We believe that our benchmark will provide valuable recommendations for the deployment of LLMs and future research on PTQ approaches.', 'abstract_zh': 'Post-Training Quantization技术在大规模语言模型压缩中的广泛应用得益于其高效性和低资源需求。然而，现有研究缺乏对每种Post-Training Quantization策略优越且适用场景的深入分析。此外，现有算法主要关注性能，忽略了模型规模、性能和量化位宽之间的权衡。为缓解这些困惑，我们在本文中提供了一个新的大规模语言模型Post-Training Quantization基准。首先，为了支持我们的基准，我们通过对现有主流方法的计算策略进行详细审查（例如，基于优化的、基于补偿的等），提出了一种全面的分类体系。然后，我们在每个类别中采用了基准方法，涵盖了不同规模（7B-70B）、量化位宽、训练级别（LLaMA1/2/3/3.1）、架构（Mixtral、DeepSeekMoE和Mamba）和模态（LLaVA1.5和VILA1.5）的一系列广泛评估。通过对比分析结果，我们总结了每种Post-Training Quantization策略的优点和模型规模-量化位宽权衡，特别是在性能方面的考虑。最后，我们进一步提出，补偿策略与其他Post-Training Quantization策略的结合可以在多种鲁棒性方面达到最佳效果。我们相信，我们的基准将为大规模语言模型的部署和未来Post-Training Quantization方法的研究提供有价值的建议。', 'title_zh': '在大规模语言模型中基准测试训练后量化：全面分类、统一评估与比较分析'}
{'arxiv_id': 'arXiv:2502.13177', 'title': 'KL Penalty Control via Perturbation for Direct Preference Optimization', 'authors': 'Sangkyu Lee, Janghoon Han, Hosung Song, Stanley Jungkyu Choi, Honglak Lee, Youngjae Yu', 'link': 'https://arxiv.org/abs/2502.13177', 'abstract': 'Direct Preference Optimization (DPO) demonstrates the advantage of aligning a large language model with human preference using only an offline dataset. However, DPO has the limitation that the KL penalty, which prevents excessive deviation from the reference model, is static throughout the training process. Several methods try to turn this static KL penalty into a dynamic one, but no approach can adaptively assign different KL penalties for each preference pair. In this paper, we propose $\\varepsilon$-Direct Preference Optimization ($\\varepsilon$-DPO), which allows adaptive control of the KL penalty strength $\\beta$ for each preference pair. Specifically, $\\varepsilon$-DPO adaptively controls $\\beta$ for each preference pair based on the monotonicity of logits as a preference model under the perturbation of $\\beta$ during training by simply reusing the logit of the current policy and the reference policy. Experimental results show that $\\varepsilon$-DPO outperforms existing direct alignment algorithms and KL penalty relaxation methods on general chatbot benchmarks, highlighting the significance of adaptive KL penalty relaxation at the instance-level in DPO.', 'abstract_zh': 'ε-直接偏好优化（ε-DPO）：在DPO中实现偏奋试配的自适应KL惩罚强度', 'title_zh': 'KL正则化 penalty 控制 via 扰动方法用于直接偏好优化'}
{'arxiv_id': 'arXiv:2502.13176', 'title': 'BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference', 'authors': 'Ahmed Burak Gulhan, Krishna Teja Chitty-Venkata, Murali Emani, Mahmut Kandemir, Venkatram Vishwanath', 'link': 'https://arxiv.org/abs/2502.13176', 'abstract': 'In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches) are essential for reducing time complexity. However, they result in a linear increase in GPU memory as the context length grows. While recent work explores KV-cache eviction and compression policies to reduce memory usage, they often consider uniform KV-caches across all attention heads, leading to suboptimal performance. We introduce BaKlaVa, a method to allocate optimal memory for individual KV-caches across the model by estimating the importance of each KV-cache. Our empirical analysis demonstrates that not all KV-caches are equally critical for LLM performance. Using a one-time profiling approach, BaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our method on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression ratio while keeping baseline performance and delivering up to an order-of-magnitude accuracy improvement at higher compression levels.', 'abstract_zh': '大型语言模型（LLM）推理中，键值（KV）缓存（KV-caches）对于减少时间复杂性至关重要。然而，随着上下文长度的增长，它们会导致GPU内存线性增加。虽然最近的工作探索了KV缓存的淘汰和压缩策略以减少内存使用，但它们通常考虑所有注意力头上的均匀KV缓存，导致性能不佳。我们提出了BaKlaVa方法，通过估计每个KV缓存的重要性为模型中的各个KV缓存分配最优内存。我们的实证分析表明，并非所有KV缓存对LLM性能都同等关键。通过一次性 profilng 方法，BaKlaVa 为每个KV缓存分配了最优的内存预算。我们在LLaMA-3-8B和Qwen2.5-7B模型上评估了该方法，实现了高达70%的压缩比，同时保持基线性能，并在高度压缩水平上实现了数量级的准确率提升。', 'title_zh': 'BaKlaVa——预算分配的KV缓存用于长上下文推理'}
{'arxiv_id': 'arXiv:2502.13175', 'title': 'Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks', 'authors': 'Wenpeng Xing, Minghao Li, Mohan Li, Meng Han', 'link': 'https://arxiv.org/abs/2502.13175', 'abstract': 'Embodied AI systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where they encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest through sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and safety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of embodied AI systems. Most prior work either addresses general AI vulnerabilities or focuses on isolated aspects, lacking a dedicated and unified framework tailored to embodied AI. This survey fills this critical gap by: (1) categorizing vulnerabilities specific to embodied AI into exogenous (e.g., physical attacks, cybersecurity threats) and endogenous (e.g., sensor failures, software flaws) origins; (2) systematically analyzing adversarial attack paradigms unique to embodied AI, with a focus on their impact on perception, decision-making, and embodied interaction; (3) investigating attack vectors targeting large vision-language models (LVLMs) and large language models (LLMs) within embodied systems, such as jailbreak attacks and instruction misinterpretation; (4) evaluating robustness challenges in algorithms for embodied perception, decision-making, and task planning; and (5) proposing targeted strategies to enhance the safety and reliability of embodied AI systems. By integrating these dimensions, we provide a comprehensive framework for understanding the interplay between vulnerabilities and safety in embodied AI.', 'abstract_zh': '具身AI系统的体载智能系统，包括机器人和自动驾驶车辆，正越来越多地集成到现实世界应用中，它们面临着来自环境和系统层面因素的一系列漏洞。这些漏洞通过传感器欺骗、恶意攻击以及任务和运动规划失效等形式表现出来，对系统的鲁棒性和安全性构成了重大挑战。尽管已经有大量研究，但现有综述较少专门关注具身AI系统的独特安全和安全挑战。大多数先前的工作要么研究通用AI漏洞，要么仅关注孤立方面，缺乏一个专门针对具身AI的统一框架。本文综述填补了这一关键空白，通过以下方式：（1）将具身AI特有的漏洞分类为外生来源（如物理攻击、网络安全威胁）和内生来源（如传感器故障、软件缺陷）；（2）系统分析具身AI特有的攻击范式，重点关注其对感知、决策和具身交互的影响；（3）研究针对具身系统中大型视觉语言模型（LVLM）和大型语言模型（LLM）的攻击向量，如逃逸攻击和指令误解释；（4）评估具身感知、决策和任务规划算法的鲁棒性挑战；（5）提出针对性策略以增强具身AI系统的安全性和可靠性。通过整合这些维度，我们提供了一个综合框架，用于理解具身AI中漏洞与安全之间的相互作用。', 'title_zh': '面向鲁棒性和安全性的实体AI：脆弱性和攻击综述'}
{'arxiv_id': 'arXiv:2502.13174', 'title': 'Generative Topology Optimization: Exploring Diverse Solutions in Structural Design', 'authors': 'Andreas Radler, Eric Volkmann, Johannes Brandstetter, Arturs Berzins', 'link': 'https://arxiv.org/abs/2502.13174', 'abstract': 'Topology optimization (TO) is a family of computational methods that derive near-optimal geometries from formal problem descriptions. Despite their success, established TO methods are limited to generating single solutions, restricting the exploration of alternative designs. To address this limitation, we introduce Generative Topology Optimization (GenTO) - a data-free method that trains a neural network to generate structurally compliant shapes and explores diverse solutions through an explicit diversity constraint. The network is trained with a solver-in-the-loop, optimizing the material distribution in each iteration. The trained model produces diverse shapes that closely adhere to the design requirements. We validate GenTO on 2D and 3D TO problems. Our results demonstrate that GenTO produces more diverse solutions than any prior method while maintaining near-optimality and being an order of magnitude faster due to inherent parallelism. These findings open new avenues for engineering and design, offering enhanced flexibility and innovation in structural optimization.', 'abstract_zh': '生成式拓扑优化（GenTO）：一种数据驱动的方法', 'title_zh': '生成拓扑优化：在结构设计中探索多样化的解决方案'}
{'arxiv_id': 'arXiv:2502.13173', 'title': 'Thinking Preference Optimization', 'authors': 'Wang Yang, Hongye Jin, Jingfeng Yang, Vipin Chaudhary, Xiaotian Han', 'link': 'https://arxiv.org/abs/2502.13173', 'abstract': "Supervised Fine-Tuning (SFT) has been a go-to and effective method for enhancing long chain-of-thought (CoT) reasoning in relatively small LLMs by fine-tuning them with long CoT responses from larger LLMs. To continually improve reasoning abilities, we can either collect new high-quality long CoT reasoning SFT data or repeatedly train on existing SFT datasets. However, acquiring new long CoT SFT data is costly and limited, while repeated training often results in a performance plateau or decline. To further boost the performance with the SFT data, we propose Thinking Preference Optimization (ThinkPO), a simple yet effective post-SFT method that enhances long CoT reasoning without requiring new long CoT responses. Instead, ThinkPO utilizes readily available or easily obtainable short CoT reasoning responses as rejected answers and long CoT responses as chosen answers for the same question. It then applies direct preference optimization to encourage the model to favor longer reasoning outputs. Experiments show that ThinkPO further improves the reasoning performance of SFT-ed models, e.g. it increases math reasoning accuracy of SFT-ed models by 8.6% and output length by 25.9%. Notably, ThinkPO is capable of continually boosting the performance of the publicly distilled SFT model, e.g., increasing the official DeepSeek-R1-Distill-Qwen-7B's performance on MATH500 from 87.4% to 91.2%.", 'abstract_zh': '监督微调（SFT）已成为通过使用大型语言模型的长链式推理（CoT）响应来增强相对较小的语言模型的长链式推理能力的一种常用且有效的方法。为了不断改进推理能力，我们可以收集新的高质量长链式推理SFT数据，或者反复训练现有的SFT数据集。然而，获取新的长链式推理SFT数据成本高且受限，而反复训练往往会导致性能 plateau 或下降。为了进一步利用SFT数据提升性能，我们提出了一种简单且有效的后微调方法——推理偏好优化（ThinkPO），该方法可以在不需额外长链式推理响应的情况下增强长链式推理能力。ThinkPO 利用现成的或易于获取的短链式推理响应作为拒绝答案，并将长链式推理响应作为相同问题的选择答案。接着，它通过直接偏好优化鼓励模型偏好更长的推理输出。实验表明，ThinkPO 进一步提高了SFT模型的推理性能，例如，使SFT模型的数学推理准确率提高了8.6%，输出长度增加了25.9%。值得注意的是，ThinkPO 能够不断提升公开精简的SFT模型的性能，例如，将官方DeepSeek-R1-Distill-Qwen-7B在MATH500上的性能从87.4%提高到91.2%。', 'title_zh': '偏好优化思考'}
{'arxiv_id': 'arXiv:2502.13172', 'title': 'Unveiling Privacy Risks in LLM Agent Memory', 'authors': 'Bo Wang, Weiyi He, Pengfei He, Shenglai Zeng, Zhen Xiang, Yue Xing, Jiliang Tang', 'link': 'https://arxiv.org/abs/2502.13172', 'abstract': "Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent's and the attacker's perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.", 'abstract_zh': '大规模语言模型（LLM）代理在各种实际应用中日益普遍。它们通过在记忆模块中存储私用户-代理交互来增强决策制定，从而为LLM代理引入新的隐私风险。在本文中，我们在黑盒设置下系统地研究了我们提出的记忆提取攻击（MEXTRA）对LLM代理的漏洞。为了从记忆中提取私有信息，我们提出了一种有效的攻击提示设计方法以及基于对LLM代理不同知识水平的自动化提示生成方法。在两个代表性的代理上的实验证明了MEXTRA的有效性。此外，我们从代理方和攻击方的角度探讨了导致记忆泄露的关键因素。我们的发现强调了在LLM代理设计和部署中需要有效的记忆保护措施的紧迫性。', 'title_zh': '揭示大语言模型代理记忆中的隐私风险'}
{'arxiv_id': 'arXiv:2502.13171', 'title': 'Web Phishing Net (WPN): A scalable machine learning approach for real-time phishing campaign detection', 'authors': 'Muhammad Fahad Zia, Sri Harish Kalidass', 'link': 'https://arxiv.org/abs/2502.13171', 'abstract': 'Phishing is the most prevalent type of cyber-attack today and is recognized as the leading source of data breaches with significant consequences for both individuals and corporations. Web-based phishing attacks are the most frequent with vectors such as social media posts and emails containing links to phishing URLs that once clicked on render host systems vulnerable to more sinister attacks. Research efforts to detect phishing URLs have involved the use of supervised learning techniques that use large amounts of data to train models and have high computational requirements. They also involve analysis of features derived from vectors including email contents thus affecting user privacy. Additionally, they suffer from a lack of resilience against evolution of threats especially with the advent of generative AI techniques to bypass these systems as with AI-generated phishing URLs. Unsupervised methods such as clustering techniques have also been used in phishing detection in the past, however, they are at times unscalable due to the use of pair-wise comparisons. They also lack high detection rates while detecting phishing campaigns. In this paper, we propose an unsupervised learning approach that is not only fast but scalable, as it does not involve pair-wise comparisons. It is able to detect entire campaigns at a time with a high detection rate while preserving user privacy; this includes the recent surge of campaigns with targeted phishing URLs generated by malicious entities using generative AI techniques.', 'abstract_zh': '基于聚类的无监督学习方法：快速、可扩展的钓鱼网址检测以保护用户隐私', 'title_zh': 'Web钓鱼网(WPN):一种实时钓鱼活动检测的可扩展机器学习方法'}
{'arxiv_id': 'arXiv:2502.13167', 'title': 'SmartLLM: Smart Contract Auditing using Custom Generative AI', 'authors': 'Jun Kevin, Pujianto Yugopuspito', 'link': 'https://arxiv.org/abs/2502.13167', 'abstract': "Smart contracts are essential to decentralized finance (DeFi) and blockchain ecosystems but are increasingly vulnerable to exploits due to coding errors and complex attack vectors. Traditional static analysis tools and existing vulnerability detection methods often fail to address these challenges comprehensively, leading to high false-positive rates and an inability to detect dynamic vulnerabilities. This paper introduces SmartLLM, a novel approach leveraging fine-tuned LLaMA 3.1 models with Retrieval-Augmented Generation (RAG) to enhance the accuracy and efficiency of smart contract auditing. By integrating domain-specific knowledge from ERC standards and employing advanced techniques such as QLoRA for efficient fine-tuning, SmartLLM achieves superior performance compared to static analysis tools like Mythril and Slither, as well as zero-shot large language model (LLM) prompting methods such as GPT-3.5 and GPT-4. Experimental results demonstrate a perfect recall of 100% and an accuracy score of 70%, highlighting the model's robustness in identifying vulnerabilities, including reentrancy and access control issues. This research advances smart contract security by offering a scalable and effective auditing solution, supporting the secure adoption of decentralized applications.", 'abstract_zh': '智能合约对于分布式金融(DeFi)和区块链生态系统至关重要，但由于编码错误和复杂的攻击向量，它们日益面临exploits的风险。传统的静态分析工具和现有的漏洞检测方法往往无法全面应对这些挑战，导致高误报率且无法检测动态漏洞。本文提出了一种新颖的方法SmartLLM，利用精细调整的LLaMA 3.1模型结合检索增强生成（RAG）技术，以提高智能合约审计的准确性和效率。通过整合来自ERC标准的领域特定知识，并采用先进的技术如QLoRA进行高效的细调，SmartLLM在性能上优于如Mythril和Slither之类的静态分析工具，以及如GPT-3.5和GPT-4之类的零样本大语言模型（LLM）提示方法。实验结果显示召回率为100%且准确率为70%，突显了该模型在识别包括重入和访问控制问题在内的漏洞方面的鲁棒性。这项研究通过提供可扩展且有效的审计解决方案，促进了智能合约的安全性，并支持分布式应用的安全采用。', 'title_zh': 'SmartLLM: 使用自定义生成式AI进行智能合约审计'}
{'arxiv_id': 'arXiv:2502.13166', 'title': 'Large Language Models Can Help Mitigate Barren Plateaus', 'authors': 'Jun Zhuang, Chaowen Guan', 'link': 'https://arxiv.org/abs/2502.13166', 'abstract': "In the era of noisy intermediate-scale quantum (NISQ) computing, Quantum Neural Networks (QNNs) have emerged as a promising approach for various applications, yet their training is often hindered by barren plateaus (BPs), where gradient variance vanishes exponentially as the model size increases. To address this challenge, we propose a new Large Language Model (LLM)-driven search framework, AdaInit, that iteratively searches for optimal initial parameters of QNNs to maximize gradient variance and therefore mitigate BPs. Unlike conventional one-time initialization methods, AdaInit dynamically refines QNN's initialization using LLMs with adaptive prompting. Theoretical analysis of the Expected Improvement (EI) proves a supremum for the search, ensuring this process can eventually identify the optimal initial parameter of the QNN. Extensive experiments across four public datasets demonstrate that AdaInit significantly enhances QNN's trainability compared to classic initialization methods, validating its effectiveness in mitigating BPs.", 'abstract_zh': '在嘈杂的中等规模量子（NISQ）计算时代，量子神经网络（QNNs）已成为多种应用的有前景的方法，但其训练往往受到梯度消失的荒原 plateau （BPs）的阻碍。为了解决这一挑战，我们提出了一种新的基于大型语言模型（LLM）的搜索框架AdaInit，该框架通过迭代搜索优化QNN的初始参数以最大化梯度方差，从而缓解BPs。与传统的单次初始化方法不同，AdaInit使用具有自适应提示的LLMs动态优化QNN的初始化。预期改进（EI）的理论分析证明了搜索的上限，确保该过程最终能够识别出QNN的最佳初始参数。在四个公开数据集上的广泛实验表明，AdaInit显著提高了QNN的可训练性，验证了其在缓解BPs方面的有效性。', 'title_zh': '大型语言模型可以帮助缓解 barren plateau 问题。'}
{'arxiv_id': 'arXiv:2502.13165', 'title': 'HedgeAgents: A Balanced-aware Multi-agent Financial Trading System', 'authors': 'Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, Xiangmin Xu', 'link': 'https://arxiv.org/abs/2502.13165', 'abstract': "As automated trading gains traction in the financial market, algorithmic investment strategies are increasingly prominent. While Large Language Models (LLMs) and Agent-based models exhibit promising potential in real-time market analysis and trading decisions, they still experience a significant -20% loss when confronted with rapid declines or frequent fluctuations, impeding their practical application. Hence, there is an imperative to explore a more robust and resilient framework. This paper introduces an innovative multi-agent system, HedgeAgents, aimed at bolstering system robustness via ``hedging'' strategies. In this well-balanced system, an array of hedging agents has been tailored, where HedgeAgents consist of a central fund manager and multiple hedging experts specializing in various financial asset classes. These agents leverage LLMs' cognitive capabilities to make decisions and coordinate through three types of conferences. Benefiting from the powerful understanding of LLMs, our HedgeAgents attained a 70% annualized return and a 400% total return over a period of 3 years. Moreover, we have observed with delight that HedgeAgents can even formulate investment experience comparable to those of human experts (this https URL).", 'abstract_zh': '随着自动化交易在金融市场中的应用日益普及，算法投资策略逐渐凸显。虽然大型语言模型（LLMs）和基于代理的模型在实时市场分析和交易决策方面展现出潜在的应用前景，但在应对快速下跌或频繁波动时，它们仍然会遭受高达20%的损失，这限制了它们的实际应用。因此，迫切需要探索更具韧性的框架。本文介绍了一种创新的多代理系统——HedgeAgents，旨在通过“风险管理”策略增强系统稳定性。在这一均衡系统中，定制了一组风险管理代理，HedgeAgents包括一个中央资金管理者和多个专注于不同金融资产类别的风险管理专家。这些代理通过利用LLMs的认知能力，并借助三种类型的会议进行协调，实现了70%的年化回报和3年400%的总回报。此外，我们高兴地发现，HedgeAgents甚至能够形成与人类专家相当的投资经验（请参见相关链接）。', 'title_zh': 'HedgeAgents：一个考虑对冲策略的多代理金融交易系统'}
{'arxiv_id': 'arXiv:2502.13164', 'title': 'Multi-Agent Actor-Critic Generative AI for Query Resolution and Analysis', 'authors': 'Mohammad Wali Ur Rahman, Ric Nevarez, Lamia Tasnim Mim, Salim Hariri', 'link': 'https://arxiv.org/abs/2502.13164', 'abstract': 'In this paper, we introduce MASQRAD (Multi-Agent Strategic Query Resolution and Diagnostic tool), a transformative framework for query resolution based on the actor-critic model, which utilizes multiple generative AI agents. MASQRAD is excellent at translating imprecise or ambiguous user inquiries into precise and actionable requests. This framework generates pertinent visualizations and responses to these focused queries, as well as thorough analyses and insightful interpretations for users. MASQRAD addresses the common shortcomings of existing solutions in domains that demand fast and precise data interpretation, such as their incapacity to successfully apply AI for generating actionable insights and their challenges with the inherent ambiguity of user queries. MASQRAD functions as a sophisticated multi-agent system but "masquerades" to users as a single AI entity, which lowers errors and enhances data interaction. This approach makes use of three primary AI agents: Actor Generative AI, Critic Generative AI, and Expert Analysis Generative AI. Each is crucial for creating, enhancing, and evaluating data interactions. The Actor AI generates Python scripts to generate data visualizations from large datasets within operational constraints, and the Critic AI rigorously refines these scripts through multi-agent debate. Finally, the Expert Analysis AI contextualizes the outcomes to aid in decision-making. With an accuracy rate of 87\\% when handling tasks related to natural language visualization, MASQRAD establishes new benchmarks for automated data interpretation and showcases a noteworthy advancement that has the potential to revolutionize AI-driven applications.', 'abstract_zh': '基于演员-评论家模型的多Agent战略查询解析与诊断框架MASQRAD', 'title_zh': '多Agentactor-批评生成AI查询解析与分析'}
{'arxiv_id': 'arXiv:2502.13162', 'title': 'ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs', 'authors': 'Ziyi Ni, Hao Wang, Huacan Wang', 'link': 'https://arxiv.org/abs/2502.13162', 'abstract': 'Large Language Models (LLMs) have achieved remarkable success in various domains but remain vulnerable to adversarial jailbreak attacks. Existing prompt-defense strategies, including parameter-modifying and parameter-free approaches, face limitations in adaptability, interpretability, and customization, constraining their effectiveness against evolving threats. To address these challenges, we propose ShieldLearner, a novel paradigm that mimics human learning in defense. Through trial and error, it autonomously distills attack signatures into a Pattern Atlas and synthesizes defense heuristics into a Meta-analysis Framework, enabling systematic and interpretable threat detection. Furthermore, we introduce Adaptive Adversarial Augmentation to generate adversarial variations of successfully defended prompts, enabling continuous self-improvement without model retraining. In addition to standard benchmarks, we create a hard test set by curating adversarial prompts from the Wildjailbreak dataset, emphasizing more concealed malicious intent. Experimental results show that ShieldLearner achieves a significantly higher defense success rate than existing baselines on both conventional and hard test sets, while also operating with lower computational overhead, making it a practical and efficient solution for real-world adversarial defense.', 'abstract_zh': '大型语言模型（LLMs）已经在多个领域取得了显著成功，但仍易受到 adversarial jailbreak 攻击。现有的提示防御策略，包括参数修改型和无参数型方法，存在适应性、可解释性和定制性方面的局限性，限制了其对不断演变威胁的防御效果。为应对这些挑战，我们提出了一种名为 ShieldLearner 的新颖范式，模仿人类在防御中的学习过程。通过试错，它自主提炼攻击特征到模式图谱，并综合生成防御启发式方法到元分析框架，从而实现系统化和可解释性的威胁检测。此外，我们引入了自适应对抗增强，生成防御成功的提示的对抗变体，实现无需模型重新训练的持续自我改进。除了标准基准外，我们还通过从 Wildjailbreak 数据集中精心筛选恶意提示创建了一个更具挑战性的测试集，强调了更隐蔽的恶意意图。实验结果表明，ShieldLearner 在传统和更具挑战性的测试集上都显著提高了防御成功率，同时具有较低的计算开销，使其成为实际应用中对抗防御的实用高效解决方案。', 'title_zh': 'ShieldLearner: 一种新的大模型 jailbreak 攻击防御范式'}
{'arxiv_id': 'arXiv:2502.13161', 'title': 'Noumenal Labs White Paper: How To Build A Brain', 'authors': 'Maxwell J. D. Ramstead, Candice Pattisapu, Jason Fox, Jeff Beck', 'link': 'https://arxiv.org/abs/2502.13161', 'abstract': 'This white paper describes some of the design principles for artificial or machine intelligence that guide efforts at Noumenal Labs. These principles are drawn from both nature and from the means by which we come to represent and understand it. The end goal of research and development in this field should be to design machine intelligences that augment our understanding of the world and enhance our ability to act in it, without replacing us. In the first two sections, we examine the core motivation for our approach: resolving the grounding problem. We argue that the solution to the grounding problem rests in the design of models grounded in the world that we inhabit, not mere word models. A machine super intelligence that is capable of significantly enhancing our understanding of the human world must represent the world as we do and be capable of generating new knowledge, building on what we already know. In other words, it must be properly grounded and explicitly designed for rational, empirical inquiry, modeled after the scientific method. A primary implication of this design principle is that agents must be capable of engaging autonomously in causal physics discovery. We discuss the pragmatic implications of this approach, and in particular, the use cases in realistic 3D world modeling and multimodal, multidimensional time series analysis.', 'abstract_zh': '本白皮书描述了 Noumenal Labs 在设计人工或机器智能时遵循的一些设计原则，这些原则来源于自然及其表现和理解的方式。这一领域研究与开发的目标应该是设计能够增强我们对世界理解并提升我们在其中行动能力的机器智能，而不取代我们。在前两部分中，我们探讨了我们方法的核心动机：解决本体论问题。我们认为本体论问题的解决方案在于建立扎根于我们所居住的世界的模型，而不仅仅是语言模型。能够显著增强我们对人类世界理解的机器超级智能必须以我们的方式表示世界，并且能够生成新的知识，建立在我们已知的基础上。换句话说，它必须是适当扎根并明确为基于理性、实证探究的设计，模仿科学方法。这一设计原则的一个主要含义是代理必须能够自主进行因果物理学发现。我们讨论了这种方法的实际意义，特别是在现实3D世界建模和多模态多维度时间序列分析方面的应用场景。', 'title_zh': 'noumenal labs 白皮书：如何构建一个大脑'}
{'arxiv_id': 'arXiv:2502.13160', 'title': 'Understanding Dynamic Diffusion Process of LLM-based Agents under Information Asymmetry', 'authors': 'Yiwen Zhang, Yifu Wu, Wenyue Hua, Xuming Hu', 'link': 'https://arxiv.org/abs/2502.13160', 'abstract': 'Large language models have been used to simulate human society using multi-agent systems. Most current social simulation research emphasizes interactive behaviors in fixed environments, ignoring information opacity, relationship variability and diffusion diversity. In this paper, we study the dynamics of information diffusion in 12 asymmetric open environments defined by information content and distribution mechanisms. We first present a general framework to capture the features of information diffusion. Then, we designed a dynamic attention mechanism to help agents allocate attention to different information, addressing the limitations of LLM-based attention. Agents start by responding to external information stimuli within a five-agent group, increasing group size and forming information circles while developing relationships and sharing information. Additionally, we observe the emergence of information cocoons, the evolution of information gaps, and the accumulation of social capital, which are closely linked to psychological, sociological, and communication theories.', 'abstract_zh': '大型语言模型已使用多agent系统模拟人类社会。现有的大多数社会仿真研究注重固定环境下的交互行为，忽视了信息不透明性、关系多变性和信息扩散多样性。本文研究了由信息内容和分布机制定义的12种不对称开放环境中的信息扩散动力学。我们首先提出了一种通用框架来捕捉信息扩散的特征，然后设计了一种动态注意力机制以帮助代理分配注意力给不同的信息，解决基于LLM的注意力机制的局限性。代理们最初在一个五agent小组内对外部信息刺激做出反应，随着小组规模的扩大并形成信息圈，同时建立关系和分享信息。此外，我们观察到信息茧房的出现、信息鸿沟的演变以及社会资本的积累，这些现象与心理、社会学和传播理论密切相关。', 'title_zh': '基于信息不对称的LLM代理动态扩散过程理解'}
{'arxiv_id': 'arXiv:2502.09720', 'title': 'NestQuant: Nested Lattice Quantization for Matrix Products and LLMs', 'authors': 'Semyon Savkin, Eitan Porat, Or Ordentlich, Yury Polyanskiy', 'link': 'https://arxiv.org/abs/2502.09720', 'abstract': "Post-training quantization (PTQ) has emerged as a critical technique for efficient deployment of large language models (LLMs). This work proposes NestQuant, a novel PTQ scheme for weights and activations that is based on self-similar nested lattices. Recent work have mathematically shown such quantizers to be information-theoretically optimal for low-precision matrix multiplication. We implement a practical low-complexity version of NestQuant based on Gosset lattice, making it a drop-in quantizer for any matrix multiplication step (e.g., in self-attention, MLP etc). For example, NestQuant quantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving perplexity of 6.6 on wikitext2. This represents more than 55% reduction in perplexity gap with respect to unquantized model (perplexity of 6.14) compared to state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various LLM evaluation benchmarks also show a reduction in performance degradation induced by quantization.", 'abstract_zh': 'Post-training 量化 (PTQ) 已成为高效部署大型语言模型 (LLMs) 的关键技术。本文提出了一种基于自相似嵌套格的新型 PTQ 方案 NestQuant，用于权重和激活值。最近的研究已经从理论上证明，此类量化器对于低精度矩阵乘法是信息论意义下的最优解。我们基于 Gosset 格实现了 NestQuant 的实用低复杂度版本，使其成为任何矩阵乘法步骤（例如，在自我注意、MLP 等中）的即插即用量化器。例如，NestQuant 将 Llama-3-8B 的权重、KV 缓存和激活值量化为 4 位， perplexity 达到 6.6，在 wikitext2 上的表现优于 Meta 的 SpinQuant（ perplexity 为 7.3），与未量化模型（ perplexity 为 6.14）相比， perplexity 差距减少了 55%。此外，在各种 LLM 评估基准上的比较也显示量化带来的性能下降有所减少。', 'title_zh': 'NestQuant: 嵌套格量化方法在矩阵乘法和大语言模型中的应用'}
