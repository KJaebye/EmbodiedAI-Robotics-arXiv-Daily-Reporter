{'arxiv_id': 'arXiv:2504.15643', 'title': 'Multimodal Perception for Goal-oriented Navigation: A Survey', 'authors': 'I-Tak Ieong, Hao Tang', 'link': 'https://arxiv.org/abs/2504.15643', 'abstract': 'Goal-oriented navigation presents a fundamental challenge for autonomous systems, requiring agents to navigate complex environments to reach designated targets. This survey offers a comprehensive analysis of multimodal navigation approaches through the unifying perspective of inference domains, exploring how agents perceive, reason about, and navigate environments using visual, linguistic, and acoustic information. Our key contributions include organizing navigation methods based on their primary environmental reasoning mechanisms across inference domains; systematically analyzing how shared computational foundations support seemingly disparate approaches across different navigation tasks; identifying recurring patterns and distinctive strengths across various navigation paradigms; and examining the integration challenges and opportunities of multimodal perception to enhance navigation capabilities. In addition, we review approximately 200 relevant articles to provide an in-depth understanding of the current landscape.', 'abstract_zh': '面向目标的导航为自主系统提出了基本挑战，要求代理在复杂环境中导航以到达指定目标。本文综述通过推理域的统一视角，对多模态导航方法进行了全面分析，探讨代理如何利用视觉、语言和声学信息来感知、推理和导航环境。我们的主要贡献包括基于推理域中主要的环境推理机制组织导航方法；系统分析共享的计算基础如何支持在不同导航任务中看似不同的方法；识别各种导航范式中的重复模式和独特优势；并探讨多模态感知的集成挑战与机遇以提高导航能力。此外，我们回顾了约200篇文章以提供当前研究格局的深入理解。', 'title_zh': '面向目标导向导航的多模态感知综述'}
{'arxiv_id': 'arXiv:2504.15929', 'title': 'Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models', 'authors': 'Saban Ozturk, Melih B. Yilmaz, Muti Kara, M. Talat Yavuz, Aykut Koç, Tolga Çukur', 'link': 'https://arxiv.org/abs/2504.15929', 'abstract': 'Diagnostic imaging relies on interpreting both images and radiology reports, but the growing data volumes place significant pressure on medical experts, yielding increased errors and workflow backlogs. Medical vision-language models (med-VLMs) have emerged as a powerful framework to efficiently process multimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit their performance hinges on how well image and text representations are aligned. Existing alignment methods, predominantly based on contrastive learning, prioritize separation between disease classes over segregation of fine-grained pathology attributes like location, size or severity, leading to suboptimal representations. Here, we propose MedTrim (Meta-entity-driven Triplet mining), a novel method that enhances image-text alignment through multimodal triplet learning synergistically guided by disease class as well as adjectival and directional pathology descriptors. Unlike common alignment methods that separate broad disease classes, MedTrim leverages structured meta-entity information to preserve subtle but clinically significant intra-class variations. For this purpose, we first introduce an ontology-based entity recognition module that extracts pathology-specific meta-entities from CXR reports, as annotations on pathology attributes are rare in public datasets. For refined sample selection in triplet mining, we then introduce a novel score function that captures an aggregate measure of inter-sample similarity based on disease classes and adjectival/directional descriptors. Lastly, we introduce a multimodal triplet alignment objective for explicit within- and cross-modal alignment between samples sharing detailed pathology characteristics. Our demonstrations indicate that MedTrim improves performance in downstream retrieval and classification tasks compared to state-of-the-art alignment methods.', 'abstract_zh': '医学影像诊断依赖于图像和放射报告的解释，但不断增加的数据量对医疗专家产生了巨大压力，导致错误增多和工作流程积压。医学视觉语言模型（med-VLMs）作为高效处理多模态影像数据的强大框架，尤其在胸部X光（CXR）评估中表现出色，但其性能取决于图像和文本表示之间的对齐程度。现有对齐方法主要基于对比学习，优先分离疾病类别而非细粒度病理特征（如位置、大小或严重程度）的区分，导致表示不佳。我们提出了一种名为MedTrim（Meta-entity-driven Triplet mining）的新方法，通过多模态三元组学习增强图像-文本对齐，该方法由疾病类别以及形容词和方向性病理描述协同引导。与通常分离广泛疾病类别的方法不同，MedTrim 使用结构化元实体信息来保留关键但细微的类内差异。为此，我们首先引入了一个基于本体的实体识别模块，从CXR报告中提取病理特定的元实体，因为公共数据集中对病理属性的标注很少。为了在三元组挖掘中进行精细化样本选择，我们引入了一个新型得分函数，该函数基于疾病类别和形容词/方向性描述捕获样本间相似性的综合度量。最后，我们引入了一个多模态三元组对齐目标，以显式方式对具有详细病理特征的样本进行模内和模间对齐。我们的演示表明，与现有最佳对齐方法相比，MedTrim 在下游检索和分类任务中提高了性能。', 'title_zh': '基于元实体的三元组挖掘方法在医疗视觉语言模型对齐中的应用'}
