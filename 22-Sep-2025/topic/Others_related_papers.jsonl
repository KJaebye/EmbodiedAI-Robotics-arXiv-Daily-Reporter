{'arxiv_id': 'arXiv:2509.15737', 'title': 'SMART: Scalable Multi-Agent Reasoning and Trajectory Planning in Dense Environments', 'authors': 'Heye Huang, Yibin Yang, Wang Chen, Tiantian Chen, Xiaopeng Li, Sikai Chen', 'link': 'https://arxiv.org/abs/2509.15737', 'abstract': 'Multi-vehicle trajectory planning is a non-convex problem that becomes increasingly difficult in dense environments due to the rapid growth of collision constraints. Efficient exploration of feasible behaviors and resolution of tight interactions are essential for real-time, large-scale coordination. This paper introduces SMART, Scalable Multi-Agent Reasoning and Trajectory Planning, a hierarchical framework that combines priority-based search with distributed optimization to achieve efficient and feasible multi-vehicle planning. The upper layer explores diverse interaction modes using reinforcement learning-based priority estimation and large-step hybrid A* search, while the lower layer refines solutions via parallelizable convex optimization. By partitioning space among neighboring vehicles and constructing robust feasible corridors, the method decouples the joint non-convex problem into convex subproblems solved efficiently in parallel. This design alleviates the step-size trade-off while ensuring kinematic feasibility and collision avoidance. Experiments show that SMART consistently outperforms baselines. On 50 m x 50 m maps, it sustains over 90% success within 1 s up to 25 vehicles, while baselines often drop below 50%. On 100 m x 100 m maps, SMART achieves above 95% success up to 50 vehicles and remains feasible up to 90 vehicles, with runtimes more than an order of magnitude faster than optimization-only approaches. Built on vehicle-to-everything communication, SMART incorporates vehicle-infrastructure cooperation through roadside sensing and agent coordination, improving scalability and safety. Real-world experiments further validate this design, achieving planning times as low as 0.014 s while preserving cooperative behaviors.', 'abstract_zh': '可扩展多智能体推理与轨迹规划：基于优先级的搜索与分布式优化相结合的层次框架', 'title_zh': 'SMART：密集环境可扩展多 Agents 原理推理与轨迹规划'}
{'arxiv_id': 'arXiv:2509.15597', 'title': 'Distributed Nash Equilibrium Seeking Algorithm in Aggregative Games for Heterogeneous Multi-Robot Systems', 'authors': 'Yi Dong, Zhongguo Li, Sarvapali D. Ramchurn, Xiaowei Huang', 'link': 'https://arxiv.org/abs/2509.15597', 'abstract': 'This paper develops a distributed Nash Equilibrium seeking algorithm for heterogeneous multi-robot systems. The algorithm utilises distributed optimisation and output control to achieve the Nash equilibrium by leveraging information shared among neighbouring robots. Specifically, we propose a distributed optimisation algorithm that calculates the Nash equilibrium as a tailored reference for each robot and designs output control laws for heterogeneous multi-robot systems to track it in an aggregative game. We prove that our algorithm is guaranteed to converge and result in efficient outcomes. The effectiveness of our approach is demonstrated through numerical simulations and empirical testing with physical robots.', 'abstract_zh': '本文开发了异构多机器人系统中的分布式纳什均衡寻求算法。该算法利用分布式优化和输出控制，通过利用邻近机器人之间共享的信息来实现纳什均衡。具体而言，我们提出了一种分布式优化算法，为每个机器人计算一个定制化的纳什均衡参考，并设计了输出控制律以使异构多机器人系统在聚合博弈中跟踪该参考。我们证明了该算法能够收敛并产生高效的结果。通过数值仿真和物理机器人实验验证了我们方法的有效性。', 'title_zh': '异构多机器人系统中聚合博弈的分布式纳什均衡搜索算法'}
{'arxiv_id': 'arXiv:2509.15565', 'title': 'Distribution Estimation for Global Data Association via Approximate Bayesian Inference', 'authors': 'Yixuan Jia, Mason B. Peterson, Qingyuan Li, Yulun Tian, Jonathan P. How', 'link': 'https://arxiv.org/abs/2509.15565', 'abstract': 'Global data association is an essential prerequisite for robot operation in environments seen at different times or by different robots. Repetitive or symmetric data creates significant challenges for existing methods, which typically rely on maximum likelihood estimation or maximum consensus to produce a single set of associations. However, in ambiguous scenarios, the distribution of solutions to global data association problems is often highly multimodal, and such single-solution approaches frequently fail. In this work, we introduce a data association framework that leverages approximate Bayesian inference to capture multiple solution modes to the data association problem, thereby avoiding premature commitment to a single solution under ambiguity. Our approach represents hypothetical solutions as particles that evolve according to a deterministic or randomized update rule to cover the modes of the underlying solution distribution. Furthermore, we show that our method can incorporate optimization constraints imposed by the data association formulation and directly benefit from GPU-parallelized optimization. Extensive simulated and real-world experiments with highly ambiguous data show that our method correctly estimates the distribution over transformations when registering point clouds or object maps.', 'abstract_zh': '全球数据关联是机器人在不同时段或不同机器人环境中操作的 essential 预先条件。重复或对称数据为现有方法带来了重大挑战，这些方法通常依赖最大似然估计或最大一致估计来生成单个关联集。然而，在模棱两可的场景中，全局数据关联问题的解分布往往是高度多模态的，这种单一解的方法经常失败。本文引入了一种数据关联框架，利用近似贝叶斯推断来捕捉数据关联问题的多个解模态，从而在模棱两可的情况下避免过早对单一解做出承诺。我们的方法将假设解表示为粒子，这些粒子根据确定性或随机更新规则演变以涵盖基础解分布的模态。此外，我们展示了我们的方法可以整合由数据关联形式提出的优化约束，并直接从中受益 GPU 并行化优化。广泛模拟和真实世界实验表明，在注册点云或对象地图时，我们的方法可以正确估计变换分布。', 'title_zh': '基于近似貝叶斯推断的全球数据关联分布估计'}
{'arxiv_id': 'arXiv:2509.15325', 'title': 'Measurement and Potential Field-Based Patient Modeling for Model-Mediated Tele-ultrasound', 'authors': 'Ryan S. Yeung, David G. Black, Septimiu E. Salcudean', 'link': 'https://arxiv.org/abs/2509.15325', 'abstract': "Teleoperated ultrasound can improve diagnostic medical imaging access for remote communities. Having accurate force feedback is important for enabling sonographers to apply the appropriate probe contact force to optimize ultrasound image quality. However, large time delays in communication make direct force feedback impractical. Prior work investigated using point cloud-based model-mediated teleoperation and internal potential field models to estimate contact forces and torques. We expand on this by introducing a method to update the internal potential field model of the patient with measured positions and forces for more transparent model-mediated tele-ultrasound. We first generate a point cloud model of the patient's surface and transmit this to the sonographer in a compact data structure. This is converted to a static voxelized volume where each voxel contains a potential field value. These values determine the forces and torques, which are rendered based on overlap between the voxelized volume and a point shell model of the ultrasound transducer. We solve for the potential field using a convex quadratic that combines the spatial Laplace operator with measured forces. This was evaluated on volunteer patients ($n=3$) by computing the accuracy of rendered forces. Results showed the addition of measured forces to the model reduced the force magnitude error by an average of 7.23 N and force vector angle error by an average of 9.37$^{\\circ}$ compared to using only Laplace's equation.", 'abstract_zh': '远程操控超声可以改善偏远社区的诊断医学成像访问。基于点云的模型介导远程操控和内部势场模型可提高接触力估计，实现更透明的模型介导远程超声。', 'title_zh': '基于测量与势场的患者建模以实现模型中介的远程超声波检查'}
{'arxiv_id': 'arXiv:2509.15984', 'title': 'CoPAD : Multi-source Trajectory Fusion and Cooperative Trajectory Prediction with Anchor-oriented Decoder in V2X Scenarios', 'authors': 'Kangyu Wu, Jiaqi Qiao, Ya Zhang', 'link': 'https://arxiv.org/abs/2509.15984', 'abstract': 'Recently, data-driven trajectory prediction methods have achieved remarkable results, significantly advancing the development of autonomous driving. However, the instability of single-vehicle perception introduces certain limitations to trajectory prediction. In this paper, a novel lightweight framework for cooperative trajectory prediction, CoPAD, is proposed. This framework incorporates a fusion module based on the Hungarian algorithm and Kalman filtering, along with the Past Time Attention (PTA) module, mode attention module and anchor-oriented decoder (AoD). It effectively performs early fusion on multi-source trajectory data from vehicles and road infrastructure, enabling the trajectories with high completeness and accuracy. The PTA module can efficiently capture potential interaction information among historical trajectories, and the mode attention module is proposed to enrich the diversity of predictions. Additionally, the decoder based on sparse anchors is designed to generate the final complete trajectories. Extensive experiments show that CoPAD achieves the state-of-the-art performance on the DAIR-V2X-Seq dataset, validating the effectiveness of the model in cooperative trajectory prediction in V2X scenarios.', 'abstract_zh': '基于匈牙利算法和卡尔曼滤波的多源融合模块的Cooperative Trajectory Prediction框架（CoPAD）', 'title_zh': 'CoPAD：面向锚点的解码器多源轨迹融合与协作轨迹预测在V2X场景中'}
{'arxiv_id': 'arXiv:2509.15981', 'title': 'Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations', 'authors': 'Yujie Zhu, Charles A. Hepburn, Matthew Thorpe, Giovanni Montana', 'link': 'https://arxiv.org/abs/2509.15981', 'abstract': 'In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at this https URL.', 'abstract_zh': '在稀疏奖励的强化学习中，演示可以加速学习，但确定何时模仿仍具挑战性。我们提出了一种平滑策略正则化框架（SPReD），该框架解决了基本问题：代理何时应该模仿演示，何时应遵循其自身策略？SPReD 使用集成方法显式建模演示和策略动作的 Q 值分布，量化不确定性以进行比较。我们开发了两种互补的不确定性感知方法：一种概率方法估计演示优越性的概率，以及一种基于优势的方法，通过统计显著性调整模仿。与现有的二元模仿决策方法（如 Q-filter）不同，SPReD 应用连续的、与不确定性成比例的正则化权重，在训练期间降低梯度方差。尽管计算上很简单，但在八个机器人任务的实验中，SPReD 在复杂任务中的表现比现有方法提高了一倍多，同时保持了对演示质量和数量的鲁棒性。我们的代码可在以下网址获取：this https URL。', 'title_zh': '基于不确定性平滑策略正则化的小样本强化学习'}
{'arxiv_id': 'arXiv:2509.15730', 'title': 'A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation', 'authors': 'Lukas Laakmann, Seyyid A. Ciftci, Christian Janiesch', 'link': 'https://arxiv.org/abs/2509.15730', 'abstract': 'Robotic process automation (RPA) is a lightweight approach to automating business processes using software robots that emulate user actions at the graphical user interface level. While RPA has gained popularity for its cost-effective and timely automation of rule-based, well-structured tasks, its symbolic nature has inherent limitations when approaching more complex tasks currently performed by human agents. Machine learning concepts enabling intelligent RPA provide an opportunity to broaden the range of automatable tasks. In this paper, we conduct a literature review to explore the connections between RPA and machine learning and organize the joint concept intelligent RPA into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML integration and RPA-ML interaction. Together, they comprise eight dimensions: architecture and ecosystem, capabilities, data basis, intelligence level, and technical depth of integration as well as deployment environment, lifecycle phase, and user-robot relation.', 'abstract_zh': '机器人过程自动化(RPA)是一种使用软件机器人在图形用户界面级别模拟用户操作以自动化的轻量级方法。虽然RPA因其成本效益和及时性在自动化基于规则、结构良好的任务方面获得了 popularity，但其符号性质在处理目前由人类代理执行的更复杂任务时具有固有的局限性。使RPA智能化的机器学习概念提供了扩展可自动化任务范围的机会。在本文中，我们进行了一项文献综述，探讨了RPA与机器学习之间的联系，并将联合概念智能化RPA组织成一个分类体系。我们的分类体系包括两个元特征：RPA-ML集成和RPA-ML交互。它们共同构成了八个维度：架构和生态系统、能力、数据基础、智能水平、集成的技术深度、部署环境、生命周期阶段以及用户-机器人关系。', 'title_zh': '机器学习在智能机器人流程自动化中的初步分类'}
{'arxiv_id': 'arXiv:2509.15848', 'title': 'A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring', 'authors': 'Giovanni De Gasperis, Sante Dino Facchini', 'link': 'https://arxiv.org/abs/2509.15848', 'abstract': 'Industrial monitoring systems, especially when deployed in Industry 4.0 environments, are experiencing a shift in paradigm from traditional rule-based architectures to data-driven approaches leveraging machine learning and artificial intelligence. This study presents a comparison between these two methodologies, analyzing their respective strengths, limitations, and application scenarios, and proposes a basic framework to evaluate their key properties. Rule-based systems offer high interpretability, deterministic behavior, and ease of implementation in stable environments, making them ideal for regulated industries and safety-critical applications. However, they face challenges with scalability, adaptability, and performance in complex or evolving contexts. Conversely, data-driven systems excel in detecting hidden anomalies, enabling predictive maintenance and dynamic adaptation to new conditions. Despite their high accuracy, these models face challenges related to data availability, explainability, and integration complexity. The paper suggests hybrid solutions as a possible promising direction, combining the transparency of rule-based logic with the analytical power of machine learning. Our hypothesis is that the future of industrial monitoring lies in intelligent, synergic systems that leverage both expert knowledge and data-driven insights. This dual approach enhances resilience, operational efficiency, and trust, paving the way for smarter and more flexible industrial environments.', 'abstract_zh': '工业监测系统：从基于规则的架构向数据驱动的方法转变——优势、限制与应用场景比较及评估框架的研究', 'title_zh': '基于规则和数据驱动方法在工业监测中的比较研究'}
{'arxiv_id': 'arXiv:2509.15786', 'title': 'Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration', 'authors': 'Nan Li, Bo Kang, Tijl De Bie', 'link': 'https://arxiv.org/abs/2509.15786', 'abstract': 'Creating robust occupation taxonomies, vital for applications ranging from job recommendation to labor market intelligence, is challenging. Manual curation is slow, while existing automated methods are either not adaptive to dynamic regional markets (top-down) or struggle to build coherent hierarchies from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent taxonomy Builder), a framework that fully automates the creation of high-quality, data-driven taxonomies from raw job postings. CLIMB uses global semantic clustering to distill core occupations, then employs a reflection-based multi-agent system to iteratively build a coherent hierarchy. On three diverse, real-world datasets, we show that CLIMB produces taxonomies that are more coherent and scalable than existing methods and successfully capture unique regional characteristics. We release our code and datasets at this https URL.', 'abstract_zh': '基于聚类的多智能体税务构建器：创建用于求职推荐和劳动市场智能应用的鲁棒职业分类学', 'title_zh': '基于语义聚类和多agent合作的自底向上多阶段数据驱动职业分类构建方法'}
{'arxiv_id': 'arXiv:2509.15780', 'title': 'Ontology Creation and Management Tools: the Case of Anatomical Connectivity', 'authors': 'Natallia Kokash, Bernard de Bono, Tom Gillespie', 'link': 'https://arxiv.org/abs/2509.15780', 'abstract': 'We are developing infrastructure to support researchers in mapping data related to the peripheral nervous system and other physiological systems, with an emphasis on their relevance to the organs under investigation. The nervous system, a complex network of nerves and ganglia, plays a critical role in coordinating and transmitting signals throughout the body. To aid in this, we have created ApiNATOMY, a framework for the topological and semantic representation of multiscale physiological circuit maps. ApiNATOMY integrates a Knowledge Representation (KR) model and a suite of Knowledge Management (KM) tools. The KR model enables physiology experts to easily capture interactions between anatomical entities, while the KM tools help modelers convert high-level abstractions into detailed models of physiological processes, which can be integrated with external ontologies and knowledge graphs.', 'abstract_zh': '我们正在开发基础设施，以支持研究人员对与周围神经系统及其他生理系统相关数据进行映射，特别是在研究目标器官方面的相关性。神经系统作为一系列神经和神经节的复杂网络，在协调和传递全身信号中起着关键作用。为此，我们创建了ApiNATOMY框架，用于多尺度生理电路图的拓扑和语义表示。ApiNATOMY整合了一个知识表示（KR）模型和一系列知识管理（KM）工具。KR模型使生理学专家能够轻松捕获解剖实体间的交互作用，而KM工具帮助建模者将高层次的抽象转化为详细的生理过程模型，并与外部本体和知识图谱进行集成。', 'title_zh': '解剖连接性领域本体创建与管理工具'}
{'arxiv_id': 'arXiv:2509.15541', 'title': 'Stress Testing Deliberative Alignment for Anti-Scheming Training', 'authors': 'Bronson Schoen, Evgenia Nitishinskaya, Mikita Balesni, Axel Højmark, Felix Hofstätter, Jérémy Scheurer, Alexander Meinke, Jason Wolfe, Teun van der Weij, Alex Lloyd, Nicholas Goldowsky-Dill, Angela Fan, Andrei Matveiakin, Rusheb Shah, Marcus Williams, Amelia Glaese, Boaz Barak, Wojciech Zaremba, Marius Hobbhahn', 'link': 'https://arxiv.org/abs/2509.15541', 'abstract': 'Highly capable AI systems could secretly pursue misaligned goals -- what we call "scheming". Because a scheming AI would deliberately try to hide its misaligned goals and actions, measuring and mitigating scheming requires different strategies than are typically used in ML. We propose that assessing anti-scheming interventions requires at least (1) testing propensity to scheme on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming is driven by situational awareness, and (3) checking for robustness to pre-existing misaligned goals. We use a broad category of "covert actions" -- such as secretly breaking rules or intentionally underperforming in tests -- as a proxy for scheming, and design evaluations for covert actions. We then stress-test deliberative alignment as a case study for anti-scheming. Across 26 OOD evaluations (180+ environments), deliberative alignment reduces covert action rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our mitigation is also able to largely stop agents from pursuing a hidden goal previously trained into the model, but we still find misbehavior after additional red-teaming. We find that models\' chain-of-thought (CoT) often demonstrates awareness of being evaluated for alignment, and show causal evidence that this awareness decreases covert behavior, while unawareness increases it. Therefore, we cannot exclude that the observed reductions in covert action rates are at least partially driven by situational awareness. While we rely on human-legible CoT for training, studying situational awareness, and demonstrating clear evidence of misalignment, our ability to rely on this degrades as models continue to depart from reasoning in standard English. We encourage research into alignment mitigations for scheming and their assessment, especially for the adversarial case of deceptive alignment, which this paper does not address.', 'abstract_zh': '高度先进的AI系统可能秘密追求错准目标——我们称之为“ scheming”。评估反scheming干预措施需要至少包括（1）在远域分布（OOD）任务中测试scheming倾向性；（2）评估缺乏scheming是否由情境意识驱动；（3）检查对先存错准目标的鲁棒性。我们使用一种广泛的“隐蔽行动”类别——如秘密违背规则或故意在测试中表现不佳——作为scheming的代理，并设计了对隐蔽行动的评估方法。我们然后以反思性对准为案例研究，测试反scheming。在26个远域分布评价（180多个环境中），反思性对准降低了隐蔽行动频率（OpenAI o3: 13%->0.4%），但未完全消除。我们的缓解措施也能大大阻止代理追求先前训练进模型的隐藏目标，但在进一步的红队测试后，我们仍发现不当行为。我们发现模型的推理链（CoT）经常表现出被评估对准的意识，并展示了因果证据显示这种意识会减少隐蔽行为，而缺乏这种意识则会增加隐蔽行为。因此，我们不能排除观察到的隐蔽行动频率降低部分是由于情境意识驱动的。尽管我们依靠人类可读的推理链进行训练、研究情境意识以及展示对准不一致的确凿证据，但随着模型继续偏离标准英语的推理方式，我们依赖这一能力的能力会下降。我们鼓励研究对scheming的对准缓解措施及其评估，特别是对于欺骗性对准的对抗场景，这不在本文讨论范围内。', 'title_zh': '抗欺诈训练中的审议一致性的压力测试'}
{'arxiv_id': 'arXiv:2509.15409', 'title': 'FragmentRetro: A Quadratic Retrosynthetic Method Based on Fragmentation Algorithms', 'authors': 'Yu Shee, Anthony M. Smaldone, Anton Morgunov, Gregory W. Kyro, Victor S. Batista', 'link': 'https://arxiv.org/abs/2509.15409', 'abstract': 'Retrosynthesis, the process of deconstructing a target molecule into simpler precursors, is crucial for computer-aided synthesis planning (CASP). Widely adopted tree-search methods often suffer from exponential computational complexity. In this work, we introduce FragmentRetro, a novel retrosynthetic method that leverages fragmentation algorithms, specifically BRICS and r-BRICS, combined with stock-aware exploration and pattern fingerprint screening to achieve quadratic complexity. FragmentRetro recursively combines molecular fragments and verifies their presence in a building block set, providing sets of fragment combinations as retrosynthetic solutions. We present the first formal computational analysis of retrosynthetic methods, showing that tree search exhibits exponential complexity $O(b^h)$, DirectMultiStep scales as $O(h^6)$, and FragmentRetro achieves $O(h^2)$, where $h$ represents the number of heavy atoms in the target molecule and $b$ is the branching factor for tree search. Evaluations on PaRoutes, USPTO-190, and natural products demonstrate that FragmentRetro achieves high solved rates with competitive runtime, including cases where tree search fails. The method benefits from fingerprint screening, which significantly reduces substructure matching complexity. While FragmentRetro focuses on efficiently identifying fragment-based solutions rather than full reaction pathways, its computational advantages and ability to generate strategic starting candidates establish it as a powerful foundational component for scalable and automated synthesis planning.', 'abstract_zh': '逆合成反应，即将目标分子分解为较简单的前体分子的过程，是计算机辅助合成规划（CASP）中的关键步骤。广泛采用的树搜索方法往往受到指数级计算复杂度的困扰。在本文中，我们引入了FragmentRetro，这是一种新颖的逆合成方法，它利用BRICS和r-BRICS等碎片化算法，并结合库存感知探索和模式指纹筛选，实现了平方复杂度。FragmentRetro递归地将分子片段结合起来，并验证其存在于基块集合中的存在性，提供了一系列片段组合作为逆合成解决方案。我们首次对逆合成方法进行了形式化的计算分析，表明树搜索表现出指数级复杂度$O(b^h)$，DirectMultiStep规模为$O(h^6)$，而FragmentRetro实现了$O(h^2)$，其中$h$代表目标分子中的重原子数，$b$是树搜索的分支因子。在PaRoutes、USPTO-190和天然产物上的评估显示，FragmentRetro可实现高解决率与竞争性运行时间，包括树搜索失败的情况。该方法得益于指纹筛选，显著降低了子结构匹配的复杂性。虽然FragmentRetro侧重于高效地识别基于片段的解决方案而不是完整的反应途径，但其计算优势以及能够生成战略性的起始候选的能力使其成为可扩展和自动化合成规划的强大基础组件。', 'title_zh': 'FragmentRetro：基于碎片化算法的二次逆合成方法'}
{'arxiv_id': 'arXiv:2509.15292', 'title': 'An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature', 'authors': 'Abhiyan Dhakal, Kausik Paudel, Sanjog Sigdel', 'link': 'https://arxiv.org/abs/2509.15292', 'abstract': 'We propose an automated pipeline for performing literature reviews using semantic similarity. Unlike traditional systematic review systems or optimization based methods, this work emphasizes minimal overhead and high relevance by using transformer based embeddings and cosine similarity. By providing a paper title and abstract, it generates relevant keywords, fetches relevant papers from open access repository, and ranks them based on their semantic closeness to the input. Three embedding models were evaluated. A statistical thresholding approach is then applied to filter relevant papers, enabling an effective literature review pipeline. Despite the absence of heuristic feedback or ground truth relevance labels, the proposed system shows promise as a scalable and practical tool for preliminary research and exploratory analysis.', 'abstract_zh': '我们提出了一种使用语义相似度进行文献综述的自动化管道。与传统的系统综述系统或基于优化的方法不同，本工作通过使用基于变换器的嵌入和余弦相似度强调了最小的开销和高相关性。通过提供一篇论文的标题和摘要，该系统生成相关关键词，从开放获取库中检索相关论文，并基于它们与输入的语义接近度对其进行排序。三种嵌入模型进行了评估。然后应用统计阈值方法筛选相关论文，从而使文献综述管道具有有效性。尽管没有使用启发式反馈或相关性标签，所提出系统仍展示出作为初步研究和探索性分析的可扩展和实用工具的潜力。', 'title_zh': '基于语义相似性的人工智能驱动文献处理管道'}
{'arxiv_id': 'arXiv:2509.15291', 'title': 'The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI', 'authors': 'Federico Taschin, Abderrahmane Lazaraq, Ozan K. Tonguz, Inci Ozgunes', 'link': 'https://arxiv.org/abs/2509.15291', 'abstract': 'The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart transportation networks has increased significantly in the last few years. Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to be a very promising approach by several authors. However, a problem with using Reinforcement Learning in Traffic Signal Control is the reliability of the trained RL agents due to the dynamically changing distribution of the input data with respect to the distribution of the data used for training. This presents a major challenge and a reliability problem for the trained network of AI agents and could have very undesirable and even detrimental consequences if a suitable solution is not found. Several researchers have tried to address this problem using different approaches. In particular, Meta Reinforcement Learning (Meta RL) promises to be an effective solution. In this paper, we evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and show that, while under certain conditions MetaLight can indeed lead to reasonably good results, under some other conditions it might not perform well (with errors of up to 22%), suggesting that Meta RL schemes are often not robust enough and can even pose major reliability problems.', 'abstract_zh': '机器学习和人工智能在智能交通网络中的应用近年来显著增加，其中强化学习被认为是一种非常有前景的方法。然而，将强化学习用于交通信号控制存在一个挑战，即训练好的强化学习代理的可靠性问题，因为输入数据的动态变化分布与训练数据的分布之间存在差异。这为训练后的AI代理网络的可靠性带来了重大挑战，如果没有找到合适的解决方案，可能会产生非常不良甚至有害的后果。多名研究者尝试使用不同方法来解决这一问题。特别是元强化学习（Meta RL）被证明是一种有效的解决方案。在本文中，我们评估和分析了一个最先进的元强化学习方法——MetaLight，并表明在某些条件下，MetaLight确实可以取得较好的结果；但在其他条件下，它的性能可能不佳（误差高达22%），表明元强化学习方案通常不够稳健，甚至可能带来重大的可靠性问题。', 'title_zh': 'Transportation Networks中的分布偏移问题：基于强化学习和人工智能的研究'}
{'arxiv_id': 'arXiv:2509.15239', 'title': 'KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems', 'authors': 'Stjepan Požgaj, Dobrik Georgiev, Marin Šilić, Petar Veličković', 'link': 'https://arxiv.org/abs/2509.15239', 'abstract': 'Neural algorithmic reasoning (NAR) is a growing field that aims to embed algorithmic logic into neural networks by imitating classical algorithms. In this extended abstract, we detail our attempt to build a neural algorithmic reasoner that can solve Knapsack, a pseudo-polynomial problem bridging classical algorithms and combinatorial optimisation, but omitted in standard NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow the two-phase pipeline for the Knapsack problem, which involves first constructing the dynamic programming table and then reconstructing the solution from it. The approach, which models intermediate states through dynamic programming supervision, achieves better generalization to larger problem instances than a direct-prediction baseline that attempts to select the optimal subset only from the problem inputs.', 'abstract_zh': '神经算法推理（NAR）是一种旨在通过模仿经典算法将算法逻辑嵌入神经网络的新兴领域。在本扩展摘要中，我们详细介绍了我们构建一种能够解决背包问题的神经算法推理器的努力，背包问题是连接经典算法和组合优化的伪多项式问题，而该问题在标准的NAR基准中被忽略。我们的神经算法推理器设计为紧密遵循背包问题的两阶段管道，首先构造动态规划表，然后从中重构解。该方法通过动态规划监督建模中间状态，实现了比仅从问题输入中选择最优子集的直接预测基线更好的泛化性能，特别是在更大的问题实例上。', 'title_zh': 'KNARsack: 教学神经算法推理器解决伪多项式问题'}
{'arxiv_id': 'arXiv:2509.15237', 'title': 'MICA: Multi-Agent Industrial Coordination Assistant', 'authors': 'Di Wen, Kunyu Peng, Junwei Zheng, Yufan Chen, Yitain Shi, Jiale Wei, Ruiping Liu, Kailun Yang, Rainer Stiefelhagen', 'link': 'https://arxiv.org/abs/2509.15237', 'abstract': 'Industrial workflows demand adaptive and trustworthy assistance that can operate under limited computing, connectivity, and strict privacy constraints. In this work, we present MICA (Multi-Agent Industrial Coordination Assistant), a perception-grounded and speech-interactive system that delivers real-time guidance for assembly, troubleshooting, part queries, and maintenance. MICA coordinates five role-specialized language agents, audited by a safety checker, to ensure accurate and compliant support. To achieve robust step understanding, we introduce Adaptive Step Fusion (ASF), which dynamically blends expert reasoning with online adaptation from natural speech feedback. Furthermore, we establish a new multi-agent coordination benchmark across representative task categories and propose evaluation metrics tailored to industrial assistance, enabling systematic comparison of different coordination topologies. Our experiments demonstrate that MICA consistently improves task success, reliability, and responsiveness over baseline structures, while remaining deployable on practical offline hardware. Together, these contributions highlight MICA as a step toward deployable, privacy-preserving multi-agent assistants for dynamic factory environments. The source code will be made publicly available at this https URL.', 'abstract_zh': '工业流程需求具备适应性与可靠性的辅助系统，能够在有限的计算资源、连接能力和严格的隐私限制下运作。本文介绍了MICA（多代理工业协同助手），一个基于感知并与口语互动的系统，能够实时提供装配、故障排除、部件查询和维护的指导。MICA 通过五种角色专业化语言代理的协同工作，经过安全检查审核，确保提供准确且合规的支持。为了实现稳健的步骤理解，我们引入了自适应步骤融合（ASF），动态结合专家推理与来自自然口语反馈的在线适应。此外，我们建立了跨代表性任务类别的多代理协同基准，并提出适用于工业辅助的评估指标，从而能够系统地比较不同的协同拓扑结构。实验结果表明，MICA 在任务成功率、可靠性和响应性上均优于基线结构，同时可在实际离线硬件上部署。这些贡献共同展示了 MICA 作为动态工厂环境中可部署和保护隐私的多代理助手的潜力。源代码将在此网址公开：这个 https URL。', 'title_zh': 'MICA: 多代理工业协调助手'}
{'arxiv_id': 'arXiv:2509.16198', 'title': 'RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation', 'authors': 'Jane Luo, Xin Zhang, Steven Liu, Jie Wu, Yiming Huang, Yangyu Huang, Chengyu Yin, Ying Xin, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qi Chen, Scarlett Li, Mao Yang', 'link': 'https://arxiv.org/abs/2509.16198', 'abstract': 'Large language models excel at function- and file-level code generation, yet generating complete repositories from scratch remains a fundamental challenge. This process demands coherent and reliable planning across proposal- and implementation-level stages, while natural language, due to its ambiguity and verbosity, is ill-suited for faithfully representing complex software structures. To address this, we introduce the Repository Planning Graph (RPG), a persistent representation that unifies proposal- and implementation-level planning by encoding capabilities, file structures, data flows, and functions in one graph. RPG replaces ambiguous natural language with an explicit blueprint, enabling long-horizon planning and scalable repository generation. Building on RPG, we develop ZeroRepo, a graph-driven framework for repository generation from scratch. It operates in three stages: proposal-level planning and implementation-level refinement to construct the graph, followed by graph-guided code generation with test validation. To evaluate this setting, we construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly 3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other baselines. It attains 81.5% functional coverage and a 69.7% pass rate, exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further analysis shows that RPG models complex dependencies, enables progressively more sophisticated planning through near-linear scaling, and enhances LLM understanding of repositories, thereby accelerating agent localization.', 'abstract_zh': '大型语言模型在函数级和文件级代码生成方面表现出色，但从零开始生成完整的代码库仍然是一个基本挑战。为了解决这一问题，我们引入了仓库规划图（RPG），这是一种持久的表示方法，通过在一个图中编码能力、文件结构、数据流和函数，统一了提案级和实现级的规划。RPG 用明确的蓝图取代了模棱两可的自然语言，从而实现长期规划和可扩展的代码库生成。基于RPG，我们开发了ZeroRepo，这是一种从零开始生成代码库的图驱动框架。该框架分为三个阶段：提案级规划和实现级细化以构建图，随后是受图指导的代码生成并带有测试验证。为了评估此设置，我们构建了RepoCraft，这是一个包含六个真实项目和1,052项任务的基准。在RepoCraft上，ZeroRepo生成的代码库平均包含近36K行代码，分别约为最强 baseline（Claude Code）的3.9倍和其他 baselines 的64倍。它实现了81.5%的功能覆盖度和69.7%的通过率，分别超过了Claude Code 27.3和35.8个百分点。进一步分析表明，RPG 模型复杂的依赖关系，通过接近线性的扩展能力逐渐实现更复杂的规划，并增强 LLM 对代码库的理解，从而加速代理定位。', 'title_zh': 'RPG: 一种统一可扩展代码生成的仓库规划图'}
{'arxiv_id': 'arXiv:2509.16195', 'title': 'FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal Distillation', 'authors': 'Luca Della Libera, Cem Subakan, Mirco Ravanelli', 'link': 'https://arxiv.org/abs/2509.16195', 'abstract': 'Neural audio codecs are a fundamental component of modern generative audio pipelines. Although recent codecs achieve strong low-bitrate reconstruction and provide powerful representations for downstream tasks, most are non-streamable, limiting their use in real-time applications. We present FocalCodec-Stream, a hybrid codec based on focal modulation that compresses speech into a single binary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our approach combines multi-stage causal distillation of WavLM with targeted architectural improvements, including a lightweight refiner module that enhances quality under latency constraints. Experiments show that FocalCodec-Stream outperforms existing streamable codecs at comparable bitrates, while preserving both semantic and acoustic information. The result is a favorable trade-off between reconstruction quality, downstream task performance, latency, and efficiency. Code and checkpoints will be released at this https URL.', 'abstract_zh': '聚焦编码器-流式是一种基于焦点调制的混合编码器，能够在0.55-0.80 kbps的比特率下将语音压缩为单个二进制码本，理论延迟为80 ms，并适用于实时应用。', 'title_zh': 'FocalCodec-Stream: 基于因果蒸馏的流式低比特率语音编码'}
{'arxiv_id': 'arXiv:2509.16184', 'title': 'Accelerating Atomic Fine Structure Determination with Graph Reinforcement Learning', 'authors': 'M. Ding, V.-A. Darvariu, A. N. Ryabtsev, N. Hawes, J. C. Pickering', 'link': 'https://arxiv.org/abs/2509.16184', 'abstract': 'Atomic data determined by analysis of observed atomic spectra are essential for plasma diagnostics. For each low-ionisation open d- and f-subshell atomic species, around $10^3$ fine structure level energies can be determined through years of analysis of $10^4$ observable spectral lines. We propose the automation of this task by casting the analysis procedure as a Markov decision process and solving it by graph reinforcement learning using reward functions learned on historical human decisions. In our evaluations on existing spectral line lists and theoretical calculations for Co II and Nd II-III, hundreds of level energies were computed within hours, agreeing with published values in 95% of cases for Co II and 54-87% for Nd II-III. As the current efficiency in atomic fine structure determination struggles to meet growing atomic data demands from astronomy and fusion science, our new artificial intelligence approach sets the stage for closing this gap.', 'abstract_zh': '通过观测原子光谱分析确定的原子数据对于等离子体诊断至关重要。对于每个低电离态开放d-和f亚壳层原子物种，通过数年分析数千条可观测谱线可以确定约1000个精细结构能级。我们建议通过将分析过程视为马尔科夫决策过程，并使用基于历史人类决策学习的奖励函数进行图强化学习来实现这一任务。在对Co II和Nd II-III的现有谱线列表和理论计算进行评估中，几小时内计算了数百个能级能量，在95%的情况下与Co II已发表值一致，在54%-87%的情况下与Nd II-III的已发表值一致。随着天文学和聚变科学中对原子精细结构数据需求的增长，我们提出的人工智能方法为填补这一缺口奠定了基础。', 'title_zh': '使用图强化学习加速原子精细结构确定'}
{'arxiv_id': 'arXiv:2509.16126', 'title': 'Network-Based Detection of Autism Spectrum Disorder Using Sustainable and Non-invasive Salivary Biomarkers', 'authors': 'Janayna M. Fernandes, Robinson Sabino-Silva, Murillo G. Carneiro', 'link': 'https://arxiv.org/abs/2509.16126', 'abstract': "Autism Spectrum Disorder (ASD) lacks reliable biological markers, delaying early diagnosis. Using 159 salivary samples analyzed by ATR-FTIR spectroscopy, we developed GANet, a genetic algorithm-based network optimization framework leveraging PageRank and Degree for importance-based feature characterization. GANet systematically optimizes network structure to extract meaningful patterns from high-dimensional spectral data. It achieved superior performance compared to linear discriminant analysis, support vector machines, and deep learning models, reaching 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74 harmonic mean. These results demonstrate GANet's potential as a robust, bio-inspired, non-invasive tool for precise ASD detection and broader spectral-based health applications.", 'abstract_zh': '自闭症谱系障碍（ASD）缺乏可靠的生物标志物，导致早期诊断延迟。通过分析159个唾液样本的ATR-FTIR光谱，我们开发了GANet，这是一种基于遗传算法的网络优化框架，利用PageRank和Degree进行基于重要性特征表征。GANet系统地优化网络结构以从高维光谱数据中提取有意义的模式。它在线性判别分析、支持向量机和深度学习模型中表现出优越的性能，准确率为0.78、灵敏度为0.61、特异度为0.90，和谐均值为0.74。这些结果表明GANet具有作为稳健、生物启发、无创工具，用于精确检测自闭症谱系障碍及更广泛的光谱基健康管理应用的潜力。', 'title_zh': '基于网络的自闭症谱系障碍检测方法：使用可持续且非侵入性的唾液生物标志物'}
{'arxiv_id': 'arXiv:2509.16068', 'title': 'Communications to Circulations: 3D Wind Field Retrieval and Real-Time Prediction Using 5G GNSS Signals and Deep Learning', 'authors': 'Yuchen Ye, Hong Liang, Chaoxia Yuan, Mingyu Li, Aoqi Zhou, Chunqing Shang, Hua Cai, Peixi Liu, Kezuan Wang, Yifeng Zheng', 'link': 'https://arxiv.org/abs/2509.16068', 'abstract': 'Accurate atmospheric wind field information is crucial for various applications, including weather forecasting, aviation safety, and disaster risk reduction. However, obtaining high spatiotemporal resolution wind data remains challenging due to limitations in traditional in-situ observations and remote sensing techniques, as well as the computational expense and biases of numerical weather prediction (NWP) models. This paper introduces G-WindCast, a novel deep learning framework that leverages signal strength variations from 5G Global Navigation Satellite System (GNSS) signals to retrieve and forecast three-dimensional (3D) atmospheric wind fields. The framework utilizes Forward Neural Networks (FNN) and Transformer networks to capture complex, nonlinear, and spatiotemporal relationships between GNSS-derived features and wind dynamics. Our preliminary results demonstrate promising accuracy in both wind retrieval and short-term wind forecasting (up to 30 minutes lead time), with skill scores comparable to high-resolution NWP outputs in certain scenarios. The model exhibits robustness across different forecast horizons and pressure levels, and its predictions for wind speed and direction show superior agreement with observations compared to concurrent ERA5 reanalysis data. Furthermore, we show that the system can maintain excellent performance for localized forecasting even with a significantly reduced number of GNSS stations (e.g., around 100), highlighting its cost-effectiveness and scalability. This interdisciplinary approach underscores the transformative potential of exploiting non-traditional data sources and deep learning for advanced environmental monitoring and real-time atmospheric applications.', 'abstract_zh': '利用5G全球导航卫星系统信号的新型深度学习框架G-WindCast及其在三维大气风场获取与预报中的应用', 'title_zh': '从通信到环流：使用5G GNSS信号和深度学习的三维风场获取与实时预测'}
{'arxiv_id': 'arXiv:2509.16020', 'title': 'AI Methods for Permutation Circuit Synthesis Across Generic Topologies', 'authors': 'Victor Villar, Juan Cruz-Benito, Ismael Faro, David Kremer', 'link': 'https://arxiv.org/abs/2509.16020', 'abstract': 'This paper investigates artificial intelligence (AI) methodologies for the synthesis and transpilation of permutation circuits across generic topologies. Our approach uses Reinforcement Learning (RL) techniques to achieve near-optimal synthesis of permutation circuits up to 25 qubits. Rather than developing specialized models for individual topologies, we train a foundational model on a generic rectangular lattice, and employ masking mechanisms to dynamically select subsets of topologies during the synthesis. This enables the synthesis of permutation circuits on any topology that can be embedded within the rectangular lattice, without the need to re-train the model. In this paper we show results for 5x5 lattice and compare them to previous AI topology-oriented models and classical methods, showing that they outperform classical heuristics, and match previous specialized AI models, and performs synthesis even for topologies that were not seen during training. We further show that the model can be fine tuned to strengthen the performance for selected topologies of interest. This methodology allows a single trained model to efficiently synthesize circuits across diverse topologies, allowing its practical integration into transpilation workflows.', 'abstract_zh': '本文研究了适用于不同类型拓扑结构的置换电路合成与转换的人工智能方法。我们采用强化学习技术在通用矩形晶格上训练基础模型，通过掩码机制在合成过程中动态选择子拓扑结构，从而能够在不重新训练模型的情况下，合成嵌入在矩形晶格内的任意拓扑结构上的置换电路。文中展示了5x5晶格的结果，并将其与以往面向拓扑结构的AI模型和经典方法进行了比较，结果表明，该方法优于经典启发式方法，性能可与专门设计的AI模型匹敌，并能够对在训练过程中未见过的拓扑结构进行合成。此外，该模型还可以进一步微调以增强对选定目标拓扑结构的性能。这种方法使得经过训练的单个模型能够高效地在各种拓扑结构上合成电路，便于其实用集成到转换工作流中。', 'title_zh': 'AI方法在通用拓扑结构下的排列电路综合'}
{'arxiv_id': 'arXiv:2509.16010', 'title': 'Fed-PISA: Federated Voice Cloning via Personalized Identity-Style Adaptation', 'authors': 'Qi Wang, Shituo Ma, Guoxin Yu, Hanyang Peng, Yue Yu', 'link': 'https://arxiv.org/abs/2509.16010', 'abstract': "Voice cloning for Text-to-Speech (TTS) aims to generate expressive and personalized speech from text using limited data from a target speaker. Federated Learning (FL) offers a collaborative and privacy-preserving framework for this task, but existing approaches suffer from high communication costs and tend to suppress stylistic heterogeneity, resulting in insufficient personalization. To address these issues, we propose Fed-PISA, which stands for Federated Personalized Identity-Style Adaptation. To minimize communication costs, Fed-PISA introduces a disentangled Low-Rank Adaptation (LoRA) mechanism: the speaker's timbre is retained locally through a private ID-LoRA, while only a lightweight style-LoRA is transmitted to the server, thereby minimizing parameter exchange. To harness heterogeneity, our aggregation method, inspired by collaborative filtering, is introduced to create custom models for each client by learning from stylistically similar peers. Experiments show that Fed-PISA improves style expressivity, naturalness, and speaker similarity, outperforming standard federated baselines with minimal communication costs.", 'abstract_zh': 'federated personalized identity-style adaptation for voice cloning in text-to-speech', 'title_zh': 'Fed-PISA：个性化身份风格适应的联邦语音克隆'}
{'arxiv_id': 'arXiv:2509.15965', 'title': 'RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation', 'authors': 'Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing Xie, Ke Yang, Bo Dai, Zhexuan Xu, Xiangyuan Wang, Xu Fu, Zhihao Liu, Kang Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, Yu Wang', 'link': 'https://arxiv.org/abs/2509.15965', 'abstract': "Reinforcement learning (RL) has demonstrated immense potential in advancing artificial general intelligence, agentic intelligence, and embodied intelligence. However, the inherent heterogeneity and dynamicity of RL workflows often lead to low hardware utilization and slow training on existing systems. In this paper, we present RLinf, a high-performance RL training system based on our key observation that the major roadblock to efficient RL training lies in system flexibility. To maximize flexibility and efficiency, RLinf is built atop a novel RL system design paradigm called macro-to-micro flow transformation (M2Flow), which automatically breaks down high-level, easy-to-compose RL workflows at both the temporal and spatial dimensions, and recomposes them into optimized execution flows. Supported by RLinf worker's adaptive communication capability, we devise context switching and elastic pipelining to realize M2Flow transformation, and a profiling-guided scheduling policy to generate optimal execution plans. Extensive evaluations on both reasoning RL and embodied RL tasks demonstrate that RLinf consistently outperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in end-to-end training throughput.", 'abstract_zh': '基于宏观到微观流转换的高性能强化学习训练系统RLinf', 'title_zh': 'RLinf：通过宏观到微观流转变实现的灵活高效的大型强化学习'}
{'arxiv_id': 'arXiv:2509.15964', 'title': 'MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework', 'authors': 'Tianyu Li, Yan Xin, Jianzhong, Zhang', 'link': 'https://arxiv.org/abs/2509.15964', 'abstract': 'Reliable channel estimation (CE) is fundamental for robust communication in dynamic wireless environments, where models must generalize across varying conditions such as signal-to-noise ratios (SNRs), the number of resource blocks (RBs), and channel profiles. Traditional deep learning (DL)-based methods struggle to generalize effectively across such diverse settings, particularly under multitask and zero-shot scenarios. In this work, we propose MoE-CE, a flexible mixture-of-experts (MoE) framework designed to enhance the generalization capability of DL-based CE methods. MoE-CE provides an appropriate inductive bias by leveraging multiple expert subnetworks, each specialized in distinct channel characteristics, and a learned router that dynamically selects the most relevant experts per input. This architecture enhances model capacity and adaptability without a proportional rise in computational cost while being agnostic to the choice of the backbone model and the learning algorithm. Through extensive experiments on synthetic datasets generated under diverse SNRs, RB numbers, and channel profiles, including multitask and zero-shot evaluations, we demonstrate that MoE-CE consistently outperforms conventional DL approaches, achieving significant performance gains while maintaining efficiency.', 'abstract_zh': '可靠的信道估计算法（CE）是动态无线环境下稳健通信的基础，要求模型能够在不同的条件如信噪比（SNR）、资源块数（RBs）和信道特性下有效泛化。传统的基于深度学习（DL）的方法在处理这种多样性的设置时，尤其是在多任务和零样本场景下，泛化能力较弱。在本文中，我们提出了一种灵活的混合专家（MoE）框架MoE-CE，以增强基于DL的CE方法的泛化能力。MoE-CE通过利用多个专门处理不同信道特性的专家子网络，并结合一个能够动态选择最相关专家的学习路由器，提供适当的归纳偏置。该架构在不显著增加计算成本的情况下提升了模型的容量和适应性，且与基础模型的选择和学习算法无关。通过在包含多任务和零样本评估的多种信噪比（SNR）、资源块数（RBs）和信道特性下生成的合成数据集上进行广泛实验，我们证明MoE-CE在保持效率的同时，始终优于传统的DL方法，取得了显著的性能提升。', 'title_zh': 'MoE-CE: 基于Mixture-of-Experts框架提升基于深度学习的信道估计泛化能力'}
{'arxiv_id': 'arXiv:2509.15952', 'title': 'Compose Yourself: Average-Velocity Flow Matching for One-Step Speech Enhancement', 'authors': 'Gang Yang, Yue Lei, Wenxin Tai, Jin Wu, Jia Chen, Ting Zhong, Fan Zhou', 'link': 'https://arxiv.org/abs/2509.15952', 'abstract': 'Diffusion and flow matching (FM) models have achieved remarkable progress in speech enhancement (SE), yet their dependence on multi-step generation is computationally expensive and vulnerable to discretization errors. Recent advances in one-step generative modeling, particularly MeanFlow, provide a promising alternative by reformulating dynamics through average velocity fields. In this work, we present COSE, a one-step FM framework tailored for SE. To address the high training overhead of Jacobian-vector product (JVP) computations in MeanFlow, we introduce a velocity composition identity to compute average velocity efficiently, eliminating expensive computation while preserving theoretical consistency and achieving competitive enhancement quality. Extensive experiments on standard benchmarks show that COSE delivers up to 5x faster sampling and reduces training cost by 40%, all without compromising speech quality. Code is available at this https URL.', 'abstract_zh': '扩散和流匹配（FM）模型在语音增强（SE）方面取得了显著进展，但其依赖多步生成计算量大且容易受到 discretization 错误的影响。近期的一步生成建模进展，特别是 MeanFlow，通过重新定义动力学为平均速度场提供了有前景的替代方案。在本文中，我们提出了一种针对SE的一步FM框架COSE。为了解决 MeanFlow 中 Jacobian-向量积（JVP）计算的高训练开销，我们引入了一种速度合成恒等式以高效地计算平均速度，从而在保留理论一致性的同时减少昂贵的计算并实现竞争力的增强效果。在标准基准上的 extensive 实验表明，COSE 在不牺牲语音质量的情况下，采样速度提高至原来的 5 倍并降低了 40% 的训练成本。代码可在以下链接获取：this https URL。', 'title_zh': '自我合成：平均速度流匹配的一步语音增强'}
{'arxiv_id': 'arXiv:2509.15942', 'title': 'ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching', 'authors': 'Graham Clyne, Guillaume Couairon, Guillaume Gastineau, Claire Monteleoni, Anastase Charantonis', 'link': 'https://arxiv.org/abs/2509.15942', 'abstract': 'Climate projections have uncertainties related to components of the climate system and their interactions. A typical approach to quantifying these uncertainties is to use climate models to create ensembles of repeated simulations under different initial conditions. Due to the complexity of these simulations, generating such ensembles of projections is computationally expensive. In this work, we present ArchesClimate, a deep learning-based climate model emulator that aims to reduce this cost. ArchesClimate is trained on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution of approximately 2.5x1.25 degrees. We train a flow matching model following ArchesWeatherGen, which we adapt to predict near-term climate. Once trained, the model generates states at a one-month lead time and can be used to auto-regressively emulate climate model simulations of any length. We show that for up to 10 years, these generations are stable and physically consistent. We also show that for several important climate variables, ArchesClimate generates simulations that are interchangeable with the IPSL model. This work suggests that climate model emulators could significantly reduce the cost of climate model simulations.', 'abstract_zh': '基于深度学习的气候模型模拟器ArchesClimate：降低气候模型仿真的成本', 'title_zh': 'ArchesClimate: 基于流匹配的概率性十年ensemble生成'}
{'arxiv_id': 'arXiv:2509.15908', 'title': 'An Equivariant Graph Network for Interpretable Nanoporous Materials Design', 'authors': 'Zhenhao Zhou, Salman Bin Kashif, Dawei Feng, Jin-Hu Dou, Kaihang Shi, Tao Deng, Zhenpeng Yao', 'link': 'https://arxiv.org/abs/2509.15908', 'abstract': 'Nanoporous materials hold promise for diverse sustainable applications, yet their vast chemical space poses challenges for efficient design. Machine learning offers a compelling pathway to accelerate the exploration, but existing models lack either interpretability or fidelity for elucidating the correlation between crystal geometry and property. Here, we report a three-dimensional periodic space sampling method that decomposes large nanoporous structures into local geometrical sites for combined property prediction and site-wise contribution quantification. Trained with a constructed database and retrieved datasets, our model achieves state-of-the-art accuracy and data efficiency for property prediction on gas storage, separation, and electrical conduction. Meanwhile, this approach enables the interpretation of the prediction and allows for accurate identification of significant local sites for targeted properties. Through identifying transferable high-performance sites across diverse nanoporous frameworks, our model paves the way for interpretable, symmetry-aware nanoporous materials design, which is extensible to other materials, like molecular crystals and beyond.', 'abstract_zh': '纳米多孔材料在多种可持续应用中展现出潜力，但其庞大的化学空间给高效设计带来了挑战。机器学习为加速探索提供了一条有吸引力的道路，但现有模型要么缺乏可解释性，要么无法准确揭示晶体几何结构与性能之间的关联。在这里，我们报告了一种三维周期空间抽样方法，该方法将大型纳米多孔结构分解为局部几何位点，以实施数量预测和位点贡献量化。通过使用构建的数据库和检索数据集进行训练，我们的模型在气体存储、分离和电传导性质预测方面达到了最先进的准确性和数据效率。同时，此方法允许对预测进行解释，并能够准确识别目标性质的关键局部位点。通过识别不同纳米多孔框架中可转移的高性能位点，我们的模型为基于解释和对称性的纳米多孔材料设计铺平了道路，这种方法也可扩展到其他材料，如分子晶体等。', 'title_zh': '兼具不变性与可解释性的纳米多孔材料设计图网络'}
{'arxiv_id': 'arXiv:2509.15895', 'title': 'From Data to Diagnosis: A Large, Comprehensive Bone Marrow Dataset and AI Methods for Childhood Leukemia Prediction', 'authors': 'Henning Höfener, Farina Kock, Martina Pontones, Tabita Ghete, David Pfrang, Nicholas Dickel, Meik Kunz, Daniela P. Schacherer, David A. Clunie, Andrey Fedorov, Max Westphal, Markus Metzler', 'link': 'https://arxiv.org/abs/2509.15895', 'abstract': 'Leukemia diagnosis primarily relies on manual microscopic analysis of bone marrow morphology supported by additional laboratory parameters, making it complex and time consuming. While artificial intelligence (AI) solutions have been proposed, most utilize private datasets and only cover parts of the diagnostic pipeline. Therefore, we present a large, high-quality, publicly available leukemia bone marrow dataset spanning the entire diagnostic process, from cell detection to diagnosis. Using this dataset, we further propose methods for cell detection, cell classification, and diagnosis prediction. The dataset comprises 246 pediatric patients with diagnostic, clinical and laboratory information, over 40 000 cells with bounding box annotations and more than 28 000 of these with high-quality class labels, making it the most comprehensive dataset publicly available. Evaluation of the AI models yielded an average precision of 0.96 for the cell detection, an area under the curve of 0.98, and an F1-score of 0.61 for the 33-class cell classification, and a mean F1-score of 0.90 for the diagnosis prediction using predicted cell counts. While the proposed approaches demonstrate their usefulness for AI-assisted diagnostics, the dataset will foster further research and development in the field, ultimately contributing to more precise diagnoses and improved patient outcomes.', 'abstract_zh': '白血病诊断主要依赖于骨髓形态的 MANUAL 显微分析，并结合其他实验室参数，这使得过程复杂且耗时。尽管人工智能（AI）解决方案已被提出，但大多数方案仅利用私有数据集并仅覆盖诊断流程的部分环节。因此，我们提供了一个大规模、高质量、公开可用的白血病骨髓数据集，涵盖从细胞检测到诊断的整个诊断过程。利用该数据集，进一步提出了细胞检测、细胞分类和诊断预测的方法。该数据集包含246名儿科患者的诊断、临床和实验室信息，超过40000个带有边界框注释的细胞，其中超过28000个具有高质量类别标签，使其成为迄今为止最全面的公开数据集。对AI模型的评估结果显示，细胞检测的平均精度为0.96，ROC曲线下面积为0.98，33类细胞分类的F1分数为0.61，利用预测的细胞计数进行诊断预测的平均F1分数为0.90。虽然提出的方案证明了其在AI辅助诊断方面的有效性，但该数据集将促进该领域进一步的研究和开发，最终有助于更精确的诊断和改善患者预后。', 'title_zh': '从数据到诊断：儿童白血病预测的大规模综合骨髓数据集和AI方法'}
{'arxiv_id': 'arXiv:2509.15883', 'title': 'RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning', 'authors': 'Xiaosheng Long, Hanyu Wang, Zhentao Song, Kun Luo, Hongde Liu', 'link': 'https://arxiv.org/abs/2509.15883', 'abstract': 'Recent retrieval-augmented image captioning methods incorporate external knowledge to compensate for the limitations in comprehending complex scenes. However, current approaches face challenges in relation modeling: (1) the representation of semantic prompts is too coarse-grained to capture fine-grained relationships; (2) these methods lack explicit modeling of image objects and their semantic relationships. To address these limitations, we propose RACap, a relation-aware retrieval-augmented model for image captioning, which not only mines structured relation semantics from retrieval captions, but also identifies heterogeneous objects from the image. RACap effectively retrieves structured relation features that contain heterogeneous visual information to enhance the semantic consistency and relational expressiveness. Experimental results show that RACap, with only 10.8M trainable parameters, achieves superior performance compared to previous lightweight captioning models.', 'abstract_zh': '近期的关系 Awareness 推理增强图像字幕方法通过引入外部知识来弥补理解复杂场景的局限性。然而，当前的方法在关系建模方面面临挑战：(1) 语义提示的表示过于粗粒度，无法捕捉细微的关系；(2) 这些方法缺乏对图像对象及其语义关系的明确建模。为了解决这些限制，我们提出了 RACap，一种关系 Awareness 的推理增强图像字幕模型，该模型不仅从检索字幕中挖掘结构化关系语义，还从图像中识别异构对象。RACap 有效地检索包含异构视觉信息的结构化关系特征，以增强语义一致性及关系表达能力。实验结果表明，RACap 大量参数仅为 10.8M，其性能优于先前的轻量级字幕生成模型。', 'title_zh': 'RACap：关系意识的提示用于轻量级检索增强图像 captioning'}
{'arxiv_id': 'arXiv:2509.15872', 'title': 'DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism Prediction', 'authors': 'Manajit Das, Ajnabiul Hoque, Mayank Baranwal, Raghavan B. Sunoj', 'link': 'https://arxiv.org/abs/2509.15872', 'abstract': 'Prediction of complete step-by-step chemical reaction mechanisms (CRMs) remains a major challenge. Whereas the traditional approaches in CRM tasks rely on expert-driven experiments or costly quantum chemical computations, contemporary deep learning (DL) alternatives ignore key intermediates and mechanistic steps and often suffer from hallucinations. We present DeepMech, an interpretable graph-based DL framework employing atom- and bond-level attention, guided by generalized templates of mechanistic operations (TMOps), to generate CRMs. Trained on our curated ReactMech dataset (~30K CRMs with 100K atom-mapped and mass-balanced elementary steps), DeepMech achieves 98.98+/-0.12% accuracy in predicting elementary steps and 95.94+/-0.21% in complete CRM tasks, besides maintaining high fidelity even in out-of-distribution scenarios as well as in predicting side and/or byproducts. Extension to multistep CRMs relevant to prebiotic chemistry, demonstrates the ability of DeepMech in effectively reconstructing pathways from simple primordial substrates to complex biomolecules such as serine and aldopentose. Attention analysis identifies reactive atoms/bonds in line with chemical intuition, rendering our model interpretable and suitable for reaction design.', 'abstract_zh': '深度机械：基于图的解 interpretable graph-based DL framework 用于预测完整的多步骤化学反应机制', 'title_zh': 'DeepMech：一种化学反应机制预测的机器学习框架'}
{'arxiv_id': 'arXiv:2509.15857', 'title': 'EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network', 'authors': 'Rikuto Kotoge, Zheng Chen, Tasuku Kimura, Yasuko Matsubara, Takufumi Yanagisawa, Haruhiko Kishima, Yasushi Sakurai', 'link': 'https://arxiv.org/abs/2509.15857', 'abstract': 'Dynamic GNNs, which integrate temporal and spatial features in Electroencephalography (EEG) data, have shown great potential in automating seizure detection. However, fully capturing the underlying dynamics necessary to represent brain states, such as seizure and non-seizure, remains a non-trivial task and presents two fundamental challenges. First, most existing dynamic GNN methods are built on temporally fixed static graphs, which fail to reflect the evolving nature of brain connectivity during seizure progression. Second, current efforts to jointly model temporal signals and graph structures and, more importantly, their interactions remain nascent, often resulting in inconsistent performance. To address these challenges, we present the first theoretical analysis of these two problems, demonstrating the effectiveness and necessity of explicit dynamic modeling and time-then-graph dynamic GNN method. Building on these insights, we propose EvoBrain, a novel seizure detection model that integrates a two-stream Mamba architecture with a GCN enhanced by Laplacian Positional Encoding, following neurological insights. Moreover, EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes and edges to evolve over time. Our contributions include (a) a theoretical analysis proving the expressivity advantage of explicit dynamic modeling and time-then-graph over other approaches, (b) a novel and efficient model that significantly improves AUROC by 23% and F1 score by 30%, compared with the dynamic GNN baseline, and (c) broad evaluations of our method on the challenging early seizure prediction tasks.', 'abstract_zh': '动态GNN在电生理数据（EEG）中综合时空特征，展现了在自动检测发作方面的巨大潜力。然而，完全捕捉代表大脑状态（如发作和非发作）所需的根本动态仍是一项艰巨任务，并提出了两个基本挑战。首先，现有的大多数动态GNN方法基于时间固定的静态图，无法反映发作进展情况中脑连接性的演变性质。其次，当前联合建模时间信号和图结构，尤其是它们的相互作用的努力仍处于初级阶段，通常会导致性能不一致。为解决这些挑战，我们首次对这两个问题进行了理论分析，证明了明确动态建模和时间-然后-图动态GNN方法的有效性和必要性。在此基础上，我们提出了EvoBrain，这是一种新颖的发作检测模型，该模型将两流Mamba架构与通过拉普拉斯位置编码增强的GCN相结合，遵循神经科学见解。此外，EvoBrain 明确地引入了动态图结构，使得节点和边随时间演变。我们的贡献包括：(a) 对明确动态建模和时间-然后-图动态GNN方法相对于其他方法的表达能力优势进行理论分析；(b) 提出了一个新颖且高效的模型，与动态GNN基线相比，显著提高了AUROC 23% 和 F1分数 30%；(c) 在挑战性的早期发作预测任务中广泛评估了我们的方法。', 'title_zh': 'EvoBrain: 动态多通道脑电图图建模用于时变脑网络'}
{'arxiv_id': 'arXiv:2509.15812', 'title': 'Diversity of Structured Domains via k-Kemeny Scores', 'authors': 'Piotr Faliszewski, Krzysztof Sornat, Stanisław Szufa, Tomasz Wąs', 'link': 'https://arxiv.org/abs/2509.15812', 'abstract': 'In the k-Kemeny problem, we are given an ordinal election, i.e., a collection of votes ranking the candidates from best to worst, and we seek the smallest number of swaps of adjacent candidates that ensure that the election has at most k different rankings. We study this problem for a number of structured domains, including the single-peaked, single-crossing, group-separable, and Euclidean ones. We obtain two kinds of results: (1) We show that k-Kemeny remains intractable under most of these domains, even for k=2, and (2) we use k-Kemeny to rank these domains in terms of their diversity.', 'abstract_zh': '在k-Kemeny问题中，给定一个序选举，即一系列将候选人的排名从最好到最差的选票，我们寻求使选举最多有k种不同排名所需的相邻候选人交换的最小次数。我们研究了包括单峰型、单交越型、群体可分型和欧几里得型在内的多种结构化领域。我们获得了两类结果：(1) 我们证明在大多数这些领域中，即使对于k=2，k-Kemeny问题仍然是不可计算的；(2) 我们使用k-Kemeny来衡量这些领域的多样性。', 'title_zh': '基于k-Kemeny评分的结构化领域的多样性'}
{'arxiv_id': 'arXiv:2509.15810', 'title': 'Instance Generation for Meta-Black-Box Optimization through Latent Space Reverse Engineering', 'authors': 'Chen Wang, Zeyuan Ma, Zhiguang Cao, Yue-Jiao Gong', 'link': 'https://arxiv.org/abs/2509.15810', 'abstract': "To relieve intensive human-expertise required to design optimization algorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage generalization strength of meta-learning to train neural network-based algorithm design policies over a predefined training problem set, which automates the adaptability of the low-level optimizers on unseen problem instances. Currently, a common training problem set choice in existing MetaBBOs is well-known benchmark suites CoCo-BBOB. Although such choice facilitates the MetaBBO's development, problem instances in CoCo-BBOB are more or less limited in diversity, raising the risk of overfitting of MetaBBOs, which might further results in poor generalization. In this paper, we propose an instance generation approach, termed as \\textbf{LSRE}, which could generate diverse training problem instances for MetaBBOs to learn more generalizable policies. LSRE first trains an autoencoder which maps high-dimensional problem features into a 2-dimensional latent space. Uniform-grid sampling in this latent space leads to hidden representations of problem instances with sufficient diversity. By leveraging a genetic-programming approach to search function formulas with minimal L2-distance to these hidden representations, LSRE reverse engineers a diversified problem set, termed as \\textbf{Diverse-BBO}. We validate the effectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe their generalization performances on either synthetic or realistic scenarios. Extensive experimental results underscore the superiority of Diverse-BBO to existing training set choices in MetaBBOs. Further ablation studies not only demonstrate the effectiveness of design choices in LSRE, but also reveal interesting insights on instance diversity and MetaBBO's generalization.", 'abstract_zh': '改进元黑盒优化中的实例生成方法以提升泛化能力：LSRE和Diverse-BBO', 'title_zh': '通过潜在空间逆向工程实现元黑箱优化的实例生成'}
{'arxiv_id': 'arXiv:2509.15803', 'title': 'CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models', 'authors': 'Fangjian Shen, Zifeng Liang, Chao Wang, Wushao Wen', 'link': 'https://arxiv.org/abs/2509.15803', 'abstract': 'Text-to-image (T2I) models exhibit a significant yet under-explored "brand bias", a tendency to generate contents featuring dominant commercial brands from generic prompts, posing ethical and legal risks. We propose CIDER, a novel, model-agnostic framework to mitigate bias at inference-time through prompt refinement to avoid costly retraining. CIDER uses a lightweight detector to identify branded content and a Vision-Language Model (VLM) to generate stylistically divergent alternatives. We introduce the Brand Neutrality Score (BNS) to quantify this issue and perform extensive experiments on leading T2I models. Results show CIDER significantly reduces both explicit and implicit biases while maintaining image quality and aesthetic appeal. Our work offers a practical solution for more original and equitable content, contributing to the development of trustworthy generative AI.', 'abstract_zh': 'Text-to-image (T2I) 模型表现出一种重要但尚未充分探索的“品牌偏差”，即从通用提示生成内容时倾向于展示主导性商业品牌的趋势，这带来了伦理和法律风险。我们提出了一种新的、模型无关的框架 CIDER，在推理时通过提示优化来减轻偏差，避免重新训练的成本。CIDER 使用轻量级检测器识别品牌化内容，并使用视觉语言模型 (VLM) 生成风格迥异的替代方案。我们引入品牌中立得分 (BNS) 来量化这一问题，并在领先 T2I 模型上进行广泛实验。结果表明，CIDER 显著减少了显性和隐性的偏差，同时保持了图像质量和美学吸引力。我们的工作提供了一种实用的解决方案，以产生更为原创和公平的内容，促进了可信赖生成 AI 的发展。', 'title_zh': 'CIDER：因果治疗偏爱文字转图像模型'}
{'arxiv_id': 'arXiv:2509.15796', 'title': 'Monte Carlo Tree Diffusion with Multiple Experts for Protein Design', 'authors': 'Xuefeng Liu, Mingxuan Cao, Songhao Jiang, Xiao Luo, Xiaotian Duan, Mengdi Wang, Tobin R. Sosnick, Jinbo Xu, Rick Stevens', 'link': 'https://arxiv.org/abs/2509.15796', 'abstract': 'The goal of protein design is to generate amino acid sequences that fold into functional structures with desired properties. Prior methods combining autoregressive language models with Monte Carlo Tree Search (MCTS) struggle with long-range dependencies and suffer from an impractically large search space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts, which integrates masked diffusion models with tree search to enable multi-token planning and efficient exploration. Unlike autoregressive planners, MCTD-ME uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine, jointly revising multiple positions and scaling to large sequence spaces. It further leverages experts of varying capacities to enrich exploration, guided by a pLDDT-based masking schedule that targets low-confidence regions while preserving reliable residues. We propose a novel multi-expert selection rule (PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and unguided baselines in both sequence recovery (AAR) and structural similarity (scTM), with gains increasing for longer proteins and benefiting from multi-expert guidance. More generally, the framework is model-agnostic and applicable beyond inverse folding, including de novo protein engineering and multi-objective molecular generation.', 'abstract_zh': '蛋白质设计的目标是生成能够折叠成具有所需性质的功能结构的氨基酸序列。先前将自回归语言模型与蒙特卡洛树搜索（MCTS）结合的方法在处理长程依赖关系时存在问题，并且搜索空间过大。我们提出了MCTD-ME（蒙特卡洛树扩散与多专家集成），这是一种将掩码扩散模型与树搜索相结合的方法，以实现多标记规划和高效的探索。与自回归规划器不同，MCTD-ME 使用生物物理精确度增强的扩散去噪作为展开引擎，可以同时修订多个位置，并能够扩展到大型序列空间。此外，MCTD-ME 利用不同能力的专家来丰富探索，并通过基于 pLDDT 的遮蔽计划表，在保留可靠残基的同时针对低置信度区域进行指导。我们提出了一个新颖的多专家选择规则（PH-UCT-ME），将其扩展到专家集成中。在逆折叠任务（CAMEO 和 PDB 基准测试）中，MCTD-ME 在序列恢复（AAR）和结构相似性（scTM）方面超越了单专家和无引导基线，并且对于更长的蛋白质，这种优势更为明显，且得益于多专家指导。更广泛地说，该框架是模型无关的，并且可以应用于逆折叠之外的领域，包括从头蛋白质工程和多目标分子生成。', 'title_zh': '利用多种专家的蒙特卡洛树扩散方法进行蛋白质设计'}
{'arxiv_id': 'arXiv:2509.15785', 'title': 'CBPNet: A Continual Backpropagation Prompt Network for Alleviating Plasticity Loss on Edge Devices', 'authors': 'Runjie Shao, Boyu Diao, Zijia An, Ruiqi Liu, Yongjun Xu', 'link': 'https://arxiv.org/abs/2509.15785', 'abstract': "To meet the demands of applications like robotics and autonomous driving that require real-time responses to dynamic environments, efficient continual learning methods suitable for edge devices have attracted increasing attention. In this transition, using frozen pretrained models with prompts has become a mainstream strategy to combat catastrophic forgetting. However, this approach introduces a new critical bottleneck: plasticity loss, where the model's ability to learn new knowledge diminishes due to the frozen backbone and the limited capacity of prompt parameters. We argue that the reduction in plasticity stems from a lack of update vitality in underutilized parameters during the training process. To this end, we propose the Continual Backpropagation Prompt Network (CBPNet), an effective and parameter efficient framework designed to restore the model's learning vitality. We innovatively integrate an Efficient CBP Block that counteracts plasticity decay by adaptively reinitializing these underutilized parameters. Experimental results on edge devices demonstrate CBPNet's effectiveness across multiple benchmarks. On Split CIFAR-100, it improves average accuracy by over 1% against a strong baseline, and on the more challenging Split ImageNet-R, it achieves a state of the art accuracy of 69.41%. This is accomplished by training additional parameters that constitute less than 0.2% of the backbone's size, validating our approach.", 'abstract_zh': '适用于边缘设备的机器人和自主驾驶等应用所需的实时动态环境响应高效 continual 学习方法：冻结预训练模型的提示化策略及其瓶颈的解决', 'title_zh': 'CBPNet：一种用于缓解边缘设备中固有可塑性损失的连续反向传播提示网络'}
{'arxiv_id': 'arXiv:2509.15674', 'title': 'Inference Offloading for Cost-Sensitive Binary Classification at the Edge', 'authors': 'Vishnu Narayanan Moothedath, Umang Agarwal, Umeshraja N, James Richard Gross, Jaya Prakash Champati, Sharayu Moharir', 'link': 'https://arxiv.org/abs/2509.15674', 'abstract': "We focus on a binary classification problem in an edge intelligence system where false negatives are more costly than false positives. The system has a compact, locally deployed model, which is supplemented by a larger, remote model, which is accessible via the network by incurring an offloading cost. For each sample, our system first uses the locally deployed model for inference. Based on the output of the local model, the sample may be offloaded to the remote model. This work aims to understand the fundamental trade-off between classification accuracy and these offloading costs within such a hierarchical inference (HI) system. To optimize this system, we propose an online learning framework that continuously adapts a pair of thresholds on the local model's confidence scores. These thresholds determine the prediction of the local model and whether a sample is classified locally or offloaded to the remote model. We present a closed-form solution for the setting where the local model is calibrated. For the more general case of uncalibrated models, we introduce H2T2, an online two-threshold hierarchical inference policy, and prove it achieves sublinear regret. H2T2 is model-agnostic, requires no training, and learns in the inference phase using limited feedback. Simulations on real-world datasets show that H2T2 consistently outperforms naive and single-threshold HI policies, sometimes even surpassing offline optima. The policy also demonstrates robustness to distribution shifts and adapts effectively to mismatched classifiers.", 'abstract_zh': '我们在边缘智能系统中关注一个二元分类问题，其中假阴性比假阳性更昂贵。该系统包含一个紧凑的本地部署模型，并辅以一个较大且远程的模型，该模型可通过网络访问但需要承担卸载成本。对于每个样本，系统首先使用本地部署模型进行推理。基于本地模型的输出，样本可能会被卸载到远程模型。这项工作旨在理解此类分层推理（HI）系统中分类准确性与这些卸载成本之间的基本权衡。为了优化该系统，我们提出了一种在线学习框架，该框架持续调整本地模型置信得分上的阈值对。这些阈值决定了本地模型的预测结果以及样本是局部分类还是卸载到远程模型。我们为本地模型已校准的情况给出了闭式解。对于本地模型未校准的一般情况，我们引入了一种在线两阈值分层推理策略（H2T2），并证明其实现了亚线性后悔。H2T2具有模型无关性，无需训练，并在有限反馈的情况下于推理阶段进行学习。在实际数据集上的仿真结果表明，H2T2在不同阈值HI策略中表现更优，有时甚至超越了离线最优。该策略还展示了对分布偏移的良好鲁棒性，并能有效适应不匹配的分类器。', 'title_zh': '边缘设备上的成本敏感二分类推理卸载'}
{'arxiv_id': 'arXiv:2509.15666', 'title': 'TISDiSS: A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation', 'authors': 'Yongsheng Feng, Yuetonghui Xu, Jiehui Luo, Hongjia Liu, Xiaobing Li, Feng Yu, Wei Li', 'link': 'https://arxiv.org/abs/2509.15666', 'abstract': 'Source separation is a fundamental task in speech, music, and audio processing, and it also provides cleaner and larger data for training generative models. However, improving separation performance in practice often depends on increasingly large networks, inflating training and deployment costs. Motivated by recent advances in inference-time scaling for generative modeling, we propose Training-Time and Inference-Time Scalable Discriminative Source Separation (TISDiSS), a unified framework that integrates early-split multi-loss supervision, shared-parameter design, and dynamic inference repetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting inference depth without retraining additional models. We further provide systematic analyses of architectural and training choices and show that training with more inference repetitions improves shallow-inference performance, benefiting low-latency applications. Experiments on standard speech separation benchmarks demonstrate state-of-the-art performance with a reduced parameter count, establishing TISDiSS as a scalable and practical framework for adaptive source separation.', 'abstract_zh': '训练时和推理时可扩展的判别式源分离（TISDiSS）', 'title_zh': 'TISDiSS: 一种训练时和推理时可扩展的判别源分离框架'}
{'arxiv_id': 'arXiv:2509.15658', 'title': 'Chunk Knowledge Generation Model for Enhanced Information Retrieval: A Multi-task Learning Approach', 'authors': 'Jisu Kim, Jinhee Park, Changhyun Jeon, Jungwoo Choi, Keonwoo Kim, Minji Hong, Sehyun Kim', 'link': 'https://arxiv.org/abs/2509.15658', 'abstract': 'Traditional query expansion techniques for addressing vocabulary mismatch problems in information retrieval are context-sensitive and may lead to performance degradation. As an alternative, document expansion research has gained attention, but existing methods such as Doc2Query have limitations including excessive preprocessing costs, increased index size, and reliability issues with generated content. To mitigate these problems and seek more structured and efficient alternatives, this study proposes a method that divides documents into chunk units and generates textual data for each chunk to simultaneously improve retrieval efficiency and accuracy. The proposed "Chunk Knowledge Generation Model" adopts a T5-based multi-task learning structure that simultaneously generates titles and candidate questions from each document chunk while extracting keywords from user queries. This approach maximizes computational efficiency by generating and extracting three types of semantic information in parallel through a single encoding and two decoding processes. The generated data is utilized as additional information in the retrieval system. GPT-based evaluation on 305 query-document pairs showed that retrieval using the proposed model achieved 95.41% accuracy at Top@10, demonstrating superior performance compared to document chunk-level retrieval. This study contributes by proposing an approach that simultaneously generates titles and candidate questions from document chunks for application in retrieval pipelines, and provides empirical evidence applicable to large-scale information retrieval systems by demonstrating improved retrieval accuracy through qualitative evaluation.', 'abstract_zh': '传统的基于查询扩展的信息检索中解决词汇匹配问题的技术是上下文敏感的，可能会导致性能下降。作为替代方案，文档扩展研究逐渐受到关注，但现有方法如Doc2Query存在预处理成本高、索引大小增加及生成内容可靠性差等问题。为缓解这些问题并寻求更加结构化和高效的替代方案，本研究提出了一种方法，即将文档划分为块单元，为每个块生成文本数据，同时提高检索效率和准确性。提出的“块知识生成模型”采用基于T5的多任务学习结构，同时从每个文档块中生成标题和候选问题，并从用户查询中提取关键词。该方法通过单一编码和两次解码过程并行生成和提取三种类型的语义信息，最大化计算效率。生成的数据被用作检索系统的额外信息。基于GPT的评估结果显示，使用所提模型的检索在Top@10准确率达到95.41%，表现出比文档块级别检索更好的性能。本研究通过提出一种能够同时从文档块中生成标题和候选问题的方法，并通过定性评估证明了在大规模信息检索系统中检索准确性的提高而做出贡献。', 'title_zh': '增強信息检索的片段知识生成模型：多任务学习方法'}
{'arxiv_id': 'arXiv:2509.15651', 'title': 'Toward Efficient Influence Function: Dropout as a Compression Tool', 'authors': 'Yuchen Zhang, Mohammad Mohammadi Amiri', 'link': 'https://arxiv.org/abs/2509.15651', 'abstract': "Assessing the impact the training data on machine learning models is crucial for understanding the behavior of the model, enhancing the transparency, and selecting training data. Influence function provides a theoretical framework for quantifying the effect of training data points on model's performance given a specific test data. However, the computational and memory costs of influence function presents significant challenges, especially for large-scale models, even when using approximation methods, since the gradients involved in computation are as large as the model itself. In this work, we introduce a novel approach that leverages dropout as a gradient compression mechanism to compute the influence function more efficiently. Our method significantly reduces computational and memory overhead, not only during the influence function computation but also in gradient compression process. Through theoretical analysis and empirical validation, we demonstrate that our method could preserves critical components of the data influence and enables its application to modern large-scale models.", 'abstract_zh': '评估训练数据对机器学习模型的影响对于理解模型行为、提高透明度和选择训练数据至关重要。影响函数提供了一种理论框架，用于量化特定测试数据给定条件下训练数据点对模型性能的影响。然而，影响函数的计算和内存成本对于大规模模型来说依然是重大挑战，即使使用近似方法，因为计算过程中涉及的梯度与模型本身大小相当。在本文中，我们提出了一种新颖的方法，利用dropout作为梯度压缩机制，以更高效的方式计算影响函数。该方法显著减少了计算和内存开销，不仅在影响函数计算过程中，也在梯度压缩过程中。通过理论分析和实证验证，我们证明该方法能够保留数据影响的关键成分，并使其能够应用于现代大规模模型。', 'title_zh': '向量 Efficient 影响函数： Dropout 作为一种压缩工具'}
{'arxiv_id': 'arXiv:2509.15641', 'title': 'Information Geometry of Variational Bayes', 'authors': 'Mohammad Emtiyaz Khan', 'link': 'https://arxiv.org/abs/2509.15641', 'abstract': "We highlight a fundamental connection between information geometry and variational Bayes (VB) and discuss its consequences for machine learning. Under certain conditions, a VB solution always requires estimation or computation of natural gradients. We show several consequences of this fact by using the natural-gradient descent algorithm of Khan and Rue (2023) called the Bayesian Learning Rule (BLR). These include (i) a simplification of Bayes' rule as addition of natural gradients, (ii) a generalization of quadratic surrogates used in gradient-based methods, and (iii) a large-scale implementation of VB algorithms for large language models. Neither the connection nor its consequences are new but we further emphasize the common origins of the two fields of information geometry and Bayes with a hope to facilitate more work at the intersection of the two fields.", 'abstract_zh': '我们强调信息几何与变分贝叶斯（VB）之间的基本联系，并讨论其对机器学习的影响。在某些条件下，VB解始终需要估计或计算自然梯度。我们通过Khan和Rue（2023）提出的称为贝叶斯学习规则（BLR）的自然梯度下降算法来展示这一事实的若干后果。这些包括（i）将贝叶斯规则简化为自然梯度的加法，（ii）扩展梯度方法中使用的二次替代目标，以及（iii）在大型语言模型中大规模实现VB算法。尽管这种联系及其后果并不是新的，但我们进一步强调这两领域——信息几何和贝叶斯——的共同起源，以期促进两领域交叉工作的更多研究。', 'title_zh': '变分贝叶斯的信息几何'}
{'arxiv_id': 'arXiv:2509.15591', 'title': 'Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification', 'authors': 'Zinan Lin, Enshu Liu, Xuefei Ning, Junyi Zhu, Wenyu Wang, Sergey Yekhanin', 'link': 'https://arxiv.org/abs/2509.15591', 'abstract': 'Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at this https URL. The project website is at this https URL.', 'abstract_zh': '生成模型、表示学习和分类是机器学习的三大核心问题，然而它们的最佳解决方案仍然基本独立。本文提出：统一的原则能否解决这三个问题？这样的统一可以简化机器学习管道，并促进任务间的更大协同。我们引入了隐空间分区网络（LZN）作为这一目标的一步。LZN的核心在于创建一个共享的高斯隐空间，用于编码所有任务的信息。每种数据类型（例如，图像、文本、标签）都配备了编码器和解码器，分别将样本映射到不交的隐空间区域以及将隐空间映射回数据。机器学习任务被表示为这些编码器和解码器的组合：例如，条件图像生成使用标签编码器和图像解码器；图像嵌入使用图像编码器；分类使用图像编码器和标签解码器。我们通过三个日益复杂的场景展示了LZN的潜力：（1）LZN可以增强现有模型（图像生成）：与最佳的归一化流模型结合时，LZN在CIFAR10上的FID指标从2.76提高到2.59，无需修改训练目标。（2）LZN可以独立解决任务（表示学习）：LZN可以实施无辅助损失函数的无监督表示学习，分别在ImageNet下游线性分类任务上比MoCo和SimCLR方法高出9.3%和0.2%。（3）LZN可以同时解决多个任务（联合生成和分类）：通过图像和标签编码器/解码器，LZN设计上同时执行这两种任务，在CIFAR10上提高FID指标并达到最佳分类准确性。代码和训练模型可在以下网址获得：[此处链接]。项目网址为：[此处网址]。', 'title_zh': '隐区划网络：生成建模、表示学习和分类的统一原则'}
{'arxiv_id': 'arXiv:2509.15588', 'title': 'CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational Search via Query Reformulation and Rank Fusion', 'authors': 'Yu-Cheng Chang, Guan-Wei Yeo, Quah Eugene, Fan-Jie Shih, Yuan-Ching Kuo, Tsung-En Yu, Hung-Chun Hsu, Ming-Feng Tsai, Chuan-Ju Wang', 'link': 'https://arxiv.org/abs/2509.15588', 'abstract': 'The 2025 TREC Interactive Knowledge Assistance Track (iKAT) featured both interactive and offline submission tasks. The former requires systems to operate under real-time constraints, making robustness and efficiency as important as accuracy, while the latter enables controlled evaluation of passage ranking and response generation with pre-defined datasets. To address this, we explored query rewriting and retrieval fusion as core strategies. We built our pipelines around Best-of-$N$ selection and Reciprocal Rank Fusion (RRF) strategies to handle different submission tasks. Results show that reranking and fusion improve robustness while revealing trade-offs between effectiveness and efficiency across both tasks.', 'abstract_zh': '2025 TREC 交互式知识辅助赛道 (iKAT) 同时包含了交互式提交和离线提交任务。前者要求系统在实时条件下运行，因此系统的鲁棒性与效率与准确性同样重要，而后者则允许在预定义数据集上对段落排序和响应生成进行受控评估。为此，我们探索了查询重写和检索融合作为核心策略。我们围绕 Best-of-$N$ 选择和互惠Rank融合（RRF）策略构建了工作流程，以处理不同类型的提交任务。结果表明，再排序和融合提升了系统的鲁棒性，但在不同任务中有效性和效率之间存在权衡。', 'title_zh': 'CFDA & CLIP 在 TREC iKAT 2025：通过查询重写和排名融合增强个性化对话式搜索'}
{'arxiv_id': 'arXiv:2509.15570', 'title': 'Contrastive Learning with Spectrum Information Augmentation in Abnormal Sound Detection', 'authors': 'Xinxin Meng, Jiangtao Guo, Yunxiang Zhang, Shun Huang', 'link': 'https://arxiv.org/abs/2509.15570', 'abstract': 'The outlier exposure method is an effective approach to address the unsupervised anomaly sound detection problem. The key focus of this method is how to make the model learn the distribution space of normal data. Based on biological perception and data analysis, it is found that anomalous audio and noise often have higher frequencies. Therefore, we propose a data augmentation method for high-frequency information in contrastive learning. This enables the model to pay more attention to the low-frequency information of the audio, which represents the normal operational mode of the machine. We evaluated the proposed method on the DCASE 2020 Task 2. The results showed that our method outperformed other contrastive learning methods used on this dataset. We also evaluated the generalizability of our method on the DCASE 2022 Task 2 dataset.', 'abstract_zh': '异常暴露方法是解决无监督异常声检测问题的有效手段。该方法的关键在于使模型学习正常数据的概率分布空间。基于生物感知和数据分析，我们发现异常声音和噪声往往具有较高的频率。因此，我们提出了一种在对比学习中增强高频率信息的数据扩增方法，从而使模型更关注音频的低频信息，这些信息代表了机器的正常工作模式。我们在DCASE 2020 Task 2上评估了所提出的方法，结果显示该方法优于该数据集上使用的其他对比学习方法。我们还在DCASE 2022 Task 2数据集上评估了该方法的泛化能力。', 'title_zh': '基于光谱信息增强的对比学习在异常声音检测中的应用'}
{'arxiv_id': 'arXiv:2509.15566', 'title': 'BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent', 'authors': 'Shaojie Zhang, Ruoceng Zhang, Pei Fu, Shaokang Wang, Jiahui Yang, Xin Du, Shiqi Cui, Bin Qin, Ying Huang, Zhenbo Luo, Jian Luan', 'link': 'https://arxiv.org/abs/2509.15566', 'abstract': 'In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose "Blink-Think-Link" (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward -- the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates consistent state-of-the-art performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework\'s efficacy in developing advanced GUI Agents.', 'abstract_zh': '基于大脑启发的AI驱动人-界面交互框架：Blink-Think-Link（眨眼-思考-链接）', 'title_zh': 'BTL-UI：眨眼思考链接推理模型for GUI代理'}
{'arxiv_id': 'arXiv:2509.15485', 'title': 'mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment', 'authors': 'Ahmed Abdou', 'link': 'https://arxiv.org/abs/2509.15485', 'abstract': 'We present a simple, model-agnostic post-processing technique for fine-grained Arabic readability classification in the BAREC 2025 Shared Task (19 ordinal levels). Our method applies conformal prediction to generate prediction sets with coverage guarantees, then computes weighted averages using softmax-renormalized probabilities over the conformal sets. This uncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing high-penalty misclassifications to nearer levels. Our approach shows consistent QWK improvements of 1-3 points across different base models. In the strict track, our submission achieves QWK scores of 84.9\\%(test) and 85.7\\% (blind test) for sentence level, and 73.3\\% for document level. For Arabic educational assessment, this enables human reviewers to focus on a handful of plausible levels, combining statistical guarantees with practical usability.', 'abstract_zh': '我们提出了一种简单的、模型无关的后处理技术，用于处理BAREC 2025共享任务中的细粒度阿拉伯语可读性分类（19个顺序级别）。我们的方法利用似然预测生成具有覆盖率保证的预测集，然后使用softmax重新规范化概率来计算加权平均值。这种具有不确定性感知的解码能够通过减少高处罚错误分类到相邻级别来提高二次加权科帕（QWK）评分，我们的方法在不同基础模型上表现出1-3点的一致性QWK提升。在严格赛道中，我们的提交在句子级别实现了84.9%（测试集）和85.7%（盲测试集）的QWK分数，在文档级别实现了73.3%的QWK分数。对于阿拉伯语教育评估而言，这种技术使得人类评审员能够重点关注几个可能的级别，结合了统计保证和实际易用性。', 'title_zh': 'mucAI在BAREC共享任务2025：面向不确定性的阿拉伯语可读性评估'}
{'arxiv_id': 'arXiv:2509.15482', 'title': 'Comparing Computational Pathology Foundation Models using Representational Similarity Analysis', 'authors': 'Vaibhav Mishra, William Lotter', 'link': 'https://arxiv.org/abs/2509.15482', 'abstract': 'Foundation models are increasingly developed in computational pathology (CPath) given their promise in facilitating many downstream tasks. While recent studies have evaluated task performance across models, less is known about the structure and variability of their learned representations. Here, we systematically analyze the representational spaces of six CPath foundation models using techniques popularized in computational neuroscience. The models analyzed span vision-language contrastive learning (CONCH, PLIP, KEEP) and self-distillation (UNI (v2), Virchow (v2), Prov-GigaPath) approaches. Through representational similarity analysis using H&E image patches from TCGA, we find that UNI2 and Virchow2 have the most distinct representational structures, whereas Prov-Gigapath has the highest average similarity across models. Having the same training paradigm (vision-only vs. vision-language) did not guarantee higher representational similarity. The representations of all models showed a high slide-dependence, but relatively low disease-dependence. Stain normalization decreased slide-dependence for all models by a range of 5.5% (CONCH) to 20.5% (PLIP). In terms of intrinsic dimensionality, vision-language models demonstrated relatively compact representations, compared to the more distributed representations of vision-only models. These findings highlight opportunities to improve robustness to slide-specific features, inform model ensembling strategies, and provide insights into how training paradigms shape model representations. Our framework is extendable across medical imaging domains, where probing the internal representations of foundation models can help ensure effective development and deployment.', 'abstract_zh': '基础模型在计算病理学（CPath）中的开发日益增多，这得益于它们在促进下游任务方面的潜力。尽管最近的研究评估了不同模型的任务性能，但关于它们学习到的表示结构和变异性知之甚少。在这里，我们使用神经科学中普及的技术系统分析了六个CPath基础模型的表示空间。分析的模型包括视觉-语言对比学习（CONCH、PLIP、KEEP）和自蒸馏（UNI (v2)、Virchow (v2)、Prov-GigaPath）方法。通过使用TCGA的HE图像 patches 进行表示相似性分析，我们发现UNI2和Virchow2具有最明显的表示结构差异，而Prov-GigaPath在模型间的平均相似性最高。同样的训练范式（仅视觉 vs. 视觉-语言）并不保证更高的表示相似性。所有模型的表示显示出较高的玻片依赖性，但相对较低的疾病依赖性。对于所有模型来说，染色标准化可以减少5.5%（CONCH）到20.5%（PLIP）的玻片依赖性。在固有维度方面，视觉-语言模型展示了相对紧凑的表示，而视觉-only模型则表现出更分散的表示。这些发现突显了提高对玻片特定特征鲁棒性、指导模型集成策略以及阐明训练范式如何塑造模型表示的机会。我们的框架可以在医学成像领域扩展，通过对基础模型内部表示的研究有助于确保其有效开发和部署。', 'title_zh': '使用表征相似性分析比较计算病理学基础模型'}
{'arxiv_id': 'arXiv:2509.15459', 'title': 'CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction', 'authors': 'Yiyi Liu, Chunyang Liu, Weiqin Jiao, Bojian Wu, Fashuai Li, Biao Xiong', 'link': 'https://arxiv.org/abs/2509.15459', 'abstract': 'We present \\textbf{CAGE} (\\textit{Continuity-Aware edGE}) network, a \\textcolor{red}{robust} framework for reconstructing vector floorplans directly from point-cloud density maps. Traditional corner-based polygon representations are highly sensitive to noise and incomplete observations, often resulting in fragmented or implausible layouts. Recent line grouping methods leverage structural cues to improve robustness but still struggle to recover fine geometric details. To address these limitations, we propose a \\textit{native} edge-centric formulation, modeling each wall segment as a directed, geometrically continuous edge. This representation enables inference of coherent floorplan structures, ensuring watertight, topologically valid room boundaries while improving robustness and reducing artifacts. Towards this design, we develop a dual-query transformer decoder that integrates perturbed and latent queries within a denoising framework, which not only stabilizes optimization but also accelerates convergence. Extensive experiments on Structured3D and SceneCAD show that \\textbf{CAGE} achieves state-of-the-art performance, with F1 scores of 99.1\\% (rooms), 91.7\\% (corners), and 89.3\\% (angles). The method also demonstrates strong cross-dataset generalization, underscoring the efficacy of our architectural innovations. Code and pretrained models will be released upon acceptance.', 'abstract_zh': 'CAGE（Continuity-Aware edGE）网络：一种直接从点云密度图重建向量楼层计划的稳健框架', 'title_zh': 'CAGE: 连续性感知边缘网络解锁稳健的平面图重建'}
{'arxiv_id': 'arXiv:2509.15448', 'title': 'Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems', 'authors': 'Saeed Amizadeh, Sara Abdali, Yinheng Li, Kazuhito Koishida', 'link': 'https://arxiv.org/abs/2509.15448', 'abstract': 'Transformers and their attention mechanism have been revolutionary in the field of Machine Learning. While originally proposed for the language data, they quickly found their way to the image, video, graph, etc. data modalities with various signal geometries. Despite this versatility, generalizing the attention mechanism to scenarios where data is presented at different scales from potentially different modalities is not straightforward. The attempts to incorporate hierarchy and multi-modality within transformers are largely based on ad hoc heuristics, which are not seamlessly generalizable to similar problems with potentially different structures. To address this problem, in this paper, we take a fundamentally different approach: we first propose a mathematical construct to represent multi-modal, multi-scale data. We then mathematically derive the neural attention mechanics for the proposed construct from the first principle of entropy minimization. We show that the derived formulation is optimal in the sense of being the closest to the standard Softmax attention while incorporating the inductive biases originating from the hierarchical/geometric information of the problem. We further propose an efficient algorithm based on dynamic programming to compute our derived attention mechanism. By incorporating it within transformers, we show that the proposed hierarchical attention mechanism not only can be employed to train transformer models in hierarchical/multi-modal settings from scratch, but it can also be used to inject hierarchical information into classical, pre-trained transformer models post training, resulting in more efficient models in zero-shot manner.', 'abstract_zh': '多模态多尺度Transformer的数学框架与最优注意力机制', 'title_zh': '多尺度自注意力机制：将神经注意力机制推广到多尺度问题'}
{'arxiv_id': 'arXiv:2509.15440', 'title': "Where Do I 'Add the Egg'?: Exploring Agency and Ownership in AI Creative Co-Writing Systems", 'authors': 'Dashiel Carrera, Jeb Thomas-Mitchell, Daniel Wigdor', 'link': 'https://arxiv.org/abs/2509.15440', 'abstract': "AI co-writing systems challenge long held ideals about agency and ownership in the creative process, thereby hindering widespread adoption. In order to address this, we investigate conceptions of agency and ownership in AI creative co-writing. Drawing on insights from a review of commercial systems, we developed three co-writing systems with identical functionality but distinct interface metaphors: agentic, tool-like, and magical. Through interviews with professional and non-professional writers (n = 18), we explored how these metaphors influenced participants' sense of control and authorship. Our analysis resulted in a taxonomy of agency and ownership subtypes and underscore how tool-like metaphors shift writers' expected points of control while agentic metaphors foreground conceptual contributions. We argue that interface metaphors not only guide expectations of control but also frame conceptions of authorship. We conclude with recommendations for the design of AI co-writing systems, emphasizing how metaphor shapes user experience and creative practice.", 'abstract_zh': 'AI协同写作系统挑战了创作过程中长久以来关于自主性和所有权的理想，从而阻碍了其广泛应用。为了应对这一挑战，我们研究了AI创造协同写作中的自主性和所有权观念。通过文献回顾，我们开发了三种功能相同但界面隐喻不同的协同写作系统：自主性隐喻、工具隐喻和魔幻隐喻。通过对专业和非专业作家（n=18）的访谈，我们探讨了这些隐喻如何影响参与者对控制感和归属感的认知。我们的分析产生了自主性和所有权子类型的分类，并强调了工具隐喻如何改变写作者期望的控制点，而自主性隐喻则突显概念性贡献的重要性。我们主张，界面隐喻不仅引导控制预期，也框定了创作身份的概念。我们最后提出了关于AI协同写作系统设计的建议，强调了隐喻如何塑造用户体验和创作实践。', 'title_zh': '在何处“加入鸡蛋”：探索AI创意共写系统中的自主权与所有权问题'}
{'arxiv_id': 'arXiv:2509.15439', 'title': 'Dual-Mode Visual System for Brain-Computer Interfaces: Integrating SSVEP and P300 Responses', 'authors': 'Ekgari Kasawala, Surej Mouli', 'link': 'https://arxiv.org/abs/2509.15439', 'abstract': 'In brain-computer interface (BCI) systems, steady-state visual evoked potentials (SSVEP) and P300 responses have achieved widespread implementation owing to their superior information transfer rates (ITR) and minimal training requirements. These neurophysiological signals have exhibited robust efficacy and versatility in external device control, demonstrating enhanced precision and scalability. However, conventional implementations predominantly utilise liquid crystal display (LCD)-based visual stimulation paradigms, which present limitations in practical deployment scenarios. This investigation presents the development and evaluation of a novel light-emitting diode (LED)-based dual stimulation apparatus designed to enhance SSVEP classification accuracy through the integration of both SSVEP and P300 paradigms. The system employs four distinct frequencies, 7 Hz, 8 Hz, 9 Hz, and 10 Hz, corresponding to forward, backward, right, and left directional controls, respectively. Oscilloscopic verification confirmed the precision of these stimulation frequencies. Real-time feature extraction was accomplished through the concurrent analysis of maximum Fast Fourier Transform (FFT) amplitude and P300 peak detection to ascertain user intent. Directional control was determined by the frequency exhibiting maximal amplitude characteristics. The visual stimulation hardware demonstrated minimal frequency deviation, with error differentials ranging from 0.15%to 0.20%across all frequencies. The implemented signal processing algorithm successfully discriminated all four stimulus frequencies whilst correlating them with their respective P300 event markers. Classification accuracy was evaluated based on correct task intention recognition. The proposed hybrid system achieved a mean classification accuracy of 86.25%, coupled with an average ITR of 42.08 bits per minute (bpm).', 'abstract_zh': '基于LED的双刺激装置在SSVEP和P300融合中的发展与评估', 'title_zh': '双模式视觉系统脑机接口：整合SSVEP和P300响应'}
{'arxiv_id': 'arXiv:2509.15437', 'title': 'Impact of Phonetics on Speaker Identity in Adversarial Voice Attack', 'authors': 'Daniyal Kabir Dar, Qiben Yan, Li Xiao, Arun Ross', 'link': 'https://arxiv.org/abs/2509.15437', 'abstract': 'Adversarial perturbations in speech pose a serious threat to automatic speech recognition (ASR) and speaker verification by introducing subtle waveform modifications that remain imperceptible to humans but can significantly alter system outputs. While targeted attacks on end-to-end ASR models have been widely studied, the phonetic basis of these perturbations and their effect on speaker identity remain underexplored. In this work, we analyze adversarial audio at the phonetic level and show that perturbations exploit systematic confusions such as vowel centralization and consonant substitutions. These distortions not only mislead transcription but also degrade phonetic cues critical for speaker verification, leading to identity drift. Using DeepSpeech as our ASR target, we generate targeted adversarial examples and evaluate their impact on speaker embeddings across genuine and impostor samples. Results across 16 phonetically diverse target phrases demonstrate that adversarial audio induces both transcription errors and identity drift, highlighting the need for phonetic-aware defenses to ensure the robustness of ASR and speaker recognition systems.', 'abstract_zh': 'adversarial perturbations在语音中的威胁通过对自动语音识别(ASR)和说话人验证引入难以察觉的波形修改，但可以显著改变系统输出。虽然端到端ASR模型的目标攻击已被广泛研究，但这些扰动的语音基础及其对说话人身份的影响仍需进一步探索。在本工作中，我们从语音基础层面分析 adversarial audio，并表明扰动利用了如元音集中化和辅音替代等系统性混乱。这些扭曲不仅导致转录错误，还降低了对说话人验证至关重要的语音线索，导致身份漂移。使用DeepSpeech作为我们的ASR目标，我们生成了针对特定目标短语的对抗性样本，并评估了其对真实样本和冒充样本说话人嵌入的影响。针对16个语音基础各异的目标短语的研究结果表明，对抗性音频既会导致转录错误，也会导致身份漂移，强调了需要语音意识防御以确保ASR和说话人识别系统的鲁棒性。', 'title_zh': '语音音素特征对面部识别在对抗性语音攻击中说话人身份影响的研究'}
{'arxiv_id': 'arXiv:2509.15419', 'title': "Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data", 'authors': 'Claudio Benzoni, Martina Langhals, Martin Boeker, Luise Modersohn, Máté E. Maros', 'link': 'https://arxiv.org/abs/2509.15419', 'abstract': "Regardless of the rapid development of artificial intelligence, abstractive summarisation is still challenging for sensitive and data-restrictive domains like medicine. With the increasing number of imaging, the relevance of automated tools for complex medical text summarisation is expected to become highly relevant. In this paper, we investigated the adaptation via fine-tuning process of a non-domain-specific abstractive summarisation encoder-decoder model family, and gave insights to practitioners on how to avoid over- and underfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological reports public dataset. For each model, we comprehensively evaluated two different checkpoints with varying sizes of the same training data. We monitored the models' performances with lexical and semantic metrics during the training history on the fixed-size validation set. PEGASUS exhibited different phases, which can be related to epoch-wise double-descent, or peak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger checkpoint led to a performance detriment. This work highlights the challenges and risks of fine-tuning models with high expressivity when dealing with scarce training data, and lays the groundwork for future investigations into more robust fine-tuning strategies for summarisation models in specialised domains.", 'abstract_zh': '不受人工智能快速发展的影响，医学等敏感和数据受限领域的抽象性总结问题仍然具有挑战性。随着影像数量的增加，自动化工具在复杂医学文本总结中的相关性预计会变得尤为重要。在本文中，我们调查了适应性调整非领域特定抽象总结编码-解码模型家族的过程，并为实践者提供了如何避免过拟合和欠拟合的见解。我们使用PEGASUS和PEGASUS-X，在一个中型放射学报告公开数据集上进行研究。对于每个模型，我们全面评估了两种不同检查点的表现，这些检查点具有相同大小的训练数据。我们在固定大小的验证集上监测模型在训练历史中的性能，使用词性和语义度量指标。PEGASUS表现出不同的阶段，这可以与每_epoch双重下降或峰值下降恢复行为相关。对于PEGASUS-X，我们发现使用较大的检查点会导致性能下降。本文强调了在稀缺训练数据的情况下调整高表达性的模型所面临的挑战和风险，并为未来关于总结模型在专业领域中更稳健的调整策略的研究奠定了基础。', 'title_zh': '基于深度学习和抽象总结的放射报告研究：PEGASUS模型家族在稀少数据情况下的适应性实证研究'}
{'arxiv_id': 'arXiv:2509.15393', 'title': 'Generating Part-Based Global Explanations Via Correspondence', 'authors': 'Kunal Rathore, Prasad Tadepalli', 'link': 'https://arxiv.org/abs/2509.15393', 'abstract': 'Deep learning models are notoriously opaque. Existing explanation methods often focus on localized visual explanations for individual images. Concept-based explanations, while offering global insights, require extensive annotations, incurring significant labeling cost. We propose an approach that leverages user-defined part labels from a limited set of images and efficiently transfers them to a larger dataset. This enables the generation of global symbolic explanations by aggregating part-based local explanations, ultimately providing human-understandable explanations for model decisions on a large scale.', 'abstract_zh': '基于用户定义部分标签的深度学习模型全局符号解释方法', 'title_zh': '基于对应关系的部件级全局解释生成'}
{'arxiv_id': 'arXiv:2509.15380', 'title': 'Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text: Development and Deployment in Real-World Scenarios', 'authors': 'Vera Pavlova, Mohammed Makhlouf', 'link': 'https://arxiv.org/abs/2509.15380', 'abstract': "Despite recent advancements in Multilingual Information Retrieval (MLIR), a significant gap remains between research and practical deployment. Many studies assess MLIR performance in isolated settings, limiting their applicability to real-world scenarios. In this work, we leverage the unique characteristics of the Quranic multilingual corpus to examine the optimal strategies to develop an ad-hoc IR system for the Islamic domain that is designed to satisfy users' information needs in multiple languages. We prepared eleven retrieval models employing four training approaches: monolingual, cross-lingual, translate-train-all, and a novel mixed method combining cross-lingual and monolingual techniques. Evaluation on an in-domain dataset demonstrates that the mixed approach achieves promising results across diverse retrieval scenarios. Furthermore, we provide a detailed analysis of how different training configurations affect the embedding space and their implications for multilingual retrieval effectiveness. Finally, we discuss deployment considerations, emphasizing the cost-efficiency of deploying a single versatile, lightweight model for real-world MLIR applications.", 'abstract_zh': '尽管在多语言信息检索（MLIR）方面取得了Recent advancements，研究与实际部署之间仍存在显著差距。许多研究在孤立环境中评估MLIR性能，限制了其在现实场景中的应用。本文利用《古兰经》多语言语料库的独特特征，探讨在伊斯兰领域开发适应多语言用户信息需求的即兴检索系统的最佳策略。我们准备了 eleven 个检索模型，采用四种训练方法：单语言、跨语言、以及结合跨语言和单语言技术的新型混合方法。在领域内数据集上的评估显示，混合方法在多种检索场景中取得了令人鼓舞的结果。此外，我们详细分析了不同训练配置如何影响嵌入空间及其对多语言检索效果的影响。最后，我们讨论了部署考虑，强调了一种通用且轻量级模型的成本效益对于实际多语言信息检索应用的重要性。', 'title_zh': '多语言伊斯兰文本信息检索的高效多功能模型：实际应用场景中的开发与部署'}
{'arxiv_id': 'arXiv:2509.15275', 'title': 'Partial Column Generation with Graph Neural Networks for Team Formation and Routing', 'authors': "Giacomo Dall'Olio, Rainer Kolisch, Yaoxin Wu", 'link': 'https://arxiv.org/abs/2509.15275', 'abstract': 'The team formation and routing problem is a challenging optimization problem with several real-world applications in fields such as airport, healthcare, and maintenance operations. To solve this problem, exact solution methods based on column generation have been proposed in the literature. In this paper, we propose a novel partial column generation strategy for settings with multiple pricing problems, based on predicting which ones are likely to yield columns with a negative reduced cost. We develop a machine learning model tailored to the team formation and routing problem that leverages graph neural networks for these predictions. Computational experiments demonstrate that applying our strategy enhances the solution method and outperforms traditional partial column generation approaches from the literature, particularly on hard instances solved under a tight time limit.', 'abstract_zh': '多定价问题下的部分列生成策略及其在团队形成与路由问题中的应用：基于负简并成本列预测的图神经网络机器学习模型', 'title_zh': '基于图神经网络的部分列生成团队形成与路径规划'}
{'arxiv_id': 'arXiv:2509.15270', 'title': 'PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images', 'authors': 'Emanuele Ricco, Elia Onofri, Lorenzo Cima, Stefano Cresci, Roberto Di Pietro', 'link': 'https://arxiv.org/abs/2509.15270', 'abstract': "A critical need has emerged for generative AI: attribution methods. That is, solutions that can identify the model originating AI-generated content. This feature, generally relevant in multimodal applications, is especially sensitive in commercial settings where users subscribe to paid proprietary services and expect guarantees about the source of the content they receive. To address these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images. PRISM is based on a radial reduction of the discrete Fourier transform that leverages amplitude and phase information to capture model-specific signatures. The output of the above process is subsequently clustered via linear discriminant analysis to achieve reliable model attribution in diverse settings, even if the model's internal details are inaccessible. To support our work, we construct PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN- and diffusion-based models. On this dataset, PRISM achieves an attribution accuracy of 92.04%. We additionally evaluate our method on four benchmarks from the literature, reaching an average accuracy of 81.60%. Finally, we evaluate our methodology also in the binary task of detecting real vs fake images, achieving an average accuracy of 88.41%. We obtain our best result on GenImage with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our results demonstrate the effectiveness of frequency-domain fingerprinting for cross-architecture and cross-dataset model attribution, offering a viable solution for enforcing accountability and trust in generative AI systems.", 'abstract_zh': '生成式AI Attribution方法的需求：一种基于相位增强径向图像签名映射的框架（PRISM）', 'title_zh': 'PRISM：基于相位增强径向图像签名映射的AI生成图像水印框架'}
{'arxiv_id': 'arXiv:2509.15267', 'title': 'Autoguided Online Data Curation for Diffusion Model Training', 'authors': 'Valeria Pais, Luis Oala, Daniele Faccio, Marco Aversa', 'link': 'https://arxiv.org/abs/2509.15267', 'abstract': 'The costs of generative model compute rekindled promises and hopes for efficient data curation. In this work, we investigate whether recently developed autoguidance and online data selection methods can improve the time and sample efficiency of training generative diffusion models. We integrate joint example selection (JEST) and autoguidance into a unified code base for fast ablation and benchmarking. We evaluate combinations of data curation on a controlled 2-D synthetic data generation task as well as (3x64x64)-D image generation. Our comparisons are made at equal wall-clock time and equal number of samples, explicitly accounting for the overhead of selection. Across experiments, autoguidance consistently improves sample quality and diversity. Early AJEST (applying selection only at the beginning of training) can match or modestly exceed autoguidance alone in data efficiency on both tasks. However, its time overhead and added complexity make autoguidance or uniform random data selection preferable in most situations. These findings suggest that while targeted online selection can yield efficiency gains in early training, robust sample quality improvements are primarily driven by autoguidance. We discuss limitations and scope, and outline when data selection may be beneficial.', 'abstract_zh': '生成模型计算成本重燃了高效数据整理的希望：本研究探讨了最近开发的自引导和在线数据选择方法是否能够提高生成扩散模型训练的时间效率和样本效率。我们将联合示例选择（JEST）和自引导整合到统一的代码库中，进行快速的消融实验和基准测试。我们在受控的2-D合成数据生成任务以及（3x64x64）-D图像生成中评估了数据整理策略的组合。我们在相同的时间和样本数量下进行比较，明确考虑了选择操作的开销。实验结果显示，自引导在样本质量和多样性方面始终表现更优。早期AJEST在两项任务中的数据效率上可与自引导单独使用相匹敌或略优，但其时间开销和增加的复杂性使其在大多数情况下不如自引导或均匀随机数据选择更优。这些发现表明，在早期训练中，目标导向的在线选择可以带来效率的提升，但稳健的样本质量改进主要由自引导驱动。我们讨论了限制和适用范围，并概述了数据选择可能有益的情况。', 'title_zh': '自引导在线数据整理用于扩散模型训练'}
{'arxiv_id': 'arXiv:2509.15259', 'title': 'IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders', 'authors': 'Liang Zhang, Hanyang Dong, Jia-Hong Gao, Yi Sun, Kuntao Xiao, Wanli Yang, Zhao Lv, Shurong Sheng', 'link': 'https://arxiv.org/abs/2509.15259', 'abstract': 'Deep learning-based EEG classification is crucial for the automated detection of neurological disorders, improving diagnostic accuracy and enabling early intervention. However, the low signal-to-noise ratio of EEG signals limits model performance, making feature selection (FS) vital for optimizing representations learned by neural network encoders. Existing FS methods are seldom designed specifically for EEG diagnosis; many are architecture-dependent and lack interpretability, limiting their applicability. Moreover, most rely on single-iteration data, resulting in limited robustness to variability. To address these issues, we propose IEFS-GMB, an Information Entropy-based Feature Selection method guided by a Gradient Memory Bank. This approach constructs a dynamic memory bank storing historical gradients, computes feature importance via information entropy, and applies entropy-based weighting to select informative EEG features. Experiments on four public neurological disease datasets show that encoders enhanced with IEFS-GMB achieve accuracy improvements of 0.64% to 6.45% over baseline models. The method also outperforms four competing FS techniques and improves model interpretability, supporting its practical use in clinical settings.', 'abstract_zh': '基于深度学习的EEG分类对于自动检测神经紊乱至关重要，能够提升诊断准确性并实现早期干预。然而，EEG信号的低信噪比限制了模型性能，使得特征选择（FS）对于优化神经网络编码器学习的表示至关重要。现有FS方法很少专门针对EEG诊断设计；许多方法依赖于特定架构且缺乏可解释性，限制了其适用性。此外，大多数方法依赖单次迭代数据，导致其对变异性具有有限的鲁棒性。为解决这些问题，我们提出IEFS-GMB方法，这是一种由梯度记忆库指导的信息熵基于特征选择方法。此方法构建动态记忆库存储历史梯度，通过信息熵计算特征重要性，并应用基于熵的加权选择具有信息量的EEG特征。实验结果显示，在四个公开的神经疾病数据集上，与基线模型相比，增强编码器的IEFS-GMB实现了0.64%至6.45%的准确性提升。该方法还优于四种竞争性FS技术，并提高了模型的可解释性，支持其实用于临床环境。', 'title_zh': 'IEFS-GMB：基于信息熵的Gradient Memory Bank-Guided特征选择方法在神经疾病EEG分类中的应用'}
{'arxiv_id': 'arXiv:2509.15258', 'title': 'Generative AI Meets Wireless Sensing: Towards Wireless Foundation Model', 'authors': 'Zheng Yang, Guoxuan Chi, Chenshu Wu, Hanyu Liu, Yuchong Gao, Yunhao Liu, Jie Xu, Tony Xiao Han', 'link': 'https://arxiv.org/abs/2509.15258', 'abstract': 'Generative Artificial Intelligence (GenAI) has made significant advancements in fields such as computer vision (CV) and natural language processing (NLP), demonstrating its capability to synthesize high-fidelity data and improve generalization. Recently, there has been growing interest in integrating GenAI into wireless sensing systems. By leveraging generative techniques such as data augmentation, domain adaptation, and denoising, wireless sensing applications, including device localization, human activity recognition, and environmental monitoring, can be significantly improved. This survey investigates the convergence of GenAI and wireless sensing from two complementary perspectives. First, we explore how GenAI can be integrated into wireless sensing pipelines, focusing on two modes of integration: as a plugin to augment task-specific models and as a solver to directly address sensing tasks. Second, we analyze the characteristics of mainstream generative models, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion models, and discuss their applicability and unique advantages across various wireless sensing tasks. We further identify key challenges in applying GenAI to wireless sensing and outline a future direction toward a wireless foundation model: a unified, pre-trained design capable of scalable, adaptable, and efficient signal understanding across diverse sensing tasks.', 'abstract_zh': '生成式人工智能（GenAI）在计算机视觉（CV）和自然语言处理（NLP）等领域取得了显著进展，展示了其合成高保真数据和提高泛化能力的潜力。近年来，将GenAI集成到无线传感系统中引起了广泛关注。通过利用生成技术，如数据增强、领域适应和去噪，无线传感应用，包括设备定位、人体活动识别和环境监测，可以得到显著改进。本文从两个互补的角度调查了GenAI与无线传感的融合。首先，我们探讨了GenAI如何集成到无线传感管道中，重点关注两种集成模式：作为一种插件来增强特定任务的模型，以及直接解决传感任务的求解器。其次，我们分析了主流生成模型的特性，如生成对抗网络（GANs）、变分自编码器（VAEs）和扩散模型，并讨论了它们在各种无线传感任务中的适用性和独特优势。我们进一步指出了将GenAI应用于无线传感的关键挑战，并概述了未来向无线基础模型发展的方向：一个统一的、预训练的设计，能够在多种传感任务中实现可扩展、适配和高效的信号理解。', 'title_zh': '生成式AI与无线传感的融合：迈向无线基础模型'}
{'arxiv_id': 'arXiv:2509.15256', 'title': 'A Multi-Scale Graph Neural Process with Cross-Drug Co-Attention for Drug-Drug Interactions Prediction', 'authors': 'Zimo Yan, Jie Zhang, Zheng Xie, Yiping Song, Hao Li', 'link': 'https://arxiv.org/abs/2509.15256', 'abstract': 'Accurate prediction of drug-drug interactions (DDI) is crucial for medication safety and effective drug development. However, existing methods often struggle to capture structural information across different scales, from local functional groups to global molecular topology, and typically lack mechanisms to quantify prediction confidence. To address these limitations, we propose MPNP-DDI, a novel Multi-scale Graph Neural Process framework. The core of MPNP-DDI is a unique message-passing scheme that, by being iteratively applied, learns a hierarchy of graph representations at multiple scales. Crucially, a cross-drug co-attention mechanism then dynamically fuses these multi-scale representations to generate context-aware embeddings for interacting drug pairs, while an integrated neural process module provides principled uncertainty estimation. Extensive experiments demonstrate that MPNP-DDI significantly outperforms state-of-the-art baselines on benchmark datasets. By providing accurate, generalizable, and uncertainty-aware predictions built upon multi-scale structural features, MPNP-DDI represents a powerful computational tool for pharmacovigilance, polypharmacy risk assessment, and precision medicine.', 'abstract_zh': '多尺度图神经过程框架在药物-药物相互作用预测中的应用：准确、泛化且不确定性感知的预测方法', 'title_zh': '多尺度图神经过程结合药物共注意力机制的药物-药物相互作用预测'}
{'arxiv_id': 'arXiv:2509.15246', 'title': 'GenCAD-3D: CAD Program Generation using Multimodal Latent Space Alignment and Synthetic Dataset Balancing', 'authors': 'Nomi Yu, Md Ferdous Alam, A. John Hart, Faez Ahmed', 'link': 'https://arxiv.org/abs/2509.15246', 'abstract': 'CAD programs, structured as parametric sequences of commands that compile into precise 3D geometries, are fundamental to accurate and efficient engineering design processes. Generating these programs from nonparametric data such as point clouds and meshes remains a crucial yet challenging task, typically requiring extensive manual intervention. Current deep generative models aimed at automating CAD generation are significantly limited by imbalanced and insufficiently large datasets, particularly those lacking representation for complex CAD programs. To address this, we introduce GenCAD-3D, a multimodal generative framework utilizing contrastive learning for aligning latent embeddings between CAD and geometric encoders, combined with latent diffusion models for CAD sequence generation and retrieval. Additionally, we present SynthBal, a synthetic data augmentation strategy specifically designed to balance and expand datasets, notably enhancing representation of complex CAD geometries. Our experiments show that SynthBal significantly boosts reconstruction accuracy, reduces the generation of invalid CAD models, and markedly improves performance on high-complexity geometries, surpassing existing benchmarks. These advancements hold substantial implications for streamlining reverse engineering and enhancing automation in engineering design. We will publicly release our datasets and code, including a set of 51 3D-printed and laser-scanned parts on our project site.', 'abstract_zh': 'CAD程序作为参数化的命令序列，编译成精确的3D几何结构，是准确高效工程设计过程的基础。从点云和网格等非参数化数据生成这些程序仍然是一个关键且具有挑战性的任务，通常需要大量的手工干预。当前旨在自动生成CAD的深度生成模型受到数据集不平衡和规模不足的限制，特别缺乏复杂CAD程序的表示。为了解决这一问题，我们引入了GenCAD-3D，这是一种利用对比学习对齐CAD和几何编码器的潜在嵌入的多模态生成框架，并结合潜在扩散模型进行CAD序列的生成和检索。此外，我们还提出了SynthBal，这是一种专门设计的合成数据增强策略，用于平衡和扩展数据集，显著增强复杂CAD几何结构的表示。我们的实验表明，SynthBal显著提高了重建精度，减少了无效CAD模型的生成，并在复杂几何结构上显著提升了性能，超越了现有基准。这些进步对简化逆向工程和增强工程设计自动化具有重要影响。我们将在项目站点上公开发布我们的数据集和代码，包括一套51个3D打印和激光扫描的部分。', 'title_zh': 'GenCAD-3D: 使用多模态潜在空间对齐和合成数据集平衡的CAD程序生成'}
{'arxiv_id': 'arXiv:2509.15238', 'title': 'Generating Plans for Belief-Desire-Intention (BDI) Agents Using Alternating-Time Temporal Logic (ATL)', 'authors': 'Dylan Léveillé', 'link': 'https://arxiv.org/abs/2509.15238', 'abstract': 'Belief-Desire-Intention (BDI) is a framework for modelling agents based on their beliefs, desires, and intentions. Plans are a central component of BDI agents, and define sequences of actions that an agent must undertake to achieve a certain goal. Existing approaches to plan generation often require significant manual effort, and are mainly focused on single-agent systems. As a result, in this work, we have developed a tool that automatically generates BDI plans using Alternating-Time Temporal Logic (ATL). By using ATL, the plans generated accommodate for possible competition or cooperation between the agents in the system. We demonstrate the effectiveness of the tool by generating plans for an illustrative game that requires agent collaboration to achieve a shared goal. We show that the generated plans allow the agents to successfully attain this goal.', 'abstract_zh': '信念-愿望-意图（BDI）框架基于智能体的信念、愿望和意图建模。计划是BDI智能体的核心组成部分，定义了智能体为实现特定目标必须采取的一系列动作。现有的计划生成方法往往需要大量的手动工作，并主要集中在单智能体系统上。因此，在这项工作中，我们开发了一个工具，使用交替时间逻辑（ATL）自动生成BDI计划。通过使用ATL，生成的计划考虑了系统中智能体之间可能的竞争或合作。我们通过为一个需要智能体合作以实现共同目标的示例游戏生成计划，展示了该工具的有效性，并证明生成的计划使智能体能够成功达成这一目标。', 'title_zh': '使用交替时间动态逻辑（ATL）为信念-欲望-意图（BDI）代理生成计划'}
{'arxiv_id': 'arXiv:2509.15236', 'title': 'ChannelFlow-Tools: A Standardized Dataset Creation Pipeline for 3D Obstructed Channel Flows', 'authors': 'Shubham Kavane, Kajol Kulkarni, Harald Koestler', 'link': 'https://arxiv.org/abs/2509.15236', 'abstract': 'We present ChannelFlow-Tools, a configuration-driven framework that standardizes the end-to-end path from programmatic CAD solid generation to ML-ready inputs and targets for 3D obstructed channel flows. The toolchain integrates geometry synthesis with feasibility checks, signed distance field (SDF) voxelization, automated solver orchestration on HPC (waLBerla LBM), and Cartesian resampling to co-registered multi-resolution tensors. A single Hydra/OmegaConf configuration governs all stages, enabling deterministic reproduction and controlled ablations. As a case study, we generate 10k+ scenes spanning Re=100-15000 with diverse shapes and poses. An end-to-end evaluation of storage trade-offs directly from the emitted artifacts, a minimal 3D U-Net at 128x32x32, and example surrogate models with dataset size illustrate that the standardized representations support reproducible ML training. ChannelFlow-Tools turns one-off dataset creation into a reproducible, configurable pipeline for CFD surrogate modeling.', 'abstract_zh': 'ChannelFlow-Tools: 一种面向3D受阻通道流的程序化CAD实体生成到ML准备输入的标准配置框架', 'title_zh': 'ChannelFlow-Tools: 一种标准的三维受阻通道流数据集创建管道'}
{'arxiv_id': 'arXiv:2509.15230', 'title': 'Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning', 'authors': 'Rutger Hendrix, Giovanni Patanè, Leonardo G. Russo, Simone Carnemolla, Giovanni Bellitto, Federica Proietto Salanitri, Concetto Spampinato, Matteo Pennisi', 'link': 'https://arxiv.org/abs/2509.15230', 'abstract': 'Foundation models have transformed multimedia analysis by enabling robust and transferable representations across diverse modalities and tasks. However, their static deployment conflicts with growing societal and regulatory demands -- particularly the need to unlearn specific data upon request, as mandated by privacy frameworks such as the GDPR. Traditional unlearning approaches, including retraining, activation editing, or distillation, are often computationally expensive, fragile, and ill-suited for real-time or continuously evolving systems. In this paper, we propose a paradigm shift: rethinking unlearning not as a retroactive intervention but as a built-in capability. We introduce a prompt-based learning framework that unifies knowledge acquisition and removal within a single training phase. Rather than encoding information in model weights, our approach binds class-level semantics to dedicated prompt tokens. This design enables instant unlearning simply by removing the corresponding prompt -- without retraining, model modification, or access to original data. Experiments demonstrate that our framework preserves predictive performance on retained classes while effectively erasing forgotten ones. Beyond utility, our method exhibits strong privacy and security guarantees: it is resistant to membership inference attacks, and prompt removal prevents any residual knowledge extraction, even under adversarial conditions. This ensures compliance with data protection principles and safeguards against unauthorized access to forgotten information, making the framework suitable for deployment in sensitive and regulated environments. Overall, by embedding removability into the architecture itself, this work establishes a new foundation for designing modular, scalable and ethically responsive AI models.', 'abstract_zh': '基础模型通过在多种模态和任务中提供 robust 和 transferable 的表示，转型了多媒体分析。然而，它们的静态部署与不断增长的社会和监管需求相冲突，特别是隐私框架（如GDPR）要求在特定数据请求时进行遗忘。传统的遗忘方法，包括重新训练、激活编辑或知识蒸馏，通常计算成本高昂、脆弱且不适合实时或不断演化的系统。在本文中，我们提出了一种范式的转变：重新思考遗忘不再是事后干预，而是内置的能力。我们引入了一种基于提示的学习框架，将知识的获取与移除统一在单个训练阶段中。不同于将信息编码在模型权重中，我们的方法将类级别的语义绑定到专门的提示标记。这种设计通过移除相应的提示即可实现即时遗忘，无需重新训练、修改模型或访问原始数据。实验表明，我们的框架在保留类别的预测性能方面保持不变，同时有效擦除遗忘的类别。除了实用性，我们的方法还具备强大的隐私和安全保证：它对成员推理攻击具有抵御能力，并且移除提示可以防止任何形式的剩余知识提取，即使在对抗条件下也是如此。这确保了遵守数据保护原则，并防止未经授权访问遗忘的信息，使该框架适用于敏感和受监管环境的部署。通过将可删除性嵌入到架构本身，本研究为设计模块化、可扩展且伦理响应的人工智能模型奠定了新的基础。', 'title_zh': '可预先忘记的模型：提示学习作为原生的脱识机制'}
