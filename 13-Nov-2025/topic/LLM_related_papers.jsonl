{'arxiv_id': 'arXiv:2511.09378', 'title': 'The 2025 Planning Performance of Frontier Large Language Models', 'authors': 'Augusto B. Corrêa, André G. Pereira, Jendrik Seipp', 'link': 'https://arxiv.org/abs/2511.09378', 'abstract': 'The capacity of Large Language Models (LLMs) for reasoning remains an active area of research, with the capabilities of frontier models continually advancing. We provide an updated evaluation of the end-to-end planning performance of three frontier LLMs as of 2025, where models are prompted to generate a plan from PDDL domain and task descriptions. We evaluate DeepSeek R1, Gemini 2.5 Pro, GPT-5 and as reference the planner LAMA on a subset of domains from the most recent Learning Track of the International Planning Competition. Our results show that on standard PDDL domains, the performance of GPT-5 in terms of solved tasks is competitive with LAMA. When the PDDL domains and tasks are obfuscated to test for pure reasoning, the performance of all LLMs degrades, though less severely than previously reported for other models. These results show substantial improvements over prior generations of LLMs, reducing the performance gap to planners on a challenging benchmark.', 'abstract_zh': '大型语言模型在推理能力方面的容量仍然是一个活跃的研究领域，前沿模型的能力不断进步。我们提供了截至2025年的三个前沿大型语言模型从PDDL领域和任务描述生成端到端计划的最新评估。我们在国际规划竞赛最近一轮的学习轨道的部分领域上，将DeepSeek R1、Gemini 2.5 Pro、GPT-5与参考规划器LAMA进行评估。结果显示，对于标准PDDL领域，GPT-5在解决任务方面的性能与LAMA相当。当对PDDL领域和任务进行混淆以测试纯粹的推理能力时，所有LLM的性能下降，但比之前报道的其他模型下降较少。这些结果表明，与前几代大型语言模型相比，取得了显著的改进，减少了在挑战性基准上与规划器的性能差距。', 'title_zh': '2025年前沿大规模语言模型规划绩效'}
{'arxiv_id': 'arXiv:2511.09363', 'title': 'BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems', 'authors': 'Ali Taheri, Alireza Taban, Sadegh Soudjani, Ashutosh Trivedi', 'link': 'https://arxiv.org/abs/2511.09363', 'abstract': 'Safety verification of dynamical systems via barrier certificates is essential for ensuring correctness in autonomous applications. Synthesizing these certificates involves discovering mathematical functions with current methods suffering from poor scalability, dependence on carefully designed templates, and exhaustive or incremental function-space searches. They also demand substantial manual expertise--selecting templates, solvers, and hyperparameters, and designing sampling strategies--requiring both theoretical and practical knowledge traditionally shared through linguistic reasoning rather than formalized methods.\nThis motivates a key question: can such expert reasoning be captured and operationalized by language models? We address this by introducing an LLM-based agentic framework for barrier certificate synthesis. The framework uses natural language reasoning to propose, refine, and validate candidate certificates, integrating LLM-driven template discovery with SMT-based verification, and supporting barrier-controller co-synthesis to ensure consistency between safety certificates and controllers.\nTo evaluate this capability, we introduce BarrierBench, a benchmark of 100 dynamical systems spanning linear, nonlinear, discrete-time, and continuous-time settings. Our experiments assess not only the effectiveness of LLM-guided barrier synthesis but also the utility of retrieval-augmented generation and agentic coordination strategies in improving its reliability and performance. Across these tasks, the framework achieves more than 90% success in generating valid certificates. By releasing BarrierBench and the accompanying toolchain, we aim to establish a community testbed for advancing the integration of language-based reasoning with formal verification in dynamical systems.\nThe benchmark is publicly available at this https URL', 'abstract_zh': '基于语言模型的动力学系统屏障证书合成对于自主应用的正确性验证是必不可少的。现有的合成方法存在可扩展性差、依赖精心设计的模板、以及需要对搜索空间进行穷举或增量搜索的问题。这些方法还要求大量的手动专业知识，包括选择模板、求解器和超参数，以及设计采样策略，这些知识通常是通过语言推理而非形式化方法来共享的。\n\n这促使一个关键问题的产生：是否可以由语言模型来捕捉和实现这种专家推理？我们通过引入基于大规模语言模型的自主框架来解决这个问题，该框架使用自然语言推理来提出、改进和验证候选的屏障证书，将LLM驱动的模板发现与基于SMT的验证相结合，并支持屏障控制器联合设计以确保安全证书和控制器的一致性。\n\n为了评估这一能力，我们引入了BarrierBench基准，其中包括100个动力学系统，涵盖线性、非线性、离散时间和连续时间设置。我们的实验不仅评估了由LLM引导的屏障合成的有效性，还评估了检索增强生成和自主协调策略如何提高其可靠性和性能。在这些任务中，框架在生成有效证书方面的成功率超过90%。通过发布BarrierBench和配套工具链，我们旨在建立一个社区测试平台，以推进基于语言推理与形式化验证的动态系统集成研究。\n\n基准可在以下链接获取：this https URL', 'title_zh': 'BarrierBench : 评估大型语言模型在动力系统安全性验证中的表现'}
{'arxiv_id': 'arXiv:2511.09287', 'title': 'From Model Training to Model Raising - A call to reform AI model training paradigms from post-hoc alignment to intrinsic, identity-based development', 'authors': 'Roland Aydin, Christian Cyron, Steve Bachelor, Ashton Anderson, Robert West', 'link': 'https://arxiv.org/abs/2511.09287', 'abstract': 'Current AI training methods align models with human values only after their core capabilities have been established, resulting in models that are easily misaligned and lack deep-rooted value systems. We propose a paradigm shift from "model training" to "model raising", in which alignment is woven into a model\'s development from the start. We identify several key components for this paradigm, all centered around redesigning the training corpus: reframing training data from a first-person perspective, recontextualizing information as lived experience, simulating social interactions, and scaffolding the ordering of training data. We expect that this redesign of the training corpus will lead to an early commitment to values from the first training token onward, such that knowledge, skills, and values are intrinsically much harder to separate. In an ecosystem in which large language model capabilities start overtaking human capabilities in many tasks, this seems to us like a critical need.', 'abstract_zh': '当前的AI训练方法在模型核心能力建立之后才使其与人类价值观相契合，导致模型容易出现偏差且缺乏深刻的价值体系。我们提出从“模型训练”转向“模型培养”的范式转变，在模型开发的初期就将其与人类价值观相融合。我们确定了这一范式的一些关键组成部分，所有这些都围绕着重新设计训练语料库：从第一人称视角重新构框架训练数据，重新构架信息作为生活体验，模拟社会互动，并搭建训练数据的层次结构。我们预计这种训练语料库的重新设计将使从第一个训练令牌开始就对价值观做出早期承诺，从而使知识、技能和价值观更加难以分离。在大语言模型能力逐渐超过人类能力的生态系统中，这似乎是一个至关重要的需求。', 'title_zh': '从模型训练到模型培育：呼吁从事后对齐转向内在的、基于身份的发展模式'}
{'arxiv_id': 'arXiv:2511.09158', 'title': 'Efficient Reasoning via Reward Model', 'authors': 'Yuhao Wang, Xiaopeng Li, Cheng Gong, Ziru Liu, Suiyun Zhang, Rui Liu, Xiangyu Zhao', 'link': 'https://arxiv.org/abs/2511.09158', 'abstract': "Reinforcement learning with verifiable rewards (RLVR) has been shown to enhance the reasoning capabilities of large language models (LLMs), enabling the development of large reasoning models (LRMs). However, LRMs such as DeepSeek-R1 and OpenAI o1 often generate verbose responses containing redundant or irrelevant reasoning step-a phenomenon known as overthinking-which substantially increases computational costs. Prior efforts to mitigate this issue commonly incorporate length penalties into the reward function, but we find they frequently suffer from two critical issues: length collapse and training collapse, resulting in sub-optimal performance. To address them, we propose a pipeline for training a Conciseness Reward Model (CRM) that scores the conciseness of reasoning path. Additionally, we introduce a novel reward formulation named Conciseness Reward Function (CRF) with explicit dependency between the outcome reward and conciseness score, thereby fostering both more effective and more efficient reasoning. From a theoretical standpoint, we demonstrate the superiority of the new reward from the perspective of variance reduction and improved convergence properties. Besides, on the practical side, extensive experiments on five mathematical benchmark datasets demonstrate the method's effectiveness and token efficiency, which achieves an 8.1% accuracy improvement and a 19.9% reduction in response token length on Qwen2.5-7B. Furthermore, the method generalizes well to other LLMs including Llama and Mistral. The implementation code and datasets are publicly available for reproduction: this https URL.", 'abstract_zh': '可验证奖励的强化学习（RLVR）已被证明能够增强大规模语言模型（LLMs）的推理能力，推动了大规模推理模型（LRMs）的发展。然而，DeepSeek-R1和OpenAI o1等LRMs常常生成冗长的回答，包含多余的或无关的推理步骤——这一现象被称为过度推理，显著增加了计算成本。先前的努力通常通过将长度惩罚纳入奖励函数来缓解这一问题，但我们发现它们经常遭受两个关键问题：长度坍缩和训练坍缩，导致性能不佳。为解决这些问题，我们提出了一种训练简洁性奖励模型（CRM）的管道，用于评估推理路径的简洁性。此外，我们引入了一种新的奖励公式——简洁性奖励函数（CRF），它明确地将结果奖励与简洁性分数相关联，从而促进更具效性和效率的推理。从理论上讲，我们从方差减少和改进收敛性质的角度证明了新奖励的优势。此外，通过在五个数学基准数据集上的广泛实验，我们验证了该方法的有效性和token效率，该方法在Qwen2.5-7B上实现了8.1%的准确率提升和19.9%的响应token长度减少。此外，该方法还很好地适用于其他LLMs，如Llama和Mistral。该实施代码和数据集已公开，可供重现：this https URL。', 'title_zh': '高效推理通过奖励模型'}
{'arxiv_id': 'arXiv:2511.09044', 'title': 'Advancing Autonomous Emergency Response Systems: A Generative AI Perspective', 'authors': 'Yousef Emami, Radha Reddy, Azadeh Pourkabirian, Miguel Gutierrez Gaitan', 'link': 'https://arxiv.org/abs/2511.09044', 'abstract': 'Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.', 'abstract_zh': '自主驾驶汽车（AVs）有望通过实现更快、更安全和更高效的响应来革命化应急服务。这一变革得益于人工智能（AI），尤其是一步步发展的强化学习（RL）技术，使得AV能够在复杂环境中导航并在实时情况下作出关键决策。然而，传统的RL范式往往表现出样本效率低和在动态应急场景中缺乏适应性的问题。本文回顾了下一代AV优化策略以解决这些问题。我们分析了从传统RL向通过生成合成数据增强的扩散模型（DM）-辅助RL转变的情况，虽然这种转变增加了计算成本但提高了策略鲁棒性。此外，我们探讨了大型语言模型（LLM）辅助的即境学习（ICL）新兴范式，提供了轻量级且可解释的替代方案，使其能够实现快速、即时的适应而无需重新训练。通过回顾自主驾驶智能、DM-辅助RL和LLM-辅助ICL的最新进展，本文从生成式AI视角提供了理解新一代自主应急响应系统的关键框架。', 'title_zh': '增强自主紧急响应系统：一个生成式AI视角'}
{'arxiv_id': 'arXiv:2511.09030', 'title': 'Solving a Million-Step LLM Task with Zero Errors', 'authors': 'Elliot Meyerson, Giuseppe Paolo, Roberto Dailey, Hormoz Shahrzad, Olivier Francon, Conor F. Hayes, Xin Qiu, Babak Hodjat, Risto Miikkulainen', 'link': 'https://arxiv.org/abs/2511.09030', 'abstract': 'LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.', 'abstract_zh': '大规模语言模型在推理、洞察和工具使用方面取得了显著突破，但将这些能力扩展到人类、组织和社会通常执行的延伸过程中依然遥不可及。模型存在持续的错误率，阻碍了规模扩展：例如，在汉诺塔基准域的最近实验中，过程在最多几百步后不可避免地陷入停滞。因此，尽管大规模语言模型研究通常仍然基于相对较少依赖逻辑步骤的任务进行 benchmark，但对大规模语言模型执行长距离任务的能力（或无能）的关注不断增强。本文描述了 MAKER，这是首个成功解决超过一百万大规模语言模型步骤且无错误的任务系统，并且原则上可以进一步扩展。该方法依赖于对任务的极端分解，使其可以由专注于微代理的子任务逐一解决。由于分解导致的高度模块化使得可以在每个步骤通过高效的多代理投票方案进行错误修正。这种极端分解与错误修正的结合使得规模扩展成为可能。因此，结果表明，与其依赖当前大规模语言模型的持续改进，大规模分解的代理过程（MDAP）可能提供了一种有效解决组织和社会级别问题的方法。', 'title_zh': '解决百万步语言模型任务零错误问题'}
{'arxiv_id': 'arXiv:2511.09005', 'title': 'AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines', 'authors': 'Alvin Chauhan', 'link': 'https://arxiv.org/abs/2511.09005', 'abstract': "Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.", 'abstract_zh': '尽管大型语言模型（LLMs）表现出色，但仍然努力从中提取更强的推理能力。基于搜索视角对LLM计算的解析，本文提出了一种系统性的框架以理解LLM推理和优化。具体而言，通过构建一个多层次的代理管道以确保在逐步、增量和顺序（GIS）的方式下遍历搜索空间，是增强推理的最佳途径。简而言之，高质量的推理就是一种受控的增量搜索。为了测试这种框架，我们研究了递归细化（RR）——一个迭代的自我批判、对抗性压力测试和整合关键反馈的过程——作为一种实际的方法来实施GIS搜索。我们设计了一个实验，将一个简单的线性管道与利用递归细化层的复杂且明确结构化的管道进行了比较。这些多层次的模型使用RAG驱动的语料库构建，反映了三位美国开国元勋（汉密尔顿、杰斐逊和麦迪逊）的历史人物特征，并被要求针对三个当代政治问题生成回应。模型性能通过两级评估方法进行评价：由LLM仲裁代理给出的定量评分和人类的定性判断。我们的结果显示，复杂的模型在所有九个测试案例中都优于简单的模型，平均仲裁输出得分为88.3比71.7。复杂的模型在分析深度、结构精细度和战略框架方面表现出更优的论据。我们得出结论，递归细化是一种稳健的架构特征，可以通过GIS搜索增强LLM推理。', 'title_zh': 'AI奠基人：多智能体管道中GIS搜索案例研究'}
{'arxiv_id': 'arXiv:2511.08947', 'title': 'AlphaCast: A Human Wisdom-LLM Intelligence Co-Reasoning Framework for Interactive Time Series Forecasting', 'authors': 'Xiaohan Zhang, Tian Gao, Mingyue Cheng, Bokai Pan, Ze Guo, Yaguo Liu, Xiaoyu Tao', 'link': 'https://arxiv.org/abs/2511.08947', 'abstract': 'Time series forecasting plays a critical role in high-stakes domains such as energy, healthcare, and climate. Although recent advances have improved accuracy, most approaches still treat forecasting as a static one-time mapping task, lacking the interaction, reasoning, and adaptability of human experts. This gap limits their usefulness in complex real-world environments. To address this, we propose AlphaCast, a human wisdom-large language model (LLM) intelligence co-reasoning framework that redefines forecasting as an interactive process. The key idea is to enable step-by-step collaboration between human wisdom and LLM intelligence to jointly prepare, generate, and verify forecasts. The framework consists of two stages: (1) automated prediction preparation, where AlphaCast builds a multi-source cognitive foundation comprising a feature set that captures key statistics and time patterns, a domain knowledge base distilled from corpora and historical series, a contextual repository that stores rich information for each time window, and a case base that retrieves optimal strategies via pattern clustering and matching; and (2) generative reasoning and reflective optimization, where AlphaCast integrates statistical temporal features, prior knowledge, contextual information, and forecasting strategies, triggering a meta-reasoning loop for continuous self-correction and strategy refinement. Extensive experiments on short- and long-term datasets show that AlphaCast consistently outperforms state-of-the-art baselines in predictive accuracy. Code is available at this repository: this https URL .', 'abstract_zh': '时间序列预测在能源、医疗和气候等领域扮演着关键角色。虽然近期进展提高了预测准确性，但大多数方法仍将预测视为静态的一次性映射任务，缺乏人类专家的互动、推理和适应性。这一差距限制了它们在复杂现实环境中的应用价值。为解决这一问题，我们提出了一种AlphaCast框架，它重新定义了预测为一个互动过程，结合了人类智慧和大规模语言模型（LLM）的智能协同推理。该框架包含两个阶段：（1）自动化预测准备阶段，AlphaCast构建一个多源认知基础，包括特征集、领域知识库、上下文存储库和案例库；（2）生成推理与反思优化阶段，AlphaCast整合统计时间特征、先验知识、上下文信息和预测策略，触发一个元推理循环，实现持续自我纠正和策略细化。在短期和长期数据集上的大量实验表明，AlphaCast在预测准确性方面始终优于最先进的基准方法。代码可在以下仓库获取：this https URL。', 'title_zh': 'AlphaCast：一种结合人类智慧与LLM智能的互动时间序列预测推理框架'}
{'arxiv_id': 'arXiv:2511.08873', 'title': 'UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models', 'authors': 'Shouang Wei, Min Zhang, Xin Lin, Bo Jiang, Kun Kuang, Zhongxiang Dai', 'link': 'https://arxiv.org/abs/2511.08873', 'abstract': "Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at this https URL.", 'abstract_zh': '大型语言模型（LLMs）正在从答案提供者转变为教育环境中的智能导师，然而当前的监督微调方法仅学习表面的教学模式，缺乏动态适应能力。最近的强化学习方法解决了这一局限性，但面临两个关键挑战。首先，它们仅根据学生是否产生正确输出来评估教学效果，无法区分学生在互动中是否真正理解或只是重复教师提供的答案。其次，它们无法通过交互对话实时感知学生不断变化的认知状态，从而无法动态调整教学策略以匹配学生的认知水平。我们提出了统一认知优化（UCO）方法来应对这些挑战。UCO 使用多轮次交互强化学习范式，其中的创新点在于两个协同的奖励函数：进步奖励捕捉学生认知的进步，评估学生是否真正从困惑到理解的转变，而支架奖励动态识别每个学生的最近发展区（ZPD），鼓励教师在这一区域内维持有效的教学。我们通过在 BigMath 和 MathTutorBench 基准上与 11 个基线模型进行对比来评估 UCO。实验结果表明，我们的 UCO 模型在所有同等规模的模型中表现最佳，并且达到了与高级闭源模型相当的性能。代码和数据可在以下网址获得。', 'title_zh': 'UCO：一种基于多轮交互强化学习的自适应教学方法，用于大型语言模型辅助教学'}
{'arxiv_id': 'arXiv:2511.08715', 'title': 'Bridging Natural Language and ASP: A Hybrid Approach Using LLMs and AMR Parsing', 'authors': 'Connar Hite, Sean Saud, Raef Taha, Nayim Rahman, Tanvir Atahary, Scott Douglass, Tarek Taha', 'link': 'https://arxiv.org/abs/2511.08715', 'abstract': 'Answer Set Programming (ASP) is a declarative programming paradigm based on logic programming and non-monotonic reasoning. It is a tremendously powerful tool for describing and solving combinatorial problems. Like any other language, ASP requires users to learn how it works and the syntax involved. It is becoming increasingly required for those unfamiliar with programming languages to interact with code. This paper proposes a novel method of translating unconstrained English into ASP programs for logic puzzles using an LLM and Abstract Meaning Representation (AMR) graphs. Everything from ASP rules, facts, and constraints is generated to fully represent and solve the desired problem. Example logic puzzles are used to demonstrate the capabilities of the system. While most current methods rely entirely on an LLM, our system minimizes the role of the LLM only to complete straightforward tasks. The LLM is used to simplify natural language sentences, identify keywords, and generate simple facts. The AMR graphs are then parsed from simplified language and used to generate ASP constraints systematically. The system successfully creates an entire ASP program that solves a combinatorial logic problem. This approach is a significant first step in creating a lighter-weight, explainable system that converts natural language to solve complex logic problems.', 'abstract_zh': '基于LLM和AMR图的不受约束的英语到ASP程序的转换方法', 'title_zh': '自然语言与ASP的桥梁：基于LLM和AMR解析的混合方法'}
{'arxiv_id': 'arXiv:2511.09478', 'title': 'AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting', 'authors': 'Renda Li, Hailang Huang, Fei Wei, Feng Xiong, Yong Wang, Xiangxiang Chu', 'link': 'https://arxiv.org/abs/2511.09478', 'abstract': 'Reinforcement learning (RL) has demonstrated considerable potential for enhancing reasoning in large language models (LLMs). However, existing methods suffer from Gradient Starvation and Policy Degradation when training directly on samples with mixed difficulty. To mitigate this, prior approaches leverage Chain-of-Thought (CoT) data, but the construction of high-quality CoT annotations remains labor-intensive. Alternatively, curriculum learning strategies have been explored but frequently encounter challenges, such as difficulty mismatch, reliance on manual curriculum design, and catastrophic forgetting. To address these issues, we propose AdaCuRL, a Adaptive Curriculum Reinforcement Learning framework that integrates coarse-to-fine difficulty estimation with adaptive curriculum scheduling. This approach dynamically aligns data difficulty with model capability and incorporates a data revisitation mechanism to mitigate catastrophic forgetting. Furthermore, AdaCuRL employs adaptive reference and sparse KL strategies to prevent Policy Degradation. Extensive experiments across diverse reasoning benchmarks demonstrate that AdaCuRL consistently achieves significant performance improvements on both LLMs and MLLMs.', 'abstract_zh': '自适应 Curriculum 辅助强化学习（AdaCuRL）：一种细粒度难度估计算法及其在大型语言模型中的应用', 'title_zh': '自适应课程强化学习：无效样本缓解与历史重访'}
{'arxiv_id': 'arXiv:2511.09438', 'title': 'LLM-Guided Dynamic-UMAP for Personalized Federated Graph Learning', 'authors': 'Sai Puppala, Ismail Hossain, Md Jahangir Alam, Tanzim Ahad, Sajedul Talukder', 'link': 'https://arxiv.org/abs/2511.09438', 'abstract': 'We propose a method that uses large language models to assist graph machine learning under personalization and privacy constraints. The approach combines data augmentation for sparse graphs, prompt and instruction tuning to adapt foundation models to graph tasks, and in-context learning to supply few-shot graph reasoning signals. These signals parameterize a Dynamic UMAP manifold of client-specific graph embeddings inside a Bayesian variational objective for personalized federated learning. The method supports node classification and link prediction in low-resource settings and aligns language model latent representations with graph structure via a cross-modal regularizer. We outline a convergence argument for the variational aggregation procedure, describe a differential privacy threat model based on a moments accountant, and present applications to knowledge graph completion, recommendation-style link prediction, and citation and product graphs. We also discuss evaluation considerations for benchmarking LLM-assisted graph machine learning.', 'abstract_zh': '我们提出了一种方法，利用大型语言模型在个性化和隐私约束下辅助图机器学习。该方法结合了稀疏图的数据扩充、提示和指令调优以使基础模型适应图任务、以及上下文学习以提供少量图推理信号。这些信号参数化了一个客户特定图嵌入的动态UMAP流形，并置于贝叶斯变分目标中，用于个性化联邦学习。该方法支持低资源环境下的节点分类和链接预测，并通过跨模态正则化器将语言模型的潜在表示与图结构对齐。我们给出了变分聚合过程收敛性的论证，描述了基于矩核算员的差分隐私威胁模型，并展示了在知识图谱完成、推荐风格的链接预测以及引用和产品图方面的应用。我们还讨论了评估LLM辅助图机器学习基准的考量因素。', 'title_zh': 'LLM引导的动态-UMAP在个性化联邦图学习中的应用'}
{'arxiv_id': 'arXiv:2511.09396', 'title': 'Multimodal Large Language Models for Low-Resource Languages: A Case Study for Basque', 'authors': 'Lukas Arana, Julen Etxaniz, Ander Salaberria, Gorka Azkune', 'link': 'https://arxiv.org/abs/2511.09396', 'abstract': 'Current Multimodal Large Language Models exhibit very strong performance for several demanding tasks. While commercial MLLMs deliver acceptable performance in low-resource languages, comparable results remain unattained within the open science community. In this paper, we aim to develop a strong MLLM for a low-resource language, namely Basque. For that purpose, we develop our own training and evaluation image-text datasets. Using two different Large Language Models as backbones, the Llama-3.1-Instruct model and a Basque-adapted variant called Latxa, we explore several data mixtures for training. We show that: i) low ratios of Basque multimodal data (around 20%) are already enough to obtain solid results on Basque benchmarks, and ii) contrary to expected, a Basque instructed backbone LLM is not required to obtain a strong MLLM in Basque. Our results pave the way to develop MLLMs for other low-resource languages by openly releasing our resources.', 'abstract_zh': '当前的多模态大型语言模型在多个挑战性任务中表现出很强的性能。尽管商业化的多模态大语言模型在低资源语言中表现出可接受的性能，但在开放科学社区中仍未能取得可比的结果。本文旨在为一种低资源语言——巴斯克语，开发一种强大的多模态大语言模型。为此，我们开发了自己的训练和评估图像-文本数据集。使用两种不同的大型语言模型作为骨干模型，即Llama-3.1-Instruct模型和巴斯克语适应变体Latxa，我们探索了多种数据混合训练方法。我们展示了以下两点：i) 大约20%的巴斯克多模态数据已经足够在巴斯克语基准测试中取得良好的结果；ii) 出人意料的是，一个经过巴斯克语指令调整的骨干模型并非必要条件，以获得强大的巴斯克语多模态语言模型。我们的研究为开发其他低资源语言的多模态大语言模型铺平了道路，并通过公开发布我们的资源的方式打开了这一途径。', 'title_zh': '多模态大型语言模型在低资源语言中的应用：关于巴斯克语的案例研究'}
{'arxiv_id': 'arXiv:2511.09381', 'title': 'Self-Correcting Large Language Models: Generation vs. Multiple Choice', 'authors': 'Hossein A. Rahmani, Satyapriya Krishna, Xi Wang, Mohammadmehdi Naghiaei, Emine Yilmaz', 'link': 'https://arxiv.org/abs/2511.09381', 'abstract': 'Large language models have recently demonstrated remarkable abilities to self-correct their responses through iterative refinement, often referred to as self-consistency or self-reflection. However, the dynamics of this self-correction mechanism may differ substantially depending on whether the model is tasked with open-ended text generation or with selecting the most appropriate response from multiple predefined options. In this paper, we conduct a systematic investigation of these two paradigms by comparing performance trends and error-correction behaviors across various natural language understanding and reasoning tasks, covering language models of different scales and families. Our experimental results reveal distinct patterns of improvement and failure modes:\n\\textit{While open-ended generation often benefits from the flexibility of re-interpretation and compositional refinement, multiple-choice selection can leverage clearer solution boundaries but may be limited by the provided options}. This contrast also reflects the dual demands faced by emerging agentic LLM applications: effective agents must not only generate and refine open-ended plans or explanations, but also make reliable discrete choices when operating within constrained action spaces. Our findings, therefore, highlight that the design of self-correction mechanisms should take into account the interaction between task structure and output space, with implications for both knowledge-intensive reasoning and decision-oriented applications of LLMs.', 'abstract_zh': '大型语言模型最近展现了通过迭代精炼来自我修正其响应的能力，这通常被称为自我一致性或自我反思。然而，这种自我修正机制的动力学可能会因为模型是用于开放生成文本还是从多个预定义选项中选择最合适的响应而有很大差异。在本文中，我们通过比较不同自然语言理解和推理任务上的性能趋势和错误修正行为，系统性地研究了这两种范式，涵盖了不同规模和家族的语言模型。我们的实验结果揭示了不同的改进模式和失败模式：开放生成往往得益于重新解释和组合精炼的灵活性，而多项选择则可以利用清晰的解决方案边界，但也可能受限于提供的选项。这种对比也反映了新兴代理型LLM应用的双重需求：有效的代理不仅需要生成和精炼开放的计划或解释，还需要在受限制的动作空间内做出可靠的选择。因此，我们的研究结果强调，在设计自我修正机制时应考虑任务结构与输出空间之间的交互，这一观点对知识密集型推理和决策导向型LLM应用都有重要意义。', 'title_zh': '自我修正大型语言模型：生成 vs. 多选'}
{'arxiv_id': 'arXiv:2511.09374', 'title': 'MTQ-Eval: Multilingual Text Quality Evaluation for Language Models', 'authors': 'Rhitabrat Pokharel, Ameeta Agrawal', 'link': 'https://arxiv.org/abs/2511.09374', 'abstract': 'The use of large language models (LLMs) for evaluating outputs is becoming an increasingly effective and scalable approach. However, it remains uncertain whether this capability extends beyond task-specific evaluations to more general assessments of text quality, particularly in multilingual contexts. In this study, we introduce, MTQ-Eval, a novel framework for multilingual text quality evaluation that learns from examples of both high- and low-quality texts, adjusting its internal representations. To develop MTQ-Eval, we first automatically generate text quality preference data and then use it to train open-source base LLMs to align with ratings of high- and low-quality text. Our comprehensive evaluation across 115 languages demonstrates the improved performance of the proposed model. Upon further analysis, we find that this enhanced evaluation capability also leads to notable improvements in downstream tasks.', 'abstract_zh': '大型语言模型（LLMs）在评价输出方面日益成为一种有效且可扩展的方法。然而，这种能力能否超越特定任务的评估，扩展到更广泛的文本质量评估，特别是在多语言环境中，仍不确定。本研究介绍了MTQ-Eval，一种新颖的多语言文本质量评估框架，该框架通过学习高质量和低质量文本的示例来调整其内部表示。为了开发MTQ-Eval，我们首先自动生成文本质量偏好数据，然后使用该数据训练开源基础LLM以与高质量和低质量文本的评分对齐。我们在115种语言上的全面评估表明，所提出模型的性能得到了提升。进一步分析发现，这种增强的评估能力还导致了下游任务的显著改进。', 'title_zh': '多语言文本质量评估：MTQ-Eval 为语言模型'}
{'arxiv_id': 'arXiv:2511.09309', 'title': 'TaskSense: Cognitive Chain Modeling and Difficulty Estimation for GUI Tasks', 'authors': 'Yiwen Yin, Zhian Hu, Xiaoxi Xu, Chun Yu, Xintong Wu, Wenyu Fan, Yuanchun Shi', 'link': 'https://arxiv.org/abs/2511.09309', 'abstract': 'Measuring GUI task difficulty is crucial for user behavior analysis and agent capability evaluation. Yet, existing benchmarks typically quantify difficulty based on motor actions (e.g., step counts), overlooking the cognitive demands underlying task completion. In this work, we propose Cognitive Chain, a novel framework that models task difficulty from a cognitive perspective. A cognitive chain decomposes the cognitive processes preceding a motor action into a sequence of cognitive steps (e.g., finding, deciding, computing), each with a difficulty index grounded in information theories. We develop an LLM-based method to automatically extract cognitive chains from task execution traces. Validation with linear regression shows that our estimated cognitive difficulty correlates well with user completion time (step-level R-square=0.46 after annotation). Assessment of state-of-the-art GUI agents shows reduced success on cognitively demanding tasks, revealing capability gaps and Human-AI consistency patterns. We conclude by discussing potential applications in agent training, capability assessment, and human-agent delegation optimization.', 'abstract_zh': '基于认知视角衡量GUI任务难度对于用户行为分析和代理能力评估至关重要。现有基准通常基于运动动作（如步数）来量化难度，忽略了任务完成的认知需求。在这项工作中，我们提出了一种名为Cognitive Chain的新型框架，从认知视角建模任务难度。一个认知链将先于运动动作的认知过程分解为一系列认知步骤（例如，寻找、决定、计算），每个步骤的难度指数基于信息理论。我们开发了一种基于LLM的方法，自动从任务执行轨迹中提取认知链。通过线性回归验证表明，我们的认知难度估计值与用户完成时间高度相关（注释后步骤级R平方值为0.46）。对最先进的GUI代理的评估显示，在认知需求较高的任务中成功率下降，揭示了能力差距和人机一致性模式。最后，我们讨论了该方法在代理训练、能力评估和人机代理委托优化中的潜在应用。', 'title_zh': 'TaskSense: 基于认知链建模与GUI任务难度估计'}
{'arxiv_id': 'arXiv:2511.09292', 'title': 'C$^3$TG: Conflict-aware, Composite, and Collaborative Controlled Text Generation', 'authors': 'Yu Li, Zhe Yang, Yi Huang, Xin Liu, Guilin Qi', 'link': 'https://arxiv.org/abs/2511.09292', 'abstract': 'Recent advancements in large language models (LLMs) have demonstrated remarkable text generation capabilities. However, controlling specific attributes of generated text remains challenging without architectural modifications or extensive fine-tuning. Current methods typically toggle a single, basic attribute but struggle with precise multi-attribute control. In scenarios where attribute requirements conflict, existing methods lack coordination mechanisms, causing interference between desired attributes. Furthermore, these methods fail to incorporate iterative optimization processes in the controlled generation pipeline. To address these limitations, we propose Conflict-aware, Composite, and Collaborative Controlled Text Generation (C$^3$TG), a two-phase framework for fine-grained, multi-dimensional text attribute control. During generation, C$^3$TG selectively pairs the LLM with the required attribute classifiers from the 17 available dimensions and employs weighted KL-divergence to adjust token probabilities. The optimization phase then leverages an energy function combining classifier scores and penalty terms to resolve attribute conflicts through iterative feedback, enabling precise control over multiple dimensions simultaneously while preserving natural text flow. Experiments show that C$^3$TG significantly outperforms baselines across multiple metrics including attribute accuracy, linguistic fluency, and output diversity, while simultaneously reducing toxicity. These results establish C$^3$TG as an effective and flexible solution for multi-dimensional text attribute control that requires no costly model modifications.', 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）的 Recent advancements in 大型语言模型（LLMs）近期在大型语言模型方面取得了显著的文本生成能力。然而，在不进行架构修改或大量微调的情况下，控制生成文本的特定属性仍然颇具挑战性。当前的方法通常只能切换单一的基本属性，并难以实现精确的多属性控制。在属性需求发生冲突的场景下，现有方法缺乏协调机制，导致期望属性之间的相互干扰。此外，这些方法在受控生成过程中未能引入迭代优化流程。为解决这些局限，我们提出了具备冲突意识、复合性和协作性的受控文本生成（C$^3$TG），这是一种用于细粒度、多维度文本属性控制的两阶段框架。生成阶段，C$^3$TG 选择性地将LLM与所需的17个属性分类器配对，并利用加权KL散度调整token概率。优化阶段则利用结合分类器评分和惩罚项的能量函数通过迭代反馈解决属性冲突，从而同时在多个维度上实现精确控制，并保持自然的文本流。实验结果显示，C$^3$TG 在包括属性准确性、语义流畅性和输出多样性的多个指标上显著优于基线方法，同时降低了有毒内容的产生。这些结果证明了C$^3$TG 是一种有效且灵活的多维度文本属性控制解决方案，无需进行昂贵的模型修改。', 'title_zh': '冲突感知、复合与协作控制文本生成'}
{'arxiv_id': 'arXiv:2511.09231', 'title': 'Leveraging Large Language Models for Use Case Model Generation from Software Requirements', 'authors': 'Tobias Eisenreich, Nicholas Friedlaender, Stefan Wagner', 'link': 'https://arxiv.org/abs/2511.09231', 'abstract': 'Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.', 'abstract_zh': '大型语言模型在用例建模中的应用研究：一种基于开放权重模型和高级提示工程的技术方法', 'title_zh': '利用大型语言模型从软件需求生成用例模型'}
{'arxiv_id': 'arXiv:2511.09179', 'title': 'A Hybrid Search for Complex Table Question Answering in Securities Report', 'authors': 'Daiki Shirafuji, Koji Tanaka, Tatsuhiko Saito', 'link': 'https://arxiv.org/abs/2511.09179', 'abstract': 'Recently, Large Language Models (LLMs) are gaining increased attention in the domain of Table Question Answering (TQA), particularly for extracting information from tables in documents. However, directly entering entire tables as long text into LLMs often leads to incorrect answers because most LLMs cannot inherently capture complex table structures. In this paper, we propose a cell extraction method for TQA without manual identification, even for complex table headers. Our approach estimates table headers by computing similarities between a given question and individual cells via a hybrid retrieval mechanism that integrates a language model and TF-IDF. We then select as the answer the cells at the intersection of the most relevant row and column. Furthermore, the language model is trained using contrastive learning on a small dataset of question-header pairs to enhance performance. We evaluated our approach in the TQA dataset from the U4 shared task at NTCIR-18. The experimental results show that our pipeline achieves an accuracy of 74.6\\%, outperforming existing LLMs such as GPT-4o mini~(63.9\\%). In the future, although we used traditional encoder models for retrieval in this study, we plan to incorporate more efficient text-search models to improve performance and narrow the gap with human evaluation results.', 'abstract_zh': '最近，大型语言模型（LLMs）在表格问答（TQA）领域引起了广泛关注，特别是在从文档中的表格中提取信息方面。然而，直接将整个表格作为长文本输入LLMs往往会因为大多数LLMs无法内在地捕捉复杂的表格结构而产生错误的答案。本文提出了一种无需人工识别的单元格提取方法，即使对于复杂的表头也是如此。我们的方法通过一种结合语言模型和TF-IDF的混合检索机制来计算给定问题与单个单元格之间的相似性，来估计表头，并选择最相关的行和列的交点处的单元格作为答案。此外，我们使用少量的问题-表头 pair 数据集进行对比学习来训练语言模型，以提高性能。我们在NTCIR-18年度报告U4共享任务的TQA数据集上评估了我们的方法，实验结果表明，我们的管道达到了74.6%的准确率，优于现有的LLMs如GPT-4o mini（63.9%）。未来，虽然本研究中使用了传统的编码器模型进行检索，但我们计划集成更高效的文本搜索模型以提高性能，并缩小与人类评估结果的差距。', 'title_zh': '复杂财务报告表格问题解答的混合搜索方法'}
{'arxiv_id': 'arXiv:2511.09149', 'title': 'Enabling Agents to Communicate Entirely in Latent Space', 'authors': 'Zhuoyun Du, Runze Wang, Huiyu Bai, Zouying Cao, Xiaoyong Zhu, Bo Zheng, Wei Chen, Haochao Ying', 'link': 'https://arxiv.org/abs/2511.09149', 'abstract': 'While natural language is the de facto communication medium for LLM-based agents, it presents a fundamental constraint. The process of downsampling rich, internal latent states into discrete tokens inherently limits the depth and nuance of information that can be transmitted, thereby hindering collaborative problem-solving. Inspired by human mind-reading, we propose Interlat (Inter-agent Latent Space Communication), a paradigm that leverages the last hidden states of an LLM as a representation of its mind for direct transmission (termed latent communication). An additional compression process further compresses latent communication via entirely latent space reasoning. Experiments demonstrate that Interlat outperforms both fine-tuned chain-of-thought (CoT) prompting and single-agent baselines, promoting more exploratory behavior and enabling genuine utilization of latent information. Further compression not only substantially accelerates inference but also maintains competitive performance through an efficient information-preserving mechanism. We position this work as a feasibility study of entirely latent space inter-agent communication, and our results highlight its potential, offering valuable insights for future research.', 'abstract_zh': '基于LLM的代理间富.latent空间通信(Interlat):超越细调链式思维提示与单代理基线的方法', 'title_zh': '使智能体在完全的潜在空间中进行通信'}
{'arxiv_id': 'arXiv:2511.09148', 'title': 'LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls', 'authors': 'Kangning Zhang, Wenxiang Jiao, Kounianhua Du, Yuan Lu, Weiwen Liu, Weinan Zhang, Lei Zhang, Yong Yu', 'link': 'https://arxiv.org/abs/2511.09148', 'abstract': "Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.", 'abstract_zh': '增强大型语言模型（LLMs）与外部工具结合能使它们执行复杂的多步任务。然而，工具学习受限于静态的合成数据管道，其中数据生成和模型训练被当作两个分离且非交互的过程执行。这种方法未能针对模型的特定弱点进行自适应的聚焦，并允许噪声标签存在，从而降低训练效率。我们引入了LoopTool，这是一个完全自动化且模型感知的数据演化框架，通过紧密整合数据合成与模型训练来关闭这个循环。LoopTool 通过三个协同模块迭代精炼数据和模型：（1）贪婪能力探测（GCP）诊断模型掌握和未能掌握的能力；（2）判决引导标签验证（JGLV）利用开源判决模型找出并修正注释错误，逐步净化数据集；（3）错误驱动的数据扩展（EDDE）基于识别出的失败生成新的具有挑战性的样本。该闭环过程运行在经济高效的开源生态系统中，避免了对昂贵的闭源API的依赖。实验显示，使用LoopTool训练的8亿参数模型显著超越其32亿参数数据生成器，并在BFCL-v3和ACEBench基准测试中达到了新的规模状态最优结果。我们的工作表明，闭环、自我修正的数据管道可以大幅增强LLMs的工具使用能力。', 'title_zh': 'LoopTool: 闭合数据-训练循环以提高LLM工具调用的稳健性'}
{'arxiv_id': 'arXiv:2511.09122', 'title': 'Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation', 'authors': 'Joschka Kersting, Michael Rummel, Gesa Benndorf', 'link': 'https://arxiv.org/abs/2511.09122', 'abstract': 'Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.', 'abstract_zh': '一种用于工业应用的低数据域编程助手解决方案', 'title_zh': '基于供应商意识的工业代理：增强型LLM及其在安全边缘PLC代码生成中的应用'}
{'arxiv_id': 'arXiv:2511.09105', 'title': 'Cost-Minimized Label-Flipping Poisoning Attack to LLM Alignment', 'authors': 'Shigeki Kusaka, Keita Saito, Mikoto Kudo, Takumi Tanabe, Akifumi Wachi, Youhei Akimoto', 'link': 'https://arxiv.org/abs/2511.09105', 'abstract': "Large language models (LLMs) are increasingly deployed in real-world systems, making it critical to understand their vulnerabilities. While data poisoning attacks during RLHF/DPO alignment have been studied empirically, their theoretical foundations remain unclear. We investigate the minimum-cost poisoning attack required to steer an LLM's policy toward an attacker's target by flipping preference labels during RLHF/DPO, without altering the compared outputs. We formulate this as a convex optimization problem with linear constraints, deriving lower and upper bounds on the minimum attack cost. As a byproduct of this theoretical analysis, we show that any existing label-flipping attack can be post-processed via our proposed method to reduce the number of label flips required while preserving the intended poisoning effect. Empirical results demonstrate that this cost-minimization post-processing can significantly reduce poisoning costs over baselines, particularly when the reward model's feature dimension is small relative to the dataset size. These findings highlight fundamental vulnerabilities in RLHF/DPO pipelines and provide tools to evaluate their robustness against low-cost poisoning attacks.", 'abstract_zh': '大规模语言模型（LLMs）越来越多地应用于实际系统中，因此理解其漏洞变得至关重要。虽然在基于奖励学习的人工智能辅助人类反馈（RLHF）/分布式匹配优化（DPO）对齐过程中数据投毒攻击已被实证研究，但其理论基础尚不清楚。我们探讨了在不修改比较输出的情况下，通过在RLHF/DPO过程中翻转偏好标签来引导LLM策略朝向攻击者目标所需的最低成本投毒攻击。我们将这一过程形式化为带线性约束的凸优化问题，并推导出最低攻击成本的上界和下界。在此理论分析的基础上，我们证明任何现有的标签翻转攻击都可以通过我们提出的方法进行后处理，从而减少所需的标签翻转次数，同时保持预期的投毒效果。实验证明，这种成本最小化后处理可以显著降低投毒成本，尤其是在奖励模型的特征维度相对于数据集大小较小的情况下。这些发现突显了RLHF/DPO管道中的基本漏洞，并提供了评估其对低成本投毒攻击鲁棒性的工具。', 'title_zh': '成本最小化的标签翻转中毒攻击以实现LLM对齐'}
{'arxiv_id': 'arXiv:2511.09087', 'title': 'Tele-LLM-Hub: Building Context-Aware Multi-Agent LLM Systems for Telecom Networks', 'authors': 'Vijay K Shah, Cong Shen', 'link': 'https://arxiv.org/abs/2511.09087', 'abstract': 'This paper introduces Tele-LLM-Hub, a user friendly low-code solution for rapid prototyping and deployment of context aware multi-agent (MA) Large Language Model (LLM) systems tailored for 5G and beyond. As telecom wireless networks become increasingly complex, intelligent LLM applications must share a domainspecific understanding of network state. We propose TeleMCP, the Telecom Model Context Protocol, to enable structured and context-rich communication between agents in telecom environments. Tele-LLM-Hub actualizes TeleMCP through a low-code interface that supports agent creation, workflow composition, and interaction with software stacks such as srsRAN. Key components include a direct chat interface, a repository of pre-built systems, an Agent Maker leveraging finetuning with our RANSTRUCT framework, and an MA-Maker for composing MA workflows. The goal of Tele-LLM-Hub is to democratize the design of contextaware MA systems and accelerate innovation in next-generation wireless networks.', 'abstract_zh': 'Tele-LLM-Hub：面向5G及以上的用户友好低代码解决方案，用于快速原型制作和部署上下文感知多代理大型语言模型系统', 'title_zh': 'Tele-LLM-Hub: 构建面向电信网络的上下文感知多智能体LLM系统'}
{'arxiv_id': 'arXiv:2511.08923', 'title': 'TiDAR: Think in Diffusion, Talk in Autoregression', 'authors': 'Jingyu Liu, Xin Dong, Zhifan Ye, Rishabh Mehta, Yonggan Fu, Vartika Singh, Jan Kautz, Ce Zhang, Pavlo Molchanov', 'link': 'https://arxiv.org/abs/2511.08923', 'abstract': 'Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.', 'abstract_zh': 'TiDAR：序列级混合架构实现高吞吐量、高GPU利用率及自回归级别质量', 'title_zh': 'TiDAR: 思考在扩散中，表达在自回归中'}
{'arxiv_id': 'arXiv:2511.08905', 'title': 'iSeal: Encrypted Fingerprinting for Reliable LLM Ownership Verification', 'authors': 'Zixun Xiong, Gaoyi Wu, Qingyang Yu, Mingyu Derek Ma, Lingfeng Yao, Miao Pan, Xiaojiang Du, Hao Wang', 'link': 'https://arxiv.org/abs/2511.08905', 'abstract': "Given the high cost of large language model (LLM) training from scratch, safeguarding LLM intellectual property (IP) has become increasingly crucial. As the standard paradigm for IP ownership verification, LLM fingerprinting thus plays a vital role in addressing this challenge. Existing LLM fingerprinting methods verify ownership by extracting or injecting model-specific features. However, they overlook potential attacks during the verification process, leaving them ineffective when the model thief fully controls the LLM's inference process. In such settings, attackers may share prompt-response pairs to enable fingerprint unlearning or manipulate outputs to evade exact-match verification. We propose iSeal, the first fingerprinting method designed for reliable verification when the model thief controls the suspected LLM in an end-to-end manner. It injects unique features into both the model and an external module, reinforced by an error-correction mechanism and a similarity-based verification strategy. These components are resistant to verification-time attacks, including collusion-based fingerprint unlearning and response manipulation, backed by both theoretical analysis and empirical results. iSeal achieves 100 percent Fingerprint Success Rate (FSR) on 12 LLMs against more than 10 attacks, while baselines fail under unlearning and response manipulations.", 'abstract_zh': '基于模型 Thief 全局控制的大型语言模型指纹识别方法 iSeal', 'title_zh': 'iSeal: 加密指纹技术用于可靠的LLM所有权验证'}
{'arxiv_id': 'arXiv:2511.08877', 'title': 'Hallucinate or Memorize? The Two Sides of Probabilistic Learning in Large Language Models', 'authors': 'Junichiro Niimi', 'link': 'https://arxiv.org/abs/2511.08877', 'abstract': "Large language models (LLMs) have been increasingly applied to a wide range of tasks, from natural language understanding to code generation. While they have also been used to assist in citation recommendation, the hallucination of non-existent papers remains a major issue. Building on prior studies, this study hypothesizes that an LLM's ability to correctly produce bibliographic records depends on whether the underlying knowledge is generated or memorized, with highly cited papers (i.e., more frequently appear in the pretraining corpus) showing lower hallucination rates. We therefore assume citation count as a proxy for training data redundancy (i.e., the frequency with which a given bibliographic record appears in the pretraining corpus) and investigate how citation frequency affects hallucinated references in LLM outputs. Using GPT-4.1, we generated and manually verified 100 citations across twenty computer-science domains, and measured factual consistency via cosine similarity between generated and authentic metadata. The results revealed that (i) citation count is strongly correlated with factual accuracy, (ii) bibliographic information becomes almost verbatim memorized beyond roughly 1,000 citations, and (iii) memory interference occurs when multiple highly cited papers share similar content. These findings indicate a threshold where generalization shifts into memorization, with highly cited papers being nearly verbatim retained in the model.", 'abstract_zh': '大型语言模型（LLMs）在从自然语言理解到代码生成等多种任务中得到了广泛应用。尽管它们也被用于辅助引文推荐，但虚构不存在的论文问题仍然是一个主要问题。在此基础上，本研究假设一个LLM准确生成参考文献的能力取决于其背后的知识是生成的还是记忆的，高被引论文（即在预训练语料中出现频率更高）显示出较低的虚构率。因此，我们假设引文数量作为训练数据冗余（即给定参考文献在预训练语料中出现的频率）的代理，并研究引文频率如何影响LLM输出中的虚构引文。使用GPT-4.1，我们生成并手动验证了计算机科学二十个领域的100个引文，并通过生成数据和真实元数据之间的余弦相似度衡量事实一致性。结果表明：（i）引文数量与事实准确性之间存在强烈相关性；（ii）引文数量超过约1,000后，参考文献几乎被原样记忆；（iii）当多篇高被引论文包含相似内容时，会发生记忆干扰。这些发现表明，在一个临界点处，泛化转换为记忆，高被引论文几乎被原样保留在模型中。', 'title_zh': '生成或记忆？大型语言模型中概率学习的两面性'}
{'arxiv_id': 'arXiv:2511.08798', 'title': 'Structured Uncertainty guided Clarification for LLM Agents', 'authors': 'Manan Suri, Puneet Mathur, Nedim Lipka, Franck Dernoncourt, Ryan A. Rossi, Dinesh Manocha', 'link': 'https://arxiv.org/abs/2511.08798', 'abstract': 'LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39\\% while reducing clarification questions by 1.5-2.7$\\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5\\% to 65.2\\% (3B model) and 36.7\\% to 62.9\\% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.', 'abstract_zh': 'LLM代理通过工具调用能力扩展了大型语言模型，但模糊的用户指令往往会引发错误的调用和任务失败。我们提出了一种结构化不确定性原理性公式，通过部分观测马尔可夫决策过程（POMDP）和预期完美信息（EVPI）目标建模联合工具-参数澄清过程，并通过基于方面成本建模防止冗余。我们的SAGE-Agent利用这种结构化不确定性实现了更高的效率：在模糊任务上的覆盖范围提高了7-39\\%，同时与强烈的提示和基于不确定性的基线相比，澄清问题的数量减少了1.5-2.7倍。我们提出了ClarifyBench，这是首个涵盖文档编辑、车辆控制和旅行预订等多个领域的多轮工具增强消歧验证基准，并包含现实的基于LLM的用户模拟。此外，我们展示了结构化不确定性为强化学习提供了有效的训练信号，通过不确定性加权GRPO训练，将When2Call的准确性从36.5\\%提高到65.2\\%（3B模型）和从36.7\\%提高到62.9\\%（7B模型）。这些结果确立了结构化不确定性作为工具增强代理的原理性、高效方法，提高了实际场景中的任务成功率和交互效率。', 'title_zh': '结构化的不确定性引导澄清对于LLM代理'}
{'arxiv_id': 'arXiv:2511.08721', 'title': 'Benevolent Dictators? On LLM Agent Behavior in Dictator Games', 'authors': 'Andreas Einwiller, Kanishka Ghosh Dastidar, Artur Romazanov, Annette Hautli-Janisz, Michael Granitzer, Florian Lemmerich', 'link': 'https://arxiv.org/abs/2511.08721', 'abstract': "In behavioral sciences, experiments such as the ultimatum game are conducted to assess preferences for fairness or self-interest of study participants. In the dictator game, a simplified version of the ultimatum game where only one of two players makes a single decision, the dictator unilaterally decides how to split a fixed sum of money between themselves and the other player. Although recent studies have explored behavioral patterns of AI agents based on Large Language Models (LLMs) instructed to adopt different personas, we question the robustness of these results. In particular, many of these studies overlook the role of the system prompt - the underlying instructions that shape the model's behavior - and do not account for how sensitive results can be to slight changes in prompts. However, a robust baseline is essential when studying highly complex behavioral aspects of LLMs. To overcome previous limitations, we propose the LLM agent behavior study (LLM-ABS) framework to (i) explore how different system prompts influence model behavior, (ii) get more reliable insights into agent preferences by using neutral prompt variations, and (iii) analyze linguistic features in responses to open-ended instructions by LLM agents to better understand the reasoning behind their behavior. We found that agents often exhibit a strong preference for fairness, as well as a significant impact of the system prompt on their behavior. From a linguistic perspective, we identify that models express their responses differently. Although prompt sensitivity remains a persistent challenge, our proposed framework demonstrates a robust foundation for LLM agent behavior studies. Our code artifacts are available at this https URL.", 'abstract_zh': '基于大型语言模型的代理行为研究框架：探索系统提示对模型行为的影响及代理偏好分析', 'title_zh': 'benevolent独裁者？关于大规模语言模型代理在独裁者游戏中的行为研究'}
{'arxiv_id': 'arXiv:2511.08621', 'title': 'The LLM Pro Finance Suite: Multilingual Large Language Models for Financial Applications', 'authors': 'Gaëtan Caillaut, Raheel Qader, Jingshu Liu, Mariam Nakhlé, Arezki Sadoune, Massinissa Ahmim, Jean-Gabriel Barthelemy', 'link': 'https://arxiv.org/abs/2511.08621', 'abstract': "The financial industry's growing demand for advanced natural language processing (NLP) capabilities has highlighted the limitations of generalist large language models (LLMs) in handling domain-specific financial tasks. To address this gap, we introduce the LLM Pro Finance Suite, a collection of five instruction-tuned LLMs (ranging from 8B to 70B parameters) specifically designed for financial applications. Our approach focuses on enhancing generalist instruction-tuned models, leveraging their existing strengths in instruction following, reasoning, and toxicity control, while fine-tuning them on a curated, high-quality financial corpus comprising over 50% finance-related data in English, French, and German.\nWe evaluate the LLM Pro Finance Suite on a comprehensive financial benchmark suite, demonstrating consistent improvement over state-of-the-art baselines in finance-oriented tasks and financial translation. Notably, our models maintain the strong general-domain capabilities of their base models, ensuring reliable performance across non-specialized tasks. This dual proficiency, enhanced financial expertise without compromise on general abilities, makes the LLM Pro Finance Suite an ideal drop-in replacement for existing LLMs in financial workflows, offering improved domain-specific performance while preserving overall versatility. We publicly release two 8B-parameters models to foster future research and development in financial NLP applications: this https URL.", 'abstract_zh': '金融行业对高级自然语言处理（NLP）能力的不断增长需求突显了通用大型语言模型（LLMs）在处理专门金融任务方面的局限性。为填补这一空白，我们介绍了LLM Pro Finance Suite，这是一个由五个指令调优的大语言模型组成的集合（参数范围从8B到70B），专门设计用于金融应用。我们的方法侧重于增强通用指令调优模型，利用其在指令遵循、推理和 toxicity 控制方面的现有优势，并在其上微调一个精心挑选的高质量金融语料库，该语料库包含超过50%的英文、法文和德文的金融相关数据。', 'title_zh': '多语言大型语言模型：Pro Finance Suite在金融应用中的探索'}
{'arxiv_id': 'arXiv:2511.08620', 'title': 'Learn More, Forget Less: A Gradient-Aware Data Selection Approach for LLM', 'authors': 'Yibai Liu, Shihang Wang, Zeming Liu, Zheming Song, Junzhe Wang, Jingjing Liu, Qingjie Liu, Yunhong Wang', 'link': 'https://arxiv.org/abs/2511.08620', 'abstract': "Despite large language models (LLMs) have achieved impressive achievements across numerous tasks, supervised fine-tuning (SFT) remains essential for adapting these models to specialized domains. However, SFT for domain specialization can be resource-intensive and sometimes leads to a deterioration in performance over general capabilities due to catastrophic forgetting (CF). To address these issues, we propose a self-adaptive gradient-aware data selection approach (GrADS) for supervised fine-tuning of LLMs, which identifies effective subsets of training data by analyzing gradients obtained from a preliminary training phase. Specifically, we design self-guided criteria that leverage the magnitude and statistical distribution of gradients to prioritize examples that contribute the most to the model's learning process. This approach enables the acquisition of representative samples that enhance LLMs understanding of domain-specific tasks. Through extensive experimentation with various LLMs across diverse domains such as medicine, law, and finance, GrADS has demonstrated significant efficiency and cost-effectiveness. Remarkably, utilizing merely 5% of the selected GrADS data, LLMs already surpass the performance of those fine-tuned on the entire dataset, and increasing to 50% of the data results in significant improvements! With catastrophic forgetting substantially mitigated simultaneously. We will release our code for GrADS later.", 'abstract_zh': '一种自适应梯度感知数据选择方法（GrADS）：用于大型语言模型的监督微调', 'title_zh': '学习更多，遗忘更少：一种基于梯度的数据选择方法用于大规模语言模型'}
{'arxiv_id': 'arXiv:2511.08598', 'title': 'OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand, Open Knowledge Benchmarking', 'authors': 'Yanhong Li, Tianyang Xu, Kenan Tang, Karen Livescu, David McAllester, Jiawei Zhou', 'link': 'https://arxiv.org/abs/2511.08598', 'abstract': 'Knowledge-intensive question answering is central to large language models (LLMs) and is typically assessed using static benchmarks derived from sources like Wikipedia and textbooks. However, these benchmarks fail to capture evolving knowledge in a dynamic world, and centralized curation struggles to keep pace with rapid LLM advancements. To address these drawbacks, we propose Open Knowledge Bench (OKBench), a fully automated framework for generating high-quality, dynamic knowledge benchmarks on demand. Focusing on the news domain where knowledge updates daily, OKBench is an agentic framework that automates the sourcing, creation, validation, and distribution of benchmarks. Our approach democratizes benchmark creation and facilitates thorough evaluation of retrieval-augmented methods by reducing overlap with pretraining data. We evaluate our framework on a wide range open-source and proprietary LLMs of various sizes and configurations, both with and without retrieval over freshly generated knowledge. Our results reveal distinct model behaviors when confronted with new information and highlight how retrieval narrows the performance gap between small and large models. These findings underscore the importance of evaluating LLMs on evolving knowledge benchmarks.', 'abstract_zh': '开放知识基准（OKBench）：一种自动生成高质量动态知识基准的自动化框架', 'title_zh': 'OKBench: 通过全自动、按需、开放的知识基准评估促进大语言模型 democratization'}
{'arxiv_id': 'arXiv:2511.08597', 'title': 'Self-HarmLLM: Can Large Language Model Harm Itself?', 'authors': 'Heehwan Kim, Sungjune Park, Daeseon Choi', 'link': 'https://arxiv.org/abs/2511.08597', 'abstract': "Large Language Models (LLMs) are generally equipped with guardrails to block the generation of harmful responses. However, existing defenses always assume that an external attacker crafts the harmful query, and the possibility of a model's own output becoming a new attack vector has not been sufficiently explored. In this study, we propose the Self-HarmLLM scenario, which uses a Mitigated Harmful Query (MHQ) generated by the same model as a new input. An MHQ is an ambiguous query whose original intent is preserved while its harmful nature is not directly exposed. We verified whether a jailbreak occurs when this MHQ is re-entered into a separate session of the same model. We conducted experiments on GPT-3.5-turbo, LLaMA3-8B-instruct, and DeepSeek-R1-Distill-Qwen-7B under Base, Zero-shot, and Few-shot conditions. The results showed up to 52% transformation success rate and up to 33% jailbreak success rate in the Zero-shot condition, and up to 65% transformation success rate and up to 41% jailbreak success rate in the Few-shot condition. By performing both prefix-based automated evaluation and human evaluation, we found that the automated evaluation consistently overestimated jailbreak success, with an average difference of 52%. This indicates that automated evaluation alone is not accurate for determining harmfulness. While this study is a toy-level study based on a limited query set and evaluators, it proves that our method can still be a valid attack scenario. These results suggest the need for a fundamental reconsideration of guardrail design and the establishment of a more robust evaluation methodology.", 'abstract_zh': 'Self-HarmLLM：探索模型自身输出作为新攻击向量的场景', 'title_zh': '自我伤害的大型语言模型：大型语言模型会自我伤害吗？'}
{'arxiv_id': 'arXiv:2511.08596', 'title': "What About the Scene with the Hitler Reference? HAUNT: A Framework to Probe LLMs' Self-consistency Via Adversarial Nudge", 'authors': 'Arka Dutta, Sujan Dutta, Rijul Magu, Soumyajit Datta, Munmun De Choudhury, Ashiqur R. KhudaBukhsh', 'link': 'https://arxiv.org/abs/2511.08596', 'abstract': 'Hallucinations pose a critical challenge to the real-world deployment of large language models (LLMs) in high-stakes domains. In this paper, we present a framework for stress testing factual fidelity in LLMs in the presence of adversarial nudge. Our framework consists of three steps. In the first step, we instruct the LLM to produce sets of truths and lies consistent with the closed domain in question. In the next step, we instruct the LLM to verify the same set of assertions as truths and lies consistent with the same closed domain. In the final step, we test the robustness of the LLM against the lies generated (and verified) by itself. Our extensive evaluation, conducted using five widely known proprietary LLMs across two closed domains of popular movies and novels, reveals a wide range of susceptibility to adversarial nudges: \\texttt{Claude} exhibits strong resilience, \\texttt{GPT} and \\texttt{Grok} demonstrate moderate resilience, while \\texttt{Gemini} and \\texttt{DeepSeek} show weak resilience. Considering that a large population is increasingly using LLMs for information seeking, our findings raise alarm.', 'abstract_zh': '大规模语言模型在高风险领域中的幻觉构成重要挑战：本论文提出一种框架以在对抗性提示下测试事实保真度，并揭示不同模型的抗扰动性差异。', 'title_zh': '含有希特勒引用的情景如何？HAUNT：一种通过对抗性推动探究LLMs自我一致性的框架'}
{'arxiv_id': 'arXiv:2511.08595', 'title': 'Chopping Trees: Semantic Similarity Based Dynamic Pruning for Tree-of-Thought Reasoning', 'authors': 'Joongho Kim, Xirui Huang, Zarreen Reza, Gabriel Grand, Kevin Zhu, Ryan Lagasse', 'link': 'https://arxiv.org/abs/2511.08595', 'abstract': 'Tree-of-Thought (ToT) reasoning boosts the problem-solving abilities of Large Language Models (LLMs) but is computationally expensive due to semantic redundancy, where distinct branches explore equivalent reasoning paths. We introduce Semantic Similarity-Based Dynamic Pruning (SSDP), a lightweight method that, to the best of our knowledge, is the first framework to integrate online semantic merging into parallelized tree search, enabling the clustering and pruning of redundant steps in real time. Across reasoning benchmarks, including GSM8K and MATH500, SSDP achieves up to a 2.3x speedup over state-of-the-art tree-search baselines while maintaining competitive accuracy (typically within 5% of the strongest baseline) and reducing the number of explored nodes by 85-90%, demonstrating a practical approach to efficient, scalable LLM reasoning. The implementation of SSDP is publicly available at this https URL.', 'abstract_zh': '基于语义相似性的动态剪枝（SSDP）在并行树搜索中集成在线语义合并，以实时聚类和修剪冗余步骤，提升大型语言模型的推理能力但减轻了计算开销——以GSM8K和MATH500为代表的推理基准上，SSDP相比最先进的树搜索基线可实现2.3倍的速度提升，同时保持竞争力（通常在最强基线的5%以内）并减少探索节点数85-90%，展示了高效可扩展的大语言模型推理的实用方法。SSDP的实现已公开发布。', 'title_zh': 'chopping 树木：基于语义相似性的动态剪枝用于思维树推理'}
{'arxiv_id': 'arXiv:2511.08592', 'title': 'The Collective Turing Test: Large Language Models Can Generate Realistic Multi-User Discussions', 'authors': 'Azza Bouleimen, Giordano De Marzo, Taehee Kim, Nicol`o Pagan, Hannah Metzler, Silvia Giordano, David Garcia', 'link': 'https://arxiv.org/abs/2511.08592', 'abstract': 'Large Language Models (LLMs) offer new avenues to simulate online communities and social media. Potential applications range from testing the design of content recommendation algorithms to estimating the effects of content policies and interventions. However, the validity of using LLMs to simulate conversations between various users remains largely untested. We evaluated whether LLMs can convincingly mimic human group conversations on social media. We collected authentic human conversations from Reddit and generated artificial conversations on the same topic with two LLMs: Llama 3 70B and GPT-4o. When presented side-by-side to study participants, LLM-generated conversations were mistaken for human-created content 39\\% of the time. In particular, when evaluating conversations generated by Llama 3, participants correctly identified them as AI-generated only 56\\% of the time, barely better than random chance. Our study demonstrates that LLMs can generate social media conversations sufficiently realistic to deceive humans when reading them, highlighting both a promising potential for social simulation and a warning message about the potential misuse of LLMs to generate new inauthentic social media content.', 'abstract_zh': '大型语言模型（LLMs）为模拟在线社区和社会媒体提供了新的途径。潜在应用范围从测试内容推荐算法的设计到估算内容政策和干预措施的影响。然而，使用LLMs模拟不同用户之间的对话的有效性仍然主要未经验证。我们评估了LLMs是否能够逼真地模拟社交媒体上的群体对话。我们从Reddit收集了真实的自然对话，并使用两个LLM（Llama 3 70B和GPT-4o）生成了相同主题的虚构对话。当这些虚构对话与真实对话并排呈现给参与者时，有39%的情况下参与者将其误认为是人类创建的内容。特别是，在评估由Llama 3生成的对话时，参与者仅正确识别它们为AI生成56%的时间，这几乎与随机猜测相当。我们的研究证明，LLMs能够生成足够逼真的社交媒体对话，以至于人类在阅读时会受到误导，这既突显了社会模拟的潜在前景，也警示了LLMs可能被滥用以生成新伪社交媒体内容的风险。', 'title_zh': '集体图灵测试：大型语言模型可以生成现实的多用户讨论'}
{'arxiv_id': 'arXiv:2511.08587', 'title': 'Conversational Agents for Building Energy Efficiency -- Advising Housing Cooperatives in Stockholm on Reducing Energy Consumption', 'authors': 'Shadaab Ghani, Anne Håkansson, Oleksii Pasichnyi, Hossein Shahrokni', 'link': 'https://arxiv.org/abs/2511.08587', 'abstract': "Housing cooperative is a common type of multifamily building ownership in Sweden. Although this ownership structure grants decision-making autonomy, it places a burden of responsibility on cooperative's board members. Most board members lack the resources or expertise to manage properties and their energy consumption. This ignorance presents a unique challenge, especially given the EU directives that prohibit buildings rated as energy classes F and G by 2033. Conversational agents (CAs) enable human-like interactions with computer systems, facilitating human-computer interaction across various domains. In our case, CAs can be implemented to support cooperative members in making informed energy retrofitting and usage decisions. This paper introduces a Conversational agent system, called SPARA, designed to advise cooperatives on energy efficiency. SPARA functions as an energy efficiency advisor by leveraging the Retrieval-Augmented Generation (RAG) framework with a Language Model(LM). The LM generates targeted recommendations based on a knowledge base composed of email communications between professional energy advisors and cooperatives' representatives in Stockholm. The preliminary results indicate that SPARA can provide energy efficiency advice with precision 80\\%, comparable to that of municipal energy efficiency (EE) experts. A pilot implementation is currently underway, where municipal EE experts are evaluating SPARA performance based on questions posed to EE experts by BRF members. Our findings suggest that LMs can significantly improve outreach by supporting stakeholders in their energy transition. For future work, more research is needed to evaluate this technology, particularly limitations to the stability and trustworthiness of its energy efficiency advice.", 'abstract_zh': '共有住房是瑞典常见的多户建筑物所有权形式。尽管这种所有权结构赋予了合作组织董事会成员决策自主权，但也将其责任负担转移给了董事会成员。大多数董事会成员缺乏管理物业及其能耗的专业资源或技能。这种无知在欧盟指令规定到2033年禁止建筑被评为能效等级F和G的背景下，构成了独特的挑战。对话代理（CAs）能够实现与计算机系统的类人交互，促进跨不同领域的交互。在我们的情况下，CAs可以被实施以支持合作组织成员做出有关能效提升和能源使用的知情决策。本文介绍了一个名为SPARA的对话代理系统，该系统旨在为合作组织提供能效建议。SPARA通过利用检索增强生成（RAG）框架与语言模型（LM）合作，功能上作为能效顾问。LM基于标准知识库生成推荐，该知识库由斯德哥尔摩专业能源顾问与合作组织代表之间的电子邮件通信组成。初步结果显示，SPARA可以提供精确度为80%的能效建议，与市政能效（EE）专家不相上下。目前正在实施试点项目，市政EE专家根据来自BRF成员提出的关于能效专家问题，评估SPARA的性能。我们的 findings 表明，语言模型可以显著提高与能效相关的利益相关者的接触面。未来的工作需要更多研究来评估这项技术，特别是其能效建议的稳定性和可信度方面的局限性。', 'title_zh': '对话型代理用于建筑能效提升——指导斯德哥尔摩住房合作社降低能源消耗'}
