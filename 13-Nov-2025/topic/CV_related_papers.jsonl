{'arxiv_id': 'arXiv:2511.09072', 'title': 'SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields', 'authors': 'Sangheon Yang, Yeongin Yoon, Hong Mo Jung, Jongwoo Lim', 'link': 'https://arxiv.org/abs/2511.09072', 'abstract': "Traditional Visual Odometry (VO) and Visual Inertial Odometry (VIO) methods rely on a 'pose-centric' paradigm, which computes absolute camera poses from the local map thus requires large-scale landmark maintenance and continuous map optimization. This approach is computationally expensive, limiting their real-time performance on resource-constrained devices. To overcome these limitations, we introduce Sparse Motion Field Visual Odometry (SMF-VO), a lightweight, 'motion-centric' framework. Our approach directly estimates instantaneous linear and angular velocity from sparse optical flow, bypassing the need for explicit pose estimation or expensive landmark tracking. We also employed a generalized 3D ray-based motion field formulation that works accurately with various camera models, including wide-field-of-view lenses. SMF-VO demonstrates superior efficiency and competitive accuracy on benchmark datasets, achieving over 100 FPS on a Raspberry Pi 5 using only a CPU. Our work establishes a scalable and efficient alternative to conventional methods, making it highly suitable for mobile robotics and wearable devices.", 'abstract_zh': '基于稀疏运动场的视觉里程计（SMF-VO）', 'title_zh': 'SMF-VO: 通过稀疏运动场直接估计 ego 运动'}
{'arxiv_id': 'arXiv:2511.09013', 'title': 'UniMM-V2X: MoE-Enhanced Multi-Level Fusion for End-to-End Cooperative Autonomous Driving', 'authors': 'Ziyi Song, Chen Xia, Chenbing Wang, Haibao Yu, Sheng Zhou, Zhisheng Niu', 'link': 'https://arxiv.org/abs/2511.09013', 'abstract': 'Autonomous driving holds transformative potential but remains fundamentally constrained by the limited perception and isolated decision-making with standalone intelligence. While recent multi-agent approaches introduce cooperation, they often focus merely on perception-level tasks, overlooking the alignment with downstream planning and control, or fall short in leveraging the full capacity of the recent emerging end-to-end autonomous driving. In this paper, we present UniMM-V2X, a novel end-to-end multi-agent framework that enables hierarchical cooperation across perception, prediction, and planning. At the core of our framework is a multi-level fusion strategy that unifies perception and prediction cooperation, allowing agents to share queries and reason cooperatively for consistent and safe decision-making. To adapt to diverse downstream tasks and further enhance the quality of multi-level fusion, we incorporate a Mixture-of-Experts (MoE) architecture to dynamically enhance the BEV representations. We further extend MoE into the decoder to better capture diverse motion patterns. Extensive experiments on the DAIR-V2X dataset demonstrate our approach achieves state-of-the-art (SOTA) performance with a 39.7% improvement in perception accuracy, a 7.2% reduction in prediction error, and a 33.2% improvement in planning performance compared with UniV2X, showcasing the strength of our MoE-enhanced multi-level cooperative paradigm.', 'abstract_zh': '自主驾驶蕴含着变革性的潜力，但仍受限于有限的感知能力和独立智能单元下的孤立决策。虽然近期的多智能体方法引入了合作的概念，但往往仅集中在感知层面的任务上，忽视了与下游规划和控制的对齐，或者未能充分利用近期兴起的端到端自主驾驶的全部能力。本文提出了UniMM-V2X，一种新型的端到端多智能体框架，能够实现感知、预测和规划层面的分层合作。框架的核心是一种多级融合策略，该策略统一了感知和预测的合作，使得智能体能够共享查询并协同推理，以实现一致且安全的决策。为了适应多样化的下游任务并进一步提高多级融合的质量，我们引入了Mixture-of-Experts (MoE) 架构，动态增强BEV表示。我们进一步将MoE扩展到解码器，以更好地捕捉多样的运动模式。在DAIR-V2X数据集上进行的广泛实验表明，与UniV2X相比，我们的方法在感知准确性上提高了39.7%，预测误差减少了7.2%，规划性能提高了33.2%，展示了我们增强的MoE多级合作范式的优越性。', 'title_zh': 'UniMM-V2X：MoE增强的多级融合端到端协同自动驾驶'}
{'arxiv_id': 'arXiv:2511.09170', 'title': 'HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests', 'authors': 'Ethan Griffiths, Maryam Haghighat, Simon Denman, Clinton Fookes, Milad Ramezani', 'link': 'https://arxiv.org/abs/2511.09170', 'abstract': 'This article presents HOTFLoc++, an end-to-end framework for LiDAR place recognition, re-ranking, and 6-DoF metric localisation in forests. Leveraging an octree-based transformer, our approach extracts hierarchical local descriptors at multiple granularities to increase robustness to clutter, self-similarity, and viewpoint changes in challenging scenarios, including ground-to-ground and ground-to-aerial in forest and urban environments. We propose a learnable multi-scale geometric verification module to reduce re-ranking failures in the presence of degraded single-scale correspondences. Our coarse-to-fine registration approach achieves comparable or lower localisation errors to baselines, with runtime improvements of two orders of magnitude over RANSAC for dense point clouds. Experimental results on public datasets show the superiority of our approach compared to state-of-the-art methods, achieving an average Recall@1 of 90.7% on CS-Wild-Places: an improvement of 29.6 percentage points over baselines, while maintaining high performance on single-source benchmarks with an average Recall@1 of 91.7% and 96.0% on Wild-Places and MulRan, respectively. Our method achieves under 2 m and 5 degrees error for 97.2% of 6-DoF registration attempts, with our multi-scale re-ranking module reducing localisation errors by ~2$\\times$ on average. The code will be available upon acceptance.', 'abstract_zh': 'HOTFLoc++：森林和城市环境中端到端的LiDAR地点识别、再排序和6-DoF度量局部化框架', 'title_zh': 'HOTFLoc++：森林中端到端分层激光雷达地点识别、重新排名及6自由度度量定位'}
{'arxiv_id': 'arXiv:2511.08615', 'title': 'A Multi-Drone Multi-View Dataset and Deep Learning Framework for Pedestrian Detection and Tracking', 'authors': 'Kosta Dakic, Kanchana Thilakarathna, Rodrigo N. Calheiros, Teng Joon Lim', 'link': 'https://arxiv.org/abs/2511.08615', 'abstract': "Multi-drone surveillance systems offer enhanced coverage and robustness for pedestrian tracking, yet existing approaches struggle with dynamic camera positions and complex occlusions. This paper introduces MATRIX (Multi-Aerial TRacking In compleX environments), a comprehensive dataset featuring synchronized footage from eight drones with continuously changing positions, and a novel deep learning framework for multi-view detection and tracking. Unlike existing datasets that rely on static cameras or limited drone coverage, MATRIX provides a challenging scenario with 40 pedestrians and a significant architectural obstruction in an urban environment. Our framework addresses the unique challenges of dynamic drone-based surveillance through real-time camera calibration, feature-based image registration, and multi-view feature fusion in bird's-eye-view (BEV) representation. Experimental results demonstrate that while static camera methods maintain over 90\\% detection and tracking precision and accuracy metrics in a simplified MATRIX environment without an obstruction, 10 pedestrians and a much smaller observational area, their performance significantly degrades in the complex environment. Our proposed approach maintains robust performance with $\\sim$90\\% detection and tracking accuracy, as well as successfully tracks $\\sim$80\\% of trajectories under challenging conditions. Transfer learning experiments reveal strong generalization capabilities, with the pretrained model achieving much higher detection and tracking accuracy performance compared to training the model from scratch. Additionally, systematic camera dropout experiments reveal graceful performance degradation, demonstrating practical robustness for real-world deployments where camera failures may occur. The MATRIX dataset and framework provide essential benchmarks for advancing dynamic multi-view surveillance systems.", 'abstract_zh': '多无人机跟踪系统在行人监控中提供了增强的覆盖范围和鲁棒性，然而现有方法在处理动态相机位置和复杂遮挡方面存在困难。本文介绍了MATRIX（多无人机复杂环境跟踪系统），这是一个包含八个无人机同步 footage 并且不断改变位置的综合数据集，以及一种新型的多视图检测和跟踪深度学习框架。不同于依赖静态相机或有限无人机覆盖范围的现有数据集，MATRIX 提供了一个具有 40 个行人在城市环境中存在重大建筑遮挡的具有挑战性的场景。本框架通过实时相机校准、基于特征的图像配准和多视图特征在鸟瞰图（BEV）表示中的融合，解决了基于无人机的动态监控的独特挑战。实验结果表明，在未受遮挡的简化 MATRIX 环境中，静态相机方法在检测和跟踪精度方面仍保持超过 90% 的指标，但在复杂环境中，其性能显著下降。我们提出的方法在检测和跟踪精度方面保持了约 90% 的鲁棒性性能，并能够在具有挑战性条件下成功跟踪约 80% 的轨迹。迁移学习实验显示了强大的泛化能力，预训练模型在检测和跟踪准确性方面明显优于从头开始训练模型。此外，系统性摄像机掉线实验显示了平滑的性能下降，证明了在实际部署中可能出现摄像机故障情况下的实用鲁棒性。MATRIX 数据集和框架为推动动态多视图监控系统的发展提供了必要的基准。', 'title_zh': '基于多无人机多视角的行人检测与跟踪数据集及深度学习框架'}
{'arxiv_id': 'arXiv:2511.09502', 'title': 'DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation', 'authors': 'Jerrin Bright, Yuhao Chen, John S. Zelek', 'link': 'https://arxiv.org/abs/2511.09502', 'abstract': "Accurate 3D human pose estimation remains a critical yet unresolved challenge, requiring both temporal coherence across frames and fine-grained modeling of joint relationships. However, most existing methods rely solely on geometric cues and predict each 3D pose independently, which limits their ability to resolve ambiguous motions and generalize to real-world scenarios. Inspired by how humans understand and anticipate motion, we introduce DreamPose3D, a diffusion-based framework that combines action-aware reasoning with temporal imagination for 3D pose estimation. DreamPose3D dynamically conditions the denoising process using task-relevant action prompts extracted from 2D pose sequences, capturing high-level intent. To model the structural relationships between joints effectively, we introduce a representation encoder that incorporates kinematic joint affinity into the attention mechanism. Finally, a hallucinative pose decoder predicts temporally coherent 3D pose sequences during training, simulating how humans mentally reconstruct motion trajectories to resolve ambiguity in perception. Extensive experiments on benchmarked Human3.6M and MPI-3DHP datasets demonstrate state-of-the-art performance across all metrics. To further validate DreamPose3D's robustness, we tested it on a broadcast baseball dataset, where it demonstrated strong performance despite ambiguous and noisy 2D inputs, effectively handling temporal consistency and intent-driven motion variations.", 'abstract_zh': '基于扩散模型的知动作因果推理与时空想象的3D人体姿态估计', 'title_zh': 'DreamPose3D：基于提示学习的生成扩散模型在3D人体姿态估计中的应用'}
{'arxiv_id': 'arXiv:2511.09443', 'title': 'BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation', 'authors': 'Hongchao Shu, Roger D. Soberanis-Mukul, Jiru Xu, Hao Ding, Morgan Ringel, Mali Shen, Saif Iftekar Sayed, Hedyeh Rafii-Tari, Mathias Unberath', 'link': 'https://arxiv.org/abs/2511.09443', 'abstract': 'Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.', 'abstract_zh': '基于视觉的内镜引导框架在支气管镜导航中的应用：一个稳健的基准数据集和优化方法', 'title_zh': 'BronchOpt：基于视觉的Fine-Tuned基础模型驱动的姿态优化及准确支气管镜导航'}
{'arxiv_id': 'arXiv:2511.09147', 'title': 'PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery', 'authors': 'Jiayue Yuan, Fangting Xie, Guangwen Ouyang, Changhai Ma, Ziyu Wu, Heyu Ding, Quan Wan, Yi Ke, Yuchen Wu, Xiaohui Cai', 'link': 'https://arxiv.org/abs/2511.09147', 'abstract': "Multi-person global human mesh recovery (HMR) is crucial for understanding crowd dynamics and interactions. Traditional vision-based HMR methods sometimes face limitations in real-world scenarios due to mutual occlusions, insufficient lighting, and privacy concerns. Human-floor tactile interactions offer an occlusion-free and privacy-friendly alternative for capturing human motion. Existing research indicates that pressure signals acquired from tactile mats can effectively estimate human pose in single-person scenarios. However, when multiple individuals walk randomly on the mat simultaneously, how to distinguish intermingled pressure signals generated by different persons and subsequently acquire individual temporal pressure data remains a pending challenge for extending pressure-based HMR to the multi-person situation. In this paper, we present \\textbf{PressTrack-HMR}, a top-down pipeline that recovers multi-person global human meshes solely from pressure signals. This pipeline leverages a tracking-by-detection strategy to first identify and segment each individual's pressure signal from the raw pressure data, and subsequently performs HMR for each extracted individual signal. Furthermore, we build a multi-person interaction pressure dataset \\textbf{MIP}, which facilitates further research into pressure-based human motion analysis in multi-person scenarios. Experimental results demonstrate that our method excels in multi-person HMR using pressure data, with 89.2~$mm$ MPJPE and 112.6~$mm$ WA-MPJPE$_{100}$, and these showcase the potential of tactile mats for ubiquitous, privacy-preserving multi-person action recognition. Our dataset \\& code are available at this https URL.", 'abstract_zh': '多人群体全局人体网格恢复（HMR）对于理解人群动态和互动至关重要。现有的基于视觉的人体网格恢复（HMR）方法在实际场景中因相互遮挡、照明不足和隐私问题有时会受到限制。人体-地面触觉交互提供了在不遮挡和隐私友好的情况下捕捉人体运动的替代方案。现有研究表明，从触觉垫中获得的压力量信号在单人场景中可以有效估计人体姿态。然而，在多人同时随机走在垫子上时，如何区分不同个体产生的混合压力量信号并随后获取个体的时序压力量数据仍然是将基础压力的HMR扩展到多人情况的待解决问题。本文提出了一种自上而下的\\textbf{PressTrack-HMR}框架，仅从压力量信号中恢复多人群的全局人体网格。该框架采用检测驱动的跟踪策略，首先从原始压力量数据中识别和分割每个个体的压力量信号，然后对提取出的每个个体信号进行HMR。此外，我们构建了一个多人交互压力量数据集\\textbf{MIP}，促进了在多人场景中基于压力量的人体运动分析研究。实验结果表明，我们的方法在使用压力量数据进行多人HMR方面表现出色，MPJPE为89.2 mm，WA-MPJPE$_{100}$为112.6 mm，展示了触觉垫在通用、隐私保护的多人动作识别中的潜力。我们的数据集与代码可在以下链接访问。', 'title_zh': '压力量化的自顶向下多人全球人体网格恢复'}
{'arxiv_id': 'arXiv:2511.09090', 'title': 'Diff-V2M: A Hierarchical Conditional Diffusion Model with Explicit Rhythmic Modeling for Video-to-Music Generation', 'authors': 'Shulei Ji, Zihao Wang, Jiaxing Yu, Xiangyuan Yang, Shuyu Li, Songruoyao Wu, Kejun Zhang', 'link': 'https://arxiv.org/abs/2511.09090', 'abstract': 'Video-to-music (V2M) generation aims to create music that aligns with visual content. However, two main challenges persist in existing methods: (1) the lack of explicit rhythm modeling hinders audiovisual temporal alignments; (2) effectively integrating various visual features to condition music generation remains non-trivial. To address these issues, we propose Diff-V2M, a general V2M framework based on a hierarchical conditional diffusion model, comprising two core components: visual feature extraction and conditional music generation. For rhythm modeling, we begin by evaluating several rhythmic representations, including low-resolution mel-spectrograms, tempograms, and onset detection functions (ODF), and devise a rhythmic predictor to infer them directly from videos. To ensure contextual and affective coherence, we also extract semantic and emotional features. All features are incorporated into the generator via a hierarchical cross-attention mechanism, where emotional features shape the affective tone via the first layer, while semantic and rhythmic features are fused in the second cross-attention layer. To enhance feature integration, we introduce timestep-aware fusion strategies, including feature-wise linear modulation (FiLM) and weighted fusion, allowing the model to adaptively balance semantic and rhythmic cues throughout the diffusion process. Extensive experiments identify low-resolution ODF as a more effective signal for modeling musical rhythm and demonstrate that Diff-V2M outperforms existing models on both in-domain and out-of-domain datasets, achieving state-of-the-art performance in terms of objective metrics and subjective comparisons. Demo and code are available at this https URL.', 'abstract_zh': '基于视频到音乐生成的差分模型（Diff-V2M）', 'title_zh': 'Diff-V2M：一种具有显式节拍建模的分层级条件扩散模型用于视频到音乐生成'}
{'arxiv_id': 'arXiv:2511.08945', 'title': 'FGM-HD: Boosting Generation Diversity of Fractal Generative Models through Hausdorff Dimension Induction', 'authors': 'Haowei Zhang, Yuanpei Zhao, Jizhe Zhou, Mao Li', 'link': 'https://arxiv.org/abs/2511.08945', 'abstract': 'Improving the diversity of generated results while maintaining high visual quality remains a significant challenge in image generation tasks. Fractal Generative Models (FGMs) are efficient in generating high-quality images, but their inherent self-similarity limits the diversity of output images. To address this issue, we propose a novel approach based on the Hausdorff Dimension (HD), a widely recognized concept in fractal geometry used to quantify structural complexity, which aids in enhancing the diversity of generated outputs. To incorporate HD into FGM, we propose a learnable HD estimation method that predicts HD directly from image embeddings, addressing computational cost concerns. However, simply introducing HD into a hybrid loss is insufficient to enhance diversity in FGMs due to: 1) degradation of image quality, and 2) limited improvement in generation diversity. To this end, during training, we adopt an HD-based loss with a monotonic momentum-driven scheduling strategy to progressively optimize the hyperparameters, obtaining optimal diversity without sacrificing visual quality. Moreover, during inference, we employ HD-guided rejection sampling to select geometrically richer outputs. Extensive experiments on the ImageNet dataset demonstrate that our FGM-HD framework yields a 39\\% improvement in output diversity compared to vanilla FGMs, while preserving comparable image quality. To our knowledge, this is the very first work introducing HD into FGM. Our method effectively enhances the diversity of generated outputs while offering a principled theoretical contribution to FGM development.', 'abstract_zh': '提高生成结果的多样性同时保持高视觉质量仍然是图像生成任务中的一个重大挑战。分形生成模型(Fractal Generative Models, FGMs)能够生成高质量的图像，但由于其固有的自相似性限制了输出图像的多样性。为了解决这一问题，我们提出了一种基于豪斯多夫维数(Hausdorff Dimension, HD)的新型方法，豪斯多夫维数是分形几何中广泛认可的概念，用于量化结构的复杂性，有助于增强生成输出的多样性。为了将HD融入FGMs中，我们提出了一种可学习的HD估算方法，该方法直接从图像嵌入中预测HD，解决了计算成本问题。然而，仅将HD引入混合损失中不足以提高FGMs的多样性：1) 图像质量下降；2) 生成多样性提升有限。为此，在训练期间，我们采用基于HD的损失，并结合单调动量驱动的调度策略逐级优化超参数，从而在不牺牲视觉质量的情况下获得最佳的多样性。此外，在推理期间，我们采用基于HD的拒绝采样选择几何结构更丰富的输出。在ImageNet数据集上的广泛实验表明，我们的FGM-HD框架相比vanilla FGMs在输出多样性方面提高了39%，同时保持了相当的图像质量。据我们所知，这是首次将HD引入FGMs的工作。我们的方法有效地提高了生成输出的多样性，并在FGM开发中提供了原则性的理论贡献。', 'title_zh': 'FGM-HD: 基于 Hausdorff 维数诱导提升分形生成模型的生成多样性'}
{'arxiv_id': 'arXiv:2511.08872', 'title': 'SasMamba: A Lightweight Structure-Aware Stride State Space Model for 3D Human Pose Estimation', 'authors': 'Hu Cui, Wenqiang Hua, Renjing Huang, Shurui Jia, Tessai Hayama', 'link': 'https://arxiv.org/abs/2511.08872', 'abstract': 'Recently, the Mamba architecture based on State Space Models (SSMs) has gained attention in 3D human pose estimation due to its linear complexity and strong global modeling capability. However, existing SSM-based methods typically apply manually designed scan operations to flatten detected 2D pose sequences into purely temporal sequences, either locally or globally. This approach disrupts the inherent spatial structure of human poses and entangles spatial and temporal features, making it difficult to capture complex pose dependencies. To address these limitations, we propose the Skeleton Structure-Aware Stride SSM (SAS-SSM), which first employs a structure-aware spatiotemporal convolution to dynamically capture essential local interactions between joints, and then applies a stride-based scan strategy to construct multi-scale global structural representations. This enables flexible modeling of both local and global pose information while maintaining linear computational complexity. Built upon SAS-SSM, our model SasMamba achieves competitive 3D pose estimation performance with significantly fewer parameters compared to existing hybrid models. The source code is available at this https URL.', 'abstract_zh': '基于骨架结构感知的步长状态空间模型（SAS-SSM）及其在3D人体姿态估计中的应用', 'title_zh': 'SasMamba：一种轻量级结构感知步态状态空间模型用于3D人体姿态估计'}
{'arxiv_id': 'arXiv:2511.08823', 'title': 'DT-NVS: Diffusion Transformers for Novel View Synthesis', 'authors': 'Wonbong Jang, Jonathan Tremblay, Lourdes Agapito', 'link': 'https://arxiv.org/abs/2511.08823', 'abstract': 'Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.', 'abstract_zh': '从单个视角生成自然场景的新型视图：一种基于3D扩散模型的方法', 'title_zh': 'DT-NVS: 扩散变换器在新颖视图合成中的应用'}
{'arxiv_id': 'arXiv:2511.08663', 'title': "3D-TDA - Topological feature extraction from 3D images for Alzheimer's disease classification", 'authors': 'Faisal Ahmed, Taymaz Akan, Fatih Gelir, Owen T. Carmichael, Elizabeth A. Disbrow, Steven A. Conrad, Mohammad A. N. Bhuiyan', 'link': 'https://arxiv.org/abs/2511.08663', 'abstract': 'Now that disease-modifying therapies for Alzheimer disease have been approved by regulatory agencies, the early, objective, and accurate clinical diagnosis of AD based on the lowest-cost measurement modalities possible has become an increasingly urgent need. In this study, we propose a novel feature extraction method using persistent homology to analyze structural MRI of the brain. This approach converts topological features into powerful feature vectors through Betti functions. By integrating these feature vectors with a simple machine learning model like XGBoost, we achieve a computationally efficient machine learning model. Our model outperforms state-of-the-art deep learning models in both binary and three-class classification tasks for ADNI 3D MRI disease diagnosis. Using 10-fold cross-validation, our model achieved an average accuracy of 97.43 percent and sensitivity of 99.09 percent for binary classification. For three-class classification, it achieved an average accuracy of 95.47 percent and sensitivity of 94.98 percent. Unlike many deep learning models, our approach does not require data augmentation or extensive preprocessing, making it particularly suitable for smaller datasets. Topological features differ significantly from those commonly extracted using convolutional filters and other deep learning machinery. Because it provides an entirely different type of information from machine learning models, it has the potential to combine topological features with other models later on.', 'abstract_zh': '基于持续同伦的脑结构MRI特征提取方法在ADNI 3D MRI疾病诊断中的应用', 'title_zh': '3D-TDA - 从3D图像中提取拓扑特征进行阿尔茨海默病分类'}
{'arxiv_id': 'arXiv:2511.08651', 'title': 'RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation', 'authors': 'Hae-Won Jo, Yeong-Jun Cho', 'link': 'https://arxiv.org/abs/2511.08651', 'abstract': 'Dynamic Scene Graph Generation (DSGG) models how object relations evolve over time in videos. However, existing methods are trained only on annotated object pairs and lack guidance for non-related pairs, making it difficult to identify meaningful relations during inference. In this paper, we propose Relation Scoring Network (RS-Net), a modular framework that scores the contextual importance of object pairs using both spatial interactions and long-range temporal context. RS-Net consists of a spatial context encoder with learnable context tokens and a temporal encoder that aggregates video-level information. The resulting relation scores are integrated into a unified triplet scoring mechanism to enhance relation prediction. RS-Net can be easily integrated into existing DSGG models without architectural changes. Experiments on the Action Genome dataset show that RS-Net consistently improves both Recall and Precision across diverse baselines, with notable gains in mean Recall, highlighting its ability to address the long-tailed distribution of relations. Despite the increased number of parameters, RS-Net maintains competitive efficiency, achieving superior performance over state-of-the-art methods.', 'abstract_zh': '关系评分网络（RS-Net）：一种用于动态场景图生成的模块化框架', 'title_zh': 'RS-Net: 基于上下文的关系评分方法用于动态场景图生成'}
{'arxiv_id': 'arXiv:2511.08645', 'title': 'Fluence Map Prediction with Deep Learning: A Transformer-based Approach', 'authors': 'Ujunwa Mgboh, Rafi Sultan, Dongxiao Zhu, Joshua Kim', 'link': 'https://arxiv.org/abs/2511.08645', 'abstract': 'Accurate fluence map prediction is essential in intensity-modulated radiation therapy (IMRT) to maximize tumor coverage while minimizing dose to healthy tissues. Conventional optimization is time-consuming and dependent on planner expertise. This study presents a deep learning framework that accelerates fluence map generation while maintaining clinical quality. An end-to-end 3D Swin-UNETR network was trained to predict nine-beam fluence maps directly from volumetric CT images and anatomical contours using 99 prostate IMRT cases (79 for training and 20 for testing). The transformer-based model employs hierarchical self-attention to capture both local anatomical structures and long-range spatial dependencies. Predicted fluence maps were imported into the Eclipse Treatment Planning System for dose recalculation, and model performance was evaluated using beam-wise fluence correlation, spatial gamma analysis, and dose-volume histogram (DVH) metrics. The proposed model achieved an average R^2 of 0.95 +/- 0.02, MAE of 0.035 +/- 0.008, and gamma passing rate of 85 +/- 10 percent (3 percent / 3 mm) on the test set, with no significant differences observed in DVH parameters between predicted and clinical plans. The Swin-UNETR framework enables fully automated, inverse-free fluence map prediction directly from anatomical inputs, enhancing spatial coherence, accuracy, and efficiency while offering a scalable and consistent solution for automated IMRT plan generation.', 'abstract_zh': '准确的剂量分布图预测是强度调制放射治疗（IMRT）中的关键，旨在最大化肿瘤覆盖同时减少对健康组织的剂量。传统优化耗时且依赖计划员的专业知识。本研究提出一种深度学习框架，以加速剂量分布图的生成并保持临床质量。基于Transformer的端到端3D Swin-UNETR网络被训练从体素CT图像和解剖轮廓直接预测九束剂量分布图，使用99例前列腺IMRT病例（79例用于训练，20例用于测试）。该模型采用层级自注意力机制捕捉局部解剖结构和长程空间依赖性。预测的剂量分布图导入Eclipse治疗计划系统进行剂量重算，并通过束级剂量分布相关性、空间伽马分析和剂量体积直方图（DVH）指标评估模型性能。在测试集上，所提出模型的平均R²为0.95 ± 0.02，平均绝对误差（MAE）为0.035 ± 0.008，85 ± 10%（3% / 3 mm）的伽马通过率，预测和临床计划的DVH参数无显著差异。Swin-UNETR框架直接从解剖输入中实现全自动、无逆向的剂量分布图预测，增强空间一致性、准确性和效率，提供一种可扩展且一致的自动IMRT计划生成解决方案。', 'title_zh': '基于Transformer的深度学习照射剂量分布预测方法'}
{'arxiv_id': 'arXiv:2511.08640', 'title': 'Predict and Resist: Long-Term Accident Anticipation under Sensor Noise', 'authors': 'Xingcheng Liu, Bin Rao, Yanchen Guan, Chengyue Wang, Haicheng Liao, Jiaxun Zhang, Chengyu Lin, Meixin Zhu, Zhenning Li', 'link': 'https://arxiv.org/abs/2511.08640', 'abstract': 'Accident anticipation is essential for proactive and safe autonomous driving, where even a brief advance warning can enable critical evasive actions. However, two key challenges hinder real-world deployment: (1) noisy or degraded sensory inputs from weather, motion blur, or hardware limitations, and (2) the need to issue timely yet reliable predictions that balance early alerts with false-alarm suppression. We propose a unified framework that integrates diffusion-based denoising with a time-aware actor-critic model to address these challenges. The diffusion module reconstructs noise-resilient image and object features through iterative refinement, preserving critical motion and interaction cues under sensor degradation. In parallel, the actor-critic architecture leverages long-horizon temporal reasoning and time-weighted rewards to determine the optimal moment to raise an alert, aligning early detection with reliability. Experiments on three benchmark datasets (DAD, CCD, A3D) demonstrate state-of-the-art accuracy and significant gains in mean time-to-accident, while maintaining robust performance under Gaussian and impulse noise. Qualitative analyses further show that our model produces earlier, more stable, and human-aligned predictions in both routine and highly complex traffic scenarios, highlighting its potential for real-world, safety-critical deployment.', 'abstract_zh': '基于去噪扩散和时间意识的actor-critic模型的事故预判统一框架', 'title_zh': '预测与抵抗：传感器噪声下的长期事故预判'}
{'arxiv_id': 'arXiv:2511.08633', 'title': 'Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising', 'authors': 'Assaf Singer, Noam Rotstein, Amir Mann, Ron Kimmel, Or Litany', 'link': 'https://arxiv.org/abs/2511.08633', 'abstract': "Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: this https URL.", 'abstract_zh': '基于扩散的视频生成可以创建逼真视频，但现有的图像和文本条件控制无法提供精确的运动控制。先前的运动条件合成方法通常需要特定模型的微调，这在计算上昂贵且限制性。我们引入了Time-to-Move（TTM），这是一种无需训练、即插即用的框架，用于以图像到视频（I2V）扩散模型为基础进行运动和外观控制的视频生成。我们的核心见解是使用通过友好操作（如剪切拖动或深度重投影）获得的粗略参考动画。受SDEdit利用粗略布局提示进行图像编辑的启发，我们将粗略动画视为粗略运动提示，并将机制适应到视频领域。我们通过图像条件保留外观，并引入区域依赖性的双重时钟去噪策略，该策略在运动指定区域强制执行强对齐，而在其他地方允许灵活性，在用户意图的忠实性和自然动力之间取得平衡。这一轻量级的采样过程修改无需额外的训练或运行时开销，并且与任何骨干兼容。在对象和相机运动基准上的广泛实验表明，TTM在逼真度和运动控制方面与现有的基于训练的基础方法相当或超过。此外，TTM引入了一种独特的能力：通过像素级条件进行精确的外观控制，超越了纯文本提示的局限。访问我们的项目页面查看视频示例和代码：this https URL。', 'title_zh': '停留时间：基于双时钟去噪的无训练移动控制视频生成'}
{'arxiv_id': 'arXiv:2511.08609', 'title': 'Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants', 'authors': 'I. Bailo, F. Buonora, G. Ciarfaglia, L. T. Consoli, A. Evangelista, M. Gabusi, M. Ghiani, C. Petracca Ciavarella, F. Picariello, F. Sarcina, F. Tuosto, V. Zullo, L. Airoldi, G. Bruno, D. D. Gobbo, S. Pezzenati, G. A. Tona', 'link': 'https://arxiv.org/abs/2511.08609', 'abstract': "The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\\%.", 'abstract_zh': '能源转型是过去几十年中的一个关键主题，旨在实现生态可持续的未来，而这样一个重要的领域不能忽视数字化、创新和可用的新技术工具。本文中描述的生成式人工智能模型是由Engineering Ingegneria Informatica SpA开发的，旨在自动化SNAM能源基础设施的工厂结构获取，SNAM是一家意大利和欧洲领先的天然气运输公司。工厂的数字化是指通过解读相关文档来记录其所有相关信息。本文的目标是设计一个基于人工智能技术的有效解决方案，以自动化提取工厂数字化所需的信息，从而简化MGM用户的工作流程。该解决方案以工厂的P&ID作为输入，每个P&ID均为pdf格式，并使用OCR、Vision LLM、对象检测、关系推理和优化算法生成包含两部分信息的输出：结构化的设计数据概述以及工厂的层次框架。为了获得令人信服的结果，我们在最先进的场景图生成模型的基础上，引入了一种全新的Transformer架构，旨在深入分析工厂组件之间的复杂关系。所列的人工智能技术的协同使用，克服了由于缺乏标准化而导致的数据高度多样性的许多障碍。在设计数据的文本信息提取方面，准确率为91%。对于工厂的拓扑结构，93%的组件被正确识别，层次结构的提取准确率约为80%。', 'title_zh': '案例研究：基于Transformer的天然气处理厂自动数字化解决方案'}
