{'arxiv_id': 'arXiv:2511.09558', 'title': 'IFG: Internet-Scale Guidance for Functional Grasping Generation', 'authors': 'Ray Muxin Liu, Mingxuan Li, Kenneth Shaw, Deepak Pathak', 'link': 'https://arxiv.org/abs/2511.09558', 'abstract': 'Large Vision Models trained on internet-scale data have demonstrated strong capabilities in segmenting and semantically understanding object parts, even in cluttered, crowded scenes. However, while these models can direct a robot toward the general region of an object, they lack the geometric understanding required to precisely control dexterous robotic hands for 3D grasping. To overcome this, our key insight is to leverage simulation with a force-closure grasping generation pipeline that understands local geometries of the hand and object in the scene. Because this pipeline is slow and requires ground-truth observations, the resulting data is distilled into a diffusion model that operates in real-time on camera point clouds. By combining the global semantic understanding of internet-scale models with the geometric precision of a simulation-based locally-aware force-closure, \\our achieves high-performance semantic grasping without any manually collected training data. For visualizations of this please visit our website at this https URL', 'abstract_zh': '互联网规模数据训练的大型视觉模型在分段和语义理解物体部分方面表现出强大能力，即使在杂乱、拥挤的场景中也是如此。然而，这些模型尽管可以引导机器人朝物体的大致位置移动，但在精确控制灵巧机械手进行三维抓取时缺乏所需的几何理解。为克服这一问题，我们的关键洞察是利用带有理解和生成场景中手部和物体局部几何形状的力闭合抓取生成管道的模拟。由于该管道运行速度较慢且需要真实观测数据，我们将其结果数据提炼成一个实时运行在摄像头点云上的扩散模型。通过结合互联网规模模型的全局语义理解和基于模拟的局部几何精度力闭合抓取，我们的方法在无需任何手动收集训练数据的情况下实现了高性能语义抓取。请访问我们的网站以查看可视化结果：this https URL。', 'title_zh': 'IFG: 面向功能抓取生成的互联网规模指导'}
{'arxiv_id': 'arXiv:2511.09555', 'title': 'SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation', 'authors': 'Hao Shi, Bin Xie, Yingfei Liu, Yang Yue, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, Gao Huang', 'link': 'https://arxiv.org/abs/2511.09555', 'abstract': 'Robotic manipulation requires precise spatial understanding to interact with objects in the real world. Point-based methods suffer from sparse sampling, leading to the loss of fine-grained semantics. Image-based methods typically feed RGB and depth into 2D backbones pre-trained on 3D auxiliary tasks, but their entangled semantics and geometry are sensitive to inherent depth noise in real-world that disrupts semantic understanding. Moreover, these methods focus on high-level geometry while overlooking low-level spatial cues essential for precise interaction. We propose SpatialActor, a disentangled framework for robust robotic manipulation that explicitly decouples semantics and geometry. The Semantic-guided Geometric Module adaptively fuses two complementary geometry from noisy depth and semantic-guided expert priors. Also, a Spatial Transformer leverages low-level spatial cues for accurate 2D-3D mapping and enables interaction among spatial features. We evaluate SpatialActor on multiple simulation and real-world scenarios across 50+ tasks. It achieves state-of-the-art performance with 87.4% on RLBench and improves by 13.9% to 19.4% under varying noisy conditions, showing strong robustness. Moreover, it significantly enhances few-shot generalization to new tasks and maintains robustness under various spatial perturbations. Project Page: this https URL', 'abstract_zh': '基于空间理解的机器人操作需要精确的空间认知来与真实世界的物体交互。基于点的方法由于稀疏采样而损失了细粒度语义。基于图像的方法通常将RGB和深度输入预训练于三维辅助任务的二维骨干网络，但它们的纠缠语义和几何结构对真实世界固有的深度噪声敏感，这干扰了语义理解。此外，这些方法关注高级几何结构，而忽略了对精确交互至关重要的低级空间线索。我们提出SpatialActor，一种松散耦合语义和几何的鲁棒机器人操作框架。语义引导几何模块自适应融合来自噪声深度和语义引导专家先验的互补几何。此外，空间变换器利用低级空间线索进行准确的二维-三维映射，并在空间特征之间实现交互。我们在50多个任务的多个仿真和真实世界场景中评估SpatialActor，其在RLBench上的性能达到最佳，为87.4%，在不同噪声条件下的性能提升13.9%至19.4%，显示出较强的鲁棒性。此外，它显著增强了基于少量样本的新任务泛化能力，同时在各种空间扰动下保持鲁棒性。项目页面：this https URL', 'title_zh': 'SpatialActor: 探索解耦的时空表示以实现鲁棒的机器人 manipulation'}
{'arxiv_id': 'arXiv:2511.09516', 'title': 'MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation', 'authors': 'Runhao Li, Wenkai Guo, Zhenyu Wu, Changyuan Wang, Haoyuan Deng, Zhenyu Weng, Yap-Peng Tan, Ziwei Wang', 'link': 'https://arxiv.org/abs/2511.09516', 'abstract': 'Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.', 'abstract_zh': '基于记忆增强提示的预训练视觉-语言-行动模型（MAP-VLA）在长时 horizon 机器人 manipulation 任务中的应用', 'title_zh': 'MAP-VLA: 增强记忆提示在机器人操作中视觉语言行动模型的研究'}
{'arxiv_id': 'arXiv:2511.09515', 'title': 'WMPO: World Model-based Policy Optimization for Vision-Language-Action Models', 'authors': 'Fangqi Zhu, Zhengyang Yan, Zicong Hong, Quanxin Shou, Xiao Ma, Song Guo', 'link': 'https://arxiv.org/abs/2511.09515', 'abstract': 'Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.', 'abstract_zh': '基于世界模型的策略优化（WMPO）：一种无需与真实环境交互的视-语-动RL框架', 'title_zh': 'WMPO: 基于世界模型的策略优化方法用于视觉-语言-行动模型'}
{'arxiv_id': 'arXiv:2511.09484', 'title': 'SPIDER: Scalable Physics-Informed Dexterous Retargeting', 'authors': 'Chaoyi Pan, Changhao Wang, Haozhi Qi, Zixi Liu, Homanga Bharadhwaj, Akash Sharma, Tingfan Wu, Guanya Shi, Jitendra Malik, Francois Hogan', 'link': 'https://arxiv.org/abs/2511.09484', 'abstract': 'Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive. In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem. However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots. To bridge this gap, we propose Scalable Physics-Informed DExterous Retargeting (SPIDER), a physics-based retargeting framework to transform and augment kinematic-only human demonstrations to dynamically feasible robot trajectories at scale. Our key insight is that human demonstrations should provide global task structure and objective, while large-scale physics-based sampling with curriculum-style virtual contact guidance should refine trajectories to ensure dynamical feasibility and correct contact sequences. SPIDER scales across diverse 9 humanoid/dexterous hand embodiments and 6 datasets, improving success rates by 18% compared to standard sampling, while being 10X faster than reinforcement learning (RL) baselines, and enabling the generation of a 2.4M frames dynamic-feasible robot dataset for policy learning. As a universal physics-based retargeting method, SPIDER can work with diverse quality data and generate diverse and high-quality data to enable efficient policy learning with methods like RL.', 'abstract_zh': '基于物理的可扩展灵巧再现（SPIDER）：大规模生成动态可行的机器人轨迹以解决灵巧和敏捷政策学习的数据稀缺问题', 'title_zh': 'SPIDER: 可扩展的物理驱动的灵巧动作转移'}
{'arxiv_id': 'arXiv:2511.09331', 'title': 'CoRL-MPPI: Enhancing MPPI With Learnable Behaviours For Efficient And Provably-Safe Multi-Robot Collision Avoidance', 'authors': 'Stepan Dergachev, Artem Pshenitsyn, Aleksandr Panov, Alexey Skrynnik, Konstantin Yakovlev', 'link': 'https://arxiv.org/abs/2511.09331', 'abstract': 'Decentralized collision avoidance remains a core challenge for scalable multi-robot systems. One of the promising approaches to tackle this problem is Model Predictive Path Integral (MPPI) -- a framework that is naturally suited to handle any robot motion model and provides strong theoretical guarantees. Still, in practice MPPI-based controller may provide suboptimal trajectories as its performance relies heavily on uninformed random sampling. In this work, we introduce CoRL-MPPI, a novel fusion of Cooperative Reinforcement Learning and MPPI to address this limitation. We train an action policy (approximated as deep neural network) in simulation that learns local cooperative collision avoidance behaviors. This learned policy is then embedded into the MPPI framework to guide its sampling distribution, biasing it towards more intelligent and cooperative actions. Notably, CoRL-MPPI preserves all the theoretical guarantees of regular MPPI. We evaluate our approach in dense, dynamic simulation environments against state-of-the-art baselines, including ORCA, BVC, and a multi-agent MPPI implementation. Our results demonstrate that CoRL-MPPI significantly improves navigation efficiency (measured by success rate and makespan) and safety, enabling agile and robust multi-robot navigation.', 'abstract_zh': '去中心化避碰仍然是可扩展多机器人系统的核心挑战。一种有前景的解决方案是模型预测路径积分（MPPI）——这是一种天然适用于处理任何机器人运动模型并提供强大理论保证的框架。然而，基于MPPI的控制器在实践中可能提供次优轨迹，其性能高度依赖于未启发式的随机采样。在本工作中，我们提出了CoRL-MPPI，这是一种将合作强化学习与MPPI相结合的新颖融合方法，以解决这一限制。我们在仿真中训练一个动作策略（近似为深度神经网络），使其学习局部合作避碰行为。然后将此学习策略嵌入到MPPI框架中，指导其采样分布，使其偏向更智能和合作的动作。值得注意的是，CoRL-MPPI 保留了常规MPPI的所有理论保证。我们使用密集动态仿真环境对我们的方法与最先进的基线方法（包括ORCA、BVC以及多Agent的MPPI实现）进行评估。我们的结果表明，CoRL-MPPI 显著提高了导航效率（通过成功率和耗时度量）并增强了安全性，从而实现了灵活且可靠的多机器人导航。', 'title_zh': 'CoRL-MPPI: 通过可学习行为提高MPPI以实现高效且证明安全的多机器人避碰'}
{'arxiv_id': 'arXiv:2511.09302', 'title': 'UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning', 'authors': 'Yan Huang, Shoujie Li, Xingting Li, Wenbo Ding', 'link': 'https://arxiv.org/abs/2511.09302', 'abstract': "Data-driven robotic learning faces an obvious dilemma: robust policies demand large-scale, high-quality demonstration data, yet collecting such data remains a major challenge owing to high operational costs, dependence on specialized hardware, and the limited spatial generalization capability of current methods. The Universal Manipulation Interface (UMI) relaxes the strict hardware requirements for data collection, but it is restricted to capturing only RGB images of a scene and omits the 3D geometric information on which many tasks rely. Inspired by DemoGen, we propose UMIGen, a unified framework that consists of two key components: (1) Cloud-UMI, a handheld data collection device that requires no visual SLAM and simultaneously records point cloud observation-action pairs; and (2) a visibility-aware optimization mechanism that extends the DemoGen pipeline to egocentric 3D observations by generating only points within the camera's field of view. These two components enable efficient data generation that aligns with real egocentric observations and can be directly transferred across different robot embodiments without any post-processing. Experiments in both simulated and real-world settings demonstrate that UMIGen supports strong cross-embodiment generalization and accelerates data collection in diverse manipulation tasks.", 'abstract_zh': '数据驱动的机器人学习面临一个明显的困境：鲁棒策略需要大量的高质量示范数据，然而由于操作成本高、依赖专门硬件以及当前方法在空间泛化能力上的局限，收集此类数据仍然是一个主要挑战。通用操作接口（UMI）放松了数据收集的严格硬件要求，但仅能捕捉场景的RGB图像，而不包含许多任务依赖的3D几何信息。受到DemoGen的启发，我们提出了UMIGen统一框架，该框架包含两个关键组件：（1）Cloud-UMI，一种无需视觉SLAM的手持数据采集设备，同时记录点云观测-动作对；（2）一种基于可见性优化机制，通过生成摄像机视野内的点来扩展DemoGen流水线，以进行第一人称3D观察。这两个组件使数据生成更加高效，并且可以实时地在不同机器人身体形态之间直接传输而无需任何后处理。实验结果表明，UMIGen支持强大的跨身体形态泛化，并加速了在多种操作任务中的数据收集。', 'title_zh': 'UMIGen：统一的自视点点云生成与跨躯体机器人模仿学习框架'}
{'arxiv_id': 'arXiv:2511.09241', 'title': 'Unveiling the Impact of Data and Model Scaling on High-Level Control for Humanoid Robots', 'authors': 'Yuxi Wei, Zirui Wang, Kangning Yin, Yue Hu, Jingbo Wang, Siheng Chen', 'link': 'https://arxiv.org/abs/2511.09241', 'abstract': 'Data scaling has long remained a critical bottleneck in robot learning. For humanoid robots, human videos and motion data are abundant and widely available, offering a free and large-scale data source. Besides, the semantics related to the motions enable modality alignment and high-level robot control learning. However, how to effectively mine raw video, extract robot-learnable representations, and leverage them for scalable learning remains an open problem. To address this, we introduce Humanoid-Union, a large-scale dataset generated through an autonomous pipeline, comprising over 260 hours of diverse, high-quality humanoid robot motion data with semantic annotations derived from human motion videos. The dataset can be further expanded via the same pipeline. Building on this data resource, we propose SCHUR, a scalable learning framework designed to explore the impact of large-scale data on high-level control in humanoid robots. Experimental results demonstrate that SCHUR achieves high robot motion generation quality and strong text-motion alignment under data and model scaling, with 37\\% reconstruction improvement under MPJPE and 25\\% alignment improvement under FID comparing with previous methods. Its effectiveness is further validated through deployment in real-world humanoid robot.', 'abstract_zh': '机器人学习中的数据缩放一直是关键瓶颈。对于类人机器人而言，人类的视频和运动数据丰富且广泛可用，提供了免费且规模庞大的数据源。此外，与运动相关的语义信息有助于模态对齐和高阶机器人控制学习。然而，如何有效挖掘原始视频、提取可学习的机器人表示，并利用它们进行可扩展学习仍是一个开放问题。为解决这一问题，我们引入了Humanoid-Union，这是一种通过自主管道生成的大规模数据集，包含了超过260小时的多样化、高质量的类人机器人运动数据，这些数据的语义注释源自人类运动视频。该数据集可通过相同管道进一步扩展。基于这一数据资源，我们提出了一种可扩展的学习框架SCHUR，旨在探索大规模数据对类人机器人高阶控制的影响。实验结果表明，SCHUR 在数据和模型缩放下实现了高质量的机器人运动生成和强大的文本-运动对齐，MPJPE 下重构性能提升了 37%，FID 下对齐性能提升了 25%，并在真实世界类人机器人部署中得到了进一步验证。', 'title_zh': '揭示数据和模型扩展对类人机器人高层次控制影响的研究'}
{'arxiv_id': 'arXiv:2511.09142', 'title': 'LODESTAR: Degeneracy-Aware LiDAR-Inertial Odometry with Adaptive Schmidt-Kalman Filter and Data Exploitation', 'authors': 'Eungchang Mason Lee, Kevin Christiansen Marsim, Hyun Myung', 'link': 'https://arxiv.org/abs/2511.09142', 'abstract': 'LiDAR-inertial odometry (LIO) has been widely used in robotics due to its high accuracy. However, its performance degrades in degenerate environments, such as long corridors and high-altitude flights, where LiDAR measurements are imbalanced or sparse, leading to ill-posed state estimation. In this letter, we present LODESTAR, a novel LIO method that addresses these degeneracies through two key modules: degeneracy-aware adaptive Schmidt-Kalman filter (DA-ASKF) and degeneracy-aware data exploitation (DA-DE). DA-ASKF employs a sliding window to utilize past states and measurements as additional constraints. Specifically, it introduces degeneracy-aware sliding modes that adaptively classify states as active or fixed based on their degeneracy level. Using Schmidt-Kalman update, it partially optimizes active states while preserving fixed states. These fixed states influence the update of active states via their covariances, serving as reference anchors--akin to a lodestar. Additionally, DA-DE prunes less-informative measurements from active states and selectively exploits measurements from fixed states, based on their localizability contribution and the condition number of the Jacobian matrix. Consequently, DA-ASKF enables degeneracy-aware constrained optimization and mitigates measurement sparsity, while DA-DE addresses measurement imbalance. Experimental results show that LODESTAR outperforms existing LiDAR-based odometry methods and degeneracy-aware modules in terms of accuracy and robustness under various degenerate conditions.', 'abstract_zh': 'LODESTAR：一种针对退化环境的新型LiDAR-惯性里程计方法', 'title_zh': 'LODESTAR：aware退化现象的LiDAR-惯性里程计，基于自适应Schmidt-Kalman滤波器和数据利用'}
{'arxiv_id': 'arXiv:2511.09141', 'title': 'RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation', 'authors': 'Xuetao Li, Wenke Huang, Nengyuan Pan, Kaiyan Zhao, Songhua Yang, Yiming Wang, Mengde Li, Mang Ye, Jifeng Xuan, Miao Li', 'link': 'https://arxiv.org/abs/2511.09141', 'abstract': 'Humanoid robots exhibit significant potential in executing diverse human-level skills. However, current research predominantly relies on data-driven approaches that necessitate extensive training datasets to achieve robust multimodal decision-making capabilities and generalizable visuomotor control. These methods raise concerns due to the neglect of geometric reasoning in unseen scenarios and the inefficient modeling of robot-target relationships within the training data, resulting in significant waste of training resources. To address these limitations, we present the Recurrent Geometric-prior Multimodal Policy (RGMP), an end-to-end framework that unifies geometric-semantic skill reasoning with data-efficient visuomotor control. For perception capabilities, we propose the Geometric-prior Skill Selector, which infuses geometric inductive biases into a vision language model, producing adaptive skill sequences for unseen scenes with minimal spatial common sense tuning. To achieve data-efficient robotic motion synthesis, we introduce the Adaptive Recursive Gaussian Network, which parameterizes robot-object interactions as a compact hierarchy of Gaussian processes that recursively encode multi-scale spatial relationships, yielding dexterous, data-efficient motion synthesis even from sparse demonstrations. Evaluated on both our humanoid robot and desktop dual-arm robot, the RGMP framework achieves 87% task success in generalization tests and exhibits 5x greater data efficiency than the state-of-the-art model. This performance underscores its superior cross-domain generalization, enabled by geometric-semantic reasoning and recursive-Gaussion adaptation.', 'abstract_zh': '类人机器人在执行多种人类水平技能方面展现出显著潜力。然而，当前研究主要依赖于数据驱动的方法，需要大量训练数据才能实现稳健的多模态决策能力和可泛化的视觉-运动控制。这些方法因忽视未见场景中的几何推理以及训练数据中机器人-目标关系的低效建模而受到质疑，导致大量训练资源浪费。为解决这些问题，我们提出了循环几何先验多模态策略（RGMP）框架，该框架将几何语义技能推理与数据高效视觉-运动控制统一起来。在感知能力方面，我们提出了几何先验技能选择器，将几何归纳偏差注入视觉语言模型中，产生适用于未见场景的自适应技能序列，对空间常识的调整降到最低。为实现数据高效的机器人运动合成，我们引入了自适应递归高斯网络，将机器人-物体相互作用参数化为紧凑的高斯过程层次结构，递归地编码多尺度空间关系，即使在稀疏演示情况下也能生成灵巧的数据高效的运动合成。在我们的人形机器人和桌面双臂机器人上进行评估，RGMP框架在泛化测试中的任务成功率达到了87%，数据效率比当前最先进的模型高出5倍。这一性能突显了其在跨域泛化中的优越性，得益于几何语义推理和递归高斯适应。', 'title_zh': 'RGMP：循环几何先验多模态政策用于通用 humanoid 机器人操作'}
{'arxiv_id': 'arXiv:2511.09119', 'title': 'Data Assessment for Embodied Intelligence', 'authors': 'Jiahao Xiao, Bowen Yan, Jianbo Zhang, Jia Wang, Chunyi Li, Zhengxue Cheng, Guangtao Zhai', 'link': 'https://arxiv.org/abs/2511.09119', 'abstract': "In embodied intelligence, datasets play a pivotal role, serving as both a knowledge repository and a conduit for information transfer. The two most critical attributes of a dataset are the amount of information it provides and how easily this information can be learned by models. However, the multimodal nature of embodied data makes evaluating these properties particularly challenging. Prior work has largely focused on diversity, typically counting tasks and scenes or evaluating isolated modalities, which fails to provide a comprehensive picture of dataset diversity. On the other hand, the learnability of datasets has received little attention and is usually assessed post-hoc through model training, an expensive, time-consuming process that also lacks interpretability, offering little guidance on how to improve a dataset. In this work, we address both challenges by introducing two principled, data-driven tools. First, we construct a unified multimodal representation for each data sample and, based on it, propose diversity entropy, a continuous measure that characterizes the amount of information contained in a dataset. Second, we introduce the first interpretable, data-driven algorithm to efficiently quantify dataset learnability without training, enabling researchers to assess a dataset's learnability immediately upon its release. We validate our algorithm on both simulated and real-world embodied datasets, demonstrating that it yields faithful, actionable insights that enable researchers to jointly improve diversity and learnability. We hope this work provides a foundation for designing higher-quality datasets that advance the development of embodied intelligence.", 'abstract_zh': '在具身智能中，数据集在知识存储和信息传递中扮演着关键角色。数据集的两个最核心属性是其提供的信息量以及模型学习这些信息的难易程度。然而，具身数据的多模态特性使得评估这些属性尤为具有挑战性。此前的工作主要集中在多样性上，通常通过计数任务和场景或评估孤立的模态来进行，未能提供数据集多样性的全面视角。另一方面，数据集的可学习性受到的关注较少，通常通过模型训练后进行评估，这不仅耗费时间和资源，且缺乏可解释性，难以为改进数据集提供指导。在这项工作中，我们通过引入两个原理性和数据驱动的工具来解决这些问题。首先，我们为每个数据样本构建了一个统一的多模态表示，并在此基础上提出了多样性熵，这是一个连续量度，用来表征数据集中的信息量。其次，我们引入了第一个可解释的数据驱动算法，能够在不进行模型训练的情况下高效地量化数据集的可学习性，使研究人员能够在数据集发布后立即对其可学习性进行评估。我们分别在模拟和实际的具身数据集上验证了该算法，结果表明它能提供忠实和可行的洞察，帮助研究人员同时提升多样性和可学习性。我们希望这项工作能够为设计更高质量的数据集并推动具身智能的发展提供基础。', 'title_zh': '数据评估与体域智能'}
{'arxiv_id': 'arXiv:2511.09104', 'title': 'Decoupling Torque and Stiffness: A Unified Modeling and Control Framework for Antagonistic Artificial Muscles', 'authors': 'Amirhossein Kazemipour, Robert K. Katzschmann', 'link': 'https://arxiv.org/abs/2511.09104', 'abstract': 'Antagonistic soft actuators built from artificial muscles (PAMs, HASELs, DEAs) promise plant-level torque-stiffness decoupling, yet existing controllers for soft muscles struggle to maintain independent control through dynamic contact transients. We present a unified framework enabling independent torque and stiffness commands in real-time for diverse soft actuator types. Our unified force law captures diverse soft muscle physics in a single model with sub-ms computation, while our cascaded controller with analytical inverse dynamics maintains decoupling despite model errors and disturbances. Using co-contraction/bias coordinates, the controller independently modulates torque via bias and stiffness via co-contraction-replicating biological impedance strategies. Simulation-based validation through contact experiments demonstrates maintained independence: 200x faster settling on soft surfaces, 81% force reduction on rigid surfaces, and stable interaction vs 22-54% stability for fixed policies. This framework provides a foundation for enabling musculoskeletal antagonistic systems to execute adaptive impedance control for safe human-robot interaction.', 'abstract_zh': '基于人工肌肉（PAMs、HASELs、DEAs）的对抗性软执行机构有望实现植物级扭矩-刚度解耦，而现有软肌肉控制器难以在动态接触瞬态过程中保持独立控制。我们提出了一种统一框架，能够在实时环境中为不同类型的软执行机构提供独立的扭矩和刚度命令。我们的统一力律在一个模型中捕捉到各种软肌肉的物理特性，并实现了毫秒级的计算；而我们的级联控制器结合分析逆动力学方法，即使在模型误差和干扰存在的情况下也能保持解耦。通过使用共收缩/偏置坐标，控制器通过偏置独立调节扭矩，通过共收缩模仿生物阻抗策略独立调节刚度。基于接触实验的仿真实验验证了维持独立性：软表面快速收敛200倍，刚表面力减少81%，与固定策略相比稳定交互稳定性提高22-54%。该框架为实现适应性阻抗控制以实现安全的人机交互提供了基础。', 'title_zh': '解耦扭矩与刚度：对抗性人工肌肉的统一建模与控制框架'}
{'arxiv_id': 'arXiv:2511.09091', 'title': 'APEX: Action Priors Enable Efficient Exploration for Robust Motion Tracking on Legged Robots', 'authors': 'Shivam Sood, Laukik Nakhwa, Sun Ge, Yuhong Cao, Jin Cheng, Fatemah Zargarbashi, Taerim Yoon, Sungjoon Choi, Stelian Coros, Guillaume Sartoretti', 'link': 'https://arxiv.org/abs/2511.09091', 'abstract': 'Learning natural, animal-like locomotion from demonstrations has become a core paradigm in legged robotics. Despite the recent advancements in motion tracking, most existing methods demand extensive tuning and rely on reference data during deployment, limiting adaptability. We present APEX (Action Priors enable Efficient Exploration), a plug-and-play extension to state-of-the-art motion tracking algorithms that eliminates any dependence on reference data during deployment, improves sample efficiency, and reduces parameter tuning effort. APEX integrates expert demonstrations directly into reinforcement learning (RL) by incorporating decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is combined with a multi-critic framework that balances task performance with motion style. Moreover, APEX enables a single policy to learn diverse motions and transfer reference-like styles across different terrains and velocities, while remaining robust to variations in reward design. We validate the effectiveness of our method through extensive experiments in both simulation and on a Unitree Go2 robot. By leveraging demonstrations to guide exploration during RL training, without imposing explicit bias toward them, APEX enables legged robots to learn with greater stability, efficiency, and generalization. We believe this approach paves the way for guidance-driven RL to boost natural skill acquisition in a wide array of robotic tasks, from locomotion to manipulation. Website and code: this https URL.', 'abstract_zh': '基于演示学习自然的仿动物运动成为腿足机器人研究的核心 paradigm。尽管最近在运动跟踪方面取得了进展，但大多数现有方法在部署时仍需要大量调优并依赖参考数据，限制了适应性。我们提出 APEX（动作先验促进高效探索）——一种可插拔扩展，作为最先进的运动跟踪算法的扩展，能够在部署时消除对参考数据的依赖，提高样本效率，减少参数调优工作量。APEX 通过引入衰减的动作先验将专家演示直接整合到强化学习（RL）中，初始时偏向于专家演示的探索，但逐渐允许策略独立探索。这与多 Critic 框架相结合，平衡任务性能与运动风格。此外，APEX 使得单个策略能够学习多种运动，并在不同地形和速度下传递参考样式的风格，同时对奖励设计的变化具有鲁棒性。我们通过在仿真和 Unitree Go2 机器人上的广泛实验验证了该方法的有效性。通过在 RL 训练期间利用演示来引导探索，而不对其施加显式的偏见，APEX 使腿足机器人能够更加稳定、高效和泛化地学习。我们相信这种方法为通过引导式 RL 在广泛机器人任务中提升自然技能获取开辟了道路，从移动到操作。网站和代码：this https URL。', 'title_zh': 'APEX: 行动先验助力腿足机器人稳健运动追踪的高效探索'}
{'arxiv_id': 'arXiv:2511.09080', 'title': 'D-AWSIM: Distributed Autonomous Driving Simulator for Dynamic Map Generation Framework', 'authors': 'Shunsuke Ito, Chaoran Zhao, Ryo Okamura, Takuya Azumi', 'link': 'https://arxiv.org/abs/2511.09080', 'abstract': 'Autonomous driving systems have achieved significant advances, and full autonomy within defined operational design domains near practical deployment. Expanding these domains requires addressing safety assurance under diverse conditions. Information sharing through vehicle-to-vehicle and vehicle-to-infrastructure communication, enabled by a Dynamic Map platform built from vehicle and roadside sensor data, offers a promising solution. Real-world experiments with numerous infrastructure sensors incur high costs and regulatory challenges. Conventional single-host simulators lack the capacity for large-scale urban traffic scenarios. This paper proposes D-AWSIM, a distributed simulator that partitions its workload across multiple machines to support the simulation of extensive sensor deployment and dense traffic environments. A Dynamic Map generation framework on D-AWSIM enables researchers to explore information-sharing strategies without relying on physical testbeds. The evaluation shows that D-AWSIM increases throughput for vehicle count and LiDAR sensor processing substantially compared to a single-machine setup. Integration with Autoware demonstrates applicability for autonomous driving research.', 'abstract_zh': '自主驾驶系统已取得显著进步，并在定义的操作设计域内接近实际部署的全自主驾驶。扩展这些领域需要在多种条件下解决安全保证问题。通过车辆间和车辆与基础设施之间的通信共享信息，利用从车辆和路边传感器数据构建的动态地图平台提供了一种有前景的解决方案。实际基础设施传感器的广泛应用会带来高昂的成本和监管挑战。传统单机模拟器无法支持大规模城市交通场景。本文提出了一种分布式模拟器D-AWSIM，该模拟器将工作负载分配到多台机器上，以支持广泛的传感器部署和密集交通环境的模拟。D-AWSIM上的动态地图生成框架使研究人员能够在不依赖物理试验床的情况下探索信息共享策略。评估结果显示，与单机设置相比，D-AWSIM在车辆数量和LiDAR传感器处理方面的吞吐量显著提高。与Autoware的集成展示了其在自主驾驶研究中的应用潜力。', 'title_zh': 'D-AWSIM：分布式自主驾驶模拟器及其用于动态地图生成框架'}
{'arxiv_id': 'arXiv:2511.09072', 'title': 'SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields', 'authors': 'Sangheon Yang, Yeongin Yoon, Hong Mo Jung, Jongwoo Lim', 'link': 'https://arxiv.org/abs/2511.09072', 'abstract': "Traditional Visual Odometry (VO) and Visual Inertial Odometry (VIO) methods rely on a 'pose-centric' paradigm, which computes absolute camera poses from the local map thus requires large-scale landmark maintenance and continuous map optimization. This approach is computationally expensive, limiting their real-time performance on resource-constrained devices. To overcome these limitations, we introduce Sparse Motion Field Visual Odometry (SMF-VO), a lightweight, 'motion-centric' framework. Our approach directly estimates instantaneous linear and angular velocity from sparse optical flow, bypassing the need for explicit pose estimation or expensive landmark tracking. We also employed a generalized 3D ray-based motion field formulation that works accurately with various camera models, including wide-field-of-view lenses. SMF-VO demonstrates superior efficiency and competitive accuracy on benchmark datasets, achieving over 100 FPS on a Raspberry Pi 5 using only a CPU. Our work establishes a scalable and efficient alternative to conventional methods, making it highly suitable for mobile robotics and wearable devices.", 'abstract_zh': '基于稀疏运动场的视觉里程计（SMF-VO）', 'title_zh': 'SMF-VO: 通过稀疏运动场直接估计 ego 运动'}
{'arxiv_id': 'arXiv:2511.09020', 'title': 'A Quantum Tunneling and Bio-Phototactic Driven Enhanced Dwarf Mongoose Optimizer for UAV Trajectory Planning and Engineering Problem', 'authors': 'Mingyang Yu, Haorui Yang, Kangning An, Xinjian Wei, Xiaoxuan Xu, Jing Xu', 'link': 'https://arxiv.org/abs/2511.09020', 'abstract': 'With the widespread adoption of unmanned aerial vehicles (UAV), effective path planning has become increasingly important. Although traditional search methods have been extensively applied, metaheuristic algorithms have gained popularity due to their efficiency and problem-specific heuristics. However, challenges such as premature convergence and lack of solution diversity still hinder their performance in complex scenarios. To address these issues, this paper proposes an Enhanced Multi-Strategy Dwarf Mongoose Optimization (EDMO) algorithm, tailored for three-dimensional UAV trajectory planning in dynamic and obstacle-rich environments. EDMO integrates three novel strategies: (1) a Dynamic Quantum Tunneling Optimization Strategy (DQTOS) to enable particles to probabilistically escape local optima; (2) a Bio-phototactic Dynamic Focusing Search Strategy (BDFSS) inspired by microbial phototaxis for adaptive local refinement; and (3) an Orthogonal Lens Opposition-Based Learning (OLOBL) strategy to enhance global exploration through structured dimensional recombination. EDMO is benchmarked on 39 standard test functions from CEC2017 and CEC2020, outperforming 14 advanced algorithms in convergence speed, robustness, and optimization accuracy. Furthermore, real-world validations on UAV three-dimensional path planning and three engineering design tasks confirm its practical applicability and effectiveness in field robotics missions requiring intelligent, adaptive, and time-efficient planning.', 'abstract_zh': '基于多策略增强矮獴优化算法的三维无人机路径规划', 'title_zh': '一种基于量子隧道效应和光生物趋化性的迷你.za狐蚁优化器在无人机航迹规划及工程问题中的增强应用'}
{'arxiv_id': 'arXiv:2511.09013', 'title': 'UniMM-V2X: MoE-Enhanced Multi-Level Fusion for End-to-End Cooperative Autonomous Driving', 'authors': 'Ziyi Song, Chen Xia, Chenbing Wang, Haibao Yu, Sheng Zhou, Zhisheng Niu', 'link': 'https://arxiv.org/abs/2511.09013', 'abstract': 'Autonomous driving holds transformative potential but remains fundamentally constrained by the limited perception and isolated decision-making with standalone intelligence. While recent multi-agent approaches introduce cooperation, they often focus merely on perception-level tasks, overlooking the alignment with downstream planning and control, or fall short in leveraging the full capacity of the recent emerging end-to-end autonomous driving. In this paper, we present UniMM-V2X, a novel end-to-end multi-agent framework that enables hierarchical cooperation across perception, prediction, and planning. At the core of our framework is a multi-level fusion strategy that unifies perception and prediction cooperation, allowing agents to share queries and reason cooperatively for consistent and safe decision-making. To adapt to diverse downstream tasks and further enhance the quality of multi-level fusion, we incorporate a Mixture-of-Experts (MoE) architecture to dynamically enhance the BEV representations. We further extend MoE into the decoder to better capture diverse motion patterns. Extensive experiments on the DAIR-V2X dataset demonstrate our approach achieves state-of-the-art (SOTA) performance with a 39.7% improvement in perception accuracy, a 7.2% reduction in prediction error, and a 33.2% improvement in planning performance compared with UniV2X, showcasing the strength of our MoE-enhanced multi-level cooperative paradigm.', 'abstract_zh': '自主驾驶蕴含着变革性的潜力，但仍受限于有限的感知能力和独立智能单元下的孤立决策。虽然近期的多智能体方法引入了合作的概念，但往往仅集中在感知层面的任务上，忽视了与下游规划和控制的对齐，或者未能充分利用近期兴起的端到端自主驾驶的全部能力。本文提出了UniMM-V2X，一种新型的端到端多智能体框架，能够实现感知、预测和规划层面的分层合作。框架的核心是一种多级融合策略，该策略统一了感知和预测的合作，使得智能体能够共享查询并协同推理，以实现一致且安全的决策。为了适应多样化的下游任务并进一步提高多级融合的质量，我们引入了Mixture-of-Experts (MoE) 架构，动态增强BEV表示。我们进一步将MoE扩展到解码器，以更好地捕捉多样的运动模式。在DAIR-V2X数据集上进行的广泛实验表明，与UniV2X相比，我们的方法在感知准确性上提高了39.7%，预测误差减少了7.2%，规划性能提高了33.2%，展示了我们增强的MoE多级合作范式的优越性。', 'title_zh': 'UniMM-V2X：MoE增强的多级融合端到端协同自动驾驶'}
{'arxiv_id': 'arXiv:2511.08942', 'title': 'Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning', 'authors': 'Mobin Habibpour, Fatemeh Afghah', 'link': 'https://arxiv.org/abs/2511.08942', 'abstract': "While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.", 'abstract_zh': '视觉-语言模型在机器人导航中的潜力解锁：从被动观察者到主动策略师的角色转变', 'title_zh': '思考、记忆、导航：基于VLM驱动推理的零样本物体目标导航'}
{'arxiv_id': 'arXiv:2511.08935', 'title': 'Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation', 'authors': 'Ningnan Wang, Weihuang Chen, Liming Chen, Haoxuan Ji, Zhongyu Guo, Xuchong Zhang, Hongbin Sun', 'link': 'https://arxiv.org/abs/2511.08935', 'abstract': 'Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.', 'abstract_zh': '基于潜在空间的语义认知探索（SCOPE）：零样本环境导航中的前沿信息利用', 'title_zh': '扩展您的SCOPE：基于潜力场的语义认知在体感视觉导航中的应用'}
{'arxiv_id': 'arXiv:2511.08912', 'title': 'A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction', 'authors': 'Jinyu Zhang, Lijun Han, Feng Jian, Lingxi Zhang, Hesheng Wang', 'link': 'https://arxiv.org/abs/2511.08912', 'abstract': "In mobile robot shared control, effectively understanding human motion intention is critical for seamless human-robot collaboration. This paper presents a novel shared control framework featuring planning-level intention prediction. A path replanning algorithm is designed to adjust the robot's desired trajectory according to inferred human intentions. To represent future motion intentions, we introduce the concept of an intention domain, which serves as a constraint for path replanning. The intention-domain prediction and path replanning problems are jointly formulated as a Markov Decision Process and solved through deep reinforcement learning. In addition, a Voronoi-based human trajectory generation algorithm is developed, allowing the model to be trained entirely in simulation without human participation or demonstration data. Extensive simulations and real-world user studies demonstrate that the proposed method significantly reduces operator workload and enhances safety, without compromising task efficiency compared with existing assistive teleoperation approaches.", 'abstract_zh': '移动机器人共享控制中，有效理解人类运动意图对于实现无缝的人机协作至关重要。本文提出了一种新的共享控制框架，该框架强调意图预测在计划级别上的应用。设计了一条路径重规划算法，根据推断的人类意图调整机器人的期望轨迹。为了表示未来的运动意图，引入了意图域的概念，作为路径重规划的约束。意图域预测和路径重规划问题被联合形式化为马尔可夫决策过程，并通过深度强化学习求解。此外，开发了一种基于Voronoi的人类轨迹生成算法，使得模型可以在完全离线仿真的情况下进行训练，不需要人类的参与或示例数据。广泛的仿真实验和现实世界用户研究表明，所提出的方法显著减少了操作员的工作负担，提高了安全性，且与现有的辅助遥操作方法相比并未牺牲任务效率。', 'title_zh': '基于计划层级意图预测的移动机器人共享控制框架'}
{'arxiv_id': 'arXiv:2511.08865', 'title': 'MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror', 'authors': 'Cong Tai, Hansheng Wu, Haixu Long, Zhengbin Long, Zhaoyu Zheng, Haodong Xiang, Tao Shen', 'link': 'https://arxiv.org/abs/2511.08865', 'abstract': 'In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.', 'abstract_zh': '基于PICO的低成本实时手部运动与姿态数据获取机器人远程操作框架：实现稳定的精确机器人 trajectories 记录以构建视觉-语言-动作数据集，并支持各种末端执行器机器人实时远程操作，从而降低上肢机器人操作研究的技术门槛，促进相关研究进展。', 'title_zh': '镜像四肢：基于RealMirror实现手部姿态获取与机器人遥操作'}
{'arxiv_id': 'arXiv:2511.08863', 'title': 'XPRESS: X-Band Radar Place Recognition via Elliptical Scan Shaping', 'authors': 'Hyesu Jang, Wooseong Yang, Ayoung Kim, Dongje Lee, Hanguen Kim', 'link': 'https://arxiv.org/abs/2511.08863', 'abstract': "X-band radar serves as the primary sensor on maritime vessels, however, its application in autonomous navigation has been limited due to low sensor resolution and insufficient information content. To enable X-band radar-only autonomous navigation in maritime environments, this paper proposes a place recognition algorithm specifically tailored for X-band radar, incorporating an object density-based rule for efficient candidate selection and intentional degradation of radar detections to achieve robust retrieval performance. The proposed algorithm was evaluated on both public maritime radar datasets and our own collected dataset, and its performance was compared against state-of-the-art radar place recognition methods. An ablation study was conducted to assess the algorithm's performance sensitivity with respect to key parameters.", 'abstract_zh': 'X波段雷达在海事船舶中的应用主要依赖其作为主要传感器，但由于传感器分辨率低和信息量不足，其在自主导航中的应用受到限制。为使X波段雷达能够在海事环境中实现自主导航，本文提出了一种专门针对X波段雷达的场所识别算法，该算法结合了基于物体密度的选择规则，并故意降级雷达检测以实现稳健的检索性能。该算法在公共海事雷达数据集和我们收集的数据集上进行了评估，并将其性能与最先进的雷达场所识别方法进行了比较。进行了消融研究以评估算法对关键参数的性能敏感性。', 'title_zh': 'XPRESS：基于椭圆扫描成型的X波段雷达Place Recognition'}
{'arxiv_id': 'arXiv:2511.08822', 'title': 'Low-cost Multi-agent Fleet for Acoustic Cooperative Localization Research', 'authors': 'Nelson Durrant, Braden Meyers, Matthew McMurray, Clayton Smith, Brighton Anderson, Tristan Hodgins, Kalliyan Velasco, Joshua G. Mangelson', 'link': 'https://arxiv.org/abs/2511.08822', 'abstract': 'Real-world underwater testing for multi-agent autonomy presents substantial financial and engineering challenges. In this work, we introduce the Configurable Underwater Group of Autonomous Robots (CoUGARs) as a low-cost, configurable autonomous-underwater-vehicle (AUV) platform for multi-agent autonomy research. The base design costs less than $3,000 USD (as of May 2025) and is based on commercially-available and 3D-printed parts, enabling quick customization for various sensor payloads and configurations. Our current expanded model is equipped with a doppler velocity log (DVL) and ultra-short-baseline (USBL) acoustic array/transducer to support research on acoustic-based cooperative localization. State estimation, navigation, and acoustic communications software has been developed and deployed using a containerized software stack and is tightly integrated with the HoloOcean simulator. The system was tested both in simulation and via in-situ field trials in Utah lakes and reservoirs.', 'abstract_zh': '面向多自主-agent自主性的实地水下测试面临着重大的财务和工程挑战。在此项工作中，我们介绍了可配置水下自主机器人组（CoUGARs）作为一种低成本且可配置的自主 underwater 机器人（AUV）平台，用于多自主-agent自主性研究。基础设计成本低于3000美元（截至2025年5月），基于商用和3D打印部件，能够快速定制各种传感器载荷和配置。我们当前扩展的模型配备了多普勒速度 log（DVL）和超短基线（USBL）声学阵列/换能器，以支持基于声波的协同定位研究。已经开发并部署了状态估计、导航和声学通信软件，并使用容器化软件栈进行集成，并与HoloOcean模拟器紧密集成。该系统在模拟中进行了测试，并在犹他州的湖泊和水库中进行了现场试验。', 'title_zh': '低成本多agent舰队在声学协同定位研究'}
{'arxiv_id': 'arXiv:2511.08778', 'title': 'Dual-Arm Whole-Body Motion Planning: Leveraging Overlapping Kinematic Chains', 'authors': 'Richard Cheng, Peter Werner, Carolyn Matl', 'link': 'https://arxiv.org/abs/2511.08778', 'abstract': 'High degree-of-freedom dual-arm robots are becoming increasingly common due to their morphology enabling them to operate effectively in human environments. However, motion planning in real-time within unknown, changing environments remains a challenge for such robots due to the high dimensionality of the configuration space and the complex collision-avoidance constraints that must be obeyed. In this work, we propose a novel way to alleviate the curse of dimensionality by leveraging the structure imposed by shared joints (e.g. torso joints) in a dual-arm robot. First, we build two dynamic roadmaps (DRM) for each kinematic chain (i.e. left arm + torso, right arm + torso) with specific structure induced by the shared joints. Then, we show that we can leverage this structure to efficiently search through the composition of the two roadmaps and largely sidestep the curse of dimensionality. Finally, we run several experiments in a real-world grocery store with this motion planner on a 19 DoF mobile manipulation robot executing a grocery fulfillment task, achieving 0.4s average planning times with 99.9% success rate across more than 2000 motion plans.', 'abstract_zh': '具有高自由度的双臂机器人由于其形态能够在人类环境中有效操作而越来越多地被使用。然而，由于配置空间的高维性和必须遵守的复杂碰撞避免约束，在未知且不断变化的环境中进行实时运动规划仍然是一个挑战。在本工作中，我们提出了一种新颖的方法，通过利用双臂机器人中共享关节（例如躯干关节）所施加的结构来缓解维数灾。首先，我们为每个运动链（即左臂+躯干，右臂+躯干）构建两个动态 roadmap (DRM)，并引入由共享关节引起的特定结构。然后，我们表明可以利用这种结构高效地搜索两个 roadmap 的组合，并大大避免维数灾。最后，我们在这个具有19个自由度的移动操作机器人上对该运动规划器进行了多次实验，在现实世界的超市中执行杂货补货任务，实现了超过2000个运动计划的平均规划时间为0.4秒，成功率高达99.9%。', 'title_zh': '双臂全身运动规划：利用重叠运动学链'}
{'arxiv_id': 'arXiv:2511.08771', 'title': 'CENIC: Convex Error-controlled Numerical Integration for Contact', 'authors': 'Vince Kurtz, Alejandro Castro', 'link': 'https://arxiv.org/abs/2511.08771', 'abstract': 'State-of-the-art robotics simulators operate in discrete time. This requires users to choose a time step, which is both critical and challenging: large steps can produce non-physical artifacts, while small steps force the simulation to run slowly. Continuous-time error-controlled integration avoids such issues by automatically adjusting the time step to achieve a desired accuracy. But existing error-controlled integrators struggle with the stiff dynamics of contact, and cannot meet the speed and scalability requirements of modern robotics workflows. We introduce CENIC, a new continuous-time integrator that brings together recent advances in convex time-stepping and error-controlled integration, inheriting benefits from both continuous integration and discrete time-stepping. CENIC runs at fast real-time rates comparable to discrete-time robotics simulators like MuJoCo, Drake and Isaac Sim, while also providing guarantees on accuracy and convergence.', 'abstract_zh': '一种新的连续时间误差控制积分器：CENIC', 'title_zh': 'CENIC: 凸误差控制数值积分方法用于接触模拟'}
{'arxiv_id': 'arXiv:2511.08741', 'title': 'ATOM-CBF: Adaptive Safe Perception-Based Control under Out-of-Distribution Measurements', 'authors': 'Kai S. Yun, Navid Azizan', 'link': 'https://arxiv.org/abs/2511.08741', 'abstract': 'Ensuring the safety of real-world systems is challenging, especially when they rely on learned perception modules to infer the system state from high-dimensional sensor data. These perception modules are vulnerable to epistemic uncertainty, often failing when encountering out-of-distribution (OoD) measurements not seen during training. To address this gap, we introduce ATOM-CBF (Adaptive-To-OoD-Measurement Control Barrier Function), a novel safe control framework that explicitly computes and adapts to the epistemic uncertainty from OoD measurements, without the need for ground-truth labels or information on distribution shifts. Our approach features two key components: (1) an OoD-aware adaptive perception error margin and (2) a safety filter that integrates this adaptive error margin, enabling the filter to adjust its conservatism in real-time. We provide empirical validation in simulations, demonstrating that ATOM-CBF maintains safety for an F1Tenth vehicle with LiDAR scans and a quadruped robot with RGB images.', 'abstract_zh': '确保真实世界系统的安全性具有挑战性，尤其是在它们依赖于从高维传感器数据中推断系统状态的学习感知模块时。这些感知模块对epistemic不确定性敏感，经常在遇到训练中未见过的(out-of-distribution, OoD)测量值时失效。为解决这一问题，我们引入了ATOM-CBF（Adaptive-To-OoD-Measurement Control Barrier Function）这一新颖的安全控制框架，该框架能够明确计算并适应从OoD测量值中获得的epistemic不确定性，而无需 ground-truth标签或分布变化信息。我们的方法包含两个关键组件：(1) OoD感知误差容限的自适应调整，以及(2) 结合该自适应误差容限的安全过滤器，使过滤器能够实时调整其保守程度。我们在仿真中提供了 empirical验证，表明ATOM-CBF能够确保使用LiDAR扫描的F1Tenth车辆和使用RGB图像的四足机器人的安全性。', 'title_zh': 'ATOM-CBF: 基于自适应安全感知的异分布测量控制'}
{'arxiv_id': 'arXiv:2511.08732', 'title': 'Intuitive Programming, Adaptive Task Planning, and Dynamic Role Allocation in Human-Robot Collaboration', 'authors': 'Marta Lagomarsino, Elena Merlo, Andrea Pupa, Timo Birr, Franziska Krebs, Cristian Secchi, Tamim Asfour, Arash Ajoudani', 'link': 'https://arxiv.org/abs/2511.08732', 'abstract': 'Remarkable capabilities have been achieved by robotics and AI, mastering complex tasks and environments. Yet, humans often remain passive observers, fascinated but uncertain how to engage. Robots, in turn, cannot reach their full potential in human-populated environments without effectively modeling human states and intentions and adapting their behavior. To achieve a synergistic human-robot collaboration (HRC), a continuous information flow should be established: humans must intuitively communicate instructions, share expertise, and express needs. In parallel, robots must clearly convey their internal state and forthcoming actions to keep users informed, comfortable, and in control. This review identifies and connects key components enabling intuitive information exchange and skill transfer between humans and robots. We examine the full interaction pipeline: from the human-to-robot communication bridge translating multimodal inputs into robot-understandable representations, through adaptive planning and role allocation, to the control layer and feedback mechanisms to close the loop. Finally, we highlight trends and promising directions toward more adaptive, accessible HRC.', 'abstract_zh': '机器人与人工智能在掌握复杂任务和环境方面取得了显著能力，但人类常作为被动观察者，感到好奇却不确定如何参与。机器人要在含有大量人类的环境中充分发挥潜力，必须有效地模拟人类状态和意图并适应其行为。为了实现人类与机器人协同合作（HRC），应建立持续的信息流通：人类必须直观地传达指令、分享专长并表达需求。同时，机器人必须清晰地传达其内部状态和即将采取的行动，以保持用户的知情权、舒适感和控制感。本文综述了 enable 人机之间直观信息交流和技能转移的关键组成部分，分析了从人类-机器人通信桥梁将多模态输入转换为机器人可理解的表示，到自适应规划和角色分配，再到控制层和反馈机制以形成闭环的完整交互管道。最后，本文指出了更适应性、更易访问的HRC的发展趋势和有前景的方向。', 'title_zh': '直觉编程、自适应任务规划与动态角色分配在人机协作中的应用'}
{'arxiv_id': 'arXiv:2511.08694', 'title': 'Practical and Performant Enhancements for Maximization of Algebraic Connectivity', 'authors': 'Leonard Jung, Alan Papalia, Kevin Doherty, Michael Everett', 'link': 'https://arxiv.org/abs/2511.08694', 'abstract': "Long-term state estimation over graphs remains challenging as current graph estimation methods scale poorly on large, long-term graphs. To address this, our work advances a current state-of-the-art graph sparsification algorithm, maximizing algebraic connectivity (MAC). MAC is a sparsification method that preserves estimation performance by maximizing the algebraic connectivity, a spectral graph property that is directly connected to the estimation error. Unfortunately, MAC remains computationally prohibitive for online use and requires users to manually pre-specify a connectivity-preserving edge set. Our contributions close these gaps along three complementary fronts: we develop a specialized solver for algebraic connectivity that yields an average 2x runtime speedup; we investigate advanced step size strategies for MAC's optimization procedure to enhance both convergence speed and solution quality; and we propose automatic schemes that guarantee graph connectivity without requiring manual specification of edges. Together, these contributions make MAC more scalable, reliable, and suitable for real-time estimation applications.", 'abstract_zh': '长期图上状态估计仍然具有挑战性，因为当前的图估计方法在大型、长期的图上扩展性差。为了应对这一挑战，我们的工作改进了当前最先进的图稀疏化算法，最大化代数连通性（MAC）。MAC是一种通过最大化代数连通性（与估计误差直接相关的谱图性质）来保持估计性能的稀疏化方法。然而，MAC仍难以用于在线应用，并要求用户手动预先指定一个保持连通性的边集。我们的贡献从三个方面解决了这些缺陷：我们开发了一个专有的代数连通性求解器，平均提供了2倍的运行时间加速；我们研究了MAC优化过程中的高级步长策略，以提高收敛速度和解决方案质量；我们提出了一种自动方案，能够保证图的连通性而不必手动指定边。这些贡献使得MAC更具有可扩展性、可靠性和适用于实时估计应用。', 'title_zh': '具有良好代数连通性的实用高效增强方法'}
{'arxiv_id': 'arXiv:2511.09170', 'title': 'HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests', 'authors': 'Ethan Griffiths, Maryam Haghighat, Simon Denman, Clinton Fookes, Milad Ramezani', 'link': 'https://arxiv.org/abs/2511.09170', 'abstract': 'This article presents HOTFLoc++, an end-to-end framework for LiDAR place recognition, re-ranking, and 6-DoF metric localisation in forests. Leveraging an octree-based transformer, our approach extracts hierarchical local descriptors at multiple granularities to increase robustness to clutter, self-similarity, and viewpoint changes in challenging scenarios, including ground-to-ground and ground-to-aerial in forest and urban environments. We propose a learnable multi-scale geometric verification module to reduce re-ranking failures in the presence of degraded single-scale correspondences. Our coarse-to-fine registration approach achieves comparable or lower localisation errors to baselines, with runtime improvements of two orders of magnitude over RANSAC for dense point clouds. Experimental results on public datasets show the superiority of our approach compared to state-of-the-art methods, achieving an average Recall@1 of 90.7% on CS-Wild-Places: an improvement of 29.6 percentage points over baselines, while maintaining high performance on single-source benchmarks with an average Recall@1 of 91.7% and 96.0% on Wild-Places and MulRan, respectively. Our method achieves under 2 m and 5 degrees error for 97.2% of 6-DoF registration attempts, with our multi-scale re-ranking module reducing localisation errors by ~2$\\times$ on average. The code will be available upon acceptance.', 'abstract_zh': 'HOTFLoc++：森林和城市环境中端到端的LiDAR地点识别、再排序和6-DoF度量局部化框架', 'title_zh': 'HOTFLoc++：森林中端到端分层激光雷达地点识别、重新排名及6自由度度量定位'}
{'arxiv_id': 'arXiv:2511.09032', 'title': 'Argus: Resilience-Oriented Safety Assurance Framework for End-to-End ADSs', 'authors': 'Dingji Wang, You Lu, Bihuan Chen, Shuo Hao, Haowen Jiang, Yifan Tian, Xin Peng', 'link': 'https://arxiv.org/abs/2511.09032', 'abstract': 'End-to-end autonomous driving systems (ADSs), with their strong capabilities in environmental perception and generalizable driving decisions, are attracting growing attention from both academia and industry. However, once deployed on public roads, ADSs are inevitably exposed to diverse driving hazards that may compromise safety and degrade system performance. This raises a strong demand for resilience of ADSs, particularly the capability to continuously monitor driving hazards and adaptively respond to potential safety violations, which is crucial for maintaining robust driving behaviors in complex driving scenarios.\nTo bridge this gap, we propose a runtime resilience-oriented framework, Argus, to mitigate the driving hazards, thus preventing potential safety violations and improving the driving performance of an ADS. Argus continuously monitors the trajectories generated by the ADS for potential hazards and, whenever the EGO vehicle is deemed unsafe, seamlessly takes control through a hazard mitigator. We integrate Argus with three state-of-the-art end-to-end ADSs, i.e., TCP, UniAD and VAD. Our evaluation has demonstrated that Argus effectively and efficiently enhances the resilience of ADSs, improving the driving score of the ADS by up to 150.30% on average, and preventing up to 64.38% of the violations, with little additional time overhead.', 'abstract_zh': '端到端自动驾驶系统（ADS）的运行时韧性框架Argus：检测与应对驾驶风险以提升安全性与性能', 'title_zh': 'Argus: 面向端到端自动驾驶系统的韧性保障框架'}
{'arxiv_id': 'arXiv:2511.08922', 'title': 'Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning', 'authors': 'Yunchang Ma, Tenglong Liu, Yixing Lan, Xin Yin, Changxin Zhang, Xinglong Zhang, Xin Xu', 'link': 'https://arxiv.org/abs/2511.08922', 'abstract': "In offline reinforcement learning, value overestimation caused by out-of-distribution (OOD) actions significantly limits policy performance. Recently, diffusion models have been leveraged for their strong distribution-matching capabilities, enforcing conservatism through behavior policy constraints. However, existing methods often apply indiscriminate regularization to redundant actions in low-quality datasets, resulting in excessive conservatism and an imbalance between the expressiveness and efficiency of diffusion modeling. To address these issues, we propose DIffusion policies with Value-conditional Optimization (DIVO), a novel approach that leverages diffusion models to generate high-quality, broadly covered in-distribution state-action samples while facilitating efficient policy improvement. Specifically, DIVO introduces a binary-weighted mechanism that utilizes the advantage values of actions in the offline dataset to guide diffusion model training. This enables a more precise alignment with the dataset's distribution while selectively expanding the boundaries of high-advantage actions. During policy improvement, DIVO dynamically filters high-return-potential actions from the diffusion model, effectively guiding the learned policy toward better performance. This approach achieves a critical balance between conservatism and explorability in offline RL. We evaluate DIVO on the D4RL benchmark and compare it against state-of-the-art baselines. Empirical results demonstrate that DIVO achieves superior performance, delivering significant improvements in average returns across locomotion tasks and outperforming existing methods in the challenging AntMaze domain, where sparse rewards pose a major difficulty.", 'abstract_zh': '基于扩散模型的值条件优化策略（DIVO）', 'title_zh': '带有值条件优化的扩散策略在 offline 强化学习中的应用'}
{'arxiv_id': 'arXiv:2511.08752', 'title': 'Information-Driven Fault Detection and Identification for Multi-Agent Spacecraft Systems: Collaborative On-Orbit Inspection Mission', 'authors': 'Akshita Gupta, Arna Bhardwaj, Yashwanth Kumar Nakka, Changrak Choi, Amir Rahmani', 'link': 'https://arxiv.org/abs/2511.08752', 'abstract': 'This work presents a global-to-local, task-aware fault detection and identification (FDI) framework for multi-spacecraft systems conducting collaborative inspection missions in low Earth orbit. The inspection task is represented by a global information-driven cost functional that integrates the sensor model, spacecraft poses, and mission-level information-gain objectives. This formulation links guidance, control, and FDI by using the same cost function to drive both global task allocation and local sensing or motion decisions. Fault detection is achieved through comparisons between expected and observed task metrics, while higher-order cost-gradient measures enable the identification of faults among sensors, actuators, and state estimators. An adaptive thresholding mechanism captures the time-varying inspection geometry and dynamic mission conditions. Simulation results for representative multi-spacecraft inspection scenarios demonstrate the reliability of fault localization and classification under uncertainty, providing a unified, information-driven foundation for resilient autonomous inspection architectures.', 'abstract_zh': '面向低地球轨道多卫星协同检查任务的全局到局部、任务驱动故障检测与识别框架', 'title_zh': '基于信息驱动的多_agent航天器系统故障检测与识别：协同在轨检查任务'}
{'arxiv_id': 'arXiv:2511.08615', 'title': 'A Multi-Drone Multi-View Dataset and Deep Learning Framework for Pedestrian Detection and Tracking', 'authors': 'Kosta Dakic, Kanchana Thilakarathna, Rodrigo N. Calheiros, Teng Joon Lim', 'link': 'https://arxiv.org/abs/2511.08615', 'abstract': "Multi-drone surveillance systems offer enhanced coverage and robustness for pedestrian tracking, yet existing approaches struggle with dynamic camera positions and complex occlusions. This paper introduces MATRIX (Multi-Aerial TRacking In compleX environments), a comprehensive dataset featuring synchronized footage from eight drones with continuously changing positions, and a novel deep learning framework for multi-view detection and tracking. Unlike existing datasets that rely on static cameras or limited drone coverage, MATRIX provides a challenging scenario with 40 pedestrians and a significant architectural obstruction in an urban environment. Our framework addresses the unique challenges of dynamic drone-based surveillance through real-time camera calibration, feature-based image registration, and multi-view feature fusion in bird's-eye-view (BEV) representation. Experimental results demonstrate that while static camera methods maintain over 90\\% detection and tracking precision and accuracy metrics in a simplified MATRIX environment without an obstruction, 10 pedestrians and a much smaller observational area, their performance significantly degrades in the complex environment. Our proposed approach maintains robust performance with $\\sim$90\\% detection and tracking accuracy, as well as successfully tracks $\\sim$80\\% of trajectories under challenging conditions. Transfer learning experiments reveal strong generalization capabilities, with the pretrained model achieving much higher detection and tracking accuracy performance compared to training the model from scratch. Additionally, systematic camera dropout experiments reveal graceful performance degradation, demonstrating practical robustness for real-world deployments where camera failures may occur. The MATRIX dataset and framework provide essential benchmarks for advancing dynamic multi-view surveillance systems.", 'abstract_zh': '多无人机跟踪系统在行人监控中提供了增强的覆盖范围和鲁棒性，然而现有方法在处理动态相机位置和复杂遮挡方面存在困难。本文介绍了MATRIX（多无人机复杂环境跟踪系统），这是一个包含八个无人机同步 footage 并且不断改变位置的综合数据集，以及一种新型的多视图检测和跟踪深度学习框架。不同于依赖静态相机或有限无人机覆盖范围的现有数据集，MATRIX 提供了一个具有 40 个行人在城市环境中存在重大建筑遮挡的具有挑战性的场景。本框架通过实时相机校准、基于特征的图像配准和多视图特征在鸟瞰图（BEV）表示中的融合，解决了基于无人机的动态监控的独特挑战。实验结果表明，在未受遮挡的简化 MATRIX 环境中，静态相机方法在检测和跟踪精度方面仍保持超过 90% 的指标，但在复杂环境中，其性能显著下降。我们提出的方法在检测和跟踪精度方面保持了约 90% 的鲁棒性性能，并能够在具有挑战性条件下成功跟踪约 80% 的轨迹。迁移学习实验显示了强大的泛化能力，预训练模型在检测和跟踪准确性方面明显优于从头开始训练模型。此外，系统性摄像机掉线实验显示了平滑的性能下降，证明了在实际部署中可能出现摄像机故障情况下的实用鲁棒性。MATRIX 数据集和框架为推动动态多视图监控系统的发展提供了必要的基准。', 'title_zh': '基于多无人机多视角的行人检测与跟踪数据集及深度学习框架'}
{'arxiv_id': 'arXiv:2511.05812', 'title': 'Evader-Agnostic Team-Based Pursuit Strategies in Partially-Observable Environments', 'authors': 'Addison Kalanther, Daniel Bostwick, Chinmay Maheshwari, Shankar Sastry', 'link': 'https://arxiv.org/abs/2511.05812', 'abstract': 'We consider a scenario where a team of two unmanned aerial vehicles (UAVs) pursue an evader UAV within an urban environment. Each agent has a limited view of their environment where buildings can occlude their field-of-view. Additionally, the pursuer team is agnostic about the evader in terms of its initial and final location, and the behavior of the evader. Consequently, the team needs to gather information by searching the environment and then track it to eventually intercept. To solve this multi-player, partially-observable, pursuit-evasion game, we develop a two-phase neuro-symbolic algorithm centered around the principle of bounded rationality. First, we devise an offline approach using deep reinforcement learning to progressively train adversarial policies for the pursuer team against fictitious evaders. This creates $k$-levels of rationality for each agent in preparation for the online phase. Then, we employ an online classification algorithm to determine a "best guess" of our current opponent from the set of iteratively-trained strategic agents and apply the best player response. Using this schema, we improved average performance when facing a random evader in our environment.', 'abstract_zh': '我们考虑一个场景，其中两只无人驾驶航空器（UAV）在城市环境中追逐一个逃逸的UAV。每个代理都有一个受限的视野，建筑物会阻挡其视野。此外，追击团队对逃逸者的位置及其行为一无所知。因此，团队需要通过搜索环境来收集信息，然后追踪这些信息以最终实施拦截。为了解决这个多玩家、部分可观测的追逐博弈问题，我们开发了一个以有限理性为核心原则的两阶段神经符号算法。首先，我们使用深度强化学习设计一个离线方法，逐步训练追击团队与虚构的逃逸者之间的对抗策略，从而为每个代理预设了$k$级理性。然后，我们使用在线分类算法来确定当前对手的最佳猜测，并应用最佳对抗策略。利用这种方案，在面对环境中的随机逃逸者时，我们改进了平均性能。', 'title_zh': '部分可观测环境中无逃逸者假设的团队追捕策略'}
