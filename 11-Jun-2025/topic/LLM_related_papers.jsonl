{'arxiv_id': 'arXiv:2506.08524', 'title': 'Teaching Physical Awareness to LLMs through Sounds', 'authors': 'Weiguo Wang, Andy Nie, Wenrui Zhou, Yi Kai, Chengchen Hu', 'link': 'https://arxiv.org/abs/2506.08524', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities in text and multimodal processing, yet they fundamentally lack physical awareness--understanding of real-world physical phenomena. In this work, we present ACORN, a framework that teaches LLMs physical awareness through sound, focusing on fundamental physical phenomena like the Doppler effect, multipath effect, and spatial relationships. To overcome data scarcity, ACORN introduce a physics-based simulator combining real-world sound sources with controlled physical channels to generate diverse training data. Using this simulator, we build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an audio encoder that processes both magnitude and phase information. By connecting our audio encoder to state-of-the-art LLMs, we demonstrate reasonable results in both simulated and real-world tasks, such as line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival estimation, paving the way for enabling LLMs to understand physical world.', 'abstract_zh': '大型语言模型（LLMs）在文本和多模态处理方面表现出色，但本质上缺乏物理感知——对现实世界物理现象的理解。本文介绍了一种名为ACORN的框架，通过声音来提升LLMs的物理感知能力，重点关注如多普勒效应、多路径效应和空间关系等基本物理现象。为克服数据稀缺问题，ACORN引入了一种基于物理的模拟器，该模拟器结合了真实世界的声音源和可控的物理通道，生成多样化训练数据。利用该模拟器，我们构建了AQA-PHY综合音频问答数据集，并提出了一种处理幅度和相位信息的音频编码器。通过将我们的音频编码器连接到最先进的LLMs，我们在仿真和真实世界任务中展示了合理的结果，如视线检测、多普勒效应估计和到达方向估计，为使LLMs理解物理世界铺平了道路。', 'title_zh': '通过声音教授物理意识给大规模语言模型'}
{'arxiv_id': 'arXiv:2506.09038', 'title': 'AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions', 'authors': 'Polina Kirichenko, Mark Ibrahim, Kamalika Chaudhuri, Samuel J. Bell', 'link': 'https://arxiv.org/abs/2506.09038', 'abstract': "For Large Language Models (LLMs) to be reliably deployed in both everyday and high-stakes domains, knowing when not to answer is equally critical as answering correctly. Real-world user queries, which can be underspecified, ill-posed, or fundamentally unanswerable, require LLMs to reason about uncertainty and selectively abstain -- i.e., refuse to answer definitively. However, abstention remains understudied, without a systematic evaluation framework for modern LLMs. In this work, we introduce AbstentionBench, a large-scale benchmark for holistically evaluating abstention across 20 diverse datasets, including questions with unknown answers, underspecification, false premises, subjective interpretations, and outdated information. Evaluating 20 frontier LLMs reveals abstention is an unsolved problem, and one where scaling models is of little use. While recent reasoning LLMs have shown impressive results in complex problem solving, surprisingly, we find that reasoning fine-tuning degrades abstention (by $24\\%$ on average), even for math and science domains on which reasoning models are explicitly trained. We find that while a carefully crafted system prompt can boost abstention in practice, it does not resolve models' fundamental inability to reason about uncertainty. We release AbstentionBench to foster research into advancing LLM reliability.", 'abstract_zh': '大型语言模型（LLMs）在日常生活和高 stakes 领域可靠部署的关键在于正确回答与合理不回答同样重要。大规模基准 AbstentionBench：全面评估 20 个多样数据集中的合理不回答能力', 'title_zh': 'AbstentionBench: LLMs在无法回答的问题上的推理失败'}
{'arxiv_id': 'arXiv:2506.08872', 'title': 'Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task', 'authors': 'Nataliya Kosmyna, Eugene Hauptmann, Ye Tong Yuan, Jessica Situ, Xian-Hao Liao, Ashly Vivian Beresnitzky, Iris Braunstein, Pattie Maes', 'link': 'https://arxiv.org/abs/2506.08872', 'abstract': "This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.", 'abstract_zh': '本研究探讨了LLM辅助作文写作的神经和行为后果。参与者被分为三组：LLM组、搜索引擎组和脑内组（无需工具）。每组在相同条件下完成了三轮任务。在第四轮任务中，LLM用户被重新分配到脑内组（LLM-to-Brain），脑内用户被重新分配到LLM条件（Brain-to-LLM）。共有54名参与者参加了前三轮任务，其中18名完成了第四轮任务。我们使用脑电图（EEG）评估了作文写作过程中的认知负荷，并使用自然语言处理分析了作文，还通过人类教师和AI评委评分。各组的命名实体、n元组模式和主题本体显示了组内一致性。EEG结果显示显著的脑连接差异：脑内组用户表现出最强且最分布的网络；搜索引擎用户表现出中等程度的参与；而LLM用户表现出最弱的连接。外部工具使用与认知活动呈负相关。在第四轮任务中，LLM-to-Brain用户显示出了减少的alpha和beta连接性，表明参与度不足。Brain-to-LLM用户表现出较高的记忆力召回和枕叶-顶叶和前额叶区域的激活，类似于搜索引擎用户。自报的作文拥有度在LLM组最低，在脑内组最高。LLM用户还难以准确引用自己的作品。尽管LLM提供了即时便利，但我们的研究结果揭示了潜在的认知成本。四个月中，LLM用户在神经、语言和行为层面持续表现不佳。这些结果引发了对LLM依赖的长期教育影响的担忧，并强调了对AI在学习中角色进行更深入研究的必要性。', 'title_zh': 'ChatGPT对大脑的影响：使用AI助手进行论文写作任务时的认知债务累积'}
{'arxiv_id': 'arXiv:2506.08800', 'title': 'Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents', 'authors': 'Irene Testini, José Hernández-Orallo, Lorenzo Pacchiardi', 'link': 'https://arxiv.org/abs/2506.08800', 'abstract': 'Data science aims to extract insights from data to support decision-making processes. Recently, Large Language Models (LLMs) are increasingly used as assistants for data science, by suggesting ideas, techniques and small code snippets, or for the interpretation of results and reporting. Proper automation of some data-science activities is now promised by the rise of LLM agents, i.e., AI systems powered by an LLM equipped with additional affordances--such as code execution and knowledge bases--that can perform self-directed actions and interact with digital environments. In this paper, we survey the evaluation of LLM assistants and agents for data science. We find (1) a dominant focus on a small subset of goal-oriented activities, largely ignoring data management and exploratory activities; (2) a concentration on pure assistance or fully autonomous agents, without considering intermediate levels of human-AI collaboration; and (3) an emphasis on human substitution, therefore neglecting the possibility of higher levels of automation thanks to task transformation.', 'abstract_zh': '数据科学旨在从数据中提取洞见以支持决策过程。近年来，大型语言模型（LLMs）越来越多地被用作数据科学的助手，通过提供想法、技术及小段代码，或对结果进行解释和报告。LLM代理的兴起为某些数据科学活动的适当自动化提供了可能，即由配备额外功能（如代码执行和知识库）的LLM驱动的AI系统可以自行执行操作并与数字环境交互。在本文中，我们调研了LLM助手和代理在数据科学中的评价。我们发现（1）主要集中于少数目标导向活动，大大忽视了数据管理和探索性活动；（2）集中在纯粹的辅助或完全自主的代理，而不考虑人类-AI协作的中间水平；（3）强调人类替代，因此忽视了通过任务转换实现更高水平自动化的可能性。', 'title_zh': '测量数据科学自动化：AI助理和代理的评估工具综述'}
{'arxiv_id': 'arXiv:2506.08771', 'title': 'Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery', 'authors': 'Yuni Susanti, Michael Färber', 'link': 'https://arxiv.org/abs/2506.08771', 'abstract': 'Inferring causal relationships between variable pairs is crucial for understanding multivariate interactions in complex systems. Knowledge-based causal discovery -- which involves inferring causal relationships by reasoning over the metadata of variables (e.g., names or textual context) -- offers a compelling alternative to traditional methods that rely on observational data. However, existing methods using Large Language Models (LLMs) often produce unstable and inconsistent results, compromising their reliability for causal inference. To address this, we introduce a novel approach that integrates Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery. Our approach identifies informative metapath-based subgraphs within KGs and further refines the selection of these subgraphs using Learning-to-Rank-based models. The top-ranked subgraphs are then incorporated into zero-shot prompts, improving the effectiveness of LLMs in inferring the causal relationship. Extensive experiments on biomedical and open-domain datasets demonstrate that our method outperforms most baselines by up to 44.4 points in F1 scores, evaluated across diverse LLMs and KGs. Our code and datasets are available on GitHub: this https URL', 'abstract_zh': '基于知识图谱的大型语言模型驱动的因果发现新方法', 'title_zh': '因果路径：在知识图中寻找用于知识导向因果发现的 informative 子图路径'}
{'arxiv_id': 'arXiv:2506.08745', 'title': 'Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning', 'authors': 'Kongcheng Zhang, Qi Yao, Shunyu Liu, Yingjie Wang, Baisheng Lai, Jieping Ye, Mingli Song, Dacheng Tao', 'link': 'https://arxiv.org/abs/2506.08745', 'abstract': 'Recent advances of Reinforcement Learning (RL) have highlighted its potential in complex reasoning tasks, yet effective training often relies on external supervision, which limits the broader applicability. In this work, we propose a novel self-rewarding reinforcement learning framework to enhance Large Language Model (LLM) reasoning by leveraging the consistency of intermediate reasoning states across different reasoning trajectories. Our key insight is that correct responses often exhibit consistent trajectory patterns in terms of model likelihood: their intermediate reasoning states tend to converge toward their own final answers (high consistency) with minimal deviation toward other candidates (low volatility). Inspired by this observation, we introduce CoVo, an intrinsic reward mechanism that integrates Consistency and Volatility via a robust vector-space aggregation strategy, complemented by a curiosity bonus to promote diverse exploration. CoVo enables LLMs to perform RL in a self-rewarding manner, offering a scalable pathway for learning to reason without external supervision. Extensive experiments on diverse reasoning benchmarks show that CoVo achieves performance comparable to or even surpassing supervised RL. Our code is available at this https URL.', 'abstract_zh': 'Recent advances of Reinforcement Learning (RL) have highlighted its potential in complex reasoning tasks, yet effective training often relies on external supervision, which limits the broader applicability. In this work, we propose a novel self-rewarding reinforcement learning framework to enhance Large Language Model (LLM) reasoning by leveraging the consistency of intermediate reasoning states across different reasoning trajectories. Our key insight is that correct responses often exhibit consistent trajectory patterns in terms of model likelihood: their intermediate reasoning states tend to converge toward their own final answers (high consistency) with minimal deviation toward other candidates (low volatility). Inspired by this observation, we introduce CoVo, an intrinsic reward mechanism that integrates Consistency and Volatility via a robust vector-space aggregation strategy, complemented by a curiosity bonus to promote diverse exploration. CoVo enables LLMs to perform RL in a self-rewarding manner, offering a scalable pathway for learning to reason without external supervision. Extensive experiments on diverse reasoning benchmarks show that CoVo achieves performance comparable to or even surpassing supervised RL. Our code is available at this https URL. \n\n自监督强化学习框架CoVo：通过利用不同推理轨迹中间推理状态的一致性增强大型语言模型的推理能力', 'title_zh': '一致的路径通向真理：自我奖励强化学习在大模型推理解析中的应用'}
{'arxiv_id': 'arXiv:2506.08486', 'title': 'RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being', 'authors': 'Rahatara Ferdousi, M Anwar Hossain', 'link': 'https://arxiv.org/abs/2506.08486', 'abstract': 'The rise of large language models (LLMs) has created new possibilities for digital twins in healthcare. However, the deployment of such systems in consumer health contexts raises significant concerns related to hallucination, bias, lack of transparency, and ethical misuse. In response to recommendations from health authorities such as the World Health Organization (WHO), we propose Responsible Health Twin (RHealthTwin), a principled framework for building and governing AI-powered digital twins for well-being assistance. RHealthTwin processes multimodal inputs that guide a health-focused LLM to produce safe, relevant, and explainable responses. At the core of RHealthTwin is the Responsible Prompt Engine (RPE), which addresses the limitations of traditional LLM configuration. Conventionally, users input unstructured prompt and the system instruction to configure the LLM, which increases the risk of hallucination. In contrast, RPE extracts predefined slots dynamically to structure both inputs. This guides the language model to generate responses that are context aware, personalized, fair, reliable, and explainable for well-being assistance. The framework further adapts over time through a feedback loop that updates the prompt structure based on user satisfaction. We evaluate RHealthTwin across four consumer health domains including mental support, symptom triage, nutrition planning, and activity coaching. RPE achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and BERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical compliance and instruction-following metrics using LLM-as-judge evaluation, outperforming baseline strategies. We envision RHealthTwin as a forward-looking foundation for responsible LLM-based applications in health and well-being.', 'abstract_zh': '大语言模型的兴起为医疗领域的数字孪生带来了新机遇，但在消费者健康领域部署此类系统引发了关于幻觉、偏见、透明度不足和伦理滥用的重大关切。根据世界卫生组织等卫生当局的建议，我们提出了负责任健康数字孪生（RHealthTwin）原则框架，以构建和治理用于福祉辅助的AI驱动数字孪生。RHealthTwin处理多模态输入，引导专注于健康的大语言模型生成安全、相关且可解释的响应。RHealthTwin的核心是负责任提示引擎（RPE），它解决了传统大语言模型配置的局限性。传统上，用户输入未结构化的提示和系统指令来配置大语言模型，增加了幻觉的风险。相反，RPE 动态提取预定义的槽位来结构化输入。这引导语言模型生成有上下文意识、个性化、公平、可靠和可解释的响应，以辅助福祉。该框架还通过一个反馈循环进行适应，该循环根据用户满意度更新提示结构。我们在包括心理健康支持、症状分类、营养规划和活动指导在内的四个消费者健康领域评估了RHealthTwin。RPE在基准数据集上实现了最先进的结果，BLEU得分为0.41，ROUGE-L得分为0.63，BERTScore得分为0.89。此外，我们在LLM作为评判者的伦理合规性和指令遵循度指标中达到了超过90%的表现，超越了基线策略。我们展望RHealthTwin将成为医疗和福祉领域负责任的大语言模型应用的基础。', 'title_zh': 'RHealthTwin: 朝着负责任的多模态数字孪生以实现个性化福祉方向努力'}
{'arxiv_id': 'arXiv:2506.08446', 'title': 'A Survey on Large Language Models for Mathematical Reasoning', 'authors': 'Peng-Yuan Wang, Tian-Shuo Liu, Chenyang Wang, Yi-Di Wang, Shu Yan, Cheng-Xing Jia, Xu-Hui Liu, Xin-Wei Chen, Jia-Cheng Xu, Ziniu Li, Yang Yu', 'link': 'https://arxiv.org/abs/2506.08446', 'abstract': 'Mathematical reasoning has long represented one of the most fundamental and challenging frontiers in artificial intelligence research. In recent years, large language models (LLMs) have achieved significant advances in this area. This survey examines the development of mathematical reasoning abilities in LLMs through two high-level cognitive phases: comprehension, where models gain mathematical understanding via diverse pretraining strategies, and answer generation, which has progressed from direct prediction to step-by-step Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical reasoning, ranging from training-free prompting to fine-tuning approaches such as supervised fine-tuning and reinforcement learning, and discuss recent work on extended CoT and "test-time scaling". Despite notable progress, fundamental challenges remain in terms of capacity, efficiency, and generalization. To address these issues, we highlight promising research directions, including advanced pretraining and knowledge augmentation techniques, formal reasoning frameworks, and meta-generalization through principled learning paradigms. This survey tries to provide some insights for researchers interested in enhancing reasoning capabilities of LLMs and for those seeking to apply these techniques to other domains.', 'abstract_zh': '数学推理一直是人工智能研究中最基本也是最具挑战性的前沿领域之一。近年来，大型语言模型（LLMs）在这一领域取得了显著进展。本文综述了LLMs在两个高层次认知阶段中数学推理能力的发展：理解阶段，模型通过多样化的预训练策略获得数学理解；以及从直接预测到逐步链式思考（CoT）推理的答案生成阶段。我们回顾了从无训练提示到微调方法（如监督微调和强化学习）增强数学推理的方法，并讨论了扩展CoT和“测试时扩展”等相关工作。尽管取得了一定的进展，但在容量、效率和泛化方面仍然存在根本性的挑战。为解决这些问题，我们强调了有前景的研究方向，包括先进的预训练和知识增强技术、形式推理框架以及通过原则性的学习范式实现元泛化。本文旨在为致力于提高LLMs推理能力的研究者和希望将这些技术应用于其他领域的研究者提供一些见解。', 'title_zh': '大型语言模型在数学推理中的研究综述'}
{'arxiv_id': 'arXiv:2506.08422', 'title': 'Transforming Expert Knowledge into Scalable Ontology via Large Language Models', 'authors': 'Ikkei Itoku, David Theil, Evelyn Eichelsdoerfer Uehara, Sreyoshi Bhaduri, Junnosuke Kuroda, Toshi Yumoto, Alex Gil, Natalie Perez, Rajesh Cherukuri, Naumaan Nayyar', 'link': 'https://arxiv.org/abs/2506.08422', 'abstract': 'Having a unified, coherent taxonomy is essential for effective knowledge representation in domain-specific applications as diverse terminologies need to be mapped to underlying concepts. Traditional manual approaches to taxonomy alignment rely on expert review of concept pairs, but this becomes prohibitively expensive and time-consuming at scale, while subjective interpretations often lead to expert disagreements. Existing automated methods for taxonomy alignment have shown promise but face limitations in handling nuanced semantic relationships and maintaining consistency across different domains. These approaches often struggle with context-dependent concept mappings and lack transparent reasoning processes. We propose a novel framework that combines large language models (LLMs) with expert calibration and iterative prompt optimization to automate taxonomy alignment. Our method integrates expert-labeled examples, multi-stage prompt engineering, and human validation to guide LLMs in generating both taxonomy linkages and supporting rationales. In evaluating our framework on a domain-specific mapping task of concept essentiality, we achieved an F1-score of 0.97, substantially exceeding the human benchmark of 0.68. These results demonstrate the effectiveness of our approach in scaling taxonomy alignment while maintaining high-quality mappings and preserving expert oversight for ambiguous cases.', 'abstract_zh': '一种结合大规模语言模型、专家校准和迭代提示优化的税务分类自动化框架', 'title_zh': '通过大型语言模型将专家知识转化为可扩展本体'}
{'arxiv_id': 'arXiv:2506.08390', 'title': 'On Reasoning Strength Planning in Large Reasoning Models', 'authors': 'Leheng Sheng, An Zhang, Zijian Wu, Weixiang Zhao, Changshuo Shen, Yi Zhang, Xiang Wang, Tat-Seng Chua', 'link': 'https://arxiv.org/abs/2506.08390', 'abstract': "Recent studies empirically reveal that large reasoning models (LRMs) can automatically allocate more reasoning strengths (i.e., the number of reasoning tokens) for harder problems, exhibiting difficulty-awareness for better task performance. While this automatic reasoning strength allocation phenomenon has been widely observed, its underlying mechanism remains largely unexplored. To this end, we provide explanations for this phenomenon from the perspective of model activations. We find evidence that LRMs pre-plan the reasoning strengths in their activations even before generation, with this reasoning strength causally controlled by the magnitude of a pre-allocated directional vector. Specifically, we show that the number of reasoning tokens is predictable solely based on the question activations using linear probes, indicating that LRMs estimate the required reasoning strength in advance. We then uncover that LRMs encode this reasoning strength through a pre-allocated directional vector embedded in the activations of the model, where the vector's magnitude modulates the reasoning strength. Subtracting this vector can lead to reduced reasoning token number and performance, while adding this vector can lead to increased reasoning token number and even improved performance. We further reveal that this direction vector consistently yields positive reasoning length prediction, and it modifies the logits of end-of-reasoning token </think> to affect the reasoning length. Finally, we demonstrate two potential applications of our findings: overthinking behavior detection and enabling efficient reasoning on simple problems. Our work provides new insights into the internal mechanisms of reasoning in LRMs and offers practical tools for controlling their reasoning behaviors. Our code is available at this https URL.", 'abstract_zh': '最近的研究实证表明，大型推理模型（LRMs）能够自动为更难的问题分配更多的推理强度（即推理标记的数量），展现出对任务性能的难度感知能力。虽然这种自动推理强度分配的现象已被广泛观察到，但其背后的机制仍然 largely unexplored。为此，我们从模型激活的角度提供了对该现象的解释。我们发现证据表明，LRMs 在生成之前就已经在其激活中预先规划了推理强度，并且这种推理强度是由预分配的方向向量的大小因果控制的。具体来说，我们展示了仅基于问题激活使用线性探针即可预测推理标记的数量，表明LRMs 在生成之前会预先估计所需的推理强度。然后，我们揭示LRMs 通过嵌入在模型激活中的预分配方向向量来编码这种推理强度，其中向量的大小调节推理强度。减去这个向量会导致推理标记数量减少和性能下降，而增加这个向量会导致推理标记数量增加，甚至性能提升。我们进一步揭示这种方向向量会一致地产生积极的推理长度预测，并通过调整端止于推理标记</think>的logits来影响推理长度。最后，我们展示了我们发现的两种潜在应用：过度推理行为检测和在简单问题上实现高效推理。我们的工作为LRMs 的推理内部机制提供了新的见解，并提供了控制其推理行为的实用工具。我们已将代码发布在如下地址：this https URL。', 'title_zh': '在大型推理模型中的推理强度规划'}
{'arxiv_id': 'arXiv:2506.08332', 'title': 'ORFS-agent: Tool-Using Agents for Chip Design Optimization', 'authors': 'Amur Ghose, Andrew B. Kahng, Sayak Kundu, Zhiang Wang', 'link': 'https://arxiv.org/abs/2506.08332', 'abstract': 'Machine learning has been widely used to optimize complex engineering workflows across numerous domains. In the context of integrated circuit design, modern flows (e.g., going from a register-transfer level netlist to physical layouts) involve extensive configuration via thousands of parameters, and small changes to these parameters can have large downstream impacts on desired outcomes - namely design performance, power, and area. Recent advances in Large Language Models (LLMs) offer new opportunities for learning and reasoning within such high-dimensional optimization tasks. In this work, we introduce ORFS-agent, an LLM-based iterative optimization agent that automates parameter tuning in an open-source hardware design flow. ORFS-agent adaptively explores parameter configurations, demonstrating clear improvements over standard Bayesian optimization approaches in terms of resource efficiency and final design metrics. Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations. Moreover, by following natural language objectives to trade off certain metrics for others, ORFS-agent demonstrates a flexible and interpretable framework for multi-objective optimization. Crucially, RFS-agent is modular and model-agnostic, and can be plugged in to any frontier LLM without any further fine-tuning.', 'abstract_zh': '基于大规模语言模型的开放源码硬件设计流程中的迭代优化代理ORFS-agent', 'title_zh': 'ORFS-agent：用于芯片设计优化的工具使用智能体'}
{'arxiv_id': 'arXiv:2506.08321', 'title': 'LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs', 'authors': 'Manooshree Patel, Rayna Bhattacharyya, Thomas Lu, Arnav Mehta, Niels Voss, Narges Norouzi, Gireeja Ranade', 'link': 'https://arxiv.org/abs/2506.08321', 'abstract': 'We present LeanTutor, a Large Language Model (LLM)-based tutoring system for math proofs. LeanTutor interacts with the student in natural language, formally verifies student-written math proofs in Lean, generates correct next steps, and provides the appropriate instructional guidance. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. The first module faithfully autoformalizes student proofs into Lean and verifies proof accuracy via successful code compilation. If the proof has an error, the incorrect step is identified. The next-step generator module outputs a valid next Lean tactic for incorrect proofs via LLM-based candidate generation and proof search. The feedback generator module leverages Lean data to produce a pedagogically-motivated natural language hint for the student user. To evaluate our system, we introduce PeanoBench, a human-written dataset derived from the Natural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each natural language proof step is paired with the corresponding logically equivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of tactics in correct proofs and accurately identifies the incorrect step in 30% of incorrect proofs. In generating natural language hints for erroneous proofs, LeanTutor outperforms a simple baseline on accuracy and relevance metrics.', 'abstract_zh': '基于大型语言模型的数学证明辅导系统LeanTutor', 'title_zh': 'LeanTutor: 一个形式验证的数学证明AI导师'}
{'arxiv_id': 'arXiv:2506.08134', 'title': 'The AI Imperative: Scaling High-Quality Peer Review in Machine Learning', 'authors': 'Qiyao Wei, Samuel Holt, Jing Yang, Markus Wulfmeier, Mihaela van der Schaar', 'link': 'https://arxiv.org/abs/2506.08134', 'abstract': 'Peer review, the bedrock of scientific advancement in machine learning (ML), is strained by a crisis of scale. Exponential growth in manuscript submissions to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite capacity of qualified reviewers, leading to concerns about review quality, consistency, and reviewer fatigue. This position paper argues that AI-assisted peer review must become an urgent research and infrastructure priority. We advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language Models (LLMs) not as replacements for human judgment, but as sophisticated collaborators for authors, reviewers, and Area Chairs (ACs). We propose specific roles for AI in enhancing factual verification, guiding reviewer performance, assisting authors in quality improvement, and supporting ACs in decision-making. Crucially, we contend that the development of such systems hinges on access to more granular, structured, and ethically-sourced peer review process data. We outline a research agenda, including illustrative experiments, to develop and validate these AI assistants, and discuss significant technical and ethical challenges. We call upon the ML community to proactively build this AI-assisted future, ensuring the continued integrity and scalability of scientific validation, while maintaining high standards of peer review.', 'abstract_zh': '机器学习（ML）领域科学进步基石的同行评审面临规模危机：AI辅助同行评审亟待成为研究和基础设施的优先事项', 'title_zh': 'AI的必然性：扩展高质量同行评审在机器学习中的应用'}
{'arxiv_id': 'arXiv:2506.08119', 'title': 'SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents', 'authors': 'Subhrangshu Nandi, Arghya Datta, Nikhil Vichare, Indranil Bhattacharya, Huzefa Raja, Jing Xu, Shayan Ray, Giuseppe Carenini, Abhi Srivastava, Aaron Chan, Man Ho Woo, Amar Kandola, Brandon Theresa, Francesco Carbone', 'link': 'https://arxiv.org/abs/2506.08119', 'abstract': 'Large Language Models (LLMs) demonstrate impressive general-purpose reasoning and problem-solving abilities. However, they struggle with executing complex, long-horizon workflows that demand strict adherence to Standard Operating Procedures (SOPs), a critical requirement for real-world industrial automation. Despite this need, there is a lack of public benchmarks that reflect the complexity, structure, and domain-specific nuances of SOPs. To address this, we present three main contributions. First, we introduce a synthetic data generation framework to create realistic, industry-grade SOPs that rigorously test the planning, reasoning, and tool-use capabilities of LLM-based agents. Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases. Third, we evaluate two prominent agent architectures: Function-Calling and ReAct Agents, on SOP-Bench, observing average success rates of only 27% and 48%, respectively. Remarkably, when the tool registry is much larger than necessary, agents invoke incorrect tools nearly 100% of the time. These findings underscore a substantial gap between current agentic capabilities of LLMs and the demands of automating real-world SOPs. Performance varies significantly by task and domain, highlighting the need for domain-specific benchmarking and architectural choices before deployment. SOP-Bench is publicly available at this http URL. We also release the prompts underpinning the data generation framework to support new domain-specific SOP benchmarks. We invite the community to extend SOP-Bench with SOPs from their industrial domains.', 'abstract_zh': '大规模语言模型（LLMs）展示了 impressive 的通用推理和问题解决能力。然而，它们在执行需要严格遵循标准操作程序（SOPs）的复杂、长期工作流程时表现不佳，这是现实世界工业自动化的一个关键要求。尽管有这一需求，仍缺乏反映SOP复杂性、结构和领域特定细微差别的公开基准。为解决这一问题，我们提出了三项主要贡献。首先，我们介绍了一种合成数据生成框架，用于创建真实且符合工业标准的SOP，以严格测试基于语言模型的代理的规划、推理和工具使用能力。其次，使用此框架，我们开发了SOP-Bench基准测试，包括来自10个工业领域的超过1,800项任务，每项任务都包含API、工具接口和由人类验证的测试案例。第三，我们评估了两种主要的代理架构：函数调用和ReAct代理，在SOP-Bench上的平均成功率分别为27%和48%。显著的是，当工具注册表远超所需时，代理几乎每次都会调用错误的工具。这些发现突显了当前语言模型在代理能力与自动化现实世界SOP需求之间的巨大差距。不同任务和领域间性能差异显著，强调了在部署前需要进行领域特定基准测试和架构选择。SOP-Bench已公开发布，可在该网址访问。我们还发布了支撑数据生成框架的提示，以支持新的领域特定SOP基准测试。我们邀请社区使用其工业领域的SOP扩展SOP-Bench。', 'title_zh': 'SOP-Bench: 复杂工业标准操作程序用于评估大规模语言模型代理'}
{'arxiv_id': 'arXiv:2506.09046', 'title': 'Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation', 'authors': 'Xiaowen Ma, Chenyang Lin, Yao Zhang, Volker Tresp, Yunpu Ma', 'link': 'https://arxiv.org/abs/2506.09046', 'abstract': 'Leveraging multiple Large Language Models(LLMs) has proven effective for addressing complex, high-dimensional tasks, but current approaches often rely on static, manually engineered multi-agent configurations. To overcome these constraints, we present the Agentic Neural Network(ANN), a framework that conceptualizes multi-agent collaboration as a layered neural network architecture. In this design, each agent operates as a node, and each layer forms a cooperative "team" focused on a specific subtask. Agentic Neural Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing inspiration from neural network forward passes, tasks are dynamically decomposed into subtasks, and cooperative agent teams with suitable aggregation methods are constructed layer by layer. (2) Backward Phase-Mirroring backpropagation, we refine both global and local collaboration through iterative feedback, allowing agents to self-evolve their roles, prompts, and coordination. This neuro-symbolic approach enables ANN to create new or specialized agent teams post-training, delivering notable gains in accuracy and adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent baselines under the same configurations, showing consistent performance improvements. Our findings indicate that ANN provides a scalable, data-driven framework for multi-agent systems, combining the collaborative capabilities of LLMs with the efficiency and flexibility of neural network principles. We plan to open-source the entire framework.', 'abstract_zh': '利用多个大型语言模型（LLMs）已证明对处理复杂、高维任务有效，但当前方法往往依赖于静态的手工工程化多智能体配置。为克服这些限制，我们提出了智能神经网络（Agentic Neural Network，ANN）框架，该框架将多智能体合作概念化为分层神经网络架构。在此设计中，每个智能体作为节点运作，每一层形成一个专注于特定子任务的“合作团队”。ANN遵循两阶段优化策略：（1）前向阶段——借鉴神经网络前向传播的概念，任务被动态分解为子任务，并逐层构建合适的合作智能体团队及其聚合方法。（2）后向阶段——模拟反向传播，通过迭代反馈精炼全局和局部合作，使智能体能够自我进化其角色、提示和协作方式。这种神经符号方法使ANN能够在训练后创建新的或专门的智能体团队，实现显著的准确性和适应性提升。ANN在四个基准数据集中，即使在相同的配置下也超越了最先进的多智能体基线，展示了一致的性能改进。我们的研究结果表明，ANN为多智能体系统提供了一个可扩展的数据驱动框架，结合了LLMs的协作能力和神经网络原则的效率与灵活性。我们计划开源整个框架。', 'title_zh': '代理神经网络：通过文本反向传播的自演化多代理系统'}
{'arxiv_id': 'arXiv:2506.09034', 'title': 'FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed', 'authors': 'Sizhe Dang, Yangyang Guo, Yanjun Zhao, Haishan Ye, Xiaodong Zheng, Guang Dai, Ivor Tsang', 'link': 'https://arxiv.org/abs/2506.09034', 'abstract': "Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks: the backward pass of first-order optimizers like Adam increases memory usage to more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order (ZO) optimizers avoid this cost by estimating gradients only from forward passes, yet existing methods like MeZO usually require many more steps to converge. Can this trade-off between speed and memory in ZO be fundamentally improved? Normalized-SGD demonstrates strong empirical performance with greater memory efficiency than Adam. In light of this, we introduce FZOO, a Fast Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward passes needed for convergence by employing batched one-sided estimates that adapt step sizes based on the standard deviation of batch losses. It also accelerates per-batch computation through the use of Rademacher random vector perturbations coupled with CUDA's parallel processing. Extensive experiments on diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3, across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy and an 18 times reduction in forward passes compared to MeZO, achieving convergence speeds comparable to Adam. We also provide theoretical analysis proving FZOO's formal equivalence to a normalized-SGD update rule and its convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling even larger memory savings. Overall, our results make single-GPU, high-speed, full-parameter fine-tuning practical and point toward future work on memory-efficient pre-training.", 'abstract_zh': 'Fine-tuning 大型语言模型 (LLMs) 经常面临 GPU 内存瓶颈：像 Adam 这样的一阶优化器的后向传播会将内存使用量增加到推理水平的 10 倍以上（例如，OPT-30B 的情况为 633 GB）。零阶（ZO）优化器通过仅从前向传播中估计梯度来避免这种成本，但现有的方法如 MeZO 通常需要更多步骤才能收敛。ZO 的这种速度与内存之间的权衡能否从根本上得到改进？实证结果显示，规范化-SGD 在内存效率方面优于 Adam，且具有强大的实证表现。鉴于此，我们提出了一种名为 FZOO 的快速零阶优化器，其目标是在 Adam 水平的速度下运行。FZOO 通过利用批处理单向估计并根据批次损失的标准差自适应调整步长，减少了达到收敛所需的总前向传播次数。此外，FZOO 还通过结合 Rademacher 随机向量扰动和 CUDA 并行处理加速了批次内计算。我们在包括 RoBERTa-large、OPT（350M-66B）、Phi-2 和 Llama3 等多种模型以及 11 项任务上进行了广泛的实验，验证了 FZOO 的有效性。平均而言，FZOO 在准确性方面优于 MeZO 3%，需要少 3 倍的前向传播次数。对于 RoBERTa-large，与 MeZO 相比，FZOO 在准确性上平均提高了 5.6%，前向传播次数减少了 18 倍，并实现了与 Adam 相媲美的收敛速度。我们还提供了理论分析，证明了 FZOO 与规范化-SGD 更新规则的形式等价及其收敛保证。FZOO 能够平滑地集成到 PEFT 技术中，从而实现更大的内存节省。总体而言，我们的结果使得单 GPU 高速全参数微调成为可能，并指出了未来内存高效预训练的工作方向。', 'title_zh': 'FZOO: 快速零阶优化器，用于将大型语言模型微调至Adam级别速度'}
{'arxiv_id': 'arXiv:2506.09033', 'title': 'Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning', 'authors': 'Haozhen Zhang, Tao Feng, Jiaxuan You', 'link': 'https://arxiv.org/abs/2506.09033', 'abstract': 'The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \\textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost this http URL is available at this https URL.', 'abstract_zh': '基于强化学习的多大型语言模型路由框架Router-R1', 'title_zh': 'Router-R1: 通过强化学习教学大规模语言模型多轮路由和聚合'}
{'arxiv_id': 'arXiv:2506.08962', 'title': 'WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis', 'authors': 'Liangliang Chen, Huiru Xie, Jacqueline Rohde, Ying Zhang', 'link': 'https://arxiv.org/abs/2506.08962', 'abstract': "This research-to-practice work-in-progress (WIP) paper presents an AI-enabled smart tutor designed to provide homework assessment and feedback for students in an undergraduate circuit analysis course. We detail the tutor's design philosophy and core components, including open-ended question answering and homework feedback generation. The prompts are carefully crafted to optimize responses across different problems. The smart tutor was deployed on the Microsoft Azure platform and is currently in use in an undergraduate circuit analysis course at the School of Electrical and Computer Engineering in a large, public, research-intensive institution in the Southeastern United States. Beyond offering personalized instruction and feedback, the tutor collects student interaction data, which is summarized and shared with the course instructor. To evaluate its effectiveness, we collected student feedback, with 90.9% of responses indicating satisfaction with the tutor. Additionally, we analyze a subset of collected data on preliminary circuit analysis topics to assess tutor usage frequency for each problem and identify frequently asked questions. These insights help instructors gain real-time awareness of student difficulties, enabling more targeted classroom instruction. In future work, we will release a full analysis once the complete dataset is available after the Spring 2025 semester. We also explore the potential applications of this smart tutor across a broader range of engineering disciplines by developing improved prompts, diagram-recognition methods, and database management strategies, which remain ongoing areas of research.", 'abstract_zh': '这项研究至实践工作进展（WIP）论文介绍了一种基于人工智能的智能辅导系统，旨在为美国东南部一所大型公立研究密集型机构电气与计算机工程学院本科生电路分析课程的学生提供家庭作业评估和反馈。我们详细阐述了该辅导系统的设计理念和核心组件，包括开放性问题回答和家庭作业反馈生成。精心设计的提示旨在优化不同问题的回应。该智能辅导系统部署在Microsoft Azure平台上，并正在该学院的本科生电路分析课程中使用。除了提供个性化指导和反馈外，该辅导系统还会收集学生互动数据，并将其总结后与课程教师分享。为评估其有效性，我们收集了学生反馈，其中90.9%的回应表示对辅导系统的满意。此外，我们还分析了部分收集的数据，针对初步电路分析主题，评估每个问题的使用频率，并确定常见问题。这些见解帮助教师实时了解学生的困难，从而能够进行更有针对性的课堂指导。未来研究中，我们将发布完整的数据分析，前提是2025年春季学期结束后可获得完整数据集。我们也探索了该智能辅导系统在更广泛工程学科中的潜在应用，通过开发改进的提示、图像识别方法和数据库管理策略，以实现这一目标，这些研究目前仍在进行中。', 'title_zh': 'WIP: 增强型大型语言模型辅助智能导师在本科电路分析中的应用'}
{'arxiv_id': 'arXiv:2506.08952', 'title': "Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions", 'authors': 'Clara Lachenmaier, Judith Sieker, Sina Zarrieß', 'link': 'https://arxiv.org/abs/2506.08952', 'abstract': "Communication among humans relies on conversational grounding, allowing interlocutors to reach mutual understanding even when they do not have perfect knowledge and must resolve discrepancies in each other's beliefs. This paper investigates how large language models (LLMs) manage common ground in cases where they (don't) possess knowledge, focusing on facts in the political domain where the risk of misinformation and grounding failure is high. We examine the ability of LLMs to answer direct knowledge questions and loaded questions that presuppose misinformation. We evaluate whether loaded questions lead LLMs to engage in active grounding and correct false user beliefs, in connection to their level of knowledge and their political bias. Our findings highlight significant challenges in LLMs' ability to engage in grounding and reject false user beliefs, raising concerns about their role in mitigating misinformation in political discourse.", 'abstract_zh': '人类交流依赖会话接地，即使对话双方不具备完善的知识并需要解决彼此信念中的分歧，也能达成相互理解。本文探讨大语言模型（LLMs）在拥有（或不拥有）特定知识时如何管理共同知识，重点关注政治领域，该领域存在高风险的错误信息和接地失败。我们考察LLMs回答直接知识问题和预设错误信息的负荷问题的能力。我们评估负荷问题是否促使LLMs积极接地并纠正用户的错误信念，这与它们的知识水平和政治偏见有关。我们的研究结果突显了LLMs在参与接地和拒绝错误用户信念方面面临的重大挑战，这引发了对其在政治话语中遏制错误信息角色的担忧。', 'title_zh': 'Can LLMs 实现 grounding 时（不）知道：对直接和负载型政治问题的研究'}
{'arxiv_id': 'arXiv:2506.08935', 'title': 'Can A Gamer Train A Mathematical Reasoning Model?', 'authors': 'Andrew Shin', 'link': 'https://arxiv.org/abs/2506.08935', 'abstract': 'While large language models (LLMs) have achieved remarkable performance in various tasks including mathematical reasoning, their development typically demands prohibitive computational resources. Recent advancements have reduced costs for training capable models, yet even these approaches rely on high-end hardware clusters. In this paper, we demonstrate that a single average gaming GPU can train a solid mathematical reasoning model, by integrating reinforcement learning and memory optimization techniques. Specifically, we train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB memory that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger, in resource-constrained environments. Our results challenge the paradigm that state-of-the-art mathematical reasoning necessitates massive infrastructure, democratizing access to high-performance AI research. this https URL.', 'abstract_zh': '大型语言模型在数学推理等任务上取得了显著performance，但其开发通常需要难以承受的计算资源。近期进展虽降低了训练能力强模型的成本，但这些方法仍依赖高端硬件集群。在本文中，我们通过结合强化学习和内存优化技术，展示了单个普通游戏GPU可以在资源受限环境中训练出与更大模型表现相当或更好的数学推理模型。我们的结果挑战了顶尖数学推理需要庞大基础设施的范式，促进了高性能AI研究的平民化访问。这个 https://。', 'title_zh': '游戏者能否训练数学推理模型？'}
{'arxiv_id': 'arXiv:2506.08920', 'title': 'PropMEND: Hypernetworks for Knowledge Propagation in LLMs', 'authors': 'Zeyu Leo Liu, Greg Durrett, Eunsol Choi', 'link': 'https://arxiv.org/abs/2506.08920', 'abstract': 'Knowledge editing techniques for large language models (LLMs) can inject knowledge that is later reproducible verbatim, but they fall short on propagating that knowledge: models cannot answer questions that require reasoning with the injected knowledge. We present a hypernetwork-based approach for knowledge propagation, named PropMEND, where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate. Our approach extends the meta-objective of MEND [29] so that gradient updates on knowledge are transformed to enable answering multi-hop questions involving that knowledge. We show improved performance on the RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop questions whose answers are not explicitly stated in the injected fact. We further introduce a new dataset, Controlled RippleEdit, to evaluate the generalization of our hypernetwork, testing knowledge propagation along relations and entities unseen during hypernetwork training. PropMEND still outperforms existing approaches in unseen entity-relation pairs, yet the performance gap decreases substantially, suggesting future work in propagating knowledge to a wide range of relations.', 'abstract_zh': '基于超网络的知识传播技术可以将知识注入大型语言模型并在需要时原样重现，但它们在传播知识方面存在局限：模型无法回答需要利用注入知识进行推理的问题。我们提出了一种基于超网络的知识传播方法，名为PropMEND，通过元学习修改语言建模损失的梯度，以促进注入信息的传播。我们的方法扩展了MEND的元目标，使得梯度更新能够转换为支持回答涉及注入知识的多跳问题。我们在RippleEdit数据集上展示了改进的性能，对于答案并未明确陈述在注入事实中的挑战性多跳问题，正确率几乎提高了一倍。我们还引入了一个新的数据集，受控RippleEdit，以评估我们超网络的泛化能力，测试在未见关系和实体下知识的传播。尽管PropMEND在未见实体-关系对上的表现仍然优于现有方法，但性能差距显著减小，这提示未来在更广泛关系下传播知识的工作。', 'title_zh': 'PropMEND：用于大语言模型知识传播的超网络'}
{'arxiv_id': 'arXiv:2506.08902', 'title': 'Intention-Conditioned Flow Occupancy Models', 'authors': 'Chongyi Zheng, Seohong Park, Sergey Levine, Benjamin Eysenbach', 'link': 'https://arxiv.org/abs/2506.08902', 'abstract': 'Large-scale pre-training has fundamentally changed how machine learning research is done today: large foundation models are trained once, and then can be used by anyone in the community (including those without data or compute resources to train a model from scratch) to adapt and fine-tune to specific tasks. Applying this same framework to reinforcement learning (RL) is appealing because it offers compelling avenues for addressing core challenges in RL, including sample efficiency and robustness. However, there remains a fundamental challenge to pre-train large models in the context of RL: actions have long-term dependencies, so training a foundation model that reasons across time is important. Recent advances in generative AI have provided new tools for modeling highly complex distributions. In this paper, we build a probabilistic model to predict which states an agent will visit in the temporally distant future (i.e., an occupancy measure) using flow matching. As large datasets are often constructed by many distinct users performing distinct tasks, we include in our model a latent variable capturing the user intention. This intention increases the expressivity of our model, and enables adaptation with generalized policy improvement. We call our proposed method intention-conditioned flow occupancy models (InFOM). Comparing with alternative methods for pre-training, our experiments on $36$ state-based and $4$ image-based benchmark tasks demonstrate that the proposed method achieves $1.8 \\times$ median improvement in returns and increases success rates by $36\\%$. Website: this https URL Code: this https URL', 'abstract_zh': '大规模预训练从根本上改变了当今的机器学习研究方式：大型基础模型只需训练一次，然后可以通过社区中的任何人都可以使用（包括那些没有数据或计算资源从头训练模型的人），来适应和微调特定任务。将相同框架应用到强化学习中是诱人的，因为它提供了应对强化学习核心挑战（包括样本效率和稳健性）的有力途径。然而，在强化学习背景下预训练大型模型仍存在基本挑战：动作具有长期依赖性，因此训练跨越时间进行推理的基础模型是重要的。生成人工智能的 recent 进展提供了建模高度复杂分布的新工具。在本文中，我们建立了一个概率模型来预测智能体将在远距离将来访问哪些状态（即，占用度量），并使用流匹配。由于大型数据集通常由许多执行不同任务的用户构建，我们将用户意图作为一个潜在变量纳入我们的模型中。这种意图增强了模型的表达能力，并能够通过泛化策略改进实现适应性。我们提出的办法称为意图条件化流占用模型（InFOM）。与替代的预训练方法相比，我们在 36 个基于状态和 4 个基于图像的基准任务上的实验表明，所提出的方法在回报上实现了中位数 1.8 倍的改进，并将成功率提高了 36%。网址：这个 https URL 代码：这个 https URL', 'title_zh': '意图条件流占用模型'}
{'arxiv_id': 'arXiv:2506.08899', 'title': 'From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis', 'authors': 'Elias Horner, Cristinel Mateis, Guido Governatori, Agata Ciabattoni', 'link': 'https://arxiv.org/abs/2506.08899', 'abstract': 'We present a novel approach to the automated semantic analysis of legal texts using large language models (LLMs), targeting their transformation into formal representations in Defeasible Deontic Logic (DDL). We propose a structured pipeline that segments complex normative language into atomic snippets, extracts deontic rules, and evaluates them for syntactic and semantic coherence. Our methodology is evaluated across various LLM configurations, including prompt engineering strategies, fine-tuned models, and multi-stage pipelines, focusing on legal norms from the Australian Telecommunications Consumer Protections Code. Empirical results demonstrate promising alignment between machine-generated and expert-crafted formalizations, showing that LLMs - particularly when prompted effectively - can significantly contribute to scalable legal informatics.', 'abstract_zh': '我们提出了一种使用大语言模型（LLMs）自动进行法律文本语义分析的新型方法，目标是将其转换为 defeasible deontic logic (DDL) 的形式化表示。我们建议了一个结构化的流水线，将复杂的规范语言分割为原子片段，提取规范规则，并评估其在语法和语义上的连贯性。我们的方法在各种 LLM 配置下进行了评估，包括提示工程策略、微调模型和多阶段流水线，重点关注澳大利亚电信消费者保护代码中的法律规范。实证结果表明，机器生成的形式化与专家手工编写的形式化之间存在令人鼓舞的一致性，显示了当有效地提示时，LLMs 可以显著贡献于可扩展的法律信息化。', 'title_zh': '从法律文本到可逆规范逻辑：基于LLM的自动语义分析研究'}
{'arxiv_id': 'arXiv:2506.08897', 'title': 'PlantBert: An Open Source Language Model for Plant Science', 'authors': 'Hiba Khey, Amine Lakhder, Salma Rouichi, Imane El Ghabi, Kamal Hejjaoui, Younes En-nahli, Fahd Kalloubi, Moez Amri', 'link': 'https://arxiv.org/abs/2506.08897', 'abstract': 'The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantBert, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantBert is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantBert to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantBert exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields. By providing a scalable and reproducible framework for high-resolution entity recognition, PlantBert bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science.', 'abstract_zh': '基于变压器的语言模型的快速进步推动了生物医学和临床自然语言处理领域的突破；然而，植物科学领域仍严重缺乏此类适应性工具。在本工作中，我们提出PlantBert，这是一种高性能的开源语言模型，专门用于从植物胁迫响应文献中提取结构化知识。PlantBert基于DeBERTa架构构建，该架构以其分离的注意力和 robust 的上下文编码著称，并在详细筛选和专家标注的摘要语料库上进行微调，主要关注裂谷扁豆（Lens culinaris）对不同非生物和生物胁迫的响应。本方法结合了基于变压器的建模与规则增强的语义后处理以及本体导向的实体规范化，使PlantBert能够精确而忠实地捕捉生物学相关的关联关系。底层语料库使用与作物本体学对齐的层次结构进行标注，涵盖了植物适应性的分子、生理、生物化学和农业学维度。PlantBert表现出强大的实体类型泛化能力，展示了在低资源科学领域实现稳健领域适应的可能性。通过提供一种可扩展且可重复的框架，用于高分辨率实体识别，PlantBert弥合了农业自然语言处理的关键缺口，并为植物基因组学、表型学和农业知识发现提供了智能、数据驱动的系统。我们的模型公开发布，以促进透明度并加速跨学科在计算植物科学中的创新。', 'title_zh': 'PlantBert: 一种开源植物科学语言模型'}
{'arxiv_id': 'arXiv:2506.08827', 'title': 'The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation', 'authors': 'Francisco Vargas, Alejandro González Coene, Gaston Escalante, Exequiel Lobón, Manuel Pulido', 'link': 'https://arxiv.org/abs/2506.08827', 'abstract': 'The extraction of information about traffic accidents from legal documents is crucial for quantifying insurance company costs. Extracting entities such as percentages of physical and/or psychological disability and the involved compensation amounts is a challenging process, even for experts, due to the subtle arguments and reasoning in the court decision. A two-step procedure is proposed: first, segmenting the document identifying the most relevant segments, and then extracting the entities. For text segmentation, two methodologies are compared: a classic method based on regular expressions and a second approach that divides the document into blocks of n-tokens, which are then vectorized using multilingual models for semantic searches (text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models (LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to the selected segments for entity extraction. For the LLaMA models, fine-tuning is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a significant number of hallucinations in extractions which are an important contention point for named entity extraction. This work shows that these hallucinations are substantially reduced after finetuning the model. The performance of the methodology based on segment vectorization and subsequent use of LLMs significantly surpasses the classic method which achieves an accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning achieves the highest accuracy 79.4%, surpassing its base version 61.7%. Notably, the base LLaMA-3 8B model already performs comparably to the finetuned LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.', 'abstract_zh': '从法律文件中提取交通事故信息对于量化保险公司成本至关重要。一种两步程序被提出：首先，识别最相关的文档段落进行分段，然后从中提取实体。在文本分段方面，对比了两种方法：基于正则表达式的传统方法和将文档划分为n-克隆块并使用多语言模型进行语义搜索（text-embedding-ada-002/MiniLM-L12-v2）的方法。随后，使用提示技术应用大型语言模型（LLaMA-2 7b、70b、LLaMA-3 8b和GPT-4 Turbo）对选定段落进行实体提取。对于LLaMA模型，进行了LoRA微调。即使在零温度下，LLaMA-2 7b也表现出大量幻觉，这在命名实体提取中是一个重要的争议点。本研究显示，模型微调后这些幻觉显著减少。基于段落向量化的方法和随后使用大型语言模型的方法在准确性上显著超越了传统方法，传统方法的准确率为39.5%。在开源模型中，微调后的LLaMA-2 70B准确率最高，达到79.4%，超过了其基础版本61.7%。值得注意的是，基础的LLaMA-3 8B模型在准确率方面已经与微调后的LLaMA-2 70B模型相当，达到了76.6%，突显了模型开发的快速进展。与此同时，GPT-4 Turbo实现了最高的准确率86.1%。', 'title_zh': 'LLaMA微调对法律文档中命名实体提取幻觉的影响'}
{'arxiv_id': 'arXiv:2506.08753', 'title': 'Factors affecting the in-context learning abilities of LLMs for dialogue state tracking', 'authors': 'Pradyoth Hegde, Santosh Kesiraju, Jan Švec, Šimon Sedláček, Bolaji Yusuf, Oldřich Plchot, Deepak K T, Jan Černocký', 'link': 'https://arxiv.org/abs/2506.08753', 'abstract': 'This study explores the application of in-context learning (ICL) to the dialogue state tracking (DST) problem and investigates the factors that influence its effectiveness. We use a sentence embedding based k-nearest neighbour method to retrieve the suitable demonstrations for ICL. The selected demonstrations, along with the test samples, are structured within a template as input to the LLM. We then conduct a systematic study to analyse the impact of factors related to demonstration selection and prompt context on DST performance. This work is conducted using the MultiWoZ2.4 dataset and focuses primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and Llama3.2-3B-Instruct models. Our findings provide several useful insights on in-context learning abilities of LLMs for dialogue state tracking.', 'abstract_zh': '本研究探索了上下文学习（ICL）在对话状态跟踪（DST）问题中的应用，并调查了影响其有效性的因素。我们使用基于句子嵌入的K最近邻方法检索适合的示范以用于ICL。所选示范与测试样本一起按照模板格式作为输入提供给LLM。然后，我们进行系统研究以分析示范选择和提示上下文因子对DST性能的影响。本研究使用MultiWoZ2.4数据集，并着重于OLMo-7B-instruct、Mistral-7B-Instruct-v0.3和Llama3.2-3B-Instruct模型。我们的发现提供了关于LLMs在对话状态跟踪中上下文学习能力的一些有用见解。', 'title_zh': '影响LLM在上下文对话状态跟踪中学习能力的因素'}
{'arxiv_id': 'arXiv:2506.08727', 'title': 'Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs', 'authors': 'Samarth Sikand, Rohit Mehra, Priyavanshi Pathania, Nikhil Bamby, Vibhu Saujanya Sharma, Vikrant Kaulgud, Sanjay Podder, Adam P. Burden', 'link': 'https://arxiv.org/abs/2506.08727', 'abstract': 'While Generative AI stands to be one of the fastest adopted technologies ever, studies have made evident that the usage of Large Language Models (LLMs) puts significant burden on energy grids and our environment. It may prove a hindrance to the Sustainability goals of any organization. A crucial step in any Sustainability strategy is monitoring or estimating the energy consumption of various components. While there exist multiple tools for monitoring energy consumption, there is a dearth of tools/frameworks for estimating the consumption or carbon emissions. Current drawbacks of both monitoring and estimation tools include high input data points, intrusive nature, high error margin, etc. We posit that leveraging emerging LLM benchmarks and related data points can help overcome aforementioned challenges while balancing accuracy of the emission estimations. To that extent, we discuss the challenges of current approaches and present our evolving framework, R-ICE, which estimates prompt level inference carbon emissions by leveraging existing state-of-the-art(SOTA) benchmark. This direction provides a more practical and non-intrusive way to enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our promising validation results suggest that benchmark-based modelling holds great potential for inference emission estimation and warrants further exploration from the scientific community.', 'abstract_zh': '基于新兴大语言模型基准的大规模语言模型推理碳排放估算框架R-ICE', 'title_zh': '突破ICE束缚：探究推理碳排放与能耗估算基准对大语言模型的机遇与挑战'}
{'arxiv_id': 'arXiv:2506.08726', 'title': 'Improved LLM Agents for Financial Document Question Answering', 'authors': 'Nelvin Tan, Zian Seng, Liang Zhang, Yu-Ching Shih, Dong Yang, Amol Salunkhe', 'link': 'https://arxiv.org/abs/2506.08726', 'abstract': "Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agent's performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance.", 'abstract_zh': '大规模语言模型（LLMs）在多项自然语言处理任务中展现了卓越的能力。然而，LLMs 在处理包含表格和文本数据的金融文档的数值问答任务上仍然存在挑战。最近的研究表明，在有参考答案的情况下，批评代理（即自我纠错）对此任务是有效的。在这一框架的基础上，本文考察了在没有参考答案的情况下批评代理的有效性，并通过实验表明，在这种情况下，批评代理的性能下降。为此，我们提出了一种改进的批评代理，以及一种超越先前最佳方法（思考程序）的表现更优且更安全的计算器代理。此外，我们研究了这些代理之间的交互方式及其对性能的影响。', 'title_zh': '改进的LLM代理模型在金融文档问答中的应用'}
{'arxiv_id': 'arXiv:2506.08712', 'title': 'ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization', 'authors': 'Hee Suk Yoon, Eunseop Yoon, Mark A. Hasegawa-Johnson, Sungwoong Kim, Chang D. Yoo', 'link': 'https://arxiv.org/abs/2506.08712', 'abstract': "We introduce ConfPO, a method for preference learning in Large Language Models (LLMs) that identifies and optimizes preference-critical tokens based solely on the training policy's confidence, without requiring any auxiliary models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO), which uniformly adjust all token probabilities regardless of their relevance to preference, ConfPO focuses optimization on the most impactful tokens. This targeted approach improves alignment quality while mitigating overoptimization (i.e., reward hacking) by using the KL divergence budget more efficiently. In contrast to recent token-level methods that rely on credit-assignment models or AI annotators, raising concerns about scalability and reliability, ConfPO is simple, lightweight, and model-free. Experimental results on challenging alignment benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO consistently outperforms uniform DAAs across various LLMs, delivering better alignment with zero additional computational overhead.", 'abstract_zh': 'ConfPO：一种基于训练策略信心进行偏好学习的方法', 'title_zh': 'ConfPO：利用策略模型置信度进行大型语言模型偏好优化中的关键词选择'}
{'arxiv_id': 'arXiv:2506.08669', 'title': 'Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search', 'authors': 'Dongge Han, Menglin Xia, Daniel Madrigal Diaz, Samuel Kessler, Ankur Mallick, Xuchao Zhang, Mirian Del Carmen Hipolito Garcia, Jin Xu, Victor Rühle, Saravan Rajmohan', 'link': 'https://arxiv.org/abs/2506.08669', 'abstract': "Small language models (SLMs) offer promising and efficient alternatives to large language models (LLMs). However, SLMs' limited capacity restricts their reasoning capabilities and makes them sensitive to prompt variations. To address these challenges, we propose a novel framework that enhances SLM reasoning capabilities through LLM generated blueprints. The blueprints provide structured, high-level reasoning guides that help SLMs systematically tackle related problems. Furthermore, our framework integrates a prompt template search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our framework demonstrates improved SLM performance across various tasks, including math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves the reasoning capabilities of SLMs without increasing model size or requiring additional training, offering a lightweight and deployment-friendly solution for on-device or resource-constrained environments.", 'abstract_zh': '小型语言模型（SLMs）提供了大型语言模型（LLMs）的有效替代方案。然而，SLMs的有限容量限制了它们的推理能力，并使其对提示变化敏感。为解决这些挑战，我们提出了一种新型框架，通过LLM生成的蓝图来增强SLMs的推理能力。蓝图提供了结构化的高级推理指南，帮助SLMs系统地解决相关问题。此外，我们的框架集成了一种提示模板搜索机制，以减轻SLMs对提示变化的敏感性。我们的框架在包括数学（GSM8K）、编程（MBPP）和逻辑推理（BBH）等各项任务上展示了改进的SLM性能。我们的方法提高了SLMs的推理能力，而无需增加模型大小或额外训练，提供了一种适用于设备端或资源受限环境的轻量级且易于部署的解决方案。', 'title_zh': '使用蓝图和提示模板搜索增强小型语言模型的推理能力'}
{'arxiv_id': 'arXiv:2506.08647', 'title': 'Summarization for Generative Relation Extraction in the Microbiome Domain', 'authors': 'Oumaima El Khettari, Solen Quiniou, Samuel Chaffron', 'link': 'https://arxiv.org/abs/2506.08647', 'abstract': 'We explore a generative relation extraction (RE) pipeline tailored to the study of interactions in the intestinal microbiome, a complex and low-resource biomedical domain. Our method leverages summarization with large language models (LLMs) to refine context before extracting relations via instruction-tuned generation. Preliminary results on a dedicated corpus show that summarization improves generative RE performance by reducing noise and guiding the model. However, BERT-based RE approaches still outperform generative models. This ongoing work demonstrates the potential of generative methods to support the study of specialized domains in low-resources setting.', 'abstract_zh': '一种用于肠道微生物组交互研究的生成性关系提取管道探索：利用大规模语言模型总结以提升低资源生物医学领域的关系提取性能', 'title_zh': '微生物组领域生成性关系抽取的总结方法'}
{'arxiv_id': 'arXiv:2506.08646', 'title': 'TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning', 'authors': 'Mingyu Zheng, Zhifan Feng, Jia Wang, Lanrui Wang, Zheng Lin, Yang Hao, Weiping Wang', 'link': 'https://arxiv.org/abs/2506.08646', 'abstract': 'Despite the commendable progress of recent LLM-based data synthesis methods, they face two limitations in generating table instruction tuning data. First, they can not thoroughly explore the vast input space of table understanding tasks, leading to limited data diversity. Second, they ignore the weaknesses in table understanding ability of the target LLM and blindly pursue the increase of data quantity, resulting in suboptimal data efficiency. In this paper, we introduce a progressive and weakness-guided data synthesis framework tailored for table instruction tuning, named TableDreamer, to mitigate the above issues. Specifically, we first synthesize diverse tables and related instructions as seed data, and then perform an iterative exploration of the input space under the guidance of the newly identified weakness data, which eventually serve as the final training data for fine-tuning the target LLM. Extensive experiments on 10 tabular benchmarks demonstrate the effectiveness of the proposed framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62% (49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms state-of-the-art data synthesis baselines which use more training data. The code and data is available at this https URL', 'abstract_zh': '尽管基于LLM的数据合成方法在近期已经取得了显著的进步，但仍面临生成表格指令调优数据的两个局限性。首先，它们无法充分探索表格理解任务的广阔输入空间，导致数据多样性有限。其次，它们忽略了目标LLM在表格理解能力上的弱点，盲目追求数据量的增加，导致数据效率不足。本文提出了一种渐进式和弱点导向的数据合成框架TableDreamer，以解决上述问题。具体而言，我们首先合成多样化的表格及其相关指令作为种子数据，然后在已识别的新弱点数据引导下逐步探索输入空间，最终作为目标LLM微调的最终训练数据。在10个表格基准上的广泛实验表明，所提出框架的有效性，使用27K GPT-4o合成数据将Llama3.1-8B-instruct的平均准确率提升了11.62%（从49.07%提升到60.69%），并优于使用更多训练数据的最新数据合成基准。相关代码和数据可在以下链接获取。', 'title_zh': 'TableDreamer: 从头开始的渐进式和弱点导向的数据合成方法用于表格指令调优'}
{'arxiv_id': 'arXiv:2506.08572', 'title': 'The Geometries of Truth Are Orthogonal Across Tasks', 'authors': 'Waiss Azizian, Michael Kirchhof, Eugene Ndiaye, Louis Bethune, Michal Klein, Pierre Ablin, Marco Cuturi', 'link': 'https://arxiv.org/abs/2506.08572', 'abstract': 'Large Language Models (LLMs) have demonstrated impressive generalization capabilities across various tasks, but their claim to practical relevance is still mired by concerns on their reliability. Recent works have proposed examining the activations produced by an LLM at inference time to assess whether its answer to a question is correct. Some works claim that a "geometry of truth" can be learned from examples, in the sense that the activations that generate correct answers can be distinguished from those leading to mistakes with a linear classifier. In this work, we underline a limitation of these approaches: we observe that these "geometries of truth" are intrinsically task-dependent and fail to transfer across tasks. More precisely, we show that linear classifiers trained across distinct tasks share little similarity and, when trained with sparsity-enforcing regularizers, have almost disjoint supports. We show that more sophisticated approaches (e.g., using mixtures of probes and tasks) fail to overcome this limitation, likely because activation vectors commonly used to classify answers form clearly separated clusters when examined across tasks.', 'abstract_zh': '大型语言模型（LLMs）在各种任务上展现了惊人的泛化能力，但其实际相关性仍然受到可靠性的质疑。近期工作提出，在推理时检查LLM生成的激活可以帮助评估其答案的正确性。一些工作声称可以从示例中学到“真理的几何结构”，意味着生成正确答案的激活可以与导致错误的激活通过线性分类器区分开。在本工作中，我们指出了这些方法的一个局限性：我们观察到这些“真理的几何结构”本质上是任务依赖的，并且无法在任务之间转移。具体来说，我们展示了跨不同任务训练的线性分类器在很大程度上缺乏相似性，在施加稀疏约束规则的情况下，几乎没有任何重叠的支持集。我们证明了使用探测器和任务混合等更复杂的方法也无法克服这一局限性，很可能是因为用于分类答案的激活向量在跨任务检查时形成了清晰分离的簇。', 'title_zh': '真理的几何学在不同任务之间是正交的'}
{'arxiv_id': 'arXiv:2506.08552', 'title': 'Efficient Post-Training Refinement of Latent Reasoning in Large Language Models', 'authors': 'Xinyuan Wang, Dongjie Wang, Wangyang Ying, Haoyue Bai, Nanxu Gong, Sixun Dong, Kunpeng Liu, Yanjie Fu', 'link': 'https://arxiv.org/abs/2506.08552', 'abstract': "Reasoning is a key component of language understanding in Large Language Models. While Chain-of-Thought prompting enhances performance via explicit intermediate steps, it suffers from sufficient token overhead and a fixed reasoning trajectory, preventing step-wise refinement. Recent advances in latent reasoning address these limitations by refining internal reasoning processes directly in the model's latent space, without producing explicit outputs. However, a key challenge remains: how to effectively update reasoning embeddings during post-training to guide the model toward more accurate solutions. To overcome this challenge, we propose a lightweight post-training framework that refines latent reasoning trajectories using two novel strategies: 1) Contrastive reasoning feedback, which compares reasoning embeddings against strong and weak baselines to infer effective update directions via embedding enhancement; 2) Residual embedding refinement, which stabilizes updates by progressively integrating current and historical gradients, enabling fast yet controlled convergence. Extensive experiments and case studies are conducted on five reasoning benchmarks to demonstrate the effectiveness of the proposed framework. Notably, a 5\\% accuracy gain on MathQA without additional training.", 'abstract_zh': '大型语言模型中的语言理解关键在于推理。尽管基于推理链的提示通过显式中间步骤增强性能，但会产生足够的标记开销并具有固定的推理轨迹，限制了逐步细化的能力。最近在潜在线索推理方面的进步通过直接在模型的潜空间中细化内部推理过程来克服这些限制，无需生成显式输出。然而，仍存在一个关键挑战：如何在后训练期间有效更新推理嵌入以引导模型向更准确的解决方案发展。为克服这一挑战，我们提出了一种轻量级后训练框架，通过两种新颖策略细化潜在线索推理轨迹：1) 对比推理反馈，将推理嵌入与强弱基准进行比较，通过嵌入增强推断有效更新方向；2) 余嵌入细化，通过逐步整合当前和历史梯度来稳定更新，实现快速且可控的收敛。在五个推理基准上的广泛实验和案例研究证明了提出框架的有效性。特别地，在MathQA上未进行额外训练的情况下获得5%的准确率提升。', 'title_zh': '大型语言模型训练后潜藏推理能力的高效精炼方法'}
{'arxiv_id': 'arXiv:2506.08500', 'title': 'DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs', 'authors': 'Arie Cattan, Alon Jacovi, Ori Ram, Jonathan Herzig, Roee Aharoni, Sasha Goldshtein, Eran Ofek, Idan Szpektor, Avi Caciularu', 'link': 'https://arxiv.org/abs/2506.08500', 'abstract': 'Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. In this work, we first propose a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. We then introduce CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. We conduct extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains.', 'abstract_zh': '检索增强生成（RAG）中的知识冲突类型及应对策略：一个高质量的标准与实验', 'title_zh': '拖入冲突：检测和解决搜索增强的大语言模型中的矛盾来源'}
{'arxiv_id': 'arXiv:2506.08488', 'title': 'EtiCor++: Towards Understanding Etiquettical Bias in LLMs', 'authors': 'Ashutosh Dwivedi, Siddhant Shivdutt Singh, Ashutosh Modi', 'link': 'https://arxiv.org/abs/2506.08488', 'abstract': 'In recent years, researchers have started analyzing the cultural sensitivity of LLMs. In this respect, Etiquettes have been an active area of research. Etiquettes are region-specific and are an essential part of the culture of a region; hence, it is imperative to make LLMs sensitive to etiquettes. However, there needs to be more resources in evaluating LLMs for their understanding and bias with regard to etiquettes. In this resource paper, we introduce EtiCor++, a corpus of etiquettes worldwide. We introduce different tasks for evaluating LLMs for knowledge about etiquettes across various regions. Further, we introduce various metrics for measuring bias in LLMs. Extensive experimentation with LLMs shows inherent bias towards certain regions.', 'abstract_zh': '近年来，研究人员开始分析大语言模型的文化敏感性。在这方面，礼仪已成为一个活跃的研究领域。礼仪是地域性的，是特定地区文化的重要组成部分；因此，使大语言模型对礼仪敏感是必不可少的。然而，需要更多的资源来评估大语言模型对礼仪的理解和偏见。在这篇资源论文中，我们介绍了一种全球礼仪语料库EtiCor++。我们介绍了不同的任务，用于评估大语言模型在不同地区的礼仪知识。此外，我们介绍了衡量大语言模型偏见的各种指标。对大语言模型的广泛实验表明，它们对某些地区的偏见是固有的。', 'title_zh': 'EtiCor++: 向理解LLM中的礼仪偏差迈进'}
{'arxiv_id': 'arXiv:2506.08487', 'title': 'Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models', 'authors': 'Sumanth Manduru, Carlotta Domeniconi', 'link': 'https://arxiv.org/abs/2506.08487', 'abstract': 'The rapid adoption of Small Language Models (SLMs) for on-device and resource-constrained deployments has outpaced our understanding of their ethical risks. To the best of our knowledge, we present the first large-scale audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma 3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we analyze both utility and fairness across ambiguous and disambiguated contexts. This evaluation reveals three key insights. First, competence and fairness need not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while exhibiting minimal bias, showing that efficient and ethical NLP is attainable. Second, social bias varies significantly by architecture: Qwen 2.5 models may appear fair, but this often reflects vacuous neutrality, random guessing, or evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2 models exhibit stronger stereotypical bias, suggesting overconfidence rather than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but increases disability-related bias in Phi-4-Mini by over 7 percentage points. These insights provide practical guidance for the responsible deployment of SLMs in applications demanding fairness and efficiency, particularly benefiting small enterprises and resource-constrained environments.', 'abstract_zh': 'Small Language Models (SLMs) 的 rapid adoption for on-device and resource-constrained deployments has outpaced our understanding of their ethical risks. To the best of our knowledge, we present the first large-scale audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters—a overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma 3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we analyze both utility and fairness across ambiguous and disambiguated contexts. This evaluation reveals three key insights. First, competence and fairness need not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while exhibiting minimal bias, showing that efficient and ethical NLP is attainable. Second, social bias varies significantly by architecture: Qwen 2.5 models may appear fair, but this often reflects vacuous neutrality, random guessing, or evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2 models exhibit stronger stereotypical bias, suggesting overconfidence rather than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but increases disability-related bias in Phi-4-Mini by over 7 percentage points. These insights provide practical guidance for the responsible deployment of SLMs in applications demanding fairness and efficiency, particularly benefiting small enterprises and resource-constrained environments。', 'title_zh': '公平性不是沉默：揭示小型语言模型中的空洞中立性'}
{'arxiv_id': 'arXiv:2506.08403', 'title': 'TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration', 'authors': 'Weiya Li, Junjie Chen, Bei Li, Boyang Liu, Zichen Wen, Nuanqiao Shan, Xiaoqian Liu, Anping Liu, Huajie Liu, Youyan Wang, Wujiuge Yin, Hu Song, Bing Huang, Zhiyuan Xia, Jialiang Chen, Linfeng Zhang', 'link': 'https://arxiv.org/abs/2506.08403', 'abstract': 'Machine translation has long been a central task in natural language processing. With the rapid advancement of large language models (LLMs), there has been remarkable progress in translation quality. However, fully realizing the translation potential of LLMs remains an open challenge. Recent studies have explored multi-agent systems to decompose complex translation tasks into collaborative subtasks, showing initial promise in enhancing translation quality through agent cooperation and specialization. Nevertheless, existing multi-agent translation frameworks largely neglect foundational insights from cognitive translation studies. These insights emphasize how human translators employ different cognitive strategies, such as balancing literal and free translation, refining expressions based on context, and iteratively evaluating outputs. To address this limitation, we propose a cognitively informed multi-agent framework called TACTIC, which stands for T ranslation A gents with Cognitive- T heoretic Interactive Collaboration. The framework comprises six functionally distinct agents that mirror key cognitive processes observed in human translation behavior. These include agents for drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. By simulating an interactive and theory-grounded translation workflow, TACTIC effectively leverages the full capacity of LLMs for high-quality translation. Experimental results on diverse language pairs from the FLORES-200 and WMT24 benchmarks show that our method consistently achieves state-of-the-art performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at this https URL.', 'abstract_zh': '认知导向的多智能体翻译框架TACTIC', 'title_zh': 'TACTIC: 具有认知理论交互合作的翻译代理'}
{'arxiv_id': 'arXiv:2506.08388', 'title': 'Reinforcement Learning Teachers of Test Time Scaling', 'authors': 'Edoardo Cetin, Tianyu Zhao, Yujin Tang', 'link': 'https://arxiv.org/abs/2506.08388', 'abstract': 'Training reasoning language models (LMs) with reinforcement learning (RL) for one-hot correctness inherently relies on the LM being able to explore and solve its task with some chance at initialization. Furthermore, a key use case of reasoning LMs is to act as teachers for distilling new students and cold-starting future RL iterations rather than being deployed themselves. From these considerations, we introduce a new framework that avoids RL\'s exploration challenge by training a new class of Reinforcement-Learned Teachers (RLTs) focused on yielding the most effective downstream distillation. RLTs are prompted with both the question and solution to each problem, and tasked to simply "connect-the-dots" with detailed explanations tailored for their students. We train RLTs with dense rewards obtained by feeding each explanation to the student and testing its understanding of the problem\'s solution. In practice, the raw outputs of a 7B RLT provide higher final performance on competition and graduate-level tasks than existing distillation and cold-starting pipelines that collect and postprocess the reasoning traces of orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness when training larger students and when applied zero-shot to out-of-distribution tasks, unlocking new levels of efficiency and re-usability for the RL reasoning framework.', 'abstract_zh': '使用强化学习训练推理语言模型以实现一对一正确性内在依赖于模型能够在初始化时探索并解决其任务。此外，推理语言模型的一个关键应用是作为老师的角色，用于指导新的学生并初始化未来的强化学习迭代，而不是直接部署。基于这些考虑，我们提出了一个新的框架，通过训练一种新的强化学习教师（RLTs），避免了强化学习的探索挑战，专注于产生最有效的下游蒸馏效果。RLTs 接受每个问题及其解，并被指派简单地“连点成线”，并为学生提供详细的解释。我们通过将每种解释输入学生并测试其对问题解的理解来用密集奖励训练RLTs。实践中，7B RLT的原始输出在比赛和研究生水平的任务上提供了比现有蒸馏和冷启动流水线更高的最终性能，这些流水线收集并处理了数量级更大的语言模型的推理踪迹。此外，RLTs 在训练更大规模的学生时保持其有效性，并在零样本情况下应用于分布外任务，为RL推理框架解锁了新的效率和可重用性。', 'title_zh': '测试时间缩放的强化学习教师'}
{'arxiv_id': 'arXiv:2506.08379', 'title': 'Reinforce LLM Reasoning through Multi-Agent Reflection', 'authors': 'Yurun Yuan, Tengyang Xie', 'link': 'https://arxiv.org/abs/2506.08379', 'abstract': 'Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm stands out for enabling dynamic solution exploration and feedback incorporation. However, existing approaches often suffer from restricted feedback spaces and lack of coordinated training of different parties, leading to suboptimal performance. To address this, we model this multi-turn refinement process as a Markov Decision Process and introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data. Theoretically, DPSDP can match the performance of any policy within the training distribution. Empirically, we instantiate DPSDP with various base models and show improvements on both in- and out-of-distribution benchmarks. For example, on benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An ablation study further confirms the benefits of multi-agent collaboration and out-of-distribution generalization.', 'abstract_zh': '利用更多推理时间的计算已被证明是提升大语言模型推理能力的有效方式。在各种方法中，验证与改进范式因其能够实现动态解决方案探索和反馈整合而脱颖而出。然而，现有方法往往受限于有限的反馈空间并缺乏不同参与方的协调训练，导致性能不佳。为解决这一问题，我们将这一多轮完善过程建模为马尔科夫决策过程，并引入DPSDP（基于动态规划的直接策略搜索）——一种强化学习算法，通过自生成数据上的直接偏好学习训练演员-评论家大语言模型系统以迭代改进答案。理论上，DPSDP可以匹配训练分布内的任何策略性能。实验中，我们使用多种基础模型实例化DPSDP，并在分布内外基准测试中显示出改进。例如，在MATH 500基准上，使用Minstral为基础模型的五轮完善步骤的多数投票将第一轮准确率从58.2%提高到63.2%。去机制化研究进一步证实了多Agent合作和分布外泛化的益处。', 'title_zh': '通过多智能体反思增强LLM推理'}
{'arxiv_id': 'arXiv:2506.08373', 'title': 'Draft-based Approximate Inference for LLMs', 'authors': 'Kevin Galim, Ethan Ewer, Wonjun Kang, Minjae Lee, Hyung Il Koo, Kangwook Lee', 'link': 'https://arxiv.org/abs/2506.08373', 'abstract': "Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at this https URL.", 'abstract_zh': '优化具有长上下文的大型语言模型（LLMs）推理 increasingly important due to the quadratic compute and linear memory complexity of Transformers. 提出一种新颖的近似 LLMS 推理框架，利用草稿模型更准确地预测令牌和键值对的重要性。具体而言，我们介绍了我们提出框架的两种实例：(i) SpecKV，利用草稿输出更有效地评估每个键值对的重要性以进行草稿缓存丢弃，(ii) SpecPC，利用草稿模型的注意力激活来识别并丢弃不重要的提示令牌。据我们所知，这是首次使用草稿模型加速近似 LLMS 推理的工作，扩展了其在传统无损推测解码之外的用途。我们通过理论和实证分析来阐述我们的方法，并展示了草稿模型和目标模型的注意力模式之间存在强烈的相关性。在长上下文基准测试中的广泛实验表明，我们的方法始终比现有基线具有更高的准确性，同时保持相同的内存使用、延迟和吞吐量的改进。代码可在以下链接获取。', 'title_zh': '基于草图的近似推理for大语言模型'}
{'arxiv_id': 'arXiv:2506.08349', 'title': 'Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving', 'authors': 'Yuxuan Zhou, Xien Liu, Chenwei Yan, Chen Ning, Xiao Zhang, Boxun Li, Xiangling Fu, Shijin Wang, Guoping Hu, Yu Wang, Ji Wu', 'link': 'https://arxiv.org/abs/2506.08349', 'abstract': "Large language models (LLMs) have demonstrated remarkable performance on various medical benchmarks, but their capabilities across different cognitive levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a multi-cognitive-level evaluation framework for assessing LLMs in the medical domain in this study. The framework integrates existing medical datasets and introduces tasks targeting three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving. Using this framework, we systematically evaluate state-of-the-art general and medical LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek. Our findings reveal a significant performance decline as cognitive complexity increases across evaluated models, with model size playing a more critical role in performance at higher cognitive levels. Our study highlights the need to enhance LLMs' medical capabilities at higher cognitive levels and provides insights for developing LLMs suited to real-world medical applications.", 'abstract_zh': '大型语言模型（LLMs）在各种医学基准测试中展现了卓越的表现，但它们在不同认知层次的能力仍待深入探索。受布卢姆分类法的启发，本文提出了一种多认知层次评估框架，用于评估LLMs在医学领域的表现。该框架整合了现有医学数据集，并引入了针对三个认知层次的任务：初步知识掌握、全面知识应用以及场景基于的问题解决。利用此框架，我们系统性地评估了六大家族顶尖的通用和医学LLMs：Llama、Qwen、Gemma、Phi、GPT和DeepSeek。研究发现，在评估模型中，随着认知复杂性的增加，其性能显著下降，模型大小在较高认知层次上的性能表现更为关键。本研究突出了提升LLMs在较高认知层次上的医学能力的需求，并为开发适用于真实世界医学应用的LLMs提供了见解。', 'title_zh': '多认知层次下大语言模型的评估：从医学知识掌握到情境基于的问题解决'}
{'arxiv_id': 'arXiv:2506.08346', 'title': 'SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models', 'authors': 'Wenhan Yao, Fen Xiao, Xiarun Chen, Jia Liu, YongQiang He, Weiping Wen', 'link': 'https://arxiv.org/abs/2506.08346', 'abstract': 'Deep speech classification tasks, including keyword spotting and speaker verification, are vital in speech-based human-computer interaction. Recently, the security of these technologies has been revealed to be susceptible to backdoor attacks. Specifically, attackers use noisy disruption triggers and speech element triggers to produce poisoned speech samples that train models to become vulnerable. However, these methods typically create only a limited number of backdoors due to the inherent constraints of the trigger function. In this paper, we propose that speech backdoor attacks can strategically focus on speech elements such as timbre and emotion, leveraging the Speech Large Language Model (SLLM) to generate diverse triggers. Increasing the number of triggers may disproportionately elevate the poisoning rate, resulting in higher attack costs and a lower success rate per trigger. We introduce the Multiple Gradient Descent Algorithm (MGDA) as a mitigation strategy to address this challenge. The proposed attack is called the Speech Prompt Backdoor Attack (SPBA). Building on this foundation, we conducted attack experiments on two speech classification tasks, demonstrating that SPBA shows significant trigger effectiveness and achieves exceptional performance in attack metrics.', 'abstract_zh': '基于语音的后门攻击：利用语音大语言模型的战略性触发要素定位（Speech Prompt Backdoor Attack: Leveraging Speech Large Language Model for Strategic Trigger Element Focusing）', 'title_zh': 'SPBA: 利用语音大规模语言模型对语音分类模型进行后门攻击'}
{'arxiv_id': 'arXiv:2506.08336', 'title': 'Your Agent Can Defend Itself against Backdoor Attacks', 'authors': 'Li Changjiang, Liang Jiacheng, Cao Bochuan, Chen Jinghui, Wang Ting', 'link': 'https://arxiv.org/abs/2506.08336', 'abstract': "Despite their growing adoption across domains, large language model (LLM)-powered agents face significant security risks from backdoor attacks during training and fine-tuning. These compromised agents can subsequently be manipulated to execute malicious operations when presented with specific triggers in their inputs or environments. To address this pressing risk, we present ReAgent, a novel defense against a range of backdoor attacks on LLM-based agents. Intuitively, backdoor attacks often result in inconsistencies among the user's instruction, the agent's planning, and its execution. Drawing on this insight, ReAgent employs a two-level approach to detect potential backdoors. At the execution level, ReAgent verifies consistency between the agent's thoughts and actions; at the planning level, ReAgent leverages the agent's capability to reconstruct the instruction based on its thought trajectory, checking for consistency between the reconstructed instruction and the user's instruction. Extensive evaluation demonstrates ReAgent's effectiveness against various backdoor attacks across tasks. For instance, ReAgent reduces the attack success rate by up to 90\\% in database operation tasks, outperforming existing defenses by large margins. This work reveals the potential of utilizing compromised agents themselves to mitigate backdoor risks.", 'abstract_zh': '尽管大型语言模型（LLM）驱动的代理在各个领域中的应用不断增加，但在训练和微调过程中，这些代理面临严重的后门攻击安全风险。这些受损的代理可能在接收到特定触发器时被操纵以执行恶意操作。为了应对这一紧迫的风险，我们提出了ReAgent，这是一种针对基于LLM代理的多种后门攻击的新颖防御措施。直观地讲，后门攻击常常导致用户指令、代理规划和执行之间的不一致性。基于这一洞察，ReAgent采用两级方法来检测潜在后门。在执行层面，ReAgent验证代理的想法和行动之间的一致性；在规划层面，ReAgent利用代理根据其思维轨迹重建指令的能力，检查重建指令与用户指令之间的一致性。广泛的评估证明了ReAgent在各种任务中对抗后门攻击的有效性。例如，在数据库操作任务中，ReAgent将攻击成功率降低多达90%，远超现有防御措施的效果。这项工作揭示了可以利用受损代理本身来减轻后门风险的潜力。', 'title_zh': '你的代理可以防御后门攻击'}
{'arxiv_id': 'arXiv:2506.08320', 'title': 'How Good LLM-Generated Password Policies Are?', 'authors': 'Vivek Vaidya, Aditya Patwardhan, Ashish Kundu', 'link': 'https://arxiv.org/abs/2506.08320', 'abstract': 'Generative AI technologies, particularly Large Language Models (LLMs), are rapidly being adopted across industry, academia, and government sectors, owing to their remarkable capabilities in natural language processing. However, despite their strengths, the inconsistency and unpredictability of LLM outputs present substantial challenges, especially in security-critical domains such as access control. One critical issue that emerges prominently is the consistency of LLM-generated responses, which is paramount for ensuring secure and reliable operations.\nIn this paper, we study the application of LLMs within the context of Cybersecurity Access Control Systems. Specifically, we investigate the consistency and accuracy of LLM-generated password policies, translating natural language prompts into executable this http URL configuration files. Our experimental methodology adopts two distinct approaches: firstly, we utilize pre-trained LLMs to generate configuration files purely from natural language prompts without additional guidance. Secondly, we provide these models with official this http URL documentation to serve as an informative baseline. We systematically assess the soundness, accuracy, and consistency of these AI-generated configurations. Our findings underscore significant challenges in the current generation of LLMs and contribute valuable insights into refining the deployment of LLMs in Access Control Systems.', 'abstract_zh': '_generative AI技术，特别是大型语言模型（LLMs），正迅速被工业、学术界和政府机构采用，这得益于它们在自然语言处理方面的卓越能力。然而，尽管LLMs具有优势，其输出的一致性和不可预测性给诸如访问控制等安全关键领域带来了重大挑战。一致性的缺乏，尤其是LLMs生成的回答的一致性问题，对于确保安全可靠的运行至关重要。\n\n在本文中，我们研究了大型语言模型在网络安全访问控制系统中的应用。具体而言，我们探讨了LLMs生成的密码策略的一致性和准确性，将自然语言提示转换为可执行的配置文件。我们实验方法采用了两种不同的方法：首先，我们利用预训练的LLMs仅从自然语言提示自动生成配置文件，不提供额外指导。其次，我们为这些模型提供官方文档作为参考基准。我们系统地评估了这些AI生成的配置文件的正确性、一致性和适用性。我们的研究结果揭示了当前LLMs生成的配置文件中存在的重要挑战，并为优化访问控制系统中LLMs的部署提供了有价值的见解。_', 'title_zh': 'LLM生成的密码策略质量如何？'}
{'arxiv_id': 'arXiv:2506.08311', 'title': 'Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study', 'authors': 'Ira Ceka, Saurabh Pujar, Shyam Ramji, Luca Buratti, Gail Kaiser, Baishakhi Ray', 'link': 'https://arxiv.org/abs/2506.08311', 'abstract': 'With the advent of large language models (LLMs), software engineering agents (SWE agents) have emerged as a powerful paradigm for automating a range of software tasks -- from code generation and repair to test case synthesis. These agents operate autonomously by interpreting user input and responding to environmental feedback. While various agent architectures have demonstrated strong empirical performance, the internal decision-making worfklows that drive their behavior remain poorly understood. Deeper insight into these workflows hold promise for improving both agent reliability and efficiency. In this work, we present the first systematic study of SWE agent behavior through the lens of execution traces. Our contributions are as follows: (1) we propose the first taxonomy of decision-making pathways across five representative agents; (2) using this taxonomy, we identify three core components essential to agent success -- bug localization, patch generation, and reproduction test generation -- and study each in depth; (3) we study the impact of test generation on successful patch production; and analyze strategies that can lead to successful test generation; (4) we further conduct the first large-scale code clone analysis comparing agent-generated and developer-written patches and provide a qualitative study revealing structural and stylistic differences in patch content. Together, these findings offer novel insights into agent design and open avenues for building agents that are both more effective and more aligned with human development practices.', 'abstract_zh': '随着大型语言模型（LLMs）的出现，软件工程代理（SWE代理）已成为自动化一系列软件任务的强大范式——从代码生成和修复到测试案例合成。这些代理通过解释用户输入并根据环境反馈自主运行。尽管各种代理架构在实证性能上表现出色，但驱动其行为的内部决策工作流程仍不清楚。对这些工作流程的更深入理解有望提高代理的可靠性和效率。在本研究中，我们通过执行轨迹的视角首次系统研究了SWE代理的行为。我们的贡献如下：（1）我们首次提出了一种涵盖五种代表性代理的决策路径分类；（2）利用这种分类，我们确定了三个关键组件是代理成功的关键——错误定位、补丁生成和再生测试生成，并深入研究了每个组件；（3）我们研究了测试生成对成功补丁生成的影响，并分析了可能导致成功测试生成的策略；（4）我们进行了首次大规模代码克隆分析，比较了代理生成的补丁和开发者编写的补丁，并提供了定性的研究结果，揭示了补丁内容在结构和风格上的差异。这些发现提供了关于代理设计的新见解，并为构建更有效且更符合人类开发实践的代理开辟了途径。', 'title_zh': '通过追溯性视角理解软件工程代理：一项实证研究'}
{'arxiv_id': 'arXiv:2506.08266', 'title': 'Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints', 'authors': 'Yaswanth Chittepu, Blossom Metevier, Will Schwarzer, Austin Hoag, Scott Niekum, Philip S. Thomas', 'link': 'https://arxiv.org/abs/2506.08266', 'abstract': 'Existing approaches to language model alignment often treat safety as a tradeoff against helpfulness, which can lead to unacceptable responses in sensitive domains. To ensure reliable performance in such settings, we propose High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a method that provides high-confidence safety guarantees while maximizing helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human preferences into helpfulness and harmlessness (safety), which are learned by training a reward model and a cost model, respectively. It then employs a two-step process to find safe solutions. In the first step, it optimizes the reward function under an intentionally pessimistic version of the cost constraint. In the second step, the trained model undergoes a safety test to verify whether its performance stays within an upper-confidence bound of the actual cost constraint. We provide a theoretical analysis of HC-RLHF, including proof that it will not return an unsafe solution with a probability greater than a user-specified threshold. For our empirical analysis, we apply HC-RLHF to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF produces safe models with high probability and can improve harmlessness and helpfulness compared to previous methods.', 'abstract_zh': '基于人类反馈的高置信度安全强化学习（HC-RLHF）：在保证安全性的同时最大化帮助性', 'title_zh': '基于高置信度安全约束的人工反馈强化学习'}
{'arxiv_id': 'arXiv:2506.08235', 'title': 'Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning', 'authors': 'Shashidhar Reddy Javaji, Yupeng Cao, Haohang Li, Yangyang Yu, Nikhil Muralidhar, Zining Zhu', 'link': 'https://arxiv.org/abs/2506.08235', 'abstract': "Large language models (LLMs) are increasingly being used for complex research tasks such as literature review, idea generation, and scientific paper analysis, yet their ability to truly understand and process the intricate relationships within complex research papers, such as the logical links between claims and supporting evidence remains largely unexplored. In this study, we present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs' capabilities in scientific claim-evidence extraction and validation, a task that reflects deeper comprehension of scientific argumentation. We systematically compare three approaches which are inspired by divide and conquer approaches, across six diverse LLMs, highlighting model-specific strengths and weaknesses in scientific comprehension. Through evaluation involving over 300 claim-evidence pairs across multiple research domains, we reveal significant limitations in LLMs' ability to process complex scientific content. Our results demonstrate that closed-source models like GPT-4 and Claude consistently outperform open-source counterparts in precision and recall across claim-evidence identification tasks. Furthermore, strategically designed three-pass and one-by-one prompting approaches significantly improve LLMs' abilities to accurately link dispersed evidence with claims, although this comes at increased computational cost. CLAIM-BENCH sets a new standard for evaluating scientific comprehension in LLMs, offering both a diagnostic tool and a path forward for building systems capable of deeper, more reliable reasoning across full-length papers.", 'abstract_zh': '大规模语言模型（LLMs）在文学综述、想法生成和科学论文分析等复杂研究任务中的应用越来越广泛，但它们在真正理解并处理复杂研究论文中的 intricate 关系，如论点与其支持证据之间的逻辑联系方面的能力尚未被充分探索。在本研究中，我们提出了 CLAIM-BENCH，一个全面的基准，用于评估 LLMs 在科学论据提取和验证方面的能力，这一任务反映了对科学研究论证更深层次的理解。我们系统地比较了三种基于分而治之方法的思想，并在六个不同的 LLMs 上进行了比较，突出了模型在科学研究理解上的特定优势和劣势。通过跨越多个研究领域的超过 300 个论据-证据对的评估，我们揭示了 LLMs 在处理复杂科学内容方面的显著局限性。我们的结果显示，闭源模型如 GPT-4 和 Claude 在论据-证据识别任务中的准确率和召回率上始终优于开源模型。此外，精心设计的三遍和逐个提示策略显著提高了 LLMs 将分散的证据与论点准确关联的能力，尽管这会导致计算成本的增加。CLAIM-BENCH 为评估 LLMs 在科学研究理解方面的标准设定了一条新途径，既是一个诊断工具，也是构建能够进行更深入、更可靠推理的系统的方法之一。', 'title_zh': 'AI能验证科学吗？基于准确的科学断言和证据推理对LLM进行基准测试'}
{'arxiv_id': 'arXiv:2506.08234', 'title': 'Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions', 'authors': 'Yu-Ang Lee, Guan-Ting Yi, Mei-Yi Liu, Jui-Chao Lu, Guan-Bo Yang, Yun-Nung Chen', 'link': 'https://arxiv.org/abs/2506.08234', 'abstract': 'Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at this https URL.', 'abstract_zh': '最近大型语言模型（LLMs）和AI系统的进展引发了复杂AI工作流设计与优化的范式转变。通过集成多个组件，复合AI系统在执行复杂任务方面日益熟练。然而，随着这些系统的复杂性增加，优化不仅是个别组件，还包括它们之间的交互的新挑战也随之出现。尽管传统的优化方法如监督微调（SFT）和强化学习（RL）仍然是基础性的，但自然语言反馈的发展引入了有前景的新方法，特别适用于优化非可微系统。本文对复合AI系统优化的最新进展进行了系统性综述，涵盖了数值技术和语言技术方法。我们正式定义了复合AI系统优化的概念，按多个关键维度对现有方法进行了分类，并指出了该快速发展的领域中的开放研究挑战和未来方向。所调研论文列表在此处公开：这个 https URL。', 'title_zh': '复合AI系统优化：方法、挑战及未来方向'}
{'arxiv_id': 'arXiv:2506.08231', 'title': 'Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework', 'authors': 'Melissa Estevez, Nisha Singh, Lauren Dyson, Blythe Adamson, Qianyu Yuan, Megan W. Hildner, Erin Fidyk, Olive Mbah, Farhad Khan, Kathi Seidl-Rathkopf, Aaron B. Cohen', 'link': 'https://arxiv.org/abs/2506.08231', 'abstract': 'Large language models (LLMs) are increasingly used to extract clinical data from electronic health records (EHRs), offering significant improvements in scalability and efficiency for real-world data (RWD) curation in oncology. However, the adoption of LLMs introduces new challenges in ensuring the reliability, accuracy, and fairness of extracted data, which are essential for research, regulatory, and clinical applications. Existing quality assurance frameworks for RWD and artificial intelligence do not fully address the unique error modes and complexities associated with LLM-extracted data. In this paper, we propose a comprehensive framework for evaluating the quality of clinical data extracted by LLMs. The framework integrates variable-level performance benchmarking against expert human abstraction, automated verification checks for internal consistency and plausibility, and replication analyses comparing LLM-extracted data to human-abstracted datasets or external standards. This multidimensional approach enables the identification of variables most in need of improvement, systematic detection of latent errors, and confirmation of dataset fitness-for-purpose in real-world research. Additionally, the framework supports bias assessment by stratifying metrics across demographic subgroups. By providing a rigorous and transparent method for assessing LLM-extracted RWD, this framework advances industry standards and supports the trustworthy use of AI-powered evidence generation in oncology research and practice.', 'abstract_zh': '大型语言模型（LLMs）越来越多地用于从电子健康记录（EHRs）中提取临床数据，为肿瘤学领域的实时数据（RWD）编目提供了显著的规模性和效率性改进。然而，LLMs的应用引入了确保提取数据的可靠性、准确性和公平性的新挑战，这对于研究、监管和临床应用至关重要。现有的RWD和人工智能的质量保证框架并未充分解决LLM提取数据特有的错误模式和复杂性。本文提出了一套全面的框架来评估由LLM提取的临床数据质量。该框架结合了变量级别的性能基准测试、内部一致性与合理性自动验证检查以及将LLM提取的数据与人工摘要数据集或外部标准进行重新分析以识别差异的方法。这种多维度的方法有助于识别最需要改进的变量，系统地检测潜在错误，并确认数据集是否适合作为研究目的的资源。此外，该框架还支持偏倚评估，通过不同人口亚组分类指标。通过提供一种严格和透明的方法来评估LLM提取的RWD，该框架推进了行业标准，并支持在肿瘤学研究和实践中的可信AI证据生成。', 'title_zh': '确保 curated EHR衍生数据的可靠性：LLM/ML提取信息和数据准确性验证框架（VALID）'}
{'arxiv_id': 'arXiv:2506.08210', 'title': 'A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation', 'authors': 'Andrew Z. Wang, Songwei Ge, Tero Karras, Ming-Yu Liu, Yogesh Balaji', 'link': 'https://arxiv.org/abs/2506.08210', 'abstract': 'Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.', 'abstract_zh': '现代解码器大型语言模型作为文本编码器在文本到图像生成中的 effectiveness 研究', 'title_zh': '全面研究解码器型大语言模型在文本到图像生成中的应用'}
{'arxiv_id': 'arXiv:2506.08184', 'title': 'Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length', 'authors': 'Chupei Wang, Jiaqiu Vince Sun', 'link': 'https://arxiv.org/abs/2506.08184', 'abstract': "Information retrieval in Large Language Models (LLMs) is increasingly recognized as intertwined with generation capabilities rather than mere lookup. While longer contexts are often assumed to improve retrieval, the effects of intra-context interference remain understudied. To address this, we adapt the proactive interference (PI) paradigm from cognitive science, where earlier information disrupts recall of newer updates. In humans, susceptibility to such interference is inversely linked to working memory capacity. We introduce PI-LLM, an evaluation that sequentially streams semantically related key-value updates and queries only the final values. Although these final values are clearly positioned just before the query, LLM retrieval accuracy declines log-linearly toward zero as interference accumulates; errors arise from retrieving previously overwritten values. Attempts to mitigate interference via prompt engineering (e.g., instructing models to ignore earlier input) yield limited success. These findings reveal a fundamental constraint on LLMs' ability to disentangle interference and flexibly manipulate information, suggesting a working memory bottleneck beyond mere context access. This calls for approaches that strengthen models' ability to suppress irrelevant content during retrieval.", 'abstract_zh': '在大型语言模型中基于干扰的信息检索', 'title_zh': '无法忘记：前向干扰揭示LLM在超出上下文长度之外的工作记忆限制'}
{'arxiv_id': 'arXiv:2506.08173', 'title': 'Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles', 'authors': 'Nguyen Phu Vinh, Anh Chung Hoang, Chris Ngo, Truong-Son Hy', 'link': 'https://arxiv.org/abs/2506.08173', 'abstract': 'Large Language Models (LLMs) have shown strong capabilities in code generation and comprehension, yet their application to complex software engineering tasks often suffers from low precision and limited interpretability. We present Repeton, a fully open-source framework that leverages LLMs for precise and automated code manipulation in real-world Git repositories. Rather than generating holistic fixes, Repeton operates through a structured patch-and-test pipeline: it iteratively diagnoses issues, proposes code changes, and validates each patch through automated testing. This stepwise process is guided by lightweight heuristics and development tools, avoiding reliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite benchmark, our method shows good performance compared to RAG-based methods in both patch validity and interpretability. By decomposing software engineering tasks into modular, verifiable stages, Repeton provides a practical path toward scalable and transparent autonomous debugging.', 'abstract_zh': '大型语言模型（LLMs）在代码生成和理解方面展现了强大的能力，但在应用于复杂的软件工程任务时往往精度较低且可解释性有限。我们提出了一种名为Repeton的全开源框架，利用LLMs进行精确且自动化的代码操作，应用于实际的Git仓库。Repeton 不是生成整体修复方案，而是通过结构化的补丁和测试管道进行操作：它迭代诊断问题，提出代码更改，并通过自动化测试验证每个补丁。这一逐步过程由轻量级的启发式方法和开发工具引导，避免依赖基于嵌入式检索系统。在SWE-bench Lite基准上评估，我们的方法在补丁有效性和可解释性方面与基于RAG的方法相比表现良好。通过将软件工程任务分解为模块化且可验证的阶段，Repeton 提供了一条实现可扩展和透明的自动调试的可行途径。', 'title_zh': 'Repeton：基于ReAct引导的结构化漏洞修复循环'}
{'arxiv_id': 'arXiv:2506.08171', 'title': 'Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models', 'authors': 'Daniel Koh, Yannic Noller, Corina S. Pasareanu, Adrians Skapars, Youcheng Sun', 'link': 'https://arxiv.org/abs/2506.08171', 'abstract': "Large language models (LLMs) have been successfully applied to a variety of coding tasks, including code generation, completion, and repair. However, more complex symbolic reasoning tasks remain largely unexplored by LLMs. This paper investigates the capacity of LLMs to reason about worst-case executions in programs through symbolic constraints analysis, aiming to connect LLMs and symbolic reasoning approaches. Specifically, we define and address the problem of worst-case symbolic constraints analysis as a measure to assess the comprehension of LLMs. We evaluate the performance of existing LLMs on this novel task and further improve their capabilities through symbolic reasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories) constraint solving and supported by a specially designed dataset of symbolic constraints. Experimental results show that our solver-aligned model, WARP-1.0-3B, consistently surpasses size-matched and even much larger baselines, demonstrating that a 3B LLM can recover the very constraints that pin down an algorithm's worst-case behaviour through reinforcement learning methods. These findings suggest that LLMs are capable of engaging in deeper symbolic reasoning, supporting a closer integration between neural network-based learning and formal methods for rigorous program analysis.", 'abstract_zh': '大型语言模型通过符号约束分析探究最坏情况执行的推理能力：连接大型语言模型和符号推理方法的新任务及强化学习优化', 'title_zh': '最坏情况符号约束分析与大型语言模型的一般化'}
{'arxiv_id': 'arXiv:2506.08147', 'title': 'Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models', 'authors': 'Muhammad Usman, Muhammad Ahmad, M. Shahiki Tash, Irina Gelbukh, Rolando Quintero Tellez, Grigori Sidorov', 'link': 'https://arxiv.org/abs/2506.08147', 'abstract': "Social media platforms are critical spaces for public discourse, shaping opinions and community dynamics, yet their widespread use has amplified harmful content, particularly hate speech, threatening online safety and inclusivity. While hate speech detection has been extensively studied in languages like English and Spanish, Urdu remains underexplored, especially using translation-based approaches. To address this gap, we introduce a trilingual dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and Spanish (3,162 samples), collected via keyword filtering, with a balanced distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology leverages attention layers as a precursor to transformer-based models and large language models (LLMs), enhancing feature extraction for multilingual hate speech detection. For non-transformer models, we use TF-IDF for feature extraction. The dataset is benchmarked using state-of-the-art models, including GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators, following rigorous guidelines, ensured high dataset quality, achieving a Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5 Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of 0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B). These results reflect improvements of 8.75% in English (over SVM baseline 0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline 0.82). Our framework offers a robust solution for multilingual hate speech detection, fostering safer digital communities worldwide.", 'abstract_zh': "社交媒体平台是公共 discourse 的关键空间，塑造意见和社群动态，但其广泛应用放大了有害内容，尤其是仇恨言论，威胁在线安全和包容性。尽管仇恨言论检测在英语和西班牙语等语言中得到了广泛研究，但乌尔都语仍相对未被充分探索，尤其是在基于翻译的方法方面。为解决这一问题，我们引入了一个包含10,193条推文的三语数据集，分别包含3,834条英语、3,197条乌尔都语和3,162条西班牙语样本，通过关键词过滤收集，具有4,849个仇恨和5,344个非仇恨标签的平衡分布。我们的方法利用注意力层作为变压器模型和大型语言模型（LLMs）之前的预处理步骤，增强多语言仇恨言论检测的功能提取。对于非变压器模型，我们使用TF-IDF进行特征提取。该数据集使用包括GPT-3.5 Turbo和Qwen 2.5 72B在内的最新模型进行基准测试，以及传统机器学习模型如SVM和其他变压器（例如BERT、RoBERTa）。三名遵循严格指南的注释者确保了高质量的数据集， Fleiss' Kappa值为0.821。结合注意力层与GPT-3.5 Turbo和Qwen 2.5 72B的方法显示出强大的性能，其中英语（GPT-3.5 Turbo）的宏F1分数为0.87，西班牙语（GPT-3.5 Turbo）为0.85，乌尔都语（Qwen 2.5 72B）为0.81，联合多语言模型（Qwen 2.5 72B）为0.88。这些结果反映了相对于SVM基准的改进，分别为8.75%（英语）、8.97%（西班牙语）、5.19%（乌尔都语）和7.32%（联合多语言模型）。我们的框架为多语言仇恨言论检测提供了稳健的解决方案，推动全球更安全的数字社区建设。", 'title_zh': '基于大规模语言模型的翻译导向方法在社交媒体多语言仇恨言论检测中应用'}
{'arxiv_id': 'arXiv:2506.08060', 'title': 'Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques', 'authors': 'Asankhaya Sharma', 'link': 'https://arxiv.org/abs/2506.08060', 'abstract': 'Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length $l$, datasets of size $\\mathrm{O}\\left( \\frac{m V}{\\varepsilon^2} \\log \\frac{m}{\\delta} \\right)$ or, with bounded context, $\\mathrm{O}\\left( \\frac{l \\log V}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$ suffice to approximate fine-tuned behavior across $m$ contexts within error $\\varepsilon$, where $V$ is the vocabulary size and $\\delta$ is the failure probability. For linear classification, datasets of size $\\mathrm{O}\\left( \\frac{d}{\\varepsilon} \\right)$ or, with fixed context, $\\mathrm{O}\\left( \\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$ are sufficient, where $d$ is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications.', 'abstract_zh': '大型语言模型已transformed自然语言处理，然而监督微调（SFT）仍然计算密集。本文在包括无界计算资源和访问微调数据集的理想假设下，正式证明通过SFT获得的能力可以用基础变压器模型在推理时的技术，特别是上下文学习（ICL），在不改变模型参数的情况下进行近似。我们将这些结果扩展到具有有限上下文字长和部分数据集访问的实际场景。对于固定输出长度$l$的文本生成任务，大小为$\\mathrm{O}\\left( \\frac{m V}{\\varepsilon^2} \\log \\frac{m}{\\delta} \\right)$或在有界上下文情况下为$\\mathrm{O}\\left( \\frac{l \\log V}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$的数据集足以在$m$个上下文中以误差$\\varepsilon$近似微调行为，其中$V$是词汇量，$\\delta$是失败概率。对于线性分类任务，大小为$\\mathrm{O}\\left( \\frac{d}{\\varepsilon} \\right)$或固定上下文中为$\\mathrm{O}\\left( \\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$的数据集足以在误差$\\varepsilon$内进行近似，其中$d$是输入维度。基于变压器的图灵完全性，这些结果为大型语言模型的高效部署提供了理论基础，实际技术如检索增强生成将理论与实际应用相连接。', 'title_zh': '通过推理时技术激发细调变压器的能力'}
{'arxiv_id': 'arXiv:2506.08027', 'title': 'Recipes for Pre-training LLMs with MXFP8', 'authors': 'Asit Mishra, Dusan Stosic, Simon Layton', 'link': 'https://arxiv.org/abs/2506.08027', 'abstract': "Precision scaling - using fewer bits to represent model parameters and related tensors during pre-training - has emerged as a compelling technique for improving GPU efficiency without sacrificing accuracy. Microscaling (MX) formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling this precision scaling aspect. These formats combine narrow floating-point data types with per-block scaling factors, offering a fine-grained approach to quantizing tensors.\nAlthough MX-formats offer the promise of improved numeric stability compared to other reduced-precision representations, in practice they must be used carefully in order to successfully converge an LLM on a multi-trillion token dataset. In this paper, we show that the rounding mode suggested in OCP specification can lead to divergence when pre-training an LLM. We show an improved rounding mode, which uses round-to-infinity to compute scaling factors, enables successful pre-training in MXFP8 for an 8B model on 15T tokens.", 'abstract_zh': '精度缩放——在预训练过程中使用较少位数来表示模型参数及相关张量——已成为一种无需牺牲准确性的方法来提高GPU效率的技术。NVIDIA最新Blackwell GPU上的Microscaling (MX) 格式代表了实现这一精度缩放方面的重大突破。这些格式结合了窄浮点数据类型和每块的缩放因子，提供了对张量进行量化的一种精细方法。尽管MX格式与其它低精度表示相比提供了更好的数值稳定性，但在实际应用中，为了在大规模的万亿级标记数据集上成功预训练LLM，必须谨慎使用。在本文中，我们展示了OCP规范中建议的舍入模式可能导致在预训练LLM时发散的情况。我们展示了一种改进的舍入模式，使用向无穷大舍入来计算缩放因子，使得在MXFP8格式下成功预训练一个8B模型并处理15T标记数据成为可能。', 'title_zh': '使用MXFP8预训练大规模语言模型的方法'}
{'arxiv_id': 'arXiv:2506.08022', 'title': 'Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining', 'authors': 'Chenxi Liu, Tianyi Xiong, Ruibo Chen, Yihan Wu, Junfeng Guo, Tianyi Zhou, Heng Huang', 'link': 'https://arxiv.org/abs/2506.08022', 'abstract': 'The task adaptation and alignment of Large Multimodal Models (LMMs) have been significantly advanced by instruction tuning and further strengthened by recent preference optimization. Yet, most LMMs still suffer from severe modality imbalance during reasoning, i.e., outweighing language prior biases over visual inputs, which bottlenecks their generalization to downstream tasks and causes hallucinations. However, existing preference optimization approaches for LMMs do not focus on restraining the internal biases of their Large Language Model (LLM) backbones when curating the training data. Moreover, they heavily rely on offline data and lack the capacity to explore diverse responses adaptive to dynamic distributional shifts during training. Meanwhile, Group Relative Policy Optimization (GRPO), a recent method using online-generated data and verified rewards to improve reasoning capabilities, remains largely underexplored in LMM alignment. In this paper, we propose a novel preference learning framework, Modality-Balancing Preference Optimization (MBPO), to address the modality imbalance in LMMs. MBPO constructs a more effective offline preference dataset by generating hard negatives, i.e., rejected responses misled by LLM biases due to limited usage of visual information, through adversarial perturbation of input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended tasks to generate online responses with verified rewards. GRPO is then employed to train the model with offline-online hybrid data. Extensive experiments demonstrate that MBPO can enhance LMM performance on challenging vision-language tasks and effectively reduce hallucinations.', 'abstract_zh': '大型多模态模型的任务适配与对齐通过指令调优得到了显著进展，并通过最近的偏好优化得到了进一步强化。然而，大多数大型多模态模型在推理过程中仍然面临严重的模态不平衡问题，即语言先验偏差过度支配视觉输入，这限制了其对下游任务的泛化能力并导致幻觉。现有的针对大型多模态模型的偏好优化方法在编排训练数据时并未关注抑制其大型语言模型（LLM）核心模块内的内部偏差。此外，它们高度依赖离线数据，并缺乏在训练过程中探索适应动态分布转移的多样化响应的能力。同时，使用在线生成数据和验证奖励提高推理能力的组相对策略优化（GRPO）方法在大型多模态模型对齐中的应用仍处于初步阶段。本文提出了一种新颖的偏好学习框架——模态平衡偏好优化（MBPO），以解决大型多模态模型中的模态不平衡问题。MBPO通过生成对抗性扰动输入图像以限制LLM偏差来误导的难负样本（即硬负样本），构建更有效的离线偏好数据集。此外，MBPO利用封闭任务易于验证的性质生成在线响应和验证奖励。然后使用离线-在线混合数据训练模型。广泛实验证明，MBPO能够提高大型多模态模型在视觉-语言任务上的性能并有效减少幻觉。', 'title_zh': '大型多模态模型的模态平衡偏好优化通过对抗负挖掘'}
{'arxiv_id': 'arXiv:2506.08018', 'title': 'KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache', 'authors': 'Fei Li, Song Liu, Weiguo Wu, Shiqiang Nie, Jinyu Wang', 'link': 'https://arxiv.org/abs/2506.08018', 'abstract': 'The high memory demands of the Key-Value (KV) Cache during the inference of Large Language Models (LLMs) severely restrict their deployment in resource-constrained platforms. Quantization can effectively alleviate the memory pressure caused by KV Cache. However, existing methods either rely on static one-size-fits-all precision allocation or fail to dynamically prioritize critical KV in long-context tasks, forcing memory-accuracy-throughput tradeoffs. In this work, we propose a novel mixed-precision quantization method for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to evaluate how individual Key and Value projection matrices affect the model loss, enabling layer-specific bit-width allocation for mix-precision quantization. It dynamically prioritizes higher precision for important layers while aggressively quantizing less influential ones, achieving a tunable balance between accuracy and efficiency. KVmix also introduces a dynamic long-context optimization strategy that adaptively keeps full-precision KV pairs for recent pivotal tokens and compresses older ones, achieving high-quality sequence generation with low memory usage. Additionally, KVmix provides efficient low-bit quantization and CUDA kernels to optimize computational overhead. On LLMs such as Llama and Mistral, KVmix achieves near-lossless inference performance with extremely low quantization configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x memory compression and a 5.3x speedup in inference throughput.', 'abstract_zh': 'KVmix：一种用于大型语言模型推理中Key-Value缓存的新型混合精度量化方法', 'title_zh': 'KVmix：基于梯度的键值缓存层重要性感知混合精度量化'}
{'arxiv_id': 'arXiv:2506.07675', 'title': 'QUITE: A Query Rewrite System Beyond Rules with LLM Agents', 'authors': 'Yuyang Song, Hanxu Yan, Jiale Lao, Yibo Wang, Yufei Li, Yuanchun Zhou, Jianguo Wang, Mingjie Tang', 'link': 'https://arxiv.org/abs/2506.07675', 'abstract': 'Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.', 'abstract_zh': '基于大语言模型的SQL查询重构：超越规则的方法', 'title_zh': 'QUITE：超出规则的查询重写系统以LLM代理为基础'}
{'arxiv_id': 'arXiv:2506.06363', 'title': 'ChemGraph: An Agentic Framework for Computational Chemistry Workflows', 'authors': 'Thang D. Pham, Aditya Tanikanti, Murat Keçeli', 'link': 'https://arxiv.org/abs/2506.06363', 'abstract': "Atomistic simulations are essential tools in chemistry and materials science, accelerating the discovery of novel catalysts, energy storage materials, and pharmaceuticals. However, running these simulations remains challenging due to the wide range of computational methods, diverse software ecosystems, and the need for expert knowledge and manual effort for the setup, execution, and validation stages. In this work, we present ChemGraph, an agentic framework powered by artificial intelligence and state-of-the-art simulation tools to streamline and automate computational chemistry and materials science workflows. ChemGraph leverages graph neural network-based foundation models for accurate yet computationally efficient calculations and large language models (LLMs) for natural language understanding, task planning, and scientific reasoning to provide an intuitive and interactive interface. Users can perform tasks such as molecular structure generation, single-point energy, geometry optimization, vibrational analysis, and thermochemistry calculations with methods ranging from tight-binding and machine learning interatomic potentials to density functional theory or wave function theory-based methods. We evaluate ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs (GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows, while more complex tasks benefit from using larger models like GPT-4o. Importantly, we show that decomposing complex tasks into smaller subtasks through a multi-agent framework enables smaller LLM models to match or exceed GPT-4o's performance in specific scenarios.", 'abstract_zh': '基于人工智能的ChemGraph框架：加速计算化学与材料科学工作流', 'title_zh': 'ChemGraph: 一个自主的计算化学工作流框架'}
{'arxiv_id': 'arXiv:2506.05695', 'title': 'Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework', 'authors': 'Lingyuan Liu, Mengxiang Zhang', 'link': 'https://arxiv.org/abs/2506.05695', 'abstract': 'Knowledge Distillation (KD) compresses large language models (LLMs) by transferring the teacher model\'s capabilities to a smaller student model, reducing inference cost and memory usage while maintaining performance. However, existing KD methods for LLMs often fail to prevent significant shifts in the student model\'s distribution during training, leading to issues such as catastrophic forgetting, mode collapse, and training-inference mismatch. To address these challenges, we propose a novel, plug-in curriculum learning framework inspired by the strength training principle of "progressive overload" (POCL), which can be seamlessly integrated into existing white-box KD approaches with minimal computational overhead. The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures. By starting with the easiest samples and progressively increasing the difficulty, the approach enhances both the stability and efficiency of learning. Extensive experiments in instruction-following settings demonstrate that POCL consistently improves the performance of distilled student models across various white-box KD methods and model families. Our findings highlight the effectiveness of sorted training samples in KD for LLMs. More generally, our work demonstrates how to structure training data within the KD process to enhance the stability and performance of distilled LLMs.', 'abstract_zh': '知识蒸馏（KD）通过将教师模型的能力转移到较小的学生模型中，压缩大型语言模型（LLMs），同时减少推理成本和内存使用，并保持性能。然而，现有的LLMs知识蒸馏方法往往无法防止学生模型分布训练过程中的显著变化，导致灾难性遗忘、模式坍缩和训练-推理不匹配等问题。为了解决这些挑战，我们提出了一种新型插件式课程学习框架，该框架灵感来源于“渐进超载”（POCL）的体力训练原则，并能与现有白盒知识蒸馏方法无缝集成，计算开销最小。该框架包含两个核心组件：（1）一个难度度量器，用于按难度对训练样本进行排序和分区；（2）一个训练调度器，在固定间隔逐步引入这些子集到蒸馏过程中，并应用逐渐升高的温度损失函数。通过从最简单的样本开始并逐步增加难度，该方法增强了学习的稳定性和效率。在指令跟随设置下的广泛实验表明，POCL方法能持续改善各种白盒知识蒸馏方法和模型家族中蒸馏学生模型的性能。我们的研究结果突显了排序训练样本在LLMs知识蒸馏中的有效性。更普遍地，我们的工作展示了如何在知识蒸馏过程中结构化训练数据以增强蒸馏LLMs的稳定性和性能。', 'title_zh': '逐步变强！通过课程学习框架增强大型语言模型的知识蒸馏'}
{'arxiv_id': 'arXiv:2506.04760', 'title': 'Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion', 'authors': 'Lingyuan Liu, Mengxiang Zhang', 'link': 'https://arxiv.org/abs/2506.04760', 'abstract': 'Large Language Models (LLMs) have shown potential in generating hypothetical documents for query expansion, thereby enhancing information retrieval performance. However, the efficacy of this method is highly dependent on the quality of the generated documents, which often requires complex prompt strategies and the integration of advanced dense retrieval techniques. This can be both costly and computationally intensive. To mitigate these limitations, we explore the use of zero-shot LLM-based query expansion to improve sparse retrieval, particularly for learned sparse retrievers. We introduce a novel fusion ranking framework, Exp4Fuse, which enhances the performance of sparse retrievers through an indirect application of zero-shot LLM-based query expansion. Exp4Fuse operates by simultaneously considering two retrieval routes-one based on the original query and the other on the LLM-augmented query. It then generates two ranked lists using a sparse retriever and fuses them using a modified reciprocal rank fusion method. We conduct extensive evaluations of Exp4Fuse against leading LLM-based query expansion methods and advanced retrieval techniques on three MS MARCO-related datasets and seven low-resource datasets. Experimental results reveal that Exp4Fuse not only surpasses existing LLM-based query expansion methods in enhancing sparse retrievers but also, when combined with advanced sparse retrievers, achieves SOTA results on several benchmarks. This highlights the superior performance and effectiveness of Exp4Fuse in improving query expansion for sparse retrieval.', 'abstract_zh': '大型语言模型在查询扩展中的潜在应用可以通过假设文档生成来提高信息检索性能，但其效果高度依赖于生成文档的质量，这通常需要复杂的提示策略和先进密集检索技术的集成，这可能是昂贵且计算密集型的。为缓解这些局限性，我们探索了零-shot LLM-based查询扩展在改进稀疏检索中的应用，特别是对于学习到的稀疏检索器。我们提出了一种新颖的融合排名框架Exp4Fuse，通过间接应用零-shot LLM-based查询扩展来增强稀疏检索器的性能。Exp4Fuse通过同时考虑两条检索路径——一条基于原始查询，另一条基于LLM增强后的查询——工作。然后使用稀疏检索器生成两个排名列表，并通过修改后的倒数排名融合方法将它们融合。我们在三个MS MARCO相关数据集和七个低资源数据集上对Exp4Fuse与领先的方法进行了广泛的评估和先进的检索技术。实验结果表明，Exp4Fuse不仅在增强稀疏检索器方面超过了现有的LLM-based查询扩展方法，而且与先进的稀疏检索器结合使用时，在多个基准上实现了SOTA结果。这突显了Exp4Fuse在提高稀疏检索查询扩展性能和效果方面的优越性。', 'title_zh': 'Exp4Fuse: 一种基于大型语言模型的查询扩展增强稀疏检索的排序融合框架'}
{'arxiv_id': 'arXiv:2506.00160', 'title': 'Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement', 'authors': 'Qihui Fan, Enfu Nan, Wenbo Li, Lei Lu, Pu Zhao, Yanzhi Wang', 'link': 'https://arxiv.org/abs/2506.00160', 'abstract': 'The growing popularity of social deduction game systems for both business applications and AI research has greatly benefited from the rapid advancements in Large Language Models (LLMs), which now demonstrate stronger reasoning and persuasion capabilities. Especially with the raise of DeepSeek R1 and V3 models, LLMs should enable a more engaging experience for human players in LLM-agent-based social deduction games like Werewolf. Previous works either fine-tuning, advanced prompting engineering, or additional experience pool to achieve engaging text-format Werewolf game experience. We propose a novel yet straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS) models designed for enhanced compatibility with various LLM models, and improved user engagement. We argue with ever enhancing LLM reasoning, extra components will be unnecessary in the case of Werewolf.', 'abstract_zh': '社会推理游戏系统在商业应用和AI研究中的日益流行得益于大规模语言模型的快速进步，这些模型现在展示了更强的推理和说服能力。尤其是在DeepSeek R1和V3模型的推动下，大规模语言模型应在基于大规模语言模型-代理的社会推理游戏中，如狼人杀，为人类玩家提供更为吸引人的体验。我们提出了一种新颖且简洁的大规模语言模型驱动的狼人杀游戏系统，该系统配备了增强兼容性的文本到语音模型，以提高用户体验。我们认为，在大规模语言模型推理能力不断增强的情况下，狼人杀游戏中额外的组件将变得不必要的。', 'title_zh': 'werewolf: 一个配备TTS的简单游戏框架以提高用户参与度'}
