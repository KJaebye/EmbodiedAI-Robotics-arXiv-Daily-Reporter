{'arxiv_id': 'arXiv:2508.19257', 'title': 'TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models', 'authors': 'Chenghao Liu, Jiachen Zhang, Chengxuan Li, Zhimu Zhou, Shixin Wu, Songfang Huang, Huiling Duan', 'link': 'https://arxiv.org/abs/2508.19257', 'abstract': 'Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\\% vs 68.4\\% baseline), cross-environment validation on SimplerEnv (4.8\\% relative improvement), and 8.7\\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.', 'abstract_zh': 'Temporal Token Fusion for Enhancing Vision-Language-Action Models in Robotic Manipulation Tasks', 'title_zh': 'TTF-VLA：基于像素注意集成的时序令牌融合视觉-语言-行动模型'}
{'arxiv_id': 'arXiv:2508.20013', 'title': 'Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach', 'authors': 'Lotte Gross, Rebecca Walter, Nicole Zoppi, Adrien Justus, Alessandro Gambetti, Qiwei Han, Maximilian Kaiser', 'link': 'https://arxiv.org/abs/2508.20013', 'abstract': "This study addresses critical industrial challenges in e-commerce product categorization, namely platform heterogeneity and the structural limitations of existing taxonomies, by developing and deploying a multimodal hierarchical classification framework. Using a dataset of 271,700 products from 40 international fashion e-commerce platforms, we integrate textual features (RoBERTa), visual features (ViT), and joint vision--language representations (CLIP). We investigate fusion strategies, including early, late, and attention-based fusion within a hierarchical architecture enhanced by dynamic masking to ensure taxonomic consistency. Results show that CLIP embeddings combined via an MLP-based late-fusion strategy achieve the highest hierarchical F1 (98.59\\%), outperforming unimodal baselines. To address shallow or inconsistent categories, we further introduce a self-supervised ``product recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with cluster purities above 86\\%. Cross-platform experiments reveal a deployment-relevant trade-off: complex late-fusion methods maximize accuracy with diverse training data, while simpler early-fusion methods generalize more effectively to unseen platforms. Finally, we demonstrate the framework's industrial scalability through deployment in EURWEB's commercial transaction intelligence platform via a two-stage inference pipeline, combining a lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance cost and accuracy.", 'abstract_zh': '本研究通过开发和部署一种多模态层级分类框架，解决了电子商务产品分类中的关键工业挑战，包括平台异质性和现有分类体系的结构性限制。使用来自40个国际时尚电子商务平台的271,700个产品数据集，我们整合了文本特征（RoBERTa）、视觉特征（ViT）及联合视觉-语言表示（CLIP）。研究探讨了包括早期融合、晚期融合及基于注意力的融合在内的融合策略，并通过动态遮掩以增强层级结构，确保分类的内在一致性。结果显示，基于MLP的晚期融合策略结合CLIP嵌入实现了最高的层级F1值（98.59%），优于单模态基线。为解决浅层或不一致的类别问题，我们进一步引入了一种自监督的“产品再分类”管道，使用SimCLR、UMAP和级联聚类，发现了具有86%以上纯度的新细分类别（例如“鞋”的子类型）。跨平台实验揭示了部署相关的权衡：复杂的晚期融合方法在多样化的训练数据中能够最大化准确率，而简单的早期融合方法则能够更有效地泛化到未见过的平台。最后，通过在EURWEB的商业交易智能平台上采用两阶段推断管道部署该框架，结合轻量级RoBERTa阶段和GPU加速的多模态阶段，展示了框架的工业可扩展性，平衡了成本和准确率。', 'title_zh': '跨平台电子商务产品类别化与再类别化：一种多模态层次分类方法'}
{'arxiv_id': 'arXiv:2508.19574', 'title': 'Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation', 'authors': 'Mingxi Fu, Fanglei Fu, Xitong Ling, Huaitian Yuan, Tian Guan, Yonghong He, Lianghui Zhu', 'link': 'https://arxiv.org/abs/2508.19574', 'abstract': "Pathological image segmentation faces numerous challenges, particularly due to ambiguous semantic boundaries and the high cost of pixel-level annotations. Although recent semi-supervised methods based on consistency regularization (e.g., UniMatch) have made notable progress, they mainly rely on perturbation-based consistency within the image modality, making it difficult to capture high-level semantic priors, especially in structurally complex pathology images. To address these limitations, we propose MPAMatch - a novel segmentation framework that performs pixel-level contrastive learning under a multimodal prototype-guided supervision paradigm. The core innovation of MPAMatch lies in the dual contrastive learning scheme between image prototypes and pixel labels, and between text prototypes and pixel labels, providing supervision at both structural and semantic levels. This coarse-to-fine supervisory strategy not only enhances the discriminative capability on unlabeled samples but also introduces the text prototype supervision into segmentation for the first time, significantly improving semantic boundary modeling. In addition, we reconstruct the classic segmentation architecture (TransUNet) by replacing its ViT backbone with a pathology-pretrained foundation model (Uni), enabling more effective extraction of pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art methods, validating its dual advantages in structural and semantic modeling.", 'abstract_zh': '病理图像分割面临着诸多挑战，特别是由于语义边界的模糊性和像素级注释的高成本。尽管基于一致性正则化的半监督方法（如UniMatch）取得了显著进展，但这些方法主要依赖于图像模态内的扰动一致性，难以捕捉高层次的语义先验，尤其是在结构复杂的病理图像中。为克服这些局限性，我们提出了一种名为MPAMatch的新分割框架，它在多模态原型引导监督范式下进行像素级对比学习。MPAMatch的核心创新在于图像原型与像素标签之间的双分支对比学习方案，以及文本原型与像素标签之间的对比学习方案，从而在结构和语义两个层面提供监督。这种从粗到细的监督策略不仅增强了对未标注样本的辨别能力，还首次将文本原型监督引入分割中，显著提升了语义边界的建模。此外，我们通过将TransUNet的经典分割架构中的ViT主干替换为预训练的病理基础模型（Uni），重构了该架构，以实现对病理相关特征更有效的提取。在GLAS、EBHI-SEG-GLAND、EBHI-SEG-CANCER和KPI上的广泛实验表明，MPAMatch在结构和语义建模方面优于现有方法。', 'title_zh': '多模态原型对齐在半监督病理图像分割中的应用'}
{'arxiv_id': 'arXiv:2508.19376', 'title': 'Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments', 'authors': 'Dikshant Sagar, Kaiwen Yu, Alejandro Yankelevich, Jianming Bian, Pierre Baldi', 'link': 'https://arxiv.org/abs/2508.19376', 'abstract': 'Recent progress in large language models (LLMs) has shown strong potential for multimodal reasoning beyond natural language. In this work, we explore the use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for classifying neutrino interactions from pixelated detector images in high-energy physics (HEP) experiments. We benchmark its performance against an established CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as classification accuracy, precision, recall, and AUC-ROC. Our results show that the VLM not only matches or exceeds CNN performance but also enables richer reasoning and better integration of auxiliary textual or semantic context. These findings suggest that VLMs offer a promising general-purpose backbone for event classification in HEP, paving the way for multimodal approaches in experimental neutrino physics.', 'abstract_zh': 'Recent进展在大规模语言模型（LLMs）在多模态推理方面的潜力超越了自然语言。在本工作中，我们探索了基于LLaMA 3.2的调优Vision-Language Model（VLM）在高能物理（HEP）实验中从像素化探测器图像分类中微子相互作用的应用。我们将其性能与NOvA和DUNE等实验中使用的成熟CNN基线进行了基准测试，评估了分类准确性、精确度、召回率和AUC-ROC等指标。结果表明，VLM不仅能匹配甚至超越CNN的表现，还能实现更丰富的推理和更好的辅助文本或语义上下文的整合。这些发现表明，VLM为HEP中的事件分类提供了一种前景广阔的通用基础架构，为实验中微子物理的多模态方法铺平了道路。', 'title_zh': 'Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments 细调视觉-语言模型以进行高能物理实验中的中子事件分析'}
{'arxiv_id': 'arXiv:2508.19320', 'title': 'MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation', 'authors': 'Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Xiaoqiang Liu, Pengfei Wan', 'link': 'https://arxiv.org/abs/2508.19320', 'abstract': 'Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.', 'abstract_zh': '近年来，交互式数字人物视频生成吸引广泛关注并取得了显著进展。然而，构建能够实时处理多样化输入信号的实用系统仍然极具挑战性，现有方法往往面临高延迟、高计算成本和控制能力有限的问题。在本工作中，我们提出了一种自回归视频生成框架，该框架能够以流式方式实现多模态交互控制和低延迟外推。通过对标准大型语言模型进行少量修改，我们的框架接受包括音频、姿态和文本在内的多模态条件编码，并输出空间上和语义上一致的表示，以指导泛化头部的去噪过程。为此，我们构建了一个规模为约20,000小时的大型对话数据集，提供了丰富的对话场景用于训练。我们还引入了深度压缩自编码器，压缩比最高可达64倍，有效地缓解了自回归模型的长期推理负担。大量实验表明，我们的方法在低延迟、高性能和精细的多模态可控性方面具有明显优势。', 'title_zh': 'MIDAS：多模态交互数字人合成 via 实时自回归视频生成'}
{'arxiv_id': 'arXiv:2508.19319', 'title': 'MedVQA-TREE: A Multimodal Reasoning and Retrieval Framework for Sarcopenia Prediction', 'authors': 'Pardis Moradbeiki, Nasser Ghadiri, Sayed Jalal Zahabi, Uffe Kock Wiil, Kristoffer Kittelmann Brockhattingen, Ali Ebrahimi', 'link': 'https://arxiv.org/abs/2508.19319', 'abstract': 'Accurate sarcopenia diagnosis via ultrasound remains challenging due to subtle imaging cues, limited labeled data, and the absence of clinical context in most models. We propose MedVQA-TREE, a multimodal framework that integrates a hierarchical image interpretation module, a gated feature-level fusion mechanism, and a novel multi-hop, multi-query retrieval strategy. The vision module includes anatomical classification, region segmentation, and graph-based spatial reasoning to capture coarse, mid-level, and fine-grained structures. A gated fusion mechanism selectively integrates visual features with textual queries, while clinical knowledge is retrieved through a UMLS-guided pipeline accessing PubMed and a sarcopenia-specific external knowledge base. MedVQA-TREE was trained and evaluated on two public MedVQA datasets (VQA-RAD and PathVQA) and a custom sarcopenia ultrasound dataset. The model achieved up to 99% diagnostic accuracy and outperformed previous state-of-the-art methods by over 10%. These results underscore the benefit of combining structured visual understanding with guided knowledge retrieval for effective AI-assisted diagnosis in sarcopenia.', 'abstract_zh': '通过超声准确诊断肌少症仍具有挑战性，因为影像线索微妙、标注数据有限以及多数模型缺乏临床背景。我们提出了MedVQA-TREE，这是一种多模态框架，结合了层次化的图像解释模块、门控特征级融合机制以及新颖的多跳多查询检索策略。视觉模块包括解剖分类、区域分割以及基于图的空间推理，以捕捉粗略、中等和精细结构。门控融合机制选择性地将视觉特征与文本查询融合，而临床知识则通过由UMLS引导的管道从PubMed和专门的肌少症外部知识库中检索。MedVQA-TREE在两个公开的MedVQA数据集（VQA-RAD和PathVQA）以及一个自定义的肌少症超声数据集上进行了训练和评估。该模型准确率最高可达99%，并在诊断准确性上超过此前最佳方法10%以上。这些结果强调了结合结构化视觉理解与引导知识检索对于有效肌少症辅助诊断AI的重要性。', 'title_zh': 'MedVQA-TREE: 一种多模态推理与检索框架用于肌少症预测'}
{'arxiv_id': 'arXiv:2508.19289', 'title': 'Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation', 'authors': 'Tai Inui, Steven Oh, Magdeline Kuan', 'link': 'https://arxiv.org/abs/2508.19289', 'abstract': 'We present an unsupervised slide-quality assessment pipeline that combines seven expert-inspired visual-design metrics (whitespace, colorfulness, edge density, brightness contrast, text density, color harmony, layout balance) with CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate presentation slides. Trained on 12k professional lecture slides and evaluated on six academic talks (115 slides), our method achieved Pearson correlations up to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual ratings, discriminant validity against speaker-delivery scores, and exploratory alignment with overall impressions. Our results show that augmenting low-level design cues with multimodal embeddings closely approximates audience perceptions of slide quality, enabling scalable, objective feedback in real time.', 'abstract_zh': '一种结合七种专家启发的视觉设计指标和CLIP-ViT嵌入的无监督幻灯片质量评估管道：基于孤立森林异常评分的方法及其与人类视觉质量评级的相关性分析', 'title_zh': '没有设计师的视角：基于设计师线索增强的无监督幻灯片质量评估研究'}
