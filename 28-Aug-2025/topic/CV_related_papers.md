# DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View 

**Title (ZH)**: DATR: 基于扩散的稀视角3D苹果树重建框架 

**Authors**: Tian Qiu, Alan Zoubi, Yiyuan Lin, Ruiming Du, Lailiang Cheng, Yu Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2508.19508)  

**Abstract**: Digital twin applications offered transformative potential by enabling real-time monitoring and robotic simulation through accurate virtual replicas of physical assets. The key to these systems is 3D reconstruction with high geometrical fidelity. However, existing methods struggled under field conditions, especially with sparse and occluded views. This study developed a two-stage framework (DATR) for the reconstruction of apple trees from sparse views. The first stage leverages onboard sensors and foundation models to semi-automatically generate tree masks from complex field images. Tree masks are used to filter out background information in multi-modal data for the single-image-to-3D reconstruction at the second stage. This stage consists of a diffusion model and a large reconstruction model for respective multi view and implicit neural field generation. The training of the diffusion model and LRM was achieved by using realistic synthetic apple trees generated by a Real2Sim data generator. The framework was evaluated on both field and synthetic datasets. The field dataset includes six apple trees with field-measured ground truth, while the synthetic dataset featured structurally diverse trees. Evaluation results showed that our DATR framework outperformed existing 3D reconstruction methods across both datasets and achieved domain-trait estimation comparable to industrial-grade stationary laser scanners while improving the throughput by $\sim$360 times, demonstrating strong potential for scalable agricultural digital twin systems. 

**Abstract (ZH)**: 基于稀疏视图的苹果树三维重建两阶段框架（DATR）：从复杂现场图像中实现高几何保真的树形重建 

---
# Inference of Human-derived Specifications of Object Placement via Demonstration 

**Title (ZH)**: 基于演示的人类衍生目标放置规范推断 

**Authors**: Alex Cuellar, Ho Chit Siu, Julie A Shah  

**Link**: [PDF](https://arxiv.org/pdf/2508.19367)  

**Abstract**: As robots' manipulation capabilities improve for pick-and-place tasks (e.g., object packing, sorting, and kitting), methods focused on understanding human-acceptable object configurations remain limited expressively with regard to capturing spatial relationships important to humans. To advance robotic understanding of human rules for object arrangement, we introduce positionally-augmented RCC (PARCC), a formal logic framework based on region connection calculus (RCC) for describing the relative position of objects in space. Additionally, we introduce an inference algorithm for learning PARCC specifications via demonstrations. Finally, we present the results from a human study, which demonstrate our framework's ability to capture a human's intended specification and the benefits of learning from demonstration approaches over human-provided specifications. 

**Abstract (ZH)**: 随着机器人在拾取和放置任务（例如物体包装、分类和配套）中的操作能力提升，专注于理解人类可接受的物体配置的方法在捕捉对人类重要的空间关系方面仍表达能力有限。为了促进机器人对物体排列人类规则的理解，我们引入了基于区域连接算术（RCC）的位置增强RCC（PARCC）形式逻辑框架，用于描述物体在空间中的相对位置。此外，我们引入了一种推理算法，通过演示学习PARCC规范。最后，我们展示了来自人类研究的结果，证明了该框架能够捕捉人类的预期规范，并且通过演示学习方法相较于人类提供的规范具有优势。 

---
# Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices 

**Title (ZH)**: 基于融合CNN网络的Patch Progression Masked Autoencoder二维OCT切片双pair进化分类方法 

**Authors**: Philippe Zhang, Weili Jiang, Yihao Li, Jing Zhang, Sarah Matta, Yubo Tan, Hui Lin, Haoshen Wang, Jiangtian Pan, Hui Xu, Laurent Borderie, Alexandre Le Guilcher, Béatrice Cochener, Chubin Ou, Gwenolé Quellec, Mathieu Lamard  

**Link**: [PDF](https://arxiv.org/pdf/2508.20064)  

**Abstract**: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments have been effective in slowing the progression of neovascular AMD, with better outcomes achieved through timely diagnosis and consistent monitoring. Tracking the progression of neovascular activity in OCT scans of patients with exudative AMD allows for the development of more personalized and effective treatment plans. This was the focus of the Monitoring Age-related Macular Degeneration Progression in Optical Coherence Tomography (MARIO) challenge, in which we participated. In Task 1, which involved classifying the evolution between two pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN network with model ensembling to further enhance the model's performance. For Task 2, which focused on predicting progression over the next three months based on current exam data, we proposed the Patch Progression Masked Autoencoder that generates an OCT for the next exam and then classifies the evolution between the current OCT and the one generated using our solution from Task 1. The results we achieved allowed us to place in the Top 10 for both tasks. Some team members are part of the same organization as the challenge organizers; therefore, we are not eligible to compete for the prize. 

**Abstract (ZH)**: 年龄相关黄斑变性（AMD）的 OCT 扫描中的病程监测：MARIO挑战任务分析与结果 

---
# WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution 

**Title (ZH)**: WaveHiT-SR：分层小波网络高效的图像超分辨率 

**Authors**: Fayaz Ali, Muhammad Zawish, Steven Davy, Radu Timofte  

**Link**: [PDF](https://arxiv.org/pdf/2508.19927)  

**Abstract**: Transformers have demonstrated promising performance in computer vision tasks, including image super-resolution (SR). The quadratic computational complexity of window self-attention mechanisms in many transformer-based SR methods forces the use of small, fixed windows, limiting the receptive field. In this paper, we propose a new approach by embedding the wavelet transform within a hierarchical transformer framework, called (WaveHiT-SR). First, using adaptive hierarchical windows instead of static small windows allows to capture features across different levels and greatly improve the ability to model long-range dependencies. Secondly, the proposed model utilizes wavelet transforms to decompose images into multiple frequency subbands, allowing the network to focus on both global and local features while preserving structural details. By progressively reconstructing high-resolution images through hierarchical processing, the network reduces computational complexity without sacrificing performance. The multi-level decomposition strategy enables the network to capture fine-grained information in lowfrequency components while enhancing high-frequency textures. Through extensive experimentation, we confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR results, achieving higher efficiency with fewer parameters, lower FLOPs, and faster speeds. 

**Abstract (ZH)**: WaveHiT-SR：嵌入小波变换的分层变压器超分辨率方法 

---
# Multispectral LiDAR data for extracting tree points in urban and suburban areas 

**Title (ZH)**: 多光谱LiDAR数据在城市和郊区间提取树木点云 

**Authors**: Narges Takhtkeshha, Gabriele Mazzacca, Fabio Remondino, Juha Hyyppä, Gottfried Mandlburger  

**Link**: [PDF](https://arxiv.org/pdf/2508.19881)  

**Abstract**: Monitoring urban tree dynamics is vital for supporting greening policies and reducing risks to electrical infrastructure. Airborne laser scanning has advanced large-scale tree management, but challenges remain due to complex urban environments and tree variability. Multispectral (MS) light detection and ranging (LiDAR) improves this by capturing both 3D spatial and spectral data, enabling detailed mapping. This study explores tree point extraction using MS-LiDAR and deep learning (DL) models. Three state-of-the-art models are evaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point Transformer V1 (PTv1). Results show the notable time efficiency and accuracy of SPT, with a mean intersection over union (mIoU) of 85.28%. The highest detection accuracy is achieved by incorporating pseudo normalized difference vegetation index (pNDVI) with spatial data, reducing error rate by 10.61 percentage points (pp) compared to using spatial information alone. These findings highlight the potential of MS-LiDAR and DL to improve tree extraction and further tree inventories. 

**Abstract (ZH)**: 基于多光谱LiDAR和深度学习的城市树木动态监测研究 

---
# ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images 

**Title (ZH)**: ERSR：一种基于椭圆约束的伪标签 refinement 和对称正则化框架用于超声图像胎头分割的半监督学习 

**Authors**: Linkuan Zhou, Zhexin Chen, Yufei Shen, Junlin Xu, Ping Xuan, Yixin Zhu, Yuqi Fang, Cong Cong, Leyi Wei, Ran Su, Jia Zhou, Qiangguo Jin  

**Link**: [PDF](https://arxiv.org/pdf/2508.19815)  

**Abstract**: Automated segmentation of the fetal head in ultrasound images is critical for prenatal monitoring. However, achieving robust segmentation remains challenging due to the poor quality of ultrasound images and the lack of annotated data. Semi-supervised methods alleviate the lack of annotated data but struggle with the unique characteristics of fetal head ultrasound images, making it challenging to generate reliable pseudo-labels and enforce effective consistency regularization constraints. To address this issue, we propose a novel semi-supervised framework, ERSR, for fetal head ultrasound segmentation. Our framework consists of the dual-scoring adaptive filtering strategy, the ellipse-constrained pseudo-label refinement, and the symmetry-based multiple consistency regularization. The dual-scoring adaptive filtering strategy uses boundary consistency and contour regularity criteria to evaluate and filter teacher outputs. The ellipse-constrained pseudo-label refinement refines these filtered outputs by fitting least-squares ellipses, which strengthens pixels near the center of the fitted ellipse and suppresses noise simultaneously. The symmetry-based multiple consistency regularization enforces multi-level consistency across perturbed images, symmetric regions, and between original predictions and pseudo-labels, enabling the model to capture robust and stable shape representations. Our method achieves state-of-the-art performance on two benchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36% with 10% and 20% labeled data, respectively. On the PSFH dataset, the scores are 91.68% and 93.70% under the same settings. 

**Abstract (ZH)**: 自动胎儿头部超声分割对于产前监测至关重要。然而，由于超声图像质量较差且缺乏标注数据，实现鲁棒分割仍具有挑战性。半监督方法虽能缓解标注数据不足的问题，但在处理胎儿头部超声图像的独特特性时却困难重重，难以生成可靠的伪标签并施加有效的一致性正则化约束。为解决这一问题，我们提出了一种新的半监督框架——ERSR，用于胎儿头部超声分割。该框架包括双评分自适应滤波策略、椭圆约束伪标签细化和基于对称性的多级一致性正则化。双评分自适应滤波策略使用边界一致性和轮廓规则性标准来评估和过滤教师输出。椭圆约束伪标签细化通过拟合最小二乘椭圆来细化这些过滤输出，从而使中心附近的像素增强，并同时抑制噪声。基于对称性的多级一致性正则化在扰动图像、对称区域以及原始预测和伪标签之间施加多级一致性约束，使模型能够捕捉到稳健且稳定的形状表征。我们的方法在两个基准上实现了最先进的性能。在其上，采用10%和20%标注数据时，HC18数据集的Dice分数分别为92.05%和95.36%。在同一设置下，PSFH数据集的分数分别为91.68%和93.70%。 

---
# A bag of tricks for real-time Mitotic Figure detection 

**Title (ZH)**: 实时有丝分裂图象检测的一系列技巧 

**Authors**: Christian Marzahl, Brian Napora  

**Link**: [PDF](https://arxiv.org/pdf/2508.19804)  

**Abstract**: Mitotic figure (MF) detection in histopathology images is challenging due to large variations in slide scanners, staining protocols, tissue types, and the presence of artifacts. This paper presents a collection of training techniques - a bag of tricks - that enable robust, real-time MF detection across diverse domains. We build on the efficient RTMDet single stage object detector to achieve high inference speed suitable for clinical deployment. Our method addresses scanner variability and tumor heterogeneity via extensive multi-domain training data, balanced sampling, and careful augmentation. Additionally, we employ targeted, hard negative mining on necrotic and debris tissue to reduce false positives. In a grouped 5-fold cross-validation across multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025 challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81, outperforming larger models and demonstrating adaptability to new, unfamiliar domains. The proposed solution offers a practical trade-off between accuracy and speed, making it attractive for real-world clinical adoption. 

**Abstract (ZH)**: 组织切片扫描仪、染色方案、组织类型和伪影的大量变化使得骨髓细胞检测在病理学图像中的实现颇具挑战。本文提出了一种训练技术集合——一系列技巧，以实现跨不同领域的稳定且实时的骨髓细胞检测。我们基于高效的RTMDet单阶段目标检测器，实现高速推断，适于临床部署。我们的方法通过广泛的多领域训练数据、均衡采样和谨慎的增强手段，解决了扫描仪间的差异性和肿瘤异质性问题。此外，我们还针对坏死和碎片组织实施了针对性的困难负样本挖掘，以降低假阳性率。在多个骨髓细胞数据集的分组5折交叉验证中，我们的模型获得了0.78至0.84的F1分数。在2025年MItosis DOmain Generalization (MIDOG) 挑战赛初步测试集中，基于单阶段RTMDet-S的方法达到0.81的F1分数，优于较大模型，并展示了对新且不熟悉的领域适应性的能力。所提解决方案提供了准确性和速度之间的实用权衡，使其适用于实际临床应用。 

---
# Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception 

**Title (ZH)**: 超越BEV：优化点级 tokens 的协同感知 

**Authors**: Yang Li, Quan Yuan, Guiyang Luo, Xiaoyuan Fu, Rui Pan, Yujia Yang, Congzhang Shao, Yuewen Liu, Jinglin Li  

**Link**: [PDF](https://arxiv.org/pdf/2508.19638)  

**Abstract**: Collaborative perception allows agents to enhance their perceptual capabilities by exchanging intermediate features. Existing methods typically organize these intermediate features as 2D bird's-eye-view (BEV) representations, which discard critical fine-grained 3D structural cues essential for accurate object recognition and localization. To this end, we first introduce point-level tokens as intermediate representations for collaborative perception. However, point-cloud data are inherently unordered, massive, and position-sensitive, making it challenging to produce compact and aligned point-level token sequences that preserve detailed structural information. Therefore, we present CoPLOT, a novel Collaborative perception framework that utilizes Point-Level Optimized Tokens. It incorporates a point-native processing pipeline, including token reordering, sequence modeling, and multi-agent spatial alignment. A semantic-aware token reordering module generates adaptive 1D reorderings by leveraging scene-level and token-level semantic information. A frequency-enhanced state space model captures long-range sequence dependencies across both spatial and spectral domains, improving the differentiation between foreground tokens and background clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop process, combining global agent-level correction with local token-level refinement to mitigate localization noise. Extensive experiments on both simulated and real-world datasets show that CoPLOT outperforms state-of-the-art models, with even lower communication and computation overhead. Code will be available at this https URL. 

**Abstract (ZH)**: 协作感知允许智能体通过交换中间特征来增强其感知能力。现有方法通常将这些中间特征组织为2D顶视图（BEV）表示，这会丢弃准确对象识别和定位所必需的关键细粒度三维结构线索。为此，我们首先引入点级令牌作为协作感知的中间表示。然而，点云数据本质上是无序的、大量的且位置敏感的，这使得生成能够保留详细结构信息的小型和对齐的点级令牌序列变得具有挑战性。因此，我们提出了CoPLOT，一种新的协作感知框架，利用点级优化令牌。CoPLOT整合了一个点本原处理流水线，包括令牌重排序、序列建模和多智能体空间对齐。一种语义感知的令牌重排序模块通过利用场景级和令牌级语义信息生成自适应的一维重排序。一种频率增强的状态空间模型捕获跨空间和频谱域的长程序列依赖性，提高前景令牌与背景杂乱区别的能力。最后，一个邻居到自身的对齐模块应用闭环过程，结合全局智能体级校正与局部令牌级细化来减轻定位噪声。在模拟和真实世界的数据集上的广泛实验表明，CoPLOT优于现有最先进的模型，且具有更低的通信和计算开销。代码将在如下链接处提供：this https URL。 

---
# Interact-Custom: Customized Human Object Interaction Image Generation 

**Title (ZH)**: Interact-Custom: 定制化的human-object交互图像生成 

**Authors**: Zhu Xu, Zhaowen Wang, Yuxin Peng, Yang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2508.19575)  

**Abstract**: Compositional Customized Image Generation aims to customize multiple target concepts within generation content, which has gained attention for its wild this http URL approaches mainly concentrate on the target entity's appearance preservation, while neglecting the fine-grained interaction control among target this http URL enable the model of such interaction control capability, we focus on human object interaction scenario and propose the task of Customized Human Object Interaction Image Generation(CHOI), which simultaneously requires identity preservation for target human object and the interaction semantic control between this http URL primary challenges exist for CHOI:(1)simultaneous identity preservation and interaction control demands require the model to decompose the human object into self-contained identity features and pose-oriented interaction features, while the current HOI image datasets fail to provide ideal samples for such feature-decomposed learning.(2)inappropriate spatial configuration between human and object may lead to the lack of desired interaction this http URL tackle it, we first process a large-scale dataset, where each sample encompasses the same pair of human object involving different interactive this http URL we design a two-stage model Interact-Custom, which firstly explicitly models the spatial configuration by generating a foreground mask depicting the interaction behavior, then under the guidance of this mask, we generate the target human object interacting while preserving their identities this http URL, if the background image and the union location of where the target human object should appear are provided by users, Interact-Custom also provides the optional functionality to specify them, offering high content controllability. Extensive experiments on our tailored metrics for CHOI task demonstrate the effectiveness of our approach. 

**Abstract (ZH)**: 组件化定制图像生成旨在生成内容中定制多个目标概念，这一领域因其潜力而受到关注。CHOI任务专注于人类物体交互场景，提出定制人类物体交互图像生成任务（CHOI），该任务同时要求保留目标人类物体的身份并在其交互语义之间进行控制。对于CHOI任务，主要存在两项挑战：（1）身份保留和交互控制的同时需求要求模型将人类物体分解为自我包含的身份特征和姿态导向的交互特征，而当前的HOI图像数据集未能提供理想的分解学习样本；（2）人类与物体之间不合适的空间配置可能导致期望的交互丢失。为应对这一挑战，我们首先处理了一个大规模的数据集，其中每个样本包含不同交互行为下的同一个人物体对。我们设计了一个两阶段模型Interact-Custom，首先通过生成描述交互行为的前景掩码显式建模空间配置，然后在掩码的指导下生成保留身份的目标人类物体的交互图像。如果用户提供背景图像和目标人类物体应出现的交集位置，Interact-Custom还提供了指定这些位置的可选功能，提供了高度的内容可控性。针对CHOI任务的大量实验表明了我们方法的有效性。 

---
# FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection 

**Title (ZH)**: FlowDet: 克服实时端到端交通检测中的视角和尺度挑战 

**Authors**: Yuhang Zhao, Zixing Wang  

**Link**: [PDF](https://arxiv.org/pdf/2508.19565)  

**Abstract**: End-to-end object detectors offer a promising NMS-free paradigm for real-time applications, yet their high computational cost remains a significant barrier, particularly for complex scenarios like intersection traffic monitoring. To address this challenge, we propose FlowDet, a high-speed detector featuring a decoupled encoder optimization strategy applied to the DETR architecture. Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to maintain high representational power across extreme scale variations. To rigorously evaluate the model's performance in environments with severe occlusion and high object density, we collected the Intersection-Flow-5k dataset, a new challenging scene for this task. Evaluated on Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by 1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference speed by 16.2%. Our work demonstrates a new path towards building highly efficient and accurate detectors for demanding, real-world perception systems. The Intersection-Flow-5k dataset is available at this https URL. 

**Abstract (ZH)**: FlowDet：一种用于交叉口交通监测的高速检测器 

---
# Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage 

**Title (ZH)**: 基于广播 footage 的计算机视觉人工智能球员跟踪软件的 concurrent validity 

**Authors**: Zachary L. Crang, Rich D. Johnston, Katie L. Mills, Johsan Billingham, Sam Robertson, Michael H. Cole, Jonathon Weakley, Adam Hewitt and, Grant M. Duthie  

**Link**: [PDF](https://arxiv.org/pdf/2508.19477)  

**Abstract**: This study aimed to: (1) understand whether commercially available computer-vision and artificial intelligence (AI) player tracking software can accurately measure player position, speed and distance using broadcast footage and (2) determine the impact of camera feed and resolution on accuracy. Data were obtained from one match at the 2022 Qatar Federation Internationale de Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds were used. Three commercial tracking providers that use computer-vision and AI participated. Providers analysed instantaneous position (x, y coordinates) and speed (m\,s^{-1}) of each player. Their data were compared with a high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to 16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across providers. Computer-vision and AI player tracking software offer the ability to track players with fair precision when players are detected by the software. Providers should use a tactical feed when tracking position and speed, which will maximise player detection, improving accuracy. Both 720p and 1080p resolutions are suitable, assuming appropriate computer-vision and AI models are implemented. 

**Abstract (ZH)**: 本研究旨在：（1）了解商用计算机视觉和人工智能（AI）球员追踪软件是否能够精确测量球员位置、速度和距离，使用的是转播 footage；（2）确定摄像机信号和分辨率对准确度的影响。数据来自2022年卡塔尔国际足球协会（FIFA）世界杯一场比赛。战术、节目和摄像机1信号被使用。三家使用计算机视觉和AI的商用追踪提供商参与其中。提供商分析了每位球员的即时位置（x、y坐标）和速度（m\s^{-1}）。随后将他们的数据与高清多摄像机追踪系统（TRACAB Gen 5）的数据进行了比较，计算了均方根误差（RMSE）和平均偏差。位置RMSE范围从1.68米到16.39米，速度RMSE范围从0.34米/秒到2.38米/秒。平均每场比赛距离的平均偏差范围从-1745米（-21.8%）到1945米（24.3%）不等。计算机视觉和AI球员追踪软件能够在软件检测到球员的情况下提供合理的追踪精度。提供商在追踪位置和速度时应使用战术信号，这将最大化球员检测，从而提高准确性。720p和1080p分辨率均适用，前提是使用适当的计算机视觉和AI模型。 

---
# Automated classification of natural habitats using ground-level imagery 

**Title (ZH)**: 基于地面图像的自然栖息地自动分类 

**Authors**: Mahdis Tourian, Sareh Rowlands, Remy Vandaele, Max Fancourt, Rebecca Mein, Hywel T. P. Williams  

**Link**: [PDF](https://arxiv.org/pdf/2508.19314)  

**Abstract**: Accurate classification of terrestrial habitats is critical for biodiversity conservation, ecological monitoring, and land-use planning. Several habitat classification schemes are in use, typically based on analysis of satellite imagery with validation by field ecologists. Here we present a methodology for classification of habitats based solely on ground-level imagery (photographs), offering improved validation and the ability to classify habitats at scale (for example using citizen-science imagery). In collaboration with Natural England, a public sector organisation responsible for nature conservation in England, this study develops a classification system that applies deep learning to ground-level habitat photographs, categorising each image into one of 18 classes defined by the 'Living England' framework. Images were pre-processed using resizing, normalisation, and augmentation; re-sampling was used to balance classes in the training data and enhance model robustness. We developed and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label to each photograph. Using five-fold cross-validation, the model demonstrated strong overall performance across 18 habitat classes, with accuracy and F1-scores varying between classes. Across all folds, the model achieved a mean F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or ambiguous classes scoring lower. These findings demonstrate the potential of this approach for ecological monitoring. Ground-level imagery is readily obtained, and accurate computational methods for habitat classification based on such data have many potential applications. To support use by practitioners, we also provide a simple web application that classifies uploaded images using our model. 

**Abstract (ZH)**: 基于地面图像的植被类型准确分类对于生物多样性保护、生态监测和土地利用规划至关重要。多种植被分类方案正在使用，通常基于卫星影像分析，并通过实地生态学家的验证。本文提出了仅基于地面图像（照片）进行分类的方法，提供了改进的验证能力，并能够大规模分类植被（例如使用公民科学照片）。与负责英格兰自然保育的公共部门组织自然英格兰合作，本研究开发了一种基于深度学习的地面植被照片分类系统，将每张图像分类为“Living England”框架定义的18个类别之一。图像进行了预处理，包括调整大小、标准化和增强；通过对训练数据采样以平衡类别并增强模型的鲁棒性。我们开发并微调了DeepLabV3-ResNet101分类器，为每张照片分配一个植被类别标签。通过五折交叉验证，该模型在18个植被类别中总体表现强劲，不同类别的准确率和F1得分各不相同。所有折上，模型的平均F1得分为0.61，视觉上明显的植被类型如裸土、淤泥和泥炭（BSSP）和裸沙（BS）达到了高于0.90的值，而混合或模糊的类别得分较低。这些发现展示了此方法在生态监测中的潜力。地面图像易于获得，基于此类数据的准确计算方法在许多应用中都有很大潜力。为了支持实践者的使用，我们还提供了一个简单的网络应用，用于使用我们的模型对上传的图像进行分类。 

---
# 2D Ultrasound Elasticity Imaging of Abdominal Aortic Aneurysms Using Deep Neural Networks 

**Title (ZH)**: 使用深度神经网络的腹部主动脉瘤二维超声弹性成像 

**Authors**: Utsav Ratna Tuladhar, Richard Simon, Doran Mix, Michael Richards  

**Link**: [PDF](https://arxiv.org/pdf/2508.19303)  

**Abstract**: Abdominal aortic aneurysms (AAA) pose a significant clinical risk due to their potential for rupture, which is often asymptomatic but can be fatal. Although maximum diameter is commonly used for risk assessment, diameter alone is insufficient as it does not capture the properties of the underlying material of the vessel wall, which play a critical role in determining the risk of rupture. To overcome this limitation, we propose a deep learning-based framework for elasticity imaging of AAAs with 2D ultrasound. Leveraging finite element simulations, we generate a diverse dataset of displacement fields with their corresponding modulus distributions. We train a model with U-Net architecture and normalized mean squared error (NMSE) to infer the spatial modulus distribution from the axial and lateral components of the displacement fields. This model is evaluated across three experimental domains: digital phantom data from 3D COMSOL simulations, physical phantom experiments using biomechanically distinct vessel models, and clinical ultrasound exams from AAA patients. Our simulated results demonstrate that the proposed deep learning model is able to reconstruct modulus distributions, achieving an NMSE score of 0.73\%. Similarly, in phantom data, the predicted modular ratio closely matches the expected values, affirming the model's ability to generalize to phantom data. We compare our approach with an iterative method which shows comparable performance but higher computation time. In contrast, the deep learning method can provide quick and effective estimates of tissue stiffness from ultrasound images, which could help assess the risk of AAA rupture without invasive procedures. 

**Abstract (ZH)**: 基于深度学习的二维超声腹主动脉瘤弹性成像框架 

---
# DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models 

**Title (ZH)**: DemoBias: 跟踪视觉基础模型中的人口统计偏见的实证研究 

**Authors**: Abu Sufian, Anirudha Ghosh, Debaditya Barman, Marco Leo, Cosimo Distante  

**Link**: [PDF](https://arxiv.org/pdf/2508.19298)  

**Abstract**: Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities across various downstream tasks, including biometric face recognition (FR) with description. However, demographic biases remain a critical concern in FR, as these foundation models often fail to perform equitably across diverse demographic groups, considering ethnicity/race, gender, and age. Therefore, through our work DemoBias, we conduct an empirical evaluation to investigate the extent of demographic biases in LVLMs for biometric FR with textual token generation tasks. We fine-tuned and evaluated three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own generated demographic-balanced dataset. We utilize several evaluation metrics, like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify and trace the performance disparities. The experimental results deliver compelling insights into the fairness and reliability of LVLMs across diverse demographic groups. Our empirical study uncovered demographic biases in LVLMs, with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino, Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably consistent. Repository: this https URL. 

**Abstract (ZH)**: 大规模视觉语言模型（LVLMs）在各种下游任务中展示了显著的能力，包括带有描述的生物特征面部识别（FR）。然而，人口统计学偏见仍然是FR中的一个关键问题，因为这些基础模型往往在不同的族群群体中表现不公，考虑族裔/种族、性别和年龄。因此，通过我们的工作DemoBias，我们进行了一项实证评估，以调查LVLMs在生物特征FR中的人口统计学偏见程度，特别是在带有文本标记生成任务的情况下。我们使用了一个自己生成的人口统计学平衡数据集对三个广泛使用的预训练LVLMs：LLaVA、BLIP-2和PaliGemma进行了微调和评估。我们使用多种评估指标，如群体特定的BERTScores和公平性差距率，来量化和追踪性能差异。实验结果提供了关于LVLMs在不同人口统计学群体中的公平性和可靠性的深刻见解。我们的实证研究发现了LVLMs中的人口统计学偏见，其中PaliGemma和LLaVA在西班牙裔/拉丁裔、 Caucasian 和南亚群体中表现出更高的差异，而BLIP-2则表现出相对一致的性能。仓库：this https URL 

---
# Object Detection with Multimodal Large Vision-Language Models: An In-depth Review 

**Title (ZH)**: 基于多模态大型视觉-语言模型的目标检测：深入综述 

**Authors**: Ranjan Sapkota, Manoj Karkee  

**Link**: [PDF](https://arxiv.org/pdf/2508.19294)  

**Abstract**: The fusion of language and vision in large vision-language models (LVLMs) has revolutionized deep learning-based object detection by enhancing adaptability, contextual reasoning, and generalization beyond traditional architectures. This in-depth review presents a structured exploration of the state-of-the-art in LVLMs, systematically organized through a three-step research review process. First, we discuss the functioning of vision language models (VLMs) for object detection, describing how these models harness natural language processing (NLP) and computer vision (CV) techniques to revolutionize object detection and localization. We then explain the architectural innovations, training paradigms, and output flexibility of recent LVLMs for object detection, highlighting how they achieve advanced contextual understanding for object detection. The review thoroughly examines the approaches used in integration of visual and textual information, demonstrating the progress made in object detection using VLMs that facilitate more sophisticated object detection and localization strategies. This review presents comprehensive visualizations demonstrating LVLMs' effectiveness in diverse scenarios including localization and segmentation, and then compares their real-time performance, adaptability, and complexity to traditional deep learning systems. Based on the review, its is expected that LVLMs will soon meet or surpass the performance of conventional methods in object detection. The review also identifies a few major limitations of the current LVLM modes, proposes solutions to address those challenges, and presents a clear roadmap for the future advancement in this field. We conclude, based on this study, that the recent advancement in LVLMs have made and will continue to make a transformative impact on object detection and robotic applications in the future. 

**Abstract (ZH)**: 大型视觉语言模型中语言与视觉的融合 revolutionized 基于深度学习的目标检测，提高了传统架构之外的适应性、上下文推理和泛化能力。本深入综述通过三步研究评审过程系统地探讨了大型视觉语言模型的最新进展，首先讨论了视觉语言模型（VLMs）在目标检测中的功能，阐述了这些模型如何利用自然语言处理（NLP）和计算机视觉（CV）技术革新目标检测和定位。随后解释了最近的大型视觉语言模型在目标检测方面的架构创新、训练范式和输出灵活性，突显了它们如何实现高级的上下文理解。该综述详细分析了视觉和文本信息集成的方法，展示了使用VLMs进行目标检测取得的进步，促进了更复杂的物体检测与定位策略。该综述还提供了全面的可视化演示，展示了大型视觉语言模型在包括定位和分割在内的各种场景中的有效性，并将其实时性能、适应性和复杂性与传统深度学习系统进行了比较。基于此综述，预计大型视觉语言模型不久将在目标检测中达到或超过传统方法的性能。此外，该综述还指出了当前大型视觉语言模型的几个主要局限性，提出了应对这些挑战的解决方案，并指明了该领域未来发展的清晰路径。本综述基于这一研究得出结论，认为近期在大型视觉语言模型方面的进展及其未来将进一步革新目标检测和机器人应用。 

---
# Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation 

**Title (ZH)**: 基于模型的有效对抗攻击 LidAR 分割净化方法 

**Authors**: Alexandros Gkillas, Ioulia Kapsali, Nikos Piperigkos, Aris S. Lalos  

**Link**: [PDF](https://arxiv.org/pdf/2508.19290)  

**Abstract**: LiDAR-based segmentation is essential for reliable perception in autonomous vehicles, yet modern segmentation networks are highly susceptible to adversarial attacks that can compromise safety. Most existing defenses are designed for networks operating directly on raw 3D point clouds and rely on large, computationally intensive generative models. However, many state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D range view representations. Despite their widespread adoption, dedicated lightweight adversarial defenses for this domain remain largely unexplored. We introduce an efficient model-based purification framework tailored for adversarial defense in 2D range-view LiDAR segmentation. We propose a direct attack formulation in the range-view domain and develop an explainable purification network based on a mathematical justified optimization problem, achieving strong adversarial resilience with minimal computational overhead. Our method achieves competitive performance on open benchmarks, consistently outperforming generative and adversarial training baselines. More importantly, real-world deployment on a demo vehicle demonstrates the framework's ability to deliver accurate operation in practical autonomous driving scenarios. 

**Abstract (ZH)**: 基于LiDAR的分割对于自动驾驶车辆的可靠感知至关重要，但现代分割网络极易受到可能危及安全的 adversarial 攻击。现有的大多数防护措施针对的是直接操作原始 3D 点云的网络，并依赖于大型且计算密集的生成模型。然而，许多最先进的 LiDAR 分割流水线在更高效的 2D 范围图表示上操作。尽管这类表示被广泛应用，但针对该领域的专用轻量级 adversarial 防护措施仍鲜有研究。我们提出了一种针对 2D 范围图 LiDAR 分割的高效模型导向净化框架，用于 adversarial 防护。我们在范围图域中提出了一个直接攻击形式，并基于数学证明的优化问题开发了一个可解释的净化网络，实现了强 adversarial 抗性并具有最小的计算开销。我们的方法在开放基准测试中表现出竞争性性能，一致优于生成性和对抗性训练基线。更重要的是，在演示车辆上的实际部署展示了该框架在实际自动驾驶场景中提供准确操作的能力。 

---
# Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration 

**Title (ZH)**: 实时直观AI绘画系统：通过形式化和情境意图整合增强人类创造力 

**Authors**: Jookyung Song, Mookyoung Kang, Nojun Kwak  

**Link**: [PDF](https://arxiv.org/pdf/2508.19254)  

**Abstract**: This paper presents a real-time generative drawing system that interprets and integrates both formal intent - the structural, compositional, and stylistic attributes of a sketch - and contextual intent - the semantic and thematic meaning inferred from its visual content - into a unified transformation process. Unlike conventional text-prompt-based generative systems, which primarily capture high-level contextual descriptions, our approach simultaneously analyzes ground-level intuitive geometric features such as line trajectories, proportions, and spatial arrangement, and high-level semantic cues extracted via vision-language models. These dual intent signals are jointly conditioned in a multi-stage generation pipeline that combines contour-preserving structural control with style- and content-aware image synthesis. Implemented with a touchscreen-based interface and distributed inference architecture, the system achieves low-latency, two-stage transformation while supporting multi-user collaboration on shared canvases. The resulting platform enables participants, regardless of artistic expertise, to engage in synchronous, co-authored visual creation, redefining human-AI interaction as a process of co-creation and mutual enhancement. 

**Abstract (ZH)**: 本文提出了一种即时生成绘图系统，该系统将形式意图（素描的结构、构图和风格属性）和语境意图（通过其视觉内容推断的语义和主题意义）统一到一个转换过程中。与主要捕捉高层次语境描述的传统基于文本提示的生成系统不同，我们的方法同时分析了低层次的直观几何特征（如线迹、比例和空间排列），以及通过视觉语言模型提取的高层次语义线索。这些双重意图信号在结合了轮廓保持结构控制和风格及内容感知图像合成的多阶段生成管道中共同条件化。该系统通过基于触屏的界面和分布式推理架构，实现了低延迟的两阶段转换，在共享画布上支持多用户协作。该平台使参与者（无论是否有艺术技能）能够进行同步、共同创作的视觉创作，重新定义了人机交互作为共创和互促的过程。 

---
