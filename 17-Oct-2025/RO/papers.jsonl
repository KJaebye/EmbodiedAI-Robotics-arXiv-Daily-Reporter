{'arxiv_id': 'arXiv:2510.14968', 'title': 'RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks', 'authors': 'Mingxuan Yan, Yuping Wang, Zechun Liu, Jiachen Li', 'link': 'https://arxiv.org/abs/2510.14968', 'abstract': 'To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at this http URL.', 'abstract_zh': '基于检索的演示分解器（RDD）：一种自动将演示分解为子任务的方法，通过与低 level 视觉运动策略训练数据的视觉特征对齐来实现。', 'title_zh': '基于检索的演示分解器：长时 horizon 任务规划者对齐的检索基表示示分解器'}
{'arxiv_id': 'arXiv:2510.14959', 'title': 'CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions', 'authors': 'Lizhi Yang, Blake Werner, Massimiliano de Sa Aaron D. Ames', 'link': 'https://arxiv.org/abs/2510.14959', 'abstract': 'Reinforcement learning (RL), while powerful and expressive, can often prioritize performance at the expense of safety. Yet safety violations can lead to catastrophic outcomes in real-world deployments. Control Barrier Functions (CBFs) offer a principled method to enforce dynamic safety -- traditionally deployed \\emph{online} via safety filters. While the result is safe behavior, the fact that the RL policy does not have knowledge of the CBF can lead to conservative behaviors. This paper proposes CBF-RL, a framework for generating safe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key attributes: (1) minimally modifying a nominal RL policy to encode safety constraints via a CBF term, (2) and safety filtering of the policy rollouts in training. Theoretically, we prove that continuous-time safety filters can be deployed via closed-form expressions on discrete-time roll-outs. Practically, we demonstrate that CBF-RL internalizes the safety constraints in the learned policy -- both enforcing safer actions and biasing towards safer rewards -- enabling safe deployment without the need for an online safety filter. We validate our framework through ablation studies on navigation tasks and on the Unitree G1 humanoid robot, where CBF-RL enables safer exploration, faster convergence, and robust performance under uncertainty, enabling the humanoid robot to avoid obstacles and climb stairs safely in real-world settings without a runtime safety filter.', 'abstract_zh': '基于控制障碍函数的强化学习（CBF-RL）', 'title_zh': 'CBF-RL：使用控制 barrier 函数进行训练的安全过滤强化学习'}
{'arxiv_id': 'arXiv:2510.14952', 'title': 'From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance', 'authors': 'Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Yibo Peng, Tao Huang, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang, Chang Xu', 'link': 'https://arxiv.org/abs/2510.14952', 'abstract': 'Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and unreliable. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking accuracy, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a general foundation for vision-language-action humanoid systems.', 'abstract_zh': '无需重定位的基于语言的类人机器人运动框架：RoboGhost', 'title_zh': '从语言到运动：基于运动潜在指导的无靶向人体控制'}
{'arxiv_id': 'arXiv:2510.14947', 'title': 'Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion', 'authors': 'Blake Werner, Lizhi Yang, Aaron D. Ames', 'link': 'https://arxiv.org/abs/2510.14947', 'abstract': 'Robust humanoid locomotion in unstructured environments requires architectures that balance fast low-level stabilization with slower perceptual decision-making. We show that a simple layered control architecture (LCA), a proprioceptive stabilizer running at high rate, coupled with a compact low-rate perceptual policy, enables substantially more robust performance than monolithic end-to-end designs, even when using minimal perception encoders. Through a two-stage training curriculum (blind stabilizer pretraining followed by perceptual fine-tuning), we demonstrate that layered policies consistently outperform one-stage alternatives in both simulation and hardware. On a Unitree G1 humanoid, our approach succeeds across stair and ledge tasks where one-stage perceptual policies fail. These results highlight that architectural separation of timescales, rather than network scale or complexity, is the key enabler for robust perception-conditioned locomotion.', 'abstract_zh': '具starttime灵活环境的人形机器人稳健运动需要平衡快速低级稳定性和缓慢感知决策的架构。我们展示了简单的分层控制架构（LCA），即以高频率运行的本体感受稳定器与紧凑的低频感知策略相结合，使性能显著增强，即使使用最少的感知编码器也是如此。通过两阶段训练课程（盲稳定器预训练后进行感知微调），我们证明分层策略在仿真和硬件中均优于单阶段替代方案。在Unitree G1人形机器人上，我们的方法在单阶段感知策略失败的台阶和凸起任务中均获得成功。这些结果表明，时间尺度上的架构分离而非网络规模或复杂性是实现稳健感知条件下的运动的关键使能因素。', 'title_zh': '所见即所得：多样性增强的稳健人形机器人行走甜点区域'}
{'arxiv_id': 'arXiv:2510.14930', 'title': 'VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin', 'authors': "Binghao Huang, Jie Xu, Iretiayo Akinola, Wei Yang, Balakumar Sundaralingam, Rowland O'Flaherty, Dieter Fox, Xiaolong Wang, Arsalan Mousavian, Yu-Wei Chao, Yunzhu Li", 'link': 'https://arxiv.org/abs/2510.14930', 'abstract': 'Humans excel at bimanual assembly tasks by adapting to rich tactile feedback -- a capability that remains difficult to replicate in robots through behavioral cloning alone, due to the suboptimality and limited diversity of human demonstrations. In this work, we present VT-Refine, a visuo-tactile policy learning framework that combines real-world demonstrations, high-fidelity tactile simulation, and reinforcement learning to tackle precise, contact-rich bimanual assembly. We begin by training a diffusion policy on a small set of demonstrations using synchronized visual and tactile inputs. This policy is then transferred to a simulated digital twin equipped with simulated tactile sensors and further refined via large-scale reinforcement learning to enhance robustness and generalization. To enable accurate sim-to-real transfer, we leverage high-resolution piezoresistive tactile sensors that provide normal force signals and can be realistically modeled in parallel using GPU-accelerated simulation. Experimental results show that VT-Refine improves assembly performance in both simulation and the real world by increasing data diversity and enabling more effective policy fine-tuning. Our project page is available at this https URL.', 'abstract_zh': '人类在双臂装配任务中通过适应丰富的触觉反馈表现出色——这一能力仅通过行为克隆难以在机器人上复制，因为人类示范的不足和有限多样性。本文提出了一种结合实际示范、高保真触觉模拟和强化学习的视触觉策略学习框架VT-Refine，以应对精确的、接触丰富的双臂装配任务。我们首先使用同步的视觉和触觉输入对一个小规模示范集进行扩散策略训练。然后，将该策略转移到配备了模拟触觉传感器的模拟数字双体内，并通过大规模强化学习进一步精炼，以提高鲁棒性和泛化能力。为了使仿真实际转移更加准确，我们利用高分辨率压阻式触觉传感器，该传感器提供了法向力信号，并可通过GPU加速模拟并进行现实建模。实验结果表明，VT-Refine通过增加数据多样性并使策略微调更加有效，在仿真和现实世界中均提高了装配性能。我们的项目页面可访问此链接：这个 https URL。', 'title_zh': 'VT-Refine: 通过仿真微调学习双臂装配的视觉-触觉反馈方法'}
{'arxiv_id': 'arXiv:2510.14902', 'title': 'VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation', 'authors': 'Han Zhao, Jiaxuan Zhang, Wenxuan Song, Pengxiang Ding, Donglin Wang', 'link': 'https://arxiv.org/abs/2510.14902', 'abstract': 'Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: this https URL.', 'abstract_zh': '当前的视觉-语言-动作（VLA）模型，基于大规模机器人数据预训练，展示了强大的多任务能力，并且在操作任务中能够很好地泛化到视觉和语言指令的变化。然而，当面对训练数据之外的对象概念，如未见过的对象描述和数据集中未见的纹理时，其成功率显著下降。为了解决这一问题，我们提出了一种新的自主框架VLA^2，该框架以OpenVLA作为执行骨干，并有效利用网页检索和目标检测等外部模块，向VLA提供目标对象的视觉和文本知识，从而缓解处理分布外对象时的泛化失败问题。基于LIBERO模拟环境，我们引入了新的对象和对象描述，构建了一个具有三个难度级别的新评估基准，以测试我们方法的有效性。我们的框架在我们设计的高难度泛化基准上成功超越了当前的最先进模型。与独立的OpenVLA基线相比，VLA^2在高难度基准上的成功率提高了44.2%，在所有定制环境中平均提高了20.2%，且不影响领域内任务的性能。项目网址：this https URL。', 'title_zh': 'VLA^2：为不可见概念操控赋能的代理框架视觉-语言-动作模型'}
{'arxiv_id': 'arXiv:2510.14893', 'title': 'STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search', 'authors': 'Helene J. Levy, Brett T. Lopez', 'link': 'https://arxiv.org/abs/2510.14893', 'abstract': 'Autonomous high-speed navigation through large, complex environments requires real-time generation of agile trajectories that are dynamically feasible, collision-free, and satisfy state or actuator constraints. Modern trajectory planning techniques primarily use numerical optimization, as they enable the systematic computation of high-quality, expressive trajectories that satisfy various constraints. However, stringent requirements on computation time and the risk of numerical instability can limit the use of optimization-based planners in safety-critical scenarios. This work presents an optimization-free planning framework called STITCHER that stitches short trajectory segments together with graph search to compute long-range, expressive, and near-optimal trajectories in real-time. STITCHER outperforms modern optimization-based planners through our innovative planning architecture and several algorithmic developments that make real-time planning possible. Extensive simulation testing is performed to analyze the algorithmic components that make up STITCHER, along with a thorough comparison with two state-of-the-art optimization planners. Simulation tests show that safe trajectories can be created within a few milliseconds for paths that span the entirety of two 50 m x 50 m environments. Hardware tests with a custom quadrotor verify that STITCHER can produce trackable paths in real-time while respecting nonconvex constraints, such as limits on tilt angle and motor forces, which are otherwise hard to include in optimization-based planners.', 'abstract_zh': '自主高速导航通过大型复杂环境需要实时生成敏捷、动态可行、无碰撞且满足状态或执行器约束的轨迹。STITCHER：一种基于图搜索的无优化实时轨迹规划框架及其性能分析', 'title_zh': 'Stitcher: 有限环境中的实时运动元搜索受限轨迹规划'}
{'arxiv_id': 'arXiv:2510.14851', 'title': 'SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time', 'authors': 'Jakob Bichler, Andreu Matoses Gimenez, Javier Alonso-Mora', 'link': 'https://arxiv.org/abs/2510.14851', 'abstract': "We present Sadcher, a real-time task assignment framework for heterogeneous multi-robot teams that incorporates dynamic coalition formation and task precedence constraints. Sadcher is trained through Imitation Learning and combines graph attention and transformers to predict assignment rewards between robots and tasks. Based on the predicted rewards, a relaxed bipartite matching step generates high-quality schedules with feasibility guarantees. We explicitly model robot and task positions, task durations, and robots' remaining processing times, enabling advanced temporal and spatial reasoning and generalization to environments with different spatiotemporal distributions compared to training. Trained on optimally solved small-scale instances, our method can scale to larger task sets and team sizes. Sadcher outperforms other learning-based and heuristic baselines on randomized, unseen problems for small and medium-sized teams with computation times suitable for real-time operation. We also explore sampling-based variants and evaluate scalability across robot and task counts. In addition, we release our dataset of 250,000 optimal schedules: this https URL", 'abstract_zh': '我们提出Sadcher——一种结合动态联盟形成和任务优先级约束的异构多机器人团队实时任务分配框架。Sadcher通过模仿学习训练，并结合图注意力和变压器来预测机器人与任务之间的分配奖励。基于预测的奖励，一个松弛的二部图匹配步骤生成具有可行性保证的高质量调度。我们明确建模了机器人和任务的位置、任务持续时间和剩余处理时间，从而实现高级的时间和空间推理，并能够推广到具有不同时空分布的环境。在最优解的小规模实例上训练，我们的方法可以扩展到更大的任务集和团队规模。在计算时间适合实时操作的情况下，Sadcher在随机未见过的小规模和中规模机器人的问题上优于其他基于学习和启发式的基线方法。我们还探索了采样变体，并评估了其在机器人和任务数量上的扩展性。此外，我们发布了包含250,000个最优调度的数据集：https://....', 'title_zh': 'SADCHER：基于注意力动态异构机器人联盟的实时调度'}
{'arxiv_id': 'arXiv:2510.14849', 'title': 'Multi Agent Switching Mode Controller for Sound Source localization', 'authors': 'Marcello Sorge, Nicola Cigarini, Riccardo Lorigiola, Giulia Michieletto, Andrea Masiero, Angelo Cenedese, Alberto Guarnieri', 'link': 'https://arxiv.org/abs/2510.14849', 'abstract': 'Source seeking is an important topic in robotic research, especially considering sound-based sensors since they allow the agents to locate a target even in critical conditions where it is not possible to establish a direct line of sight. In this work, we design a multi- agent switching mode control strategy for acoustic-based target localization. Two scenarios are considered: single source localization, in which the agents are driven maintaining a rigid formation towards the target, and multi-source scenario, in which each agent searches for the targets independently from the others.', 'abstract_zh': '基于声源的多agent切换模式目标定位研究', 'title_zh': '多代理切换模式控制器在声源定位中的应用'}
{'arxiv_id': 'arXiv:2510.14830', 'title': 'RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning', 'authors': 'Kun Lei, Huanyu Li, Dongjie Yu, Zhenyu Wei, Lingxiao Guo, Zhennan Jiang, Ziyu Wang, Shiyu Liang, Huazhe Xu', 'link': 'https://arxiv.org/abs/2510.14830', 'abstract': 'Real-world robotic manipulation in homes and factories demands reliability, efficiency, and robustness that approach or surpass skilled human operators. We present RL-100, a real-world reinforcement learning training framework built on diffusion visuomotor policies trained bu supervised learning. RL-100 introduces a three-stage pipeline. First, imitation learning leverages human priors. Second, iterative offline reinforcement learning uses an Offline Policy Evaluation procedure, abbreviated OPE, to gate PPO-style updates that are applied in the denoising process for conservative and reliable improvement. Third, online reinforcement learning eliminates residual failure modes. An additional lightweight consistency distillation head compresses the multi-step sampling process in diffusion into a single-step policy, enabling high-frequency control with an order-of-magnitude reduction in latency while preserving task performance. The framework is task-, embodiment-, and representation-agnostic and supports both 3D point clouds and 2D RGB inputs, a variety of robot platforms, and both single-step and action-chunk policies. We evaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control, such as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth folding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100 attains 100\\% success across evaluated trials for a total of 900 out of 900 episodes, including up to 250 out of 250 consecutive trials on one task. The method achieves near-human teleoperation or better time efficiency and demonstrates multi-hour robustness with uninterrupted operation lasting up to two hours.', 'abstract_zh': '面向家庭和工厂的真实世界机器人操作需求可靠性、效率和 robustness 接近或超越熟练的人类操作者：RL-100 实验室强化学习训练框架', 'title_zh': 'RL-100: 实用的现实世界强化学习操控机器人'}
{'arxiv_id': 'arXiv:2510.14827', 'title': 'Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping', 'authors': 'Yufei Zhu, Shih-Min Yang, Andrey Rudenko, Tomasz P. Kucner, Achim J. Lilienthal, Martin Magnusson', 'link': 'https://arxiv.org/abs/2510.14827', 'abstract': 'Safe and efficient robot operation in complex human environments can benefit from good models of site-specific motion patterns. Maps of Dynamics (MoDs) provide such models by encoding statistical motion patterns in a map, but existing representations use discrete spatial sampling and typically require costly offline construction. We propose a continuous spatio-temporal MoD representation based on implicit neural functions that directly map coordinates to the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the need for discretization and imputation for unevenly sampled regions, enabling smooth generalization across both space and time. Evaluated on a large public dataset with long-term real-world people tracking data, our method achieves better accuracy of motion representation and smoother velocity distributions in sparse regions while still being computationally efficient, compared to available baselines. The proposed approach demonstrates a powerful and efficient way of modeling complex human motion patterns.', 'abstract_zh': '复杂人类环境中的安全高效机器人操作可以从特定场地的运动模式模型中受益。我们提出了一种基于隐式神经函数的连续时空MoD表示，该表示直接将坐标映射到半包卷混合模型的参数上，从而消除了离散化和不均匀采样区域的插补需求，能够在空间和时间上实现平滑泛化。在大型公共数据集上的评估结果表明，与现有基线方法相比，该方法在稀疏区域实现了更好的运动表示准确性和更平滑的速度分布，同时保持了计算效率。提出的 approach 展示了一种强大而高效的复杂人类运动模式建模方式。', 'title_zh': '时空运动映射的神经隐式流场'}
{'arxiv_id': 'arXiv:2510.14783', 'title': 'SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning', 'authors': 'Aderik Verraest, Stavrow Bahnam, Robin Ferede, Guido de Croon, Christophe De Wagter', 'link': 'https://arxiv.org/abs/2510.14783', 'abstract': "Autonomous drone racing (ADR) systems have recently achieved champion-level performance, yet remain highly specific to drone racing. While end-to-end vision-based methods promise broader applicability, no system to date simultaneously achieves full sim-to-real transfer, onboard execution, and champion-level performance. In this work, we present SkyDreamer, to the best of our knowledge, the first end-to-end vision-based ADR policy that maps directly from pixel-level representations to motor commands. SkyDreamer builds on informed Dreamer, a model-based reinforcement learning approach where the world model decodes to privileged information only available during training. By extending this concept to end-to-end vision-based ADR, the world model effectively functions as an implicit state and parameter estimator, greatly improving interpretability. SkyDreamer runs fully onboard without external aid, resolves visual ambiguities by tracking progress using the state decoded from the world model's hidden state, and requires no extrinsic camera calibration, enabling rapid deployment across different drones without retraining. Real-world experiments show that SkyDreamer achieves robust, high-speed flight, executing tight maneuvers such as an inverted loop, a split-S and a ladder, reaching speeds of up to 21 m/s and accelerations of up to 6 g. It further demonstrates a non-trivial visual sim-to-real transfer by operating on poor-quality segmentation masks, and exhibits robustness to battery depletion by accurately estimating the maximum attainable motor RPM and adjusting its flight path in real-time. These results highlight SkyDreamer's adaptability to important aspects of the reality gap, bringing robustness while still achieving extremely high-speed, agile flight.", 'abstract_zh': '自主无人机竞速（ADR）系统 recently 已经达到了冠军级别的性能，但仍高度特化于无人机竞速。虽然端到端基于视觉的方法有望实现更广泛的应用，但至今为止还没有一个系统能够同时实现完整的仿真到真实世界的转移、板载执行和冠军级性能。在本工作中，我们提出了 SkyDreamer，据我们所知，这是第一个端到端基于视觉的 ADR 策略，能够直接从像素级表示映射到电机命令。SkyDreamer 以有信息量的 Dreamer 为基础，这是一种基于模型的强化学习方法，其中世界模型仅在训练期间才解码特权信息。通过将这一概念扩展到端到端基于视觉的 ADR 中，世界模型有效地充当了隐式状态和参数估算器，极大地提高了可解释性。SkyDreamer 完全在板载运行，不依赖外部帮助，通过跟踪由世界模型隐状态解码的状态来解决视觉模糊性，且无需额外的相机校准，从而可以在不同的无人机上快速部署而无需重新训练。实验证明，SkyDreamer 实现了稳健而高速的飞行，执行诸如倒立环、分割 S 和梯子等精确机动，达到了最大速度 21 m/s 和最大加速度 6 g。此外，SkyDreamer 进一步展示了在质量较差的分割掩模上进行非平凡的仿真到现实世界转换的能力，并通过准确估计最大可实现的电机 RPM 并实时调整飞行路径展示了对电池耗尽的鲁棒性。这些结果突显了 SkyDreamer 在现实差距关键方面的适应性，同时仍然实现了极其高速和敏捷的飞行。', 'title_zh': 'SkyDreamer：基于模型的强化学习端到端可解释无人机竞速'}
{'arxiv_id': 'arXiv:2510.14771', 'title': 'Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation', 'authors': 'Xu Chi, Chao Zhang, Yang Su, Lingfeng Dou, Fujia Yang, Jiakuo Zhao, Haoyu Zhou, Xiaoyou Jia, Yong Zhou, Shan An', 'link': 'https://arxiv.org/abs/2510.14771', 'abstract': 'Accurate and high-fidelity demonstration data acquisition is a critical bottleneck for deploying robot Imitation Learning (IL) systems, particularly when dealing with heterogeneous robotic platforms. Existing teleoperation systems often fail to guarantee high-precision data collection across diverse types of teleoperation devices. To address this, we developed Open TeleDex, a unified teleoperation framework engineered for demonstration data collection. Open TeleDex specifically tackles the TripleAny challenge, seamlessly supporting any robotic arm, any dexterous hand, and any external input device. Furthermore, we propose a novel hand pose retargeting algorithm that significantly boosts the interoperability of Open TeleDex, enabling robust and accurate compatibility with an even wider spectrum of heterogeneous master and slave equipment. Open TeleDex establishes a foundational, high-quality, and publicly available platform for accelerating both academic research and industry development in complex robotic manipulation and IL.', 'abstract_zh': '准确且高保真的示范数据采集是部署机器人模仿学习系统的关键瓶颈，尤其是在处理异构机器人平台时。现有的远程操纵系统往往无法保证在多种类型的远程操纵设备之间进行高精度的数据采集。为解决这一问题，我们开发了Open TeleDex，这是一种统一的远程操纵框架，专为示范数据采集而设计。Open TeleDex特别解决了TripleAny挑战，无缝支持任何类型的机器人臂、任何灵巧手以及任何外部输入设备。此外，我们提出了一种新颖的手部姿态重新目标算法，显著提升了Open TeleDex的互操作性，使其能够与更广泛的异构主从设备实现更 robust 和准确的兼容性。Open TeleDex 为加速复杂机器人操作和模仿学习的学术研究和工业发展奠定了坚实、高质量且公开可用的基础平台。', 'title_zh': 'Open TeleDex：一种硬件无关的模仿学习灵巧操作远程操作系统'}
{'arxiv_id': 'arXiv:2510.14768', 'title': 'Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery', 'authors': 'Fan Yang, Zixuan Huang, Abhinav Kumar, Sergio Aguilera Marinovic, Soshi Iba, Rana Soltani Zarrin, Dmitry Berenson', 'link': 'https://arxiv.org/abs/2510.14768', 'abstract': 'Real-world dexterous manipulation often encounters unexpected errors and disturbances, which can lead to catastrophic failures, such as dropping the manipulated object. To address this challenge, we focus on the problem of catching a falling object while it remains within grasping range and, importantly, resetting the system to a configuration favorable for resuming the primary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a reinforcement learning framework that incorporates a Neural Descriptor Field (NDF)-inspired module to extract implicit contact features. Compared to methods that rely solely on object pose or point cloud input, NDFs can directly reason about finger-object correspondence and adapt to different object geometries. Our experiments show that incorporating contact features improves training efficiency, enhances convergence performance for RL training, and ultimately leads to more successful recoveries. Additionally, we demonstrate that CADRE can generalize zero-shot to unseen objects with different geometries.', 'abstract_zh': 'Real-world灵巧操作经常遇到意外的错误和干扰，这可能导致灾难性的失败，比如抓取对象时将其掉落。为应对这一挑战，我们关注在对象仍处于抓取范围内时捕捉掉落对象的问题，并且重要的是，将系统重置到有利于恢复主要操作任务的配置。我们提出了接触感知动态恢复（CADRE），这是一种 reinforcement learning 框架，结合了受 Neural Descriptor Field (NDF) 启发的模块以提取隐式接触特征。与仅依赖对象姿态或点云输入的方法相比，NDF们可以直接推理指对象对应关系，并适应不同对象的几何形状。我们的实验表明，集成接触特征可以提高训练效率，增强 RL 训练的收敛性能，并最终实现更成功的恢复。此外，我们展示了 CADRE 能够零样本泛化到不同几何形状的未见过的对象。', 'title_zh': '利用神经描述符场学习接触感知动态恢复'}
{'arxiv_id': 'arXiv:2510.14677', 'title': 'When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks', 'authors': 'Steffen Hagedorn, Luka Donkov, Aron Distelzweig, Alexandru P. Condurache', 'link': 'https://arxiv.org/abs/2510.14677', 'abstract': 'Planner evaluation in closed-loop simulation often uses rule-based traffic agents, whose simplistic and passive behavior can hide planner deficiencies and bias rankings. Widely used IDM agents simply follow a lead vehicle and cannot react to vehicles in adjacent lanes, hindering tests of complex interaction capabilities. We address this issue by integrating the state-of-the-art learned traffic agent model SMART into nuPlan. Thus, we are the first to evaluate planners under more realistic conditions and quantify how conclusions shift when narrowing the sim-to-real gap. Our analysis covers 14 recent planners and established baselines and shows that IDM-based simulation overestimates planning performance: nearly all scores deteriorate. In contrast, many planners interact better than previously assumed and even improve in multi-lane, interaction-heavy scenarios like lane changes or turns. Methods trained in closed-loop demonstrate the best and most stable driving performance. However, when reaching their limits in augmented edge-case scenarios, all learned planners degrade abruptly, whereas rule-based planners maintain reasonable basic behavior. Based on our results, we suggest SMART-reactive simulation as a new standard closed-loop benchmark in nuPlan and release the SMART agents as a drop-in alternative to IDM at this https URL.', 'abstract_zh': '基于闭环仿真的规划器评估通常使用基于规则的交通代理，其简单的被动行为可能隐藏规划器缺陷并偏倚排名。广泛使用的IDM代理仅跟随前车，无法应对相邻车道的车辆，阻碍了对复杂交互能力的测试。我们通过将最先进的学习交通代理模型SMART集成到nuPlan中来解决这一问题，从而首次在更接近现实的条件下评估规划器，并量化缩小仿真实际差距后结论的变化。我们的分析涵盖了14个近期规划器和基准，并显示基于IDM的仿真相对于规划性能的估计过高：几乎所有分数都恶化了。相反，许多规划器的交互性能优于先前认为的，甚至在车道变换或转弯等多车道、交互密集的场景中表现更好。在闭环中训练的方法展现出最佳且最稳定的驾驶性能。然而，当在增强的极限案例场景中达到极限时，所有学习的规划器都急剧退化，而基于规则的规划器则保持基本合理的性能。基于我们的结果，我们建议将SMART反应性仿真作为nuPlan中的新标准闭环基准，并在该网址https://link.toSMART.agents.replacement提供SMART代理作为IDM的即插即用替代方案。', 'title_zh': '当规划者遇到现实：学习到的反应式交通代理如何影响nuPlan基准测试'}
{'arxiv_id': 'arXiv:2510.14647', 'title': 'Spatially anchored Tactile Awareness for Robust Dexterous Manipulation', 'authors': 'Jialei Huang, Yang Ye, Yuanqing Gong, Xuezhou Zhu, Yang Gao, Kaifeng Zhang', 'link': 'https://arxiv.org/abs/2510.14647', 'abstract': "Dexterous manipulation requires precise geometric reasoning, yet existing visuo-tactile learning methods struggle with sub-millimeter precision tasks that are routine for traditional model-based approaches. We identify a key limitation: while tactile sensors provide rich contact information, current learning frameworks fail to effectively leverage both the perceptual richness of tactile signals and their spatial relationship with hand kinematics. We believe an ideal tactile representation should explicitly ground contact measurements in a stable reference frame while preserving detailed sensory information, enabling policies to not only detect contact occurrence but also precisely infer object geometry in the hand's coordinate system. We introduce SaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an end-to-end policy framework that explicitly anchors tactile features to the hand's kinematic frame through forward kinematics, enabling accurate geometric reasoning without requiring object models or explicit pose estimation. Our key insight is that spatially grounded tactile representations allow policies to not only detect contact occurrence but also precisely infer object geometry in the hand's coordinate system. We validate SaTA on challenging dexterous manipulation tasks, including bimanual USB-C mating in free space, a task demanding sub-millimeter alignment precision, as well as light bulb installation requiring precise thread engagement and rotational control, and card sliding that demands delicate force modulation and angular precision. These tasks represent significant challenges for learning-based methods due to their stringent precision requirements. Across multiple benchmarks, SaTA significantly outperforms strong visuo-tactile baselines, improving success rates by up to 30 percentage while reducing task completion times by 27 percentage.", 'abstract_zh': '灵巧操作需要精确的几何推理，而现有的基于视觉-触觉学习的方法在亚毫米级精度的任务上仍存在困难，这是传统基于模型的方法所擅长的。我们识别出一个关键限制：尽管触觉传感器提供了丰富的接触信息，当前的学习框架未能有效利用触觉信号的感知丰富性和其与手部运动学的空间关系。我们认为理想的触觉表示方式应该明确地将接触测量值锚定在一个稳定的参考框架中，同时保留详细的感官信息，使策略不仅能检测接触的发生，还能精确推断物体在手部坐标系统中的几何形状。我们引入了SaTA（空间锚定的触觉意识，用于灵巧操作）——一个端到端的策略框架，通过正向运动学将触觉特征明确锚定到手部的运动学框架，从而在无需物体模型或显式姿态估计的情况下实现精确的几何推理。我们的核心见解是，空间锚定的触觉表示使策略不仅能检测接触的发生，还能精确推断物体在手部坐标系统中的几何形状。我们在一系列挑战性的灵巧操作任务上验证了SaTA，包括自由空间中的双臂USB-C连接、需要亚毫米级对准精度的任务，以及要求精确牙纹配合和旋转控制的灯泡安装，还有需要精细力调节和角度精度的卡片滑动。这些任务对基于学习的方法构成了重大挑战，因为它们具有严格的技术要求。在多个基准测试中，SaTA 显著优于强大的基于视觉-触觉基线，成功率提高了30个百分点，完成任务时间减少了27个百分点。', 'title_zh': '空间锚定的触觉感知以实现稳健的灵巧操作'}
{'arxiv_id': 'arXiv:2510.14643', 'title': 'Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation', 'authors': "Lara Brudermüller, Brandon Hung, Xinghao Zhu, Jiuguang Wang, Nick Hawes, Preston Culbertson, Simon Le Cleac'h", 'link': 'https://arxiv.org/abs/2510.14643', 'abstract': 'We present a generative predictive control (GPC) framework that amortizes sampling-based Model Predictive Control (SPC) by bootstrapping it with conditional flow-matching models trained on SPC control sequences collected in simulation. Unlike prior work relying on iterative refinement or gradient-based solvers, we show that meaningful proposal distributions can be learned directly from noisy SPC data, enabling more efficient and informed sampling during online planning. We further demonstrate, for the first time, the application of this approach to real-world contact-rich loco-manipulation with a quadruped robot. Extensive experiments in simulation and on hardware show that our method improves sample efficiency, reduces planning horizon requirements, and generalizes robustly across task variations.', 'abstract_zh': '我们提出了一种生成预测控制(GPC)框架，通过使用基于模拟收集的SPC控制序列训练的条件流匹配模型来加速基于采样的模型预测控制(SPC)。我们展示了可以直接从噪声SPC数据中学习有意义的提案分布，从而在在线规划期间实现更高效的、更有信息量的采样。此外，我们首次展示了这种方法在四足机器人进行接触丰富型移动操作中的应用。大量的模拟和硬件实验表明，我们的方法提高了采样效率，减少了规划 horizon 的要求，并且能够稳健地泛化到任务变化。', 'title_zh': '基于采样 MPC 的生成模型：一种适应性接触丰富操作的-bootstrap 方法'}
{'arxiv_id': 'arXiv:2510.14627', 'title': 'GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement', 'authors': 'Yao Zhong, Hanzhi Chen, Simon Schaefer, Anran Zhang, Stefan Leutenegger', 'link': 'https://arxiv.org/abs/2510.14627', 'abstract': 'Robots are expected to serve as intelligent assistants, helping humans with everyday household organization. A central challenge in this setting is the task of object placement, which requires reasoning about both semantic preferences (e.g., common-sense object relations) and geometric feasibility (e.g., collision avoidance). We present GOPLA, a hierarchical framework that learns generalizable object placement from augmented human demonstrations. A multi-modal large language model translates human instructions and visual inputs into structured plans that specify pairwise object relationships. These plans are then converted into 3D affordance maps with geometric common sense by a spatial mapper, while a diffusion-based planner generates placement poses guided by test-time costs, considering multi-plan distributions and collision avoidance. To overcome data scarcity, we introduce a scalable pipeline that expands human placement demonstrations into diverse synthetic training data. Extensive experiments show that our approach improves placement success rates by 30.04 percentage points over the runner-up, evaluated on positioning accuracy and physical plausibility, demonstrating strong generalization across a wide range of real-world robotic placement scenarios.', 'abstract_zh': '机器人预期作为智能助手，帮助人类进行日常家庭组织。在这个场景下的一个核心挑战是物体放置task，这要求同时考虑语义偏好（例如，常识性物体关系）和几何可行性（例如，碰撞避免）。我们提出了GOPLA，一种分级框架，通过增强的人类示范学习可泛化的物体放置。多模态大语言模型将人类指令和视觉输入转化为结构化计划，指定物体对之间的关系。这些计划随后通过空间映射转换为包含几何常识的3D可利用性地图，而基于扩散的计划生成放置姿态，考虑多计划分布和碰撞避免。为了克服数据稀缺性，我们引入了一种可扩展的流水线，将人类的物体放置示范扩展为多样化的合成训练数据。广泛的实验结果显示，我们的方法在定位准确性和物理可 plausibility 方面的置信成功率提高了30.04个百分点，展示了在广泛的真实世界机器人放置场景中的强大泛化能力。', 'title_zh': 'GOPLA: 通过人工排列的合成增强实现可泛化的物体放置学习'}
{'arxiv_id': 'arXiv:2510.14615', 'title': 'Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models', 'authors': 'Edward Sandra, Lander Vanroye, Dries Dirckx, Ruben Cartuyvels, Jan Swevers, Wilm Decré', 'link': 'https://arxiv.org/abs/2510.14615', 'abstract': 'Classical methods in robot motion planning, such as sampling-based and optimization-based methods, often struggle with scalability towards higher-dimensional state spaces and complex environments. Diffusion models, known for their capability to learn complex, high-dimensional and multi-modal data distributions, provide a promising alternative when applied to motion planning problems and have already shown interesting results. However, most of the current approaches train their model for a single environment, limiting their generalization to environments not seen during training. The techniques that do train a model for multiple environments rely on a specific camera to provide the model with the necessary environmental information and therefore always require that sensor. To effectively adapt to diverse scenarios without the need for retraining, this research proposes Context-Aware Motion Planning Diffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic diffusion model, conditioned on sensor-agnostic contextual information. An attention mechanism, integrated in the well-known U-Net architecture, conditions the model on an arbitrary number of contextual parameters. CAMPD is evaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art approaches on real-world tasks, showing its ability to generalize to unseen environments and generate high-quality, multi-modal trajectories, at a fraction of the time required by existing methods.', 'abstract_zh': '基于扩散模型的Context-Aware运动规划方法：无需重新训练的有效适应多样化场景', 'title_zh': '基于上下文条件化的扩散模型加速多模态运动规划'}
{'arxiv_id': 'arXiv:2510.14612', 'title': 'Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning', 'authors': 'Gabriel Fischer Abati, João Carlos Virgolino Soares, Giulio Turrisi, Victor Barasuol, Claudio Semini', 'link': 'https://arxiv.org/abs/2510.14612', 'abstract': "This paper presents a novel approach for representing proprioceptive time-series data from quadruped robots as structured two-dimensional images, enabling the use of convolutional neural networks for learning locomotion-related tasks. The proposed method encodes temporal dynamics from multiple proprioceptive signals, such as joint positions, IMU readings, and foot velocities, while preserving the robot's morphological structure in the spatial arrangement of the image. This transformation captures inter-signal correlations and gait-dependent patterns, providing a richer feature space than direct time-series processing. We apply this concept in the problem of contact estimation, a key capability for stable and adaptive locomotion on diverse terrains. Experimental evaluations on both real-world datasets and simulated environments show that our image-based representation consistently enhances prediction accuracy and generalization over conventional sequence-based models, underscoring the potential of cross-modal encoding strategies for robotic state learning. Our method achieves superior performance on the contact dataset, improving contact state accuracy from 87.7% to 94.5% over the recently proposed MI-HGNN method, using a 15 times shorter window size.", 'abstract_zh': '本文提出了一种新颖的方法，将四足机器人 proprioceptive 时间序列数据表示为结构化的二维图像，从而能够使用卷积神经网络学习与步态相关的任务。所提出的方法编码了多个 proprioceptive 信号（如关节位置、IMU 读数和足部速度）的时间动态，同时在图像的空间排列中保留了机器人的形态结构。这种转换捕捉了信号间的相关性和步态依赖的模式，提供了比直接时间序列处理更丰富的特征空间。我们将这一概念应用于接触估计问题，这是在多种地形上实现稳定和自适应步态的关键能力。我们在现实世界数据集和模拟环境中进行的实验评估表明，与传统的基于序列的模型相比，我们的图像表示方法在预测准确性和泛化能力上均得到了提升，突显了跨模态编码策略在机器人状态学习中的潜在价值。我们的方法在接触数据集上表现出更优的性能，在比最近提出的 MI-HGNN 方法短15倍的窗口大小下，接触状态准确率从87.7%提高到了94.5%。', 'title_zh': '本体感觉图像： quadruped 机器人本体感觉数据的图像表示及其接触估计学习。'}
{'arxiv_id': 'arXiv:2510.14584', 'title': 'A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning', 'authors': 'Benno Wingender, Nils Dengler, Rohit Menon, Sicong Pan, Maren Bennewitz', 'link': 'https://arxiv.org/abs/2510.14584', 'abstract': 'To reliably pick and place unknown objects under real-world sensing noise remains a challenging task, as existing methods rely on strong object priors (e.g., CAD models), or planar-support assumptions, limiting generalization and unified reasoning between grasping and placing. In this work, we introduce a generalized placeability metric that evaluates placement poses directly from noisy point clouds, without any shape priors. The metric jointly scores stability, graspability, and clearance. From raw geometry, we extract the support surfaces of the object to generate diverse candidates for multi-orientation placement and sample contacts that satisfy collision and stability constraints. By conditioning grasp scores on each candidate placement, our proposed method enables model-free unified pick-and-place reasoning and selects grasp-place pairs that lead to stable, collision-free placements. On unseen real objects and non-planar object supports, our metric delivers CAD-comparable accuracy in predicting stability loss and generally produces more physically plausible placements than learning-based predictors.', 'abstract_zh': '在真实世界感知噪声下可靠地抓取和放置未知对象仍然是一个具有挑战性的任务，现有方法依赖于强大的物体先验（例如CAD模型）或平面支撑假设，限制了泛化能力和抓取与放置之间的统一推理。在本文中，我们引入了一个通用的放置度量，该度量直接从嘈杂的点云中评估放置姿态，而不依赖任何形式的形状先验。该度量联合评分稳定、可抓取性和避让性。从原始几何结构中，我们提取物体的支撑表面，生成多种朝向的放置候选，并采样满足碰撞和稳定性约束的接触点。通过在每个候选放置上条件化抓取评分，我们提出的方法实现了无需模型的统一抓取和放置推理，并选择能够导致稳定、无碰撞放置的抓取-放置配对。在未见过的真实对象和非平面支撑对象上，我们的度量在预测稳定性损失的准确性和生成更符合物理真实的放置方面与基于学习的预测器相比表现更佳。', 'title_zh': '无模型统一取放推理的广义可放置性度量'}
{'arxiv_id': 'arXiv:2510.14546', 'title': 'QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps', 'authors': 'Matti Pekkanen, Francesco Verdoja, Ville Kyrki', 'link': 'https://arxiv.org/abs/2510.14546', 'abstract': 'Embeddings from Visual-Language Models are increasingly utilized to represent semantics in robotic maps, offering an open-vocabulary scene understanding that surpasses traditional, limited labels. Embeddings enable on-demand querying by comparing embedded user text prompts to map embeddings via a similarity metric. The key challenge in performing the task indicated in a query is that the robot must determine the parts of the environment relevant to the query.\nThis paper proposes a solution to this challenge. We leverage natural-language synonyms and antonyms associated with the query within the embedding space, applying heuristics to estimate the language space relevant to the query, and use that to train a classifier to partition the environment into matches and non-matches. We evaluate our method through extensive experiments, querying both maps and standard image benchmarks. The results demonstrate increased queryability of maps and images. Our querying technique is agnostic to the representation and encoder used, and requires limited training.', 'abstract_zh': '视觉语言模型嵌入在机器人地图中的应用：基于嵌入空间的自然语言同义词与反义词扩展的环境查询方法', 'title_zh': 'QuASH: 使用自然语言启发式查询视觉语言机器人地图'}
{'arxiv_id': 'arXiv:2510.14511', 'title': 'Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots', 'authors': 'Mingtian Du, Suhas Raghavendra Kulkarni, Simone Kager, Domenico Campolo', 'link': 'https://arxiv.org/abs/2510.14511', 'abstract': 'This paper establishes analytical stability criteria for robot-mediated human-human (dyadic) interaction systems, focusing on haptic communication under network-induced time delays. Through frequency-domain analysis supported by numerical simulations, we identify both delay-independent and delay-dependent stability criteria. The delay-independent criterion guarantees stability irrespective of the delay, whereas the delay-dependent criterion is characterised by a maximum tolerable delay before instability occurs. The criteria demonstrate dependence on controller and robot dynamic parameters, where increasing stiffness reduces the maximum tolerable delay in a non-linear manner, thereby heightening system vulnerability. The proposed criteria can be generalised to a wide range of robot-mediated interactions and serve as design guidelines for stable remote dyadic systems. Experiments with robots performing human-like movements further illustrate the correlation between stability and motor performance. The findings of this paper suggest the prerequisites for effective delay-compensation strategies.', 'abstract_zh': '本文建立了由机器人介导的人与人（双人）交互系统的稳定性准则，重点讨论了网络引起的时延下的触觉通信稳定性。通过结合数值仿真支持的频域分析，我们确定了既依赖时延又独立于时延的稳定性准则。既不依赖于时延的准则保证了无论时延如何系统都是稳定的，而依赖于时延的准则则通过存在导致不稳定的最大容忍时延来表征。这些准则显示出对控制器和机器人动力学参数的依赖性，其中增加刚度以非线性方式减少了最大容忍时延，从而提高了系统的脆弱性。提出的标准可以推广到各种由机器人介导的交互，并作为稳定远程双人系统的设汁指南。机器人执行人类动作的实验进一步证明了稳定性和运动性能之间的关系。本文的研究结果表明了有效的时延补偿策略的前提条件。', 'title_zh': '机器人介导的延迟触觉双人互动中的稳定性标准与电机性能'}
{'arxiv_id': 'arXiv:2510.14467', 'title': 'Restoring Noisy Demonstration for Imitation Learning With Diffusion Models', 'authors': 'Shang-Fu Chen, Co Yong, Shao-Hua Sun', 'link': 'https://arxiv.org/abs/2510.14467', 'abstract': "Imitation learning (IL) aims to learn a policy from expert demonstrations and has been applied to various applications. By learning from the expert policy, IL methods do not require environmental interactions or reward signals. However, most existing imitation learning algorithms assume perfect expert demonstrations, but expert demonstrations often contain imperfections caused by errors from human experts or sensor/control system inaccuracies. To address the above problems, this work proposes a filter-and-restore framework to best leverage expert demonstrations with inherent noise. Our proposed method first filters clean samples from the demonstrations and then learns conditional diffusion models to recover the noisy ones. We evaluate our proposed framework and existing methods in various domains, including robot arm manipulation, dexterous manipulation, and locomotion. The experiment results show that our proposed framework consistently outperforms existing methods across all the tasks. Ablation studies further validate the effectiveness of each component and demonstrate the framework's robustness to different noise types and levels. These results confirm the practical applicability of our framework to noisy offline demonstration data.", 'abstract_zh': '基于滤波与恢复的模仿学习框架：应对内在噪声的专家演示利用', 'title_zh': '基于扩散模型修复噪声示范以实现模仿学习'}
{'arxiv_id': 'arXiv:2510.14454', 'title': 'Towards Adaptable Humanoid Control via Adaptive Motion Tracking', 'authors': 'Tao Huang, Huayi Wang, Junli Ren, Kangning Yin, Zirui Wang, Xiao Chen, Feiyu Jia, Wentao Zhang, Junfeng Long, Jingbo Wang, Jiangmiao Pang', 'link': 'https://arxiv.org/abs/2510.14454', 'abstract': 'Humanoid robots are envisioned to adapt demonstrated motions to diverse real-world conditions while accurately preserving motion patterns. Existing motion prior approaches enable well adaptability with a few motions but often sacrifice imitation accuracy, whereas motion-tracking methods achieve accurate imitation yet require many training motions and a test-time target motion to adapt. To combine their strengths, we introduce AdaMimic, a novel motion tracking algorithm that enables adaptable humanoid control from a single reference motion. To reduce data dependence while ensuring adaptability, our method first creates an augmented dataset by sparsifying the single reference motion into keyframes and applying light editing with minimal physical assumptions. A policy is then initialized by tracking these sparse keyframes to generate dense intermediate motions, and adapters are subsequently trained to adjust tracking speed and refine low-level actions based on the adjustment, enabling flexible time warping that further improves imitation accuracy and adaptability. We validate these significant improvements in our approach in both simulation and the real-world Unitree G1 humanoid robot in multiple tasks across a wide range of adaptation conditions. Videos and code are available at this https URL.', 'abstract_zh': '类人机器人通过单一参考动作实现适应性控制，同时保持运动模式的准确性和可适应性。', 'title_zh': '面向适应性人体控制的自适应运动追踪'}
{'arxiv_id': 'arXiv:2510.14414', 'title': 'RoboANKLE: Design, Development, and Functional Evaluation of a Robotic Ankle with a Motorized Compliant Unit', 'authors': 'Baris Baysal, Omid Arfaie, Ramazan Unal', 'link': 'https://arxiv.org/abs/2510.14414', 'abstract': 'This study presents a powered transtibial prosthesis with complete push-off assistance, RoboANKLE. The design aims to fulfill specific requirements, such as a sufficient range of motion (RoM) while providing the necessary torque for achieving natural ankle motion in daily activities. Addressing the challenges faced in designing active transtibial prostheses, such as maintaining energetic autonomy and minimizing weight, is vital for the study. With this aim, we try to imitate the human ankle by providing extensive push-off assistance to achieve a natural-like torque profile. Thus, Energy Store and Extended Release mechanism (ESER) is employed with a novel Extra Energy Storage (EES) mechanism. Kinematic and kinetic analyses are carried out to determine the design parameters and assess the design performance. Subsequently, a Computer-Aided Design (CAD) model is built and used in comprehensive dynamic and structural analyses. These analyses are used for the design performance evaluation and determine the forces and torques applied to the prosthesis, which aids in optimizing the design for minimal weight via structural analysis and topology optimization. The design of the prototype is then finalized and manufactured for experimental evaluation to validate the design and functionality. The prototype is realized with a mass of 1.92 kg and dimensions of 261x107x420 mm. The Functional evaluations of the RoboANKLE revealed that it is capable of achieving the natural maximum dorsi-flexion angle with 95% accuracy. Also, Thanks to the implemented mechanisms, the results show that RoboANKLE can generate 57% higher than the required torque for natural walking. The result of the power generation capacity of the RoboANKLE is 10% more than the natural power during the gait cycle.', 'abstract_zh': '一种提供完全推离辅助的电动�ModelError假肢RoboANKLE的研究', 'title_zh': 'RoboANKLE：具有电机化顺应单元的机器人踝关节的设计、开发与功能评价'}
{'arxiv_id': 'arXiv:2510.14357', 'title': 'SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation', 'authors': 'Xiaobei Zhao, Xingqi Lyu, Xiang Li', 'link': 'https://arxiv.org/abs/2510.14357', 'abstract': 'Agricultural robots are emerging as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily rely on manual operation or fixed rail systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling robots to navigate to the target positions following the natural language instructions. In practical agricultural scenarios, navigation instructions often repeatedly occur, yet AgriVLN treat each instruction as an independent episode, overlooking the potential of past experiences to provide spatial context for subsequent ones. To bridge this gap, we propose the method of Spatial Understanding Memory for Agricultural Vision-and-Language Navigation (SUM-AgriVLN), in which the SUM module employs spatial understanding and save spatial memory through 3D reconstruction and representation. When evaluated on the A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47 to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m, demonstrating the state-of-the-art performance in the agricultural domain. Code: this https URL.', 'abstract_zh': '农业机器人正在成为各种农业任务的强大助手，但仍主要依赖手动操作或固定轨道系统进行移动。AgriVLN方法和A2A基准首次将视觉语言导航（VLN）扩展到农业领域，使机器人能够根据自然语言指令导航到目标位置。在实际 agricultural 场景中，导航指令经常重复出现，但 AgriVLN 将每条指令视为独立的场景，忽视了过往经验可能为后续指令提供的空间上下文。为解决这一问题，我们提出了一种农业视觉语言导航中的空间理解记忆方法（SUM-AgriVLN），其中 SUM 模块通过三维重建和表示来实现空间理解并保存空间记忆。在 A2A 基准上的评估表明，我们的 SUM-AgriVLN 能将成功率从 0.47 提高到 0.54，同时导航误差略有增加，从 2.91m 增加到 2.93m，展示了农业领域的先进性能。代码：this https URL。', 'title_zh': 'SUM-AgriVLN: 空间理解记忆在农业视觉与语言导航中的应用'}
{'arxiv_id': 'arXiv:2510.14338', 'title': 'Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion', 'authors': 'Yuanhong Zeng, Anushri Dixit', 'link': 'https://arxiv.org/abs/2510.14338', 'abstract': 'In this work, we study risk-aware reinforcement learning for quadrupedal locomotion. Our approach trains a family of risk-conditioned policies using a Conditional Value-at-Risk (CVaR) constrained policy optimization technique that provides improved stability and sample efficiency. At deployment, we adaptively select the best performing policy from the family of policies using a multi-armed bandit framework that uses only observed episodic returns, without any privileged environment information, and adapts to unknown conditions on the fly. Hence, we train quadrupedal locomotion policies at various levels of robustness using CVaR and adaptively select the desired level of robustness online to ensure performance in unknown environments. We evaluate our method in simulation across eight unseen settings (by changing dynamics, contacts, sensing noise, and terrain) and on a Unitree Go2 robot in previously unseen terrains. Our risk-aware policy attains nearly twice the mean and tail performance in unseen environments compared to other baselines and our bandit-based adaptation selects the best-performing risk-aware policy in unknown terrain within two minutes of operation.', 'abstract_zh': '基于CVaR的风险感知强化学习在四足机器人运动中的研究与应用', 'title_zh': '具有臂端适配的风险意识强化学习在四足行走中的应用'}
{'arxiv_id': 'arXiv:2510.14300', 'title': 'Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning', 'authors': 'Weijie Shen, Yitian Liu, Yuhao Wu, Zhixuan Liang, Sijia Gu, Dehui Wang, Tian Nian, Lei Xu, Yusen Qin, Jiangmiao Pang, Xinping Guan, Xiaokang Yang, Yao Mu', 'link': 'https://arxiv.org/abs/2510.14300', 'abstract': 'Vision-Language-Action (VLA) models are experiencing rapid development and demonstrating promising capabilities in robotic manipulation tasks. However, scaling up VLA models presents several critical challenges: (1) Training new VLA models from scratch demands substantial computational resources and extensive datasets. Given the current scarcity of robot data, it becomes particularly valuable to fully leverage well-pretrained VLA model weights during the scaling process. (2) Real-time control requires carefully balancing model capacity with computational efficiency. To address these challenges, We propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits pretrained weights from dense VLA models, and scales up the action expert by substituting the feedforward layers into sparsely activated MoE layers. AdaMoE employs a decoupling technique that decouples expert selection from expert weighting through an independent scale adapter working alongside the traditional router. This enables experts to be selected based on task relevance while contributing with independently controlled weights, allowing collaborative expert utilization rather than winner-takes-all dynamics. Our approach demonstrates that expertise need not monopolize. Instead, through collaborative expert utilization, we can achieve superior performance while maintaining computational efficiency. AdaMoE consistently outperforms the baseline model across key benchmarks, delivering performance gains of 1.8% on LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks.', 'abstract_zh': '基于视觉-语言-行动的模型（VLA）在机器人操作任务中正经历快速发展并展现出令人期待的能力。然而，扩展VLA模型面临若干关键挑战：（1）从头训练新的VLA模型需要大量计算资源和广泛的数据集。鉴于当前机器人数据的稀缺性，利用预训练的VLA模型权重在扩展过程中变得尤为重要。 （2）实时控制需要在模型容量与计算效率之间精细平衡。为应对这些挑战，我们提出AdaMoE，这是一种MoE架构，继承了密集VLA模型的预训练权重，并通过将前馈层替换为稀疏激活的MoE层来扩展动作专家。AdaMoE采用解耦技术，通过独立比例适配器与传统路由器并行工作来解耦专家选择与权重分配。这使专家可以根据任务相关性被选择，并以独立控制的权重进行贡献，从而实现专家的协同利用而非赢家通吃。我们的方法证明专家不一定需要垄断，通过协同利用专家，我们可以在保持计算效率的同时实现优于基准模型的性能。在关键基准上，AdaMoE在LIBERO上优于基准模型1.8%，在RoboTwin上优于基准模型9.3%。最重要的是，实际实验中的显著21.5%的性能提升验证了其在机器人操作任务中的实用有效性。', 'title_zh': '专家知识无需垄断：面向视觉-语言-行动学习的动作专业化专家混合模型'}
{'arxiv_id': 'arXiv:2510.14293', 'title': 'Learning Human-Humanoid Coordination for Collaborative Object Carrying', 'authors': 'Yushi Du, Yixuan Li, Baoxiong Jia, Yutang Lin, Pei Zhou, Wei Liang, Yanchao Yang, Siyuan Huang', 'link': 'https://arxiv.org/abs/2510.14293', 'abstract': "Human-humanoid collaboration shows significant promise for applications in healthcare, domestic assistance, and manufacturing. While compliant robot-human collaboration has been extensively developed for robotic arms, enabling compliant human-humanoid collaboration remains largely unexplored due to humanoids' complex whole-body dynamics. In this paper, we propose a proprioception-only reinforcement learning approach, COLA, that combines leader and follower behaviors within a single policy. The model is trained in a closed-loop environment with dynamic object interactions to predict object motion patterns and human intentions implicitly, enabling compliant collaboration to maintain load balance through coordinated trajectory planning. We evaluate our approach through comprehensive simulator and real-world experiments on collaborative carrying tasks, demonstrating the effectiveness, generalization, and robustness of our model across various terrains and objects. Simulation experiments demonstrate that our model reduces human effort by 24.7%. compared to baseline approaches while maintaining object stability. Real-world experiments validate robust collaborative carrying across different object types (boxes, desks, stretchers, etc.) and movement patterns (straight-line, turning, slope climbing). Human user studies with 23 participants confirm an average improvement of 27.4% compared to baseline models. Our method enables compliant human-humanoid collaborative carrying without requiring external sensors or complex interaction models, offering a practical solution for real-world deployment.", 'abstract_zh': '人形机器人与人类的合作在医疗、家庭辅助和制造领域展现出显著的潜力。尽管 compliant 机器人与人类协作已经广泛应用于机器人臂操作中，但人类形机器人与人类的 compliant 合作仍处于未被充分探索的状态，主要由于人类形机器人复杂的全身动力学特性。本文提出了一种仅基于本体感知的强化学习方法 COLA，该方法在单一策略中结合了领导者和跟随者的行为。模型通过具有动态物体交互的闭环环境进行训练，以隐式预测物体运动模式和人类意图，从而通过协调轨迹规划维持负载平衡。通过综合的模拟器和现实世界的实验评估协作搬运任务，展示了该模型在各种地形和物体上的有效性、泛化能力和鲁棒性。模拟实验表明，与基准方法相比，该模型在保持物体稳定性的前提下将人类的努力降低了24.7%。实地实验验证了不同物体类型（箱子、桌子、担架等）和运动模式（直线、转弯、坡道攀爬）下的鲁棒协作搬运能力。23名参与者的用户研究表明，该方法相对于基准模型平均提高了27.4%。本方法可以在不需要外部传感器或复杂交互模型的情况下实现合规的人形机器人与人类协作搬运，为实际部署提供了一种实用的解决方案。', 'title_zh': '学习人类-类人机器人协作搬运物体中的协调控制'}
{'arxiv_id': 'arXiv:2510.14234', 'title': 'Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space', 'authors': 'Ning Han, Gu Gong, Bin Zhang, Yuexuan Xu, Bohan Yang, Yunhui Liu, David Navarro-Alarcon', 'link': 'https://arxiv.org/abs/2510.14234', 'abstract': "Manipulating three-dimensional (3D) deformable objects presents significant challenges for robotic systems due to their infinite-dimensional state space and complex deformable dynamics. This paper proposes a novel model-free approach for shape control with constraints imposed on key points. Unlike existing methods that rely on feature dimensionality reduction, the proposed controller leverages the coordinates of key points as the feature vector, which are extracted from the deformable object's point cloud using deep learning methods. This approach not only reduces the dimensionality of the feature space but also retains the spatial information of the object. By extracting key points, the manipulation of deformable objects is simplified into a visual servoing problem, where the shape dynamics are described using a deformation Jacobian matrix. To enhance control accuracy, a prescribed performance control method is developed by integrating barrier Lyapunov functions (BLF) to enforce constraints on the key points. The stability of the closed-loop system is rigorously analyzed and verified using the Lyapunov method. Experimental results further demonstrate the effectiveness and robustness of the proposed method.", 'abstract_zh': '基于关键点约束的三维可变形物体形状控制新方法', 'title_zh': '空间潜在空间中可变形物体操作的指定性能控制'}
{'arxiv_id': 'arXiv:2510.14117', 'title': 'ViTacGen: Robotic Pushing with Vision-to-Touch Generation', 'authors': 'Zhiyuan Wu, Yijiong Lin, Yongqiang Zhao, Xuyang Zhang, Zhuo Chen, Nathan Lepora, Shan Luo', 'link': 'https://arxiv.org/abs/2510.14117', 'abstract': "Robotic pushing is a fundamental manipulation task that requires tactile feedback to capture subtle contact forces and dynamics between the end-effector and the object. However, real tactile sensors often face hardware limitations such as high costs and fragility, and deployment challenges involving calibration and variations between different sensors, while vision-only policies struggle with satisfactory performance. Inspired by humans' ability to infer tactile states from vision, we propose ViTacGen, a novel robot manipulation framework designed for visual robotic pushing with vision-to-touch generation in reinforcement learning to eliminate the reliance on high-resolution real tactile sensors, enabling effective zero-shot deployment on visual-only robotic systems. Specifically, ViTacGen consists of an encoder-decoder vision-to-touch generation network that generates contact depth images, a standardized tactile representation, directly from visual image sequence, followed by a reinforcement learning policy that fuses visual-tactile data with contrastive learning based on visual and generated tactile observations. We validate the effectiveness of our approach in both simulation and real world experiments, demonstrating its superior performance and achieving a success rate of up to 86\\%.", 'abstract_zh': '视觉触觉生成的强化学习机器人推动物理框架', 'title_zh': 'ViTacGen: 视觉到触觉的生成在机器人推举中'}
{'arxiv_id': 'arXiv:2510.14072', 'title': 'Partial Feedback Linearization Control of a Cable-Suspended Multirotor Platform for Stabilization of an Attached Load', 'authors': 'Hemjyoti Das, Christian Ott', 'link': 'https://arxiv.org/abs/2510.14072', 'abstract': 'In this work, we present a novel control approach based on partial feedback linearization (PFL) for the stabilization of a suspended aerial platform with an attached load. Such systems are envisioned for various applications in construction sites involving cranes, such as the holding and transportation of heavy objects. Our proposed control approach considers the underactuation of the whole system while utilizing its coupled dynamics for stabilization. We demonstrate using numerical stability analysis that these coupled terms are crucial for the stabilization of the complete system. We also carried out robustness analysis of the proposed approach in the presence of external wind disturbances, sensor noise, and uncertainties in system dynamics. As our envisioned target application involves cranes in outdoor construction sites, our control approaches rely on only onboard sensors, thus making it suitable for such applications. We carried out extensive simulation studies and experimental tests to validate our proposed control approach.', 'abstract_zh': '基于部分反馈线性化的悬吊空中平台载重稳定控制新方法', 'title_zh': '基于部分反馈线性化控制的电缆悬吊多旋翼平台附载物稳定控制'}
{'arxiv_id': 'arXiv:2510.14065', 'title': 'Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning', 'authors': 'Gaoyuan Liu, Joris de Winter, Yuri Durodie, Denis Steckelmacher, Ann Nowe, Bram Vanderborght', 'link': 'https://arxiv.org/abs/2510.14065', 'abstract': 'Task and motion planning (TAMP) for robotics manipulation necessitates long-horizon reasoning involving versatile actions and skills. While deterministic actions can be crafted by sampling or optimizing with certain constraints, planning actions with uncertainty, i.e., probabilistic actions, remains a challenge for TAMP. On the contrary, Reinforcement Learning (RL) excels in acquiring versatile, yet short-horizon, manipulation skills that are robust with uncertainties. In this letter, we design a method that integrates RL skills into TAMP pipelines. Besides the policy, a RL skill is defined with data-driven logical components that enable the skill to be deployed by symbolic planning. A plan refinement sub-routine is designed to further tackle the inevitable effect uncertainties. In the experiments, we compare our method with baseline hierarchical planning from both TAMP and RL fields and illustrate the strength of the method. The results show that by embedding RL skills, we extend the capability of TAMP to domains with probabilistic skills, and improve the planning efficiency compared to the previous methods.', 'abstract_zh': '基于强化学习的技巧集成到任务与动作规划中以处理概率性技能的任务与动作规划', 'title_zh': '基于乐观强化学习的技能插入的任务与运动规划'}
{'arxiv_id': 'arXiv:2510.14063', 'title': 'Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming', 'authors': 'Nan Li, Jiming Ren, Haris Miller, Samuel Coogan, Karen M. Feigh, Ye Zhao', 'link': 'https://arxiv.org/abs/2510.14063', 'abstract': 'Multi-Agent Task Assignment and Planning (MATP) has attracted growing attention but remains challenging in terms of scalability, spatial reasoning, and adaptability in obstacle-rich environments. To address these challenges, we propose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming, which advances MATP by introducing a novel obstacle-aware strategy for task assignment. First, we develop an adaptive Halton sequence map, the first known application of Halton sampling with obstacle-aware adaptation in MATP, which adjusts sampling density based on obstacle distribution. Second, we propose a cluster-auction-selection framework that integrates obstacle-aware clustering with weighted auctions and intra-cluster task selection. These mechanisms jointly enable effective coordination among heterogeneous robots while maintaining scalability and near-optimal allocation performance. In addition, our framework leverages an LLM to interpret human instructions and directly guide the planner in real time. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in task assignment quality, scalability, adaptability to dynamic changes, and overall execution performance compared to state-of-the-art MATP baselines. A project website is available at this https URL.', 'abstract_zh': '多机器人任务分配与规划（MATP）在处理规模性、空间推理以及多障碍环境中的适应性方面仍然具有挑战性。为了解决这些挑战，我们提出了OATH：适应性障碍感知任务分配与规划，该方法通过引入新的障碍感知策略来推进MATP。首先，我们开发了一种自适应Halton序列图，这是首次将Halton采样与障碍感知调整应用于MATP中，基于障碍分布调整采样密度。其次，我们提出了一种聚类拍卖选择框架，该框架结合了障碍感知聚类、加权拍卖以及内部簇内任务选择。这些机制共同实现了异构机器人之间的有效协调，同时保持可扩展性和接近最优的分配性能。此外，我们的框架利用大语言模型来解释人类指令并实时指导规划器。我们通过在NVIDIA Isaac Sim中的验证，展示了与最先进的MATP基线相比，在任务分配质量、可扩展性、对动态变化的适应性以及整体执行性能方面的显著改进。项目网站可通过该链接访问。', 'title_zh': '异构机器人团队的自适应障碍aware任务分配与规划'}
{'arxiv_id': 'arXiv:2510.14018', 'title': 'Spatially Intelligent Patrol Routes for Concealed Emitter Localization by Robot Swarms', 'authors': 'Adam Morris, Timothy Pelham, Edmund R. Hunt', 'link': 'https://arxiv.org/abs/2510.14018', 'abstract': "This paper introduces a method for designing spatially intelligent robot swarm behaviors to localize concealed radio emitters. We use differential evolution to generate geometric patrol routes that localize unknown signals independently of emitter parameters, a key challenge in electromagnetic surveillance. Patrol shape and antenna type are shown to influence information gain, which in turn determines the effective triangulation coverage. We simulate a four-robot swarm across eight configurations, assigning pre-generated patrol routes based on a specified patrol shape and sensing capability (antenna type: omnidirectional or directional). An emitter is placed within the map for each trial, with randomized position, transmission power and frequency. Results show that omnidirectional localization success rates are driven primarily by source location rather than signal properties, with failures occurring most often when sources are placed in peripheral areas of the map. Directional antennas are able to overcome this limitation due to their higher gain and directivity, with an average detection success rate of 98.75% compared to 80.25% for omnidirectional. Average localization errors range from 1.01-1.30 m for directional sensing and 1.67-1.90 m for omnidirectional sensing; while directional sensing also benefits from shorter patrol edges. These results demonstrate that a swarm's ability to predict electromagnetic phenomena is directly dependent on its physical interaction with the environment. Consequently, spatial intelligence, realized here through optimized patrol routes and antenna selection, is a critical design consideration for effective robotic surveillance.", 'abstract_zh': '基于空间智能的机器人 swarm 定位隐蔽电磁发射器行为设计方法', 'title_zh': '机器人 swarm 在隐蔽发射源定位中的智能巡逻路径规划'}
{'arxiv_id': 'arXiv:2510.14000', 'title': 'A Diffusion-Refined Planner with Reinforcement Learning Priors for Confined-Space Parking', 'authors': 'Mingyang Jiang, Yueyuan Li, Jiaru Zhang, Songan Zhang, Ming Yang', 'link': 'https://arxiv.org/abs/2510.14000', 'abstract': 'The growing demand for parking has increased the need for automated parking planning methods that can operate reliably in confined spaces. In restricted and complex environments, high-precision maneuvers are required to achieve a high success rate in planning, yet existing approaches often rely on explicit action modeling, which faces challenges when accurately modeling the optimal action distribution. In this paper, we propose DRIP, a diffusion-refined planner anchored in reinforcement learning (RL) prior action distribution, in which an RL-pretrained policy provides prior action distributions to regularize the diffusion training process. During the inference phase the denoising process refines these coarse priors into more precise action distributions. By steering the denoising trajectory through the reinforcement learning prior distribution during training, the diffusion model inherits a well-informed initialization, resulting in more accurate action modeling, a higher planning success rate, and reduced inference steps. We evaluate our approach across parking scenarios with varying degrees of spatial constraints. Experimental results demonstrate that our method significantly improves planning performance in confined-space parking environments while maintaining strong generalization in common scenarios.', 'abstract_zh': 'Growing停车需求增加了在受限空间中可靠实现自动化停车规划方法的需求。在受限和复杂环境中，高精度操作对于规划的成功率至关重要，但现有方法往往依赖于显式动作建模，这在准确建模最优动作分布时面临挑战。本文提出了一种名为DRIP的扩散精炼规划器，该规划器基于强化学习（RL）先验动作分布，其中预训练的RL策略为扩散训练过程提供先验动作分布以正则化训练过程。在推理阶段，去噪过程将这些粗糙的先验转换为更精确的动作分布。通过在训练过程中引导去噪路径通过强化学习先验分布，扩散模型继承了良好的初始化，从而实现更准确的动作建模、更高的规划成功率和较少的推理步骤。我们在具有不同空间约束的停车场景中评估了该方法。实验结果表明，我们的方法在受限空间停车环境中显著提高了规划性能，同时在常见场景中保持了强大的泛化能力。', 'title_zh': '受强化学习先验约束的扩散精化规划算法用于受限空间泊车'}
{'arxiv_id': 'arXiv:2510.14976', 'title': 'Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation', 'authors': 'Shaowei Liu, Chuan Guo, Bing Zhou, Jian Wang', 'link': 'https://arxiv.org/abs/2510.14976', 'abstract': 'Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.', 'abstract_zh': '近距人体交互姿态承载丰富的交互动力学上下文信息。基于此类姿态，人类能够直观地推断出上下文并预判可能的过去和未来动态，依托强烈的人类行为先验。受到这一观察的启发，我们提出了Ponimator，一种基于近距交互姿态的多功能交互动画框架。我们的训练数据包括来自运动捕捉交互数据集的近距离接触两人姿态及其周围的时间上下文。利用交互姿态先验，Ponimator采用两个条件扩散模型：（1）一个姿态动画器，利用时间先验从交互姿态生成动态运动序列；（2）一个姿态生成器，应用空间先验从单个姿态、文本或两者合成交互姿态，当交互姿态不可用时。总体而言，Ponimator支持多种任务，包括基于图像的交互动画、反应动画以及文本到交互的合成，促进高质量运动捕捉数据中的交互知识向开放场景的转移。在不同数据集和应用领域的实证实验表明了姿态先验的通用性以及我们框架的有效性和鲁棒性。', 'title_zh': 'Ponimator: 展开互动姿态以实现多样化的真人互动动画'}
{'arxiv_id': 'arXiv:2510.14946', 'title': 'EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices', 'authors': 'Romina Aalishah, Mozhgan Navardi, Tinoosh Mohsenin', 'link': 'https://arxiv.org/abs/2510.14946', 'abstract': 'Deployment of efficient and accurate Deep Learning models has long been a challenge in autonomous navigation, particularly for real-time applications on resource-constrained edge devices. Edge devices are limited in computing power and memory, making model efficiency and compression essential. In this work, we propose EdgeNavMamba, a reinforcement learning-based framework for goal-directed navigation using an efficient Mamba object detection model. To train and evaluate the detector, we introduce a custom shape detection dataset collected in diverse indoor settings, reflecting visual cues common in real-world navigation. The object detector serves as a pre-processing module, extracting bounding boxes (BBOX) from visual input, which are then passed to an RL policy to control goal-oriented navigation. Experimental results show that the student model achieved a reduction of 67% in size, and up to 73% in energy per inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5, while keeping the same performance as the teacher model. EdgeNavMamba also maintains high detection accuracy in MiniWorld and IsaacLab simulators while reducing parameters by 31% compared to the baseline. In the MiniWorld simulator, the navigation policy achieves over 90% success across environments of varying complexity.', 'abstract_zh': '基于强化学习的EdgeNavMamba：高效准确的目标导向导航框架', 'title_zh': 'EdgeNavMamba: 优化对象检测以提高边缘设备能量效率的Mamba算法'}
{'arxiv_id': 'arXiv:2510.14914', 'title': 'Design of Paper Robot Building Kits', 'authors': 'Ruhan Yang, Ellen Yi-Luen Do', 'link': 'https://arxiv.org/abs/2510.14914', 'abstract': 'Building robots is an engaging activity that provides opportunities for hands-on learning. However, traditional robot-building kits are usually costly with limited functionality due to material and technology constraints. To improve the accessibility and flexibility of such kits, we take paper as the building material and extensively explore the versatility of paper-based interactions. Based on an analysis of current robot-building kits and paper-based interaction research, we propose a design space for devising paper robots. We also analyzed our building kit designs using this design space, where these kits demonstrate the potential of paper as a cost-effective material for robot building. As a starting point, our design space and building kit examples provide a guideline that inspires and informs future research and development of novel paper robot-building kits.', 'abstract_zh': '将纸张作为构建材料探索纸基交互的潜力：构建成本-effective的纸机器人', 'title_zh': '纸机器人搭建套件的设计'}
{'arxiv_id': 'arXiv:2510.14836', 'title': 'QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models', 'authors': 'Yixuan Li, Yuhui Chen, Mingcai Zhou, Haoran Li', 'link': 'https://arxiv.org/abs/2510.14836', 'abstract': 'Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks. However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control. To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task. A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.', 'abstract_zh': '空间知觉与推理对于视觉-语言-行动（VLA）模型完成精细操作任务至关重要。然而，现有方法往往缺乏理解并推理关键3D结构的能力，以实现精确控制。为解决这一限制，我们提出QDepth-VLA，这是一种通用框架，通过辅助深度预测任务增强VLA模型。专门设计的深度专家预测由VQ-VAE编码器获得的深度图的量化潜在令牌，使模型能够学习深度感知的表示，捕捉关键的几何线索。实验结果表明，QDepth-VLA在模拟基准和真实世界任务上表现出强大的空间推理能力和竞争力的操作性能。', 'title_zh': 'QDepth-VLA：量化深度预测作为视觉-语言-动作模型的辅助监督'}
{'arxiv_id': 'arXiv:2510.14828', 'title': 'RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning', 'authors': 'Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li', 'link': 'https://arxiv.org/abs/2510.14828', 'abstract': "Improving the reasoning capabilities of embodied agents is crucial for robots to complete complex human instructions in long-view manipulation tasks successfully. Despite the success of large language models and vision language models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue facing challenges in performing long-horizon manipulation tasks in complex real-world environments, owing to their restricted common sense and reasoning capabilities. Considering that aligning general-purpose vision language models to robotic planning tasks via supervised fine-tuning suffers from poor generalization and insufficient physical understanding, we propose RoboGPT-R1, a two-stage fine-tuning framework for embodied planning. In this framework, supervised training acquires foundational knowledge through expert sequences, followed by RL to address the model's shortcomings in visual-spatial understanding and reasoning. To achieve physical understanding and action sequence consistency in multi-step reasoning tasks, we design a rule-based reward function that simultaneously considers long-horizon performance and action constraint in the environment. The reasoning model, trained on Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini, by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the EmbodiedBench benchmark.", 'abstract_zh': '提升具身代理的推理能力对于机器人成功完成长期复杂人类指令的操作任务至关重要。尽管基于监督微调（SFT）的大语言模型和视觉语言模型在规划任务中取得了成功，但在复杂真实环境中的长时 horizon 操作任务中仍面临挑战，这是由于它们的常识和推理能力受限。考虑到通过监督微调将通用视觉语言模型对准机器人规划任务会遭受泛化能力差和物理理解不足的问题，我们提出了一种两阶段的具身规划微调框架 RoboGPT-R1。在该框架中，监督训练通过专家序列获取基础知识，随后通过强化学习来解决模型在视觉空间理解和推理方面的不足。为了在多步推理任务中实现物理理解和动作序列一致，我们设计了一种基于规则的奖励函数，同时考虑了环境中的长期表现和动作约束。在具身基准测试（EmbodiedBench）上，由 Qwen2.5-VL-3B 训练的推理模型显著优于更大的模型 GPT-4o-mini，性能提升21.33%，并且比在 Qwen2.5-VL-7B 上训练的其他工作高出20.33%。', 'title_zh': 'RoboGPT-R1：增强机器人规划的强化学习方法'}
{'arxiv_id': 'arXiv:2510.14653', 'title': 'Requirement Identification for Traffic Simulations in Driving Simulators', 'authors': 'Sven Tarlowski, Lutz Eckstein', 'link': 'https://arxiv.org/abs/2510.14653', 'abstract': 'This paper addresses the challenge of ensuring realistic traffic conditions by proposing a methodology that systematically identifies traffic simulation requirements. Using a structured approach based on sub-goals in each study phase, specific technical needs are derived for microscopic levels, agent models, and visual representation. The methodology aims to maintain a high degree of fidelity, enhancing both the validity of experimental outcomes and participant engagement. By providing a clear link between study objectives and traffic simulation design, this approach supports robust automotive development and testing.', 'abstract_zh': '本文提出了一种方法学，以系统地识别交通仿真需求的方式应对确保现实主义交通条件的挑战。该方法学基于每个研究阶段的子目标采用结构化方法，为微观层次、代理模型和可视化表示提取具体的技术需求。该方法学旨在保持高度的保真度，提高实验结果的有效性和参与者 engagement。通过清晰地连接研究目标与交通仿真的设计，该方法支持 robust 的 automotive 开发与测试。', 'title_zh': '驾驶模拟器中交通模拟的需求识别'}
{'arxiv_id': 'arXiv:2510.14354', 'title': 'Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration', 'authors': 'Siddharth Tourani, Jayaram Reddy, Sarvesh Thakur, K Madhava Krishna, Muhammad Haris Khan, N Dinesh Reddy', 'link': 'https://arxiv.org/abs/2510.14354', 'abstract': 'With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has become available. This prompts the question of how to utilize this data for geometric reasoning of scenes. While many RGB-D registration meth- ods rely on geometric and feature-based similarity, we take a different approach. We use cycle-consistent keypoints as salient points to enforce spatial coherence constraints during matching, improving correspondence accuracy. Additionally, we introduce a novel pose block that combines a GRU recurrent unit with transformation synchronization, blending historical and multi-view data. Our approach surpasses previous self- supervised registration methods on ScanNet and 3DMatch, even outperforming some older supervised methods. We also integrate our components into existing methods, showing their effectiveness.', 'abstract_zh': '随着消费级深度相机的兴起，大量未标记的RGB-D数据变得可用。这促使我们思考如何利用这些数据来进行场景的几何推理。尽管许多RGB-D配准方法依赖于几何和特征相似性，我们采取了不同的方法。我们使用循环一致的关键点作为显著点，在匹配过程中施加空间连贯性约束，提高对应关系的准确性。此外，我们引入了一种新的姿态模块，该模块结合了GRU递归单元与变换同步，融合了历史和多视角数据。我们的方法在ScanNet和3DMatch上超过了之前的自监督配准方法，甚至优于一些较早的监督方法。我们还将我们的组件集成到现有方法中，展示了它们的有效性。', 'title_zh': '利用周期一致锚点进行自我监督的RGB-D配准'}
{'arxiv_id': 'arXiv:2510.13810', 'title': 'Choreographing Trash Cans: On Speculative Futures of Weak Robots in Public Spaces', 'authors': 'Minja Axelsson, Lea Luka Sikau', 'link': 'https://arxiv.org/abs/2510.13810', 'abstract': 'Delivering groceries or cleaning airports, mobile robots exist in public spaces. While these examples showcase robots that execute tasks, this paper explores mobile robots that encourage posthuman collaboration rather than managing environments independently. With feigned fragility, cuteness and incomplete functionalities, the so-called "weak robots" invite passersby to engage not only on a utilitarian level, but also through imaginative and emotional responses. After examining the workings of "weak robots" by queering notions of function and ability, we introduce two speculative design fiction vignettes that describe choreographies of such robots in future urban spaces -- one exploring a utopian weak robot and the other a dystopian weak robot. We introduce these speculations in order to discuss how different values may drive design decisions, and how such decisions may shape and drive different socio-technical futures in which robots and humans share public spaces that incentivise collaboration.', 'abstract_zh': '递送 groceries 或清洁机场，移动机器人存在于公共空间。虽然这些例子展示了执行任务的机器人，本文探讨的是鼓励后人类协作而非独立管理环境的移动机器人。通过虚构的脆弱性、可爱性和不完整的功能，所谓的“弱机器人”邀请路人不仅从实用层面参与，还通过想象和情感反应进行参与。通过质疑功能和能力的概念，我们介绍了两个关于未来城市空间中此类机器人编舞的 speculative 设计幻想片段——一个探讨乌托邦弱机器人，另一个探讨反乌托邦弱机器人。我们提出这些设想是为了讨论不同的价值观如何驱动设计决策，并探讨这些决策如何塑造和驱动机器人与人类共享的公共空间中的不同社会技术未来，这些未来鼓励合作。', 'title_zh': 'choreographing 垃圾桶：关于公共空间中弱机器人 speculate 性未来的思考'}
{'arxiv_id': 'arXiv:2509.26255', 'title': 'ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning', 'authors': 'Yichao Liang, Dat Nguyen, Cambridge Yang, Tianyang Li, Joshua B. Tenenbaum, Carl Edward Rasmussen, Adrian Weller, Zenna Tavares, Tom Silver, Kevin Ellis', 'link': 'https://arxiv.org/abs/2509.26255', 'abstract': "Long-horizon embodied planning is challenging because the world does not only change through an agent's actions: exogenous processes (e.g., water heating, dominoes cascading) unfold concurrently with the agent's actions. We propose a framework for abstract world models that jointly learns (i) symbolic state representations and (ii) causal processes for both endogenous actions and exogenous mechanisms. Each causal process models the time course of a stochastic cause-effect relation. We learn these world models from limited data via variational Bayesian inference combined with LLM proposals. Across five simulated tabletop robotics environments, the learned models enable fast planning that generalizes to held-out tasks with more objects and more complex goals, outperforming a range of baselines.", 'abstract_zh': '长时程体态规划具有挑战性，因为世界不仅通过代理的动作发生变化：外生过程（如热水加热、多米诺骨牌连锁反应）与代理的动作同时展开。我们提出了一种抽象世界模型框架，该框架联合学习（i）符号状态表示和（ii）因果过程，包括内生动作和外生机制。每个因果过程模型了一种随机因果关系的时间进程。我们通过结合变分贝叶斯推断和大语言模型提案的方法，从有限数据中学习这些世界模型。在五个模拟桌面机器人环境中，所学模型能够实现快速规划，并能在包含更多物体和更复杂目标的保留任务上泛化，优于多种基线方法。', 'title_zh': 'ExoPredicator: 学习动态世界的抽象模型以进行机器人规划'}
