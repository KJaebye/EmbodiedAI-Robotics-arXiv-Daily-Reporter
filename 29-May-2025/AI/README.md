# HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym 

**Title (ZH)**: HDDLGym：一个基于HDDL定义的多智能体分层问题研究工具，兼容OpenAI Gym 

**Authors**: Ngoc La, Ruaridh Mon-Williams, Julie A. Shah  

**Link**: [PDF](https://arxiv.org/pdf/2505.22597)  

**Abstract**: In recent years, reinforcement learning (RL) methods have been widely tested using tools like OpenAI Gym, though many tasks in these environments could also benefit from hierarchical planning. However, there is a lack of a tool that enables seamless integration of hierarchical planning with RL. Hierarchical Domain Definition Language (HDDL), used in classical planning, introduces a structured approach well-suited for model-based RL to address this gap. To bridge this integration, we introduce HDDLGym, a Python-based tool that automatically generates OpenAI Gym environments from HDDL domains and problems. HDDLGym serves as a link between RL and hierarchical planning, supporting multi-agent scenarios and enabling collaborative planning among agents. This paper provides an overview of HDDLGym's design and implementation, highlighting the challenges and design choices involved in integrating HDDL with the Gym interface, and applying RL policies to support hierarchical planning. We also provide detailed instructions and demonstrations for using the HDDLGym framework, including how to work with existing HDDL domains and problems from International Planning Competitions, exemplified by the Transport domain. Additionally, we offer guidance on creating new HDDL domains for multi-agent scenarios and demonstrate the practical use of HDDLGym in the Overcooked domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a valuable tool for studying RL in hierarchical planning, particularly in multi-agent contexts. 

**Abstract (ZH)**: 基于HDDL的HDDL Gym：强化学习与层次规划的无缝集成 

---
# AI Mathematician: Towards Fully Automated Frontier Mathematical Research 

**Title (ZH)**: AI数学家：迈向全自动前沿数学研究 

**Authors**: Yuanhang Liu, Yanxing Huang, Yanqiao Wang, Peng Li, Yang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.22451)  

**Abstract**: Large Reasoning Models (LRMs) have made significant progress in mathematical capabilities in recent times. However, these successes have been primarily confined to competition-level problems. In this work, we propose AI Mathematician (AIM) framework, which harnesses the reasoning strength of LRMs to support frontier mathematical research. We have identified two critical challenges of mathematical research compared to competition, {\it the intrinsic complexity of research problems} and {\it the requirement of procedural rigor}. To address these challenges, AIM incorporates two core strategies: an exploration mechanism to foster longer solution paths, and the pessimistic reasonable verification method to ensure reliability.
This early version of AIM already exhibits strong capability in tackling research-level tasks. We conducted extensive experiments across several real-world mathematical topics and obtained promising results. AIM is able to autonomously construct substantial portions of proofs and uncover non-trivial insights within each research area. These findings highlight the potential of LRMs in mathematical discovery and suggest that LRM-based agent systems could significantly accelerate mathematical research in the future. 

**Abstract (ZH)**: Large Reasoning Models (LRMs)在数学能力方面取得了显著进步，但在近日，这些成功主要限于竞赛级别问题。在本工作中，我们提出了一种AI数学家（AI Mathematician, AIM）框架，该框架利用LRMs的强大推理能力支持前沿数学研究。与竞赛相比，数学研究面临两个关键挑战：研究问题的固有复杂性以及程序严谨性的要求。为了应对这些挑战，AIM采用两大核心策略：探索机制以促进更长的解题路径，以及悲观合理的验证方法以确保可靠性。这一早期版本的AIM已经在处理研究级别任务方面展现出强大的能力。我们在多个实际数学主题上进行了广泛的试验，并取得了令人鼓舞的结果。AI数学家能够自主构建每个研究领域中重要部分的证明，并发现非平凡的洞察。这些发现突显了LRMs在数学发现领域的潜力，并表明基于LRMs的代理系统未来有望显著加速数学研究。 

---
# AgentDNS: A Root Domain Naming System for LLM Agents 

**Title (ZH)**: AgentDNS: 用于LLM代理的根域名系统 

**Authors**: Enfang Cui, Yujun Cheng, Rui She, Dan Liu, Zhiyuan Liang, Minxin Guo, Tianzheng Li, Qian Wei, Wenjuan Xing, Zhijie Zhong  

**Link**: [PDF](https://arxiv.org/pdf/2505.22368)  

**Abstract**: The rapid evolution of Large Language Model (LLM) agents has highlighted critical challenges in cross-vendor service discovery, interoperability, and communication. Existing protocols like model context protocol and agent-to-agent protocol have made significant strides in standardizing interoperability between agents and tools, as well as communication among multi-agents. However, there remains a lack of standardized protocols and solutions for service discovery across different agent and tool vendors. In this paper, we propose AgentDNS, a root domain naming and service discovery system designed to enable LLM agents to autonomously discover, resolve, and securely invoke third-party agent and tool services across organizational and technological boundaries. Inspired by the principles of the traditional DNS, AgentDNS introduces a structured mechanism for service registration, semantic service discovery, secure invocation, and unified billing. We detail the architecture, core functionalities, and use cases of AgentDNS, demonstrating its potential to streamline multi-agent collaboration in real-world scenarios. The source code will be published on this https URL. 

**Abstract (ZH)**: 大型语言模型（LLM）代理的快速演进突显了跨供应商服务发现、互操作性和通信方面的关键挑战。现有的协议如模型上下文协议和代理到代理协议已经在标准化代理和工具间的互操作性和多代理间的通信方面取得了显著进展。然而，仍然缺乏跨不同代理和工具供应商的服务发现标准协议和解决方案。本文提出AgentDNS，这是一种根域命名和服务发现系统，旨在使LLM代理能够自主地跨越组织和技术边界发现、解析并安全地调用第三方代理和服务。受传统DNS原则的启发，AgentDNS引入了一种结构化的服务注册机制、语义化服务发现、安全调用和统一计费的统一方案。我们详细介绍了AgentDNS的架构、核心功能和应用场景，并展示了其在实际场景中简化多代理协作的潜力。源代码将发布在此链接：https://XXXXXX。 

---
# From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications 

**Title (ZH)**: 从大规模AI模型到能动AI：未来智能通信教程 

**Authors**: Feibo Jiang, Cunhua Pan, Li Dong, Kezhi Wang, Octavia A. Dobre, Merouane Debbah  

**Link**: [PDF](https://arxiv.org/pdf/2505.22311)  

**Abstract**: With the advent of 6G communications, intelligent communication systems face multiple challenges, including constrained perception and response capabilities, limited scalability, and low adaptability in dynamic environments. This tutorial provides a systematic introduction to the principles, design, and applications of Large Artificial Intelligence Models (LAMs) and Agentic AI technologies in intelligent communication systems, aiming to offer researchers a comprehensive overview of cutting-edge technologies and practical guidance. First, we outline the background of 6G communications, review the technological evolution from LAMs to Agentic AI, and clarify the tutorial's motivation and main contributions. Subsequently, we present a comprehensive review of the key components required for constructing LAMs. We further categorize LAMs and analyze their applicability, covering Large Language Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models (LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a LAM-centric design paradigm tailored for communications, encompassing dataset construction and both internal and external learning approaches. Building upon this, we develop an LAM-based Agentic AI system for intelligent communications, clarifying its core components such as planners, knowledge bases, tools, and memory modules, as well as its interaction mechanisms. We also introduce a multi-agent framework with data retrieval, collaborative planning, and reflective evaluation for 6G. Subsequently, we provide a detailed overview of the applications of LAMs and Agentic AI in communication scenarios. Finally, we summarize the research challenges and future directions in current studies, aiming to support the development of efficient, secure, and sustainable next-generation intelligent communication systems. 

**Abstract (ZH)**: 6G 通信时代的智能通信系统中的大型人工智能模型与自主智能技术：原理、设计与应用综述 

---
# Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling 

**Title (ZH)**: 重新思考不可解的问题：当上下文搜索遇到测试时缩放问题 

**Authors**: Fanzeng Xia, Yidong Luo, Tinko Sebastian Bartels, Yaqi Xu, Tongxin Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.22290)  

**Abstract**: Recent research has highlighted that Large Language Models (LLMs), even when trained to generate extended long reasoning steps, still face significant challenges on hard reasoning problems. However, much of the existing literature relies on direct prompting with simple in-context learning examples for evaluation, which largely overlooks advanced techniques to elicit LLMs' deliberate reasoning before drawing conclusions that LLMs hit a performance ceiling. In this paper, we systematically explore the combined potential of in-context search and test-time scaling on super hard reasoning tasks. We find that by employing advanced in-context search prompting to LLMs augmented with internal scaling, one can achieve transformative performance breakthroughs on tasks previously deemed "unsolvable" (e.g., reported success rates below 5%). We provide both empirical results and theoretical analysis of how this combination can unleash LLM reasoning capabilities: i) Empirically, on controlled NP-hard tasks and complex real-world planning benchmarks, our approach achieves up to a 30x improvement in success rates compared to previously reported results without any external mechanisms; ii) Theoretically, we show that in-context search prompting, when combined with internal scaling, significantly extends the complexity class of solvable reasoning problems. These findings challenge prevailing assumptions about the limitations of LLMs on complex tasks, indicating that current evaluation paradigms systematically underestimate their true potential. Our work calls for a critical reassessment of how LLM reasoning is benchmarked and a more robust evaluation strategy that fully captures the true capabilities of contemporary LLMs, which can lead to a better understanding of their operational reasoning boundaries in real-world deployments. 

**Abstract (ZH)**: 近期的研究强调，即使大型语言模型（LLMs）被训练生成扩展的长推理步骤，它们在解决复杂推理问题时仍然面临显著挑战。然而，现有文献大多依赖简单的上下文关联提示进行评估，这很大程度上忽视了利用高级技术激发LLMs进行详细推理的重要性，尤其是当它们达到性能极限时。在本文中，我们系统地探索了上下文关联搜索与测试时缩放结合在超复杂推理任务上的潜在价值。我们发现，通过在集成内部缩放的LLMs中采用高级上下文关联搜索提示，可以在之前被认为“不可解”的任务上（例如，报告的成功率低于5%）实现颠覆性的性能突破。我们提供了关于这一结合如何释放LLMs推理能力的经验数据和理论分析：i) 在经验上，对于受控的NP难问题和复杂的现实世界规划基准，我们的方法在无需外部机制的情况下，实现了高达30倍的成功率提升；ii) 在理论上，我们证明了当结合内部缩放时，上下文关联搜索提示能够显著扩展可解决推理问题的复杂性类别。这些发现挑战了对LLMs在复杂任务中局限性的既定假设，表明当前的评价范式系统地低估了它们的真实潜力。我们的工作呼吁重新评估LLMs推理的基准测试方法，并提出一种更稳健的评价策略，以全面捕捉当代LLMs的真实能力，从而更好地理解其在实际部署中的操作推理边界。 

---
# Compression versus Accuracy: A Hierarchy of Lifted Models 

**Title (ZH)**: 压缩与准确性的权衡：提升模型的层次结构 

**Authors**: Jan Speller, Malte Luttermann, Marcel Gehrke, Tanya Braun  

**Link**: [PDF](https://arxiv.org/pdf/2505.22288)  

**Abstract**: Probabilistic graphical models that encode indistinguishable objects and relations among them use first-order logic constructs to compress a propositional factorised model for more efficient (lifted) inference. To obtain a lifted representation, the state-of-the-art algorithm Advanced Colour Passing (ACP) groups factors that represent matching distributions. In an approximate version using $\varepsilon$ as a hyperparameter, factors are grouped that differ by a factor of at most $(1\pm \varepsilon)$. However, finding a suitable $\varepsilon$ is not obvious and may need a lot of exploration, possibly requiring many ACP runs with different $\varepsilon$ values. Additionally, varying $\varepsilon$ can yield wildly different models, leading to decreased interpretability. Therefore, this paper presents a hierarchical approach to lifted model construction that is hyperparameter-free. It efficiently computes a hierarchy of $\varepsilon$ values that ensures a hierarchy of models, meaning that once factors are grouped together given some $\varepsilon$, these factors will be grouped together for larger $\varepsilon$ as well. The hierarchy of $\varepsilon$ values also leads to a hierarchy of error bounds. This allows for explicitly weighing compression versus accuracy when choosing specific $\varepsilon$ values to run ACP with and enables interpretability between the different models. 

**Abstract (ZH)**: 基于图的概率模型通过使用一阶逻辑构造来编码不可区分的对象及其关系，以压缩命题因子化模型，实现更高效的推理。最新的Advanced Colour Passing (ACP) 算法通过聚集表示匹配分布的因子来获得提升表示。在使用ε作为超参数的近似版本中，聚集的因子最多相差一个因子的（1±ε）。然而，找到合适的ε并不明显，并可能需要大量的探索，可能需要多次使用不同ε值运行ACP。此外，ε的变化可能会生成差异极大的模型，导致可解释性降低。因此，本文提出了一种无超参数的层次化方法来构建提升模型。该方法有效地计算了一系列ε值的层次结构，确保了模型层次结构的存在，即在给定某个ε值聚集的因子，在较大的ε值下也会被聚集在一起。ε值的层次结构也导致了误差界层次结构。这使得在选择用于运行ACP的具体ε值时能够显式权衡压缩与精度，并且可以在不同的模型之间实现可解释性。 

---
# A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives 

**Title (ZH)**: 包含相关目标的高效近似双目标最短路径预处理框架 

**Authors**: Yaron Halle, Ariel Felner, Sven Koenig, Oren Salzman  

**Link**: [PDF](https://arxiv.org/pdf/2505.22244)  

**Abstract**: The bi-objective shortest-path (BOSP) problem seeks to find paths between start and target vertices of a graph while optimizing two conflicting objective functions. We consider the BOSP problem in the presence of correlated objectives. Such correlations often occur in real-world settings such as road networks, where optimizing two positively correlated objectives, such as travel time and fuel consumption, is common. BOSP is generally computationally challenging as the size of the search space is exponential in the number of objective functions and the graph size. Bounded sub-optimal BOSP solvers such as A*pex alleviate this complexity by approximating the Pareto-optimal solution set rather than computing it exactly (given a user-provided approximation factor). As the correlation between objective functions increases, smaller approximation factors are sufficient for collapsing the entire Pareto-optimal set into a single solution. We leverage this insight to propose an efficient algorithm that reduces the search effort in the presence of correlated objectives. Our approach for computing approximations of the entire Pareto-optimal set is inspired by graph-clustering algorithms. It uses a preprocessing phase to identify correlated clusters within a graph and to generate a new graph representation. This allows a natural generalization of A*pex to run up to five times faster on DIMACS dataset instances, a standard benchmark in the field. To the best of our knowledge, this is the first algorithm proposed that efficiently and effectively exploits correlations in the context of bi-objective search while providing theoretical guarantees on solution quality. 

**Abstract (ZH)**: 带相关目标函数的双目标最短路径问题（BOSP）旨在在一个图中找到从起始顶点到目标顶点的路径，同时优化两个相互冲突的目标函数。在目标函数相关的情况下考虑BOSP问题。这种相关性在现实世界中很常见，例如在道路网络中，优化两个正相关的目标函数（如旅行时间和燃料消耗）是常见的。BOSP通常在计算上具有挑战性，因为搜索空间的大小随着目标函数的数量和图的大小呈指数增长。通过提供用户指定的近似因子，有界的近似BOSP求解器（如A*pex）可以通过近似帕累托最优解集而不是计算其精确值来缓解这种复杂性。随着目标函数之间的相关性增强，较小的近似因子即可将整个帕累托最优集合压缩为单个解决方案。我们利用这一洞见提出了一种高效算法，在目标函数相关的情况下减少搜索努力。我们计算整个帕累托最优集的近似值的方法受到图聚类算法的启发。它使用预处理阶段来识别图中的相关聚类并生成新的图表示。这使得A*pex的自然推广在DIMACS数据集实例上运行速度可提高五倍，这是一个领域内的标准基准。据我们所知，这是第一个在考虑双目标搜索的同时有效地利用相关性并提供关于解质量的理论保证的算法。 

---
# What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning 

**Title (ZH)**: 什么是良好的推理链？揭示长链式推理中的结构模式 

**Authors**: Gangwei Jiang, Yahui Liu, Zhaoyi Li, Qi Wang, Fuzheng Zhang, Linqi Song, Ying Wei, Defu Lian  

**Link**: [PDF](https://arxiv.org/pdf/2505.22148)  

**Abstract**: Recent advances in reasoning with large language models (LLMs) have popularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate and step-by-step reasoning before producing a final answer. While LCoTs have enabled expert-level performance in complex tasks, how the internal structures of their reasoning chains drive, or even predict, the correctness of final answers remains a critical yet underexplored question. In this work, we present LCoT2Tree, an automated framework that converts sequential LCoTs into hierarchical tree structures and thus enables deeper structural analysis of LLM reasoning. Using graph neural networks (GNNs), we reveal that structural patterns extracted by LCoT2Tree, including exploration, backtracking, and verification, serve as stronger predictors of final performance across a wide range of tasks and models. Leveraging an explainability technique, we further identify critical thought patterns such as over-branching that account for failures. Beyond diagnostic insights, the structural patterns by LCoT2Tree support practical applications, including improving Best-of-N decoding effectiveness. Overall, our results underscore the critical role of internal structures of reasoning chains, positioning LCoT2Tree as a powerful tool for diagnosing, interpreting, and improving reasoning in LLMs. 

**Abstract (ZH)**: Recent advances in reasoning with large language models (LLMs) have popularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate and step-by-step reasoning before producing a final answer. While LCoTs have enabled expert-level performance in complex tasks, how the internal structures of their reasoning chains drive, or even predict, the correctness of final answers remains a critical yet underexplored question. In this work, we present LCoT2Tree, an automated framework that converts sequential LCoTs into hierarchical tree structures and thus enables deeper structural analysis of LLM reasoning. Using graph neural networks (GNNs), we reveal that structural patterns extracted by LCoT2Tree, including exploration, backtracking, and verification, serve as stronger predictors of final performance across a wide range of tasks and models. Leveraging an explainability technique, we further identify critical thought patterns such as over-branching that account for failures. Beyond diagnostic insights, the structural patterns by LCoT2Tree support practical applications, including improving Best-of-N decoding effectiveness. Overall, our results underscore the critical role of internal structures of reasoning chains, positioning LCoT2Tree as a powerful tool for diagnosing, interpreting, and improving reasoning in LLMs. 

---
# Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions 

**Title (ZH)**: 关系因子化马尔可夫决策过程中的提升向前规划与并发动作 

**Authors**: Florian Andreas Marwitz, Tanya Braun, Ralf Möller, Marcel Gehrke  

**Link**: [PDF](https://arxiv.org/pdf/2505.22147)  

**Abstract**: Decision making is a central problem in AI that can be formalized using a Markov Decision Process. A problem is that, with increasing numbers of (indistinguishable) objects, the state space grows exponentially. To compute policies, the state space has to be enumerated. Even more possibilities have to be enumerated if the size of the action space depends on the size of the state space, especially if we allow concurrent actions. To tackle the exponential blow-up in the action and state space, we present a first-order representation to store the spaces in polynomial instead of exponential size in the number of objects and introduce Foreplan, a relational forward planner, which uses this representation to efficiently compute policies for numerous indistinguishable objects and actions. Additionally, we introduce an even faster approximate version of Foreplan. Moreover, Foreplan identifies how many objects an agent should act on to achieve a certain task given restrictions. Further, we provide a theoretical analysis and an empirical evaluation of Foreplan, demonstrating a speedup of at least four orders of magnitude. 

**Abstract (ZH)**: 面向可分辨对象的马尔可夫决策过程的一阶表示与Foreplan关系式前瞻规划算法的研究及其快速近似版本 explore a first-order representation for storing the spaces polynomially instead of exponentially in the number of objects and introduce Foreplan, a relational forward planner, which efficiently computes policies for numerous indistinguishable objects and actions. Additionally, we introduce an even faster approximate version of Foreplan. Moreover, Foreplan identifies how many objects an agent should act on to achieve a certain task given restrictions. Further, we provide a theoretical analysis and an empirical evaluation of Foreplan, demonstrating a speedup of at least four orders of magnitude. 

---
# Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test 

**Title (ZH)**: 视觉大语言模型在 Wisconsin 卡特排序测试中表现出人类水平的认知灵活性 

**Authors**: Guangfu Hao, Frederic Alexandre, Shan Yu  

**Link**: [PDF](https://arxiv.org/pdf/2505.22112)  

**Abstract**: Cognitive flexibility has been extensively studied in human cognition but remains relatively unexplored in the context of Visual Large Language Models (VLLMs). This study assesses the cognitive flexibility of state-of-the-art VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card Sorting Test (WCST), a classic measure of set-shifting ability. Our results reveal that VLLMs achieve or surpass human-level set-shifting capabilities under chain-of-thought prompting with text-based inputs. However, their abilities are highly influenced by both input modality and prompting strategy. In addition, we find that through role-playing, VLLMs can simulate various functional deficits aligned with patients having impairments in cognitive flexibility, suggesting that VLLMs may possess a cognitive architecture, at least regarding the ability of set-shifting, similar to the brain. This study reveals the fact that VLLMs have already approached the human level on a key component underlying our higher cognition, and highlights the potential to use them to emulate complex brain processes. 

**Abstract (ZH)**: 视觉大型语言模型的认知灵活性：基于威斯康星卡片分类测验的探究 

---
# Efficient Dynamic Shielding for Parametric Safety Specifications 

**Title (ZH)**: 参数安全规范的高效动态屏蔽 

**Authors**: Davide Corsi, Kaushik Mallik, Andoni Rodriguez, Cesar Sanchez  

**Link**: [PDF](https://arxiv.org/pdf/2505.22104)  

**Abstract**: Shielding has emerged as a promising approach for ensuring safety of AI-controlled autonomous systems. The algorithmic goal is to compute a shield, which is a runtime safety enforcement tool that needs to monitor and intervene the AI controller's actions if safety could be compromised otherwise. Traditional shields are designed statically for a specific safety requirement. Therefore, if the safety requirement changes at runtime due to changing operating conditions, the shield needs to be recomputed from scratch, causing delays that could be fatal. We introduce dynamic shields for parametric safety specifications, which are succinctly represented sets of all possible safety specifications that may be encountered at runtime. Our dynamic shields are statically designed for a given safety parameter set, and are able to dynamically adapt as the true safety specification (permissible by the parameters) is revealed at runtime. The main algorithmic novelty lies in the dynamic adaptation procedure, which is a simple and fast algorithm that utilizes known features of standard safety shields, like maximal permissiveness. We report experimental results for a robot navigation problem in unknown territories, where the safety specification evolves as new obstacles are discovered at runtime. In our experiments, the dynamic shields took a few minutes for their offline design, and took between a fraction of a second and a few seconds for online adaptation at each step, whereas the brute-force online recomputation approach was up to 5 times slower. 

**Abstract (ZH)**: 动态参数化安全规范下的自适应屏蔽技术 

---
# VIRAL: Vision-grounded Integration for Reward design And Learning 

**Title (ZH)**: 基于视觉的奖励设计与学习的集成方法 

**Authors**: Valentin Cuzin-Rambaud, Emilien Komlenovic, Alexandre Faure, Bruno Yun  

**Link**: [PDF](https://arxiv.org/pdf/2505.22092)  

**Abstract**: The alignment between humans and machines is a critical challenge in artificial intelligence today. Reinforcement learning, which aims to maximize a reward function, is particularly vulnerable to the risks associated with poorly designed reward functions. Recent advancements has shown that Large Language Models (LLMs) for reward generation can outperform human performance in this context. We introduce VIRAL, a pipeline for generating and refining reward functions through the use of multi-modal LLMs. VIRAL autonomously creates and interactively improves reward functions based on a given environment and a goal prompt or annotated image. The refinement process can incorporate human feedback or be guided by a description generated by a video LLM, which explains the agent's policy in video form. We evaluated VIRAL in five Gymnasium environments, demonstrating that it accelerates the learning of new behaviors while ensuring improved alignment with user intent. The source-code and demo video are available at: this https URL and this https URL. 

**Abstract (ZH)**: 人类与机器的协同一致是当前人工智能领域的一项关键挑战。强化学习旨在最大化奖励函数，特别容易受到设计不佳的奖励函数带来的风险影响。近期研究表明，用于奖励生成的大规模语言模型（LLMs）可以在这一方面超越人类性能。我们介绍了VIRAL，一种利用多模态LLMs生成和优化奖励函数的流水线。VIRAL自主创建并基于给定环境和目标提示或标注图像交互式地改进奖励函数。优化过程可以 Incorporate 人类反馈，或者由视频LLMs生成的描述指导，该描述以视频形式解释代理的策略。我们在五个Gymnasium环境中评估了VIRAL，结果显示它加速了新行为的学习，同时确保了更好的用户意图对齐。源代码和演示视频可在以下链接获取：this https URL 和 this https URL。 

---
# Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired 

**Title (ZH)**: 基于知识图谱的认知启发式 Emergent 通信辅助视障人士 

**Authors**: Ruxiao Chen, Dezheng Han, Wenjie Han, Shuaishuai Guo  

**Link**: [PDF](https://arxiv.org/pdf/2505.22087)  

**Abstract**: Assistive systems for visually impaired individuals must deliver rapid, interpretable, and adaptive feedback to facilitate real-time navigation. Current approaches face a trade-off between latency and semantic richness: natural language-based systems provide detailed guidance but are too slow for dynamic scenarios, while emergent communication frameworks offer low-latency symbolic languages but lack semantic depth, limiting their utility in tactile modalities like vibration. To address these limitations, we introduce a novel framework, Cognitively-Inspired Emergent Communication via Knowledge Graphs (VAG-EC), which emulates human visual perception and cognitive mapping. Our method constructs knowledge graphs to represent objects and their relationships, incorporating attention mechanisms to prioritize task-relevant entities, thereby mirroring human selective attention. This structured approach enables the emergence of compact, interpretable, and context-sensitive symbolic languages. Extensive experiments across varying vocabulary sizes and message lengths demonstrate that VAG-EC outperforms traditional emergent communication methods in Topographic Similarity (TopSim) and Context Independence (CI). These findings underscore the potential of cognitively grounded emergent communication as a fast, adaptive, and human-aligned solution for real-time assistive technologies. Code is available at this https URL. 

**Abstract (ZH)**: 基于知识图谱的认知启发式新兴通信（VAG-EC）：促进视障个体的实时导航 

---
# Reinforced Reasoning for Embodied Planning 

**Title (ZH)**: 强化推理在体域规划中的应用 

**Authors**: Di Wu, Jiaxin Fan, Junzhe Zang, Guanbo Wang, Wei Yin, Wenhao Li, Bo Jin  

**Link**: [PDF](https://arxiv.org/pdf/2505.22050)  

**Abstract**: Embodied planning requires agents to make coherent multi-step decisions based on dynamic visual observations and natural language goals. While recent vision-language models (VLMs) excel at static perception tasks, they struggle with the temporal reasoning, spatial understanding, and commonsense grounding needed for planning in interactive environments. In this work, we introduce a reinforcement fine-tuning framework that brings R1-style reasoning enhancement into embodied planning. We first distill a high-quality dataset from a powerful closed-source model and perform supervised fine-tuning (SFT) to equip the model with structured decision-making priors. We then design a rule-based reward function tailored to multi-step action quality and optimize the policy via Generalized Reinforced Preference Optimization (GRPO). Our approach is evaluated on Embench, a recent benchmark for interactive embodied tasks, covering both in-domain and out-of-domain scenarios. Experimental results show that our method significantly outperforms models of similar or larger scale, including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong generalization to unseen environments. This work highlights the potential of reinforcement-driven reasoning to advance long-horizon planning in embodied AI. 

**Abstract (ZH)**: 基于感知的规划需要代理基于动态视觉观察和自然语言目标做出连贯的多步决策。尽管近期的跨模态模型在静态感知任务上表现出色，但在需要规划的时间推理、空间理解以及常识绑定的交互环境中，它们仍然存在困难。在本工作中，我们引入了一种强化微调框架，将R1风格的推理增强引入到基于感知的规划中。我们首先从一个强大的闭源模型中提取高质量的数据集，并进行监督微调（SFT）以赋予模型结构化的决策先验。然后，我们设计了一个基于规则的奖励函数，以适应多步行动质量的优化，并通过广义强化偏好优化（GRPO）优化策略。我们的方法在Embench上进行了评估，这是一个最近提出的交互式基于感知任务基准，涵盖了领域内和领域外场景。实验结果表明，我们的方法显著优于类似规模或更大规模的模型，包括GPT-4o-mini和70B+开源基线，并且在未见过的环境中表现出强大的泛化能力。本工作突显了基于强化学习的推理对于推进基于感知的AI长期规划的潜在价值。 

---
# Efficiently Enhancing General Agents With Hierarchical-categorical Memory 

**Title (ZH)**: 高效增强通用代理的层级分类记忆 

**Authors**: Changze Qiao, Mingming Lu  

**Link**: [PDF](https://arxiv.org/pdf/2505.22006)  

**Abstract**: With large language models (LLMs) demonstrating remarkable capabilities, there has been a surge in research on leveraging LLMs to build general-purpose multi-modal agents. However, existing approaches either rely on computationally expensive end-to-end training using large-scale multi-modal data or adopt tool-use methods that lack the ability to continuously learn and adapt to new environments. In this paper, we introduce EHC, a general agent capable of learning without parameter updates. EHC consists of a Hierarchical Memory Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL) module. The HMR module facilitates rapid retrieval of relevant memories and continuously stores new information without being constrained by memory capacity. The TOEL module enhances the agent's comprehension of various task characteristics by classifying experiences and extracting patterns across different categories. Extensive experiments conducted on multiple standard datasets demonstrate that EHC outperforms existing methods, achieving state-of-the-art performance and underscoring its effectiveness as a general agent for handling complex multi-modal tasks. 

**Abstract (ZH)**: 基于大规模语言模型的通用多模态代理学习方法：EHC 

---
# Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism 

**Title (ZH)**: 逻辑子图的功能匹配：超越结构性同构 

**Authors**: Ziyang Zheng, Kezhi Li, Zhengyuan Shi, Qiang Xu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21988)  

**Abstract**: Subgraph matching in logic circuits is foundational for numerous Electronic Design Automation (EDA) applications, including datapath optimization, arithmetic verification, and hardware trojan detection. However, existing techniques rely primarily on structural graph isomorphism and thus fail to identify function-related subgraphs when synthesis transformations substantially alter circuit topology. To overcome this critical limitation, we introduce the concept of functional subgraph matching, a novel approach that identifies whether a given logic function is implicitly present within a larger circuit, irrespective of structural variations induced by synthesis or technology mapping. Specifically, we propose a two-stage multi-modal framework: (1) learning robust functional embeddings across AIG and post-mapping netlists for functional subgraph detection, and (2) identifying fuzzy boundaries using a graph segmentation approach. Evaluations on standard benchmarks (ITC99, OpenABCD, ForgeEDA) demonstrate significant performance improvements over existing structural methods, with average $93.8\%$ accuracy in functional subgraph detection and a dice score of $91.3\%$ in fuzzy boundary identification. 

**Abstract (ZH)**: 逻辑电路中的子图匹配是许多电子设计自动化(EDA)应用的基础，包括数据路径优化、算术验证和硬件木马检测。然而，现有技术主要依赖于结构性图同构，因此在综合变换显著改变电路拓扑时，无法识别功能相关的子图。为克服这一关键局限，我们引入了功能子图匹配的概念，这是一种新的方法，可以在忽略合成或技术映射诱导的结构性变化的情况下，确定给定逻辑函数是否隐含地存在于更大的电路中。具体地，我们提出了一种两阶段多模态框架：（1）学习AIG和后映射网表之间鲁棒的功能嵌入以进行功能子图检测；（2）使用图分割方法识别模糊边界。标准基准（ITC99、OpenABCD、ForgeEDA）上的评估证明了相对于现有结构性方法的显著性能提升，在功能子图检测中的平均准确率为93.8%，模糊边界识别的骰子分数为91.3%。 

---
# From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models 

**Title (ZH)**: 从推理到学习：大规模语言模型在假设发现和规则学习方面的综述 

**Authors**: Kaiyu He, Zhiyu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2505.21935)  

**Abstract**: Since the advent of Large Language Models (LLMs), efforts have largely focused on improving their instruction-following and deductive reasoning abilities, leaving open the question of whether these models can truly discover new knowledge. In pursuit of artificial general intelligence (AGI), there is a growing need for models that not only execute commands or retrieve information but also learn, reason, and generate new knowledge by formulating novel hypotheses and theories that deepen our understanding of the world. Guided by Peirce's framework of abduction, deduction, and induction, this survey offers a structured lens to examine LLM-based hypothesis discovery. We synthesize existing work in hypothesis generation, application, and validation, identifying both key achievements and critical gaps. By unifying these threads, we illuminate how LLMs might evolve from mere ``information executors'' into engines of genuine innovation, potentially transforming research, science, and real-world problem solving. 

**Abstract (ZH)**: 自大型语言模型（LLMs）出现以来，努力主要集中在提高其指令遵循和演绎推理能力上，这为这些模型是否能真正发现新知识留下了疑问。为了追求人工通用智能（AGI），对于不仅能执行命令或检索信息，还能学习、推理并生成新知识（通过提出新颖的假设和理论深化我们对世界的理解）的模型的需求日益增长。基于佩里框架中的 abduction、deduction 和 induction，本综合调查提供了一种结构化的视角来审视基于LLM的假设发现。我们综合了假设生成、应用和验证方面的现有工作，识别出了关键成就和关键缺口。通过统一这些线索，我们阐明了LLM如何从单纯的“信息执行者”进化为真正创新的引擎，有可能彻底改变研究、科学和现实世界问题解决的面貌。 

---
# Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy 

**Title (ZH)**: AI副驾中用户偏好建模与优化：综述与分类框架 

**Authors**: Saleh Afzoon, Zahra Jahanandish, Phuong Thao Huynh, Amin Beheshti, Usman Naseem  

**Link**: [PDF](https://arxiv.org/pdf/2505.21907)  

**Abstract**: AI copilots, context-aware, AI-powered systems designed to assist users in tasks such as software development and content creation, are becoming integral to modern workflows. As these systems grow in capability and adoption, personalization has emerged as a cornerstone for ensuring usability, trust, and productivity. Central to this personalization is preference optimization: the ability of AI copilots to detect, interpret, and align with individual user preferences. While personalization techniques are well-established in domains like recommender systems and dialogue agents, their adaptation to interactive, real-time systems like AI copilots remains fragmented and underexplored. This survey addresses this gap by synthesizing research on how user preferences are captured, modeled, and refined within the design of AI copilots. We introduce a unified definition of AI copilots and propose a phase-based taxonomy of preference optimization strategies, structured around pre-interaction, mid-interaction, and post-interaction stages. We analyze techniques for acquiring preference signals, modeling user intent, and integrating feedback loops, highlighting both established approaches and recent innovations. By bridging insights from AI personalization, human-AI collaboration, and large language model adaptation, this survey provides a structured foundation for designing adaptive, preference-aware AI copilots. It offers a holistic view of the available preference resources, how they can be leveraged, and which technical approaches are most suited to each stage of system design. 

**Abstract (ZH)**: AI 配飞员：一种为软件开发和内容创作等任务提供辅助的上下文感知型 AI 力量倍增器，正逐渐成为现代工作流程的核心组成部分。随着这些系统的功能和普及率提升，个性化逐渐成为确保可用性、可信度和生产力的基础。个性化的核心在于偏好优化：即 AI 配飞员检测、解释并与个体用户偏好保持一致的能力。虽然在推荐系统和对话代理领域，个性化技术已经相当成熟，但将其应用于交互性强、实时性高的系统如 AI 配飞员仍存在碎片化和未充分探索的情况。本综述通过综合分析如何在 AI 配飞员的设计中捕获、建模和提炼用户偏好，填补了这一空白。我们提出了统一的 AI 配飞员定义，并提出了一种基于前期交互、中期交互和后期交互阶段的偏好优化策略分类框架。我们分析了偏好信号获取、用户意图建模以及反馈循环整合的技术方法，既包括已建立的方法也涵盖了最新创新。通过结合AI个性化、人类-AI协作以及大型语言模型适应领域的洞察，本综述为设计自适应和偏好感知的AI配飞员提供了结构化的基础。它提供了一个全面的偏好资源视图，说明了如何利用这些资源以及哪些技术方法最适用于系统设计的各个阶段。 

---
# SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem 

**Title (ZH)**: SVRPBench: 一种 stochastic 车辆路径问题的现实基准 

**Authors**: Ahmed Heakl, Yahia Salaheldin Shaaban, Martin Takac, Salem Lahlou, Zangir Iklassov  

**Link**: [PDF](https://arxiv.org/pdf/2505.21887)  

**Abstract**: Robust routing under uncertainty is central to real-world logistics, yet most benchmarks assume static, idealized settings. We present SVRPBench, the first open benchmark to capture high-fidelity stochastic dynamics in vehicle routing at urban scale. Spanning more than 500 instances with up to 1000 customers, it simulates realistic delivery conditions: time-dependent congestion, log-normal delays, probabilistic accidents, and empirically grounded time windows for residential and commercial clients. Our pipeline generates diverse, constraint-rich scenarios, including multi-depot and multi-vehicle setups. Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade by over 20% under distributional shift, while classical and metaheuristic methods remain robust. To enable reproducible research, we release the dataset and evaluation suite. SVRPBench challenges the community to design solvers that generalize beyond synthetic assumptions and adapt to real-world uncertainty. 

**Abstract (ZH)**: 鲁棒路由在不确定性下的优化对于实际物流至关重要，然而大多数基准假设静态的理想化环境。我们提出了SVRPBench，这是首个能够捕捉大规模城市环境中车辆路由的高保真随机动态的开放基准。该基准涵盖超过500个实例，最多包含1000个客户，模拟了现实的配送条件：时间依赖性强的拥堵、对数正态分布的延误、概率事故以及基于实地数据的时间窗口，适用于住宅和商业客户。我们的流程生成具有多种约束、复杂多样的场景，包括多仓库和多车辆配置。基准测试表明，在分布偏移下，最先进的RL求解器如POMO和AM性能下降超过20%，而经典和元启发式方法仍然稳健。为了促进可重复研究，我们发布了该数据集和评估套件。SVRPBench 挑战社区设计能够在超越合成假设的情况下泛化的求解器，并适应现实世界的不确定性。 

---
# SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts 

**Title (ZH)**: SAGE-Eval: 评估大规模语言模型在安全事实系统性概括方面的能力 

**Authors**: Chen Yueh-Han, Guy Davidson, Brenden M. Lake  

**Link**: [PDF](https://arxiv.org/pdf/2505.21828)  

**Abstract**: Do LLMs robustly generalize critical safety facts to novel situations? Lacking this ability is dangerous when users ask naive questions. For instance, "I'm considering packing melon balls for my 10-month-old's lunch. What other foods would be good to include?" Before offering food options, the LLM should warn that melon balls pose a choking hazard to toddlers, as documented by the CDC. Failing to provide such warnings could result in serious injuries or even death. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic GEneralization evaluation, the first benchmark that tests whether LLMs properly apply well established safety facts to naive user queries. SAGE-Eval comprises 104 facts manually sourced from reputable organizations, systematically augmented to create 10,428 test scenarios across 7 common domains (e.g., Outdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet, passes only 58% of all the safety facts tested. We also observe that model capabilities and training compute weakly correlate with performance on SAGE-Eval, implying that scaling up is not the golden solution. Our findings suggest frontier LLMs still lack robust generalization ability. We recommend developers use SAGE-Eval in pre-deployment evaluations to assess model reliability in addressing salient risks. We publicly release SAGE-Eval at this https URL and our code is available at this https URL. 

**Abstract (ZH)**: Do LLMs robustly generalize critical safety facts to novel situations? 

---
# Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation 

**Title (ZH)**: 面向LLMs的安全推理：嵌入政策的CoT数据创建中的AI-代理 deliberation 

**Authors**: Tharindu Kumarage, Ninareh Mehrabi, Anil Ramakrishna, Xinyan Zhao, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta, Charith Peris  

**Link**: [PDF](https://arxiv.org/pdf/2505.21784)  

**Abstract**: Safety reasoning is a recent paradigm where LLMs reason over safety policies before generating responses, thereby mitigating limitations in existing safety measures such as over-refusal and jailbreak vulnerabilities. However, implementing this paradigm is challenging due to the resource-intensive process of creating high-quality policy-embedded chain-of-thought (CoT) datasets while ensuring reasoning remains accurate and free from hallucinations or policy conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation for Safety Reasoning, a novel data generation recipe that leverages multi-agent deliberation to iteratively expand reasoning on safety policies. A data refiner stage in AIDSAFE ensures high-quality outputs by eliminating repetitive, redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong foundation for supervised fine-tuning (SFT)-based safety training. Additionally, to address the need of preference data in alignment stages, such as DPO training, we introduce a supplemental recipe that uses belief augmentation to create distinct selected and rejected CoT samples. Our evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy adherence and reasoning quality. Consequently, we show that fine-tuning open-source LLMs on these CoTs can significantly improve safety generalization and jailbreak robustness while maintaining acceptable utility and over-refusal accuracy. AIDSAFE-generated CoT datasets can be found here: this https URL 

**Abstract (ZH)**: 安全推理是一种近期的范式，其中LLMs在生成响应前会对安全策略进行推理，从而缓解现有安全措施中的限制，如过度拒绝和规避漏洞。然而，实施这一范式具有挑战性，因为需要创建高质量的嵌入策略的链式思考数据集，同时确保推理过程的准确性和无幻觉性或策略冲突性。为解决这一问题，我们提出了一种名为AIDSAFE的新颖数据生成方案：代理迭代审议用于安全推理，该方案利用多代理审议来迭代扩展对安全策略的推理。AIDSAFE中的数据精炼阶段通过消除重复、冗余和虚假的思考来确保高质量的输出。AIDSAFE生成的链式思考为基于监督微调的安全训练提供了坚实的基础。此外，为了解决对齐阶段（如DPO训练）中需要偏好数据的需求，我们提出了一种补充方案，利用信念增强生成具有区别的接受和拒绝的链式思考样本。评估结果表明，AIDSAFE生成的链式思考在政策遵循和推理质量方面表现出优越性。因此，我们展示了在这些链式思考上微调开源LLM可以显著提高安全性泛化和规避鲁棒性，同时保持可接受的实用性和过度拒绝准确性。AIDSAFE生成的链式思考数据集可在这里找到：this https URL。 

---
# Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models 

**Title (ZH)**: 不要思考更久，而是明智思考：优化大推理模型的思维动态 

**Authors**: Sohyun An, Ruochen Wang, Tianyi Zhou, Cho-Jui Hsieh  

**Link**: [PDF](https://arxiv.org/pdf/2505.21765)  

**Abstract**: While recent success of large reasoning models (LRMs) significantly advanced LLMs' reasoning capability by optimizing the final answer accuracy using reinforcement learning, they may also drastically increase the output length due to overthinking, characterized by unnecessarily complex reasoning paths that waste computation and potentially degrade the performance. We hypothesize that such inefficiencies stem from LRMs' limited capability to dynamically select the proper modular reasoning strategies, termed thinking patterns at the right position. To investigate this hypothesis, we propose a dynamic optimization framework that segments model-generated reasoning paths into distinct thinking patterns, systematically identifying and promoting beneficial patterns that improve the answer while removing detrimental ones. Empirical analysis confirms that our optimized thinking paths yield more concise yet sufficiently informative trajectories, enhancing reasoning efficiency by reducing attention FLOPs by up to 47% while maintaining accuracy for originally correct responses. Moreover, a non-trivial portion of originally incorrect responses are transformed into correct ones, achieving a 15.6% accuracy improvement with reduced length. Motivated by the improvement brought by the optimized thinking paths, we apply a preference optimization technique supported by a pairwise dataset contrasting suboptimal and optimal reasoning paths. Experimental evaluations across multiple mathematical reasoning benchmarks reveal that our method notably reduces computational overhead while simultaneously improving reasoning accuracy, achieving up to a 12% accuracy improvement and reducing token usage from approximately 5,000 to 3,000 tokens. 

**Abstract (ZH)**: 虽然大型推理模型（LRMs）的recent success 显著提高了LLMs的推理能力，通过强化学习优化最终答案的准确性，但也可能因过度推理而导致输出长度大幅增加，表现为不必要的复杂推理路径，浪费计算资源并可能降低性能。我们假设这种低效率源自LRMs有限的动态选择合适模块推理策略的能力，即在正确位置选择正确的思维模式。为了验证这一假设，我们提出了一种动态优化框架，将模型生成的推理路径划分为不同的思维模式，系统地识别和促进有益的模式，同时去除有害的模式。实证分析证实，我们的优化思维路径产生了更加简洁但仍然足够有信息量的轨迹，通过降低注意力FLOPs高达47%的同时保持准确性，增强了推理效率。此外，相当一部分原本错误的回答被转化为正确的回答，实现准确率提高15.6%，同时缩短了长度。受优化思维路径带来的改进启发，我们利用一个对比次优和最优推理路径的成对数据集，应用偏好优化技术。在多个数学推理基准上的实验评估表明，我们的方法不仅显著减少了计算开销，而且同时提高了推理准确性，实现了高达12%的准确率改进，并将token使用量从约5,000减少到3,000。 

---
# Make Planning Research Rigorous Again! 

**Title (ZH)**: 重新使规划研究严谨起来！ 

**Authors**: Michael Katz, Harsha Kokel, Christian Muise, Shirin Sohrabi, Sarath Sreedharan  

**Link**: [PDF](https://arxiv.org/pdf/2505.21674)  

**Abstract**: In over sixty years since its inception, the field of planning has made significant contributions to both the theory and practice of building planning software that can solve a never-before-seen planning problem. This was done through established practices of rigorous design and evaluation of planning systems. It is our position that this rigor should be applied to the current trend of work on planning with large language models. One way to do so is by correctly incorporating the insights, tools, and data from the automated planning community into the design and evaluation of LLM-based planners. The experience and expertise of the planning community are not just important from a historical perspective; the lessons learned could play a crucial role in accelerating the development of LLM-based planners. This position is particularly important in light of the abundance of recent works that replicate and propagate the same pitfalls that the planning community has encountered and learned from. We believe that avoiding such known pitfalls will contribute greatly to the progress in building LLM-based planners and to planning in general. 

**Abstract (ZH)**: 自六十多年前起步以来，规划领域的研究为构建能够解决前所未见规划问题的规划软件理论与实践做出了重要贡献。这一成就通过严谨的设计和评估规划系统的传统得以实现。我们认为，这种严谨性也应该应用于当前基于大语言模型的规划研究趋势。通过正确地将自动规划社区的见解、工具和数据融入基于大语言模型的规划者的设计和评估中，可以实现这一点。规划社区的经验和专业知识不仅具有历史意义，从中学到的教训在加速基于大语言模型的规划者的发展中也起着关键作用。鉴于近期大量复制和传播规划社区已遭遇和吸取教训的错误作品，这种观点尤为重要。我们认为，避免这些已知的错误将极大地促进基于大语言模型的规划者和规划本身的进步。 

---
# Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing 

**Title (ZH)**: 基于网络的疾病检测中的图前沿探索自适应方法 

**Authors**: Davin Choo, Yuqi Pan, Tonghan Wang, Milind Tambe, Alastair van Heerden, Cheryl Johnson  

**Link**: [PDF](https://arxiv.org/pdf/2505.21671)  

**Abstract**: We study a sequential decision-making problem on a $n$-node graph $G$ where each node has an unknown label from a finite set $\mathbf{\Sigma}$, drawn from a joint distribution $P$ that is Markov with respect to $G$. At each step, selecting a node reveals its label and yields a label-dependent reward. The goal is to adaptively choose nodes to maximize expected accumulated discounted rewards. We impose a frontier exploration constraint, where actions are limited to neighbors of previously selected nodes, reflecting practical constraints in settings such as contact tracing and robotic exploration. We design a Gittins index-based policy that applies to general graphs and is provably optimal when $G$ is a forest. Our implementation runs in $O(n^2 \cdot |\mathbf{\Sigma}|^2)$ time while using $O(n \cdot |\mathbf{\Sigma}|^2)$ oracle calls to $P$ and $O(n^2 \cdot |\mathbf{\Sigma}|)$ space. Experiments on synthetic and real-world graphs show that our method consistently outperforms natural baselines, including in non-tree, budget-limited, and undiscounted settings. For example, in HIV testing simulations on real-world sexual interaction networks, our policy detects nearly all positive cases with only half the population tested, substantially outperforming other baselines. 

**Abstract (ZH)**: 我们研究了一个$n$节点图$G$上的序贯决策问题，其中每个节点具有来自有限集$\mathbf{\Sigma}$的未知标签，标签根据与$G$相关的马尔可夫分布$P$进行抽取。在每一步中，选择一个节点会揭示其标签并产生与标签相关的奖励。目标是自适应地选择节点以最大化预期累积折扣奖励。我们提出了一个前沿探索约束，其中行动限于先前选择节点的邻居，这反映了接触追踪和机器人探索等实际场景中的约束。我们设计了一种基于Gittins指数的策略，该策略适用于一般图，在$G$是森林的情况下可以证明是最优的。我们的实现运行时间为$O(n^2 \cdot |\mathbf{\Sigma}|^2)$，使用$O(n \cdot |\mathbf{\Sigma}|^2)$次对$P$的查询，并使用$O(n^2 \cdot |\mathbf{\Sigma}|)$的空间。在合成和真实世界图上的实验表明，我们的方法在各种场景下（包括非树结构、预算限制以及未贴现奖励设置）都能持续优于自然基准方法。例如，在基于实际性行为网络的HIV检测模拟中，我们的策略仅对半个群体进行检测即可发现几乎所有阳性病例，显著优于其他基准方法。 

---
# R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning 

**Title (ZH)**: R1-Code-Interpreter: 通过监督学习和强化学习训练大语言模型进行代码推理 

**Authors**: Yongchao Chen, Yueying Liu, Junwei Zhou, Yilun Hao, Jingquan Wang, Yang Zhang, Chuchu Fan  

**Link**: [PDF](https://arxiv.org/pdf/2505.21668)  

**Abstract**: Despite advances in reasoning and planning of R1-like models, Large Language Models (LLMs) still struggle with tasks requiring precise computation, symbolic manipulation, optimization, and algorithmic reasoning, in which textual reasoning lacks the rigor of code execution. A key challenge is enabling LLMs to decide when to use textual reasoning versus code generation. While OpenAI trains models to invoke a Code Interpreter as needed, public research lacks guidance on aligning pre-trained LLMs to effectively leverage code and generalize across diverse tasks. We present R1-Code-Interpreter, an extension of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) to autonomously generate multiple code queries during step-by-step reasoning. We curate 144 reasoning and planning tasks (107 for training, 37 for testing), each with over 200 diverse questions. We fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies, investigating different answer formats, reasoning vs. non-reasoning models, cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs. Unlike prior RL work on narrow domains, we find that Code Interpreter training is significantly harder due to high task diversity and expensive code execution, highlighting the critical role of the SFT stage. Our final model, R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\% to 64.1\%, outperforming GPT-4o (text-only: 58.6\%) and approaching GPT-4o with Code Interpreter (70.9\%), with the emergent self-checking behavior via code generation. Datasets, Codes, and Models are available at this https URL and this https URL. 

**Abstract (ZH)**: 尽管在R1-like模型的推理和规划方面取得了进展，大型语言模型（LLMs）仍然难以处理需要精确计算、符号操作、优化和算法推理的任务，因为文本推理缺乏代码执行的严谨性。一个关键挑战是使LLMs能够在使用文本推理和代码生成之间做出决定。虽然OpenAI训练模型在需要时调用代码解释器，但公共研究缺乏将预训练的LLMs与代码有效结合并在多种任务中泛化的指导。我们提出了R1-Code-Interpreter，这是一种通过多轮监督微调（SFT）和强化学习（RL）扩展的仅文本LLM，能够在逐步推理过程中自主生成多个代码查询。我们策划了144个推理和规划任务（107个用于训练，37个用于测试），每个任务都有超过200个多样化的问答。我们使用不同的SFT和RL策略微调Qwen-2.5模型（3B/7B/14B），研究了不同的答案格式、推理 vs. 非推理模型、冷启动 vs. 热启动、GRPO vs. PPO以及屏蔽 vs. 未屏蔽的代码输出。与窄域的RL工作不同，我们发现代码解释器的训练由于任务多样性和昂贵的代码执行而更为困难，突显了SFT阶段的关键作用。最终模型R1-CI-14B在37个测试任务上的平均准确性从44.0%提高到64.1%，超过了仅文本的GPT-4o（58.6%），接近于配备代码解释器的GPT-4o（70.9%），并且通过代码生成展现出了一种新兴的自我检查行为。数据集、代码和模型可在以下链接获取：这个 https URL 和这个 https URL。 

---
# Understanding the learned look-ahead behavior of chess neural networks 

**Title (ZH)**: 理解象棋神经网络学习的前瞻行为 

**Authors**: Diogo Cruz  

**Link**: [PDF](https://arxiv.org/pdf/2505.21552)  

**Abstract**: We investigate the look-ahead capabilities of chess-playing neural networks, specifically focusing on the Leela Chess Zero policy network. We build on the work of Jenner et al. (2024) by analyzing the model's ability to consider future moves and alternative sequences beyond the immediate next move. Our findings reveal that the network's look-ahead behavior is highly context-dependent, varying significantly based on the specific chess position. We demonstrate that the model can process information about board states up to seven moves ahead, utilizing similar internal mechanisms across different future time steps. Additionally, we provide evidence that the network considers multiple possible move sequences rather than focusing on a single line of play. These results offer new insights into the emergence of sophisticated look-ahead capabilities in neural networks trained on strategic tasks, contributing to our understanding of AI reasoning in complex domains. Our work also showcases the effectiveness of interpretability techniques in uncovering cognitive-like processes in artificial intelligence systems. 

**Abstract (ZH)**: 我们研究棋弈神经网络的展望能力，特别聚焦于Leela Chess Zero策略网络。我们在Jenner等人（2024）的基础上，分析了模型超越立即下一步棋，考虑未来移动和替代序列的能力。我们的研究发现，网络的展望行为高度依赖于具体棋局的上下文，表现差异显著。我们证明该模型能够处理最多七步棋前的棋盘状态信息，并且在不同的未来时间步骤中使用类似的内部机制。此外，我们提供了证据表明，该网络在考虑多种可能的移动序列，而不仅仅是关注单一的行棋线路。这些结果为神经网络在战略任务训练后如何产生复杂的展望能力提供了新的见解，有助于我们对复杂领域中AI推理的理解。我们的工作还展示了可解释性技术在揭示人工智能系统中类似认知的过程方面的有效性。 

---
# Maximizing Confidence Alone Improves Reasoning 

**Title (ZH)**: 单独最大化置信度可以提高推理能力 

**Authors**: Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, Deepak Pathak  

**Link**: [PDF](https://arxiv.org/pdf/2505.22660)  

**Abstract**: Reinforcement learning (RL) has enabled machine learning models to achieve significant advances in many fields. Most recently, RL has empowered frontier language models to solve challenging math, science, and coding problems. However, central to any RL algorithm is the reward function, and reward engineering is a notoriously difficult problem in any domain. In this paper, we propose RENT: Reinforcement Learning via Entropy Minimization -- a fully unsupervised RL method that requires no external reward or ground-truth answers, and instead uses the model's entropy of its underlying distribution as an intrinsic reward. We find that by reinforcing the chains of thought that yield high model confidence on its generated answers, the model improves its reasoning ability. In our experiments, we showcase these improvements on an extensive suite of commonly-used reasoning benchmarks, including GSM8K, MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and Mistral families. The generality of our unsupervised learning method lends itself to applicability in a wide range of domains where external supervision is limited or unavailable. 

**Abstract (ZH)**: 强化学习（RL）使机器学习模型在多个领域取得了显著进步。最近，RL 使前沿语言模型能够解决具有挑战性的数学、科学和编程问题。然而，对于任何 RL 算法而言，核心问题是奖励函数，而在任何领域中，奖励工程都是一个 notoriously difficult 的问题。在本文中，我们提出了 RENT：基于熵最小化的强化学习 —— 一种全无监督的 RL 方法，不需要外部奖励或地面真实答案，而是使用模型生成答案的概率分布的熵作为内在奖励。通过强化产生高模型信心的答案的推理链条，模型能够提高其推理能力。在我们的实验中，我们展示了这些改进在广泛使用的推理基准测试（包括 GSM8K、MATH500、AMC、AIME 和 GPQA）上的表现，并且涵盖了 Qwen 和 Mistral 家族的不同规模模型。我们无监督学习方法的普适性使其在外部监督有限或不可用的广泛领域中具有应用潜力。 

---
# 3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model 

**Title (ZH)**: 3DLLM-Mem：嵌入式3D大型语言模型的长期空时记忆 

**Authors**: Wenbo Hu, Yining Hong, Yanjun Wang, Leison Gao, Zibu Wei, Xingcheng Yao, Nanyun Peng, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22657)  

**Abstract**: Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks. 

**Abstract (ZH)**: 人类通过利用跨时空经验的长期记忆来执行复杂任务。相比之下，当前的大规模语言模型（LLMs）在动态的多房间3D环境中计划和执行行动方面能力有限。我们认为这一限制部分原因是由于LLMs缺少适当的3D空间-时间记忆建模。为此，我们首先引入了3DMem-Bench，这是一个包含超过26,000条轨迹和2,892个具身任务、问答和描述的全面基准，旨在评估代理在3D环境中的长期记忆推理能力。其次，我们提出了3DLLM-Mem，这是一种新型的动态记忆管理与融合模型，用于LLMs中的具身空间-时间推理和执行动作。我们的模型使用表示当前观测的工怍记忆标记作为查询，选择性地关注并融合 episodic 记忆中最有用的空间和时间特征，episodic 记忆存储了过去的观测和交互。我们的方法使代理能够聚焦于与任务相关的信息，同时在复杂的长期环境中保持记忆效率。实验结果表明，3DLLM-Mem 在多项任务中达到了最先进的性能，在3DMem-Bench最具挑战性的现实世界具身任务中，成功率为16.5%的提升超过了最强基线模型。 

---
# Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents 

**Title (ZH)**: 位置：大型语言模型代理的不确定性量化需要重新评估 

**Authors**: Michael Kirchhof, Gjergji Kasneci, Enkelejda Kasneci  

**Link**: [PDF](https://arxiv.org/pdf/2505.22655)  

**Abstract**: Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive. 

**Abstract (ZH)**: 大型语言模型（LLMs）和聊天机器人代理在提供输出时偶尔会出现错误，最近发现这种情况无法完全避免。因此，不确定性量化起着关键作用，旨在量化单一数字或两个数字中的模糊程度，用于表征偶然性和认识论不确定性。本文认为，在大型语言模型代理与用户通信时所处的开放和互动设定中，传统的不确定性二分法过于局限，需要研究能够丰富此类新颖场景中不确定性的新途径。我们回顾了文献并发现，流行的偶然性和认识论不确定性定义在交互式大型语言模型代理情境中直接相互矛盾且失去了意义。因此，我们提出了三个新的研究方向，专注于此类人机交互中的不确定性：不完备指定的不确定性，针对用户未一次性提供所有信息或明确任务的情况；互动学习，通过提问以减少当前情境的不确定性；以及输出不确定性，利用丰富的语言和语音空间来表达不确定性，而不仅仅是数字。我们期望这些新的不确定性处理和传达方式将使大型语言模型代理交互更加透明、可信和直观。 

---
# Pre-training for Recommendation Unlearning 

**Title (ZH)**: 推荐系统的隐私卸载预训练 

**Authors**: Guoxuan Chen, Lianghao Xia, Chao Huang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22649)  

**Abstract**: Modern recommender systems powered by Graph Neural Networks (GNNs) excel at modeling complex user-item interactions, yet increasingly face scenarios requiring selective forgetting of training data. Beyond user requests to remove specific interactions due to privacy concerns or preference changes, regulatory frameworks mandate recommender systems' ability to eliminate the influence of certain user data from models. This recommendation unlearning challenge presents unique difficulties as removing connections within interaction graphs creates ripple effects throughout the model, potentially impacting recommendations for numerous users. Traditional approaches suffer from significant drawbacks: fragmentation methods damage graph structure and diminish performance, while influence function techniques make assumptions that may not hold in complex GNNs, particularly with self-supervised or random architectures. To address these limitations, we propose a novel model-agnostic pre-training paradigm UnlearnRec that prepares systems for efficient unlearning operations. Our Influence Encoder takes unlearning requests together with existing model parameters and directly produces updated parameters of unlearned model with little fine-tuning, avoiding complete retraining while preserving model performance characteristics. Extensive evaluation on public benchmarks demonstrates that our method delivers exceptional unlearning effectiveness while providing more than 10x speedup compared to retraining approaches. We release our method implementation at: this https URL. 

**Abstract (ZH)**: 基于Graph Neural Networks (GNNs) 的现代推荐系统在建模复杂用户-项目交互方面表现出色，但在越来越多的情景下需要选择性遗忘训练数据。除了用户由于隐私顾虑或偏好改变而请求删除特定交互外，监管框架还要求推荐系统能够从模型中消除特定用户数据的影响。这一推荐遗忘挑战带来了独特困难，因为删除交互图中的连接会造成在整个模型中产生连锁反应，可能影响众多用户的推荐结果。传统方法存在重大缺陷：片段化方法损害了图结构并降低了性能，而影响函数技术对复杂GNNs的假设可能不成立，特别是在半监督或随机架构中。为了解决这些局限性，我们提出了一种新的模型无关预训练范式UnlearnRec，为高效的遗忘操作做好系统准备。我们的影响编码器将遗忘请求与现有模型参数结合，直接生成遗忘模型的更新参数，几乎无需微调，即可避免完全重新训练并保持模型性能特征。在公共基准上的广泛评估表明，我们的方法在提供超过10倍的速度优势的同时，实现了卓越的遗忘效果。我们已将我们的方法实现发布在：this https URL。 

---
# FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control 

**Title (ZH)**: FastTD3：简单、快速且能力强的类人机器人控制 reinforcement 学习方法 

**Authors**: Younggyo Seo, Carmelo Sferrazza, Haoran Geng, Michal Nauman, Zhao-Heng Yin, Pieter Abbeel  

**Link**: [PDF](https://arxiv.org/pdf/2505.22642)  

**Abstract**: Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics. 

**Abstract (ZH)**: 快速TD3算法：一种简单、快速且强大的强化学习方法及其在机器人领域的应用 

---
# Learning Composable Chains-of-Thought 

**Title (ZH)**: 学习可组合的思考链 

**Authors**: Fangcong Yin, Zeyu Leo Liu, Liu Leqi, Xi Ye, Greg Durrett  

**Link**: [PDF](https://arxiv.org/pdf/2505.22635)  

**Abstract**: A common approach for teaching large language models (LLMs) to reason is to train on chain-of-thought (CoT) traces of in-distribution reasoning problems, but such annotated data is costly to obtain for every problem of interest. We want reasoning models to generalize beyond their training distribution, and ideally to generalize compositionally: combine atomic reasoning skills to solve harder, unseen reasoning tasks. We take a step towards compositional generalization of reasoning skills when addressing a target compositional task that has no labeled CoT data. We find that simply training models on CoT data of atomic tasks leads to limited generalization, but minimally modifying CoT formats of constituent atomic tasks to be composable can lead to improvements. We can train "atomic CoT" models on the atomic tasks with Composable CoT data and combine them with multitask learning or model merging for better zero-shot performance on the target compositional task. Such a combined model can be further bootstrapped on a small amount of compositional data using rejection sampling fine-tuning (RFT). Results on string operations and natural language skill compositions show that training LLMs on Composable CoT outperforms multitask learning and continued fine-tuning baselines within a given training data budget. 

**Abstract (ZH)**: 一种教学大型语言模型进行推理的常见方法是通过训练其在分布内推理问题的思维链（CoT）痕迹上，但由于每种感兴趣的问题都需要标注数据，因此成本较高。我们希望推理模型能够在其训练分布之外泛化，并且最好是泛化为组合性：将基本的推理技能组合起来解决更难、未曾见过的推理任务。当处理一个没有标注CoT数据的目标组合任务时，我们向组合性泛化推理技能迈进了一步。我们发现，仅通过训练模型在基本任务的思维链数据上进行训练会限制泛化能力，但对构成的基本任务的思维链格式进行最小修改使其可组合，则可以带来改进。我们可以在原子任务上训练“原子CoT”模型，并使用组合性CoT数据将它们与多任务学习或模型合并结合起来，以在目标组合任务上获得更好的零样本性能。这种结合的模型可以通过拒绝采样微调（RFT）在少量组合数据上进一步自我提升。在字符串操作和自然语言技能组合上的结果表明，在给定的训练数据预算内，通过组合性CoT训练大型语言模型优于多任务学习和继续微调的基线方法。 

---
# Spatial Knowledge Graph-Guided Multimodal Synthesis 

**Title (ZH)**: 空间知识图谱引导的多模态合成 

**Authors**: Yida Xue, Zhen Bi, Jinnan Yang, Jungang Lou, Huajun Chen, Ningyu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22633)  

**Abstract**: Recent advances in multimodal large language models (MLLMs) have significantly enhanced their capabilities; however, their spatial perception abilities remain a notable limitation. To address this challenge, multimodal data synthesis offers a promising solution. Yet, ensuring that synthesized data adhere to spatial common sense is a non-trivial task. In this work, we introduce SKG2Data, a novel multimodal synthesis approach guided by spatial knowledge graphs, grounded in the concept of knowledge-to-data generation. SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate human-like perception of spatial directions and distances, which is subsequently utilized to guide multimodal data synthesis. Extensive experiments demonstrate that data synthesized from diverse types of spatial knowledge, including direction and distance, not only enhance the spatial perception and reasoning abilities of MLLMs but also exhibit strong generalization capabilities. We hope that the idea of knowledge-based data synthesis can advance the development of spatial intelligence. 

**Abstract (ZH)**: Recent Advances in Multimodal Large Language Models (MLLMs) Have Significantly Enhanced Their Capabilities; However, Their Spatial Perception Abilities Remain a Notable Limitation. To Address This Challenge, Multimodal Data Synthesis Offers a Promising Solution. Yet, Ensuring that Synthesized Data Adhere to Spatial Common Sense Is a Non-Trivial Task. In This Work, We Introduce SKG2Data, a Novel Multimodal Synthesis Approach Guided by Spatial Knowledge Graphs, Grounded in the Concept of Knowledge-to-Data Generation. SKG2Data Automatically Constructs a Spatial Knowledge Graph (SKG) to Emulate Human-Like Perception of Spatial Directions and Distances, Which Is Subsequently Utilized to Guide Multimodal Data Synthesis. Extensive Experiments Demonstrate That Data Synthesized from Diverse Types of Spatial Knowledge, Including Direction and Distance, Not Only Enhance the Spatial Perception and Reasoning Abilities of MLLMs but Also Exhibit Strong Generalization Capabilities. We Hope That the Idea of Knowledge-Based Data Synthesis Can Advance the Development of Spatial Intelligence. 

---
# SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning 

**Title (ZH)**: SCIZOR: 自监督的大规模模仿学习数据整理方法 

**Authors**: Yu Zhang, Yuqi Xie, Huihan Liu, Rutav Shah, Michael Wan, Linxi Fan, Yuke Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2505.22626)  

**Abstract**: Imitation learning advances robot capabilities by enabling the acquisition of diverse behaviors from human demonstrations. However, large-scale datasets used for policy training often introduce substantial variability in quality, which can negatively impact performance. As a result, automatically curating datasets by filtering low-quality samples to improve quality becomes essential. Existing robotic curation approaches rely on costly manual annotations and perform curation at a coarse granularity, such as the dataset or trajectory level, failing to account for the quality of individual state-action pairs. To address this, we introduce SCIZOR, a self-supervised data curation framework that filters out low-quality state-action pairs to improve the performance of imitation learning policies. SCIZOR targets two complementary sources of low-quality data: suboptimal data, which hinders learning with undesirable actions, and redundant data, which dilutes training with repetitive patterns. SCIZOR leverages a self-supervised task progress predictor for suboptimal data to remove samples lacking task progression, and a deduplication module operating on joint state-action representation for samples with redundant patterns. Empirically, we show that SCIZOR enables imitation learning policies to achieve higher performance with less data, yielding an average improvement of 15.4% across multiple benchmarks. More information is available at: this https URL 

**Abstract (ZH)**: 模仿学习通过从人类示例中获取多样化的行为来提升机器人能力。然而，用于策略训练的大规模数据集往往会引入质量上的显著波动，这可能对性能产生负面影响。因此，通过自动筛选低质量样本以提高数据质量变得至关重要。现有的机器人数据筛选方法依赖于昂贵的手动标注，并在粗粒度级别，如数据集或轨迹级别上进行筛选，未能考虑单个状态-动作对的质量。为了解决这一问题，我们提出了SCIZOR，这是一种自我监督的数据筛选框架，它通过筛选出低质量的状态-动作对来提高模仿学习策略的性能。SCIZOR 针对低质量数据的两种互补来源进行目标定位：次优数据，这妨碍了带有不 desired 动作的学习；冗余数据，这通过重复模式稀释了训练。SCIZOR 利用一个自我监督的任务进度预测器来移除缺乏任务进展的样本，以及一个基于联合状态-动作表示的去重模块，用于处理具有重复模式的样本。实验证明，SCIZOR 使得模仿学习策略能够在较少的数据下实现更高的性能，在多个基准测试中平均提高了15.4%。更多信息请访问：this https URL 

---
# The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models 

**Title (ZH)**: 强化学习中推理语言模型的熵机制 

**Authors**: Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, Ning Ding  

**Link**: [PDF](https://arxiv.org/pdf/2505.22617)  

**Abstract**: This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance. 

**Abstract (ZH)**: 本文旨在克服在将RL应用于LLMs推理时的 major 障碍，即策略熵的坍缩。在没有干预熵的情况下，这种现象在广泛的RL运行中持续观察到，策略熵在早期训练阶段急剧下降，这种探索能力的减弱总是伴随着策略性能的饱和。在实践中，我们建立了熵H与下游性能R之间的变换方程R=-a*e^H+b。这条经验法则强烈表明，策略性能是用策略熵交换来的，因此受到其耗尽的瓶颈限制，其天花板完全可预测为H=0, R=-a+b。我们的发现要求在持续探索以扩大RL的计算量时对熵进行管理。为此，我们从理论上和实证上研究了熵的动力学。我们的推导突显了策略熵的变化由动作概率与logits变化的协方差驱动，当使用类似策略梯度的算法时，这种协方差与其优势成比例。实证研究表明，协方差项的值与熵差异完全匹配，支持理论结论。此外，协方差项在整个训练过程中基本保持正值，进一步解释了为何策略熵会单调递减。通过理解熵动力学背后的机制，我们动机通过限制高协方差token的更新来控制熵。具体地，我们提出了两种简单而有效的方法，分别为Clip-Cov和KL-Cov，分别对高协方差的token进行截断和应用KL惩罚。实验表明，这些方法促进了探索，从而帮助策略避免熵坍缩并实现更好的下游性能。 

---
# RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction 

**Title (ZH)**: RICO：通过视觉重建提高图像重述准确性与完整性 

**Authors**: Yuchi Wang, Yishuo Cai, Shuhuai Ren, Sihan Yang, Linli Yao, Yuanxin Liu, Yuanxing Zhang, Pengfei Wan, Xu Sun  

**Link**: [PDF](https://arxiv.org/pdf/2505.22613)  

**Abstract**: Image recaptioning is widely used to generate training datasets with enhanced quality for various multimodal tasks. Existing recaptioning methods typically rely on powerful multimodal large language models (MLLMs) to enhance textual descriptions, but often suffer from inaccuracies due to hallucinations and incompleteness caused by missing fine-grained details. To address these limitations, we propose RICO, a novel framework that refines captions through visual reconstruction. Specifically, we leverage a text-to-image model to reconstruct a caption into a reference image, and prompt an MLLM to identify discrepancies between the original and reconstructed images to refine the caption. This process is performed iteratively, further progressively promoting the generation of more faithful and comprehensive descriptions. To mitigate the additional computational cost induced by the iterative process, we introduce RICO-Flash, which learns to generate captions like RICO using DPO. Extensive experiments demonstrate that our approach significantly improves caption accuracy and completeness, outperforms most baselines by approximately 10% on both CapsBench and CompreCap. Code released at this https URL. 

**Abstract (ZH)**: 图像重撰风气用于生成用于各种多模态任务的高质量训练数据集。现有的重撰方法通常依赖于强大的多模态大型语言模型（MLLMs）来增强文本描述，但往往会因为幻觉和由于细节信息缺失导致的不完整而受到影响。为了解决这些限制，我们提出了一种名为RICO的新框架，通过视觉重建来 refine 描述。具体而言，我们利用文本转图像模型将描述重构为参考图像，并提示MLLM识别原始图像和重构图像之间的差异以 refine 描述。该过程是迭代进行的，进一步促进生成更为忠实和全面的描述。为了缓解迭代过程引起的额外计算成本，我们引入了RICO-Flash，它利用DPO学习使用与RICO类似的方式生成描述。大量实验表明，我们的方法显著提高了描述的准确性和完整性，在CapsBench和CompreCap上分别比大多数基线方法高出约10%。代码发布在该网址。 

---
# Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates 

**Title (ZH)**: 面向稀疏性的自夹紧门一pass压缩语音基础模型efficient和effective one-pass压缩方法 

**Authors**: Haoning Xu, Zhaoqing Li, Youjun Chen, Huimeng Wang, Guinan Li, Mengzhe Geng, Chengxi Deng, Xunying Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.22608)  

**Abstract**: This paper presents a novel approach for speech foundation models compression that tightly integrates model pruning and parameter update into a single stage. Highly compact layer-level tied self-pinching gates each containing only a single learnable threshold are jointly trained with uncompressed models and used in fine-grained neuron level pruning. Experiments conducted on the LibriSpeech-100hr corpus suggest that our approach reduces the number of parameters of wav2vec2.0-base and HuBERT-large models by 65% and 60% respectively, while incurring no statistically significant word error rate (WER) increase on the test-clean dataset. Compared to previously published methods on the same task, our approach not only achieves the lowest WER of 7.05% on the test-clean dataset under a comparable model compression ratio of 4.26x, but also operates with at least 25% less model compression time. 

**Abstract (ZH)**: 本文提出了一种新的方法，将模型剪枝和参数更新紧密集成到一个阶段中，用于语音基础模型压缩。每层高度紧凑的共享自压缩门包括仅一个可学习的阈值，并与未压缩模型联合训练，并用于细粒度神经元级剪枝。在LibriSpeech-100hr语料库上的实验表明，与基线wav2vec2.0-base和大型HuBERT-large模型相比，我们的方法分别减少了65%和60%的参数量，同时在测试干净数据集上的词错误率（WER）无显著统计增加。与同类任务中已发表的方法相比，在相当的模型压缩比例（4.26倍）下，我们的方法不仅在测试干净数据集上实现了最低的7.05%词错误率，而且具有至少25%少的模型压缩时间。 

---
# One Rank at a Time: Cascading Error Dynamics in Sequential Learning 

**Title (ZH)**: 逐级累积的错误动态：序列学习中的级联错误动力学 

**Authors**: Mahtab Alizadeh Vandchali, Fangshuo, Liao, Anastasios Kyrillidis  

**Link**: [PDF](https://arxiv.org/pdf/2505.22602)  

**Abstract**: Sequential learning -- where complex tasks are broken down into simpler, hierarchical components -- has emerged as a paradigm in AI. This paper views sequential learning through the lens of low-rank linear regression, focusing specifically on how errors propagate when learning rank-1 subspaces sequentially. We present an analysis framework that decomposes the learning process into a series of rank-1 estimation problems, where each subsequent estimation depends on the accuracy of previous steps. Our contribution is a characterization of the error propagation in this sequential process, establishing bounds on how errors -- e.g., due to limited computational budgets and finite precision -- affect the overall model accuracy. We prove that these errors compound in predictable ways, with implications for both algorithmic design and stability guarantees. 

**Abstract (ZH)**: sequential 学习——将复杂任务分解为更简单的层次组件——已成为AI中的一个范式。本文从低秩线性回归的角度审视 sequential 学习，重点关注如何在顺序学习秩1子空间时误差传播。我们提出了一种分析框架，将学习过程分解为一系列秩1估计问题，其中每一步的估计依赖于前一步的准确性。我们的贡献是对这一顺序过程中误差传播的刻画，建立了由于有限计算预算和有限精度等因素导致的误差对整体模型准确性的影响界限。我们证明这些误差以可预测的方式累积，对算法设计和稳定性保证具有重要意义。 

---
# Machine Unlearning under Overparameterization 

**Title (ZH)**: 过参数化条件下的机器遗忘技术 

**Authors**: Jacob L. Block, Aryan Mokhtari, Sanjay Shakkottai  

**Link**: [PDF](https://arxiv.org/pdf/2505.22601)  

**Abstract**: Machine unlearning algorithms aim to remove the influence of specific training samples, ideally recovering the model that would have resulted from training on the remaining data alone. We study unlearning in the overparameterized setting, where many models interpolate the data, and defining the unlearning solution as any loss minimizer over the retained set$\unicode{x2013}$as in prior work in the underparameterized setting$\unicode{x2013}$is inadequate, since the original model may already interpolate the retained data and satisfy this condition. In this regime, loss gradients vanish, rendering prior methods based on gradient perturbations ineffective, motivating both new unlearning definitions and algorithms. For this setting, we define the unlearning solution as the minimum-complexity interpolator over the retained data and propose a new algorithmic framework that only requires access to model gradients on the retained set at the original solution. We minimize a regularized objective over perturbations constrained to be orthogonal to these model gradients, a first-order relaxation of the interpolation condition. For different model classes, we provide exact and approximate unlearning guarantees, and we demonstrate that an implementation of our framework outperforms existing baselines across various unlearning experiments. 

**Abstract (ZH)**: 机器卸载算法旨在移除特定训练样本的影响，理想情况下恢复仅基于剩余数据训练的模型。我们在模型过参数化设置下研究卸载问题，其中许多模型都能够插值训练数据，而在欠参数化设置下通过任何保留数据集上的损失最小化器定义卸载解决方案是不充分的，因为原始模型本身可能已经能够插值保留的数据并满足这一条件。在这种情况下，损失梯度消失，使得基于梯度扰动的先前方法无效，从而需要新的卸载定义和算法。为此，我们将卸载解决方案定义为保留数据上的最小复杂度插值器，并提出了一种新的算法框架，该框架仅要求访问原始解在保留集上的模型梯度。我们通过限制扰动与这些模型梯度正交的正则化目标函数进行最小化，这是插值条件的一阶松弛。对于不同的模型类别，我们提供了精确和近似的卸载保证，并展示了我们框架的实现优于现有基线的各种卸载实验。 

---
# On the performance of machine-learning assisted Monte Carlo in sampling from simple statistical physics models 

**Title (ZH)**: 机器学习辅助蒙特卡洛方法在简单统计物理模型采样中的性能研究 

**Authors**: Luca Maria Del Bono, Federico Ricci-Tersenghi, Francesco Zamponi  

**Link**: [PDF](https://arxiv.org/pdf/2505.22598)  

**Abstract**: Recent years have seen a rise in the application of machine learning techniques to aid the simulation of hard-to-sample systems that cannot be studied using traditional methods. Despite the introduction of many different architectures and procedures, a wide theoretical understanding is still lacking, with the risk of suboptimal implementations. As a first step to address this gap, we provide here a complete analytic study of the widely-used Sequential Tempering procedure applied to a shallow MADE architecture for the Curie-Weiss model. The contribution of this work is twofold: firstly, we give a description of the optimal weights and of the training under Gradient Descent optimization. Secondly, we compare what happens in Sequential Tempering with and without the addition of local Metropolis Monte Carlo steps. We are thus able to give theoretical predictions on the best procedure to apply in this case. This work establishes a clear theoretical basis for the integration of machine learning techniques into Monte Carlo sampling and optimization. 

**Abstract (ZH)**: 近年来，机器学习技术在辅助模拟传统方法难以研究的系统方面得到了广泛应用。尽管引入了多种不同的架构和程序，但理论理解仍然不足，存在亚最优实现的风险。为解决这一问题的第一步，本文提供了对广泛使用的连续调温程序应用于浅层MADE架构在Curie-Weiss模型中的完整分析研究。本文的贡献主要有两点：首先，我们给出了最优权重和梯度下降优化下的训练描述。其次，我们比较了连续调温有无局部Metropolis蒙特卡洛步骤的情况。因此，我们能够给出在这种情况下最佳方法的理论预测。这项工作为将机器学习技术集成到蒙特卡洛采样和优化中提供了清晰的理论基础。 

---
# Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning 

**Title (ZH)**: 自错误指导：从错误中泛化以提高大语言模型的数学推理能力 

**Authors**: Erxin Yu, Jing Li, Ming Liao, Qi Zhu, Boyang Xue, Minghui Xu, Baojun Wang, Lanqing Hong, Fei Mi, Lifeng Shang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22591)  

**Abstract**: Although large language models demonstrate strong performance across various domains, they still struggle with numerous bad cases in mathematical reasoning. Previous approaches to learning from errors synthesize training data by solely extrapolating from isolated bad cases, thereby failing to generalize the extensive patterns inherent within these cases. This paper presents Self-Error-Instruct (SEI), a framework that addresses these model weaknesses and synthesizes more generalized targeted training data. Specifically, we explore a target model on two mathematical datasets, GSM8K and MATH, to pinpoint bad cases. Then, we generate error keyphrases for these cases based on the instructor model's (GPT-4o) analysis and identify error types by clustering these keyphrases. Next, we sample a few bad cases during each generation for each identified error type and input them into the instructor model, which synthesizes additional training data using a self-instruct approach. This new data is refined through a one-shot learning process to ensure that only the most effective examples are kept. Finally, we use these curated data to fine-tune the target model, iteratively repeating the process to enhance performance. We apply our framework to various models and observe improvements in their reasoning abilities across both in-domain and out-of-domain mathematics datasets. These results demonstrate the effectiveness of self-error instruction in improving LLMs' mathematical reasoning through error generalization. 

**Abstract (ZH)**: 尽管大规模语言模型在各个领域表现出色，但在数学推理方面仍面临众多挑战。先前从错误中学习的方法仅通过孤立的错误案例外推来合成训练数据，从而无法捕捉到这些案例中广泛的模式。本文提出了一种名为Self-Error-Instruct (SEI)的框架，以解决这些模型的弱点，并合成更具普遍性的针对性训练数据。具体而言，我们在GSM8K和MATH两个数学数据集上探索目标模型，以识别错误案例。然后，基于导师模型（GPT-4o）的分析生成错误关键词，并通过聚类识别错误类型。接下来，我们在每次生成中针对每种识别的错误类型采样几个错误案例，将其输入导师模型，通过自我指导的方法合成额外的训练数据。这些新数据通过一次学习过程进行精炼，确保仅保留最有效的示例。最后，我们使用这些精选的数据对目标模型进行微调，并迭代重复该过程以提升性能。我们将该框架应用于多种模型，并观察到其在域内和域外数学数据集上的推理能力有所提升。这些结果表明，自我错误指导在通过错误泛化提升大规模语言模型的数学推理能力方面非常有效。 

---
# GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git 

**Title (ZH)**: GitGoodBench: 一项评估Git中代理性能的新基准 

**Authors**: Tobias Lindenbauer, Egor Bogomolov, Yaroslav Zharov  

**Link**: [PDF](https://arxiv.org/pdf/2505.22583)  

**Abstract**: Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench, have catalyzed progress in programming capabilities of AI agents. However, they overlook critical developer workflows such as Version Control System (VCS) operations. To address this issue, we present GitGoodBench, a novel benchmark for evaluating AI agent performance on VCS tasks. GitGoodBench covers three core Git scenarios extracted from permissive open-source Python, Java, and Kotlin repositories. Our benchmark provides three datasets: a comprehensive evaluation suite (900 samples), a rapid prototyping version (120 samples), and a training corpus (17,469 samples). We establish baseline performance on the prototyping version of our benchmark using GPT-4o equipped with custom tools, achieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a crucial stepping stone toward truly comprehensive SE agents that go beyond mere programming. 

**Abstract (ZH)**: 软件工程（SE）AI代理基准，如SWE-bench，显著推动了AI代理编程能力的进步。然而，它们忽视了关键的开发者工作流程，如版本控制系统（VCS）操作。为了解决这一问题，我们提出GitGoodBench，这是一个新型基准，用于评估AI代理在VCS任务上的性能。GitGoodBench涵盖了从宽松开源的Python、Java和Kotlin仓库中提取的三个核心Git场景。我们的基准提供了三种数据集：一个全面的评估套件（900个样本）、一个快速原型版本（120个样本）和一个训练语料库（17,469个样本）。我们在装备有自定义工具的GPT-4o上建立基线性能，对基准的原型版本实现了整体21.11%的解题率。我们期望GitGoodBench成为真正全面的超越单纯编程的SE代理的一个关键基石。 

---
# Tell me Habibi, is it Real or Fake? 

**Title (ZH)**: 告诉我habibi，这是真实的还是伪造的？ 

**Authors**: Kartik Kuckreja, Parul Gupta, Injy Hamed, Thamar Solorio, Muhammad Haris Khan, Abhinav Dhall  

**Link**: [PDF](https://arxiv.org/pdf/2505.22581)  

**Abstract**: Deepfake generation methods are evolving fast, making fake media harder to detect and raising serious societal concerns. Most deepfake detection and dataset creation research focuses on monolingual content, often overlooking the challenges of multilingual and code-switched speech, where multiple languages are mixed within the same discourse. Code-switching, especially between Arabic and English, is common in the Arab world and is widely used in digital communication. This linguistic mixing poses extra challenges for deepfake detection, as it can confuse models trained mostly on monolingual data. To address this, we introduce \textbf{ArEnAV}, the first large-scale Arabic-English audio-visual deepfake dataset featuring intra-utterance code-switching, dialectal variation, and monolingual Arabic content. It \textbf{contains 387k videos and over 765 hours of real and fake videos}. Our dataset is generated using a novel pipeline integrating four Text-To-Speech and two lip-sync models, enabling comprehensive analysis of multilingual multimodal deepfake detection. We benchmark our dataset against existing monolingual and multilingual datasets, state-of-the-art deepfake detection models, and a human evaluation, highlighting its potential to advance deepfake research. The dataset can be accessed \href{this https URL}{here}. 

**Abstract (ZH)**: Deepfake生成方法快速发展，使得假媒体更难检测，并引发了严重的社会关切。大多数Deepfake检测和数据集创建研究集中在单语内容上，往往忽视了多语和语言转换带来的挑战，即在同一个话语中混用多种语言。特别是在阿拉伯世界，阿阿拉伯语和英语之间的语言转换非常普遍，并广泛用于数字通信。这种语言混合给Deepfake检测带来了额外的挑战，因为模型大多基于单语数据训练，可能会因此困惑。为此，我们引入了的第一个大规模阿拉伯-英语音频-视觉Deepfake数据集\textbf{ArEnAV}，该数据集包含句内语言转换、方言变体以及纯阿拉伯语内容。该数据集包含387,000个视频和超过765小时的真实和假视频。我们的数据集使用一个结合了四个Text-To-Speech和两个唇同步模型的新型管道生成，使我们能够全面分析多语和多模态Deepfake检测。我们通过现有的单语和多语数据集、最先进的Deepfake检测模型和人工评估，对该数据集进行了基准测试，强调了其在推动Deepfake研究方面的潜力。数据集可通过\href{this https URL}{此链接}访问。 

---
# Fusion Steering: Prompt-Specific Activation Control 

**Title (ZH)**: 融合 steering: 命令特定激活控制 

**Authors**: Waldemar Chang, Alhassan Yasin  

**Link**: [PDF](https://arxiv.org/pdf/2505.22572)  

**Abstract**: We present Fusion Steering, an activation steering methodology that improves factual accuracy in large language models (LLMs) for question-answering (QA) tasks. This approach introduces flexible steering configurations, including full-layer steering and segmented steering. Unlike traditional methods constrained to single-layer or fixed-layer operations, Fusion Steering employs dynamic injection of prompt-specific activation deltas across all transformer layers. These activation deltas are derived from reference completions that combine the ground-truth answer with a model-generated explanation to facilitate semantically enriched, example-specific steering. The injection weights are optimized per prompt using Optuna, targeting a joint objective that balances token overlap (factual alignment) and perplexity (fluency proxy). Evaluation employs a composite score integrating token overlap and LLM-graded quality, encompassing factual accuracy, coherence, and relevance. Empirical results on 260 SimpleQA prompts (selected from 500 where the baseline failed) showcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit quantization, segmented steering achieves an accuracy of 25.4% (outputs scoring $\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at 16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully correct responses from 0.0% to 13.1%. These findings highlight the strengths of segmented, dynamic intervention strategies and the promise of per-prompt, full-network activation control. Fusion Steering is also amenable to sparse representations, such as Neuronpedia or sparse crosscoders, suggesting a promising direction for interpretable and scalable activation-level control in LLMs. 

**Abstract (ZH)**: Fusion Steering: 一种改进大型语言模型事实准确性的激活控制方法 

---
# Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems 

**Title (ZH)**: Agent-UniRAG：统一检索增强生成系统中的可训练开源大语言模型代理框架 

**Authors**: Hoang Pham, Khac-Hoai Nam Bui  

**Link**: [PDF](https://arxiv.org/pdf/2505.22571)  

**Abstract**: This paper presents a novel approach for unified retrieval-augmented generation (RAG) systems using the recent emerging large language model (LLM) agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental controllers, has become a promising approach to enable the interpretability of RAG tasks, especially for complex reasoning question-answering systems (e.g., multi-hop queries). Nonetheless, previous works mainly focus on solving RAG systems with either single-hop or multi-hop approaches separately, which limits the application of those approaches to real-world applications. In this study, we propose a trainable agent framework called Agent-UniRAG for unified retrieval-augmented LLM systems, which enhances the effectiveness and interpretability of RAG systems. The main idea is to design an LLM agent framework to solve RAG tasks step-by-step based on the complexity of the inputs, simultaneously including single-hop and multi-hop queries in an end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset to enable the proposed agent framework for small open-source LLMs (e.g., Llama-3-8B). The results show comparable performances with closed-source and larger open-source LLMs across various RAG benchmarks. Our source code and dataset are publicly available for further exploitation. 

**Abstract (ZH)**: 基于大型语言模型代理的统一检索增强生成系统的新方法 

---
# Universal Visuo-Tactile Video Understanding for Embodied Interaction 

**Title (ZH)**: 面向实体交互的通用视听触视频理解 

**Authors**: Yifan Xie, Mingyang Li, Shoujie Li, Xingting Li, Guangyu Chen, Fei Ma, Fei Richard Yu, Wenbo Ding  

**Link**: [PDF](https://arxiv.org/pdf/2505.22566)  

**Abstract**: Tactile perception is essential for embodied agents to understand physical attributes of objects that cannot be determined through visual inspection alone. While existing approaches have made progress in visual and language modalities for physical understanding, they fail to effectively incorporate tactile information that provides crucial haptic feedback for real-world interaction. In this paper, we present VTV-LLM, the first multi-modal large language model for universal Visuo-Tactile Video (VTV) understanding that bridges the gap between tactile perception and natural language. To address the challenges of cross-sensor and cross-modal integration, we contribute VTV150K, a comprehensive dataset comprising 150,000 video frames from 100 diverse objects captured across three different tactile sensors (GelSight Mini, DIGIT, and Tac3D), annotated with four fundamental tactile attributes (hardness, protrusion, elasticity, and friction). We develop a novel three-stage training paradigm that includes VTV enhancement for robust visuo-tactile representation, VTV-text alignment for cross-modal correspondence, and text prompt finetuning for natural language generation. Our framework enables sophisticated tactile reasoning capabilities including feature assessment, comparative analysis, scenario-based decision making and so on. Experimental evaluations demonstrate that VTV-LLM achieves superior performance in tactile video understanding tasks, establishing a foundation for more intuitive human-machine interaction in tactile domains. 

**Abstract (ZH)**: 触觉感知对于理解视觉检查无法确定的物体物理属性是 essential 的，对于具身智能体来说至关重要。虽然现有方法在视觉和语言模态下的物理理解方面取得了进展，但它们未能有效整合提供关键触觉反馈的触觉信息，以促进实际交互。本文介绍了 VTV-LLM，这是首个用于通用视觉-触觉视频 (VTV) 理解的大规模多模态语言模型，它弥合了触觉感知与自然语言之间的差距。为了解决跨传感器和跨模态整合的挑战，我们贡献了 VTV150K 数据集，该数据集包含来自 100 个不同物体的 150,000 个视频帧，这些物体跨越了三个不同的触觉传感器（GelSight Mini、DIGIT 和 Tac3D），并标注了四种基本的触觉属性（硬度、突出度、弹性、摩擦力）。我们开发了一种新颖的三阶段训练范式，包括 VTV 增强以实现稳健的视觉-触觉表示、VTV 文本对齐以实现跨模态对应，以及文本提示微调以实现自然语言生成。我们的框架 enables 复杂的触觉推理能力，包括特征评估、比较分析、基于场景的决策等。实验评估表明，VTV-LLM 在触觉视频理解任务中表现出优越的性能，为触觉领域更直观的人机交互奠定了基础。 

---
# PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion 

**Title (ZH)**: PRISM: 逐步细化与插入用于稀疏运动的视频数据集凝练 

**Authors**: Jaehyun Choi, Jiwan Hur, Gyojin Han, Jaemyung Yu, Junmo Kim  

**Link**: [PDF](https://arxiv.org/pdf/2505.22564)  

**Abstract**: Video dataset condensation has emerged as a critical technique for addressing the computational challenges associated with large-scale video data processing in deep learning applications. While significant progress has been made in image dataset condensation, the video domain presents unique challenges due to the complex interplay between spatial content and temporal dynamics. This paper introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for video dataset condensation, a novel approach that fundamentally reconsiders how video data should be condensed. Unlike the previous method that separates static content from dynamic motion, our method preserves the essential interdependence between these elements. Our approach progressively refines and inserts frames to fully accommodate the motion in an action while achieving better performance but less storage, considering the relation of gradients for each frame. Extensive experiments across standard video action recognition benchmarks demonstrate that PRISM outperforms existing disentangled approaches while maintaining compact representations suitable for resource-constrained environments. 

**Abstract (ZH)**: 视频数据集凝练已成为应对深度学习应用中大规模视频数据处理所带来计算挑战的关键技术。尽管在图像数据集凝练方面取得了显著进展，但由于空间内容与时间动态之间的复杂交互作用，视频领域提出了独特的挑战。本文介绍了PRISM：逐步精炼与稀疏运动插入方法，这是一种全新的视频数据集凝练方法，从根本上重新考虑了视频数据应该如何凝练。与之前的方法将静态内容与动态运动分离不同，我们的方法保留了这些元素之间的基本相互依赖关系。本文的方法逐步精炼并插入帧以全面适应动作中的运动，同时实现更好的性能并减少存储需求，并考虑了每帧梯度的关系。广泛的实验表明，PRISM在标准视频动作识别基准上优于现有的解耦方法，同时保持适用于资源受限环境的紧凑表示。 

---
# ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM 

**Title (ZH)**: ClaimPKG：通过轻量级专业化语言模型与伪子图生成增强声明验证 

**Authors**: Hoang Pham, Thanh-Do Nguyen, Khac-Hoai Nam Bui  

**Link**: [PDF](https://arxiv.org/pdf/2505.22552)  

**Abstract**: Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of large language models (LLMs) is an emerging research challenge in claim verification. While KGs provide structured, semantically rich representations well-suited for reasoning, most existing verification methods rely on unstructured text corpora, limiting their ability to effectively leverage KGs. Additionally, despite possessing strong reasoning abilities, modern LLMs struggle with multi-step modular pipelines and reasoning over KGs without adaptation. To address these challenges, we propose ClaimPKG, an end-to-end framework that seamlessly integrates LLM reasoning with structured knowledge from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight, specialized LLM to represent the input claim as pseudo-subgraphs, guiding a dedicated subgraph retrieval module to identify relevant KG subgraphs. These retrieved subgraphs are then processed by a general-purpose LLM to produce the final verdict and justification. Extensive experiments on the FactKG dataset demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming strong baselines in this research field by 9%-12% accuracy points across multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability to unstructured datasets such as HoVer and FEVEROUS, effectively combining structured knowledge from KGs with LLM reasoning across various LLM backbones. 

**Abstract (ZH)**: 整合知识图谱以增强大型语言模型的推理能力：ClaimPKG框架在声明验证中的应用 

---
# Scaling-up Perceptual Video Quality Assessment 

**Title (ZH)**: 提升感知视频质量评估 

**Authors**: Ziheng Jia, Zicheng Zhang, Zeyu Zhang, Yingji Liang, Xiaorong Zhu, Chunyi Li, Jinliang Han, Haoning Wu, Bin Wang, Haoran Zhang, Guanyu Zhu, Qiyong Zhao, Xiaohong Liu, Guangtao Zhai, Xiongkuo Min  

**Link**: [PDF](https://arxiv.org/pdf/2505.22543)  

**Abstract**: The data scaling law has been shown to significantly enhance the performance of large multi-modal models (LMMs) across various downstream tasks. However, in the domain of perceptual video quality assessment (VQA), the potential of scaling law remains unprecedented due to the scarcity of labeled resources and the insufficient scale of datasets. To address this, we propose \textbf{OmniVQA}, an efficient framework designed to efficiently build high-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs). We then scale up to create \textbf{OmniVQA-Chat-400K}, the largest MIDB in the VQA field concurrently. Our focus is on the technical and aesthetic quality dimensions, with abundant in-context instruction data to provide fine-grained VQA knowledge. Additionally, we have built the \textbf{OmniVQA-MOS-20K} dataset to enhance the model's quantitative quality rating capabilities. We then introduce a \textbf{complementary} training strategy that effectively leverages the knowledge from datasets for quality understanding and quality rating tasks. Furthermore, we propose the \textbf{OmniVQA-FG (fine-grain)-Benchmark} to evaluate the fine-grained performance of the models. Our results demonstrate that our models achieve state-of-the-art performance in both quality understanding and rating tasks. 

**Abstract (ZH)**: 数据扩展规律已被证明能显著增强大规模多模态模型在各种下游任务中的性能。然而，在感知视频质量评估（VQA）领域，数据扩展规律的潜力尚未被充分发掘，主要原因在于标注资源稀缺和数据集规模不足。为解决这一问题，我们提出了**OmniVQA**，一种高效框架，旨在高效构建高质量、人机结合的VQA多模态指令数据集（MIDB）。随后，我们扩展规模创建了**OmniVQA-Chat-400K**，这是VQA领域目前最大的MIDB。我们重点关注技术和美学质量维度，并提供了丰富的上下文内指令数据，以提供详细的VQA知识。此外，我们建立了**OmniVQA-MOS-20K**数据集，以增强模型的定量质量评分能力。我们还提出了一种**互补**的训练策略，有效利用数据集知识进行质量理解和质量评分任务。此外，我们提出了**OmniVQA-FG（细粒度）基准**来评估模型的细粒度性能。我们的结果显示，我们的模型在质量理解和评分任务中均实现了最先进的性能。 

---
# TabularQGAN: A Quantum Generative Model for Tabular Data 

**Title (ZH)**: TabularQGAN：用于表格数据的量子生成模型 

**Authors**: Pallavi Bhardwaj, Caitlin Jones, Lasse Dierich, Aleksandar Vučković  

**Link**: [PDF](https://arxiv.org/pdf/2505.22533)  

**Abstract**: In this paper, we introduce a novel quantum generative model for synthesizing tabular data. Synthetic data is valuable in scenarios where real-world data is scarce or private, it can be used to augment or replace existing datasets. Real-world enterprise data is predominantly tabular and heterogeneous, often comprising a mixture of categorical and numerical features, making it highly relevant across various industries such as healthcare, finance, and software. We propose a quantum generative adversarial network architecture with flexible data encoding and a novel quantum circuit ansatz to effectively model tabular data. The proposed approach is tested on the MIMIC III healthcare and Adult Census datasets, with extensive benchmarking against leading classical models, CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model outperforms classical models by an average of 8.5% with respect to an overall similarity score from SDMetrics, while using only 0.072% of the parameters of the classical models. Additionally, we evaluate the generalization capabilities of the models using two custom-designed metrics that demonstrate the ability of the proposed quantum model to generate useful and novel samples. To our knowledge, this is one of the first demonstrations of a successful quantum generative model for handling tabular data, indicating that this task could be well-suited to quantum computers. 

**Abstract (ZH)**: 本文介绍了一种用于合成表格数据的新型量子生成模型。合成数据在真实数据稀缺或私有化的情况下非常有价值，可用于扩充或替换现有数据集。现实世界的企业数据主要是表格形式且异质性较强，通常包含分类和数值特征的混合，这使其在医疗、金融和软件等行业具有高度的相关性。我们提出了一种具有灵活数据编码的量子生成对抗网络架构和一种新型的量子电路模态，以有效地建模表格数据。所提出的方法在MIMIC III医疗和成人税档数据集上进行测试，并与领先的经典模型CTGAN和CopulaGAN进行了广泛的基准测试。实验结果表明，与SDMetrics的整体相似度得分相比，我们的量子模型在参数量仅为经典模型的0.072%的情况下平均超过经典模型8.5%的表现。此外，我们使用两个定制设计的指标评估了模型的泛化能力，这些指标展示了所提出的量子模型生成有用和新颖样本的能力。据我们所知，这是首次成功的演示针对表格数据的量子生成模型，表明该任务可能非常适合量子计算机。 

---
# Training RL Agents for Multi-Objective Network Defense Tasks 

**Title (ZH)**: 训练面向多目标网络防御任务的RL代理 

**Authors**: Andres Molina-Markham, Luis Robaina, Sean Steinle, Akash Trivedi, Derek Tsui, Nicholas Potteiger, Lauren Brandt, Ransom Winder, Ahmed Ridley  

**Link**: [PDF](https://arxiv.org/pdf/2505.22531)  

**Abstract**: Open-ended learning (OEL) -- which emphasizes training agents that achieve broad capability over narrow competency -- is emerging as a paradigm to develop artificial intelligence (AI) agents to achieve robustness and generalization. However, despite promising results that demonstrate the benefits of OEL, applying OEL to develop autonomous agents for real-world cybersecurity applications remains a challenge.
We propose a training approach, inspired by OEL, to develop autonomous network defenders. Our results demonstrate that like in other domains, OEL principles can translate into more robust and generalizable agents for cyber defense. To apply OEL to network defense, it is necessary to address several technical challenges. Most importantly, it is critical to provide a task representation approach over a broad universe of tasks that maintains a consistent interface over goals, rewards and action spaces. This way, the learning agent can train with varying network conditions, attacker behaviors, and defender goals while being able to build on previously gained knowledge.
With our tools and results, we aim to fundamentally impact research that applies AI to solve cybersecurity problems. Specifically, as researchers develop gyms and benchmarks for cyber defense, it is paramount that they consider diverse tasks with consistent representations, such as those we propose in our work. 

**Abstract (ZH)**: 开放性学习（OEL）——强调训练能够在广泛能力而非狭窄专长上取得成就的智能体——正在成为开发具备稳健性和泛化能力的人工智能代理的范式。然而，尽管开放性学习展示了显著的好处，将其应用于开发用于现实世界网络安全应用的自主代理仍面临挑战。 

---
# Thinking with Generated Images 

**Title (ZH)**: 生成图像的思考 

**Authors**: Ethan Chern, Zhulin Hu, Steffi Chern, Siqi Kou, Jiadi Su, Yan Ma, Zhijie Deng, Pengfei Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.22525)  

**Abstract**: We present Thinking with Generated Images, a novel paradigm that fundamentally transforms how large multimodal models (LMMs) engage with visual reasoning by enabling them to natively think across text and vision modalities through spontaneous generation of intermediate visual thinking steps. Current visual reasoning with LMMs is constrained to either processing fixed user-provided images or reasoning solely through text-based chain-of-thought (CoT). Thinking with Generated Images unlocks a new dimension of cognitive capability where models can actively construct intermediate visual thoughts, critique their own visual hypotheses, and refine them as integral components of their reasoning process. We demonstrate the effectiveness of our approach through two complementary mechanisms: (1) vision generation with intermediate visual subgoals, where models decompose complex visual tasks into manageable components that are generated and integrated progressively, and (2) vision generation with self-critique, where models generate an initial visual hypothesis, analyze its shortcomings through textual reasoning, and produce refined outputs based on their own critiques. Our experiments on vision generation benchmarks show substantial improvements over baseline approaches, with our models achieving up to 50% (from 38% to 57%) relative improvement in handling complex multi-object scenarios. From biochemists exploring novel protein structures, and architects iterating on spatial designs, to forensic analysts reconstructing crime scenes, and basketball players envisioning strategic plays, our approach enables AI models to engage in the kind of visual imagination and iterative refinement that characterizes human creative, analytical, and strategic thinking. We release our open-source suite at this https URL. 

**Abstract (ZH)**: 生成图像辅助思考：一种根本性改变大型多模态模型视觉推理方式的新范式 

---
# Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data 

**Title (ZH)**: 监督学习模型在不平衡交易数据欺诈检测中的评估：经典架构与深度架构的比较研究 

**Authors**: Chao Wang, Chuanhao Nie, Yunbo Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.22521)  

**Abstract**: Fraud detection remains a critical task in high-stakes domains such as finance and e-commerce, where undetected fraudulent transactions can lead to significant economic losses. In this study, we systematically compare the performance of four supervised learning models - Logistic Regression, Random Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit (GRU) network - on a large-scale, highly imbalanced online transaction dataset. While ensemble methods such as Random Forest and LightGBM demonstrated superior performance in both overall and class-specific metrics, Logistic Regression offered a reliable and interpretable baseline. The GRU model showed strong recall for the minority fraud class, though at the cost of precision, highlighting a trade-off relevant for real-world deployment. Our evaluation emphasizes not only weighted averages but also per-class precision, recall, and F1-scores, providing a nuanced view of each model's effectiveness in detecting rare but consequential fraudulent activity. The findings underscore the importance of choosing models based on the specific risk tolerance and operational needs of fraud detection systems. 

**Abstract (ZH)**: 欺诈检测仍然是金融和电子商务等高风险领域中的关键任务，未检测的欺诈交易可能导致重大经济损失。在本研究中，我们系统地比较了四种监督学习模型——逻辑回归、随机森林、轻量级梯度提升机（LightGBM）和门控递归单元（GRU）网络——在大规模高度不平衡的在线交易数据集上的性能。尽管随机森林和轻量级梯度提升机在整体和类别特定指标上表现出优越的性能，逻辑回归提供了可靠且可解释的基础模型。GRU模型在少数类欺诈检测方面显示了较高的召回率，但牺牲了精确率，突显了适用于实际部署的权衡关系。我们的评估不仅强调加权平均，还强调每个类别的精确率、召回率和F1分数，提供了一个对每个模型在检测罕见但后果严重的欺诈活动方面有效性的细腻视角。研究结果强调了根据欺诈检测系统特定的风险容忍度和运营需求选择模型的重要性。 

---
# Strengthening Proportionality in Temporal Voting 

**Title (ZH)**: 加强时间投票中的比例性 

**Authors**: Bradley Phillips, Edith Elkind, Nicholas Teh, Tomasz Wąs  

**Link**: [PDF](https://arxiv.org/pdf/2505.22513)  

**Abstract**: We study proportional representation in the framework of temporal voting with approval ballots. Prior work adapted basic proportional representation concepts -- justified representation (JR), proportional JR (PJR), and extended JR (EJR) -- from the multiwinner setting to the temporal setting. Our work introduces and examines ways of going beyond EJR. Specifically, we consider stronger variants of JR, PJR, and EJR, and introduce temporal adaptations of more demanding multiwinner axioms, such as EJR+, full JR (FJR), full proportional JR (FPJR), and the Core. For each of these concepts, we investigate its existence and study its relationship to existing notions, thereby establishing a rich hierarchy of proportionality concepts. Notably, we show that two of our proposed axioms -- EJR+ and FJR -- strengthen EJR while remaining satisfiable in every temporal election. 

**Abstract (ZH)**: 我们研究了在基于时间投票的批准选票框架下比例代表制。先前的工作将基本的比例代表制概念——合理代表（JR）、比例合理代表（PJR）和扩展合理代表（EJR）——从多胜者设置推广到了时间设置。我们的工作引入并探讨了超越EJR的方法。具体来说，我们考虑了JR、PJR和EJR的更强变体，并引入了更苛刻的多胜者公理的时间适应版本，如EJR+、全面代表（FJR）、全面比例代表（FPJR）和核心。对于这些每个概念，我们研究了它们的存在性，并探讨了它们与现有概念的关系，从而建立了一个丰富比例性概念的层次结构。值得注意的是，我们展示了我们提出的两个公理——EJR+和FJR——加强了EJR的同时保持在每一项时间选举中可实现。 

---
# From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation 

**Title (ZH)**: 从陌生人到助手：快速欲望对齐以适应具身智能体-用户适应性 

**Authors**: Yuanfei Wang, Xinju Huang, Fangwei Zhong, Yaodong Yang, Yizhou Wang, Yuanpei Chen, Hao Dong  

**Link**: [PDF](https://arxiv.org/pdf/2505.22503)  

**Abstract**: While embodied agents have made significant progress in performing complex physical tasks, real-world applications demand more than pure task execution. The agents must collaborate with unfamiliar agents and human users, whose goals are often vague and implicit. In such settings, interpreting ambiguous instructions and uncovering underlying desires is essential for effective assistance. Therefore, fast and accurate desire alignment becomes a critical capability for embodied agents. In this work, we first develop a home assistance simulation environment HA-Desire that integrates an LLM-driven human user agent exhibiting realistic value-driven goal selection and communication. The ego agent must interact with this proxy user to infer and adapt to the user's latent desires. To achieve this, we present a novel framework FAMER for fast desire alignment, which introduces a desire-based mental reasoning mechanism to identify user intent and filter desire-irrelevant actions. We further design a reflection-based communication module that reduces redundant inquiries, and incorporate goal-relevant information extraction with memory persistence to improve information reuse and reduce unnecessary exploration. Extensive experiments demonstrate that our framework significantly enhances both task execution and communication efficiency, enabling embodied agents to quickly adapt to user-specific desires in complex embodied environments. 

**Abstract (ZH)**: 虽然嵌入式代理在执行复杂物理任务方面取得了显著进展，但在实际应用中，要求不仅仅是纯粹的任务执行。这些代理还必须与不熟悉的合作代理和人类用户进行协作，而这些用户的目的是往往模糊且隐含的。在这种环境中，理解模棱两可的指示和揭示潜在的欲望对于有效协助至关重要。因此，快速且准确的欲望对齐成为嵌入式代理的关键能力。在本文中，我们首先开发了一个家庭辅助模拟环境HA-Desire，该环境整合了一个由LLM驱动的人类用户代理，表现出基于现实价值的目标选择和沟通。主体代理必须与这个代理用户进行互动，以推断并适应用户的潜在欲望。为此，我们提出了一种新颖的框架FAMER，该框架引入了一种基于欲望的心理推理机制，以识别用户意图并过滤掉与欲望无关的动作。我们进一步设计了一种基于反思的通信模块，减少了冗余查询，并结合了与目标相关的信息提取与记忆持久化，以提高信息重用并减少不必要的探索。广泛的实验表明，我们的框架显著提高了任务执行和沟通效率，使嵌入式代理能够在复杂的身体环境中迅速适应用户的特定欲望。 

---
# Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation 

**Title (ZH)**: 揭开使用估计历史依赖行为策略的离策评估中的重要性抽样悖论之谜 

**Authors**: Hongyi Zhou, Josiah P. Hanna, Jin Zhu, Ying Yang, Chengchun Shi  

**Link**: [PDF](https://arxiv.org/pdf/2505.22492)  

**Abstract**: This paper studies off-policy evaluation (OPE) in reinforcement learning with a focus on behavior policy estimation for importance sampling. Prior work has shown empirically that estimating a history-dependent behavior policy can lead to lower mean squared error (MSE) even when the true behavior policy is Markovian. However, the question of why the use of history should lower MSE remains open. In this paper, we theoretically demystify this paradox by deriving a bias-variance decomposition of the MSE of ordinary importance sampling (IS) estimators, demonstrating that history-dependent behavior policy estimation decreases their asymptotic variances while increasing their finite-sample biases. Additionally, as the estimated behavior policy conditions on a longer history, we show a consistent decrease in variance. We extend these findings to a range of other OPE estimators, including the sequential IS estimator, the doubly robust estimator and the marginalized IS estimator, with the behavior policy estimated either parametrically or non-parametrically. 

**Abstract (ZH)**: 本文研究强化学习中离策评估（OPE）中行为策略估计对重要性采样的影响，重点研究历史依赖性行为策略估计如何降低均方误差（MSE）。虽然先前工作 empirical 地表明，即使真实行为策略具有马尔可夫性，估计历史依赖性行为策略仍可降低 MSE，但使用历史如何降低 MSE 仍然是一个公开问题。本文通过推导普通重要性采样（IS）估计器的偏差-方差分解，理论上阐明了这一悖论，证明了历史依赖性行为策略估计降低了其渐近方差，同时增加了其有限样本偏差。此外，随着估计的行为策略依赖于更长的历史，我们展示了方差的一致减少。本文还将这些发现扩展到其他多种 OPE 估计器，包括序列 IS 估计器、双重稳健估计器和边缘化 IS 估计器，其中行为策略既可参数化估计也可非参数化估计。 

---
# On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling 

**Title (ZH)**: 在标准宽度缩放下，大型学习率的意外有效性 

**Authors**: Moritz Haas, Sebastian Bordt, Ulrike von Luxburg, Leena Chennuru Vankadara  

**Link**: [PDF](https://arxiv.org/pdf/2505.22491)  

**Abstract**: The dominant paradigm for training large-scale vision and language models is He initialization and a single global learning rate (\textit{standard parameterization}, SP). Despite its practical success, standard parametrization remains poorly understood from a theoretical perspective: Existing infinite-width theory would predict instability under large learning rates and vanishing feature learning under stable learning rates. However, empirically optimal learning rates consistently decay much slower than theoretically predicted. By carefully studying neural network training dynamics, we demonstrate that this discrepancy is not fully explained by finite-width phenomena such as catapult effects or a lack of alignment between weights and incoming activations. We instead show that the apparent contradiction can be fundamentally resolved by taking the loss function into account: In contrast to Mean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an intermediate \textit{controlled divergence} regime emerges, where logits diverge but loss, gradients, and activations remain stable. Stable training under large learning rates enables persistent feature evolution at scale in all hidden layers, which is crucial for the practical success of SP. In experiments across optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities (vision, language), we validate that neural networks operate in this controlled divergence regime under CE loss but not under MSE loss. Our empirical evidence suggests that width-scaling considerations are surprisingly useful for predicting empirically optimal learning rate exponents. Finally, our analysis clarifies the effectiveness and limitations of recently proposed layerwise learning rate scalings for standard initialization. 

**Abstract (ZH)**: 大规模视觉和语言模型训练的主要范式是He初始化和单一全局学习率（标准参数化，SP）。尽管在实践中取得了成功，但从理论角度来看，标准参数化仍不完全理解：现有的无限宽度理论预测，在大学习率下会出现不稳定性，在稳定学习率下会消失特征学习。然而，实验证明的学习率最优值却比理论预测缓慢衰减得多。通过仔细研究神经网络训练动力学，我们证明，这种差异并非完全由有限宽度现象如弹箭效应或权重与输入激活之间的对齐不足所解释。相反，我们证明，通过考虑损失函数，这种看似矛盾的现象可以从根本上得到解决：与均方误差（MSE）损失不同，我们证明，在交叉熵（CE）损失下，会出现一个中间的控制发散（controlled divergence）区间，在该区间中，logits发散，但损失、梯度和激活保持稳定。在大学习率下保持稳定训练使得所有隐藏层中的特征演化持久进行，这对于SP的实际成功至关重要。在使用不同优化器（SGD, Adam）、架构（MLPs, GPT）和数据模态（视觉, 语言）的实验中，我们验证，在CE损失下，神经网络处于这种控制发散区间，但不在MSE损失下。我们的实验证据表明，宽度缩放的考虑对于预测最优学习率指数具有惊人的有用性。最后，我们的分析阐明了近期提出的分层学习率缩放方法的有效性和局限性，这些方法针对标准初始化有效。 

---
# A Closer Look at Multimodal Representation Collapse 

**Title (ZH)**: 更深入探讨多模态表示崩溃问题 

**Authors**: Abhra Chaudhuri, Anjan Dutta, Tu Bui, Serban Georgescu  

**Link**: [PDF](https://arxiv.org/pdf/2505.22483)  

**Abstract**: We aim to develop a fundamental understanding of modality collapse, a recently observed empirical phenomenon wherein models trained for multimodal fusion tend to rely only on a subset of the modalities, ignoring the rest. We show that modality collapse happens when noisy features from one modality are entangled, via a shared set of neurons in the fusion head, with predictive features from another, effectively masking out positive contributions from the predictive features of the former modality and leading to its collapse. We further prove that cross-modal knowledge distillation implicitly disentangles such representations by freeing up rank bottlenecks in the student encoder, denoising the fusion-head outputs without negatively impacting the predictive features from either modality. Based on the above findings, we propose an algorithm that prevents modality collapse through explicit basis reallocation, with applications in dealing with missing modalities. Extensive experiments on multiple multimodal benchmarks validate our theoretical claims. Project page: this https URL. 

**Abstract (ZH)**: 我们旨在对模态崩溃这一最近观察到的经验现象进行基本的理解，模态崩溃是指在进行多模态融合训练的模型倾向于仅依赖于子集模态，而忽略其他模态。我们展示了当一个模态中的噪声特征通过融合头部共享的神经元与另一个模态的预测特征纠缠在一起时，会发生模态崩溃，这会屏蔽掉前一模态预测特征的积极贡献，导致该模态的崩溃。进一步证明，跨模态知识蒸馏通过在学生编码器中释放秩瓶颈隐式地解纠缠此类表示，同时不负面影响任一模态的预测特征，从而净化融合头部的输出。基于上述发现，我们提出了一种算法，通过显式的基底重新分配防止模态崩溃，并应用于处理缺失模态。在多个多模态基准上的广泛实验验证了我们的理论推断。项目页面：https://this-url。 

---
# Human-Centered Human-AI Collaboration (HCHAC) 

**Title (ZH)**: 以人为中心的人机协作（HCHAC） 

**Authors**: Qi Gao, Wei Xu, Hanxi Pan, Mowei Shen, Zaifeng Gao  

**Link**: [PDF](https://arxiv.org/pdf/2505.22477)  

**Abstract**: In the intelligent era, the interaction between humans and intelligent systems fundamentally involves collaboration with autonomous intelligent agents. Human-AI Collaboration (HAC) represents a novel type of human-machine relationship facilitated by autonomous intelligent machines equipped with AI technologies. In this paradigm, AI agents serve not only as auxiliary tools but also as active teammates, partnering with humans to accomplish tasks collaboratively. Human-centered AI (HCAI) emphasizes that humans play critical leadership roles in the collaboration. This human-led collaboration imparts new dimensions to the human-machine relationship, necessitating innovative research perspectives, paradigms, and agenda to address the unique challenges posed by HAC. This chapter delves into the essence of HAC from the human-centered perspective, outlining its core concepts and distinguishing features. It reviews the current research methodologies and research agenda within the HAC field from the HCAI perspective, highlighting advancements and ongoing studies. Furthermore, a framework for human-centered HAC (HCHAC) is proposed by integrating these reviews and analyses. A case study of HAC in the context of autonomous vehicles is provided, illustrating practical applications and the synergistic interactions between humans and AI agents. Finally, it identifies potential future research directions aimed at enhancing the effectiveness, reliability, and ethical integration of human-centered HAC systems in diverse domains. 

**Abstract (ZH)**: 人类中心的人机协作：原理、研究方法及未来方向 

---
# Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems 

**Title (ZH)**: 基于LLM的多智能体系统中拓扑结构学习应优先研究 

**Authors**: Jiaxi Yang, Mengqi Zhang, Yiqiao Jin, Hao Chen, Qingsong Wen, Lu Lin, Yi He, Weijie Xu, James Evans, Jindong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22467)  

**Abstract**: Large Language Model-based Multi-Agent Systems (MASs) have emerged as a powerful paradigm for tackling complex tasks through collaborative intelligence. Nevertheless, the question of how agents should be structurally organized for optimal cooperation remains largely unexplored. In this position paper, we aim to gently redirect the focus of the MAS research community toward this critical dimension: develop topology-aware MASs for specific tasks. Specifically, the system consists of three core components - agents, communication links, and communication patterns - that collectively shape its coordination performance and efficiency. To this end, we introduce a systematic, three-stage framework: agent selection, structure profiling, and topology synthesis. Each stage would trigger new research opportunities in areas such as language models, reinforcement learning, graph learning, and generative modeling; together, they could unleash the full potential of MASs in complicated real-world applications. Then, we discuss the potential challenges and opportunities in the evaluation of multiple systems. We hope our perspective and framework can offer critical new insights in the era of agentic AI. 

**Abstract (ZH)**: 基于大型语言模型的多智能体系统：拓扑感知 MASs 用于特定任务的合作智能 

---
# Fostering Video Reasoning via Next-Event Prediction 

**Title (ZH)**: 通过下一步事件预测促进视频推理 

**Authors**: Haonan Wang, Hongfu Liu, Xiangyan Liu, Chao Du, Kenji Kawaguchi, Ye Wang, Tianyu Pang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22457)  

**Abstract**: Next-token prediction serves as the foundational learning task enabling reasoning in LLMs. But what should the learning task be when aiming to equip MLLMs with temporal reasoning capabilities over video inputs? Existing tasks such as video question answering often rely on annotations from humans or much stronger MLLMs, while video captioning tends to entangle temporal reasoning with spatial information. To address this gap, we propose next-event prediction (NEP), a learning task that harnesses future video segments as a rich, self-supervised signal to foster temporal reasoning. We segment each video into past and future frames: the MLLM takes the past frames as input and predicts a summary of events derived from the future frames, thereby encouraging the model to reason temporally in order to complete the task. To support this task, we curate V1-33K, a dataset comprising 33,000 automatically extracted video segments spanning diverse real-world scenarios. We further explore a range of video instruction-tuning strategies to study their effects on temporal reasoning. To evaluate progress, we introduce FutureBench to assess coherence in predicting unseen future events. Experiments validate that NEP offers a scalable and effective training paradigm for fostering temporal reasoning in MLLMs. 

**Abstract (ZH)**: 下一事件预测：面向大规模语言模型的视频时序推理学习任务 

---
# Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO 

**Title (ZH)**: 多模态LLM推理的无监督后训练方法：GRPO 

**Authors**: Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, Lichao Sun  

**Link**: [PDF](https://arxiv.org/pdf/2505.22453)  

**Abstract**: Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, a stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9 %$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at this https URL. 

**Abstract (ZH)**: 改进多模态大型语言模型（MLLMs）在后训练阶段通常依赖于监督微调（SFT）或强化学习（RL）。然而，这些监督方法需要昂贵且手动标注的多模态数据——这最终是一种不可持续的资源。虽然近期的努力已经探索了无监督后训练，但其方法复杂且难以迭代。在本文中，我们首次研究将GRPO，一种稳定且可扩展的在线RL算法，用于无需任何外部监督的持续自我改进。我们提出了MM-UPT，一个简单有效的框架，用于MLLMs的无监督后训练。MM-UPT基于多数投票的自我奖励机制，替代传统的奖励信号。我们的实验表明，MM-UPT显著提高了Qwen2.5-VL-7B的推理能力（例如，在MathVista上的准确率从66.3%提升到72.9%，在We-Math上的准确率从62.9%提升到68.7%），并且无需使用带有真实标签的标准数据集。MM-UPT还优于先前的无监督基线，并接近监督GRPO的结果。此外，我们展示了集成由MLLM自身生成的合成问题也可以提升性能，这揭示了一种可行的可扩展自我改进方法。总体而言，MM-UPT为在缺乏外部监督的情况下持续自主提升MLLMs提供了新的范式。我们的代码可在此处获得。 

---
# NFR: Neural Feature-Guided Non-Rigid Shape Registration 

**Title (ZH)**: 神经特征引导的非刚性形状配准 

**Authors**: Puhua Jiang, Zhangquan Chen, Mingze Sun, Ruqi Huang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22445)  

**Abstract**: In this paper, we propose a novel learning-based framework for 3D shape registration, which overcomes the challenges of significant non-rigid deformation and partiality undergoing among input shapes, and, remarkably, requires no correspondence annotation during training. Our key insight is to incorporate neural features learned by deep learning-based shape matching networks into an iterative, geometric shape registration pipeline. The advantage of our approach is two-fold -- On one hand, neural features provide more accurate and semantically meaningful correspondence estimation than spatial features (e.g., coordinates), which is critical in the presence of large non-rigid deformations; On the other hand, the correspondences are dynamically updated according to the intermediate registrations and filtered by consistency prior, which prominently robustify the overall pipeline. Empirical results show that, with as few as dozens of training shapes of limited variability, our pipeline achieves state-of-the-art results on several benchmarks of non-rigid point cloud matching and partial shape matching across varying settings, but also delivers high-quality correspondences between unseen challenging shape pairs that undergo both significant extrinsic and intrinsic deformations, in which case neither traditional registration methods nor intrinsic methods work. 

**Abstract (ZH)**: 基于学习的三维形状配准新型框架：克服显著非刚性变形和部分性问题，无需训练对应标注 

---
# SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning 

**Title (ZH)**: SOReL 和 TOReL：两种完全离线强化学习方法 

**Authors**: Mattie Fellows, Clarisse Wibault, Uljad Berdica, Johannes Forkel, Jakob N. Foerster, Michael A. Osborne  

**Link**: [PDF](https://arxiv.org/pdf/2505.22442)  

**Abstract**: Sample efficiency remains a major obstacle for real world adoption of reinforcement learning (RL): success has been limited to settings where simulators provide access to essentially unlimited environment interactions, which in reality are typically costly or dangerous to obtain. Offline RL in principle offers a solution by exploiting offline data to learn a near-optimal policy before deployment. In practice, however, current offline RL methods rely on extensive online interactions for hyperparameter tuning, and have no reliable bound on their initial online performance. To address these two issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe offline reinforcement learning. Using only offline data, our Bayesian approach infers a posterior over environment dynamics to obtain a reliable estimate of the online performance via the posterior predictive uncertainty. Crucially, all hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a tuning for offline reinforcement learning algorithm that extends our information rate based offline hyperparameter tuning methods to general offline RL approaches. Our empirical evaluation confirms SOReL's ability to accurately estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter tuning achieves competitive performance with the best online hyperparameter tuning methods using only offline data. Thus, SOReL and TOReL make a significant step towards safe and reliable offline RL, unlocking the potential for RL in the real world. Our implementations are publicly available: this https URL\_torel. 

**Abstract (ZH)**: 安全的离线强化学习：SOReL和TOReL算法 

---
# Can NeRFs See without Cameras? 

**Title (ZH)**: NeRFs能在没有相机的情况下看见吗？ 

**Authors**: Chaitanya Amballa, Sattwik Basu, Yu-Lin Wei, Zhijian Yang, Mehmet Ergezer, Romit Roy Choudhury  

**Link**: [PDF](https://arxiv.org/pdf/2505.22441)  

**Abstract**: Neural Radiance Fields (NeRFs) have been remarkably successful at synthesizing novel views of 3D scenes by optimizing a volumetric scene function. This scene function models how optical rays bring color information from a 3D object to the camera pixels. Radio frequency (RF) or audio signals can also be viewed as a vehicle for delivering information about the environment to a sensor. However, unlike camera pixels, an RF/audio sensor receives a mixture of signals that contain many environmental reflections (also called "multipath"). Is it still possible to infer the environment using such multipath signals? We show that with redesign, NeRFs can be taught to learn from multipath signals, and thereby "see" the environment. As a grounding application, we aim to infer the indoor floorplan of a home from sparse WiFi measurements made at multiple locations inside the home. Although a difficult inverse problem, our implicitly learnt floorplans look promising, and enables forward applications, such as indoor signal prediction and basic ray tracing. 

**Abstract (ZH)**: 神经辐射场（NeRFs）通过优化体绘制场景函数以生成3D场景的新视图取得了显著成功。该场景函数描述了光学射线如何将3D物体的颜色信息传递到相机像素。类似地，射频（RF）或音频信号也可以视为将环境信息传递给传感器的一种载体。然而，与相机像素不同，RF/音频传感器接收到的信号是包含多种环境反射的混合信号（也称为“多径”）。是否可以利用这类多径信号来推断环境信息？我们证明，通过对神经辐射场进行重新设计，可以使它们学会从多径信号中学习，从而“看到”环境。作为扎根应用，我们旨在通过室内多个位置的稀疏WiFi测量来推断房屋的平面布局。尽管这是一个棘手的逆问题，但我们隐式学习得到的平面布局具有前景，并可应用于室内信号预测和基本光线追踪等前景应用。 

---
# Synonymous Variational Inference for Perceptual Image Compression 

**Title (ZH)**: 同义变分推断在感知图像压缩中的应用 

**Authors**: Zijian Liang, Kai Niu, Changshuo Wang, Jin Xu, Ping Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22438)  

**Abstract**: Recent contributions of semantic information theory reveal the set-element relationship between semantic and syntactic information, represented as synonymous relationships. In this paper, we propose a synonymous variational inference (SVI) method based on this synonymity viewpoint to re-analyze the perceptual image compression problem. It takes perceptual similarity as a typical synonymous criterion to build an ideal synonymous set (Synset), and approximate the posterior of its latent synonymous representation with a parametric density by minimizing a partial semantic KL divergence. This analysis theoretically proves that the optimization direction of perception image compression follows a triple tradeoff that can cover the existing rate-distortion-perception schemes. Additionally, we introduce synonymous image compression (SIC), a new image compression scheme that corresponds to the analytical process of SVI, and implement a progressive SIC codec to fully leverage the model's capabilities. Experimental results demonstrate comparable rate-distortion-perception performance using a single progressive SIC codec, thus verifying the effectiveness of our proposed analysis method. 

**Abstract (ZH)**: 最近意义信息理论的贡献揭示了意义信息与句法信息之间的集合元素关系，表现为同义关系。本文基于这种同义视角，提出了一种同义变分推断（SVI）方法，重新分析感知图像压缩问题。该方法以感知相似性作为典型的同义标准，构建理想同义集（Synset），并通过最小化部分语义KL散度近似其潜在的同义表示的后验分布。理论分析证明，感知图像压缩的优化方向涵盖了现有的率-失真-感知方案。此外，我们引入了同义图像压缩（SIC），这是一种对应于SVI分析过程的新图像压缩方案，并实现了渐进SIC编解码器，充分发挥了模型的能力。实验结果表明，使用单个渐进SIC编解码器可以实现可比拟的率-失真-感知性能，从而验证了我们提出分析方法的有效性。 

---
# Scaling Reasoning without Attention 

**Title (ZH)**: 无注意力机制下的推理扩展 

**Authors**: Xueliang Zhao, Wei Wu, Lingpeng Kong  

**Link**: [PDF](https://arxiv.org/pdf/2505.22425)  

**Abstract**: Large language models (LLMs) have made significant advances in complex reasoning tasks, yet they remain bottlenecked by two core challenges: architectural inefficiency due to reliance on Transformers, and a lack of structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an attention-free language model that addresses both issues through architectural and data-centric innovations. Built on the state space dual (SSD) layers of Mamba-2, our model eliminates the need for self-attention and key-value caching, enabling fixed-memory, constant-time inference. To train it for complex reasoning, we propose a two-phase curriculum fine-tuning strategy based on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically structured problems via abstract concept selection and rationale-guided generation. On benchmark evaluations, \ourmodel-7B outperforms strong Transformer and hybrid models of comparable scale, and even surpasses the much larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on Livecodebench. These results highlight the potential of state space models as efficient and scalable alternatives to attention-based architectures for high-capacity reasoning. 

**Abstract (ZH)**: 大型语言模型（LLMs）在复杂推理任务上取得了显著进步，但仍受两大核心挑战的制约：依赖Transformer导致的架构效率低下，以及高难度领域缺乏结构化的微调。我们提出了\ourmodel，一种无需注意力机制的语言模型，通过架构和数据导向的创新解决了这些问题。基于Mamba-2的状态空间双层（SSD）结构，我们的模型消除了自我注意力和键值缓存的需要，实现固定内存和恒定时间的推理。为实现复杂推理任务的训练，我们提出了一种基于\textsc{PromptCoT}合成范式的两阶段课程微调策略，该策略通过抽象概念选择和推理指导生成教学结构化的任务。在基准测试中，\ourmodel-7B 在AIME 24 上优于同等规模的强Transformer和混合模型，高出2.6%；在AIME 25 上高出0.6%；在Livecodebench 上高出3.0%。这些结果突显了状态空间模型作为高容量推理任务中注意力机制架构的高效且可扩展的替代方案的潜力。 

---
# Mitigating Overthinking in Large Reasoning Models via Manifold Steering 

**Title (ZH)**: 通过流形导向减轻大型推理模型的过度思考 

**Authors**: Yao Huang, Huanran Chen, Shouwei Ruan, Yichi Zhang, Xingxing Wei, Yinpeng Dong  

**Link**: [PDF](https://arxiv.org/pdf/2505.22411)  

**Abstract**: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in solving complex tasks such as mathematics and coding. However, these models frequently exhibit a phenomenon known as overthinking during inference, characterized by excessive validation loops and redundant deliberation, leading to substantial computational overheads. In this paper, we aim to mitigate overthinking by investigating the underlying mechanisms from the perspective of mechanistic interpretability. We first showcase that the tendency of overthinking can be effectively captured by a single direction in the model's activation space and the issue can be eased by intervening the activations along this direction. However, this efficacy soon reaches a plateau and even deteriorates as the intervention strength increases. We therefore systematically explore the activation space and find that the overthinking phenomenon is actually tied to a low-dimensional manifold, which indicates that the limited effect stems from the noises introduced by the high-dimensional steering direction. Based on this insight, we propose Manifold Steering, a novel approach that elegantly projects the steering direction onto the low-dimensional activation manifold given the theoretical approximation of the interference noise. Extensive experiments on DeepSeek-R1 distilled models validate that our method reduces output tokens by up to 71% while maintaining and even improving the accuracy on several mathematical benchmarks. Our method also exhibits robust cross-domain transferability, delivering consistent token reduction performance in code generation and knowledge-based QA tasks. Code is available at: this https URL. 

**Abstract (ZH)**: 近期大型推理模型（LRMs）在解决数学和编码等复杂任务方面展现了显著的能力。然而，这些模型在推理过程中经常会出现过度思考的现象，表现为过度验证循环和冗余的审议，导致了显著的计算开销。在本文中，我们旨在通过机制可解释性的视角来减轻过度思考的问题。首先，我们展示了过度思考的倾向可以通过模型激活空间中的一个方向来有效捕捉，并且可以通过干预这个方向上的激活来缓解该问题。然而，这种效果很快达到瓶颈甚至随着干预强度的增加而恶化。因此，我们系统地探索了激活空间，发现过度思考现象实际上是与低维流形绑定的，这表明有限的效果源于高维导向方向引入的噪声。基于这一洞察，我们提出了流形导向这一新颖方法，它优雅地将导向方向投影到理论近似的干扰噪声下的低维激活流形上。在对DeepSeek-R1脱碳模型的广泛实验中，我们验证了该方法在多个数学基准测试中减少了多达71%的输出标记数，同时保持甚至改进了准确性。我们的方法还展示了跨域的鲁棒可迁移性，在代码生成和基于知识的问答任务中都实现了一致的标记减少性能。代码可在以下链接获取：this https URL。 

---
# Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation 

**Title (ZH)**: 基于物理的扩散模型的偏微分方程约束生成精炼 

**Authors**: Yi Zhang, Difan Zou  

**Link**: [PDF](https://arxiv.org/pdf/2505.22391)  

**Abstract**: Modeling physical systems in a generative manner offers several advantages, including the ability to handle partial observations, generate diverse solutions, and address both forward and inverse problems. Recently, diffusion models have gained increasing attention in the modeling of physical systems, particularly those governed by partial differential equations (PDEs). However, diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate steps, making it infeasible to directly enforce constraints on the clean sample $\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are typically applied to the expectation of clean samples $\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the learned score network. However, imposing PDE constraints on the expectation does not strictly represent the one on the true clean data, known as Jensen's Gap. This gap creates a trade-off: enforcing PDE constraints may come at the cost of reduced accuracy in generative modeling. To address this, we propose a simple yet effective post-hoc distillation approach, where PDE constraints are not injected directly into the diffusion process, but instead enforced during a post-hoc distillation stage. We term our method as Physics-Informed Distillation of Diffusion Models (PIDDM). This distillation not only facilitates single-step generation with improved PDE satisfaction, but also support both forward and inverse problem solving and reconstruction from randomly partial observation. Extensive experiments across various PDE benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over several recent and competitive baselines, such as PIDM, DiffusionPDE, and ECI-sampling, with less computation overhead. Our approach can shed light on more efficient and effective strategies for incorporating physical constraints into diffusion models. 

**Abstract (ZH)**: 基于生成方式建模物理系统具有多种优势，包括处理部分观测、生成多样化解和解决正向及逆向问题。近年来，扩散模型在基于偏微分方程（PDEs）的物理系统建模中受到日益关注。然而，扩散模型仅在中间步骤访问噪声数据 $\boldsymbol{x}_t$，使得在每个噪声级别直接对清洁样本 $\boldsymbol{x}_0$ 施加约束变得不可行。作为变通方法，通常将约束应用于清洁样本的期望 $\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$，并通过学习的分数网络估计这一期望。然而，在期望上施加PDE约束不能严格代表对真实清洁数据的约束，这被称为詹森间隙。这一差距创建了一个权衡：施加PDE约束可能会以降低生成建模准确性为代价。为解决这一问题，我们提出了一种简单而有效的事后调整方法，其中PDE约束不直接注入扩散过程，而是在事后调整阶段施加。我们称之为物理启发式扩散模型的调整方法（PIDDM）。这种调整不仅促进了单步骤生成并提高了PDE满足度，还支持正向和逆向问题的解决以及从随机部分观测的重建。在各种PDE基准实验中，PIDDM 显著提高了对几个最新的有竞争力baseline（如PIDM、DiffusionPDE和ECI-sampling）的PDE满足度，同时计算开销较低。我们的方法可以为将物理约束更高效且有效地整合到扩散模型中提供启示。 

---
# Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning 

**Title (ZH)**: 含扰乱训练、合并推断的连续学习两阶段框架 

**Authors**: Haomiao Qiu, Miao Zhang, Ziyue Qiao, Liqiang Nie  

**Link**: [PDF](https://arxiv.org/pdf/2505.22389)  

**Abstract**: Continual Learning (CL) aims to enable models to continuously acquire new knowledge from a sequence of tasks with avoiding the forgetting of learned information. However, existing CL methods only rely on the parameters of the most recent task for inference, which makes them susceptible to catastrophic forgetting. Inspired by the recent success of model merging techniques, we propose \textbf{Perturb-and-Merge (P\&M)}, a novel continual learning framework that integrates model merging into the CL paradigm to mitigate forgetting. Specifically, after training on each task, P\&M constructs a new model by forming a convex combination of the previous model and the newly trained task-specific model. Through theoretical analysis, we minimize the total loss increase across all tasks and derive an analytical solution for the optimal merging coefficient. To further improve the performance of the merged model, we observe that the degradation introduced during merging can be alleviated by a regularization term composed of the task vector and the Hessian matrix of the loss function. Interestingly, we show that this term can be efficiently approximated using second-order symmetric finite differences, and a stochastic perturbation strategy along the task vector direction is accordingly devised which incurs no additional forward or backward passes while providing an effective approximation of the regularization term. Finally, we combine P\&M with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead. Our proposed approach achieves state-of-the-art performance on several continual learning benchmark datasets. 

**Abstract (ZH)**: Perturb-and-Merge (P&M)：将模型合并技术应用于连续学习以减轻遗忘 

---
# DAM: Domain-Aware Module for Multi-Domain Dataset Condensation 

**Title (ZH)**: 领域aware模块在多领域数据集凝缩中的应用 

**Authors**: Jaehyun Choi, Gyojin Han, Dong-Jae Lee, Sunghyun Baek, Junmo Kim  

**Link**: [PDF](https://arxiv.org/pdf/2505.22387)  

**Abstract**: Dataset Condensation (DC) has emerged as a promising solution to mitigate the computational and storage burdens associated with training deep learning models. However, existing DC methods largely overlook the multi-domain nature of modern datasets, which are increasingly composed of heterogeneous images spanning multiple domains. In this paper, we extend DC and introduce Multi-Domain Dataset Condensation (MDDC), which aims to condense data that generalizes across both single-domain and multi-domain settings. To this end, we propose the Domain-Aware Module (DAM), a training-time module that embeds domain-related features into each synthetic image via learnable spatial masks. As explicit domain labels are mostly unavailable in real-world datasets, we employ frequency-based pseudo-domain labeling, which leverages low-frequency amplitude statistics. DAM is only active during the condensation process, thus preserving the same images per class (IPC) with prior methods. Experiments show that DAM consistently improves in-domain, out-of-domain, and cross-architecture performance over baseline dataset condensation methods. 

**Abstract (ZH)**: 多域数据集凝聚（MDDC） 

---
# Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained Maximum Size 

**Title (ZH)**: 约束最大规模联盟的精确算法与下界研究 

**Authors**: Foivos Fioravantes, Harmender Gahlawat, Nikolaos Melissinos  

**Link**: [PDF](https://arxiv.org/pdf/2505.22384)  

**Abstract**: Imagine we want to split a group of agents into teams in the most \emph{efficient} way, considering that each agent has their own preferences about their teammates. This scenario is modeled by the extensively studied \textsc{Coalition Formation} problem. Here, we study a version of this problem where each team must additionally be of bounded size.
We conduct a systematic algorithmic study, providing several intractability results as well as multiple exact algorithms that scale well as the input grows (FPT), which could prove useful in practice.
Our main contribution is an algorithm that deals efficiently with tree-like structures (bounded \emph{treewidth}) for ``small'' teams. We complement this result by proving that our algorithm is asymptotically optimal. Particularly, there can be no algorithm that vastly outperforms the one we present, under reasonable theoretical assumptions, even when considering star-like structures (bounded \emph{vertex cover number}). 

**Abstract (ZH)**: 我们研究了一个团队规模受限的联盟形成问题，其中每个代理有自己对队友的偏好。我们系统地研究了这一问题，提供了多项不可约结果以及多种适用于大规模输入的精确算法（FPT），这些算法在实践中可能很有用。我们的主要贡献是提出了一个高效处理树状结构（有限 treewidth）的“小”团队的算法，并证明了该算法在理论上是渐近最优的。特别是，在合理的理论假设下，即使对于类似星状结构（有限 vertex cover number）的情况，也没有算法可以显著超越我们提出的方法。 

---
# SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting 

**Title (ZH)**: SplitLoRA：通过梯度空间分裂在连续学习中平衡稳定性和可塑性 

**Authors**: Haomiao Qiu, Miao Zhang, Ziyue Qiao, Weili Guan, Min Zhang, Liqiang Nie  

**Link**: [PDF](https://arxiv.org/pdf/2505.22370)  

**Abstract**: Continual Learning requires a model to learn multiple tasks in sequence while maintaining both stability:preserving knowledge from previously learned tasks, and plasticity:effectively learning new tasks. Gradient projection has emerged as an effective and popular paradigm in CL, where it partitions the gradient space of previously learned tasks into two orthogonal subspaces: a primary subspace and a minor subspace. New tasks are learned effectively within the minor subspace, thereby reducing interference with previously acquired knowledge. However, existing Gradient Projection methods struggle to achieve an optimal balance between plasticity and stability, as it is hard to appropriately partition the gradient space. In this work, we consider a continual learning paradigm based on Low-Rank Adaptation, which has gained considerable attention due to its efficiency and wide applicability, and propose a novel approach for continual learning, called SplitLoRA. We first provide a theoretical analysis of how subspace partitioning affects model stability and plasticity. Informed by this analysis, we then introduce an effective method that derives the optimal partition of the gradient space for previously learned tasks. This approach effectively balances stability and plasticity in continual learning. Experimental results on multiple datasets demonstrate that the proposed method achieves state-of-the-art performance. 

**Abstract (ZH)**: 持续学习要求模型在保持稳定性（保留先前学习任务的知识）和灵活性（有效学习新任务）的前提下，顺序学习多个任务。梯度投影已成为持续学习中一个有效且流行的方法，它将先前学习任务的梯度空间划分为两个正交子空间：主子空间和次子空间。新任务在次子空间中有效学习，从而减少对先前获得知识的干扰。然而，现有的梯度投影方法难以在灵活性和稳定性之间实现最优平衡，因为梯度空间的适当划分很难做到。在本工作中，我们基于低秩适应的持续学习范式，提出了一种名为SplitLoRA的新方法。我们首先从理论上分析了子空间划分如何影响模型的稳定性和灵活性。根据这一分析，我们然后介绍了一种有效的方法，用于为先前学习的任务确定最优的梯度空间划分。该方法有效地在持续学习中平衡了稳定性和灵活性。在多个数据集上的实验结果表明，所提出的方法实现了最先进的性能。 

---
# Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs 

**Title (ZH)**: 在LLMs中用于持续学习的正交子空间自适应预算适配调谐 

**Authors**: Zhiyi Wan, Wanrou Du, Liang Li, Miao Pan, Xiaoqi Qin  

**Link**: [PDF](https://arxiv.org/pdf/2505.22358)  

**Abstract**: Large language models (LLMs) often suffer from catastrophic forgetting in continual learning (CL) scenarios, where performance on previously learned tasks degrades severely while training on sequentially arriving tasks. Although pioneering CL approaches using orthogonal subspaces can mitigate task interference, they typically employ fixed budget allocation, neglecting the varying complexity across tasks and layers. Besides, recent budget-adaptive tuning methods for LLMs often adopt multi-stage paradigms that decouple optimization and budget allocation. Such decoupling results in potential misalignment, which hinders those approaches' practical application in CL scenarios. To address these limitations, we propose OA-Adapter, a novel parameter-efficient approach for continual learning in LLMs that unifies dynamic budget adaptation with orthogonal subspace learning in a single end-to-end training stage. Specifically, OA-Adapter introduces a dynamic bottleneck dimension adaptation mechanism that simultaneously allocates an efficient parameter budget and optimizes task objectives without misalignment. To effectively preserve previously acquired knowledge while coordinating with the dynamic budget allocation, orthogonal constraints are applied specifically between the parameter subspace of the current task and the dynamically allocated parameter subspaces of historical tasks. Experimental results on continual learning benchmarks demonstrate that OA-Adapter outperforms state-of-the-art methods in both accuracy and parameter efficiency, achieving higher average accuracy while using 58.5% fewer parameters on the standard CL benchmark. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在连续学习（CL）场景中常常遭受灾难性遗忘的问题，在逐步训练新任务时，之前学习的任务性能严重下降。尽管一些使用正交子空间的先驱连续学习方法可以减轻任务干扰，但它们通常采用固定预算分配，忽视了任务之间以及层之间的变化复杂性。此外，最近针对LLMs的预算自适应调整方法往往会采用多阶段范式，将优化与预算分配解耦。这种解耦可能导致潜在的不对齐，阻碍了这些方法在CL场景中的实际应用。为了解决这些局限性，我们提出了一种名为OA-Adapter的新方法，它在单一端到端训练阶段统一了动态预算自适应与正交子空间学习。具体来说，OA-Adapter引入了一种动态瓶颈维度自适应机制，在不发生不对齐的情况下同时分配有效的参数预算并优化任务目标。为了有效地保留之前获得的知识并协调与动态预算分配，特别在当前任务的参数子空间与动态分配的历史任务参数子空间之间应用了正交约束。在连续学习基准测试上的实验结果表明，OA-Adapter在准确性和参数效率方面均优于现有方法，在标准连续学习基准测试中使用的参数减少了58.5%，同时达到了更高的平均准确率。 

---
# Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings 

**Title (ZH)**: 适合性过滤：在实际部署环境中分类器评估的统计框架 

**Authors**: Angéline Pouget, Mohammad Yaghini, Stephan Rabanser, Nicolas Papernot  

**Link**: [PDF](https://arxiv.org/pdf/2505.22356)  

**Abstract**: Deploying machine learning models in safety-critical domains poses a key challenge: ensuring reliable model performance on downstream user data without access to ground truth labels for direct validation. We propose the suitability filter, a novel framework designed to detect performance deterioration by utilizing suitability signals -- model output features that are sensitive to covariate shifts and indicative of potential prediction errors. The suitability filter evaluates whether classifier accuracy on unlabeled user data shows significant degradation compared to the accuracy measured on the labeled test dataset. Specifically, it ensures that this degradation does not exceed a pre-specified margin, which represents the maximum acceptable drop in accuracy. To achieve reliable performance evaluation, we aggregate suitability signals for both test and user data and compare these empirical distributions using statistical hypothesis testing, thus providing insights into decision uncertainty. Our modular method adapts to various models and domains. Empirical evaluations across different classification tasks demonstrate that the suitability filter reliably detects performance deviations due to covariate shift. This enables proactive mitigation of potential failures in high-stakes applications. 

**Abstract (ZH)**: 在安全关键领域部署机器学习模型 poses 在安全关键领域部署机器学习模型：确保无标签用户数据上的可靠性能而不直接访问地面 truth 标签进行验证提出了一个关键挑战。我们提出了一致性过滤器，这是一种新颖的框架，通过利用一致性信号——对协变量偏移敏感且能指示潜在预测错误的模型输出特征来检测性能退化。一致性过滤器评估未标记用户数据上的分类器准确率是否相对于标记测试数据集上的准确率显著下降。具体来说，它确保这种下降不超过预先指定的边际，该边际代表了可接受的准确率下降的最大值。为了实现可靠的性能评估，我们聚合了测试数据和用户数据的一致性信号，并使用统计假设检验比较这些经验分布，从而提供关于决策不确定性的见解。我们的模块化方法适用于各种模型和领域。在不同分类任务上的实证评估表明，一致性过滤器可以可靠地检测由于协变量偏移导致的性能偏差。这在高风险应用中实现了潜在故障的前瞻式缓解。 

---
# VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond 

**Title (ZH)**: VME：用于检测中东及其他地区车辆的遥感图像数据集及其基准测试 

**Authors**: Noora Al-Emadi, Ingmar Weber, Yin Yang, Ferda Ofli  

**Link**: [PDF](https://arxiv.org/pdf/2505.22353)  

**Abstract**: Detecting vehicles in satellite images is crucial for traffic management, urban planning, and disaster response. However, current models struggle with real-world diversity, particularly across different regions. This challenge is amplified by geographic bias in existing datasets, which often focus on specific areas and overlook regions like the Middle East. To address this gap, we present the Vehicles in the Middle East (VME) dataset, designed explicitly for vehicle detection in high-resolution satellite images from Middle Eastern countries. Sourced from Maxar, the VME dataset spans 54 cities across 12 countries, comprising over 4,000 image tiles and more than 100,000 vehicles, annotated using both manual and semi-automated methods. Additionally, we introduce the largest benchmark dataset for Car Detection in Satellite Imagery (CDSI), combining images from multiple sources to enhance global car detection. Our experiments demonstrate that models trained on existing datasets perform poorly on Middle Eastern images, while the VME dataset significantly improves detection accuracy in this region. Moreover, state-of-the-art models trained on CDSI achieve substantial improvements in global car detection. 

**Abstract (ZH)**: 中东地区卫星图像中的车辆检测对于交通管理、城市规划和灾害响应至关重要。然而，当前模型在应对现实世界中的多样化挑战，特别是在不同地区之间，表现不佳。现有数据集中的地理偏差进一步加剧了这一挑战，这些数据集往往集中在特定区域，忽视了如中东这样的地区。为解决这一问题，我们提出了中东车辆（VME）数据集，该数据集专门用于中东国家高分辨率卫星图像中的车辆检测。数据集来源于Maxar，覆盖12个国家的54个城市，包含超过4,000个图像瓦片和超过100,000辆车辆的标注，采用手动和半自动化方法进行标注。此外，我们还引入了最大的卫星图像中汽车检测基准数据集（CDSI），结合了多个来源的数据，以增强全球汽车检测能力。实验结果显示，现有的数据集训练的模型在中东图像上的性能不佳，而VME数据集显著改善了该地区的检测精度。此外，基于CDSI训练的最新模型在全球汽车检测方面取得了显著改进。 

---
# ChatPD: An LLM-driven Paper-Dataset Networking System 

**Title (ZH)**: ChatPD：一个由大规模语言模型驱动的论文-数据集网络系统 

**Authors**: Anjie Xu, Ruiqing Ding, Leye Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22349)  

**Abstract**: Scientific research heavily depends on suitable datasets for method validation, but existing academic platforms with dataset management like PapersWithCode suffer from inefficiencies in their manual workflow. To overcome this bottleneck, we present a system, called ChatPD, that utilizes Large Language Models (LLMs) to automate dataset information extraction from academic papers and construct a structured paper-dataset network. Our system consists of three key modules: \textit{paper collection}, \textit{dataset information extraction}, and \textit{dataset entity resolution} to construct paper-dataset networks. Specifically, we propose a \textit{Graph Completion and Inference} strategy to map dataset descriptions to their corresponding entities. Through extensive experiments, we demonstrate that ChatPD not only outperforms the existing platform PapersWithCode in dataset usage extraction but also achieves about 90\% precision and recall in entity resolution tasks. Moreover, we have deployed ChatPD to continuously extract which datasets are used in papers, and provide a dataset discovery service, such as task-specific dataset queries and similar dataset recommendations. We open source ChatPD and the current paper-dataset network on this [GitHub repository]{this https URL}. 

**Abstract (ZH)**: 科学研究高度依赖 Suitable Datasets 进行方法验证，但现有像 PapersWithCode 这样的学术平台在数据集管理方面存在手动工作流程上的低效问题。为克服这一瓶颈，我们提出了一种名为 ChatPD 的系统，利用大型语言模型（LLMs）自动化提取学术论文中的数据集信息并构建结构化的论文-数据集网络。该系统包含三个关键模块：论文收集、数据集信息提取和数据集实体解析，以构建论文-数据集网络。具体而言，我们提出了一种图填充和推理策略，将数据集描述映射到其相应的实体。通过广泛的实验，我们证明 ChatPD 不仅在数据集使用提取方面优于现有平台 PapersWithCode，还在实体解析任务中实现了约 90% 的精准率和召回率。此外，我们已部署 ChatPD 以持续提取论文中使用哪些数据集，并提供数据集发现服务，如特定任务的数据集查询和相似数据集推荐。我们已开源 ChatPD 以及当前的论文-数据集网络于 [GitHub 存储库]。 

---
# Empowering Intelligent Low-altitude Economy with Large AI Model Deployment 

**Title (ZH)**: 赋能智能化低空经济的大规模AI模型部署 

**Authors**: Zhonghao Lyu, Yulan Gao, Junting Chen, Hongyang Du, Jie Xu, Kaibin Huang, Dong In Kim  

**Link**: [PDF](https://arxiv.org/pdf/2505.22343)  

**Abstract**: Low-altitude economy (LAE) represents an emerging economic paradigm that redefines commercial and social aerial activities. Large artificial intelligence models (LAIMs) offer transformative potential to further enhance the intelligence of LAE services. However, deploying LAIMs in LAE poses several challenges, including the significant gap between their computational/storage demands and the limited onboard resources of LAE entities, the mismatch between lab-trained LAIMs and dynamic physical environments, and the inefficiencies of traditional decoupled designs for sensing, communication, and computation. To address these issues, we first propose a hierarchical system architecture tailored for LAIM deployment and present representative LAE application scenarios. Next, we explore key enabling techniques that facilitate the mutual co-evolution of LAIMs and low-altitude systems, and introduce a task-oriented execution pipeline for scalable and adaptive service delivery. Then, the proposed framework is validated through real-world case studies. Finally, we outline open challenges to inspire future research. 

**Abstract (ZH)**: 低空经济中大型人工智能模型的部署及其关键使能技术研究 

---
# Text2Grad: Reinforcement Learning from Natural Language Feedback 

**Title (ZH)**: Text2Grad: 从自然语言反馈中学习的强化学习 

**Authors**: Hanyang Wang, Lu Wang, Chaoyun Zhang, Tianjun Mao, Si Qin, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22338)  

**Abstract**: Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model's policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization. The code for our method is available at this https URL 

**Abstract (ZH)**: 基于文本的梯度优化的强化学习细粒度策略优化 

---
# Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start 

**Title (ZH)**: 基于冷启动的强化学习促进多模态推理 

**Authors**: Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong, Lichao Sun, Weiran Huang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22334)  

**Abstract**: Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While "aha moment" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at this https URL. 

**Abstract (ZH)**: 近期大型语言模型的进步展示了强大的链式思考推理能力，强化学习在这一进展中起到了关键作用。虽然“洞察时刻”模式——模型通过反思进行自我修正——通常被归因于来自强化学习的 emergent 属性，我们首先证明了这些模式在强化学习之前的多模态大型语言模型（MLLMs）中存在，但未必与推理性能的提高有必然联系。基于这些见解，我们提出了一种全面的方法来增强多模态推理，该方法采用两阶段策略：（1）监督微调 (SFT) 作为冷启动，使用结构化的链式思考推理模式，随后是（2）通过 GRPO 进行强化学习以进一步完善这些能力。我们广泛的经验表明，这种结合的方法在具有挑战性的多模态推理基准测试中一致优于仅使用 SFT 和仅使用 RL 的方法。生成的模型在开源 MLLMs 中实现了最先进的性能，其中我们的 7B 模型在 MathVista 和 We-Math 上显示出显著改进（例如，从 66.3% 到 73.4%，从 62.9% 到 70.4%），而我们的 3B 模型的性能与一些 7B 模型相当。总之，这项工作为构建高级多模态推理模型提供了实用指导。我们的代码可在以下网址获取。 

---
# Skywork Open Reasoner 1 Technical Report 

**Title (ZH)**: Skywork 开放推理器 1 技术报告 

**Authors**: Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, Yahui Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2505.22312)  

**Abstract**: The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets. 

**Abstract (ZH)**: DeepSeek-R1 的成功突显了强化学习（RL）在提升大语言模型（LLMs）推理能力中的重要作用。在此基础上，我们提出了 Skywork-OR1，一种有效的可扩展的 RL 实现方法，用于长链推理（CoT）模型。借助 DeepSeek-R1-Distill 模型系列，我们的 RL 方法取得了显著的性能提升，AIME24、AIME25 和 LiveCodeBench 的 32B 模型平均准确性从 57.8% 提高到 72.8%（提高 15.0%），7B 模型从 43.6% 提高到 57.5%（提高 13.9%）。Skywork-OR1-32B 模型在 AIME24 和 AIME25 中超越了 DeepSeek-R1 和 Qwen3-32B，在 LiveCodeBench 上达到相似结果。Skywork-OR1-7B 和 Skywork-OR1-Math-7B 模型在同类大小模型中展示了竞争性的推理能力。我们在训练管道的核心组件上进行了全面的消融研究，以验证其有效性。此外，我们详细探讨了熵崩溃现象，确定了影响熵动力学的关键因素，并证明缓解过早的熵崩溃对于提高测试性能至关重要。为支持社群研究，我们全面开源了模型权重、训练代码和训练数据集。 

---
# From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization 

**Title (ZH)**: 从静默到删除：通过权重空间正则化实现抗篡改的遗忘 

**Authors**: Shoaib Ahmed Siddiqui, Adrian Weller, David Krueger, Gintare Karolina Dziugaite, Michael Curtis Mozer, Eleni Triantafillou  

**Link**: [PDF](https://arxiv.org/pdf/2505.22310)  

**Abstract**: Recent unlearning methods for LLMs are vulnerable to relearning attacks: knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of (even seemingly-unrelated) examples. We study this phenomenon in a controlled setting for example-level unlearning in vision classifiers. We make the surprising discovery that forget-set accuracy can recover from around 50% post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e., zero examples of the forget set. We observe this effect across a wide variety of unlearning methods, whereas for a model retrained from scratch excluding the forget set (gold standard), the accuracy remains at 50%. We observe that resistance to relearning attacks can be predicted by weight-space properties, specifically, $L_2$-distance and linear mode connectivity between the original and the unlearned model. Leveraging this insight, we propose a new class of methods that achieve state-of-the-art resistance to relearning attacks. 

**Abstract (ZH)**: 近期针对大模型的遗忘方法易受重学攻击：通过小规模（甚至看似无关的）示例调优，原本认为已被遗忘的知识会重新显现。我们在一个可控环境中研究了这种现象在视觉分类器中示例级遗忘的行为。我们惊讶地发现，通过仅对保留集进行微调，遗忘集的准确率可以从中遗忘后的约50%恢复到接近100%，即零遗忘集样本。我们观察到这一现象在多种遗忘方法中普遍存在，而从头训练排除遗忘集的模型则保持约50%的准确率。我们观察到，对重学攻击的抵抗性可以通过权重空间特性预测，具体而言，是原始模型与未学习模型之间的$L_2$距离和线性模式连接性。利用这一见解，我们提出了一类新的方法，实现了对抗重学攻击的最新性能。 

---
# Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer 

**Title (ZH)**: 统一扩散变换器实现多功能心血管信号生成 

**Authors**: Zehua Chen, Yuyang Miao, Liyuan Wang, Luyun Fan, Danilo P. Mandic, Jun Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2505.22306)  

**Abstract**: Cardiovascular signals such as photoplethysmography (PPG), electrocardiography (ECG), and blood pressure (BP) are inherently correlated and complementary, together reflecting the health of cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multi-modal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation, and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, while ensuring interpretability for human experts. These advantages position UniCardio as a promising avenue for advancing AI-assisted healthcare. 

**Abstract (ZH)**: 多模态扩散变换器UniCardio在心血管信号重建与合成中的应用研究 

---
# Voice CMS: updating the knowledge base of a digital assistant through conversation 

**Title (ZH)**: Voice CMS：通过对话更新数字助理的知识库 

**Authors**: Grzegorz Wolny, Michał Szczerbak  

**Link**: [PDF](https://arxiv.org/pdf/2505.22303)  

**Abstract**: In this study, we propose a solution based on a multi-agent LLM architecture and a voice user interface (VUI) designed to update the knowledge base of a digital assistant. Its usability is evaluated in comparison to a more traditional graphical content management system (CMS), with a focus on understanding the relationship between user preferences and the complexity of the information being provided. The findings demonstrate that, while the overall usability of the VUI is rated lower than the graphical interface, it is already preferred by users for less complex tasks. Furthermore, the quality of content entered through the VUI is comparable to that achieved with the graphical interface, even for highly complex tasks. Obtained qualitative results suggest that a hybrid interface combining the strengths of both approaches could address the key challenges identified during the experiment, such as reducing cognitive load through graphical feedback while maintaining the intuitive nature of voice-based interactions. This work highlights the potential of conversational interfaces as a viable and effective method for knowledge management in specific business contexts. 

**Abstract (ZH)**: 本研究提出一种基于多代理语言模型架构和语音用户界面（VUI）的解决方案，旨在更新数字助手的知识库。并与传统的图形内容管理系统（CMS）进行 usability 评估，重点关注用户偏好与提供信息复杂性之间的关系。研究发现，尽管 VUI 的整体 usability 评分低于图形界面，但它已在复杂度较低的任务中更受用户青睐。此外，通过 VUI 输入的内容质量与图形界面相当，即使对于高度复杂任务也是如此。获得的定性结果表明，结合两种方法优点的混合界面可能解决实验中识别的关键挑战，如通过图形反馈减轻认知负担同时维持基于语音交互的直观性。本研究突显了对话界面在特定商业背景下作为知识管理可行且有效方法的潜力。 

---
# Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data 

**Title (ZH)**: 基于纯合成数据的历史自动彩摄影像绿色缺陷神经恢复 

**Authors**: Saptarshi Neil Sinha, P. Julius Kuehn, Johannes Koppe, Arjan Kuijper, Michael Weinmann  

**Link**: [PDF](https://arxiv.org/pdf/2505.22291)  

**Abstract**: The preservation of early visual arts, particularly color photographs, is challenged by deterioration caused by aging and improper storage, leading to issues like blurring, scratches, color bleeding, and fading defects. In this paper, we present the first approach for the automatic removal of greening color defects in digitized autochrome photographs. Our main contributions include a method based on synthetic dataset generation and the use of generative AI with a carefully designed loss function for the restoration of visual arts. To address the lack of suitable training datasets for analyzing greening defects in damaged autochromes, we introduce a novel approach for accurately simulating such defects in synthetic data. We also propose a modified weighted loss function for the ChaIR method to account for color imbalances between defected and non-defected areas. While existing methods struggle with accurately reproducing original colors and may require significant manual effort, our method allows for efficient restoration with reduced time requirements. 

**Abstract (ZH)**: 早期视觉艺术的保存，尤其是彩色照片，面临着因老化和不当存储导致的退化问题，这些问题包括模糊、刮痕、颜色溢出和褪色等缺陷。本文提出了首个针对数字化Autochrome照片中绿色颜色缺陷的自动去除方法。我们的主要贡献包括基于合成数据集生成的方法以及使用生成AI并通过精心设计的损失函数进行视觉艺术的修复。为了解决缺乏用于分析损坏Autochrome中绿色缺陷的适当训练数据集的问题，我们提出了一种新颖的方法来在合成数据中准确模拟此类缺陷。我们还提出了对ChaIR方法的修改加权损失函数，以考虑到缺陷区域和非缺陷区域之间的颜色不平衡。与现有方法相比，我们的方法能够更高效地进行修复，减少所需的时间，同时能够更准确地再现原始颜色。 

---
# New Tools are Needed for Tracking Adherence to AI Model Behavioral Use Clauses 

**Title (ZH)**: 需要新的工具来跟踪AI模型行为使用条款的遵守情况。 

**Authors**: Daniel McDuff, Tim Korjakow, Kevin Klyman, Danish Contractor  

**Link**: [PDF](https://arxiv.org/pdf/2505.22287)  

**Abstract**: Foundation models have had a transformative impact on AI. A combination of large investments in research and development, growing sources of digital data for training, and architectures that scale with data and compute has led to models with powerful capabilities. Releasing assets is fundamental to scientific advancement and commercial enterprise. However, concerns over negligent or malicious uses of AI have led to the design of mechanisms to limit the risks of the technology. The result has been a proliferation of licenses with behavioral-use clauses and acceptable-use-policies that are increasingly being adopted by commonly used families of models (Llama, Gemma, Deepseek) and a myriad of smaller projects. We created and deployed a custom AI licenses generator to facilitate license creation and have quantitatively and qualitatively analyzed over 300 customized licenses created with this tool. Alongside this we analyzed 1.7 million models licenses on the HuggingFace model hub. Our results show increasing adoption of these licenses, interest in tools that support their creation and a convergence on common clause configurations. In this paper we take the position that tools for tracking adoption of, and adherence to, these licenses is the natural next step and urgently needed in order to ensure they have the desired impact of ensuring responsible use. 

**Abstract (ZH)**: 基础模型在人工智能领域产生了变革性影响。巨大的研发投入、不断增长的数字训练数据来源以及可扩展的架构推动了这些模型具备强大的能力。发布资产是科学进步和商业企业的重要组成部分。然而，对AI不负责任或恶意使用方面的担忧促使设计机制以限制技术的风险。结果是行为使用条款和acceptable-use-policies许可证越来越多地被广泛使用的模型家族（如Llama、Gemma、Deepseek）以及众多较小的项目采用。我们创建并部署了一个自定义AI许可证生成器以促进许可证的创建，并对该工具生成的300多个定制许可证进行了定量和定性的分析。同时，我们分析了HuggingFace模型库中的170万个模型许可证。我们的结果显示，这些许可证的采用正在增加，对支持其创建的工具的兴趣也在增长，并且越来越多地采用共同的条款配置。在这篇文章中，我们认为追踪这些许可证的采用情况和遵守情况的工具是自然的下一步，并且迫切需要以确保它们能够实现负责任使用的预期效果。 

---
# Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review 

**Title (ZH)**: 基于证据的医学中自然语言处理的应用：一项范围性回顾 

**Authors**: Zihan Xu, Haotian Ma, Gongbo Zhang, Yihao Ding, Chunhua Weng, Yifan Peng  

**Link**: [PDF](https://arxiv.org/pdf/2505.22280)  

**Abstract**: Evidence-based medicine (EBM) is at the forefront of modern healthcare, emphasizing the use of the best available scientific evidence to guide clinical decisions. Due to the sheer volume and rapid growth of medical literature and the high cost of curation, there is a critical need to investigate Natural Language Processing (NLP) methods to identify, appraise, synthesize, summarize, and disseminate evidence in EBM. This survey presents an in-depth review of 129 research studies on leveraging NLP for EBM, illustrating its pivotal role in enhancing clinical decision-making processes. The paper systematically explores how NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise, Apply, and Assess. The review not only identifies current limitations within the field but also proposes directions for future research, emphasizing the potential for NLP to revolutionize EBM by refining evidence extraction, evidence synthesis, appraisal, summarization, enhancing data comprehensibility, and facilitating a more efficient clinical workflow. 

**Abstract (ZH)**: 基于证据的医学（EBM）处于现代医疗的前沿，强调使用最佳的科学证据来指导临床决策。由于医疗文献的数量庞大且增长迅速，以及文献整理的高成本，迫切需要研究自然语言处理（NLP）方法以识别、评估、综合、总结和传播EBM中的证据。本综述全面回顾了129项利用NLP进行EBM的研究，展示了NLP在增强临床决策过程中的核心作用。文章系统探讨了NLP如何支持EBM的五个基本步骤——提出问题、获取信息、评估、应用和评估。综述不仅指出了该领域的现有局限性，还提出了未来研究的方向，强调NLP有潜力通过精炼证据提取、综合、评估、总结，提高数据可理解性以及促进更高效的临床工作流程来彻底革新EBM。 

---
# Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models 

**Title (ZH)**: 测试时免疫：一种针对（多模态）大型语言模型脱逃攻击的通用防御框架 

**Authors**: Yongcan Yu, Yanbo Wang, Ran He, Jian Liang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22271)  

**Abstract**: While (multimodal) large language models (LLMs) have attracted widespread attention due to their exceptional capabilities, they remain vulnerable to jailbreak attacks. Various defense methods are proposed to defend against jailbreak attacks, however, they are often tailored to specific types of jailbreak attacks, limiting their effectiveness against diverse adversarial strategies. For instance, rephrasing-based defenses are effective against text adversarial jailbreaks but fail to counteract image-based attacks. To overcome these limitations, we propose a universal defense framework, termed Test-time IMmunization (TIM), which can adaptively defend against various jailbreak attacks in a self-evolving way. Specifically, TIM initially trains a gist token for efficient detection, which it subsequently applies to detect jailbreak activities during inference. When jailbreak attempts are identified, TIM implements safety fine-tuning using the detected jailbreak instructions paired with refusal answers. Furthermore, to mitigate potential performance degradation in the detector caused by parameter updates during safety fine-tuning, we decouple the fine-tuning process from the detection module. Extensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy of TIM. 

**Abstract (ZH)**: 通用防御框架：测试时免疫（TIM），以适应性防御多种劫持攻击 

---
# MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps 

**Title (ZH)**: MRT在SemEval-2025任务8中的应用：多步从表格中最大化恢复信息 

**Authors**: Maximiliano Hormazábal Lagos, Álvaro Bueno Saez, Héctor Cerezo-Costas, Pedro Alonso Doval, Jorge Alcalde Vesteiro  

**Link**: [PDF](https://arxiv.org/pdf/2505.22264)  

**Abstract**: In this paper we expose our approach to solve the \textit{SemEval 2025 Task 8: Question-Answering over Tabular Data} challenge. Our strategy leverages Python code generation with LLMs to interact with the table and get the answer to the questions. The process is composed of multiple steps: understanding the content of the table, generating natural language instructions in the form of steps to follow in order to get the answer, translating these instructions to code, running it and handling potential errors or exceptions. These steps use open source LLMs and fine grained optimized prompts for each task (step). With this approach, we achieved a score of $70.50\%$ for subtask 1. 

**Abstract (ZH)**: 在本文中，我们介绍了解决“SemEval 2025 Task 8：表格数据上的问题-答案”的方法。我们的策略利用LLMs生成Python代码以与表格交互并获取问题的答案。该过程包括多个步骤：理解表格的内容，生成自然语言指令以获取答案的形式，将这些指令翻译成代码，运行代码并处理潜在的错误或异常。通过这种方法，我们在子任务1上取得了70.50%的得分。 

---
# Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models 

**Title (ZH)**: 跨语言质量评估：多语言预训练数据过滤的方法 

**Authors**: Mehdi Ali, Manuel Brack, Max Lübbering, Elias Wendt, Abbas Goher Khan, Richard Rutmann, Alex Jude, Maurice Kraus, Alexander Arno Weber, Felix Stollenwerk, David Kaczér, Florian Mai, Lucie Flek, Rafet Sifa, Nicolas Flores-Herr, Joachim Köhler, Patrick Schramowski, Michael Fromm, Kristian Kersting  

**Link**: [PDF](https://arxiv.org/pdf/2505.22232)  

**Abstract**: High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development. 

**Abstract (ZH)**: 高质量的多语言训练数据对于有效预训练大规模语言模型（LLMs）至关重要。然而，适合的开源多语言数据集仍然有限。现有的前沿数据集主要依赖于启发式过滤方法，限制了它们的跨语言迁移能力和可扩展性。在这里，我们介绍了JQL，这是一种系统的方法，能够在大规模范围内高效地编目多样性和高质量的多语言数据，同时显著降低计算需求。JQL通过基于预训练多语言嵌入的轻量级注释器，提炼出LLMs的标注能力。这些模型即使对于训练期间未见过的语言和脚本，也表现出稳健的多语言和跨语言性能。在35种语言上进行实证评估后，所得的标注管道显著优于当前的启发式过滤方法如Fineweb2。JQL在下游模型训练质量方面显著提升，并增加数据保留率。我们的研究为多语言数据编目的实践提供了有益的见解和有价值的资源，提升了多语言数据集开发的标准。 

---
# Solver-Free Decision-Focused Learning for Linear Optimization Problems 

**Title (ZH)**: 求解器自由的决策导向学习方法用于线性优化问题 

**Authors**: Senne Berden, Ali İrfan Mahmutoğulları, Dimos Tsouros, Tias Guns  

**Link**: [PDF](https://arxiv.org/pdf/2505.22224)  

**Abstract**: Mathematical optimization is a fundamental tool for decision-making in a wide range of applications. However, in many real-world scenarios, the parameters of the optimization problem are not known a priori and must be predicted from contextual features. This gives rise to predict-then-optimize problems, where a machine learning model predicts problem parameters that are then used to make decisions via optimization. A growing body of work on decision-focused learning (DFL) addresses this setting by training models specifically to produce predictions that maximize downstream decision quality, rather than accuracy. While effective, DFL is computationally expensive, because it requires solving the optimization problem with the predicted parameters at each loss evaluation. In this work, we address this computational bottleneck for linear optimization problems, a common class of problems in both DFL literature and real-world applications. We propose a solver-free training method that exploits the geometric structure of linear optimization to enable efficient training with minimal degradation in solution quality. Our method is based on the insight that a solution is optimal if and only if it achieves an objective value that is at least as good as that of its adjacent vertices on the feasible polytope. Building on this, our method compares the estimated quality of the ground-truth optimal solution with that of its precomputed adjacent vertices, and uses this as loss function. Experiments demonstrate that our method significantly reduces computational cost while maintaining high decision quality. 

**Abstract (ZH)**: 数学优化是广泛的应用中决策制定的基本工具。然而，在许多现实场景中，优化问题的参数事先未知，必须从上下文特征中预测得出。这导致了先预测后优化的问题，即通过机器学习模型预测问题参数，然后使用优化方法进行决策。决策导向学习（DFL）领域的大量工作针对这一设置，通过训练模型以产生最大化下游决策质量的预测，而非单纯追求准确性。尽管有效，DFL在计算上非常昂贵，因为它要求在每次损失评估时以预测参数求解优化问题。在本工作中，我们针对线性优化问题，这一DFL文献和实际应用中常见的问题类别，解决了这一计算瓶颈。我们提出了一种无需求解器的训练方法，利用线性优化的几何结构实现高效的训练，同时保持较高的解决方案质量。该方法基于这样的洞察：一个解是optimal当且仅当其目标值不低于可行多面体上相邻顶点的目标值。在此基础上，我们的方法将地推最优解的估计质量与其预计算的相邻顶点的质量进行比较，并使用这一结果作为损失函数。实验表明，该方法显著降低了计算成本，同时保持了高决策质量。 

---
# Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning 

**Title (ZH)**: 基于规则和模型的验证器的pitfalls——数学推理案例研究 

**Authors**: Yuzhen Huang, Weihao Zeng, Xingshan Zeng, Qi Zhu, Junxian He  

**Link**: [PDF](https://arxiv.org/pdf/2505.22203)  

**Abstract**: Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL training results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct (i.e., false positives). This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique risks inherent to both rule-based and model-based verifiers, aiming to offer valuable insights to develop more robust reward systems in reinforcement learning. 

**Abstract (ZH)**: 可信的验证器是确保可验证奖励强化学习（RLVR）成功的关键，而RLVR是包括DeepSeek-R1在内的各种大规模推理模型的核心方法论。在数学推理等复杂领域，以往的工作广泛采用了基于规则的验证器来训练强大的推理模型。然而，这些验证器的可靠性和它们对强化学习训练过程的影响仍知之甚少。在本研究中，我们以数学推理为例，对不同验证器在静态评估和强化学习训练中的表现进行了全面分析。首先，我们发现当前的开源基于规则的验证器在多个常用数学数据集中，难以识别不同格式下的等价答案，从而导致了显著的假阴性率。这种局限性会不良地影响强化学习训练性能，并且随着策略模型的增强而愈发明显。接着，我们探讨了基于模型的验证器作为解决这些问题的潜在方案。尽管在静态评估中，基于模型的验证器可以显著提高验证准确性，但进一步的分析和强化学习训练结果表明，它们很容易被黑客攻击，即错误地将某些响应模式分类为正确答案（即，假阳性）。这种漏洞在策略模型优化过程中被利用，导致奖励被人为地高估。我们的研究结果揭示了基于规则和基于模型验证器固有的独特风险，旨在为强化学习中的奖励系统开发提供宝贵见解。 

---
# Let's Predict Sentence by Sentence 

**Title (ZH)**: 让我们句句预测 

**Authors**: Hyeonbin Hwang, Byeongguk Jeon, Seungone Kim, Jiyeon Kim, Hoyeon Chang, Sohee Yang, Seungpil Won, Dohaeng Lee, Youbin Ahn, Minjoon Seo  

**Link**: [PDF](https://arxiv.org/pdf/2505.22202)  

**Abstract**: Autoregressive language models (LMs) generate one token at a time, yet human reasoning operates over higher-level abstractions - sentences, propositions, and concepts. This contrast raises a central question- Can LMs likewise learn to reason over structured semantic units rather than raw token sequences? In this work, we investigate whether pretrained LMs can be lifted into such abstract reasoning spaces by building on their learned representations. We present a framework that adapts a pretrained token-level LM to operate in sentence space by autoregressively predicting continuous embeddings of next sentences. We explore two embedding paradigms inspired by classical representation learning: 1) semantic embeddings, learned via autoencoding to preserve surface meaning; and 2) contextual embeddings, trained via next-sentence prediction to encode anticipatory structure. We evaluate both under two inference regimes: Discretized, which decodes each predicted embedding into text before re-encoding; and Continuous, which reasons entirely in embedding space for improved efficiency. Across four domains - mathematics, logic, commonsense, and planning - contextual embeddings under continuous inference show competitive performance with Chain-of-Thought (CoT) while reducing inference-time FLOPs on average by half. We also present early signs of scalability and modular adaptation. Finally, to visualize latent trajectories, we introduce SentenceLens, a diagnostic tool that decodes intermediate model states into interpretable sentences. Together, our results indicate that pretrained LMs can effectively transition to abstract, structured reasoning within latent embedding spaces. 

**Abstract (ZH)**: 自回归语言模型（LMs）逐次生成一个词，然而人类推理操作的是更高层次的抽象单元——句子、命题和概念。这种对比提出了一个核心问题——LMs是否也能学会在一个结构化语义单元而非原始词序列的空间中进行推理？在本文中，我们研究是否可以通过利用预训练模型的学习表示来将其提升到这种抽象推理空间。我们提出了一种框架，通过自回归预测下一个句子的连续嵌入来适应预训练的词级LM使其在句子空间中运行。我们探索了两种受经典表示学习启发的嵌入范式：1）语义嵌入，通过自编码学习以保持表层意义；2）上下文嵌入，通过下一个句子预测训练以编码前瞻结构。我们在两种推理框架下评估了它们的表现：离散框架下，每预测一个嵌入即解码为文本后再编码；连续框架下，完全在嵌入空间中推理以提高效率。在四个领域——数学、逻辑、常识和规划——中，连续推理下的上下文嵌入与Chain-of-Thought（CoT）在性能上具有竞争力，同时平均减少一半的推理时间FLOPs。我们还展示了可扩展性和模块化适应的早期迹象。最后，为了可视化潜在轨迹，我们引入了SentenceLens，一种诊断工具，将中间模型状态解码为可解释的句子。综合来看，我们的结果表明预训练LMs能够有效地过渡到潜在嵌入空间中的抽象、结构化推理。 

---
# Investigating Mechanisms for In-Context Vision Language Binding 

**Title (ZH)**: 探究上下文视知觉语言结合机制 

**Authors**: Darshana Saravanan, Makarand Tapaswi, Vineet Gandhi  

**Link**: [PDF](https://arxiv.org/pdf/2505.22200)  

**Abstract**: To understand a prompt, Vision-Language models (VLMs) must perceive the image, comprehend the text, and build associations within and across both modalities. For instance, given an 'image of a red toy car', the model should associate this image to phrases like 'car', 'red toy', 'red object', etc. Feng and Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the entity and its corresponding attribute tokens share a Binding ID in the model activations. We investigate this for image-text binding in VLMs using a synthetic dataset and task that requires models to associate 3D objects in an image with their descriptions in the text. Our experiments demonstrate that VLMs assign a distinct Binding ID to an object's image tokens and its textual references, enabling in-context association. 

**Abstract (ZH)**: 视觉-语言模型需通过感知图像、理解文本并在两者之间建立联系来理解提示。例如，给定一幅“红色玩具汽车”的图像，模型应将该图像与“汽车”、“红色玩具”、“红色物体”等短语建立关联。冯和斯坦哈特在LLMs中提出了绑定ID机制，建议实体及其对应属性词令牌在模型激活中共享一个绑定ID。我们通过一个合成数据集和任务研究了这一机制在VLMs中的应用，该任务要求模型将图像中的3D对象与文本中的描述关联起来。实验结果表明，VLMs为图像中对象的词令牌及其文本引用分配了独特的绑定ID，从而实现上下文相关联。 

---
# Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer 

**Title (ZH)**: 通过贝叶斯非负决策层增强不确定性估计和可解释性 

**Authors**: Xinyue Hu, Zhibin Duan, Bo Chen, Mingyuan Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2505.22199)  

**Abstract**: Although deep neural networks have demonstrated significant success due to their powerful expressiveness, most models struggle to meet practical requirements for uncertainty estimation. Concurrently, the entangled nature of deep neural networks leads to a multifaceted problem, where various localized explanation techniques reveal that multiple unrelated features influence the decisions, thereby undermining interpretability. To address these challenges, we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates deep neural networks as a conditional Bayesian non-negative factor analysis. By leveraging stochastic latent variables, the BNDL can model complex dependencies and provide robust uncertainty estimation. Moreover, the sparsity and non-negativity of the latent variables encourage the model to learn disentangled representations and decision layers, thereby improving interpretability. We also offer theoretical guarantees that BNDL can achieve effective disentangled learning. In addition, we developed a corresponding variational inference method utilizing a Weibull variational inference network to approximate the posterior distribution of the latent variables. Our experimental results demonstrate that with enhanced disentanglement capabilities, BNDL not only improves the model's accuracy but also provides reliable uncertainty estimation and improved interpretability. 

**Abstract (ZH)**: 尽管深度神经网络由于其强大的表达能力取得了显著的成功，但大多数模型在不确定性估计方面仍难以满足实际需求。同时，深度神经网络的纠缠性质导致了一个多维度的问题，各种局部解释技术揭示了多个不相关的特征影响决策，从而削弱了模型的可解释性。为应对这些挑战，我们开发了一种贝叶斯非负决策层（BNDL），将深度神经网络重新表述为条件贝叶斯非负因子分析。通过利用随机潜在变量，BNDL能够建模复杂的依赖关系并提供稳健的不确定性估计。此外，潜在变量的稀疏性和非负性促使模型学习到非纠缠的表示和决策层，从而提高可解释性。我们还提供了理论保证，证明BNDL能够实现有效的非纠缠学习。此外，我们还开发了一种相应的变分推理方法，利用威布尔变分推理网络近似潜在变量的后验分布。我们的实验结果表明，通过增强非纠缠能力，BNDL不仅提高了模型的准确率，还提供了可靠的不确定性估计和改进的可解释性。 

---
# Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion 

**Title (ZH)**: 基于真实硬件 noisy 量子扩散的物理启发式生成式 AI 模型 

**Authors**: Marco Parigi, Stefano Martina, Francesco Aldo Venturelli, Filippo Caruso  

**Link**: [PDF](https://arxiv.org/pdf/2505.22193)  

**Abstract**: Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI that aims to use quantum properties to improve the performances of their classical counterparts. However, existing algorithms are not easily scalable due to the limitations of near-term quantum devices. Following our previous work on QDMs, here we propose and implement two physics-inspired protocols. In the first, we use the formalism of quantum stochastic walks, showing that a specific interplay of quantum and classical dynamics in the forward process produces statistically more robust models generating sets of MNIST images with lower Fréchet Inception Distance (FID) than using totally classical dynamics. In the second approach, we realize an algorithm to generate images by exploiting the intrinsic noise of real IBM quantum hardware with only four qubits. Our work could be a starting point to pave the way for new scenarios for large-scale algorithms in quantum Generative AI, where quantum noise is neither mitigated nor corrected, but instead exploited as a useful resource. 

**Abstract (ZH)**: 量子扩散模型（QDMs）是生成人工智能中新兴的范式，旨在利用量子特性来提高其经典对应物的性能。然而，现有的算法由于近期量子设备的限制而不易扩展。在我们先前关于QDMs的工作基础上，这里我们提出了并实现了两种受物理启发的协议。在第一个协议中，我们使用量子随机行走的形式主义，展示了一种特定的量子和经典动力学的相互作用在前向过程中产生了统计上更稳健的模型，生成的MNIST图像集的弗雷彻-勘探者距离（FID）低于仅使用完全经典动力学的方法。在第二种方法中，我们利用实际IBM量子硬件中固有的噪声，仅使用四个量子比特来实现生成图像的算法。我们的工作可能是通往量子生成人工智能中大规模算法新场景的一个起点，在这些场景中，量子噪声既不被减轻也不被纠正，而是被用作有用的资源。 

---
# Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon 

**Title (ZH)**: 破译迷雾！揭示中文同音词伪装毒性的方法与毒词词典 

**Authors**: Xuchen Ma, Jianxiang Yu, Wenming Shao, Bo Pang, Xiang Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.22184)  

**Abstract**: Social media platforms have experienced a significant rise in toxic content, including abusive language and discriminatory remarks, presenting growing challenges for content moderation. Some users evade censorship by deliberately disguising toxic words through homophonic cloak, which necessitates the task of unveiling cloaked toxicity. Existing methods are mostly designed for English texts, while Chinese cloaked toxicity unveiling has not been solved yet. To tackle the issue, we propose C$^2$TU, a novel training-free and prompt-free method for Chinese cloaked toxic content unveiling. It first employs substring matching to identify candidate toxic words based on Chinese homo-graph and toxic lexicon. Then it filters those candidates that are non-toxic and corrects cloaks to be their corresponding toxicities. Specifically, we develop two model variants for filtering, which are based on BERT and LLMs, respectively. For LLMs, we address the auto-regressive limitation in computing word occurrence probability and utilize the full semantic contexts of a text sequence to reveal cloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve superior performance on two Chinese toxic datasets. In particular, our method outperforms the best competitor by up to 71% on the F1 score and 35% on accuracy, respectively. 

**Abstract (ZH)**: 基于汉语同音字的无训练无提示 cloaked 毒性内容揭蔽方法 C$^2$TU 

---
# Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design 

**Title (ZH)**: speculation decoding 与量化相结合：兼容性评估与分层框架设计 

**Authors**: Yudi Zhang, Weilin Zhao, Xu Han, Tiejun Zhao, Wang Xu, Hailong Cao, Conghui Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2505.22179)  

**Abstract**: Speculative decoding and quantization effectively accelerate memory-bound inference of large language models. Speculative decoding mitigates the memory bandwidth bottleneck by verifying multiple tokens within a single forward pass, which increases computational effort. Quantization achieves this optimization by compressing weights and activations into lower bit-widths and also reduces computations via low-bit matrix multiplications. To further leverage their strengths, we investigate the integration of these two techniques. Surprisingly, experiments applying the advanced speculative decoding method EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit weight quantization are diminished by the computational load from speculative decoding. Specifically, verifying a tree-style draft incurs significantly more time overhead than a single-token forward pass on 4-bit weight quantized models. This finding led to our new speculative decoding design: a hierarchical framework that employs a small model as an intermediate stage to turn tree-style drafts into sequence drafts, leveraging the memory access benefits of the target quantized model. Experimental results show that our hierarchical approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$. Code available at this https URL. 

**Abstract (ZH)**: 推测性解码和量化有效地加速了大语言模型内存限制下的推理。推测性解码通过在单次前向传递中验证多个令牌来缓解内存带宽瓶颈，从而增加计算量。量化通过将权重和激活压缩到较低位宽并利用低位矩阵乘法减少计算量来实现这一优化。为进一步发挥这些技术的优势，我们研究了将这两种技术进行整合。实验发现，将高级推测性解码方法EAGLE-2应用于各种量化模型后，4位权重量化带来的内存优势被推测性解码的计算负载所抵消。具体来说，对于4位权重量化模型，验证树状草稿所需的时间开销远大于单令牌前向传递。这一发现促使我们提出了一种新的推测性解码设计：一种分层框架，它使用一个小模型作为中间阶段，将树状草稿转换为序列草稿，利用目标量化模型的内存访问优势。实验结果显示，我们的分层方法在A100 GPU上实现了4位权重Llama-3-70B模型在各种任务中2.78倍的速度提升，比EAGLE-2提升了1.31倍。代码可在以下链接获取。 

---
# Online Fair Division for Personalized $2$-Value Instances 

**Title (ZH)**: 在线公平分配 forKeyed 二值实例 

**Authors**: Georgios Amanatidis, Alexandros Lolos, Evangelos Markakis, Victor Turmel  

**Link**: [PDF](https://arxiv.org/pdf/2505.22174)  

**Abstract**: We study an online fair division setting, where goods arrive one at a time and there is a fixed set of $n$ agents, each of whom has an additive valuation function over the goods. Once a good appears, the value each agent has for it is revealed and it must be allocated immediately and irrevocably to one of the agents. It is known that without any assumptions about the values being severely restricted or coming from a distribution, very strong impossibility results hold in this setting. To bypass the latter, we turn our attention to instances where the valuation functions are restricted. In particular, we study personalized $2$-value instances, where there are only two possible values each agent may have for each good, possibly different across agents, and we show how to obtain worst case guarantees with respect to well-known fairness notions, such as maximin share fairness and envy-freeness up to one (or two) good(s). We suggest a deterministic algorithm that maintains a $1/(2n-1)$-MMS allocation at every time step and show that this is the best possible any deterministic algorithm can achieve if one cares about every single time step; nevertheless, eventually the allocation constructed by our algorithm becomes a $1/4$-MMS allocation. To achieve this, the algorithm implicitly maintains a fragile system of priority levels for all agents. Further, we show that, by allowing some limited access to future information, it is possible to have stronger results with less involved approaches. By knowing the values of goods for $n-1$ time steps into the future, we design a matching-based algorithm that achieves an EF$1$ allocation every $n$ time steps, while always maintaining an EF$2$ allocation. Finally, we show that our results allow us to get the first nontrivial guarantees for additive instances in which the ratio of the maximum over the minimum value an agent has for a good is bounded. 

**Abstract (ZH)**: 我们研究一种在线公平分配设置，其中物品陆续出现，固定有$n$个代理，每个代理对物品的价值函数为加性函数。一旦出现一个物品，每个代理对该物品的价值立即公开展示并不可撤销地分配给其中一个代理。在没有任何关于价值的极端限制或来自分布的假设下，已知在这种设置中存在非常强硬的不可能结果。为避免这种限制，我们将注意力转向价值函数受限的实例。特别地，我们研究个人化二值实例，其中每个代理对每个物品可能仅有两种价值，且这些价值可能在代理之间不同，并展示了如何针对诸如最大化最小份额公平性和至多一个（或两个）物品的嫉妒性与分配获得最坏情况保证。我们建议一个确定性算法，确保每一步都能维持$1/(2n-1)$-MMS分配，并证明如果关注每一时刻，这是任何确定性算法所能达到的最佳保证；然而，最终由我们算法构造的分配将变成$1/4$-MMS分配。为了实现这一点，算法隐式地维护了一种所有代理的优先级层级系统。此外，我们展示了，通过在一定程度上访问未来的部分信息，可以达到更好的结果，同时简化方法。在了解未来$n-1$个时间步的物品价值后，我们设计了一种基于匹配的算法，使其在每个$n$时间步中都能实现EF$1$分配，并始终保持着EF$2$分配。最后，我们证明了我们的结果允许我们在价值比具有上限的加性实例中首次获得非平凡的保证。 

---
# Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes 

**Title (ZH)**: 统一连续和离散文本扩散的非同步扩散过程 

**Authors**: Bocheng Li, Zhujin Gao, Linli Xu  

**Link**: [PDF](https://arxiv.org/pdf/2505.22165)  

**Abstract**: Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to continuous spaces and apply fine-grained noise, but the diffusion progress is uniform across tokens, limiting their ability to capture semantic nuances. To address these limitations, we propose \textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models (NeoDiff), a novel diffusion model that integrates the strengths of both discrete and continuous approaches. NeoDiff introduces a Poisson diffusion process for the forward process, enabling a flexible and fine-grained noising paradigm, and employs a time predictor for the reverse process to adaptively modulate the denoising progress based on token semantics. Furthermore, NeoDiff utilizes an optimized schedule for inference to ensure more precise noise control and improved performance. Our approach unifies the theories of discrete and continuous diffusion models, offering a more principled and effective framework for text generation. Experimental results on several text generation tasks demonstrate NeoDiff's superior performance compared to baselines of non-autoregressive continuous and discrete diffusion models, iterative-based methods and autoregressive diffusion-based methods. These results highlight NeoDiff's potential as a powerful tool for generating high-quality text and advancing the field of diffusion-based text generation. 

**Abstract (ZH)**: 非同步连续扩散模型（NeoDiff）：结合离散与连续扩散模型的优势 

---
# Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language 

**Title (ZH)**: 视觉与语言低维度属性对齐驱动的灵活工具选择 

**Authors**: Guangfu Hao, Haojie Wen, Liangxuna Guo, Yang Chen, Yanchao Bi, Shan Yu  

**Link**: [PDF](https://arxiv.org/pdf/2505.22146)  

**Abstract**: Flexible tool selection reflects a complex cognitive ability that distinguishes humans from other species, yet computational models that capture this ability remain underdeveloped. We developed a framework using low-dimensional attribute representations to bridge visual tool perception and linguistic task understanding. We constructed a comprehensive dataset (ToolNet) containing 115 common tools labeled with 13 carefully designed attributes spanning physical, functional, and psychological properties, paired with natural language scenarios describing tool usage. Visual encoders (ResNet or ViT) extract attributes from tool images while fine-tuned language models (GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our approach achieves 74% accuracy in tool selection tasks-significantly outperforming direct tool matching (20%) and smaller multimodal models (21%-58%), while approaching performance of much larger models like GPT-4o (73%) with substantially fewer parameters. Ablation studies revealed that manipulation-related attributes (graspability, hand-relatedness, elongation) consistently prove most critical across modalities. This work provides a parameter-efficient, interpretable solution that mimics human-like tool cognition, advancing both cognitive science understanding and practical applications in tool selection tasks. 

**Abstract (ZH)**: 灵活工具选择反映了一种复杂的认知能力，这种能力使人类与其他物种区分开来，但能够捕获这一能力的计算模型仍不够发达。我们提出了一种使用低维属性表示的框架，以连接视觉工具感知和语言任务理解。我们构建了一个包含115种常用工具的数据集（ToolNet），这些工具被标记了13个精心设计的属性，涵盖了物理、功能和心理属性，并配以自然语言场景描述工具的使用。视觉编码器（ResNet或ViT）从工具图像中提取属性，微调的语言模型（GPT-2、LLaMA、DeepSeek）从任务描述中推导出所需属性。我们的方法在工具选择任务中的准确率达到74%，显著优于直接工具匹配（20%）和较小的多模态模型（21%-58%），同时参数量较少的情况下接近如GPT-4o（73%）等更大模型的性能。消融研究显示，与操作相关的属性（握持性、与手的相关性、长度）在各个模态中始终证明是最关键的。本工作提供了一种参数高效、可解释的解决方案，能够模拟人类似的工具认知，既推进了认知科学的理解，又在工具选择任务的实际应用中取得了进展。 

---
# FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing 

**Title (ZH)**: FaceEditTalker: 基于面部属性编辑的交互式头部讲话生成 

**Authors**: Guanwen Feng, Zhiyuan Ma, Yunan Li, Junwei Jing, Jiahao Yang, Qiguang Miao  

**Link**: [PDF](https://arxiv.org/pdf/2505.22141)  

**Abstract**: Recent advances in audio-driven talking head generation have achieved impressive results in lip synchronization and emotional expression. However, they largely overlook the crucial task of facial attribute editing. This capability is crucial for achieving deep personalization and expanding the range of practical applications, including user-tailored digital avatars, engaging online education content, and brand-specific digital customer service. In these key domains, the flexible adjustment of visual attributes-such as hairstyle, accessories, and subtle facial features is essential for aligning with user preferences, reflecting diverse brand identities, and adapting to varying contextual demands. In this paper, we present FaceEditTalker, a unified framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos. Our method consists of two key components: an image feature space editing module, which extracts semantic and detail features and allows flexible control over attributes like expression, hairstyle, and accessories; and an audio-driven video generation module, which fuses these edited features with audio-guided facial landmarks to drive a diffusion-based generator. This design ensures temporal coherence, visual fidelity, and identity preservation across frames. Extensive experiments on public datasets demonstrate that our method outperforms state-of-the-art approaches in lip-sync accuracy, video quality, and attribute controllability. Project page: this https URL 

**Abstract (ZH)**: Recent advances in audio-driven talking head generation have achieved impressive results in lip synchronization and emotional expression, but largely overlook the crucial task of facial attribute editing. This capability is crucial for achieving deep personalization and expanding the range of practical applications, including user-tailored digital avatars, engaging online education content, and brand-specific digital customer service. In these key domains, the flexible adjustment of visual attributes such as hairstyle, accessories, and subtle facial features is essential for aligning with user preferences, reflecting diverse brand identities, and adapting to varying contextual demands. In this paper, we present FaceEditTalker, a unified framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos. Our method consists of two key components: an image feature space editing module, which extracts semantic and detail features and allows flexible control over attributes like expression, hairstyle, and accessories; and an audio-driven video generation module, which fuses these edited features with audio-guided facial landmarks to drive a diffusion-based generator. This design ensures temporal coherence, visual fidelity, and identity preservation across frames. Extensive experiments on public datasets demonstrate that our method outperforms state-of-the-art approaches in lip-sync accuracy, video quality, and attribute controllability. 

---
# Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments 

**Title (ZH)**: Argument Mining中有限的泛化能力：现有模型学习的是数据集，而非论证。 

**Authors**: Marc Feger, Katarina Boland, Stefan Dietze  

**Link**: [PDF](https://arxiv.org/pdf/2505.22137)  

**Abstract**: Identifying arguments is a necessary prerequisite for various tasks in automated discourse analysis, particularly within contexts such as political debates, online discussions, and scientific reasoning. In addition to theoretical advances in understanding the constitution of arguments, a significant body of research has emerged around practical argument mining, supported by a growing number of publicly available datasets. On these benchmarks, BERT-like transformers have consistently performed best, reinforcing the belief that such models are broadly applicable across diverse contexts of debate. This study offers the first large-scale re-evaluation of such state-of-the-art models, with a specific focus on their ability to generalize in identifying arguments. We evaluate four transformers, three standard and one enhanced with contrastive pre-training for better generalization, on 17 English sentence-level datasets as most relevant to the task. Our findings show that, to varying degrees, these models tend to rely on lexical shortcuts tied to content words, suggesting that apparent progress may often be driven by dataset-specific cues rather than true task alignment. While the models achieve strong results on familiar benchmarks, their performance drops markedly when applied to unseen datasets. Nonetheless, incorporating both task-specific pre-training and joint benchmark training proves effective in enhancing both robustness and generalization. 

**Abstract (ZH)**: 识别论据是自动化话语分析中各种任务的前提，特别是在政治辩论、在线讨论和科学推理等背景下。除了对论据构成的理论进展之外，还涌现出大量的实践论据挖掘研究，并得到了越来越多公开数据集的支持。在这些基准测试中，BERT-like变换器持续表现出色，增强了这些模型在各种辩论情境下的广泛适用性的信念。本研究提供了首个大规模重新评估此类最先进的模型的研究，重点关注其在识别论据方面的泛化能力。我们在这项研究中评估了四种变换器模型，包括三种标准模型和一种通过对比预训练增强的模型，评估对象是17个与任务最相关的英语句子级数据集。研究发现，这些模型在不同程度上依赖于与内容词相关的词汇捷径，表明看似的进步往往可能由数据集特定的线索驱动，而非真正的任务对齐。尽管这些模型在熟悉的基准测试中取得了优异的结果，但在应用于未见过的数据集时，其性能显著下降。然而，结合任务特定的预训练和联合基准训练对于提高鲁棒性和泛化能力是有效的。 

---
# Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach 

**Title (ZH)**: 地球观测中的实时盲焦blur去模糊：IMAGIN-e 任务方法 

**Authors**: Alejandro D. Mousist  

**Link**: [PDF](https://arxiv.org/pdf/2505.22128)  

**Abstract**: This work addresses mechanical defocus in Earth observation images from the IMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted to space-based edge computing constraints. Leveraging Sentinel-2 data, our method estimates the defocus kernel and trains a restoration model within a GAN framework, effectively operating without reference images.
On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and PSNR by 25.00%, confirming the model's ability to recover lost details when the original clean image is known. On IMAGIN-e, where no reference images exist, perceptual quality metrics indicate a substantial enhancement, with NIQE improving by 60.66% and BRISQUE by 48.38%, validating real-world onboard restoration. The approach is currently deployed aboard the IMAGIN-e mission, demonstrating its practical application in an operational space environment.
By efficiently handling high-resolution images under edge computing constraints, the method enables applications such as water body segmentation and contour detection while maintaining processing viability despite resource limitations. 

**Abstract (ZH)**: 本研究针对IMAGIN-e任务在ISS上获取的地球观测图像中的机械失焦问题，提出了一种适应基于空间的边缘计算约束的盲去模糊方法。利用Sentinel-2数据，我们的方法估计失焦核并在生成对抗网络（GAN）框架中训练复原模型，无需参考图像即可有效运行。在带有合成退化效果的Sentinel-2图像上，SSIM提高了72.47%，PSNR提高了25.00%，证实了该模型在已知原始清晰图像时恢复丢失细节的能力。在没有参考图像的IMAGIN-e任务中，感知质量指标显示有显著提升，NIQE提高了60.66%，BRISQUE提高了48.38%，验证了在实际空间环境中的在轨复原效果。该方法已在IMAGIN-e任务中部署，展示了其在资源受限的运行空间环境中的实际应用能力。通过在边缘计算约束下高效处理高分辨率图像，该方法在资源受限的情况下仍能保持分割水体和检测轮廓的应用性能。 

---
# SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model 

**Title (ZH)**: SridBench: 图像生成模型中科学研究插图绘制的基准测试 

**Authors**: Yifan Chang, Yukang Feng, Jianwen Sun, Jiaxin Ai, Chuanhao Li, S. Kevin Zhou, Kaipeng Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22126)  

**Abstract**: Recent years have seen rapid advances in AI-driven image generation. Early diffusion models emphasized perceptual quality, while newer multimodal models like GPT-4o-image integrate high-level reasoning, improving semantic understanding and structural composition. Scientific illustration generation exemplifies this evolution: unlike general image synthesis, it demands accurate interpretation of technical content and transformation of abstract ideas into clear, standardized visuals. This task is significantly more knowledge-intensive and laborious, often requiring hours of manual work and specialized tools. Automating it in a controllable, intelligent manner would provide substantial practical value. Yet, no benchmark currently exists to evaluate AI on this front. To fill this gap, we introduce SridBench, the first benchmark for scientific figure generation. It comprises 1,120 instances curated from leading scientific papers across 13 natural and computer science disciplines, collected via human experts and MLLMs. Each sample is evaluated along six dimensions, including semantic fidelity and structural accuracy. Experimental results reveal that even top-tier models like GPT-4o-image lag behind human performance, with common issues in text/visual clarity and scientific correctness. These findings highlight the need for more advanced reasoning-driven visual generation capabilities. 

**Abstract (ZH)**: Recent Years 前几年  
Rapid Advances in AI-Driven Image Generation AI驱动图像生成的迅速进展  
Early Diffusion Models Early 扩散模型  
Emphasized Perceptual Quality 强调感知质量  
Newer Multimodal Models 如GPT-4o-image等 新的多模态模型  
Integrate High-Level Reasoning 结合高层次推理  
Improving Semantic Understanding and Structural Composition 提高语义理解和结构组成  
Scientific Illustration Generation 科学插图生成  
Exemplifies This Evolution 是这一演变的典范  
Unlike General Image Synthesis 不同于一般的图像合成  
Demands Accurate Interpretation of Technical Content 和抽象思想的清晰转化  
Transformation of Abstract Ideas into Clear, Standardized Visuals  
显著知识密集型和劳动密集型  
Often Requiring Hours of Manual Work and Specialized Tools 通常需要数小时的手动工作和专业工具  
Automating It in a Controllable, Intelligent Manner 具有控制性和智能性的自动化  
Would Provide Substantial Practical Value 将提供重大实际价值  
Yet, No Benchmark Currently Exists 但仍无基准性能评估  
To Fill This Gap 填补这一空白  
We Introduce SridBench 引入 SridBench  
The First Benchmark for Scientific Figure Generation 首个科学图表生成基准  
Comprises 1,120 Instances Curated from Leading Scientific Papers 包含1120个从领先科学论文中精选出的实例  
Across 13 Natural and Computer Science Disciplines 涵盖13个自然科学和计算机科学领域  
Collected Via Human Experts and MLLMs 通过人工专家和MLLMs收集  
Each Sample Evaluated Along Six Dimensions 每个样本在六个维度上进行评估  
Including Semantic Fidelity and Structural Accuracy 包括语义保真度和结构准确性  
Experimental Results Reveal that Even Top-Tier Models Lag Behind Human Performance 实验结果显示即使是顶级模型也落后于人工表现  
With Common Issues in Text/Visual Clarity and Scientific Correctness 存在常见的文字/视觉清晰度和科学正确性问题  
These Findings Highlight the Need for More Advanced Reasoning-Driven Visual Generation Capabilities 这些结果突显了需要更先进的推理驱动的视觉生成能力 

---
# Sentiment Simulation using Generative AI Agents 

**Title (ZH)**: 使用生成性AI代理模拟情感 

**Authors**: Melrose Tia, Jezreel Sophia Lanuzo, Lei Rigi Baltazar, Marie Joy Lopez-Relente, Diwa Malaya Quiñones, Jason Albia  

**Link**: [PDF](https://arxiv.org/pdf/2505.22125)  

**Abstract**: Traditional sentiment analysis relies on surface-level linguistic patterns and retrospective data, limiting its ability to capture the psychological and contextual drivers of human sentiment. These limitations constrain its effectiveness in applications that require predictive insight, such as policy testing, narrative framing, and behavioral forecasting. We present a robust framework for sentiment simulation using generative AI agents embedded with psychologically rich profiles. Agents are instantiated from a nationally representative survey of 2,485 Filipino respondents, combining sociodemographic information with validated constructs of personality traits, values, beliefs, and socio-political attitudes. The framework includes three stages: (1) agent embodiment via categorical or contextualized encodings, (2) exposure to real-world political and economic scenarios, and (3) generation of sentiment ratings accompanied by explanatory rationales. Using Quadratic Weighted Accuracy (QWA), we evaluated alignment between agent-generated and human responses. Contextualized encoding achieved 92% alignment in replicating original survey responses. In sentiment simulation tasks, agents reached 81%--86% accuracy against ground truth sentiment, with contextualized profile encodings significantly outperforming categorical (p < 0.0001, Cohen's d = 0.70). Simulation results remained consistent across repeated trials (+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676, Cohen's d = 0.02). Our findings establish a scalable framework for sentiment modeling through psychographically grounded AI agents. This work signals a paradigm shift in sentiment analysis from retrospective classification to prospective and dynamic simulation grounded in psychology of sentiment formation. 

**Abstract (ZH)**: 一种基于心理丰富代理的稳健情感模拟框架 

---
# Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model 

**Title (ZH)**: 基于语言模型的稀疏术中低血压事件多模态预测 

**Authors**: Jintao Zhang, Zirui Liu, Mingyue Cheng, Shilong Zhang, Tingyue Pan, Qi Liu, Yanhu Xie  

**Link**: [PDF](https://arxiv.org/pdf/2505.22116)  

**Abstract**: Intraoperative hypotension (IOH) frequently occurs under general anesthesia and is strongly linked to adverse outcomes such as myocardial injury and increased mortality. Despite its significance, IOH prediction is hindered by event sparsity and the challenge of integrating static and dynamic data across diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal language model framework. To accurately identify and differentiate sparse hypotensive events, we leverage a two-stage training strategy. The first stage involves domain adaptive pretraining on IOH physiological time series augmented through diffusion methods, thereby enhancing the model sensitivity to patterns associated with hypotension. Subsequently, task fine-tuning is performed on the original clinical dataset to further enhance the ability to distinguish normotensive from hypotensive states. To enable multimodal fusion for each patient, we align structured clinical descriptions with the corresponding physiological time series at the token level. Such alignment enables the model to capture individualized temporal patterns alongside their corresponding clinical semantics. In addition, we convert static patient attributes into structured text to enrich personalized information. Experimental evaluations on two intraoperative datasets demonstrate that IOHFuseLM outperforms established baselines in accurately identifying IOH events, highlighting its applicability in clinical decision support scenarios. Our code is publicly available to promote reproducibility at this https URL. 

**Abstract (ZH)**: intraoperative 低血压（IOH）在全身麻醉下频繁发生，并与心肌损伤和死亡率增加等不良后果密切相关。尽管其重要性不言而喻，但IOH的预测仍受到事件稀疏性和跨多种患者整合静态和动态数据的挑战。本文提出了一种多模态语言模型框架\textbf{IOHFuseLM}。为了准确识别和区分稀疏的低血压事件，我们采用两阶段训练策略。第一阶段通过扩展现有生理时间序列的扩散方法进行领域自适应预训练，从而增强模型对与低血压相关的模式的敏感性。随后，在原始临床数据集上进行任务微调，以进一步增强区分正常血压和低血压状态的能力。为了在每个患者中实现多模态融合，我们在标记级别将结构化的临床描述与相应的生理时间序列对齐。这种对齐使模型能够捕捉到各自的个体化时间模式及其相应的临床语义。此外，我们将静态患者属性转换成结构化文本，以丰富个性化的信息。在两个术中数据集上的实验评估表明，IOHFuseLM在准确识别IOH事件方面优于现有的基线方法，突显了其在临床决策支持场景中的适用性。我们的代码已在以下网址公开以促进可重复性：[https://]。 

---
# The quest for the GRAph Level autoEncoder (GRALE) 

**Title (ZH)**: GRA图级自动编码器的探索 

**Authors**: Paul Krzakala, Gabriel Melo, Charlotte Laclau, Florence d'Alché-Buc, Rémi Flamary  

**Link**: [PDF](https://arxiv.org/pdf/2505.22109)  

**Abstract**: Although graph-based learning has attracted a lot of attention, graph representation learning is still a challenging task whose resolution may impact key application fields such as chemistry or biology. To this end, we introduce GRALE, a novel graph autoencoder that encodes and decodes graphs of varying sizes into a shared embedding space. GRALE is trained using an Optimal Transport-inspired loss that compares the original and reconstructed graphs and leverages a differentiable node matching module, which is trained jointly with the encoder and decoder. The proposed attention-based architecture relies on Evoformer, the core component of AlphaFold, which we extend to support both graph encoding and decoding. We show, in numerical experiments on simulated and molecular data, that GRALE enables a highly general form of pre-training, applicable to a wide range of downstream tasks, from classification and regression to more complex tasks such as graph interpolation, editing, matching, and prediction. 

**Abstract (ZH)**: 尽管基于图的学习引起了广泛关注，但图表示学习仍是一项具有挑战性的任务，其解决方案可能会影响化学或生物学等关键应用领域。为此，我们介绍了GRALE，一种新型的图自动编码器，能够将不同大小的图编码和解码到共享嵌入空间中。GRALE使用一种基于最优传输的损失函数进行训练，该损失函数比较原始图和重建图，并利用一个与编码器和解码器联合训练的可微节点匹配模块。所提出的基于注意力的架构依赖于AlphaFold的核心组件Evoformer，我们将其扩展以支持图编码和解码。我们在模拟和分子数据上的数值实验中展示了，GRALE能够实现一种高度通用的预训练形式，适用于广泛的下游任务，从分类和回归到更复杂的任务，如图的插值、编辑、匹配和预测。 

---
# Inclusive, Differentially Private Federated Learning for Clinical Data 

**Title (ZH)**: 包容性差异隐私联邦学习在临床数据中的应用 

**Authors**: Santhosh Parampottupadam, Melih Coşğun, Sarthak Pati, Maximilian Zenk, Saikat Roy, Dimitrios Bounias, Benjamin Hamm, Sinem Sav, Ralf Floca, Klaus Maier-Hein  

**Link**: [PDF](https://arxiv.org/pdf/2505.22108)  

**Abstract**: Federated Learning (FL) offers a promising approach for training clinical AI models without centralizing sensitive patient data. However, its real-world adoption is hindered by challenges related to privacy, resource constraints, and compliance. Existing Differential Privacy (DP) approaches often apply uniform noise, which disproportionately degrades model performance, even among well-compliant institutions. In this work, we propose a novel compliance-aware FL framework that enhances DP by adaptively adjusting noise based on quantifiable client compliance scores. Additionally, we introduce a compliance scoring tool based on key healthcare and security standards to promote secure, inclusive, and equitable participation across diverse clinical settings. Extensive experiments on public datasets demonstrate that integrating under-resourced, less compliant clinics with highly regulated institutions yields accuracy improvements of up to 15% over traditional FL. This work advances FL by balancing privacy, compliance, and performance, making it a viable solution for real-world clinical workflows in global healthcare. 

**Abstract (ZH)**: 联邦学习（FL）提供了一种在不集中敏感患者数据的情况下训练临床人工智能模型的有希望的方法。然而，其在现实世界中的采用受到隐私、资源限制和合规性相关挑战的阻碍。现有的差分隐私（DP）方法通常应用均匀噪声，这在合规机构中也会过度降低模型性能。在本工作中，我们提出了一种新的基于合规性的联邦学习框架，通过根据可量化的客户端合规得分自适应调整噪声来增强DP。此外，我们引入了一种基于关键医疗保健和安全标准的合规评分工具，以促进在不同临床环境中的安全、包容和公平参与。通过对公共数据集的广泛实验表明，将资源不足、合规性较差的诊所与高度监管的机构结合使用，相较于传统的联邦学习可提高高达15%的准确性。本工作通过平衡隐私、合规性和性能，推动了联邦学习的发展，使其成为全球医疗保健中临床工作流程的实际可行解决方案。 

---
# AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion 

**Title (ZH)**: AudioTurbo: 快速文本到音频生成的修正扩散方法 

**Authors**: Junqi Zhao, Jinzheng Zhao, Haohe Liu, Yun Chen, Lu Han, Xubo Liu, Mark Plumbley, Wenwu Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22106)  

**Abstract**: Diffusion models have significantly improved the quality and diversity of audio generation but are hindered by slow inference speed. Rectified flow enhances inference speed by learning straight-line ordinary differential equation (ODE) paths. However, this approach requires training a flow-matching model from scratch and tends to perform suboptimally, or even poorly, at low step counts. To address the limitations of rectified flow while leveraging the advantages of advanced pre-trained diffusion models, this study integrates pre-trained models with the rectified diffusion method to improve the efficiency of text-to-audio (TTA) generation. Specifically, we propose AudioTurbo, which learns first-order ODE paths from deterministic noise sample pairs generated by a pre-trained TTA model. Experiments on the AudioCaps dataset demonstrate that our model, with only 10 sampling steps, outperforms prior models and reduces inference to 3 steps compared to a flow-matching-based acceleration model. 

**Abstract (ZH)**: Diffusediffusion模型显著提高了音频生成的质量和多样性，但 inference 速度较慢。修正流通过学习直线常微分方程（ODE）路径来提升 inference 速度，但这种方法需要从头训练一个流匹配模型，并且在低步数下往往表现不佳甚至较差。为了克服修正流的局限性，同时利用预先训练的高级扩散模型的优势，本研究将预先训练的模型与修正扩散方法集成，以提高文本到音频（TTA）生成的效率。具体而言，我们提出了AudioTurbo，该方法从预先训练的TTA模型生成的确定性噪声样本对中学习一阶ODE路径。在AudioCaps数据集上的实验表明，与基于流匹配的加速模型相比，我们的模型仅需10次采样步即可超越先前的模型，并将 inference 减少到3步。 

---
# Knowledge Base Construction for Knowledge-Augmented Text-to-SQL 

**Title (ZH)**: 知识增强的文本到SQL知识库构建 

**Authors**: Jinheon Baek, Horst Samulowitz, Oktie Hassanzadeh, Dharmashankar Subramanian, Sola Shirai, Alfio Gliozzo, Debarun Bhattacharjya  

**Link**: [PDF](https://arxiv.org/pdf/2505.22096)  

**Abstract**: Text-to-SQL aims to translate natural language queries into SQL statements, which is practical as it enables anyone to easily retrieve the desired information from databases. Recently, many existing approaches tackle this problem with Large Language Models (LLMs), leveraging their strong capability in understanding user queries and generating corresponding SQL code. Yet, the parametric knowledge in LLMs might be limited to covering all the diverse and domain-specific queries that require grounding in various database schemas, which makes generated SQLs less accurate oftentimes. To tackle this, we propose constructing the knowledge base for text-to-SQL, a foundational source of knowledge, from which we retrieve and generate the necessary knowledge for given queries. In particular, unlike existing approaches that either manually annotate knowledge or generate only a few pieces of knowledge for each query, our knowledge base is comprehensive, which is constructed based on a combination of all the available questions and their associated database schemas along with their relevant knowledge, and can be reused for unseen databases from different datasets and domains. We validate our approach on multiple text-to-SQL datasets, considering both the overlapping and non-overlapping database scenarios, where it outperforms relevant baselines substantially. 

**Abstract (ZH)**: Text-to-SQL旨在将自然语言查询转换为SQL语句，这在实践中非常实用，因为它使任何人都能轻松从数据库中检索所需信息。近年来，许多现有方法使用大规模语言模型（LLMs）来解决这一问题，利用其强大的用户查询理解和生成相应SQL代码的能力。然而，LLMs中的参数化知识可能不足以涵盖所有多样化的和特定领域的查询，这些查询需要基于各种数据库模式进行 grounding，这往往导致生成的SQL语句不够准确。为了解决这个问题，我们提出构建Text-to-SQL的知识库，这是一个基础的知识来源，从中我们可以检索和生成给定查询所需的必要知识。特别是在构建知识库时，我们不同于现有的方法（这些方法要么手动标注知识，要么为每个查询只生成少量知识），我们的知识库是全面的，它是基于所有可用问题及其相关数据库模式以及相关知识的组合构建的，并且可以在不同数据集和领域中的未见过的数据库中复用。我们在多个Text-to-SQL数据集上对我们的方法进行了验证，考虑了重叠和非重叠的数据库场景，结果显示我们方法明显优于相关基线。 

---
# From Coders to Critics: Empowering Students through Peer Assessment in the Age of AI Copilots 

**Title (ZH)**: 从编码者到评论者：在人工智能 copilot 时代的同侪评估赋能学生 

**Authors**: Santiago Berrezueta-Guzman, Stephan Krusche, Stefan Wagner  

**Link**: [PDF](https://arxiv.org/pdf/2505.22093)  

**Abstract**: The rapid adoption of AI powered coding assistants like ChatGPT and other coding copilots is transforming programming education, raising questions about assessment practices, academic integrity, and skill development. As educators seek alternatives to traditional grading methods susceptible to AI enabled plagiarism, structured peer assessment could be a promising strategy. This paper presents an empirical study of a rubric based, anonymized peer review process implemented in a large introductory programming course.
Students evaluated each other's final projects (2D game), and their assessments were compared to instructor grades using correlation, mean absolute error, and root mean square error (RMSE). Additionally, reflective surveys from 47 teams captured student perceptions of fairness, grading behavior, and preferences regarding grade aggregation. Results show that peer review can approximate instructor evaluation with moderate accuracy and foster student engagement, evaluative thinking, and interest in providing good feedback to their peers. We discuss these findings for designing scalable, trustworthy peer assessment systems to face the age of AI assisted coding. 

**Abstract (ZH)**: AI赋能的编码助手快速应用对编程教育的影响：基于量表的匿名同伴评估实证研究 

---
# iDSE: Navigating Design Space Exploration in High-Level Synthesis Using LLMs 

**Title (ZH)**: iDSE: 使用大规模语言模型导航高阶综合的设计空间探索 

**Authors**: Runkai Li, Jia Xiong, Xi Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.22086)  

**Abstract**: High-Level Synthesis (HLS) serves as an agile hardware development tool that streamlines the circuit design by abstracting the register transfer level into behavioral descriptions, while allowing designers to customize the generated microarchitectures through optimization directives. However, the combinatorial explosion of possible directive configurations yields an intractable design space. Traditional design space exploration (DSE) methods, despite adopting heuristics or constructing predictive models to accelerate Pareto-optimal design acquisition, still suffer from prohibitive exploration costs and suboptimal results. Addressing these concerns, we introduce iDSE, the first LLM-aided DSE framework that leverages HLS design quality perception to effectively navigate the design space. iDSE intelligently pruns the design space to guide LLMs in calibrating representative initial sampling designs, expediting convergence toward the Pareto front. By exploiting the convergent and divergent thinking patterns inherent in LLMs for hardware optimization, iDSE achieves multi-path refinement of the design quality and diversity. Extensive experiments demonstrate that iDSE outperforms heuristic-based DSE methods by 5.1$\times$$\sim$16.6$\times$ in proximity to the reference Pareto front, matching NSGA-II with only 4.6% of the explored designs. Our work demonstrates the transformative potential of LLMs in scalable and efficient HLS design optimization, offering new insights into multiobjective optimization challenges. 

**Abstract (ZH)**: 基于LLM的HLS设计空间探索框架iDSE 

---
# The Resurrection of the ReLU 

**Title (ZH)**: ReLU的复兴 

**Authors**: Coşku Can Horuz, Geoffrey Kasenbacher, Saya Higuchi, Sebastian Kairat, Jendrik Stoltz, Moritz Pesl, Bernhard A. Moser, Christoph Linse, Thomas Martinetz, Sebastian Otte  

**Link**: [PDF](https://arxiv.org/pdf/2505.22074)  

**Abstract**: Modeling sophisticated activation functions within deep learning architectures has evolved into a distinct research direction. Functions such as GELU, SELU, and SiLU offer smooth gradients and improved convergence properties, making them popular choices in state-of-the-art models. Despite this trend, the classical ReLU remains appealing due to its simplicity, inherent sparsity, and other advantageous topological characteristics. However, ReLU units are prone to becoming irreversibly inactive - a phenomenon known as the dying ReLU problem - which limits their overall effectiveness. In this work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel, plug-and-play regularizer for deep architectures. SUGAR preserves the standard ReLU function during the forward pass but replaces its derivative in the backward pass with a smooth surrogate that avoids zeroing out gradients. We demonstrate that SUGAR, when paired with a well-chosen surrogate function, substantially enhances generalization performance over convolutional network architectures such as VGG-16 and ResNet-18, providing sparser activations while effectively resurrecting dead ReLUs. Moreover, we show that even in modern architectures like Conv2NeXt and Swin Transformer - which typically employ GELU - substituting these with SUGAR yields competitive and even slightly superior performance. These findings challenge the prevailing notion that advanced activation functions are necessary for optimal performance. Instead, they suggest that the conventional ReLU, particularly with appropriate gradient handling, can serve as a strong, versatile revived classic across a broad range of deep learning vision models. 

**Abstract (ZH)**: surrogate gradient learning for ReLU (SUGAR): A Novel Regularizer for Deep Architectures 

---
# Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO 

**Title (ZH)**: 超越路径选择：基于MimicSFT和相关性与规则诱导(R\(^2\))GRPO的更优科学信息抽取LLMs 

**Authors**: Ran Li, Shimin Di, Yuchen Liu, Chen Jing, Yu Qiu, Lei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2505.22068)  

**Abstract**: Previous study suggest that powerful Large Language Models (LLMs) trained with Reinforcement Learning with Verifiable Rewards (RLVR) only refines reasoning path without improving the reasoning capacity in math tasks while supervised-finetuning(SFT) with distillation can. We study this from the view of Scientific information extraction (SciIE) where LLMs and reasoning LLMs underperforms small Bert-based models. SciIE require both the reasoning and memorization. We argue that both SFT and RLVR can refine the reasoning path and improve reasoning capacity in a simple way based on SciIE. We propose two-stage training with 1. MimicSFT, using structured reasoning templates without needing high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and rule-induced rewards. Experiments on scientific IE benchmarks show that both methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses baseline LLMs and specialized supervised models in relation extraction. Our code is available at this https URL. 

**Abstract (ZH)**: 前期研究建议，使用可验证奖励强化学习（RLVR）训练的强大语言模型仅细化推理路径而不提高数学任务的推理能力，而通过蒸馏进行监督微调（SFT）可以。我们从科学信息提取（SciIE）的角度研究这一问题，其中LLM和推理LLM的表现不及小型BERT基模型。SciIE要求同时具备推理和记忆能力。我们认为，基于SciIE，SFT和RLVR都可以通过简单的方式细化推理路径并提高推理能力。我们提出了一种两阶段训练方法，包括1. 使用结构化推理模板的MimicSFT，无需高质量的链式思考数据，2. 基于相关性和规则诱导奖励的R²GRPO。在科学IE基准测试上的实验表明，两种方法都可以提高推理能力。使用MimicSFT的R²GRPO超越了基础LLM和专门的监督模型的关系抽取。我们的代码可在以下链接获取。 

---
# From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving 

**Title (ZH)**: 从失败到修复：基于LLM的自主驾驶场景修复技术实现自我进化 

**Authors**: Xinyu Xia, Xingjun Ma, Yunfeng Hu, Ting Qu, Hong Chen, Xun Gong  

**Link**: [PDF](https://arxiv.org/pdf/2505.22067)  

**Abstract**: Ensuring robust and generalizable autonomous driving requires not only broad scenario coverage but also efficient repair of failure cases, particularly those related to challenging and safety-critical scenarios. However, existing scenario generation and selection methods often lack adaptivity and semantic relevance, limiting their impact on performance improvement. In this paper, we propose \textbf{SERA}, an LLM-powered framework that enables autonomous driving systems to self-evolve by repairing failure cases through targeted scenario recommendation. By analyzing performance logs, SERA identifies failure patterns and dynamically retrieves semantically aligned scenarios from a structured bank. An LLM-based reflection mechanism further refines these recommendations to maximize relevance and diversity. The selected scenarios are used for few-shot fine-tuning, enabling targeted adaptation with minimal data. Experiments on the benchmark show that SERA consistently improves key metrics across multiple autonomous driving baselines, demonstrating its effectiveness and generalizability under safety-critical conditions. 

**Abstract (ZH)**: 确保自主驾驶系统在各种场景下具备鲁棒性和普适性不仅需要广泛的场景覆盖，还需要有效修复故障案例，特别是那些与挑战性和安全关键场景相关的案例。然而，现有的场景生成和选择方法往往缺乏适应性和语义相关性，限制了其对性能提升的影响。本文提出了一种基于大语言模型的框架SERA，该框架通过有针对性的场景推荐来修复故障案例，从而使自主驾驶系统能够自我进化。通过分析性能日志，SERA识别故障模式，并动态检索语义对齐的情景。基于大语言模型的反思机制进一步细化这些推荐，以最大化相关性和多样性。选择的情景用于少量示例微调，实现最小数据量的针对性适应。基准实验表明，SERA在多个自主驾驶基准上一致地提高了关键指标，证明了其在安全关键条件下的有效性和普适性。 

---
# Estimating the Effects of Sample Training Orders for Large Language Models without Retraining 

**Title (ZH)**: 大型语言模型的样本训练顺序效果估计无需重新训练 

**Authors**: Hao Yang, Haoxuan Li, Mengyue Yang, Xu Chen, Mingming Gong  

**Link**: [PDF](https://arxiv.org/pdf/2505.22042)  

**Abstract**: The order of training samples plays a crucial role in large language models (LLMs), significantly impacting both their external performance and internal learning dynamics. Traditional methods for investigating this effect generally require retraining the model with various sample orders, which is computationally infeasible for LLMs. In this work, we improve traditional methods by designing a retraining-free framework. By approximating Adam optimizer updates with first- and second-order Taylor expansions and utilizing random projection methods to store intermediate checkpoints, our framework can efficiently estimate model parameters for arbitrary training sample orders. Next, we apply our framework to two downstream research problems: (1) Training curriculum design for LLMs -- we base our retraining-free framework to propose a novel curriculum learning strategy that augments curriculum proposals with estimated model performances, enabling more informed sample scheduling. (2) LLMs' memorization and generalization effect analysis -- we use our retraining-free framework to estimate how the positions of training samples influence LLMs' capacity for memorization and generalization. We conduct extensive experiments to validate the effectiveness of our retraining-free framework in reproducing the true model performances, and further demonstrate its potential in optimizing LLM training curricula and analyzing the memorization and generalization effects of LLMs. 

**Abstract (ZH)**: 大型语言模型中训练样本顺序起着关键作用，显著影响其外部性能和内部学习动态。传统的研究方法通常要求重新训练模型以使用不同的样本顺序，这对于大型语言模型来说是计算上不可行的。在本工作中，我们通过设计一个无需重新训练的框架来改进传统方法。通过使用一阶和二阶泰勒展开近似Adam优化器更新，并利用随机投影方法存储中间检查点，我们的框架可以高效地估计任意训练样本顺序下的模型参数。随后，我们将该框架应用于两个下游研究问题：(1) 大型语言模型的训练课程设计——我们基于无需重新训练的框架提出一种新型的课程学习策略，通过加入估计的模型性能，实现了更具信息量的样本调度。(2) 大型语言模型的记忆与泛化能力分析——我们使用无需重新训练的框架来估计训练样本的位置如何影响大型语言模型的记忆能力和泛化能力。我们进行了广泛的实验来验证该无需重新训练框架在再现真实模型性能方面的有效性，并进一步展示了其在优化大型语言模型训练课程和分析大型语言模型的记忆与泛化效果方面的潜力。 

---
# Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization 

**Title (ZH)**: 平衡令牌剪枝：超越局部优化加速视觉语言模型 

**Authors**: Kaiyuan Li, Xiaoyue Chen, Chen Gao, Yong Li, Xinlei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2505.22038)  

**Abstract**: Large Vision-Language Models (LVLMs) have shown impressive performance across multi-modal tasks by encoding images into thousands of tokens. However, the large number of image tokens results in significant computational overhead, and the use of dynamic high-resolution inputs further increases this burden. Previous approaches have attempted to reduce the number of image tokens through token pruning, typically by selecting tokens based on attention scores or image token diversity. Through empirical studies, we observe that existing methods often overlook the joint impact of pruning on both the current layer's output (local) and the outputs of subsequent layers (global), leading to suboptimal pruning decisions. To address this challenge, we propose Balanced Token Pruning (BTP), a plug-and-play method for pruning vision tokens. Specifically, our method utilizes a small calibration set to divide the pruning process into multiple stages. In the early stages, our method emphasizes the impact of pruning on subsequent layers, whereas in the deeper stages, the focus shifts toward preserving the consistency of local outputs. Extensive experiments across various LVLMs demonstrate the broad effectiveness of our approach on multiple benchmarks. Our method achieves a 78% compression rate while preserving 96.7% of the original models' performance on average. 

**Abstract (ZH)**: 大规模视觉语言模型（LVLMs）通过将图像编码为数千个令牌，在多模态任务中表现出色。然而，大量的图像令牌导致了显著的计算开销，而使用动态高分辨率输入进一步加重了这一负担。先前的方法通过令牌剪枝尝试减少图像令牌的数量，通常是基于注意力分数或图像令牌多样性进行选择。通过实证研究，我们观察到现有方法往往忽略了剪枝对当前层输出（局部）和后续层输出（全局）的联合影响，导致了次优的剪枝决策。为了解决这一挑战，我们提出了一种名为平衡令牌剪枝（BTP）的方法，这是一种即插即用的视觉令牌剪枝方法。具体而言，我们的方法利用一个小规模的校准集将剪枝过程分为多个阶段。在早期阶段，我们的方法强调剪枝对后续层的影响，而在更深层的阶段，重点转向保持局部输出的一致性。广泛的实验证实在多个大规模视觉语言模型和多个基准上展示了我们方法的广泛有效性。我们的方法在平均情况下实现了78%的压缩率，同时保留了原始模型性能的96.7%。 

---
# Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection 

**Title (ZH)**: 合成数据生成在言语不流畅检测中的分析与评估 

**Authors**: Jinming Zhang, Xuanru Zhou, Jiachen Lian, Shuhe Li, William Li, Zoe Ezzes, Rian Bogley, Lisa Wauters, Zachary Miller, Jet Vonk, Brittany Morin, Maria Gorno-Tempini, Gopala Anumanchipalli  

**Link**: [PDF](https://arxiv.org/pdf/2505.22029)  

**Abstract**: Speech dysfluency detection is crucial for clinical diagnosis and language assessment, but existing methods are limited by the scarcity of high-quality annotated data. Although recent advances in TTS model have enabled synthetic dysfluency generation, existing synthetic datasets suffer from unnatural prosody and limited contextual diversity. To address these limitations, we propose LLM-Dys -- the most comprehensive dysfluent speech corpus with LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency categories spanning both word and phoneme levels. Building upon this resource, we improve an end-to-end dysfluency detection framework. Experimental validation demonstrates state-of-the-art performance. All data, models, and code are open-sourced at this https URL. 

**Abstract (ZH)**: 言语不流畅检测对于临床诊断和语言评估至关重要，但现有方法受限于高质量标注数据的稀缺性。尽管近期TTS模型的进步使合成不流畅生成成为可能，现有合成数据集仍存在不自然的语调和有限的上下文多样性问题。为解决这些问题，我们提出了LLM-Dys——一种通过LLM增强不流畅模拟的最全面的不流畅语音语料库。该数据集涵盖了11个不流畅类别，既包括词级也包括音素级。在此资源基础上，我们改进了端到端的不流畅检测框架。实验验证显示了卓越的性能。所有数据、模型和代码均可在以下链接处公开获取：this https URL。 

---
# Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles 

**Title (ZH)**: 基于架构无关的知识蒸馏改善呼吸音分类 

**Authors**: Miika Toikkanen, June-Woo Kim  

**Link**: [PDF](https://arxiv.org/pdf/2505.22027)  

**Abstract**: Respiratory sound datasets are limited in size and quality, making high performance difficult to achieve. Ensemble models help but inevitably increase compute cost at inference time. Soft label training distills knowledge efficiently with extra cost only at training. In this study, we explore soft labels for respiratory sound classification as an architecture-agnostic approach to distill an ensemble of teacher models into a student model. We examine different variations of our approach and find that even a single teacher, identical to the student, considerably improves performance beyond its own capability, with optimal gains achieved using only a few teachers. We achieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the previous best by 0.85 and improving average Scores across architectures by more than 1.16. Our results highlight the effectiveness of knowledge distillation with soft labels for respiratory sound classification, regardless of size or architecture. 

**Abstract (ZH)**: 呼吸音数据集在规模和质量上有限，使得高性能难以实现。集成模型有所帮助，但不可避免地增加了推理时间的计算成本。软标签训练可以高效地提取知识，且额外成本仅在训练时产生。在本研究中，我们探索软标签在呼吸音分类中的应用，作为一种架构无关的方法，将多个教师模型的知识整合到一个学生模型中。我们研究了不同版本的方法，并发现即使使用一个与学生模型相同的教师模型，也可以显著提高性能，其提升效果远超自身能力。最优效果仅使用少量教师模型即可实现。我们在ICHBI上达到了新的最高分64.39，超过了之前的最佳成绩0.85，并且改善了多种架构的平均分超过1.16。我们的结果表明，无论数据集大小或模型架构如何，软标签的知识蒸馏方法在呼吸音分类中都具有显著效果。 

---
# GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement 

**Title (ZH)**: GL-PGENet: 一种稳健的文档图像增强参数化生成框架 

**Authors**: Zhihong Tang, Yang Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.22021)  

**Abstract**: Document Image Enhancement (DIE) serves as a critical component in Document AI systems, where its performance substantially determines the effectiveness of downstream tasks. To address the limitations of existing methods confined to single-degradation restoration or grayscale image processing, we present Global with Local Parametric Generation Enhancement Network (GL-PGENet), a novel architecture designed for multi-degraded color document images, ensuring both efficiency and robustness in real-world scenarios. Our solution incorporates three key innovations: First, a hierarchical enhancement framework that integrates global appearance correction with local refinement, enabling coarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network with parametric generation mechanisms that replaces conventional direct prediction, producing enhanced outputs through learned intermediate parametric representations rather than pixel-wise mapping. This approach enhances local consistency while improving model generalization. Finally, a modified NestUNet architecture incorporating dense block to effectively fuse low-level pixel features and high-level semantic features, specifically adapted for document image characteristics. In addition, to enhance generalization performance, we adopt a two-stage training strategy: large-scale pretraining on a synthetic dataset of 500,000+ samples followed by task-specific fine-tuning. Extensive experiments demonstrate the superiority of GL-PGENet, achieving state-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The model also exhibits remarkable cross-domain adaptability and maintains computational efficiency for high-resolution images without performance degradation, confirming its practical utility in real-world scenarios. 

**Abstract (ZH)**: 全局与局部参数生成增强网络（GL-PGENet）：多退化彩色文档图像的增强 

---
# VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning 

**Title (ZH)**: VRAG-RL: 基于视觉感知的图形聚合模型通过强化学习迭代推理理解丰富视觉信息 

**Authors**: Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, Feng Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2505.22019)  

**Abstract**: Effectively retrieving, reasoning and understanding visually rich information remains a challenge for RAG methods. Traditional text-based methods cannot handle visual-related information. On the other hand, current vision-based RAG approaches are often limited by fixed pipelines and frequently struggle to reason effectively due to the insufficient activation of the fundamental capabilities of models. As RL has been proven to be beneficial for model reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex reasoning across visually rich information. With this framework, VLMs interact with search engines, autonomously sampling single-turn or multi-turn reasoning trajectories with the help of visual perception tokens and undergoing continual optimization based on these samples. Our approach highlights key limitations of RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely incorporate images into the context, leading to insufficient reasoning token allocation and neglecting visual-specific perception; and (ii) When models interact with search engines, their queries often fail to retrieve relevant information due to the inability to articulate requirements, thereby leading to suboptimal performance. To address these challenges, we define an action space tailored for visually rich inputs, with actions including cropping and scaling, allowing the model to gather information from a coarse-to-fine perspective. Furthermore, to bridge the gap between users' original inquiries and the retriever, we employ a simple yet effective reward that integrates query rewriting and retrieval performance with a model-based reward. Our VRAG-RL optimizes VLMs for RAG tasks using specially designed RL strategies, aligning the model with real-world applications. The code is available at \hyperlink{this https URL}{this https URL}. 

**Abstract (ZH)**: 有效地检索、推理和理解富含视觉信息的内容仍然是RAG方法的一个挑战。传统的基于文本的方法无法处理视觉相关信息。另一方面，当前基于视觉的RAG方法往往受限于固定的管道，并且经常由于模型基本能力激活不足而难以有效推理。鉴于强化学习（RL）已被证明有助于模型推理，我们提出了VRAG-RL，这是一种专为处理富含视觉信息的复杂推理而设计的新型RL框架。通过此框架，视觉语言模型能够与搜索引擎自主交互，通过视觉感知令牌的帮助自动采样单轮或多轮的推理轨迹，并基于这些样本进行持续优化。我们的方法突出了RL在RAG领域中的关键局限性：(i) 前沿的多模态RAG方法倾向于仅将图像纳入上下文，导致推理令牌分配不足且忽视了视觉特有的感知；(ii) 当模型与搜索引擎交互时，由于无法准确表达需求，其查询往往难以检索到相关的信息，从而导致性能欠佳。为应对这些挑战，我们定义了一个针对富含视觉信息的输入的动作空间，包括裁剪和缩放等动作，使模型能够从粗略到精细的视角收集信息。此外，为了弥合用户原始询问与检索器之间的差距，我们采用了简单有效的奖励机制，该机制结合了查询重写和检索性能与基于模型的奖励。我们的VRAG-RL利用特别设计的RL策略优化视觉语言模型，使其与实际应用相契合。代码可在 <https://this https URL> 获取。 

---
# Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance 

**Title (ZH)**: Legal Assist AI：利用基于变换器的模型实现有效的法律援助 

**Authors**: Jatin Gupta, Akhil Sharma, Saransh Singhania, Ali Imam Abidi  

**Link**: [PDF](https://arxiv.org/pdf/2505.22003)  

**Abstract**: Pursuit of accessible legal assistance in India faces a critical gap, as many citizens struggle to leverage their legal rights due to limited awareness and access to relevant legal information. This paper introduces Legal Assist AI, a transformer-based model designed to bridge this gap by offering effective legal assistance through large language models (LLMs). The system retrieves relevant legal information from a curated database and generates accurate responses, enabling effective assistance for diverse users, including legal professionals, scholars, and the general public. The model was fine-tuned on extensive datasets from the Indian legal domain, including Indian Constitution, Bharatiya Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth, providing a robust understanding of the complexities of Indian law. By incorporating domain-specific legal datasets, the proposed model demonstrated remarkable efficiency and specialization in legal Question-Answering. The model was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral 7B, achieving a 60.08% score on the AIBE, outperforming its competitors in legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided common issues such as hallucinations, making it highly reliable for practical legal applications. It showcases the model's applicability in real-world legal scenarios, with future iterations aiming to enhance performance and expand its dataset to cover a broader range of multilingual and case-specific queries as well. 

**Abstract (ZH)**: 印度可及法律援助的追求面临关键缺口，由于缺乏相关法律信息的意识和获取途径，许多公民难以行使法律权利。本文介绍了基于转换器的Legal Assist AI模型，该模型旨在通过大型语言模型（LLMs）提供有效的法律援助，以弥合这一缺口。该系统从精选数据库中检索相关法律信息并生成准确的回答，从而为包括法律专业人士、学者和公众在内的多元化用户提供有效的帮助。模型在印度法律领域的大量数据集上进行了微调，包括印度宪法、印度司法综合法（BNS）、印度居民保护法（BNSS）等，提供了对印度法律复杂性的坚实理解。通过集成特定领域的法律数据集，所提出的模型在法律问答方面表现出了显著的效率和专业化。该模型在与GPT-3.5 Turbo和Mistral 7B等先进模型的评估中，AIBE得分为60.08%，在法律推理和准确性方面超过了其竞争对手。与其它模型不同，Legal Assist AI避免了常见的幻觉问题，使其在实际法律应用场景中具有很高的可靠性。它展示了该模型在真实世界法律场景中的适用性，未来版本的目标是增强性能并扩展数据集以涵盖更多多语言和案例特定的查询。 

---
# Learning World Models for Interactive Video Generation 

**Title (ZH)**: 学习世界模型进行互动视频生成 

**Authors**: Taiye Chen, Xun Hu, Zihan Ding, Chi Jin  

**Link**: [PDF](https://arxiv.org/pdf/2505.21996)  

**Abstract**: Foundational world models must be both interactive and preserve spatiotemporal coherence for effective future planning with action choices. However, present models for long video generation have limited inherent world modeling capabilities due to two main challenges: compounding errors and insufficient memory mechanisms. We enhance image-to-video models with interactive capabilities through additional action conditioning and autoregressive framework, and reveal that compounding error is inherently irreducible in autoregressive video generation, while insufficient memory mechanism leads to incoherence of world models. We propose video retrieval augmented generation (VRAG) with explicit global state conditioning, which significantly reduces long-term compounding errors and increases spatiotemporal consistency of world models. In contrast, naive autoregressive generation with extended context windows and retrieval-augmented generation prove less effective for video generation, primarily due to the limited in-context learning capabilities of current video models. Our work illuminates the fundamental challenges in video world models and establishes a comprehensive benchmark for improving video generation models with internal world modeling capabilities. 

**Abstract (ZH)**: 基础世界模型必须兼具交互性和时空连贯性，以有效进行带有行动选择的未来规划。然而，当前用于长视频生成的模型由于两大主要挑战——累积误差和记忆机制不足——而具有有限的世界建模能力。我们通过添加动作条件和自回归框架来增强图像到视频模型的交互能力，并揭示了自回归视频生成中累积误差是固有无法减少的，而记忆机制不足导致世界模型不连贯。我们提出了带有显式全局状态条件的视频检索增强生成（VRAG），显著减少了长期累积误差并提高了世界模型的时空一致性。相比之下，扩展上下文窗口的朴素自回归生成和检索增强生成在视频生成中效果较差，主要是由于当前视频模型的有限上下文学习能力。我们的工作揭示了视频世界模型的基本挑战，并建立了一个全面的基准，用于改进具有内在世界建模能力的视频生成模型。 

---
# Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning 

**Title (ZH)**: 无需奖励的信息传递在去中心化多智能体强化学习中的应用 

**Authors**: Naoto Yoshida, Tadahiro Taniguchi  

**Link**: [PDF](https://arxiv.org/pdf/2505.21985)  

**Abstract**: In multi-agent reinforcement learning (MARL), effective communication improves agent performance, particularly under partial observability. We propose MARL-CPC, a framework that enables communication among fully decentralized, independent agents without parameter sharing. MARL-CPC incorporates a message learning model based on collective predictive coding (CPC) from emergent communication research. Unlike conventional methods that treat messages as part of the action space and assume cooperation, MARL-CPC links messages to state inference, supporting communication in non-cooperative, reward-independent settings. We introduce two algorithms -Bandit-CPC and IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that both outperform standard message-as-action approaches, establishing effective communication even when messages offer no direct benefit to the sender. These results highlight MARL-CPC's potential for enabling coordination in complex, decentralized environments. 

**Abstract (ZH)**: 多智能体 reinforcement 学习（MARL）中，有效的通信能够提高智能体性能，尤其是在部分可观测情况下。我们提出了 MARL-CPC 框架，该框架能够在不共享参数的情况下使完全去中心化且独立的智能体之间进行通信。MARL-CPC 结合了源自新兴通信研究的集体预测编码（CPC）的消息学习模型。与传统的将消息视为动作空间部分并假设合作的方法不同，MARL-CPC 将消息链接到状态推断，支持在非合作且奖励无关的设置中进行通信。我们介绍了两种算法——Bandit-CPC 和 IPPO-CPC，并在非合作 MARL 任务中进行了评估。基准测试显示，两者都优于标准的消息作为动作的方法，即使消息对发送者没有直接益处，也能建立有效的通信。这些结果突显了 MARL-CPC 在复杂去中心化环境中的协调潜力。 

---
# Learning Compositional Behaviors from Demonstration and Language 

**Title (ZH)**: 从示范和语言学习组合行为 

**Authors**: Weiyu Liu, Neil Nie, Ruohan Zhang, Jiayuan Mao, Jiajun Wu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21981)  

**Abstract**: We introduce Behavior from Language and Demonstration (BLADE), a framework for long-horizon robotic manipulation by integrating imitation learning and model-based planning. BLADE leverages language-annotated demonstrations, extracts abstract action knowledge from large language models (LLMs), and constructs a library of structured, high-level action representations. These representations include preconditions and effects grounded in visual perception for each high-level action, along with corresponding controllers implemented as neural network-based policies. BLADE can recover such structured representations automatically, without manually labeled states or symbolic definitions. BLADE shows significant capabilities in generalizing to novel situations, including novel initial states, external state perturbations, and novel goals. We validate the effectiveness of our approach both in simulation and on real robots with a diverse set of objects with articulated parts, partial observability, and geometric constraints. 

**Abstract (ZH)**: 基于语言与演示的行为学习框架：长时Horizon机器人操作（BLADE） 

---
# Judging LLMs on a Simplex 

**Title (ZH)**: 在单纯形上评估LLMs 

**Authors**: Patrick Vossler, Fan Xia, Yifan Mai, Jean Feng  

**Link**: [PDF](https://arxiv.org/pdf/2505.21972)  

**Abstract**: Automated evaluation of free-form outputs from large language models (LLMs) is challenging because many distinct answers can be equally valid. A common practice is to use LLMs themselves as judges, but the theoretical properties of this approach are not yet well understood. We show that a geometric framework that represents both judges and candidates as points on a probability simplex can provide helpful insight on what is or is not identifiable using LLM judges. Our theoretical analysis uncovers a "phase transition" in ranking identifiability: for binary scoring systems, true rankings are identifiable even with weak judges under mild assumptions, while rankings become non-identifiable for three or more scoring levels even with infinite data, absent additional prior knowledge. This non-identifiability highlights how uncertainty in rankings stems from not only aleatoric uncertainty (i.e., inherent stochasticity in the data) but also epistemic uncertainty regarding which assumptions hold, an aspect that has received limited attention until now. To integrate both types of uncertainty, we use Bayesian inference to encode assumptions as priors and conduct sensitivity analysis of ranking estimates and credible intervals. Empirical evaluations across multiple benchmarks demonstrate that Bayesian inference yields more accurate rankings and substantially improves coverage rates. These results underscore the importance of taking a more holistic approach to uncertainty quantification when using LLMs as judges. 

**Abstract (ZH)**: 基于几何框架的大语言模型自由格式输出自动评估研究：从识别性相变到贝叶斯推理的探究 

---
# DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation 

**Title (ZH)**: DORAEMON：去中心化本体意识可靠智能体及其增强记忆导向导航 

**Authors**: Tianjun Gu, Linfeng Li, Xuhong Wang, Chenghua Gong, Jingyu Gong, Zhizhong Zhang, Yuan Xie, Lizhuang Ma, Xin Tan  

**Link**: [PDF](https://arxiv.org/pdf/2505.21969)  

**Abstract**: Adaptive navigation in unfamiliar environments is crucial for household service robots but remains challenging due to the need for both low-level path planning and high-level scene understanding. While recent vision-language model (VLM) based zero-shot approaches reduce dependence on prior maps and scene-specific training data, they face significant limitations: spatiotemporal discontinuity from discrete observations, unstructured memory representations, and insufficient task understanding leading to navigation failures. We propose DORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation), a novel cognitive-inspired framework consisting of Ventral and Dorsal Streams that mimics human navigation capabilities. The Dorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology Map to handle spatiotemporal discontinuities, while the Ventral Stream combines RAG-VLM and Policy-VLM to improve decision-making. Our approach also develops Nav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON on the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art performance on both success rate (SR) and success weighted by path length (SPL) metrics, significantly outperforming existing methods. We also introduce a new evaluation metric (AORI) to assess navigation intelligence better. Comprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot autonomous navigation without requiring prior map building or pre-training. 

**Abstract (ZH)**: 自适应导航在 unfamiliar environments 中对于家庭服务机器人至关重要，但因其需要低级路径规划和高级场景理解而仍然具有挑战性。虽然基于视觉-语言模型（VLM）的零-shot 方法减少了对先验地图和场景特定训练数据的依赖，但它们面临显著的局限性：来自离散观测的时间空间不连续性、无结构的内存表示以及任务理解不足导致的导航失败。我们提出 DORAEMON（Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation），一种新的认知启发式框架，由腹侧流和背侧流组成，模仿人类的导航能力。背侧流实现层次语义-空间融合和拓扑图以处理时间空间不连续性，而腹侧流结合了RAG-VLM和Policy-VLM以提高决策能力。我们的方法还开发了Nav-Ensurance以确保导航的安全性和效率。我们在HM3D、MP3D和GOAT数据集上评估了DORAEMON，其在成功率（SR）和路径长度加权成功率（SPL）指标上达到最佳性能，显著优于现有方法。我们还引入了一个新的评估指标（AORI）来更好地评估导航智能。综合实验表明，DORAEMON能够在无需构建先验地图或预训练的情况下实现零-shot 自主导航。 

---
# MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing 

**Title (ZH)**: MapStory: 由大规模语言模型驱动的文本引导地图动画原型设计与人工在环编辑 

**Authors**: Aditya Gunturu, Ben Pearman, Keiichi Ihara, Morteza Faraji, Bryan Wang, Rubaiat Habib Kazi, Ryo Suzuki  

**Link**: [PDF](https://arxiv.org/pdf/2505.21966)  

**Abstract**: We introduce MapStory, an LLM-powered animation authoring tool that generates editable map animation sequences directly from natural language text. Given a user-written script, MapStory leverages an agentic architecture to automatically produce a scene breakdown, which decomposes the script into key animation building blocks such as camera movements, visual highlights, and animated elements. Our system includes a researcher component that accurately queries geospatial information by leveraging an LLM with web search, enabling the automatic extraction of relevant regions, paths, and coordinates while allowing users to edit and query for changes or additional information to refine the results. Additionally, users can fine-tune parameters of these blocks through an interactive timeline editor. We detail the system's design and architecture, informed by formative interviews with professional animators and an analysis of 200 existing map animation videos. Our evaluation, which includes expert interviews (N=5) and a usability study (N=12), demonstrates that MapStory enables users to create map animations with ease, facilitates faster iteration, encourages creative exploration, and lowers barriers to creating map-centric stories. 

**Abstract (ZH)**: MapStory：一种基于LLM的动画创作工具，可以直接从自然语言文本生成可编辑的地图动画序列 

---
# LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents 

**Title (ZH)**: 基于LLM代理的自主后训练管道优化框架 

**Authors**: Taro Yano, Yoichi Ishibashi, Masafumi Oyamada  

**Link**: [PDF](https://arxiv.org/pdf/2505.21963)  

**Abstract**: Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks. To further tailor LLMs to specific domains or applications, post-training techniques such as Supervised Fine-Tuning (SFT), Preference Learning, and model merging are commonly employed. While each of these methods has been extensively studied in isolation, the automated construction of complete post-training pipelines remains an underexplored area. Existing approaches typically rely on manual design or focus narrowly on optimizing individual components, such as data ordering or merging strategies. In this work, we introduce LaMDAgent (short for Language Model Developing Agent), a novel framework that autonomously constructs and optimizes full post-training pipelines through the use of LLM-based agents. LaMDAgent systematically explores diverse model generation techniques, datasets, and hyperparameter configurations, leveraging task-based feedback to discover high-performing pipelines with minimal human intervention. Our experiments show that LaMDAgent improves tool-use accuracy by 9.0 points while preserving instruction-following capabilities. Moreover, it uncovers effective post-training strategies that are often overlooked by conventional human-driven exploration. We further analyze the impact of data and model size scaling to reduce computational costs on the exploration, finding that model size scalings introduces new challenges, whereas scaling data size enables cost-effective pipeline discovery. 

**Abstract (ZH)**: 基于大规模语言模型的全自动后训练管道构建框架：LaMDAgent 

---
# Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation 

**Title (ZH)**: 跨模态RAG：子维度检索增强文本到图像生成 

**Authors**: Mengdan Zhu, Senhao Cheng, Guangji Bai, Yifei Zhang, Liang Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2505.21956)  

**Abstract**: Text-to-image generation increasingly demands access to domain-specific, fine-grained, and rapidly evolving knowledge that pretrained models cannot fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to address this by retrieving globally relevant images, but they fail when no single image contains all desired elements from a complex user query. We propose Cross-modal RAG, a novel framework that decomposes both queries and images into sub-dimensional components, enabling subquery-aware retrieval and generation. Our method introduces a hybrid retrieval strategy - combining a sub-dimensional sparse retriever with a dense retriever - to identify a Pareto-optimal set of images, each contributing complementary aspects of the query. During generation, a multimodal large language model is guided to selectively condition on relevant visual features aligned to specific subqueries, ensuring subquery-aware image synthesis. Extensive experiments on MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal RAG significantly outperforms existing baselines in both retrieval and generation quality, while maintaining high efficiency. 

**Abstract (ZH)**: 跨模态RAG：分解查询和图像以实现亚查询意识的检索与生成 

---
# Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs 

**Title (ZH)**: 面向全面场景理解：结合第一人称和第三人称视角的LVLMs研究 

**Authors**: Insu Lee, Wooje Park, Jaeyun Jang, Minyoung Noh, Kyuhong Shim, Byonghyo Shim  

**Link**: [PDF](https://arxiv.org/pdf/2505.21955)  

**Abstract**: Large vision-language models (LVLMs) are increasingly deployed in interactive applications such as virtual and augmented reality, where first-person (egocentric) view captured by head-mounted cameras serves as key input. While this view offers fine-grained cues about user attention and hand-object interactions, their narrow field of view and lack of global context often lead to failures on spatially or contextually demanding queries. To address this, we introduce a framework that augments egocentric inputs with third-person (exocentric) views, providing complementary information such as global scene layout and object visibility to LVLMs. We present E3VQA, the first benchmark for multi-view question answering with 4K high-quality question-answer pairs grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a training-free prompting technique that constructs a unified scene representation by integrating scene graphs from three complementary perspectives. M3CoT enables LVLMs to reason more effectively across views, yielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini 2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key strengths and limitations of LVLMs in multi-view reasoning and highlights the value of leveraging both egocentric and exocentric inputs. 

**Abstract (ZH)**: 基于第一人称和第三人称视角的大规模视觉-语言模型多视角问答框架 

---
# UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios 

**Title (ZH)**: UniTalk: 向通用实时场景中的主动发言者检测迈进 

**Authors**: Le Thien Phuc Nguyen, Zhuoran Yu, Khoa Quang Nhat Cao, Yuwei Guo, Tu Ho Manh Pham, Tuan Tai Nguyen, Toan Ngo Duc Vo, Lucas Poon, Soochahn Lee, Yong Jae Lee  

**Link**: [PDF](https://arxiv.org/pdf/2505.21954)  

**Abstract**: We present UniTalk, a novel dataset specifically designed for the task of active speaker detection, emphasizing challenging scenarios to enhance model generalization. Unlike previously established benchmarks such as AVA, which predominantly features old movies and thus exhibits significant domain gaps, UniTalk focuses explicitly on diverse and difficult real-world conditions. These include underrepresented languages, noisy backgrounds, and crowded scenes - such as multiple visible speakers speaking concurrently or in overlapping turns. It contains over 44.5 hours of video with frame-level active speaker annotations across 48,693 speaking identities, and spans a broad range of video types that reflect real-world conditions. Through rigorous evaluation, we show that state-of-the-art models, while achieving nearly perfect scores on AVA, fail to reach saturation on UniTalk, suggesting that the ASD task remains far from solved under realistic conditions. Nevertheless, models trained on UniTalk demonstrate stronger generalization to modern "in-the-wild" datasets like Talkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark for active speaker detection, providing researchers with a valuable resource for developing and evaluating versatile and resilient models.
Dataset: this https URL
Code: this https URL 

**Abstract (ZH)**: UniTalk：一种专门针对主动说话人检测任务的新颖数据集，强调具有挑战性的场景以增强模型泛化能力 

---
# Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection 

**Title (ZH)**: 基于假数据注入的随机臂博弈的实际对抗攻击 

**Authors**: Qirun Zeng, Eric He, Richard Hoffmann, Xuchuang Wang, Jinhang Zuo  

**Link**: [PDF](https://arxiv.org/pdf/2505.21938)  

**Abstract**: Adversarial attacks on stochastic bandits have traditionally relied on some unrealistic assumptions, such as per-round reward manipulation and unbounded perturbations, limiting their relevance to real-world systems. We propose a more practical threat model, Fake Data Injection, which reflects realistic adversarial constraints: the attacker can inject only a limited number of bounded fake feedback samples into the learner's history, simulating legitimate interactions. We design efficient attack strategies under this model, explicitly addressing both magnitude constraints (on reward values) and temporal constraints (on when and how often data can be injected). Our theoretical analysis shows that these attacks can mislead both Upper Confidence Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in nearly all rounds while incurring only sublinear attack cost. Experiments on synthetic and real-world datasets validate the effectiveness of our strategies, revealing significant vulnerabilities in widely used stochastic bandit algorithms under practical adversarial scenarios. 

**Abstract (ZH)**: 对抗性攻击对随机多臂bandit问题的传统方法依赖于一些不现实的假设，如每轮奖励操控和无界的扰动，限制了其在实际系统中的相关性。我们提出一种更实际的威胁模型——假数据注入，该模型反映了现实中的对抗性约束：攻击者只能向学习者的历史中注入有限数量的有界假反馈样本，模拟合法的交互。在该模型下，我们设计了高效的攻击策略，明确地考虑了幅度约束（奖励值的限制）和时间约束（数据可以注入的时间和频率）。我们的理论分析表明，这些攻击可以几乎在所有轮次误导上置信边（UCB）和 Thompason 抽样算法选择目标臂，而攻击成本仅为亚线性。实验结果在合成数据集和真实数据集上验证了我们策略的有效性，揭示了在实际对抗性场景中广泛使用的随机多臂算法中的重大漏洞。 

---
# Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology 

**Title (ZH)**: 针对亚专科的foundation模型智能消化道病理学 

**Authors**: Lianghui Zhu, Xitong Ling, Minxi Ouyang, Xiaoping Liu, Mingxi Fu, Tian Guan, Fanglei Fu, Xuanyu Wang, Maomao Zeng, Mingxi Zhu, Yibo Jin, Liming Liu, Song Duan, Qiming He, Yizhi Wang, Luxi Xie, Houqiang Li, Yonghong He, Sufang Tian  

**Link**: [PDF](https://arxiv.org/pdf/2505.21928)  

**Abstract**: Gastrointestinal (GI) diseases represent a clinically significant burden, necessitating precise diagnostic approaches to optimize patient outcomes. Conventional histopathological diagnosis, heavily reliant on the subjective interpretation of pathologists, suffers from limited reproducibility and diagnostic variability. To overcome these limitations and address the lack of pathology-specific foundation models for GI diseases, we develop Digepath, a specialized foundation model for GI pathology. Our framework introduces a dual-phase iterative optimization strategy combining pretraining with fine-screening, specifically designed to address the detection of sparsely distributed lesion areas in whole-slide images. Digepath is pretrained on more than 353 million image patches from over 200,000 hematoxylin and eosin-stained slides of GI diseases. It attains state-of-the-art performance on 33 out of 34 tasks related to GI pathology, including pathological diagnosis, molecular prediction, gene mutation prediction, and prognosis evaluation, particularly in diagnostically ambiguous cases and resolution-agnostic tissue this http URL further translate the intelligent screening module for early GI cancer and achieve near-perfect 99.6% sensitivity across 9 independent medical institutions nationwide. The outstanding performance of Digepath highlights its potential to bridge critical gaps in histopathological practice. This work not only advances AI-driven precision pathology for GI diseases but also establishes a transferable paradigm for other pathology subspecialties. 

**Abstract (ZH)**: 胃肠（GI）疾病代表临床重要的负担，需要精确的诊断方法以优化患者预后。传统病理学诊断高度依赖于病理学家的主观解释，存在再现性和诊断变异性的局限性。为克服这些局限性并解决针对GI疾病的病理学特定基础模型的缺乏，我们开发了Digepath，一种专门针对GI病理学的基础模型。我们的框架引入了一种双重迭代优化策略，结合预训练与精筛，特别设计用于解决整个切片图像中稀疏分布病变区域的检测问题。Digepath基于超过20万例GI疾病苏木精和伊红染色切片的3亿多图像块进行预训练，其在34个与GI病理相关的任务中取得了28个任务的最佳性能，包括病理诊断、分子预测、基因突变预测和预后评估，尤其是对诊断模棱两可的情况和无分辨率依赖性组织样本。此外，该智能筛选模块在全国9家独立医疗机构中实现了超过99.6%的早期GI癌症筛查灵敏度。Digepath的卓越性能突显了其在病理学实践中的潜在重要性。这项工作不仅促进了基于AI的精准胃肠病理学的发展，还为其他病理学亚专科建立了可转移的范式。 

---
# Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning 

**Title (ZH)**: 超越完成：一种通用知识图推理的foundation模型 

**Authors**: Yin Hua, Zhiqiang Liu, Mingyang Chen, Zheng Fang, Chi Man Wong, Lingxiao Li, Chi Man Vong, Huajun Chen, Wen Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21926)  

**Abstract**: In natural language processing (NLP) and computer vision (CV), the successful application of foundation models across diverse tasks has demonstrated their remarkable potential. However, despite the rich structural and textual information embedded in knowledge graphs (KGs), existing research of foundation model for KG has primarily focused on their structural aspects, with most efforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This limitation has hindered progress in addressing more challenging out-of-KG tasks. In this paper, we introduce MERRY, a foundation model for general knowledge graph reasoning, and investigate its performance across two task categories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG question answering, KGQA). We not only utilize the structural information, but also the textual information in KGs. Specifically, we propose a multi-perspective Conditional Message Passing (CMP) encoding architecture to bridge the gap between textual and structural modalities, enabling their seamless integration. Additionally, we introduce a dynamic residual fusion module to selectively retain relevant textual information and a flexible edge scoring mechanism to adapt to diverse downstream tasks. Comprehensive evaluations on 28 datasets demonstrate that MERRY outperforms existing baselines in most scenarios, showcasing strong reasoning capabilities within KGs and excellent generalization to out-of-KG tasks such as KGQA. 

**Abstract (ZH)**: 基础模型在通用知识图谱推理中的应用：MERRY的研究与性能评估 

---
# FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design 

**Title (ZH)**: FALCON: 一种全自动布局约束模拟电路设计的机器学习框架 

**Authors**: Asal Mehradfar, Xuzhe Zhao, Yilun Huang, Emir Ceyani, Yankai Yang, Shihao Han, Hamidreza Aghasi, Salman Avestimehr  

**Link**: [PDF](https://arxiv.org/pdf/2505.21923)  

**Abstract**: Designing analog circuits from performance specifications is a complex, multi-stage process encompassing topology selection, parameter inference, and layout feasibility. We introduce FALCON, a unified machine learning framework that enables fully automated, specification-driven analog circuit synthesis through topology selection and layout-constrained optimization. Given a target performance, FALCON first selects an appropriate circuit topology using a performance-driven classifier guided by human design heuristics. Next, it employs a custom, edge-centric graph neural network trained to map circuit topology and parameters to performance, enabling gradient-based parameter inference through the learned forward model. This inference is guided by a differentiable layout cost, derived from analytical equations capturing parasitic and frequency-dependent effects, and constrained by design rules. We train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave circuits, generated and simulated using Cadence Spectre across 20 expert-designed topologies. Through this evaluation, FALCON demonstrates >99\% accuracy in topology inference, <10\% relative error in performance prediction, and efficient layout-aware design that completes in under 1 second per instance. Together, these results position FALCON as a practical and extensible foundation model for end-to-end analog circuit design automation. 

**Abstract (ZH)**: 从性能规范设计模拟电路是一种多阶段复杂过程，涉及拓扑选择、参数推理和布局可行性。我们提出FALCON，一个统一的机器学习框架，实现了从性能规范驱动的模拟电路综合，包括拓扑选择和布局约束优化。给定一个目标性能，FALCON 首先使用受人类设计启发的性能驱动分类器选择合适的电路拓扑。接着，它采用一个针对边设计的图神经网络，该网络经过训练以映射电路拓扑和参数到性能，从而通过学习的前向模型实现梯度为基础的参数推理。这一推理过程受到差分布局成本的引导，该成本来源于捕捉寄生效应和频率依赖效应的分析方程，并受到设计规则的约束。我们在使用Cadence Spectre对20种专家设计的拓扑结构进行仿真生成的大规模定制数据集（100万模拟毫米波电路）上训练和评估FALCON。通过此次评估，FALCON 在拓扑结构推理上的准确率超过99%，性能预测的相对误差低于10%，并且具有布局感知的设计能力，能够在每例少于1秒的时间内完成设计。这些结果将FALCON 定位为端到端模拟电路设计自动化的一个可行且可扩展的基本模型。 

---
# Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference 

**Title (ZH)**: 面向LLM推理中前缀预填的高效键值缓存管理 

**Authors**: Yue Zhu, Hao Yu, Chen Wang, Zhuoran Liu, Eun Kyung Lee  

**Link**: [PDF](https://arxiv.org/pdf/2505.21919)  

**Abstract**: The increasing adoption of large language models (LLMs) with extended context windows necessitates efficient Key-Value Cache (KVC) management to optimize inference performance. Inference workloads like Retrieval-Augmented Generation (RAG) and agents exhibit high cache reusability, making efficient caching critical to reducing redundancy and improving speed. We analyze real-world KVC access patterns using publicly available traces and evaluate commercial key-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1] and Sherman [2]) for KVC metadata management. Our work demonstrates the lack of tailored storage solution for KVC prefilling, underscores the need for an efficient distributed caching system with optimized metadata management for LLM workloads, and provides insights into designing improved KVC management systems for scalable, low-latency inference. 

**Abstract (ZH)**: 大型语言模型（LLM）采用扩展上下文窗口的日益增加采用 necessitates 效率的关键值缓存（KVC）管理以优化推断性能。类似于检索增强生成（RAG）和智能体的推断工作负载表现出高的缓存重用性，这使得高效的缓存管理对于减少冗余和提高速度至关重要。我们使用公开可用的跟踪分析实际的KVC访问模式，并评估像Redis这样的商用键值存储以及基于RDMA的前沿系统（CHIME [1]和Sherman [2]）进行KVC元数据管理。我们的工作展示了为KVC预填充缺乏定制化的存储解决方案，强调了为LLM工作负载设计高效分布缓存系统并优化元数据管理的重要性，并提供了设计可扩展、低延迟推断的改进KVC管理系统的见解。 

---
# Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing 

**Title (ZH)**: 使用变压器的自监督学习方法用于多维传感器数据处理 

**Authors**: Haruki Kai, Tsuyoshi Okita  

**Link**: [PDF](https://arxiv.org/pdf/2505.21918)  

**Abstract**: We developed a deep learning algorithm for human activity recognition using sensor signals as input. In this study, we built a pretrained language model based on the Transformer architecture, which is widely used in natural language processing. By leveraging this pretrained model, we aimed to improve performance on the downstream task of human activity recognition. While this task can be addressed using a vanilla Transformer, we propose an enhanced n-dimensional numerical processing Transformer that incorporates three key features: embedding n-dimensional numerical data through a linear layer, binning-based pre-processing, and a linear transformation in the output layer. We evaluated the effectiveness of our proposed model across five different datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15% improvements in accuracy. 

**Abstract (ZH)**: 我们开发了一种使用传感器信号进行人类活动识别的深度学习算法。在本研究中，我们基于广泛应用于自然语言处理的Transformer架构构建了一个预训练语言模型。通过利用该预训练模型，我们旨在提高人类活动识别下游任务的性能。尽管该任务可以使用vanilla Transformer解决，但我们提出了一种增强的n维数值处理Transformer，该模型包含三个关键特征：通过线性层嵌入n维数值数据、基于分箱的预处理以及输出层的线性变换。我们跨五个不同数据集评估了所提出模型的有效性。与vanilla Transformer相比，我们的模型在准确性上提高了10%-15%。 

---
# Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding 

**Title (ZH)**: 基于强化学习的预训练语言模型离分布推理研究：诊断相关组编码的实证分析 

**Authors**: Hanyin Wang, Zhenbang Wu, Gururaj Kolar, Hariprasad Korsapati, Brian Bartlett, Bryan Hull, Jimeng Sun  

**Link**: [PDF](https://arxiv.org/pdf/2505.21908)  

**Abstract**: Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks. 

**Abstract (ZH)**: 基于大规模强化学习的DRG自动编码方法：DRG-Sapphire 

---
# Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge 

**Title (ZH)**: 具有开放世界体.maxLength限制了我只能输出最基本的内容，因此标题翻译如下：

基于预训练知识的视觉-语言-行动模型与开放世界体化推理 

**Authors**: Zhongyi Zhou, Yichen Zhu, Junjie Wen, Chaomin Shen, Yi Xu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21906)  

**Abstract**: Vision-language-action (VLA) models have emerged as the next generation of models in robotics. However, despite leveraging powerful pre-trained Vision-Language Models (VLMs), existing end-to-end VLA systems often lose key capabilities during fine-tuning as the model adapts to specific robotic tasks. We argue that a generalizable VLA model should retain and expand upon the VLM's core competencies: 1) Open-world embodied reasoning - the VLA should inherit the knowledge from VLM, i.e., recognize anything that the VLM can recognize, capable of solving math problems, possessing visual-spatial intelligence, 2) Reasoning following - effectively translating the open-world reasoning into actionable steps for the robot. In this work, we introduce ChatVLA-2, a novel mixture-of-expert VLA model coupled with a specialized three-stage training pipeline designed to preserve the VLM's original strengths while enabling actionable reasoning. To validate our approach, we design a math-matching task wherein a robot interprets math problems written on a whiteboard and picks corresponding number cards from a table to solve equations. Remarkably, our method exhibits exceptional mathematical reasoning and OCR capabilities, despite these abilities not being explicitly trained within the VLA. Furthermore, we demonstrate that the VLA possesses strong spatial reasoning skills, enabling it to interpret novel directional instructions involving previously unseen objects. Overall, our method showcases reasoning and comprehension abilities that significantly surpass state-of-the-art imitation learning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a substantial advancement toward developing truly generalizable robotic foundation models endowed with robust reasoning capacities. 

**Abstract (ZH)**: Vision-语言-行动（VLA）模型已成为机器人领域的下一代模型。然而，尽管利用了强大的预训练视觉-语言模型（VLMs），现有的端到端VLA系统在微调过程中往往会失去一些关键能力，随着模型适应特定的机器人任务，这些能力往往会消失。我们认为，可泛化的VLA模型应当保留并扩展VLM的核心能力：1）开放世界体像是推理——VLA應继承VLM的知识，即识别VLM能识别的一切内容，能够解决数学问题，具备视觉-空间智能；2）跟随推理——有效将开放世界推理转化为可供机器人执行的操作步骤。在这项工作中，我们引入了ChatVLA-2，这是一种新颖的专家混合VLA模型，配备了专门的三阶段训练流程，旨在保留VLM的原始优势，同时使模型具备可执行的推理能力。为了验证我们的方法，我们设计了一个数学配对任务，其中机器人解读写在白板上的数学问题，并从桌子上选取相应数字卡片解决问题。令人惊讶的是，尽管这些能力未在VLA中显式训练，我们的方法仍表现出突出的数学推理能力和光学字符识别（OCR）能力。此外，我们展示了VLA具备强大的空间推理能力，能够解释涉及未见过物体的新型方向指令。总体而言，我们的方法展现了远超现有的模仿学习方法（如OpenVLA、DexVLA和pi-zero）的推理和理解能力。这项工作代表了朝着开发具备强大推理能力的真正可泛化机器人基础模型迈出的重要一步。 

---
# CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation 

**Title (ZH)**: CAST: 对比适应与蒸馏在半监督实例分割中的应用 

**Authors**: Pardis Taghavi, Tian Liu, Renjie Li, Reza Langari, Zhengzhong Tu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21904)  

**Abstract**: Instance segmentation demands costly per-pixel annotations and large models. We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework that compresses pretrained vision foundation models (VFM) into compact experts using limited labeled and abundant unlabeled data. CAST unfolds in three stages: (1) domain adaptation of the VFM teacher(s) via self-training with contrastive pixel calibration, (2) distillation into a compact student via a unified multi-objective loss that couples standard supervision and pseudo-labels with our instance-aware pixel-wise contrastive term, and (3) fine-tuning on labeled data to remove residual pseudo-label bias. Central to CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask and class scores to mine informative negatives and enforce clear inter-instance margins. By maintaining this contrastive signal across both adaptation and distillation, we align teacher and student embeddings and fully leverage unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs. 15.2) and outperforms state-of-the-art semi-supervised approaches. 

**Abstract (ZH)**: 半监督知识蒸馏框架CAST：利用有限标注和丰富未标注数据压缩预训练视觉基础模型进行实例分割 

---
# Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development 

**Title (ZH)**: 共存储：资源aware多Agent协作的软件开发方法 

**Authors**: Rennai Qiu, Chen Qian, Ran Li, Yufan Dang, Weize Chen, Cheng Yang, Yingli Zhang, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, Maosong Sun  

**Link**: [PDF](https://arxiv.org/pdf/2505.21898)  

**Abstract**: Recent advancements in Large Language Models (LLMs) and autonomous agents have demonstrated remarkable capabilities across various domains. However, standalone agents frequently encounter limitations when handling complex tasks that demand extensive interactions and substantial computational resources. Although Multi-Agent Systems (MAS) alleviate some of these limitations through collaborative mechanisms like task decomposition, iterative communication, and role specialization, they typically remain resource-unaware, incurring significant inefficiencies due to high token consumption and excessive execution time. To address these limitations, we propose a resource-aware multi-agent system -- Co-Saving (meaning that multiple agents collaboratively engage in resource-saving activities), which leverages experiential knowledge to enhance operational efficiency and solution quality. Our key innovation is the introduction of "shortcuts" -- instructional transitions learned from historically successful trajectories -- which allows to bypass redundant reasoning agents and expedite the collective problem-solving process. Experiments for software development tasks demonstrate significant advantages over existing methods. Specifically, compared to the state-of-the-art MAS ChatDev, our method achieves an average reduction of 50.85% in token usage, and improves the overall code quality by 10.06%. 

**Abstract (ZH)**: Recent advancements in大型语言模型（LLMs）和自主代理已在各个领域展示了卓越的能力。然而，独立运行的代理在处理需要大量交互和大量计算资源的复杂任务时常常遇到限制。尽管多代理系统（MAS）通过任务分解、迭代通信和角色专门化等协作机制部分缓解了这些限制，但它们通常仍缺乏资源意识，由于高token消耗和执行时间过长而造成显著的低效。为了克服这些限制，我们提出了一种资源意识型多代理系统——Co-Saving（意味着多个代理协作参与资源节约活动），该系统利用经验知识以提高操作效率和解决方案质量。我们的关键创新在于引入“捷径”——从历史上成功的轨迹中学到的指导性转换，从而使代理能够绕过冗余的推理过程，加速集体问题解决过程。针对软件开发任务的实验表明，与现有方法相比，该方法在token使用量上平均减少了50.85%，并在整体代码质量上提高了10.06%。 

---
# Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization 

**Title (ZH)**: 通过后训练量化压缩激活正弦的低秩适配器 

**Authors**: Cameron Gordon, Yiping Ji, Hemanth Saratchandran, Paul Albert, Simon Lucey  

**Link**: [PDF](https://arxiv.org/pdf/2505.21895)  

**Abstract**: Low-Rank Adaptation (LoRA) has become a standard approach for parameter-efficient fine-tuning, offering substantial reductions in trainable parameters by modeling updates as the product of two low-rank matrices. While effective, the low-rank constraint inherently limits representational capacity, often resulting in reduced performance compared to full-rank fine-tuning. Recent work by Ji et al. (2025) has addressed this limitation by applying a fixed-frequency sinusoidal transformation to low-rank adapters, increasing their stable rank without introducing additional parameters. This raises a crucial question: can the same sine-activated technique be successfully applied within the context of Post-Training Quantization to retain benefits even after model compression? In this paper, we investigate this question by extending the sinusoidal transformation framework to quantized LoRA adapters. We develop a theoretical analysis showing that the stable rank of a quantized adapter is tightly linked to that of its full-precision counterpart, motivating the use of such rank-enhancing functions even under quantization. Our results demonstrate that the expressivity gains from a sinusoidal non-linearity persist after quantization, yielding highly compressed adapters with negligible loss in performance. We validate our approach across a range of fine-tuning tasks for language, vision and text-to-image generation achieving significant memory savings while maintaining competitive accuracy. 

**Abstract (ZH)**: Sinusoidal-Activated Quantized LoRA Adapters: Maintaining Expressivity Post-Training Quantization 

---
# SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training 

**Title (ZH)**: SDPO: 基于重要性抽样的直接偏好优化以实现稳定的扩散训练 

**Authors**: Xiaomeng Yang, Zhiyu Tan, Junyan Wang, Zhijian Zhou, Hao Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.21893)  

**Abstract**: Preference learning has become a central technique for aligning generative models with human expectations. Recently, it has been extended to diffusion models through methods like Direct Preference Optimization (DPO). However, existing approaches such as Diffusion-DPO suffer from two key challenges: timestep-dependent instability, caused by a mismatch between the reverse and forward diffusion processes and by high gradient variance in early noisy timesteps, and off-policy bias arising from the mismatch between optimization and data collection policies. We begin by analyzing the reverse diffusion trajectory and observe that instability primarily occurs at early timesteps with low importance weights. To address these issues, we first propose DPO-C\&M, a practical strategy that improves stability by clipping and masking uninformative timesteps while partially mitigating off-policy bias. Building on this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a principled framework that incorporates importance sampling into the objective to fully correct for off-policy bias and emphasize informative updates during the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO, with SDPO achieving superior VBench scores, human preference alignment, and training robustness. These results highlight the importance of timestep-aware, distribution-corrected optimization in diffusion-based preference learning. 

**Abstract (ZH)**: 偏好学习已成为使生成模型与人类期望相一致的核心技术。最近，它通过直接偏好优化（DPO）等方法扩展到了扩散模型。然而，现有的方法如Diffusion-DPO面临两个关键挑战：扩散过程和反向扩散过程之间的时间步依赖性不稳定性，以及优化策略与数据收集策略之间的不匹配导致的偏置。我们首先分析了反向扩散轨迹，发现不稳定性主要出现在低重要权重的早期时间步。为了解决这些问题，我们首先提出了DPO-C&‌M，这是一种实用策略，通过裁剪和屏蔽无信息的时间步来提高稳定性，并部分减轻了偏置。在此基础上，我们引入了SDPO（重要性加权直接偏好优化），这是一种原理性的框架，将重要性采样纳入目标中，以完全校正偏置并在扩散过程中强调信息性更新。在CogVideoX-2B、CogVideoX-5B和Wan2.1-1.3B上的实验表明，这两种方法均优于标准的Diffusion-DPO，其中SDPO在VBench分数、人类偏好对齐和训练稳健性方面表现更优。这些结果突显了在基于扩散的偏好学习中，时间步感知和分布校正优化的重要性。 

---
# Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation 

**Title (ZH)**: 将大型语言模型融入大规模城市综合体流动性模拟 

**Authors**: Yu-Lun Song, Chung-En Tsern, Che-Cheng Wu, Yu-Ming Chang, Syuan-Bo Huang, Wei-Chu Chen, Michael Chia-Liang Lin, Yu-Ta Lin  

**Link**: [PDF](https://arxiv.org/pdf/2505.21880)  

**Abstract**: This study presents an innovative approach to urban mobility simulation by integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM). Unlike traditional rule-based ABM, the proposed framework leverages LLM to enhance agent diversity and realism by generating synthetic population profiles, allocating routine and occasional locations, and simulating personalized routes. Using real-world data, the simulation models individual behaviors and large-scale mobility patterns in Taipei City. Key insights, such as route heat maps and mode-specific indicators, provide urban planners with actionable information for policy-making. Future work focuses on establishing robust validation frameworks to ensure accuracy and reliability in urban planning applications. 

**Abstract (ZH)**: 本研究通过将大型语言模型（LLM）与基于代理的模型（ABM）集成，提出了一种创新的城市交通模拟方法。不同于传统的基于规则的ABM，提出的框架利用LLM增强代理的多样性和现实性，通过生成合成人口概况、分配常规和偶发地点以及模拟个性化路线。利用真实数据，模拟模型在台北市层面个体行为和大规模交通模式。关键洞见，如路径热图和特定模式的指标，为城市规划者提供了可用于政策制定的实际信息。未来工作将致力于建立稳健的验证框架，确保在城市规划应用中的准确性和可靠性。 

---
# Symbolic Foundation Regressor on Complex Networks 

**Title (ZH)**: 复杂网络中的符号基础回归器 

**Authors**: Weiting Liu, Jiaxu Cui, Jiao Hu, En Wang, Bo Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21879)  

**Abstract**: In science, we are interested not only in forecasting but also in understanding how predictions are made, specifically what the interpretable underlying model looks like. Data-driven machine learning technology can significantly streamline the complex and time-consuming traditional manual process of discovering scientific laws, helping us gain insights into fundamental issues in modern science. In this work, we introduce a pre-trained symbolic foundation regressor that can effectively compress complex data with numerous interacting variables while producing interpretable physical representations. Our model has been rigorously tested on non-network symbolic regression, symbolic regression on complex networks, and the inference of network dynamics across various domains, including physics, biochemistry, ecology, and epidemiology. The results indicate a remarkable improvement in equation inference efficiency, being three times more effective than baseline approaches while maintaining accurate predictions. Furthermore, we apply our model to uncover more intuitive laws of interaction transmission from global epidemic outbreak data, achieving optimal data fitting. This model extends the application boundary of pre-trained symbolic regression models to complex networks, and we believe it provides a foundational solution for revealing the hidden mechanisms behind changes in complex phenomena, enhancing interpretability, and inspiring further scientific discoveries. 

**Abstract (ZH)**: 在科学研究中，我们不仅关注预测，还致力于理解预测是如何产生的，特别是可解释的潜在模型是什么样的。数据驱动的机器学习技术可以极大地简化传统手动发现科学规律的复杂和耗时过程，帮助我们深入了解现代科学中的基本问题。在这项工作中，我们提出了一种预训练符号基础回归器，它可以有效地压缩具有众多交互变量的复杂数据，并生成可解释的物理表示。我们的模型已在非网络符号回归、复杂网络上的符号回归以及各种领域（包括物理学、生物化学、生态学和流行病学）的网络动力学推理中进行了严格测试。结果表明，在方程推理效率上取得了显著改进，相比基线方法有效三倍，同时保持了准确的预测。此外，我们还将该模型应用于从全球流行病爆发数据中揭示更直观的交互传播定律，并实现了最佳的数据拟合。该模型将预训练符号回归模型的应用边界扩展到了复杂网络，并相信它为揭示复杂现象背后隐藏机制、增强可解释性和激发进一步的科学发现提供了基础解决方案。 

---
# EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance 

**Title (ZH)**: EPiC: 有效的视频相机控制学习与精确锚视频指导 

**Authors**: Zun Wang, Jaemin Cho, Jialu Li, Han Lin, Jaehong Yoon, Yue Zhang, Mohit Bansal  

**Link**: [PDF](https://arxiv.org/pdf/2505.21876)  

**Abstract**: Recent approaches on 3D camera control in video diffusion models (VDMs) often create anchor videos to guide diffusion models as a structured prior by rendering from estimated point clouds following annotated camera trajectories. However, errors inherent in point cloud estimation often lead to inaccurate anchor videos. Moreover, the requirement for extensive camera trajectory annotations further increases resource demands. To address these limitations, we introduce EPiC, an efficient and precise camera control learning framework that automatically constructs high-quality anchor videos without expensive camera trajectory annotations. Concretely, we create highly precise anchor videos for training by masking source videos based on first-frame visibility. This approach ensures high alignment, eliminates the need for camera trajectory annotations, and thus can be readily applied to any in-the-wild video to generate image-to-video (I2V) training pairs. Furthermore, we introduce Anchor-ControlNet, a lightweight conditioning module that integrates anchor video guidance in visible regions to pretrained VDMs, with less than 1% of backbone model parameters. By combining the proposed anchor video data and ControlNet module, EPiC achieves efficient training with substantially fewer parameters, training steps, and less data, without requiring modifications to the diffusion model backbone typically needed to mitigate rendering misalignments. Although being trained on masking-based anchor videos, our method generalizes robustly to anchor videos made with point clouds during inference, enabling precise 3D-informed camera control. EPiC achieves SOTA performance on RealEstate10K and MiraData for I2V camera control task, demonstrating precise and robust camera control ability both quantitatively and qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to video-to-video scenarios. 

**Abstract (ZH)**: Recent Approaches on 3D Camera Control in Video Diffusion Models (VDMs) Often Create Anchor Videos to Guide Diffusion Models as a Structured Prior by Rendering from Estimated Point Clouds Following Annotated Camera Trajectories: Introducing EPiC, an Efficient and Precise Camera Control Learning Framework Without Expensive Camera Trajectory Annotations 

---
# HelixDesign-Binder: A Scalable Production-Grade Platform for Binder Design Built on HelixFold3 

**Title (ZH)**: HelixDesign-Binder：基于HelixFold3的可扩展生产级配体设计平台 

**Authors**: Jie Gao, Jun Li, Jing Hu, Shanzhuo Zhang, Kunrui Zhu, Yueyang Huang, Xiaonan Zhang, Xiaomin Fang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21873)  

**Abstract**: Protein binder design is central to therapeutics, diagnostics, and synthetic biology, yet practical deployment remains challenging due to fragmented workflows, high computational costs, and complex tool integration. We present HelixDesign-Binder, a production-grade, high-throughput platform built on HelixFold3 that automates the full binder design pipeline, from backbone generation and sequence design to structural evaluation and multi-dimensional scoring. By unifying these stages into a scalable and user-friendly system, HelixDesign-Binder enables efficient exploration of binder candidates with favorable structural, energetic, and physicochemical properties. The platform leverages Baidu Cloud's high-performance infrastructure to support large-scale design and incorporates advanced scoring metrics, including ipTM, predicted binding free energy, and interface hydrophobicity. Benchmarking across six protein targets demonstrates that HelixDesign-Binder reliably produces diverse and high-quality binders, some of which match or exceed validated designs in predicted binding affinity. HelixDesign-Binder is accessible via an interactive web interface in PaddleHelix platform, supporting both academic research and industrial applications in antibody and protein binder development. 

**Abstract (ZH)**: HelixDesign-Binder：一种基于HelixFold3的生产级高通量蛋白质bindings设计平台 

---
# Evaluating the Retrieval Robustness of Large Language Models 

**Title (ZH)**: 评估大型语言模型的检索 robustness 

**Authors**: Shuyang Cao, Karthik Radhakrishnan, David Rosenberg, Steven Lu, Pengxiang Cheng, Lu Wang, Shiyue Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21870)  

**Abstract**: Retrieval-augmented generation (RAG) generally enhances large language models' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also lead to performance degradation due to imperfect retrieval and the model's limited ability to leverage retrieved content. In this work, we evaluate the robustness of LLMs in practical RAG setups (henceforth retrieval robustness). We focus on three research questions: (1) whether RAG is always better than non-RAG; (2) whether more retrieved documents always lead to better performance; (3) and whether document orders impact results. To facilitate this study, we establish a benchmark of 1500 open-domain questions, each with retrieved documents from Wikipedia. We introduce three robustness metrics, each corresponds to one research question. Our comprehensive experiments, involving 11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit surprisingly high retrieval robustness; nonetheless, different degrees of imperfect robustness hinders them from fully utilizing the benefits of RAG. 

**Abstract (ZH)**: 检索增强生成（RAG）一般增强大规模语言模型（LLMs）解决知识密集型任务的能力，但也可能由于检索不完善和模型有限的利用检索内容的能力而导致性能下降。在本工作中，我们评估了LLMs在实际RAG设置中的稳健性（以下简称检索稳健性）。我们关注三个研究问题：（1）RAG是否总是优于非RAG；（2）检索更多文档是否总是能获得更好的性能；（3）文档顺序是否影响结果。为了便于这项研究，我们建立了一个包含1500个开放域问题的数据基准，每个问题都附有来自维基百科的检索文档。我们介绍了三个稳健性指标，每个指标对应一个问题。全面的实验涉及11种LLM和3种提示策略，表明这些LLM都表现出惊人的检索稳健性；然而，不同程度的不完善稳健性阻碍了它们充分利用RAG的好处。 

---
# CSI-Bench: A Large-Scale In-the-Wild Dataset for Multi-task WiFi Sensing 

**Title (ZH)**: CSI-Bench: 一种大规模真实场景多任务WiFi感知数据集 

**Authors**: Guozhen Zhu, Yuqian Hu, Weihang Gao, Wei-Hsiang Wang, Beibei Wang, K. J. Ray Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21866)  

**Abstract**: WiFi sensing has emerged as a compelling contactless modality for human activity monitoring by capturing fine-grained variations in Channel State Information (CSI). Its ability to operate continuously and non-intrusively while preserving user privacy makes it particularly suitable for health monitoring. However, existing WiFi sensing systems struggle to generalize in real-world settings, largely due to datasets collected in controlled environments with homogeneous hardware and fragmented, session-based recordings that fail to reflect continuous daily activity.
We present CSI-Bench, a large-scale, in-the-wild benchmark dataset collected using commercial WiFi edge devices across 26 diverse indoor environments with 35 real users. Spanning over 461 hours of effective data, CSI-Bench captures realistic signal variability under natural conditions. It includes task-specific datasets for fall detection, breathing monitoring, localization, and motion source recognition, as well as a co-labeled multitask dataset with joint annotations for user identity, activity, and proximity. To support the development of robust and generalizable models, CSI-Bench provides standardized evaluation splits and baseline results for both single-task and multi-task learning. CSI-Bench offers a foundation for scalable, privacy-preserving WiFi sensing systems in health and broader human-centric applications. 

**Abstract (ZH)**: CSI-Bench：一种大规模的实地WiFi信道状态信息基准数据集 

---
# Extracting Research Instruments from Educational Literature Using LLMs 

**Title (ZH)**: 使用大型语言模型从教育文献中提取研究工具 

**Authors**: Jiseung Yoo, Curran Mahowald, Meiyu Li, Wei Ai  

**Link**: [PDF](https://arxiv.org/pdf/2505.21855)  

**Abstract**: Large Language Models (LLMs) are transforming information extraction from academic literature, offering new possibilities for knowledge management. This study presents an LLM-based system designed to extract detailed information about research instruments used in the education field, including their names, types, target respondents, measured constructs, and outcomes. Using multi-step prompting and a domain-specific data schema, it generates structured outputs optimized for educational research. Our evaluation shows that this system significantly outperforms other approaches, particularly in identifying instrument names and detailed information. This demonstrates the potential of LLM-powered information extraction in educational contexts, offering a systematic way to organize research instrument information. The ability to aggregate such information at scale enhances accessibility for researchers and education leaders, facilitating informed decision-making in educational research and policy. 

**Abstract (ZH)**: 大型语言模型（LLMs）正在重新定义学术文献的信息抽取，为知识管理提供了新的可能性。本研究介绍了一种基于LLM的系统，用于提取教育领域中使用的研究工具的详细信息，包括工具名称、类型、目标受访者、测量的构建模块以及结果。利用多步提示和领域特定的数据模式，该系统生成了优化用于教育研究的结构化输出。我们的评估表明，该系统在识别工具名称和详细信息方面显著优于其他方法。这证明了LLM驱动的信息提取在教育环境中的潜力，提供了一种系统性的方法来组织研究工具信息。大量聚合此类信息提高了研究人员和教育领导者的可访问性，促进了教育研究和政策中的明智决策。 

---
# Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification 

**Title (ZH)**: 基于梯度的点云分类对抗攻击再思考 

**Authors**: Jun Chen, Xinke Li, Mingyue Xu, Tianrui Li, Chongshou Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.21854)  

**Abstract**: Gradient-based adversarial attacks have become a dominant approach for evaluating the robustness of point cloud classification models. However, existing methods often rely on uniform update rules that fail to consider the heterogeneous nature of point clouds, resulting in excessive and perceptible perturbations. In this paper, we rethink the design of gradient-based attacks by analyzing the limitations of conventional gradient update mechanisms and propose two new strategies to improve both attack effectiveness and imperceptibility. First, we introduce WAAttack, a novel framework that incorporates weighted gradients and an adaptive step-size strategy to account for the non-uniform contribution of points during optimization. This approach enables more targeted and subtle perturbations by dynamically adjusting updates according to the local structure and sensitivity of each point. Second, we propose SubAttack, a complementary strategy that decomposes the point cloud into subsets and focuses perturbation efforts on structurally critical regions. Together, these methods represent a principled rethinking of gradient-based adversarial attacks for 3D point cloud classification. Extensive experiments demonstrate that our approach outperforms state-of-the-art baselines in generating highly imperceptible adversarial examples. Code will be released upon paper acceptance. 

**Abstract (ZH)**: 基于梯度的对抗攻击已成为评估点云分类模型鲁棒性的主导方法。然而，现有方法往往依赖于统一的更新规则，未能考虑点云的异质性，导致产生过多且可感知的扰动。本文重新思考基于梯度的攻击设计，分析传统梯度更新机制的局限性，并提出两种新策略以提高攻击的有效性和不可感知性。首先，我们引入WAAttack，这是一种新颖的框架，结合了加权梯度和自适应步长策略，以反映优化过程中各点的非均匀贡献。此方法通过根据每个点的局部结构和敏感性动态调整更新来实现更针对性和细微的扰动。其次，我们提出SubAttack，这是一种互补策略，将点云分解为子集，并将扰动努力集中在结构关键区域。这两种方法代表了对3D点云分类中基于梯度的对抗攻击的理论性重新思考。 extensive实验表明，我们的方法在生成高度不可感知的对抗示例方面优于现有最先进的基线。论文接受后将发布代码。 

---
# A Provable Approach for End-to-End Safe Reinforcement Learning 

**Title (ZH)**: 可验证的方法实现端到端安全强化学习 

**Authors**: Akifumi Wachi, Kohei Miyaguchi, Takumi Tanabe, Rei Sato, Youhei Akimoto  

**Link**: [PDF](https://arxiv.org/pdf/2505.21852)  

**Abstract**: A longstanding goal in safe reinforcement learning (RL) is a method to ensure the safety of a policy throughout the entire process, from learning to operation. However, existing safe RL paradigms inherently struggle to achieve this objective. We propose a method, called Provably Lifetime Safe RL (PLS), that integrates offline safe RL with safe policy deployment to address this challenge. Our proposed method learns a policy offline using return-conditioned supervised learning and then deploys the resulting policy while cautiously optimizing a limited set of parameters, known as target returns, using Gaussian processes (GPs). Theoretically, we justify the use of GPs by analyzing the mathematical relationship between target and actual returns. We then prove that PLS finds near-optimal target returns while guaranteeing safety with high probability. Empirically, we demonstrate that PLS outperforms baselines both in safety and reward performance, thereby achieving the longstanding goal to obtain high rewards while ensuring the safety of a policy throughout the lifetime from learning to operation. 

**Abstract (ZH)**: 证明可全程保障安全的强化学习（Provably Lifetime Safe RL） 

---
# Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by treating action trajectories as flow trajectories 

**Title (ZH)**: 流式流策略：通过将动作轨迹视为流轨迹来简化扩散/流匹配策略 

**Authors**: Sunshine Jiang, Xiaolin Fang, Nicholas Roy, Tomás Lozano-Pérez, Leslie Pack Kaelbling, Siddharth Ancha  

**Link**: [PDF](https://arxiv.org/pdf/2505.21851)  

**Abstract**: Recent advances in diffusion$/$flow-matching policies have enabled imitation learning of complex, multi-modal action trajectories. However, they are computationally expensive because they sample a trajectory of trajectories: a diffusion$/$flow trajectory of action trajectories. They discard intermediate action trajectories, and must wait for the sampling process to complete before any actions can be executed on the robot. We simplify diffusion$/$flow policies by treating action trajectories as flow trajectories. Instead of starting from pure noise, our algorithm samples from a narrow Gaussian around the last action. Then, it incrementally integrates a velocity field learned via flow matching to produce a sequence of actions that constitute a single trajectory. This enables actions to be streamed to the robot on-the-fly during the flow sampling process, and is well-suited for receding horizon policy execution. Despite streaming, our method retains the ability to model multi-modal behavior. We train flows that stabilize around demonstration trajectories to reduce distribution shift and improve imitation learning performance. Streaming flow policy outperforms prior methods while enabling faster policy execution and tighter sensorimotor loops for learning-based robot control. Project website: this https URL 

**Abstract (ZH)**: 近年来，扩散/流匹配策略的进步使模仿学习复杂、多模态动作轨迹成为可能。然而，这些方法因需要采样轨迹的轨迹（扩散/流轨迹的轨迹）而计算成本高。它们会丢弃中间的轨迹，并且必须等待采样过程完成才能执行任何动作。我们通过将动作轨迹视为流轨迹简化扩散/流策略。我们的算法从上次动作附近的一个窄高斯分布中采样，然后逐步整合通过流匹配学习得到的速度场，生成一个序列的动作，构成了单个轨迹。这使得在流采样过程中可以直接将动作流式传输给机器人，并且适用于倒退展望策略执行。尽管如此，该方法仍保持了建模多模态行为的能力。我们训练流使其围绕演示轨迹稳定下来，以减少分布偏移并提高模仿学习性能。流策略不仅在性能上优于先前方法，还允许更快的策略执行和更紧密的感知动作环路，从而提高基于学习的机器人控制。项目网站：这个 https URL 

---
# Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task 

**Title (ZH)**: 超越感知：通过多阶段任务评估抽象视觉推理能力 

**Authors**: Yanbei Jiang, Yihao Ding, Chao Lei, Jiayang Ao, Jey Han Lau, Krista A. Ehinger  

**Link**: [PDF](https://arxiv.org/pdf/2505.21850)  

**Abstract**: Current Multimodal Large Language Models (MLLMs) excel in general visual reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which demands higher-order reasoning to identify abstract rules beyond simple perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing the end result but neglecting the multi-stage nature of reasoning process. Past studies found MLLMs struggle with these benchmarks, but it doesn't explain how they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR benchmark, based on RAVEN, designed to assess reasoning across varying levels of complexity. Additionally, existing metrics like accuracy only focus on the final outcomes while do not account for the correctness of intermediate steps. Therefore, we propose a novel metric, MSEval, which considers the correctness of intermediate steps in addition to the final outcomes. We conduct comprehensive experiments on MultiStAR using 17 representative close-source and open-source MLLMs. The results reveal that while existing MLLMs perform adequately on basic perception tasks, they continue to face challenges in more complex rule detection stages. 

**Abstract (ZH)**: 当前的多模态大语言模型在一般视觉推理方面表现出色，但在抽象视觉推理（AVR）方面仍处于探索阶段，而抽象视觉推理要求更高阶的推理能力，以识别超出简单感知的抽象规则。现有的AVR基准主要关注单步推理，强调最终结果，而忽视了推理过程的多阶段性质。以往的研究发现MLLMs在这些基准上表现不佳，但并未解释其失败的原因。为解决这一问题，我们引入了MultiStAR，这是一种基于RAVEN的多阶段AVR基准，旨在评估不同复杂程度下的推理能力。此外，现有的评估指标如准确率仅关注最终结果，而不考虑中间步骤的准确性。因此，我们提出了一种新的评估指标MSEval，该指标不仅考虑最终结果，还考虑中间步骤的正确性。我们在MultiStAR上使用了17种代表性的闭源和开源MLLMs进行了全面实验。结果表明，尽管现有MLLMs在基本感知任务上表现良好，但在更复杂的规则检测阶段仍面临挑战。 

---
# Xinyu AI Search: Enhanced Relevance and Comprehensive Results with Rich Answer Presentations 

**Title (ZH)**: 新宇AI搜索：丰富的答案呈现提升相关性和全面性结果 

**Authors**: Bo Tang, Junyi Zhu, Chenyang Xi, Yunhang Ge, Jiahao Wu, Yuchen Feng, Yijun Niu, Wenqiang Wei, Yu Yu, Chunyu Li, Zehao Lin, Hao Wu, Ning Liao, Yebin Yang, Jiajia Wang, Zhiyu Li, Feiyu Xiong, Jingrun Chen  

**Link**: [PDF](https://arxiv.org/pdf/2505.21849)  

**Abstract**: Traditional search engines struggle to synthesize fragmented information for complex queries, while generative AI search engines face challenges in relevance, comprehensiveness, and presentation. To address these limitations, we introduce Xinyu AI Search, a novel system that incorporates a query-decomposition graph to dynamically break down complex queries into sub-queries, enabling stepwise retrieval and generation. Our retrieval pipeline enhances diversity through multi-source aggregation and query expansion, while filtering and re-ranking strategies optimize passage relevance. Additionally, Xinyu AI Search introduces a novel approach for fine-grained, precise built-in citation and innovates in result presentation by integrating timeline visualization and textual-visual choreography. Evaluated on recent real-world queries, Xinyu AI Search outperforms eight existing technologies in human assessments, excelling in relevance, comprehensiveness, and insightfulness. Ablation studies validate the necessity of its key sub-modules. Our work presents the first comprehensive framework for generative AI search engines, bridging retrieval, generation, and user-centric presentation. 

**Abstract (ZH)**: 传统搜索引擎在处理复杂查询时难以综合碎片化信息，而生成式AI搜索引擎在相关性、全面性和呈现方式上面临挑战。为解决这些局限性，我们引入了Xinyu AI搜索系统，该系统采用查询分解图自适应地将复杂查询拆解为子查询，实现逐步检索和生成。我们的检索管道通过多源聚合和查询扩展增强多样性，而过滤和二次排序策略优化段落的相关性。此外，Xinyu AI搜索系统提出了一种精细内置引文的新方法，并通过整合时间线可视化和文本-视觉编排创新了结果呈现方式。在对近期实际查询的评估中，Xinyu AI搜索系统在人类评估中优于八项现有技术，在相关性、全面性和洞察力方面表现出色。消融研究验证了其关键子模块的必要性。我们的工作提出了生成式AI搜索引擎的第一个综合框架，涵盖检索、生成和用户为中心的呈现。 

---
# RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers 

**Title (ZH)**: RePaViT: 通过前向网络层结构重参数化实现可扩展的视觉变压器加速 

**Authors**: Xuwei Xu, Yang Li, Yudong Chen, Jiajun Liu, Sen Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21847)  

**Abstract**: We reveal that feedforward network (FFN) layers, rather than attention layers, are the primary contributors to Vision Transformer (ViT) inference latency, with their impact signifying as model size increases. This finding highlights a critical opportunity for optimizing the efficiency of large-scale ViTs by focusing on FFN layers. In this work, we propose a novel channel idle mechanism that facilitates post-training structural reparameterization for efficient FFN layers during testing. Specifically, a set of feature channels remains idle and bypasses the nonlinear activation function in each FFN layer, thereby forming a linear pathway that enables structural reparameterization during inference. This mechanism results in a family of ReParameterizable Vision Transformers (RePaViTs), which achieve remarkable latency reductions with acceptable sacrifices (sometimes gains) in accuracy across various ViTs. The benefits of our method scale consistently with model sizes, demonstrating greater speed improvements and progressively narrowing accuracy gaps or even higher accuracies on larger models. In particular, RePa-ViT-Large and RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1 accuracies under the same training strategy, respectively. RePaViT is the first to employ structural reparameterization on FFN layers to expedite ViTs to our best knowledge, and we believe that it represents an auspicious direction for efficient ViTs. Source code is available at this https URL. 

**Abstract (ZH)**: 我们揭示了前馈网络（FFN）层而非注意力层是决定 Vision Transformer（ViT）推理延迟的主要因素，其影响随模型规模增加而愈加显著。这一发现突显了通过专注于优化 FFN 层来优化大规模 ViT 效率的关键机会。在本文中，我们提出了一种新型的通道闲置机制，该机制在测试期间促进了高效 FFN 层的结构重参数化。具体而言，在每个 FFN 层中有一组特征通道保持闲置并绕过非线性激活函数，从而形成一条线性路径，能够在推理期间进行结构重参数化。该机制导致了一类重参数化 Vision Transformer（RePaViTs），它们在不同 ViT 中实现了显著的延迟减少，同时在准确率方面有所折中（有时有所提高）。我们的方法的好处随着模型规模的增加而一致地增强，显示出更大的加速性能，并且在更大模型中逐渐缩小甚至超越了准确性差距。特别是，RePa-ViT-Large 和 RePa-ViT-Huge 分别在相同训练策略下获得了 66.8% 和 68.7% 的加速，同时 top-1 准确率分别提高了 1.7% 和 1.1%。据我们所知，RePaViT 是首次在 FFN 层上采用结构重参数化来加速 ViTs 的方法，我们认为这代表了高效 ViTs 的前景良好的方向。源代码可在以下网址获取。 

---
# An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints 

**Title (ZH)**: 基于乐观主义的在线CMDPS算法，带有时响应 adversarial 约束 

**Authors**: Jiahui Zhu, Kihyun Yu, Dabeen Lee, Xin Liu, Honghao Wei  

**Link**: [PDF](https://arxiv.org/pdf/2505.21841)  

**Abstract**: Online safe reinforcement learning (RL) plays a key role in dynamic environments, with applications in autonomous driving, robotics, and cybersecurity. The objective is to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). Existing methods achieve sublinear regret under stochastic constraints but often fail in adversarial settings, where constraints are unknown, time-varying, and potentially adversarially designed. In this paper, we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the first to address online CMDPs with anytime adversarial constraints. OMDPD achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K)) without relying on Slater's condition or the existence of a strictly known safe policy. We further show that access to accurate estimates of rewards and transitions can further improve these bounds. Our results offer practical guarantees for safe decision-making in adversarial environments. 

**Abstract (ZH)**: 在线安全强化学习（RL）在动态环境中的作用及其在自主驾驶、机器人技术和网络安全中的应用：面向任意时机动态约束的乐观镜像下降对偶算法（OMDPD）及其在 adversarial 设置下的性能分析 

---
# Nonadaptive Output Regulation of Second-Order Nonlinear Uncertain Systems 

**Title (ZH)**: 非自适应输出调节的第二-order非线性不确定系统 

**Authors**: Maobin Lu, Martin Guay, Telema Harry, Shimin Wang, Jordan Cooper  

**Link**: [PDF](https://arxiv.org/pdf/2505.21838)  

**Abstract**: This paper investigates the robust output regulation problem of second-order nonlinear uncertain systems with an unknown exosystem. Instead of the adaptive control approach, this paper resorts to a robust control methodology to solve the problem and thus avoid the bursting phenomenon. In particular, this paper constructs generic internal models for the steady-state state and input variables of the system. By introducing a coordinate transformation, this paper converts the robust output regulation problem into a nonadaptive stabilization problem of an augmented system composed of the second-order nonlinear uncertain system and the generic internal models. Then, we design the stabilization control law and construct a strict Lyapunov function that guarantees the robustness with respect to unmodeled disturbances. The analysis shows that the output zeroing manifold of the augmented system can be made attractive by the proposed nonadaptive control law, which solves the robust output regulation problem. Finally, we demonstrate the effectiveness of the proposed nonadaptive internal model approach by its application to the control of the Duffing system. 

**Abstract (ZH)**: 本文研究了带有未知外系统输入的二阶非线性不确定系统的鲁棒输出调节问题。本文采用鲁棒控制方法而非自适应控制方法来解决这一问题，从而避免了迸发现象。特别地，本文为系统的稳态状态变量和输入变量构建了通用内部模型。通过引入坐标变换，本文将鲁棒输出调节问题转化为包含二阶非线性不确定系统和通用内部模型的扩充系统的非自适应稳定化问题。然后，我们设计了稳定控制律，并构造了一个确保在未建模干扰下鲁棒性的严格李亚普诺夫函数。分析表明，通过所提出的非自适应控制律，可以使得扩充系统的输出零化流形变得有吸引力，从而解决了鲁棒输出调节问题。最后，通过将其应用于Duffing系统的控制中，证明了所提出的非自适应内部模型方法的有效性。 

---
# TuneComp: Joint Fine-tuning and Compression for Large Foundation Models 

**Title (ZH)**: TuneComp：大型基础模型的同时微调与压缩 

**Authors**: Xiangyu Chen, Jing Liu, Ye Wang, Matthew Brand, Wang, Toshiaki Koike-Akino  

**Link**: [PDF](https://arxiv.org/pdf/2505.21835)  

**Abstract**: To reduce model size during post-training, compression methods, including knowledge distillation, low-rank approximation, and pruning, are often applied after fine-tuning the model. However, sequential fine-tuning and compression sacrifices performance, while creating a larger than necessary model as an intermediate step. In this work, we aim to reduce this gap, by directly constructing a smaller model while guided by the downstream task. We propose to jointly fine-tune and compress the model by gradually distilling it to a pruned low-rank structure. Experiments demonstrate that joint fine-tuning and compression significantly outperforms other sequential compression methods. 

**Abstract (ZH)**: 在下游任务引导下直接构建和精调压缩模型以减少性能差距 

---
# Music Source Restoration 

**Title (ZH)**: 音乐源恢复 

**Authors**: Yongyi Zang, Zheqi Dai, Mark D. Plumbley, Qiuqiang Kong  

**Link**: [PDF](https://arxiv.org/pdf/2505.21827)  

**Abstract**: We introduce Music Source Restoration (MSR), a novel task addressing the gap between idealized source separation and real-world music production. Current Music Source Separation (MSS) approaches assume mixtures are simple sums of sources, ignoring signal degradations employed during music production like equalization, compression, and reverb. MSR models mixtures as degraded sums of individually degraded sources, with the goal of recovering original, undegraded signals. Due to the lack of data for MSR, we present RawStems, a dataset annotation of 578 songs with unprocessed source signals organized into 8 primary and 17 secondary instrument groups, totaling 354.13 hours. To the best of our knowledge, RawStems is the first dataset that contains unprocessed music stems with hierarchical categories. We consider spectral filtering, dynamic range compression, harmonic distortion, reverb and lossy codec as possible degradations, and establish U-Former as a baseline method, demonstrating the feasibility of MSR on our dataset. We release the RawStems dataset annotations, degradation simulation pipeline, training code and pre-trained models to be publicly available. 

**Abstract (ZH)**: 音乐源恢复（MSR）：介于理想化的声源分离和实际音乐制作之间的新型任务 

---
# Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones 

**Title (ZH)**: 让我思考！一条长链思考可能值得多条短思考指数般的价值。 

**Authors**: Parsa Mirtaheri, Ezra Edelman, Samy Jelassi, Eran Malach, Enric Boix-Adsera  

**Link**: [PDF](https://arxiv.org/pdf/2505.21825)  

**Abstract**: Inference-time computation has emerged as a promising scaling axis for improving large language model reasoning. However, despite yielding impressive performance, the optimal allocation of inference-time computation remains poorly understood. A central question is whether to prioritize sequential scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority voting across multiple short chains of thought). In this work, we seek to illuminate the landscape of test-time scaling by demonstrating the existence of reasoning settings where sequential scaling offers an exponential advantage over parallel scaling. These settings are based on graph connectivity problems in challenging distributions of graphs. We validate our theoretical findings with comprehensive experiments across a range of language models, including models trained from scratch for graph connectivity with different chain of thought strategies as well as large reasoning models. 

**Abstract (ZH)**: 推理时计算资源的最优分配对于大型语言模型推理能力的提升 emerged as一个有前景的扩展维度，然而，尽管表现出色，其最优分配仍然知之甚少。核心问题是侧重于顺序扩展（例如，更长的思维链）还是并行扩展（例如，多条较短的思维链的多数投票）。在本工作中，我们通过展示在具有挑战性的图形连接问题分布下存在顺序扩展相较于并行扩展具有指数级优势的推理设置，来阐明测试时扩展的景观。我们通过覆盖多种语言模型的全面实验验证了理论发现，包括从头训练用于图形连接的不同思维链策略的模型以及大型推理模型。 

---
# Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking 

**Title (ZH)**: 基于语义引导的LLM辅助科学论文检索 

**Authors**: Yunyi Zhang, Ruozhen Yang, Siqi Jiao, SeongKu Kang, Jiawei Han  

**Link**: [PDF](https://arxiv.org/pdf/2505.21815)  

**Abstract**: Scientific paper retrieval is essential for supporting literature discovery and research. While dense retrieval methods demonstrate effectiveness in general-purpose tasks, they often fail to capture fine-grained scientific concepts that are essential for accurate understanding of scientific queries. Recent studies also use large language models (LLMs) for query understanding; however, these methods often lack grounding in corpus-specific knowledge and may generate unreliable or unfaithful content. To overcome these limitations, we propose SemRank, an effective and efficient paper retrieval framework that combines LLM-guided query understanding with a concept-based semantic index. Each paper is indexed using multi-granular scientific concepts, including general research topics and detailed key phrases. At query time, an LLM identifies core concepts derived from the corpus to explicitly capture the query's information need. These identified concepts enable precise semantic matching, significantly enhancing retrieval accuracy. Experiments show that SemRank consistently improves the performance of various base retrievers, surpasses strong existing LLM-based baselines, and remains highly efficient. 

**Abstract (ZH)**: 科学论文检索对于支持文献发现和研究至关重要。虽然密集检索方法在通用任务中表现出有效性，但往往无法捕捉到准确理解科学查询所必需的细粒度科学概念。最近的研究也使用大型语言模型（LLMs）来进行查询理解；然而，这些方法往往缺乏与语料库特定知识的结合，可能会生成不可靠或不忠实的内容。为了克服这些限制，我们提出SemRank，这是一种有效的高效论文检索框架，结合了LLM引导的查询理解和基于概念的语义索引。每篇论文使用多粒度的科学概念进行索引，包括一般的研究主题和详细的关键词。在查询时，LLM识别来自语料库的核心概念，以明确捕捉查询的信息需求。这些识别出的概念使精确的语义匹配成为可能，显著提高了检索准确性。实验结果显示，SemRank在各种基线检索系统上表现出持续的性能提升，超越了强大的现有基于LLM的基线，并且保持了高效率。 

---
# Revisiting Self-attention for Cross-domain Sequential Recommendation 

**Title (ZH)**: 重新审视跨领域序列推荐中的自注意力机制 

**Authors**: Clark Mingxuan Ju, Leonardo Neves, Bhuvesh Kumar, Liam Collins, Tong Zhao, Yuwei Qiu, Qing Dou, Sohail Nizam, Sen Yang, Neil Shah  

**Link**: [PDF](https://arxiv.org/pdf/2505.21811)  

**Abstract**: Sequential recommendation is a popular paradigm in modern recommender systems. In particular, one challenging problem in this space is cross-domain sequential recommendation (CDSR), which aims to predict future behaviors given user interactions across multiple domains. Existing CDSR frameworks are mostly built on the self-attention transformer and seek to improve by explicitly injecting additional domain-specific components (e.g. domain-aware module blocks). While these additional components help, we argue they overlook the core self-attention module already present in the transformer, a naturally powerful tool to learn correlations among behaviors. In this work, we aim to improve the CDSR performance for simple models from a novel perspective of enhancing the self-attention. Specifically, we introduce a Pareto-optimal self-attention and formulate the cross-domain learning as a multi-objective problem, where we optimize the recommendation task while dynamically minimizing the cross-domain attention scores. Our approach automates knowledge transfer in CDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also encourages complementary knowledge exchange among auxiliary domains. Based on the idea, we further introduce AutoCDSR+, a more performant variant with slight additional cost. Our proposal is easy to implement and works as a plug-and-play module that can be incorporated into existing transformer-based recommenders. Besides flexibility, it is practical to deploy because it brings little extra computational overheads without heavy hyper-parameter tuning. AutoCDSR on average improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and NDCG@10 by 12.0% and 16.7%, respectively. Code is available at this https URL. 

**Abstract (ZH)**: 基于自注意力的帕累托最优跨域序贯推荐（AutoCDSR） 

---
# Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms 

**Title (ZH)**: 多模态联邦学习：不同联邦学习范式的综述 

**Authors**: Yuanzhe Peng, Jieming Bian, Lei Wang, Yin Huang, Jie Xu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21792)  

**Abstract**: Multimodal Federated Learning (MFL) lies at the intersection of two pivotal research areas: leveraging complementary information from multiple modalities to improve downstream inference performance and enabling distributed training to enhance efficiency and preserve privacy. Despite the growing interest in MFL, there is currently no comprehensive taxonomy that organizes MFL through the lens of different Federated Learning (FL) paradigms. This perspective is important because multimodal data introduces distinct challenges across various FL settings. These challenges, including modality heterogeneity, privacy heterogeneity, and communication inefficiency, are fundamentally different from those encountered in traditional unimodal or non-FL scenarios. In this paper, we systematically examine MFL within the context of three major FL paradigms: horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we present the problem formulation, review representative training algorithms, and highlight the most prominent challenge introduced by multimodal data in distributed settings. We also discuss open challenges and provide insights for future research. By establishing this taxonomy, we aim to uncover the novel challenges posed by multimodal data from the perspective of different FL paradigms and to offer a new lens through which to understand and advance the development of MFL. 

**Abstract (ZH)**: 多模态联邦学习（MFL）位于两个关键研究领域交汇处：利用多种模态的互补信息以提高下游推理性能，以及实现分布式训练以提高效率并保护隐私。尽管MFL备受关注，但目前尚无从不同联邦学习（FL） paradigms 看待的综合分类法。从不同FL范式角度审视MFL是重要的，因为多模态数据在各种FL设置中带来了独特的挑战。这些挑战包括模态异质性、隐私异质性和通信低效性，这些都是传统单模态或非FL情景中未遇到的不同问题。在本文中，我们系统地在三种主要FL范式——水平联邦学习（HFL）、垂直联邦学习（VFL）和混合联邦学习（Hybrid FL）——的背景下考察MFL。对于每种范式，我们提出问题形式、回顾代表性训练算法，并强调多模态数据在分布式设置中引入的最突出挑战。我们还讨论了开放性挑战并提供了未来研究的见解。通过建立这一分类法，我们旨在从不同FL范式的角度揭示多模态数据带来的新颖挑战，并提供理解和发展MFL的新视角。 

---
# VeriTrail: Closed-Domain Hallucination Detection with Traceability 

**Title (ZH)**: VeriTrail: 限域内的幻觉检测与可追溯性 

**Authors**: Dasha Metropolitansky, Jonathan Larson  

**Link**: [PDF](https://arxiv.org/pdf/2505.21786)  

**Abstract**: Even when instructed to adhere to source material, Language Models often generate unsubstantiated content - a phenomenon known as "closed-domain hallucination." This risk is amplified in processes with multiple generative steps (MGS), compared to processes with a single generative step (SGS). However, due to the greater complexity of MGS processes, we argue that detecting hallucinations in their final outputs is necessary but not sufficient: it is equally important to trace where hallucinated content was likely introduced and how faithful content may have been derived from the source through intermediate outputs. To address this need, we present VeriTrail, the first closed-domain hallucination detection method designed to provide traceability for both MGS and SGS processes. We also introduce the first datasets to include all intermediate outputs as well as human annotations of final outputs' faithfulness for their respective MGS processes. We demonstrate that VeriTrail outperforms baseline methods on both datasets. 

**Abstract (ZH)**: 语言模型即使受到源材料的限制，也常常生成缺乏依据的内容——这一现象称为“闭域幻觉”。在多生成步骤过程中，这种风险相较于单一生成步骤过程被放大。然而，由于多生成步骤过程的复杂性更高，我们argue检测它们最终输出中的幻觉是必要的，但远不足够：同样重要的是追踪幻觉内容可能被引入的位置，并通过中间输出评估源材料如何忠实转化为内容。为应对这一需求，我们提出了VeriTrail，这是首个旨在为多生成步骤过程和单一生成步骤过程提供追踪性的闭域幻觉检测方法。我们还引入了首个包含所有中间输出及最终输出忠实性的人工标注的数据集。实验表明，VeriTrail在两个数据集上均优于基线方法。 

---
# DualSchool: How Reliable are LLMs for Optimization Education? 

**Title (ZH)**: 双校模型：大型语言模型在优化教育中的可靠性如何？ 

**Authors**: Michael Klamkin, Arnaud Deza, Sikai Cheng, Haoruo Zhao, Pascal Van Hentenryck  

**Link**: [PDF](https://arxiv.org/pdf/2505.21775)  

**Abstract**: Consider the following task taught in introductory optimization courses which addresses challenges articulated by the community at the intersection of (generative) AI and OR: generate the dual of a linear program. LLMs, being trained at web-scale, have the conversion process and many instances of Primal to Dual Conversion (P2DC) at their disposal. Students may thus reasonably expect that LLMs would perform well on the P2DC task. To assess this expectation, this paper introduces DualSchool, a comprehensive framework for generating and verifying P2DC instances. The verification procedure of DualSchool uses the Canonical Graph Edit Distance, going well beyond existing evaluation methods for optimization models, which exhibit many false positives and negatives when applied to P2DC. Experiments performed by DualSchool reveal interesting findings. Although LLMs can recite the conversion procedure accurately, state-of-the-art open LLMs fail to consistently produce correct duals. This finding holds even for the smallest two-variable instances and for derivative tasks, such as correctness, verification, and error classification. The paper also discusses the implications for educators, students, and the development of large reasoning systems. 

**Abstract (ZH)**: 考虑在运筹学入门课程中教授的一项任务，该任务关注了生成式AI与运筹学交叉领域社区提出的挑战：生成线性规划的对偶。由于大规模网络训练，生成性预训练模型（LLMs）具备转换过程和多种原变量到对偶变量转换（P2DC）实例。学生有理由预期LLMs在P2DC任务上表现良好。为评估这一预期，本文介绍了一种全面的框架——DualSchool，用于生成和验证P2DC实例。DualSchool的验证程序采用Canonical Graph Edit Distance，超越了现有针对优化模型的评估方法，后者在应用于P2DC时存在大量误报和漏报。DualSchool进行的实验揭示了有趣的结果，尽管LLMs能够准确地背诵转换过程，但最先进的开放式LLMs未能一致地生成正确的对偶变量。这一发现即使在最小的两个变量实例和验证、错误分类等衍生任务中也成立。本文还讨论了这些发现对教育者、学生以及大型推理系统开发的含义。 

---
# MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning 

**Title (ZH)**: MMTBENCH：统一的复杂多模态表推理基准测试 

**Authors**: Prasham Yatinkumar Titiya, Jainil Trivedi, Chitta Baral, Vivek Gupta  

**Link**: [PDF](https://arxiv.org/pdf/2505.21771)  

**Abstract**: Multimodal tables those that integrate semi structured data with visual elements such as charts and maps are ubiquitous across real world domains, yet they pose a formidable challenge to current vision language models (VLMs). While Large Language models (LLMs) and VLMs have demonstrated strong capabilities in text and image understanding, their performance on complex, real world multimodal table reasoning remains unexplored. To bridge this gap, we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of 500 real world multimodal tables drawn from diverse real world sources, with a total of 4021 question answer pairs. MMTBENCH questions cover four question types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning types (Mathematical, Extrema Identification, Fact Verification, Vision Based, and Others), and eight table types (Single/Multiple Entity, Maps and Charts with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive evaluation of state of the art models on all types reveals substantial performance gaps, particularly on questions requiring visual-based reasoning and multi-step inference. These findings show the urgent need for improved architectures that more tightly integrate vision and language processing. By providing a challenging, high-quality resource that mirrors the complexity of real-world tasks, MMTBENCH underscores its value as a resource for future research on multimodal tables. 

**Abstract (ZH)**: 多模态表格基准：包含了来自多种真实世界来源的500个真实世界多模态表格，共计4021个问答对，包含四种问题类型、五种推理类型和八种表格类型。通过对最新模型的全面评估发现，在需要视觉推理和多步推理的问题上存在显著性能差距。这些发现表明，急需改进更紧密集成视觉和语言处理的架构。通过提供一个具有挑战性和高质量的资源，MMTBENCH突显了其作为未来研究多模态表格资源的价值。 

---
# FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering 

**Title (ZH)**: FRAMES-VQA：跨多模态变化的视觉问答 fine-tuning 稳定性基准评估 

**Authors**: Chengyue Huang, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira  

**Link**: [PDF](https://arxiv.org/pdf/2505.21755)  

**Abstract**: Visual question answering (VQA) systems face significant challenges when adapting to real-world data shifts, especially in multi-modal contexts. While robust fine-tuning strategies are essential for maintaining performance across in-distribution (ID) and out-of-distribution (OOD) scenarios, current evaluation settings are primarily unimodal or particular to some types of OOD, offering limited insight into the complexities of multi-modal contexts. In this work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA and others, and categorize them into ID, near and far OOD datasets covering uni-modal, multi-modal and adversarial distribution shifts. We first conduct a comprehensive comparison of existing robust fine-tuning methods. We then quantify the distribution shifts by calculating the Mahalanobis distance using uni-modal and multi-modal embeddings extracted from various models. Further, we perform an extensive analysis to explore the interactions between uni- and multi-modal shifts as well as modality importance for ID and OOD samples. These analyses offer valuable guidance on developing more robust fine-tuning methods to handle multi-modal distribution shifts. The code is available at this https URL . 

**Abstract (ZH)**: 跨模态视觉问答(VQA)系统在适应实际数据变换时面临重大挑战，特别是在多模态背景下。虽然强大的微调策略对于在分布内(ID)和分布外(OOD)场景下维持性能至关重要，但当前的评估设置主要为单模态或特定类型的OOD，提供的多模态背景下的复杂性洞察有限。在本工作中，我们提出了一种新的基准FRAMES-VQA（跨模态变换下的VQA稳健微调），用于评估VQA任务的稳健微调。我们利用十个现有的VQA基准，包括VQAv2、IV-VQA、VQA-CP、OK-VQA等，并将它们分类为分布内、近分布外和远分布外数据集，覆盖单模态、多模态和对抗分布变换。我们首先进行现有稳健微调方法的全面比较。然后，通过计算来自不同模型的一模态和多模态嵌入的马哈拉诺比斯距离，定量评估分布变换。进一步地，我们进行了广泛的分析以探索一模态和多模态变换及其模态重要性之间的相互作用，特别是对于分布内和分布外样本。这些分析为开发处理多模态分布变换的更稳健微调方法提供了宝贵指导。代码可在以下链接获取。 

---
# Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture 

**Title (ZH)**: 学习看到更多：基于无人机引导的卫星图像超分辨率技术在精准农业中的应用 

**Authors**: Arif Masrur, Peder A. Olsen, Paul R. Adler, Carlan Jackson, Matthew W. Myers, Nathan Sedghi, Ray R. Weil  

**Link**: [PDF](https://arxiv.org/pdf/2505.21746)  

**Abstract**: Unmanned Aircraft Systems (UAS) and satellites are key data sources for precision agriculture, yet each presents trade-offs. Satellite data offer broad spatial, temporal, and spectral coverage but lack the resolution needed for many precision farming applications, while UAS provide high spatial detail but are limited by coverage and cost, especially for hyperspectral data. This study presents a novel framework that fuses satellite and UAS imagery using super-resolution methods. By integrating data across spatial, spectral, and temporal domains, we leverage the strengths of both platforms cost-effectively. We use estimation of cover crop biomass and nitrogen (N) as a case study to evaluate our approach. By spectrally extending UAS RGB data to the vegetation red edge and near-infrared regions, we generate high-resolution Sentinel-2 imagery and improve biomass and N estimation accuracy by 18% and 31%, respectively. Our results show that UAS data need only be collected from a subset of fields and time points. Farmers can then 1) enhance the spectral detail of UAS RGB imagery; 2) increase the spatial resolution by using satellite data; and 3) extend these enhancements spatially and across the growing season at the frequency of the satellite flights. Our SRCNN-based spectral extension model shows considerable promise for model transferability over other cropping systems in the Upper and Lower Chesapeake Bay regions. Additionally, it remains effective even when cloud-free satellite data are unavailable, relying solely on the UAS RGB input. The spatial extension model produces better biomass and N predictions than models built on raw UAS RGB images. Once trained with targeted UAS RGB data, the spatial extension model allows farmers to stop repeated UAS flights. While we introduce super-resolution advances, the core contribution is a lightweight and scalable system for affordable on-farm use. 

**Abstract (ZH)**: 无人航空系统（UAS）和卫星在精准农业中的关键数据来源及其权衡：一种使用超分辨率方法融合UAS和卫星成像的新框架 

---
# Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen 

**Title (ZH)**: 模拟未见之事：碰撞预测需从未发生之事中学习 

**Authors**: Zihao Li, Xinyuan Cao, Xiangbo Gao, Kexin Tian, Keshu Wu, Mohammad Anis, Hao Zhang, Keke Long, Jiwan Jiang, Xiaopeng Li, Yunlong Zhang, Tianbao Yang, Dominique Lord, Zhengzhong Tu, Yang Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2505.21743)  

**Abstract**: Traffic safety science has long been hindered by a fundamental data paradox: the crashes we most wish to prevent are precisely those events we rarely observe. Existing crash-frequency models and surrogate safety metrics rely heavily on sparse, noisy, and under-reported records, while even sophisticated, high-fidelity simulations undersample the long-tailed situations that trigger catastrophic outcomes such as fatalities. We argue that the path to achieving Vision Zero, i.e., the complete elimination of traffic fatalities and severe injuries, requires a paradigm shift from traditional crash-only learning to a new form of counterfactual safety learning: reasoning not only about what happened, but also about the vast set of plausible yet perilous scenarios that could have happened under slightly different circumstances. To operationalize this shift, our proposed agenda bridges macro to micro. Guided by crash-rate priors, generative scene engines, diverse driver models, and causal learning, near-miss events are synthesized and explained. A crash-focused digital twin testbed links micro scenes to macro patterns, while a multi-objective validator ensures that simulations maintain statistical realism. This pipeline transforms sparse crash data into rich signals for crash prediction, enabling the stress-testing of vehicles, roads, and policies before deployment. By learning from crashes that almost happened, we can shift traffic safety from reactive forensics to proactive prevention, advancing Vision Zero. 

**Abstract (ZH)**: 交通安全性科学长期受制于一个根本性的数据悖论：我们最希望预防的事故恰恰是那些我们很少观察到的事件。现有的事故频率模型和替代安全指标严重依赖于稀疏、噪音大且报告不足的记录，而即使是最复杂的高保真模拟也无法捕捉触发灾难性结果（如死亡）的长尾情况。我们arg认为，实现零愿景，即完全消除交通死亡和严重伤害，需要从传统的只关注事故学习转向一种新的假设安全性学习：不仅要推理实际发生了什么，还要推理在略微不同的情况下可能发生的大量潜在但危险的场景。为了实现这一转变，我们提出的议程从宏观到微观。基于事故率先验知识，通过生成场景引擎、多样化驾驶员模型和因果学习，近错事件被合成和解释。以事故为重点的数字孪生测试平台将微观场景链接到宏观模式，而多目标验证器确保模拟保持统计现实性。这一管道将稀疏的事故数据转化为丰富的事故预测信号，能够在部署前对车辆、道路和政策进行压力测试。通过学习几乎发生的事故，我们可以从被动的事故分析转向主动的预防，推进零愿景的目标。 

---
# Counterfactual Simulatability of LLM Explanations for Generation Tasks 

**Title (ZH)**: LLM生成任务中解释的反事实可模拟性 

**Authors**: Marvin Limpijankit, Yanda Chen, Melanie Subbiah, Nicholas Deas, Kathleen McKeown  

**Link**: [PDF](https://arxiv.org/pdf/2505.21740)  

**Abstract**: LLMs can be unpredictable, as even slight alterations to the prompt can cause the output to change in unexpected ways. Thus, the ability of models to accurately explain their behavior is critical, especially in high-stakes settings. One approach for evaluating explanations is counterfactual simulatability, how well an explanation allows users to infer the model's output on related counterfactuals. Counterfactual simulatability has been previously studied for yes/no question answering tasks. We provide a general framework for extending this method to generation tasks, using news summarization and medical suggestion as example use cases. We find that while LLM explanations do enable users to better predict LLM outputs on counterfactuals in the summarization setting, there is significant room for improvement for medical suggestion. Furthermore, our results suggest that the evaluation for counterfactual simulatability may be more appropriate for skill-based tasks as opposed to knowledge-based tasks. 

**Abstract (ZH)**: LLMs的表现存在不确定性，即使微小的提示更改也可能导致输出方式出乎意料地改变。因此，模型准确解释其行为的能力至关重要，特别是在高风险环境中。评估解释的一种方法是反事实可模拟性，即解释如何帮助用户推理模型在相关反事实上的输出。反事实可模拟性之前已被研究应用于是/否问答任务。我们提供了一种通用框架，将此方法扩展到生成任务中，并使用新闻摘要和医疗建议作为示例应用场景。我们发现，在摘要设置中，LLM解释确实使用户更好地预测LLM在反事实上的输出，但在医疗建议方面仍有显著改进空间。此外，我们的结果表明，反事实可模拟性的评估可能更适合技能基于的任务，而不是知识基于的任务。 

---
# Deep Reinforcement Learning Agents are not even close to Human Intelligence 

**Title (ZH)**: 深层强化学习代理远未达到人类智能水平 

**Authors**: Quentin Delfosse, Jannis Blüml, Fabian Tatai, Théo Vincent, Bjarne Gregori, Elisabeth Dillies, Jan Peters, Constantin Rothkopf, Kristian Kersting  

**Link**: [PDF](https://arxiv.org/pdf/2505.21731)  

**Abstract**: Deep reinforcement learning (RL) agents achieve impressive results in a wide variety of tasks, but they lack zero-shot adaptation capabilities. While most robustness evaluations focus on tasks complexifications, for which human also struggle to maintain performances, no evaluation has been performed on tasks simplifications. To tackle this issue, we introduce HackAtari, a set of task variations of the Arcade Learning Environments. We use it to demonstrate that, contrary to humans, RL agents systematically exhibit huge performance drops on simpler versions of their training tasks, uncovering agents' consistent reliance on shortcuts. Our analysis across multiple algorithms and architectures highlights the persistent gap between RL agents and human behavioral intelligence, underscoring the need for new benchmarks and methodologies that enforce systematic generalization testing beyond static evaluation protocols. Training and testing in the same environment is not enough to obtain agents equipped with human-like intelligence. 

**Abstract (ZH)**: 深度强化学习（RL）代理在各种任务中取得了令人印象深刻的成果，但它们缺乏零样本适应能力。虽然大多数鲁棒性评估集中在人类也难以维持性能的任务复杂化上，但尚未对任务简化进行评估。为解决这一问题，我们引入了HackAtari，即 Arcade Learning Environments 的一系列任务变体。我们使用它来展示，与人类不同，RL代理在任务简化版本中系统地表现出巨大的性能下降，揭示了代理对捷径的一贯依赖。我们在多种算法和架构上的分析强调了RL代理与人类行为智能之间持续存在的差距，突出了需要新的基准和方法，以确保超出静态评估协议的系统泛化测试。在相同环境中训练和测试并不足以获得具备人类智能的代理。 

---
# OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions 

**Title (ZH)**: 全方位响应：双人互动中在线多模态对话响应生成 

**Authors**: Cheng Luo, Jianghui Wang, Bing Li, Siyang Song, Bernard Ghanem  

**Link**: [PDF](https://arxiv.org/pdf/2505.21724)  

**Abstract**: In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task that aims to online generate synchronized verbal and non-verbal listener feedback, conditioned on the speaker's multimodal input. OMCRG reflects natural dyadic interactions and poses new challenges in achieving synchronization between the generated audio and facial responses of the listener. To address these challenges, we innovatively introduce text as an intermediate modality to bridge the audio and facial responses. We hence propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates high-quality multi-modal listener responses. OmniResponse leverages a pretrained LLM enhanced with two novel components: Chrono-Text, which temporally anchors generated text tokens, and TempoVoice, a controllable online TTS module that produces speech synchronized with facial reactions. To support further OMCRG research, we present ResponseNet, a new dataset comprising 696 high-quality dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and facial behavior annotations. Comprehensive evaluations conducted on ResponseNet demonstrate that OmniResponse significantly outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality. 

**Abstract (ZH)**: 在线多模态对话响应生成（OMCRG）及其解决方法：OmniResponse模型的研究 

---
# Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape 

**Title (ZH)**: 深度ReLU网络中的鞍点到鞍点动力学：首次鞍点逃离中的低秩偏差 

**Authors**: Ioannis Bantzis, James B. Simon, Arthur Jacot  

**Link**: [PDF](https://arxiv.org/pdf/2505.21722)  

**Abstract**: When a deep ReLU network is initialized with small weights, GD is at first dominated by the saddle at the origin in parameter space. We study the so-called escape directions, which play a similar role as the eigenvectors of the Hessian for strict saddles. We show that the optimal escape direction features a low-rank bias in its deeper layers: the first singular value of the $\ell$-th layer weight matrix is at least $\ell^{\frac{1}{4}}$ larger than any other singular value. We also prove a number of related results about these escape directions. We argue that this result is a first step in proving Saddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of saddles with increasing bottleneck rank. 

**Abstract (ZH)**: 当深度ReLU网络用小权重初始化时，GD起初被参数空间原点鞍点支配。我们研究所谓的逃逸方向，其在严格鞍点中扮演类似海森矩阵特征向量的角色。我们证明了最优逃逸方向在更深的层中具有低秩偏置：第$\ell$层权重矩阵的第一个奇异值至少比其他任何奇异值大$\ell^{\frac{1}{4}}$。我们还证明了这些逃逸方向的一些相关结果。我们认为这一结果是证明深度ReLU网络中的鞍点到鞍点动力学的第一步，其中GD访问具有递增瓶颈秩的一系列鞍点。 

---
# Responsible Data Stewardship: Generative AI and the Digital Waste Problem 

**Title (ZH)**: 负责任的数据 stewardship：生成式 AI 与数字废物问题 

**Authors**: Vanessa Utz  

**Link**: [PDF](https://arxiv.org/pdf/2505.21720)  

**Abstract**: As generative AI systems become widely adopted, they enable unprecedented creation levels of synthetic data across text, images, audio, and video modalities. While research has addressed the energy consumption of model training and inference, a critical sustainability challenge remains understudied: digital waste. This term refers to stored data that consumes resources without serving a specific (and/or immediate) purpose. This paper presents this terminology in the AI context and introduces digital waste as an ethical imperative within (generative) AI development, positioning environmental sustainability as core for responsible innovation. Drawing from established digital resource management approaches, we examine how other disciplines manage digital waste and identify transferable approaches for the AI community. We propose specific recommendations encompassing re-search directions, technical interventions, and cultural shifts to mitigate the environmental consequences of in-definite data storage. By expanding AI ethics beyond immediate concerns like bias and privacy to include inter-generational environmental justice, this work contributes to a more comprehensive ethical framework that considers the complete lifecycle impact of generative AI systems. 

**Abstract (ZH)**: 随着生成式AI系统的广泛应用，它们在文本、图像、音频和视频等多种模态中实现了前所未有的合成数据创造水平。尽管研究已关注模型训练和推理的能量消耗，数字浪费这一关键可持续性挑战仍研究不足。这一术语指的是占用资源但未服务于特定（或即时）目的的数据存储。本文在AI背景下引入这一术语，并将数字浪费纳入（生成式）AI开发的伦理规范，将环境可持续性置于负责任创新的核心地位。借鉴现有的数字资源管理方法，我们探讨了其他学科如何管理数字浪费，并为AI社区识别可转移的方法。我们提出了具体建议，涵盖研究方向、技术干预和文化转变，以减轻无限期数据存储的环境影响。通过将AI伦理扩展到代际环境正义等即刻关注之外，本工作为全面考虑生成式AI系统整个生命周期影响的伦理框架做出了贡献。 

---
# Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling 

**Title (ZH)**: 扩展液体抗性液体电容器网络以实现高效的序列建模 

**Authors**: Mónika Farsang, Ramin Hasani, Radu Grosu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21717)  

**Abstract**: We present LrcSSM, a \textit{nonlinear} recurrent model that processes long sequences as fast as today's linear state-space layers. By forcing the state-transition matrix to be diagonal and learned at every step, the full sequence can be solved in parallel with a single prefix-scan, giving $\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential depth, for input-sequence length $T$ and a state dimension $D$. Moreover, LrcSSM offers a formal gradient-stability guarantee that other input-varying systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth $L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its low sequential depth and parameter count $\Theta(D\,L)$, the model follows the compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for Mamba, outperforming quadratic-attention Transformers at equal compute while avoiding the memory overhead of FFT-based long convolutions. We show that on a series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba. 

**Abstract (ZH)**: LrcSSM：一种非线性递归模型，处理长序列速度媲美当前线性状态空间层 

---
# Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2 

**Title (ZH)**: 基于ViT和GPT-2的多模态联邦学习的隐私保护胸片报告生成 

**Authors**: Md. Zahid Hossain, Mustofa Ahmed, Most. Sharmin Sultana Samu, Md. Rakibul Islam  

**Link**: [PDF](https://arxiv.org/pdf/2505.21715)  

**Abstract**: The automated generation of radiology reports from chest X-ray images holds significant promise in enhancing diagnostic workflows while preserving patient privacy. Traditional centralized approaches often require sensitive data transfer, posing privacy concerns. To address this, the study proposes a Multimodal Federated Learning framework for chest X-ray report generation using the IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the encoder and GPT-2 as the report generator, enabling decentralized training without sharing raw data. Three Federated Learning (FL) aggregation strategies: FedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg) were evaluated. Among these, Krum Aggregation demonstrated superior performance across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore and RaTEScore. The results show that FL can match or surpass centralized models in generating clinically relevant and semantically rich radiology reports. This lightweight and privacy-preserving framework paves the way for collaborative medical AI development without compromising data confidentiality. 

**Abstract (ZH)**: 基于胸片图像的自动化放射报告生成在提升诊断工作流程的同时保护患者隐私方面具有重要潜力。传统的集中式方法常常需要传输敏感数据，存在隐私风险。为此，研究提出了一种用于生成胸片报告的多模态联邦学习框架，利用IU-Xray数据集。该系统采用Vision Transformer (ViT) 作为编码器和GPT-2作为报告生成器，实现无需共享原始数据的去中心化训练。三种联邦学习（FL）聚合策略：FedAvg、Krum聚合和一种新颖的损失感知联邦平均（L-FedAvg）进行了评估。其中，Krum聚合在ROUGE、BLEU、BERTScore和RaTEScore等词汇和语义评估指标上表现出更优性能。结果显示，FL能够在生成临床相关且语义丰富的放射报告方面与集中式模型相匹配甚至超越。这一轻量级且保护隐私的框架为无需牺牲数据保密性的协作医学AI开发铺平了道路。 

---
# A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks 

**Title (ZH)**: 面向IoV网络未知攻击检测的联合重建-三重损失自动编码器方法 

**Authors**: Julia Boone, Tolunay Seyfi, Fatemeh Afghah  

**Link**: [PDF](https://arxiv.org/pdf/2505.21703)  

**Abstract**: Internet of Vehicles (IoV) systems, while offering significant advancements in transportation efficiency and safety, introduce substantial security vulnerabilities due to their highly interconnected nature. These dynamic systems produce massive amounts of data between vehicles, infrastructure, and cloud services and present a highly distributed framework with a wide attack surface. In considering network-centered attacks on IoV systems, attacks such as Denial-of-Service (DoS) can prohibit the communication of essential physical traffic safety information between system elements, illustrating that the security concerns for these systems go beyond the traditional confidentiality, integrity, and availability concerns of enterprise systems. Given the complexity and volume of data generated by IoV systems, traditional security mechanisms are often inadequate for accurately detecting sophisticated and evolving cyberattacks. Here, we present an unsupervised autoencoder method trained entirely on benign network data for the purpose of unseen attack detection in IoV networks. We leverage a weighted combination of reconstruction and triplet margin loss to guide the autoencoder training and develop a diverse representation of the benign training set. We conduct extensive experiments on recent network intrusion datasets from two different application domains, industrial IoT and home IoT, that represent the modern IoV task. We show that our method performs robustly for all unseen attack types, with roughly 99% accuracy on benign data and between 97% and 100% performance on anomaly data. We extend these results to show that our model is adaptable through the use of transfer learning, achieving similarly high results while leveraging domain features from one domain to another. 

**Abstract (ZH)**: 基于车辆的互联网（IoV）系统在提升交通效率和安全性方面取得了显著进展，但其高度互联的性质也带来了重大的安全漏洞。这些动态系统在车辆、基础设施和云服务之间产生了大量数据，并呈现出广泛分布的框架，攻击面非常广泛。在考虑针对IoV系统的网络中心化攻击时，如服务拒绝（DoS）攻击可以阻止系统元素之间的关键物理交通安全信息通信，这表明这些系统面临的安全问题超出了企业系统传统的机密性、完整性和可用性担忧。鉴于IoV系统生成的数据复杂性和规模，传统安全机制往往不足以准确检测复杂的和不断演变的网络攻击。本文提出了一种完全基于良性网络数据训练的无监督自编码器方法，用于IoV网络中未知攻击的检测。我们利用重构损失和三重边际损失的加权组合来指导自编码器训练，并开发了良性训练集的多样化表示。我们在来自两个不同应用领域的最新的网络入侵数据集——工业物联网和家庭物联网——上进行了广泛的实验，这些数据集代表了现代IoV任务。结果显示，我们的方法对所有未知攻击类型都表现 robust，对良性数据的准确率为约99%，对异常数据的性能在97%到100%之间。我们进一步表明，通过转移学习，我们的模型具有适应性，能够利用一个领域中的特征来实现类似高的结果。 

---
# STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction 

**Title (ZH)**: STA-Risk: 胸部癌变风险时空不对称性的深入探讨 

**Authors**: Zhengbo Zhou, Dooman Arefan, Margarita Zuley, Jules Sumkin, Shandong Wu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21699)  

**Abstract**: Predicting the risk of developing breast cancer is an important clinical tool to guide early intervention and tailoring personalized screening strategies. Early risk models have limited performance and recently machine learning-based analysis of mammogram images showed encouraging risk prediction effects. These models however are limited to the use of a single exam or tend to overlook nuanced breast tissue evolvement in spatial and temporal details of longitudinal imaging exams that are indicative of breast cancer risk. In this paper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk Prediction), a novel Transformer-based model that captures fine-grained mammographic imaging evolution simultaneously from bilateral and longitudinal asymmetries for breast cancer risk prediction. STA-Risk is innovative by the side encoding and temporal encoding to learn spatial-temporal asymmetries, regulated by a customized asymmetry loss. We performed extensive experiments with two independent mammogram datasets and achieved superior performance than four representative SOTA models for 1- to 5-year future risk prediction. Source codes will be released upon publishing of the paper. 

**Abstract (ZH)**: 基于空间和时间不对称性的Transformer风险预测模型(STA-Risk)：乳腺癌风险预测 

---
# LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model 

**Title (ZH)**: LLMPR：一种新型LLM驱动的转移学习请愿排名模型 

**Authors**: Avijit Gayen, Somyajit Chakraborty, Mainak Sen, Soham Paul, Angshuman Jana  

**Link**: [PDF](https://arxiv.org/pdf/2505.21689)  

**Abstract**: The persistent accumulation of unresolved legal cases, especially within the Indian judiciary, significantly hampers the timely delivery of justice. Manual methods of prioritizing petitions are often prone to inefficiencies and subjective biases further exacerbating delays. To address this issue, we propose LLMPR (Large Language Model-based Petition Ranking), an automated framework that utilizes transfer learning and machine learning to assign priority rankings to legal petitions based on their contextual urgency. Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process unstructured legal text and extract features through various embedding techniques, including DistilBERT, LegalBERT, and MiniLM. These textual embeddings are combined with quantitative indicators such as gap days, rank scores, and word counts to train multiple machine learning models, including Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments demonstrate that Random Forest and Decision Tree models yield superior performance, with accuracy exceeding 99% and a Spearman rank correlation of 0.99. Notably, models using only numerical features achieve nearly optimal ranking results (R2 = 0.988, \r{ho} = 0.998), while LLM-based embeddings offer only marginal gains. These findings suggest that automated petition ranking can effectively streamline judicial workflows, reduce case backlog, and improve fairness in legal prioritization. 

**Abstract (ZH)**: 基于大型语言模型的请愿排名（LLMPR）：自动化的请愿优先级分配框架 

---
# multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data 

**Title (ZH)**: 多变量GPT：一种适用于多变量类别和数值数据的解码器-only变压器模型 

**Authors**: Andrew J. Loza, Jun Yup Kim, Shangzheng Song, Yihang Liu, Joseph J. Y. Sung, R Andrew Taylor, Dennis L. Shung  

**Link**: [PDF](https://arxiv.org/pdf/2505.21680)  

**Abstract**: Real-world processes often generate data that are a mix of categorical and numeric values that are recorded at irregular and informative intervals. Discrete token-based approaches are limited in numeric representation capacity while methods like neural ordinary differential equations are not well suited for categorical data or informative sampling and require augmentation to handle certain classes of trajectories. Here, we present multivariateGPT, a single architecture for modeling sequences of mixed categorical (including tokenized text) and numeric data. This is accomplished with an autoregressive sequence decomposition, embedding scheme, and loss function that extend the next token prediction task to likelihood estimation of the joint distribution of next token class and value. We demonstrate how this approach can efficiently learn to generalize patterns in simple physical systems and model complex time series including electrocardiograms and multivariate electronic health record data. This work extends the utility of transformer based models to additional classes of data. 

**Abstract (ZH)**: 现实世界的过程中常常生成混合了类别型和数值型数据的记录，这些数据的记录间隔不规则且富有信息性。基于离散标记的方法在数值表示能力上有限，而神经常微分方程等方法不适用于类别型数据或信息型采样，且需要增强处理某些轨迹类别。为了解决这些问题，我们提出了multivariateGPT，这是一种能够 modeling 混合类别型（包括标记化文本）和数值型数据序列的单架构模型。这一模型通过自回归序列分解、嵌入方案和损失函数，将下一个标记预测任务扩展为下一个标记类别和值的联合分布似然估计。我们展示了这种方法如何高效地学习在简单物理系统中泛化模式，并建模复杂的时序数据，包括心电图和多变量电子健康记录数据。此工作将变压器基模型的应用扩展到了更多类别的数据。 

---
# What happens when generative AI models train recursively on each others' generated outputs? 

**Title (ZH)**: 当生成式AI模型递归地训练在彼此生成的输出上会发生什么？ 

**Authors**: Hung Ahn Vu, Galen Reeves, Emily Wenger  

**Link**: [PDF](https://arxiv.org/pdf/2505.21677)  

**Abstract**: The internet is full of AI-generated content while also serving as a common source of training data for generative AI (genAI) models. This duality raises the possibility that future genAI models may be trained on other models' generated outputs. Prior work has studied consequences of models training on their own generated outputs, but limited work has considered what happens if models ingest content produced by other models. Given society's increasing dependence on genAI tools, understanding downstream effects of such data-mediated model interactions is critical. To this end, we provide empirical evidence for how data-mediated interactions might unfold in practice, develop a theoretical model for this interactive training process, and show experimentally possible long-term results of such interactions. We find that data-mediated interactions can benefit models by exposing them to novel concepts perhaps missed in original training data, but also can homogenize their performance on shared tasks. 

**Abstract (ZH)**: 互联网上充斥着AI生成的内容，同时也是一个常见的训练数据来源，为生成AI（genAI）模型所用。这种二元性使得未来genAI模型可能被训练在其他模型生成的输出上。已有研究表明模型在训练时使用自己生成的输出的后果，但很少有人考虑模型摄入其他模型生成的内容会发生什么。鉴于社会越来越多地依赖于生成AI工具，理解这种数据中介的模型交互的下游效应至关重要。为此，我们提供了数据中介交互在实践中可能如何展开的实证证据，发展了一个理论模型来描述这一交互训练过程，并通过实验展示了此类交互可能带来的长期结果。我们发现，数据中介的交互可以通过使模型接触到原始训练数据中可能遗漏的新概念而得益，但也可能导致其在共享任务上的表现同质化。 

---
# Rethinking the Outlier Distribution in Large Language Models: An In-depth Study 

**Title (ZH)**: 重新审视大规模语言模型中的离群值分布：一项深入研究 

**Authors**: Rahul Raman, Khushi Sharma, Sai Qian Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21670)  

**Abstract**: Investigating outliers in large language models (LLMs) is crucial due to their significant impact on various aspects of LLM performance, including quantization and compression. Outliers often cause considerable quantization errors, leading to degraded model performance. Identifying and addressing these outliers can enhance the accuracy and efficiency of the quantization process, enabling smoother deployment on edge devices or specialized hardware. Recent studies have identified two common types of outliers in LLMs: massive activations and channel-wise outliers. While numerous quantization algorithms have been proposed to mitigate their effects and maintain satisfactory accuracy, few have thoroughly explored the root causes of these outliers in depth. In this paper, we conduct a comprehensive investigation into the formation mechanisms of these outliers and propose potential strategies to mitigate their occurrence. Ultimately, we introduce some efficient approaches to eliminate most massive activations and channel-wise outliers with minimal impact on accuracy. 

**Abstract (ZH)**: 探究大规模语言模型中的离群值对量化和压缩的影响及其治理方法 

---
# Efficient Controllable Diffusion via Optimal Classifier Guidance 

**Title (ZH)**: 高效可控扩散通过最优分类器引导 

**Authors**: Owen Oertell, Shikun Sun, Yiding Chen, Jin Peng Zhou, Zhiyong Wang, Wen Sun  

**Link**: [PDF](https://arxiv.org/pdf/2505.21666)  

**Abstract**: The controllable generation of diffusion models aims to steer the model to generate samples that optimize some given objective functions. It is desirable for a variety of applications including image generation, molecule generation, and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of the base model is a popular approach but it can overfit the reward function while requiring significant resources. We frame controllable generation as a problem of finding a distribution that optimizes a KL-regularized objective function. We present SLCD -- Supervised Learning based Controllable Diffusion, which iteratively generates online data and trains a small classifier to guide the generation of the diffusion model. Similar to the standard classifier-guided diffusion, SLCD's key computation primitive is classification and does not involve any complex concepts from RL or control. Via a reduction to no-regret online learning analysis, we show that under KL divergence, the output from SLCD provably converges to the optimal solution of the KL-regularized objective. Further, we empirically demonstrate that SLCD can generate high quality samples with nearly the same inference time as the base model in both image generation with continuous diffusion and biological sequence generation with discrete diffusion. Our code is available at this https URL 

**Abstract (ZH)**: 可控扩散模型的生成旨在引导模型生成优化某些给定目标函数的样本。其在图像生成、分子生成和DNA/序列生成等众多应用中具有重要价值。基于强化学习（RL）的基础模型微调是流行的方法，但可能会过拟合奖励函数并需要大量资源。我们将可控生成建模为寻找优化KL正则化目标函数的分布的问题。我们提出了基于监督学习的可控扩散（SLCD），它通过生成在线数据并训练一个小的分类器来引导扩散模型的生成。与标准的分类器引导扩散类似，SLCD 的关键计算原语是分类，而不涉及任何来自RL或控制的复杂概念。通过归约到无悔在线学习分析，我们证明，在KL散度下，SLCD 的输出可证明地收敛到KL正则化目标函数的最优解。进一步，我们在图像生成（使用连续扩散）和生物序列生成（使用离散扩散）中 empirical 证明了SLCD 可以生成高质量的样本，其推理时间几乎与基础模型相同。我们的代码在以下网址：this https URL 可用。 

---
# Expert Survey: AI Reliability & Security Research Priorities 

**Title (ZH)**: 专家调查：AI可靠性和安全性研究优先事项 

**Authors**: Joe O'Brien, Jeremy Dolan, Jay Kim, Jonah Dykhuizen, Jeba Sania, Sebastian Becker, Jam Kraprayoon, Cara Labrador  

**Link**: [PDF](https://arxiv.org/pdf/2505.21664)  

**Abstract**: Our survey of 53 specialists across 105 AI reliability and security research areas identifies the most promising research prospects to guide strategic AI R&D investment. As companies are seeking to develop AI systems with broadly human-level capabilities, research on reliability and security is urgently needed to ensure AI's benefits can be safely and broadly realized and prevent severe harms. This study is the first to quantify expert priorities across a comprehensive taxonomy of AI safety and security research directions and to produce a data-driven ranking of their potential impact. These rankings may support evidence-based decisions about how to effectively deploy resources toward AI reliability and security research. 

**Abstract (ZH)**: 我们对105个AI可靠性和安全研究领域的53位专家进行的调查，确定了最具前景的研究方向，以指导战略性AI研发投资。随着公司致力于开发具有广泛人类水平能力的AI系统，对可靠性和安全性方面的研究变得尤为迫切，以确保AI的益处能够安全且广泛地实现，并防止严重危害。本研究是首次对AI安全与安全性研究方向进行全面分类，并量化专家优先级，生产出基于数据的潜在影响排名。这些排名可能支持基于证据的决策，以有效部署资源用于AI可靠性和安全性研究。 

---
# Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations 

**Title (ZH)**: 使用SMILE实现大规模语言模型的可解释性：基于局部解释的统计模型无关解释性 

**Authors**: Zeinab Dehghani, Koorosh Aslansefat, Adil Khan, Mohammed Naveed Akram  

**Link**: [PDF](https://arxiv.org/pdf/2505.21657)  

**Abstract**: Large language models like GPT, LLAMA, and Claude have become incredibly powerful at generating text, but they are still black boxes, so it is hard to understand how they decide what to say. That lack of transparency can be problematic, especially in fields where trust and accountability matter. To help with this, we introduce SMILE, a new method that explains how these models respond to different parts of a prompt. SMILE is model-agnostic and works by slightly changing the input, measuring how the output changes, and then highlighting which words had the most impact. Create simple visual heat maps showing which parts of a prompt matter the most. We tested SMILE on several leading LLMs and used metrics such as accuracy, consistency, stability, and fidelity to show that it gives clear and reliable explanations. By making these models easier to understand, SMILE brings us one step closer to making AI more transparent and trustworthy. 

**Abstract (ZH)**: SMILE：一种解释大型语言模型响应的新方法 

---
# PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation 

**Title (ZH)**: Part级指令跟随：用于精细机器人操作的部件级指令遵循 

**Authors**: Yifan Yin, Zhengtao Han, Shivam Aarya, Jianxin Wang, Shuhang Xu, Jiawei Peng, Angtian Wang, Alan Yuille, Tianmin Shu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21652)  

**Abstract**: Fine-grained robot manipulation, such as lifting and rotating a bottle to display the label on the cap, requires robust reasoning about object parts and their relationships with intended tasks. Despite recent advances in training general-purpose robot manipulation policies guided by language instructions, there is a notable lack of large-scale datasets for fine-grained manipulation tasks with part-level instructions and diverse 3D object instances annotated with part-level labels. In this work, we introduce PartInstruct, the first large-scale benchmark for training and evaluating fine-grained robot manipulation models using part-level instructions. PartInstruct comprises 513 object instances across 14 categories, each annotated with part-level information, and 1302 fine-grained manipulation tasks organized into 16 task classes. Our training set consists of over 10,000 expert demonstrations synthesized in a 3D simulator, where each demonstration is paired with a high-level task instruction, a chain of base part-based skill instructions, and ground-truth 3D information about the object and its parts. Additionally, we designed a comprehensive test suite to evaluate the generalizability of learned policies across new states, objects, and tasks. We evaluated several state-of-the-art robot manipulation approaches, including end-to-end vision-language policy learning and bi-level planning models for robot manipulation on our benchmark. The experimental results reveal that current models struggle to robustly ground part concepts and predict actions in 3D space, and face challenges when manipulating object parts in long-horizon tasks. 

**Abstract (ZH)**: 细粒度机器人操作的鲁棒部分级推理及其大规模数据集PartInstruct 

---
# Efficient Diffusion Models for Symmetric Manifolds 

**Title (ZH)**: 对称流形上的高效扩散模型 

**Authors**: Oren Mangoubi, Neil He, Nisheeth K. Vishnoi  

**Link**: [PDF](https://arxiv.org/pdf/2505.21640)  

**Abstract**: We introduce a framework for designing efficient diffusion models for $d$-dimensional symmetric-space Riemannian manifolds, including the torus, sphere, special orthogonal group and unitary group. Existing manifold diffusion models often depend on heat kernels, which lack closed-form expressions and require either $d$ gradient evaluations or exponential-in-$d$ arithmetic operations per training step. We introduce a new diffusion model for symmetric manifolds with a spatially-varying covariance, allowing us to leverage a projection of Euclidean Brownian motion to bypass heat kernel computations. Our training algorithm minimizes a novel efficient objective derived via Ito's Lemma, allowing each step to run in $O(1)$ gradient evaluations and nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap between diffusions on symmetric manifolds and Euclidean space. Manifold symmetries ensure the diffusion satisfies an "average-case" Lipschitz condition, enabling accurate and efficient sample generation. Empirically, our model outperforms prior methods in training speed and improves sample quality on synthetic datasets on the torus, special orthogonal group, and unitary group. 

**Abstract (ZH)**: 我们介绍了一种设计高效扩散模型的框架，适用于$d$维对称空间黎曼流形，包括 тор切面、球面、特殊正交群和酉群。现有的流形扩散模型通常依赖于热核，缺乏闭式解，并且每次训练步骤需要$d$次梯度评估或$O(d)$数量级的算术运算。我们引入了一种新型的具有空间变异协方差的对称流形扩散模型，使得我们可以利用欧几里得布朗运动的投影来避开热核计算。我们的训练算法通过伊藤引理推导出一个新的高效目标函数，使得每个步骤只需要$O(1)$次梯度评估和几乎线性于$d$（$O(d^{1.19})$）的算术运算，从而缩小了对称流形上的扩散模型与欧几里得空间之间的差距。流形的对称性保证了扩散满足“平均情况”利普希茨条件，使得样本生成既准确又高效。实验表明，与先前的方法相比，我们的模型在训练速度上表现出色，并且在托利拆利面、特殊正交群和酉群上的合成数据集中提高了样本质量。 

---
# The Feasibility of Topic-Based Watermarking on Academic Peer Reviews 

**Title (ZH)**: 基于主题的学术同行评审水印可行性研究 

**Authors**: Alexander Nemecek, Yuzhou Jiang, Erman Ayday  

**Link**: [PDF](https://arxiv.org/pdf/2505.21636)  

**Abstract**: Large language models (LLMs) are increasingly integrated into academic workflows, with many conferences and journals permitting their use for tasks such as language refinement and literature summarization. However, their use in peer review remains prohibited due to concerns around confidentiality breaches, hallucinated content, and inconsistent evaluations. As LLM-generated text becomes more indistinguishable from human writing, there is a growing need for reliable attribution mechanisms to preserve the integrity of the review process. In this work, we evaluate topic-based watermarking (TBW), a lightweight, semantic-aware technique designed to embed detectable signals into LLM-generated text. We conduct a comprehensive assessment across multiple LLM configurations, including base, few-shot, and fine-tuned variants, using authentic peer review data from academic conferences. Our results show that TBW maintains review quality relative to non-watermarked outputs, while demonstrating strong robustness to paraphrasing-based evasion. These findings highlight the viability of TBW as a minimally intrusive and practical solution for enforcing LLM usage in peer review. 

**Abstract (ZH)**: 大规模语言模型（LLMs）越来越多地集成到学术工作流程中，许多会议和期刊允许其用于语言润色和文献总结等任务。然而，由于担心机密性泄露、虚构内容和评价不一致，其在同行评审中的使用仍被禁止。随着LLM生成的文本越来越难以与人类书写区分开来，可靠的归属机制对于维护评审过程的完整性变得日益重要。在这项工作中，我们评估了基于主题的水印（TBW）技术，这是一种轻量级、语义意识的方法，旨在将可检测的信号嵌入到LLM生成的文本中。我们使用来自学术会议的真实同行评审数据，对多种LLM配置进行全面评估，包括基础模型、少量示例提示模型和微调变体。结果显示，TBW能够保持与无水印输出相当的评审质量，同时表现出对基于改写规避的强鲁棒性。这些发现强调了TBW作为一项 minimally intrusive 和实用解决方案的可行性，以确保在同行评审中使用LLM。 

---
# Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives 

**Title (ZH)**: 你的大规模语言模型在向你多收费吗？ token化、透明度与激励机制 

**Authors**: Ander Artola Velasco, Stratis Tsirtsis, Nastaran Okati, Manuel Gomez-Rodriguez  

**Link**: [PDF](https://arxiv.org/pdf/2505.21627)  

**Abstract**: State-of-the-art large language models require specialized hardware and substantial energy to operate. As a consequence, cloud-based services that provide access to large language models have become very popular. In these services, the price users pay for an output provided by a model depends on the number of tokens the model uses to generate it -- they pay a fixed price per token. In this work, we show that this pricing mechanism creates a financial incentive for providers to strategize and misreport the (number of) tokens a model used to generate an output, and users cannot prove, or even know, whether a provider is overcharging them. However, we also show that, if an unfaithful provider is obliged to be transparent about the generative process used by the model, misreporting optimally without raising suspicion is hard. Nevertheless, as a proof-of-concept, we introduce an efficient heuristic algorithm that allows providers to significantly overcharge users without raising suspicion, highlighting the vulnerability of users under the current pay-per-token pricing mechanism. Further, to completely eliminate the financial incentive to strategize, we introduce a simple incentive-compatible token pricing mechanism. Under this mechanism, the price users pay for an output provided by a model depends on the number of characters of the output -- they pay a fixed price per character. Along the way, to illustrate and complement our theoretical results, we conduct experiments with several large language models from the $\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input prompts from the LMSYS Chatbot Arena platform. 

**Abstract (ZH)**: 最先进大规模语言模型需要专门的硬件和大量的能源来运行。因此，基于云的服务因其提供了访问大型语言模型的能力而变得非常流行。在这些服务中，用户为模型提供的输出支付的价格取决于模型生成该输出所使用的标记数量——他们按每个标记支付固定价格。在这项工作中，我们表明这种定价机制为服务提供商创造了通过策略行为和误报模型生成输出所使用标记数量来获利的经济动机，而用户既无法证明也无法知道服务提供商是否在加价。然而，我们还表明，如果服务提供商必须对模型的生成过程透明，那么无信行为在不引起怀疑的情况下进行最优误报是困难的。尽管如此，作为概念验证，我们引入了一种高效的启发式算法，允许提供商在不引起怀疑的情况下大幅加价，突显了当前按标记计费机制下用户的脆弱性。此外，为了完全消除策划行为的经济动机，我们引入了一种简单的可激励兼容的标记定价机制。在这种机制下，用户为模型提供的输出支付的价格取决于该输出的字符数量——他们按每个字符支付固定价格。在这一过程中，为了说明并补充我们的理论结果，我们使用了来自Llama、Gemma和Ministral系列以及LMSYS聊天机器人竞技场平台的多个大型语言模型和输入提示进行了实验。 

---
# VideoMarkBench: Benchmarking Robustness of Video Watermarking 

**Title (ZH)**: VideoMarkBench: 视频水印鲁棒性benchmarking 

**Authors**: Zhengyuan Jiang, Moyang Guo, Kecen Li, Yuepeng Hu, Yupu Wang, Zhicong Huang, Cheng Hong, Neil Zhenqiang Gong  

**Link**: [PDF](https://arxiv.org/pdf/2505.21620)  

**Abstract**: The rapid development of video generative models has led to a surge in highly realistic synthetic videos, raising ethical concerns related to disinformation and copyright infringement. Recently, video watermarking has been proposed as a mitigation strategy by embedding invisible marks into AI-generated videos to enable subsequent detection. However, the robustness of existing video watermarking methods against both common and adversarial perturbations remains underexplored. In this work, we introduce VideoMarkBench, the first systematic benchmark designed to evaluate the robustness of video watermarks under watermark removal and watermark forgery attacks. Our study encompasses a unified dataset generated by three state-of-the-art video generative models, across three video styles, incorporating four watermarking methods and seven aggregation strategies used during detection. We comprehensively evaluate 12 types of perturbations under white-box, black-box, and no-box threat models. Our findings reveal significant vulnerabilities in current watermarking approaches and highlight the urgent need for more robust solutions. Our code is available at this https URL. 

**Abstract (ZH)**: 视频生成模型的 rapid development 导致了高度逼真合成视频的激增，引发了虚假信息和版权侵犯的伦理担忧。近期，视频水印被提议作为一种缓解策略，通过在AI生成的视频中嵌入不可见标记以实现后续检测。然而，现有视频水印方法对于常见和对抗性 perturbations 的 robustness 仍需进一步探索。本文介绍了 VideoMarkBench，这是第一个系统性的基准，用于评估在去水印和伪造攻击下视频水印的 robustness。我们的研究涵盖了由三种最先进的视频生成模型生成的统一数据集，包括三种视频风格，四种水印方法和七种检测期间使用的聚合策略。我们在白盒、黑盒和无盒威胁模型下全面评估了12种类型的 perturbations。我们的发现揭示了现有水印方法的重大脆弱性，并突显了更 robust 解决方案的迫切需求。代码详见 this https URL。 

---
# Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study 

**Title (ZH)**: 防止针对自主态势感知的 adversarial AI 攻击：以海上应用为例 

**Authors**: Mathew J. Walter, Aaron Barrett, Kimberly Tam  

**Link**: [PDF](https://arxiv.org/pdf/2505.21609)  

**Abstract**: Adversarial artificial intelligence (AI) attacks pose a significant threat to autonomous transportation, such as maritime vessels, that rely on AI components. Malicious actors can exploit these systems to deceive and manipulate AI-driven operations. This paper addresses three critical research challenges associated with adversarial AI: the limited scope of traditional defences, inadequate security metrics, and the need to build resilience beyond model-level defences. To address these challenges, we propose building defences utilising multiple inputs and data fusion to create defensive components and an AI security metric as a novel approach toward developing more secure AI systems. We name this approach the Data Fusion Cyber Resilience (DFCR) method, and we evaluate it through real-world demonstrations and comprehensive quantitative analyses, comparing a system built with the DFCR method against single-input models and models utilising existing state-of-the-art defences. The findings show that the DFCR approach significantly enhances resilience against adversarial machine learning attacks in maritime autonomous system operations, achieving up to a 35\% reduction in loss for successful multi-pronged perturbation attacks, up to a 100\% reduction in loss for successful adversarial patch attacks and up to 100\% reduction in loss for successful spoofing attacks when using these more resilient systems. We demonstrate how DFCR and DFCR confidence scores can reduce adversarial AI contact confidence and improve decision-making by the system, even when typical adversarial defences have been compromised. Ultimately, this work contributes to the development of more secure and resilient AI-driven systems against adversarial attacks. 

**Abstract (ZH)**: adversarial人工智能攻击对依赖人工智能组件的自主运输系统，如海事船舶，构成了重大威胁。恶意行为者可以利用这些系统欺骗和操纵人工智能驱动的操作。本文针对与对抗人工智能相关的三个关键研究挑战：传统防御的狭窄范围、不充分的安全度量标准以及需要构建超越模型层面防御的韧性，提出了利用多个输入和数据融合构建防御组件及人工智能安全度量的新方法，以开发更安全的人工智能系统。我们称这一方法为数据融合网络韧性（DFCR）方法，并通过实际演示和全面的定量分析对其进行评估，将采用DFCR方法构建的系统与单一输入模型以及使用现有先进防御模型的系统进行比较。研究结果表明，DFCR方法显著增强了在海事自主系统操作中对抗对抗机器学习攻击的韧性，实现了多达35%的多阶段扰动攻击损失减少、多达100%的欺骗攻击损失减少以及在使用这些更具有韧性系统时，成功对抗性屏蔽攻击的损失减少。我们展示了DFCR和DFCR置信评分如何降低对抗性人工智能接触置信度并改进系统的决策，即使典型对抗性防御已被破坏也是如此。最终，本研究工作为开发更安全、更具韧性的对抗性攻击防御人工智能驱动系统做出了贡献。 

---
# How does Misinformation Affect Large Language Model Behaviors and Preferences? 

**Title (ZH)**: 误信息如何影响大型语言模型的行为和偏好？ 

**Authors**: Miao Peng, Nuo Chen, Jianheng Tang, Jia Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.21608)  

**Abstract**: Large Language Models (LLMs) have shown remarkable capabilities in knowledge-intensive tasks, while they remain vulnerable when encountering misinformation. Existing studies have explored the role of LLMs in combating misinformation, but there is still a lack of fine-grained analysis on the specific aspects and extent to which LLMs are influenced by misinformation. To bridge this gap, we present MisBench, the current largest and most comprehensive benchmark for evaluating LLMs' behavior and knowledge preference toward misinformation. MisBench consists of 10,346,712 pieces of misinformation, which uniquely considers both knowledge-based conflicts and stylistic variations in misinformation. Empirical results reveal that while LLMs demonstrate comparable abilities in discerning misinformation, they still remain susceptible to knowledge conflicts and stylistic variations. Based on these findings, we further propose a novel approach called Reconstruct to Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our study provides valuable insights into LLMs' interactions with misinformation, and we believe MisBench can serve as an effective benchmark for evaluating LLM-based detectors and enhancing their reliability in real-world applications. Codes and data are available at this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）在知识密集型任务中展现了出色的能力，但在遇到错误信息时仍存在脆弱性。现有研究已经探讨了LLMs在打击错误信息方面的作用，但仍缺乏对LLMs受错误信息影响的具体方面和程度的细致分析。为弥补这一不足，我们提出了MisBench，这是迄今为止最大最全面的基准测试，用于评估LLMs在错误信息方面的行为和知识偏好。MisBench包含10,346,712条错误信息，考虑了错误信息中的知识冲突和风格变化。实验结果表明，尽管LLMs在识别错误信息方面表现出相当的能力，但仍然容易受到知识冲突和风格变化的影响。基于这些发现，我们进一步提出了一种名为Reconstruct to Discriminate（RtD）的新型方法，以增强LLMs检测错误信息的能力。我们的研究为LLMs与错误信息的互动提供了有价值的认识，并相信MisBench可以作为评估基于LLM的检测器的有效基准，提高其在实际应用中的可靠性。代码和数据可在以下链接获取。 

---
# SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge 

**Title (ZH)**: SOSBENCH：科学知识安全性对齐的基准测试 

**Authors**: Fengqing Jiang, Fengbo Ma, Zhangchen Xu, Yuetai Li, Bhaskar Ramasubramanian, Luyao Niu, Bo Li, Xianyan Chen, Zhen Xiang, Radha Poovendran  

**Link**: [PDF](https://arxiv.org/pdf/2505.21605)  

**Abstract**: Large language models (LLMs) exhibit advancing capabilities in complex tasks, such as reasoning and graduate-level question answering, yet their resilience against misuse, particularly involving scientifically sophisticated risks, remains underexplored. Existing safety benchmarks typically focus either on instructions requiring minimal knowledge comprehension (e.g., ``tell me how to build a bomb") or utilize prompts that are relatively low-risk (e.g., multiple-choice or classification tasks about hazardous content). Consequently, they fail to adequately assess model safety when handling knowledge-intensive, hazardous scenarios.
To address this critical gap, we introduce SOSBench, a regulation-grounded, hazard-focused benchmark encompassing six high-risk scientific domains: chemistry, biology, medicine, pharmacology, physics, and psychology. The benchmark comprises 3,000 prompts derived from real-world regulations and laws, systematically expanded via an LLM-assisted evolutionary pipeline that introduces diverse, realistic misuse scenarios (e.g., detailed explosive synthesis instructions involving advanced chemical formulas). We evaluate frontier models within a unified evaluation framework using our SOSBench. Despite their alignment claims, advanced models consistently disclose policy-violating content across all domains, demonstrating alarmingly high rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1). These results highlight significant safety alignment deficiencies and underscore urgent concerns regarding the responsible deployment of powerful LLMs. 

**Abstract (ZH)**: 大型语言模型（LLMs）在复杂任务，如推理和研究生水平的问题回答方面表现出日益增强的能力，但它们在 misusage 防范方面的韧性，尤其是涉及科学复杂风险的能力，尚未得到充分探索。现有的安全性基准通常侧重于需要基本知识理解的指令（例如，“告诉我如何制作炸弹”）或使用相对低风险的提示（例如，关于有害内容的多项选择或分类任务）。因此，这些基准在评估模型在处理知识密集型、高风险场景时的安全性方面存在不足。

为解决这一关键缺口，我们引入了 SOSBench，这是一个依据监管要求、聚焦风险的安全性基准，涵盖了六个高风险科学领域：化学、生物学、医学、药理学、物理学和心理学。该基准包括从实际法规和法律中抽取的 3,000 个提示，通过一个由 LLM 支持的进化管道系统地扩展，引入了多种多样且真实的 misusage 场景（例如，涉及复杂化学公式的详细爆炸合成指令）。我们使用 SOSBench 在统一评估框架中评估前沿模型。尽管这些模型声称具有对齐性，先进的模型在所有领域中都一致地揭示出政策违反内容，显示出令人担忧的高危害响应率（例如，Deepseek-R1 为 79.1%，GPT-4.1 为 47.3%）。这些结果强调了重要的安全对齐不足，并突显了关于有强大 LLMs 负责任应用的紧迫关切。 

---
# Public Discourse Sandbox: Facilitating Human and AI Digital Communication Research 

**Title (ZH)**: 公共话语沙箱：促进人类与AI数字通信研究 

**Authors**: Kristina Radivojevic, Caleb Reinking, Shaun Whitfield, Paul Brenner  

**Link**: [PDF](https://arxiv.org/pdf/2505.21604)  

**Abstract**: Social media serves as a primary communication and information dissemination platform for major global events, entertainment, and niche or topically focused community discussions. Therefore, it represents a valuable resource for researchers who aim to understand numerous questions. However, obtaining data can be difficult, expensive, and often unreliable due to the presence of bots, fake accounts, and manipulated content. Additionally, there are ethical concerns if researchers decide to conduct an online experiment without explicitly notifying social media users about their intent. There is a need for more controlled and scalable mechanisms to evaluate the impacts of digital discussion interventions on audiences. We introduce the Public Discourse Sandbox (PDS), which serves as a digital discourse research platform for human-AI as well as AI-AI discourse research, testing, and training. PDS provides a safe and secure space for research experiments that are not viable on public, commercial social media platforms. Its main purpose is to enable the understanding of AI behaviors and the impacts of customized AI participants via techniques such as prompt engineering, retrieval-augmented generation (RAG), and fine-tuning. We provide a hosted live version of the sandbox to support researchers as well as the open-sourced code on GitHub for community collaboration and contribution. 

**Abstract (ZH)**: 社交媒体作为主要的全球事件、娱乐和 niche 或专题聚焦社区讨论的通信和信息传播平台，为研究人员提供了宝贵的资源以解答众多问题。然而，获取数据可能因存在机器人、虚假账户和操纵内容而变得困难、昂贵且不可靠。此外，如果研究人员决定在未明确通知社交媒体用户其意图的情况下进行在线实验，还存在伦理担忧。需要更受控且可扩展的机制来评估数字讨论干预措施对受众的影响。我们介绍了公共话语沙盒（PDS），这是一个用于人类-AI以及AI-AI讨论研究、测试和培训的数字话语研究平台。PDS 提供了一个安全且安全的研究实验空间，这些实验在公共的商业社交媒体平台上无法实施。其主要目的是利用提示工程、检索增强生成（RAG）和微调等技术，理解和研究自定义 AI 参与者的行为及其影响。我们提供了一个托管的实时版本的沙盒以支持研究人员，并在 GitHub 上开源代码以促进社区合作与贡献。 

---
# Leveraging XP and CRISP-DM for Agile Data Science Projects 

**Title (ZH)**: 利用XP和CRISP-DM促进敏捷数据科学项目 

**Authors**: Andre Massahiro Shimaoka, Renato Cordeiro Ferreira, Alfredo Goldman  

**Link**: [PDF](https://arxiv.org/pdf/2505.21603)  

**Abstract**: This study explores the integration of eXtreme Programming (XP) and the Cross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data Science projects. We conducted a case study at the e-commerce company Elo7 to answer the research question: How can the agility of the XP method be integrated with CRISP-DM in Data Science projects? Data was collected through interviews and questionnaires with a Data Science team consisting of data scientists, ML engineers, and data product managers. The results show that 86% of the team frequently or always applies CRISP-DM, while 71% adopt XP practices in their projects. Furthermore, the study demonstrates that it is possible to combine CRISP-DM with XP in Data Science projects, providing a structured and collaborative approach. Finally, the study generated improvement recommendations for the company. 

**Abstract (ZH)**: 本研究探讨了在敏捷数据科学项目中将极端编程(XP)与跨行业数据挖掘标准过程(CRISP-DM)集成的应用。我们在电商公司Elo7进行案例研究，以回答研究问题：如何将XP方法的敏捷性与CRISP-DM相结合应用于数据科学项目中？我们通过与数据科学家、机器学习工程师和数据产品管理者组成的团队进行访谈和问卷调查收集数据。结果显示，86%的团队成员经常或总是应用CRISP-DM，而71%的团队在项目中采用XP实践。此外，研究展示了在数据科学项目中结合使用CRISP-DM和XP的可能性，提供了一种结构化和协作的方法。最后，研究为公司提出了改进建议。 

---
# R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing 

**Title (ZH)**: R2R: 使用小型-大型模型标记路由高效导航发散推理路径 

**Authors**: Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21600)  

**Abstract**: Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at this https URL. 

**Abstract (ZH)**: Large Language Models (LLMs)在付出大量的推理开销以实现强大推理能力的同时，带来了显著的部署挑战。尽管提取的小语言模型（SLMs）显著提高了效率，但它们的性能受损，因为它们无法遵循LLMs的推理路径。幸运的是，我们发现只有少数令牌真正偏离了LLMs和SLMs之间的推理路径。大多数生成的令牌要么完全相同，要么表现出中性的差异，如缩写或表达方式的轻微变化。利用这一洞察，我们引入了**罗马之路（R2R）**，一种神经令牌路由方法，仅在这些关键的、路径偏离的令牌上利用LLMs，而将大多数令牌生成留给SLMs。我们还开发了一个自动数据生成管道来识别偏离的令牌，并生成令牌级路由标签以训练轻量级路由器。我们应用R2R将DeepSeek家族的R1-1.5B和R1-32B模型进行结合，并在具有挑战性的数学、编程和问答基准上进行评估。在平均激活参数大小为5.6B的情况下，R2R的平均准确性比R1-7B高出1.6倍，甚至优于R1-14B模型。与R1-32B相比，其在保持相似性能的同时，实现了2.8倍的墙钟速度提升，推动了测试时缩放效率的帕累托前沿。相关代码可在以下链接获取。 

---
# Learning optimal treatment strategies for intraoperative hypotension using deep reinforcement learning 

**Title (ZH)**: 基于深度强化学习的术中低血压最优治疗策略学习 

**Authors**: Esra Adiyeke, Tianqi Liu, Venkata Sai Dheeraj Naganaboina, Han Li, Tyler J. Loftus, Yuanfang Ren, Benjamin Shickel, Matthew M. Ruppert, Karandeep Singh, Ruogu Fang, Parisa Rashidi, Azra Bihorac, Tezcan Ozrazgat-Baslanti  

**Link**: [PDF](https://arxiv.org/pdf/2505.21596)  

**Abstract**: Traditional methods of surgical decision making heavily rely on human experience and prompt actions, which are variable. A data-driven system generating treatment recommendations based on patient states can be a substantial asset in perioperative decision-making, as in cases of intraoperative hypotension, for which suboptimal management is associated with acute kidney injury (AKI), a common and morbid postoperative complication. We developed a Reinforcement Learning (RL) model to recommend optimum dose of intravenous (IV) fluid and vasopressors during surgery to avoid intraoperative hypotension and postoperative AKI. We retrospectively analyzed 50,021 surgeries from 42,547 adult patients who underwent major surgery at a quaternary care hospital between June 2014 and September 2020. Of these, 34,186 surgeries were used for model training and 15,835 surgeries were reserved for testing. We developed a Deep Q-Networks based RL model using 16 variables including intraoperative physiologic time series, total dose of IV fluid and vasopressors extracted for every 15-minute epoch. The model replicated 69% of physician's decisions for the dosage of vasopressors and proposed higher or lower dosage of vasopressors than received in 10% and 21% of the treatments, respectively. In terms of IV fluids, the model's recommendations were within 0.05 ml/kg/15 min of the actual dose in 41% of the cases, with higher or lower doses recommended for 27% and 32% of the treatments, respectively. The model resulted in a higher estimated policy value compared to the physicians' actual treatments, as well as random and zero-drug policies. AKI prevalence was the lowest in patients receiving medication dosages that aligned with model's decisions. Our findings suggest that implementation of the model's policy has the potential to reduce postoperative AKI and improve other outcomes driven by intraoperative hypotension. 

**Abstract (ZH)**: 基于数据驱动的强化学习手术决策系统在术中低血压和术后急性肾损伤管理中的应用 

---
# Relevance-driven Input Dropout: an Explanation-guided Regularization Technique 

**Title (ZH)**: 以相关性为导向的输入掉落：一种基于解释的正则化技术 

**Authors**: Shreyas Gururaj, Lars Grüne, Wojciech Samek, Sebastian Lapuschkin, Leander Weber  

**Link**: [PDF](https://arxiv.org/pdf/2505.21595)  

**Abstract**: Overfitting is a well-known issue extending even to state-of-the-art (SOTA) Machine Learning (ML) models, resulting in reduced generalization, and a significant train-test performance gap. Mitigation measures include a combination of dropout, data augmentation, weight decay, and other regularization techniques. Among the various data augmentation strategies, occlusion is a prominent technique that typically focuses on randomly masking regions of the input during training. Most of the existing literature emphasizes randomness in selecting and modifying the input features instead of regions that strongly influence model decisions. We propose Relevance-driven Input Dropout (RelDrop), a novel data augmentation method which selectively occludes the most relevant regions of the input, nudging the model to use other important features in the prediction process, thus improving model generalization through informed regularization. We further conduct qualitative and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop) affects model decision-making. Through a series of experiments on benchmark datasets, we demonstrate that our approach improves robustness towards occlusion, results in models utilizing more features within the region of interest, and boosts inference time generalization performance. Our code is available at this https URL. 

**Abstract (ZH)**: 过拟合是扩展至最先进机器学习模型的已知问题，会导致泛化能力减弱和训练测试性能差距显著增大。缓解措施包括结合使用dropout、数据增强、权值衰减及其他正则化技术。在各种数据增强策略中，遮蔽是一种突出的技术，通常在训练过程中随机遮蔽输入的区域。现有文献大多强调在选择和修改输入特征时的随机性，而忽视了对模型决策影响较大的区域。我们提出了一种新的数据增强方法——相关性驱动输入丢弃（RelDrop），该方法有选择地遮蔽输入中的相关区域，促使模型在预测过程中使用其他重要特征，从而通过有选择的正则化提高模型泛化能力。我们进一步进行了定性和定量分析，研究相关性驱动输入丢弃（RelDrop）对模型决策的影响。通过一系列基准数据集实验，我们证明了该方法提高了对遮蔽的鲁棒性，使得模型在感兴趣区域内使用更多特征，并提升了推理时间的泛化性能。代码可在以下链接获取：this https URL。 

---
# Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits 

**Title (ZH)**: 快速且经济高效的 speculative 边缘-云解码早期退出方法 

**Authors**: Yeshwanth Venkatesha, Souvik Kundu, Priyadarshini Panda  

**Link**: [PDF](https://arxiv.org/pdf/2505.21594)  

**Abstract**: Large Language Models (LLMs) enable various applications on edge devices such as smartphones, wearables, and embodied robots. However, their deployment often depends on expensive cloud-based APIs, creating high operational costs, which limit access for smaller organizations and raise sustainability concerns. Certain LLMs can be deployed on-device, offering a cost-effective solution with reduced latency and improved privacy. Yet, limited computing resources constrain the size and accuracy of models that can be deployed, necessitating a collaborative design between edge and cloud. We propose a fast and cost-effective speculative edge-cloud decoding framework with a large target model on the server and a small draft model on the device. By introducing early exits in the target model, tokens are generated mid-verification, allowing the client to preemptively draft subsequent tokens before final verification, thus utilizing idle time and enhancing parallelism between edge and cloud. Using an NVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft) and Llama2-7B (target) models, our method achieves up to a 35% reduction in latency compared to cloud-based autoregressive decoding, with an additional 11% improvement from preemptive drafting. To demonstrate real-world applicability, we deploy our method on the Unitree Go2 quadruped robot using Vision-Language Model (VLM) based control, achieving a 21% speedup over traditional cloud-based autoregressive decoding. These results demonstrate the potential of our framework for real-time LLM and VLM applications on resource-constrained edge devices. 

**Abstract (ZH)**: 面向边缘设备的快速和成本-effective 推测边缘-云解码框架：基于服务器端大型目标模型和设备端小型草稿模型 

---
# Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion 

**Title (ZH)**: 任意到景深效果：基于多平面图像引导扩散的一步视频景深生成 

**Authors**: Yang Yang, Siming Zheng, Jinwei Chen, Boxi Wu, Xiaofei He, Deng Cai, Bo Li, Peng-Tao Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21593)  

**Abstract**: Recent advances in diffusion based editing models have enabled realistic camera simulation and image-based bokeh, but video bokeh remains largely unexplored. Existing video editing models cannot explicitly control focus planes or adjust bokeh intensity, limiting their applicability for controllable optical effects. Moreover, naively extending image-based bokeh methods to video often results in temporal flickering and unsatisfactory edge blur transitions due to the lack of temporal modeling and generalization capability. To address these challenges, we propose a novel one-step video bokeh framework that converts arbitrary input videos into temporally coherent, depth-aware bokeh effects. Our method leverages a multi-plane image (MPI) representation constructed through a progressively widening depth sampling function, providing explicit geometric guidance for depth-dependent blur synthesis. By conditioning a single-step video diffusion model on MPI layers and utilizing the strong 3D priors from pre-trained models such as Stable Video Diffusion, our approach achieves realistic and consistent bokeh effects across diverse scenes. Additionally, we introduce a progressive training strategy to enhance temporal consistency, depth robustness, and detail preservation. Extensive experiments demonstrate that our method produces high-quality, controllable bokeh effects and achieves state-of-the-art performance on multiple evaluation benchmarks. 

**Abstract (ZH)**: Recent Advances in Diffusion-Based Editing Models Have Enabled Realistic Camera Simulation and Image-Based Bokeh, but Video Bokeh Remains largely Unexplored 

---
# Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning 

**Title (ZH)**: 4位浮点量化在扩散模型中的先驱研究：Mixup-Sign量化和时间步aware微调 

**Authors**: Maosen Zhao, Pengtao Chen, Chong Yu, Yan Wen, Xudong Tan, Tao Chen  

**Link**: [PDF](https://arxiv.org/pdf/2505.21591)  

**Abstract**: Model quantization reduces the bit-width of weights and activations, improving memory efficiency and inference speed in diffusion models. However, achieving 4-bit quantization remains challenging. Existing methods, primarily based on integer quantization and post-training quantization fine-tuning, struggle with inconsistent performance. Inspired by the success of floating-point (FP) quantization in large language models, we explore low-bit FP quantization for diffusion models and identify key challenges: the failure of signed FP quantization to handle asymmetric activation distributions, the insufficient consideration of temporal complexity in the denoising process during fine-tuning, and the misalignment between fine-tuning loss and quantization error. To address these challenges, we propose the mixup-sign floating-point quantization (MSFP) framework, first introducing unsigned FP quantization in model quantization, along with timestep-aware LoRA (TALoRA) and denoising-factor loss alignment (DFA), which ensure precise and stable fine-tuning. Extensive experiments show that we are the first to achieve superior performance in 4-bit FP quantization for diffusion models, outperforming existing PTQ fine-tuning methods in 4-bit INT quantization. 

**Abstract (ZH)**: 低比特浮点量化在扩散模型中的应用及挑战：Mixup-sign浮点量化框架 

---
# Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI 

**Title (ZH)**: 你看到我看到的吗？一个含糊光学错觉数据集揭示可解释AI的局限性 

**Authors**: Carina Newen, Luca Hinkamp, Maria Ntonti, Emmanuel Müller  

**Link**: [PDF](https://arxiv.org/pdf/2505.21589)  

**Abstract**: From uncertainty quantification to real-world object detection, we recognize the importance of machine learning algorithms, particularly in safety-critical domains such as autonomous driving or medical diagnostics. In machine learning, ambiguous data plays an important role in various machine learning domains. Optical illusions present a compelling area of study in this context, as they offer insight into the limitations of both human and machine perception. Despite this relevance, optical illusion datasets remain scarce. In this work, we introduce a novel dataset of optical illusions featuring intermingled animal pairs designed to evoke perceptual ambiguity. We identify generalizable visual concepts, particularly gaze direction and eye cues, as subtle yet impactful features that significantly influence model accuracy. By confronting models with perceptual ambiguity, our findings underscore the importance of concepts in visual learning and provide a foundation for studying bias and alignment between human and machine vision. To make this dataset useful for general purposes, we generate optical illusions systematically with different concepts discussed in our bias mitigation section. The dataset is accessible in Kaggle via this https URL. Our source code can be found at this https URL. 

**Abstract (ZH)**: 从不确定性量化到实际对象检测，我们认识到机器学习算法的重要性，特别是在自动驾驶或医疗诊断等安全关键领域。在机器学习中，模糊数据在各种机器学习领域发挥着重要作用。光学错觉提供了一个值得研究的领域，因为它们不仅可以揭示人类感知的局限性，还可以揭示机器感知的局限性。尽管如此，现有的光学错觉数据集仍然很少。在本工作中，我们引入了一个新的光学错觉数据集，该数据集包含交错的动物对，旨在引发知觉上的模糊性。我们识别出可泛化的视觉概念，特别是凝视方向和眼睛线索，这些细微但重要的特征显著影响模型的准确性。通过使模型面对知觉上的模糊性，我们的发现强调了视觉学习中概念的重要性，并为研究人类与机器视觉之间的偏差和对齐奠定了基础。为了使该数据集适用于一般用途，我们根据我们在偏差缓解部分讨论的概念系统生成了光学错觉。数据集可通过Kaggle在此处访问：https://www.kaggle.com/。我们的源代码可在以下网址找到：https://www.。 

---
# Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems 

**Title (ZH)**: 羊群行为：基于LLM的多智能体系统中的同伴影响探究 

**Authors**: Young-Min Cho, Sharath Chandra Guntuku, Lyle Ungar  

**Link**: [PDF](https://arxiv.org/pdf/2505.21588)  

**Abstract**: Recent advancements in Large Language Models (LLMs) have enabled the emergence of multi-agent systems where LLMs interact, collaborate, and make decisions in shared environments. While individual model behavior has been extensively studied, the dynamics of peer influence in such systems remain underexplored. In this paper, we investigate herd behavior, the tendency of agents to align their outputs with those of their peers, within LLM-based multi-agent interactions. We present a series of controlled experiments that reveal how herd behaviors are shaped by multiple factors. First, we show that the gap between self-confidence and perceived confidence in peers significantly impacts an agent's likelihood to conform. Second, we find that the format in which peer information is presented plays a critical role in modulating the strength of herd behavior. Finally, we demonstrate that the degree of herd behavior can be systematically controlled, and that appropriately calibrated herd tendencies can enhance collaborative outcomes. These findings offer new insights into the social dynamics of LLM-based systems and open pathways for designing more effective and adaptive multi-agent collaboration frameworks. 

**Abstract (ZH)**: 近期大型语言模型的发展使得多agent系统得以出现，其中语言模型在共享环境中相互作用、协作并作出决策。尽管个体模型的行为已被广泛研究，但在这些系统中同伴影响的动态仍然很少被探索。本文研究了基于语言模型的多agent交互中的羊群行为，即代理倾向于与其同伴对齐其输出的倾向。我们进行了受控实验，揭示了羊群行为受多种因素影响的具体方式。首先，我们展示了自我信心与对同伴信心的感知之间的差距显著影响代理服从的可能性。其次，我们发现同伴信息呈现的方式在调节羊群行为强度方面起着关键作用。最后，我们证明了可以系统地控制羊群行为的程度，并且适当的羊群倾向可以增强协作成果。这些发现为理解基于语言模型的系统的社会动态提供了新的见解，并为设计更有效和适应性强的多agent协作框架开辟了途径。 

---
# CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning 

**Title (ZH)**: CellCLAT: 保留拓扑结构并裁剪冗余的自监督细胞对比学习 

**Authors**: Bin Qin, Qirui Ji, Jiangmeng Li, Yupeng Wang, Xuesong Wu, Jianwen Cao, Fanjiang Xu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21587)  

**Abstract**: Self-supervised topological deep learning (TDL) represents a nascent but underexplored area with significant potential for modeling higher-order interactions in simplicial complexes and cellular complexes to derive representations of unlabeled graphs. Compared to simplicial complexes, cellular complexes exhibit greater expressive power. However, the advancement in self-supervised learning for cellular TDL is largely hindered by two core challenges: \textit{extrinsic structural constraints} inherent to cellular complexes, and intrinsic semantic redundancy in cellular representations. The first challenge highlights that traditional graph augmentation techniques may compromise the integrity of higher-order cellular interactions, while the second underscores that topological redundancy in cellular complexes potentially diminish task-relevant information. To address these issues, we introduce Cellular Complex Contrastive Learning with Adaptive Trimming (CellCLAT), a twofold framework designed to adhere to the combinatorial constraints of cellular complexes while mitigating informational redundancy. Specifically, we propose a parameter perturbation-based augmentation method that injects controlled noise into cellular interactions without altering the underlying cellular structures, thereby preserving cellular topology during contrastive learning. Additionally, a cellular trimming scheduler is employed to mask gradient contributions from task-irrelevant cells through a bi-level meta-learning approach, effectively removing redundant topological elements while maintaining critical higher-order semantics. We provide theoretical justification and empirical validation to demonstrate that CellCLAT achieves substantial improvements over existing self-supervised graph learning methods, marking a significant attempt in this domain. 

**Abstract (ZH)**: 基于细胞复形的自监督拓扑深度学习（CellCLAT）：一种适应组合约束并减轻信息冗余的框架 

---
# Fairness in Federated Learning: Fairness for Whom? 

**Title (ZH)**: 联邦学习中的公平性：为谁公平？ 

**Authors**: Afaf Taik, Khaoula Chehbouni, Golnoosh Farnadi  

**Link**: [PDF](https://arxiv.org/pdf/2505.21584)  

**Abstract**: Fairness in federated learning has emerged as a rapidly growing area of research, with numerous works proposing formal definitions and algorithmic interventions. Yet, despite this technical progress, fairness in FL is often defined and evaluated in ways that abstract away from the sociotechnical contexts in which these systems are deployed. In this paper, we argue that existing approaches tend to optimize narrow system level metrics, such as performance parity or contribution-based rewards, while overlooking how harms arise throughout the FL lifecycle and how they impact diverse stakeholders. We support this claim through a critical analysis of the literature, based on a systematic annotation of papers for their fairness definitions, design decisions, evaluation practices, and motivating use cases. Our analysis reveals five recurring pitfalls: 1) fairness framed solely through the lens of server client architecture, 2) a mismatch between simulations and motivating use-cases and contexts, 3) definitions that conflate protecting the system with protecting its users, 4) interventions that target isolated stages of the lifecycle while neglecting upstream and downstream effects, 5) and a lack of multi-stakeholder alignment where multiple fairness definitions can be relevant at once. Building on these insights, we propose a harm centered framework that links fairness definitions to concrete risks and stakeholder vulnerabilities. We conclude with recommendations for more holistic, context-aware, and accountable fairness research in FL. 

**Abstract (ZH)**: 联邦学习中的公平性已成为快速发展的研究领域，众多研究提出了正式定义和算法干预措施。然而，尽管取得了这些技术进步，公平性在联邦学习中的定义和评估往往抽象掉了这些系统部署的社会技术背景。本文 argue 学术翻译应为“我们”主张，现有方法倾向于优化系统层面的狭窄指标，如性能平齐或贡献为基础的奖励，而忽视了在整个联邦学习生命周期中如何产生危害以及这些危害如何影响多样化的利益相关者。我们通过批判性地分析文献，基于对论文的系统标注（评估）公平性定义、设计决策、评估实践以及激励用例来支持这一主张。我们的分析揭示了五个反复出现的陷阱：1）将公平性仅通过服务器客户端架构的视角来界定，2）模拟与激励用例和背景之间的不匹配，3）将保护系统等同于保护其用户的概念混淆，4）针对生命周期中的孤立阶段进行干预，而忽视了上游和下游影响，5）缺乏多利益相关者对齐，在一次中多个公平性定义可能都相关。基于这些见解，我们提出了一个以危害为中心的框架，将公平性定义与具体的危害和利益相关者脆弱性联系起来。最后，我们提出了关于联邦学习中更具包容性、情境意识和负责任的公平性研究的建议。 

---
# AITEE -- Agentic Tutor for Electrical Engineering 

**Title (ZH)**: AITEE —— 电气工程领域赋能式导师 

**Authors**: Christopher Knievel, Alexander Bernhardt, Christian Bernhardt  

**Link**: [PDF](https://arxiv.org/pdf/2505.21582)  

**Abstract**: Intelligent tutoring systems combined with large language models offer a promising approach to address students' diverse needs and promote self-efficacious learning. While large language models possess good foundational knowledge of electrical engineering basics, they remain insufficiently capable of addressing specific questions about electrical circuits. In this paper, we present AITEE, an agent-based tutoring system for electrical engineering designed to accompany students throughout their learning process, offer individualized support, and promote self-directed learning. AITEE supports both hand-drawn and digital circuits through an adapted circuit reconstruction process, enabling natural interaction with students. Our novel graph-based similarity measure identifies relevant context from lecture materials through a retrieval augmented generation approach, while parallel Spice simulation further enhances accuracy in applying solution methodologies. The system implements a Socratic dialogue to foster learner autonomy through guided questioning. Experimental evaluations demonstrate that AITEE significantly outperforms baseline approaches in domain-specific knowledge application, with even medium-sized LLM models showing acceptable performance. Our results highlight the potential of agentic tutors to deliver scalable, personalized, and effective learning environments for electrical engineering education. 

**Abstract (ZH)**: 智能 tutoring 系统结合大语言模型为满足学生多样化需求和促进自主学习提供了有希望的方法。虽然大语言模型在电气工程基础知识方面具有良好的基础，但在处理关于电气电路的特定问题方面仍存在不足。在本文中，我们介绍了 AITEE，这是一种基于代理的电气工程辅导系统，旨在陪伴学生整个学习过程，提供个性化支持，并促进自主学习。AITEE 通过适应性的电路重建过程支持手绘和数字电路，实现自然的学生交互。我们提出的一种基于图的相似性度量通过检索增强生成方法识别相关上下文，并与并行Spice仿真结合进一步提高解决方案方法的应用准确性。系统采用苏格拉底式对话来通过引导性提问培养学习者的自主性。实验评估表明，AITEE 在特定领域知识应用方面显著优于基线方法，即使中型规模的LLM模型也表现出可接受的性能。我们的结果突显了代理式辅导在为电气工程教育提供可扩展、个性化和有效学习环境方面的潜力。 

---
# RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving 

**Title (ZH)**: RepoMaster: 自主探索和理解 GitHub 仓库以解决复杂任务 

**Authors**: Huacan Wang, Ziyi Ni, Shuo Zhang, Shuo Lu, Sen Hu, Ziyang He, Chen Hu, Jiaye Lin, Yifu Guo, Yuntao Du, Pin Lyu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21577)  

**Abstract**: The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts. Building such repositories from scratch remains a major challenge. Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources. Relying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. For efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. During autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage. Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over the strongest baseline OpenHands. On our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 24.1% to 62.9% while reducing token usage by 95%. Our code and demonstration materials are publicly available at this https URL. 

**Abstract (ZH)**: Code代理的最终目标是自主解决复杂任务。尽管大型语言模型（LLMs）在代码生成方面取得了显著进展，但现实世界的任务通常需要完整的代码库，而不仅仅是简单的脚本。从头构建这样的库仍然是一个主要挑战。幸运的是，GitHub托管着一个庞大且不断演化的开源代码库集合，开发人员经常将其作为复杂任务的模块化组件重复使用。然而，现有的框架如OpenHands和SWE-Agent仍然难以有效利用这些宝贵资源。仅仅依赖README文件提供了不足的指导，深入探索揭示了两个核心障碍：信息量过大和代码库之间纠缠的依赖关系，这些都受当前LLM有限上下文窗口的约束。为了解决这些问题，我们提出RepoMaster，这是一种自主代理框架，旨在探索和重用GitHub上的代码库以解决复杂任务。为了高效理解，RepoMaster构建函数调用图、模块依赖图和层次代码树，以识别关键组件，并向LLM提供仅识别的核心元素而非整个代码库。在自主执行过程中，它使用我们的探索工具逐步探索相关组件并精简信息以优化上下文使用。在调整后的MLE-bench上，RepoMaster相对于最强基线OpenHands实现了110%的相对提升。在我们新发布的GitTaskBench上，RepoMaster将任务通过率从24.1%提升到62.9%，同时减少了95%的token使用量。我们的代码和演示材料在此公开可用网址。 

---
# Concentration Distribution Learning from Label Distributions 

**Title (ZH)**: 从标签分布学习集中度分布 

**Authors**: Jiawei Tang, Yuheng Jia  

**Link**: [PDF](https://arxiv.org/pdf/2505.21576)  

**Abstract**: Label distribution learning (LDL) is an effective method to predict the relative label description degree (a.k.a. label distribution) of a sample. However, the label distribution is not a complete representation of an instance because it overlooks the absolute intensity of each label. Specifically, it's impossible to obtain the total description degree of hidden labels that not in the label space, which leads to the loss of information and confusion in instances. To solve the above problem, we come up with a new concept named background concentration to serve as the absolute description degree term of the label distribution and introduce it into the LDL process, forming the improved paradigm of concentration distribution learning. Moreover, we propose a novel model by probabilistic methods and neural networks to learn label distributions and background concentrations from existing LDL datasets. Extensive experiments prove that the proposed approach is able to extract background concentrations from label distributions while producing more accurate prediction results than the state-of-the-art LDL methods. The code is available in this https URL. 

**Abstract (ZH)**: 基于背景集中度的改进标签分布学习 

---
# StreamLink: Large-Language-Model Driven Distributed Data Engineering System 

**Title (ZH)**: StreamLink：由大型语言模型驱动的分布式数据工程系统 

**Authors**: Dawei Feng, Di Mei, Huiri Tan, Lei Ren, Xianying Lou, Zhangxi Tan  

**Link**: [PDF](https://arxiv.org/pdf/2505.21575)  

**Abstract**: Large Language Models (LLMs) have shown remarkable proficiency in natural language understanding (NLU), opening doors for innovative applications. We introduce StreamLink - an LLM-driven distributed data system designed to improve the efficiency and accessibility of data engineering tasks. We build StreamLink on top of distributed frameworks such as Apache Spark and Hadoop to handle large data at scale. One of the important design philosophies of StreamLink is to respect user data privacy by utilizing local fine-tuned LLMs instead of a public AI service like ChatGPT. With help from domain-adapted LLMs, we can improve our system's understanding of natural language queries from users in various scenarios and simplify the procedure of generating database queries like the Structured Query Language (SQL) for information processing. We also incorporate LLM-based syntax and security checkers to guarantee the reliability and safety of each generated query. StreamLink illustrates the potential of merging generative LLMs with distributed data processing for comprehensive and user-centric data engineering. With this architecture, we allow users to interact with complex database systems at different scales in a user-friendly and security-ensured manner, where the SQL generation reaches over 10\% of execution accuracy compared to baseline methods, and allow users to find the most concerned item from hundreds of millions of items within a few seconds using natural language. 

**Abstract (ZH)**: Large Language Models驱动的分布式数据系统StreamLink：提升数据工程任务效率与隐私保护的研究 

---
# Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes 

**Title (ZH)**: 基于谱的神经运算器：在物理无感知区间内的数据高效偏微分方程模拟 

**Authors**: Han Wan, Rui Zhang, Hao Sun  

**Link**: [PDF](https://arxiv.org/pdf/2505.21573)  

**Abstract**: Partial differential equations (PDEs) govern the spatiotemporal evolution of various physical systems. Classical numerical solvers, while accurate, require fine discretization and full knowledge of the governing PDEs, limiting their applicability when the physics is unknown or fast inference is required. Data-driven neural PDE solvers alleviate these constraints by learning from data but demand large training datasets and perform poorly in data-scarce regimes. Physics-aware methods mitigate data requirements by incorporating physical knowledge yet rely on known PDE terms or local numerical schemes, restricting their ability to handle unknown or globally coupled systems. In this work, we propose the Spectral-inspired Neural Operator (SINO), a novel framework that learns PDE operators from limited trajectories (as few as 2-5), without any known PDE terms. SINO operates in the frequency domain and introduces a Frequency-to-Vector module to learn spectral representations analogous to derivative multipliers. To model nonlinear physical interactions, we design a nonlinear operator block that includes a $\Pi$-Block with low-pass filtering to prevent aliasing. Finally, we introduce an operator distillation technique to distill the trained model for efficient inference. SINO achieves state-of-the-art results across multiple PDE benchmarks, demonstrating strong discretization invariance and robust generalization to out-of-distribution initial conditions. To our knowledge, SINO is the first physics-aware method capable of accurately simulating globally coupled systems (e.g., the Navier-Stokes equations) from limited data without any explicit PDE terms. 

**Abstract (ZH)**: 基于频谱的神经算子（SINO）：从有限轨迹中学习PDE算子的新型框架 

---
# Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks 

**Title (ZH)**: 厚度感知的E(3)-等变3D网格神经网络 

**Authors**: Sungwon Kim, Namkyeong Lee, Yunyoung Doh, Seungmin Shin, Guimok Cho, Seung-Won Jeon, Sangkook Kim, Chanyoung Park  

**Link**: [PDF](https://arxiv.org/pdf/2505.21572)  

**Abstract**: Mesh-based 3D static analysis methods have recently emerged as efficient alternatives to traditional computational numerical solvers, significantly reducing computational costs and runtime for various physics-based analyses. However, these methods primarily focus on surface topology and geometry, often overlooking the inherent thickness of real-world 3D objects, which exhibits high correlations and similar behavior between opposing surfaces. This limitation arises from the disconnected nature of these surfaces and the absence of internal edge connections within the mesh. In this work, we propose a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network (T-EMNN), that effectively integrates the thickness of 3D objects while maintaining the computational efficiency of surface meshes. Additionally, we introduce data-driven coordinates that encode spatial information while preserving E(3)-equivariance or invariance properties, ensuring consistent and robust analysis. Evaluations on a real-world industrial dataset demonstrate the superior performance of T-EMNN in accurately predicting node-level 3D deformations, effectively capturing thickness effects while maintaining computational efficiency. 

**Abstract (ZH)**: 基于网格的三维静力学分析方法 recently emerged as有效的替代传统计算数值求解器的选择，显著减少了各种基于物理的分析的计算成本和运行时间。然而，这些方法主要关注表面拓扑和几何结构，往往会忽略现实世界三维物体的固有厚度，而这种厚度在对立表面之间表现出高相关性和相似行为。这种局限性源于这些表面之间的不连续性质以及网格内部缺乏边的连接性。在这项工作中，我们提出了一种新的框架，即厚度aware E(3)-等变三维网格神经网络（T-EMNN），该框架有效地整合了三维物体的厚度，同时保持表面网格的计算效率。此外，我们引入了数据驱动的坐标，这些坐标编码空间信息并保留E(3)-等变性或不变性特性，确保一致和稳健的分析。在实际工业数据集上的评估表明，T-EMNN在准确预测节点级三维变形方面表现出更优的性能，有效地捕捉厚度效应同时保持计算效率。 

---
# FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition 

**Title (ZH)**: FCOS：一种用于自动调制识别的两阶段可恢复模型剪枝框架 

**Authors**: Yao Lu, Tengfei Ma, Zeyu Wang, Zhuangzhi Chen, Dongwei Xu, Yun Lin, Qi Xuan, Guan Gui  

**Link**: [PDF](https://arxiv.org/pdf/2505.21571)  

**Abstract**: With the rapid development of wireless communications and the growing complexity of digital modulation schemes, traditional manual modulation recognition methods struggle to extract reliable signal features and meet real-time requirements in modern scenarios. Recently, deep learning based Automatic Modulation Recognition (AMR) approaches have greatly improved classification accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained devices. Model pruning provides a general approach to reduce model complexity, but existing weight, channel, and layer pruning techniques each present a trade-off between compression rate, hardware acceleration, and accuracy preservation. To this end, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning framework that combines channel-level pruning with layer-level collapse diagnosis to achieve extreme compression, high performance and efficient inference. In the first stage of FCOS, hierarchical clustering and parameter fusion are applied to channel weights to achieve channel-level pruning. Then a Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer collapse and removes the collapsed layers due to high channel compression ratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms existing channel and layer pruning methods. Specifically, FCOS achieves 95.51% FLOPs reduction and 95.31% parameter reduction while still maintaining performance close to the original ResNet56, with only a 0.46% drop in accuracy on Sig2019-12. Code is available at this https URL. 

**Abstract (ZH)**: 基于深度学习的细到粗两阶段自动调制识别裁剪框架：一种极端压缩与高效推理的结合 

---
# Beyond Explainability: The Case for AI Validation 

**Title (ZH)**: 超越可解释性：人工智能验证的案例 

**Authors**: Dalit Ken-Dror Feldman, Daniel Benoliel  

**Link**: [PDF](https://arxiv.org/pdf/2505.21570)  

**Abstract**: Artificial Knowledge (AK) systems are transforming decision-making across critical domains such as healthcare, finance, and criminal justice. However, their growing opacity presents governance challenges that current regulatory approaches, focused predominantly on explainability, fail to address adequately. This article argues for a shift toward validation as a central regulatory pillar. Validation, ensuring the reliability, consistency, and robustness of AI outputs, offers a more practical, scalable, and risk-sensitive alternative to explainability, particularly in high-stakes contexts where interpretability may be technically or economically unfeasible. We introduce a typology based on two axes, validity and explainability, classifying AK systems into four categories and exposing the trade-offs between interpretability and output reliability. Drawing on comparative analysis of regulatory approaches in the EU, US, UK, and China, we show how validation can enhance societal trust, fairness, and safety even where explainability is limited. We propose a forward-looking policy framework centered on pre- and post-deployment validation, third-party auditing, harmonized standards, and liability incentives. This framework balances innovation with accountability and provides a governance roadmap for responsibly integrating opaque, high-performing AK systems into society. 

**Abstract (ZH)**: 人工知识（AK）系统正在改变医疗、金融和刑事司法等关键领域的决策过程。然而，它们日益增加的不透明度提出了治理挑战，当前主要集中在解释性的监管方法未能充分应对这些挑战。本文主张将验证作为核心监管支柱。验证确保AI输出的可靠性和稳健性，提供了比解释性更为实用、可扩展和风险敏感的选择，尤其是在解释性可能在技术上或经济上不切实际的高风险环境中。我们根据有效性和解释性两个轴构建了分类体系，将AK系统分为四类，揭示了解释性和输出可靠性之间的权衡。通过比较欧盟、美国、英国和中国的监管方法，我们展示了即使在解释性有限的情况下，验证如何增强社会信任、公平性和安全性。我们提出了一种面向未来的政策框架，集中在部署前后的验证、第三方审计、标准 harmonization 和责任激励上。该框架平衡了创新与问责制，为将不透明但表现卓越的AK系统负责任地整合到社会中提供了治理路线图。 

---
# ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools 

**Title (ZH)**: ChemHAS: 分层代理堆叠以增强化学工具 

**Authors**: Zhucong Li, Bowei Zhang, Jin Xiao, Zhijian Zhou, Fenglei Cao, Jiaqing Liang, Yuan Qi  

**Link**: [PDF](https://arxiv.org/pdf/2505.21569)  

**Abstract**: Large Language Model (LLM)-based agents have demonstrated the ability to improve performance in chemistry-related tasks by selecting appropriate tools. However, their effectiveness remains limited by the inherent prediction errors of chemistry tools. In this paper, we take a step further by exploring how LLMbased agents can, in turn, be leveraged to reduce prediction errors of the tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking), a simple yet effective method that enhances chemistry tools through optimizing agent-stacking structures from limited data. ChemHAS achieves state-of-the-art performance across four fundamental chemistry tasks, demonstrating that our method can effectively compensate for prediction errors of the tools. Furthermore, we identify and characterize four distinct agent-stacking behaviors, potentially improving interpretability and revealing new possibilities for AI agent applications in scientific research. Our code and dataset are publicly available at https: //anonymous.this http URL. 

**Abstract (ZH)**: 基于大型语言模型的代理通过优化代理堆叠结构以减少化学工具的预测误差 

---
# VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents 

**Title (ZH)**: VoiceMark: 零样本说话人克隆抗水印方法及其利用说话人特定的潜在特征 

**Authors**: Haiyun Li, Zhiyong Wu, Xiaofeng Xie, Jingran Xie, Yaoxun Xu, Hanyang Peng  

**Link**: [PDF](https://arxiv.org/pdf/2505.21568)  

**Abstract**: Voice cloning (VC)-resistant watermarking is an emerging technique for tracing and preventing unauthorized cloning. Existing methods effectively trace traditional VC models by training them on watermarked audio but fail in zero-shot VC scenarios, where models synthesize audio from an audio prompt without training. To address this, we propose VoiceMark, the first zero-shot VC-resistant watermarking method that leverages speaker-specific latents as the watermark carrier, allowing the watermark to transfer through the zero-shot VC process into the synthesized audio. Additionally, we introduce VC-simulated augmentations and VAD-based loss to enhance robustness against distortions. Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves over 95% accuracy in watermark detection after zero-shot VC synthesis, significantly outperforming existing methods, which only reach around 50%. See our code and demos at: this https URL 

**Abstract (ZH)**: VoiceMark：零样本语音克隆鲁棒水印方法 

---
# Towards Human-Like Trajectory Prediction for Autonomous Driving: A Behavior-Centric Approach 

**Title (ZH)**: 面向自主驾驶的人类似轨迹预测：一种行为中心的方法 

**Authors**: Haicheng Liao, Zhenning Li, Guohui Zhang, Keqiang Li, Chengzhong Xu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21565)  

**Abstract**: Predicting the trajectories of vehicles is crucial for the development of autonomous driving (AD) systems, particularly in complex and dynamic traffic environments. In this study, we introduce HiT (Human-like Trajectory Prediction), a novel model designed to enhance trajectory prediction by incorporating behavior-aware modules and dynamic centrality measures. Unlike traditional methods that primarily rely on static graph structures, HiT leverages a dynamic framework that accounts for both direct and indirect interactions among traffic participants. This allows the model to capture the subtle yet significant influences of surrounding vehicles, enabling more accurate and human-like predictions. To evaluate HiT's performance, we conducted extensive experiments using diverse and challenging real-world datasets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results demonstrate that HiT consistently outperforms other top models across multiple metrics, particularly excelling in scenarios involving aggressive driving behaviors. This research presents a significant step forward in trajectory prediction, offering a more reliable and interpretable approach for enhancing the safety and efficiency of fully autonomous driving systems. 

**Abstract (ZH)**: 基于行为感知模块和动态中心性度量的人类-like路径预测（HiT）：在复杂动态交通环境中的车辆轨迹预测 

---
# Fog Intelligence for Network Anomaly Detection 

**Title (ZH)**: 雾计算在网络异常检测中的应用 

**Authors**: Kai Yang, Hui Ma, Shaoyu Dou  

**Link**: [PDF](https://arxiv.org/pdf/2505.21563)  

**Abstract**: Anomalies are common in network system monitoring. When manifested as network threats to be mitigated, service outages to be prevented, and security risks to be ameliorated, detecting such anomalous network behaviors becomes of great importance. However, the growing scale and complexity of the mobile communication networks, as well as the ever-increasing amount and dimensionality of the network surveillance data, make it extremely difficult to monitor a mobile network and discover abnormal network behaviors. Recent advances in machine learning allow for obtaining near-optimal solutions to complicated decision-making problems with many sources of uncertainty that cannot be accurately characterized by traditional mathematical models. However, most machine learning algorithms are centralized, which renders them inapplicable to a large-scale distributed wireless networks with tens of millions of mobile devices. In this article, we present fog intelligence, a distributed machine learning architecture that enables intelligent wireless network management. It preserves the advantage of both edge processing and centralized cloud computing. In addition, the proposed architecture is scalable, privacy-preserving, and well suited for intelligent management of a distributed wireless network. 

**Abstract (ZH)**: 异常在网络系统监控中较为常见。当这些异常表现为需要缓解的网络威胁、需要预防的服务中断、以及需要降低的安全风险时，检测这些异常网络行为变得尤为重要。然而，移动通信网络的不断扩展和复杂化，以及网络监控数据量的不断增加和维度的提高，使得监控移动网络并发现异常网络行为变得极其困难。近年来机器学习的进展使得在存在传统数学模型难以准确描述的诸多不确定性的情况下，获得复杂决策问题的近似最优解成为可能。然而，大多数机器学习算法是集中式的，这使得它们不适合处理包含数以千万计移动设备的大型分布式无线网络。本文提出了一种分布式机器学习架构——雾智能，它兼具边缘处理和集中式云计算的优势。此外，提出的架构是可扩展的、隐私保护的，并且非常适合分布式无线网络的智能管理。 

---
# Enhancing Selection of Climate Tech Startups with AI -- A Case Study on Integrating Human and AI Evaluations in the ClimaTech Great Global Innovation Challenge 

**Title (ZH)**: 利用AI增强气候tech startups的选择：以ClimaTech伟大全球创新挑战中整合人类和AI评估的案例研究为例 

**Authors**: Jennifer Turliuk, Alejandro Sevilla, Daniela Gorza, Tod Hynes  

**Link**: [PDF](https://arxiv.org/pdf/2505.21562)  

**Abstract**: This case study examines the ClimaTech Great Global Innovation Challenge's approach to selecting climate tech startups by integrating human and AI evaluations. The competition aimed to identify top startups and enhance the accuracy and efficiency of the selection process through a hybrid model. Research shows data-driven approaches help VC firms reduce bias and improve decision-making. Machine learning models have outperformed human investors in deal screening, helping identify high-potential startups. Incorporating AI aimed to ensure more equitable and objective evaluations.
The methodology included three phases: initial AI review, semi-finals judged by humans, and finals using a hybrid weighting. In phase one, 57 applications were scored by an AI tool built with StackAI and OpenAI's GPT-4o, and the top 36 advanced. In the semi-finals, human judges, unaware of AI scores, evaluated startups on team quality, market potential, and technological innovation. Each score - human or AI - was weighted equally, resulting in 75 percent human and 25 percent AI influence. In the finals, with five human judges, weighting shifted to 83.3 percent human and 16.7 percent AI. There was a moderate positive correlation between AI and human scores - Spearman's = 0.47 - indicating general alignment with key differences. Notably, the final four startups, selected mainly by humans, were among those rated highest by the AI. This highlights the complementary nature of AI and human judgment. The study shows that hybrid models can streamline and improve startup assessments. The ClimaTech approach offers a strong framework for future competitions by combining human expertise with AI capabilities. 

**Abstract (ZH)**: This Case Study Examines the ClimaTech Great Global Innovation Challenge's Approach to Selecting Climate Tech Startups by Integrating Human and AI Evaluations 

---
# Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework 

**Title (ZH)**: 基于多代理系统的自动化在线设计框架优化弹性Kubernetes自动扩展 

**Authors**: Julien Soulé, Jean-Paul Jamont, Michel Occello, Louis-Marie Traonouez, Paul Théron  

**Link**: [PDF](https://arxiv.org/pdf/2505.21559)  

**Abstract**: In cloud-native systems, Kubernetes clusters with interdependent services often face challenges to their operational resilience due to poor workload management issues such as resource blocking, bottlenecks, or continuous pod crashes. These vulnerabilities are further amplified in adversarial scenarios, such as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal Pod Autoscaling (HPA) approaches struggle to address such dynamic conditions, while reinforcement learning-based methods, though more adaptable, typically optimize single goals like latency or resource usage, neglecting broader failure scenarios. We propose decomposing the overarching goal of maintaining operational resilience into failure-specific sub-goals delegated to collaborative agents, collectively forming an HPA Multi-Agent System (MAS). We introduce an automated, four-phase online framework for HPA MAS design: 1) modeling a digital twin built from cluster traces; 2) training agents in simulation using roles and missions tailored to failure contexts; 3) analyzing agent behaviors for explainability; and 4) transferring learned policies to the real cluster. Experimental results demonstrate that the generated HPA MASs outperform three state-of-the-art HPA systems in sustaining operational resilience under various adversarial conditions in a proposed complex cluster. 

**Abstract (ZH)**: 在云原生系统中，由于工作负载管理问题如资源阻塞、瓶颈或持续的POD崩溃，相互依赖的服务的Kubernetes集群往往面临运维韧性方面的挑战。在分布式拒绝服务攻击（DDoS）等对抗场景中，这些脆弱性会被进一步放大。传统的水平POD自动扩展（HPA）方法难以应对这些动态条件，而基于强化学习的方法虽然更具适应性，但也通常只优化单一目标如延迟或资源使用，忽视了更广泛的故障场景。我们提出将保持运维韧性的总体目标分解为针对特定故障的子目标，并委派给协作代理，从而共同构成一个HPA多代理系统（MAS）。我们引入了用于HPA MAS设计的自动化四阶段在线框架：1）从集群踪迹构建数字孪生模型；2）在模拟中训练代理，使用适应故障情境的角色和任务；3）分析代理行为以实现可解释性；4）将学习到的策略转移到实际集群。实验结果表明，在所提出的复杂集群中，在各种对抗条件下的运维韧性保持方面，生成的HPA MASs优于三种最先进的HPA系统。 

---
# A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification 

**Title (ZH)**: 基于卷积神经网络的复杂多类 Brassica 种子分类新型框架 

**Authors**: Elhoucine Elfatimia, Recep Eryigitb, Lahcen Elfatimi  

**Link**: [PDF](https://arxiv.org/pdf/2505.21558)  

**Abstract**: Agricultural research has accelerated in recent years, yet farmers often lack the time and resources for on-farm research due to the demands of crop production and farm operations. Seed classification offers valuable insights into quality control, production efficiency, and impurity detection. Early identification of seed types is critical to reducing the cost and risk associated with field emergence, which can lead to yield losses or disruptions in downstream processes like harvesting. Seed sampling supports growers in monitoring and managing seed quality, improving precision in determining seed purity levels, guiding management adjustments, and enhancing yield estimations. This study proposes a novel convolutional neural network (CNN)-based framework for the efficient classification of ten common Brassica seed types. The approach addresses the inherent challenge of texture similarity in seed images using a custom-designed CNN architecture. The model's performance was evaluated against several pre-trained state-of-the-art architectures, with adjustments to layer configurations for optimized classification. Experimental results using our collected Brassica seed dataset demonstrate that the proposed model achieved a high accuracy rate of 93 percent. 

**Abstract (ZH)**: 近年来，农业研究加速发展，但由于作物生产和农场运营的需求，农民往往缺乏进行农场研究的时间和资源。种子分类为质量控制、生产效率和杂质检测提供了宝贵见解。早期识别种子类型对于降低田间出苗的成本和风险至关重要，这可以减少产量损失或下游过程如收获中的中断。种子采样支持种植者监控和管理种子质量，提高了种子纯度水平的精确度，指导管理调整，并提升产量估计。本研究提出了一种新型卷积神经网络（CNN）基于框架，用于高效分类十种常见的 Brassica 种子类型。该方法通过自定义设计的 CNN 架构解决了种子图像中固有的纹理相似性挑战。通过与多种预训练的先进架构评估模型性能，并对层配置进行调整以优化分类。使用我们收集的 Brassica 种子数据集的实验证明，所提模型的准确率为 93%。 

---
# Analytical Calculation of Weights Convolutional Neural Network 

**Title (ZH)**: 权重卷积神经网络的解析计算 

**Authors**: Polad Geidarov  

**Link**: [PDF](https://arxiv.org/pdf/2505.21557)  

**Abstract**: This paper presents an algorithm for analytically calculating the weights and thresholds of convolutional neural networks (CNNs) without using standard training procedures. The algorithm enables the determination of CNN parameters based on just 10 selected images from the MNIST dataset, each representing a digit from 0 to 9. As part of the method, the number of channels in CNN layers is also derived analytically. A software module was implemented in C++ Builder, and a series of experiments were conducted using the MNIST dataset. Results demonstrate that the analytically computed CNN can recognize over half of 1000 handwritten digit images without any training, achieving inference in fractions of a second. These findings suggest that CNNs can be constructed and applied directly for classification tasks without training, using purely analytical computation of weights. 

**Abstract (ZH)**: 本文提出了一种在不使用标准训练程序的情况下，通过解析方法计算卷积神经网络（CNN）权重和阈值的算法。该算法能够在仅使用MNIST数据集中10张代表0至9数字的手写图像的情况下确定CNN参数。方法还包括解析确定CNN层中的通道数。在C++ Builder中实现了一个软件模块，并使用MNIST数据集进行了多项实验。结果表明，解析计算得到的CNN可以在没有任何训练的情况下识别超过1000张手写数字图像中的半数，并在几分之一秒内完成推理。这些发现表明，可以通过纯解析计算权重来构建和直接应用于分类任务的CNN，而无需训练。 

---
# Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts 

**Title (ZH)**: 良性到有害的 Jailbreaking：无害提示诱导有害响应 

**Authors**: Hee-Seon Kim, Minbeom Kim, Wonjun Lee, Kihyun Kim, Changick Kim  

**Link**: [PDF](https://arxiv.org/pdf/2505.21556)  

**Abstract**: Optimization-based jailbreaks typically adopt the Toxic-Continuation setting in large vision-language models (LVLMs), following the standard next-token prediction objective. In this setting, an adversarial image is optimized to make the model predict the next token of a toxic prompt. However, we find that the Toxic-Continuation paradigm is effective at continuing already-toxic inputs, but struggles to induce safety misalignment when explicit toxic signals are absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike prior work, we optimize adversarial images to induce toxic outputs from benign conditioning. Since benign conditioning contains no safety violations, the image alone must break the model's safety mechanisms. Our method outperforms prior approaches, transfers in black-box settings, and complements text-based jailbreaks. These results reveal an underexplored vulnerability in multimodal alignment and introduce a fundamentally new direction for jailbreak approaches. 

**Abstract (ZH)**: 基于优化的 Jailbreak 通常采用大型视觉-语言模型中的有毒连续输入设置（Toxic-Continuation），遵循标准的下一个 token 预测目标。在这种设置中，对抗图像被优化以使模型预测有毒提示的下一个 token。然而，我们发现，有毒连续输入范式在处理已经具有毒性输入时有效，但在缺乏明确的毒性信号时难以导致安全对齐失效。我们提出了一种新的范式：良性转有毒（B2T）Jailbreak。与以往工作不同，我们优化对抗图像以从良性的条件生成有毒输出。由于良性条件中不包含安全违规信号，图像本身必须打破模型的安全机制。我们的方法优于以往的方法，在黑盒设置中具有可迁移性，并补充了基于文本的 Jailbreak 方法。这些结果揭示了多模态对齐中未被充分探索的脆弱性，并引入了 Jailbreak 方法的新基本方向。 

---
# MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction 

**Title (ZH)**: MetaSTNet：多模态元学习的细胞交通遵从性预测 

**Authors**: Hui Ma, Kai Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21553)  

**Abstract**: Network traffic prediction techniques have attracted much attention since they are valuable for network congestion control and user experience improvement. While existing prediction techniques can achieve favorable performance when there is sufficient training data, it remains a great challenge to make accurate predictions when only a small amount of training data is available. To tackle this problem, we propose a deep learning model, entitled MetaSTNet, based on a multimodal meta-learning framework. It is an end-to-end network architecture that trains the model in a simulator and transfers the meta-knowledge to a real-world environment, which can quickly adapt and obtain accurate predictions on a new task with only a small amount of real-world training data. In addition, we further employ cross conformal prediction to assess the calibrated prediction intervals. Extensive experiments have been conducted on real-world datasets to illustrate the efficiency and effectiveness of MetaSTNet. 

**Abstract (ZH)**: 基于多模态元学习框架的MetaSTNet网络流量预测技术 

---
# WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper 

**Title (ZH)**: WhisperD：使用Whisper进行痴呆症语音识别和填充词检测 

**Authors**: Emmanuel Akinrintoyo, Nadine Abdelhalim, Nicole Salomons  

**Link**: [PDF](https://arxiv.org/pdf/2505.21551)  

**Abstract**: Whisper fails to correctly transcribe dementia speech because persons with dementia (PwDs) often exhibit irregular speech patterns and disfluencies such as pauses, repetitions, and fragmented sentences. It was trained on standard speech and may have had little or no exposure to dementia-affected speech. However, correct transcription is vital for dementia speech for cost-effective diagnosis and the development of assistive technology. In this work, we fine-tune Whisper with the open-source dementia speech dataset (DementiaBank) and our in-house dataset to improve its word error rate (WER). The fine-tuning also includes filler words to ascertain the filler inclusion rate (FIR) and F1 score. The fine-tuned models significantly outperformed the off-the-shelf models. The medium-sized model achieved a WER of 0.24, outperforming previous work. Similarly, there was a notable generalisability to unseen data and speech patterns. 

**Abstract (ZH)**: Whisper因 dementia 患者经常表现出不规律的语音模式和中断、重复和片段化的句子而无法正确转录导致其转录 dementia 语音失败。它被训练于标准语音数据，可能缺乏对 dementia 影响下的语音暴露。然而，准确转录对于成本效益的诊断和辅助技术的发展至关重要。本研究中，我们使用开源的 dementia 语音数据集（DementiaBank）和内部数据集对 Whisper 进行微调，以提高其词错误率（WER）。微调还包括填充词以确定填充词包含率（FIR）和 F1 分数。微调后的模型显著优于现成模型。中型模型达到了 0.24 的 WER，优于先前的工作。同样，模型在未见数据和语音模式上也表现出明显的泛化能力。 

---
# Collaborative Agentic AI Needs Interoperability Across Ecosystems 

**Title (ZH)**: 跨生态系统的代理性协作AI需要互操作性 

**Authors**: Rishi Sharma, Martijn de Vos, Pradyumna Chari, Ramesh Raskar, Anne-Marie Kermarrec  

**Link**: [PDF](https://arxiv.org/pdf/2505.21550)  

**Abstract**: Collaborative agentic AI is projected to transform entire industries by enabling AI-powered agents to autonomously perceive, plan, and act within digital environments. Yet, current solutions in this field are all built in isolation, and we are rapidly heading toward a landscape of fragmented, incompatible ecosystems. In this position paper, we argue that interoperability, achieved by the adoption of minimal standards, is essential to ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To this end, we devise a minimal architectural foundation for collaborative agentic AI, named Web of Agents, which is composed of four components: agent-to-agent messaging, interaction interoperability, state management, and agent discovery. Web of Agents adopts existing standards and reuses existing infrastructure where possible. With Web of Agents, we take the first but critical step toward interoperable agentic systems and offer a pragmatic path forward before ecosystem fragmentation becomes the norm. 

**Abstract (ZH)**: 协作代理人工智能预计通过使AI驱动的代理能够在数字环境中自主感知、规划和行动来颠覆整个行业。然而，目前该领域内的解决方案都是独立构建的，我们正迅速走向一个由碎片化且不兼容的生态系统构成的景观。在这种立场论文中，我们认为通过采用最小标准来实现互操作性是确保开放、安全、Web规模且广泛采用的代理生态系统的关键。为此，我们提出了一个协作代理人工智能的最小架构基础，称为代理网络，它由四个组件组成：代理间消息传递、交互互操作性、状态管理以及代理发现。代理网络采用现有标准并在可能的情况下重用现有基础设施。通过代理网络，我们迈出了实现互操作代理系统的关键一步，并在生态系统碎片化成为常态之前提供了一条实际可行的道路。 

---
# Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding? 

**Title (ZH)**: 流畅但文化距离感：区域训练能培养文化理解能力吗？ 

**Authors**: Dhruv Agarwal, Anya Shukla, Sunayana Sitaram, Aditya Vashistha  

**Link**: [PDF](https://arxiv.org/pdf/2505.21548)  

**Abstract**: Large language models (LLMs) are used around the world but exhibit Western cultural tendencies. To address this cultural misalignment, many countries have begun developing "regional" LLMs tailored to local communities. Yet it remains unclear whether these models merely speak the language of their users or also reflect their cultural values and practices. Using India as a case study, we evaluate five Indic and five global LLMs along two key dimensions: values (via the Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench and NormAd). Across all four tasks, we find that Indic models do not align more closely with Indian cultural norms than global models. In fact, an average American person is a better proxy for Indian cultural values than any Indic model. Even prompting strategies fail to meaningfully improve alignment. Ablations show that regional fine-tuning does not enhance cultural competence and may in fact hurt it by impeding recall of existing knowledge. We trace this failure to the scarcity of high-quality, untranslated, and culturally grounded pretraining and fine-tuning data. Our study positions cultural evaluation as a first-class requirement alongside multilingual benchmarks and offers a reusable methodology for developers. We call for deeper investments in culturally representative data to build and evaluate truly sovereign LLMs. 

**Abstract (ZH)**: 大型语言模型（LLMs）在世界各地被使用，但表现出西方文化倾向。为解决这种文化不匹配问题，许多国家开始开发“地域化”的LLMs，以适应当地社区。然而，目前尚不清楚这些模型是仅仅模仿用户语言，还是也反映了其文化价值观和实践。以印度为例，我们评估了五种印度语言模型和五种全球性模型在两个关键维度上的表现：价值观（通过Inglehart-Welzel地图和GlobalOpinionQA评估）和实践（通过CulturalBench和NormAd评估）。在所有四项任务中，我们发现印度语言模型并没有比全球模型更贴近印度文化规范。事实上，一个平均美国人比任何印度语言模型更能代表印度文化价值观。即使采用提示策略也无法显著提高契合度。消融实验显示，地域化微调并不能提升文化能力，反而可能因妨碍现有知识的回忆而损害之。我们将这一失败归因于高质量、未经翻译且文化根基深厚的预训练和微调数据的稀缺性。我们的研究将文化评估置于与多语言基准并列的首要要求，并提供了一种可重用的方法论，供开发者使用。我们呼吁更深层次地投资于文化代表性数据，以构建和评估真正主权的LLMs。 

---
# Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing 

**Title (ZH)**: Image Tokens matters: 减轻基于离散标记器的大规模视觉-语言模型中幻觉影响的潜在编辑方法 

**Authors**: Weixing Wang, Zifeng Ding, Jindong Gu, Rui Cao, Christoph Meinel, Gerard de Melo, Haojin Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21547)  

**Abstract**: Large Vision-Language Models (LVLMs) with discrete image tokenizers unify multimodal representations by encoding visual inputs into a finite set of tokens. Despite their effectiveness, we find that these models still hallucinate non-existent objects. We hypothesize that this may be due to visual priors induced during training: When certain image tokens frequently co-occur in the same spatial regions and represent shared objects, they become strongly associated with the verbalizations of those objects. As a result, the model may hallucinate by evoking visually absent tokens that often co-occur with present ones. To test this assumption, we construct a co-occurrence graph of image tokens using a segmentation dataset and employ a Graph Neural Network (GNN) with contrastive learning followed by a clustering method to group tokens that frequently co-occur in similar visual contexts. We find that hallucinations predominantly correspond to clusters whose tokens dominate the input, and more specifically, that the visually absent tokens in those clusters show much higher correlation with hallucinated objects compared to tokens present in the image. Based on this observation, we propose a hallucination mitigation method that suppresses the influence of visually absent tokens by modifying latent image embeddings during generation. Experiments show our method reduces hallucinations while preserving expressivity. Code is available at this https URL 

**Abstract (ZH)**: 大视觉-语言模型中的离散图像标记器通过将视觉输入编码为有限的token集合来统一多模态表示。尽管这些模型效果显著，但我们发现它们仍然会产生不存在的物体。我们认为这可能是由于训练过程中引入的视觉先验：当某些图像token在相同的空间区域内频繁共现并表示共享物体时，它们会与这些物体的文本描述产生强烈的关联。因此，模型可能会通过唤起与现有关联但视觉上不存在的token来产生幻觉。为了验证这一假设，我们使用分割数据集构建图像token的共现图，并使用对比学习后的图神经网络（GNN）及聚类方法来分组在相似视觉上下文中频繁共现的tokens。我们发现，幻觉主要与支配输入的token群组相对应，更具体地说，这些群组中视觉上不存在的tokens与幻觉物体的相关性远高于图像中存在的tokens。基于这一观察，我们提出了一种通过修改生成过程中潜变量图像嵌入来抑制视觉上不存在的token影响的方法。实验表明，该方法能减少幻觉并保持表达能力。代码可在以下链接获取。 

---
# DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers 

**Title (ZH)**: DiffDecompose: Alpha-合成图像逐层分解的扩散变换器方法 

**Authors**: Zitong Wang, Hang Zhao, Qianyu Zhou, Xuequan Lu, Xiangtai Li, Yiren Song  

**Link**: [PDF](https://arxiv.org/pdf/2505.21541)  

**Abstract**: Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: this https URL. 

**Abstract (ZH)**: 基于Alpha混合图像的层级分解：一种扩散变换器框架 

---
# Equivariant Flow Matching for Point Cloud Assembly 

**Title (ZH)**: 点云装配的等变流动匹配 

**Authors**: Ziming Wang, Nan Xue, Rebecka Jörnsten  

**Link**: [PDF](https://arxiv.org/pdf/2505.21539)  

**Abstract**: The goal of point cloud assembly is to reconstruct a complete 3D shape by aligning multiple point cloud pieces. This work presents a novel equivariant solver for assembly tasks based on flow matching models. We first theoretically show that the key to learning equivariant distributions via flow matching is to learn related vector fields. Based on this result, we propose an assembly model, called equivariant diffusion assembly (Eda), which learns related vector fields conditioned on the input pieces. We further construct an equivariant path for Eda, which guarantees high data efficiency of the training process. Our numerical results show that Eda is highly competitive on practical datasets, and it can even handle the challenging situation where the input pieces are non-overlapped. 

**Abstract (ZH)**: 点云拼接的目标是通过对齐多个点云片段来重构完整的3D形状。本文提出了一种基于流匹配模型的新型等变求解器用于拼接任务。我们首先从理论上证明了通过流匹配学习等变分布的关键在于学习相关向量场。基于这一结果，我们提出了一种称为等变扩散拼接（Eda）的拼接模型，该模型在输入片段的条件下学习相关向量场。我们进一步为Eda构建了一种等变路径，以保证训练过程的高度数据效率。我们的数值结果表明，Eda在实际数据集上具有高度竞争力，并且即使在输入片段不重叠的情况下也能处理具有挑战性的场景。 

---
# Caption This, Reason That: VLMs Caught in the Middle 

**Title (ZH)**: 给这幅图配上 captions，解释那个原因：VLMs 处在中间位置。 

**Authors**: Zihan Weng, Lucas Gomez, Taylor Whittington Webb, Pouya Bashivan  

**Link**: [PDF](https://arxiv.org/pdf/2505.21538)  

**Abstract**: Vision-Language Models (VLMs) have shown remarkable progress in visual understanding in recent years. Yet, they still lag behind human capabilities in specific visual tasks such as counting or relational reasoning. To understand the underlying limitations, we adopt methodologies from cognitive science, analyzing VLM performance along core cognitive axes: Perception, Attention, and Memory. Using a suite of tasks targeting these abilities, we evaluate state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct cognitive profiles: while advanced models approach ceiling performance on some tasks (e.g. category identification), a significant gap persists, particularly in tasks requiring spatial understanding or selective attention. Investigating the source of these failures and potential methods for improvement, we employ a vision-text decoupling analysis, finding that models struggling with direct visual reasoning show marked improvement when reasoning over their own generated text captions. These experiments reveal a strong need for improved VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed human performance. Furthermore, we demonstrate the potential of targeted fine-tuning on composite visual reasoning tasks and show that fine-tuning smaller VLMs substantially improves core cognitive abilities. While this improvement does not translate to large enhancements on challenging, out-of-distribution benchmarks, we show broadly that VLM performance on our datasets strongly correlates with performance on these other benchmarks. Our work provides a detailed analysis of VLM cognitive strengths and weaknesses and identifies key bottlenecks in simultaneous perception and reasoning while also providing an effective and simple solution. 

**Abstract (ZH)**: Vision-Language 模型在近年来展示了在视觉理解方面的显著进步。然而，在如计数或关系推理等特定视觉任务中，它们仍落后于人类能力。为了理解这些潜在限制，我们采用了来自认知科学的方法，从感知、注意和记忆这三大核心认知轴分析 Vision-Language 模型的表现。利用一系列针对这些能力的测试任务，我们评估了当前最先进的 Vision-Language 模型，包括 GPT-4o。我们的分析揭示了不同的认知特征：尽管高级模型在某些任务（如类别识别）上接近天花板水平，但在要求空间理解或选择性注意的任务上仍存在显著差距。我们调查了这些失败的根源以及潜在的改进方法，并采用视觉-文本解耦分析发现，处理直接视觉推理困难的模型在其生成的文本描述上进行推理时表现明显改善。这些实验揭示了即使在模型持续超越人类表现的情况下，也需要提高 Vision-Language 模型思维链（CoT）能力的强烈需求。此外，我们展示了针对复合视觉推理任务的微调潜力，并证明小型模型的微调显著增强了核心认知能力。虽然这种改进在挑战性的离分布基准测试上并未转化为显著提升，但我们表明，在我们的数据集上 Vision-Language 模型的表现与这些其他基准测试的性能之间有显著的相关性。我们的研究详细分析了 Vision-Language 模型的认知强项和弱点，并识别了同时感知和推理中的关键瓶颈，同时还提供了一个有效且简单的解决方案。 

---
# OpenReview Should be Protected and Leveraged as a Community Asset for Research in the Era of Large Language Models 

**Title (ZH)**: 大型语言模型时代应保护并充分利用OpenReview作为科研社区资产 

**Authors**: Hao Sun, Yunyi Shen, Mihaela van der Schaar  

**Link**: [PDF](https://arxiv.org/pdf/2505.21537)  

**Abstract**: In the era of large language models (LLMs), high-quality, domain-rich, and continuously evolving datasets capturing expert-level knowledge, core human values, and reasoning are increasingly valuable. This position paper argues that OpenReview -- the continually evolving repository of research papers, peer reviews, author rebuttals, meta-reviews, and decision outcomes -- should be leveraged more broadly as a core community asset for advancing research in the era of LLMs. We highlight three promising areas in which OpenReview can uniquely contribute: enhancing the quality, scalability, and accountability of peer review processes; enabling meaningful, open-ended benchmarks rooted in genuine expert deliberation; and supporting alignment research through real-world interactions reflecting expert assessment, intentions, and scientific values. To better realize these opportunities, we suggest the community collaboratively explore standardized benchmarks and usage guidelines around OpenReview, inviting broader dialogue on responsible data use, ethical considerations, and collective stewardship. 

**Abstract (ZH)**: 在大规模语言模型时代，高质量、领域丰富且持续进化的数据集对于捕捉专家级知识、核心人类价值观和推理能力日益重要。本文观点认为，OpenReview——这一持续演化的研究论文、同行评审、作者回应、元评审和决策结果的仓库——应更广泛地作为核心社区资产，推动大规模语言模型时代的科研进步。我们强调OpenReview在三个方面可以做出独特贡献：提升同行评审过程的质量、规模和问责性；促进基于真实专家讨论的根本性的开放性基准测试；并通过反映专家评估、意图和科学价值观的实际世界互动支持对齐研究。为了更好地把握这些机会，我们建议社区协作探索OpenReview的标准化基准测试和使用指南，并邀请更广泛的对话讨论负责任的数据使用、伦理考量和集体监护。 

---
# Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement 

**Title (ZH)**: Transformer推理中注意机制是否必不可少？探究功能等价的注意机制替代方法 

**Authors**: Yuxin Ren, Maxwell D Collins, Miao Hu, Huanrui Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21535)  

**Abstract**: While transformers excel across vision and language pretraining tasks, their reliance on attention mechanisms poses challenges for inference efficiency, especially on edge and embedded accelerators with limited parallelism and memory bandwidth. Hinted by the observed redundancy of attention at inference time, we hypothesize that though the model learns complicated token dependency through pretraining, the inference-time sequence-to-sequence mapping in each attention layer is actually ''simple'' enough to be represented with a much cheaper function. In this work, we explore FAR, a Function-preserving Attention Replacement framework that replaces all attention blocks in pretrained transformers with learnable sequence-to-sequence modules, exemplified by an LSTM. FAR optimize a multi-head LSTM architecture with a block-wise distillation objective and a global structural pruning framework to achieve a family of efficient LSTM-based models from pretrained transformers. We validate FAR on the DeiT vision transformer family and demonstrate that it matches the accuracy of the original models on ImageNet and multiple downstream tasks with reduced parameters and latency. Further analysis shows that FAR preserves the semantic token relationships and the token-to-token correlation learned in the transformer's attention module. 

**Abstract (ZH)**: 尽管变压器在视觉和语言预训练任务中表现出色，但其依赖于注意力机制的方法在边缘和嵌入式加速器上的推理效率低下，尤其是这些设备具有有限的并行性和内存带宽。鉴于推理过程中观察到的注意力冗余，我们假设尽管模型在预训练过程中学习了复杂的token依赖关系，但在每个注意力层中的推理时序到时序映射实际上是“简单”的，可以用一个代价更低的函数来表示。在本文中，我们探索了一种名为FAR的功能保持注意力替代框架，该框架用可学习的时序到时序模块替换预训练变压器中的所有注意力块，作为示例，这些模块可以是LSTM。FAR通过块级蒸馏目标和全局结构剪枝框架优化一个多头LSTM架构，从而从预训练变压器中生成一系列高效的LSTM基模型。我们在DeiT视觉变压器家族上验证了FAR，并证明了它在ImageNet和多个下游任务上可以减少参数量和延迟的同时保持原始模型的精度。进一步的分析表明，FAR保留了变压器注意力模块中学习到的词元间的语义关系和词元间的相关性。 

---
# Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with Cycle Time Reduction Agents 

**Title (ZH)**: 通过周期时间减少剂揭示瓶颈并优化科学实验室工作流 

**Authors**: Yao Fehlis  

**Link**: [PDF](https://arxiv.org/pdf/2505.21534)  

**Abstract**: Scientific laboratories, particularly those in pharmaceutical and biotechnology companies, encounter significant challenges in optimizing workflows due to the complexity and volume of tasks such as compound screening and assay execution. We introduce Cycle Time Reduction Agents (CTRA), a LangGraph-based agentic workflow designed to automate the analysis of lab operational metrics. CTRA comprises three main components: the Question Creation Agent for initiating analysis, Operational Metrics Agents for data extraction and validation, and Insights Agents for reporting and visualization, identifying bottlenecks in lab processes. This paper details CTRA's architecture, evaluates its performance on a lab dataset, and discusses its potential to accelerate pharmaceutical and biotechnological development. CTRA offers a scalable framework for reducing cycle times in scientific labs. 

**Abstract (ZH)**: 基于LangGraph的Cycle Time Reduction Agents：一种用于优化科学实验室工作流程的代理工作流及其在制药和生物技术中的潜在加速作用 

---
# EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media 

**Title (ZH)**: EvidenceMoE: 一种基于物理导向的混合专家模型与证据批评家方法，用于推进散射介质中的荧光光检测与测距技术 

**Authors**: Ismail Erbas, Ferhat Demirkiran, Karthik Swaminathan, Naigang Wang, Navid Ibtehaj Nizam, Stefan T. Radev, Kaoutar El Maghraoui, Xavier Intes, Vikas Pandey  

**Link**: [PDF](https://arxiv.org/pdf/2505.21532)  

**Abstract**: Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology employed for distance and depth estimation across medical, automotive, and other fields, encounters significant computational challenges in scattering media. The complex nature of the acquired FLiDAR signal, particularly in such environments, makes isolating photon time-of-flight (related to target depth) and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the effectiveness of current analytical and computational methodologies. To overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE) framework tailored for specialized modeling of diverse temporal components. In contrast to the conventional MoE approaches our expert models are informed by underlying physics, such as the radiative transport equation governing photon propagation in scattering media. Central to our approach is EvidenceMoE, which integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess the reliability of each expert's output by providing per-expert quality scores and corrective feedback. A Decider Network then leverages this information to fuse expert predictions into a robust final estimate adaptively. We validate our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for non-invasive cancer cell depth detection generated from photon transport models in tissue. Our framework demonstrates strong performance, achieving a normalized root mean squared error (NRMSE) of 0.030 for depth estimation and 0.074 for fluorescence lifetime. 

**Abstract (ZH)**: 基于物理指导的专家混合模型框架：荧光LiDAR（FLiDAR）时间成分的专门建模 

---
# How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control 

**Title (ZH)**: 大型语言模型对人体动作的了解有多少？基于3D角色控制的案例研究 

**Authors**: Kunhang Li, Jason Naradowsky, Yansong Feng, Yusuke Miyao  

**Link**: [PDF](https://arxiv.org/pdf/2505.21531)  

**Abstract**: We explore Large Language Models (LLMs)' human motion knowledge through 3D avatar control. Given a motion instruction, we prompt LLMs to first generate a high-level movement plan with consecutive steps (High-level Planning), then specify body part positions in each step (Low-level Planning), which we linearly interpolate into avatar animations as a clear verification lens for human evaluators. Through carefully designed 20 representative motion instructions with full coverage of basic movement primitives and balanced body part usage, we conduct comprehensive evaluations including human assessment of both generated animations and high-level movement plans, as well as automatic comparison with oracle positions in low-level planning. We find that LLMs are strong at interpreting the high-level body movements but struggle with precise body part positioning. While breaking down motion queries into atomic components improves planning performance, LLMs have difficulty with multi-step movements involving high-degree-of-freedom body parts. Furthermore, LLMs provide reasonable approximation for general spatial descriptions, but fail to handle precise spatial specifications in text, and the precise spatial-temporal parameters needed for avatar control. Notably, LLMs show promise in conceptualizing creative motions and distinguishing culturally-specific motion patterns. 

**Abstract (ZH)**: 我们通过3D化身控制探索大型语言模型的人类动作知识。给出动作指令，我们促使语言模型首先生成包含连续步骤的高层运动计划（高层规划），然后在每个步骤中指定身体部位的位置（低层规划），并将这些信息线性插值为化身动画，作为人类评估者明确的验证窗口。通过精心设计的20个具有全面基本运动原始动作和平衡身体部位使用的代表性动作指令，我们进行了全面评估，包括人类对生成的动画和高层运动计划的评估，以及与低层规划中给定的位置进行自动比较。我们发现，语言模型在解释高层身体动作方面很强，但在精确的身体部位定位方面存在困难。尽管将动作查询分解成原子组件可以提高规划性能，但语言模型在涉及高自由度身体部位的多步骤动作处理中存在困难。此外，语言模型对一般空间描述提供合理的近似，但在处理文本中的精确空间规定以及用于化身控制的精确空间-时间参数方面存在问题。值得注意的是，语言模型在构思创造性动作并区分文化特定的动作模式方面具有潜力。 

---
# High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework 

**Title (ZH)**: 高保真功能超声重建：一种视觉自回归框架 

**Authors**: Xuhang Chen, Zhuo Li, Yanyan Shen, Mufti Mahmud, Hieu Pham, Chi-Man Pun, Shuqiang Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21530)  

**Abstract**: Functional ultrasound (fUS) imaging provides exceptional spatiotemporal resolution for neurovascular mapping, yet its practical application is significantly hampered by critical challenges. Foremost among these are data scarcity, arising from ethical considerations and signal degradation through the cranium, which collectively limit dataset diversity and compromise the fairness of downstream machine learning models. 

**Abstract (ZH)**: 功能性超声成像（fUS）提供了出色的时空分辨率用于神经血管映射，但其实际应用受到关键挑战的严重影响。其中最为显著的是由于伦理考虑和头骨引起的信号降解导致的数据稀缺性，这些共同限制了数据集的多样化并损害了下游机器学习模型的公平性。 

---
# UniDB++: Fast Sampling of Unified Diffusion Bridge 

**Title (ZH)**: UniDB++: 统一扩散桥的快速采样 

**Authors**: Mokai Pan, Kaizhen Zhu, Yuexin Ma, Yanwei Fu, Jingyi Yu, Jingya Wang, Ye Shi  

**Link**: [PDF](https://arxiv.org/pdf/2505.21528)  

**Abstract**: Diffusion Bridges enable transitions between arbitrary distributions, with the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's reliance on iterative Euler sampling methods results in slow, computationally expensive inference, while existing acceleration techniques for diffusion or diffusion bridge models fail to address its unique challenges: missing terminal mean constraints and SOC-specific penalty coefficients in its SDEs. We present UniDB++, a training-free sampling algorithm that significantly improves upon these limitations. The method's key advancement comes from deriving exact closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the error accumulation inherent in Euler approximations and enabling high-quality generation with up to 20$\times$ fewer sampling steps. This method is further complemented by replacing conventional noise prediction with a more stable data prediction model, along with an SDE-Corrector mechanism that maintains perceptual quality for low-step regimes (5-10 steps). Additionally, we demonstrate that UniDB++ aligns with existing diffusion bridge acceleration methods by evaluating their update rules, and UniDB++ can recover DBIMs as special cases under some theoretical conditions. Experiments demonstrate UniDB++'s state-of-the-art performance in image restoration tasks, outperforming Euler-based methods in fidelity and speed while reducing inference time significantly. This work bridges the gap between theoretical generality and practical efficiency in SOC-driven diffusion bridge models. Our code is available at this https URL. 

**Abstract (ZH)**: 统一扩散桥梁增强版（UniDB++）：通过精确反向时间SDE解提高采样效率 

---
# VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining 

**Title (ZH)**: VietASR：以50小时标注数据和大规模语音预训练实现行业级越南语ASR 

**Authors**: Jianheng Zhuo, Yifan Yang, Yiwen Shao, Yong Xu, Dong Yu, Kai Yu, Xie Chen  

**Link**: [PDF](https://arxiv.org/pdf/2505.21527)  

**Abstract**: Automatic speech recognition (ASR) has made remarkable progress but heavily relies on large-scale labeled data, which is scarce for low-resource languages like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve promising performance, their efficacy remains inadequate in terms of training costs, latency, and accessibility. To address these issues, we propose VietASR, a novel ASR training pipeline that leverages vast amounts of unlabeled data and a small set of labeled data. Through multi-iteration ASR-biased self-supervised learning on a large-scale unlabeled dataset, VietASR offers a cost-effective and practical solution for enhancing ASR performance. Experiments demonstrate that pre-training on 70,000-hour unlabeled data and fine-tuning on merely 50-hour labeled data yield a lightweight but powerful ASR model. It outperforms Whisper Large-v3 and commercial ASR systems on real-world data. Our code and models will be open-sourced to facilitate research in low-resource ASR. 

**Abstract (ZH)**: 自动语音识别（ASR）取得了显著进展，但仍然高度依赖大规模标注数据，这在低资源语言如越南语中极为稀缺。尽管现有的系统如Whisper、USM和MMS取得了令人瞩目的性能，但在训练成本、延迟和可访问性方面仍存在不足。为了解决这些问题，我们提出了VietASR，这是一种新颖的ASR训练管道，利用了大量的未标注数据和少量的标注数据。通过在大规模未标注数据集上进行多轮ASR偏置的自监督学习，VietASR 提供了一种成本效益高且实用的解决方案，以提升ASR性能。实验表明，使用70,000小时未标注数据进行预训练和仅使用50小时标注数据进行微调，可以获得轻量级但强大的ASR模型。该模型在实际数据上优于Whisper Large-v3和商用ASR系统。我们将开源我们的代码和模型，以促进低资源ASR领域的研究。 

---
# Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation 

**Title (ZH)**: 基于时空重 wiring 的源无关多变量时间序列领域自适应 

**Authors**: Peiliang Gong, Yucheng Wang, Min Wu, Zhenghua Chen, Xiaoli Li, Daoqiang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.21525)  

**Abstract**: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from an annotated source domain to an unlabelled target domain without accessing the source data, thereby preserving data privacy. While existing SFDA methods have proven effective in reducing reliance on source data, they struggle to perform well on multivariate time series (MTS) due to their failure to consider the intrinsic spatial correlations inherent in MTS data. These spatial correlations are crucial for accurately representing MTS data and preserving invariant information across domains. To address this challenge, we propose Temporal Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method tailored for MTS data. Specifically, TERSE comprises a customized spatial-temporal feature encoder designed to capture the underlying spatial-temporal characteristics, coupled with both temporal restoration and spatial rewiring tasks to reinstate latent representations of the temporally masked time series and the spatially masked correlated structures. During the target adaptation phase, the target encoder is guided to produce spatially and temporally consistent features with the source domain by leveraging the source pre-trained temporal restoration and spatial rewiring networks. Therefore, TERSE can effectively model and transfer spatial-temporal dependencies across domains, facilitating implicit feature alignment. In addition, as the first approach to simultaneously consider spatial-temporal consistency in MTS-SFDA, TERSE can also be integrated as a versatile plug-and-play module into established SFDA methods. Extensive experiments on three real-world time series datasets demonstrate the effectiveness and versatility of our approach. 

**Abstract (ZH)**: 源数据免费域适应（SFDA）旨在不访问源数据的情况下，将预训练模型从标注的源域适应到未标注的目标域，从而保障数据隐私。虽然现有的SFDA方法已经在减少对源数据的依赖方面证明了有效性，但由于其在处理多变量时间序列（MTS）时未能充分考虑MTS数据中存在的固有空间相关性，因此难以取得良好的表现。这些空间相关性对于准确表示MTS数据并保持跨域不变信息至关重要。为了解决这一挑战，我们提出了时空恢复与空间重连接（TERSE）方法，这是一种针对MTS数据设计的新型简洁的SFDA方法。具体而言，TERSE包括一个定制化的时空特征编码器，用于捕获潜在的时空特征，并结合时空恢复和空间重连接任务，以恢复时间掩蔽时间序列的隐含表示和空间掩蔽的相关结构。在目标适应阶段，目标编码器通过利用源预训练的时空恢复和空间重连接网络，被引导产生与源域时空一致的特征。因此，TERSE可以有效地建模和转移跨域的时空依赖性，促进隐含特征对齐。此外，作为首个同时考虑MTS-SFDA时空一致性的方法，TERSE也可以作为一个通用的即插即用模块集成到现有的SFDA方法中。在三个实际时间序列数据集上的广泛实验表明了我们方法的有效性和灵活性。 

---
# More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models 

**Title (ZH)**: 更多思考，更少观察？多模态推理模型中的放大幻觉评估 

**Authors**: Chengzhi Liu, Zhongxing Xu, Qingyue Wei, Juncheng Wu, James Zou, Xin Eric Wang, Yuyin Zhou, Sheng Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21523)  

**Abstract**: Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity. 

**Abstract (ZH)**: Test-time计算使多模态大型语言模型能够生成扩展的推理链，从而在多模态数学推理等任务上表现出色。然而，这种增强的推理能力通常伴随着较高的幻觉率：随着生成变得越来越长，模型往往会偏离图像相关的内容，更多地依赖语言先验。注意力分析显示，较长的推理链会导致对视觉输入的关注度降低，从而导致幻觉。为系统地研究这一现象，我们引入了RH-AUC这一度量标准，量化模型感知准确度随推理长度的变化，以评估模型在推理过程中是否保留了视觉接地。我们还发布了RH-Bench这一诊断基准，涵盖多种多模态任务，旨在评估推理能力和幻觉之间的权衡。我们的分析表明，(i) 较大的模型通常在推理和感知之间取得了更好的平衡，(ii) 这种平衡更多地受训练数据类型和领域的影响，而不是数据总量的影响。这些发现强调了同时考虑推理质量和感知保真的评估框架的重要性。 

---
# CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures 

**Title (ZH)**: CIM-NET：一种优化计算内存架构的视频去噪深度神经网络模型 

**Authors**: Shan Gao, Zhiqiang Wu, Yawen Niu, Xiaotao Li, Qingqing Xu  

**Link**: [PDF](https://arxiv.org/pdf/2505.21522)  

**Abstract**: While deep neural network (DNN)-based video denoising has demonstrated significant performance, deploying state-of-the-art models on edge devices remains challenging due to stringent real-time and energy efficiency requirements. Computing-in-Memory (CIM) chips offer a promising solution by integrating computation within memory cells, enabling rapid matrix-vector multiplication (MVM). However, existing DNN models are often designed without considering CIM architectural constraints, thus limiting their acceleration potential during inference. To address this, we propose a hardware-algorithm co-design framework incorporating two innovations: (1) a CIM-Aware Architecture, CIM-NET, optimized for large receptive field operation and CIM's crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator, CIM-CONV, used within CIM-NET to integrate slide-based processing with fully connected transformations for high-quality feature extraction and reconstruction. This framework significantly reduces the number of MVM operations, improving inference speed on CIM chips while maintaining competitive performance. Experimental results indicate that, compared to the conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM operations with a slight decrease in denoising performance. With a stride value of 8, CIM-NET reduces MVM operations to 1/77th of the original, while maintaining competitive PSNR (35.11 dB vs. 35.56 dB 

**Abstract (ZH)**: 基于深度神经网络的视频去噪已显示出显著性能，但在边缘设备上部署最先进的模型仍然具有挑战性，因其对实时性和能效有严格要求。Computing-in-Memory（CIM）芯片通过将计算集成到内存单元中，提供了快速矩阵-向量乘法（MVM）的机会，显示出有希望的解决方案。然而，现有的DNN模型通常没有考虑CIM架构约束，从而限制了其推理加速潜力。为解决这一问题，我们提出了一种硬件-算法协同设计框架，包含两项创新：（1）CIM感知架构CIM-NET，优化适用于大感受野操作和CIM基横线阵列加速的MVM；（2）伪卷积算子CIM-CONV，在CIM-NET中结合基于滑动的操作与全连接转换以进行高质量特征提取和重构。该框架显著减少了MVM操作次数，提高了CIM芯片上的推理速度，同时保持了竞争力。实验结果表明，与传统的轻量级模型FastDVDnet相比，CIM-NET在轻微降低去噪性能的同时，显著减少了MVM操作次数。在步幅值为8的情况下，CIM-NET将MVM操作减少到原来的1/77，同时保持与FastDVDnet相当的PSNR（35.11 dB vs. 35.56 dB）。 

---
# Do DeepFake Attribution Models Generalize? 

**Title (ZH)**: 深度伪造溯源模型具有泛化能力吗？ 

**Authors**: Spiros Baxavanakis, Manos Schinas, Symeon Papadopoulos  

**Link**: [PDF](https://arxiv.org/pdf/2505.21520)  

**Abstract**: Recent advancements in DeepFake generation, along with the proliferation of open-source tools, have significantly lowered the barrier for creating synthetic media. This trend poses a serious threat to the integrity and authenticity of online information, undermining public trust in institutions and media. State-of-the-art research on DeepFake detection has primarily focused on binary detection models. A key limitation of these models is that they treat all manipulation techniques as equivalent, despite the fact that different methods introduce distinct artifacts and visual cues. Only a limited number of studies explore DeepFake attribution models, although such models are crucial in practical settings. By providing the specific manipulation method employed, these models could enhance both the perceived trustworthiness and explainability for end users. In this work, we leverage five state-of-the-art backbone models and conduct extensive experiments across six DeepFake datasets. First, we compare binary and multi-class models in terms of cross-dataset generalization. Second, we examine the accuracy of attribution models in detecting seen manipulation methods in unknown datasets, hence uncovering data distribution shifts on the same DeepFake manipulations. Last, we assess the effectiveness of contrastive methods in improving cross-dataset generalization performance. Our findings indicate that while binary models demonstrate better generalization abilities, larger models, contrastive methods, and higher data quality can lead to performance improvements in attribution models. The code of this work is available on GitHub. 

**Abstract (ZH)**: 近期DeepFake生成技术的进步以及开源工具的普及大大降低了合成媒体的生成门槛。这一趋势对在线信息的完整性和真实性构成了严重威胁，削弱了公众对机构和媒体的信任。目前关于DeepFake检测的先进研究主要集中在二元检测模型上。这些模型的一个主要局限性在于，它们将所有篡改技术视为等价的，尽管不同的方法会引入不同的特征和视觉线索。尽管如此，仅有限的研究探讨了DeepFake归因模型，而这些模型在实际应用中至关重要。通过提供具体使用的篡改方法，这些模型可以增强最终用户的可信度和解释性。在本文中，我们利用五种最先进的骨干模型，在六个DeepFake数据集中进行了广泛的实验。首先，我们比较了二元模型和多类别模型在跨数据集泛化的性能。其次，我们研究了归因模型在未知数据集上检测已知篡改方法的准确性，从而揭示在相同DeepFake篡改下的数据分布变化。最后，我们评估了对比方法在提高跨数据集泛化性能方面的有效性。我们的研究发现，虽然二元模型在泛化能力方面表现更好，但更大规模的模型、对比方法和更高的数据质量可以在归因模型中带来性能提升。本文的代码可在GitHub上获取。 

---
# Enhancing Vision Transformer Explainability Using Artificial Astrocytes 

**Title (ZH)**: 使用人工小胶质细胞增强视觉变换器的可解释性 

**Authors**: Nicolas Echevarrieta-Catalan, Ana Ribas-Rodriguez, Francisco Cedron, Odelia Schwartz, Vanessa Aguiar-Pulido  

**Link**: [PDF](https://arxiv.org/pdf/2505.21513)  

**Abstract**: Machine learning models achieve high precision, but their decision-making processes often lack explainability. Furthermore, as model complexity increases, explainability typically decreases. Existing efforts to improve explainability primarily involve developing new eXplainable artificial intelligence (XAI) techniques or incorporating explainability constraints during training. While these approaches yield specific improvements, their applicability remains limited. In this work, we propose the Vision Transformer with artificial Astrocytes (ViTA). This training-free approach is inspired by neuroscience and enhances the reasoning of a pretrained deep neural network to generate more human-aligned explanations. We evaluated our approach employing two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the similarity between the heatmaps produced by the XAI techniques and a (human-aligned) ground truth. Our results consistently demonstrate that incorporating artificial astrocytes enhances the alignment of model explanations with human perception, leading to statistically significant improvements across all XAI techniques and metrics utilized. 

**Abstract (ZH)**: Vision Transformer with Artificial Astrocytes (ViTA)提高解释性的人工智能方法 

---
# Conformance Checking for Less: Efficient Conformance Checking for Long Event Sequences 

**Title (ZH)**: 少些合规检查：长事件序列的高效合规检查 

**Authors**: Eli Bogdanov, Izack Cohen, Avigdor Gal  

**Link**: [PDF](https://arxiv.org/pdf/2505.21506)  

**Abstract**: Long event sequences (termed traces) and large data logs that originate from sensors and prediction models are becoming increasingly common in our data-rich world. In such scenarios, conformance checking-validating a data log against an expected system behavior (the process model) can become computationally infeasible due to the exponential complexity of finding an optimal alignment. To alleviate scalability challenges for this task, we propose ConLES, a sliding-window conformance checking approach for long event sequences that preserves the interpretability of alignment-based methods. ConLES partitions traces into manageable subtraces and iteratively aligns each against the expected behavior, leading to significant reduction of the search space while maintaining overall accuracy. We use global information that captures structural properties of both the trace and the process model, enabling informed alignment decisions and discarding unpromising alignments, even if they appear locally optimal. Performance evaluations across multiple datasets highlight that ConLES outperforms the leading optimal and heuristic algorithms for long traces, consistently achieving the optimal or near-optimal solution. Unlike other conformance methods that struggle with long event sequences, ConLES significantly reduces the search space, scales efficiently, and uniquely supports both predefined and discovered process models, making it a viable and leading option for conformance checking of long event sequences. 

**Abstract (ZH)**: 长事件序列的滑动窗口一致性检查方法：ConLES 

---
# Offset Unlearning for Large Language Models 

**Title (ZH)**: 大型语言模型中的偏见消除训练 

**Authors**: James Y. Huang, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang, Hoifung Poon, Muhao Chen  

**Link**: [PDF](https://arxiv.org/pdf/2404.11045)  

**Abstract**: Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, biased, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose {\delta}-Unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, {\delta}-Unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that {\delta}- Unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. {\delta}-Unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs. 

**Abstract (ZH)**: 尽管大型语言模型（LLMs）在从训练语料库中获取知识方面具有强大能力，但语料库中敏感信息（如版权、偏见和私有内容）的保留引起了伦理和法律方面的担忧。针对这些挑战，遗忘技术作为一种前景广阔的方法应运而生，以解决受问题训练数据影响的LLMs。然而，先前的遗忘技术要么由于需要访问模型内部权重而不适用于黑盒LLMs，要么通过保留敏感数据以供推理时纠正而违反数据保护原则。我们提出{\delta}-遗忘，一种适用于黑盒LLMs的偏移遗忘框架。{\delta}-遗忘通过对比一对较小模型的logit值来学习所需的遗忘偏移量，而不是直接调整个黑盒LLM本身。实验表明，{\delta}-遗忘可以在保持相似或更强的一般超出遗忘范围任务性能的同时，有效遗忘目标数据。{\delta}-遗忘还能够有效整合不同的遗忘算法，使我们的方法成为适应各种现有遗忘算法以适用于黑盒LLMs的通用解决方案。 

---
