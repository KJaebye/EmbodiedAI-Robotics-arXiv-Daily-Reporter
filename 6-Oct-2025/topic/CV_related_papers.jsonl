{'arxiv_id': 'arXiv:2510.03011', 'title': '3D-CovDiffusion: 3D-Aware Diffusion Policy for Coverage Path Planning', 'authors': 'Chenyuan Chen, Haoran Ding, Ran Ding, Tianyu Liu, Zewen He, Anqing Duan, Dezhen Song, Xiaodan Liang, Yoshihiko Nakamura', 'link': 'https://arxiv.org/abs/2510.03011', 'abstract': 'Diffusion models, as a class of deep generative models, have recently emerged as powerful tools for robot skills by enabling stable training with reliable convergence. In this paper, we present an end-to-end framework for generating long, smooth trajectories that explicitly target high surface coverage across various industrial tasks, including polishing, robotic painting, and spray coating. The conventional methods are always fundamentally constrained by their predefined functional forms, which limit the shapes of the trajectories they can represent and make it difficult to handle complex and diverse tasks. Moreover, their generalization is poor, often requiring manual redesign or extensive parameter tuning when applied to new scenarios. These limitations highlight the need for more expressive generative models, making diffusion-based approaches a compelling choice for trajectory generation. By iteratively denoising trajectories with carefully learned noise schedules and conditioning mechanisms, diffusion models not only ensure smooth and consistent motion but also flexibly adapt to the task context. In experiments, our method improves trajectory continuity, maintains high coverage, and generalizes to unseen shapes, paving the way for unified end-to-end trajectory learning across industrial surface-processing tasks without category-specific models. On average, our approach improves Point-wise Chamfer Distance by 98.2\\% and smoothness by 97.0\\%, while increasing surface coverage by 61\\% compared to prior methods. The link to our code can be found \\href{this https URL}{here}.', 'abstract_zh': '基于扩散模型的端到端轨迹生成框架：面向工业任务的高表面覆盖率平滑轨迹生成', 'title_zh': '3D-CovDiffusion: 三维导向的覆盖路径规划扩散策略'}
{'arxiv_id': 'arXiv:2510.02627', 'title': 'A Trajectory Generator for High-Density Traffic and Diverse Agent-Interaction Scenarios', 'authors': 'Ruining Yang, Yi Xu, Yixiao Chen, Yun Fu, Lili Su', 'link': 'https://arxiv.org/abs/2510.02627', 'abstract': 'Accurate trajectory prediction is fundamental to autonomous driving, as it underpins safe motion planning and collision avoidance in complex environments. However, existing benchmark datasets suffer from a pronounced long-tail distribution problem, with most samples drawn from low-density scenarios and simple straight-driving behaviors. This underrepresentation of high-density scenarios and safety critical maneuvers such as lane changes, overtaking and turning is an obstacle to model generalization and leads to overly optimistic evaluations. To address these challenges, we propose a novel trajectory generation framework that simultaneously enhances scenarios density and enriches behavioral diversity. Specifically, our approach converts continuous road environments into a structured grid representation that supports fine-grained path planning, explicit conflict detection, and multi-agent coordination. Built upon this representation, we introduce behavior-aware generation mechanisms that combine rule-based decision triggers with Frenet-based trajectory smoothing and dynamic feasibility constraints. This design allows us to synthesize realistic high-density scenarios and rare behaviors with complex interactions that are often missing in real data. Extensive experiments on the large-scale Argoverse 1 and Argoverse 2 datasets demonstrate that our method significantly improves both agent density and behavior diversity, while preserving motion realism and scenario-level safety. Our synthetic data also benefits downstream trajectory prediction models and enhances performance in challenging high-density scenarios.', 'abstract_zh': '准确的轨迹预测是自主驾驶的基础，因为它对于在复杂环境中的安全运动规划和碰撞避免至关重要。然而，现有的基准数据集存在明显的长尾分布问题，大多数样本来自低密度场景和简单的直线驾驶行为。这种对高密度场景和诸如变道、超车、转弯等关键安全操作的缺乏代表，阻碍了模型的泛化，并导致过于乐观的评估。为解决这些挑战，我们提出了一种新的轨迹生成框架，该框架同时增强了场景密度并丰富了行为多样性。具体而言，我们的方法将连续的道路环境转换为结构化的网格表示，支持精细路规划、明确的冲突检测和多Agent协调。在此表示的基础上，我们引入了意识行为生成机制，结合基于规则的决策触发器和Frenet轨迹平滑以及动态可行性约束。该设计使我们能够合成具有复杂交互的真实高密度场景和稀有行为，而在真实数据中这些交互往往缺失。在大规模的Argoverse 1和Argoverse 2数据集上的广泛实验表明，我们的方法在提高Agent密度和行为多样性方面取得了显著提升，同时保持了运动的真实性和场景级别的安全性。我们的合成数据也对下游轨迹预测模型有好处，并在具有挑战性的高密度场景中提高了性能。', 'title_zh': '高密度交通及多样化代理交互场景的轨迹生成器'}
{'arxiv_id': 'arXiv:2510.02469', 'title': 'SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting', 'authors': 'Sung-Yeon Park, Adam Lee, Juanwu Lu, Can Cui, Luyang Jiang, Rohit Gupta, Kyungtae Han, Ahmadreza Moradipari, Ziran Wang', 'link': 'https://arxiv.org/abs/2510.02469', 'abstract': "Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: this https URL", 'abstract_zh': '基于传感器数据的驾驶场景编辑正 emerges as a promising alternative to traditional虚拟驾驶模拟器。然而，现有的框架由于编辑能力有限，难以高效生成逼真的场景。为应对这些挑战，我们提出了SIMSplat，一种与语言对齐的高斯点编辑器，作为一种语言控制的编辑器，SIMSplat能够使用自然语言提示进行直观操作。通过将语言与高斯重建的场景对齐，它还支持直接查询道路对象，从而实现精确和灵活的编辑。该方法提供了详细的对象级编辑，包括添加新对象和修改车辆和行人的轨迹，并通过多智能体运动预测进行预测路径细化，以生成场景中所有智能体之间的逼真交互。在Waymo数据集上的实验展示了SIMSplat广泛的编辑能力和在各种场景中的适应性。项目页面: this https URL。', 'title_zh': 'SIMSplat: 基于语言对齐4D高斯点积的预测性驾驶场景编辑'}
{'arxiv_id': 'arXiv:2510.03135', 'title': 'Mask2IV: Interaction-Centric Video Generation via Mask Trajectories', 'authors': 'Gen Li, Bo Zhao, Jianfei Yang, Laura Sevilla-Lara', 'link': 'https://arxiv.org/abs/2510.03135', 'abstract': 'Generating interaction-centric videos, such as those depicting humans or robots interacting with objects, is crucial for embodied intelligence, as they provide rich and diverse visual priors for robot learning, manipulation policy training, and affordance reasoning. However, existing methods often struggle to model such complex and dynamic interactions. While recent studies show that masks can serve as effective control signals and enhance generation quality, obtaining dense and precise mask annotations remains a major challenge for real-world use. To overcome this limitation, we introduce Mask2IV, a novel framework specifically designed for interaction-centric video generation. It adopts a decoupled two-stage pipeline that first predicts plausible motion trajectories for both actor and object, then generates a video conditioned on these trajectories. This design eliminates the need for dense mask inputs from users while preserving the flexibility to manipulate the interaction process. Furthermore, Mask2IV supports versatile and intuitive control, allowing users to specify the target object of interaction and guide the motion trajectory through action descriptions or spatial position cues. To support systematic training and evaluation, we curate two benchmarks covering diverse action and object categories across both human-object interaction and robotic manipulation scenarios. Extensive experiments demonstrate that our method achieves superior visual realism and controllability compared to existing baselines.', 'abstract_zh': 'Mask2IV：一种用于交互中心视频生成的新型框架', 'title_zh': 'Mask2IV: 基于交互的视频生成通过掩码轨迹'}
{'arxiv_id': 'arXiv:2510.03104', 'title': 'Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields', 'authors': 'Zhiting Mei, Ola Shorinwa, Anirudha Majumdar', 'link': 'https://arxiv.org/abs/2510.03104', 'abstract': 'Semantic distillation in radiance fields has spurred significant advances in open-vocabulary robot policies, e.g., in manipulation and navigation, founded on pretrained semantics from large vision models. While prior work has demonstrated the effectiveness of visual-only semantic features (e.g., DINO and CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit of geometry-grounding in distilled fields remains an open question. In principle, visual-geometry features seem very promising for spatial tasks such as pose estimation, prompting the question: Do geometry-grounded semantic features offer an edge in distilled fields? Specifically, we ask three critical questions: First, does spatial-grounding produce higher-fidelity geometry-aware semantic features? We find that image features from geometry-grounded backbones contain finer structural details compared to their counterparts. Secondly, does geometry-grounding improve semantic object localization? We observe no significant difference in this task. Thirdly, does geometry-grounding enable higher-accuracy radiance field inversion? Given the limitations of prior work and their lack of semantics integration, we propose a novel framework SPINE for inverting radiance fields without an initial guess, consisting of two core components: coarse inversion using distilled semantics, and fine inversion using photometric-based optimization. Surprisingly, we find that the pose estimation accuracy decreases with geometry-grounded features. Our results suggest that visual-only features offer greater versatility for a broader range of downstream tasks, although geometry-grounded features contain more geometric detail. Notably, our findings underscore the necessity of future research on effective strategies for geometry-grounding that augment the versatility and performance of pretrained semantic features.', 'abstract_zh': '在辐射场中的语义蒸馏促进了基于大型视觉模型预训练语义的开放词汇机器人策略的重大进展，如操作和导航方面。虽然前期工作表明仅视觉语义特征（如DINO和CLIP）在Gaussian Splatting和神经辐射场中的有效性，但在蒸馏场中几何约束潜在的好处仍然是一个开放问题。鉴于视觉-几何特征在空间任务如姿态估计方面的前景，我们提出了一个问题：几何约束的语义特征是否在蒸馏场中更具优势？具体而言，我们提出了三个关键问题：首先，空间约束是否能产生更高保真度的几何感知语义特征？我们发现，基于几何约束的骨干网络的图像特征比其对应特征包含更精细的结构细节。其次，几何约束是否能提高语义物体定位的准确性？我们没有观察到显著差异。最后，几何约束是否能提高辐射场反转的准确性？鉴于前期工作的局限性和缺乏语义集成，我们提出了一种新的框架SPINE，用于在没有初始猜测的情况下反转辐射场，该框架由两个核心组件组成：粗略反转使用蒸馏的语义，精细反转使用基于光度的优化。令人惊讶的是，我们发现姿态估计精度随着几何约束特征的增加而降低。我们的结果表明，仅视觉特征在更广泛下游任务中更具灵活性，尽管几何约束特征包含更多的几何细节。值得注意的是，我们的发现突显了未来研究有效几何约束策略的必要性，以增强预训练语义特征的灵活性和性能。', 'title_zh': '几何与视觉相遇：重返蒸馏字段中的预训练语义'}
{'arxiv_id': 'arXiv:2510.02423', 'title': 'RefineShot: Rethinking Cinematography Understanding with Foundational Skill Evaluation', 'authors': 'Hang Wu, Yujun Cai, Haonan Ge, Hongkai Chen, Ming-Hsuan Yang, Yiwei Wang', 'link': 'https://arxiv.org/abs/2510.02423', 'abstract': "Cinematography understanding refers to the ability to recognize not only the visual content of a scene but also the cinematic techniques that shape narrative meaning. This capability is attracting increasing attention, as it enhances multimodal understanding in real-world applications and underpins coherent content creation in film and media. As the most comprehensive benchmark for this task, ShotBench spans a wide range of cinematic concepts and VQA-style evaluations, with ShotVL achieving state-of-the-art results on it. However, our analysis reveals that ambiguous option design in ShotBench and ShotVL's shortcomings in reasoning consistency and instruction adherence undermine evaluation reliability, limiting fair comparison and hindering future progress. To overcome these issues, we systematically refine ShotBench through consistent option restructuring, conduct the first critical analysis of ShotVL's reasoning behavior, and introduce an extended evaluation protocol that jointly assesses task accuracy and core model competencies. These efforts lead to RefineShot, a refined and expanded benchmark that enables more reliable assessment and fosters future advances in cinematography understanding.", 'abstract_zh': 'cinematography理解指的是不仅识别人物场景中的视觉内容，还能识别塑造叙事意义的影视技术的能力。随着这一能力在实际应用中增强多模态理解并在电影和媒体中支持内容创作方面的重要性不断增加，这引起了越来越多的关注。作为该任务最全面的基准，ShotBench涵盖了广泛的影视概念和基于VQA的评估，ShotVL在其中达到了最先进的技术水平。然而，我们的分析表明，ShotBench中含糊不清的选项设计以及ShotVL在推理一致性和指令遵守方面的不足，损害了评价的可靠性，限制了公平比较并妨碍了未来的发展。为克服这些问题，我们系统地改进了ShotBench，通过一致的选项重组进行了第一次对ShotVL推理行为的重要分析，并引入了一种综合评估协议，该协议联合评估任务准确性和核心模型能力。这些努力导致了RefineShot，一个改进和扩展的基准，能够进行更可靠的评估并促进cinematography理解的未来进步。', 'title_zh': 'RefineShot: 从基础技能评估重新思考 cinematography 理解'}
{'arxiv_id': 'arXiv:2510.03216', 'title': 'Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image Segmentation', 'authors': 'Talha Ahmed, Nehal Ahmed Shaikh, Hassan Mohy-ud-Din', 'link': 'https://arxiv.org/abs/2510.03216', 'abstract': 'For equitable deployment of AI tools in hospitals and healthcare facilities, we need Deep Segmentation Networks that offer high performance and can be trained on cost-effective GPUs with limited memory and large batch sizes. In this work, we propose Wave-GMS, a lightweight and efficient multi-scale generative model for medical image segmentation. Wave-GMS has a substantially smaller number of trainable parameters, does not require loading memory-intensive pretrained vision foundation models, and supports training with large batch sizes on GPUs with limited memory. We conducted extensive experiments on four publicly available datasets (BUS, BUSI, Kvasir-Instrument, and HAM10000), demonstrating that Wave-GMS achieves state-of-the-art segmentation performance with superior cross-domain generalizability, while requiring only ~2.6M trainable parameters. Code is available at this https URL.', 'abstract_zh': '公平部署医院和医疗设施中的人工智能工具：一种轻量高效的多尺度生成模型Wave-GMS及其医学图像分割应用', 'title_zh': 'Wave-GMS：轻量级多尺度生成模型用于医学图像分割'}
{'arxiv_id': 'arXiv:2510.03122', 'title': 'HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion', 'authors': 'Shiyi Zhang, Dong Liang, Hairong Zheng, Yihang Zhou', 'link': 'https://arxiv.org/abs/2510.03122', 'abstract': 'The reconstruction of visual information from brain activity fosters interdisciplinary integration between neuroscience and computer vision. However, existing methods still face challenges in accurately recovering highly complex visual stimuli. This difficulty stems from the characteristics of natural scenes: low-level features exhibit heterogeneity, while high-level features show semantic entanglement due to contextual overlaps. Inspired by the hierarchical representation theory of the visual cortex, we propose the HAVIR model, which separates the visual cortex into two hierarchical regions and extracts distinct features from each. Specifically, the Structural Generator extracts structural information from spatial processing voxels and converts it into latent diffusion priors, while the Semantic Extractor converts semantic processing voxels into CLIP embeddings. These components are integrated via the Versatile Diffusion model to synthesize the final image. Experimental results demonstrate that HAVIR enhances both the structural and semantic quality of reconstructions, even in complex scenes, and outperforms existing models.', 'abstract_zh': '从大脑活动重建视觉信息促进神经科学与计算机视觉的跨学科整合。然而，现有方法在准确恢复高度复杂的视觉刺激方面仍然面临挑战。这一困难源于自然场景的特征：低级特征表现出异质性，而高级特征由于上下文重叠则表现出语义交织。受视皮质分层表示理论的启发，我们提出了HAVIR模型，将视皮质分为两个分层区域，并从每个区域中提取不同的特征。具体来说，结构生成器从空间处理体素中提取结构信息并转化为潜在扩散先验，而语义提取器将语义处理体素转化为CLIP嵌入。这些组件通过通用扩散模型综合，以合成最终图像。实验结果表明，HAVIR在复杂场景中增强了重建的结构和语义质量，并优于现有模型。', 'title_zh': 'HAVIR: 层次视觉引导的图像重建方法，基于CLIP指导的多功能扩散模型'}
{'arxiv_id': 'arXiv:2510.03075', 'title': 'What Drives Compositional Generalization in Visual Generative Models?', 'authors': 'Karim Farid, Rajat Sahay, Yumna Ali Alnaggar, Simon Schrodi, Volker Fischer, Cordelia Schmid, Thomas Brox', 'link': 'https://arxiv.org/abs/2510.03075', 'abstract': 'Compositional generalization, the ability to generate novel combinations of known concepts, is a key ingredient for visual generative models. Yet, not all mechanisms that enable or inhibit it are fully understood. In this work, we conduct a systematic study of how various design choices influence compositional generalization in image and video generation in a positive or negative way. Through controlled experiments, we identify two key factors: (i) whether the training objective operates on a discrete or continuous distribution, and (ii) to what extent conditioning provides information about the constituent concepts during training. Building on these insights, we show that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based objective can improve compositional performance in discrete models like MaskGIT.', 'abstract_zh': '组成泛化能力，即生成已知概念新颖组合的能力，是视觉生成模型的关键要素。然而，并非所有促进或抑制这种能力的机制都已被完全理解。在本工作中，我们系统地研究了各种设计选择是如何以积极或消极的方式影响图像和视频生成中的组成泛化能力。通过受控实验，我们识别出两个关键因素：(i) 训练目标作用于离散还是连续分布，以及(ii) 条件信息在训练过程中提供给组成概念的程度。基于这些见解，我们显示，通过使用辅助连续JEPA基目标放宽MaskGIT的离散损失，可以改进像MaskGIT这样的离散模型的组成性能。', 'title_zh': '视觉生成模型中组分泛化的驱动因素探究'}
{'arxiv_id': 'arXiv:2510.03049', 'title': 'When and Where do Events Switch in Multi-Event Video Generation?', 'authors': 'Ruotong Liao, Guowen Huang, Qing Cheng, Thomas Seidl, Daniel Cremers, Volker Tresp', 'link': 'https://arxiv.org/abs/2510.03049', 'abstract': 'Text-to-video (T2V) generation has surged in response to challenging questions, especially when a long video must depict multiple sequential events with temporal coherence and controllable content. Existing methods that extend to multi-event generation omit an inspection of the intrinsic factor in event shifting. The paper aims to answer the central question: When and where multi-event prompts control event transition during T2V generation. This work introduces MEve, a self-curated prompt suite for evaluating multi-event text-to-video (T2V) generation, and conducts a systematic study of two representative model families, i.e., OpenSora and CogVideoX. Extensive experiments demonstrate the importance of early intervention in denoising steps and block-wise model layers, revealing the essential factor for multi-event video generation and highlighting the possibilities for multi-event conditioning in future models.', 'abstract_zh': '文本到视频（T2V）生成在应对挑战性问题时蓬勃发展，尤其是在长视频必须描绘多个具有时间连贯性和可控内容的序列事件时。现有方法虽然能够扩展到多事件生成，但忽略了事件转换内在因素的考察。本文旨在回答核心问题：在T2V生成过程中，多事件提示何时及如何控制事件过渡。本文引入了MEve，一个自营收集的提示套件，用于评估多事件文本到视频（T2V）生成，并对两个代表性的模型家族，即OpenSora和CogVideoX，进行了系统的研究。广泛的实验表明，在去噪步骤和块状模型层中早期干预的重要性，揭示了多事件视频生成的关键因素，并指出了未来模型中多事件条件的可能性。', 'title_zh': '多事件视频生成中，事件何时以及在何地切换？'}
{'arxiv_id': 'arXiv:2510.02869', 'title': 'Representing Beauty: Towards a Participatory but Objective Latent Aesthetics', 'authors': 'Alexander Michael Rusnak', 'link': 'https://arxiv.org/abs/2510.02869', 'abstract': 'What does it mean for a machine to recognize beauty? While beauty remains a culturally and experientially compelling but philosophically elusive concept, deep learning systems increasingly appear capable of modeling aesthetic judgment. In this paper, we explore the capacity of neural networks to represent beauty despite the immense formal diversity of objects for which the term applies. By drawing on recent work on cross-model representational convergence, we show how aesthetic content produces more similar and aligned representations between models which have been trained on distinct data and modalities - while unaesthetic images do not produce more aligned representations. This finding implies that the formal structure of beautiful images has a realist basis - rather than only as a reflection of socially constructed values. Furthermore, we propose that these realist representations exist because of a joint grounding of aesthetic form in physical and cultural substance. We argue that human perceptual and creative acts play a central role in shaping these the latent spaces of deep learning systems, but that a realist basis for aesthetics shows that machines are not mere creative parrots but can produce novel creative insights from the unique vantage point of scale. Our findings suggest that human-machine co-creation is not merely possible, but foundational - with beauty serving as a teleological attractor in both cultural production and machine perception.', 'abstract_zh': '机器如何识别美：一种含义及其背后的实证基础探究', 'title_zh': '呈现美：向着一种参与式但客观的潜在美学方向探索'}
{'arxiv_id': 'arXiv:2510.02692', 'title': 'Fine-Tuning Diffusion Models via Intermediate Distribution Shaping', 'authors': 'Gautham Govind Anil, Shaan Ul Haque, Nithish Kannen, Dheeraj Nagaraj, Sanjay Shakkottai, Karthikeyan Shanmugam', 'link': 'https://arxiv.org/abs/2510.02692', 'abstract': 'Diffusion models are widely used for generative tasks across domains. While pre-trained diffusion models effectively capture the training data distribution, it is often desirable to shape these distributions using reward functions to align with downstream applications. Policy gradient methods, such as Proximal Policy Optimization (PPO), are widely used in the context of autoregressive generation. However, the marginal likelihoods required for such methods are intractable for diffusion models, leading to alternative proposals and relaxations. In this context, we unify variants of Rejection sAmpling based Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with reshaped rewards. We then introduce P-GRAFT to shape distributions at intermediate noise levels and demonstrate empirically that this can lead to more effective fine-tuning. We mathematically explain this via a bias-variance tradeoff. Motivated by this, we propose inverse noise correction to improve flow models without leveraging explicit rewards. We empirically evaluate our methods on text-to-image(T2I) generation, layout generation, molecule generation and unconditional image generation. Notably, our framework, applied to Stable Diffusion 2, improves over policy gradient methods on popular T2I benchmarks in terms of VQAScore and shows an $8.81\\%$ relative improvement over the base model. For unconditional image generation, inverse noise correction improves FID of generated images at lower FLOPs/image.', 'abstract_zh': '扩散模型在跨领域生成任务中广泛应用。虽然预训练的扩散模型能够有效地捕捉训练数据分布，但在下游应用中常常需要使用奖励函数来调整这些分布。策略梯度方法，如近端策略优化（PPO），在自回归生成的上下文中广泛应用。然而，此类方法所需的边际似然对于扩散模型来说通常是不可计算的，从而导致了替代方案和放松。在此背景下，我们将排斥采样基于微调（RAFT）的各种变体统一为GRAFT，并表明这实际上是使用重塑后的奖励隐式执行PPO。然后，我们介绍了P-GRAFT，在中间噪声水平下重塑分布，并通过经验表明这可以导致更有效的微调。我们通过偏差-方差权衡的数学解释来说明这一点。受此启发，我们提出了逆噪声校正来提高流动模型的性能，而无需利用显式的奖励。我们在文本到图像生成、布局生成、分子生成和无条件图像生成中实验评估了我们的方法。值得注意的是，将我们的框架应用于Stable Diffusion 2，在流行的文字到图像基准测试中，VQAScore上优于策略梯度方法，并且相对于基础模型的相对改进达到8.81%。对于无条件图像生成，逆噪声校正降低了每张图像计算量的同时提高了生成图像的FID。', 'title_zh': '通过中间分布塑造 fine-tuning 微调扩散模型'}
{'arxiv_id': 'arXiv:2510.02571', 'title': 'How Confident are Video Models? Empowering Video Models to Express their Uncertainty', 'authors': 'Zhiting Mei, Ola Shorinwa, Anirudha Majumdar', 'link': 'https://arxiv.org/abs/2510.02571', 'abstract': 'Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents.', 'abstract_zh': '生成式视频模型展示了令人印象深刻的文本到视频能力，推动了其在许多实际应用中的广泛应用。然而，就像大型语言模型（LLMs）一样，视频生成模型往往会产生幻觉，即使在事实不正确的情况下也能生成看似合理的视频。尽管先前的研究已经对LLMs进行了广泛的不确定量化（UQ）研究，但尚不存在对视频模型的UQ方法，这引发了重要的安全问题。据我们所知，本文代表了对视频模型的不确定性量化进行研究的第一项工作。我们提出了一种生成式视频模型的不确定性量化框架，包括：（i）一种基于鲁棒秩相关估计的不确定性校准度量，不需要严格的建模假设；（ii）一种针对视频模型的黑箱不确定性量化方法（称为S-QUBED），该方法利用潜在建模来严格地将预测不确定性分解为其aleatoric和epistemic组成部分；以及（iii）一种用于在视频模型中促进校准基准测试的不确定性量化数据集。通过在潜在空间中条件化生成任务，我们将由模糊的任务规范引起的不确定性与由知识不足引起的不确定性区分开来。通过对基准视频数据集进行广泛的实验，我们证明S-QUBED计算了与任务准确性负相关的校准总不确定性估计，并且能够有效地计算aleatoric和epistemic组成部分。', 'title_zh': '视频模型有多确定？使视频模型表达其不确定性'}
{'arxiv_id': 'arXiv:2510.02390', 'title': 'Hyperparameters are all you need: Using five-step inference for an original diffusion model to generate images comparable to the latest distillation model', 'authors': 'Zilai Li', 'link': 'https://arxiv.org/abs/2510.02390', 'abstract': 'The diffusion model is a state-of-the-art generative model that generates an image by applying a neural network iteratively. Moreover, this generation process is regarded as an algorithm solving an ordinary differential equation or a stochastic differential equation. Based on the analysis of the truncation error of the diffusion ODE and SDE, our study proposes a training-free algorithm that generates high-quality 512 x 512 and 1024 x 1024 images in eight steps, with flexible guidance scales. To the best of my knowledge, our algorithm is the first one that samples a 1024 x 1024 resolution image in 8 steps with an FID performance comparable to that of the latest distillation model, but without additional training. Meanwhile, our algorithm can also generate a 512 x 512 image in 8 steps, and its FID performance is better than the inference result using state-of-the-art ODE solver DPM++ 2m in 20 steps. We validate our eight-step image generation algorithm using the COCO 2014, COCO 2017, and LAION datasets. And our best FID performance is 15.7, 22.35, and 17.52. While the FID performance of DPM++2m is 17.3, 23.75, and 17.33. Further, it also outperforms the state-of-the-art AMED-plugin solver, whose FID performance is 19.07, 25.50, and 18.06. We also apply the algorithm in five-step inference without additional training, for which the best FID performance in the datasets mentioned above is 19.18, 23.24, and 19.61, respectively, and is comparable to the performance of the state-of-the-art AMED Pulgin solver in eight steps, SDXL-turbo in four steps, and the state-of-the-art diffusion distillation model Flash Diffusion in five steps. We also validate our algorithm in synthesizing 1024 * 1024 images within 6 steps, whose FID performance only has a limited distance to the latest distillation algorithm. The code is in repo: this https URL', 'abstract_zh': '基于扩散ODE和SDE裁剪误差的训练-free高分辨率图像生成算法', 'title_zh': '你需要的所有超参数：使用五步推理生成与最新蒸馏模型相媲美的图像的原始扩散模型'}
