{'arxiv_id': 'arXiv:2509.19105', 'title': 'Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation', 'authors': 'Sarvesh Prajapati, Ananya Trivedi, Nathaniel Hanson, Bruce Maxwell, Taskin Padir', 'link': 'https://arxiv.org/abs/2509.19105', 'abstract': 'Successful navigation in outdoor environments requires accurate prediction of the physical interactions between the robot and the terrain. To this end, several methods rely on geometric or semantic labels to classify traversable surfaces. However, such labels cannot distinguish visually similar surfaces that differ in material properties. Spectral sensors enable inference of material composition from surface reflectance measured across multiple wavelength bands. Although spectral sensing is gaining traction in robotics, widespread deployment remains constrained by the need for custom hardware integration, high sensor costs, and compute-intensive processing pipelines. In this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net), a deep neural network designed to bridge the gap between the accessibility of RGB sensing and the rich material information provided by spectral data. RS-Net predicts spectral signatures from RGB patches, which we map to terrain labels and friction coefficients. The resulting terrain classifications are integrated into a sampling-based motion planner for a wheeled robot operating in outdoor environments. Likewise, the friction estimates are incorporated into a contact-force-based MPC for a quadruped robot navigating slippery surfaces. Thus, we introduce a framework that learns the task-relevant physical property once during training and thereafter relies solely on RGB sensing at test time. The code is available at this https URL.', 'abstract_zh': '基于RGB图像到光谱签名的神经网络（RS-Net）：桥梁感知与光谱数据丰富材料信息的 gap', 'title_zh': '基于RGB图像的光谱特征图映射及其地形感知导航'}
{'arxiv_id': 'arXiv:2509.18979', 'title': 'Category-Level Object Shape and Pose Estimation in Less Than a Millisecond', 'authors': 'Lorenzo Shaikewitz, Tim Nguyen, Luca Carlone', 'link': 'https://arxiv.org/abs/2509.18979', 'abstract': "Object shape and pose estimation is a foundational robotics problem, supporting tasks from manipulation to scene understanding and navigation. We present a fast local solver for shape and pose estimation which requires only category-level object priors and admits an efficient certificate of global optimality. Given an RGB-D image of an object, we use a learned front-end to detect sparse, category-level semantic keypoints on the target object. We represent the target object's unknown shape using a linear active shape model and pose a maximum a posteriori optimization problem to solve for position, orientation, and shape simultaneously. Expressed in unit quaternions, this problem admits first-order optimality conditions in the form of an eigenvalue problem with eigenvector nonlinearities. Our primary contribution is to solve this problem efficiently with self-consistent field iteration, which only requires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector pair at each iterate. Solving a linear system for the corresponding Lagrange multipliers gives a simple global optimality certificate. One iteration of our solver runs in about 100 microseconds, enabling fast outlier rejection. We test our method on synthetic data and a variety of real-world settings, including two public datasets and a drone tracking scenario. Code is released at this https URL.", 'abstract_zh': '基于类别先验的快速局部求解器：形状与姿态估计', 'title_zh': '毫秒级类别级物体形状和姿态估计'}
{'arxiv_id': 'arXiv:2509.18865', 'title': 'Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation', 'authors': 'Masato Kobayashi, Thanpimon Buamanee', 'link': 'https://arxiv.org/abs/2509.18865', 'abstract': 'We propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral control-based imitation learning to handle more than one task within a single model. Conventional bilateral control methods exploit joint angle, velocity, torque, and vision for precise manipulation but require task-specific models, limiting their generality. Bi-VLA overcomes this limitation by utilizing robot joint angle, velocity, and torque data from leader-follower bilateral control with visual features and natural language instructions through SigLIP and FiLM-based fusion. We validated Bi-VLA on two task types: one requiring supplementary language cues and another distinguishable solely by vision. Real-robot experiments showed that Bi-VLA successfully interprets vision-language combinations and improves task success rates compared to conventional bilateral control-based imitation learning. Our Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language significantly enhances versatility. Experimental results validate the effectiveness of Bi-VLA in real-world tasks. For additional material, please visit the website: this https URL', 'abstract_zh': '基于双边控制的视觉-语言融合动作生成的双向控制 imitation 学习（Bi-VLA）', 'title_zh': '基于视觉-语言融合的双边控制 imitation 学习行动生成'}
{'arxiv_id': 'arXiv:2509.18609', 'title': 'PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving', 'authors': 'Chengran Yuan, Zijian Lu, Zhanqi Zhang, Yimin Zhao, Zefan Huang, Shuo Sun, Jiawei Sun, Jiahui Li, Christina Dao Wen Lee, Dongen Li, Marcelo H. Ang Jr', 'link': 'https://arxiv.org/abs/2509.18609', 'abstract': 'End-to-end motion planning is promising for simplifying complex autonomous driving pipelines. However, challenges such as scene understanding and effective prediction for decision-making continue to present substantial obstacles to its large-scale deployment. In this paper, we present PIE, a pioneering framework that integrates advanced perception, reasoning, and intention modeling to dynamically capture interactions between the ego vehicle and surrounding agents. It incorporates a bidirectional Mamba fusion that addresses data compression losses in multimodal fusion of camera and LiDAR inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize adaptive trajectory inference. PIE adopts an action-motion interaction module to effectively utilize state predictions of surrounding agents to refine ego planning. The proposed framework is thoroughly validated on the NAVSIM benchmark. PIE, without using any ensemble and data augmentation techniques, achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of prior state-of-the-art methods. Comprehensive quantitative and qualitative analyses demonstrate that PIE is capable of reliably generating feasible and high-quality ego trajectories.', 'abstract_zh': '端到端运动规划对于简化复杂的自动驾驶管道具有潜力，但场景理解及有效预测决策等方面仍存在挑战，阻碍其大规模部署。本文提出PIE，一种结合高级感知、推理和意图建模的先驱框架，动态捕捉ego车辆与周围代理之间的互动。该框架整合双向Mamba融合以解决多模态融合中摄像头和LiDAR输入的数据压缩损失问题，并采用新颖的增强推理解码器整合Mamba和专家混合模型，以促进场景合规的锚点选择和自适应轨迹推理优化。PIE采用动作-运动交互模块有效利用周围代理的状态预测来细化ego规划。本文在NASVCIM基准上对提出的框架进行了全面验证，在不使用任何集成和数据增强技术的情况下，PIE取得了88.9 PDM分数和85.6 EPDM分数，超过了现有最先进的方法。全面的定量和定性分析表明，PIE能够可靠地生成可行且高质量的ego轨迹。', 'title_zh': 'PIE: 感知与交互增强的端到端自动驾驶运动规划'}
{'arxiv_id': 'arXiv:2509.18330', 'title': 'The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020', 'authors': 'Marsette Vona', 'link': 'https://arxiv.org/abs/2509.18330', 'abstract': 'The Landform contextual mesh fuses 2D and 3D data from up to thousands of Mars 2020 rover images, along with orbital elevation and color maps from Mars Reconnaissance Orbiter, into an interactive 3D terrain visualization. Contextual meshes are built automatically for each rover location during mission ground data system processing, and are made available to mission scientists for tactical and strategic planning in the Advanced Science Targeting Tool for Robotic Operations (ASTTRO). A subset of them are also deployed to the "Explore with Perseverance" public access website.', 'abstract_zh': '火星2020漫游者图像的地形情境化网格融合了来自数千张漫游者图像的2D和3D数据，以及来自火星侦察轨道器的轨道高程和色彩地图，并提供了交互式的3D地形可视化。在任务地面数据系统处理过程中，自动为每个漫游者位置构建情境化网格，并提供给任务科学家在机器人操作高级科学目标选择工具（ASTTRO）中进行战术和战略规划。其中一部分也被部署到“使用 perseverance 探索”的公众访问网站上。', 'title_zh': '火星2020地形情境网格：自动融合表面和轨道地形'}
{'arxiv_id': 'arXiv:2509.18566', 'title': 'Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction', 'authors': 'Xiaoting Yin, Hao Shi, Kailun Yang, Jiajun Zhai, Shangwei Guo, Lin Wang, Kaiwei Wang', 'link': 'https://arxiv.org/abs/2509.18566', 'abstract': 'Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.', 'abstract_zh': '从单目事件摄像头视频重建动态人体与静态场景依然具有挑战性，尤其是在快速运动下，RGB帧受运动模糊影响。事件摄像头表现出显著的优势，如微秒级的时间分辨率，使其成为动态人体重建的优越传感器选择。因此，我们提出了一种新型的事件引导的人景重建框架，通过3D高斯点积技术，从单目事件摄像头联合建模人体和场景。具体而言，统一的3D高斯集合携带可学习的语义属性；只有被分类为人体的高斯点进行变形以实现动画效果，而场景高斯点保持静态。为对抗模糊，我们提出了一种事件引导的损失，该损失匹配连续渲染中的模拟亮度变化与事件流，从而在快速运动区域提升局部保真度。该方法消除了对外部人体掩码的需求，并简化了不同高斯集合的管理。在两个基准数据集ZJU-MoCap-Blur和MMHPSD-Blur上，它实现了人体-场景重建的最先进成果，特别是在PSNR/SSIM和降低LPIPS方面，特别是在高速运动主体方面取得了显著提升。', 'title_zh': '事件引导的3D高斯点云方法用于动态人类和场景重建'}
{'arxiv_id': 'arXiv:2509.18350', 'title': 'OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata', 'authors': 'Oussema Dhaouadi, Riccardo Marin, Johannes Meier, Jacques Kaiser, Daniel Cremers', 'link': 'https://arxiv.org/abs/2509.18350', 'abstract': 'Accurate visual localization from aerial views is a fundamental problem with applications in mapping, large-area inspection, and search-and-rescue operations. In many scenarios, these systems require high-precision localization while operating with limited resources (e.g., no internet connection or GNSS/GPS support), making large image databases or heavy 3D models impractical. Surprisingly, little attention has been given to leveraging orthographic geodata as an alternative paradigm, which is lightweight and increasingly available through free releases by governmental authorities (e.g., the European Union). To fill this gap, we propose OrthoLoC, the first large-scale dataset comprising 16,425 UAV images from Germany and the United States with multiple modalities. The dataset addresses domain shifts between UAV imagery and geospatial data. Its paired structure enables fair benchmarking of existing solutions by decoupling image retrieval from feature matching, allowing isolated evaluation of localization and calibration performance. Through comprehensive evaluation, we examine the impact of domain shifts, data resolutions, and covisibility on localization accuracy. Finally, we introduce a refinement technique called AdHoP, which can be integrated with any feature matcher, improving matching by up to 95% and reducing translation error by up to 63%. The dataset and code are available at: this https URL.', 'abstract_zh': '从空中视角实现准确的视觉定位是一个基本问题，应用于制图、大面积检查和搜索救援操作。在许多场景中，这些系统在资源有限的情况下（例如，没有互联网连接或GNSS/GPS支持）需要高精度定位，使得大规模图像数据库或重大的3D模型不切实际。令人惊讶的是，很少有研究关注利用正射地理数据作为替代方案，该方案轻量级且随着政府机构（例如欧盟）的免费发布越来越可用。为填补这一空白，我们提出OrthoLoC，这是首个包含来自德国和美国的16,425张UAV图像的大规模数据集，具有多种模态。数据集解决了UAV图像与地理空间数据之间的域转移问题。其成对结构使现有的解决方案能够在解耦图像检索和特征匹配的情况下进行公平基准测试，允许独立评估定位和校准性能。通过全面评估，我们探讨了域转移、数据分辨率和共可见性对定位精度的影响。最后，我们介绍了一种称为AdHoP的精简技术，它可以与任何特征匹配器集成，最多可提高匹配精度95%，并减少平移误差63%。该数据集和代码可在以下链接获取：this https URL。', 'title_zh': '正射LoC：使用正射地理数据的UAV 6-DoF定位与校准'}
{'arxiv_id': 'arXiv:2509.18527', 'title': 'FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning', 'authors': 'Ziwen Chen, Zhong Wang', 'link': 'https://arxiv.org/abs/2509.18527', 'abstract': 'The sport of fencing, like many other sports, faces challenges in refereeing: subjective calls, human errors, bias, and limited availability in practice environments. We present FERA (Fencing Referee Assistant), a prototype AI referee for foil fencing which integrates pose-based multi-label action recognition and rule-based reasoning. FERA extracts 2D joint positions from video, normalizes them, computes a 101-dimensional kinematic feature set, and applies a Transformer for multi-label move and blade classification. To determine priority and scoring, FERA applies a distilled language model with encoded right-of-way rules, producing both a decision and an explanation for each exchange. With limited hand-labeled data, a 5-fold cross-validation achieves an average macro-F1 score of 0.549, outperforming multiple baselines, including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla Transformer. While not ready for deployment, these results demonstrate a promising path towards automated referee assistance in foil fencing and new opportunities for AI applications, such as coaching in the field of fencing.', 'abstract_zh': '击剑运动作为其他许多运动一样，裁判工作面临挑战：主观判罚、人为错误、偏见以及实践环境的有限可用性。我们提出了一种击剑裁判助手FERA，这是一种基于姿势的多标签动作识别与规则推理相结合的AI裁判原型，适用于花剑比赛。FERA从视频中提取2D关节位置，进行标准化处理，计算出101维的动力学特征集，并应用Transformer进行多标签动作和剑法分类。为了确定优先级和计分，FERA应用了一种精炼的语言模型，结合了优先权规则编码，为每一回合提供决策和解释。尽管仅使用少量手动标注的数据，5折交叉验证达到了平均宏F1分数0.549，优于多个基准模型，包括时间卷积网络（TCN）、BiLSTM和纯Transformer模型。虽然尚未准备好部署，但这些结果展示了自动化击剑裁判辅助的新前景，并为击剑领域的教练应用等新的AI应用场景提供了可能。', 'title_zh': 'FERA：基于姿势的多标签动作识别和规则推理的击剑裁判助手'}
{'arxiv_id': 'arXiv:2509.19277', 'title': 'MOIS-SAM2: Exemplar-based Segment Anything Model 2 for multilesion interactive segmentation of neurobromas in whole-body MRI', 'authors': 'Georgii Kolokolnikov, Marie-Lena Schmalhofer, Sophie G\x7fötz, Lennart Well, Said Farschtschi, Victor-Felix Mautner, Inka Ristow, Rene Werner', 'link': 'https://arxiv.org/abs/2509.19277', 'abstract': 'Background and Objectives: Neurofibromatosis type 1 is a genetic disorder characterized by the development of numerous neurofibromas (NFs) throughout the body. Whole-body MRI (WB-MRI) is the clinical standard for detection and longitudinal surveillance of NF tumor growth. Existing interactive segmentation methods fail to combine high lesion-wise precision with scalability to hundreds of lesions. This study proposes a novel interactive segmentation model tailored to this challenge.\nMethods: We introduce MOIS-SAM2, a multi-object interactive segmentation model that extends the state-of-the-art, transformer-based, promptable Segment Anything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 was trained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired using T2-weighted fat-suppressed sequences. The dataset was split at the patient level into a training set and four test sets (one in-domain and three reflecting different domain shift scenarios, e.g., MRI field strength variation, low tumor burden, differences in clinical site and scanner vendor).\nResults: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of 0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC: 0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintained under MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC: 0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1 scores ranged from 0.62 to 0.78 across test sets. Preliminary inter-reader variability analysis showed model-to-expert agreement (DSC: 0.62-0.68), comparable to inter-expert agreement (DSC: 0.57-0.69).\nConclusions: The proposed MOIS-SAM2 enables efficient and scalable interactive segmentation of NFs in WB-MRI with minimal user input and strong generalization, supporting integration into clinical workflows.', 'abstract_zh': '背景与目的：神经纤维瘤病1型是一种遗传性疾病，特征是在全身范围内发展成多个神经纤维瘤（NFs）。全身磁共振成像（WB-MRI）是检测和纵向监测NF肿瘤生长的临床标准。现有的交互分割方法难以同时实现高病变-wise精度和对数百个病变的可扩展性。本研究提出了一种针对这一挑战的新颖交互分割模型。\n\n方法：我们引入了MOIS-SAM2，这是一种多对象交互分割模型，通过基于示例的语义传播扩展了最先进的、基于变换器的可提示分割一切模型2（SAM2）。MOIS-SAM2在来自84位神经纤维瘤病1型患者119份WB-MRI扫描数据集上进行训练和评估，该数据集使用T2加权脂肪抑制序列获取。数据集在患者层面划分为训练集和四个测试集（其中一个是领域内测试集，三个反映了不同的领域迁移场景，例如MRI磁场强度变化、低肿瘤负担、临床场所和扫描器供应商的差异）。\n\n结果：在领域内测试集上，MOIS-SAM2的扫描-wise DSC为0.60，优于基线3D nnU-Net（DSC：0.54）和SAM2（DSC：0.35）。在MRI磁场强度变化下，该模型的性能保持不变（DSC：0.53），在扫描器供应商差异下也有提升（DSC：0.50），在低肿瘤负担情况下则有所提高（DSC：0.61）。病变检测F1分数在不同测试集中范围从0.62到0.78。初步的互读者变异性分析表明，模型与专家的一致性（DSC：0.62-0.68）与专家之间的一致性相当（DSC：0.57-0.69）。\n\n结论：提出的MOIS-SAM2能够实现高效的、可扩展的WB-MRI中NF的交互分割，具有最小的用户输入和强泛化能力，支持临床工作流程的集成。', 'title_zh': 'MOIS-SAM2：基于例证的全身MRI中神经纤维瘤多病灶交互分割模型2'}
{'arxiv_id': 'arXiv:2509.19252', 'title': 'Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps', 'authors': 'Gabriel Maldonado, Narges Rashvand, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi', 'link': 'https://arxiv.org/abs/2509.19252', 'abstract': "Continuous human motion understanding remains a core challenge in computer vision due to its high dimensionality and inherent redundancy. Efficient compression and representation are crucial for analyzing complex motion dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework with dense motion tokenization for compressing spatio-temporal heatmaps while preserving the fine-grained traces of human motion. Our approach combines dense motion tokenization with adversarial refinement, which eliminates reconstruction artifacts like motion smearing and temporal misalignment observed in non-adversarial baselines. Our experiments on the CMU Panoptic dataset provide conclusive evidence of our method's superiority, outperforming the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%. Furthermore, our dense tokenization strategy enables a novel analysis of motion complexity, revealing that 2D motion can be optimally represented with a compact 128-token vocabulary, while 3D motion's complexity demands a much larger 1024-token codebook for faithful reconstruction. These results establish practical deployment feasibility across diverse motion analysis applications. The code base for this work is available at this https URL.", 'abstract_zh': '基于对抗精练的密集运动TOKEN化VQ-GAN框架：压缩时空热量图并保留细粒度的人体运动轨迹', 'title_zh': '对抗精炼VQ-GAN结合密集运动标记化用于时空热图'}
{'arxiv_id': 'arXiv:2509.19165', 'title': 'RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions', 'authors': 'Yun Wang, Junjie Hu, Junhui Hou, Chenghao Zhang, Renwei Yang, Dapeng Oliver Wu', 'link': 'https://arxiv.org/abs/2509.19165', 'abstract': 'Recent self-supervised stereo matching methods have made significant progress, but their performance significantly degrades under adverse weather conditions such as night, rain, and fog. We identify two primary weaknesses contributing to this performance degradation. First, adverse weather introduces noise and reduces visibility, making CNN-based feature extractors struggle with degraded regions like reflective and textureless areas. Second, these degraded regions can disrupt accurate pixel correspondences, leading to ineffective supervision based on the photometric consistency assumption. To address these challenges, we propose injecting robust priors derived from the visual foundation model into the CNN-based feature extractor to improve feature representation under adverse weather conditions. We then introduce scene correspondence priors to construct robust supervisory signals rather than relying solely on the photometric consistency assumption. Specifically, we create synthetic stereo datasets with realistic weather degradations. These datasets feature clear and adverse image pairs that maintain the same semantic context and disparity, preserving the scene correspondence property. With this knowledge, we propose a robust self-supervised training paradigm, consisting of two key steps: robust self-supervised scene correspondence learning and adverse weather distillation. Both steps aim to align underlying scene results from clean and adverse image pairs, thus improving model disparity estimation under adverse weather effects. Extensive experiments demonstrate the effectiveness and versatility of our proposed solution, which outperforms existing state-of-the-art self-supervised methods. Codes are available at \\textcolor{blue}{this https URL}.', 'abstract_zh': 'Recent 自监督立体匹配方法在恶劣天气条件下的表现显著提升，但在夜间、雨天和雾天等恶劣天气条件下，其性能会显著下降。我们识别出两种主要弱点导致这种性能下降。首先，恶劣天气引入噪声并降低可见度，使得基于CNN的特征提取器难以处理反射和纹理不足的区域。其次，这些退化区域会干扰准确的像素对应关系，导致基于光度一致性假设的监督无效。为了解决这些挑战，我们提出将源自视觉基础模型的鲁棒先验注入基于CNN的特征提取器中，以在恶劣天气条件下改进特征表示。然后，我们引入场景对应先验来构建鲁棒的监督信号，而不仅仅是依赖光度一致性假设。具体来说，我们创建了具有现实天气退化特征的合成立体数据集。这些数据集包含清晰和恶劣条件下的图像配对，保持相同的语义上下文和视差，保持场景对应性。基于这些知识，我们提出了一种鲁棒的自监督训练范式，包括两个关键步骤：鲁棒自监督场景对应学习和恶劣天气知识蒸馏。这两个步骤旨在对齐来自清晰图像和恶劣图像配对的底层场景结果，从而在恶劣天气效应下改善模型视差估计。广泛的实验表明，我们提出的方法具有良好的有效性和通用性，超越了现有最先进的自监督方法。代码可在 \\textcolor{blue}{此链接} 获取。', 'title_zh': 'RoSe: 严峻天气条件下鲁棒的自监督立体匹配'}
{'arxiv_id': 'arXiv:2509.19135', 'title': 'GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding', 'authors': 'Wenying Luo, Zhiyuan Lin, Wenhao Xu, Minghao Liu, Zhi Li', 'link': 'https://arxiv.org/abs/2509.19135', 'abstract': 'Human mobility traces, often recorded as sequences of check-ins, provide a unique window into both short-term visiting patterns and persistent lifestyle regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal framework designed to advance mobility analysis by explicitly modeling the semantic and temporal complexity of human movement. The framework consists of four key innovations. First, a Spatio-Temporal Concept Encoder (STCE) integrates geographic location, POI category semantics, and periodic temporal rhythms into unified vector representations. Second, a Cognitive Trajectory Memory (CTM) adaptively filters historical visits, emphasizing recent and behaviorally salient events in order to capture user intent more effectively. Third, a Lifestyle Concept Bank (LCB) contributes structured human preference cues, such as activity types and lifestyle patterns, to enhance interpretability and personalization. Finally, task-oriented generative heads transform the learned representations into predictions for multiple downstream tasks. We conduct extensive experiments on four widely used real-world datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate performance on three benchmark tasks: next-location prediction, trajectory-user identification, and time estimation. The results demonstrate consistent and substantial improvements over strong baselines, confirming the effectiveness of GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond raw performance gains, our findings also suggest that generative modeling provides a promising foundation for building more robust, interpretable, and generalizable systems for human mobility intelligence.', 'abstract_zh': '基于时空概念的生成框架：人类移动分析的新视角（GSTM-HMU）', 'title_zh': 'GSTM-HMU：生成式时空建模为人流移动理解'}
{'arxiv_id': 'arXiv:2509.18938', 'title': 'No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning', 'authors': 'Matheus Vinícius Todescato, Joel Luís Carbonera', 'link': 'https://arxiv.org/abs/2509.18938', 'abstract': 'While deep learning, including Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), has significantly advanced classification performance, its typical reliance on extensive annotated datasets presents a major obstacle in many practical scenarios where such data is scarce. Vision-language models (VLMs) and transfer learning with pre-trained visual models appear as promising techniques to deal with this problem. This paper proposes a novel zero-shot image classification framework that combines a VLM and a pre-trained visual model within a self-learning cycle. Requiring only the set of class names and no labeled training data, our method utilizes a confidence-based pseudo-labeling strategy to train a lightweight classifier directly on the test data, enabling dynamic adaptation. The VLM identifies high-confidence samples, and the pre-trained visual model enhances their visual representations. These enhanced features then iteratively train the classifier, allowing the system to capture complementary semantic and visual cues without supervision. Notably, our approach avoids VLM fine-tuning and the use of large language models, relying on the visual-only model to reduce the dependence on semantic representation. Experimental evaluations on ten diverse datasets demonstrate that our approach outperforms the baseline zero-shot method.', 'abstract_zh': '一种结合视觉语言模型和预训练视觉模型的零样本图像分类自学习框架', 'title_zh': '无需标签：协作自学习的零样本图像分类'}
{'arxiv_id': 'arXiv:2509.18917', 'title': 'LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models', 'authors': 'Amirhesam Aghanouri, Cristina Olaverri-Monreal', 'link': 'https://arxiv.org/abs/2509.18917', 'abstract': "Autonomous vehicles (AVs) are expected to revolutionize transportation by improving efficiency and safety. Their success relies on 3D vision systems that effectively sense the environment and detect traffic agents. Among sensors AVs use to create a comprehensive view of surroundings, LiDAR provides high-resolution depth data enabling accurate object detection, safe navigation, and collision avoidance. However, collecting real-world LiDAR data is time-consuming and often affected by noise and sparsity due to adverse weather or sensor limitations. This work applies a denoising diffusion probabilistic model (DDPM), enhanced with novel noise scheduling and time-step embedding techniques to generate high-quality synthetic data for augmentation, thereby improving performance across a range of computer vision tasks, particularly in AV perception. These modifications impact the denoising process and the model's temporal awareness, allowing it to produce more realistic point clouds based on the projection. The proposed method was extensively evaluated under various configurations using the IAMCV and KITTI-360 datasets, with four performance metrics compared against state-of-the-art (SOTA) methods. The results demonstrate the model's superior performance over most existing baselines and its effectiveness in mitigating the effects of noisy and sparse LiDAR data, producing diverse point clouds with rich spatial relationships and structural detail.", 'abstract_zh': '自主驾驶车辆中的去噪扩散概率模型在3D视觉任务中的应用：生成高質量合成数据以改善感知性能', 'title_zh': '基于去噪扩散概率模型的LiDAR点云图像生成'}
{'arxiv_id': 'arXiv:2509.18831', 'title': 'Text Slider: Efficient and Plug-and-Play Continuous Concept Control for Image/Video Synthesis via LoRA Adapters', 'authors': 'Pin-Yen Chiu, I-Sheng Fang, Jun-Cheng Chen', 'link': 'https://arxiv.org/abs/2509.18831', 'abstract': 'Recent advances in diffusion models have significantly improved image and video synthesis. In addition, several concept control methods have been proposed to enable fine-grained, continuous, and flexible control over free-form text prompts. However, these methods not only require intensive training time and GPU memory usage to learn the sliders or embeddings but also need to be retrained for different diffusion backbones, limiting their scalability and adaptability. To address these limitations, we introduce Text Slider, a lightweight, efficient and plug-and-play framework that identifies low-rank directions within a pre-trained text encoder, enabling continuous control of visual concepts while significantly reducing training time, GPU memory consumption, and the number of trainable parameters. Furthermore, Text Slider supports multi-concept composition and continuous control, enabling fine-grained and flexible manipulation in both image and video synthesis. We show that Text Slider enables smooth and continuous modulation of specific attributes while preserving the original spatial layout and structure of the input. Text Slider achieves significantly better efficiency: 5$\\times$ faster training than Concept Slider and 47$\\times$ faster than Attribute Control, while reducing GPU memory usage by nearly 2$\\times$ and 4$\\times$, respectively.', 'abstract_zh': 'Recent Advances in Diffusion Models Have Significantly Enhanced Image and Video Synthesis: Introducing Text Slider, a Lightweight and Efficient Framework for Continuous Visual Concept Control', 'title_zh': '文本滑块：通过LoRA适配器实现的高效可插拔连续概念控制以进行图像/视频合成'}
{'arxiv_id': 'arXiv:2509.18765', 'title': 'DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision', 'authors': 'Azad Singh, Deepak Mishra', 'link': 'https://arxiv.org/abs/2509.18765', 'abstract': 'Self-supervised learning (SSL) has emerged as a powerful paradigm for medical image representation learning, particularly in settings with limited labeled data. However, existing SSL methods often rely on complex architectures, anatomy-specific priors, or heavily tuned augmentations, which limit their scalability and generalizability. More critically, these models are prone to shortcut learning, especially in modalities like chest X-rays, where anatomical similarity is high and pathology is subtle. In this work, we introduce DiSSECT -- Discrete Self-Supervision for Efficient Clinical Transferable Representations, a framework that integrates multi-scale vector quantization into the SSL pipeline to impose a discrete representational bottleneck. This constrains the model to learn repeatable, structure-aware features while suppressing view-specific or low-utility patterns, improving representation transfer across tasks and domains. DiSSECT achieves strong performance on both classification and segmentation tasks, requiring minimal or no fine-tuning, and shows particularly high label efficiency in low-label regimes. We validate DiSSECT across multiple public medical imaging datasets, demonstrating its robustness and generalizability compared to existing state-of-the-art approaches.', 'abstract_zh': '自监督学习（SSL）已成为一种在有限标注数据环境下用于医学图像表示学习的强大范式。然而，现有的SSL方法往往依赖于复杂的架构、解剖特征特异性先验知识或高度调整的数据增强，这限制了它们的可扩展性和普适性。更关键的是，这些模型容易产生快捷学习，特别是在肺部X光片等解剖相似性高而病理表现微妙的模态中。在本文中，我们提出了DiSSECT——离散自监督学习以实现高效临床转移表示，该框架将多尺度向量量化集成到SSL管道中，以施加离散的表示瓶颈。这迫使模型学习可重复的、结构感知的特征，同时抑制视图特定的或低效的模式，从而在不同任务和领域中提高表示转移。DiSSECT在分类和分割任务上均表现出强大性能，需要极少或无需微调，并且在低标记数据条件下显示出特别高的标签效率。我们在多个公开的医学影像数据集上验证了DiSSECT，展示了其与现有最先进技术相比的鲁棒性和普适性。', 'title_zh': 'DiSSECT: 通过离散自我监督构建转移ready的医学图像表示'}
{'arxiv_id': 'arXiv:2509.18683', 'title': 'LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection', 'authors': 'Lanhu Wu, Zilin Gao, Hao Fei, Mong-Li Lee, Wynne Hsu', 'link': 'https://arxiv.org/abs/2509.18683', 'abstract': 'RGB-D salient object detection (SOD) aims to identify the most conspicuous objects in a scene with the incorporation of depth cues. Existing methods mainly rely on CNNs, limited by the local receptive fields, or Vision Transformers that suffer from the cost of quadratic complexity, posing a challenge in balancing performance and computational efficiency. Recently, state space models (SSM), Mamba, have shown great potential for modeling long-range dependency with linear complexity. However, directly applying SSM to RGB-D SOD may lead to deficient local semantics as well as the inadequate cross-modality fusion. To address these issues, we propose a Local Emphatic and Adaptive Fusion state space model (LEAF-Mamba) that contains two novel components: 1) a local emphatic state space module (LE-SSM) to capture multi-scale local dependencies for both modalities. 2) an SSM-based adaptive fusion module (AFM) for complementary cross-modality interaction and reliable cross-modality integration. Extensive experiments demonstrate that the LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in both efficacy and efficiency. Moreover, our method can achieve excellent performance on the RGB-T SOD task, proving a powerful generalization ability.', 'abstract_zh': 'RGB-D显著目标检测（SOD）旨在利用深度线索识别场景中的最突出对象。现有的方法主要依赖CNNs，受局部感受野的限制，或依赖Vision Transformers但带来计算复杂度的二次增加，这在性能和计算效率之间提出了挑战。最近，状态空间模型（SSM）Mamba在以线性复杂度建模长程依赖方面显示出巨大潜力。然而，直接将SSM应用于RGB-D SOD可能导致局部语义不足以及跨模态融合不足。为解决这些问题，我们提出了一种包含两个新颖组件的局部强化和自适应融合状态空间模型（LEAF-Mamba）：1）一种局部强化状态空间模块（LE-SSM），用于捕捉两种模态的多尺度局部依赖性；2）一种基于SSM的自适应融合模块（AFM），用于互补的跨模态交互和可靠的跨模态集成。大量实验表明，LEAF-Mamba在效能和效率上均优于16种最先进的RGB-D SOD方法。此外，我们的方法在RGB-T SOD任务中也能实现出色的表现，证明了强大的泛化能力。', 'title_zh': 'LEAF-Mamba：局部强化和自适应融合状态空间模型在RGB-D显著目标检测中的应用'}
{'arxiv_id': 'arXiv:2509.18561', 'title': 'SoundCompass: Navigating Target Sound Extraction With Effective Directional Clue Integration In Complex Acoustic Scenes', 'authors': 'Dayun Choi, Jung-Woo Choi', 'link': 'https://arxiv.org/abs/2509.18561', 'abstract': 'Recent advances in target sound extraction (TSE) utilize directional clues derived from direction of arrival (DoA), which represent an inherent spatial property of sound available in any acoustic scene. However, previous DoA-based methods rely on hand-crafted features or discrete encodings, which lose fine-grained spatial information and limit adaptability. We propose SoundCompass, an effective directional clue integration framework centered on a Spectral Pairwise INteraction (SPIN) module that captures cross-channel spatial correlations in the complex spectrogram domain to preserve full spatial information in multichannel signals. The input feature expressed in terms of spatial correlations is fused with a DoA clue represented as spherical harmonics (SH) encoding. The fusion is carried out across overlapping frequency subbands, inheriting the benefits reported in the previous band-split architectures. We also incorporate the iterative refinement strategy, chain-of-inference (CoI), in the TSE framework, which recursively fuses DoA with sound event activation estimated from the previous inference stage. Experiments demonstrate that SoundCompass, combining SPIN, SH embedding, and CoI, robustly extracts target sources across diverse signal classes and spatial configurations.', 'abstract_zh': 'Recent Advances in Target Sound Extraction (TSE) Utilizing Spectral Pairwise Interaction (SPIN) for Directional Clue Integration', 'title_zh': 'SoundCompass: 在复杂声景中通过有效的方向性线索集成进行目标声音提取'}
{'arxiv_id': 'arXiv:2509.18461', 'title': "Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake Content Before It's Created?", 'authors': 'Ayan Sar, Sampurna Roy, Tanupriya Choudhury, Ajith Abraham', 'link': 'https://arxiv.org/abs/2509.18461', 'abstract': 'Generative adversarial networks (GANs) and diffusion models have dramatically advanced deepfake technology, and its threats to digital security, media integrity, and public trust have increased rapidly. This research explored zero-shot deepfake detection, an emerging method even when the models have never seen a particular deepfake variation. In this work, we studied self-supervised learning, transformer-based zero-shot classifier, generative model fingerprinting, and meta-learning techniques that better adapt to the ever-evolving deepfake threat. In addition, we suggested AI-driven prevention strategies that mitigated the underlying generation pipeline of the deepfakes before they occurred. They consisted of adversarial perturbations for creating deepfake generators, digital watermarking for content authenticity verification, real-time AI monitoring for content creation pipelines, and blockchain-based content verification frameworks. Despite these advancements, zero-shot detection and prevention faced critical challenges such as adversarial attacks, scalability constraints, ethical dilemmas, and the absence of standardized evaluation benchmarks. These limitations were addressed by discussing future research directions on explainable AI for deepfake detection, multimodal fusion based on image, audio, and text analysis, quantum AI for enhanced security, and federated learning for privacy-preserving deepfake detection. This further highlighted the need for an integrated defense framework for digital authenticity that utilized zero-shot learning in combination with preventive deepfake mechanisms. Finally, we highlighted the important role of interdisciplinary collaboration between AI researchers, cybersecurity experts, and policymakers to create resilient defenses against the rising tide of deepfake attacks.', 'abstract_zh': '生成对抗网络（GANs）和扩散模型大大推动了换脸技术的发展，其对数字安全、媒体完整性和公共信任的威胁迅速增加。本研究探讨了零样本换脸检测方法，即使模型从未见过特定的换脸变体。在本工作中，我们研究了自监督学习、基于变换器的零样本分类器、生成模型指纹识别以及更适应不断演变的换脸威胁的元学习技术。此外，我们提出了一种基于AI的预防策略，可以在此类换脸生成之前减轻其潜在生成管道。这些策略包括用于创建换脸生成器的对抗性扰动、用于内容真实性验证的数字水印、用于内容生成管道的实时AI监控以及基于区块链的内容验证框架。尽管取得了这些进展，零样本检测和预防仍然面临着诸如对抗性攻击、可扩展性限制、伦理困境和缺乏标准化评估基准等关键挑战。这些问题通过讨论深度换脸检测的可解释AI、基于图像、音频和文本分析的多模态融合、量子AI增强安全性以及联邦学习实现隐私保护的深度换脸检测的未来研究方向来解决。这进一步强调了需要一个集成的防伪框架，该框架结合了零样本学习和预防换脸机制，以实现数字真实性的保护。最后，我们强调了人工智能研究人员、网络安全专家和政策制定者之间跨学科合作的重要作用，以创建对不断上升的深度换脸攻击具有弹性的防御措施。', 'title_zh': '零样本视觉深度假信息检测：AI能否预测和防止假内容的创造？'}
{'arxiv_id': 'arXiv:2509.18354', 'title': 'A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data', 'authors': 'Mehrdad Moradi, Shengzhe Chen, Hao Yan, Kamran Paynabar', 'link': 'https://arxiv.org/abs/2509.18354', 'abstract': 'Anomaly detection in images is typically addressed by learning from collections of training data or relying on reference samples. In many real-world scenarios, however, such training data may be unavailable, and only the test image itself is provided. We address this zero-shot setting by proposing a single-image anomaly localization method that leverages the inductive bias of convolutional neural networks, inspired by Deep Image Prior (DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key assumption is that natural images often exhibit unified textures and patterns, and that anomalies manifest as localized deviations from these repetitive or stochastic patterns. To learn the deep image prior, we design a patch-based training framework where the input image is fed directly into the network for self-reconstruction, rather than mapping random noise to the image as done in DIP. To avoid the model simply learning an identity mapping, we apply masking, patch shuffling, and small Gaussian noise. In addition, we use a perceptual loss based on inner-product similarity to capture structure beyond pixel fidelity. Our approach needs no external training data, labels, or references, and remains robust in the presence of noise or missing pixels. SSDnet achieves 0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the fabric dataset, outperforming state-of-the-art methods. The implementation code will be released at this https URL', 'abstract_zh': '单张图像异常定位方法：基于卷积神经网络的深度图像先验单图像分解网络', 'title_zh': '无需训练数据：一张图像即可实现零样本异常定位'}
{'arxiv_id': 'arXiv:2509.18214', 'title': 'Automatic Classification of Magnetic Chirality of Solar Filaments from H-Alpha Observations', 'authors': 'Alexis Chalmers, Azim Ahmadzadeh', 'link': 'https://arxiv.org/abs/2509.18214', 'abstract': 'In this study, we classify the magnetic chirality of solar filaments from H-Alpha observations using state-of-the-art image classification models. We establish the first reproducible baseline for solar filament chirality classification on the MAGFiLO dataset. The MAGFiLO dataset contains over 10,000 manually-annotated filaments from GONG H-Alpha observations, making it the largest dataset for filament detection and classification to date. Prior studies relied on much smaller datasets, which limited their generalizability and comparability. We fine-tuned several pre-trained, image classification architectures, including ResNet, WideResNet, ResNeXt, and ConvNeXt, and also applied data augmentation and per-class loss weights to optimize the models. Our best model, ConvNeXtBase, achieves a per-class accuracy of 0.69 for left chirality filaments and $0.73$ for right chirality filaments.', 'abstract_zh': '本研究使用最先进的图像分类模型从H-Alpha观测中对太阳细丝的磁旋方向进行分类，并在MAGFiLO数据集上建立了首个可再现的基准。MAGFiLO数据集包含来自GONG H-Alpha观测的逾10,000个手工标注的细丝，使其成为迄今为止用于细丝检测和分类的最大数据集。先前的研究依赖于规模小得多的数据集，限制了其泛化能力和可比性。我们微调了包括ResNet、WideResNet、ResNeXt和ConvNeXt在内的几种预训练的图像分类架构，并应用了数据增强和类别损失权重以优化模型。我们的最佳模型ConvNeXtBase在左旋细丝上的每类别准确率为0.69，在右旋细丝上的每类别准确率为0.73。', 'title_zh': '基于H-α观测的太阳日珥磁旋结构自动分类'}
{'arxiv_id': 'arXiv:2509.18193', 'title': 'TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection', 'authors': 'Omar H. Khater, Abdul Jabbar Siddiqui, Aiman El-Maleh, M. Shamim Hossain', 'link': 'https://arxiv.org/abs/2509.18193', 'abstract': "Deploying deep learning models in agriculture is difficult because edge devices have limited resources, but this work presents a compressed version of EcoWeedNet using structured channel pruning, quantization-aware training (QAT), and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the challenges of pruning complex architectures with residual shortcuts, attention mechanisms, concatenations, and CSP blocks, the model size was reduced by up to 68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n (with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9% mAP50, proving it to be both efficient and effective for precision agriculture.", 'abstract_zh': '使用结构化信道修剪、量化感知训练和NVIDIA TensorRT加速，本工作提出了EcoWeedNet的压缩版本，应用于农业中的深度学习模型部署。尽管面对含有残差捷径、注意力机制、拼接和CSP模块的复杂架构修剪挑战，模型大小减少至最大68.5%，计算量减少3.2 GFLOPs，推理速度达到184 FPS，优于FP16基本模型28.7%。在CottonWeedDet12数据集上，39.5%修剪比的EcoWeedNet优于YOLO11n和YOLO12n（仅修剪20%），精度83.7%，召回率77.5%，mAP50为85.9%，证明其在精准农业中高效且有效。', 'title_zh': 'TinyEcoWeedNet: 边缘高效实时航空农业杂草检测'}
{'arxiv_id': 'arXiv:2509.18190', 'title': 'HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing', 'authors': 'Junseong Shin, Seungwoo Chung, Yunjeong Yang, Tae Hyun Kim', 'link': 'https://arxiv.org/abs/2509.18190', 'abstract': 'Dehazing involves removing haze or fog from images to restore clarity and improve visibility by estimating atmospheric scattering effects. While deep learning methods show promise, the lack of paired real-world training data and the resulting domain gap hinder generalization to real-world scenarios. In this context, physics-grounded learning becomes crucial; however, traditional methods based on the Atmospheric Scattering Model (ASM) often fall short in handling real-world complexities and diverse haze patterns. To solve this problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF), HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones, enhancing real-world dehazing performance with only a single inference step. Additionally, we introduce a non-homogeneous haze generation method using Markov Chain Brownian Motion (MCBM) to address the scarcity of paired real-world data. By simulating realistic haze patterns through MCBM, we enhance the adaptability of HazeFlow to diverse real-world scenarios. Through extensive experiments, we demonstrate that HazeFlow achieves state-of-the-art performance across various real-world dehazing benchmark datasets.', 'abstract_zh': '去雾涉及从图像中去除雾或 Fog，通过估计大气散射效应来恢复清晰度和改善可见度。尽管深度学习方法显示出潜力，但由于缺乏配对的实际训练数据和由此产生的领域差距，其泛化能力受限于实际场景。在这种背景下，基于物理的学习变得至关重要；然而，传统的基于大气散射模型（ASM）的方法往往在处理现实世界的复杂性和多样化的雾状模式时表现不佳。为了解决这个问题，我们提出了 HazeFlow，一种基于ODE的新颖框架，将ASM重新公式化为常微分方程（ODE）。受修正流（RF）的启发，HazeFlow 学习一个最优的 ODE 轨迹，将雾状图像映射为干净图像，仅通过单次推理步骤便提升了实际去雾性能。此外，我们引入了一种基于马尔可夫链布朗运动（MCBM）的非齐次去雾生成方法，以应对配对实际数据的稀缺性。通过 MCBM 模拟现实雾状模式，我们增强了 HazeFlow 对多种实际场景的适应性。通过广泛的实验，我们展示了 HazeFlow 在各种实际去雾基准数据集上达到了最先进的性能。', 'title_zh': 'HazeFlow: 重新审视散射模型作为ODE及其在真实世界去雾霾中的非均匀生成方法'}
