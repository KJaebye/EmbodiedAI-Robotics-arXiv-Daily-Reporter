{'arxiv_id': 'arXiv:2503.09296', 'title': 'MonoSLAM: Robust Monocular SLAM with Global Structure Optimization', 'authors': 'Bingzheng Jiang, Jiayuan Wang, Han Ding, Lijun Zhu', 'link': 'https://arxiv.org/abs/2503.09296', 'abstract': "This paper presents a robust monocular visual SLAM system that simultaneously utilizes point, line, and vanishing point features for accurate camera pose estimation and mapping. To address the critical challenge of achieving reliable localization in low-texture environments, where traditional point-based systems often fail due to insufficient visual features, we introduce a novel approach leveraging Global Primitives structural information to improve the system's robustness and accuracy performance. Our key innovation lies in constructing vanishing points from line features and proposing a weighted fusion strategy to build Global Primitives in the world coordinate system. This strategy associates multiple frames with non-overlapping regions and formulates a multi-frame reprojection error optimization, significantly improving tracking accuracy in texture-scarce scenarios. Evaluations on various datasets show that our system outperforms state-of-the-art methods in trajectory precision, particularly in challenging environments.", 'abstract_zh': '一种同时利用点、线和消失点特征的鲁棒单目视觉SLAM系统', 'title_zh': '单目SLAM：带全局结构优化的鲁棒单目SLAM'}
{'arxiv_id': 'arXiv:2503.09024', 'title': 'Traffic Regulation-aware Path Planning with Regulation Databases and Vision-Language Models', 'authors': 'Xu Han, Zhiwen Wu, Xin Xia, Jiaqi Ma', 'link': 'https://arxiv.org/abs/2503.09024', 'abstract': "This paper introduces and tests a framework integrating traffic regulation compliance into automated driving systems (ADS). The framework enables ADS to follow traffic laws and make informed decisions based on the driving environment. Using RGB camera inputs and a vision-language model (VLM), the system generates descriptive text to support a regulation-aware decision-making process, ensuring legal and safe driving practices. This information is combined with a machine-readable ADS regulation database to guide future driving plans within legal constraints. Key features include: 1) a regulation database supporting ADS decision-making, 2) an automated process using sensor input for regulation-aware path planning, and 3) validation in both simulated and real-world environments. Particularly, the real-world vehicle tests not only assess the framework's performance but also evaluate the potential and challenges of VLMs to solve complex driving problems by integrating detection, reasoning, and planning. This work enhances the legality, safety, and public trust in ADS, representing a significant step forward in the field.", 'abstract_zh': '本文介绍并测试了一种将交通法规遵守整合到自动驾驶系统（ADS）中的框架。该框架使ADS能够遵循交通法规并基于驾驶环境做出明智的决策。利用RGB摄像头输入和视觉语言模型（VLM），系统生成描述性文本以支持法规意识决策过程，确保合法和安全的驾驶实践。这些信息与可机器读取的ADS法规数据库相结合，以在法律约束内指导未来的驾驶计划。关键功能包括：1）支持ADS决策的法规数据库，2）基于传感器输入的自动化过程进行法规意识路径规划，3）在模拟和实际环境中进行验证。特别地，实际车辆测试不仅评估了该框架的性能，还评估了VLMs通过集成检测、推理和规划来解决复杂驾驶问题的潜力和挑战。本文增强了ADS的合法性、安全性和公众信任，代表了该领域的一项重要进展。', 'title_zh': '基于交通管制数据库和视觉语言模型的路径规划与交通管制意识规划'}
{'arxiv_id': 'arXiv:2503.09191', 'title': 'Learning Appearance and Motion Cues for Panoptic Tracking', 'authors': 'Juana Valeria Hurtado, Sajad Marvi, Rohit Mohan, Abhinav Valada', 'link': 'https://arxiv.org/abs/2503.09191', 'abstract': 'Panoptic tracking enables pixel-level scene interpretation of videos by integrating instance tracking in panoptic segmentation. This provides robots with a spatio-temporal understanding of the environment, an essential attribute for their operation in dynamic environments. In this paper, we propose a novel approach for panoptic tracking that simultaneously captures general semantic information and instance-specific appearance and motion features. Unlike existing methods that overlook dynamic scene attributes, our approach leverages both appearance and motion cues through dedicated network heads. These interconnected heads employ multi-scale deformable convolutions that reason about scene motion offsets with semantic context and motion-enhanced appearance features to learn tracking embeddings. Furthermore, we introduce a novel two-step fusion module that integrates the outputs from both heads by first matching instances from the current time step with propagated instances from previous time steps and subsequently refines associations using motion-enhanced appearance embeddings, improving robustness in challenging scenarios. Extensive evaluations of our proposed \\netname model on two benchmark datasets demonstrate that it achieves state-of-the-art performance in panoptic tracking accuracy, surpassing prior methods in maintaining object identities over time. To facilitate future research, we make the code available at this http URL', 'abstract_zh': '全景跟踪通过结合全景分割中的实例跟踪，实现视频的像素级场景解释。这为机器人提供了时空环境理解能力，是其在动态环境中操作的关键属性。在本文中，我们提出了一种新颖的全景跟踪方法，同时捕捉通用语义信息和实例特定的外观和运动特征。与现有方法忽略动态场景属性不同，我们的方法通过专用网络头利用外观和运动线索。这些相互连接的头使用多尺度可变形卷积，结合语义上下文和运动增强的外观特征来学习跟踪嵌入。此外，我们引入了一种新的两步融合模块，首先通过匹配当前时间步的实例与之前时间步传播的实例，然后使用运动增强的外观嵌入进一步细化关联，增强在挑战性场景中的鲁棒性。我们在两个基准数据集上的广泛评估表明，我们的\\netname模型在全景跟踪准确性上达到了最先进的性能，优于先前方法在保持对象身份方面的表现。为了促进未来研究，我们已在此网址处提供了代码。', 'title_zh': '学习外观和运动线索进行全景跟踪'}
{'arxiv_id': 'arXiv:2503.09040', 'title': 'Motion Blender Gaussian Splatting for Dynamic Reconstruction', 'authors': 'Xinyu Zhang, Haonan Chang, Yuhan Liu, Abdeslam Boularias', 'link': 'https://arxiv.org/abs/2503.09040', 'abstract': 'Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes. However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application. To address this, we propose Motion Blender Gaussian Splatting (MB-GS), a novel framework that uses motion graph as an explicit and sparse motion representation. The motion of graph links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions determining the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MB-GS achieves state-of-the-art performance on the iPhone dataset while being competitive on HyperNeRF. Additionally, we demonstrate the application potential of our method in generating novel object motions and robot demonstrations through motion editing. Video demonstrations can be found at this https URL.', 'abstract_zh': '高保真动态场景重建的一种强大工具：高斯点积已 emerge 作为一种强大的工具，用于高保真重建动态场景。然而，现有方法主要依赖于隐式运动表示，如将运动编码到神经网络或每个高斯参数中，这使得难以进一步操控重建的运动。这种缺乏显式可控性限制了现有方法只能回放录制的运动，这妨碍了其更广泛的应用。为了解决这一问题，我们提出了运动混合高斯点积（Motion Blender Gaussian Splatting，MB-GS）这一新颖框架，该框架使用运动图作为显式且稀疏的运动表示。通过双四元数皮肤技术，运动图边的运动传播到个体高斯点上，可学习的权重绘画函数确定每个边的影响。通过可微渲染，运动图和3D高斯点共同从输入视频中进行优化。实验结果显示，MB-GS 在 iPhone 数据集上达到了最先进的性能，同时在 HyperNeRF 上竞争力强。此外，我们展示了该方法在通过运动编辑生成新型物体运动和机器人演示方面的应用潜力。相关视频演示可访问以下链接：this https URL。', 'title_zh': '动态重建中的运动混合高斯绘制'}
{'arxiv_id': 'arXiv:2503.08930', 'title': 'Acoustic Neural 3D Reconstruction Under Pose Drift', 'authors': 'Tianxiang Lin, Mohamad Qadri, Kevin Zhang, Adithya Pediredla, Christopher A. Metzler, Michael Kaess', 'link': 'https://arxiv.org/abs/2503.08930', 'abstract': 'We consider the problem of optimizing neural implicit surfaces for 3D reconstruction using acoustic images collected with drifting sensor poses. The accuracy of current state-of-the-art 3D acoustic modeling algorithms is highly dependent on accurate pose estimation; small errors in sensor pose can lead to severe reconstruction artifacts. In this paper, we propose an algorithm that jointly optimizes the neural scene representation and sonar poses. Our algorithm does so by parameterizing the 6DoF poses as learnable parameters and backpropagating gradients through the neural renderer and implicit representation. We validated our algorithm on both real and simulated datasets. It produces high-fidelity 3D reconstructions even under significant pose drift.', 'abstract_zh': '我们考虑使用漂移传感器姿态收集的声学图像优化3D重建中的神经隐式表面的问题。当前最先进的3D声学建模算法的准确性高度依赖于姿态估计的准确性；传感器姿态中的小误差可能导致严重的重建伪影。在本文中，我们提出了一种联合优化神经场景表示和声纳姿态的算法。该算法通过将6自由度姿态参数化为可学习的参数，并通过神经渲染器和隐式表示反向传播梯度来实现。我们在真实数据集和模拟数据集上验证了该算法，即使在显著的姿态漂移下也能生成高保真的3D重建。', 'title_zh': '基于姿态漂移的声学神经3D重建'}
{'arxiv_id': 'arXiv:2503.08929', 'title': 'HessianForge: Scalable LiDAR reconstruction with Physics-Informed Neural Representation and Smoothness Energy Constraints', 'authors': 'Hrishikesh Viswanath, Md Ashiqur Rahman, Chi Lin, Damon Conover, Aniket Bera', 'link': 'https://arxiv.org/abs/2503.08929', 'abstract': 'Accurate and efficient 3D mapping of large-scale outdoor environments from LiDAR measurements is a fundamental challenge in robotics, particularly towards ensuring smooth and artifact-free surface reconstructions. Although the state-of-the-art methods focus on memory-efficient neural representations for high-fidelity surface generation, they often fail to produce artifact-free manifolds, with artifacts arising due to noisy and sparse inputs. To address this issue, we frame surface mapping as a physics-informed energy optimization problem, enforcing surface smoothness by optimizing an energy functional that penalizes sharp surface ridges. Specifically, we propose a deep learning based approach that learns the signed distance field (SDF) of the surface manifold from raw LiDAR point clouds using a physics-informed loss function that optimizes the $L_2$-Hessian energy of the surface. Our learning framework includes a hierarchical octree based input feature encoding and a multi-scale neural network to iteratively refine the signed distance field at different scales of resolution. Lastly, we introduce a test-time refinement strategy to correct topological inconsistencies and edge distortions that can arise in the generated mesh. We propose a \\texttt{CUDA}-accelerated least-squares optimization that locally adjusts vertex positions to enforce feature-preserving smoothing. We evaluate our approach on large-scale outdoor datasets and demonstrate that our approach outperforms current state-of-the-art methods in terms of improved accuracy and smoothness. Our code is available at \\href{this https URL}{this https URL}', 'abstract_zh': '大规模室外环境从LiDAR测量中实现精确且高效的三维建模是一项基本挑战，尤其是在确保无瑕疵表面重建平滑性方面。尽管最先进的方法专注于高保真表面生成的内存高效神经表征，它们往往无法产生无瑕疵的数据流形，其中的瑕疵多由噪声和稀疏的输入产生。为了解决这一问题，我们将表面建模框架化为一个受物理约束的能量优化问题，通过优化惩罚尖锐表面嵴的数据流形表面平滑性能量功能来强制表面平滑性。具体而言，我们提出了一种基于深度学习的方法，该方法使用一个受物理约束的损失函数从原始LiDAR点云中学习数据流形的符号距离场(SDF)，该损失函数优化表面的$L_2$海森能量。我们的学习框架包括基于层次八叉树的输入特征编码和多尺度神经网络，以在不同分辨率级别上逐步细化符号距离场。最后，我们引入了一种测试时间细化策略来纠正生成网格中可能出现的拓扑不一致性和边缘畸变。我们提出了一种基于CUDA加速的最小二乘优化方法，以局部调整顶点位置来实现特征保持平滑。我们在大规模室外数据集上评估了我们的方法，并证明了我们在准确性和平滑性方面优于当前最先进的方法。我们的代码可在\\url{this https URL}访问。', 'title_zh': 'HessianForge: 具有物理信息神经表示和平滑能量约束的大规模LiDAR重建'}
{'arxiv_id': 'arXiv:2503.08695', 'title': 'Out-of-Distribution Segmentation in Autonomous Driving: Problems and State of the Art', 'authors': 'Youssef Shoeb, Azarm Nowzad, Hanno Gottschalk', 'link': 'https://arxiv.org/abs/2503.08695', 'abstract': 'In this paper, we review the state of the art in Out-of-Distribution (OoD) segmentation, with a focus on road obstacle detection in automated driving as a real-world application. We analyse the performance of existing methods on two widely used benchmarks, SegmentMeIfYouCan Obstacle Track and LostAndFound-NoKnown, highlighting their strengths, limitations, and real-world applicability. Additionally, we discuss key challenges and outline potential research directions to advance the field. Our goal is to provide researchers and practitioners with a comprehensive perspective on the current landscape of OoD segmentation and to foster further advancements toward safer and more reliable autonomous driving systems.', 'abstract_zh': '本文回顾了异常分布（OoD）分割的最新进展，并重点分析了在自动驾驶中道路障碍检测的实际应用。我们在SegmentMeIfYouCan Obstacle Track和LostAndFound-NoKnown两个广泛使用的基准上评估现有方法的表现，指出了它们的优势、局限性和实际应用前景。此外，我们讨论了关键挑战，并概述了潜在的研究方向以推进该领域的发展。本文旨在为研究人员和实践者提供当前异常分布分割领域全面的视角，并促进向更安全可靠的自动驾驶系统的进一步发展。', 'title_zh': '自动驾驶中未知分布分割：问题与最新进展'}
{'arxiv_id': 'arXiv:2503.09537', 'title': 'GenHPE: Generative Counterfactuals for 3D Human Pose Estimation with Radio Frequency Signals', 'authors': 'Shuokang Huang, Julie A. McCann', 'link': 'https://arxiv.org/abs/2503.09537', 'abstract': 'Human pose estimation (HPE) detects the positions of human body joints for various applications. Compared to using cameras, HPE using radio frequency (RF) signals is non-intrusive and more robust to adverse conditions, exploiting the signal variations caused by human interference. However, existing studies focus on single-domain HPE confined by domain-specific confounders, which cannot generalize to new domains and result in diminished HPE performance. Specifically, the signal variations caused by different human body parts are entangled, containing subject-specific confounders. RF signals are also intertwined with environmental noise, involving environment-specific confounders. In this paper, we propose GenHPE, a 3D HPE approach that generates counterfactual RF signals to eliminate domain-specific confounders. GenHPE trains generative models conditioned on human skeleton labels, learning how human body parts and confounders interfere with RF signals. We manipulate skeleton labels (i.e., removing body parts) as counterfactual conditions for generative models to synthesize counterfactual RF signals. The differences between counterfactual signals approximately eliminate domain-specific confounders and regularize an encoder-decoder model to learn domain-independent representations. Such representations help GenHPE generalize to new subjects/environments for cross-domain 3D HPE. We evaluate GenHPE on three public datasets from WiFi, ultra-wideband, and millimeter wave. Experimental results show that GenHPE outperforms state-of-the-art methods and reduces estimation errors by up to 52.2mm for cross-subject HPE and 10.6mm for cross-environment HPE.', 'abstract_zh': 'RF信号生成的人体姿态估计（GenHPE）：生成反事实RF信号消除领域特定混杂因素的3D姿态估计方法', 'title_zh': 'GenHPE: 基于无线电频率信号的生成对抗性-counterfactuals人体三维姿态估计'}
{'arxiv_id': 'arXiv:2503.09535', 'title': 'Evaluating Visual Explanations of Attention Maps for Transformer-based Medical Imaging', 'authors': 'Minjae Chung, Jong Bum Won, Ganghyun Kim, Yujin Kim, Utku Ozbulak', 'link': 'https://arxiv.org/abs/2503.09535', 'abstract': 'Although Vision Transformers (ViTs) have recently demonstrated superior performance in medical imaging problems, they face explainability issues similar to previous architectures such as convolutional neural networks. Recent research efforts suggest that attention maps, which are part of decision-making process of ViTs can potentially address the explainability issue by identifying regions influencing predictions, especially in models pretrained with self-supervised learning. In this work, we compare the visual explanations of attention maps to other commonly used methods for medical imaging problems. To do so, we employ four distinct medical imaging datasets that involve the identification of (1) colonic polyps, (2) breast tumors, (3) esophageal inflammation, and (4) bone fractures and hardware implants. Through large-scale experiments on the aforementioned datasets using various supervised and self-supervised pretrained ViTs, we find that although attention maps show promise under certain conditions and generally surpass GradCAM in explainability, they are outperformed by transformer-specific interpretability methods. Our findings indicate that the efficacy of attention maps as a method of interpretability is context-dependent and may be limited as they do not consistently provide the comprehensive insights required for robust medical decision-making.', 'abstract_zh': '尽管视觉transformer（ViT）在医疗影像问题上最近展示了优越的性能，它们在可解释性方面面临着与卷积神经网络等先前架构相似的问题。近期的研究表明，作为ViT决策过程一部分的注意力图有潜力通过识别影响预测的区域来解决可解释性问题，尤其是在自我监督学习预训练的模型中。在本工作中，我们比较了注意力图的视觉解释与其他常用方法在医疗影像问题中的表现。为此，我们采用了涉及（1）结肠息肉识别，（2）乳腺肿瘤识别，（3）食管炎症识别，以及（4）骨折和医疗器械植入物识别的四种不同的医疗影像数据集。通过在上述数据集上使用各种监督和自我监督预训练的ViT进行大规模实验，我们发现虽然注意力图在某些条件下具有前景，在可解释性方面通常超越了GradCAM，但在解释性方面仍被特定于transformer的可解释性方法超越。我们的研究结果表明，注意力图作为解释方法的有效性是情境依赖的，并且可能受到限制，因为它们不能一致地提供进行稳健医疗决策所需的全面洞察。', 'title_zh': '基于变换器的医学影像注意力图可视化解释评估'}
{'arxiv_id': 'arXiv:2503.09436', 'title': 'PromptMap: An Alternative Interaction Style for AI-Based Image Generation', 'authors': 'Krzysztof Adamkiewicz, Paweł W. Woźniak, Julia Dominiak, Andrzej Romanowski, Jakob Karolus, Stanislav Frolov', 'link': 'https://arxiv.org/abs/2503.09436', 'abstract': 'Recent technological advances popularized the use of image generation among the general public. Crafting effective prompts can, however, be difficult for novice users. To tackle this challenge, we developed PromptMap, a new interaction style for text-to-image AI that allows users to freely explore a vast collection of synthetic prompts through a map-like view with semantic zoom. PromptMap groups images visually by their semantic similarity, allowing users to discover relevant examples. We evaluated PromptMap in a between-subject online study ($n=60$) and a qualitative within-subject study ($n=12$). We found that PromptMap supported users in crafting prompts by providing them with examples. We also demonstrated the feasibility of using LLMs to create vast example collections. Our work contributes a new interaction style that supports users unfamiliar with prompting in achieving a satisfactory image output.', 'abstract_zh': 'Recent 技术进步普及了图像生成的应用，但新手用户可能难以制作有效的提示。为了应对这一挑战，我们开发了PromptMap，这是一种新的文本到图像AI交互样式，用户可以通过类似地图的视图以语义缩放的方式自由探索庞大的合成提示集合。PromptMap通过语义相似性对图像进行视觉分组，让用户发现相关示例。我们在一项包含60名受试者的在线研究中评估了PromptMap，并进行了一项包含12名受试者的定性单被试研究。我们发现，PromptMap通过提供示例支持用户制作提示。我们还展示了使用大语言模型创建庞大示例集合的可行性。我们的工作贡献了一种新的交互样式，支持不熟悉提示技术的用户获得满意的图像输出。', 'title_zh': 'PromptMap: 一种基于AI的图像生成的替代交互方式'}
{'arxiv_id': 'arXiv:2503.09403', 'title': 'Multi-Agent Image Restoration', 'authors': 'Xu Jiang, Gehui Li, Bin Chen, Jian Zhang', 'link': 'https://arxiv.org/abs/2503.09403', 'abstract': 'Image restoration (IR) is challenging due to the complexity of real-world degradations. While many specialized and all-in-one IR models have been developed, they fail to effectively handle complex, mixed degradations. Recent agentic methods RestoreAgent and AgenticIR leverage intelligent, autonomous workflows to alleviate this issue, yet they suffer from suboptimal results and inefficiency due to their resource-intensive finetunings, and ineffective searches and tool execution trials for satisfactory outputs. In this paper, we propose MAIR, a novel Multi-Agent approach for complex IR problems. We introduce a real-world degradation prior, categorizing degradations into three types: (1) scene, (2) imaging, and (3) compression, which are observed to occur sequentially in real world, and reverse them in the opposite order. Built upon this three-stage restoration framework, MAIR emulates a team of collaborative human specialists, including a "scheduler" for overall planning and multiple "experts" dedicated to specific degradations. This design minimizes search space and trial efforts, improving image quality while reducing inference costs. In addition, a registry mechanism is introduced to enable easy integration of new tools. Experiments on both synthetic and real-world datasets show that proposed MAIR achieves competitive performance and improved efficiency over the previous agentic IR system. Code and models will be made available.', 'abstract_zh': '复杂图像恢复的多智能体方法（MAIR）', 'title_zh': '多智能体图像恢复'}
{'arxiv_id': 'arXiv:2503.09399', 'title': 'ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation', 'authors': 'Tobias Christian Nauen, Brian Moser, Federico Raue, Stanislav Frolov, Andreas Dengel', 'link': 'https://arxiv.org/abs/2503.09399', 'abstract': 'Transformers, particularly Vision Transformers (ViTs), have achieved state-of-the-art performance in large-scale image classification. However, they often require large amounts of data and can exhibit biases that limit their robustness and generalizability. This paper introduces ForAug, a novel data augmentation scheme that addresses these challenges and explicitly includes inductive biases, which commonly are part of the neural network architecture, into the training data. ForAug is constructed by using pretrained foundation models to separate and recombine foreground objects with different backgrounds, enabling fine-grained control over image composition during training. It thus increases the data diversity and effective number of training samples. We demonstrate that training on ForNet, the application of ForAug to ImageNet, significantly improves the accuracy of ViTs and other architectures by up to 4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks. Importantly, ForAug enables novel ways of analyzing model behavior and quantifying biases. Namely, we introduce metrics for background robustness, foreground focus, center bias, and size bias and show that training on ForNet substantially reduces these biases compared to training on ImageNet. In summary, ForAug provides a valuable tool for analyzing and mitigating biases, enabling the development of more robust and reliable computer vision models. Our code and dataset are publicly available at this https URL.', 'abstract_zh': 'ForAug: 一种新的数据增强方案及其在图像分类中的应用', 'title_zh': 'ForAug: 结合前景和背景以减轻偏差并改进视觉变换器训练'}
{'arxiv_id': 'arXiv:2503.09396', 'title': 'Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting with Progressive Self-Training', 'authors': 'Jiatong Xia, Lingqiao Liu', 'link': 'https://arxiv.org/abs/2503.09396', 'abstract': "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in synthesizing novel views after training on a given set of viewpoints. However, its rendering quality deteriorates when the synthesized view deviates significantly from the training views. This decline occurs due to (1) the model's difficulty in generalizing to out-of-distribution scenarios and (2) challenges in interpolating fine details caused by substantial resolution changes and occlusions. A notable case of this limitation is close-up view generation--producing views that are significantly closer to the object than those in the training set. To tackle this issue, we propose a novel approach for close-up view generation based by progressively training the 3DGS model with self-generated data. Our solution is based on three key ideas. First, we leverage the See3D model, a recently introduced 3D-aware generative model, to enhance the details of rendered views. Second, we propose a strategy to progressively expand the ``trust regions'' of the 3DGS model and update a set of reference views for See3D. Finally, we introduce a fine-tuning strategy to carefully update the 3DGS model with training data generated from the above schemes. We further define metrics for close-up views evaluation to facilitate better research on this problem. By conducting evaluations on specifically selected scenarios for close-up views, our proposed approach demonstrates a clear advantage over competitive solutions.", 'abstract_zh': '基于逐级训练的3DGS近景视图生成方法', 'title_zh': 'Close-up-GS: 在渐进自训练增强下的一种用于三维高斯点云的近距离视图合成方法'}
{'arxiv_id': 'arXiv:2503.09378', 'title': 'Pig behavior dataset and Spatial-temporal perception and enhancement networks based on the attention mechanism for pig behavior recognition', 'authors': 'Fangzheng Qi, Zhenjie Hou, En Lin, Xing Li, iuzhen Liang, Xinwen Zhou', 'link': 'https://arxiv.org/abs/2503.09378', 'abstract': "The recognition of pig behavior plays a crucial role in smart farming and welfare assurance for pigs. Currently, in the field of pig behavior recognition, the lack of publicly available behavioral datasets not only limits the development of innovative algorithms but also hampers model robustness and algorithm this http URL paper proposes a dataset containing 13 pig behaviors that significantly impact this http URL on this dataset, this paper proposes a spatial-temporal perception and enhancement networks based on the attention mechanism to model the spatiotemporal features of pig behaviors and their associated interaction areas in video data. The network is composed of a spatiotemporal perception network and a spatiotemporal feature enhancement network. The spatiotemporal perception network is responsible for establishing connections between the pigs and the key regions of their behaviors in the video data. The spatiotemporal feature enhancement network further strengthens the important spatial features of individual pigs and captures the long-term dependencies of the spatiotemporal features of individual behaviors by remodeling these connections, thereby enhancing the model's perception of spatiotemporal changes in pig behaviors. Experimental results demonstrate that on the dataset established in this paper, our proposed model achieves a MAP score of 75.92%, which is an 8.17% improvement over the best-performing traditional model. This study not only improces the accuracy and generalizability of individual pig behavior recognition but also provides new technological tools for modern smart farming. The dataset and related code will be made publicly available alongside this paper.", 'abstract_zh': '猪行为识别在智能 farming 和猪的福利保障中起着关键作用。当前，在猪行为识别领域，缺乏公开的行为数据集不仅限制了创新算法的发展，也阻碍了模型的鲁棒性和算法性能的提升。本文提出一个包含13种对猪行为影响显著的数据集。基于注意力机制，本文提出了空间-时间感知和增强网络，用于建模猪行为及其相关交互区域的空间-时间特征。该网络由空间-时间感知网络和空间-时间特征增强网络组成。空间-时间感知网络负责在视频数据中建立猪与行为关键区域之间的连接。空间-时间特征增强网络进一步增强了单个猪的重要空间特征，并通过重塑这些连接捕获个体行为的空间-时间特征的长期依赖性，从而增强了模型对猪行为空间-时间变化的感知能力。实验结果表明，与最佳传统模型相比，本文提出的模型在本文建立的数据集上的MAP得分为75.92%，提高了8.17%。本研究不仅提高了个体猪行为识别的准确性和普适性，还为现代智能 farming 提供了新的技术工具。本文还将与论文一起公开该数据集及相关代码。', 'title_zh': '基于注意力机制的猪行为识别时空注意与增强网络及猪行为数据集'}
{'arxiv_id': 'arXiv:2503.09332', 'title': 'SDD-4DGS: Static-Dynamic Aware Decoupling in Gaussian Splatting for 4D Scene Reconstruction', 'authors': 'Dai Sun, Huhao Guan, Kun Zhang, Xike Xie, S. Kevin Zhou', 'link': 'https://arxiv.org/abs/2503.09332', 'abstract': 'Dynamic and static components in scenes often exhibit distinct properties, yet most 4D reconstruction methods treat them indiscriminately, leading to suboptimal performance in both cases. This work introduces SDD-4DGS, the first framework for static-dynamic decoupled 4D scene reconstruction based on Gaussian Splatting. Our approach is built upon a novel probabilistic dynamic perception coefficient that is naturally integrated into the Gaussian reconstruction pipeline, enabling adaptive separation of static and dynamic components. With carefully designed implementation strategies to realize this theoretical framework, our method effectively facilitates explicit learning of motion patterns for dynamic elements while maintaining geometric stability for static structures. Extensive experiments on five benchmark datasets demonstrate that SDD-4DGS consistently outperforms state-of-the-art methods in reconstruction fidelity, with enhanced detail restoration for static structures and precise modeling of dynamic motions. The code will be released.', 'abstract_zh': '基于高斯Splating的静态-动态解耦4D场景重建框架SDD-4DGS', 'title_zh': 'SDD-4DGS：静态-动态感知的高斯点云解耦在四维场景重建中的应用'}
{'arxiv_id': 'arXiv:2503.09151', 'title': 'Reangle-A-Video: 4D Video Generation as Video-to-Video Translation', 'authors': 'Hyeonho Jeong, Suhyeon Lee, Jong Chul Ye', 'link': 'https://arxiv.org/abs/2503.09151', 'abstract': 'We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: this https URL', 'abstract_zh': '我们介绍了Reangle-A-Video，这是一种生成单输入视频同步多视角视频的统一框架。与主流方法通过大规模4D数据集训练多视角视频扩散模型不同，我们的方法将多视角视频生成任务重新 framing 为视频到视频的翻译，利用公开可用的图像和视频扩散先验。本质上，Reangle-A-Video 分为两个阶段进行。 (1) 多视角运动学习：一个图像到视频的扩散变换器在自我监督的方式下同步微调，以从一组变形视频中提炼出视点不变的运动。 (2) 多视角一致的图像到图像翻译：在推理时跨视角一致性指导之下，使用DUSt3R将输入视频的首帧变形并填补到各种摄像机视角，生成多视角一致的起始图像。广泛的静态视图传输和动态摄像机控制实验表明，Reangle-A-Video 超越了现有方法，为多视角视频生成提供了新的解决方案。我们将公开发布我们的代码和数据。项目页面：这个\xa0https://链接URL。', 'title_zh': 'Reangle-A-Video: 视频到视频的4D视频生成'}
{'arxiv_id': 'arXiv:2503.09132', 'title': 'Investigation of Frame Differences as Motion Cues for Video Object Segmentation', 'authors': 'Sota Kawamura, Hirotada Honda, Shugo Nakamura, Takashi Sano', 'link': 'https://arxiv.org/abs/2503.09132', 'abstract': 'Automatic Video Object Segmentation (AVOS) refers to the task of autonomously segmenting target objects in video sequences without relying on human-provided annotations in the first frames. In AVOS, the use of motion information is crucial, with optical flow being a commonly employed method for capturing motion cues. However, the computation of optical flow is resource-intensive, making it unsuitable for real-time applications, especially on edge devices with limited computational resources. In this study, we propose using frame differences as an alternative to optical flow for motion cue extraction. We developed an extended U-Net-like AVOS model that takes a frame on which segmentation is performed and a frame difference as inputs, and outputs an estimated segmentation map. Our experimental results demonstrate that the proposed model achieves performance comparable to the model with optical flow as an input, particularly when applied to videos captured by stationary cameras. Our results suggest the usefulness of employing frame differences as motion cues in cases with limited computational resources.', 'abstract_zh': '自动视频对象分割（AVOS）是指在视频序列中自主分割目标对象的任务，而无需依赖人类提供的初始帧注释。在AVOS中，运动信息的使用至关重要，光学流是常用的方法之一，用于捕捉运动线索。然而，计算光学流资源密集，使其不适合实时应用，特别是在计算资源受限的边缘设备上。在本研究中，我们提出了使用帧差异作为替代方法来提取运动线索。我们开发了一种扩展的类似U-Net的AVOS模型，该模型将用于分割的帧和帧差异作为输入，并输出估计的分割图。我们的实验结果表明，所提出模型在应用到由固定摄像机拍摄的视频时，其性能与将光学流作为输入的模型相当。我们的结果表明，在计算资源有限的情况下，使用帧差异作为运动线索具有实用性。', 'title_zh': '帧差异在视频物体分割中的运动线索研究'}
{'arxiv_id': 'arXiv:2503.09046', 'title': 'Discovering Influential Neuron Path in Vision Transformers', 'authors': 'Yifan Wang, Yifei Liu, Yingdong Shi, Changming Li, Anqi Pang, Sibei Yang, Jingyi Yu, Kan Ren', 'link': 'https://arxiv.org/abs/2503.09046', 'abstract': "Vision Transformer models exhibit immense power yet remain opaque to human understanding, posing challenges and risks for practical applications. While prior research has attempted to demystify these models through input attribution and neuron role analysis, there's been a notable gap in considering layer-level information and the holistic path of information flow across layers. In this paper, we investigate the significance of influential neuron paths within vision Transformers, which is a path of neurons from the model input to output that impacts the model inference most significantly. We first propose a joint influence measure to assess the contribution of a set of neurons to the model outcome. And we further provide a layer-progressive neuron locating approach that efficiently selects the most influential neuron at each layer trying to discover the crucial neuron path from input to output within the target model. Our experiments demonstrate the superiority of our method finding the most influential neuron path along which the information flows, over the existing baseline solutions. Additionally, the neuron paths have illustrated that vision Transformers exhibit some specific inner working mechanism for processing the visual information within the same image category. We further analyze the key effects of these neurons on the image classification task, showcasing that the found neuron paths have already preserved the model capability on downstream tasks, which may also shed some lights on real-world applications like model pruning. The project website including implementation code is available at this https URL.", 'abstract_zh': '视觉Transformer模型展现出巨大的能力但依旧对人类理解不透明，这为其实用应用带来了挑战和风险。尽管早期研究试图通过输入归因和神经元作用分析来揭开这些模型的面纱，但在考虑层级信息以及跨层信息流的整体路径方面仍存在明显不足。在本文中，我们探讨了视觉Transformer中具有影响力的神经元路径的重要性，这是一种从模型输入到输出的路径，对模型推断的影响最大。我们首先提出了一种联合影响度量方法来评估一组神经元对模型结果的贡献，并进一步提供了一种逐层神经元定位方法，该方法可以在每一层中高效地选择最具影响力的神经元，以发现目标模型从输入到输出的关键神经元路径。我们的实验表明，我们方法找到的信息流路径相比现有的基线解决方案具有优势。此外，神经元路径表明视觉Transformer在处理相同图像类别内的视觉信息时表现出一些特定的内部工作机制。我们进一步分析了这些神经元对抗图像分类任务的关键影响，展示了找到的神经元路径已经保留了模型在下游任务中的能力，这也有助于模型剪枝等实际应用。项目网站和实现代码可通过以下链接访问。', 'title_zh': '发现视觉transformer中的关键神经元路径'}
{'arxiv_id': 'arXiv:2503.08939', 'title': 'KAN-Mixers: a new deep learning architecture for image classification', 'authors': 'Jorge Luiz dos Santos Canuto, Linnyer Beatrys Ruiz Aylon, Rodrigo Clemente Thom de Souza', 'link': 'https://arxiv.org/abs/2503.08939', 'abstract': 'Due to their effective performance, Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures have become the standard for solving computer vision tasks. Such architectures require large data sets and rely on convolution and self-attention operations. In 2021, MLP-Mixer emerged, an architecture that relies only on Multilayer Perceptron (MLP) and achieves extremely competitive results when compared to CNNs and ViTs. Despite its good performance in computer vision tasks, the MLP-Mixer architecture may not be suitable for refined feature extraction in images. Recently, the Kolmogorov-Arnold Network (KAN) was proposed as a promising alternative to MLP models. KANs promise to improve accuracy and interpretability when compared to MLPs. Therefore, the present work aims to design a new mixer-based architecture, called KAN-Mixers, using KANs as main layers and evaluate its performance, in terms of several performance metrics, in the image classification task. As main results obtained, the KAN-Mixers model was superior to the MLP, MLP-Mixer and KAN models in the Fashion-MNIST and CIFAR-10 datasets, with 0.9030 and 0.6980 of average accuracy, respectively.', 'abstract_zh': '基于KAN的混合架构在图像分类任务中的性能评估', 'title_zh': 'KAN-Mixers：一种新的图像分类深度学习架构'}
{'arxiv_id': 'arXiv:2503.08867', 'title': 'Zero-Shot Action Generalization with Limited Observations', 'authors': 'Abdullah Alchihabi, Hanping Zhang, Yuhong Guo', 'link': 'https://arxiv.org/abs/2503.08867', 'abstract': 'Reinforcement Learning (RL) has demonstrated remarkable success in solving sequential decision-making problems. However, in real-world scenarios, RL agents often struggle to generalize when faced with unseen actions that were not encountered during training. Some previous works on zero-shot action generalization rely on large datasets of action observations to capture the behaviors of new actions, making them impractical for real-world applications. In this paper, we introduce a novel zero-shot framework, Action Generalization from Limited Observations (AGLO). Our framework has two main components: an action representation learning module and a policy learning module. The action representation learning module extracts discriminative embeddings of actions from limited observations, while the policy learning module leverages the learned action representations, along with augmented synthetic action representations, to learn a policy capable of handling tasks with unseen actions. The experimental results demonstrate that our framework significantly outperforms state-of-the-art methods for zero-shot action generalization across multiple benchmark tasks, showcasing its effectiveness in generalizing to new actions with minimal action observations.', 'abstract_zh': '基于有限观测的零样本动作泛化框架（Action Generalization from Limited Observations）', 'title_zh': '基于有限观察的零样本动作泛化'}
{'arxiv_id': 'arXiv:2503.08745', 'title': 'Neural Network for Blind Unmixing: a novel MatrixConv Unmixing (MCU) Approach', 'authors': 'Chao Zhou, Wei Pu, Miguel Rodrigues', 'link': 'https://arxiv.org/abs/2503.08745', 'abstract': 'Hyperspectral image (HSI) unmixing is a challenging research problem that tries to identify the constituent components, known as endmembers, and their corresponding proportions, known as abundances, in the scene by analysing images captured by hyperspectral cameras. Recently, many deep learning based unmixing approaches have been proposed with the surge of machine learning techniques, especially convolutional neural networks (CNN). However, these methods face two notable challenges: 1. They frequently yield results lacking physical significance, such as signatures corresponding to unknown or non-existent materials. 2. CNNs, as general-purpose network structures, are not explicitly tailored for unmixing tasks. In response to these concerns, our work draws inspiration from double deep image prior (DIP) techniques and algorithm unrolling, presenting a novel network structure that effectively addresses both issues. Specifically, we first propose a MatrixConv Unmixing (MCU) approach for endmember and abundance estimation, respectively, which can be solved via certain iterative solvers. We then unroll these solvers to build two sub-networks, endmember estimation DIP (UEDIP) and abundance estimation DIP (UADIP), to generate the estimation of endmember and abundance, respectively. The overall network is constructed by assembling these two sub-networks. In order to generate meaningful unmixing results, we also propose a composite loss function. To further improve the unmixing quality, we also add explicitly a regularizer for endmember and abundance estimation, respectively. The proposed methods are tested for effectiveness on both synthetic and real datasets.', 'abstract_zh': '高光谱图像(HSI)解混是挑战性的研究问题，旨在通过分析高光谱相机捕获的图像来识别构成组件（称为端元）及其相应的比例（称为丰度）。近年来，随着机器学习技术特别是卷积神经网络(CNN)的兴起，提出了许多基于深度学习的解混方法。然而，这些方法面临两个显著挑战：1. 经常产生缺乏物理意义的结果，例如与未知或不存在的材料对应的签名。2. 卷积神经网络作为通用的网络结构，并未专门为解混任务进行明确设计。为应对这些问题，我们的工作借鉴了双层深度图像先验(DIP)技术和算法展开技术，提出了一种新的网络结构，有效解决了上述两个问题。具体地，我们首先提出了矩阵卷积解混(MCU)方法，分别用于端元和丰度估计，这两种方法可以通过某些迭代求解器解决。然后，我们将这些求解器展开以构建两个子网络：端元估计DIP(UEDIP)和丰度估计DIP(UADIP)，以分别生成端元和丰度的估计值。整个网络通过组装这两个子网络来构建。为了生成有意义的解混结果，我们还提出了复合损失函数。为了进一步提高解混质量，我们还分别对端元和丰度估计添加了正则化项。我们对合成数据集和真实数据集的有效性进行了测试。', 'title_zh': '基于神经网络的盲解混：一种新型MatrixConv解混（MCU）方法'}
{'arxiv_id': 'arXiv:2503.08737', 'title': 'Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models', 'authors': 'In Cho, Youngbeom Yoo, Subin Jeon, Seon Joo Kim', 'link': 'https://arxiv.org/abs/2503.08737', 'abstract': 'Constructing a compressed latent space through a variational autoencoder (VAE) is the key for efficient 3D diffusion models. This paper introduces COD-VAE, a VAE that encodes 3D shapes into a COmpact set of 1D latent vectors without sacrificing quality. COD-VAE introduces a two-stage autoencoder scheme to improve compression and decoding efficiency. First, our encoder block progressively compresses point clouds into compact latent vectors via intermediate point patches. Second, our triplane-based decoder reconstructs dense triplanes from latent vectors instead of directly decoding neural fields, significantly reducing computational overhead of neural fields decoding. Finally, we propose uncertainty-guided token pruning, which allocates resources adaptively by skipping computations in simpler regions and improves the decoder efficiency. Experimental results demonstrate that COD-VAE achieves 16x compression compared to the baseline while maintaining quality. This enables 20.8x speedup in generation, highlighting that a large number of latent vectors is not a prerequisite for high-quality reconstruction and generation.', 'abstract_zh': '通过变分自编码器（VAE）构建压缩隐空间是高效3D扩散模型的关键。本文介绍了一种名为COD-VAE的VAE，它将3D形状编码为紧凑的1D隐区间集合，而不牺牲质量。COD-VAE引入了两阶段自编码方案以提高压缩和解码效率。首先，我们的编码块逐步通过中间点片将点云压缩为紧凑的隐向量。其次，我们的基于三平面的解码器从隐向量重构密集的三平面，而不是直接解码神经场，从而显著减少了神经场解码的计算开销。最后，我们提出了基于不确定性指导的标记剪枝方法，通过在简单区域跳过计算来适应性地分配资源，从而提高解码效率。实验结果表明，与 baseline 相比，COD-VAE 实现了16倍的压缩比，同时保持了质量。这使得生成速度提高了20.8倍，展示了高质量重建和生成并不一定需要大量的隐向量。', 'title_zh': '用64个潜在向量表示3D形状：适用于3D扩散模型'}
{'arxiv_id': 'arXiv:2503.08729', 'title': 'Preserving Product Fidelity in Large Scale Image Recontextualization with Diffusion Models', 'authors': 'Ishaan Malhi, Praneet Dutta, Ellie Talius, Sally Ma, Brendan Driscoll, Krista Holden, Garima Pruthi, Arunachalam Narayanaswamy', 'link': 'https://arxiv.org/abs/2503.08729', 'abstract': "We present a framework for high-fidelity product image recontextualization using text-to-image diffusion models and a novel data augmentation pipeline. This pipeline leverages image-to-video diffusion, in/outpainting & negatives to create synthetic training data, addressing limitations of real-world data collection for this task. Our method improves the quality and diversity of generated images by disentangling product representations and enhancing the model's understanding of product characteristics. Evaluation on the ABO dataset and a private product dataset, using automated metrics and human assessment, demonstrates the effectiveness of our framework in generating realistic and compelling product visualizations, with implications for applications such as e-commerce and virtual product showcasing.", 'abstract_zh': '基于文本到图像扩散模型和新型数据增强管道的高保真产品图重新语境化框架', 'title_zh': '在大规模图像重新情境化中保持产品 fidelity 的扩散模型方法'}
{'arxiv_id': 'arXiv:2503.08700', 'title': 'Real-Time Semantic Segmentation of Aerial Images Using an Embedded U-Net: A Comparison of CPU, GPU, and FPGA Workflows', 'authors': 'Julien Posso, Hugo Kieffer, Nicolas Menga, Omar Hlimi, Sébastien Tarris, Hubert Guerard, Guy Bois, Matthieu Couderc, Eric Jenn', 'link': 'https://arxiv.org/abs/2503.08700', 'abstract': "This study introduces a lightweight U-Net model optimized for real-time semantic segmentation of aerial images, targeting the efficient utilization of Commercial Off-The-Shelf (COTS) embedded computing platforms. We maintain the accuracy of the U-Net on a real-world dataset while significantly reducing the model's parameters and Multiply-Accumulate (MAC) operations by a factor of 16. Our comprehensive analysis covers three hardware platforms (CPU, GPU, and FPGA) and five different toolchains (TVM, FINN, Vitis AI, TensorFlow GPU, and cuDNN), assessing each on metrics such as latency, power consumption, memory footprint, energy efficiency, and FPGA resource usage. The results highlight the trade-offs between these platforms and toolchains, with a particular focus on the practical deployment challenges in real-world applications. Our findings demonstrate that while the FPGA with Vitis AI emerges as the superior choice due to its performance, energy efficiency, and maturity, it requires specialized hardware knowledge, emphasizing the need for a balanced approach in selecting embedded computing solutions for semantic segmentation tasks", 'abstract_zh': '本研究介绍了一种针对机载图像实时语义分割优化的轻量化U-Net模型，旨在高效利用商用现货（COTS）嵌入式计算平台。我们在保持U-Net在真实数据集上准确性的基础上，显著减少了模型的参数和乘积累加（MAC）操作，降低了16倍。我们的综合分析涵盖了三种硬件平台（CPU、GPU和FPGA）和五种不同的工具链（TVM、FINN、Vitis AI、TensorFlow GPU和cuDNN），评估指标包括延迟、功耗、内存占用、能量效率和FPGA资源使用情况。研究结果强调了这些平台和工具链之间的权衡，特别关注实际应用场景中的部署挑战。我们的发现表明，尽管使用Vitis AI的FPGA因其性能、能源效率和成熟度而成为更优选择，但其需要专门的硬件知识，强调了在语义分割任务中选择嵌入式计算解决方案时需要平衡这一点。', 'title_zh': '使用嵌入式U-Net进行实时机载图像语义分割：CPU、GPU和FPGA工作流的比较'}
{'arxiv_id': 'arXiv:2503.04824', 'title': 'ProReflow: Progressive Reflow with Decomposed Velocity', 'authors': 'Lei Ke, Haohang Xu, Xuefei Ning, Yu Li, Jiajun Li, Haoling Li, Yuxuan Lin, Dongsheng Jiang, Yujiu Yang, Linfeng Zhang', 'link': 'https://arxiv.org/abs/2503.04824', 'abstract': 'Diffusion models have achieved significant progress in both image and video generation while still suffering from huge computation costs. As an effective solution, flow matching aims to reflow the diffusion process of diffusion models into a straight line for a few-step and even one-step generation. However, in this paper, we suggest that the original training pipeline of flow matching is not optimal and introduce two techniques to improve it. Firstly, we introduce progressive reflow, which progressively reflows the diffusion models in local timesteps until the whole diffusion progresses, reducing the difficulty of flow matching. Second, we introduce aligned v-prediction, which highlights the importance of direction matching in flow matching over magnitude matching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness of our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on MSCOCO2014 validation set with only 4 sampling steps, close to our teacher model (32 DDIM steps, FID = 10.05).', 'abstract_zh': '流匹配技术在改进扩散模型图像和视频生成中的应用及其优化方法', 'title_zh': '渐进式分解速度再镕: ProReflow'}
