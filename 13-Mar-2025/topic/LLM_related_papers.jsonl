{'arxiv_id': 'arXiv:2503.09567', 'title': 'Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models', 'authors': 'Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, Wangxiang Che', 'link': 'https://arxiv.org/abs/2503.09567', 'abstract': 'Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies in the application of long chain-of-thought (Long CoT) characteristics, which enhance reasoning abilities and enable the solution of intricate problems. However, despite these developments, a comprehensive survey on Long CoT is still lacking, limiting our understanding of its distinctions from traditional short chain-of-thought (Short CoT) and complicating ongoing debates on issues like "overthinking" and "test-time scaling." This survey seeks to fill this gap by offering a unified perspective on Long CoT. (1) We first distinguish Long CoT from Short CoT and introduce a novel taxonomy to categorize current reasoning paradigms. (2) Next, we explore the key characteristics of Long CoT: deep reasoning, extensive exploration, and feasible reflection, which enable models to handle more complex tasks and produce more efficient, coherent outcomes compared to the shallower Short CoT. (3) We then investigate key phenomena such as the emergence of Long CoT with these characteristics, including overthinking, and test-time scaling, offering insights into how these processes manifest in practice. (4) Finally, we identify significant research gaps and highlight promising future directions, including the integration of multi-modal reasoning, efficiency improvements, and enhanced knowledge frameworks. By providing a structured overview, this survey aims to inspire future research and further the development of logical reasoning in artificial intelligence.', 'abstract_zh': 'Recent advancements in reasoning with large language models (RLLMs)，如OpenAI-O1和DeepSeek-R1，在数学和编码等复杂领域展示了其出色的能力。其成功的关键因素之一在于长链推理（Long CoT）特征的应用，这增强了推理能力，并使解决复杂问题成为可能。然而，尽管取得这些进展，关于长链推理的全面综述仍然缺乏，限制了我们对它与传统短链推理（Short CoT）的区别理解，并使得关于“过度推理”和“测试时扩展”等问题的争论复杂化。本综述旨在通过提供统一的视角来填补这一空白。(1) 首先，我们将长链推理与短链推理区分开来，并引入一种新颖的分类法来划分当前的推理范式。(2) 然后，我们探讨长链推理的关键特征：深入推理、广泛探索和可行的反思，这些特征使模型能够处理更复杂的任务，并比表层的短链推理产生更有效和连贯的结果。(3) 接着，我们研究这些特征出现的关键现象，包括过度推理和测试时扩展等，提供这些过程在实践中表现的见解。(4) 最后，我们确定了重要的研究缺口，并强调了有希望的未来方向，包括多模态推理的整合、效率改进和知识框架的增强。通过提供结构化的概述，本综述旨在激发未来研究，并推动人工智能中逻辑推理的发展。', 'title_zh': '向推理时代迈进：长链推理综述——针对大规模语言模型的逻辑推理研究'}
{'arxiv_id': 'arXiv:2503.09501', 'title': 'ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning', 'authors': 'Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen', 'link': 'https://arxiv.org/abs/2503.09501', 'abstract': 'Recent research on Reasoning of Large Language Models (LLMs) has sought to further enhance their performance by integrating meta-thinking -- enabling models to monitor, evaluate, and control their reasoning processes for more adaptive and effective problem-solving. However, current single-agent work lacks a specialized design for acquiring meta-thinking, resulting in low efficacy. To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking. ReMA decouples the reasoning process into two hierarchical agents: a high-level meta-thinking agent responsible for generating strategic oversight and plans, and a low-level reasoning agent for detailed executions. Through iterative reinforcement learning with aligned objectives, these agents explore and learn collaboration, leading to improved generalization and robustness. Experimental results demonstrate that ReMA outperforms single-agent RL baselines on complex reasoning tasks, including competitive-level mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation studies further illustrate the evolving dynamics of each distinct agent, providing valuable insights into how the meta-thinking reasoning process enhances the reasoning capabilities of LLMs.', 'abstract_zh': 'Recent研究大语言模型（LLMs）的推理研究旨在通过整合元思考进一步提升其性能，使模型能够监控、评估和控制其推理过程，实现更适应性和有效的解决问题。然而，当前单智能体工作的设计未能专门针对获得元思考进行优化，导致其效果较低。为应对这一挑战，我们提出了一种新型框架——强化元思考智能体（ReMA），该框架利用多智能体强化学习（MARL）来激发元思考行为，鼓励大语言模型进行反思性思考。ReMA 将推理过程分解为两个层次的智能体：高层元思考智能体负责生成战略监督和计划，而低层推理智能体负责详细的执行。通过具有对齐目标的迭代强化学习，这些智能体探索并学习协作，从而提高泛化能力和鲁棒性。实验结果表明，ReMA 在复杂推理任务（包括竞争级别的数学基准和大语言模型作为裁判的基准）中优于单智能体 RL 基准。全面的消融研究进一步阐述了每个独立智能体的演变动态，提供了宝贵的见解，说明了元思考推理过程如何增强大语言模型的推理能力。', 'title_zh': 'ReMA：利用多 agent 强化学习进行元思考的大型语言模型学习方法'}
{'arxiv_id': 'arXiv:2503.09598', 'title': 'How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation', 'authors': 'Ruohao Guo, Wei Xu, Alan Ritter', 'link': 'https://arxiv.org/abs/2503.09598', 'abstract': "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the extent to which they could tacitly spread misinformation emerges as a critical safety concern. Current research primarily evaluates LLMs on explicit false statements, overlooking how misinformation often manifests subtly as unchallenged premises in real-world user interactions. We curated ECHOMIST, the first comprehensive benchmark for implicit misinformation, where the misinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based on rigorous selection criteria and carefully curated data from diverse sources, including real-world human-AI conversations and social media interactions. We also introduce a new evaluation metric to measure whether LLMs can recognize and counter false information rather than amplify users' misconceptions. Through an extensive empirical study on a wide range of LLMs, including GPT-4, Claude, and Llama, we find that current models perform alarmingly poorly on this task, often failing to detect false premises and generating misleading explanations. Our findings underscore the critical need for an increased focus on implicit misinformation in LLM safety research.", 'abstract_zh': '作为大规模语言模型（LLMs）在各种场景中广泛部署，它们默示传播虚假信息的程度已成为一个关键的安全 concern。当前的研究主要评估LLMs在显性错误陈述上的表现，忽视了虚假信息在实际用户交互中常以未受挑战的前提形式悄然出现。我们编纂了ECHOMIST，这是首个全面的隐性虚假信息基准数据集，其中的误导性假设被嵌入到用户针对LLMs的查询中。ECHOMIST基于严格的筛选标准，数据来源多样，包括现实世界的人机对话和社会媒体互动。我们还引入了一个新的评估指标，以衡量LLMs能否识别和反驳虚假信息，而不是放大用户的误解。通过对包括GPT-4、Claude和Llama在内的多种LLM进行广泛的实证研究，我们发现当前的模型在这项任务上的表现令人担忧，经常无法检测到虚假前提并生成误导性解释。我们的研究结果强调了在LLM安全研究中加强对隐性虚假信息的关注的紧迫性。', 'title_zh': '如何防护5G辐射？探究大语言模型对隐含虚假信息的响应'}
{'arxiv_id': 'arXiv:2503.09579', 'title': 'Cost-Optimal Grouped-Query Attention for Long-Context LLMs', 'authors': 'Yingfa Chen, Yutong Wu, Xu Han, Zhiyuan Liu, Maosong Sun', 'link': 'https://arxiv.org/abs/2503.09579', 'abstract': 'Building effective and efficient Transformer-based large language models (LLMs) has recently become a research focus, requiring maximizing model language capabilities and minimizing training and deployment costs. Existing efforts have primarily described complex relationships among model performance, parameter size, and data size, as well as searched for the optimal compute allocation to train LLMs. However, they overlook the impacts of context length and attention head configuration (the number of query and key-value heads in grouped-query attention) on training and inference. In this paper, we systematically compare models with different parameter sizes, context lengths, and attention head configurations in terms of model performance, computational cost, and memory cost. Then, we extend the existing scaling methods, which are based solely on parameter size and training compute, to guide the construction of cost-optimal LLMs during both training and inference. Our quantitative scaling studies show that, when processing sufficiently long sequences, a larger model with fewer attention heads can achieve a lower loss while incurring lower computational and memory costs. Our findings provide valuable insights for developing practical LLMs, especially in long-context processing scenarios. We will publicly release our code and data.', 'abstract_zh': '构建高效且有效的基于Transformer的大语言模型（LLMs）近期成为研究重点，需要最大化模型语言能力并最小化训练和部署成本。现有工作主要描述了模型性能、参数量和数据量之间复杂的相互关系，并寻找训练LLMs的最佳计算分配。然而，它们忽略了上下文长度和注意力头配置（分组查询注意力中的查询和键值头的数量）对训练和推理的影响。在本文中，我们系统地比较了具有不同参数量、上下文长度和注意力头配置的模型在模型性能、计算成本和内存成本方面的差异。然后，我们将现有的仅基于参数量和训练计算量的扩展方法扩展，以指导在训练和推理过程中构建成本最优的LLMs。我们的定量扩展研究表明，处理足够长的序列时，具有较少注意力头的大型模型可以在较低的计算和内存成本下实现较低的损失。我们的发现为开发实际的LLMs，尤其是在长上下文处理场景中提供了宝贵的见解。我们将公开发布我们的代码和数据。', 'title_zh': '面向长期上下文LLM的分组查询注意力成本优化技术'}
{'arxiv_id': 'arXiv:2503.09573', 'title': 'Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models', 'authors': 'Marianne Arriola, Aaron Gokaslan, Justin T Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, Volodymyr Kuleshov', 'link': 'https://arxiv.org/abs/2503.09573', 'abstract': 'Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: this https URL', 'abstract_zh': '扩散语言模型因其并行生成和可控性等独特优势，在概率建模和固定长度生成方面优于自回归模型。然而，它们在概率建模方面仍有不足，并且受限于固定长度生成。本文介绍了一类块扩散语言模型，介于离散去噪扩散和自回归模型之间。块扩散通过支持灵活长度生成和通过KV缓存和并行 token 抽样提高推理效率，解决了这两种方法的关键限制。我们提出了一种构建有效块扩散模型的方案，包括高效的训练算法、梯度方差估计器以及数据驱动的噪声调度，以最小化方差。块扩散在语言建模基准测试中设定了新的性能标准，并且能够生成任意长度的序列。项目代码、模型权重和博客文章可在项目页面获取：this https URL', 'title_zh': '块扩散：自回归与扩散语言模型的插值'}
{'arxiv_id': 'arXiv:2503.09516', 'title': 'Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning', 'authors': 'Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, Jiawei Han', 'link': 'https://arxiv.org/abs/2503.09516', 'abstract': 'Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Retrieval augmentation and tool-use training approaches where a search engine is treated as a tool lack complex multi-turn retrieval flexibility or require large-scale supervised data. Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns -- solely through reinforcement learning (RL) -- to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at this https URL.', 'abstract_zh': '高效地获取外部知识和最新的信息对于大型语言模型（LLMs）的有效推理和文本生成至关重要。将搜索引擎作为工具的检索增强和工具使用训练方法缺乏复杂的多轮检索灵活性，或者需要大量的监督数据。在推理过程中通过提示具有推理能力的高级LLM使用搜索引擎并不是最优方案，因为LLM并未学会如何与搜索引擎进行最优互动。本文介绍了Search-R1，这是DeepSeek-R1模型的一种扩展，使得LLM通过强化学习（RL）自主在逐步推理过程中实时生成多个搜索查询。Search-R1利用检索到的标记掩蔽进行稳定的学习，并采用基于简单结果的奖励函数来优化LLM的滚动部署。在七个问答数据集上的实验表明，Search-R1分别在Qwen2.5-7B、Qwen2.5-3B和LLaMA3.2-3B上比SOTA基线提高了26%、21%和10%的性能。本文还进一步提供了关于RL优化方法、LLM选择和响应长度动态的实证见解。相关代码和模型检查点可在以下链接获取。', 'title_zh': 'Search-R1: 通过强化学习训练大规模语言模型进行推理和利用搜索引擎'}
{'arxiv_id': 'arXiv:2503.09433', 'title': 'CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection', 'authors': 'Richard A. Dubniczky, Krisztofer Zoltán Horvát, Tamás Bisztray, Mohamed Amine Ferrag, Lucas C. Cordeiro, Norbert Tihanyi', 'link': 'https://arxiv.org/abs/2503.09433', 'abstract': 'Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at this https URL.', 'abstract_zh': '识别源代码中的漏洞在关键软件组件中尤为重要。现有的方法如静态分析、动态分析、形式验证以及最近的大规模语言模型被广泛用于检测安全缺陷。本文引入了CASTLE（CWE Automated Security Testing and Low-Level Evaluation）基准框架，以评估不同方法的漏洞检测能力。我们使用一个手工制作的数据集评估了13种静态分析工具、10种大模型和2种形式验证工具，该数据集涵盖了250个微型基准程序和25种常见的CWE。我们提出了CASTLE评分，这是一种新的评估指标，以确保公平比较。结果显示：形式验证工具ESBMC能够最小化误报，但在超出模型检查范围的漏洞，如弱加密或SQL注入方面表现不佳。静态分析器因高误报率增加开发者的手动验证工作。大模型在CASTLE数据集中识别小型代码片段中的漏洞表现十分出色，但在代码规模增长时，其准确性下降且生成错误信息增多。这些结果表明，大模型可能在未来安全解决方案中扮演关键角色，特别是在代码补全框架中，它们可以提供实时指导以防止漏洞。数据集可通过此链接访问：https://。', 'title_zh': 'CASTLE：面向 CWE 检测的静态代码分析器和大语言模型基准数据集'}
{'arxiv_id': 'arXiv:2503.09382', 'title': 'Towards Next-Generation Recommender Systems: A Benchmark for Personalized Recommendation Assistant with LLMs', 'authors': 'Jiani Huang, Shijie Wang, Liang-bo Ning, Wenqi Fan, Shuaiqiang Wang, Dawei Yin, Qing Li', 'link': 'https://arxiv.org/abs/2503.09382', 'abstract': "Recommender systems (RecSys) are widely used across various modern digital platforms and have garnered significant attention. Traditional recommender systems usually focus only on fixed and simple recommendation scenarios, making it difficult to generalize to new and unseen recommendation tasks in an interactive paradigm. Recently, the advancement of large language models (LLMs) has revolutionized the foundational architecture of RecSys, driving their evolution into more intelligent and interactive personalized recommendation assistants. However, most existing studies rely on fixed task-specific prompt templates to generate recommendations and evaluate the performance of personalized assistants, which limits the comprehensive assessments of their capabilities. This is because commonly used datasets lack high-quality textual user queries that reflect real-world recommendation scenarios, making them unsuitable for evaluating LLM-based personalized recommendation assistants. To address this gap, we introduce RecBench+, a new dataset benchmark designed to access LLMs' ability to handle intricate user recommendation needs in the era of LLMs. RecBench+ encompasses a diverse set of queries that span both hard conditions and soft preferences, with varying difficulty levels. We evaluated commonly used LLMs on RecBench+ and uncovered below findings: 1) LLMs demonstrate preliminary abilities to act as recommendation assistants, 2) LLMs are better at handling queries with explicitly stated conditions, while facing challenges with queries that require reasoning or contain misleading information. Our dataset has been released at this https URL.", 'abstract_zh': '推荐系统（RecSys）在各种现代数字平台中广泛使用并引起了广泛关注。传统推荐系统通常仅专注于固定且简单的推荐场景，这使得它们难以在交互式范式中泛化到新的和未见过的推荐任务。近年来，大型语言模型（LLMs）的进步彻底改变了推荐系统的基本架构，推动其演变为更加智能和互动的个性化推荐助手。然而，现有的大多数研究依赖于固定的针对特定任务的提示模板来生成推荐并评估个性化助手的性能，这限制了对它们能力的全面评估。这是因为常用的数据集缺乏反映真实世界推荐场景的高质量文本用户查询，使得它们不适用于评估基于LLM的个性化推荐助手。为解决这一问题，我们引入了RecBench+，这是一个新的数据集基准，旨在评估LLMs在LLM时代处理复杂用户推荐需求的能力。RecBench+包含了涵盖硬条件和软偏好、不同难度级别的查询集。我们在RecBench+上评估了常用的大型语言模型，并发现了以下结论：1）大型语言模型初步展示了作为推荐助手的能力；2）大型语言模型在处理明确条件的查询方面表现更佳，但在需要推理或包含误导信息的查询方面面临挑战。我们的数据集已在以下链接发布：https://github.com/recbench/recbench-plus。', 'title_zh': '面向下一代推荐系统：基于大语言模型的个性化推荐助手基准'}
{'arxiv_id': 'arXiv:2503.09358', 'title': 'RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports', 'authors': 'Jiushen Cai, Weihang Zhang, Hanruo Liu, Ningli Wang, Huiqi Li', 'link': 'https://arxiv.org/abs/2503.09358', 'abstract': 'Standardization of clinical reports is crucial for improving the quality of healthcare and facilitating data integration. The lack of unified standards, including format, terminology, and style, is a great challenge in clinical fundus diagnostic reports, which increases the difficulty for large language models (LLMs) to understand the data. To address this, we construct a bilingual standard terminology, containing fundus clinical terms and commonly used descriptions in clinical diagnosis. Then, we establish two models, RetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented dataset simulating clinical scenarios, demonstrates powerful standardization behaviors. However, it encounters a challenge of limitation to cover a wider range of diseases. To further enhance standardization performance, we build RetSTA-7B, which integrates a substantial amount of standardized data generated by RetSTA-7B-Zero along with corresponding English data, covering diverse complex clinical scenarios and achieving report-level standardization for the first time. Experimental results demonstrate that RetSTA-7B outperforms other compared LLMs in bilingual standardization task, which validates its superior performance and generalizability. The checkpoints are available at this https URL.', 'abstract_zh': '临床报告的标准规范化对于提高医疗质量并促进数据整合至关重要。临床视网膜诊断报告缺乏统一的标准，包括格式、术语和风格，这是巨大的挑战，增加了大型语言模型理解数据的难度。为应对这一挑战，我们构建了一种双语标准术语，包含视网膜临床术语和临床诊断中常用的描述。然后，我们建立了两个模型：RetSTA-7B-Zero和RetSTA-7B。RetSTA-7B-Zero在模拟临床场景的扩充数据集上进行微调，展现了强大的标准化行为。然而，它在覆盖更广泛的疾病方面存在局限。为了进一步增强标准化性能，我们构建了RetSTA-7B，它整合了大量的由RetSTA-7B-Zero生成的标准化数据以及对应的英语文本，覆盖了多样复杂的临床场景，并首次实现了报告级别的标准化。实验结果表明，RetSTA-7B在双语标准化任务中优于其他对比的大型语言模型，验证了其优越的性能和泛化能力。模型检查点见此链接。', 'title_zh': 'RetSTA: 一种基于LLM的临床眼底图像报告标准化方法'}
{'arxiv_id': 'arXiv:2503.09348', 'title': 'MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding', 'authors': 'Zhoutong Ye, Mingze Sun, Huan-ang Gao, Chun Yu, Yuanchun Shi', 'link': 'https://arxiv.org/abs/2503.09348', 'abstract': "Large multimodal models (LMMs) have demonstrated significant potential as generalists in vision-language (VL) tasks. However, there remains a significant gap between state-of-the-art LMMs and human performance when it comes to complex tasks that require a combination of fundamental VL capabilities, as well as tasks involving the grounding of complex instructions. To thoroughly investigate the human-LMM gap and its underlying causes, we propose MOAT, a diverse benchmark with complex real-world VL tasks that are challenging for LMMs. Specifically, the tasks in MOAT require LMMs to engage in generalist problem solving by integrating fundamental VL capabilities such as reading text, counting, understanding spatial relations, grounding textual and visual instructions, etc. All these abilities fit into a taxonomy proposed by us that contains 10 fundamental VL capabilities, enabling MOAT to provide a fine-grained view of LMMs' strengths and weaknesses. Besides, MOAT is the first benchmark to explicitly evaluate LMMs' ability to ground complex text and visual instructions, which is essential to many real-world applications. We evaluate over 20 proprietary and open source LMMs, as well as humans, on MOAT, and found that humans achieved 82.7% accuracy while the best performing LMM (OpenAI o1) achieved only 38.8%. To guide future model development, we analyze common trends in our results and discuss the underlying causes of observed performance gaps between LMMs and humans, focusing on which VL capability forms the bottleneck in complex tasks, whether test time scaling improves performance on MOAT, and how tiling harms LMMs' capability to count. Code and data are available at this https URL.", 'abstract_zh': '大型多模态模型在视觉-语言任务中的通用潜力已得到显著展现，但当涉及需要结合基本视觉-语言能力的复杂任务，以及涉及复杂指令的 anchoring 任务时，与人类表现之间仍存在显著差距。为彻底探究人-多模态模型差距及其根本原因，我们提出 MOAT，一个包含具有挑战性的现实世界视觉-语言任务的综合性基准，旨在挑战多模态模型。具体而言，MOAT 中的任务要求模型结合阅读文本、计数、理解空间关系、锚定文本和视觉指令等基本视觉-语言能力进行通用问题解决。这些能力符合我们提出的包含 10 个基本视觉-语言能力的分类体系，使得 MOAT 能够提供对多模态模型优势和弱点的精细视角。此外，MOAT 是首个明确评估多模态模型锚定复杂文本和视觉指令能力的基准，这对许多实际应用至关重要。我们对 20 个自研和开源的多模态模型以及人类在 MOAT 上进行了评估，发现人类的准确率为 82.7%，而表现最佳的模型 (OpenAI o1) 的准确率为仅 38.8%。为了指导未来模型开发，我们分析了结果中的常见趋势，并讨论了多模态模型与人类表现差距背后的根本原因，重点关注哪些视觉-语言能力在复杂任务中成为瓶颈，测试时间的扩展是否在 MOAT 上提高性能，以及切片如何损害模型的计数能力。代码和数据可在以下链接获取。', 'title_zh': 'MOAT: 评估LMMs在能力集成和指令关联方面的表现'}
{'arxiv_id': 'arXiv:2503.09347', 'title': 'Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts', 'authors': 'Hongyu Chen, Seraphina Goldfarb-Tarrant', 'link': 'https://arxiv.org/abs/2503.09347', 'abstract': 'Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98\\%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments.', 'abstract_zh': '大型语言模型（LLMs）越来越多地被用作自动化评估器来评估生成内容的安全性，但其在这一角色中的可靠性仍有待确定。本研究评估了11种不同类型的LLM裁判模型在关键安全领域中的表现，考察了三个关键方面：重复评判任务中的自我一致性、与人类评判的一致性以及对输入艺术（如道歉或冗长措辞）的敏感性。研究发现，LLM裁判中存在偏向性，这可能导致对哪种内容来源更安全的最终判决产生重大扭曲，从而削弱了比较评估的有效性。值得注意的是，仅道歉语言艺术即可将评判者偏好偏斜高达98%。与预期相反，较大的模型并不总是更具有鲁棒性，而较小的模型有时对特定艺术更具抵抗力。为了减轻LLM评估者的鲁棒性问题，我们研究了陪审团式评估方法，即从多个模型的决策中进行聚合。尽管这种方法既提高了鲁棒性又增强了与人类判断的一致性，但即使在最佳陪审团配置下，对艺术的敏感性仍然存在。这些结果强调了需要多样化且对艺术具有抵抗力的方法的重要性，以确保可靠的安全性评估。', 'title_zh': '更安全还是更幸运？大规模语言模型作为安全性评估器并不 robust 至artifact'}
{'arxiv_id': 'arXiv:2503.09334', 'title': 'CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data', 'authors': 'Adel ElZemity, Budi Arief, Shujun Li', 'link': 'https://arxiv.org/abs/2503.09334', 'abstract': "The integration of large language models (LLMs) into cyber security applications presents significant opportunities, such as enhancing threat analysis and malware detection, but can also introduce critical risks and safety concerns, including personal data leakage and automated generation of new malware. To address these challenges, we developed CyberLLMInstruct, a dataset of 54,928 instruction-response pairs spanning cyber security tasks such as malware analysis, phishing simulations, and zero-day vulnerabilities. The dataset was constructed through a multi-stage process. This involved sourcing data from multiple resources, filtering and structuring it into instruction-response pairs, and aligning it with real-world scenarios to enhance its applicability. Seven open-source LLMs were chosen to test the usefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we rigorously assess the safety of fine-tuned models using the OWASP top 10 framework, finding that fine-tuning reduces safety resilience across all tested LLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B against prompt injection drops from 0.95 to 0.15). In our second example, we show that these same fine-tuned models can also achieve up to 92.50 percent accuracy on the CyberMetric benchmark. These findings highlight a trade-off between performance and safety, showing the importance of adversarial testing and further research into fine-tuning methodologies that can mitigate safety risks while still improving performance across diverse datasets and domains. All scripts required to reproduce the dataset, along with examples and relevant resources for replicating our results, will be made available upon the paper's acceptance.", 'abstract_zh': '大型语言模型（LLMs）在网络安全应用中的集成既带来了显著机会，也引入了关键风险和安全顾虑，包括个人数据泄露和新型恶意软件的自动化生成。为应对这些挑战，我们开发了CyberLLMInstruct数据集，包含54,928个指令-响应对，涵盖诸如恶意软件分析、鱼叉 phishing 模拟和零日漏洞等网络安全任务。该数据集通过多阶段过程构建，涉及从多个资源采集数据、筛选和结构化为指令-响应对，并与现实场景对齐以增强其实用性。我们选择了七个开源LLMs进行测试：Phi 3 Mini 3.8B、Mistral 7B、Qwen 2.5 7B、Llama 3 8B、Llama 3.1 8B、Gemma 2 9B 和 Llama 2 70B。在我们的主要示例中，我们使用OWASP TOP 10框架严格评估调优模型的安全性，发现调优降低了所有测试LLMs的安全韧性，并对每种对抗性攻击（例如，对Llama 3.1 8B针对提示注入的安全评分从0.95降至0.15）产生负面影响。在我们的第二个示例中，我们展示了这些相同的调优模型在CyberMetric基准测试中可达到高达92.50%的准确性。这些发现突显了性能与安全之间的权衡关系，强调了对抗性测试的重要性，并进一步研究可以减轻安全风险同时在多样数据集和领域中改进性能的调优方法的重要性。论文被接受后，将提供所有用于重现数据集的脚本，以及复制我们结果的示例和相关资源。', 'title_zh': 'CyberLLMInstruct: 一种使用网络安全数据分析细调LLM安全性的新数据集'}
{'arxiv_id': 'arXiv:2503.09326', 'title': 'A Survey on Enhancing Causal Reasoning Ability of Large Language Models', 'authors': 'Xin Li, Zhuo Cai, Shoujin Wang, Kun Yu, Fang Chen', 'link': 'https://arxiv.org/abs/2503.09326', 'abstract': "Large language models (LLMs) have recently shown remarkable performance in language tasks and beyond. However, due to their limited inherent causal reasoning ability, LLMs still face challenges in handling tasks that require robust causal reasoning ability, such as health-care and economic analysis. As a result, a growing body of research has focused on enhancing the causal reasoning ability of LLMs. Despite the booming research, there lacks a survey to well review the challenges, progress and future directions in this area. To bridge this significant gap, we systematically review literature on how to strengthen LLMs' causal reasoning ability in this paper. We start from the introduction of background and motivations of this topic, followed by the summarisation of key challenges in this area. Thereafter, we propose a novel taxonomy to systematically categorise existing methods, together with detailed comparisons within and between classes of methods. Furthermore, we summarise existing benchmarks and evaluation metrics for assessing LLMs' causal reasoning ability. Finally, we outline future research directions for this emerging field, offering insights and inspiration to researchers and practitioners in the area.", 'abstract_zh': '大型语言模型（LLMs）在语言任务和相关领域中最近展现了卓越的性能。然而，由于其有限的固有因果推理能力，LLMs在处理需要 robust 因果推理能力的任务（如医疗和经济分析）时仍面临挑战。因此，越来越多的研究聚焦于增强LLMs的因果推理能力。尽管该领域研究迅速发展，但缺乏对该领域挑战、进展和未来方向的综述。为填补这一重要空白，本文系统地回顾了如何增强LLMs的因果推理能力的研究文献。我们从背景和动机介绍开始，随后总结了该领域的关键挑战。之后，我们提出了一种新的分类法，系统地对现有方法进行分类，并进行了详细的类内和类间比较。此外，我们总结了现有的基准和评估指标，用于评估LLMs的因果推理能力。最后，我们展望了该新兴领域的未来研究方向，为该领域的研究人员和实践者提供了见解和灵感。', 'title_zh': '大型语言模型因果推理能力增强进展综述'}
{'arxiv_id': 'arXiv:2503.09311', 'title': 'Adaptive political surveys and GPT-4: Tackling the cold start problem with simulated user interactions', 'authors': 'Fynn Bachmann, Daan van der Weijden, Lucien Heitz, Cristina Sarasua, Abraham Bernstein', 'link': 'https://arxiv.org/abs/2503.09311', 'abstract': 'Adaptive questionnaires dynamically select the next question for a survey participant based on their previous answers. Due to digitalisation, they have become a viable alternative to traditional surveys in application areas such as political science. One limitation, however, is their dependency on data to train the model for question selection. Often, such training data (i.e., user interactions) are unavailable a priori. To address this problem, we (i) test whether Large Language Models (LLM) can accurately generate such interaction data and (ii) explore if these synthetic data can be used to pre-train the statistical model of an adaptive political survey. To evaluate this approach, we utilise existing data from the Swiss Voting Advice Application (VAA) Smartvote in two ways: First, we compare the distribution of LLM-generated synthetic data to the real distribution to assess its similarity. Second, we compare the performance of an adaptive questionnaire that is randomly initialised with one pre-trained on synthetic data to assess their suitability for training. We benchmark these results against an "oracle" questionnaire with perfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately generates answers to the Smartvote questionnaire from the perspective of different Swiss parties. Furthermore, we demonstrate that initialising the statistical model with synthetic data can (i) significantly reduce the error in predicting user responses and (ii) increase the candidate recommendation accuracy of the VAA. Our work emphasises the considerable potential of LLMs to create training data to improve the data collection process in adaptive questionnaires in LLM-affine areas such as political surveys.', 'abstract_zh': '自适应问卷能够根据受访者的先前答案动态选择下一个问题。由于数字化的发展，它们已成为政治科学等领域传统调查的可行替代方案。然而，它们的一个限制在于对训练问题选择模型所需的数据的依赖。通常，这类训练数据（即用户交互数据）在先验情况下不可用。为解决这一问题，我们（i）测试大型语言模型（LLM）是否能够准确生成此类交互数据，并（ii）探讨这些合成数据是否可以用于预先训练自适应政治调查的统计模型。为了评估这一方法，我们利用瑞士投票建议应用（VAA）Smartvote的现有数据，采用两种方式：首先，我们将LLM生成的合成数据分布与实际数据的分布进行比较以评估其相似度；其次，我们将一个随机初始化的自适应问卷与一个预先训练在合成数据上的自适应问卷进行比较，评估它们的训练适用性。我们将这些结果与一个具有先验完美知识的“顾问”问卷进行了对比。我们发现，现成的LLM（GPT-4）能够从瑞士不同政党的视角准确生成Smartvote问卷的答案。此外，我们展示了使用合成数据初始化统计模型可以（i）显著减少预测用户响应的误差，并且（ii）提高VAA的候选人推荐准确性。我们的工作强调了大型语言模型在自适应问卷中创建训练数据的巨大潜力，特别是在政治调查等LLM相关领域，以改善数据收集过程。', 'title_zh': '自适应政治调查与GPT-4：通过模拟用户交互解决冷启动问题'}
{'arxiv_id': 'arXiv:2503.09223', 'title': 'LREF: A Novel LLM-based Relevance Framework for E-commerce', 'authors': 'Tian Tang, Zhixing Tian, Zhenyu Zhu, Chenyang Wang, Haiqing Hu, Guoyu Tang, Lin Liu, Sulong Xu', 'link': 'https://arxiv.org/abs/2503.09223', 'abstract': 'Query and product relevance prediction is a critical component for ensuring a smooth user experience in e-commerce search. Traditional studies mainly focus on BERT-based models to assess the semantic relevance between queries and products. However, the discriminative paradigm and limited knowledge capacity of these approaches restrict their ability to comprehend the relevance between queries and products fully. With the rapid advancement of Large Language Models (LLMs), recent research has begun to explore their application to industrial search systems, as LLMs provide extensive world knowledge and flexible optimization for reasoning processes. Nonetheless, directly leveraging LLMs for relevance prediction tasks introduces new challenges, including a high demand for data quality, the necessity for meticulous optimization of reasoning processes, and an optimistic bias that can result in over-recall. To overcome the above problems, this paper proposes a novel framework called the LLM-based RElevance Framework (LREF) aimed at enhancing e-commerce search relevance. The framework comprises three main stages: supervised fine-tuning (SFT) with Data Selection, Multiple Chain of Thought (Multi-CoT) tuning, and Direct Preference Optimization (DPO) for de-biasing. We evaluate the performance of the framework through a series of offline experiments on large-scale real-world datasets, as well as online A/B testing. The results indicate significant improvements in both offline and online metrics. Ultimately, the model was deployed in a well-known e-commerce application, yielding substantial commercial benefits.', 'abstract_zh': '基于大规模语言模型的查询与产品相关性框架（LREF）研究', 'title_zh': 'LREF：一种基于大语言模型的相关性框架在电子商务中的应用'}
{'arxiv_id': 'arXiv:2503.09217', 'title': 'Evaluating the Generalizability of LLMs in Automated Program Repair', 'authors': 'Fengjie Li, Jiajun Jiang, Jiajun Sun, Hongyu Zhang', 'link': 'https://arxiv.org/abs/2503.09217', 'abstract': "LLM-based automated program repair methods have attracted significant attention for their state-of-the-art performance. However, they were primarily evaluated on a few well known datasets like Defects4J, raising questions about their effectiveness on new datasets. In this study, we evaluate 11 top-performing LLMs on DEFECTS4J-TRANS, a new dataset derived from transforming Defects4J while maintaining the original semantics. Results from experiments on both Defects4J and DEFECTS4J-TRANS show that all studied LLMs have limited generalizability in APR tasks, with the average number of correct and plausible patches decreasing by 49.48% and 42.90%, respectively, on DEFECTS4J-TRANS. Further investigation into incorporating additional repair-relevant information in repair prompts reveals that, although this information significantly enhances the LLMs' capabilities (increasing the number of correct and plausible patches by up to 136.67% and 121.82%, respectively), performance still falls short of their original results. This indicates that prompt engineering alone is insufficient to substantially enhance LLMs' repair capabilities. Based on our study, we also offer several recommendations for future research.", 'abstract_zh': '基于LLM的自动化程序修复方法在新数据集上的有效性研究', 'title_zh': '评估大规模语言模型在自动化程序修复中的泛化能力'}
{'arxiv_id': 'arXiv:2503.09153', 'title': 'Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News Detection', 'authors': 'Chaowei Zhang, Zongling Feng, Zewei Zhang, Jipeng Qiang, Guandong Xu, Yun Li', 'link': 'https://arxiv.org/abs/2503.09153', 'abstract': "The questionable responses caused by knowledge hallucination may lead to LLMs' unstable ability in decision-making. However, it has never been investigated whether the LLMs' hallucination is possibly usable to generate negative reasoning for facilitating the detection of fake news. This study proposes a novel supervised self-reinforced reasoning rectification approach - SR$^3$ that yields both common reasonable reasoning and wrong understandings (negative reasoning) for news via LLMs reflection for semantic consistency learning. Upon that, we construct a negative reasoning-based news learning model called - \\emph{NRFE}, which leverages positive or negative news-reasoning pairs for learning the semantic consistency between them. To avoid the impact of label-implicated reasoning, we deploy a student model - \\emph{NRFE-D} that only takes news content as input to inspect the performance of our method by distilling the knowledge from \\emph{NRFE}. The experimental results verified on three popular fake news datasets demonstrate the superiority of our method compared with three kinds of baselines including prompting on LLMs, fine-tuning on pre-trained SLMs, and other representative fake news detection methods.", 'abstract_zh': '知识幻觉引起的可疑响应可能导致LLMs在决策方面的能力不稳定。然而，尚未研究LLMs的幻觉是否可能用于生成否定推理，以促进fake news的检测。本研究提出了一种新颖的监督自我增强推理校正方法——SR$^3$，该方法通过LLMs的反思生成新闻的合理推理和错误理解（否定推理），以实现语义一致性学习。在此基础上，我们构建了一种基于否定推理的新闻学习模型——NRFE，该模型利用正向或负向新闻推理配对来学习它们之间的语义一致性。为了避免标签相关推理的影响，我们部署了一种学生模型——NRFE-D，该模型仅接受新闻内容作为输入，通过从NRFE中提取知识来检验我们方法的性能。实验结果在三个流行的fake news数据集上验证了我们的方法优于三种基线，包括对LLMs的提示、在预训练SLMs上微调以及其他代表性的fake news检测方法。', 'title_zh': '基于LLM的负向推理在假新闻检测中的实用性探究'}
{'arxiv_id': 'arXiv:2503.09114', 'title': 'Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of Language Model Inference at the Edge', 'authors': 'Maximilian Abstreiter, Sasu Tarkoma, Roberto Morabito', 'link': 'https://arxiv.org/abs/2503.09114', 'abstract': 'The rapid rise of Language Models (LMs) has expanded the capabilities of natural language processing, powering applications from text generation to complex decision-making. While state-of-the-art LMs often boast hundreds of billions of parameters and are primarily deployed in data centers, recent trends show a growing focus on compact models-typically under 10 billion parameters-enabled by techniques such as quantization and other model compression techniques. This shift paves the way for LMs on edge devices, offering potential benefits such as enhanced privacy, reduced latency, and improved data sovereignty. However, the inherent complexity of even these smaller models, combined with the limited computing resources of edge hardware, raises critical questions about the practical trade-offs in executing LM inference outside the cloud. To address these challenges, we present a comprehensive evaluation of generative LM inference on representative CPU-based and GPU-accelerated edge devices. Our study measures key performance indicators-including memory usage, inference speed, and energy consumption-across various device configurations. Additionally, we examine throughput-energy trade-offs, cost considerations, and usability, alongside an assessment of qualitative model performance. While quantization helps mitigate memory overhead, it does not fully eliminate resource bottlenecks, especially for larger models. Our findings quantify the memory and energy constraints that must be considered for practical real-world deployments, offering concrete insights into the trade-offs between model size, inference performance, and efficiency. The exploration of LMs at the edge is still in its early stages. We hope this study provides a foundation for future research, guiding the refinement of models, the enhancement of inference efficiency, and the advancement of edge-centric AI systems.', 'abstract_zh': '语言模型的快速崛起扩展了自然语言处理的能力，推动了从文本生成到复杂决策的应用。虽然最先进的语言模型通常拥有数十亿参数，并主要部署在数据中心，但最近的趋势显示，人们越来越注重紧凑型模型——通常参数量不到100亿，通过量化和其他模型压缩技术得以实现。这种转变为边缘设备上的语言模型铺平了道路，提供了增强隐私、减少延迟和提高数据主权的潜在益处。然而，即使这些较小模型的固有复杂性与边缘硬件的有限计算资源相结合，也引发了在云外执行语言模型推理时实际权衡的关键问题。为应对这些挑战，我们对代表性的基于CPU和GPU加速的边缘设备上生成型语言模型推理进行了全面评估。我们的研究测量了包括内存使用、推理速度和能耗在内的关键性能指标，并涵盖了各种设备配置。此外，我们还分析了吞吐量-能耗权衡、成本考虑、易用性以及模型性能的质量评估。虽然量化有助于缓解内存开销，但并不能完全消除资源瓶颈，尤其是对于较大模型而言。我们的研究量化了实际现场部署中必须考虑的内存和能源约束，提供了关于模型规模、推理性能和效率之间权衡的具体见解。边缘设备上的语言模型探索仍处于早期阶段。我们希望这项研究能为基础研究提供基础，指导模型的精炼、推理效率的提升以及边缘为中心的AI系统的发展。', 'title_zh': '有时痛苦但必定前景广阔：边缘端语言模型推理的可行性与权衡'}
{'arxiv_id': 'arXiv:2503.09089', 'title': 'LocAgent: Graph-Guided LLM Agents for Code Localization', 'authors': 'Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, Xingyao Wang', 'link': 'https://arxiv.org/abs/2503.09089', 'abstract': 'Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at this https URL.', 'abstract_zh': '代码定位——识别需要在代码库中进行更改的具体位置——是软件维护中一个基础而具有挑战性的任务。现有方法在识别相关代码段时难以高效地导航复杂代码库。挑战在于将自然语言问题描述与合适的代码元素连接起来，通常需要跨越层次结构和多个依赖关系进行推理。我们引入了LocAgent框架，通过图表示来解决代码定位问题。通过将代码库解析为有向异构图，LocAgent创建了一个轻量级的表示，能够捕捉代码结构（文件、类、函数）及其依赖关系（导入、调用、继承），从而使LLM代理能够通过强大的多跳推理有效地搜索和定位相关实体。在实际基准上的实验结果表明，我们的方法显著提高了代码定位的准确性。特别地，使用微调后的Qwen-2.5-Coder-Instruct-32B模型的方法在文件级别定位方面的准确率达到92.7%，并降低了成本（约86%），同时在多次尝试（Pass@10）下提高了GitHub问题解决成功率12%。我们的代码可在以下链接获取。', 'title_zh': 'LocAgent: 图引导的大语言模型码位定位代理'}
{'arxiv_id': 'arXiv:2503.09066', 'title': 'Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States', 'authors': 'Xin Wei Chia, Jonathan Pan', 'link': 'https://arxiv.org/abs/2503.09066', 'abstract': "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they remain vulnerable to adversarial manipulations such as jailbreaking via prompt injection attacks. These attacks bypass safety mechanisms to generate restricted or harmful content. In this study, we investigated the underlying latent subspaces of safe and jailbroken states by extracting hidden activations from a LLM. Inspired by attractor dynamics in neuroscience, we hypothesized that LLM activations settle into semi stable states that can be identified and perturbed to induce state transitions. Using dimensionality reduction techniques, we projected activations from safe and jailbroken responses to reveal latent subspaces in lower dimensional spaces. We then derived a perturbation vector that when applied to safe representations, shifted the model towards a jailbreak state. Our results demonstrate that this causal intervention results in statistically significant jailbreak responses in a subset of prompts. Next, we probed how these perturbations propagate through the model's layers, testing whether the induced state change remains localized or cascades throughout the network. Our findings indicate that targeted perturbations induced distinct shifts in activations and model responses. Our approach paves the way for potential proactive defenses, shifting from traditional guardrail based methods to preemptive, model agnostic techniques that neutralize adversarial states at the representation level.", 'abstract_zh': '大型语言模型在各种任务中表现出色，但仍然容易受到诸如提示注入攻击在内的恶意操控。这些攻击绕过了安全机制，生成受限或有害的内容。本研究通过从大型语言模型中提取隐藏激活，探究安全和被破解状态下的潜在隐空间。受神经科学中吸引子动力学的启发，我们假设大型语言模型的激活会稳定在半稳定的状态下，并可以通过干扰这些状态来诱导状态转换。利用降维技术，我们将安全和被破解响应的激活投影到低维空间，揭示潜在隐空间。随后，我们推导出一个扰动向量，当应用于安全表示时，可以将模型引导至破解状态。我们的结果表明，这种因果干预在一部分提示下产生了统计学上显著的破解响应。接着，我们研究了这些扰动如何通过模型的各层传播，检验诱导的状态变化是否局部化或在整个网络中传播。我们的研究发现，针对性的扰动引起了激活和模型响应的独特转变。我们的方法为潜在的主动防御铺平了道路，从传统的防护栏方法转向预先防范、模型无关的技术，以在表示级别中中和对抗状态。', 'title_zh': '探查大规模语言模型中的隐含子空间以提升AI安全：识别和操控对抗状态'}
{'arxiv_id': 'arXiv:2503.09035', 'title': 'ManeuverGPT Agentic Control for Safe Autonomous Stunt Maneuvers', 'authors': 'Shawn Azdam, Pranav Doma, Aliasghar Moj Arab', 'link': 'https://arxiv.org/abs/2503.09035', 'abstract': 'The next generation of active safety features in autonomous vehicles should be capable of safely executing evasive hazard-avoidance maneuvers akin to those performed by professional stunt drivers to achieve high-agility motion at the limits of vehicle handling. This paper presents a novel framework, ManeuverGPT, for generating and executing high-dynamic stunt maneuvers in autonomous vehicles using large language model (LLM)-based agents as controllers. We target aggressive maneuvers, such as J-turns, within the CARLA simulation environment and demonstrate an iterative, prompt-based approach to refine vehicle control parameters, starting tabula rasa without retraining model weights. We propose an agentic architecture comprised of three specialized agents (1) a Query Enricher Agent for contextualizing user commands, (2) a Driver Agent for generating maneuver parameters, and (3) a Parameter Validator Agent that enforces physics-based and safety constraints. Experimental results demonstrate successful J-turn execution across multiple vehicle models through textual prompts that adapt to differing vehicle dynamics. We evaluate performance via established success criteria and discuss limitations regarding numeric precision and scenario complexity. Our findings underscore the potential of LLM-driven control for flexible, high-dynamic maneuvers, while highlighting the importance of hybrid approaches that combine language-based reasoning with algorithmic validation.', 'abstract_zh': '下一代自主车辆中的主动安全功能应具备像专业特技驾驶者那样在车辆操控极限内执行规避危险的机动操作的能力。本文提出了一种新颖的框架，ManeuverGPT，该框架使用基于大规模语言模型（LLM）的代理作为控制器，以在自主车辆中生成和执行高强度特技机动。我们在CARLA仿真环境中针对激进机动（如J形转弯）进行目标定位，并展示了一种基于提示的迭代方法来细化车辆控制参数，从头开始无需重新训练模型权重。我们提出了一种由三个专门代理组成的机构型架构（1）查询增补代理，用于固化用户命令；（2）驾驶员代理，用于生成机动参数；（3）参数验证代理，用于施加基于物理和安全的约束。实验结果表明，通过适应不同车辆动态的文本提示，可以成功执行多种车型的J形转弯。我们通过既定的成功标准评估性能，并讨论了数字精度和场景复杂性方面的限制。我们的研究结果强调了LLM驱动控制在灵活执行高强度机动方面的潜力，同时指出了将基于语言的推理与算法验证相结合的混合方法的重要性。', 'title_zh': 'ManeuverGPT赋能的安全自主特技操纵'}
{'arxiv_id': 'arXiv:2503.09032', 'title': 'Teaching LLMs How to Learn with Contextual Fine-Tuning', 'authors': 'Younwoo Choi, Muhammad Adil Asif, Ziwen Han, John Willes, Rahul G. Krishnan', 'link': 'https://arxiv.org/abs/2503.09032', 'abstract': 'Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When human\'s learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, "can prompting help us teach LLMs how to learn". In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the model\'s interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains.', 'abstract_zh': '提示大型语言模型：通过提供预期模型操作的背景，在快速发展的领域中微调大型语言模型以提高其在新领域内的知识类型或泛化推理能力。探究提示在教学大型语言模型中的作用：一种新的指令微调变体——上下文微调，利用模拟人类学习和解决问题的认知策略的提示，指导训练过程，以改善模型对领域特定知识的解释和理解。我们实验证明，这种简单而有效的修改能显著提高大型语言模型在医疗和金融领域内快速适应新数据集的能力。', 'title_zh': '教LLM如何通过上下文微调进行学习'}
{'arxiv_id': 'arXiv:2503.09020', 'title': 'Enhancing High-Quality Code Generation in Large Language Models with Comparative Prefix-Tuning', 'authors': 'Yuan Jiang, Yujian Zhang, Liang Lu, Christoph Treude, Xiaohong Su, Shan Huang, Tiantian Wang', 'link': 'https://arxiv.org/abs/2503.09020', 'abstract': "Large Language Models (LLMs) have been widely adopted in commercial code completion engines, significantly enhancing coding efficiency and productivity. However, LLMs may generate code with quality issues that violate coding standards and best practices, such as poor code style and maintainability, even when the code is functionally correct. This necessitates additional effort from developers to improve the code, potentially negating the efficiency gains provided by LLMs. To address this problem, we propose a novel comparative prefix-tuning method for controllable high-quality code generation. Our method introduces a single, property-specific prefix that is prepended to the activations of the LLM, serving as a lightweight alternative to fine-tuning. Unlike existing methods that require training multiple prefixes, our approach trains only one prefix and leverages pairs of high-quality and low-quality code samples, introducing a sequence-level ranking loss to guide the model's training. This comparative approach enables the model to better understand the differences between high-quality and low-quality code, focusing on aspects that impact code quality. Additionally, we design a data construction pipeline to collect and annotate pairs of high-quality and low-quality code, facilitating effective training. Extensive experiments on the Code Llama 7B model demonstrate that our method improves code quality by over 100% in certain task categories, while maintaining functional correctness. We also conduct ablation studies and generalization experiments, confirming the effectiveness of our method's components and its strong generalization capability.", 'abstract_zh': '大规模语言模型（LLMs）已广泛应用于商业代码完成引擎中，显著提升了编码效率和生产力。然而，LLMs可能生成违反编码标准和最佳实践的代码，即使这些代码从功能上是正确的，也可能包含代码风格和可维护性不佳的问题。这需要开发人员付出额外的努力来改进代码，从而可能抵消LLMs带来的效率提升。为了解决这一问题，我们提出了一种新颖的可控高质量代码生成的比较前缀调优方法。该方法引入了一个特定属性的前缀，将其添加到LLM的激活之前，作为微调的轻量级替代方案。与现有需要训练多个前缀的方法不同，我们的方法仅训练一个前缀，并利用高质量和低质量代码样本的配对，引入序列级排序损失以指导模型的训练。这种比较方法使模型更好地理解高质量和低质量代码之间的差异，关注影响代码质量的因素。此外，我们设计了一个数据构建管道来收集和标注高质量和低质量代码配对，便于有效的训练。在Code Llama 7B模型上的广泛实验表明，我们的方法在某些任务类别中将代码质量提高了超过100%，同时保持功能正确性。我们还进行了消融研究和泛化实验，验证了我们方法各组成部分的有效性及其强大的泛化能力。', 'title_zh': '使用比较性前缀调优提升大型语言模型中的高质量代码生成'}
{'arxiv_id': 'arXiv:2503.09002', 'title': 'KNighter: Transforming Static Analysis with LLM-Synthesized Checkers', 'authors': 'Chenyuan Yang, Zijie Zhao, Zichen Xie, Haoyu Li, Lingming Zhang', 'link': 'https://arxiv.org/abs/2503.09002', 'abstract': 'Static analysis is a powerful technique for bug detection in critical systems like operating system kernels. However, designing and implementing static analyzers is challenging, time-consuming, and typically limited to predefined bug patterns. While large language models (LLMs) have shown promise for static analysis, directly applying them to scan large codebases remains impractical due to computational constraints and contextual limitations.\nWe present KNighter, the first approach that unlocks practical LLM-based static analysis by automatically synthesizing static analyzers from historical bug patterns. Rather than using LLMs to directly analyze massive codebases, our key insight is leveraging LLMs to generate specialized static analyzers guided by historical patch knowledge. KNighter implements this vision through a multi-stage synthesis pipeline that validates checker correctness against original patches and employs an automated refinement process to iteratively reduce false positives. Our evaluation on the Linux kernel demonstrates that KNighter generates high-precision checkers capable of detecting diverse bug patterns overlooked by existing human-written analyzers. To date, KNighter-synthesized checkers have discovered 70 new bugs/vulnerabilities in the Linux kernel, with 56 confirmed and 41 already fixed. 11 of these findings have been assigned CVE numbers. This work establishes an entirely new paradigm for scalable, reliable, and traceable LLM-based static analysis for real-world systems via checker synthesis.', 'abstract_zh': 'KNighter：通过历史漏洞模式自动生成的实用大语言模型驱动的静态分析方法', 'title_zh': 'KNighter: 通过LLM合成的检查器改造静态分析'}
{'arxiv_id': 'arXiv:2503.08990', 'title': 'JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing', 'authors': 'Vasudev Gohil', 'link': 'https://arxiv.org/abs/2503.08990', 'abstract': 'Large language models (LLMs) have shown great promise as language understanding and decision making tools, and they have permeated various aspects of our everyday life. However, their widespread availability also comes with novel risks, such as generating harmful, unethical, or offensive content, via an attack called jailbreaking. Despite extensive efforts from LLM developers to align LLMs using human feedback, they are still susceptible to jailbreak attacks. To tackle this issue, researchers often employ red-teaming to understand and investigate jailbreak prompts. However, existing red-teaming approaches lack effectiveness, scalability, or both. To address these issues, we propose JBFuzz, a novel effective, automated, and scalable red-teaming technique for jailbreaking LLMs.\nJBFuzz is inspired by the success of fuzzing for detecting bugs/vulnerabilities in software. We overcome three challenges related to effectiveness and scalability by devising novel seed prompts, a lightweight mutation engine, and a lightweight and accurate evaluator for guiding the fuzzer. Assimilating all three solutions results in a potent fuzzer that only requires black-box access to the target LLM. We perform extensive experimental evaluation of JBFuzz using nine popular and widely-used LLMs. We find that JBFuzz successfully jailbreaks all LLMs for various harmful/unethical questions, with an average attack success rate of 99%. We also find that JBFuzz is extremely efficient as it jailbreaks a given LLM for a given question in 60 seconds on average. Our work highlights the susceptibility of the state-of-the-art LLMs to jailbreak attacks even after safety alignment, and serves as a valuable red-teaming tool for LLM developers.', 'abstract_zh': '大型语言模型（LLMs）在语言理解和决策方面展现了巨大的潜力，并渗透到了我们日常生活中的多个方面。然而，它们的广泛应用也带来了新型风险，如通过名为“ jailbreaking ”的攻击生成有害、不道德或冒犯性的内容。尽管LLM开发者付出了大量努力通过人类反馈对模型进行对齐，它们仍然容易受到“ jailbreaking ”攻击。为解决这一问题，研究人员常常采用“红队”方法理解并调查“ jailbreaking ”提示。然而，现有的“红队”方法在有效性或可扩展性方面存在不足。为此，我们提出了一种名为JBFuzz的新颖有效、自动化和可扩展的“红队”技术，用于破解LLMs。', 'title_zh': 'JBFuzz：高效有效地破解LLMs的方法'}
{'arxiv_id': 'arXiv:2503.08919', 'title': 'Backtracking for Safety', 'authors': 'Bilgehan Sel, Dingcheng Li, Phillip Wallis, Vaishakh Keshava, Ming Jin, Siddhartha Reddy Jonnalagadda', 'link': 'https://arxiv.org/abs/2503.08919', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities across various tasks, but ensuring their safety and alignment with human values remains crucial. Current safety alignment methods, such as supervised fine-tuning and reinforcement learning-based approaches, can exhibit vulnerabilities to adversarial attacks and often result in shallow safety alignment, primarily focusing on preventing harmful content in the initial tokens of the generated output. While methods like resetting can help recover from unsafe generations by discarding previous tokens and restarting the generation process, they are not well-suited for addressing nuanced safety violations like toxicity that may arise within otherwise benign and lengthy generations. In this paper, we propose a novel backtracking method designed to address these limitations. Our method allows the model to revert to a safer generation state, not necessarily at the beginning, when safety violations occur during generation. This approach enables targeted correction of problematic segments without discarding the entire generated text, thereby preserving efficiency. We demonstrate that our method dramatically reduces toxicity appearing through the generation process with minimal impact to efficiency.', 'abstract_zh': '大型语言模型（LLMs）已在多种任务中展示了卓越的能力，但确保其安全并与其人类价值观保持一致仍至关重要。当前的安全对齐方法，如监督微调和基于强化学习的方法，可能会表现出对对抗攻击的脆弱性，并且往往导致浅层次的安全对齐，主要集中在防止生成输出初始词汇中的有害内容。尽管重置等方法可以帮助通过丢弃先前的词汇并重新开始生成过程来从不安全的生成中恢复，但它们并不适合解决可能在否则看似无害和冗长的生成过程中产生的细微安全违规，如毒性。在本文中，我们提出了一种新的回溯方法，旨在解决这些局限性。该方法在生成过程中发生安全违规时允许模型回退到一个更安全的生成状态，而不仅仅是初始状态。这种方法允许对有问题的段落进行有针对性的修正，而不必丢弃整个生成的文本，从而保持效率。我们证明，我们的方法能够显著减少生成过程中出现的毒性现象，且对效率的影响 minimal。', 'title_zh': '回溯以确保安全'}
{'arxiv_id': 'arXiv:2503.08908', 'title': 'Interpreting the Repeated Token Phenomenon in Large Language Models', 'authors': 'Itay Yona, Ilia Shumailov, Jamie Hayes, Federico Barbero, Yossi Gandelsman', 'link': 'https://arxiv.org/abs/2503.08908', 'abstract': "Large Language Models (LLMs), despite their impressive capabilities, often fail to accurately repeat a single word when prompted to, and instead output unrelated text. This unexplained failure mode represents a vulnerability, allowing even end-users to diverge models away from their intended behavior. We aim to explain the causes for this phenomenon and link it to the concept of ``attention sinks'', an emergent LLM behavior crucial for fluency, in which the initial token receives disproportionately high attention scores. Our investigation identifies the neural circuit responsible for attention sinks and shows how long repetitions disrupt this circuit. We extend this finding to other non-repeating sequences that exhibit similar circuit disruptions. To address this, we propose a targeted patch that effectively resolves the issue without negatively impacting the model's overall performance. This study provides a mechanistic explanation for an LLM vulnerability, demonstrating how interpretability can diagnose and address issues, and offering insights that pave the way for more secure and reliable models.", 'abstract_zh': '大型语言模型（LLMs）尽管具备 impressive 的能力，但在被提示重复单个单词时，往往无法准确重复，反而生成不相关的内容。这种无法解释的失效模式代表了一种漏洞，甚至允许最终用户使模型偏离预期行为。我们旨在解释这一现象的原因，并将其与“注意力陷阱”这一对流畅性至关重要的新兴 LLM 行为联系起来，在这种行为中，初始标记获得了不成比例高的注意力分数。我们的研究确定了负责注意力陷阱的神经电路，并展示了长时间重复如何破坏这一电路。我们将这一发现扩展到其他类似电路破坏的非重复序列。为了解决这一问题，我们提出了一种针对性的补丁，该补丁能够在不负面影响模型整体性能的前提下有效解决问题。本研究提供了对 LLM 漏洞的机制性解释，展示了可解释性如何诊断和解决这些问题，并提供了使模型更安全和可靠的见解。', 'title_zh': '大规模语言模型中重复 token 现象的解读'}
{'arxiv_id': 'arXiv:2503.08879', 'title': 'LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference', 'authors': 'Guangtao Wang, Shubhangi Upasani, Chen Wu, Darshan Gandhi, Jonathan Li, Changran Hu, Bo Li, Urmish Thakker', 'link': 'https://arxiv.org/abs/2503.08879', 'abstract': 'Efficient long-context inference is critical as large language models (LLMs) adopt context windows of ranging from 128K to 1M tokens. However, the growing key-value (KV) cache and the high computational complexity of attention create significant bottlenecks in memory usage and latency. In this paper, we find that attention in diverse long-context tasks exhibits sparsity, and LLMs implicitly "know" which tokens can be dropped or evicted at the head level after the pre-filling stage. Based on this insight, we propose Self-Attention Guided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for long-context inference. After prefilling, our method performs a one-time top-k selection at both the token and head levels to compress the KV cache, enabling efficient inference with the reduced cache. Evaluations on LongBench and three long-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct, and Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable to full attention while significantly improving efficiency. Specifically, SAGE-KV achieves 4x higher memory efficiency with improved accuracy over the static KV cache selection method StreamLLM, and 2x higher memory efficiency with better accuracy than the dynamic KV cache selection method Quest.', 'abstract_zh': '高效的长上下文推理对于大型语言模型（LLMs）至关重要，LLMs采用的上下文窗口范围从128K到1M个令牌。然而， growing key-value (KV) 缓存的增长和注意力机制的高计算复杂度在内存使用和延迟方面造成了显著瓶颈。在本文中，我们发现不同的长上下文任务中的注意力表现出稀疏性，并且LLMs在预填充阶段后在头部级别“知道”哪些令牌可以被丢弃或淘汰。基于这一洞察，我们提出了Self-Attention Guided Eviction (SAGE-KV)，一种简单的有效的方法，用于长上下文推理中的KV缓存淘汰。经过预填充后，我们的方法在令牌和头部级别进行一次性的top-k选择，以压缩KV缓存，从而使使用减少的缓存实现高效的推理。在LongBench以及三个长上下文LLM（Llama3.1-8B-Instruct-128k、Llama3-8B-Prolong-512k-Instruct和Qwen2.5-7B-Instruct-128k）上的评估显示，SAGE-KV在保持与全注意力相当的准确性的基础上，显著提高了效率。具体而言，与静态KV缓存选择方法StreamLLM相比，SAGE-KV在提高准确性的基础上实现了4倍的内存效率提升；与动态KV缓存选择方法Quest相比，SAGE-KV在更高的内存效率基础上实现了更好的准确率。', 'title_zh': 'LLMs了解该丢弃什么：自我注意力引导的KV缓存淘汰以实现高效长上下文推理'}
{'arxiv_id': 'arXiv:2503.08823', 'title': 'ResBench: Benchmarking LLM-Generated FPGA Designs with Resource Awareness', 'authors': 'Ce Guo, Tong Zhao', 'link': 'https://arxiv.org/abs/2503.08823', 'abstract': "Field-Programmable Gate Arrays (FPGAs) are widely used in modern hardware design, yet writing Hardware Description Language (HDL) code for FPGA implementation remains labor-intensive and complex. Large Language Models (LLMs) have emerged as a promising tool for automating HDL generation, but existing benchmarks for LLM HDL code generation primarily evaluate functional correctness while overlooking the critical aspect of hardware resource efficiency. Moreover, current benchmarks lack diversity, failing to capture the broad range of real-world FPGA applications. To address these gaps, we introduce ResBench, the first resource-oriented benchmark explicitly designed to differentiate between resource-optimized and inefficient LLM-generated HDL. ResBench consists of 56 problems across 12 categories, covering applications from finite state machines to financial computing. Our evaluation framework systematically integrates FPGA resource constraints, with a primary focus on Lookup Table (LUT) usage, enabling a realistic assessment of hardware efficiency. Experimental results reveal substantial differences in resource utilization across LLMs, demonstrating ResBench's effectiveness in distinguishing models based on their ability to generate resource-optimized FPGA designs.", 'abstract_zh': '基于资源的资源优化现场可编程门阵列硬件描述语言生成基准（ResBench）', 'title_zh': 'ResBench: 基于资源感知的LLM生成FPGA设计基准测试'}
{'arxiv_id': 'arXiv:2503.08815', 'title': 'Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations', 'authors': 'Danielle Villa, Maria Chang, Keerthiram Murugesan, Rosario Uceda-Sosa, Karthikeyan Natesan Ramamurthy', 'link': 'https://arxiv.org/abs/2503.08815', 'abstract': "Large Language Models (LLMs) are often asked to explain their outputs to enhance accuracy and transparency. However, evidence suggests that these explanations can misrepresent the models' true reasoning processes. One effective way to identify inaccuracies or omissions in these explanations is through consistency checking, which typically involves asking follow-up questions. This paper introduces, cross-examiner, a new method for generating follow-up questions based on a model's explanation of an initial question. Our method combines symbolic information extraction with language model-driven question generation, resulting in better follow-up questions than those produced by LLMs alone. Additionally, this approach is more flexible than other methods and can generate a wider variety of follow-up questions.", 'abstract_zh': '大规模语言模型（LLMs）往往被要求解释其输出以提高准确性和透明度。然而，证据表明这些解释可能会歪曲模型的真实推理过程。一种有效的方法是通过一致性检查来识别这些解释中的不准确或遗漏，这通常涉及提出后续问题。本文介绍了交叉检视者（Cross-examiner），一种基于模型对初始问题解释生成后续问题的新方法。该方法结合了符号信息提取与基于语言模型的问题生成，产生的后续问题比单独使用LLM生成的问题质量更高。此外，该方法比其他方法更具灵活性，能够生成更多样化的后续问题。', 'title_zh': 'Cross-Examiner: 评估大型语言模型生成的解释一致性'}
{'arxiv_id': 'arXiv:2503.08796', 'title': 'Robust Multi-Objective Controlled Decoding of Large Language Models', 'authors': 'Seongho Son, William Bankes, Sangwoong Yoon, Shyam Sundhar Ramesh, Xiaohang Tang, Ilija Bogunovic', 'link': 'https://arxiv.org/abs/2503.08796', 'abstract': 'Test-time alignment of Large Language Models (LLMs) to human preferences offers a flexible way to generate responses aligned to diverse objectives without extensive retraining of LLMs. Existing methods achieve alignment to multiple objectives simultaneously (e.g., instruction-following, helpfulness, conciseness) by optimizing their corresponding reward functions. However, they often rely on predefined weights or optimize for averages, sacrificing one objective for another and leading to unbalanced outcomes. To address this, we introduce Robust Multi-Objective Decoding (RMOD), a novel inference-time algorithm that optimizes for improving worst-case rewards. RMOD formalizes the robust decoding problem as a maximin two-player game between reward weights and the sampling policy, solving for the Nash equilibrium. We show that the game reduces to a convex optimization problem to find the worst-case weights, while the best response policy can be computed analytically. We also introduce a practical RMOD variant designed for efficient decoding with contemporary LLMs, incurring minimal computational overhead compared to non-robust Multi-Objective Decoding (MOD) methods. Our experimental results showcase the effectiveness of RMOD in generating responses equitably aligned with diverse objectives, outperforming baselines up to 20%.', 'abstract_zh': '大型语言模型（LLMs）在测试时与人类偏好的对齐为生成符合多样目标的响应提供了一种灵活的方法，而无需对LLMs进行大量重新训练。现有方法通过优化相应的奖励函数同时实现对多个目标的对齐（例如，指令遵循、有用性、简洁性）。然而，它们往往依赖于预定义的权重或优化平均值，导致在不同的目标之间牺牲某一个目标，从而产生不平衡的结果。为解决这一问题，我们引入了鲁棒多目标解码（RMOD），这是一种新型的推理时算法，旨在优化最坏情况奖励的改进。RMOD将鲁棒解码问题形式化为奖励权重和采样策略之间的最大化最小化二人博弈，并求解纳什均衡。我们证明该博弈可减少为一个凸优化问题以找出最坏情况的权重，而最优反应策略可以解析计算。我们还引入了一种实用的RMOD变体，该变体适用于当代LLMs的高效解码，并且相对于非鲁棒多目标解码（MOD）方法的计算开销最小。我们的实验证明了RMOD在生成公平地符合多样目标的响应方面的有效性，相对于基线方法可提高20%以上。', 'title_zh': '鲁棒多目标控制解码大型语言模型'}
{'arxiv_id': 'arXiv:2503.08750', 'title': 'Exposing Product Bias in LLM Investment Recommendation', 'authors': 'Yuhan Zhi, Xiaoyu Zhang, Longtian Wang, Shumin Jiang, Shiqing Ma, Xiaohong Guan, Chao Shen', 'link': 'https://arxiv.org/abs/2503.08750', 'abstract': "Large language models (LLMs), as a new generation of recommendation engines, possess powerful summarization and data analysis capabilities, surpassing traditional recommendation systems in both scope and performance. One promising application is investment recommendation. In this paper, we reveal a novel product bias in LLM investment recommendation, where LLMs exhibit systematic preferences for specific products. Such preferences can subtly influence user investment decisions, potentially leading to inflated valuations of products and financial bubbles, posing risks to both individual investors and market stability. To comprehensively study the product bias, we develop an automated pipeline to create a dataset of 567,000 samples across five asset classes (stocks, mutual funds, cryptocurrencies, savings, and portfolios). With this dataset, we present the bf first study on product bias in LLM investment recommendations. Our findings reveal that LLMs exhibit clear product preferences, such as certain stocks (e.g., `AAPL' from Apple and `MSFT' from Microsoft). Notably, this bias persists even after applying debiasing techniques. We urge AI researchers to take heed of the product bias in LLM investment recommendations and its implications, ensuring fairness and security in the digital space and market.", 'abstract_zh': "大型语言模型（LLMs）作为新一代推荐系统，具备强大的总结和数据分析能力，超越了传统推荐系统的范围和性能。在投资推荐领域的应用前景广阔。本文揭示了LLMs投资推荐中的一种新型产品偏差，即LLMs对特定产品表现出系统的偏好。这些偏好可能微妙地影响用户的投资决策，导致产品被过度估值，可能引发金融泡沫，对个人投资者和市场稳定构成风险。为全面研究产品偏差，我们开发了一种自动化管道，创建了一个包含567,000个样本的五类资产（股票、共同基金、加密货币、储蓄和投资组合）数据集。基于此数据集，我们进行了首个关于LLMs投资推荐中产品偏差的研究。研究发现，LLMs表现出明 显的产品偏好，例如某些股票（如来自苹果的`AAPL'和来自微软的`MSFT'）。值得注意的是，即使应用去偏技术，这种偏差依然存在。我们敦促AI研究人员关注LLMs投资推荐中的产品偏差及其影响，确保数字空间和市场的公平与安全。", 'title_zh': '暴露LLM在投资推荐中的产品偏见'}
{'arxiv_id': 'arXiv:2503.08727', 'title': 'Training Plug-n-Play Knowledge Modules with Deep Context Distillation', 'authors': 'Lucas Caccia, Alan Ansell, Edoardo Ponti, Ivan Vulić, Alessandro Sordoni', 'link': 'https://arxiv.org/abs/2503.08727', 'abstract': 'Dynamically integrating new or rapidly evolving information after (Large) Language Model pre-training remains challenging, particularly in low-data scenarios or when dealing with private and specialized documents. In-context learning and retrieval-augmented generation (RAG) face limitations, including their high inference costs and their inability to capture global document information. In this paper, we propose a way of modularizing knowledge by training document-level Knowledge Modules (KMs). KMs are lightweight components implemented as parameter-efficient LoRA modules, which are trained to store information about new documents and can be easily plugged into models on demand. We show that next-token prediction performs poorly as the training objective for KMs. We instead propose Deep Context Distillation: we learn KMs parameters such as to simulate hidden states and logits of a teacher that takes the document in context. Our method outperforms standard next-token prediction and pre-instruction training techniques, across two datasets. Finally, we highlight synergies between KMs and retrieval-augmented generation.', 'abstract_zh': '在大型语言模型预训练后动态集成新信息或快速演变的信息仍具挑战性，特别是在低数据场景或处理私有和专业化文档时。基于上下文的学习和检索增强生成（RAG）面临高推理成本和难以捕捉全局文档信息的局限。本文提出了一种通过训练文档级知识模块（KMs）对知识进行模块化的方法。KMs是轻量级的参数高效LoRA模块，用于存储新文档的信息，并可根据需要随时插入模型中。我们展示了下一标记预测不适合作为KMs的训练目标。相反，我们提出了深度上下文蒸馏：通过学习KMs参数，使其模拟教师模型在上下文中处理文档时的隐藏状态和logits。我们的方法在两个数据集上均优于标准的下一标记预测和预指令训练技术。最后，我们强调了KMs与检索增强生成之间的协同作用。', 'title_zh': '基于深度语境蒸馏的即插即用知识模块训练'}
{'arxiv_id': 'arXiv:2503.08716', 'title': 'AuthorMist: Evading AI Text Detectors with Reinforcement Learning', 'authors': 'Isaac David, Arthur Gervais', 'link': 'https://arxiv.org/abs/2503.08716', 'abstract': 'In the age of powerful AI-generated text, automatic detectors have emerged to identify machine-written content. This poses a threat to author privacy and freedom, as text authored with AI assistance may be unfairly flagged. We propose AuthorMist, a novel reinforcement learning-based system to transform AI-generated text into human-like writing. AuthorMist leverages a 3-billion-parameter language model as a backbone, fine-tuned with Group Relative Policy Optimization (GPRO) to paraphrase text in a way that evades AI detectors.\nOur framework establishes a generic approach where external detector APIs (GPTZero, WinstonAI, this http URL, etc.) serve as reward functions within the reinforcement learning loop, enabling the model to systematically learn outputs that these detectors are less likely to classify as AI-generated. This API-as-reward methodology can be applied broadly to optimize text against any detector with an accessible interface. Experiments on multiple datasets and detectors demonstrate that AuthorMist effectively reduces the detectability of AI-generated text while preserving the original meaning. Our evaluation shows attack success rates ranging from 78.6% to 96.2% against individual detectors, significantly outperforming baseline paraphrasing methods. AuthorMist maintains high semantic similarity (above 0.94) with the original text while successfully evading detection. These results highlight limitations in current AI text detection technologies and raise questions about the sustainability of the detection-evasion arms race.', 'abstract_zh': '在强大AI生成文本的时代，自动检测器 Emerged to Identify Machine-Generated Content 并对其进行识别，这给作者隐私和自由带来了威胁，因为接受AI辅助撰写的文本可能会被不公平地标记为机器生成。我们提出了AuthorMist，这是一种基于强化学习的新颖系统，旨在将AI生成的文本转换为人类风格的写作。AuthorMist利用了一个30亿参数的语言模型作为基础，并通过组相对策略优化（GPRO）进行了微调，以以避开AI检测器的方式来重述文本。我们的框架提供了一种通用的方法，其中外部检测器API（如GPTZero、WinstonAI、等）在强化学习循环中作为奖励函数使用，使模型能够系统地学习这些检测器不太可能将其分类为AI生成的输出。这种方法可以广泛应用于优化文本以对抗任何具有访问接口的检测器。在多个数据集和检测器上的实验表明，AuthorMist有效地降低了AI生成文本的可检测性，同时保留了原始意义。我们的评估显示，AuthorMist在针对个别检测器的攻击成功率范围为78.6%至96.2%，显著优于基线重述方法。AuthorMist在保留与原始文本高语义相似度（超过0.94）的同时成功躲避检测。这些结果突显了当前AI文本检测技术的局限性，并引发了关于检测-躲避军备竞赛可持续性的讨论。', 'title_zh': 'AuthorMist: 用强化学习规避AI文本检测器'}
{'arxiv_id': 'arXiv:2503.08709', 'title': 'Simulating Influence Dynamics with LLM Agents', 'authors': 'Mehwish Nasim, Syed Muslim Gilani, Amin Qasmi, Usman Naseem', 'link': 'https://arxiv.org/abs/2503.08709', 'abstract': 'This paper introduces a simulator designed for opinion dynamics researchers to model competing influences within social networks in the presence of LLM-based agents. By integrating established opinion dynamics principles with state-of-the-art LLMs, this tool enables the study of influence propagation and counter-misinformation strategies. The simulator is particularly valuable for researchers in social science, psychology, and operations research, allowing them to analyse societal phenomena without requiring extensive coding expertise. Additionally, the simulator will be openly available on GitHub, ensuring accessibility and adaptability for those who wish to extend its capabilities for their own research.', 'abstract_zh': '本文介绍了一种模拟器，该模拟器旨在帮助意见动力学研究人员在基于LLM的代理存在的情况下，建模社交网络中相互竞争的影响。通过将成熟的意见动力学原理与最先进的LLM相结合，该工具能够研究影响传播和反误信息策略。该模拟器特别适合社会科学研究、心理学和运筹学领域的研究人员，允许他们在无需大量编程专业知识的情况下分析社会现象。此外，该模拟器将在GitHub上开放获取，确保其对希望扩展其功能以满足自己研究需求的人们具有可访问性和适应性。', 'title_zh': '使用LLM代理模拟影响动力学'}
{'arxiv_id': 'arXiv:2503.08708', 'title': 'TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on Machine-Generated Text Detectors', 'authors': 'Jingyi Zheng, Junfeng Wang, Zhen Sun, Wenhan Dong, Yule Liu, Xinlei He', 'link': 'https://arxiv.org/abs/2503.08708', 'abstract': 'As Large Language Models (LLMs) advance, Machine-Generated Texts (MGTs) have become increasingly fluent, high-quality, and informative. Existing wide-range MGT detectors are designed to identify MGTs to prevent the spread of plagiarism and misinformation. However, adversaries attempt to humanize MGTs to evade detection (named evading attacks), which requires only minor modifications to bypass MGT detectors. Unfortunately, existing attacks generally lack a unified and comprehensive evaluation framework, as they are assessed using different experimental settings, model architectures, and datasets. To fill this gap, we introduce the Text-Humanization Benchmark (TH-Bench), the first comprehensive benchmark to evaluate evading attacks against MGT detectors. TH-Bench evaluates attacks across three key dimensions: evading effectiveness, text quality, and computational overhead. Our extensive experiments evaluate 6 state-of-the-art attacks against 13 MGT detectors across 6 datasets, spanning 19 domains and generated by 11 widely used LLMs. Our findings reveal that no single evading attack excels across all three dimensions. Through in-depth analysis, we highlight the strengths and limitations of different attacks. More importantly, we identify a trade-off among three dimensions and propose two optimization insights. Through preliminary experiments, we validate their correctness and effectiveness, offering potential directions for future research.', 'abstract_zh': '随着大型语言模型（LLMs）的发展，机器生成文本（MGTs）变得越来越流畅、高质量且信息丰富。现有的广泛范围的MGT检测器设计用于识别MGTs以防止剽窃和错误信息的传播。然而，攻击者试图通过对MGT进行人性化修改来规避检测（称为规避攻击），只需要进行少量修改即可绕过MGT检测器。不幸的是，现有的攻击缺乏一个统一且全面的评估框架，因为它们通常使用不同的实验设置、模型架构和数据集进行评估。为了填补这一空白，我们引入了文本人性化基准（TH-Bench），这是第一个用于评估MGT检测器规避攻击的全面基准。TH-Bench从三个关键维度评估攻击：规避效果、文本质量和计算开销。我们的广泛实验在六个数据集上评估了六种最先进的攻击，这些数据集涵盖了19个领域，由11个广泛使用的LLMs生成。我们的发现表明，没有单一的规避攻击在所有三个维度中表现优异。通过深入分析，我们强调了不同攻击的优点和局限。更重要的是，我们识别出了三个维度之间的权衡，并提出了两种优化见解。通过初步实验，我们验证了它们的正确性和有效性，为未来的相关研究提供了潜在方向。', 'title_zh': 'TH-Bench: 通过人性化AI文本评估机器生成文本检测器的规避攻击'}
{'arxiv_id': 'arXiv:2503.08704', 'title': 'Life-Cycle Routing Vulnerabilities of LLM Router', 'authors': 'Qiqi Lin, Xiaoyang Ji, Shengfang Zhai, Qingni Shen, Zhi Zhang, Yuejian Fang, Yansong Gao', 'link': 'https://arxiv.org/abs/2503.08704', 'abstract': 'Large language models (LLMs) have achieved remarkable success in natural language processing, yet their performance and computational costs vary significantly. LLM routers play a crucial role in dynamically balancing these trade-offs. While previous studies have primarily focused on routing efficiency, security vulnerabilities throughout the entire LLM router life cycle, from training to inference, remain largely unexplored. In this paper, we present a comprehensive investigation into the life-cycle routing vulnerabilities of LLM routers. We evaluate both white-box and black-box adversarial robustness, as well as backdoor robustness, across several representative routing models under extensive experimental settings. Our experiments uncover several key findings: 1) Mainstream DNN-based routers tend to exhibit the weakest adversarial and backdoor robustness, largely due to their strong feature extraction capabilities that amplify vulnerabilities during both training and inference; 2) Training-free routers demonstrate the strongest robustness across different attack types, benefiting from the absence of learnable parameters that can be manipulated. These findings highlight critical security risks spanning the entire life cycle of LLM routers and provide insights for developing more robust models.', 'abstract_zh': '大型语言模型（LLMs）在自然语言处理领域取得了显著成功，但其性能和计算成本差异明显。LLM路由器在动态平衡这些权衡方面发挥着关键作用。尽管以往研究主要关注路由效率，但整个LLM路由器生命周期（从训练到推断）中的安全漏洞依然鲜有探索。本文对LLM路由器的生命周期路由漏洞进行了全面调查，评估了多种代表性路由模型在广泛实验条件下的白盒和黑盒对抗鲁棒性以及后门鲁棒性。实验揭示了几项关键发现：1）基于主流深度神经网络（DNN）的路由器展现出最弱的对抗和后门鲁棒性，主要是由于其强大的特征提取能力在训练和推断过程中放大了漏洞；2）无需训练的路由器在不同攻击类型下表现出最强的鲁棒性，得益于没有可被操纵的学习参数。这些发现突显了整个LLM路由器生命周期中的关键安全风险，并为开发更鲁棒的模型提供了见解。', 'title_zh': 'LLM Router 生命周期路由漏洞'}
