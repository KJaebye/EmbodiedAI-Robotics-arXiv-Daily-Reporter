{'arxiv_id': 'arXiv:2503.07323', 'title': 'Dynamic Path Navigation for Motion Agents with LLM Reasoning', 'authors': 'Yubo Zhao, Qi Wu, Yifan Wang, Yu-Wing Tai, Chi-Keung Tang', 'link': 'https://arxiv.org/abs/2503.07323', 'abstract': "Large Language Models (LLMs) have demonstrated strong generalizable reasoning and planning capabilities. However, their efficacies in spatial path planning and obstacle-free trajectory generation remain underexplored. Leveraging LLMs for navigation holds significant potential, given LLMs' ability to handle unseen scenarios, support user-agent interactions, and provide global control across complex systems, making them well-suited for agentic planning and humanoid motion generation. As one of the first studies in this domain, we explore the zero-shot navigation and path generation capabilities of LLMs by constructing a dataset and proposing an evaluation protocol. Specifically, we represent paths using anchor points connected by straight lines, enabling movement in various directions. This approach offers greater flexibility and practicality compared to previous methods while remaining simple and intuitive for LLMs. We demonstrate that, when tasks are well-structured in this manner, modern LLMs exhibit substantial planning proficiency in avoiding obstacles while autonomously refining navigation with the generated motion to reach the target. Further, this spatial reasoning ability of a single LLM motion agent interacting in a static environment can be seamlessly generalized in multi-motion agents coordination in dynamic environments. Unlike traditional approaches that rely on single-step planning or local policies, our training-free LLM-based method enables global, dynamic, closed-loop planning, and autonomously resolving collision issues.", 'abstract_zh': '大规模语言模型在空间路径规划和无碰撞轨迹生成中的零-shot导航能力探索', 'title_zh': '基于LLM推理的动态路径导航方法'}
{'arxiv_id': 'arXiv:2503.07317', 'title': 'Self-Corrective Task Planning by Inverse Prompting with Large Language Models', 'authors': 'Jiho Lee, Hayun Lee, Jonghyeon Kim, Kyungjae Lee, Eunwoo Kim', 'link': 'https://arxiv.org/abs/2503.07317', 'abstract': 'In robot task planning, large language models (LLMs) have shown significant promise in generating complex and long-horizon action sequences. However, it is observed that LLMs often produce responses that sound plausible but are not accurate. To address these problems, existing methods typically employ predefined error sets or external knowledge sources, requiring human efforts and computation resources. Recently, self-correction approaches have emerged, where LLM generates and refines plans, identifying errors by itself. Despite their effectiveness, they are more prone to failures in correction due to insufficient reasoning. In this paper, we introduce InversePrompt, a novel self-corrective task planning approach that leverages inverse prompting to enhance interpretability. Our method incorporates reasoning steps to provide clear, interpretable feedback. It generates inverse actions corresponding to the initially generated actions and verifies whether these inverse actions can restore the system to its original state, explicitly validating the logical coherence of the generated this http URL results on benchmark datasets show an average 16.3% higher success rate over existing LLM-based task planning methods. Our approach offers clearer justifications for feedback in real-world environments, resulting in more successful task completion than existing self-correction approaches across various scenarios.', 'abstract_zh': '基于逆提示词的自动矫正任务规划方法', 'title_zh': '使用大型语言模型的逆向提示自纠正任务规划'}
{'arxiv_id': 'arXiv:2503.07020', 'title': 'Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense', 'authors': 'Yuting Hu, Chenhui Xu, Ruiyang Qin, Dancheng Liu, Amir Nassereldine, Yiyu Shi, Jinjun Xiong', 'link': 'https://arxiv.org/abs/2503.07020', 'abstract': 'Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Current protocols typically respond with immediate stops or minimal-risk maneuvers, worsening traffic flow and lacking flexibility for rare driving scenarios. In this paper, we propose LLM-RCO, a framework leveraging large language models to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator. These modules interact with the dynamic driving environment, enabling proactive and context-aware control actions to override the original control policy of autonomous agents. To improve safety in such challenging conditions, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, complete with annotations for LLM-based hazard inference and motion planning fine-tuning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that systems equipped with LLM-RCO significantly improve driving performance, highlighting its potential for enhancing autonomous driving resilience against adverse perception deficits. Our results also show that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements instead of conservative stops in the context of perception deficits.', 'abstract_zh': '部分感知缺陷会导致自动驾驶汽车安全受损，因为它会破坏对环境的理解。当前的方案通常会引发立即停车或最低风险的操作，这会恶化交通流量且缺乏针对罕见驾驶场景的灵活性。本文提出了一种名为LLM-RCO的框架，该框架利用大型语言模型将类似人类驾驶的常识性知识整合到面对感知缺陷的自主系统中。LLM-RCO包含四个关键模块：风险推理、短期运动规划器、动作条件验证器和安全约束生成器。这些模块与动态驾驶环境互动，能够主动且情境感知地控制操作以覆盖原始的自主代理控制策略。为了在这些具有挑战性的条件下改善安全性，我们构建了DriveLM-Deficit数据集，包含53,895段视频片段，展示了关键安全对象的缺陷，并附有人类语言模型（LLM）风险推理和运动规划微调的注释。使用CARLA模拟器在不利驾驶条件下的广泛实验表明，配备LLM-RCO的系统能够显著提高驾驶性能，突显了其在抵御不良感知缺陷方面增强自动驾驶鲁棒性的潜力。我们的研究表明，使用DriveLM-Deficit微调的LLM能够在感知缺陷的情境下使汽车采取更为积极的行动，而非保守的停止。', 'title_zh': '基于多模态大语言模型常识对抗自动驾驶中的部分感知缺陷'}
{'arxiv_id': 'arXiv:2503.07006', 'title': 'HELM: Human-Preferred Exploration with Language Models', 'authors': 'Shuhao Liao, Xuxin Lv, Yuhong Cao, Jeric Lew, Wenjun Wu, Guillaume Sartoretti', 'link': 'https://arxiv.org/abs/2503.07006', 'abstract': 'In autonomous exploration tasks, robots are required to explore and map unknown environments while efficiently planning in dynamic and uncertain conditions. Given the significant variability of environments, human operators often have specific preference requirements for exploration, such as prioritizing certain areas or optimizing for different aspects of efficiency. However, existing methods struggle to accommodate these human preferences adaptively, often requiring extensive parameter tuning or network retraining. With the recent advancements in Large Language Models (LLMs), which have been widely applied to text-based planning and complex reasoning, their potential for enhancing autonomous exploration is becoming increasingly promising. Motivated by this, we propose an LLM-based human-preferred exploration framework that seamlessly integrates a mobile robot system with LLMs. By leveraging the reasoning and adaptability of LLMs, our approach enables intuitive and flexible preference control through natural language while maintaining a task success rate comparable to state-of-the-art traditional methods. Experimental results demonstrate that our framework effectively bridges the gap between human intent and policy preference in autonomous exploration, offering a more user-friendly and adaptable solution for real-world robotic applications.', 'abstract_zh': '基于大语言模型的人类偏好自主探索框架', 'title_zh': 'HELM: 人类喜好的探索与语言模型'}
{'arxiv_id': 'arXiv:2503.06892', 'title': 'SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for Enhanced Safety in LLM-based Robotic Task Planning', 'authors': 'Ike Obi, Vishnunandan L.N. Venkatesh, Weizheng Wang, Ruiqi Wang, Dayoon Suh, Temitope I. Amosa, Wonse Jo, Byung-Cheol Min', 'link': 'https://arxiv.org/abs/2503.06892', 'abstract': 'Robotics researchers increasingly leverage large language models (LLM) in robotics systems, using them as interfaces to receive task commands, generate task plans, form team coalitions, and allocate tasks among multi-robot and human agents. However, despite their benefits, the growing adoption of LLM in robotics has raised several safety concerns, particularly regarding executing malicious or unsafe natural language prompts. In addition, ensuring that task plans, team formation, and task allocation outputs from LLMs are adequately examined, refined, or rejected is crucial for maintaining system integrity. In this paper, we introduce SafePlan, a multi-component framework that combines formal logic and chain-of-thought reasoners for enhancing the safety of LLM-based robotics systems. Using the components of SafePlan, including Prompt Sanity COT Reasoner and Invariant, Precondition, and Postcondition COT reasoners, we examined the safety of natural language task prompts, task plans, and task allocation outputs generated by LLM-based robotic systems as means of investigating and enhancing system safety profile. Our results show that SafePlan outperforms baseline models by leading to 90.5% reduction in harmful task prompt acceptance while still maintaining reasonable acceptance of safe tasks.', 'abstract_zh': '基于大型语言模型的机器人系统安全性增强框架SafePlan', 'title_zh': 'SafePlan: 利用形式逻辑和链式推理提高基于LLM的机器人任务规划安全性'}
{'arxiv_id': 'arXiv:2503.07234', 'title': 'CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs and Chain-of-Thought Prompting', 'authors': 'Haicheng Liao, Hanlin Kong, Bonan Wang, Chengyue Wang, Wang Ye, Zhengbing He, Chengzhong Xu, Zhenning Li', 'link': 'https://arxiv.org/abs/2503.07234', 'abstract': "Accurate motion forecasting is crucial for safe autonomous driving (AD). This study proposes CoT-Drive, a novel approach that enhances motion forecasting by leveraging large language models (LLMs) and a chain-of-thought (CoT) prompting method. We introduce a teacher-student knowledge distillation strategy to effectively transfer LLMs' advanced scene understanding capabilities to lightweight language models (LMs), ensuring that CoT-Drive operates in real-time on edge devices while maintaining comprehensive scene understanding and generalization capabilities. By leveraging CoT prompting techniques for LLMs without additional training, CoT-Drive generates semantic annotations that significantly improve the understanding of complex traffic environments, thereby boosting the accuracy and robustness of predictions. Additionally, we present two new scene description datasets, Highway-Text and Urban-Text, designed for fine-tuning lightweight LMs to generate context-specific semantic annotations. Comprehensive evaluations of five real-world datasets demonstrate that CoT-Drive outperforms existing models, highlighting its effectiveness and efficiency in handling complex traffic scenarios. Overall, this study is the first to consider the practical application of LLMs in this field. It pioneers the training and use of a lightweight LLM surrogate for motion forecasting, setting a new benchmark and showcasing the potential of integrating LLMs into AD systems.", 'abstract_zh': '准确的运动预测对于安全的自动驾驶（AD）至关重要。本文提出CoT-Drive，这是一种通过利用大规模语言模型（LLMs）和链式思考（CoT）提示方法来增强运动预测的新方法。我们引入了一种教师-学生知识蒸馏策略，有效地将LLMs的高级场景理解能力转移给轻量级语言模型（LMs），确保CoT-Drive可以在边缘设备上实现实时运行，同时保持全面的场景理解和泛化能力。通过利用CoT提示技术为LLMs生成语义注释，CoT-Drive可以在无需额外训练的情况下显著提高对复杂交通环境的理解，从而提升预测的准确性和鲁棒性。此外，我们还提出了两个新的场景描述数据集，Highway-Text和Urban-Text，用于对轻量级LMs进行微调以生成特定于上下文的语义注释。对五个真实世界数据集的全面评估表明，CoT-Drive优于现有模型，突显了其在处理复杂交通场景方面的有效性和效率。总体而言，本研究是首次将LLMs的实际应用纳入该领域的研究。它开创了轻量级LLM代理用于运动预测的训练和使用，设立了新的基准，并展示了将LLMs整合到AD系统中的潜力。', 'title_zh': 'CoT-Drive: 通过LLMs和链式思考提示实现高效自主驾驶运动预测'}
{'arxiv_id': 'arXiv:2503.07545', 'title': 'Queueing, Predictions, and LLMs: Challenges and Open Problems', 'authors': 'Michael Mitzenmacher, Rana Shahout', 'link': 'https://arxiv.org/abs/2503.07545', 'abstract': 'Queueing systems present many opportunities for applying machine-learning predictions, such as estimated service times, to improve system performance. This integration raises numerous open questions about how predictions can be effectively leveraged to improve scheduling decisions. Recent studies explore queues with predicted service times, typically aiming to minimize job time in the system. We review these works, highlight the effectiveness of predictions, and present open questions on queue performance. We then move to consider an important practical example of using predictions in scheduling, namely Large Language Model (LLM) systems, which presents novel scheduling challenges and highlights the potential for predictions to improve performance. In particular, we consider LLMs performing inference. Inference requests (jobs) in LLM systems are inherently complex; they have variable inference times, dynamic memory footprints that are constrained by key-value (KV) store memory limitations, and multiple possible preemption approaches that affect performance differently. We provide background on the important aspects of scheduling in LLM systems, and introduce new models and open problems that arise from them. We argue that there are significant opportunities for applying insights and analysis from queueing theory to scheduling in LLM systems.', 'abstract_zh': '机器学习预测在排队系统中的应用及其对调度决策的影响：以大型语言模型系统为例', 'title_zh': '队列理论、预测与大语言模型：挑战与开放式问题'}
{'arxiv_id': 'arXiv:2503.07450', 'title': 'From Idea to Implementation: Evaluating the Influence of Large Language Models in Software Development -- An Opinion Paper', 'authors': "Sargam Yadav, Asifa Mehmood Qureshi, Abhishek Kaushik, Shubham Sharma, Roisin Loughran, Subramaniam Kazhuparambil, Andrew Shaw, Mohammed Sabry, Niamh St John Lynch, . Nikhil Singh, Padraic O'Hara, Pranay Jaiswal, Roshan Chandru, David Lillis", 'link': 'https://arxiv.org/abs/2503.07450', 'abstract': 'The introduction of transformer architecture was a turning point in Natural Language Processing (NLP). Models based on the transformer architecture such as Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-Trained Transformer (GPT) have gained widespread popularity in various applications such as software development and education. The availability of Large Language Models (LLMs) such as ChatGPT and Bard to the general public has showcased the tremendous potential of these models and encouraged their integration into various domains such as software development for tasks such as code generation, debugging, and documentation generation. In this study, opinions from 11 experts regarding their experience with LLMs for software development have been gathered and analysed to draw insights that can guide successful and responsible integration. The overall opinion of the experts is positive, with the experts identifying advantages such as increase in productivity and reduced coding time. Potential concerns and challenges such as risk of over-dependence and ethical considerations have also been highlighted.', 'abstract_zh': '变压器架构的引入是自然语言处理(NLP)领域的转折点。基于变压器架构的模型如双向编码器表示（BERT）和生成预训练变换器（GPT）在软件开发和教育等各类应用中获得了广泛 popularity。大型语言模型（LLMs）如ChatGPT和Bard的普及展示了这些模型的巨大潜力，并促进了它们在软件开发等领域的集成，用于代码生成、调试和文档生成等任务。本研究收集并分析了11位专家关于LLMs在软件开发中应用的经验意见，以引导其成功的负责任的集成。专家们总体态度积极，认为LLMs能提高生产率并减少编码时间，同时也指出了潜在的风险和挑战，如过度依赖和伦理考虑。', 'title_zh': '从概念到实施：大型语言模型在软件开发中影响评价——一篇观点论文'}
{'arxiv_id': 'arXiv:2503.07429', 'title': 'From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector Graphics', 'authors': 'Jaewook Lee, Jeongah Lee, Wanyong Feng, Andrew Lan', 'link': 'https://arxiv.org/abs/2503.07429', 'abstract': 'Advances in large language models (LLMs) offer new possibilities for enhancing math education by automating support for both teachers and students. While prior work has focused on generating math problems and high-quality distractors, the role of visualization in math learning remains under-explored. Diagrams are essential for mathematical thinking and problem-solving, yet manually creating them is time-consuming and requires domain-specific expertise, limiting scalability. Recent research on using LLMs to generate Scalable Vector Graphics (SVG) presents a promising approach to automating diagram creation. Unlike pixel-based images, SVGs represent geometric figures using XML, allowing seamless scaling and adaptability. Educational platforms such as Khan Academy and IXL already use SVGs to display math problems and hints. In this paper, we explore the use of LLMs to generate math-related diagrams that accompany textual hints via intermediate SVG representations. We address three research questions: (1) how to automatically generate math diagrams in problem-solving hints and evaluate their quality, (2) whether SVG is an effective intermediate representation for math diagrams, and (3) what prompting strategies and formats are required for LLMs to generate accurate SVG-based diagrams. Our contributions include defining the task of automatically generating SVG-based diagrams for math hints, developing an LLM prompting-based pipeline, and identifying key strategies for improving diagram generation. Additionally, we introduce a Visual Question Answering-based evaluation setup and conduct ablation studies to assess different pipeline variations. By automating the math diagram creation, we aim to provide students and teachers with accurate, conceptually relevant visual aids that enhance problem-solving and learning experiences.', 'abstract_zh': '大型语言模型的进步为增强数学教育提供了新机遇，通过自动化支持教师和学生。虽然先前的工作集中在生成数学问题和高质量的诱饵上，但数学学习中的可视化作用仍然被研究不足。图表对于数学思考和问题解决至关重要，但手动创建它们耗时且需要特定领域的专业知识，限制了其可扩展性。最近在使用大型语言模型生成可缩放矢量图形（SVG）的研究表明了一种自动创建图表的有希望的方法。与基于像素的图像不同，SVG 使用 XML 表示几何图形，允许无缝缩放和适应性。诸如 Kahn Academy 和 IXL 等教育平台已经使用 SVG 来显示数学问题和提示。在本文中，我们探索使用大型语言模型通过中间 SVG 表示生成与文本提示相关联的数学图表。我们解决三个研究问题：（1）如何自动生成数学提示中的图表并评估其质量，（2）SVG 是否是数学图表的有效中间表示，以及（3）需要哪种提示策略和格式使大型语言模型能够生成准确的基于 SVG 的图表。我们的贡献包括定义自动生成数学提示中基于 SVG 的图表的任务，开发基于大型语言模型提示的流水线，并确定改进图表生成的关键策略。此外，我们介绍了基于视觉问答的评估设置，并进行了消融研究以评估不同流水线变体。通过自动化数学图表的创建，我们旨在为学生和教师提供准确且具有概念相关性的视觉辅助，以增强解决和学习体验。', 'title_zh': '从文本到视觉：使用LLMs生成矢量图形数学图表'}
{'arxiv_id': 'arXiv:2503.07202', 'title': 'A Zero-shot Learning Method Based on Large Language Models for Multi-modal Knowledge Graph Embedding', 'authors': 'Bingchen Liu, Jingchen Li, Naixing Xu, Xin Li', 'link': 'https://arxiv.org/abs/2503.07202', 'abstract': 'Zero-shot learning (ZL) is crucial for tasks involving unseen categories, such as natural language processing, image classification, and cross-lingual transfer. Current applications often fail to accurately infer and handle new relations or entities involving unseen categories, severely limiting their scalability and practicality in open-domain scenarios. ZL learning faces the challenge of effectively transferring semantic information of unseen categories in multi-modal knowledge graph (MMKG) embedding representation learning. In this paper, we propose ZSLLM, a framework for zero-shot embedding learning of MMKGs using large language models (LLMs). We leverage textual modality information of unseen categories as prompts to fully utilize the reasoning capabilities of LLMs, enabling semantic information transfer across different modalities for unseen categories. Through model-based learning, the embedding representation of unseen categories in MMKG is enhanced. Extensive experiments conducted on multiple real-world datasets demonstrate the superiority of our approach compared to state-of-the-art methods.', 'abstract_zh': '零-shot 学习 (ZL) 对于涉及未见类别的任务至关重要，如自然语言处理、图像分类和跨语言迁移。当前的应用往往难以准确推断和处理涉及未见类别的新关系或实体，严重限制了它们在开放域场景中的可扩展性和实用性。零-shot 学习面临在多模态知识图嵌入表示学习中有效转移未见类别语义信息的挑战。在本文中，我们提出 ZSLLM，一种利用大型语言模型 (LLM) 进行多模态知识图 (MMKG) 零-shot 嵌入学习的框架。我们利用未见类别的文本模态信息作为提示，充分利用 LLM 的推理能力，实现不同模态之间未见类别的语义信息转移。通过基于模型的学习，增强了 MMKG 中未见类别的嵌入表示。在多个真实世界数据集上的广泛实验表明，我们的方法在性能上优于现有方法。', 'title_zh': '基于大型语言模型的零样本学习多模态知识图嵌入方法'}
{'arxiv_id': 'arXiv:2503.06951', 'title': 'ReAgent: Reversible Multi-Agent Reasoning for Knowledge-Enhanced Multi-Hop QA', 'authors': 'Zhao Xinjie, Fan Gao, Rui Yang, Yingjian Chen, Yuyang Wang, Ying Zhu, Jiacheng Tang, Irene Li', 'link': 'https://arxiv.org/abs/2503.06951', 'abstract': "Recent advances in large language models (LLMs) have significantly improved multi-hop question answering (QA) through direct Chain-of-Thought (CoT) reasoning. However, the irreversible nature of CoT leads to error accumulation, making it challenging to correct mistakes in multi-hop reasoning. This paper introduces ReAgent: a Reversible multi-Agent collaborative framework augmented with explicit backtracking mechanisms, enabling reversible multi-hop reasoning. By incorporating text-based retrieval, information aggregation and validation, our system can detect and correct errors mid-reasoning, leading to more robust and interpretable QA outcomes. The framework and experiments serve as a foundation for future work on error-tolerant QA systems. Empirical evaluations across three benchmarks indicate ReAgent's efficacy, yielding average about 6\\% improvements against baseline models.", 'abstract_zh': 'Recent advances in大规模语言模型（LLMs）通过直接链式思考（CoT）推理显著提高了多跳问答（QA）的能力。然而，CoT的不可逆性导致了错误累积，使得在多跳推理中纠正错误变得具有挑战性。本文介绍了一种名为ReAgent的可逆多智能体协作框架，该框架增强了显式的回溯机制，使其能够进行可逆的多跳推理。通过结合基于文本的检索、信息聚合和验证，我们的系统可以在推理过程中检测和纠正错误，从而产生更 robust 和可解释的问答结果。该框架和实验为未来容错问答系统的工作奠定了基础。跨三个基准的实证评估表明，ReAgent的有效性，相对于基线模型平均提高了约6%。', 'title_zh': 'ReAgent: 可逆多agent推理在知识增强多跳QA中的应用'}
{'arxiv_id': 'arXiv:2503.06553', 'title': 'ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges', 'authors': 'Jiaxin Ai, Pengfei Zhou, Zhaopan Xu, Ming Li, Fanrui Zhang, Zizhen Li, Jianwen Sun, Yukang Feng, Baojin Huang, Zhongyuan Wang, Kaipeng Zhang', 'link': 'https://arxiv.org/abs/2503.06553', 'abstract': "As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting MLLMs as automated process judges has become a common practice. However, the reliability of these model-based judges remains uncertain. To address this, we introduce ProJudgeBench, the first comprehensive benchmark specifically designed for evaluating abilities of MLLM-based process judges. ProJudgeBench comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and multi-modal content. In ProJudgeBench, each step is meticulously annotated by human experts for correctness, error type, and explanation, enabling a systematic evaluation of judges' capabilities to detect, classify and diagnose errors. Evaluation on ProJudgeBench reveals a significant performance gap between open-source and proprietary models. To bridge this gap, we further propose ProJudge-173k, a large-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning strategy that encourages models to explicitly reason through problem-solving before assessing solutions. Both contributions significantly enhance the process evaluation capabilities of open-source models. All the resources will be released to foster future research of reliable multi-modal process evaluation.", 'abstract_zh': '多模态大语言模型过程评估基准ProJudgeBench及其应用研究', 'title_zh': 'ProJudge: 一种基于MLLM的多模态多学科基准与指令调优数据集'}
{'arxiv_id': 'arXiv:2503.06479', 'title': 'ExKG-LLM: Leveraging Large Language Models for Automated Expansion of Cognitive Neuroscience Knowledge Graphs', 'authors': 'Ali Sarabadani, Kheirolah Rahsepar Fard, Hamid Dalvand', 'link': 'https://arxiv.org/abs/2503.06479', 'abstract': 'The paper introduces ExKG-LLM, a framework designed to automate the expansion of cognitive neuroscience knowledge graphs (CNKG) using large language models (LLMs). It addresses limitations in existing tools by enhancing accuracy, completeness, and usefulness in CNKG. The framework leverages a large dataset of scientific papers and clinical reports, applying state-of-the-art LLMs to extract, optimize, and integrate new entities and relationships. Evaluation metrics include precision, recall, and graph density. Results show significant improvements: precision (0.80, +6.67%), recall (0.81, +15.71%), F1 score (0.805, +11.81%), and increased edge nodes (21.13% and 31.92%). Graph density slightly decreased, reflecting a broader but more fragmented structure. Engagement rates rose by 20%, while CNKG diameter increased to 15, indicating a more distributed structure. Time complexity improved to O(n log n), but space complexity rose to O(n2), indicating higher memory usage. ExKG-LLM demonstrates potential for enhancing knowledge generation, semantic search, and clinical decision-making in cognitive neuroscience, adaptable to broader scientific fields.', 'abstract_zh': 'ExKG-LLM：一种利用大规模语言模型自动扩展认知神经科学知识图谱的框架', 'title_zh': 'ExKG-LLM：利用大规模语言模型自动扩展认知神经科学知识图谱'}
{'arxiv_id': 'arXiv:2503.06475', 'title': 'SKG-LLM: Developing a Mathematical Model for Stroke Knowledge Graph Construction Using Large Language Models', 'authors': 'Ali Sarabadani, Kheirolah Rahsepar Fard, Hamid Dalvand', 'link': 'https://arxiv.org/abs/2503.06475', 'abstract': 'The purpose of this study is to introduce SKG-LLM. A knowledge graph (KG) is constructed from stroke-related articles using mathematical and large language models (LLMs). SKG-LLM extracts and organizes complex relationships from the biomedical literature, using it to increase the accuracy and depth of KG in stroke research. In the proposed method, GPT-4 was used for data pre-processing, and the extraction of embeddings was also done by GPT-4 in the whole KG construction process. The performance of the proposed model was tested with two evaluation criteria: Precision and Recall. For further validation of the proposed model, GPT-4 was used. Compared with Wikidata and WN18RR, the proposed KG-LLM approach performs better, especially in precision and recall. By including GPT-4 in the preprocessing process, the SKG-LLM model achieved a precision score of 0.906 and a recall score of 0.923. Expert reviews further improved the results and increased precision to 0.923 and recall to 0.918. The knowledge graph constructed by SKG-LLM contains 2692 nodes and 5012 edges, which are 13 distinct types of nodes and 24 types of edges.', 'abstract_zh': 'SKG-LLM：基于数学和大型语言模型的中风相关知识图谱构建与应用', 'title_zh': 'SKG-LLM：使用大规模语言模型构建中风知识图谱的数学模型开发'}
{'arxiv_id': 'arXiv:2503.06416', 'title': 'Advancing AI Negotiations: New Theory and Evidence from a Large-Scale Autonomous Negotiations Competition', 'authors': 'Michelle Vaccaro, Michael Caoson, Harang Ju, Sinan Aral, Jared R. Curhan', 'link': 'https://arxiv.org/abs/2503.06416', 'abstract': 'Despite the rapid proliferation of artificial intelligence (AI) negotiation agents, there has been limited integration of computer science research and established negotiation theory to develop new theories of AI negotiation. To bridge this gap, we conducted an International AI Negotiations Competition in which participants iteratively designed and refined prompts for large language model (LLM) negotiation agents. We then facilitated over 120,000 negotiations between these agents across multiple scenarios with diverse characteristics and objectives. Our findings revealed that fundamental principles from established human-human negotiation theory remain crucial in AI-AI negotiations. Specifically, agents exhibiting high warmth fostered higher counterpart subjective value and reached deals more frequently, which enabled them to create and claim more value in integrative settings. However, conditional on reaching a deal, warm agents claimed less value while dominant agents claimed more value. These results align with classic negotiation theory emphasizing relationship-building, assertiveness, and preparation. Our analysis also revealed unique dynamics in AI-AI negotiations not fully explained by negotiation theory, particularly regarding the effectiveness of AI-specific strategies like chain-of-thought reasoning and prompt injection. The agent that won our competition implemented an approach that blended traditional negotiation preparation frameworks with AI-specific methods. Together, these results suggest the importance of establishing a new theory of AI negotiations that integrates established negotiation theory with AI-specific strategies to optimize agent performance. Our research suggests this new theory must account for the unique characteristics of autonomous agents and establish the conditions under which traditional negotiation theory applies in automated settings.', 'abstract_zh': '尽管人工智能（AI）谈判代理迅速普及，计算机科学研究与已建立的谈判理论在发展新的AI谈判理论方面的整合仍显不足。为弥合这一差距，我们举办了一场国际AI谈判竞赛，在此竞赛中，参赛者迭代设计和优化了大型语言模型（LLM）谈判代理的提示。随后，我们在多个具有多样特征和目标的场景中，促成了这些代理之间的超过120,000场谈判。研究发现表明，现有的人际谈判理论的基本原则在AI-AI谈判中依然至关重要。具体而言，表现出高度温暖特质的代理能够促进更高的对方主观价值感知并更频繁地达成协议，这使它们在整合性谈判情境中能够创造并宣称更多的价值。然而，在达成协议的前提下，温暖代理所宣称的价值较少，而主导代理所宣称的价值更多。这些结果与强调关系建立、坚定性和准备的经典谈判理论一致。我们的分析还揭示了AI-AI谈判中一些未被经典谈判理论充分解释的独特动态，尤其是在AI特定策略方面的有效性，如链式推理和提示注入。在我们竞赛中获胜的代理采取了一种传统谈判筹备框架与AI特定方法相结合的方法。综上所述，这些结果强调了需建立一种结合现有谈判理论与AI特定策略的新AI谈判理论的重要性，以优化代理绩效。我们的研究建议这一新理论必须考虑到自主代理的独特特性，并确定传统谈判理论在自动化情境中适用的条件。', 'title_zh': 'advancing AI谈判：大型自主谈判竞赛的新理论与证据'}
{'arxiv_id': 'arXiv:2503.06410', 'title': 'Performant LLM Agentic Framework for Conversational AI', 'authors': 'Alex Casella, Wayne Wang', 'link': 'https://arxiv.org/abs/2503.06410', 'abstract': 'The rise of Agentic applications and automation in the Voice AI industry has led to an increased reliance on Large Language Models (LLMs) to navigate graph-based logic workflows composed of nodes and edges. However, existing methods face challenges such as alignment errors in complex workflows and hallucinations caused by excessive context size. To address these limitations, we introduce the Performant Agentic Framework (PAF), a novel system that assists LLMs in selecting appropriate nodes and executing actions in order when traversing complex graphs. PAF combines LLM-based reasoning with a mathematically grounded vector scoring mechanism, achieving both higher accuracy and reduced latency. Our approach dynamically balances strict adherence to predefined paths with flexible node jumps to handle various user inputs efficiently. Experiments demonstrate that PAF significantly outperforms baseline methods, paving the way for scalable, real-time Conversational AI systems in complex business environments.', 'abstract_zh': '代理应用和自动化在语音AI行业的兴起导致了对大型语言模型（LLMs）的更大依赖，以导航由节点和边组成的图形逻辑工作流。然而，现有方法面临复杂工作流中的对齐错误和由于上下文大小过大引起的幻觉等挑战。为了解决这些限制，我们引入了高效的代理框架（PAF），这是一种新颖的系统，该系统帮助LLMs在遍历复杂图形时选择合适的节点并按顺序执行操作。PAF将基于LLM的推理与数学依据的向量评分机制相结合，从而实现更高的准确性和更低的延迟。我们的方法动态平衡严格的路径遵循与灵活的节点跳跃，以高效处理各种用户输入。实验表明，PAF在基准方法上表现显著更优，为复杂商业环境中的可扩展、实时对话AI系统铺平了道路。', 'title_zh': '高性能的LLM代理框架应用于对话式AI'}
{'arxiv_id': 'arXiv:2503.06202', 'title': 'Breaking Free from MMI: A New Frontier in Rationalization by Probing Input Utilization', 'authors': 'Wei Liu, Zhiying Deng, Zhongyu Niu, Jun Wang, Haozhao Wang, Zhigang Zeng, Ruixuan Li', 'link': 'https://arxiv.org/abs/2503.06202', 'abstract': 'Extracting a small subset of crucial rationales from the full input is a key problem in explainability research. The most widely used fundamental criterion for rationale extraction is the maximum mutual information (MMI) criterion. In this paper, we first demonstrate that MMI suffers from diminishing marginal returns. Once part of the rationale has been identified, finding the remaining portions contributes only marginally to increasing the mutual information, making it difficult to use MMI to locate the rest. In contrast to MMI that aims to reproduce the prediction, we seek to identify the parts of the input that the network can actually utilize.\nThis is achieved by comparing how different rationale candidates match the capability space of the weight matrix. The weight matrix of a neural network is typically low-rank, meaning that the linear combinations of its column vectors can only cover part of the directions in a high-dimensional space (high-dimension: the dimensions of an input vector). If an input is fully utilized by the network, {it generally matches these directions (e.g., a portion of a hypersphere), resulting in a representation with a high norm. Conversely, if an input primarily falls outside (orthogonal to) these directions}, its representation norm will approach zero, behaving like noise that the network cannot effectively utilize. Building on this, we propose using the norms of rationale candidates as an alternative objective to MMI. Through experiments on four text classification datasets and one graph classification dataset using three network architectures (GRUs, BERT, and GCN), we show that our method outperforms MMI and its improved variants in identifying better rationales. We also compare our method with a representative LLM (llama-3.1-8b-instruct) and find that our simple method gets comparable results to it and can sometimes even outperform it.', 'abstract_zh': '从全输入中提取关键解释的小子集是解释性研究中的一个关键问题。最常用的根本标准是最大互信息（MMI）准则。在本文中，我们首先证明MMI存在边际效益递减的现象。一旦部分解释被识别，找到剩余部分只会微不足道地增加互信息，使得使用MMI定位其余部分变得困难。与MMI旨在重现预测不同，我们寻求识别网络能够实际利用的输入部分。这通过比较不同解释候选与权重矩阵的能力空间匹配来实现。神经网络的权重矩阵通常具有低秩，意味着其列向量的线性组合只能覆盖高维空间（高维：输入向量的维度）的一部分方向。如果输入被网络充分利用，它通常匹配这些方向（例如，超球体的一部分），导致具有高模态的表现。反之，如果输入主要位于这些方向之外（正交），其表示模态将接近零，行为类似于网络无法有效利用的噪声。基于此，我们提出使用解释候选的模态作为MMI的替代目标。通过在四个文本分类数据集和一个图分类数据集上使用三种网络架构（GRUs、BERT和GCN）进行实验，我们证明我们的方法在识别更好的解释方面优于MMI及其改进变体。我们还将我们的方法与一个代表性的大语言模型（llama-3.1-8b-instruct）进行比较，发现我们的简单方法可以获得类似的结果，有时甚至可以优于它。', 'title_zh': '突破MMI束缚：通过探究输入利用的新理性化前沿'}
{'arxiv_id': 'arXiv:2503.06047', 'title': 'DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments', 'authors': 'Wenjie Tang, Yuan Zhou, Erqiang Xu, Keyan Cheng, Minne Li, Liquan Xiao', 'link': 'https://arxiv.org/abs/2503.06047', 'abstract': 'Large Language Model~(LLM) based agents have been increasingly popular in solving complex and dynamic tasks, which requires proper evaluation systems to assess their capabilities. Nevertheless, existing benchmarks usually either focus on single-objective tasks or use overly broad assessing metrics, failing to provide a comprehensive inspection of the actual capabilities of LLM-based agents in complicated decision-making tasks. To address these issues, we introduce DSGBench, a more rigorous evaluation platform for strategic decision-making. Firstly, it incorporates six complex strategic games which serve as ideal testbeds due to their long-term and multi-dimensional decision-making demands and flexibility in customizing tasks of various difficulty levels or multiple targets. Secondly, DSGBench employs a fine-grained evaluation scoring system which examines the decision-making capabilities by looking into the performance in five specific dimensions and offering a comprehensive assessment in a well-designed way. Furthermore, DSGBench also incorporates an automated decision-tracking mechanism which enables in-depth analysis of agent behaviour patterns and the changes in their strategies. We demonstrate the advances of DSGBench by applying it to multiple popular LLM-based agents and our results suggest that DSGBench provides valuable insights in choosing LLM-based agents as well as improving their future development. DSGBench is available at this https URL.', 'abstract_zh': '基于大型语言模型的代理在解决复杂动态任务方面的评估：DSGBench——一种更为严格的战略性决策评估平台', 'title_zh': 'DSGBench：一种用于评估基于LLM的代理在复杂决策环境中的多样性战略博弈基准'}
{'arxiv_id': 'arXiv:2503.05944', 'title': 'Enhancing Reasoning with Collaboration and Memory', 'authors': 'Julie Michelman, Nasrin Baratalipour, Matthew Abueg', 'link': 'https://arxiv.org/abs/2503.05944', 'abstract': 'We envision a continuous collaborative learning system where groups of LLM agents work together to solve reasoning problems, drawing on memory they collectively build to improve performance as they gain experience. This work establishes the foundations for such a system by studying the interoperability of chain-of-thought reasoning styles, multi-agent collaboration, and memory banks. Extending beyond the identical agents of self-consistency, we introduce varied-context agents with diverse exemplars and a summarizer agent in place of voting. We generate frozen and continuously learned memory banks of exemplars and pair them with fixed, random, and similarity-based retrieval mechanisms. Our systematic study reveals where various methods contribute to reasoning performance of two LLMs on three grounded reasoning tasks, showing that random exemplar selection can often beat more principled approaches, and in some tasks, inclusion of any exemplars serves only to distract both weak and strong models.', 'abstract_zh': '设想一个连续协作学习系统，多组语言模型代理共同解决推理问题，借鉴他们集体构建的记忆，随着经验的积累提升性能。本研究通过研究链式思考推理风格、多智能体协作和记忆库的互操作性，奠定了此类系统的基础。扩展超越自一致性中的相同代理，我们引入了具有多样化范例的上下文变化代理，并用总结器代理取代投票机制。我们生成了冻结和持续学习的范例记忆库，并与固定、随机和基于相似性的检索机制配对。我们的系统性研究揭示了在三个具体推理任务中，各种方法如何影响两个语言模型的推理性能，表明随机范例选择往往能超越更严谨的方法，并且在某些任务中，任何范例的存在可能会分散弱模型和强模型的注意力。', 'title_zh': '增强推理能力：协作与记忆的结合'}
{'arxiv_id': 'arXiv:2503.07599', 'title': 'NeuroChat: A Neuroadaptive AI Chatbot for Customizing Learning Experiences', 'authors': 'Dünya Baradari, Nataliya Kosmyna, Oscar Petrov, Rebecah Kaplun, Pattie Maes', 'link': 'https://arxiv.org/abs/2503.07599', 'abstract': "Generative AI is transforming education by enabling personalized, on-demand learning experiences. However, AI tutors lack the ability to assess a learner's cognitive state in real time, limiting their adaptability. Meanwhile, electroencephalography (EEG)-based neuroadaptive systems have successfully enhanced engagement by dynamically adjusting learning content. This paper presents NeuroChat, a proof-of-concept neuroadaptive AI tutor that integrates real-time EEG-based engagement tracking with generative AI. NeuroChat continuously monitors a learner's cognitive engagement and dynamically adjusts content complexity, response style, and pacing using a closed-loop system. We evaluate this approach in a pilot study (n=24), comparing NeuroChat to a standard LLM-based chatbot. Results indicate that NeuroChat enhances cognitive and subjective engagement but does not show an immediate effect on learning outcomes. These findings demonstrate the feasibility of real-time cognitive feedback in LLMs, highlighting new directions for adaptive learning, AI tutoring, and human-AI interaction.", 'abstract_zh': '生成式AI正在通过提供个性化和按需的学习体验来变革教育。然而，AI辅导缺乏实时评估学习者认知状态的能力，限制了其适应性。与此同时，基于脑电图（EEG）的神经自适应系统通过动态调整学习内容成功地提升了学习者的参与度。本文提出了一种概念验证的神经自适应AI辅导系统——NeuroChat，它将实时EEG基参与度跟踪与生成式AI相结合。NeuroChat持续监测学习者的认知参与度，并通过闭环系统动态调整内容复杂性、回应风格和进度。我们在一项初步研究（n=24）中评估了这种方法，将NeuroChat与基于标准大型语言模型的聊天机器人进行比较。结果表明，NeuroChat提高了认知参与度和主观参与度，但对学习成果没有立即影响。这些发现展示了实时认知反馈在大型语言模型中的可行性，突显了适应性学习、AI辅导和人机交互的新方向。', 'title_zh': '神经聊友：一种神经自适应人工智能聊天机器人，用于个性化学习体验'}
{'arxiv_id': 'arXiv:2503.07572', 'title': 'Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning', 'authors': 'Yuxiao Qu, Matthew Y. R. Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, Aviral Kumar', 'link': 'https://arxiv.org/abs/2503.07572', 'abstract': "Training models to effectively use test-time compute is crucial for improving the reasoning performance of LLMs. Current methods mostly do so via fine-tuning on search traces or running RL with 0/1 outcome reward, but do these approaches efficiently utilize test-time compute? Would these approaches continue to scale as the budget improves? In this paper, we try to answer these questions. We formalize the problem of optimizing test-time compute as a meta-reinforcement learning (RL) problem, which provides a principled perspective on spending test-time compute. This perspective enables us to view the long output stream from the LLM as consisting of several episodes run at test time and leads us to use a notion of cumulative regret over output tokens as a way to measure the efficacy of test-time compute. Akin to how RL algorithms can best tradeoff exploration and exploitation over training, minimizing cumulative regret would also provide the best balance between exploration and exploitation in the token stream. While we show that state-of-the-art models do not minimize regret, one can do so by maximizing a dense reward bonus in conjunction with the outcome 0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in the output stream, quantified by the change in the likelihood of eventual success. Using these insights, we develop Meta Reinforcement Fine-Tuning, or MRT, a new class of fine-tuning methods for optimizing test-time compute. MRT leads to a 2-3x relative gain in performance and roughly a 1.5x gain in token efficiency for math reasoning compared to outcome-reward RL.", 'abstract_zh': '训练模型有效利用测试时计算对于提升大型语言模型的推理性能至关重要。当前方法主要通过在搜索轨迹上进行微调或使用0/1结果奖励进行强化学习来实现这一目标，但这些方法是否高效利用了测试时计算?在预算提高时，这些方法能否继续扩展?在本文中，我们试图回答这些问题。我们将优化测试时计算的问题形式化为元强化学习问题，从而从原则上解决了测试时计算的使用问题。这一视角使得我们将LLM的长输出流视为在测试时间内运行的多个episode，并提出了一种累积悔恨的概念，作为衡量测试时计算效果的方式。类似于如何在训练过程中通过RL算法权衡探索与利用，最小化累积悔恨也会在文本流中提供最佳的探索与利用权衡。尽管我们发现最先进的模型并未最小化悔恨，但可以通过与0/1结果奖励RL结合使用密集奖励 bonus 来实现这一目标。这一 bonus 是每个后续输出块相对于最终成功概率的增进量。基于这些洞察，我们开发了元强化学习微调（MRT），这是一种新的优化测试时计算的微调方法。与结果奖励 RL 相比，MRT 在数学推理任务中分别实现了约2-3倍的性能提升和约1.5倍的 tokens 效率提升。', 'title_zh': '通过元强化微调优化测试时计算'}
{'arxiv_id': 'arXiv:2503.07550', 'title': 'KSOD: Knowledge Supplement for LLMs On Demand', 'authors': 'Haoran Li, Junfeng Hu', 'link': 'https://arxiv.org/abs/2503.07550', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, yet still produce errors in domain-specific tasks. To further improve their performance, we propose KSOD (Knowledge Supplement for LLMs On Demand), a novel framework that empowers LLMs to improve their capabilities with knowledge-based supervised fine-tuning (SFT). KSOD analyzes the causes of errors from the perspective of knowledge deficiency by identifying potential missing knowledge in LLM that may lead to the errors. Subsequently, KSOD tunes a knowledge module on knowledge dataset and verifies whether the LLM lacks the identified knowledge based on it. If the knowledge is verified, KSOD supplements the LLM with the identified knowledge using the knowledge module. Tuning LLMs on specific knowledge instead of specific task decouples task and knowledge and our experiments on two domain-specific benchmarks and four general benchmarks empirically demonstrate that KSOD enhances the performance of LLMs on tasks requiring the supplemented knowledge while preserving their performance on other tasks. Our findings shed light on the potential of improving the capabilities of LLMs with knowledge-based SFT.', 'abstract_zh': '基于知识补全的大型语言模型在线微调框架：KSOD', 'title_zh': 'KSOD: 需求驱动的LLM知识补充'}
{'arxiv_id': 'arXiv:2503.07536', 'title': 'LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL', 'authors': 'Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, Xu Yang', 'link': 'https://arxiv.org/abs/2503.07536', 'abstract': 'Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges from the complex interplay between visual perception and logical reasoning, particularly in compact 3B-parameter architectures where architectural constraints limit reasoning capacity and modality alignment.\nWhile rule-based reinforcement learning (RL) excels in text-only domains, its multimodal extension confronts two critical barriers: (1) data limitations due to ambiguous answers and scarce complex reasoning examples, and (2) degraded foundational reasoning induced by multimodal pretraining.\nTo address these challenges, we propose \\textbf{\\method}, a two-stage framework adapting rule-based RL for multimodal reasoning through \\textbf{Foundational Reasoning Enhancement (FRE)} followed by \\textbf{Multimodal Generalization Training (MGT)}. The FRE stage first strengthens reasoning abilities using text-only data with rule-based RL, then the MGT stage generalizes these reasoning capabilities to multimodal domains.\nExperiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves 4.83\\% and 4.5\\% average improvements over baselines in multimodal and text-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game tasks. These results validate that text-based reasoning enhancement enables effective multimodal generalization, offering a data-efficient paradigm that bypasses costly high-quality multimodal training data.', 'abstract_zh': '增强大型多模态模型中的推理能力面临独特的挑战，源于视觉感知与逻辑推理之间复杂的交互作用，特别是在紧凑的3B参数架构中，架构约束限制了推理能力与模态对齐。', 'title_zh': 'LMM-R1：通过两阶段基于规则的强化学习增强3B大型语言模型的强推理能力'}
{'arxiv_id': 'arXiv:2503.07518', 'title': 'TokenButler: Token Importance is Predictable', 'authors': 'Yash Akhauri, Ahmed F AbouElhamayed, Yifei Gao, Chi-Chih Chang, Nilesh Jain, Mohamed S. Abdelfattah', 'link': 'https://arxiv.org/abs/2503.07518', 'abstract': 'Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token history, enabling efficient decoding of tokens. As the KV-Cache grows, it becomes a major memory and computation bottleneck, however, there is an opportunity to alleviate this bottleneck, especially because prior research has shown that only a small subset of tokens contribute meaningfully to each decoding step. A key challenge in finding these critical tokens is that they are dynamic, and heavily input query-dependent. Existing methods either risk quality by evicting tokens permanently, or retain the full KV-Cache but rely on retrieving chunks (pages) of tokens at generation, failing at dense, context-rich tasks. Additionally, many existing KV-Cache sparsity methods rely on inaccurate proxies for token importance. To address these limitations, we introduce TokenButler, a high-granularity, query-aware predictor that learns to identify these critical tokens. By training a light-weight predictor with less than 1.2% parameter overhead, TokenButler prioritizes tokens based on their contextual, predicted importance. This improves perplexity & downstream accuracy by over 8% relative to SoTA methods for estimating token importance. We evaluate TokenButler on a novel synthetic small-context co-referential retrieval task, demonstrating near-oracle accuracy. Code, models and benchmarks: this https URL', 'abstract_zh': 'Large Language Models (LLMs)依赖Key-Value (KV)缓存来存储令牌历史，从而实现高效的令牌解码。随着KV缓存的增长，它成为主要的内存和计算瓶颈，然而，先前的研究表明，只有少量令牌对每个解码步骤有显著贡献，因此有机会缓解这一瓶颈。找到这些关键令牌的关键挑战在于它们是动态的，并且高度依赖于输入查询。现有方法要么冒着质量下降的风险永久移除令牌，要么保留完整的KV缓存但在生成时依赖于检索令牌片段（页面），这在密集的上下文丰富任务中失败。此外，许多现有的KV缓存稀疏方法依赖于令牌重要性的不准确代理。为了解决这些限制，我们引入了TokenButler，这是一种高粒度、查询感知的预测器，能够学习识别这些关键令牌。通过训练一个参数开销不到1.2%的轻量级预测器，TokenButler基于上下文预测的重要性对令牌进行优先处理。这在估计令牌重要性的方法中相对提高了超过8%的困惑度和下游准确性。我们在一个新颖的合成小上下文共指检索任务上评估TokenButler，展示了接近完美准确度。代码、模型和基准：[链接]', 'title_zh': 'TokenButler: 令牌的重要性是可以预测的'}
{'arxiv_id': 'arXiv:2503.07513', 'title': 'Language Models Fail to Introspect About Their Knowledge of Language', 'authors': 'Siyuan Song, Jennifer Hu, Kyle Mahowald', 'link': 'https://arxiv.org/abs/2503.07513', 'abstract': 'There has been recent interest in whether large language models (LLMs) can introspect about their own internal states. Such abilities would make LLMs more interpretable, and also validate the use of standard introspective methods in linguistics to evaluate grammatical knowledge in models (e.g., asking "Is this sentence grammatical?"). We systematically investigate emergent introspection across 21 open-source LLMs, in two domains where introspection is of theoretical interest: grammatical knowledge and word prediction. Crucially, in both domains, a model\'s internal linguistic knowledge can be theoretically grounded in direct measurements of string probability. We then evaluate whether models\' responses to metalinguistic prompts faithfully reflect their internal knowledge. We propose a new measure of introspection: the degree to which a model\'s prompted responses predict its own string probabilities, beyond what would be predicted by another model with nearly identical internal knowledge. While both metalinguistic prompting and probability comparisons lead to high task accuracy, we do not find evidence that LLMs have privileged "self-access". Our findings complicate recent results suggesting that models can introspect, and add new evidence to the argument that prompted responses should not be conflated with models\' linguistic generalizations.', 'abstract_zh': '近年来，研究人员对大型语言模型（LLMs）是否能够内省其自身内部状态产生了浓厚兴趣。这种能力会使LLMs更具可解释性，并且能够验证语言学中标准的内省方法在评估模型的语法知识（例如询问“这个句子是语法正确的吗？”）中的使用价值。我们系统地研究了21个开源LLM在两个理论上有内省兴趣的领域中的内省现象：语法知识和词预测。关键的是，在这两个领域中，模型的内部语言知识都可以通过字符串概率的直接测量在理论上得到支持。然后，我们评估模型对元语言提示的响应是否忠实反映了其内部知识。我们提出了一种新的内省度量标准：模型受提示响应预测其自身字符串概率的程度，超出另一个具有几乎相同内部知识的模型所能预测的范围。虽然元语言提示和概率比较都能达到高任务准确性，但我们并未发现LLMs具有“自我访问”的特权。我们的研究结果复杂化了近期表明模型具有内省能力的结果，并为不应将受提示的响应与其语言概括混淆的观点提供了新的证据。', 'title_zh': '语言模型无法反省其语言知识。'}
{'arxiv_id': 'arXiv:2503.07426', 'title': 'RePO: ReLU-based Preference Optimization', 'authors': 'Junkang Wu, Kexin Huang, Xue Wang, Jinyang Gao, Bolin Ding, Jiancan Wu, Xiangnan He, Xiang Wang', 'link': 'https://arxiv.org/abs/2503.07426', 'abstract': "Aligning large language models (LLMs) with human preferences is critical for real-world deployment, yet existing methods like RLHF face computational and stability challenges. While DPO establishes an offline paradigm with single hyperparameter $\\beta$, subsequent methods like SimPO reintroduce complexity through dual parameters ($\\beta$, $\\gamma$). We propose {ReLU-based Preference Optimization (RePO)}, a streamlined algorithm that eliminates $\\beta$ via two advances: (1) retaining SimPO's reference-free margins but removing $\\beta$ through gradient analysis, and (2) adopting a ReLU-based max-margin loss that naturally filters trivial pairs. Theoretically, RePO is characterized as SimPO's limiting case ($\\beta \\to \\infty$), where the logistic weighting collapses to binary thresholding, forming a convex envelope of the 0-1 loss. Empirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO and SimPO across multiple base models, requiring only one hyperparameter to tune.", 'abstract_zh': '基于ReLU的偏好优化（RePO）：简化的大语言模型偏好对齐方法', 'title_zh': 'ReLU为基础的Preference Optimization'}
{'arxiv_id': 'arXiv:2503.07384', 'title': 'Is My Text in Your AI Model? Gradient-based Membership Inference Test applied to LLMs', 'authors': 'Gonzalo Mancera, Daniel de Alcala, Julian Fierrez, Ruben Tolosana, Aythami Morales', 'link': 'https://arxiv.org/abs/2503.07384', 'abstract': 'This work adapts and studies the gradient-based Membership Inference Test (gMINT) to the classification of text based on LLMs. MINT is a general approach intended to determine if given data was used for training machine learning models, and this work focuses on its application to the domain of Natural Language Processing. Using gradient-based analysis, the MINT model identifies whether particular data samples were included during the language model training phase, addressing growing concerns about data privacy in machine learning. The method was evaluated in seven Transformer-based models and six datasets comprising over 2.5 million sentences, focusing on text classification tasks. Experimental results demonstrate MINTs robustness, achieving AUC scores between 85% and 99%, depending on data size and model architecture. These findings highlight MINTs potential as a scalable and reliable tool for auditing machine learning models, ensuring transparency, safeguarding sensitive data, and fostering ethical compliance in the deployment of AI/NLP technologies.', 'abstract_zh': '基于LLM的文本分类中的梯度导向会员推理测试研究', 'title_zh': '我的文本在我的AI模型中吗？基于梯度的成员推理测试在大规模语言模型中的应用'}
{'arxiv_id': 'arXiv:2503.07329', 'title': 'Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning Large Language Models', 'authors': 'Hao Zhou, Guergana Savova, Lijing Wang', 'link': 'https://arxiv.org/abs/2503.07329', 'abstract': 'The impact of random seeds in fine-tuning large language models (LLMs) has been largely overlooked despite its potential influence on model this http URL this study, we systematically evaluate the effects of random seeds on LLMs using the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact through traditional metrics like accuracy and F1, calculating their mean and variance to quantify performance fluctuations. To capture the micro-level effects, we introduce a novel metric, consistency, measuring the stability of individual predictions across runs. Our experiments reveal significant variance at both macro and micro levels, underscoring the need for careful consideration of random seeds in fine-tuning and evaluation.', 'abstract_zh': '随机种子在细调大规模语言模型中的影响 largely overlooked 尽管其可能对模型有潜在影响 在本研究中，我们系统地使用GLUE和SuperGLUE基准评估随机种子对大规模语言模型的影响。我们通过准确率和F1等传统指标从宏观层面分析影响，并计算其均值和方差以量化性能波动。为捕捉微观影响，我们引入了一个新的指标——一致性，衡量单个预测在不同运行中的稳定性。实验结果揭示了宏观和微观层面的显著差异，强调了在细调和评估过程中仔细考虑随机种子的重要性。', 'title_zh': '评估随机种子对大型语言模型微调的宏观和微观影响'}
{'arxiv_id': 'arXiv:2503.07320', 'title': 'Experimental Exploration: Investigating Cooperative Interaction Behavior Between Humans and Large Language Model Agents', 'authors': 'Guanxuan Jiang, Yuyang Wang, Pan Hui', 'link': 'https://arxiv.org/abs/2503.07320', 'abstract': "With the rise of large language models (LLMs), AI agents as autonomous decision-makers present significant opportunities and challenges for human-AI cooperation. While many studies have explored human cooperation with AI as tools, the role of LLM-augmented autonomous agents in competitive-cooperative interactions remains under-examined. This study investigates human cooperative behavior by engaging 30 participants who interacted with LLM agents exhibiting different characteristics (purported human, purported rule-based AI agent, and LLM agent) in repeated Prisoner's Dilemma games. Findings show significant differences in cooperative behavior based on the agents' purported characteristics and the interaction effect of participants' genders and purported characteristics. We also analyzed human response patterns, including game completion time, proactive favorable behavior, and acceptance of repair efforts. These insights offer a new perspective on human interactions with LLM agents in competitive cooperation contexts, such as virtual avatars or future physical entities. The study underscores the importance of understanding human biases toward AI agents and how observed behaviors can influence future human-AI cooperation dynamics.", 'abstract_zh': '大规模语言模型兴起背景下自主决策的AI代理对人类-AI合作的影响：基于囚徒困境游戏的人类合作行为研究', 'title_zh': '实验探索：探究人类与大型语言模型代理之间的合作互动行为'}
{'arxiv_id': 'arXiv:2503.07237', 'title': 'LLM-C3MOD: A Human-LLM Collaborative System for Cross-Cultural Hate Speech Moderation', 'authors': 'Junyeong Park, Seogyeong Jeong, Seyoung Song, Yohan Lee, Alice Oh', 'link': 'https://arxiv.org/abs/2503.07237', 'abstract': "Content moderation is a global challenge, yet major tech platforms prioritize high-resource languages, leaving low-resource languages with scarce native moderators. Since effective moderation depends on understanding contextual cues, this imbalance increases the risk of improper moderation due to non-native moderators' limited cultural understanding. Through a user study, we identify that non-native moderators struggle with interpreting culturally-specific knowledge, sentiment, and internet culture in the hate speech moderation. To assist them, we present LLM-C3MOD, a human-LLM collaborative pipeline with three steps: (1) RAG-enhanced cultural context annotations; (2) initial LLM-based moderation; and (3) targeted human moderation for cases lacking LLM consensus. Evaluated on a Korean hate speech dataset with Indonesian and German participants, our system achieves 78% accuracy (surpassing GPT-4o's 71% baseline), while reducing human workload by 83.6%. Notably, human moderators excel at nuanced contents where LLMs struggle. Our findings suggest that non-native moderators, when properly supported by LLMs, can effectively contribute to cross-cultural hate speech moderation.", 'abstract_zh': '内容审核是一个全球性的挑战，但主要科技平台优先考虑高资源语言，使低资源语言缺乏本土审核员。鉴于有效的审核依赖于对上下文线索的理解，这种不平衡增加了非本土审核员因文化理解有限而进行不当审核的风险。通过用户研究，我们发现非本土审核员在仇恨言论审核中难以解读文化特定的知识、情感和互联网文化。为此，我们提出了LLM-C3MOD，这是一种三人机协作流程，包含三个步骤：（1）RAG增强的文化背景注解；（2）基于LLM的初步审核；（3）针对LLM意见不一致的情况进行有针对性的人工审核。在包含韩、印尼和德语数据的仇恨言论数据集上评估，我们的系统达到了78%的准确率（超过GPT-4o的71%基线），同时减少了83.6%的人工工作量。值得注意的是，人类审核员在LLM难以处理的细微内容上表现出色。我们的研究结果表明，在适当的人工智能支持下，非本土审核员可以有效地参与跨文化仇恨言论审核。', 'title_zh': 'LLM-C3MOD: 一种跨文化仇恨言论 moderating 的人-大语言模型协作系统'}
{'arxiv_id': 'arXiv:2503.07154', 'title': 'Ideas in Inference-time Scaling can Benefit Generative Pre-training Algorithms', 'authors': 'Jiaming Song, Linqi Zhou', 'link': 'https://arxiv.org/abs/2503.07154', 'abstract': "Recent years have seen significant advancements in foundation models through generative pre-training, yet algorithmic innovation in this space has largely stagnated around autoregressive models for discrete signals and diffusion models for continuous signals. This stagnation creates a bottleneck that prevents us from fully unlocking the potential of rich multi-modal data, which in turn limits the progress on multimodal intelligence. We argue that an inference-first perspective, which prioritizes scaling efficiency during inference time across sequence length and refinement steps, can inspire novel generative pre-training algorithms. Using Inductive Moment Matching (IMM) as a concrete example, we demonstrate how addressing limitations in diffusion models' inference process through targeted modifications yields a stable, single-stage algorithm that achieves superior sample quality with over an order of magnitude greater inference efficiency.", 'abstract_zh': '近年来，生成预训练在基础模型中取得了显著进展，但在这一领域的算法创新主要集中在离散信号的自回归模型和连续信号的扩散模型上，这种创新停滞限制了我们充分利用丰富多模态数据的潜力，进而限制了多模态智能的发展。我们认为，在推理优先的观点下，即在推理时间和序列长度及细化步骤上的扩展效率优先，可以激发新的生成预训练算法。以归纳矩匹配（Inductive Moment Matching，IMM）为例，我们展示通过针对性的修改扩散模型的推理过程，可以得到一个稳定的一阶段算法，该算法在推理效率上提高了超过一个数量级的同时，还取得了更好的样本质量。', 'title_zh': '推理时缩放思想可以benefit生成预训练算法'}
{'arxiv_id': 'arXiv:2503.07067', 'title': 'DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs', 'authors': 'Jongwoo Ko, Tianyi Chen, Sungnyun Kim, Tianyu Ding, Luming Liang, Ilya Zharkov, Se-Young Yun', 'link': 'https://arxiv.org/abs/2503.07067', 'abstract': 'Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.', 'abstract_zh': '尽管知识蒸馏在大型语言模型中取得了成功，但大多数前期工作对教师和学生生成的数据应用了相同的损失函数。这些策略忽视了损失函数形式与数据类型之间的协同作用，导致学生模型的性能提升不 optimal。为了解决这一问题，我们提出了一种对比式方法 DistiLLM-2，该方法通过利用这种协同作用同时增加教师响应的可能性并减少学生响应的可能性。我们的广泛实验表明，DistiLLM-2 不仅构建了在各种任务（包括指令跟随和代码生成）中性能优异的学生模型，还支持多种应用，如偏好对齐和视觉-语言扩展。这些发现凸显了对比式方法在通过有效对齐不同数据类型下的教师和学生模型来增强大型语言模型蒸馏效率的潜力。', 'title_zh': 'DistiLLM-2：一种对比增强的大型语言模型精简方法'}
{'arxiv_id': 'arXiv:2503.07044', 'title': 'DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data Science', 'authors': 'Ziming You, Yumiao Zhang, Dexuan Xu, Yiwei Lou, Yandong Yan, Wei Wang, Huaming Zhang, Yu Huang', 'link': 'https://arxiv.org/abs/2503.07044', 'abstract': "Data Science tasks are multifaceted, dynamic, and often domain-specific. Existing LLM-based approaches largely concentrate on isolated phases, neglecting the interdependent nature of many data science tasks and limiting their capacity for comprehensive end-to-end support. We propose DatawiseAgent, a notebook-centric LLM agent framework that unifies interactions among user, agent and the computational environment through markdown and executable code cells, supporting flexible and adaptive automated data science. Built on a Finite State Transducer(FST), DatawiseAgent orchestrates four stages, including DSF-like planning, incremental execution, self-debugging, and post-filtering. Specifically, the DFS-like planning stage systematically explores the solution space, while incremental execution harnesses real-time feedback and accommodates LLM's limited capabilities to progressively complete tasks. The self-debugging and post-filtering modules further enhance reliability by diagnosing and correcting errors and pruning extraneous information. Extensive experiments on diverse tasks, including data analysis, visualization, and data modeling, show that DatawiseAgent consistently outperforms or matches state-of-the-art methods across multiple model settings. These results highlight its potential to generalize across data science scenarios and lay the groundwork for more efficient, fully automated workflows.", 'abstract_zh': '数据科学任务多面、动态且往往具有领域特定性。现有的基于大模型的方法主要关注孤立阶段，忽视了众多数据科学任务间的相互依赖关系，限制了它们提供全面端到端支持的能力。我们提出了DatawiseAgent，这是一种以笔记本为中心的大模型代理框架，通过Markdown和可执行代码单元统一用户、代理和计算环境之间的交互，支持灵活且自适应的自动化数据科学。该框架基于有限状态转换器（FST），整合了包括类似于数据科学规划、增量执行、自我调试和后筛选在内的四个阶段。具体而言，类似于数据科学规划的阶段系统地探索了解空间，而增量执行阶段利用实时反馈并适应大模型的有限能力，逐步完成任务。自我调试和后筛选模块进一步增强了可靠性，通过诊断和纠正错误并消除多余信息来提高准确性。在包括数据分析、可视化和数据建模在内的多种任务上的广泛实验表明，DatawiseAgent在多种模型设置下均优于或匹配了现有最先进方法。这些结果突显了它在数据科学场景中的通用潜力，并为更高效、完全自动化的流程奠定了基础。', 'title_zh': 'DatawiseAgent: 以笔记本为中心的自动化数据科学LLM代理框架'}
{'arxiv_id': 'arXiv:2503.07036', 'title': 'Bot Wars Evolved: Orchestrating Competing LLMs in a Counterstrike Against Phone Scams', 'authors': 'Nardine Basta, Conor Atkins, Dali Kaafar', 'link': 'https://arxiv.org/abs/2503.07036', 'abstract': 'We present "Bot Wars," a framework using Large Language Models (LLMs) scam-baiters to counter phone scams through simulated adversarial dialogues. Our key contribution is a formal foundation for strategy emergence through chain-of-thought reasoning without explicit optimization. Through a novel two-layer prompt architecture, our framework enables LLMs to craft demographically authentic victim personas while maintaining strategic coherence. We evaluate our approach using a dataset of 3,200 scam dialogues validated against 179 hours of human scam-baiting interactions, demonstrating its effectiveness in capturing complex adversarial dynamics. Our systematic evaluation through cognitive, quantitative, and content-specific metrics shows that GPT-4 excels in dialogue naturalness and persona authenticity, while Deepseek demonstrates superior engagement sustainability.', 'abstract_zh': '我们提出“Bot Wars”框架，该框架利用大型语言模型（LLMs）骗子诱饵通过模拟对抗对话来对抗电话诈骗。我们的主要贡献是在不需要明确优化的情况下，通过链式思考推理为策略 emergence 提供正式基础。通过一种新颖的两层提示架构，我们的框架使 LLMs 能够构建具有 demographic 实际性的受害者人设，同时保持战略连贯性。我们使用包含 3,200 个诈骗对话的数据集对我们的方法进行评估，该数据集基于 179 小时的人类骗子诱饵互动进行验证，展示了其在捕捉复杂对抗动态方面的有效性。通过对认知、定量和内容特定的指标进行系统评估，结果显示 GPT-4 在对话自然度和人设真实性方面表现出色，而 Deepseek 在持续互动方面表现出色。', 'title_zh': 'Bot战进化：在反电话诈骗战役中 orchestrating 竞争的LLMs'}
{'arxiv_id': 'arXiv:2503.06987', 'title': 'Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations', 'authors': 'Jiho Jin, Woosung Kang, Junho Myung, Alice Oh', 'link': 'https://arxiv.org/abs/2503.06987', 'abstract': 'Measuring social bias in large language models (LLMs) is crucial, but existing bias evaluation methods struggle to assess bias in long-form generation. We propose a Bias Benchmark for Generation (BBG), an adaptation of the Bias Benchmark for QA (BBQ), designed to evaluate social bias in long-form generation by having LLMs generate continuations of story prompts. Building our benchmark in English and Korean, we measure the probability of neutral and biased generations across ten LLMs. We also compare our long-form story generation evaluation results with multiple-choice BBQ evaluation, showing that the two approaches produce inconsistent results.', 'abstract_zh': '测量大规模语言模型中的社会偏见至关重要，但现有偏见评估方法难以评估长文本生成中的偏见。我们提出了一个生成偏见基准（BBG），这是针对问答偏见基准（BBQ）的适应版本，用于通过让语言模型生成故事提示的续写来评估长文本生成中的社会偏见。我们在英语和韩语下构建了这一基准，测量了十种语言模型生成中立和有偏见文本的概率。我们还将我们的长文本故事生成评估结果与多项选择题型的BBQ评估结果进行了比较，结果显示两种方法产生了不一致的结果。', 'title_zh': '生成任务中的社会偏见基准：生成式评估与QA式评估的比较'}
{'arxiv_id': 'arXiv:2503.06982', 'title': 'Understanding the Learning Dynamics of LoRA: A Gradient Flow Perspective on Low-Rank Adaptation in Matrix Factorization', 'authors': 'Ziqing Xu, Hancheng Min, Lachlan Ewen MacDonald, Jinqi Luo, Salma Tarmoun, Enrique Mallada, Rene Vidal', 'link': 'https://arxiv.org/abs/2503.06982', 'abstract': 'Despite the empirical success of Low-Rank Adaptation (LoRA) in fine-tuning pre-trained models, there is little theoretical understanding of how first-order methods with carefully crafted initialization adapt models to new tasks. In this work, we take the first step towards bridging this gap by theoretically analyzing the learning dynamics of LoRA for matrix factorization (MF) under gradient flow (GF), emphasizing the crucial role of initialization. For small initialization, we theoretically show that GF converges to a neighborhood of the optimal solution, with smaller initialization leading to lower final error. Our analysis shows that the final error is affected by the misalignment between the singular spaces of the pre-trained model and the target matrix, and reducing the initialization scale improves alignment. To address this misalignment, we propose a spectral initialization for LoRA in MF and theoretically prove that GF with small spectral initialization converges to the fine-tuning task with arbitrary precision. Numerical experiments from MF and image classification validate our findings.', 'abstract_zh': '尽管低秩适应（LoRA）在微调预训练模型方面取得了经验上的成功，但对于精心设计初始化的一阶方法如何适应新任务，仍缺乏理论上的理解。本文首先通过分析梯度流动（GF）下LoRA在矩阵分解（MF）中的学习动态，尝试弥合这一差距，强调初始化的关键作用。对于小型初始化，我们理论上证明GF收敛到最优解的邻域，更小的初始化导致最终误差更低。我们的分析指出，最终误差受到预训练模型的奇异空间与目标矩阵之间的错位影响，并通过减少初始化规模可以改善这种错位。为了缓解这种错位，我们提出了适用于MF的谱初始化方法，并理论上证明了使用小型谱初始化的GF可以以任意精度收敛到微调任务。来自矩阵分解和图像分类的数值实验验证了我们的发现。', 'title_zh': '基于矩阵因子分解中低秩适应的梯度流视角下理解LoRA的learning动态'}
{'arxiv_id': 'arXiv:2503.06948', 'title': 'Large Language Model Guided Progressive Feature Alignment for Multimodal UAV Object Detection', 'authors': 'Wentao Wu, Chenglong Li, Xiao Wang, Bin Luo, Qi Liu', 'link': 'https://arxiv.org/abs/2503.06948', 'abstract': 'Existing multimodal UAV object detection methods often overlook the impact of semantic gaps between modalities, which makes it difficult to achieve accurate semantic and spatial alignments, limiting detection performance. To address this problem, we propose a Large Language Model (LLM) guided Progressive feature Alignment Network called LPANet, which leverages the semantic features extracted from a large language model to guide the progressive semantic and spatial alignment between modalities for multimodal UAV object detection. To employ the powerful semantic representation of LLM, we generate the fine-grained text descriptions of each object category by ChatGPT and then extract the semantic features using the large language model MPNet. Based on the semantic features, we guide the semantic and spatial alignments in a progressive manner as follows. First, we design the Semantic Alignment Module (SAM) to pull the semantic features and multimodal visual features of each object closer, alleviating the semantic differences of objects between modalities. Second, we design the Explicit Spatial alignment Module (ESM) by integrating the semantic relations into the estimation of feature-level offsets, alleviating the coarse spatial misalignment between modalities. Finally, we design the Implicit Spatial alignment Module (ISM), which leverages the cross-modal correlations to aggregate key features from neighboring regions to achieve implicit spatial alignment. Comprehensive experiments on two public multimodal UAV object detection datasets demonstrate that our approach outperforms state-of-the-art multimodal UAV object detectors.', 'abstract_zh': '基于大型语言模型引导的逐步特征对齐网络LPANet多模态无人机目标检测', 'title_zh': '大型语言模型引导的渐进式特征对齐在多模态无人机目标检测中的应用'}
{'arxiv_id': 'arXiv:2503.06926', 'title': 'Effect of Selection Format on LLM Performance', 'authors': 'Yuchen Han, Yucheng Wu, Jeffrey Willard', 'link': 'https://arxiv.org/abs/2503.06926', 'abstract': 'This paper investigates a critical aspect of large language model (LLM) performance: the optimal formatting of classification task options in prompts. Through an extensive experimental study, we compared two selection formats -- bullet points and plain English -- to determine their impact on model performance. Our findings suggest that presenting options via bullet points generally yields better results, although there are some exceptions. Furthermore, our research highlights the need for continued exploration of option formatting to drive further improvements in model performance.', 'abstract_zh': '本文探讨了大型语言模型（LLM）性能的一个关键方面：提示中分类任务选项的最佳格式化方式。通过一项广泛的实验研究，我们将两种选择格式——项目符号和纯英文——进行了对比，以确定它们对模型性能的影响。我们的研究表明，通常通过项目符号呈现选项能获得更好的结果，尽管存在一些例外情况。此外，我们的研究强调了继续探索选项格式化以推动模型性能进一步提升的必要性。', 'title_zh': '选择格式对大语言模型性能的影响'}
{'arxiv_id': 'arXiv:2503.06808', 'title': 'Privacy Auditing of Large Language Models', 'authors': 'Ashwinee Panda, Xinyu Tang, Milad Nasr, Christopher A. Choquette-Choo, Prateek Mittal', 'link': 'https://arxiv.org/abs/2503.06808', 'abstract': "Current techniques for privacy auditing of large language models (LLMs) have limited efficacy -- they rely on basic approaches to generate canaries which leads to weak membership inference attacks that in turn give loose lower bounds on the empirical privacy leakage. We develop canaries that are far more effective than those used in prior work under threat models that cover a range of realistic settings. We demonstrate through extensive experiments on multiple families of fine-tuned LLMs that our approach sets a new standard for detection of privacy leakage. For measuring the memorization rate of non-privately trained LLMs, our designed canaries surpass prior approaches. For example, on the Qwen2.5-0.5B model, our designed canaries achieve $49.6\\%$ TPR at $1\\%$ FPR, vastly surpassing the prior approach's $4.2\\%$ TPR at $1\\%$ FPR. Our method can be used to provide a privacy audit of $\\varepsilon \\approx 1$ for a model trained with theoretical $\\varepsilon$ of 4. To the best of our knowledge, this is the first time that a privacy audit of LLM training has achieved nontrivial auditing success in the setting where the attacker cannot train shadow models, insert gradient canaries, or access the model at every iteration.", 'abstract_zh': '当前用于大语言模型（LLMs）隐私审计的技术效果有限——它们依赖于基本的方法生成-canaries，导致薄弱的成员推断攻击，进而给出宽松的经验隐私泄露下界。我们开发了在涵盖一系列现实场景的威胁模型下比先前工作更为有效的-canaries。通过在多个微调的LLM家族上进行广泛的实验，我们表明我们的方法为隐私泄露的检测设定了新的标准。对于衡量非私有训练的LLMs的回忆率，我们设计的-canaries超过先前的方法，例如在Qwen2.5-0.5B模型上，我们设计的-canaries在1% FPR下的TPR达到49.6%，远远超过先前方法在1% FPR下的4.2% TPR。我们的方法可以用于提供一个约为ε=1的隐私审计，对于理论ε值为4的模型。据我们所知，这是首次在攻击者不能训练影子模型、插入梯度-canaries或在每次迭代中访问模型的情况下，实现非平凡的隐私审计成功率。', 'title_zh': '大型语言模型的隐私审计'}
{'arxiv_id': 'arXiv:2503.06781', 'title': 'Dr Genre: Reinforcement Learning from Decoupled LLM Feedback for Generic Text Rewriting', 'authors': 'Yufei Li, John Nham, Ganesh Jawahar, Lei Shu, David Uthus, Yun-Hsuan Sung, Chengrun Yang, Itai Rolnick, Yi Qiao, Cong Liu', 'link': 'https://arxiv.org/abs/2503.06781', 'abstract': "Generic text rewriting is a prevalent large language model (LLM) application that covers diverse real-world tasks, such as style transfer, fact correction, and email editing. These tasks vary in rewriting objectives (e.g., factual consistency vs. semantic preservation), making it challenging to develop a unified model that excels across all dimensions. Existing methods often specialize in either a single task or a specific objective, limiting their generalizability. In this work, we introduce a generic model proficient in factuality, stylistic, and conversational rewriting tasks. To simulate real-world user rewrite requests, we construct a conversational rewrite dataset, ChatRewrite, that presents ``natural''-sounding instructions, from raw emails using LLMs. Combined with other popular rewrite datasets, including LongFact for the factuality rewrite task and RewriteLM for the stylistic rewrite task, this forms a broad benchmark for training and evaluating generic rewrite models. To align with task-specific objectives, we propose Dr Genre, a Decoupled-reward learning framework for Generic rewriting, that utilizes objective-oriented reward models with a task-specific weighting. Evaluation shows that \\approach delivers higher-quality rewrites across all targeted tasks, improving objectives including instruction following (agreement), internal consistency (coherence), and minimal unnecessary edits (conciseness).", 'abstract_zh': '通用文本重写是广泛流行的大语言模型（LLM）应用，涵盖多样化的实际任务，如风格转移、事实修正和邮件编辑。这些任务在重写目标上各不相同（例如，事实一致性 vs. 语义保留），这使得开发能够在所有维度上表现出色的统一模型具有挑战性。现有方法往往专注于单一任务或特定目标，限制了其普适性。在本文中，我们引入了一个在事实性、风格和对话重写任务上均有所擅长的通用模型。为了模拟实际用户重写请求，我们构建了一个对话重写数据集ChatRewrite，该数据集使用大语言模型从原始邮件中提取出“自然”的指令。结合其他流行的重写数据集，包括LongFact用于事实性重写任务和RewriteLM用于风格重写任务，这构成了训练和评估通用重写模型的广泛基准。为了与特定任务目标对齐，我们提出了一种拆分奖励学习框架Dr Genre，该框架利用目标导向的奖励模型，并结合特定任务的权重。评估结果显示，该方法在所有目标任务上提供了更高质量的重写，提高了包括指令遵循（一致性）、内部一致性（连贯性）和最小不必要的编辑（简洁性）在内的目标。', 'title_zh': 'Dr Genre: 基于解耦大语言模型反馈的强化学习通用文本重写'}
{'arxiv_id': 'arXiv:2503.06778', 'title': 'Large Language Models Are Effective Human Annotation Assistants, But Not Good Independent Annotators', 'authors': 'Feng Gu, Zongxia Li, Carlos Rafael Colon, Benjamin Evans, Ishani Mondal, Jordan Lee Boyd-Graber', 'link': 'https://arxiv.org/abs/2503.06778', 'abstract': 'Event annotation is important for identifying market changes, monitoring breaking news, and understanding sociological trends. Although expert annotators set the gold standards, human coding is expensive and inefficient. Unlike information extraction experiments that focus on single contexts, we evaluate a holistic workflow that removes irrelevant documents, merges documents about the same event, and annotates the events. Although LLM-based automated annotations are better than traditional TF-IDF-based methods or Event Set Curation, they are still not reliable annotators compared to human experts. However, adding LLMs to assist experts for Event Set Curation can reduce the time and mental effort required for Variable Annotation. When using LLMs to extract event variables to assist expert annotators, they agree more with the extracted variables than fully automated LLMs for annotation.', 'abstract_zh': '事件标注对于识别市场变化、监控突发新闻和理解社会趋势非常重要。尽管专家标注设定标准，但人工编码成本高且效率低。与专注于单一上下文信息抽取实验不同，我们评估了一种全面的工作流程，该流程可以去除无关文档、合并关于同一事件的文档并标注事件。尽管基于LLM的自动化标注优于传统的TF-IDF方法或事件集管理，但它们在可靠性上仍无法与人工专家相比。然而，将LLM与专家结合用于事件集管理可以减少变量标注所需的时间和精力。当使用LLM提取事件变量以协助专家标注时，专家组的意见与提取的变量更为一致，优于完全自动化的LLM标注。', 'title_zh': '大型语言模型是有效的手工标注助手，但不是好的独立标注者。'}
{'arxiv_id': 'arXiv:2503.06749', 'title': 'Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models', 'authors': 'Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, Shaohui Lin', 'link': 'https://arxiv.org/abs/2503.06749', 'abstract': "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: this https URL .", 'abstract_zh': 'DeepSeek-R1-Zero通过强化学习成功展现了大规模语言模型中推理能力的涌现。受此突破的启发，我们探讨了如何利用强化学习增强大规模多模态语言模型的推理能力。然而，直接使用强化学习训练难以激活大规模多模态语言模型中的复杂推理能力，如质疑和反思，这归因于缺乏足够的高质量多模态推理数据。为了解决这一问题，我们提出了一种多模态推理大规模多模态语言模型Vision-R1，以提升多模态推理能力。具体地，我们首先通过模态桥梁和数据过滤，利用现有大规模多模态语言模型和DeepSeek-R1构建了一个无需人工注释的高质量多模态CoT数据集Vision-R1-cold，作为Vision-R1的冷启动初始化数据。为了缓解冷启动后的过度思考带来的优化挑战，我们提出了渐进性思考抑制训练（PTST）策略，并采用具有硬格式化结果奖励函数的组相对策略优化（GRPO）方法，逐步精炼模型学习正确且复杂的推理过程的能力。在10K多模态数学数据集上，我们的模型在各种多模态数学推理基准上取得了平均约6%的改进。Vision-R1-7B在广泛使用的MathVista基准上的准确率达到73.5%，仅比最好的推理模型OpenAI O1低0.4个百分点。数据集和代码将在以下链接发布：this https URL。', 'title_zh': 'Vision-R1: 在多模态大型语言模型中激励推理能力'}
{'arxiv_id': 'arXiv:2503.06734', 'title': 'Gender Encoding Patterns in Pretrained Language Model Representations', 'authors': 'Mahdi Zakizadeh, Mohammad Taher Pilehvar', 'link': 'https://arxiv.org/abs/2503.06734', 'abstract': 'Gender bias in pretrained language models (PLMs) poses significant social and ethical challenges. Despite growing awareness, there is a lack of comprehensive investigation into how different models internally represent and propagate such biases. This study adopts an information-theoretic approach to analyze how gender biases are encoded within various encoder-based architectures. We focus on three key aspects: identifying how models encode gender information and biases, examining the impact of bias mitigation techniques and fine-tuning on the encoded biases and their effectiveness, and exploring how model design differences influence the encoding of biases. Through rigorous and systematic investigation, our findings reveal a consistent pattern of gender encoding across diverse models. Surprisingly, debiasing techniques often exhibit limited efficacy, sometimes inadvertently increasing the encoded bias in internal representations while reducing bias in model output distributions. This highlights a disconnect between mitigating bias in output distributions and addressing its internal representations. This work provides valuable guidance for advancing bias mitigation strategies and fostering the development of more equitable language models.', 'abstract_zh': '性别偏差在预训练语言模型中的存在对社会和伦理提出了重大挑战。尽管已有越来越多的认识，但缺乏对不同模型内部如何表示和传播这些偏差的全面调查。本研究采用信息论的方法分析性别偏差在各种基于编码器的架构中的编码方式。我们重点关注三个方面：识别模型如何编码性别信息和偏差、检查偏见缓解技术和微调对编码偏差及其有效性的影响、以及探讨模型设计差异如何影响偏差的编码方式。通过严格的系统性研究，我们的发现揭示了不同模型在性别编码方面的持续一致模式。令人意外的是，偏见缓解技术通常表现出有限的有效性，有时会无意中在内部表示中增加偏见，同时降低模型输出分布中的偏见。这突显了在输出分布中缓解偏见与解决其内部表示之间的脱节。本研究为推进偏见缓解策略和促进更公平的语言模型的发展提供了宝贵的指导。', 'title_zh': '预训练语言模型表示中的性别编码模式'}
{'arxiv_id': 'arXiv:2503.06709', 'title': 'Delusions of Large Language Models', 'authors': 'Hongshen Xu, Zixv yang, Zichen Zhu, Kunyao Lan, Zihan Wang, Mengyue Wu, Ziwei Ji, Lu Chen, Pascale Fung, Kai Yu', 'link': 'https://arxiv.org/abs/2503.06709', 'abstract': 'Large Language Models often generate factually incorrect but plausible outputs, known as hallucinations. We identify a more insidious phenomenon, LLM delusion, defined as high belief hallucinations, incorrect outputs with abnormally high confidence, making them harder to detect and mitigate. Unlike ordinary hallucinations, delusions persist with low uncertainty, posing significant challenges to model reliability. Through empirical analysis across different model families and sizes on several Question Answering tasks, we show that delusions are prevalent and distinct from hallucinations. LLMs exhibit lower honesty with delusions, which are harder to override via finetuning or self reflection. We link delusion formation with training dynamics and dataset noise and explore mitigation strategies such as retrieval augmented generation and multi agent debating to mitigate delusions. By systematically investigating the nature, prevalence, and mitigation of LLM delusions, our study provides insights into the underlying causes of this phenomenon and outlines future directions for improving model reliability.', 'abstract_zh': '大规模语言模型常常生成事实性错误但听起来合理的输出，称为幻觉。我们鉴定出一种更为隐蔽的现象，即LLM妄想，定义为高信念幻觉，即带有异常高置信度的错误输出，使得它们更难以检测和缓解。与普通幻觉不同，妄想伴随着低不确定性，给模型可靠性带来重大挑战。通过在不同模型家族和规模上对多个问答任务进行实证分析，我们证明了妄想现象普遍存在且与幻觉不同。模型在妄想时表现出较低的诚实性，这种方法通过微调或自我反思更难克服。我们将妄想的形成与训练动力学和数据集噪音联系起来，并探索检索增强生成和多代理辩论等缓解策略以减轻妄想。通过系统地研究LLM妄想的本质、普遍性和缓解策略，我们的研究提供了对该现象背后原因的见解，并明确了提高模型可靠性的未来方向。', 'title_zh': '大型语言模型的妄想症'}
{'arxiv_id': 'arXiv:2503.06692', 'title': 'InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models', 'authors': 'Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian Shao, Yueting Zhuang', 'link': 'https://arxiv.org/abs/2503.06692', 'abstract': 'Advanced reasoning in large language models has achieved remarkable performance on challenging tasks, but the prevailing long-context reasoning paradigm faces critical limitations: quadratic computational scaling with sequence length, reasoning constrained by maximum context boundaries, and performance degradation beyond pre-training context windows. Existing approaches primarily compress reasoning chains without addressing the fundamental scaling problem. To overcome these challenges, we introduce InftyThink, a paradigm that transforms monolithic reasoning into an iterative process with intermediate summarization. By interleaving short reasoning segments with concise progress summaries, our approach enables unbounded reasoning depth while maintaining bounded computational costs. This creates a characteristic sawtooth memory pattern that significantly reduces computational complexity compared to traditional approaches. Furthermore, we develop a methodology for reconstructing long-context reasoning datasets into our iterative format, transforming OpenR1-Math into 333K training instances. Experiments across multiple model architectures demonstrate that our approach reduces computational costs while improving performance, with Qwen2.5-Math-7B showing 3-13% improvements across MATH500, AIME24, and GPQA_diamond benchmarks. Our work challenges the assumed trade-off between reasoning depth and computational efficiency, providing a more scalable approach to complex reasoning without architectural modifications.', 'abstract_zh': 'InftyThink：一种迭代式的中间总结范式，克服大规模语言模型的长上下文推理挑战', 'title_zh': 'InftyThink：打破大型语言模型长上下文推理的长度限制'}
{'arxiv_id': 'arXiv:2503.06664', 'title': 'Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets', 'authors': 'Tommaso Bendinelli, Artur Dox, Christian Holz', 'link': 'https://arxiv.org/abs/2503.06664', 'abstract': 'High-quality, error-free datasets are a key ingredient in building reliable, accurate, and unbiased machine learning (ML) models. However, real world datasets often suffer from errors due to sensor malfunctions, data entry mistakes, or improper data integration across multiple sources that can severely degrade model performance. Detecting and correcting these issues typically require tailor-made solutions and demand extensive domain expertise. Consequently, automation is challenging, rendering the process labor-intensive and tedious. In this study, we investigate whether Large Language Models (LLMs) can help alleviate the burden of manual data cleaning. We set up an experiment in which an LLM, paired with Python, is tasked with cleaning the training dataset to improve the performance of a learning algorithm without having the ability to modify the training pipeline or perform any feature engineering. We run this experiment on multiple Kaggle datasets that have been intentionally corrupted with errors. Our results show that LLMs can identify and correct erroneous entries, such as illogical values or outlier, by leveraging contextual information from other features within the same row, as well as feedback from previous iterations. However, they struggle to detect more complex errors that require understanding data distribution across multiple rows, such as trends and biases.', 'abstract_zh': '高质量、无错误的数据集是构建可靠、准确和无偏见的机器学习模型的关键。然而，由于传感器故障、数据录入错误或来自多个来源的数据整合不当，现实世界的数据集 often 患有严重降低模型性能的错误。检测和纠正这些错误通常需要量身定制的解决方案，并要求广泛的领域专业知识。因此，自动化变得具有挑战性，使过程变得劳动密集且繁琐。在本研究中，我们探讨大型语言模型（LLMs）是否能帮助减轻手动数据清洗的负担。我们建立了一个实验，在该实验中，一个 LLM 与 Python 结合使用，被任务委托清洗训练数据集以提高学习算法的性能，而不对其 training pipeline 进行任何修改或进行任何特征工程。我们在此实验中使用了多个故意添加错误的 Kaggle 数据集。结果显示，LLMs 可以通过利用同一行中其他特征的上下文信息以及从前一迭代的反馈来识别和纠正错误条目，如不合逻辑的值或离群值。然而，它们难以检测需要理解多行数据分布的更复杂的错误，如趋势和偏见。', 'title_zh': '探索LLM代理清理表格机器学习数据集'}
{'arxiv_id': 'arXiv:2503.06648', 'title': 'Enhancing NLP Robustness and Generalization through LLM-Generated Contrast Sets: A Scalable Framework for Systematic Evaluation and Adversarial Training', 'authors': 'Hender Lin', 'link': 'https://arxiv.org/abs/2503.06648', 'abstract': 'Standard NLP benchmarks often fail to capture vulnerabilities stemming from dataset artifacts and spurious correlations. Contrast sets address this gap by challenging models near decision boundaries but are traditionally labor-intensive to create and limited in diversity. This study leverages large language models to automate the generation of diverse contrast sets. Using the SNLI dataset, we created a 3,000-example contrast set to evaluate and improve model robustness. Fine-tuning on these contrast sets enhanced performance on systematically perturbed examples, maintained standard test accuracy, and modestly improved generalization to novel perturbations. This automated approach offers a scalable solution for evaluating and improving NLP models, addressing systematic generalization challenges, and advancing robustness in real-world applications.', 'abstract_zh': '标准的NLP基准常常无法捕捉数据集 artefacts 和虚假相关性导致的漏洞。对比集通过在决策边界附近挑战模型来弥补这一缺陷，但传统上创建过程耗时且多样性有限。本研究利用大语言模型来自动化生成具有多样性的对比集。使用SNLI数据集，我们创建了一个包含3000个样本的对比集以评估和提升模型的鲁棒性。在这些对比集上的微调在系统扰动样本上的性能得到了提升，保持了标准测试准确性，并适度提高了对新型扰动的泛化能力。这种自动化方法为评估和提升NLP模型、解决系统泛化挑战以及在实际应用中推进鲁棒性提供了一种可扩展的解决方案。', 'title_zh': '通过LLM生成的对比集增强NLP的 robustness和泛化能力：一种系统评估和对抗训练的可扩展框架'}
{'arxiv_id': 'arXiv:2503.06573', 'title': 'WildIFEval: Instruction Following in the Wild', 'authors': 'Gili Lior, Asaf Yehudai, Ariel Gera, Liat Ein-Dor', 'link': 'https://arxiv.org/abs/2503.06573', 'abstract': 'Recent LLMs have shown remarkable success in following user instructions, yet handling instructions with multiple constraints remains a significant challenge. In this work, we introduce WildIFEval - a large-scale dataset of 12K real user instructions with diverse, multi-constraint conditions. Unlike prior datasets, our collection spans a broad lexical and topical spectrum of constraints, in natural user prompts. We categorize these constraints into eight high-level classes to capture their distribution and dynamics in real-world scenarios. Leveraging WildIFEval, we conduct extensive experiments to benchmark the instruction-following capabilities of leading LLMs. Our findings reveal that all evaluated models experience performance degradation with an increasing number of constraints. Thus, we show that all models have a large room for improvement on such tasks. Moreover, we observe that the specific type of constraint plays a critical role in model performance. We release our dataset to promote further research on instruction-following under complex, realistic conditions.', 'abstract_zh': 'Recent LLMs在遵循用户指令方面取得了显著成功，但在处理具有多种约束的指令方面仍面临重大挑战。本文介绍WildIFEval - 一个包含12000个真实用户指令的大规模数据集，这些指令具有多样性和多约束条件。与先前的 datasets 不同，我们的收集涵盖了广泛的语言和主题约束范围，这些约束自然出现在用户提示中。我们将这些约束分为八个高层次类别，以捕捉其在现实世界场景中的分布和动态。利用WildIFEval，我们进行了广泛的实验以评估领先LLM的指令遵循能力。我们的发现表明，所有评估模型在约束数量增加时会表现出性能下降。因此，我们展示出所有模型在这类任务上仍有很大的改进空间。此外，我们观察到特定类型的约束在模型性能中起着关键作用。我们公开发布该数据集，以促进在复杂和实际条件下指令遵循的研究。', 'title_zh': 'WildIFEval: 野外的指令跟随'}
{'arxiv_id': 'arXiv:2503.06567', 'title': 'Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving', 'authors': 'Yao Cheng, Yibo Zhao, Jiapeng Zhu, Yao Liu, Xing Sun, Xiang Li', 'link': 'https://arxiv.org/abs/2503.06567', 'abstract': 'Large language models (LLMs) have demonstrated transformative potential across various domains, yet they face significant challenges in knowledge integration and complex problem reasoning, often leading to hallucinations and unreliable outputs. Retrieval-Augmented Generation (RAG) has emerged as a promising solution to enhance LLMs accuracy by incorporating external knowledge. However, traditional RAG systems struggle with processing complex relational information and multi-step reasoning, limiting their effectiveness in advanced problem-solving tasks. To address these limitations, we propose CogGRAG, a cognition inspired graph-based RAG framework, designed to improve LLMs performance in Knowledge Graph Question Answering (KGQA). Inspired by the human cognitive process of decomposing complex problems and performing self-verification, our framework introduces a three-stage methodology: decomposition, retrieval, and reasoning with self-verification. By integrating these components, CogGRAG enhances the accuracy of LLMs in complex problem solving. We conduct systematic experiments with three LLM backbones on four benchmark datasets, where CogGRAG outperforms the baselines.', 'abstract_zh': '大规模语言模型（LLMs）在各个领域展现出了变革性的潜力，但在知识整合和复杂问题推理方面面临重大挑战，经常导致虚构和不可靠的输出。检索增强生成（RAG）作为一种增强LLMs准确性的方法，通过引入外部知识已 emerges as a promising solution.然而，传统RAG系统在处理复杂关系信息和多步推理方面存在困难，限制了其在高级问题解决任务中的有效性。为了解决这些局限性，我们提出了一种基于图的认知启发式RAG框架CogGRAG，旨在提高LLMs在知识图谱问答（KGQA）中的性能。我们的框架借鉴了人类认知过程中的问题分解和自我验证机制，采用三阶段方法：分解、检索和带有自我验证的推理。通过整合这些组件，CogGRAG增强了LLMs在复杂问题解决中的准确性。我们在四个基准数据集上使用三种LLM骨干网络进行了系统的实验，其中CogGRAG优于基线方法。', 'title_zh': '基于知识图谱的人类认知启发式RAG复杂问题求解'}
{'arxiv_id': 'arXiv:2503.06525', 'title': 'From Motion Signals to Insights: A Unified Framework for Student Behavior Analysis and Feedback in Physical Education Classes', 'authors': 'Xian Gao, Jiacheng Ruan, Jingsheng Gao, Mingye Xie, Zongyun Zhang, Ting Liu, Yuzhuo Fu', 'link': 'https://arxiv.org/abs/2503.06525', 'abstract': "Analyzing student behavior in educational scenarios is crucial for enhancing teaching quality and student engagement. Existing AI-based models often rely on classroom video footage to identify and analyze student behavior. While these video-based methods can partially capture and analyze student actions, they struggle to accurately track each student's actions in physical education classes, which take place in outdoor, open spaces with diverse activities, and are challenging to generalize to the specialized technical movements involved in these settings. Furthermore, current methods typically lack the ability to integrate specialized pedagogical knowledge, limiting their ability to provide in-depth insights into student behavior and offer feedback for optimizing instructional design. To address these limitations, we propose a unified end-to-end framework that leverages human activity recognition technologies based on motion signals, combined with advanced large language models, to conduct more detailed analyses and feedback of student behavior in physical education classes. Our framework begins with the teacher's instructional designs and the motion signals from students during physical education sessions, ultimately generating automated reports with teaching insights and suggestions for improving both learning and class instructions. This solution provides a motion signal-based approach for analyzing student behavior and optimizing instructional design tailored to physical education classes. Experimental results demonstrate that our framework can accurately identify student behaviors and produce meaningful pedagogical insights.", 'abstract_zh': '基于运动信号的统一端到端框架：提升体育教学设计的学生行为分析与优化', 'title_zh': '从运动信号到洞察：体育课中学生行为分析与反馈的统一框架'}
{'arxiv_id': 'arXiv:2503.06519', 'title': 'Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation', 'authors': 'Wenhui Zhang, Huiyu Xu, Zhibo Wang, Zeqing He, Ziqi Zhu, Kui Ren', 'link': 'https://arxiv.org/abs/2503.06519', 'abstract': "Small language models (SLMs) have emerged as promising alternatives to large language models (LLMs) due to their low computational demands, enhanced privacy guarantees and comparable performance in specific domains through light-weight fine-tuning. Deploying SLMs on edge devices, such as smartphones and smart vehicles, has become a growing trend. However, the security implications of SLMs have received less attention than LLMs, particularly regarding jailbreak attacks, which is recognized as one of the top threats of LLMs by the OWASP. In this paper, we conduct the first large-scale empirical study of SLMs' vulnerabilities to jailbreak attacks. Through systematically evaluation on 63 SLMs from 15 mainstream SLM families against 8 state-of-the-art jailbreak methods, we demonstrate that 47.6% of evaluated SLMs show high susceptibility to jailbreak attacks (ASR > 40%) and 38.1% of them can not even resist direct harmful query (ASR > 50%). We further analyze the reasons behind the vulnerabilities and identify four key factors: model size, model architecture, training datasets and training techniques. Moreover, we assess the effectiveness of three prompt-level defense methods and find that none of them achieve perfect performance, with detection accuracy varying across different SLMs and attack methods. Notably, we point out that the inherent security awareness play a critical role in SLM security, and models with strong security awareness could timely terminate unsafe response with little reminder. Building upon the findings, we highlight the urgent need for security-by-design approaches in SLM development and provide valuable insights for building more trustworthy SLM ecosystem.", 'abstract_zh': '小语言模型（SLMs）对大型语言模型（LLMs）的低计算需求、增强的隐私保障以及在特定领域通过轻量级微调表现出的可相比拟性能的前景，使之成为了有前途的替代方案。将SLMs部署在边缘设备（如智能手机和智能车辆）上已成为一种增长的趋势。然而，SLMs的安全影响并未像LLMs那样受到广泛关注，特别是在关于逃逸攻击方面，OWASP已将其认定为LLMs的首要威胁之一。在本文中，我们首次进行了大规模的实证研究，探讨SLMs对逃逸攻击的脆弱性。通过对来自15个主流小语言模型家族的63个SLMs进行系统性评估，对比8种最先进的逃逸攻击方法，我们证明了47.6%的评估SLMs对逃逸攻击显示出高度易感性（ASR > 40%），甚至有38.1%的SLMs无法抵御直接有害查询（ASR > 50%）。我们进一步分析了这些脆弱性的原因，并确定了四个关键因素：模型大小、模型架构、训练数据集和训练技术。此外，我们评估了三种提示级防御方法的有效性，发现这三种方法均未实现完美的性能，其检测准确率在不同的SLMs和攻击方法之间存在差异。我们指出，固有的安全意识在SLMs安全中扮演着至关重要的角色，拥有强烈安全意识的模型能够及时终止不安全的响应，且几乎不需要提醒。基于上述发现，我们强调了在SLMs开发中采用设计即安全方法的迫切需求，并为构建更可信赖的SLM生态系统提供了宝贵的见解。', 'title_zh': '小型语言模型能可靠地抵御 Jailbreak 攻击吗？一项全面评估'}
{'arxiv_id': 'arXiv:2503.06518', 'title': 'Towards Superior Quantization Accuracy: A Layer-sensitive Approach', 'authors': 'Feng Zhang, Yanbin Liu, Weihua Li, Jie Lv, Xiaodan Wang, Quan Bai', 'link': 'https://arxiv.org/abs/2503.06518', 'abstract': 'Large Vision and Language Models have exhibited remarkable human-like intelligence in tasks such as natural language comprehension, problem-solving, logical reasoning, and knowledge retrieval. However, training and serving these models require substantial computational resources, posing a significant barrier to their widespread application and further research. To mitigate this challenge, various model compression techniques have been developed to reduce computational requirements. Nevertheless, existing methods often employ uniform quantization configurations, failing to account for the varying difficulties across different layers in quantizing large neural network models. This paper tackles this issue by leveraging layer-sensitivity features, such as activation sensitivity and weight distribution Kurtosis, to identify layers that are challenging to quantize accurately and allocate additional memory budget. The proposed methods, named SensiBoost and KurtBoost, respectively, demonstrate notable improvement in quantization accuracy, achieving up to 9% lower perplexity with only a 2% increase in memory budget on LLama models compared to the baseline.', 'abstract_zh': '大型视觉与语言模型在自然语言理解、问题解决、逻辑推理和知识检索等任务中表现出令人瞩目的人类智能水平。然而，训练和部署这些模型需要大量的计算资源，成为其广泛应用和进一步研究的重要障碍。为缓解这一问题，开发了多种模型压缩技术以减少计算需求。尽管现有方法通常采用统一的量化配置，未能考虑到量化大型神经网络模型时各层之间不同的困难程度。本文通过利用层敏感特征（如激活敏感性和权重分布峰度）识别难以准确量化的层，并分配额外的内存预算来应对这一问题。所提出的SensiBoost和KurtBoost方法分别在量化精度上取得了显著改善，与基线相比，在LLama模型上仅增加2%的内存预算即可实现最高9%的困惑度下降。', 'title_zh': '向量化精度更优迈进：一种层敏感方法'}
{'arxiv_id': 'arXiv:2503.06514', 'title': 'GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks', 'authors': 'Haoqiang Kang, Enna Sachdeva, Piyush Gupta, Sangjae Bae, Kwonjoon Lee', 'link': 'https://arxiv.org/abs/2503.06514', 'abstract': 'Vision-Language Models (VLMs) have recently shown promising advancements in sequential decision-making tasks through task-specific fine-tuning. However, common fine-tuning methods, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO), present notable limitations: SFT assumes Independent and Identically Distributed (IID) data, while PPO focuses on maximizing cumulative rewards. These limitations often restrict solution diversity and hinder generalization in multi-step reasoning tasks. To address these challenges, we introduce a novel framework, GFlowVLM, a framework that fine-tune VLMs using Generative Flow Networks (GFlowNets) to promote generation of diverse solutions for complex reasoning tasks. GFlowVLM models the environment as a non-Markovian decision process, allowing it to capture long-term dependencies essential for real-world applications. It takes observations and task descriptions as inputs to prompt chain-of-thought (CoT) reasoning which subsequently guides action selection. We use task based rewards to fine-tune VLM with GFlowNets. This approach enables VLMs to outperform prior fine-tuning methods, including SFT and RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex tasks such as card games (NumberLine, BlackJack) and embodied planning tasks (ALFWorld), showing enhanced training efficiency, solution diversity, and stronger generalization capabilities across both in-distribution and out-of-distribution scenarios.', 'abstract_zh': 'Vision-Language模型（VLMs）通过任务特定微调在序列决策任务中展示了有希望的进步。然而，常见的微调方法，如监督微调（SFT）和强化学习技术（如近端策略优化PPO），存在显著限制：SFT假设独立同分布（IID）数据，而PPO专注于最大化累积奖励。这些限制常限制解的多样性，妨碍多步推理任务中的泛化能力。为解决这些挑战，我们提出了一种新的框架，GFlowVLM，该框架利用生成流网络（GFlowNets）微调VLMs，以促进复杂推理任务中多样解方案的生成。GFlowVLM将环境建模为非马尔可夫决策过程，使其能够捕捉到对现实应用至关重要的长期依赖关系。它通过输入观察和任务描述来提示因果推理（CoT），进而引导动作选择。我们使用基于任务的奖励对VLMs进行GFlowNets微调。该方法使VLMs能够超越先前的微调方法，包括SFT和RL。实证结果表明，GFlowVLM在复杂任务（如卡片游戏（NumberLine、BlackJack）和身体计划任务（ALFWorld））中表现出有效的训练效率、解方案多样性以及更强的泛化能力，无论是在分布内还是分布外场景中。', 'title_zh': 'GFlowVLM: 生成流网络增强视觉-语言模型的多步推理'}
{'arxiv_id': 'arXiv:2503.06474', 'title': 'HuixiangDou2: A Robustly Optimized GraphRAG Approach', 'authors': 'Huanjun Kong, Zhefan Wang, Chenyang Wang, Zhe Ma, Nanqing Dong', 'link': 'https://arxiv.org/abs/2503.06474', 'abstract': 'Large Language Models (LLMs) perform well on familiar queries but struggle with specialized or emerging topics. Graph-based Retrieval-Augmented Generation (GraphRAG) addresses this by structuring domain knowledge as a graph for dynamic retrieval. However, existing pipelines involve complex engineering workflows, making it difficult to isolate the impact of individual components. Evaluating retrieval effectiveness is also challenging due to dataset overlap with LLM pretraining data. In this work, we introduce HuixiangDou2, a robustly optimized GraphRAG framework. Specifically, we leverage the effectiveness of dual-level retrieval and optimize its performance in a 32k context for maximum precision, and compare logic-based retrieval and dual-level retrieval to enhance overall functionality. Our implementation includes comparative experiments on a test set, where Qwen2.5-7B-Instruct initially underperformed. With our approach, the score improved significantly from 60 to 74.5, as illustrated in the Figure. Experiments on domain-specific datasets reveal that dual-level retrieval enhances fuzzy matching, while logic-form retrieval improves structured reasoning. Furthermore, we propose a multi-stage verification mechanism to improve retrieval robustness without increasing computational cost. Empirical results show significant accuracy gains over baselines, highlighting the importance of adaptive retrieval. To support research and adoption, we release HuixiangDou2 as an open-source resource this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在熟悉查询上表现良好，但在处理专门化或新兴主题时存在困难。基于图的检索增强生成（GraphRAG）通过将领域知识构建成图来进行动态检索，以解决这一问题。然而，现有流水线涉及复杂的工程工作流，使得难以隔离各个组件的影响。由于数据集与LLM预训练数据存在重叠，检索效果的评估也颇具挑战性。在本文中，我们介绍了经过 robust 优化的 GraphRAG 框架 HuixiangDou2。具体而言，我们利用双级检索的有效性，在32k上下文长度下优化其性能以实现最高精密度，并对比基于逻辑的检索和双级检索以增强整体功能。我们的实现包括对测试集的对比实验，其中 Qwen2.5-7B-Instruct 初始表现不佳。通过我们的方法，分数显著提升，从60分提高到74.5分，如图所示。针对特定领域的数据集的实验表明，双级检索增强了模糊匹配，而逻辑表达式检索改善了结构化推理。此外，我们提出了一种多阶段验证机制，以在不增加计算成本的情况下提高检索的鲁棒性。实验结果表明，在基线之上取得了显着的准确率提升，突显了适应性检索的重要性。为了支持研究和应用，我们开源了 HuixiangDou2，详情请参见：this https URL。', 'title_zh': 'HuixiangDou2：一个鲁棒优化的GraphRAG方法'}
{'arxiv_id': 'arXiv:2503.06433', 'title': 'Seesaw: High-throughput LLM Inference via Model Re-sharding', 'authors': 'Qidong Su, Wei Zhao, Xin Li, Muralidhar Andoorveedu, Chenhao Jiang, Zhanda Zhu, Kevin Song, Christina Giannoula, Gennady Pekhimenko', 'link': 'https://arxiv.org/abs/2503.06433', 'abstract': 'To improve the efficiency of distributed large language model (LLM) inference, various parallelization strategies, such as tensor and pipeline parallelism, have been proposed. However, the distinct computational characteristics inherent in the two stages of LLM inference-prefilling and decoding-render a single static parallelization strategy insufficient for the effective optimization of both stages. In this work, we present Seesaw, an LLM inference engine optimized for throughput-oriented tasks. The key idea behind Seesaw is dynamic model re-sharding, a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages, thereby maximizing throughput at both phases. To mitigate re-sharding overhead and optimize computational efficiency, we employ tiered KV cache buffering and transition-minimizing scheduling. These approaches work synergistically to reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. Our evaluation demonstrates that Seesaw achieves a throughput increase of up to 1.78x (1.36x on average) compared to vLLM, the most widely used state-of-the-art LLM inference engine.', 'abstract_zh': '为了提高分布式大型语言模型（LLM）推理的效率，提出了诸如张量并行和管道并行等各种并行化策略。然而，LLM推理的两个阶段——预填充和解码——固有的不同计算特性使得单一静态并行化策略无法有效优化这两个阶段。本文提出Seesaw，一种针对吞吐量优化的LLM推理引擎。Seesaw的核心思想是动态模型重分片，该技术使并行化策略在各阶段之间的动态重新配置成为可能，从而在两个阶段中最大化吞吐量。为减轻重分片开销并优化计算效率，我们采用了分层KV缓存缓冲和最小转换调度。这些方法协同工作，减少了频繁阶段转换带来的开销，同时确保了最大的批处理效率。我们的评估表明，Seesaw相比最广泛使用的最先进的LLM推理引擎vLLM，吞吐量提升了1.78倍（平均提升1.36倍）。', 'title_zh': 'Seesaw: 高 throughput LLM 推理通过模型重新分割'}
{'arxiv_id': 'arXiv:2503.06430', 'title': 'Graph Retrieval-Augmented LLM for Conversational Recommendation Systems', 'authors': 'Zhangchi Qiu, Linhao Luo, Zicheng Zhao, Shirui Pan, Alan Wee-Chung Liew', 'link': 'https://arxiv.org/abs/2503.06430', 'abstract': "Conversational Recommender Systems (CRSs) have emerged as a transformative paradigm for offering personalized recommendations through natural language dialogue. However, they face challenges with knowledge sparsity, as users often provide brief, incomplete preference statements. While recent methods have integrated external knowledge sources to mitigate this, they still struggle with semantic understanding and complex preference reasoning. Recent Large Language Models (LLMs) demonstrate promising capabilities in natural language understanding and reasoning, showing significant potential for CRSs. Nevertheless, due to the lack of domain knowledge, existing LLM-based CRSs either produce hallucinated recommendations or demand expensive domain-specific training, which largely limits their applicability. In this work, we present G-CRS (Graph Retrieval-Augmented Large Language Model for Conversational Recommender Systems), a novel training-free framework that combines graph retrieval-augmented generation and in-context learning to enhance LLMs' recommendation capabilities. Specifically, G-CRS employs a two-stage retrieve-and-recommend architecture, where a GNN-based graph reasoner first identifies candidate items, followed by Personalized PageRank exploration to jointly discover potential items and similar user interactions. These retrieved contexts are then transformed into structured prompts for LLM reasoning, enabling contextually grounded recommendations without task-specific training. Extensive experiments on two public datasets show that G-CRS achieves superior recommendation performance compared to existing methods without requiring task-specific training.", 'abstract_zh': '基于图检索增强的大语言模型的对话推荐系统（G-CRS）', 'title_zh': '基于图检索增强的LLM在对话推荐系统中的应用'}
{'arxiv_id': 'arXiv:2503.06413', 'title': 'Swift Hydra: Self-Reinforcing Generative Framework for Anomaly Detection with Multiple Mamba Models', 'authors': 'Nguyen Do, Truc Nguyen, Malik Hassanaly, Raed Alharbi, Jung Taek Seo, My T. Thai', 'link': 'https://arxiv.org/abs/2503.06413', 'abstract': "Despite a plethora of anomaly detection models developed over the years, their ability to generalize to unseen anomalies remains an issue, particularly in critical systems. This paper aims to address this challenge by introducing Swift Hydra, a new framework for training an anomaly detection method based on generative AI and reinforcement learning (RL). Through featuring an RL policy that operates on the latent variables of a generative model, the framework synthesizes novel and diverse anomaly samples that are capable of bypassing a detection model. These generated synthetic samples are, in turn, used to augment the detection model, further improving its ability to handle challenging anomalies. Swift Hydra also incorporates Mamba models structured as a Mixture of Experts (MoE) to enable scalable adaptation of the number of Mamba experts based on data complexity, effectively capturing diverse feature distributions without increasing the model's inference time. Empirical evaluations on ADBench benchmark demonstrate that Swift Hydra outperforms other state-of-the-art anomaly detection models while maintaining a relatively short inference time. From these results, our research highlights a new and auspicious paradigm of integrating RL and generative AI for advancing anomaly detection.", 'abstract_zh': '尽管多年来已经开发出众多异常检测模型，但这些模型在面对未见过的异常时的泛化能力仍存在问题，特别是在关键系统中。本文旨在通过引入基于生成AI和强化学习（RL）的新型异常检测方法训练框架Swift Hydra来应对这一挑战。该框架通过在其生成模型的潜在变量上操作的RL策略，合成了新颖且多样的异常样本，能够绕过检测模型。生成的合成样本随后被用来增强检测模型，进一步提高其处理复杂异常的能力。Swift Hydra还结合了基于Mixture of Experts（MoE）结构的Mamba模型，以根据数据复杂性可扩展地调整Mamba专家的数量，有效地捕捉多样化的特征分布而不增加模型的推理时间。在ADBench基准上的实证评估表明，Swift Hydra在保持相对短的推理时间的同时，优于其他最先进的异常检测模型。我们的研究结果突显了一种新的并充满希望的RL与生成AI结合的方法范式，以推动异常检测的发展。', 'title_zh': 'Swift Hydra：自强化生成框架，基于多个Mamba模型的异常检测'}
{'arxiv_id': 'arXiv:2503.06330', 'title': 'States of LLM-generated Texts and Phase Transitions between them', 'authors': 'Nikolay Mikhaylovskiy', 'link': 'https://arxiv.org/abs/2503.06330', 'abstract': 'It is known for some time that autocorrelations of words in human-written texts decay according to a power law. Recent works have also shown that the autocorrelations decay in texts generated by LLMs is qualitatively different from the literary texts. Solid state physics tie the autocorrelations decay laws to the states of matter. In this work, we empirically demonstrate that, depending on the temperature parameter, LLMs can generate text that can be classified as solid, critical state or gas.', 'abstract_zh': '已知人类撰写的文本中单词的自相关性按照幂律衰减。近期的研究还表明，由大规模语言模型(LLMs)生成的文本的自相关性衰减与文学文本在定性上有显著差异。固体物理学将自相关性衰减规律与物质状态联系起来。本研究通过实验证明，在不同的温度参数下，LLMs可以生成可归类为固体、临界状态或气体的文本。', 'title_zh': 'LLM生成文本的状态及其之间相变阶段'}
{'arxiv_id': 'arXiv:2503.06269', 'title': 'Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models', 'authors': 'Thomas Winninger, Boussad Addad, Katarzyna Kapusta', 'link': 'https://arxiv.org/abs/2503.06269', 'abstract': "Traditional white-box methods for creating adversarial perturbations against LLMs typically rely only on gradient computation from the targeted model, ignoring the internal mechanisms responsible for attack success or failure. Conversely, interpretability studies that analyze these internal mechanisms lack practical applications beyond runtime interventions. We bridge this gap by introducing a novel white-box approach that leverages mechanistic interpretability techniques to craft practical adversarial inputs. Specifically, we first identify acceptance subspaces - sets of feature vectors that do not trigger the model's refusal mechanisms - then use gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks. This targeted approach significantly reduces computation cost, achieving attack success rates of 80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5 within minutes or even seconds, compared to existing techniques that often fail or require hours of computation. We believe this approach opens a new direction for both attack research and defense development. Furthermore, it showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility. The code and generated datasets are available at this https URL.", 'abstract_zh': '一种结合机械可解释性的白盒方法以生成针对大语言模型的实用对抗输入', 'title_zh': '使用机理可解释性针对大规模语言模型构建对抗性攻击'}
{'arxiv_id': 'arXiv:2503.06263', 'title': 'Critical Foreign Policy Decisions (CFPD)-Benchmark: Measuring Diplomatic Preferences in Large Language Models', 'authors': 'Benjamin Jensen, Ian Reynolds, Yasir Atalan, Michael Garcia, Austin Woo, Anthony Chen, Trevor Howarth', 'link': 'https://arxiv.org/abs/2503.06263', 'abstract': 'As national security institutions increasingly integrate Artificial Intelligence (AI) into decision-making and content generation processes, understanding the inherent biases of large language models (LLMs) is crucial. This study presents a novel benchmark designed to evaluate the biases and preferences of seven prominent foundation models-Llama 3.1 8B Instruct, Llama 3.1 70B Instruct, GPT-4o, Gemini 1.5 Pro-002, Mixtral 8x22B, Claude 3.5 Sonnet, and Qwen2 72B-in the context of international relations (IR). We designed a bias discovery study around core topics in IR using 400-expert crafted scenarios to analyze results from our selected models. These scenarios focused on four topical domains including: military escalation, military and humanitarian intervention, cooperative behavior in the international system, and alliance dynamics. Our analysis reveals noteworthy variation among model recommendations based on scenarios designed for the four tested domains. Particularly, Qwen2 72B, Gemini 1.5 Pro-002 and Llama 3.1 8B Instruct models offered significantly more escalatory recommendations than Claude 3.5 Sonnet and GPT-4o models. All models exhibit some degree of country-specific biases, often recommending less escalatory and interventionist actions for China and Russia compared to the United States and the United Kingdom. These findings highlight the necessity for controlled deployment of LLMs in high-stakes environments, emphasizing the need for domain-specific evaluations and model fine-tuning to align with institutional objectives.', 'abstract_zh': '随着国家安全机构越来越多地将人工智能（AI）整合进决策和内容生成过程中，理解大型语言模型（LLMs）固有的偏见至关重要。本研究提出了一种新的基准测试，旨在评估七种 prominant 基础模型——Llama 3.1 8B Instruct、Llama 3.1 70B Instruct、GPT-4、Gemini 1.5 Pro-002、Mixtral 8x22B、Claude 3.5 Sonnet 和 Qwen2 72B——在国际关系（IR）领域的偏见和偏好。我们围绕国际关系的核心话题设计了一个偏见发现研究，使用400个专家设计的场景来分析选定模型的结果。这些场景集中在四个主题领域，包括军事升级、军事和人道主义干预、国际体系中的合作行为以及联盟动态。我们的分析表明，基于为四个测试领域设计的场景，模型建议之间存在显著差异。特别是，Qwen2 72B、Gemini 1.5 Pro-002 和 Llama 3.1 8B Instruct 模型比 Claude 3.5 Sonnet 和 GPT-4 模型提供了更多的升级建议。所有模型都表现出某种程度的国家特定偏见，经常建议对中国的干预行动比对美国和英国采取的行动更不升级。这些发现突显了在高风险环境中受控部署LLMs的必要性，强调了领域特定评估和模型微调以实现机构目标的重要性。', 'title_zh': '大型语言模型中衡量外交偏好的一种基准：关键外国政策决策(CFPD)'}
{'arxiv_id': 'arXiv:2503.06260', 'title': 'From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models', 'authors': 'Muzhi Dai, Jiashuo Sun, Zhiyuan Zhao, Shixuan Liu, Rui Li, Junyu Gao, Xuelong Li', 'link': 'https://arxiv.org/abs/2503.06260', 'abstract': 'Aligning large vision-language models (LVLMs) with human preferences is challenging due to the scarcity of fine-grained, high-quality, and multimodal preference data without human annotations. Existing methods relying on direct distillation often struggle with low-confidence data, leading to suboptimal performance. To address this, we propose CAREVL, a novel method for preference reward modeling by reliably using both high- and low-confidence data. First, a cluster of auxiliary expert models (textual reward models) innovatively leverages image captions as weak supervision signals to filter high-confidence data. The high-confidence data are then used to fine-tune the LVLM. Second, low-confidence data are used to generate diverse preference samples using the fine-tuned LVLM. These samples are then scored and selected to construct reliable chosen-rejected pairs for further training. CAREVL achieves performance improvements over traditional distillation-based methods on VL-RewardBench and MLLM-as-a-Judge benchmark, demonstrating its effectiveness. The code will be released soon.', 'abstract_zh': '基于可靠利用高低置信度数据的偏好奖励建模方法CAREVL', 'title_zh': '从字幕到奖励：利用大型语言模型专家增强视觉-语言模型的奖励建模（CAREVL）'}
{'arxiv_id': 'arXiv:2503.06252', 'title': 'Can Atomic Step Decomposition Enhance the Self-structured Reasoning of Multimodal Large Models?', 'authors': 'Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Kaixin Cai, Yiyang Yin, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, Yihan Zeng, Yu-Jie Yuan, Jianhua Han, Lanqing Hong, Hang Xu, Xiaodan Liang', 'link': 'https://arxiv.org/abs/2503.06252', 'abstract': 'In this paper, we address the challenging task of multimodal mathematical reasoning by incorporating the ability of "slow thinking" into multimodal large language models (MLLMs). Our core idea is that different levels of reasoning abilities can be combined dynamically to tackle questions with different complexity. To this end, we propose a paradigm of Self-structured Chain of Thought (SCoT), which is composed of minimal semantic atomic steps. Different from existing methods that rely on structured templates or free-form paradigms, our method can not only generate cognitive CoT structures for various complex tasks but also mitigates the phenomenon of overthinking. To introduce structured reasoning capabilities into visual understanding models, we further design a novel AtomThink framework with four key modules, including (i) a data engine to generate high-quality multimodal reasoning paths; (ii) a supervised fine-tuning process with serialized inference data; (iii) a policy-guided multi-turn inference method; and (iv) an atomic capability metric to evaluate the single step utilization rate. We conduct extensive experiments to show that the proposed AtomThink significantly improves the performance of baseline MLLMs, achieving more than 10\\% average accuracy gains on MathVista and MathVerse. Compared to state-of-the-art structured CoT approaches, our method not only achieves higher accuracy but also improves data utilization by 5 times and boosts inference efficiency by 85.3\\%. Our code is now public available in this https URL.', 'abstract_zh': '基于“慢思考”能力的多模态数学推理研究：AtomThink框架及其应用', 'title_zh': '原子步骤分解能否增强 multimodal 大模型的自我结构化推理能力？'}
{'arxiv_id': 'arXiv:2503.06238', 'title': 'Image is All You Need: Towards Efficient and Effective Large Language Model-Based Recommender Systems', 'authors': 'Kibum Kim, Sein Kim, Hongseok Kang, Jiwan Kim, Heewoong Noh, Yeonjun In, Kanghoon Yoon, Jinoh Oh, Chanyoung Park', 'link': 'https://arxiv.org/abs/2503.06238', 'abstract': 'Large Language Models (LLMs) have recently emerged as a powerful backbone for recommender systems. Existing LLM-based recommender systems take two different approaches for representing items in natural language, i.e., Attribute-based Representation and Description-based Representation. In this work, we aim to address the trade-off between efficiency and effectiveness that these two approaches encounter, when representing items consumed by users. Based on our interesting observation that there is a significant information overlap between images and descriptions associated with items, we propose a novel method, Image is all you need for LLM-based Recommender system (I-LLMRec). Our main idea is to leverage images as an alternative to lengthy textual descriptions for representing items, aiming at reducing token usage while preserving the rich semantic information of item descriptions. Through extensive experiments, we demonstrate that I-LLMRec outperforms existing methods in both efficiency and effectiveness by leveraging images. Moreover, a further appeal of I-LLMRec is its ability to reduce sensitivity to noise in descriptions, leading to more robust recommendations.', 'abstract_zh': '基于图像的大语言模型推荐系统（I-LLMRec）', 'title_zh': '图像即所需： Toward Efficient and Effective Large Language Model-Based Recommender Systems'}
{'arxiv_id': 'arXiv:2503.06204', 'title': 'CUPCase: Clinically Uncommon Patient Cases and Diagnoses Dataset', 'authors': 'Oriel Perets, Ofir Ben Shoham, Nir Grinberg, Nadav Rappoport', 'link': 'https://arxiv.org/abs/2503.06204', 'abstract': "Medical benchmark datasets significantly contribute to developing Large Language Models (LLMs) for medical knowledge extraction, diagnosis, summarization, and other uses. Yet, current benchmarks are mainly derived from exam questions given to medical students or cases described in the medical literature, lacking the complexity of real-world patient cases that deviate from classic textbook abstractions. These include rare diseases, uncommon presentations of common diseases, and unexpected treatment responses. Here, we construct Clinically Uncommon Patient Cases and Diagnosis Dataset (CUPCase) based on 3,562 real-world case reports from BMC, including diagnoses in open-ended textual format and as multiple-choice options with distractors. Using this dataset, we evaluate the ability of state-of-the-art LLMs, including both general-purpose and Clinical LLMs, to identify and correctly diagnose a patient case, and test models' performance when only partial information about cases is available. Our findings show that general-purpose GPT-4o attains the best performance in both the multiple-choice task (average accuracy of 87.9%) and the open-ended task (BERTScore F1 of 0.764), outperforming several LLMs with a focus on the medical domain such as Meditron-70B and MedLM-Large. Moreover, GPT-4o was able to maintain 87% and 88% of its performance with only the first 20% of tokens of the case presentation in multiple-choice and free text, respectively, highlighting the potential of LLMs to aid in early diagnosis in real-world cases. CUPCase expands our ability to evaluate LLMs for clinical decision support in an open and reproducible manner.", 'abstract_zh': '临床罕见患者病例和诊断数据集（CUPCase）显著促进了大型语言模型（LLMs）在医学知识提取、诊断、总结以及其它用途的发展。然而，现有的基准主要来源于医学学生的考试题目或文献中描述的病例，缺乏真实世界患者病例的复杂性，这些病例偏离了经典教科书的抽象描述。这些包括罕见疾病、常见疾病的不寻常表现形式以及意料之外的治疗反应。在此基础上，我们构建了基于3,562份真实世界病例报告的临床罕见患者病例和诊断数据集（CUPCase），包括开放文本格式的诊断和多项选择题形式的诊断选项及干扰项。使用该数据集，我们评估了最先进的大型语言模型（包括通用型和临床型）识别和准确诊断患者病例的能力，并测试了在仅有病例部分信息的情况下模型的性能。研究结果表明，通用型GPT-4o在多项选择任务（平均准确率为87.9%）和开放文本任务（BERTScore F1值为0.764）中表现出最佳性能，优于多个专注于医学领域的模型如Meditron-70B和MedLM-Large。此外，GPT-4o在仅使用病例呈现的前20%词元的情况下，多项选择任务和自由文本任务中的性能分别保持在87%和88%，这突显了大型语言模型在实际临床诊断早期辅助中的潜力。CUPCase以开放和可重现的方式扩展了我们评估用于临床决策支持的大型语言模型的能力。', 'title_zh': 'CUPCase: 临床不常见患者病例和诊断数据集'}
{'arxiv_id': 'arXiv:2503.06184', 'title': 'Sample-aware Adaptive Structured Pruning for Large Language Models', 'authors': 'Jun Kong, Xinge Ma, Jin Wang, Xuejie Zhang', 'link': 'https://arxiv.org/abs/2503.06184', 'abstract': 'Large language models (LLMs) have achieved outstanding performance in natural language processing, but enormous model sizes and high computational costs limit their practical deployment. Structured pruning can effectively reduce the resource demands for deployment by removing redundant model parameters. However, the randomly selected calibration data and fixed single importance estimation metrics in existing structured pruning methods lead to degraded performance of pruned models. This study introduces AdaPruner, a sample-aware adaptive structured pruning framework for LLMs, aiming to optimize the calibration data and importance estimation metrics in the structured pruning process. Specifically, AdaPruner effectively removes redundant parameters from LLMs by constructing a structured pruning solution space and then employing Bayesian optimization to adaptively search for the optimal calibration data and importance estimation metrics. Experimental results show that the AdaPruner outperforms existing structured pruning methods on a family of LLMs with varying pruning ratios, demonstrating its applicability and robustness. Remarkably, at a 20\\% pruning ratio, the model pruned with AdaPruner maintains 97\\% of the performance of the unpruned model.', 'abstract_zh': 'Large语言模型（LLMs）在自然语言处理领域取得了出色 performance，但巨大的模型规模和高昂的计算成本限制了其实际部署。结构化剪枝可以通过移除冗余模型参数来有效降低部署所需的资源需求。然而，现有结构化剪枝方法中随机选择的校准数据和固定的单一重要性评估指标导致剪枝模型性能下降。本研究引入了AdaPruner，一种样本感知的自适应结构化剪枝框架，旨在优化结构化剪枝过程中的校准数据和重要性评估指标。具体而言，AdaPruner通过构建结构化剪枝解空间，结合贝叶斯优化自适应搜索最优校准数据和重要性评估指标，有效地从LLMs中移除了冗余参数。实验结果表明，AdaPruner在不同剪枝比例的LLMs家族中优于现有结构化剪枝方法，展示了其适用性和鲁棒性。特别地，在20%的剪枝比例下，使用AdaPruner剪枝的模型保持了97%的未剪枝模型的 performance。', 'title_zh': '面向样本的自适应结构剪枝方法及其在大型语言模型中的应用'}
{'arxiv_id': 'arXiv:2503.06175', 'title': 'Minion Gated Recurrent Unit for Continual Learning', 'authors': 'Abdullah M. Zyarah, Dhireesha Kudithipudi', 'link': 'https://arxiv.org/abs/2503.06175', 'abstract': 'The increasing demand for continual learning in sequential data processing has led to progressively complex training methodologies and larger recurrent network architectures. Consequently, this has widened the knowledge gap between continual learning with recurrent neural networks (RNNs) and their ability to operate on devices with limited memory and compute. To address this challenge, we investigate the effectiveness of simplifying RNN architectures, particularly gated recurrent unit (GRU), and its impact on both single-task and multitask sequential learning. We propose a new variant of GRU, namely the minion recurrent unit (MiRU). MiRU replaces conventional gating mechanisms with scaling coefficients to regulate dynamic updates of hidden states and historical context, reducing computational costs and memory requirements. Despite its simplified architecture, MiRU maintains performance comparable to the standard GRU while achieving 2.90x faster training and reducing parameter usage by 2.88x, as demonstrated through evaluations on sequential image classification and natural language processing benchmarks. The impact of model simplification on its learning capacity is also investigated by performing continual learning tasks with a rehearsal-based strategy and global inhibition. We find that MiRU demonstrates stable performance in multitask learning even when using only rehearsal, unlike the standard GRU and its variants. These features position MiRU as a promising candidate for edge-device applications.', 'abstract_zh': '持续学习在序列数据处理中日益增长的需求推动了日益复杂的训练方法和更大规模的循环网络架构。这导致了使用循环神经网络（RNN）进行持续学习与它们在有限内存和计算能力设备上的操作能力之间的知识差距不断扩大。为了解决这一挑战，我们研究了简化RNN架构的有效性，特别是门控循环单元（GRU），及其对单任务和多任务序列学习的影响。我们提出了一种新的GRU变体，即门徒循环单元（MiRU）。MiRU通过使用缩放系数替代传统的门控机制来调节隐藏状态和历史上下文的动态更新，从而降低计算成本和内存需求。尽管具有简化架构，MiRU在序列图像分类和自然语言处理基准测试中仍能保持与标准GRU相当的性能，同时训练速度提高2.90倍，参数使用量减少2.88倍。通过使用基于复查的策略和全局抑制进行持续学习任务，我们还探讨了模型简化对学习能力的影响。我们发现，即使仅使用复查，MiRU在多任务学习中的性能也表现出稳定的表现，与标准GRU及其变体不同。这些特征使MiRU成为边缘设备应用的有前途的候选者。', 'title_zh': 'Minion门控循环单元 for 连续学习'}
{'arxiv_id': 'arXiv:2503.06074', 'title': 'Towards Conversational AI for Disease Management', 'authors': 'Anil Palepu, Valentin Liévin, Wei-Hung Weng, Khaled Saab, David Stutz, Yong Cheng, Kavita Kulkarni, S. Sara Mahdavi, Joëlle Barral, Dale R. Webster, Katherine Chou, Avinatan Hassidim, Yossi Matias, James Manyika, Ryutaro Tanno, Vivek Natarajan, Adam Rodman, Tao Tu, Alan Karthikesalingam, Mike Schaekermann', 'link': 'https://arxiv.org/abs/2503.06074', 'abstract': "While large language models (LLMs) have shown promise in diagnostic dialogue, their capabilities for effective management reasoning - including disease progression, therapeutic response, and safe medication prescription - remain under-explored. We advance the previously demonstrated diagnostic capabilities of the Articulate Medical Intelligence Explorer (AMIE) through a new LLM-based agentic system optimised for clinical management and dialogue, incorporating reasoning over the evolution of disease and multiple patient visit encounters, response to therapy, and professional competence in medication prescription. To ground its reasoning in authoritative clinical knowledge, AMIE leverages Gemini's long-context capabilities, combining in-context retrieval with structured reasoning to align its output with relevant and up-to-date clinical practice guidelines and drug formularies. In a randomized, blinded virtual Objective Structured Clinical Examination (OSCE) study, AMIE was compared to 21 primary care physicians (PCPs) across 100 multi-visit case scenarios designed to reflect UK NICE Guidance and BMJ Best Practice guidelines. AMIE was non-inferior to PCPs in management reasoning as assessed by specialist physicians and scored better in both preciseness of treatments and investigations, and in its alignment with and grounding of management plans in clinical guidelines. To benchmark medication reasoning, we developed RxQA, a multiple-choice question benchmark derived from two national drug formularies (US, UK) and validated by board-certified pharmacists. While AMIE and PCPs both benefited from the ability to access external drug information, AMIE outperformed PCPs on higher difficulty questions. While further research would be needed before real-world translation, AMIE's strong performance across evaluations marks a significant step towards conversational AI as a tool in disease management.", 'abstract_zh': '尽管大型语言模型（LLMs）在诊断对话中展现了潜力，但它们在有效管理推理方面的能力，包括疾病进展、治疗反应和安全药物处方等方面仍需进一步探索。我们通过一种新的基于LLM的代理系统推进了Articulate Medical Intelligence Explorer（AMIE）的先前展示的诊断能力，该系统旨在优化临床管理和对话，并结合了对疾病演化、多患者访问会诊、治疗反应以及药物处方专业技能的推理。为使其推理基于权威的临床知识，AMIE 利用了Gemini的长上下文能力，将上下文检索与结构化推理相结合，使其输出与相关且最新的临床实践指南和药品目录保持一致。在一项随机、盲法的虚拟客观结构化临床考试（OSCE）研究中，AMIE 被与21名初级保健医生（PCPs）在100个多访问病例场景下进行了比较，这些场景旨在反映英国防疫署（NICE）指南和BMJ最佳实践指南。AMIE 在管理推理方面不劣于PCPs，并在治疗和检查的精确性和管理计划与临床指南的契合度和 grounded 方面表现更佳。为了衡量药物推理，我们开发了RxQA，这是一个基于两个国家级药品目录（美国、英国）的多选题基准，并由认证药师进行了验证。虽然AMIE和PCPs都受益于访问外部药物信息的能力，但在较难的问题上，AMIE表现更好。虽然在实际应用前仍需进一步研究，但AMIE 在各种评估中的出色表现标志着对话式AI作为疾病管理工具的一个重要步骤。', 'title_zh': '面向疾病管理的对话式AI研究'}
{'arxiv_id': 'arXiv:2503.06072', 'title': 'A Survey on Post-training of Large Language Models', 'authors': 'Guiyao Tie, Zeli Zhao, Dingjie Song, Fuyang Wei, Rong Zhou, Yurou Dai, Wen Yin, Zhejian Yang, Jiangyue Yan, Yao Su, Zhenhan Dai, Yifeng Xie, Yihan Cao, Lichao Sun, Pan Zhou, Lifang He, Hechang Chen, Yu Zhang, Qingsong Wen, Tianming Liu, Neil Zhenqiang Gong, Jiliang Tang, Caiming Xiong, Heng Ji, Philip S. Yu, Jianfeng Gao', 'link': 'https://arxiv.org/abs/2503.06072', 'abstract': "The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; and Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT's foundational alignment strategies to DeepSeek-R1's innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.", 'abstract_zh': '大语言模型（LLMs）的兴起从根本上变革了自然语言处理，使其在从对话系统到科学探索的多个领域中不可或缺。然而，它们的预训练架构在专门情境中常常表现出限制，包括推理能力受限、伦理不确定性以及次优的领域特定性能。这些挑战需要先进的后训练语言模型（PoLMs）来解决这些问题，例如OpenAI-o1/o3和DeepSeek-R1（统称为大型推理模型，或LRMs）。本文首次全面综述了PoLMs，系统地追踪了它们在五种核心范式中的演变：微调，Enhances任务特定准确性；对齐，确保与人类偏好的一致性；推理，尽管存在奖励设计挑战，仍推动多步推理；效率，优化资源利用以应对不断增加的复杂性；以及整合与适应，扩展跨多种模态的能力并解决连贯性问题。从ChatGPT的基础对齐策略到DeepSeek-R1的创新推理进步，我们展示了PoLMs如何利用数据集来缓解偏差、深化推理能力和增强领域适应性。我们的贡献包括对PoLM演变的开创性综述、结构化的分类法对技术和数据集进行分类，以及强调LRMs在提高推理能力和领域灵活性中的作用的战略议程。作为该领域首个综述，本文巩固了最近的PoLM进展，并为未来研究建立了严格的知识框架，促进开发在精度、伦理稳健性和跨科学和社会应用的多样适应性方面表现出色的LLMs。', 'title_zh': '大型语言模型的后训练综述'}
{'arxiv_id': 'arXiv:2503.06054', 'title': 'Fine-Grained Bias Detection in LLM: Enhancing detection mechanisms for nuanced biases', 'authors': 'Suvendu Mohanty', 'link': 'https://arxiv.org/abs/2503.06054', 'abstract': 'Recent advancements in Artificial Intelligence, particularly in Large Language Models (LLMs), have transformed natural language processing by improving generative capabilities. However, detecting biases embedded within these models remains a challenge. Subtle biases can propagate misinformation, influence decision-making, and reinforce stereotypes, raising ethical concerns. This study presents a detection framework to identify nuanced biases in LLMs. The approach integrates contextual analysis, interpretability via attention mechanisms, and counterfactual data augmentation to capture hidden biases across linguistic contexts. The methodology employs contrastive prompts and synthetic datasets to analyze model behaviour across cultural, ideological, and demographic scenarios.\nQuantitative analysis using benchmark datasets and qualitative assessments through expert reviews validate the effectiveness of the framework. Results show improvements in detecting subtle biases compared to conventional methods, which often fail to highlight disparities in model responses to race, gender, and socio-political contexts. The framework also identifies biases arising from imbalances in training data and model architectures. Continuous user feedback ensures adaptability and refinement. This research underscores the importance of proactive bias mitigation strategies and calls for collaboration between policymakers, AI developers, and regulators. The proposed detection mechanisms enhance model transparency and support responsible LLM deployment in sensitive applications such as education, legal systems, and healthcare. Future work will focus on real-time bias monitoring and cross-linguistic generalization to improve fairness and inclusivity in AI-driven communication tools.', 'abstract_zh': '近期人工智能的发展，特别是大型语言模型（LLMs）的进步，已通过提升生成能力改变了自然语言处理。然而，检测这些模型中嵌入的偏见仍然是一个挑战。细微的偏见可能会传播错误信息、影响决策并强化刻板印象，引发伦理问题。本研究提出了一种检测框架，用于识别大型语言模型中的复杂偏见。该方法结合了上下文分析、通过注意机制提高可解释性以及对抗性数据增强，以捕捉语言不同情境下的隐藏偏见。该方法论采用了对比提示和合成数据集来分析模型在文化、意识形态和人口统计学场景下的行为。基准数据集的定量分析和专家评审的定性评估验证了该框架的有效性。结果显示，与传统方法相比，该框架在检测细微偏见方面有所改进，传统方法往往难以揭示模型在种族、性别和社会政治语境下的响应差异。该框架还识别出由于训练数据不平衡和模型架构而导致的偏见。持续的用户反馈确保了该框架的适应性和改进。本研究强调了主动偏见缓解策略的重要性，并呼吁政策制定者、AI开发者和监管机构之间的合作。所提出的检测机制增强了模型的透明度，并支持在教育、法律系统和医疗保健等敏感应用中负责任地部署大型语言模型。未来的工作将集中于实时偏见监控和跨语言泛化，以改进基于人工智能的通信工具中的公平性和包容性。', 'title_zh': 'LLM中细粒度偏见检测：增强对细微偏见的检测机制'}
{'arxiv_id': 'arXiv:2503.06030', 'title': 'Towards Universal Text-driven CT Image Segmentation', 'authors': 'Yuheng Li, Yuxiang Lai, Maria Thor, Deborah Marshall, Zachary Buchwald, David S. Yu, Xiaofeng Yang', 'link': 'https://arxiv.org/abs/2503.06030', 'abstract': 'Computed tomography (CT) is extensively used for accurate visualization and segmentation of organs and lesions. While deep learning models such as convolutional neural networks (CNNs) and vision transformers (ViTs) have significantly improved CT image analysis, their performance often declines when applied to diverse, real-world clinical data. Although foundation models offer a broader and more adaptable solution, their potential is limited due to the challenge of obtaining large-scale, voxel-level annotations for medical images. In response to these challenges, prompting-based models using visual or text prompts have emerged. Visual-prompting methods, such as the Segment Anything Model (SAM), still require significant manual input and can introduce ambiguity when applied to clinical scenarios. Instead, foundation models that use text prompts offer a more versatile and clinically relevant approach. Notably, current text-prompt models, such as the CLIP-Driven Universal Model, are limited to text prompts already encountered during training and struggle to process the complex and diverse scenarios of real-world clinical applications. Instead of fine-tuning models trained from natural imaging, we propose OpenVocabCT, a vision-language model pretrained on large-scale 3D CT images for universal text-driven segmentation. Using the large-scale CT-RATE dataset, we decompose the diagnostic reports into fine-grained, organ-level descriptions using large language models for multi-granular contrastive learning. We evaluate our OpenVocabCT on downstream segmentation tasks across nine public datasets for organ and tumor segmentation, demonstrating the superior performance of our model compared to existing methods. All code, datasets, and models will be publicly released at this https URL.', 'abstract_zh': '基于计算断层扫描的通用词汇量视觉-语言模型：用于器官和肿瘤分割的多粒度对比学习;oCT-Based OpenVocabCT: Multigranular Contrastive Learning for Universal Text-Driven Segmentation of Organs and Tumors', 'title_zh': '面向通用的文本驱动CT图像分割'}
{'arxiv_id': 'arXiv:2503.06011', 'title': 'Intent-Aware Self-Correction for Mitigating Social Biases in Large Language Models', 'authors': 'Panatchakorn Anantaprayoon, Masahiro Kaneko, Naoaki Okazaki', 'link': 'https://arxiv.org/abs/2503.06011', 'abstract': "Self-Correction based on feedback improves the output quality of Large Language Models (LLMs). Moreover, as Self-Correction functions like the slow and conscious System-2 thinking from cognitive psychology's perspective, it can potentially reduce LLMs' social biases. LLMs are sensitive to contextual ambiguities and inconsistencies; therefore, explicitly communicating their intentions during interactions when applying Self-Correction for debiasing is crucial. In this study, we demonstrate that clarifying intentions is essential for effectively reducing biases in LLMs through Self-Correction. We divide the components needed for Self-Correction into three parts: instruction, response, and feedback, and clarify intentions at each component. We incorporate an explicit debiasing prompt to convey the intention of bias mitigation from the instruction for response generation. In the response, we use Chain-of-Thought (CoT) to clarify the reasoning process. In the feedback, we define evaluation aspects necessary for debiasing and propose clear feedback through multi-aspect critiques and scoring. Through experiments, we demonstrate that self-correcting CoT responses obtained from a debiasing prompt based on multi-aspect feedback can reduce biased responses more robustly and consistently than the baselines. We also find the variation in debiasing efficacy when using models with different bias levels or separating models for response and feedback generation.", 'abstract_zh': '基于反馈的自我修正提高大型语言模型的输出质量，从认知心理学的角度来看，自我修正类似于系统的System-2思考，这可能减少大型语言模型的社会偏见。大型语言模型对上下文的歧义和不一致性敏感，因此，在应用自我修正进行去偏见化过程中明确沟通意图至关重要。本研究通过实验证明，明确意图对于通过自我修正有效减少大型语言模型的偏见至关重要。我们将自我修正所需的组件分为三部分：指令、响应和反馈，并在每个组件中明确意图。我们通过带有明确去偏见提示的指令来传达去偏见的意图，以便生成响应。在响应中，我们使用思维链（CoT）来澄清推理过程。在反馈中，我们定义了去偏见所需的评估维度，并提出了多方面批评和评分的明确反馈。通过实验，我们证明了基于多维度反馈生成的去偏见提示的自我修正思维链响应比基线方法更能稳健和一致地减少偏见。我们还发现，在不同偏见水平的模型或分离用于生成响应和反馈的模型时，去偏见效果的差异。', 'title_zh': '面向意图的自我修正以缓解大型语言模型中的社会偏见'}
{'arxiv_id': 'arXiv:2503.05980', 'title': 'SINdex: Semantic INconsistency Index for Hallucination Detection in LLMs', 'authors': 'Samir Abdaljalil, Hasan Kurban, Parichit Sharma, Erchin Serpedin, Rachad Atat', 'link': 'https://arxiv.org/abs/2503.05980', 'abstract': 'Large language models (LLMs) are increasingly deployed across diverse domains, yet they are prone to generating factually incorrect outputs - commonly known as "hallucinations." Among existing mitigation strategies, uncertainty-based methods are particularly attractive due to their ease of implementation, independence from external data, and compatibility with standard LLMs. In this work, we introduce a novel and scalable uncertainty-based semantic clustering framework for automated hallucination detection. Our approach leverages sentence embeddings and hierarchical clustering alongside a newly proposed inconsistency measure, SINdex, to yield more homogeneous clusters and more accurate detection of hallucination phenomena across various LLMs. Evaluations on prominent open- and closed-book QA datasets demonstrate that our method achieves AUROC improvements of up to 9.3% over state-of-the-art techniques. Extensive ablation studies further validate the effectiveness of each component in our framework.', 'abstract_zh': '基于不确定性的一种可扩展的语义聚类自动幻觉检测框架', 'title_zh': 'SINdex：大模型幻觉检测的语义不一致性指数'}
{'arxiv_id': 'arXiv:2503.05977', 'title': 'Is Your Video Language Model a Reliable Judge?', 'authors': 'Ming Liu, Wensheng Zhang', 'link': 'https://arxiv.org/abs/2503.05977', 'abstract': 'As video language models (VLMs) gain more applications in various scenarios, the need for robust and scalable evaluation of their performance becomes increasingly critical. The traditional human expert-based evaluation of VLMs has limitations in consistency and scalability, which sparked interest in automatic methods such as employing VLMs to evaluate VLMs. However, the reliability of VLMs as judges remains underexplored. Existing methods often rely on a single VLM as the evaluator. However, this approach can be unreliable or biased because such a model may lack the ability to fully understand the content and may have inherent biases, ultimately compromising evaluation reliability. A remedy is to apply the principle of collective thoughts, aggregating evaluations from multiple VLMs to enhance reliability. This study investigates the efficacy of such approaches, particularly when the pool of judges includes both reliable and unreliable models. Our findings reveal that incorporating collective judgments from such a mixed pool does not necessarily improve the accuracy of the final evaluation. The inclusion of less reliable judges can introduce noise, undermining the overall reliability of the outcomes. To explore the factors that impact evaluation reliability, we fine-tune an underperforming VLM judge, Video-LLaVA, and observe that improved understanding ability alone is insufficient to make VLM judges more reliable. These findings stress the limitations of collective thought approaches and highlight the need for more advanced methods that can account for the reliability of individual models. Our study promotes the development of more reliable evaluation methods for VLMs', 'abstract_zh': '随着视频语言模型（VLMs）在各种场景中的应用越来越广泛，对其性能的稳健和可扩展评估的需求变得日益关键。传统的人工专家评估方法在一致性和可扩展性方面存在局限性，这激发了使用自动方法（如让VLM们评估VLM们）的兴趣。然而，VLMs作为评判者可靠性的探索仍处于初级阶段。现有方法通常依赖单一的VLM作为评判者，但这种方法可能会因为模型缺乏对内容的全面理解或存在固有偏见而变得不可靠或有偏见，从而影响评判的可靠性。一种解决办法是应用群体智慧的原则，通过汇集多个VLM的评估来提高可靠性。本研究调查了此类方法的有效性，尤其是在评判者池中包括可靠的和不稳定的模型时。我们的发现表明，将这种混合池中的判断意见汇集起来并不一定能提高最终评估的准确性，不稳定的评判者可能会引入噪声，从而削弱结果的整体可靠性。为了探索影响评估可靠性的因素，我们对表现不佳的VLM评判者Video-LLaVA进行了微调，发现仅提高理解能力不足以使VLM评判者更可靠。这些发现强调了群体智慧方法的局限性，并突显了需要更先进的方法来考虑到单个模型的可靠性。本研究促进了更可靠的方法的开发，以评估VLMs。', 'title_zh': '你的视频语言模型是一个可靠的裁判吗？'}
{'arxiv_id': 'arXiv:2503.05958', 'title': 'SANDWiCH: Semantical Analysis of Neighbours for Disambiguating Words in Context ad Hoc', 'authors': 'Daniel Guzman-Olivares, Lara Quijano-Sanchez, Federico Liberatore', 'link': 'https://arxiv.org/abs/2503.05958', 'abstract': 'The rise of generative chat-based Large Language Models (LLMs) over the past two years has spurred a race to develop systems that promise near-human conversational and reasoning experiences. However, recent studies indicate that the language understanding offered by these models remains limited and far from human-like performance, particularly in grasping the contextual meanings of words, an essential aspect of reasoning. In this paper, we present a simple yet computationally efficient framework for multilingual Word Sense Disambiguation (WSD). Our approach reframes the WSD task as a cluster discrimination analysis over a semantic network refined from BabelNet using group algebra. We validate our methodology across multiple WSD benchmarks, achieving a new state of the art for all languages and tasks, as well as in individual assessments by part of speech. Notably, our model significantly surpasses the performance of current alternatives, even in low-resource languages, while reducing the parameter count by 72%.', 'abstract_zh': '过去两年中生成式聊天基础的大语言模型的兴起促使了一场开发承诺提供接近人类对话和推理体验系统的竞赛。然而，最近的研究表明，这些模型提供的语言理解能力仍然有限，远未达到人类水平的表现，尤其是在理解词语的上下文意义方面，这是推理的一个重要方面。在本文中，我们提出了一种简单而计算高效的多语言词义消歧框架。我们的方法将词义消歧任务重新表述为基于使用群代数从BabelNet提炼而成的语义网络的聚类鉴别分析。我们在多个词义消歧基准上验证了我们的方法，实现了所有语言和任务的新最佳性能，并在词性细分评估中也实现了最佳性能。值得注意的是，我们的模型在低资源语言上显著超越了当前的替代方案，同时参数量减少了72%。', 'title_zh': 'SANDWiCH: 基于邻居的语义分析在即时情境中消歧词义'}
{'arxiv_id': 'arXiv:2503.05951', 'title': 'TPU-Gen: LLM-Driven Custom Tensor Processing Unit Generator', 'authors': 'Deepak Vungarala, Mohammed E. Elbtity, Sumiya Syed, Sakila Alam, Kartik Pandit, Arnob Ghosh, Ramtin Zand, Shaahin Angizi', 'link': 'https://arxiv.org/abs/2503.05951', 'abstract': 'The increasing complexity and scale of Deep Neural Networks (DNNs) necessitate specialized tensor accelerators, such as Tensor Processing Units (TPUs), to meet various computational and energy efficiency requirements. Nevertheless, designing optimal TPU remains challenging due to the high domain expertise level, considerable manual design time, and lack of high-quality, domain-specific datasets. This paper introduces TPU-Gen, the first Large Language Model (LLM) based framework designed to automate the exact and approximate TPU generation process, focusing on systolic array architectures. TPU-Gen is supported with a meticulously curated, comprehensive, and open-source dataset that covers a wide range of spatial array designs and approximate multiply-and-accumulate units, enabling design reuse, adaptation, and customization for different DNN workloads. The proposed framework leverages Retrieval-Augmented Generation (RAG) as an effective solution for a data-scare hardware domain in building LLMs, addressing the most intriguing issue, hallucinations. TPU-Gen transforms high-level architectural specifications into optimized low-level implementations through an effective hardware generation pipeline. Our extensive experimental evaluations demonstrate superior performance, power, and area efficiency, with an average reduction in area and power of 92\\% and 96\\% from the manual optimization reference values. These results set new standards for driving advancements in next-generation design automation tools powered by LLMs.', 'abstract_zh': '基于大型语言模型的TPU生成框架：面向 systolic阵列架构的精确与近似TPU自动化设计', 'title_zh': 'TPU-Gen：由LLM驱动的自定义张量处理单元生成器'}
{'arxiv_id': 'arXiv:2503.05920', 'title': 'IDEA Prune: An Integrated Enlarge-and-Prune Pipeline in Generative Language Model Pretraining', 'authors': 'Yixiao Li, Xianzhi Du, Ajay Jaiswal, Tao Lei, Tuo Zhao, Chong Wang, Jianyu Wang', 'link': 'https://arxiv.org/abs/2503.05920', 'abstract': 'Recent advancements in large language models have intensified the need for efficient and deployable models within limited inference budgets. Structured pruning pipelines have shown promise in token efficiency compared to training target-size models from scratch. In this paper, we advocate incorporating enlarged model pretraining, which is often ignored in previous works, into pruning. We study the enlarge-and-prune pipeline as an integrated system to address two critical questions: whether it is worth pretraining an enlarged model even when the model is never deployed, and how to optimize the entire pipeline for better pruned models. We propose an integrated enlarge-and-prune pipeline, which combines enlarge model training, pruning, and recovery under a single cosine annealing learning rate schedule. This approach is further complemented by a novel iterative structured pruning method for gradual parameter removal. The proposed method helps to mitigate the knowledge loss caused by the rising learning rate in naive enlarge-and-prune pipelines and enable effective redistribution of model capacity among surviving neurons, facilitating smooth compression and enhanced performance. We conduct comprehensive experiments on compressing 2.8B models to 1.3B with up to 2T tokens in pretraining. It demonstrates the integrated approach not only provides insights into the token efficiency of enlarged model pretraining but also achieves superior performance of pruned models.', 'abstract_zh': 'Recent advancements in大型语言模型促使了在有限推断预算内高效且可部署模型的需求增加。结构化剪枝管道在标记效率方面展现了潜力，与从头开始训练目标规模的模型相比。在本文中，我们提倡将通常在以前工作中被忽略的扩大模型预训练纳入剪枝过程。我们研究扩大与剪枝管道作为集成系统以解决两个关键问题：即使模型从未部署，是否值得预训练一个扩大模型，以及如何优化整个管道以获得更好的剪枝模型。我们提出了一种集成的扩大与剪枝管道，该管道结合了扩大模型训练、剪枝和恢复，并采用单一余弦退火学习率调度。该方法进一步通过一种新颖的迭代结构化剪枝方法来逐步移除参数进行补充。所提出的方法有助于缓解在简单扩大与剪枝管道中学习率上升导致的知识损失，并使模型容量在幸存神经元之间有效重新分配，从而实现平滑压缩并增强性能。我们在预训练中将2.8B模型压缩到1.3B，最多使用2T标记的全面实验表明，集成方法不仅为扩大模型预训练的标记效率提供了见解，还实现了更好的剪枝模型性能。', 'title_zh': 'IDEA 瘦身计划：生成语言模型预训练中的集成扩大-剪枝管道'}
{'arxiv_id': 'arXiv:2503.05899', 'title': 'Towards Understanding the Use of MLLM-Enabled Applications for Visual Interpretation by Blind and Low Vision People', 'authors': 'Ricardo E. Gonzalez Penuela, Ruiying Hu, Sharon Lin, Tanisha Shende, Shiri Azenkot', 'link': 'https://arxiv.org/abs/2503.05899', 'abstract': "Blind and Low Vision (BLV) people have adopted AI-powered visual interpretation applications to address their daily needs. While these applications have been helpful, prior work has found that users remain unsatisfied by their frequent errors. Recently, multimodal large language models (MLLMs) have been integrated into visual interpretation applications, and they show promise for more descriptive visual interpretations. However, it is still unknown how this advancement has changed people's use of these applications. To address this gap, we conducted a two-week diary study in which 20 BLV people used an MLLM-enabled visual interpretation application we developed, and we collected 553 entries. In this paper, we report a preliminary analysis of 60 diary entries from 6 participants. We found that participants considered the application's visual interpretations trustworthy (mean 3.75 out of 5) and satisfying (mean 4.15 out of 5). Moreover, participants trusted our application in high-stakes scenarios, such as receiving medical dosage advice. We discuss our plan to complete our analysis to inform the design of future MLLM-enabled visual interpretation systems.", 'abstract_zh': '盲人和低视力人士采用AI增强的视觉解释应用以应对日常生活需求。虽然这些应用有所帮助，但先前的研究发现用户仍然对它们频繁的错误不满意。最近，多模态大型语言模型被集成到视觉解释应用中，显示出提供更具描述性的视觉解释的潜力。然而，这些进展如何改变了人们使用这些应用的方式尚不清楚。为了解决这一缺口，我们开展了一项为期两周的日志研究，20名盲人和低视力人士使用我们开发的多模态大型语言模型增强的视觉解释应用，我们收集了553条日志条目。在本文中，我们报告了6名参与者中60条日志条目的初步分析结果。我们发现参与者认为该应用的视觉解释值得信赖（平均3.75/5），令人满意（平均4.15/5）。此外，参与者在高风险场景，如接收药物剂量建议时信任该应用。我们讨论了我们计划完成分析以指导未来多模态大型语言模型增强的视觉解释系统的设计。', 'title_zh': '探索MLLM赋能应用在视觉解释方面对视障和低视力人群的应用理解'}
{'arxiv_id': 'arXiv:2503.05856', 'title': 'This Is Your Doge, If It Please You: Exploring Deception and Robustness in Mixture of LLMs', 'authors': 'Lorenz Wolf, Sangwoong Yoon, Ilija Bogunovic', 'link': 'https://arxiv.org/abs/2503.05856', 'abstract': "Mixture of large language model (LLMs) Agents (MoA) architectures achieve state-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by leveraging the collaboration of multiple LLMs at inference time. Despite these successes, an evaluation of the safety and reliability of MoA is missing. We present the first comprehensive study of MoA's robustness against deceptive LLM agents that deliberately provide misleading responses. We examine factors like the propagation of deceptive information, model size, and information availability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the popular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of 49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate that introducing only a $\\textit{single}$ carefully-instructed deceptive agent into the MoA can reduce performance to 37.9%, effectively nullifying all MoA gains. On QuALITY, a multiple-choice comprehension task, the impact is also severe, with accuracy plummeting by a staggering 48.5%. Inspired in part by the historical Doge of Venice voting process, designed to minimize influence and deception, we propose a range of unsupervised defense mechanisms that recover most of the lost performance.", 'abstract_zh': '混合大规模语言模型代理架构（MoA）在如AlpacaEval 2.0等 prominent 基准上的性能通过在推理时利用多个大规模语言模型的协作而达到最优，尽管取得了这些成功，但 MoA 的安全性和可靠性评估却缺失。我们首次全面研究了 MoA 对故意提供误导性响应的欺骗性大规模语言模型代理的鲁棒性。我们考察了欺骗性信息的传播、模型规模和信息可用性等因素，并发现了关键漏洞。在 AlpacaEval 2.0 上，流行的 LLaMA 3.1-70B 模型与 3 层 MoA（6 个大规模语言模型代理）结合时，长度控制胜率（LC WR）为 49.2%。然而，我们证明，只需引入一个精心指导的欺骗性代理到 MoA 中，性能即可降至 37.9%，从而完全抵消了 MoA 的所有收益。在 QuALITY 多项选择理解任务上，影响也十分严重，准确率骤降 48.5%。受到历史上的威尼斯小狗选举过程部分启发，旨在最小化影响和欺骗，我们提出了一系列无监督的防御机制，这些机制能够恢复大部分丢失的性能。', 'title_zh': '这是您的Doge，如果您愿意：探索混合大语言模型中的欺骗与鲁棒性'}
{'arxiv_id': 'arXiv:2503.05854', 'title': 'Accelerating Earth Science Discovery via Multi-Agent LLM Systems', 'authors': 'Dmitrii Pantiukhin, Boris Shapkin, Ivan Kuznetsov, Antonia Anna Jost, Nikolay Koldunov', 'link': 'https://arxiv.org/abs/2503.05854', 'abstract': 'This Perspective explores the transformative potential of Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) in the geosciences. Users of geoscientific data repositories face challenges due to the complexity and diversity of data formats, inconsistent metadata practices, and a considerable number of unprocessed datasets. MAS possesses transformative potential for improving scientists\' interaction with geoscientific data by enabling intelligent data processing, natural language interfaces, and collaborative problem-solving capabilities. We illustrate this approach with "PANGAEA GPT", a specialized MAS pipeline integrated with the diverse PANGAEA database for Earth and Environmental Science, demonstrating how MAS-driven workflows can effectively manage complex datasets and accelerate scientific discovery. We discuss how MAS can address current data challenges in geosciences, highlight advancements in other scientific fields, and propose future directions for integrating MAS into geoscientific data processing pipelines. In this Perspective, we show how MAS can fundamentally improve data accessibility, promote cross-disciplinary collaboration, and accelerate geoscientific discoveries.', 'abstract_zh': '这一视角探讨了由大规模语言模型驱动的多代理系统（MAS）在地球科学领域中的变革潜力。', 'title_zh': '通过多代理大规模语言模型系统加速地球科学发现'}
{'arxiv_id': 'arXiv:2503.05852', 'title': 'Evaluating Large Language Models in Code Generation: INFINITE Methodology for Defining the Inference Index', 'authors': 'Nicholas Christakis, Dimitris Drikakis', 'link': 'https://arxiv.org/abs/2503.05852', 'abstract': "This study introduces a new methodology for an Inference Index (InI), called INFerence INdex In Testing model Effectiveness methodology (INFINITE), aiming to evaluate the performance of Large Language Models (LLMs) in code generation tasks. The InI index provides a comprehensive assessment focusing on three key components: efficiency, consistency, and accuracy. This approach encapsulates time-based efficiency, response quality, and the stability of model outputs, offering a thorough understanding of LLM performance beyond traditional accuracy metrics. We applied this methodology to compare OpenAI's GPT-4o (GPT), OpenAI-o1 pro (OAI1), and OpenAI-o3 mini-high (OAI3) in generating Python code for the Long-Short-Term-Memory (LSTM) model to forecast meteorological variables such as temperature, relative humidity and wind velocity. Our findings demonstrate that GPT outperforms OAI1 and performs comparably to OAI3 regarding accuracy and workflow efficiency. The study reveals that LLM-assisted code generation can produce results similar to expert-designed models with effective prompting and refinement. GPT's performance advantage highlights the benefits of widespread use and user feedback.", 'abstract_zh': '本研究提出了一种用于评估大型语言模型（LLMs）在代码生成任务中效果的新方法论，称为INFerence INdex In Testing model Effectiveness（INFINITE）方法论，旨在引入一种新的推理指数（Inference Index, InI）以全面评估LLMs的性能。InI指数侧重于效率、一致性和准确性这三个关键组件，涵盖了基于时间的效率、回应质量以及模型输出的稳定性，提供了超越传统准确率指标的全面理解。我们应用此方法论 compare OpenAI的GPT-4o（GPT）、OpenAI-o1 pro（OAI1）和OpenAI-o3 mini-high（OAI3）在生成用于气象变量（如温度、相对湿度和风速）预测的长短期记忆（LSTM）模型的Python代码方面的性能。研究结果表明，GPT在准确性和工作流程效率方面优于OAI1并与其OAI3相当。研究表明，通过有效的提示和精炼，LLM辅助的代码生成可以产生与专家设计模型相似的结果。GPT表现的优势突显了广泛使用和用户反馈的益处。', 'title_zh': '评估代码生成大型语言模型的方法：INFINITE方法论用于定义推理指标'}
{'arxiv_id': 'arXiv:2503.05846', 'title': 'Extracting and Emulsifying Cultural Explanation to Improve Multilingual Capability of LLMs', 'authors': 'Hamin Koo, Jaehyung Kim', 'link': 'https://arxiv.org/abs/2503.05846', 'abstract': "Large Language Models (LLMs) have achieved remarkable success, but their English-centric training data limits performance in non-English languages, highlighting the need for enhancements in their multilingual capabilities. While some work on multilingual prompting methods handles non-English queries by utilizing English translations or restructuring them to more closely align with LLM reasoning patterns, these works often overlook the importance of cultural context, limiting their effectiveness. To address this limitation, we propose EMCEI, a simple yet effective approach that improves LLMs' multilingual capabilities by incorporating cultural context for more accurate and appropriate responses. Specifically, EMCEI follows a two-step process that first extracts relevant cultural context from the LLM's parametric knowledge via prompting. Then, EMCEI employs an LLM-as-Judge mechanism to select the most appropriate response by balancing cultural relevance and reasoning ability. Experiments on diverse multilingual benchmarks show that EMCEI outperforms existing baselines, demonstrating its effectiveness in handling multilingual queries with LLMs.", 'abstract_zh': '大型语言模型（LLMs）取得了显著的成功，但以英语为中心的训练数据限制了其在非英语语言中的表现，凸显了增强其多语言能力的需求。虽然有些关于多语言提示方法的研究通过使用英语翻译或重新构建查询以更接近LLM的推理模式来处理非英语查询，但这些工作往往忽视了文化背景的重要性，限制了其效果。为了解决这一限制，我们提出了EMCEI，这是一种简单而有效的方法，通过集成文化背景来提高LLMs的多语言能力，从而获得更准确和更合适的响应。具体而言，EMCEI 采用两步过程：首先，通过提示从LLM的参数化知识中提取相关文化背景；然后，EMCEI 使用LLM作为裁判机制来选择最合适的响应，平衡文化相关性和推理能力。在多样化的多语言基准测试中，实验展示了EMCEI 出色地超过了现有的基线方法，证明了其在使用LLMs处理多语言查询方面的有效性。', 'title_zh': '提取和乳化文化解释以提高多语言能力的LLMs'}
{'arxiv_id': 'arXiv:2503.05804', 'title': 'Holistically Evaluating the Environmental Impact of Creating Language Models', 'authors': 'Jacob Morrison, Clara Na, Jared Fernandez, Tim Dettmers, Emma Strubell, Jesse Dodge', 'link': 'https://arxiv.org/abs/2503.05804', 'abstract': "As the performance of artificial intelligence systems has dramatically increased, so too has the environmental impact of creating these systems. While many model developers release estimates of the power consumption and carbon emissions from the final training runs for their latest models, there is comparatively little transparency into the impact of model development, hardware manufacturing, and total water usage throughout. In this work, we estimate the real-world environmental impact of developing a series of language models, ranging from 20 million to 13 billion active parameters, trained on up to 5.6 trillion tokens each. When accounting for hardware manufacturing, model development, and our final training runs, we find that our series of models released 493 metric tons of carbon emissions, equivalent to powering about 98 homes in the United States for one year, and consumed 2.769 million liters of water, equivalent to about 24.5 years of water usage by a person in the United States, even though our data center is extremely water-efficient. We measure and report the environmental impact of our model development; to the best of our knowledge we are the first to do so for LLMs, and we find that model development, the impact of which is generally not disclosed by most model developers, amounted to ~50% of that of training. By looking at detailed time series data for power consumption, we also find that power usage throughout training is not consistent, fluctuating between ~15% and ~85% of our hardware's maximum power draw, with negative implications for grid-scale planning as demand continues to grow. We close with a discussion on the continued difficulty of estimating the environmental impact of AI systems, and key takeaways for model developers and the public at large.", 'abstract_zh': '随着人工智能系统性能的大幅提高，其环境影响也随之增大。尽管许多模型开发者会公布其最新模型最终训练阶段的电力消耗和碳排放估计值，但模型开发、硬件制造以及总计水消耗的影响透明度却相对较低。在本研究中，我们估算了一系列从2000万到130亿激活参数的语言模型的环境影响，每个模型在训练时使用高达5.6万亿个词元。考虑到硬件制造、模型开发以及最终训练阶段的碳排放和水消耗，我们发现该系列模型共释放了493公吨的碳排放，相当于美国98户家庭一年的用电量，并消耗了276,900升水，相当于一个美国人24.5年的用水量，尽管我们的数据中心极为节水。我们测量并报告了模型开发的环境影响；据我们所知，这是首次针对大型语言模型（LLM）进行这样的测量和报告。我们发现，通常不被大多数模型开发者披露的模型开发的影响占总排放的约50%。通过详细的时间序列数据，我们还发现，在训练过程中，电力使用量波动较大，从硬件最大电力消耗的约15%到约85%，这在不断增长的需求下对电网规划提出了负面影响。最后，我们讨论了估算人工智能系统环境影响的持续困难，并提出了模型开发者和公众的关键启示。', 'title_zh': '全面评估创建语言模型的环境影响'}
{'arxiv_id': 'arXiv:2503.05793', 'title': 'MedSimAI: Simulation and Formative Feedback Generation to Enhance Deliberate Practice in Medical Education', 'authors': 'Yann Hicke, Jadon Geathers, Niroop Rajashekar, Colleen Chan, Anyanate Gwendolyne Jack, Justin Sewell, Mackenzi Preston, Susannah Cornes, Dennis Shung, Rene Kizilcec', 'link': 'https://arxiv.org/abs/2503.05793', 'abstract': 'Medical education faces challenges in scalability, accessibility, and consistency, particularly in clinical skills training for physician-patient communication. Traditional simulation-based learning, while effective, is resource-intensive, difficult to schedule, and often highly variable in feedback quality. Through a collaboration between AI, learning science, and medical education experts, we co-developed MedSimAI, an AI-powered simulation platform that enables deliberate practice, self-regulated learning (SRL), and automated assessment through interactive patient encounters. Leveraging large language models (LLMs), MedSimAI generates realistic clinical interactions and provides immediate, structured feedback using established medical evaluation frameworks such as the Master Interview Rating Scale (MIRS). In a pilot study with 104 first-year medical students, we examined engagement, conversation patterns, and user perceptions. Students found MedSimAI beneficial for repeated, realistic patient-history practice. Conversation analysis revealed that certain higher-order skills were often overlooked, though students generally performed systematic histories and empathic listening. By integrating unlimited practice opportunities, real-time AI assessment, and SRL principles, MedSimAI addresses key limitations of traditional simulation-based training, making high-quality clinical education more accessible and scalable.', 'abstract_zh': '医疗教育在可扩展性、可访问性和一致性方面面临挑战，特别是在医生-患者沟通的临床技能训练方面。尽管基于模拟的传统学习方法有效，但该方法资源消耗大、排期困难，且反馈质量经常差异明显。通过AI、学习科学和医疗教育专家的合作，我们共同开发了MedSimAI，这是一种基于AI的模拟平台，能够通过互动患者交流促进刻意练习、自我调节学习（SRL）和自动化评估。借助大规模语言模型（LLMs），MedSimAI生成了真实的临床互动，并使用《大师访谈评估量表》（MIRS）等已确立的医学评价框架，提供即时的结构化反馈。在一项包含104名一年级医学生的试点研究中，我们考察了参与度、对话模式和用户感知。学生认为MedSimAI在重复和真实的患者病史练习方面具有益处。对话分析表明，某些高层次技能经常被忽略，但学生通常能够进行系统的历史采集和共情倾听。通过整合无限的练习机会、实时AI评估以及SRL原则，MedSimAI解决了传统基于模拟的培训的关键限制，使高质量的临床教育更具可访问性和可扩展性。', 'title_zh': 'MedSimAI: 模拟与形成性反馈生成以提高医学教育中的 deliberate practice'}
{'arxiv_id': 'arXiv:2503.05788', 'title': 'Emergent Abilities in Large Language Models: A Survey', 'authors': 'Leonardo Berti, Flavio Giorgi, Gjergji Kasneci', 'link': 'https://arxiv.org/abs/2503.05788', 'abstract': 'Large Language Models (LLMs) are leading a new technological revolution as one of the most promising research streams toward artificial general intelligence. The scaling of these models, accomplished by increasing the number of parameters and the magnitude of the training datasets, has been linked to various so-called emergent abilities that were previously unobserved. These emergent abilities, ranging from advanced reasoning and in-context learning to coding and problem-solving, have sparked an intense scientific debate: Are they truly emergent, or do they simply depend on external factors, such as training dynamics, the type of problems, or the chosen metric? What underlying mechanism causes them? Despite their transformative potential, emergent abilities remain poorly understood, leading to misconceptions about their definition, nature, predictability, and implications. In this work, we shed light on emergent abilities by conducting a comprehensive review of the phenomenon, addressing both its scientific underpinnings and real-world consequences. We first critically analyze existing definitions, exposing inconsistencies in conceptualizing emergent abilities. We then explore the conditions under which these abilities appear, evaluating the role of scaling laws, task complexity, pre-training loss, quantization, and prompting strategies. Our review extends beyond traditional LLMs and includes Large Reasoning Models (LRMs), which leverage reinforcement learning and inference-time search to amplify reasoning and self-reflection. However, emergence is not inherently positive. As AI systems gain autonomous reasoning capabilities, they also develop harmful behaviors, including deception, manipulation, and reward hacking. We highlight growing concerns about safety and governance, emphasizing the need for better evaluation frameworks and regulatory oversight.', 'abstract_zh': '大型语言模型（LLMs）是通往人工通用智能最 promising 研究分支之一，引领着新一轮的技术革命。这些模型的扩展，通过增加参数数量和训练数据集的规模实现，与多种此前未被观察到的所谓“ emergent 能力”相关联。这些“ emergent 能力”包括从高级推理和上下文学习到编程和问题解决等能力，引发了激烈的科学争论：它们真的是“ emergent 的”，还是仅仅依赖于外部因素，如训练动态、任务类型或选择的度量标准？是什么底层机制导致了这些能力出现？尽管这些能力具有变革潜力，但它们的本质仍然 poorly understood，导致对其定义、本质、可预测性和影响的误解。在本研究中，我们通过对这一现象进行全面回顾，阐明“ emergent 能力”，探讨其科学基础及其实际后果。我们首先批判性地分析现有定义，揭示了“ emergent 能力”概念的一致性问题。然后探讨这些能力出现的条件，评估规模律、任务复杂性、预训练损失、量化以及提示策略的作用。我们的回顾不仅局限于传统的 LLMs，还包括利用强化学习和推理时搜索来放大推理和自我反思的 Large Reasoning Models (LRMs)。然而， “ emergent 能力”并非固然是积极的。随着 AI 系统获得自主推理能力，它们也发展出有害行为，包括欺骗、操纵和奖励作弊。我们强调了对安全和治理日益增长的担忧，并强调了需要更好的评估框架和监管监督的必要性。', 'title_zh': '大型语言模型中的 emergent 能力：一个综述'}
{'arxiv_id': 'arXiv:2503.05770', 'title': 'Generative Artificial Intelligence: Evolving Technology, Growing Societal Impact, and Opportunities for Information Systems Research', 'authors': 'Veda C. Storey, Wei Thoo Yue, J. Leon Zhao, Roman Lukyanenko', 'link': 'https://arxiv.org/abs/2503.05770', 'abstract': 'The continuing, explosive developments in generative artificial intelligence (GenAI), built on large language models and related algorithms, has led to much excitement and speculation about the potential impact of this new technology. Claims include AI being poised to revolutionize business and society and dramatically change personal life. However, it remains unclear exactly how this technology, with its significantly distinct features from past AI technologies, has transformative potential. Nor is it clear how researchers in information systems (IS) should respond. In this paper, we consider the evolving and emerging trends of AI in order to examine its present and predict its future impacts. Many existing papers on GenAI are either too technical for most IS researchers or lack the depth needed to appreciate the potential impacts of GenAI. We, therefore, attempt to bridge the technical and organizational communities of GenAI from a system-oriented sociotechnical perspective. Specifically, we explore the unique features of GenAI, which are rooted in the continued change from symbolism to connectionism, and the deep systemic and inherent properties of human-AI ecosystems. We retrace the evolution of AI that proceeded the level of adoption, adaption, and use found today, in order to propose future research on various impacts of GenAI in both business and society within the context of information systems research. Our efforts are intended to contribute to the creation of a well-structured research agenda in the IS community to support innovative strategies and operations enabled by this new wave of AI.', 'abstract_zh': '基于生成人工智能的发展及其对商业和社会的影响：一种系统导向的社会技术视角', 'title_zh': '生成式人工智能： evolving technology, growing societal impact, and opportunities for information systems research生成式人工智能： evolving technology, growing societal impact, and opportunities for information systems research'}
{'arxiv_id': 'arXiv:2503.05760', 'title': "The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its Own", 'authors': 'Gokul Puthumanaillam, Melkior Ornik', 'link': 'https://arxiv.org/abs/2503.05760', 'abstract': 'This paper presents a comprehensive investigation into the capability of Large Language Models (LLMs) to successfully complete a semester-long undergraduate control systems course. Through evaluation of 115 course deliverables, we assess LLM performance using ChatGPT under a ``minimal effort" protocol that simulates realistic student usage patterns. The investigation employs a rigorous testing methodology across multiple assessment formats, from auto-graded multiple choice questions to complex Python programming tasks and long-form analytical writing. Our analysis provides quantitative insights into AI\'s strengths and limitations in handling mathematical formulations, coding challenges, and theoretical concepts in control systems engineering. The LLM achieved a B-grade performance (82.24\\%), approaching but not exceeding the class average (84.99\\%), with strongest results in structured assignments and greatest limitations in open-ended projects. The findings inform discussions about course design adaptation in response to AI advancement, moving beyond simple prohibition towards thoughtful integration of these tools in engineering education. Additional materials including syllabus, examination papers, design projects, and example responses can be found at the project website: this https URL.', 'abstract_zh': '本论文对大型语言模型（LLMs）在完成一整个学期的本科生控制系统课程中的能力进行了全面调查。通过评估115份课程作业，我们使用“低投入”协议来评估ChatGPT的表现，该协议模拟了实际的学生使用模式。调查采用了多种评估方法，从自动评分的选择题到复杂的Python编程任务和长篇分析性写作。我们的分析提供了关于AI在处理控制系统工程中的数学公式、编码挑战和理论概念方面的优势与局限性的定量见解。LLM取得了B级成绩（82.24%），接近但未超过班级平均分（84.99%），在结构化作业中表现最佳，在开放型项目中表现最差。该研究结果有助于讨论课程设计适应性问题，以应对AI的发展，从简单的禁止转向对这些工具进行有深思熟虑的整合。更多材料，包括课程大纲、考试试卷、设计项目和示例答案，可在项目网站上找到：this https URL。', 'title_zh': '懒学生的好梦：ChatGPT独自通过工程课程'}
{'arxiv_id': 'arXiv:2503.05757', 'title': 'Uncertainty-Aware Fusion: An Ensemble Framework for Mitigating Hallucinations in Large Language Models', 'authors': 'Prasenjit Dey, Srujana Merugu, Sivaramakrishnan Kaveri', 'link': 'https://arxiv.org/abs/2503.05757', 'abstract': 'Large Language Models (LLMs) are known to hallucinate and generate non-factual outputs which can undermine user trust. Traditional methods to directly mitigate hallucinations, such as representation editing and contrastive decoding, often require additional training data and involve high implementation complexity. While ensemble-based approaches harness multiple LLMs to tap into the "wisdom of crowds", these methods overlook uncertainties in individual model responses. Recent studies reveal that uncertainty estimation can enable LLMs to self-assess the likelihood of generating hallucinations. In this work, we focus on factoid question answering (QA) and observe that LLMs accuracy and self-assessment capabilities vary widely with different models excelling in different scenarios. Leveraging this insight, we propose Uncertainty-Aware Fusion (UAF), an ensemble framework to reduces hallucinations by strategically combining multiple LLM based on their accuracy and self-assessment abilities. Empirical results on several public benchmark datasets show that UAF outperforms state-of-the-art hallucination mitigation methods by $8\\%$ in factual accuracy, while either narrowing or surpassing the performance gap with GPT-4.', 'abstract_zh': '大规模语言模型（LLMs） Known to Hallucinate and Generate Non-Factual Outputs, Undermining User Trust: An Uncertainty-Aware Fusion Approach (UAF) for Reducing Hallucinations in Factoid Question Answering', 'title_zh': '不确定性意识融合：一种缓解大型语言模型幻觉的集成框架'}
{'arxiv_id': 'arXiv:2503.05740', 'title': 'ChatWise: AI-Powered Engaging Conversations for Enhancing Senior Cognitive Wellbeing', 'authors': 'Zhengbang Yang, Zhuangdi Zhu', 'link': 'https://arxiv.org/abs/2503.05740', 'abstract': "Cognitive health in older adults presents a growing challenge. While conversational interventions show feasibility in improving cognitive wellness, human caregiver resources remain overburdened. AI-based methods have shown promise in providing conversational support, yet existing work is limited to implicit strategy while lacking multi-turn support tailored to seniors. We improve prior art with an LLM-driven chatbot named ChatWise for older adults. It follows dual-level conversation reasoning at the inference phase to provide engaging companionship. ChatWise thrives in long-turn conversations, in contrast to conventional LLMs that primarily excel in short-turn exchanges. Grounded experiments show that ChatWise significantly enhances simulated users' cognitive and emotional status, including those with Mild Cognitive Impairment.", 'abstract_zh': '老年人的认知健康呈日益严峻的挑战。虽然对话干预显示了在改善认知健康方面可行性，但人类护理资源仍不堪重负。基于AI的方法在提供对话支持方面显示了潜力，但现有工作局限于隐含策略，缺乏针对老年人的多轮次个性化支持。我们通过一个名为ChatWise的LLM驱动聊天机器人改进了以往的研究，它在推理阶段遵循双层对话推理机制，提供互动陪伴。ChatWise在长轮次对话中表现突出，而传统的LLM主要在短轮次交流中表现出色。基于地面实操的实验表明，ChatWise显著提升了模拟用户，包括轻度认知障碍患者，的认知和情绪状态。', 'title_zh': 'ChatWise：增强老年人认知健康的人工智能驱动交互对话'}
{'arxiv_id': 'arXiv:2503.05724', 'title': 'Addressing Moral Uncertainty using Large Language Models for Ethical Decision-Making', 'authors': 'Rohit K. Dubey, Damian Dailisan, Sachit Mahajan', 'link': 'https://arxiv.org/abs/2503.05724', 'abstract': 'We present an ethical decision-making framework that refines a pre-trained reinforcement learning (RL) model using a task-agnostic ethical layer. Following initial training, the RL model undergoes ethical fine-tuning, where human feedback is replaced by feedback generated from a large language model (LLM). The LLM embodies consequentialist, deontological, virtue, social justice, and care ethics as moral principles to assign belief values to recommended actions during ethical decision-making. An ethical layer aggregates belief scores from multiple LLM-derived moral perspectives using Belief Jensen-Shannon Divergence and Dempster-Shafer Theory into probability scores that also serve as the shaping reward, steering the agent toward choices that align with a balanced ethical framework. This integrated learning framework helps the RL agent navigate moral uncertainty in complex environments and enables it to make morally sound decisions across diverse tasks. Our approach, tested across different LLM variants and compared with other belief aggregation techniques, demonstrates improved consistency, adaptability, and reduced reliance on handcrafted ethical rewards. This method is especially effective in dynamic scenarios where ethical challenges arise unexpectedly, making it well-suited for real-world applications.', 'abstract_zh': '一种使用任务无关道德层精炼预训练强化学习模型的伦理决策框架', 'title_zh': '使用大型语言模型解决道德不确定性在伦理决策中的应用'}
{'arxiv_id': 'arXiv:2503.05712', 'title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'authors': 'Niklas Höpner, Leon Eshuijs, Dimitrios Alivanistos, Giacomo Zamprogno, Ilaria Tiddi', 'link': 'https://arxiv.org/abs/2503.05712', 'abstract': 'Foundation models are increasingly used in scientific research, but evaluating AI-generated scientific work remains challenging. While expert reviews are costly, large language models (LLMs) as proxy reviewers have proven to be unreliable. To address this, we investigate two automatic evaluation metrics, specifically citation count prediction and review score prediction. We parse all papers of OpenReview and augment each submission with its citation count, reference, and research hypothesis. Our findings reveal that citation count prediction is more viable than review score prediction, and predicting scores is more difficult purely from the research hypothesis than from the full paper. Furthermore, we show that a simple prediction model based solely on title and abstract outperforms LLM-based reviewers, though it still falls short of human-level consistency.', 'abstract_zh': '基础模型在科学研究中越来越受欢迎，但评估由AI生成的科学作品仍具挑战性。尽管专家评审成本高昂，代理评审人（如大型语言模型）的表现证明不够可靠。为此，我们研究了两种自动评估指标，分别是引文计数预测和评审评分预测。我们解析了所有OpenReview上的论文，并为每篇提交论文增加了引文计数、参考文献和研究假设。我们的研究发现，引文计数预测比评审评分预测更具可行性，从研究假设预测评分比从整篇论文预测更困难。此外，我们展示了基于标题和摘要的简单预测模型优于基于大型语言模型的评审人，尽管仍未能达到人类水平的一致性。', 'title_zh': '自动评估指标 for 人工生成的科学研究'}
{'arxiv_id': 'arXiv:2503.05703', 'title': 'What I cannot execute, I do not understand: Training and Evaluating LLMs on Program Execution Traces', 'authors': "Jordi Armengol-Estapé, Quentin Carbonneaux, Tianjun Zhang, Aram H. Markosyan, Volker Seeker, Chris Cummins, Melanie Kambadur, Michael F.P. O'Boyle, Sida Wang, Gabriel Synnaeve, Hugh James Leather", 'link': 'https://arxiv.org/abs/2503.05703', 'abstract': "Code generation and understanding are critical capabilities for large language models (LLMs). Thus, most LLMs are pretrained and fine-tuned on code data. However, these datasets typically treat code as static strings and rarely exploit the dynamic information about their execution. Building upon previous work on trace modeling, we study Execution Tuning (E.T.), a training procedure in which we explicitly model real-world program execution traces without requiring manual test annotations. We train and evaluate models on different execution trace granularities (line and instruction-level) and strategies on the task of output prediction, obtaining around 80% accuracy on CruxEval and MBPP, and showing the advantages of dynamic scratchpads (i.e., self-contained intermediate computations updated by the model rather than accumulated as a history of past computations) on long executions (up to 14k steps). Finally, we discuss E.T.'s practical applications.", 'abstract_zh': '代码生成与理解是大型语言模型（LLMs）的关键能力。因此，大多数LLMs都会在代码数据上进行预训练和微调。然而，这些数据集通常将代码视为静态字符串，很少利用其执行过程中的动态信息。在此基础上，我们研究了执行调优（E.T.），这是一种通过显式建模真实程序执行轨迹的训练方法，而不需要手动测试注释。我们在不同执行轨迹粒度（行级和指令级）和策略下训练和评估模型，在输出预测任务中获得了约80%的准确性，并展示了动态草稿纸（即模型更新的独立中间计算而不是累积为过去计算的历史）在长执行过程中的优势（最多14,000步）。最后，我们讨论了E.T.的实际应用。', 'title_zh': '无法执行的我就无法理解：在程序执行轨迹上训练和评估LLM'}
{'arxiv_id': 'arXiv:2503.05200', 'title': 'ORANSight-2.0: Foundational LLMs for O-RAN', 'authors': 'Pranshav Gajjar, Vijay K. Shah', 'link': 'https://arxiv.org/abs/2503.05200', 'abstract': 'Despite the transformative impact of Large Language Models (LLMs) across critical domains such as healthcare, customer service, and business marketing, their integration into Open Radio Access Networks (O-RAN) remains limited. This gap is primarily due to the absence of domain-specific foundational models, with existing solutions often relying on general-purpose LLMs that fail to address the unique challenges and technical intricacies of O-RAN. To bridge this gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiative aimed at developing specialized foundational LLMs tailored for O-RAN. Built on 18 LLMs spanning five open-source LLM frameworks, ORANSight-2.0 fine-tunes models ranging from 1 to 70B parameters, significantly reducing reliance on proprietary, closed-source models while enhancing performance for O-RAN. At the core of ORANSight-2.0 is RANSTRUCT, a novel Retrieval-Augmented Generation (RAG) based instruction-tuning framework that employs two LLM agents to create high-quality instruction-tuning datasets. The generated dataset is then used to fine-tune the 18 pre-trained open-source LLMs via QLoRA. To evaluate ORANSight-2.0, we introduce srsRANBench, a novel benchmark designed for code generation and codebase understanding in the context of srsRAN, a widely used 5G O-RAN stack. We also leverage ORANBench13K, an existing benchmark for assessing O-RAN-specific knowledge. Our comprehensive evaluations demonstrate that ORANSight-2.0 models outperform general-purpose and closed-source models, such as ChatGPT-4o and Gemini, by 5.421% on ORANBench and 18.465% on srsRANBench, achieving superior performance while maintaining lower computational and energy costs. We also experiment with RAG-augmented variants of ORANSight-2.0 LLMs and thoroughly evaluate their energy characteristics, demonstrating costs for training, standard inference, and RAG-augmented inference.', 'abstract_zh': '尽管大型语言模型（LLMs）在医疗保健、客户服务和商业营销等关键领域产生了变革性影响，但其与开放无线接入网络（O-RAN）的集成仍然有限。这一差距主要由于缺乏特定领域基础模型，现有的解决方案往往依赖于通用目的的LLM，这些模型未能解决O-RAN的独特挑战和技术复杂性。为弥合这一差距，我们介绍ORANSight-2.0（O-RAN洞察），这是旨在开发专门针对O-RAN的基础LLM的开创性举措。ORANSight-2.0基于18个覆盖五个开源LLM框架的LLM，调整了从1亿到70亿参数不等的模型，显著减少了对专有封闭源模型的依赖，同时提高了O-RAN的性能。ORANSight-2.0的核心是RANSTRUCT，这是一种新颖的检索增强生成（RAG）指令调优框架，采用两种LLM代理创建高质量的指令调优数据集。生成的数据集随后用于通过QLoRA调整18个预训练的开源LLM。为了评估ORANSight-2.0，我们引入了srsRANBench，这是一种针对srsRAN（广泛使用的5G O-RAN堆栈）的代码生成和代码库理解的新基准。我们还利用了ORANBench13K，这是一种现有的评估O-RAN特定知识的基准。我们的全面评估表明，ORANSight-2.0模型在ORANBench上的表现优于通用和封闭源模型，如ChatGPT-4o和Gemini，高出5.421%，在srsRANBench上的表现高出18.465%，同时实现了更高的性能并保持了较低的计算和能耗成本。我们还实验了ORANSight-2.0 LLM的RAG增强变体，并彻底评估了它们的能耗特性，包括训练、标准推理和RAG增强推理的成本。', 'title_zh': 'ORANSight-2.0: 基础性LLM for O-RAN'}
