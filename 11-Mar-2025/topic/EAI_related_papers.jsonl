{'arxiv_id': 'arXiv:2503.07576', 'title': 'Analyzing Symmetries of Swarms of Mobile Robots Using Equivariant Dynamical Systems', 'authors': 'Raphael Gerlach, Sören von der Gracht', 'link': 'https://arxiv.org/abs/2503.07576', 'abstract': 'In this article, we investigate symmetry properties of distributed systems of mobile robots. We consider a swarm of $n\\in\\mathbb{N}$ robots in the $\\mathcal{OBLOT}$ model and analyze their collective $\\mathcal{F}$sync dynamics using of equivariant dynamical systems theory. To this end, we show that the corresponding evolution function commutes with rotational and reflective transformations of $\\mathbb{R}^2$. These form a group that is isomorphic to $\\mathbf{O}(2) \\times S_n$, the product group of the orthogonal group and the permutation on $n$ elements. The theory of equivariant dynamical systems is used to deduce a hierarchy along which symmetries of a robot swarm can potentially increase following an arbitrary protocol. By decoupling the Look phase from the Compute and Move phases in the mathematical description of an LCM cycle, this hierarchy can be characterized in terms of automorphisms of connectivity graphs. In particular, we find all possible types of symmetry increase, if the decoupled Compute and Move phase is invertible. Finally, we apply our results to protocols which induce state-dependent linear dynamics, where the reduced system consisting of only the Compute and Move phase is linear.', 'abstract_zh': '分布式移动机器人系统的对称性性质研究：基于等变动力系统理论的群集同步动态分析', 'title_zh': '分析移动机器人群体的对称性using等变动力系统'}
{'arxiv_id': 'arXiv:2503.07557', 'title': 'AutoSpatial: Visual-Language Reasoning for Social Robot Navigation through Efficient Spatial Reasoning Learning', 'authors': 'Yangzhe Kong, Daeun Song, Jing Liang, Dinesh Manocha, Ziyu Yao, Xuesu Xiao', 'link': 'https://arxiv.org/abs/2503.07557', 'abstract': "We present a novel method, AutoSpatial, an efficient approach with structured spatial grounding to enhance VLMs' spatial reasoning. By combining minimal manual supervision with large-scale Visual Question-Answering (VQA) pairs auto-labeling, our approach tackles the challenge of VLMs' limited spatial understanding in social navigation tasks. By applying a hierarchical two-round VQA strategy during training, AutoSpatial achieves both global and detailed understanding of scenarios, demonstrating more accurate spatial perception, movement prediction, Chain of Thought (CoT) reasoning, final action, and explanation compared to other SOTA approaches. These five components are essential for comprehensive social navigation reasoning. Our approach was evaluated using both expert systems (GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet) that provided cross-validation scores and human evaluators who assigned relative rankings to compare model performances across four key aspects. Augmented by the enhanced spatial reasoning capabilities, AutoSpatial demonstrates substantial improvements by averaged cross-validation score from expert systems in: perception & prediction (up to 10.71%), reasoning (up to 16.26%), action (up to 20.50%), and explanation (up to 18.73%) compared to baseline models trained only on manually annotated data.", 'abstract_zh': '一种新型方法：AutoSpatial，一种结合结构化空间接地的高效途径，以增强VLMs的空间推理能力', 'title_zh': 'AutoSpatial: 通过高效的空间推理学习实现社会机器人导航的视觉-语言推理'}
{'arxiv_id': 'arXiv:2503.07547', 'title': 'Bi-Directional Mental Model Reconciliation for Human-Robot Interaction with Large Language Models', 'authors': 'Nina Moorman, Michelle Zhao, Matthew B. Luebbers, Sanne Van Waveren, Reid Simmons, Henny Admoni, Sonia Chernova, Matthew Gombolay', 'link': 'https://arxiv.org/abs/2503.07547', 'abstract': "In human-robot interactions, human and robot agents maintain internal mental models of their environment, their shared task, and each other. The accuracy of these representations depends on each agent's ability to perform theory of mind, i.e. to understand the knowledge, preferences, and intentions of their teammate. When mental models diverge to the extent that it affects task execution, reconciliation becomes necessary to prevent the degradation of interaction. We propose a framework for bi-directional mental model reconciliation, leveraging large language models to facilitate alignment through semi-structured natural language dialogue. Our framework relaxes the assumption of prior model reconciliation work that either the human or robot agent begins with a correct model for the other agent to align to. Through our framework, both humans and robots are able to identify and communicate missing task-relevant context during interaction, iteratively progressing toward a shared mental model.", 'abstract_zh': '人类与机器人交互中的双向心智模型 reconciliation 框架：利用大规模语言模型促进半结构化自然语言对话中的对齐', 'title_zh': '双向心智模型调和：面向大规模语言模型的人机交互'}
{'arxiv_id': 'arXiv:2503.07511', 'title': 'PointVLA: Injecting the 3D World into Vision-Language-Action Models', 'authors': 'Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, Yichen Zhu', 'link': 'https://arxiv.org/abs/2503.07511', 'abstract': 'Vision-Language-Action (VLA) models excel at robotic tasks by leveraging large-scale 2D vision-language pretraining, but their reliance on RGB images limits spatial reasoning critical for real-world interaction. Retraining these models with 3D data is computationally prohibitive, while discarding existing 2D datasets wastes valuable resources. To bridge this gap, we propose PointVLA, a framework that enhances pre-trained VLAs with point cloud inputs without requiring retraining. Our method freezes the vanilla action expert and injects 3D features via a lightweight modular block. To identify the most effective way of integrating point cloud representations, we conduct a skip-block analysis to pinpoint less useful blocks in the vanilla action expert, ensuring that 3D features are injected only into these blocks--minimizing disruption to pre-trained representations.\nExtensive experiments demonstrate that PointVLA outperforms state-of-the-art 2D imitation learning methods, such as OpenVLA, Diffusion Policy and DexVLA, across both simulated and real-world robotic tasks. Specifically, we highlight several key advantages of PointVLA enabled by point cloud integration: (1) Few-shot multi-tasking, where PointVLA successfully performs four different tasks using only 20 demonstrations each; (2) Real-vs-photo discrimination, where PointVLA distinguishes real objects from their images, leveraging 3D world knowledge to improve safety and reliability; (3) Height adaptability, Unlike conventional 2D imitation learning methods, PointVLA enables robots to adapt to objects at varying table height that unseen in train data. Furthermore, PointVLA achieves strong performance in long-horizon tasks, such as picking and packing objects from a moving conveyor belt, showcasing its ability to generalize across complex, dynamic environments.', 'abstract_zh': '基于点云的视觉-语言-动作（PointVLA）模型通过结合大规模2D视觉语言预训练在机器人任务中表现出色，但其依赖于RGB图像限制了其在现实世界交互中的空间推理能力。通过3D数据重新训练这些模型在计算上是不可行的，而丢弃现有的2D数据集则会浪费宝贵的资源。为此，我们提出了一种PointVLA框架，无需重新训练即可增强预训练的VLAs模型，并通过一个轻量级模块注入3D特征。为了确定集成点云表示的最有效方式，我们进行了跳块分析以识别范式动作专家中不太有用的块，从而确保仅将3D特征注入这些块中，以最小化对预训练表示的干扰。\n\n广泛的实验表明，PointVLA在模拟和真实世界机器人任务中均优于最先进的2Dimitation学习方法，如OpenVLA、Diffusion Policy和DexVLA。具体而言，PointVLA通过点云集成提供了几个关键优势：（1）少样本多任务处理，PointVLA仅使用每项任务20个演示就成功执行了四种不同任务；（2）真实物体与照片区分，PointVLA利用3D世界知识区分真实物体与其图像，从而提高安全性和可靠性；（3）不同高度适应性，不同于传统的2Dimitation学习方法，PointVLA使机器人能够适应训练数据中未见过的高度不同的物体。此外，PointVLA在长时间任务中表现出色，如从移动传送带上抓取和打包物体，展示了其在复杂动态环境中的泛化能力。', 'title_zh': 'PointVLA: 将三维世界注入视觉-语言-行动模型'}
{'arxiv_id': 'arXiv:2503.07481', 'title': 'Learning Physics-Based Full-Body Human Reaching and Grasping from Brief Walking References', 'authors': 'Yitang Li, Mingxian Lin, Zhuo Lin, Yipeng Deng, Yue Cao, Li Yi', 'link': 'https://arxiv.org/abs/2503.07481', 'abstract': 'Existing motion generation methods based on mocap data are often limited by data quality and coverage. In this work, we propose a framework that generates diverse, physically feasible full-body human reaching and grasping motions using only brief walking mocap data. Base on the observation that walking data captures valuable movement patterns transferable across tasks and, on the other hand, the advanced kinematic methods can generate diverse grasping poses, which can then be interpolated into motions to serve as task-specific guidance. Our approach incorporates an active data generation strategy to maximize the utility of the generated motions, along with a local feature alignment mechanism that transfers natural movement patterns from walking data to enhance both the success rate and naturalness of the synthesized motions. By combining the fidelity and stability of natural walking with the flexibility and generalizability of task-specific generated data, our method demonstrates strong performance and robust adaptability in diverse scenes and with unseen objects.', 'abstract_zh': '基于 mocap 数据生成多样且物理可行的全身人体抓取运动的框架', 'title_zh': '基于物理的全身体姿抓取学习从短暂行走参考中学习'}
{'arxiv_id': 'arXiv:2503.07404', 'title': 'Towards Safe Robot Foundation Models', 'authors': 'Maximilian Tölle, Theo Gruner, Daniel Palenicek, Jonas Günster, Puze Liu, Joe Watson, Davide Tateo, Jan Peters', 'link': 'https://arxiv.org/abs/2503.07404', 'abstract': "Robot foundation models hold the potential for deployment across diverse environments, from industrial applications to household tasks. While current research focuses primarily on the policies' generalization capabilities across a variety of tasks, it fails to address safety, a critical requirement for deployment on real-world systems. In this paper, we introduce a safety layer designed to constrain the action space of any generalist policy appropriately. Our approach uses ATACOM, a safe reinforcement learning algorithm that creates a safe action space and, therefore, ensures safe state transitions. By extending ATACOM to generalist policies, our method facilitates their deployment in safety-critical scenarios without requiring any specific safety fine-tuning. We demonstrate the effectiveness of this safety layer in an air hockey environment, where it prevents a puck-hitting agent from colliding with its surroundings, a failure observed in generalist policies.", 'abstract_zh': '机器人基础模型在从工业应用到家庭任务的多种环境中具有部署潜力。虽然当前研究主要关注策略在各种任务间的泛化能力，但未能解决安全性这一在实际系统部署中的关键要求。本文提出了一种安全层，旨在适当地约束任何通用策略的动作空间。我们的方法利用ATACOM安全强化学习算法，创建一个安全的动作空间，从而确保状态的安全部署。通过将ATACOM扩展到通用策略，我们的方法在其无需任何特定安全微调的情况下，能够促进其在安全关键场景中的部署。在桌上冰球环境中，我们展示了这种安全层的有效性，它防止了击球代理与周围环境发生碰撞，这是通用策略中观察到的一种失败情况。', 'title_zh': '面向安全的机器人基础模型研究'}
{'arxiv_id': 'arXiv:2503.07360', 'title': 'AffordDexGrasp: Open-set Language-guided Dexterous Grasp with Generalizable-Instructive Affordance', 'authors': 'Yi-Lin Wei, Mu Lin, Yuhao Lin, Jian-Jian Jiang, Xiao-Ming Wu, Ling-An Zeng, Wei-Shi Zheng', 'link': 'https://arxiv.org/abs/2503.07360', 'abstract': "Language-guided robot dexterous generation enables robots to grasp and manipulate objects based on human commands. However, previous data-driven methods are hard to understand intention and execute grasping with unseen categories in the open set. In this work, we explore a new task, Open-set Language-guided Dexterous Grasp, and find that the main challenge is the huge gap between high-level human language semantics and low-level robot actions. To solve this problem, we propose an Affordance Dexterous Grasp (AffordDexGrasp) framework, with the insight of bridging the gap with a new generalizable-instructive affordance representation. This affordance can generalize to unseen categories by leveraging the object's local structure and category-agnostic semantic attributes, thereby effectively guiding dexterous grasp generation. Built upon the affordance, our framework introduces Affordacne Flow Matching (AFM) for affordance generation with language as input, and Grasp Flow Matching (GFM) for generating dexterous grasp with affordance as input. To evaluate our framework, we build an open-set table-top language-guided dexterous grasp dataset. Extensive experiments in the simulation and real worlds show that our framework surpasses all previous methods in open-set generalization.", 'abstract_zh': '开放集语言引导灵巧抓取', 'title_zh': 'AffordDexGrasp: 开集语言引导的通用指导性抓取'}
{'arxiv_id': 'arXiv:2503.07340', 'title': 'Research and Design on Intelligent Recognition of Unordered Targets for Robots Based on Reinforcement Learning', 'authors': 'Yiting Mao, Dajun Tao, Shengyuan Zhang, Tian Qi, Keqin Li', 'link': 'https://arxiv.org/abs/2503.07340', 'abstract': 'In the field of robot target recognition research driven by artificial intelligence (AI), factors such as the disordered distribution of targets, the complexity of the environment, the massive scale of data, and noise interference have significantly restricted the improvement of target recognition accuracy. Against the backdrop of the continuous iteration and upgrading of current AI technologies, to meet the demand for accurate recognition of disordered targets by intelligent robots in complex and changeable scenarios, this study innovatively proposes an AI - based intelligent robot disordered target recognition method using reinforcement learning. This method processes the collected target images with the bilateral filtering algorithm, decomposing them into low - illumination images and reflection images. Subsequently, it adopts differentiated AI strategies, compressing the illumination images and enhancing the reflection images respectively, and then fuses the two parts of images to generate a new image. On this basis, this study deeply integrates deep learning, a core AI technology, with the reinforcement learning algorithm. The enhanced target images are input into a deep reinforcement learning model for training, ultimately enabling the AI - based intelligent robot to efficiently recognize disordered targets. Experimental results show that the proposed method can not only significantly improve the quality of target images but also enable the AI - based intelligent robot to complete the recognition task of disordered targets with higher efficiency and accuracy, demonstrating extremely high application value and broad development prospects in the field of AI robots.', 'abstract_zh': '基于人工智能的机器人乱序目标识别方法：借助强化学习的低照度与反射图像融合技术', 'title_zh': '基于强化学习的机器人无序目标智能识别研究与设计'}
{'arxiv_id': 'arXiv:2503.07338', 'title': 'Temporal Triplane Transformers as Occupancy World Models', 'authors': 'Haoran Xu, Peixi Peng, Guang Tan, Yiqian Chang, Yisen Zhao, Yonghong Tian', 'link': 'https://arxiv.org/abs/2503.07338', 'abstract': "Recent years have seen significant advances in world models, which primarily focus on learning fine-grained correlations between an agent's motion trajectory and the resulting changes in its surrounding environment. However, existing methods often struggle to capture such fine-grained correlations and achieve real-time predictions. To address this, we propose a new 4D occupancy world model for autonomous driving, termed T$^3$Former. T$^3$Former begins by pre-training a compact triplane representation that efficiently compresses the 3D semantically occupied environment. Next, T$^3$Former extracts multi-scale temporal motion features from the historical triplane and employs an autoregressive approach to iteratively predict the next triplane changes. Finally, T$^3$Former combines the triplane changes with the previous ones to decode them into future occupancy results and ego-motion trajectories. Experimental results demonstrate the superiority of T$^3$Former, achieving 1.44$\\times$ faster inference speed (26 FPS), while improving the mean IoU to 36.09 and reducing the mean absolute planning error to 1.0 meters.", 'abstract_zh': '最近几年，世界模型取得了显著进展，主要集中在学习智能体运动轨迹与其周围环境变化之间的精细关联。然而，现有方法往往难以捕捉这种精细关联并实现实时预测。为解决这一问题，我们提出了一种新的4D占位世界模型，名为T$^3$Former。T$^3$Former首先通过预训练一个紧凑的三平面表示来高效压缩3D语义占位环境。接着，T$^3$Former从历史三平面中提取多尺度时间运动特征，并采用自回归方法迭代预测下一个三平面的变化。最后，T$^3$Former将三平面变化与之前的进行组合，解码成未来占位结果和自运动轨迹。实验结果表明，T$^3$Former在保持较高精度的同时，实现了1.44倍的推理速度提升（26 FPS），同时使平均IoU提升至36.09，平均绝对规划误差降低至1.0米。', 'title_zh': '时空三视图变换器作为 occupancy 世界模型'}
{'arxiv_id': 'arXiv:2503.07312', 'title': 'Bioinspired Sensing of Undulatory Flow Fields Generated by Leg Kicks in Swimming', 'authors': 'Jun Wang, Tongsheng Shen, Dexin Zhao, Feitian Zhang', 'link': 'https://arxiv.org/abs/2503.07312', 'abstract': "The artificial lateral line (ALL) is a bioinspired flow sensing system for underwater robots, comprising of distributed flow sensors. The ALL has been successfully applied to detect the undulatory flow fields generated by body undulation and tail-flapping of bioinspired robotic fish. However, its feasibility and performance in sensing the undulatory flow fields produced by human leg kicks during swimming has not been systematically tested and studied. This paper presents a novel sensing framework to investigate the undulatory flow field generated by swimmer's leg kicks, leveraging bioinspired ALL sensing. To evaluate the feasibility of using the ALL system for sensing the undulatory flow fields generated by swimmer leg kicks, this paper designs an experimental platform integrating an ALL system and a lab-fabricated human leg model. To enhance the accuracy of flow sensing, this paper proposes a feature extraction method that dynamically fuses time-domain and time-frequency characteristics. Specifically, time-domain features are extracted using one-dimensional convolutional neural networks and bidirectional long short-term memory networks (1DCNN-BiLSTM), while time-frequency features are extracted using short-term Fourier transform and two-dimensional convolutional neural networks (STFT-2DCNN). These features are then dynamically fused based on attention mechanisms to achieve accurate sensing of the undulatory flow field. Furthermore, extensive experiments are conducted to test various scenarios inspired by human swimming, such as leg kick pattern recognition and kicking leg localization, achieving satisfactory results.", 'abstract_zh': '基于生物启发的人工侧线系统的泳姿腿部踢动诱导波动流场感知框架', 'title_zh': '仿生感知游泳过程中腿踢产生的波动流场'}
{'arxiv_id': 'arXiv:2503.07238', 'title': 'Learning and planning for optimal synergistic human-robot coordination in manufacturing contexts', 'authors': 'Samuele Sandrini, Marco Faroni, Nicola Pedrocchi', 'link': 'https://arxiv.org/abs/2503.07238', 'abstract': "Collaborative robotics cells leverage heterogeneous agents to provide agile production solutions. Effective coordination is essential to prevent inefficiencies and risks for human operators working alongside robots. This paper proposes a human-aware task allocation and scheduling model based on Mixed Integer Nonlinear Programming to optimize efficiency and safety starting from task planning stages. The approach exploits synergies that encode the coupling effects between pairs of tasks executed in parallel by the agents, arising from the safety constraints imposed on robot agents. These terms are learned from previous executions using a Bayesian estimation; the inference of the posterior probability distribution of the synergy coefficients is performed using the Markov Chain Monte Carlo method. The synergy enhances task planning by adapting the nominal duration of the plan according to the effect of the operator's presence. Simulations and experimental results demonstrate that the proposed method produces improved human-aware task plans, reducing unuseful interference between agents, increasing human-robot distance, and achieving up to an 18\\% reduction in process execution time.", 'abstract_zh': '协作机器人细胞利用异构代理提供敏捷生产解决方案。基于混合整数非线性规划的人机aware任务分配与调度模型在任务规划阶段优化效率与安全。该方法利用编码了作业组合效用的协同效应，这些效应源自对机器人作业的安全约束，并通过贝叶斯估计从先前执行中学习；后验概率分布的推断使用马尔可夫链蒙特卡洛方法进行。协同作用通过根据操作员存在效应调整计划的名义持续时间来增强任务规划。仿真和实验结果表明，所提方法生成了更合适的人机aware任务计划，减少了代理之间的无用干扰，增加了人机距离，并实现了最高18%的工艺执行时间减少。', 'title_zh': '学习与规划以实现最优协同的人机制造协作'}
{'arxiv_id': 'arXiv:2503.07111', 'title': 'PoseLess: Depth-Free Vision-to-Joint Control via Direct Image Mapping with VLM', 'authors': 'Alan Dao, Dinh Bach Vu, Tuan Le Duc Anh, Bui Quang Huy', 'link': 'https://arxiv.org/abs/2503.07111', 'abstract': 'This paper introduces PoseLess, a novel framework for robot hand control that eliminates the need for explicit pose estimation by directly mapping 2D images to joint angles using tokenized representations. Our approach leverages synthetic training data generated through randomized joint configurations, enabling zero-shot generalization to real-world scenarios and cross-morphology transfer from robotic to human hands. By tokenizing visual inputs and employing a transformer-based decoder, PoseLess achieves robust, low-latency control while addressing challenges such as depth ambiguity and data scarcity. Experimental results demonstrate competitive performance in joint angle prediction accuracy without relying on any human-labelled dataset.', 'abstract_zh': 'PoseLess：一种无需显式姿态估计的机器人手控制新型框架', 'title_zh': 'PoseLess: 无需深度的视图到关节控制直接图像映射ewith VLM'}
{'arxiv_id': 'arXiv:2503.07049', 'title': 'VMTS: Vision-Assisted Teacher-Student Reinforcement Learning for Multi-Terrain Locomotion in Bipedal Robots', 'authors': 'Fu Chen, Rui Wan, Peidong Liu, Nanxing Zheng, Bo Zhou', 'link': 'https://arxiv.org/abs/2503.07049', 'abstract': "Bipedal robots, due to their anthropomorphic design, offer substantial potential across various applications, yet their control is hindered by the complexity of their structure. Currently, most research focuses on proprioception-based methods, which lack the capability to overcome complex terrain. While visual perception is vital for operation in human-centric environments, its integration complicates control further. Recent reinforcement learning (RL) approaches have shown promise in enhancing legged robot locomotion, particularly with proprioception-based methods. However, terrain adaptability, especially for bipedal robots, remains a significant challenge, with most research focusing on flat-terrain scenarios. In this paper, we introduce a novel mixture of experts teacher-student network RL strategy, which enhances the performance of teacher-student policies based on visual inputs through a simple yet effective approach. Our method combines terrain selection strategies with the teacher policy, resulting in superior performance compared to traditional models. Additionally, we introduce an alignment loss between the teacher and student networks, rather than enforcing strict similarity, to improve the student's ability to navigate diverse terrains. We validate our approach experimentally on the Limx Dynamic P1 bipedal robot, demonstrating its feasibility and robustness across multiple terrain types.", 'abstract_zh': '双足机器人由于其类人的设计，在各类应用中具备巨大潜力，但其控制受到结构复杂性的阻碍。目前，大多研究集中在基于 proprioception 的方法上，这些方法难以克服复杂地形。尽管视觉感知对于人在中心环境的操作至关重要，但其集成会进一步增加控制的复杂性。最近的强化学习（RL）方法在提高-legged机器人行走能力方面显示出前景，特别是在基于 proprioception 的方法上。然而，地形适应性，尤其是对于双足机器人，仍然是一个重大挑战，目前大多数研究集中在平坦地形场景上。本文我们提出了一种新颖的专家混合教师-学生网络 RL 策略，通过一个简单而有效的方法增强了基于视觉输入的教师-学生策略的表现。我们的方法结合了地形选择策略与教师策略，相比传统模型表现出更优的性能。此外，我们引入了教师网络与学生网络之间的对齐损失，而非强制严格相似性，以提高学生网络在多变地形下的导航能力。我们在 Limx Dynamic P1 双足机器人上进行了实验验证，展示了其在多种地形类型下的可行性和鲁棒性。', 'title_zh': '视觉辅助师徒强化学习在双足机器人多地形运动中的应用'}
{'arxiv_id': 'arXiv:2503.07017', 'title': 'How to Train Your Robots? The Impact of Demonstration Modality on Imitation Learning', 'authors': 'Haozhuo Li, Yuchen Cui, Dorsa Sadigh', 'link': 'https://arxiv.org/abs/2503.07017', 'abstract': 'Imitation learning is a promising approach for learning robot policies with user-provided data. The way demonstrations are provided, i.e., demonstration modality, influences the quality of the data. While existing research shows that kinesthetic teaching (physically guiding the robot) is preferred by users for the intuitiveness and ease of use, the majority of existing manipulation datasets were collected through teleoperation via a VR controller or spacemouse. In this work, we investigate how different demonstration modalities impact downstream learning performance as well as user experience. Specifically, we compare low-cost demonstration modalities including kinesthetic teaching, teleoperation with a VR controller, and teleoperation with a spacemouse controller. We experiment with three table-top manipulation tasks with different motion constraints. We evaluate and compare imitation learning performance using data from different demonstration modalities, and collected subjective feedback on user experience. Our results show that kinesthetic teaching is rated the most intuitive for controlling the robot and provides cleanest data for best downstream learning performance. However, it is not preferred as the way for large-scale data collection due to the physical load. Based on such insight, we propose a simple data collection scheme that relies on a small number of kinesthetic demonstrations mixed with data collected through teleoperation to achieve the best overall learning performance while maintaining low data-collection effort.', 'abstract_zh': '基于不同演示模态的模仿学习研究：从用户提供的数据中学习机器人策略', 'title_zh': '如何训练你的机器人？示范模态对模仿学习的影响'}
{'arxiv_id': 'arXiv:2503.06995', 'title': 'Physics-informed Neural Network Predictive Control for Quadruped Locomotion', 'authors': 'Haolin Li, Yikang Chai, Bailin Lv, Lecheng Ruan, Hang Zhao, Ye Zhao, Jianwen Luo', 'link': 'https://arxiv.org/abs/2503.06995', 'abstract': "This study introduces a unified control framework that addresses the challenge of precise quadruped locomotion with unknown payloads, named as online payload identification-based physics-informed neural network predictive control (OPI-PINNPC). By integrating online payload identification with physics-informed neural networks (PINNs), our approach embeds identified mass parameters directly into the neural network's loss function, ensuring physical consistency while adapting to changing load conditions. The physics-constrained neural representation serves as an efficient surrogate model within our nonlinear model predictive controller, enabling real-time optimization despite the complex dynamics of legged locomotion. Experimental validation on our quadruped robot platform demonstrates 35% improvement in position and orientation tracking accuracy across diverse payload conditions (25-100 kg), with substantially faster convergence compared to previous adaptive control methods. Our framework provides a adaptive solution for maintaining locomotion performance under variable payload conditions without sacrificing computational efficiency.", 'abstract_zh': '基于在线负载识别的物理约束神经网络预测控制（OPI-PINNPC）的统一控制框架', 'title_zh': '基于物理信息的神经网络预测控制在四足运动中的应用'}
{'arxiv_id': 'arXiv:2503.06937', 'title': 'Handle Object Navigation as Weighted Traveling Repairman Problem', 'authors': 'Ruimeng Liu, Xinhang Xu, Shenghai Yuan, Lihua Xie', 'link': 'https://arxiv.org/abs/2503.06937', 'abstract': 'Zero-Shot Object Navigation (ZSON) requires agents to navigate to objects specified via open-ended natural language without predefined categories or prior environmental knowledge. While recent methods leverage foundation models or multi-modal maps, they often rely on 2D representations and greedy strategies or require additional training or modules with high computation load, limiting performance in complex environments and real applications. We propose WTRP-Searcher, a novel framework that formulates ZSON as a Weighted Traveling Repairman Problem (WTRP), minimizing the weighted waiting time of viewpoints. Using a Vision-Language Model (VLM), we score viewpoints based on object-description similarity, projected onto a 2D map with depth information. An open-vocabulary detector identifies targets, dynamically updating goals, while a 3D embedding feature map enhances spatial awareness and environmental recall. WTRP-Searcher outperforms existing methods, offering efficient global planning and improved performance in complex ZSON tasks. Code and more demos will be avaliable on this https URL.', 'abstract_zh': '零样本对象导航（ZSON）要求智能体依据开放领域自然语言描述导航至指定对象，无需预定义类别或先验环境知识。尽管近期方法利用基础模型或多模态地图，但它们常常依赖2D表示和贪婪策略，或需要额外训练和具有高计算负载的模块，从而在复杂环境和实际应用中限制了性能。我们提出了WTRP-Searcher，一种将ZSON形式化为加权旅行修复员问题（WTRP）的新框架，以最小化视点的加权等待时间。利用视觉-语言模型（VLM），我们基于对象描述相似性对视点进行评分，并投影到具有深度信息的2D地图上。开放词汇检测器识别目标，动态更新目标，而3D嵌入特征图增强空间感知和环境记忆。WTRP-Searcher在现有方法中表现出色，提供高效的全局规划并改善了复杂ZSON任务的性能。相关代码和更多演示可在以下链接获取：this https URL。', 'title_zh': '处理对象导航问题作为加权旅行修理工问题'}
{'arxiv_id': 'arXiv:2503.06866', 'title': 'Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety Perception', 'authors': 'Wanjing Huang, Tongjie Pan, Yalan Ye', 'link': 'https://arxiv.org/abs/2503.06866', 'abstract': 'Recent advancements in large language models (LLMs) have expanded their role in robotic task planning. However, while LLMs have been explored for generating feasible task sequences, their ability to ensure safe task execution remains underdeveloped. Existing methods struggle with structured risk perception, making them inadequate for safety-critical applications where low-latency hazard adaptation is required. To address this limitation, we propose a Graphormer-enhanced risk-aware task planning framework that combines LLM-based decision-making with structured safety modeling. Our approach constructs a dynamic spatio-semantic safety graph, capturing spatial and contextual risk factors to enable online hazard detection and adaptive task refinement. Unlike existing methods that rely on predefined safety constraints, our framework introduces a context-aware risk perception module that continuously refines safety predictions based on real-time task execution. This enables a more flexible and scalable approach to robotic planning, allowing for adaptive safety compliance beyond static rules. To validate our framework, we conduct experiments in the AI2-THOR environment. The experiments results validates improvements in risk detection accuracy, rising safety notice, and task adaptability of our framework in continuous environments compared to static rule-based and LLM-only baselines. Our project is available at this https URL', 'abstract_zh': '最近大语言模型（LLMs）的进步扩展了其在机器人任务规划中的作用。然而，尽管LLMs已被探索用于生成可行的任务序列，但它们确保安全任务执行的能力仍然不够完善。现有方法在结构化风险感知方面存在困难，这使它们在需要低延迟危险适应的安全关键应用中显得不足。为解决这一局限，我们提出了一种结合基于LLM的决策和结构化安全建模的Graphormer增强的风险感知任务规划框架。我们的方法构建了一个动态空间语义安全图，捕获空间和上下文风险因素，以实现在线危险检测和自适应任务优化。与依赖预定义安全约束的现有方法不同，我们的框架引入了一个上下文感知的风险感知模块，该模块能够根据实时任务执行持续细化安全预测。这使机器人规划方法更加灵活和可扩展，允许超越静态规则的自适应安全合规性。为了验证我们的框架，我们在AI2-THOR环境中进行了实验。实验结果验证了我们的框架在连续环境中相比静态规则基础和仅LLM基准的改进在风险检测准确性、安全提示和任务适应性方面的提升。我们的项目可在以下链接访问：this https URL', 'title_zh': 'Graphormer引导的任务规划：超越静态规则的LLM安全性感知'}
{'arxiv_id': 'arXiv:2503.06831', 'title': 'One-Shot Dual-Arm Imitation Learning', 'authors': 'Yilong Wang, Edward Johns', 'link': 'https://arxiv.org/abs/2503.06831', 'abstract': 'We introduce One-Shot Dual-Arm Imitation Learning (ODIL), which enables dual-arm robots to learn precise and coordinated everyday tasks from just a single demonstration of the task. ODIL uses a new three-stage visual servoing (3-VS) method for precise alignment between the end-effector and target object, after which replay of the demonstration trajectory is sufficient to perform the task. This is achieved without requiring prior task or object knowledge, or additional data collection and training following the single demonstration. Furthermore, we propose a new dual-arm coordination paradigm for learning dual-arm tasks from a single demonstration. ODIL was tested on a real-world dual-arm robot, demonstrating state-of-the-art performance across six precise and coordinated tasks in both 4-DoF and 6-DoF settings, and showing robustness in the presence of distractor objects and partial occlusions. Videos are available at: this https URL.', 'abstract_zh': 'One-Shot 双臂模仿学习 (ODIL): 仅从单一示范学习精确协调的双臂日常任务', 'title_zh': '单次学习双臂模仿学习'}
{'arxiv_id': 'arXiv:2503.06814', 'title': 'Unlocking Generalization for Robotics via Modularity and Scale', 'authors': 'Murtaza Dalal', 'link': 'https://arxiv.org/abs/2503.06814', 'abstract': 'How can we build generalist robot systems? Scale may not be enough due to the significant multimodality of robotics tasks, lack of easily accessible data and the challenges of deploying on physical hardware. Meanwhile, most deployed robotic systems today are inherently modular and can leverage the independent generalization capabilities of each module to perform well. Therefore, this thesis seeks to tackle the task of building generalist robot agents by integrating these components into one: combining modularity with large-scale learning for general purpose robot control. The first question we consider is: how can we build modularity and hierarchy into learning systems? Our key insight is that rather than having the agent learn hierarchy and low-level control end-to-end, we can enforce modularity via planning to enable more efficient and capable robot learners. Next, we come to the role of scale in building generalist robot systems. To scale, neural networks require vast amounts of diverse data, expressive architectures to fit the data and a source of supervision to generate the data. We leverage a powerful supervision source: classical planning, which can generalize, but is expensive to run and requires access to privileged information to perform well in practice. We use these planners to supervise large-scale policy learning in simulation to produce generalist agents. Finally, we consider how to unify modularity with large-scale policy learning to build real-world robot systems capable of performing zero-shot manipulation. We do so by tightly integrating key ingredients of modular high and mid-level planning, learned local control, procedural scene generation and large-scale policy learning for sim2real transfer. We demonstrate that this recipe can produce a single, generalist agent that can solve challenging long-horizon manipulation tasks in the real world.', 'abstract_zh': '如何构建通才机器人系统？由于机器人任务的高度多模态性、易于获取的数据缺乏以及在物理硬件上的部署挑战，规模可能并不足够。与此同时，当前部署的大多数机器人系统本质上是模块化的，可以通过利用每个模块的独立泛化能力来有效地执行任务。因此，本论文旨在通过将模块化与大规模学习结合起来，构建通用用途的机器人控制，来解决构建通才机器人代理的问题。首先，我们考虑的问题是：如何将模块化和层次结构构建到学习系统中？我们的关键见解是，与其让代理端到端学习层次结构和低级控制，我们可以通过规划来强制实现模块化，从而实现更高效的机器人学习者。接着，我们探讨了构建通才机器人系统中的规模问题。为了实现规模扩展，神经网络需要大量的多样化数据、能很好地拟合数据的表达性架构以及数据生成的监督源。我们利用一个强大的监督源：经典规划，它可以泛化但运行成本高，并且需要访问特权信息才能在实践中表现良好。我们使用这些规划器在仿真环境中监督大规模策略学习，从而生成通才代理。最后，我们考虑如何将模块化与大规模策略学习统一起来，以构建能够在现实世界中执行零样本操作的机器人系统。我们通过紧密整合模块化高层和中层规划的关键要素、学习局部控制、过程化场景生成和大规模策略学习，来实现从仿真到现实的迁移。我们展示了这一结合可以产生一个单一的通才代理，该代理能在现实世界中解决具有挑战性的长时间操作任务。', 'title_zh': '通过模块化和规模实现机器人领域的泛化能力解锁'}
{'arxiv_id': 'arXiv:2503.06791', 'title': 'AutoMisty: A Multi-Agent LLM Framework for Automated Code Generation in the Misty Social Robot', 'authors': 'Xiao Wang, Lu Dong, Sahana Rangasrinivasan, Ifeoma Nwogu, Srirangaraj Setlur, Venugopal Govindaraju', 'link': 'https://arxiv.org/abs/2503.06791', 'abstract': "The social robot's open API allows users to customize open-domain interactions. However, it remains inaccessible to those without programming experience. In this work, we introduce AutoMisty, the first multi-agent collaboration framework powered by large language models (LLMs), to enable the seamless generation of executable Misty robot code from natural language instructions. AutoMisty incorporates four specialized agent modules to manage task decomposition, assignment, problem-solving, and result synthesis. Each agent incorporates a two-layer optimization mechanism, with self-reflection for iterative refinement and human-in-the-loop for better alignment with user preferences. AutoMisty ensures a transparent reasoning process, allowing users to iteratively refine tasks through natural language feedback for precise execution. To evaluate AutoMisty's effectiveness, we designed a benchmark task set spanning four levels of complexity and conducted experiments in a real Misty robot environment. Extensive evaluations demonstrate that AutoMisty not only consistently generates high-quality code but also enables precise code control, significantly outperforming direct reasoning with ChatGPT-4o and ChatGPT-o1. All code, optimized APIs, and experimental videos will be publicly released through the webpage: this https URL", 'abstract_zh': '基于大型语言模型的多代理协作框架AutoMisty：自然语言指令生成可执行Misty机器人代码', 'title_zh': 'AutoMisty: 一种基于Misty社交机器人的多代理大语言模型自动化代码生成框架'}
{'arxiv_id': 'arXiv:2503.06771', 'title': 'Task-Oriented Connectivity for Networked Robotics with Generative AI and Semantic Communications', 'authors': 'Peizheng Li, Adnan Aijaz', 'link': 'https://arxiv.org/abs/2503.06771', 'abstract': 'The convergence of robotics, advanced communication networks, and artificial intelligence (AI) holds the promise of transforming industries through fully automated and intelligent operations. In this work, we introduce a novel co-working framework for robots that unifies goal-oriented semantic communication (SemCom) with a Generative AI (GenAI)-agent under a semantic-aware network. SemCom prioritizes the exchange of meaningful information among robots and the network, thereby reducing overhead and latency. Meanwhile, the GenAI-agent leverages generative AI models to interpret high-level task instructions, allocate resources, and adapt to dynamic changes in both network and robotic environments. This agent-driven paradigm ushers in a new level of autonomy and intelligence, enabling complex tasks of networked robots to be conducted with minimal human intervention. We validate our approach through a multi-robot anomaly detection use-case simulation, where robots detect, compress, and transmit relevant information for classification. Simulation results confirm that SemCom significantly reduces data traffic while preserving critical semantic details, and the GenAI-agent ensures task coordination and network adaptation. This synergy provides a robust, efficient, and scalable solution for modern industrial environments.', 'abstract_zh': '机器人、先进通信网络和人工智能融合的共融框架：基于语义感知网络的目标导向语义通信与生成式AI代理一体化方法', 'title_zh': '任务导向连通性：结合生成AI与语义通信的网络化机器人技术'}
{'arxiv_id': 'arXiv:2503.06733', 'title': 'Embodied multi-modal sensing with a soft modular arm powered by physical reservoir computing', 'authors': 'Jun Wang, Suyi Li', 'link': 'https://arxiv.org/abs/2503.06733', 'abstract': "Soft robots have become increasingly popular for complex manipulation tasks requiring gentle and safe contact. However, their softness makes accurate control challenging, and high-fidelity sensing is a prerequisite to adequate control performance. To this end, many flexible and embedded sensors have been created over the past decade, but they inevitably increase the robot's complexity and stiffness. This study demonstrates a novel approach that uses simple bending strain gauges embedded inside a modular arm to extract complex information regarding its deformation and working conditions. The core idea is based on physical reservoir computing (PRC): A soft body's rich nonlinear dynamic responses, captured by the inter-connected bending sensor network, could be utilized for complex multi-modal sensing with a simple linear regression algorithm. Our results show that the soft modular arm reservoir can accurately predict body posture (bending angle), estimate payload weight, determine payload orientation, and even differentiate two payloads with only minimal difference in weight -- all using minimal digital computing power.", 'abstract_zh': '软体机器人的复杂操作任务需要温柔安全的接触，但其柔软性使得精确控制极具挑战性，高保真感知是实现充足控制性能的前提。为此，近年来开发了许多柔性和嵌入式传感器，但它们不可避免地增加了机器人的复杂性和刚性。本研究展示了一种新颖的方法，即在模块化手臂内部嵌入简单弯曲应变片以提取其变形和工作状态的复杂信息。核心思想基于物理蓄流计算（PRC）：通过互联的弯曲传感器网络捕捉软体生物体丰富的非线性动态响应，可以使用简单的线性回归算法实现复杂多模态感知。我们的研究表明，软模块臂蓄流器能够准确预测身体姿态（弯曲角度）、估计负载重量、确定负载方向，并且在重量仅有微小差异的情况下还能区分两个负载，这全部只需最少的数字计算能力。', 'title_zh': '软模块化机械臂基于物理存储池计算的动力化多模态传感'}
{'arxiv_id': 'arXiv:2503.06669', 'title': 'AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems', 'authors': 'AgiBot-World-Contributors, Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xu Huang, Shu Jiang, Yuxin Jiang, Cheng Jing, Hongyang Li, Jialu Li, Chiming Liu, Yi Liu, Yuxiang Lu, Jianlan Luo, Ping Luo, Yao Mu, Yuehan Niu, Yixuan Pan, Jiangmiao Pang, Yu Qiao, Guanghui Ren, Cheng Ruan, Jiaqi Shan, Yongjian Shen, Chengshi Shi, Mingkang Shi, Modi Shi, Chonghao Sima, Jianheng Song, Huijie Wang, Wenhao Wang, Dafeng Wei, Chengen Xie, Guo Xu, Junchi Yan, Cunbiao Yang, Lei Yang, Shukai Yang, Maoqing Yao, Jia Zeng, Chi Zhang, Qinglin Zhang, Bin Zhao, Chengyue Zhao, Jiaqi Zhao, Jianchao Zhu', 'link': 'https://arxiv.org/abs/2503.06669', 'abstract': 'We explore how scalable robot data can address real-world challenges for generalized robotic manipulation. Introducing AgiBot World, a large-scale platform comprising over 1 million trajectories across 217 tasks in five deployment scenarios, we achieve an order-of-magnitude increase in data scale compared to existing datasets. Accelerated by a standardized collection pipeline with human-in-the-loop verification, AgiBot World guarantees high-quality and diverse data distribution. It is extensible from grippers to dexterous hands and visuo-tactile sensors for fine-grained skill acquisition. Building on top of data, we introduce Genie Operator-1 (GO-1), a novel generalist policy that leverages latent action representations to maximize data utilization, demonstrating predictable performance scaling with increased data volume. Policies pre-trained on our dataset achieve an average performance improvement of 30% over those trained on Open X-Embodiment, both in in-domain and out-of-distribution scenarios. GO-1 exhibits exceptional capability in real-world dexterous and long-horizon tasks, achieving over 60% success rate on complex tasks and outperforming prior RDT approach by 32%. By open-sourcing the dataset, tools, and models, we aim to democratize access to large-scale, high-quality robot data, advancing the pursuit of scalable and general-purpose intelligence.', 'abstract_zh': '我们探索可扩展的机器人数据如何解决通用机器人操作的实际挑战。通过引入包含超过100万条轨迹的AgiBot World平台，该平台涵盖了五种部署场景下的217项任务，我们实现了与现有数据集相比量级的数据规模提升。借助标准化的数据收集管道和人工在环验证，AgiBot World确保了高质量和多样性的数据分布。该平台支持从抓取器扩展到灵巧手以及视觉-触觉传感器，以实现精细技能的学习。基于数据，我们提出了Genie Operator-1（GO-1），这是一种新颖的一般主义策略，利用潜在动作表示来最大化数据利用，展示了随数据量增加而可预测的性能提升。在我们的数据集上预训练的策略在同域和异域场景下的性能平均提高了30%，优于Open X-Embodiment。GO-1在现实世界的灵巧和长期任务中表现出色，复杂任务的成功率超过60%，比之前的RDT方法性能高出32%。通过开源数据集、工具和模型，我们旨在普及大规模高质量机器人数据的使用，推动可扩展和通用人工智能的发展。', 'title_zh': 'AgiBot 世界角斗场：大规模 manipulation 平台，用于可扩展和智能的具身系统'}
{'arxiv_id': 'arXiv:2503.06359', 'title': 'Deep Reinforcement Learning-Based Semi-Autonomous Control for Magnetic Micro-robot Navigation with Immersive Manipulation', 'authors': 'Yudong Mao, Dandan Zhang', 'link': 'https://arxiv.org/abs/2503.06359', 'abstract': 'Magnetic micro-robots have demonstrated immense potential in biomedical applications, such as in vivo drug delivery, non-invasive diagnostics, and cell-based therapies, owing to their precise maneuverability and small size. However, current micromanipulation techniques often rely solely on a two-dimensional (2D) microscopic view as sensory feedback, while traditional control interfaces do not provide an intuitive manner for operators to manipulate micro-robots. These limitations increase the cognitive load on operators, who must interpret limited feedback and translate it into effective control actions. To address these challenges, we propose a Deep Reinforcement Learning-Based Semi-Autonomous Control (DRL-SC) framework for magnetic micro-robot navigation in a simulated microvascular system. Our framework integrates Mixed Reality (MR) to facilitate immersive manipulation of micro-robots, thereby enhancing situational awareness and control precision. Simulation and experimental results demonstrate that our approach significantly improves navigation efficiency, reduces control errors, and enhances the overall robustness of the system in simulated microvascular environments.', 'abstract_zh': '基于深度 reinforcement 学习的混合现实辅助磁微机器人自主导航框架', 'title_zh': '基于深度强化学习的半自主控制方法及其在沉浸式微磁机器人导航中的应用'}
{'arxiv_id': 'arXiv:2503.06309', 'title': 'On the Fly Adaptation of Behavior Tree-Based Policies through Reinforcement Learning', 'authors': 'Marco Iannotta, Johannes A. Stork, Erik Schaffernicht, Todor Stoyanov', 'link': 'https://arxiv.org/abs/2503.06309', 'abstract': 'With the rising demand for flexible manufacturing, robots are increasingly expected to operate in dynamic environments where local -- such as slight offsets or size differences in workpieces -- are common. We propose to address the problem of adapting robot behaviors to these task variations with a sample-efficient hierarchical reinforcement learning approach adapting Behavior Tree (BT)-based policies. We maintain the core BT properties as an interpretable, modular framework for structuring reactive behaviors, but extend their use beyond static tasks by inherently accommodating local task variations. To show the efficiency and effectiveness of our approach, we conduct experiments both in simulation and on a Franka Emika Panda 7-DoF, with the manipulator adapting to different obstacle avoidance and pivoting tasks.', 'abstract_zh': '随着对灵活制造的需求不断增长，机器人被期望在局部任务差异（如工件的轻微偏移或尺寸差异）常见的动态环境中操作。我们提出了一种样本高效层次强化学习方法，通过调整行为树（BT）基于的策略来解决机器人行为适应这些任务变化的问题。我们保留行为树的核心属性作为可解释的模块化框架来结构化反应性行为，并通过固有地适应局部任务变化来扩展其用途。为了展示我们方法的效率和有效性，我们在仿真和Franka Emika Panda 7-DoF机械臂上进行了实验，机械臂适应了不同的障碍物规避和旋转任务。', 'title_zh': '基于强化学习的行为树策略-flyadaptation'}
{'arxiv_id': 'arXiv:2503.06300', 'title': 'Efficient Gradient-Based Inference for Manipulation Planning in Contact Factor Graphs', 'authors': 'Jeongmin Lee, Sunkyung Park, Minji Lee, Dongjun Lee', 'link': 'https://arxiv.org/abs/2503.06300', 'abstract': 'This paper presents a framework designed to tackle a range of planning problems arise in manipulation, which typically involve complex geometric-physical reasoning related to contact and dynamic constraints. We introduce the Contact Factor Graph (CFG) to graphically model these diverse factors, enabling us to perform inference on the graphs to approximate the distribution and sample appropriate solutions. We propose a novel approach that can incorporate various phenomena of contact manipulation as differentiable factors, and develop an efficient inference algorithm for CFG that leverages this differentiability along with the conditional probabilities arising from the structured nature of contact. Our results demonstrate the capability of our framework in generating viable samples and approximating posterior distributions for various manipulation scenarios.', 'abstract_zh': '本文提出了一种框架，用于解决 manipulation 中出现的一系列规划问题，这些问题通常涉及复杂的几何-物理推理，包括接触和动态约束。我们引入了接触因子图（CFG）来图形化建模这些多样化的因素，从而能够在图上进行推理以近似分布并采样合适的解。我们提出了一种新的方法，可以将接触操作的各种现象作为可微分因子进行整合，并开发了一种利用这些可微分性和接触结构化性质产生的条件概率的高效推理算法。我们的结果表明，该框架在各种操作场景中生成有效样本和近似后验分布的能力。', 'title_zh': '基于梯度的高效推理在接触因子图中的操作规划'}
{'arxiv_id': 'arXiv:2503.06241', 'title': 'A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment', 'authors': 'Koji Inoue, Yuki Okafuji, Jun Baba, Yoshiki Ohira, Katsuya Hyodo, Tatsuya Kawahara', 'link': 'https://arxiv.org/abs/2503.06241', 'abstract': 'Turn-taking is a crucial aspect of human-robot interaction, directly influencing conversational fluidity and user engagement. While previous research has explored turn-taking models in controlled environments, their robustness in real-world settings remains underexplored. In this study, we propose a noise-robust voice activity projection (VAP) model, based on a Transformer architecture, to enhance real-time turn-taking in dialogue robots. To evaluate the effectiveness of the proposed system, we conducted a field experiment in a shopping mall, comparing the VAP system with a conventional cloud-based speech recognition system. Our analysis covered both subjective user evaluations and objective behavioral analysis. The results showed that the proposed system significantly reduced response latency, leading to a more natural conversation where both the robot and users responded faster. The subjective evaluations suggested that faster responses contribute to a better interaction experience.', 'abstract_zh': '人类-机器人对话轮换是影响对话流畅性和用户参与度的关键因素，在现实世界环境中，先前研究提出的轮换模型的 robust 性仍然有待探索。本文提出基于变压器架构的噪声鲁棒语音活动投影（VAP）模型，以增强对话机器人的实时轮换能力。为了评估该系统的效果，我们在购物中心进行了实地实验，将 VAP 系统与传统的云基语音识别系统进行了比较。分析涵盖了主观用户评价和客观行为分析。结果表明，所提出系统显著减少了响应延迟，使对话更加自然，机器人和用户都能更快地响应。这种更快速的响应被主观评价认为改善了交互体验。', 'title_zh': '一种适用于真实对话机器人的抗噪声轮流说话系统：一个实地实验'}
{'arxiv_id': 'arXiv:2503.06227', 'title': 'GAT-Grasp: Gesture-Driven Affordance Transfer for Task-Aware Robotic Grasping', 'authors': 'Ruixiang Wang, Huayi Zhou, Xinyue Yao, Guiliang Liu, Kui Jia', 'link': 'https://arxiv.org/abs/2503.06227', 'abstract': 'Achieving precise and generalizable grasping across diverse objects and environments is essential for intelligent and collaborative robotic systems. However, existing approaches often struggle with ambiguous affordance reasoning and limited adaptability to unseen objects, leading to suboptimal grasp execution. In this work, we propose GAT-Grasp, a gesture-driven grasping framework that directly utilizes human hand gestures to guide the generation of task-specific grasp poses with appropriate positioning and orientation. Specifically, we introduce a retrieval-based affordance transfer paradigm, leveraging the implicit correlation between hand gestures and object affordances to extract grasping knowledge from large-scale human-object interaction videos. By eliminating the reliance on pre-given object priors, GAT-Grasp enables zero-shot generalization to novel objects and cluttered environments. Real-world evaluations confirm its robustness across diverse and unseen scenarios, demonstrating reliable grasp execution in complex task settings.', 'abstract_zh': '实现多样物体和环境下的精确且通用的抓取是智能化协作机器人系统的关键。然而，现有方法往往在模糊的用途推理和对未见物体的适应性方面存在局限，导致抓取执行效果不佳。为此，我们提出了一种基于手势的抓取框架GAT-Grasp，该框架直接利用人类手部手势来引导生成具有适当定位和方向的任务特定抓取姿态。具体而言，我们引入了一种检索驱动的用途转移 paradigm，利用手部手势与物体用途之间的隐式关联，从大规模的人机交互视频中抽取抓取知识。通过消除对先验物体知识的依赖，GAT-Grasp 实现了对新型物体和杂乱环境的零样本泛化。实际世界评估证实了其在各种未见过的复杂场景中的稳健性，展示了在复杂任务设置中可靠的抓取执行能力。', 'title_zh': 'GAT-Grasp: 基于手势驱动的适应任务的抓取精度传递'}
{'arxiv_id': 'arXiv:2503.06135', 'title': 'FlowMP: Learning Motion Fields for Robot Planning with Conditional Flow Matching', 'authors': 'Khang Nguyen, An T. Le, Tien Pham, Manfred Huber, Jan Peters, Minh Nhat Vu', 'link': 'https://arxiv.org/abs/2503.06135', 'abstract': 'Prior flow matching methods in robotics have primarily learned velocity fields to morph one distribution of trajectories into another. In this work, we extend flow matching to capture second-order trajectory dynamics, incorporating acceleration effects either explicitly in the model or implicitly through the learning objective. Unlike diffusion models, which rely on a noisy forward process and iterative denoising steps, flow matching trains a continuous transformation (flow) that directly maps a simple prior distribution to the target trajectory distribution without any denoising procedure. By modeling trajectories with second-order dynamics, our approach ensures that generated robot motions are smooth and physically executable, avoiding the jerky or dynamically infeasible trajectories that first-order models might produce. We empirically demonstrate that this second-order conditional flow matching yields superior performance on motion planning benchmarks, achieving smoother trajectories and higher success rates than baseline planners. These findings highlight the advantage of learning acceleration-aware motion fields, as our method outperforms existing motion planning methods in terms of trajectory quality and planning success.', 'abstract_zh': '机器人领域的先验流匹配方法主要学习速度场以将一个轨迹分布转换为另一个分布。在本工作中，我们将流匹配扩展到捕获轨迹的二阶动力学，通过显式地在模型中或通过学习目标隐式地纳入加速度效果。与依赖于噪声前向过程和去噪步骤的扩散模型不同，流匹配训练一个连续变换（流），直接将简单的先验分布映射为目标轨迹分布，而无需任何去噪过程。通过建模二阶动力学，我们的方法确保生成的机器人运动平滑且物理可执行，避免了由一阶模型可能产生的生硬或动力学不可行的轨迹。我们实验证明，这种二阶条件流匹配在运动规划基准测试中表现出更优性能，生成更平滑的轨迹并具有更高的成功率。这些发现强调了学习加速度感知运动场的优势，我们的方法在轨迹质量和规划成功率方面优于现有运动规划方法。', 'title_zh': 'FlowMP：基于条件流匹配的学习运动场方法用于机器人规划'}
{'arxiv_id': 'arXiv:2503.06083', 'title': 'T-CBF: Traversability-based Control Barrier Function to Navigate Vertically Challenging Terrain', 'authors': 'Manas Gupta, Xuesu Xiao', 'link': 'https://arxiv.org/abs/2503.06083', 'abstract': 'Safety has been of paramount importance in motion planning and control techniques and is an active area of research in the past few years. Most safety research for mobile robots target at maintaining safety with the notion of collision avoidance. However, safety goes beyond just avoiding collisions, especially when robots have to navigate unstructured, vertically challenging, off-road terrain, where vehicle rollover and immobilization is as critical as collisions. In this work, we introduce a novel Traversability-based Control Barrier Function (T-CBF), in which we use neural Control Barrier Functions (CBFs) to achieve safety beyond collision avoidance on unstructured vertically challenging terrain by reasoning about new safety aspects in terms of traversability. The neural T-CBF trained on safe and unsafe observations specific to traversability safety is then used to generate safe trajectories. Furthermore, we present experimental results in simulation and on a physical Verti-4 Wheeler (V4W) platform, demonstrating that T-CBF can provide traversability safety while reaching the goal position. T-CBF planner outperforms previously developed planners by 30\\% in terms of keeping the robot safe and mobile when navigating on real world vertically challenging terrain.', 'abstract_zh': '基于通行性的控制壁垒函数在垂直挑战性非结构化地形上的安全性研究', 'title_zh': '基于通过性控制屏障函数的导航垂直挑战地形方法'}
{'arxiv_id': 'arXiv:2503.06060', 'title': 'STAR: A Foundation Model-driven Framework for Robust Task Planning and Failure Recovery in Robotic Systems', 'authors': 'Md Sadman Sakib, Yu Sun', 'link': 'https://arxiv.org/abs/2503.06060', 'abstract': "Modern robotic systems, deployed across domains from industrial automation to domestic assistance, face a critical challenge: executing tasks with precision and adaptability in dynamic, unpredictable environments. To address this, we propose STAR (Smart Task Adaptation and Recovery), a novel framework that synergizes Foundation Models (FMs) with dynamically expanding Knowledge Graphs (KGs) to enable resilient task planning and autonomous failure recovery. While FMs offer remarkable generalization and contextual reasoning, their limitations, including computational inefficiency, hallucinations, and output inconsistencies hinder reliable deployment. STAR mitigates these issues by embedding learned knowledge into structured, reusable KGs, which streamline information retrieval, reduce redundant FM computations, and provide precise, scenario-specific insights. The framework leverages FM-driven reasoning to diagnose failures, generate context-aware recovery strategies, and execute corrective actions without human intervention or system restarts. Unlike conventional approaches that rely on rigid protocols, STAR dynamically expands its KG with experiential knowledge, ensuring continuous adaptation to novel scenarios. To evaluate the effectiveness of this approach, we developed a comprehensive dataset that includes various robotic tasks and failure scenarios. Through extensive experimentation, STAR demonstrated an 86% task planning accuracy and 78% recovery success rate, showing significant improvements over baseline methods. The framework's ability to continuously learn from experience while maintaining structured knowledge representation makes it particularly suitable for long-term deployment in real-world applications.", 'abstract_zh': '现代机器人系统在工业自动化到家庭辅助等多个领域部署，面临着一个关键挑战：在动态的、不可预测的环境中精确而灵活地执行任务。为应对这一挑战，我们提出了STAR（智能任务适应与恢复）框架，该框架结合了基础模型（FMs）与动态扩展的知识图谱（KGs），以实现鲁棒的任务规划和自主故障恢复。虽然FMs在泛化和上下文推理方面表现出色，但其计算效率低下、幻觉和输出不一致等问题限制了其可靠部署。STAR通过将学习的知识嵌入到结构化且可重复使用的知识图谱中来缓解这些问题，从而简化信息检索，减少重复的FM计算，并提供精确的、特定场景的洞察。该框架利用基础模型驱动的推理来诊断故障、生成情境感知的恢复策略，并在无需人工干预或系统重启的情况下执行纠正措施。不同于依赖于刚性协议的常规方法，STAR动态扩展其知识图谱以获取经验知识，确保能够持续适应新颖场景。为了评估该方法的有效性，我们开发了一个全面的数据集，其中包括各种机器人任务和故障场景。通过广泛的实验，STAR展示了86%的任务规划准确率和78%的恢复成功率，显示出相对于基线方法有显著提高。该框架具备在经验中持续学习同时保持结构化知识表示的能力，使其特别适合在实际应用中的长期部署。', 'title_zh': 'STAR：一种基于基础模型的任务规划与故障恢复框架在机器人系统中的鲁棒实现'}
{'arxiv_id': 'arXiv:2503.06050', 'title': 'Energy-Efficient Motion Planner for Legged Robots', 'authors': 'Alexander Schperberg, Marcel Menner, Stefano Di Cairano', 'link': 'https://arxiv.org/abs/2503.06050', 'abstract': "We propose an online motion planner for legged robot locomotion with the primary objective of achieving energy efficiency. The conceptual idea is to leverage a placement set of footstep positions based on the robot's body position to determine when and how to execute steps. In particular, the proposed planner uses virtual placement sets beneath the hip joints of the legs and executes a step when the foot is outside of such placement set. Furthermore, we propose a parameter design framework that considers both energy-efficiency and robustness measures to optimize the gait by changing the shape of the placement set along with other parameters, such as step height and swing time, as a function of walking speed. We show that the planner produces trajectories that have a low Cost of Transport (CoT) and high robustness measure, and evaluate our approach against model-free Reinforcement Learning (RL) and motion imitation using biological dog motion priors as the reference. Overall, within low to medium velocity range, we show a 50.4% improvement in CoT and improved robustness over model-free RL, our best performing baseline. Finally, we show ability to handle slippery surfaces, gait transitions, and disturbances in simulation and hardware with the Unitree A1 robot.", 'abstract_zh': '一种基于腿部位置的在线运动规划算法：实现能效与鲁棒性的优化', 'title_zh': '腿式机器人节能运动规划'}
{'arxiv_id': 'arXiv:2503.06026', 'title': 'Zero-Shot Peg Insertion: Identifying Mating Holes and Estimating SE(2) Poses with Vision-Language Models', 'authors': 'Masaru Yajima, Kei Ota, Asako Kanezaki, Rei Kawakami', 'link': 'https://arxiv.org/abs/2503.06026', 'abstract': 'Achieving zero-shot peg insertion, where inserting an arbitrary peg into an unseen hole without task-specific training, remains a fundamental challenge in robotics. This task demands a highly generalizable perception system capable of detecting potential holes, selecting the correct mating hole from multiple candidates, estimating its precise pose, and executing insertion despite uncertainties. While learning-based methods have been applied to peg insertion, they often fail to generalize beyond the specific peg-hole pairs encountered during training. Recent advancements in Vision-Language Models (VLMs) offer a promising alternative, leveraging large-scale datasets to enable robust generalization across diverse tasks. Inspired by their success, we introduce a novel zero-shot peg insertion framework that utilizes a VLM to identify mating holes and estimate their poses without prior knowledge of their geometry. Extensive experiments demonstrate that our method achieves 90.2% accuracy, significantly outperforming baselines in identifying the correct mating hole across a wide range of previously unseen peg-hole pairs, including 3D-printed objects, toy puzzles, and industrial connectors. Furthermore, we validate the effectiveness of our approach in a real-world connector insertion task on a backpanel of a PC, where our system successfully detects holes, identifies the correct mating hole, estimates its pose, and completes the insertion with a success rate of 88.3%. These results highlight the potential of VLM-driven zero-shot reasoning for enabling robust and generalizable robotic assembly.', 'abstract_zh': '实现零样本 peg 插入，即在无特定任务训练的情况下将任意 peg 插入未见过的孔中，仍然是机器人技术中的一个基本挑战。这项任务要求具备高度泛化能力的感知系统，能够检测潜在的孔，从多个候选孔中选择正确的匹配孔，估计其精确的姿态，并在存在不确定性的情况下执行插入。虽然基于学习的方法已被应用于 peg 插入，但它们往往无法在超出训练过程中遇到的特定 peg-孔对的情况下泛化。近期视觉-语言模型（VLMs）的进展提供了另一种有前景的替代方案，利用大规模数据集在多种任务上实现稳健泛化。受其成功启发，我们提出了一种新的零样本 peg 插入框架，利用 VLM 无需先验几何知识即可识别匹配孔并估计其姿态。广泛实验表明，我们的方法在识别正确匹配孔方面达到了 90.2% 的准确率，在各种以前未见过的 peg-孔对中，包括 3D 打印对象、玩具拼图和工业连接器，显著优于基线方法。此外，我们还在 PC 后面板的连接器插入任务中验证了我们方法的有效性，我们的系统成功地检测孔、识别正确的匹配孔、估计其姿态，并以 88.3% 的成功率完成插入。这些结果凸显了 VLM 驱动的零样本推理在实现稳健和泛化的机器人装配方面的潜力。', 'title_zh': '零样本钉钉插入：基于视觉-语言模型的接合孔识别与SE(2)姿态估计'}
{'arxiv_id': 'arXiv:2503.05997', 'title': 'Learning to Drive by Imitating Surrounding Vehicles', 'authors': 'Yasin Sonmez, Hanna Krasowski, Murat Arcak', 'link': 'https://arxiv.org/abs/2503.05997', 'abstract': "Imitation learning is a promising approach for training autonomous vehicles (AV) to navigate complex traffic environments by mimicking expert driver behaviors. However, a major challenge in this paradigm lies in effectively utilizing available driving data, as collecting new data is resource-intensive and often limited in its ability to cover diverse driving scenarios. While existing imitation learning frameworks focus on leveraging expert demonstrations, they often overlook the potential of additional complex driving data from surrounding traffic participants. In this paper, we propose a data augmentation strategy that enhances imitation learning by leveraging the observed trajectories of nearby vehicles, captured through the AV's sensors, as additional expert demonstrations. We introduce a vehicle selection sampling strategy that prioritizes informative and diverse driving behaviors, contributing to a richer and more diverse dataset for training. We evaluate our approach using the state-of-the-art learning-based planning method PLUTO on the nuPlan dataset and demonstrate that our augmentation method leads to improved performance in complex driving scenarios. Specifically, our method reduces collision rates and improves safety metrics compared to the baseline. Notably, even when using only 10% of the original dataset, our method achieves performance comparable to that of the full dataset, with improved collision rates. Our findings highlight the importance of leveraging diverse real-world trajectory data in imitation learning and provide insights into data augmentation strategies for autonomous driving.", 'abstract_zh': '模仿学习是一种有前景的方法，通过模仿专家驾驶员行为来训练自动驾驶车辆（AV）在复杂交通环境中导航。然而，这一范式中存在的主要挑战在于有效利用可用的驾驶数据，因为收集新数据成本高昂且往往难以涵盖多种多样的驾驶场景。虽然现有的模仿学习框架侧重于利用专家演示，但往往会忽视周围交通参与者提供的复杂驾驶数据的潜力。在本文中，我们提出了一种数据增强策略，该策略通过利用自动驾驶车辆传感器捕获的附近车辆的观测轨迹，作为额外的专家演示来增强模仿学习。我们引入了一种车辆选择采样策略，该策略优先选择具有信息性和多样性的驾驶行为，从而为训练提供更丰富和多样化的数据集。我们使用最先进的基于学习的规划方法PLUTO在nuPlan数据集上评估了我们的方法，并且表明我们的增强方法在复杂驾驶场景中表现出更好的性能。具体来说，我们的方法减少了碰撞率并提高了安全指标，与基线方法相比。值得注意的是，即使仅使用原始数据集的10%，我们的方法在碰撞率方面也达到了与完整数据集相当的性能。我们的研究结果强调了在模仿学习中利用多样化的实际轨迹数据的重要性，并为自动驾驶中的数据增强策略提供了见解。', 'title_zh': '通过模仿周围车辆学习驾驶'}
{'arxiv_id': 'arXiv:2503.05887', 'title': 'MatchMaker: Automated Asset Generation for Robotic Assembly', 'authors': 'Yian Wang, Bingjie Tang, Chuang Gan, Dieter Fox, Kaichun Mo, Yashraj Narang, Iretiayo Akinola', 'link': 'https://arxiv.org/abs/2503.05887', 'abstract': 'Robotic assembly remains a significant challenge due to complexities in visual perception, functional grasping, contact-rich manipulation, and performing high-precision tasks. Simulation-based learning and sim-to-real transfer have led to recent success in solving assembly tasks in the presence of object pose variation, perception noise, and control error; however, the development of a generalist (i.e., multi-task) agent for a broad range of assembly tasks has been limited by the need to manually curate assembly assets, which greatly constrains the number and diversity of assembly problems that can be used for policy learning. Inspired by recent success of using generative AI to scale up robot learning, we propose MatchMaker, a pipeline to automatically generate diverse, simulation-compatible assembly asset pairs to facilitate learning assembly skills. Specifically, MatchMaker can 1) take a simulation-incompatible, interpenetrating asset pair as input, and automatically convert it into a simulation-compatible, interpenetration-free pair, 2) take an arbitrary single asset as input, and generate a geometrically-mating asset to create an asset pair, 3) automatically erode contact surfaces from (1) or (2) according to a user-specified clearance parameter to generate realistic parts. We demonstrate that data generated by MatchMaker outperforms previous work in terms of diversity and effectiveness for downstream assembly skill learning. For videos and additional details, please see our project website: this https URL.', 'abstract_zh': '基于视觉感知、功能抓取、高接触操作以及执行高精度任务的复杂性，机器人装配仍是一项重大挑战。基于模拟的学习和从模拟到现实的迁移已成功解决了因物体姿态变化、感知噪声和控制误差导致的装配任务；然而，开发适用于广泛装配任务的通用型（即多任务）智能体受到手动编排装配资源的限制，大大制约了可用的装配问题数量及其多样性，从而影响策略学习。受使用生成式AI扩大机器人学习规模的最新成果启发，我们提出MatchMaker，一种自动生成多样化、可模拟装配资产对的管道，以便于装配技能的学习。具体而言，MatchMaker可以1）将一个不兼容模拟、相互穿插的资产对作为输入，并自动转换为兼容模拟、无穿插的资产对；2）将任意单个资产作为输入，生成几何互补的资产以创建资产对；3）根据用户指定的间隙参数，自动对（1）或（2）中的接触面进行侵蚀，生成具有现实感的部件。我们证明，由MatchMaker生成的数据在下游装配技能学习方面在多样性和效果上优于先前的工作。更多视频和细节，请参见我们的项目网站：this https URL。', 'title_zh': 'MatchMaker：自动资产生成用于机器人装配'}
{'arxiv_id': 'arXiv:2503.05833', 'title': 'Refined Policy Distillation: From VLA Generalists to RL Experts', 'authors': 'Tobias Jülg, Wolfram Burgard, Florian Walter', 'link': 'https://arxiv.org/abs/2503.05833', 'abstract': "Recent generalist Vision-Language-Action Models (VLAs) can perform a variety of tasks on real robots with remarkable generalization capabilities. However, reported success rates are often not on par with those of expert policies. Moreover, VLAs usually do not work out of the box and often must be fine-tuned as they are sensitive to setup changes. In this work, we present Refined Policy Distillation (RPD), an RL-based policy refinement method that enables the distillation of large generalist models into small, high-performing expert policies. The student policy is guided during the RL exploration by actions of a teacher VLA for increased sample efficiency and faster convergence. Different from previous work that focuses on applying VLAs to real-world experiments, we create fine-tuned versions of Octo and OpenVLA for ManiSkill2 to evaluate RPD in simulation. As our results for different manipulation tasks demonstrate, RPD enables the RL agent to learn expert policies that surpass the teacher's performance in both dense and sparse reward settings. Our approach is even robust to changes in the camera perspective and can generalize to task variations that the underlying VLA cannot solve.", 'abstract_zh': 'recent 通用视觉-语言-行动模型 (VLAs) 在现实机器人上可以执行多种任务，并展现出显著的泛化能力。然而，报道的成功率往往不如专家策略。此外，VLAs 通常无法开箱即用，并且往往需要微调，因为它们对环境设置变化极为敏感。在本工作中，我们提出了一种基于强化学习 (RL) 的策略精炼方法 Refined Policy Distillation (RPD)，该方法能够将大规模通用模型精炼为小规模、高性能的专家策略。学生策略在 RL 探索过程中由教师 VLA 的行动引导，以提高样本效率并加快收敛速度。与以往专注于将 VLAs 应用于实际实验的工作不同，我们为 ManiSkill2 创建了 Octo 和 OpenVLA 的微调版本，以评估 RPD 在仿真中的效果。我们的结果显示，在不同的操作任务中，RPD 使 RL 剂能够学习出超越教师策略性能的专家策略，无论是在稠密奖励设置还是稀疏奖励设置中。此外，我们的方法甚至对相机视角的变化具有鲁棒性，并且能够泛化到基础 VLA 无法解决的任务变化。', 'title_zh': '精炼的策略蒸馏：从多领域专家到RL专家'}
{'arxiv_id': 'arXiv:2503.05825', 'title': 'A Human-In-The-Loop Simulation Framework for Evaluating Control Strategies in Gait Assistive Robots', 'authors': 'Yifan Wang, Sherwin Stephen Chan, Mingyuan Lei, Lek Syn Lim, Henry Johan, Bingran Zuo, Wei Tech Ang', 'link': 'https://arxiv.org/abs/2503.05825', 'abstract': "As the global population ages, effective rehabilitation and mobility aids will become increasingly critical. Gait assistive robots are promising solutions, but designing adaptable controllers for various impairments poses a significant challenge. This paper presented a Human-In-The-Loop (HITL) simulation framework tailored specifically for gait assistive robots, addressing unique challenges posed by passive support systems. We incorporated a realistic physical human-robot interaction (pHRI) model to enable a quantitative evaluation of robot control strategies, highlighting the performance of a speed-adaptive controller compared to a conventional PID controller in maintaining compliance and reducing gait distortion. We assessed the accuracy of the simulated interactions against that of the real-world data and revealed discrepancies in the adaptation strategies taken by the human and their effect on the human's gait. This work underscored the potential of HITL simulation as a versatile tool for developing and fine-tuning personalized control policies for various users.", 'abstract_zh': '随着全球人口老龄化，有效的康复和移动辅助工具将变得越来越重要。步态辅助机器人是很有前景的解决方案，但设计适用于各种障碍的适应性控制器面临着重大挑战。本文介绍了一个针对步态辅助机器人的循环多人在回路（HITL）仿真框架，专门解决了被动支持系统所带来的独特挑战。我们整合了一个真实的物理人机交互（pHRI）模型，以定量评估机器人控制策略，突出了速度自适应控制器与传统PID控制器在保持顺应性和减少步态扭曲方面的性能差异。我们评估了模拟交互的准确性与现实世界数据的准确性，并揭示了人类在适应策略上的差异及其对人类步态的影响。本项工作强调了HITL仿真作为开发和微调适用于各种用户个性化控制策略的多功能工具的潜力。', 'title_zh': '带有闭环的人机仿真框架：评估辅助行走机器人控制策略的有效性'}
{'arxiv_id': 'arXiv:2503.07608', 'title': 'AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning', 'authors': 'Bo Jiang, Shaoyu Chen, Qian Zhang, Wenyu Liu, Xinggang Wang', 'link': 'https://arxiv.org/abs/2503.07608', 'abstract': 'OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing a crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities. Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs a two-stage planning reasoning training strategy that combines SFT with RL. As a result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning. Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research.', 'abstract_zh': 'AlphaDrive：一种结合RL和推理的 Vision-Language 模型自主驾驶框架', 'title_zh': 'AlphaDrive：通过强化学习和推理释放大规模预训练模型在自动驾驶中的潜力'}
{'arxiv_id': 'arXiv:2503.07376', 'title': 'AttentionSwarm: Reinforcement Learning with Attention Control Barier Function for Crazyflie Drones in Dynamic Environments', 'authors': 'Grik Tadevosyan, Valerii Serpiva, Aleksey Fedoseev, Roohan Ahmed Khan, Demetros Aschu, Faryal Batool, Nickolay Efanov, Artem Mikhaylov, Dzmitry Tsetserukou', 'link': 'https://arxiv.org/abs/2503.07376', 'abstract': 'We introduce AttentionSwarm, a novel benchmark designed to evaluate safe and efficient swarm control across three challenging environments: a landing environment with obstacles, a competitive drone game setting, and a dynamic drone racing scenario. Central to our approach is the Attention Model Based Control Barrier Function (CBF) framework, which integrates attention mechanisms with safety-critical control theory to enable real-time collision avoidance and trajectory optimization. This framework dynamically prioritizes critical obstacles and agents in the swarms vicinity using attention weights, while CBFs formally guarantee safety by enforcing collision-free constraints. The safe attention net algorithm was developed and evaluated using a swarm of Crazyflie 2.1 micro quadrotors, which were tested indoors with the Vicon motion capture system to ensure precise localization and control. Experimental results show that our system achieves landing accuracy of 3.02 cm with a mean time of 23 s and collision-free landings in a dynamic landing environment, 100% and collision-free navigation in a drone game environment, and 95% and collision-free navigation for a dynamic multiagent drone racing environment, underscoring its effectiveness and robustness in real-world scenarios. This work offers a promising foundation for applications in dynamic environments where safety and fastness are paramount.', 'abstract_zh': '我们引入了AttentionSwarm，这是一种新型基准，用于评估在三个具有挑战性的环境中安全且高效的群控制：包含障碍物的降落环境、竞争型无人机游戏设置以及动态无人机竞速场景。我们方法的核心是基于注意力机制的控制障碍函数（CBF）框架，该框架将注意力机制与关键的安全控制理论相结合，以实现实时碰撞 avoidance 和轨迹优化。该框架通过使用注意力权重动态优先处理群周围的关键障碍物和个体，同时通过施加无碰撞约束条件来正式保证安全性。安全注意力网络算法是在室内使用Vicon运动捕捉系统测距的Crazyflie 2.1 微型四旋翼无人机群上开发和验证的。实验结果表明，我们的系统在动态降落环境中的着陆精度为3.02 cm，平均时间为23秒，并实现了无碰撞着陆；在无人机游戏环境中实现了100%的无碰撞导航；在动态多智能体无人机竞速环境中实现了95%的无碰撞导航，证明了其在实际场景中的有效性和鲁棒性。这项工作为在安全性和快速性至关重要的动态环境中应用提供了有前景的基础。', 'title_zh': 'AttentionSwarm: 基于注意力控制屏障函数的动态环境中 Crazyflie 无人机的强化学习'}
{'arxiv_id': 'arXiv:2503.07319', 'title': 'Human Machine Co-Adaptation Model and Its Convergence Analysis', 'authors': 'Steven W. Su, Yaqi Li, Kairui Guo, Rob Duffield', 'link': 'https://arxiv.org/abs/2503.07319', 'abstract': "The key to robot-assisted rehabilitation lies in the design of the human-machine interface, which must accommodate the needs of both patients and machines. Current interface designs primarily focus on machine control algorithms, often requiring patients to spend considerable time adapting. In this paper, we introduce a novel approach based on the Cooperative Adaptive Markov Decision Process (CAMDPs) model to address the fundamental aspects of the interactive learning process, offering theoretical insights and practical guidance. We establish sufficient conditions for the convergence of CAMDPs and ensure the uniqueness of Nash equilibrium points. Leveraging these conditions, we guarantee the system's convergence to a unique Nash equilibrium point. Furthermore, we explore scenarios with multiple Nash equilibrium points, devising strategies to adjust both Value Evaluation and Policy Improvement algorithms to enhance the likelihood of converging to the global minimal Nash equilibrium point. Through numerical experiments, we illustrate the effectiveness of the proposed conditions and algorithms, demonstrating their applicability and robustness in practical settings. The proposed conditions for convergence and the identification of a unique optimal Nash equilibrium contribute to the development of more effective adaptive systems for human users in robot-assisted rehabilitation.", 'abstract_zh': '机器人辅助康复的关键在于人性化界面的设计，必须兼顾患者和机器的需求。目前的界面设计主要集中在机器控制算法上，通常需要患者花费大量时间进行适应。在本文中，我们引入了一种基于合作自适应马尔可夫决策过程（CAMDPs）模型的新方法，以解决交互学习过程中的基本问题，提供理论洞察和实用指导。我们建立了CAMDPs收敛的充分条件，并确保纳什均衡点的唯一性。利用这些条件，我们保证系统的收敛到唯一的纳什均衡点。此外，我们探讨了存在多个纳什均衡点的情景，并制定策略调整价值评估和策略改进算法，以提高收敛到全局最小纳什均衡点的概率。通过数值实验，我们展示了所提条件和算法的有效性，证明它们在实际应用中的适用性和鲁棒性。所提的收敛条件和唯一的最优纳什均衡点的识别为机器人辅助康复中更具效性的自适应系统开发做出了贡献。', 'title_zh': '人类机器共适应模型及其收敛性分析'}
{'arxiv_id': 'arXiv:2503.06960', 'title': 'A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning', 'authors': 'Xin Wen, Bingchen Zhao, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi', 'link': 'https://arxiv.org/abs/2503.06960', 'abstract': 'Pre-trained vision models (PVMs) are fundamental to modern robotics, yet their optimal configuration remains unclear. Through systematic evaluation, we find that while DINO and iBOT outperform MAE across visuomotor control and perception tasks, they struggle when trained on non-(single-)object-centric (NOC) data--a limitation strongly correlated with their diminished ability to learn object-centric representations. This investigation indicates that the ability to form object-centric representations from the non-object-centric robotics dataset is the key to success for PVMs. Motivated by this discovery, we designed SlotMIM, a method that induces object-centric representations by introducing a semantic bottleneck to reduce the number of prototypes to encourage the emergence of objectness as well as cross-view consistency regularization for encouraging multiview invariance. Our experiments encompass pre-training on object-centric, scene-centric, web-crawled, and ego-centric data. Across all settings, our approach learns transferrable representations and achieves significant improvements over prior work in image recognition, scene understanding, and robot learning evaluations. When scaled up with million-scale datasets, our method also demonstrates superior data efficiency and scalability. Our code and models are publicly available at this https URL.', 'abstract_zh': '预训练视觉模型（PVMs）是现代机器人技术的基础，但其最优配置尚不明确。通过系统评估，我们发现虽然DINO和iBOT在视觉运动控制和感知任务中优于MAE，但在非物体中心（NOC）数据上的训练上表现出色受限——这一限制与其学习物体中心表示的能力下降紧密相关。这项研究表明，从非物体中心的机器人数据集中形成物体中心表示的能力是PVMs成功的关键。受这一发现的启发，我们设计了SlotMIM方法，通过引入语义瓶颈减少原型的数量，以促进物体性的出现以及跨视图一致性正则化以促进多视图不变性。我们的实验涵盖了基于物体中心、场景中心、网络爬取和自我中心数据的预训练。在所有设置中，我们的方法学习到可迁移的表示，并在图像识别、场景理解以及机器人学习评估中实现了显著改进。当扩展到大规模数据集时，我们的方法还展示了更高的数据效率和可扩展性。我们的代码和模型已在该网址公开。', 'title_zh': '基于数据的预训练视觉模型在机器人学习中的 revisit'}
{'arxiv_id': 'arXiv:2503.06313', 'title': 'Advancing Autonomous Vehicle Intelligence: Deep Learning and Multimodal LLM for Traffic Sign Recognition and Robust Lane Detection', 'authors': 'Chandan Kumar Sah, Ankit Kumar Shaw, Xiaoli Lian, Arsalan Shahid Baig, Tuopu Wen, Kun Jiang, Mengmeng Yang, Diange Yang', 'link': 'https://arxiv.org/abs/2503.06313', 'abstract': 'Autonomous vehicles (AVs) require reliable traffic sign recognition and robust lane detection capabilities to ensure safe navigation in complex and dynamic environments. This paper introduces an integrated approach combining advanced deep learning techniques and Multimodal Large Language Models (MLLMs) for comprehensive road perception. For traffic sign recognition, we systematically evaluate ResNet-50, YOLOv8, and RT-DETR, achieving state-of-the-art performance of 99.8% with ResNet-50, 98.0% accuracy with YOLOv8, and achieved 96.6% accuracy in RT-DETR despite its higher computational complexity. For lane detection, we propose a CNN-based segmentation method enhanced by polynomial curve fitting, which delivers high accuracy under favorable conditions. Furthermore, we introduce a lightweight, Multimodal, LLM-based framework that directly undergoes instruction tuning using small yet diverse datasets, eliminating the need for initial pretraining. This framework effectively handles various lane types, complex intersections, and merging zones, significantly enhancing lane detection reliability by reasoning under adverse conditions. Despite constraints in available training resources, our multimodal approach demonstrates advanced reasoning capabilities, achieving a Frame Overall Accuracy (FRM) of 53.87%, a Question Overall Accuracy (QNS) of 82.83%, lane detection accuracies of 99.6% in clear conditions and 93.0% at night, and robust performance in reasoning about lane invisibility due to rain (88.4%) or road degradation (95.6%). The proposed comprehensive framework markedly enhances AV perception reliability, thus contributing significantly to safer autonomous driving across diverse and challenging road scenarios.', 'abstract_zh': '自动驾驶车辆需要可靠的交通标志识别和稳健的车道检测能力，以确保在复杂和动态环境中的安全导航。本文介绍了一种结合先进深度学习技术和多模态大规模语言模型（MLLMs）的综合方法，用于全面的道路感知。在交通标志识别方面，我们系统评估了ResNet-50、YOLOv8和RT-DETR，分别取得了99.8%、98.0%和96.6%的准确率。对于车道检测，我们提出了一种基于CNN的分割方法，并通过多项式曲线拟合进行增强，在有利条件下实现了高精度。此外，我们介绍了一种轻量级、多模态的基于LLM的框架，该框架直接通过使用小而多样化的数据集进行指令调优，无需初始预训练。该框架能够有效处理各种车道类型、复杂交叉口和合流区，通过在恶劣条件下的推理显著提高了车道检测的可靠性。尽管存在可用训练资源的限制，我们的多模态方法展示了高级推理能力，实现了53.87%的Frame Overall Accuracy (FRM)、82.83%的Question Overall Accuracy (QNS)，以及在清晰条件下99.6%、夜间93.0%的车道检测准确率，并在雨天（88.4%）或路面降级（95.6%）条件下表现出稳健的关于车道不可见性的推理性能。所提出的整体框架显著增强了自动驾驶车辆的道路感知可靠性，从而在各种复杂的道路场景中为更安全的自动驾驶做出了重要贡献。', 'title_zh': '增强自主车辆智能：深度学习与多模态LLM在交通标志识别和稳健车道检测中的应用'}
{'arxiv_id': 'arXiv:2503.06170', 'title': 'Object-Centric World Model for Language-Guided Manipulation', 'authors': 'Youngjoon Jeong, Junha Chun, Soonwoo Cha, Taesup Kim', 'link': 'https://arxiv.org/abs/2503.06170', 'abstract': 'A world model is essential for an agent to predict the future and plan in domains such as autonomous driving and robotics. To achieve this, recent advancements have focused on video generation, which has gained significant attention due to the impressive success of diffusion models. However, these models require substantial computational resources. To address these challenges, we propose a world model leveraging object-centric representation space using slot attention, guided by language instructions. Our model perceives the current state as an object-centric representation and predicts future states in this representation space conditioned on natural language instructions. This approach results in a more compact and computationally efficient model compared to diffusion-based generative alternatives. Furthermore, it flexibly predicts future states based on language instructions, and offers a significant advantage in manipulation tasks where object recognition is crucial. In this paper, we demonstrate that our latent predictive world model surpasses generative world models in visuo-linguo-motor control tasks, achieving superior sample and computation efficiency. We also investigate the generalization performance of the proposed method and explore various strategies for predicting actions using object-centric representations.', 'abstract_zh': '一种世界模型对于自主驾驶和机器人等领域中的代理预测未来并进行规划是必不可少的。为了实现这一目标，近期的研究重点集中在视频生成上，这得益于扩散模型取得了显著的成功。然而，这些模型需要大量的计算资源。为了解决这些问题，我们提出了一种利用对象中心表示空间的世界模型，该模型由语言指令引导，采用槽注意机制。我们的模型将当前状态视为对象中心的表示，并根据自然语言指令预测这种表示空间中的未来状态。这种方法相比于基于扩散的生成模型更加紧凑和计算效率高。此外，它可以根据语言指令灵活地预测未来状态，特别是在物体识别至关重要的操作任务中具有显著优势。在本文中，我们证明了我们提出的隐状态预测世界模型在视知觉-语言-动作控制任务中超越了生成世界模型，实现了更优的样本和计算效率。我们还研究了所提出方法的泛化性能，并探索了利用对象中心表示预测动作的各种策略。', 'title_zh': '基于对象中心的世界模型以语言引导操作'}
{'arxiv_id': 'arXiv:2503.06138', 'title': 'System 0/1/2/3: Quad-process theory for multi-timescale embodied collective cognitive systems', 'authors': 'Tadahiro Taniguchi, Yasushi Hirai, Masahiro Suzuki, Shingo Murata, Takato Horii, Kazutoshi Tanaka', 'link': 'https://arxiv.org/abs/2503.06138', 'abstract': "This paper introduces the System 0/1/2/3 framework as an extension of dual-process theory, employing a quad-process model of cognition. Expanding upon System 1 (fast, intuitive thinking) and System 2 (slow, deliberative thinking), we incorporate System 0, which represents pre-cognitive embodied processes, and System 3, which encompasses collective intelligence and symbol emergence. We contextualize this model within Bergson's philosophy by adopting multi-scale time theory to unify the diverse temporal dynamics of cognition. System 0 emphasizes morphological computation and passive dynamics, illustrating how physical embodiment enables adaptive behavior without explicit neural processing. Systems 1 and 2 are explained from a constructive perspective, incorporating neurodynamical and AI viewpoints. In System 3, we introduce collective predictive coding to explain how societal-level adaptation and symbol emergence operate over extended timescales. This comprehensive framework ranges from rapid embodied reactions to slow-evolving collective intelligence, offering a unified perspective on cognition across multiple timescales, levels of abstraction, and forms of human intelligence. The System 0/1/2/3 model provides a novel theoretical foundation for understanding the interplay between adaptive and cognitive processes, thereby opening new avenues for research in cognitive science, AI, robotics, and collective intelligence.", 'abstract_zh': 'System 0/1/2/3框架：一种扩展的认知双过程理论的四过程模型', 'title_zh': '系统0/1/2/3：多时标 embodied集体认知系统四过程理论'}
{'arxiv_id': 'arXiv:2503.05836', 'title': 'Safe Distributed Learning-Enhanced Predictive Control for Multiple Quadrupedal Robots', 'authors': 'Weishu Zhan, Zheng Liang, Hongyu Song, Wei Pan', 'link': 'https://arxiv.org/abs/2503.05836', 'abstract': 'Quadrupedal robots exhibit remarkable adaptability in unstructured environments, making them well-suited for formation control in real-world applications. However, keeping stable formations while ensuring collision-free navigation presents significant challenges due to dynamic obstacles, communication constraints, and the complexity of legged locomotion. This paper proposes a distributed model predictive control framework for multi-quadruped formation control, integrating Control Lyapunov Functions to ensure formation stability and Control Barrier Functions for decentralized safety enforcement. To address the challenge of dynamically changing team structures, we introduce Scale-Adaptive Permutation-Invariant Encoding (SAPIE), which enables robust feature encoding of neighboring robots while preserving permutation invariance. Additionally, we develop a low-latency Data Distribution Service-based communication protocol and an event-triggered deadlock resolution mechanism to enhance real-time coordination and prevent motion stagnation in constrained spaces. Our framework is validated through high-fidelity simulations in NVIDIA Omniverse Isaac Sim and real-world experiments using our custom quadrupedal robotic system, XG. Results demonstrate stable formation control, real-time feasibility, and effective collision avoidance, validating its potential for large-scale deployment.', 'abstract_zh': '四足机器人在未结构化环境中表现出显著的适应能力，使其非常适合实际应用中的编队控制。然而，在确保无碰撞导航的同时保持稳定编队面临着由于动态障碍物、通信限制和腿足运动复杂性所带来的重大挑战。本文提出了一种分布式模型预测控制框架，用于多四足机器人编队控制，结合使用Control Lyapunov函数确保编队稳定性并使用Control Barrier函数进行去中心化安全性执行。为应对团队结构动态变化的挑战，引入了可缩放自适应排列不变编码（SAPIE），该方法能够在保持排列不变性的同时，实现邻近机器人鲁棒特征编码。此外，我们还开发了一种基于低延迟数据分布服务的通信协议和事件触发的死锁解决机制，以增强实时协调并防止在受限空间中运动停滞。本文框架通过在NVIDIA Omniverse Isaac Sim进行高保真仿真及使用我们定制的四足机器人系统XG进行实地实验进行了验证。结果表明，该框架实现了稳定的编队控制、实时可行性以及有效的碰撞避免，验证了其在大规模部署中的潜力。', 'title_zh': '安全分布式学习增强预测控制 for 多足机器人'}
{'arxiv_id': 'arXiv:2503.05818', 'title': 'Closing the Intent-to-Reality Gap via Fulfillment Priority Logic', 'authors': 'Bassel El Mabsout, Abdelrahman AbdelGawad, Renato Mancuso', 'link': 'https://arxiv.org/abs/2503.05818', 'abstract': 'Practitioners designing reinforcement learning policies face a fundamental challenge: translating intended behavioral objectives into representative reward functions. This challenge stems from behavioral intent requiring simultaneous achievement of multiple competing objectives, typically addressed through labor-intensive linear reward composition that yields brittle results. Consider the ubiquitous robotics scenario where performance maximization directly conflicts with energy conservation. Such competitive dynamics are resistant to simple linear reward combinations. In this paper, we present the concept of objective fulfillment upon which we build Fulfillment Priority Logic (FPL). FPL allows practitioners to define logical formula representing their intentions and priorities within multi-objective reinforcement learning. Our novel Balanced Policy Gradient algorithm leverages FPL specifications to achieve up to 500\\% better sample efficiency compared to Soft Actor Critic. Notably, this work constitutes the first implementation of non-linear utility scalarization design, specifically for continuous control problems.', 'abstract_zh': '实践者在设计强化学习策略时面临一个基本挑战：将意图行为目标转化为代表性的奖励函数。此挑战源于行为意图需要同时实现多个竞争性目标，通常通过劳动密集型的线性奖励组合来解决，但结果往往是脆弱的。以普遍存在的机器人场景为例，性能最大化与能量 conservation直接冲突。这种竞争性动态难以通过简单的线性奖励组合来解决。本文介绍了目标履行的概念，并在此基础上构建了履行优先逻辑（FPL）。FPL 允许实践者在多目标强化学习中定义逻辑公式来表示他们的意图和优先级。我们提出的平衡策略梯度算法利用 FPL 规范，与 Soft Actor-Critic 相比，实现了高达 500% 的样本效率提升。值得注意的是，本项工作是首次为连续控制问题设计非线性效用标量化的方法。', 'title_zh': '通过履行优先逻辑缩小意图与现实之间的差距'}
{'arxiv_id': 'arXiv:2503.05808', 'title': 'DriveGen: Towards Infinite Diverse Traffic Scenarios with Large Models', 'authors': 'Shenyu Zhang, Jiaguo Tian, Zhengbang Zhu, Shan Huang, Jucheng Yang, Weinan Zhang', 'link': 'https://arxiv.org/abs/2503.05808', 'abstract': "Microscopic traffic simulation has become an important tool for autonomous driving training and testing. Although recent data-driven approaches advance realistic behavior generation, their learning still relies primarily on a single real-world dataset, which limits their diversity and thereby hinders downstream algorithm optimization. In this paper, we propose DriveGen, a novel traffic simulation framework with large models for more diverse traffic generation that supports further customized designs. DriveGen consists of two internal stages: the initialization stage uses large language model and retrieval technique to generate map and vehicle assets; the rollout stage outputs trajectories with selected waypoint goals from visual language model and a specific designed diffusion planner. Through this two-staged process, DriveGen fully utilizes large models' high-level cognition and reasoning of driving behavior, obtaining greater diversity beyond datasets while maintaining high realism. To support effective downstream optimization, we additionally develop DriveGen-CS, an automatic corner case generation pipeline that uses failures of the driving algorithm as additional prompt knowledge for large models without the need for retraining or fine-tuning. Experiments show that our generated scenarios and corner cases have a superior performance compared to state-of-the-art baselines. Downstream experiments further verify that the synthesized traffic of DriveGen provides better optimization of the performance of typical driving algorithms, demonstrating the effectiveness of our framework.", 'abstract_zh': '一种用于多样化交通生成的大模型驱动的行驶模拟框架DriveGen', 'title_zh': 'DriveGen: 向无尽多样的交通场景进发——借助大规模模型'}
{'arxiv_id': 'arXiv:2503.06580', 'title': 'Agent models: Internalizing Chain-of-Action Generation into Reasoning models', 'authors': 'Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, Jitao Sang', 'link': 'https://arxiv.org/abs/2503.06580', 'abstract': 'Traditional agentic workflows rely on external prompts to manage interactions with tools and the environment, which limits the autonomy of reasoning models. We position \\emph{Large Agent Models (LAMs)} that internalize the generation of \\emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when and how to use external tools. Our proposed AutoCoA framework combines supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the model to seamlessly switch between reasoning and action while efficiently managing environment interactions. Main components include step-level action triggering, trajectory-level CoA optimization, and an internal world model to reduce real-environment interaction costs. Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models significantly outperform ReAct-based workflows in task completion, especially in tasks that require long-term reasoning and multi-step actions. Code and dataset are available at this https URL', 'abstract_zh': '传统代理工作流依赖外部提示来管理与工具和环境的互动，这限制了推理模型的自主 reasoning 能力。我们提出的大代理模型（LAMs）内化了操作链（CoA）的生成，使模型能够自主决定何时以及如何使用外部工具。我们提出的 AutoCoA 框架结合了监督微调（SFT）和强化学习（RL），允许模型在高效管理环境互动的同时，无缝地在推理和操作之间切换。主要组件包括步骤级操作触发、轨迹级 CoA 优化以及内部世界模型以减少对真实环境的互动成本。在开域 QA 任务上的评估表明，通过 AutoCoA 训练的代理模型在任务完成上显著优于基于 ReAct 的工作流，尤其是在需要长期推理和多步操作的任务中。代码和数据集可在以下链接获取。', 'title_zh': '代理模型：将链式操作生成内嵌到推理模型中'}
{'arxiv_id': 'arXiv:2503.05828', 'title': 'Market-based Architectures in RL and Beyond', 'authors': 'Abhimanyu Pallavi Sudhir, Long Tran-Thanh', 'link': 'https://arxiv.org/abs/2503.05828', 'abstract': "Market-based agents refer to reinforcement learning agents which determine their actions based on an internal market of sub-agents. We introduce a new type of market-based algorithm where the state itself is factored into several axes called ``goods'', which allows for greater specialization and parallelism than existing market-based RL algorithms. Furthermore, we argue that market-based algorithms have the potential to address many current challenges in AI, such as search, dynamic scaling and complete feedback, and demonstrate that they may be seen to generalize neural networks; finally, we list some novel ways that market algorithms may be applied in conjunction with Large Language Models for immediate practical applicability.", 'abstract_zh': '基于市场的智能体：一种将状态分解为多个“商品”轴的新型市场机制强化学习算法及其应用', 'title_zh': '基于市场的架构在RL及更广泛的领域'}
{'arxiv_id': 'arXiv:2503.07587', 'title': 'Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru', 'authors': 'Dunant Cusipuma, David Ortega, Victor Flores-Benites, Arturo Deza', 'link': 'https://arxiv.org/abs/2503.07587', 'abstract': 'As multimodal foundational models start being deployed experimentally in Self-Driving cars, a reasonable question we ask ourselves is how similar to humans do these systems respond in certain driving situations -- especially those that are out-of-distribution? To study this, we create the Robusto-1 dataset that uses dashcam video data from Peru, a country with one of the worst (aggressive) drivers in the world, a high traffic index, and a high ratio of bizarre to non-bizarre street objects likely never seen in training. In particular, to preliminarly test at a cognitive level how well Foundational Visual Language Models (VLMs) compare to Humans in Driving, we move away from bounding boxes, segmentation maps, occupancy maps or trajectory estimation to multi-modal Visual Question Answering (VQA) comparing both humans and machines through a popular method in systems neuroscience known as Representational Similarity Analysis (RSA). Depending on the type of questions we ask and the answers these systems give, we will show in what cases do VLMs and Humans converge or diverge allowing us to probe on their cognitive alignment. We find that the degree of alignment varies significantly depending on the type of questions asked to each type of system (Humans vs VLMs), highlighting a gap in their alignment.', 'abstract_zh': '多模态基础模型在自动驾驶汽车中的实验部署引发了我们对这些系统在某些驾驶情况下的响应与人类相似性如何的思考，尤其是在这些情况超出了训练分布的情况下。为研究这一问题，我们创建了Robusto-1数据集，该数据集使用了秘鲁的车载摄像头视频数据，秘鲁是世界上驾驶行为最为激进的国家之一，交通密度高，并且街景物体中非常规物体的比例很高，这些物体在训练中可能从未出现过。通过对代表性脑成像方法——表示相似性分析(RSA)来比较基础视觉语言模型(VLMs)和人类在多模态视觉问答(VQA)中的表现，我们初步探讨了基础视觉语言模型在认知层面上如何与人类进行比较。通过提问不同类型的问题并分析这些系统给出的回答，我们将展示VLMs和人类在哪些情况下一致或相异，从而揭示他们在认知上的对齐程度。我们发现，不同类型的系统（人类 vs VLMs）对不同类型问题的回答程度存在显著差异，这揭示了它们之间认知对齐的差距。', 'title_zh': 'Robusto-1数据集：比较人类和大规模语言模型在来自秘鲁的实际分布外自主驾驶VQA中的表现'}
{'arxiv_id': 'arXiv:2503.07453', 'title': 'Is a Good Foundation Necessary for Efficient Reinforcement Learning? The Computational Role of the Base Model in Exploration', 'authors': 'Dylan J. Foster, Zakaria Mhammedi, Dhruv Rohatgi', 'link': 'https://arxiv.org/abs/2503.07453', 'abstract': 'Language model alignment (or, reinforcement learning) techniques that leverage active exploration -- deliberately encouraging the model to produce diverse, informative responses -- offer the promise of super-human capabilities. However, current understanding of algorithm design primitives for computationally efficient exploration with language models is limited. To better understand how to leverage access to powerful pre-trained generative models to improve the efficiency of exploration, we introduce a new computational framework for RL with language models, in which the learner interacts with the model through a sampling oracle. Focusing on the linear softmax model parameterization, we provide new results that reveal the computational-statistical tradeoffs of efficient exploration:\n1. Necessity of coverage: Coverage refers to the extent to which the pre-trained model covers near-optimal responses -- a form of hidden knowledge. We show that coverage, while not necessary for data efficiency, lower bounds the runtime of any algorithm in our framework.\n2. Inference-time exploration: We introduce a new algorithm, SpannerSampling, which obtains optimal data efficiency and is computationally efficient whenever the pre-trained model enjoys sufficient coverage, matching our lower bound. SpannerSampling leverages inference-time computation with the pre-trained model to reduce the effective search space for exploration.\n3. Insufficiency of training-time interventions: We contrast the result above by showing that training-time interventions that produce proper policies cannot achieve similar guarantees in polynomial time.\n4. Computational benefits of multi-turn exploration: Finally, we show that under additional representational assumptions, one can achieve improved runtime (replacing sequence-level coverage with token-level coverage) through multi-turn exploration.', 'abstract_zh': '语言模型对齐（或强化学习）技术利用主动探索——故意促使模型生成多样且有信息量的响应——提供了超人类能力的潜力。然而，当前计算高效探索的语言模型算法设计基础非常有限。为更好地理解如何利用强大预训练生成模型提高探索效率，我们引入了一种新的以语言模型为框架的计算框架，在该框架中，学习者通过采样或acles与模型交互。聚焦于线性softmax模型参数化，我们提供了新的结果，揭示了高效探索的计算-统计权衡：\n1. 覆盖的必要性：覆盖指的是预训练模型涵盖近最优响应的程度——一种隐藏知识的形式。我们展示，虽然覆盖不是数据效率所必需的，但它在我们的框架中任何算法的运行时下界。\n2. 推理时探索：我们介绍了一种新的算法SpannerSampling，该算法在预训练模型享有充分覆盖时，实现了最佳的数据效率和计算效率，与我们的下界相匹配。SpannerSampling利用预训练模型的推理时计算来减少探索的有效搜索空间。\n3. 无训练时介入的不足：与上述结果形成对比，我们展示了训练时介入以生成正确策略无法在多项式时间内实现类似保证。\n4. 多轮探索的计算优势：最后，我们展示了在附加表示假设下，可以通过多轮探索实现更好的运行时性能（用令牌级覆盖代替序列级覆盖），从而改善运行时性能。', 'title_zh': '打好基础对于高效强化学习是否必要？基模型在探索中的计算作用'}
{'arxiv_id': 'arXiv:2503.07020', 'title': 'Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense', 'authors': 'Yuting Hu, Chenhui Xu, Ruiyang Qin, Dancheng Liu, Amir Nassereldine, Yiyu Shi, Jinjun Xiong', 'link': 'https://arxiv.org/abs/2503.07020', 'abstract': 'Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Current protocols typically respond with immediate stops or minimal-risk maneuvers, worsening traffic flow and lacking flexibility for rare driving scenarios. In this paper, we propose LLM-RCO, a framework leveraging large language models to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator. These modules interact with the dynamic driving environment, enabling proactive and context-aware control actions to override the original control policy of autonomous agents. To improve safety in such challenging conditions, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, complete with annotations for LLM-based hazard inference and motion planning fine-tuning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that systems equipped with LLM-RCO significantly improve driving performance, highlighting its potential for enhancing autonomous driving resilience against adverse perception deficits. Our results also show that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements instead of conservative stops in the context of perception deficits.', 'abstract_zh': '感知缺陷会通过干扰环境理解来损害自主车辆的安全。当前的协议通常会立即停车或执行最小风险操作，这会恶化交通流量并且缺乏应对罕见驾驶场景的灵活性。在本文中，我们提出了LLM-RCO框架，利用大规模语言模型将类似人类驾驶的经验常识整合到面临感知缺陷的自主系统中。LLM-RCO包含四个关键模块：危险推断、短期运动规划器、动作条件验证器和安全约束生成器。这些模块与动态驾驶环境互动，使系统能够采取先发制人且情境感知的控制行动，以覆盖原始的自主代理控制策略。为了在这些具有挑战性的条件下提高安全性，我们构建了包含53,895个视频片段的DriveLM-Deficit数据集，这些片段展示了关键安全对象的缺陷，并附有基于LLM的危险推断和运动规划微调的注释。在使用CARLA模拟器进行的恶劣驾驶条件下的广泛实验中，装有LLM-RCO的系统显著提高了驾驶性能，突显了其在对抗不良感知缺陷方面增强自主驾驶弹性的潜力。我们的结果还表明，在感知缺陷的背景下，通过DriveLM-Deficit微调的LLM能够使车辆采取更加积极的移动行为，而非保守地停车。', 'title_zh': '利用多模态LLM常识对抗自动驾驶中的部分知觉缺陷'}
{'arxiv_id': 'arXiv:2503.06893', 'title': 'Policy Regularization on Globally Accessible States in Cross-Dynamics Reinforcement Learning', 'authors': 'Zhenghai Xue, Lang Feng, Jiacheng Xu, Kang Kang, Xiang Wen, Bo An, Shuicheng Yan', 'link': 'https://arxiv.org/abs/2503.06893', 'abstract': "To learn from data collected in diverse dynamics, Imitation from Observation (IfO) methods leverage expert state trajectories based on the premise that recovering expert state distributions in other dynamics facilitates policy learning in the current one. However, Imitation Learning inherently imposes a performance upper bound of learned policies. Additionally, as the environment dynamics change, certain expert states may become inaccessible, rendering their distributions less valuable for imitation. To address this, we propose a novel framework that integrates reward maximization with IfO, employing F-distance regularized policy optimization. This framework enforces constraints on globally accessible states--those with nonzero visitation frequency across all considered dynamics--mitigating the challenge posed by inaccessible states. By instantiating F-distance in different ways, we derive two theoretical analysis and develop a practical algorithm called Accessible State Oriented Policy Regularization (ASOR). ASOR serves as a general add-on module that can be incorporated into various RL approaches, including offline RL and off-policy RL. Extensive experiments across multiple benchmarks demonstrate ASOR's effectiveness in enhancing state-of-the-art cross-domain policy transfer algorithms, significantly improving their performance.", 'abstract_zh': '基于观察的模仿（IfO）方法从多样动态数据中学习：一种结合奖励最大化和F-距离正则化的可访问状态导向策略正则化框架', 'title_zh': '在交叉动力系统强化学习中全局可访问状态的策略正则化'}
{'arxiv_id': 'arXiv:2503.06797', 'title': 'Multimodal AI-driven Biomarker for Early Detection of Cancer Cachexia', 'authors': 'Sabeen Ahmed, Nathan Parker, Margaret Park, Evan W. Davis, Jennifer B. Permuth, Matthew B. Schabath, Yasin Yilmaz, Ghulam Rasool', 'link': 'https://arxiv.org/abs/2503.06797', 'abstract': 'Cancer cachexia is a multifactorial syndrome characterized by progressive muscle wasting, metabolic dysfunction, and systemic inflammation, leading to reduced quality of life and increased mortality. Despite extensive research, no single definitive biomarker exists, as cachexia-related indicators such as serum biomarkers, skeletal muscle measurements, and metabolic abnormalities often overlap with other conditions. Existing composite indices, including the Cancer Cachexia Index (CXI), Modified CXI (mCXI), and Cachexia Score (CASCO), integrate multiple biomarkers but lack standardized thresholds, limiting their clinical utility. This study proposes a multimodal AI-based biomarker for early cancer cachexia detection, leveraging open-source large language models (LLMs) and foundation models trained on medical data. The approach integrates heterogeneous patient data, including demographics, disease status, lab reports, radiological imaging (CT scans), and clinical notes, using a machine learning framework that can handle missing data. Unlike previous AI-based models trained on curated datasets, this method utilizes routinely collected clinical data, enhancing real-world applicability. Additionally, the model incorporates confidence estimation, allowing the identification of cases requiring expert review for precise clinical interpretation. Preliminary findings demonstrate that integrating multiple data modalities improves cachexia prediction accuracy at the time of cancer diagnosis. The AI-based biomarker dynamically adapts to patient-specific factors such as age, race, ethnicity, weight, cancer type, and stage, avoiding the limitations of fixed-threshold biomarkers. This multimodal AI biomarker provides a scalable and clinically viable solution for early cancer cachexia detection, facilitating personalized interventions and potentially improving treatment outcomes and patient survival.', 'abstract_zh': '癌症恶病质是一种多因素综合征，特征为渐进性肌肉消耗、代谢功能障碍和全身炎症，导致生活质量下降和死亡率增加。尽管进行了大量研究，但至今没有单一确凿的生物标志物，因为与癌症恶病质相关的指标，如血液生物标志物、骨骼肌测量和代谢异常往往与其他疾病重叠。现有的综合指标，包括癌症恶病质指数（CXI）、修改版CXI（mCXI）和恶病质评分（CASCO），整合了多种生物标志物，但缺乏标准化门槛，限制了其临床实用性。本研究提出了一种基于多模态人工智能的生物标志物，用于早期癌症恶病质检测，利用开源的大规模语言模型（LLMs）和基于医学数据训练的基础模型。该方法整合了异质性患者数据，包括人口统计学、疾病状态、实验室报告、放射影像（CT扫描）和临床记录，使用能够处理缺失数据的机器学习框架。与之前基于精心标注数据集训练的AI模型不同，该方法利用了常规收集的临床数据，提高了其实用性。此外，该模型还包含了置信度估计的功能，可以识别需要专家复核以进行精确临床解释的病例。初步发现表明，整合多种数据模态可以提高癌症诊断时恶病质预测的准确性。基于人工智能的生物标志物可以根据年龄、种族、民族、体重、癌症类型和分期等患者特定因素动态调整，避免了固定阈值生物标志物的局限性。这一多模态人工智能生物标志物提供了一种可扩展且临床可行的解决方案，用于早期癌症恶病质的检测，有助于个性化干预并可能改善治疗结果和患者存活率。', 'title_zh': '多模态AI驱动的生物标志物用于早期检测癌症恶病质'}
{'arxiv_id': 'arXiv:2503.06542', 'title': 'ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model with Interleaved Multimodal Generation via Asymmetric Synergy', 'authors': 'Jianwen Sun, Yukang Feng, Chuanhao Li, Fanrui Zhang, Zizhen Li, Jiaxin Ai, Sizhuo Zhou, Yu Dai, Shenglin Zhang, Kaipeng Zhang', 'link': 'https://arxiv.org/abs/2503.06542', 'abstract': 'Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate" algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at this https URL.', 'abstract_zh': '资源高效且纯粹的自回归框架ARMOR：通过微调现有的多模态大型语言模型实现统一的多模态理解和生成', 'title_zh': 'ARMOR v0.1：通过交错多模态生成增强的自回归多模态理解模型异构协同'}
{'arxiv_id': 'arXiv:2503.06497', 'title': 'Evaluation of Safety Cognition Capability in Vision-Language Models for Autonomous Driving', 'authors': 'Enming Zhang, Peizhe Gong, Xingyuan Dai, Yisheng Lv, Qinghai Miao', 'link': 'https://arxiv.org/abs/2503.06497', 'abstract': 'Assessing the safety of vision-language models (VLMs) in autonomous driving is particularly important; however, existing work mainly focuses on traditional benchmark evaluations. As interactive components within autonomous driving systems, VLMs must maintain strong safety cognition during interactions. From this perspective, we propose a novel evaluation method: Safety Cognitive Driving Benchmark (SCD-Bench) . To address the large-scale annotation challenge for SCD-Bench, we develop the Autonomous Driving Image-Text Annotation System (ADA) . Additionally, to ensure data quality in SCD-Bench, our dataset undergoes manual refinement by experts with professional knowledge in autonomous driving. We further develop an automated evaluation method based on large language models (LLMs). To verify its effectiveness, we compare its evaluation results with those of expert human evaluations, achieving a consistency rate of 99.74%. Preliminary experimental results indicate that existing open-source models still lack sufficient safety cognition, showing a significant gap compared to GPT-4o. Notably, lightweight models (1B-4B) demonstrate minimal safety cognition. However, since lightweight models are crucial for autonomous driving systems, this presents a significant challenge for integrating VLMs into the field.', 'abstract_zh': '评估自动驾驶中视觉语言模型的安全性尤其重要；然而，现有工作主要集中在传统基准评估上。从这一视角出发，我们提出了一种新型评估方法：安全认知驾驶基准（SCD-Bench）。为应对SCD-Bench的大规模标注挑战，我们开发了自动驾驶图像-文本标注系统（ADA）。此外，为确保SCD-Bench的数据质量，我们的数据集经过了专业自动驾驶知识专家的手动 refinement。我们进一步基于大规模语言模型（LLMs）开发了一种自动评估方法。通过与专家人工评估的结果进行比较，其一致性率达到99.74%，证明了其有效性。初步实验结果表明，现有开源模型在安全性认知方面仍然不足，与GPT-4o相比存在显著差距。值得注意的是，轻量级模型（1B-4B）在安全性认知方面表现极弱。然而，由于轻量级模型对自动驾驶系统至关重要，这为将视觉语言模型集成到该领域带来了重大挑战。', 'title_zh': '自动驾驶场景下视觉语言模型的安全认知能力评估'}
{'arxiv_id': 'arXiv:2503.06486', 'title': 'PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training', 'authors': 'Cong Chen, Mingyu Liu, Chenchen Jing, Yizhou Zhou, Fengyun Rao, Hao Chen, Bo Zhang, Chunhua Shen', 'link': 'https://arxiv.org/abs/2503.06486', 'abstract': "This paper aims to address the challenge of hallucinations in Multimodal Large Language Models (MLLMs) particularly for dense image captioning tasks. To tackle the challenge, we identify the current lack of a metric that finely measures the caption quality in concept level. We hereby introduce HalFscore, a novel metric built upon the language graph and is designed to evaluate both the accuracy and completeness of dense captions at a granular level. Additionally, we identify the root cause of hallucination as the model's over-reliance on its language prior. To address this, we propose PerturboLLaVA, which reduces the model's reliance on the language prior by incorporating adversarially perturbed text during training. This method enhances the model's focus on visual inputs, effectively reducing hallucinations and producing accurate, image-grounded descriptions without incurring additional computational overhead. PerturboLLaVA significantly improves the fidelity of generated captions, outperforming existing approaches in handling multimodal hallucinations and achieving improved performance across general multimodal benchmarks.", 'abstract_zh': '本文旨在解决多模态大语言模型（MLLMs）在密集图像描述任务中幻觉挑战。为此，我们识别出当前缺乏一种细粒度衡量描述质量的指标，特别是在概念层级。我们引入了HalFscore，这是一种基于语言图的新颖度量标准，旨在从细粒度层面评估密集描述的准确性和完整性。此外，我们确定幻觉的根本原因是模型过度依赖于语言先验。为解决这一问题，我们提出了PerturboLLaVA，通过在训练过程中引入对抗性扰动文本，降低模型对语言先验的依赖，从而增强模型对视觉输入的关注，有效减少幻觉并生成准确、图像相关的描述，而无需增加额外的计算开销。PerturboLLaVA显着提高了生成描述的忠实度，在处理多模态幻觉和跨通用多模态基准上优于现有方法。', 'title_zh': 'PerturboLLaVA: 通过对视觉进行扰动性训练减少多模态幻觉'}
{'arxiv_id': 'arXiv:2503.06422', 'title': 'GenAI for Simulation Model in Model-Based Systems Engineering', 'authors': 'Lin Zhang, Yuteng Zhang, Dusit Niyato, Lei Ren, Pengfei Gu, Zhen Chen, Yuanjun Laili, Wentong Cai, Agostino Bruzzone', 'link': 'https://arxiv.org/abs/2503.06422', 'abstract': 'Generative AI (GenAI) has demonstrated remarkable capabilities in code generation, and its integration into complex product modeling and simulation code generation can significantly enhance the efficiency of the system design phase in Model-Based Systems Engineering (MBSE). In this study, we introduce a generative system design methodology framework for MBSE, offering a practical approach for the intelligent generation of simulation models for system physical properties. First, we employ inference techniques, generative models, and integrated modeling and simulation languages to construct simulation models for system physical properties based on product design documents. Subsequently, we fine-tune the language model used for simulation model generation on an existing library of simulation models and additional datasets generated through generative modeling. Finally, we introduce evaluation metrics for the generated simulation models for system physical properties. Our proposed approach to simulation model generation presents the innovative concept of scalable templates for simulation models. Using these templates, GenAI generates simulation models for system physical properties through code completion. The experimental results demonstrate that, for mainstream open-source Transformer-based models, the quality of the simulation model is significantly improved using the simulation model generation method proposed in this paper.', 'abstract_zh': '基于生成式AI的Model-Based Systems Engineering中仿真模型智能化生成方法论框架', 'title_zh': '基于模型系统工程中的GenAI仿真模型'}
{'arxiv_id': 'arXiv:2503.06157', 'title': 'UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces', 'authors': 'Baining Zhao, Jianjie Fang, Zichao Dai, Ziyou Wang, Jirong Zha, Weichen Zhang, Chen Gao, Yue Wang, Jinqiang Cui, Xinlei Chen, Yong Li', 'link': 'https://arxiv.org/abs/2503.06157', 'abstract': 'Large multimodal models exhibit remarkable intelligence, yet their embodied cognitive abilities during motion in open-ended urban 3D space remain to be explored. We introduce a benchmark to evaluate whether video-large language models (Video-LLMs) can naturally process continuous first-person visual observations like humans, enabling recall, perception, reasoning, and navigation. We have manually control drones to collect 3D embodied motion video data from real-world cities and simulated environments, resulting in 1.5k video clips. Then we design a pipeline to generate 5.2k multiple-choice questions. Evaluations of 17 widely-used Video-LLMs reveal current limitations in urban embodied cognition. Correlation analysis provides insight into the relationships between different tasks, showing that causal reasoning has a strong correlation with recall, perception, and navigation, while the abilities for counterfactual and associative reasoning exhibit lower correlation with other tasks. We also validate the potential for Sim-to-Real transfer in urban embodiment through fine-tuning.', 'abstract_zh': '大型多模态模型表现出显著的智能，但在开放的城市3D空间中运动过程中的体现认知能力仍有待探索。我们引入了一个基准来评估视频大规模语言模型（Video-LLMs）是否能自然处理连续的第一人称视觉观察，如同人类一般，实现回忆、感知、推理和导航。我们手动操控无人机，从真实城市和模拟环境中收集了3D沉浸式运动视频数据，总共得到1500个视频片段。然后我们设计了一个流水线生成了5200个多项选择题。对17种广泛使用的Video-LLMs的评估揭示了当前在城市体现认知方面的局限性。相关性分析揭示了不同任务之间的关系，表明因果推理与回忆、感知和导航之间存在强烈的相关性，而反事实推理和关联推理的能力与其它任务的相关性较低。我们还通过微调验证了城市体现中的模拟到现实迁移的潜力。', 'title_zh': 'UrbanVideo-Bench: 城市空间视频数据在体态智能中的视觉-语言模型评估基准'}
{'arxiv_id': 'arXiv:2503.06073', 'title': 'GEM: Empowering MLLM for Grounded ECG Understanding with Time Series and Images', 'authors': 'Xiang Lan, Feng Wu, Kai He, Qinghao Zhao, Shenda Hong, Mengling Feng', 'link': 'https://arxiv.org/abs/2503.06073', 'abstract': "While recent multimodal large language models (MLLMs) have advanced automated ECG interpretation, they still face two key limitations: (1) insufficient multimodal synergy between time series signals and visual ECG representations, and (2) limited explainability in linking diagnoses to granular waveform evidence. We introduce GEM, the first MLLM unifying ECG time series, 12-lead ECG images and text for grounded and clinician-aligned ECG interpretation. GEM enables feature-grounded analysis, evidence-driven reasoning, and a clinician-like diagnostic process through three core innovations: a dual-encoder framework extracting complementary time series and image features, cross-modal alignment for effective multimodal understanding, and knowledge-guided instruction generation for generating high-granularity grounding data (ECG-Grounding) linking diagnoses to measurable parameters ($e.g.$, QRS/PR Intervals). Additionally, we propose the Grounded ECG Understanding task, a clinically motivated benchmark designed to comprehensively assess the MLLM's capability in grounded ECG understanding. Experimental results on both existing and our proposed benchmarks show GEM significantly improves predictive performance (CSN $7.4\\% \\uparrow$), explainability ($22.7\\% \\uparrow$), and grounding ($24.8\\% \\uparrow$), making it more suitable for real-world clinical applications. GitHub repository: this https URL", 'abstract_zh': '最近的多模态大型语言模型虽已在心电图自动解读方面取得了进展，但仍面临两个关键限制：(1) 心电时间序列信号和视觉心电图表示之间的多模态协同不足，以及(2) 在将诊断与细粒度波形证据联系起来时的解释性有限。我们提出了GEM，首个将心电时间序列、12导联心电图图像和文本统一起来，实现基于证据的临床对齐心电图解读的多模态大型语言模型。GEM通过三项核心创新实现基于特征的分析、基于证据的推理以及类似临床医生的诊断过程：双编码器框架提取互补的时间序列和图像特征，跨模态对齐以实现有效的多模态理解，以及知识引导的指令生成，生成细粒度的接地数据（ECG-Grounding），将其与可测量参数（例如，QRS/PR间期）联系起来。此外，我们还提出了基于证据的心电图理解任务，这是一个临床驱动的基准测试，旨在全面评估多模态大型语言模型在基于证据的心电图理解能力。我们的实验结果表明，GEM在预测性能（CSN提高7.4%）、解释性（提高22.7%）和接地能力（提高24.8%）方面显著提升，使其更适用于临床应用。GitHub仓库：https://github.com/your-repository。', 'title_zh': 'GEM: 赋能基于时间序列和图像的心电图接地理解的大语言模型'}
{'arxiv_id': 'arXiv:2503.05823', 'title': 'Introduction to Artificial Consciousness: History, Current Trends and Ethical Challenges', 'authors': 'Aïda Elamrani', 'link': 'https://arxiv.org/abs/2503.05823', 'abstract': 'With the significant progress of artificial intelligence (AI) and consciousness science, artificial consciousness (AC) has recently gained popularity. This work provides a broad overview of the main topics and current trends in AC. The first part traces the history of this interdisciplinary field to establish context and clarify key terminology, including the distinction between Weak and Strong AC. The second part examines major trends in AC implementations, emphasising the synergy between Global Workspace and Attention Schema, as well as the problem of evaluating the internal states of artificial systems. The third part analyses the ethical dimension of AC development, revealing both critical risks and transformative opportunities. The last part offers recommendations to guide AC research responsibly, and outlines the limitations of this study as well as avenues for future research. The main conclusion is that while AC appears both indispensable and inevitable for scientific progress, serious efforts are required to address the far-reaching impact of this innovative research path.', 'abstract_zh': '随着人工智能（AI）和意识科学的显著进步，人工意识（AC）近年来受到了广泛关注。本文提供了人工意识领域主要主题和当前趋势的广泛综述。第一部分追溯了这一跨学科领域的历史，以建立背景并澄清关键术语，包括弱人工意识与强人工意识的区别。第二部分分析了AC实现中的主要趋势，强调了全局工作空间与注意力-schema的协同作用，以及评估人工系统内部状态的问题。第三部分探讨了AC开发的伦理维度，揭示了既有的重大风险和变革机会。最后一部分提供了指导负责任进行AC研究的建议，并概述了本研究的局限性以及未来研究的方向。主要结论是，尽管AC对于科学进步既是不可或缺的，也是不可避免的，但必须做出严肃的努力来应对这一创新研究路径带来的深远影响。', 'title_zh': '人工意识导论：历史、当前趋势与伦理挑战'}
{'arxiv_id': 'arXiv:2503.05776', 'title': 'FAA-CLIP: Federated Adversarial Adaptation of CLIP', 'authors': 'Yihang Wu, Ahmad Chaddad, Christian Desrosiers, Tareef Daqqaq, Reem Kateb', 'link': 'https://arxiv.org/abs/2503.05776', 'abstract': "Despite the remarkable performance of vision language models (VLMs) such as Contrastive Language Image Pre-training (CLIP), the large size of these models is a considerable obstacle to their use in federated learning (FL) systems where the parameters of local client models need to be transferred to a global server for aggregation. Another challenge in FL is the heterogeneity of data from different clients, which affects the generalization performance of the solution. In addition, natural pre-trained VLMs exhibit poor generalization ability in the medical datasets, suggests there exists a domain gap. To solve these issues, we introduce a novel method for the Federated Adversarial Adaptation (FAA) of CLIP. Our method, named FAA-CLIP, handles the large communication costs of CLIP using a light-weight feature adaptation module (FAM) for aggregation, effectively adapting this VLM to each client's data while greatly reducing the number of parameters to transfer. By keeping CLIP frozen and only updating the FAM parameters, our method is also computationally efficient. Unlike existing approaches, our FAA-CLIP method directly addresses the problem of domain shifts across clients via a domain adaptation (DA) module. This module employs a domain classifier to predict if a given sample is from the local client or the global server, allowing the model to learn domain-invariant representations. Extensive experiments on six different datasets containing both natural and medical images demonstrate that FAA-CLIP can generalize well on both natural and medical datasets compared to recent FL approaches. Our codes are available at this https URL.", 'abstract_zh': 'federated adversarial adaptation of clip for medical and natural images', 'title_zh': 'FAA-CLIP: 联邦对抗适应的CLIP'}
{'arxiv_id': 'arXiv:2503.05765', 'title': 'Encoding Inequity: Examining Demographic Bias in LLM-Driven Robot Caregiving', 'authors': 'Raj Korpan', 'link': 'https://arxiv.org/abs/2503.05765', 'abstract': 'As robots take on caregiving roles, ensuring equitable and unbiased interactions with diverse populations is critical. Although Large Language Models (LLMs) serve as key components in shaping robotic behavior, speech, and decision-making, these models may encode and propagate societal biases, leading to disparities in care based on demographic factors. This paper examines how LLM-generated responses shape robot caregiving characteristics and responsibilities when prompted with different demographic information related to sex, gender, sexuality, race, ethnicity, nationality, disability, and age. Findings show simplified descriptions for disability and age, lower sentiment for disability and LGBTQ+ identities, and distinct clustering patterns reinforcing stereotypes in caregiving narratives. These results emphasize the need for ethical and inclusive HRI design.', 'abstract_zh': '随着机器人承担起护理角色，确保与多样人群进行公平无偏见的互动至关重要。尽管大型语言模型（LLMs）在塑造机器人行为、言语和决策中起着关键作用，但这些模型可能编码并传播社会偏见，导致基于人口统计因素的护理不平等。本文探讨了当LLM在处理与性别、性别、性取向、种族、 ethnicity（注：此处应为“族裔”）、国籍、残疾和年龄相关的不同人口统计信息时，如何生成响应影响机器人护理特征和职责。研究发现，残疾和年龄的简化描述，残疾和LGBTQ+身份的较低情感价值，以及强化护理叙事中刻板印象的类别聚类模式。这些结果强调了伦理和包容性人机交互设计的必要性。', 'title_zh': '编码不公平性：探究LLM驱动的机器人护理中的人口统计偏见'}
{'arxiv_id': 'arXiv:2503.05723', 'title': 'AI Mimicry and Human Dignity: Chatbot Use as a Violation of Self-Respect', 'authors': 'Jan-Willem van der Rijt, Dimitri Coelho Mollo, Bram Vaassen', 'link': 'https://arxiv.org/abs/2503.05723', 'abstract': "This paper investigates how human interactions with AI-powered chatbots may offend human dignity. Current chatbots, driven by large language models (LLMs), mimic human linguistic behaviour but lack the moral and rational capacities essential for genuine interpersonal respect. Human beings are prone to anthropomorphise chatbots. Indeed, chatbots appear to be deliberately designed to elicit that response. As a result, human beings' behaviour toward chatbots often resembles behaviours typical of interaction between moral agents. Drawing on a second-personal, relational account of dignity, we argue that interacting with chatbots in this way is incompatible with the dignity of users. We show that, since second-personal respect is premised on reciprocal recognition of second-personal authority, behaving towards chatbots in ways that convey second-personal respect is bound to misfire in morally problematic ways, given the lack of reciprocity. Consequently, such chatbot interactions amount to subtle but significant violations of self-respect: the respect we are dutybound to show for our own dignity. We illustrate this by discussing four actual chatbot use cases (information retrieval, customer service, advising, and companionship), and propound that the increasing societal pressure to engage in such interactions with chatbots poses a hitherto underappreciated threat to human dignity.", 'abstract_zh': '本文探讨人类与AI聊天机器人互动过程中可能侵犯人类尊严的方式。当前由大规模语言模型驱动的聊天机器人模仿人类语言行为，但缺乏实现真正人际尊重所需的情感和理性能力。人类倾向于将聊天机器人拟人化，事实上，聊天机器人似乎被有意设计成引发这种反应。因此，人类对待聊天机器人的行为往往类似于与道德主体互动的行为。基于一种第二人称关系视角的尊严观，我们argue认为以这种方式与聊天机器人互动与用户尊严不相容。我们表明，因为第二人称尊重的前提是相互承认第二人称权威，因此以表现为第二人称尊重的方式对待聊天机器人必然以道德上有问题的方式失效，鉴于缺乏互惠性。因此，这种聊天机器人互动构成了对自我尊重的微妙但重要的违反：我们有义务为此类尊严展示。我们通过讨论四个实际的聊天机器人应用场景（信息检索、客户服务、咨询和陪伴）来阐述这一点，并提出，社会上越来越大的压力促使人们与聊天机器人进行此类互动，构成了对人类尊严的一种以前未曾充分认识到的威胁。', 'title_zh': 'AI模仿与人类尊严：聊天机器人的使用是对自我尊重的侵犯'}
