{'arxiv_id': 'arXiv:2503.06953', 'title': 'MERLION: Marine ExploRation with Language guIded Online iNformative Visual Sampling and Enhancement', 'authors': 'Shrutika Vishal Thengane, Marcel Bartholomeus Prasetyo, Yu Xiang Tan, Malika Meghjani', 'link': 'https://arxiv.org/abs/2503.06953', 'abstract': "Autonomous and targeted underwater visual monitoring and exploration using Autonomous Underwater Vehicles (AUVs) can be a challenging task due to both online and offline constraints. The online constraints comprise limited onboard storage capacity and communication bandwidth to the surface, whereas the offline constraints entail the time and effort required for the selection of desired key frames from the video data. An example use case of targeted underwater visual monitoring is finding the most interesting visual frames of fish in a long sequence of an AUV's visual experience. This challenge of targeted informative sampling is further aggravated in murky waters with poor visibility. In this paper, we present MERLION, a novel framework that provides semantically aligned and visually enhanced summaries for murky underwater marine environment monitoring and exploration. Specifically, our framework integrates (a) an image-text model for semantically aligning the visual samples to the users' needs, (b) an image enhancement model for murky water visual data and (c) an informative sampler for summarizing the monitoring experience. We validate our proposed MERLION framework on real-world data with user studies and present qualitative and quantitative results using our evaluation metric and show improved results compared to the state-of-the-art approaches. We have open-sourced the code for MERLION at the following link this https URL.", 'abstract_zh': '使用自主水下机器人（AUVs）进行自主和定向的水下视觉监控与探索是一项由于在线和离线约束而具有挑战性的任务。在线约束包括有限的机载存储容量和与水面的通信带宽，而离线约束则涉及从视频数据中选择所需关键帧所需的时间和努力。定向水下视觉监控的一个示例用例是在AUV长时间视觉体验序列中找到鱼类的最有趣视觉帧。在能见度差的浑浊水域中，这一以信息为导向的采样挑战更为严峻。本文提出了一种名为MERLION的新型框架，提供了语义对齐且视觉增强的摘要，用于浑浊水下海洋环境的监控与探索。具体而言，我们的框架融合了(a) 图像-文本模型，用于语义对齐视觉样本以满足用户需求，(b) 浑浊水可视化数据增强模型，以及(c) 信息性采样器，用于总结监控体验。我们通过用户研究在实际数据上验证了我们提出的MERLION框架，并使用评价指标展示了定性和定量结果，结果优于现有最先进的方法。我们已开源MERLION的代码，链接如下：https://github.com/<your-repo-name>。', 'title_zh': 'MERLION: 海洋探索与语言引导的在线信息性视觉采样和增强'}
{'arxiv_id': 'arXiv:2503.07587', 'title': 'Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru', 'authors': 'Dunant Cusipuma, David Ortega, Victor Flores-Benites, Arturo Deza', 'link': 'https://arxiv.org/abs/2503.07587', 'abstract': 'As multimodal foundational models start being deployed experimentally in Self-Driving cars, a reasonable question we ask ourselves is how similar to humans do these systems respond in certain driving situations -- especially those that are out-of-distribution? To study this, we create the Robusto-1 dataset that uses dashcam video data from Peru, a country with one of the worst (aggressive) drivers in the world, a high traffic index, and a high ratio of bizarre to non-bizarre street objects likely never seen in training. In particular, to preliminarly test at a cognitive level how well Foundational Visual Language Models (VLMs) compare to Humans in Driving, we move away from bounding boxes, segmentation maps, occupancy maps or trajectory estimation to multi-modal Visual Question Answering (VQA) comparing both humans and machines through a popular method in systems neuroscience known as Representational Similarity Analysis (RSA). Depending on the type of questions we ask and the answers these systems give, we will show in what cases do VLMs and Humans converge or diverge allowing us to probe on their cognitive alignment. We find that the degree of alignment varies significantly depending on the type of questions asked to each type of system (Humans vs VLMs), highlighting a gap in their alignment.', 'abstract_zh': '当多模态基础模型开始在自动驾驶汽车中实验部署时，我们合理地自问这些系统在某些驾驶情景下与人类的响应有多相似，尤其是在那些超出训练分布的情景下。为了研究这一问题，我们创建了Robusto-1数据集，该数据集使用了秘鲁（世界上驾驶行为最 aggressive 的国家之一）的行车记录仪视频数据，具有高交通密度和高比例的古怪街道物体，这些物体很可能从未出现在训练中。特别是，为了初步测试基础视觉语言模型（VLMs）与人类在驾驶情景下的认知水平表现，我们从边界框、分割图、占用地图或轨迹估计转向多模态视觉问答（VQA），并通过系统神经科学中的一种流行方法——表征相似性分析（RSA），对比人类和机器的回答。根据我们提出的问题类型及系统给出的回答，我们将展示在哪些情况下基础视觉语言模型和人类会趋于一致或存在分歧，从而使我们能够探究它们的认知对齐情况。我们发现，响应的一致性程度取决于对不同类型系统（人类 vs VLMs）提出的问题类型，突显了它们之间认知对齐的差距。', 'title_zh': 'Robusto-1 数据集：将人类与多模态预训练模型在来自秘鲁的实际分布外自主驾驶问答任务中进行比较'}
{'arxiv_id': 'arXiv:2503.06978', 'title': 'Lightweight Multimodal Artificial Intelligence Framework for Maritime Multi-Scene Recognition', 'authors': 'Xinyu Xi, Hua Yang, Shentai Zhang, Yijie Liu, Sijin Sun, Xiuju Fu', 'link': 'https://arxiv.org/abs/2503.06978', 'abstract': 'Maritime Multi-Scene Recognition is crucial for enhancing the capabilities of intelligent marine robotics, particularly in applications such as marine conservation, environmental monitoring, and disaster response. However, this task presents significant challenges due to environmental interference, where marine conditions degrade image quality, and the complexity of maritime scenes, which requires deeper reasoning for accurate recognition. Pure vision models alone are insufficient to address these issues. To overcome these limitations, we propose a novel multimodal Artificial Intelligence (AI) framework that integrates image data, textual descriptions and classification vectors generated by a Multimodal Large Language Model (MLLM), to provide richer semantic understanding and improve recognition accuracy. Our framework employs an efficient multimodal fusion mechanism to further enhance model robustness and adaptability in complex maritime environments. Experimental results show that our model achieves 98$\\%$ accuracy, surpassing previous SOTA models by 3.5$\\%$. To optimize deployment on resource-constrained platforms, we adopt activation-aware weight quantization (AWQ) as a lightweight technique, reducing the model size to 68.75MB with only a 0.5$\\%$ accuracy drop while significantly lowering computational overhead. This work provides a high-performance solution for real-time maritime scene recognition, enabling Autonomous Surface Vehicles (ASVs) to support environmental monitoring and disaster response in resource-limited settings.', 'abstract_zh': '海上多场景识别对于提升智能海洋机器人能力至关重要，特别是在海洋保护、环境监测和灾害响应等应用中。然而，由于环境干扰导致的图像质量问题以及海上场景的复杂性，使这一任务面临巨大挑战，要求进行更深层次的推理以确保准确识别。仅依赖纯视觉模型无法解决这些问题。为克服这些局限性，我们提出了一种新颖的多模态人工智能框架，该框架结合了图像数据、由多模态大语言模型（MLLM）生成的文本描述和分类向量，提供更丰富的语义理解并提高识别准确性。我们的框架采用高效的多模态融合机制，进一步增强模型在复杂海洋环境中的鲁棒性和适应性。实验结果表明，我们的模型准确率达到98%，比 previous SOTA 模型高3.5%。为了优化在资源受限平台上的部署，我们采用了激活意识权重量化（AWQ）作为一种轻量级技术，使模型大小减小到68.75MB，准确率仅下降0.5%，并显著降低了计算开销。这项工作提供了一种高性能的实时海上场景识别解决方案，使自主水面车辆（ASVs）能够在资源受限的环境中支持环境监测和灾害响应。', 'title_zh': '轻量级多模态人工智能框架海上多场景识别'}
{'arxiv_id': 'arXiv:2503.07259', 'title': 'COMODO: Cross-Modal Video-to-IMU Distillation for Efficient Egocentric Human Activity Recognition', 'authors': 'Baiyu Chen, Wilson Wongso, Zechen Li, Yonchanok Khaokaew, Hao Xue, Flora Salim', 'link': 'https://arxiv.org/abs/2503.07259', 'abstract': 'Egocentric video-based models capture rich semantic information and have demonstrated strong performance in human activity recognition (HAR). However, their high power consumption, privacy concerns, and dependence on lighting conditions limit their feasibility for continuous on-device recognition. In contrast, inertial measurement unit (IMU) sensors offer an energy-efficient and privacy-preserving alternative, yet they suffer from limited large-scale annotated datasets, leading to weaker generalization in downstream tasks. To bridge this gap, we propose COMODO, a cross-modal self-supervised distillation framework that transfers rich semantic knowledge from the video modality to the IMU modality without requiring labeled annotations. COMODO leverages a pretrained and frozen video encoder to construct a dynamic instance queue, aligning the feature distributions of video and IMU embeddings. By distilling knowledge from video representations, our approach enables the IMU encoder to inherit rich semantic information from video while preserving its efficiency for real-world applications. Experiments on multiple egocentric HAR datasets demonstrate that COMODO consistently improves downstream classification performance, achieving results comparable to or exceeding fully supervised fine-tuned models. Moreover, COMODO exhibits strong cross-dataset generalization. Benefiting from its simplicity, our method is also generally applicable to various video and time-series pre-trained models, offering the potential to leverage more powerful teacher and student foundation models in future research. The code is available at this https URL .', 'abstract_zh': '基于自监督跨模态 distillation 的 COMODO 框架将丰富的语义知识从视频模态转移至 IMU 模态，以提高隐私保护并降低能耗，从而在持续的设备端活动识别任务中更可行。', 'title_zh': 'COMODO: 跨模态视频到IMU的精简学习以实现高效的第一人称人体活动识别'}
{'arxiv_id': 'arXiv:2503.06973', 'title': 'A Multimodal Benchmark Dataset and Model for Crop Disease Diagnosis', 'authors': 'Xiang Liu, Zhaoxiang Liu, Huan Hu, Zezhou Chen, Kohou Wang, Kai Wang, Shiguo Lian', 'link': 'https://arxiv.org/abs/2503.06973', 'abstract': 'While conversational generative AI has shown considerable potential in enhancing decision-making for agricultural professionals, its exploration has predominantly been anchored in text-based interactions. The evolution of multimodal conversational AI, leveraging vast amounts of image-text data from diverse sources, marks a significant stride forward. However, the application of such advanced vision-language models in the agricultural domain, particularly for crop disease diagnosis, remains underexplored. In this work, we present the crop disease domain multimodal (CDDM) dataset, a pioneering resource designed to advance the field of agricultural research through the application of multimodal learning techniques. The dataset comprises 137,000 images of various crop diseases, accompanied by 1 million question-answer pairs that span a broad spectrum of agricultural knowledge, from disease identification to management practices. By integrating visual and textual data, CDDM facilitates the development of sophisticated question-answering systems capable of providing precise, useful advice to farmers and agricultural professionals. We demonstrate the utility of the dataset by finetuning state-of-the-art multimodal models, showcasing significant improvements in crop disease diagnosis. Specifically, we employed a novel finetuning strategy that utilizes low-rank adaptation (LoRA) to finetune the visual encoder, adapter and language model simultaneously. Our contributions include not only the dataset but also a finetuning strategy and a benchmark to stimulate further research in agricultural technology, aiming to bridge the gap between advanced AI techniques and practical agricultural applications. The dataset is available at https: //github.com/UnicomAI/UnicomBenchmark/tree/main/CDDMBench.', 'abstract_zh': '面向农作物疾病的多模态数据集（CDDM）：推动农业研究的多模态学习技术应用', 'title_zh': '多模态作物病害诊断数据集与模型'}
{'arxiv_id': 'arXiv:2503.06894', 'title': 'Improving cognitive diagnostics in pathology: a deep learning approach for augmenting perceptional understanding of histopathology images', 'authors': 'Xiaoqian Hu', 'link': 'https://arxiv.org/abs/2503.06894', 'abstract': 'In Recent Years, Digital Technologies Have Made Significant Strides In Augmenting-Human-Health, Cognition, And Perception, Particularly Within The Field Of Computational-Pathology. This Paper Presents A Novel Approach To Enhancing The Analysis Of Histopathology Images By Leveraging A Mult-modal-Model That Combines Vision Transformers (Vit) With Gpt-2 For Image Captioning. The Model Is Fine-Tuned On The Specialized Arch-Dataset, Which Includes Dense Image Captions Derived From Clinical And Academic Resources, To Capture The Complexities Of Pathology Images Such As Tissue Morphologies, Staining Variations, And Pathological Conditions. By Generating Accurate, Contextually Captions, The Model Augments The Cognitive Capabilities Of Healthcare Professionals, Enabling More Efficient Disease Classification, Segmentation, And Detection. The Model Enhances The Perception Of Subtle Pathological Features In Images That Might Otherwise Go Unnoticed, Thereby Improving Diagnostic Accuracy. Our Approach Demonstrates The Potential For Digital Technologies To Augment Human Cognitive Abilities In Medical Image Analysis, Providing Steps Toward More Personalized And Accurate Healthcare Outcomes.', 'abstract_zh': '近年来，数字技术在增强人类健康、认知和感知方面取得了显著进展，特别是体现在计算病理学领域。本文提出了一种通过结合视觉变换器（ViT）和GPT-2进行图像描述的新方法，以提升组织病理学图像的分析能力。该模型在专门的Arch-数据集上进行微调，该数据集包含从临床和学术资源提取的密集图像描述，用于捕捉病理学图像的复杂性，如组织形态、染色变异和病理状况。通过生成准确且上下文相关的描述，该模型增强了医疗保健专业人员的认知能力，有助于更高效地进行疾病分类、分割和检测。该模型增强了对图像中细微病理特征的感知，从而提高了诊断准确性。本文展示了数字技术在医学图像分析中增强人类认知能力的潜力，朝着更个性化和准确的 healthcare 出come迈进。', 'title_zh': '提升病理认知诊断：一种增强组织病理图像感知理解的深度学习方法'}
{'arxiv_id': 'arXiv:2503.06828', 'title': 'Towards a Multimodal MRI-Based Foundation Model for Multi-Level Feature Exploration in Segmentation, Molecular Subtyping, and Grading of Glioma', 'authors': 'Somayeh Farahani, Marjaneh Hejazi, Antonio Di Ieva, Emad Fatemizadeh, Sidong Liu', 'link': 'https://arxiv.org/abs/2503.06828', 'abstract': 'Accurate, noninvasive glioma characterization is crucial for effective clinical management. Traditional methods, dependent on invasive tissue sampling, often fail to capture the spatial heterogeneity of the tumor. While deep learning has improved segmentation and molecular profiling, few approaches simultaneously integrate tumor morphology and molecular features. Foundation deep learning models, which learn robust, task-agnostic representations from large-scale datasets, hold great promise but remain underutilized in glioma imaging biomarkers. We propose the Multi-Task SWIN-UNETR (MTS-UNET) model, a novel foundation-based framework built on the BrainSegFounder model, pretrained on large-scale neuroimaging data. MTS-UNET simultaneously performs glioma segmentation, histological grading, and molecular subtyping (IDH mutation and 1p/19q co-deletion). It incorporates two key modules: Tumor-Aware Feature Encoding (TAFE) for multi-scale, tumor-focused feature extraction and Cross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch signals associated with IDH mutation. The model was trained and validated on a diverse, multi-center cohort of 2,249 glioma patients from seven public datasets. MTS-UNET achieved a mean Dice score of 84% for segmentation, along with AUCs of 90.58% for IDH mutation, 69.22% for 1p/19q co-deletion prediction, and 87.54% for grading, significantly outperforming baseline models (p<=0.05). Ablation studies validated the essential contributions of the TAFE and CMD modules and demonstrated the robustness of the framework. The foundation-based MTS-UNET model effectively integrates tumor segmentation with multi-level classification, exhibiting strong generalizability across diverse MRI datasets. This framework shows significant potential for advancing noninvasive, personalized glioma management by improving predictive accuracy and interpretability.', 'abstract_zh': '准确无创胶质瘤表征对于有效临床管理至关重要。传统的依赖侵入性组织取样的方法通常无法捕捉肿瘤的空间异质性。虽然深度学习提高了分割和分子谱型分析的能力，但很少有方法能够同时整合肿瘤形态和分子特征。基于基础的深度学习模型，通过学习大规模数据集中的稳健任务无关表示，具有巨大的潜力但在胶质瘤成像生物标志物中的应用仍然不足。我们提出了多任务SWIN-UNETR（MTS-UNET）模型，该模型基于BrainSegFounder模型构建，并在大规模神经影像数据上进行了预训练。MTS-UNET同时执行胶质瘤分割、组织学分级和分子亚型分类（IDH突变和1p/19q共丢失）。该模型包含两个关键模块：肿瘤意识特征编码（TAFE）用于多尺度、肿瘤相关的特征提取，以及跨模态差异（CMD）用于突出与IDH突变相关的细小T2-FLAIR不匹配信号。该模型在七个公开数据集的2249例多中心胶质瘤患者中进行了训练和验证。MTS-UNET在分割上的平均Dice分数为84%，IDH突变的AUC为90.58%，1p/19q共丢失预测的AUC为69.22%，分级的AUC为87.54%，显著优于基线模型（p≤0.05）。消融研究验证了TAFE和CMD模块的关键贡献，并展示了该框架的稳健性。基于基础的MTS-UNET模型有效地将肿瘤分割与多层次分类结合起来，展示了在多种MRI数据集上的强泛化能力。该框架展示了通过提高预测准确性和可解释性来促进无创、个性化胶质瘤管理的重要潜力。', 'title_zh': '基于多模态MRI的胶质瘤多级特征探索、分子亚类分型和分级的基础模型研究'}
{'arxiv_id': 'arXiv:2503.06764', 'title': 'SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation', 'authors': 'Zisheng Chen, Chunwei Wang, Xiuwei Chen, Hang Xu, Jianhua Han, Xiandan Liang', 'link': 'https://arxiv.org/abs/2503.06764', 'abstract': 'We present SemHiTok, a unified image Tokenizer via Semantic-Guided Hierarchical codebook that provides consistent discrete feature representations for multimodal understanding and generation tasks. Recently, unified multimodal large models (MLLMs) for understanding and generation have sparked exploration within research community. Previous works attempt to train a unified image tokenizer by combining loss functions for semantic feature reconstruction and pixel reconstruction. However, due to the differing levels of features prioritized by multimodal understanding and generation tasks, joint training methods face significant challenges in achieving a good trade-off. SemHiTok addresses this challenge through Semantic-Guided Hierarchical codebook which builds texture sub-codebooks on pre-trained semantic codebook. This design decouples the training of semantic reconstruction and pixel reconstruction and equips the tokenizer with low-level texture feature extraction capability without degradation of high-level semantic feature extraction ability. Our experiments demonstrate that SemHiTok achieves state-of-the-art rFID score at 256X256resolution compared to other unified tokenizers, and exhibits competitive performance on multimodal understanding and generation tasks.', 'abstract_zh': 'SemHiTok：一种基于语义引导层次码本的统一图像分词器', 'title_zh': 'SemHiTok: 基于语义引导多层次码本的统一图像分词器及其在多模态理解和生成中的应用'}
{'arxiv_id': 'arXiv:2503.06405', 'title': 'Heterogeneous bimodal attention fusion for speech emotion recognition', 'authors': 'Jiachen Luo, Huy Phan, Lin Wang, Joshua Reiss', 'link': 'https://arxiv.org/abs/2503.06405', 'abstract': 'Multi-modal emotion recognition in conversations is a challenging problem due to the complex and complementary interactions between different modalities. Audio and textual cues are particularly important for understanding emotions from a human perspective. Most existing studies focus on exploring interactions between audio and text modalities at the same representation level. However, a critical issue is often overlooked: the heterogeneous modality gap between low-level audio representations and high-level text representations. To address this problem, we propose a novel framework called Heterogeneous Bimodal Attention Fusion (HBAF) for multi-level multi-modal interaction in conversational emotion recognition. The proposed method comprises three key modules: the uni-modal representation module, the multi-modal fusion module, and the inter-modal contrastive learning module. The uni-modal representation module incorporates contextual content into low-level audio representations to bridge the heterogeneous multi-modal gap, enabling more effective fusion. The multi-modal fusion module uses dynamic bimodal attention and a dynamic gating mechanism to filter incorrect cross-modal relationships and fully exploit both intra-modal and inter-modal interactions. Finally, the inter-modal contrastive learning module captures complex absolute and relative interactions between audio and text modalities. Experiments on the MELD and IEMOCAP datasets demonstrate that the proposed HBAF method outperforms existing state-of-the-art baselines.', 'abstract_zh': '多模态对话情感识别中的异构模态差距给跨模态情感识别带来了挑战。由于不同模态间的复杂互补交互，从人类视角理解情感尤为关键。现有大多数研究集中在探索音频和文本模态在同一表示层面上的交互。然而，一个关键问题被忽视了：低级音频表示与高级文本表示之间的异构模态差距。为解决这一问题，我们提出了一种名为Heterogeneous Bimodal Attention Fusion (HBAF)的新框架，用于对话情感识别中的多层多模态交互。该方法包含三个关键模块：单模态表示模块、多模态融合模块和跨模态对比学习模块。单模态表示模块将上下文内容融入低级音频表示中，以弥合异构多模态差距，实现更有效的融合。多模态融合模块利用动态双模态注意力和动态门控机制过滤错误的跨模态关系，充分挖掘内在模态和跨模态交互。最后，跨模态对比学习模块捕获音频和文本模态之间复杂绝对和相对交互。在MELD和IEMOCAP数据集上的实验表明，提出的HBAF方法优于现有最先进的基线方法。', 'title_zh': '异质双模态注意力融合在语音情感识别中的应用'}
{'arxiv_id': 'arXiv:2503.06287', 'title': 'Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding', 'authors': 'Seil Kang, Jinyeong Kim, Junhyeok Kim, Seong Jae Hwang', 'link': 'https://arxiv.org/abs/2503.06287', 'abstract': 'Visual grounding seeks to localize the image region corresponding to a free-form text description. Recently, the strong multimodal capabilities of Large Vision-Language Models (LVLMs) have driven substantial improvements in visual grounding, though they inevitably require fine-tuning and additional model components to explicitly generate bounding boxes or segmentation masks. However, we discover that a few attention heads in frozen LVLMs demonstrate strong visual grounding capabilities. We refer to these heads, which consistently capture object locations related to text semantics, as localization heads. Using localization heads, we introduce a straightforward and effective training-free visual grounding framework that utilizes text-to-image attention maps from localization heads to identify the target objects. Surprisingly, only three out of thousands of attention heads are sufficient to achieve competitive localization performance compared to existing LVLM-based visual grounding methods that require fine-tuning. Our findings suggest that LVLMs can innately ground objects based on a deep comprehension of the text-image relationship, as they implicitly focus on relevant image regions to generate informative text outputs. All the source codes will be made available to the public.', 'abstract_zh': '视觉定位旨在定位与自由形式文本描述相对应的图像区域。近年来，大型多模态模型（LVLMs）的强大能力推动了视觉定位的显著进步，尽管它们不可避免地需要微调和附加模型组件以明确生成边界框或分割掩码。然而，我们发现冻结的LVLM中的一些建模注意力头展示了强大的视觉定位能力。我们将这些注意力头称为定位头，这些头能够一致地捕获与文本语义相关的物体位置。使用定位头，我们提出了一种简单且有效的无需训练的视觉定位框架，利用定位头的文本到图像注意力图来识别目标物体。令人惊讶的是，仅需数千个注意力头中不足三个即可实现与现有需要微调的LVLM基视觉定位方法相当的定位性能。我们的研究结果表明，LVLM能够基于对图文关系的深层理解，天生能够将物体进行视觉定位，它们隐式地关注相关的图像区域以生成有意义的文本输出。所有源代码将对公众开放。', 'title_zh': '你的大型 vision-language 模型只需几个注意力头即可实现视觉定位。'}
{'arxiv_id': 'arXiv:2503.06211', 'title': 'Text-Speech Language Models with Improved Cross-Modal Transfer by Aligning Abstraction Levels', 'authors': 'Santiago Cuervo, Adel Moumen, Yanis Labrak, Sameer Khurana, Antoine Laurent, Mickael Rouvier, Ricard Marxer', 'link': 'https://arxiv.org/abs/2503.06211', 'abstract': 'Text-Speech Language Models (TSLMs) -- language models trained to jointly process and generate text and speech -- aim to enable cross-modal knowledge transfer to overcome the scaling limitations of unimodal speech LMs. The predominant approach to TSLM training expands the vocabulary of a pre-trained text LM by appending new embeddings and linear projections for speech, followed by fine-tuning on speech data. We hypothesize that this method limits cross-modal transfer by neglecting feature compositionality, preventing text-learned functions from being fully leveraged at appropriate abstraction levels. To address this, we propose augmenting vocabulary expansion with modules that better align abstraction levels across layers. Our models, \\textsc{SmolTolk}, rival or surpass state-of-the-art TSLMs trained with orders of magnitude more compute. Representation analyses and improved multimodal performance suggest our method enhances cross-modal transfer.', 'abstract_zh': 'Text-Speech 语言模型 (TSLMs) -- 旨在联合处理和生成文本与语音的语言模型 -- 力求克服单模态语音语言模型的规模限制，实现跨模态知识的转移。主流的 TSLM 训练方法通过在预训练文本语言模型的词汇表中附加新的语音嵌入和线性投影，并随后在语音数据上进行微调。我们认为这种方法通过忽视特征组合性，限制了文本学习功能在适当抽象层次上的充分利用。为此，我们提议使用更好地对齐各层抽象级别的模块来扩充词汇表。我们的模型 \\textsc{SmolTolk} 在计算量小数量级的情况下，与最先进的 TSLMs 具备相当或更优的表现。代表性和增强的跨模态性能分析表明，我们的方法可以增强跨模态转移。', 'title_zh': '具有改进跨模态转移的文本-语音语言模型通过抽象层次对齐'}
{'arxiv_id': 'arXiv:2503.06108', 'title': 'Multi-modal expressive personality recognition in data non-ideal audiovisual based on multi-scale feature enhancement and modal augment', 'authors': 'Weixuan Kong, Jinpeng Yu, Zijun Li, Hanwei Liu, Jiqing Qu, Hui Xiao, Xuefeng Li', 'link': 'https://arxiv.org/abs/2503.06108', 'abstract': "Automatic personality recognition is a research hotspot in the intersection of computer science and psychology, and in human-computer interaction, personalised has a wide range of applications services and other scenarios. In this paper, an end-to-end multimodal performance personality is established for both visual and auditory modal datarecognition network , and the through feature-level fusion , which effectively of the two modalities is carried out the cross-attention mechanismfuses the features of the two modal data; and a is proposed multiscale feature enhancement modalitiesmodule , which enhances for visual and auditory boththe expression of the information of effective the features and suppresses the interference of the redundant information. In addition, during the training process, this paper proposes a modal enhancement training strategy to simulate non-ideal such as modal loss and noise interferencedata situations , which enhances the adaptability ofand the model to non-ideal data scenarios improves the robustness of the model. Experimental results show that the method proposed in this paper is able to achieve an average Big Five personality accuracy of , which outperforms existing 0.916 on the personality analysis dataset ChaLearn First Impressionother methods based on audiovisual and audio-visual both modalities. The ablation experiments also validate our proposed , respectivelythe contribution of module and modality enhancement strategy to the model performance. Finally, we simulate in the inference phase multi-scale feature enhancement six non-ideal data scenarios to verify the modal enhancement strategy's improvement in model robustness.", 'abstract_zh': '自动人格识别是计算机科学与心理学交叉领域以及人机交互中的研究热点，个性化服务等场景具有广泛的应用。本文建立了一个端到端的多模态性能人格识别网络，通过特征级融合，利用交叉注意力机制有效融合视听模态特征，并提出了一种多尺度特征增强模块，增强视听模态的有效信息表达并抑制冗余信息干扰。此外，在训练过程中，本文提出了一种模态增强训练策略，模拟诸如模态失真和噪声干扰等非理想情况，提高模型对非理想数据场景的适应性和鲁棒性。实验结果表明，本文提出的方法在ChaLearn First Impression人格分析数据集上实现了平均五大人格特质识别准确率为0.916，优于基于视听和音视频模态的其他方法。消融实验进一步验证了所提出的模块和模态增强策略对模型性能的贡献。最后，在推理阶段模拟六种非理想数据场景，验证模态增强策略对模型鲁棒性的提升。', 'title_zh': '基于多尺度特征增强和模态扩增的非理想音频视觉数据多模态表达性个性识别'}
{'arxiv_id': 'arXiv:2503.05858', 'title': 'Bimodal Connection Attention Fusion for Speech Emotion Recognition', 'authors': 'Jiachen Luo, Huy Phan, Lin Wang, Joshua D. Reiss', 'link': 'https://arxiv.org/abs/2503.05858', 'abstract': 'Multi-modal emotion recognition is challenging due to the difficulty of extracting features that capture subtle emotional differences. Understanding multi-modal interactions and connections is key to building effective bimodal speech emotion recognition systems. In this work, we propose Bimodal Connection Attention Fusion (BCAF) method, which includes three main modules: the interactive connection network, the bimodal attention network, and the correlative attention network. The interactive connection network uses an encoder-decoder architecture to model modality connections between audio and text while leveraging modality-specific features. The bimodal attention network enhances semantic complementation and exploits intra- and inter-modal interactions. The correlative attention network reduces cross-modal noise and captures correlations between audio and text. Experiments on the MELD and IEMOCAP datasets demonstrate that the proposed BCAF method outperforms existing state-of-the-art baselines.', 'abstract_zh': '双向模态连接注意力融合方法（BCAF）在多模态情感识别中的应用', 'title_zh': '双模态连接注意力融合在语音情感识别中的应用'}
{'arxiv_id': 'arXiv:2503.05778', 'title': 'DreamNet: A Multimodal Framework for Semantic and Emotional Analysis of Sleep Narratives', 'authors': 'Tapasvi Panchagnula', 'link': 'https://arxiv.org/abs/2503.05778', 'abstract': 'Dream narratives provide a unique window into human cognition and emotion, yet their systematic analysis using artificial intelligence has been underexplored. We introduce DreamNet, a novel deep learning framework that decodes semantic themes and emotional states from textual dream reports, optionally enhanced with REM-stage EEG data. Leveraging a transformer-based architecture with multimodal attention, DreamNet achieves 92.1% accuracy and 88.4% F1-score in text-only mode (DNet-T) on a curated dataset of 1,500 anonymized dream narratives, improving to 99.0% accuracy and 95.2% F1-score with EEG integration (DNet-M). Strong dream-emotion correlations (e.g., falling-anxiety, r = 0.91, p < 0.01) highlight its potential for mental health diagnostics, cognitive science, and personalized therapy. This work provides a scalable tool, a publicly available enriched dataset, and a rigorous methodology, bridging AI and psychological research.', 'abstract_zh': '梦叙事提供了一扇洞察人类认知和情感的独特窗口， yet 其人工智能驱动的系统分析尚属未探索领域。我们引入了 DreamNet，一种新颖的深度学习框架，用于从文本梦境报告中解码语义主题和情绪状态，可选地增强以 REM 阶段的 EEG 数据。利用基于变换器的架构和多模态注意力，DreamNet 在仅文本模式（DNet-T）下对 1,500 份匿名梦叙事精心筛选的数据集实现了 92.1% 的准确率和 88.4% 的 F1 分数，而在结合 EEG 数据后（DNet-M）则分别提高到 99.0% 和 95.2%。强烈的梦与情绪关联（例如，掉落-焦虑，r = 0.91，p < 0.01）突显了其在心理健康诊断、认知科学和个人化治疗方面的潜力。这项工作提供了一个可扩展的工具、一个公开的丰富数据集和一种严格的 方法论，融合了人工智能和心理学研究。', 'title_zh': 'DreamNet：多模态睡眠叙事语义与情感分析框架'}
