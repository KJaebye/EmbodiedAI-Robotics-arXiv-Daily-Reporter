{'arxiv_id': 'arXiv:2506.21077', 'title': 'CURL-SLAM: Continuous and Compact LiDAR Mapping', 'authors': 'Kaicheng Zhang, Shida Xu, Yining Ding, Xianwen Kong, Sen Wang', 'link': 'https://arxiv.org/abs/2506.21077', 'abstract': "This paper studies 3D LiDAR mapping with a focus on developing an updatable and localizable map representation that enables continuity, compactness and consistency in 3D maps. Traditional LiDAR Simultaneous Localization and Mapping (SLAM) systems often rely on 3D point cloud maps, which typically require extensive storage to preserve structural details in large-scale environments. In this paper, we propose a novel paradigm for LiDAR SLAM by leveraging the Continuous and Ultra-compact Representation of LiDAR (CURL) introduced in [1]. Our proposed LiDAR mapping approach, CURL-SLAM, produces compact 3D maps capable of continuous reconstruction at variable densities using CURL's spherical harmonics implicit encoding, and achieves global map consistency after loop closure. Unlike popular Iterative Closest Point (ICP)-based LiDAR odometry techniques, CURL-SLAM formulates LiDAR pose estimation as a unique optimization problem tailored for CURL and extends it to local Bundle Adjustment (BA), enabling simultaneous pose refinement and map correction. Experimental results demonstrate that CURL-SLAM achieves state-of-the-art 3D mapping quality and competitive LiDAR trajectory accuracy, delivering sensor-rate real-time performance (10 Hz) on a CPU. We will release the CURL-SLAM implementation to the community.", 'abstract_zh': '基于CURL的可更新和定位的3D LiDAR映射研究', 'title_zh': 'CURL-SLAM: 连续且紧凑的激光雷达建图'}
{'arxiv_id': 'arXiv:2506.20969', 'title': 'ThermalDiffusion: Visual-to-Thermal Image-to-Image Translation for Autonomous Navigation', 'authors': 'Shruti Bansal, Wenshan Wang, Yifei Liu, Parv Maheshwari', 'link': 'https://arxiv.org/abs/2506.20969', 'abstract': 'Autonomous systems rely on sensors to estimate the environment around them. However, cameras, LiDARs, and RADARs have their own limitations. In nighttime or degraded environments such as fog, mist, or dust, thermal cameras can provide valuable information regarding the presence of objects of interest due to their heat signature. They make it easy to identify humans and vehicles that are usually at higher temperatures compared to their surroundings. In this paper, we focus on the adaptation of thermal cameras for robotics and automation, where the biggest hurdle is the lack of data. Several multi-modal datasets are available for driving robotics research in tasks such as scene segmentation, object detection, and depth estimation, which are the cornerstone of autonomous systems. However, they are found to be lacking in thermal imagery. Our paper proposes a solution to augment these datasets with synthetic thermal data to enable widespread and rapid adaptation of thermal cameras. We explore the use of conditional diffusion models to convert existing RGB images to thermal images using self-attention to learn the thermal properties of real-world objects.', 'abstract_zh': '自主系统依赖传感器来估计其周围的环境。然而，摄像头、LiDAR和RADAR各自都有局限性。在夜间或能见度降低的环境中，如雾、霭或尘埃，热成像相机由于其热信号可以提供有关目标物体存在的有价值的信息。它们使得识别通常比周围环境温度更高的行人和车辆变得容易。本文专注于将热成像相机应用于机器人技术和自动化领域，最大的障碍是缺乏数据。有多模态数据集可供驾驶机器人研究使用，这些数据集在场景分割、物体检测和深度估计等任务中至关重要，是自主系统的基础。然而，这些数据集缺乏热图像。本文提出了一种解决方案，通过加入合成热数据来增强这些数据集，以便广泛快速地适应热成像相机。我们探讨了使用条件扩散模型将现有的RGB图像转换为热图像的方法，并利用自注意力机制学习真实世界物体的热特性。', 'title_zh': '热扩散：视觉到红外的图像到图像翻译在自主导航中的应用'}
{'arxiv_id': 'arXiv:2506.21420', 'title': 'EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting', 'authors': 'Taoyu Wu, Yiyi Miao, Zhuoxiao Li, Haocheng Zhao, Kang Dang, Jionglong Su, Limin Yu, Haoang Li', 'link': 'https://arxiv.org/abs/2506.21420', 'abstract': 'Efficient three-dimensional reconstruction and real-time visualization are critical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in efficient 3D reconstruction and rendering. Most 3DGS-based Simultaneous Localization and Mapping (SLAM) methods only rely on the appearance constraints for optimizing both 3DGS and camera poses. However, in endoscopic scenarios, the challenges include photometric inconsistencies caused by non-Lambertian surfaces and dynamic motion from breathing affects the performance of SLAM systems. To address these issues, we additionally introduce optical flow loss as a geometric constraint, which effectively constrains both the 3D structure of the scene and the camera motion. Furthermore, we propose a depth regularisation strategy to mitigate the problem of photometric inconsistencies and ensure the validity of 3DGS depth rendering in endoscopic scenes. In addition, to improve scene representation in the SLAM system, we improve the 3DGS refinement strategy by focusing on viewpoints corresponding to Keyframes with suboptimal rendering quality frames, achieving better rendering results. Extensive experiments on the C3VD static dataset and the StereoMIS dynamic dataset demonstrate that our method outperforms existing state-of-the-art methods in novel view synthesis and pose estimation, exhibiting high performance in both static and dynamic surgical scenes. The source code will be publicly available upon paper acceptance.', 'abstract_zh': '高效三维重建和实时可视化在内窥镜等外科场景中至关重要。近年来，3D高斯点积（3DGS）在高效三维重建与渲染方面展现了显著性能。大多数基于3DGS的SLAM方法仅依赖外观约束来优化3DGS和相机姿态。然而，在内窥镜场景中，非朗伯表面导致的光度不一致性及呼吸引起的动态运动影响了SLAM系统的性能。为解决这些问题，我们还引入了光流损失作为几何约束，有效限制了场景的3D结构和相机运动。此外，我们提出了深度正则化策略，以减轻光度不一致性问题，并确保3DGS在内窥镜场景中的深度渲染有效性。为了改进SLAM系统中的场景表示，我们通过关注渲染质量不佳的关键帧对应的视角来改进3DGS细化策略，从而获得更好的渲染结果。在C3VD静态数据集和StereoMIS动态数据集上的广泛实验显示，我们的方法在新颖视图合成和姿态估计方面优于现有最先进的方法，在静态和动态外科场景中表现出高性能。论文被接受后，源代码将公开。', 'title_zh': 'EndoFlow-SLAM：基于流约束高斯点云的实时内窥镜SLAM'}
{'arxiv_id': 'arXiv:2506.21358', 'title': 'ToosiCubix: Monocular 3D Cuboid Labeling via Vehicle Part Annotations', 'authors': 'Behrooz Nasihatkon, Hossein Resani, Amirreza Mehrzadian', 'link': 'https://arxiv.org/abs/2506.21358', 'abstract': "Many existing methods for 3D cuboid annotation of vehicles rely on expensive and carefully calibrated camera-LiDAR or stereo setups, limiting their accessibility for large-scale data collection. We introduce ToosiCubix, a simple yet powerful approach for annotating ground-truth cuboids using only monocular images and intrinsic camera parameters. Our method requires only about 10 user clicks per vehicle, making it highly practical for adding 3D annotations to existing datasets originally collected without specialized equipment. By annotating specific features (e.g., wheels, car badge, symmetries) across different vehicle parts, we accurately estimate each vehicle's position, orientation, and dimensions up to a scale ambiguity (8 DoF). The geometric constraints are formulated as an optimization problem, which we solve using a coordinate descent strategy, alternating between Perspective-n-Points (PnP) and least-squares subproblems. To handle common ambiguities such as scale and unobserved dimensions, we incorporate probabilistic size priors, enabling 9 DoF cuboid placements. We validate our annotations against the KITTI and Cityscapes3D datasets, demonstrating that our method offers a cost-effective and scalable solution for high-quality 3D cuboid annotation.", 'abstract_zh': '一种仅使用单目图像和内窥镜相机参数进行车辆3D立方体标注的简单有效方法：ToosiCubix', 'title_zh': 'ToosiCubix: 通过车辆部件标注的单目3D立方体标定'}
{'arxiv_id': 'arXiv:2506.21234', 'title': 'Real-Time ESFP: Estimating, Smoothing, Filtering, and Pose-Mapping', 'authors': 'Qifei Cui, Yuang Zhou, Ruichen Deng', 'link': 'https://arxiv.org/abs/2506.21234', 'abstract': "This paper presents ESFP, an end-to-end pipeline that converts monocular RGB video into executable joint trajectories for a low-cost 4-DoF desktop arm. ESFP comprises four sequential modules. (1) Estimating: ROMP lifts each frame to a 24-joint 3-D skeleton. (2) Smoothing: the proposed HPSTM-a sequence-to-sequence Transformer with self-attention-combines long-range temporal context with a differentiable forward-kinematics decoder, enforcing constant bone lengths and anatomical plausibility while jointly predicting joint means and full covariances. (3) Filtering: root-normalized trajectories are variance-weighted according to HPSTM's uncertainty estimates, suppressing residual noise. (4) Pose-Mapping: a geometric retargeting layer transforms shoulder-elbow-wrist triples into the uArm's polar workspace, preserving wrist orientation.", 'abstract_zh': 'ESFP：一种从单目RGB视频端到端生成低成本4-DOF桌面臂可执行关节轨迹的流水线', 'title_zh': '实时ESFP：估计、平滑、滤波与姿态映射'}
{'arxiv_id': 'arXiv:2506.21198', 'title': 'Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation', 'authors': 'Yihong Cao, Jiaming Zhang, Xu Zheng, Hao Shi, Kunyu Peng, Hang Liu, Kailun Yang, Hui Zhang', 'link': 'https://arxiv.org/abs/2506.21198', 'abstract': 'Panoramic image processing is essential for omni-context perception, yet faces constraints like distortions, perspective occlusions, and limited annotations. Previous unsupervised domain adaptation methods transfer knowledge from labeled pinhole data to unlabeled panoramic images, but they require access to source pinhole data. To address these, we introduce a more practical task, i.e., Source-Free Occlusion-Aware Seamless Segmentation (SFOASS), and propose its first solution, called UNconstrained Learning Omni-Context Knowledge (UNLOCK). Specifically, UNLOCK includes two key modules: Omni Pseudo-Labeling Learning and Amodal-Driven Context Learning. While adapting without relying on source data or target labels, this framework enhances models to achieve segmentation with 360° viewpoint coverage and occlusion-aware reasoning. Furthermore, we benchmark the proposed SFOASS task through both real-to-real and synthetic-to-real adaptation settings. Experimental results show that our source-free method achieves performance comparable to source-dependent methods, yielding state-of-the-art scores of 10.9 in mAAP and 11.6 in mAP, along with an absolute improvement of +4.3 in mAPQ over the source-only method. All data and code will be made publicly available at this https URL.', 'abstract_zh': '无源域适应全景图像无遮挡感知无缝分割', 'title_zh': '解锁约束：无源域aware遮挡自洽分割'}
{'arxiv_id': 'arXiv:2506.21185', 'title': 'Out-of-Distribution Semantic Occupancy Prediction', 'authors': 'Yuheng Zhang, Mengfei Duan, Kunyu Peng, Yuhang Wang, Ruiping Liu, Fei Teng, Kai Luo, Zhiyong Li, Kailun Yang', 'link': 'https://arxiv.org/abs/2506.21185', 'abstract': '3D Semantic Occupancy Prediction is crucial for autonomous driving, providing a dense, semantically rich environmental representation. However, existing methods focus on in-distribution scenes, making them susceptible to Out-of-Distribution (OoD) objects and long-tail distributions, which increases the risk of undetected anomalies and misinterpretations, posing safety hazards. To address these challenges, we introduce Out-of-Distribution Semantic Occupancy Prediction, targeting OoD detection in 3D voxel space. To fill the gaps in the dataset, we propose a Synthetic Anomaly Integration Pipeline that injects synthetic anomalies while preserving realistic spatial and occlusion patterns, enabling the creation of two datasets: VAA-KITTI and VAA-KITTI-360. We introduce OccOoD, a novel framework integrating OoD detection into 3D semantic occupancy prediction, with Voxel-BEV Progressive Fusion (VBPF) leveraging an RWKV-based branch to enhance OoD detection via geometry-semantic fusion. Experimental results demonstrate that OccOoD achieves state-of-the-art OoD detection with an AuROC of 67.34% and an AuPRCr of 29.21% within a 1.2m region, while maintaining competitive occupancy prediction performance. The established datasets and source code will be made publicly available at this https URL.', 'abstract_zh': '3D语义占用度外分布检测对于自主驾驶至关重要，它提供了密集且语义丰富的环境表示。然而，现有方法主要关注于分布内的场景，这使得它们容易受到外分布（OoD）对象和长尾分布的影响，增加了未检测异常和误解释的风险，从而带来安全隐患。为解决这些问题，我们提出了外分布语义占用度预测，针对3D体素空间中的OoD检测。为了填补数据集的空白，我们提出了一种合成异常集成管道，该管道在保留现实空间和遮挡模式的同时注入合成异常，从而创建了两个数据集：VAA-KITTI和VAA-KITTI-360。我们提出了OccOoD，这是一种新的框架，将OoD检测集成到3D语义占用度预测中，其中体素-鸟瞰图渐进融合（VBPF）通过几何-语义融合利用RWKV基分支来增强OoD检测。实验结果表明，OccOoD在1.2米区域内的AuROC为67.34%，AuPRCr为29.21%，实现了最先进的OoD检测性能，同时保持了竞争力的占用度预测性能。已建立的数据集和源代码将在此处公开。', 'title_zh': '分布外语义占用预测'}
{'arxiv_id': 'arXiv:2506.21121', 'title': 'GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction', 'authors': 'Muleilan Pei, Shaoshuai Shi, Lu Zhang, Peiliang Li, Shaojie Shen', 'link': 'https://arxiv.org/abs/2506.21121', 'abstract': 'Trajectory prediction for surrounding agents is a challenging task in autonomous driving due to its inherent uncertainty and underlying multimodality. Unlike prevailing data-driven methods that primarily rely on supervised learning, in this paper, we introduce a novel Graph-oriented Inverse Reinforcement Learning (GoIRL) framework, which is an IRL-based predictor equipped with vectorized context representations. We develop a feature adaptor to effectively aggregate lane-graph features into grid space, enabling seamless integration with the maximum entropy IRL paradigm to infer the reward distribution and obtain the policy that can be sampled to induce multiple plausible plans. Furthermore, conditioned on the sampled plans, we implement a hierarchical parameterized trajectory generator with a refinement module to enhance prediction accuracy and a probability fusion strategy to boost prediction confidence. Extensive experimental results showcase our approach not only achieves state-of-the-art performance on the large-scale Argoverse & nuScenes motion forecasting benchmarks but also exhibits superior generalization abilities compared to existing supervised models.', 'abstract_zh': '基于图的逆强化学习（GoIRL）框架在自动驾驶中的路径预测', 'title_zh': 'GoIRL: 面向图的逆强化学习 multimodal 轨迹预测'}
{'arxiv_id': 'arXiv:2506.20795', 'title': 'How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?', 'authors': 'Stephanie Käs, Anton Burenko, Louis Markert, Onur Alp Culha, Dennis Mack, Timm Linder, Bastian Leibe', 'link': 'https://arxiv.org/abs/2506.20795', 'abstract': 'Gestures enable non-verbal human-robot communication, especially in noisy environments like agile production. Traditional deep learning-based gesture recognition relies on task-specific architectures using images, videos, or skeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs) and Vision Language Models (VLMs) with their strong generalization abilities offer potential to reduce system complexity by replacing dedicated task-specific modules. This study investigates adapting such models for dynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art VFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing skeleton-based approach). We introduce NUGGET, a dataset tailored for human-robot communication in intralogistics environments, to evaluate the different gesture recognition approaches. In our experiments, HD-GCN achieves best performance, but V-JEPA comes close with a simple, task-specific classification head - thus paving a possible way towards reducing system complexity, by using it as a shared multi-task model. In contrast, Gemini struggles to differentiate gestures based solely on textual descriptions in the zero-shot setting, highlighting the need of further research on suitable input representations for gestures.', 'abstract_zh': '手势使非言语的人机通信成为可能，特别是在像 agile 生产这样噪声环境中的应用。传统的基于深度学习的手势识别依赖于特定任务的架构，使用图像、视频或骨骼姿态估计作为输入。同时，具有强大泛化能力的视觉基础模型（VFMs）和视觉语言模型（VLMs）有可能通过替代专用的特定任务模块来简化系统复杂性。本研究调查了将此类模型适应于动态的全身心势识别，并对比了 V-JEPA（最先进的 VFMs）、Gemini Flash 2.0（一种多模态 VLM）和 HD-GCN（一种表现最佳的基于骨骼的方法）。我们引入了 NUGGET 数据集，该数据集专门针对仓储物流环境中的手语识别，以评估不同的手语识别方法。在我们的实验中，HD-GCN 达到了最佳性能，而 V-JEPA 通过一个简单的特定任务分类头达到了相近的性能，从而为使用其作为共享多任务模型以减少系统复杂性铺平了一条可能的道路。相比之下，Gemini 在零样本设置下仅凭文字描述难以区分手势，这突显了进一步研究适合手势识别的输入表示形式所需。', 'title_zh': '基于骨骼的动作识别：基础模型与骨架基方法在人机交互中的对比研究'}
{'arxiv_id': 'arXiv:2505.12246', 'title': 'SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving', 'authors': 'Muleilan Pei, Jiayao Shan, Peiliang Li, Jieqi Shi, Jing Huo, Yang Gao, Shaojie Shen', 'link': 'https://arxiv.org/abs/2505.12246', 'abstract': "Online scene perception and topology reasoning are critical for autonomous vehicles to understand their driving environments, particularly for mapless driving systems that endeavor to reduce reliance on costly High-Definition (HD) maps. However, recent advances in online scene understanding still face limitations, especially in long-range or occluded scenarios, due to the inherent constraints of onboard sensors. To address this challenge, we propose a Standard-Definition (SD) Map Enhanced scene Perception and Topology reasoning (SEPT) framework, which explores how to effectively incorporate the SD map as prior knowledge into existing perception and reasoning pipelines. Specifically, we introduce a novel hybrid feature fusion strategy that combines SD maps with Bird's-Eye-View (BEV) features, considering both rasterized and vectorized representations, while mitigating potential misalignment between SD maps and BEV feature spaces. Additionally, we leverage the SD map characteristics to design an auxiliary intersection-aware keypoint detection task, which further enhances the overall scene understanding performance. Experimental results on the large-scale OpenLane-V2 dataset demonstrate that by effectively integrating SD map priors, our framework significantly improves both scene perception and topology reasoning, outperforming existing methods by a substantial margin.", 'abstract_zh': '标准定义地图增强的场景感知和拓扑推理框架：SEPT', 'title_zh': 'SEPT: 标准清晰度地图增强的场景感知与拓扑推理方法及其在自动驾驶中的应用'}
{'arxiv_id': 'arXiv:2506.21458', 'title': 'Spatial Mental Modeling from Limited Views', 'authors': 'Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, Saining Xie, Manling Li, Jiajun Wu, Li Fei-Fei', 'link': 'https://arxiv.org/abs/2506.21458', 'abstract': 'Can Vision Language Models (VLMs) imagine the full scene from just a few views, like humans do? Humans form spatial mental models, internal representations of unseen space, to reason about layout, perspective, and motion. Our new MindCube benchmark with 21,154 questions across 3,268 images exposes this critical gap, where existing VLMs exhibit near-random performance. Using MindCube, we systematically evaluate how well VLMs build robust spatial mental models through representing positions (cognitive mapping), orientations (perspective-taking), and dynamics (mental simulation for "what-if" movements). We then explore three approaches to help VLMs approximate spatial mental models, including unseen intermediate views, natural language reasoning chains, and cognitive maps. The significant improvement comes from a synergistic approach, "map-then-reason", that jointly trains the model to first generate a cognitive map and then reason upon it. By training models to reason over these internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding reinforcement learning pushed performance even further to 70.7% (+32.9%). Our key insight is that such scaffolding of spatial mental models, actively constructing and utilizing internal structured spatial representations with flexible reasoning processes, significantly improves understanding of unobservable space.', 'abstract_zh': '视觉语言模型能否像人类一样仅从少数视角想象完整场景？MindCube：评估视觉语言模型构建空间mental模型能力的新基准', 'title_zh': '基于有限视角的空间心理建模'}
{'arxiv_id': 'arXiv:2506.21546', 'title': 'HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation', 'authors': 'Xinzhuo Li, Adheesh Juvekar, Xingyou Liu, Muntasir Wahed, Kiet A. Nguyen, Ismini Lourentzou', 'link': 'https://arxiv.org/abs/2506.21546', 'abstract': 'Recent progress in vision-language segmentation has significantly advanced grounded visual understanding. However, these models often exhibit hallucinations by producing segmentation masks for objects not grounded in the image content or by incorrectly labeling irrelevant regions. Existing evaluation protocols for segmentation hallucination primarily focus on label or textual hallucinations without manipulating the visual context, limiting their capacity to diagnose critical failures. In response, we introduce HalluSegBench, the first benchmark specifically designed to evaluate hallucinations in visual grounding through the lens of counterfactual visual reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual instance pairs spanning 281 unique object classes, and a set of newly introduced metrics that quantify hallucination sensitivity under visually coherent scene edits. Experiments on HalluSegBench with state-of-the-art vision-language segmentation models reveal that vision-driven hallucinations are significantly more prevalent than label-driven ones, with models often persisting in false segmentation, highlighting the need for counterfactual reasoning to diagnose grounding fidelity.', 'abstract_zh': 'Recent Progress in Vision-Language Segmentation and Its Challenges in Grounded Visual Understanding: Introducing HalluSegBench for Evaluating Hallucinations through Counterfactual Visual Reasoning', 'title_zh': 'HalluSegBench: 反事实视觉推理在分割幻觉评估中的应用'}
{'arxiv_id': 'arXiv:2506.21364', 'title': 'CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection', 'authors': 'Zhixin Cheng, Jiacheng Deng, Xinjun Li, Xiaotian Yin, Bohao Liao, Baoqun Yin, Wenfei Yang, Tianzhu Zhang', 'link': 'https://arxiv.org/abs/2506.21364', 'abstract': 'Detection-free methods typically follow a coarse-to-fine pipeline, extracting image and point cloud features for patch-level matching and refining dense pixel-to-point correspondences. However, differences in feature channel attention between images and point clouds may lead to degraded matching results, ultimately impairing registration accuracy. Furthermore, similar structures in the scene could lead to redundant correspondences in cross-modal matching. To address these issues, we propose Channel Adaptive Adjustment Module (CAA) and Global Optimal Selection Module (GOS). CAA enhances intra-modal features and suppresses cross-modal sensitivity, while GOS replaces local selection with global optimization. Experiments on RGB-D Scenes V2 and 7-Scenes demonstrate the superiority of our method, achieving state-of-the-art performance in image-to-point cloud registration.', 'abstract_zh': 'Detection-free方法通常采用粗到细的流程，提取图像和点云特征以进行像素块级别的匹配并细化密集的像素到点对应关系。然而，图像和点云特征通道注意力的差异可能导致匹配结果退化，最终影响注册精度。此外，场景中的相似结构可能会导致跨模态匹配中的冗余对应关系。为了解决这些问题，我们提出了通道自适应调整模块(CAA)和全局最优选择模块(GOS)。CAA增强内模特征并抑制跨模态敏感性，而GOS用全局优化替代局部选择。在RGB-D Scenes V2和7-Scenes数据集上的实验表明，我们的方法在图像到点云注册方面具有优越性，并达到了最先进的性能。', 'title_zh': 'CA-I2P：具有全局最优选择的通道自适应注册网络'}
{'arxiv_id': 'arXiv:2506.21269', 'title': 'Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management: A Study on Speed Classification in Suzhou', 'authors': 'Pengfei Fan, Yuli Zhang, Xinheng Wang, Ruiyuan Jiang, Hankang Gu, Dongyao Jia, Shangbo Wang', 'link': 'https://arxiv.org/abs/2506.21269', 'abstract': 'This study presents and publicly releases the Suzhou Urban Road Acoustic Dataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive data-acquisition protocols and annotation guidelines to ensure transparency and reproducibility of the experimental workflow. To model the coupling between vehicular noise and driving speed, we propose a bimodal-feature-fusion deep convolutional neural network (BMCNN). During preprocessing, an adaptive denoising and normalization strategy is applied to suppress environmental background interference; in the network architecture, parallel branches extract Mel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features, which are subsequently fused via a cross-modal attention mechanism in the intermediate feature space to fully exploit time-frequency information. Experimental results demonstrate that BMCNN achieves a classification accuracy of 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic dataset. Ablation studies and robustness tests on the Suzhou dataset further validate the contributions of each module to performance improvement and overfitting mitigation. The proposed acoustics-based speed classification method can be integrated into smart-city traffic management systems for real-time noise monitoring and speed estimation, thereby optimizing traffic flow control, reducing roadside noise pollution, and supporting sustainable urban planning.', 'abstract_zh': '苏州城市道路声学数据集及基于双模态特征融合的深度卷积神经网络方法（SZUR-声学数据集与BMCNN方法）', 'title_zh': '基于苏州的车辆声学数据集成用于增强城市交通管理：关于速度分类的研究'}
{'arxiv_id': 'arXiv:2506.21209', 'title': 'BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models', 'authors': 'Louis Kerner, Michel Meintz, Bihe Zhao, Franziska Boenisch, Adam Dziedzic', 'link': 'https://arxiv.org/abs/2506.21209', 'abstract': "State-of-the-art text-to-image models like Infinity generate photorealistic images at an unprecedented speed. These models operate in a bitwise autoregressive manner over a discrete set of tokens that is practically infinite in size. However, their impressive generative power comes with a growing risk: as their outputs increasingly populate the Internet, they are likely to be scraped and reused as training data-potentially by the very same models. This phenomenon has been shown to lead to model collapse, where repeated training on generated content, especially from the models' own previous versions, causes a gradual degradation in performance. A promising mitigation strategy is watermarking, which embeds human-imperceptible yet detectable signals into generated images-enabling the identification of generated content. In this work, we introduce BitMark, a robust bitwise watermarking framework for Infinity. Our method embeds a watermark directly at the bit level of the token stream across multiple scales (also referred to as resolutions) during Infinity's image generation process. Our bitwise watermark subtly influences the bits to preserve visual fidelity and generation speed while remaining robust against a spectrum of removal techniques. Furthermore, it exhibits high radioactivity, i.e., when watermarked generated images are used to train another image generative model, this second model's outputs will also carry the watermark. The radioactive traces remain detectable even when only fine-tuning diffusion or image autoregressive models on images watermarked with our BitMark. Overall, our approach provides a principled step toward preventing model collapse in image generative models by enabling reliable detection of generated outputs.", 'abstract_zh': '最先进的文本到图像模型如Infinity以 unprecedented的速度生成逼真图像。这些模型以位级自回归的方式作用于一个 Practically无限大的离散标记集上。然而，它们强大的生成能力伴随着一个日益增长的风险：随着它们的输出越来越充斥互联网，这些输出很可能会被抓取并重新用作训练数据——甚至可能是由这些模型本身之前版本生成的内容。这种现象已被证明会导致模型崩溃，反复训练生成的内容会导致性能逐渐退化。一种有前景的缓解策略是水印技术，该技术在生成图像中嵌入人类无法察觉但可检测的信号，以识别生成的内容。在这项工作中，我们引入了BitMark，一种适用于Infinity的 robust位级水印框架。我们的方法在Infinity图像生成过程中，在标记流的不同尺度（也称为分辨率）直接嵌入水印。我们的位级水印微妙地影响位以保持视觉保真度和生成速度，同时对各种去除技术保持稳健。此外，它还表现出很高的放射性，即使用水印生成的图像来训练另一张图像生成模型时，这一第二个模型的输出也将携带水印。即使仅对带有BitMark水印的图像进行细调扩散或图像自回归模型，放射性痕迹仍然可被检测到。总体而言，我们的方法提供了一种有原则的步骤，以通过确保生成输出的可靠检测来防止图像生成模型中模型崩溃的发生。', 'title_zh': 'BitMark for Infinity: 水印化位级自回归图像生成模型'}
{'arxiv_id': 'arXiv:2506.21167', 'title': 'A Hierarchical Deep Learning Approach for Minority Instrument Detection', 'authors': "Dylan Sechet, Francesca Bugiotti, Matthieu Kowalski, Edouard d'Hérouville, Filip Langiewicz", 'link': 'https://arxiv.org/abs/2506.21167', 'abstract': 'Identifying instrument activities within audio excerpts is vital in music information retrieval, with significant implications for music cataloging and discovery. Prior deep learning endeavors in musical instrument recognition have predominantly emphasized instrument classes with ample data availability. Recent studies have demonstrated the applicability of hierarchical classification in detecting instrument activities in orchestral music, even with limited fine-grained annotations at the instrument level. Based on the Hornbostel-Sachs classification, such a hierarchical classification system is evaluated using the MedleyDB dataset, renowned for its diversity and richness concerning various instruments and music genres. This work presents various strategies to integrate hierarchical structures into models and tests a new class of models for hierarchical music prediction. This study showcases more reliable coarse-level instrument detection by bridging the gap between detailed instrument identification and group-level recognition, paving the way for further advancements in this domain.', 'abstract_zh': '识别音频段落中的乐器活动对于音乐信息检索至关重要，对音乐编目与发现具有重大意义。既有深度学习研究主要集中在数据丰富的乐器类别上。近期研究表明，层次分类方法在管弦乐中检测乐器活动具有可行性，即使细粒度的乐器标注有限。基于Hornbostel-Sachs分类体系，本研究使用MedleyDB数据集评估层次分类系统，该数据集涵盖多种乐器和音乐流派，具有多样性和丰富性。本研究提出了多种将层次结构整合到模型中的策略，并测试了一类新的层次音乐预测模型。此项研究通过弥合详细乐器识别与群体级识别之间的差距，展示了更可靠的粗粒度乐器检测方法，为该领域的进一步发展奠定了基础。', 'title_zh': '基于分层深度学习的少数乐器检测方法'}
{'arxiv_id': 'arXiv:2506.21162', 'title': 'A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver Tumour Ablation', 'authors': 'Shuwei Xing, Derek W. Cool, David Tessier, Elvis C.S. Chen, Terry M. Peters, Aaron Fenster', 'link': 'https://arxiv.org/abs/2506.21162', 'abstract': '3D ultrasound (US) imaging has shown significant benefits in enhancing the outcomes of percutaneous liver tumour ablation. Its clinical integration is crucial for transitioning 3D US into the therapeutic domain. However, challenges of tumour identification in US images continue to hinder its broader adoption. In this work, we propose a novel framework for integrating 3D US into the standard ablation workflow. We present a key component, a clinically viable 2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to reduce registration complexity. To facilitate efficient verification of the registration workflow, we also propose an intuitive multimodal image visualization technique. In our study, 2D US-CT/MRI registration achieved a landmark distance error of approximately 2-4 mm with a runtime of 0.22s per image pair. Additionally, non-rigid registration reduced the mean alignment error by approximately 40% compared to rigid registration. Results demonstrated the efficacy of the proposed 2D US-CT/MRI registration workflow. Our integration framework advanced the capabilities of 3D US imaging in improving percutaneous tumour ablation, demonstrating the potential to expand the therapeutic role of 3D US in clinical interventions.', 'abstract_zh': '3D超声成像在增强经皮肝肿瘤消融效果方面的显著优势及其临床整合对于将3D超声引入治疗领域至关重要。然而，肿瘤在超声图像中的识别挑战继续阻碍其更广泛的采用。在此工作中，我们提出了一种新的框架，用于将3D超声整合到标准消融工作流程中。我们提出了一种临床可行的2D超声-CT/MRI配准方法，利用3D超声作为中介以降低配准复杂性。为了方便高效验证配准工作流程，我们还提出了一种直观的多模态图像可视化技术。在我们的研究中，2D超声-CT/MRI配准的地标距离误差约为2-4毫米，运行时间为每张图像对0.22秒。此外，非刚性配准相比刚性配准将平均对齐误差降低了约40%。结果表明，所提出的2D超声-CT/MRI配准工作流程的有效性。我们的集成框架提升了3D超声成像在改善经皮肿瘤消融方面的功能，展示了3D超声在临床干预中扩展治疗作用的潜力。', 'title_zh': '一种将3D超声集成到经皮肝肿瘤消融的新框架'}
{'arxiv_id': 'arXiv:2506.21151', 'title': 'Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels', 'authors': 'Aida Moafi, Danial Moafi, Evgeny M. Mirkes, Gerry P. McCann, Abbas S. Alatrany, Jayanth R. Arnold, Mostafa Mehdipour Ghazi', 'link': 'https://arxiv.org/abs/2506.21151', 'abstract': "The accurate segmentation of myocardial scars from cardiac MRI is essential for clinical assessment and treatment planning. In this study, we propose a robust deep-learning pipeline for fully automated myocardial scar detection and segmentation by fine-tuning state-of-the-art models. The method explicitly addresses challenges of label noise from semi-automatic annotations, data heterogeneity, and class imbalance through the use of Kullback-Leibler loss and extensive data augmentation. We evaluate the model's performance on both acute and chronic cases and demonstrate its ability to produce accurate and smooth segmentations despite noisy labels. In particular, our approach outperforms state-of-the-art models like nnU-Net and shows strong generalizability in an out-of-distribution test set, highlighting its robustness across various imaging conditions and clinical tasks. These results establish a reliable foundation for automated myocardial scar quantification and support the broader clinical adoption of deep learning in cardiac imaging.", 'abstract_zh': '心脏MRI中心肌疤痕的准确分割对于临床评估和治疗计划至关重要。本研究提出了一种稳健的深度学习管道，用于完全自动的心肌疤痕检测和分割，通过微调先进模型来解决标签噪声、数据异质性和类别不平衡等挑战。方法通过使用Kullback-Leibler损失和大量数据增强，明确解决了上述挑战。我们在急性和慢性病例上评估了模型的性能并展示了其能够在噪声标签下生成准确且平滑的分割的能力。特别地，我们的方法在nnU-Net等先进模型中表现出色，并在分布外测试集中显示出强大的泛化能力，突显了其在各种成像条件和临床任务中的 robust 性。这些结果为自动心肌疤痕量化建立了可靠基础，并支持深度学习在心脏成像中的更广泛临床应用。', 'title_zh': '基于嘈杂标签的心脏MRI心肌疤痕分割的稳健深度学习'}
{'arxiv_id': 'arXiv:2506.21045', 'title': 'Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling', 'authors': 'Hansam Cho, Seoung Bum Kim', 'link': 'https://arxiv.org/abs/2506.21045', 'abstract': 'Text-guided diffusion models have become essential for high-quality image synthesis, enabling dynamic image editing. In image editing, two crucial aspects are editability, which determines the extent of modification, and faithfulness, which reflects how well unaltered elements are preserved. However, achieving optimal results is challenging because of the inherent trade-off between editability and faithfulness. To address this, we propose Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with minimal impact on editability. FGS incorporates faithfulness guidance to strengthen the preservation of input image information and introduces a scheduling strategy to resolve misalignment between editability and faithfulness. Experimental results demonstrate that FGS achieves superior faithfulness while maintaining editability. Moreover, its compatibility with various editing methods enables precise, high-quality image edits across diverse tasks.', 'abstract_zh': '文本指导的保真度引导和调度（FGS）：在高保真图像合成中的动态图像编辑', 'title_zh': '基于指导和调度改进扩散模型驱动的图像编辑保真度'}
{'arxiv_id': 'arXiv:2506.20988', 'title': 'Segment Anything in Pathology Images with Natural Language', 'authors': 'Zhixuan Chen, Junlin Hou, Liqi Lin, Yihui Wang, Yequan Bie, Xi Wang, Yanning Zhou, Ronald Cheong Kin Chan, Hao Chen', 'link': 'https://arxiv.org/abs/2506.20988', 'abstract': "Pathology image segmentation is crucial in computational pathology for analyzing histological features relevant to cancer diagnosis and prognosis. However, current methods face major challenges in clinical applications due to limited annotated data and restricted category definitions. To address these limitations, we propose PathSegmentor, the first text-prompted segmentation foundation model designed specifically for pathology images. We also introduce PathSeg , the largest and most comprehensive dataset for pathology segmentation, built from 17 public sources and containing 275k image-mask-label triples across 160 diverse categories. With PathSegmentor, users can perform semantic segmentation using natural language prompts, eliminating the need for laborious spatial inputs such as points or boxes. Extensive experiments demonstrate that PathSegmentor outperforms specialized models with higher accuracy and broader applicability, while maintaining a compact architecture. It significantly surpasses existing spatial- and text-prompted models by 0.145 and 0.429 in overall Dice scores, respectively, showing strong robustness in segmenting complex structures and generalizing to external datasets. Moreover, PathSegmentor's outputs enhance the interpretability of diagnostic models through feature importance estimation and imaging biomarker discovery, offering pathologists evidence-based support for clinical decision-making. This work advances the development of explainable AI in precision oncology.", 'abstract_zh': '病理图像分割是计算病理学中分析与癌症诊断和预后相关的组织学特征的关键。然而，现有方法在临床应用中面临重大挑战，主要原因在于标注数据有限和类别定义受限。为解决这些局限性，我们提出PathSegmentor，这是首款专门为病理图像设计的文字引导分割基础模型。我们还引入了PathSeg，这是最大的最全面的病理分割数据集，包含来自17个公开数据源的275,000个图像-掩模-标签三元组，涵盖了160个不同类别。借助PathSegmentor，用户可以通过自然语言提示进行语义分割，从而消除了需要繁琐的空间输入（如点或框）的需要。广泛实验表明，PathSegmentor在准确性和适用性方面优于专门模型，同时保持紧凑的架构。它在总体交集数量Dice分数上分别比现有的空间提示和文字提示模型高出0.145和0.429，显示出在分割复杂结构和泛化到外部数据集上的强大鲁棒性。PathSegmentor的输出通过特征重要性估计和成像生物标志物发现增强了解释性诊断模型的可解释性，为病理学家提供了基于证据的临床决策支持。这项工作促进了精准肿瘤学中可解释AI的发展。', 'title_zh': '使用自然语言在病理图像中分割 Anything'}
{'arxiv_id': 'arXiv:2506.20977', 'title': 'From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging', 'authors': 'Tao Liu, Dafeng Zhang, Gengchen Li, Shizhuo Liu, Yongqi Song, Senmao Li, Shiqi Yang, Boqian Li, Kai Wang, Yaxing Wang', 'link': 'https://arxiv.org/abs/2506.20977', 'abstract': 'Face aging has become a crucial task in computer vision, with applications ranging from entertainment to healthcare. However, existing methods struggle with achieving a realistic and seamless transformation across the entire lifespan, especially when handling large age gaps or extreme head poses. The core challenge lies in balancing age accuracy and identity preservation--what we refer to as the Age-ID trade-off. Most prior methods either prioritize age transformation at the expense of identity consistency or vice versa. In this work, we address this issue by proposing a two-pass face aging framework, named Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first pass focuses on solving age accuracy by introducing an adaptive noise injection (AdaNI) mechanism. This mechanism is guided by including prompt descriptions of age and gender for the given person as the textual condition. Also, by adjusting the noise level, we can control the strength of aging while allowing more flexibility in transforming the face. However, identity preservation is weakly ensured here to facilitate stronger age transformations. In the second pass, we enhance identity preservation while maintaining age-specific features by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace and Rotate-CLIP. This pass allows for denoising the transformed image from the first pass, ensuring stronger identity preservation without compromising the aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL protocols, show that our Cradle2Cane outperforms existing face aging methods in age accuracy and identity consistency.', 'abstract_zh': '基于少步文本到图像扩散模型的Cradle2Cane两阶段人脸老化框架', 'title_zh': '从婴儿到拐杖：一种高保真生命周期面部老化两步框架'}
{'arxiv_id': 'arXiv:2506.20967', 'title': 'DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing', 'authors': 'Lingling Cai, Kang Zhao, Hangjie Yuan, Xiang Wang, Yingya Zhang, Kejie Huang', 'link': 'https://arxiv.org/abs/2506.20967', 'abstract': 'The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in video generation. However, directly applying existing video editing methods to Video DiTs often incurs substantial computational overhead, due to resource-intensive attention modification or finetuning. To alleviate this problem, we present DFVEdit, an efficient zero-shot video editing method tailored for Video DiTs. DFVEdit eliminates the need for both attention modification and fine-tuning by directly operating on clean latents via flow transformation. To be more specific, we observe that editing and sampling can be unified under the continuous flow perspective. Building upon this foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a theoretically unbiased estimation of DFV -- and integrate Implicit Cross Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further enhance editing quality. DFVEdit excels in practical efficiency, offering at least 20x inference speed-up and 85\\% memory reduction on Video DiTs compared to attention-engineering-based editing methods. Extensive quantitative and qualitative experiments demonstrate that DFVEdit can be seamlessly applied to popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art performance on structural fidelity, spatial-temporal consistency, and editing quality.', 'abstract_zh': '基于Video DiTs的高效零样本视频编辑方法', 'title_zh': 'DFVEdit: 条件增量流向量在零样本视频编辑中的应用'}
{'arxiv_id': 'arXiv:2506.20946', 'title': 'Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models', 'authors': 'Donggoo Kang, Jangyeong Kim, Dasol Jeong, Junyoung Choi, Jeonga Wi, Hyunmin Lee, Joonho Gwon, Joonki Paik', 'link': 'https://arxiv.org/abs/2506.20946', 'abstract': 'Current texture synthesis methods, which generate textures from fixed viewpoints, suffer from inconsistencies due to the lack of global context and geometric understanding. Meanwhile, recent advancements in video generation models have demonstrated remarkable success in achieving temporally consistent videos. In this paper, we introduce VideoTex, a novel framework for seamless texture synthesis that leverages video generation models to address both spatial and temporal inconsistencies in 3D textures. Our approach incorporates geometry-aware conditions, enabling precise utilization of 3D mesh structures. Additionally, we propose a structure-wise UV diffusion strategy, which enhances the generation of occluded areas by preserving semantic information, resulting in smoother and more coherent textures. VideoTex not only achieves smoother transitions across UV boundaries but also ensures high-quality, temporally stable textures across video frames. Extensive experiments demonstrate that VideoTex outperforms existing methods in texture fidelity, seam blending, and stability, paving the way for dynamic real-time applications that demand both visual quality and temporal coherence.', 'abstract_zh': '基于视频生成的无缝纹理合成框架', 'title_zh': '基于几何感知扩散和时序视频模型的一致零样本3D纹理合成'}
{'arxiv_id': 'arXiv:2506.20877', 'title': 'THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion', 'authors': 'Calin Teodor Ioan', 'link': 'https://arxiv.org/abs/2506.20877', 'abstract': 'Monocular depth estimation methods traditionally train deep models to infer depth directly from RGB pixels. This implicit learning often overlooks explicit monocular cues that the human visual system relies on, such as occlusion boundaries, shading, and perspective. Rather than expecting a network to discover these cues unaided, we present ThirdEye, a cue-aware pipeline that deliberately supplies each cue through specialised, pre-trained, and frozen networks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3) equipped with a key-value working-memory module that weights them by reliability. An adaptive-bins transformer head then produces a high-resolution disparity map. Because the cue experts are frozen, ThirdEye inherits large amounts of external supervision while requiring only modest fine-tuning. This extended version provides additional architectural detail, neuroscientific motivation, and an expanded experimental protocol; quantitative results will appear in a future revision.', 'abstract_zh': '单目深度估计方法通常训练深度模型直接从RGB像素中推断深度。这种隐式的学习往往忽略了人类视觉系统依赖的显式单目线索，如遮挡边界、阴影和透视。我们不期望网络无辅助地发现这些线索，而是提出了ThirdEye，一种线索感知管道，通过专门的、预训练的和冻结的网络故意提供每种线索。这些线索在配备有关键值工作记忆模块的三层皮层结构（V1->V2->V3）中融合，该模块根据可靠性加权这些线索。然后，自适应区间变压器头部生成高分辨率的视差图。由于线索专家是冻结的，ThirdEye可以继承大量的外部监督，同时只需要适度的微调。本文提供了架构细节、神经科学动机以及扩展的实验协议；定量结果将在未来的修订中出现。', 'title_zh': 'THIRDEYE：基于脑启发多阶段融合的线索aware单目深度估计'}
{'arxiv_id': 'arXiv:2506.20832', 'title': 'Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models', 'authors': 'Cansu Korkmaz, Ahmet Murat Tekalp, Zafer Dogan', 'link': 'https://arxiv.org/abs/2506.20832', 'abstract': 'Super-resolution (SR) is an ill-posed inverse problem with many feasible solutions consistent with a given low-resolution image. On one hand, regressive SR models aim to balance fidelity and perceptual quality to yield a single solution, but this trade-off often introduces artifacts that create ambiguity in information-critical applications such as recognizing digits or letters. On the other hand, diffusion models generate a diverse set of SR images, but selecting the most trustworthy solution from this set remains a challenge. This paper introduces a robust, automated framework for identifying the most trustworthy SR sample from a diffusion-generated set by leveraging the semantic reasoning capabilities of vision-language models (VLMs). Specifically, VLMs such as BLIP-2, GPT-4o, and their variants are prompted with structured queries to assess semantic correctness, visual quality, and artifact presence. The top-ranked SR candidates are then ensembled to yield a single trustworthy output in a cost-effective manner. To rigorously assess the validity of VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid metric that quantifies SR reliability based on three complementary components: semantic similarity via CLIP embeddings, structural integrity using SSIM on edge maps, and artifact sensitivity through multi-level wavelet decomposition. We empirically show that TWS correlates strongly with human preference in both ambiguous and natural images, and that VLM-guided selections consistently yield high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail to reflect information fidelity, our approach offers a principled, scalable, and generalizable solution for navigating the uncertainty of the diffusion SR space. By aligning outputs with human expectations and semantic correctness, this work sets a new benchmark for trustworthiness in generative SR.', 'abstract_zh': '一种基于视觉语言模型的高保真超分辨样本选择框架', 'title_zh': '利用视觉语言模型选择由扩散模型生成的可信赖超分辨率样本'}
{'arxiv_id': 'arXiv:2506.20689', 'title': 'U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs', 'authors': 'Racheal Mukisa, Arvind K. Bansal', 'link': 'https://arxiv.org/abs/2506.20689', 'abstract': 'Artificial intelligence, including deep learning models, will play a transformative role in automated medical image analysis for the diagnosis of cardiac disorders and their management. Automated accurate delineation of cardiac images is the first necessary initial step for the quantification and automated diagnosis of cardiac disorders. In this paper, we propose a deep learning based enhanced UNet model, U-R-Veda, which integrates convolution transformations, vision transformer, residual links, channel-attention, and spatial attention, together with edge-detection based skip-connections for an accurate fully-automated semantic segmentation of cardiac magnetic resonance (CMR) images. The model extracts local-features and their interrelationships using a stack of combination convolution blocks, with embedded channel and spatial attention in the convolution block, and vision transformers. Deep embedding of channel and spatial attention in the convolution block identifies important features and their spatial localization. The combined edge information with channel and spatial attention as skip connection reduces information-loss during convolution transformations. The overall model significantly improves the semantic segmentation of CMR images necessary for improved medical image analysis. An algorithm for the dual attention module (channel and spatial attention) has been presented. Performance results show that U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The model outperforms the accuracy attained by other models, based on DSC and HD metrics, especially for the delineation of right-ventricle and left-ventricle-myocardium.', 'abstract_zh': '人工智能，包括深度学习模型，将在心血管疾病诊断及其管理的自动化医学图像分析中发挥变革性作用。准确的自动心脏图像分割是心脏疾病量化和自动化诊断的第一步必要步骤。本文提出了一种基于深度学习的增强UNet模型U-R-Veda，该模型结合了卷积变换、视觉变换器、残差连接、通道注意和空间注意，并通过基于边缘检测的跳跃连接实现心脏磁共振（CMR）图像的准确全自动语义分割。模型利用堆叠组合卷积块提取局部特征及其相互关系，并在卷积块中嵌入通道和空间注意，以及视觉变换器，以识别重要特征及其空间定位。结合边缘信息与通道和空间注意作为跳跃连接，减少了卷积变换中的信息损失。整体模型显著提高了必要的心脏磁共振图像语义分割性能，从而改进医学图像分析。提出了双注意模块（通道注意和空间注意）的算法。性能结果显示，U-R-Veda基于DSC指标的平均准确率为95.2%。该模型在基于DSC和HD指标的精度上优于其他模型，尤其是在右心室和左心室心肌的分割方面。', 'title_zh': 'U-R-VEDA：结合UNET、残差链接、边缘和双关注机制以及视觉变压器的心脏磁共振图像精确语义分割'}
{'arxiv_id': 'arXiv:2504.15217', 'title': 'DRAGON: Distributional Rewards Optimize Diffusion Generative Models', 'authors': 'Yatong Bai, Jonah Casebeer, Somayeh Sojoudi, Nicholas J. Bryan', 'link': 'https://arxiv.org/abs/2504.15217', 'abstract': 'We present Distributional RewArds for Generative OptimizatioN (DRAGON), a versatile framework for fine-tuning media generation models towards a desired outcome. Compared with traditional reinforcement learning with human feedback (RLHF) or pairwise preference approaches such as direct preference optimization (DPO), DRAGON is more flexible. It can optimize reward functions that evaluate either individual examples or distributions of them, making it compatible with a broad spectrum of instance-wise, instance-to-distribution, and distribution-to-distribution rewards. Leveraging this versatility, we construct novel reward functions by selecting an encoder and a set of reference examples to create an exemplar distribution. When cross-modality encoders such as CLAP are used, the reference examples may be of a different modality (e.g., text versus audio). Then, DRAGON gathers online and on-policy generations, scores them to construct a positive demonstration set and a negative set, and leverages the contrast between the two sets to maximize the reward. For evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20 different reward functions, including a custom music aesthetics model, CLAP score, Vendi diversity, and Frechet audio distance (FAD). We further compare instance-wise (per-song) and full-dataset FAD settings while ablating multiple FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an 81.45% average win rate. Moreover, reward functions based on exemplar sets indeed enhance generations and are comparable to model-based rewards. With an appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality win rate without training on human preference annotations. As such, DRAGON exhibits a new approach to designing and optimizing reward functions for improving human-perceived quality. Sound examples at this https URL.', 'abstract_zh': '我们都呈现了适用于生成优化的分布奖励框架（DRAGON），这是一种多功能框架，旨在根据期望结果微调媒体生成模型。与传统的基于人类反馈的强化学习（RLHF）或成对偏好方法（如直接偏好优化DPO）相比，DRAGON更具灵活性。DRAGON可以优化评估单个示例或它们的分布的奖励函数，使其兼容广泛实例、实例到分布以及分布到分布的奖励。利用这种灵活性，我们通过选择编码器和一组参考示例来构造一个示例分布，进而构建新的奖励函数。当使用跨模态编码器（如CLAP）时，参考示例可能属于不同的模态（例如，文本与音频）。然后，DRAGON收集在线和政策生成，对它们进行评分以构建积极示范集和负面集，并利用两者的对比来最大化奖励。在评估方面，我们使用20种不同的奖励函数对音频领域的文本到音乐扩散模型进行了微调，包括一个自定义的音乐美学模型、CLAP评分、Vendi多样性以及弗雷切音频距离（FAD）。我们进一步比较了单曲级和整个数据集级别的FAD设置，并消融了多种FAD编码器和参考集。在所有20个目标奖励中，DRAGON实现了81.45%的平均胜率。此外，基于示例集的奖励函数确实可以提升生成性能，并且与基于模型的奖励相当。通过适当的示例集，DRAGON在无需训练人类偏好注释的情况下实现了60.95%的人类投票音乐质量胜率。因此，DRAGON展示了设计和优化提高人类感知质量的奖励函数的新方法。示音频文件请参见此链接：https://。', 'title_zh': 'DRAGON: 分布性奖励优化扩散生成模型'}
