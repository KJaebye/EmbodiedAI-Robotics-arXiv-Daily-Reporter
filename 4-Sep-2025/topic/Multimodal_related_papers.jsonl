{'arxiv_id': 'arXiv:2509.03477', 'title': 'Robult: Leveraging Redundancy and Modality Specific Features for Robust Multimodal Learning', 'authors': 'Duy A. Nguyen, Abhi Kamboj, Minh N. Do', 'link': 'https://arxiv.org/abs/2509.03477', 'abstract': 'Addressing missing modalities and limited labeled data is crucial for advancing robust multimodal learning. We propose Robult, a scalable framework designed to mitigate these challenges by preserving modality-specific information and leveraging redundancy through a novel information-theoretic approach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled (PU) contrastive loss that maximizes task-relevant feature alignment while effectively utilizing limited labeled data in semi-supervised settings, and (2) a latent reconstruction loss that ensures unique modality-specific information is retained. These strategies, embedded within a modular design, enhance performance across various downstream tasks and ensure resilience to incomplete modalities during inference. Experimental results across diverse datasets validate that Robult achieves superior performance over existing approaches in both semi-supervised learning and missing modality contexts. Furthermore, its lightweight design promotes scalability and seamless integration with existing architectures, making it suitable for real-world multimodal applications.', 'abstract_zh': '解决缺失模态和有限标记数据对于推进稳健的多模态学习至关重要。我们提出Robult，这是一种可扩展的框架，通过保留模态特定信息并利用新颖的信息论方法中的冗余性来缓解这些挑战。Robult 优化了两个核心目标：（1）一种软正负未标记（PU）对比损失，该损失最大化任务相关特征对齐，同时在半监督设置中有效利用有限标记数据；（2）潜在重构损失，确保保留独特的模态特定信息。这些策略嵌入在模块化设计中，在各种下游任务中提高性能，并确保在推断过程中对不完整模态的鲁棒性。跨多个数据集的实验结果验证了Robult在半监督学习和缺失模态情景中均实现了优于现有方法的性能。此外，其轻量级设计促进了可扩展性，并与现有架构无缝集成，使其适用于实际的多模态应用。', 'title_zh': 'Robult: 利用冗余性和模态特定特征进行鲁棒多模态学习'}
{'arxiv_id': 'arXiv:2509.03025', 'title': 'Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens', 'authors': 'Sohee Kim, Soohyun Ryu, Joonhyung Park, Eunho Yang', 'link': 'https://arxiv.org/abs/2509.03025', 'abstract': "Large Vision-Language Models (LVLMs) generate contextually relevant responses by jointly interpreting visual and textual inputs. However, our finding reveals they often mistakenly perceive text inputs lacking visual evidence as being part of the image, leading to erroneous responses. In light of this finding, we probe whether LVLMs possess an internal capability to determine if textual concepts are grounded in the image, and discover a specific subset of Feed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons, that consistently signal the visual absence through a distinctive activation pattern. Leveraging these patterns, we develop a detection module that systematically classifies whether an input token is visually grounded. Guided by its prediction, we propose a method to refine the outputs by reinterpreting question prompts or replacing the detected absent tokens during generation. Extensive experiments show that our method effectively mitigates the models' tendency to falsely presume the visual presence of text input and its generality across various LVLMs.", 'abstract_zh': '大型视觉-语言模型通过联合解释视觉和文本输入生成上下文相关响应。然而，我们的发现表明，它们常常错误地将缺乏视觉证据的文本输入视为图像的一部分，导致错误的响应。鉴于这一发现，我们探究了大型视觉-语言模型是否具有内部能力来判断文本概念是否基于图像，并发现了一类特定的前向神经网络（FFN）神经元，称为视觉缺失感知（VA）神经元，这些神经元通过特有的激活模式一致地表明视觉缺失。利用这些模式，我们开发了一个检测模块，系统地分类输入标记是否基于视觉。根据其预测，我们提出了一种通过重新解释问题提示或在生成过程中替换检测到的缺失标记来改进模型输出的方法。广泛实验表明，我们的方法有效地减轻了模型假定文本输入视觉存在的倾向，并且适用于各种大型视觉-语言模型。', 'title_zh': '揭示大型视觉-语言模型对视觉缺失词的响应'}
