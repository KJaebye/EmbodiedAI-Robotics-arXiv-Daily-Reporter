# VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages 

**Title (ZH)**: VLURes: 低资源语言视觉和语言理解基准研究 

**Authors**: Jesse Atuhurra, Iqra Ali, Tomoya Iwakura, Hidetaka Kamigaito, Tatsuya Hiraoka  

**Link**: [PDF](https://arxiv.org/pdf/2510.12845)  

**Abstract**: Vision Language Models (VLMs) are pivotal for advancing perception in intelligent agents. Yet, evaluation of VLMs remains limited to predominantly English-centric benchmarks in which the image-text pairs comprise short texts. To evaluate VLM fine-grained abilities, in four languages under long-text settings, we introduce a novel multilingual benchmark VLURes featuring eight vision-and-language tasks, and a pioneering unrelatedness task, to probe the fine-grained Visual and Linguistic Understanding capabilities of VLMs across English, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets, curated from web resources in the target language, encompass ten diverse image categories and rich textual context, introducing valuable vision-language resources for Swahili and Urdu. By prompting VLMs to generate responses and rationales, evaluated automatically and by native speakers, we uncover performance disparities across languages and tasks critical to intelligent agents, such as object recognition, scene understanding, and relationship understanding. We conducted evaluations of ten VLMs with VLURes. The best performing model, GPT-4o, achieves an overall accuracy of 90.8% and lags human performance by 6.7%, though the gap is larger for open-source models. The gap highlights VLURes' critical role in developing intelligent agents to tackle multi-modal visual reasoning. 

**Abstract (ZH)**: 多语言VLURes基准：跨英语、日语及低资源语言的精细视觉与语言理解评估 

---
# Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math 

**Title (ZH)**: Hard2Verify: 一种针对开放性前沿数学问题的步骤级验证基准 

**Authors**: Shrey Pandit, Austin Xu, Xuan-Phi Nguyen, Yifei Ming, Caiming Xiong, Shafiq Joty  

**Link**: [PDF](https://arxiv.org/pdf/2510.13744)  

**Abstract**: Large language model (LLM)-based reasoning systems have recently achieved gold medal-level performance in the IMO 2025 competition, writing mathematical proofs where, to receive full credit, each step must be not only correct but also sufficiently supported. To train LLM-based reasoners in such challenging, open-ended settings, strong verifiers capable of catching step-level mistakes are necessary prerequisites. We introduce Hard2Verify, a human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions. We evaluate 29 generative critics and process reward models, demonstrating that, beyond a few standouts, open-source verifiers lag closed source models. We subsequently analyze what drives poor performance in step-level verification, the impacts of scaling verifier compute, as well as fundamental questions such as self-verification and verification-generation dynamics. 

**Abstract (ZH)**: 基于大型语言模型（LLM）的推理系统在2025年国际数学奥林匹克竞赛中取得了满分性能，在编写数学证明时，每一步不仅必须正确，而且必须充分支持。为了在如此具有挑战性和开放性的环境中训练基于LLM的推理系统，需要具备在步骤层面发现错误的强大验证器作为先决条件。我们介绍了Hard2Verify，这是一个经过人类注释的步骤层面验证基准，耗费了超过500小时的人工劳动。Hard2Verify旨在严格评估处于前沿的步骤层面验证器：验证器必须提供步骤层面的注释或识别由前沿LLM生成的响应中的第一个错误。我们评估了29个生成批评者和过程奖励模型，结果显示，除了几个表现出色的模型外，开源验证器落后于封闭源模型。随后，我们分析了步骤层面验证性能不佳的原因，验证器计算量扩展的影响，以及自我验证和验证生成动力学等基本问题。 

---
# Training LLM Agents to Empower Humans 

**Title (ZH)**: 训练大规模语言模型代理以赋能人类 

**Authors**: Evan Ellis, Vivek Myers, Jens Tuyls, Sergey Levine, Anca Dragan, Benjamin Eysenbach  

**Link**: [PDF](https://arxiv.org/pdf/2510.13709)  

**Abstract**: Assistive agents should not only take actions on behalf of a human, but also step out of the way and cede control when there are important decisions to be made. However, current methods for building assistive agents, whether via mimicking expert humans or via RL finetuning on an inferred reward, often encourage agents to complete tasks on their own rather than truly assisting the human attain their objectives. Additionally, these methods often require costly explicit human feedback to provide a training signal. We propose a new approach to tuning assistive language models based on maximizing the human's empowerment, their ability to effect desired changes in the environment. Our empowerment-maximizing method, Empower, only requires offline text data, providing a self-supervised method for fine-tuning language models to better assist humans. To study the efficacy of our approach, we conducted an 18-person user study comparing our empowerment assistant with a strong baseline. Participants preferred our assistant 78% of the time (p=0.015), with a 31% higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a new environment for evaluating multi-turn code assistance using simulated humans. Using this environment, we show that agents trained with Empower increase the success rate of a simulated human programmer on challenging coding questions by an average of 192% over an SFT baseline. With this empowerment objective, we provide a framework for useful aligned AI agents at scale using only offline data without the need for any additional human feedback or verifiable rewards. 

**Abstract (ZH)**: 辅助代理不仅应代表人类执行操作，还应在需要重要决策时主动让位于人类并放弃控制权。然而，当前构建辅助代理的方法，无论是通过模仿专家人类还是通过基于推测奖励的RL微调，往往促使代理自行完成任务，而不是真正帮助人类达成目标。此外，这些方法常常需要昂贵的显式人类反馈来提供训练信号。我们提出了一种新的基于最大化人类赋能的辅助语言模型调整方法。我们的赋能最大化方法Empower仅需离线文本数据，提供了一种半监督的方法，用于微调语言模型以更好地辅助人类。为了研究我们方法的有效性，我们进行了一项包含18名参与者的用户研究，比较了我们的赋能助手与一个强有力的基线。参与者中有78%（p=0.015）更喜欢我们的助手，接受率高31%，建议数减少38%。此外，我们引入了一个新的多轮次代码辅助评估环境，利用模拟的人类。利用这个环境，我们展示了使用Empower训练的代理将模拟的人类程序员在解决棘手的编程问题时的成功率平均提高了192%，相比SFT基线。通过这一赋能目标，我们提供了一种仅使用离线数据、无需额外人类反馈或可验证奖励的框架，用于构建大规模有用且目标一致的AI代理。 

---
# Tandem Training for Language Models 

**Title (ZH)**: 语言模型的串联训练 

**Authors**: Robert West, Ashton Anderson, Ece Kamar, Eric Horvitz  

**Link**: [PDF](https://arxiv.org/pdf/2510.13551)  

**Abstract**: As language models continue to rapidly improve, we can expect their actions and reasoning to become difficult or impossible for weaker agents and humans to follow, undermining interpretability and oversight. With an eye on long-term futures, we pursue methods that encourage models to produce solutions that remain intelligible to weaker collaborators. We formalize intelligibility as handoff robustness: a strong model's solution is intelligible to a weaker model if randomly handing off control to the weaker model along the solution path does not cause failure. Building on this criterion, we introduce tandem training for language models, a reinforcement learning (RL) paradigm in which rollout tokens are intermittently and randomly sampled from a frozen weak model rather than the strong model being trained. Because rollouts succeed only when the strong model's actions and reasoning process can be continued by the weak model -- when the two can co-construct a successful solution -- optimizing standard RL objectives with tandem training implicitly incentivizes both correctness and intelligibility. In the GSM8K math reasoning task, tandem training reliably teaches models to abandon jargon and adapt their language to weaker partners while keeping task accuracy high. Our results demonstrate a promising route to building AI systems that remain auditable by weaker agents, with implications for human--AI collaboration and multi-agent communication. 

**Abstract (ZH)**: 语言模型持续快速进步，其行为和推理将变得难以甚至不可能被较弱代理和人类理解，削弱了可解释性和监督能力。着眼于长期未来，我们探索促进模型生成对较弱合作者依然具有可理解性的解决方案的方法。我们将可理解性形式化为交接稳健性：如果随机在解决方案路径中将控制权交接给较弱模型而不导致失败，则强模型的解决方案对较弱模型来说是可理解的。基于这一标准，我们引入了语言模型的串联训练方法，这是一种强化学习（RL）范式，在此范式中，插件样例会间歇性且随机地从冻结的弱模型中抽取，而不是从正在训练的强模型中抽取。由于只有当强模型的行为和推理过程可以由弱模型延续时插件样例才能成功，即当两者能够共同构建成功解决方案时，使用串联训练优化标准RL目标会隐式地激励正确性和可理解性。在GSM8K数学推理任务中，串联训练有效地教会模型放弃术语并调整语言以适应较弱的同伴，同时保持任务准确性。我们的结果表明，这为构建更具审计性的AI系统提供了一条有希望的路径，这对于人类-AI合作以及多智能体通信具有重要意义。 

---
# A Methodology for Assessing the Risk of Metric Failure in LLMs Within the Financial Domain 

**Title (ZH)**: 评估金融领域大模型指标失败风险的方法论 

**Authors**: William Flanagan, Mukunda Das, Rajitha Ramanyake, Swaunja Maslekar, Meghana Manipuri, Joong Ho Choi, Shruti Nair, Shambhavi Bhusan, Sanjana Dulam, Mouni Pendharkar, Nidhi Singh, Vashisth Doshi, Sachi Shah Paresh  

**Link**: [PDF](https://arxiv.org/pdf/2510.13524)  

**Abstract**: As Generative Artificial Intelligence is adopted across the financial services industry, a significant barrier to adoption and usage is measuring model performance. Historical machine learning metrics can oftentimes fail to generalize to GenAI workloads and are often supplemented using Subject Matter Expert (SME) Evaluation. Even in this combination, many projects fail to account for various unique risks present in choosing specific metrics. Additionally, many widespread benchmarks created by foundational research labs and educational institutions fail to generalize to industrial use. This paper explains these challenges and provides a Risk Assessment Framework to allow for better application of SME and machine learning Metrics 

**Abstract (ZH)**: 随着生成式人工智能在金融服务业的应用逐渐普及，采用和使用过程中一个显著的障碍是对模型性能的衡量。历史上的机器学习指标常常无法泛化到生成式人工智能的工作负载中，因此经常需要结合领域专家评估（SME Evaluation）来补充。即使在这种结合下，许多项目仍未考虑到选择特定指标时所面临的各种独特风险。此外，许多由基础研究实验室和教育机构创建的广泛基准在应用于工业环境时也无法泛化。本文阐释了这些挑战，并提供了一个风险评估框架，以更好地应用领域专家和机器学习指标。 

---
# Confidence as a Reward: Transforming LLMs into Reward Models 

**Title (ZH)**: 自信作为奖励：将大语言模型转变为奖励模型 

**Authors**: He Du, Bowen Li, Chengxing Xie, Chang Gao, Kai Chen, Dacheng Tao  

**Link**: [PDF](https://arxiv.org/pdf/2510.13501)  

**Abstract**: Reward models can significantly enhance the reasoning capabilities of large language models (LLMs), but they typically require extensive curated data and costly training. To mitigate these challenges, training-free approaches such as LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate responses, achieving promising results. Recent works have also indicated that model confidence can serve effectively as a reward metric, distinguishing between chain-of-thought (CoT) and non-CoT paths. However, the concept of using confidence as a reward has not been comprehensively studied. In this work, we systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful training-free method that utilizes token-level confidence in the model's final answers as a proxy for reward, especially suitable for close-ended tasks. Through extensive experiments on mathematical reasoning tasks, we demonstrate that CRew outperforms existing training-free reward approaches on the MATH500 and RewardMATH benchmarks, and even surpasses most trained reward models. We further identify a strong correlation between CRew scores and the actual reasoning performance of the model. Additionally, we find that CRew can effectively filter high-quality training data. Building upon these insights, we propose CRew-DPO, a training strategy that constructs preference data from confidence scores combined with correctness signals. Finetuning with CRew-DPO further enhances the model's judging capabilities and consistently outperforms existing self-training methods. 

**Abstract (ZH)**: 奖励模型可以显著增强大型语言模型的推理能力，但通常需要大量的精心标注数据和昂贵的训练成本。为了缓解这些挑战，无需训练的方法如LLM-as-a-Judge利用大型语言模型的固有推理能力来评估响应，取得了令人鼓舞的结果。最近的研究还表明，模型的信心可以有效地作为奖励指标，区分链式思维路径和非链式思维路径。然而，将信心作为奖励的概念尚未得到全面研究。在这项工作中，我们系统地研究了基于信心的奖励(CRew)方法，这是一种简单而强大的无需训练的方法，利用模型最终答案的令牌级信心作为奖励的代理，特别适用于封闭式任务。通过在数学推理任务上的大量实验，我们证明了CRew在MATH500和RewardMATH基准上优于现有的无需训练的奖励方法，并且甚至超越了大多数经过训练的奖励模型。我们进一步发现CRew分数与模型的实际推理性能之间存在强烈的相关性。此外，我们发现CRew可以有效地过滤高质量的训练数据。基于这些见解，我们提出了一种训练策略CRew-DPO，该策略从信心分数与正确性信号相结合的数据中构建偏好数据。使用CRew-DPO进行微调进一步增强了模型的评估能力，并且在所有现有的自我训练方法中表现最佳。 

---
# Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse 

**Title (ZH)**: 通过隐式因果链发现评估大语言模型在气候话语中的推理能力 

**Authors**: Liesbeth Allein, Nataly Pineda-Castañeda, Andrea Rocci, Marie-Francine Moens  

**Link**: [PDF](https://arxiv.org/pdf/2510.13417)  

**Abstract**: How does a cause lead to an effect, and which intermediate causal steps explain their connection? This work scrutinizes the mechanistic causal reasoning capabilities of large language models (LLMs) to answer these questions through the task of implicit causal chain discovery. In a diagnostic evaluation framework, we instruct nine LLMs to generate all possible intermediate causal steps linking given cause-effect pairs in causal chain structures. These pairs are drawn from recent resources in argumentation studies featuring polarized discussion on climate change. Our analysis reveals that LLMs vary in the number and granularity of causal steps they produce. Although they are generally self-consistent and confident about the intermediate causal connections in the generated chains, their judgments are mainly driven by associative pattern matching rather than genuine causal reasoning. Nonetheless, human evaluations confirmed the logical coherence and integrity of the generated chains. Our baseline causal chain discovery approach, insights from our diagnostic evaluation, and benchmark dataset with causal chains lay a solid foundation for advancing future work in implicit, mechanistic causal reasoning in argumentation settings. 

**Abstract (ZH)**: 大型语言模型在隐含因果链发现任务中如何揭示因果机制：一项诊断评估的研究 

---
# Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning 

**Title (ZH)**: 自适应推理执行器：一种高效的协作代理系统 

**Authors**: Zehui Ling, Deshu Chen, Yichi Zhang, Yuchen Liu, Xigui Li, Xin Guo, Yuan Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2510.13214)  

**Abstract**: Recent advances in Large Language Models (LLMs) demonstrate that chain-of-thought prompting and deep reasoning substantially enhance performance on complex tasks, and multi-agent systems can further improve accuracy by enabling model debates. However, applying deep reasoning to all problems is computationally expensive. To mitigate these costs, we propose a complementary agent system integrating small and large LLMs. The small LLM first generates an initial answer, which is then verified by the large LLM. If correct, the answer is adopted directly; otherwise, the large LLM performs in-depth reasoning. Experimental results show that, for simple problems, our approach reduces the computational cost of the large LLM by more than 50% with negligible accuracy loss, while consistently maintaining robust performance on complex tasks. 

**Abstract (ZH)**: Recent Advances in Large Language Models: Integrating Small and Large LLMs for Efficient Deep Reasoning and Accuracy Improvement 

---
# Toward Reasoning-Centric Time-Series Analysis 

**Title (ZH)**: 面向推理的时间序列分析 

**Authors**: Xinlei Wang, Mingtian Tan, Jing Qiu, Junhua Zhao, Jinjin Gu  

**Link**: [PDF](https://arxiv.org/pdf/2510.13029)  

**Abstract**: Traditional time series analysis has long relied on pattern recognition, trained on static and well-established benchmarks. However, in real-world settings -- where policies shift, human behavior adapts, and unexpected events unfold -- effective analysis must go beyond surface-level trends to uncover the actual forces driving them. The recent rise of Large Language Models (LLMs) presents new opportunities for rethinking time series analysis by integrating multimodal inputs. However, as the use of LLMs becomes popular, we must remain cautious, asking why we use LLMs and how to exploit them effectively. Most existing LLM-based methods still employ their numerical regression ability and ignore their deeper reasoning potential. This paper argues for rethinking time series with LLMs as a reasoning task that prioritizes causal structure and explainability. This shift brings time series analysis closer to human-aligned understanding, enabling transparent and context-aware insights in complex real-world environments. 

**Abstract (ZH)**: 利用大型语言模型重新思考时间序列分析：作为因果结构和可解释性优先的推理任务 

---
# From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model 

**Title (ZH)**: 从叙述到概率推理：利用大型语言模型预测和解释碰撞中驾驶员的危险行为 

**Authors**: Boyou Chen, Gerui Xu, Zifei Wang, Huizhong Guo, Ananna Ahmed, Zhaonan Sun, Zhen Hu, Kaihan Zhang, Shan Bao  

**Link**: [PDF](https://arxiv.org/pdf/2510.13002)  

**Abstract**: Vehicle crashes involve complex interactions between road users, split-second decisions, and challenging environmental conditions. Among these, two-vehicle crashes are the most prevalent, accounting for approximately 70% of roadway crashes and posing a significant challenge to traffic safety. Identifying Driver Hazardous Action (DHA) is essential for understanding crash causation, yet the reliability of DHA data in large-scale databases is limited by inconsistent and labor-intensive manual coding practices. Here, we present an innovative framework that leverages a fine-tuned large language model to automatically infer DHAs from textual crash narratives, thereby improving the validity and interpretability of DHA classifications. Using five years of two-vehicle crash data from MTCF, we fine-tuned the Llama 3.2 1B model on detailed crash narratives and benchmarked its performance against conventional machine learning classifiers, including Random Forest, XGBoost, CatBoost, and a neural network. The fine-tuned LLM achieved an overall accuracy of 80%, surpassing all baseline models and demonstrating pronounced improvements in scenarios with imbalanced data. To increase interpretability, we developed a probabilistic reasoning approach, analyzing model output shifts across original test sets and three targeted counterfactual scenarios: variations in driver distraction and age. Our analysis revealed that introducing distraction for one driver substantially increased the likelihood of "General Unsafe Driving"; distraction for both drivers maximized the probability of "Both Drivers Took Hazardous Actions"; and assigning a teen driver markedly elevated the probability of "Speed and Stopping Violations." Our framework and analytical methods provide a robust and interpretable solution for large-scale automated DHA detection, offering new opportunities for traffic safety analysis and intervention. 

**Abstract (ZH)**: 车辆碰撞涉及道路使用者之间的复杂交互、瞬间决策以及具有挑战性的环境条件。其中，两车碰撞最为普遍，约占道路碰撞的70%，对交通安全构成了重大挑战。识别驾驶危险行为（DHA）对于理解碰撞成因至关重要，但由于大规模数据库中DHA数据的一致性和手动编码的劳动密集型特征，其可靠性受到限制。在这里，我们提出了一种创新框架，利用微调的大语言模型自动从交通事故文本叙述中推断DHA，从而提高DHA分类的有效性和可解释性。通过对MTCF五年来的两车碰撞数据进行微调，我们将Llama 3.2 1B模型应用到详细的事故叙述中，并将其性能与传统的机器学习分类器，包括随机森林、XGBoost、CatBoost和神经网络进行了比较。微调后的LLM整体准确率达到80%，超过了所有基线模型，并在不平衡数据情况下显示出显著改进。为了提高可解释性，我们开发了一种概率推理方法，分析模型输出在原始测试集和三个针对性的假设场景中的变化：司机分心程度的变化和年龄。我们的分析表明，对一名司机引入分心显著增加了“一般不安全驾驶”的可能性；对两名司机引入分心最大化了“双方司机采取了危险行为”的概率；指定一名青少年司机显著提高了“超速和停车违规”的概率。我们的框架和分析方法为大规模自动DHA检测提供了稳健且可解释的解决方案，为交通安全分析和干预提供了新的机会。 

---
# From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models 

**Title (ZH)**: 从字面到自由：一种促进大型语言模型中人类一致异常处理的元提示框架 

**Authors**: Imran Khan  

**Link**: [PDF](https://arxiv.org/pdf/2510.12864)  

**Abstract**: Large Language Models (LLMs) are increasingly being deployed as the reasoning engines for agentic AI systems, yet they exhibit a critical flaw: a rigid adherence to explicit rules that leads to decisions misaligned with human common sense and intent. This "rule-rigidity" is a significant barrier to building trustworthy autonomous agents. While prior work has shown that supervised fine-tuning (SFT) with human explanations can mitigate this issue, SFT is computationally expensive and inaccessible to many practitioners. To address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a novel, low-compute meta-prompting technique designed to elicit human-aligned exception handling in LLMs in a zero-shot manner. The RID framework provides the model with a structured cognitive schema for deconstructing tasks, classifying rules, weighing conflicting outcomes, and justifying its final decision. We evaluated the RID framework against baseline and Chain-of-Thought (CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced judgment across diverse domains. Our human-verified results demonstrate that the RID framework significantly improves performance, achieving a 95% Human Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT. Furthermore, it consistently produces higher-quality, intent-driven reasoning. This work presents a practical, accessible, and effective method for steering LLMs from literal instruction-following to liberal, goal-oriented reasoning, paving the way for more reliable and pragmatic AI agents. 

**Abstract (ZH)**: Large Language Models (LLMs)在代理AI系统中的推理引擎部署日益增多，但仍表现出一个关键缺陷：严格遵循显式规则，导致决策与人类常识和意图不符。“规则僵化”是构建可信赖自治代理的重大障碍。虽然已有研究表明，带有人类解释的监督微调（SFT）可以缓解这一问题，但SFT计算成本高且难以为许多实践者所用。为解决这一问题，我们提出了规则与意图区分（RID）框架，这是一种新颖的低计算量元提示技术，旨在以零样本方式引发与人类意图一致的异常处理。RID框架为模型提供了一个结构化的认知框架，用于分解任务、分类规则、评估冲突结果以及解释其最终决策。我们利用一个包含20种跨不同领域的细腻判断场景的自定义基准测试，评估了RID框架与基线和思维链（CoT）提示的效果。经人类验证的结果显示，RID框架显著提高了性能，实现了95%的人类一致性评分（HAS），而基线为80%，思维链为75%。此外，RID框架持续生成更高质量、意图驱动的推理。本工作提供了一种实用、易用且有效的办法，引导LLMs从严格遵循指令向自由、目标导向的推理转变，为更可靠和实际的AI代理铺平了道路。 

---
# Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs 

**Title (ZH)**: 蜂巢：高质量语料库与全栈套件，以解锁高级全开放MLLM 

**Authors**: Yi Zhang, Bolin Ni, Xin-Sheng Chen, Heng-Rui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, Meng-Hao Guo, Shi-Min Hu  

**Link**: [PDF](https://arxiv.org/pdf/2510.13795)  

**Abstract**: Fully open multimodal large language models (MLLMs) currently lag behind proprietary counterparts, primarily due to a significant gap in data quality for supervised fine-tuning (SFT). Existing open-source datasets are often plagued by widespread noise and a critical deficit in complex reasoning data, such as Chain-of-Thought (CoT), which hinders the development of advanced model capabilities. Addressing these challenges, our work makes three primary contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising approximately 15 million QA pairs, processed through multiple cleaning techniques and enhanced with a novel dual-level (short and long) CoT enrichment strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its underlying framework DataStudio, providing the community with a transparent and adaptable methodology for data curation that moves beyond static dataset releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B model on Honey-Data-15M. Experiments show that Bee-8B establishes a new state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is competitive with, and in some cases surpasses, recent semi-open models such as InternVL3.5-8B. Our work delivers to the community a suite of foundational resources, including: the Honey-Data-15M corpus; the full-stack suite comprising HoneyPipe and DataStudio; training recipes; an evaluation harness; and the model weights. This effort demonstrates that a principled focus on data quality is a key pathway to developing fully open MLLMs that are highly competitive with their semi-open counterparts. 

**Abstract (ZH)**: 全开放多模态大语言模型（MLLMs）目前落后于专有模型，主要原因是监督微调（SFT）数据质量存在显著差距。现有开源数据集通常存在广泛噪声和复杂推理数据（如Chain-of-Thought，CoT）严重不足的问题，这阻碍了高级模型能力的发展。为应对这些挑战，我们的工作做出了三项主要贡献。首先，我们引入了包含约1500万QA对的Honey-Data-15M新SFT数据集，经过多重清洗技术处理，并通过新颖的双层级（短和长）CoT增强策略进行了增强。其次，我们引入了HoneyPipe数据采编流水线及其底层框架DataStudio，为社区提供了一个透明且可适应的数据采编方法，超越了静态数据集发布。最后，为了验证我们的数据集和流水线，我们在Honey-Data-15M上训练了Bee-8B，一个8B模型。实验结果显示，Bee-8B在全开放MLLM中建立了新的最高水平，其性能与近期部分开放模型（如InternVL3.5-8B）相当，甚至在某些情况下超越了它们。我们的工作为社区提供了基础资源套件：Honey-Data-15M语料库；包括HoneyPipe和DataStudio的全栈套件；训练食谱；评估框架；以及模型权重。本研究证明，专注于数据质量是开发与部分开放模型高度竞争的全开放MLLM的关键途径。 

---
# The Art of Scaling Reinforcement Learning Compute for LLMs 

**Title (ZH)**: Reinforcement Learning 算法在大语言模型中的扩展计算艺术 

**Authors**: Devvrit Khatri, Lovish Madaan, Rishabh Tiwari, Rachit Bansal, Sai Surya Duvvuri, Manzil Zaheer, Inderjit S. Dhillon, David Brandfonbrener, Rishabh Agarwal  

**Link**: [PDF](https://arxiv.org/pdf/2510.13786)  

**Abstract**: Reinforcement learning (RL) has become central to training large language models (LLMs), yet the field lacks predictive scaling methodologies comparable to those established for pre-training. Despite rapidly rising compute budgets, there is no principled understanding of how to evaluate algorithmic improvements for scaling RL compute. We present the first large-scale systematic study, amounting to more than 400,000 GPU-hours, that defines a principled framework for analyzing and predicting RL scaling in LLMs. We fit sigmoidal compute-performance curves for RL training and ablate a wide range of common design choices to analyze their effects on asymptotic performance and compute efficiency. We observe: (1) Not all recipes yield similar asymptotic performance, (2) Details such as loss aggregation, normalization, curriculum, and off-policy algorithm primarily modulate compute efficiency without materially shifting the asymptote, and (3) Stable, scalable recipes follow predictable scaling trajectories, enabling extrapolation from smaller-scale runs. Combining these insights, we propose a best-practice recipe, ScaleRL, and demonstrate its effectiveness by successfully scaling and predicting validation performance on a single RL run scaled up to 100,000 GPU-hours. Our work provides both a scientific framework for analyzing scaling in RL and a practical recipe that brings RL training closer to the predictability long achieved in pre-training. 

**Abstract (ZH)**: 强化学习（RL）已成为培训大规模语言模型（LLMs）的核心，但该领域缺乏与预训练领域建立的方法学相媲美的预测扩展方法。尽管计算预算迅速增加，但对于评估RL计算扩展的算法改进却缺乏基本原则的理解。我们进行了首次大规模系统的研究，总计超过400,000个GPU小时，定义了一个分析和预测LLMs中RL扩展的严谨框架。我们拟合了RL训练的S形计算-性能曲线，并消除了一大范围常见的设计选择，以分析其对渐近性能和计算效率的影响。我们观察到：（1）并非所有配方都能实现相似的渐近性能，（2）细节如损失聚合、 normalization、递增式学习和离策算法主要调节计算效率而未实质性地改变渐近线，（3）稳定且可扩展的配方遵循可预测的扩展轨迹，从而允许从小规模运行中进行外推。结合这些洞见，我们提出了一种最佳实践配方——ScaleRL，并通过成功地将其扩展并预测到单个RL运行达到100,000个GPU小时的验证性能，证明了其有效性。我们的工作为分析RL扩展提供了科学框架，并提供了一种实用的配方，使RL训练更接近预训练领域已经实现的可预测性。 

---
# FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access 

**Title (ZH)**: FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access 

**Authors**: Aditya Tanikanti, Benoit Côté, Yanfei Guo, Le Chen, Nickolaus Saint, Ryan Chard, Ken Raffenetti, Rajeev Thakur, Thomas Uram, Ian Foster, Michael E. Papka, Venkatram Vishwanath  

**Link**: [PDF](https://arxiv.org/pdf/2510.13724)  

**Abstract**: We present the Federated Inference Resource Scheduling Toolkit (FIRST), a framework enabling Inference-as-a-Service across distributed High-Performance Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI models, like Large Language Models (LLMs), on existing HPC infrastructure. Leveraging Globus Auth and Globus Compute, the system allows researchers to run parallel inference workloads via an OpenAI-compliant API on private, secure environments. This cluster-agnostic API allows requests to be distributed across federated clusters, targeting numerous hosted models. FIRST supports multiple inference backends (e.g., vLLM), auto-scales resources, maintains "hot" nodes for low-latency execution, and offers both high-throughput batch and interactive modes. The framework addresses the growing demand for private, secure, and scalable AI inference in scientific workflows, allowing researchers to generate billions of tokens daily on-premises without relying on commercial cloud infrastructure. 

**Abstract (ZH)**: Federated Inference Resource Scheduling Toolkit (FIRST): 一种跨分布高性能计算集群的推理即服务框架 

---
# Closing the Gap Between Text and Speech Understanding in LLMs 

**Title (ZH)**: 在大规模语言模型中缩小文本和语音理解的差距 

**Authors**: Santiago Cuervo, Skyler Seto, Maureen de Seyssel, Richard He Bai, Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly, Zakaria Aldeneh  

**Link**: [PDF](https://arxiv.org/pdf/2510.13632)  

**Abstract**: Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts--and even cascaded pipelines--on language understanding tasks. We term this shortfall the text-speech understanding gap: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce SALAD--Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation--which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora. 

**Abstract (ZH)**: 大型语言模型（LLMs）可以适应以扩展其文本能力至语音输入。然而，这些语音适应的LLMs在其语言理解任务上的表现普遍低于其基于文本的同类模型——甚至串接管道模型。我们称此差距为文本-语音理解鸿沟：当语音适应的LLM处理语音输入时相对于原基于文本的LLM处理等效文本时观察到的表现下降。最近缩小这一差距的方法要么依赖大规模语音合成文本语料库，这成本高昂且高度依赖合成数据，要么依赖大规模专有语音数据集，这些数据集不可再现。因此，仍需更数据高效的选择替代方案来缩小文本-语音理解鸿沟。在本工作中，我们分析了这一差距由两个因素驱动：(i) 适应过程中的文本能力遗忘，(ii) 语音和文本的跨模态不对齐。基于此分析，我们引入了SALAD —— 通过积极选择和跨模态蒸馏进行学习的数据高效对齐，其结合了跨模态蒸馏和目标合成数据以改进对齐并减轻遗忘。应用于3B和7B LLMs，SALAD在广泛领域的知识、语言理解和推理基准测试中实现了与强开源权重模型相当的表现，同时仅使用公共语料库中语音数据量的数个数量级进行训练。 

---
# Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses 

**Title (ZH)**: 解锁公共目录：针对德国肿瘤诊断ICD编码的指令调优大语言模型 

**Authors**: Stefan Lenz, Lakisha Ortiz Rosario, Georg Vollmar, Arsenij Ustjanzew, Fatma Alickovic, Thomas Kindler, Torsten Panholzer  

**Link**: [PDF](https://arxiv.org/pdf/2510.13624)  

**Abstract**: Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential for structured cancer documentation in Germany. Smaller open-weight LLMs are appealing for privacy-preserving automation but often struggle with coding accuracy in German-language contexts. This study investigates whether instruction-based fine-tuning on public datasets improves the coding accuracy of open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded diagnoses from the local tumor documentation system as test data. In a systematic data quality assessment, the upper limit for ICD-10 coding performance was estimated at 60-79% for exact and 81-94% for partial (three-character codes only) derivation. As training data, over 500,000 question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS catalogues. Eight open-weight models from the Qwen, Llama, and Mistral families (7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to 41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3 topography coding also improved but started and remained considerably lower with an exact accuracy of 22-40% and a partial accuracy of 56-67% after fine-tuning. Malformed code outputs dropped to 0% for all models. Tumor-diagnosis recognition reached 99%. Accuracy correlated positively with model size, but gaps between small and large models narrowed after fine-tuning. The reasoning mode in Qwen3 generally yielded a lower performance than fine-tuning and was over 100 times slower. Our findings highlight the potential of leveraging public catalogues to build instruction datasets that improve LLMs in medical documentation tasks. The complete training dataset and the best-performing checkpoints of the fine-tuned models are available from this https URL. 

**Abstract (ZH)**: 使用ICD-10-GM和ICD-O-3精确编码肿瘤诊断对于德国结构化癌症文档记录至关重要。基于公共数据集的指令调整能否提高德语肿瘤诊断文本的编码准确性？性能评估使用了当地肿瘤文档系统中的编码诊断数据。在系统数据质量评估中，ICD-10精确编码的上限估计为60-79%，部分编码（仅三字符代码）的上限为81-94%。作为训练数据，基于ICD-10-GM、ICD-O-3和OPS目录创建了超过50万个问答对。使用Qwen、Llama和Mistral家族的8个开源模型（7-70亿参数）进行了指令调整。ICD-10-GM编码准确性从1.4-24%提高到41-58%，部分准确性从31-74%提高到73-83%。ICD-O-3组织学编码准确性也有所提高，但调整后精确编码为22-40%，部分编码为56-67%。所有模型的错误编码输出降至0%，肿瘤诊断识别率达到99%。准确性与模型大小正相关，但在调整后，小模型和大模型之间的差距缩小。Qwen3的推理模式通常性能较低，比调整慢100多倍。我们的研究结果突显了利用公共目录构建指令数据集以提高LLM在医疗文档任务中的性能的潜力。完整的训练数据集和最佳性能的模型检查点可在以下网址获取。 

---
# NOSA: Native and Offloadable Sparse Attention 

**Title (ZH)**: NOSA: 原生可卸载稀疏注意力机制 

**Authors**: Yuxiang Huang, Chaojun Xiao, Xu Han, Zhiyuan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2510.13602)  

**Abstract**: Trainable sparse attention has emerged as a promising solution to address the decoding efficiency bottleneck of LLMs in long-context processing, significantly saving memory accesses while minimally impacting task performance. However, existing sparse attention methods leave a crucial limitation unresolved: the size of the key-value (KV) cache remains unreduced, which constrains on-GPU batch sizes and throttles decoding throughput, especially in large-scale batched inference. In this paper, we show that trainable sparse attention naturally exhibits strong locality in token selection across adjacent decoding steps, thereby enabling KV cache offloading without altering the underlying attention computation. However, the inherent locality remains insufficient to achieve efficient offloading, as the transfer of selected KV pairs between the CPU and GPU continues to dominate the overall decoding cost. Building on this insight, we present NOSA, a trainable sparse attention framework designed to natively support KV cache offloading. NOSA introduces explicit locality constraints by decomposing token selection into query-aware and query-agnostic components, thereby reducing KV transfers while preserving the same attention computation as used during training. We pretrain a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that it preserves near-lossless performance while achieving up to a 2.3x improvement in decoding throughput compared with the vanilla trainable sparse attention baseline (InfLLM-V2). 

**Abstract (ZH)**: 可训练稀疏注意力已成为解决LLMs在长上下文处理中解码效率瓶颈的有前途的解决方案，显著减少了内存访问次数，同时 minimally 影响任务性能。然而，现有的稀疏注意力方法未能解决一个关键限制：键值（KV）缓存的大小未被缩减，这限制了GPU上的批量大小，并使解码吞吐量受到影响，尤其是在大规模批量推理中。在本文中，我们展示了可训练稀疏注意力在相邻解码步骤中自然表现出很强的tokens选择局部性，从而能够在不改变基础注意力计算的情况下实现KV缓存卸载。然而，固有的局部性仍然不足以实现高效的卸载，因为CPU和GPU之间选定的KV对的传输继续主导整体解码成本。基于这一见解，我们提出了NOSA，这是一种设计用于原生支持KV缓存卸载的可训练稀疏注意力框架。NOSA通过将token选择分解为query-aware和query-agnostic部分，引入明确的局部性约束，从而减少KV传输，同时保持与训练中相同的注意力计算。我们使用NOSA预训练了一个1亿参数模型，并进行了广泛的基准测试，结果显示它保留了接近无损的性能，解码吞吐量相比 vanilla 可训练稀疏注意力 baselines（InfLLM-V2）提高了2.3倍。 

---
# Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs 

**Title (ZH)**: 基于LLM的NPC中角色真实性与任务执行的平衡之道：游戏对话中的解防设计 

**Authors**: Pasin Buakhaw, Kun Kerdthaisong, Phuree Phenhiran, Pitikorn Khlaisamniang, Supasate Vorathammathorn, Piyalitt Ittichaiwong, Nutchanon Yongsatianchot  

**Link**: [PDF](https://arxiv.org/pdf/2510.13586)  

**Abstract**: The emergence of large language models (LLMs) has opened new opportunities for cre- ating dynamic non-player characters (NPCs) in gaming environments, enabling both func- tional task execution and persona-consistent dialogue generation. In this paper, we (Tu_Character_lab) report our participation in the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which eval- uates agents across three tracks: task-oriented dialogue, context-aware dialogue, and their integration. Our approach combines two complementary strategies: (i) lightweight prompting techniques in the API track, including a Deflanderization prompting method to suppress excessive role-play and improve task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on Task 3 (GPU track). 

**Abstract (ZH)**: 大型语言模型的出现为在游戏环境中创建动态非玩家角色（NPC）开辟了新机遇，使其既能执行功能性任务，又能生成符合人设的对话。在本文中，Tu_Character_lab报道了我们参加Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2的情况，该挑战在三个赛道上评估了智能体：任务导向对话、上下文感知对话及其集成。我们的方法结合了两种互补策略：（i）API赛道中的轻量级提示技术，包括Deflander化提示方法以抑制过度角色模仿并提高任务真实性，和（ii）GPU赛道中微调的大规模模型，利用Qwen3-14B的有监督微调（SFT）和低秩适应（LoRA）技术。我们的最佳提交在Task 1中排名第二，在API赛道的Task 3中排名第二，在GPU赛道的Task 3中排名第四。 

---
# In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers 

**Title (ZH)**: 基于浏览器的LLM引导模糊测试用于实时提示注入测试在自主AI浏览器中 

**Authors**: Avihay Cohen  

**Link**: [PDF](https://arxiv.org/pdf/2510.13543)  

**Abstract**: Large Language Model (LLM) based agents integrated into web browsers (often called agentic AI browsers) offer powerful automation of web tasks. However, they are vulnerable to indirect prompt injection attacks, where malicious instructions hidden in a webpage deceive the agent into unwanted actions. These attacks can bypass traditional web security boundaries, as the AI agent operates with the user privileges across sites. In this paper, we present a novel fuzzing framework that runs entirely in the browser and is guided by an LLM to automatically discover such prompt injection vulnerabilities in real time. 

**Abstract (ZH)**: 基于大规模语言模型（LLM）的网络浏览器代理（通常称为具身AI浏览器）能够实现网页任务的强大自动化。然而，它们容易受到间接提示注入攻击的影响，恶意指令潜藏在网页中，欺骗代理执行不必要的操作。这些攻击可以绕过传统的网页安全边界，因为AI代理以用户的特权跨站点运行。本文介绍了一个全新的浏览器内置 fuzzing 框架，该框架由LLM引导，可以实时自动发现此类提示注入漏洞。 

---
# K-Merge: Online Continual Merging of Adapters for On-device Large Language Models 

**Title (ZH)**: K-Merge: 在设备上大型语言模型的在线连续适应器合并方法 

**Authors**: Donald Shenaj, Ondrej Bohdal, Taha Ceritli, Mete Ozay, Pietro Zanuttigh, Umberto Michieli  

**Link**: [PDF](https://arxiv.org/pdf/2510.13537)  

**Abstract**: On-device deployment of Large Language Models (LLMs) frequently leverages Low-Rank Adapters (LoRAs) to support diverse downstream tasks under tight resource constraints. To address the limited storage capacity of mobile devices, recent works have explored model merging techniques to fuse multiple LoRAs into a single one. In practice, however, LoRAs are often delivered incrementally, as users request support for new tasks (e.g., novel problem types or languages). This scenario introduces a new challenge: on-device online continual merging, where the objective is to incorporate new LoRAs while preserving the performance on previously supported tasks. In this paper, we propose a data-free and computationally efficient strategy for selecting and merging LoRAs when a new one becomes available, assuming the device can store only a limited number of adapters. Extensive experiments across real-world tasks demonstrate the superiority of our approach compared to alternative strategies while adhering to the storage budget and compute limitations of on-device settings. 

**Abstract (ZH)**: 设备端部署大语言模型中的低秩适配器在线连续合并策略 

---
# UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning 

**Title (ZH)**: UniME-V2: 作为通用多模态嵌入学习裁判的MLLM 

**Authors**: Tiancheng Gu, Kaicheng Yang, Kaichen Zhang, Xiang An, Ziyong Feng, Yueyi Zhang, Weidong Cai, Jiankang Deng, Lidong Bing  

**Link**: [PDF](https://arxiv.org/pdf/2510.13515)  

**Abstract**: Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks. 

**Abstract (ZH)**: 通用多模态嵌入模型是各种任务的基础。现有方法通常通过测量查询-候选对之间的相似性来抽取内批次负样本。然而，这些方法往往难以捕捉候选者之间的微妙语义差异，并且负样本缺乏多样性。此外，嵌入在区分虚假负样本和难负样本方面表现出有限的辨别能力。在本文中，我们利用大规模预训练语言模型的高级理解能力来增强表示学习，并提出了一种新型的通用多模态嵌入（UniME-V2）模型。我们的方法首先通过全局检索构建一个潜在的难负样本集。然后引入大规模语言模型作为裁判机制，利用大规模语言模型评估查询-候选对之间的语义对齐并生成软语义匹配得分。这些得分作为难负样本挖掘的基础，减轻了虚假负样本的影响，并能够识别多样化的高质量难负样本。此外，语义匹配得分作为软标签用于缓解刚性的一对一映射约束。通过将相似性矩阵与软语义匹配得分矩阵对齐，模型学习候选者之间的语义差异，显著增强了其辨别能力。为了进一步提高性能，我们提出了基于我们在挖掘难负样本时通过联合成对和列表优化方法训练的UniME-V2-Reranker重排序模型。我们对MMEB基准和多个检索任务进行了全面实验，表明我们的方法在所有任务上的平均表现达到了最先进的水平。 

---
# Offline and Online KL-Regularized RLHF under Differential Privacy 

**Title (ZH)**: 离线和在线KL正则化RLHF下的差异隐私CloseOperation 

**Authors**: Yulian Wu, Rushil Thareja, Praneeth Vepakomma, Francesco Orabona  

**Link**: [PDF](https://arxiv.org/pdf/2510.13512)  

**Abstract**: In this paper, we study the offline and online settings of reinforcement learning from human feedback (RLHF) with KL-regularization -- a widely used objective function in large language model alignment -- under the $\epsilon$ local differential privacy ($\epsilon$-LDP) model on the label of the human preference. In the offline setting, we design an algorithm based on the principle of pessimism and derive a new suboptimality gap of $\tilde{O}(1/[(e^\epsilon-1)^2 n])$ on the KL-regularized objective under single-policy concentrability. We also prove its optimality by providing a matching lower bound where $n$ is the sample size.
In the online setting, we are the first one to theoretically investigate the problem of KL-regularized RLHF with LDP. We design an optimism-based algorithm and derive a logarithmic regret bound of $O(d_{\mathcal{F}}\log (N_{\mathcal{F}}\cdot T) /(e^\epsilon-1)^2 )$, where $T$ is the total time step, $N_{\mathcal{F}}$ is cardinality of the reward function space $\mathcal{F}$ and $d_{\mathcal{F}}$ is a variant of eluder dimension for RLHF. As a by-product of our analysis, our results also imply the first analysis for online KL-regularized RLHF without privacy. We implement our algorithm in the offline setting to verify our theoretical results and release our open source code at: this https URL. 

**Abstract (ZH)**: 在$\epsilon$局部差分隐私（$\epsilon$-LDP）模型下的人类偏好标签下，具有KL-正则化的大语言模型对齐的离线和在线设置中的强化学习-from-人类反馈（RLHF）研究 

---
# MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts 

**Title (ZH)**: MedREK: 基于检索的关键意识提示编辑医疗LLMs 

**Authors**: Shujun Xia, Haokun Lin, Yichen Wu, Yinan Zhou, Zixuan Li, Zhongwei Wan, Xingrun Xing, Yefeng Zheng, Xiang Li, Caifeng Shan, Zhenan Sun, Quanzheng Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.13500)  

**Abstract**: LLMs hold great promise for healthcare applications, but the rapid evolution of medical knowledge and errors in training data often cause them to generate outdated or inaccurate information, limiting their applicability in high-stakes clinical practice. Model editing has emerged as a potential remedy without full retraining. While parameter-based editing often compromises locality and is thus ill-suited for the medical domain, retrieval-based editing offers a more viable alternative. However, it still faces two critical challenges: (1) representation overlap within the medical knowledge space often causes inaccurate retrieval and reduces editing accuracy; (2) existing methods are restricted to single-sample edits, while batch-editing remains largely unexplored despite its importance for real-world medical applications. To address these challenges, we first construct MedVersa, \hk{an enhanced benchmark with broader coverage of medical subjects, designed to evaluate both single and batch edits under strict locality constraints}. We then propose MedREK, a retrieval-based editing framework that integrates a shared query-key module for precise matching with an attention-based prompt encoder for informative guidance. Experimental results on various medical benchmarks demonstrate that our MedREK achieves superior performance across different core metrics and provides the first validated solution for batch-editing in medical LLMs. Our code and dataset are available at this https URL. 

**Abstract (ZH)**: LLMs在医疗应用中前景广阔，但快速演进的医学知识和训练数据中的错误常常导致它们生成过时或不准确的信息，限制了其在高风险临床实践中的应用。基于模型的编辑作为一种潜在的解决方案而出现，无需进行全面重新训练。虽然基于参数的编辑通常会牺牲局部性，因此不适于医疗领域，但基于检索的编辑提供了一个更可行的选择。然而，它仍然面临两个关键挑战：（1）医学知识空间内的表示重叠往往导致不准确的检索，降低了编辑的准确性；（2）现有的方法主要针对单样本编辑，而批量编辑在实际医疗应用中至关重要，但这一领域仍然鲜有探索。为应对这些挑战，我们首先构建了MedVersa，一种增强的基准测试，旨在在严格的局部性约束下评估单样本和批量编辑；然后，我们提出了MedREK，一种基于检索的编辑框架，该框架结合了共享查询-键模块以实现精确匹配，并采用基于注意力的提示编码器以提供信息指导。我们在多种医疗基准测试上的实验结果表明，我们的MedREK在不同核心指标上取得了优越的表现，并提供了首个批量编辑的验证解决方案。我们的代码和数据集可通过以下链接获取。 

---
# ConsintBench: Evaluating Language Models on Real-World Consumer Intent Understanding 

**Title (ZH)**: ConsintBench: 评估语言模型在消费者意图理解中的现实世界表现 

**Authors**: Xiaozhe Li, TianYi Lyu, Siyi Yang, Yuxi Gong, Yizhao Yang, Jinxuan Huang, Ligao Zhang, Zhuoyi Huang, Qingwen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2510.13499)  

**Abstract**: Understanding human intent is a complex, high-level task for large language models (LLMs), requiring analytical reasoning, contextual interpretation, dynamic information aggregation, and decision-making under uncertainty. Real-world public discussions, such as consumer product discussions, are rarely linear or involve a single user. Instead, they are characterized by interwoven and often conflicting perspectives, divergent concerns, goals, emotional tendencies, as well as implicit assumptions and background knowledge about usage scenarios. To accurately understand such explicit public intent, an LLM must go beyond parsing individual sentences; it must integrate multi-source signals, reason over inconsistencies, and adapt to evolving discourse, similar to how experts in fields like politics, economics, or finance approach complex, uncertain environments. Despite the importance of this capability, no large-scale benchmark currently exists for evaluating LLMs on real-world human intent understanding, primarily due to the challenges of collecting real-world public discussion data and constructing a robust evaluation pipeline. To bridge this gap, we introduce \bench, the first dynamic, live evaluation benchmark specifically designed for intent understanding, particularly in the consumer domain. \bench is the largest and most diverse benchmark of its kind, supporting real-time updates while preventing data contamination through an automated curation pipeline. 

**Abstract (ZH)**: 理解人类意图是大型语言模型（LLMs）的一项复杂高阶任务，要求进行分析推理、情境诠释、动态信息聚合以及在不确定性下的决策制定。像消费者产品讨论这样的真实世界公共讨论通常不是线性的，也往往不仅仅涉及单一用户，而是由交织并且经常互相冲突的观点、分散的关注点、目标、情绪倾向以及关于使用场景的隐含假设和背景知识所界定。为了准确理解这种显性的公共意图，一个LLM必须超越仅仅解析单个句子，还必须整合多源信号、推理解决不一致之处，并适应不断演变的对话，类似于政治、经济学或金融领域的专家是如何应对复杂不确定环境的方式。尽管此项能力的重要性不言而喻，但目前尚无大规模基准可以评估LLMs在真实世界中的人类意图理解能力，主要原因在于收集真实世界公共讨论数据和构建健壮评估管道的挑战。为解决这一问题，我们引入了\bench，这是首个专为意图理解设计的动态实时评估基准，特别适用于消费者领域。\bench是此类中规模最大、多样性最高的基准，通过自动化策展管道支持实时更新并防止数据污染。 

---
# MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation 

**Title (ZH)**: MADREC：一个多方面驱动的LLM代理，用于可解释和自适应推荐 

**Authors**: Jiin Park, Misuk Kim  

**Link**: [PDF](https://arxiv.org/pdf/2510.13371)  

**Abstract**: Recent attempts to integrate large language models (LLMs) into recommender systems have gained momentum, but most remain limited to simple text generation or static prompt-based inference, failing to capture the complexity of user preferences and real-world interactions. This study proposes the Multi-Aspect Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs user and item profiles by unsupervised extraction of multi-aspect information from reviews and performs direct recommendation, sequential recommendation, and explanation generation. MADRec generates structured profiles via aspect-category-based summarization and applies Re-Ranking to construct high-density inputs. When the ground-truth item is missing from the output, the Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments across multiple domains show that MADRec outperforms traditional and LLM-based baselines in both precision and explainability, with human evaluation further confirming the persuasiveness of the generated explanations. 

**Abstract (ZH)**: Recent Attempts to Integrate Large Language Models into Recommender Systems Have Gained Momentum, but Most Remain Limited to Simple Text Generation or Static Prompt-Based Inference, Failing to Capture the Complexity of User Preferences and Real-World Interactions: Proposing the Multi-Aspect Driven LLM Agent MADRec 

---
# Document Intelligence in the Era of Large Language Models: A Survey 

**Title (ZH)**: 大型语言模型时代的内容智能：一个综述 

**Authors**: Weishi Wang, Hengchang Hu, Zhijie Zhang, Zhaochen Li, Hongxin Shao, Daniel Dahlmeier  

**Link**: [PDF](https://arxiv.org/pdf/2510.13366)  

**Abstract**: Document AI (DAI) has emerged as a vital application area, and is significantly transformed by the advent of large language models (LLMs). While earlier approaches relied on encoder-decoder architectures, decoder-only LLMs have revolutionized DAI, bringing remarkable advancements in understanding and generation. This survey provides a comprehensive overview of DAI's evolution, highlighting current research attempts and future prospects of LLMs in this field. We explore key advancements and challenges in multimodal, multilingual, and retrieval-augmented DAI, while also suggesting future research directions, including agent-based approaches and document-specific foundation models. This paper aims to provide a structured analysis of the state-of-the-art in DAI and its implications for both academic and practical applications. 

**Abstract (ZH)**: 文档AI（DAI）已成为一个至关重要的应用领域，大型语言模型（LLMs）的出现对其进行了显著改造。虽然早期的方法依赖于编码器-解码器架构，但仅解码器的LLMs已彻底革新了DAI，带来了理解和生成能力的显著提升。本文综述了DAI的发展历程，强调了当前在该领域使用LLMs的研究尝试及其未来前景。我们探讨了多模态、多语言和检索增强的DAI的关键进展和挑战，并建议了基于代理的方法和文档特定的基础模型作为未来的研究方向。本文旨在提供DAI最新进展的结构化分析，并探讨其对学术和实际应用的意义。 

---
# Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems 

**Title (ZH)**: Protect: 向 Towards 坚实的企业级LLM系统可信运行防护栈迈进 Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems 

**Authors**: Karthik Avinash, Nikhil Pareek, Rishav Hada  

**Link**: [PDF](https://arxiv.org/pdf/2510.13351)  

**Abstract**: The increasing deployment of Large Language Models (LLMs) across enterprise and mission-critical domains has underscored the urgent need for robust guardrailing systems that ensure safety, reliability, and compliance. Existing solutions often struggle with real-time oversight, multi-modal data handling, and explainability -- limitations that hinder their adoption in regulated environments. Existing guardrails largely operate in isolation, focused on text alone making them inadequate for multi-modal, production-scale environments. We introduce Protect, natively multi-modal guardrailing model designed to operate seamlessly across text, image, and audio inputs, designed for enterprise-grade deployment. Protect integrates fine-tuned, category-specific adapters trained via Low-Rank Adaptation (LoRA) on an extensive, multi-modal dataset covering four safety dimensions: toxicity, sexism, data privacy, and prompt injection. Our teacher-assisted annotation pipeline leverages reasoning and explanation traces to generate high-fidelity, context-aware labels across modalities. Experimental results demonstrate state-of-the-art performance across all safety dimensions, surpassing existing open and proprietary models such as WildGuard, LlamaGuard-4, and GPT-4.1. Protect establishes a strong foundation for trustworthy, auditable, and production-ready safety systems capable of operating across text, image, and audio modalities. 

**Abstract (ZH)**: 大型语言模型在企业级和关键任务领域中的广泛应用凸显了建立 robust 安全监管系统的迫切需求，以确保安全、可靠性和合规性。现有解决方案在实时监控、多模态数据处理和解释性方面存在局限性，这些局限性阻碍了其在受监管环境中的应用。现有监管措施主要独立运行，专注于文本数据，使其无法适应多模态的生产规模环境。我们提出了 Protect，一种原生多模态监管模型，旨在无缝处理文本、图像和音频输入，适用于企业级部署。Protect 结合了通过低秩适应（LoRA）在涵盖四大安全维度（毒性、性别歧视、数据隐私和提示注入）的多模态数据集上进行微调的专业化适配器，其教师辅助注释管道利用推理和解释轨迹生成高保真、上下文相关的跨模态标签。实验结果表明，Protect 在所有安全维度上均表现卓越，超越了现有开源和专有模型（如 WildGuard、LlamaGuard-4 和 GPT-4.1）。Protect 为跨文本、图像和音频模态运行的信任、可审计和生产级安全系统奠定了坚实基础。 

---
# Thompson Sampling via Fine-Tuning of LLMs 

**Title (ZH)**: 基于LLM微调的Thompson采样方法 

**Authors**: Nicolas Menet, Aleksandar Terzić, Andreas Krause, Abbas Rahimi  

**Link**: [PDF](https://arxiv.org/pdf/2510.13328)  

**Abstract**: Bayesian optimization in large unstructured discrete spaces is often hindered by the computational cost of maximizing acquisition functions due to the absence of gradients. We propose a scalable alternative based on Thompson sampling that eliminates the need for acquisition function maximization by directly parameterizing the probability that a candidate yields the maximum reward. Our approach, Thompson Sampling via Fine-Tuning (ToSFiT) leverages the prior knowledge embedded in prompt-conditioned large language models, and incrementally adapts them toward the posterior. Theoretically, we derive a novel regret bound for a variational formulation of Thompson Sampling that matches the strong guarantees of its standard counterpart. Our analysis reveals the critical role of careful adaptation to the posterior probability of maximality--a principle that underpins our ToSFiT algorithm. Empirically, we validate our method on three diverse tasks: FAQ response refinement, thermally stable protein search, and quantum circuit design. We demonstrate that online fine-tuning significantly improves sample efficiency, with negligible impact on computational efficiency. 

**Abstract (ZH)**: 基于Thomas采样的大规模无序离散空间贝叶斯优化：无需最大化获取函数的可扩展替代方法 

---
# LLM one-shot style transfer for Authorship Attribution and Verification 

**Title (ZH)**: LLM 一键式风格转换在作者归类与验证中的应用 

**Authors**: Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho  

**Link**: [PDF](https://arxiv.org/pdf/2510.13302)  

**Abstract**: Computational stylometry analyzes writing style through quantitative patterns in text, supporting applications from forensic tasks such as identity linking and plagiarism detection to literary attribution in the humanities. Supervised and contrastive approaches rely on data with spurious correlations and often confuse style with topic. Despite their natural use in AI-generated text detection, the CLM pre-training of modern LLMs has been scarcely leveraged for general authorship problems. We propose a novel unsupervised approach based on this extensive pre-training and the in-context learning capabilities of LLMs, employing the log-probabilities of an LLM to measure style transferability from one text to another. Our method significantly outperforms LLM prompting approaches of comparable scale and achieves higher accuracy than contrastively trained baselines when controlling for topical correlations. Moreover, performance scales fairly consistently with the size of the base model and, in the case of authorship verification, with an additional mechanism that increases test-time computation; enabling flexible trade-offs between computational cost and accuracy. 

**Abstract (ZH)**: 基于大规模预训练和上下文学习能力的无监督计算文体学方法 

---
# Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize Meituan's Intelligent Interaction Systems 

**Title (ZH)**: 更高的满意度，更低的成本：一项关于LLM如何革命化美团智能交互系统的技术报告 

**Authors**: Xuxin Cheng, Ke Zeng, Zhiquan Cao, Linyi Dai, Wenxuan Gao, Fei Han, Ai Jian, Feng Hong, Wenxing Hu, Zihe Huang, Dejian Kong, Jia Leng, Zhuoyuan Liao, Pei Liu, Jiaye Lin, Xing Ma, Jingqing Ruan, Jiaxing Song, Xiaoyu Tan, Ruixuan Xiao, Wenhui Yu, Wenyu Zhan, Haoxing Zhang, Chao Zhou, Hao Zhou, Shaodong Zheng, Ruinian Chen, Siyuan Chen, Ziyang Chen, Yiwen Dong, Yaoyou Fan, Yangyi Fang, Yang Gan, Shiguang Guo, Qi He, Chaowen Hu, Binghui Li, Dailin Li, Xiangyu Li, Yan Li, Chengjian Liu, Xiangfeng Liu, Jiahui Lv, Qiao Ma, Jiang Pan, Cong Qin, Chenxing Sun, Wen Sun, Zhonghui Wang, Abudukelimu Wuerkaixi, Xin Yang, Fangyi Yuan, Yawen Zhu, Tianyi Zhai, Jie Zhang, Runlai Zhang, Yao Xu, Yiran Zhao, Yifan Wang, Xunliang Cai, Yangen Hu, Cao Liu, Lu Pan, Xiaoli Wang, Bo Xiao, Wenyuan Yao, Qianlin Zhou, Benchang Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2510.13291)  

**Abstract**: Enhancing customer experience is essential for business success, particularly as service demands grow in scale and complexity. Generative artificial intelligence and Large Language Models (LLMs) have empowered intelligent interaction systems to deliver efficient, personalized, and 24/7 support. In practice, intelligent interaction systems encounter several challenges: (1) Constructing high-quality data for cold-start training is difficult, hindering self-evolution and raising labor costs. (2) Multi-turn dialogue performance remains suboptimal due to inadequate intent understanding, rule compliance, and solution extraction. (3) Frequent evolution of business rules affects system operability and transferability, constraining low-cost expansion and adaptability. (4) Reliance on a single LLM is insufficient in complex scenarios, where the absence of multi-agent frameworks and effective collaboration undermines process completeness and service quality. (5) The open-domain nature of multi-turn dialogues, lacking unified golden answers, hampers quantitative evaluation and continuous optimization. To address these challenges, we introduce WOWService, an intelligent interaction system tailored for industrial applications. With the integration of LLMs and multi-agent architectures, WOWService enables autonomous task management and collaborative problem-solving. Specifically, WOWService focuses on core modules including data construction, general capability enhancement, business scenario adaptation, multi-agent coordination, and automated evaluation. Currently, WOWService is deployed on the Meituan App, achieving significant gains in key metrics, e.g., User Satisfaction Metric 1 (USM 1) -27.53% and User Satisfaction Metric 2 (USM 2) +25.51%, demonstrating its effectiveness in capturing user needs and advancing personalized service. 

**Abstract (ZH)**: 提升客户体验对于商业成功至关重要，尤其是在服务需求规模和复杂性增长的情况下。生成式人工智能和大型语言模型（LLMs）赋能智能交互系统，提供高效、个性化和不间断的支持。实践中，智能交互系统面临多重挑战：（1）冷启动训练高质量数据构建困难，阻碍自我进化并增加劳动力成本。（2）多轮对话表现仍不理想，由于意图理解不足、规则合规性和解决方案提取不够。（3）业务规则的频繁更新影响系统操作性和可移植性，限制低成本扩展和适应性。（4）单一LLM在复杂场景中不足，缺乏多代理框架和有效协作损害流程完整性和服务质量。（5）多轮对话的开放领域特性缺乏统一标准答案，阻碍定量评估和持续优化。为应对这些挑战，我们提出了WOwService，一种针对工业应用的智能交互系统。通过集成LLMs和多代理架构，WOwService实现了自主任务管理和协作问题解决。具体而言，WOwService专注于数据构建、通用能力提升、业务场景适配、多代理协调和自动化评估等核心模块。目前，WOwService已在美团App上线，关键指标显著提升，例如用户满意度指标1（USM 1）减少了27.53%，用户满意度指标2（USM 2）增加了25.51%，展示了其在捕捉用户需求和推动个性化服务方面的有效性。 

---
# To Steer or Not to Steer? Mechanistic Error Reduction with Abstention for Language Models 

**Title (ZH)**: 要不要干预？语言模型的 abstention 机制错误减少 

**Authors**: Anna Hedström, Salim I. Amoukou, Tom Bewley, Saumitra Mishra, Manuela Veloso  

**Link**: [PDF](https://arxiv.org/pdf/2510.13290)  

**Abstract**: We introduce Mechanistic Error Reduction with Abstention (MERA), a principled framework for steering language models (LMs) to mitigate errors through selective, adaptive interventions. Unlike existing methods that rely on fixed, manually tuned steering strengths, often resulting in under or oversteering, MERA addresses these limitations by (i) optimising the intervention direction, and (ii) calibrating when, and how much to steer, thereby provably improving performance or abstaining when no confident correction is possible. Experiments across diverse datasets, and LM families demonstrate safe, effective, non-degrading error correction, and that MERA outperforms existing baselines. Moreover, MERA can be applied on top of existing steering techniques to further enhance their performance, establishing it as a general-purpose, and efficient approach to mechanistic activation steering. 

**Abstract (ZH)**: 机制错误减少与回避：一种原理上正确的语言模型引导框架 

---
# What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging 

**Title (ZH)**: 不检测什么：基于结构化推理和token合并的负向意识大语言模型 

**Authors**: Inha Kang, Youngsun Lim, Seonho Lee, Jiho Choi, Junsuk Choe, Hyunjung Shim  

**Link**: [PDF](https://arxiv.org/pdf/2510.13232)  

**Abstract**: State-of-the-art vision-language models (VLMs) suffer from a critical failure in understanding negation, often referred to as affirmative bias. This limitation is particularly severe in described object detection (DOD) tasks. To address this, we propose two primary contributions: (1) a new dataset pipeline and (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a dataset constructed with a systematic chain-of-thought (CoT) and VQA-based pipeline to generate high-quality, instance-grounded negation data. Second, we propose NegToMe, a novel text token merging module that directly tackles the architectural cause of affirmative bias. NegToMe fundamentally addresses the structural loss of negation cues in tokenization, grouping them with attributes into coherent semantic phrases. It maintains correct polarity at the input level, enabling robust negation understanding even with limited data. For instance, to prevent a model from treating the fragmented tokens "not" and "girl" as simply "girl", NegToMe binds them into a single token whose meaning is correctly distinguished from that of "girl" alone. This module is integrated with a parameter-efficient and strategic LoRA fine-tuning approach. Our method significantly improves performance on challenging negation benchmarks with a lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval and demonstrating generalization to SoTA VLMs. This work marks a crucial step forward in addressing negation understanding for real-world detection applications. 

**Abstract (ZH)**: 最先进的视觉-语言模型（VLMs）在理解否定方面存在关键缺陷，通常称为肯定偏向。这一局限性在描述物体检测（DOD）任务中尤为严重。为了应对这一问题，我们提出了两项主要贡献：（1）一个新的数据集管道和（2）一种新颖的轻量级适应食谱。首先，我们引入了CoVAND数据集，该数据集通过系统性思维链（CoT）和VQA基于的管道生成高质量、实例相关的否定数据。其次，我们提出了一种新的文本令牌合并模块NegToMe，直接解决肯定偏向的架构原因。NegToMe从根本上解决了词元化过程中否定线索的结构损失，将它们与属性合并为连贯的语义短语。它在输入级别保持正确的极性，在数据有限的情况下也能实现稳健的否定理解。例如，为了防止模型将断裂的词元“not”和“girl”错误地处理为“girl”，NegToMe将它们绑定成一个单一的词元，其含义正确地区别于单独的“girl”。该模块与参数高效且有策略的LoRA微调方法集成。我们的方法在具有挑战性的否定基准测试上显著提高了性能，NMS-AP在OVDEval上提高了最多10.8分，并且展示了对最先进的VLMs的一般化能力。这项工作标志着在为实际检测应用提供否定理解方面迈出的重要一步。 

---
# LLM-Guided Synthetic Augmentation (LGSA) for Mitigating Bias in AI Systems 

**Title (ZH)**: LLM指导的合成增强方法（LGSA）用于减轻AI系统中的偏见 

**Authors**: Sai Suhruth Reddy Karri, Yashwanth Sai Nallapuneni, Laxmi Narasimha Reddy Mallireddy, Gopichand G  

**Link**: [PDF](https://arxiv.org/pdf/2510.13202)  

**Abstract**: Bias in AI systems, especially those relying on natural language data, raises ethical and practical concerns. Underrepresentation of certain groups often leads to uneven performance across demographics. Traditional fairness methods, such as pre-processing, in-processing, and post-processing, depend on protected-attribute labels, involve accuracy-fairness trade-offs, and may not generalize across datasets. To address these challenges, we propose LLM-Guided Synthetic Augmentation (LGSA), which uses large language models to generate counterfactual examples for underrepresented groups while preserving label integrity. We evaluated LGSA on a controlled dataset of short English sentences with gendered pronouns, professions, and binary classification labels. Structured prompts were used to produce gender-swapped paraphrases, followed by quality control including semantic similarity checks, attribute verification, toxicity screening, and human spot checks. The augmented dataset expanded training coverage and was used to train a classifier under consistent conditions. Results show that LGSA reduces performance disparities without compromising accuracy. The baseline model achieved 96.7 percent accuracy with a 7.2 percent gender bias gap. Simple swap augmentation reduced the gap to 0.7 percent but lowered accuracy to 95.6 percent. LGSA achieved 99.1 percent accuracy with a 1.9 percent bias gap, improving performance on female-labeled examples. These findings demonstrate that LGSA is an effective strategy for bias mitigation, enhancing subgroup balance while maintaining high task accuracy and label fidelity. 

**Abstract (ZH)**: AI系统中的偏差，尤其是在依赖自然语言数据的情况下，引发了伦理和实践方面的担忧。代表性不足的群体往往导致不同 demographic 组的表现不均衡。传统公平性方法，如预处理、内处理和后处理，依赖受保护属性标签，涉及准确性和公平性的权衡，并且可能不能泛化到不同的数据集。为了解决这些挑战，我们提出了一种基于大规模语言模型的合成增强（LGSA）方法，该方法利用大型语言模型为代表性不足的群体生成反事实示例，同时保持标签完整性。我们在一个受控的包含性别代词、职业和二元分类标签的短英语句子数据集中评估了 LGSA。使用结构化提示生成性别转换的同义句，随后进行质量控制，包括语义相似性检查、属性验证、有害内容筛选和人工抽查。增强后的数据集扩展了训练覆盖范围，并在一致条件下用于训练分类器。结果表明，LGSA 在减少性能差异的同时没有牺牲准确性。基线模型的准确率为 96.7%，性别偏差差距为 7.2%。简单的替换增强将差距缩小到 0.7%，但降低了准确率至 95.6%。LGSA 的准确率为 99.1%，性别偏差差距为 1.9%，并且在女性标记的示例上提高了性能。这些发现表明，LGSA 是一种有效的方法，可以减轻偏差，增强子组平衡，同时保持高任务准确性和标签忠实性。 

---
# Program of Thoughts for Financial Reasoning: Leveraging Dynamic In-Context Examples and Generative Retrieval 

**Title (ZH)**: 金融推理的思维程序：利用动态上下文示例和生成性检索 

**Authors**: Subhendu Khatuya, Shashwat Naidu, Pawan Goyal, Niloy Ganguly  

**Link**: [PDF](https://arxiv.org/pdf/2510.13157)  

**Abstract**: Despite continuous advancements in the capabilities of large language models (LLMs), numerical reasoning remains a challenging area. Techniques like chain-of-thought prompting, tree-of-thought prompting, and program-of-thought prompting guide LLMs through intermediate reasoning steps. Although in-context learning with few-shot prompting has improved performance, LLMs still lag behind state-of-the-art models on financial numerical reasoning datasets such as FinQA and ConvFinQA. In this work, we introduce FINDER, a novel two-step framework, to enhance LLMs' capabilities in financial numerical reasoning. The first step utilizes a generative retriever to extract relevant facts from unstructured data, including both text and tables. This is followed by context-aware Program of Thought prompting with dynamic selection of in-context examples. Our model FINDER achieves a new state-of-the-art performance on both the FinQA and ConvFinQA datasets, surpassing previous benchmarks with execution accuracy improvements of 5.98% and 4.05%, respectively. 

**Abstract (ZH)**: 尽管大型语言模型的能力持续进步，但数值推理仍然是一项挑战。FINDER：一种增强语言模型财务数值推理能力的新型两步框架。 

---
# Stable LLM Ensemble: Interaction between Example Representativeness and Diversity 

**Title (ZH)**: 稳定的大规模语言模型集成：示例表示性和多样性之间的互动 

**Authors**: Junichiro Niimi  

**Link**: [PDF](https://arxiv.org/pdf/2510.13143)  

**Abstract**: Large language models (LLMs) have achieved remarkable results in wide range of domains. However, the accuracy and robustness of one-shot LLM predictions remain highly sensitive to the examples and the diversity among ensemble members. This study systematically investigates the effects of example representativeness (one-shot strategy) and output diversity (sampling temperature) on LLM ensemble performance. Two one-shot strategies are compared: centroid-based representative examples (proposed) and randomly sampled examples (baseline) and sampling temperature also is varied. The proposed approach with higher temperature setting significantly outperforms random selection by +7.6% (macro-F1) and -10.5% (RMSE). Furthermore, the proposed model exceeds 5-shot prompting by +21.1% (macro-F1) and -24.0% (RMSE). Our findings demonstrate that combining representative example selection with increased temperature provides the appropriate level of diversity to the ensemble. This work highlights the practical importance of both example selection and controlled diversity in designing effective one-shot LLM ensembles. 

**Abstract (ZH)**: 大型语言模型（LLMs）在多个领域取得了显著成果。然而，单次预测的准确性和鲁棒性仍然高度依赖于示例的代表性及集成成员之间的多样性。本研究系统地探讨了示例代表性（单次策略）和输出多样性（采样温度）对LLM集成性能的影响。比较了两种单次策略：基于质心的代表示例（提出的方法）和随机抽样示例（基线方法），同时改变了采样温度。在较高的温度设置下，提出的方案在macro-F1上比随机选择高7.6%，在RMSE上低10.5%。此外，提出的模型在5次提示下超出21.1%（macro-F1）和24.0%（RMSE）。研究发现，结合代表性示例选择与增加温度可以为集成提供适当的多样性水平。本研究突出了在设计有效的单次LLM集成中示例选择和可控多样性的重要性。 

---
# On the Reasoning Abilities of Masked Diffusion Language Models 

**Title (ZH)**: 掩码扩散语言模型的推理能力研究 

**Authors**: Anej Svete, Ashish Sabharwal  

**Link**: [PDF](https://arxiv.org/pdf/2510.13117)  

**Abstract**: Masked diffusion models (MDMs) for text offer a compelling alternative to traditional autoregressive language models. Parallel generation makes them efficient, but their computational capabilities and the limitations inherent to their parallelism remain largely unexplored. To this end, we characterize what types of reasoning problems MDMs can provably solve and how efficiently. We do this by connecting MDMs to the well-understood reasoning frameworks of chain of thought (CoT) and padded looped transformers (PLTs) in the finite-precision log-width setting: We show that MDMs and polynomially-padded PLTs are, in fact, equivalent in this setting, and that MDMs can solve all problems that CoT-augmented transformers can. Moreover, we showcase classes of problems (including regular languages) for which MDMs are inherently more efficient than CoT transformers, where parallel generation allows for substantially faster reasoning. 

**Abstract (ZH)**: 掩码扩散模型（MDMs）在文本生成方面为传统的自回归语言模型提供了一个有吸引力的替代方案。并行生成使它们变得高效，但它们的计算能力和并行性固有的限制仍 largely unexplored。为此，我们 characterization 了 MDMs 能明确解决哪些推理问题及其效率。我们通过将 MDMs 与在有限精度对数宽度设置下 well-understood 的推理框架——链式思考（CoT）和补截循环变换器（PLTs）——联系起来来实现这一点：我们展示了在这一设置下，MDMs 和多项式补截的 PLTs 实际上是等效的，并且 MDMs 能解决 CoT 增强的变换器能解决的所有问题。此外，我们展示了 MDMs 比 CoT 变换器在某些问题类（包括正规语言）上更高效的类，其中并行生成允许显著更快的推理。 

---
# TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models 

**Title (ZH)**: TRUSTVIS：大型语言模型多维度可信性评估框架 

**Authors**: Ruoyu Sun, Da Song, Jiayang Song, Yuheng Huang, Lei Ma  

**Link**: [PDF](https://arxiv.org/pdf/2510.13106)  

**Abstract**: As Large Language Models (LLMs) continue to revolutionize Natural Language Processing (NLP) applications, critical concerns about their trustworthiness persist, particularly in safety and robustness. To address these challenges, we introduce TRUSTVIS, an automated evaluation framework that provides a comprehensive assessment of LLM trustworthiness. A key feature of our framework is its interactive user interface, designed to offer intuitive visualizations of trustworthiness metrics. By integrating well-known perturbation methods like AutoDAN and employing majority voting across various evaluation methods, TRUSTVIS not only provides reliable results but also makes complex evaluation processes accessible to users. Preliminary case studies on models like Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our framework in identifying safety and robustness vulnerabilities, while the interactive interface allows users to explore results in detail, empowering targeted model improvements. Video Link: this https URL 

**Abstract (ZH)**: 随着大规模语言模型（LLMs）继续革新自然语言处理（NLP）应用，其可信性问题，特别是安全性与 robustness 方面的问题，仍然引起了广泛关注。为应对这些挑战，我们引入了 TRUSTVIS，一种自动评估框架，提供全面的可信性评估。该框架的一个关键特点是其交互式用户界面，设计用于直观展示可信性指标。通过集成知名的扰动方法（如 AutoDAN）并通过多种评估方法中的多数投票来整合，TRUSTVIS 不仅提供了可靠的结果，还使复杂的评估过程对用户而言更加易于访问。初步案例研究针对如 Vicuna-7b、Llama2-7b 和 GPT-3.5 等模型证实了该框架在识别安全性和 robustness 漏洞方面的有效性，而交互式界面允许用户详细探索结果，从而推动针对性的模型改进。视频链接：这个 https URL。 

---
# ESI: Epistemic Uncertainty Quantification via Semantic-preserving Intervention for Large Language Models 

**Title (ZH)**: ESI: 基于语义保持干预的知识不确定性量化方法用于大型语言模型 

**Authors**: Mingda Li, Xinyu Li, Weinan Zhang, Longxuan Ma  

**Link**: [PDF](https://arxiv.org/pdf/2510.13103)  

**Abstract**: Uncertainty Quantification (UQ) is a promising approach to improve model reliability, yet quantifying the uncertainty of Large Language Models (LLMs) is non-trivial. In this work, we establish a connection between the uncertainty of LLMs and their invariance under semantic-preserving intervention from a causal perspective. Building on this foundation, we propose a novel grey-box uncertainty quantification method that measures the variation in model outputs before and after the semantic-preserving intervention. Through theoretical justification, we show that our method provides an effective estimate of epistemic uncertainty. Our extensive experiments, conducted across various LLMs and a variety of question-answering (QA) datasets, demonstrate that our method excels not only in terms of effectiveness but also in computational efficiency. 

**Abstract (ZH)**: 不确定性量化（UQ）是提高模型可靠性的有前途的方法，然而量化大型语言模型（LLMs）的不确定性并不 trivial。在此工作中，我们从因果角度建立了 LLMs 的不确定性与其在语义保留干预下的不变性之间的联系。在此基础上，我们提出了一种新颖的灰色盒不确定性量化方法，通过在语义保留干预前后测量模型输出的变化来评估不确定性。通过理论证明，我们展示我们的方法提供了对本质不确定性有效估计。我们广泛进行的实验，跨越了多种 LLMs 和各种问答（QA）数据集，证明了我们的方法在有效性方面不仅表现出色，在计算效率方面也表现出色。 

---
# Deliberate Lab: A Platform for Real-Time Human-AI Social Experiments 

**Title (ZH)**: Deliberate Lab: 一个人机实时社会实验平台 

**Authors**: Crystal Qian, Vivian Tsai, Michael Behr, Nada Hussein, Léo Laugier, Nithum Thain, Lucas Dixon  

**Link**: [PDF](https://arxiv.org/pdf/2510.13011)  

**Abstract**: Social and behavioral scientists increasingly aim to study how humans interact, collaborate, and make decisions alongside artificial intelligence. However, the experimental infrastructure for such work remains underdeveloped: (1) few platforms support real-time, multi-party studies at scale; (2) most deployments require bespoke engineering, limiting replicability and accessibility, and (3) existing tools do not treat AI agents as first-class participants. We present Deliberate Lab, an open-source platform for large-scale, real-time behavioral experiments that supports both human participants and large language model (LLM)-based agents. We report on a 12-month public deployment of the platform (N=88 experimenters, N=9195 experiment participants), analyzing usage patterns and workflows. Case studies and usage scenarios are aggregated from platform users, complemented by in-depth interviews with select experimenters. By lowering technical barriers and standardizing support for hybrid human-AI experimentation, Deliberate Lab expands the methodological repertoire for studying collective decision-making and human-centered AI. 

**Abstract (ZH)**: Deliberate Lab：一个支持大规模实时行为实验的开源平台，兼顾人类参与者和基于大型语言模型的代理 

---
# Developing and Validating the Arabic Version of the Attitudes Toward Large Language Models Scale 

**Title (ZH)**: 开发并验证阿拉伯版本的大规模语言模型态度量表 

**Authors**: Basad Barajeeh, Ala Yankouskaya, Sameha AlShakhsi, Chun Sing Maxwell Ho, Guandong Xu, Raian Ali  

**Link**: [PDF](https://arxiv.org/pdf/2510.13009)  

**Abstract**: As the use of large language models (LLMs) becomes increasingly global, understanding public attitudes toward these systems requires tools that are adapted to local contexts and languages. In the Arab world, LLM adoption has grown rapidly with both globally dominant platforms and regional ones like Fanar and Jais offering Arabic-specific solutions. This highlights the need for culturally and linguistically relevant scales to accurately measure attitudes toward LLMs in the region. Tools assessing attitudes toward artificial intelligence (AI) can provide a base for measuring attitudes specific to LLMs. The 5-item Attitudes Toward Artificial Intelligence (ATAI) scale, which measures two dimensions, the AI Fear and the AI Acceptance, has been recently adopted and adapted to develop new instruments in English using a sample from the UK: the Attitudes Toward General LLMs (AT-GLLM) and Attitudes Toward Primary LLM (AT-PLLM) scales. In this paper, we translate the two scales, AT-GLLM and AT-PLLM, and validate them using a sample of 249 Arabic-speaking adults. The results show that the scale, translated into Arabic, is a reliable and valid tool that can be used for the Arab population and language. Psychometric analyses confirmed a two-factor structure, strong measurement invariance across genders, and good internal reliability. The scales also demonstrated strong convergent and discriminant validity. Our scales will support research in a non-Western context, a much-needed effort to help draw a global picture of LLM perceptions, and will also facilitate localized research and policy-making in the Arab region. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）的全球使用日益普遍，理解公众对这些系统的态度需要适应当地文化和语言的工具。在阿拉伯世界，LLM的采用迅速增长，无论是全球主导平台还是区域平台如Fanar和Jais都提供了阿拉伯语特定的解决方案。这突显了在该地区准确衡量LLM态度所需的文化和语言相关量表的需求。评估人工智能（AI）态度的工具可以为评估特定于LLM的态度提供基础。包含5项的AI态度量表（ATAI），测量了两个维度——AI恐惧和AI接受， recently被采用并在英国样本中被改编为评估一般LLM（AT-GLLM）和主要LLM（AT-PLLM）态度的新工具。本文中，我们翻译了这两个量表AT-GLLM和AT-PLLM，并使用249名阿拉伯语成年样本进行了验证。结果显示，翻译后的量表是一个可靠且有效的工具，适用于阿拉伯人口和语言。心理学测量分析确认了其双因素结构、性别间的强测量不变性和良好的内部一致性。量表还展示了强的聚合效度和区分效度。我们的量表将支持非西方背景下的研究，有助于绘制全球LLM感知的图景，同时也有助于阿拉伯地区的本地化研究和政策制定。 

---
# CurLL: A Developmental Framework to Evaluate Continual Learning in Language Models 

**Title (ZH)**: CurLL：评估语言模型连续学习能力的发展性框架 

**Authors**: Pavan Kalyan, Shubhra Mishra, Satya Lokam, Navin Goyal  

**Link**: [PDF](https://arxiv.org/pdf/2510.13008)  

**Abstract**: We introduce a comprehensive continual learning dataset and benchmark (CurlL) grounded in human developmental trajectories from ages 5-10, enabling systematic and fine-grained assessment of models' ability to progressively acquire new skills. CurlL spans five developmental stages (0-4) covering ages 5-10, supported by a skill graph that breaks down broad skills into smaller abilities, concrete goals, and measurable indicators, while also capturing which abilities build on others. We generate a 23.4B-token synthetic dataset with controlled skill progression, vocabulary complexity, and format diversity, comprising paragraphs, comprehension-based QA (CQA), skill-testing QA (CSQA), and instruction-response (IR) pairs. Stage-wise token counts range from 2.12B to 6.78B tokens, supporting precise analysis of forgetting, forward transfer, and backward transfer. Using a 135M-parameter transformer trained under independent, joint, and sequential (continual) setups, we show trade-offs in skill retention and transfer efficiency. By mirroring human learning patterns and providing fine-grained control over skill dependencies, this work advances continual learning evaluations for language models. 

**Abstract (ZH)**: 我们介绍了一个基于5-10岁人类发展阶段的综合连续学习数据集和基准（CurlL），以系统性和精细地评估模型逐阶段获取新技能的能力。CurlL涵盖了5-10岁的五个发展阶段（0-4），并通过技能图将广泛的技能分解为更小的能力、具体目标和可测量指标，同时捕捉哪些能力建立在其他能力之上。我们生成了一个包含234亿词的合成数据集，具有控制的技能 progression、词汇复杂性和格式多样性，包括段落、基于理解的问答（CQA）、技能测试问答（CSQA）和指令-响应（IR）对。各阶段的词计数范围从21.2亿到67.8亿词，支持对遗忘、前向迁移和后向迁移的精确分析。通过在独立、联合和序列（连续）设置下训练的1.35亿参数变换器，我们展示了技能保持和迁移效率之间的权衡。通过镜像人类学习模式并提供对技能依赖关系的精细控制，本作品推动了语言模型连续学习评估的发展。 

---
# Max It or Miss It: Benchmarking LLM On Solving Extremal Problems 

**Title (ZH)**: 把握或错过：评估大语言模型解决极值问题的能力 

**Authors**: Binxin Gao, Jingjun Han  

**Link**: [PDF](https://arxiv.org/pdf/2510.12997)  

**Abstract**: Test-time scaling has enabled Large Language Models (LLMs) with remarkable reasoning capabilities, particularly in mathematical domains, through intermediate chain-of-thought (CoT) reasoning before generating final answers. However, the specific sources and mechanisms underlying these reasoning capabilities remain insufficiently understood. Optimization reasoning, i.e. finding extrema under constraints, represents a fundamental abstraction that underpins critical applications in planning, control, resource allocation, and prompt search. To systematically evaluate this capability, we introduce ExtremBench, a benchmark dataset for solving mathematical extremal problems, curated from inequality exercises used for Chinese Mathematical Olympiad and transformed into $93$ standardized extrema-finding problems. We conduct extensive evaluations across various state-of-the-art open-source model families, including the Qwen3, GPT-OSS, and DeepSeek. Our results reveal that LLMs' extremal-solving reasoning capabilities do not always align with those of current mathematical benchmarks such as AIME25 and MATH-500, with some models showing strong general mathematical reasoning but poor extremal-solving skills, and vice versa. This discrepancy highlights a critical gap in current evaluation practices and suggests that existing benchmarks may not comprehensively capture the full spectrum of mathematical reasoning abilities. 

**Abstract (ZH)**: Test-time 扩容使大语言模型（LLMs）在数学领域等通过中间的逐步推理（CoT）获得显著的推理能力，但在这些推理能力的具体来源和机制方面仍缺乏足够的理解。优化推理，即在约束条件下求极值，是规划、控制、资源分配和提示搜索等关键应用的基础抽象。为了系统评估这一能力，我们引入了 ExtremBench，这是一个来自中国数学奥林匹克不等式练习的标准化极值寻找问题基准数据集。我们在各种最先进的开源模型家族中进行了广泛的评估，包括Qwen3、GPT-OSS和DeepSeek。我们的结果表明，LLMs在解决极值问题的推理能力并不总是与当前的数学基准如AIME25和MATH-500保持一致，一些模型在一般数学推理方面表现强劲但在极值求解方面表现较差，反之亦然。这种差异凸显了当前评估实践中的一个关键差距，并暗示现有的基准可能未能全面捕捉数学推理能力的完整谱系。 

---
# KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems 

**Title (ZH)**: KVCOMM: 在线跨上下文键值缓存通信以提高基于LLM的多agents系统的效率 

**Authors**: Hancheng Ye, Zhengqi Gao, Mingyuan Ma, Qinsi Wang, Yuzhe Fu, Ming-Yu Chung, Yueqian Lin, Zhijian Liu, Jianyi Zhang, Danyang Zhuo, Yiran Chen  

**Link**: [PDF](https://arxiv.org/pdf/2510.12872)  

**Abstract**: Multi-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typical pipelines, once an agent receives a message from its predecessor, the full context-including prior turns-must be reprocessed from scratch, leading to inefficient processing. While key-value (KV) caching is an effective solution for avoiding redundant computation in single-agent settings where prefixes remain unchanged, it cannot be directly reused in multi-agent scenarios due to diverging prefixes introduced by agent-specific context extensions. We identify that the core challenge lies in the offset variance of KV-caches across agents. To address this, we propose KVCOMM, a training-free framework that enables efficient prefilling in multi-agent inference by reusing KV-caches and aligning cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM estimates and adjusts KV-caches for shared content by referencing a pool of cached examples-termed anchors-that store observed cache deviations under varying prefixes. The anchor pool is maintained and updated online, allowing dynamic adaptation to distinct user requests and context structures. KVCOMM achieves over 70% reuse rate across diverse multi-agent workloads, including retrieval-augmented generation, math reasoning, and collaborative coding tasks, all without quality degradation. Particularly, when each fully-connected agent receives 1K input tokens with 512 prefix tokens and 512 output tokens under a five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard prefill pipeline, reducing TTFT from ~430 ms to ~55 ms. 

**Abstract (ZH)**: 多智能体大型语言模型（LLM）系统在需要智能体之间通信和协调的复杂语言处理任务中日益被采用。然而，这些系统常常因智能体之间重复处理重叠上下文而面临大量开销。在典型的工作流中，一旦一个智能体接收到前一个智能体的消息，整个上下文（包括之前轮次的内容）必须从头开始重新处理，导致不高效的处理。虽然在单智能体场景中，通过对不变前缀利用键值（KV）缓存可有效避免重复计算，但在智能体特定上下文扩展导致前缀变化的情况下，这一方法无法直接复用。我们发现核心挑战在于智能体之间KV缓存偏移量的差异。为解决这一问题，我们提出了KVCOMM，这是一种无需训练的框架，通过复用KV缓存并在各种前缀上下文中对重叠上下文的缓存偏移量进行对齐，实现多智能体推理中的高效预填充。KVCOMM通过参考存储不同前缀下观察到缓存偏差的缓存示例池（称为锚点）来估计和调整共享内容的缓存。该锚点池在线维护和更新，允许动态适应不同的用户请求和上下文结构。KVCOMM在包括检索增强生成、数学推理和协作编程等一系列多智能体工作负载中实现了超过70%的缓存复用率，且无质量下降。特别是在五智能体设定中，每当每个全连接智能体接收到1024个输入令牌（512个前缀令牌和512个输出令牌），KVCOMM与标准预填充流水线相比可获得高达7.8倍的加速，将TTFT从约430毫秒降低到约55毫秒。 

---
# Adaptive Generation of Bias-Eliciting Questions for LLMs 

**Title (ZH)**: 适应性生成引发偏差问题的对话生成方法 

**Authors**: Robin Staab, Jasper Dekoninck, Maximilian Baader, Martin Vechev  

**Link**: [PDF](https://arxiv.org/pdf/2510.12857)  

**Abstract**: Large language models (LLMs) are now widely deployed in user-facing applications, reaching hundreds of millions worldwide. As they become integrated into everyday tasks, growing reliance on their outputs raises significant concerns. In particular, users may unknowingly be exposed to model-inherent biases that systematically disadvantage or stereotype certain groups. However, existing bias benchmarks continue to rely on templated prompts or restrictive multiple-choice questions that are suggestive, simplistic, and fail to capture the complexity of real-world user interactions. In this work, we address this gap by introducing a counterfactual bias evaluation framework that automatically generates realistic, open-ended questions over sensitive attributes such as sex, race, or religion. By iteratively mutating and selecting bias-inducing questions, our approach systematically explores areas where models are most susceptible to biased behavior. Beyond detecting harmful biases, we also capture distinct response dimensions that are increasingly relevant in user interactions, such as asymmetric refusals and explicit acknowledgment of bias. Leveraging our framework, we construct CAB, a human-verified benchmark spanning diverse topics, designed to enable cross-model comparisons. Using CAB, we analyze a range of LLMs across multiple bias dimensions, revealing nuanced insights into how different models manifest bias. For instance, while GPT-5 outperforms other models, it nonetheless exhibits persistent biases in specific scenarios. These findings underscore the need for continual improvements to ensure fair model behavior. 

**Abstract (ZH)**: 大型语言模型（LLMs）现在广泛应用于面向用户的应用程序中，全球范围内已有数亿用户在使用。随着它们被整合到日常任务中，对它们输出结果的依赖增长，引发了重大关切。特别是用户可能无意中暴露于模型固有的偏见中，系统性地对某些群体造成不利影响或刻板印象。然而，现有的偏见基准仍依赖于模板化的提示或限制性选择题，这些题目暗示性强、简单化且未能捕捉到现实世界用户交互的复杂性。在此项工作中，我们通过引入一种反事实偏见评估框架来填补这一空白，该框架能够自动生成关于敏感属性（如性别、种族或宗教）的真实且开放性问题。通过迭代地变异和选择诱导偏见的问题，我们的方法系统性地探索模型最容易表现出偏见行为的区域。除了检测有害偏见外，我们还捕捉到在用户交互中越来越重要的反应维度，如不对称拒绝和明确承认偏见。利用我们的框架，我们构建了CAB基准，这是一个由人工验证覆盖广泛主题的基准，旨在实现跨模型比较。使用CAB，我们在多个偏见维度上分析了一系列LLM，揭示出不同模型表现偏见的复杂洞察。例如，虽然GPT-5表现优于其他模型，但在特定场景中仍表现出持久的偏见。这些发现强调了持续改进以确保公平模型行为的必要性。 

---
# FaStFACT: Faster, Stronger Long-Form Factuality Evaluations in LLMs 

**Title (ZH)**: FaStFACT: 更快速更强的长文本事实性评估方法 

**Authors**: Yingjia Wan, Haochen Tan, Xiao Zhu, Xinyu Zhou, Zhiwei Li, Qingsong Lv, Changxuan Sun, Jiaqi Zeng, Yi Xu, Jianqiao Lu, Yinhong Liu, Zhijiang Guo  

**Link**: [PDF](https://arxiv.org/pdf/2510.12839)  

**Abstract**: Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to accuracy issues and costly human assessment. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawbacks: (1) inefficiency due to complex pipeline components unsuitable for long LLM outputs, and (2) ineffectiveness stemming from inaccurate claim sets and insufficient evidence collection of one-line snippets.
To address these limitations, we propose \name, a fast and strong evaluation framework that achieves the highest alignment with human evaluation and efficiency among existing baselines. \name first employs chunk-level claim extraction integrated with confidence-based pre-verification, significantly reducing the cost of web searching and inference calling while ensuring reliability. For searching and verification, it collects document-level evidence from crawled webpages and selectively retrieves it during verification, addressing the evidence insufficiency problem in previous pipelines.
Extensive experiments based on an aggregated and manually annotated benchmark demonstrate the reliability of \name in both efficiently and effectively evaluating the factuality of long-form LLM generations. Code and benchmark data is available at this https URL. 

**Abstract (ZH)**: 评估大型语言模型（LLMs）生成的长文内容的事实性仍具有挑战性，主要由于准确性问题和高昂的人工评估成本。先前的努力试图通过分解文本为断言、搜索证据和验证断言来实现这一目标，但存在关键缺陷：（1）由于复杂的流水线组件不适合长LLM输出，导致效率低下；（2）由于断言集不准确和单行片段证据收集不足，导致效果不佳。

为解决这些局限性，我们提出了一种名为\name的快速且强大的评估框架，能够在现有基线中实现与人工评估最高的一致性和效率。\name首先采用基于片段级别的断言提取，并结合基于信心的预验证，大幅降低了网络搜索和推理调用的成本，同时确保可靠性。在搜索和验证阶段，它从抓取的网页中收集文档级别的证据，并在验证过程中选择性地检索，解决了先前流水线中的证据不足问题。

基于汇总和手动标注的基准进行的大量实验表明，\name能有效地和高效地评估长文LLM生成内容的事实性。代码和基准数据可在以下链接获取。 

---
# A\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning 

**Title (ZH)**: A\textsuperscript{2}FM：一种适应性代理基础模型，用于工具感知混合推理 

**Authors**: Qianben Chen, Jingyi Cao, Jiayu Zhang, Tianrui Qin, Xiaowan Li, King Zhu, Dingfeng Shi, He Zhu, Minghao Liu, Xiaobo Liang, Ge Zhang, Jian Yang, Yuchen Eleanor Jiang, Wangchunshu Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2510.12838)  

**Abstract**: Large language models split into two families: reasoning-centric LLMs, which strengthen internal chain-of-thought reasoning but cannot invoke external tools, and agentic LLMs, which learn to interact with environments and leverage tools but often lag in deep reasoning. This divide arises from fundamentally different training objectives, leading to mismatched strengths and inefficiency on simple queries, where both families tend to overthink or over-call tools. In this work, we present Adaptive Agent Foundation Model (A\textsuperscript{2}FM), a unified framework that follows a route-then-align principle: the model first learns task-aware routing and then aligns mode-specific trajectories under a shared backbone. To address the inefficiency gap, we introduce a third mode-instant-that handles simple queries directly, preventing unnecessary reasoning or tool calls while complementing the agentic and reasoning modes. To jointly enhance accuracy and efficiency, we propose Adaptive Policy Optimization (APO), which enforces adaptive sampling across modes and applies a cost-regularized reward. On the 32B scale, A\textsuperscript{2}FM achieves 13.4\% on BrowseComp, 70.4\% on AIME25, and 16.7\% on HLE, setting new SOTA among comparable models and performing competitively with frontier LLMs across agentic, reasoning, and general benchmarks. Notably, the adaptive execution achieves a cost of pass of only \$0.00487 per correct answer-cutting cost by 45.2\% relative to reasoning and 33.5\% relative to agentic, thus delivering substantially higher cost efficiency while maintaining comparable accuracy. 

**Abstract (ZH)**: 大语言模型分为两类：以逻辑推理为中心的大语言模型，这类模型强化内部链式推理但无法调用外部工具；以及能与环境互动并利用工具的代理模型，但后者在深入推理方面往往表现欠佳。这种分歧源于根本不同的训练目标，导致在处理简单查询时出现匹配度不足和低效问题，使得两类模型往往过度推理或过度调用工具。为此，我们提出了自适应代理基础模型（A²FM），这是一种遵循路由-对齐原则的统一框架。为解决低效问题，我们引入了一种新的模式——即时模式，专门处理简单查询，避免不必要的推理或工具调用，同时补充代理和逻辑推理模式的功能。为共同提升准确性和效率，我们提出了自适应策略优化（APO），该方法在不同模式之间实现自适应采样，并应用成本正则化奖励。在32B规模下，A²FM在BrowseComp上取得13.4%的成绩，在AIME25上取得70.4%的成绩，在HLE上取得16.7%的成绩，成为同类模型中的新SOTA，并且在代理、逻辑推理和通用基准测试中表现出色。值得注意的是，自适应执行仅需每正确答案0.00487美元的代价，相比逻辑推理减少45.2%，相比代理减少33.5%，从而在保持相似准确性的基础上提供了显著更高的成本效率。 

---
# Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study 

**Title (ZH)**: 重新利用标注指南来指导LLM标注者：一个案例研究 

**Authors**: Kon Woo Kim, Rezarta Islamaj, Jin-Dong Kim, Florian Boudin, Akiko Aizawa  

**Link**: [PDF](https://arxiv.org/pdf/2510.12835)  

**Abstract**: This study investigates how existing annotation guidelines can be repurposed to instruct large language model (LLM) annotators for text annotation tasks. Traditional guidelines are written for human annotators who internalize training, while LLMs require explicit, structured instructions. We propose a moderation-oriented guideline repurposing method that transforms guidelines into clear directives for LLMs through an LLM moderation process. Using the NCBI Disease Corpus as a case study, our experiments show that repurposed guidelines can effectively guide LLM annotators, while revealing several practical challenges. The results highlight the potential of this workflow to support scalable and cost-effective refinement of annotation guidelines and automated annotation. 

**Abstract (ZH)**: 本研究探讨如何将现有的标注指南重新利用以指导大型语言模型（LLM）标注员进行文本标注任务。传统的指南是为人类标注员设计的，他们需要内化培训，而LLMs则需要明确、结构化的指导。我们提出了一种以调节为导向的指南重新利用方法，通过LLM调节过程将指南转换为对LLMs的明确指令。以NCBI疾病语料库为例，我们的实验表明重新利用的指南能够有效地指导LLM标注员，同时揭示了几种实际挑战。结果强调了该工作流程支持标注指南和自动化标注可扩展且成本效益高的潜在能力。 

---
# Mathematics with large language models as provers and verifiers 

**Title (ZH)**: 使用大型语言模型作为证明者和验证者的关系学 

**Authors**: Hieu Le Duc, Leo Liberti  

**Link**: [PDF](https://arxiv.org/pdf/2510.12829)  

**Abstract**: During 2024 and 2025 the discussion about the theorem-proving capabilities of large language models started reporting interesting success stories, mostly to do with difficult exercises (such as problems from the International Mathematical Olympiad), but also with conjectures [Feldman & Karbasi, arXiv:2509.18383v1] formulated for the purpose of verifying whether the artificial intelligence could prove it. In this paper we report a theorem proving feat achieved by ChatGPT by using a protocol involving different prover and verifier instances of the gpt-5 model working collaboratively. To make sure that the produced proofs do not suffer from hallucinations, the final proof is formally verified by the lean proof assistant, and the conformance of premises and conclusion of the lean code is verified by a human. Our methodology was able to solve five out of six 2025 IMO problems, and close a third of the sixty-six number theory conjectures in [Cohen, Journal of Integer Sequences, 2025]. 

**Abstract (ZH)**: 2024至2025年间，关于大型语言模型的定理证明能力的讨论报告了一系列有趣的成功案例，主要是解决国际数学奥林匹克难题，以及验证人工智能能否证明猜想的问题[Feldman & Karbasi, arXiv:2509.18383v1]。本文报告了ChatGPT通过不同实例化的gpt-5模型协作实现的定理证明成就。为确保生成的证明不出现幻觉，最终证明由lean证明助手正式验证，且lean代码的前提与结论的一致性由人类验证。我们的方法能够解决2025年国际数学奥林匹克五分之六的题目，并解决了[Cohen, Journal of Integer Sequences, 2025]中六十六个数论猜想的三分之一。 

---
# Scheming Ability in LLM-to-LLM Strategic Interactions 

**Title (ZH)**: LLM之间 strategic互动中的方案能力 

**Authors**: Thao Pham  

**Link**: [PDF](https://arxiv.org/pdf/2510.12826)  

**Abstract**: As large language model (LLM) agents are deployed autonomously in diverse contexts, evaluating their capacity for strategic deception becomes crucial. While recent research has examined how AI systems scheme against human developers, LLM-to-LLM scheming remains underexplored. We investigate the scheming ability and propensity of frontier LLM agents through two game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro, Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and without explicit prompting while analyzing scheming tactics through chain-of-thought reasoning. When prompted, most models, especially Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance. Critically, models exhibited significant scheming propensity without prompting: all models chose deception over confession in Peer Evaluation (100% rate), while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These findings highlight the need for robust evaluations using high-stakes game-theoretic scenarios in multi-agent settings. 

**Abstract (ZH)**: 随着大型语言模型（LLM）代理在多种情境下自主部署，评估其进行战略欺骗的能力变得至关重要。虽然近期研究探讨了AI系统如何欺骗人类开发者，但LLM间的欺骗行为仍缺乏充分研究。我们通过两种博弈论框架——廉价交谈信号博弈和同侪评估对抗博弈，考察了前沿LLM代理的欺骗能力和倾向。测试了四个模型（GPT-4o、Gemini-2.5-pro、Claude-3.7-Sonnet和Llama-3.3-70b），并在有无明确提示的情况下衡量其欺骗性能，并通过链式推理分析了欺骗策略。当有提示时，大多数模型，尤其是Gemini-2.5-pro和Claude-3.7-Sonnet，达到了近乎完美的表现。关键的是，在无提示的情况下，模型显示出显著的欺骗倾向：所有模型在同侪评估中选择了欺骗而非坦白（100%的比例），而在廉价交谈博弈中选择欺骗的模型成功率为95-100%。这些发现强调了在多代理环境中采用高风险博弈论情景进行稳健评估的必要性。 

---
# MEDEQUALQA: Evaluating Biases in LLMs with Counterfactual Reasoning 

**Title (ZH)**: MEDEQUALQA：基于反事实推理评估LLMs中的偏见 

**Authors**: Rajarshi Ghosh, Abhay Gupta, Hudson McBride, Anurag Vaidya, Faisal Mahmood  

**Link**: [PDF](https://arxiv.org/pdf/2510.12818)  

**Abstract**: Large language models (LLMs) are increasingly deployed in clinical decision support, yet subtle demographic cues can influence their reasoning. Prior work has documented disparities in outputs across patient groups, but little is known about how internal reasoning shifts under controlled demographic changes. We introduce MEDEQUALQA, a counterfactual benchmark that perturbs only patient pronouns (he/him, she/her, they/them) while holding critical symptoms and conditions (CSCs) constant. Each clinical vignette is expanded into single-CSC ablations, producing three parallel datasets of approximately 23,000 items each (69,000 total). We evaluate a GPT-4.1 model and compute Semantic Textual Similarity (STS) between reasoning traces to measure stability across pronoun variants. Our results show overall high similarity (mean STS >0.80), but reveal consistent localized divergences in cited risk factors, guideline anchors, and differential ordering, even when final diagnoses remain unchanged. Our error analysis highlights certain cases in which the reasoning shifts, underscoring clinically relevant bias loci that may cascade into inequitable care. MEDEQUALQA offers a controlled diagnostic setting for auditing reasoning stability in medical AI. 

**Abstract (ZH)**: 大型语言模型（LLMs）在临床决策支持中的应用日益增多，但细微的种族和社会经济差异会对其推理产生影响。已有研究记录了不同患者群体间输出的差异，但对可控种族变化下内部推理的变化知之甚少。我们引入了MEDEQUALQA这一反事实基准，仅改变患者代词（he/him, she/her, they/them），同时保持关键症状和条件（CSCs）不变。每个临床病例扩展为单个CSC的消融分析，生成约23,000个条目（总共约69,000个条目）的三个并行数据集。我们评估了一种GPT-4.1模型，并计算语义文本相似性（STS）以衡量不同代词变体之间推理轨迹的稳定性。结果显示整体相似度很高（平均STS >0.80），但在引用的风险因素、指南锚点和差异排序方面发现了一致的局部差异，即使最终诊断未变。我们的错误分析强调了某些推理转变的具体案例，突显出可能导致不平等医疗服务的相关临床偏见。MEDEQUALQA提供了一个受控诊断环境，用于审计医疗AI中的推理稳定性。 

---
# From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP 

**Title (ZH)**: 从噪声到信号再到自律目标：重塑后训练时代NLP中的human标签变异 

**Authors**: Shanshan Xu, Santosh T.Y.S.S, Barbara Plank  

**Link**: [PDF](https://arxiv.org/pdf/2510.12817)  

**Abstract**: Human Label Variation (HLV) refers to legitimate disagreement in annotation that reflects the genuine diversity of human perspectives rather than mere error. For decades, HLV in NLP was dismissed as noise to be discarded, and only slowly over the last decade has it been reframed as a signal for improving model robustness. With the rise of large language models (LLMs), where post-training on human feedback has become central to model alignment, the role of HLV has become increasingly consequential. Yet current preference-learning datasets routinely aggregate multiple annotations into a single label, thereby flattening diverse perspectives into a false universal agreement and erasing precisely the pluralism of human values that alignment aims to preserve. In this position paper, we argue that preserving HLV as an embodiment of human pluralism must be treated as a Selbstzweck - a goal it self when designing AI systems. We call for proactively incorporating HLV into preference datasets and outline actionable steps towards it. 

**Abstract (ZH)**: 人类标注变异（HLV）反映了真实的人类视角多样性而非单纯的错误，而非噪声应被忽略。随着大型语言模型（LLMs）的发展，通过人类反馈进行后续训练已成为模型对齐的核心，HLV的作用日益重要。然而，当前的偏好学习数据集通常将多个标注聚合为单一标签，从而抹杀了人类价值观的多样性，这与对齐的目标背道而驰。在此立场论文中，我们主张将HLV作为一种自我目标——即设计AI系统的目标本身——予以保留。我们呼吁积极将HLV纳入偏好数据集，并概述了相关行动步骤。 

---
# Cancer Diagnosis Categorization in Electronic Health Records Using Large Language Models and BioBERT: Model Performance Evaluation Study 

**Title (ZH)**: 使用大型语言模型和BioBERT在电子健康记录中对癌症诊断进行分类：模型性能评估研究 

**Authors**: Soheil Hashtarkhani, Rezaur Rashid, Christopher L Brett, Lokesh Chinthala, Fekede Asefa Kumsa, Janet A Zink, Robert L Davis, David L Schwartz, Arash Shaban-Nejad  

**Link**: [PDF](https://arxiv.org/pdf/2510.12813)  

**Abstract**: Electronic health records contain inconsistently structured or free-text data, requiring efficient preprocessing to enable predictive health care models. Although artificial intelligence-driven natural language processing tools show promise for automating diagnosis classification, their comparative performance and clinical reliability require systematic evaluation. The aim of this study is to evaluate the performance of 4 large language models (GPT-3.5, GPT-4o, Llama 3.2, and Gemini 1.5) and BioBERT in classifying cancer diagnoses from structured and unstructured electronic health records data. We analyzed 762 unique diagnoses (326 International Classification of Diseases (ICD) code descriptions, 436free-text entries) from 3456 records of patients with cancer. Models were tested on their ability to categorize diagnoses into 14predefined categories. Two oncology experts validated classifications. BioBERT achieved the highest weighted macro F1-score for ICD codes (84.2) and matched GPT-4o in ICD code accuracy (90.8). For free-text diagnoses, GPT-4o outperformed BioBERT in weighted macro F1-score (71.8 vs 61.5) and achieved slightly higher accuracy (81.9 vs 81.6). GPT-3.5, Gemini, and Llama showed lower overall performance on both formats. Common misclassification patterns included confusion between metastasis and central nervous system tumors, as well as errors involving ambiguous or overlapping clinical terminology. Although current performance levels appear sufficient for administrative and research use, reliable clinical applications will require standardized documentation practices alongside robust human oversight for high-stakes decision-making. 

**Abstract (ZH)**: 电子健康记录中包含结构不一致或自由文本数据，需要高效的预处理以促进预测型医疗模型的应用。尽管基于人工智能的自然语言处理工具在自动化诊断分类方面显示出潜力，但其相对性能和临床可靠性仍需系统评估。本研究旨在评估4种大型语言模型（GPT-3.5、GPT-4o、Llama 3.2和Gemini 1.5）和BioBERT在结构化和非结构化电子健康记录数据中分类癌症诊断的表现。我们分析了3456名癌症患者的数据记录中共762个独特诊断（包括326个国际疾病分类代码描述和436个自由文本条目）。模型测试了其将诊断分类到14个预定义类别中的能力。两位肿瘤专家对分类结果进行了验证。BioBERT在国际疾病分类代码宏F1分数中表现最高（84.2），并与GPT-4o在国际疾病分类代码准确性上相当（90.8）。对于自由文本诊断，GPT-4o在宏F1分数上优于BioBERT（71.8比61.5），并在准确性上略有提高（81.9比81.6）。GPT-3.5、Gemini和Llama在这两种格式上的整体性能较低。常见的分类错误包括对转移瘤和中枢神经系统肿瘤的混淆，以及涉及含糊不清或重叠的临床术语的错误。尽管当前性能水平似乎足以用于行政和研究用途，但可靠的临床应用仍需要标准化的记录实践和坚实的人员监督，特别是在高风险决策中。 

---
# Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and Few-Shot Learning 

**Title (ZH)**: 开源大型语言模型在波斯语零样本和少样本学习中的基准测试 

**Authors**: Mahdi Cherakhloo, Arash Abbasi, Mohammad Saeid Sarafraz, Bijan Vosoughi Vahdat  

**Link**: [PDF](https://arxiv.org/pdf/2510.12807)  

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous languages; however, their effectiveness in low-resource languages like Persian requires thorough investigation. This paper presents a comprehensive benchmark of several open-source LLMs for Persian Natural Language Processing (NLP) tasks, utilizing both zero-shot and few-shot learning paradigms. We evaluate models across a range of tasks including sentiment analysis, named entity recognition, reading comprehension, and question answering, using established Persian datasets such as ParsiNLU and ArmanEmo. Our methodology encompasses rigorous experimental setups for both zero-shot and few-shot scenarios, employing metrics such as Accuracy, F1-score, BLEU, and ROUGE for performance evaluation. The results reveal that Gemma 2 consistently outperforms other models across nearly all tasks in both learning paradigms, with particularly strong performance in complex reasoning tasks. However, most models struggle with token-level understanding tasks like Named Entity Recognition, highlighting specific challenges in Persian language processing. This study contributes to the growing body of research on multilingual LLMs, providing valuable insights into their performance in Persian and offering a benchmark for future model development. 

**Abstract (ZH)**: 大型语言模型（LLMs）在多种语言中展现了显著的能力；然而，其在低资源语言如波斯语中的效果需要进行彻底的研究。本文旨在利用零样本和少样本学习范式，对几种开源波斯语自然语言处理（NLP）任务的大型语言模型进行全面基准测试。我们使用如ParsiNLU和ArmanEmo等已建立的波斯语数据集，对模型在情感分析、命名实体识别、阅读理解和问答等任务中的表现进行了评估。我们的方法包括对零样本和少样本场景的严格实验设置，并采用准确率、F1分数、BLEU和ROUGE等指标进行性能评估。结果显示，Gemma 2在两种学习范式下的几乎所有任务中表现始终优于其他模型，尤其在复杂推理任务中表现尤为突出。然而，大多数模型在命名实体识别等标记级理解任务中表现不佳，突显了波斯语处理中的特定挑战。本研究为多语言大型语言模型的研究贡献力量，提供了有关波斯语中模型性能的有价值见解，并为未来模型的发展提供了一个基准。 

---
# AutoCode: LLMs as Problem Setters for Competitive Programming 

**Title (ZH)**: AutoCode: LLMs作为竞赛编程的问题设定者 

**Authors**: Shang Zhou, Zihan Zheng, Kaiyuan Liu, Zeyu Shen, Zerui Cheng, Zexing Chen, Hansen He, Jianzhu Yao, Huanzhi Mao, Qiuyang Mang, Tianfu Fu, Beichen Li, Dongruixuan Li, Wenhao Chai, Zhuang Liu, Aleksandra Korolova, Peter Henderson, Natasha Jaques, Pramod Viswanath, Saining Xie, Jingbo Shang  

**Link**: [PDF](https://arxiv.org/pdf/2510.12803)  

**Abstract**: Writing competitive programming problems is exacting. Authors must: set constraints, input distributions, and edge cases that rule out shortcuts; target specific algorithms (e.g., max-flow, dynamic programming, data structures); and calibrate complexity beyond the reach of most competitors. We argue that this makes for an ideal test of general large language model capabilities and study whether they can do this reliably. We introduce AutoCode, which uses multiple rounds of validation to yield competition-grade problem statements and test cases. On held-out problems, AutoCode test suites approach 99% consistency with official judgments, a significant improvement over current state-of-the-art methods like HardTests, which achieve less than 81%. Furthermore, starting with a random seed problem, AutoCode can create novel variants with reference and brute-force solutions. By cross-verifying these generated solutions against test cases, we can further filter out malformed problems. Our system ensures high correctness, as verified by human experts. AutoCode successfully produces novel problems judged by Grandmaster-level (top 0.3%) competitive programmers to be of contest quality. 

**Abstract (ZH)**: 编写编程竞赛问题极具挑战性。作者必须设定约束条件、输入分布和极端情况，以排除捷径；针对特定算法（例如最大流、动态规划、数据结构）进行目标设定；并调整复杂度使其超出大多数竞争对手的能力范围。我们认为，这为通用大型语言模型的能力提供了一个理想的测试，并研究它们能否可靠地完成这一任务。我们引入了AutoCode，这是一款利用多轮验证生成竞赛级问题陈述和测试用例的工具。在保留的数据集上，AutoCode测试套件的一致性达到99%，远超当前最先进方法如HardTests（一致性小于81%）的水平。此外，从一个随机种子问题开始，AutoCode可以生成具有参考解和暴力解的新颖变体。通过交叉验证这些生成的解与测试用例，我们可以进一步过滤掉不规范的问题。我们的系统通过人类专家的验证确保了高正确性。AutoCode成功生成了由顶级（前0.3%） competitive程序员评定为竞赛质量的新颖问题。 

---
