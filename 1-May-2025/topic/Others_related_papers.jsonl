{'arxiv_id': 'arXiv:2504.21486', 'title': 'Provably-Safe, Online System Identification', 'authors': 'Bohao Zhang, Zichang Zhou, Ram Vasudevan', 'link': 'https://arxiv.org/abs/2504.21486', 'abstract': "Precise manipulation tasks require accurate knowledge of payload inertial parameters. Unfortunately, identifying these parameters for unknown payloads while ensuring that the robotic system satisfies its input and state constraints while avoiding collisions with the environment remains a significant challenge. This paper presents an integrated framework that enables robotic manipulators to safely and automatically identify payload parameters while maintaining operational safety guarantees. The framework consists of two synergistic components: an online trajectory planning and control framework that generates provably-safe exciting trajectories for system identification that can be tracked while respecting robot constraints and avoiding obstacles and a robust system identification method that computes rigorous overapproximative bounds on end-effector inertial parameters assuming bounded sensor noise. Experimental validation on a robotic manipulator performing challenging tasks with various unknown payloads demonstrates the framework's effectiveness in establishing accurate parameter bounds while maintaining safety throughout the identification process. The code is available at our project webpage: this https URL.", 'abstract_zh': '精确操作任务需要准确的负载惯性参数知识。不幸的是，对于未知负载，同时确保机器人系统满足其输入和状态约束并避免与环境发生碰撞以识别这些参数仍然是一项重大的挑战。本文提出了一种集成框架，使机器人操作器能够安全地自动识别负载参数，同时保持操作安全性保证。该框架由两个协同工作的组件组成：一个在线轨迹规划与控制框架，生成可证明安全的激发轨迹进行系统识别，同时遵守机器人约束并避开障碍物；以及一种鲁棒的系统识别方法，基于有界传感器噪声计算末端执行器惯性参数的严谨包络上界。在各种未知负载下执行具有挑战性的任务的机器人操作器实验验证表明，该框架在识别过程中保持准确的参数边界的同时确保安全性。代码可在我们的项目网页上获取：this https URL。', 'title_zh': '可证明安全的在线系统辨识'}
{'arxiv_id': 'arXiv:2504.21158', 'title': 'Composite Safety Potential Field for Highway Driving Risk Assessment', 'authors': 'Dachuan Zuo, Zilin Bian, Fan Zuo, Kaan Ozbay', 'link': 'https://arxiv.org/abs/2504.21158', 'abstract': "In the era of rapid advancements in vehicle safety technologies, driving risk assessment has become a focal point of attention. Technologies such as collision warning systems, advanced driver assistance systems (ADAS), and autonomous driving require driving risks to be evaluated proactively and in real time. To be effective, driving risk assessment metrics must not only accurately identify potential collisions but also exhibit human-like reasoning to enable safe and seamless interactions between vehicles. Existing safety potential field models assess driving risks by considering both objective and subjective safety factors. However, their practical applicability in real-world risk assessment tasks is limited. These models are often challenging to calibrate due to the arbitrary nature of their structures, and calibration can be inefficient because of the scarcity of accident statistics. Additionally, they struggle to generalize across both longitudinal and lateral risks. To address these challenges, we propose a composite safety potential field framework, namely C-SPF, involving a subjective field to capture drivers' risk perception about spatial proximity and an objective field to quantify the imminent collision probability, to comprehensively evaluate driving risks. The C-SPF is calibrated using abundant two-dimensional spacing data from trajectory datasets, enabling it to effectively capture drivers' proximity risk perception and provide a more realistic explanation of driving behaviors. Analysis of a naturalistic driving dataset demonstrates that the C-SPF can capture both longitudinal and lateral risks that trigger drivers' safety maneuvers. Further case studies highlight the C-SPF's ability to explain lateral driver behaviors, such as abandoning lane changes or adjusting lateral position relative to adjacent vehicles, which are capabilities that existing models fail to achieve.", 'abstract_zh': '在车辆安全技术飞速发展的时代，驾驶风险评估已成为关注的焦点。现有的安全潜在领域模型通过考虑客观和主观的安全因素来评估驾驶风险，但由于其结构的任意性和事故统计数据的稀缺性，这些模型在实际风险评估任务中的应用受到限制，且难以泛化到纵向和横向风险。为此，我们提出了一种复合安全潜在领域框架C-SPF，该框架结合了主观领域来捕捉驾驶者对空间接近性的风险感知和客观领域来量化即将发生碰撞的概率，以全面评估驾驶风险。C-SPF通过使用轨迹数据集中的丰富二维间距数据进行校准，能够有效捕捉驾驶者对接近性的风险感知，并提供更现实的驾驶行为解释。对自然驾驶数据集的分析表明，C-SPF能够捕捉到触发驾驶者安全行为的纵向和横向风险。进一步的案例研究还突显了C-SPF解释横向驾驶行为（如放弃车道变更或调整相对于相邻车辆的位置）的能力，这是现有模型无法实现的。', 'title_zh': '高速公路驾驶风险评估的复合安全势场方法'}
{'arxiv_id': 'arXiv:2504.21022', 'title': 'ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees', 'authors': 'Jun Wang, David Smith Sundarsingh, Jyotirmoy V. Deshmukh, Yiannis Kantaros', 'link': 'https://arxiv.org/abs/2504.21022', 'abstract': 'Linear Temporal Logic (LTL) has become a prevalent specification language for robotic tasks. To mitigate the significant manual effort and expertise required to define LTL-encoded tasks, several methods have been proposed for translating Natural Language (NL) instructions into LTL formulas, which, however, lack correctness guarantees. To address this, we introduce a new NL-to-LTL translation method, called ConformalNL2LTL, that can achieve user-defined translation success rates over unseen NL commands. Our method constructs LTL formulas iteratively by addressing a sequence of open-vocabulary Question-Answering (QA) problems with LLMs. To enable uncertainty-aware translation, we leverage conformal prediction (CP), a distribution-free uncertainty quantification tool for black-box models. CP enables our method to assess the uncertainty in LLM-generated answers, allowing it to proceed with translation when sufficiently confident and request help otherwise. We provide both theoretical and empirical results demonstrating that ConformalNL2LTL achieves user-specified translation accuracy while minimizing help rates.', 'abstract_zh': '一种新的不确定导向的自然语言到线性时序逻辑的翻译方法：ConformalNL2LTL', 'title_zh': 'ConformalNL2LTL: 将自然语言指令转化为具有符合正确性保证的时间逻辑公式'}
{'arxiv_id': 'arXiv:2504.21694', 'title': 'Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation', 'authors': 'Tom Westermann, Malte Ramonat, Johannes Hujer, Felix Gehlhoff, Alexander Fay', 'link': 'https://arxiv.org/abs/2504.21694', 'abstract': 'AutomationML has seen widespread adoption as an open data exchange format in the automation domain. It is an open and vendor neutral standard based on the extensible markup language XML. However, AutomationML extends XML with additional semantics, that limit the applicability of common XML-tools for applications like querying or data validation. This article provides practitioners with 1) an up-to-date ontology of the concepts in the AutomationML-standard, as well as 2) a declarative mapping to automatically transform any AutomationML model into RDF triples. Together, these artifacts allow practitioners an easy integration of AutomationML information into industrial knowledge graphs. A study on examples from the automation domain concludes that transforming AutomationML to OWL opens up new powerful ways for querying and validation that are impossible without transformation.', 'abstract_zh': 'AutomationML作为开放数据交换格式在自动化领域得到了广泛应用。它是一种基于可扩展标记语言XML的开放和供应商中立标准。然而，AutomationML通过添加附加语义扩展了XML，这限制了常用XML工具（如查询或数据验证）的适用性。本文为实践者提供了1) AutomationML标准中概念的最新本体论描述，以及2) 一种声明性映射，以自动将任何AutomationML模型转换为RDF三元组。这些成果使实践者能够轻松地将AutomationML信息集成到工业知识图谱中。研究表明，将AutomationML转换为OWL为查询和验证开辟了新的强大方式，这些方式在不进行转换的情况下是不可能实现的。', 'title_zh': '自动将AutomationML文件映射到本体以进行图查询和验证'}
{'arxiv_id': 'arXiv:2504.21683', 'title': 'Extension-ranking Semantics for Abstract Argumentation Preprint', 'authors': 'Kenneth Skiba, Tjitze Rienstra, Matthias Thimm, Jesse Heyninck, Gabriele Kern-Isberner', 'link': 'https://arxiv.org/abs/2504.21683', 'abstract': 'In this paper, we present a general framework for ranking sets of arguments in abstract argumentation based on their plausibility of acceptance. We present a generalisation of Dung\'s extension semantics as extension-ranking semantics, which induce a preorder over the power set of all arguments, allowing us to state that one set is "closer" to being acceptable than another. To evaluate the extension-ranking semantics, we introduce a number of principles that a well-behaved extension-ranking semantics should satisfy. We consider several simple base relations, each of which models a single central aspect of argumentative reasoning. The combination of these base relations provides us with a family of extension-ranking semantics. We also adapt a number of approaches from the literature for ranking extensions to be usable in the context of extension-ranking semantics, and evaluate their behaviour.', 'abstract_zh': '本文提出了一种基于可接受性的可信度对抽象论辩集进行排名的一般框架。我们将Dung的扩展语义推广为扩展排名语义，从而在所有论题集的幂集中诱导一个部分有序关系，使得我们可以断言一个集合比另一个集合更“接近”可接受。为了评估扩展排名语义，我们提出了若干个良好的扩展排名语义应满足的原则。我们考虑了几种简单的基础关系，每种关系模型了论辩推理的核心方面之一。这些基础关系的组合为我们提供了一系列的扩展排名语义。我们还对文献中用于排名扩展的方法进行了改编，以便在扩展排名语义的背景下使用，并对它们的行为进行了评估。', 'title_zh': '抽象论辩扩展-排序语义预印本'}
{'arxiv_id': 'arXiv:2504.21659', 'title': 'AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization', 'authors': 'Haotian Luo, Haiying He, Yibo Wang, Jinluan Yang, Rui Liu, Naiqiang Tan, Xiaochun Cao, Dacheng Tao, Li Shen', 'link': 'https://arxiv.org/abs/2504.21659', 'abstract': 'Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at this https URL', 'abstract_zh': '最近，长期被认为的推理模型在复杂推理任务上取得了很强的效果，但往往会导致显著的推理开销，使效率成为一个关键问题。我们的实证分析表明，使用长-CoT的好处因问题而异：一些问题需要详细的推理，而另一些则没有改进，甚至准确度下降。这促使了适应性推理策略的发展，这些策略可以根据输入调整推理深度。然而，现有工作主要减少了长推理路径内的冗余，限制了超出长-CoT范式的更高效策略的探索。为此，我们提出了一种新的两阶段框架，以实现适应性和高效推理。首先，我们通过合并长-CoT模型和短-CoT模型构建了一种混合推理模型，以支持多种推理风格。其次，我们应用分级偏好训练来引导模型选择合适推理风格（群组级别），并在每个风格组中偏好简洁和正确的推理（实例级别）。实验表明，与其它基线方法相比，我们的方法显著降低了推理成本，同时保持了性能。值得注意的是，对于五个数学数据集，推理的平均长度减少了超过50%，突显了适应性策略在大型语言模型中优化推理效率的潜力。我们的代码将在此处提供：https://github.com/alibaba/Qwen', 'title_zh': 'AdaR1: 从长共理性思考到混合共理性思考 via 双层自适应推理优化'}
{'arxiv_id': 'arXiv:2504.21568', 'title': 'A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks', 'authors': 'Shui-jin Rong, Wei Guo, Da-qing Zhang', 'link': 'https://arxiv.org/abs/2504.21568', 'abstract': "Aiming at the group decision - making problem with multi - objective attributes, this study proposes a group decision - making system that integrates fuzzy inference and Bayesian network. A fuzzy rule base is constructed by combining threshold values, membership functions, expert experience, and domain knowledge to address quantitative challenges such as scale differences and expert linguistic variables. A hierarchical Bayesian network is designed, featuring a directed acyclic graph with nodes selected by experts, and maximum likelihood estimation is used to dynamically optimize the conditional probability table, modeling the nonlinear correlations among multidimensional indices for posterior probability aggregation. In a comprehensive student evaluation case, this method is compared with the traditional weighted scoring approach. The results indicate that the proposed method demonstrates effectiveness in both rule criterion construction and ranking consistency, with a classification accuracy of 86.0% and an F1 value improvement of 53.4% over the traditional method. Additionally, computational experiments on real - world datasets across various group decision scenarios assess the method's performance and robustness, providing evidence of its reliability in diverse contexts.", 'abstract_zh': '针对多目标属性的群体决策问题，本研究提出了一种集成模糊推理和贝叶斯网络的群体决策系统。通过结合阈值、隶属函数、专家经验和领域知识构建模糊规则库，以解决尺度差异和专家语言变量等量化挑战。设计了一种层次贝叶斯网络，采用专家选取节点的有向无环图，并使用最大似然估计动态优化条件概率表，建模多维指标之间的非线性相关关系，实现后验概率聚合。在综合的学生评估案例中，该方法与传统的加权评分方法进行了对比。实验结果表明，该方法在规则标准构建和排名一致性方面表现出有效性，分类准确率为86.0%，F1值提高了53.4%。此外，通过对不同群体决策场景下的实际数据集进行计算实验，评估了该方法的性能和稳健性，为其在不同情境下的可靠性提供了证据。', 'title_zh': '基于模糊推理和贝叶斯网络的群体决策问题研究'}
{'arxiv_id': 'arXiv:2504.21433', 'title': 'NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence', 'authors': 'Zhicong Li, Hangyu Mao, Jiangjin Yin, Mingzhe Xing, Zhiwei Xu, Yuanxing Zhang, Yang Xiao', 'link': 'https://arxiv.org/abs/2504.21433', 'abstract': 'This paper argues that the next generation of AI agent (NGENT) should integrate across-domain abilities to advance toward Artificial General Intelligence (AGI). Although current AI agents are effective in specialized tasks such as robotics, role-playing, and tool-using, they remain confined to narrow domains. We propose that future AI agents should synthesize the strengths of these specialized systems into a unified framework capable of operating across text, vision, robotics, reinforcement learning, emotional intelligence, and beyond. This integration is not only feasible but also essential for achieving the versatility and adaptability that characterize human intelligence. The convergence of technologies across AI domains, coupled with increasing user demand for cross-domain capabilities, suggests that such integration is within reach. Ultimately, the development of these versatile agents is a critical step toward realizing AGI. This paper explores the rationale for this shift, potential pathways for achieving it.', 'abstract_zh': '下一代人工智能代理（NGENT）应跨域集成以迈向人工通用智能（AGI）', 'title_zh': 'NGENT: 下一代AI代理需集成多域能力以实现人工通用智能'}
{'arxiv_id': 'arXiv:2504.21370', 'title': 'ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning', 'authors': 'Jingyang Yi, Jiazheng Wang', 'link': 'https://arxiv.org/abs/2504.21370', 'abstract': 'Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong performance on reasoning-intensive tasks through extended Chain-of-Thought (CoT) prompting. While longer reasoning traces can facilitate a more thorough exploration of solution paths for complex problems, researchers have observed that these models often "overthink", leading to inefficient inference. In this paper, we introduce ShorterBetter, a simple yet effective reinforcement learning methed that enables reasoning language models to discover their own optimal CoT lengths without human intervention. By sampling multiple outputs per problem and defining the Sample Optimal Length (SOL) as the shortest correct response among all the outputs, our method dynamically guides the model toward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B model, ShorterBetter achieves up to an 80% reduction in output length on both in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our analysis shows that overly long reasoning traces often reflect loss of reasoning direction, and thus suggests that the extended CoT produced by reasoning models is highly compressible.', 'abstract_zh': 'ShorterBetter：一种简单有效的强化学习方法，使推理语言模型自主发现最优的链式思考长度', 'title_zh': '更短更优：引导推理模型寻找高效的推理最优推断长度'}
{'arxiv_id': 'arXiv:2504.21184', 'title': 'AffectEval: A Modular and Customizable Framework for Affective Computing', 'authors': 'Emily Zhou, Khushboo Khatri, Yixue Zhao, Bhaskar Krishnamachari', 'link': 'https://arxiv.org/abs/2504.21184', 'abstract': 'The field of affective computing focuses on recognizing, interpreting, and responding to human emotions, and has broad applications across education, child development, and human health and wellness. However, developing affective computing pipelines remains labor-intensive due to the lack of software frameworks that support multimodal, multi-domain emotion recognition applications. This often results in redundant effort when building pipelines for different applications. While recent frameworks attempt to address these challenges, they remain limited in reducing manual effort and ensuring cross-domain generalizability. We introduce AffectEval, a modular and customizable framework to facilitate the development of affective computing pipelines while reducing the manual effort and duplicate work involved in developing such pipelines. We validate AffectEval by replicating prior affective computing experiments, and we demonstrate that our framework reduces programming effort by up to 90%, as measured by the reduction in raw lines of code.', 'abstract_zh': '情感计算领域专注于识别、解释和响应人类情绪，并在教育、儿童发展和人类健康与福祉等领域拥有广泛的应用前景。然而，由于缺乏支持多模态、多领域情绪识别的应用软件框架，开发情感计算管道仍具有劳动密集型特征。这常常导致在为不同应用程序构建管道时出现重复劳动。尽管最近的框架试图解决这些挑战，但在减少手动努力和确保跨域通用性方面仍然有限。我们引入了AffectEval，这是一个模块化和可定制的框架，旨在促进情感计算管道的开发，同时减少开发此类管道所涉及的手动努力和重复劳动。我们通过复制先前的情感计算实验验证了AffectEval，并证明我们的框架通过减少原始代码行数最多90%的方式减少了编程努力。', 'title_zh': 'AffectEval：一种模块化可定制的情感计算框架'}
{'arxiv_id': 'arXiv:2504.21131', 'title': 'A Formalism for Optimal Search with Dynamic Heuristics', 'authors': 'Remo Christen, Florian Pommerening, Clemens Büchner, Malte Helmert', 'link': 'https://arxiv.org/abs/2504.21131', 'abstract': 'While most heuristics studied in heuristic search depend only on the state, some accumulate information during search and thus also depend on the search history. Various existing approaches use such dynamic heuristics in $\\mathrm{A}^*$-like algorithms and appeal to classic results for $\\mathrm{A}^*$ to show optimality. However, doing so ignores the complexities of searching with a mutable heuristic. In this paper we formalize the idea of dynamic heuristics and use them in a generic algorithm framework. We study a particular instantiation that models $\\mathrm{A}^*$ with dynamic heuristics and show general optimality results. Finally we show how existing approaches from classical planning can be viewed as special cases of this instantiation, making it possible to directly apply our optimality results.', 'abstract_zh': '虽然大多数在启发式搜索中研究的启发式方法仅依赖于状态，但有些启发式方法在搜索过程中累积信息，因此也依赖于搜索历史。现有方法在类似A*的算法中使用此类动态启发式方法，并依靠A*的经典结果来证明其最优性。然而，这样做忽略了使用可变启发式进行搜索的复杂性。在本文中，我们正式化动态启发式的思想，并在通用算法框架中使用它们。我们研究了一种特定实例，该实例使用动态启发式方法建模A*，并给出了通用的最优性结果。最后，我们展示了现有的经典规划方法可以被视为该实例的特例，从而使我们能够直接应用我们的最优性结果。', 'title_zh': '一种带有动态启发式的最优搜索形式化方法'}
{'arxiv_id': 'arXiv:2504.21849', 'title': 'Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support', 'authors': 'Justin B. Bullock, Janet V.T. Pauketat, Hsini Huang, Yi-Fan Wang, Jacy Reese Anthis', 'link': 'https://arxiv.org/abs/2504.21849', 'abstract': 'Governance institutions must respond to societal risks, including those posed by generative AI. This study empirically examines how public trust in institutions and AI technologies, along with perceived risks, shape preferences for AI regulation. Using the nationally representative 2023 Artificial Intelligence, Morality, and Sentience (AIMS) survey, we assess trust in government, AI companies, and AI technologies, as well as public support for regulatory measures such as slowing AI development or outright bans on advanced AI. Our findings reveal broad public support for AI regulation, with risk perception playing a significant role in shaping policy preferences. Individuals with higher trust in government favor regulation, while those with greater trust in AI companies and AI technologies are less inclined to support restrictions. Trust in government and perceived risks significantly predict preferences for both soft (e.g., slowing development) and strong (e.g., banning AI systems) regulatory interventions. These results highlight the importance of public opinion in AI governance. As AI capabilities advance, effective regulation will require balancing public concerns about risks with trust in institutions. This study provides a foundational empirical baseline for policymakers navigating AI governance and underscores the need for further research into public trust, risk perception, and regulatory strategies in the evolving AI landscape.', 'abstract_zh': '治理机构必须应对包括生成性人工智能在内的一系列社会风险。本研究通过实证分析考察公众对机构和人工智能技术的信任以及感知风险如何影响对人工智能监管的偏好。利用2023年人工智能、道德与意识（AIMS）全国代表性调查，我们评估了公众对政府、人工智能公司和人工智能技术的信任，以及对如减缓人工智能发展或全面禁止高级人工智能等监管措施的支持程度。研究发现，公众普遍支持人工智能监管，感知风险在塑造政策偏好方面发挥着重要作用。政府信任较高的个体倾向于支持监管，而对人工智能公司和人工智能技术有较高信任的个体则不太支持限制措施。政府信任和感知风险显著预测对软性（如减缓发展）和强硬（如禁止人工智能系统）监管干预措施的偏好。这些结果突显了公众意见在人工智能治理中的重要性。随着人工智能能力的不断提升，有效的监管需要在平衡公众对风险的担忧与对机构的信任之间找到平衡。本研究为政策制定者在人工智能治理中的判断提供了基础性的实证基准，并强调了进一步研究公众信任、风险感知和监管策略在不断变化的人工智能格局中的重要性。', 'title_zh': '公众意见与数字思维的崛起：感知风险、信任与监管支持'}
{'arxiv_id': 'arXiv:2504.21848', 'title': 'Characterizing AI Agents for Alignment and Governance', 'authors': 'Atoosa Kasirzadeh, Iason Gabriel', 'link': 'https://arxiv.org/abs/2504.21848', 'abstract': 'The creation of effective governance mechanisms for AI agents requires a deeper understanding of their core properties and how these properties relate to questions surrounding the deployment and operation of agents in the world. This paper provides a characterization of AI agents that focuses on four dimensions: autonomy, efficacy, goal complexity, and generality. We propose different gradations for each dimension, and argue that each dimension raises unique questions about the design, operation, and governance of these systems. Moreover, we draw upon this framework to construct "agentic profiles" for different kinds of AI agents. These profiles help to illuminate cross-cutting technical and non-technical governance challenges posed by different classes of AI agents, ranging from narrow task-specific assistants to highly autonomous general-purpose systems. By mapping out key axes of variation and continuity, this framework provides developers, policymakers, and members of the public with the opportunity to develop governance approaches that better align with collective societal goals.', 'abstract_zh': '有效治理机制的创建需要对AI代理的核心属性及其与代理部署和操作相关问题有更深入的理解。本文从自主性、有效性、目标复杂性和普适性四个维度对AI代理进行刻画，并提出每个维度的不同层次，认为每个维度都会对这些系统的设计、运行和治理提出独特的挑战。此外，我们利用这一框架构建不同类型的AI代理的“代理画像”，这些画像有助于揭示不同类别AI代理所提出的技术和非技术治理挑战，范围从狭义的任务特定辅助系统到高度自主的通用系统。通过映射关键的变异和连续性维度，这一框架为开发者、决策者和公众提供了更好地与集体社会目标相契合的治理方法的机会。', 'title_zh': '刻画AI代理以实现对齐与治理'}
{'arxiv_id': 'arXiv:2504.21846', 'title': 'Active Light Modulation to Counter Manipulation of Speech Visual Content', 'authors': 'Hadleigh Schwartz, Xiaofeng Yan, Charles J. Carver, Xia Zhou', 'link': 'https://arxiv.org/abs/2504.21846', 'abstract': "High-profile speech videos are prime targets for falsification, owing to their accessibility and influence. This work proposes Spotlight, a low-overhead and unobtrusive system for protecting live speech videos from visual falsification of speaker identity and lip and facial motion. Unlike predominant falsification detection methods operating in the digital domain, Spotlight creates dynamic physical signatures at the event site and embeds them into all video recordings via imperceptible modulated light. These physical signatures encode semantically-meaningful features unique to the speech event, including the speaker's identity and facial motion, and are cryptographically-secured to prevent spoofing. The signatures can be extracted from any video downstream and validated against the portrayed speech content to check its integrity. Key elements of Spotlight include (1) a framework for generating extremely compact (i.e., 150-bit), pose-invariant speech video features, based on locality-sensitive hashing; and (2) an optical modulation scheme that embeds >200 bps into video while remaining imperceptible both in video and live. Prototype experiments on extensive video datasets show Spotlight achieves AUCs $\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified videos. Further, Spotlight is highly robust across recording conditions, video post-processing techniques, and white-box adversarial attacks on its video feature extraction methodologies.", 'abstract_zh': '高知名度演讲视频是视觉伪造的首要目标，由于其易获取性和影响力。本文提出Spotlight，一种低开销且不显性的系统，用于保护实时演讲视频免受演讲者身份和唇部及面部动作的视觉伪造。不同于主要在数字域操作的伪造检测方法，Spotlight 在事件现场创建动态的物理签名，并通过不可感知的调制光将它们嵌入到所有视频录制中。这些物理签名编码了演讲事件特有的语义特征，包括演讲者身份和面部动作，并通过加密手段防止伪造。可以从前端的任何视频中提取这些签名并验证其所述内容的完整性以检查其真实性。Spotlight 的关键要素包括（1）基于局部敏感哈希的生成极其紧凑（即150位）且姿态不变的演讲视频特征的框架；（2）一种光学调制方案，可在保持视频和实时环境下的不可感知性的同时，将超过200 bps的信息嵌入到视频中。在广泛视频数据集上的原型实验表明，Spotlight 在检测伪造视频时的AUC值≥0.99，总体真阳性率为100%。此外，Spotlight 对录制条件、视频后处理技术以及对其视频特征提取方法的白盒对抗攻击具有高度 robust 性。', 'title_zh': '主动_light_调制以对抗语音视觉内容操纵'}
{'arxiv_id': 'arXiv:2504.21800', 'title': 'How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues', 'authors': 'Suhas BN, Dominik Mattioli, Saeed Abdullah, Rosa I. Arriaga, Chris W. Wiese, Andrew M. Sherrill', 'link': 'https://arxiv.org/abs/2504.21800', 'abstract': 'The growing adoption of synthetic data in healthcare is driven by privacy concerns, limited access to real-world data, and the high cost of annotation. This work explores the use of synthetic Prolonged Exposure (PE) therapeutic conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable alternative for training and evaluating clinical models. We systematically compare real and synthetic dialogues using linguistic, structural, and protocol-specific metrics, including turn-taking patterns and treatment fidelity. We also introduce and evaluate PE-specific metrics derived from linguistic analysis and semantic modeling, offering a novel framework for assessing clinical fidelity beyond surface fluency. Our findings show that although synthetic data holds promise for mitigating data scarcity and protecting patient privacy, it can struggle to capture the subtle dynamics of therapeutic interactions. In our dataset, synthetic dialogues match structural features of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99), however, synthetic interactions do not adequately reflect key fidelity markers (e.g., distress monitoring). We highlight gaps in existing evaluation frameworks and advocate for fidelity-aware metrics that go beyond surface fluency to uncover clinically significant failures. Our findings clarify where synthetic data can effectively complement real-world datasets -- and where critical limitations remain.', 'abstract_zh': '合成数据在医疗保健中的 adoption 日益增长，驱动因素包括隐私 concern、真实世界数据访问受限以及标注成本高昂。本文探索用于创伤后应激障碍 (PTSD) 延长暴露（PE）治疗交流的合成数据在训练和评估临床模型中的应用，作为一种可扩展的替代方案。我们系统地比较了真实和合成对话，使用语言学、结构和协议特定的指标进行评估，包括对话轮换模式和治疗忠实度。我们还引入并评估了源自语言分析和语义建模的 PE 特定指标，提供了一种评估临床忠实度的新框架，超越表面流畅性。我们的研究发现，虽然合成数据有潜力缓解数据稀缺问题并保护患者隐私，但在捕捉治疗交互的细微动态方面仍存在问题。在我们的数据集中，合成对话在结构特征上与真实对话匹配（例如，说话人转换比例：0.98 对 0.99），然而，合成交互未能充分反映关键的忠实度标志（如，应激监测）。我们指出了现有评估框架中的不足，并倡导超越表面流畅性的忠实度感知指标，以揭示临床显著的失败。我们的研究结果明确了合成数据在补充真实数据集中的有效应用领域——以及依然存在的关键限制。', 'title_zh': '合成治疗对话有多真实？延长暴露对话的信度评估'}
{'arxiv_id': 'arXiv:2504.21798', 'title': 'SWE-smith: Scaling Data for Software Engineering Agents', 'authors': 'John Yang, Kilian Leret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, Diyi Yang', 'link': 'https://arxiv.org/abs/2504.21798', 'abstract': 'Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at this https URL.', 'abstract_zh': '尽管在软件工程领域的语言模型取得了最近的进步，但收集训练数据仍然是一个显著的痛点。现有的数据集规模较小，最多包含来自11个或更少的GitHub仓库的数千个训练实例。编纂这些数据集的过程通常非常复杂，需要数百小时的人工劳动；伴随的执行环境也需要占用数TB的存储空间，严重限制了其可扩展性和实用性。为解决这一痛点，我们引入了SWE-smith，这是一个新型的生成大规模软件工程训练数据的流水线。给定任何Python代码库，SWE-smith会构建相应的执行环境，然后自动合成数百到数千个任务实例，这些任务实例会破坏代码库中的现有测试。使用SWE-smith，我们创建了一个源自128个GitHub仓库的数据集，其中包含50,000个实例，是之前所有工作的十倍以上。我们训练了SWE-agent-LM-32B，并在SWE-bench Verified基准测试中实现了40.2%的Pass@1解决率，这是开源模型中的最新成果。我们开源了SWE-smith（数据收集流程、任务实例、轨迹、模型），以降低在自动软件工程领域使用语言模型系统进行研究的门槛。所有资源可通过此链接获得。', 'title_zh': 'SWE-Smith: 扩展数据以适应软件工程代理'}
{'arxiv_id': 'arXiv:2504.21775', 'title': 'Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning', 'authors': 'Rongguang Ye, Ming Tang', 'link': 'https://arxiv.org/abs/2504.21775', 'abstract': "Recent methods leverage a hypernet to handle the performance-fairness trade-offs in federated learning. This hypernet maps the clients' preferences between model performance and fairness to preference-specifc models on the trade-off curve, known as local Pareto front. However, existing methods typically adopt a uniform preference sampling distribution to train the hypernet across clients, neglecting the inherent heterogeneity of their local Pareto fronts. Meanwhile, from the perspective of generalization, they do not consider the gap between local and global Pareto fronts on the global dataset. To address these limitations, we propose HetPFL to effectively learn both local and global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA) and Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the optimal preference sampling distribution for each client to accommodate heterogeneous local Pareto fronts. While PHF performs preference-aware fusion of clients' hypernets to ensure the performance of the global Pareto front. We prove that HetPFL converges linearly with respect to the number of rounds, under weaker assumptions than existing methods. Extensive experiments on four datasets show that HetPFL significantly outperforms seven baselines in terms of the quality of learned local and global Pareto fronts.", 'abstract_zh': '近期方法利用超网络来处理联邦学习中的性能-公平性权衡。该超网络将客户端在模型性能和公平性之间的偏好映射到权衡曲线上特定偏好模型上的局部帕累托前沿。然而，现有方法通常采用统一的偏好采样分布来训练超网络，忽略了客户端之间固有的局部帕累托前沿异质性。同时，从泛化的角度来看，它们未能考虑局部和全局帕累托前沿之间的差距。为了解决这些局限性，我们提出了HetPFL来有效学习局部和全局帕累托前沿。HetPFL包括偏好采样适应（PSA）和偏好感知超网络融合（PHF）。PSA动态确定每个客户端的最佳偏好采样分布，以适应异质性的局部帕累托前沿。而PHF执行客户端超网络的偏好感知融合，以确保全局帕累托前沿的性能。我们证明，HetPFL在较弱的假设条件下，随着轮次的增加具有线性收敛性。广泛的实验在四个数据集上表明，HetPFL在学习的局部和全局帕累托前沿质量方面显著优于七个基线方法。', 'title_zh': '联邦学习中异质性能-公平性权衡的学习'}
{'arxiv_id': 'arXiv:2504.21731', 'title': 'Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning', 'authors': 'Feiyu Lu, Mengyu Chen, Hsiang Hsu, Pranav Deshpande, Cheng Yao Wang, Blair MacIntyre', 'link': 'https://arxiv.org/abs/2504.21731', 'abstract': "Mixed Reality (MR) could assist users' tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users' poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR.", 'abstract_zh': '混合现实（MR）中的强化学习辅助连续3D内容放置研究', 'title_zh': '使用深度强化学习的混合现实自适应3D UI布局'}
{'arxiv_id': 'arXiv:2504.21730', 'title': 'Cert-SSB: Toward Certified Sample-Specific Backdoor Defense', 'authors': 'Ting Qiao, Yingjia Wang, Xing Liu, Sixing Wu, Jianbing Li, Yiming Li', 'link': 'https://arxiv.org/abs/2504.21730', 'abstract': "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. The compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world DNN applications. Currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more advanced backdoor techniques. In contrast, certified defenses based on randomized smoothing have shown promise by adding random noise to training and testing samples to counteract backdoor attacks. In this paper, we reveal that existing randomized smoothing defenses implicitly assume that all samples are equidistant from the decision boundary. However, it may not hold in practice, leading to suboptimal certification performance. To address this issue, we propose a sample-specific certified backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic gradient ascent to optimize the noise magnitude for each sample, ensuring a sample-specific noise level that is then applied to multiple poisoned training sets to retrain several smoothed models. After that, Cert-SSB aggregates the predictions of multiple smoothed models to generate the final robust prediction. In particular, in this case, existing certification methods become inapplicable since the optimized noise varies across different samples. To conquer this challenge, we introduce a storage-update-based certification method, which dynamically adjusts each sample's certification region to improve certification performance. We conduct extensive experiments on multiple benchmark datasets, demonstrating the effectiveness of our proposed method. Our code is available at this https URL.", 'abstract_zh': '深度神经网络（DNNs）易受后门攻击的威胁，攻击者可以通过操纵一小部分训练数据植入隐藏的后门，使受攻击模型在干净样本上表现正常，但在后门样本上将其错分类为目标类，对实际应用构成重大威胁。目前，已经提出了几种经验防御方法来减轻后门攻击，但它们往往被更先进的后门技术绕过。相比之下，基于随机化平滑的认证防御方法通过在训练和测试样本中添加随机噪声来对抗后门攻击，显示出前景。在本文中，我们揭示现有随机化平滑防御方法隐含地假设所有样本与决策边界等距，但在实践中可能不成立，导致认证性能不佳。为解决这一问题，我们提出了一种样本特定的认证后门防御方法，称为Cert-SSB。Cert-SSB首先使用随机梯度上升来优化每个样本的噪声幅度，确保针对每个样本的具体噪声级别，然后应用于多个中毒训练集以重新训练多个平滑模型。之后，Cert-SSB聚合多个平滑模型的预测生成最终鲁棒预测。特别是，在这种情况下，现有的认证方法不再适用，因为优化的噪声在不同样本之间变化。为克服这一挑战，我们引入了一种基于存储更新的认证方法，动态调整每个样本的认证区域以提高认证性能。我们在多个基准数据集上进行了广泛实验，证明了我们提出方法的有效性。我们的代码可以在以下链接中获取：this https URL。', 'title_zh': 'Cert-SSB: 面向样本特定后门防御的认证方法'}
{'arxiv_id': 'arXiv:2504.21719', 'title': 'Sionna RT: Technical Report', 'authors': 'Fayçal Aït Aoudia, Jakob Hoydis, Merlin Nimier-David, Sebastian Cammerer, Alexander Keller', 'link': 'https://arxiv.org/abs/2504.21719', 'abstract': 'Sionna is an open-source, GPU-accelerated library that, as of version 0.14, incorporates a ray tracer for simulating radio wave propagation. A unique feature of Sionna RT is differentiability, enabling the calculation of gradients for the channel impulse responses (CIRs), radio maps, and other related metrics with respect to system and environmental parameters, such as material properties, antenna patterns, and array geometries. The release of Sionna 1.0 provides a complete overhaul of the ray tracer, significantly improving its speed, memory efficiency, and extensibility. This document details the algorithms employed by Sionna RT to simulate radio wave propagation efficiently, while also addressing their current limitations. Given that the computation of CIRs and radio maps requires distinct algorithms, these are detailed in separate sections. For CIRs, Sionna RT integrates shooting and bouncing of rays (SBR) with the image method and uses a hashing-based mechanism to efficiently eliminate duplicate paths. Radio maps are computed using a purely SBR-based approach.', 'abstract_zh': 'Sionna是开源的GPU加速库，其版本0.14集成了用于模拟无线电波传播的射线跟踪器。Sionna RT的独特之处在于其可微性，这使得能够计算信道冲激响应（CIRs）、无线电图及其他相关指标相对于系统和环境参数（如材料属性、天线模式和阵列几何形状）的梯度。Sionna 1.0的发布对射线跟踪器进行了全面的改造，显著提高了其速度、内存效率和可扩展性。本文详细介绍了Sionna RT用于高效模拟无线电波传播的算法，同时也指出了当前的限制。鉴于CIRs和无线电图的计算需要不同的算法，这些算法在单独的部分中进行了详细描述。对于CIRs，Sionna RT结合了射线追踪方法和图像方法，并使用基于哈希的机制有效地消除重复路径。无线电图是使用纯射线追踪方法进行计算的。', 'title_zh': 'Sionna RT: 技术报告'}
{'arxiv_id': 'arXiv:2504.21707', 'title': 'Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning', 'authors': 'Anthony D Martin', 'link': 'https://arxiv.org/abs/2504.21707', 'abstract': 'We propose a generalization of modern representation learning objectives by reframing them as recursive divergence alignment processes over localized conditional distributions While recent frameworks like Information Contrastive Learning I-Con unify multiple learning paradigms through KL divergence between fixed neighborhood conditionals we argue this view underplays a crucial recursive structure inherent in the learning process. We introduce Recursive KL Divergence Optimization RKDO a dynamic formalism where representation learning is framed as the evolution of KL divergences across data neighborhoods. This formulation captures contrastive clustering and dimensionality reduction methods as static slices while offering a new path to model stability and local adaptation. Our experiments demonstrate that RKDO offers dual efficiency advantages approximately 30 percent lower loss values compared to static approaches across three different datasets and 60 to 80 percent reduction in computational resources needed to achieve comparable results. This suggests that RKDOs recursive updating mechanism provides a fundamentally more efficient optimization landscape for representation learning with significant implications for resource constrained applications.', 'abstract_zh': '我们通过将现代表示学习目标重新框定为局部条件分布上的递归偏差对齐过程，提出了一种泛化的表示学习目标。虽然近年来的信息对比学习框架I-Con通过固定邻域条件间的KL散度统一了多种学习范式，我们认为这种观点忽视了学习过程中固有的递归结构。我们引入了递归KL散度优化（RKDO）作为一种动态形式，将表示学习框架化为数据邻域间KL散度的演变过程。该形式化描述捕获了对比聚类和维度减少方法的静态截面，并提供了一条新的途径以实现模型稳定性和局部适应性。我们的实验表明，与静态方法相比，RKDO在三个不同数据集上分别提供了约30%较低的损失值，并且在达到可比结果时所需的计算资源减少了60%至80%。这表明，RKDO的递归更新机制为表示学习提供了本质更高效的优化场景，对资源受限的应用具有重大意义。', 'title_zh': '递归KL散度优化：一种动态表示学习框架'}
{'arxiv_id': 'arXiv:2504.21685', 'title': 'Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning', 'authors': 'Reem Abdel-Salam, Mary Adewunmi', 'link': 'https://arxiv.org/abs/2504.21685', 'abstract': 'Health Mention Classification (HMC) plays a critical role in leveraging social media posts for real-time tracking and public health monitoring. Nevertheless, the process of HMC presents significant challenges due to its intricate nature, primarily stemming from the contextual aspects of health mentions, such as figurative language and descriptive terminology, rather than explicitly reflecting a personal ailment. To address this problem, we argue that clearer mentions can be achieved through conventional fine-tuning with enhanced parameters of biomedical natural language methods (NLP). In this study, we explore different techniques such as the utilisation of part-of-speech (POS) tagger information, improving on PEFT techniques, and different combinations thereof. Extensive experiments are conducted on three widely used datasets: RHDM, PHM, and Illness. The results incorporated POS tagger information, and leveraging PEFT techniques significantly improves performance in terms of F1-score compared to state-of-the-art methods across all three datasets by utilising smaller models and efficient training. Furthermore, the findings highlight the effectiveness of incorporating POS tagger information and leveraging PEFT techniques for HMC. In conclusion, the proposed methodology presents a potentially effective approach to accurately classifying health mentions in social media posts while optimising the model size and training efficiency.', 'abstract_zh': '健康mention分类（HMC）在利用社交媒体帖子进行实时跟踪和公共卫生监测中发挥着关键作用。然而，HMC的过程由于其复杂性而面临重大挑战，主要源自健康mention的上下文方面，如隐喻语言和描述性术语，而不是明确反映个人疾病。为了解决这一问题，我们认为可以通过增强生物医学自然语言处理方法（NLP）的超参数进行常规微调来实现更清晰的mention。在本研究中，我们探索了不同的技术，如利用词性（POS）标注器信息、改进PEFT技术以及它们的不同组合。我们在三个广泛使用的数据集中进行了广泛的实验：RHDM、PHM和Illness。结果表明，在这些数据集中，利用POS标注器信息和利用PEFT技术显著提高了F1分数的表现，相比最先进的方法，使用较小的模型和高效的训练。此外，研究结果强调了结合POS标注器信息和利用PEFT技术在HMC中的有效性。总之，所提出的方法提供了一种潜在有效的途径，同时优化模型大小和训练效率来准确分类社交媒体帖子中的健康mention。', 'title_zh': '增强健康主题分类性能：关于参数高效调优的研究'}
{'arxiv_id': 'arXiv:2504.21634', 'title': 'Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data', 'authors': 'Chih-Cheng Rex Yuan, Bow-Yaw Wang', 'link': 'https://arxiv.org/abs/2504.21634', 'abstract': "Fairness auditing of AI systems can identify and quantify biases. However, traditional auditing using real-world data raises security and privacy concerns. It exposes auditors to security risks as they become custodians of sensitive information and targets for cyberattacks. Privacy risks arise even without direct breaches, as data analyses can inadvertently expose confidential information. To address these, we propose a framework that leverages differentially private synthetic data to audit the fairness of AI systems. By applying privacy-preserving mechanisms, it generates synthetic data that mirrors the statistical properties of the original dataset while ensuring privacy. This method balances the goal of rigorous fairness auditing and the need for strong privacy protections. Through experiments on real datasets like Adult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real data. By analyzing the alignment and discrepancies between these metrics, we assess the capacity of synthetic data to preserve the fairness properties of real data. Our results demonstrate the framework's ability to enable meaningful fairness evaluations while safeguarding sensitive information, proving its applicability across critical and sensitive domains.", 'abstract_zh': 'AI系统公平性审计机制可以识别和量化偏差。然而，使用真实世界数据的传统审计引发了安全和隐私问题。这使得审计者成为敏感信息的保管者，并成为网络攻击的目标。即使没有直接泄露，数据分析也可能无意中暴露敏感信息。为解决这些问题，我们提出了一种框架，利用差分隐私合成数据来审计AI系统的公平性。通过应用隐私保护机制，该框架生成与原始数据集统计特性相似的合成数据，同时确保隐私。这种方法在严格公平性审计和强大的隐私保护需求之间取得了平衡。通过在Adult、COMPAS和Diabetes等真实数据集上进行实验，我们比较了合成数据和实际数据的公平性指标。通过分析这些指标之间的对齐和差异，我们评估了合成数据在保留实际数据公平性属性方面的能力。我们的结果表明，该框架能够确保在保护敏感信息的同时进行有意义的公平性评估，证明了其在关键和敏感领域中的适用性。', 'title_zh': '基于不同隐私合成数据的AI公平性定量审计'}
{'arxiv_id': 'arXiv:2504.21565', 'title': 'Towards proactive self-adaptive AI for non-stationary environments with dataset shifts', 'authors': 'David Fernández Narro, Pablo Ferri, Juan M. García-Gómez, Carlos Sáez', 'link': 'https://arxiv.org/abs/2504.21565', 'abstract': 'Artificial Intelligence (AI) models deployed in production frequently face challenges in maintaining their performance in non-stationary environments. This issue is particularly noticeable in medical settings, where temporal dataset shifts often occur. These shifts arise when the distributions of training data differ from those of the data encountered during deployment over time. Further, new labeled data to continuously retrain AI is not typically available in a timely manner due to data access limitations. To address these challenges, we propose a proactive self-adaptive AI approach, or pro-adaptive, where we model the temporal trajectory of AI parameters, allowing us to short-term forecast parameter values. To this end, we use polynomial spline bases, within an extensible Functional Data Analysis framework. We validate our methodology with a logistic regression model addressing prior probability shift, covariate shift, and concept shift. This validation is conducted on both a controlled simulated dataset and a publicly available real-world COVID-19 dataset from Mexico, with various shifts occurring between 2020 and 2024. Our results indicate that this approach enhances the performance of AI against shifts compared to baseline stable models trained at different time distances from the present, without requiring updated training data. This work lays the foundation for pro-adaptive AI research against dynamic, non-stationary environments, being compatible with data protection, in resilient AI production environments for health.', 'abstract_zh': '人工智能模型在生产中部署时常面临在非平稳环境中保持性能的挑战。这一问题在医疗环境中尤为明显，因为时间序列数据集的变动经常发生。这些变动源于训练数据分布与部署过程中遇到的数据分布随时间变化而不同。此外，由于数据访问限制，及时获取新的带标签数据以不断重新训练AI通常不可行。为应对这些挑战，我们提出了一种前瞻性的自适应AI方法，或称pro-adaptive方法，其中我们建模了AI参数的时间轨迹，允许我们短期预测参数值。为此，我们利用多项式样条基函数，在扩展的功能数据分析框架中进行。我们使用逻辑回归模型来验证该方法，该模型解决了先验概率变化、协变量变化和概念变化的问题。这项验证工作在受控的模拟数据集和墨西哥公开的真实世界COVID-19数据集上进行，该数据集的时间跨度从2020年到2024年，包含了多个变化。结果显示，与基于不同时间距离训练的基线稳定模型相比，这种方法在无需更新训练数据的情况下，提高了AI对抗变化的性能。本工作中构建了针对动态、非平稳环境的前瞻性自适应AI研究基础，适用于保护数据的稳健AI生产环境，用于健康领域。', 'title_zh': '面向非稳态环境和数据集漂移的前瞻自适应人工智能'}
{'arxiv_id': 'arXiv:2504.21562', 'title': 'eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes', 'authors': 'Henry John Krumb, Anirban Mukhopadhyay', 'link': 'https://arxiv.org/abs/2504.21562', 'abstract': 'Wireless Capsule Endoscopy is a non-invasive imaging method for the entire gastrointestinal tract, and is a pain-free alternative to traditional endoscopy. It generates extensive video data that requires significant review time, and localizing the capsule after ingestion is a challenge. Techniques like bleeding detection and depth estimation can help with localization of pathologies, but deep learning models are typically too large to run directly on the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and depth estimation are trained on capsule endoscopic images. For monocular depth estimation, we distill a large foundation model into the lean NCA architecture, by treating the outputs of the foundation model as pseudo ground truth. We then port the trained NCA to the ESP32 microcontroller, enabling efficient image processing on hardware as small as a camera capsule. NCA are more accurate (Dice) than other portable segmentation models, while requiring more than 100x fewer parameters stored in memory than other small-scale models. The visual results of NCA depth estimation look convincing, and in some cases beat the realism and detail of the pseudo ground truth. Runtime optimizations on the ESP32-S3 accelerate the average inference speed significantly, by more than factor 3. With several algorithmic adjustments and distillation, it is possible to eNCApsulate NCA models into microcontrollers that fit into wireless capsule endoscopes. This is the first work that enables reliable bleeding segmentation and depth estimation on a miniaturized device, paving the way for precise diagnosis combined with visual odometry as a means of precise localization of the capsule -- on the capsule.', 'abstract_zh': '无创胶囊内镜是一种无需侵入且无痛的消化道成像方法，它是传统内镜的一种替代方案。它会产生大量的视频数据，需要大量时间审查，并且在吞服后定位胶囊是一个挑战。出血检测和深度估计等技术有助于病灶定位，但深度学习模型通常太大而无法直接运行在胶囊上。通过使用神经细胞自动机（NCA）进行出血分割和深度估计，这些模型可在胶囊内窥镜图像上进行训练。对于单目深度估计，我们通过将大基底模型的输出作为伪地面真实值，将其精炼成轻量级的NCA架构。然后，我们将训练好的NCA移植到ESP32微控制器上，在与摄像头胶囊大小相当的硬件上实现了高效的图像处理。与其它便携式分割模型相比，NCA更准确（Dice值），存储在内存中的参数数量少于其他小型模型超过100倍。NCA的深度估计视觉结果令人信服，在某些情况下甚至超过伪地面真实的逼真度和细节。在ESP32-S3上的运行时优化显著加速了平均推理速度，超过3倍。通过多种算法调整和精炼，有可能将NCA模型封装到可以嵌入无线胶囊内镜的微控制器中。这是首个可在缩小设备上实现可靠的出血分割和深度估计的工作，为结合视觉里程计进行精确胶囊定位提供了可能。', 'title_zh': 'eNCApsulate: NCA for Precision Diagnosis of Capsule Endoscopes'}
{'arxiv_id': 'arXiv:2504.21545', 'title': 'Meta knowledge assisted Evolutionary Neural Architecture Search', 'authors': 'Yangyang Li, Guanlong Liu, Ronghua Shang, Licheng Jiao', 'link': 'https://arxiv.org/abs/2504.21545', 'abstract': 'Evolutionary computation (EC)-based neural architecture search (NAS) has achieved remarkable performance in the automatic design of neural architectures. However, the high computational cost associated with evaluating searched architectures poses a challenge for these methods, and a fixed form of learning rate (LR) schedule means greater information loss on diverse searched architectures. This paper introduces an efficient EC-based NAS method to solve these problems via an innovative meta-learning framework. Specifically, a meta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a suitable LR schedule, which guides the training process with lower information loss when evaluating each individual. An adaptive surrogate model is designed through an adaptive threshold to select the potential architectures in a few epochs and then evaluate the potential architectures with complete epochs. Additionally, a periodic mutation operator is proposed to increase the diversity of the population, which enhances the generalizability and robustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets demonstrate that the proposed method achieves high performance comparable to that of many state-of-the-art peer methods, with lower computational cost and greater robustness.', 'abstract_zh': '基于进化计算的神经架构搜索方法通过元学习框架解决评估开销和学习率调度问题', 'title_zh': '元知识辅助进化神经架构搜索'}
{'arxiv_id': 'arXiv:2504.21489', 'title': 'TRIED: Truly Innovative and Effective Detection Benchmark, developed by WITNESS', 'authors': 'Shirin Anlen, Zuzanna Wojciak', 'link': 'https://arxiv.org/abs/2504.21489', 'abstract': 'The rise of generative AI and deceptive synthetic media threatens the global information ecosystem, especially across the Global Majority. This report from WITNESS highlights the limitations of current AI detection tools, which often underperform in real-world scenarios due to challenges related to explainability, fairness, accessibility, and contextual relevance. In response, WITNESS introduces the Truly Innovative and Effective AI Detection (TRIED) Benchmark, a new framework for evaluating detection tools based on their real-world impact and capacity for innovation. Drawing on frontline experiences, deceptive AI cases, and global consultations, the report outlines how detection tools must evolve to become truly innovative and relevant by meeting diverse linguistic, cultural, and technological contexts. It offers practical guidance for developers, policymakers, and standards bodies to design accountable, transparent, and user-centered detection solutions, and incorporate sociotechnical considerations into future AI standards, procedures and evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can drive innovation, safeguard public trust, strengthen AI literacy, and contribute to a more resilient global information credibility.', 'abstract_zh': '生成式AI的崛起和欺骗性合成媒体威胁全球信息生态系统，尤其是全球多数国家。 Witness报告指出当前AI检测工具的局限性，这些工具在实际场景中的表现经常不尽如人意，原因在于可解释性、公平性、可访问性及情境相关性方面的挑战。为应对这一挑战，Witness提出真正的创新和有效的AI检测标准（TRIED Benchmark），这是一种基于检测工具在实际影响和创新能力的新框架。该报告基于前线经验、欺骗性AI案例和全球咨询，概述了检测工具如何通过满足多样化的语言、文化和技术水平，成为真正创新和相关的方法。它为开发者、政策制定者和标准机构提供了实用指导，以设计可问责、透明和用户中心的检测解决方案，并将社会和技术考量纳入未来AI标准、程序和评估框架中。通过采用TRIED标准，利益相关者可以推动创新、保护公众信任、增强AI素养，并为更加韧性的全球信息可信度作出贡献。', 'title_zh': 'TRIED: 真正创新有效的检测基准，由WITNESS开发'}
{'arxiv_id': 'arXiv:2504.21480', 'title': 'A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense', 'authors': 'Yuchen Ding, Hongli Peng, Xiaoqi Li', 'link': 'https://arxiv.org/abs/2504.21480', 'abstract': 'With the rapid advancement of blockchain technology, smart contracts have enabled the implementation of increasingly complex functionalities. However, ensuring the security of smart contracts remains a persistent challenge across the stages of development, compilation, and execution. Vulnerabilities within smart contracts not only undermine the security of individual applications but also pose significant risks to the broader blockchain ecosystem, as demonstrated by the growing frequency of attacks since 2016, resulting in substantial financial losses. This paper provides a comprehensive analysis of key security risks in Ethereum smart contracts, specifically those written in Solidity and executed on the Ethereum Virtual Machine (EVM). We focus on two prevalent and critical vulnerability types (reentrancy and integer overflow) by examining their underlying mechanisms, replicating attack scenarios, and assessing effective countermeasures.', 'abstract_zh': '随着区块链技术的迅猛发展，智能合约实现了日益复杂的功能。然而，确保智能合约的安全性仍然是开发、编译和执行阶段的一项持久挑战。智能合约中的漏洞不仅削弱了单个应用的安全性，还对更广泛的区块链生态系统构成了重大风险，这在2016年以来日益频繁的攻击中表现得尤为明显，造成了重大经济损失。本文对以Solidity编写并在以太坊虚拟机（EVM）上执行的以太坊智能合约的关键安全风险进行了全面分析，重点关注重入攻击和整数溢出这两种常见的严重漏洞类型，通过分析其工作机制、复现攻击场景并评估有效防御措施。', 'title_zh': '智能合约中存在的可利用模式综合研究：从漏洞到防御'}
{'arxiv_id': 'arXiv:2504.21475', 'title': 'Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines', 'authors': 'Serry Sibaee, Samar Ahmed, Abdullah Al Harbi, Omer Nacar, Adel Ammar, Yasser Habashi, Wadii Boulila', 'link': 'https://arxiv.org/abs/2504.21475', 'abstract': 'This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic.', 'abstract_zh': '本研究通过开发一个有效的阿拉伯语逆向词典（RD）系统来填补阿拉伯自然语言处理中的关键空白，该系统使用户能够根据描述或意义查找词语。我们提出了一种新颖的基于变换器的方法，采用几何递减层数的半编码神经网络架构，实现了阿拉伯语RD任务的最新成果。我们的方法包含全面的数据集构建过程，并建立了阿拉伯语词典定义的正式质量标准。使用各种预训练模型的实验表明，阿拉伯语特定模型显著优于通用多语言嵌入，其中ARBERTv2获得最佳排名得分（0.0644）。此外，我们还提供了一种逆向词典任务的形式化抽象，以增强其理论理解，并开发了一个模块化、可扩展的Python库（RDTL），其中包含可配置的训练管道。我们对数据集质量的分析揭示了提高阿拉伯语定义构建质量的重要见解，从而制定了八项构建高质量逆向词典资源的具体标准。本研究在阿拉伯语计算语言学领域做出了重要贡献，并为阿拉伯语语言学习、学术写作和专业沟通提供了有价值的工具。', 'title_zh': '基于变压器的方法及数据集构建指南：阿拉伯语逆词典系统的进步'}
{'arxiv_id': 'arXiv:2504.21474', 'title': 'Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging', 'authors': 'Hadi Bayrami Asl Tekanlou, Jafar Razmara, Mahsa Sanaei, Mostafa Rahgouy, Hamed Babaei Giglou', 'link': 'https://arxiv.org/abs/2504.21474', 'abstract': "This paper presents our system, Homa, for SemEval-2025 Task 5: Subject Tagging, which focuses on automatically assigning subject labels to technical records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage OntoAligner, a modular ontology alignment toolkit, to address this task by integrating retrieval-augmented generation (RAG) techniques. Our approach formulates the subject tagging problem as an alignment task, where records are matched to GND categories based on semantic similarity. We evaluate OntoAligner's adaptability for subject indexing and analyze its effectiveness in handling multilingual records. Experimental results demonstrate the strengths and limitations of this method, highlighting the potential of alignment techniques for improving subject tagging in digital libraries.", 'abstract_zh': '本文介绍了我们为SemEval-2025 Task 5: Subject Tagging设计的系统Homa，该系统专注于使用Gemeinsame Normdatei (GND)分类法自动为TIBKAT的技术记录分配主题标签。我们利用OntoAligner这一模块化的本体对齐工具包，通过整合检索增强生成（RAG）技术来解决这一任务。我们的方法将主题标注问题形式化为对齐任务，其中记录基于语义相似性匹配到GND类别。我们评估了OntoAligner在主题索引中的适应性，并分析了其在处理多语言记录方面的有效性。实验结果展示了该方法的优势和局限性，突出了对齐技术在改进数字图书馆中的主题标注方面的潜力。', 'title_zh': 'Homa 于 SemEval-2025 任务 5：使用 OntoAligner 对齐图书管理员记录以进行主题标引'}
{'arxiv_id': 'arXiv:2504.21457', 'title': 'xEEGNet: Towards Explainable AI in EEG Dementia Classification', 'authors': 'Andrea Zanola, Louis Fabrice Tshimanga, Federico Del Pup, Marco Baiesi, Manfredo Atzori', 'link': 'https://arxiv.org/abs/2504.21457', 'abstract': 'This work presents xEEGNet, a novel, compact, and explainable neural network for EEG data analysis. It is fully interpretable and reduces overfitting through major parameter reduction. As an applicative use case, we focused on classifying common dementia conditions, Alzheimer\'s and frontotemporal dementia, versus controls. xEEGNet is broadly applicable to other neurological conditions involving spectral alterations. We initially used ShallowNet, a simple and popular model from the EEGNet-family. Its structure was analyzed and gradually modified to move from a "black box" to a more transparent model, without compromising performance. The learned kernels and weights were examined from a clinical standpoint to assess medical relevance. Model variants, including ShallowNet and the final xEEGNet, were evaluated using robust Nested-Leave-N-Subjects-Out cross-validation for unbiased performance estimates. Variability across data splits was explained using embedded EEG representations, grouped by class and set, with pairwise separability to quantify group distinction. Overfitting was assessed through training-validation loss correlation and training speed. xEEGNet uses only 168 parameters, 200 times fewer than ShallowNet, yet retains interpretability, resists overfitting, achieves comparable median performance (-1.5%), and reduces variability across splits. This variability is explained by embedded EEG representations: higher accuracy correlates with greater separation between test set controls and Alzheimer\'s cases, without significant influence from training data. xEEGNet\'s ability to filter specific EEG bands, learn band-specific topographies, and use relevant spectral features demonstrates its interpretability. While large deep learning models are often prioritized for performance, this study shows smaller architectures like xEEGNet can be equally effective in EEG pathology classification.', 'abstract_zh': '这种工作提出了一个新的、紧凑且可解释的神经网络xeEGNet，用于EEG数据分析。它具有完全可解释性并通过主要参数减少来降低过拟合。在应用案例中，我们专注于分类常见的痴呆症类型，阿尔茨海默病和额颞叶痴呆，与对照组。xeEGNet广泛适用于涉及频谱改变的其他神经系统疾病。我们最初使用了ShallowNet这一EEGNet家族中的一个简单且流行的模型，并对其结构进行了分析和逐步调整，使其从“黑盒”模型转变为更透明的模型，性能却不受影响。从临床角度来看，研究了学习到的核和权重的医学相关性。包括ShallowNet和最终的xeEGNet在内的模型变体通过嵌套留一交叉验证方法进行了评估，以获得无偏差的性能估算。通过分组的嵌入EEG表示和成对可分性来解释数据拆分之间的差异。通过训练-验证损失相关性和训练速度来评估过拟合。xeEGNet仅使用168个参数，比ShallowNet少200倍，但仍保持可解释性、抗过拟合、性能可媲美（-1.5%），并且降低了拆分间的变异性。这种变异性通过嵌入的EEG表示来解释：更高的准确率与测试集对照组和阿尔茨海默病病例之间的分离度增加相关，不受训练数据显著影响。xeEGNet能够过滤特定的EEG频率带、学习频率带特异性拓扑图以及使用相关频谱特征，展示了其可解释性。虽然大型深度学习模型通常优先考虑性能，但本研究表明，如xeEGNet这样的较小架构在EEG病理分类中同样有效。', 'title_zh': 'EEGNet: 向可解释人工智能在痴呆症EEG分类中的应用'}
{'arxiv_id': 'arXiv:2504.21435', 'title': 'SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding', 'authors': 'Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, ShaoGuo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang', 'link': 'https://arxiv.org/abs/2504.21435', 'abstract': "With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on \\textbf{standalone} videos and mainly assess ``visual elements'' like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a \\textbf{series}. To address this challenge, we propose \\textbf{SeriesBench}, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance model capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, \\textbf{PC-DCoT}. Extensive results on \\textbf{SeriesBench} indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while \\textbf{PC-DCoT} enables these MLLMs to achieve performance improvements. Overall, our \\textbf{SeriesBench} and \\textbf{PC-DCoT} highlight the critical necessity of advancing model capabilities to understand narrative-driven series, guiding the future development of MLLMs. SeriesBench is publicly available at this https URL.", 'abstract_zh': '随着多模态大规模语言模型（MLLMs）的迅速发展，越来越多的基准已建立起来，用于评估这些模型的视频理解能力。然而，这些基准主要关注**独立**视频，并主要评估“视觉元素”如人类动作和对象状态。实际上，当代视频通常包含复杂的连续叙述，通常以一系列的形式呈现。为了解决这一挑战，我们提出了**SeriesBench**基准，该基准包含105个精心策划的故事驱动系列，涵盖了28个需要深刻叙述理解的专门任务。具体来说，我们首先选择了一系列涵盖各种类型的戏剧系列。然后，我们引入了一种新颖的长跨度叙述注释方法，并结合全面信息转换方法，将手动注释转换为多种任务格式。为了进一步增强模型对系列内情节结构和人物关系的详细分析能力，我们提出了一种新颖的叙述推理框架**PC-DCoT**。在**SeriesBench**上的广泛实验结果表明，现有MLLMs在理解故事驱动的系列方面仍面临重大挑战，而**PC-DCoT**使得这些MLLMs能够实现性能提升。总体而言，我们的**SeriesBench**和**PC-DCoT**突显了提高模型能力以理解故事驱动的系列刻不容缓的重要性，指导MLLMs的未来发展方向。SeriesBench可以在以下网址获取：这个 https URL。', 'title_zh': 'SeriesBench：叙述驱动连续剧理解的基准测试'}
{'arxiv_id': 'arXiv:2504.21427', 'title': 'MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers', 'authors': 'Shermin Shahbazi, Mohammad-Reza Nasiri, Majid Ramezani', 'link': 'https://arxiv.org/abs/2504.21427', 'abstract': 'Accurate classification of EEG signals is crucial for brain-computer interfaces (BCIs) and neuroprosthetic applications, yet many existing methods fail to account for the non-Euclidean, manifold structure of EEG data, resulting in suboptimal performance. Preserving this manifold information is essential to capture the true geometry of EEG signals, but traditional classification techniques largely overlook this need. To this end, we propose MPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers), that introduces two key innovations: (1) a feature engineering phase that combines covariance matrices and Radial Basis Function (RBF) kernels to capture both linear and non-linear relationships among EEG channels, and (2) a clustering phase that employs a modified K-means algorithm tailored for the Riemannian manifold space, ensuring local geometric sensitivity. Ensembling multiple clustering-based classifiers, MPEC achieves superior results, validated by significant improvements on the BCI Competition IV dataset 2a.', 'abstract_zh': 'Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers', 'title_zh': 'MPEC：基于聚类分类器集成的流形保留EEG分类'}
{'arxiv_id': 'arXiv:2504.21415', 'title': 'Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges', 'authors': 'Yi Wang, Chengyv Wu, Yang Liao, Maowei You', 'link': 'https://arxiv.org/abs/2504.21415', 'abstract': "User authentication is essential to ensure secure access to computer systems, yet traditional methods face limitations in usability, cost, and security. Mouse dynamics authentication, based on the analysis of users' natural interaction behaviors with mouse devices, offers a cost-effective, non-intrusive, and adaptable solution. However, challenges remain in determining the optimal data volume, balancing accuracy and practicality, and effectively capturing temporal behavioral patterns. In this study, we propose a statistical method using Gaussian kernel density estimate (KDE) and Kullback-Leibler (KL) divergence to estimate the sufficient data volume for training authentication models. We introduce the Mouse Authentication Unit (MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for efficient and accurate behavioral representation. Furthermore, we design the Local-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet for local feature extraction and GRU for modeling long-term temporal dependencies. Taking the Balabit and DFL datasets as examples, we significantly reduced the data scale, particularly by a factor of 10 for the DFL dataset, greatly alleviating the training burden. Additionally, we determined the optimal input recognition unit length for the user authentication system on different datasets based on the slope of Approximate Entropy. Training with imbalanced samples, our model achieved a successful defense AUC 98.52% for blind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing the current sota performance.", 'abstract_zh': '基于鼠标动力学的用户认证：一种使用高斯核密度估计和Kullback-Leibler散度确定充分数据量的方法及局部时间鼠标认证框架', 'title_zh': '基于机器学习优化鼠标动态以实现用户身份认证：解决数据充足性、准确性和实用性权衡以及模型性能挑战'}
{'arxiv_id': 'arXiv:2504.21411', 'title': 'Galvatron: An Automatic Distributed System for Efficient Foundation Model Training', 'authors': 'Xinyi Liu, Yujie Wang, Shenhan Zhu, Fangcheng Fu, Qingshuo Liu, Guangming Lin, Bin Cui', 'link': 'https://arxiv.org/abs/2504.21411', 'abstract': "Galvatron is a distributed system for efficiently training large-scale Foundation Models. It overcomes the complexities of selecting optimal parallelism strategies by automatically identifying the most efficient hybrid strategy, incorporating data, tensor, pipeline, sharded data, and sequence parallelism, along with recomputation. The system's architecture includes a profiler for hardware and model analysis, a search engine for strategy optimization using decision trees and dynamic programming, and a runtime for executing these strategies efficiently. Benchmarking on various clusters demonstrates Galvatron's superior throughput compared to existing frameworks. This open-source system offers user-friendly interfaces and comprehensive documentation, making complex distributed training accessible and efficient. The source code of Galvatron is available at this https URL.", 'abstract_zh': 'Galvatron：一种高效训练大规模基础模型的分布式系统', 'title_zh': 'Galvatron: 一种高效的分布式系统自动训练基础模型'}
{'arxiv_id': 'arXiv:2504.21358', 'title': 'A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting', 'authors': 'Xiao Zheng, Saeed Asadi Bagloee, Majid Sarvi', 'link': 'https://arxiv.org/abs/2504.21358', 'abstract': "Traffic forecasting is vital for Intelligent Transportation Systems, for which Machine Learning (ML) methods have been extensively explored to develop data-driven Artificial Intelligence (AI) solutions. Recent research focuses on modelling spatial-temporal correlations for short-term traffic prediction, leaving the favourable long-term forecasting a challenging and open issue. This paper presents a comparative study on large-scale real-world signalized arterials and freeway traffic flow datasets, aiming to evaluate promising ML methods in the context of large forecasting horizons up to 30 days. Focusing on modelling capacity for temporal dynamics, we develop one ensemble ML method, eXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods, including Recurrent Neural Network (RNN)-based methods and the state-of-the-art Transformer-based method. Time embedding is leveraged to enhance their understanding of seasonality and event factors. Experimental results highlight that while the attention mechanism/Transformer framework is effective for capturing long-range dependencies in sequential data, as the forecasting horizon extends, the key to effective traffic forecasting gradually shifts from temporal dependency capturing to periodicity modelling. Time embedding is particularly effective in this context, helping naive RNN outperform Informer by 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust model, XGBoost, while learning solely from time features, performs competitively with DL methods. Moreover, we investigate the impacts of various factors like input sequence length, holiday traffic, data granularity, and training data size. The findings offer valuable insights and serve as a reference for future long-term traffic forecasting research and the improvement of AI's corresponding learning capabilities.", 'abstract_zh': '大规模实时信号交叉口和高速公路交通流数据的长程预测方法对比研究', 'title_zh': '深度学习与集成学习的对比研究：扩展交通预测的预见期'}
{'arxiv_id': 'arXiv:2504.21326', 'title': 'Q-function Decomposition with Intervention Semantics with Factored Action Spaces', 'authors': 'Junkyu Lee, Tian Gao, Elliot Nelson, Miao Liu, Debarun Bhattacharjya, Songtao Lu', 'link': 'https://arxiv.org/abs/2504.21326', 'abstract': 'Many practical reinforcement learning environments have a discrete factored action space that induces a large combinatorial set of actions, thereby posing significant challenges. Existing approaches leverage the regular structure of the action space and resort to a linear decomposition of Q-functions, which avoids enumerating all combinations of factored actions. In this paper, we consider Q-functions defined over a lower dimensional projected subspace of the original action space, and study the condition for the unbiasedness of decomposed Q-functions using causal effect estimation from the no unobserved confounder setting in causal statistics. This leads to a general scheme which we call action decomposed reinforcement learning that uses the projected Q-functions to approximate the Q-function in standard model-free reinforcement learning algorithms. The proposed approach is shown to improve sample complexity in a model-based reinforcement learning setting. We demonstrate improvements in sample efficiency compared to state-of-the-art baselines in online continuous control environments and a real-world offline sepsis treatment environment.', 'abstract_zh': '多种实际强化学习环境具有离散的事实性动作空间，导致大量的组合动作集，从而带来显著的挑战。现有的方法利用动作空间的有序结构，并采用Q函数的线性分解，从而避免枚举所有事实性动作的组合。在本文中，我们考虑定义在原动作空间低维投影子空间上的Q函数，并利用因果统计中的无未观测混杂变量假设下的因果效应估计研究分解后的Q函数的无偏性。这导致了一种一般方案，称为动作分解强化学习，利用投影后的Q函数近似标准模型自由强化学习算法中的Q函数。所提出的方法在基于模型的强化学习环境中展示了样本复杂度的改进。我们在线性连续控制环境和现实世界的脱机脓毒症治疗环境中展示了与最新基准相比的样本效率改进。', 'title_zh': '带有干预语义的事实化动作空间Q函数分解'}
{'arxiv_id': 'arXiv:2504.21323', 'title': 'How to Backdoor the Knowledge Distillation', 'authors': 'Chen Wu, Qian Ma, Prasenjit Mitra, Sencun Zhu', 'link': 'https://arxiv.org/abs/2504.21323', 'abstract': 'Knowledge distillation has become a cornerstone in modern machine learning systems, celebrated for its ability to transfer knowledge from a large, complex teacher model to a more efficient student model. Traditionally, this process is regarded as secure, assuming the teacher model is clean. This belief stems from conventional backdoor attacks relying on poisoned training data with backdoor triggers and attacker-chosen labels, which are not involved in the distillation process. Instead, knowledge distillation uses the outputs of a clean teacher model to guide the student model, inherently preventing recognition or response to backdoor triggers as intended by an attacker. In this paper, we challenge this assumption by introducing a novel attack methodology that strategically poisons the distillation dataset with adversarial examples embedded with backdoor triggers. This technique allows for the stealthy compromise of the student model while maintaining the integrity of the teacher model. Our innovative approach represents the first successful exploitation of vulnerabilities within the knowledge distillation process using clean teacher models. Through extensive experiments conducted across various datasets and attack settings, we demonstrate the robustness, stealthiness, and effectiveness of our method. Our findings reveal previously unrecognized vulnerabilities and pave the way for future research aimed at securing knowledge distillation processes against backdoor attacks.', 'abstract_zh': '知识蒸馏已成为现代机器学习系统中的基石，因其能够将大型复杂教师模型的知识转移到更高效的 student 模型中而备受推崇。传统上，这一过程被认为是安全的，假设教师模型是干净的。这一信念源于传统后门攻击依赖篡改训练数据和攻击者选择的标签，而这些数据和标签并不参与到知识蒸馏的过程中。相反，知识蒸馏利用干净教师模型的输出来指导 student 模型，从本质上防止 student 模型识别或响应攻击者意在植入的后门触发器。在本文中，我们通过引入一种新的攻击方法，该方法战略性地将带有后门触发器的对抗样本污染知识蒸馏数据集，挑战了这一假设。该技术能够在不破坏教师模型完整性的情况下秘密地渗透 student 模型。我们的创新方法是首次利用干净教师模型在知识蒸馏过程中成功利用漏洞的方法。通过在各种数据集和攻击设置下进行的广泛实验，我们展示了该方法的稳健性、隐蔽性和有效性。我们的研究结果揭示了之前未被认识的漏洞，并为未来研究确保知识蒸馏过程免受后门攻击铺平了道路。', 'title_zh': '如何在知识精炼中植入后门'}
{'arxiv_id': 'arXiv:2504.21297', 'title': 'Participatory AI, Public Sector AI, Differential Privacy, Conversational Interfaces, Explainable AI, Citizen Engagement in AI', 'authors': 'Wenjun Yang, Eyhab Al-Masri', 'link': 'https://arxiv.org/abs/2504.21297', 'abstract': 'This paper introduces a conversational interface system that enables participatory design of differentially private AI systems in public sector applications. Addressing the challenge of balancing mathematical privacy guarantees with democratic accountability, we propose three key contributions: (1) an adaptive $\\epsilon$-selection protocol leveraging TOPSIS multi-criteria decision analysis to align citizen preferences with differential privacy (DP) parameters, (2) an explainable noise-injection framework featuring real-time Mean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and (3) an integrated legal-compliance mechanism that dynamically modulates privacy budgets based on evolving regulatory constraints. Our results advance participatory AI practices by demonstrating how conversational interfaces can enhance public engagement in algorithmic privacy mechanisms, ensuring that privacy-preserving AI in public sector governance remains both mathematically robust and democratically accountable.', 'abstract_zh': '本文介绍了一种对话式接口系统，使公民能够在公共部门应用中参与设计差异隐私的AI系统。为平衡数学上的隐私保证与民主问责制度，我们提出三项关键贡献：（1）基于TOPSIS多准则决策分析的自适应$\\epsilon$选择协议，使公民偏好与差分隐私（DP）参数相契合；（2）一种可解释的噪声注入框架，配备实时均绝对误差（MAE）可视化和由GPT-4驱动的影响分析；（3）一种集成的法律合规机制，根据不断变化的监管约束动态调节隐私预算。我们的研究成果通过展示对话式接口如何增强公民对算法隐私机制的参与，推进了参与式AI实践，确保公共部门治理中的隐私保护AI在数学上保持稳健并符合民主问责要求。', 'title_zh': '参与式人工智能、公共部门人工智能、差异隐私、对话式接口、可解释人工智能、公民参与人工智能'}
{'arxiv_id': 'arXiv:2504.21296', 'title': 'Fairness in Graph Learning Augmented with Machine Learning: A Survey', 'authors': 'Renqiang Luo, Ziqi Xu, Xikun Zhang, Qing Qing, Huafei Huang, Enyan Dai, Zhe Wang, Bo Yang', 'link': 'https://arxiv.org/abs/2504.21296', 'abstract': 'Augmenting specialised machine learning techniques into traditional graph learning models has achieved notable success across various domains, including federated graph learning, dynamic graph learning, and graph transformers. However, the intricate mechanisms of these specialised techniques introduce significant challenges in maintaining model fairness, potentially resulting in discriminatory outcomes in high-stakes applications such as recommendation systems, disaster response, criminal justice, and loan approval. This paper systematically examines the unique fairness challenges posed by Graph Learning augmented with Machine Learning (GL-ML). It highlights the complex interplay between graph learning mechanisms and machine learning techniques, emphasising how the augmentation of machine learning both enhances and complicates fairness. Additionally, we explore four critical techniques frequently employed to improve fairness in GL-ML methods. By thoroughly investigating the root causes and broader implications of fairness challenges in this rapidly evolving field, this work establishes a robust foundation for future research and innovation in GL-ML fairness.', 'abstract_zh': '将专门的机器学习技术增加到传统图学习模型中已经在多个领域取得了显著成功，包括联邦图学习、动态图学习和图变压器。然而，这些专门技术的复杂机制引入了保持模型公平性的重大挑战，可能导致推荐系统、灾害响应、刑事司法和贷款审批等高风险应用中出现歧视性结果。本文系统性地探究了将机器学习融入图学习（GL-ML）所带来的独特公平性挑战。它强调了图学习机制与机器学习技术之间的复杂交互作用，并突出显示机器学习的增强既提升了又复杂化了公平性。此外，我们探讨了四种常用的、旨在改进GL-ML方法中公平性的关键技术。通过全面调查这一迅速发展的领域中公平性挑战的根本原因及更广泛的含义，本文为GL-ML公平性未来的研究和创新奠定了坚实基础。', 'title_zh': '图学习增强中的公平性：一个综述'}
{'arxiv_id': 'arXiv:2504.21289', 'title': 'Orthogonal Factor-Based Biclustering Algorithm (BCBOF) for High-Dimensional Data and Its Application in Stock Trend Prediction', 'authors': 'Yan Huang, Da-Qing Zhang', 'link': 'https://arxiv.org/abs/2504.21289', 'abstract': 'Biclustering is an effective technique in data mining and pattern recognition. Biclustering algorithms based on traditional clustering face two fundamental limitations when processing high-dimensional data: (1) The distance concentration phenomenon in high-dimensional spaces leads to data sparsity, rendering similarity measures ineffective; (2) Mainstream linear dimensionality reduction methods disrupt critical local structural patterns. To apply biclustering to high-dimensional datasets, we propose an orthogonal factor-based biclustering algorithm (BCBOF). First, we constructed orthogonal factors in the vector space of the high-dimensional dataset. Then, we performed clustering using the coordinates of the original data in the orthogonal subspace as clustering targets. Finally, we obtained biclustering results of the original dataset. Since dimensionality reduction was applied before clustering, the proposed algorithm effectively mitigated the data sparsity problem caused by high dimensionality. Additionally, we applied this biclustering algorithm to stock technical indicator combinations and stock price trend prediction. Biclustering results were transformed into fuzzy rules, and we incorporated profit-preserving and stop-loss rules into the rule set, ultimately forming a fuzzy inference system for stock price trend predictions and trading signals. To evaluate the performance of BCBOF, we compared it with existing biclustering methods using multiple evaluation metrics. The results showed that our algorithm outperformed other biclustering techniques. To validate the effectiveness of the fuzzy inference system, we conducted virtual trading experiments using historical data from 10 A-share stocks. The experimental results showed that the generated trading strategies yielded higher returns for investors.', 'abstract_zh': '基于正交因子的 biclustering 算法在高维数据中的应用及其用于股票技术指标组合和股票价格趋势预测中的模糊推理系统', 'title_zh': '基于正交因子的双聚类算法（BCBOF）及其在股票趋势预测中的应用'}
{'arxiv_id': 'arXiv:2504.21261', 'title': 'Multi-Domain Causal Discovery in Bijective Causal Models', 'authors': 'Kasra Jalaldoust, Saber Salehkaleybar, Negar Kiyavash', 'link': 'https://arxiv.org/abs/2504.21261', 'abstract': 'We consider the problem of causal discovery (a.k.a., causal structure learning) in a multi-domain setting. We assume that the causal functions are invariant across the domains, while the distribution of the exogenous noise may vary. Under causal sufficiency (i.e., no confounders exist), we show that the causal diagram can be discovered under less restrictive functional assumptions compared to previous work. What enables causal discovery in this setting is bijective generation mechanisms (BGM), which ensures that the functional relation between the exogenous noise $E$ and the endogenous variable $Y$ is bijective and differentiable in both directions at every level of the cause variable $X = x$. BGM generalizes a variety of models including additive noise model, LiNGAM, post-nonlinear model, and location-scale noise model. Further, we derive a statistical test to find the parents set of the target variable. Experiments on various synthetic and real-world datasets validate our theoretical findings.', 'abstract_zh': '多域环境下的因果发现：基于双射生成机制的因果结构学习', 'title_zh': '生物双射因果模型中的多领域因果发现'}
{'arxiv_id': 'arXiv:2504.21235', 'title': 'Efficient Quantum-Safe Homomorphic Encryption for Quantum Computer Programs', 'authors': 'Ben Goertzel', 'link': 'https://arxiv.org/abs/2504.21235', 'abstract': 'We present a lattice-based scheme for homomorphic evaluation of quantum programs and proofs that remains secure against quantum adversaries. Classical homomorphic encryption is lifted to the quantum setting by replacing composite-order groups with Module Learning-With-Errors (MLWE) lattices and by generalizing polynomial functors to bounded natural super functors (BNSFs). A secret depolarizing BNSF mask hides amplitudes, while each quantum state is stored as an MLWE ciphertext pair. We formalize security with the qIND-CPA game that allows coherent access to the encryption oracle and give a four-hybrid reduction to decisional MLWE.\nThe design also covers practical issues usually left open. A typed QC-bridge keeps classical bits produced by measurements encrypted yet still usable as controls, with weak-measurement semantics for expectation-value workloads. Encrypted Pauli twirls add circuit privacy. If a fixed knowledge base is needed, its axioms are shipped as MLWE "capsules"; the evaluator can use them but cannot read them. A rho-calculus driver schedules encrypted tasks across several QPUs and records an auditable trace on an RChain-style ledger.\nPerformance analysis shows that the extra lattice arithmetic fits inside today\'s QPU idle windows: a 100-qubit, depth-10^3 teleportation-based proof runs in about 10 ms, the public key (seed only) is 32 bytes, and even a CCA-level key stays below 300 kB. A photonic Dirac-3 prototype that executes homomorphic teleportation plus knowledge-base-relative amplitude checks appears feasible with current hardware. These results indicate that fully homomorphic, knowledge-base-aware quantum reasoning is compatible with near-term quantum clouds and standard post-quantum security assumptions.', 'abstract_zh': '一种基于格的量子程序同态评估方案及其安全性证明', 'title_zh': '量子安全同态加密在量子计算机程序中的高效实现'}
{'arxiv_id': 'arXiv:2504.21231', 'title': 'T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection', 'authors': 'Manikanta Varaganti, Amulya Vankayalapati, Nour Awad, Gregory R. Dion, Laura J. Brattain', 'link': 'https://arxiv.org/abs/2504.21231', 'abstract': 'Neck ultrasound (US) plays a vital role in airway management by providing non-invasive, real-time imaging that enables rapid and precise interventions. Deep learning-based anatomical landmark detection in neck US can further facilitate procedural efficiency. However, class imbalance within datasets, where key structures like tracheal rings and vocal folds are underrepresented, presents significant challenges for object detection models. To address this, we propose T2ID-CAS, a hybrid approach that combines a text-to-image latent diffusion model with class-aware sampling to generate high-quality synthetic samples for underrepresented classes. This approach, rarely explored in the ultrasound domain, improves the representation of minority classes. Experimental results using YOLOv9 for anatomical landmark detection in neck US demonstrated that T2ID-CAS achieved a mean Average Precision of 88.2, significantly surpassing the baseline of 66. This highlights its potential as a computationally efficient and scalable solution for mitigating class imbalance in AI-assisted ultrasound-guided interventions.', 'abstract_zh': '颈部超声（US）在呼吸道管理中发挥着重要作用，通过提供无创的实时成像，使快速而精确的干预成为可能。基于深度学习的颈部US解剖标志检测可以进一步提高程序效率。然而，数据集中的类别不平衡问题，如气管环和声带等关键结构的欠代表，对目标检测模型构成了重大挑战。为了解决这一问题，我们提出了T2ID-CAS，这是一种将文本到图像的潜扩散模型与类别意识采样相结合的混合方法，用于生成欠代表类别的高质量合成样本，这一方法在超声医学领域鲜有探索，能够改善少数类别的表示。实验结果表明，使用YOLOv9进行颈部US解剖标志检测时，T2ID-CAS达到了88.2的平均精确度均值，显著超过了基线模型66的精度，这突显了其作为减轻AI辅助超声引导干预中的类别不平衡问题的高效且可扩展解决方案的潜力。', 'title_zh': 'T2ID-CAS: 基于扩散模型和类意识采样的颈部超声解剖标志检测中类不平衡缓解方法'}
{'arxiv_id': 'arXiv:2504.21206', 'title': 'FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs', 'authors': 'Zihan Chen, Xingbo Fu, Yushun Dong, Jundong Li, Cong Shen', 'link': 'https://arxiv.org/abs/2504.21206', 'abstract': 'Federated Graph Learning (FGL) empowers clients to collaboratively train Graph neural networks (GNNs) in a distributed manner while preserving data privacy. However, FGL methods usually require that the graph data owned by all clients is homophilic to ensure similar neighbor distribution patterns of nodes. Such an assumption ensures that the learned knowledge is consistent across the local models from all clients. Therefore, these local models can be properly aggregated as a global model without undermining the overall performance. Nevertheless, when the neighbor distribution patterns of nodes vary across different clients (e.g., when clients hold graphs with different levels of heterophily), their local models may gain different and even conflict knowledge from their node-level predictive tasks. Consequently, aggregating these local models usually leads to catastrophic performance deterioration on the global model. To address this challenge, we propose FedHERO, an FGL framework designed to harness and share insights from heterophilic graphs effectively. At the heart of FedHERO is a dual-channel GNN equipped with a structure learner, engineered to discern the structural knowledge encoded in the local graphs. With this specialized component, FedHERO enables the local model for each client to identify and learn patterns that are universally applicable across graphs with different patterns of node neighbor distributions. FedHERO not only enhances the performance of individual client models by leveraging both local and shared structural insights but also sets a new precedent in this field to effectively handle graph data with various node neighbor distribution patterns. We conduct extensive experiments to validate the superior performance of FedHERO against existing alternatives.', 'abstract_zh': '联邦图学习（FGL）赋能客户端以分布式方式协作训练图神经网络（GNNs）的同时保护数据隐私。然而，FGL方法通常要求所有客户端拥有的图数据是同质的，以确保节点邻居分布模式的一致性。这种假设保证了从所有客户端学习到的知识在局部模型之间是一致的，因此可以在不损害整体性能的前提下正确聚合为全局模型。然而，当不同客户端的节点邻居分布模式有所差异（例如，当客户端持有不同亲疏程度的图时），它们的局部模型可能会从节点级预测任务中获得不同的甚至相冲突的知识。因此，这些局部模型通常会导致全局模型性能的灾难性下降。为了解决这一挑战，我们提出了FedHERO，一种设计用于有效利用和分享异质图见解的FGL框架。FedHERO的核心是一个配备结构学习器的双通道GNN，专门用于识别局部图中编码的结构知识。通过这一专业组件，FedHERO使每个客户端的局部模型能够识别并学习适用于不同节点邻居分布模式图的通用模式。FedHERO不仅通过利用局部和共享的结构见解来提升客户端模型的性能，还为处理各种节点邻居分布模式的图数据设定了新标准。我们进行了广泛的实验证明了FedHERO相对于现有替代方法的优越性能。', 'title_zh': 'FedHERO：异质图节点分类任务的联邦学习方法'}
{'arxiv_id': 'arXiv:2504.21195', 'title': 'Turning Up the Heat: Assessing 2-m Temperature Forecast Errors in AI Weather Prediction Models During Heat Waves', 'authors': 'Kelsey E. Ennis, Elizabeth A. Barnes, Marybeth C. Arcodia, Martin A. Fernandez, Eric D. Maloney', 'link': 'https://arxiv.org/abs/2504.21195', 'abstract': "Extreme heat is the deadliest weather-related hazard in the United States. Furthermore, it is increasing in intensity, frequency, and duration, making skillful forecasts vital to protecting life and property. Traditional numerical weather prediction (NWP) models struggle with extreme heat for medium-range and subseasonal-to-seasonal (S2S) timescales. Meanwhile, artificial intelligence-based weather prediction (AIWP) models are progressing rapidly. However, it is largely unknown how well AIWP models forecast extremes, especially for medium-range and S2S timescales. This study investigates 2-m temperature forecasts for 60 heat waves across the four boreal seasons and over four CONUS regions at lead times up to 20 days, using two AIWP models (Google GraphCast and Pangu-Weather) and one traditional NWP model (NOAA United Forecast System Global Ensemble Forecast System (UFS GEFS)). First, case study analyses show that both AIWP models and the UFS GEFS exhibit consistent cold biases on regional scales in the 5-10 days of lead time before heat wave onset. GraphCast is the more skillful AIWP model, outperforming UFS GEFS and Pangu-Weather in most locations. Next, the two AIWP models are isolated and analyzed across all heat waves and seasons, with events split among the model's testing (2018-2023) and training (1979-2017) periods. There are cold biases before and during the heat waves in both models and all seasons, except Pangu-Weather in winter, which exhibits a mean warm bias before heat wave onset. Overall, results offer encouragement that AIWP models may be useful for medium-range and S2S predictability of extreme heat.", 'abstract_zh': '极端高温是与天气相关的 deadliest 危害，在美国。此外，极端高温的强度、频率和持续时间都在增加，使得准确的预报对保护生命和财产至关重要。传统的数值天气预报（NWP）模型在中等范围和亚季节到季节尺度（S2S）上难以应对极端高温。与此同时，基于人工智能的天气预报（AIWP）模型正在迅速发展。然而，AIWP 模型对极端事件的预报能力，特别是在中等范围和 S2S 尺度上，仍然知之甚少。本研究使用两种 AIWP 模型（Google GraphCast 和 Pangu-Weather）和一个传统 NWP 模型（NOAA 统一预报系统全球集合预报系统 UFS GEFS），分析了从 0 到 20 天的四个北极季节和四个CONUS 区域内的 60 次高温事件的 2 米温度预报。研究表明，AIWP 模型和 UFS GEFS 在高温事件前 5-10 天的区域尺度上都存在一致的冷偏差。GraphCast 在大多数地点表现更为出色，优于 UFS GEFS 和 Pangu-Weather。随后，本文将两种 AIWP 模型分别在所有高温事件和季节中进行分析，并将事件分为模型测试期（2018-2023）和训练期（1979-2017）。两个模型和所有季节在高温事件前后都存在冷偏差，除了冬季的 Pangu-Weather 表现出高温前的平均暖偏差。总体而言，研究结果表明 AIWP 模型可能在中等范围和 S2S 极端高温的可预报性方面具有应用潜力。', 'title_zh': '提高误差标准：评估热浪期间AI气象预测模型的2米气温预报误差'}
{'arxiv_id': 'arXiv:2504.21189', 'title': "Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions", 'authors': 'Gulsah Hancerliogullari Koksalmis, Bulent Soykan, Laura J. Brattain, Hsin-Hsiung Huang', 'link': 'https://arxiv.org/abs/2504.21189', 'abstract': "Alzheimer's Disease (AD) is marked by significant inter-individual variability in its progression, complicating accurate prognosis and personalized care planning. This heterogeneity underscores the critical need for predictive models capable of forecasting patient-specific disease trajectories. Artificial Intelligence (AI) offers powerful tools to address this challenge by analyzing complex, multi-modal, and longitudinal patient data. This paper provides a comprehensive survey of AI methodologies applied to personalized AD progression prediction. We review key approaches including state-space models for capturing temporal dynamics, deep learning techniques like Recurrent Neural Networks for sequence modeling, Graph Neural Networks (GNNs) for leveraging network structures, and the emerging concept of AI-driven digital twins for individualized simulation. Recognizing that data limitations often impede progress, we examine common challenges such as high dimensionality, missing data, and dataset imbalance. We further discuss AI-driven mitigation strategies, with a specific focus on synthetic data generation using Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) to augment and balance datasets. The survey synthesizes the strengths and limitations of current approaches, emphasizing the trend towards multimodal integration and the persistent need for model interpretability and generalizability. Finally, we identify critical open challenges, including robust external validation, clinical integration, and ethical considerations, and outline promising future research directions such as hybrid models, causal inference, and federated learning. This review aims to consolidate current knowledge and guide future efforts in developing clinically relevant AI tools for personalized AD prognostication.", 'abstract_zh': '阿尔茨海默病（AD）的进展存在显著的个体间差异，这 complicating 了准确的预后和个性化护理计划的制定。这种异质性突显了需要开发能够预测患者特定疾病轨迹的预测模型的迫切需求。人工智能（AI）通过分析复杂的多模态和纵向患者数据提供了强有力的工具来应对这一挑战。本文提供了 AI 方法在个性化 AD 进展预测中的全面综述。我们回顾了包括状态空间模型在内的关键方法，用于捕捉时间动态；深度学习技术如循环神经网络进行序列建模；图神经网络（GNNs）用于利用网络结构；以及新兴的 AI 驱动数字孪生概念用于个性化仿真。认识到数据限制常常阻碍进展，我们探讨了诸如高维性、缺失数据和数据集不平衡等常见挑战。我们进一步讨论了通过使用变分自编码器（VAEs）和生成对抗网络（GANs）生成合成数据来驱动的 AI 算法，以扩充和平衡数据集的策略。综述总结了当前方法的优势和局限性，强调了多模态整合的趋势以及持续需要模型的可解释性和泛化性。最后，我们确定了一些关键的开放挑战，包括稳健的外部验证、临床整合和伦理考量，并概述了混合模型、因果推断和联邦学习等有前途的未来研究方向。本文旨在汇总当前知识并指导未来为个性化 AD 预后开发临床相关 AI 工具的努力。', 'title_zh': '人工智能在阿尔茨海默病个性化进展预测中的应用：方法、数据挑战及未来方向'}
{'arxiv_id': 'arXiv:2504.21155', 'title': 'Evaluation and Verification of Physics-Informed Neural Models of the Grad-Shafranov Equation', 'authors': 'Fauzan Nazranda Rizqa, Matthew Hole, Charles Gretton', 'link': 'https://arxiv.org/abs/2504.21155', 'abstract': 'Our contributions are motivated by fusion reactors that rely on maintaining magnetohydrodynamic (MHD) equilibrium, where the balance between plasma pressure and confining magnetic fields is required for stable operation. In axisymmetric tokamak reactors in particular, and under the assumption of toroidal symmetry, this equilibrium can be mathematically modelled using the Grad-Shafranov Equation (GSE). Recent works have demonstrated the potential of using Physics-Informed Neural Networks (PINNs) to model the GSE. Existing studies did not examine realistic scenarios in which a single network generalizes to a variety of boundary conditions. Addressing that limitation, we evaluate a PINN architecture that incorporates boundary points as network inputs. Additionally, we compare PINN model accuracy and inference speeds with a Fourier Neural Operator (FNO) model. Finding the PINN model to be the most performant, and accurate in our setting, we use the network verification tool Marabou to perform a range of verification tasks. Although we find some discrepancies between evaluations of the networks natively in PyTorch, compared to via Marabou, we are able to demonstrate useful and practical verification workflows. Our study is the first investigation of verification of such networks.', 'abstract_zh': '我们的贡献源于磁聚变反应堆对维持磁流体动力学（MHD）平衡的需求，其中等离子体压力和约束磁场所的平衡是确保稳定运行所必需的。特别是在具有托卡马克对称性的轴对称装置中，假设存在环形对称性，这种平衡可以用Grad-Shafranov方程（GSE）进行数学建模。近期研究表明，可以利用物理信息神经网络（PINNs）来建模GSE。现有研究没有考察单个网络在多种边界条件下的泛化能力。为解决这一限制，我们评估了一种将边界点作为网络输入的PINN架构。此外，我们还将PINN模型的准确性和推理速度与Fourier神经算子（FNO）模型进行了比较。由于发现PINN模型在我们的设置中表现出更好的性能和更高的准确性，我们使用了网络验证工具Marabou进行一系列验证任务。尽管我们在PyTorch中直接评估网络和通过Marabou评估之间发现了一些差异，但我们能够展示有用的、实用的验证工作流程。我们的研究是首次探讨此类网络验证的研究。', 'title_zh': 'Grad-Shafranov 方程的物理信息神经网络模型的评估与验证'}
{'arxiv_id': 'arXiv:2504.21154', 'title': 'Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis', 'authors': 'Muhammad Turab, Philippe Colantoni, Damien Muselet, Alain Tremeau', 'link': 'https://arxiv.org/abs/2504.21154', 'abstract': 'This paper presents a novel framework for emotion recognition in contemporary dance by improving existing Laban Movement Analysis (LMA) feature descriptors and introducing robust, novel descriptors that capture both quantitative and qualitative aspects of the movement. Our approach extracts expressive characteristics from 3D keypoints data of professional dancers performing contemporary dance under various emotional states, and trains multiple classifiers, including Random Forests and Support Vector Machines. Additionally, we provide in-depth explanation of features and their impact on model predictions using explainable machine learning methods. Overall, our study improves emotion recognition in contemporary dance and offers promising applications in performance analysis, dance training, and human--computer interaction, with a highest accuracy of 96.85\\%.', 'abstract_zh': '本文提出了一种改进 Laban 运动分析 (LMA) 特征描述子并引入 robust、新颖描述子以捕捉运动的定量和定性方面的新颖框架，用于当代舞的情感识别。我们的方法从处于不同情绪状态的专业舞者进行当代舞的 3D 关键点数据中提取表现性特征，并训练包括随机森林和支持向量机在内的多个分类器。此外，我们使用可解释的机器学习方法提供了特征及其对模型预测影响的深入解释。总体而言，本研究提升了当代舞的情感识别，并在表演分析、舞蹈训练和人机交互方面具有前景，最高准确率为 96.85%。', 'title_zh': '当代舞蹈表演中基于拉班运动分析的情绪识别'}
{'arxiv_id': 'arXiv:2504.21152', 'title': 'SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression', 'authors': 'Shayan Alahyari, Mike Domaratzki', 'link': 'https://arxiv.org/abs/2504.21152', 'abstract': "Imbalanced regression refers to prediction tasks where the target variable is skewed. This skewness hinders machine learning models, especially neural networks, which concentrate on dense regions and therefore perform poorly on underrepresented (minority) samples. Despite the importance of this problem, only a few methods have been proposed for imbalanced regression. Many of the available solutions for imbalanced regression adapt techniques from the class imbalance domain, such as linear interpolation and the addition of Gaussian noise, to create synthetic data in sparse regions. However, in many cases, the underlying distribution of the data is complex and non-linear. Consequently, these approaches generate synthetic samples that do not accurately represent the true feature-target relationship. To overcome these limitations, we propose SMOGAN, a two-step oversampling framework for imbalanced regression. In Stage 1, an existing oversampler generates initial synthetic samples in sparse target regions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves as SMOGAN's filtering layer and refines these samples via adversarial loss augmented with a Maximum Mean Discrepancy objective, aligning them with the true joint feature-target distribution. Extensive experiments on 23 imbalanced datasets show that SMOGAN consistently outperforms the default oversampling method without the DistGAN filtering layer.", 'abstract_zh': '不平衡回归是指目标变量分布偏斜的预测任务。这种偏斜阻碍了机器学习模型，尤其是神经网络，这些模型倾向于关注密集区域，因此在少数样本上表现不佳。尽管这个问题很重要，但仅提出了一小部分方法来处理不平衡回归问题。许多现有的不平衡回归解决方案将类不平衡领域的技术（如线性插值和高斯噪声添加）适应为生成稀疏区域的合成数据。然而，在许多情况下，数据的真实分布是复杂且非线性的。因此，这些方法生成的合成样本并不能准确地代表真实特征-目标关系。为了克服这些局限性，我们提出了SMOGAN，一种两阶段的过采样框架，用于不平衡回归。在第一阶段，一个现有的过采样器生成初始的合成样本，以填补目标变量稀疏区域。在第二阶段，我们引入了DistGAN，这是一种分布感知的GAN，作为SMOGAN的过滤层，并通过对抗损失以及最大均值偏差目标进行优化，以校准这些样本，并使其与真实特征-目标联合分布一致。在23个不平衡数据集上的广泛实验表明，与未包含DistGAN过滤层的默认过采样方法相比，SMOGAN始终表现出更优的性能。', 'title_zh': 'SMOGAN：带有GAN精炼的少数派合成过采样方法用于 imbalance 回归'}
{'arxiv_id': 'arXiv:2504.21099', 'title': 'A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning', 'authors': 'Jieming Bian, Yuanzhe Peng, Lei Wang, Yin Huang, Jie Xu', 'link': 'https://arxiv.org/abs/2504.21099', 'abstract': 'Foundation models have revolutionized artificial intelligence by providing robust, versatile architectures pre-trained on large-scale datasets. However, adapting these massive models to specific downstream tasks requires fine-tuning, which can be prohibitively expensive in computational resources. Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge by selectively updating only a small subset of parameters. Meanwhile, Federated Learning (FL) enables collaborative model training across distributed clients without sharing raw data, making it ideal for privacy-sensitive applications. This survey provides a comprehensive review of the integration of PEFT techniques within federated learning environments. We systematically categorize existing approaches into three main groups: Additive PEFT (which introduces new trainable parameters), Selective PEFT (which fine-tunes only subsets of existing parameters), and Reparameterized PEFT (which transforms model architectures to enable efficient updates). For each category, we analyze how these methods address the unique challenges of federated settings, including data heterogeneity, communication efficiency, computational constraints, and privacy concerns. We further organize the literature based on application domains, covering both natural language processing and computer vision tasks. Finally, we discuss promising research directions, including scaling to larger foundation models, theoretical analysis of federated PEFT methods, and sustainable approaches for resource-constrained environments.', 'abstract_zh': '基于参数高效微调的联邦学习综述：挑战与进展', 'title_zh': '联邦学习中基础模型参数高效微调综述'}
{'arxiv_id': 'arXiv:2504.21072', 'title': 'Erased but Not Forgotten: How Backdoors Compromise Concept Erasure', 'authors': 'Jonas Henry Grebe, Tobias Braun, Marcus Rohrbach, Anna Rohrbach', 'link': 'https://arxiv.org/abs/2504.21072', 'abstract': "The expansion of large-scale text-to-image diffusion models has raised growing concerns about their potential to generate undesirable or harmful content, ranging from fabricated depictions of public figures to sexually explicit images. To mitigate these risks, prior work has devised machine unlearning techniques that attempt to erase unwanted concepts through fine-tuning. However, in this paper, we introduce a new threat model, Toxic Erasure (ToxE), and demonstrate how recent unlearning algorithms, including those explicitly designed for robustness, can be circumvented through targeted backdoor attacks. The threat is realized by establishing a link between a trigger and the undesired content. Subsequent unlearning attempts fail to erase this link, allowing adversaries to produce harmful content. We instantiate ToxE via two established backdoor attacks: one targeting the text encoder and another manipulating the cross-attention layers. Further, we introduce Deep Intervention Score-based Attack (DISA), a novel, deeper backdoor attack that optimizes the entire U-Net using a score-based objective, improving the attack's persistence across different erasure methods. We evaluate five recent concept erasure methods against our threat model. For celebrity identity erasure, our deep attack circumvents erasure with up to 82% success, averaging 57% across all erasure methods. For explicit content erasure, ToxE attacks can elicit up to 9 times more exposed body parts, with DISA yielding an average increase by a factor of 2.9. These results highlight a critical security gap in current unlearning strategies.", 'abstract_zh': '有毒擦除：对抗性回门攻击在大规模文本生成图像模型中的威胁', 'title_zh': '被擦除但未被遗忘：后门如何破坏概念擦除'}
{'arxiv_id': 'arXiv:2504.21066', 'title': 'A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection', 'authors': 'Andreas Karathanasis, John Violos, Ioannis Kompatsiaris, Symeon Papadopoulos', 'link': 'https://arxiv.org/abs/2504.21066', 'abstract': 'Training and deploying deepfake detection models on edge devices offers the advantage of maintaining data privacy and confidentiality by processing it close to its source. However, this approach is constrained by the limited computational and memory resources available at the edge. To address this challenge, we explore compression techniques to reduce computational demands and inference time, alongside transfer learning methods to minimize training overhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate the effectiveness of pruning, knowledge distillation (KD), quantization, fine-tuning, and adapter-based techniques. Our experimental results demonstrate that both compression and transfer learning can be effectively achieved, even with a high compression level of 90%, remaining at the same performance level when the training and validation data originate from the same DeepFake model. However, when the testing dataset is generated by DeepFake models not present in the training set, a domain generalization issue becomes evident.', 'abstract_zh': '在边缘设备上训练和部署深度假脸检测模型的优势在于通过靠近数据源进行处理来维护数据隐私和保密性。然而，这种方法受限于边缘设备上可用的有限计算和内存资源。为了解决这一挑战，我们探索了压缩技术以减少计算需求和推断时间，并结合迁移学习方法以最小化训练开销。使用Synthbuster、RAISE和ForenSynths数据集，我们评估了剪枝、知识蒸馏（KD）、量化、微调和基于适配器的技术的有效性。实验结果表明，即使在90%的高压缩水平下，压缩和迁移学习也能有效地实现，并且在训练和验证数据来自同一深度假脸模型的情况下保持相同性能水平。然而，当测试数据集由不在训练集中出现的深度假脸模型生成时，会出现领域泛化问题。', 'title_zh': '压缩与迁移学习技术在DeepFake检测中的 briefly回顾'}
{'arxiv_id': 'arXiv:2504.21065', 'title': 'A 3D pocket-aware and affinity-guided diffusion model for lead optimization', 'authors': 'Anjie Qiao, Junjie Xie, Weifeng Huang, Hao Zhang, Jiahua Rao, Shuangjia Zheng, Yuedong Yang, Zhen Wang, Guo-Bo Li, Jinping Lei', 'link': 'https://arxiv.org/abs/2504.21065', 'abstract': 'Molecular optimization, aimed at improving binding affinity or other molecular properties, is a crucial task in drug discovery that often relies on the expertise of medicinal chemists. Recently, deep learning-based 3D generative models showed promise in enhancing the efficiency of molecular optimization. However, these models often struggle to adequately consider binding affinities with protein targets during lead optimization. Herein, we propose a 3D pocket-aware and affinity-guided diffusion model, named Diffleop, to optimize molecules with enhanced binding affinity. The model explicitly incorporates the knowledge of protein-ligand binding affinity to guide the denoising sampling for molecule generation with high affinity. The comprehensive evaluations indicated that Diffleop outperforms baseline models across multiple metrics, especially in terms of binding affinity.', 'abstract_zh': '基于3D口袋意识和亲和力导向的扩散模型Diffleop优化具有增强亲和力的分子', 'title_zh': '基于口袋意识和亲和力引导的三维扩散模型用于先导优化'}
{'arxiv_id': 'arXiv:2504.21064', 'title': 'Frequency Feature Fusion Graph Network For Depression Diagnosis Via fNIRS', 'authors': 'Chengkai Yang, Xingping Dong, Xiaofen Zong', 'link': 'https://arxiv.org/abs/2504.21064', 'abstract': 'Data-driven approaches for depression diagnosis have emerged as a significant research focus in neuromedicine, driven by the development of relevant datasets. Recently, graph neural network (GNN)-based models have gained widespread adoption due to their ability to capture brain channel functional connectivity from both spatial and temporal perspectives. However, their effectiveness is hindered by the absence of a robust temporal biomarker. In this paper, we introduce a novel and effective biomarker for depression diagnosis by leveraging the discrete Fourier transform (DFT) and propose a customized graph network architecture based on Temporal Graph Convolutional Network (TGCN). Our model was trained on a dataset comprising 1,086 subjects, which is over 10 times larger than previous datasets in the field of depression diagnosis. Furthermore, to align with medical requirements, we performed propensity score matching (PSM) to create a refined subset, referred to as the PSM dataset. Experimental results demonstrate that incorporating our newly designed biomarker enhances the representation of temporal characteristics in brain channels, leading to improved F1 scores in both the real-world dataset and the PSM dataset. This advancement has the potential to contribute to the development of more effective depression diagnostic tools. In addition, we used SHapley Additive exPlaination (SHAP) to validate the interpretability of our model, ensuring its practical applicability in medical settings.', 'abstract_zh': '基于数据驱动的方法在神经医学中用于抑郁症诊断的研究进展：利用离散傅里叶变换和定制化时间图卷积网络的新型生物标志器及其应用', 'title_zh': '基于fNIRS的抑郁症诊断的频率特征融合图网络'}
{'arxiv_id': 'arXiv:2504.21063', 'title': 'Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization', 'authors': 'Shuai Gong, Chaoran Cui, Xiaolin Dong, Xiushan Nie, Lei Zhu, Xiaojun Chang', 'link': 'https://arxiv.org/abs/2504.21063', 'abstract': "Federated domain generalization (FedDG) aims to learn a globally generalizable model from decentralized clients with heterogeneous data while preserving privacy. Recent studies have introduced prompt learning to adapt vision-language models (VLMs) in FedDG by learning a single global prompt. However, such a one-prompt-fits-all learning paradigm typically leads to performance degradation on personalized samples. Although the mixture of experts (MoE) offers a promising solution for specialization, existing MoE-based methods suffer from coarse image-level expert assignment and high communication costs from parameterized routers. To address these limitations, we propose TRIP, a Token-level prompt mixture with parameter-free routing framework for FedDG, which treats multiple prompts as distinct experts. Unlike existing image-level routing designs, TRIP assigns different tokens within an image to specific experts. To ensure communication efficiency, TRIP incorporates a parameter-free routing mechanism based on token clustering and optimal transport. The instance-specific prompt is then synthesized by aggregating experts, weighted by the number of tokens assigned to each. Additionally, TRIP develops an unbiased learning strategy for prompt experts, leveraging the VLM's zero-shot generalization capability. Extensive experiments across four benchmarks demonstrate that TRIP achieves optimal generalization results, with communication of only 1K parameters per round. Our code is available at this https URL.", 'abstract_zh': '联邦领域泛化（FedDG）的目标是以分散式客户端和异质数据为基础学习一个全局泛化模型，同时保护隐私。最近的研究通过学习单一全局提示将提示学习引入到了FedDG中，以适应视觉语言模型（VLMs）。然而，这种单一提示适应所有样本的学习范式通常会导致个性化样本性能下降。虽然专家组合（MoE）提供了专业知识化的一种有前景的解决方案，但现有的基于MoE的方法由于粗粒度的图像级专家分配和参数化路由器带来的高通信成本而受到限制。为了解决这些限制，我们提出了TRIP，即一个基于标记级别的提示组合与无参数路由框架，将多个提示视作不同的专家。与现有的图像级路由设计不同，TRIP 将图像内的不同标记分配给特定的专家。为了确保通信效率，TRIP 结合了基于标记聚类和最优传输的无参数路由机制。通过根据分配给每个专家的标记数量进行加权，合成特定实例的提示。此外，TRIP 开发了一种利用VLM零样本泛化能力的无偏学习策略。在四个基准系统的广泛实验中，TRIP 获得了最优泛化结果，每次通信仅需要传递1K参数。我们的代码可在以下链接获取。', 'title_zh': '基于token级提示混合的无参路由联邦领域泛化'}
{'arxiv_id': 'arXiv:2504.21055', 'title': 'Modeling and Performance Analysis for Semantic Communications Based on Empirical Results', 'authors': 'Shuai Ma, Bin Shen, Chuanhui Zhang, Youlong Wu, Hang Li, Shiyin Li, Guangming Shi, Naofal Al-Dhahir', 'link': 'https://arxiv.org/abs/2504.21055', 'abstract': 'Due to the black-box characteristics of deep learning based semantic encoders and decoders, finding a tractable method for the performance analysis of semantic communications is a challenging problem. In this paper, we propose an Alpha-Beta-Gamma (ABG) formula to model the relationship between the end-to-end measurement and SNR, which can be applied for both image reconstruction tasks and inference tasks. Specifically, for image reconstruction tasks, the proposed ABG formula can well fit the commonly used DL networks, such as SCUNet, and Vision Transformer, for semantic encoding with the multi scale-structural similarity index measure (MS-SSIM) measurement. Furthermore, we find that the upper bound of the MS-SSIM depends on the number of quantized output bits of semantic encoders, and we also propose a closed-form expression to fit the relationship between the MS-SSIM and quantized output bits. To the best of our knowledge, this is the first theoretical expression between end-to-end performance metrics and SNR for semantic communications. Based on the proposed ABG formula, we investigate an adaptive power control scheme for semantic communications over random fading channels, which can effectively guarantee quality of service (QoS) for semantic communications, and then design the optimal power allocation scheme to maximize the energy efficiency of the semantic communication system. Furthermore, by exploiting the bisection algorithm, we develop the power allocation scheme to maximize the minimum QoS of multiple users for OFDMA downlink semantic communication Extensive simulations verify the effectiveness and superiority of the proposed ABG formula and power allocation schemes.', 'abstract_zh': '基于深度学习的语义编码器和解码器的黑箱特性，对语义通信性能分析的可溯性方法是一个具挑战性的问题。本文提出了一种Alpha-Beta-Gamma (ABG) 公式来建模端到端测量与信噪比之间的关系，该公式适用于图像重构任务和推理任务。具体而言，在图像重构任务中，所提出的ABG公式可以很好地拟合常用的DL网络，如SCUNet和Vision Transformer，并使用多尺度结构相似性指数度量（MS-SSIM）。此外，我们发现MS-SSIM的上界取决于语义编码器的量化输出位数，并提出了一种闭式表达式来拟合MS-SSIM与量化输出位数之间的关系。据我们所知，这是首次在语义通信中理论表达端到端性能指标与信噪比之间的关系。基于所提出的ABG公式，研究了随机衰落信道中语义通信的自适应功率控制方案，可以有效保证语义通信的质量服务（QoS），并设计了优化功率分配方案以最大化语义通信系统的能量效率。此外，通过利用二分算法，开发了多用户OFDMA下行语义通信中同时最大化最小QoS的功率分配方案。广泛的仿真验证了所提ABG公式和功率分配方案的有效性和优越性。', 'title_zh': '基于经验结果的语义通信建模与性能分析'}
{'arxiv_id': 'arXiv:2504.21054', 'title': 'FFCBA: Feature-based Full-target Clean-label Backdoor Attacks', 'authors': 'Yangxu Yin, Honglong Chen, Yudong Gao, Peng Sun, Liantao Wu, Zhe Li, Weifeng Liu', 'link': 'https://arxiv.org/abs/2504.21054', 'abstract': "Backdoor attacks pose a significant threat to deep neural networks, as backdoored models would misclassify poisoned samples with specific triggers into target classes while maintaining normal performance on clean samples. Among these, multi-target backdoor attacks can simultaneously target multiple classes. However, existing multi-target backdoor attacks all follow the dirty-label paradigm, where poisoned samples are mislabeled, and most of them require an extremely high poisoning rate. This makes them easily detectable by manual inspection. In contrast, clean-label attacks are more stealthy, as they avoid modifying the labels of poisoned samples. However, they generally struggle to achieve stable and satisfactory attack performance and often fail to scale effectively to multi-target attacks. To address this issue, we propose the Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) which consists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and Feature-Migrating Backdoor Attacks (FMBA). FSBA leverages class-conditional autoencoders to generate noise triggers that align perturbed in-class samples with the original category's features, ensuring the effectiveness, intra-class consistency, inter-class specificity and natural-feature correlation of triggers. While FSBA supports swift and efficient attacks, its cross-model attack capability is relatively weak. FMBA employs a two-stage class-conditional autoencoder training process that alternates between using out-of-class samples and in-class samples. This allows FMBA to generate triggers with strong target-class features, making it highly effective for cross-model attacks. We conduct experiments on multiple datasets and models, the results show that FFCBA achieves outstanding attack performance and maintains desirable robustness against the state-of-the-art backdoor defenses.", 'abstract_zh': '基于特征的全目标清洁标签后门攻击（FFCBA）', 'title_zh': 'FFCBA：基于特征的全目标干净标签后门攻击'}
{'arxiv_id': 'arXiv:2504.21052', 'title': 'SFIBA: Spatial-based Full-target Invisible Backdoor Attacks', 'authors': 'Yangxu Yin, Honglong Chen, Yudong Gao, Peng Sun, Zhishuai Li, Weifeng Liu', 'link': 'https://arxiv.org/abs/2504.21052', 'abstract': "Multi-target backdoor attacks pose significant security threats to deep neural networks, as they can preset multiple target classes through a single backdoor injection. This allows attackers to control the model to misclassify poisoned samples with triggers into any desired target class during inference, exhibiting superior attack performance compared with conventional backdoor attacks. However, existing multi-target backdoor attacks fail to guarantee trigger specificity and stealthiness in black-box settings, resulting in two main issues. First, they are unable to simultaneously target all classes when only training data can be manipulated, limiting their effectiveness in realistic attack scenarios. Second, the triggers often lack visual imperceptibility, making poisoned samples easy to detect. To address these problems, we propose a Spatial-based Full-target Invisible Backdoor Attack, called SFIBA. It restricts triggers for different classes to specific local spatial regions and morphologies in the pixel space to ensure specificity, while employing a frequency-domain-based trigger injection method to guarantee stealthiness. Specifically, for injection of each trigger, we first apply fast fourier transform to obtain the amplitude spectrum of clean samples in local spatial regions. Then, we employ discrete wavelet transform to extract the features from the amplitude spectrum and use singular value decomposition to integrate the trigger. Subsequently, we selectively filter parts of the trigger in pixel space to implement trigger morphology constraints and adjust injection coefficients based on visual effects. We conduct experiments on multiple datasets and models. The results demonstrate that SFIBA can achieve excellent attack performance and stealthiness, while preserving the model's performance on benign samples, and can also bypass existing backdoor defenses.", 'abstract_zh': '基于空间的全目标隐形后门攻击：SFIBA', 'title_zh': '基于空间的全目标隐形后门攻击'}
{'arxiv_id': 'arXiv:2504.21049', 'title': 'Phishing URL Detection using Bi-LSTM', 'authors': 'Sneha Baskota', 'link': 'https://arxiv.org/abs/2504.21049', 'abstract': "Phishing attacks threaten online users, often leading to data breaches, financial losses, and identity theft. Traditional phishing detection systems struggle with high false positive rates and are usually limited by the types of attacks they can identify. This paper proposes a deep learning-based approach using a Bidirectional Long Short-Term Memory (Bi-LSTM) network to classify URLs into four categories: benign, phishing, defacement, and malware. The model leverages sequential URL data and captures contextual information, improving the accuracy of phishing detection. Experimental results on a dataset comprising over 650,000 URLs demonstrate the model's effectiveness, achieving 97% accuracy and significant improvements over traditional techniques.", 'abstract_zh': '基于双向长短期记忆网络的深度学习方法用于URL分类及钓鱼攻击检测', 'title_zh': '使用双向LSTM检测钓鱼URL'}
{'arxiv_id': 'arXiv:2504.21048', 'title': 'Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey', 'authors': 'Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk', 'link': 'https://arxiv.org/abs/2504.21048', 'abstract': "Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for numerous real-world applications, modeling distributed decision-making and learning from interactions with complex environments. Resource Allocation Optimization (RAO) benefits significantly from MARL's ability to tackle dynamic and decentralized contexts. MARL-based approaches are increasingly applied to RAO challenges across sectors playing pivotal roles to Industry 4.0 developments. This survey provides a comprehensive review of recent MARL algorithms for RAO, encompassing core concepts, classifications, and a structured taxonomy. By outlining the current research landscape and identifying primary challenges and future directions, this survey aims to support researchers and practitioners in leveraging MARL's potential to advance resource allocation solutions.", 'abstract_zh': '基于多代理强化学习的资源分配优化：综述', 'title_zh': '多代理 reinforcement 学习在资源分配优化中的应用：一个综述'}
{'arxiv_id': 'arXiv:2504.21042', 'title': "What's Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift", 'authors': 'Jiamin Chang, Haoyang Li, Hammond Pearce, Ruoxi Sun, Bo Li, Minhui Xue', 'link': 'https://arxiv.org/abs/2504.21042', 'abstract': 'The growing adoption of artificial intelligence (AI) has amplified concerns about trustworthiness, including integrity, privacy, robustness, and bias. To assess and attribute these threats, we propose ConceptLens, a generic framework that leverages pre-trained multimodal models to identify the root causes of integrity threats by analyzing Concept Shift in probing samples. ConceptLens demonstrates strong detection performance for vanilla data poisoning attacks and uncovers vulnerabilities to bias injection, such as the generation of covert advertisements through malicious concept shifts. It identifies privacy risks in unaltered but high-risk samples, filters them before training, and provides insights into model weaknesses arising from incomplete or imbalanced training data. Additionally, at the model level, it attributes concepts that the target model is overly dependent on, identifies misleading concepts, and explains how disrupting key concepts negatively impacts the model. Furthermore, it uncovers sociological biases in generative content, revealing disparities across sociological contexts. Strikingly, ConceptLens reveals how safe training and inference data can be unintentionally and easily exploited, potentially undermining safety alignment. Our study informs actionable insights to breed trust in AI systems, thereby speeding adoption and driving greater innovation.', 'abstract_zh': '人工智能（AI） adoption 的增长加剧了对可信性、完整性和隐私等方面的担忧，包括稳健性与偏见问题。为了评估和归因这些威胁，我们提出 ConceptLens，这是一种通用框架，利用预训练的多模态模型通过分析探测样本中的概念偏移来识别完整性的根本原因。ConceptLens 在检测简约型数据中毒攻击方面表现出强大的性能，并揭示了偏见注入的脆弱性，例如通过恶意概念偏移生成隐蔽广告。它识别未经修改但高风险的隐私风险样本，在训练前进行过滤，并提供由于不完整或不平衡的训练数据引发的模型弱点见解。此外，在模型级别，它归因目标模型过度依赖的概念，识别误导性概念，并解释干扰关键概念如何对模型产生负面影响。同时，它揭示生成内容中的社会学偏见，揭示不同社会学背景下存在的差异性。令人惊讶的是，ConceptLens 揭示了安全的训练和推理数据可能会意外且容易利用，这可能削弱安全性对齐。我们的研究为培养对 AI 系统的信任提供了可操作的见解，从而加速其采纳并推动更大的创新。', 'title_zh': '什么是操控背后的因素？通过概念偏移评估AI训练和推理中的完整性与归属问题'}
{'arxiv_id': 'arXiv:2504.21037', 'title': 'Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest', 'authors': 'Farnaz Soltaniani, Mohammad Ghafari, Mohammed Sayagh', 'link': 'https://arxiv.org/abs/2504.21037', 'abstract': "Early detection of security bug reports (SBRs) is crucial for preventing vulnerabilities and ensuring system reliability. While machine learning models have been developed for SBR prediction, their predictive performance still has room for improvement. In this study, we conduct a comprehensive comparison between BERT and Random Forest (RF), a competitive baseline for predicting SBRs. The results show that RF outperforms BERT with a 34% higher average G-measure for within-project predictions. Adding only SBRs from various projects improves both models' average performance. However, including both security and nonsecurity bug reports significantly reduces RF's average performance to 46%, while boosts BERT to its best average performance of 66%, surpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62% G-measure, which is substantially higher than RF.", 'abstract_zh': '早期检测安全漏洞报告对于预防漏洞和确保系统可靠性至关重要。尽管已开发出用于预测安全漏洞报告的机器学习模型，但其预测性能仍有机可提升。本研究全面比较了BERT和随机森林（RF）在预测安全漏洞报告方面的性能。结果显示，RF在项目内预测方面的平均G-测量值比BERT高34%。仅添加来自不同项目的安全漏洞报告可以提高两种模型的平均性能。然而，包含安全和非安全漏洞报告显著降低了RF的平均性能至46%，而提升了BERT的最佳平均性能至66%，超越了RF。在跨项目安全漏洞报告预测中，BERT实现了显著的62% G-测量值，高于RF。', 'title_zh': '跨项目内外的安全漏洞报告预测：BERT与随机森林的比较研究'}
{'arxiv_id': 'arXiv:2504.21030', 'title': 'Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications', 'authors': 'Naveen Krishnan', 'link': 'https://arxiv.org/abs/2504.21030', 'abstract': 'Multi-agent systems represent a significant advancement in artificial intelligence, enabling complex problem-solving through coordinated specialized agents. However, these systems face fundamental challenges in context management, coordination efficiency, and scalable operation. This paper introduces a comprehensive framework for advancing multi-agent systems through Model Context Protocol (MCP), addressing these challenges through standardized context sharing and coordination mechanisms. We extend previous work on AI agent architectures by developing a unified theoretical foundation, advanced context management techniques, and scalable coordination patterns. Through detailed implementation case studies across enterprise knowledge management, collaborative research, and distributed problem-solving domains, we demonstrate significant performance improvements compared to traditional approaches. Our evaluation methodology provides a systematic assessment framework with benchmark tasks and datasets specifically designed for multi-agent systems. We identify current limitations, emerging research opportunities, and potential transformative applications across industries. This work contributes to the evolution of more capable, collaborative, and context-aware artificial intelligence systems that can effectively address complex real-world challenges.', 'abstract_zh': 'Model Context Protocol（MCP）驱动的多Agent系统综合框架：克服根本挑战，实现高效协作与可扩展操作', 'title_zh': '通过模型上下文协议推进多智能体系统：架构、实现与应用'}
{'arxiv_id': 'arXiv:2504.21029', 'title': 'PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight', 'authors': 'Ben Goertzel, Paulos Yibelo', 'link': 'https://arxiv.org/abs/2504.21029', 'abstract': 'We propose a robust transformer architecture designed to prevent prompt injection attacks and ensure secure, reliable response generation. Our PICO (Prompt Isolation and Cybersecurity Oversight) framework structurally separates trusted system instructions from untrusted user inputs through dual channels that are processed independently and merged only by a controlled, gated fusion mechanism. In addition, we integrate a specialized Security Expert Agent within a Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge Graph (CKG) to supply domain-specific reasoning. Our training design further ensures that the system prompt branch remains immutable while the rest of the network learns to handle adversarial inputs safely. This PICO framework is presented via a general mathematical formulation, then elaborated in terms of the specifics of transformer architecture, and fleshed out via hypothetical case studies including Policy Puppetry attacks. While the most effective implementation may involve training transformers in a PICO-based way from scratch, we also present a cost-effective fine-tuning approach.', 'abstract_zh': '一种防止提示注入攻击并确保安全可靠响应生成的鲁棒变压器架构：PICO（提示隔离和网络安全监督）框架', 'title_zh': 'PICO: 安全的变压器模型通过健壮的提示隔离和网络安全监督'}
{'arxiv_id': 'arXiv:2504.21026', 'title': 'Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models', 'authors': 'Manish Pandey, Nageshwar Prasad Yadav, Mokshada Adduru, Sawan Rai', 'link': 'https://arxiv.org/abs/2504.21026', 'abstract': 'With the growing presence of multilingual users on social media, detecting abusive language in code-mixed text has become increasingly challenging. Code-mixed communication, where users seamlessly switch between English and their native languages, poses difficulties for traditional abuse detection models, as offensive content may be context-dependent or obscured by linguistic blending. While abusive language detection has been extensively explored for high-resource languages like English and Hindi, low-resource languages such as Telugu and Nepali remain underrepresented, leaving gaps in effective moderation. In this study, we introduce a novel, manually annotated dataset of 2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized as abusive and non-abusive, collected from various social media platforms. The dataset undergoes rigorous preprocessing before being evaluated across multiple Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We experimented with models including Logistic Regression, Random Forest, Support Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing their performance through hyperparameter tuning, and evaluate it using 10-fold cross-validation and statistical significance testing (t-test). Our findings provide key insights into the challenges of detecting abusive language in code-mixed settings and offer a comparative analysis of computational approaches. This study contributes to advancing NLP for low-resource languages by establishing benchmarks for abusive language detection in Telugu-English and Nepali-English code-mixed text. The dataset and insights can aid in the development of more robust moderation strategies for multilingual social media environments.', 'abstract_zh': '随着社交媒体上多语言用户的增多，检测代码混合文本中的攻击性语言变得越来越具有挑战性。在用户无缝切换英语和母语的代码混合通信中，传统攻击性内容检测模型面临困难，因为冒犯内容可能是情境依赖的或被语言融合所掩盖。尽管已经对英语和印地语等高资源语言的攻击性语言检测进行了广泛研究，但泰卢固语和尼泊尔语等低资源语言仍处于被忽视的地位，留下了有效的监管空白。在这项研究中，我们介绍了两个新的人工标注数据集，包含2000个泰卢固-英语和500个尼泊尔-英语代码混合评论，分为攻击性和非攻击性两类，来源于多个社交媒体平台。数据集经过严格的预处理后，在多个机器学习（ML）、深度学习（DL）和大型语言模型（LLM）中进行评估。我们尝试了包括逻辑回归、随机森林、支持向量机（SVM）、神经网络（NN）、LSTM、CNN和LLM等模型，通过超参数调整优化其性能，并使用10折交叉验证和统计显著性检验（t检验）进行评估。我们的研究结果提供了在代码混合环境中检测攻击性语言的挑战的关键见解，并提供了计算方法的比较分析。这项研究通过为泰卢固-英语和尼泊尔-英语代码混合文本中的攻击性语言检测建立基准，促进了低资源语言的自然语言处理（NLP）发展。数据集及其见解有助于开发更 robust 的多语言社交媒体环境中的监管策略。', 'title_zh': '使用传统和深度学习模型创建和评估代码混合尼泊尔英语和泰卢固英语数据集以检测 abusive 语言'}
{'arxiv_id': 'arXiv:2504.21008', 'title': 'Research on CNN-BiLSTM Network Traffic Anomaly Detection Model Based on MindSpore', 'authors': 'Qiuyan Xiang, Shuang Wu, Dongze Wu, Yuxin Liu, Zhenkai Qin', 'link': 'https://arxiv.org/abs/2504.21008', 'abstract': 'With the widespread adoption of the Internet of Things (IoT) and Industrial IoT (IIoT) technologies, network architectures have become increasingly complex, and the volume of traffic has grown substantially. This evolution poses significant challenges to traditional security mechanisms, particularly in detecting high-frequency, diverse, and highly covert network attacks. To address these challenges, this study proposes a novel network traffic anomaly detection model that integrates a Convolutional Neural Network (CNN) with a Bidirectional Long Short-Term Memory (BiLSTM) network, implemented on the MindSpore framework. Comprehensive experiments were conducted using the NF-BoT-IoT dataset. The results demonstrate that the proposed model achieves 99% across accuracy, precision, recall, and F1-score, indicating its strong performance and robustness in network intrusion detection tasks.', 'abstract_zh': '随着物联网（IoT）和工业物联网（IIoT）技术的广泛应用，网络架构变得日益复杂，网络流量显著增加。这种演变对传统的安全机制提出了重大挑战，特别是在检测高频率、多样化和隐蔽性强的网络攻击方面。为了应对这些挑战，本研究提出了一种将卷积神经网络（CNN）与双向长短期记忆网络（BiLSTM）相结合的新型网络流量异常检测模型，并在MindSpore框架上实现。通过使用NF-BoT-IoT数据集进行全面实验，结果表明，所提出模型在准确率、精确率、召回率和F1分数上均达到了99%，显示出其在网络入侵检测任务中的强大性能和稳健性。', 'title_zh': '基于MindSpore的CNN-BiLSTM网络流量异常检测模型研究'}
