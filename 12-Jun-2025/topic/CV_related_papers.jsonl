{'arxiv_id': 'arXiv:2506.09552', 'title': 'Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments', 'authors': 'Fatemeh Mohammadi Amin, Darwin G. Caldwell, Hans Wernher van de Venn', 'link': 'https://arxiv.org/abs/2506.09552', 'abstract': 'The robust interpretation of 3D environments is crucial for human-robot collaboration (HRC) applications, where safety and operational efficiency are paramount. Semantic segmentation plays a key role in this context by enabling a precise and detailed understanding of the environment. Considering the intense data hunger for real-world industrial annotated data essential for effective semantic segmentation, this paper introduces a pioneering approach in the Sim2Real domain adaptation for semantic segmentation of 3D point cloud data, specifically tailored for HRC. Our focus is on developing a network that robustly transitions from simulated environments to real-world applications, thereby enhancing its practical utility and impact on a safe HRC.\nIn this work, we propose a dual-stream network architecture (FUSION) combining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional Neural Networks (CNN) augmented with residual layers as a Sim2Real domain adaptation algorithm for an industrial environment. The proposed model was evaluated on real-world HRC setups and simulation industrial point clouds, it showed increased state-of-the-art performance, achieving a segmentation accuracy of 97.76%, and superior robustness compared to existing methods.', 'abstract_zh': '三维环境的稳健解释对于人机协作（HRC）应用至关重要，其中安全性和操作效率为首要。语义分割在此背景下发挥关键作用，通过使环境理解精确和详细。鉴于实际工业标注数据对于有效语义分割的极度需求，本文提出了一种在三维点云数据语义分割中用于工业环境的Sim2Real领域适应的开创性方法。我们专注于开发一个能够在模拟环境与实际应用之间稳健过渡的网络，从而提高其实用价值和在安全HRC中的影响。\n\n在此工作中，我们提出了一种双流网络架构（FUSION），结合了动态图卷积神经网络（DGCNN）和具有残差层的卷积神经网络（CNN），作为一种工业环境的Sim2Real领域适应算法。所提出的模型在实际工业HRC设置和模拟工业点云上进行了评估，显示了比现有方法的最先进的性能，准确率达到97.76%，且具有更强的鲁棒性。', 'title_zh': '增强人机协作：面向工业环境点云分割的Sim2Real领域自适应算法'}
{'arxiv_id': 'arXiv:2506.09217', 'title': 'Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule', 'authors': 'Boyu Jiang, Liang Shi, Zhengzhi Lin, Loren Stowe, Feng Guo', 'link': 'https://arxiv.org/abs/2506.09217', 'abstract': 'The performance of perception systems in autonomous driving systems (ADS) is strongly influenced by object distance, scene dynamics, and environmental conditions such as weather. AI-based perception outputs are inherently stochastic, with variability driven by these external factors, while traditional evaluation metrics remain static and event-independent, failing to capture fluctuations in confidence over time. In this work, we introduce the Perception Characteristics Distance (PCD) -- a novel evaluation metric that quantifies the farthest distance at which an object can be reliably detected, incorporating uncertainty in model outputs. To support this, we present the SensorRainFall dataset, collected on the Virginia Smart Road using a sensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear and daylight-rain scenarios, with precise ground-truth distances to the target objects. Statistical analysis reveals the presence of change points in the variance of detection confidence score with distance. By averaging the PCD values across a range of detection quality thresholds and probabilistic thresholds, we compute the mean PCD (mPCD), which captures the overall perception characteristics of a system with respect to detection distance. Applying state-of-the-art perception models shows that mPCD captures meaningful reliability differences under varying weather conditions -- differences that static metrics overlook. PCD provides a principled, distribution-aware measure of perception performance, supporting safer and more robust ADS operation, while the SensorRainFall dataset offers a valuable benchmark for evaluation. The SensorRainFall dataset is publicly available at this https URL, and the evaluation code is open-sourced at this https URL.', 'abstract_zh': '自主驾驶系统（ADS）中感知系统的性能受物体距离、场景动态以及天气等环境条件的强烈影响。基于AI的感知输出本质上具有随机性，其变异性由这些外部因素驱动，而传统的评估指标保持静态且独立于事件，未能捕捉随时间变化的信心波动。在本文中，我们引入了感知特性距离（PCD）——一种新的评估指标，用于量化能在可靠检测到目标对象的最远距离，同时考虑了模型输出的不确定性。为了支持这一评估指标，我们介绍了在受控的昼夜晴朗和昼夜雨天场景下，使用传感器装备车辆（摄像头、雷达、LiDAR）在维吉尼亚智能公路上收集的SensorRainFall数据集，该数据集提供了目标物体精确的地面真实距离。统计分析揭示了检测置信度分数随距离变化的突变点。通过在一系列检测质量阈值和概率阈值下平均PCD值，我们计算了均值感知特性距离（mPCD），它表征了系统在检测距离方面的整体感知特性。应用最先进的感知模型表明，mPCD在不同天气条件下捕捉到了有意义的可靠性差异，而静态指标未能捕捉到这些差异。PCD提供了一种原理上合理、分布感知性强的感知性能评估方法，有助于实现更安全、更可靠的ADS操作，而SensorRainFall数据集则为评估提供了宝贵的基准。SensorRainFall数据集可通过以下链接获取：this https URL，评估代码在此链接开源：this https URL。', 'title_zh': '感知特性距离：在特定决策规则下的动态条件下感知系统稳定性和鲁棒性测量'}
{'arxiv_id': 'arXiv:2506.09098', 'title': 'WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras', 'authors': 'Yangjie Cui, Boyang Gao, Yiwei Zhang, Xin Dong, Jinwu Xiang, Daochun Li, Zhan Tu', 'link': 'https://arxiv.org/abs/2506.09098', 'abstract': 'Previous studies on event camera sensing have demonstrated certain detection performance using dense event representations. However, the accumulated noise in such dense representations has received insufficient attention, which degrades the representation quality and increases the likelihood of missed detections. To address this challenge, we propose the Wavelet Denoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event cameras. In particular, a dense event representation is presented first, which enables real-time reconstruction of events as tensors. Then, a wavelet transform method is designed to filter noise in the event representations. Such a method is integrated into the backbone for feature extraction. The extracted features are subsequently fed into a transformer-based network for object prediction. To further reduce inference time, we incorporate the Dynamic Reorganization Convolution Block (DRCB) as a fusion module within the hybrid encoder. The proposed method has been evaluated on three event-based object detection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that WD-DETR outperforms tested state-of-the-art methods. Additionally, we implement our approach on a common onboard computer for robots, the NVIDIA Jetson Orin NX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16, which is exceptionally well-suited for real-time perception of onboard robotic systems.', 'abstract_zh': 'Wavelet Denoising-enhanced DEtection TRansformer网络在事件相机感知中的应用', 'title_zh': 'WD-DETR：小波去噪增强的实时目标检测变换器——适用于事件相机的机器人感知'}
{'arxiv_id': 'arXiv:2506.09748', 'title': 'Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints', 'authors': 'Xiangkai Zhang, Xiang Zhou, Mao Chen, Yuchen Lu, Xu Yang, Zhiyong Liu', 'link': 'https://arxiv.org/abs/2506.09748', 'abstract': "Absolute localization, aiming to determine an agent's location with respect to a global reference, is crucial for unmanned aerial vehicles (UAVs) in various applications, but it becomes challenging when global navigation satellite system (GNSS) signals are unavailable. Vision-based absolute localization methods, which locate the current view of the UAV in a reference satellite map to estimate its position, have become popular in GNSS-denied scenarios. However, existing methods mostly rely on traditional and low-level image matching, suffering from difficulties due to significant differences introduced by cross-source discrepancies and temporal variations. To overcome these limitations, in this paper, we introduce a hierarchical cross-source image matching method designed for UAV absolute localization, which integrates a semantic-aware and structure-constrained coarse matching module with a lightweight fine-grained matching module. Specifically, in the coarse matching module, semantic features derived from a vision foundation model first establish region-level correspondences under semantic and structural constraints. Then, the fine-grained matching module is applied to extract fine features and establish pixel-level correspondences. Building upon this, a UAV absolute visual localization pipeline is constructed without any reliance on relative localization techniques, mainly by employing an image retrieval module before the proposed hierarchical image matching modules. Experimental evaluations on public benchmark datasets and a newly introduced CS-UAV dataset demonstrate superior accuracy and robustness of the proposed method under various challenging conditions, confirming its effectiveness.", 'abstract_zh': '基于层级跨源图像匹配的无人机绝对视觉定位方法', 'title_zh': '基于语义和结构约束的无人机绝对视觉定位分层图像匹配'}
{'arxiv_id': 'arXiv:2506.09650', 'title': 'HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios', 'authors': 'Kunyu Peng, Junchao Huang, Xiangsheng Huang, Di Wen, Junwei Zheng, Yufan Chen, Kailun Yang, Jiamin Wu, Chongqing Hao, Rainer Stiefelhagen', 'link': 'https://arxiv.org/abs/2506.09650', 'abstract': 'Action segmentation is a core challenge in high-level video understanding, aiming to partition untrimmed videos into segments and assign each a label from a predefined action set. Existing methods primarily address single-person activities with fixed action sequences, overlooking multi-person scenarios. In this work, we pioneer textual reference-guided human action segmentation in multi-person settings, where a textual description specifies the target person for segmentation. We introduce the first dataset for Referring Human Action Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137 fine-grained actions with 33h video data, together with textual descriptions for this new task. Benchmarking existing action recognition methods on RHAS133 using VLM-based feature extractors reveals limited performance and poor aggregation of visual cues for the target person. To address this, we propose a holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF, leveraging a novel cross-input gate attentional xLSTM to enhance holistic-partial long-range reasoning and a novel Fourier condition to introduce more fine-grained control to improve the action segmentation generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse evaluation settings. The code is available at this https URL.', 'abstract_zh': '多人员场景下的文本引导人体动作分割：一种全新的 holistic-partial 意识 Fourier 条件扩散框架（HopaDIFF）', 'title_zh': 'HopaDIFF：全面-局部aware傅里叶条件扩散在多人大规模场景中人类动作分割中的应用'}
{'arxiv_id': 'arXiv:2506.09278', 'title': 'UFM: A Simple Path towards Unified Dense Correspondence with Flow', 'authors': 'Yuchen Zhang, Nikhil Keetha, Chenwei Lyu, Bhuvan Jhamb, Yutian Chen, Yuheng Qiu, Jay Karhade, Shreyas Jha, Yaoyu Hu, Deva Ramanan, Sebastian Scherer, Wenshan Wang', 'link': 'https://arxiv.org/abs/2506.09278', 'abstract': 'Dense image correspondence is central to many applications, such as visual odometry, 3D reconstruction, object association, and re-identification. Historically, dense correspondence has been tackled separately for wide-baseline scenarios and optical flow estimation, despite the common goal of matching content between two images. In this paper, we develop a Unified Flow & Matching model (UFM), which is trained on unified data for pixels that are co-visible in both source and target images. UFM uses a simple, generic transformer architecture that directly regresses the (u,v) flow. It is easier to train and more accurate for large flows compared to the typical coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than state-of-the-art flow methods (Unimatch), while also having 62% less error and 6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to demonstrate that unified training can outperform specialized approaches across both domains. This result enables fast, general-purpose correspondence and opens new directions for multi-modal, long-range, and real-time correspondence tasks.', 'abstract_zh': '密集图像对应在许多应用中至关重要，如视觉里程计、三维重建、对象关联和再识别。历史上，密集对应在宽基线场景和光学流估计中分别处理，尽管它们的共同目标是在两张图像之间匹配内容。在本文中，我们开发了一种统一流动与匹配模型（UFM），该模型在两张源图像和目标图像中共可见像素的统一数据上进行训练。UFM 使用一种简单且通用的变压器架构，直接回归 (u,v) 流。与之前工作中的典型粗到细代价卷积相比，它更容易训练且对于大流更为准确。UFM 在与最先进的流动方法（Unimatch）相比时更为准确，误差降低了62%，速度快6.7倍，而且在共视匹配器（RoMa）上更为准确。UFM 是第一个表明统一训练能够在两个领域都超越专门方法的结果。这一结果开启了快速、通用对应的新方向，并为多模态、长距离和实时对应任务开辟了新方向。', 'title_zh': 'UFM：通往统一密集对应的一种简单途径，基于流的方法'}
{'arxiv_id': 'arXiv:2506.09068', 'title': 'BG-HOP: A Bimanual Generative Hand-Object Prior', 'authors': 'Sriram Krishna, Sravan Chittupalli, Sungjae Park', 'link': 'https://arxiv.org/abs/2506.09068', 'abstract': "In this work, we present BG-HOP, a generative prior that seeks to model bimanual hand-object interactions in 3D. We address the challenge of limited bimanual interaction data by extending existing single-hand generative priors, demonstrating preliminary results in capturing the joint distribution of hands and objects. Our experiments showcase the model's capability to generate bimanual interactions and synthesize grasps for given objects. We make code and models publicly available.", 'abstract_zh': 'BG-HOP: 一种生成先验模型，用于建模三维双手物体交互', 'title_zh': 'BG-HOP: 一种双手生成手物体先验'}
{'arxiv_id': 'arXiv:2506.09997', 'title': 'DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos', 'authors': 'Chieh Hubert Lin, Zhaoyang Lv, Songyin Wu, Zhen Xu, Thu Nguyen-Phuoc, Hung-Yu Tseng, Julian Straub, Numair Khan, Lei Xiao, Ming-Hsuan Yang, Yuheng Ren, Richard Newcombe, Zhao Dong, Zhengqin Li', 'link': 'https://arxiv.org/abs/2506.09997', 'abstract': 'We introduce the Deformable Gaussian Splats Large Reconstruction Model (DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian splats from a monocular posed video of any dynamic scene. Feed-forward scene reconstruction has gained significant attention for its ability to rapidly create digital replicas of real-world environments. However, most existing models are limited to static scenes and fail to reconstruct the motion of moving objects. Developing a feed-forward model for dynamic scene reconstruction poses significant challenges, including the scarcity of training data and the need for appropriate 3D representations and training paradigms. To address these challenges, we introduce several key technical contributions: an enhanced large-scale synthetic dataset with ground-truth multi-view videos and dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian representation that is easy to learn, supports high-quality dynamic view synthesis, and enables long-range 3D tracking; and a large transformer network that achieves real-time, generalizable dynamic scene reconstruction. Extensive qualitative and quantitative experiments demonstrate that DGS-LRM achieves dynamic scene reconstruction quality comparable to optimization-based methods, while significantly outperforming the state-of-the-art predictive dynamic reconstruction method on real-world examples. Its predicted physically grounded 3D deformation is accurate and can readily adapt for long-range 3D tracking tasks, achieving performance on par with state-of-the-art monocular video 3D tracking methods.', 'abstract_zh': '可变形高斯点大规模重建模型 (DGS-LRM): 从任意动态场景的单目视频预测可变形 3D 高斯点的首个前馈方法', 'title_zh': 'DGS-LRM: 基于单目视频的实时变形三维高斯重建'}
{'arxiv_id': 'arXiv:2506.09993', 'title': 'Text-Aware Image Restoration with Diffusion Models', 'authors': 'Jaewon Min, Jin Hyeon Kim, Paul Hyunbin Cho, Jaeeun Lee, Jihye Park, Minkyu Park, Sangpil Kim, Hyunhee Park, Seungryong Kim', 'link': 'https://arxiv.org/abs/2506.09993', 'abstract': 'Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: this https URL', 'abstract_zh': '图像恢复旨在恢复退化图像。然而，现有的基于扩散的恢复方法尽管在自然图像恢复方面取得了巨大成功，但在重建退化图像中的文本区域时往往难以做到忠实重构。这些方法经常生成合理但错误的文字样图案，我们将这一现象称为文本图像幻视。本文介绍了一种新的恢复任务——文本感知图像恢复（TAIR），该任务要求同时恢复视觉内容和文本保真度。为应对这一任务，我们提出了SA-Text，这是一个包含10万张高质量场景图像的大规模基准数据集，这些图像密集标注了多样且复杂的文本实例。此外，我们提出了一种多任务扩散框架TeReDiff，将扩散模型的内部特征整合到一个文本检测模块中，使两个组件能够从联合训练中受益，从而提取丰富的文本表示，这些表示用于后续去噪步骤中作为提示。大量实验表明，我们的方法在文本识别准确性方面始终优于最先进的恢复方法，取得了显著的提升。见我们的项目页面：this https URL。', 'title_zh': '基于文本的图像恢复扩散模型'}
{'arxiv_id': 'arXiv:2506.09954', 'title': 'Vision Generalist Model: A Survey', 'authors': 'Ziyi Wang, Yongming Rao, Shuofeng Sun, Xinrun Liu, Yi Wei, Xumin Yu, Zuyan Liu, Yanbo Wang, Hongmin Liu, Jie Zhou, Jiwen Lu', 'link': 'https://arxiv.org/abs/2506.09954', 'abstract': 'Recently, we have witnessed the great success of the generalist model in natural language processing. The generalist model is a general framework trained with massive data and is able to process various downstream tasks simultaneously. Encouraged by their impressive performance, an increasing number of researchers are venturing into the realm of applying these models to computer vision tasks. However, the inputs and outputs of vision tasks are more diverse, and it is difficult to summarize them as a unified representation. In this paper, we provide a comprehensive overview of the vision generalist models, delving into their characteristics and capabilities within the field. First, we review the background, including the datasets, tasks, and benchmarks. Then, we dig into the design of frameworks that have been proposed in existing research, while also introducing the techniques employed to enhance their performance. To better help the researchers comprehend the area, we take a brief excursion into related domains, shedding light on their interconnections and potential synergies. To conclude, we provide some real-world application scenarios, undertake a thorough examination of the persistent challenges, and offer insights into possible directions for future research endeavors.', 'abstract_zh': '最近，通用模型在自然语言处理领域取得了巨大成功。通用模型是一种通过大规模数据训练的通用框架，能够同时处理多种下游任务。受到其出色表现的鼓舞，越来越多的研究人员将这些模型应用于计算机视觉任务。然而，视觉任务的输入和输出更为多样化，难以将其统一概括。本文对视觉通用模型进行了全面概述，探讨了其在该领域的特点和能力。首先，我们回顾了背景信息，包括数据集、任务和基准测试。接着，我们深入探讨了现有研究中提出的框架设计，并介绍了增强其性能的技术。为了更好地帮助研究人员理解该领域，我们对相关领域进行了简要的调研，揭示了它们之间的联系和潜在协同效应。最后，我们提供了实际应用场景，详细分析了持续存在的挑战，并提出了未来研究方向的见解。', 'title_zh': '视觉通用模型：一篇综述'}
{'arxiv_id': 'arXiv:2506.09953', 'title': 'Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos', 'authors': 'Benjamin Reichman, Constantin Patsch, Jack Truxal, Atishay Jain, Larry Heck', 'link': 'https://arxiv.org/abs/2506.09953', 'abstract': 'In outside knowledge visual question answering (OK-VQA), the model must identify relevant visual information within an image and incorporate external knowledge to accurately respond to a question. Extending this task to a visually grounded dialogue setting based on videos, a conversational model must both recognize pertinent visual details over time and answer questions where the required information is not necessarily present in the visual information. Moreover, the context of the overall conversation must be considered for the subsequent dialogue. To explore this task, we introduce a dataset comprised of $2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$ interleaved dialogue turns. While the dialogue context is visually grounded in specific video segments, the questions further require external knowledge that is not visually present. Thus, the model not only has to identify relevant video parts but also leverage external knowledge to converse within the dialogue. We further provide several baselines evaluated on our dataset and show future challenges associated with this task. The dataset is made publicly available here: this https URL.', 'abstract_zh': '在外包知识视觉问答（OK-VQA）中，模型必须识别图像内的相关视觉信息并结合外部知识以准确回答问题。将此任务扩展到基于视频的对话设置中，对话模型不仅要随着时间识别相关视觉细节，还要回答那些所需信息不一定在视觉信息中出现的问题。此外，整个对话的上下文必须被考虑以供后续对话使用。为了探索这一任务，我们引入了一个包含2017个视频和5986个人工标注对话的数据集，共计40954个对话回合。尽管对话的上下文是基于特定视频片段的视觉 grounding，但问题进一步要求外部知识，而这在视觉信息中未出现。因此，模型不仅要识别相关视频部分，还要利用外部知识进行对话。我们还提供了在该数据集上评估的几种基线，并展示了与此任务相关的未来挑战。该数据集已公开发布于此：this https URL。', 'title_zh': 'Outside Knowledge Conversational Video (OKCV) 数据集 -- 视频上的对话'}
{'arxiv_id': 'arXiv:2506.09952', 'title': 'UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting', 'authors': 'Ziyi Wang, Yanran Zhang, Jie Zhou, Jiwen Lu', 'link': 'https://arxiv.org/abs/2506.09952', 'abstract': "The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at this https URL.", 'abstract_zh': '点云数据的尺度多样性给开发统一的3D视觉表示学习技术带来了显著挑战。目前，很少存在统一的3D模型，且现有预训练方法在对象级和场景级点云上均未表现出同等效用。在这项工作中，我们引入了UniPre3D，这是首个能够无缝应用于任意尺度点云和任意架构3D模型的统一预训练方法。我们的方法以预测高斯基础元作为预训练任务，并采用可微分的高斯插值生成图像，从而提供精确的像素级监督和端到端优化。为进一步调节预训练任务的复杂性并引导模型关注几何结构，我们将预训练图像模型的2D特征整合进来，以引入成熟的空间纹理知识。我们通过广泛实验验证了所提方法在多种对象级和场景级任务上的通用有效性，使用了多种点云模型作为骨干。代码详见这个链接。', 'title_zh': 'UniPre3D：统一的3D点云模型跨模态高斯绘制预训练'}
{'arxiv_id': 'arXiv:2506.09943', 'title': 'CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models', 'authors': 'Aaron Foss, Chloe Evans, Sasha Mitts, Koustuv Sinha, Ammar Rizvi, Justine T. Kao', 'link': 'https://arxiv.org/abs/2506.09943', 'abstract': "We introduce CausalVQA, a benchmark dataset for video question answering (VQA) composed of question-answer pairs that probe models' understanding of causality in the physical world. Existing VQA benchmarks either tend to focus on surface perceptual understanding of real-world videos, or on narrow physical reasoning questions created using simulation environments. CausalVQA fills an important gap by presenting challenging questions that are grounded in real-world scenarios, while focusing on models' ability to predict the likely outcomes of different actions and events through five question types: counterfactual, hypothetical, anticipation, planning and descriptive. We designed quality control mechanisms that prevent models from exploiting trivial shortcuts, requiring models to base their answers on deep visual understanding instead of linguistic cues. We find that current frontier multimodal models fall substantially below human performance on the benchmark, especially on anticipation and hypothetical questions. This highlights a challenge for current systems to leverage spatial-temporal reasoning, understanding of physical principles, and comprehension of possible alternatives to make accurate predictions in real-world settings.", 'abstract_zh': '因果关系视觉问答基准数据集：探究模型在物理世界中因果关系理解的能力', 'title_zh': '因果VQA：面向视频模型的物理基础因果推理基准'}
{'arxiv_id': 'arXiv:2506.09836', 'title': 'DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction', 'authors': 'Junli Deng, Ping Shi, Qipei Li, Jinyang Guo', 'link': 'https://arxiv.org/abs/2506.09836', 'abstract': 'Reconstructing intricate, ever-changing environments remains a central ambition in computer vision, yet existing solutions often crumble before the complexity of real-world dynamics. We present DynaSplat, an approach that extends Gaussian Splatting to dynamic scenes by integrating dynamic-static separation and hierarchical motion modeling. First, we classify scene elements as static or dynamic through a novel fusion of deformation offset statistics and 2D motion flow consistency, refining our spatial representation to focus precisely where motion matters. We then introduce a hierarchical motion modeling strategy that captures both coarse global transformations and fine-grained local movements, enabling accurate handling of intricate, non-rigid motions. Finally, we integrate physically-based opacity estimation to ensure visually coherent reconstructions, even under challenging occlusions and perspective shifts. Extensive experiments on challenging datasets reveal that DynaSplat not only surpasses state-of-the-art alternatives in accuracy and realism but also provides a more intuitive, compact, and efficient route to dynamic scene reconstruction.', 'abstract_zh': '重构复杂多变的环境仍是计算机视觉领域的核心目标，现有解决方案往往难以应对真实世界动态的复杂性。我们提出了DynaSplat方法，该方法通过结合动态-静态分离和分层运动建模，将高斯点云扩展到动态场景。首先，我们通过一种新颖的形变偏移统计与二维运动流一致性融合方法，对场景元素进行静态或动态分类，从而精细化空间表示，集中在关键的运动区域。然后，我们引入了一种分层运动建模策略，既能捕捉粗粒度全局变换，又能捕捉细粒度局部运动，从而准确处理复杂的非刚性运动。最后，我们整合了基于物理的不透明度估计，确保在挑战性的遮挡和视角变化情况下，重构结果具有视觉一致性。广泛的实验结果表明，DynaSplat不仅在准确性和真实性上超越了现有最先进的方法，还提供了一条更为直观、紧凑且高效的动态场景重构途径。', 'title_zh': 'DynaSplat: 动静态高斯点云渲染及其层次运动分解场景重建'}
{'arxiv_id': 'arXiv:2506.09785', 'title': 'A theoretical framework for self-supervised contrastive learning for continuous dependent data', 'authors': 'Alexander Marusov, Alexander Yuhay, Alexey Zaytsev', 'link': 'https://arxiv.org/abs/2506.09785', 'abstract': 'Self-supervised learning (SSL) has emerged as a powerful approach to learning representations, particularly in the field of computer vision. However, its application to dependent data, such as temporal and spatio-temporal domains, remains underexplored. Besides, traditional contrastive SSL methods often assume \\emph{semantic independence between samples}, which does not hold for dependent data exhibiting complex correlations. We propose a novel theoretical framework for contrastive SSL tailored to \\emph{continuous dependent data}, which allows the nearest samples to be semantically close to each other. In particular, we propose two possible \\textit{ground truth similarity measures} between objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive an analytical form for the \\textit{estimated similarity matrix} that accommodates both types of closeness between samples, thereby introducing dependency-aware loss functions. We validate our approach, \\emph{Dependent TS2Vec}, on temporal and spatio-temporal downstream problems. Given the dependency patterns presented in the data, our approach surpasses modern ones for dependent data, highlighting the effectiveness of our theoretically grounded loss functions for SSL in capturing spatio-temporal dependencies. Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with accuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on the drought classification task, which involves complex spatio-temporal patterns, our method achieves a $7$\\% higher ROC-AUC score.', 'abstract_zh': '自监督学习（SSL）已成为学习表示的强大方法，特别是在计算机视觉领域。然而，其在时间依赖性数据如时间域和空时域中的应用仍待进一步探索。此外，传统的对比自监督学习方法通常假设样本之间具有语义独立性，这对于展示复杂相关性的依赖性数据并不成立。我们提出了一种针对连续依赖性数据的新型对比自监督学习理论框架，该框架允许最近邻样本彼此具有语义上的接近性。特别地，我们提出了两种可能的“真实相似性度量”——“硬接近”和“软接近”。在此基础上，我们推导出一种能够适应样本之间两种接近性的估计相似性矩阵的形式，并由此引入了依赖性感知的损失函数。我们在时间序列和空时下游问题上验证了我们的方法，即Dependent TS2Vec。对于数据中展示的依赖性模式，我们的方法超越了现代依赖性数据方法，突显了我们理论支撑的损失函数在自监督学习中捕捉空时依赖性的有效性。具体而言，我们在标准UEA和UCR基准测试上分别取得了4.17%和2.08%的准确性提升。此外，在涉及复杂空时模式的干旱分类任务中，我们的方法获得了7%更高的ROC-AUC分数。', 'title_zh': '自监督对比学习的理论框架用于连续依赖数据'}
{'arxiv_id': 'arXiv:2506.09777', 'title': 'Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space', 'authors': 'Anton Razzhigaev, Matvey Mikhalchuk, Klim Kireev, Igor Udovichenko, Andrey Kuznetsov, Aleksandr Petiushko', 'link': 'https://arxiv.org/abs/2506.09777', 'abstract': 'Reconstructing facial images from black-box recognition models poses a significant privacy threat. While many methods require access to embeddings, we address the more challenging scenario of model inversion using only similarity scores. This paper introduces DarkerBB, a novel approach that reconstructs color faces by performing zero-order optimization within a PCA-derived eigenface space. Despite this highly limited information, experiments on LFW, AgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves state-of-the-art verification accuracies in the similarity-only setting, with competitive query efficiency.', 'abstract_zh': '从黑色盒模型重建面部图像构成了重大的隐私威胁。尽管许多方法需要访问嵌入式信息，我们通过仅使用相似度分数来解决更具挑战性的模型反向工程问题。本文提出了DarkerBB，一种新颖的方法，通过在PCA衍生的特征脸空间中进行零阶优化来重建彩色面部。尽管仅限于此高度有限的信息，实验结果表明，在仅使用相似度分数的情况下，DarkerBB实现了最先进的验证准确率，并且具有竞争力的查询效率。', 'title_zh': '通过特征脸空间的零阶优化反向推理黑盒面部识别系统'}
{'arxiv_id': 'arXiv:2506.09740', 'title': 'ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models', 'authors': 'Qin Zhou, Zhiyang Zhang, Jinglong Wang, Xiaobin Li, Jing Zhang, Qian Yu, Lu Sheng, Dong Xu', 'link': 'https://arxiv.org/abs/2506.09740', 'abstract': 'Diffusion models excel at image generation. Recent studies have shown that these models not only generate high-quality images but also encode text-image alignment information through attention maps or loss functions. This information is valuable for various downstream tasks, including segmentation, text-guided image editing, and compositional image generation. However, current methods heavily rely on the assumption of perfect text-image alignment in diffusion models, which is not the case. In this paper, we propose using zero-shot referring image segmentation as a proxy task to evaluate the pixel-level image and class-level text alignment of popular diffusion models. We conduct an in-depth analysis of pixel-text misalignment in diffusion models from the perspective of training data bias. We find that misalignment occurs in images with small sized, occluded, or rare object classes. Therefore, we propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text alignment in diffusion models based on the evidence lower bound (ELBO) of likelihood. Our method is training-free and generic, eliminating the need to identify the specific cause of misalignment and works well across various diffusion model architectures. Extensive experiments on commonly used benchmark datasets on image segmentation and generation have verified the effectiveness of our proposed calibration approach.', 'abstract_zh': '扩散模型在图像生成中表现出色。最近的研究表明，这些模型不仅可以生成高质量的图像，还能通过注意力图或损失函数编码文本-图像对齐信息。这些信息对包括分割、文本引导的图像编辑和组成性图像生成在内的多种下游任务具有价值。然而，当前方法严重依赖于扩散模型中文本-图像对齐完美性的假设，实际情况并非如此。本文提出使用零-shot 引用图像分割作为代理任务，评估流行扩散模型的像素级图像对齐和类别级文本对齐。我们从训练数据偏差的角度深入分析了扩散模型中的像素-文本对齐偏差问题。我们发现，在小尺寸、被遮挡或罕见对象类别图像中存在对齐偏差。因此，我们提出了基于似然性证据下界（ELBO）的ELBO-T2IAlign方法，以简单有效的方式校准扩散模型中的像素-文本对齐。该方法无需训练，并具有通用性，无需识别对齐偏差的具体原因，适用于各种扩散模型架构。 extensive实验表明，我们的校准方法在常用的图像分割和生成基准数据集上具有有效性。', 'title_zh': '基于ELBO的通用方法：用于校准扩散模型中像素级文本-图像对齐的ELBO-T2IAlign'}
{'arxiv_id': 'arXiv:2506.09644', 'title': 'DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning', 'authors': 'Dongxu Liu, Yuang Peng, Haomiao Tang, Yuwei Chen, Chunrui Han, Zheng Ge, Daxin Jiang, Mingxue Liao', 'link': 'https://arxiv.org/abs/2506.09644', 'abstract': "Autoencoders empower state-of-the-art image and video generative models by compressing pixels into a latent space through visual tokenization. Although recent advances have alleviated the performance degradation of autoencoders under high compression ratios, addressing the training instability caused by GAN remains an open challenge. While improving spatial compression, we also aim to minimize the latent space dimensionality, enabling more efficient and compact representations. To tackle these challenges, we focus on improving the decoder's expressiveness. Concretely, we propose DGAE, which employs a diffusion model to guide the decoder in recovering informative signals that are not fully decoded from the latent representation. With this design, DGAE effectively mitigates the performance degradation under high spatial compression rates. At the same time, DGAE achieves state-of-the-art performance with a 2x smaller latent space. When integrated with Diffusion Models, DGAE demonstrates competitive performance on image generation for ImageNet-1K and shows that this compact latent representation facilitates faster convergence of the diffusion model.", 'abstract_zh': '自动编码器通过视觉标记化将像素压缩到潜在空间，增强最先进的图像和视频生成模型。尽管近期进展缓解了在高压缩比下自动编码器性能下降的问题，GAN引发的训练稳定性问题仍是一个开放挑战。在提高空间压缩的同时，我们还致力于最小化潜在空间维度，以实现更高效和紧凑的表示。为应对这些挑战，我们专注于提升解码器的表达能力。具体而言，我们提出了一种基于扩散模型的DGAE方法，通过扩散模型引导解码器恢复从潜在表示中未完全解码的有意义信号。通过这种设计，DGAE在高空间压缩率下有效地缓解了性能下降的问题。同时，DGAE以2倍较小的潜在空间实现了最先进的性能。将DGAE与扩散模型结合使用时，它在ImageNet-1K图像生成任务上显示出竞争力，并表明这种紧凑的潜在表示有助于扩散模型更快收敛。', 'title_zh': 'DGAE：扩散引导自编码器在高效潜在表示学习中的应用'}
{'arxiv_id': 'arXiv:2506.09445', 'title': 'TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision', 'authors': 'Ayush Gupta, Anirban Roy, Rama Chellappa, Nathaniel D. Bastian, Alvaro Velasquez, Susmit Jha', 'link': 'https://arxiv.org/abs/2506.09445', 'abstract': 'We address the problem of video question answering (video QA) with temporal grounding in a weakly supervised setup, without any temporal annotations. Given a video and a question, we generate an open-ended answer grounded with the start and end time. For this task, we propose TOGA: a vision-language model for Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune TOGA to jointly generate the answer and the temporal grounding. We operate in a weakly supervised setup where the temporal grounding annotations are not available. We generate pseudo labels for temporal grounding and ensure the validity of these labels by imposing a consistency constraint between the question of a grounding response and the response generated by a question referring to the same temporal segment. We notice that jointly generating the answers with the grounding improves performance on question answering as well as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate weakly supervised grounded question answering. For open-ended QA, we consider the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art performance for both tasks on these benchmarks.', 'abstract_zh': '我们在弱监督下解决带有时间定位的视频问答问题，无需任何时间标注。给定一个视频和一个问题，我们生成一个开放式答案，并标注起始和结束时间。为此任务，我们提出TOGA：一种用于弱监督下带有时间定位的开放式视频问答的视觉-语言模型。我们在一个缺乏时间标注的时间定位弱监督设置中指令调用来同时生成答案和时间定位。我们通过在定位响应的问题与指向同一时间段的相同问题生成的回答之间施加一致性约束来为时间定位生成伪标签，确保这些标签的有效性。我们注意到同时生成答案与定位可以提高问答以及定位的性能。我们分别在带有定位的问答和开放式问答任务上评估TOGA。对于带有定位的问答，我们考虑了设计用于评估弱监督下带有定位的问答的NExT-GQA基准。对于开放式问答，我们考虑了MSVD-QA和ActivityNet-QA基准。我们在这些基准上分别实现了这两种任务的最佳性能。', 'title_zh': 'TOGA：基于时间的开放性视频QA弱监督方法'}
{'arxiv_id': 'arXiv:2506.09411', 'title': 'Synthetic Human Action Video Data Generation with Pose Transfer', 'authors': 'Vaclav Knapp, Matyas Bohacek', 'link': 'https://arxiv.org/abs/2506.09411', 'abstract': 'In video understanding tasks, particularly those involving human motion, synthetic data generation often suffers from uncanny features, diminishing its effectiveness for training. Tasks such as sign language translation, gesture recognition, and human motion understanding in autonomous driving have thus been unable to exploit the full potential of synthetic data. This paper proposes a method for generating synthetic human action video data using pose transfer (specifically, controllable 3D Gaussian avatar models). We evaluate this method on the Toyota Smarthome and NTU RGB+D datasets and show that it improves performance in action recognition tasks. Moreover, we demonstrate that the method can effectively scale few-shot datasets, making up for groups underrepresented in the real training data and adding diverse backgrounds. We open-source the method along with RANDOM People, a dataset with videos and avatars of novel human identities for pose transfer crowd-sourced from the internet.', 'abstract_zh': '视频理解任务中，特别是在人体动作涉及的任务中，合成数据生成往往存在不自然的特点，这降低了其训练效果。诸如手语翻译、姿态识别和自动驾驶中的人体动作理解等任务因此未能充分利用合成数据的潜力。本文提出了一种使用姿态转移（具体来说是可控的3D高斯 avatar模型）生成合成人体动作视频数据的方法。我们在Toyota Smarthome和NTU RGB+D数据集上评估了该方法，并展示了其在动作识别任务中的性能提升。此外，我们证明该方法可以有效扩大少样本数据集的规模，弥补现实训练数据中代表性不足的群体，并增加多样化的背景环境。我们开源了该方法以及从互联网中 crowdsourced 来的包含新颖人体身份视频和avatar的RANDOM People数据集。', 'title_zh': '合成人体动作视频数据生成与姿态转移'}
{'arxiv_id': 'arXiv:2506.09363', 'title': 'SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing', 'authors': 'Hongguang Zhu, Yunchao Wei, Mengyu Wang, Siyu Jiao, Yan Fang, Jiannan Huang, Yao Zhao', 'link': 'https://arxiv.org/abs/2506.09363', 'abstract': "Diffusion models (DMs) have achieved significant progress in text-to-image generation. However, the inevitable inclusion of sensitive information during pre-training poses safety risks, such as unsafe content generation and copyright infringement. Concept erasing finetunes weights to unlearn undesirable concepts, and has emerged as a promising solution. However, existing methods treat unsafe concept as a fixed word and repeatedly erase it, trapping DMs in ``word concept abyss'', which prevents generalized concept-related erasing. To escape this abyss, we introduce semantic-augment erasing which transforms concept word erasure into concept domain erasure by the cyclic self-check and self-erasure. It efficiently explores and unlearns the boundary representation of concept domain through semantic spatial relationships between original and training DMs, without requiring additional preprocessed data. Meanwhile, to mitigate the retention degradation of irrelevant concepts while erasing unsafe concepts, we further propose the global-local collaborative retention mechanism that combines global semantic relationship alignment with local predicted noise preservation, effectively expanding the retentive receptive field for irrelevant concepts. We name our method SAGE, and extensive experiments demonstrate the comprehensive superiority of SAGE compared with other methods in the safe generation of DMs. The code and weights will be open-sourced at this https URL.", 'abstract_zh': '差分模型（DMs）在文本到图像生成方面取得了显著进展。然而，预训练过程中不可避免地包含敏感信息导致了安全风险，如不安全内容生成和版权侵犯。概念擦除微调权重以消除不良概念，并已成为一种有前景的解决方案。然而，现有方法将不安全概念视为固定词，并反复擦除它，将DMs困在“词汇概念深渊”中，这阻碍了对泛化概念相关擦除的能力。为了逃离这个深渊，我们引入了语义增强擦除，通过循环自检和自擦除将概念词擦除转化为概念领域擦除。它通过原始和训练DMs之间的语义空间关系高效地探索和遗忘概念领域的边界表示，无需额外预处理数据。同时，为在擦除不安全概念时减轻与之无关的概念保留降级，我们进一步提出了一种全局-局部协作保留机制，该机制结合了全局语义关系对齐与局部预测噪声保留，有效地扩展了与无关概念相关的保留感受野。我们称我们的方法为SAGE，并且广泛的实验表明，在DMs的安全生成方面，SAGE相比其他方法具有全面的优势。代码和权重将在该网址开放源代码。', 'title_zh': 'SAGE: 探索语义增强擦除在不安全概念领域边界上的应用'}
{'arxiv_id': 'arXiv:2506.09350', 'title': 'Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation', 'authors': 'Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, Lu Jiang', 'link': 'https://arxiv.org/abs/2506.09350', 'abstract': 'Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at this https URL', 'abstract_zh': '现有的大型视频生成模型计算密集，阻碍了其在实时和交互应用中的采用。在本文中，我们提出了一种自回归对抗后训练（AAPT）方法，将预训练的潜空间视频扩散模型转换为实时交互式视频生成器。我们的模型使用单次神经函数评估（1NFE）逐帧自回归生成潜在帧，并能够实时流式传输结果并接收交互式响应作为控制生成下一帧的指令。与现有方法不同，我们的方法探索了对抗训练作为一种有效的自回归生成范式。这不仅使我们能够设计一种更高效的单步生成架构，充分利用KV缓存，还使我们能够以一种证明对减少长时间视频生成过程中错误累积有效的学生强迫训练模型的方式进行训练。我们的实验表明，我们的8B模型可以在单块H100上以736x416分辨率实现每秒24帧的实时流式传输视频生成，或者在8块H100上生成长达一分钟（1440帧）的1280x720分辨率视频。请访问我们的研究网站：[访问链接]。', 'title_zh': '自回归对抗后训练实时交互式视频生成'}
{'arxiv_id': 'arXiv:2506.09095', 'title': 'Foundation Models in Medical Imaging -- A Review and Outlook', 'authors': 'Vivien van Veldhuizen, Vanessa Botha, Chunyao Lu, Melis Erdal Cesur, Kevin Groot Lipman, Edwin D. de Jong, Hugo Horlings, Clárisa Sanchez, Cees Snoek, Ritse Mann, Eric Marcus, Jonas Teuwen', 'link': 'https://arxiv.org/abs/2506.09095', 'abstract': 'Foundation models (FMs) are changing the way medical images are analyzed by learning from large collections of unlabeled data. Instead of relying on manually annotated examples, FMs are pre-trained to learn general-purpose visual features that can later be adapted to specific clinical tasks with little additional supervision. In this review, we examine how FMs are being developed and applied in pathology, radiology, and ophthalmology, drawing on evidence from over 150 studies. We explain the core components of FM pipelines, including model architectures, self-supervised learning methods, and strategies for downstream adaptation. We also review how FMs are being used in each imaging domain and compare design choices across applications. Finally, we discuss key challenges and open questions to guide future research.', 'abstract_zh': '基础模型（FMs）正在通过学习大规模未标注数据来改变医学图像的分析方式。FMs在不需要依赖手动标注示例的情况下，预先训练以学习通用视觉特征，这些特征可以稍后通过少量额外监督适应特定的临床任务。在本文综述中，我们基于超过150项研究，探讨FMs在病理学、放射学和眼科学中的开发与应用，并解释了FM流水线的核心组件，包括模型架构、自监督学习方法以及下游适应策略。我们还回顾了FMs在每个成像领域中的应用，并跨应用程序比较设计选择。最后，我们讨论了关键挑战和开放问题，以指导未来的研究。', 'title_zh': '医学成像中的基础模型：综述与展望'}
{'arxiv_id': 'arXiv:2506.09083', 'title': 'BakuFlow: A Streamlining Semi-Automatic Label Generation Tool', 'authors': 'Jerry Lin, Partick P. W. Chen', 'link': 'https://arxiv.org/abs/2506.09083', 'abstract': 'Accurately labeling (or annotation) data is still a bottleneck in computer vision, especially for large-scale tasks where manual labeling is time-consuming and error-prone. While tools like LabelImg can handle the labeling task, some of them still require annotators to manually label each image. In this paper, we introduce BakuFlow, a streamlining semi-automatic label generation tool. Key features include (1) a live adjustable magnifier for pixel-precise manual corrections, improving user experience; (2) an interactive data augmentation module to diversify training datasets; (3) label propagation for rapidly copying labeled objects between consecutive frames, greatly accelerating annotation of video data; and (4) an automatic labeling module powered by a modified YOLOE framework. Unlike the original YOLOE, our extension supports adding new object classes and any number of visual prompts per class during annotation, enabling flexible and scalable labeling for dynamic, real-world datasets. These innovations make BakuFlow especially effective for object detection and tracking, substantially reducing labeling workload and improving efficiency in practical computer vision and industrial scenarios.', 'abstract_zh': '准确标注数据仍然是计算机视觉中的一个瓶颈，尤其是在大规模任务中，手动标注既耗时又容易出错。虽然有LabelImg这样的工具可以处理标注任务，但其中一些工具仍然要求标注员手动标注每张图片。本文介绍了一种名为BakuFlow的半自动标注工具，关键功能包括：（1）动态可调节放大镜以实现像素级精确的手动修正，提升用户体验；（2）交互式数据增强模块以多样化训练数据集；（3）标签传播以快速复制连续帧之间的标注对象，大幅加速视频数据标注；（4）基于修改后的YOLOE框架的自动标注模块。与原始YOLOE不同，我们的扩展支持在标注过程中为每个类添加新的对象类别和任意数量的视觉提示，从而实现灵活且可扩展的标注，适用于动态的真实世界数据集。这些创新使BakuFlow特别适用于目标检测和跟踪，大幅减少了标注工作量并提高了实际计算机视觉和工业场景中的效率。', 'title_zh': 'BakuFlow: 一种流程化半自动标签生成工具'}
{'arxiv_id': 'arXiv:2506.09070', 'title': 'STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support', 'authors': 'Chenqi Zhang, Yu Feng, Jieru Zhao, Guangda Liu, Wenchao Ding, Chentao Wu, Minyi Guo', 'link': 'https://arxiv.org/abs/2506.09070', 'abstract': '3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and sparse Gaussian-based representation. However, 3DGS struggles to meet the real-time requirement of 90 frames per second (FPS) on resource-constrained mobile devices, achieving only 2 to 9 this http URL accelerators focus on compute efficiency but overlook memory efficiency, leading to redundant DRAM traffic. We introduce STREAMINGGS, a fully streaming 3DGS algorithm-architecture co-design that achieves fine-grained pipelining and reduces DRAM traffic by transforming from a tile-centric rendering to a memory-centric rendering. Results show that our design achieves up to 45.7 $\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.', 'abstract_zh': '3D高斯渲染（3DGS）因其高效性和稀疏的高斯表示而受到欢迎。然而，3DGS在资源受限的移动设备上难以满足每秒90帧（FPS）的需求，只能达到2到9倍的加速。现有加速器注重计算效率但忽视了内存效率，导致冗余的DRAM流量。我们提出了STREAMINGGS，这是一种完全流水线的3DGS算法-架构协同设计，通过从以_tile为中心的渲染转变为以_memory为中心的渲染，实现了精细粒度的流水线化并减少了DRAM流量。实验结果表明，与移动Ampere GPU相比，我们的设计可实现高达45.7倍的加速和62.9倍的能量节省。', 'title_zh': '基于体素的流式3D高斯点表示方法：内存优化与架构支持'}
{'arxiv_id': 'arXiv:2506.09065', 'title': 'Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis', 'authors': 'Abigail Copiaco, Christian Ritz, Yassine Himeur, Valsamma Eapen, Ammar Albanna, Wathiq Mansoor', 'link': 'https://arxiv.org/abs/2506.09065', 'abstract': "The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the past decade, posing significant challenges in communication, behavior, and focus for affected individuals. Current diagnostic techniques, though effective, are time-intensive, leading to high social and economic costs. This work introduces an AI-powered assistive technology designed to streamline ASD diagnosis and management, enhancing convenience for individuals with ASD and efficiency for caregivers and therapists. The system integrates transfer learning with image transforms derived from eye gaze variables to diagnose ASD. This facilitates and opens opportunities for in-home periodical diagnosis, reducing stress for individuals and caregivers, while also preserving user privacy through the use of image transforms. The accessibility of the proposed method also offers opportunities for improved communication between guardians and therapists, ensuring regular updates on progress and evolving support needs. Overall, the approach proposed in this work ensures timely, accessible diagnosis while protecting the subjects' privacy, improving outcomes for individuals with ASD.", 'abstract_zh': '自闭症谱系障碍（ASD）的流行率在过去十年中迅速上升，给受影响个体的沟通、行为和专注带来了重大挑战。现有的诊断技术虽然有效，但耗时较长，导致社会和经济成本高昂。本研究介绍了基于人工智能的辅助技术，旨在简化ASD的诊断和管理流程，提高受影响个体的便利性和护理者及治疗师的效率。该系统结合了基于视线变量的图像变换和迁移学习，以诊断ASD。这促进了居家定期诊断的可能性，减轻了个体和护理者的精神压力，同时通过图像变换保护用户隐私。该方法的可访问性还为监护人和治疗师之间的沟通提供了机会，确保了定期更新进展和不断变化的支持需求。总之，本研究提出的方法确保了及时、可访问的诊断，并保护了受试者的隐私，从而改善了ASD患者的预后。', 'title_zh': '基于眼球凝视变量导出的图像变换在渐进式自闭症诊断中的探索'}
