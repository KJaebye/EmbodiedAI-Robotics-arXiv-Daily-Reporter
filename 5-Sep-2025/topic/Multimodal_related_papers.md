# OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection 

**Title (ZH)**: OVGrasp: 基于多模态意图检测的开放词汇抓取辅助 

**Authors**: Chen Hu, Shan Luo, Letizia Gionfrida  

**Link**: [PDF](https://arxiv.org/pdf/2509.04324)  

**Abstract**: Grasping assistance is essential for restoring autonomy in individuals with motor impairments, particularly in unstructured environments where object categories and user intentions are diverse and unpredictable. We present OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp assistance that integrates RGB-D vision, open-vocabulary prompts, and voice commands to enable robust multimodal interaction. To enhance generalization in open environments, OVGrasp incorporates a vision-language foundation model with an open-vocabulary mechanism, allowing zero-shot detection of previously unseen objects without retraining. A multimodal decision-maker further fuses spatial and linguistic cues to infer user intent, such as grasp or release, in multi-object scenarios. We deploy the complete framework on a custom egocentric-view wearable exoskeleton and conduct systematic evaluations on 15 objects across three grasp types. Experimental results with ten participants demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%, outperforming state-of-the-art baselines and achieving improved kinematic alignment with natural hand motion. 

**Abstract (ZH)**: 基于软外骨骼的层级控制框架OVGrasp：结合RGB-D视觉、开放词汇提示和语音指令实现稳健多模态交互 

---
# QuesGenie: Intelligent Multimodal Question Generation 

**Title (ZH)**: QuesGenie: 智能多模态问题生成 

**Authors**: Ahmed Mubarak, Amna Ahmed, Amira Nasser, Aya Mohamed, Fares El-Sadek, Mohammed Ahmed, Ahmed Salah, Youssef Sobhy  

**Link**: [PDF](https://arxiv.org/pdf/2509.03535)  

**Abstract**: In today's information-rich era, learners have access to abundant educational resources, but the lack of practice materials tailored to these resources presents a significant challenge. This project addresses that gap by developing a multi-modal question generation system that can automatically generate diverse question types from various content formats. The system features four major components: multi-modal input handling, question generation, reinforcement learning from human feedback (RLHF), and an end-to-end interactive interface. This project lays the foundation for automated, scalable, and intelligent question generation, carefully balancing resource efficiency, robust functionality and a smooth user experience. 

**Abstract (ZH)**: 在信息丰富的时代，学习者可以访问丰富的教育资源，但缺乏针对这些资源定制的练习材料构成了一个重大挑战。该项目通过开发一个能够从多种内容格式中自动生成多样化题型的多模态问题生成系统来解决这一缺口。该系统包含四大组件：多模态输入处理、问题生成、基于人类反馈的强化学习（RLHF）以及端到端的交互界面。该项目为自动化、可扩展和智能化的问题生成奠定了基础，精心平衡了资源效率、稳健功能和流畅的用户体验。 

---
# Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages 

**Title (ZH)**: 基于AI的多模态提案工具以增加消息交叉评估 

**Authors**: Alejandro Álvarez Castro, Joaquín Ordieres-Meré  

**Link**: [PDF](https://arxiv.org/pdf/2509.03529)  

**Abstract**: Earnings calls represent a uniquely rich and semi-structured source of financial communication, blending scripted managerial commentary with unscripted analyst dialogue. Although recent advances in financial sentiment analysis have integrated multi-modal signals, such as textual content and vocal tone, most systems rely on flat document-level or sentence-level models, failing to capture the layered discourse structure of these interactions. This paper introduces a novel multi-modal framework designed to generate semantically rich and structurally aware embeddings of earnings calls, by encoding them as hierarchical discourse trees. Each node, comprising either a monologue or a question-answer pair, is enriched with emotional signals derived from text, audio, and video, as well as structured metadata including coherence scores, topic labels, and answer coverage assessments. A two-stage transformer architecture is proposed: the first encodes multi-modal content and discourse metadata at the node level using contrastive learning, while the second synthesizes a global embedding for the entire conference. Experimental results reveal that the resulting embeddings form stable, semantically meaningful representations that reflect affective tone, structural logic, and thematic alignment. Beyond financial reporting, the proposed system generalizes to other high-stakes unscripted communicative domains such as tele-medicine, education, and political discourse, offering a robust and explainable approach to multi-modal discourse representation. This approach offers practical utility for downstream tasks such as financial forecasting and discourse evaluation, while also providing a generalizable method applicable to other domains involving high-stakes communication. 

**Abstract (ZH)**: earnings电话代表了一种独特丰富且半结构化的财务沟通来源，将剧本化的企业管理评论与非剧本化的分析师对话相结合。尽管最近在财务情绪分析方面的进展已经整合了多模态信号，如文本内容和语音语调，但大多数系统仍然依赖于平面的文档级或句级模型，未能捕捉到这些互动的多层次话语结构。本文介绍了一种新颖的多模态框架，旨在通过将earnings电话编码为分层话语树来生成语义丰富且结构意识强的嵌入。每个节点，包括独白或问答对，都会从文本、音频和视频中提取情感信号，并结合包括连贯性评分、主题标签和答案覆盖率评估在内的结构化元数据。提出了一种两阶段变换器架构：第一阶段通过对比学习在节点级别编码多模态内容和话语元数据，而第二阶段综合生成整个会议的全局嵌入。实验结果表明，生成的嵌入形成了稳定且语义上有意义的表示，反映了情感基调、结构逻辑和主题对齐。除了财务报告，所提出的系统还适用于其他高风险非剧本对话领域，如远程医疗、教育和政治论述，提供了一种稳健且可解释的多模态话语表示方法。该方法为财务预测和话语评估等下游任务提供了实际用途，同时也提供了一种适用于涉及高风险通信的其他领域的通用方法。 

---
