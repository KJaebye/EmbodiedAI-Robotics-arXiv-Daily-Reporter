# MultiClear: Multimodal Soft Exoskeleton Glove for Transparent Object Grasping Assistance 

**Title (ZH)**: MultiClear：多模态柔软外骨骼手套，用于透明物体抓取辅助 

**Authors**: Chen Hu, Timothy Neate, Shan Luo, Letizia Gionfrida  

**Link**: [PDF](https://arxiv.org/pdf/2504.03379)  

**Abstract**: Grasping is a fundamental skill for interacting with the environment. However, this ability can be difficult for some (e.g. due to disability). Wearable robotic solutions can enhance or restore hand function, and recent advances have leveraged computer vision to improve grasping capabilities. However, grasping transparent objects remains challenging due to their poor visual contrast and ambiguous depth cues. Furthermore, while multimodal control strategies incorporating tactile and auditory feedback have been explored to grasp transparent objects, the integration of vision with these modalities remains underdeveloped. This paper introduces MultiClear, a multimodal framework designed to enhance grasping assistance in a wearable soft exoskeleton glove for transparent objects by fusing RGB data, depth data, and auditory signals. The exoskeleton glove integrates a tendon-driven actuator with an RGB-D camera and a built-in microphone. To achieve precise and adaptive control, a hierarchical control architecture is proposed. For the proposed hierarchical control architecture, a high-level control layer provides contextual awareness, a mid-level control layer processes multimodal sensory inputs, and a low-level control executes PID motor control for fine-tuned grasping adjustments. The challenge of transparent object segmentation was managed by introducing a vision foundation model for zero-shot segmentation. The proposed system achieves a Grasping Ability Score of 70.37%, demonstrating its effectiveness in transparent object manipulation. 

**Abstract (ZH)**: 多模态框架MultiClear增强穿戴式软exo手套在透明物体抓取中的辅助能力 

---
# DML-RAM: Deep Multimodal Learning Framework for Robotic Arm Manipulation using Pre-trained Models 

**Title (ZH)**: 基于预训练模型的深度多模态学习框架：应用于机器人臂操作 

**Authors**: Sathish Kumar, Swaroop Damodaran, Naveen Kumar Kuruba, Sumit Jha, Arvind Ramanathan  

**Link**: [PDF](https://arxiv.org/pdf/2504.03423)  

**Abstract**: This paper presents a novel deep learning framework for robotic arm manipulation that integrates multimodal inputs using a late-fusion strategy. Unlike traditional end-to-end or reinforcement learning approaches, our method processes image sequences with pre-trained models and robot state data with machine learning algorithms, fusing their outputs to predict continuous action values for control. Evaluated on BridgeData V2 and Kuka datasets, the best configuration (VGG16 + Random Forest) achieved MSEs of 0.0021 and 0.0028, respectively, demonstrating strong predictive performance and robustness. The framework supports modularity, interpretability, and real-time decision-making, aligning with the goals of adaptive, human-in-the-loop cyber-physical systems. 

**Abstract (ZH)**: 一种集成多模态输入的新型深度学习机器人手臂 manipulation 框架：基于晚融合策略的研究 

---
# Towards deployment-centric multimodal AI beyond vision and language 

**Title (ZH)**: 面向部署的多模态AI超越视觉和语言 

**Authors**: Xianyuan Liu, Jiayang Zhang, Shuo Zhou, Thijs L. van der Plas, Avish Vijayaraghavan, Anastasiia Grishina, Mengdie Zhuang, Daniel Schofield, Christopher Tomlinson, Yuhan Wang, Ruizhe Li, Louisa van Zeeland, Sina Tabakhi, Cyndie Demeocq, Xiang Li, Arunav Das, Orlando Timmerman, Thomas Baldwin-McDonald, Jinge Wu, Peizhen Bai, Zahraa Al Sahili, Omnia Alwazzan, Thao N. Do, Mohammod N.I. Suvon, Angeline Wang, Lucia Cipolina-Kun, Luigi A. Moretti, Lucas Farndale, Nitisha Jain, Natalia Efremova, Yan Ge, Marta Varela, Hak-Keung Lam, Oya Celiktutan, Ben R. Evans, Alejandro Coca-Castro, Honghan Wu, Zahraa S. Abdallah, Chen Chen, Valentin Danchev, Nataliya Tkachenko, Lei Lu, Tingting Zhu, Gregory G. Slabaugh, Roger K. Moore, William K. Cheung, Peter H. Charlton, Haiping Lu  

**Link**: [PDF](https://arxiv.org/pdf/2504.03603)  

**Abstract**: Multimodal artificial intelligence (AI) integrates diverse types of data via machine learning to improve understanding, prediction, and decision-making across disciplines such as healthcare, science, and engineering. However, most multimodal AI advances focus on models for vision and language data, while their deployability remains a key challenge. We advocate a deployment-centric workflow that incorporates deployment constraints early to reduce the likelihood of undeployable solutions, complementing data-centric and model-centric approaches. We also emphasise deeper integration across multiple levels of multimodality and multidisciplinary collaboration to significantly broaden the research scope beyond vision and language. To facilitate this approach, we identify common multimodal-AI-specific challenges shared across disciplines and examine three real-world use cases: pandemic response, self-driving car design, and climate change adaptation, drawing expertise from healthcare, social science, engineering, science, sustainability, and finance. By fostering multidisciplinary dialogue and open research practices, our community can accelerate deployment-centric development for broad societal impact. 

**Abstract (ZH)**: 多模态人工智能通过机器学习整合多种类型的数据，以提高跨医学、科学和工程等领域的问题理解、预测和决策能力。然而，大多数多模态人工智能的进步集中在视觉和语言数据模型上，部署仍是一个关键挑战。我们提倡一种以部署为中心的工作流程，早期整合部署约束以减少不可部署解决方案的可能性，同时补充数据为中心和模型为中心的方法。我们还强调在多个层面实现更深层次的多模态融合，并促进跨学科合作，以显著拓宽研究范围，超越视觉和语言领域。为了实现这一目标，我们确定了跨学科共享的多模态人工智能特定挑战，并分析了三个实际应用案例：疫情应对、自动驾驶汽车设计和气候变化适应，吸取了医学、社会科学、工程学、科学、可持续发展和金融领域的专业知识。通过促进跨学科对话和开放研究实践，我们的社群可以加速以部署为中心的发展，以实现广泛的社会影响。 

---
# Stance-Driven Multimodal Controlled Statement Generation: New Dataset and Task 

**Title (ZH)**: 基于立场的多模态受控语句生成：新数据集与任务 

**Authors**: Bingqian Wang, Quan Fang, Jiachen Sun, Xiaoxiao Ma  

**Link**: [PDF](https://arxiv.org/pdf/2504.03295)  

**Abstract**: Formulating statements that support diverse or controversial stances on specific topics is vital for platforms that enable user expression, reshape political discourse, and drive social critique and information dissemination. With the rise of Large Language Models (LLMs), controllable text generation towards specific stances has become a promising research area with applications in shaping public opinion and commercial marketing. However, current datasets often focus solely on pure texts, lacking multimodal content and effective context, particularly in the context of stance detection. In this paper, we formally define and study the new problem of stance-driven controllable content generation for tweets with text and images, where given a multimodal post (text and image/video), a model generates a stance-controlled response. To this end, we create the Multimodal Stance Generation Dataset (StanceGen2024), the first resource explicitly designed for multimodal stance-controllable text generation in political discourse. It includes posts and user comments from the 2024 U.S. presidential election, featuring text, images, videos, and stance annotations to explore how multimodal political content shapes stance expression. Furthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework that integrates weighted fusion of multimodal features and stance guidance to improve semantic consistency and stance control. We release the dataset and code (this https URL) for public use and further research. 

**Abstract (ZH)**: 基于多模态内容生成的立场驱动可控表达：以2024年美国大选推文为例 

---
