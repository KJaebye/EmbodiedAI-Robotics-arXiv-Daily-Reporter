{'arxiv_id': 'arXiv:2504.03369', 'title': 'Point Cloud-based Grasping for Soft Hand Exoskeleton', 'authors': 'Chen Hu, Enrica Tricomi, Eojin Rho, Daekyum Kim, Lorenzo Masia, Shan Luo, Letizia Gionfrida', 'link': 'https://arxiv.org/abs/2504.03369', 'abstract': 'Grasping is a fundamental skill for interacting with and manipulating objects in the environment. However, this ability can be challenging for individuals with hand impairments. Soft hand exoskeletons designed to assist grasping can enhance or restore essential hand functions, yet controlling these soft exoskeletons to support users effectively remains difficult due to the complexity of understanding the environment. This study presents a vision-based predictive control framework that leverages contextual awareness from depth perception to predict the grasping target and determine the next control state for activation. Unlike data-driven approaches that require extensive labelled datasets and struggle with generalizability, our method is grounded in geometric modelling, enabling robust adaptation across diverse grasping scenarios. The Grasping Ability Score (GAS) was used to evaluate performance, with our system achieving a state-of-the-art GAS of 91% across 15 objects and healthy participants, demonstrating its effectiveness across different object types. The proposed approach maintained reconstruction success for unseen objects, underscoring its enhanced generalizability compared to learning-based models.', 'abstract_zh': '基于视觉的预测控制框架：利用深度感知的上下文意识预测抓取目标并确定下一控制状态', 'title_zh': '基于点云的软手外骨骼抓取技术'}
{'arxiv_id': 'arXiv:2504.03129', 'title': 'GraphSeg: Segmented 3D Representations via Graph Edge Addition and Contraction', 'authors': 'Haozhan Tang, Tianyi Zhang, Oliver Kroemer, Matthew Johnson-Roberson, Weiming Zhi', 'link': 'https://arxiv.org/abs/2504.03129', 'abstract': "Robots operating in unstructured environments often require accurate and consistent object-level representations. This typically requires segmenting individual objects from the robot's surroundings. While recent large models such as Segment Anything (SAM) offer strong performance in 2D image segmentation. These advances do not translate directly to performance in the physical 3D world, where they often over-segment objects and fail to produce consistent mask correspondences across views. In this paper, we present GraphSeg, a framework for generating consistent 3D object segmentations from a sparse set of 2D images of the environment without any depth information. GraphSeg adds edges to graphs and constructs dual correspondence graphs: one from 2D pixel-level similarities and one from inferred 3D structure. We formulate segmentation as a problem of edge addition, then subsequent graph contraction, which merges multiple 2D masks into unified object-level segmentations. We can then leverage \\emph{3D foundation models} to produce segmented 3D representations. GraphSeg achieves robust segmentation with significantly fewer images and greater accuracy than prior methods. We demonstrate state-of-the-art performance on tabletop scenes and show that GraphSeg enables improved performance on downstream robotic manipulation tasks. Code available at this https URL.", 'abstract_zh': 'robots在非结构化环境中的操作通常需要准确且一致的对象级表示。这通常需要将个体物体从机器人的周围环境中分割出来。虽然近年来的大模型如Segment Anything (SAM)在2D图像分割方面表现出强大的性能，但这些进步并未直接转化为在物理3D世界中的性能，其中它们往往会过度分割物体，并且无法在不同视角下产生一致的掩码对应关系。在本文中，我们提出GraphSeg框架，用于从环境的稀疏2D图像集中生成一致的3D对象分割，无需任何深度信息。GraphSeg会向图中添加边并构建双重对应图：一个是基于2D像素级相似性的，另一个是基于推断出的3D结构的。我们将分割问题形式化为边添加问题，随后进行图收缩，从而将多个2D掩码合并为统一的对象级分割。我们还可以利用\\emph{3D基础模型}来生成分割的3D表示。GraphSeg通过使用显著较少的图像实现更鲁棒且更精确的分割，优于先前的方法。我们在桌面上场景中展示了最先进的性能，并证明GraphSeg能够使下游机器人 manipulation任务的表现得以提升。代码已发布于这个httpsURL。', 'title_zh': 'GraphSeg：通过图边增加和收缩实现的分段3D表示'}
{'arxiv_id': 'arXiv:2504.03171', 'title': 'Real-Time Roadway Obstacle Detection for Electric Scooters Using Deep Learning and Multi-Sensor Fusion', 'authors': 'Zeyang Zheng, Arman Hosseini, Dong Chen, Omid Shoghli, Arsalan Heydarian', 'link': 'https://arxiv.org/abs/2504.03171', 'abstract': 'The increasing adoption of electric scooters (e-scooters) in urban areas has coincided with a rise in traffic accidents and injuries, largely due to their small wheels, lack of suspension, and sensitivity to uneven surfaces. While deep learning-based object detection has been widely used to improve automobile safety, its application for e-scooter obstacle detection remains unexplored. This study introduces a novel ground obstacle detection system for e-scooters, integrating an RGB camera, and a depth camera to enhance real-time road hazard detection. Additionally, the Inertial Measurement Unit (IMU) measures linear vertical acceleration to identify surface vibrations, guiding the selection of six obstacle categories: tree branches, manhole covers, potholes, pine cones, non-directional cracks, and truncated domes. All sensors, including the RGB camera, depth camera, and IMU, are integrated within the Intel RealSense Camera D435i. A deep learning model powered by YOLO detects road hazards and utilizes depth data to estimate obstacle proximity. Evaluated on the seven hours of naturalistic riding dataset, the system achieves a high mean average precision (mAP) of 0.827 and demonstrates excellent real-time performance. This approach provides an effective solution to enhance e-scooter safety through advanced computer vision and data fusion. The dataset is accessible at this https URL, and the project code is hosted on this https URL.', 'abstract_zh': '电助力踏板车(e-scooter)在城市区域的日益普及 coincided with 交通事故和伤害的增加，主要原因在于其小型车轮、缺乏减震装置以及对不平路面的敏感性。尽管基于深度学习的目标检测技术已被广泛应用于汽车安全改进，但其在电助力踏板车障碍检测方面的应用尚未得到探索。本研究引入了一种新型电助力踏板车地面障碍检测系统，结合RGB相机和深度相机以增强实时道路障碍检测。此外，惯性测量单元(IMU)测量线性垂直加速度以识别路面振动，并据此将障碍物分为六类：树枝、井盖、坑洞、松果、无方向裂缝和截顶圆顶。所有传感器，包括RGB相机、深度相机和IMU，均集成在Intel RealSense Camera D435i中。由YOLO驱动的深度学习模型检测道路障碍，并利用深度数据估算障碍物距离。该系统在七小时的自然骑行数据集上进行了评估，平均精度(mAP)达到0.827，并展示了出色的实时性能。该方法通过先进的计算机视觉和数据融合为提高电助力踏板车安全提供了一种有效解决方案。数据集可访问于此 https URL，项目代码托管于此 https URL。', 'title_zh': '基于深度学习和多传感器融合的电动滑板车实时道路障碍检测'}
{'arxiv_id': 'arXiv:2504.03600', 'title': 'MedSAM2: Segment Anything in 3D Medical Images and Videos', 'authors': 'Jun Ma, Zongxin Yang, Sumin Kim, Bihui Chen, Mohammed Baharoon, Adibvafa Fallahpour, Reza Asakereh, Hongwei Lyu, Bo Wang', 'link': 'https://arxiv.org/abs/2504.03600', 'abstract': 'Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos with comprehensive user studies. Here, we present MedSAM2, a promptable segmentation foundation model for 3D image and video segmentation. The model is developed by fine-tuning the Segment Anything Model 2 on a large medical dataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming previous models across a wide range of organs, lesions, and imaging modalities. Furthermore, we implement a human-in-the-loop pipeline to facilitate the creation of large-scale datasets resulting in, to the best of our knowledge, the most extensive user study to date, involving the annotation of 5,000 CT lesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames, demonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is also integrated into widely used platforms with user-friendly interfaces for local and cloud deployment, making it a practical tool for supporting efficient, scalable, and high-quality segmentation in both research and healthcare environments.', 'abstract_zh': '可提示的3D医学图像与视频分割基础模型MedSAM2', 'title_zh': 'MedSAM2: 三维医学图像和视频中的实例分割'}
{'arxiv_id': 'arXiv:2504.03490', 'title': 'BUFF: Bayesian Uncertainty Guided Diffusion Probabilistic Model for Single Image Super-Resolution', 'authors': 'Zihao He, Shengchuan Zhang, Runze Hu, Yunhang Shen, Yan Zhang', 'link': 'https://arxiv.org/abs/2504.03490', 'abstract': "Super-resolution (SR) techniques are critical for enhancing image quality, particularly in scenarios where high-resolution imagery is essential yet limited by hardware constraints. Existing diffusion models for SR have relied predominantly on Gaussian models for noise generation, which often fall short when dealing with the complex and variable texture inherent in natural scenes. To address these deficiencies, we introduce the Bayesian Uncertainty Guided Diffusion Probabilistic Model (BUFF). BUFF distinguishes itself by incorporating a Bayesian network to generate high-resolution uncertainty masks. These masks guide the diffusion process, allowing for the adjustment of noise intensity in a manner that is both context-aware and adaptive. This novel approach not only enhances the fidelity of super-resolved images to their original high-resolution counterparts but also significantly mitigates artifacts and blurring in areas characterized by complex textures and fine details. The model demonstrates exceptional robustness against complex noise patterns and showcases superior adaptability in handling textures and edges within images. Empirical evidence, supported by visual results, illustrates the model's robustness, especially in challenging scenarios, and its effectiveness in addressing common SR issues such as blurring. Experimental evaluations conducted on the DIV2K dataset reveal that BUFF achieves a notable improvement, with a +0.61 increase compared to baseline in SSIM on BSD100, surpassing traditional diffusion approaches by an average additional +0.20dB PSNR gain. These findings underscore the potential of Bayesian methods in enhancing diffusion processes for SR, paving the way for future advancements in the field.", 'abstract_zh': 'Bayesian不确定性引导扩散概率模型（ BUFF）：用于增强图像质量的复杂纹理场景超分辨率', 'title_zh': 'BUFF: 基于贝叶斯不确定性引导扩散的概率模型单张图像超分辨率'}
{'arxiv_id': 'arXiv:2504.03469', 'title': 'Physics-informed 4D X-ray image reconstruction from ultra-sparse spatiotemporal data', 'authors': 'Zisheng Yao, Yuhe Zhang, Zhe Hu, Robert Klöfkorn, Tobias Ritschel, Pablo Villanueva-Perez', 'link': 'https://arxiv.org/abs/2504.03469', 'abstract': 'The unprecedented X-ray flux density provided by modern X-ray sources offers new spatiotemporal possibilities for X-ray imaging of fast dynamic processes. Approaches to exploit such possibilities often result in either i) a limited number of projections or spatial information due to limited scanning speed, as in time-resolved tomography, or ii) a limited number of time points, as in stroboscopic imaging, making the reconstruction problem ill-posed and unlikely to be solved by classical reconstruction approaches. 4D reconstruction from such data requires sample priors, which can be included via deep learning (DL). State-of-the-art 4D reconstruction methods for X-ray imaging combine the power of AI and the physics of X-ray propagation to tackle the challenge of sparse views. However, most approaches do not constrain the physics of the studied process, i.e., a full physical model. Here we present 4D physics-informed optimized neural implicit X-ray imaging (4D-PIONIX), a novel physics-informed 4D X-ray image reconstruction method combining the full physical model and a state-of-the-art DL-based reconstruction method for 4D X-ray imaging from sparse views. We demonstrate and evaluate the potential of our approach by retrieving 4D information from ultra-sparse spatiotemporal acquisitions of simulated binary droplet collisions, a relevant fluid dynamic process. We envision that this work will open new spatiotemporal possibilities for various 4D X-ray imaging modalities, such as time-resolved X-ray tomography and more novel sparse acquisition approaches like X-ray multi-projection imaging, which will pave the way for investigations of various rapid 4D dynamics, such as fluid dynamics and composite testing.', 'abstract_zh': '现代X射线源提供的前所未有的X射线通量密度为快速动态过程的X射线成像提供了新的时空可能性。利用此类可能性的方法往往会导致时间分辨层析成像中投影或空间信息有限，或者在荧光成像中时间点有限，从而使重建问题变得病态，难以通过经典重建方法解决。从稀疏数据中进行4D重建需要样本先验，可以通过深度学习（DL）纳入其中。最新的4D重建方法结合了人工智能和X射线传播的物理原理，以应对稀疏视图挑战。然而，大多数方法并未约束所研究过程的物理定律，即整个物理模型。我们提出了4D物理约束优化神经隐式X射线成像（4D-PIONIX），这是一种结合完整物理模型和基于DL的4D X射线成像稀疏视图先进重建方法的新颖物理约束4D X射线图像重建方法。通过从模拟二元液滴碰撞的超稀疏时空采集中检索4D信息，我们展示了并评估了我们方法的潜力。我们设想，这项工作将为时间分辨X射线层析成像以及X射线多投影成像等更新型稀疏采集方法开辟新的时空可能性，从而为介观动力学和复合材料测试等各种快速4D动力学的研究铺平道路。', 'title_zh': '基于物理约束的4D X射线图像超稀疏空时数据重建'}
{'arxiv_id': 'arXiv:2504.03241', 'title': 'Rotation Invariance in Floor Plan Digitization using Zernike Moments', 'authors': 'Marius Graumann, Jan Marius Stürmer, Tobias Koch', 'link': 'https://arxiv.org/abs/2504.03241', 'abstract': 'Nowadays, a lot of old floor plans exist in printed form or are stored as scanned raster images. Slight rotations or shifts may occur during scanning. Bringing floor plans of this form into a machine readable form to enable further use, still poses a problem. Therefore, we propose an end-to-end pipeline that pre-processes the image and leverages a novel approach to create a region adjacency graph (RAG) from the pre-processed image and predict its nodes. By incorporating normalization steps into the RAG feature extraction, we significantly improved the rotation invariance of the RAG feature calculation. Moreover, applying our method leads to an improved F1 score and IoU on rotated data. Furthermore, we proposed a wall splitting algorithm for partitioning walls into segments associated with the corresponding rooms.', 'abstract_zh': '如今，大量旧楼计划以印刷形式存在或以扫描的位图图像形式存储。扫描过程中可能会发生轻微的旋转或偏移。将这种形式的楼计划转换成机器可读的格式以供进一步使用，仍存在一个问题。因此，我们提出了一种端到端的流程，该流程预处理图像并利用新颖的方法从预处理的图像中创建区域相邻图（RAG）并预测其节点。通过将归一化步骤纳入RAG特征提取，我们显著提高了RAG特征计算的旋转不变性。此外，应用我们的方法可提高旋转数据上的F1分数和IoU。同时，我们还提出了一种墙壁分割算法，用于将墙壁分割成与相应房间相关的段。', 'title_zh': '使用Zernike矩的地板平面图数字化的旋转不变性研究'}
{'arxiv_id': 'arXiv:2504.03238', 'title': 'Malware Detection in Docker Containers: An Image is Worth a Thousand Logs', 'authors': 'Akis Nousias, Efklidis Katsaros, Evangelos Syrmos, Panagiotis Radoglou-Grammatikis, Thomas Lagkas, Vasileios Argyriou, Ioannis Moscholios, Evangelos Markakis, Sotirios Goudos, Panagiotis Sarigiannidis', 'link': 'https://arxiv.org/abs/2504.03238', 'abstract': 'Malware detection is increasingly challenged by evolving techniques like obfuscation and polymorphism, limiting the effectiveness of traditional methods. Meanwhile, the widespread adoption of software containers has introduced new security challenges, including the growing threat of malicious software injection, where a container, once compromised, can serve as entry point for further cyberattacks. In this work, we address these security issues by introducing a method to identify compromised containers through machine learning analysis of their file systems. We cast the entire software containers into large RGB images via their tarball representations, and propose to use established Convolutional Neural Network architectures on a streaming, patch-based manner. To support our experiments, we release the COSOCO dataset--the first of its kind--containing 3364 large-scale RGB images of benign and compromised software containers at this https URL. Our method detects more malware and achieves higher F1 and Recall scores than all individual and ensembles of VirusTotal engines, demonstrating its effectiveness and setting a new standard for identifying malware-compromised software containers.', 'abstract_zh': '恶意软件检测 increasingly受到混淆技术和多形性等演变技术的挑战，限制了传统方法的有效性。同时，软件容器的广泛应用引入了新的安全挑战，包括恶意软件注入的日益严重威胁，一旦容器被攻破，就可能成为进一步网络攻击的入口。在此项工作中，我们通过机器学习分析容器文件系统来识别受感染的容器，以应对这些安全问题。我们将整个软件容器通过其tarball表示转化为大规模的RGB图像，并提出了一种基于流式、补丁级的卷积神经网络架构。为支持我们的实验，我们发布了COSOCO数据集——这是首个包含3364个良性与受感染的软件容器大规模RGB图像的数据集，可在以下链接访问：https://cosoco.alicloudapi.com。我们的方法检测到更多的恶意软件，并且在F1和召回率上优于所有单个和组合的VirusTotal引擎，证明了其有效性并为识别受恶意软件感染的软件容器设定了新的标准。', 'title_zh': 'Docker容器中的恶意软件检测：一幅图胜过千行日志'}
{'arxiv_id': 'arXiv:2504.03235', 'title': 'Crash Time Matters: HybridMamba for Fine-Grained Temporal Localization in Traffic Surveillance Footage', 'authors': 'Ibne Farabi Shihab, Anuj Sharma', 'link': 'https://arxiv.org/abs/2504.03235', 'abstract': 'Traffic crash detection in long-form surveillance videos is critical for emergency response and infrastructure planning but remains difficult due to the brief and rare nature of crash events. We introduce HybridMamba, a novel architecture that combines visual transformers with state-space temporal modeling to achieve accurate crash time localization. Our method uses multi-level token compression and hierarchical temporal processing to remain computationally efficient without sacrificing temporal resolution. Evaluated on a large-scale dataset from the Iowa Department of Transportation, HybridMamba achieves a mean absolute error of 1.50 seconds, with 65.2 percent of predictions within one second of the ground truth. It outperforms recent video-language models such as TimeChat and VideoLLaMA2 by up to 2.8 seconds, while using significantly fewer parameters. Our results demonstrate strong generalization across videos ranging from 2 to 40 minutes in diverse conditions. HybridMamba offers a robust and efficient solution for fine-grained temporal localization in traffic surveillance. The code will be released upon publication.', 'abstract_zh': '基于长时监控视频的交通事故检测对于应急响应和基础设施规划至关重要，但由于事故事件短暂且稀少，这一任务仍具有挑战性。我们提出了一种名为HybridMamba的新型架构，该架构结合了视觉变换器与状态空间时间建模，以实现精确的事故时间定位。该方法通过多级令牌压缩和层次时间处理，在保持时空分辨率的同时保持了高效的计算性能。在爱荷华州交通运输部的大规模数据集上评估，HybridMamba实现了1.50秒的平均绝对误差，其中65.2%的预测与真实值相差不超过1秒。与近期的视频语言模型TimeChat和VideoLLaMA2相比，HybridMamba在性能上高出多达2.8秒，同时参数量显著减少。我们的结果表明，HybridMamba在不同条件下的2到40分钟视频中展现出强大的泛化能力。HybridMamba为交通监控中的细粒度时间定位提供了稳健且高效的解决方案。代码将在出版后发布。', 'title_zh': '碰撞时间至关重要：HybridMamba在交通监控视频中实现细粒度时间定位'}
{'arxiv_id': 'arXiv:2504.03198', 'title': 'Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video', 'authors': 'Jiaxin Guo, Wenzhen Dong, Tianyu Huang, Hao Ding, Ziyi Wang, Haomin Kuang, Qi Dou, Yun-Hui Liu', 'link': 'https://arxiv.org/abs/2504.03198', 'abstract': "Reconstructing 3D scenes from monocular surgical videos can enhance surgeon's perception and therefore plays a vital role in various computer-assisted surgery tasks. However, achieving scale-consistent reconstruction remains an open challenge due to inherent issues in endoscopic videos, such as dynamic deformations and textureless surfaces. Despite recent advances, current methods either rely on calibration or instrument priors to estimate scale, or employ SfM-like multi-stage pipelines, leading to error accumulation and requiring offline optimization. In this paper, we present Endo3R, a unified 3D foundation model for online scale-consistent reconstruction from monocular surgical video, without any priors or extra optimization. Our model unifies the tasks by predicting globally aligned pointmaps, scale-consistent video depths, and camera parameters without any offline optimization. The core contribution of our method is expanding the capability of the recent pairwise reconstruction model to long-term incremental dynamic reconstruction by an uncertainty-aware dual memory mechanism. The mechanism maintains history tokens of both short-term dynamics and long-term spatial consistency. Notably, to tackle the highly dynamic nature of surgical scenes, we measure the uncertainty of tokens via Sampson distance and filter out tokens with high uncertainty. Regarding the scarcity of endoscopic datasets with ground-truth depth and camera poses, we further devise a self-supervised mechanism with a novel dynamics-aware flow loss. Abundant experiments on SCARED and Hamlyn datasets demonstrate our superior performance in zero-shot surgical video depth prediction and camera pose estimation with online efficiency. Project page: this https URL.", 'abstract_zh': '从单目手术视频重建一致标度的3D场景可以增强外科医生的感知，并在各种计算机辅助手术任务中发挥重要作用。然而，由于内窥镜视频固有的动态变形和无纹理表面等问题，实现一致标度的重建仍然是一个开放的挑战。尽管取得了近期的进步，当前的方法要么依赖标定或器械先验来估计标度，要么采用类似SfM的多阶段流水线，导致错误累积，并需要离线优化。在本文中，我们提出Endo3R，一种用于在线一致标度重建的统一3D基础模型，无需任何先验或额外优化。我们的模型通过预测全局对齐的点图、一致标度的视频深度和相机参数来统一任务，而不进行任何离线优化。我们的方法的核心贡献是通过一种不确定性意识的双记忆机制，将最近的两两重建模型扩展到长期增量动态重建。该机制维护了短期动态和长期空间一致性的历史令牌。值得注意的是，为了解决手术场景的高度动态性质，我们通过Sampson距离来测量令牌的不确定性，并过滤掉具有高不确定性的令牌。鉴于内窥镜数据集稀缺且缺乏地面真实深度和相机姿态，我们进一步提出了一种自我监督机制，并设计了一种新的动态意识流损失。在SCARED和Hamlyn数据集上的大量实验表明，我们的方法在零样本手术视频深度预测和相机姿态估计方面具有在线效率和优越的表现。项目页面：this https URL。', 'title_zh': 'Endo3R：统一的动态单目内窥视频在线重建'}
{'arxiv_id': 'arXiv:2504.03164', 'title': 'NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving', 'authors': 'Kexin Tian, Jingrui Mao, Yunlong Zhang, Jiwan Jiang, Yang Zhou, Zhengzhong Tu', 'link': 'https://arxiv.org/abs/2504.03164', 'abstract': "Recent advancements in Vision-Language Models (VLMs) have demonstrated strong potential for autonomous driving tasks. However, their spatial understanding and reasoning-key capabilities for autonomous driving-still exhibit significant limitations. Notably, none of the existing benchmarks systematically evaluate VLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we propose NuScenes-SpatialQA, the first large-scale ground-truth-based Question-Answer (QA) benchmark specifically designed to evaluate the spatial understanding and reasoning capabilities of VLMs in autonomous driving. Built upon the NuScenes dataset, the benchmark is constructed through an automated 3D scene graph generation pipeline and a QA generation pipeline. The benchmark systematically evaluates VLMs' performance in both spatial understanding and reasoning across multiple dimensions. Using this benchmark, we conduct extensive experiments on diverse VLMs, including both general and spatial-enhanced models, providing the first comprehensive evaluation of their spatial capabilities in autonomous driving. Surprisingly, the experimental results show that the spatial-enhanced VLM outperforms in qualitative QA but does not demonstrate competitiveness in quantitative QA. In general, VLMs still face considerable challenges in spatial understanding and reasoning.", 'abstract_zh': "Recent advancements in Vision-Language Models (VLMs) have demonstrated strong potential for autonomous driving tasks. However, their spatial understanding and reasoning-key capabilities for autonomous driving-still exhibit significant limitations. Notably, none of the existing benchmarks systematically evaluate VLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we propose NuScenes-SpatialQA, the first large-scale ground-truth-based Question-Answer (QA) benchmark specifically designed to evaluate the spatial understanding and reasoning capabilities of VLMs in autonomous driving.", 'title_zh': 'NuScenes-空间QA：自动驾驶中视觉语言模型的空间理解与推理基准'}
{'arxiv_id': 'arXiv:2504.03135', 'title': 'Hierarchical Modeling for Medical Visual Question Answering with Cross-Attention Fusion', 'authors': 'Junkai Zhang, Bin Li, Shoujun Zhou, Yue Du', 'link': 'https://arxiv.org/abs/2504.03135', 'abstract': 'Medical Visual Question Answering (Med-VQA) answers clinical questions using medical images, aiding diagnosis. Designing the MedVQA system holds profound importance in assisting clinical diagnosis and enhancing diagnostic accuracy. Building upon this foundation, Hierarchical Medical VQA extends Medical VQA by organizing medical questions into a hierarchical structure and making level-specific predictions to handle fine-grained distinctions. Recently, many studies have proposed hierarchical MedVQA tasks and established datasets, However, several issues still remain: (1) imperfect hierarchical modeling leads to poor differentiation between question levels causing semantic fragmentation across hierarchies. (2) Excessive reliance on implicit learning in Transformer-based cross-modal self-attention fusion methods, which obscures crucial local semantic correlations in medical scenarios. To address these issues, this study proposes a HiCA-VQA method, including two modules: Hierarchical Prompting for fine-grained medical questions and Hierarchical Answer Decoders. The hierarchical prompting module pre-aligns hierarchical text prompts with image features to guide the model in focusing on specific image regions according to question types, while the hierarchical decoder performs separate predictions for questions at different levels to improve accuracy across granularities. The framework also incorporates a cross-attention fusion module where images serve as queries and text as key-value pairs. Experiments on the Rad-Restruct benchmark demonstrate that the HiCA-VQA framework better outperforms existing state-of-the-art methods in answering hierarchical fine-grained questions. This study provides an effective pathway for hierarchical visual question answering systems, advancing medical image understanding.', 'abstract_zh': '基于层次结构的医疗视觉问答（HiCA-VQA）：提高细粒度医疗图像理解', 'title_zh': '医疗跨注意力融合的分层建模视觉问答'}
{'arxiv_id': 'arXiv:2504.03118', 'title': 'NuWa: Deriving Lightweight Task-Specific Vision Transformers for Edge Devices', 'authors': 'Ziteng Wei, Qiang He, Bing Li, Feifei Chen, Yun Yang', 'link': 'https://arxiv.org/abs/2504.03118', 'abstract': "Vision Transformers (ViTs) excel in computer vision tasks but lack flexibility for edge devices' diverse needs. A vital issue is that ViTs pre-trained to cover a broad range of tasks are \\textit{over-qualified} for edge devices that usually demand only part of a ViT's knowledge for specific tasks. Their task-specific accuracy on these edge devices is suboptimal. We discovered that small ViTs that focus on device-specific tasks can improve model accuracy and in the meantime, accelerate model inference. This paper presents NuWa, an approach that derives small ViTs from the base ViT for edge devices with specific task requirements. NuWa can transfer task-specific knowledge extracted from the base ViT into small ViTs that fully leverage constrained resources on edge devices to maximize model accuracy with inference latency assurance. Experiments with three base ViTs on three public datasets demonstrate that compared with state-of-the-art solutions, NuWa improves model accuracy by up to $\\text{11.83}\\%$ and accelerates model inference by 1.29$\\times$ - 2.79$\\times$. Code for reproduction is available at this https URL.", 'abstract_zh': 'Vision Transformers (ViTs)在计算机视觉任务中表现出色，但在应对边缘设备多样化需求时缺乏灵活性。一个关键问题是，预先训练以覆盖广泛任务的ViTs对通常只需特定任务部分知识的边缘设备来说过于胜任，这导致它们在这些边缘设备上的任务特定准确性欠佳。我们发现，专注于特定任务的较小ViTs可以提高模型精度，并同时加速模型推理。本文提出NuWa方法，旨在为具有特定任务需求的边缘设备生成小型ViTs。NuWa能够将从基础ViT提取的任务特定知识转移到小型ViTs中，充分利用边缘设备受限的资源，确保在保证推理延迟的前提下最大化模型精度。在三个基础ViT和三个公开数据集上的实验结果显示，与当前最佳解决方案相比，NuWa可提高模型精度高达11.83%，并将模型推理速度提升1.29至2.79倍。相关代码可在以下链接中复制再现：this https URL。', 'title_zh': 'NuWa: 为边缘设备提取轻量级任务特定视觉变压器'}
{'arxiv_id': 'arXiv:2504.03108', 'title': 'Multi-Granularity Vision Fastformer with Fusion Mechanism for Skin Lesion Segmentation', 'authors': 'Xuanyu Liu, Huiyun Yao, Jinggui Gao, Zhongyi Guo, Xue Zhang, Yulin Dong', 'link': 'https://arxiv.org/abs/2504.03108', 'abstract': "Background:Convolutional Neural Networks(CNN) and Vision Transformers(ViT) are the main techniques used in Medical image segmentation. However, CNN is limited to local contextual information, and ViT's quadratic complexity results in significant computational costs. At the same time, equipping the model to distinguish lesion boundaries with varying degrees of severity is also a challenge encountered in skin lesion segmentation. Purpose:This research aims to optimize the balance between computational costs and long-range dependency modelling and achieve excellent generalization across lesions with different degrees of severity. Methods:we propose a lightweight U-shape network that utilizes Vision Fastformer with Fusion Mechanism (VFFM-UNet). We inherit the advantages of Fastformer's additive attention mechanism, combining element-wise product and matrix product for comprehensive feature extraction and channel reduction to save computational costs. In order to accurately identify the lesion boundaries with varying degrees of severity, we designed Fusion Mechanism including Multi-Granularity Fusion and Channel Fusion, which can process the feature maps in the granularity and channel levels to obtain different contextual information. Results:Comprehensive experiments on the ISIC2017, ISIC2018 and PH2 datasets demonstrate that VFFM-UNet outperforms existing state-of-the-art models regarding parameter numbers, computational complexity and segmentation performance. In short, compared to MISSFormer, our model achieves superior segmentation performance while reducing parameter and computation costs by 101x and 15x, respectively. Conclusions:Both quantitative and qualitative analyses show that VFFM-UNet sets a new benchmark by reaching an ideal balance between parameter numbers, computational complexity, and segmentation performance compared to existing state-of-the-art models.", 'abstract_zh': '背景：卷积神经网络(CNN)和视觉变换器(ViT)是医学图像分割的主要技术。然而，CNN局限于局部上下文信息，而ViT的二次复杂度导致了显著的计算成本。同时，识别不同严重程度的皮肤病变边界也是一个挑战。目的：本研究旨在优化计算成本与长距离依赖建模之间的平衡，并实现对不同严重程度病变的优秀泛化能力。方法：我们提出了一种轻量级U型网络，利用结合了融合机制的视觉快速变换器(VFFM-UNet)。我们继承了Fastformer的加性注意机制的优点，结合元素级乘积和矩阵乘积进行全面的特征提取，并通过通道减少来节省计算成本。为了准确识别不同严重程度的病变边界，我们设计了包括多粒度融合和通道融合的融合机制，可以在粒度和通道级别处理特征图以获得不同的上下文信息。结果：在ISIC2017、ISIC2018和PH2数据集上的综合实验表明，VFFM-UNet在参数数量、计算复杂度和分割性能方面优于现有最先进的模型。简而言之，与MISSFormer相比，我们的模型在参数和计算成本分别减少了101倍和15倍的同时，实现了更优秀的分割性能。结论：定量和定性分析表明，VFFM-UNet在参数数量、计算复杂度和分割性能方面达到了理想的平衡，设立了新的基准，优于现有的最先进的模型。', 'title_zh': '多粒度视觉Fastformer融合机制皮肤病变分割'}
{'arxiv_id': 'arXiv:2504.03052', 'title': 'Cooperative Inference for Real-Time 3D Human Pose Estimation in Multi-Device Edge Networks', 'authors': 'Hyun-Ho Choi, Kangsoo Kim, Ki-Ho Lee, Kisong Lee', 'link': 'https://arxiv.org/abs/2504.03052', 'abstract': 'Accurate and real-time three-dimensional (3D) pose estimation is challenging in resource-constrained and dynamic environments owing to its high computational complexity. To address this issue, this study proposes a novel cooperative inference method for real-time 3D human pose estimation in mobile edge computing (MEC) networks. In the proposed method, multiple end devices equipped with lightweight inference models employ dual confidence thresholds to filter ambiguous images. Only the filtered images are offloaded to an edge server with a more powerful inference model for re-evaluation, thereby improving the estimation accuracy under computational and communication constraints. We numerically analyze the performance of the proposed inference method in terms of the inference accuracy and end-to-end delay and formulate a joint optimization problem to derive the optimal confidence thresholds and transmission time for each device, with the objective of minimizing the mean per-joint position error (MPJPE) while satisfying the required end-to-end delay constraint. To solve this problem, we demonstrate that minimizing the MPJPE is equivalent to maximizing the sum of the inference accuracies for all devices, decompose the problem into manageable subproblems, and present a low-complexity optimization algorithm to obtain a near-optimal solution. The experimental results show that a trade-off exists between the MPJPE and end-to-end delay depending on the confidence thresholds. Furthermore, the results confirm that the proposed cooperative inference method achieves a significant reduction in the MPJPE through the optimal selection of confidence thresholds and transmission times, while consistently satisfying the end-to-end delay requirement in various MEC environments.', 'abstract_zh': '资源受限和动态环境下基于移动边缘计算的实时三维人体姿态估计联合推断方法', 'title_zh': '多设备边缘网络中的实时3D人体姿态协作推理'}
{'arxiv_id': 'arXiv:2504.02949', 'title': 'VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning', 'authors': 'Xianwei Zhuang, Yuxin Xie, Yufan Deng, Dongchao Yang, Liming Liang, Jinghan Ru, Yuguo Yin, Yuexian Zou', 'link': 'https://arxiv.org/abs/2504.02949', 'abstract': 'In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at this https URL.', 'abstract_zh': '在本文中，我们呈现了VARGPT-v1.1，这是一种先进的统一视觉自回归模型，基于我们之前的框架VARGPT。该模型保留了视觉理解中的下一个令牌预测和图像合成中的下一个尺度生成的双重范式。具体而言，VARGPT-v1.1 集成了：(1) 一种新颖的训练策略，结合迭代的视觉指令调优与直接偏好优化(DPO)强化学习，(2) 包含8.3M视觉生成指令对的扩展训练语料库，(3) 升级的基于Qwen2的语言模型主干，(4) 增强的图像生成分辨率，以及(5) 无需架构修改的 Emergent 图像编辑能力。这些进步使VARGPT-v1.1 在多模态理解和文本到图像指令跟随任务中达到了最先进的性能，显示出在理解和生成方面的重要改进。通过视觉指令调优，模型获得了图像编辑功能，同时保持了与前任的一致架构，揭示了统一视觉理解、生成和编辑的潜力。我们的研究结果表明，设计良好的统一视觉自回归模型可以有效采用来自大型语言模型(LLMs)的灵活训练策略，显示出强大的可扩展性。相关代码库和模型权重可在以下链接公开获取。', 'title_zh': 'VARGPT-v1.1: 通过迭代指令调优和强化学习改进视觉自回归统一模型'}
{'arxiv_id': 'arXiv:2504.02880', 'title': 'Global Rice Multi-Class Segmentation Dataset (RiceSEG): A Comprehensive and Diverse High-Resolution RGB-Annotated Images for the Development and Benchmarking of Rice Segmentation Algorithms', 'authors': 'Junchi Zhou, Haozhou Wang, Yoichiro Kato, Tejasri Nampally, P. Rajalakshmi, M. Balram, Keisuke Katsura, Hao Lu, Yue Mu, Wanneng Yang, Yangmingrui Gao, Feng Xiao, Hongtao Chen, Yuhao Chen, Wenjuan Li, Jingwen Wang, Fenghua Yu, Jian Zhou, Wensheng Wang, Xiaochun Hu, Yuanzhu Yang, Yanfeng Ding, Wei Guo, Shouyang Liu', 'link': 'https://arxiv.org/abs/2504.02880', 'abstract': 'Developing computer vision-based rice phenotyping techniques is crucial for precision field management and accelerating breeding, thereby continuously advancing rice production. Among phenotyping tasks, distinguishing image components is a key prerequisite for characterizing plant growth and development at the organ scale, enabling deeper insights into eco-physiological processes. However, due to the fine structure of rice organs and complex illumination within the canopy, this task remains highly challenging, underscoring the need for a high-quality training dataset. Such datasets are scarce, both due to a lack of large, representative collections of rice field images and the time-intensive nature of annotation. To address this gap, we established the first comprehensive multi-class rice semantic segmentation dataset, RiceSEG. We gathered nearly 50,000 high-resolution, ground-based images from five major rice-growing countries (China, Japan, India, the Philippines, and Tanzania), encompassing over 6,000 genotypes across all growth stages. From these original images, 3,078 representative samples were selected and annotated with six classes (background, green vegetation, senescent vegetation, panicle, weeds, and duckweed) to form the RiceSEG dataset. Notably, the sub-dataset from China spans all major genotypes and rice-growing environments from the northeast to the south. Both state-of-the-art convolutional neural networks and transformer-based semantic segmentation models were used as baselines. While these models perform reasonably well in segmenting background and green vegetation, they face difficulties during the reproductive stage, when canopy structures are more complex and multiple classes are involved. These findings highlight the importance of our dataset for developing specialized segmentation models for rice and other crops.', 'abstract_zh': '基于计算机视觉的水稻表型技术开发对于精确田间管理及加速育种具有重要意义，从而不断推进水稻生产。水稻表型任务中的图像组件区分是表征器官尺度植物生长和发育的关键前提，有助于深入理解生态生理过程。然而，由于水稻器官的精细结构和冠层内的复杂光照，这一任务依然极具挑战性，迫切需要高质量的训练数据集。这类数据集稀缺，主要是因为缺乏大型、代表性强的水稻田图像集合以及注释工作量大。为填补这一空白，我们建立了首个全面的多类水稻语义分割数据集RiceSEG。我们从五个主要水稻种植国家（中国、日本、印度、菲律宾和坦桑尼亚）收集了近50,000张高分辨率地面图像，涵盖了超过6,000个基因型的所有生长阶段。从这些原始图像中，选择了3,078个代表性样本，并根据六类（背景、绿色植被、衰老植被、穗轴、杂草和水葫芦）进行了标注，形成了RiceSEG数据集。特别地，中国子数据集涵盖了东北至南部的所有主要基因型和水稻种植环境。使用了最新的卷积神经网络和基于变压器的语义分割模型作为基准。尽管这些模型在分割背景和绿色植被方面表现良好，但在生殖期，由于冠层结构更加复杂且涉及多个类别，它们面临困难。这些发现强调了我们的数据集对于开发专门化的水稻及其他作物分割模型的重要性。', 'title_zh': '全球多类水稻分割数据集（RiceSEG）：高分辨率RGB标注图像的综合多样数据集，用于水稻分割算法的开发与 benchmarking'}
{'arxiv_id': 'arXiv:2504.02860', 'title': 'Computer Vision and Deep Learning for 4D Augmented Reality', 'authors': 'Karthik Shivashankar', 'link': 'https://arxiv.org/abs/2504.02860', 'abstract': 'The prospect of 4D video in Extended Reality (XR) platform is huge and exciting, it opens a whole new way of human computer interaction and the way we perceive the reality and consume multimedia. In this thesis, we have shown that feasibility of rendering 4D video in Microsoft mixed reality platform. This enables us to port any 3D performance capture from CVSSP into XR product like the HoloLens device with relative ease. However, if the 3D model is too complex and is made up of millions of vertices, the data bandwidth required to port the model is a severe limitation with the current hardware and communication system. Therefore, in this project we have also developed a compact representation of both shape and appearance of the 4d video sequence using deep learning models to effectively learn the compact representation of 4D video sequence and reconstruct it without affecting the shape and appearance of the video sequence.', 'abstract_zh': '4D视频在扩展现实(XR)平台上的前景巨大而令人兴奋：基于Microsoft混合现实平台的实现与挑战', 'title_zh': '计算机视觉与深度学习在4D增强现实中的应用'}
