{'arxiv_id': 'arXiv:2504.08704', 'title': 'Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing', 'authors': 'Vinal Asodia, Zhenhua Feng, Saber Fallah', 'link': 'https://arxiv.org/abs/2504.08704', 'abstract': 'Effective leveraging of real-world driving datasets is crucial for enhancing the training of autonomous driving systems. While Offline Reinforcement Learning enables the training of autonomous vehicles using such data, most available datasets lack meaningful reward labels. Reward labeling is essential as it provides feedback for the learning algorithm to distinguish between desirable and undesirable behaviors, thereby improving policy performance. This paper presents a novel pipeline for generating human-aligned reward labels. The proposed approach addresses the challenge of absent reward signals in real-world datasets by generating labels that reflect human judgment and safety considerations. The pipeline incorporates an adaptive safety component, activated by analyzing semantic segmentation maps, allowing the autonomous vehicle to prioritize safety over efficiency in potential collision scenarios. The proposed pipeline is applied to an occluded pedestrian crossing scenario with varying levels of pedestrian traffic, using synthetic and simulation data. The results indicate that the generated reward labels closely match the simulation reward labels. When used to train the driving policy using Behavior Proximal Policy Optimisation, the results are competitive with other baselines. This demonstrates the effectiveness of our method in producing reliable and human-aligned reward signals, facilitating the training of autonomous driving systems through Reinforcement Learning outside of simulation environments and in alignment with human values.', 'abstract_zh': '有效的利用实际驾驶数据对于增强自动驾驶系统训练至关重要。虽然 Offline Reinforcement Learning 可以利用此类数据训练自动驾驶车辆，但大多数可用的数据集缺乏有意义的奖励标签。奖励标签是必要的，因为它为学习算法提供了反馈，使其能够区分 desirable 和 undesirable 行为，从而提高策略性能。本文提出了一种新的生成人类对齐奖励标签的管道。所提出的方法通过生成反映人类判断和安全考虑的标签，解决了现实世界数据集中缺乏奖励信号的挑战。该管道包含一个基于分析语义分割图的自适应安全组件，允许自动驾驶车辆在潜在碰撞场景中优先考虑安全而非效率。所提出的管道应用于具有不同行人流量的遮挡行人过街场景，使用合成和仿真数据。结果表明，生成的奖励标签与仿真奖励标签高度一致。当用于使用 Behavior Proximal Policy Optimisation 训练驾驶策略时，结果与其他基线相当。这证明了该方法在生成可靠且与人类价值观相一致的奖励信号方面的有效性，促进了在仿真环境之外和与人类价值观一致的条件下通过 Reinforcement Learning 对自动驾驶系统进行训练。', 'title_zh': '基于人类对齐奖励标注的自主紧急制动在行人遮挡过街场景下的离线强化学习'}
{'arxiv_id': 'arXiv:2504.08615', 'title': 'Tactile sensing enables vertical obstacle negotiation for elongate many-legged robots', 'authors': 'Juntao He, Baxi Chong, Vincent R Nienhusser, Massimiliano Iaschi, Sehoon Ha, Daniel I. Goldman', 'link': 'https://arxiv.org/abs/2504.08615', 'abstract': "Many-legged elongated robots show promise for reliable mobility on rugged landscapes. However, most studies on these systems focus on motion planning in the 2D horizontal plane (e.g., translation and rotation) without addressing rapid vertical motion. Despite their success on mild rugged terrains, recent field tests reveal a critical need for 3D behaviors (e.g., climbing or traversing tall obstacles) in real-world application. The challenges of 3D motion planning partially lie in designing sensing and control for a complex high-degree-of-freedom system, typically with over 25 degrees of freedom. To address the first challenge, we propose a tactile antenna system that enables the robot to probe obstacles and gather information about the structure of the environment. Building on this sensory input, we develop a control framework that integrates data from the antenna and foot contact sensors to dynamically adjust the robot's vertical body undulation for effective climbing. With the addition of simple, low-bandwidth tactile sensors, a robot with high static stability and redundancy exhibits predictable climbing performance in complex environments using a simple feedback controller. Laboratory and outdoor experiments demonstrate the robot's ability to climb obstacles up to five times its height. Moreover, the robot exhibits robust climbing capabilities on obstacles covered with flowable, robot-sized random items and those characterized by rapidly changing curvatures. These findings demonstrate an alternative solution to perceive the environment and facilitate effective response for legged robots, paving ways towards future highly capable, low-profile many-legged robots.", 'abstract_zh': '多-legged伸长型机器人在崎岖地形上的可靠移动显示出巨大潜力。然而，大多数关于这些系统的研究集中在2D水平面内的运动规划（如平移和旋转）上，没有解决快速垂直运动的问题。尽管在轻度崎岖地形上表现出色，最近的实地测试揭示了在实际应用中对3D行为（如攀爬或穿越高障碍物）的迫切需求。3D运动规划的挑战部分在于设计复杂高自由度系统的传感和控制，通常自由度超过25个。为应对第一个挑战，我们提出了一种触觉天线系统，使机器人能够探测障碍物并收集环境结构信息。在此基础上，我们开发了一种控制框架，将天线和足接触传感器的数据结合起来，动态调整机器人垂直身体起伏以实现有效的攀爬。通过添加简单的低带宽触觉传感器，在具有高静态稳定性和冗余性的机器人上，使用简单的反馈控制器即可在复杂环境中实现可预测的攀爬性能。实验室和户外实验表明，该机器人能够攀爬高达其高度五倍的障碍物。此外，该机器人还在覆盖有流动且机器人大小的随机物品的障碍物及具有快速变化曲率的障碍物上表现出鲁棒的攀爬能力。这些发现提供了一种感知环境和促进腿部机器人有效响应的替代方案，为未来低轮廓、多-legged高效能机器人的发展铺平了道路。', 'title_zh': '触觉感知使长多腿机器人能够应对垂直障碍物'}
{'arxiv_id': 'arXiv:2504.08604', 'title': 'Neural Fidelity Calibration for Informative Sim-to-Real Adaptation', 'authors': 'Youwei Yu, Lantao Liu', 'link': 'https://arxiv.org/abs/2504.08604', 'abstract': "Deep reinforcement learning can seamlessly transfer agile locomotion and navigation skills from the simulator to real world. However, bridging the sim-to-real gap with domain randomization or adversarial methods often demands expert physics knowledge to ensure policy robustness. Even so, cutting-edge simulators may fall short of capturing every real-world detail, and the reconstructed environment may introduce errors due to various perception uncertainties. To address these challenges, we propose Neural Fidelity Calibration (NFC), a novel framework that employs conditional score-based diffusion models to calibrate simulator physical coefficients and residual fidelity domains online during robot execution. Specifically, the residual fidelity reflects the simulation model shift relative to the real-world dynamics and captures the uncertainty of the perceived environment, enabling us to sample realistic environments under the inferred distribution for policy fine-tuning. Our framework is informative and adaptive in three key ways: (a) we fine-tune the pretrained policy only under anomalous scenarios, (b) we build sequential NFC online with the pretrained NFC's proposal prior, reducing the diffusion model's training burden, and (c) when NFC uncertainty is high and may degrade policy improvement, we leverage optimistic exploration to enable hallucinated policy optimization. Our framework achieves superior simulator calibration precision compared to state-of-the-art methods across diverse robots with high-dimensional parametric spaces. We study the critical contribution of residual fidelity to policy improvement in simulation and real-world experiments. Notably, our approach demonstrates robust robot navigation under challenging real-world conditions, such as a broken wheel axle on snowy surfaces.", 'abstract_zh': '深度强化学习可以从仿真器无缝转移敏捷运动和导航技能到现实世界。然而，通过领域随机化或对抗方法 bridging sim-to-real 隔离往往需要专家物理知识以确保策略稳健性。即使如此，最新的仿真器可能无法捕获所有现实世界的细节，重建的环境也可能由于各种感知不确定性引入错误。为应对这些挑战，我们提出了一种新型框架神经保真度校准（NFC），该框架利用条件评分扩散模型在线校准仿真物理系数和剩余保真度域，同时机器人执行。具体而言，剩余保真度反映了仿真模型相对于真实世界动力学的变化，并捕获感知环境的不确定性，使我们能够根据推断出的分布采样现实环境以进行策略微调。我们的框架在三个方面具有启发性和适应性：(a) 我们仅在异常场景中微调预训练策略，(b) 我们构建序列化的 NFC，在线构建在预训练 NFC 的提案先验之上，减轻扩散模型的训练负担，(c) 当 NFC 不确定性高且可能降低策略改进时，我们利用乐观探索以允许臆想策略优化。与现有最佳方法相比，我们的框架在具有高维参数空间的各种机器人中实现了优越的仿真校准精度。我们在仿真和真实世界实验中研究了剩余保真度对策略改进的关键贡献。值得注意的是，我们的方法在具有挑战性的现实世界条件下展示了稳健的机器人导航，例如雪地表面轮轴损坏。', 'title_zh': '神经保真度校准以实现信息性的仿真实践转换'}
{'arxiv_id': 'arXiv:2504.08603', 'title': 'FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment', 'authors': 'Sebastián Barbas Laina, Simon Boche, Sotiris Papatheodorou, Simon Schaefer, Jaehyung Jung, Stefan Leutenegger', 'link': 'https://arxiv.org/abs/2504.08603', 'abstract': 'Geometrically accurate and semantically expressive map representations have proven invaluable to facilitate robust and safe mobile robot navigation and task planning. Nevertheless, real-time, open-vocabulary semantic understanding of large-scale unknown environments is still an open problem. In this paper we present FindAnything, an open-world mapping and exploration framework that incorporates vision-language information into dense volumetric submaps. Thanks to the use of vision-language features, FindAnything bridges the gap between pure geometric and open-vocabulary semantic information for a higher level of understanding while allowing to explore any environment without the help of any external source of ground-truth pose information. We represent the environment as a series of volumetric occupancy submaps, resulting in a robust and accurate map representation that deforms upon pose updates when the underlying SLAM system corrects its drift, allowing for a locally consistent representation between submaps. Pixel-wise vision-language features are aggregated from efficient SAM (eSAM)-generated segments, which are in turn integrated into object-centric volumetric submaps, providing a mapping from open-vocabulary queries to 3D geometry that is scalable also in terms of memory usage. The open-vocabulary map representation of FindAnything achieves state-of-the-art semantic accuracy in closed-set evaluations on the Replica dataset. This level of scene understanding allows a robot to explore environments based on objects or areas of interest selected via natural language queries. Our system is the first of its kind to be deployed on resource-constrained devices, such as MAVs, leveraging vision-language information for real-world robotic tasks.', 'abstract_zh': '几何准确且语义丰富的地图表示对于促进移动机器人 robust 和安全的导航及任务规划具有重要作用。然而，实时、开放式词汇语义理解大规模未知环境仍然是一个待解决的问题。本文提出 FindAnything，一种结合视觉-语言信息的开放世界建图与探索框架，通过使用视觉-语言特征，在纯几何和开放式词汇语义信息之间架起桥梁，实现更高层次的理解，同时允许在无需任何外部姿态信息源的情况下探索任何环境。我们将环境表示为一系列体积占用子地图，产生一种在姿态更新时能够变形的稳健且精确的地图表示，当底层 SLAM 系统纠正漂移时，能够提供局部一致的子地图表示。像素级的视觉-语言特征从高效的 eSAM 生成的分割中聚合而来，并整合进以物体为中心的体积子地图，提供一种从开放式词汇查询到 3D 几何的映射，同时在内存使用方面具备可扩展性。FindAnything 的开放式词汇地图表示在 Replica 数据集的封闭集评估中达到最先进的语义准确性。这种场景理解水平使机器人能够根据通过自然语言查询选定的对象或关注区域来探索环境。我们的系统是首个在资源受限设备（如 MAVs）上部署的系统，利用视觉-语言信息为现实中的机器人任务提供支持。', 'title_zh': 'FindAnything：任意环境机器人探索的开放词汇和物体中心映射'}
{'arxiv_id': 'arXiv:2504.08431', 'title': 'The Composite Visual-Laser Navigation Method Applied in Indoor Poultry Farming Environments', 'authors': 'Jiafan Lu, Dongcheng Hu, Yitian Ye, Anqi Liu, Zixian Zhang, Xin Peng', 'link': 'https://arxiv.org/abs/2504.08431', 'abstract': 'Indoor poultry farms require inspection robots to maintain precise environmental control, which is crucial for preventing the rapid spread of disease and large-scale bird mortality. However, the complex conditions within these facilities, characterized by areas of intense illumination and water accumulation, pose significant challenges. Traditional navigation methods that rely on a single sensor often perform poorly in such environments, resulting in issues like laser drift and inaccuracies in visual navigation line extraction. To overcome these limitations, we propose a novel composite navigation method that integrates both laser and vision technologies. This approach dynamically computes a fused yaw angle based on the real-time reliability of each sensor modality, thereby eliminating the need for physical navigation lines. Experimental validation in actual poultry house environments demonstrates that our method not only resolves the inherent drawbacks of single-sensor systems, but also significantly enhances navigation precision and operational efficiency. As such, it presents a promising solution for improving the performance of inspection robots in complex indoor poultry farming settings.', 'abstract_zh': '室内养禽农场需要使用检测机器人来维持精确的环境控制，这对于预防疾病快速蔓延和大规模禽类死亡至关重要。然而，这些设施内的复杂条件，如强光区域和积水区，提出了重大挑战。传统的依赖单一传感器的导航方法在这种环境中往往表现不佳，导致激光漂移和视觉导航线提取不准确的问题。为此，我们提出了一种新颖的复合导航方法，整合了激光和视觉技术。该方法动态计算基于每个传感器模态实时可靠性的融合偏航角，从而避免了物理导航线的需求。实验验证表明，我们的方法不仅解决了单一传感器系统固有的问题，还显著提高了导航精度和操作效率。因此，它为改善复杂室内养禽农场检测机器人的性能提供了有前景的解决方案。', 'title_zh': '应用于室内养禽环境的复合视觉-激光导航方法'}
{'arxiv_id': 'arXiv:2504.08395', 'title': "Human strategies for correcting `human-robot' errors during a laundry sorting task", 'authors': 'Pepita Barnard, Maria J Galvez Trigo, Dominic Price, Sue Cobb, Gisela Reyes-Cruz, Gustavo Berumen, David Branson III, Mojtaba A. Khanesar, Mercedes Torres Torres, Michel Valstar', 'link': 'https://arxiv.org/abs/2504.08395', 'abstract': "Mental models and expectations underlying human-human interaction (HHI) inform human-robot interaction (HRI) with domestic robots. To ease collaborative home tasks by improving domestic robot speech and behaviours for human-robot communication, we designed a study to understand how people communicated when failure occurs. To identify patterns of natural communication, particularly in response to robotic failures, participants instructed Laundrobot to move laundry into baskets using natural language and gestures. Laundrobot either worked error-free, or in one of two error modes. Participants were not advised Laundrobot would be a human actor, nor given information about error modes. Video analysis from 42 participants found speech patterns, included laughter, verbal expressions, and filler words, such as ``oh'' and ``ok'', also, sequences of body movements, including touching one's own face, increased pointing with a static finger, and expressions of surprise. Common strategies deployed when errors occurred, included correcting and teaching, taking responsibility, and displays of frustration. The strength of reaction to errors diminished with exposure, possibly indicating acceptance or resignation. Some used strategies similar to those used to communicate with other technologies, such as smart assistants. An anthropomorphic robot may not be ideally suited to this kind of task. Laundrobot's appearance, morphology, voice, capabilities, and recovery strategies may have impacted how it was perceived. Some participants indicated Laundrobot's actual skills were not aligned with expectations; this made it difficult to know what to expect and how much Laundrobot understood. Expertise, personality, and cultural differences may affect responses, however these were not assessed.", 'abstract_zh': '人类心智模型与期望下的人际交互对人机交互的研究：以家用机器人为例——以洗碗机器人Laundrobot为对象探究交互中沟通方式及应对错误策略', 'title_zh': '人类在洗衣分类任务中纠正“人-机器人”错误的策略'}
{'arxiv_id': 'arXiv:2504.08338', 'title': 'RINGO: Real-time Navigation with a Guiding Trajectory for Aerial Manipulators in Unknown Environments', 'authors': 'Zhang Zhaopeng, Wu Shizhen, Guo Chenfeng, Fang Yongchun, Han Jianda, Liang Xiao', 'link': 'https://arxiv.org/abs/2504.08338', 'abstract': 'Motion planning for aerial manipulators in constrained environments has typically been limited to known environments or simplified to that of multi-rotors, which leads to poor adaptability and overly conservative trajectories. This paper presents RINGO:~Real-time Navigation with a Guiding Trajectory, a novel planning framework that enables aerial manipulators to navigate unknown environments in real time. The proposed method simultaneously considers the positions of both the multi-rotor and the end-effector. A pre-obtained multi-rotor trajectory serves as a guiding reference, allowing the end-effector to generate a smooth, collision-free, and workspace-compatible trajectory. Leveraging the convex hull property of B-spline curves, we theoretically guarantee that the trajectory remains within the reachable workspace. To the best of our knowledge, this is the first work that enables real-time navigation of aerial manipulators in unknown environments. The simulation and experimental results show the effectiveness of the proposed method. The proposed method generates less conservative trajectories than approaches that consider only the multi-rotor.', 'abstract_zh': '基于约束环境的空中 manipulator 运动规划通常局限于已知环境或简化为多旋翼环境，导致适应性差和过于保守的轨迹。本文提出了 RINGO：实时导航与引导轨迹，一种新型规划框架，使空中 manipulator 能够实时导航未知环境。所提出的方法同时考虑多旋翼和末端执行器的位置。预先获得的多旋翼轨迹作为引导参考，使末端执行器生成平滑、无碰撞且工作空间兼容的轨迹。利用 B-样条曲线的凸包性质，我们理论上保证轨迹保持在可达工作空间内。据我们所知，这是第一项使空中 manipulator 能在未知环境中实现实时导航的工作。仿真和实验结果表明所提出方法的有效性。与仅考虑多旋翼的方法相比，所提出的方法生成的轨迹更为保守。', 'title_zh': 'RINGO: 未知环境中超实时导航的引导轨迹方法'}
{'arxiv_id': 'arXiv:2504.08246', 'title': 'Spectral Normalization for Lipschitz-Constrained Policies on Learning Humanoid Locomotion', 'authors': 'Jaeyong Shin, Woohyun Cha, Donghyeon Kim, Junhyeok Cha, Jaeheung Park', 'link': 'https://arxiv.org/abs/2504.08246', 'abstract': 'Reinforcement learning (RL) has shown great potential in training agile and adaptable controllers for legged robots, enabling them to learn complex locomotion behaviors directly from experience. However, policies trained in simulation often fail to transfer to real-world robots due to unrealistic assumptions such as infinite actuator bandwidth and the absence of torque limits. These conditions allow policies to rely on abrupt, high-frequency torque changes, which are infeasible for real actuators with finite bandwidth.\nTraditional methods address this issue by penalizing aggressive motions through regularization rewards, such as joint velocities, accelerations, and energy consumption, but they require extensive hyperparameter tuning. Alternatively, Lipschitz-Constrained Policies (LCP) enforce finite bandwidth action control by penalizing policy gradients, but their reliance on gradient calculations introduces significant GPU memory overhead. To overcome this limitation, this work proposes Spectral Normalization (SN) as an efficient replacement for enforcing Lipschitz continuity. By constraining the spectral norm of network weights, SN effectively limits high-frequency policy fluctuations while significantly reducing GPU memory usage. Experimental evaluations in both simulation and real-world humanoid robot show that SN achieves performance comparable to gradient penalty methods while enabling more efficient parallel training.', 'abstract_zh': '强化学习（RL）在训练灵活适应的腿部机器人控制器方面显示出了巨大潜力，使其能够直接从经验中学习复杂的运动行为。然而，模拟中训练的策略由于无理假设（如无限的执行器带宽和缺乏扭矩限制）往往无法转移到实际机器人上，这些假设允许策略依赖于急剧、高频率的扭矩变化，而实际具有有限带宽的执行器无法实现这一点。传统方法通过正则化奖励（如关节速度、加速度和能耗）限制激进的动作，但需要大量超参数调优。另一种方法是通过惩罚策略梯度来强制执行有限带宽动作控制的Lipschitz-Constrained Policies（LCP），但其依赖梯度计算导致显著增加GPU内存开销。为克服这一限制，本文提议使用谱规范化（Spectral Normalization，SN）作为Lipschitz连续性的有效替代方法。通过约束网络权重的谱范数，SN有效地限制了高频率策略波动，同时显着减少GPU内存使用。实验评估在仿真和实际人形机器人中均表明，SN在实现与梯度惩罚方法相当的性能的同时，能够实现更高效的并行训练。', 'title_zh': '谱归一化在学习类人行走策略的Lipschitz约束下的应用'}
{'arxiv_id': 'arXiv:2504.08117', 'title': 'Design Activity for Robot Faces: Evaluating Child Responses To Expressive Faces', 'authors': 'Denielle Oliva, Joshua Knight, Tyler J Becker, Heather Amistani, Monica Nicolescu, David Feil-Seifer', 'link': 'https://arxiv.org/abs/2504.08117', 'abstract': "Facial expressiveness plays a crucial role in a robot's ability to engage and interact with children. Prior research has shown that expressive robots can enhance child engagement during human-robot interactions. However, many robots used in therapy settings feature non-personalized, static faces designed with traditional facial feature considerations, which can limit the depth of interactions and emotional connections. Digital faces offer opportunities for personalization, yet the current landscape of robot face design lacks a dynamic, user-centered approach. Specifically, there is a significant research gap in designing robot faces based on child preferences. Instead, most robots in child-focused therapy spaces are developed from an adult-centric perspective. We present a novel study investigating the influence of child-drawn digital faces in child-robot interactions. This approach focuses on a design activity with children instructed to draw their own custom robot faces. We compare the perceptions of social intelligence (PSI) of two implementations: a generic digital face and a robot face, personalized using the user's drawn robot faces. The results of this study show the perceived social intelligence of a child-drawn robot was significantly higher compared to a generic face.", 'abstract_zh': '面部表情在机器人与儿童互动能力中扮演着 crucial 角色。先前的研究表明，富有表情的机器人能够增强儿童在人机交互过程中对机器人的参与度。然而，许多用于治疗场景中的机器人拥有非个性化、静态的脸部设计，这限制了互动的深度以及情感连接。数字脸部提供了个性化的机会，但当前机器人脸部设计的格局缺乏以用户为中心的动力学方法。特别是，在基于儿童偏好设计机器人脸部方面存在显著的研究空白。相反，大多数面向儿童的治疗空间中的机器人是从成人视角开发的。我们提出了一项新的研究，调查儿童绘制的数字脸部对儿童与机器人互动的影响。这种方法侧重于一种设计活动，让儿童绘制他们自己的定制机器人脸部。我们比较了两种实现的社交智能感知（PSI）：一个通用的数字脸和一个使用用户绘制的机器人脸进行个性化的机器人脸。研究结果表明，儿童绘制的机器人脸的感知社交智能显著高于通用脸。', 'title_zh': '机器人脸部设计活动：评价具表现力脸部对儿童的响应'}
{'arxiv_id': 'arXiv:2504.08531', 'title': 'Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions', 'authors': 'Tommaso Galliena, Tommaso Apicella, Stefano Rosa, Pietro Morerio, Alessio Del Bue, Lorenzo Natale', 'link': 'https://arxiv.org/abs/2504.08531', 'abstract': "We present a self-supervised method to improve an agent's abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at this https URL", 'abstract_zh': '我们提出一种自监督方法，以提高智能体在主动探索通用环境时描述任意对象的能力。这是一个具有挑战性的问题，因为当前模型很难因应不同视角和杂乱环境获得连贯的图像描述。我们提出一个三阶段框架，通过共识机制 fine-tune 存在的描述模型，以增强不同视角下的描述准确性和一致性。首先，智能体探索环境，收集噪声图像-描述对。然后，利用大型语言模型中的共识机制为每个对象实例提取一致的伪描述。最后，这些伪描述用于 fine-tune 现成的描述模型，并结合对比学习。我们在手动标注的测试集中分析了描述模型、探索策略、伪标签方法及 fine-tuning 策略的性能。结果显示，一种策略可以被训练来挖掘比经典基线更高的不一致样本。结合所有策略的伪描述方法在语义相似度上优于其他现有方法，fine-tuning 显著提高了描述的准确性和一致性。代码和测试集注释可在以下网址获取。', 'title_zh': '基于躯体的图像描述：自监督学习代理及其空间连贯的图像描述'}
{'arxiv_id': 'arXiv:2504.08417', 'title': 'Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial Observability', 'authors': 'Paul J. Pritz, Kin K. Leung', 'link': 'https://arxiv.org/abs/2504.08417', 'abstract': "Reinforcement learning in partially observable environments is typically challenging, as it requires agents to learn an estimate of the underlying system state. These challenges are exacerbated in multi-agent settings, where agents learn simultaneously and influence the underlying state as well as each others' observations. We propose the use of learned beliefs on the underlying state of the system to overcome these challenges and enable reinforcement learning with fully decentralized training and execution. Our approach leverages state information to pre-train a probabilistic belief model in a self-supervised fashion. The resulting belief states, which capture both inferred state information as well as uncertainty over this information, are then used in a state-based reinforcement learning algorithm to create an end-to-end model for cooperative multi-agent reinforcement learning under partial observability. By separating the belief and reinforcement learning tasks, we are able to significantly simplify the policy and value function learning tasks and improve both the convergence speed and the final performance. We evaluate our proposed method on diverse partially observable multi-agent tasks designed to exhibit different variants of partial observability.", 'abstract_zh': '在部分可观测环境中的强化学习通常具有挑战性，因为这要求智能体学习系统状态的估计。在多智能体设置中，这些挑战进一步加剧，因为智能体同时学习并影响系统状态以及彼此的观察。我们提出使用对系统状态的学得信念来克服这些挑战，并实现完全去中心化的训练和执行的强化学习。我们的方法利用状态信息以自监督方式预训练一个概率信念模型。由此产生的信念状态不仅捕捉了推断出的状态信息，还捕捉了对该信息的不确定性，然后在基于状态的强化学习算法中使用这些信念状态，以端到端的方式构建在部分可观测性下的合作多智能体强化学习模型。通过将信念学习和强化学习任务分离，我们能够显著简化策略和价值函数的学习任务，并提高收敛速度和最终性能。我们通过设计旨在展现不同部分可观测性变体的多样化的部分可观测多智能体任务来评估我们所提出的方法。', 'title_zh': '部分可观状态下协作多智能体强化学习的信念状态研究'}
{'arxiv_id': 'arXiv:2504.08000', 'title': 'Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning', 'authors': 'Jiahua Lan, Sen Zhang, Haixia Pan, Ruijun Liu, Li Shen, Dacheng Tao', 'link': 'https://arxiv.org/abs/2504.08000', 'abstract': 'In contrast to the human ability to continuously acquire knowledge, agents struggle with the stability-plasticity dilemma in deep reinforcement learning (DRL), which refers to the trade-off between retaining existing skills (stability) and learning new knowledge (plasticity). Current methods focus on balancing these two aspects at the network level, lacking sufficient differentiation and fine-grained control of individual neurons. To overcome this limitation, we propose Neuron-level Balance between Stability and Plasticity (NBSP) method, by taking inspiration from the observation that specific neurons are strongly relevant to task-relevant skills. Specifically, NBSP first (1) defines and identifies RL skill neurons that are crucial for knowledge retention through a goal-oriented method, and then (2) introduces a framework by employing gradient masking and experience replay techniques targeting these neurons to preserve the encoded existing skills while enabling adaptation to new tasks. Numerous experimental results on the Meta-World and Atari benchmarks demonstrate that NBSP significantly outperforms existing approaches in balancing stability and plasticity.', 'abstract_zh': '基于神经元层面稳定性和可塑性平衡的方法（NBSP）：超越深 reinforcement 学习中的稳定性-可塑性困境', 'title_zh': '深层强化学习中神经元级稳定性与可塑性的平衡'}
{'arxiv_id': 'arXiv:2504.08388', 'title': 'MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft', 'authors': 'Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, Jiang Bian', 'link': 'https://arxiv.org/abs/2504.08388', 'abstract': 'World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate $4$ to $7$ frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released.', 'abstract_zh': '基于Minecraft的实时交互世界建模：MineWorld', 'title_zh': 'MineWorld: 一个实时开源的 Minecraft 交互世界模型'}
{'arxiv_id': 'arXiv:2504.08195', 'title': 'Graph Based Deep Reinforcement Learning Aided by Transformers for Multi-Agent Cooperation', 'authors': 'Michael Elrod, Niloufar Mehrabi, Rahul Amin, Manveen Kaur, Long Cheng, Jim Martin, Abolfazl Razi', 'link': 'https://arxiv.org/abs/2504.08195', 'abstract': 'Mission planning for a fleet of cooperative autonomous drones in applications that involve serving distributed target points, such as disaster response, environmental monitoring, and surveillance, is challenging, especially under partial observability, limited communication range, and uncertain environments. Traditional path-planning algorithms struggle in these scenarios, particularly when prior information is not available. To address these challenges, we propose a novel framework that integrates Graph Neural Networks (GNNs), Deep Reinforcement Learning (DRL), and transformer-based mechanisms for enhanced multi-agent coordination and collective task execution. Our approach leverages GNNs to model agent-agent and agent-goal interactions through adaptive graph construction, enabling efficient information aggregation and decision-making under constrained communication. A transformer-based message-passing mechanism, augmented with edge-feature-enhanced attention, captures complex interaction patterns, while a Double Deep Q-Network (Double DQN) with prioritized experience replay optimizes agent policies in partially observable environments. This integration is carefully designed to address specific requirements of multi-agent navigation, such as scalability, adaptability, and efficient task execution. Experimental results demonstrate superior performance, with 90% service provisioning and 100% grid coverage (node discovery), while reducing the average steps per episode to 200, compared to 600 for benchmark methods such as particle swarm optimization (PSO), greedy algorithms and DQN.', 'abstract_zh': '适用于分布式目标点应用（如灾害响应、环境监测和 surveillance）的编队合作自主无人机群任务规划，在部分可观测性、有限通信范围和不确定环境下的挑战，传统路径规划算法难以应对，尤其是在缺乏先验信息的情况下。为应对这些挑战，我们提出了一种将图神经网络（GNNs）、深度强化学习（DRL）和基于变换器的机制相结合的新型框架，以增强多智能体协调和集体任务执行。该方法利用GNNs通过自适应图构建来建模智能体-智能体和智能体-目标交互，从而在受限通信条件下实现高效的信息聚合和决策。基于变换器的消息传递机制，结合边特征增强的注意力机制捕获复杂的交互模式，而双重深度Q网络（Double DQN）结合优先经验回放则在部分可观测环境中优化智能体策略。该集成设计特别考虑了多智能体导航的具体要求，如扩展性、适应性和高效的任务执行。实验结果表明，该方法在服务提供比例达到90%、网格覆盖（节点发现）为100%的同时，将每集isode的平均步骤数降至200，而基准方法，如粒子群优化（PSO）、贪婪算法和DQN，这一数字为600。', 'title_zh': '基于图的深度强化学习辅助变换器在多智能体合作中的应用'}
{'arxiv_id': 'arXiv:2504.08161', 'title': 'Rethinking the Foundations for Continual Reinforcement Learning', 'authors': 'Michael Bowling, Esraa Elelimy', 'link': 'https://arxiv.org/abs/2504.08161', 'abstract': 'Algorithms and approaches for continual reinforcement learning have gained increasing attention. Much of this early progress rests on the foundations and standard practices of traditional reinforcement learning, without questioning if they are well-suited to the challenges of continual learning agents. We suggest that many core foundations of traditional RL are, in fact, antithetical to the goals of continual reinforcement learning. We enumerate four such foundations: the Markov decision process formalism, a focus on optimal policies, the expected sum of rewards as the primary evaluation metric, and episodic benchmark environments that embrace the other three foundations. Shedding such sacredly held and taught concepts is not easy. They are self-reinforcing in that each foundation depends upon and holds up the others, making it hard to rethink each in isolation. We propose an alternative set of all four foundations that are better suited to the continual learning setting. We hope to spur on others in rethinking the traditional foundations, proposing and critiquing alternatives, and developing new algorithms and approaches enabled by better-suited foundations.', 'abstract_zh': '持续增强学习的算法与方法引起了越来越多的关注。尽管早期的进步在很大程度上建立在传统增强学习的基础和标准实践之上，但并未质疑这些方法是否适合持续学习代理所面临的挑战。我们认为，传统RL的许多核心基础实际上与持续增强学习的目标相对立。我们列出了四个这样的基础：马尔可夫决策过程的形式化表述、对最优策略的关注、预期奖励之和作为主要评估指标，以及采用其他三个基础的阶段性基准环境。放弃这些被神圣化的概念并不容易。这些基础相辅相成，互相支撑，使得单独重新思考每个基础变得困难。我们提出了一套更适合持续学习环境的四个方面的新基础。我们希望激励其他人重新思考传统的基础，提出并批判替代方案，以及开发由更合适的基础支撑的新算法与方法。', 'title_zh': '重思连续强化学习的基础'}
