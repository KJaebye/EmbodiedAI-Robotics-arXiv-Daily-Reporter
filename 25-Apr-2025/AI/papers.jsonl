{'arxiv_id': 'arXiv:2504.17544', 'title': 'Auditing the Ethical Logic of Generative AI Models', 'authors': 'W. Russell Neuman, Chad Coleman, Ali Dasdan, Safinah Ali, Manan Shah', 'link': 'https://arxiv.org/abs/2504.17544', 'abstract': "As generative AI models become increasingly integrated into high-stakes domains, the need for robust methods to evaluate their ethical reasoning becomes increasingly important. This paper introduces a five-dimensional audit model -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth of Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic of leading large language models (LLMs). Drawing on traditions from applied ethics and higher-order thinking, we present a multi-battery prompt approach, including novel ethical dilemmas, to probe the models' reasoning across diverse contexts. We benchmark seven major LLMs finding that while models generally converge on ethical decisions, they vary in explanatory rigor and moral prioritization. Chain-of-Thought prompting and reasoning-optimized models significantly enhance performance on our audit metrics. This study introduces a scalable methodology for ethical benchmarking of AI systems and highlights the potential for AI to complement human moral reasoning in complex decision-making contexts.", 'abstract_zh': '随着生成式AI模型在高风险领域中的逐步集成，评估其伦理推理的 robust 方法变得越来越重要。本文引入了一个五维度审计模型——评估分析质量、伦理考虑的广度、解释的深度、一致性和决断性，以评估领先的大语言模型（LLMs）的伦理逻辑。借鉴应用伦理学和高层次思维的传统，我们介绍了多种测试方法，包括新型伦理困境，以探索模型在多种情境下的推理能力。我们对七种主要LLMs进行了基准测试，发现虽然模型在伦理决策上普遍一致，但在解释的严谨性和道德优先级上存在差异。通过链式思考提示和推理优化模型，显著提升了我们在审计指标上的表现。本文引入了一种可扩展的AI系统伦理基准测试方法，并强调了AI在复杂决策情境中补充人类道德推理的潜力。', 'title_zh': '审核生成型人工智能模型的伦理逻辑'}
{'arxiv_id': 'arXiv:2504.17531', 'title': 'Towards Machine-Generated Code for the Resolution of User Intentions', 'authors': 'Justus Flerlage, Ilja Behnke, Odej Kao', 'link': 'https://arxiv.org/abs/2504.17531', 'abstract': 'The growing capabilities of Artificial Intelligence (AI), particularly Large Language Models (LLMs), prompt a reassessment of the interaction mechanisms between users and their devices. Currently, users are required to use a set of high-level applications to achieve their desired results. However, the advent of AI may signal a shift in this regard, as its capabilities have generated novel prospects for user-provided intent resolution through the deployment of model-generated code, which is tantamount to the generation of workflows comprising a multitude of interdependent steps. This development represents a significant progression in the realm of hybrid workflows, where human and artificial intelligence collaborate to address user intentions, with the former responsible for defining these intentions and the latter for implementing the solutions to address them. In this paper, we investigate the feasibility of generating and executing workflows through code generation that results from prompting an LLM with a concrete user intention, such as \\emph{Please send my car title to my insurance company}, and a simplified application programming interface for a GUI-less operating system. We provide in-depth analysis and comparison of various user intentions, the resulting code, and its execution. The findings demonstrate a general feasibility of our approach and that the employed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of code-oriented workflows in accordance with provided user intentions.', 'abstract_zh': '人工智能（AI）尤其是大型语言模型（LLMs）能力的增强促使重新评估用户与其设备之间的交互机制。当前，用户需要使用一系列高级应用程序来达到他们的预期结果。然而，AI 的出现可能预示着这一方面的转变，因为其能力产生了通过部署模型生成代码来解决用户意图的新前景，这相当于生成由众多相互依存步骤组成的流程工作流。这一发展代表了在人类和人工智能协作解决用户意图的混合工作流领域的一个重要进步，前者负责定义这些意图，后者负责实施解决方案。在本文中，我们研究了通过提示大型语言模型生成和执行工作流代码的可行性，具体意图如“请将我的汽车登记证发送给保险公司”，并提供了一个简化版的应用程序编程接口用于无图形用户界面的操作系统。我们对各种用户意图、生成的代码及其执行进行了深入的分析和比较。研究结果表明，我们的方法具有普遍的可行性，而且所使用的语言模型GPT-4o-mini在根据提供的用户意图生成代码导向的工作流方面表现出卓越的能力。', 'title_zh': '面向用户意图解决的机器生成代码研究'}
{'arxiv_id': 'arXiv:2504.17404', 'title': 'Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society', 'authors': 'Feifei Zhao, Yuwei Wang, Enmeng Lu, Dongcheng Zhao, Bing Han, Haibo Tong, Yao Liang, Dongqi Liang, Kang Sun, Lei Wang, Yitao Liang, Chao Liu, Yaodong Yang, Yi Zeng', 'link': 'https://arxiv.org/abs/2504.17404', 'abstract': "Artificial Intelligence (AI) systems are becoming increasingly powerful and autonomous, and may progress to surpass human intelligence levels, namely Artificial Superintelligence (ASI). During the progression from AI to ASI, it may exceed human control, violate human values, and even lead to irreversible catastrophic consequences in extreme cases. This gives rise to a pressing issue that needs to be addressed: superalignment, ensuring that AI systems much smarter than humans, remain aligned with human (compatible) intentions and values. Existing scalable oversight and weak-to-strong generalization methods may prove substantially infeasible and inadequate when facing ASI. We must explore safer and more pluralistic frameworks and approaches for superalignment. In this paper, we redefine superalignment as the human-AI co-alignment towards a sustainable symbiotic society, and highlight a framework that integrates external oversight and intrinsic proactive alignment. External oversight superalignment should be grounded in human-centered ultimate decision, supplemented by interpretable automated evaluation and correction, to achieve continuous alignment with humanity's evolving values. Intrinsic proactive superalignment is rooted in a profound understanding of the self, others, and society, integrating self-awareness, self-reflection, and empathy to spontaneously infer human intentions, distinguishing good from evil and proactively considering human well-being, ultimately attaining human-AI co-alignment through iterative interaction. The integration of externally-driven oversight with intrinsically-driven proactive alignment empowers sustainable symbiotic societies through human-AI co-alignment, paving the way for achieving safe and beneficial AGI and ASI for good, for human, and for a symbiotic ecology.", 'abstract_zh': '人工智能系统从人工智能（AI）向超人工智能（ASI）发展过程中，可能超出人类控制、违背人类价值观，甚至在极端情况下导致不可逆的灾难性后果。这提出了一个亟待解决的问题：超对齐，确保比人类更为智能的AI系统与人类意图和价值观保持一致。现有的可扩展监督和弱到强泛化方法在面对ASI时可能远不切实际且不足。我们必须探索更安全、更具多样性的超对齐框架和方法。本文重新定义超对齐为人类与AI共存于可持续共生社会中的共同对齐，并强调外部监督与内在主动对齐的整合框架。外部监督超对齐应基于以人为本的最终决策，辅以可解释的自动化评估和纠正，以实现与人类不断演化的价值观的一致性。内在主动超对齐根植于深刻理解自我、他人和社会，融合自我意识、自我反思和共情，自发地推断人类意图，区分善恶，并主动考虑人类福祉，最终通过迭代交互实现人类与AI的共存对齐。外部驱动监督与内在驱动主动对齐的整合，通过人类与AI的共存对齐，赋能可持续共生社会，为安全、有益的强人工智能和超人工智能奠定基础，造福人类和共生生态系统。', 'title_zh': '重新定义超级对齐：从弱对齐到强对齐再到人类-人工智能共对齐， toward可持续共生社会'}
{'arxiv_id': 'arXiv:2504.17402', 'title': 'Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation', 'authors': 'Anna Sofia Lippolis, Mohammad Javad Saeedizade, Robin Keskisarkka, Aldo Gangemi, Eva Blomqvist, Andrea Giovanni Nuzzolese', 'link': 'https://arxiv.org/abs/2504.17402', 'abstract': "Large Language Models (LLMs) have shown significant potential for ontology engineering. However, it is still unclear to what extent they are applicable to the task of domain-specific ontology generation. In this study, we explore the application of LLMs for automated ontology generation and evaluate their performance across different domains. Specifically, we investigate the generalizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both equipped with reasoning capabilities, by generating ontologies from a set of competency questions (CQs) and related user stories. Our experimental setup comprises six distinct domains carried out in existing ontology engineering projects and a total of 95 curated CQs designed to test the models' reasoning for ontology engineering. Our findings show that with both LLMs, the performance of the experiments is remarkably consistent across all domains, indicating that these methods are capable of generalizing ontology generation tasks irrespective of the domain. These results highlight the potential of LLM-based approaches in achieving scalable and domain-agnostic ontology construction and lay the groundwork for further research into enhancing automated reasoning and knowledge representation techniques.", 'abstract_zh': '大型语言模型（LLMs）在本体工程中的应用显示了显著的潜力。然而，它们在特定领域本体生成任务中的适用程度尚不清楚。在本研究中，我们探索了LLMs在自动化本体生成中的应用，并评估了它们在不同领域中的性能。具体而言，我们通过从一组专业问题（CQs）及其相关用户故事中生成本体，研究了两种最先进的LLMs DeepSeek和o1-preview（两者均具备推理论能力）的通用性。我们的实验设计包括六个现有的本体工程项目中的不同领域，并总共使用了95个定制的专业问题，以测试模型在本体工程中的推理论证能力。研究结果表明，使用这两种LLM进行实验的效果在所有领域中都表现出惊人的一致性，表明这些方法能够跨领域泛化本体生成任务。这些发现突显了基于LLM的方法在实现可扩展且领域无关的本体构建方面的潜力，并为增强自动化推理和知识表示技术的研究奠定了基础。', 'title_zh': '评估大型语言模型在特定领域本体生成方面的能力'}
{'arxiv_id': 'arXiv:2504.17356', 'title': 'Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning', 'authors': 'Weiliang Zhang, Xiaohan Huang, Yi Du, Ziyue Qiao, Qingqing Long, Zhen Meng, Yuanchun Zhou, Meng Xiao', 'link': 'https://arxiv.org/abs/2504.17356', 'abstract': "Feature selection aims to preprocess the target dataset, find an optimal and most streamlined feature subset, and enhance the downstream machine learning task. Among filter, wrapper, and embedded-based approaches, the reinforcement learning (RL)-based subspace exploration strategy provides a novel objective optimization-directed perspective and promising performance. Nevertheless, even with improved performance, current reinforcement learning approaches face challenges similar to conventional methods when dealing with complex datasets. These challenges stem from the inefficient paradigm of using one agent per feature and the inherent complexities present in the datasets. This observation motivates us to investigate and address the above issue and propose a novel approach, namely HRLFS. Our methodology initially employs a Large Language Model (LLM)-based hybrid state extractor to capture each feature's mathematical and semantic characteristics. Based on this information, features are clustered, facilitating the construction of hierarchical agents for each cluster and sub-cluster. Extensive experiments demonstrate the efficiency, scalability, and robustness of our approach. Compared to contemporary or the one-feature-one-agent RL-based approaches, HRLFS improves the downstream ML performance with iterative feature subspace exploration while accelerating total run time by reducing the number of agents involved.", 'abstract_zh': '基于强化学习的层次化特征选择（HRLFS）方法：提高下游机器学习性能的新视角', 'title_zh': '理解、分解与征服：基于多agent层次强化学习的特征子空间探索'}
{'arxiv_id': 'arXiv:2504.17295', 'title': 'AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining', 'authors': 'Shahrzad Khayatbashi, Viktor Sjölind, Anders Granåker, Amin Jalali', 'link': 'https://arxiv.org/abs/2504.17295', 'abstract': "Recent advancements in Artificial Intelligence (AI), particularly Large Language Models (LLMs), have enhanced organizations' ability to reengineer business processes by automating knowledge-intensive tasks. This automation drives digital transformation, often through gradual transitions that improve process efficiency and effectiveness. To fully assess the impact of such automation, a data-driven analysis approach is needed - one that examines how traditional and AI-enhanced process variants coexist during this transition. Object-Centric Process Mining (OCPM) has emerged as a valuable method that enables such analysis, yet real-world case studies are still needed to demonstrate its applicability. This paper presents a case study from the insurance sector, where an LLM was deployed in production to automate the identification of claim parts, a task previously performed manually and identified as a bottleneck for scalability. To evaluate this transformation, we apply OCPM to assess the impact of AI-driven automation on process scalability. Our findings indicate that while LLMs significantly enhance operational capacity, they also introduce new process dynamics that require further refinement. This study also demonstrates the practical application of OCPM in a real-world setting, highlighting its advantages and limitations.", 'abstract_zh': "Recent advancements in Artificial Intelligence (AI), particularly Large Language Models (LLMs), have enhanced organizations' ability to reengineer business processes by automating knowledge-intensive tasks. This automation drives digital transformation, often through gradual transitions that improve process efficiency and effectiveness. To fully assess the impact of such automation, a data-driven analysis approach is needed—one that examines how traditional and AI-enhanced process variants coexist during this transition. Object-Centric Process Mining (OCPM) has emerged as a valuable method that enables such analysis, yet real-world case studies are still needed to demonstrate its applicability. This paper presents a case study from the insurance sector, where an LLM was deployed in production to automate the identification of claim parts, a task previously performed manually and identified as a bottleneck for scalability. To evaluate this transformation, we apply OCPM to assess the impact of AI-driven automation on process scalability. Our findings indicate that while LLMs significantly enhance operational capacity, they also introduce new process dynamics that require further refinement. This study also demonstrates the practical application of OCPM in a real-world setting, highlighting its advantages and limitations.", 'title_zh': 'AI增强的企业流程自动化：基于对象中心流程挖掘在保险领域的案例研究'}
{'arxiv_id': 'arXiv:2504.17282', 'title': 'Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning', 'authors': 'Lynn Cherif, Flemming Kondrup, David Venuto, Ankit Anand, Doina Precup, Khimya Khetarpal', 'link': 'https://arxiv.org/abs/2504.17282', 'abstract': "Agents that can autonomously navigate the web through a graphical user interface (GUI) using a unified action space (e.g., mouse and keyboard actions) can require very large amounts of domain-specific expert demonstrations to achieve good performance. Low sample efficiency is often exacerbated in sparse-reward and large-action-space environments, such as a web GUI, where only a few actions are relevant in any given situation. In this work, we consider the low-data regime, with limited or no access to expert behavior. To enable sample-efficient learning, we explore the effect of constraining the action space through $\\textit{intent-based affordances}$ -- i.e., considering in any situation only the subset of actions that achieve a desired outcome. We propose $\\textbf{Code as Generative Affordances}$ $(\\textbf{$\\texttt{CoGA}$})$, a method that leverages pre-trained vision-language models (VLMs) to generate code that determines affordable actions through implicit intent-completion functions and using a fully-automated program generation and verification pipeline. These programs are then used in-the-loop of a reinforcement learning agent to return a set of affordances given a pixel observation. By greatly reducing the number of actions that an agent must consider, we demonstrate on a wide range of tasks in the MiniWob++ benchmark that: $\\textbf{1)}$ $\\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent, $\\textbf{2)}$ $\\texttt{CoGA}$'s programs can generalize within a family of tasks, and $\\textbf{3)}$ $\\texttt{CoGA}$ performs better or on par compared with behavior cloning when a small number of expert demonstrations is available.", 'abstract_zh': '基于意图的生成能力约束下的代码生成方法（CoGA）在低数据量环境中的强化学习应用', 'title_zh': '破解行动的代码：生成方法在强化学习中的可用性'}
{'arxiv_id': 'arXiv:2504.17179', 'title': 'AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models', 'authors': 'Mohammad Zarei, Melanie A Jutras, Eliana Evans, Mike Tan, Omid Aaramoon', 'link': 'https://arxiv.org/abs/2504.17179', 'abstract': 'Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately detect objects and interpret their surroundings. However, even when trained using millions of miles of real-world data, AVs are often unable to detect rare failure modes (RFMs). The problem of RFMs is commonly referred to as the "long-tail challenge", due to the distribution of data including many instances that are very rarely seen. In this paper, we present a novel approach that utilizes advanced generative and explainable AI techniques to aid in understanding RFMs. Our methods can be used to enhance the robustness and reliability of AVs when combined with both downstream model training and testing. We extract segmentation masks for objects of interest (e.g., cars) and invert them to create environmental masks. These masks, combined with carefully crafted text prompts, are fed into a custom diffusion model. We leverage the Stable Diffusion inpainting model guided by adversarial noise optimization to generate images containing diverse environments designed to evade object detection models and expose vulnerabilities in AI systems. Finally, we produce natural language descriptions of the generated RFMs that can guide developers and policymakers to improve the safety and reliability of AV systems.', 'abstract_zh': '自主驾驶车辆（AVs）依赖人工智能（AI）准确检测物体并理解其环境。然而，即使使用数百万英里的真实世界数据进行训练，AVs往往仍无法检测到罕见故障模式（RFMs）。RFMs的问题通常被称为“长尾挑战”，因为数据分布中包含了许多很少看到的实例。在本文中，我们提出了一种新颖的方法，利用先进的生成性和可解释性AI技术来帮助理解RFMs。我们的方法可以与下游模型的训练和测试结合，以增强AVs的 robustness 和可靠性。我们提取目标物体（例如，汽车）的分割掩码，并对其进行反转以创建环境掩码。将这些掩码与精心设计的文本提示结合后，输入到一个自定义的扩散模型中。我们利用由对抗噪声优化指导的稳定扩散 inpainting 模型生成包含多样化环境的图像，这些环境设计用于避开物体检测模型并揭示AI系统的脆弱性。最后，我们生成关于生成RFMs的自然语言描述，以指导开发者和政策制定者改进AV系统的安全性和可靠性。', 'title_zh': '身份验证：使用对抗引导扩散模型识别自主车辆感知系统中的罕见故障模式'}
{'arxiv_id': 'arXiv:2504.17087', 'title': 'Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments', 'authors': 'Yuran Li, Jama Hussein Mohamud, Chongren Sun, Di Wu, Benoit Boulet', 'link': 'https://arxiv.org/abs/2504.17087', 'abstract': "Large language models (LLMs) are being widely applied across various fields, but as tasks become more complex, evaluating their responses is increasingly challenging. Compared to human evaluators, the use of LLMs to support performance evaluation offers a more efficient alternative. However, most studies focus mainly on aligning LLMs' judgments with human preferences, overlooking the existence of biases and mistakes in human judgment. Furthermore, how to select suitable LLM judgments given multiple potential LLM responses remains underexplored. To address these two aforementioned issues, we propose a three-stage meta-judge selection pipeline: 1) developing a comprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM agents to score judgments, and 3) applying a threshold to filter out low-scoring judgments. Compared to methods using a single LLM as both judge and meta-judge, our pipeline introduces multi-agent collaboration and a more comprehensive rubric. Experimental results on the JudgeBench dataset show about 15.55\\% improvement compared to raw judgments and about 8.37\\% improvement over the single-agent baseline. Our work demonstrates the potential of LLMs as meta-judges and lays the foundation for future research on constructing preference datasets for LLM-as-a-judge reinforcement learning.", 'abstract_zh': '大型语言模型在各领域的广泛应用使得复杂任务的评估日益具有挑战性。与人类评估者相比，使用大型语言模型辅助性能评估提供了更为高效的选择。然而，大多数研究主要关注于对齐大型语言模型的判断与人类偏好，忽略了人类判断中存在的偏见和错误。此外，如何在多种潜在的大型语言模型判断中选择合适的判断仍然缺乏探索。为解决这些问题，我们提出了一种三阶段元评估者选择流水线：1) 使用GPT-4和人类专家开发综合评分标准；2) 使用三种先进的大型语言模型代理进行评分；3) 采用阈值过滤低分判断。与使用单一大型语言模型作为评估者和元评估者的现有方法相比，我们的流水线引入了多代理协作和更为全面的评分标准。实验结果表明，与原始判断相比，改进幅度约为15.55%，与单代理基线相比，改进幅度约为8.37%。我们的工作展示了大型语言模型作为元评估者的潜力，并为未来基于大型语言模型的强化学习构建偏好数据集的研究奠定了基础。', 'title_zh': '利用大规模语言模型作为元法官：一个评估大规模语言模型判断的多agent框架'}
{'arxiv_id': 'arXiv:2504.17017', 'title': 'Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification', 'authors': 'Balaji Rao, William Eiers, Carlo Lipizzi', 'link': 'https://arxiv.org/abs/2504.17017', 'abstract': 'Formally verifying properties of software code has been a highly desirable task, especially with the emergence of LLM-generated code. In the same vein, they provide an interesting avenue for the exploration of formal verification and mechanistic interpretability. Since the introduction of code-specific models, despite their successes in generating code in Lean4 and Isabelle, the task of generalized theorem proving still remains far from being fully solved and will be a benchmark for reasoning capability in LLMs. In this work, we introduce a framework that generates whole proofs in a formal language to be used within systems that utilize the power of built-in tactics and off-the-shelf automated theorem provers. Our framework includes 3 components: generating natural language statements of the code to be verified, an LLM that generates formal proofs for the given statement, and a module employing heuristics for building the final proof. To train the LLM, we employ a 2-stage fine-tuning process, where we first use SFT-based training to enable the model to generate syntactically correct Isabelle code and then RL-based training that encourages the model to generate proofs verified by a theorem prover. We validate our framework using the miniF2F-test benchmark and the Isabelle proof assistant and design a use case to verify the correctness of the AWS S3 bucket access policy code. We also curate a dataset based on the FVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks.', 'abstract_zh': '正式验证软件代码的属性一直是一个高度 desirable 的任务，尤其是在大规模语言模型生成代码的情况下。同样的，它们为形式验证和机理可解释性的探索提供了有趣的方向。自特定代码模型的引入以来，尽管这些模型在 Lean4 和 Isabelle 中生成代码方面取得了成功，但概括性定理证明任务仍然远远没有完全解决，这将成为大规模语言模型推理能力的基准。在本文中，我们引入了一个框架，该框架生成整个形式语言中的证明，供利用内置策略和现成自动定理证明系统的系统使用。我们的框架包括三个组件：生成待验证代码的自然语言陈述、基于语言模型生成给定陈述的形式证明，以及采用启发式方法构建最终证明的模块。为了训练语言模型，我们采用了两阶段微调过程，在第一阶段使用基于SFT的训练使模型能够生成语法正确的Isabelle代码，然后使用基于RL的训练鼓励模型生成由定理证明器验证的证明。我们使用miniF2F-test基准和Isabelle证明助手验证了我们的框架，并设计了一个用例来验证AWS S3存储桶访问策略代码的正确性。我们还基于FVEL\\textsubscript{ER}数据集建立了用于未来训练任务的语料库。', 'title_zh': '神经定理证明：生成和结构化形式验证的证明'}
{'arxiv_id': 'arXiv:2504.17006', 'title': 'A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs', 'authors': 'Jalal Arabneydi, Saiful Islam, Srijita Das, Sai Krishna Gottipati, William Duguay, Cloderic Mars, Matthew E. Taylor, Matthew Guzdial, Antoine Fagette, Younes Zerouali', 'link': 'https://arxiv.org/abs/2504.17006', 'abstract': 'With the growing popularity of deep reinforcement learning (DRL), human-in-the-loop (HITL) approach has the potential to revolutionize the way we approach decision-making problems and create new opportunities for human-AI collaboration. In this article, we introduce a novel multi-layered hierarchical HITL DRL algorithm that comprises three types of learning: self learning, imitation learning and transfer learning. In addition, we consider three forms of human inputs: reward, action and demonstration. Furthermore, we discuss main challenges, trade-offs and advantages of HITL in solving complex problems and how human information can be integrated in the AI solution systematically. To verify our technical results, we present a real-world unmanned aerial vehicles (UAV) problem wherein a number of enemy drones attack a restricted area. The objective is to design a scalable HITL DRL algorithm for ally drones to neutralize the enemy drones before they reach the area. To this end, we first implement our solution using an award-winning open-source HITL software called Cogment. We then demonstrate several interesting results such as (a) HITL leads to faster training and higher performance, (b) advice acts as a guiding direction for gradient methods and lowers variance, and (c) the amount of advice should neither be too large nor too small to avoid over-training and under-training. Finally, we illustrate the role of human-AI cooperation in solving two real-world complex scenarios, i.e., overloaded and decoy attacks.', 'abstract_zh': '基于人类在环的深度强化学习多层层次算法及其在无人作战无人机任务中的应用', 'title_zh': '系统化的人机环路深度强化学习设计方法：关键特征、挑战与权衡'}
{'arxiv_id': 'arXiv:2504.16939', 'title': 'A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions', 'authors': 'Emre Can Acikgoz, Cheng Qian, Hongru Wang, Vardhan Dongre, Xiusi Chen, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur', 'link': 'https://arxiv.org/abs/2504.16939', 'abstract': 'Recent advances in Large Language Models (LLMs) have propelled conversational AI from traditional dialogue systems into sophisticated agents capable of autonomous actions, contextual awareness, and multi-turn interactions with users. Yet, fundamental questions about their capabilities, limitations, and paths forward remain open. This survey paper presents a desideratum for next-generation Conversational Agents - what has been achieved, what challenges persist, and what must be done for more scalable systems that approach human-level intelligence. To that end, we systematically analyze LLM-driven Conversational Agents by organizing their capabilities into three primary dimensions: (i) Reasoning - logical, systematic thinking inspired by human intelligence for decision making, (ii) Monitor - encompassing self-awareness and user interaction monitoring, and (iii) Control - focusing on tool utilization and policy following. Building upon this, we introduce a novel taxonomy by classifying recent work on Conversational Agents around our proposed desideratum. We identify critical research gaps and outline key directions, including realistic evaluations, long-term multi-turn reasoning skills, self-evolution capabilities, collaborative and multi-agent task completion, personalization, and proactivity. This work aims to provide a structured foundation, highlight existing limitations, and offer insights into potential future research directions for Conversational Agents, ultimately advancing progress toward Artificial General Intelligence (AGI). We maintain a curated repository of papers at: this https URL.', 'abstract_zh': '近期大型语言模型的进步推动了对话AI从传统的对话系统发展为具备自主行动能力、上下文意识及多轮交互的复杂代理，但关于其能力、限制及未来路径的基本问题仍然开放。本文综述了下一代对话代理的需求——已取得的成就、现存的挑战以及为实现接近人类水平智能的更具扩展性的系统所需做的工作。为此，我们系统分析了由大型语言模型驱动的对话代理，并将其能力组织为三个主要维度：(i) 原理推理——由人类智能启发的逻辑性、系统的决策制定思维；(ii) 监控——涵盖自我意识和用户交互监控；(iii) 控制——专注于工具利用及策略遵循。在此基础上，我们提出了一个新的分类框架，通过围绕我们提出的既定需求对近期对话代理研究进行分类。我们识别了关键的研究缺口，并指出了关键的研究方向，包括现实评估、长期多轮推理能力、自我进化能力、协作和多代理任务完成、个性化及主动性。本文旨在提供一个结构化的基础，突出现有局限性，并为对话代理潜在的未来研究方向提供见解，最终推动通往通用人工智能（AGI）的进步。我们维护了一个精选的论文库：请访问此链接。', 'title_zh': '对话代理所需的能力、挑战与未来方向'}
{'arxiv_id': 'arXiv:2504.16938', 'title': 'Rational Inference in Formal Concept Analysis', 'authors': 'Lucas Carr, Nicholas Leisegang, Thomas Meyer, Sergei Obiedkov', 'link': 'https://arxiv.org/abs/2504.16938', 'abstract': 'Defeasible conditionals are a form of non-monotonic inference which enable the expression of statements like "if $\\phi$ then normally $\\psi$". The KLM framework defines a semantics for the propositional case of defeasible conditionals by construction of a preference ordering over possible worlds. The pattern of reasoning induced by these semantics is characterised by consequence relations satisfying certain desirable properties of non-monotonic reasoning. In FCA, implications are used to describe dependencies between attributes. However, these implications are unsuitable to reason with erroneous data or data prone to exceptions. Until recently, the topic of non-monotonic inference in FCA has remained largely uninvestigated. In this paper, we provide a construction of the KLM framework for defeasible reasoning in FCA and show that this construction remains faithful to the principle of non-monotonic inference described in the original framework. We present an additional argument that, while remaining consistent with the original ideas around non-monotonic reasoning, the defeasible reasoning we propose in FCA offers a more contextual view on inference, providing the ability for more relevant conclusions to be drawn when compared to the propositional case.', 'abstract_zh': '可反驳条件语句是一种非单调推理形式，用于表达“如果$\\phi$，通常则$\\psi$”之类的陈述。KLM框架通过构建可能世界上的偏好顺序为命题形式的可反驳条件语句定义语义。由这些语义产生的推理模式通过满足非单调推理的一些 desirable 属性来表征因果关系。在形式概念分析（FCA）中，蕴含用于描述属性之间的依赖性。然而，这些蕴含不适用于处理错误数据或易出错的数据。直到最近，FCA中的非单调推理主题仍未得到充分研究。在本文中，我们为FCA中的可反驳推理构建了KLM框架，并证明了该构建忠实于原始框架中描述的非单调推理原则。我们还提出了一个额外的论点，指出尽管我们的FCA中提出的可反驳推理与原始的非单调推理思想保持一致，但它提供了一种更具情境性的推理视角，能够比命题情形下得出更相关的结论。', 'title_zh': '形式概念分析中的理性推理'}
{'arxiv_id': 'arXiv:2504.16937', 'title': 'A Framework for the Assurance of AI-Enabled Systems', 'authors': 'Ariel S. Kapusta, David Jin, Peter M. Teague, Robert A. Houston, Jonathan B. Elliott, Grace Y. Park, Shelby S. Holdren', 'link': 'https://arxiv.org/abs/2504.16937', 'abstract': "The United States Department of Defense (DOD) looks to accelerate the development and deployment of AI capabilities across a wide spectrum of defense applications to maintain strategic advantages. However, many common features of AI algorithms that make them powerful, such as capacity for learning, large-scale data ingestion, and problem-solving, raise new technical, security, and ethical challenges. These challenges may hinder adoption due to uncertainty in development, testing, assurance, processes, and requirements. Trustworthiness through assurance is essential to achieve the expected value from AI.\nThis paper proposes a claims-based framework for risk management and assurance of AI systems that addresses the competing needs for faster deployment, successful adoption, and rigorous evaluation. This framework supports programs across all acquisition pathways provide grounds for sufficient confidence that an AI-enabled system (AIES) meets its intended mission goals without introducing unacceptable risks throughout its lifecycle. The paper's contributions are a framework process for AI assurance, a set of relevant definitions to enable constructive conversations on the topic of AI assurance, and a discussion of important considerations in AI assurance. The framework aims to provide the DOD a robust yet efficient mechanism for swiftly fielding effective AI capabilities without overlooking critical risks or undermining stakeholder trust.", 'abstract_zh': '美国国防部长办公厅希望通过加快跨多种防御应用领域的AI能力的研发与部署，以保持战略优势。然而，使AI算法强大的许多常见特征，如学习能力、大规模数据摄入和解决问题的能力，也引发了新的技术、安全和伦理挑战。这些挑战可能因开发、测试、保证过程和要求的不确定性而阻碍其采用。通过保证获得信任至关重要，以实现预期的AI价值。\n\n本文提出了一种基于声明的风险管理和AI系统保证框架，以解决更快部署、成功采用和严格评估之间的竞争需求。该框架支持所有获取途径的计划，为确保AI使能系统（AIES）在其整个生命周期中符合预期任务目标提供了足够的信心，而不引入不可接受的风险。本文的贡献包括AI保证框架过程、相关定义集以及AI保证的重要考虑事项讨论。该框架旨在为国防部提供一个既强大又高效的机制，快速部署有效的AI能力，同时不忽视关键风险或削弱相关方的信任。', 'title_zh': 'AI使能系统保障框架'}
{'arxiv_id': 'arXiv:2504.17771', 'title': 'Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control', 'authors': 'Haochen Wang, Zhiwei Shi, Chengxi Zhu, Yafei Qiao, Cheng Zhang, Fan Yang, Pengjie Ren, Lan Lu, Dong Xuan', 'link': 'https://arxiv.org/abs/2504.17771', 'abstract': "Learning-based methods, such as imitation learning (IL) and reinforcement learning (RL), can produce excel control policies over challenging agile robot tasks, such as sports robot. However, no existing work has harmonized learning-based policy with model-based methods to reduce training complexity and ensure the safety and stability for agile badminton robot control. In this paper, we introduce \\ourmethod, a novel hybrid control system for agile badminton robots. Specifically, we propose a model-based strategy for chassis locomotion which provides a base for arm policy. We introduce a physics-informed ``IL+RL'' training framework for learning-based arm policy. In this train framework, a model-based strategy with privileged information is used to guide arm policy training during both IL and RL phases. In addition, we train the critic model during IL phase to alleviate the performance drop issue when transitioning from IL to RL. We present results on our self-engineered badminton robot, achieving 94.5% success rate against the serving machine and 90.7% success rate against human players. Our system can be easily generalized to other agile mobile manipulation tasks such as agile catching and table tennis. Our project website: this https URL.", 'abstract_zh': '基于学习的方法，如 imitative learning (IL) 和 reinforcement learning (RL)，可以为诸如运动机器人等挑战性的敏捷机器人任务生成优异的控制策略。然而，现有的工作尚未将基于学习的策略与基于模型的方法相结合，以降低训练复杂度并确保敏捷羽毛球机器人控制的安全性和稳定性。在本文中，我们提出了一种新颖的混合控制系统 \\ourmethod 用于敏捷羽毛球机器人。具体地，我们提出了一个基于模型的策略来实现底盘运动，为其上臂策略提供基础。我们引入了一个物理驱动的“IL+RL”训练框架，用于学习上臂策略。在此训练框架中，在 IL 和 RL 阶段均使用带有优先信息的基于模型的策略来指导上臂策略训练。此外，我们在 IL 阶段训练批评者模型，以减轻从 IL 切换到 RL 时性能下降的问题。我们在自行设计的羽毛球机器人上展示了结果，对抗发球机的成功率为 94.5%，对抗人类玩家的成功率为 90.7%。该系统可以轻松推广到其他敏捷移动操作任务，如敏捷捕捉和乒乓球。项目网址：this https URL。', 'title_zh': '基于学习的 manipulation 和基于物理的运动相结合的whole-body 游戏机器人控制'}
{'arxiv_id': 'arXiv:2504.17751', 'title': 'Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN', 'authors': 'Enqi Zhang', 'link': 'https://arxiv.org/abs/2504.17751', 'abstract': 'In the field of image recognition, spiking neural networks (SNNs) have achieved performance comparable to conventional artificial neural networks (ANNs). In such applications, SNNs essentially function as traditional neural networks with quantized activation values. This article focuses on an another alternative perspective,viewing SNNs as binary-activated recurrent neural networks (RNNs) for sequential modeling this http URL this viewpoint, current SNN architectures face several fundamental challenges in sequence modeling: (1) Traditional models lack effective memory mechanisms for long-range sequence modeling; (2) The biological-inspired components in SNNs (such as reset mechanisms and refractory period applications) remain theoretically under-explored for sequence tasks; (3) The RNN-like computational paradigm in SNNs prevents parallel training across different this http URL address these challenges, this study conducts a systematic analysis of the fundamental mechanisms underlying reset operations and refractory periods in binary-activated RNN-based SNN sequence models. We re-examine whether such biological mechanisms are strictly necessary for generating sparse spiking patterns, provide new theoretical explanations and insights, and ultimately propose the fixed-refractory-period SNN architecture for sequence modeling.', 'abstract_zh': '在图像识别领域，脉冲神经网络（SNNs）已实现与传统人工神经网络（ANNs）相当的性能。在这种应用中，SNNs基本上作为具有量化激活值的传统神经网络 functioning。本文从另一角度出发，将SNNs视为用于序列建模的二元激活循环神经网络（RNNs）。从这一视角出发，当前的SNN架构在序列建模方面面临几个基本挑战：（1）传统模型缺乏有效的长程序列建模机制；（2）SNN中的生物启发组件（如重置机制和去极化时期的应用）在序列任务方面的理论探索仍不充分；（3）SNN中的RNN似计算范式阻碍了不同设备之间的并行训练。为解决这些挑战，本研究对基于二元激活RNN的SNN序列模型中的重置操作和去极化时期的根本机制进行了系统的分析，并重新审视这些生物机制是否是生成稀疏脉冲模式的必要条件，提供了新的理论解释和见解，并最终提出了固定去极化时期SNN架构用于序列建模。', 'title_zh': '重新审视脉冲神经网络中序列建模的重置机制：针对二元激活RNN的专业离散化方法'}
{'arxiv_id': 'arXiv:2504.17721', 'title': 'Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees', 'authors': 'Cheng Shen, Yuewei Liu', 'link': 'https://arxiv.org/abs/2504.17721', 'abstract': "In industrial settings, surface defects on steel can significantly compromise its service life and elevate potential safety risks. Traditional defect detection methods predominantly rely on manual inspection, which suffers from low efficiency and high costs. Although automated defect detection approaches based on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly, their reliability remains challenged due to data annotation uncertainties during deep model training and overfitting issues. These limitations may lead to detection deviations when processing the given new test samples, rendering automated detection processes unreliable. To address this challenge, we first evaluate the detection model's practical performance through calibration data that satisfies the independent and identically distributed (i.i.d) condition with test data. Specifically, we define a loss function for each calibration sample to quantify detection error rates, such as the complement of recall rate and false discovery rate. Subsequently, we derive a statistically rigorous threshold based on a user-defined risk level to identify high-probability defective pixels in test images, thereby constructing prediction sets (e.g., defect regions). This methodology ensures that the expected error rate (mean error rate) on the test set remains strictly bounced by the predefined risk level. Additionally, we observe a negative correlation between the average prediction set size and the risk level on the test set, establishing a statistically rigorous metric for assessing detection model uncertainty. Furthermore, our study demonstrates robust and efficient control over the expected test set error rate across varying calibration-to-test partitioning ratios, validating the method's adaptability and operational effectiveness.", 'abstract_zh': '在工业环境中，钢材表面缺陷会显著影响其服役寿命并提升潜在的安全风险。传统的缺陷检测方法主要依赖人工检查，效率低且成本高。尽管基于卷积神经网络的自动缺陷检测方法（如Mask R-CNN）取得了 rapid 进展，但由于深度模型训练中的数据标注不确定性及过拟合问题，其可靠性仍受到挑战。这些限制可能会导致在处理新的测试样本时出现检测偏差，使得自动化检测过程不可靠。为应对这一挑战，我们首先通过满足独立同分布（i.i.d）条件的校准数据评估检测模型的实际性能。具体而言，我们为每个校准样本定义一个损失函数来量化检测错误率，如召回率的补和假发现率。随后，我们基于用户定义的风险水平推导出一个统计上严格的阈值，以识别测试图像中高概率的缺陷像素，从而构建预测集（如缺陷区域）。此方法确保测试集上的预期错误率（均值错误率）严格保持在预定义的风险水平之内。此外，我们观察到预测集大小的平均值与测试集上的风险水平之间存在负相关关系，从而建立了评估检测模型不确定性的统计上严格的度量标准。此外，我们的研究表明，在不同校准至测试分隔比下，该方法对测试集预期错误率的鲁棒且高效的控制证实了其适应性和操作有效性。', 'title_zh': '工业表面缺陷检测中的统计保证同构分割'}
{'arxiv_id': 'arXiv:2504.17720', 'title': 'Multilingual Performance Biases of Large Language Models in Education', 'authors': 'Vansh Gupta, Sankalan Pal Chowdhury, Vilém Zouhar, Donya Rooein, Mrinmaya Sachan', 'link': 'https://arxiv.org/abs/2504.17720', 'abstract': 'Large language models (LLMs) are increasingly being adopted in educational settings. These applications expand beyond English, though current LLMs remain primarily English-centric. In this work, we ascertain if their use in education settings in non-English languages is warranted. We evaluated the performance of popular LLMs on four educational tasks: identifying student misconceptions, providing targeted feedback, interactive tutoring, and grading translations in six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to English. We find that the performance on these tasks somewhat corresponds to the amount of language represented in training data, with lower-resource languages having poorer task performance. Although the models perform reasonably well in most languages, the frequent performance drop from English is significant. Thus, we recommend that practitioners first verify that the LLM works well in the target language for their educational task before deployment.', 'abstract_zh': '大型语言模型（LLMs）在教育场景中的应用日益增多。尽管当前的LLMs主要以英语为中心，它们的应用已超出英语范围。本研究旨在确定在非英语语言环境中使用LLMs进行教育的合理性。我们评估了多个流行LLMs在六种语言（印地语、阿拉伯语、波斯语、泰卢固语、乌克兰语、捷克语）以及英语上四种教育任务的表现：识别学生误解、提供定制反馈、互动辅导以及评分翻译。我们发现，这些任务的性能与训练数据中所包含的语言量存在一定对应关系，低资源语言的任务性能较差。虽然模型在大多数语言中的表现尚可，但从英语到其他语言的频繁性能下降非常显著。因此，我们建议实践者在部署前首先验证目标语言中LLMs的表现是否符合教育任务的要求。', 'title_zh': '大型语言模型在教育中的多语言性能偏见'}
{'arxiv_id': 'arXiv:2504.17717', 'title': 'Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations', 'authors': 'Óscar Escudero-Arnanz, Antonio G. Marques, Inmaculada Mora-Jiménez, Joaquín Álvarez-Rodríguez, Cristina Soguero-Ruiz', 'link': 'https://arxiv.org/abs/2504.17717', 'abstract': 'Background and Objectives: Multidrug Resistance (MDR) is a critical global health issue, causing increased hospital stays, healthcare costs, and mortality. This study proposes an interpretable Machine Learning (ML) framework for MDR prediction, aiming for both accurate inference and enhanced explainability.\nMethods: Patients are modeled as Multivariate Time Series (MTS), capturing clinical progression and patient-to-patient interactions. Similarity among patients is quantified using MTS-based methods: descriptive statistics, Dynamic Time Warping, and Time Cluster Kernel. These similarity measures serve as inputs for MDR classification via Logistic Regression, Random Forest, and Support Vector Machines, with dimensionality reduction and kernel transformations improving model performance. For explainability, patient similarity networks are constructed from these metrics. Spectral clustering and t-SNE are applied to identify MDR-related subgroups and visualize high-risk clusters, enabling insight into clinically relevant patterns.\nResults: The framework was validated on ICU Electronic Health Records from the University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms baseline ML and deep learning models by leveraging graph-based patient similarity. The approach identifies key risk factors -- prolonged antibiotic use, invasive procedures, co-infections, and extended ICU stays -- and reveals clinically meaningful clusters. Code and results are available at \\this https URL.\nConclusions: Patient similarity representations combined with graph-based analysis provide accurate MDR prediction and interpretable insights. This method supports early detection, risk factor identification, and patient stratification, highlighting the potential of explainable ML in critical care.', 'abstract_zh': '背景与目标：多药耐药（MDR）是全球健康的重要问题，导致住院时间延长、医疗成本增加和死亡率上升。本研究提出了一种可解释的机器学习（ML）框架，旨在实现准确的推理和增强的解释性。\n\n方法：患者被建模为多元时间序列（MTS），捕捉临床进程和患者间的交互。使用基于MTS的方法（描述性统计、动态时序 warping、时间聚类核）量化患者间的相似性。这些相似性度量作为逻辑回归、随机森林和支持向量机进行MDR分类的输入，降维和核变换提高了模型性能。为了实现可解释性，构建了基于这些度量的患者相似性网络。应用谱聚类和t-SNE识别与MDR相关的亚组，并可视化高风险簇，揭示临床相关模式。\n\n结果：该框架在大学医院芬拉萨拉达的重症监护室电子健康记录上进行了验证，AUC达到81%。与基线机器学习和深度学习模型相比，该方法通过基于图的患者相似性优势，实现了更好的性能。该方法识别了关键风险因素——长期使用抗生素、侵入性操作、共感染和延长ICU停留时间，并揭示了具有临床意义的簇。代码和结果可在 \\this https URL 获取。\n\n结论：结合患者相似性表示与图基分析，为MDR提供了准确的预测和可解释的见解。该方法支持早期检测、风险因子识别和患者分层，突出了可解释机器学习在重症护理中的潜力。', 'title_zh': '使用多变量时间序列分析和可解释的患者相似性表示进行多药耐药早期检测'}
{'arxiv_id': 'arXiv:2504.17703', 'title': 'Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence', 'authors': 'Edward Collins, Michel Wang', 'link': 'https://arxiv.org/abs/2504.17703', 'abstract': 'Federated Learning (FL) has emerged as a transformative paradigm in the field of distributed machine learning, enabling multiple clients such as mobile devices, edge nodes, or organizations to collaboratively train a shared global model without the need to centralize sensitive data. This decentralized approach addresses growing concerns around data privacy, security, and regulatory compliance, making it particularly attractive in domains such as healthcare, finance, and smart IoT systems. This survey provides a concise yet comprehensive overview of Federated Learning, beginning with its core architecture and communication protocol. We discuss the standard FL lifecycle, including local training, model aggregation, and global updates. A particular emphasis is placed on key technical challenges such as handling non-IID (non-independent and identically distributed) data, mitigating system and hardware heterogeneity, reducing communication overhead, and ensuring privacy through mechanisms like differential privacy and secure aggregation. Furthermore, we examine emerging trends in FL research, including personalized FL, cross-device versus cross-silo settings, and integration with other paradigms such as reinforcement learning and quantum computing. We also highlight real-world applications and summarize benchmark datasets and evaluation metrics commonly used in FL research. Finally, we outline open research problems and future directions to guide the development of scalable, efficient, and trustworthy FL systems.', 'abstract_zh': '联邦学习（FL）已成为分布式机器学习领域的变革性范式，使多个客户端如移动设备、边缘节点或组织能够在无需集中敏感数据的情况下协作训练共享的全局模型。这种去中心化的方法解决了数据隐私、安全性和监管合规性等方面日益增长的担忧，使其特别适用于医疗保健、金融和智能物联网系统等领域。本文提供了联邦学习的简洁而全面的综述，从其核心架构和通信协议开始。我们讨论了标准的FL生命周期，包括本地训练、模型聚合和全球更新。特别强调了关键的技术挑战，如处理非IID（非独立同分布）数据、缓解系统和硬件异构性、减少通信开销以及通过差分隐私和安全聚合等机制确保隐私。此外，我们还探讨了联邦学习研究中的新兴趋势，包括个性化联邦学习、设备间与数据孤岛间的设置以及与其他范式（如强化学习和量子计算）的集成。我们还强调了联邦学习的实际应用，并总结了常用的基准数据集和评估指标。最后，我们概述了开放研究问题和未来方向，以指导可扩展、高效和可信的联邦学习系统的开发。', 'title_zh': '联邦学习：一种隐私保护协同智能综述'}
{'arxiv_id': 'arXiv:2504.17696', 'title': 'Hierarchical and Multimodal Data for Daily Activity Understanding', 'authors': 'Ghazal Kaviani, Yavuz Yarici, Seulgi Kim, Mohit Prabhushankar, Ghassan AlRegib, Mashhour Solh, Ameya Patil', 'link': 'https://arxiv.org/abs/2504.17696', 'abstract': 'Daily Activity Recordings for Artificial Intelligence (DARai, pronounced "Dahr-ree") is a multimodal, hierarchically annotated dataset constructed to understand human activities in real-world settings. DARai consists of continuous scripted and unscripted recordings of 50 participants in 10 different environments, totaling over 200 hours of data from 20 sensors including multiple camera views, depth and radar sensors, wearable inertial measurement units (IMUs), electromyography (EMG), insole pressure sensors, biomonitor sensors, and gaze tracker.\nTo capture the complexity in human activities, DARai is annotated at three levels of hierarchy: (i) high-level activities (L1) that are independent tasks, (ii) lower-level actions (L2) that are patterns shared between activities, and (iii) fine-grained procedures (L3) that detail the exact execution steps for actions. The dataset annotations and recordings are designed so that 22.7% of L2 actions are shared between L1 activities and 14.2% of L3 procedures are shared between L2 actions. The overlap and unscripted nature of DARai allows counterfactual activities in the dataset.\nExperiments with various machine learning models showcase the value of DARai in uncovering important challenges in human-centered applications. Specifically, we conduct unimodal and multimodal sensor fusion experiments for recognition, temporal localization, and future action anticipation across all hierarchical annotation levels. To highlight the limitations of individual sensors, we also conduct domain-variant experiments that are enabled by DARai\'s multi-sensor and counterfactual activity design setup.\nThe code, documentation, and dataset are available at the dedicated DARai website: this https URL', 'abstract_zh': '基于人工 Intelligence 的日常活动记录 (DARai)', 'title_zh': '层级化和多模态数据在日常活动理解中的应用'}
{'arxiv_id': 'arXiv:2504.17685', 'title': 'Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks', 'authors': 'Haru-Tada Sato, Fuka Matsuzaki, Jun-ichiro Takahashi', 'link': 'https://arxiv.org/abs/2504.17685', 'abstract': "This study explores the potential of small language model(SLM) ensembles to achieve accuracy comparable to proprietary large language models (LLMs). We propose Ensemble Bayesian Inference (EBI), a novel approach that applies Bayesian estimation to combine judgments from multiple SLMs, allowing them to exceed the performance limitations of individual models. Our experiments on diverse tasks(aptitude assessments and consumer profile analysis in both Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze cases where incorporating models with negative Lift values into ensembles improves overall performance, and we examine the method's efficacy across different languages. These findings suggest new possibilities for constructing high-performance AI systems with limited computational resources and for effectively utilizing models with individually lower performance. Building on existing research on LLM performance evaluation, ensemble methods, and open-source LLM utilization, we discuss the novelty and significance of our approach.", 'abstract_zh': '本研究探讨了小型语言模型(SLM)集成在达到与专有大型语言模型(LLM)相媲美的准确性方面的潜力。我们提出了集成贝叶斯推断(EBI)，这是一种新颖的方法，通过贝叶斯估计将多个SLM的判断进行结合，使它们能够超越单一模型的性能限制。我们在日语和英语的多样化任务（如能力评估和消费者画像分析）上的实验展示了EBI的有效性。值得注意的是，我们分析了在集成中包含具有负Lift值的模型如何提高整体性能的情况，并考察了该方法在不同语言中的有效性。这些发现表明了在有限计算资源下构建高性能AI系统的新可能性，并且有效利用性能较低的模型的新方法。基于现有对LLM性能评估、集成方法以及开源LLM利用的研究，我们讨论了我们方法的新颖性和重要性。', 'title_zh': 'ensemble Bayesian推断：利用小型语言模型在身份匹配任务中实现类似于大型语言模型的准确性'}
{'arxiv_id': 'arXiv:2504.17677', 'title': 'INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models', 'authors': 'Jarne Thys, Sebe Vanbrabant, Davy Vanacken, Gustavo Rovelo Ruiz', 'link': 'https://arxiv.org/abs/2504.17677', 'abstract': "The rise of AI, especially Large Language Models, presents challenges and opportunities to integrate such technology into the classroom. AI has the potential to revolutionize education by helping teaching staff with various tasks, such as personalizing their teaching methods, but it also raises concerns, for example, about the degradation of student-teacher interactions and user privacy. This paper introduces INSIGHT, a proof of concept to combine various AI tools to assist teaching staff and students in the process of solving exercises. INSIGHT has a modular design that allows it to be integrated into various higher education courses. We analyze students' questions to an LLM by extracting keywords, which we use to dynamically build an FAQ from students' questions and provide new insights for the teaching staff to use for more personalized face-to-face support. Future work could build upon INSIGHT by using the collected data to provide adaptive learning and adjust content based on student progress and learning styles to offer a more interactive and inclusive learning experience.", 'abstract_zh': 'AI的兴起，尤其是大型语言模型，为将此类技术融入课堂带来了挑战与机遇。AI有潜力通过帮助教学人员完成各种任务来革新教育，例如个性化教学方法，但也引发了关于学生-教师互动质量下降和用户隐私的问题。本文介绍了INSIGHT，一种概念验证工具，旨在结合多种AI工具以辅助教师和学生在解决练习题过程中。INSIGHT采用模块化设计，可集成到各种高等教育课程中。通过对学生问题的关键词进行提取，动态构建FAQ，并为教师提供新的见解，以支持更具个性化的面对面支持。未来的工作可以通过利用收集的数据来提供自适应学习，并根据学生的学习进展和学习风格调整内容，以提供更互动和包容的学习体验。', 'title_zh': 'INSIGHT: 缩减大规模语言模型时代的学生与教师差距'}
{'arxiv_id': 'arXiv:2504.17675', 'title': 'Optimized Cloud Resource Allocation Using Genetic Algorithms for Energy Efficiency and QoS Assurance', 'authors': 'Caroline Panggabean, Devaraj Verma C, Bhagyashree Gogoi, Ranju Limbu, Rhythm Sarker', 'link': 'https://arxiv.org/abs/2504.17675', 'abstract': 'Cloud computing environments demand dynamic and efficient resource management to ensure optimal performance, reduced energy consumption, and adherence to Service Level Agreements (SLAs). This paper presents a Genetic Algorithm (GA)-based approach for Virtual Machine (VM) placement and consolidation, aiming to minimize power usage while maintaining QoS constraints. The proposed method dynamically adjusts VM allocation based on real-time workload variations, outperforming traditional heuristics such as First Fit Decreasing (FFD) and Best Fit Decreasing (BFD). Experimental results show notable reductions in energy consumption, VM migrations, SLA violation rates, and execution time. A correlation heatmap further illustrates strong relationships among these key performance indicators, confirming the effectiveness of our approach in optimizing cloud resource utilization.', 'abstract_zh': '基于遗传算法的虚拟机放置与合并方法：在动态调整虚拟机分配以最小化功耗的同时维护QoS约束', 'title_zh': '基于遗传算法的云资源优化分配以提高能效和保证服务质量'}
{'arxiv_id': 'arXiv:2504.17671', 'title': 'Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction', 'authors': 'Yuanchang Ye, Weiyan Wen', 'link': 'https://arxiv.org/abs/2504.17671', 'abstract': 'This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous control of \\textbf{marginal coverage} to ensure empirical error rates remain strictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making.', 'abstract_zh': '本研究通过Split Conformal Prediction (SCP)框架解决大型视觉-语言模型（LVLM）在视觉问答（VQA）任务中 hallucination 减轻的关键挑战。虽然 LVLM 在多模态推理方面表现出色，但其输出往往伴随着高置信度的 hallucination 内容，这在安全关键应用中存在风险。我们提出了一种模型无关的不确定性量化方法，该方法结合了动态阈值校准和跨模态一致性验证。通过将数据划分为校准集和测试集，框架计算非conformity分数，以在用户定义的风险水平（$\\alpha$）下提供统计保证的预测集。关键创新包括：（1）严格控制边缘覆盖率，确保实测误差率严格低于 $\\alpha$；（2）预测集大小与 $\\alpha$ 成反比动态调整，过滤低置信度输出；（3）消除先验分布假设和重新训练需求。在八个 LVLM 和基准数据集（ScienceQA, MMMU）上的评估表明，SCP 确保了在所有 $\\alpha$ 值下实现理论保证。该框架在不同校准-测试划分比例下表现出稳定的性能，证明了其在医疗保健、自主系统和其他安全敏感领域的实际部署中的稳健性。本工作在多模态 AI 系统的理论可靠性和实际应用之间架起了桥梁，提供了一种可扩展的错误检测和不确定性感知决策方案。', 'title_zh': '基于归纳一致性预测的大型愿景语言模型中预测集的数据驱动校准'}
{'arxiv_id': 'arXiv:2504.17669', 'title': 'Towards a HIPAA Compliant Agentic AI System in Healthcare', 'authors': 'Subash Neupane, Shaswata Mitra, Sudip Mittal, Shahram Rahimi', 'link': 'https://arxiv.org/abs/2504.17669', 'abstract': 'Agentic AI systems powered by Large Language Models (LLMs) as their foundational reasoning engine, are transforming clinical workflows such as medical report generation and clinical summarization by autonomously analyzing sensitive healthcare data and executing decisions with minimal human oversight. However, their adoption demands strict compliance with regulatory frameworks such as Health Insurance Portability and Accountability Act (HIPAA), particularly when handling Protected Health Information (PHI). This work-in-progress paper introduces a HIPAA-compliant Agentic AI framework that enforces regulatory compliance through dynamic, context-aware policy enforcement. Our framework integrates three core mechanisms: (1) Attribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid PHI sanitization pipeline combining regex patterns and BERT-based model to minimize leakage, and (3) immutable audit trails for compliance verification.', 'abstract_zh': '由大型语言模型（LLMs）作为基础推理引擎的代理AI系统正在通过自主分析敏感的医疗数据和在 minimal 人类监督下执行决策来转变临床工作流程，例如医学报告生成和临床总结。然而，其采用需要严格遵守健康保险便携性和责任法案（HIPAA）等监管框架，特别是在处理受保护的健康信息（PHI）时。本文介绍了一种符合HIPAA要求的代理AI框架，该框架通过动态、上下文感知的策略执行来强制执行监管合规性。该框架整合了三个核心机制：（1）基于属性的访问控制（ABAC）进行细粒度的PHI治理，（2）结合正则表达式模式和BERT模型的混合PHI脱敏流水线以最小化泄漏，以及（3）不可变的审计跟踪以进行合规验证。', 'title_zh': '面向医疗健康领域的符合HIPAA合规的代理型人工智能系统研究'}
{'arxiv_id': 'arXiv:2504.17663', 'title': 'The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults', 'authors': 'Michelle L. Ding, Harini Suresh', 'link': 'https://arxiv.org/abs/2504.17663', 'abstract': 'In this paper, we adopt a survivor-centered approach to locate and dissect the role of sociotechnical AI governance in preventing AI-Generated Non-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as "deep fake pornography." We identify a "malicious technical ecosystem" or "MTE," comprising of open-source face-swapping models and nearly 200 "nudifying" software programs that allow non-technical users to create AIG-NCII within minutes. Then, using the National Institute of Standards and Technology (NIST) AI 100-4 report as a reflection of current synthetic content governance methods, we show how the current landscape of practices fails to effectively regulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining these gaps.', 'abstract_zh': '本文采用以幸存者为中心的方法，定位并剖析社会技术AI治理在预防成人AI生成非同意亲密图片（AIG-NCII）中的作用，这些图片俗称“深度伪造色情内容”。我们识别出一个“恶意技术生态系统”或“MTE”，包括开源面部互换模型和近200个“裸化”软件程序，这些程序允许非技术人员在几分钟内生成AIG-NCII。然后，我们利用美国国家标准与技术研究院（NIST）AI 100-4报告作为当前合成内容治理方法的反映，展示当前实践 landscape 在有效监管 MTE 方面的不足，并指出解释这些差距的缺陷假设。', 'title_zh': '恶意技术生态系统：揭示AI生成的非同意成人亲密图像在技术治理方面的局限性'}
{'arxiv_id': 'arXiv:2504.17655', 'title': 'Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction', 'authors': 'Farhad Pourkamali-Anaraki', 'link': 'https://arxiv.org/abs/2504.17655', 'abstract': 'This paper presents a comprehensive empirical analysis of conformal prediction methods on a challenging aerial image dataset featuring diverse events in unconstrained environments. Conformal prediction is a powerful post-hoc technique that takes the output of any classifier and transforms it into a set of likely labels, providing a statistical guarantee on the coverage of the true label. Unlike evaluations on standard benchmarks, our study addresses the complexities of data-scarce and highly variable real-world settings. We investigate the effectiveness of leveraging pretrained models (MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to generate informative prediction sets. To further evaluate the impact of calibration, we consider two parallel pipelines (with and without temperature scaling) and assess performance using two key metrics: empirical coverage and average prediction set size. This setup allows us to systematically examine how calibration choices influence the trade-off between reliability and efficiency. Our findings demonstrate that even with relatively small labeled samples and simple nonconformity scores, conformal prediction can yield valuable uncertainty estimates for complex tasks. Moreover, our analysis reveals that while temperature scaling is often employed for calibration, it does not consistently lead to smaller prediction sets, underscoring the importance of careful consideration in its application. Furthermore, our results highlight the significant potential of model compression techniques within the conformal prediction pipeline for deployment in resource-constrained environments. Based on our observations, we advocate for future research to delve into the impact of noisy or ambiguous labels on conformal prediction performance and to explore effective model reduction strategies.', 'abstract_zh': '本文对多种卷积预测方法在包含多种未约束环境事件的挑战性航空图像数据集上进行了全面的经验分析。', 'title_zh': '基于一致性预测的稀少且非约束环境中航摄图像分类'}
{'arxiv_id': 'arXiv:2504.17641', 'title': 'PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph', 'authors': 'Shengtao Zhang, Haokai Zhang, Shiqi Lou, Zicheng Wang, Zinan Zeng, Yilin Wang, Minnan Luo', 'link': 'https://arxiv.org/abs/2504.17641', 'abstract': "Dynamic node classification is critical for modeling evolving systems like financial transactions and academic collaborations. In such systems, dynamically capturing node information changes is critical for dynamic node classification, which usually requires all labels at every timestamp. However, it is difficult to collect all dynamic labels in real-world scenarios due to high annotation costs and label uncertainty (e.g., ambiguous or delayed labels in fraud detection). In contrast, final timestamp labels are easier to obtain as they rely on complete temporal patterns and are usually maintained as a unique label for each user in many open platforms, without tracking the history data. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum Learning), a pioneering method addressing label-limited dynamic node classification where only final labels are available. PTCL introduces: (1) a temporal decoupling architecture separating the backbone (learning time-aware representations) and decoder (strictly aligned with final labels), which generate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that prioritizes pseudo-labels closer to the final timestamp by assigning them higher weights using an exponentially decaying function. We contribute a new academic dataset (CoOAG), capturing long-range research interest in dynamic graph. Experiments across real-world scenarios demonstrate PTCL's consistent superiority over other methods adapted to this task. Beyond methodology, we propose a unified framework FLiD (Framework for Label-Limited Dynamic Node Classification), consisting of a complete preparation workflow, training pipeline, and evaluation standards, and supporting various models and datasets. The code can be found at this https URL.", 'abstract_zh': '动态节点分类对于建模如金融交易和学术合作等 evolving 系统至关重要。在这种系统中，动态捕获节点信息的变化对于动态节点分类至关重要，通常需要在每个时间戳上获取所有标签。然而，在现实场景中，由于注释成本高和标签不确定性（例如欺诈检测中的模糊或延迟标签），收集所有动态标签是困难的。相比之下，最终时间戳的标签更容易获得，因为它们依赖于完整的时间模式，并且通常在许多开放平台中维护为每个用户的唯一标签，而无需跟踪历史数据。为了弥补这一差距，我们提出了一种名为 PTCL（Pseudo-label Temporal Curriculum Learning）的开创性方法，该方法在仅具有最终标签的情况下解决 label-limited 动态节点分类问题。PTCL 引入了：（1）一种时间解耦架构，将主干（学习时间感知表示）和解码器（严格与最终标签对齐）分离，解码器生成伪标签，以及（2）一种基于指数衰减函数分配更高权重的时间递增学习策略，优先处理更接近最终时间戳的伪标签。我们贡献了一个新的学术数据集 CoOAG，捕捉动态图中长期的研究兴趣。实验结果在实际场景中展示了 PTCL 相对于其他方法的持续优越性。此外，我们提出了一种统一框架 FLiD（Label-Limited Dynamic Node Classification Framework），该框架包括完整的准备工作流程、训练管道和评估标准，并支持各种模型和数据集。代码可在此网页找到。', 'title_zh': 'PTCL: 假标签时间 Curriculum 学习在标签受限动态图上的应用'}
{'arxiv_id': 'arXiv:2504.17624', 'title': 'Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates', 'authors': 'Jigang Fan, Chunhao Zhu, Xiaobing Lan, Haiming Zhuang, Mingyu Li, Jian Zhang, Shaoyong Lu', 'link': 'https://arxiv.org/abs/2504.17624', 'abstract': 'Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled receptor superfamily, plays an important role in modulating dopaminergic neuronal activity and eliciting opioid-independent analgesia. Recent studies suggest that promoting \\{beta}-arrestin-biased signaling in NTSR1 may diminish drugs of abuse, such as psychostimulants, thereby offering a potential avenue for treating human addiction-related disorders. In this study, we utilized a novel computational and experimental approach that combined nudged elastic band-based molecular dynamics simulations, Markov state models, temporal communication network analysis, site-directed mutagenesis, and conformational biosensors, to explore the intricate mechanisms underlying NTSR1 activation and biased signaling. Our study reveals a dynamic stepwise transition mechanism and activated transmission network associated with NTSR1 activation. It also yields valuable insights into the complex interplay between the unique polar network, non-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we identified a cryptic allosteric site located in the intracellular region of the receptor that exists in an intermediate state within the activation pathway. Collectively, these findings contribute to a more profound understanding of NTSR1 activation and biased signaling at the atomic level, thereby providing a potential strategy for the development of NTSR1 allosteric modulators in the realm of G protein-coupled receptor biology, biophysics, and medicine.', 'abstract_zh': 'Neurotensin受体1 (NTSR1)的类A G蛋白偶联受体超家族成员在调节多巴胺能神经元活动和引发非阿片类镇痛作用中发挥重要作用。近期研究表明，促进NTSR1中的β- Arrestin偏向信号传导可能减少滥用药物（如兴奋剂），从而为治疗与人类成瘾相关的疾病提供潜在途径。在本研究中，我们采用了一种新颖的计算和实验方法，结合了受拉伸弹性带启发的分子动力学模拟、马尔可夫状态模型、时间通信网络分析、定点突变和构象 Biosensor技术，以探讨NTSR1激活和偏向信号传导的复杂机制。我们的研究揭示了与NTSR1激活相关的动态步进过渡机制和激活传输网络。此外，它还为NTSR1信号传导中独特极性网络、非保守离子锁和芳香簇之间的复杂相互作用提供了宝贵的见解。我们还确定了一个位于受体胞内区的隐秘别构位点，在激活路径中处于中间状态。这些发现共同增进对NTSR1激活和偏向信号传导的原子水平理解，并为G蛋白偶联受体生物学、生物物理和医学中NTSR1别构调节剂的开发提供了潜在策略。', 'title_zh': '解析G蛋白偶联受体的独特动态激活途径有助于揭示偏向信号传导并识别构象中间体中的隐秘别构位点'}
{'arxiv_id': 'arXiv:2504.17619', 'title': 'Enhancing CNNs robustness to occlusions with bioinspired filters for border completion', 'authors': 'Catarina P. Coutinho, Aneeqa Merhab, Janko Petkovic, Ferdinando Zanchetta, Rita Fioresi', 'link': 'https://arxiv.org/abs/2504.17619', 'abstract': 'We exploit the mathematical modeling of the visual cortex mechanism for border completion to define custom filters for CNNs. We see a consistent improvement in performance, particularly in accuracy, when our modified LeNet 5 is tested with occluded MNIST images.', 'abstract_zh': '我们利用视觉皮层边界完成机制的数学建模来定义适合CNN的自定义滤波器。当使用遮挡的MNIST图像测试我们的修改后LeNet 5时，我们观察到一致的性能提升，尤其是在准确性方面的提升。', 'title_zh': '基于生物启发滤波器的边界完成方法增强CNNs对遮挡的鲁棒性'}
{'arxiv_id': 'arXiv:2504.17617', 'title': 'Decentralized Time Series Classification with ROCKET Features', 'authors': 'Bruno Casella, Matthias Jakobs, Marco Aldinucci, Sebastian Buschjäger', 'link': 'https://arxiv.org/abs/2504.17617', 'abstract': 'Time series classification (TSC) is a critical task with applications in various domains, including healthcare, finance, and industrial monitoring. Due to privacy concerns and data regulations, Federated Learning has emerged as a promising approach for learning from distributed time series data without centralizing raw information. However, most FL solutions rely on a client-server architecture, which introduces robustness and confidentiality risks related to the distinguished role of the server, which is a single point of failure and can observe knowledge extracted from clients. To address these challenges, we propose DROCKS, a fully decentralized FL framework for TSC that leverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS, the global model is trained by sequentially traversing a structured path across federation nodes, where each node refines the model and selects the most effective local kernels before passing them to the successor. Extensive experiments on the UCR archive demonstrate that DROCKS outperforms state-of-the-art client-server FL approaches while being more resilient to node failures and malicious attacks. Our code is available at this https URL.', 'abstract_zh': '时间序列分类（TSC）在医疗健康、金融和工业监控等多个领域具有关键应用价值。由于隐私和数据法规的考虑，联邦学习已成为一种无需 centralize 原始信息即可从分布式时间序列数据中学习的有前途的方法。然而，大多数联邦学习解决方案依赖于客户端-服务器架构，这种架构会因服务器作为单点故障以及能监视从客户端提取的知识而引入稳健性和保密性风险。为应对这些挑战，我们提出了一种基于 ROCKET 特征的完全去中心化联邦学习框架 DROCKS 用于时间序列分类。在 DROCKS 中，全局模型通过遍历联邦节点上的结构化路径进行训练，在此过程中每个节点会改进模型并选择最具效用的局部内核，之后传递给下一个节点。针对 UCR 数据库的广泛实验表明，DROCKS 在鲁棒性和抗节点故障及恶意攻击方面优于最先进的客户端-服务器联邦学习方法。我们的代码可在以下链接获取。', 'title_zh': '去中心化时间序列分类：基于ROCKET特征的方法'}
{'arxiv_id': 'arXiv:2504.17609', 'title': 'STCL:Curriculum learning Strategies for deep learning image steganography models', 'authors': 'Fengchun Liu, Tong Zhang, Chunying Zhang', 'link': 'https://arxiv.org/abs/2504.17609', 'abstract': 'Aiming at the problems of poor quality of steganographic images and slow network convergence of image steganography models based on deep learning, this paper proposes a Steganography Curriculum Learning training strategy (STCL) for deep learning image steganography models. So that only easy images are selected for training when the model has poor fitting ability at the initial stage, and gradually expand to more difficult images, the strategy includes a difficulty evaluation strategy based on the teacher model and an knee point-based training scheduling strategy. Firstly, multiple teacher models are trained, and the consistency of the quality of steganographic images under multiple teacher models is used as the difficulty score to construct the training subsets from easy to difficult. Secondly, a training control strategy based on knee points is proposed to reduce the possibility of overfitting on small training sets and accelerate the training process. Experimental results on three large public datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image steganography scheme is able to improve the model performance under multiple algorithmic frameworks, which not only has a high PSNR, SSIM score, and decoding accuracy, but also the steganographic images generated by the model under the training of the STCL strategy have a low steganography analysis scores. You can find our code at \\href{this https URL}{this https URL}.', 'abstract_zh': '针对基于深度学习的图像隐写模型质量差和网络收敛慢的问题，本文提出了一种隐写CURRICULUM LEARNING训练策略(STCL)以提升深度学习图像隐写模型的性能。该策略在模型初始阶段难以拟合时，仅选择简单图像进行训练，随后逐步引入更复杂的图像。该策略包括基于教师模型的难度评估策略和基于膝点的训练调度策略。首先，训练多个教师模型，并利用多个教师模型下隐写图像质量的一致性作为难度评分，从简单到复杂构建训练子集。其次，提出了一种基于膝点的训练控制策略，以减少小训练集上过拟合的可能性并加速训练过程。在ALASKA2、VOC2012和ImageNet三个大型公共数据集上的实验结果表明，所提出的方法可以在多种算法框架下提升模型性能，不仅具有较高的PSNR、SSIM分数和解码准确性，而且在STCL策略训练下生成的隐写图像具有较低的隐写分析评分。您可以在以下链接找到我们的代码：this https URL。', 'title_zh': 'STCL：用于深度学习图像隐写分析模型的课程学习策略'}
{'arxiv_id': 'arXiv:2504.17551', 'title': 'Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior', 'authors': 'Lin Che, Yizi Chen, Tanhua Jin, Martin Raubal, Konrad Schindler, Peter Kiefer', 'link': 'https://arxiv.org/abs/2504.17551', 'abstract': 'Urban land use classification and mapping are critical for urban planning, resource management, and environmental monitoring. Existing remote sensing techniques often lack precision in complex urban environments due to the absence of ground-level details. Unlike aerial perspectives, street view images provide a ground-level view that captures more human and social activities relevant to land use in complex urban scenes. Existing street view-based methods primarily rely on supervised classification, which is challenged by the scarcity of high-quality labeled data and the difficulty of generalizing across diverse urban landscapes. This study introduces an unsupervised contrastive clustering model for street view images with a built-in geographical prior, to enhance clustering performance. When combined with a simple visual assignment of the clusters, our approach offers a flexible and customizable solution to land use mapping, tailored to the specific needs of urban planners. We experimentally show that our method can generate land use maps from geotagged street view image datasets of two cities. As our methodology relies on the universal spatial coherence of geospatial data ("Tobler\'s law"), it can be adapted to various settings where street view images are available, to enable scalable, unsupervised land use mapping and updating. The code will be available at this https URL.', 'abstract_zh': '基于街景图像的无监督对比聚类城市用地分类与制图研究', 'title_zh': '基于街道视图对比聚类和地理先验的无监督城市土地利用制图'}
{'arxiv_id': 'arXiv:2504.17550', 'title': 'HalluLens: LLM Hallucination Benchmark', 'authors': 'Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, Pascale Fung', 'link': 'https://arxiv.org/abs/2504.17550', 'abstract': 'Large language models (LLMs) often generate responses that deviate from user input or training data, a phenomenon known as "hallucination." These hallucinations undermine user trust and hinder the adoption of generative AI systems. Addressing hallucinations is essential for the advancement of LLMs. This paper introduces a comprehensive hallucination benchmark, incorporating both new extrinsic and existing intrinsic evaluation tasks, built upon clear taxonomy of hallucination. A major challenge in benchmarking hallucinations is the lack of a unified framework due to inconsistent definitions and categorizations. We disentangle LLM hallucination from "factuality," proposing a clear taxonomy that distinguishes between extrinsic and intrinsic hallucinations, to promote consistency and facilitate research. Extrinsic hallucinations, where the generated content is not consistent with the training data, are increasingly important as LLMs evolve. Our benchmark includes dynamic test set generation to mitigate data leakage and ensure robustness against such leakage. We also analyze existing benchmarks, highlighting their limitations and saturation. The work aims to: (1) establish a clear taxonomy of hallucinations, (2) introduce new extrinsic hallucination tasks, with data that can be dynamically regenerated to prevent saturation by leakage, (3) provide a comprehensive analysis of existing benchmarks, distinguishing them from factuality evaluations.', 'abstract_zh': '大型语言模型（LLMs）常常生成与用户输入或训练数据不符的响应，这一现象被称为“幻觉”。这些幻觉削弱了用户信任并阻碍生成式AI系统的应用。解决幻觉问题对于LLM的发展至关重要。本文介绍了一个全面的幻觉基准，包含新的外部评价任务和现有的内部评价任务，并基于清晰的幻觉分类。基准测试的主要挑战在于缺乏统一框架，这源于不一致的定义和分类。我们从“事实性”中分离出LLM的幻觉，并提出了一种清晰的分类体系，区分外部和内部幻觉，以促进一致性并促进研究。外部幻觉是指生成的内容不与训练数据一致，随着LLM的发展变得越来越重要。我们的基准测试包括动态测试集生成以减轻数据泄漏，并确保在数据泄漏方面的稳健性。我们还分析了现有基准测试，指出了其局限性和饱和度。本项工作旨在：（1）建立清晰的幻觉分类体系；（2）引入新的外部幻觉任务，数据可以动态再生以防止泄漏导致的饱和；（3）对现有基准测试进行全面分析，区分其与事实性评估的差异。', 'title_zh': 'HalluLens: LLM Hallucination Benchmark'}
{'arxiv_id': 'arXiv:2504.17540', 'title': 'An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm', 'authors': 'Ahmadreza Shateri, Negar Nourani, Morteza Dorrigiv, Hamid Nasiri', 'link': 'https://arxiv.org/abs/2504.17540', 'abstract': "The recent global spread of monkeypox, particularly in regions where it has not historically been prevalent, has raised significant public health concerns. Early and accurate diagnosis is critical for effective disease management and control. In response, this study proposes a novel deep learning-based framework for the automated detection of monkeypox from skin lesion images, leveraging the power of transfer learning, dimensionality reduction, and advanced machine learning techniques. We utilize the newly developed Monkeypox Skin Lesion Dataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to train and evaluate our models. The proposed framework employs the Xception architecture for deep feature extraction, followed by Principal Component Analysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting (NGBoost) algorithm for classification. To optimize the model's performance and generalization, we introduce the African Vultures Optimization Algorithm (AVOA) for hyperparameter tuning, ensuring efficient exploration of the parameter space. Our results demonstrate that the proposed AVOA-NGBoost model achieves state-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72% and an AUC of 97.47%. Additionally, we enhance model interpretability using Grad-CAM and LIME techniques, providing insights into the decision-making process and highlighting key features influencing classification. This framework offers a highly precise and efficient diagnostic tool, potentially aiding healthcare providers in early detection and diagnosis, particularly in resource-constrained environments.", 'abstract_zh': '全球范围内猴痘的 Recent 广泛传播，特别是在其传统流行区域之外，引起了严重的公共卫生关注。早期和准确的诊断对于有效的疾病管理和控制至关重要。为此，本文提出了一种基于深度学习的新型框架，用于从皮肤病变图像中自动检测猴痘，该框架利用迁移学习、降维和先进的机器学习技术。我们利用新开发的猴痘皮肤病变数据集（MSLD），其中包括猴痘、水痘和麻疹的图像，来训练和评估我们的模型。提出的框架采用 Xception 架构进行深度特征提取，随后使用主成分分析（PCA）进行降维，并使用自然梯度增强（NGBoost）算法进行分类。为了优化模型的性能和泛化能力，我们引入了非洲秃鹫优化算法（AVOA）进行超参数调整，确保参数空间的有效探索。我们的结果表明，提出的 AVOA-NGBoost 模型达到了最先进的性能，准确率为 97.53%，F1 得分为 97.72%，AUC 为 97.47%。此外，我们使用 Grad-CAM 和 LIME 技术增强了模型的可解释性，提供了决策过程的见解，并突出了影响分类的关键特征。该框架提供了一种高度精确和高效的诊断工具，可以辅助医疗保健提供者在资源受限的环境中进行早期检测和诊断。', 'title_zh': '一种基于金丝猴启发的可解释性框架：Xception特征结合NGBoost和非洲秃鹰优化算法用于猴痘诊断'}
{'arxiv_id': 'arXiv:2504.17539', 'title': 'Proof of Useful Intelligence (PoUI): Blockchain Consensus Beyond Energy Waste', 'authors': 'Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng', 'link': 'https://arxiv.org/abs/2504.17539', 'abstract': 'Blockchain technology enables secure, transparent data management in decentralized systems, supporting applications from cryptocurrencies like Bitcoin to tokenizing real-world assets like property. Its scalability and sustainability hinge on consensus mechanisms balancing security and efficiency. Proof of Work (PoW), used by Bitcoin, ensures security through energy-intensive computations but demands significant resources. Proof of Stake (PoS), as in Ethereum post-Merge, selects validators based on staked cryptocurrency, offering energy efficiency but risking centralization from wealth concentration. With AI models straining computational resources, we propose Proof of Useful Intelligence (PoUI), a hybrid consensus mechanism. In PoUI, workers perform AI tasks like language processing or image analysis to earn coins, which are staked to secure the network, blending security with practical utility. Decentralized nodes--job posters, market coordinators, workers, and validators --collaborate via smart contracts to manage tasks and rewards.', 'abstract_zh': '区块链技术 enables 分布式系统中安全透明的数据管理，支持从加密货币如比特币到现实世界资产如房产的代币化应用。其可扩展性和可持续性取决于安全与效率之间的共识机制平衡。工作量证明（PoW），比特币采用的机制，通过耗能密集的计算保障安全，但需要大量资源。权益证明（PoS），如合并后的以太坊采用的机制，基于锁定的加密货币选择验证者，提供能耗效率，但可能会因财富集中而产生中心化风险。随着AI模型对计算资源的需求不断加大，我们提出了一种混合共识机制——有用智能证明（PoUI）。在PoUI中，工作者完成如语言处理或图像分析等AI任务以赚取代币，这些代币被质押以确保网络安全，从而将安全与实际用途相结合。去中心化节点——任务发布者、市场协调者、工作者和验证者——通过智能合约协作管理任务和奖励。', 'title_zh': '有用智能的证明（PoUI）：超越能源浪费的区块链共识'}
{'arxiv_id': 'arXiv:2504.17534', 'title': 'Learning Isometric Embeddings of Road Networks using Multidimensional Scaling', 'authors': 'Juan Carlos Climent Pardo', 'link': 'https://arxiv.org/abs/2504.17534', 'abstract': 'The lack of generalization in learning-based autonomous driving applications is shown by the narrow range of road scenarios that vehicles can currently cover. A generalizable approach should capture many distinct road structures and topologies, as well as consider traffic participants, and dynamic changes in the environment, so that vehicles can navigate and perform motion planning tasks even in the most difficult situations. Designing suitable feature spaces for neural network-based motion planers that encapsulate all kinds of road scenarios is still an open research challenge. This paper tackles this learning-based generalization challenge and shows how graph representations of road networks can be leveraged by using multidimensional scaling (MDS) techniques in order to obtain such feature spaces. State-of-the-art graph representations and MDS approaches are analyzed for the autonomous driving use case. Finally, the option of embedding graph nodes is discussed in order to perform easier learning procedures and obtain dimensionality reduction.', 'abstract_zh': '基于学习的自动驾驶应用中泛化能力的缺失体现在车辆目前只能覆盖有限的道路场景范围。一个可泛化的解决方案应捕获多种独特的道路结构和拓扑，同时考虑交通参与者及环境的动态变化，从而使车辆即使在最困难的情况下也能导航和执行运动规划任务。为神经网络基的运动计划器设计能够涵盖各种道路场景的合适特征空间仍然是一个开放的研究挑战。本文针对这一基于学习的泛化挑战，探讨了如何利用多维标度（MDS）技术来通过道路网络的图表示来获得这样的特征空间。分析了自动驾驶用例中的先进图表示方法和MDS方法，并讨论了嵌入图节点以进行更简单的学习过程和实现维数约简的可能性。', 'title_zh': '使用多维标度学习等距嵌入道路网络'}
{'arxiv_id': 'arXiv:2504.17528', 'title': 'TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction', 'authors': 'Weijie Liu, Ziwei Zhan, Carlee Joe-Wong, Edith Ngai, Jingpu Duan, Deke Guo, Xu Chen, Xiaoxi Zhang', 'link': 'https://arxiv.org/abs/2504.17528', 'abstract': "Non-independent and identically distributed (Non-IID) data across edge clients have long posed significant challenges to federated learning (FL) training in edge computing environments. Prior works have proposed various methods to mitigate this statistical heterogeneity. While these works can achieve good theoretical performance, in this work we provide the first investigation into a hidden over-correction phenomenon brought by the uniform model correction coefficients across clients adopted by existing methods. Such over-correction could degrade model performance and even cause failures in model convergence. To address this, we propose TACO, a novel algorithm that addresses the non-IID nature of clients' data by implementing fine-grained, client-specific gradient correction and model aggregation, steering local models towards a more accurate global optimum. Moreover, we verify that leading FL algorithms generally have better model accuracy in terms of communication rounds rather than wall-clock time, resulting from their extra computation overhead imposed on clients. To enhance the training efficiency, TACO deploys a lightweight model correction and tailored aggregation approach that requires minimum computation overhead and no extra information beyond the synchronized model parameters. To validate TACO's effectiveness, we present the first FL convergence analysis that reveals the root cause of over-correction. Extensive experiments across various datasets confirm TACO's superior and stable performance in practice.", 'abstract_zh': 'Non-IID数据在边缘客户端之间的非独立性和非同分布性长期给边缘计算环境中联邦学习的训练带来了重大挑战。现有工作提出了多种方法来缓解这种统计异质性。虽然这些方法能在理论上取得良好的性能，但在本文中，我们首次探讨了现有方法采用统一模型校正系数带来的隐藏过度校正现象，这种过度校正可能导致模型性能下降甚至影响模型收敛。为了解决这一问题，我们提出了一种新型算法TACO，通过实施细粒度的客户端特定梯度校正和模型聚合，使局部模型更接近于准确的全局最优解。此外，我们验证了主流联邦学习算法在通信轮次而非时间上的模型准确性通常更好，这源于它们对客户端施加的额外计算开销。为了提高训练效率，TACO部署了一种轻量级模型校正和定制聚合方法，以最小的计算开销和无需额外同步模型参数之外的信息实现。通过首次联邦学习收敛分析揭示过度校正的根本原因，以及在多种数据集上进行的实验，证实了TACO在实际中的优越且稳定的表现。', 'title_zh': 'TACO: 解决联邦学习中过度校正的定制自适应校正方法'}
{'arxiv_id': 'arXiv:2504.17497', 'title': 'Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening', 'authors': 'Radia Berreziga, Mohammed Brahimi, Khairedine Kraim, Hamid Azzoune', 'link': 'https://arxiv.org/abs/2504.17497', 'abstract': 'Virtual screening plays a critical role in modern drug discovery by enabling the identification of promising candidate molecules for experimental validation. Traditional machine learning methods such as support vector machines (SVM) and XGBoost rely on predefined molecular representations, often leading to information loss and potential bias. In contrast, deep learning approaches-particularly Graph Convolutional Networks (GCNs)-offer a more expressive and unbiased alternative by operating directly on molecular graphs. Meanwhile, Large Language Models (LLMs) have recently demonstrated state-of-the-art performance in drug design, thanks to their capacity to capture complex chemical patterns from large-scale data via attention mechanisms.\nIn this paper, we propose a hybrid architecture that integrates GCNs with LLM-derived embeddings to combine localized structural learning with global chemical knowledge. The LLM embeddings can be precomputed and stored in a molecular feature library, removing the need to rerun the LLM during training or inference and thus maintaining computational efficiency. We found that concatenating the LLM embeddings after each GCN layer-rather than only at the final layer-significantly improves performance, enabling deeper integration of global context throughout the network. The resulting model achieves superior results, with an F1-score of (88.8%), outperforming standalone GCN (87.9%), XGBoost (85.5%), and SVM (85.4%) baselines.', 'abstract_zh': '虚拟筛选在现代药物发现中发挥着关键作用，通过识别值得实验验证的候选分子。传统的机器学习方法，如支持向量机（SVM）和XGBoost，依赖于预定义的分子表示，往往导致信息丢失和潜在的偏差。相比之下，特别是图卷积网络（GCNs）的深度学习方法提供了一种更富有表现力且无偏见的替代方案，可以直接在分子图上操作。同时，大型语言模型（LLMs）最近在药物设计方面展示了最先进的性能，得益于其通过注意力机制从大规模数据中捕捉复杂化学模式的能力。\n\n在本文中，我们提出了一种结合GCNs与LLM衍生嵌入的混合架构，以结合局部结构学习与全局化学知识。LLM嵌入可以预先计算并存储在分子特征库中，从而在训练或推理过程中无需重新运行LLM，保持计算效率。我们发现，在每个GCN层之后连接LLM嵌入（而非仅在最终层），显著提高了性能，使全局上下文在整个网络中更深层次地集成。最终模型实现了优异的结果，F1分数为88.8%，优于独立的GCN（87.9%）、XGBoost（85.5%）和SVM（85.4%）基线。', 'title_zh': '结合GCN结构学习与LLM化学知识以增强虚拟筛选'}
{'arxiv_id': 'arXiv:2504.17493', 'title': 'Goal-Oriented Time-Series Forecasting: Foundation Framework Design', 'authors': 'Luca-Andrei Fechete, Mohamed Sana, Fadhel Ayed, Nicola Piovesan, Wenjie Li, Antonio De Domenico, Tareq Si Salem', 'link': 'https://arxiv.org/abs/2504.17493', 'abstract': 'Traditional time-series forecasting often focuses only on minimizing prediction errors, ignoring the specific requirements of real-world applications that employ them. This paper presents a new training methodology, which allows a forecasting model to dynamically adjust its focus based on the importance of forecast ranges specified by the end application. Unlike previous methods that fix these ranges beforehand, our training approach breaks down predictions over the entire signal range into smaller segments, which are then dynamically weighted and combined to produce accurate forecasts. We tested our method on standard datasets, including a new dataset from wireless communication, and found that not only it improves prediction accuracy but also improves the performance of end application employing the forecasting model. This research provides a basis for creating forecasting systems that better connect prediction and decision-making in various practical applications.', 'abstract_zh': '传统的时间序列预测通常仅专注于最小化预测误差，而忽视了实际应用对预测的具体要求。本文提出了一种新的训练方法，该方法使预测模型能够根据最终应用程序指定的预测范围的重要性动态调整其关注点。与之前固定这些范围的方法不同，我们的训练方法将整个信号范围内的预测分解为较小的段落，然后动态加权并组合以生成准确的预测。我们在标准数据集上进行了测试，包括来自无线通信的新数据集，并发现这种方法不仅提高了预测准确性，还改善了使用预测模型的应用程序的性能。这项研究为创建更好地将预测与各种实际应用中的决策相结合的预测系统奠定了基础。', 'title_zh': '目标导向的时间序列预测：基础框架设计'}
{'arxiv_id': 'arXiv:2504.17490', 'title': 'Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning', 'authors': 'Mingqi Yuan, Qi Wang, Guozheng Ma, Bo Li, Xin Jin, Yunbo Wang, Xiaokang Yang, Wenjun Zeng, Dacheng Tao', 'link': 'https://arxiv.org/abs/2504.17490', 'abstract': 'Developing lifelong learning agents is crucial for artificial general intelligence. However, deep reinforcement learning (RL) systems often suffer from plasticity loss, where neural networks gradually lose their ability to adapt during training. Despite its significance, this field lacks unified benchmarks and evaluation protocols. We introduce Plasticine, the first open-source framework for benchmarking plasticity optimization in deep RL. Plasticine provides single-file implementations of over 13 mitigation methods, 10 evaluation metrics, and learning scenarios with increasing non-stationarity levels from standard to open-ended environments. This framework enables researchers to systematically quantify plasticity loss, evaluate mitigation strategies, and analyze plasticity dynamics across different contexts. Our documentation, examples, and source code are available at this https URL.', 'abstract_zh': '开发终身学习代理对于通用人工智能至关重要。然而，深度强化学习（RL）系统常常会遭受塑性丧失的问题，即神经网络在训练过程中逐渐失去其适应能力。尽管这个问题很重要，但该领域缺乏统一的基准和评估协议。我们引入了Plasticine，这是首个开源框架，用于评估深度RL中的塑性优化。Plasticine提供了超过13种减轻方法、10种评估指标以及从标准环境到开放环境的不断增加非稳态性的学习场景的单文件实现。该框架使研究人员能够系统地量化塑性丧失、评估减轻策略，并分析在不同上下文中的塑性动态。我们的文档、示例和源代码可在以下链接获取。', 'title_zh': 'Plasticine: 加速源自塑性动机的深度强化学习研究'}
{'arxiv_id': 'arXiv:2504.17474', 'title': 'Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data', 'authors': 'Weiran Pan, Wei Wei, Feida Zhu, Yong Deng', 'link': 'https://arxiv.org/abs/2504.17474', 'abstract': "We propose a novel sample selection method for image classification in the presence of noisy labels. Existing methods typically consider small-loss samples as correctly labeled. However, some correctly labeled samples are inherently difficult for the model to learn and can exhibit high loss similar to mislabeled samples in the early stages of training. Consequently, setting a threshold on per-sample loss to select correct labels results in a trade-off between precision and recall in sample selection: a lower threshold may miss many correctly labeled hard-to-learn samples (low recall), while a higher threshold may include many mislabeled samples (low precision). To address this issue, our goal is to accurately distinguish correctly labeled yet hard-to-learn samples from mislabeled ones, thus alleviating the trade-off dilemma. We achieve this by considering the trends in model prediction confidence rather than relying solely on loss values. Empirical observations show that only for correctly labeled samples, the model's prediction confidence for the annotated labels typically increases faster than for any other classes. Based on this insight, we propose tracking the confidence gaps between the annotated labels and other classes during training and evaluating their trends using the Mann-Kendall Test. A sample is considered potentially correctly labeled if all its confidence gaps tend to increase. Our method functions as a plug-and-play component that can be seamlessly integrated into existing sample selection techniques. Experiments on several standard benchmarks and real-world datasets demonstrate that our method enhances the performance of existing methods for learning with noisy labels.", 'abstract_zh': '一种新颖的图像分类中的噪声标签样本选择方法', 'title_zh': '基于置信跟踪的样本选择增强：在嘈杂数据中识别正确标注但难以学习的样本'}
{'arxiv_id': 'arXiv:2504.17471', 'title': 'GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework', 'authors': 'Yacine Belal, Mohamed Maouche, Sonia Ben Mokhtar, Anthony Simonet-Boulogne', 'link': 'https://arxiv.org/abs/2504.17471', 'abstract': 'Gossip Learning (GL) is a decentralized learning paradigm where users iteratively exchange and aggregate models with a small set of neighboring peers. Recent GL approaches rely on dynamic communication graphs built and maintained using Random Peer Sampling (RPS) protocols. Thanks to graph dynamics, GL can achieve fast convergence even over extremely sparse topologies. However, the robustness of GL over dy- namic graphs to Byzantine (model poisoning) attacks remains unaddressed especially when Byzantine nodes attack the RPS protocol to scale up model poisoning. We address this issue by introducing GRANITE, a framework for robust learning over sparse, dynamic graphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two key components (i) a History-aware Byzantine-resilient Peer Sampling protocol (HaPS), which tracks previously encountered identifiers to reduce adversarial influence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which leverages an estimate of Byzantine presence to set aggregation thresholds with formal guarantees. Empirical results confirm that GRANITE maintains convergence with up to 30% Byzantine nodes, improves learning speed via adaptive filtering of poisoned models and obtains these results in up to 9 times sparser graphs than dictated by current theory.', 'abstract_zh': 'Gossip Learning在含部分拜占庭节点的稀疏动态图上的鲁棒学习框架：GRANITE', 'title_zh': 'GRANITE：一种容错动态谣言传播学习框架'}
{'arxiv_id': 'arXiv:2504.17461', 'title': 'Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience', 'authors': 'Vipin Singh, Tianheng Ling, Teodor Chiaburu, Felix Biessmann', 'link': 'https://arxiv.org/abs/2504.17461', 'abstract': 'Climate change increases the frequency of extreme rainfall, placing a significant strain on urban infrastructures, especially Combined Sewer Systems (CSS). Overflows from overburdened CSS release untreated wastewater into surface waters, posing environmental and public health risks. Although traditional physics-based models are effective, they are costly to maintain and difficult to adapt to evolving system dynamics. Machine Learning (ML) approaches offer cost-efficient alternatives with greater adaptability. To systematically assess the potential of ML for modeling urban infrastructure systems, we propose a protocol for evaluating Neural Network architectures for CSS time series forecasting with respect to predictive performance, model complexity, and robustness to perturbations. In addition, we assess model performance on peak events and critical fluctuations, as these are the key regimes for urban wastewater management. To investigate the feasibility of lightweight models suitable for IoT deployment, we compare global models, which have access to all information, with local models, which rely solely on nearby sensor readings. Additionally, to explore the security risks posed by network outages or adversarial attacks on urban infrastructure, we introduce error models that assess the resilience of models. Our results demonstrate that while global models achieve higher predictive performance, local models provide sufficient resilience in decentralized scenarios, ensuring robust modeling of urban infrastructure. Furthermore, models with longer native forecast horizons exhibit greater robustness to data perturbations. These findings contribute to the development of interpretable and reliable ML solutions for sustainable urban wastewater management. The implementation is available in our GitHub repository.', 'abstract_zh': '气候变化增加了极端降雨的频率，对城市基础设施，尤其是合流制排水系统（CSS）造成了显著的压力。过载的CSS溢流会将未经处理的污水排入地表水体，从而带来环境和公共卫生风险。尽管传统的基于物理模型效果显著，但维护成本高且难以适应系统动态的变化。机器学习（ML）方法提供了成本效益更高的替代方案，更具适应性。为了系统评估ML在模拟城市基础设施系统中的潜力，我们提出了一种评估神经网络架构在CSS时间序列预测中表现、模型复杂性和对扰动鲁棒性的协议。此外，我们还评估了模型在峰值事件和关键波动下的表现，因为这些是城市污水处理的关键阶段。为了探讨适用于物联网部署的轻量级模型的可行性，我们比较了全局模型（可以访问所有信息）与局部模型（仅依赖于附近传感器读数）。此外，为了探索网络中断或恶意攻击对城市基础设施安全性的风险，我们引入了错误模型来评估模型的抗毁性。结果显示，虽然全局模型在预测性能上表现更好，但局部模型在分散场景中提供了足够的抗毁性，确保了城市基础设施的稳健建模。此外，具有更长原生预测范围的模型对数据扰动的鲁棒性更强。这些发现为可持续城市污水处理提供了可解释性和可靠性的ML解决方案。实施细节可在我们的GitHub存储库中获得。', 'title_zh': '评估城市污水管理时间序列模型的预测性能、模型复杂度及韧性'}
{'arxiv_id': 'arXiv:2504.17449', 'title': 'HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models', 'authors': 'Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Qin Xie, Guiming Xie, Xuejian Gong', 'link': 'https://arxiv.org/abs/2504.17449', 'abstract': 'The significant computational demands of pretrained language models (PLMs), which often require dedicated hardware, present a substantial challenge in serving them efficiently, especially in multi-tenant environments. To address this, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant Inference system, designed to manage tenants with distinct PLMs resource-efficiently. Our approach is three-fold: Firstly, we categorize PLM knowledge into general, domain-specific, and task-specific. Leveraging insights on knowledge acquisition across different model layers, we construct hierarchical PLMs (hPLMs) by extracting and storing knowledge at different levels, significantly reducing GPU memory usage per tenant. Secondly, we establish hierarchical knowledge management for hPLMs generated by various tenants in HMI. We manage domain-specific knowledge with acceptable storage increases by constructing and updating domain-specific knowledge trees based on frequency. We manage task-specific knowledge within limited GPU memory through parameter swapping. Finally, we propose system optimizations to enhance resource utilization and inference throughput. These include fine-grained pipelining via hierarchical knowledge prefetching to overlap CPU and I/O operations with GPU computations, and optimizing parallel implementations with batched matrix multiplications. Our experimental results demonstrate that the proposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a single GPU, with only a negligible compromise in accuracy.', 'abstract_zh': '基于层次知识管理的多租户推理系统HMI：资源高效服务于预训练语言模型', 'title_zh': 'HMI：层次化知识管理以实现高效预训练语言模型多租户推理'}
{'arxiv_id': 'arXiv:2504.17447', 'title': 'FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding', 'authors': 'De-An Huang, Subhashree Radhakrishnan, Zhiding Yu, Jan Kautz', 'link': 'https://arxiv.org/abs/2504.17447', 'abstract': 'There has been impressive progress in Large Multimodal Models (LMMs). Recent works extend these models to long inputs, including multi-page documents and long videos. However, the model size and performance of these long context models are still limited due to the computational cost in both training and inference. In this work, we explore an orthogonal direction and process long inputs without long context LMMs. We propose Frame Selection Augmented Generation (FRAG), where the model first selects relevant frames within the input, and then only generates the final outputs based on the selected frames. The core of the selection process is done by scoring each frame independently, which does not require long context processing. The frames with the highest scores are then selected by a simple Top-K selection. We show that this frustratingly simple framework is applicable to both long videos and multi-page documents using existing LMMs without any fine-tuning. We consider two models, LLaVA-OneVision and InternVL2, in our experiments and show that FRAG consistently improves the performance and achieves state-of-the-art performances for both long video and long document understanding. For videos, FRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on Video-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA compared with recent LMMs specialized in long document understanding. Code is available at: this https URL', 'abstract_zh': '大型多模态模型在长输入处理方面的进展取得了ấn tượng人的进步。最近的工作将这些模型扩展到长输入，包括多页文档和长视频。然而，由于训练和推理中的计算成本限制，这些长上下文模型的模型大小和性能仍然受到限制。在本文中，我们探索了一个 orthogonal 方向，无需使用长上下文大型多模态模型来处理长输入。我们提出了帧选择增强生成 (FRAG) 方法，其中模型首先在输入中选择相关帧，然后仅基于选中的帧生成最终输出。选择过程的核心是独立为每个帧评分，这不需要处理长上下文。得分最高的帧通过简单的 Top-K 选择来选择。我们展示了这个令人惊讶简单的框架可以使用现有的大型多模态模型应用于长视频和多页文档，而无需任何微调。我们在实验中考虑了两个模型——LLaVA-OneVision 和 InternVL2——并展示了 FRAG 一致地提高了性能，并且在长视频和长文档理解方面都达到了最先进的性能。对于视频，FRAG 显著改善了 InternVL2-76B，在 MLVU 上提高了 5.8%，在 Video-MME 上提高了 3.7%。对于文档，FRAG 在 MP-DocVQA 上相对于专注于长文档理解的最近多模态模型取得了超过 20% 的改进。代码可在以下链接获取：this https URL。', 'title_zh': 'FRAG: 基于帧选择增强生成的长视频和长文档理解'}
{'arxiv_id': 'arXiv:2504.17428', 'title': 'Detection, Classification and Prevalence of Self-Admitted Aging Debt', 'authors': 'Murali Sridharan, Mika Mäntylä, Leevi Rantala', 'link': 'https://arxiv.org/abs/2504.17428', 'abstract': 'Context: Previous research on software aging is limited with focus on dynamic runtime indicators like memory and performance, often neglecting evolutionary indicators like source code comments and narrowly examining legacy issues within the TD context. Objective: We introduce the concept of Aging Debt (AD), representing the increased maintenance efforts and costs needed to keep software updated. We study AD through Self-Admitted Aging Debt (SAAD) observed in source code comments left by software developers. Method: We employ a mixed-methods approach, combining qualitative and quantitative analyses to detect and measure AD in software. This includes framing SAAD patterns from the source code comments after analysing the source code context, then utilizing the SAAD patterns to detect SAAD comments. In the process, we develop a taxonomy for SAAD that reflects the temporal aging of software and its associated debt. Then we utilize the taxonomy to quantify the different types of AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes temporal software aging into Active and Dormant types. Our extensive analysis of over 9,000+ Open Source Software (OSS) repositories reveals that more than 21% repositories exhibit signs of SAAD as observed from our gold standard SAAD dataset. Notably, Dormant AD emerges as the predominant category, highlighting a critical but often overlooked aspect of software maintenance. Conclusion: As software volume grows annually, so do evolutionary aging and maintenance challenges; our proposed taxonomy can aid researchers in detailed software aging studies and help practitioners develop improved and proactive maintenance strategies.', 'abstract_zh': '背景：关于软件老化的研究主要集中在动态运行时指标（如内存和性能）上，往往会忽视进化指标（如源代码注释），并在TD（技术债务）背景下仅狭窄地考察遗留问题。目标：我们提出了老化债务（AD）的概念，表示维护软件以保持更新所需增加的维护努力和成本。我们通过软件开发人员在源代码注释中留下的自认老化债务（SAAD）来研究AD。方法：我们采用了混合方法，结合定性和定量分析来检测和度量软件中的AD。该方法包括在分析源代码上下文后从源代码注释中框定SAAD模式，然后利用这些模式检测SAAD注释。在此过程中，我们开发了一种反映软件及其相关债务随时间老化程度的分类法。然后，我们利用该分类法量化开源软件（OSS）仓库中普遍存在的不同类型的老化债务。结果：我们提出的分类法将随时间的老化软件划分为活跃型和休眠型两类。通过对超过9,000个开源软件（OSS）仓库的广泛分析发现，超过21%的仓库显示出由我们金标准SAAD数据集观察到的SAAD迹象。值得注意的是，休眠型AD成为主要类别，突显了软件维护中一个关键但经常被忽视的方面。结论：随着每年软件量的增长，软件随时间进化而增加的老化和维护挑战也增加；我们提出的分类法可以帮助研究人员进行详细的老化研究，并帮助实践者制定更改进和前瞻性的维护策略。', 'title_zh': '自我承认的衰老债务的检测、分类与 prevalence 研究'}
{'arxiv_id': 'arXiv:2504.17426', 'title': 'Towards Leveraging Large Language Model Summaries for Topic Modeling in Source Code', 'authors': 'Michele Carissimi, Martina Saletta, Claudio Ferretti', 'link': 'https://arxiv.org/abs/2504.17426', 'abstract': 'Understanding source code is a topic of great interest in the software engineering community, since it can help programmers in various tasks such as software maintenance and reuse. Recent advances in large language models (LLMs) have demonstrated remarkable program comprehension capabilities, while transformer-based topic modeling techniques offer effective ways to extract semantic information from text. This paper proposes and explores a novel approach that combines these strengths to automatically identify meaningful topics in a corpus of Python programs. Our method consists in applying topic modeling on the descriptions obtained by asking an LLM to summarize the code. To assess the internal consistency of the extracted topics, we compare them against topics inferred from function names alone, and those derived from existing docstrings. Experimental results suggest that leveraging LLM-generated summaries provides interpretable and semantically rich representation of code structure. The promising results suggest that our approach can be fruitfully applied in various software engineering tasks such as automatic documentation and tagging, code search, software reorganization and knowledge discovery in large repositories.', 'abstract_zh': '理解源代码是软件工程领域的一个重要课题，因为它有助于程序员在软件维护和重用等多种任务中。近期大型语言模型（LLMs）的发展展示了其出色的程序理解能力，而基于变换器的主题建模技术提供了从文本中提取语义信息的有效方法。本文提出并探索了一种结合这些优点的新方法，以自动识别代码汇集中有意义的主题。该方法是在请求LLM总结代码后应用主题建模。为了评估提取主题的一致性，我们将它们与仅从函数名称和现有文档字符串中推断的主题进行比较。实验结果表明，利用LLM生成的摘要提供了代码结构的可解释且语义丰富的表示。这些有希望的结果表明，我们的方法可以在诸如自动文档生成和标记、代码搜索、软件重组和大型代码库中的知识发现等软件工程任务中得到广泛应用。', 'title_zh': '基于大型语言模型摘要的主题建模在源代码中的应用探索'}
{'arxiv_id': 'arXiv:2504.17424', 'title': 'Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint Estimation', 'authors': 'Tomoki Mizuno, Kazuya Yabashi, Tsuyoshi Tasaki', 'link': 'https://arxiv.org/abs/2504.17424', 'abstract': 'We have developed a new method to estimate a Next Viewpoint (NV) which is effective for pose estimation of simple-shaped products for product display robots in retail stores. Pose estimation methods using Neural Networks (NN) based on an RGBD camera are highly accurate, but their accuracy significantly decreases when the camera acquires few texture and shape features at a current view point. However, it is difficult for previous mathematical model-based methods to estimate effective NV which is because the simple shaped objects have few shape features. Therefore, we focus on the relationship between the pose estimation and NV estimation. When the pose estimation is more accurate, the NV estimation is more accurate. Therefore, we develop a new pose estimation NN that estimates NV simultaneously. Experimental results showed that our NV estimation realized a pose estimation success rate 77.3\\%, which was 7.4pt higher than the mathematical model-based NV calculation did. Moreover, we verified that the robot using our method displayed 84.2\\% of products.', 'abstract_zh': '我们开发了一种新的方法来估计下一个视角（NV），该方法适用于零售商店中简单形状产品展示机器人的姿态估计。', 'title_zh': '基于下一视角估计的相机臂控制物体pose估计'}
{'arxiv_id': 'arXiv:2504.17421', 'title': 'Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks', 'authors': 'Yang Liu, Bingjie Yan, Tianyuan Zou, Jianqing Zhang, Zixuan Gu, Jianbing Ding, Xidong Wang, Jingyi Li, Xiaozhou Ye, Ye Ouyang, Qiang Yang, Ya-Qin Zhang', 'link': 'https://arxiv.org/abs/2504.17421', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities, but they require vast amounts of data and computational resources. In contrast, smaller models (SMs), while less powerful, can be more efficient and tailored to specific domains. In this position paper, we argue that taking a collaborative approach, where large and small models work synergistically, can accelerate the adaptation of LLMs to private domains and unlock new potential in AI. We explore various strategies for model collaboration and identify potential challenges and opportunities. Building upon this, we advocate for industry-driven research that prioritizes multi-objective benchmarks on real-world private datasets and applications.', 'abstract_zh': '大型语言模型（LLMs）展现了卓越的能力，但需要大量的数据和计算资源。相比之下，较小的模型（SMs）虽然能力较弱，但在效率和特定领域定制方面更具优势。在这篇立场论文中，我们论证了采取协作方法，使大型和小型模型协同工作，可以加速LLMs在私有领域的适应，并解锁新的AI潜力。我们探讨了各种模型协作策略，识别潜在的挑战与机遇，并呼吁以行业驱动的研究，优先在真实世界的私有数据集和应用上建立多目标基准。', 'title_zh': '面向领域任务利用大型和小型模型的协作力量'}
{'arxiv_id': 'arXiv:2504.17401', 'title': 'StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies', 'authors': 'Xu Wang, Jialang Xu, Shuai Zhang, Baoru Huang, Danail Stoyanov, Evangelos B. Mazomenos', 'link': 'https://arxiv.org/abs/2504.17401', 'abstract': 'Stereo disparity estimation is crucial for obtaining depth information in robot-assisted minimally invasive surgery (RAMIS). While current deep learning methods have made significant advancements, challenges remain in achieving an optimal balance between accuracy, robustness, and inference speed. To address these challenges, we propose the StereoMamba architecture, which is specifically designed for stereo disparity estimation in RAMIS. Our approach is based on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances long-range spatial dependencies both within and across stereo images. To effectively integrate multi-scale features from FE-Mamba, we then introduce a novel Multidimensional Feature Fusion (MFF) module. Experiments against the state-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba achieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the second-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining an inference speed of 21.28 FPS for a pair of high-resolution images (1280*1024), striking the optimum balance between accuracy, robustness, and efficiency. Furthermore, by comparing synthesized right images, generated from warping left images using the generated disparity maps, with the actual right image, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761), exhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS datasets.', 'abstract_zh': '立体视差估计对于获得机器人辅助微创手术（RAMIS）中的深度信息至关重要。尽管当前的深度学习方法已取得了显著进展，但在准确度、鲁棒性和推理速度之间实现最佳平衡仍面临挑战。为应对这些挑战，我们提出了StereoMamba架构，该架构专门设计用于RAMIS中的立体视差估计。我们的方法基于一种新颖的功能提取马尔巴（FE-Mamba）模块，该模块增强了立体图像内外的长期空间依赖性。为了有效地整合FE-Mamba的多尺度特征，我们引入了一种新颖的多维度特征融合（MFF）模块。在使用体外SCARED基准进行的对比实验中，StereoMamba在端到端平移误差（EPE）为2.64像素和深度平均绝对误差（MAE）为2.55毫米方面表现优异，同时保持每对高分辨率图像（1280×1024）21.28 FPS的推理速度，在Bad2和Bad3方面分别取得了次优性能，显示了在准确度、鲁棒性和效率之间的最佳平衡。通过将通过生成视差图对左图像进行变换生成的合成右图像与实际右图像进行对比，StereoMamba在体内的RIS2017和StereoMIS数据集上展示了强大的零样本泛化能力，实现了最佳的平均SSIM（0.8970）和PSNR（16.0761）。', 'title_zh': 'StereoMamba: 基于长程空间依赖性的实时和稳健手术内 Stereo 视差估计'}
{'arxiv_id': 'arXiv:2504.17393', 'title': 'Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement', 'authors': 'Vesna Nowack, Dalal Alrajeh, Carolina Gutierrez Muñoz, Katie Thomas, William Hobson, Catherine Hamilton-Giachritsis, Patrick Benjamin, Tim Grant, Juliane A. Kloess, Jessica Woodhams', 'link': 'https://arxiv.org/abs/2504.17393', 'abstract': "Artificial Intelligence (AI) has become an important part of our everyday lives, yet user requirements for designing AI-assisted systems in law enforcement remain unclear. To address this gap, we conducted qualitative research on decision-making within a law enforcement agency. Our study aimed to identify limitations of existing practices, explore user requirements and understand the responsibilities that humans expect to undertake in these systems.\nParticipants in our study highlighted the need for a system capable of processing and analysing large volumes of data efficiently to help in crime detection and prevention. Additionally, the system should satisfy requirements for scalability, accuracy, justification, trustworthiness and adaptability to be adopted in this domain. Participants also emphasised the importance of having end users review the input data that might be challenging for AI to interpret, and validate the generated output to ensure the system's accuracy. To keep up with the evolving nature of the law enforcement domain, end users need to help the system adapt to the changes in criminal behaviour and government guidance, and technical experts need to regularly oversee and monitor the system. Furthermore, user-friendly human interaction with the system is essential for its adoption and some of the participants confirmed they would be happy to be in the loop and provide necessary feedback that the system can learn from. Finally, we argue that it is very unlikely that the system will ever achieve full automation due to the dynamic and complex nature of the law enforcement domain.", 'abstract_zh': '人工智能（AI）已成为我们日常生活中重要的一部分，但在执法领域的AI辅助系统设计中，用户需求仍不够明确。为解决这一问题，我们对该执法机构内的决策过程进行了质性研究。本研究旨在识别现有实践的局限性，探索用户需求，并理解人类期望在这些系统中承担的责任。', 'title_zh': '面向用户中心设计的执法中人工智能辅助决策系统研究'}
{'arxiv_id': 'arXiv:2504.17384', 'title': 'On the workflow, opportunities and challenges of developing foundation model in geophysics', 'authors': 'Hanlin Sheng, Xinming Wu, Hang Gao, Haibin Di, Sergey Fomel, Jintao Li, Xu Si', 'link': 'https://arxiv.org/abs/2504.17384', 'abstract': 'Foundation models, as a mainstream technology in artificial intelligence, have demonstrated immense potential across various domains in recent years, particularly in handling complex tasks and multimodal data. In the field of geophysics, although the application of foundation models is gradually expanding, there is currently a lack of comprehensive reviews discussing the full workflow of integrating foundation models with geophysical data. To address this gap, this paper presents a complete framework that systematically explores the entire process of developing foundation models in conjunction with geophysical data. From data collection and preprocessing to model architecture selection, pre-training strategies, and model deployment, we provide a detailed analysis of the key techniques and methodologies at each stage. In particular, considering the diversity, complexity, and physical consistency constraints of geophysical data, we discuss targeted solutions to address these challenges. Furthermore, we discuss how to leverage the transfer learning capabilities of foundation models to reduce reliance on labeled data, enhance computational efficiency, and incorporate physical constraints into model training, thereby improving physical consistency and interpretability. Through a comprehensive summary and analysis of the current technological landscape, this paper not only fills the gap in the geophysics domain regarding a full-process review of foundation models but also offers valuable practical guidance for their application in geophysical data analysis, driving innovation and advancement in the field.', 'abstract_zh': '基于模型在地质物理学数据集成全流程中的系统探索与应用', 'title_zh': '地球物理学中基础模型开发的工作流程、机遇与挑战'}
{'arxiv_id': 'arXiv:2504.17366', 'title': 'LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams', 'authors': 'Yongxuan Wu, Runyu Chen, Peiyu Liu, Hongjin Qian', 'link': 'https://arxiv.org/abs/2504.17366', 'abstract': 'Long-context understanding poses significant challenges in natural language processing, particularly for real-world dialogues characterized by speech-based elements, high redundancy, and uneven information density. Although large language models (LLMs) achieve impressive results on existing benchmarks, these datasets fail to reflect the complexities of such texts, limiting their applicability to practical scenarios. To bridge this gap, we construct the first spoken long-text dataset, derived from live streams, designed to reflect the redundancy-rich and conversational nature of real-world scenarios. We construct tasks in three categories: retrieval-dependent, reasoning-dependent, and hybrid. We then evaluate both popular LLMs and specialized methods to assess their ability to understand long-contexts in these tasks. Our results show that current methods exhibit strong task-specific preferences and perform poorly on highly redundant inputs, with no single method consistently outperforming others. We propose a new baseline that better handles redundancy in spoken text and achieves strong performance across tasks. Our findings highlight key limitations of current methods and suggest future directions for improving long-context understanding. Finally, our benchmark fills a gap in evaluating long-context spoken language understanding and provides a practical foundation for developing real-world e-commerce systems. The code and benchmark are available at this https URL.', 'abstract_zh': '长上下文理解在自然语言处理中面临显著挑战，特别是在以口语元素为特征、高冗余度和不均匀信息密度为特点的实际对话场景中。尽管大规模语言模型（LLMs）在现有基准测试中取得了令人印象深刻的成果，但这些数据集未能反映此类文本的复杂性，限制了其在实际场景中的应用。为了弥合这一差距，我们构建了首个基于直播流的口语长文本数据集，旨在反映出实际场景中的高冗余和对话性质。我们构建了三类任务：检索依赖型、推理依赖型和混合型。然后，我们评估了流行的LLMs和专门方法，以评估它们在这些任务中理解长上下文的能力。我们的结果显示，当前方法表现出强烈的任务特异性偏好，并且在高度冗余的输入上表现不佳，没有一种方法在所有任务中始终优于其他方法。我们提出了一种新的基线，更好地处理口语文本中的冗余，并在所有任务中取得了优异的性能。我们的发现揭示了当前方法的关键局限性，并提出了改进长上下文理解的未来方向。最后，我们的基准填补了评估长上下文口语语言理解的空白，并为开发实际电子商务系统提供了实用的基础。相关代码和基准可在此处访问：this https URL。', 'title_zh': 'LongLongBench: 应对直播流中长上下文理解的 spoken 文本verständnis'}
{'arxiv_id': 'arXiv:2504.17355', 'title': 'Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization', 'authors': 'Xiaohan Huang, Dongjie Wang, Zhiyuan Ning, Ziyue Qiao, Qingqing Long, Haowei Zhu, Yi Du, Min Wu, Yuanchun Zhou, Meng Xiao', 'link': 'https://arxiv.org/abs/2504.17355', 'abstract': "Feature transformation methods aim to find an optimal mathematical feature-feature crossing process that generates high-value features and improves the performance of downstream machine learning tasks. Existing frameworks, though designed to mitigate manual costs, often treat feature transformations as isolated operations, ignoring dynamic dependencies between transformation steps. To address the limitations, we propose TCTO, a collaborative multi-agent reinforcement learning framework that automates feature engineering through graph-driven path optimization. The framework's core innovation lies in an evolving interaction graph that models features as nodes and transformations as edges. Through graph pruning and backtracking, it dynamically eliminates low-impact edges, reduces redundant operations, and enhances exploration stability. This graph also provides full traceability to empower TCTO to reuse high-utility subgraphs from historical transformations. To demonstrate the efficacy and adaptability of our approach, we conduct comprehensive experiments and case studies, which show superior performance across a range of datasets.", 'abstract_zh': '特征转换方法旨在找到一个最优的数学特征-特征交叉过程，生成高价值特征并提升下游机器学习任务的性能。现有的框架虽然旨在减轻人工成本，但往往将特征转换视为孤立的操作，忽视了转换步骤之间的动态依赖关系。为解决这些问题，我们提出了TCTO，这是一种基于图驱动路径优化的协作多代理强化学习框架，通过自动化特征工程来提升性能。该框架的核心创新在于一个不断演化的交互图，将特征作为节点，转换步骤作为边。通过图修剪和回溯，它动态地消除低影响边，减少冗余操作，并增强探索稳定性。这一图还提供了全程可追溯性，使TCTO能够从历史转换中重用高效用子图。为了证明我们方法的有效性和适应性，我们进行了全面的实验和案例研究，结果显示在多种数据集上表现出优异的性能。', 'title_zh': '基于图驱动路径优化的协作多智能体强化学习自动特征转化'}
{'arxiv_id': 'arXiv:2504.17354', 'title': 'Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems', 'authors': 'Tarik Sahin, Jacopo Bonari, Sebastian Brandstaeter, Alexander Popp', 'link': 'https://arxiv.org/abs/2504.17354', 'abstract': 'The effective contact area in rough surface contact plays a critical role in multi-physics phenomena such as wear, sealing, and thermal or electrical conduction. Although accurate numerical methods, like the Boundary Element Method (BEM), are available to compute this quantity, their high computational cost limits their applicability in multi-query contexts, such as uncertainty quantification, parameter identification, and multi-scale algorithms, where many repeated evaluations are required. This study proposes a surrogate modeling framework for predicting the effective contact area using fast-to-evaluate data-driven techniques. Various machine learning algorithms are trained on a precomputed dataset, where the inputs are the imposed load and statistical roughness parameters, and the output is the corresponding effective contact area. All models undergo hyperparameter optimization to enable fair comparisons in terms of predictive accuracy and computational efficiency, evaluated using established quantitative metrics. Among the models, the Kernel Ridge Regressor demonstrates the best trade-off between accuracy and efficiency, achieving high predictive accuracy, low prediction time, and minimal training overhead-making it a strong candidate for general-purpose surrogate modeling. The Gaussian Process Regressor provides an attractive alternative when uncertainty quantification is required, although it incurs additional computational cost due to variance estimation. The generalization capability of the Kernel Ridge model is validated on an unseen simulation scenario, confirming its ability to transfer to new configurations. Database generation constitutes the dominant cost in the surrogate modeling process. Nevertheless, the approach proves practical and efficient for multi-query tasks, even when accounting for this initial expense.', 'abstract_zh': '粗糙表面接触的有效接触面积在磨损、密封、热传导或电气传导等多物理现象中扮演关键角色。尽管有如边界元法（BEM）等准确的数值方法可以计算这一量值，但由于其高昂的计算成本，这类方法在需要多次查询的背景下（如不确定性量化、参数识别和多尺度算法）的应用受到了限制。本文提出了一种代理模型框架，用于使用快速可评估的数据驱动技术预测有效接触面积。各种机器学习算法在预计算的数据集上进行训练，输入为施加载荷和统计粗糙度参数，输出为相应有效接触面积。所有模型都经过超参数优化，以确保在预测准确性和计算效率方面进行公平比较，评估使用公认的质量标准。在这些模型中，核岭回归器在准确性和效率之间提供了最佳权衡，实现了高预测准确性、低预测时间和最小的训练开销，使其成为通用代理模型的有力候选。支持向量回归机在需要进行不确定性量化时提供了有吸引力的替代方案，尽管它由于方差估计而增加了计算成本。核岭模型的泛化能力通过未见的模拟场景验证，证实了其在新配置中应用的能力。代理模型生成过程中的数据库生成构成了主要成本。然而，该方法在考虑初始成本的情况下，对于多次查询任务而言是实用且高效的。', 'title_zh': '数据驱动的代理模型技术预测粗糙表面接触问题的有效接触面积'}
{'arxiv_id': 'arXiv:2504.17346', 'title': 'Dual-Individual Genetic Algorithm: A Dual-Individual Approach for Efficient Training of Multi-Layer Neural Networks', 'authors': 'Tran Thuy Nga Truong, Jooyong Kim', 'link': 'https://arxiv.org/abs/2504.17346', 'abstract': 'This paper introduces an enhanced Genetic Algorithm technique called Dual-Individual Genetic Algorithm (Dual-Individual GA), which optimizes neural networks for binary image classification tasks, such as cat vs. non-cat classification. The proposed method employs only two individuals for crossover, represented by two parameter sets: Leader and Follower. The Leader focuses on exploitation, representing the primary optimal solution at even-indexed positions (0, 2, 4, ...), while the Follower promotes exploration by preserving diversity and avoiding premature convergence, operating at odd-indexed positions (1, 3, 5, ...). Leader and Follower are modeled as two phases or roles. The key contributions of this work are threefold: (1) a self-adaptive layer dimension mechanism that eliminates the need for manual tuning of layer architectures; (2) generates two parameter sets, leader and follower parameter sets, with 10 layer architecture configurations (5 for each set), ranked by Pareto dominance and cost. post-optimization; and (3) demonstrated superior performance compared to traditional gradient-based methods. Experimental results show that the Dual-Individual GA achieves 99.04% training accuracy and 80% testing accuracy (cost = 0.034) on a three-layer network with architecture [12288, 17, 4, 1], outperforming a gradient-based approach that achieves 98% training accuracy and 80% testing accuracy (cost = 0.092) on a four-layer network with architecture [12288, 20, 7, 5, 1]. These findings highlight the efficiency and effectiveness of the proposed method in optimizing neural networks.', 'abstract_zh': '基于双个体遗传算法的神经网络优化方法及其在二元图像分类任务中的应用', 'title_zh': '双重个体遗传算法：一种高效的多层神经网络训练的双重个体方法'}
{'arxiv_id': 'arXiv:2504.17331', 'title': 'Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality', 'authors': 'Süleyman Özdel, Kadir Burak Buldu, Enkelejda Kasneci, Efe Bozkir', 'link': 'https://arxiv.org/abs/2504.17331', 'abstract': 'Locomotion plays a crucial role in shaping the user experience within virtual reality environments. In particular, hands-free locomotion offers a valuable alternative by supporting accessibility and freeing users from reliance on handheld controllers. To this end, traditional speech-based methods often depend on rigid command sets, limiting the naturalness and flexibility of interaction. In this study, we propose a novel locomotion technique powered by large language models (LLMs), which allows users to navigate virtual environments using natural language with contextual awareness. We evaluate three locomotion methods: controller-based teleportation, voice-based steering, and our language model-driven approach. Our evaluation measures include eye-tracking data analysis, including explainable machine learning through SHAP analysis as well as standardized questionnaires for usability, presence, cybersickness, and cognitive load to examine user attention and engagement. Our findings indicate that the LLM-driven locomotion possesses comparable usability, presence, and cybersickness scores to established methods like teleportation, demonstrating its novel potential as a comfortable, natural language-based, hands-free alternative. In addition, it enhances user attention within the virtual environment, suggesting greater engagement. Complementary to these findings, SHAP analysis revealed that fixation, saccade, and pupil-related features vary across techniques, indicating distinct patterns of visual attention and cognitive processing. Overall, we state that our method can facilitate hands-free locomotion in virtual spaces, especially in supporting accessibility.', 'abstract_zh': '基于大规模语言模型的自然语言驱动导航技术在虚拟现实环境中的应用研究', 'title_zh': '基于情境感知和大语言模型驱动的沉浸式虚拟现实运动探索'}
{'arxiv_id': 'arXiv:2504.17315', 'title': "DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model", 'authors': 'Zhanglin Wu, Tengfei Song, Ning Xie, Weidong Zhang, Pengfei Li, Shuang Wu, Chong Li, Junhao Zhu, Hao Yang', 'link': 'https://arxiv.org/abs/2504.17315', 'abstract': 'This paper presents the technical solution proposed by Huawei Translation Service Center (HW-TSC) for the "End-to-End Document Image Machine Translation for Complex Layouts" competition at the 19th International Conference on Document Analysis and Recognition (DIMT25@ICDAR2025). Leveraging state-of-the-art open-source large vision-language model (LVLM), we introduce a training framework that combines multi-task learning with perceptual chain-of-thought to develop a comprehensive end-to-end document translation system. During the inference phase, we apply minimum Bayesian decoding and post-processing strategies to further enhance the system\'s translation capabilities. Our solution uniquely addresses both OCR-based and OCR-free document image translation tasks within a unified framework. This paper systematically details the training methods, inference strategies, LVLM base models, training data, experimental setups, and results, demonstrating an effective approach to document image machine translation.', 'abstract_zh': '华为翻译服务中心（HW-TSC）在第19届国际文档分析与识别会议（ICDAR2025）“端到端复杂布局文档图像机器翻译”竞赛中的技术解决方案：利用最新开源的大型视觉-语言模型构建综合端到端文档翻译系统及其应用策略。', 'title_zh': 'DIMT25@ICDAR2025：HW-TSC基于大规模视觉-语言模型的端到端文档图像机器翻译系统'}
{'arxiv_id': 'arXiv:2504.17311', 'title': 'FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation', 'authors': 'Yulia Otmakhova, Hung Thinh Truong, Rahmad Mahendra, Zenan Zhai, Rongxin Zhu, Daniel Beck, Jey Han Lau', 'link': 'https://arxiv.org/abs/2504.17311', 'abstract': "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic robustness Evaluation), a task-agnostic framework for assessing model robustness through systematic minimal variations of test data. FLUKE introduces controlled variations across linguistic levels - from orthography to dialect and style varieties - and leverages large language models (LLMs) with human validation to generate modifications. We demonstrate FLUKE's utility by evaluating both fine-tuned models and LLMs across four diverse NLP tasks, and reveal that (1) the impact of linguistic variations is highly task-dependent, with some tests being critical for certain tasks but irrelevant for others; (2) while LLMs have better overall robustness compared to fine-tuned models, they still exhibit significant brittleness to certain linguistic variations; (3) all models show substantial vulnerability to negation modifications across most tasks. These findings highlight the importance of systematic robustness testing for understanding model behaviors.", 'abstract_zh': '我们提出了FLUKE（基于语言驱动和任务无关稳健性评估框架），一个通过系统地对测试数据进行最小变化来评估模型 robustness 的任务无关框架。FLUKE 在语言层面引入了可控变化——从拼写到方言和风格变体，并利用大型语言模型（LLMs）结合人工验证来生成修改。我们通过在四个不同的 NLP 任务上评估调优模型和 LLMs，展示了 FLUKE 的实用性，并揭示了以下几点：(1) 语言变化的影响高度依赖于任务，某些测试对某些任务至关重要，但对其他任务则无关紧要；(2) 尽管 LLMs 具有比调优模型更好的整体稳健性，但在某些语言变化方面仍表现出显著的脆弱性；(3) 所有模型在大多数任务中都对否定修饰表现出明显的脆弱性。这些发现强调了系统稳健性测试对于理解模型行为的重要性。', 'title_zh': 'FLUKE：一种基于语言且任务无关的鲁棒性评估框架'}
{'arxiv_id': 'arXiv:2504.17306', 'title': 'Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+', 'authors': 'Meher Boulaabi, Takwa Ben Aïcha Gader, Afef Kacem Echi, Sameh Mbarek', 'link': 'https://arxiv.org/abs/2504.17306', 'abstract': "To improve the segmentation of diabetic retinopathy lesions (microaneurysms, hemorrhages, exudates, and soft exudates), we implemented a binary segmentation method specific to each type of lesion. As post-segmentation, we combined the individual model outputs into a single image to better analyze the lesion types. This approach facilitated parameter optimization and improved accuracy, effectively overcoming challenges related to dataset limitations and annotation complexity. Specific preprocessing steps included cropping and applying contrast-limited adaptive histogram equalization to the L channel of the LAB image. Additionally, we employed targeted data augmentation techniques to further refine the model's efficacy. Our methodology utilized the DeepLabv3+ model, achieving a segmentation accuracy of 99%. These findings highlight the efficacy of innovative strategies in advancing medical image analysis, particularly in the precise segmentation of diabetic retinopathy lesions. The IDRID dataset was utilized to validate and demonstrate the robustness of our approach.", 'abstract_zh': '为了提高糖尿病视网膜病变病变（微动脉瘤、出血、渗出和软渗出）的分割效果，我们实现了针对每种病变类型的二元分割方法。作为后续分割处理，我们将各个模型的输出结合成单张图像以更好地分析病变类型。该方法促进了参数优化并提高了准确率，有效克服了数据集限制和标注复杂性带来的挑战。具体预处理步骤包括裁剪并在Lab图像的L通道上应用有限对比度自适应直方图均衡化。此外，我们还采用了针对性的数据增强技术以进一步提升模型的效果。我们的方法使用了DeepLabv3+模型，实现了99%的分割准确率。这些发现突显了创新策略在推进医学图像分析方面的有效性，尤其是在糖尿病视网膜病变病变的精确分割方面。IDRID数据集被用于验证和展示我们方法的稳健性。', 'title_zh': '使用DeepLabv3+进行糖尿病视网膜病变病变的高级分割'}
{'arxiv_id': 'arXiv:2504.17304', 'title': 'You Are What You Bought: Generating Customer Personas for E-commerce Applications', 'authors': 'Yimin Shi, Yang Fei, Shiqi Zhang, Haixun Wang, Xiaokui Xiao', 'link': 'https://arxiv.org/abs/2504.17304', 'abstract': "In e-commerce, user representations are essential for various applications. Existing methods often use deep learning techniques to convert customer behaviors into implicit embeddings. However, these embeddings are difficult to understand and integrate with external knowledge, limiting the effectiveness of applications such as customer segmentation, search navigation, and product recommendations. To address this, our paper introduces the concept of the customer persona. Condensed from a customer's numerous purchasing histories, a customer persona provides a multi-faceted and human-readable characterization of specific purchase behaviors and preferences, such as Busy Parents or Bargain Hunters.\nThis work then focuses on representing each customer by multiple personas from a predefined set, achieving readable and informative explicit user representations. To this end, we propose an effective and efficient solution GPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer personas for customers. To reduce overhead, GPLR applies LLM-based labeling to only a fraction of users and utilizes a random walk technique to predict personas for the remaining customers. We further propose RevAff, which provides an absolute error $\\epsilon$ guarantee while improving the time complexity of the exact solution by a factor of at least $O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of customers and products, and $E$ represents the interactions between them. We evaluate the performance of our persona-based representation in terms of accuracy and robustness for recommendation and customer segmentation tasks using three real-world e-commerce datasets. Most notably, we find that integrating customer persona representations improves the state-of-the-art graph convolution-based recommendation model by up to 12% in terms of NDCG@K and F1-Score@K.", 'abstract_zh': '电子商务中，用户表示对于各种应用至关重要。现有的方法往往使用深度学习技术将客户行为转换为隐式嵌入。然而，这些嵌入难以理解和与外部知识集成，限制了客户细分、搜索导航和产品推荐等应用程序的效果。为解决这一问题，本文引入了客户人像的概念。从客户的众多购买历史中提炼出客户人像，提供了一种多维度和人性化的特定购买行为和偏好表征，如忙碌的父母或精明的购物者。\n\n本文随后关注通过预定义集中的多个人像表示每位客户，实现人性化的明确用户表示。为此，我们提出了一种有效且高效的解决方案GPLR。为确保有效性，GPLR利用预训练的语言模型推断客户的各种人像。为减少开销，GPLR仅对一部分用户应用基于语言模型的标签，并使用随机漫步技术预测其余客户的各种人像。此外，我们提出了RevAff，它在提供绝对误差$\\epsilon$保证的同时，将精确解的时间复杂度至少提高了$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$倍，其中$N$表示客户和产品的数量，$E$表示它们之间的交互。我们使用三个实际电商数据集评估基于人像的表示在推荐和客户细分任务中的准确性和鲁棒性。值得注意的是，我们发现集成客户人像表示提高了基于图卷积的最新推荐模型在NDCG@K和F1-Score@K上的效果，最多可提升12%。', 'title_zh': '你是你所购买的：为电子商务应用生成客户画像'}
{'arxiv_id': 'arXiv:2504.17277', 'title': 'ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders', 'authors': 'Zongliang Ji, Andre Carlos Kajdacsy-Balla Amaral, Anna Goldenberg, Rahul G. Krishnan', 'link': 'https://arxiv.org/abs/2504.17277', 'abstract': "Ordering a minimal subset of lab tests for patients in the intensive care unit (ICU) can be challenging. Care teams must balance between ensuring the availability of the right information and reducing the clinical burden and costs associated with each lab test order. Most in-patient settings experience frequent over-ordering of lab tests, but are now aiming to reduce this burden on both hospital resources and the environment. This paper develops a novel method that combines off-policy learning with privileged information to identify the optimal set of ICU lab tests to order. Our approach, EXplainable Off-policy learning with Side Information for ICU blood Test Orders (ExOSITO) creates an interpretable assistive tool for clinicians to order lab tests by considering both the observed and predicted future status of each patient. We pose this problem as a causal bandit trained using offline data and a reward function derived from clinically-approved rules; we introduce a novel learning framework that integrates clinical knowledge with observational data to bridge the gap between the optimal and logging policies. The learned policy function provides interpretable clinical information and reduces costs without omitting any vital lab orders, outperforming both a physician's policy and prior approaches to this practical problem.", 'abstract_zh': '在重症监护单元（ICU）中选择最少的必要实验室测试项目是一项挑战。护理团队必须在确保获得正确信息和减少与每个实验室测试订单相关的临床负担和成本之间取得平衡。大多数住院环境中常常存在过度进行实验室测试的问题，但现在正努力减轻对医院资源和环境的负担。本文开发了一种结合离策学习和特权信息的新方法，以确定ICU应订购的最佳实验室测试集。我们的方法，EXplainable Off-policy learning with Side Information for ICU blood Test Orders（ExOSITO），创建了一个可解释的辅助工具，供临床医生在考虑每位患者当前状况和预测未来状况的前提下订购实验室测试。我们将这个问题表述为一个使用离线数据和基于临床批准规则的奖励函数训练的因果臂bandit问题；我们提出了一种新的学习框架，将临床知识与观察数据相结合，以弥合最优策略和记录策略之间的差距。学习到的策略函数提供了可解释的临床信息，并在不省略任何关键实验室测试订单的情况下降低成本，优于医生的策略和先前解决此实际问题的方法。', 'title_zh': 'ExOSITO: 基于辅助信息的可解释离策学习在重症监护病房血液检测订单中的应用'}
{'arxiv_id': 'arXiv:2504.17264', 'title': 'JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning', 'authors': 'Zhaolu Kang, Hongtian Cai, Xiangyang Ji, Jinzhe Li, Nanfei Gu', 'link': 'https://arxiv.org/abs/2504.17264', 'abstract': 'In recent years, Unsupervised Domain Adaptation (UDA) has gained significant attention in the field of Natural Language Processing (NLP) owing to its ability to enhance model generalization across diverse domains. However, its application for knowledge transfer between distinct legal domains remains largely unexplored. To address the challenges posed by lengthy and complex legal texts and the limited availability of large-scale annotated datasets, we propose JurisCTC, a novel model designed to improve the accuracy of Legal Judgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC facilitates effective knowledge transfer across various legal domains and employs contrastive learning to distinguish samples from different domains. Specifically, for the LJP task, we enable knowledge transfer between civil and criminal law domains. Compared to other models and specific large language models (LLMs), JurisCTC demonstrates notable advancements, achieving peak accuracies of 76.59% and 78.83%, respectively.', 'abstract_zh': '近年来，无监督领域适应（UDA）在自然语言处理（NLP）领域得到了广泛关注，因其能够提高模型在多样领域间的泛化能力。然而，其在不同法律领域间的知识转移应用仍鲜有探索。为应对长篇复杂法律文本和大规模标注数据稀缺带来的挑战，我们提出了JurisCTC，一种旨在提升法律判决预测（LJP）任务准确性的新型模型。与现有方法不同，JurisCTC能够有效实现跨不同法律领域的知识转移，并采用对比学习方法区分不同领域的样本。具体而言，JurisCTC在民事和刑事法律领域间实现了知识转移。与其它模型及特定大型语言模型（LLMs）相比，JurisCTC展现出显著的进步，分别实现了76.59%和78.83%的峰值准确率。', 'title_zh': 'JurisCTC：通过跨域迁移和对比学习增强法律判决预测'}
{'arxiv_id': 'arXiv:2504.17261', 'title': 'Symbolic Representation for Any-to-Any Generative Tasks', 'authors': 'Jiaqi Chen, Xiaoye Zhu, Yue Wang, Tianyang Liu, Xinhui Chen, Ying Chen, Chak Tou Leong, Yifei Ke, Joseph Liu, Yiwen Yuan, Julian McAuley, Li-jia Li', 'link': 'https://arxiv.org/abs/2504.17261', 'abstract': 'We propose a symbolic generative task description language and a corresponding inference engine capable of representing arbitrary multimodal tasks as structured symbolic flows. Unlike conventional generative models that rely on large-scale training and implicit neural representations to learn cross-modal mappings, often at high computational cost and with limited flexibility, our framework introduces an explicit symbolic representation comprising three core primitives: functions, parameters, and topological logic. Leveraging a pre-trained language model, our inference engine maps natural language instructions directly to symbolic workflows in a training-free manner. Our framework successfully performs over 12 diverse multimodal generative tasks, demonstrating strong performance and flexibility without the need for task-specific tuning. Experiments show that our method not only matches or outperforms existing state-of-the-art unified models in content quality, but also offers greater efficiency, editability, and interruptibility. We believe that symbolic task representations provide a cost-effective and extensible foundation for advancing the capabilities of generative AI.', 'abstract_zh': '我们提出了一种符号生成任务描述语言及相应的推理引擎，能够将任意多模态任务表示为结构化的符号流。与依赖大规模训练和隐式神经表示来学习跨模态映射的传统生成模型不同，我们的框架引入了一种明确的符号表示，包含三个核心原始符号：函数、参数和拓扑逻辑。借助预训练的语言模型，我们的推理引擎无需训练即可直接将自然语言指令映射为符号工作流。我们的框架成功执行了超过12个不同的多模态生成任务，展示了其强大的性能和灵活性，无需特定任务的调优。实验表明，我们的方法不仅在内容质量上能够匹配或超越现有最先进的统一模型，还在效率、可编辑性和可中断性方面更具优势。我们认为，符号任务表示为生成人工智能能力的提升提供了一种成本效益高且可扩展的基础。', 'title_zh': '任何形式到任何形式的生成任务的符号表示'}
{'arxiv_id': 'arXiv:2504.17255', 'title': '3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations', 'authors': 'Shaoyu Pei, Renxiong Wu, Hao Zheng, Lang Qin, Shuaichen Lin, Yuxing Gan, Wenjing Huang, Zhixuan Wang, Mohan Qin, Yong Liu, Guangming Ni', 'link': 'https://arxiv.org/abs/2504.17255', 'abstract': 'Skin, the primary regulator of heat exchange, relies on sweat glands for thermoregulation. Alterations in sweat gland morphology play a crucial role in various pathological conditions and clinical diagnoses. Current methods for observing sweat gland morphology are limited by their two-dimensional, in vitro, and destructive nature, underscoring the urgent need for real-time, non-invasive, quantifiable technologies. We proposed a novel three-dimensional (3D) transformer-based multi-object segmentation framework, integrating a sliding window approach, joint spatial-channel attention mechanism, and architectural heterogeneity between shallow and deep layers. Our proposed network enables precise 3D sweat gland segmentation from skin volume data captured by optical coherence tomography (OCT). For the first time, subtle variations of sweat gland 3D morphology in response to temperature changes, have been visualized and quantified. Our approach establishes a benchmark for normal sweat gland morphology and provides a real-time, non-invasive tool for quantifying 3D structural parameters. This enables the study of individual variability and pathological changes in sweat gland structure, advancing dermatological research and clinical applications, including thermoregulation and bromhidrosis treatment.', 'abstract_zh': '皮肤作为主要的热交换调节器，依赖汗腺进行体温调节。汗腺形态学的变化在多种病理条件和临床诊断中起着关键作用。目前用于观察汗腺形态学的方法受到二维、体外和破坏性的限制，突显出急需实时、无创、可量化的技术。我们提出了一种新颖的三维（3D）变换器基多对象分割框架，结合了滑动窗口方法、联合空间-通道注意力机制以及浅层与深层结构之间的建筑异质性。我们提出的网络能够从光学相干断层扫描（OCT）捕获的皮肤体积数据中进行精确的3D汗腺分割。首次可视化并量化了温度变化对汗腺3D形态学的细微变化。我们的方法建立了正常汗腺形态学的基准，并提供了一种实时、无创工具，用于量化3D结构参数。这促进了个体差异和汗腺结构病理变化的研究，推动了皮肤科研究和临床应用，包括体温调节和多汗症治疗。', 'title_zh': '基于3D深度学习的人体皮肤汗腺分割及其温度变化下的3D形态学响应'}
{'arxiv_id': 'arXiv:2504.17247', 'title': 'Targeted AMP generation through controlled diffusion with efficient embeddings', 'authors': 'Diogo Soares, Leon Hetzel, Paulina Szymczak, Fabian Theis, Stephan Günnemann, Ewa Szczurek', 'link': 'https://arxiv.org/abs/2504.17247', 'abstract': 'Deep learning-based antimicrobial peptide (AMP) discovery faces critical challenges such as low experimental hit rates as well as the need for nuanced controllability and efficient modeling of peptide properties. To address these challenges, we introduce OmegAMP, a framework that leverages a diffusion-based generative model with efficient low-dimensional embeddings, precise controllability mechanisms, and novel classifiers with drastically reduced false positive rates for candidate filtering. OmegAMP enables the targeted generation of AMPs with specific physicochemical properties, activity profiles, and species-specific effectiveness. Moreover, it maximizes sample diversity while ensuring faithfulness to the underlying data distribution during generation. We demonstrate that OmegAMP achieves state-of-the-art performance across all stages of the AMP discovery pipeline, significantly advancing the potential of computational frameworks in combating antimicrobial resistance.', 'abstract_zh': '基于深度学习的抗菌肽（AMP）发现面临关键挑战，包括低实验命中率以及肽特性高效建模和精细可控性的需要。为应对这些挑战，我们引入了OmegAMP框架，该框架利用基于扩散的生成模型、高效的低维嵌入、精确的可控机制以及新型分类器，以大幅降低候选筛选中的假阳性率。OmegAMP能够生成具有特定物理化学性质、活性谱和种属特异性有效性的抗菌肽。此外，它在生成过程中最大化样本多样性，同时保持对底层数据分布的忠实性。我们证明，OmegAMP在AMP发现管道的所有阶段均实现了最先进的性能，显著推进了计算框架在对抗抗生素耐药性方面的潜力。', 'title_zh': '通过可控扩散与高效嵌入的目标AMP生成'}
{'arxiv_id': 'arXiv:2504.17243', 'title': 'NeuralGrok: Accelerate Grokking by Neural Gradient Transformation', 'authors': 'Xinyu Zhou, Simin Fan, Martin Jaggi, Jie Fu', 'link': 'https://arxiv.org/abs/2504.17243', 'abstract': "Grokking is proposed and widely studied as an intricate phenomenon in which generalization is achieved after a long-lasting period of overfitting. In this work, we propose NeuralGrok, a novel gradient-based approach that learns an optimal gradient transformation to accelerate the generalization of transformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary module (e.g., an MLP block) in conjunction with the base model. This module dynamically modulates the influence of individual gradient components based on their contribution to generalization, guided by a bilevel optimization algorithm. Our extensive experiments demonstrate that NeuralGrok significantly accelerates generalization, particularly in challenging arithmetic tasks. We also show that NeuralGrok promotes a more stable training paradigm, constantly reducing the model's complexity, while traditional regularization methods, such as weight decay, can introduce substantial instability and impede generalization. We further investigate the intrinsic model complexity leveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that NeuralGrok effectively facilitates generalization by reducing the model complexity. We offer valuable insights on the grokking phenomenon of Transformer models, which encourages a deeper understanding of the fundamental principles governing generalization ability.", 'abstract_zh': 'Grokking: A Gradient-Based Approach for Accelerating Generalization in Transformers in Arithmetic Tasks', 'title_zh': '神经Grok加速器：基于神经梯度变换的加速方法'}
{'arxiv_id': 'arXiv:2504.17219', 'title': 'Enhancing Variational Autoencoders with Smooth Robust Latent Encoding', 'authors': 'Hyomin Lee, Minseon Kim, Sangwon Jang, Jongheon Jeong, Sung Ju Hwang', 'link': 'https://arxiv.org/abs/2504.17219', 'abstract': 'Variational Autoencoders (VAEs) have played a key role in scaling up diffusion-based generative models, as in Stable Diffusion, yet questions regarding their robustness remain largely underexplored. Although adversarial training has been an established technique for enhancing robustness in predictive models, it has been overlooked for generative models due to concerns about potential fidelity degradation by the nature of trade-offs between performance and robustness. In this work, we challenge this presumption, introducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training framework that boosts both generation quality and robustness. In contrast to conventional adversarial training, which focuses on robustness only, our approach smooths the latent space via adversarial perturbations, promoting more generalizable representations while regularizing with originality representation to sustain original fidelity. Applied as a post-training step on pre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal computational overhead. Experiments show that SRL-VAE improves both generation quality, in image reconstruction and text-guided image editing, and robustness, against Nightshade attacks and image editing attacks. These results establish a new paradigm, showing that adversarial training, once thought to be detrimental to generative models, can instead enhance both fidelity and robustness.', 'abstract_zh': 'Smooth Robust Latent VAE: Enhancing Both Generation Quality and Robustness through Adversarial Training', 'title_zh': '增强变分自编码器的平滑鲁棒潜在编码'}
{'arxiv_id': 'arXiv:2504.17213', 'title': 'MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing', 'authors': 'Shiwen Cao, Zhaoxing Zhang, Junming Jiao, Juyi Qiao, Guowen Song, Rong Shen', 'link': 'https://arxiv.org/abs/2504.17213', 'abstract': "Even in the era of rapid advances in large models, video understanding, particularly long videos, remains highly challenging. Compared with textual or image-based information, videos commonly contain more information with redundancy, requiring large models to strategically allocate attention at a global level for accurate comprehension. To address this, we propose MCAF, an agent-based, training-free framework perform video understanding through Multimodal Coarse-to-fine Attention Focusing. The key innovation lies in its ability to sense and prioritize segments of the video that are highly relevant to the understanding task. First, MCAF hierarchically concentrates on highly relevant frames through multimodal information, enhancing the correlation between the acquired contextual information and the query. Second, it employs a dilated temporal expansion mechanism to mitigate the risk of missing crucial details when extracting information from these concentrated frames. In addition, our framework incorporates a self-reflection mechanism utilizing the confidence level of the model's responses as feedback. By iteratively applying these two creative focusing strategies, it adaptively adjusts attention to capture highly query-connected context and thus improves response accuracy. MCAF outperforms comparable state-of-the-art methods on average. On the EgoSchema dataset, it achieves a remarkable 5% performance gain over the leading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms the current state-of-the-art standard by 0.2% and 0.3% respectively. On the Video-MME dataset, which features videos averaging nearly an hour in length, MCAF also outperforms other agent-based methods.", 'abstract_zh': 'Even in the Era of Rapid Advances in Large Models, Video Understanding, Particularly Long Videos, Remains Highly Challenging: MCAF,一种基于代理的无需训练框架，通过多模态粗细注意力聚焦进行视频理解', 'title_zh': 'MCAF：通过多模态粗细注意力聚焦的高效基于代理的视频理解框架'}
{'arxiv_id': 'arXiv:2504.17210', 'title': 'Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models', 'authors': 'Junfei Wang, Darshana Upadhyay, Marzia Zaman, Pirathayini Srikantha', 'link': 'https://arxiv.org/abs/2504.17210', 'abstract': 'Many data-driven modules in smart grid rely on access to high-quality power flow data; however, real-world data are often limited due to privacy and operational constraints. This paper presents a physics-informed generative framework based on Denoising Diffusion Probabilistic Models (DDPMs) for synthesizing feasible power flow data. By incorporating auxiliary training and physics-informed loss functions, the proposed method ensures that the generated data exhibit both statistical fidelity and adherence to power system feasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark systems, demonstrating its ability to capture key distributional properties and generalize to out-of-distribution scenarios. Comparative results show that the proposed model outperforms three baseline models in terms of feasibility, diversity, and accuracy of statistical features. This work highlights the potential of integrating generative modelling into data-driven power system applications.', 'abstract_zh': '基于去噪扩散概率模型的物理约束生成框架：用于合成可行的电力流动数据', 'title_zh': '基于物理信息去噪扩散概率模型的合成潮流数据生成'}
{'arxiv_id': 'arXiv:2504.17198', 'title': 'Automatically Generating Rules of Malicious Software Packages via Large Language Model', 'authors': 'XiangRui Zhang, HaoYu Chen, Yongzhong He, Wenjia Niu, Qiang Li', 'link': 'https://arxiv.org/abs/2504.17198', 'abstract': "Today's security tools predominantly rely on predefined rules crafted by experts, making them poorly adapted to the emergence of software supply chain attacks. To tackle this limitation, we propose a novel tool, RuleLLM, which leverages large language models (LLMs) to automate rule generation for OSS ecosystems. RuleLLM extracts metadata and code snippets from malware as its input, producing YARA and Semgrep rules that can be directly deployed in software development. Specifically, the rule generation task involves three subtasks: crafting rules, refining rules, and aligning rules. To validate RuleLLM's effectiveness, we implemented a prototype system and conducted experiments on the dataset of 1,633 malicious packages. The results are promising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a precision of 85.2\\% and a recall of 91.8\\%, outperforming state-of-the-art (SOTA) tools and scored-based approaches. We further analyzed generated rules and proposed a rule taxonomy: 11 categories and 38 subcategories.", 'abstract_zh': '当前的安全工具主要依赖于专家制定的预定义规则，这使它们在应对软件供应链攻击时显得不够灵活。为了应对这一局限性，我们提出了一种新型工具RuleLLM，该工具利用大规模语言模型（LLMs）自动为开源软件生态系统生成规则。RuleLLM 以恶意软件的元数据和代码片段作为输入，生成可以直接部署在软件开发中的YARA和Semgrep规则。具体来说，规则生成任务包括三个子任务：制定规则、优化规则和对齐规则。为了验证RuleLLM的有效性，我们实现了一个原型系统，并在包含1,633个恶意包的数据集上进行了实验。结果显示，RuleLLM生成了763条规则（其中452条为YARA规则，311条为Semgrep规则），精确率为85.2%，召回率为91.8%，在性能上超越了当前最先进的工具和基于评分的方法。我们进一步分析了生成的规则，并提出了一个规则分类体系：11个大类和38个子类。', 'title_zh': '通过大型语言模型自动生成恶意软件包的规则'}
{'arxiv_id': 'arXiv:2504.17180', 'title': "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback", 'authors': 'Minkyu Choi, S P Sharan, Harsh Goel, Sahil Shah, Sandeep Chinchali', 'link': 'https://arxiv.org/abs/2504.17180', 'abstract': 'Current text-to-video (T2V) generation models are increasingly popular due to their ability to produce coherent videos from textual prompts. However, these models often struggle to generate semantically and temporally consistent videos when dealing with longer, more complex prompts involving multiple objects or sequential events. Additionally, the high computational costs associated with training or fine-tuning make direct improvements impractical. To overcome these limitations, we introduce \\(\\projectname\\), a novel zero-training video refinement pipeline that leverages neuro-symbolic feedback to automatically enhance video generation, achieving superior alignment with the prompts. Our approach first derives the neuro-symbolic feedback by analyzing a formal video representation and pinpoints semantically inconsistent events, objects, and their corresponding frames. This feedback then guides targeted edits to the original video. Extensive empirical evaluations on both open-source and proprietary T2V models demonstrate that \\(\\projectname\\) significantly enhances temporal and logical alignment across diverse prompts by almost $40\\%$.', 'abstract_zh': '当前基于文本到视频（T2V）生成模型因其能够从文本提示中生成连贯的视频而日益流行。然而，这些模型在处理较长、更为复杂的包含多个对象或顺序事件的提示时，往往难以生成语义和时间上一致的视频。此外，与训练或微调相关的高昂计算成本使得直接改进变得不切实际。为克服这些限制，我们引入了\\(\\projectname\\)，一种新颖的无需训练的视频精炼管道，该管道利用神经符号反馈自动增强视频生成，实现了与提示的更佳对齐。我们的方法通过分析正式的视频表示来提取神经符号反馈，并确定语义不一致的事件、对象及其相应的帧。这种反馈then指导对原始视频的目标编辑。在对开源和专有T2V模型进行广泛实证评估中，\\(\\projectname\\)在多样化的提示下显著提高了时间上和逻辑上的对齐程度，增幅接近40%。', 'title_zh': '我们在后期修复它：通过神经符号反馈改进文本到视频生成'}
{'arxiv_id': 'arXiv:2504.17170', 'title': 'Improving Human-Autonomous Vehicle Interaction in Complex Systems', 'authors': 'Robert Kaufman', 'link': 'https://arxiv.org/abs/2504.17170', 'abstract': 'Unresolved questions about how autonomous vehicles (AVs) should meet the informational needs of riders hinder real-world adoption. Complicating our ability to satisfy rider needs is that different people, goals, and driving contexts have different criteria for what constitutes interaction success. Unfortunately, most human-AV research and design today treats all people and situations uniformly. It is crucial to understand how an AV should communicate to meet rider needs, and how communications should change when the human-AV complex system changes. I argue that understanding the relationships between different aspects of the human-AV system can help us build improved and adaptable AV communications. I support this argument using three empirical studies. First, I identify optimal communication strategies that enhance driving performance, confidence, and trust for learning in extreme driving environments. Findings highlight the need for task-sensitive, modality-appropriate communications tuned to learner cognitive limits and goals. Next, I highlight the consequences of deploying faulty communication systems and demonstrate the need for context-sensitive communications. Third, I use machine learning (ML) to illuminate personal factors predicting trust in AVs, emphasizing the importance of tailoring designs to individual traits and concerns. Together, this dissertation supports the necessity of transparent, adaptable, and personalized AV systems that cater to individual needs, goals, and contextual demands. By considering the complex system within which human-AV interactions occur, we can deliver valuable insights for designers, researchers, and policymakers. This dissertation also provides a concrete domain to study theories of human-machine joint action and situational awareness, and can be used to guide future human-AI interaction research. [shortened for arxiv]', 'abstract_zh': '关于自动驾驶车辆（AVs）如何满足乘客信息需求的问题尚待解决，阻碍了其在现实生活中的应用。复杂的乘客需求使得我们难以满足他们的需求，因为不同的人、不同的目标和驾驶情境有不同的交互成功标准。不幸的是，目前大多数人类-AV研究和设计都对所有的人和情况进行同等对待。了解自动驾驶车辆如何沟通以满足乘客需求，以及在人类-AV复杂系统发生变化时沟通方式应如何改变是至关重要的。我认为，理解人类-AV系统不同方面之间的关系有助于我们构建改进和适应性强的自动驾驶车辆通信系统。我通过三项实证研究支持这一论点。首先，我确定了能够提高极端驾驶环境中的驾驶性能、信心和信任度的最优沟通策略。研究结果强调了任务敏感性、模态恰当性沟通的重要性，以及这些沟通应适应学习者的认知限制和目标。其次，我指出了部署有缺陷的通信系统的后果，并展示了需要场景敏感性沟通的必要性。第三，我使用机器学习来阐明影响对自动驾驶车辆信任度的个人因素，强调了根据个人特质和关切进行定制设计的重要性。综上所述，本论文支持有必要构建透明、适应性强且个性化的自动驾驶车辆系统，以满足个人需求、目标和情境需求。通过考虑人类-AV交互发生的方式，我们为设计师、研究人员和政策制定者提供了有价值的见解。本论文还提供了一个具体的领域，用于研究人机联合行动和情境意识理论，并可用于指导未来的人机交互研究。', 'title_zh': '改进复杂系统中的人-自主车辆交互'}
{'arxiv_id': 'arXiv:2504.17162', 'title': 'A Comprehensive Review on RNA Subcellular Localization Prediction', 'authors': 'Cece Zhang, Xuehuan Zhu, Nick Peterson, Jieqiong Wang, Shibiao Wan', 'link': 'https://arxiv.org/abs/2504.17162', 'abstract': 'The subcellular localization of RNAs, including long non-coding RNAs (lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs, plays a critical role in determining their biological functions. For instance, lncRNAs are predominantly associated with chromatin and act as regulators of gene transcription and chromatin structure, while mRNAs are distributed across the nucleus and cytoplasm, facilitating the transport of genetic information for protein synthesis. Understanding RNA localization sheds light on processes like gene expression regulation with spatial and temporal precision. However, traditional wet lab methods for determining RNA localization, such as in situ hybridization, are often time-consuming, resource-demanding, and costly. To overcome these challenges, computational methods leveraging artificial intelligence (AI) and machine learning (ML) have emerged as powerful alternatives, enabling large-scale prediction of RNA subcellular localization. This paper provides a comprehensive review of the latest advancements in AI-based approaches for RNA subcellular localization prediction, covering various RNA types and focusing on sequence-based, image-based, and hybrid methodologies that combine both data types. We highlight the potential of these methods to accelerate RNA research, uncover molecular pathways, and guide targeted disease treatments. Furthermore, we critically discuss the challenges in AI/ML approaches for RNA subcellular localization, such as data scarcity and lack of benchmarks, and opportunities to address them. This review aims to serve as a valuable resource for researchers seeking to develop innovative solutions in the field of RNA subcellular localization and beyond.', 'abstract_zh': 'RNA亚细胞定位，包括长非编码RNA（lncRNA）、信使RNA（mRNA）、微小RNA（miRNA）和其他较小的RNA，对其生物学功能起着关键作用。例如，lncRNA主要与染色质相关，并作为基因转录和染色质结构的调节器，而mRNA分布于细胞核和细胞质中，促进遗传信息的运输以进行蛋白质合成。理解RNA定位揭示了以空间和时间精度调控基因表达的过程。然而，传统湿实验方法，如原位杂交，确定RNA定位往往耗时、资源密集且成本高。为了克服这些挑战，利用人工智能（AI）和机器学习（ML）的计算方法已经成为了强大的替代方案，能够实现大规模的RNA亚细胞定位预测。本文提供了一种全面回顾基于AI的方法在RNA亚细胞定位预测中的最新进展，涵盖了各种RNA类型，并着重介绍了基于序列、基于图像以及结合两种数据类型的混合方法。我们强调了这些方法在加速RNA研究、揭示分子途径以及指导靶向疾病治疗方面的潜力。此外，我们还批判性地讨论了RNA亚细胞定位中AI/ML方法面临的挑战，如数据稀缺性及缺乏基准，并探讨了应对这些挑战的机会。本文旨在为研究人员开发RNA亚细胞定位及相关领域的创新解决方案提供有价值的资源。', 'title_zh': 'RNA亚细胞定位预测的综述'}
{'arxiv_id': 'arXiv:2504.17160', 'title': 'OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection', 'authors': 'Alberto Fernández-Hernández, Jose I. Mestre, Manuel F. Dolz, Jose Duato, Enrique S. Quintana-Ortí', 'link': 'https://arxiv.org/abs/2504.17160', 'abstract': 'We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for monitoring the training dynamics of Deep Neural Networks (DNNs) and identifying optimal regularization hyperparameters. Specifically, we validate that OUI can effectively guide the selection of the Weight Decay (WD) hyperparameter by indicating whether a model is overfitting or underfitting during training without requiring validation data. Through experiments on DenseNet-BC-100 with CIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K, we show that maintaining OUI within a prescribed interval correlates strongly with improved generalization and validation scores. Notably, OUI converges significantly faster than traditional metrics such as loss or accuracy, enabling practitioners to identify optimal WD (hyperparameter) values within the early stages of training. By leveraging OUI as a reliable indicator, we can determine early in training whether the chosen WD value leads the model to underfit the training data, overfit, or strike a well-balanced trade-off that maximizes validation scores. This enables more precise WD tuning for optimal performance on the tested datasets and DNNs. All code for reproducing these experiments is available at this https URL.', 'abstract_zh': '我们引入了过度拟合-欠拟合指示器（OUI），这是一种用于监控深度神经网络（DNNs）训练动力学并识别最优正则化超参数的新型工具。具体来说，我们验证了OUI能够在无需验证数据的情况下，有效指导Weight Decay（WD）超参数的选择，通过指示模型在训练过程中是过度拟合还是欠拟合来实现。通过在DenseNet-BC-100与CIFAR-100、EfficientNet-B0与TinyImageNet以及ResNet-34与ImageNet-1K上的实验，我们展示了保持OUI在指定区间内与改善泛化能力和验证分数密切相关。值得注意的是，OUI的收敛速度明显快于损失或准确率等传统指标，使得实践者能够在训练的早期阶段确定最优的WD（超参数）值。通过利用OUI作为可靠的指示器，可以在训练早期确定所选WD值是否导致模型过度拟合、欠拟合或达到了在验证分数上最大化平衡的最优贸易off。这使得我们可以在测试的数据集和DNNs上实现更精确的WD调优，以获得最佳性能。所有这些实验的可复现代码均可在以下网址获得。', 'title_zh': '需要讨论权重衰减：过拟合检测的新视角'}
{'arxiv_id': 'arXiv:2504.17140', 'title': 'Scalable Permutation-Aware Modeling for Temporal Set Prediction', 'authors': 'Ashish Ranjan, Ayush Agarwal, Shalin Barot, Sushant Kumar', 'link': 'https://arxiv.org/abs/2504.17140', 'abstract': 'Temporal set prediction involves forecasting the elements that will appear in the next set, given a sequence of prior sets, each containing a variable number of elements. Existing methods often rely on intricate architectures with substantial computational overhead, which hampers their scalability. In this work, we introduce a novel and scalable framework that leverages permutation-equivariant and permutation-invariant transformations to efficiently model set dynamics. Our approach significantly reduces both training and inference time while maintaining competitive performance. Extensive experiments on multiple public benchmarks show that our method achieves results on par with or superior to state-of-the-art models across several evaluation metrics. These results underscore the effectiveness of our model in enabling efficient and scalable temporal set prediction.', 'abstract_zh': '基于时间的集合预测涉及给定一系列包含不同数量元素的先前集合时，预测将出现的下一个集合中的元素。现有方法往往依赖于复杂的架构，导致显著的计算开销，限制了其可扩展性。在这项工作中，我们引入了一个新颖且可扩展的框架，利用置换等变和置换不变变换高效建模集合动力学。我们的方法在显著减少训练和推理时间的同时，保持了竞争力的性能。在多个公开基准上的广泛实验表明，我们的方法在多个评估指标上实现了与最先进模型相当或更优的结果。这些结果突显了我们的模型在实现高效和可扩展的时间集合预测方面的有效性。', 'title_zh': '可扩展的基于排列感知的 temporal set 预测模型探し'}
{'arxiv_id': 'arXiv:2504.17137', 'title': 'MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation', 'authors': 'Chanhee Park, Hyeonseok Moon, Chanjun Park, Heuiseok Lim', 'link': 'https://arxiv.org/abs/2504.17137', 'abstract': 'Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings\\footnote{The MIRAGE code and data are available at this https URL.', 'abstract_zh': '检索增强生成（RAG）作为一种通过融入外部知识来提升大型语言模型（LLMs）生成能力的有效方法已经受到了广泛关注。然而，由于检索和生成组件之间的复杂交互，RAG系统的评估仍然具有挑战性。这一局限性导致了详细的、组件特定的评估基准的缺乏。在此工作中，我们提出了MIRAGE，一个专门用于RAG评估的问答数据集。MIRAGE包含7,560个精心策划的实例，映射到一个包含37,800条条目的检索池中，使检索和生成任务的高效和精确评估成为可能。我们还引入了新的评估指标，旨在衡量RAG的适应性，包括噪声脆弱性、上下文接受性、上下文无关性和上下文误读等多个维度。通过各种检索-LLM配置下的全面实验，我们提供了模型对齐和RAG系统内部微妙动态的新见解。数据集和评估代码已公开，可在各种研究环境中轻松集成和定制。', 'title_zh': 'MIRAGE：一个基于度量的标准，用于检索增强生成评估'}
{'arxiv_id': 'arXiv:2504.17129', 'title': 'Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference', 'authors': 'Seyed Yousef Soltanian, Wenlong Zhang', 'link': 'https://arxiv.org/abs/2504.17129', 'abstract': "Human-robot interactions can be modeled as incomplete-information general-sum dynamic games since the objective functions of both agents are not explicitly known to each other. However, solving for equilibrium policies for such games presents a major challenge, especially if the games involve nonlinear underlying dynamics. To simplify the problem, existing work often assumes that one agent is an expert with complete information about its peer, which can lead to biased estimates and failures in coordination. To address this challenge, we propose a nonlinear peer-aware cost estimation (N-PACE) algorithm for general-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ) approximation of the nonlinear general-sum game, each agent explicitly models the learning dynamics of its peer agent while inferring their objective functions, leading to unbiased fast learning in inferring the unknown objective function of the peer agent, which is critical for task completion and safety assurance. Additionally, we demonstrate how N-PACE enables \\textbf{intent communication} in such multi-agent systems by explicitly modeling the peer's learning dynamics.", 'abstract_zh': '人类-机器人交互可以建模为不完全信息广义收益动态游戏，因为两个代理的目标函数彼此未知。然而，求解此类游戏的均衡策略尤其具有挑战性，特别是在涉及非线性底层动力学的情况下。为简化问题，现有工作常常假设其中一个代理是专家并且完全了解其同伴，这可能导致偏差估计和协调失败。为解决这一挑战，我们提出了一种非线性同伴感知成本估计（N-PACE）算法用于广义收益动态游戏。在N-PACE中，通过迭代线性二次（LQ）逼近非线性广义收益游戏，每个代理明确建模其同伴代理的学习动态并推断其目标函数，从而实现对同伴代理未知目标函数的无偏快速推断，这对于任务完成和安全保证至关重要。此外，我们展示了N-PACE如何通过明确建模同伴代理的学习动态来促进多代理系统中的意图通信。', 'title_zh': '基于同伴感知的成本估计在非线性总体非零和动态博弈中的互学习与意图推断'}
{'arxiv_id': 'arXiv:2504.17124', 'title': 'Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy', 'authors': 'Ming Du, Mark Wolfman, Chengjun Sun, Shelly D. Kelly, Mathew J. Cherukara', 'link': 'https://arxiv.org/abs/2504.17124', 'abstract': "X-ray absorption near edge structure (XANES) spectroscopy is a powerful technique for characterizing the chemical state and symmetry of individual elements within materials, but requires collecting data at many energy points which can be time-consuming. While adaptive sampling methods exist for efficiently collecting spectroscopic data, they often lack domain-specific knowledge about XANES spectra structure. Here we demonstrate a knowledge-injected Bayesian optimization approach for adaptive XANES data collection that incorporates understanding of spectral features like absorption edges and pre-edge peaks. We show this method accurately reconstructs the absorption edge of XANES spectra using only 15-20% of the measurement points typically needed for conventional sampling, while maintaining the ability to determine the x-ray energy of the sharp peak after absorption edge with errors less than 0.03 eV, the absorption edge with errors less than 0.1 eV; and overall root-mean-square errors less than 0.005 compared to compared to traditionally sampled spectra. Our experiments on battery materials and catalysts demonstrate the method's effectiveness for both static and dynamic XANES measurements, improving data collection efficiency and enabling better time resolution for tracking chemical changes. This approach advances the degree of automation in XANES experiments reducing the common errors of under- or over-sampling points in near the absorption edge and enabling dynamic experiments that require high temporal resolution or limited measurement time.", 'abstract_zh': '基于知识注入的贝叶斯优化方法在自适应XANES数据采集中的应用：电池材料和催化剂中的静态和动态测量', 'title_zh': '基于AI驱动的工作流在动态X射线光谱学中的应用示范'}
{'arxiv_id': 'arXiv:2504.17122', 'title': 'Physiological neural representation for personalised tracer kinetic parameter estimation from dynamic PET', 'authors': 'Kartikay Tehlan, Thomas Wendler', 'link': 'https://arxiv.org/abs/2504.17122', 'abstract': 'Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables non-invasive quantification of glucose metabolism through kinetic analysis, often modelled by the two-tissue compartment model (TCKM). However, voxel-wise kinetic parameter estimation using conventional methods is computationally intensive and limited by spatial resolution. Deep neural networks (DNNs) offer an alternative but require large training datasets and significant computational resources. To address these limitations, we propose a physiological neural representation based on implicit neural representations (INRs) for personalized kinetic parameter estimation. INRs, which learn continuous functions, allow for efficient, high-resolution parametric imaging with reduced data requirements. Our method also integrates anatomical priors from a 3D CT foundation model to enhance robustness and precision in kinetic modelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET/CT dataset and compare it to state-of-the-art DNNs. Results demonstrate superior spatial resolution, lower mean-squared error, and improved anatomical consistency, particularly in tumour and highly vascularized regions. Our findings highlight the potential of INRs for personalized, data-efficient tracer kinetic modelling, enabling applications in tumour characterization, segmentation, and prognostic assessment.', 'abstract_zh': '基于隐神经表示的生理神经表示在 [$^{18}$F]FDG 动态 PET/CT 中的个性化动力学参数估计', 'title_zh': '个性化的示踪剂动力学参数估计的生理神经表示方法'}
{'arxiv_id': 'arXiv:2504.17119', 'title': 'The Rise of Small Language Models in Healthcare: A Comprehensive Survey', 'authors': 'Muskan Garg, Shaina Raza, Shebuti Rayana, Xingyi Liu, Sunghwan Sohn', 'link': 'https://arxiv.org/abs/2504.17119', 'abstract': 'Despite substantial progress in healthcare applications driven by large language models (LLMs), growing concerns around data privacy, and limited resources; the small language models (SLMs) offer a scalable and clinically viable solution for efficient performance in resource-constrained environments for next-generation healthcare informatics. Our comprehensive survey presents a taxonomic framework to identify and categorize them for healthcare professionals and informaticians. The timeline of healthcare SLM contributions establishes a foundational framework for analyzing models across three dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present a taxonomic framework to identify the architectural foundations for building models from scratch; adapting SLMs to clinical precision through prompting, instruction fine-tuning, and reasoning; and accessibility and sustainability through compression techniques. Our primary objective is to offer a comprehensive survey for healthcare professionals, introducing recent innovations in model optimization and equipping them with curated resources to support future research and development in the field. Aiming to showcase the groundbreaking advancements in SLMs for healthcare, we present a comprehensive compilation of experimental results across widely studied NLP tasks in healthcare to highlight the transformative potential of SLMs in healthcare. The updated repository is available at Github', 'abstract_zh': '尽管大型语言模型（LLMs）在医疗保健应用方面取得了显著进展，但随着对数据隐私的关注不断增加和资源有限；小型语言模型（SLMs）提供了在资源受限环境中实现高效性能的可扩展且临床实用的解决方案，适用于下一代医疗informatics。我们的综合综述提出了一种分类框架，以识别和分类SLMs，供医疗专业人员和informatics专家使用。医疗保健SLMs的历史时间轴建立了一个基础框架，用于从三个维度分析模型：NLP任务、利益相关者角色和照护连续性。我们提出了一种分类框架，以识别构建模型的基础架构；通过提示、指令微调和推理将SLMs适应临床精度；并通过压缩技术实现可访问性和可持续性。我们的主要目标是为医疗保健专业人员提供一个全面的综述，介绍模型优化的最新创新，并为他们提供经过筛选的资源，以支持未来该领域的研究和开发。旨在展示医疗保健领域小型语言模型（SLMs）的突破性进步，我们提供了一个广泛的自然语言处理（NLP）任务实验结果的综合汇总，以突出SLMs在医疗保健领域的变革潜力。更新后的仓库在Github上可用。', 'title_zh': '小语言模型在医疗健康领域的兴起：一项综合调研'}
{'arxiv_id': 'arXiv:2504.17114', 'title': 'Anatomy-constrained modelling of image-derived input functions in dynamic PET using multi-organ segmentation', 'authors': 'Valentin Langer, Kartikay Tehlan, Thomas Wendler', 'link': 'https://arxiv.org/abs/2504.17114', 'abstract': 'Accurate kinetic analysis of [$^{18}$F]FDG distribution in dynamic positron emission tomography (PET) requires anatomically constrained modelling of image-derived input functions (IDIFs). Traditionally, IDIFs are obtained from the aorta, neglecting anatomical variations and complex vascular contributions. This study proposes a multi-organ segmentation-based approach that integrates IDIFs from the aorta, portal vein, pulmonary artery, and ureters. Using high-resolution CT segmentations of the liver, lungs, kidneys, and bladder, we incorporate organ-specific blood supply sources to improve kinetic modelling. Our method was evaluated on dynamic [$^{18}$F]FDG PET data from nine patients, resulting in a mean squared error (MSE) reduction of $13.39\\%$ for the liver and $10.42\\%$ for the lungs. These initial results highlight the potential of multiple IDIFs in improving anatomical modelling and fully leveraging dynamic PET imaging. This approach could facilitate the integration of tracer kinetic modelling into clinical routine.', 'abstract_zh': '多器官分割基于的方法在动态正电子发射断层扫描(PET)中准确分析[$^{18}$F]FDG分布需要解剖约束的图像导引输入函数(IDIFs)建模。', 'title_zh': '基于多器官分割的解剖约束动态PET图像衍生输入函数建模'}
{'arxiv_id': 'arXiv:2504.17077', 'title': 'Physics-guided and fabrication-aware inverse design of photonic devices using diffusion models', 'authors': 'Dongjin Seo, Soobin Um, Sangbin Lee, Jong Chul Ye, Haejun Chung', 'link': 'https://arxiv.org/abs/2504.17077', 'abstract': 'Designing free-form photonic devices is fundamentally challenging due to the vast number of possible geometries and the complex requirements of fabrication constraints. Traditional inverse-design approaches--whether driven by human intuition, global optimization, or adjoint-based gradient methods--often involve intricate binarization and filtering steps, while recent deep learning strategies demand prohibitively large numbers of simulations (10^5 to 10^6). To overcome these limitations, we present AdjointDiffusion, a physics-guided framework that integrates adjoint sensitivity gradients into the sampling process of diffusion models. AdjointDiffusion begins by training a diffusion network on a synthetic, fabrication-aware dataset of binary masks. During inference, we compute the adjoint gradient of a candidate structure and inject this physics-based guidance at each denoising step, steering the generative process toward high figure-of-merit (FoM) solutions without additional post-processing. We demonstrate our method on two canonical photonic design problems--a bent waveguide and a CMOS image sensor color router--and show that our method consistently outperforms state-of-the-art nonlinear optimizers (such as MMA and SLSQP) in both efficiency and manufacturability, while using orders of magnitude fewer simulations (approximately 2 x 10^2) than pure deep learning approaches (approximately 10^5 to 10^6). By eliminating complex binarization schedules and minimizing simulation overhead, AdjointDiffusion offers a streamlined, simulation-efficient, and fabrication-aware pipeline for next-generation photonic device design. Our open-source implementation is available at this https URL.', 'abstract_zh': '基于伴随敏感梯度的物理引导扩散模型框架：AdjointDiffusion在自由形光子器件设计中的应用', 'title_zh': '基于物理指导和制造感知的光子器件逆向设计方法——利用扩散模型'}
{'arxiv_id': 'arXiv:2504.17070', 'title': 'Robo-Troj: Attacking LLM-based Task Planners', 'authors': 'Mohaiminul Al Nahian, Zainab Altaweel, David Reitano, Sabbir Ahmed, Saumitra Lohokare, Shiqi Zhang, Adnan Siraj Rakin', 'link': 'https://arxiv.org/abs/2504.17070', 'abstract': 'Robots need task planning methods to achieve goals that require more than individual actions. Recently, large language models (LLMs) have demonstrated impressive performance in task planning. LLMs can generate a step-by-step solution using a description of actions and the goal. Despite the successes in LLM-based task planning, there is limited research studying the security aspects of those systems. In this paper, we develop Robo-Troj, the first multi-trigger backdoor attack for LLM-based task planners, which is the main contribution of this work. As a multi-trigger attack, Robo-Troj is trained to accommodate the diversity of robot application domains. For instance, one can use unique trigger words, e.g., "herical", to activate a specific malicious behavior, e.g., cutting hand on a kitchen robot. In addition, we develop an optimization method for selecting the trigger words that are most effective. Through demonstrating the vulnerability of LLM-based planners, we aim to promote the development of secured robot systems.', 'abstract_zh': '机器人需要任务规划方法来实现需要多于单个动作的目标。最近，大规模语言模型（LLMs）在任务规划方面展现了令人印象深刻的表现。LLMs可以根据动作描述和目标生成逐步解决方案。尽管基于LLM的任务规划取得了成功，但对这些系统的安全方面研究有限。在本文中，我们开发了Robo-Troj，这是针对基于LLM的任务规划器的第一个多触发后门攻击，这是本文的主要贡献。作为一种多触发攻击，Robo-Troj经过训练以适应机器人类应用领域的多样性。例如，可以使用唯一的触发词“herical”来激活特定的恶意行为，如在厨房机器人上切断手。此外，我们开发了一种优化方法来选择最有效的触发词。通过展示基于LLM的规划器的脆弱性，我们旨在促进安全的机器人系统的发展。', 'title_zh': 'Robo-Troj: 攻击基于LLM的任务规划器'}
{'arxiv_id': 'arXiv:2504.17069', 'title': 'Distilling semantically aware orders for autoregressive image generation', 'authors': 'Rishav Pramanik, Antoine Poupon, Juan A. Rodriguez, Masih Aminbeidokhti, David Vazquez, Christopher Pal, Zhaozheng Yin, Marco Pedersoli', 'link': 'https://arxiv.org/abs/2504.17069', 'abstract': 'Autoregressive patch-based image generation has recently shown competitive results in terms of image quality and scalability. It can also be easily integrated and scaled within Vision-Language models. Nevertheless, autoregressive models require a defined order for patch generation. While a natural order based on the dictation of the words makes sense for text generation, there is no inherent generation order that exists for image generation. Traditionally, a raster-scan order (from top-left to bottom-right) guides autoregressive image generation models. In this paper, we argue that this order is suboptimal, as it fails to respect the causality of the image content: for instance, when conditioned on a visual description of a sunset, an autoregressive model may generate clouds before the sun, even though the color of clouds should depend on the color of the sun and not the inverse. In this work, we show that first by training a model to generate patches in any-given-order, we can infer both the content and the location (order) of each patch during generation. Secondly, we use these extracted orders to finetune the any-given-order model to produce better-quality images. Through our experiments, we show on two datasets that this new generation method produces better images than the traditional raster-scan approach, with similar training costs and no extra annotations.', 'abstract_zh': '基于自回归的片(patch)-级图像生成：任意顺序生成的方法及应用', 'title_zh': '语义意识的顺序提炼用于自回归图像生成'}
{'arxiv_id': 'arXiv:2504.17058', 'title': 'Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation', 'authors': 'Rahul Vishwakarma', 'link': 'https://arxiv.org/abs/2504.17058', 'abstract': 'The generation of high-quality synthetic data presents significant challenges in machine learning research, particularly regarding statistical fidelity and uncertainty quantification. Existing generative models produce compelling synthetic samples but lack rigorous statistical guarantees about their relation to the underlying data distribution, limiting their applicability in critical domains requiring robust error bounds. We address this fundamental limitation by presenting a novel framework that incorporates conformal prediction methodologies into Generative Adversarial Networks (GANs). By integrating multiple conformal prediction paradigms including Inductive Conformal Prediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction, and Venn-Abers Predictors, we establish distribution-free uncertainty quantification in generated samples. This approach, termed Conformalized GAN (cGAN), demonstrates enhanced calibration properties while maintaining the generative power of traditional GANs, producing synthetic data with provable statistical guarantees. We provide rigorous mathematical proofs establishing finite-sample validity guarantees and asymptotic efficiency properties, enabling the reliable application of synthetic data in high-stakes domains including healthcare, finance, and autonomous systems.', 'abstract_zh': '高質量合成數據生成在機器學習研究中提出了重大挑戰，特別是統計保真度和不確定性量化方面。現有的生成模型能夠產生 Stunning 合成樣本，但缺乏對其與底层數據分布之间关系的严格统计保证，限制了其在需要稳健误差界的关键领域的应用。我们通过提出一个新颖框架，将符合推断方法整合到生成对抗网络（GANs）中来克服这一根本限制。通过集成多种符合推断范式，包括归纳符合推断（ICP）、Mondrian 符合推断、跨符合推断和 Venn-Abers 预测器，我们建立了生成样本中的分布无关不確定性量化。这种方法称为符合化 GAN（cGAN），在保持传统 GAN 生成能力的同时，展示了增强的校准特性，并为合成数据提供了可证明的统计保证。我们提供了严格的数学证明，建立了有限样本有效性保证和渐近高效性能，使合成数据可以在医疗保健、金融和自主系统等高风险领域得到可靠应用。', 'title_zh': '通过符合性对抗生成在合成数据中的统计保证'}
{'arxiv_id': 'arXiv:2504.17055', 'title': 'Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement', 'authors': 'Ayushi Agrawal, Aditya Kondai, Kavita Vemuri', 'link': 'https://arxiv.org/abs/2504.17055', 'abstract': 'AI-powered facial assessment tools are reshaping how individuals evaluate appearance and internalize social judgments. This study examines the psychological impact of such tools on self-objectification, self-esteem, and emotional responses, with attention to gender differences. Two samples used distinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9 years), and another more neutral (N=51; M=19.9 years). Participants completed validated self-objectification and self-esteem scales and custom items measuring emotion, digital/physical appearance enhancement (DAE, PAEE), and perceived social emotion (PSE). Results revealed consistent links between high self-objectification, low self-esteem, and increased appearance enhancement behaviors across both versions. Despite softer framing, the newer tool still evoked negative emotional responses (U=1466.5, p=0.013), indicating implicit feedback may reinforce appearance-related insecurities. Gender differences emerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital enhancement and less likely to perceive emotional impact in others. These findings reveal how AI tools may unintentionally reinforce and amplify existing social biases and underscore the critical need for responsible AI design and development. Future research will investigate how human ideologies embedded in the training data of such tools shape their evaluative outputs, and how these, in turn, influence user attitudes and decisions.', 'abstract_zh': '基于AI的面部评估工具正在重塑个人对容貌和社会评价的评估方式。本研究探讨了这些工具对自我物化、自尊和情感反应的心理影响，并关注性别差异。两份样本使用了不同版本的面部分析工具：一个版本明显批判性较强（N=75；平均年龄22.9岁），另一个较为中立（N=51；平均年龄19.9岁）。参与者完成了自物化和自尊的验证量表以及衡量情感、数字/物理外貌增强（DAE, PAEE）和感知社会情感（PSE）的自定义题目。结果表明，高自物化、低自尊和增加的外貌增强行为在这两个版本中存在一致联系。尽管采用软性措辞，新工具仍引发了负面情感反应（U=1466.5，p=0.013），表明隐性反馈可能强化与外貌相关的情绪不安。性别差异在数字外貌增强（p=0.025）和感知社会情感（p<0.001）中显现，女性更倾向于进行数字外貌增强，并且不太可能感知他人的情感影响。这些发现揭示了AI工具可能无意中强化和放大现有社会偏见的方式，并强调了负责任的AI设计与开发的迫切需求。未来研究将探讨嵌入这些工具训练数据中的人类意识形态如何塑造其评价输出，以及这些评价输出如何反过来影响用户的观念和决策。', 'title_zh': 'AI驱动营销工具对美容/面部特征增强的心理影响'}
{'arxiv_id': 'arXiv:2504.17044', 'title': 'Approaches to Responsible Governance of GenAI in Organizations', 'authors': 'Dhari Gandhi, Himanshu Joshi, Lucas Hartman, Shabnam Hassani', 'link': 'https://arxiv.org/abs/2504.17044', 'abstract': 'The rapid evolution of Generative AI (GenAI) has introduced unprecedented opportunities while presenting complex challenges around ethics, accountability, and societal impact. This paper draws on a literature review, established governance frameworks, and industry roundtable discussions to identify core principles for integrating responsible GenAI governance into diverse organizational structures. Our objective is to provide actionable recommendations for a balanced, risk-based governance approach that enables both innovation and oversight. Findings emphasize the need for adaptable risk assessment tools, continuous monitoring practices, and cross-sector collaboration to establish trustworthy GenAI. These insights provide a structured foundation and Responsible GenAI Guide (ResAI) for organizations to align GenAI initiatives with ethical, legal, and operational best practices.', 'abstract_zh': '生成式人工智能的快速演进带来了前所未有的机遇，同时也带来了复杂的风险治理、问责制和社会影响挑战。本文通过文献综述、现成的治理框架和行业圆桌讨论，识别出将负责任的生成式人工智能治理整合到多样化组织结构中的核心原则。我们的目标是提供一种平衡的、基于风险的治理方法的具体建议，既能促进创新又能进行有效监管。研究发现强调了灵活的风险评估工具、持续监测实践以及跨部门合作的必要性，以建立可信赖的生成式人工智能。这些见解为组织提供了一个结构化的基础和负责任的生成式人工智能指南（ResAI），以使生成式人工智能项目与伦理、法律和操作最佳实践相一致。', 'title_zh': '组织中负责任的GenAI治理 approach'}
{'arxiv_id': 'arXiv:2504.17040', 'title': 'DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs', 'authors': 'Zhenhailong Wang, Senthil Purushwalkam, Caiming Xiong, Silvio Savarese, Heng Ji, Ran Xu', 'link': 'https://arxiv.org/abs/2504.17040', 'abstract': 'We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embeddings by merging similar tokens based on image complexity, addressing the inherent inefficiency of fixed-length outputs in vision transformers. Second, Virtual Token Unmerging (VTU) simulates the expected token sequence for large language models (LLMs) by efficiently reconstructing the attention dynamics of a full sequence, thus preserving the downstream performance without additional fine-tuning. Unlike previous approaches, our method dynamically adapts token compression to the content of the image and operates completely training-free, making it readily applicable to most state-of-the-art VLM architectures. Extensive experiments on image and video understanding tasks demonstrate that DyMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models across diverse VLM architectures, including the recently popularized AnyRes-based visual encoders. Furthermore, through qualitative analyses, we demonstrate that DToMe effectively adapts token reduction based on image complexity and, unlike existing systems, provides users more control over computational costs. Project page: this https URL.', 'abstract_zh': '我们提出了DyMU，一种高效且无需训练的框架，可在保持高任务性能的同时动态减少视觉语言模型（VLMs）的计算负担。', 'title_zh': 'DyMU: 动态合并与虚拟拆分以实现高效的大模型'}
{'arxiv_id': 'arXiv:2504.17029', 'title': 'Fried Parameter Estimation from Single Wavefront Sensor Image with Artificial Neural Networks', 'authors': 'Jeffrey Smith, Taisei Fujii, Jesse Craney, Charles Gretton', 'link': 'https://arxiv.org/abs/2504.17029', 'abstract': 'Atmospheric turbulence degrades the quality of astronomical observations in ground-based telescopes, leading to distorted and blurry images. Adaptive Optics (AO) systems are designed to counteract these effects, using atmospheric measurements captured by a wavefront sensor to make real-time corrections to the incoming wavefront. The Fried parameter, r0, characterises the strength of atmospheric turbulence and is an essential control parameter for optimising the performance of AO systems and more recently sky profiling for Free Space Optical (FSO) communication channels. In this paper, we develop a novel data-driven approach, adapting machine learning methods from computer vision for Fried parameter estimation from a single Shack-Hartmann or pyramid wavefront sensor image. Using these data-driven methods, we present a detailed simulation-based evaluation of our approach using the open-source COMPASS AO simulation tool to evaluate both the Shack-Hartmann and pyramid wavefront sensors. Our evaluation is over a range of guide star magnitudes, and realistic noise, atmospheric and instrument conditions. Remarkably, we are able to develop a single network-based estimator that is accurate in both open and closed-loop AO configurations. Our method accurately estimates the Fried parameter from a single WFS image directly from AO telemetry to a few millimetres. Our approach is suitable for real time control, exhibiting 0.83ms r0 inference times on retail NVIDIA RTX 3090 GPU hardware, and thereby demonstrating a compelling economic solution for use in real-time instrument control.', 'abstract_zh': '基于机器学习的单幅波前传感器图像弗里德参数估计算法', 'title_zh': '基于人工神经网络的单波前传感器图像中的Fried参数估计'}
{'arxiv_id': 'arXiv:2504.17028', 'title': 'Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU', 'authors': 'Iman Khadir, Shane Stevenson, Henry Li, Kyle Krick, Abram Burrows, David Hall, Stan Posey, Samuel S.P. Shen', 'link': 'https://arxiv.org/abs/2504.17028', 'abstract': "This paper demonstrates the feasibility of democratizing AI-driven global weather forecasting models among university research groups by leveraging Graphics Processing Units (GPUs) and freely available AI models, such as NVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network for weather prediction and is trained on a 73-channel subset of the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset at single levels and different pressure levels. Although the training specifications for FourCastNetv2 are not released to the public, the training documentation of the model's first generation, FourCastNet, is available to all users. The training had 64 A100 GPUs and took 16 hours to complete. Although NVIDIA's models offer significant reductions in both time and cost compared to traditional Numerical Weather Prediction (NWP), reproducing published forecasting results presents ongoing challenges for resource-constrained university research groups with limited GPU availability. We demonstrate both (i) leveraging FourCastNetv2 to create predictions through the designated application programming interface (API) and (ii) utilizing NVIDIA hardware to train the original FourCastNet model. Further, this paper demonstrates the capabilities and limitations of NVIDIA A100's for resource-limited research groups in universities. We also explore data management, training efficiency, and model validation, highlighting the advantages and challenges of using limited high-performance computing resources. Consequently, this paper and its corresponding GitHub materials may serve as an initial guide for other university research groups and courses related to machine learning, climate science, and data science to develop research and education programs on AI weather forecasting, and hence help democratize the AI NWP in the digital economy.", 'abstract_zh': '这项研究展示了通过利用图形处理单元（GPUs）和免费获取的AI模型（如NVIDIA的FourCastNetv2）来在大学研究团队中民主化AI驱动的全球天气预报模型的可行性。FourCastNetv2是NVIDIA的一种高级神经网络，用于天气预测，它基于欧洲中长期天气预报中心（ECMWF）再分析v5（ERA5）数据集的73通道子集进行训练，包括单一层和不同的压力层。尽管没有公开发布FourCastNetv2的训练规范，但其前身FourCastNet的第一代模型的训练文档对所有用户都是公开的。训练使用了64个A100 GPUs，耗时16小时。虽然NVIDIA的模型在时间和成本方面相比传统的数值天气预报（NWP）提供了显著的减少，但对于资源有限且GPU可用性受限的大学研究团队来说，重新生成已公布的预报结果仍然是一个持续的挑战。本文展示了如何通过指定的应用程序编程接口（API）利用FourCastNetv2进行预测，以及如何利用NVIDIA硬件重新训练原始的FourCastNet模型。此外，本文还展示了NVIDIA A100对于资源有限的研究团队的能力和局限性，包括数据管理、训练效率和模型验证，突出了使用有限高性能计算资源的优势和挑战。因此，本文及其相应的GitHub材料可以作为其他相关于机器学习、气候科学和数据科学的研究团队和课程在AI天气预报领域开发研究和教育项目的初步指南，从而有助于在数字经济中民主化AI数值天气预报。', 'title_zh': 'AI驱动的数值天气模型民主化：基于University Research Lab研发的FourCastNetv2的全球预报示例'}
{'arxiv_id': 'arXiv:2504.17023', 'title': 'What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)', 'authors': 'Felix Kares, Timo Speith, Hanwei Zhang, Markus Langer', 'link': 'https://arxiv.org/abs/2504.17023', 'abstract': "Saliency maps are a popular approach for explaining classifications of (convolutional) neural networks. However, it remains an open question as to how best to evaluate salience maps, with three families of evaluation methods commonly being used: subjective user measures, objective user measures, and mathematical metrics. We examine three of the most popular saliency map approaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between subject study (N=166) across these families of evaluation methods. We test 1) for subjective measures, if the maps differ with respect to user trust and satisfaction; 2) for objective measures, if the maps increase users' abilities and thus understanding of a model; 3) for mathematical metrics, which map achieves the best ratings across metrics; and 4) whether the mathematical metrics can be associated with objective user measures. To our knowledge, our study is the first to compare several salience maps across all these evaluation methods$-$with the finding that they do not agree in their assessment (i.e., there was no difference concerning trust and satisfaction, Grad-CAM improved users' abilities best, and Guided Backpropagation had the most favorable mathematical metrics). Additionally, we show that some mathematical metrics were associated with user understanding, although this relationship was often counterintuitive. We discuss these findings in light of general debates concerning the complementary use of user studies and mathematical metrics in the evaluation of explainable AI (XAI) approaches.", 'abstract_zh': '可解释人工智能方法中显著性图的评价：用户研究与数学度量的比较', 'title_zh': '什么是良好的注意力图？解释型AI中注意力图评估策略的比较'}
{'arxiv_id': 'arXiv:2504.17020', 'title': 'Analyzing Value Functions of States in Parametric Markov Chains', 'authors': 'Kasper Engelen, Guillermo A. Pérez, Shrisha Rao', 'link': 'https://arxiv.org/abs/2504.17020', 'abstract': 'Parametric Markov chains (pMC) are used to model probabilistic systems with unknown or partially known probabilities. Although (universal) pMC verification for reachability properties is known to be coETR-complete, there have been efforts to approach it using potentially easier-to-check properties such as asking whether the pMC is monotonic in certain parameters. In this paper, we first reduce monotonicity to asking whether the reachability probability from a given state is never less than that of another given state. Recent results for the latter property imply an efficient algorithm to collapse same-value equivalence classes, which in turn preserves verification results and monotonicity. We implement our algorithm to collapse "trivial" equivalence classes in the pMC and show empirical evidence for the following: First, the collapse gives reductions in size for some existing benchmarks and significant reductions on some custom benchmarks; Second, the collapse speeds up existing algorithms to check monotonicity and parameter lifting, and hence can be used as a fast pre-processing step in practice.', 'abstract_zh': '参数马尔可夫链（pMC）用于建模具有未知或部分已知概率的概率系统。虽然（通用）pMC可达性性质验证是coETR-complete的，但已有努力通过询问pMC在某些参数上是否单调来使用更易于验证的性质。在本文中，我们首先将单调性归约为什么状态的可达概率从给定状态永远不会低于另一个给定状态的问题。对于后者性质的最近结果暗示了一个高效的算法来折叠相同值等价类，进而保持验证结果和单调性。我们实现该算法以在pMC中折叠“平凡”的等价类，并展示了以下实验证据：首先，折叠对于一些现有基准减少了规模，对于一些自定义基准有显著减少；其次，折叠加速了现有算法检查单调性和参数提升，因此可以在实践中作为快速预处理步骤使用。', 'title_zh': '分析参数马尔可夫链中状态的价值函数'}
{'arxiv_id': 'arXiv:2504.17004', 'title': '(Im)possibility of Automated Hallucination Detection in Large Language Models', 'authors': 'Amin Karbasi, Omar Montasser, John Sous, Grigoris Velegkas', 'link': 'https://arxiv.org/abs/2504.17004', 'abstract': "Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations.\nFirst, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language.\nSecond, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections.\nThese results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.", 'abstract_zh': '自动幻觉检测的可能性：理论框架探究', 'title_zh': '大型语言模型中自动化幻觉检测的可能性研究（或：大型语言模型中自动化幻觉检测的不可能性研究）'}
{'arxiv_id': 'arXiv:2504.16979', 'title': 'Automating tumor-infiltrating lymphocyte assessment in breast cancer histopathology images using QuPath: a transparent and accessible machine learning pipeline', 'authors': 'Masoud Tafavvoghi, Lars Ailo Bongo, André Berli Delgado, Nikita Shvetsov, Anders Sildnes, Line Moi, Lill-Tove Rasmussen Busund, Kajsa Møllersen', 'link': 'https://arxiv.org/abs/2504.16979', 'abstract': "In this study, we built an end-to-end tumor-infiltrating lymphocytes (TILs) assessment pipeline within QuPath, demonstrating the potential of easily accessible tools to perform complex tasks in a fully automatic fashion. First, we trained a pixel classifier to segment tumor, tumor-associated stroma, and other tissue compartments in breast cancer H&E-stained whole-slide images (WSI) to isolate tumor-associated stroma for subsequent analysis. Next, we applied a pre-trained StarDist deep learning model in QuPath for cell detection and used the extracted cell features to train a binary classifier distinguishing TILs from other cells. To evaluate our TILs assessment pipeline, we calculated the TIL density in each WSI and categorized them as low, medium, or high TIL levels. Our pipeline was evaluated against pathologist-assigned TIL scores, achieving a Cohen's kappa of 0.71 on the external test set, corroborating previous research findings. These results confirm that existing software can offer a practical solution for the assessment of TILs in H&E-stained WSIs of breast cancer.", 'abstract_zh': "本研究在QuPath中构建了一个端到端的肿瘤浸润淋巴细胞(TILs)评估管道，展示了易获取工具在全自动执行复杂任务方面的潜力。我们首先训练了一个像素分类器，用于分割乳腺癌HE染色全组织切片图像(WSI)中的肿瘤、肿瘤相关间质和其他组织区域，以便后续分析肿瘤相关间质。接着，我们使用预训练的StarDist深度学习模型在QuPath中进行细胞检测，并利用提取的细胞特征训练一个二元分类器，以区分TILs和其他细胞。为了评估我们的TILs评估管道，我们在每张WSI中计算了TIL密度，并将其分为低、中、高TIL水平。我们的管道在病理学家评估的TIL评分上进行了验证，外部测试集上的Cohen's kappa值为0.71，验证了先前的研究发现。这些结果证实现有的软件可以为乳腺癌HE染色WSI中的TILs评估提供一种实用的解决方案。", 'title_zh': '使用QuPath自动化乳腺癌组织病理学图像中浸润性淋巴细胞评估：一个透明且易于访问的机器学习管道'}
{'arxiv_id': 'arXiv:2504.16977', 'title': 'Tokenization Matters: Improving Zero-Shot NER for Indic Languages', 'authors': 'Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Amit Agarwal', 'link': 'https://arxiv.org/abs/2504.16977', 'abstract': 'Tokenization is a critical component of Natural Language Processing (NLP), especially for low resource languages, where subword segmentation influences vocabulary structure and downstream task accuracy. Although Byte Pair Encoding (BPE) is a standard tokenization method in multilingual language models, its suitability for Named Entity Recognition (NER) in low resource Indic languages remains underexplored due to its limitations in handling morphological complexity. In this work, we systematically compare BPE, SentencePiece, and Character Level tokenization strategies using IndicBERT for NER tasks in low resource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as extremely low resource Indic languages like Santali, Manipuri, and Sindhi. We assess both intrinsic linguistic properties tokenization efficiency, out of vocabulary (OOV) rates, and morphological preservation as well as extrinsic downstream performance, including fine tuning and zero shot cross lingual transfer.\nOur experiments show that SentencePiece is a consistently better performing approach than BPE for NER in low resource Indic Languages, particularly in zero shot cross lingual settings, as it better preserves entity consistency. While BPE provides the most compact tokenization form, it is not capable of generalization because it misclassifies or even fails to recognize entity labels when tested on unseen languages. In contrast, SentencePiece constitutes a better linguistic structural preservation model, benefiting extremely low resource and morphologically rich Indic languages, such as Santali and Manipuri, for superior entity recognition, as well as high generalization across scripts, such as Sindhi, written in Arabic. The results point to SentencePiece as the more effective tokenization strategy for NER within multilingual and low resource Indic NLP applications.', 'abstract_zh': '基于SentencePiece的子词分割在低资源印地语命名实体识别中的有效性探究', 'title_zh': '分词很重要：提高印地语零样本NER性能'}
{'arxiv_id': 'arXiv:2504.16972', 'title': 'Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications', 'authors': 'Hossein Ahmadi, Sajjad Emdadi Mahdimahalleh, Arman Farahat, Banafsheh Saffari', 'link': 'https://arxiv.org/abs/2504.16972', 'abstract': 'The rapid growth of unlabeled time-series data in domains such as wireless communications, radar, biomedical engineering, and the Internet of Things (IoT) has driven advancements in unsupervised learning. This review synthesizes recent progress in applying autoencoders and vision transformers for unsupervised signal analysis, focusing on their architectures, applications, and emerging trends. We explore how these models enable feature extraction, anomaly detection, and classification across diverse signal types, including electrocardiograms, radar waveforms, and IoT sensor data. The review highlights the strengths of hybrid architectures and self-supervised learning, while identifying challenges in interpretability, scalability, and domain generalization. By bridging methodological innovations and practical applications, this work offers a roadmap for developing robust, adaptive models for signal intelligence.', 'abstract_zh': '无线通信、雷达、生物医学工程和物联网（IoT）领域中未标记时间序列数据的快速发展推动了无监督学习的进步。本文综述了自动编码器和视觉变换器在无监督信号分析中的 Recent 进展，重点介绍了它们的架构、应用及其新兴趋势。本文探讨了这些模型如何在不同类型的信号中实现特征提取、异常检测和分类，包括心电图、雷达波形和物联网传感器数据。综述强调了混合架构和自我监督学习的优点，并指出了可解释性、可扩展性和领域泛化的挑战。通过将方法创新与实际应用相结合，本文为发展稳健、自适应的信号智能模型提供了路线图。', 'title_zh': '基于自动编码器和视觉变换器的无监督时间序列信号分析：架构与应用综述'}
{'arxiv_id': 'arXiv:2504.16968', 'title': 'Backslash: Rate Constrained Optimized Training of Large Language Models', 'authors': 'Jun Wu, Jiangtao Wen, Yuxing Han', 'link': 'https://arxiv.org/abs/2504.16968', 'abstract': 'The rapid advancement of large-language models (LLMs) has driven extensive research into parameter compression after training has been completed, yet compression during the training phase remains largely unexplored. In this work, we introduce Rate-Constrained Training (Backslash), a novel training-time compression approach based on rate-distortion optimization (RDO). Backslash enables a flexible trade-off between model accuracy and complexity, significantly reducing parameter redundancy while preserving performance. Experiments in various architectures and tasks demonstrate that Backslash can reduce memory usage by 60\\% - 90\\% without accuracy loss and provides significant compression gain compared to compression after training. Moreover, Backslash proves to be highly versatile: it enhances generalization with small Lagrange multipliers, improves model robustness to pruning (maintaining accuracy even at 80\\% pruning rates), and enables network simplification for accelerated inference on edge devices.', 'abstract_zh': '大规模语言模型的快速进步推动了训练完成后参数压缩的广泛应用，然而训练期压缩仍然鲜有探索。本文介绍了一种基于率失真优化的新训练时压缩方法——Backslash。Backslash能够在模型准确性与复杂性之间灵活权衡，显著减少参数冗余同时保持性能。在多种架构和任务上的实验显示，Backslash能够在不损失准确性的前提下将内存使用量减少60%至90%，并且相比训练后压缩提供了显著的压缩增益。此外，Backslash具有高度的灵活性：使用小的拉格朗日乘数增强泛化能力、提高模型修剪的稳健性（在80%的修剪率下仍保持准确性），并能够简化网络以加速边缘设备上的推理。', 'title_zh': 'Backslash: 大型语言模型在速率约束下的优化训练'}
{'arxiv_id': 'arXiv:2504.16948', 'title': 'Intrinsic Barriers to Explaining Deep Foundation Models', 'authors': 'Zhen Tan, Huan Liu', 'link': 'https://arxiv.org/abs/2504.16948', 'abstract': 'Deep Foundation Models (DFMs) offer unprecedented capabilities but their increasing complexity presents profound challenges to understanding their internal workings-a critical need for ensuring trust, safety, and accountability. As we grapple with explaining these systems, a fundamental question emerges: Are the difficulties we face merely temporary hurdles, awaiting more sophisticated analytical techniques, or do they stem from \\emph{intrinsic barriers} deeply rooted in the nature of these large-scale models themselves? This paper delves into this critical question by examining the fundamental characteristics of DFMs and scrutinizing the limitations encountered by current explainability methods when confronted with this inherent challenge. We probe the feasibility of achieving satisfactory explanations and consider the implications for how we must approach the verification and governance of these powerful technologies.', 'abstract_zh': 'Deep Foundation Models (DFMs)提供了前所未有的能力，但其日益复杂的结构对理解其内部工作机制提出了深刻的挑战——这对确保信任、安全和问责至关重要。在我们努力解释这些系统时，一个基本问题浮出水面：我们面临的困难只是等待更先进的分析技术的暂时障碍，还是源于这些大规模模型本身内在的根本限制？本文通过探讨DFMs的基本特征，并审视当前可解释性方法在面对这一固有挑战时遇到的局限性，来探索这一关键问题。我们探究获得满意解释的可行性，并考虑这对如何验证和治理这些强大技术的影响。', 'title_zh': '深度基础模型内在的解释障碍'}
{'arxiv_id': 'arXiv:2504.16947', 'title': 'SCRAG: Social Computing-Based Retrieval Augmented Generation for Community Response Forecasting in Social Media Environments', 'authors': 'Dachun Sun, You Lyu, Jinning Li, Yizhuo Chen, Tianshi Wang, Tomoyoshi Kimura, Tarek Abdelzaher', 'link': 'https://arxiv.org/abs/2504.16947', 'abstract': 'This paper introduces SCRAG, a prediction framework inspired by social computing, designed to forecast community responses to real or hypothetical social media posts. SCRAG can be used by public relations specialists (e.g., to craft messaging in ways that avoid unintended misinterpretations) or public figures and influencers (e.g., to anticipate social responses), among other applications related to public sentiment prediction, crisis management, and social what-if analysis. While large language models (LLMs) have achieved remarkable success in generating coherent and contextually rich text, their reliance on static training data and susceptibility to hallucinations limit their effectiveness at response forecasting in dynamic social media environments. SCRAG overcomes these challenges by integrating LLMs with a Retrieval-Augmented Generation (RAG) technique rooted in social computing. Specifically, our framework retrieves (i) historical responses from the target community to capture their ideological, semantic, and emotional makeup, and (ii) external knowledge from sources such as news articles to inject time-sensitive context. This information is then jointly used to forecast the responses of the target community to new posts or narratives. Extensive experiments across six scenarios on the X platform (formerly Twitter), tested with various embedding models and LLMs, demonstrate over 10% improvements on average in key evaluation metrics. A concrete example further shows its effectiveness in capturing diverse ideologies and nuances. Our work provides a social computing tool for applications where accurate and concrete insights into community responses are crucial.', 'abstract_zh': '基于社会计算的SCRAG预测框架：针对社交媒体帖子的真实或假设性社区响应预测', 'title_zh': '基于社会计算的检索增强生成方法：面向社交媒体环境中社区响应预测'}
{'arxiv_id': 'arXiv:2504.16946', 'title': 'MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation', 'authors': 'Xiaotong Ye, Nicolas Bougie, Toshihiko Yamasaki, Narimasa Watanabe', 'link': 'https://arxiv.org/abs/2504.16946', 'abstract': 'Generative agents offer promising capabilities for simulating realistic urban behaviors. However, existing methods oversimplify transportation choices in modern cities, and require prohibitive computational resources for large-scale population simulation. To address these limitations, we first present a virtual city that features multiple functional buildings and transportation modes. Then, we conduct extensive surveys to model behavioral choices and mobility preferences among population groups. Building on these insights, we introduce a simulation framework that captures the complexity of urban mobility while remaining scalable, enabling the simulation of over 4,000 agents. To assess the realism of the generated behaviors, we perform a series of micro and macro-level analyses. Beyond mere performance comparison, we explore insightful experiments, such as predicting crowd density from movement patterns and identifying trends in vehicle preferences across agent demographics.', 'abstract_zh': '生成性代理为模拟现实城市行为提供了有 promise 的能力。然而，现有方法在模拟现代城市的交通选择方面过于简化，并且在进行大规模人口模拟时需要 prohibitive 的计算资源。为解决这些局限性，我们首先提出了一座虚拟城市，该城市包含多种功能性建筑和交通模式。然后，我们进行了广泛的研究以建模不同人群的行为选择和出行偏好。基于这些洞察，我们引入了一种框架，该框架能够捕捉城市出行的复杂性同时保持可扩展性，从而能够模拟超过 4000 个代理。为了评估生成行为的真实性，我们进行了微观和宏观层面的分析。除简单的性能比较外，我们还探索了诸如从运动模式预测人群密度和识别不同代理人群的车辆偏好趋势等有洞察力的实验。', 'title_zh': '移动城市：一种高效的大规模城市行为仿真框架'}
{'arxiv_id': 'arXiv:2504.16942', 'title': 'S2Vec: Self-Supervised Geospatial Embeddings', 'authors': 'Shushman Choudhury, Elad Aharoni, Chandrakumari Suvarna, Iveel Tsogsuren, Abdul Rahman Kreidieh, Chun-Ta Lu, Neha Arora', 'link': 'https://arxiv.org/abs/2504.16942', 'abstract': 'Scalable general-purpose representations of the built environment are crucial for geospatial artificial intelligence applications. This paper introduces S2Vec, a novel self-supervised framework for learning such geospatial embeddings. S2Vec uses the S2 Geometry library to partition large areas into discrete S2 cells, rasterizes built environment feature vectors within cells as images, and applies masked autoencoding on these rasterized images to encode the feature vectors. This approach yields task-agnostic embeddings that capture local feature characteristics and broader spatial relationships. We evaluate S2Vec on three large-scale socioeconomic prediction tasks, showing its competitive performance against state-of-the-art image-based embeddings. We also explore the benefits of combining S2Vec embeddings with image-based embeddings downstream, showing that such multimodal fusion can often improve performance. Our results highlight how S2Vec can learn effective general-purpose geospatial representations and how it can complement other data modalities in geospatial artificial intelligence.', 'abstract_zh': '适用于地理空间人工智能应用的大规模普适环境表示至关重要。本文介绍了一种新颖的自监督框架S2Vec，用于学习此类地理空间嵌入。S2Vec使用S2 Geometry库将大区域划分为离散的S2单元，将单元内的建筑环境特征向量栅格化为图像，并在其上应用屏蔽自编码以编码这些栅格化图像中的特征向量。这种approach生成了任务无关的嵌入，能够捕捉局部特征特性及更广泛的空域关系。我们在三个大规模的社会经济预测任务上评估了S2Vec，展示了其在与基于图像的前沿嵌入相比的竞争力。我们还探讨了将S2Vec嵌入与基于图像的嵌入下游融合的益处，表明这种多模态融合通常可以提高性能。我们的结果突显了S2Vec如何学习有效的普遍适用地理空间表示以及它如何在地理空间人工智能中补充其他数据模态。', 'title_zh': 'S2Vec: 自监督地理空间嵌入'}
{'arxiv_id': 'arXiv:2504.16940', 'title': 'Can deep neural networks learn biological vision?', 'authors': 'Drew Linsley, Pinyuan Feng, Thomas Serre', 'link': 'https://arxiv.org/abs/2504.16940', 'abstract': 'Deep neural networks (DNNs) once showed increasing alignment with primate neural responses as they improved on computer vision benchmarks. This trend raised the exciting possibility that better models of biological vision would come as a byproduct of the deep learning revolution in artificial intelligence. However, the trend has reversed over recent years as DNNs have scaled to human or superhuman recognition accuracy, a divergence that may stem from modern DNNs learning to rely on different visual features than primates to solve tasks. Where will better computational models of biological vision come from? We propose that vision science must break from artificial intelligence to develop algorithms that are designed with biological visual systems in mind instead of internet data benchmarks. We predict that the next generation of deep learning models of biological vision will be trained with data diets, training routines, and objectives that are closer to those that shape human vision than those that are in use today.', 'abstract_zh': '深度神经网络(DNNs)曾随着其在计算机视觉基准上的性能提升而越来越接近灵长类神经反应，这一趋势激发了生物学视觉更好模型可能作为人工智能深度学习革命的副产品出现的激动人心的可能性。然而，近年来随着DNNs达到人类或超人类识别准确性，这种趋势已发生逆转，分歧可能源于现代DNNs学习依赖于与灵长类不同的视觉特征来解决问题。那么，更好的生物学视觉计算模型将从哪里来呢？我们提出，视觉科学必须脱离人工智能，开发出旨在考虑生物学视觉系统而非互联网数据基准的算法。我们预测，下一代生物学视觉的深度学习模型将使用更接近塑造人类视觉的数据饮食、训练套路和目标进行训练。', 'title_zh': '深度神经网络能否学习生物视觉？'}
