# "Does the cafe entrance look accessible? Where is the door?" Towards Geospatial AI Agents for Visual Inquiries 

**Title (ZH)**: 咖啡店入口看起来便于进入吗？门在哪里？面向地理空间AI代理的视觉查询研究 

**Authors**: Jon E. Froehlich, Jared Hwang, Zeyu Wang, John S. O'Meara, Xia Su, William Huang, Yang Zhang, Alex Fiannaca, Philip Nelson, Shaun Kane  

**Link**: [PDF](https://arxiv.org/pdf/2508.15752)  

**Abstract**: Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work. 

**Abstract (ZH)**: 交互式数字地图已革新人们的旅行和世界认知方式；然而，它们依赖于GIS数据库中的预存结构化数据（例如，道路网络、POI索引），限制了其解决与世界面貌相关的地理事物可视化问题的能力。我们提出了Geo-Visual代理的概念——多模态AI代理，能够通过分析大规模的地理空间图像库（包括街道景观、地点照片和航空影像等）来理解和回应关于世界的精炼的空间视觉查询，结合传统的GIS数据源。我们定义了这一概念，描述了感知和交互方法，提供了三个示例，并列举了未来工作的关键挑战与机遇。 

---
# DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding 

**Title (ZH)**: DesignCLIP: 基于CLIP的多模态学习设计专利理解 

**Authors**: Zhu Wang, Homaira Huda Shomee, Sathya N. Ravi, Sourav Medya  

**Link**: [PDF](https://arxiv.org/pdf/2508.15297)  

**Abstract**: In the field of design patent analysis, traditional tasks such as patent classification and patent image retrieval heavily depend on the image data. However, patent images -- typically consisting of sketches with abstract and structural elements of an invention -- often fall short in conveying comprehensive visual context and semantic information. This inadequacy can lead to ambiguities in evaluation during prior art searches. Recent advancements in vision-language models, such as CLIP, offer promising opportunities for more reliable and accurate AI-driven patent analysis. In this work, we leverage CLIP models to develop a unified framework DesignCLIP for design patent applications with a large-scale dataset of U.S. design patents. To address the unique characteristics of patent data, DesignCLIP incorporates class-aware classification and contrastive learning, utilizing generated detailed captions for patent images and multi-views image learning. We validate the effectiveness of DesignCLIP across various downstream tasks, including patent classification and patent retrieval. Additionally, we explore multimodal patent retrieval, which provides the potential to enhance creativity and innovation in design by offering more diverse sources of inspiration. Our experiments show that DesignCLIP consistently outperforms baseline and SOTA models in the patent domain on all tasks. Our findings underscore the promise of multimodal approaches in advancing patent analysis. The codebase is available here: this https URL. 

**Abstract (ZH)**: 设计专利分析中的设计CLIP框架：基于大规模美国设计专利数据的统一方法 

---
