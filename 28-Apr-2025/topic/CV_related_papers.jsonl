{'arxiv_id': 'arXiv:2504.18521', 'title': 'E-VLC: A Real-World Dataset for Event-based Visible Light Communication And Localization', 'authors': 'Shintaro Shiba, Quan Kong, Norimasa Kobori', 'link': 'https://arxiv.org/abs/2504.18521', 'abstract': 'Optical communication using modulated LEDs (e.g., visible light communication) is an emerging application for event cameras, thanks to their high spatio-temporal resolutions. Event cameras can be used simply to decode the LED signals and also to localize the camera relative to the LED marker positions. However, there is no public dataset to benchmark the decoding and localization in various real-world settings. We present, to the best of our knowledge, the first public dataset that consists of an event camera, a frame camera, and ground-truth poses that are precisely synchronized with hardware triggers. It provides various camera motions with various sensitivities in different scene brightness settings, both indoor and outdoor. Furthermore, we propose a novel method of localization that leverages the Contrast Maximization framework for motion estimation and compensation. The detailed analysis and experimental results demonstrate the advantages of LED-based localization with events over the conventional AR-marker--based one with frames, as well as the efficacy of the proposed method in localization. We hope that the proposed dataset serves as a future benchmark for both motion-related classical computer vision tasks and LED marker decoding tasks simultaneously, paving the way to broadening applications of event cameras on mobile devices. this https URL', 'abstract_zh': '基于调制LED的光通信事件相机应用：事件相机用于解码LED信号和定位LED标记位置，但由于缺乏公共数据集以在各种实际场景中 benchmark 解码和定位性能，我们首次推出了一个包含事件相机、帧相机和精确同步的地面真相姿态的数据集。该数据集提供了不同场景亮度设置下、室内和室外的各种相机运动和不同的灵敏度。此外，我们提出了一种新的基于对比最大化框架的定位方法，用于运动估计和补偿。详细的分析和实验结果表明，基于LED的事件定位方法在运动估计和补偿方面比基于帧的传统AR标记方法更优，同时展示了所提出方法在定位任务中的有效性。我们希望该数据集能够成为未来经典计算机视觉中的运动相关任务和LED标记解码任务的基准，为事件相机在移动设备中的广泛应用铺平道路。这个 https://doi.org/10.5281/zenodo.6543219', 'title_zh': 'E-VLC：事件驱动型可见光通信及定位的现实世界数据集'}
{'arxiv_id': 'arXiv:2504.18419', 'title': 'A Multimodal Hybrid Late-Cascade Fusion Network for Enhanced 3D Object Detection', 'authors': 'Carlo Sgaravatti, Roberto Basla, Riccardo Pieroni, Matteo Corno, Sergio M. Savaresi, Luca Magri, Giacomo Boracchi', 'link': 'https://arxiv.org/abs/2504.18419', 'abstract': 'We present a new way to detect 3D objects from multimodal inputs, leveraging both LiDAR and RGB cameras in a hybrid late-cascade scheme, that combines an RGB detection network and a 3D LiDAR detector. We exploit late fusion principles to reduce LiDAR False Positives, matching LiDAR detections with RGB ones by projecting the LiDAR bounding boxes on the image. We rely on cascade fusion principles to recover LiDAR False Negatives leveraging epipolar constraints and frustums generated by RGB detections of separate views. Our solution can be plugged on top of any underlying single-modal detectors, enabling a flexible training process that can take advantage of pre-trained LiDAR and RGB detectors, or train the two branches separately. We evaluate our results on the KITTI object detection benchmark, showing significant performance improvements, especially for the detection of Pedestrians and Cyclists.', 'abstract_zh': '我们提出了一种新的多模态输入下检测3D物体的方法，利用混合晚阶段级联方案结合RGB相机和LiDAR检测器。我们利用晚融合原则减少LiDAR假阳性，通过将LiDAR边界框投影到图像上来匹配RGB检测结果。我们依靠级联融合原则利用单视图RGB检测产生的透视约束和截锥体来恢复LiDAR假阴性。该解决方案可以叠加在任何底层单模态检测器之上，具有灵活的训练过程，可以利用预训练的LiDAR和RGB检测器，或单独训练两个分支。我们在KITTI物体检测基准上评估了我们的结果，显示出显著的性能提升，特别是在行人的检测方面。', 'title_zh': '增强三维目标检测的多模态混合晚cascade融合网络'}
{'arxiv_id': 'arXiv:2504.18355', 'title': 'Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes', 'authors': 'Maximilian Xiling Li, Korbinian Rudolf, Nils Blank, Rudolf Lioutikov', 'link': 'https://arxiv.org/abs/2504.18355', 'abstract': 'Robotic agents need to understand how to interact with objects in their environment, both autonomously and during human-robot interactions. Affordance detection on 3D point clouds, which identifies object regions that allow specific interactions, has traditionally relied on deep learning models like PointNet++, DGCNN, or PointTransformerV3. However, these models operate as black boxes, offering no insight into their decision-making processes. Prototypical Learning methods, such as ProtoPNet, provide an interpretable alternative to black-box models by employing a "this looks like that" case-based reasoning approach. However, they have been primarily applied to image-based tasks. In this work, we apply prototypical learning to models for affordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet benchmark dataset show that prototypical models achieve competitive performance with state-of-the-art black-box models and offer inherent interpretability. This makes prototypical models a promising candidate for human-robot interaction scenarios that require increased trust and safety.', 'abstract_zh': '机器人代理需要理解如何自主地与环境中的物体交互，以及在人机交互过程中与物体交互。物体区域的可用性检测（affordance detection）基于3D点云，传统上依赖于PointNet++、DGCNN或PointTransformerV3等深度学习模型。然而，这些模型作为黑盒操作，不提供其决策过程的见解。原型学习方法，如ProtoPNet，通过采用“看起来像”的案例推理方法提供了对黑盒模型的可解释替代方案。然而，它们主要应用于基于图像的任务。在本工作中，我们将原型学习应用于3D点云上的可用性检测模型。实验表明，原型模型在3D-AffordanceNet基准数据集上达到了与最先进的黑盒模型相当的性能，并具有内在的可解释性。这使得原型模型成为需要增加信任和安全的人机交互场景的有前景候选者。', 'title_zh': '基于概率原型的可解释的3D点云功能检测'}
{'arxiv_id': 'arXiv:2504.18235', 'title': 'BiasBench: A reproducible benchmark for tuning the biases of event cameras', 'authors': 'Andreas Ziegler, David Joseph, Thomas Gossard, Emil Moldovan, Andreas Zell', 'link': 'https://arxiv.org/abs/2504.18235', 'abstract': "Event-based cameras are bio-inspired sensors that detect light changes asynchronously for each pixel. They are increasingly used in fields like computer vision and robotics because of several advantages over traditional frame-based cameras, such as high temporal resolution, low latency, and high dynamic range. As with any camera, the output's quality depends on how well the camera's settings, called biases for event-based cameras, are configured. While frame-based cameras have advanced automatic configuration algorithms, there are very few such tools for tuning these biases. A systematic testing framework would require observing the same scene with different biases, which is tricky since event cameras only generate events when there is movement. Event simulators exist, but since biases heavily depend on the electrical circuit and the pixel design, available simulators are not well suited for bias tuning. To allow reproducibility, we present BiasBench, a novel event dataset containing multiple scenes with settings sampled in a grid-like pattern. We present three different scenes, each with a quality metric of the downstream application. Additionally, we present a novel, RL-based method to facilitate online bias adjustments.", 'abstract_zh': '基于事件的相机是受生物学启发的传感器，能够异步检测每个像素的光照变化。由于与传统的基于帧的相机相比具有高时间分辨率、低延迟和高动态范围等优势，它们在计算机视觉和机器人技术等领域越来越受到重视。类似于任何相机，输出的质量取决于相机设置（对于基于事件的相机称为偏置）的配置情况。尽管基于帧的相机已经拥有先进的自动配置算法，调整这些偏置的工具却相对较少。系统性的测试框架需要使用不同的偏置观测相同的场景，但由于事件相机仅在检测到运动时才生成事件，因此这颇具挑战性。现有的事件模拟器存在，但由于偏置高度依赖于电子电路和像素设计，这些模拟器不适用于偏置调整。为了保证可重复性，我们提出了BiasBench，这是一个新颖的基于事件的数据集，包含多个采用网格采样方法设置的场景。我们还呈现了三种不同的场景，并提供了每个场景下游应用的质量度量。此外，我们还介绍了一种新颖的基于强化学习的方法，以促进在线偏置调整。', 'title_zh': 'BiasBench: 一种可再现的事件摄像机偏置调整基准测试'}
{'arxiv_id': 'arXiv:2504.17890', 'title': 'Quaternion Domain Super MDS for 3D Localization', 'authors': 'Keigo Masuoka, Takumi Takahashi, Giuseppe Thadeu Freitas de Abreu, Hideki Ochiai', 'link': 'https://arxiv.org/abs/2504.17890', 'abstract': 'We propose a novel low-complexity three-dimensional (3D) localization algorithm for wireless sensor networks, termed quaternion-domain super multidimensional scaling (QD-SMDS). This algorithm reformulates the conventional SMDS, which was originally developed in the real domain, into the quaternion domain. By representing 3D coordinates as quaternions, the method enables the construction of a rank-1 Gram edge kernel (GEK) matrix that integrates both relative distance and angular (phase) information between nodes, maximizing the noise reduction effect achieved through low-rank truncation via singular value decomposition (SVD). The simulation results indicate that the proposed method demonstrates a notable enhancement in localization accuracy relative to the conventional SMDS algorithm, particularly in scenarios characterized by substantial measurement errors.', 'abstract_zh': 'Quaternion域超多维标度量化三维无线传感器网络定位算法（QD-SMDS）', 'title_zh': '三维定位领域的四元数域超MDS方法'}
{'arxiv_id': 'arXiv:2504.18453', 'title': 'Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation', 'authors': 'Peiyuan Jing, Kinhei Lee, Zhenxuan Zhang, Huichi Zhou, Zhengqing Yuan, Zhifan Gao, Lei Zhu, Giorgos Papanastasiou, Yingying Fang, Guang Yang', 'link': 'https://arxiv.org/abs/2504.18453', 'abstract': "Radiology report generation is critical for efficiency but current models lack the structured reasoning of experts, hindering clinical trust and explainability by failing to link visual findings to precise anatomical locations. This paper introduces BoxMed-RL, a groundbreaking unified training framework for generating spatially verifiable and explainable radiology reports. Built on a large vision-language model, BoxMed-RL revolutionizes report generation through two integrated phases: (1) In the Pretraining Phase, we refine the model via medical concept learning, using Chain-of-Thought supervision to internalize the radiologist-like workflow, followed by spatially verifiable reinforcement, which applies reinforcement learning to align medical findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze the pretrained weights and train a downstream adapter to ensure fluent and clinically credible reports. This framework precisely mimics radiologists' workflow, compelling the model to connect high-level medical concepts with definitive anatomical evidence. Extensive experiments on public datasets demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR and ROUGE-L metrics compared to state-of-the-art methods. An average 5% improvement in large language model-based metrics further underscores BoxMed-RL's robustness in generating high-quality radiology reports.", 'abstract_zh': 'BoxMed-RL：一种生成空间可验证和解释性强的放射学报告的统一训练框架', 'title_zh': '像放射学家一样思考：基于因果推理和强化学习的可验证报告生成'}
{'arxiv_id': 'arXiv:2504.18447', 'title': 'Iterative Event-based Motion Segmentation by Variational Contrast Maximization', 'authors': 'Ryo Yamaki, Shintaro Shiba, Guillermo Gallego, Yoshimitsu Aoki', 'link': 'https://arxiv.org/abs/2504.18447', 'abstract': 'Event cameras provide rich signals that are suitable for motion estimation since they respond to changes in the scene. As any visual changes in the scene produce event data, it is paramount to classify the data into different motions (i.e., motion segmentation), which is useful for various tasks such as object detection and visual servoing. We propose an iterative motion segmentation method, by classifying events into background (e.g., dominant motion hypothesis) and foreground (independent motion residuals), thus extending the Contrast Maximization framework. Experimental results demonstrate that the proposed method successfully classifies event clusters both for public and self-recorded datasets, producing sharp, motion-compensated edge-like images. The proposed method achieves state-of-the-art accuracy on moving object detection benchmarks with an improvement of over 30%, and demonstrates its possibility of applying to more complex and noisy real-world scenes. We hope this work broadens the sensitivity of Contrast Maximization with respect to both motion parameters and input events, thus contributing to theoretical advancements in event-based motion segmentation estimation. this https URL', 'abstract_zh': '事件相机提供了一种丰富的信号，适用于运动估计，因为它们对场景变化有所响应。由于场景中的任何视觉变化都会产生事件数据，因此对数据进行不同运动的分类（即运动分割）至关重要，这有助于各种任务，如目标检测和视觉伺服。我们提出了一种迭代运动分割方法，通过将事件分为背景（例如，主导运动假设）和前景（独立运动残差），从而扩展了对比最大化框架。实验结果表明，所提出的方法能够成功地对公共数据集和自录制数据集中的事件簇进行分类，生成清晰、运动补偿的边缘般图像。所提出的方法在移动目标检测基准测试中达到了最先进的精度，改进幅度超过30%，并证明了其应用于更复杂和嘈杂的现实场景的可能性。我们希望通过这项工作增强对比最大化方法对运动参数和输入事件的敏感性，从而促进基于事件的运动分割估计的理论进步。[原文链接]', 'title_zh': '基于变异对比最大化迭代事件驱动运动分割'}
{'arxiv_id': 'arXiv:2504.18361', 'title': 'COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization', 'authors': 'Haozhen Yan, Yan Hong, Jiahui Zhan, Yikun Ji, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang', 'link': 'https://arxiv.org/abs/2504.18361', 'abstract': 'Recent advancements in image manipulation have achieved unprecedented progress in generating photorealistic content, but also simultaneously eliminating barriers to arbitrary manipulation and editing, raising concerns about multimedia authenticity and cybersecurity. However, existing Image Manipulation Detection and Localization (IMDL) methodologies predominantly focus on splicing or copy-move forgeries, lacking dedicated benchmarks for inpainting-based manipulations. To bridge this gap, we present COCOInpaint, a comprehensive benchmark specifically designed for inpainting detection, with three key contributions: 1) High-quality inpainting samples generated by six state-of-the-art inpainting models, 2) Diverse generation scenarios enabled by four mask generation strategies with optional text guidance, and 3) Large-scale coverage with 258,266 inpainted images with rich semantic diversity. Our benchmark is constructed to emphasize intrinsic inconsistencies between inpainted and authentic regions, rather than superficial semantic artifacts such as object shapes. We establish a rigorous evaluation protocol using three standard metrics to assess existing IMDL approaches. The dataset will be made publicly available to facilitate future research in this area.', 'abstract_zh': 'Recent advancements in image manipulation have achieved unprecedented progress in generating photorealistic content, but also simultaneously eliminating barriers to arbitrary manipulation and editing, raising concerns about multimedia authenticity and cybersecurity. However, existing Image Manipulation Detection and Localization (IMDL) methodologies predominantly focus on splicing or copy-move forgeries, lacking dedicated benchmarks for inpainting-based manipulations. To bridge this gap, we present COCOInpaint, a comprehensive benchmark specifically designed for inpainting detection, with three key contributions: 1) 高质量的由六种先进 inpainting 模型生成的 inpainting 样本，2) 通过四种不同的遮罩生成策略实现的多样化生成场景，可选配文本指导，3) 包含258,266张富有语义多样性的 inpainted 图像的大规模覆盖。我们的基准侧重于强调 inpainted 区域与真实区域之间的内在不一致性，而不是表面的语义伪影如物体形状。我们利用三种标准度量建立了严格的评估协议，以评估现有的图像伪造检测和定位方法。数据集将公开发布，以促进该领域的未来研究。', 'title_zh': 'COCO-Inpaint: 一种图像 inpaint 检测和操作定位基准'}
{'arxiv_id': 'arXiv:2504.18249', 'title': 'Event-Based Eye Tracking. 2025 Event-based Vision Workshop', 'authors': 'Qinyu Chen, Chang Gao, Min Liu, Daniele Perrone, Yan Ru Pei, Zuowen Wang, Zhuo Zou, Shihang Tan, Tao Han, Guorui Lu, Zhen Xu, Junyuan Ding, Ziteng Wang, Zongwei Wu, Han Han, Yuliang Wu, Jinze Chen, Wei Zhai, Yang Cao, Zheng-jun Zha, Nuwan Bandara, Thivya Kandappu, Archan Misra, Xiaopeng Lin, Hongxiang Huang, Hongwei Ren, Bojun Cheng, Hoang M. Truong, Vinh-Thuan Ly, Huy G. Tran, Thuan-Phat Nguyen, Tram T. Doan', 'link': 'https://arxiv.org/abs/2504.18249', 'abstract': 'This survey serves as a review for the 2025 Event-Based Eye Tracking Challenge organized as part of the 2025 CVPR event-based vision workshop. This challenge focuses on the task of predicting the pupil center by processing event camera recorded eye movement. We review and summarize the innovative methods from teams rank the top in the challenge to advance future event-based eye tracking research. In each method, accuracy, model size, and number of operations are reported. In this survey, we also discuss event-based eye tracking from the perspective of hardware design.', 'abstract_zh': '2025事件驱动眼动追踪挑战综述：面向CVPR事件驱动视觉研讨会的回顾与总结', 'title_zh': '基于事件的注视跟踪。2025基于事件的视觉研讨会'}
{'arxiv_id': 'arXiv:2504.18201', 'title': 'Multi-Grained Compositional Visual Clue Learning for Image Intent Recognition', 'authors': 'Yin Tang, Jiankai Li, Hongyu Yang, Xuan Dong, Lifeng Fan, Weixin Li', 'link': 'https://arxiv.org/abs/2504.18201', 'abstract': 'In an era where social media platforms abound, individuals frequently share images that offer insights into their intents and interests, impacting individual life quality and societal stability. Traditional computer vision tasks, such as object detection and semantic segmentation, focus on concrete visual representations, while intent recognition relies more on implicit visual clues. This poses challenges due to the wide variation and subjectivity of such clues, compounded by the problem of intra-class variety in conveying abstract concepts, e.g. "enjoy life". Existing methods seek to solve the problem by manually designing representative features or building prototypes for each class from global features. However, these methods still struggle to deal with the large visual diversity of each intent category. In this paper, we introduce a novel approach named Multi-grained Compositional visual Clue Learning (MCCL) to address these challenges for image intent recognition. Our method leverages the systematic compositionality of human cognition by breaking down intent recognition into visual clue composition and integrating multi-grained features. We adopt class-specific prototypes to alleviate data imbalance. We treat intent recognition as a multi-label classification problem, using a graph convolutional network to infuse prior knowledge through label embedding correlations. Demonstrated by a state-of-the-art performance on the Intentonomy and MDID datasets, our approach advances the accuracy of existing methods while also possessing good interpretability. Our work provides an attempt for future explorations in understanding complex and miscellaneous forms of human expression.', 'abstract_zh': '在社交媒体平台泛滥的时代，个体经常分享能够揭示其意图和兴趣的照片，影响个人生活质量和社会稳定性。传统计算机视觉任务，如对象检测和语义分割，侧重于具体的视觉表示，而意图识别则更多依赖于隐含的视觉线索。但由于这类线索存在广泛变化和主观性，且在传达抽象概念时存在类别内的多样性问题，如“享受生活”，现有方法通过手动设计代表性特征或从全局特征构建原型来解决问题，但仍难以应对每类意图的大量视觉多样性。在本文中，我们提出了一种新的方法，即多粒度组成性视觉线索学习（MCCL），以应对这些挑战。我们的方法通过分解意图识别为视觉线索组成，并结合多粒度特征，利用人类认知的系统组合性。我们采用类别特定的原型以缓解数据不平衡问题，并将意图识别视为一个多标签分类问题，通过图卷积网络引入标签嵌入关联的先验知识，展示了在Intentonomy和MDID数据集上的优异性能，同时提高了现有方法的准确性且具有良好的可解释性。我们的工作提供了未来探索理解和解释人类表达复杂多样的尝试。', 'title_zh': '多粒度组成性视觉线索学习在图像意图识别中的应用'}
{'arxiv_id': 'arXiv:2504.18165', 'title': 'PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models', 'authors': 'Michel Gokan Khan, Renan Guarese, Fabian Johnson, Xi Vincent Wang, Anders Bergman, Benjamin Edvinsson, Mario Romero, Jérémy Vachier, Jan Kronqvist', 'link': 'https://arxiv.org/abs/2504.18165', 'abstract': "We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning framework that combines camera and sensory data with 3D Gaussian Splatting and computer vision models for digital twinning, object tracking, and Key Performance Indicators (KPIs) extraction in industrial production lines. By utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam offers a semi-automated approach to object tracking and spatial mapping, enabling digital twins that capture real-time KPIs such as availability, performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts in the production line. We validate the effectiveness of PerfCam through a practical deployment within realistic test production lines in the pharmaceutical industry and contribute an openly published dataset to support further research and development in the field. The results demonstrate PerfCam's ability to deliver actionable insights through its precise digital twin capabilities, underscoring its value as an effective tool for developing usable digital twins in smart manufacturing environments and extracting operational analytics.", 'abstract_zh': '我们介绍PerfCam，一个开源的概念验证（PoC）数字孪生框架，结合了相机和传感器数据、3D高斯点绘技术和计算机视觉模型，用于工业生产线的数字孪生、对象跟踪和关键绩效指标（KPI）提取。通过利用3D重建和卷积神经网络（CNNs），PerfCam 提供了一种半自动的对象跟踪和空间映射方法，能够捕捉到如可用性、性能、整体设备有效性（OEE）以及传送带速率等实时KPI的精确数字孪生。我们通过在制药行业中的实际测试生产线上部署PerfCam，验证了其有效性，并公开发布了一个数据集以支持该领域的进一步研究与开发。研究结果表明，PerfCam 通过其精确的数字孪生能力提供了可操作的见解，突显了其在智能制造环境中开发实用数字孪生和提取运营分析方面的价值。', 'title_zh': 'PerfCam: 生产线数字孪生基于3D高斯点显示和视觉模型'}
{'arxiv_id': 'arXiv:2504.18068', 'title': 'S3MOT: Monocular 3D Object Tracking with Selective State Space Model', 'authors': 'Zhuohao Yan, Shaoquan Feng, Xingxing Li, Yuxuan Zhou, Chunxi Xia, Shengyu Li', 'link': 'https://arxiv.org/abs/2504.18068', 'abstract': 'Accurate and reliable multi-object tracking (MOT) in 3D space is essential for advancing robotics and computer vision applications. However, it remains a significant challenge in monocular setups due to the difficulty of mining 3D spatiotemporal associations from 2D video streams. In this work, we present three innovative techniques to enhance the fusion and exploitation of heterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State Space Model (HSSM), a novel data association mechanism that compresses contextual tracking cues across multiple paths, enabling efficient and comprehensive assignment decisions with linear complexity. HSSM features a global receptive field and dynamic weights, in contrast to traditional linear assignment algorithms that rely on hand-crafted association costs. (2) We propose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI pooling by directly using dense feature maps for contrastive learning, thus improving object re-identification accuracy under challenging conditions such as varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation through VeloSSM, an encoder-decoder architecture that models temporal dependencies in velocity to capture motion dynamics, overcoming the limitations of frame-based 3D inference. Experiments on the KITTI public test benchmark demonstrate the effectiveness of our method, achieving a new state-of-the-art performance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best by significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness and efficiency for monocular 3D MOT tasks. The code and models are available at this https URL.', 'abstract_zh': '准确可靠的单目三维多对象跟踪（MOT）在机器人技术和计算机视觉应用中至关重要。然而，在单目设置中由于难以从二维视频流中挖掘三维时空关联，使其成为一个重大挑战。在本工作中，我们提出三种创新技术以增强单目三维多对象跟踪中异构线索的融合与利用：（1）我们引入匈牙利状态空间模型（HSSM），这是一种新的数据关联机制，能够跨多个路径压缩上下文跟踪线索，实现线性复杂度下的高效且全面的分配决策。HSSM 具有全局感受野和动态权重，而传统的线性分配算法依赖于手工设计的关联成本。（2）我们提出全卷积一阶段嵌入（FCOE），其通过直接使用密集特征图进行对比学习，消除了ROI池化，从而在多视角和光照变化等具有挑战性的条件下提高物体再识别的准确性。（3）我们通过速度编码器-解码器架构VeloSSM增强六自由度姿态估计，该架构建模了时间相关的速度依赖性以捕捉运动动态，克服了基于帧的三维推断的局限性。我们的方法在KITTI公开测试基准上的实验表明其有效性，实现了76.86的HOTA，在31 FPS下达到新最佳性能。与之前最佳方法相比，我们的方法在HOTA和AssA上分别取得了2.63和3.62的显著提升，展示了其在单目三维多对象跟踪任务中的鲁棒性和效率。代码和模型可在以下链接获取。', 'title_zh': 'S3MOT：基于选择性状态空间模型的单目三维目标跟踪'}
{'arxiv_id': 'arXiv:2504.18057', 'title': 'Opportunistic Collaborative Planning with Large Vision Model Guided Control and Joint Query-Service Optimization', 'authors': 'Jiayi Chen, Shuai Wang, Guoliang Li, Wei Xu, Guangxu Zhu, Derrick Wing Kwan Ng, Chengzhong Xu', 'link': 'https://arxiv.org/abs/2504.18057', 'abstract': 'Navigating autonomous vehicles in open scenarios is a challenge due to the difficulties in handling unseen objects. Existing solutions either rely on small models that struggle with generalization or large models that are resource-intensive. While collaboration between the two offers a promising solution, the key challenge is deciding when and how to engage the large model. To address this issue, this paper proposes opportunistic collaborative planning (OCP), which seamlessly integrates efficient local models with powerful cloud models through two key innovations. First, we propose large vision model guided model predictive control (LVM-MPC), which leverages the cloud for LVM perception and decision making. The cloud output serves as a global guidance for a local MPC, thereby forming a closed-loop perception-to-control system. Second, to determine the best timing for large model query and service, we propose collaboration timing optimization (CTO), including object detection confidence thresholding (ODCT) and cloud forward simulation (CFS), to decide when to seek cloud assistance and when to offer cloud service. Extensive experiments show that the proposed OCP outperforms existing methods in terms of both navigation time and success rate.', 'abstract_zh': '基于机会的合作规划：在开放场景中导航自主车辆', 'title_zh': '基于大型视觉模型引导控制及联合查询-服务优化的机会性协同规划'}
{'arxiv_id': 'arXiv:2504.18049', 'title': 'A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images', 'authors': 'Xin Li, Wenhui Zhu, Peijie Qiu, Oana M. Dumitrascu, Amal Youssef, Yalin Wang', 'link': 'https://arxiv.org/abs/2504.18049', 'abstract': "In the field of medical imaging, the advent of deep learning, especially the application of convolutional neural networks (CNNs) has revolutionized the analysis and interpretation of medical images. Nevertheless, deep learning methods usually rely on large amounts of labeled data. In medical imaging research, the acquisition of high-quality labels is both expensive and difficult. The introduction of Vision Transformers (ViT) and self-supervised learning provides a pre-training strategy that utilizes abundant unlabeled data, effectively alleviating the label acquisition challenge while broadening the breadth of data utilization. However, ViT's high computational density and substantial demand for computing power, coupled with the lack of localization characteristics of its operations on image patches, limit its efficiency and applicability in many application scenarios. In this study, we employ nn-MobileNet, a lightweight CNN framework, to implement a BERT-style self-supervised learning approach. We pre-train the network on the unlabeled retinal fundus images from the UK Biobank to improve downstream application performance. We validate the results of the pre-trained model on Alzheimer's disease (AD), Parkinson's disease (PD), and various retinal diseases identification. The results show that our approach can significantly improve performance in the downstream tasks. In summary, this study combines the benefits of CNNs with the capabilities of advanced self-supervised learning in handling large-scale unlabeled data, demonstrating the potential of CNNs in the presence of label scarcity.", 'abstract_zh': '在医学影像领域，深度学习的出现，尤其是卷积神经网络（CNNs）的应用， telah革命性地改变了医学影像的分析和解释。然而，深度学习方法通常依赖大量标记数据。在医学影像研究中，高质量标签的获取既昂贵又困难。通过引入视觉变换器（ViT）和自监督学习，提供了一种利用丰富未标记数据的预训练策略，有效缓解了标签获取的挑战，同时也扩展了数据利用的广度。然而，ViT的高计算密度和对计算能力的大量需求，以及其对图像块操作缺乏定位特性，限制了其在许多应用场景中的效率和适用性。在本研究中，我们采用轻量级CNN框架nn-MobileNet，实现了一种类似BERT的自监督学习方法。我们在英国生物银行的未标记视网膜底片图像上进行预训练，以提高下游应用性能。我们通过验证阿尔茨海默病（AD）、帕金森病（PD）和各种视网膜疾病识别的结果，展示了预训练模型的表现。结果显示，我们的方法能够在下游任务中显著提高性能。总之，本研究结合了CNN的优势和先进自监督学习处理大规模未标记数据的能力，展示了在标签稀缺情况下CNN的潜力。', 'title_zh': '基于BERT风格的自监督学习CNN在视网膜图像疾病识别中的应用'}
{'arxiv_id': 'arXiv:2504.17829', 'title': 'Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing', 'authors': 'Vlad Vasilescu, Ana Neacsu, Daniela Faur', 'link': 'https://arxiv.org/abs/2504.17829', 'abstract': 'Single-image dehazing is an important topic in remote sensing applications, enhancing the quality of acquired images and increasing object detection precision. However, the reliability of such structures has not been sufficiently analyzed, which poses them to the risk of imperceptible perturbations that can significantly hinder their performance. In this work, we show that state-of-the-art image-to-image dehazing transformers are susceptible to adversarial noise, with even 1 pixel change being able to decrease the PSNR by as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies aimed at increasing the robustness of pre-trained transformers. Our methods results in comparable clean performance, while significantly increasing the protection against adversarial data. We further present their applicability in two remote sensing scenarios, showcasing their robust behavior for out-of-distribution data. The source code for adversarial fine-tuning and attack algorithms can be found at this http URL.', 'abstract_zh': '单张图像去雾霾是遥感应用中的一个重要课题，旨在提高获取图像的质量并提高物体检测精度。然而，此类结构的可靠性尚未得到充分分析，使其面临可能显著妨碍其性能的不可感知扰动的风险。在本文中，我们展示了最先进的图像到图像去雾霾变换器对对抗噪声的敏感性，甚至一个像素的变化也会使PSNR降低2.8 dB。随后，我们提出了两种轻量级微调策略，旨在提高预训练变换器的鲁棒性。我们的方法在获得可比的去噪性能的同时，显著增强了对抗数据的防护能力。此外，我们展示了其在两种遥感场景中的适用性，证明了其对分布外数据的鲁棒行为。对抗微调和攻击算法的源代码可在此网址找到。', 'title_zh': 'adversarially-robust transformers单像去雾霾的微调研究'}
{'arxiv_id': 'arXiv:2504.17825', 'title': 'Dual Prompting Image Restoration with Diffusion Transformers', 'authors': 'Dehong Kong, Fan Li, Zhixin Wang, Jiaqi Xu, Renjing Pei, Wenbo Li, WenQi Ren', 'link': 'https://arxiv.org/abs/2504.17825', 'abstract': 'Recent state-of-the-art image restoration methods mostly adopt latent diffusion models with U-Net backbones, yet still facing challenges in achieving high-quality restoration due to their limited capabilities. Diffusion transformers (DiTs), like SD3, are emerging as a promising alternative because of their better quality with scalability. In this paper, we introduce DPIR (Dual Prompting Image Restoration), a novel image restoration method that effectivly extracts conditional information of low-quality images from multiple perspectives. Specifically, DPIR consits of two branches: a low-quality image conditioning branch and a dual prompting control branch. The first branch utilizes a lightweight module to incorporate image priors into the DiT with high efficiency. More importantly, we believe that in image restoration, textual description alone cannot fully capture its rich visual characteristics. Therefore, a dual prompting module is designed to provide DiT with additional visual cues, capturing both global context and local appearance. The extracted global-local visual prompts as extra conditional control, alongside textual prompts to form dual prompts, greatly enhance the quality of the restoration. Extensive experimental results demonstrate that DPIR delivers superior image restoration performance.', 'abstract_zh': '最近的先进图像恢复方法大多采用具有U-Net骨干的潜在扩散模型，但由于其局限性，仍面临实现高质量恢复的挑战。扩散变压器（DiTs），如SD3，因其更好的质量和可扩展性正 emergence 作为一种有前景的替代方案。在本文中，我们提出了一种名为DPIR（Dual Prompting Image Restoration）的新颖图像恢复方法，能够从多个角度有效提取低质量图像的条件信息。具体来说，DPIR 包含两个分支：一个低质量图像条件分支和一个双重提示控制分支。第一个分支利用一个轻量级模块以高效率将图像先验信息整合到 DiT 中。更重要的是，我们认为在图像恢复中，仅依赖文本描述无法完全捕捉其丰富的视觉特征。因此，我们设计了一个双重提示模块，为 DiT 提供额外的视觉线索，捕获全局上下文和局部外观。提取出的全局-局部视觉提示作为额外条件控制，与文本提示一起构成双重提示，极大地提升了恢复质量。广泛的实验结果表明，DPIR 能够提供优异的图像恢复性能。', 'title_zh': '双提示图像恢复扩散变换器'}
{'arxiv_id': 'arXiv:2504.17822', 'title': 'A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw', 'authors': 'Wenwen Li, Chia-Yu Hsu, Sizhe Wang, Zhining Gu, Yili Yang, Brendan M. Rogers, Anna Liljedahl', 'link': 'https://arxiv.org/abs/2504.17822', 'abstract': "Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost landforms with significant environmental impacts. Mapping these RTS is crucial because their appearance serves as a clear indication of permafrost thaw. However, their small scale compared to other landform features, vague boundaries, and spatiotemporal variation pose significant challenges for accurate detection. In this paper, we employed a state-of-the-art deep learning model, the Cascade Mask R-CNN with a multi-scale vision transformer-based backbone, to delineate RTS features across the Arctic. Two new strategies were introduced to optimize multimodal learning and enhance the model's predictive performance: (1) a feature-level, residual cross-modality attention fusion strategy, which effectively integrates feature maps from multiple modalities to capture complementary information and improve the model's ability to understand complex patterns and relationships within the data; (2) pre-trained unimodal learning followed by multimodal fine-tuning to alleviate high computing demand while achieving strong model performance. Experimental results demonstrated that our approach outperformed existing models adopting data-level fusion, feature-level convolutional fusion, and various attention fusion strategies, providing valuable insights into the efficient utilization of multimodal data for RTS mapping. This research contributes to our understanding of permafrost landforms and their environmental implications.", 'abstract_zh': '北极地区退化融沉（Retrogressive Thaw Slumps, RTS）的测绘：基于多尺度视觉变换器的Cascade Mask R-CNN模型研究', 'title_zh': '基于多尺度视觉变换器的多模态GeoAI模型：绘制北极冻土 thaw 地图'}
