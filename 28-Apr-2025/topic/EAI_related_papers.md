# Opportunistic Collaborative Planning with Large Vision Model Guided Control and Joint Query-Service Optimization 

**Title (ZH)**: 基于大型视觉模型引导控制与联合查询-服务优化的机会性协作规划 

**Authors**: Jiayi Chen, Shuai Wang, Guoliang Li, Wei Xu, Guangxu Zhu, Derrick Wing Kwan Ng, Chengzhong Xu  

**Link**: [PDF](https://arxiv.org/pdf/2504.18057)  

**Abstract**: Navigating autonomous vehicles in open scenarios is a challenge due to the difficulties in handling unseen objects. Existing solutions either rely on small models that struggle with generalization or large models that are resource-intensive. While collaboration between the two offers a promising solution, the key challenge is deciding when and how to engage the large model. To address this issue, this paper proposes opportunistic collaborative planning (OCP), which seamlessly integrates efficient local models with powerful cloud models through two key innovations. First, we propose large vision model guided model predictive control (LVM-MPC), which leverages the cloud for LVM perception and decision making. The cloud output serves as a global guidance for a local MPC, thereby forming a closed-loop perception-to-control system. Second, to determine the best timing for large model query and service, we propose collaboration timing optimization (CTO), including object detection confidence thresholding (ODCT) and cloud forward simulation (CFS), to decide when to seek cloud assistance and when to offer cloud service. Extensive experiments show that the proposed OCP outperforms existing methods in terms of both navigation time and success rate. 

**Abstract (ZH)**: 在开放场景中导航自主车辆是一项挑战，因为难以处理未见过的对象。现有的解决方案要么依赖于小型模型但泛化能力差，要么依赖于资源密集型的大模型。尽管两者之间的协作提供了有希望的解决方案，但关键挑战是如何决定何时以及如何激活大模型。为解决这个问题，本文提出了机会性协作规划（OCP），通过两种关键创新无缝地将高效的本地模型与强大的云模型相结合。首先，我们提出了大视图模型引导模型预测控制（LVM-MPC），该模型利用云进行LVM感知和决策。云端输出作为局部MPC的全局指导，从而形成一个从感知到控制的闭环系统。其次，为了确定大型模型查询和服务的最佳时机，我们提出了协作时间优化（CTO），包括对象检测置信阈值（ODCT）和云前向仿真（CFS），以决定何时寻求云辅助以及何时提供云服务。广泛的实验结果表明，提出的OCP在导航时间和成功率方面均优于现有方法。 

---
# Virtual Roads, Smarter Safety: A Digital Twin Framework for Mixed Autonomous Traffic Safety Analysis 

**Title (ZH)**: 虚拟道路，更智慧的安全：一种混合自主交通安全性分析的数字孪生框架 

**Authors**: Hao Zhang, Ximin Yue, Kexin Tian, Sixu Li, Keshu Wu, Zihao Li, Dominique Lord, Yang Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2504.17968)  

**Abstract**: This paper presents a digital-twin platform for active safety analysis in mixed traffic environments. The platform is built using a multi-modal data-enabled traffic environment constructed from drone-based aerial LiDAR, OpenStreetMap, and vehicle sensor data (e.g., GPS and inclinometer readings). High-resolution 3D road geometries are generated through AI-powered semantic segmentation and georeferencing of aerial LiDAR data. To simulate real-world driving scenarios, the platform integrates the CAR Learning to Act (CARLA) simulator, Simulation of Urban MObility (SUMO) traffic model, and NVIDIA PhysX vehicle dynamics engine. CARLA provides detailed micro-level sensor and perception data, while SUMO manages macro-level traffic flow. NVIDIA PhysX enables accurate modeling of vehicle behaviors under diverse conditions, accounting for mass distribution, tire friction, and center of mass. This integrated system supports high-fidelity simulations that capture the complex interactions between autonomous and conventional vehicles. Experimental results demonstrate the platform's ability to reproduce realistic vehicle dynamics and traffic scenarios, enhancing the analysis of active safety measures. Overall, the proposed framework advances traffic safety research by enabling in-depth, physics-informed evaluation of vehicle behavior in dynamic and heterogeneous traffic environments. 

**Abstract (ZH)**: 基于多模态数据的混合交通环境主动安全分析数字孪生平台 

---
# CIVIL: Causal and Intuitive Visual Imitation Learning 

**Title (ZH)**: 因果直观视觉imitation学习：CIVIL 

**Authors**: Yinlong Dai, Robert Ramirez Sanchez, Ryan Jeronimus, Shahabedin Sagheb, Cara M. Nunez, Heramb Nemlekar, Dylan P. Losey  

**Link**: [PDF](https://arxiv.org/pdf/2504.17959)  

**Abstract**: Today's robots learn new tasks by imitating human examples. However, this standard approach to visual imitation learning is fundamentally limited: the robot observes what the human does, but not why the human chooses those behaviors. Without understanding the features that factor into the human's decisions, robot learners often misinterpret the data and fail to perform the task when the environment changes. We therefore propose a shift in perspective: instead of asking human teachers just to show what actions the robot should take, we also enable humans to indicate task-relevant features using markers and language prompts. Our proposed algorithm, CIVIL, leverages this augmented data to filter the robot's visual observations and extract a feature representation that causally informs human actions. CIVIL then applies these causal features to train a transformer-based policy that emulates human behaviors without being confused by visual distractors. Our simulations, real-world experiments, and user study demonstrate that robots trained with CIVIL can learn from fewer human demonstrations and perform better than state-of-the-art baselines, especially in previously unseen scenarios. See videos at our project website: this https URL 

**Abstract (ZH)**: 今天的手动机器人通过模仿人类示例学习新任务。然而，这种视觉模仿学习的标准方法存在根本性的局限性：机器人只能观察人类做了什么，但无法了解人类为什么会选择这些行为。没有理解影响人类决策的特征，机器人往往误读数据，在环境变化时无法完成任务。因此，我们提出一种新的视角：不再只是要求人类教师展示机器人应采取的动作，我们还允许人类使用标记和语言提示来指出与任务相关的特征。我们提出的算法CIVIL利用这种增强的数据来过滤机器人的视觉观察，并提取一个因果指示人类行为的特征表示。CIVIL随后利用这些因果特征训练基于变换器的策略，使其模仿人类行为而不被视觉干扰所迷惑。我们的模拟、真实世界实验和用户研究显示，使用CIVIL训练的机器人可以从更少的人类示范中学习，并在前所未见的场景中表现优于最先进的基线方法。详见项目网站上的视频：this https URL 

---
# Learning Attentive Neural Processes for Planning with Pushing Actions 

**Title (ZH)**: 学习注意力神经过程以执行推行动作的规划 

**Authors**: Atharv Jain, Seiji Shaw, Nicholas Roy  

**Link**: [PDF](https://arxiv.org/pdf/2504.17924)  

**Abstract**: Our goal is to enable robots to plan sequences of tabletop actions to push a block with unknown physical properties to a desired goal pose on the table. We approach this problem by learning the constituent models of a Partially-Observable Markov Decision Process (POMDP), where the robot can observe the outcome of a push, but the physical properties of the block that govern the dynamics remain unknown. The pushing problem is a difficult POMDP to solve due to the challenge of state estimation. The physical properties have a nonlinear relationship with the outcomes, requiring computationally expensive methods, such as particle filters, to represent beliefs. Leveraging the Attentive Neural Process architecture, we propose to replace the particle filter with a neural network that learns the inference computation over the physical properties given a history of actions. This Neural Process is integrated into planning as the Neural Process Tree with Double Progressive Widening (NPT-DPW). Simulation results indicate that NPT-DPW generates more effective plans faster than traditional particle filter methods, even in complex pushing scenarios. 

**Abstract (ZH)**: 我们的目标是使机器人能够计划一序列桌面操作，以推动一个具有未知物理特性的立方体到达桌子上的目标姿态。我们通过学习部分可观测马尔可夫决策过程（POMDP）的组成部分模型来解决这个问题，在该过程中，机器人可以观察到推动的结果，但控制动力学的物理特性仍然未知。由于状态估计的挑战，推动问题是解决POMDP的一个难题。物理特性与结果之间存在非线性关系，需要使用计算密集型方法，例如粒子滤波器，来表示信念。利用注意力神经过程架构，我们提出用一个神经网络替换粒子滤波器，该神经网络可以在给定动作历史的情况下学习推断计算物理属性的过程。该神经过程与双重逐步扩展的神经过程树（NPT-DPW）结合，用于规划。模拟结果表明，与传统的粒子滤波器方法相比，NPT-DPW即使在复杂推动场景中也能更快生成更有效的规划。 

---
# Beyond Task and Motion Planning: Hierarchical Robot Planning with General-Purpose Policies 

**Title (ZH)**: 超越任务与运动规划：基于通用策略的层次化机器人规划 

**Authors**: Benned Hedegaard, Ziyi Yang, Yichen Wei, Ahmed Jaafar, Stefanie Tellex, George Konidaris, Naman Shah  

**Link**: [PDF](https://arxiv.org/pdf/2504.17901)  

**Abstract**: Task and motion planning is a well-established approach for solving long-horizon robot planning problems. However, traditional methods assume that each task-level robot action, or skill, can be reduced to kinematic motion planning. In this work, we address the challenge of planning with both kinematic skills and closed-loop motor controllers that go beyond kinematic considerations. We propose a novel method that integrates these controllers into motion planning using Composable Interaction Primitives (CIPs), enabling the use of diverse, non-composable pre-learned skills in hierarchical robot planning. Toward validating our Task and Skill Planning (TASP) approach, we describe ongoing robot experiments in real-world scenarios designed to demonstrate how CIPs can allow a mobile manipulator robot to effectively combine motion planning with general-purpose skills to accomplish complex tasks. 

**Abstract (ZH)**: 基于任务和动作规划的机器人长期规划方法已经成熟。然而，传统方法假设每个任务级机器人操作或技能都可以简化为动力学运动规划。本工作中，我们解决了同时规划动力学技能和超越动力学考虑的闭环电机控制器的挑战。我们提出了一种新的方法，通过可组合交互本原（CIPs）将这些控制器整合到运动规划中，从而使不同且不可组合的先验学习技能能够用于层次化机器人规划。为了验证我们的任务和技能规划（TASP）方法，我们描述了在真实场景中进行的机器人实验，旨在展示CIPs如何使移动 manipulator 机器人有效地将运动规划与通用技能结合以完成复杂任务。 

---
# Autonomous Navigation Of Quadrupeds Using Coverage Path Planning 

**Title (ZH)**: 四足机器人的覆盖路径规划自主导航 

**Authors**: Alexander James Becoy, Kseniia Khomenko, Luka Peternel, Raj Thilak Rajan  

**Link**: [PDF](https://arxiv.org/pdf/2504.17880)  

**Abstract**: This paper proposes a novel method of coverage path planning for the purpose of scanning an unstructured environment autonomously. The method uses the morphological skeleton of the prior 2D navigation map via SLAM to generate a sequence of points of interest (POIs). This sequence is then ordered to create an optimal path given the robot's current position. To control the high-level operation, a finite state machine is used to switch between two modes: navigating towards a POI using Nav2, and scanning the local surrounding. We validate the method in a leveled indoor obstacle-free non-convex environment on time efficiency and reachability over five trials. The map reader and the path planner can quickly process maps of width and height ranging between [196,225] pixels and [185,231] pixels in 2.52 ms/pixel and 1.7 ms/pixel, respectively, where their computation time increases with 22.0 ns/pixel and 8.17 $\mu$s/pixel, respectively. The robot managed to reach 86.5\% of all waypoints over all five runs. The proposed method suffers from drift occurring in the 2D navigation map. 

**Abstract (ZH)**: 本文提出了一种新型的覆盖率路径规划方法，用于自主扫描未结构化环境。该方法通过SLAM之前的2D导航地图的形态骨架生成一系列兴趣点（POIs），并根据机器人当前位置对这些点进行排序以生成最优路径。为了控制高级操作，使用有限状态机在两种模式之间切换：使用Nav2朝向POI导航，以及扫描局部周围环境。我们在一个平坦的室内无障碍非凸环境中进行了五次试验，验证了该方法在时间效率和可达性方面的表现。地图读取器和路径规划器分别以每像素2.52毫秒和1.7毫秒的速度处理宽度和高度在[196,225]像素和[185,231]像素之间的地图，其计算时间分别增加22.0纳秒/像素和8.17微秒/像素。机器人在五次运行中成功到达了所有航点的86.5%。提出的路径规划方法受到2D导航地图中漂移的影响。 

---
# Flow Matching Ergodic Coverage 

**Title (ZH)**: 流匹配遍历覆盖 

**Authors**: Max Muchen Sun, Allison Pinosky, Todd Murphey  

**Link**: [PDF](https://arxiv.org/pdf/2504.17872)  

**Abstract**: Ergodic coverage effectively generates exploratory behaviors for embodied agents by aligning the spatial distribution of the agent's trajectory with a target distribution, where the difference between these two distributions is measured by the ergodic metric. However, existing ergodic coverage methods are constrained by the limited set of ergodic metrics available for control synthesis, fundamentally limiting their performance. In this work, we propose an alternative approach to ergodic coverage based on flow matching, a technique widely used in generative inference for efficient and scalable sampling. We formally derive the flow matching problem for ergodic coverage and show that it is equivalent to a linear quadratic regulator problem with a closed-form solution. Our formulation enables alternative ergodic metrics from generative inference that overcome the limitations of existing ones. These metrics were previously infeasible for control synthesis but can now be supported with no computational overhead. Specifically, flow matching with the Stein variational gradient flow enables control synthesis directly over the score function of the target distribution, improving robustness to the unnormalized distributions; on the other hand, flow matching with the Sinkhorn divergence flow enables an optimal transport-based ergodic metric, improving coverage performance on non-smooth distributions with irregular supports. We validate the improved performance and competitive computational efficiency of our method through comprehensive numerical benchmarks and across different nonlinear dynamics. We further demonstrate the practicality of our method through a series of drawing and erasing tasks on a Franka robot. 

**Abstract (ZH)**: 基于流匹配的遍历覆盖：一种生成推断中的流动匹配方法以生成体表代理的探索行为 

---
# A Large Vision-Language Model based Environment Perception System for Visually Impaired People 

**Title (ZH)**: 基于大型视觉-语言模型的视觉障碍人士环境感知系统 

**Authors**: Zezhou Chen, Zhaoxiang Liu, Kai Wang, Kohou Wang, Shiguo Lian  

**Link**: [PDF](https://arxiv.org/pdf/2504.18027)  

**Abstract**: It is a challenging task for visually impaired people to perceive their surrounding environment due to the complexity of the natural scenes. Their personal and social activities are thus highly limited. This paper introduces a Large Vision-Language Model(LVLM) based environment perception system which helps them to better understand the surrounding environment, by capturing the current scene they face with a wearable device, and then letting them retrieve the analysis results through the device. The visually impaired people could acquire a global description of the scene by long pressing the screen to activate the LVLM output, retrieve the categories of the objects in the scene resulting from a segmentation model by tapping or swiping the screen, and get a detailed description of the objects they are interested in by double-tapping the screen. To help visually impaired people more accurately perceive the world, this paper proposes incorporating the segmentation result of the RGB image as external knowledge into the input of LVLM to reduce the LVLM's hallucination. Technical experiments on POPE, MME and LLaVA-QA90 show that the system could provide a more accurate description of the scene compared to Qwen-VL-Chat, exploratory experiments show that the system helps visually impaired people to perceive the surrounding environment effectively. 

**Abstract (ZH)**: 视觉受损人士基于大型视觉-语言模型的环境感知系统及其应用研究 

---
# RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation 

**Title (ZH)**: RSRNav: 基于空间关系推理的图像目标导航 

**Authors**: Zheng Qin, Le Wang, Yabing Wang, Sanping Zhou, Gang Hua, Wei Tang  

**Link**: [PDF](https://arxiv.org/pdf/2504.17991)  

**Abstract**: Recent image-goal navigation (ImageNav) methods learn a perception-action policy by separately capturing semantic features of the goal and egocentric images, then passing them to a policy network. However, challenges remain: (1) Semantic features often fail to provide accurate directional information, leading to superfluous actions, and (2) performance drops significantly when viewpoint inconsistencies arise between training and application. To address these challenges, we propose RSRNav, a simple yet effective method that reasons spatial relationships between the goal and current observations as navigation guidance. Specifically, we model the spatial relationship by constructing correlations between the goal and current observations, which are then passed to the policy network for action prediction. These correlations are progressively refined using fine-grained cross-correlation and direction-aware correlation for more precise navigation. Extensive evaluation of RSRNav on three benchmark datasets demonstrates superior navigation performance, particularly in the "user-matched goal" setting, highlighting its potential for real-world applications. 

**Abstract (ZH)**: Recent Image-Goal Navigation Methods Learn a Perception-Action Policy by Reasoning Spatial Relationships 

---
# High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures 

**Title (ZH)**: Spot上高性能强化学习：基于分布性度量优化模拟参数 

**Authors**: A. J Miller, Fangzhou Yu, Michael Brauckmann, Farbod Farshidian  

**Link**: [PDF](https://arxiv.org/pdf/2504.17857)  

**Abstract**: This work presents an overview of the technical details behind a high performance reinforcement learning policy deployment with the Spot RL Researcher Development Kit for low level motor access on Boston Dynamics Spot. This represents the first public demonstration of an end to end end reinforcement learning policy deployed on Spot hardware with training code publicly available through Nvidia IsaacLab and deployment code available through Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean Discrepancy to quantify the distributional dissimilarity of data collected on hardware and in simulation to measure our sim2real gap. We use these measures as a scoring function for the Covariance Matrix Adaptation Evolution Strategy to optimize simulated parameters that are unknown or difficult to measure from Spot. Our procedure for modeling and training produces high quality reinforcement learning policies capable of multiple gaits, including a flight phase. We deploy policies capable of over 5.2ms locomotion, more than triple Spots default controller maximum speed, robustness to slippery surfaces, disturbance rejection, and overall agility previously unseen on Spot. We detail our method and release our code to support future work on Spot with the low level API. 

**Abstract (ZH)**: 基于Boston Dynamics Spot的高性能强化学习策略部署技术细节综述：低层次电机访问下的Spot RL Researcher Development Kit首次公开展示及Sim2Real差距量化 

---
# Learning Underwater Active Perception in Simulation 

**Title (ZH)**: 基于仿真的水下主动感知学习 

**Authors**: Alexandre Cardaillac, Donald G. Dansereau  

**Link**: [PDF](https://arxiv.org/pdf/2504.17817)  

**Abstract**: When employing underwater vehicles for the autonomous inspection of assets, it is crucial to consider and assess the water conditions. Indeed, they have a significant impact on the visibility, which also affects robotic operations. Turbidity can jeopardise the whole mission as it may prevent correct visual documentation of the inspected structures. Previous works have introduced methods to adapt to turbidity and backscattering, however, they also include manoeuvring and setup constraints. We propose a simple yet efficient approach to enable high-quality image acquisition of assets in a broad range of water conditions. This active perception framework includes a multi-layer perceptron (MLP) trained to predict image quality given a distance to a target and artificial light intensity. We generated a large synthetic dataset including ten water types with different levels of turbidity and backscattering. For this, we modified the modelling software Blender to better account for the underwater light propagation properties. We validated the approach in simulation and showed significant improvements in visual coverage and quality of imagery compared to traditional approaches. The project code is available on our project page at this https URL. 

**Abstract (ZH)**: 使用水下车辆进行资产自主检查时，考虑和评估水况至关重要。确实，水况对能见度有重大影响，也影响着机器人的操作。悬浮物质可能会危及整个任务，因为它可能妨碍对检查结构的正确视觉记录。先前的工作引入了适应悬浮物质和后向散射的方法，然而这些方法也包括机动和设置约束。我们提出了一种简单而有效的方法，可以在广泛水况条件下实现高质量的图像获取。该主动感知框架包括一个训练用于根据目标距离和人工光强度预测图像质量的多层感知器（MLP）。我们生成了一个包含不同悬浮物质和后向散射水平的十种水类型的大型合成数据集。为此，我们修改了建模软件Blender，以更好地考虑水下光传播特性。我们在模拟中验证了该方法，并展示了与传统方法相比在视觉覆盖范围和图像质量上显著改进。项目代码可在我们的项目页面（此链接）获取。 

---
# Near-Driven Autonomous Rover Navigation in Complex Environments: Extensions to Urban Search-and-Rescue and Industrial Inspection 

**Title (ZH)**: 近地驱动自主探测车在复杂环境中的自主导航：面向城市搜索与救援及工业检查的应用扩展 

**Authors**: Dhadkan Shrestha, Lincoln Bhattarai  

**Link**: [PDF](https://arxiv.org/pdf/2504.17794)  

**Abstract**: This paper explores the use of an extended neuroevolutionary approach, based on NeuroEvolution of Augmenting Topologies (NEAT), for autonomous robots in dynamic environments associated with hazardous tasks like firefighting, urban search-and-rescue (USAR), and industrial inspections. Building on previous research, it expands the simulation environment to larger and more complex settings, demonstrating NEAT's adaptability across different applications. By integrating recent advancements in NEAT and reinforcement learning, the study uses modern simulation frameworks for realism and hybrid algorithms for optimization. Experimental results show that NEAT-evolved controllers achieve success rates comparable to state-of-the-art deep reinforcement learning methods, with superior structural adaptability. The agents reached ~80% success in outdoor tests, surpassing baseline models. The paper also highlights the benefits of transfer learning among tasks and evaluates the effectiveness of NEAT in complex 3D navigation. Contributions include evaluating NEAT for diverse autonomous applications and discussing real-world deployment considerations, emphasizing the approach's potential as an alternative or complement to deep reinforcement learning in autonomous navigation tasks. 

**Abstract (ZH)**: 基于NEAT扩展神经进化方法在动态环境自主机器人中的应用：以消防、都市搜救和工业检查为例 

---
# MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind 

**Title (ZH)**: 多模态推理与心理理论增强的狼人代理：MultiMind 

**Authors**: Zheng Zhang, Nuoqian Xiao, Qi Chai, Deheng Ye, Hao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2504.18039)  

**Abstract**: Large Language Model (LLM) agents have demonstrated impressive capabilities in social deduction games (SDGs) like Werewolf, where strategic reasoning and social deception are essential. However, current approaches remain limited to textual information, ignoring crucial multimodal cues such as facial expressions and tone of voice that humans naturally use to communicate. Moreover, existing SDG agents primarily focus on inferring other players' identities without modeling how others perceive themselves or fellow players. To address these limitations, we use One Night Ultimate Werewolf (ONUW) as a testbed and present MultiMind, the first framework integrating multimodal information into SDG agents. MultiMind processes facial expressions and vocal tones alongside verbal content, while employing a Theory of Mind (ToM) model to represent each player's suspicion levels toward others. By combining this ToM model with Monte Carlo Tree Search (MCTS), our agent identifies communication strategies that minimize suspicion directed at itself. Through comprehensive evaluation in both agent-versus-agent simulations and studies with human players, we demonstrate MultiMind's superior performance in gameplay. Our work presents a significant advancement toward LLM agents capable of human-like social reasoning across multimodal domains. 

**Abstract (ZH)**: 大型语言模型代理在社会推理游戏中的多模态应用：MultiMind框架克服了现有方法的局限性 

---
# ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing 

**Title (ZH)**: ApproXAI：使用近似计算实现可解释人工智能的能源高效硬件加速 

**Authors**: Ayesha Siddique, Khurram Khalil, Khaza Anuarul Hoque  

**Link**: [PDF](https://arxiv.org/pdf/2504.17929)  

**Abstract**: Explainable artificial intelligence (XAI) enhances AI system transparency by framing interpretability as an optimization problem. However, this approach often necessitates numerous iterations of computationally intensive operations, limiting its applicability in real-time scenarios. While recent research has focused on XAI hardware acceleration on FPGAs and TPU, these methods do not fully address energy efficiency in real-time settings. To address this limitation, we propose XAIedge, a novel framework that leverages approximate computing techniques into XAI algorithms, including integrated gradients, model distillation, and Shapley analysis. XAIedge translates these algorithms into approximate matrix computations and exploits the synergy between convolution, Fourier transform, and approximate computing paradigms. This approach enables efficient hardware acceleration on TPU-based edge devices, facilitating faster real-time outcome interpretations. Our comprehensive evaluation demonstrates that XAIedge achieves a $2\times$ improvement in energy efficiency compared to existing accurate XAI hardware acceleration techniques while maintaining comparable accuracy. These results highlight the potential of XAIedge to significantly advance the deployment of explainable AI in energy-constrained real-time applications. 

**Abstract (ZH)**: 可解释的人工智能（XAI）通过将解释性问题作为优化问题来增强AI系统的透明度。然而，这种方法往往需要多次进行计算密集型操作，限制了其在实时场景中的应用。尽管最近的研究集中在FPGA和TPU上的XAI硬件加速上，但这些方法并未完全解决实时场景中的能效问题。为解决这一限制，我们提出了XAIedge框架，该框架利用近似计算技术到XAI算法中，包括集成梯度、模型蒸馏和Shapley分析。XAIedge将这些算法转化为近似矩阵计算，并利用卷积、傅里叶变换和近似计算范式的协同作用。该方法能够在基于TPU的边缘设备上实现高效的硬件加速，促进更快的实时结果解释。我们的综合评估表明，XAIedge在能效方面实现了比现有精确XAI硬件加速技术两倍的改进，同时保持了相当的准确性。这些结果突显了XAIedge在能源受限的实时应用中显著推进可解释AI部署的潜力。 

---
# Action Flow Matching for Continual Robot Learning 

**Title (ZH)**: 连续机器人学习中的动作流程匹配 

**Authors**: Alejandro Murillo-Gonzalez, Lantao Liu  

**Link**: [PDF](https://arxiv.org/pdf/2504.18471)  

**Abstract**: Continual learning in robotics seeks systems that can constantly adapt to changing environments and tasks, mirroring human adaptability. A key challenge is refining dynamics models, essential for planning and control, while addressing issues such as safe adaptation, catastrophic forgetting, outlier management, data efficiency, and balancing exploration with exploitation -- all within task and onboard resource constraints. Towards this goal, we introduce a generative framework leveraging flow matching for online robot dynamics model alignment. Rather than executing actions based on a misaligned model, our approach refines planned actions to better match with those the robot would take if its model was well aligned. We find that by transforming the actions themselves rather than exploring with a misaligned model -- as is traditionally done -- the robot collects informative data more efficiently, thereby accelerating learning. Moreover, we validate that the method can handle an evolving and possibly imperfect model while reducing, if desired, the dependency on replay buffers or legacy model snapshots. We validate our approach using two platforms: an unmanned ground vehicle and a quadrotor. The results highlight the method's adaptability and efficiency, with a record 34.2\% higher task success rate, demonstrating its potential towards enabling continual robot learning. Code: this https URL. 

**Abstract (ZH)**: 机器人领域的连续学习寻求能够不断适应变化环境和任务的系统，模拟人类的适应能力。一个关键挑战是在处理安全适应、灾难性遗忘、离群值管理、数据效率以及在任务和机载资源约束下平衡探索与利用等问题的同时，精炼动力学模型，这对于规划和控制至关重要。为实现这一目标，我们引入了一种生成框架，利用流匹配进行在线机器人动力学模型对齐。我们的方法不是基于错配的模型执行动作，而是通过对计划动作本身进行改进，使其更好地与机器人在其模型对齐良好时可能采取的动作匹配。我们发现，通过直接变换动作本身而不是使用错配的模型进行探索，机器人可以更有效地收集信息性数据，从而加速学习。此外，我们验证了该方法可以处理不断变化且可能不完备的模型，同时在需要时减少对重放缓冲区或遗留模型快照的依赖。我们使用两个平台验证了该方法：无人驾驶地面车辆和四旋翼无人机。结果强调了该方法的适应性和效率，最高任务成功率提升了34.2%，展示了其在实现连续机器人学习方面的潜力。代码：详见链接。 

---
# Aligning Language Models for Icelandic Legal Text Summarization 

**Title (ZH)**: 冰岛法律文本摘要的语言模型对齐 

**Authors**: Þórir Hrafn Harðarson, Hrafn Loftsson, Stefán Ólafsson  

**Link**: [PDF](https://arxiv.org/pdf/2504.18180)  

**Abstract**: The integration of language models in the legal domain holds considerable promise for streamlining processes and improving efficiency in managing extensive workloads. However, the specialized terminology, nuanced language, and formal style of legal texts can present substantial challenges. This study examines whether preference-based training techniques, specifically Reinforcement Learning from Human Feedback and Direct Preference Optimization, can enhance models' performance in generating Icelandic legal summaries that align with domain-specific language standards and user preferences. We compare models fine-tuned with preference training to those using conventional supervised learning. Results indicate that preference training improves the legal accuracy of generated summaries over standard fine-tuning but does not significantly enhance the overall quality of Icelandic language usage. Discrepancies between automated metrics and human evaluations further underscore the importance of qualitative assessment in developing language models for the legal domain. 

**Abstract (ZH)**: 语言模型在法律领域的集成具有显著潜力，可以简化流程并提高管理大量工作负载的效率。然而，法律文本的专业术语、细腻的语言和正式风格为这一过程带来了显著挑战。本研究考察了基于偏好的训练技术，即人类反馈强化学习和直接偏好优化，是否能提高模型生成符合法律领域语言标准和用户偏好的冰岛法律摘要的能力。我们将使用偏好训练微调的模型与使用传统监督学习微调的模型进行对比。结果表明，偏好训练能够提高生成摘要的法律准确性，但并未显著提升冰岛语言使用的整体质量。自动化评估指标与人类评估之间的差异进一步强调了在开发法律领域语言模型时进行定性评估的重要性。 

---
