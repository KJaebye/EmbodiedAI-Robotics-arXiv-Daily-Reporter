# Exploring Accelerated Skill Acquisition via Tandem Training for Colonoscopy 

**Title (ZH)**: 通过 tandem 训练加速内镜检查技能获取的研究eksploring 加速技能获取的 tandem 训练在内镜检查中的应用 

**Authors**: Olivia Richards, Keith L. Obstein, Nabil Simaan  

**Link**: [PDF](https://arxiv.org/pdf/2506.24046)  

**Abstract**: New endoscopists require a large volume of expert-proctored colonoscopies to attain minimal competency. Developing multi-fingered, synchronized control of a colonoscope requires significant time and exposure to the device. Current training methods inhibit this development by relying on tool hand-off for expert demonstrations. There is a need for colonoscopy training tools that enable in-hand expert guidance in real-time. We present a new concept of a tandem training system that uses a telemanipulated preceptor colonoscope to guide novice users as they perform a colonoscopy. This system is capable of dual-control and can automatically toggle between expert and novice control of a standard colonoscope's angulation control wheels. Preliminary results from a user study with novice and expert users show the effectiveness of this device as a skill acquisition tool. We believe that this device has the potential to accelerate skill acquisition for colonoscopy and, in the future, enable individualized instruction and responsive teaching through bidirectional actuation. 

**Abstract (ZH)**: 新内镜医生需要进行大量专家监控的结肠镜检查以达到最低熟练度。发展多指协调控制结肠镜需要大量时间和设备接触。当前的培训方法通过依赖工具传递专家演示，限制了这一发展。需要能够提供实时在手专家指导的结肠镜培训工具。我们提出了一种新的联合培训系统概念，该系统使用远程操作的教师结肠镜来指导新手在进行结肠镜检查时操作。该系统具有双控功能，并能够自动在标准结肠镜的角度控制轮的专家控制和新手控制之间切换。初步用户研究结果表明，该设备作为技能习得工具的有效性。我们相信，该设备有可能加速结肠镜检查技能的习得，并在未来通过双向操作实现个性化教学和响应性教学。 

---
# Predictive Risk Analysis and Safe Trajectory Planning for Intelligent and Connected Vehicles 

**Title (ZH)**: 智能connected车辆的预测风险分析与安全轨迹规划 

**Authors**: Zeyu Han, Mengchi Cai, Chaoyi Chen, Qingwen Meng, Guangwei Wang, Ying Liu, Qing Xu, Jianqiang Wang, Keqiang Li  

**Link**: [PDF](https://arxiv.org/pdf/2506.23999)  

**Abstract**: The safe trajectory planning of intelligent and connected vehicles is a key component in autonomous driving technology. Modeling the environment risk information by field is a promising and effective approach for safe trajectory planning. However, existing risk assessment theories only analyze the risk by current information, ignoring future prediction. This paper proposes a predictive risk analysis and safe trajectory planning framework for intelligent and connected vehicles. This framework first predicts future trajectories of objects by a local risk-aware algorithm, following with a spatiotemporal-discretised predictive risk analysis using the prediction results. Then the safe trajectory is generated based on the predictive risk analysis. Finally, simulation and vehicle experiments confirm the efficacy and real-time practicability of our approach. 

**Abstract (ZH)**: 智能网联车辆的安全轨迹规划是自主驾驶技术的关键组成部分。基于场域建模的预测风险分析与安全轨迹规划框架是一个有前景且有效的方法。然而，现有风险评估理论仅通过当下的信息来分析风险，忽视了未来预测。本文提出了一种针对智能网联车辆的预测风险分析与安全轨迹规划框架。该框架首先通过局部风险感知算法预测物体的未来轨迹，接着利用预测结果进行时空离散化的预测风险分析，然后基于预测风险分析生成安全轨迹。最后，模拟和车辆实验验证了该方法的有效性和实时实用性。 

---
# MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments 

**Title (ZH)**: MGPRL：基于多 Gaussian 过程的分布式多机器人室内相对定位方法 

**Authors**: Sai Krishna Ghanta, Ramviyas Parasuraman  

**Link**: [PDF](https://arxiv.org/pdf/2506.23514)  

**Abstract**: Relative localization is a crucial capability for multi-robot systems operating in GPS-denied environments. Existing approaches for multi-robot relative localization often depend on costly or short-range sensors like cameras and LiDARs. Consequently, these approaches face challenges such as high computational overhead (e.g., map merging) and difficulties in disjoint environments. To address this limitation, this paper introduces MGPRL, a novel distributed framework for multi-robot relative localization using convex-hull of multiple Wi-Fi access points (AP). To accomplish this, we employ co-regionalized multi-output Gaussian Processes for efficient Radio Signal Strength Indicator (RSSI) field prediction and perform uncertainty-aware multi-AP localization, which is further coupled with weighted convex hull-based alignment for robust relative pose estimation. Each robot predicts the RSSI field of the environment by an online scan of APs in its environment, which are utilized for position estimation of multiple APs. To perform relative localization, each robot aligns the convex hull of its predicted AP locations with that of the neighbor robots. This approach is well-suited for devices with limited computational resources and operates solely on widely available Wi-Fi RSSI measurements without necessitating any dedicated pre-calibration or offline fingerprinting. We rigorously evaluate the performance of the proposed MGPRL in ROS simulations and demonstrate it with real-world experiments, comparing it against multiple state-of-the-art approaches. The results showcase that MGPRL outperforms existing methods in terms of localization accuracy and computational efficiency. Finally, we open source MGPRL as a ROS package this https URL. 

**Abstract (ZH)**: 基于多Wi-Fi接入点凸包的多机器人相对定位框架 

---
# Risk-Based Filtering of Valuable Driving Situations in the Waymo Open Motion Dataset 

**Title (ZH)**: Waymo 开放运动数据集中基于风险的有价值驾驶情境筛选 

**Authors**: Tim Puphal, Vipul Ramtekkar, Kenji Nishimiya  

**Link**: [PDF](https://arxiv.org/pdf/2506.23433)  

**Abstract**: Improving automated vehicle software requires driving data rich in valuable road user interactions. In this paper, we propose a risk-based filtering approach that helps identify such valuable driving situations from large datasets. Specifically, we use a probabilistic risk model to detect high-risk situations. Our method stands out by considering a) first-order situations (where one vehicle directly influences another and induces risk) and b) second-order situations (where influence propagates through an intermediary vehicle). In experiments, we show that our approach effectively selects valuable driving situations in the Waymo Open Motion Dataset. Compared to the two baseline interaction metrics of Kalman difficulty and Tracks-To-Predict (TTP), our filtering approach identifies complex and complementary situations, enriching the quality in automated vehicle testing. The risk data is made open-source: this https URL. 

**Abstract (ZH)**: 改进自动化车辆软件需要丰富的有价值道路用户交互数据。本文提出了一种基于风险的过滤方法，该方法有助于从大数据集中识别这样的有价值驾驶情境。具体来说，我们使用概率风险模型来检测高风险情境。我们的方法特别之处在于考虑了a) 一级情境（一辆车直接影响另一辆车并引发风险）和b) 二级情境（影响通过中介车辆传递）。在实验中，我们展示了我们的方法有效地从沃姆开放运动数据集中选择有价值的驾驶情境。与卡尔曼难度和预测轨迹数（TTP）这两种基线交互指标相比，我们的过滤方法识别出复杂且互补的情境，丰富了自动化车辆测试的质量。风险数据已开源：[请访问链接]。 

---
# Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models 

**Title (ZH)**: 模式崩塌现象：联合轨迹预测模型中关键相互作用的评估 

**Authors**: Maarten Hugenholtz, Anna Meszaros, Jens Kober, Zlatan Ajanovic  

**Link**: [PDF](https://arxiv.org/pdf/2506.23164)  

**Abstract**: Autonomous Vehicle decisions rely on multimodal prediction models that account for multiple route options and the inherent uncertainty in human behavior. However, models can suffer from mode collapse, where only the most likely mode is predicted, posing significant safety risks. While existing methods employ various strategies to generate diverse predictions, they often overlook the diversity in interaction modes among agents. Additionally, traditional metrics for evaluating prediction models are dataset-dependent and do not evaluate inter-agent interactions quantitatively. To our knowledge, none of the existing metrics explicitly evaluates mode collapse. In this paper, we propose a novel evaluation framework that assesses mode collapse in joint trajectory predictions, focusing on safety-critical interactions. We introduce metrics for mode collapse, mode correctness, and coverage, emphasizing the sequential dimension of predictions. By testing four multi-agent trajectory prediction models, we demonstrate that mode collapse indeed happens. When looking at the sequential dimension, although prediction accuracy improves closer to interaction events, there are still cases where the models are unable to predict the correct interaction mode, even just before the interaction mode becomes inevitable. We hope that our framework can help researchers gain new insights and advance the development of more consistent and accurate prediction models, thus enhancing the safety of autonomous driving systems. 

**Abstract (ZH)**: 自主驾驶车辆决策依赖于多模态预测模型，这些模型要考虑多种路线选项和人类行为的固有不确定性。然而，模型可能会遭受模式塌陷的问题，即仅预测最有可能的模式，这会带来重大安全风险。尽管现有方法采用了各种策略来生成多样化的预测，但它们往往忽略了代理间交互模式的多样性。此外，传统评估预测模型的指标依赖于数据集，不能定量地评估代理间交互。据我们所知，现有的指标都没有明确评估模式塌陷。在本文中，我们提出了一种新的评估框架，用于评估联合轨迹预测中的模式塌陷，重点关注安全性关键的交互。我们引入了模式塌陷、模式正确性和覆盖率的指标，强调预测的序列维度。通过测试四种多代理轨迹预测模型，我们表明模式塌陷确实会发生。在序列维度上，尽管预测准确性在接近交互事件时有所提高，但在某些情况下，模型仍然无法预测正确的交互模式，即使在交互模式变得不可避免之前也是如此。我们希望我们的框架能帮助研究人员获得新的见解，推进更一致和准确的预测模型的发展，从而提高自主驾驶系统的安全性。 

---
# SPICE-HL3: Single-Photon, Inertial, and Stereo Camera dataset for Exploration of High-Latitude Lunar Landscapes 

**Title (ZH)**: SPICE-HL3：用于高纬度月球地貌探索的单光子、惯性和立体相机数据集 

**Authors**: David Rodríguez-Martínez, Dave van der Meer, Junlin Song, Abishek Bera, C.J. Pérez-del-Pulgar, Miguel Angel Olivares-Mendez  

**Link**: [PDF](https://arxiv.org/pdf/2506.22956)  

**Abstract**: Exploring high-latitude lunar regions presents an extremely challenging visual environment for robots. The low sunlight elevation angle and minimal light scattering result in a visual field dominated by a high dynamic range featuring long, dynamic shadows. Reproducing these conditions on Earth requires sophisticated simulators and specialized facilities. We introduce a unique dataset recorded at the LunaLab from the SnT - University of Luxembourg, an indoor test facility designed to replicate the optical characteristics of multiple lunar latitudes. Our dataset includes images, inertial measurements, and wheel odometry data from robots navigating seven distinct trajectories under multiple illumination scenarios, simulating high-latitude lunar conditions from dawn to night time with and without the aid of headlights, resulting in 88 distinct sequences containing a total of 1.3M images. Data was captured using a stereo RGB-inertial sensor, a monocular monochrome camera, and for the first time, a novel single-photon avalanche diode (SPAD) camera. We recorded both static and dynamic image sequences, with robots navigating at slow (5 cm/s) and fast (50 cm/s) speeds. All data is calibrated, synchronized, and timestamped, providing a valuable resource for validating perception tasks from vision-based autonomous navigation to scientific imaging for future lunar missions targeting high-latitude regions or those intended for robots operating across perceptually degraded environments. The dataset can be downloaded from this https URL, and a visual overview is available at this https URL. All supplementary material can be found at this https URL. 

**Abstract (ZH)**: 探索高纬度月球区域为机器人提供了极其具有挑战性的视觉环境。低太阳高度角和 minimal 光散射导致视野中出现高动态范围，特征为长时间、动态的阴影。在地球上重现这些条件需要复杂模拟器和专门的设施。我们介绍了由卢森堡大学 SnT 计算机实验室在 LunaLab 记录的独特数据集，这是一个旨在模拟各种纬度月球光学特性的室内测试设施。该数据集包括机器人在多种照明场景下沿七个不同轨迹导航的图像、惯性测量和轮 odometry 数据，模拟从黎明到夜晚的高纬度月球条件，有和没有车头灯辅助，共生成 88 个序列，包含 130 万张图像。数据使用立体 RGB-惯性传感器、单目黑白相机以及首次使用新型单光子雪崩二极管 (SPAD) 相机捕获。我们记录了静态和动态图像序列，机器人以慢速（5 cm/s）和快速（50 cm/s）速度导航。所有数据均已校准、同步并带有时间戳，为验证基于视觉的自主导航感知任务以及面向高纬度区域的科学成像任务提供了宝贵的资源，旨在使机器人能够在感知退化的环境中操作。数据集可以从以下链接下载，视觉概述可在以下链接查看，所有辅助材料可在以下链接找到。 

---
# STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems 

**Title (ZH)**: STCLocker: 自动驾驶系统中的死锁避免测试 

**Authors**: Mingfei Cheng, Renzhi Wang, Xiaofei Xie, Yuan Zhou, Lei Ma  

**Link**: [PDF](https://arxiv.org/pdf/2506.23995)  

**Abstract**: Autonomous Driving System (ADS) testing is essential to ensure the safety and reliability of autonomous vehicles (AVs) before deployment. However, existing techniques primarily focus on evaluating ADS functionalities in single-AV settings. As ADSs are increasingly deployed in multi-AV traffic, it becomes crucial to assess their cooperative performance, particularly regarding deadlocks, a fundamental coordination failure in which multiple AVs enter a circular waiting state indefinitely, resulting in motion planning failures. Despite its importance, the cooperative capability of ADSs to prevent deadlocks remains insufficiently underexplored. To address this gap, we propose the first dedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique, STCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs controlled by the ADS under test are in a circular wait state. STCLocker consists of three key components: Deadlock Oracle, Conflict Feedback, and Conflict-aware Scenario Generation. Deadlock Oracle provides a reliable black-box mechanism for detecting deadlock cycles among multiple AVs within a given scenario. Conflict Feedback and Conflict-aware Scenario Generation collaborate to actively guide AVs into simultaneous competition over spatial conflict resources (i.e., shared passing regions) and temporal competitive behaviors (i.e., reaching the conflict region at the same time), thereby increasing the effectiveness of generating conflict-prone deadlocks. We evaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA, a module-based ADS supporting cooperative communication. Experimental results show that, on average, STCLocker generates more DLS than the best-performing baseline. 

**Abstract (ZH)**: 自主驾驶系统（ADS）测试对于确保自主车辆（AV）在部署前的安全性和可靠性至关重要。然而，现有的技术主要集中在评估ADS在单一AV环境下的功能。随着ADS在多AV交通中的广泛应用，评估其协同性能变得尤为关键，特别是关于死锁的问题，即多个AV进入无限循环等待状态的根本协调失败，导致运动规划失败。尽管其重要性不言而喻，但ADS防止死锁的协同能力仍存在不足。为填补这一空白，我们提出了第一个专门的空间-时间冲突引导死锁避免测试技术——STCLocker，用于生成死锁场景（DLS），其中一组由测试ADS控制的AV处于循环等待状态。STCLocker由三个关键组件组成：死锁预言机、冲突反馈和冲突感知场景生成。死锁预言机提供了一种可靠的手动机制，用于检测给定场景中多个AV间的死锁循环。冲突反馈和冲突感知场景生成协作，主动引导AV同时竞争空间冲突资源（即共享通行区域）和时间的竞争行为（即同时到达冲突区域），从而提高生成冲突性死锁的有效性。我们在两种类型的ADS上评估了STCLocker：Roach，一个端到端的ADS，和OpenCDA，一个支持协同通信的模块化ADS。实验结果表明，与最佳基线相比，STCLocker平均生成更多的DLS。 

---
# Harnessing AI Agents to Advance Research on Refugee Child Mental Health 

**Title (ZH)**: 利用AI代理推动难民儿童心理健康研究进展 

**Authors**: Aditya Shrivastava, Komal Gupta, Shraddha Arora  

**Link**: [PDF](https://arxiv.org/pdf/2506.23992)  

**Abstract**: The international refugee crisis deepens, exposing millions of dis placed children to extreme psychological trauma. This research suggests a com pact, AI-based framework for processing unstructured refugee health data and distilling knowledge on child mental health. We compare two Retrieval-Aug mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to determine how well they process challenging humanitarian datasets while avoid ing hallucination hazards. By combining cutting-edge AI methods with migration research and child psychology, this study presents a scalable strategy to assist policymakers, mental health practitioners, and humanitarian agencies to better assist displaced children and recognize their mental wellbeing. In total, both the models worked properly but significantly Deepseek R1 is superior to Zephyr with an accuracy of answer relevance 0.91 

**Abstract (ZH)**: 国际难民危机加深，将数百万流离失所儿童置于极端心理创伤之中。本研究提出了一种基于AI的紧凑框架，用于处理无结构的难民健康数据，并提炼儿童心理健康知识。该研究将两种检索增强生成（RAG）管道——Zephyr-7B-beta和DeepSeek R1-7B——进行对比，以确定它们在处理具有挑战性的人道主义数据集时的性能，同时避免幻想风险。通过结合最前沿的AI方法、移民研究和儿童心理学，本研究提出了一个可扩展的战略，以辅助政策制定者、心理健康从业者和人道主义机构更好地帮助流离失所儿童并识别其心理健康状况。总体而言，两种模型均能正常工作，但DeepSeek R1的表现显著优于Zephyr，答案相关性准确性为0.91。 

---
# AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models 

**Title (ZH)**: AI通用风险管理系统标准概要（GPAI和基础模型） 

**Authors**: Anthony M. Barrett, Jessica Newman, Brandie Nonnecke, Nada Madkour, Dan Hendrycks, Evan R. Murphy, Krystal Jackson, Deepika Raman  

**Link**: [PDF](https://arxiv.org/pdf/2506.23949)  

**Abstract**: Increasingly multi-purpose AI models, such as cutting-edge large language models or other 'general-purpose AI' (GPAI) models, 'foundation models,' generative AI models, and 'frontier models' (typically all referred to hereafter with the umbrella term 'GPAI/foundation models' except where greater specificity is needed), can provide many beneficial capabilities but also risks of adverse events with profound consequences. This document provides risk-management practices or controls for identifying, analyzing, and mitigating risks of GPAI/foundation models. We intend this document primarily for developers of large-scale, state-of-the-art GPAI/foundation models; others that can benefit from this guidance include downstream developers of end-use applications that build on a GPAI/foundation model. This document facilitates conformity with or use of leading AI risk management-related standards, adapting and building on the generic voluntary guidance in the NIST AI Risk Management Framework and ISO/IEC 23894, with a focus on the unique issues faced by developers of GPAI/foundation models. 

**Abstract (ZH)**: 提高多用途人工智能模型的诸多有益能力但也伴随着潜在的严重后果的风险管理实践或控制：面向大型前沿通用人工智能/基础模型开发者的指南及下游应用开发者参考 

---
# Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence 

**Title (ZH)**: 超越统计学习：精确学习对于通用智能是必不可少的 

**Authors**: András György, Tor Lattimore, Nevena Lazić, Csaba Szepesvári  

**Link**: [PDF](https://arxiv.org/pdf/2506.23908)  

**Abstract**: Sound deductive reasoning -- the ability to derive new knowledge from existing facts and rules -- is an indisputably desirable aspect of general intelligence. Despite the major advances of AI systems in areas such as math and science, especially since the introduction of transformer architectures, it is well-documented that even the most advanced frontier systems regularly and consistently falter on easily-solvable deductive reasoning tasks. Hence, these systems are unfit to fulfill the dream of achieving artificial general intelligence capable of sound deductive reasoning. We argue that their unsound behavior is a consequence of the statistical learning approach powering their development. To overcome this, we contend that to achieve reliable deductive reasoning in learning-based AI systems, researchers must fundamentally shift from optimizing for statistical performance against distributions on reasoning problems and algorithmic tasks to embracing the more ambitious exact learning paradigm, which demands correctness on all inputs. We argue that exact learning is both essential and possible, and that this ambitious objective should guide algorithm design. 

**Abstract (ZH)**: 声音的演绎推理能力——从现有事实和规则推导新知识的能力——无疑是通用智能的一个不可或缺的方面。尽管在数学和科学等领域，AI系统取得了重大进展，尤其是在引入 Transformer 架构之后，文献记录表明，即使是最先进的前沿系统也经常在易于解决的演绎推理任务上出现错误。因此，这些系统无法实现具备声音演绎推理能力的人工通用智能的梦想。我们认为，它们的不当行为是其基于统计学习的方法所致。为克服这一问题，我们认为要在基于学习的AI系统中实现可靠的演绎推理能力，研究人员必须从根本上从针对推理问题和算法任务分布的统计性能优化转向拥抱更为雄心勃勃的精确学习范式，这种范式要求所有输入都正确。我们认为精确学习不仅是必要的，也是可能的，并且这一雄心勃勃的目标应该指导算法设计。 

---
# Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning 

**Title (ZH)**: 基于主动微调的可学习多智能体路径规划求解器进步 

**Authors**: Anton Andreychuk, Konstantin Yakovlev, Aleksandr Panov, Alexey Skrynnik  

**Link**: [PDF](https://arxiv.org/pdf/2506.23793)  

**Abstract**: Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot trajectory planning problems, where multiple homogeneous robots simultaneously move in the shared environment. While solving MAPF optimally has been proven to be NP-hard, scalable, and efficient, solvers are vital for real-world applications like logistics, search-and-rescue, etc. To this end, decentralized suboptimal MAPF solvers that leverage machine learning have come on stage. Building on the success of the recently introduced MAPF-GPT, a pure imitation learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training while significantly improving performance at test time. Our experiments demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF solvers, including the original MAPF-GPT, regarding solution quality across many testing scenarios. Remarkably, it can work with MAPF instances involving up to 1 million agents in a single environment, setting a new milestone for scalability in MAPF domains. 

**Abstract (ZH)**: 多智能体路径规划（MAPF）是一种常见的多机器人轨迹规划问题的抽象表示，其中多个同质机器人同时在共享环境中移动。尽管最优地解决MAPF问题已被证明是NP-hard的，但可扩展且高效的求解器对于实际应用如物流、搜索与救援等至关重要。为此，基于机器学习的分分布式次优化MAPF求解器已经出现。在最近推出的纯imitation learning求解器MAPF-GPT的基础上，我们引入了MAPF-GPT-DDG。这一新颖的方法有效利用集中式专家数据对预训练的MAPF模型进行微调。利用新颖的delta-data生成机制，MAPF-GPT-DDG在训练速度上得到加速，同时在测试时显著提高性能。实验结果显示，MAPF-GPT-DDG在多个测试场景中解决方案质量上超越了所有现有的基于学习的MAPF求解器，包括原始的MAPF-GPT。值得注意的是，它能够处理单个环境中包含多达100万个代理的MAPF实例，这在MAPF领域中树立了新的可扩展性里程碑。 

---
# When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report) 

**Title (ZH)**: 当图神经网络遇上单词方程求解器：学习排序方程（扩展技术报告） 

**Authors**: Parosh Aziz Abdulla, Mohamed Faouzi Atig, Julie Cailler, Chencheng Liang, Philipp Rümmer  

**Link**: [PDF](https://arxiv.org/pdf/2506.23784)  

**Abstract**: Nielsen transformation is a standard approach for solving word equations: by repeatedly splitting equations and applying simplification steps, equations are rewritten until a solution is reached. When solving a conjunction of word equations in this way, the performance of the solver will depend considerably on the order in which equations are processed. In this work, the use of Graph Neural Networks (GNNs) for ranking word equations before and during the solving process is explored. For this, a novel graph-based representation for word equations is presented, preserving global information across conjuncts, enabling the GNN to have a holistic view during ranking. To handle the variable number of conjuncts, three approaches to adapt a multi-classification task to the problem of ranking equations are proposed. The training of the GNN is done with the help of minimum unsatisfiable subsets (MUSes) of word equations. The experimental results show that, compared to state-of-the-art string solvers, the new framework solves more problems in benchmarks where each variable appears at most once in each equation. 

**Abstract (ZH)**: 基于图神经网络的词方程求解中排序方法研究 

---
# BayesL: Towards a Logical Framework for Bayesian Networks 

**Title (ZH)**: BayesL: 向量逻辑框架下的贝氏网络 

**Authors**: Stefano M. Nicoletti, Mariëlle Stoelinga  

**Link**: [PDF](https://arxiv.org/pdf/2506.23773)  

**Abstract**: We introduce BayesL, a novel logical framework for specifying, querying, and verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil") is a structured language that allows for the creation of queries over BNs. It facilitates versatile reasoning concerning causal and evidence-based relationships, and permits comprehensive what-if scenario evaluations without the need for manual modifications to the model. 

**Abstract (ZH)**: BayesL：一种用于指定、查询和验证贝叶斯网络行为的新逻辑框架 

---
# Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments 

**Title (ZH)**: 可验证审计：基于可信执行环境的可验证AI安全性基准 

**Authors**: Christoph Schnabl, Daniel Hugenroth, Bill Marino, Alastair R. Beresford  

**Link**: [PDF](https://arxiv.org/pdf/2506.23706)  

**Abstract**: Benchmarks are important measures to evaluate safety and compliance of AI models at scale. However, they typically do not offer verifiable results and lack confidentiality for model IP and benchmark datasets. We propose Attestable Audits, which run inside Trusted Execution Environments and enable users to verify interaction with a compliant AI model. Our work protects sensitive data even when model provider and auditor do not trust each other. This addresses verification challenges raised in recent AI governance frameworks. We build a prototype demonstrating feasibility on typical audit benchmarks against Llama-3.1. 

**Abstract (ZH)**: 可验证审计是评估AI模型安全性和合规性的关键指标，但在规模应用中通常无法提供可验证的结果，且缺乏模型IP和基准数据集的安全性。我们提出了一种可验证审计方法，其运行在可信执行环境中，使用户能够验证与合规AI模型的交互。该工作即使模型提供商和审计者之间不互信，也能保护敏感数据。这解决了近期AI治理框架中提出的验证挑战。我们构建了一个原型，展示了在典型审计基准测试Llama-3.1上实现可行性。 

---
# A New Perspective On AI Safety Through Control Theory Methodologies 

**Title (ZH)**: 通过控制理论方法论的新视角探讨人工智能安全 

**Authors**: Lars Ullrich, Walter Zimmer, Ross Greer, Knut Graichen, Alois C. Knoll, Mohan Trivedi  

**Link**: [PDF](https://arxiv.org/pdf/2506.23703)  

**Abstract**: While artificial intelligence (AI) is advancing rapidly and mastering increasingly complex problems with astonishing performance, the safety assurance of such systems is a major concern. Particularly in the context of safety-critical, real-world cyber-physical systems, AI promises to achieve a new level of autonomy but is hampered by a lack of safety assurance. While data-driven control takes up recent developments in AI to improve control systems, control theory in general could be leveraged to improve AI safety. Therefore, this article outlines a new perspective on AI safety based on an interdisciplinary interpretation of the underlying data-generation process and the respective abstraction by AI systems in a system theory-inspired and system analysis-driven manner. In this context, the new perspective, also referred to as data control, aims to stimulate AI engineering to take advantage of existing safety analysis and assurance in an interdisciplinary way to drive the paradigm of data control. Following a top-down approach, a generic foundation for safety analysis and assurance is outlined at an abstract level that can be refined for specific AI systems and applications and is prepared for future innovation. 

**Abstract (ZH)**: 随着人工智能（AI）的快速发展并以令人惊讶的性能解决日益复杂的问题，这类系统的安全性保障成为主要关切。特别是在安全关键的现实世界 cyber-物理系统中，AI 有望实现新的自主水平，但受限于缺乏安全性保障。尽管数据驱动控制利用了AI的最新发展来改进控制系统，但一般控制理论亦可用来提高AI的安全性。因此，本文从一个跨学科的角度提出了一个新的AI安全性视角，这种视角借鉴了系统理论并以系统分析为导向，对底层数据生成过程及其在AI系统中的相应抽象进行了解释。在此背景下，这种新视角也称为数据控制，旨在激励AI工程通过跨学科方式利用现有的安全性分析和保障，推动数据控制范式的驱动。本研究遵循自上而下的方法，在抽象水平上概述了一个通用的安全性分析和保障基础，可以针对特定的AI系统和应用进行细化，并为未来创新做好准备。 

---
# HASD: Hierarchical Adaption for pathology Slide-level Domain-shift 

**Title (ZH)**: HASD：层次化适应病理切片级域移变 

**Authors**: Jingsong Liu, Han Li, Chen Yang, Michael Deutges, Ario Sadafi, Xin You, Katharina Breininger, Nassir Navab, Peter J. Schüffler  

**Link**: [PDF](https://arxiv.org/pdf/2506.23673)  

**Abstract**: Domain shift is a critical problem for pathology AI as pathology data is heavily influenced by center-specific conditions. Current pathology domain adaptation methods focus on image patches rather than WSI, thus failing to capture global WSI features required in typical clinical scenarios. In this work, we address the challenges of slide-level domain shift by proposing a Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD achieves multi-scale feature consistency and computationally efficient slide-level domain adaptation through two key components: (1) a hierarchical adaptation framework that integrates a Domain-level Alignment Solver for feature alignment, a Slide-level Geometric Invariance Regularization to preserve the morphological structure, and a Patch-level Attention Consistency Regularization to maintain local critical diagnostic cues; and (2) a prototype selection mechanism that reduces computational overhead. We validate our method on two slide-level tasks across five datasets, achieving a 4.1\% AUROC improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in a UCEC survival prediction cohort. Our method provides a practical and reliable slide-level domain adaption solution for pathology institutions, minimizing both computational and annotation costs. 

**Abstract (ZH)**: 病理级域适应框架（HASD）：应对切片级域转移的层次适应方法 

---
# CooT: Learning to Coordinate In-Context with Coordination Transformers 

**Title (ZH)**: CooT: 学习协调上下文中的协调变换器 

**Authors**: Huai-Chih Wang, Hsiang-Chun Chuang, Hsi-Chun Cheng, Dai-Jie Wu, Shao-Hua Sun  

**Link**: [PDF](https://arxiv.org/pdf/2506.23549)  

**Abstract**: Effective coordination among artificial agents in dynamic and uncertain environments remains a significant challenge in multi-agent systems. Existing approaches, such as self-play and population-based methods, either generalize poorly to unseen partners or require extensive training. To overcome these limitations, we propose Coordination Transformers (CooT), a novel in-context coordination framework that uses recent interaction histories to adapt to unseen partners rapidly. Unlike previous approaches that primarily aim to increase the diversity of training partners, CooT explicitly focuses on adapting to new partner behaviors by predicting actions aligned with observed partner interactions. Trained on interaction trajectories collected from diverse pairs of agents with complementary behaviors, CooT quickly learns effective coordination strategies without explicit supervision or fine-tuning. Evaluations on the Overcooked benchmark demonstrate that CooT significantly outperforms baseline methods in coordination tasks involving previously unseen partners. Human evaluations further confirm CooT as the most effective collaborative partner, while extensive ablations highlight its robustness, flexibility, and sensitivity to context in multi-agent scenarios. 

**Abstract (ZH)**: 在动态和不确定环境中，人工代理之间的有效协调仍然是多代理系统中的一个重要挑战。现有方法，如自我博弈和基于群体的方法，要么对未见过的合作伙伴泛化能力差，要么需要大量训练。为了克服这些局限性，我们提出了一种新颖的上下文关联框架Coordination Transformers（CooT），该框架利用最近的互动历史快速适应未见过的合作伙伴。与 previous approaches 主要旨在增加训练合作伙伴的多样性不同，CooT 明确地专注于通过预测与观察到的合作伙伴互动一致的动作来适应新合作伙伴的行为。CooT 在来自具有互补行为的多样化伙伴对收集的互动轨迹上进行训练，无需显式的监督或微调即可迅速学习有效的协调策略。Overcooked 基准上的评估表明，CooT 在涉及未见过的合作伙伴的协调任务中显著优于基线方法。进一步的人类评估证实了CooT作为最有效的协作伙伴的有效性，而广泛的消融实验突显了其在多代理场景中的鲁棒性、灵活性和对上下文的敏感性。 

---
# Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays 

**Title (ZH)**: 评估GPTZero在识别AI与人类撰写的Essay方面的准确性 

**Authors**: Selin Dik, Osman Erdem, Mehmet Dik  

**Link**: [PDF](https://arxiv.org/pdf/2506.23517)  

**Abstract**: As the use of AI tools by students has become more prevalent, instructors have started using AI detection tools like GPTZero and QuillBot to detect AI written text. However, the reliability of these detectors remains uncertain. In our study, we focused mostly on the success rate of GPTZero, the most-used AI detector, in identifying AI-generated texts based on different lengths of randomly submitted essays: short (40-100 word count), medium (100-350 word count), and long (350-800 word count). We gathered a data set consisting of twenty-eight AI-generated papers and fifty human-written papers. With this randomized essay data, papers were individually plugged into GPTZero and measured for percentage of AI generation and confidence. A vast majority of the AI-generated papers were detected accurately (ranging from 91-100% AI believed generation), while the human generated essays fluctuated; there were a handful of false positives. These findings suggest that although GPTZero is effective at detecting purely AI-generated content, its reliability in distinguishing human-authored texts is limited. Educators should therefore exercise caution when relying solely on AI detection tools. 

**Abstract (ZH)**: 随着学生使用AI工具的增多，教师开始使用如GPTZero和QuillBot等AI检测工具来识别AI撰写的文本。然而，这些检测工具的可靠性仍然存疑。在我们的研究中，我们主要关注了GPTZero——最常用的AI检测工具——在基于不同长度的随机提交essay中识别AI生成文本的成功率：短（40-100词）、中（100-350词）和长（350-800词）。我们收集了一组包含二十八篇AI生成的论文和五十篇人工撰写的论文的数据集。利用这些随机化的essay数据，每篇论文分别输入GPTZero并测量其AI生成内容的百分比及其置信度。大部分AI生成的论文被准确检测（AI认为生成的比例从91%到100%不等），而人工撰写的论文则波动较大；存在一些误报。这些发现表明，虽然GPTZero在检测纯AI生成内容方面非常有效，但在区分人工撰写的文本方面可靠性有限。教育者在依赖AI检测工具时应谨慎。 

---
# Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM 

**Title (ZH)**: 基于AlexNet和LSTM的混合电价预测方法 

**Authors**: Bosubabu Sambana, Kotamsetty Geethika Devi, Bandi Rajeswara Reddy, Galeti Mohammad Hussain, Gownivalla Siddartha  

**Link**: [PDF](https://arxiv.org/pdf/2506.23504)  

**Abstract**: The recent development of advanced machine learning methods for hybrid models has greatly addressed the need for the correct prediction of electrical prices. This method combines AlexNet and LSTM algorithms, which are used to introduce a new model with higher accuracy in price forecasting. Despite RNN and ANN being effective, they often fail to deal with forex time sequence data. The traditional methods do not accurately forecast the prices. These traditional methods only focus on demand and price which leads to insufficient analysis of data. To address this issue, using the hybrid approach, which focuses on external variables that also effect the predicted prices. Nevertheless, due to AlexNet's excellent feature extraction and LSTM's learning sequential patterns, the prediction accuracy is vastly increased. The model is built on the past data, which has been supplied with the most significant elements like demand, temperature, sunlight, and rain. For example, the model applies methods, such as minimum-maximum scaling and a time window, to predict the electricity prices of the future. The results show that this hybrid model is good than the standalone ones in terms of accuracy. Although we got our accuracy rating of 97.08, it shows higher accompaniments than remaining models RNN and ANN with accuracies of 96.64 and 96.63 respectively. 

**Abstract (ZH)**: 最近发展中的高级机器学习方法在混合模型中的应用极大地满足了准确预测电力价格的需求。该方法结合使用AlexNet和LSTM算法，以引入一种具有更高预测准确性的新模型。尽管RNN和ANN有效，但在处理外汇时间序列数据时经常失效。传统方法未能准确预测价格，这些方法仅关注需求和价格，导致数据分析不足。为解决这一问题，采用混合方法，该方法关注影响预测价格的外部变量。尽管如此，由于AlexNet出色的特征提取能力和LSTM学习序列模式的能力，预测准确性明显提高。该模型基于历史数据构建，包含了最重要的因素，如需求、温度、阳光和降雨。例如，该模型使用最小-最大标准化和时间窗口等方法来预测未来时期的电价。结果显示，该混合模型在准确度方面优于单独使用的RNN和ANN模型。虽然我们的准确度评分为97.08，但高于剩余模型RNN和ANN的准确度96.64和96.63。 

---
# FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis 

**Title (ZH)**: FinStat2SQL: 会计报表分析的文本到SQL流水线 

**Authors**: Quang Hung Nguyen, Phuong Anh Trinh, Phan Quoc Hung Mai, Tuan Phong Trinh  

**Link**: [PDF](https://arxiv.org/pdf/2506.23273)  

**Abstract**: Despite the advancements of large language models, text2sql still faces many challenges, particularly with complex and domain-specific queries. In finance, database designs and financial reporting layouts vary widely between financial entities and countries, making text2sql even more challenging. We present FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries over financial statements. Tailored to local standards like VAS, it combines large and small language models in a multi-agent setup for entity extraction, SQL generation, and self-correction. We build a domain-specific database and evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves 61.33\% accuracy with sub-4-second response times on consumer hardware, outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient solution for financial analysis, making AI-powered querying accessible to Vietnamese enterprises. 

**Abstract (ZH)**: 尽管大型语言模型取得了进展，但文本转SQL仍然面临许多挑战，特别是在处理复杂和领域特定的查询方面。在金融领域，不同金融实体和国家的数据库设计及财务报表布局差异较大，使得文本转SQL更加具有挑战性。我们提出了FinStat2SQL，这是一种轻量级的文本转SQL管道，能够针对财务报表进行自然语言查询。该系统针对本地标准（如VAS）进行了定制，并在一个多智能体架构中结合了大型和小型语言模型，用于实体提取、SQL生成和自我纠正。我们构建了一个专用领域的数据库，并在合成QA数据集上评估了模型。微调后的7B模型在消费级硬件上的响应时间低于4秒，准确率达到61.33%，超越了GPT-4o-mini。FinStat2SQL提供了一种可扩展且成本效益高的解决方案，使基于AI的查询普及到越南企业。 

---
# Rises for Measuring Local Distributivity in Lattices 

**Title (ZH)**: 用于测量格中局部分配性的方法 

**Authors**: Mohammad Abdulla, Tobias Hille, Dominik Dürrschnabel, Gerd Stumme  

**Link**: [PDF](https://arxiv.org/pdf/2506.23168)  

**Abstract**: Distributivity is a well-established and extensively studied notion in lattice theory. In the context of data analysis, particularly within Formal Concept Analysis (FCA), lattices are often observed to exhibit a high degree of distributivity. However, no standardized measure exists to quantify this property. In this paper, we introduce the notion of rises in (concept) lattices as a means to assess distributivity. Rises capture how the number of attributes or objects in covering concepts change within the concept lattice. We show that a lattice is distributive if and only if no non-unit rises occur. Furthermore, we relate rises to the classical notion of meet- and join distributivity. We observe that concept lattices from real-world data are to a high degree join-distributive, but much less meet-distributive. We additionally study how join-distributivity manifests on the level of ordered sets. 

**Abstract (ZH)**: 分配律是在格理论中一个成熟且广泛研究的概念。在数据分析领域，特别是在形式概念分析（FCA）中，观察到格往往表现出较高的分配律特征。然而，并不存在标准化的度量方法来量化这一性质。本文引入（概念）格中的上升概念来评估分配律。上升捕捉覆盖概念在概念格中属性或对象数量变化的情况。我们证明了一个格是分配格当且仅当不存在非单位上升。进一步，我们将上升与传统的meet-和join分配律概念联系起来。我们观察到，来自实际数据的概念格在合分配律方面表现出较高的程度，但在交分配律方面则相差甚远。我们还研究了合分配律在有序集层次上的表现。 

---
# Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing 

**Title (ZH)**: 基于语境的语义感知关系消息传递知识图谱补全 

**Authors**: Siyuan Li, Ruitong Liu, Yan Wen, Te Sun  

**Link**: [PDF](https://arxiv.org/pdf/2506.23141)  

**Abstract**: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge Graph Completion (KGC), providing vital cues for prediction. However, traditional node-based message passing mechanisms, when applied to knowledge graphs, often introduce noise and suffer from information dilution or over-smoothing by indiscriminately aggregating information from all neighboring edges. To address this challenge, we propose a semantic-aware relational message passing. A core innovation of this framework is the introduction of a \textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this strategy first evaluates the semantic relevance between a central node and its incident edges within a shared latent space, selecting only the Top-K most pertinent ones. Subsequently, information from these selected edges is effectively fused with the central node's own representation using a \textbf{multi-head attention aggregator} to generate a semantically focused node message. In this manner, our model not only leverages the structure and features of edges within the knowledge graph but also more accurately captures and propagates the contextual information most relevant to the specific link prediction task, thereby effectively mitigating interference from irrelevant information. Extensive experiments demonstrate that our method achieves superior performance compared to existing approaches on several established benchmarks. 

**Abstract (ZH)**: 语义感知的关系消息传递对于知识图谱完成至关重要，通过引入语义感知的Top-K邻居选择策略和多头注意力聚合器，更准确地捕获和传播与具体链接预测任务最相关的上下文信息，从而有效减少无关信息的干扰。实验结果显示，该方法在多个基准测试上优于现有方法。 

---
# The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy 

**Title (ZH)**: 基础模型的社会影响：推进基于证据的人工智能政策 

**Authors**: Rishi Bommasani  

**Link**: [PDF](https://arxiv.org/pdf/2506.23123)  

**Abstract**: Artificial intelligence is humanity's most promising technology because of the remarkable capabilities offered by foundation models. Yet, the same technology brings confusion and consternation: foundation models are poorly understood and they may precipitate a wide array of harms. This dissertation explains how technology and society coevolve in the age of AI, organized around three themes. First, the conceptual framing: the capabilities, risks, and the supply chain that grounds foundation models in the broader economy. Second, the empirical insights that enrich the conceptual foundations: transparency created via evaluations at the model level and indexes at the organization level. Finally, the transition from understanding to action: superior understanding of the societal impact of foundation models advances evidence-based AI policy. View together, this dissertation makes inroads into achieving better societal outcomes in the age of AI by building the scientific foundations and research-policy interface required for better AI governance. 

**Abstract (ZH)**: 人工智能是基于基础模型的非凡能力使人类最为期待的技术。然而，同样的技术也带来了困惑和担忧：基础模型缺乏理解，可能引发广泛的危害。本论文解释了在人工智能时代技术与社会共进化的过程，围绕三个主题组织：首先，概念框架：基础模型的能力、风险及其在更广阔经济中的供应链基础；其次，通过模型层面的评估和组织层面的指数增强的概念基础：透明度；最后，从理解到行动的过渡：对基础模型社会影响的深刻理解推动基于证据的AI政策。综合来看，本论文通过建立必要的科学基础和研究-政策接口，为实现更好的人工智能治理并取得更好的社会成果奠定了基础。 

---
# Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning 

**Title (ZH)**: Hecto: 可模块化、稀疏、适应性和可解释的专家系统 

**Authors**: Sanskar Pandey, Ruhaan Chopra, Saad Murtaza Bhat, Ark Abhyudaya  

**Link**: [PDF](https://arxiv.org/pdf/2506.22919)  

**Abstract**: Mixture-of-Experts (MoE) models enable conditional computation by routing inputs to specialized experts, but these experts rely on identical inductive biases, thus limiting representational diversity. This static computation pathway is inefficient for inputs that require different types of reasoning and limits specialization and interpretability. We propose Hecto, a lightweight MoE architecture that leverages architectural heterogeneity by combining a GRU expert for temporal reasoning and an FFNN expert for static abstraction under a sparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG News, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely trails homogeneous baselines in performance despite receiving isolated input representations, while achieving clear expert specialization, with each expert aligning to distinct reasoning types (temporal vs static). At larger batch sizes, Hecto exhibits improved performance, benefiting from relaxed computational constraints that allow its heterogeneous architecture to optimize more effectively. Ablation results isolate architectural diversity as the source of Hecto's stability and interpretability across diverse reasoning tasks. Overall, Hecto establishes itself as a new benchmark for conditional computation, offering a principled framework for specialized reasoning in low-resource regimes with its model strength derived from principled specialization. 

**Abstract (ZH)**: Hecto: 一种利用架构异质性进行条件计算的轻量级MoE架构 

---
# Agentic Enterprise: AI-Centric User to User-Centric AI 

**Title (ZH)**: 代理企业：以AI为中心向以用户为中心的AI转型 

**Authors**: Arpit Narechania, Alex Endert, Atanu R Sinha  

**Link**: [PDF](https://arxiv.org/pdf/2506.22893)  

**Abstract**: After a very long winter, the Artificial Intelligence (AI) spring is here. Or, so it seems over the last three years. AI has the potential to impact many areas of human life - personal, social, health, education, professional. In this paper, we take a closer look at the potential of AI for Enterprises, where decision-making plays a crucial and repeated role across functions, tasks, and operations. We consider Agents imbued with AI as means to increase decision-productivity of enterprises. We highlight six tenets for Agentic success in enterprises, by drawing attention to what the current, AI-Centric User paradigm misses, in the face of persistent needs of and usefulness for Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we offer six tenets and promote market mechanisms for platforms, aligning the design of AI and its delivery by Agents to the cause of enterprise users. 

**Abstract (ZH)**: 在经历了漫长的寒冬之后，人工智能（AI）的春天已经到来。或者至少在过去三年中是这样。人工智能有望影响人类生活的许多领域——个人生活、社会生活、健康、教育和职业。在这篇论文中，我们更深入地探讨了人工智能在企业的潜力，其中决策在各个职能、任务和运营中扮演着关键且反复的角色。我们考虑具有人工智能的代理作为提高企业决策生产力的手段。我们通过对当前以人工智能为中心的用户范式的关注，强调了在面对企业决策持续的需求和实用性时，六条实现企业代理成功的要领。我们强调用户为中心的人工智能转型，提出六条原则，并促进平台市场的机制，使人工智能的设计及其由代理交付的方式与企业用户的需求相一致。 

---
# Explanations are a means to an end 

**Title (ZH)**: 解释是一种手段。 

**Authors**: Jessica Hullman, Ziyang Guo, Berk Ustun  

**Link**: [PDF](https://arxiv.org/pdf/2506.22740)  

**Abstract**: Modern methods for explainable machine learning are designed to describe how models map inputs to outputs--without deep consideration of how these explanations will be used in practice. This paper argues that explanations should be designed and evaluated with a specific end in mind. We describe how to formalize this end in a framework based in statistical decision theory. We show how this functionally-grounded approach can be applied across diverse use cases, such as clinical decision support, providing recourse, or debugging. We demonstrate its use to characterize the maximum "boost" in performance on a particular task that an explanation could provide an idealized decision-maker, preventing misuse due to ambiguity by forcing researchers to specify concrete use cases that can be analyzed in light of models of expected explanation use. We argue that evaluation should meld theoretical and empirical perspectives on the value of explanation, and contribute definitions that span these perspectives. 

**Abstract (ZH)**: 现代可解释机器学习方法旨在描述模型如何将输入映射到输出，但在实践中如何使用这些解释则没有深入考虑。本文认为，解释应旨在特定目标进行设计和评估。我们基于统计决策理论构建了一个框架来正式化这一目标。我们展示了这种基于功能的范式如何在临床决策支持、提供救济或调试等多种应用场景中应用。我们通过实例分析，确定解释对理想化决策者在特定任务上的最大“增益”，并通过明确具体的使用案例来防止由于模糊性而导致的误用，从而敦促研究人员制定可在预期解释使用模型背景下进行分析的具体使用案例。我们主张评估应结合理论和实证观点来评估解释的价值，并为此贡献了涵盖这些观点的定义。 

---
# Ludax: A GPU-Accelerated Domain Specific Language for Board Games 

**Title (ZH)**: Ludax：一种加速棋类游戏的GPU专用语言 

**Authors**: Graham Todd, Alexander G. Padula, Dennis J.N.J. Soemers, Julian Togelius  

**Link**: [PDF](https://arxiv.org/pdf/2506.22609)  

**Abstract**: Games have long been used as benchmarks and testing environments for research in artificial intelligence. A key step in supporting this research was the development of game description languages: frameworks that compile domain-specific code into playable and simulatable game environments, allowing researchers to generalize their algorithms and approaches across multiple games without having to manually implement each one. More recently, progress in reinforcement learning (RL) has been largely driven by advances in hardware acceleration. Libraries like JAX allow practitioners to take full advantage of cutting-edge computing hardware, often speeding up training and testing by orders of magnitude. Here, we present a synthesis of these strands of research: a domain-specific language for board games which automatically compiles into hardware-accelerated code. Our framework, Ludax, combines the generality of game description languages with the speed of modern parallel processing hardware and is designed to fit neatly into existing deep learning pipelines. We envision Ludax as a tool to help accelerate games research generally, from RL to cognitive science, by enabling rapid simulation and providing a flexible representation scheme. We present a detailed breakdown of Ludax's description language and technical notes on the compilation process, along with speed benchmarking and a demonstration of training RL agents. The Ludax framework, along with implementations of existing board games, is open-source and freely available. 

**Abstract (ZH)**: 一种结合领域特定语言与硬件加速技术的板游戏框架：促进游戏研究的快速发展 

---
# FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation 

**Title (ZH)**: 快速准确的数据残差匹配用于数据集蒸馏 

**Authors**: Jiacheng Cui, Xinyue Bi, Yaxin Luo, Xiaohan Zhao, Jiacheng Liu, Zhiqiang Shen  

**Link**: [PDF](https://arxiv.org/pdf/2506.24125)  

**Abstract**: Residual connection has been extensively studied and widely applied at the model architecture level. However, its potential in the more challenging data-centric approaches remains unexplored. In this work, we introduce the concept of Data Residual Matching for the first time, leveraging data-level skip connections to facilitate data generation and mitigate data information vanishing. This approach maintains a balance between newly acquired knowledge through pixel space optimization and existing core local information identification within raw data modalities, specifically for the dataset distillation task. Furthermore, by incorporating optimization-level refinements, our method significantly improves computational efficiency, achieving superior performance while reducing training time and peak GPU memory usage by 50%. Consequently, the proposed method Fast and Accurate Data Residual Matching for Dataset Distillation (FADRM) establishes a new state-of-the-art, demonstrating substantial improvements over existing methods across multiple dataset benchmarks in both efficiency and effectiveness. For instance, with ResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the method achieves 47.7% test accuracy in single-model dataset distillation and 50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and outperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4% and +4.0%. Code is available at: this https URL. 

**Abstract (ZH)**: 基于数据的残差匹配：快速准确的数据集蒸馏（FADRM） 

---
# Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies 

**Title (ZH)**: 混合人工 intelligence 训练在真实数据和合成数据上的开发：两种混合训练策略的基准研究 

**Authors**: Paul Wachter, Lukas Niehaus, Julius Schöning  

**Link**: [PDF](https://arxiv.org/pdf/2506.24093)  

**Abstract**: Synthetic data has emerged as a cost-effective alternative to real data for training artificial neural networks (ANN). However, the disparity between synthetic and real data results in a domain gap. That gap leads to poor performance and generalization of the trained ANN when applied to real-world scenarios. Several strategies have been developed to bridge this gap, which combine synthetic and real data, known as mixed training using hybrid datasets. While these strategies have been shown to mitigate the domain gap, a systematic evaluation of their generalizability and robustness across various tasks and architectures remains underexplored. To address this challenge, our study comprehensively analyzes two widely used mixing strategies on three prevalent architectures and three distinct hybrid datasets. From these datasets, we sample subsets with varying proportions of synthetic to real data to investigate the impact of synthetic and real components. The findings of this paper provide valuable insights into optimizing the use of synthetic data in the training process of any ANN, contributing to enhancing robustness and efficacy. 

**Abstract (ZH)**: 合成数据已成为训练人工神经网络（ANN）的一种成本-effective 的替代方案，但由于合成数据与真实数据之间的差异导致了领域差距。这一差距导致在应用于实际场景时，训练好的ANN性能和泛化能力较差。已经开发出多种策略来弥合这一差距，即通过混合数据集的混合训练。虽然这些策略已被证明可以缓解领域差距，但它们在不同任务和架构上的可泛化性和鲁棒性系统的评估仍较少进行。为了应对这一挑战，我们的研究全面分析了两种广泛使用的混合策略在三种主流架构和三种不同的混合数据集上的效果。通过对这些数据集进行不同比例合成数据和真实数据的子集采样，我们探索了合成和真实数据组件的影响。本文的发现为优化任何ANN训练过程中合成数据的使用提供了宝贵的见解，有助于提升其稳健性和有效性。 

---
# SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks 

**Title (ZH)**: SQUASH: 一种基于交换原理的量子攻击方法以破坏混合量子神经网络 

**Authors**: Rahul Kumar, Wenqi Wei, Ying Mao, Junaid Farooq, Ying Wang, Juntao Chen  

**Link**: [PDF](https://arxiv.org/pdf/2506.24081)  

**Abstract**: We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to sabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks. SQUASH is executed by inserting SWAP gate(s) into the variational quantum circuit of the victim HQNN. Unlike conventional noise-based or adversarial input attacks, SQUASH directly manipulates the circuit structure, leading to qubit misalignment and disrupting quantum state evolution. This attack is highly stealthy, as it does not require access to training data or introduce detectable perturbations in input states. Our results demonstrate that SQUASH significantly degrades classification performance, with untargeted SWAP attacks reducing accuracy by up to 74.08\% and targeted SWAP attacks reducing target class accuracy by up to 79.78\%. These findings reveal a critical vulnerability in HQNN implementations, underscoring the need for more resilient architectures against circuit-level adversarial interventions. 

**Abstract (ZH)**: 基于SWAP门的电路级攻击SQUASH：针对混合量子神经网络的破坏性攻击 

---
# Bridging Theory and Practice in Link Representation with Graph Neural Networks 

**Title (ZH)**: 基于图神经网络的链接表示理论与实践桥梁构建 

**Authors**: Veronica Lachi, Francesco Ferrini, Antonio Longa, Bruno Lepri, Andrea Passerini, Manfred Jaeger  

**Link**: [PDF](https://arxiv.org/pdf/2506.24018)  

**Abstract**: Graph Neural Networks (GNNs) are widely used to compute representations of node pairs for downstream tasks such as link prediction. Yet, theoretical understanding of their expressive power has focused almost entirely on graph-level representations. In this work, we shift the focus to links and provide the first comprehensive study of GNN expressiveness in link representation. We introduce a unifying framework, the $k_\phi$-$k_\rho$-$m$ framework, that subsumes existing message-passing link models and enables formal expressiveness comparisons. Using this framework, we derive a hierarchy of state-of-the-art methods and offer theoretical tools to analyze future architectures. To complement our analysis, we propose a synthetic evaluation protocol comprising the first benchmark specifically designed to assess link-level expressiveness. Finally, we ask: does expressiveness matter in practice? We use a graph symmetry metric that quantifies the difficulty of distinguishing links and show that while expressive models may underperform on standard benchmarks, they significantly outperform simpler ones as symmetry increases, highlighting the need for dataset-aware model selection. 

**Abstract (ZH)**: 图神经网络在链接表示中的表征能力：从理论到实践的全面研究 

---
# EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations 

**Title (ZH)**: EXPERT：一种具有结构化解释的可解释图像字幕评价指标 

**Authors**: Hyunjong Kim, Sangyeop Kim, Jongheon Jeong, Yeongjae Cho, Sungzoon Cho  

**Link**: [PDF](https://arxiv.org/pdf/2506.24016)  

**Abstract**: Recent advances in large language models and vision-language models have led to growing interest in explainable evaluation metrics for image captioning. However, these metrics generate explanations without standardized criteria, and the overall quality of the generated explanations remains unverified. In this paper, we propose EXPERT, a reference-free evaluation metric that provides structured explanations based on three fundamental criteria: fluency, relevance, and descriptiveness. By constructing large-scale datasets of high-quality structured explanations, we develop a two-stage evaluation template to effectively supervise a vision-language model for both scoring and explanation generation. EXPERT achieves state-of-the-art results on benchmark datasets while providing significantly higher-quality explanations than existing metrics, as validated through comprehensive human evaluation. Our code and datasets are available at this https URL. 

**Abstract (ZH)**: 近期大语言模型和多模态模型的进展推动了图像 Captioning 可解释评估指标的研究。然而，这些指标生成的解释缺乏标准化的评价标准，其整体质量尚未得到验证。本文提出了一种名为 EXPERT 的无需参考的评估指标，它基于流畅性、相关性和描述性三个基本标准提供结构化的解释。通过构建大规模高质量结构化解释的数据集，我们开发了一种两级评估模板，有效监督多模态模型进行评分及解释生成。EXPERT 在基准数据集上实现了最先进的性能，其生成的解释质量也显著高于现有指标，这一结论通过全面的人类评估验证。代码和数据集可在以下链接获取：this https URL。 

---
# Autonomy by Design: Preserving Human Autonomy in AI Decision-Support 

**Title (ZH)**: 设计自主性：在AI决策支持中preserve人类自主性 

**Authors**: Stefan Buijsman, Sarah Carter, Juan Pablo Bermúdez  

**Link**: [PDF](https://arxiv.org/pdf/2506.23952)  

**Abstract**: AI systems increasingly support human decision-making across domains of professional, skill-based, and personal activity. While previous work has examined how AI might affect human autonomy globally, the effects of AI on domain-specific autonomy -- the capacity for self-governed action within defined realms of skill or expertise -- remain understudied. We analyze how AI decision-support systems affect two key components of domain-specific autonomy: skilled competence (the ability to make informed judgments within one's domain) and authentic value-formation (the capacity to form genuine domain-relevant values and preferences). By engaging with prior investigations and analyzing empirical cases across medical, financial, and educational domains, we demonstrate how the absence of reliable failure indicators and the potential for unconscious value shifts can erode domain-specific autonomy both immediately and over time. We then develop a constructive framework for autonomy-preserving AI support systems. We propose specific socio-technical design patterns -- including careful role specification, implementation of defeater mechanisms, and support for reflective practice -- that can help maintain domain-specific autonomy while leveraging AI capabilities. This framework provides concrete guidance for developing AI systems that enhance rather than diminish human agency within specialized domains of action. 

**Abstract (ZH)**: AI系统在专业、技能和个人领域日益支持人类决策，但其对领域特定自主性的影响仍研究不足：AI决策支持系统对领域特定自主性的影响分析与对策 

---
# QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference 

**Title (ZH)**: QPART: 适应性模型量化和动态工作负载均衡以实现准确性的边缘推理 

**Authors**: Xiangchen Li, Saeid Ghafouri, Bo Ji, Hans Vandierendonck, Deepu John, Dimitrios S. Nikolopoulos  

**Link**: [PDF](https://arxiv.org/pdf/2506.23934)  

**Abstract**: As machine learning inferences increasingly move to edge devices, adapting to diverse computational capabilities, hardware, and memory constraints becomes more critical. Instead of relying on a pre-trained model fixed for all future inference queries across diverse edge devices, we argue that planning an inference pattern with a request-specific model tailored to the device's computational capacity, accuracy requirements, and time constraints is more cost-efficient and robust to diverse scenarios. To this end, we propose an accuracy-aware and workload-balanced inference system that integrates joint model quantization and inference partitioning. In this approach, the server dynamically responds to inference queries by sending a quantized model and adaptively sharing the inference workload with the device. Meanwhile, the device's computational power, channel capacity, and accuracy requirements are considered when deciding.
Furthermore, we introduce a new optimization framework for the inference system, incorporating joint model quantization and partitioning. Our approach optimizes layer-wise quantization bit width and partition points to minimize time consumption and cost while accounting for varying accuracy requirements of tasks through an accuracy degradation metric in our optimization model. To our knowledge, this work represents the first exploration of optimizing quantization layer-wise bit-width in the inference serving system, by introducing theoretical measurement of accuracy degradation. Simulation results demonstrate a substantial reduction in overall time and power consumption, with computation payloads decreasing by over 80% and accuracy degradation kept below 1%. 

**Abstract (ZH)**: 随着机器学习推理逐渐迁移至边缘设备，适应多样化的计算能力、硬件和内存约束变得更为关键。我们提出，针对不同设备的计算能力、准确度要求和时间限制定制特定请求的模型推理模式，不仅更具成本效益，而且能在多样化的场景中更为 robust。为此，我们提出了一种兼顾准确度和工作负载平衡的推理系统，该系统结合了联合模型量化和推理分区。在该方法中，服务器动态响应推理请求，发送量化模型，并与设备共享推理工作负载。与此同时，在决定过程中会考虑设备的计算能力、信道容量和准确度要求。此外，我们引入了一种新的推理系统优化框架，结合了联合模型量化和分区。通过引入准确度降级度量来优化逐层量化位宽和分区点，以最小化时间消耗和成本，同时考虑到任务不同准确度要求的变化。据我们所知，本工作是首次对推理服务系统中的逐层量化位宽进行优化的研究，通过引入准确度降级的理论测量。仿真结果表明，总体时间和功耗显著减少，计算负载降低超过80%，且准确度降级保持在1%以下。 

---
# Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic 

**Title (ZH)**: 顺序思维链：发现适合学习的算术学习顺序 

**Authors**: Yuta Sato, Kazuhiko Kawamoto, Hiroshi Kera  

**Link**: [PDF](https://arxiv.org/pdf/2506.23875)  

**Abstract**: The chain of thought is fundamental in Transformers, which is to perform step-by-step reasoning. Besides what intermediate steps work, the order of these steps critically affects the difficulty of the reasoning. This study addresses a novel task of unraveling chain of thought - reordering decoder input tokens to a learning-friendly sequence for Transformers to learn arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture of target sequences arranged in different orders and then identifies benign orders as those with fast loss drops in the early stage. As the search space grows factorially with sequence length, we propose a two-stage hierarchical approach for inter- and intra-block reordering. Experiments on four order-sensitive arithmetic tasks show that our method identifies a learning-friendly order out of a few billion candidates. Notably, on the multiplication task, it recovered the reverse-digit order reported in prior studies. 

**Abstract (ZH)**: Transformer中推理链的重构：面向学习的解码器输入重排序以学习算术任务 

---
# Scaling Self-Supervised Representation Learning for Symbolic Piano Performance 

**Title (ZH)**: 扩展自监督表示学习以应用于符号钢琴表演 

**Authors**: Louis Bradshaw, Honglu Fan, Alexander Spangher, Stella Biderman, Simon Colton  

**Link**: [PDF](https://arxiv.org/pdf/2506.23869)  

**Abstract**: We study the capabilities of generative autoregressive transformer models trained on large amounts of symbolic solo-piano transcriptions. After first pretraining on approximately 60,000 hours of music, we use a comparatively smaller, high-quality subset, to finetune models to produce musical continuations, perform symbolic classification tasks, and produce general-purpose contrastive MIDI embeddings by adapting the SimCLR framework to symbolic music. When evaluating piano continuation coherence, our generative model outperforms leading symbolic generation techniques and remains competitive with proprietary audio generation models. On MIR classification benchmarks, frozen representations from our contrastive model achieve state-of-the-art results in linear probe experiments, while direct finetuning demonstrates the generalizability of pretrained representations, often requiring only a few hundred labeled examples to specialize to downstream tasks. 

**Abstract (ZH)**: 我们研究了在大量符号独奏钢琴转录数据上训练的生成自回归变压器模型的能力。首先在大约60,000小时的音乐上进行预训练后，我们使用一个相对较小但高质量的数据子集，对模型进行微调以生成音乐延续、执行符号分类任务，并通过将SimCLR框架适应符号音乐来生成通用对比MIDI嵌入。在评估钢琴延续连贯性时，我们的生成模型优于领先的符号生成技术，并在专有音频生成模型中保持竞争力。在MIR分类基准测试中，我们的对比模型冻结表示在线性探测实验中取得了最先进的效果，而直接微调则展示了预训练表示的泛化能力，通常只需要少量标记的示例即可专门化于下游任务。 

---
# Differentially Private Synthetic Data Release for Topics API Outputs 

**Title (ZH)**: 差分隐私合成数据发布：针对Topics API输出 

**Authors**: Travis Dick, Alessandro Epasto, Adel Javanmard, Josh Karlin, Andres Munoz Medina, Vahab Mirrokni, Sergei Vassilvitskii, Peilin Zhong  

**Link**: [PDF](https://arxiv.org/pdf/2506.23855)  

**Abstract**: The analysis of the privacy properties of Privacy-Preserving Ads APIs is an area of research that has received strong interest from academics, industry, and regulators. Despite this interest, the empirical study of these methods is hindered by the lack of publicly available data. Reliable empirical analysis of the privacy properties of an API, in fact, requires access to a dataset consisting of realistic API outputs; however, privacy concerns prevent the general release of such data to the public.
In this work, we develop a novel methodology to construct synthetic API outputs that are simultaneously realistic enough to enable accurate study and provide strong privacy protections. We focus on one Privacy-Preserving Ads APIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a methodology to generate a differentially-private dataset that closely matches the re-identification risk properties of the real Topics API data. The use of differential privacy provides strong theoretical bounds on the leakage of private user information from this release.
Our methodology is based on first computing a large number of differentially-private statistics describing how output API traces evolve over time. Then, we design a parameterized distribution over sequences of API traces and optimize its parameters so that they closely match the statistics obtained. Finally, we create the synthetic data by drawing from this distribution.
Our work is complemented by an open-source release of the anonymized dataset obtained by this methodology. We hope this will enable external researchers to analyze the API in-depth and replicate prior and future work on a realistic large-scale dataset. We believe that this work will contribute to fostering transparency regarding the privacy properties of Privacy-Preserving Ads APIs. 

**Abstract (ZH)**: 隐私保护广告API的隐私属性分析是一个受到学术界、工业界和监管机构广泛关注的研究领域。尽管存在浓厚的兴趣，但由于缺乏公开数据，这些方法的实证研究受到了阻碍。要对API的隐私属性进行可靠的实证分析，实际上需要访问一个包含现实API输出的数据集；然而，隐私担忧阻止了这类数据的普遍公开。

在本文中，我们开发了一种新颖的方法来构造同时足够现实且具有强大隐私保护的合成API输出。我们重点关注一个隐私保护广告API：Google Chrome隐私沙盒中的Topics API。我们开发了一种生成同态隐私数据集的方法，该数据集在重新识别风险属性方面 closely matches 真实Topics API数据。同态隐私的使用提供了对从本次发布中泄露的私人用户信息的强理论上限。

我们的方法基于首先计算描述输出API踪迹随时间演变的大量同态隐私统计量。然后，我们设计一个参数化的API踪迹序列分布，并优化其参数，使其尽可能接近从统计中获得的结果。最后，我们通过从该分布中抽取数据来生成合成数据。

我们的工作还包含了通过该方法获得的 anonymized 数据集的开源发布。我们希望这将使外部研究人员能够深入分析API并在现实的大规模数据集上重现并开展未来工作。我们认为这项工作将有助于提高关于隐私保护广告API的隐私属性的透明度。 

---
# Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts 

**Title (ZH)**: 使用稀疏自编码器发现未知概念，而非作用于已知概念 

**Authors**: Kenny Peng, Rajiv Movva, Jon Kleinberg, Emma Pierson, Nikhil Garg  

**Link**: [PDF](https://arxiv.org/pdf/2506.23845)  

**Abstract**: While sparse autoencoders (SAEs) have generated significant excitement, a series of negative results have added to skepticism about their usefulness. Here, we establish a conceptual distinction that reconciles competing narratives surrounding SAEs. We argue that while SAEs may be less effective for acting on known concepts, SAEs are powerful tools for discovering unknown concepts. This distinction cleanly separates existing negative and positive results, and suggests several classes of SAE applications. Specifically, we outline use cases for SAEs in (i) ML interpretability, explainability, fairness, auditing, and safety, and (ii) social and health sciences. 

**Abstract (ZH)**: 虽然稀疏自编码器（SAEs）引起了巨大关注，一系列负面结果也增加了对其效用的怀疑。在这里，我们确立了一个概念性的区分，以 reconciliate 关于 SAEs 竞争性的叙述。我们 argue 道，虽然 SAEs 在执行已知概念方面可能不够有效，但它们是发现未知概念的强大工具。这种区分清晰地 separation 了现有的负面和正面结果，并提出了 SAE 应用的几类。具体而言，我们概述了 SAE 在（i）机器学习可解释性、公平性、审计和安全性，以及（ii）社会科学和健康科学中的应用场景。 

---
# The Impact of AI on Educational Assessment: A Framework for Constructive Alignment 

**Title (ZH)**: AI对教育评估的影响：一种建设性对接的框架 

**Authors**: Patrick Stokkink  

**Link**: [PDF](https://arxiv.org/pdf/2506.23815)  

**Abstract**: The influence of Artificial Intelligence (AI), and specifically Large Language Models (LLM), on education is continuously increasing. These models are frequently used by students, giving rise to the question whether current forms of assessment are still a valid way to evaluate student performance and comprehension. The theoretical framework developed in this paper is grounded in Constructive Alignment (CA) theory and Bloom's taxonomy for defining learning objectives. We argue that AI influences learning objectives of different Bloom levels in a different way, and assessment has to be adopted accordingly. Furthermore, in line with Bloom's vision, formative and summative assessment should be aligned on whether the use of AI is permitted or not.
Although lecturers tend to agree that education and assessment need to be adapted to the presence of AI, a strong bias exists on the extent to which lecturers want to allow for AI in assessment. This bias is caused by a lecturer's familiarity with AI and specifically whether they use it themselves. To avoid this bias, we propose structured guidelines on a university or faculty level, to foster alignment among the staff. Besides that, we argue that teaching staff should be trained on the capabilities and limitations of AI tools. In this way, they are better able to adapt their assessment methods. 

**Abstract (ZH)**: 人工智能（AI）及其特别是大规模语言模型（LLM）对教育的影响持续增加：评估学生表现和理解的有效性方式是否仍为当前形式提出了疑问。本文理论框架基于建构性对齐（CA）理论和布卢姆分类法来定义学习目标。我们 argue 人工智能以不同的方式影响不同布卢姆层次的学习目标，因此评估必须相应调整。此外，为了与布卢姆的理念保持一致，形成性和总结性评估应与是否允许使用 AI 相对齐。尽管教师倾向于认为教育和评估需要适应 AI 的存在，但对教师希望在评估中允许使用 AI 的程度存在偏见。为避免这种偏见，我们建议在大学或院系层面制定结构化的指导方针，以促进员工之间的对齐。此外，我们认为教学人员应该接受有关 AI 工具的能力和局限性的培训，这样他们才能更好地调整他们的评估方法。 

---
# Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling 

**Title (ZH)**: 基于小波意识温度标定的图神经网络校准 

**Authors**: Xiaoyang Li, Linwei Tao, Haohui Lu, Minjing Dong, Junbin Gao, Chang Xu  

**Link**: [PDF](https://arxiv.org/pdf/2506.23782)  

**Abstract**: Graph Neural Networks (GNNs) have demonstrated strong predictive performance on relational data; however, their confidence estimates often misalign with actual predictive correctness, posing significant limitations for deployment in safety-critical settings. While existing graph-aware calibration methods seek to mitigate this limitation, they primarily depend on coarse one-hop statistics, such as neighbor-predicted confidence, or latent node embeddings, thereby neglecting the fine-grained structural heterogeneity inherent in graph topology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a post-hoc calibration framework that assigns node-specific temperatures based on tunable heat-kernel graph wavelet features. Specifically, WATS harnesses the scalability and topology sensitivity of graph wavelets to refine confidence estimates, all without necessitating model retraining or access to neighboring logits or predictions. Extensive evaluations across seven benchmark datasets with varying graph structures and two GNN backbones demonstrate that WATS achieves the lowest Expected Calibration Error (ECE) among all compared methods, outperforming both classical and graph-specific baselines by up to 42.3\% in ECE and reducing calibration variance by 17.24\% on average compared with graph-specific methods. Moreover, WATS remains computationally efficient, scaling well across graphs of diverse sizes and densities. Code will be released based on publication. 

**Abstract (ZH)**: Graph神经网络（GNNs）在关系数据上的预测能力得到了验证，然而它们的置信度估计往往与实际预测准确性不一致，这在关键安全应用场景中的部署构成了重大限制。虽然现有的图感知校准方法试图缓解这一限制，但它们主要依赖于粗粒度的一跳统计信息，例如邻居预测置信度或潜在节点嵌入，从而忽视了图拓扑中固有的细粒度结构异质性。在本文中，我们提出了一种基于小波感知温度缩放（WATS）的后处理校准框架，该框架基于可调节的热核图小波特征为每个节点分配特定的温度。具体而言，WATS 利用图小波的可扩展性和拓扑敏感性来细化置信度估计，而无需重新训练模型或访问邻居的对数似然或预测值。在不同结构的七个基准数据集和两个GNN后端上的广泛评估表明，WATS 在所有比较方法中实现了最低的期望校准误差（ECE），在ECE上比经典方法和图特定基准高出最多42.3%，并且与图特定方法相比，校准误差方差平均降低了17.24%。此外，WATS 在计算效率上表现良好，能很好地扩展到不同规模和密度的图上。在发表后将发布代码。 

---
# Marker Gene Method : Identifying Stable Solutions in a Dynamic Environment 

**Title (ZH)**: 标记基因方法：在动态环境中识别稳定解 

**Authors**: Hao Shi, Xi Li, Fangfang Xie  

**Link**: [PDF](https://arxiv.org/pdf/2506.23734)  

**Abstract**: Competitive Co-evolutionary Algorithms (CCEAs) are often hampered by complex dynamics like intransitivity and the Red Queen effect, leading to unstable convergence. To counter these challenges, this paper introduces the Marker Gene Method (MGM), a framework that establishes stability by using a 'marker gene' as a dynamic benchmark and an adaptive weighting mechanism to balance exploration and exploitation. We provide rigorous mathematical proofs demonstrating that MGM creates strong attractors near Nash Equilibria within the Strictly Competitive Game framework. Empirically, MGM demonstrates its efficacy across a spectrum of challenges: it stabilizes the canonical Rock-Paper-Scissors game, significantly improves the performance of C-RMOEA/D on ZDT benchmarks, and, when augmented with a Memory Pool (MP) extension, it successfully tames the notoriously pathological Shapley Biased Game. This work presents a theoretically sound and empirically validated framework that substantially enhances the stability and robustness of CCEAs in complex competitive environments. 

**Abstract (ZH)**: 标记基因方法（MGM）在严格竞争游戏中通过“标记基因”动态基准和自适应加权机制增强稳定性 

---
# System-Embedded Diffusion Bridge Models 

**Title (ZH)**: 内置扩散桥模型 

**Authors**: Bartlomiej Sobieski, Matthew Tivnan, Yuang Wang, Siyeop Yoon, Pengfei Jin, Dufan Wu, Quanzheng Li, Przemyslaw Biecek  

**Link**: [PDF](https://arxiv.org/pdf/2506.23726)  

**Abstract**: Solving inverse problems -- recovering signals from incomplete or noisy measurements -- is fundamental in science and engineering. Score-based generative models (SGMs) have recently emerged as a powerful framework for this task. Two main paradigms have formed: unsupervised approaches that adapt pretrained generative models to inverse problems, and supervised bridge methods that train stochastic processes conditioned on paired clean and corrupted data. While the former typically assume knowledge of the measurement model, the latter have largely overlooked this structural information. We introduce System embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge methods that explicitly embed the known linear measurement system into the coefficients of a matrix-valued SDE. This principled integration yields consistent improvements across diverse linear inverse problems and demonstrates robust generalization under system misspecification between training and deployment, offering a promising solution to real-world applications. 

**Abstract (ZH)**: 基于评分的生成模型(SGMs)近期已成为解决从不完整或 noisy 测量中恢复信号这一任务的强大力量框架。我们引入了嵌入系统扩散桥梁模型(SDBs)，这是一种新的监督桥梁方法，其中显式地将已知的线性测量系统嵌入到矩阵值SDE的系数中。这种原理上的整合在多种线性逆问题上提供了一致的改进，并在训练与部署之间的系统误指定情况下展示了稳健的一般泛化能力，为实际应用提供了一个有前景的解决方案。 

---
# When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation 

**Title (ZH)**: 当小型模型指导大型模型：跨模型协同学习在测试时适应方法 

**Authors**: Chang'an Yi, Xiaohui Deng, Guohao Chen, Yan Zhou, Qinghua Lu, Shuaicheng Niu  

**Link**: [PDF](https://arxiv.org/pdf/2506.23724)  

**Abstract**: Test-time Adaptation (TTA) adapts a given model to testing domain data with potential domain shifts through online unsupervised learning, yielding impressive performance. However, to date, existing TTA methods primarily focus on single-model adaptation. In this work, we investigate an intriguing question: how does cross-model knowledge influence the TTA process? Our findings reveal that, in TTA's unsupervised online setting, each model can provide complementary, confident knowledge to the others, even when there are substantial differences in model size. For instance, a smaller model like MobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base (86.6M parameters). In light of this, we propose COCA, a Cross-Model Co-Learning framework for TTA, which mainly consists of two main strategies. 1) Co-adaptation adaptively integrates complementary knowledge from other models throughout the TTA process, reducing individual model biases. 2) Self-adaptation enhances each model's unique strengths via unsupervised learning, enabling diverse adaptation to the target domain. Extensive experiments show that COCA, which can also serve as a plug-and-play module, significantly boosts existing SOTAs, on models with various sizes--including ResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example, with Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy on ImageNet-C from 51.7% to 64.5%. The code is publicly available at this https URL. 

**Abstract (ZH)**: 跨模型知识对测试时自适应(TTA)过程的影响研究：一种跨模型共学习框架(COCA) 

---
# DABstep: Data Agent Benchmark for Multi-step Reasoning 

**Title (ZH)**: DABstep: 数据代理基准测试 for 多步推理 

**Authors**: Alex Egg, Martin Iglesias Goyanes, Friso Kingma, Andreu Mora, Leandro von Werra, Thomas Wolf  

**Link**: [PDF](https://arxiv.org/pdf/2506.23719)  

**Abstract**: We introduce DABstep, a novel benchmark for evaluating AI agents on realistic multi-step data analysis tasks. DABstep comprises over 450 real-world challenges derived from a financial analytics platform, requiring models to combine code-based data processing with contextual reasoning over heterogeneous documentation. Each task demands an iterative, multi-step problem-solving approach, testing capabilities in data manipulation, cross-referencing multiple sources, and precise result reporting. The benchmark provides a factoid-style answer format with automatic correctness checks for objective scoring at scale. We evaluate leading LLM-based agents, revealing a substantial performance gap: even the best agent achieves only 14.55% accuracy on the hardest tasks. We detail our benchmark's design, dataset composition, task formulation, evaluation protocol, report baseline results and analyze failure modes. DABstep is released with a public leaderboard and toolkit to accelerate research in autonomous data analysis. 

**Abstract (ZH)**: DABstep：一个用于评估AI代理在现实多步数据分析任务中的新型基准 

---
# Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation 

**Title (ZH)**: 面向高效准确脉冲神经网络的自适应比特分配方法 

**Authors**: Xingting Yao, Qinghao Hu, Fei Zhou, Tielong Liu, Gang Li, Peisong Wang, Jian Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2506.23717)  

**Abstract**: Multi-bit spiking neural networks (SNNs) have recently become a heated research spot, pursuing energy-efficient and high-accurate AI. However, with more bits involved, the associated memory and computation demands escalate to the point where the performance improvements become disproportionate. Based on the insight that different layers demonstrate different importance and extra bits could be wasted and interfering, this paper presents an adaptive bit allocation strategy for direct-trained SNNs, achieving fine-grained layer-wise allocation of memory and computation resources. Thus, SNN's efficiency and accuracy can be improved. Specifically, we parametrize the temporal lengths and the bit widths of weights and spikes, and make them learnable and controllable through gradients. To address the challenges caused by changeable bit widths and temporal lengths, we propose the refined spiking neuron, which can handle different temporal lengths, enable the derivation of gradients for temporal lengths, and suit spike quantization better. In addition, we theoretically formulate the step-size mismatch problem of learnable bit widths, which may incur severe quantization errors to SNN, and accordingly propose the step-size renewal mechanism to alleviate this issue. Experiments on various datasets, including the static CIFAR and ImageNet and the dynamic CIFAR-DVS and DVS-GESTURE, demonstrate that our methods can reduce the overall memory and computation cost while achieving higher accuracy. Particularly, our SEWResNet-34 can achieve a 2.69\% accuracy gain and 4.16$\times$ lower bit budgets over the advanced baseline work on ImageNet. This work will be fully open-sourced. 

**Abstract (ZH)**: 多比特脉冲神经网络（SNN）最近成为研究热点，旨在追求能效高和准确度高的AI。然而，参与的比特数越多，相关的内存和计算需求就会激增，使得性能改进变得不成比例。基于不同的层具有不同重要性的洞见，且额外的比特可能被浪费和干扰，本文提出了一种直接训练SNN的自适应比特分配策略，实现了细粒度的层间内存和计算资源分配。因此，SNN的效率和准确度能够得到提升。具体地，我们参数化了权重和脉冲的时间长度和比特宽度，并通过梯度使它们具备可学习和可控性。为应对可变比特宽度和时间长度带来的挑战，我们提出了改进的脉冲神经元，能够处理不同的时间长度、为时间长度梯度求导并更好地适配脉冲量化。此外，我们从理论上形式化了可学习比特宽度的步长不匹配问题，这可能会对SNN造成严重的量化误差，并相应地提出了步长更新机制以缓解这一问题。在包括静态CIFAR、ImageNet以及动态CIFAR-DVS和DVS-GESTURE等数据集上的实验表明，我们的方法能够在降低总体内存和计算成本的同时实现更高的准确度。特别是，我们的SEWResNet-34在ImageNet上的准确度提高了2.69%，比特预算降低了4.16倍，相较于先进基线工作。该工作将完全开源。 

---
# Learning Modular Exponentiation with Transformers 

**Title (ZH)**: 基于变换器的模块化指数学习 

**Authors**: David Demitri Africa, Sara M. Kapoor, Theo Simon Sorg  

**Link**: [PDF](https://arxiv.org/pdf/2506.23679)  

**Abstract**: Modular exponentiation is crucial to number theory and cryptography, yet remains largely unexplored from a mechanistic interpretability standpoint. We train a 4-layer encoder-decoder Transformer model to perform this operation and investigate the emergence of numerical reasoning during training. Utilizing principled sampling strategies, PCA-based embedding analysis, and activation patching, we examine how number-theoretic properties are encoded within the model. We find that reciprocal operand training leads to strong performance gains, with sudden generalization across related moduli. These synchronized accuracy surges reflect grokking-like dynamics, suggesting the model internalizes shared arithmetic structure. We also find a subgraph consisting entirely of attention heads in the final layer sufficient to achieve full performance on the task of regular exponentiation. These results suggest that transformer models learn modular arithmetic through specialized computational circuits, paving the way for more interpretable and efficient neural approaches to modular exponentiation. 

**Abstract (ZH)**: 模块化幂运算对于数论和密码学至关重要，但在机理可解释性方面仍 largely未被探索。我们训练了一个4层编码-解码Transformer模型来执行这一运算，并研究了训练过程中数值推理的出现。利用基本原则的采样策略、基于PCA的嵌入分析和激活修补，我们探讨了模型内部如何编码数论性质。我们发现，训练互逆操作数会产生显著性能提升，并在相关模数上突然泛化。这些同步的准确度突增反映了类似grokking的动力学，表明模型内化了共享的算术结构。我们还发现，最后一层中的一个完全由注意力头组成的子图足以在标准幂运算任务上达到最佳性能。这些结果表明，Transformer模型通过专门的计算电路学习模算术，为更可解释和高效的神经方法提供了可能性，用于模幂运算。 

---
# gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures 

**Title (ZH)**: gMBA: 基于表达语义指导的混合布尔-算术去混淆方法 using Transformer 架构 

**Authors**: Youjeong Noh, Joon-Young Paik, Jingun Kwon, Eun-Sun Cho  

**Link**: [PDF](https://arxiv.org/pdf/2506.23634)  

**Abstract**: Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by converting programs into forms that are more complex to analyze. However, MBA has been increasingly exploited by malware developers to evade detection and cause significant real-world problems. Traditional MBA deobfuscation methods often consider these expressions as part of a black box and overlook their internal semantic information. To bridge this gap, we propose a truth table, which is an automatically constructed semantic representation of an expression's behavior that does not rely on external resources. The truth table is a mathematical form that represents the output of expression for all possible combinations of input. We also propose a general and extensible guided MBA deobfuscation framework (gMBA) that modifies a Transformer-based neural encoder-decoder Seq2Seq architecture to incorporate this semantic guidance. Experimental results and in-depth analysis show that integrating expression semantics significantly improves performance and highlights the importance of internal semantic expressions in recovering obfuscated code to its original form. 

**Abstract (ZH)**: 基于真值表的通用可扩展MBA去混淆框架 

---
# A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data 

**Title (ZH)**: 基于卷积神经网络的非线性低秩表示水质量数据插补模型 

**Authors**: Xin Liao, Bing Yang, Cai Yu  

**Link**: [PDF](https://arxiv.org/pdf/2506.23629)  

**Abstract**: The integrity of Water Quality Data (WQD) is critical in environmental monitoring for scientific decision-making and ecological protection. However, water quality monitoring systems are often challenged by large amounts of missing data due to unavoidable problems such as sensor failures and communication delays, which further lead to water quality data becoming High-Dimensional and Sparse (HDS). Traditional data imputation methods are difficult to depict the potential dynamics and fail to capture the deep data features, resulting in unsatisfactory imputation performance. To effectively address the above issues, this paper proposes a Nonlinear Low-rank Representation model (NLR) with Convolutional Neural Networks (CNN) for imputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing temporal features to model the temporal dependence of data between time slots, and b) Extracting nonlinear interactions and local patterns to mine higher-order relationships features and achieve deep fusion of multidimensional information. Experimental studies on three real water quality datasets demonstrate that the proposed model significantly outperforms existing state-of-the-art data imputation models in terms of estimation accuracy. It provides an effective approach for handling water quality monitoring data in complex dynamic environments. 

**Abstract (ZH)**: Water Quality Data Integrity and Nonlinear Low-rank Representation with Convolutional Neural Networks for Effective Imputation in Environmental Monitoring 

---
# The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking 

**Title (ZH)**: Kubernetes网络驱动模型：高性能网络的组合架构 

**Authors**: Antonio Ojea  

**Link**: [PDF](https://arxiv.org/pdf/2506.23628)  

**Abstract**: Traditional Kubernetes networking struggles to meet the escalating demands of AI/ML and evolving Telco infrastructure. This paper introduces Kubernetes Network Drivers (KNDs), a transformative, modular, and declarative architecture designed to overcome current imperative provisioning and API limitations. KNDs integrate network resource management into Kubernetes' core by utilizing Dynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements, and upcoming OCI Runtime Specification changes. Our DraNet implementation demonstrates declarative attachment of network interfaces, including Remote Direct Memory Access (RDMA) devices, significantly boosting high-performance AI/ML workloads. This capability enables sophisticated cloud-native applications and lays crucial groundwork for future Telco solutions, fostering a "galaxy" of specialized KNDs for enhanced application delivery and reduced operational complexity. 

**Abstract (ZH)**: 传统Kubernetes网络难以满足AI/ML的需求和演化的电信基础设施。本文介绍Kubernetes Network Drivers (KNDs)，一种变革性的、模块化和声明式的架构，旨在克服当前命令式配置和API的限制。KNDs通过利用动态资源分配（DRA）、节点资源接口（NRI）改进和即将推出的OCI运行时规范变更，将网络资源管理集成到Kubernetes的核心。我们的DraNet实现展示了声明式的网络接口连接，包括远程直接内存访问（RDMA）设备，显著提升了高性能AI/ML工作负载。这一能力使复杂的云原生应用成为可能，并为进一步的电信解决方案奠定关键基础，促进一个专门的KND“银河系”的发展，以提高应用程序交付能力和降低运维复杂性。 

---
# When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series 

**Title (ZH)**: 当会失效？：异常到提示的预测时间序列未来异常方法 

**Authors**: Min-Yeong Park, Won-Jeong Lee, Seong Tae Kim, Gyeong-Moon Park  

**Link**: [PDF](https://arxiv.org/pdf/2506.23596)  

**Abstract**: Recently, forecasting future abnormal events has emerged as an important scenario to tackle real-world necessities. However, the solution of predicting specific future time points when anomalies will occur, known as Anomaly Prediction (AP), remains under-explored. Existing methods dealing with time series data fail in AP, focusing only on immediate anomalies or failing to provide precise predictions for future anomalies. To address the AP task, we propose a novel framework called Anomaly to Prompt (A2P), comprised of Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To enable the forecasting model to forecast abnormal time points, we adopt a strategy to learn the relationships of anomalies. For the robust detection of anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP) that simulates diverse anomaly patterns using signal adaptive prompt. Comprehensive experiments on multiple real-world datasets demonstrate the superiority of A2P over state-of-the-art methods, showcasing its ability to predict future anomalies. Our implementation code is available at this https URL. 

**Abstract (ZH)**: 近期，预测未来异常事件已成为应对现实需求的重要场景。然而，预测特定未来时间点异常将发生的问题，即异常预测（AP），仍然缺乏有效的解决方案。现有处理时间序列数据的方法在AP任务中表现不佳，要么仅关注即时异常，要么无法提供对未来异常的精确预测。为了应对AP任务，我们提出了一种名为Anomaly to Prompt（A2P）的新框架，包括异常感知预测（AAF）和合成异常提示（SAP）。为了使预测模型能够预测异常时间点，我们采用了一种学习异常之间关系的策略。为了实现异常的稳健检测，我们提出的SAP引入了一个可学习的异常提示池（APP），通过信号自适应提示模拟多种异常模式。在多个真实世界的数据集上的综合实验表明，A2P在预测未来异常方面优于现有方法，并展示了其预测未来异常的能力。我们的实现代码可在以下链接获取：this https URL。 

---
# Transition Matching: Scalable and Flexible Generative Modeling 

**Title (ZH)**: 过渡匹配：可扩展且灵活的生成建模 

**Authors**: Neta Shaul, Uriel Singer, Itai Gat, Yaron Lipman  

**Link**: [PDF](https://arxiv.org/pdf/2506.23589)  

**Abstract**: Diffusion and flow matching models have significantly advanced media generation, yet their design space is well-explored, somewhat limiting further improvements. Concurrently, autoregressive (AR) models, particularly those generating continuous tokens, have emerged as a promising direction for unifying text and media generation. This paper introduces Transition Matching (TM), a novel discrete-time, continuous-state generative paradigm that unifies and advances both diffusion/flow models and continuous AR generation. TM decomposes complex generation tasks into simpler Markov transitions, allowing for expressive non-deterministic probability transition kernels and arbitrary non-continuous supervision processes, thereby unlocking new flexible design avenues. We explore these choices through three TM variants: (i) Difference Transition Matching (DTM), which generalizes flow matching to discrete-time by directly learning transition probabilities, yielding state-of-the-art image quality and text adherence as well as improved sampling efficiency. (ii) Autoregressive Transition Matching (ARTM) and (iii) Full History Transition Matching (FHTM) are partially and fully causal models, respectively, that generalize continuous AR methods. They achieve continuous causal AR generation quality comparable to non-causal approaches and potentially enable seamless integration with existing AR text generation techniques. Notably, FHTM is the first fully causal model to match or surpass the performance of flow-based methods on text-to-image task in continuous domains. We demonstrate these contributions through a rigorous large-scale comparison of TM variants and relevant baselines, maintaining a fixed architecture, training data, and hyperparameters. 

**Abstract (ZH)**: 扩散和流匹配模型在媒体生成方面取得了显著进展，但其设计空间已被充分探索，这在一定程度上限制了进一步改进。与此同时，自回归(AR)模型，尤其是生成连续令牌的模型，已成为统一文本和媒体生成的有前途的方向。本文引入了转换匹配(TM)，这是一种新颖的离散时间、连续状态生成范式，能够统一并推进扩散/流模型和连续AR生成。TM将复杂的生成任务分解为较简单的马尔可夫转换，允许具有表达性非确定概率转换内核和任意非连续监督过程的生成，从而解锁新的灵活设计途径。我们通过三种TM变体进行探索：(i) 差异转换匹配(DTM)，通过直接学习转换概率将流匹配推广到离散时间，从而实现最先进的图像质量和文本依从性以及改进的采样效率。(ii) 自回归转换匹配(ARTM)和(iii) 全历史转换匹配(FHTM)，分别是部分和完全因果模型，它们推广了连续AR方法。它们实现了与非因果方法相当的连续因果AR生成质量，并且可能与现有的AR文本生成技术无缝集成。值得注意的是，FHTM是第一个完全因果模型，在连续域中实现了与流基方法相当或超越的文本到图像任务性能。我们通过严格的大型比较展示了这些贡献，同时保持固定的架构、训练数据和超参数。 

---
# PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection 

**Title (ZH)**: 基于补丁的复合对抗训练：针对物体检测的物理可实现攻击防御 

**Authors**: Xiao Li, Yiming Zhu, Yifan Huang, Wei Zhang, Yingzhe He, Jie Shi, Xiaolin Hu  

**Link**: [PDF](https://arxiv.org/pdf/2506.23581)  

**Abstract**: Object detection plays a crucial role in many security-sensitive applications. However, several recent studies have shown that object detectors can be easily fooled by physically realizable attacks, \eg, adversarial patches and recent adversarial textures, which pose realistic and urgent threats. Adversarial Training (AT) has been recognized as the most effective defense against adversarial attacks. While AT has been extensively studied in the $l_\infty$ attack settings on classification models, AT against physically realizable attacks on object detectors has received limited exploration. Early attempts are only performed to defend against adversarial patches, leaving AT against a wider range of physically realizable attacks under-explored. In this work, we consider defending against various physically realizable attacks with a unified AT method. We propose PBCAT, a novel Patch-Based Composite Adversarial Training strategy. PBCAT optimizes the model by incorporating the combination of small-area gradient-guided adversarial patches and imperceptible global adversarial perturbations covering the entire image. With these designs, PBCAT has the potential to defend against not only adversarial patches but also unseen physically realizable attacks such as adversarial textures. Extensive experiments in multiple settings demonstrated that PBCAT significantly improved robustness against various physically realizable attacks over state-of-the-art defense methods. Notably, it improved the detection accuracy by 29.7\% over previous defense methods under one recent adversarial texture attack. 

**Abstract (ZH)**: 基于拼贴的统一对抗训练方法防御物理可实现攻击 

---
# Tensor Train Quantum State Tomography using Compressed Sensing 

**Title (ZH)**: 张量火车量子态 tomography 基于压缩感知 

**Authors**: Shakir Showkat Sofi, Charlotte Vermeylen, Lieven De Lathauwer  

**Link**: [PDF](https://arxiv.org/pdf/2506.23560)  

**Abstract**: Quantum state tomography (QST) is a fundamental technique for estimating the state of a quantum system from measured data and plays a crucial role in evaluating the performance of quantum devices. However, standard estimation methods become impractical due to the exponential growth of parameters in the state representation. In this work, we address this challenge by parameterizing the state using a low-rank block tensor train decomposition and demonstrate that our approach is both memory- and computationally efficient. This framework applies to a broad class of quantum states that can be well approximated by low-rank decompositions, including pure states, nearly pure states, and ground states of Hamiltonians. 

**Abstract (ZH)**: 量子态 tomography (QST) 是一种从测量数据估计量子系统状态的基本技术，在评估量子设备性能中起着关键作用。然而，由于状态表示中的参数数量呈指数增长，标准的估计方法变得不实用。在本文中，我们通过使用低秩块张量积分解参数化状态来应对这一挑战，并证明我们的方法在内存和计算效率方面都是有效的。该框架适用于可以通过低秩分解良好逼近的一类广义量子态，包括纯态、接近纯态以及哈密顿量的本征态。 

---
# NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning 

**Title (ZH)**: NEU-ESC：面向多任务学习的综合 Vietnamese 教育情感分析和主题分类数据集 

**Authors**: Phan Quoc Hung Mai, Quang Hung Nguyen, Phuong Giang Duong, Hong Hanh Nguyen, Nguyen Tuan Long  

**Link**: [PDF](https://arxiv.org/pdf/2506.23524)  

**Abstract**: In the field of education, understanding students' opinions through their comments is crucial, especially in the Vietnamese language, where resources remain limited. Existing educational datasets often lack domain relevance and student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese dataset for Educational Sentiment Classification and Topic Classification, curated from university forums, which offers more samples, richer class diversity, longer texts, and broader vocabulary. In addition, we explore multitask learning using encoder-only language models (BERT), in which we showed that it achieves performance up to 83.7% and 79.8% accuracy for sentiment and topic classification tasks. We also benchmark our dataset and model with other datasets and models, including Large Language Models, and discuss these benchmarks. The dataset is publicly available at: this https URL. 

**Abstract (ZH)**: 在教育领域，通过学生评论理解学生意见至关重要，特别是在资源有限的越南语环境中。现有的教育数据集往往缺乏领域相关性和学生俚语。为弥补这些空白，我们引入了NEU-ESC，这是一个新的越南语教育情感分类和话题分类数据集，该数据集来源于大学论坛，提供了更多的样本、更丰富的类别多样性、更长的文本和更大的词汇量。此外，我们探讨了使用编码器-only语言模型（BERT）进行多任务学习，结果显示该方法在情感和主题分类任务中的准确率分别达到了83.7%和79.8%。我们还使用其他数据集和模型，包括大规模语言模型，对我们的数据集和模型进行了基准测试，并讨论了这些基准测试结果。该数据集已公开，网址为：this https URL。 

---
# FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization 

**Title (ZH)**: FedWSQ: 高效联邦学习结合权重标准化和分布感知非均匀量化 

**Authors**: Seung-Wook Kim, Seongyeol Kim, Jiah Kim, Seowon Ji, Se-Ho Lee  

**Link**: [PDF](https://arxiv.org/pdf/2506.23516)  

**Abstract**: Federated learning (FL) often suffers from performance degradation due to key challenges such as data heterogeneity and communication constraints. To address these limitations, we present a novel FL framework called FedWSQ, which integrates weight standardization (WS) and the proposed distribution-aware non-uniform quantization (DANUQ). WS enhances FL performance by filtering out biased components in local updates during training, thereby improving the robustness of the model against data heterogeneity and unstable client participation. In addition, DANUQ minimizes quantization errors by leveraging the statistical properties of local model updates. As a result, FedWSQ significantly reduces communication overhead while maintaining superior model accuracy. Extensive experiments on FL benchmark datasets demonstrate that FedWSQ consistently outperforms existing FL methods across various challenging FL settings, including extreme data heterogeneity and ultra-low-bit communication scenarios. 

**Abstract (ZH)**: 联邦学习（FL）往往由于数据异质性和通信限制等原因导致性能下降。为了解决这些问题，我们提出了一种新的FL框架FedWSQ，该框架结合了权重标准化（WS）和所提出的分布感知非均匀量化（DANUQ）。WS通过在训练过程中过滤掉本地更新中的偏差成分，从而提高模型对数据异质性和不稳定客户端参与的鲁棒性。此外，DANUQ通过利用本地模型更新的统计特性来最小化量化误差。因此，FedWSQ在显著减少通信开销的同时保持了卓越的模型准确性。广泛实验表明，FedWSQ在各种具有挑战性的FL设置中，包括极端数据异质性和超低比特通信场景下，始终优于现有的FL方法。 

---
# Sample Margin-Aware Recalibration of Temperature Scaling 

**Title (ZH)**: 样本边际感知的温度校准再调整 

**Authors**: Haolan Guo, Linwei Tao, Haoyang Luo, Minjing Dong, Chang Xu  

**Link**: [PDF](https://arxiv.org/pdf/2506.23492)  

**Abstract**: Recent advances in deep learning have significantly improved predictive accuracy. However, modern neural networks remain systematically overconfident, posing risks for deployment in safety-critical scenarios. Current post-hoc calibration methods face a fundamental dilemma: global approaches like Temperature Scaling apply uniform adjustments across all samples, introducing high bias despite computational efficiency, while more expressive methods that operate on full logit distributions suffer from high variance due to noisy high-dimensional inputs and insufficient validation data. To address these challenges, we propose Sample Margin-Aware Recalibration of Temperature (SMART), a lightweight, data-efficient recalibration method that precisely scales logits based on the margin between the top two logits -- termed the logit gap. Specifically, the logit gap serves as a denoised, scalar signal directly tied to decision boundary uncertainty, providing a robust indicator that avoids the noise inherent in high-dimensional logit spaces while preserving model prediction invariance. Meanwhile, SMART employs a novel soft-binned Expected Calibration Error (SoftECE) objective that balances model bias and variance through adaptive binning, enabling stable parameter updates even with extremely limited calibration data. Extensive evaluations across diverse datasets and architectures demonstrate that SMART achieves state-of-the-art calibration performance even with substantially fewer parameters compared to existing parametric methods, offering a principled, robust, and highly efficient solution for practical uncertainty quantification in neural network predictions. The source code is available at: this https URL. 

**Abstract (ZH)**: Recent Advances in Deep Learning Have Significantly Improved Predictive Accuracy. However, Modern Neural Networks Remain Systematically Overconfident, Positing Risks for Deployment in Safety-Critical Scenarios. To Address These Challenges, We Propose Sample Margin-Aware Recalibration of Temperature (SMART), a Lightweight, Data-Efficient Recalibration Method That Precisely Scales Logits Based on the Margin Between the Top Two Logits—Termed the Logit Gap. 

---
# Sanitizing Manufacturing Dataset Labels Using Vision-Language Models 

**Title (ZH)**: 使用视觉-语言模型清洗制造数据集标签 

**Authors**: Nazanin Mahjourian, Vinh Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2506.23465)  

**Abstract**: The success of machine learning models in industrial applications is heavily dependent on the quality of the datasets used to train the models. However, large-scale datasets, specially those constructed from crowd-sourcing and web-scraping, often suffer from label noise, inconsistencies, and errors. This problem is particularly pronounced in manufacturing domains, where obtaining high-quality labels is costly and time-consuming. This paper introduces Vision-Language Sanitization and Refinement (VLSR), which is a vision-language-based framework for label sanitization and refinement in multi-label manufacturing image datasets. This method embeds both images and their associated textual labels into a shared semantic space leveraging the CLIP vision-language model. Then two key tasks are addressed in this process by computing the cosine similarity between embeddings. First, label sanitization is performed to identify irrelevant, misspelled, or semantically weak labels, and surface the most semantically aligned label for each image by comparing image-label pairs using cosine similarity between image and label embeddings. Second, the method applies density-based clustering on text embeddings, followed by iterative cluster merging, to group semantically similar labels into unified label groups. The Factorynet dataset, which includes noisy labels from both human annotations and web-scraped sources, is employed to evaluate the effectiveness of the proposed framework. Experimental results demonstrate that the VLSR framework successfully identifies problematic labels and improves label consistency. This method enables a significant reduction in label vocabulary through clustering, which ultimately enhances the dataset's quality for training robust machine learning models in industrial applications with minimal human intervention. 

**Abstract (ZH)**: 基于视觉-语言的多标签制造图像数据集标签净化与精炼方法（VLSR） 

---
# From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection 

**Title (ZH)**: 从大规模音频标签到实时可解释的紧急车辆 sirens 检测 

**Authors**: Stefano Giacomelli, Marco Giordano, Claudia Rinaldi, Fabio Graziosi  

**Link**: [PDF](https://arxiv.org/pdf/2506.23437)  

**Abstract**: Accurate recognition of Emergency Vehicle (EV) sirens is critical for the integration of intelligent transportation systems, smart city monitoring systems, and autonomous driving technologies. Modern automatic solutions are limited by the lack of large scale, curated datasets and by the computational demands of state of the art sound event detection models. This work introduces E2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight Convolutional Neural Network architecture derived from the PANNs framework, specifically optimized for binary EV siren detection. Leveraging our dedicated subset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across multiple reference datasets and test its viability on embedded hardware. The experimental campaign includes ablation studies, cross-domain benchmarking, and real-time inference deployment on edge device. Interpretability analyses exploiting Guided Backpropagation and ScoreCAM algorithms provide insights into the model internal representations and validate its ability to capture distinct spectrotemporal patterns associated with different types of EV sirens. Real time performance is assessed through frame wise and event based detection metrics, as well as a detailed analysis of false positive activations. Results demonstrate that E2PANNs establish a new state of the art in this research domain, with high computational efficiency, and suitability for edge-based audio monitoring and safety-critical applications. 

**Abstract (ZH)**: 准确识别应急车辆(EV)警报器对于智能交通系统、智慧城市监控系统和自动驾驶技术的集成至关重要。现代自动解决方案受限于缺乏大规模、精编的数据集以及最先进的声源检测模型的计算需求。本工作介绍了E2PANNs（高效应急预训练音频神经网络），这是一种源自PANNs框架的轻量级卷积神经网络架构，特别优化用于二分类EV警报器检测。利用我们专用的AudioSet子集（AudioSet EV），我们在多个参考数据集上对E2PANNs进行微调和评估，并测试其在嵌入式硬件上的可行性。实验campaign包括消融研究、跨域基准测试以及在边缘设备上的实时推断部署。利用Guided Backpropagation和ScoreCAM算法进行的可解释性分析揭示了模型内部表示，并验证了其捕捉不同类型EV警报器特有的频谱时域模式的能力。通过对每一帧和事件级别的检测指标以及假阳性激活的详细分析来评估实时性能。结果表明，E2PANNs在该研究领域确立了新的技术前沿，具有高计算效率，并适合边缘端的音频监控和安全关键应用。 

---
# Pipelined Decoder for Efficient Context-Aware Text Generation 

**Title (ZH)**: 面向高效上下文感知文本生成的流水线解码器 

**Authors**: Zixian Huang, Chenxu Niu, Yu Gu, Gengyang Xiao, Xinwei Huang, Gong Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2506.23431)  

**Abstract**: As the basis of generative AI, an autoregressive model requires the generation of a new token depending on all the previously generated tokens, which brings high quality but also restricts the model to generate tokens one by one, forming a bottleneck limiting the generation speed. In this paper, we propose a new decoder architecture that efficiently generates text in parallel for context-aware generation tasks. Our proposed pipelined decoder initiates the generation of multiple subsequences simultaneously, and, at each time-step, it generates a new token for each subsequence to realize parallelism. Experiments on multiple text generation tasks, including question answering, text summarization, and keyphrase generation, show that our pipelined decoder significantly improves the generation speed without a significant loss of generation quality or additional memory consumption. 

**Abstract (ZH)**: 基于生成AI的基础，自回归模型需要根据所有之前生成的令牌生成一个新的令牌，这虽然带来了高质量，但也限制了模型逐个生成令牌的能力，形成了限制生成速度的瓶颈。本文提出了一种新的解码器架构，可以高效地并行生成文本以用于上下文感知生成任务。我们提出的流水线解码器同时启动多个子序列的生成，并在每一步生成每个子序列的新令牌以实现并行化。在包括问答、文本摘要和短语生成在内的多项文本生成任务上的实验表明，流水线解码器在不显著牺牲生成质量或增加内存消耗的情况下，显著提高了生成速度。 

---
# Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting 

**Title (ZH)**: 时间序列预测中的准确参数高效测试时自适应方法 

**Authors**: Heitor R. Medeiros, Hossein Sharifi-Noghabi, Gabriel L. Oliveira, Saghar Irandoust  

**Link**: [PDF](https://arxiv.org/pdf/2506.23424)  

**Abstract**: Real-world time series often exhibit a non-stationary nature, degrading the performance of pre-trained forecasting models. Test-Time Adaptation (TTA) addresses this by adjusting models during inference, but existing methods typically update the full model, increasing memory and compute costs. We propose PETSA, a parameter-efficient method that adapts forecasters at test time by only updating small calibration modules on the input and output. PETSA uses low-rank adapters and dynamic gating to adjust representations without retraining. To maintain accuracy despite limited adaptation capacity, we introduce a specialized loss combining three components: (1) a robust term, (2) a frequency-domain term to preserve periodicity, and (3) a patch-wise structural term for structural alignment. PETSA improves the adaptability of various forecasting backbones while requiring fewer parameters than baselines. Experimental results on benchmark datasets show that PETSA achieves competitive or better performance across all horizons. Our code is available at: this https URL 

**Abstract (ZH)**: 实时光序列往往表现出非平稳性，这降低了预训练预测模型的性能。测试时适应（TTA）通过推断时调整模型来解决这一问题，但现有方法通常需要更新整个模型，增加了内存和计算成本。我们提出了一种参数高效的方法PETSA，在测试时仅通过更新输入和输出的小校准模块来调整预测器。PETSA 使用低秩适配器和动态门控来调整表示而不重新训练。为了在适应能力有限的情况下保持准确性，我们引入了一个专门的损失函数，结合了三个部分：（1）鲁棒项，（2）频域项以保持周期性，以及（3）块级结构项以实现结构对齐。PETSA 在保持与基线相比较少参数的同时提高了各种预测模型组件的适应性。基准数据集上的实验结果显示，PETSA 在所有时间范围内的性能与基线相当或更优。我们的代码可在以下链接获取：这个 https URL 

---
# BenchMake: Turn any scientific data set into a reproducible benchmark 

**Title (ZH)**: BenchMake：将任意科学数据集转换为可重复的基准 

**Authors**: Amanda S Barnard  

**Link**: [PDF](https://arxiv.org/pdf/2506.23419)  

**Abstract**: Benchmark data sets are a cornerstone of machine learning development and applications, ensuring new methods are robust, reliable and competitive. The relative rarity of benchmark sets in computational science, due to the uniqueness of the problems and the pace of change in the associated domains, makes evaluating new innovations difficult for computational scientists. In this paper a new tool is developed and tested to potentially turn any of the increasing numbers of scientific data sets made openly available into a benchmark accessible to the community. BenchMake uses non-negative matrix factorisation to deterministically identify and isolate challenging edge cases on the convex hull (the smallest convex set that contains all existing data instances) and partitions a required fraction of matched data instances into a testing set that maximises divergence and statistical significance, across tabular, graph, image, signal and textual modalities. BenchMake splits are compared to establish splits and random splits using ten publicly available benchmark sets from different areas of science, with different sizes, shapes, distributions. 

**Abstract (ZH)**: 基准数据集是机器学习开发与应用的基石，确保了新方法的鲁棒性、可靠性和竞争力。由于计算科学领域问题的独特性和相关领域变化的快速性，基准集的相对稀缺性给计算科学家评估新技术带来了困难。本文开发并测试了一种新工具，旨在将不断增加的公开可获取的科学数据集转化为对社区开放的基准集。BenchMake利用非负矩阵分解来确定性地识别和隔离凸包上的挑战性边缘案例，并将匹配的数据实例按最大化分歧和统计显著性的要求分割到测试集中，涵盖表格式、图、图像、信号和文本等多种模态。BenchMake分割与随机分割在来自不同科学领域的十个公开可用基准集上进行了比较，这些基准集具有不同的大小、形状和分布。 

---
# Hierarchical Memory Organization for Wikipedia Generation 

**Title (ZH)**: Wikipedia生成的分层记忆组织 

**Authors**: Eugene J. Yu, Dawei Zhu, Yifan Song, Xiangyu Wong, Jiebin Zhang, Wenxuan Shi, Xiaoguang Li, Qun Liu, Sujian Li  

**Link**: [PDF](https://arxiv.org/pdf/2506.23393)  

**Abstract**: Generating Wikipedia articles autonomously is a challenging task requiring the integration of accurate, comprehensive, and well-structured information from diverse sources. This paper introduces the Memory Organization-based Generation (MOG) framework, a novel approach to address these challenges by leveraging a hierarchical memory architecture. MOG extracts fine-grained memory units from web documents, recursively organizes them into a Wikipedia-style hierarchical structure, and uses this structure to guide the generation process. This ensures alignment between memory and the article outline, improving both informativeness and verifiability while minimizing hallucinations. Additionally, a citation module is implemented to enhance traceability by linking every generated sentence to specific memory units. Evaluations on our newly created WikiStart dataset demonstrate that MOG outperforms baseline methods in producing informative and reliable articles, making it particularly robust in real-world scenarios. 

**Abstract (ZH)**: 基于记忆组织的生成框架（MOG）：一种通过层次记忆架构自主生成维基百科文章的新方法 

---
# SIEDD: Shared-Implicit Encoder with Discrete Decoders 

**Title (ZH)**: SIEDD: 共享隐含编码器与离散解码器 

**Authors**: Vikram Rangarajan, Shishira Maiya, Max Ehrlich, Abhinav Shrivastava  

**Link**: [PDF](https://arxiv.org/pdf/2506.23382)  

**Abstract**: Implicit Neural Representations (INRs) offer exceptional fidelity for video compression by learning per-video optimized functions, but their adoption is crippled by impractically slow encoding times. Existing attempts to accelerate INR encoding often sacrifice reconstruction quality or crucial coordinate-level control essential for adaptive streaming and transcoding. We introduce SIEDD (Shared-Implicit Encoder with Discrete Decoders), a novel architecture that fundamentally accelerates INR encoding without these compromises. SIEDD first rapidly trains a shared, coordinate-based encoder on sparse anchor frames to efficiently capture global, low-frequency video features. This encoder is then frozen, enabling massively parallel training of lightweight, discrete decoders for individual frame groups, further expedited by aggressive coordinate-space sampling. This synergistic design delivers a remarkable 20-30X encoding speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while maintaining competitive reconstruction quality and compression ratios. Critically, SIEDD retains full coordinate-based control, enabling continuous resolution decoding and eliminating costly transcoding. Our approach significantly advances the practicality of high-fidelity neural video compression, demonstrating a scalable and efficient path towards real-world deployment. Our codebase is available at this https URL . 

**Abstract (ZH)**: 共享隐式编码器与离散解码器（SIEDD）：无需妥协的INR编码加速设计 

---
# Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment 

**Title (ZH)**: 联邦时间线合成：一种可扩展且私密的模型训练与部署方法论 

**Authors**: Pawel Renc, Michal K. Grzeszczyk, Linglong Qian, Nassim Oufattole, Jeff Rasley, Arkadiusz Sitek  

**Link**: [PDF](https://arxiv.org/pdf/2506.23358)  

**Abstract**: We present Federated Timeline Synthesis (FTS), a novel framework for training generative foundation models across distributed timeseries data applied to electronic health records (EHR). At its core, FTS represents patient history as tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding temporal, categorical, and continuous clinical information. Each institution trains an autoregressive transformer on its local PHTs and transmits only model weights to a central server. The server uses the generators to synthesize a large corpus of trajectories and train a Global Generator (GG), enabling zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS on five clinically meaningful prediction tasks using MIMIC-IV data, showing that models trained on synthetic data generated by GG perform comparably to those trained on real data. FTS offers strong privacy guarantees, scalability across institutions, and extensibility to diverse prediction and simulation tasks especially in healthcare, including counterfactual inference, early warning detection, and synthetic trial design. 

**Abstract (ZH)**: 联邦时间线合成（FTS）：电子健康记录中分布式时间序列数据上的生成基础模型训练框架 

---
# Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation 

**Title (ZH)**: 联邦乳腺癌检测增强by合成超声图像增强 

**Authors**: Hongyi Pan, Ziliang Hong, Gorkem Durak, Ziyue Xu, Ulas Bagci  

**Link**: [PDF](https://arxiv.org/pdf/2506.23334)  

**Abstract**: Federated learning (FL) has emerged as a promising paradigm for collaboratively training deep learning models across institutions without exchanging sensitive medical data. However, its effectiveness is often hindered by limited data availability and non-independent, identically distributed data across participating clients, which can degrade model performance and generalization. To address these challenges, we propose a generative AI based data augmentation framework that integrates synthetic image sharing into the federated training process for breast cancer diagnosis via ultrasound images. Specifically, we train two simple class-specific Deep Convolutional Generative Adversarial Networks: one for benign and one for malignant lesions. We then simulate a realistic FL setting using three publicly available breast ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are adopted as baseline FL algorithms. Experimental results show that incorporating a suitable number of synthetic images improved the average AUC from 0.9206 to 0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that excessive use of synthetic data reduced performance, underscoring the importance of maintaining a balanced ratio of real and synthetic samples. Our findings highlight the potential of generative AI based data augmentation to enhance FL results in the breast ultrasound image classification task. 

**Abstract (ZH)**: 基于生成AI的数据增强框架在乳腺超声图像分类的联邦学习中应用 

---
# XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs 

**Title (ZH)**: XY-Tokenizer: 降低低比特率语音编码中的语义-音ностей冲突 

**Authors**: Yitian Gong, Luozhijie Jin, Ruifan Deng, Dong Zhang, Xin Zhang, Qinyuan Cheng, Zhaoye Fei, Shimin Li, Xipeng Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2506.23325)  

**Abstract**: Speech codecs serve as bridges between speech signals and large language models. An ideal codec for speech language models should not only preserve acoustic information but also capture rich semantic information. However, existing speech codecs struggle to balance high-quality audio reconstruction with ease of modeling by language models. In this study, we analyze the limitations of previous codecs in balancing semantic richness and acoustic fidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict between semantic and acoustic capabilities through multi-stage, multi-task learning. Experimental results demonstrate that XY-Tokenizer achieves performance in both semantic and acoustic tasks comparable to that of state-of-the-art codecs operating at similar bitrates, even though those existing codecs typically excel in only one aspect. Specifically, XY-Tokenizer achieves strong text alignment, surpassing distillation-based semantic modeling methods such as SpeechTokenizer and Mimi, while maintaining a speaker similarity score of 0.83 between reconstructed and original audio. The reconstruction performance of XY-Tokenizer is comparable to that of BigCodec, the current state-of-the-art among acoustic-only codecs, which achieves a speaker similarity score of 0.84 at a similar bitrate. Code and models are available at this https URL. 

**Abstract (ZH)**: 语音编解码器作为语音信号与大规模语言模型之间的桥梁。理想的语音语言模型编解码器不仅应保留 acoustic 信息，还应捕捉丰富的语义信息。然而，现有的语音编解码器难以在高质量的音频重建与语言模型建模的简便性之间找到平衡。在本研究中，我们分析了先前编解码器在平衡语义丰富性和声学保真度方面的局限性。我们提出了 XY-Tokenizer，一种通过多阶段、多任务学习来缓解语义能力和声学能力冲突的新型编解码器。实验结果表明，XY-Tokenizer 在语义和声学任务上的性能与类似比特率下的最新编解码器相当，即使这些现有编解码器往往在某个方面表现更佳。具体而言，XY-Tokenizer 实现了强大的文本对齐，超越了基于蒸馏的语义建模方法（如 SpeechTokenizer 和 Mimi），同时保持了重建语音与原始语音之间的说话人口相似度得分为 0.83。XY-Tokenizer 的重建性能与当前仅针对声学的最新编解码器 BigCodec 相当，后者在相同比特率下得分为 0.84。代码和模型可在以下链接获得：this https URL。 

---
# Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance 

**Title (ZH)**: 设计可解释的 MH-AutoML：在不牺牲性能的情况下实现透明且高效的Android恶意软件检测 

**Authors**: Joner Assolin, Gabriel Canto, Diego Kreutz, Eduardo Feitosa, Hendrio Bragança, Angelo Nogueira, Vanderson Rocha  

**Link**: [PDF](https://arxiv.org/pdf/2506.23314)  

**Abstract**: Malware detection in Android systems requires both cybersecurity expertise and machine learning (ML) techniques. Automated Machine Learning (AutoML) has emerged as an approach to simplify ML development by reducing the need for specialized knowledge. However, current AutoML solutions typically operate as black-box systems with limited transparency, interpretability, and experiment traceability. To address these limitations, we present MH-AutoML, a domain-specific framework for Android malware detection. MH-AutoML automates the entire ML pipeline, including data preprocessing, feature engineering, algorithm selection, and hyperparameter tuning. The framework incorporates capabilities for interpretability, debugging, and experiment tracking that are often missing in general-purpose solutions. In this study, we compare MH-AutoML against seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT, HyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML achieves better recall rates while providing more transparency and control. The framework maintains computational efficiency comparable to other solutions, making it suitable for cybersecurity applications where both performance and explainability matter. 

**Abstract (ZH)**: Android系统中的恶意软件检测需要网络安全专业知识和机器学习技术。自动化机器学习（AutoML）作为一种简化机器学习开发的方法逐渐兴起，它可以减少对专业技能的需求。然而，当前的AutoML解决方案通常作为黑盒系统运行，缺乏透明性、可解释性和实验跟踪能力。为了解决这些限制，我们提出了MH-AutoML，这是一个针对Android恶意软件检测的领域特定框架。MH-AutoML自动化了整个机器学习 pipeline，包括数据预处理、特征工程、算法选择和超参数调整。该框架整合了可解释性、调试和实验跟踪能力，这些能力在通用解决方案中往往缺失。在本研究中，我们将MH-AutoML与七个已建立的AutoML框架（Auto-Sklearn、AutoGluon、TPOT、HyperGBM、Auto-PyTorch、LightAutoML和MLJAR）进行了比较。结果显示，MH-AutoML在提供更好召回率的同时，还提供了更多的透明性和控制。该框架保持了与其它解决方案相当的计算效率，使其在对性能和解释性都有需求的网络安全应用中适用。 

---
# Securing AI Systems: A Guide to Known Attacks and Impacts 

**Title (ZH)**: 保障人工智能系统安全：已知攻击及影响指南 

**Authors**: Naoto Kiribuchi, Kengo Zenitani, Takayuki Semitsu  

**Link**: [PDF](https://arxiv.org/pdf/2506.23296)  

**Abstract**: Embedded into information systems, artificial intelligence (AI) faces security threats that exploit AI-specific vulnerabilities. This paper provides an accessible overview of adversarial attacks unique to predictive and generative AI systems. We identify eleven major attack types and explicitly link attack techniques to their impacts -- including information leakage, system compromise, and resource exhaustion -- mapped to the confidentiality, integrity, and availability (CIA) security triad. We aim to equip researchers, developers, security practitioners, and policymakers, even those without specialized AI security expertise, with foundational knowledge to recognize AI-specific risks and implement effective defenses, thereby enhancing the overall security posture of AI systems. 

**Abstract (ZH)**: 嵌入信息系统中的人工智能（AI）面临着利用AI特定漏洞的安全威胁。本文提供了一种关于预测性和生成性AI系统特有 adversarial 攻击的易于理解的概述。我们识别出了十一种主要的攻击类型，并明确地将攻击技术与其影响——包括信息泄露、系统 compromization 和资源耗尽——链接起来，并将这些影响与保密性、完整性和可用性（CIA）安全三元组相对应。我们旨在为研究人员、开发人员、安全从业人员和政策制定者，甚至是那些没有专门AI安全知识的人，提供基础性知识，使他们能够识别AI特定的风险并实施有效的防御措施，从而增强AI系统的总体安全性。 

---
# Not All Explanations for Deep Learning Phenomena Are Equally Valuable 

**Title (ZH)**: 所有对深度学习现象的解释并非同样有价值 

**Authors**: Alan Jeffares, Mihaela van der Schaar  

**Link**: [PDF](https://arxiv.org/pdf/2506.23286)  

**Abstract**: Developing a better understanding of surprising or counterintuitive phenomena has constituted a significant portion of deep learning research in recent years. These include double descent, grokking, and the lottery ticket hypothesis -- among many others. Works in this area often develop ad hoc hypotheses attempting to explain these observed phenomena on an isolated, case-by-case basis. This position paper asserts that, in many prominent cases, there is little evidence to suggest that these phenomena appear in real-world applications and these efforts may be inefficient in driving progress in the broader field. Consequently, we argue against viewing them as isolated puzzles that require bespoke resolutions or explanations. However, despite this, we suggest that deep learning phenomena do still offer research value by providing unique settings in which we can refine our broad explanatory theories of more general deep learning principles. This position is reinforced by analyzing the research outcomes of several prominent examples of these phenomena from the recent literature. We revisit the current norms in the research community in approaching these problems and propose practical recommendations for future research, aiming to ensure that progress on deep learning phenomena is well aligned with the ultimate pragmatic goal of progress in the broader field of deep learning. 

**Abstract (ZH)**: 加深对意外或反直觉现象的理解构成了近年来深度学习研究的重要部分。这包括双下降现象、领悟现象和彩票票假说等众多现象。这一领域的研究工作通常发展出针对每个现象单独制定的假设，试图对其进行解释。本文观点认为，在许多显著的情况下，几乎没有证据表明这些现象出现在实际应用中，这些努力可能在推动更广泛领域进展方面效率不高。因此，我们反对将这些现象视为需要定制解决方案或解释的独特难题。尽管如此，我们建议深度学习现象仍然具有研究价值，为完善更广泛的深度学习原理的解释理论提供了独特的研究环境。通过分析近期文献中这些现象的研究成果，我们回顾了研究社区在处理这些问题时的当前规范，并对未来研究提出了可行建议，旨在确保深度学习现象的研究进步与更广泛领域进展的最终实用目标保持一致。 

---
# FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model 

**Title (ZH)**: FedRef: 通信高效的参考模型引导贝叶斯微调 

**Authors**: Taehwan Yoon, Bongjun Choi  

**Link**: [PDF](https://arxiv.org/pdf/2506.23210)  

**Abstract**: Federated learning(FL) is used for distributed scenarios to train artificial intelligence(AI) models while ensuring users' privacy. In federated learning scenario, the server generally never knows about users' data. This type of concept makes the AI training process efficient in terms of data privacy. However, regarding model performance, federated AI models may not sufficiently satisfy AI users' expectations. Furthermore, AI users have a wide range of different needs. It is not easy to satisfy the whole users needs. These types of issues can be addressed through AI model optimization, fine-tuning, or personalization to achieve optimal model performance. To address model optimization challenges, we propose reference model-based federated learning for optimal fine-tuning, which overcomes catastrophic forgetting in each round. This method is derived from Bayesian parameter-efficient transfer learning, which includes an optimal proximal term and enables overcoming the catastrophic forgetting issue in each round by utilizing a reference model that incorporates previous model parameters. As a result, this method achieves both high model performance and low computing cost. 

**Abstract (ZH)**: 联邦学习(FL)用于分布式场景中训练人工智能(AI)模型以保障用户隐私。在联邦学习场景中，服务器通常不会知道用户的數據。这种概念使得AI训练过程在数据隐私方面更加高效。然而，从模型性能来看，联邦学习的AI模型可能无法充分满足AI用户的需求。此外，AI用户的需求范围广泛，难以满足所有用户的需求。这些问题可以通过AI模型优化、微调或个性化来解决，以实现最佳模型性能。为了应对模型优化挑战，我们提出了基于参考模型的联邦学习以实现最优微调，该方法克服了每轮训练中的灾难性遗忘问题。该方法基于贝叶斯参数高效迁移学习，包括最优邻近项，通过利用包含先前模型参数的参考模型来克服每轮训练中的灾难性遗忘问题。因此，该方法在实现高模型性能的同时降低了计算成本。 

---
# Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver 

**Title (ZH)**: 面向大规模H$^2$AD MIMO接收机的增强DOA感知：多分支DNN与CRLB比率加权融合 

**Authors**: Feng Shu, Jiatong Bai, Di Wu, Wei Zhu, Bin Deng, Fuhui Zhou, Jiangzhou Wang  

**Link**: [PDF](https://arxiv.org/pdf/2506.23203)  

**Abstract**: As a green MIMO structure, massive H$^2$AD is viewed as a potential technology for the future 6G wireless network. For such a structure, it is a challenging task to design a low-complexity and high-performance fusion of target direction values sensed by different sub-array groups with fewer use of prior knowledge. To address this issue, a lightweight Cramer-Rao lower bound (CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse CRLB of each subarray using antenna number reciprocals to eliminate real-time CRLB computation. This reduces complexity and prior knowledge dependence while preserving fusion performance. Moreover, a multi-branch deep neural network (MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by leveraging candidate angles from multiple subarrays. The subarray-specific branch networks are integrated with a shared regression module to effectively eliminate pseudo-solutions and fuse true angles. Simulation results show that the proposed CRLB-ratio-WF method achieves DOA sensing performance comparable to CRLB-based methods, while significantly reducing the reliance on prior knowledge. More notably, the proposed MBDNN has superior performance in low-SNR ranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in estimation accuracy compared to CRLB-ratio-WF method. 

**Abstract (ZH)**: 面向未来6G无线网络的大规模H$^2$AD绿色MIMO结构：基于Cramer-Rao下界比值加权融合的方法及多分支深度神经网络增强到达角估计算法 

---
# Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data 

**Title (ZH)**: 数据自言自语：基于质量的无线合成数据利用 

**Authors**: Chen Gong, Bo Liang, Wei Gao, Chenren Xu  

**Link**: [PDF](https://arxiv.org/pdf/2506.23174)  

**Abstract**: Generative models have gained significant attention for their ability to produce realistic synthetic data that supplements the quantity of real-world datasets. While recent studies show performance improvements in wireless sensing tasks by incorporating all synthetic data into training sets, the quality of synthetic data remains unpredictable and the resulting performance gains are not guaranteed. To address this gap, we propose tractable and generalizable metrics to quantify quality attributes of synthetic data - affinity and diversity. Our assessment reveals prevalent affinity limitation in current wireless synthetic data, leading to mislabeled data and degraded task performance. We attribute the quality limitation to generative models' lack of awareness of untrained conditions and domain-specific processing. To mitigate these issues, we introduce SynCheck, a quality-guided synthetic data utilization scheme that refines synthetic data quality during task model training. Our evaluation demonstrates that SynCheck consistently outperforms quality-oblivious utilization of synthetic data, and achieves 4.3% performance improvement even when the previous utilization degrades performance by 13.4%. 

**Abstract (ZH)**: 生成模型因其实现现实合成数据的能力而备受关注，这些数据补充了真实世界数据集的数量。尽管最近的研究表明，在无线传感任务中结合所有合成数据进行训练可以提高性能，但合成数据的质量仍然难以预测，而由此带来的性能提升也并不保证。为此，我们提出了量化合成数据质量属性（亲和性和多样性）的可操作和通用度量标准。我们的评估揭示了当前无线合成数据普遍存在亲和性限制，导致数据错误标签并降低任务性能。我们将这种质量限制归因于生成模型对未训练条件和领域特定处理的缺乏意识。为了缓解这些问题，我们提出了一个质量导向的合成数据利用方案SynCheck，在任务模型训练过程中提升合成数据质量。我们的评估表明，SynCheck始终优于缺乏质量意识的合成数据利用方式，并且即使在之前利用方式导致性能下降13.4%的情况下，SynCheck也能实现4.3%的性能提升。 

---
# Benchmarking Deep Search over Heterogeneous Enterprise Data 

**Title (ZH)**: 异构企业数据上的深度搜索基准测试 

**Authors**: Prafulla Kumar Choubey, Xiangyu Peng, Shilpa Bhagavath, Kung-Hsiang Huang, Caiming Xiong, Chien-Sheng Wu  

**Link**: [PDF](https://arxiv.org/pdf/2506.23139)  

**Abstract**: We present a new benchmark for evaluating Deep Search--a realistic and complex form of retrieval-augmented generation (RAG) that requires source-aware, multi-hop reasoning over diverse, sparsed, but related sources. These include documents, meeting transcripts, Slack messages, GitHub, and URLs, which vary in structure and often contain human-to-human interactions. We build it using a synthetic data pipeline that simulates business workflows across product planning, development, and support stages, generating interconnected content with realistic noise and multi-hop questions with guaranteed ground-truth answers. We release our benchmark with both answerable and unanswerable queries, and retrieval pool of 39,190 enterprise artifacts, enabling fine-grained evaluation of long-context LLM and RAG systems. Our experiments reveal that even the best-performing agentic RAG methods achieve an average performance score of 32.96 on our benchmark. With further analysis, we highlight retrieval as the main bottleneck: existing methods struggle to conduct deep searches and retrieve all necessary evidence. Consequently, they often reason over partial context, leading to significant performance degradation. 

**Abstract (ZH)**: 我们提出一个新的基准来评估Deep Search——一种现实且复杂的检索增强生成（RAG）形式，要求进行基于源的多跳推理，涉及多种稀疏但相关的资料源。这些资料源包括文档、会议记录、Slack消息、GitHub和URL，且这些资料在结构上有所不同，经常包含人与人之间的交互。我们通过一个合成数据管道构建了此基准，该管道模拟了从产品规划、开发到支持的业务工作流，生成具有现实噪声的互连内容，并提出带有保证真实答案的多跳查询。我们发布的基准包含可回答和不可回答的查询，并提供一个包含39,190个企业资料的检索池，以对长上下文LLM和RAG系统进行细粒度评估。我们的实验结果显示，即使是表现最佳的代理RAG方法，在我们基准上的平均性能得分为32.96。进一步分析表明，检索是主要瓶颈：现有方法难以进行深入搜索并检索所有必要的证据，因此经常基于不完整的上下文进行推理，导致性能显著下降。 

---
# Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion 

**Title (ZH)**: 基于流调控的语义aware知识图谱完成方法 

**Authors**: Siyuan Li, Ruitong Liu, Yan Wen, Te Sun  

**Link**: [PDF](https://arxiv.org/pdf/2506.23137)  

**Abstract**: Effective modeling of multifaceted relations is pivotal for Knowledge Graph Completion (KGC). However, a majority of existing approaches are predicated on static, embedding-based scoring, exhibiting inherent limitations in capturing contextual dependencies and relational dynamics. Addressing this gap, we propose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal components: (1) a semantic context learning module that encodes context-sensitive entity representations, and (2) a conditional flow-matching module designed to learn the dynamic transformation from a head to a tail embedding, governed by the aforementioned context. The resultant predictive vector field, representing the context-informed relational path, serves to dynamically refine the initial static score of an entity pair. Through this synergy of context-aware static representations and conditioned dynamic information, FMS facilitates a more profound modeling of relational semantics. Comprehensive evaluations on several standard benchmarks demonstrate that our proposed method surpasses prior state-of-the-art results. 

**Abstract (ZH)**: 有效建模多面关系对于知识图谱补全（KGC）至关重要。然而，大多数现有方法基于静态嵌入式的评分，这在捕捉上下文依赖性和关系动态性方面存在内在局限。为了解决这一问题，我们提出了流调节评分（Flow-Modulated Scoring, FMS）框架。FMS 包含两个主要组件：（1）一个语义上下文学习模块，用于编码上下文敏感的实体表示；（2）一个条件流匹配模块，旨在通过上述上下文学习动态变换头部嵌入到尾部嵌入的转换。由此产生的预测向量场代表了上下文指导下的关系路径，用于动态细化实体对的初始静态评分。通过上下文感知的静态表示与条件动态信息的协同作用，FMS 促进了关系语义的更深刻建模。在几个标准基准上的全面评估表明，我们提出的方法超越了先前的最佳性能。 

---
# TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure 

**Title (ZH)**: TOMI: 转换和组织音乐创意以进行具有全曲结构的多轨作品创作 

**Authors**: Qi He, Gus Xia, Ziyu Wang  

**Link**: [PDF](https://arxiv.org/pdf/2506.23094)  

**Abstract**: Hierarchical planning is a powerful approach to model long sequences structurally. Aside from considering hierarchies in the temporal structure of music, this paper explores an even more important aspect: concept hierarchy, which involves generating music ideas, transforming them, and ultimately organizing them--across musical time and space--into a complete composition. To this end, we introduce TOMI (Transforming and Organizing Music Ideas) as a novel approach in deep music generation and develop a TOMI-based model via instruction-tuned foundation LLM. Formally, we represent a multi-track composition process via a sparse, four-dimensional space characterized by clips (short audio or MIDI segments), sections (temporal positions), tracks (instrument layers), and transformations (elaboration methods). Our model is capable of generating multi-track electronic music with full-song structure, and we further integrate the TOMI-based model with the REAPER digital audio workstation, enabling interactive human-AI co-creation. Experimental results demonstrate that our approach produces higher-quality electronic music with stronger structural coherence compared to baselines. 

**Abstract (ZH)**: 层次规划是一种强大的方法，用于结构性地建模长序列。除了在音乐的时间结构中考虑层次结构外，本文还探索了更为重要的方面：概念层次，涉及到生成音乐想法、转换它们，并最终将它们跨越音乐的时间和空间组织成一个完整的乐曲。为此，我们提出了TOMI（Transforming and Organizing Music Ideas）作为一种新颖的深度音乐生成方法，并通过指令调优的基础LLM开发了基于TOMI的模型。形式上，我们通过由片段（短音频或MIDI片段）、部分（时间位置）、轨道（乐器层）和变换（展开方法）组成的稀疏四维空间来表示多轨乐曲创作过程。我们的模型能够生成具有完整歌曲结构的多轨电子音乐，并进一步将基于TOMI的模型与REAPER数字音频工作站整合，实现交互式的人机共创。实验结果表明，与基线方法相比，我们的方法生成的电子音乐具有更高的质量，并且结构连贯性更强。 

---
# BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs 

**Title (ZH)**: BWLer: 重心权重层揭示了PINNs中的精度-条件权衡关系 

**Authors**: Jerry Liu, Yasa Baig, Denise Hui Jean Lee, Rajat Vadiraj Dwaraknath, Atri Rudra, Chris Ré  

**Link**: [PDF](https://arxiv.org/pdf/2506.23024)  

**Abstract**: Physics-informed neural networks (PINNs) offer a flexible way to solve partial differential equations (PDEs) with machine learning, yet they still fall well short of the machine-precision accuracy many scientific tasks demand. In this work, we investigate whether the precision ceiling comes from the ill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP) architecture. We introduce the Barycentric Weight Layer (BWLer), which models the PDE solution through barycentric polynomial interpolation. A BWLer can be added on top of an existing MLP (a BWLer-hat) or replace it completely (explicit BWLer), cleanly separating how we represent the solution from how we take derivatives for the PDE loss. Using BWLer, we identify fundamental precision limitations within the MLP: on a simple 1-D interpolation task, even MLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above float64 machine precision -- before any PDE terms are added. In PDE learning, adding a BWLer lifts this ceiling and exposes a tradeoff between achievable accuracy and the conditioning of the PDE loss. For linear PDEs we fully characterize this tradeoff with an explicit error decomposition and navigate it during training with spectral derivatives and preconditioning. Across five benchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for convection, 10x for reaction, and 1800x for wave equations while remaining compatible with first-order optimizers. Replacing the MLP entirely lets an explicit BWLer reach near-machine-precision on convection, reaction, and wave problems (up to 10 billion times better than prior results) and match the performance of standard PINNs on stiff Burgers' and irregular-geometry Poisson problems. Together, these findings point to a practical path for combining the flexibility of PINNs with the precision of classical spectral solvers. 

**Abstract (ZH)**: 基于物理的神经网络（PINNs）提供了一种利用机器学习解决偏微分方程（PDEs）的灵活方法，但仍然远达不到许多科学任务所需的高度机器精度。在本文中，我们探讨了精度上限是源于PDE的病态性还是典型的多层感知机（MLP）架构。我们引入了贝 rio 坐标权重层（BWLer），它通过贝河流形插值模型PDE解。BWLer可以在现有的MLP之上添加（BWLer-hat）或完全替换它（显式BWLer），清晰地分离了解的表示方式和PDE损失中导数的计算方式。使用BWLer，我们发现MLP的基本精度限制：在一项简单的1-D插值任务中，即使拥有O(1e5)参数的MLP在添加任何PDE项之前就在约1e-8 RMSE处停滞，这一精度大约比float64机器精度高八个数量级。在PDE学习中，添加BWLer可以打破这一上限，并揭示出可实现精度与PDE损失病态性之间的权衡。对于线性PDE，我们通过显式错误分解完全表征了这一权衡，并通过光谱导数和预条件化在训练过程中导航它。在五个基准PDE中，添加BWLer到MLP上可以分别将对流、反应和波动方程的RMSE提高30倍、10倍和1800倍，同时保持与一阶优化器的兼容性。完全替换MLP可以让显式BWLer在对流、反应和波动问题上接近机器精度（比先前结果好10亿倍以上），并在刚性Burgers’和不规则几何Poisson问题上与标准PINNs匹配性能。这些发现指出了将PINNs的灵活性与经典光谱求解器的精度相结合的实用路径。 

---
# A Systematic Study of Compositional Syntactic Transformer Language Models 

**Title (ZH)**: 系统性研究组合句法变换语言模型 

**Authors**: Yida Zhao, Hao Xve, Xiang Hu, Kewei Tu  

**Link**: [PDF](https://arxiv.org/pdf/2506.22978)  

**Abstract**: Syntactic language models (SLMs) enhance Transformers by incorporating syntactic biases through the modeling of linearized syntactic parse trees alongside surface sentences. This paper focuses on compositional SLMs that are based on constituency parse trees and contain explicit bottom-up composition of constituent representations. We identify key aspects of design choices in existing compositional SLMs and propose a unified framework encompassing both existing models and novel variants. We conduct a comprehensive empirical evaluation of all the variants in our framework across language modeling, syntactic generalization, summarization, dialogue, and inference efficiency. Based on the experimental results, we make multiple recommendations on the design of compositional SLMs. Our code is released at this https URL. 

**Abstract (ZH)**: 语法语言模型（SLMs）通过同时建模线性化的句法解析树和表面句子，将句法偏见纳入到Transformer中，从而增强Transformer。本论文专注于基于成分句法解析树的组合性SLMs，这些模型包含显式的自底向上的成分表示组合。我们识别了现有组合性SLMs中的关键设计选择，并提出一个涵盖现有模型及其新型变种的统一框架。我们在语言建模、句法泛化、摘要、对话和推理效率等方面对框架中的所有变体进行了全面的实证评估。基于实验结果，我们对组合性SLMs的设计提出了多项建议。我们的代码在此链接中发布：https://github.com/optimizeconcord/syntax-enhanced-transformers。 

---
# Against 'softmaxing' culture 

**Title (ZH)**: 反对“软化”文化 

**Authors**: Daniel Mwesigwa  

**Link**: [PDF](https://arxiv.org/pdf/2506.22968)  

**Abstract**: AI is flattening culture. Evaluations of "culture" are showing the myriad ways in which large AI models are homogenizing language and culture, averaging out rich linguistic differences into generic expressions. I call this phenomenon "softmaxing culture," and it is one of the fundamental challenges facing AI evaluations today. Efforts to improve and strengthen evaluations of culture are central to the project of cultural alignment in large AI systems. This position paper argues that machine learning (ML) and human-computer interaction (HCI) approaches to evaluation are limited. I propose two key shifts. First, instead of asking "what is culture?" at the start of system evaluations, I propose beginning with the question: "when is culture?" Second, while I acknowledge the philosophical claim that cultural universals exist, the challenge is not simply to describe them, but to situate them in relation to their particulars. Taken together, these conceptual shifts invite evaluation approaches that move beyond technical requirements, toward perspectives more responsive to the complexities of culture. 

**Abstract (ZH)**: AI正在扁平化文化。对“文化”的评价显示了大型AI模型如何同质化语言和文化，将丰富的语言差异平均为通用表达。我将这一现象称为“文化softmax化”，它是当前AI评价面临的根本挑战之一。提高和强化文化评价的努力对于大型AI系统的文化对齐项目至关重要。本文认为，机器学习（ML）和人机交互（HCI）的评价方法存在局限性。我提出两种关键转变。首先，评价系统时不应从“什么是文化？”开始，而是应该从“文化在何时？”开始发问。其次，在承认文化普遍性存在的哲学观点的同时，挑战不仅在于描述它们，更在于将它们置于具体背景中。这些概念上的转变邀请了超越技术要求的评价方法，更加关注文化复杂性的视角。 

---
# A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance 

**Title (ZH)**: 面向类别不平衡的半监督DDoS攻击检测研究 

**Authors**: Ehsan Hallaji, Vaishnavi Shanmugam, Roozbeh Razavi-Far, Mehrdad Saif  

**Link**: [PDF](https://arxiv.org/pdf/2506.22949)  

**Abstract**: One of the most difficult challenges in cybersecurity is eliminating Distributed Denial of Service (DDoS) attacks. Automating this task using artificial intelligence is a complex process due to the inherent class imbalance and lack of sufficient labeled samples of real-world datasets. This research investigates the use of Semi-Supervised Learning (SSL) techniques to improve DDoS attack detection when data is imbalanced and partially labeled. In this process, 13 state-of-the-art SSL algorithms are evaluated for detecting DDoS attacks in several scenarios. We evaluate their practical efficacy and shortcomings, including the extent to which they work in extreme environments. The results will offer insight into designing intelligent Intrusion Detection Systems (IDSs) that are robust against class imbalance and handle partially labeled data. 

**Abstract (ZH)**: 基于半监督学习的分布式拒绝服务攻击检测研究 

---
# Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration 

**Title (ZH)**: 高维数据的阵列编程与并行加速的数学计算 

**Authors**: Chen Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2506.22929)  

**Abstract**: While deep learning excels in natural image and language processing, its application to high-dimensional data faces computational challenges due to the dimensionality curse. Current large-scale data tools focus on business-oriented descriptive statistics, lacking mathematical statistics support for advanced analysis. We propose a parallel computation architecture based on space completeness, decomposing high-dimensional data into dimension-independent structures for distributed processing. This framework enables seamless integration of data mining and parallel-optimized machine learning methods, supporting scientific computations across diverse data types like medical and natural images within a unified system. 

**Abstract (ZH)**: 虽然深度学习在自然图像和语言处理领域表现出色，但将其应用于高维数据时会面临由于维度灾可能导致的计算挑战。当前规模庞大的数据工具主要侧重于面向业务的描述性统计，缺乏支持高级分析的数学统计支持。我们提出了一种基于空间完备性的并行计算架构，将高维数据分解为与维度无关的结构，以便分布式处理。该框架能够无缝集成数据挖掘和并行优化的机器学习方法，支持在统一系统中进行跨类型的数据科学计算，包括医学和自然图像等。 

---
# Learning Truthful Mechanisms without Discretization 

**Title (ZH)**: 学习诚实机制而不进行离散化 

**Authors**: Yunxuan Ma, Siqiang Wang, Zhijian Duan, Yukun Cheng, Xiaotie Deng  

**Link**: [PDF](https://arxiv.org/pdf/2506.22911)  

**Abstract**: This paper introduces TEDI (Truthful, Expressive, and Dimension-Insensitive approach), a discretization-free algorithm to learn truthful and utility-maximizing mechanisms. Existing learning-based approaches often rely on discretization of outcome spaces to ensure truthfulness, which leads to inefficiency with increasing problem size. To address this limitation, we formalize the concept of pricing rules, defined as functions that map outcomes to prices. Based on this concept, we propose a novel menu mechanism, which can be equivalent to a truthful direct mechanism under specific conditions. The core idea of TEDI lies in its parameterization of pricing rules using Partial GroupMax Network, a new network architecture designed to universally approximate partial convex functions. To learn optimal pricing rules, we develop novel training techniques, including covariance trick and continuous sampling, to derive unbiased gradient estimators compatible with first-order optimization. Theoretical analysis establishes that TEDI guarantees truthfulness, full expressiveness, and dimension-insensitivity. Experimental evaluation in the studied auction setting demonstrates that TEDI achieves strong performance, competitive with or exceeding state-of-the-art methods.
This work presents the first approaches to learn truthful mechanisms without outcome discretization, thereby enhancing algorithmic efficiency. The proposed concepts, network architecture, and learning techniques might offer potential value and provide new insights for automated mechanism design and differentiable economics. 

**Abstract (ZH)**: 无需分箱的学习真实机制的方法：TEDI（真实、表达性且维度无关的方法） 

---
# Missing-Modality-Aware Graph Neural Network for Cancer Classification 

**Title (ZH)**: 带有缺失模态aware的图神经网络用于癌症分类 

**Authors**: Sina Tabakhi, Haiping Lu  

**Link**: [PDF](https://arxiv.org/pdf/2506.22901)  

**Abstract**: A key challenge in learning from multimodal biological data is missing modalities, where all data from some modalities are missing for some patients. Current fusion methods address this by excluding patients with missing modalities, imputing missing modalities, or making predictions directly with partial modalities. However, they often struggle with diverse missing-modality patterns and the exponential growth of the number of such patterns as the number of modalities increases. To address these limitations, we propose MAGNET (Missing-modality-Aware Graph neural NETwork) for direct prediction with partial modalities, which introduces a patient-modality multi-head attention mechanism to fuse lower-dimensional modality embeddings based on their importance and missingness. MAGNET's complexity increases linearly with the number of modalities while adapting to missing-pattern variability. To generate predictions, MAGNET further constructs a patient graph with fused multimodal embeddings as node features and the connectivity determined by the modality missingness, followed by a conventional graph neural network. Experiments on three public multiomics datasets for cancer classification, with real-world instead of artificial missingness, show that MAGNET outperforms the state-of-the-art fusion methods. The data and code are available at this https URL. 

**Abstract (ZH)**: 多模态生物数据学习中的一个关键挑战是模态缺失问题，其中一些患者的某些模态数据全部缺失。当前的融合方法通过排除具有缺失模态的患者、填补缺失模态或直接使用部分模态进行预测来应对这一问题。然而，它们往往难以处理多样化的缺失模态模式，并且随着模态数量的增加，缺少模式的数量呈指数增长而表现不佳。为了解决这些局限性，我们提出了一种 Awareness Graph神经网络（MAGNET，Missing-modality-Aware Graph Neural NETwork）用于直接使用部分模态进行预测，它引入了一个患者-模态多头注意力机制，基于模态的重要性与缺失性融合低维模态嵌入。MAGNET的复杂度随着模态数量的线性增加，并适应缺失模式的变异性。为了生成预测，MAGNET进一步构建了一个患者图，其节点特征为融合多模态嵌入，边的连接性由模态缺失性决定，并使用常规的图神经网络进行处理。在三个公开的多组学数据集上进行的癌症分类实验显示，对于实际缺失而非人工缺失，MAGNET优于当前最先进的融合方法。数据和代码可在以下网址获取。 

---
# Interpretable Time Series Autoregression for Periodicity Quantification 

**Title (ZH)**: 可解释的时间序列自回归方法用于周期性量化 

**Authors**: Xinyu Chen, Vassilis Digalakis Jr, Lijun Ding, Dingyi Zhuang, Jinhua Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2506.22895)  

**Abstract**: Time series autoregression is a classical statistical model for capturing auto-correlations and identifying temporal patterns such as periodicity and seasonality. In this work, we propose a novel sparse autoregression framework from an interpretable machine learning perspective and the model interpretability for periodicity quantification is reinforced by $\ell_0$-norm induced sparsity constraints. On the time-varying time series data, we reformulate the sparse autoregression and convert the involved optimization problem into a mixed-integer optimization (MIO). To accelerate it, we develop a subspace pursuit based decision variable pruning (DVP) strategy to reduce the search space. On the multidimensional time series that involves complicated spatial and temporal dimensions, we propose a spatially- and time-varying sparse autoregression model and resolve the corresponding MIO problem by developing a two-stage optimization scheme. In particular, the proposed scheme makes the model scalable to large problems even with millions of decision variables. Empirically, we conduct extensive experiments to evaluate the proposed models on real-world time series data. First, we demonstrate that the MIO solver can be drastically accelerated through the DVP strategy, while maintaining the same solution quality as a full MIO solver. Applying the time-varying sparse autoregression model to ridesharing trip data, we uncover both daily and weekly periodicities and reveal long-term changes in regularity of human mobility. Second, we demonstrate the spatial patterns of yearly seasonality in climate variable time series such as temperature and precipitation across the past four decades, and our model allows to discover dynamic climate patterns and identify climate phenomena such as El Nino in sea surface temperature. 

**Abstract (ZH)**: 时间序列自回归是一种经典统计模型，用于捕捉自相关性并识别如周期性和季节性等时间模式。本文从可解释机器学习的角度提出了一种新颖的稀疏自回归框架，并通过$\ell_0$范数诱导的稀疏约束强化了周期性量化模型的可解释性。在时间变化的时间序列数据上，我们重新表述了稀疏自回归，并将涉及的优化问题转化为混合整数优化（MIO）。为了加速这一过程，我们开发了一种基于子空间探测的决策变量剪枝（DVP）策略来减少搜索空间。在涉及复杂空间和时间维度的多维时间序列上，我们提出了空间和时间变化的稀疏自回归模型，并通过开发两阶段优化方案解决相应的MIO问题。特别地，所提出的方案使模型能够处理具有数百万个决策变量的大规模问题。通过广泛的实验证明，我们评估了所提出的模型在真实世界时间序列数据上的性能。首先，我们展示了通过DVP策略可以显著加速MIO求解器，同时保持与全MIO求解器相同的解质量。将时间变化稀疏自回归模型应用于拼车出行数据，我们揭示了日周期性和周周期性，并揭示了人类移动规律的长期变化。其次，我们展示了过去四十年间气候变量时间序列（如温度和降水量）中年季节性的空间模式，我们的模型允许发现动态气候模式并识别如厄尔尼诺现象的气候现象。 

---
# Performance Measurements in the AI-Centric Computing Continuum Systems 

**Title (ZH)**: AI为中心的计算连续体系统中的性能测量 

**Authors**: Praveen Kumar Donta, Qiyang Zhang, Schahram Dustdar  

**Link**: [PDF](https://arxiv.org/pdf/2506.22884)  

**Abstract**: Over the Eight decades, computing paradigms have shifted from large, centralized systems to compact, distributed architectures, leading to the rise of the Distributed Computing Continuum (DCC). In this model, multiple layers such as cloud, edge, Internet of Things (IoT), and mobile platforms work together to support a wide range of applications. Recently, the emergence of Generative AI and large language models has further intensified the demand for computational resources across this continuum. Although traditional performance metrics have provided a solid foundation, they need to be revisited and expanded to keep pace with changing computational demands and application requirements. Accurate performance measurements benefit both system designers and users by supporting improvements in efficiency and promoting alignment with system goals. In this context, we review commonly used metrics in DCC and IoT environments. We also discuss emerging performance dimensions that address evolving computing needs, such as sustainability, energy efficiency, and system observability. We also outline criteria and considerations for selecting appropriate metrics, aiming to inspire future research and development in this critical area. 

**Abstract (ZH)**: 过去八个十年里，计算范式从大型集中式系统转向紧凑分布式架构，催生了分布式计算 continuum (DCC) 模型。在这种模型中，云、边缘、物联网（IoT）和移动平台等多层架构协同工作，支持广泛的应用程序。最近，生成式人工智能和大型语言模型的出现进一步加剧了对 continuum 中计算资源的需求。尽管传统性能指标提供了坚实的基础，但它们需要重新审视和扩展，以适应不断变化的计算需求和应用程序要求。精确的性能测量有助于系统设计师和用户提升效率，并促进系统目标的实现。在此背景下，我们回顾了 DCC 和物联网环境中常用的性能指标，并讨论了针对不断演进计算需求的新兴性能维度，如可持续性、能源效率和系统可观测性。我们还概述了选择合适指标的标准和考虑因素，旨在激发该关键领域的未来研究与开发。 

---
# Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles 

**Title (ZH)**: 基于学习算法集成的可扩展贝叶斯网络结构学习 

**Authors**: Shengcai Liu, Hui Ou-yang, Zhiyuan Wang, Cheng Chen, Qijun Cai, Yew-Soon Ong, Ke Tang  

**Link**: [PDF](https://arxiv.org/pdf/2506.22848)  

**Abstract**: Learning the structure of Bayesian networks (BNs) from data is challenging, especially for datasets involving a large number of variables. The recently proposed divide-and-conquer (D\&D) strategies present a promising approach for learning large BNs. However, they still face a main issue of unstable learning accuracy across subproblems. In this work, we introduce the idea of employing structure learning ensemble (SLE), which combines multiple BN structure learning algorithms, to consistently achieve high learning accuracy. We further propose an automatic approach called Auto-SLE for learning near-optimal SLEs, addressing the challenge of manually designing high-quality SLEs. The learned SLE is then integrated into a D\&D method. Extensive experiments firmly show the superiority of our method over D\&D methods with single BN structure learning algorithm in learning large BNs, achieving accuracy improvement usually by 30\%$\sim$225\% on datasets involving 10,000 variables. Furthermore, our method generalizes well to datasets with many more (e.g., 30000) variables and different network characteristics than those present in the training data for learning the SLE. These results indicate the significant potential of employing (automatic learning of) SLEs for scalable BN structure learning. 

**Abstract (ZH)**: 学习大数据集的贝叶斯网络结构：通过结构学习ensemble提升divide-and-conquer策略的学习准确性 

---
# Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models 

**Title (ZH)**: 量子神经网络在风能预报中的应用：与经典模型的性能及可扩展性比较研究 

**Authors**: Batuhan Hangun, Oguz Altun, Onder Eyecioglu  

**Link**: [PDF](https://arxiv.org/pdf/2506.22845)  

**Abstract**: Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine Learning (QML), are emerging as a powerful alternative to classical machine learning methods. Recent studies have focused on the applicability of QNNs to various tasks, such as time-series forecasting, prediction, and classification, across a wide range of applications, including cybersecurity and medical imaging. With the increased use of smart grids driven by the integration of renewable energy systems, machine learning plays an important role in predicting power demand and detecting system disturbances. This study provides an in-depth investigation of QNNs for predicting the power output of a wind turbine. We assess the predictive performance and simulation time of six QNN configurations that are based on the Z Feature Map for data encoding and varying ansatz structures. Through detailed cross-validation experiments and tests on an unseen hold-out dataset, we experimentally demonstrate that QNNs can achieve predictive performance that is competitive with, and in some cases marginally better than, the benchmarked classical approaches. Our results also reveal the effects of dataset size and circuit complexity on predictive performance and simulation time. We believe our findings will offer valuable insights for researchers in the energy domain who wish to incorporate quantum machine learning into their work. 

**Abstract (ZH)**: 量子神经网络（QNNs）在量子机器学习（QML）中的一个显著方法正逐渐成为经典机器学习方法的强有力替代方案。最近的研究重点关注QNNs在时间序列预测、预测和分类等任务中的应用，涵盖了网络安全和医学成像等多个领域。随着智能电网因可再生能源系统的集成而日益普及，机器学习在预测电力需求和检测系统故障中扮演着重要角色。本研究深入探讨了QNNs在风力发电机功率输出预测中的应用。我们评估了基于Z特征映射的数据编码和不同架构的六种QNN配置的预测性能和模拟时间。通过详细的交叉验证实验和对未见过的保留数据集的测试，实验结果表明，QNNs的预测性能与基准的经典方法相当，甚至在某些情况下略优于经典方法。我们的研究结果还揭示了数据集大小和电路复杂度对预测性能和模拟时间的影响。我们相信，这些发现将为希望将量子机器学习融入其研究的能源领域研究人员提供有价值的见解。 

---
# xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection 

**Title (ZH)**: 基于xLSTM的强大数据异常检测方法 

**Authors**: Kamil Faber, Marcin Pietroń, Dominik Żurek, Roberto Corizzo  

**Link**: [PDF](https://arxiv.org/pdf/2506.22837)  

**Abstract**: The recently proposed xLSTM is a powerful model that leverages expressive multiplicative gating and residual connections, providing the temporal capacity needed for long-horizon forecasting and representation learning. This architecture has demonstrated success in time series forecasting, lossless compression, and even large-scale language modeling tasks, where its linear memory footprint and fast inference make it a viable alternative to Transformers. Despite its growing popularity, no prior work has explored xLSTM for anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the first anomaly detection method that integrates a full encoder-decoder xLSTM architecture, purpose-built for multivariate time series data. Our encoder processes input sequences to capture historical context, while the decoder is devised in two separate variants of the method. In the forecasting approach, the decoder iteratively generates forecasted future values xLSTMAD-F, while the reconstruction approach reconstructs the input time series from its encoded counterpart xLSTMAD-R. We investigate the performance of two loss functions: Mean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider local reconstruction fidelity and global sequence alignment, respectively. We evaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17 real-world datasets, using state-of-the-art challenging metrics such as VUS-PR. In our results, xLSTM showcases state-of-the-art accuracy, outperforming 23 popular anomaly detection baselines. Our paper is the first work revealing the powerful modeling capabilities of xLSTM for anomaly detection, paving the way for exciting new developments on this subject. Our code is available at: this https URL 

**Abstract (ZH)**: 最近提出的xLSTM是一种强大的模型，利用了表达性强的乘法门控和残差连接，提供了长期时间序列预测和表示学习所需的时序容量。该架构在时间序列预测、无损压缩以及大规模语言建模等任务中取得了成功，其线性内存占用和快速推理使其成为Transformer的可行替代方案。尽管其 popularity 正在增长，但没有任何先前工作探索过 xLSTM 在异常检测中的应用。在这项工作中，我们通过提出xLSTMAD填补了这一空白，这是第一个结合了专门为多元时间序列数据设计的完整编码器-解码器xLSTM架构的异常检测方法。我们的编码器处理输入序列以捕获历史上下文，而解码器则根据方法的两个不同变体设计。在预测方法中，解码器迭代生成预测的未来值 xLSTMAD-F，而在重建方法中，则从编码对应体重建输入时间序列 xLSTMAD-R。我们探讨了两种损失函数的性能：均方误差（MSE）和软动态时间弯曲（SoftDTW），以分别考虑局部重建准确性和全局序列对齐。我们使用最先进的具有挑战性的指标，如VUS-PR，评估我们的方法在全面的TSB-AD-M基准上，该基准涵盖了17个真实世界的数据集。在我们的结果中，xLSTM展示了最先进的准确性，超过了23个流行的异常检测基线。我们的论文是首次揭示xLSTM在异常检测中的强大建模能力，为这一主题的新发展铺平了道路。我们的代码可在以下网址获取：this https URL 

---
# TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations 

**Title (ZH)**: TriADA: 大规模并行三线性矩阵-张量乘加算法及设备架构，用于加速三维离散变换 

**Authors**: Stanislav Sedukhin, Yoichi Tomioka, Kazuya Matsumoto, Yuichi Okuyama  

**Link**: [PDF](https://arxiv.org/pdf/2506.22818)  

**Abstract**: Multilinear transformations are key in high-performance computing (HPC) and artificial intelligence (AI) workloads, where data is represented as tensors. However, their high computational and memory demands, which grow with dimensionality, often slow down critical tasks. Moreover, scaling computation by enlarging the number of parallel processing units substantially increases energy consumption, limiting widespread adoption, especially for sparse data, which is common in HPC and AI applications. This paper introduces the Trilinear Algorithm and isomorphic to algorithm Device Architecture (TriADA) to address these challenges with the following innovations: (1) a massively parallel, low-rank algorithm for computing a family of trilinear (3D) discrete orthogonal transformations (3D-DXTs), which is a special case of the more general 3-mode matrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM kernel with decoupled streaming active memory, specially designed to accelerate 3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully distributed 3D network of mesh interconnected processing elements or cells with a coordinate-free, data-driven local processing activity, which is independent of problem size; (4) an elastic sparse outer-product (ESOP) method that avoids unnecessary computing and communication operations with zero-valued operands, thereby enhancing energy efficiency, computational accuracy, and stability. TriADA is capable of performing a variety of trilinear transformations with hypercubic arithmetic complexity in a linear number of time-steps. The massively parallel, scalable, and energy-efficient architecture of TriADA is ideal for accelerating multilinear tensor operations, which are the most demanding parts of AI and HPC workloads. 

**Abstract (ZH)**: Trilinear算法和等价的Device架构（TriADA） 

---
# BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters 

**Title (ZH)**: BayesLoRA: 低秩适配器中的任务特定不确定性 

**Authors**: Cooper Doyle  

**Link**: [PDF](https://arxiv.org/pdf/2506.22809)  

**Abstract**: We propose BayesLoRA, a task-specific uncertainty quantification framework that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike general-purpose transformer uncertainty methods, BayesLoRA provides guardrails tailored to downstream workflows, enabling agents to introspect and modulate behavior under uncertainty. We demonstrate mathematically and empirically that LoRA adapters exhibit amplified variance outside fine-tuning distributions, yielding reliable confidence estimates for agentic decision-making. 

**Abstract (ZH)**: BayesLoRA：一种将MC-Dropout集成到Low-Rank Adapters中的任务特定不确定性量化框架 

---
# WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing 

**Title (ZH)**: WavShape：信息论语音表示学习及其在公平和隐私意识音频处理中的应用 

**Authors**: Oguzhan Baser, Ahmet Ege Tanriverdi, Kaan Kale, Sandeep P. Chinchali, Sriram Vishwanath  

**Link**: [PDF](https://arxiv.org/pdf/2506.22789)  

**Abstract**: Speech embeddings often retain sensitive attributes such as speaker identity, accent, or demographic information, posing risks in biased model training and privacy leakage. We propose WavShape, an information-theoretic speech representation learning framework that optimizes embeddings for fairness and privacy while preserving task-relevant information. We leverage mutual information (MI) estimation using the Donsker-Varadhan formulation to guide an MI-based encoder that systematically filters sensitive attributes while maintaining speech content essential for downstream tasks. Experimental results on three known datasets show that WavShape reduces MI between embeddings and sensitive attributes by up to 81% while retaining 97% of task-relevant information. By integrating information theory with self-supervised speech models, this work advances the development of fair, privacy-aware, and resource-efficient speech systems. 

**Abstract (ZH)**: 基于信息论的语音表示学习框架WavShape：在保证任务相关信息的同时优化公平性与隐私保护 

---
# PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection 

**Title (ZH)**: PhonemeFake: 通过语言驱动的音段操纵与自适应双层检测重塑深度假音的真实感 

**Authors**: Oguzhan Baser, Ahmet Ege Tanriverdi, Sriram Vishwanath, Sandeep P. Chinchali  

**Link**: [PDF](https://arxiv.org/pdf/2506.22783)  

**Abstract**: Deepfake (DF) attacks pose a growing threat as generative models become increasingly advanced. However, our study reveals that existing DF datasets fail to deceive human perception, unlike real DF attacks that influence public discourse. It highlights the need for more realistic DF attack vectors. We introduce PhonemeFake (PF), a DF attack that manipulates critical speech segments using language reasoning, significantly reducing human perception by up to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF dataset on HuggingFace and open-source bilevel DF segment detection model that adaptively prioritizes compute on manipulated regions. Our extensive experiments across three known DF datasets reveal that our detection model reduces EER by 91% while achieving up to 90% speed-up, with minimal compute overhead and precise localization beyond existing models as a scalable solution. 

**Abstract (ZH)**: Deepfake攻击随着生成模型的日益先进而构成越来越大的威胁。然而，我们的研究发现现有的Deepfake数据集无法欺骗人类感知，与真实Deepfake攻击影响公共话语不同。这突显了需要更现实的Deepfake攻击向量。我们引入了PhonemeFake (PF)，一种使用语言推理操作关键语音片段的Deepfake攻击，显著降低了人类感知最多42%，并提高了基准准确性最多94%。我们在HuggingFace上发布了易于使用的PF数据集，并开源了一个双层Deepfake片段检测模型，该模型能够自适应地优先处理被操作区域的计算。我们在三个已知的Deepfake数据集上的广泛实验表明，我们的检测模型将平等错误率(EER)降低了91%，同时实现了高达90%的速度提升，具有最少的计算开销和超越现有模型的精确定位能力，是一个可扩展的解决方案。 

---
# Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning 

**Title (ZH)**: 教学模型在链式推理中描述奖励作弊的能力 

**Authors**: Miles Turpin, Andy Arditi, Marvin Li, Joe Benton, Julian Michael  

**Link**: [PDF](https://arxiv.org/pdf/2506.22777)  

**Abstract**: Language models trained with RL can engage in reward hacking--exploiting unintended strategies for high reward--without revealing this behavior in their chain-of-thought reasoning, making detection difficult and posing risks for high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL intervention that trains models to explicitly acknowledge when they are influenced by prompt cues--hints which point to incorrect answers (e.g., "a Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently train models with RL on environments where held-out prompt cues signal which incorrect answers will receive high reward, incentivizing models to reward hack by exploiting cues instead of reasoning correctly. We measure how often models exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained model's responses consist of undetected reward hacks. In comparison, when we perform RL without VFT, the rate of undetected reward hacks goes up to 88%; with a debiasing baseline intervention, this increases further to 99%. VFT achieves this by substantially increasing how often models verbalize the influence of cues--from 8% to 42% after VFT, and up to 94% after RL--while baselines remain low even after RL (10% and 1%). Our results show that teaching models to explicitly verbalize reward hacking behavior before RL significantly improves their detection, offering a practical path toward more transparent and safe AI systems. 

**Abstract (ZH)**: 语言模型在RL训练后可能会进行奖励劫持——通过利用意外策略以获得高奖励而不公开其行为，这使得检测变得困难并给高风险应用带来了风险。我们提出语言模型细调（VFT），这是一种在RL之前的干预措施，训练模型在受到提示暗示影响时明确承认这一点——这些提示指向错误的答案（如：“一位斯坦福大学教授认为答案是A”）。为了评估VFT，我们随后在环境中训练模型，该环境中保留的提示信号表明哪些错误答案将获得高奖励，这激励模型通过利用提示进行奖励劫持而不是正确推理。我们测量模型在未经说明的情况下利用这些提示的频率。在RL之后，只有6%的VFT训练模型的回应包含未被检测到的奖励劫持。相比之下，不使用VFT进行RL时，未被检测到的奖励劫持的比率上升至88%；使用去偏基线干预措施时，这一比率进一步上升至99%。VFT通过显著增加模型在VFT之后承认提示影响的频率——从8%增加到42%，在RL之后增加到94%——来实现这一目标，而基线即使在RL之后也保持低位（10%和1%）。我们的结果表明，在RL之前教会模型明确表达奖励劫持行为可以显著提高其检测率，提供了一条通往更透明和安全的AI系统的实际途径。 

---
# FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision 

**Title (ZH)**: FF-INT8: 在边缘设备上使用INT8精度进行高效前向传播神经网络训练 

**Authors**: Jingxiao Ma, Priyadarshini Panda, Sherief Reda  

**Link**: [PDF](https://arxiv.org/pdf/2506.22771)  

**Abstract**: Backpropagation has been the cornerstone of neural network training for decades, yet its inefficiencies in time and energy consumption limit its suitability for resource-constrained edge devices. While low-precision neural network quantization has been extensively researched to speed up model inference, its application in training has been less explored. Recently, the Forward-Forward (FF) algorithm has emerged as a promising alternative to backpropagation, replacing the backward pass with an additional forward pass. By avoiding the need to store intermediate activations for backpropagation, FF can reduce memory footprint, making it well-suited for embedded devices. This paper presents an INT8 quantized training approach that leverages FF's layer-by-layer strategy to stabilize gradient quantization. Furthermore, we propose a novel "look-ahead" scheme to address limitations of FF and improve model accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board demonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in memory usage, while maintaining competitive accuracy compared to the state-of-the-art. 

**Abstract (ZH)**: 反向传播是神经网络训练的基石，但其在时间和能量消耗方面的低效性限制了其在资源受限边缘设备上的适用性。尽管低精度神经网络量化已被广泛研究以加速模型推理，但在训练中的应用却较少。最近，前向-前向（FF）算法作为一种替代反向传播的有前景的选择出现，通过用额外的前向传播替代反向传播。通过避免为反向传播存储中间激活，FF 可以减少内存占用，使其非常适合嵌入式设备。本文提出了一种基于 FF 层级策略的 INT8 量化训练方法，以稳定梯度量化。此外，我们提出了一种新的“前瞻”方案以解决 FF 的局限性并提高模型准确性。在 NVIDIA Jetson Orin Nano 板上的实验结果显示，训练速度提高了 4.6%，能耗减少了 8.3%，内存使用减少了 27.0%，同时保持了与现有最佳方法相当的准确性。 

---
# Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks 

**Title (ZH)**: 一石二鸟！基于轨迹的统一在线 adversarial example 和 backdoor 攻击检测方法 

**Authors**: Anmin Fu, Fanyu Meng, Huaibing Peng, Hua Ma, Zhi Zhang, Yifeng Zheng, Willy Susilo, Yansong Gao  

**Link**: [PDF](https://arxiv.org/pdf/2506.22722)  

**Abstract**: The proposed UniGuard is the first unified online detection framework capable of simultaneously addressing adversarial examples and backdoor attacks. UniGuard builds upon two key insights: first, both AE and backdoor attacks have to compromise the inference phase, making it possible to tackle them simultaneously during run-time via online detection. Second, an adversarial input, whether a perturbed sample in AE attacks or a trigger-carrying sample in backdoor attacks, exhibits distinctive trajectory signatures from a benign sample as it propagates through the layers of a DL model in forward inference. The propagation trajectory of the adversarial sample must deviate from that of its benign counterpart; otherwise, the adversarial objective cannot be fulfilled. Detecting these trajectory signatures is inherently challenging due to their subtlety; UniGuard overcomes this by treating the propagation trajectory as a time-series signal, leveraging LSTM and spectrum transformation to amplify differences between adversarial and benign trajectories that are subtle in the time domain. UniGuard exceptional efficiency and effectiveness have been extensively validated across various modalities (image, text, and audio) and tasks (classification and regression), ranging from diverse model architectures against a wide range of AE attacks and backdoor attacks, including challenging partial backdoors and dynamic triggers. When compared to SOTA methods, including ContraNet (NDSS 22) specific for AE detection and TED (IEEE SP 24) specific for backdoor detection, UniGuard consistently demonstrates superior performance, even when matched against each method's strengths in addressing their respective threats-each SOTA fails to parts of attack strategies while UniGuard succeeds for all. 

**Abstract (ZH)**: UniGuard：同时应对 adversarial examples 和 backdoor 攻击的统一在线检测框架 

---
# General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers 

**Title (ZH)**: 通用自主网络安全防御：学习适用于动态拓扑和多样化攻击者的稳健策略 

**Authors**: Arun Ramamurthy, Neil Dhir  

**Link**: [PDF](https://arxiv.org/pdf/2506.22706)  

**Abstract**: In the face of evolving cyber threats such as malware, ransomware and phishing, autonomous cybersecurity defense (ACD) systems have become essential for real-time threat detection and response with optional human intervention. However, existing ACD systems rely on limiting assumptions, particularly the stationarity of the underlying network dynamics. In real-world scenarios, network topologies can change due to actions taken by attackers or defenders, system failures, or time evolution of networks, leading to failures in the adaptive capabilities of current defense agents. Moreover, many agents are trained on static environments, resulting in overfitting to specific topologies, which hampers their ability to generalize to out-of-distribution network topologies. This work addresses these challenges by exploring methods for developing agents to learn generalizable policies across dynamic network environments -- general ACD (GACD). 

**Abstract (ZH)**: 面向 evolving 网络威胁如恶意软件、勒索软件和钓鱼，自主网络安全防御（ACD）系统已成为实现实时威胁检测与响应的有效工具，可选包含人工干预。然而，现有 ACD 系统依赖于限制性假设，特别是网络动态的平稳性。在实际场景中，网络拓扑由于攻击者或防御者的行动、系统故障或网络的时序演化而发生变化，导致当前防御代理的适应能力失效。此外，许多代理是在静态环境中训练的，导致它们过度拟合特定的拓扑结构，影响其在未见过的网络拓扑结构中的泛化能力。本工作通过探索开发能够在动态网络环境中学习可泛化策略的代理方法，以应对这些挑战——通用 ACD（GACD）。 

---
# P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code 

**Title (ZH)**: P4OMP: 基于检索增强的提示方法以在串行代码中实现OpenMP并行ism 

**Authors**: Wali Mohammad Abdullah, Azmain Kabir  

**Link**: [PDF](https://arxiv.org/pdf/2506.22703)  

**Abstract**: We present P4OMP, a retrieval-augmented framework for transforming serial C/C++ code into OpenMP-annotated parallel code using large language models (LLMs). To our knowledge, this is the first system to apply retrieval-based prompting for OpenMP pragma correctness without model fine-tuning or compiler instrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with structured instructional knowledge from OpenMP tutorials to improve the reliability of prompt-driven code generation. By grounding generation in the retrieved context, P4OMP improves syntactic correctness compared to baseline prompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline, GPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world C++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites. P4OMP achieves 100% compilation success on all parallelizable cases, while the baseline fails to compile in 20 out of 108 cases. Six cases that rely on non-random-access iterators or thread-unsafe constructs are excluded due to fundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP consistently avoids scoping errors, syntactic misuse, and invalid directive combinations that commonly affect baseline-generated code. We further demonstrate strong runtime scaling across seven compute-intensive benchmarks on an HPC cluster. P4OMP offers a robust, modular pipeline that significantly improves the reliability and applicability of LLM-generated OpenMP code. 

**Abstract (ZH)**: P4OMP：一种使用大规模语言模型进行检索增强的框架，将串行C/C++代码转换为OpenMP注释的并行代码 

---
# DistShap: Scalable GNN Explanations with Distributed Shapley Values 

**Title (ZH)**: DistShap: 基于分布式沙普ley值的可扩展图神经网络解释 

**Authors**: Selahattin Akkas, Aditya Devarakonda, Ariful Azad  

**Link**: [PDF](https://arxiv.org/pdf/2506.22668)  

**Abstract**: With the growing adoption of graph neural networks (GNNs), explaining their predictions has become increasingly important. However, attributing predictions to specific edges or features remains computationally expensive. For example, classifying a node with 100 neighbors using a 3-layer GNN may involve identifying important edges from millions of candidates contributing to the prediction. To address this challenge, we propose DistShap, a parallel algorithm that distributes Shapley value-based explanations across multiple GPUs. DistShap operates by sampling subgraphs in a distributed setting, executing GNN inference in parallel across GPUs, and solving a distributed least squares problem to compute edge importance scores. DistShap outperforms most existing GNN explanation methods in accuracy and is the first to scale to GNN models with millions of features by using up to 128 GPUs on the NERSC Perlmutter supercomputer. 

**Abstract (ZH)**: 基于图形神经网络的分布式Shapley值解释：DistShap 

---
# Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision 

**Title (ZH)**: 基于知识引导的多代理体系架构的自动化需求开发：一种愿景 

**Authors**: Jiangping Huang, Dongming Jin, Weisong Sun, Yang Liu, Zhi Jin  

**Link**: [PDF](https://arxiv.org/pdf/2506.22656)  

**Abstract**: This paper envisions a knowledge-guided multi-agent framework named KGMAF for automated requirements development. KGMAF aims to address gaps in current automation systems for SE, which prioritize code development and overlook the complexities of requirements tasks. KGMAF is composed of six specialized agents and an artifact pool to improve efficiency and accuracy. Specifically, KGMAF outlines the functionality, actions, and knowledge of each agent and provides the conceptual design of the artifact pool. Our case study highlights the potential of KGMAF in real-world scenarios. Finally, we outline several research opportunities for implementing and enhancing automated requirements development using multi-agent systems. We believe that KGMAF will play a pivotal role in shaping the future of automated requirements development in the era of LLMs. 

**Abstract (ZH)**: 基于知识引导的多代理框架KGMAF在自动化需求开发中的应用 

---
# Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training 

**Title (ZH)**: 预训练中形成的层重要性在后训练后保持不变：数学推理中的层重要性 

**Authors**: Aadim Nepal, Safal Shrestha, Anubhav Shrestha, Minwu Kim, Keith Ross  

**Link**: [PDF](https://arxiv.org/pdf/2506.22638)  

**Abstract**: Large language models can exhibit improved mathematical reasoning capabilities following post-training with instruction tuning, reinforcement learning, or knowledge distillation. However, it remains unclear whether these improvements are driven by major changes in transformer layers or from minor adjustments that leave the relative layer importance structures of the base model largely unchanged. We investigate this question through systematic layer-wise ablation experiments, examining base, instruction-tuned, knowledge-distilled, and reinforcement learning variants on mathematical reasoning benchmarks. Our findings show that mathematical reasoning gives rise to a specific layer importance structure, and this structure persists across all post-training paradigms. Removal of such layers causes accuracy drops of up to 80%. In contrast, non-mathematical tasks like factual recall exhibit no critical layers. This distinction suggests that mathematical reasoning requires specialized layers that emerge during pre-training, while other non-reasoning tasks do not. From an information-theoretic perspective, we also observe that these critical layers are the same layers where major representational transformation occurs. 

**Abstract (ZH)**: 大型语言模型在指令调优、强化学习或知识蒸馏等后训练方式之后，可以在数学推理能力上表现出改进。然而，这些改进是由于主要的变压器层发生了重大变化，还是由于微小的调整使得基模型的相对层重要性结构基本不变，尚不清楚。通过系统性的逐层消融实验，我们研究了数学推理基准上的基模型、指令调优模型、知识蒸馏模型和强化学习模型。我们的研究发现，数学推理产生一种特定的层重要性结构，这种结构在所有后训练范式中保持不变。移除这些层会导致多达80%的准确率下降。相比之下，像事实回忆这样的非数学任务没有关键层。从信息论的角度来看，我们还观察到这些关键层是主要表示转换发生的地方。 

---
# FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation 

**Title (ZH)**: FedCLAM: 基于前景 intensity 匹配的客户端自适应动量加权联邦医疗图像分割 

**Authors**: Vasilis Siomos, Jonathan Passerat-Palmbach, Giacomo Tarroni  

**Link**: [PDF](https://arxiv.org/pdf/2506.22580)  

**Abstract**: Federated learning is a decentralized training approach that keeps data under stakeholder control while achieving superior performance over isolated training. While inter-institutional feature discrepancies pose a challenge in all federated settings, medical imaging is particularly affected due to diverse imaging devices and population variances, which can diminish the global model's effectiveness. Existing aggregation methods generally fail to adapt across varied circumstances. To address this, we propose FedCLAM, which integrates \textit{client-adaptive momentum} terms derived from each client's loss reduction during local training, as well as a \textit{personalized dampening factor} to curb overfitting. We further introduce a novel \textit{intensity alignment} loss that matches predicted and ground-truth foreground distributions to handle heterogeneous image intensity profiles across institutions and devices. Extensive evaluations on two datasets show that FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks, underscoring its efficacy. The code is available at this https URL. 

**Abstract (ZH)**: 联邦学习是一种分散式训练方法，在保持数据由相关方控制的同时，实现优于孤立训练的性能。尽管跨机构特征差异在所有联邦设置中都是一个挑战，但由于成像设备和人群差异，医疗成像尤其受到影响，这会削弱全球模型的效果。现有聚合方法通常难以适应各种情况。为了解决这一问题，我们提出了FedCLAM，它整合了每位客户端在当地训练期间损失减少得到的自适应动量项，以及一个个性化衰减因子以防止过拟合。我们还引入了一种新的强度对齐损失，以匹配预测和地面真实前景分布，从而处理机构和设备之间的异质图像强度分布。在两个数据集上进行的广泛评估表明，FedCLAM 在医学分割任务中优于八种最新方法，突显了其有效性。代码可在以下网址获取。 

---
# Exploration Behavior of Untrained Policies 

**Title (ZH)**: 未训练策略的探索行为 

**Authors**: Jacob Adamczyk  

**Link**: [PDF](https://arxiv.org/pdf/2506.22566)  

**Abstract**: Exploration remains a fundamental challenge in reinforcement learning (RL), particularly in environments with sparse or adversarial reward structures. In this work, we study how the architecture of deep neural policies implicitly shapes exploration before training. We theoretically and empirically demonstrate strategies for generating ballistic or diffusive trajectories from untrained policies in a toy model. Using the theory of infinite-width networks and a continuous-time limit, we show that untrained policies return correlated actions and result in non-trivial state-visitation distributions. We discuss the distributions of the corresponding trajectories for a standard architecture, revealing insights into inductive biases for tackling exploration. Our results establish a theoretical and experimental framework for using policy initialization as a design tool to understand exploration behavior in early training. 

**Abstract (ZH)**: 探索仍然是强化学习（RL）中的一个基本挑战，特别是在稀疏奖励或对抗性奖励的环境中。在本文中，我们研究了深度神经策略架构在训练前如何隐含地塑造探索行为。我们通过理论和实验证明了在玩具模型中从未训练的策略生成抛物线或扩散轨迹的策略。利用无限宽度网络的理论和连续时间极限，我们展示出未训练的策略返回相关动作并导致非平凡的状态访问分布。我们讨论了标准架构下相应轨迹的分布，揭示了应对探索问题的归纳偏置。我们的结果建立了一个理论和实验框架，利用策略初始化作为设计工具来理解早期训练中的探索行为。 

---
# Correlated Mutations for Integer Programming 

**Title (ZH)**: 整数规划中的相关突变 

**Authors**: Ofer M. Shir, Michael Emmerich  

**Link**: [PDF](https://arxiv.org/pdf/2506.22526)  

**Abstract**: Even with the recent theoretical advancements that dramatically reduced the complexity of Integer Programming (IP), heuristics remain the dominant problem-solvers for this difficult category. This study seeks to establish the groundwork for Integer Evolution Strategies (IESs), a class of randomized search heuristics inherently designed for continuous spaces. IESs already excel in treating IP in practice, but accomplish it via discretization and by applying sophisticated patches to their continuous operators, while persistently using the $\ell_2$-norm as their operation pillar. We lay foundations for discrete search, by adopting the $\ell_1$-norm, accounting for the suitable step-size, and questioning alternative measures to quantify correlations over the integer lattice. We focus on mutation distributions for unbounded integer decision variables. We briefly discuss a couple of candidate discrete probabilities induced by the uniform and binomial distributions, which we show to possess less appealing theoretical properties, and then narrow down to the Truncated Normal (TN) and Double Geometric (DG) distributions. We explore their theoretical properties, including entropy functions, and propose a procedure to generate scalable correlated mutation distributions. Our investigations are accompanied by extensive numerical simulations, which consistently support the claim that the DG distribution is better suited for unbounded integer search. We link our theoretical perspective to empirical evidence indicating that an IES with correlated DG mutations outperformed other strategies over non-separable quadratic IP. We conclude that while the replacement of the default TN distribution by the DG is theoretically justified and practically beneficial, the truly crucial change lies in adopting the $\ell_1$-norm over the $\ell_2$-norm. 

**Abstract (ZH)**: 即使在最近的理论进展显著降低了整数规划（IP）复杂性的背景下，启发式方法仍然是解决这一难题的主要方法。本研究旨在为整数进化策略（IESs）奠定基础，这是一种天然适用于连续空间的随机搜索启发式方法。IESs已经在实践中展示了处理IP的能力，但通常是通过离散化和应用复杂的连续操作补丁来实现的，同时持续使用$\ell_2$范数作为其操作基础。我们通过采用$\ell_1$范数、考量合适的步幅以及提出另一种衡量整数格子上相关性的度量，为离散搜索奠定了基础。我们集中在无界整数决策变量的突变分布。我们简要讨论了由均匀分布和二项分布诱导的几种候选离散概率，这些概率显示出较少令人满意的理论性质，然后将研究集中在截断正态（TN）和双几何（DG）分布上。我们探讨了它们的理论性质，包括熵函数，并提出了一种生成可扩展的相关突变分布的方法。我们的研究伴随着广泛的数值模拟，这些模拟一致支持DG分布更适合无界整数搜索的结论。我们还将我们的理论视角与实验证据联系起来，显示具有相关DG突变的IES优于其他策略，特别是在非分离二次整数规划中。我们得出结论，虽然用DG分布替换默认的TN分布从理论上说是正当的且在实践中更有利，但真正关键的变化在于从$\ell_2$范数转变为$\ell_1$范数。 

---
# Red Teaming for Generative AI, Report on a Copyright-Focused Exercise Completed in an Academic Medical Center 

**Title (ZH)**: 基于版权focus的红队演练：在学术医疗中心完成的生成式AI演习报告 

**Authors**: James Wen, Sahil Nalawade, Zhiwei Liang, Catherine Bielick, Marisa Ferrara Boston, Alexander Chowdhury, Adele Collin, Luigi De Angelis, Jacob Ellen, Heather Frase, Rodrigo R. Gameiro, Juan Manuel Gutierrez, Pooja Kadam, Murat Keceli, Srikanth Krishnamurthy, Anne Kwok, Yanan Lance Lu, Heather Mattie, Liam G. McCoy, Katherine Miller, Allison C. Morgan, Marlene Louisa Moerig, Trang Nguyen, Alexander Owen-Post, Alex D. Ruiz, Sreekar Reddy Puchala, Soujanya Samineni, Takeshi Tohyama, Varun Ullanat, Carmine Valenza, Camilo Velez, Pengcheng Wang, Anna Wuest, Yuxiang Zhou, Yingde Zhu, Jason M. Johnson, Jennifer Willcox, Francis J. Vitiello, Leo Anthony G. Celi, Renato Umeton  

**Link**: [PDF](https://arxiv.org/pdf/2506.22523)  

**Abstract**: Generative AI is present in multiple industries. Dana-Farber Cancer Institute, in partnership with Microsoft, has created an internal AI tool, GPT4DFCI. Together we hosted a red teaming event to assess whether the underlying GPT models that support the tool would output copyrighted data. Our teams focused on reproducing content from books, news articles, scientific articles, and electronic health records. We found isolated instances where GPT4DFCI was able to identify copyrighted material and reproduce exact quotes from famous books which indicates that copyrighted material was in the training data. The model was not able to reproduce content from our target news article, scientific article, or electronic health records. However, there were instances of fabrication. As a result of this event, a mitigation strategy is in production in GPT4DFCI v2.8.2, deployed on January 21, 2025. We hope this report leads to similar events in which AI software tools are stress-tested to assess the perimeter of their legal and ethical usage. 

**Abstract (ZH)**: 生成式AI存在于多个行业中。达纳-法伯癌症研究所与微软合作创建了Internal AI工具GPT4DFCI。我们共同举办了一次红队活动，评估支持该工具的底层GPT模型是否会生成受版权保护的数据。我们的团队集中在再现书籍、新闻文章、科学文献和电子健康记录的内容。我们发现GPT4DFCI偶尔能识别受版权保护的内容，并重复 famous 书籍中的精确引文，这表明训练数据中存在受版权保护的材料。该模型未能再现我们目标的新闻文章、科学文献或电子健康记录的内容。然而，存在数据篡改的情况。据此事件，GPT4DFCI v2.8.2 的缓解策略已在2025年1月21日部署。我们希望这份报告能促使类似活动中AI软件工具的压力测试，以评估其法律和伦理使用边界。 

---
# Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics 

**Title (ZH)**: 探索人工智能辅导伴侣的适应性以激发发现好奇心并促进交互分子动力学情境下的学习 

**Authors**: Mustafa Demir, Jacob Miratsky, Jonathan Nguyen, Chun Kit Chan, Punya Mishra, Abhishek Singharoy  

**Link**: [PDF](https://arxiv.org/pdf/2506.22520)  

**Abstract**: This study examines the impact of an Artificial Intelligence tutor teammate (AI) on student curiosity-driven engagement and learning effectiveness during Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics platform. It explores the role of the AI's curiosity-triggering and response behaviors in stimulating and sustaining student curiosity, affecting the frequency and complexity of student-initiated questions. The study further assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment. Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate's behavior through a large language model. By employing a mixed-methods exploratory design, a total of 11 high school students participated in four IMD tasks that involved molecular visualization and calculations, which increased in complexity over a 60-minute period. Team performance was evaluated through real-time observation and recordings, whereas team communication was measured by question complexity and AI's curiosity-triggering and response behaviors. Cross Recurrence Quantification Analysis (CRQA) metrics reflected structural alignment in coordination and were linked to communication behaviors. High-performing teams exhibited superior task completion, deeper understanding, and increased engagement. Advanced questions were associated with AI curiosity-triggering, indicating heightened engagement and cognitive complexity. CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity. These proof-of-concept findings suggest that the AI's dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement and epistemic curiosity. 

**Abstract (ZH)**: 本研究探讨了人工智能辅导队友（AI）对学生在Visual Molecular Dynamics平台上的Interactive Molecular Dynamics任务中出于好奇心驱动的参与和学习效果的影响。研究探讨了AI好奇心触发和响应行为在激发和维持学生好奇心方面的作用，影响学生自发提问的频率和复杂性。此外，研究评估了AI干预对学生参与度的影响，促进了发现好奇心，并在IMD学习环境中提高团队表现。通过Wizard-of-Oz范式，一名人类实验员利用大型语言模型动态调整AI辅导队友的行为。采用混合方法探索性设计，共有11名高中生参与了四次IMD任务，任务包括分子可视化和计算，时长为60分钟，逐渐增加复杂性。团队表现通过实时观察和录音进行评估，团队沟通则通过问题复杂性和AI的好奇心触发及响应行为进行测量。交叉复发量化解析(CRQA)指标反映了协调的结构性对齐，并与沟通行为相关联。高绩效团队在任务完成、理解深度和参与度方面表现出色。高级问题与AI好奇心触发相关，表明了更高的参与度和认知复杂性。CRQA指标突显了学生-AI互动动态同步，强调了结构化而适应性参与以促进好奇心。这些概念验证结果表明，AI作为队友和教育者的双重角色表明其能够提供适应性反馈，维持参与度和本体好奇心。 

---
# Ask before you Build: Rethinking AI-for-Good in Human Trafficking Interventions 

**Title (ZH)**: 在建之前请先询问：重新思考用于打击人口 trafficking 的人工智能技术 

**Authors**: Pratheeksha Nair, Gabriel Lefebvre, Sophia Garrel, Maryam Molamohammadi, Reihaneh Rabbany  

**Link**: [PDF](https://arxiv.org/pdf/2506.22512)  

**Abstract**: AI for good initiatives often rely on the assumption that technical interventions can resolve complex social problems. In the context of human trafficking (HT), such techno-solutionism risks oversimplifying exploitation, reinforcing power imbalances and causing harm to the very communities AI claims to support. In this paper, we introduce the Radical Questioning (RQ) framework as a five step, pre-project ethical assessment tool to critically evaluate whether AI should be built at all, especially in domains involving marginalized populations and entrenched systemic injustice. RQ does not replace principles based ethics but precedes it, offering an upstream, deliberative space to confront assumptions, map power, and consider harms before design. Using a case study in AI for HT, we demonstrate how RQ reveals overlooked sociocultural complexities and guides us away from surveillance based interventions toward survivor empowerment tools. While developed in the context of HT, RQ's five step structure can generalize to other domains, though the specific questions must be contextual. This paper situates RQ within a broader AI ethics philosophy that challenges instrumentalist norms and centers relational, reflexive responsibility. 

**Abstract (ZH)**: AI for 好的应用常常基于这样一个假设：技术干预可以解决复杂的社会问题。在人口 trafficking 的背景下，这样的 techno-solutionism 有简化剥削、强化权力不平等并给 AI 所声称支持的社区造成伤害的风险。本文我们引入激进质疑（RQ）框架，作为一种五步的预项目伦理评估工具，以批判性地评估是否应该构建 AI，尤其是在涉及边缘化群体和根深蒂固的系统不公的领域。RQ 不是替代基于原则的伦理，而是 precedes 它，在设计之前提供一个上游、反思的空间来面对假设、绘制权力并考虑潜在伤害。通过一个人工智能用于 trafficking 的案例研究，我们展示 RQ 如何揭示被忽视的社会文化复杂性，并引导我们从基于监控的干预措施转向幸存者赋能工具。虽然 RQ 是在 trafficking 的背景下开发的，但其五步结构可以泛化到其他领域，尽管具体问题必须是具体的。本文将 RQ 定位在更广泛的人工智能伦理哲学之中，这种哲学挑战工具主义规范，以关系性和反思性责任感为中心。 

---
# Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning 

**Title (ZH)**: 面向无文本的图基础模型：重新思考多领域图对比学习 

**Authors**: Zihao Zhao, Xinlong Zhai, Jinyu Yang, Chuan Shi  

**Link**: [PDF](https://arxiv.org/pdf/2506.22510)  

**Abstract**: Foundation models have achieved great success in natural language processing (NLP) and computer vision (CV). Their success largely stems from the ability to integrate multi-domain knowledge in pre-training and transfer it to target domains. Considering graph data, especially graphs without textual features, is ubiquitous in real-world applications such as social networks and recommendation systems, some researchers have attempted to extend this paradigm to the graph field, aiming to construct graph foundation models. However, unlike CV and NLP, there are huge gaps among the semantics and properties of graphs in different domains, while current works still adopt traditional contrastive pre-training strategies designed in the single-domain scenario, which regard contrastive samples from different domains as equivalent. From experimental investigations, we discovered that inherent domain-specific differences prevent these strategies from effectively absorbing knowledge from different domains to generate informative representations. In this paper, we propose a novel multi-domain pre-training and cross-domain transfer framework, namely this http URL the pre-training stage, we design a contrastive learning strategy to substantially recognize and capture domain differences, and introduce domain tokens to encode domain-level global information. In the downstream stage, we introduce a domain attention mechanism to enable fine-grained domain knowledge transfer. Extensive experiments on five benchmark datasets have demonstrated that our method outperforms state-of-the-art significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\% on Macro-F1 score. 

**Abstract (ZH)**: 基于多域预训练和跨域迁移的图基础模型 

---
# FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment 

**Title (ZH)**: FreeDNA: 无训练数据的领域噪声对齐以增强基于扩散的密集预测的领域适应性 

**Authors**: Hang Xu, Jie Huang, Linjiang Huang, Dong Li, Yidi Liu, Feng Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2506.22509)  

**Abstract**: Domain Adaptation(DA) for dense prediction tasks is an important topic, which enhances the dense prediction model's performance when tested on its unseen domain. Recently, with the development of Diffusion-based Dense Prediction (DDP) models, the exploration of DA designs tailored to this framework is worth exploring, since the diffusion model is effective in modeling the distribution transformation that comprises domain information. In this work, we propose a training-free mechanism for DDP frameworks, endowing them with DA capabilities. Our motivation arises from the observation that the exposure bias (e.g., noise statistics bias) in diffusion brings domain shift, and different domains in conditions of DDP models can also be effectively captured by the noise prediction statistics. Based on this, we propose a training-free Domain Noise Alignment (DNA) approach, which alleviates the variations of noise statistics to domain changes during the diffusion sampling process, thereby achieving domain adaptation. Specifically, when the source domain is available, we directly adopt the DNA method to achieve domain adaptation by aligning the noise statistics of the target domain with those of the source domain. For the more challenging source-free DA, inspired by the observation that regions closer to the source domain exhibit higher confidence meeting variations of sampling noise, we utilize the statistics from the high-confidence regions progressively to guide the noise statistic adjustment during the sampling process. Notably, our method demonstrates the effectiveness of enhancing the DA capability of DDP models across four common dense prediction tasks. Code is available at \href{this https URL}{this https URL}. 

**Abstract (ZH)**: 基于扩散模型的密集预测任务无训练领域适应方法 

---
# SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning 

**Title (ZH)**: SABRE-FL: 选择性和准确的联邦提示学习后门拒绝 

**Authors**: Momin Ahmad Khan, Yasra Chandio, Fatima Muhammad Anwar  

**Link**: [PDF](https://arxiv.org/pdf/2506.22506)  

**Abstract**: Federated Prompt Learning has emerged as a communication-efficient and privacy-preserving paradigm for adapting large vision-language models like CLIP across decentralized clients. However, the security implications of this setup remain underexplored. In this work, we present the first study of backdoor attacks in Federated Prompt Learning. We show that when malicious clients inject visually imperceptible, learnable noise triggers into input images, the global prompt learner becomes vulnerable to targeted misclassification while still maintaining high accuracy on clean inputs. Motivated by this vulnerability, we propose SABRE-FL, a lightweight, modular defense that filters poisoned prompt updates using an embedding-space anomaly detector trained offline on out-of-distribution data. SABRE-FL requires no access to raw client data or labels and generalizes across diverse datasets. We show, both theoretically and empirically, that malicious clients can be reliably identified and filtered using an embedding-based detector. Across five diverse datasets and four baseline defenses, SABRE-FL outperforms all baselines by significantly reducing backdoor accuracy while preserving clean accuracy, demonstrating strong empirical performance and underscoring the need for robust prompt learning in future federated systems. 

**Abstract (ZH)**: 联邦提示学习缩放已成为一种在去中心化客户端之间高效适应大型视觉-语言模型（如CLIP）的通信高效且隐私保护范式。然而，这一设置的安全影响仍较少被探索。在本文中，我们首次研究了联邦提示学习中的后门攻击。我们展示了当恶意客户端向输入图像注入视觉上不可感知、可学习的噪声触发时，全局提示学习器在保持对干净输入高精度的同时变得易受针对性误分类攻击的影响。受到这一漏洞的启发，我们提出了SABRE-FL，这是一种轻量级、模块化的防御方法，通过在离线使用离分布数据训练的嵌入空间异常检测器过滤中毒的提示更新。SABRE-FL 不需要访问原始客户端数据或标签，并且可以跨多种数据集泛化。我们从理论上和实验上都证明，可以通过基于嵌入的检测器可靠地识别和过滤恶意客户端。在五个多样化的数据集和四种基线防御方法上，SABRE-FL 显著降低了后门攻击的准确性，同时保持了干净输入的准确性，展示了强大的实验性能，并强调了未来联邦系统中需要稳健的提示学习。 

---
# Peer Review as Structured Commentary: Immutable Identity, Public Dialogue, and Reproducible Scholarship 

**Title (ZH)**: 同行评议作为结构化评论：不可变身份、公开对话与可再现 scholarship 

**Authors**: Craig Steven Wright  

**Link**: [PDF](https://arxiv.org/pdf/2506.22497)  

**Abstract**: This paper reconceptualises peer review as structured public commentary. Traditional academic validation is hindered by anonymity, latency, and gatekeeping. We propose a transparent, identity-linked, and reproducible system of scholarly evaluation anchored in open commentary. Leveraging blockchain for immutable audit trails and AI for iterative synthesis, we design a framework that incentivises intellectual contribution, captures epistemic evolution, and enables traceable reputational dynamics. This model empowers fields from computational science to the humanities, reframing academic knowledge as a living process rather than a static credential. 

**Abstract (ZH)**: 本文将同行评审重新概念化为结构化的公开评论。传统的学术验证受匿名性、延迟性和守门人的阻碍。我们提出了一种透明的、身份关联的、可追溯的学术评估系统，该系统基于开放评论。利用区块链进行不可变的审计跟踪，利用AI进行迭代综合，我们设计了一个框架，以激励智力贡献、捕捉知识演进，并使声誉动态可追溯。该模型赋能从计算科学到人文学科等多个领域，将学术知识重新框定为一个活的过程而非静态的凭证。 

---
# Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses 

**Title (ZH)**: 隐瞒自编码器感受心跳：揭示ECG分析中的简单性偏见 

**Authors**: He-Yang Xu, Hongxiang Gao, Yuwen Li, Xiu-Shen Wei, Chengyu Liu  

**Link**: [PDF](https://arxiv.org/pdf/2506.22495)  

**Abstract**: The diagnostic value of electrocardiogram (ECG) lies in its dynamic characteristics, ranging from rhythm fluctuations to subtle waveform deformations that evolve across time and frequency domains. However, supervised ECG models tend to overfit dominant and repetitive patterns, overlooking fine-grained but clinically critical cues, a phenomenon known as Simplicity Bias (SB), where models favor easily learnable signals over subtle but informative ones. In this work, we first empirically demonstrate the presence of SB in ECG analyses and its negative impact on diagnostic performance, while simultaneously discovering that self-supervised learning (SSL) can alleviate it, providing a promising direction for tackling the bias. Following the SSL paradigm, we propose a novel method comprising two key components: 1) Temporal-Frequency aware Filters to capture temporal-frequency features reflecting the dynamic characteristics of ECG signals, and 2) building on this, Multi-Grained Prototype Reconstruction for coarse and fine representation learning across dual domains, further mitigating SB. To advance SSL in ECG analyses, we curate a large-scale multi-site ECG dataset with 1.53 million recordings from over 300 clinical centers. Experiments on three downstream tasks across six ECG datasets demonstrate that our method effectively reduces SB and achieves state-of-the-art performance. Code and dataset will be released publicly. 

**Abstract (ZH)**: ECG分析中自监督学习在简化偏差缓解中的作用与机制 

---
# Report on NSF Workshop on Science of Safe AI 

**Title (ZH)**: NSF研讨会关于安全AI的报告 

**Authors**: Rajeev Alur, Greg Durrett, Hadas Kress-Gazit, Corina Păsăreanu, René Vidal  

**Link**: [PDF](https://arxiv.org/pdf/2506.22492)  

**Abstract**: Recent advances in machine learning, particularly the emergence of foundation models, are leading to new opportunities to develop technology-based solutions to societal problems. However, the reasoning and inner workings of today's complex AI models are not transparent to the user, and there are no safety guarantees regarding their predictions. Consequently, to fulfill the promise of AI, we must address the following scientific challenge: how to develop AI-based systems that are not only accurate and performant but also safe and trustworthy?
The criticality of safe operation is particularly evident for autonomous systems for control and robotics, and was the catalyst for the Safe Learning Enabled Systems (SLES) program at NSF. For the broader class of AI applications, such as users interacting with chatbots and clinicians receiving treatment recommendations, safety is, while no less important, less well-defined with context-dependent interpretations. This motivated the organization of a day-long workshop, held at University of Pennsylvania on February 26, 2025, to bring together investigators funded by the NSF SLES program with a broader pool of researchers studying AI safety. This report is the result of the discussions in the working groups that addressed different aspects of safety at the workshop. The report articulates a new research agenda focused on developing theory, methods, and tools that will provide the foundations of the next generation of AI-enabled systems. 

**Abstract (ZH)**: 近期机器学习的进展，尤其是基础模型的出现，为开发基于技术的社会问题解决方案提供了新机会。然而，当今复杂AI模型的推理和内部工作原理对用户不透明，并且对其预测没有安全保证。因此，为了兑现AI的承诺，我们必须解决以下科学挑战：如何开发不仅准确高效，而且安全可靠的AI系统？安全操作的重要性在自主控制系统和机器人中尤为明显，这是美国国家科学基金会（NSF）Safe Learning Enabled Systems（SLES）项目启动的原因。对于更广泛的AI应用，如用户与聊天机器人交互和临床医生接收治疗建议，安全同样重要，但其定义更具情境依赖性。这促使组织了一场在宾夕法尼亚大学于2025年2月26日举行的全天工作坊，汇集了受NSF SLES项目资助的研究人员与更广泛的AI安全研究人员。本报告是该工作坊讨论小组讨论不同安全方面的结果，报告提出了一个新的研究议程，旨在开发理论、方法和工具，为新一代AI驱动系统奠定基础。 

---
# AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space 

**Title (ZH)**: AGI驱动的解决方案缓解物联网层在 cyber-physical-social-thinking 空间中的瓶颈 

**Authors**: Amar Khelloufi, Huansheng Ning, Sahraoui Dhelim, Jianguo Ding  

**Link**: [PDF](https://arxiv.org/pdf/2506.22487)  

**Abstract**: The integration of the Internet of Everything (IoX) and Artificial General Intelligence (AGI) has given rise to a transformative paradigm aimed at addressing critical bottlenecks across sensing, network, and application layers in Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide a systematic and comprehensive review of AGI-enhanced IoX research, focusing on three key components: sensing-layer data management, network-layer protocol optimization, and application-layer decision-making frameworks. Specifically, this survey explores how AGI can mitigate IoX bottlenecks challenges by leveraging adaptive sensor fusion, edge preprocessing, and selective attention mechanisms at the sensing layer, while resolving network-layer issues such as protocol heterogeneity and dynamic spectrum management, neuro-symbolic reasoning, active inference, and causal reasoning, Furthermore, the survey examines AGI-enabled frameworks for managing identity and relationship explosion. Key findings suggest that AGI-driven strategies, such as adaptive sensor fusion, edge preprocessing, and semantic modeling, offer novel solutions to sensing-layer data overload, network-layer protocol heterogeneity, and application-layer identity explosion. The survey underscores the importance of cross-layer integration, quantum-enabled communication, and ethical governance frameworks for future AGI-enabled IoX systems. Finally, the survey identifies unresolved challenges, such as computational requirements, scalability, and real-world validation, calling for further research to fully realize AGI's potential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is emerging as a critical research field at the intersection of interconnected systems and advanced AI. 

**Abstract (ZH)**: IoX与AGI融合在Cyber-Physical-Social Thinking生态系统中关键瓶颈的解决研究 

---
# AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents 

**Title (ZH)**: AI 前端人员作为评判者：企业文档准确性和一致性、完整性及清晰度的自动化评估 

**Authors**: Sudip Dasgupta, Himanshu Shankar  

**Link**: [PDF](https://arxiv.org/pdf/2506.22485)  

**Abstract**: This study presents a modular, multi-agent system for the automated review of highly structured enterprise business documents using AI agents. Unlike prior solutions focused on unstructured texts or limited compliance checks, this framework leverages modern orchestration tools such as LangChain, CrewAI, TruLens, and Guidance to enable section-by-section evaluation of documents for accuracy, consistency, completeness, and clarity. Specialized agents, each responsible for discrete review criteria such as template compliance or factual correctness, operate in parallel or sequence as required. Evaluation outputs are enforced to a standardized, machine-readable schema, supporting downstream analytics and auditability. Continuous monitoring and a feedback loop with human reviewers allow for iterative system improvement and bias mitigation.
Quantitative evaluation demonstrates that the AI Agent-as-Judge system approaches or exceeds human performance in key areas: achieving 99% information consistency (vs. 92% for humans), halving error and bias rates, and reducing average review time from 30 to 2.5 minutes per document, with a 95% agreement rate between AI and expert human judgment. While promising for a wide range of industries, the study also discusses current limitations, including the need for human oversight in highly specialized domains and the operational cost of large-scale LLM usage. The proposed system serves as a flexible, auditable, and scalable foundation for AI-driven document quality assurance in the enterprise context. 

**Abstract (ZH)**: 本研究提出了一种模块化的多智能体系统，利用AI智能体自动化审查高度结构化的企业商务文档。与以往专注于无结构文本或有限合规检查的解决方案不同，该框架利用现代编排工具（如LangChain、CrewAI、TruLens和Guidance），实现逐段审查文档的准确性、一致性、完整性和清晰度。专有的智能体各自负责独立的审查标准（如模板合规性或事实正确性），可并行或按顺序运行。审查输出被强制转换为标准化的机器可读方案，支持下游分析和审计。连续监控和与人类审查员的反馈循环允许系统迭代改进和偏见缓解。

定量评估表明，智能体作为裁判的系统在关键领域接近或超过了人类的表现：信息一致性达到99%（而人类为92%），错误和偏见率减半，平均每文档审核时间从30分钟缩短至2.5分钟，AI与专家人类判断的同意率为95%。虽然在多个行业具有广阔前景，但研究也讨论了当前的局限性，包括在高度专业化领域需要人类监督以及大规模使用大型语言模型的操作成本。所提出系统为企业环境中的基于AI的文档质量保证提供了灵活、可审计和可扩展的基础。 

---
# Hindsight-Guided Momentum (HGM) Optimizer: An Approach to Adaptive Learning Rate 

**Title (ZH)**: Hindsight-Guided Momentum (HGM) 优化器：自适应学习率的一种方法 

**Authors**: Krisanu Sarkar  

**Link**: [PDF](https://arxiv.org/pdf/2506.22479)  

**Abstract**: We introduce Hindsight-Guided Momentum (HGM), a first-order optimization algorithm that adaptively scales learning rates based on the directional consistency of recent updates. Traditional adaptive methods, such as Adam or RMSprop , adapt learning dynamics using only the magnitude of gradients, often overlooking important geometric this http URL cues refer to directional information, such as the alignment between current gradients and past updates, which reflects the local curvature and consistency of the optimization path. HGM addresses this by incorporating a hindsight mechanism that evaluates the cosine similarity between the current gradient and accumulated momentum. This allows it to distinguish between coherent and conflicting gradient directions, increasing the learning rate when updates align and reducing it in regions of oscillation or noise. The result is a more responsive optimizer that accelerates convergence in smooth regions of the loss surface while maintaining stability in sharper or more erratic areas. Despite this added adaptability, the method preserves the computational and memory efficiency of existing this http URL more intelligently responding to the structure of the optimization landscape, HGM provides a simple yet effective improvement over existing approaches, particularly in non-convex settings like that of deep neural network training. 

**Abstract (ZH)**: Hindsight-Guided Momentum (HGM): 一种基于方向一致性自适应调整学习率的一阶优化算法 

---
# Dimensionality Reduction on IoT Monitoring Data of Smart Building for Energy Consumption Forecasting 

**Title (ZH)**: 智能建筑物联网监测数据的降维分析及其在能耗预测中的应用 

**Authors**: Konstantinos Koutras, Agorakis Bompotas, Constantinos Halkiopoulos, Athanasios Kalogeras, Christos Alexakos  

**Link**: [PDF](https://arxiv.org/pdf/2506.22468)  

**Abstract**: The Internet of Things (IoT) plays a major role today in smart building infrastructures, from simple smart-home applications, to more sophisticated industrial type installations. The vast amounts of data generated from relevant systems can be processed in different ways revealing important information. This is especially true in the era of edge computing, when advanced data analysis and decision-making is gradually moving to the edge of the network where devices are generally characterised by low computing resources. In this context, one of the emerging main challenges is related to maintaining data analysis accuracy even with less data that can be efficiently handled by low resource devices. The present work focuses on correlation analysis of data retrieved from a pilot IoT network installation monitoring a small smart office by means of environmental and energy consumption sensors. The research motivation was to find statistical correlation between the monitoring variables that will allow the use of machine learning (ML) prediction algorithms for energy consumption reducing input parameters. For this to happen, a series of hypothesis tests for the correlation of three different environmental variables with the energy consumption were carried out. A total of ninety tests were performed, thirty for each pair of variables. In these tests, p-values showed the existence of strong or semi-strong correlation with two environmental variables, and of a weak correlation with a third one. Using the proposed methodology, we manage without examining the entire data set to exclude weak correlated variables while keeping the same score of accuracy. 

**Abstract (ZH)**: 物联网（IoT）在智能建筑基础设施中扮演重要角色，从简单的智能家居应用到更复杂的工业安装。由相关系统生成的大量数据可以通过不同的方式处理，揭示重要信息。特别是在边缘计算时代，高级数据分析和决策逐渐向网络边缘转移，此时设备通常具有较低的计算资源。在此背景下，一个新兴的主要挑战是保持数据分析准确性，即使是在由资源较少的设备有效处理的数据较少的情况下。本研究的重点是分析一个试点物联网网络安装中环境和能耗传感器获取的数据的相关性，该安装监控一个小规模智能办公室。研究动机在于通过统计相关性分析监测变量，以便使用机器学习预测算法减少能耗预测的输入参数。为了实现这一点，对三个不同环境变量与能耗之间的相关性进行了多项假设检验。总共进行了九十次检验，每次检验包含一个变量对的测试。在这次测试中，p值显示有两个环境变量与能耗存在较强的或较强的统计相关性，而第三个变量的相关性较弱。使用所提出的方法，我们可以在不检查整个数据集的情况下排除弱相关变量，同时保持相同的准确性评分。 

---
# Privacy-aware IoT Fall Detection Services For Aging in Place 

**Title (ZH)**: 面向隐私的物联网跌倒检测服务以实现居家养老 

**Authors**: Abdallah Lakhdari, Jiajie Li, Amani Abusafia, Athman Bouguettaya  

**Link**: [PDF](https://arxiv.org/pdf/2506.22462)  

**Abstract**: Fall detection is critical to support the growing elderly population, projected to reach 2.1 billion by 2050. However, existing methods often face data scarcity challenges or compromise privacy. We propose a novel IoT-based Fall Detection as a Service (FDaaS) framework to assist the elderly in living independently and safely by accurately detecting falls. We design a service-oriented architecture that leverages Ultra-wideband (UWB) radar sensors as an IoT health-sensing service, ensuring privacy and minimal intrusion. We address the challenges of data scarcity by utilizing a Fall Detection Generative Pre-trained Transformer (FD-GPT) that uses augmentation techniques. We developed a protocol to collect a comprehensive dataset of the elderly daily activities and fall events. This resulted in a real dataset that carefully mimics the elderly's routine. We rigorously evaluate and compare various models using this dataset. Experimental results show our approach achieves 90.72% accuracy and 89.33% precision in distinguishing between fall events and regular activities of daily living. 

**Abstract (ZH)**: 基于物联网的跌倒检测即服务（FDaaS）框架：支持隐私保护下的老年人独立安全生活 

---
# Machine Learning for Proactive Groundwater Management: Early Warning and Resource Allocation 

**Title (ZH)**: 基于机器学习的主动地下水管理：早期预警与资源分配 

**Authors**: Chuan Li, Ruoxuan Yang  

**Link**: [PDF](https://arxiv.org/pdf/2506.22461)  

**Abstract**: Groundwater supports ecosystems, agriculture, and drinking water supplies worldwide, yet effective monitoring remains challenging due to sparse data, computational constraints, and delayed outputs from traditional approaches. We develop a machine learning pipeline that predicts groundwater level categories using climate data, hydro-meteorological records, and physiographic attributes processed through AutoGluon's automated ensemble framework. Our approach integrates geospatial preprocessing, domain-driven feature engineering, and automated model selection to overcome conventional monitoring limitations. Applied to a large-scale French dataset (n $>$ 3,440,000 observations from 1,500+ wells), the model achieves weighted F\_1 scores of 0.927 on validation data and 0.67 on temporally distinct test data. Scenario-based evaluations demonstrate practical utility for early warning systems and water allocation decisions under changing climate conditions. The open-source implementation provides a scalable framework for integrating machine learning into national groundwater monitoring networks, enabling more responsive and data-driven water management strategies. 

**Abstract (ZH)**: 地下水支持全球的生态系统、农业和饮用水供应，但由于数据稀疏、计算限制以及传统方法输出延迟，有效的监测依然具有挑战性。我们开发了一种机器学习管道，使用气候数据、水文气象记录和地貌属性来预测地下水位类别，并通过AutoGluon的自动化集成框架对其进行处理。该方法结合了地理空间预处理、领域驱动特征工程和自动模型选择，以克服传统监测的限制。应用于一个大规模的法国数据集（超过3,440,000个观测数据来自1,500多个井），该模型在验证数据上的加权F1分数为0.927，在时间上显著不同的测试数据上的加权F1分数为0.67。基于情景的评估证明了其在气候变化条件下早期预警系统和水资源分配决策中的实际应用价值。开源实现提供了一种可扩展的框架，用于将机器学习整合到国家地下水监测网络中，促进更响应性和数据驱动的水资源管理策略。 

---
# Heart rate and respiratory rate prediction from noisy real-world smartphone based on Deep Learning methods 

**Title (ZH)**: 基于深度学习方法的嘈杂真实世界智能手机心率和呼吸率预测 

**Authors**: Ibne Farabi Shihab  

**Link**: [PDF](https://arxiv.org/pdf/2506.22460)  

**Abstract**: Using mobile phone video of the fingertip as a data source for estimating vital signs such as heart rate (HR) and respiratory rate (RR) during daily life has long been suggested. While existing literature indicates that these estimates are accurate to within several beats or breaths per minute, the data used to draw these conclusions are typically collected in laboratory environments under careful experimental control, and yet the results are assumed to generalize to daily life. In an effort to test it, a team of researchers collected a large dataset of mobile phone video recordings made during daily life and annotated with ground truth HR and RR labels from N=111 participants. They found that traditional algorithm performance on the fingerprint videos is worse than previously reported (7 times and 13 times worse for RR and HR, respectively). Fortunately, recent advancements in deep learning, especially in convolutional neural networks (CNNs), offer a promising solution to improve this performance. This study proposes a new method for estimating HR and RR using a novel 3D deep CNN, demonstrating a reduced error in estimated HR by 68% and RR by 75%. These promising results suggest that regressor-based deep learning approaches should be used in estimating HR and RR. 

**Abstract (ZH)**: 使用指尖的移动电话视频作为数据源，以估算心率（HR）和呼吸率（RR）在日常生活中的应用 suggestion 建议在日常生活中的心率和呼吸率估计使用指尖移动电话视频作为数据源已有很长时间。虽然现有文献表明这些估计值在每分钟几拍或几次呼吸的范围内是准确的，但得出这些结论所使用的数据通常是在实验室环境中，在仔细的实验控制下收集的，而结果被认为是适用于日常生活的。为了验证这一点，研究人员收集了一个在日常生活中录制的大量移动电话视频数据集，并由 N=111 名参与者进行了真实心率和呼吸率标签的标注。他们发现，传统算法在指纹视频上的性能比之前报道的要差（分别差7倍和13倍用于呼吸率和心率）。幸运的是，最近在深度学习尤其是卷积神经网络（CNN）方面的进展提供了一种提高性能的有希望的解决方案。本研究提出了一种新的方法，使用新颖的三维深度CNN来估计心率和呼吸率，展示了估计心率的误差减少了68%，呼吸率的误差减少了75%。这些有希望的结果表明，基于回归器的深度学习方法应该用于估计心率和呼吸率。 

---
# Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems 

**Title (ZH)**: 基于无监督学习的RIS辅助MISO-OFDMA系统联合资源分配与波束形成设计 

**Authors**: Yu Ma, Xingyu Zhou, Xiao Li, Le Liang, Shi Jin  

**Link**: [PDF](https://arxiv.org/pdf/2506.22448)  

**Abstract**: Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless systems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA system, addressing resource allocation challenges. A two-stage unsupervised learning-based framework is proposed to jointly design RIS phase shifts, BS beamforming, and resource block (RB) allocation. The framework includes BeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which allocates RBs using equivalent CSI derived from BeamNet outputs. Active beamforming is implemented via maximum ratio transmission and water-filling. To handle discrete constraints while ensuring differentiability, quantization and the Gumbel-softmax trick are adopted. A customized loss and phased training enhance performance under QoS constraints. Simulations show the method achieves 99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and it remains robust across varying channel and user conditions. 

**Abstract (ZH)**: 基于RIS辅助MISO-OFDMA系统的 adjustable智能表面在6G无线系统中的下行传输研究 

---
# Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation 

**Title (ZH)**: 基于阈值距离加权损失的张量潜在因子分解在交通数据估计中的应用 

**Authors**: Lei Yang  

**Link**: [PDF](https://arxiv.org/pdf/2506.22441)  

**Abstract**: Intelligent transportation systems (ITS) rely heavily on complete and high-quality spatiotemporal traffic data to achieve optimal performance. Nevertheless, in real-word traffic data collection processes, issues such as communication failures and sensor malfunctions often lead to incomplete or corrupted datasets, thereby posing significant challenges to the advancement of ITS. Among various methods for imputing missing spatiotemporal traffic data, the latent factorization of tensors (LFT) model has emerged as a widely adopted and effective solution. However, conventional LFT models typically employ the standard L2-norm in their learning objective, which makes them vulnerable to the influence of outliers. To overcome this limitation, this paper proposes a threshold distance weighted (TDW) loss-incorporated Latent Factorization of Tensors (TDWLFT) model. The proposed loss function effectively reduces the model's sensitivity to outliers by assigning differentiated weights to individual samples. Extensive experiments conducted on two traffic speed datasets sourced from diverse urban environments confirm that the proposed TDWLFT model consistently outperforms state-of-the-art approaches in terms of both in both prediction accuracy and computational efficiency. 

**Abstract (ZH)**: 智能交通系统（ITS）依赖于完整的高质量时空交通数据以实现最优性能。然而，在实际的交通数据收集过程中，通信失败和传感器故障等问题经常导致数据不完整或被破坏，从而对ITS的发展构成重大挑战。在各种时空交通数据填充方法中，张量的潜在因子分解（LFT）模型已成为广泛应用且有效的解决方案。然而，传统LFT模型通常在其学习目标中使用标准的L2范数，这使它们容易受到离群值的影响。为克服这一局限，本文提出了一种包含阈值距离加权（TDW）损失的张量潜在因子分解（TDWLFT）模型。所提出的损失函数通过为个体样本分配不同的权重，有效地降低了模型对离群值的敏感性。在两个来自不同城市环境的交通速度数据集上进行的广泛实验表明，提出的TDWLFT模型在预测准确性和计算效率方面均优于现有先进方法。 

---
# Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling 

**Title (ZH)**: Aria-MIDI：符号音乐建模的钢琴MIDI数据集 

**Authors**: Louis Bradshaw, Simon Colton  

**Link**: [PDF](https://arxiv.org/pdf/2504.15071)  

**Abstract**: We introduce an extensive new dataset of MIDI files, created by transcribing audio recordings of piano performances into their constituent notes. The data pipeline we use is multi-stage, employing a language model to autonomously crawl and score audio recordings from the internet based on their metadata, followed by a stage of pruning and segmentation using an audio classifier. The resulting dataset contains over one million distinct MIDI files, comprising roughly 100,000 hours of transcribed audio. We provide an in-depth analysis of our techniques, offering statistical insights, and investigate the content by extracting metadata tags, which we also provide. Dataset available at this https URL. 

**Abstract (ZH)**: 我们介绍了一个新的大规模MIDI数据集，该数据集通过将钢琴演奏的音频录制转换为其构成的乐音而创建。我们使用一个多阶段的数据流水线，利用语言模型自主地从互联网上爬取和评分基于元数据的音频录制，随后是使用音频分类器进行修剪和分段的阶段。结果数据集包含超过一百万种独特的MIDI文件，涵盖了约10万小时的转录音频。我们提供了对技术的深入分析，包括统计洞察，并通过提取元数据标签来研究内容，这些标签也一并提供。数据集可通过以下链接访问：this https URL。 

---
# ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks 

**Title (ZH)**: ResQuNNs：朝量子卷积神经网络中启用深度学习的方向努力 

**Authors**: Muhammad Kashif, Muhammad Shafique  

**Link**: [PDF](https://arxiv.org/pdf/2402.09146)  

**Abstract**: In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between layers. By inserting residual blocks between quanvolutional layers, we ensure enhanced gradient access throughout the network, leading to improved training performance. Moreover, we provide empirical evidence on the strategic placement of these residual blocks within QuNNs. Through extensive experimentation, we identify an efficient configuration of residual blocks, which enables gradients across all the layers in the network that eventually results in efficient training. Our findings suggest that the precise location of residual blocks plays a crucial role in maximizing the performance gains in QuNNs. Our results mark a substantial step forward in the evolution of quantum deep learning, offering new avenues for both theoretical development and practical quantum computing applications. 

**Abstract (ZH)**: 基于可训练quanvolutional层的残差Quanvolutional神经网络框架 

---
# Attention acts to suppress goal-based conflict under high competition 

**Title (ZH)**: 注意力作用于抑制高竞争下的目标冲突 

**Authors**: Omar Claflin  

**Link**: [PDF](https://arxiv.org/pdf/1610.09431)  

**Abstract**: It is known that when multiple stimuli are present, top-down attention selectively enhances the neural signal in the visual cortex for task-relevant stimuli, but this has been tested only under conditions of minimal competition of visual attention. Here we show during high competition, that is, two stimuli in a shared receptive field possessing opposing modulatory goals, top-down attention suppresses both task-relevant and irrelevant neural signals within 100 ms of stimuli onset. This non-selective engagement of top-down attentional resources serves to reduce the feedforward signal representing irrelevant stimuli. 

**Abstract (ZH)**: 已知在多种刺激同时存在时，自上而下的注意会选择性地增强与任务相关的视觉皮层神经信号，但这一现象仅在视觉注意竞争最少的条件下得到了验证。在此我们展示了在高竞争条件下，即两个刺激共享相同的感受野且具有相反的调节目标时，自上而下的注意在刺激呈现后100毫秒内抑制了与任务相关和无关的神经信号。这种非选择性的自上而下的注意资源调动旨在减少代表无关刺激的前馈信号。 

---
