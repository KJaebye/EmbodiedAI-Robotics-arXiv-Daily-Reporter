{'arxiv_id': 'arXiv:2507.07306', 'title': 'ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning', 'authors': 'Yichen Lu, Wei Dai, Jiaen Liu, Ching Wing Kwok, Zongheng Wu, Xudong Xiao, Ao Sun, Sheng Fu, Jianyuan Zhan, Yian Wang, Takatomo Saito, Sicheng Lai', 'link': 'https://arxiv.org/abs/2507.07306', 'abstract': 'LLM-based translation agents have achieved highly human-like translation results and are capable of handling longer and more complex contexts with greater efficiency. However, they are typically limited to text-only inputs. In this paper, we introduce ViDove, a translation agent system designed for multimodal input. Inspired by the workflow of human translators, ViDove leverages visual and contextual background information to enhance the translation process. Additionally, we integrate a multimodal memory system and long-short term memory modules enriched with domain-specific knowledge, enabling the agent to perform more accurately and adaptively in real-world scenarios. As a result, ViDove achieves significantly higher translation quality in both subtitle generation and general translation tasks, with a 28% improvement in BLEU scores and a 15% improvement in SubER compared to previous state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark for long-form automatic video subtitling and translation, featuring 17 hours of high-quality, human-annotated data. Our code is available here: this https URL', 'abstract_zh': '基于LLM的多模态翻译代理已经实现了高度人性化的翻译效果，并且能够更高效地处理更长和更复杂的上下文。然而，它们通常限于只处理文本输入。本文介绍了ViDove，一个设计用于处理多模态输入的翻译代理系统。受人类翻译工作流程的启发，ViDove利用视觉和上下文背景信息来增强翻译过程。此外，我们集成了一个多模态记忆系统和包含领域特定知识的长短时记忆模块，使代理能够在实际场景中更加准确和适应性地工作。因此，ViDove在字幕生成和通用翻译任务中的翻译质量显著提高，BLEU分数提高了28%，SubER提高了15%，优于以往的最先进的基线。此外，我们还引入了DoveBench，一个新的长视频字幕和自动翻译基准，包含17小时高质量的人工标注数据。我们的代码可在以下链接获取：this https URL。', 'title_zh': 'ViDove：一种基于多模态上下文和记忆增强推理的翻译代理系统'}
{'arxiv_id': 'arXiv:2507.07572', 'title': 'Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation', 'authors': 'Yupu Liang, Yaping Zhang, Zhiyang Zhang, Yang Zhao, Lu Xiang, Chengqing Zong, Yu Zhou', 'link': 'https://arxiv.org/abs/2507.07572', 'abstract': 'Document Image Machine Translation (DIMT) aims to translate text within document images, facing generalization challenges due to limited training data and the complex interplay between visual and textual information. To address these challenges, we introduce M4Doc, a novel single-to-mix modality alignment framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an image-only encoder with the multimodal representations of an MLLM, pre-trained on large-scale document image datasets. This alignment enables a lightweight DIMT model to learn crucial visual-textual correlations during training. During inference, M4Doc bypasses the MLLM, maintaining computational efficiency while benefiting from its multimodal knowledge. Comprehensive experiments demonstrate substantial improvements in translation quality, especially in cross-domain generalization and challenging document image scenarios.', 'abstract_zh': '文档图像机器翻译（DIMT）旨在翻译文档图像内的文本，由于训练数据有限和视觉信息与文本信息的复杂交互，面临着泛化能力的挑战。为应对这些挑战，我们提出了M4Doc，这是一种新型的单模态到多模态对齐框架，利用多模态大型语言模型（MLLMs）。M4Doc通过在大规模文档图像数据集上预训练的MLLM的多模态表示，对齐仅图像编码器。这种对齐在训练过程中使轻量级DIMT模型能够学习关键的视觉-文本相关性。在推理过程中，M4Doc跳过了MLLM，保持计算效率的同时受益于其多模态知识。全面的实验结果显示，在跨领域泛化和具有挑战性的文档图像场景中，翻译质量有了显著提升。', 'title_zh': '单模态到多模态对齐的多模态大型语言模型在文档图像机器翻译中的应用'}
{'arxiv_id': 'arXiv:2507.07151', 'title': 'Robust Multimodal Large Language Models Against Modality Conflict', 'authors': 'Zongmeng Zhang, Wengang Zhou, Jie Zhao, Houqiang Li', 'link': 'https://arxiv.org/abs/2507.07151', 'abstract': 'Despite the impressive capabilities of multimodal large language models (MLLMs) in vision-language tasks, they are prone to hallucinations in real-world scenarios. This paper investigates the hallucination phenomenon in MLLMs from the perspective of modality conflict. Unlike existing works focusing on the conflicts between model responses and inputs, we study the inherent conflicts in inputs from different modalities that place MLLMs in a dilemma and directly lead to hallucinations. We formally define the modality conflict and construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this phenomenon in vision-language tasks. Three methods based on prompt engineering, supervised fine-tuning, and reinforcement learning are proposed to alleviate the hallucination caused by modality conflict. Extensive experiments are conducted on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised fine-tuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that leads to hallucinations and provides more insights into the robustness of MLLMs.', 'abstract_zh': '尽管多模态大规模语言模型（MLLMs）在视觉语言任务中表现出色，但在现实场景中容易出现幻觉现象。本文从模态冲突的角度研究MLLMs的幻觉现象。不同于现有工作关注模型响应与输入之间的冲突，我们研究不同模态输入固有的冲突，这些冲突将MLLMs置于困境并直接导致幻觉现象。我们正式定义了模态冲突，并构建了一个名为Multimodal Modality Conflict (MMMC)的数据集，以在视觉语言任务中模拟这一现象。我们提出了基于提示工程、监督微调和强化学习的三种方法，以缓解由模态冲突引起的幻觉。我们在MMMC数据集上进行了大量实验，以分析这些方法的优缺点。结果显示，强化学习方法在缓解模态冲突引起的幻觉方面表现最佳，而监督微调方法表现出有希望且稳定的性能。我们的工作照亮了导致幻觉的未被注意的模态冲突，并为进一步理解MLLMs的健壮性提供了新的见解。', 'title_zh': '鲁棒多模态大型语言模型对抗模态冲突'}
