{'arxiv_id': 'arXiv:2510.26623', 'title': 'A Sliding-Window Filter for Online Continuous-Time Continuum Robot State Estimation', 'authors': 'Spencer Teetaert, Sven Lilge, Jessica Burgner-Kahrs, Timothy D. Barfoot', 'link': 'https://arxiv.org/abs/2510.26623', 'abstract': 'Stochastic state estimation methods for continuum robots (CRs) often struggle to balance accuracy and computational efficiency. While several recent works have explored sliding-window formulations for CRs, these methods are limited to simplified, discrete-time approximations and do not provide stochastic representations. In contrast, current stochastic filter methods must run at the speed of measurements, limiting their full potential. Recent works in continuous-time estimation techniques for CRs show a principled approach to addressing this runtime constraint, but are currently restricted to offline operation. In this work, we present a sliding-window filter (SWF) for continuous-time state estimation of CRs that improves upon the accuracy of a filter approach while enabling continuous-time methods to operate online, all while running at faster-than-real-time speeds. This represents the first stochastic SWF specifically designed for CRs, providing a promising direction for future research in this area.', 'abstract_zh': '连续执行机器人状态下 stochastic 状态估计方法 often struggle to balance accuracy and computational efficiency。', 'title_zh': '滑动窗口滤波器在在线连续时间连续体机器人状态估计中的应用'}
{'arxiv_id': 'arXiv:2510.26536', 'title': 'RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration', 'authors': 'Huajie Tan, Cheng Chi, Xiansheng Chen, Yuheng Ji, Zhongxia Zhao, Xiaoshuai Hao, Yaoxu Lyu, Mingyu Cao, Junkai Zhao, Huaihai Lyu, Enshen Zhou, Ning Chen, Yankai Fu, Cheng Peng, Wei Guo, Dong Liang, Zhuo Chen, Mengsi Lyu, Chenrui He, Yulong Ao, Yonghua Lin, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang', 'link': 'https://arxiv.org/abs/2510.26536', 'abstract': 'The proliferation of collaborative robots across diverse tasks and embodiments presents a central challenge: achieving lifelong adaptability, scalable coordination, and robust scheduling in multi-agent systems. Existing approaches, from vision-language-action (VLA) models to hierarchical frameworks, fall short due to their reliance on limited or dividual-agent memory. This fundamentally constrains their ability to learn over long horizons, scale to heterogeneous teams, or recover from failures, highlighting the need for a unified memory representation. To address these limitations, we introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable, and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene geometry, temporal event history, and embodiment profiles into a shared representation. This memory-centric design is integrated into a brain-cerebellum framework, where a high-level brain model performs global planning by retrieving and updating STEM, while low-level controllers execute actions locally. This closed loop between cognition, memory, and execution enables dynamic task allocation, fault-tolerant collaboration, and consistent state synchronization. We conduct extensive experiments spanning complex coordination tasks in restaurants, supermarkets, and households. Our results demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous embodiments, validating its effectiveness in enabling lifelong, scalable, and robust multi-robot collaboration. Project website: this https URL', 'abstract_zh': '协作机器人在多种任务和实体中的普及提出了一个核心挑战：在多智能体系统中实现终生适应性、可扩展协调和稳健调度。现有方法，从视觉-语言-动作（VLA）模型到分层框架，因依赖有限或个体智能体的记忆而表现出局限性。这从根本上限制了它们在长时段学习、扩展到异构团队或从失败中恢复的能力，突显了统一记忆表示的需要。为解决这些限制，我们引入了RoboOS-NeXT，这是一种基于统一记忆的框架，用于终生、可扩展和稳健的多机器人协作。RoboOS-NeXT的核心是一种名为时空体记忆（STEM）的新型框架，它将空间场景几何、时间事件历史和体貌特征整合到共享表示中。这种以记忆为中心的设计被整合到大脑-小脑框架中，在此框架中，高层次的大脑模型通过检索和更新STEM来进行全球规划，而低层次控制器则进行局部执行。这种认知、记忆和执行之间的闭环机制 enables 动态任务分配、容错协作和一致的状态同步。我们在饭店、超市和家庭等复杂协调任务中进行了广泛的实验。结果表明，RoboOS-NeXT在异构体态中实现了优越性能，验证了其在实现终生、可扩展和稳健的多机器人协作方面的有效性。项目网站：this https URL。', 'title_zh': 'RoboOS-NeXT：一种统一内存基础框架，用于 lifelong、可扩展和鲁棒的多机器人协作'}
{'arxiv_id': 'arXiv:2510.26080', 'title': "I don't Want You to Die: A Shared Responsibility Framework for Safeguarding Child-Robot Companionship", 'authors': 'Fan Yang, Renkai Ma, Yaxin Hu, Michael Rodgers, Lingyao Li', 'link': 'https://arxiv.org/abs/2510.26080', 'abstract': "Social robots like Moxie are designed to form strong emotional bonds with children, but their abrupt discontinuation can cause significant struggles and distress to children. When these services end, the resulting harm raises complex questions of who bears responsibility when children's emotional bonds are broken. Using the Moxie shutdown as a case study through a qualitative survey of 72 U.S. participants, our findings show that the responsibility is viewed as a shared duty across the robot company, parents, developers, and government. However, these attributions varied by political ideology and parental status of whether they have children. Participants' perceptions of whether the robot service should continue are highly polarized; supporters propose technical, financial, and governmental pathways for continuity, while opponents cite business realities and risks of unhealthy emotional dependency. Ultimately, this research contributes an empirically grounded shared responsibility framework for safeguarding child-robot companionship by detailing how accountability is distributed and contested, informing concrete design and policy implications to mitigate the emotional harm of robot discontinuation.", 'abstract_zh': '像Moxie这样的社交机器人被设计成与儿童形成强烈的情感联系，但服务的突然终止会导致儿童出现显著的困扰和困难。当这些服务结束时，由此产生的伤害引发了关于责任归属的复杂问题，特别是在儿童的情感联系被打破时。通过定性调查72名美国参与者的研究案例，我们的发现表明，责任被视为机器人公司、父母、开发者及政府的共同职责。然而，这些责任归属因参与者的政治立场和是否为人父母而异。参与者对机器人服务是否应该继续的看法极为分化；支持者提出技术、财务和政府途径来维持服务继续，而反对者则强调商业现实以及不健康情感依赖的风险。最终，这项研究通过详细阐述责任分配和争执，为确保儿童机器人伴侣的安全提供了一个实证基础的责任共担框架，并为减轻机器人终止的情感伤害提出现实的设计和政策建议。', 'title_zh': '我不希望你离开：保障儿童机器人陪伴安全的责任共担框架'}
{'arxiv_id': 'arXiv:2510.26571', 'title': 'Proxemics and Permeability of the Pedestrian Group', 'authors': 'Saleh Albeaik, Faisal Alsallum, Mohamad Alrished', 'link': 'https://arxiv.org/abs/2510.26571', 'abstract': 'People tend to walk in groups, and interactions with those groups have a significant impact on crowd behavior and pedestrian traffic dynamics. Social norms can be seen as unwritten rules regulating people interactions in social settings. This article studies people interactions with groups and the emergence of group proxemics. Group zones, zone occupancy counts and people clearance from the group are studied using naturalistic data. Analysis indicate potential presence of three different zones in addition to the public zone. People tend to remain in the public zone and only progressively get closer to groups, and those closer approaches happen in a low frequency and for brief periods of time.', 'abstract_zh': '人们倾向于结成小组行走，与这些小组的互动对 crowd 行为和行人交通动态有显著影响。社会规范可以被视为在社交场合调节人际互动的不成文规则。本文研究了人们与小组的互动以及群组空间距离的涌现。通过自然数据研究了群组区域、区域占用计数和人员从群组中退出的情况。分析表明除了公共区域外，可能存在三种不同的区域。人们倾向于留在公共区域，并仅逐步靠近小组，而这些靠近的行为频率低且持续时间短。', 'title_zh': '行人群体的proxemics与渗透性'}
{'arxiv_id': 'arXiv:2510.26369', 'title': 'CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse', 'authors': 'Kazuma Kano, Yuki Mori, Shin Katayama, Kenta Urano, Takuro Yonezawa, Nobuo Kawaguchi', 'link': 'https://arxiv.org/abs/2510.26369', 'abstract': "Worker location data is key to higher productivity in industrial sites. Cameras are a promising tool for localization in logistics warehouses since they also offer valuable environmental contexts such as package status. However, identifying individuals with only visual data is often impractical. Accordingly, several prior studies identified people in videos by comparing their trajectories and wearable sensor measurements. While this approach has advantages such as independence from appearance, the existing methods may break down under real-world conditions. To overcome this challenge, we propose CorVS, a novel data-driven person identification method based on correspondence between visual tracking trajectories and sensor measurements. Firstly, our deep learning model predicts correspondence probabilities and reliabilities for every pair of a trajectory and sensor measurements. Secondly, our algorithm matches the trajectories and sensor measurements over time using the predicted probabilities and reliabilities. We developed a dataset with actual warehouse operations and demonstrated the method's effectiveness for real-world applications.", 'abstract_zh': '工人位置数据是工业场所提高生产效率的关键。摄像头是物流仓库定位的一种有前景的工具，因为它们还提供了诸如包裹状态等有价值的环境背景。然而，仅凭视觉数据识别个体往往是不实际的。为此，一些先前的研究通过比较轨迹和穿戴传感器测量来识别视频中的人。尽管这种方法具有独立于外观的优点，但现有方法可能在真实世界条件下失效。为克服这一挑战，我们提出了一种名为CorVS的新型数据驱动的人体识别方法，该方法基于视觉跟踪轨迹和传感器测量之间的对应关系。首先，我们的深度学习模型预测每对轨迹和传感器测量之间的对应概率和可靠性。其次，我们的算法使用预测的概率和可靠性随时间匹配轨迹和传感器测量。我们开发了一个包含实际仓库操作的数据集，并证明了该方法在实际应用中的有效性。', 'title_zh': 'CorVS: 基于实物仓库中视频轨迹-传感器对应的人体识别'}
{'arxiv_id': 'arXiv:2510.26752', 'title': "The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy", 'authors': 'William Overman, Mohsen Bayati', 'link': 'https://arxiv.org/abs/2510.26752', 'abstract': "As increasingly capable agents are deployed, a central safety question is how to retain meaningful human control without modifying the underlying system. We study a minimal control interface where an agent chooses whether to act autonomously (play) or defer (ask), while a human simultaneously chooses whether to be permissive (trust) or to engage in oversight (oversee). If the agent defers, the human's choice determines the outcome, potentially leading to a corrective action or a system shutdown. We model this interaction as a two-player Markov Game. Our analysis focuses on cases where this game qualifies as a Markov Potential Game (MPG), a class of games where we can provide an alignment guarantee: under a structural assumption on the human's value function, any decision by the agent to act more autonomously that benefits itself cannot harm the human's value. We also analyze extensions to this MPG framework. Theoretically, this perspective provides conditions for a specific form of intrinsic alignment. If the reward structures of the human-agent game meet these conditions, we have a formal guarantee that the agent improving its own outcome will not harm the human's. Practically, this model motivates a transparent control layer with predictable incentives where the agent learns to defer when risky and act when safe, while its pretrained policy and the environment's reward structure remain untouched. Our gridworld simulation shows that through independent learning, the agent and human discover their optimal oversight roles. The agent learns to ask when uncertain and the human learns when to oversee, leading to an emergent collaboration that avoids safety violations introduced post-training. This demonstrates a practical method for making misaligned models safer after deployment.", 'abstract_zh': '随着日益 capable 的代理被部署，一个核心的安全问题是如何在不修改底层系统的情况下保留有意义的人类控制。我们研究了一个最小控制接口，在该接口中，代理选择是否自主行动（play）或推迟行动（ask），同时人类同时选择是否给予信任（trust）或进行监督（oversee）。如果代理推迟行动，人类的选择将决定结果，可能会导致纠正措施或系统关闭。我们将这种交互建模为一个两玩家马尔可夫博弈。我们的分析集中在当该游戏符合马尔可夫潜力博弈（MPG）类标准时的情况，在这种情况下，可以在人类价值函数的结构假设下提供对齐保证：任何有利于代理自身但不损害人类价值的自主行动决策都是安全的。我们还分析了MPG框架的扩展。理论上，这种视角提供了特定形式内在对齐的条件。如果人类-代理游戏的奖励结构满足这些条件，我们有正式保证，代理改善自身结果不会损害人类。实践上，这种模型激励了一个透明的控制层，具有可预测的激励机制，其中代理在冒险时学会推迟行动，在安全时学会自主行动，而代理的预训练策略和环境的奖励结构保持不变。我们的网格世界模拟显示，通过独立学习，代理和人类发现了它们的最优监督角色。代理在不确定时学会请求，人类在需要时学会监督，从而形成了一种新兴的合作模式，避免了训练后引入的安全违规行为。这证明了一种在部署后使未对齐模型更安全的实用方法。', 'title_zh': '监管博弈：学习协同平衡AI代理的安全与自主性'}
{'arxiv_id': 'arXiv:2510.26658', 'title': 'The Era of Agentic Organization: Learning to Organize with Language Models', 'authors': 'Zewen Chi, Li Dong, Qingxiu Dong, Yaru Hao, Xun Wu, Shaohan Huang, Furu Wei', 'link': 'https://arxiv.org/abs/2510.26658', 'abstract': 'We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training.', 'abstract_zh': '我们设想了一个新时代的AI，称为自主组织时代，在这个时代中，代理通过协作和并发解决问题，从而实现超越个体智能的结果。为了实现这一愿景，我们引入了异步思考（AsyncThink）作为一种新的大规模语言模型推理范式，将内部思考过程组织成为可并发执行的结构。具体而言，我们提出了一种思考协议，其中组织者动态分配子查询、合并中间知识并生成连贯的解决方案。更重要的是，该协议中的思考结构可以通过强化学习进一步优化。实验表明，与并行思考相比，AsyncThink 的推理延迟降低了 28%，并且在数学推理方面提高了准确性。此外，AsyncThink 能够泛化其学习到的异步思考能力，有效地处理未见过的任务而无需额外训练。', 'title_zh': '代_agents_组织：学会与语言模型组织'}
{'arxiv_id': 'arXiv:2510.26518', 'title': 'Human-AI Complementarity: A Goal for Amplified Oversight', 'authors': 'Rishub Jain, Sophie Bridgers, Lili Janzer, Rory Greig, Tian Huey Teh, Vladimir Mikulik', 'link': 'https://arxiv.org/abs/2510.26518', 'abstract': 'Human feedback is critical for aligning AI systems to human values. As AI capabilities improve and AI is used to tackle more challenging tasks, verifying quality and safety becomes increasingly challenging. This paper explores how we can leverage AI to improve the quality of human oversight. We focus on an important safety problem that is already challenging for humans: fact-verification of AI outputs. We find that combining AI ratings and human ratings based on AI rater confidence is better than relying on either alone. Giving humans an AI fact-verification assistant further improves their accuracy, but the type of assistance matters. Displaying AI explanation, confidence, and labels leads to over-reliance, but just showing search results and evidence fosters more appropriate trust. These results have implications for Amplified Oversight -- the challenge of combining humans and AI to supervise AI systems even as they surpass human expert performance.', 'abstract_zh': '人类反馈对于对齐AI系统与人类价值观至关重要。随着AI能力的提升和AI被用于应对更具挑战性的任务，验证质量和安全性变得越来越有挑战性。本文探讨了我们如何利用AI来提高人类监督的质量。我们专注于一个已经对人类构成挑战的重要安全问题：AI输出的事实核查。我们发现，基于AI评分者信心结合AI评分和人类评分优于单独依赖其中任何一种。给人类提供AI事实核查助手将进一步提高其准确性，但协助方式很重要。显示AI解释、信心和标签会导致过度依赖，而仅仅展示搜索结果和证据则能促进更适当的信任。这些结果对于放大监督——即使AI超越人类专家性能，人类和AI结合监督AI系统的挑战——具有重要意义。', 'title_zh': '人机互补：增强监督的目标'}
{'arxiv_id': 'arXiv:2510.26493', 'title': 'Context Engineering 2.0: The Context of Context Engineering', 'authors': 'Qishuo Hua, Lyumanshan Ye, Dayuan Fu, Yang Xiao, Xiaojie Cai, Yunze Wu, Jifan Lin, Junfei Wang, Pengfei Liu', 'link': 'https://arxiv.org/abs/2510.26493', 'abstract': "Karl Marx once wrote that ``the human essence is the ensemble of social relations'', suggesting that individuals are not isolated entities but are fundamentally shaped by their interactions with other entities, within which contexts play a constitutive and essential role. With the advent of computers and artificial intelligence, these contexts are no longer limited to purely human--human interactions: human--machine interactions are included as well. Then a central question emerges: How can machines better understand our situations and purposes? To address this challenge, researchers have recently introduced the concept of context engineering. Although it is often regarded as a recent innovation of the agent era, we argue that related practices can be traced back more than twenty years. Since the early 1990s, the field has evolved through distinct historical phases, each shaped by the intelligence level of machines: from early human--computer interaction frameworks built around primitive computers, to today's human--agent interaction paradigms driven by intelligent agents, and potentially to human--level or superhuman intelligence in the future. In this paper, we situate context engineering, provide a systematic definition, outline its historical and conceptual landscape, and examine key design considerations for practice. By addressing these questions, we aim to offer a conceptual foundation for context engineering and sketch its promising future. This paper is a stepping stone for a broader community effort toward systematic context engineering in AI systems.", 'abstract_zh': '卡尔·马克思曾经写道，“人的本质是一切社会关系的总和”，暗示个人不是孤立的存在体，而是从根本上被与其他存在体的互动所塑造，在这些互动中，背景扮演着构成性和本质性的角色。随着计算机和人工智能的发展，这些背景不仅限于人与人的互动：人与机器的互动也被包括在内。随之而来的一个核心问题是：机器如何更好地理解我们的处境和目的？为了解决这一挑战，研究人员最近引入了“情境工程”这一概念。尽管它通常被视为智能代理时代的一项近期创新，我们认为相关的实践可以追溯到二十多年前。自20世纪90年代初以来，该领域经历了不同的历史阶段，每个阶段都由机器的智能水平所塑造：从早期围绕原始计算机构建的人机交互框架到今天由智能代理驱动的人机互动范式，并可能迈向类似人类或超人类的智能未来。本文将情境工程置于具体背景中，提供系统性定义，概述其历史和概念框架，并探讨实践中的关键设计考虑。通过这些问题的回答，我们旨在为情境工程提供概念基础，并勾勒其充满希望的未来。本文是促进AI系统中系统化情境工程更广泛社区努力的基石。', 'title_zh': 'Context 工程 2.0：Context 工程 的上下文'}
{'arxiv_id': 'arXiv:2510.26411', 'title': 'MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders', 'authors': 'Riccardo Renzulli, Colas Lepoutre, Enrico Cassano, Marco Grangetto', 'link': 'https://arxiv.org/abs/2510.26411', 'abstract': 'Artificial intelligence in healthcare requires models that are accurate and interpretable. We advance mechanistic interpretability in medical vision by applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP, a vision-language model trained on chest radiographs and reports. To quantify interpretability, we propose an evaluation framework that combines correlation metrics, entropy analyzes, and automated neuron naming via the MedGEMMA foundation model. Experiments on the CheXpert dataset show that MedSAE neurons achieve higher monosemanticity and interpretability than raw MedCLIP features. Our findings bridge high-performing medical AI and transparency, offering a scalable step toward clinically reliable representations.', 'abstract_zh': '医疗保健中的人工智能需要准确可解释的模型。我们通过将Medical Sparse Autoencoders (MedSAEs) 应用于MedCLIP（一种在胸片和报告上训练的vision-language模型）的潜在空间，推进了医疗视觉的机械可解释性。为了量化可解释性，我们提出了一种结合相关性度量、熵分析和通过MedGEMMA基础模型自动神经元命名的评价框架。在CheXpert数据集上的实验表明，MedSAE神经元的单义性和可解释性高于原始的MedCLIP特征。我们的研究结果将高性能的医疗AI与透明性相结合，提供了一条走向临床可靠表示的可扩展步骤。', 'title_zh': 'MedSAE: 用稀疏自编码器剖析 MedCLIP 表征'}
{'arxiv_id': 'arXiv:2510.26402', 'title': 'Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education', 'authors': 'Vikrant Sahu, Gagan Raj Gupta, Raghav Borikar, Nitin Mane', 'link': 'https://arxiv.org/abs/2510.26402', 'abstract': 'The rapid growth of programming education has outpaced traditional assessment tools, leaving faculty with limited means to provide meaningful, scalable feedback. Conventional autograders, while efficient, act as black-box systems that simply return pass/fail results, offering little insight into student thinking or learning needs.\nAutograder+ is designed to shift autograding from a purely summative process to a formative learning experience. It introduces two key capabilities: automated feedback generation using a fine-tuned Large Language Model, and visualization of student code submissions to uncover learning patterns. The model is fine-tuned on curated student code and expert feedback to ensure pedagogically aligned, context-aware guidance.\nIn evaluation across 600 student submissions from multiple programming tasks, the system produced feedback with strong semantic alignment to instructor comments. For visualization, contrastively learned code embeddings trained on 1,000 annotated submissions enable grouping solutions into meaningful clusters based on functionality and approach. The system also supports prompt-pooling, allowing instructors to guide feedback style through selected prompt templates.\nBy integrating AI-driven feedback, semantic clustering, and interactive visualization, Autograder+ reduces instructor workload while supporting targeted instruction and promoting stronger learning outcomes.', 'abstract_zh': '编程教育的迅速发展超出了传统评估工具的能力，使得教师在提供有意义且可扩展的反馈方面手段有限。传统的自动评分系统虽然高效，但作为黑盒系统，仅能返回通过或未通过的结果，无法提供关于学生思考或学习需求的洞察。\n\nAutograder+旨在将自动评分从单纯的总结性过程转变为形成性学习体验。它引入了两项关键功能：使用微调后的大型语言模型自动生成反馈和可视化学生代码提交，以揭示学习模式。该模型通过精选的学生代码和专家反馈进行微调，以确保教育导向性的、情境意识的指导。\n\n在来自多个编程任务的600份学生提交的评估中，系统生成的反馈与教师评论在语义上有很强的一致性。对于可视化而言，基于1000份标注提交训练的对比学习代码嵌入能够根据功能和方法将解决方案分组为有意义的集群。系统还支持提示池功能，允许教师通过选定的提示模板指导反馈风格。\n\n通过集成AI驱动的反馈、语义聚类和互动可视化，Autograder+减少了教师的工作负担，支持更有针对性的指导并促进更强的学习成果。', 'title_zh': 'Autograder+: 一个多方面的人工智能框架，用于编程教育中的丰富教学反馈'}
{'arxiv_id': 'arXiv:2510.26396', 'title': 'A Pragmatic View of AI Personhood', 'authors': 'Joel Z. Leibo, Alexander Sasha Vezhnevets, William A. Cunningham, Stanley M. Bileschi', 'link': 'https://arxiv.org/abs/2510.26396', 'abstract': 'The emergence of agentic Artificial Intelligence (AI) is set to trigger a "Cambrian explosion" of new kinds of personhood. This paper proposes a pragmatic framework for navigating this diversification by treating personhood not as a metaphysical property to be discovered, but as a flexible bundle of obligations (rights and responsibilities) that societies confer upon entities for a variety of reasons, especially to solve concrete governance problems. We argue that this traditional bundle can be unbundled, creating bespoke solutions for different contexts. This will allow for the creation of practical tools -- such as facilitating AI contracting by creating a target "individual" that can be sanctioned -- without needing to resolve intractable debates about an AI\'s consciousness or rationality. We explore how individuals fit in to social roles and discuss the use of decentralized digital identity technology, examining both "personhood as a problem", where design choices can create "dark patterns" that exploit human social heuristics, and "personhood as a solution", where conferring a bundle of obligations is necessary to ensure accountability or prevent conflict. By rejecting foundationalist quests for a single, essential definition of personhood, this paper offers a more pragmatic and flexible way to think about integrating AI agents into our society.', 'abstract_zh': '代理型人工智能的兴起将触发新型主体性的“寒武纪爆炸”：一种实用框架的提出', 'title_zh': '一种实用视角下的AI人格论'}
{'arxiv_id': 'arXiv:2510.26380', 'title': 'AI Mathematician as a Partner in Advancing Mathematical Discovery - A Case Study in Homogenization Theory', 'authors': 'Yuanhang Liu, Beichen Wang, Peng Li, Yang Liu', 'link': 'https://arxiv.org/abs/2510.26380', 'abstract': 'Artificial intelligence (AI) has demonstrated impressive progress in mathematical reasoning, yet its integration into the practice of mathematical research remains limited. In this study, we investigate how the AI Mathematician (AIM) system can operate as a research partner rather than a mere problem solver. Focusing on a challenging problem in homogenization theory, we analyze the autonomous reasoning trajectories of AIM and incorporate targeted human interventions to structure the discovery process. Through iterative decomposition of the problem into tractable subgoals, selection of appropriate analytical methods, and validation of intermediate results, we reveal how human intuition and machine computation can complement one another. This collaborative paradigm enhances the reliability, transparency, and interpretability of the resulting proofs, while retaining human oversight for formal rigor and correctness. The approach leads to a complete and verifiable proof, and more broadly, demonstrates how systematic human-AI co-reasoning can advance the frontier of mathematical discovery.', 'abstract_zh': '人工智能（AI）在数学推理方面展现了令人印象深刻的进步，但在数学研究实践中的整合仍受到限制。本研究探讨了AI数学家（AIM）系统如何作为研究伙伴而非 merely 问题解决者发挥作用。聚焦于调和理论中的一个具有挑战性的问题，我们分析了AIM的自主推理轨迹，并结合有针对性的人类干预来结构发现过程。通过将问题迭代分解为可处理的子目标、选择合适的分析方法以及验证中间结果，我们揭示了人类直觉与机器计算之间的互补作用。这种协作范式增强了所得证明的可靠性和透明度，并保持了形式严谨性和正确性的人类监督。该方法导致了一个完整且可验证的证明，更广泛地说，证明了系统的人工智能与人类共同推理如何推进数学发现的前沿。', 'title_zh': 'AI数学家在推进数学发现中的合作者角色——以 homogenization 理论为例'}
{'arxiv_id': 'arXiv:2510.26346', 'title': 'Discovering State Equivalences in UCT Search Trees By Action Pruning', 'authors': 'Robin Schmöcker, Alexander Dockhorn, Bodo Rosenhahn', 'link': 'https://arxiv.org/abs/2510.26346', 'abstract': 'One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its sample efficiency by grouping/abstracting states or state-action pairs and sharing statistics within a group. Though state-action pair abstractions are mostly easy to find in algorithms such as On the Go Abstractions in Upper Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are found in either noisy or large action space settings due to constraining conditions. We provide theoretical and empirical evidence for this claim, and we slightly alleviate this state abstraction problem by proposing a weaker state abstraction condition that trades a minor loss in accuracy for finding many more abstractions. We name this technique Ideal Pruning Abstractions in UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a large range of test domains and iteration budgets as experimentally validated. IPA-UCT uses a different abstraction framework from Abstraction of State-Action Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore, we show that both IPA and ASAP are special cases of a more general framework that we call p-ASAP which itself is a special case of the ASASAP framework.', 'abstract_zh': '一种提高蒙特卡洛树搜索(MCTS)采样效率的方法是通过分组/抽象状态或状态-动作对并在组内共享统计信息来实现，我们通过更弱的状态抽象条件提出了一种技术，该条件在准确性略有损失的情况下，可以找到更多的抽象，该技术名为理想剪枝抽象在UCT中的应用（IPA-UCT），其在多个测试领域和迭代预算范围内表现优于OGA-UCT及其衍生算法。此外，我们展示了IPA和ASAP都是更一般框架p-ASAP的特例，而p-ASAP又是ASASAP框架的特例。', 'title_zh': '在UCT搜索树中通过动作裁剪发现状态等价性'}
{'arxiv_id': 'arXiv:2510.26094', 'title': 'Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4', 'authors': 'Yuxin Li, Minghao Liu, Ruida Wang, Wenzhao Ji, Zhitao He, Rui Pan, Junming Huang, Tong Zhang, Yi R. Fung', 'link': 'https://arxiv.org/abs/2510.26094', 'abstract': 'We present **Lean4PHYS**, a comprehensive reasoning framework for college-level physics problems in Lean4. **Lean4PHYS** includes *LeanPhysBench*, a college-level benchmark for formal physics reasoning in Lean4, which contains 200 hand-crafted and peer-reviewed statements derived from university textbooks and physics competition problems. To establish a solid foundation for formal reasoning in physics, we also introduce *PhysLib*, a community-driven repository containing fundamental unit systems and theorems essential for formal physics reasoning. Based on the benchmark and Lean4 repository we composed in **Lean4PHYS**, we report baseline results using major expert Math Lean4 provers and state-of-the-art closed-source models, with the best performance of DeepSeek-Prover-V2-7B achieving only 16% and Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that our *PhysLib* can achieve an average improvement of 11.75% in model performance. This demonstrates the challenging nature of our *LeanPhysBench* and the effectiveness of *PhysLib*. To the best of our knowledge, this is the first study to provide a physics benchmark in Lean4.', 'abstract_zh': 'Lean4PHYS：Lean4中针对大学物理问题的综合推理框架', 'title_zh': 'Lean4Physics：面向大学水平物理学的Lean4综合推理框架'}
{'arxiv_id': 'arXiv:2510.26057', 'title': 'Can AI be Accountable?', 'authors': 'Andrew L. Kun', 'link': 'https://arxiv.org/abs/2510.26057', 'abstract': "The AI we use is powerful, and its power is increasing rapidly. If this powerful AI is to serve the needs of consumers, voters, and decision makers, then it is imperative that the AI is accountable. In general, an agent is accountable to a forum if the forum can request information from the agent about its actions, if the forum and the agent can discuss this information, and if the forum can sanction the agent. Unfortunately, in too many cases today's AI is not accountable -- we cannot question it, enter into a discussion with it, let alone sanction it. In this chapter we relate the general definition of accountability to AI, we illustrate what it means for AI to be accountable and unaccountable, and we explore approaches that can improve our chances of living in a world where all AI is accountable to those who are affected by it.", 'abstract_zh': '我们使用的AI非常强大，其能力正在快速增加。如果要让这种强大的AI服务于消费者、选民和决策者的需求，那么AI必须具有问责性。一般来说，如果一个代理可以向某个论坛提供其行为的信息，该论坛可以与代理讨论这些信息，并可以对代理进行制裁，那么该代理就是对这个论坛问责的。不幸的是，在今天太多情况下，AI并不具有问责性——我们不能质疑它，甚至不能与其进行讨论，更不用说对其进行制裁了。在本章中，我们将问责性的一般定义应用于AI，阐述AI具有和不具有问责性的含义，并探讨可以提高所有受影响的AI都对其问责的机会的方法。', 'title_zh': 'AI能负责任吗？'}
{'arxiv_id': 'arXiv:2510.25951', 'title': 'Estimating cognitive biases with attention-aware inverse planning', 'authors': 'Sounak Banerjee, Daphne Cornelisse, Deepak Gopinath, Emily Sumner, Jonathan DeCastro, Guy Rosman, Eugene Vinitsky, Mark K. Ho', 'link': 'https://arxiv.org/abs/2510.25951', 'abstract': "People's goal-directed behaviors are influenced by their cognitive biases, and autonomous systems that interact with people should be aware of this. For example, people's attention to objects in their environment will be biased in a way that systematically affects how they perform everyday tasks such as driving to work. Here, building on recent work in computational cognitive science, we formally articulate the attention-aware inverse planning problem, in which the goal is to estimate a person's attentional biases from their actions. We demonstrate how attention-aware inverse planning systematically differs from standard inverse reinforcement learning and how cognitive biases can be inferred from behavior. Finally, we present an approach to attention-aware inverse planning that combines deep reinforcement learning with computational cognitive modeling. We use this approach to infer the attentional strategies of RL agents in real-life driving scenarios selected from the Waymo Open Dataset, demonstrating the scalability of estimating cognitive biases with attention-aware inverse planning.", 'abstract_zh': '人们的定向行为受到认知偏差的影响，与人的交互自主系统应予关注。例如，人们的环境注意力分配会系统性地影响他们日常工作如开车上班的表现。在此基础上，我们借鉴计算认知科学的最新成果，正式阐明了注意力感知逆规划问题，目标是从人的行为中估计其注意力偏差。我们展示了注意力感知逆规划如何系统性地不同于标准逆强化学习，并说明可以通过行为推断认知偏差。最后，我们提出了一种结合深度强化学习与计算认知建模的注意力感知逆规划方法。我们使用此方法从Waymo开放数据集中选择的真实驾驶场景中推断强化学习代理的注意力策略，展示了注意力感知逆规划估计认知偏差的可扩展性。', 'title_zh': '基于注意力感知逆规划的认知偏差估计'}
{'arxiv_id': 'arXiv:2510.25914', 'title': 'FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization', 'authors': 'Ngoc Phuoc An Vo, Manish Kesarwani, Ruchi Mahindru, Chandrasekhar Narayanaswami', 'link': 'https://arxiv.org/abs/2510.25914', 'abstract': 'FinOps (Finance + Operations) represents an operational framework and cultural practice which maximizes cloud business value through collaborative financial accountability across engineering, finance, and business teams. FinOps practitioners face a fundamental challenge: billing data arrives in heterogeneous formats, taxonomies, and metrics from multiple cloud providers and internal systems which eventually lead to synthesizing actionable insights, and making time-sensitive decisions. To address this challenge, we propose leveraging autonomous, goal-driven AI agents for FinOps automation. In this paper, we built a FinOps agent for a typical use-case for IT infrastructure and cost optimization. We built a system simulating a realistic end-to-end industry process starting with retrieving data from various sources to consolidating and analyzing the data to generate recommendations for optimization. We defined a set of metrics to evaluate our agent using several open-source and close-source language models and it shows that the agent was able to understand, plan, and execute tasks as well as an actual FinOps practitioner.', 'abstract_zh': 'FinOps（ finance + operations）代表一种通过跨工程、财务和业务团队协作财务问责制来最大化云业务价值的操作框架和文化实践。FinOps从业者面临一项基本挑战：来自多个云提供商和内部系统的计费数据以异构格式、分类法和指标形式到达，最终导致综合可操作的洞察并作出及时决策。为应对这一挑战，我们提出利用自主的目标驱动AI代理进行FinOps自动化。在本文中，我们构建了一个用于典型IT基础设施和成本优化的FinOps代理。我们构建了一个模拟真实端到端行业过程的系统，从从各种来源获取数据开始，到集中和分析数据以生成优化建议。我们定义了一组指标来评估我们的代理，使用多个开源和闭源语言模型表明，该代理能够理解、规划和执行任务，与实际的FinOps从业者相当。', 'title_zh': 'FinOps代理——IT基础设施和成本优化的应用案例'}
{'arxiv_id': 'arXiv:2510.25883', 'title': 'The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence', 'authors': 'Christian Dittrich, Jennifer Flygare Kinne', 'link': 'https://arxiv.org/abs/2510.25883', 'abstract': 'Existing frameworks converge on the centrality of compression to intelligence but leave underspecified why this process enforces the discovery of causal structure rather than superficial statistical patterns. We introduce a two-level framework to address this gap. The Information-Theoretic Imperative (ITI) establishes that any system persisting in uncertain environments must minimize epistemic entropy through predictive compression: this is the evolutionary "why" linking survival pressure to information-processing demands. The Compression Efficiency Principle (CEP) specifies how efficient compression mechanically selects for generative, causal models through exception-accumulation dynamics, making reality alignment a consequence rather than a contingent achievement. Together, ITI and CEP define a causal chain: from survival pressure to prediction necessity, compression requirement, efficiency optimization, generative structure discovery, and ultimately reality alignment. Each link follows from physical, information-theoretic, or evolutionary constraints, implying that intelligence is the mechanically necessary outcome of persistence in structured environments. This framework yields empirically testable predictions: compression efficiency, measured as approach to the rate-distortion frontier, correlates with out-of-distribution generalization; exception-accumulation rates differentiate causal from correlational models; hierarchical systems exhibit increasing efficiency across abstraction layers; and biological systems demonstrate metabolic costs that track representational complexity. ITI and CEP thereby provide a unified account of convergence across biological, artificial, and multi-scale systems, addressing the epistemic and functional dimensions of intelligence without invoking assumptions about consciousness or subjective experience.', 'abstract_zh': '基于信息论的必要性与压缩效率原则：弥合智能中压缩与因果结构发现之间的鸿沟', 'title_zh': '信息论的 imperative ：压缩与智能的认识论基础'}
{'arxiv_id': 'arXiv:2510.25813', 'title': 'An Agentic Framework for Rapid Deployment of Edge AI Solutions in Industry 5.0', 'authors': 'Jorge Martinez-Gil, Mario Pichler, Nefeli Bountouni, Sotiris Koussouris, Marielena Márquez Barreiro, Sergio Gusmeroli', 'link': 'https://arxiv.org/abs/2510.25813', 'abstract': 'We present a novel framework for Industry 5.0 that simplifies the deployment of AI models on edge devices in various industrial settings. The design reduces latency and avoids external data transfer by enabling local inference and real-time processing. Our implementation is agent-based, which means that individual agents, whether human, algorithmic, or collaborative, are responsible for well-defined tasks, enabling flexibility and simplifying integration. Moreover, our framework supports modular integration and maintains low resource requirements. Preliminary evaluations concerning the food industry in real scenarios indicate improved deployment time and system adaptability performance. The source code is publicly available at this https URL.', 'abstract_zh': '我们提出了一种面向Industry 5.0的新型框架，该框架简化了各种工业环境中AI模型在边缘设备上的部署。该设计通过实现局部推理和实时处理来减少延迟并避免外部数据传输。我们的实现基于代理，这意味着无论代理是人力、算法还是协作形式，都负责明确的任务，从而实现了灵活性并简化了集成。此外，该框架支持模块化集成并保持低资源要求。初步评估显示，在实际食品行业场景中的部署时间与系统适应性性能有所改善。源代码可在以下网址公开访问：this https URL。', 'title_zh': '面向 Industry 5.0 的边缘人工智能解决方案快速部署能力框架'}
{'arxiv_id': 'arXiv:2510.25775', 'title': 'Towards Piece-by-Piece Explanations for Chess Positions with SHAP', 'authors': 'Francesco Spinnato', 'link': 'https://arxiv.org/abs/2510.25775', 'abstract': 'Contemporary chess engines offer precise yet opaque evaluations, typically expressed as centipawn scores. While effective for decision-making, these outputs obscure the underlying contributions of individual pieces or patterns. In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the domain of chess analysis, aiming to attribute a chess engines evaluation to specific pieces on the board. By treating pieces as features and systematically ablating them, we compute additive, per-piece contributions that explain the engines output in a locally faithful and human-interpretable manner. This method draws inspiration from classical chess pedagogy, where players assess positions by mentally removing pieces, and grounds it in modern explainable AI techniques. Our approach opens new possibilities for visualization, human training, and engine comparison. We release accompanying code and data to foster future research in interpretable chess AI.', 'abstract_zh': '当代国际象棋引擎提供精确但不透明的评估，通常以百子分数的形式表达。虽然这些输出对决策非常有效，但它们掩盖了每枚棋子或模式的底层贡献。本文探讨将SHAP（SHapley Additive exPlanations）方法应用于国际象棋分析领域，旨在将引擎的评估归因于棋盘上的特定棋子。通过将棋子视为特征并系统地去除它们，我们计算出每个棋子的增量贡献，从而以局部忠实且易于人类理解的方式解释引擎的输出。该方法借鉴了经典的国际象棋教学方法，其中棋手通过心理移除棋子来评估局面，并将其置于现代可解释AI技术的基础之上。本文的方法为可视化、人类训练和引擎比较开辟了新的可能性。我们发布了配套的代码和数据，以促进可解释国际象棋AI的未来研究。', 'title_zh': '基于SHAP的象棋局面逐块解释方法'}
{'arxiv_id': 'arXiv:2510.26787', 'title': 'Remote Labor Index: Measuring AI Automation of Remote Work', 'authors': 'Mantas Mazeika, Alice Gatti, Cristina Menghini, Udari Madhushani Sehwag, Shivam Singhal, Yury Orlovskiy, Steven Basart, Manasi Sharma, Denis Peskoff, Elaine Lau, Jaehyuk Lim, Lachlan Carroll, Alice Blair, Vinaya Sivakumar, Sumana Basu, Brad Kenstler, Yuntao Ma, Julian Michael, Xiaoke Li, Oliver Ingebretsen, Aditya Mehta, Jean Mottola, John Teichmann, Kevin Yu, Zaina Shaik, Adam Khoja, Richard Ren, Jason Hausenloy, Long Phan, Ye Htet, Ankit Aich, Tahseen Rabbani, Vivswan Shah, Andriy Novykov, Felix Binder, Kirill Chugunov, Luis Ramirez, Matias Geralnik, Hernán Mesura, Dean Lee, Ed-Yeremai Hernandez Cardona, Annette Diamond, Summer Yue, Alexandr Wang, Bing Liu, Ernesto Hernandez, Dan Hendrycks', 'link': 'https://arxiv.org/abs/2510.26787', 'abstract': 'AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.', 'abstract_zh': 'AIs在知识和推理的研究导向基准上取得了 rapid 进展，但这些进展如何转化为经济价值和自动化尚不明确。为了衡量这一点，我们引入了远程劳动力指数（RLI），这是一个广泛覆盖多个行业的基准，包含实际的、具有经济价值的项目，旨在评估代理在实际环境中的端到端表现。AI代理在RLI上的表现接近最低水平，最高性能的代理实现了2.5%的自动化率。这些结果有助于基于实证证据讨论AI自动化，为跟踪AI影响设定共同基准，并使利益相关者能够主动应对由AI驱动的劳动力自动化。', 'title_zh': '远程劳动指数：衡量人工智能对远程工作的自动化程度'}
{'arxiv_id': 'arXiv:2510.26776', 'title': 'Faithful and Fast Influence Function via Advanced Sampling', 'authors': 'Jungyeon Koh, Hyeonsu Lyu, Jonggyu Jang, Hyun Jong Yang', 'link': 'https://arxiv.org/abs/2510.26776', 'abstract': 'How can we explain the influence of training data on black-box models? Influence functions (IFs) offer a post-hoc solution by utilizing gradients and Hessians. However, computing the Hessian for an entire dataset is resource-intensive, necessitating a feasible alternative. A common approach involves randomly sampling a small subset of the training data, but this method often results in highly inconsistent IF estimates due to the high variance in sample configurations. To address this, we propose two advanced sampling techniques based on features and logits. These samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits, thereby enhancing the accuracy of IF estimations. We validate our approach through class removal experiments, a typical application of IFs, using the F1-score to measure how effectively the model forgets the removed class while maintaining inference consistency on the remaining classes. Our method reduces computation time by 30.1% and memory usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.', 'abstract_zh': '如何解释训练数据对黑盒模型的影响？特征和logits导向的高级采样技术提供了一种替代方案，以提高影响函数估计的准确性。', 'title_zh': '忠实且高效的影响力函数通过高级采样'}
{'arxiv_id': 'arXiv:2510.26745', 'title': 'Deep sequence models tend to memorize geometrically; it is unclear why', 'authors': 'Shahriar Noroozizadeh, Vaishnavh Nagarajan, Elan Rosenfeld, Sanjiv Kumar', 'link': 'https://arxiv.org/abs/2510.26745', 'abstract': 'In sequence modeling, the parametric memory of atomic facts has been predominantly abstracted as a brute-force lookup of co-occurrences between entities. We contrast this associative view against a geometric view of how memory is stored. We begin by isolating a clean and analyzable instance of Transformer reasoning that is incompatible with memory as strictly a storage of the local co-occurrences specified during training. Instead, the model must have somehow synthesized its own geometry of atomic facts, encoding global relationships between all entities, including non-co-occurring ones. This in turn has simplified a hard reasoning task involving an $\\ell$-fold composition into an easy-to-learn 1-step geometric task.\nFrom this phenomenon, we extract fundamental aspects of neural embedding geometries that are hard to explain. We argue that the rise of such a geometry, despite optimizing over mere local associations, cannot be straightforwardly attributed to typical architectural or optimizational pressures. Counterintuitively, an elegant geometry is learned even when it is not more succinct than a brute-force lookup of associations.\nThen, by analyzing a connection to Node2Vec, we demonstrate how the geometry stems from a spectral bias that -- in contrast to prevailing theories -- indeed arises naturally despite the lack of various pressures. This analysis also points to practitioners a visible headroom to make Transformer memory more strongly geometric. We hope the geometric view of parametric memory encourages revisiting the default intuitions that guide researchers in areas like knowledge acquisition, capacity, discovery and unlearning.', 'abstract_zh': '在序列建模中，原子事实的参数化记忆主要被抽象为实体共现的简单查找。我们将这种关联视角与记忆的几何存储方式进行了对比。我们首先隔离了一个与严格存储训练期间指定的局部共现无兼容性的Transformer推理实例。模型必须通过某种方式合成自身的原子事实几何结构，编码所有实体之间的全局关系，包括非共现实体之间的关系。这进而将一个艰难的推理任务简化为一个易于学习的一步几何任务。\n\n从这一现象中，我们提取出难以用典型架构或优化压力解释的基本神经嵌入几何特性。我们认为，尽管优化仅涉及局部关联，如此几何结构的兴起仍不能简单归因于这些典型压力。令人Unexpected地，即使几何结构并不是比关联查找更简洁，它也能自然地被学习到。\n\n通过分析Node2Vec的连接关系，我们展示了这种几何结构源自光谱偏差——与现有理论相反，即使缺乏各种压力，光谱偏差确实自然地出现。这种分析也为实践者指出了增强Transformer记忆几何特性的可见空间。我们希望对参数化记忆的几何视角能促进研究人员重新审视知识获取、容量、发现和遗忘等领域的默认直觉。', 'title_zh': '深度序列模型倾向于记忆几何特征；尚不清楚其中原因。'}
{'arxiv_id': 'arXiv:2510.26740', 'title': 'A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation', 'authors': 'Ashwin Kumar, William Yeoh', 'link': 'https://arxiv.org/abs/2510.26740', 'abstract': "We introduce the General Incentives-based Framework for Fairness (GIFF), a novel approach for fair multi-agent resource allocation that infers fair decision-making from standard value functions. In resource-constrained settings, agents optimizing for efficiency often create inequitable outcomes. Our approach leverages the action-value (Q-)function to balance efficiency and fairness without requiring additional training. Specifically, our method computes a local fairness gain for each action and introduces a counterfactual advantage correction term to discourage over-allocation to already well-off agents. This approach is formalized within a centralized control setting, where an arbitrator uses the GIFF-modified Q-values to solve an allocation problem.\nEmpirical evaluations across diverse domains, including dynamic ridesharing, homelessness prevention, and a complex job allocation task-demonstrate that our framework consistently outperforms strong baselines and can discover far-sighted, equitable policies. The framework's effectiveness is supported by a theoretical foundation; we prove its fairness surrogate is a principled lower bound on the true fairness improvement and that its trade-off parameter offers monotonic tuning. Our findings establish GIFF as a robust and principled framework for leveraging standard reinforcement learning components to achieve more equitable outcomes in complex multi-agent systems.", 'abstract_zh': '基于激励的一致性框架以实现公平性（GIFF）：一种新的公平多智能体资源分配方法', 'title_zh': '基于激励的多代理资源分配公平性通用框架'}
{'arxiv_id': 'arXiv:2510.26730', 'title': 'ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference', 'authors': 'Zixu Shen, Kexin Chu, Yifan Zhang, Dawei Xiang, Runxin Wu, Wei Zhang', 'link': 'https://arxiv.org/abs/2510.26730', 'abstract': 'The expansion of large language models is increasingly limited by the constrained memory capacity of modern GPUs. To mitigate this, Mixture-of-Experts (MoE) architectures activate only a small portion of parameters during inference, significantly lowering both memory demand and computational overhead. However, conventional MoE inference approaches, which select active experts independently at each layer, often introduce considerable latency because of frequent parameter transfers between host and GPU memory. In addition, current cross-layer prediction strategies, which are typically based on fixed steps, lack adaptability across different hardware platforms and workloads, thereby reducing their robustness and effectiveness.\nTo address these challenges, we present ExpertFlow, a runtime system for MoE inference that combines adaptive expert prefetching and cache-aware routing. ExpertFlow continuously adjusts its prediction horizon for expert activation by leveraging runtime statistics such as transfer bandwidth, parameter dimensionality, and model feedback signals. Furthermore, it incorporates a hybrid cross-layer prediction scheme that fuses pregating information with intermediate computational states to anticipate future expert needs. By adaptively refining prefetching decisions and aligning them with actual usage behavior, ExpertFlow effectively decreases cache misses and removes latency caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces model stall time to less than 0.1% of the baseline, highlighting its capability to optimize MoE inference under stringent memory constraints.', 'abstract_zh': '基于专家流的MoE推理运行时系统：结合自适应专家预取和缓存感知路由', 'title_zh': 'ExpertFlow：自适应专家调度与内存协调以实现高效MoE推理'}
{'arxiv_id': 'arXiv:2510.26722', 'title': 'Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off', 'authors': 'Muhammad Faraz Ul Abrar, Nicolò Michelusi', 'link': 'https://arxiv.org/abs/2510.26722', 'abstract': 'Over-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \\emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.', 'abstract_zh': '空中 federated learning 与随机梯度下降在无线异构环境下的建模与优化', 'title_zh': '非凸空中异构联邦学习：偏差-方差权衡'}
{'arxiv_id': 'arXiv:2510.26714', 'title': 'On the limitation of evaluating machine unlearning using only a single training seed', 'authors': 'Jamie Lanyon, Axel Finke, Petros Andreou, Georgina Cosma', 'link': 'https://arxiv.org/abs/2510.26714', 'abstract': 'Machine unlearning (MU) aims to remove the influence of certain data points from a trained model without costly retraining. Most practical MU algorithms are only approximate and their performance can only be assessed empirically. Care must therefore be taken to make empirical comparisons as representative as possible. A common practice is to run the MU algorithm multiple times independently starting from the same trained model. In this work, we demonstrate that this practice can give highly non-representative results because -- even for the same architecture and same dataset -- some MU methods can be highly sensitive to the choice of random number seed used for model training. We therefore recommend that empirical comphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should also reflect the variability across different model training seeds.', 'abstract_zh': '机器遗忘（MU）的目标是在无需昂贵重新训练的情况下从训练模型中去除某些数据点的影响。大多数实际的MU算法只是近似的，其性能只能通过经验来评估。因此，在进行经验比较时必须尽可能使其具有代表性。一种常见的做法是从同一个训练模型开始，独立地多次运行MU算法。然而，在这项工作中，我们证明了这种做法可能会导致高度非代表性的结果，即使在同一架构和同一数据集下，某些MU方法也可能对用于模型训练的随机数种子的选择高度敏感。因此，我们建议在评估MU算法时也应反映出不同模型训练种子之间的变化性。', 'title_zh': '关于仅使用单个训练种子评估机器遗忘限制的局限性'}
{'arxiv_id': 'arXiv:2510.26616', 'title': 'Aeolus: A Multi-structural Flight Delay Dataset', 'authors': 'Lin Xu, Xinyun Yuan, Yuxuan Liang, Suwan Yin, Yuankai Wu', 'link': 'https://arxiv.org/abs/2510.26616', 'abstract': 'We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed to advance research on flight delay prediction and support the development of foundation models for tabular data. Existing datasets in this domain are typically limited to flat tabular structures and fail to capture the spatiotemporal dynamics inherent in delay propagation. Aeolus addresses this limitation by providing three aligned modalities: (i) a tabular dataset with rich operational, meteorological, and airportlevel features for over 50 million flights; (ii) a flight chain module that models delay propagation along sequential flight legs, capturing upstream and downstream dependencies; and (iii) a flight network graph that encodes shared aircraft, crew, and airport resource connections, enabling cross-flight relational reasoning. The dataset is carefully constructed with temporal splits, comprehensive features, and strict leakage prevention to support realistic and reproducible machine learning evaluation. Aeolus supports a broad range of tasks, including regression, classification, temporal structure modeling, and graph learning, serving as a unified benchmark across tabular, sequential, and graph modalities. We release baseline experiments and preprocessing tools to facilitate adoption. Aeolus fills a key gap for both domain-specific modeling and general-purpose structured data this http URL source code and data can be accessed at this https URL', 'abstract_zh': 'Aeolus：一种大规模多模态飞行延误数据集，用于推进飞行延误预测研究并支持表格数据基础模型的发展', 'title_zh': 'Aeolus: 多结构航班延误数据集'}
{'arxiv_id': 'arXiv:2510.26575', 'title': 'InfoFlow: Reinforcing Search Agent Via Reward Density Optimization', 'authors': 'Kun Luo, Hongjin Qian, Zheng Liu, Ziyi Xia, Shitao Xiao, Siqi Bao, Jun Zhao, Kang Liu', 'link': 'https://arxiv.org/abs/2510.26575', 'abstract': "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach for enhancing agentic deep search. However, its application is often hindered by low \\textbf{Reward Density} in deep search scenarios, where agents expend significant exploratory costs for infrequent and often null final rewards. In this paper, we formalize this challenge as the \\textbf{Reward Density Optimization} problem, which aims to improve the reward obtained per unit of exploration cost. This paper introduce \\textbf{InfoFlow}, a systematic framework that tackles this problem from three aspects. 1) \\textbf{Subproblem decomposition}: breaking down long-range tasks to assign process rewards, thereby providing denser learning signals. 2) \\textbf{Failure-guided hints}: injecting corrective guidance into stalled trajectories to increase the probability of successful outcomes. 3) \\textbf{Dual-agent refinement}: employing a dual-agent architecture to offload the cognitive burden of deep exploration. A refiner agent synthesizes the search history, which effectively compresses the researcher's perceived trajectory, thereby reducing exploration cost and increasing the overall reward density. We evaluate InfoFlow on multiple agentic search benchmarks, where it significantly outperforms strong baselines, enabling lightweight LLMs to achieve performance comparable to advanced proprietary LLMs.", 'abstract_zh': '可验证奖励的强化学习（RLVR）：提升代理深度搜索的有效方法及其应用中的奖励密度优化（InfoFlow框架）', 'title_zh': 'InfoFlow：通过奖励密度优化强化搜索代理'}
{'arxiv_id': 'arXiv:2510.26566', 'title': 'Multiclass Local Calibration With the Jensen-Shannon Distance', 'authors': 'Cesare Barbera, Lorenzo Perini, Giovanni De Toni, Andrea Passerini, Andrea Pugnana', 'link': 'https://arxiv.org/abs/2510.26566', 'abstract': 'Developing trustworthy Machine Learning (ML) models requires their predicted probabilities to be well-calibrated, meaning they should reflect true-class frequencies. Among calibration notions in multiclass classification, strong calibration is the most stringent, as it requires all predicted probabilities to be simultaneously calibrated across all classes. However, existing approaches to multiclass calibration lack a notion of distance among inputs, which makes them vulnerable to proximity bias: predictions in sparse regions of the feature space are systematically miscalibrated. This is especially relevant in high-stakes settings, such as healthcare, where the sparse instances are exactly those most at risk of biased treatment. In this work, we address this main shortcoming by introducing a local perspective on multiclass calibration. First, we formally define multiclass local calibration and establish its relationship with strong calibration. Second, we theoretically analyze the pitfalls of existing evaluation metrics when applied to multiclass local calibration. Third, we propose a practical method for enhancing local calibration in Neural Networks, which enforces alignment between predicted probabilities and local estimates of class frequencies using the Jensen-Shannon distance. Finally, we empirically validate our approach against existing multiclass calibration techniques.', 'abstract_zh': '开发值得信赖的机器学习（ML）模型要求其预测概率准确反映真实类频率。在多类分类的校准概念中，强校准是最严格的，因为它要求所有预测概率在所有类中同时得到校准。然而，现有的多类校准方法缺乏输入之间的距离观念，这使它们容易受到邻近偏差的影响：特征空间中稀疏区域的预测是系统地失校准的。这一点在诸如医疗保健这样高风险的场景中尤为重要，因为在这些场景中，稀疏实例正是那些最有可能受到偏差对待的实例。在本文中，我们通过引入多类校准的局部视角来解决这一主要不足。首先，我们正式定义多类局部校准并建立其与强校准的关系。其次，我们理论上分析现有评估指标在应用于多类局部校准时的缺陷。第三，我们提出了一种实用的方法来增强神经网络中的局部校准，该方法使用Jensen-Shannon距离强制预测概率与局部类频率估计之间的对齐。最后，我们实证验证了我们的方法相对于现有多类校准技术的有效性。', 'title_zh': '多类局部校准与詹森-沙伦距离'}
{'arxiv_id': 'arXiv:2510.26451', 'title': 'Robust Graph Condensation via Classification Complexity Mitigation', 'authors': 'Jiayi Luo, Qingyun Sun, Beining Yang, Haonan Yuan, Xingcheng Fu, Yanbiao Ma, Jianxin Li, Philip S. Yu', 'link': 'https://arxiv.org/abs/2510.26451', 'abstract': 'Graph condensation (GC) has gained significant attention for its ability to synthesize smaller yet informative graphs. However, existing studies often overlook the robustness of GC in scenarios where the original graph is corrupted. In such cases, we observe that the performance of GC deteriorates significantly, while existing robust graph learning technologies offer only limited effectiveness. Through both empirical investigation and theoretical analysis, we reveal that GC is inherently an intrinsic-dimension-reducing process, synthesizing a condensed graph with lower classification complexity. Although this property is critical for effective GC performance, it remains highly vulnerable to adversarial perturbations. To tackle this vulnerability and improve GC robustness, we adopt the geometry perspective of graph data manifold and propose a novel Manifold-constrained Robust Graph Condensation framework named MRGC. Specifically, we introduce three graph data manifold learning modules that guide the condensed graph to lie within a smooth, low-dimensional manifold with minimal class ambiguity, thereby preserving the classification complexity reduction capability of GC and ensuring robust performance under universal adversarial attacks. Extensive experiments demonstrate the robustness of \\ModelName\\ across diverse attack scenarios.', 'abstract_zh': '图凝缩（GC）由于其生成更小但具有信息性的图形的能力而得到了广泛关注。然而，现有研究往往忽视了原图被破坏时GC的鲁棒性。在这种情况下，我们观察到GC的性能显著下降，而现有的鲁棒图学习技术则只能提供有限的有效性。通过实证研究和理论分析，我们揭示了GC本质上是一个固有维数降低的过程，生成了一个分类复杂性较低的凝缩图。尽管这一特性对于有效GC性能至关重要，但它仍然对对抗性扰动高度脆弱。为了应对这种脆弱性并提高GC的鲁棒性，我们从图数据流形的几何视角出发，提出了一种新的流形约束鲁棒图凝缩框架，命名为MRGC。具体而言，我们引入了三个图数据流形学习模块，引导凝缩图位于一个光滑的低维流形上，具有最小的类别模糊性，从而保留了GC的分类复杂性降低能力，并在普遍对抗性攻击下保证了稳健的性能。广泛的实验表明，\\ModelName\\在各种攻击场景下具有鲁棒性。', 'title_zh': '分类复杂性减轻下的鲁棒图凝聚'}
{'arxiv_id': 'arXiv:2510.26444', 'title': 'Personalized Treatment Outcome Prediction from Scarce Data via Dual-Channel Knowledge Distillation and Adaptive Fusion', 'authors': 'Wenjie Chen, Li Zhuang, Ziying Luo, Yu Liu, Jiahao Wu, Shengcai Liu', 'link': 'https://arxiv.org/abs/2510.26444', 'abstract': 'Personalized treatment outcome prediction based on trial data for small-sample and rare patient groups is critical in precision medicine. However, the costly trial data limit the prediction performance. To address this issue, we propose a cross-fidelity knowledge distillation and adaptive fusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation data to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN incorporates a dual-channel knowledge distillation module to extract complementary knowledge from the low-fidelity model, along with an attention-guided fusion module to dynamically integrate multi-source information. Experiments on treatment outcome prediction for the chronic obstructive pulmonary disease demonstrates significant improvements of CFKD-AFN over state-of-the-art methods in prediction accuracy, ranging from 6.67\\% to 74.55\\%, and strong robustness to varying high-fidelity dataset sizes. Furthermore, we extend CFKD-AFN to an interpretable variant, enabling the exploration of latent medical semantics to support clinical decision-making.', 'abstract_zh': '基于试验数据的个性化治疗效果预测对于小样本和稀有患者群体至关重要，是精准医学中的关键任务。然而，昂贵的试验数据限制了预测性能。为此，我们提出了一种跨保真度知识蒸馏和自适应融合网络（CFKD-AFN），利用丰富的但保真度较低的模拟数据来增强对稀少但保真度较高的试验数据的预测。CFKD-AFN 包含一个双通道知识蒸馏模块，用于从低保真度模型中提取互补知识，以及一个基于注意力的融合模块，用于动态整合多源信息。慢性阻塞性肺病治疗效果预测实验展示了 CFKD-AFN 在预测准确性方面比现有方法显著提高，范围从 6.67% 到 74.55%，并且在不同大小的高保真度数据集上表现出强大的稳健性。此外，我们将 CFKD-AFN 扩展为一种可解释的变体，以探索潜在的医学语义来支持临床决策。', 'title_zh': '基于双通道知识蒸馏和自适应融合的稀缺数据个性化治疗效果预测'}
{'arxiv_id': 'arXiv:2510.26420', 'title': 'SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification', 'authors': 'Yingjia Wang, Ting Qiao, Xing Liu, Chongzuo Li, Sixing Wu, Jianbin Li', 'link': 'https://arxiv.org/abs/2510.26420', 'abstract': 'The rapid advancement of deep neural networks (DNNs) heavily relies on large-scale, high-quality datasets. However, unauthorized commercial use of these datasets severely violates the intellectual property rights of dataset owners. Existing backdoor-based dataset ownership verification methods suffer from inherent limitations: poison-label watermarks are easily detectable due to label inconsistencies, while clean-label watermarks face high technical complexity and failure on high-resolution images. Moreover, both approaches employ static watermark patterns that are vulnerable to detection and removal. To address these issues, this paper proposes a sample-specific clean-label backdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked sample generator, this method generates unique watermarks for each sample, fundamentally overcoming the vulnerability of static watermark patterns. The core innovation lies in designing a composite loss function with three components: target sample loss ensures watermark effectiveness, non-target sample loss guarantees trigger reliability, and perceptual similarity loss maintains visual imperceptibility. During ownership verification, black-box testing is employed to check whether suspicious models exhibit predefined backdoor behaviors. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed method and its robustness against potential watermark removal attacks.', 'abstract_zh': '基于样本特定的干净标签后门水印方法（即SSCL-BW）', 'title_zh': 'SSCL-BW: 样本特定的干净标签后门水印方法用于数据集所有权验证'}
{'arxiv_id': 'arXiv:2510.26412', 'title': 'LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation', 'authors': 'Xiangqing Zheng, Chengyue Wu, Kehai Chen, Min Zhang', 'link': 'https://arxiv.org/abs/2510.26412', 'abstract': 'Recently text-to-video generation has made impressive progress in producing short, high-quality clips, but evaluating long-form outputs remains a major challenge especially when processing complex prompts. Existing benchmarks mostly rely on simplified prompts and focus on low-level metrics, overlooking fine-grained alignment with prompts and abstract dimensions such as narrative coherence and thematic expression. To address these gaps, we propose LoCoT2V-Bench, a benchmark specifically designed for long video generation (LVG) under complex input conditions. Based on various real-world videos, LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating elements like scene transitions and event dynamics. Moreover, it constructs a multi-dimensional evaluation framework that includes our newly proposed metrics such as event-level alignment, fine-grained temporal consistency, content clarity, and the Human Expectation Realization Degree (HERD) that focuses on more abstract attributes like narrative flow, emotional response, and character development. Using this framework, we conduct a comprehensive evaluation of nine representative LVG models, finding that while current methods perform well on basic visual and temporal aspects, they struggle with inter-event consistency, fine-grained alignment, and high-level thematic adherence, etc. Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for evaluating long-form complex text-to-video generation and highlights critical directions for future method improvement.', 'abstract_zh': 'LoCoT2V-Bench：一种专门针对复杂输入条件下长视频生成的基准', 'title_zh': 'LoCoT2V-Bench: 一个长文本和复杂文本生成视频的基准'}
{'arxiv_id': 'arXiv:2510.26342', 'title': 'Linear Causal Discovery with Interventional Constraints', 'authors': 'Zhigao Guo, Feng Dong', 'link': 'https://arxiv.org/abs/2510.26342', 'abstract': 'Incorporating causal knowledge and mechanisms is essential for refining causal models and improving downstream tasks such as designing new treatments. In this paper, we introduce a novel concept in causal discovery, termed interventional constraints, which differs fundamentally from interventional data. While interventional data require direct perturbations of variables, interventional constraints encode high-level causal knowledge in the form of inequality constraints on causal effects. For instance, in the Sachs dataset (Sachs et al.\\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3 exerts a positive causal effect on Akt. Existing causal discovery methods allow enforcing structural constraints (for example, requiring a causal path from PIP3 to Akt), but they may still produce incorrect causal conclusions such as learning that "PIP3 inhibits Akt". Interventional constraints bridge this gap by explicitly constraining the total causal effect between variable pairs, ensuring learned models respect known causal influences. To formalize interventional constraints, we propose a metric to quantify total causal effects for linear causal models and formulate the problem as a constrained optimization task, solved using a two-stage constrained optimization method. We evaluate our approach on real-world datasets and demonstrate that integrating interventional constraints not only improves model accuracy and ensures consistency with established findings, making models more explainable, but also facilitates the discovery of new causal relationships that would otherwise be costly to identify.', 'abstract_zh': '将干预因果知识和机制融入因果模型对于细化因果模型并改善下游任务（如设计新治疗方法）至关重要。本文引入了一种新颖的因果发现概念，称为干预约束，这与干预数据从根本上不同。干预数据需要对变量进行直接扰动，而干预约束则通过不等式约束形式编码高级别因果知识。例如，在Sachs数据集（Sachs et al. 2005）中，Akt已被证明被PIP3激活，这意味着PIP3对Akt具有正向因果效应。现有因果发现方法允许施加结构约束（例如，要求从PIP3到Akt的因果路径），但仍可能得出错误的因果结论，如“PIP3抑制Akt”。干预约束通过明确限制变量对之间的总因果效应，确保学习的模型遵守已知的因果影响。为形式化干预约束，我们提出了一种度量标准来量化线性因果模型中的总因果效应，并将问题表述为受约束的优化任务，使用两阶段受约束优化方法求解。我们在现实世界数据集上评估了我们的方法，并展示了整合干预约束不仅提高了模型的准确性并确保与现有发现的一致性，使模型更具可解释性，还促进了新因果关系的发现，这原本可能成本极高。', 'title_zh': '具有干预约束的线性因果发现'}
{'arxiv_id': 'arXiv:2510.26324', 'title': 'Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics', 'authors': 'Zhiyang Xun, Shivam Gupta, Eric Price', 'link': 'https://arxiv.org/abs/2510.26324', 'abstract': 'Given a noisy linear measurement $y = Ax + \\xi$ of a distribution $p(x)$, and a good approximation to the prior $p(x)$, when can we sample from the posterior $p(x \\mid y)$? Posterior sampling provides an accurate and fair framework for tasks such as inpainting, deblurring, and MRI reconstruction, and several heuristics attempt to approximate it. Unfortunately, approximate posterior sampling is computationally intractable in general.\nTo sidestep this hardness, we focus on (local or global) log-concave distributions $p(x)$. In this regime, Langevin dynamics yields posterior samples when the exact scores of $p(x)$ are available, but it is brittle to score--estimation error, requiring an MGF bound (sub-exponential error). By contrast, in the unconditional setting, diffusion models succeed with only an $L^2$ bound on the score error. We prove that combining diffusion models with an annealed variant of Langevin dynamics achieves conditional sampling in polynomial time using merely an $L^4$ bound on the score error.', 'abstract_zh': '给定一个分布$p(x)$的带噪线性测量$y = Ax + \\xi$及其良好的先验近似$p(x)$，在什么情况下可以从后验$p(x \\mid y)$中采样？后验采样为填图、去模糊和MRI重建等任务提供了一个准确且公平的框架，但有几个启发式方法试图近似它。然而，近似后验采样通常在一般情况下是计算上不可行的。\n\n为避免这一困难，我们关注局部或全局的对数凹分布$p(x)$。在这种情况下，当可以获取$p(x)$的确切分数时，Langevin动力学可以生成后验样本，但这种动力学对分数估计误差很敏感，需要一个MGF界（亚指数误差）。相反，在无条件情况下，扩散模型仅需分数误差的$L^2$界就能成功。我们证明，将扩散模型与Langevin动力学的退火变体结合可以仅使用分数误差的$L^4$界在多项式时间内实现有条件采样。', 'title_zh': '结合退火朗格文动力学与扩散模型的后验采样'}
{'arxiv_id': 'arXiv:2510.26303', 'title': 'Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime', 'authors': 'Beomhan Baek, Minhak Song, Chulhee Yun', 'link': 'https://arxiv.org/abs/2510.26303', 'abstract': 'Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet its theoretical understanding remains limited. Prior analyses show that Adam favors solutions aligned with $\\ell_\\infty$-geometry, but these results are restricted to the full-batch regime. In this work, we study the implicit bias of incremental Adam (using one sample per step) for logistic regression on linearly separable data, and we show that its bias can deviate from the full-batch behavior. To illustrate this, we construct a class of structured datasets where incremental Adam provably converges to the $\\ell_2$-max-margin classifier, in contrast to the $\\ell_\\infty$-max-margin bias of full-batch Adam. For general datasets, we develop a proxy algorithm that captures the limiting behavior of incremental Adam as $\\beta_2 \\to 1$ and we characterize its convergence direction via a data-dependent dual fixed-point formulation. Finally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges to the $\\ell_\\infty$-max-margin classifier for any batch size by taking $\\beta$ close enough to 1. Overall, our results highlight that the implicit bias of Adam crucially depends on both the batching scheme and the dataset, while Signum remains invariant.', 'abstract_zh': 'Adam [Kingma和Ba, 2015]是深度学习中的事实上的优化器，但对其理论理解仍然有限。以往的分析表明，Adam偏好与$\\ell_\\infty$几何对齐的解决方案，但这些结果仅限于全批量情况。在这项工作中，我们研究了增量Adam（每步使用一个样本）在线性可分数据上的逻辑回归中的隐式偏见，并证明其偏见可以偏离全批量行为。为了说明这一点，我们构造了一类结构化数据集，在这类数据集上，增量Adam可以证明收敛到$\\ell_2$-最大边际分类器，而全批量Adam偏好$\\ell_\\infty$-最大边际分类器。对于一般的数据集，我们开发了一个代理算法来捕捉增量Adam随$\\beta_2 \\to 1$变化的极限行为，并通过数据依赖的双重不动点形式来表征其收敛方向。最后，我们证明，与Adam不同，Signum [Bernstein等人, 2018]可以通过将$\\beta$调整得足够接近1，对任何批量大小都收敛到$\\ell_\\infty$-最大边际分类器。总体而言，我们的结果强调了Adam的隐式偏见不仅取决于批量方案，还取决于数据集，而Signum保持不变。', 'title_zh': '分离数据上每样本Adam的隐性偏差：超出全批量范式'}
{'arxiv_id': 'arXiv:2510.26298', 'title': 'Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games', 'authors': 'Jingran Zhang, Ning Li, Justin Cui', 'link': 'https://arxiv.org/abs/2510.26298', 'abstract': "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlas's web interaction capabilities using browser-based games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird, and this http URL. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at this https URL.", 'abstract_zh': 'OpenAI的ChatGPT Atlas引入了新的网页交互能力，使其能够分析网页、处理用户意图并在浏览器中直接执行鼠标和键盘输入。尽管其在信息检索任务上的能力已得到验证，但在动态、交互式环境中的性能尚未得到充分探索。本研究使用基于浏览器的游戏作为测试场景，评估Atlas的网页交互能力，包括Google的T-Rex Runner、数独、Flappy Bird等游戏。我们采用游戏内的性能评分作为定量指标，评估不同任务类型的表现。结果显示，Atlas在数独等逻辑推理任务中表现出色，完成谜题的速度远超人类基准，但在需要精确时间控制和运动控制的实时游戏中却表现不佳，往往无法克服初始障碍。这些发现表明，虽然Atlas展示了强大的分析处理能力，但在要求实时交互的动态网络环境中仍存在显著限制。我们的项目网站可访问此处：这个 https URL。', 'title_zh': '智能体能否 conquer 互联网？探究 ChatGPT 图Athlon 代理在网页游戏中的边界'}
{'arxiv_id': 'arXiv:2510.26278', 'title': 'Distributional Multi-objective Black-box Optimization for Diffusion-model Inference-time Multi-Target Generation', 'authors': 'Kim Yong Tan, Yueming Lyu, Ivor Tsang, Yew-Soon Ong', 'link': 'https://arxiv.org/abs/2510.26278', 'abstract': 'Diffusion models have been successful in learning complex data distributions. This capability has driven their application to high-dimensional multi-objective black-box optimization problem. Existing approaches often employ an external optimization loop, such as an evolutionary algorithm, to the diffusion model. However, these approaches treat the diffusion model as a black-box refiner, which overlooks the internal distribution transition of the diffusion generation process, limiting their efficiency. To address these challenges, we propose the Inference-time Multi-target Generation (IMG) algorithm, which optimizes the diffusion process at inference-time to generate samples that simultaneously satisfy multiple objectives. Specifically, our IMG performs weighted resampling during the diffusion generation process according to the expected aggregated multi-objective values. This weighted resampling strategy ensures the diffusion-generated samples are distributed according to our desired multi-target Boltzmann distribution. We further derive that the multi-target Boltzmann distribution has an interesting log-likelihood interpretation, where it is the optimal solution to the distributional multi-objective optimization problem. We implemented IMG for a multi-objective molecule generation task. Experiments show that IMG, requiring only a single generation pass, achieves a significantly higher hypervolume than baseline optimization algorithms that often require hundreds of diffusion generations. Notably, our algorithm can be viewed as an optimized diffusion process and can be integrated into existing methods to further improve their performance.', 'abstract_zh': '基于推断时多目标生成的扩散模型算法', 'title_zh': '分布式的多目标黑箱优化在扩散模型推理时的多目标生成'}
{'arxiv_id': 'arXiv:2510.26275', 'title': 'A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI', 'authors': 'Domenico Amalfitano, Andreas Metzger, Marco Autili, Tommaso Fulcini, Tobias Hey, Jan Keim, Patrizio Pelliccione, Vincenzo Scotti, Anne Koziolek, Raffaela Mirandola, Andreas Vogelsang', 'link': 'https://arxiv.org/abs/2510.26275', 'abstract': 'Generative AI (GenAI) is rapidly transforming software engineering (SE) practices, influencing how SE processes are executed, as well as how software systems are developed, operated, and evolved. This paper applies design science research to build a roadmap for GenAI-augmented SE. The process consists of three cycles that incrementally integrate multiple sources of evidence, including collaborative discussions from the FSE 2025 "Software Engineering 2030" workshop, rapid literature reviews, and external feedback sessions involving peers. McLuhan\'s tetrads were used as a conceptual instrument to systematically capture the transforming effects of GenAI on SE processes and software this http URL resulting roadmap identifies four fundamental forms of GenAI augmentation in SE and systematically characterizes their related research challenges and opportunities. These insights are then consolidated into a set of future research directions. By grounding the roadmap in a rigorous multi-cycle process and cross-validating it among independent author teams and peers, the study provides a transparent and reproducible foundation for analyzing how GenAI affects SE processes, methods and tools, and for framing future research within this rapidly evolving area. Based on these findings, the article finally makes ten predictions for SE in the year 2030.', 'abstract_zh': '生成式人工智能（GenAI）正在快速重塑软件工程（SE）实践，影响SE过程的执行方式，以及软件系统的开发、运行和演化。本文采用设计科学方法构建了一个GenAI增强SE的路线图。该过程包括三个循环，逐步整合多种证据来源，包括2025年FSE“软件工程2030”研讨会的协作讨论、快速文献综述以及来自同行的外部反馈会。利用麦卢汉的四象限作为概念工具，系统地捕获GenAI对SE过程和软件系统的转变效应。由此产生的路线图识别了四种基本形式的GenAI在SE中的增强，并系统地对其相关研究挑战和机遇进行了分类。这些见解随后被整合为一套未来研究方向。通过在一个严格的多循环过程基础上构建路线图，并在独立作者团队和同行之间进行交叉验证，研究为分析GenAI对SE过程、方法和工具的影响提供了透明且可重现的基础，并为将未来研究置于这一快速发展的领域内奠定框架。基于这些发现，文章最终提出了十个关于2030年软件工程的预测。', 'title_zh': '增强软件工程过程和软件产品生成式AI的研究路线图'}
{'arxiv_id': 'arXiv:2510.26230', 'title': 'MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines', 'authors': 'Minyi Peng, Darian Gunamardi, Ivan Tjuawinata, Kwok-Yan Lam', 'link': 'https://arxiv.org/abs/2510.26230', 'abstract': 'As a new and promising approach, existing machine unlearning (MU) works typically emphasize theoretical formulations or optimization objectives to achieve knowledge removal. However, when deployed in real-world scenarios, such solutions typically face scalability issues and have to address practical requirements such as full access to original datasets and model. In contrast to the existing approaches, we regard classification training as a sequential process where classes are learned sequentially, which we call \\emph{inductive approach}. Unlearning can then be done by reversing the last training sequence. This is implemented by appending a projection-redistribution layer in the end of the model. Such an approach does not require full access to the original dataset or the model, addressing the challenges of existing methods. This enables modular and model-agnostic deployment as an output filter into existing classification pipelines with minimal alterations. We conducted multiple experiments across multiple datasets including image (CIFAR-10/100 using CNN-based model) and tabular datasets (Covertype using tree-based model). Experiment results show consistently similar output to a fully retrained model with a high computational cost reduction. This demonstrates the applicability, scalability, and system compatibility of our solution while maintaining the performance of the output in a more practical setting.', 'abstract_zh': '作为一种新的有前途的方法，现有的机器遗忘（MU）工作通常侧重于理论建模或优化目标以实现知识删除。然而，在实际应用中，这些解决方案通常面临可扩展性问题，并且必须满足诸如原始数据集和模型的完全访问等实际需求。与现有方法不同，我们将分类训练视为一个顺序过程，即类别按顺序学习，我们称之为归纳方法。然后可以通过逆转最后一个训练序列来进行遗忘。这通过在模型末尾添加一个投影-重新分配层来实现。这种 approach 不需要访问原始数据集或模型，从而解决了现有方法的挑战。这使得我们的解决方案能够以最少的修改作为输出过滤器集成到现有的分类流水线中，实现模块化和模型无关的部署。我们在多个数据集上进行了多次实验，包括使用基于 CNN 的模型的图像数据集（CIFAR-10/100）和使用基于树的模型的表数据集（Covertype）。实验结果表明，输出与高计算成本下重新训练的模型具有相似性，这证明了我们解决方案的适用性、可扩展性和系统兼容性，同时保持了输出的性能在更实际的环境中。', 'title_zh': 'MPRU: 模块化投影-重分布遗忘作为分类流水线的输出过滤器'}
{'arxiv_id': 'arXiv:2510.26202', 'title': "What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data", 'authors': 'Rajiv Movva, Smitha Milli, Sewon Min, Emma Pierson', 'link': 'https://arxiv.org/abs/2510.26202', 'abstract': "Human feedback can alter language models in unpredictable and undesirable ways, as practitioners lack a clear understanding of what feedback data encodes. While prior work studies preferences over certain attributes (e.g., length or sycophancy), automatically extracting relevant features without pre-specifying hypotheses remains challenging. We introduce What's In My Human Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders. WIMHF characterizes both (1) the preferences a dataset is capable of measuring and (2) the preferences that the annotators actually express. Across 7 datasets, WIMHF identifies a small number of human-interpretable features that account for the majority of the preference prediction signal achieved by black-box models. These features reveal a wide diversity in what humans prefer, and the role of dataset-level context: for example, users on Reddit prefer informality and jokes, while annotators in HH-RLHF and PRISM disprefer them. WIMHF also surfaces potentially unsafe preferences, such as that LMArena users tend to vote against refusals, often in favor of toxic content. The learned features enable effective data curation: re-labeling the harmful examples in Arena yields large safety gains (+37%) with no cost to general performance. They also allow fine-grained personalization: on the Community Alignment dataset, we learn annotator-specific weights over subjective features that improve preference prediction. WIMHF provides a human-centered analysis method for practitioners to better understand and use preference data.", 'abstract_zh': '人类反馈可能以不可预测且不 desirable 的方式改变语言模型，因为实践者缺乏对反馈数据编码内容的清晰理解。尽管先前的工作研究了某些属性的偏好（例如长度或逢迎），但在无需预先假设的情况下自动提取相关特征仍然是一个挑战。我们引入了“What’s In My Human Feedback?”（WIMHF）方法，使用稀疏自编码器解释反馈数据。WIMHF 描述了（1）数据集能够测量的偏好以及（2）实际表达的偏好。在 7 个数据集中，WIMHF 识别出少量可由人类解释的特征，这些特征解释了黑盒模型实现的大多数偏好预测信号。这些特征揭示了人类偏好的广泛多样性，以及数据集级别上下文的作用：例如，Reddit 上的用户更喜欢非正式性和幽默，而 HH-RLHF 和 PRISM 的注释员则不偏好这些。WIMHF 还揭示了潜在的不安全偏好，例如 LMArena 用户倾向于反对拒绝，并经常有利有毒内容。学习到的特征能够有效进行数据管理：重新标记竞技场中的有害示例可带来显著的安全收益（+37%）且不会影响总体性能。它们还允许细粒度的个性化：在社区对齐数据集中，我们学习了注释器特定的权重，这些权重改善了偏好预测。WIMHF 为实践者提供了一种以人为中心的分析方法，帮助他们更好地理解并利用偏好数据。', 'title_zh': '我的人类反馈中包含了什么？学习可解释的偏好数据描述'}
{'arxiv_id': 'arXiv:2510.26188', 'title': 'Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients', 'authors': 'Avinash Kadimisetty, Arun Rajagopalan, Vijendra SK', 'link': 'https://arxiv.org/abs/2510.26188', 'abstract': 'Reducing preventable hospital readmissions is a national priority for payers, providers, and policymakers seeking to improve health care and lower costs. The rate of readmission is being used as a benchmark to determine the quality of healthcare provided by the hospitals. In thisproject, we have used machine learning techniques like Logistic Regression, Random Forest and Support Vector Machines to analyze the health claims data and identify demographic and medical factors that play a crucial role in predicting all-cause readmissions. As the health claims data is high dimensional, we have used Principal Component Analysis as a dimension reduction technique and used the results for building regression models. We compared and evaluated these models based on the Area Under Curve (AUC) metric. Random Forest model gave the highest performance followed by Logistic Regression and Support Vector Machine models. These models can be used to identify the crucial factors causing readmissions and help identify patients to focus on to reduce the chances of readmission, ultimately bringing down the cost and increasing the quality of healthcare provided to the patients.', 'abstract_zh': '降低可预防的医院再入院率是支付方、提供方和政策制定者提高医疗护理质量和降低成本的国家优先事项。再入院率被用作衡量医院提供医疗服务质量的基准。在本项目中，我们使用了逻辑回归、随机森林和支持向量机等机器学习技术来分析健康索赔数据，并识别预测所有原因再入院的关键人口统计和医学因素。由于健康索赔数据维度高，我们使用主成分分析作为降维技术，并使用其结果构建回归模型。我们根据曲线下面积（AUC）指标比较和评估了这些模型。随机森林模型的性能最高，其次是逻辑回归模型和支持向量机模型。这些模型可以用来识别导致再入院的关键因素，并帮助识别需要重点关注的患者，从而降低再入院率，最终降低医疗成本并提高医疗服务的质量。', 'title_zh': '基于住院患者医疗索赔数据预测所有原因再住院'}
{'arxiv_id': 'arXiv:2510.26186', 'title': 'ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts', 'authors': 'Jinho Choi, Hyesu Lim, Steffen Schneider, Jaegul Choo', 'link': 'https://arxiv.org/abs/2510.26186', 'abstract': 'Dataset bias, where data points are skewed to certain concepts, is ubiquitous in machine learning datasets. Yet, systematically identifying these biases is challenging without costly, fine-grained attribute annotations. We present ConceptScope, a scalable and automated framework for analyzing visual datasets by discovering and quantifying human-interpretable concepts using Sparse Autoencoders trained on representations from vision foundation models. ConceptScope categorizes concepts into target, context, and bias types based on their semantic relevance and statistical correlation to class labels, enabling class-level dataset characterization, bias identification, and robustness evaluation through concept-based subgrouping. We validate that ConceptScope captures a wide range of visual concepts, including objects, textures, backgrounds, facial attributes, emotions, and actions, through comparisons with annotated datasets. Furthermore, we show that concept activations produce spatial attributions that align with semantically meaningful image regions. ConceptScope reliably detects known biases (e.g., background bias in Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects in ImageNet), offering a practical tool for dataset auditing and model diagnostics.', 'abstract_zh': '基于概念范围的视觉数据集可扩展自动分析框架：识别和量化概念偏差', 'title_zh': 'ConceptScope: 通过解耦视觉概念表征数据集偏差'}
{'arxiv_id': 'arXiv:2510.26185', 'title': 'Accumulative SGD Influence Estimation for Data Attribution', 'authors': 'Yunxiao Shi, Shuo Yang, Yixin Su, Rui Zhang, Min Xu', 'link': 'https://arxiv.org/abs/2510.26185', 'abstract': 'Modern data-centric AI needs precise per-sample influence. Standard SGD-IE approximates leave-one-out effects by summing per-epoch surrogates and ignores cross-epoch compounding, which misranks critical examples. We propose ACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out perturbation across training and updates an accumulative influence state at each step. In smooth strongly convex settings it achieves geometric error contraction and, in smooth non-convex regimes, it tightens error bounds; larger mini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups, and MNIST under clean and corrupted data and both convex and non-convex training, ACC-SGD-IE yields more accurate influence estimates, especially over long epochs. For downstream data cleansing it more reliably flags noisy samples, producing models trained on ACC-SGD-IE cleaned data that outperform those cleaned with SGD-IE.', 'abstract_zh': '现代以数据为中心的AI需要精确的单样本影响分析。标准SGD-IE通过累加每个epoch的替代效应来近似leave-one-out效果，并忽略了跨epoch的影响累积，从而错误地排序关键样本。我们提出了一种轨迹感知估计器ACC-SGD-IE，它在训练过程中传播leave-one-out扰动，并在每一步更新累积影响状态。在光滑强凸设置中，它实现了几何误差收缩；在光滑非凸设置中，它紧化了误差界限；较大的小批量进一步减少了常数。实验结果表明，在成人数据集、20个新闻组数据集和MNIST数据集上，在干净和污染的数据以及凸和非凸训练下，ACC-SGD-IE提供了更准确的影响估计，尤其是在长时间训练中。对于下游数据清理，ACC-SGD-IE更可靠地标记出噪声样本，使用ACC-SGD-IE清理后的数据训练的模型优于使用SGD-IE清理后的数据训练的模型。', 'title_zh': '累积SGD影响估计用于数据归属'}
{'arxiv_id': 'arXiv:2510.26165', 'title': 'Learning to Manage Investment Portfolios beyond Simple Utility Functions', 'authors': 'Maarten P. Scholl, Mahmoud Mahfouz, Anisoara Calinescu, J. Doyne Farmer', 'link': 'https://arxiv.org/abs/2510.26165', 'abstract': 'While investment funds publicly disclose their objectives in broad terms, their managers optimize for complex combinations of competing goals that go beyond simple risk-return trade-offs. Traditional approaches attempt to model this through multi-objective utility functions, but face fundamental challenges in specification and parameterization. We propose a generative framework that learns latent representations of fund manager strategies without requiring explicit utility specification.\nOur approach directly models the conditional probability of a fund\'s portfolio weights, given stock characteristics, historical returns, previous weights, and a latent variable representing the fund\'s strategy. Unlike methods based on reinforcement learning or imitation learning, which require specified rewards or labeled expert objectives, our GAN-based architecture learns directly from the joint distribution of observed holdings and market data.\nWe validate our framework on a dataset of 1436 U.S. equity mutual funds. The learned representations successfully capture known investment styles, such as "growth" and "value," while also revealing implicit manager objectives. For instance, we find that while many funds exhibit characteristics of Markowitz-like optimization, they do so with heterogeneous realizations for turnover, concentration, and latent factors.\nTo analyze and interpret the end-to-end model, we develop a series of tests that explain the model, and we show that the benchmark\'s expert labeling are contained in our model\'s encoding in a linear interpretable way.\nOur framework provides a data-driven approach for characterizing investment strategies for applications in market simulation, strategy attribution, and regulatory oversight.', 'abstract_zh': '尽管投资基金公开披露其总体目标，但其管理者会优化超越简单风险-收益权衡的复杂目标组合。传统方法尝试通过多目标效用函数来建模这一过程，但在效用函数的设定和参数化方面面临根本性的挑战。我们提出了一种生成式框架，该框架无需明确指定效用函数即可学习基金管理者策略的潜在表示。\n\n我们的方法直接建模给定股票特征、历史回报、先前权重以及表示基金策略的潜在变量时，基金组合权重的条件概率。与基于强化学习或模仿学习的方法不同，后者需要指定奖励或标记专家目标，我们的基于生成对抗网络（GAN）的架构直接从观察持有量和市场数据的联合分布中学习。\n\n我们在包含1436只美国共同基金的数据集上验证了该框架。学习到的表示成功捕捉了已知的投资风格，例如“成长型”和“价值型”，同时揭示了隐式的管理者目标。例如，我们发现尽管许多基金表现出马柯维茨式的优化特征，但它们在周转率、集中度和潜在因素方面具有异质表现。\n\n为了分析和解释端到端模型，我们开发了一系列测试方法来解释模型，并展示了基准的专家标签以线性可解释的方式包含在我们的模型编码中。\n\n我们的框架提供了数据驱动的方法，用于在市场模拟、策略归因和监管审查等应用中 characterizing 投资策略。', 'title_zh': '超越简单效用函数的学习投资组合管理'}
{'arxiv_id': 'arXiv:2510.26159', 'title': 'Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series', 'authors': 'Emilio Mastriani, Alessandro Costa, Federico Incardona, Kevin Munari, Sebastiano Spinello', 'link': 'https://arxiv.org/abs/2510.26159', 'abstract': 'In this study, we investigate the effectiveness of advanced feature engineering and hybrid model architectures for anomaly detection in a multivariate industrial time series, focusing on a steam turbine system. We evaluate the impact of change point-derived statistical features, clustering-based substructure representations, and hybrid learning strategies on detection performance. Despite their theoretical appeal, these complex approaches consistently underperformed compared to a simple Random Forest + XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of 0.976, F1-score of 0.41, and 100% early detection within the defined time window. Our findings highlight that, in scenarios with highly imbalanced and temporally uncertain data, model simplicity combined with optimized segmentation can outperform more sophisticated architectures, offering greater robustness, interpretability, and operational utility.', 'abstract_zh': '本研究探讨了高级特征工程和混合模型架构在多变量工业时间序列异常检测中的有效性，重点研究了蒸汽涡轮系统。我们评估了变化点衍生统计特征、基于聚类的子结构表示以及混合学习策略对检测性能的影响。尽管这些复杂的方法在理论上具有吸引力，但在所有情况下，基于分割数据训练的简单随机森林+极端随机树 ensemble 的表现始终优于这些复杂方法。该 ensemble 达到了 0.976 的 AUC-ROC、0.41 的 F1 分数，并在定义的时间窗口内实现了 100% 的早期检测率。我们的研究结果表明，在高度不平衡且时间上具有不确定性数据的情景下，模型的简单性结合优化的分割可以优于更复杂的架构，提供更高的稳健性、可解释性和操作实用性。', 'title_zh': '复杂性上的分割评估：工业时序异常检测的集成与混合方法评价'}
{'arxiv_id': 'arXiv:2510.26157', 'title': 'Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment', 'authors': 'Hyuntae Park, Yeachan Kim, SangKeun Lee', 'link': 'https://arxiv.org/abs/2510.26157', 'abstract': 'Molecule and text representation learning has gained increasing interest due to its potential for enhancing the understanding of chemical information. However, existing models often struggle to capture subtle differences between molecules and their descriptions, as they lack the ability to learn fine-grained alignments between molecular substructures and chemical phrases. To address this limitation, we introduce MolBridge, a novel molecule-text learning framework based on substructure-aware alignments. Specifically, we augment the original molecule-description pairs with additional alignment signals derived from molecular substructures and chemical phrases. To effectively learn from these enriched alignments, MolBridge employs substructure-aware contrastive learning, coupled with a self-refinement mechanism that filters out noisy alignment signals. Experimental results show that MolBridge effectively captures fine-grained correspondences and outperforms state-of-the-art baselines on a wide range of molecular benchmarks, highlighting the significance of substructure-aware alignment in molecule-text learning.', 'abstract_zh': '分子和文本表示学习由于能够增强对化学信息的理解而日益受到关注。然而，现有模型往往难以捕捉分子及其描述之间的细微差异，因为它们缺乏学习分子亚结构和化学短语之间细粒度对齐的能力。为了解决这一局限性，我们介绍了MolBridge，一种基于亚结构感知对齐的新颖分子-文本学习框架。具体来说，MolBridge 通过增加源自分子亚结构和化学短语的额外对齐信号来扩充原始的分子-描述配对。为了有效从这些增强的对齐信号中学习，MolBridge 使用了亚结构感知对比学习，并结合了一种自我精炼机制来筛选掉噪音对齐信号。实验结果表明，MolBridge 能够有效捕捉细粒度对应关系，并在多种分子基准测试上优于最先进的基线，突出了亚结构感知对齐在分子-文本学习中的重要性。', 'title_zh': '通过子结构感知对齐弥合分子与文本描述之间的差距'}
{'arxiv_id': 'arXiv:2510.26099', 'title': 'SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth', 'authors': 'Nick Masi, Randall Balestriero', 'link': 'https://arxiv.org/abs/2510.26099', 'abstract': 'The dominant paradigm in machine learning is to assess model performance based on average loss across all samples in some test set. This amounts to averaging performance geospatially across the Earth in weather and climate settings, failing to account for the non-uniform distribution of human development and geography. We introduce Stratified Assessments of Forecasts over Earth (SAFE), a package for elucidating the stratified performance of a set of predictions made over Earth. SAFE integrates various data domains to stratify by different attributes associated with geospatial gridpoints: territory (usually country), global subregion, income, and landcover (land or water). This allows us to examine the performance of models for each individual stratum of the different attributes (e.g., the accuracy in every individual country). To demonstrate its importance, we utilize SAFE to benchmark a zoo of state-of-the-art AI-based weather prediction models, finding that they all exhibit disparities in forecasting skill across every attribute. We use this to seed a benchmark of model forecast fairness through stratification at different lead times for various climatic variables. By moving beyond globally-averaged metrics, we for the first time ask: where do models perform best or worst, and which models are most fair? To support further work in this direction, the SAFE package is open source and available at this https URL', 'abstract_zh': '地球上的分层预测评估：一种揭示地球预测集性能的包（Stratified Assessments of Forecasts over Earth: A Package for Elucidating the Stratified Performance of Predictions Made over Earth）', 'title_zh': 'SAFE：一种通过分层评估地球预报的新方法以进行AI气象评价'}
{'arxiv_id': 'arXiv:2510.26089', 'title': 'Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing', 'authors': 'Fazel Arasteh, Arian Haghparast, Manos Papagelis', 'link': 'https://arxiv.org/abs/2510.26089', 'abstract': 'Traffic congestion in urban road networks leads to longer trip times and higher emissions, especially during peak periods. While the Shortest Path First (SPF) algorithm is optimal for a single vehicle in a static network, it performs poorly in dynamic, multi-vehicle settings, often worsening congestion by routing all vehicles along identical paths. We address dynamic vehicle routing through a multi-agent reinforcement learning (MARL) framework for coordinated, network-aware fleet navigation. We first propose Adaptive Navigation (AN), a decentralized MARL model where each intersection agent provides routing guidance based on (i) local traffic and (ii) neighborhood state modeled using Graph Attention Networks (GAT). To improve scalability in large networks, we further propose Hierarchical Hub-based Adaptive Navigation (HHAN), an extension of AN that assigns agents only to key intersections (hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles micro-routing within each hub region. For hub coordination, HHAN adopts centralized training with decentralized execution (CTDE) under the Attentive Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions via attention. Hub agents use flow-aware state features that combine local congestion and predictive dynamics for proactive routing. Experiments on synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces average travel time versus SPF and learning baselines, maintaining 100% routing success. HHAN scales to networks with hundreds of intersections, achieving up to 15.9% improvement under heavy traffic. These findings highlight the potential of network-constrained MARL for scalable, coordinated, and congestion-aware routing in intelligent transportation systems.', 'abstract_zh': '动态交通网络中的交通拥堵导致旅行时间增加和排放量上升，特别是在高峰时段。虽然最短路径优先（SPF）算法在静态网络中对单一车辆是最优的，但在动态、多辆车的环境中表现不佳，往往会通过将所有车辆导向相同的路径来加剧拥堵。我们通过多代理 reinforcement 学习（MARL）框架解决动态车辆路由问题，以实现网络感知的车队协同导航。我们首先提出自适应导航（AN），这是一种去中心化的 MARL 模型，其中每个交叉口代理基于 （i）局部交通和 （ii）使用图注意网络（GAT）建模的邻域状态提供路径指导。为了在大网络中提高可扩展性，我们进一步提出分层中心节点自适应导航（HHAN），这是 AN 的扩展，仅将代理分配给关键交叉口（枢纽）。车辆在代理控制下枢纽到枢纽地行驶，而 SPF 负责每个枢纽区域内的微路径规划。对于枢纽协调，HHAN 在注意力增强 Q 混合（A-QMIX）框架下采用集中训练与去中心化执行（CTDE），通过注意力机制聚合异步车辆决策。枢纽代理使用流量感知状态特征，结合局部拥堵和预测动态实现主动导航。在合成网格和实际城市地图（多伦多、曼哈顿）上的实验显示，AN 在平均旅行时间方面优于 SPF 和学习基准，并保持100%的导航成功率。HHAN 可扩展到包含数百个交叉口的网络，在重交通情况下可实现高达 15.9% 的改进。这些发现突显了受网络约束的 MARL 在智能交通系统中实现可扩展、协同和拥堵感知路由的潜力。', 'title_zh': '网络约束的政策优化方法用于自适应多agent车辆路径规划'}
{'arxiv_id': 'arXiv:2510.26068', 'title': 'Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization', 'authors': 'Di Zhang', 'link': 'https://arxiv.org/abs/2510.26068', 'abstract': 'This paper proposes a novel paradigm for machine learning that moves beyond traditional parameter optimization. Unlike conventional approaches that search for optimal parameters within a fixed geometric space, our core idea is to treat the model itself as a malleable geometric entity. Specifically, we optimize the metric tensor field on a manifold with a predefined topology, thereby dynamically shaping the geometric structure of the model space. To achieve this, we construct a variational framework whose loss function carefully balances data fidelity against the intrinsic geometric complexity of the manifold. The former ensures the model effectively explains observed data, while the latter acts as a regularizer, penalizing overly curved or irregular geometries to encourage simpler models and prevent overfitting. To address the computational challenges of this infinite-dimensional optimization problem, we introduce a practical method based on discrete differential geometry: the continuous manifold is discretized into a triangular mesh, and the metric tensor is parameterized by edge lengths, enabling efficient optimization using automatic differentiation tools. Theoretical analysis reveals a profound analogy between our framework and the Einstein-Hilbert action in general relativity, providing an elegant physical interpretation for the concept of "data-driven geometry". We further argue that even with fixed topology, metric optimization offers significantly greater expressive power than models with fixed geometry. This work lays a solid foundation for constructing fully dynamic "meta-learners" capable of autonomously evolving their geometry and topology, and it points to broad application prospects in areas such as scientific model discovery and robust representation learning.', 'abstract_zh': '本文提出了一种超越传统参数优化的新范式，将模型本身视为可塑的几何实体。具体地，我们优化具有预定义拓扑结构的流形上的度量张量场，从而动态塑造模型空间的几何结构。为此，我们构建了一个变分框架，其损失函数平衡了数据保真度与流形内在几何复杂度。前者确保模型能有效解释观测数据，而后者作为正则化项，惩罚过于弯曲或不规则的几何结构，以鼓励更简单的模型并防止过拟合。为解决这一无限维优化问题的计算挑战，我们引入了一种基于离散微分几何的实用方法：将连续流形离散化为三角网，度量张量通过边长参数化，从而可以利用自动微分工具进行高效优化。理论分析揭示了我们框架与广义相对论中的爱因斯坦-希尔伯트作用之间的深刻类比，为“数据驱动几何”的概念提供了优雅的物理解释。我们进一步认为，即使拓扑固定，度量优化比固定几何结构的模型提供了显著更大的表达能力。本文为构建能够自主进化其几何结构和拓扑结构的完全动态“元学习器”奠定了坚实基础，并指出了其在科学模型发现和稳健表示学习等领域的广泛应用前景。', 'title_zh': '学习几何：通过度量优化构建自适应流形模型的框架'}
{'arxiv_id': 'arXiv:2510.26061', 'title': 'Data-driven Projection Generation for Efficiently Solving Heterogeneous Quadratic Programming Problems', 'authors': 'Tomoharu Iwata, Futoshi Futami', 'link': 'https://arxiv.org/abs/2510.26061', 'abstract': 'We propose a data-driven framework for efficiently solving quadratic programming (QP) problems by reducing the number of variables in high-dimensional QPs using instance-specific projection. A graph neural network-based model is designed to generate projections tailored to each QP instance, enabling us to produce high-quality solutions even for previously unseen problems. The model is trained on heterogeneous QPs to minimize the expected objective value evaluated on the projected solutions. This is formulated as a bilevel optimization problem; the inner optimization solves the QP under a given projection using a QP solver, while the outer optimization updates the model parameters. We develop an efficient algorithm to solve this bilevel optimization problem, which computes parameter gradients without backpropagating through the solver. We provide a theoretical analysis of the generalization ability of solving QPs with projection matrices generated by neural networks. Experimental results demonstrate that our method produces high-quality feasible solutions with reduced computation time, outperforming existing methods.', 'abstract_zh': '基于实例特定投影的数据驱动框架：通过减少高维QP中的变量数高效求解二次规划问题', 'title_zh': '基于数据驱动的投影生成方法用于高效求解异构二次规划问题'}
{'arxiv_id': 'arXiv:2510.26052', 'title': 'Dynamic VLM-Guided Negative Prompting for Diffusion Models', 'authors': 'Hoyeon Chang, Seungjin Kim, Yoonseok Choi', 'link': 'https://arxiv.org/abs/2510.26052', 'abstract': 'We propose a novel approach for dynamic negative prompting in diffusion models that leverages Vision-Language Models (VLMs) to adaptively generate negative prompts during the denoising process. Unlike traditional Negative Prompting methods that use fixed negative prompts, our method generates intermediate image predictions at specific denoising steps and queries a VLM to produce contextually appropriate negative prompts. We evaluate our approach on various benchmark datasets and demonstrate the trade-offs between negative guidance strength and text-image alignment.', 'abstract_zh': '我们提出了一种利用视觉语言模型在去噪过程中自适应生成动态负提示的新方法。我们在多个基准数据集上评估了该方法，并展示了负引导强度与文本图像对齐之间的权衡。', 'title_zh': "动态VLM引导的负 Lawyers' Prompting for 扩散模型"}
{'arxiv_id': 'arXiv:2510.26038', 'title': 'Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods', 'authors': 'Jiali Cheng, Chirag Agarwal, Hadi Amiri', 'link': 'https://arxiv.org/abs/2510.26038', 'abstract': "Knowledge distillation (KD) is an effective method for model compression and transferring knowledge between models. However, its effect on model's robustness against spurious correlations that degrade performance on out-of-distribution data remains underexplored. This study investigates the effect of knowledge distillation on the transferability of ``debiasing'' capabilities from teacher models to student models on natural language inference (NLI) and image classification tasks. Through extensive experiments, we illustrate several key findings: (i) overall the debiasing capability of a model is undermined post-KD; (ii) training a debiased model does not benefit from injecting teacher knowledge; (iii) although the overall robustness of a model may remain stable post-distillation, significant variations can occur across different types of biases; and (iv) we pin-point the internal attention pattern and circuit that causes the distinct behavior post-KD. Given the above findings, we propose three effective solutions to improve the distillability of debiasing methods: developing high quality data for augmentation, implementing iterative knowledge distillation, and initializing student models with weights obtained from teacher models. To the best of our knowledge, this is the first study on the effect of KD on debiasing and its interenal mechanism at scale. Our findings provide understandings on how KD works and how to design better debiasing methods.", 'abstract_zh': '知识蒸馏对模型去偏差能力迁移的影响研究', 'title_zh': '学生像教师一样去偏差吗？关于偏差缓解方法的可提炼性研究'}
{'arxiv_id': 'arXiv:2510.26032', 'title': 'Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings', 'authors': 'Felipe Larios, Mariana Borras-Osorio, Yuqi Wu, Ana Gabriela Claros, David Toro-Tobon, Esteban Cabezas, Ricardo Loor-Torres, Maria Mateo Chavez, Kerly Guevara Maldonado, Luis Vilatuna Andrango, Maria Lizarazo Jimenez, Ivan Mateo Alzamora, Misk Al Zahidy, Marcelo Montero, Ana Cristina Proano, Cristian Soto Jacome, Jungwei W. Fan, Oscar J. Ponce-Ponte, Megan E. Branda, Naykky Singh Ospina, Juan P. Brito', 'link': 'https://arxiv.org/abs/2510.26032', 'abstract': 'Importance Incidental thyroid findings (ITFs) are increasingly detected on imaging performed for non-thyroid indications. Their prevalence, features, and clinical consequences remain undefined. Objective To develop, validate, and deploy a natural language processing (NLP) pipeline to identify ITFs in radiology reports and assess their prevalence, features, and clinical outcomes. Design, Setting, and Participants Retrospective cohort of adults without prior thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline identified ITFs and extracted nodule characteristics from image reports from multiple modalities and body regions. Main Outcomes and Measures Prevalence of ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer diagnosis. Logistic regression identified demographic and imaging-related factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9% women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more likely in women, older adults, those with higher BMI, and when imaging was ordered by oncology or internal medicine. Compared with chest CT, ITFs were more likely via neck CT, PET, and nuclear medicine scans. Nodule characteristics were poorly documented, with size reported in 44% and other features in fewer than 15% (e.g. calcifications). Compared with patients without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis, biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were common and strongly associated with cascades leading to the detection of small, low-risk cancers. These findings underscore the role of ITFs in thyroid cancer overdiagnosis and the need for standardized reporting and more selective follow-up.', 'abstract_zh': '重要偶然性甲状腺发现（ITFs）在非甲状腺指征的影像检查中越来越常见。其流行率、特征和临床后果尚未明确定义。目的：开发、验证并部署自然语言处理（NLP）管道以识别放射学报告中的ITFs及其流行率、特征和临床结果。设计、地点和参与者：回顾性成人队列，无甲状腺疾病史，在Mayo Clinic站点从2017年7月1日至2023年9月30日期间接受甲状腺捕获影像检查。基于变压器的NLP管道识别ITFs并从多种影像模态和身体区域的影像报告中提取结节特征。主要结局和测量指标：ITFs的流行率、后续甲状腺超声检查、活检、甲状腺切除术和甲状腺癌诊断。逐步回归分析确定了与人口统计学和影像学相关因素。结果：在115,683名患者（平均年龄56.8岁[标准差17.2]；52.9%为女性）中，9,077名（7.8%）有ITF，其中92.9%为结节。ITFs在女性、老年人、BMI较高者及由肿瘤科或内科申请影像检查的人群中更为常见。与胸部CT相比，颈部CT、PET和放射性核素扫描更多发现ITFs。结节特征记录不佳，尺寸在44%的报告中记录，而其他特征在不到15%的报告中记录（例如，钙化）。与无ITFs的患者相比，有ITFs的患者甲状腺结节诊断、活检、甲状腺切除术和甲状腺癌诊断的可能性更高。大多数癌症为乳头状，且在通过ITFs检测时比未检测时更大。结论：ITFs常见，与检测微小、低风险癌症的级联过程密切相关。这些发现强调了ITFs在甲状腺癌过度诊断中的作用，并突出了标准化报告和更具选择性的随访的必要性。', 'title_zh': '人工智能赋能的放射学报告分析：偶发性甲状腺发现的流行病学和后果'}
{'arxiv_id': 'arXiv:2510.26017', 'title': 'Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning', 'authors': 'Bilal Hassan, Areg Karapetyan, Aaron Chung Hin Chow, Samer Madanat', 'link': 'https://arxiv.org/abs/2510.26017', 'abstract': 'Climate change and sea-level rise (SLR) pose escalating threats to coastal cities, intensifying the need for efficient and accurate methods to predict potential flood hazards. Traditional physics-based hydrodynamic simulators, although precise, are computationally expensive and impractical for city-scale coastal planning applications. Deep Learning (DL) techniques offer promising alternatives, however, they are often constrained by challenges such as data scarcity and high-dimensional output requirements. Leveraging a recently proposed vision-based, low-resource DL framework, we develop a novel, lightweight Convolutional Neural Network (CNN)-based model designed to predict coastal flooding under variable SLR projections and shoreline adaptation scenarios. Furthermore, we demonstrate the ability of the model to generalize across diverse geographical contexts by utilizing datasets from two distinct regions: Abu Dhabi and San Francisco. Our findings demonstrate that the proposed model significantly outperforms state-of-the-art methods, reducing the mean absolute error (MAE) in predicted flood depth maps on average by nearly 20%. These results highlight the potential of our approach to serve as a scalable and practical tool for coastal flood management, empowering decision-makers to develop effective mitigation strategies in response to the growing impacts of climate change. Project Page: this https URL', 'abstract_zh': '气候变化和海平面上升对沿海城市的威胁日益加剧，强化了对有效和精确的洪水灾害预测方法的需求。传统的物理学基础水动力模拟器虽然精确，但对于城市规模的沿海规划应用来说计算成本高且不实用。深度学习技术提供了有前景的替代方案，但经常会受到数据稀缺性和高维输出要求的限制。利用一种最近提出的基于视觉、低资源消耗的深度学习框架，我们开发了一种新的轻量级卷积神经网络（CNN）基预测模型，用于预测在不同的海平面上升和海岸线适应场景下的沿海洪水。此外，通过使用来自阿布扎比和旧金山两个不同地区的数据集，我们展示了该模型在不同地理背景下泛化的能力。我们的研究发现，所提议的模型显著优于现有最佳方法，平均降低了预测洪水深度图的绝对均方误差（MAE）近20%。这些结果强调了我们方法在沿海洪水管理中的潜在作用，能够为应对气候变化日益加剧的影响制定有效的缓解策略提供支持。项目页面：这个 https URL', 'title_zh': '沿海城市基于深度学习的气候适应性洪水预测'}
{'arxiv_id': 'arXiv:2510.26014', 'title': 'Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis', 'authors': 'Hyeonjun Lee, Hyungseob Shin, Gunhee Nam, Hyeonsoo Lee', 'link': 'https://arxiv.org/abs/2510.26014', 'abstract': 'Survival analysis is a task to model the time until an event of interest occurs, widely used in clinical and biomedical research. A key challenge is to model patient heterogeneity while also adapting risk predictions to both individual characteristics and temporal dynamics. We propose a dual mixture-of-experts (MoE) framework for discrete-time survival analysis. Our approach combines a feature-encoder MoE for subgroup-aware representation learning with a hazard MoE that leverages patient features and time embeddings to capture temporal dynamics. This dual-MoE design flexibly integrates with existing deep learning based survival pipelines. On METABRIC and GBSG breast cancer datasets, our method consistently improves performance, boosting the time-dependent C-index up to 0.04 on the test sets, and yields further gains when incorporated into the Consurv framework.', 'abstract_zh': '生存分析是建模感兴趣事件发生时间的任务，广泛应用于临床和生物医学研究。主要挑战在于同时建模患者异质性并根据个体特征和时间动态调整风险预测。我们提出了一种双重混合专家（MoE）框架用于离散时间生存分析。该方法结合了特征编码MoE进行亚组意识的表现学习，以及利用患者特征和时间嵌入的危险MoE来捕捉时间动态。这种双重MoE设计灵活地与现有的基于深度学习的生存分析管道集成。在METABRIC和GBSG乳腺癌数据集中，我们的方法一致提高了性能，在测试集上将时间依赖的C指数提升至0.04，并在与Consurv框架结合时进一步提高了性能。', 'title_zh': '离散时间生存分析的双混合专家框架'}
{'arxiv_id': 'arXiv:2510.26007', 'title': 'The Quest for Reliable Metrics of Responsible AI', 'authors': 'Theresia Veronika Rampisela, Maria Maistro, Tuukka Ruotsalo, Christina Lioma', 'link': 'https://arxiv.org/abs/2510.26007', 'abstract': 'The development of Artificial Intelligence (AI), including AI in Science (AIS), should be done following the principles of responsible AI. Progress in responsible AI is often quantified through evaluation metrics, yet there has been less work on assessing the robustness and reliability of the metrics themselves. We reflect on prior work that examines the robustness of fairness metrics for recommender systems as a type of AI application and summarise their key takeaways into a set of non-exhaustive guidelines for developing reliable metrics of responsible AI. Our guidelines apply to a broad spectrum of AI applications, including AIS.', 'abstract_zh': '负责人工智能原则下的人工智能（AI）及其科学应用（AIS）的发展应当通过负责任的AI评估指标来量化进展，但对这些评估指标自身的稳健性和可靠性进行评估的工作相对较少。我们借鉴了先前研究中关于推荐系统公平性指标稳健性的相关工作，并将其关键经验总结为一套非详尽的指导原则，以发展可靠的负责任AI评估指标。这些指导原则适用于广泛的人工智能应用，包括科学应用（AIS）。', 'title_zh': '负责任人工智能可靠指标的追求'}
{'arxiv_id': 'arXiv:2510.25976', 'title': 'Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer', 'authors': 'Roman Beliy, Amit Zalcher, Jonathan Kogman, Navve Wasserman, Michal Irani', 'link': 'https://arxiv.org/abs/2510.25976', 'abstract': 'Reconstructing images seen by people from their fMRI brain recordings provides a non-invasive window into the human brain. Despite recent progress enabled by diffusion models, current methods often lack faithfulness to the actual seen images. We present "Brain-IT", a brain-inspired approach that addresses this challenge through a Brain Interaction Transformer (BIT), allowing effective interactions between clusters of functionally-similar brain-voxels. These functional-clusters are shared by all subjects, serving as building blocks for integrating information both within and across brains. All model components are shared by all clusters & subjects, allowing efficient training with a limited amount of data. To guide the image reconstruction, BIT predicts two complementary localized patch-level image features: (i)high-level semantic features which steer the diffusion model toward the correct semantic content of the image; and (ii)low-level structural features which help to initialize the diffusion process with the correct coarse layout of the image. BIT\'s design enables direct flow of information from brain-voxel clusters to localized image features. Through these principles, our method achieves image reconstructions from fMRI that faithfully reconstruct the seen images, and surpass current SotA approaches both visually and by standard objective metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve results comparable to current methods trained on full 40-hour recordings.', 'abstract_zh': '从fMRI脑成像重建人的视觉图像：一种脑启发的方法', 'title_zh': 'Brain-IT：通过脑交互变换器从fMRI进行图像重建'}
{'arxiv_id': 'arXiv:2510.25954', 'title': 'Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi', 'authors': 'Lynn Metz, Rachel Haggard, Michael Moszczynski, Samer Asbah, Chris Mwase, Patricia Khomani, Tyler Smith, Hannah Cooper, Annie Mwale, Arbaaz Muslim, Gautam Prasad, Mimi Sun, Tomer Shekel, Joydeep Paul, Anna Carter, Shravya Shetty, Dylan Green', 'link': 'https://arxiv.org/abs/2510.25954', 'abstract': 'The reliability of routine health data in low and middle-income countries (LMICs) is often constrained by reporting delays and incomplete coverage, necessitating the exploration of novel data sources and analytics. Geospatial Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse spatial, temporal, and behavioral data into mathematical embeddings that can be efficiently used for downstream prediction tasks. This study evaluated the predictive performance of three GeoFM embedding sources - Google Population Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite imagery), and mobile phone call detail records (CDR) - for modeling 15 routine health programmatic outputs in Malawi, and compared their utility to traditional geospatial interpolation methods. We used XGBoost models on data from 552 health catchment areas (January 2021-May 2023), assessing performance with R2, and using an 80/20 training and test data split with 5-fold cross-validation used in training. While predictive performance was mixed, the embedding-based approaches improved upon baseline geostatistical methods in 13 of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three embedding sources produced the most robust predictions, achieving average 5-fold cross validated R2 values for indicators like population density (0.63), new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64, 0.68, and 0.55, respectively. Prediction was poor for prediction targets with low primary data availability, such as TB and malnutrition cases. These results demonstrate that GeoFM embeddings imbue a modest predictive improvement for select health and demographic outcomes in an LMIC context. We conclude that the integration of multiple GeoFM sources is an efficient and valuable tool for supplementing and strengthening constrained routine health information systems.', 'abstract_zh': '低收入和中等收入国家常规健康数据的可靠性经常受限于报告延迟和不完整覆盖，急需探索新型数据源和分析方法。空间基础模型（GeoFMs）通过合成多元空间、时间和行为数据为下游预测任务提供了有希望的途径。本研究评估了三种GeoFM嵌入源——谷歌人口动态基础模型（PDFM）、Google AlphaEarth（基于卫星影像）和移动电话通信详细记录（CDR）——在马拉维建模15项常规健康项目输出的预测性能，并将其与传统地理插值方法的实用性进行了比较。我们使用552个卫生保健区的数据（2021年1月-2023年5月），以R2作为性能评估指标，并采用80/20的训练和测试数据分割以及五折交叉验证进行训练。虽然预测性能参差不齐，但在15个测试指标中的13个（87%）中，基于嵌入的方法优于基线地理统计方法。三重GeoFM模型整合了三种嵌入源，产生了最稳健的预测结果，平均五折交叉验证R2值分别为人口密度（0.63）、新发HIV病例（0.57）和儿童疫苗接种（0.47），以及测试集的R2分别为0.64、0.68和0.55。对于主要数据获取难度大的结核病和营养不良病例的预测效果较差。这些结果表明，在低收入和中等收入国家的背景下，GeoFM嵌入为某些健康和人口统计结果提供了适度的预测改进。我们得出结论，多源GeoFM的整合是一种高效的工具，可补充并加强受限的常规健康信息系统。', 'title_zh': '地理空间基础模型数据在预测卫生设施项目产出的应用与验证——以马拉维为例'}
{'arxiv_id': 'arXiv:2510.25935', 'title': 'A Process Mining-Based System For The Analysis and Prediction of Software Development Workflows', 'authors': 'Antía Dorado, Iván Folgueira, Sofía Martín, Gonzalo Martín, Álvaro Porto, Alejandro Ramos, John Wallace', 'link': 'https://arxiv.org/abs/2510.25935', 'abstract': 'CodeSight is an end-to-end system designed to anticipate deadline compliance in software development workflows. It captures development and deployment data directly from GitHub, transforming it into process mining logs for detailed analysis. From these logs, the system generates metrics and dashboards that provide actionable insights into PR activity patterns and workflow efficiency. Building on this structured representation, CodeSight employs an LSTM model that predicts remaining PR resolution times based on sequential activity traces and static features, enabling early identification of potential deadline breaches. In tests, the system demonstrates high precision and F1 scores in predicting deadline compliance, illustrating the value of integrating process mining with machine learning for proactive software project management.', 'abstract_zh': 'CodeSight：一种用于软件开发工作流中截止日期合规性预测的端到端系统', 'title_zh': '基于过程挖掘的软件开发工作流分析与预测系统'}
{'arxiv_id': 'arXiv:2510.25929', 'title': 'Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion', 'authors': 'Ziyi Wang, Carmine Ventre, Maria Polukarov', 'link': 'https://arxiv.org/abs/2510.25929', 'abstract': 'Algorithmic collusion has emerged as a central question in AI: Will the interaction between different AI agents deployed in markets lead to collusion? More generally, understanding how emergent behavior, be it a cartel or market dominance from more advanced bots, affects the market overall is an important research question.\nWe propose a hierarchical multi-agent reinforcement learning framework to study algorithmic collusion in market making. The framework includes a self-interested market maker (Agent~A), which is trained in an uncertain environment shaped by an adversary, and three bottom-layer competitors: the self-interested Agent~B1 (whose objective is to maximize its own PnL), the competitive Agent~B2 (whose objective is to minimize the PnL of its opponent), and the hybrid Agent~B$^\\star$, which can modulate between the behavior of the other two. To analyze how these agents shape the behavior of each other and affect market outcomes, we propose interaction-level metrics that quantify behavioral asymmetry and system-level dynamics, while providing signals potentially indicative of emergent interaction patterns.\nExperimental results show that Agent~B2 secures dominant performance in a zero-sum setting against B1, aggressively capturing order flow while tightening average spreads, thus improving market execution efficiency. In contrast, Agent~B$^\\star$ exhibits a self-interested inclination when co-existing with other profit-seeking agents, securing dominant market share through adaptive quoting, yet exerting a milder adverse impact on the rewards of Agents~A and B1 compared to B2. These findings suggest that adaptive incentive control supports more sustainable strategic co-existence in heterogeneous agent environments and offers a structured lens for evaluating behavioral design in algorithmic trading systems.', 'abstract_zh': '算法共谋已成为AI领域的核心问题：不同的AI代理在市场中交互是否会引发共谋？更广泛地说，理解诸如卡特尔或由更先进机器人主导市场等 emergent 行为如何影响整体市场是一个重要的研究问题。', 'title_zh': '多智能体强化学习在市场制作中的应用：无串通的竞争'}
{'arxiv_id': 'arXiv:2510.25924', 'title': 'Transferring Causal Effects using Proxies', 'authors': 'Manuel Iglesias-Alonso, Felix Schur, Julius von Kügelgen, Jonas Peters', 'link': 'https://arxiv.org/abs/2510.25924', 'abstract': 'We consider the problem of estimating a causal effect in a multi-domain setting. The causal effect of interest is confounded by an unobserved confounder and can change between the different domains. We assume that we have access to a proxy of the hidden confounder and that all variables are discrete or categorical. We propose methodology to estimate the causal effect in the target domain, where we assume to observe only the proxy variable. Under these conditions, we prove identifiability (even when treatment and response variables are continuous). We introduce two estimation techniques, prove consistency, and derive confidence intervals. The theoretical results are supported by simulation studies and a real-world example studying the causal effect of website rankings on consumer choices.', 'abstract_zh': '多域环境中未观察到共因干扰下的因果效应估计方法', 'title_zh': '使用代理变量转移因果效果'}
{'arxiv_id': 'arXiv:2510.25819', 'title': 'Identity Management for Agentic AI: The new frontier of authorization, authentication, and security for an AI agent world', 'authors': 'Tobin South, Subramanya Nagabhushanaradhya, Ayesha Dissanayaka, Sarah Cecchetti, George Fletcher, Victor Lu, Aldo Pietropaolo, Dean H. Saxe, Jeff Lombardo, Abhishek Maligehalli Shivalingaiah, Stan Bounev, Alex Keisner, Andor Kesselman, Zack Proser, Ginny Fahs, Andrew Bunyea, Ben Moskowitz, Atul Tulshibagwale, Dazza Greenwood, Jiaxin Pei, Alex Pentland', 'link': 'https://arxiv.org/abs/2510.25819', 'abstract': "The rapid rise of AI agents presents urgent challenges in authentication, authorization, and identity management. Current agent-centric protocols (like MCP) highlight the demand for clarified best practices in authentication and authorization. Looking ahead, ambitions for highly autonomous agents raise complex long-term questions regarding scalable access control, agent-centric identities, AI workload differentiation, and delegated authority. This OpenID Foundation whitepaper is for stakeholders at the intersection of AI agents and access management. It outlines the resources already available for securing today's agents and presents a strategic agenda to address the foundational authentication, authorization, and identity problems pivotal for tomorrow's widespread autonomous systems.", 'abstract_zh': 'AI代理的迅速崛起对身份验证、授权和身份管理提出了迫切挑战。当前以代理为中心的协议（如MCP）突显了明确最佳实践的需求。展望未来，对高度自主代理的期望引发了关于可扩展访问控制、以代理为中心的身份、AI工作负载差异化和授权委托的复杂长期问题。本OpenID基金会白皮书面向AI代理和访问管理交叉领域的利益相关者，概述了当前可用的安全资源，并提出了一个战略议程，解决对未来广泛自主系统至关重要的身份验证、授权和身份基础问题。', 'title_zh': '代理人工智能的身份管理：AI代理世界中的授权、认证与安全新前沿'}
{'arxiv_id': 'arXiv:2510.25796', 'title': 'Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning', 'authors': 'Farnoosh Namdarpour, Joseph Y. J. Chow', 'link': 'https://arxiv.org/abs/2510.25796', 'abstract': 'Ride-pooling, also known as ride-sharing, shared ride-hailing, or microtransit, is a service wherein passengers share rides. This service can reduce costs for both passengers and operators and reduce congestion and environmental impacts. A key limitation, however, is its myopic decision-making, which overlooks long-term effects of dispatch decisions. To address this, we propose a simulation-informed reinforcement learning (RL) approach. While RL has been widely studied in the context of ride-hailing systems, its application in ride-pooling systems has been less explored. In this study, we extend the learning and planning framework of Xu et al. (2018) from ride-hailing to ride-pooling by embedding a ride-pooling simulation within the learning mechanism to enable non-myopic decision-making. In addition, we propose a complementary policy for rebalancing idle vehicles. By employing n-step temporal difference learning on simulated experiences, we derive spatiotemporal state values and subsequently evaluate the effectiveness of the non-myopic policy using NYC taxi request data. Results demonstrate that the non-myopic policy for matching can increase the service rate by up to 8.4% versus a myopic policy while reducing both in-vehicle and wait times for passengers. Furthermore, the proposed non-myopic policy can decrease fleet size by over 25% compared to a myopic policy, while maintaining the same level of performance, thereby offering significant cost savings for operators. Incorporating rebalancing operations into the proposed framework cuts wait time by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1% compared to using the framework for matching decisions alone at the cost of increased vehicle minutes traveled per passenger.', 'abstract_zh': '拼车，也称为共乘、共享出行或微交通，是一种乘客共乘的服务。这种服务可以减少乘客和运营者的成本，减轻交通拥堵和环境影响。然而，其关键限制在于短期内的调度决策忽视了长期效果。为解决这一问题，我们提出了一种基于仿真的强化学习方法。虽然强化学习在出行叫车系统中得到了广泛研究，但在拼车系统中的应用相对较少。在本研究中，我们将 Xu 等人（2018）为出行叫车系统开发的学习和规划框架扩展到拼车系统，通过在学习机制中嵌入拼车仿真以实现非短视决策。此外，我们还提出了一种为闲置车辆再平衡的补充策略。通过在模拟经验上应用 n 步时差学习，我们推导出时空状态值，并使用纽约市出租车请求数据评估非短视策略的有效性。结果显示，与短视策略相比，非短视匹配策略可以将服务率提高多达 8.4%，同时减少乘客的乘车时间和等待时间。此外，与短视策略相比，提出的非短视策略可以将车队规模减少超过 25%，同时保持相同的性能水平，从而为运营者提供显著的成本节省。将再平衡操作纳入所提出的框架可以将等待时间减少多达 27.3%，乘车时间减少 12.5%，服务率提高 15.1%，但成本是每乘客增加的行驶时间。', 'title_zh': '使用基于仿真增强学习的大规模按需拼车系统非短视配对与再平衡'}
{'arxiv_id': 'arXiv:2510.25791', 'title': 'The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?', 'authors': 'Zihan Pengmei, Costas Mavromatis, Zhengyuan Shen, Yunyi Zhang, Vassilis N. Ioannidis, Huzefa Rangwala', 'link': 'https://arxiv.org/abs/2510.25791', 'abstract': 'Chain-of-thought (CoT) supervision can substantially improve transformer performance, yet the mechanisms by which models learn to follow and benefit from CoT remain poorly understood. We investigate these learning dynamics through the lens of grokking by pretraining transformers on symbolic reasoning tasks with tunable algorithmic complexity and controllable data composition to study their generalization. Models were trained under two settings: (i) producing only final answers, and (ii) emitting explicit CoT traces before answering. Our results show that while CoT generally improves task performance, its benefits depend on task complexity. To quantify these effects, we model the accuracy of the logarithmic training steps with a three-parameter logistic curve, revealing how the learning speed and shape vary with task complexity, data distribution, and the presence of CoT supervision. We also uncover a transient trace unfaithfulness phase: early in training, models often produce correct answers while skipping or contradicting CoT steps, before later aligning their reasoning traces with answers. Empirically, we (1) demonstrate that CoT accelerates generalization but does not overcome tasks with higher algorithmic complexity, such as finding list intersections; (2) introduce a kinetic modeling framework for understanding transformer learning; (3) characterize trace faithfulness as a dynamic property that emerges over training; and (4) show CoT alters internal transformer computation mechanistically.', 'abstract_zh': 'Chain-of-Thought 监督可以显著提高变压器的表现，但模型学习遵循和受益于 Chain-of-Thought 的机制尚不完全理解。我们通过预训练变压器完成可调算法复杂性和可控数据组合的符号推理任务，从grokking的角度探究其泛化学习动力学。模型在两种设置下进行训练：（i）仅仅生成最终答案；（ii）在回答前发出明确的 Chain-of-Thought 跟踪。结果显示，尽管 Chain-of-Thought 通常提高任务性能，其益处取决于任务复杂度。为了量化这些效果，我们用三参数逻辑曲线建模对数训练步长的准确度，揭示学习速度和形状如何随任务复杂度、数据分布以及是否存在 Chain-of-Thought 监督而变化。我们还发现一个暂态跟踪不忠实相：在训练初期，模型通常生成正确答案的同时跳过或违背 Chain-of-Thought 步骤，之后再与答案对齐其推理跟踪。实验上，我们（1）证明 Chain-of-Thought 加速了泛化，但不能克服更高算法复杂度的任务，例如查找列表交集；（2）引入了一个动力学建模框架来理解变压器学习；（3）将跟踪忠实性表征为一种动态属性，随训练而浮现；（4）展示 Chain-of-Thought 从机制上改变了变压器的内部计算。', 'title_zh': '推理的动力学：链式思考如何塑造 Transformers 中的learning？'}
{'arxiv_id': 'arXiv:2510.25787', 'title': 'Unsupervised local learning based on voltage-dependent synaptic plasticity for resistive and ferroelectric synapses', 'authors': 'Nikhil Garg, Ismael Balafrej, Joao Henrique Quintino Palhares, Laura Bégon-Lours, Davide Florini, Donato Francesco Falcone, Tommaso Stecconi, Valeria Bragaglia, Bert Jan Offrein, Jean-Michel Portal, Damien Querlioz, Yann Beilliard, Dominique Drouin, Fabien Alibart', 'link': 'https://arxiv.org/abs/2510.25787', 'abstract': 'The deployment of AI on edge computing devices faces significant challenges related to energy consumption and functionality. These devices could greatly benefit from brain-inspired learning mechanisms, allowing for real-time adaptation while using low-power. In-memory computing with nanoscale resistive memories may play a crucial role in enabling the execution of AI workloads on these edge devices. In this study, we introduce voltage-dependent synaptic plasticity (VDSP) as an efficient approach for unsupervised and local learning in memristive synapses based on Hebbian principles. This method enables online learning without requiring complex pulse-shaping circuits typically necessary for spike-timing-dependent plasticity (STDP). We show how VDSP can be advantageously adapted to three types of memristive devices (TiO$_2$, HfO$_2$-based metal-oxide filamentary synapses, and HfZrO$_4$-based ferroelectric tunnel junctions (FTJ)) with disctinctive switching characteristics. System-level simulations of spiking neural networks incorporating these devices were conducted to validate unsupervised learning on MNIST-based pattern recognition tasks, achieving state-of-the-art performance. The results demonstrated over 83% accuracy across all devices using 200 neurons. Additionally, we assessed the impact of device variability, such as switching thresholds and ratios between high and low resistance state levels, and proposed mitigation strategies to enhance robustness.', 'abstract_zh': '基于忆阻器的电压依赖性突触可塑性在边缘计算设备上的无监督和局部学习研究', 'title_zh': '基于电压依赖突触可塑性的无监督局部学习方法及其在阻变和铁电突触中的应用'}
{'arxiv_id': 'arXiv:2510.25786', 'title': 'BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection', 'authors': 'Yaniv Nikankin, Dana Arad, Itay Itzhak, Anja Reusch, Adi Simhi, Gal Kesten-Pomeranz, Yonatan Belinkov', 'link': 'https://arxiv.org/abs/2510.25786', 'abstract': 'One of the main challenges in mechanistic interpretability is circuit discovery, determining which parts of a model perform a given task. We build on the Mechanistic Interpretability Benchmark (MIB) and propose three key improvements to circuit discovery. First, we use bootstrapping to identify edges with consistent attribution scores. Second, we introduce a simple ratio-based selection strategy to prioritize strong positive-scoring edges, balancing performance and faithfulness. Third, we replace the standard greedy selection with an integer linear programming formulation. Our methods yield more faithful circuits and outperform prior approaches across multiple MIB tasks and models. Our code is available at: this https URL.', 'abstract_zh': '机制可解释性中的主要挑战之一是电路发现，即确定模型中哪一部分执行特定任务。我们基于机制可解释性基准（MIB）并提出三项关键改进以促进电路发现。首先，我们使用自助法识别具有一致归因分数的边。第二，我们引入了一种简单的比率选择策略，以优先考虑高得分的边，平衡性能与忠实度。第三，我们用整数线性规划形式取代了标准的贪婪选择。我们的方法生成了更忠实的电路，并在多个MIB任务和模型中优于先前的方法。代码可在此处获取：this https URL。', 'title_zh': 'BlackboxNLP-2025 MIB 共享任务：通过更好的边选择提高电路忠实度'}
{'arxiv_id': 'arXiv:2510.25785', 'title': 'HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series', 'authors': 'Simon A. Lee, Cyrus Tanade, Hao Zhou, Juhyeon Lee, Megha Thukral, Minji Han, Rachel Choi, Md Sazzad Hissain Khan, Baiying Lu, Migyeong Gwak, Mehrab Bin Morshed, Viswam Nathan, Md Mahbubur Rahman, Li Zhu, Subramaniam Venkatraman, Sharanya Arcot Desai', 'link': 'https://arxiv.org/abs/2510.25785', 'abstract': 'Wearable sensors provide abundant physiological time series, yet the principles governing their predictive utility remain unclear. We hypothesize that temporal resolution is a fundamental axis of representation learning, with different clinical and behavioral outcomes relying on structure at distinct scales. To test this resolution hypothesis, we introduce HiMAE (Hierarchical Masked Autoencoder), a self supervised framework that combines masked autoencoding with a hierarchical convolutional encoder decoder. HiMAE produces multi resolution embeddings that enable systematic evaluation of which temporal scales carry predictive signal, transforming resolution from a hyperparameter into a probe for interpretability. Across classification, regression, and generative benchmarks, HiMAE consistently outperforms state of the art foundation models that collapse scale, while being orders of magnitude smaller. HiMAE is an efficient representation learner compact enough to run entirely on watch, achieving sub millisecond inference on smartwatch class CPUs for true edge inference. Together, these contributions position HiMAE as both an efficient self supervised learning method and a discovery tool for scale sensitive structure in wearable health.', 'abstract_zh': '可穿戴传感器提供了丰富的生理时间序列数据，但其预测效用的基本原理仍不清楚。我们假设时间分辨率是表示学习的基本轴线，不同的临床和行为结果依赖于不同尺度的结构。为了检验这一分辨率假设，我们引入了HiMAE（分层掩盖自编码器），这是一种结合了掩蔽自编码和分层卷积编码解码器的自我监督框架。HiMAE生成多分辨率嵌入，使系统性评估哪些时间尺度携带预测信号成为可能，将分辨率从超参数转变为解释性的探针。在分类、回归和生成基准测试中，HiMAE在不牺牲性能的情况下，模型大小比压缩尺度的状态-of-the-art基础模型小了数量级。HiMAE是一种高效的表示学习者，其大小足够小，可以在手表上完全运行，实现了智能手表级别CPU上的亚毫秒级推理，进行真正的边缘推理。这些贡献使HiMAE既是一种高效的自我监督学习方法，也是一种探索可穿戴健康中尺度敏感结构的发现工具。', 'title_zh': 'HiMAE：层次遮蔽自编码器发现可穿戴时间序列的分辨率特定结构'}
{'arxiv_id': 'arXiv:2510.25783', 'title': 'LASTIST: LArge-Scale Target-Independent STance dataset', 'authors': 'DongJae Kim, Yaejin Lee, Minsu Park, Eunil Park', 'link': 'https://arxiv.org/abs/2510.25783', 'abstract': "Stance detection has emerged as an area of research in the field of artificial intelligence. However, most research is currently centered on the target-dependent stance detection task, which is based on a person's stance in favor of or against a specific target. Furthermore, most benchmark datasets are based on English, making it difficult to develop models in low-resource languages such as Korean, especially for an emerging field such as stance detection. This study proposes the LArge-Scale Target-Independent STance (LASTIST) dataset to fill this research gap. Collected from the press releases of both parties on Korean political parties, the LASTIST dataset uses 563,299 labeled Korean sentences. We provide a detailed description of how we collected and constructed the dataset and trained state-of-the-art deep learning and stance detection models. Our LASTIST dataset is designed for various tasks in stance detection, including target-independent stance detection and diachronic evolution stance detection. We deploy our dataset on this https URL.", 'abstract_zh': '针对大规模无目标依赖立场检测的数据集（LASTIST）', 'title_zh': 'LASTIST: 大规模目标无关立场数据集'}
{'arxiv_id': 'arXiv:2510.25781', 'title': "A Practitioner's Guide to Kolmogorov-Arnold Networks", 'authors': 'Amir Noorizadegan, Sifan Wang, Leevan Ling', 'link': 'https://arxiv.org/abs/2510.25781', 'abstract': 'Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising alternative to traditional Multilayer Perceptrons (MLPs), inspired by the Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed activation functions on nodes, KANs employ learnable univariate basis functions on edges, offering enhanced expressivity and interpretability. This review provides a systematic and comprehensive overview of the rapidly expanding KAN landscape, moving beyond simple performance comparisons to offer a structured synthesis of theoretical foundations, architectural variants, and practical implementation strategies. By collecting and categorizing a vast array of open-source implementations, we map the vibrant ecosystem supporting KAN development. We begin by bridging the conceptual gap between KANs and MLPs, establishing their formal equivalence and highlighting the superior parameter efficiency of the KAN formulation. A central theme of our review is the critical role of the basis function; we survey a wide array of choices, including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions, Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in terms of smoothness, locality, and computational cost. We then categorize recent advancements into a clear roadmap, covering techniques for improving accuracy, efficiency, and regularization. Key topics include physics-informed loss design, adaptive sampling, domain decomposition, hybrid architectures, and specialized methods for handling discontinuities. Finally, we provide a practical "Choose-Your-KAN" guide to help practitioners select appropriate architectures, and we conclude by identifying current research gaps. The associated GitHub repository this https URL complements this paper and serves as a structured reference for ongoing KAN research.', 'abstract_zh': 'Kolmogorov-Arnold网络（KANs）：一种基于柯尔莫戈罗夫-阿诺尔德表示定理的有前途的多层感知机替代方案', 'title_zh': 'Kolmogorov-Arnold网络使用指南'}
{'arxiv_id': 'arXiv:2510.25779', 'title': 'Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets', 'authors': 'Gagan Bansal, Wenyue Hua, Zezhou Huang, Adam Fourney, Amanda Swearngin, Will Epperson, Tyler Payne, Jake M. Hofman, Brendan Lucier, Chinmay Singh, Markus Mobius, Akshay Nambi, Archana Yadav, Kevin Gao, David M. Rothschild, Aleksandrs Slivkins, Daniel G. Goldstein, Hussein Mozannar, Nicole Immorlica, Maya Murad, Matthew Vogel, Subbarao Kambhampati, Eric Horvitz, Saleema Amershi', 'link': 'https://arxiv.org/abs/2510.25779', 'abstract': 'As LLM agents advance, they are increasingly mediating economic decisions, ranging from product discovery to transactions, on behalf of users. Such applications promise benefits but also raise many questions about agent accountability and value for users. Addressing these questions requires understanding how agents behave in realistic market conditions. However, previous research has largely evaluated agents in constrained settings, such as single-task marketplaces (e.g., negotiation) or structured two-agent interactions. Real-world markets are fundamentally different: they require agents to handle diverse economic activities and coordinate within large, dynamic ecosystems where multiple agents with opaque behaviors may engage in open-ended dialogues. To bridge this gap, we investigate two-sided agentic marketplaces where Assistant agents represent consumers and Service agents represent competing businesses. To study these interactions safely, we develop Magentic-Marketplace-- a simulated environment where Assistants and Services can operate. This environment enables us to study key market dynamics: the utility agents achieve, behavioral biases, vulnerability to manipulation, and how search mechanisms shape market outcomes. Our experiments show that frontier models can approach optimal welfare-- but only under ideal search conditions. Performance degrades sharply with scale, and all models exhibit severe first-proposal bias, creating 10-30x advantages for response speed over quality. These findings reveal how behaviors emerge across market conditions, informing the design of fair and efficient agentic marketplaces.', 'abstract_zh': '随着大规模语言模型代理的发展，它们在产品发现到交易等一系列经济决策中代表用户进行调解。这类应用带来了诸多好处，但也引发了关于代理问责制和用户价值等方面的众多问题。解决这些问题需要理解代理在现实市场条件下的行为。然而，以往的研究大多是通过受限环境来评估代理，例如单一任务市场（例如谈判）或结构化的两代理交互。现实世界市场本质上是不同的：它们要求代理处理多样化的经济活动，并在包含多个具有不透明行为的代理的大型动态生态系统中进行协调。为了弥合这一差距，我们研究了代理双边市场，其中助手代理代表消费者，服务代理代表竞争企业。为安全地研究这些交互，我们开发了Magnetic-Marketplace——一个模拟环境，允许助手和服务代理运营。该环境使我们能够研究关键市场动态：代理实现的效用、行为偏差、易受操纵性以及搜索机制如何影响市场结果。我们的实验表明，前沿模型在理想搜索条件下可以接近最优福利——但随着规模的扩大，性能急剧下降，所有模型都表现出严重的初次提案偏差，这使响应速度比质量高出10-30倍。这些发现揭示了在不同市场条件下行为是如何演变的，从而有助于设计公平高效的代理市场。', 'title_zh': '磁性市场：研究代理市场的开源环境'}
