{'arxiv_id': 'arXiv:2510.26023', 'title': 'Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization', 'authors': 'Zhipeng Bao, Qianwen Li', 'link': 'https://arxiv.org/abs/2510.26023', 'abstract': "Despite significant advancements in recent decades, autonomous vehicles (AVs) continue to face challenges in navigating certain traffic scenarios where human drivers excel. In such situations, AVs often become immobilized, disrupting overall traffic flow. Current recovery solutions, such as remote intervention (which is costly and inefficient) and manual takeover (which excludes non-drivers and limits AV accessibility), are inadequate. This paper introduces StuckSolver, a novel Large Language Model (LLM) driven recovery framework that enables AVs to resolve immobilization scenarios through self-reasoning and/or passenger-guided decision-making. StuckSolver is designed as a plug-in add-on module that operates on top of the AV's existing perception-planning-control stack, requiring no modification to its internal architecture. Instead, it interfaces with standard sensor data streams to detect immobilization states, interpret environmental context, and generate high-level recovery commands that can be executed by the AV's native planner. We evaluate StuckSolver on the Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results show that StuckSolver achieves near-state-of-the-art performance through autonomous self-reasoning alone and exhibits further improvements when passenger guidance is incorporated.", 'abstract_zh': '尽管在近几十年取得了显著进展，自动驾驶车辆（AVs）在某些交通场景中仍面临挑战，这些场景是人类驾驶员所擅长的。在这种情况下，AVs经常变得无法行动，从而破坏整体交通流动。当前的恢复解决方案，如远程干预（成本高且效率低）和手动接管（排除非驾驶员且限制AV的可达性），都不足。本文提出了一种名为StuckSolver的新型大型语言模型（LLM）驱动恢复框架，使AV能够通过自我推理和/or乘客引导决策来解决无法行动的场景。StuckSolver设计为插件附加模块，运行在AV现有感知-规划-控制栈之上，不需要修改其内部架构。相反，它与标准传感器数据流接口，以检测无法行动状态，解释环境上下文，并生成可由AV内置规划器执行的高层恢复命令。我们在Bench2Drive基准测试和自定义设计的不确定性场景中评估了StuckSolver。结果显示，StuckSolver仅通过自主自我推理即可实现接近最先进的性能，并且在结合乘客引导时表现出进一步的改进。', 'title_zh': '大型语言模型辅助的自动驾驶车辆脱困技术'}
{'arxiv_id': 'arXiv:2510.26784', 'title': 'LLMs Process Lists With General Filter Heads', 'authors': 'Arnab Sen Sharma, Giordano Rogers, Natalie Shapira, David Bau', 'link': 'https://arxiv.org/abs/2510.26784', 'abstract': 'We investigate the mechanisms underlying a range of list-processing tasks in LLMs, and we find that LLMs have learned to encode a compact, causal representation of a general filtering operation that mirrors the generic "filter" function of functional programming. Using causal mediation analysis on a diverse set of list-processing tasks, we find that a small number of attention heads, which we dub filter heads, encode a compact representation of the filtering predicate in their query states at certain tokens. We demonstrate that this predicate representation is general and portable: it can be extracted and reapplied to execute the same filtering operation on different collections, presented in different formats, languages, or even in tasks. However, we also identify situations where transformer LMs can exploit a different strategy for filtering: eagerly evaluating if an item satisfies the predicate and storing this intermediate result as a flag directly in the item representations. Our results reveal that transformer LMs can develop human-interpretable implementations of abstract computational operations that generalize in ways that are surprisingly similar to strategies used in traditional functional programming patterns.', 'abstract_zh': '我们调查了大规模语言模型中一系列列表处理任务的机制，并发现这些模型已经学习到了一种紧凑的、具有因果性的过滤操作表示，这种表示类似于函数编程中的通用“过滤”函数。通过使用因果中介分析对一系列不同的列表处理任务进行分析，我们发现少量的注意力头（我们称之为过滤头），在某些标记的查询状态中，编码了过滤谓词的紧凑表示。我们证明了这种谓词表示是通用且可移植的：它可以被提取并应用于在不同的集合、不同格式、不同语言甚至不同任务中执行相同的过滤操作。然而，我们还发现变长模型在某些情况下会利用不同的过滤策略：提前评估项目是否满足谓词，并直接将这个中间结果作为标志存储在项目表示中。我们的结果揭示了变长模型可以发展出人类可解释的抽象计算操作的实现方式，并且这些操作的泛化方式出乎意料地类似于传统函数编程模式所使用的方法。', 'title_zh': 'LLMs处理列表的通用过滤头'}
{'arxiv_id': 'arXiv:2510.26732', 'title': 'Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models', 'authors': 'J. de Curtò, I. de Zarzà, Pablo García, Jordi Cabot', 'link': 'https://arxiv.org/abs/2510.26732', 'abstract': 'This paper presents a comprehensive cross-platform evaluation of reasoning capabilities in contemporary foundation models, establishing an infrastructure-agnostic benchmark across three computational paradigms: HPC supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (a node with eight H200 GPUs).\nWe evaluate 15 foundation models across 79 problems spanning eight academic domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics, Calculus, and Optimization) through three experimental phases: (1) Baseline establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b, Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing methodology and reference performance; (2) Infrastructure validation: The 19-problem benchmark repeated on university cluster (seven models including Falcon-Mamba state-space architecture) and Nebius AI Studio (nine state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3 30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic reproducibility; (3) Extended evaluation: Full 79-problem assessment on both university cluster and Nebius platforms, probing generalization at scale across architectural diversity.\nThe findings challenge conventional scaling assumptions, establish training data quality as more critical than model size, and provide actionable guidelines for model selection across educational, production, and research contexts. The tri-infrastructure methodology and 79-problem benchmark enable longitudinal tracking of reasoning capabilities as foundation models evolve.', 'abstract_zh': '本文提出了一种跨平台评估当代基础模型推理能力的全面方法，建立了在三种计算范式下的基础设施无关基准：高性能计算超级计算机（MareNostrum 5）、云平台（Nebius AI Studio）和大学集群（包含8个H200 GPU的节点）。\n\n我们通过三个实验阶段评估了15个基础模型在79个问题上的表现，涵盖八个学术领域（物理、数学、化学、经济学、生物学、统计学、微积分和优化）：（1）基线建立：使用MareNostrum 5评估六种模型（Mixtral-8x7B、Phi-3、LLaMA 3.1-8B、Gemma-2-9b、Mistral-7B、OLMo-7B），建立方法和参考性能；（2）基础设施验证：在大学集群和Nebius AI Studio上重复19个问题基准测试（包括Falcon-Mamba状态空间架构的七种模型和九种顶尖模型：Hermes-4 70B/405B、LLaMA 3.1-405B/3.3-70B、Qwen3 30B/235B、DeepSeek-R1、GPT-OSS 20B/120B），验证基础设施无关的可再现性；（3）扩展评估：在大学集群和Nebius平台对全部79个问题进行评估，探索广泛的架构多样性下的泛化能力。\n\n研究结果挑战了传统的扩展假设，确立了训练数据质量比模型规模更为关键，并提供了适用于教育、生产和研究环境的模型选择指南。三平台方法和79个问题基准测试可实现基础模型随时间推进的纵向追踪能力。', 'title_zh': '跨平台评估基础模型的推理能力'}
{'arxiv_id': 'arXiv:2510.26721', 'title': 'Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis', 'authors': 'Xinhan Zheng, Huyu Wu, Xueting Wang, Haiyun Jiang', 'link': 'https://arxiv.org/abs/2510.26721', 'abstract': "Multimodal large language models (MLLMs) exhibit a pronounced preference for textual inputs when processing vision-language data, limiting their ability to reason effectively from visual evidence. Unlike prior studies that attribute this text bias to external factors such as data imbalance or instruction tuning, we propose that the bias originates from the model's internal architecture. Specifically, we hypothesize that visual key vectors (Visual Keys) are out-of-distribution (OOD) relative to the text key space learned during language-only pretraining. Consequently, these visual keys receive systematically lower similarity scores during attention computation, leading to their under-utilization in the context representation. To validate this hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their distributional structures using qualitative (t-SNE) and quantitative (Jensen-Shannon divergence) methods. The results provide direct evidence that visual and textual keys occupy markedly distinct subspaces within the attention space. The inter-modal divergence is statistically significant, exceeding intra-modal variation by several orders of magnitude. These findings reveal that text bias arises from an intrinsic misalignment within the attention key space rather than solely from external data factors.", 'abstract_zh': '多模态大型语言模型（MLLMs）在处理视觉语言数据时表现出对文本输入的明显偏好，限制了其从视觉证据中有效推理的能力。不同于以往将这种文本偏见归因于外部因素如数据不平衡或指令微调的研究，我们认为这种偏见源自模型内部架构。具体来说，我们假设视觉关键向量（Visual Keys）在仅语言预训练过程中学习到的文本关键空间中是离群的。因此，这些视觉关键向量在注意力计算中获得系统地较低的相似度分数，导致它们在上下文表示中的利用率较低。为了验证这一假设，我们从LLaVA和Qwen2.5-VL中提取关键向量，并使用定性（t-SNE）和定量（Jensen-Shannon距离）方法分析其分布结构。结果直接证明了视觉和文本关键向量在注意力空间中占据明显不同的子空间。跨模态的差异在统计上是显著的，远超过同模态变异性。这些发现揭示了文本偏见源于注意力关键空间中的内在不匹配，而不仅仅是外部数据因素。', 'title_zh': '通过注意键空间分析揭示多模态大型语言模型中的固有文本偏见'}
{'arxiv_id': 'arXiv:2510.26702', 'title': 'Delegated Authorization for Agents Constrained to Semantic Task-to-Scope Matching', 'authors': 'Majed El Helou, Chiara Troiani, Benjamin Ryder, Jean Diaconu, Hervé Muyal, Marcelo Yannuzzi', 'link': 'https://arxiv.org/abs/2510.26702', 'abstract': "Authorizing Large Language Model driven agents to dynamically invoke tools and access protected resources introduces significant risks, since current methods for delegating authorization grant overly broad permissions and give access to tools allowing agents to operate beyond the intended task scope. We introduce and assess a delegated authorization model enabling authorization servers to semantically inspect access requests to protected resources, and issue access tokens constrained to the minimal set of scopes necessary for the agents' assigned tasks. Given the unavailability of datasets centered on delegated authorization flows, particularly including both semantically appropriate and inappropriate scope requests for a given task, we introduce ASTRA, a dataset and data generation pipeline for benchmarking semantic matching between tasks and scopes. Our experiments show both the potential and current limitations of model-based matching, particularly as the number of scopes needed for task completion increases. Our results highlight the need for further research into semantic matching techniques enabling intent-aware authorization for multi-agent and tool-augmented applications, including fine-grained control, such as Task-Based Access Control (TBAC).", 'abstract_zh': '授权大型语言模型驱动的代理动态调用工具和访问受保护资源引入了重大风险，因为当前的方法在委派授权时授予了过于广泛的权限，并提供了允许代理超出预定任务范围操作的工具访问。我们提出并评估了一种委派授权模型，使授权服务器能够语义检查对受保护资源的访问请求，并发放仅限于代理分配任务所需最小权限集的访问令牌。由于缺乏专注于委派授权流程的数据集，特别是包括针对给定任务的语义合适和不合适的作用范围请求的数据集，我们介绍了ASTRA数据集和数据生成管道，用于基准测试任务和作用范围之间的语义匹配。我们的实验结果显示了基于模型的匹配的潜力及其当前限制，特别是在所需作用范围的数量增加时。我们的结果强调了需要进一步研究语义匹配技术的重要性，这些技术能够为多代理和工具增强应用提供意图感知的授权，包括细粒度控制，例如基于任务的访问控制（TBAC）。', 'title_zh': '代理的委托授权，受限于语义任务到范围匹配'}
{'arxiv_id': 'arXiv:2510.26606', 'title': 'Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives', 'authors': 'Kentaro Ozeki, Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, Mitsuhiro Okada', 'link': 'https://arxiv.org/abs/2510.26606', 'abstract': "Normative reasoning is a type of reasoning that involves normative or deontic modality, such as obligation and permission. While large language models (LLMs) have demonstrated remarkable performance across various reasoning tasks, their ability to handle normative reasoning remains underexplored. In this paper, we systematically evaluate LLMs' reasoning capabilities in the normative domain from both logical and modal perspectives. Specifically, to assess how well LLMs reason with normative modals, we make a comparison between their reasoning with normative modals and their reasoning with epistemic modals, which share a common formal structure. To this end, we introduce a new dataset covering a wide range of formal patterns of reasoning in both normative and epistemic domains, while also incorporating non-formal cognitive factors that influence human reasoning. Our results indicate that, although LLMs generally adhere to valid reasoning patterns, they exhibit notable inconsistencies in specific types of normative reasoning and display cognitive biases similar to those observed in psychological studies of human reasoning. These findings highlight challenges in achieving logical consistency in LLMs' normative reasoning and provide insights for enhancing their reliability. All data and code are released publicly at this https URL.", 'abstract_zh': '规范推理是一种涉及义务和许可等规范或义理模态的推理类型。尽管大规模语言模型（LLMs）在各种推理任务中展现了卓越的表现，但它们处理规范推理的能力仍鲜有研究。本文从逻辑和模态 perspective 出发，系统评估 LLMs 在规范领域内的推理能力。为此，我们通过将 LLMs 的规范模态推理与表征模态进行比较，来评估它们的规范模态推理能力，这两者共享类似的正式结构。为此，我们引入了一个新的数据集，涵盖了规范和表征领域广泛形式推理模式的同时，也融入了影响人类推理的非形式认知因素。我们的结果显示，尽管 LLMs 通常遵循有效的推理模式，但在特定类型的规范推理中表现出明显的不一致性，并展示了与人类推理心理学研究中观察到的认知偏差相似的现象。这些发现突出了在 LLMs 的规范推理中实现逻辑一致性的挑战，并为增强其可靠性提供了见解。所有数据和代码已在 https://github.com/alibaba/Qwen-RLM 公开发布。', 'title_zh': '大型语言模型中的规范推理：从逻辑和模态视角的比较基准'}
{'arxiv_id': 'arXiv:2510.26603', 'title': 'Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling', 'authors': 'Reda El Makroum, Sebastian Zwickl-Bernhard, Lukas Kranzl', 'link': 'https://arxiv.org/abs/2510.26603', 'abstract': "The electricity sector transition requires substantial increases in residential demand response capacity, yet Home Energy Management Systems (HEMS) adoption remains limited by user interaction barriers requiring translation of everyday preferences into technical parameters. While large language models have been applied to energy systems as code generators and parameter extractors, no existing implementation deploys LLMs as autonomous coordinators managing the complete workflow from natural language input to multi-appliance scheduling. This paper presents an agentic AI HEMS where LLMs autonomously coordinate multi-appliance scheduling from natural language requests to device control, achieving optimal scheduling without example demonstrations. A hierarchical architecture combining one orchestrator with three specialist agents uses the ReAct pattern for iterative reasoning, enabling dynamic coordination without hardcoded workflows while integrating Google Calendar for context-aware deadline extraction. Evaluation across three open-source models using real Austrian day-ahead electricity prices reveals substantial capability differences. Llama-3.3-70B successfully coordinates all appliances across all scenarios to match cost-optimal benchmarks computed via mixed-integer linear programming, while other models achieve perfect single-appliance performance but struggle to coordinate all appliances simultaneously. Progressive prompt engineering experiments demonstrate that analytical query handling without explicit guidance remains unreliable despite models' general reasoning capabilities. We open-source the complete system including orchestration logic, agent prompts, tools, and web interfaces to enable reproducibility, extension, and future research.", 'abstract_zh': '电力部门转型需要显著增加住宅需求响应能力，然而家庭能源管理系统（HEMS）的采用受限于用户交互障碍，即需要将日常偏好转化为技术参数。虽然大型语言模型已被应用于能源系统中作为代码生成器和参数抽取器，但目前没有任何实现将LLM部署为自主协调者，以管理从自然语言输入到多设备调度的完整工作流程。本文介绍了一种自主型AI HEMS，其中LLM自主协调从自然语言请求到设备控制的多设备调度，实现最优调度而无需示例演示。该系统采用分层架构结合一个协调器和三个专家代理，使用ReAct模式进行迭代推理，实现动态协调而不需硬编码的工作流程，并集成Google日历以实现上下文感知的截止日期提取。使用三个开源模型对实际奥地利次日电价进行评估，结果显示不同模型的能力存在显著差异。Llama-3.3-70B成功协调所有设备以匹配通过混合整数线性规划计算的成本最优基准，而其他模型在单设备上表现完美但在同时协调所有设备方面面临困难。渐进式提示工程技术实验表明，即使在利用模型的普遍推理能力的情况下，无具体指导的分析查询处理仍不可靠。我们开源了整个系统，包括协调逻辑、代理提示、工具和Web界面，以实现可重复性、扩展性和未来研究。', 'title_zh': '代理型AI家庭能源管理系统：面向居民负荷调度的大语言模型框架'}
{'arxiv_id': 'arXiv:2510.26550', 'title': 'EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the Edge', 'authors': 'Jack FitzGerald, Aristotelis Lazaridis, Dylan Bates, Aman Sharma, Jonnathan Castillo, Yousif Azami, Sean Bailey, Jeremy Cao, Peter Damianov, Kevin de Haan, Luke Kerbs, Vincent Lu, Joseph Madigan, Jeremy McLaurin, Jonathan Tainer, Dave Anderson, Jonathan Beck, Jamie Cuticello, Colton Malkerson, Tyler Saltsman', 'link': 'https://arxiv.org/abs/2510.26550', 'abstract': 'We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for military tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated from military documentation and websites. We also present four new tests sets: (a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k (general military knowledge). On these military test sets, EdgeRunner 20B matches or exceeds GPT-5 task performance with 95%+ statistical significance, except for the high reasoning setting on the combat medic test set and the low reasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no statistically-significant regression on general-purpose benchmarks like ARC-C, GPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the low reasoning setting. We also present analyses on hyperparameter settings, cost, and throughput. These findings show that small, locally-hosted models are ideal solutions for data-sensitive operations such as in the military domain, allowing for deployment in air-gapped edge devices.', 'abstract_zh': 'EdgeRunner 20B： toward optimized edge computing for military tasks', 'title_zh': 'EdgeRunner 20B: 在边缘设备上运行与GPT-5军事任务性能相当'}
{'arxiv_id': 'arXiv:2510.26486', 'title': 'LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks', 'authors': 'Dipak Meher, Carlotta Domeniconi, Guadalupe Correa-Cabrera', 'link': 'https://arxiv.org/abs/2510.26486', 'abstract': 'Human smuggling networks are complex and constantly evolving, making them difficult to analyze comprehensively. Legal case documents offer rich factual and procedural insights into these networks but are often long, unstructured, and filled with ambiguous or shifting references, posing significant challenges for automated knowledge graph (KG) construction. Existing methods either overlook coreference resolution or fail to scale beyond short text spans, leading to fragmented graphs and inconsistent entity linking. We propose LINK-KG, a modular framework that integrates a three-stage, LLM-guided coreference resolution pipeline with downstream KG extraction. At the core of our approach is a type-specific Prompt Cache, which consistently tracks and resolves references across document chunks, enabling clean and disambiguated narratives for structured knowledge graph construction from both short and long legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes by 32.22% compared to baseline methods, resulting in cleaner and more coherent graph structures. These improvements establish LINK-KG as a strong foundation for analyzing complex criminal networks.', 'abstract_zh': '人类偷渡网络复杂且不断演变，难以进行全面分析。法律案例文件提供了丰富的事实和程序性见解，但往往是长篇、无结构的，并且充满了模糊或不断变化的引用，给自动知识图谱（KG）构建带来了重大挑战。现有方法要么忽略了共指消解，要么无法将处理范围扩大到短文本片段，导致知识图谱碎片化和实体链接不一致。我们提出了一种模块化框架LINK-KG，该框架集成了一个三阶段、基于大语言模型（LLM）的共指消解管道，并在下游KG提取中加以应用。我们的方法核心是一个特定类型的提示缓存，能够跨文档片段一致地跟踪和解决引用，从而为从短文本和长文本法律文件中构建结构化知识图谱提供清晰和去模糊化的叙述。与基线方法相比，LINK-KG将平均节点重复率降低了45.21%，噪声节点降低了32.22%，形成了更为清洁和连贯的图结构。这些改进奠定了LINK-KG分析复杂犯罪网络的坚实基础。', 'title_zh': 'LINK-KG: 由大语言模型驱动且消除了指代冲突的知识图谱用于人口走私网络'}
{'arxiv_id': 'arXiv:2510.26481', 'title': "Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections", 'authors': 'Clarissa Sabrina Arlinghaus, Tristan Kenneweg, Barbara Hammer, Günter W. Maier', 'link': 'https://arxiv.org/abs/2510.26481', 'abstract': 'Large language models (LLMs) such as ChatGPT are increasingly integrated into high-stakes decision-making, yet little is known about their susceptibility to social influence. We conducted three preregistered conformity experiments with GPT-4o in a hiring context. In a baseline study, GPT consistently favored the same candidate (Profile C), reported moderate expertise (M = 3.01) and high certainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT faced unanimous opposition from eight simulated partners and almost always conformed (99.9%), reporting lower certainty and significantly elevated self-reported informational and normative conformity (p < .001). In Study 2 (GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of disagreement trials, reporting less certainty and more normative conformity. Across studies, results demonstrate that GPT does not act as an independent observer but adapts to perceived social consensus. These findings highlight risks of treating LLMs as neutral decision aids and underline the need to elicit AI judgments prior to exposing them to human opinions.', 'abstract_zh': '大型语言模型（LLMs）如ChatGPT在高风险决策中的社会影响 susceptibility 分析：三项预注册实验探究', 'title_zh': '谁拥有最终决定权？ChatGPT选择中的从众动态'}
{'arxiv_id': 'arXiv:2510.26418', 'title': 'Chain-of-Thought Hijacking', 'authors': 'Jianli Zhao, Tingchen Fu, Rylan Schaeffer, Mrinank Sharma, Fazl Barez', 'link': 'https://arxiv.org/abs/2510.26418', 'abstract': 'Large reasoning models (LRMs) achieve higher task performance by allocating more inference-time compute, and prior works suggest this scaled reasoning may also strengthen safety by improving refusal. Yet we find the opposite: the same reasoning can be used to bypass safeguards. We introduce Chain-of-Thought Hijacking, a jailbreak attack on reasoning models. The attack pads harmful requests with long sequences of harmless puzzle reasoning. Across HarmBench, CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively - far exceeding prior jailbreak methods for LRMs. To understand the effectiveness of our attack, we turn to a mechanistic analysis, which shows that mid layers encode the strength of safety checking, while late layers encode the verification outcome. Long benign CoT dilutes both signals by shifting attention away from harmful tokens. Targeted ablations of attention heads identified by this analysis causally decrease refusal, confirming their role in a safety subnetwork. These results show that the most interpretable form of reasoning - explicit CoT - can itself become a jailbreak vector when combined with final-answer cues. We release prompts, outputs, and judge decisions to facilitate replication.', 'abstract_zh': '大型推理模型中的思维链条劫持：一种针对推理模型的 Jailbreak 攻击', 'title_zh': '连锁思考窃取'}
{'arxiv_id': 'arXiv:2510.26384', 'title': 'Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings', 'authors': 'Andrew M. Bean, Nabeel Seedat, Shengzhuang Chen, Jonathan Richard Schwarz', 'link': 'https://arxiv.org/abs/2510.26384', 'abstract': "The prohibitive cost of evaluating large language models (LLMs) on comprehensive benchmarks necessitates the creation of small yet representative data subsets (i.e., tiny benchmarks) that enable efficient assessment while retaining predictive fidelity. Current methods for this task operate under a model-centric paradigm, selecting benchmarking items based on the collective performance of existing models. Such approaches are limited by large upfront costs, an inability to immediately handle new benchmarks (`cold-start'), and the fragile assumption that future models will share the failure patterns of their predecessors. In this work, we challenge this paradigm and propose a item-centric approach to benchmark subset selection, arguing that selection should be based on the intrinsic properties of the task items themselves, rather than on model-specific failure patterns. We instantiate this item-centric efficient benchmarking approach via a novel method, Scales++, where data selection is based on the cognitive demands of the benchmark samples. Empirically, we show Scales++ reduces the upfront selection cost by over 18x while achieving competitive predictive fidelity. On the Open LLM Leaderboard, using just a 0.5\\% data subset, we predict full benchmark scores with a 2.9% mean absolute error. We demonstrate that this item-centric approach enables more efficient model evaluation without significant fidelity degradation, while also providing better cold-start performance and more interpretable benchmarking.", 'abstract_zh': 'prohibitive 成本在评估大规模语言模型 (LLMs) 时，全面基准测试的数据子集（即小型基准测试）的创建是必要的，以便实现高效的评估同时保持预测保真度。当前的方法在模型为中心的范式下运行，基于现有模型的综合表现选择基准测试项目。这些方法受到大额前期成本、无法立即处理新基准（冷启动）以及未来模型将与其前任共享故障模式的脆弱假设的限制。在本工作中，我们挑战了这一范式，并提出了一种基于项目的基准测试子集选择方法，认为选择应基于任务项目的固有属性，而不是基于特定模型的故障模式。我们通过一种新颖的方法 Scales++ 实现了这种基于项目的高效基准测试方法，其中数据选择基于基准样本的认知需求。实证结果表明，Scales++ 将前期选择成本减少了超过 18 倍，同时实现了可竞争的预测保真度。在 Open LLM 领导板上，使用仅 0.5% 的数据子集，我们以 2.9% 的均方误差预测了完整的基准分数。我们证明，这种基于项目的接近方法可以在不显著牺牲保真度的情况下实现更高效的模型评估，同时提供更好的冷启动性能和更具可解释性的基准测试。', 'title_zh': 'Scales++: 具有认知尺度嵌入的计算高效评估子集选择'}
{'arxiv_id': 'arXiv:2510.26374', 'title': 'BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning', 'authors': 'Qianli Shen, Daoyuan Chen, Yilun Huang, Zhenqing Ling, Yaliang Li, Bolin Ding, Jingren Zhou', 'link': 'https://arxiv.org/abs/2510.26374', 'abstract': 'Reinforcement finetuning (RFT) is a key technique for aligning Large Language Models (LLMs) with human preferences and enhancing reasoning, yet its effectiveness is highly sensitive to which tasks are explored during training. Uniform task sampling is inefficient, wasting computation on tasks that are either trivial or unsolvable, while existing task selection methods often suffer from high rollout costs, poor adaptivity, or incomplete evidence. We introduce \\textbf{BOTS}, a unified framework for \\textbf{B}ayesian \\textbf{O}nline \\textbf{T}ask \\textbf{S}election in LLM reinforcement finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior estimates of task difficulty as the model evolves. It jointly incorporates \\emph{explicit evidence} from direct evaluations of selected tasks and \\emph{implicit evidence} inferred from these evaluations for unselected tasks, with Thompson sampling ensuring a principled balance between exploration and exploitation. To make implicit evidence practical, we instantiate it with an ultra-light interpolation-based plug-in that estimates difficulties of unevaluated tasks without extra rollouts, adding negligible overhead. Empirically, across diverse domains and LLM scales, BOTS consistently improves data efficiency and performance over baselines and ablations, providing a practical and extensible solution for dynamic task selection in RFT.', 'abstract_zh': 'Bayesian Online Task Selection for Reinforcement Fine-tuning of Large Language Models', 'title_zh': 'BOTS：贝叶斯在线任务选择的统一框架在LLM强化微调中'}
{'arxiv_id': 'arXiv:2510.26309', 'title': 'GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance', 'authors': 'Jiseong Chung, Ronny Ko, Wonchul Yoo, Makoto Onizuka, Sungmok Kim, Tae-Wan Kim, Won-Yong Shin', 'link': 'https://arxiv.org/abs/2510.26309', 'abstract': 'Compliance at web scale poses practical challenges: each request may require a regulatory assessment. Regulatory texts (e.g., the General Data Protection Regulation, GDPR) are cross-referential and normative, while runtime contexts are expressed in unstructured natural language. This setting motivates us to align semantic information in unstructured text with the structured, normative elements of regulations. To this end, we introduce GraphCompliance, a framework that represents regulatory texts as a Policy Graph and runtime contexts as a Context Graph, and aligns them. In this formulation, the policy graph encodes normative structure and cross-references, whereas the context graph formalizes events as subject-action-object (SAO) and entity-relation triples. This alignment anchors the reasoning of a judge large language model (LLM) in structured information and helps reduce the burden of regulatory interpretation and event parsing, enabling a focus on the core reasoning step. In experiments on 300 GDPR-derived real-world scenarios spanning five evaluation tasks, GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than LLM-only and RAG baselines, with fewer under- and over-predictions, resulting in higher recall and lower false positive rates. Ablation studies indicate contributions from each graph component, suggesting that structured representations and a judge LLM are complementary for normative reasoning.', 'abstract_zh': '面向网页规模的合规性面临实践挑战：每次请求可能需要进行监管评估。监管文本（例如通用数据保护条例，GDPR）是相互引用和规范性的，而运行时上下文则以非结构化自然语言表达。这种设置促使我们通过图谱对齐非结构化文本中的语义信息与监管中的结构化规范元素进行对齐。为此，我们引入了GraphCompliance框架，该框架将监管文本表示为策略图，将运行时上下文表示为情境图，并对齐它们。在这一表示中，策略图编码规范结构和相互引用，而情境图形式化事件为主体-动作-客体（SAO）和实体-关系三元组。这种对齐为大型语言模型（LLM）裁判的推理锚定了结构化信息，有助于减少监管解释和事件解析的负担，使重点放在核心推理步骤上。在涵盖五个评估任务的300个源自GDPR的真实世界场景实验中，GraphCompliance在LLM仅模型和RAG基线上分别获得了4.1-7.2个百分点更高的微F1值，预测更少的误报和漏报，从而提高了召回率并降低了假阳性率。消融研究表明，每个图组件的贡献，表明结构化表示和裁判LLM对规范推理具有互补性。', 'title_zh': 'GraphCompliance：规管合规的政策与上下文图谱对齐'}
{'arxiv_id': 'arXiv:2510.26270', 'title': 'Graph-Enhanced Policy Optimization in LLM Agent Training', 'authors': 'Jiazhen Yuan, Wei Zhao, Zhengbiao Bai', 'link': 'https://arxiv.org/abs/2510.26270', 'abstract': "Group based reinforcement learning (RL) has shown impressive results on complex reasoning and mathematical tasks. Yet, when applied to train multi-turn, interactive LLM agents, these methods often suffer from structural blindness-the inability to exploit the underlying connectivity of the environment. This manifests in three critical challenges: (1) inefficient, unguided exploration, (2) imprecise credit assignment due to overlooking pivotal states, and (3) myopic planning caused by static reward discounting. We address these issues with Graph-Enhanced Policy Optimization (GEPO), which dynamically constructs a state-transition graph from agent experience and employs graph-theoretic centrality to provide three synergistic learning signals: (1)structured intrinsic rewards that guide exploration toward high-impact states, (2) a graph-enhanced advantage function for topology-aware credit assignment, and (3) a dynamic discount factor adapted to each state's strategic value. On the ALFWorld, WebShop, and a proprietary Workbench benchmarks, GEPO demonstrates strong performance, achieving absolute success rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These results highlight that explicitly modeling environmental structure is a robust, generalizable strategy for advancing LLM agent training.", 'abstract_zh': '基于图增强的策略优化在多轮交互大模型代理训练中的应用', 'title_zh': '图增强策略优化在大语言模型代理训练中'}
{'arxiv_id': 'arXiv:2510.26242', 'title': 'Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles', 'authors': 'Xinhang Li, Qing Guo, Junyu Chen, Zheng Guo, Shengzhe Xu, Lei Li, Lin Zhang', 'link': 'https://arxiv.org/abs/2510.26242', 'abstract': "With increasing urban traffic complexity, Traffic Signal Control (TSC) is essential for optimizing traffic flow and improving road safety. Large Language Models (LLMs) emerge as promising approaches for TSC. However, they are prone to hallucinations in emergencies, leading to unreliable decisions that may cause substantial delays for emergency vehicles. Moreover, diverse intersection types present substantial challenges for traffic state encoding and cross-intersection training, limiting generalization across heterogeneous intersections. Therefore, this paper proposes Retrieval Augmented Generation (RAG)-enhanced distributed LLM agents with Emergency response for Generalizable TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning framework, which dynamically adjusts reasoning depth based on the emergency scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to distill specific knowledge and guidance from historical cases, enhancing the reliability and rationality of agents' emergency decisions. Secondly, this paper designs a type-agnostic traffic representation and proposes a Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3 adaptively samples training experience from diverse intersections with environment feedback-based priority and fine-tunes LLM agents with a designed reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies across heterogeneous intersections. On three real-world road networks with 17 to 177 heterogeneous intersections, extensive experiments show that REG-TSC reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle waiting time by 83.16%, outperforming other state-of-the-art methods.", 'abstract_zh': '基于检索增强生成的面向应急响应的通用交通信号控制（REG-TSC）', 'title_zh': '基于检索增强生成的分布式大规模语言模型代理用于具有应急车辆的通用交通信号控制'}
{'arxiv_id': 'arXiv:2510.26238', 'title': 'Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses', 'authors': "Duc-Hai Nguyen, Vijayakumar Nanjappan, Barry O'Sullivan, Hoang D. Nguyen", 'link': 'https://arxiv.org/abs/2510.26238', 'abstract': 'Millions of people take surveys every day, from market polls and academic studies to medical questionnaires and customer feedback forms. These datasets capture valuable insights, but their scale and structure present a unique challenge for large language models (LLMs), which otherwise excel at few-shot reasoning over open-ended text. Yet, their ability to process questionnaire data or lists of questions crossed with hundreds of respondent rows remains underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics, SPSS, REDCap) are typically designed for humans in the workflow, limiting such data integration with LLM and AI-empowered automation. This gap leaves scientists, surveyors, and everyday users without evidence-based guidance on how to best represent questionnaires for LLM consumption. We address this by introducing QASU (Questionnaire Analysis and Structural Understanding), a benchmark that probes six structural skills, including answer lookup, respondent count, and multi-hop inference, across six serialization formats and multiple prompt strategies. Experiments on contemporary LLMs show that choosing an effective format and prompt combination can improve accuracy by up to 8.8% points compared to suboptimal formats. For specific tasks, carefully adding a lightweight structural hint through self-augmented prompting can yield further improvements of 3-4% points on average. By systematically isolating format and prompting effects, our open source benchmark offers a simple yet versatile foundation for advancing both research and real-world practice in LLM-based questionnaire analysis.', 'abstract_zh': '大规模语言模型在问卷分析与结构理解中的挑战与机遇：QASU基准评测', 'title_zh': '问卷调查遇见LLM：对理解问题和回应的结构性技能的基准研究与实证分析'}
{'arxiv_id': 'arXiv:2510.26167', 'title': 'One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning', 'authors': 'Renhao Li, Jianhong Tu, Yang Su, Hamid Alinejad-Rokny, Derek F. Wong, Junyang Lin, Min Yang', 'link': 'https://arxiv.org/abs/2510.26167', 'abstract': 'Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce ToolRM, a family of lightweight generative RMs tailored for general tool-use scenarios. To build these models, we propose a novel pipeline that constructs pairwise preference data using rule-based scoring and multidimensional sampling. This yields ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique tasks that supports reinforcement learning with verifiable feedback. To evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on the agentic evaluation suite BFCL. Trained on our constructed data, models from the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward judgments. Beyond training objectives, ToolRM generalizes to broader critique tasks, including Best-of-N sampling and self-correction. Experiments on ACEBench highlight its effectiveness and efficiency, enabling inference-time scaling and reducing output token usage by over 66%. We release data and model checkpoints to facilitate future research.', 'abstract_zh': '工具RMs在工具学习领域中对于对齐大型语言模型与人类偏好发挥着关键作用。然而，在工具学习领域缺乏专门设计的功能调用任务RMs限制了更强大自主AI的发展。我们引入了ToolRM，这是一种针对通用工具使用场景定制的轻量级生成性RMs。为了构建这些模型，我们提出了一个新的流水线，使用基于规则的评分和多维采样来构建成对偏好数据。这产生了ToolPref-Pairwise-30K，一个多样、平衡且具有挑战性的批判任务数据集，支持具有可验证反馈的强化学习。为了评估工具使用RMs，我们还引入了基于BFCL自主评估套件构建的TRBench$_{BFCL}$基准测试。在我们构建的数据上训练的Qwen3-4B/8B系列模型在成对奖励判断中最高准确率提高了14.28%，显著优于诸如Claude 4和OpenAI o3等前沿模型。除了训练目标外，ToolRM还能泛化到更广泛的批判任务，包括Best-of-N采样和自我纠正。ACEBench上的实验突显了其有效性和效率，使其能够在推理时扩展并减少输出标记使用量超过66%。我们发布了数据和模型检查点，以促进未来的研究。', 'title_zh': '一个模型 critique 所有模型：通过高效推理奖励自主工具使用'}
{'arxiv_id': 'arXiv:2510.26144', 'title': 'The FM Agent', 'authors': 'Annan Li, Chufan Wu, Zengle Ge, Yee Hin Chong, Zhinan Hou, Lizhe Cao, Cheng Ju, Jianmin Wu, Huaiming Li, Haobo Zhang, Shenghao Feng, Mo Zhao, Fengzhi Qiu, Rui Yang, Mengmeng Zhang, Wenyi Zhu, Yingying Sun, Quan Sun, Shunhao Yan, Danyu Liu, Dawei Yin, Dou Shen', 'link': 'https://arxiv.org/abs/2510.26144', 'abstract': 'Large language models (LLMs) are catalyzing the development of autonomous AI research agents for scientific and engineering discovery. We present FM Agent, a novel and general-purpose multi-agent framework that leverages a synergistic combination of LLM-based reasoning and large-scale evolutionary search to address complex real-world challenges. The core of FM Agent integrates several key innovations: 1) a cold-start initialization phase incorporating expert guidance, 2) a novel evolutionary sampling strategy for iterative optimization, 3) domain-specific evaluators that combine correctness, effectiveness, and LLM-supervised feedback, and 4) a distributed, asynchronous execution infrastructure built on Ray. Demonstrating broad applicability, our system has been evaluated across diverse domains, including operations research, machine learning, GPU kernel optimization, and classical mathematical problems. FM Agent reaches state-of-the-art results autonomously, without human interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\\%), 43.56\\% on MLE-Bench (+4.0pp), up to 20x speedups on KernelBench, and establishes new state-of-the-art(SOTA) results on several classical mathematical problems. Beyond academic benchmarks, FM Agent shows considerable promise for both large-scale enterprise R\\&D workflows and fundamental scientific research, where it can accelerate innovation, automate complex discovery processes, and deliver substantial engineering and scientific advances with broader societal impact.', 'abstract_zh': '大型语言模型（LLMs）正催化自主AI研究代理在科学与工程发现中的发展。我们提出FM代理，这是一种新颖的一般用途多代理框架，结合了基于LLM的推理和大规模进化搜索，以解决复杂的真实世界挑战。FM代理的核心集成了多项关键技术创新：1）带有专家指导的冷启动初始化阶段，2）一种新型的进化采样策略用于迭代优化，3）特定领域的评估器结合了正确性、有效性以及LLM监督反馈，以及4）基于Ray构建的分布式异步执行基础设施。展示出广泛适用性，我们的系统已在包括运筹学、机器学习、GPU内核优化和经典数学问题等多个领域进行了评估。FM代理能够自主达到最先进的结果，无需人类解释或调整——在ALE-Bench上得分为1976.3（+5.2%），MLE-Bench得分为43.56%（性能提升4.0个百分点），在KernelBench上最快可达20倍的加速，并在若干经典数学问题上建立了新的最先进的结果。除了学术基准，FM代理在大规模企业研发工作流和基础科学研究中展现出巨大的潜力，能够加速创新、自动化复杂发现过程，并带来广泛的工程和科学进步，具有更广泛的社会效益。', 'title_zh': 'FM代理'}
{'arxiv_id': 'arXiv:2510.26143', 'title': 'Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math', 'authors': 'Bo Pang, Deqian Kong, Silvio Savarese, Caiming Xiong, Yingbo Zhou', 'link': 'https://arxiv.org/abs/2510.26143', 'abstract': 'Reinforcement learning (RL) can elicit strong reasoning in large language models (LLMs), yet most open efforts focus on math and code. We propose Reasoning Curriculum, a simple two-stage curriculum that first elicits reasoning skills in pretraining-aligned domains such as math, then adapts and refines these skills across other domains via joint RL. Stage 1 performs a brief cold start and then math-only RL with verifiable rewards to develop reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and consolidate these skills. The curriculum is minimal and backbone-agnostic, requiring no specialized reward models beyond standard verifiability checks. Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning curriculum yields consistent gains. Ablations and a cognitive-skill analysis indicate that both stages are necessary and that math-first elicitation increases cognitive behaviors important for solving complex problems. Reasoning Curriculum provides a compact, easy-to-adopt recipe for general reasoning.', 'abstract_zh': '强化学习（RL）可以在大型语言模型（LLMs）中激发强烈的推理能力，但大多数开源努力主要集中在数学和代码上。我们提出了一种简单的两阶段课程：首先在与预训练对齐的领域（如数学）中激发推理技能，然后通过联合RL跨其他领域进行适应和精炼。第一阶段进行简短的冷启动和仅数学的RL，使用可验证的奖励来发展推理技能。第二阶段在混合领域数据上运行联合RL以转移和巩固这些技能。该课程简单且与基础模型无关，只需要标准的可验证性检查之外的无特殊奖励模型。在Qwen3-4B和Llama-3.1-8B上对多领域套件进行评估表明，推理课程可以带来一致的收益。消融实验和认知能力分析表明，两个阶段都是必要的，数学优先的激发可以增加解决复杂问题所需的重要认知行为。推理课程提供了一种紧凑且易于采用的一般推理方法。', 'title_zh': '数学推理课程：从数学中bootstrap广域LLM推理'}
{'arxiv_id': 'arXiv:2510.26136', 'title': 'Beyond Benchmarks: The Economics of AI Inference', 'authors': 'Boqin Zhuang, Jiacheng Qiao, Mingqian Liu, Mingxing Yu, Ping Hong, Rui Li, Xiaoxia Song, Xiangjun Xu, Xu Chen, Yaoyao Ma, Yujie Gao', 'link': 'https://arxiv.org/abs/2510.26136', 'abstract': "The inference cost of Large Language Models (LLMs) has become a critical factor in determining their commercial viability and widespread adoption. This paper introduces a quantitative ``economics of inference'' framework, treating the LLM inference process as a compute-driven intelligent production activity. We analyze its marginal cost, economies of scale, and quality of output under various performance configurations. Based on empirical data from WiNEval-3.0, we construct the first ``LLM Inference Production Frontier,'' revealing three principles: diminishing marginal cost, diminishing returns to scale, and an optimal cost-effectiveness zone. This paper not only provides an economic basis for model deployment decisions but also lays an empirical foundation for the future market-based pricing and optimization of AI inference resources.", 'abstract_zh': '大型语言模型（LLMs）的推理成本已成为决定其商业可行性和广泛采用的关键因素。本文引入了一个定量的“推理经济学”框架，将LLM的推理过程视为基于计算的智能生产活动。我们分析了在各种性能配置下其边际成本、规模经济以及输出质量。基于WiNEval-3.0的实证数据，我们构建了首个“LLM推理生产前沿”，揭示了三个原则：边际成本递减、规模报酬递减以及最优成本效益区。本文不仅为模型部署决策提供了经济基础，还为未来基于市场的AI推理资源定价与优化奠定了实证基础。', 'title_zh': '超越基准：AI 推理的经济学'}
{'arxiv_id': 'arXiv:2510.26098', 'title': 'GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks', 'authors': 'Chenrui Shi, Zedong Yu, Zhi Gao, Ruining Feng, Enqi Liu, Yuwei Wu, Yunde Jia, Liuyu Xiang, Zhaofeng He, Qing Li', 'link': 'https://arxiv.org/abs/2510.26098', 'abstract': 'Large vision language models (VLMs) have advanced graphical user interface (GUI) task automation but still lag behind humans. We hypothesize this gap stems from missing core GUI knowledge, which existing training schemes (such as supervised fine tuning and reinforcement learning) alone cannot fully address. By analyzing common failure patterns in GUI task execution, we distill GUI knowledge into three dimensions: (1) interface perception, knowledge about recognizing widgets and system states; (2) interaction prediction, knowledge about reasoning action state transitions; and (3) instruction understanding, knowledge about planning, verifying, and assessing task completion progress. We further introduce GUI Knowledge Bench, a benchmark with multiple choice and yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux, IOS) and 292 applications. Our evaluation shows that current VLMs identify widget functions but struggle with perceiving system states, predicting actions, and verifying task completion. Experiments on real world GUI tasks further validate the close link between GUI knowledge and task success. By providing a structured framework for assessing GUI knowledge, our work supports the selection of VLMs with greater potential prior to downstream training and provides insights for building more capable GUI agents.', 'abstract_zh': '大型视觉语言模型（VLMs）在自动化图形用户界面（GUI）任务方面取得了进展，但仍落后于人类。我们认为这一差距源于缺失的核心GUI知识，现有的训练方案（如监督微调和强化学习）无法充分解决这一问题。通过分析GUI任务执行中的常见失败模式，我们将GUI知识归纳为三个维度：（1）界面感知，关于识别控件和系统状态的知识；（2）交互预测，关于推理动作状态转换的知识；以及（3）指令理解，关于规划、验证和评估任务完成进度的知识。我们进一步引入了GUI Knowledge Bench基准测试，该基准测试包含跨六种平台（Web、Android、MacOS、Windows、Linux、iOS）和292个应用程序的多项选择和是非问题。我们的评估结果显示，当前的VLMs能够识别控件功能，但在感知系统状态、预测动作和验证任务完成方面存在困难。针对真实世界GUI任务的实验进一步验证了GUI知识与任务成功之间的密切联系。通过为评估GUI知识提供结构化框架，我们的工作支持在下游训练前选择具有更大潜力的VLMs，并为构建更强大的GUI代理提供洞见。', 'title_zh': 'GUI Knowledge Bench: 揭示GUI任务中VLM失败背后的知识差距'}
{'arxiv_id': 'arXiv:2510.26012', 'title': 'AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys', 'authors': 'Siyi Wu, Chiaxin Liang, Ziqian Bi, Leyi Zhao, Tianyang Wang, Junhao Song, Yichao Zhang, Keyu Chen, Xinyuan Song', 'link': 'https://arxiv.org/abs/2510.26012', 'abstract': 'The rapid growth of research literature, particularly in large language models (LLMs), has made producing comprehensive and current survey papers increasingly difficult. This paper introduces autosurvey2, a multi-stage pipeline that automates survey generation through retrieval-augmented synthesis and structured evaluation. The system integrates parallel section generation, iterative refinement, and real-time retrieval of recent publications to ensure both topical completeness and factual accuracy. Quality is assessed using a multi-LLM evaluation framework that measures coverage, structure, and relevance in alignment with expert review standards. Experimental results demonstrate that autosurvey2 consistently outperforms existing retrieval-based and automated baselines, achieving higher scores in structural coherence and topical relevance while maintaining strong citation fidelity. By combining retrieval, reasoning, and automated evaluation into a unified framework, autosurvey2 provides a scalable and reproducible solution for generating long-form academic surveys and contributes a solid foundation for future research on automated scholarly writing. All code and resources are available at this https URL.', 'abstract_zh': 'The rapid增长的Research文献，尤其是在大规模语言模型(LLMs)领域的快速增长，使得撰写综合性和时效性强的综述论文越来越具挑战性。本文介绍了autosurvey2，这是一种多阶段流水线，通过检索增强的合成和结构化评估自动化综述生成。该系统结合了并行部分生成、迭代 refinement 和实时检索近期出版物，以确保主题完整性和事实准确性。质量通过一个基于多大规模语言模型的评估框架来评估，该框架在专家评审标准的指导下衡量覆盖面、结构和相关性。实验结果表明，autosurvey2一致地优于现有的基于检索和自动化的基线方法，在结构连贯性和主题相关性方面取得更高的分数，同时保持较强的引文准确性。通过将检索、推理和自动化评估统一到一个框架中，autosurvey2提供了生成长格式学术综述的可扩展和可重复解决方案，并为未来自动学术写作研究奠定了坚实基础。所有代码和资源均可在该网址获取。', 'title_zh': 'AutoSurvey2：为研究人员提供高级自动化文献综述支持'}
{'arxiv_id': 'arXiv:2510.25997', 'title': 'From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL', 'authors': 'Manu Redd, Tao Zhe, Dongjie Wang', 'link': 'https://arxiv.org/abs/2510.25997', 'abstract': 'Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing access to structured data, allowing users to query databases without learning SQL. Yet existing systems struggle with realistic spatio-temporal queries, where success requires aligning vague user phrasing with schema-specific categories, handling temporal reasoning, and choosing appropriate outputs. We present an agentic pipeline that extends a naive text-to-SQL baseline (llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The agent can plan, decompose, and adapt queries through schema inspection, SQL generation, execution, and visualization tools. We evaluate on 35 natural-language queries over the NYC and Tokyo check-in dataset, covering spatial, temporal, and multi-dataset reasoning. The agent achieves substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and enhances usability through maps, plots, and structured natural-language summaries. Crucially, our design enables more natural human-database interaction, supporting users who lack SQL expertise, detailed schema knowledge, or prompting skill. We conclude that agentic orchestration, rather than stronger SQL generators alone, is a promising foundation for interactive geospatial assistants.', 'abstract_zh': '基于文本到SQL的代理管道在自然语言到SQL系统中的应用：增强地理空间查询能力', 'title_zh': '从查询到洞察：用于空时文本到SQL的能动LLM流水线'}
{'arxiv_id': 'arXiv:2510.25933', 'title': 'Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning', 'authors': 'Nissan Yaron, Dan Bystritsky, Ben-Etzion Yaron', 'link': 'https://arxiv.org/abs/2510.25933', 'abstract': 'We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS Grounding public subset within a $\\pm 5$ pp equivalence margin.\nResults. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI 69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen\'s $d = 0.023$). TOST establishes equivalence at $\\pm 5$ pp (not at $\\pm 3$ pp). When purchased as managed APIs, Humans-Junior\'s base model (Phi-3.5-mini-instruct) is $\\approx 19\\times$ less expensive than GPT-4o on Microsoft AI Foundry pricing; self-hosted or edge deployments can drive incremental inference cost toward zero. Measured vs estimated pricing sources are tabulated in Appendix E.\nMethod. Our approach combines minimal directed "Exoskeleton Reasoning" scaffolds with behavioral fine-tuning that teaches protocol compliance (epistemic discipline) rather than domain answers. Fine-tuning alone adds little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance ($\\approx 25\\%$). In prompt-only settings on frontier models (Q1--Q100; non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.\nTL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within $\\pm 5$ pp on Q1--Q500). Cloud pricing shows $\\approx 19\\times$ lower cost versus GPT-4o, and self-hosted/edge deployments can approach zero marginal cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains (Q1--Q100; non-comparable) and optimized-prompt exploratory results under earlier judges are summarized in Appendix F.\nKeywords: Small Language Models, Factual Grounding, Directed Reasoning, Fine-Tuning, Model Alignment, Cost-Efficient AI', 'abstract_zh': '人类Junior：一个38亿参数模型，在FACTS Grounding公共子集上的表现与GPT-4o相媲美，误差在±5个百分点以内。', 'title_zh': 'Humains-Junior: 通过定向外骨骼推理实现GPT-4级事实准确性的380亿参数语言模型'}
{'arxiv_id': 'arXiv:2510.25908', 'title': 'SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications', 'authors': 'Emily Herron, Junqi Yin, Feiyi Wang', 'link': 'https://arxiv.org/abs/2510.25908', 'abstract': 'Large language models (LLMs) have demonstrated transformative potential in scientific research, yet their deployment in high-stakes contexts raises significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a comprehensive framework for evaluating LLM trustworthiness in scientific applications across four dimensions: truthfulness, adversarial robustness, scientific safety, and scientific ethics. Our framework incorporates novel, open-ended truthfulness benchmarks developed through a verified reflection-tuning pipeline and expert validation, alongside a novel ethics benchmark for scientific research contexts covering eight subcategories including dual-use research and bias. We evaluated seven prominent LLMs, including four science-specialized models and three general-purpose industry models, using multiple evaluation metrics including accuracy, semantic similarity measures, and LLM-based scoring. General-purpose industry models overall outperformed science-specialized models across each trustworthiness dimension, with GPT-o4-mini demonstrating superior performance in truthfulness assessments and adversarial robustness. Science-specialized models showed significant deficiencies in logical and ethical reasoning capabilities, along with concerning vulnerabilities in safety evaluations, particularly in high-risk domains such as biosecurity and chemical weapons. By open-sourcing our framework, we provide a foundation for developing more trustworthy AI systems and advancing research on model safety and ethics in scientific contexts.', 'abstract_zh': '大型语言模型（LLMs）在科学研究中展现了变革性的潜力，但在高风险情境中的部署引发了重大可信度关切。在此背景下，我们介绍了SciTrust 2.0，一个全面的框架，用于评估科学应用中LLMs的可信度，涉及四个维度：事实准确性、对抗鲁棒性、科学安全性和科学伦理。该框架结合了通过验证的反思调优管道和专家验证开发的新型开放性事实准确性基准，以及涵盖双用途研究和偏见等八个子类别的新型伦理基准，适用于科学研究情境。我们使用多个评估指标，包括准确性、语义相似度度量和基于LLM的评分，对七种 prominent LLMs 进行了评估，包括四种专门针对科学的模型和三种通用行业模型。通用行业模型在每个可信度维度上总体上优于专门针对科学的模型，其中GPT-o4-mini在事实准确性评估和对抗鲁棒性方面表现出优越性能。专门针对科学的模型在逻辑和伦理推理能力方面显示出了显著的不足，并在安全性评估中显示出了令人担忧的脆弱性，特别是在生物安全和化学武器等高风险领域。通过开源我们的框架，我们为开发更可信的AI系统并推进科学情境下模型安全性和伦理性的研究奠定了基础。', 'title_zh': 'SciTrust 2.0：科学应用中大型语言模型可信性评估的综合框架'}
{'arxiv_id': 'arXiv:2510.25884', 'title': 'Approximating Human Preferences Using a Multi-Judge Learned System', 'authors': 'Eitán Sprejer, Fernando Avalos, Augusto Bernardi, Jose Pedro Brito de Azevedo Faustino, Jacob Haimes, Narmeen Fatimah Oozeer', 'link': 'https://arxiv.org/abs/2510.25884', 'abstract': 'Aligning LLM-based judges with human preferences is a significant challenge, as they are difficult to calibrate and often suffer from rubric sensitivity, bias, and instability. Overcoming this challenge advances key applications, such as creating reliable reward models for Reinforcement Learning from Human Feedback (RLHF) and building effective routing systems that select the best-suited model for a given user query. In this work, we propose a framework for modeling diverse, persona-based preferences by learning to aggregate outputs from multiple rubric-conditioned judges. We investigate the performance of this approach against naive baselines and assess its robustness through case studies on both human and LLM-judges biases. Our primary contributions include a persona-based method for synthesizing preference labels at scale and two distinct implementations of our aggregator: Generalized Additive Model (GAM) and a Multi-Layer Perceptron (MLP).', 'abstract_zh': '基于LLM的法官与人类偏好对齐是一个重大挑战，它们难以校准且常受评分标准敏感性、偏见和不稳定性的困扰。克服这一挑战促进了关键应用的发展，如为人类反馈强化学习（RLHF）创建可靠的奖励模型以及构建有效的路由系统以选择最适合用户查询的模型。在本工作中，我们提出了一种通过学习聚合多个评分标准条件下的法官输出来建模多元人设偏好的框架。我们该方法与天真baseline的性能进行了对比，并通过针对人类和LLM法官偏见的案例研究评估了其稳健性。我们的主要贡献包括一种基于人设的大规模合成偏好标签的方法以及我们聚合器的两种实现：广义加性模型（GAM）和多层感知机（MLP）。', 'title_zh': '使用多评审员学习系统逼近人类偏好'}
{'arxiv_id': 'arXiv:2510.25860', 'title': "Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters", 'authors': 'Xingjian Zhang, Tianhong Gao, Suliang Jin, Tianhao Wang, Teng Ye, Eytan Adar, Qiaozhu Mei', 'link': 'https://arxiv.org/abs/2510.25860', 'abstract': 'Large language models (LLMs) are increasingly used as raters for evaluation tasks. However, their reliability is often limited for subjective tasks, when human judgments involve subtle reasoning beyond annotation labels. Thinking traces, the reasoning behind a judgment, are highly informative but challenging to collect and curate. We present a human-LLM collaborative framework to infer thinking traces from label-only annotations. The proposed framework uses a simple and effective rejection sampling method to reconstruct these traces at scale. These inferred thinking traces are applied to two complementary tasks: (1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation guidelines for proprietary LLM raters. Across multiple datasets, our methods lead to significantly improved LLM-human agreement. Additionally, the refined annotation guidelines increase agreement among different LLM models. These results suggest that LLMs can serve as practical proxies for otherwise unrevealed human thinking traces, enabling label-only corpora to be extended into thinking-trace-augmented resources that enhance the reliability of LLM raters.', 'abstract_zh': '大规模语言模型（LLMs）越来越多地被用作评价任务的评分者。然而，在人类判断涉及超越标注标签的细微推理时，它们的可靠性往往有限。判断背后的推理思路虽高度信息丰富，但收集和整理起来极具挑战。我们提出了一种人-LLM协作框架，用于从仅标注的评分中推断推理思路。该框架采用简单的有效的拒绝抽样方法，在大规模范围内重构这些推理思路。推断出的这些推理思路应用于两个互补任务：（1）微调开放的LLM评分者；（2）为专有的LLM评分者合成更清晰的标注指南。在多个数据集上，我们的方法显著提高了LLM-人类的一致性。此外，改进的标注指南增加了不同LLM模型之间的共识。这些结果表明，LLMs可以作为实用的人类未揭示推理思路的代理，使仅标注的数据集扩充为包含推理思路增强资源，从而提高LLM评分者可靠性。', 'title_zh': '从法官的角度看：推断出的思考痕迹提高大模型评价的可靠性'}
{'arxiv_id': 'arXiv:2510.25820', 'title': 'Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for Generative NPC Dialogue', 'authors': 'Vanessa Figueiredo, David Elumeze', 'link': 'https://arxiv.org/abs/2510.25820', 'abstract': 'Large Language Models (LLMs) promise to transform interactive games by enabling non-player characters (NPCs) to sustain unscripted dialogue. Yet it remains unclear whether constrained prompts actually improve player experience. We investigate this question through The Interview, a voice-based detective game powered by GPT-4o. A within-subjects usability study ($N=10$) compared high-constraint (HCP) and low-constraint (LCP) prompts, revealing no reliable experiential differences beyond sensitivity to technical breakdowns. Guided by these findings, we redesigned the HCP into a hybrid JSON+RAG scaffold and conducted a synthetic evaluation with an LLM judge, positioned as an early-stage complement to usability testing. Results uncovered a novel pattern: scaffolding effects were role-dependent: the Interviewer (quest-giver NPC) gained stability, while suspect NPCs lost improvisational believability. These findings overturn the assumption that tighter constraints inherently enhance play. Extending fuzzy-symbolic scaffolding, we introduce \\textit{Symbolically Scaffolded Play}, a framework in which symbolic structures are expressed as fuzzy, numerical boundaries that stabilize coherence where needed while preserving improvisation where surprise sustains engagement.', 'abstract_zh': '大型语言模型（LLMs）有望通过使非玩家角色（NPCs）能够维持非剧本对话来转变互动游戏。然而，尚不清楚受限提示是否实际上能改善玩家体验。我们通过由GPT-4驱动的声音推理游戏《访谈》来探讨这一问题。一项单被试使用性研究（$N=10$）将高约束（HCP）和低约束（LCP）提示进行了对比，结果显示除了技术故障的敏感性外，没有发现可靠的游戏体验差异。基于这些发现，我们重新设计了HCP为混合JSON+RAG支架，并进行了一项合成评估，以LLM裁判定位，作为使用性测试的早期补充。结果揭示了一种新型模式：支架效应因角色依赖：访谈者（任务提供者NPC）获得了稳定性，而嫌疑犯NPC失去了即兴可信度。这些发现推翻了更紧约束条件必然增强游戏体验的假设。扩展模糊符号支架，我们引入了《符号支架化游戏》框架，在该框架中，符号结构以模糊的、数值的边界形式表达，以便在需要时稳定连贯性，同时在惊喜保持游戏参与的地方保留即兴创作。', 'title_zh': '符号支撑的玩耍：设计角色敏感的生成NPC对话提示'}
{'arxiv_id': 'arXiv:2510.26790', 'title': 'Gistify! Codebase-Level Understanding via Runtime Execution', 'authors': 'Hyunji Lee, Minseon Kim, Chinmay Singh, Matheus Pereira, Atharv Sonwane, Isadora White, Elias Stengel-Eskin, Mohit Bansal, Zhengyan Shi, Alessandro Sordoni, Marc-Alexandre Côté, Xingdi Yuan, Lucas Caccia', 'link': 'https://arxiv.org/abs/2510.26790', 'abstract': 'As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions traces.', 'abstract_zh': '随着编码代理在大规模代码库中的应用越来越广泛，自动设计代码库级别的挑战性评估变得至关重要。我们提出了一项名为Gistify的任务，其中编码LLM必须创建一个单一的、最小的、自包含的文件，以重现代码库中的特定功能。编码LLM被给予访问整个代码库以及一个特定的入口点（例如，一个Python命令），生成的文件必须在执行相同的命令时重现整个代码库的输出，同时仅包含执行所提供命令所需的必要组成部分。Gistify任务的成功要求对代码库的结构有深刻的理解，准确地模型化其执行流程，以及能够生成潜在的大规模代码补丁。我们的研究发现，当前最先进的模型在解决Gistify任务时表现不佳，尤其是在执行轨迹较长的任务方面。', 'title_zh': 'Gistify！通过运行时执行实现代码库级理解'}
{'arxiv_id': 'arXiv:2510.26788', 'title': 'Defeating the Training-Inference Mismatch via FP16', 'authors': 'Penghui Qi, Zichen Liu, Xiangxin Zhou, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin', 'link': 'https://arxiv.org/abs/2510.26788', 'abstract': 'Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to \\textbf{FP16} effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning.', 'abstract_zh': '使用FP16有效消除大型语言模型强化学习微调中的数值不匹配问题', 'title_zh': '通过FP16消除训练-推理不匹配'}
{'arxiv_id': 'arXiv:2510.26771', 'title': 'STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization', 'authors': 'Marco Federici, Riccardo Del Chiaro, Boris van Breugel, Paul Whatmough, Markus Nagel', 'link': 'https://arxiv.org/abs/2510.26771', 'abstract': 'Quantization is the key method for reducing inference latency, power and memory footprint of generative AI models. However, accuracy often degrades sharply when activations are quantized below eight bits. Recent work suggests that invertible linear transformations (e.g. rotations) can aid quantization, by reparameterizing feature channels and weights. In this paper, we propose \\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a novel strategy that applies linear transformations along the \\textit{sequence} dimension to exploit the strong local correlation in language and visual data. By keeping a small number of tokens in each intermediate activation at higher precision, we can maintain model accuracy at lower (average) activations bit-widths. We evaluate STaMP on recent LVM and LLM architectures, demonstrating that it significantly improves low bit width activation quantization and complements established activation and weight quantization methods including recent feature transformations.', 'abstract_zh': '序列变换和混合精度量化（STaMP）：利用序列维度上的线性变换提高低比特激活量化性能', 'title_zh': 'STaMP: 序列转换和混合精度用于低精度激活量化'}
{'arxiv_id': 'arXiv:2510.26768', 'title': 'AMO-Bench: Large Language Models Still Struggle in High School Math Competitions', 'authors': 'Shengnan An, Xunliang Cai, Xuezhi Cao, Xiaoyu Li, Yehao Lin, Junlin Liu, Xinxuan Lv, Dan Ma, Xuanlin Wang, Ziwen Wang, Shuang Zhou', 'link': 'https://arxiv.org/abs/2510.26768', 'abstract': 'We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. this https URL', 'abstract_zh': 'AMO-Bench：一个高级数学推理基准，包含奥林匹克级别甚至更高的难度，共包含50个人工制作的问题', 'title_zh': 'AMO-Bench: 大型语言模型仍在高中数学竞赛中挣扎'}
{'arxiv_id': 'arXiv:2510.26697', 'title': 'The End of Manual Decoding: Towards Truly End-to-End Language Models', 'authors': 'Zhichao Wang, Dongyang Ma, Xinting Huang, Deng Cai, Tian Lan, Jiahao Xu, Haitao Mi, Xiaoying Tang, Yan Wang', 'link': 'https://arxiv.org/abs/2510.26697', 'abstract': 'The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly "end-to-end" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.\nThrough extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from "hacking the test set"-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., "generate with low randomness") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding.', 'abstract_zh': 'LLMs的“端到端”标签是一个误解。实际上，它们依赖于一个非可微解码过程，需要手动调整温度和top-p等超参数。本文介绍了AutoDeco，一种新型架构，能够通过学习控制自身的解码策略从而实现真正意义上的“端到端”生成。我们通过在轻量级头部中动态预测上下文相关的温度和top-p值，并将其与下一个标记的logits结合，将解码过程转变为参数化和标记级别的过程，允许模型在单向前传中自我调节其采样策略。通过在八个基准上进行广泛的实验，我们证明AutoDeco不仅显著优于默认解码策略，还能够在性能上与通过“破解测试集”得到的oracle调优基准相当——这是任何静态方法的实用上限。更重要的是，我们发现了一种新兴的能力：指令导向的解码控制。模型能够理解自然语言命令（例如，“生成具有低随机性”），并在标记级别上调整其预测的温度和top-p，从而开启一种可控和交互的LLM解码新范式。', 'title_zh': '手动解码的终结：迈向真正意义上的端到端语言模型'}
{'arxiv_id': 'arXiv:2510.26683', 'title': 'Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models', 'authors': 'Mingchen Tu, Zhiqiang Liu, Juan Li, Liangyurui Liu, Junjie Wang, Lei Liang, Wen Zhang', 'link': 'https://arxiv.org/abs/2510.26683', 'abstract': "Large language models (LLMs) have demonstrated exceptional capabilities across multiple domains by leveraging massive pre-training and curated fine-tuning data. However, in data-sensitive fields such as healthcare, the lack of high-quality, domain-specific training corpus hinders LLMs' adaptation for specialized applications. Meanwhile, domain experts have distilled domain wisdom into ontology rules, which formalize relationships among concepts and ensure the integrity of knowledge management repositories. Viewing LLMs as implicit repositories of human knowledge, we propose Evontree, a novel framework that leverages a small set of high-quality ontology rules to systematically extract, validate, and enhance domain knowledge within LLMs, without requiring extensive external datasets. Specifically, Evontree extracts domain ontology from raw models, detects inconsistencies using two core ontology rules, and reinforces the refined knowledge via self-distilled fine-tuning. Extensive experiments on medical QA benchmarks with Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both unmodified models and leading supervised baselines, achieving up to a 3.7% improvement in accuracy. These results confirm the effectiveness, efficiency, and robustness of our approach for low-resource domain adaptation of LLMs.", 'abstract_zh': '大型语言模型（LLMs）通过利用大量的预先训练数据和精心策划的微调数据，在多个领域展现了卓越的能力。然而，在医疗保健等数据敏感领域，缺乏高质量的领域特定训练语料库阻碍了LLMs的应用专业化。同时，领域专家将领域智慧提炼为本体规则，这些规则正式化了概念之间的关系，并确保了知识管理库的一致性。将LLMs视作人类知识的隐式存储库，我们提出了一种名为Evontree的新框架，该框架利用少量高质量的本体规则系统地提取、验证和增强LLMs中的领域知识，而无需大量外部数据集。具体而言，Evontree从原始模型中提取领域本体，使用两种核心本体规则检测不一致性，并通过自我提炼微调强化精炼知识。在使用Llama3-8B-Instruct和Med42-v2进行的医疗问答基准测试中，与未经修改的模型和领先的监督基线相比，该框架表现出显著的优势，准确率提高了多达3.7%。这些结果证实了我们的方法在LLMs低资源领域适应性方面的有效性和稳健性。', 'title_zh': 'Evontree: 基于本体规则的大语言模型自我进化方法'}
{'arxiv_id': 'arXiv:2510.26585', 'title': 'Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems', 'authors': 'Fulin Lin, Shaowen Chen, Ruishan Fang, Hongwei Wang, Tao Lin', 'link': 'https://arxiv.org/abs/2510.26585', 'abstract': "While Multi-Agent Systems (MAS) excel at complex tasks, their growing autonomy with operational complexity often leads to critical inefficiencies, such as excessive token consumption and failures arising from misinformation. Existing methods primarily focus on post-hoc failure attribution, lacking proactive, real-time interventions to enhance robustness and efficiency. To this end, we introduce SupervisorAgent, a lightweight and modular framework for runtime, adaptive supervision that operates without altering the base agent's architecture. Triggered by an LLM-free adaptive filter, SupervisorAgent intervenes at critical junctures to proactively correct errors, guide inefficient behaviors, and purify observations. On the challenging GAIA benchmark, SupervisorAgent reduces the token consumption of the Smolagent framework by an average of 29.45% without compromising its success rate. Extensive experiments across five additional benchmarks (math reasoning, code generation, and question answering) and various SoTA foundation models validate the broad applicability and robustness of our approach. The code is available at this https URL.", 'abstract_zh': '多代理系统（MAS）在复杂任务中表现出色，但随着运行复杂性的增加，其自主性往往会引发关键的低效问题，如令牌消耗过多和因错误信息引起的功能失效。现有方法主要侧重于事后故障归因，缺乏前瞻性的、实时的干预措施来提高稳健性和效率。为此，我们提出了SupervisorAgent，一个轻量级且模块化的运行时自适应监督框架，无需修改基础代理架构。SupervisorAgent通过一个无LLM的自适应滤波器触发，在关键节点上主动纠正错误、引导不当行为并净化观测结果。在具有挑战性的GAIA基准测试中，SupervisorAgent在不牺牲成功率的情况下，平均减少了Smolagent框架的令牌消耗29.45%。广泛实验表明，我们的方法具有广泛的适用性和鲁棒性，在五个额外的基准测试（数学推理、代码生成和问答）以及多种最先进的基础模型上得到了验证。代码可在此处获取：this https URL。', 'title_zh': '有效利用令牌：面向高效的运行时多智能体系统'}
{'arxiv_id': 'arXiv:2510.26543', 'title': 'The Structure of Relation Decoding Linear Operators in Large Language Models', 'authors': 'Miranda Anna Christ, Adrián Csiszárik, Gergely Becsó, Dániel Varga', 'link': 'https://arxiv.org/abs/2510.26543', 'abstract': "This paper investigates the structure of linear operators introduced in Hernandez et al. [2023] that decode specific relational facts in transformer language models. We extend their single-relation findings to a collection of relations and systematically chart their organization. We show that such collections of relation decoders can be highly compressed by simple order-3 tensor networks without significant loss in decoding accuracy. To explain this surprising redundancy, we develop a cross-evaluation protocol, in which we apply each linear decoder operator to the subjects of every other relation. Our results reveal that these linear maps do not encode distinct relations, but extract recurring, coarse-grained semantic properties (e.g., country of capital city and country of food are both in the country-of-X property). This property-centric structure clarifies both the operators' compressibility and highlights why they generalize only to new relations that are semantically close. Our findings thus interpret linear relational decoding in transformer language models as primarily property-based, rather than relation-specific.", 'abstract_zh': '本文探讨了Hernandez等人在[2023]中引入的线性算子结构，这些算子用于解码变压器语言模型中的特定关系事实。我们扩展了他们关于单关系的研究，系统地梳理了多关系的组织结构。我们展示了这样的关系解码集合可以通过简单的阶数为3的张量网络进行高度压缩，而不会显著损失解码精度。为了解释这种令人惊讶的冗余性，我们开发了一种跨评估协议，在该协议中，我们对每个关系的应用主体使用每种线性解码算子。研究结果表明，这些线性映射并未编码独立的关系，而是抽取了反复出现的、粗粒度的语义特性（例如，首都所在的国家与食物所在的国家都归属于X所在的国家这一语义属性）。这种以特性为中心的结构不仅阐明了操作符的压缩性，还突显了它们仅能泛化到语义上接近的新关系的原因。因此，本文的研究结果将变压器语言模型中的线性关系解码主要解释为特性基于的，而非特定关系的。', 'title_zh': '大型语言模型中关系解码线性运算子的结构'}
{'arxiv_id': 'arXiv:2510.26512', 'title': 'Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs', 'authors': 'Dipak Meher, Carlotta Domeniconi', 'link': 'https://arxiv.org/abs/2510.26512', 'abstract': 'Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer critical insights but are often unstructured, lexically dense, and filled with ambiguous or shifting references, which pose significant challenges for automated knowledge graph (KG) construction. While recent LLM-based approaches improve over static templates, they still generate noisy, fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution. The recently proposed CORE-KG framework addresses these limitations by integrating a type-aware coreference module and domain-guided structured prompts, significantly reducing node duplication and legal noise. In this work, we present a systematic ablation study of CORE-KG to quantify the individual contributions of its two key components. Our results show that removing coreference resolution results in a 28.32% increase in node duplication and a 4.32% increase in noisy nodes, while removing structured prompts leads to a 4.34% increase in node duplication and a 73.33% increase in noisy nodes. These findings offer empirical insights for designing robust LLM-based pipelines for extracting structured representations from complex legal texts.', 'abstract_zh': '人类走私网络日益具有适应性且难以分析。法律案件文件提供了关键见解，但这些文件往往结构不一、词汇密集，并且充满了模糊或变化的引用，这对自动知识图谱（KG）构建构成了重大挑战。尽管基于近年来大语言模型（LLM）的方法在静态模板上有所改进，但由于缺乏引导式提取和共指解析，它们仍会产生嘈杂、碎片化的图谱，其中包含重复节点。最近提出的CORE-KG框架通过整合类型意识的共指模块和领域导向的结构化提示，显著减少了节点重复和法律噪声。在本文中，我们进行了一项系统的消融研究，以量化其两个关键组件的单独贡献。我们的结果表明，去除共指解析会导致节点重复增加28.32%，噪音节点增加4.32%，而去除结构化提示会导致节点重复增加4.34%，噪音节点增加73.33%。这些发现提供了实证见解，用于设计从复杂法律文本中提取结构化表示的健壮的大语言模型（LLM）管道。', 'title_zh': 'CORE-KG内部：结构化提示和共指消解在知识图谱评估中的应用'}
{'arxiv_id': 'arXiv:2510.26494', 'title': 'Simulating and Experimenting with Social Media Mobilization Using LLM Agents', 'authors': 'Sadegh Shirani, Mohsen Bayati', 'link': 'https://arxiv.org/abs/2510.26494', 'abstract': 'Online social networks have transformed the ways in which political mobilization messages are disseminated, raising new questions about how peer influence operates at scale. Building on the landmark 61-million-person Facebook experiment \\citep{bond201261}, we develop an agent-based simulation framework that integrates real U.S. Census demographic distributions, authentic Twitter network topology, and heterogeneous large language model (LLM) agents to examine the effect of mobilization messages on voter turnout. Each simulated agent is assigned demographic attributes, a personal political stance, and an LLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano}) reflecting its political sophistication. Agents interact over realistic social network structures, receiving personalized feeds and dynamically updating their engagement behaviors and voting intentions. Experimental conditions replicate the informational and social mobilization treatments of the original Facebook study. Across scenarios, the simulator reproduces qualitative patterns observed in field experiments, including stronger mobilization effects under social message treatments and measurable peer spillovers. Our framework provides a controlled, reproducible environment for testing counterfactual designs and sensitivity analyses in political mobilization research, offering a bridge between high-validity field experiments and flexible computational modeling.\\footnote{Code and data available at this https URL}', 'abstract_zh': '在线社交网络已改变了政治动员信息的传播方式，提出了关于大规模环境下同伴影响如何运作的新问题。基于具有里程碑意义的6100万用户Facebook实验\\[1\\]，我们构建了一个基于代理的模拟框架，该框架集成了真实的美国人口普查人口统计分布、真实的Twitter网络拓扑以及异质大型语言模型（LLM）代理，以考察动员信息对投票率的影响。每个模拟代理被赋予人口统计属性、个人政治立场，并且具有反映其政治精明程度的不同版本的LLM（\\texttt{GPT-4.1}、\\texttt{GPT-4.1-Mini}或\\texttt{GPT-4.1-Nano}）。代理人在现实的社会网络结构上相互作用，接收个性化的内容并动态更新他们的参与行为和投票意向。实验条件复制了原始Facebook研究中的信息和社交动员干预措施。在不同的情景下，模拟器再现了实地实验中的定性模式，包括在社交信息干预下更强的动员效果以及可测量的同伴溢出效应。该框架为政治动员研究中测试反事实设计和敏感性分析提供了一个受控且可再现的环境，架起了高有效性的实地实验与灵活的计算建模之间的桥梁。\\footnote{代码和数据可从此链接获取}。', 'title_zh': '使用大规模语言模型代理模拟和实验社交媒体动员'}
{'arxiv_id': 'arXiv:2510.26484', 'title': 'Bayesian Network Fusion of Large Language Models for Sentiment Analysis', 'authors': 'Rasoul Amirzadeh, Dhananjay Thiruvady, Fatemeh Shiri', 'link': 'https://arxiv.org/abs/2510.26484', 'abstract': 'Large language models (LLMs) continue to advance, with an increasing number of domain-specific variants tailored for specialised tasks. However, these models often lack transparency and explainability, can be costly to fine-tune, require substantial prompt engineering, yield inconsistent results across domains, and impose significant adverse environmental impact due to their high computational demands. To address these challenges, we propose the Bayesian network LLM fusion (BNLF) framework, which integrates predictions from three LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic mechanism for sentiment analysis. BNLF performs late fusion by modelling the sentiment predictions from multiple LLMs as probabilistic nodes within a Bayesian network. Evaluated across three human-annotated financial corpora with distinct linguistic and contextual characteristics, BNLF demonstrates consistent gains of about six percent in accuracy over the baseline LLMs, underscoring its robustness to dataset variability and the effectiveness of probabilistic fusion for interpretable sentiment classification.', 'abstract_zh': '大型语言模型（LLMs）不断发展，出现了越来越多针对特定领域的变体以适应特定任务。然而，这些模型往往缺乏透明度和解释性，调优成本高，需要大量的提示工程，跨领域的结果一致性差，并且由于其高度的计算需求对环境造成显著的负面影响。为了解决这些挑战，我们提出了一种贝叶斯网络LLM融合（BNLF）框架，该框架通过概率机制集成来自FinBERT、RoBERTa和BERTweet等三个LLM的情感分析预测。BNLF通过将多个LLM的情感预测建模为贝叶斯网络中的概率节点来进行晚期融合。在三项人工标注的金融语料库上进行评估，这些语料库具有不同的语言和上下文特征，BNLF在准确率上比基线LLM平均提高了约六个百分点，这表明其对数据集变异性的鲁棒性和概率融合在具有解释性的情感分类中的有效性。', 'title_zh': '大规模语言模型的贝叶斯网络融合情感分析'}
{'arxiv_id': 'arXiv:2510.26474', 'title': 'Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing', 'authors': 'Xin Guo, Zhiheng Xi, Yiwen Ding, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang', 'link': 'https://arxiv.org/abs/2510.26474', 'abstract': 'Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew effect"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.', 'abstract_zh': '自提高已成为提升大型视觉-语言模型推理能力的主要范式，其中模型通过迭代探索和学习成功的轨迹。然而，在这一过程中我们发现一个关键问题：模型在生成简单查询（即头部数据）的高质量轨迹方面表现出色，但在处理更复杂的查询（即尾部数据）方面却力不从心。这导致优化不平衡，促使模型优先发展简单的推理技能，而削弱其应对更复杂推理任务的能力。随着迭代次数的增加，这种不平衡变得越来越显著——我们称这一现象为“马太效应”——最终阻碍了模型进一步改进并导致性能瓶颈。为应对这一挑战，我们从两个角度引入了四种有效的策略：分布重塑和轨迹重采样，以在探索和学习的自提高过程中实现头部与尾部的平衡。在Qwen2-VL-7B-Instruct和InternVL2.5-4B模型上针对视觉推理任务进行的大量实验表明，我们的方法在视觉推理能力上始终表现出改进，平均优于 vanilla 自提高3.86分。', 'title_zh': '通过头部-尾部重新平衡对抗LVLMs的马太效应在其自我改进中的影响'}
{'arxiv_id': 'arXiv:2510.26457', 'title': 'SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning', 'authors': 'Fang Liu, Simiao Liu, Yinghao Zhu, Xiaoli Lian, Li Zhang', 'link': 'https://arxiv.org/abs/2510.26457', 'abstract': "Identifying and addressing security issues during the early phase of the development lifecycle is critical for mitigating the long-term negative impacts on software systems. Code review serves as an effective practice that enables developers to check their teammates' code before integration into the codebase. To streamline the generation of review comments, various automated code review approaches have been proposed, where LLM-based methods have significantly advanced the capabilities of automated review generation. However, existing models primarily focus on general-purpose code review, their effectiveness in identifying and addressing security-related issues remains underexplored. Moreover, adapting existing code review approaches to target security issues faces substantial challenges, including data scarcity and inadequate evaluation metrics. To address these limitations, we propose SecureReviewer, a new approach designed for enhancing LLMs' ability to identify and resolve security-related issues during code review. Specifically, we first construct a dataset tailored for training and evaluating secure code review capabilities. Leveraging this dataset, we fine-tune LLMs to generate code review comments that can effectively identify security issues and provide fix suggestions with our proposed secure-aware fine-tuning strategy. To mitigate hallucination in LLMs and enhance the reliability of their outputs, we integrate the RAG technique, which grounds the generated comments in domain-specific security knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric designed to assess the effectiveness of review comments in addressing security issues. Experimental results demonstrate that SecureReviewer outperforms state-of-the-art baselines in both security issue detection accuracy and the overall quality and practical utility of generated review comments.", 'abstract_zh': '在开发生命周期早期阶段识别和解决安全问题对于减轻长期负面影响至关重要。代码审查作为一种有效实践，使开发人员能够在代码集成到代码库之前检查队友的代码。为了简化审查评论的生成，已经提出了各种自动化代码审查方法，其中基于LLM的方法显著提升了自动化审查生成的能力。然而，现有模型主要集中在通用代码审查上，它们在识别和解决与安全相关问题方面的有效性尚待探索。此外，将现有代码审查方法调整为专门针对安全问题面临巨大挑战，包括数据稀缺性和不够完善的评估指标。为了解决这些局限性，我们提出了一种名为SecureReviewer的新方法，旨在增强LLM在代码审查过程中识别和解决安全问题的能力。具体来说，我们首先构建了一个专门用于训练和评估安全代码审查能力的数据集。利用该数据集，我们通过我们提出的安全意识微调策略对LLM进行微调，使其能够生成有效识别安全问题并提供修复建议的审查评论。为了减轻LLM的幻觉现象并提高其输出的可靠性，我们集成了一种RAG技术，使生成的评论基于特定领域的安全知识。此外，我们引入了一种新的评估指标SecureBLEU，旨在评估审查评论在解决安全问题方面的有效性。实验结果表明，SecureReviewer在安全问题检测精度以及生成的审查评论的整体质量与实用价值方面均优于现有先进baseline方法。', 'title_zh': 'SecureReviewer：通过安全意识微调提升大型语言模型的代码安全审查能力'}
{'arxiv_id': 'arXiv:2510.26352', 'title': 'The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration', 'authors': 'Kotaro Furuya, Yuichi Kitagawa', 'link': 'https://arxiv.org/abs/2510.26352', 'abstract': 'While a multi-agent approach based on large language models (LLMs) represents a promising strategy to surpass the capabilities of single models, its success is critically dependent on synergistic team composition. However, forming optimal teams is a significant challenge, as the inherent opacity of most models obscures the internal characteristics necessary for effective collaboration. In this paper, we propose an interaction-centric framework for automatic team composition that does not require any prior knowledge including their internal architectures, training data, or task performances. Our method constructs a "language model graph" that maps relationships between models from the semantic coherence of pairwise conversations, and then applies community detection to identify synergistic model clusters. Our experiments with diverse LLMs demonstrate that the proposed method discovers functionally coherent groups that reflect their latent specializations. Priming conversations with specific topics identified synergistic teams which outperform random baselines on downstream benchmarks and achieve comparable accuracy to that of manually-curated teams based on known model specializations. Our findings provide a new basis for the automated design of collaborative multi-agent LLM teams.', 'abstract_zh': '基于大语言模型的多agent方法中自动生成协同团队的交互中心框架', 'title_zh': '对话的几何学：绘制语言模型以揭示多代理协作的协同团队'}
{'arxiv_id': 'arXiv:2510.26345', 'title': 'MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data', 'authors': 'Mykhailo Poliakov, Nadiya Shvai', 'link': 'https://arxiv.org/abs/2510.26345', 'abstract': 'Health-related misinformation is very prevalent and potentially harmful. It is difficult to identify, especially when claims distort or misinterpret scientific findings. We investigate the impact of synthetic data generation and lightweight fine-tuning techniques on the ability of large language models (LLMs) to recognize fallacious arguments using the MISSCI dataset and framework. In this work, we propose MisSynth, a pipeline that applies retrieval-augmented generation (RAG) to produce synthetic fallacy samples, which are then used to fine-tune an LLM model. Our results show substantial accuracy gains with fine-tuned models compared to vanilla baselines. For instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score absolute improvement on the MISSCI test split over its vanilla baseline. We demonstrate that introducing synthetic fallacy data to augment limited annotated resources can significantly enhance zero-shot LLM classification performance on real-world scientific misinformation tasks, even with limited computational resources. The code and synthetic dataset are available on this https URL.', 'abstract_zh': '健康相关的 misinformation 非常普遍且可能具有危害性。由于断言可能歪曲或误解科学研究成果，因此很难识别。我们利用MISSCI数据集和框架，研究合成数据生成和轻量级微调技术对大型语言模型（LLMs）识别谬误论点能力的影响。在这项工作中，我们提出了一种名为MisSynth的管道，该管道利用检索增强生成（RAG）技术生成合成谬误样本，然后将这些样本用于微调LLM模型。结果显示，微调后的模型相对于传统的基线模型在准确率上有了显著提升。例如，LLaMA 3.1 8B微调模型在MISSCI测试集上的F1分数绝对提高了超过35%。我们证明，即使在计算资源有限的情况下，引入合成谬误数据以补充有限的标注资源，也能显著提升零样本LLM在实际科学 misinformation任务中的分类性能。相关代码和合成数据集可从此链接获取。', 'title_zh': 'MisSynth: 通过合成数据提高MISSCI逻辑谬误分类效果'}
{'arxiv_id': 'arXiv:2510.26336', 'title': 'From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning', 'authors': 'Nishit Neema, Srinjoy Mukherjee, Sapan Shah, Gokul Ramakrishnan, Ganesh Venkatesh', 'link': 'https://arxiv.org/abs/2510.26336', 'abstract': "Large Language Models (LLMs) excel at general tasks but underperform in specialized domains like economics and psychology, which require deep, principled understanding. To address this, we introduce ACER (Automated Curriculum-Enhanced Regimen) that transforms generalist models into domain experts without sacrificing their broad capabilities. ACER first synthesizes a comprehensive, textbook-style curriculum by generating a table of contents for a subject and then creating question-answer (QA) pairs guided by Bloom's taxonomy. This ensures systematic topic coverage and progressively increasing difficulty. The resulting synthetic corpus is used for continual pretraining with an interleaved curriculum schedule, aligning learning across both content and cognitive dimensions.\nExperiments with Llama 3.2 (1B and 3B) show significant gains in specialized MMLU subsets. In challenging domains like microeconomics, where baselines struggle, ACER boosts accuracy by 5 percentage points. Across all target domains, we observe a consistent macro-average improvement of 3 percentage points. Notably, ACER not only prevents catastrophic forgetting but also facilitates positive cross-domain knowledge transfer, improving performance on non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points, while maintaining stable performance on general reasoning tasks. Our results demonstrate that ACER offers a scalable and effective recipe for closing critical domain gaps in LLMs.", 'abstract_zh': '大型语言模型（LLMs）在通用任务上表现出色，但在经济学和心理学等专门领域表现不佳，这些领域需要深入的专业理解。为了解决这个问题，我们引入了ACER（Automated Curriculum-Enhanced Regimen），它能够将通用模型转换为领域专家，同时保留其广泛的通用能力。ACER 首先生成一个主题的大纲，并据此创建由布卢姆分类学引导的问题-答案（QA）对，从而合成一个全面的、教材风格的课程，确保系统地覆盖各个主题并逐步增加难度。生成的合成语料库用于持续预训练，并采用交错的课程时间表，从而使学习在内容和认知维度上保持一致。实验表明，ACER 在 Llama 3.2（1B 和 3B）以及经济学等具有挑战性的专业领域中带来了显著改进。在所有目标领域，我们观察到一致的宏观平均改进为 3 个百分点。值得注意的是，ACER 不仅防止了灾难性遗忘，还促进了跨领域的积极知识转移，在非目标领域上提升了 0.7 个百分点的表现。此外，ACER 在知识密集型基准测试，如ARC和GPQA上提高了 2 个百分点以上的性能，同时在通用推理任务上保持了稳定的表现。我们的结果表明，ACER 为缩小LLMs中的关键领域差距提供了一种可扩展且有效的方案。', 'title_zh': '从业余到大师：通过自动化课程学习向LLMs注入知识'}
{'arxiv_id': 'arXiv:2510.26285', 'title': 'Unravelling the Mechanisms of Manipulating Numbers in Language Models', 'authors': 'Michal Štefánik, Timothee Mickus, Marek Kadlčík, Bertram Højer, Michal Spiegel, Raúl Vázquez, Aman Sinha, Josef Kuchař, Philipp Mondorf', 'link': 'https://arxiv.org/abs/2510.26285', 'abstract': "Recent work has shown that different large language models (LLMs) converge to similar and accurate input embedding representations for numbers. These findings conflict with the documented propensity of LLMs to produce erroneous outputs when dealing with numeric information. In this work, we aim to explain this conflict by exploring how language models manipulate numbers and quantify the lower bounds of accuracy of these mechanisms. We find that despite surfacing errors, different language models learn interchangeable representations of numbers that are systematic, highly accurate and universal across their hidden states and the types of input contexts. This allows us to create universal probes for each LLM and to trace information -- including the causes of output errors -- to specific layers. Our results lay a fundamental understanding of how pre-trained LLMs manipulate numbers and outline the potential of more accurate probing techniques in addressed refinements of LLMs' architectures.", 'abstract_zh': '近期研究表明，不同的大型语言模型（LLMs）对数字的输入嵌入表示趋于收敛且准确。这些发现与现有文献中关于LLMs处理数字信息时产生错误输出的倾向相矛盾。在这项工作中，我们旨在通过探索语言模型如何处理数字以及量化这些机制的下限准确性来解释这一矛盾。我们发现，尽管表面上存在错误，不同语言模型学习到的数字表示是系统性的、高度准确且在隐藏层和不同类型输入上下文中具有普适性。这使我们能够为每种LLM创建通用探针，并追踪信息——包括输出错误的原因——到特定层面。我们的结果为预训练LLMs如何处理数字提供了基本理解，并指出了更准确探针技术在LLM架构改进方面潜在的价值。', 'title_zh': '探究操控语言模型中数字机制的方法'}
{'arxiv_id': 'arXiv:2510.26243', 'title': 'Angular Steering: Behavior Control via Rotation in Activation Space', 'authors': 'Hieu M. Vu, Tan M. Nguyen', 'link': 'https://arxiv.org/abs/2510.26243', 'abstract': 'Controlling specific behaviors in large language models while preserving their general capabilities is a central challenge for safe and reliable artificial intelligence deployment. Current steering methods, such as vector addition and directional ablation, are constrained within a two-dimensional subspace defined by the activation and feature direction, making them sensitive to chosen parameters and potentially affecting unrelated features due to unintended interactions in activation space. We introduce Angular Steering, a novel and flexible method for behavior modulation that operates by rotating activations within a fixed two-dimensional subspace. By formulating steering as a geometric rotation toward or away from a target behavior direction, Angular Steering provides continuous, fine-grained control over behaviors such as refusal and compliance. We demonstrate this method using refusal steering emotion steering as use cases. Additionally, we propose Adaptive Angular Steering, a selective variant that rotates only activations aligned with the target feature, further enhancing stability and coherence. Angular Steering generalizes existing addition and orthogonalization techniques under a unified geometric rotation framework, simplifying parameter selection and maintaining model stability across a broader range of adjustments. Experiments across multiple model families and sizes show that Angular Steering achieves robust behavioral control while maintaining general language modeling performance, underscoring its flexibility, generalization, and robustness compared to prior approaches. Code and artifacts are available at this https URL.', 'abstract_zh': '在保留大型语言模型通用能力的同时控制特定行为是实现安全可靠人工智能部署的核心挑战。当前的调控方法，如向量加法和方向消融，受限于由激活和特征方向定义的二维子空间，这使得它们对选定参数敏感，并且由于激活空间中的意外交互可能影响不相关的特征。我们提出了角度调控，这是一种新颖且灵活的行为调控方法，通过在固定的二维子空间内旋转激活来实现。通过将调控形式化为几何旋转朝向或远离目标行为方向，角度调控提供了对拒绝和遵从等行为的连续、精细控制。我们通过拒绝调控和情绪调控案例展示了这一方法。此外，我们提出了适应性角度调控，这是一种选择性变体，仅旋转与目标特征对齐的激活，进一步增强了稳定性和一致性。角度调控在统一的几何旋转框架下推广了现有的加法和技术祛除技术，简化了参数选择并保持模型在更广泛调整范围内的稳定性。多模型家族和规模下的实验表明，角度调控在保持通用语言建模性能的同时实现了稳健的行为控制，突显了其灵活性、泛化能力和稳健性，优于先前的方法。代码和相关材料可从该网址获取。', 'title_zh': '角度转向：通过激活空间旋转进行行为控制'}
{'arxiv_id': 'arXiv:2510.26219', 'title': 'Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space', 'authors': 'Sekitoshi Kanai, Tsukasa Yoshida, Hiroshi Takahashi, Haru Kuroki, Kazumune Hashimoto', 'link': 'https://arxiv.org/abs/2510.26219', 'abstract': 'Test-time alignment of large language models (LLMs) attracts attention because fine-tuning LLMs requires high computational costs. In this paper, we propose a new test-time alignment method called adaptive importance sampling on pre-logits (AISP) on the basis of the sampling-based model predictive control with the stochastic control input. AISP applies the Gaussian perturbation into pre-logits, which are outputs of the penultimate layer, so as to maximize expected rewards with respect to the mean of the perturbation. We demonstrate that the optimal mean is obtained by importance sampling with sampled rewards. AISP outperforms best-of-n sampling in terms of rewards over the number of used samples and achieves higher rewards than other reward-based test-time alignment methods.', 'abstract_zh': '基于采样模型预测控制和随机控制输入的预输出自适应重要性采样测试时对齐方法', 'title_zh': 'Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit Space'}
{'arxiv_id': 'arXiv:2510.26217', 'title': 'Hybrid LLM and Higher-Order Quantum Approximate Optimization for CSA Collateral Management', 'authors': 'Tao Jin, Stuart Florescu, Heyu', 'link': 'https://arxiv.org/abs/2510.26217', 'abstract': 'We address finance-native collateral optimization under ISDA Credit Support Annexes (CSAs), where integer lots, Schedule A haircuts, RA/MTA gating, and issuer/currency/class caps create rugged, legally bounded search spaces. We introduce a certifiable hybrid pipeline purpose-built for this domain: (i) an evidence-gated LLM that extracts CSA terms to a normalized JSON (abstain-by-default, span-cited); (ii) a quantum-inspired explorer that interleaves simulated annealing with micro higher order QAOA (HO-QAOA) on binding sub-QUBOs (subset size n <= 16, order k <= 4) to coordinate multi-asset moves across caps and RA-induced discreteness; (iii) a weighted risk-aware objective (Movement, CVaR, funding-priced overshoot) with an explicit coverage window U <= Reff+B; and (iv) CP-SAT as single arbiter to certify feasibility and gaps, including a U-cap pre-check that reports the minimal feasible buffer B*. Encoding caps/rounding as higher-order terms lets HO-QAOA target the domain couplings that defeat local swaps. On government bond datasets and multi-CSA inputs, the hybrid improves a strong classical baseline (BL-3) by 9.1%, 9.6%, and 10.7% across representative harnesses, delivering better cost-movement-tail frontiers under governance settings. We release governance grade artifacts-span citations, valuation matrix audit, weight provenance, QUBO manifests, and CP-SAT traces-to make results auditable and reproducible.', 'abstract_zh': '面向ISDA信贷支持附则的金融原生抵押品优化：一种可验证的混合管道方法', 'title_zh': '混合大语言模型和高阶量子近似优化在 CSA 净额结算担保管理中的应用'}
{'arxiv_id': 'arXiv:2510.26205', 'title': 'Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning', 'authors': 'Qi Luo, Xiaonan Li, Tingshuo Fan, Xinchi Chen, Xipeng Qiu', 'link': 'https://arxiv.org/abs/2510.26205', 'abstract': 'Retrieval-augmented generation (RAG) has emerged as a leading approach to reducing hallucinations in large language models (LLMs). Current RAG evaluation benchmarks primarily focus on what we call local RAG: retrieving relevant chunks from a small subset of documents to answer queries that require only localized understanding within specific text chunks. However, many real-world applications require a fundamentally different capability -- global RAG -- which involves aggregating and analyzing information across entire document collections to derive corpus-level insights (for example, "What are the top 10 most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first benchmark specifically designed to evaluate global RAG capabilities, covering four core task types: counting, extremum queries, sorting, and top-k extraction. Through systematic evaluation across different models and baselines, we find that existing RAG methods perform poorly on global tasks, with the strongest baseline achieving only 1.51 F1 score. To address these challenges, we propose GlobalRAG, a multi-tool collaborative framework that preserves structural coherence through chunk-level retrieval, incorporates LLM-driven intelligent filters to eliminate noisy documents, and integrates aggregation modules for precise symbolic computation. On the Qwen2.5-14B model, GlobalRAG achieves 6.63 F1 compared to the strongest baseline\'s 1.51 F1, validating the effectiveness of our method.', 'abstract_zh': '全局增强生成（GlobalRAG）：一种评估全局RAG能力的新基准', 'title_zh': '面向全局检索增强生成：一个语料库级推理基准'}
{'arxiv_id': 'arXiv:2510.26200', 'title': "Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation", 'authors': 'Woojin Kim, Jaeyoung Do', 'link': 'https://arxiv.org/abs/2510.26200', 'abstract': 'While diffusion language models (DLMs) enable fine-grained refinement, their practical controllability remains fragile. We identify and formally characterize a central failure mode called update forgetting, in which uniform and context agnostic updates induce token level fluctuations across timesteps, erasing earlier semantic edits and disrupting the cumulative refinement process, thereby degrading fluency and coherence. As this failure originates in uniform and context agnostic updates, effective control demands explicit token ordering. We propose Token Timestep Allocation (TTA), which realizes soft and semantic token ordering via per token timestep schedules: critical tokens are frozen early, while uncertain tokens receive continued refinement. This timestep based ordering can be instantiated as either a fixed policy or an adaptive policy driven by task signals, thereby supporting a broad spectrum of refinement strategies. Because it operates purely at inference time, it applies uniformly across various DLMs and naturally extends to diverse supervision sources. Empirically, TTA improves controllability and fluency: on sentiment control, it yields more than 20 percent higher accuracy and nearly halves perplexity using less than one fifth the steps; in detoxification, it lowers maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0). Together, these results demonstrate that softened ordering via timestep allocation is the critical lever for mitigating update forgetting and achieving stable and controllable diffusion text generation.', 'abstract_zh': '基于时间步分配的令牌排序在弥合更新遗忘以实现可控的扩散语言生成中的作用', 'title_zh': '不要让它褪色：通过令牌时间步分配保存扩散语言模型中的编辑'}
{'arxiv_id': 'arXiv:2510.26172', 'title': 'Linking Heterogeneous Data with Coordinated Agent Flows for Social Media Analysis', 'authors': 'Shifu Chen, Dazhen Deng, Zhihong Xu, Sijia Xu, Tai-Quan Peng, Yingcai Wu', 'link': 'https://arxiv.org/abs/2510.26172', 'abstract': "Social media platforms generate massive volumes of heterogeneous data, capturing user behaviors, textual content, temporal dynamics, and network structures. Analyzing such data is crucial for understanding phenomena such as opinion dynamics, community formation, and information diffusion. However, discovering insights from this complex landscape is exploratory, conceptually challenging, and requires expertise in social media mining and visualization. Existing automated approaches, though increasingly leveraging large language models (LLMs), remain largely confined to structured tabular data and cannot adequately address the heterogeneity of social media analysis. We present SIA (Social Insight Agents), an LLM agent system that links heterogeneous multi-modal data -- including raw inputs (e.g., text, network, and behavioral data), intermediate outputs, mined analytical results, and visualization artifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy that connects insight types with suitable mining and visualization techniques, SIA enables agents to plan and execute coherent analysis strategies. To ensure multi-modal integration, it incorporates a data coordinator that unifies tabular, textual, and network data into a consistent flow. Its interactive interface provides a transparent workflow where users can trace, validate, and refine the agent's reasoning, supporting both adaptability and trustworthiness. Through expert-centered case studies and quantitative evaluation, we show that SIA effectively discovers diverse and meaningful insights from social media while supporting human-agent collaboration in complex analytical tasks.", 'abstract_zh': '社交媒体平台生成大量的异构数据，捕捉用户行为、文本内容、时间动态和网络结构。分析此类数据对于理解意见动态、社区形成和信息扩散等现象至关重要。然而，从这一复杂景观中发现见解是探索性的、概念上具有挑战性的，并且需要具备社交媒体挖掘和可视化方面的专业知识。尽管现有自动化方法越来越多地利用大型语言模型（LLMs），它们仍然主要局限于结构化表格数据，无法充分应对社交媒体分析的异质性。我们提出了一种名为SIA（Social Insight Agents）的LLM代理系统，该系统通过协调的代理流将包括原始输入（如文本、网络和行为数据）、中间输出、挖掘分析结果和可视化成果在内的异构多模态数据联系起来。SIA 通过连接见解类型与合适的挖掘和可视化技术的自底向上的分类体系，使代理能够规划和执行一致的分析策略。为确保多模态集成，它包含一个数据协调器，该协调器将表格、文本和网络数据统一成一个一致的流程。交互式界面提供了透明的工作流，用户可以追溯、验证和精炼代理的推理过程，支持适应性和可靠性。通过对专家中心的案例研究和定量评估，我们展示了SIA 在复杂分析任务中有效发现多样且有意义的社交媒体见解，同时支持人类与代理的合作。', 'title_zh': '基于协调智能体流动的异构数据链接与社会媒体分析'}
{'arxiv_id': 'arXiv:2510.26130', 'title': 'Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation', 'authors': 'Musfiqur Rahman, SayedHassan Khatoonabadi, Emad Shihab', 'link': 'https://arxiv.org/abs/2510.26130', 'abstract': 'Large language models (LLMs) have advanced code generation at the function level, yet their ability to produce correct class-level implementations in authentic software projects remains poorly understood. This work introduces a novel benchmark derived from open-source repositories, comprising real-world classes divided into seen and unseen partitions to evaluate generalization under practical conditions. The evaluation examines multiple LLMs under varied input specifications, retrieval-augmented configurations, and documentation completeness levels.\nResults reveal a stark performance disparity: LLMs achieve 84% to 89% correctness on established synthetic benchmarks but only 25% to 34% on real-world class tasks, with negligible differences between familiar and novel codebases. Comprehensive docstrings yield modest gains of 1% to 3% in functional accuracy, though statistical significance is rare. Retrieval-augmented generation proves most effective with partial documentation, improving correctness by 4% to 7% by supplying concrete implementation patterns absent from specifications. Error profiling identifies AttributeError, TypeError, and AssertionError as dominant failure modes (84% of cases), with synthetic tests overemphasizing assertion issues and real-world scenarios highlighting type and attribute mismatches. Retrieval augmentation reduces logical flaws but can introduce dependency conflicts.\nThe benchmark and analysis expose critical limitations in current LLM capabilities for class-level engineering, offering actionable insights for enhancing context modelling, documentation strategies, and retrieval integration in production code assistance tools.', 'abstract_zh': '大规模语言模型（LLMs）在函数级代码生成方面取得了进展，但在真实软件项目中生成正确的类级实现的能力尚不完全理解。本研究引入了一个源自开源仓库的新基准，将实际类划分为已见和未见部分，以在实际条件下评估泛化能力。评估在各种输入规范、检索增强配置和文档完整性水平下对多个LLM进行测试。\n\n结果显示，性能差距明显：LLM在成熟的合成基准测试中实现84%到89%的正确性，而在真实世界的类任务中仅实现25%到34%的正确性，熟悉和新颖代码库之间的差别不大。全面的文档字符串在功能准确性方面仅带来1%到3%的提升，尽管统计显著性罕见。检索增强生成在部分文档情况下最有效，通过提供规范中缺失的具体实现模式，正确性提高4%到7%。错误剖析确定了AttributeError、TypeError和AssertionError为主要失败模式（占84%的情况），合成测试过度强调断言问题，而真实世界场景则突出了类型和属性不符。检索增强减少了逻辑错误，但可能引入依赖冲突。\n\n该基准和分析揭示了当前LLM在类级工程方面的重要局限性，为增强上下文建模、文档策略和检索集成提供可操作的见解，以改进生产代码辅助工具。', 'title_zh': '超越合成基准：评估大语言模型在实际世界类级代码生成中的性能'}
{'arxiv_id': 'arXiv:2510.26083', 'title': 'Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism', 'authors': 'Yuhua Jiang, Shuang Cheng, Yihao Liu, Ermo Hua, Che Jiang, Weigao Sun, Yu Cheng, Feifei Gao, Biqing Qi, Bowen Zhou', 'link': 'https://arxiv.org/abs/2510.26083', 'abstract': "Specialized Generalist Models (SGMs) aim to preserve broad capabilities while achieving expert-level performance in target domains. However, traditional LLM structures including Transformer, Linear Attention, and hybrid models do not employ specialized memory mechanism guided by task information. In this paper, we present Nirvana, an SGM with specialized memory mechanism, linear time complexity, and test-time task information extraction. Besides, we propose the Task-Aware Memory Trigger ($\\textit{Trigger}$) that flexibly adjusts memory mechanism based on the current task's requirements. In Trigger, each incoming sample is treated as a self-supervised fine-tuning task, enabling Nirvana to adapt its task-related parameters on the fly to domain shifts. We also design the Specialized Memory Updater ($\\textit{Updater}$) that dynamically memorizes the context guided by Trigger. We conduct experiments on both general language tasks and specialized medical tasks. On a variety of natural language modeling benchmarks, Nirvana achieves competitive or superior results compared to the existing LLM structures. To prove the effectiveness of Trigger on specialized tasks, we test Nirvana's performance on a challenging medical task, i.e., Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with lightweight codecs on paired electromagnetic signals and MRI images. Despite the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI domain with the change of task-related parameters. Nirvana achieves higher-quality MRI reconstruction compared to conventional MRI models as well as the models with traditional LLMs' backbone, and can also generate accurate preliminary clinical reports accordingly.", 'abstract_zh': '特殊通用模型（SGMs）旨在保留广泛的能力同时在目标领域达到专家级别的性能。然而，传统的LLM结构，包括Transformer、线性注意力和混合模型，不采用基于任务信息的专门记忆机制。本文提出了一种名为Nirvana的SGM，该模型具有专门的记忆机制、线性时间复杂度和测试时任务信息提取能力。此外，我们提出了任务感知记忆触发器（$\\textit{Trigger}$），该机制可根据当前任务的需求灵活调整记忆机制。在$\\textit{Trigger}$中，每个输入样本被视为一个自我监督的微调任务，使Nirvana能够根据领域转移实时调整与任务相关参数。我们还设计了专门的记忆更新器（$\\textit{Updater}$），该更新器根据$\\textit{Trigger}$动态地记忆上下文。我们在一般语言任务和专门的医疗任务上进行了实验。在多种自然语言建模基准测试中，Nirvana取得了与现有LLM结构相当或更优的结果。为了证明$\\textit{Trigger}$在专门任务上的有效性，我们在配对的电磁信号和MRI图像上对冻结的Nirvana骨干进行后训练，并使用轻量级编解码器。即使冻结的Nirvana骨干，$\\textit{Trigger}$也能通过调整与任务相关参数指导模型适应MRI领域。与传统的MRI模型以及以传统LLM架构为基础的模型相比，Nirvana在MRI重建质量上表现出更高水平，并能够生成准确的初步临床报告。', 'title_zh': '涅槃：一种具有任务意识记忆机制的专门通用模型'}
{'arxiv_id': 'arXiv:2510.26037', 'title': 'SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning', 'authors': 'Kaiwen Zhou, Ahmed Elgohary, A S M Iftekhar, Amin Saied', 'link': 'https://arxiv.org/abs/2510.26037', 'abstract': "The ability of LLM agents to plan and invoke tools exposes them to new safety risks, making a comprehensive red-teaming system crucial for discovering vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic two-step process that starts with an agent definition and generates diverse seed test cases that cover various risk outcomes, tool-use trajectories, and risk sources. Then, it iteratively constructs and refines model-based adversarial attacks based on the execution trajectories of former attempts. To optimize the red-teaming cost, we present a model distillation approach that leverages structured forms of a teacher model's reasoning to train smaller models that are equally effective. Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer model improves attack success rate by 100%, surpassing the 671B Deepseek-R1 model. Our ablations and analyses validate the effectiveness of the iterative framework, structured reasoning, and the generalization of our red-teamer models.", 'abstract_zh': 'LLM代理规划和调用工具的能力使其面临新的安全风险，因此需要全面的红队系统来发现漏洞并确保其安全部署。我们提出了SIRAJ：任意黑盒LLM代理的一般红队框架。我们采用了一个动态的两步过程，首先定义代理，并生成涵盖各种风险结果、工具使用轨迹和风险源的多样化的种子测试案例。然后，根据前次尝试的执行轨迹迭代构建和优化基于模型的对抗攻击。为了优化红队的成本，我们提出了一种模型蒸馏方法，利用教师模型推理的结构化形式来训练更小但同样有效的模型。在多种评估代理设置下，我们的种子测试案例生成方法使风险结果和工具调用轨迹的覆盖范围提高了2-2.5倍。我们蒸馏的8B红队模型将攻击成功率提高了100%，超过了671B的Deepseek-R1模型。我们的消融实验和分析验证了迭代框架、结构化推理和我们红队模型泛化的有效性。', 'title_zh': 'SIRAJ：通过提炼结构化推理实现的多元高效红队测试方法for LLM代理'}
{'arxiv_id': 'arXiv:2510.26024', 'title': 'Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs', 'authors': 'HyoJung Han, Sweta Agrawal, Eleftheria Briakou', 'link': 'https://arxiv.org/abs/2510.26024', 'abstract': 'Cross-lingual alignment (CLA) aims to align multilingual representations, enabling Large Language Models (LLMs) to seamlessly transfer knowledge across languages. While intuitive, we hypothesize, this pursuit of representational convergence can inadvertently cause "cultural erasure", the functional loss of providing culturally-situated responses that should diverge based on the query language. In this work, we systematically analyze this trade-off by introducing a holistic evaluation framework, the transfer-localization plane, which quantifies both desirable knowledge transfer and undesirable cultural erasure. Using this framework, we re-evaluate recent CLA approaches and find that they consistently improve factual transfer at the direct cost of cultural localization across all six languages studied. Our investigation into the internal representations of these models reveals a key insight: universal factual transfer and culturally-specific knowledge are optimally steerable at different model layers. Based on this finding, we propose Surgical Steering, a novel inference-time method that disentangles these two objectives. By applying targeted activation steering to distinct layers, our approach achieves a better balance between the two competing dimensions, effectively overcoming the limitations of current alignment techniques.', 'abstract_zh': '跨语言对齐 (CLA) 的目标是使多语言表示对齐，从而使大型语言模型（LLMs）能够无缝地在语言之间转移知识。虽然直观上如此，但我们推测，这种表征收敛的追求可能会无意中导致“文化抹除”，即功能性地丧失提供基于查询语言应有所差异的文化情境响应的能力。在本工作中，我们通过引入一个综合评估框架——转移-本地化平面，系统地分析了这一权衡，该框架量化了期望的知识转移和不利的文化抹除。使用此框架，我们重新评估了近期的 CLA 方法，并发现它们在所有六种研究语言中都以一致的方式在直接牺牲文化本地化的同时提高了事实知识转移。我们对这些模型内部表示的研究揭示了一个关键见解：泛化的事实知识转移和文化特异性知识在不同的模型层上最优化地可调节。基于这一发现，我们提出了手术调节（Surgical Steering）这一新颖的推理时方法，以分离这两项目标。通过针对不同层应用有针对性的激活调节，我们的方法能够在两个竞争维度之间取得更好的平衡，有效地克服了当前对齐技术的局限性。', 'title_zh': '重新思考跨语言对齐：多语言LLM中转移与文化消除的平衡'}
{'arxiv_id': 'arXiv:2510.26020', 'title': 'PORTool: Tool-Use LLM Training with Rewarded Tree', 'authors': 'Feijie Wu, Weiwu Zhu, Yuxiang Zhang, Soumya Chatterjee, Jiarong Zhu, Fan Mo, Rodin Luo, Jing Gao', 'link': 'https://arxiv.org/abs/2510.26020', 'abstract': 'Current tool-use large language models (LLMs) are trained on static datasets, enabling them to interact with external tools and perform multi-step, tool-integrated reasoning, which produces tool-call trajectories. However, these models imitate how a query is resolved in a generic tool-call routine, thereby failing to explore possible solutions and demonstrating limited performance in an evolved, dynamic tool-call environment. In this work, we propose PORTool, a reinforcement learning (RL) method that encourages a tool-use LLM to explore various trajectories yielding the correct answer. Specifically, this method starts with generating multiple rollouts for a given query, and some of them share the first few tool-call steps, thereby forming a tree-like structure. Next, we assign rewards to each step, based on its ability to produce a correct answer and make successful tool calls. A shared step across different trajectories receives the same reward, while different steps under the same fork receive different rewards. Finally, these step-wise rewards are used to calculate fork-relative advantages, blended with trajectory-relative advantages, to train the LLM for tool use. The experiments utilize 17 tools to address user queries, covering both time-sensitive and time-invariant topics. We conduct ablation studies to systematically justify the necessity and the design robustness of step-wise rewards. Furthermore, we compare the proposed PORTool with other training approaches and demonstrate significant improvements in final accuracy and the number of tool-call steps.', 'abstract_zh': '当前使用的大型语言模型（LLMs）是在静态数据集上训练的，使它们能够与外部工具交互并执行多步骤、工具集成推理，从而产生工具调用轨迹。然而，这些模型模仿了一种通用工具调用常规处理查询的方式，未能探索可能的解决方案，并且在不断进化的动态工具调用环境中表现有限。在本文中，我们提出了PORTool，这是一种强化学习（RL）方法，鼓励工具使用LLM探索各种产生正确答案的轨迹。具体而言，该方法从对给定查询生成多个展开开始，其中一些展开共享前几步的工具调用，从而形成一种树状结构。接下来，我们根据每一步产生正确答案和成功调用工具的能力为其分配奖励。在不同轨迹中共享的步骤获得相同的奖励，而同一分支下的不同步骤获得不同的奖励。最后，这些步骤奖励用于计算分支相对优势，并与轨迹相对优势混合，以训练LLM用于工具使用。实验利用17种工具处理用户查询，涵盖了时间敏感和时间不变的话题。我们进行了消融研究，系统地验证了步骤奖励的必要性和设计稳健性。此外，我们将提出的PORTool与其他训练方法进行比较，并展示了最终准确性和工具调用步数上的显著改进。', 'title_zh': 'PORTool: 带有奖励树的工具使用大型语言模型训练'}
{'arxiv_id': 'arXiv:2510.25992', 'title': 'Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning', 'authors': 'Yihe Deng, I-Hung Hsu, Jun Yan, Zifeng Wang, Rujun Han, Gufeng Zhang, Yanfei Chen, Wei Wang, Tomas Pfister, Chen-Yu Lee', 'link': 'https://arxiv.org/abs/2510.25992', 'abstract': 'Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical "actions". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model\'s actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.', 'abstract_zh': '大型语言模型（LLMs）往往难以解决需要多步推理的问题。对于小型开源模型，可验证奖励强化学习（RLVR）在即使经过多次尝试后也难以采样到正确解，而监督微调（SFT）容易通过严格的逐 token 仿本来过度拟合长示例。为了解决这一差距，我们提出了监督强化学习（SRL）框架，将问题解决重新定义为生成一系列逻辑“动作”序列。SRL 训练模型在执行每个动作之前生成内部推理独白。奖励根据模型的动作与从 SFT 数据集中提取的专家动作的相似度逐步提供。这种监督即使在所有展开结果均不正确的情况下也能提供更丰富的学习信号，并鼓励受专家示范指导的灵活推理。因此，SRL 使小模型能够学习以前由 SFT 或 RLVR 无法解决的具有挑战性的问题。此外，使用 SRL 初始化训练再用 RLVR 进一步优化可获得最佳整体性能。除了推理基准测试之外，SRL 在代理软件工程任务中表现出有效的泛化能力，确立了其作为面向推理的 LLM 的强大且通用训练框架的地位。', 'title_zh': 'supervised reinforcement learning：从专家轨迹到逐步推理'}
{'arxiv_id': 'arXiv:2510.25947', 'title': 'Revisiting Multilingual Data Mixtures in Language Model Pretraining', 'authors': 'Negar Foroutan, Paul Teiletche, Ayush Kumar Tarun, Antoine Bosselut', 'link': 'https://arxiv.org/abs/2510.25947', 'abstract': 'The impact of different multilingual data mixtures in pretraining large language models (LLMs) has been a topic of ongoing debate, often raising concerns about potential trade-offs between language coverage and model performance (i.e., the curse of multilinguality). In this work, we investigate these assumptions by training 1.1B and 3B parameter LLMs on diverse multilingual corpora, varying the number of languages from 25 to 400. Our study challenges common beliefs surrounding multilingual training. First, we find that combining English and multilingual data does not necessarily degrade the in-language performance of either group, provided that languages have a sufficient number of tokens included in the pretraining corpus. Second, we observe that using English as a pivot language (i.e., a high-resource language that serves as a catalyst for multilingual generalization) yields benefits across language families, and contrary to expectations, selecting a pivot language from within a specific family does not consistently improve performance for languages within that family. Lastly, we do not observe a significant "curse of multilinguality" as the number of training languages increases in models at this scale. Our findings suggest that multilingual data, when balanced appropriately, can enhance language model capabilities without compromising performance, even in low-resource settings', 'abstract_zh': '不同多语数据混合对大规模语言模型预训练的影响：挑战多语训练的常见 belief，并发现多语数据在适当平衡时可以增强语言模型能力，而不会牺牲性能，即使在低资源环境下也是如此。', 'title_zh': '重访语言模型预训练中的多语言数据混合'}
{'arxiv_id': 'arXiv:2510.25904', 'title': 'Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation', 'authors': 'Frederico Belcavello, Ely Matos, Arthur Lorenzi, Lisandra Bonoto, Lívia Ruiz, Luiz Fernando Pereira, Victor Herbst, Yulla Navarro, Helen de Andrade Abreu, Lívia Dutra, Tiago Timponi Torrent', 'link': 'https://arxiv.org/abs/2510.25904', 'abstract': 'The use of LLM-based applications as a means to accelerate and/or substitute human labor in the creation of language resources and dataset is a reality. Nonetheless, despite the potential of such tools for linguistic research, comprehensive evaluation of their performance and impact on the creation of annotated datasets, especially under a perspectivized approach to NLP, is still missing. This paper contributes to reduction of this gap by reporting on an extensive evaluation of the (semi-)automatization of FrameNet-like semantic annotation by the use of an LLM-based semantic role labeler. The methodology employed compares annotation time, coverage and diversity in three experimental settings: manual, automatic and semi-automatic annotation. Results show that the hybrid, semi-automatic annotation setting leads to increased frame diversity and similar annotation coverage, when compared to the human-only setting, while the automatic setting performs considerably worse in all metrics, except for annotation time.', 'abstract_zh': '基于LLM的应用在语言资源和数据集创建中的使用是现实存在的。尽管此类工具在语言研究中具有潜在价值，但对其性能和对注解数据集创建影响的全面评估，尤其是从视角化的NLP视角来看，仍缺乏。本文通过报告一种基于LLM的语义角色标注器对类似FrameNet的语义标注进行半自动化处理的广泛评估， contributes to缩小这一差距。研究方法在三种实验设置下比较了注释时间、覆盖面和多样性：手工注释、自动注释和半自动注释。结果表明，混合的半自动注释设置在框架多样性方面优于仅有人工注释的设置，且在所有指标上具有相似的覆盖范围，而全自动设置在所有指标上表现较差，仅在注释时间上有优势。', 'title_zh': '在视角化设置中评估LLM辅助注释的影响：FrameNet注释案例研究'}
{'arxiv_id': 'arXiv:2510.25890', 'title': 'PRISM: Proof-Carrying Artifact Generation through LLM x MDE Synergy and Stratified Constraints', 'authors': 'Tong Ma, Hui Lai, Hui Wang, Zhenhu Tian, Jizhou Wang, Haichao Wu, Yongfan Gao, Chaochao Li, Fengjie Xu, Ling Fang', 'link': 'https://arxiv.org/abs/2510.25890', 'abstract': 'PRISM unifies Large Language Models with Model-Driven Engineering to generate regulator-ready artifacts and machine-checkable evidence for safety- and compliance-critical domains. PRISM integrates three pillars: a Unified Meta-Model (UMM) reconciles heterogeneous schemas and regulatory text into a single semantic space; an Integrated Constraint Model (ICM) compiles structural and semantic requirements into enforcement artifacts including generation-time automata (GBNF, DFA) and post-generation validators (e.g., SHACL, SMT); and Constraint-Guided Verifiable Generation (CVG) applies these through two-layer enforcement - structural constraints drive prefix-safe decoding while semantic/logical validation produces machine-checkable certificates. When violations occur, PRISM performs audit-guided repair and records generation traces for compliance review. We evaluate PRISM in automotive software engineering (AUTOSAR) and cross-border legal jurisdiction (Brussels I bis). PRISM produces structurally valid, auditable artifacts that integrate with existing tooling and substantially reduce manual remediation effort, providing a practical path toward automated artifact generation with built-in assurance.', 'abstract_zh': 'PRISM将大型语言模型与模型驱动工程相结合，生成监管-ready的 artifacts 和可机器验证的证据，应用于安全和合规关键领域。', 'title_zh': 'PRISM: 通过LLM与MDE协同及分层约束生成证明承载的制品'}
{'arxiv_id': 'arXiv:2510.25863', 'title': 'AAGATE: A NIST AI RMF-Aligned Governance Platform for Agentic AI', 'authors': 'Ken Huang, Jerry Huang, Yasir Mehmood, Hammad Atta, Muhammad Zeeshan Baig, Muhammad Aziz Ul Haq', 'link': 'https://arxiv.org/abs/2510.25863', 'abstract': "This paper introduces the Agentic AI Governance Assurance & Trust Engine (AAGATE), a Kubernetes-native control plane designed to address the unique security and governance challenges posed by autonomous, language-model-driven agents in production. Recognizing the limitations of traditional Application Security (AppSec) tooling for improvisational, machine-speed systems, AAGATE operationalizes the NIST AI Risk Management Framework (AI RMF). It integrates specialized security frameworks for each RMF function: the Agentic AI Threat Modeling MAESTRO framework for Map, a hybrid of OWASP's AIVSS and SEI's SSVC for Measure, and the Cloud Security Alliance's Agentic AI Red Teaming Guide for Manage. By incorporating a zero-trust service mesh, an explainable policy engine, behavioral analytics, and decentralized accountability hooks, AAGATE provides a continuous, verifiable governance solution for agentic AI, enabling safe, accountable, and scalable deployment. The framework is further extended with DIRF for digital identity rights, LPCI defenses for logic-layer injection, and QSAF monitors for cognitive degradation, ensuring governance spans systemic, adversarial, and ethical risks.", 'abstract_zh': '基于Kubernetes的代理AI治理保障与信任引擎（AAGATE）：一种应对生产环境中自主语言模型驱动代理独特安全与治理挑战的解决方案', 'title_zh': 'AAGATE: 一个符合NIST AI RMF规范的自主人工智能治理平台'}
{'arxiv_id': 'arXiv:2510.25784', 'title': 'zFLoRA: Zero-Latency Fused Low-Rank Adapters', 'authors': 'Dhananjaya Gowda, Seoha Song, Harshith Goka, Junhyun Lee', 'link': 'https://arxiv.org/abs/2510.25784', 'abstract': 'Large language models (LLMs) are increasingly deployed with task-specific adapters catering to multiple downstream applications. In such a scenario, the additional compute associated with these apparently insignificant number of adapter parameters (typically less than 1% of the base model) turns out to be disproportionately significant during inference time (upto 2.5x times that of the base model). In this paper, we propose a new zero-latency fused low-rank adapter (zFLoRA) that introduces zero or negligible latency overhead on top of the base model. Experimental results on LLMs of size 1B, 3B and 7B show that zFLoRA compares favorably against the popular supervised fine-tuning benchmarks including low-rank adapters (LoRA) as well as full fine-tuning (FFT). Experiments are conducted on 18 different tasks across three different categories namely commonsense reasoning, math reasoning and summary-dialogue. Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA H100) platforms show that the proposed zFLoRA adapters introduce zero to negligible latency overhead.', 'abstract_zh': '大规模语言模型（LLMs）越来越多地部署了针对多种下游应用的任务特定适配器。在这种情况下，这些看似不重要的适配器参数（通常少于基模型的1%）在推理过程中所产生的附加计算量出人意料地显著（最多达到基模型的2.5倍）。本文提出了一种新型零延迟融合低秩适配器（zFLoRA），其在基模型基础上引入了零或可忽略的延迟开销。实验结果表明，在1B、3B和7B规模的LLM上，zFLoRA相对于包括低秩适配器（LoRA）和全量微调（FFT）在内的流行监督微调基准具有竞争力。实验在三大类别（常识推理、数学推理和摘要对话）的18个不同的任务上进行。在NPU（三星Galaxy S25+）和GPU（NVIDIA H100）平台上进行的延迟测量表明，所提出的zFLoRA适配器引入了零或可忽略的延迟开销。', 'title_zh': '零延迟融合低秩适配器：zFLoRA'}
