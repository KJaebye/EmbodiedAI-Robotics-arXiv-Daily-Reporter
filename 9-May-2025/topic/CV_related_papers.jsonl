{'arxiv_id': 'arXiv:2505.05223', 'title': 'Multi-Objective Reinforcement Learning for Adaptive Personalized Autonomous Driving', 'authors': 'Hendrik Surmann, Jorge de Heuvel, Maren Bennewitz', 'link': 'https://arxiv.org/abs/2505.05223', 'abstract': 'Human drivers exhibit individual preferences regarding driving style. Adapting autonomous vehicles to these preferences is essential for user trust and satisfaction. However, existing end-to-end driving approaches often rely on predefined driving styles or require continuous user feedback for adaptation, limiting their ability to support dynamic, context-dependent preferences. We propose a novel approach using multi-objective reinforcement learning (MORL) with preference-driven optimization for end-to-end autonomous driving that enables runtime adaptation to driving style preferences. Preferences are encoded as continuous weight vectors to modulate behavior along interpretable style objectives$\\unicode{x2013}$including efficiency, comfort, speed, and aggressiveness$\\unicode{x2013}$without requiring policy retraining. Our single-policy agent integrates vision-based perception in complex mixed-traffic scenarios and is evaluated in diverse urban environments using the CARLA simulator. Experimental results demonstrate that the agent dynamically adapts its driving behavior according to changing preferences while maintaining performance in terms of collision avoidance and route completion.', 'abstract_zh': '人类驾驶员表现出对驾驶风格的个体偏好。将自主车辆适应这些偏好对于增强用户信任和满意度至关重要。然而，现有的端到端驾驶方法通常依赖于预定义的驾驶风格或需要持续的用户反馈来适应，这限制了它们支持动态、上下文依赖的偏好能力。我们提出了一种使用多目标强化学习（MORL）和偏好驱动优化的端到端自主驾驶新型方法，能够运行时适应驾驶风格偏好。偏好被编码为连续权重向量，以调节沿可解释风格目标的行为——包括效率、舒适性、速度和进攻性——而无需重新训练策略。我们的单策略代理在复杂的混行交通场景中集成了基于视觉的感知能力，并使用CARLA模拟器在多种城市环境中进行评估。实验结果表明，代理能够根据变化的偏好动态调整其驾驶行为，同时在碰撞避免和路线完成方面保持性能。', 'title_zh': '多目标强化学习在自适应个性化自动驾驶中的应用'}
{'arxiv_id': 'arXiv:2505.04989', 'title': 'CPP-DIP: Multi-objective Coverage Path Planning for MAVs in Dispersed and Irregular Plantations', 'authors': 'Weijie Kuang, Hann Woei Ho, Ye Zhou', 'link': 'https://arxiv.org/abs/2505.04989', 'abstract': 'Coverage Path Planning (CPP) is vital in precision agriculture to improve efficiency and resource utilization. In irregular and dispersed plantations, traditional grid-based CPP often causes redundant coverage over non-vegetated areas, leading to waste and pollution. To overcome these limitations, we propose CPP-DIP, a multi-objective CPP framework designed for Micro Air Vehicles (MAVs). The framework transforms the CPP task into a Traveling Salesman Problem (TSP) and optimizes flight paths by minimizing travel distance, turning angles, and intersection counts. Unlike conventional approaches, our method does not rely on GPS-based environmental modeling. Instead, it uses aerial imagery and a Histogram of Oriented Gradients (HOG)-based approach to detect trees and extract image coordinates. A density-aware waypoint strategy is applied: Kernel Density Estimation (KDE) is used to reduce redundant waypoints in dense regions, while a greedy algorithm ensures complete coverage in sparse areas. To verify the generality of the framework, we solve the resulting TSP using three different methods: Greedy Heuristic Insertion (GHI), Ant Colony Optimization (ACO), and Monte Carlo Reinforcement Learning (MCRL). Then an object-based optimization is applied to further refine the resulting path. Additionally, CPP-DIP integrates ForaNav, our insect-inspired navigation method, for accurate tree localization and tracking. The experimental results show that MCRL offers a balanced solution, reducing the travel distance by 16.9 % compared to ACO while maintaining a similar performance to GHI. It also improves path smoothness by reducing turning angles by 28.3 % and 59.9 % relative to ACO and GHI, respectively, and effectively eliminates intersections. These results confirm the robustness and effectiveness of CPP-DIP in different TSP solvers.', 'abstract_zh': '基于多目标的微型无人机路径规划框架（CPP-DIP）', 'title_zh': 'CPP-DIP: 多目标覆盖路径规划在分散和不规则林分中的应用'}
{'arxiv_id': 'arXiv:2505.04831', 'title': 'Steerable Scene Generation with Post Training and Inference-Time Search', 'authors': 'Nicholas Pfaff, Hongkai Dai, Sergey Zakharov, Shun Iwase, Russ Tedrake', 'link': 'https://arxiv.org/abs/2505.04831', 'abstract': 'Training robots in simulation requires diverse 3D scenes that reflect the specific challenges of downstream tasks. However, scenes that satisfy strict task requirements, such as high-clutter environments with plausible spatial arrangement, are rare and costly to curate manually. Instead, we generate large-scale scene data using procedural models that approximate realistic environments for robotic manipulation, and adapt it to task-specific goals. We do this by training a unified diffusion-based generative model that predicts which objects to place from a fixed asset library, along with their SE(3) poses. This model serves as a flexible scene prior that can be adapted using reinforcement learning-based post training, conditional generation, or inference-time search, steering generation toward downstream objectives even when they differ from the original data distribution. Our method enables goal-directed scene synthesis that respects physical feasibility and scales across scene types. We introduce a novel MCTS-based inference-time search strategy for diffusion models, enforce feasibility via projection and simulation, and release a dataset of over 44 million SE(3) scenes spanning five diverse environments. Website with videos, code, data, and model weights: this https URL', 'abstract_zh': '在模拟中训练机器人需要多样化的3D场景，这些场景能够反映下游任务的具体挑战。然而，满足严格任务要求的场景，如高杂乱度环境且具有合理的空间布局，是罕见且成本高昂的手动编排。相反，我们使用近似真实环境的程序化模型生成大规模场景数据，并将其适应于特定任务目标。我们通过训练一个统一的基于扩散的生成模型来预测应从固定资产库中放置哪些物体及其SE(3)姿态来实现这一点。该模型作为一种灵活的先验场景模型，可以通过基于强化学习的后训练、条件生成或推理时的搜索进行调整，即使目标与原始数据分布不同也能引导生成。我们的方法能够实现兼顾物理可行性的目标导向场景合成，并适用于不同类型的场景。我们引入了一种新的基于MCTS的扩散模型推理时搜索策略，通过投影和模拟确保可行性，并发布了包含超过4400万组SE(3)场景的数据集，覆盖五个不同的环境。网站提供视频、代码、数据和模型权重：this https URL。', 'title_zh': '基于后训练和推理时搜索的可 steering 场景生成'}
{'arxiv_id': 'arXiv:2505.05059', 'title': 'Enhancing Reinforcement Learning for the Floorplanning of Analog ICs with Beam Search', 'authors': 'Sandro Junior Della Rovere, Davide Basso, Luca Bortolussi, Mirjana Videnovic-Misic, Husni Habal', 'link': 'https://arxiv.org/abs/2505.05059', 'abstract': "The layout of analog ICs requires making complex trade-offs, while addressing device physics and variability of the circuits. This makes full automation with learning-based solutions hard to achieve. However, reinforcement learning (RL) has recently reached significant results, particularly in solving the floorplanning problem. This paper presents a hybrid method that combines RL with a beam (BS) strategy. The BS algorithm enhances the agent's inference process, allowing for the generation of flexible floorplans by accomodating various objective weightings, and addressing congestion without without the need for policy retraining or fine-tuning. Moreover, the RL agent's generalization ability stays intact, along with its efficient handling of circuit features and constraints. Experimental results show approx. 5-85% improvement in area, dead space and half-perimeter wire length compared to a standard RL application, along with higher rewards for the agent. Moreover, performance and efficiency align closely with those of existing state-of-the-art techniques.", 'abstract_zh': '基于强化学习与束搜索策略的混合方法在模拟IC布局中的应用', 'title_zh': '使用_beam_搜索增强模拟IC版图规划的强化学习'}
{'arxiv_id': 'arXiv:2505.05356', 'title': 'Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields', 'authors': "Runfeng Li, Mikhail Okunev, Zixuan Guo, Anh Ha Duong, Christian Richardt, Matthew O'Toole, James Tompkin", 'link': 'https://arxiv.org/abs/2505.05356', 'abstract': 'We present a method to reconstruct dynamic scenes from monocular continuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that achieves similar or better accuracy than neural volumetric approaches and is 100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a single viewpoint is a significant challenge in computer vision. In C-ToF radiance field reconstruction, the property of interest-depth-is not directly measured, causing an additional challenge. This problem has a large and underappreciated impact upon the optimization when using a fast primitive-based scene representation like 3D Gaussian splatting, which is commonly used with multi-view data to produce satisfactory results and is brittle in its optimization otherwise. We incorporate two heuristics into the optimization to improve the accuracy of scene geometry represented by Gaussians. Experimental results show that our approach produces accurate reconstructions under constrained C-ToF sensing conditions, including for fast motions like swinging baseball bats. this https URL', 'abstract_zh': '我们提出了一种方法，使用单目连续波时间飞行（C-ToF）相机的原始传感器样本来重建动态场景，实现了与神经体积方法相当或更好的精度，并且速度快100倍。快速从单一视角实现高保真动态3D重建是计算机视觉中的一个重大挑战。在C-ToF辐照度场重建中，需要测量的属性-深度-无法直接测量，导致了额外的挑战。当使用诸如3D高斯散斑等快速基础场景表示时，这个问题对优化产生了巨大且被低估的影响，而后者在多视角数据中通常能产生满意的结果，但在其他情况下优化过程是脆弱的。我们通过引入两个启发式方法改进了基于高斯表示的场景几何精度。实验结果表明，我们的方法在受限的C-ToF感测条件下产生了准确的重建结果，包括快速动作如挥动棒球 bat。', 'title_zh': '高斯分布的时间飞行：在动态辐射场中间接优化深度'}
{'arxiv_id': 'arXiv:2505.05321', 'title': 'Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery', 'authors': 'Chintan B. Maniyar, Minakshi Kumar, Gengchen Mai', 'link': 'https://arxiv.org/abs/2505.05321', 'abstract': 'Accurate building segmentation from high-resolution RGB imagery remains challenging due to spectral similarity with non-building features, shadows, and irregular building geometries. In this study, we present a comprehensive deep learning framework for multiscale building segmentation using RGB aerial and satellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate a diverse, multi-sensor dataset and introduce feature-augmented inputs by deriving secondary representations including Principal Component Analysis (PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index (MBI), and Sobel edge filters from RGB channels. These features guide a Res-U-Net architecture in learning complex spatial patterns more effectively. We also propose training policies incorporating layer freezing, cyclical learning rates, and SuperConvergence to reduce training time and resource usage. Evaluated on a held-out WorldView-3 image, our model achieves an overall accuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of 0.80, outperforming existing RGB-based benchmarks. This study demonstrates the effectiveness of combining multi-resolution imagery, feature augmentation, and optimized training strategies for robust building segmentation in remote sensing applications.', 'abstract_zh': '高分辨率RGB影像中多尺度建筑分割的精确建模依然具有挑战性，这归因于其与非建筑特征、阴影和不规则建筑几何形状在光谱上的相似性。本研究提出了一种综合的深度学习框架，利用分辨率从0.4米到2.7米的RGB航空和卫星影像进行多尺度建筑分割。我们构建了一个多样化的多传感器数据集，并通过从RGB通道中衍生出二次表示，包括主成分分析（PCA）、可见差异植被指数（VDVI）、形态学建筑指数（MBI）和Sobel边缘滤波器来增强特征输入。这些特征引导Res-U-Net架构更有效地学习复杂的空间模式。我们还提出了一种结合层冻结、循环学习率和SuperConvergence的训练策略，以减少训练时间和资源消耗。在WorldView-3影像上进行评估，我们的模型总体准确率为96.5%，F1分数为0.86，交集覆蓋率（IoU）为0.80，优于现有基于RGB的基准。本研究展示了将多分辨率影像、特征增强以及优化的训练策略结合应用于遥感应用中进行稳健建筑分割的有效性。', 'title_zh': '特征增强深度网络在高分辨率无人机和卫星图像中的多尺度建筑分割'}
{'arxiv_id': 'arXiv:2505.05291', 'title': 'Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection', 'authors': 'Benjamin A. Cohen, Jonathan Fhima, Meishar Meisel, Baskin Meital, Luis Filipe Nakayama, Eran Berkowitz, Joachim A. Behar', 'link': 'https://arxiv.org/abs/2505.05291', 'abstract': 'Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to learn robust representations from large-scale natural image datasets, enhancing their generalization across domains. In retinal imaging, foundation models pretrained on either natural or ophthalmic data have shown promise, but the benefits of in-domain pretraining remain uncertain. To investigate this, we benchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets totaling 70,000 expert-annotated images for the task of moderate-to-late age-related macular degeneration (AMD) identification. Our results show that iBOT pretrained on natural images achieves the highest out-of-distribution generalization, with AUROCs of 0.80-0.97, outperforming domain-specific models, which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining, which achieved AUROCs of 0.68-0.91. These findings highlight the value of foundation models in improving AMD identification and challenge the assumption that in-domain pretraining is necessary. Furthermore, we release BRAMD, an open-access dataset (n=587) of DFIs with AMD labels from Brazil.', 'abstract_zh': '自监督学习（SSL）使视觉变换器（ViTs）能够从大规模自然图像数据集中学习 robust 表示，从而增强其跨领域的泛化能力。在视网膜成像中，预训练于自然数据或眼科数据的基础模型显示出前景，但领域内预训练的好处尚不明确。为了探究这一问题，我们对六种自监督预训练的ViTs在七个数字视网膜图像（DFI）数据集上进行了基准测试，这些数据集总计包含70,000张由专家标注的图像，用于中晚期年龄相关黄斑变性（AMD）识别任务。研究结果表明，预训练于自然图像的iBOT取得了最佳的跨领域泛化性能，AUROCs为0.80-0.97，优于专门领域模型（AUROCs为0.78-0.96）及无预训练的基础ViT-L（AUROCs为0.68-0.91）。这些发现突显了基础模型在提高AMD识别方面的价值，并挑战了领域内预训练必要的假设。此外，我们发布了BRAMD，这是一个包含来自巴西587张标注有AMD标签的数字视网膜图像的开源数据集。', 'title_zh': '眼科基础模型在临床显著性黄斑变性检测中的基准测试'}
{'arxiv_id': 'arXiv:2505.05054', 'title': 'Direct Image Classification from Fourier Ptychographic Microscopy Measurements without Reconstruction', 'authors': 'Navya Sonal Agarwal, Jan Philipp Schneider, Kanchana Vaishnavi Gandikota, Syed Muhammad Kazim, John Meshreki, Ivo Ihrke, Michael Moeller', 'link': 'https://arxiv.org/abs/2505.05054', 'abstract': 'The computational imaging technique of Fourier Ptychographic Microscopy (FPM) enables high-resolution imaging with a wide field of view and can serve as an extremely valuable tool, e.g. in the classification of cells in medical applications. However, reconstructing a high-resolution image from tens or even hundreds of measurements is computationally expensive, particularly for a wide field of view. Therefore, in this paper, we investigate the idea of classifying the image content in the FPM measurements directly without performing a reconstruction step first. We show that Convolutional Neural Networks (CNN) can extract meaningful information from measurement sequences, significantly outperforming the classification on a single band-limited image (up to 12 %) while being significantly more efficient than a reconstruction of a high-resolution image. Furthermore, we demonstrate that a learned multiplexing of several raw measurements allows maintaining the classification accuracy while reducing the amount of data (and consequently also the acquisition time) significantly.', 'abstract_zh': 'Fourier Ptychographic Microscopy中的图像内容直接分类：无需重建的高效分类方法', 'title_zh': '直接从傅里叶 Ptychographic 显微镜测量中进行图像分类无需重建。'}
{'arxiv_id': 'arXiv:2505.05001', 'title': 'StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps', 'authors': 'Lang Nie, Chunyu Lin, Kang Liao, Yun Zhang, Shuaicheng Liu, Yao Zhao', 'link': 'https://arxiv.org/abs/2505.05001', 'abstract': 'We retarget video stitching to an emerging issue, named warping shake, which unveils the temporal content shakes induced by sequentially unsmooth warps when extending image stitching to video stitching. Even if the input videos are stable, the stitched video can inevitably cause undesired warping shakes and affect the visual experience. To address this issue, we propose StabStitch++, a novel video stitching framework to realize spatial stitching and temporal stabilization with unsupervised learning simultaneously. First, different from existing learning-based image stitching solutions that typically warp one image to align with another, we suppose a virtual midplane between original image planes and project them onto it. Concretely, we design a differentiable bidirectional decomposition module to disentangle the homography transformation and incorporate it into our spatial warp, evenly spreading alignment burdens and projective distortions across two views. Then, inspired by camera paths in video stabilization, we derive the mathematical expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. Finally, a warp smoothing model is presented to produce stable stitched videos with a hybrid loss to simultaneously encourage content alignment, trajectory smoothness, and online collaboration. Compared with StabStitch that sacrifices alignment for stabilization, StabStitch++ makes no compromise and optimizes both of them simultaneously, especially in the online mode. To establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. Experiments exhibit that StabStitch++ surpasses current solutions in stitching performance, robustness, and efficiency, offering compelling advancements in this field by building a real-time online video stitching system.', 'abstract_zh': '面向 warp shake 问题的视频拼接研究：一种无监督学习驱动的空间拼接与 temporal 稳定同步框架', 'title_zh': 'StabStitch++: 基于时空双向变换的无监督在线视频拼接'}
{'arxiv_id': 'arXiv:2505.04946', 'title': 'T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models', 'authors': 'Xuyang Guo, Jiayan Huo, Zhenmei Shi, Zhao Song, Jiahao Zhang, Jiale Zhao', 'link': 'https://arxiv.org/abs/2505.04946', 'abstract': "Thanks to recent advancements in scalable deep architectures and large-scale pretraining, text-to-video generation has achieved unprecedented capabilities in producing high-fidelity, instruction-following content across a wide range of styles, enabling applications in advertising, entertainment, and education. However, these models' ability to render precise on-screen text, such as captions or mathematical formulas, remains largely untested, posing significant challenges for applications requiring exact textual accuracy. In this work, we introduce T2VTextBench, the first human-evaluation benchmark dedicated to evaluating on-screen text fidelity and temporal consistency in text-to-video models. Our suite of prompts integrates complex text strings with dynamic scene changes, testing each model's ability to maintain detailed instructions across frames. We evaluate ten state-of-the-art systems, ranging from open-source solutions to commercial offerings, and find that most struggle to generate legible, consistent text. These results highlight a critical gap in current video generators and provide a clear direction for future research aimed at enhancing textual manipulation in video synthesis.", 'abstract_zh': '基于文本的时间一致性屏幕文本评估基准：T2VTextBench', 'title_zh': 'T2VTextBench: 用于视频生成模型中文本控制的人工评估基准'}
{'arxiv_id': 'arXiv:2505.04888', 'title': 'Cross-Branch Orthogonality for Improved Generalization in Face Deepfake Detection', 'authors': 'Tharindu Fernando, Clinton Fookes, Sridha Sridharan, Simon Denman', 'link': 'https://arxiv.org/abs/2505.04888', 'abstract': 'Remarkable advancements in generative AI technology have given rise to a spectrum of novel deepfake categories with unprecedented leaps in their realism, and deepfakes are increasingly becoming a nuisance to law enforcement authorities and the general public. In particular, we observe alarming levels of confusion, deception, and loss of faith regarding multimedia content within society caused by face deepfakes, and existing deepfake detectors are struggling to keep up with the pace of improvements in deepfake generation. This is primarily due to their reliance on specific forgery artifacts, which limits their ability to generalise and detect novel deepfake types. To combat the spread of malicious face deepfakes, this paper proposes a new strategy that leverages coarse-to-fine spatial information, semantic information, and their interactions while ensuring feature distinctiveness and reducing the redundancy of the modelled features. A novel feature orthogonality-based disentanglement strategy is introduced to ensure branch-level and cross-branch feature disentanglement, which allows us to integrate multiple feature vectors without adding complexity to the feature space or compromising generalisation. Comprehensive experiments on three public benchmarks: FaceForensics++, Celeb-DF, and the Deepfake Detection Challenge (DFDC) show that these design choices enable the proposed approach to outperform current state-of-the-art methods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a cross-dataset evaluation setting.', 'abstract_zh': '显著的生成AI技术进步催生了一系列前所未有的高真实度深伪类别，深伪内容日益成为执法机构和普通公众的困扰。特别是，我们观察到社会上关于多媒体内容存在着令人担忧的混淆、欺骗和信任损失现象，现有的深伪检测方法难以跟上深伪生成技术的快速发展。这主要是因为它们依赖于特定的伪造特征，限制了它们对新型深伪类型的泛化和检测能力。为应对恶意面部深伪的传播，本文提出了一种新策略，该策略结合了粗细尺度空间信息、语义信息及其交互作用，同时确保特征的差异性和减少模型特征的冗余性。引入了一种新颖的基于特征正交性的解耦策略，确保分支级和跨分支特征的解耦，从而在不会增加特征空间复杂性或损害泛化能力的情况下，能够整合多个特征向量。在三个公开基准FaceForensics++、Celeb-DF和Deepfake Detection Challenge (DFDC)上的全面实验表明，这些设计选择使所提出的方法在Celeb-DF数据集上的检测性能比当前最先进的方法高出5%，在DFDC数据集上的检测性能高出7%，特别是在跨数据集评估设置中。', 'title_zh': '跨支路正交性以提高面部深度合成检测的泛化能力'}
{'arxiv_id': 'arXiv:2505.04864', 'title': 'Auto-regressive transformation for image alignment', 'authors': 'Kanggeon Lee, Soochahn Lee, Kyoung Mu Lee', 'link': 'https://arxiv.org/abs/2505.04864', 'abstract': 'Existing methods for image alignment struggle in cases involving feature-sparse regions, extreme scale and field-of-view differences, and large deformations, often resulting in suboptimal accuracy. Robustness to these challenges improves through iterative refinement of the transformation field while focusing on critical regions in multi-scale image representations. We thus propose Auto-Regressive Transformation (ART), a novel method that iteratively estimates the coarse-to-fine transformations within an auto-regressive framework. Leveraging hierarchical multi-scale features, our network refines the transformations using randomly sampled points at each scale. By incorporating guidance from the cross-attention layer, the model focuses on critical regions, ensuring accurate alignment even in challenging, feature-limited conditions. Extensive experiments across diverse datasets demonstrate that ART significantly outperforms state-of-the-art methods, establishing it as a powerful new method for precise image alignment with broad applicability.', 'abstract_zh': '现有的图像对齐方法在特征稀疏区域、极端尺度和视场差异以及大形变的情况下表现不佳，常常导致亚最优的准确性。通过在多尺度图像表示中迭代细化变换场并聚焦关键区域，可以提高对这些挑战的鲁棒性。因此，我们提出了自回归变换（ART），这是一种在自回归框架内迭代估计粗到细变换的新方法。利用分层多尺度特征，我们的网络在每个尺度上随机采样点以细化变换。通过引入交叉注意力层的指导，模型能够聚焦于关键区域，即使在特征受限的挑战条件下也能确保准确对齐。广泛的实验证明，ART在多个数据集上显著优于现有最先进的方法，确立了其作为一种适用于精确图像对齐的强大新方法的地位。', 'title_zh': '自动回归转换for图像对齐'}
{'arxiv_id': 'arXiv:2505.04802', 'title': 'ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling', 'authors': 'Xiao Wang, Jong-Youl Choi, Takuya Kurihaya, Isaac Lyngaas, Hong-Jun Yoon, Ming Fan, Nasik Muhammad Nafi, Aristeidis Tsaris, Ashwin M. Aji, Maliha Hossain, Mohamed Wahib, Dali Wang, Peter Thornton, Prasanna Balaprakash, Moetasim Ashfaq, Dan Lu', 'link': 'https://arxiv.org/abs/2505.04802', 'abstract': 'Sparse observations and coarse-resolution climate models limit effective regional decision-making, underscoring the need for robust downscaling. However, existing AI methods struggle with generalization across variables and geographies and are constrained by the quadratic complexity of Vision Transformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundation model for global, hyper-resolution climate downscaling. ORBIT-2 incorporates two key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecture with residual learning and Bayesian regularization for efficient, robust prediction; and (2) TILES, a tile-wise sequence scaling algorithm that reduces self-attention complexity from quadratic to linear, enabling long-sequence processing and massive parallelism. ORBIT-2 scales to 10 billion parameters across 32,768 GPUs, achieving up to 1.8 ExaFLOPS sustained throughput and 92-98% strong scaling efficiency. It supports downscaling to 0.9 km global resolution and processes sequences up to 4.2 billion tokens. On 7 km resolution benchmarks, ORBIT-2 achieves high accuracy with R^2 scores in the range of 0.98 to 0.99 against observation data.', 'abstract_zh': '稀疏的观测数据和粗分辨率的气候模型限制了有效的区域决策制定，强调了需要稳健的降尺度方法。然而，现有的AI方法在变量和地理方面的泛化能力有限，并且受到Vision Transformer（ViT）自注意力二次复杂性的约束。我们引入了ORBIT-2，这是一种适用于全球高分辨率气候降尺度的大规模基础模型。ORBIT-2 包含两项关键创新：（1）残差轻量级ViT（Reslim），一种具有残差学习和贝叶斯正则化的轻量级架构，用于高效稳健的预测；（2）适用于序列缩放的TILES算法，将自注意力复杂性从二次降低到线性，从而实现长序列处理和大规模并行处理。ORBIT-2 跨32,768块GPU可扩展至100亿参数，实现高达1.8 ExaFLOPS的持续吞吐量，并具有92-98%的强扩展效率。它支持将其分辨率细分为0.9公里的全球分辨率，并处理最长42亿个标记的序列。在7公里分辨率的基准测试中，ORBIT-2 在观测数据的 R² 得分范围为0.98到0.99，实现了高精度。', 'title_zh': 'ORBIT-2: 扩展级谢스cale视觉基础模型用于天气和气候下scalings'}
{'arxiv_id': 'arXiv:2505.04664', 'title': 'Advancing 3D Medical Image Segmentation: Unleashing the Potential of Planarian Neural Networks in Artificial Intelligence', 'authors': 'Ziyuan Huang, Kevin Huggins, Srikar Bellur', 'link': 'https://arxiv.org/abs/2505.04664', 'abstract': "Our study presents PNN-UNet as a method for constructing deep neural networks that replicate the planarian neural network (PNN) structure in the context of 3D medical image data. Planarians typically have a cerebral structure comprising two neural cords, where the cerebrum acts as a coordinator, and the neural cords serve slightly different purposes within the organism's neurological system. Accordingly, PNN-UNet comprises a Deep-UNet and a Wide-UNet as the nerve cords, with a densely connected autoencoder performing the role of the brain. This distinct architecture offers advantages over both monolithic (UNet) and modular networks (Ensemble-UNet). Our outcomes on a 3D MRI hippocampus dataset, with and without data augmentation, demonstrate that PNN-UNet outperforms the baseline UNet and several other UNet variants in image segmentation.", 'abstract_zh': '我们的研究提出了PNN-UNet方法，用于在3D医学图像数据中构建模仿扁虫神经网络（PNN）结构的深度神经网络。PNN-UNet包括一个Deep-UNet和一个Wide-UNet作为神经索，而一个密集连接的自编码器执行大脑的功能。这种独特的架构在整体（UNet）和模块化网络（Ensemble-UNet）之上具有优势。在无数据增强和有数据增强的3D MRI海马体数据集上的结果表明，PNN-UNet在图像分割方面优于基线UNet和其他几种UNet变体。', 'title_zh': '推进3D医学图像分割：释放Planarian神经网络在人工智能中的潜力'}
