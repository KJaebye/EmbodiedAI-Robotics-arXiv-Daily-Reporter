{'arxiv_id': 'arXiv:2505.05396', 'title': 'A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods', 'authors': 'Stefanos Gkikas', 'link': 'https://arxiv.org/abs/2505.05396', 'abstract': 'From the original abstract:\nThis thesis initially aims to study the pain assessment process from a clinical-theoretical perspective while exploring and examining existing automatic approaches. Building on this foundation, the primary objective of this Ph.D. project is to develop innovative computational methods for automatic pain assessment that achieve high performance and are applicable in real clinical settings. A primary goal is to thoroughly investigate and assess significant factors, including demographic elements that impact pain perception, as recognized in pain research, through a computational standpoint. Within the limits of the available data in this research area, our goal was to design, develop, propose, and offer automatic pain assessment pipelines for unimodal and multimodal configurations that are applicable to the specific requirements of different scenarios. The studies published in this Ph.D. thesis showcased the effectiveness of the proposed methods, achieving state-of-the-art results. Additionally, they paved the way for exploring new approaches in artificial intelligence, foundation models, and generative artificial intelligence.', 'abstract_zh': '本论文最初旨在从临床理论视角研究疼痛评估过程，并探索和检验现有的自动评估方法。在此基础上，本博士项目的主要目标是开发高性能且适用于实际临床环境的创新计算方法进行自动疼痛评估。主要目标之一是从计算角度全面研究和评估影响疼痛感知的关键因素，包括疼痛研究中认可的人口统计学因素。在这一研究领域的数据限制下，我们旨在设计、开发、提出并提供适用于不同场景特定需求的一模性和多模性自动疼痛评估管道。本博士论文中发表的研究展示了所提出方法的有效性，达到了最先进的成果，并为探索人工智能、基础模型和生成式人工智能的新方法奠定了基础。', 'title_zh': '基于多模态数据和深度机器学习的疼痛评估框架'}
{'arxiv_id': 'arXiv:2505.05422', 'title': 'TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation', 'authors': 'Haokun Lin, Teng Wang, Yixiao Ge, Yuying Ge, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun, Ying Shan', 'link': 'https://arxiv.org/abs/2505.05422', 'abstract': 'Pioneering token-based works such as Chameleon and Emu3 have established a foundation for multimodal unification but face challenges of high training computational overhead and limited comprehension performance due to a lack of high-level semantics. In this paper, we introduce TokLIP, a visual tokenizer that enhances comprehension by semanticizing vector-quantized (VQ) tokens and incorporating CLIP-level semantics while enabling end-to-end multimodal autoregressive training with standard VQ tokens. TokLIP integrates a low-level discrete VQ tokenizer with a ViT-based token encoder to capture high-level continuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize high-level features, TokLIP disentangles training objectives for comprehension and generation, allowing the direct application of advanced VQ tokenizers without the need for tailored quantization operations. Our empirical results demonstrate that TokLIP achieves exceptional data efficiency, empowering visual tokens with high-level semantic understanding while enhancing low-level generative capacity, making it well-suited for autoregressive Transformers in both comprehension and generation tasks. The code and models are available at this https URL.', 'abstract_zh': '基于令牌的先驱工作Chameleon和Emu3为多模态统一奠定了基础，但由于缺乏高层语义，面临高额训练计算开销和有限理解性能的挑战。本文介绍TokLIP，一个通过语义化向量量化（VQ）令牌和融入CLIP级语义来提升理解能力的同时，支持使用标准VQ令牌进行端到端多模态自回归训练的视觉令牌化器。TokLIP将低级离散VQ令牌化器与基于ViT的令牌编码器结合，以捕捉高层连续语义。与此前将高层特征离散化的做法（例如VILA-U）不同，TokLIP分离了理解与生成的训练目标，允许直接应用高级VQ令牌化器，无需特定的量化操作。我们的实验证明，TokLIP在数据效率方面表现出色，赋予视觉令牌高层语义理解能力，同时增强低级生成能力，使其适合于理解与生成任务中的自回归Transformer。代码和模型可在如下链接获取：this https URL。', 'title_zh': 'TokLIP：将视觉令牌与CLIP结合用于多模态理解和生成'}
{'arxiv_id': 'arXiv:2505.05318', 'title': 'Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects', 'authors': 'Agnese Chiatti, Sara Bernardini, Lara Shibelski Godoy Piccolo, Viola Schiaffonati, Matteo Matteucci', 'link': 'https://arxiv.org/abs/2505.05318', 'abstract': 'The rapid adoption of Vision Language Models (VLMs), pre-trained on large image-text and video-text datasets, calls for protecting and informing users about when to trust these systems. This survey reviews studies on trust dynamics in user-VLM interactions, through a multi-disciplinary taxonomy encompassing different cognitive science capabilities, collaboration modes, and agent behaviours. Literature insights and findings from a workshop with prospective VLM users inform preliminary requirements for future VLM trust studies.', 'abstract_zh': '视觉语言模型（VLMs）的快速采纳，这些模型预先在大规模图像-文本和视频-文本数据集上训练，要求保护并告知用户在何时信任这些系统。本文综述了用户-VLM互动中信任动态的研究，通过涵盖不同认知科学能力、合作模式和代理行为的多学科分类体系来进行。文献综述和面向潜在VLM用户的研讨会成果为未来VLM信任研究提供了初步要求。', 'title_zh': '视觉语言模型中用户信任的映射：研究概览、挑战与前景'}
{'arxiv_id': 'arXiv:2505.05071', 'title': 'FG-CLIP: Fine-Grained Visual and Textual Alignment', 'authors': 'Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Gengshen Zhang, Dawei Leng, Yuhui Yin', 'link': 'https://arxiv.org/abs/2505.05071', 'abstract': "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The related data, code, and models are available at this https URL.", 'abstract_zh': '细粒度CLIP（FG-CLIP）：通过三种关键创新增强细粒度理解', 'title_zh': 'FG-CLIP: 细粒度视觉和文本对齐'}
{'arxiv_id': 'arXiv:2505.05040', 'title': 'Image-Text Relation Prediction for Multilingual Tweets', 'authors': 'Matīss Rikters, Edison Marrese-Taylor', 'link': 'https://arxiv.org/abs/2505.05040', 'abstract': 'Various social networks have been allowing media uploads for over a decade now. Still, it has not always been clear what is their relation with the posted text or even if there is any at all. In this work, we explore how multilingual vision-language models tackle the task of image-text relation prediction in different languages, and construct a dedicated balanced benchmark data set from Twitter posts in Latvian along with their manual translations into English. We compare our results to previous work and show that the more recently released vision-language model checkpoints are becoming increasingly capable at this task, but there is still much room for further improvement.', 'abstract_zh': '各种社交网络允许用户上传媒体已有十多年的时间，但这些媒体与发布的文本之间的关系尚不明确，甚至有时并不存在任何关系。在这项工作中，我们探究多语言视觉-语言模型在不同语言下如何处理图像-文本关系预测任务，并从拉脱维亚语的Twitter帖子及其手工翻译成英文的数据集中构建了一个专门的平衡基准数据集。我们将我们的结果与以往的工作进行比较，并表明最近发布的视觉-语言模型预训练检查点在这一任务上的能力不断提升，但仍有很多改进的空间。', 'title_zh': '多语言推文中图像-文本关系预测'}
{'arxiv_id': 'arXiv:2505.04653', 'title': 'Advancing Conversational Diagnostic AI with Multimodal Reasoning', 'authors': 'Khaled Saab, Jan Freyberg, Chunjong Park, Tim Strother, Yong Cheng, Wei-Hung Weng, David G.T. Barrett, David Stutz, Nenad Tomasev, Anil Palepu, Valentin Liévin, Yash Sharma, Roma Ruparel, Abdullah Ahmed, Elahe Vedadi, Kimberly Kanada, Cian Hughes, Yun Liu, Geoff Brown, Yang Gao, Sean Li, S. Sara Mahdavi, James Manyika, Katherine Chou, Yossi Matias, Avinatan Hassidim, Dale R. Webster, Pushmeet Kohli, S.M. Ali Eslami, Joëlle Barral, Adam Rodman, Vivek Natarajan, Mike Schaekermann, Tao Tu, Alan Karthikesalingam, Ryutaro Tanno', 'link': 'https://arxiv.org/abs/2505.04653', 'abstract': 'Large Language Models (LLMs) have demonstrated great potential for conducting diagnostic conversations but evaluation has been largely limited to language-only interactions, deviating from the real-world requirements of remote care delivery. Instant messaging platforms permit clinicians and patients to upload and discuss multimodal medical artifacts seamlessly in medical consultation, but the ability of LLMs to reason over such data while preserving other attributes of competent diagnostic conversation remains unknown. Here we advance the conversational diagnosis and management performance of the Articulate Medical Intelligence Explorer (AMIE) through a new capability to gather and interpret multimodal data, and reason about this precisely during consultations. Leveraging Gemini 2.0 Flash, our system implements a state-aware dialogue framework, where conversation flow is dynamically controlled by intermediate model outputs reflecting patient states and evolving diagnoses. Follow-up questions are strategically directed by uncertainty in such patient states, leading to a more structured multimodal history-taking process that emulates experienced clinicians. We compared AMIE to primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of chat-based consultations with patient actors. We constructed 105 evaluation scenarios using artifacts like smartphone skin photos, ECGs, and PDFs of clinical documents across diverse conditions and demographics. Our rubric assessed multimodal capabilities and other clinically meaningful axes like history-taking, diagnostic accuracy, management reasoning, communication, and empathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9 multimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The results show clear progress in multimodal conversational diagnostic AI, but real-world translation needs further research.', 'abstract_zh': '大型语言模型（LLMs）在进行诊断对话方面展现了巨大潜力，但目前的评估主要集中在语言交互上，偏离了远程医疗服务的实际需求。即时通讯平台允许医生和患者在医疗咨询中无缝上传和讨论多模态医疗 artifacts，但大型语言模型在处理这类数据的同时保留高效诊断对话的其他特征的能力仍未知。通过引入一种新的能力来收集和解释多模态数据，并在咨询过程中精确地推理这些数据，我们提升了综合医学智能探索器（AMIE）的对话诊断和管理性能。利用Gemini 2.0 Flash，我们的系统实现了具备状态感知的对话框架，其中对话流程动态受到反映患者状态和演化诊断的中间模型输出的控制。策略性地引导后续问题依据患者状态的不确定性，这会产生一个结构化的多模态病史采集过程，逼近临床经验丰富的医生。我们在一项随机、盲法、类似OSCE的基于聊天的咨询研究中将AMIE与全科医生（PCPs）进行了比较。我们构建了105个评估场景，使用智能手机皮肤照片、心电图以及临床文件的PDF等多样的artifact，覆盖不同的条件和人口统计信息。我们的评分标准评估了多模态能力及其他具有临床意义的维度，如病史采集、诊断准确性、管理推理、沟通和同理心。专科评估显示，AMIE在7/9个多模态轴和29/32个非多模态轴（包括诊断准确性）上优于全科医生。结果表明，在多模态对话诊断AI方面取得了明显的进步，但实际应用仍需进一步研究。', 'title_zh': '基于多模态推理推进对话诊断AI'}
{'arxiv_id': 'arXiv:2505.04650', 'title': 'Multimodal Benchmarking and Recommendation of Text-to-Image Generation Models', 'authors': 'Kapil Wanaskar, Gaytri Jena, Magdalini Eirinaki', 'link': 'https://arxiv.org/abs/2505.04650', 'abstract': 'This work presents an open-source unified benchmarking and evaluation framework for text-to-image generation models, with a particular focus on the impact of metadata augmented prompts. Leveraging the DeepFashion-MultiModal dataset, we assess generated outputs through a comprehensive set of quantitative metrics, including Weighted Score, CLIP (Contrastive Language Image Pre-training)-based similarity, LPIPS (Learned Perceptual Image Patch Similarity), FID (Frechet Inception Distance), and retrieval-based measures, as well as qualitative analysis. Our results demonstrate that structured metadata enrichments greatly enhance visual realism, semantic fidelity, and model robustness across diverse text-to-image architectures. While not a traditional recommender system, our framework enables task-specific recommendations for model selection and prompt design based on evaluation metrics.', 'abstract_zh': '本研究提出了一种开源统一的文本到图像生成模型基准测试与评估框架，特别关注元数据增强提示的影响。通过利用DeepFashion-MultiModal数据集，我们使用一系列定量指标进行了评估，包括加权分数、CLIP基于的相似性、LPIPS（学习感知图像块相似性）、FID（弗谢赫特 inception 距离）以及检索基评估指标，并结合定性分析。我们的结果表明，结构化的元数据增强显著提升了跨多种文本到图像架构的视觉真实性、语义准确性和模型稳健性。虽然不是传统的推荐系统，但该框架基于评估指标提供了任务特定的模型选择和提示设计建议。', 'title_zh': '多模态文本到图像生成模型的基准测试与推荐'}
{'arxiv_id': 'arXiv:2505.04642', 'title': 'Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture', 'authors': 'Nischal Mandal, Yang Li', 'link': 'https://arxiv.org/abs/2505.04642', 'abstract': 'Multimodal sentiment analysis, a pivotal task in affective computing, seeks to understand human emotions by integrating cues from language, audio, and visual signals. While many recent approaches leverage complex attention mechanisms and hierarchical architectures, we propose a lightweight, yet effective fusion-based deep learning model tailored for utterance-level emotion classification. Using the benchmark IEMOCAP dataset, which includes aligned text, audio-derived numeric features, and visual descriptors, we design a modality-specific encoder using fully connected layers followed by dropout regularization. The modality-specific representations are then fused using simple concatenation and passed through a dense fusion layer to capture cross-modal interactions. This streamlined architecture avoids computational overhead while preserving performance, achieving a classification accuracy of 92% across six emotion categories. Our approach demonstrates that with careful feature engineering and modular design, simpler fusion strategies can outperform or match more complex models, particularly in resource-constrained environments.', 'abstract_zh': '多模态情感分析是情感计算中的一个关键任务，旨在通过综合语言、音频和视觉信号来理解人类情感。虽然许多最近的方法利用了复杂的注意力机制和分层结构，我们提出了一种轻量级但有效的基于融合的深度学习模型，专门用于短语级别情感分类。利用包含对齐文本、音频衍生数值特征和视觉描述的基准IEMOCAP数据集，我们设计了模态特定编码器，使用全连接层后接 dropout 正则化。模态特定表示随后通过简单的串联融合，并通过密集融合层捕捉跨模态交互。这种精简的架构避免了计算开销同时保持了性能，实现了六个情感类别上的分类准确率92%。我们的方法证明了在精心特征工程和模块化设计下，简单的融合策略可以在资源受限环境中超越或匹配更复杂模型。', 'title_zh': '重塑多模态情感分析：一种高精度简化融合架构'}
