{'arxiv_id': 'arXiv:2510.09817', 'title': 'Cross-Sensor Touch Generation', 'authors': 'Samanta Rodriguez, Yiming Dou, Miquel Oller, Andrew Owens, Nima Fazeli', 'link': 'https://arxiv.org/abs/2510.09817', 'abstract': "Today's visuo-tactile sensors come in many shapes and sizes, making it challenging to develop general-purpose tactile representations. This is because most models are tied to a specific sensor design. To address this challenge, we propose two approaches to cross-sensor image generation. The first is an end-to-end method that leverages paired data (Touch2Touch). The second method builds an intermediate depth representation and does not require paired data (T2D2: Touch-to-Depth-to-Touch). Both methods enable the use of sensor-specific models across multiple sensors via the cross-sensor touch generation process. Together, these models offer flexible solutions for sensor translation, depending on data availability and application needs. We demonstrate their effectiveness on downstream tasks such as in-hand pose estimation and behavior cloning, successfully transferring models trained on one sensor to another. Project page: this https URL.", 'abstract_zh': '今天的手眼触觉传感器种类多样，这给开发通用的触觉表示带来了挑战。由于大多数模型都与特定传感器设计绑定，我们提出了两种跨传感器图像生成的方法。第一种是端到端的方法，利用配对数据（Touch2Touch）。第二种方法构建中间的深度表示，不需要配对数据（T2D2：Touch-to-Depth-to-Touch）。这两种方法通过跨传感器触觉生成过程，使得多种传感器可以使用传感器特定模型。这些模型根据数据可用性和应用需求提供了灵活的传感器转换解决方案。我们在手内姿态估计和行为克隆等下游任务中展示了它们的有效性，并成功地将一种传感器上训练的模型迁移到另一种传感器。项目页面：请点击这里。', 'title_zh': '跨传感器触觉生成'}
{'arxiv_id': 'arXiv:2510.10689', 'title': 'OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs', 'authors': 'Caorui Li, Yu Chen, Yiyan Ji, Jin Xu, Zhenyu Cui, Shihao Li, Yuanxing Zhang, Jiafu Tang, Zhenghao Song, Dingling Zhang, Ying He, Haoxiang Liu, Yuxuan Wang, Qiufeng Wang, Zhenhe Wu, Jiehui Luo, Zhiyu Pan, Weihao Xie, Chenchen Zhang, Zhaohui Wang, Jiayi Tian, Yanghai Wang, Zhe Cao, Minxin Dai, Ke Wang, Runzhe Wen, Yinghao Ma, Yaning Pan, Sungkyun Chang, Termeh Taheri, Haiwen Xia, Christos Plachouras, Emmanouil Benetos, Yizhi Li, Ge Zhang, Jian Yang, Tianhao Peng, Zili Wang, Minghao Liu, Junran Peng, Zhaoxiang Zhang, Jiaheng Liu', 'link': 'https://arxiv.org/abs/2510.10689', 'abstract': 'Recent advances in multimodal large language models (MLLMs) have demonstrated substantial potential in video understanding. However, existing benchmarks fail to comprehensively evaluate synergistic reasoning capabilities across audio and visual modalities, often neglecting either one of the modalities or integrating them in a logically inconsistent manner. To bridge this gap, we introduce OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to assessing synergistic audio-visual understanding, with a strong emphasis on modality complementarity and logical consistency. Specifically, OmniVideoBench comprises 1000 high-quality question-answer(QA) pairs, each annotated with step-by-step reasoning traces, derived from 628 diverse videos ranging from several seconds to 30 minutes, and manually verified to guarantee complete correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully designed question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, and beyond, thereby capturing the essential challenges of video understanding. Evaluation of multiple MLLMs on OmniVideoBench reveals a pronounced gap between model performance and human reasoning, with open-source models lagging significantly behind their closed-source counterparts, underscoring the inherent difficulty of genuine audio-visual reasoning. We will release OmniVideoBench to foster the development of MLLMs with stronger and more generalizable reasoning capabilities.', 'abstract_zh': '最近多模态大型语言模型（MLLMs）在视频理解方面展现出巨大的潜力。然而，现有的基准未能全面评估跨音频和视觉模态的协同推理能力，往往会忽视其中一种模态或者以逻辑不一致的方式整合它们。为填补这一空白，我们提出了一种名为OmniVideoBench的大规模严格设计基准，专注于评估协同的音频-视觉理解，特别强调模态互补性和逻辑一致性。具体而言，OmniVideoBench包含了1000个高质量的问题-答案（QA）对，每个问题-答案对都带有逐步推理痕迹，并基于628种多样性的视频（时长从几秒钟到30分钟不等）手工标注，以确保完全正确性和独特性。此外，OmniVideoBench涵盖了13种精心设计的问题类型，涵盖了时间推理、空间定位、计数、因果推理、总结等方面，从而捕捉到了视频理解的核心挑战。在OmniVideoBench上对多种MLLMs的评估显示，模型性能与人类推理之间存在显著差距，开源模型远落后于闭源模型，强调了真正意义上的音频-视觉推理的固有难度。我们将发布OmniVideoBench，以促进具有更强更通用推理能力的MLLMs的发展。', 'title_zh': '全方位视频基准：面向全方位MLLMs的视听理解评估'}
{'arxiv_id': 'arXiv:2510.10633', 'title': 'Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion', 'authors': 'Jiabao Shi, Minfeng Qi, Lefeng Zhang, Di Wang, Yingjie Zhao, Ziying Li, Yalong Xing, Ningran Li', 'link': 'https://arxiv.org/abs/2510.10633', 'abstract': 'Multimodal text-to-image generation remains constrained by the difficulty of maintaining semantic alignment and professional-level detail across diverse visual domains. We propose a multi-agent reinforcement learning framework that coordinates domain-specialized agents (e.g., focused on architecture, portraiture, and landscape imagery) within two coupled subsystems: a text enhancement module and an image generation module, each augmented with multimodal integration components. Agents are trained using Proximal Policy Optimization (PPO) under a composite reward function that balances semantic similarity, linguistic visual quality, and content diversity. Cross-modal alignment is enforced through contrastive learning, bidirectional attention, and iterative feedback between text and image. Across six experimental settings, our system significantly enriches generated content (word count increased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion methods, Transformer-based strategies achieve the highest composite score (0.521), despite occasional stability issues. Multimodal ensembles yield moderate consistency (ranging from 0.444 to 0.481), reflecting the persistent challenges of cross-modal semantic grounding. These findings underscore the promise of collaborative, specialization-driven architectures for advancing reliable multimodal generative systems.', 'abstract_zh': '多模态文本到图像生成仍然受限于在多种视觉领域中维持语义对齐和专业级细节的难度。我们提出了一种多智能体强化学习框架，该框架协调专注于建筑、肖像和风景图像的专业智能体，并在两个耦合子系统中协同工作：文本增强模块和图像生成模块，每个模块都增加了多模态集成组件。智能体使用加权策略梯度优化（PPO）在综合奖励函数下训练，该函数平衡了语义相似性、语义视觉质量和内容多样性。通过对比学习、双向注意力和文本与图像的迭代反馈来确保跨模态对齐。在六个实验设置中，我们的系统显著丰富了生成内容（词汇量增加了1614%）同时降低了ROUGE-1分数69.7%。在融合方法中，基于Transformer的策略获得了最高的综合分数（0.521），尽管偶尔存在稳定性问题。多模态集成展现出中等的一致性（从0.444到0.481），反映了跨模态语义接地的持续挑战。这些发现强调了协作和专业化驱动架构在推进可靠多模态生成系统方面的潜力。', 'title_zh': '多 agent 强化学习与语义融合的协作文本到图像生成'}
{'arxiv_id': 'arXiv:2510.10285', 'title': 'Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control', 'authors': 'Haolang Lu, Bolun Chu, WeiYe Fu, Guoshun Nan, Junning Liu, Minghui Pan, Qiankun Li, Yi Yu, Hua Wang, Kun Wang', 'link': 'https://arxiv.org/abs/2510.10285', 'abstract': 'Multimodal large reasoning models (MLRMs) are rapidly advancing vision-language reasoning and are emerging as a foundation for cross-modal intelligence. Hallucination remains a persistent failure mode, manifesting itself as erroneous reasoning chains and misinterpretation of visual content. In this study, we observe that attention heads exhibit a staged division: shallow heads predominantly serve perception, while deeper heads shift toward symbolic reasoning, revealing two major causes of hallucination, namely perceptual bias and reasoning drift. To address these issues, we propose a lightweight and interpretable two-step plugin, Functional Head Identification and Class-conditioned Rescaling, which locates perception- and reasoning-oriented heads and regulates their contributions without retraining. Evaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six benchmarks across three domains, and four baselines show that our plugin achieves an average improvement of 5% and up to 15%, with only <1% additional computation and 9% of baseline latency. Our approach is completely model-agnostic and significantly enhances both the reliability and interpretability of the off-the-shelf MLRMs, thereby enabling their safe deployment in high-stakes applications. Our code is available at this https URL.', 'abstract_zh': '多模态大型推理模型中的幻觉现象：注意力头部的阶段划分与功能辨识及条件归一化插件研究', 'title_zh': '通过功能注意力控制减轻多模态推理中的幻觉'}
{'arxiv_id': 'arXiv:2510.10117', 'title': 'DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay', 'authors': 'Yunxiang Mo, Tianshi Zheng, Qing Zong, Jiayu Liu, Baixuan Xu, Yauwai Yim, Chunkit Chan, Jiaxin Bai, Yangqiu Song', 'link': 'https://arxiv.org/abs/2510.10117', 'abstract': 'Multimodal abductive reasoning--the generation and selection of explanatory hypotheses from partial observations--is a cornerstone of intelligence. Current evaluations of this ability in vision-language models (VLMs) are largely confined to static, single-agent tasks. Inspired by Dixit, we introduce DixitWorld, a comprehensive evaluation suite designed to deconstruct this challenge. DIXITWORLD features two core components: DixitArena, a dynamic, multi-agent environment that evaluates both hypothesis generation (a "storyteller" crafting cryptic clues) and hypothesis selection ("listeners" choosing the target image from decoys) under imperfect information; and DixitBench, a static QA benchmark that isolates the listener\'s task for efficient, controlled evaluation. Results from DixitArena reveal distinct, role-dependent behaviors: smaller open-source models often excel as creative storytellers, producing imaginative yet less discriminative clues, whereas larger proprietary models demonstrate superior overall performance, particularly as listeners. Performance on DixitBench strongly correlates with listener results in DixitArena, validating it as a reliable proxy for hypothesis selection. Our findings reveal a key trade-off between generative creativity and discriminative understanding in multimodal abductive reasoning, a central challenge for developing more balanced and capable vision-language agents.', 'abstract_zh': '多模态归因推理——从部分观察中生成和选择解释性假设——是智能的基础。受迪克特启发，我们引入了DixitWorld，一个全面的评估套件，旨在分解这一挑战。DIXITWORLD包含两个核心组件：DixitArena，一个动态的多代理环境，评估在不完全信息条件下假设生成（“讲故事者”创作含糊的线索）和假设选择（“听众”从诱饵中选择目标图像）的能力；以及DixitBench，一个静态的问答基准，孤立听众的任务，以便进行高效的可控评估。DixitArena的结果显示了角色依赖的行为：较小的开源模型往往在创造性的讲故事者方面表现出色，产生富有想象力但区分性较差的线索，而较大的专有模型则在总体表现上表现出色，尤其是在听众方面。DixitBench上的性能与DixitArena中听众的表现高度相关，验证了其作为假设选择可靠代理的有效性。我们的研究揭示了生成创造性和区分性理解之间的关键权衡，这是开发更加平衡和有能力的视觉-语言代理的核心挑战。', 'title_zh': 'DixitWorld：多模态演绎推理在视觉-语言模型中的评估——基于多-agent Dixit游戏 chơi'}
{'arxiv_id': 'arXiv:2510.10069', 'title': 'SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation', 'authors': 'Zeyu Ling, Xiaodong Gu, Jiangnan Tang, Changqing Zou', 'link': 'https://arxiv.org/abs/2510.10069', 'abstract': 'We introduce SyncLipMAE, a self-supervised pretraining framework for talking-face video that learns synchronization-aware and transferable facial dynamics from unlabeled audio-visual streams. Our approach couples masked visual modeling with cross-modal contrastive alignment and employs three per-frame prompt tokens that explicitly encode the essential factors of a talking-face frame - identity, vocal motion (speech-synchronized facial dynamics), and ambient motion (audio-agnostic movements such as blinks and head pose). The contrastive objective uses time-aligned vocal-motion and audio tokens as positives and misaligned pairs as negatives, driving both modalities into a shared embedding space and yielding token-level audio-visual stream synchronization. After pretraining, the aligned audio tokens together with the visual prompt tokens (identity, vocal motion, ambient motion) form a unified interface for four disparate downstream settings: (i) audio-visual stream synchronization; (ii) facial emotion and head/face action recognition; (iii) visual speech recognition; and (iv) visual dubbing, for which we enable indistinguishable audio- or video-driven control within a single model. Across four task families that require distinct capabilities, SyncLipMAE achieves state-of-the-art results, underscoring the effectiveness of synchronization-aware, factorized self-supervised pretraining.', 'abstract_zh': 'SyncLipMAE：一种面向 talking-face 视频的自监督预训练框架，学习同步感知和可迁移的面部动态', 'title_zh': 'SyncLipMAE：音频-视觉说话人脸表示的对比掩码预训练'}
{'arxiv_id': 'arXiv:2510.11718', 'title': 'CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images', 'authors': 'Chengqi Duan, Kaiyue Sun, Rongyao Fang, Manyuan Zhang, Yan Feng, Ying Luo, Yufang Liu, Ke Wang, Peng Pei, Xunliang Cai, Hongsheng Li, Yi Ma, Xihui Liu', 'link': 'https://arxiv.org/abs/2510.11718', 'abstract': 'Recent advances in Large Language Models (LLMs) and Vision Language Models (VLMs) have shown significant progress in mathematical reasoning, yet they still face a critical bottleneck with problems requiring visual assistance, such as drawing auxiliary lines or plotting functions to solve the problems. Most LLMs and VLMs are constrained to text-only reasoning chains, while multimodal unified models that can generate interleaved text and images lack the necessary precision and controllability for such tasks. To address this, we propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for "thinking with images" in mathematics. Our approach leverages the VLM to generate text reasoning as well as executable plotting code, which is then rendered into images as "visual thought", to solve mathematical problems. To achieve this, we first construct Math-VR, the first large-scale, bilingual dataset and benchmark for Mathematics problems with Visual Reasoning, comprising 178K samples. Second, to create high-quality training data, we develop a state-of-the-art image-to-code converter specialized for parsing complex mathematical figures into codes. Finally, using these training data, we train the CodePlot-CoT model for solving mathematical problems. Experimental results show that our model achieves up to 21% increase over base model on our new benchmark, fully validating the efficacy of our proposed code-driven reasoning paradigm. Our work opens a new direction for multimodal mathematical reasoning and provides the community with the first large-scale dataset, comprehensive benchmark, and strong approach for such problems. To facilitate future research, we make our datasets, code, and pretrained models publicly available at this https URL.', 'abstract_zh': 'Recent advances in 大型语言模型（LLMs）和视觉语言模型（VLMs）在数学推理方面取得了显著进展，但仍面临需要视觉辅助的问题瓶颈，如绘制辅助线或绘图以解决问题。大多数LLMs和VLMs仅限于文本推理链，而能够生成交织文本和图像的多模态统一模型缺乏完成此类任务所需的精度和可控性。为解决这一问题，我们提出了一种名为CodePlot-CoT的代码驱动的推理框架，用于数学中的“带图思考”。我们的方法利用VLM生成文本推理和可执行的绘图代码，然后将其渲染为“视觉思考”，以解决数学问题。为此，我们首先构建了Math-VR，这是首个大规模双语数学视觉推理数据集和基准，包含178K样本。其次，为了生成高质量的训练数据，我们开发了一种专用于解析复杂数学图形的最先进的图像到代码转换器。最后，利用这些训练数据，我们训练了CodePlot-CoT模型以解决数学问题。实验结果表明，与基线模型相比，我们的模型在我们的新基准上性能提高了21%，完全验证了我们提出的代码驱动推理框架的有效性。我们的工作为多模态数学推理开辟了一个新方向，并为社区提供了第一个大规模数据集、综合基准和强大方法。为了促进未来研究，我们已将数据集、代码和预训练模型公开发布。', 'title_zh': 'CodePlot-CoT: 通过代码驱动图像思考的数学视觉推理'}
{'arxiv_id': 'arXiv:2510.11330', 'title': 'Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap', 'authors': 'KiHyun Nam, Jongmin Choi, Hyeongkeun Lee, Jungwoo Heo, Joon Son Chung', 'link': 'https://arxiv.org/abs/2510.11330', 'abstract': 'Contrastive audio-language pretraining yields powerful joint representations, yet a persistent audio-text modality gap limits the benefits of coupling multimodal encoders with large language models (LLMs). We present Diffusion-Link, a diffusion-based modality-bridging module that generatively maps audio embeddings into the text-embedding distribution. The module is trained at the output embedding from the frozen multimodal encoder and implemented as a lightweight network with three residual MLP blocks. To assess the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on Automatic Audio Captioning (AAC); to our knowledge, this is the first application of diffusion-based modality bridging to AAC. We report two results. (1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link reduces the modality gap the most among prior diffusion-based methods and shows a collective migration of audio embeddings toward the text distribution. (2) Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised captioning without external knowledge, with relative gains up to 52.5% and 7.5%, respectively. These findings show that closing the modality gap is pivotal for effective coupling between multimodal encoders and LLMs, and diffusion-based modality bridging offers a promising direction beyond knowledge-retrieval-centric designs. Code will be released upon acceptance this https URL', 'abstract_zh': '基于扩散的模态连接模块Diffusion-Link增强了多模态编码器-大规模语言模型耦合，但持续存在的音频-文本模态差距限制了其益处。我们提出Diffusion-Link，一种基于扩散的模态桥梁模块，生成性地将音频嵌入映射至文本嵌入分布。该模块在冻结的多模态编码器的输出嵌入处进行训练，并作为带有三个残差MLP块的轻量级网络实现。为了评估Diffusion-Link对多模态编码器-大规模语言模型耦合的影响，我们在自动音频描述(Automatic Audio Captioning, AAC)上进行了评估；据我们所知，这是首次将基于扩散的模态桥梁应用到AAC中。我们报告了两个结果。(1) 模态差距分析：根据相似性和几何标准，Diffusion-Link在先前的基于扩散的方法中减少模态差距最多，并显示出音频嵌入集体向文本分布迁移。(2) 下游AAC：将Diffusion-Link附加到相同的基本多模态大规模语言模型，在AudioCaps上实现了零样本和全监督描述的新最佳性能，分别提高了累计52.5%和7.5%。这些发现表明，缩小模态差距对于多模态编码器和大规模语言模型之间的有效耦合至关重要，基于扩散的模态桥梁提供了超越知识检索为中心的设计的一种有前途的方向。代码将在接受后发布：<https://>。', 'title_zh': '扩散链接：用于桥接音频-文本模态差距的扩散概率模型'}
{'arxiv_id': 'arXiv:2510.11110', 'title': 'PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities', 'authors': 'Cheol-Hui Lee, Hwa-Yeon Lee, Min-Kyung Jung, Dong-Joo Kim', 'link': 'https://arxiv.org/abs/2510.11110', 'abstract': 'Missing or corrupted modalities are common in physiological signal-based medical applications owing to hardware constraints or motion artifacts. However, most existing methods assume the availability of all modalities, resulting in substantial performance degradation in the absence of any modality. To overcome this limitation, this study proposes PhysioME, a robust framework designed to ensure reliable performance under missing modality conditions. PhysioME adopts: (1) a multimodal self-supervised learning approach that combines contrastive learning with masked prediction; (2) a Dual-PathNeuroNet backbone tailored to capture the temporal dynamics of each physiological signal modality; and (3) a restoration decoder that reconstructs missing modality tokens, enabling flexible processing of incomplete inputs. The experimental results show that PhysioME achieves high consistency and generalization performance across various missing modality scenarios. These findings highlight the potential of PhysioME as a reliable tool for supporting clinical decision-making in real-world settings with imperfect data availability.', 'abstract_zh': '生理信号驱动的医疗应用中由于硬件限制或运动伪影常会出现模态缺失或损坏。然而，现有大多数方法假设所有模态均可用，导致在缺少任何模态时性能大幅下降。为克服这一局限，本研究提出PhysioME，一种稳健框架，旨在在模态缺失条件下确保可靠性能。PhysioME采用：(1) 结合对比学习与掩蔽预测的多模态自监督学习方法；(2) 专门设计以捕捉每种生理信号模态时序动态的Dual-PathNeuroNet主干；以及(3) 一种重建解码器，能够重建缺失模态令牌，使 incomplete 输入的灵活处理成为可能。实验结果表明，PhysioME 在各种模态缺失场景中实现了高一致性和泛化性能。这些发现突显了PhysioME作为在数据可用性不完美的实际环境中支持临床决策的可靠工具的潜力。', 'title_zh': 'PhysioME：一种 robust 的多模态自监督框架，用于处理缺失模态的生理信号'}
{'arxiv_id': 'arXiv:2510.10921', 'title': 'FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model', 'authors': 'Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Ji Ao, Dawei Leng, Yuhui Yin', 'link': 'https://arxiv.org/abs/2510.10921', 'abstract': 'Fine-grained vision-language understanding requires precise alignment between visual content and linguistic descriptions, a capability that remains limited in current models, particularly in non-English settings. While models like CLIP perform well on global alignment, they often struggle to capture fine-grained details in object attributes, spatial relations, and linguistic expressions, with limited support for bilingual comprehension. To address these challenges, we introduce FG-CLIP 2, a bilingual vision-language model designed to advance fine-grained alignment for both English and Chinese. Our approach leverages rich fine-grained supervision, including region-text matching and long-caption modeling, alongside multiple discriminative objectives. We further introduce the Textual Intra-modal Contrastive (TIC) loss to better distinguish semantically similar captions. Trained on a carefully curated mixture of large-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual performance. To enable rigorous evaluation, we present a new benchmark for Chinese multimodal understanding, featuring long-caption retrieval and bounding box classification. Extensive experiments on 29 datasets across 8 tasks show that FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results in both languages. We release the model, code, and benchmark to facilitate future research on bilingual fine-grained alignment.', 'abstract_zh': '细粒度跨模态理解要求视觉内容与语言描述之间精确对齐，当前模型在非英语环境中尤其难以实现这一能力。尽管CLIP等模型在全局对齐上表现良好，但在捕捉对象属性、空间关系和语言表达的细粒度细节方面仍有局限，且双语理解支持有限。为解决这些挑战，我们引入了FG-CLIP 2，一种旨在提升英日双语细粒度对齐的双语跨模态模型。我们的方法利用丰富的细粒度监督，包括区域-文本匹配和长描述建模，并结合多个判别性目标。此外，我们引入了文本内模态对比（TIC）损失，以更好地区分语义相似的描述。FG-CLIP 2 在大量精选的英汉混合数据上训练，实现了强大的双语性能。为了进行严格的评估，我们提出了一套针对中文多模态理解的新基准，包含长描述检索和边界框分类。在8个任务的29个数据集上的广泛实验显示，FG-CLIP 2 在双语中均超越现有方法，达到最新水平。我们发布了该模型、代码和基准以促进未来双语细粒度对齐的研究。', 'title_zh': 'FG-CLIP 2：一种双语细粒度跨模态对齐模型'}
{'arxiv_id': 'arXiv:2510.10889', 'title': 'Topological Alignment of Shared Vision-Language Embedding Space', 'authors': 'Junwon You, Dasol Kang, Jae-Hun Jung', 'link': 'https://arxiv.org/abs/2510.10889', 'abstract': 'Contrastive Vision-Language Models (VLMs) have demonstrated strong zero-shot capabilities. However, their cross-modal alignment remains biased toward English due to limited multilingual multimodal data. Recent multilingual extensions have alleviated this gap but enforce instance-level alignment while neglecting the global geometry of the shared embedding space. We address this problem by introducing ToMCLIP (Topological Alignment for Multilingual CLIP), a topology-aware framework aligning embedding spaces with topology-preserving constraints. The proposed method applies persistent homology to define a topological alignment loss and approximates persistence diagram with theoretical error bounds using graph sparsification strategy. This work validates the proposed approach, showing enhanced structural coherence of multilingual representations, higher zero-shot accuracy on the CIFAR-100, and stronger multilingual retrieval performance on the xFlickr&CO. Beyond VLMs, the proposed approach provides a general method for incorporating topological alignment into representation learning.', 'abstract_zh': '拓扑意识的多语文本-图像对齐框架：ToMCLIP（拓扑约束下的多语文本-图像对齐）', 'title_zh': '共享视觉-语言嵌入空间的拓扑对齐'}
{'arxiv_id': 'arXiv:2510.10671', 'title': 'Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey', 'authors': 'Jinxuan Li, Chaolei Tan, Haoxuan Chen, Jianxin Ma, Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai', 'link': 'https://arxiv.org/abs/2510.10671', 'abstract': 'Image-Language Foundation Models (ILFM) have demonstrated remarkable success in image-text understanding/generation tasks, providing transferable multimodal representations that generalize across diverse downstream image-based tasks. The advancement of video-text research has spurred growing interest in extending image-based models to the video domain. This paradigm, known as image-to-video transfer learning, succeeds in alleviating the substantial data and computational requirements associated with training video-language foundation models from scratch for video-text learning. This survey provides the first comprehensive review of this emerging field, which begins by summarizing the widely used ILFM and their capabilities. We then systematically classify existing image-to-video transfer learning strategies into two categories: frozen features and modified features, depending on whether the original representations from ILFM are preserved or undergo modifications. Building upon the task-specific nature of image-to-video transfer, this survey methodically elaborates these strategies and details their applications across a spectrum of video-text learning tasks, ranging from fine-grained (e.g., spatio-temporal video grounding) to coarse-grained (e.g., video question answering). We further present a detailed experimental analysis to investigate the efficacy of different image-to-video transfer learning paradigms on a range of downstream video understanding tasks. Finally, we identify prevailing challenges and highlight promising directions for future research. By offering a comprehensive and structured overview, this survey aims to establish a structured roadmap for advancing video-text learning based on existing ILFM, and to inspire future research directions in this rapidly evolving domain.', 'abstract_zh': 'Image-Language 基础模型 (ILFM) 在图像-文本理解/生成任务中取得了显著的成功，提供了跨多种下游图像任务泛化的可转移多模态表示。视频-文本研究的进展激发了将基于图像的模型扩展到视频域的兴趣。这一范式被称为图像到视频迁移学习，成功地减轻了从零开始训练视频-语言基础模型所关联的大数据和计算需求。本文综述为该新兴领域提供了首个全面回顾，首先总结了广泛使用的ILFM及其能力。我们系统地将现有的图像到视频迁移学习策略分为两类：冻结特征和修改特征，取决于ILFM原始表示是否保持或修改。基于图像到视频迁移的特定任务性质，本文详细阐述了这些策略及其在视频-文本学习任务谱系中的应用，从精细粒度（例如，空时视频接地）到粗粒度（例如，视频问答）。我们进一步提供详细的实验分析，以调查不同图像到视频迁移学习范式在多种下游视频理解任务中的有效性。最后，我们指出了现有的挑战并强调了未来研究的前景方向。通过提供全面且结构化的综述，本文旨在为基于现有ILFM推动视频-文本学习建立一个结构化的路线图，并激发该快速发展的领域中的未来研究方向。', 'title_zh': '基于图像-语言基础模型的图像到视频转移学习综述'}
{'arxiv_id': 'arXiv:2510.10560', 'title': 'BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices', 'authors': 'Euhid Aman, Esteban Carlin, Hsing-Kuo Pao, Giovanni Beltrame, Ghaluh Indah Permata Sari, Yie-Tarng Chen', 'link': 'https://arxiv.org/abs/2510.10560', 'abstract': 'Cross-attention transformers and other multimodal vision-language models excel at grounding and generation; however, their extensive, full-precision backbones make it challenging to deploy them on edge devices. Memory-augmented architectures enhance the utilization of past context; however, most works rarely pair them with aggressive edge-oriented quantization. We introduce BitMar, a quantized multimodal transformer that proposes an external human-like episodic memory for effective image-text generation on hardware with limited resources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and one for vision (DiNOv2-based), to create compact embeddings that are combined and used to query a fixed-size key-value episodic memory. During vector retrieval, the BitNet decoder applies per-layer conditioning, which increases the contextual relevance of generated content. The decoder also employs attention sinks with a sliding-window mechanism to process long or streaming inputs under tight memory budgets. The combination of per-layer conditioning and sliding-window attention achieves a strong quality-speed trade-off, delivering competitive captioning and multimodal understanding at low latency with a small model footprint. These characteristics make BitMar well-suited for edge deployment.', 'abstract_zh': 'BitMar：一种用于边缘设备的量化多模态变压器', 'title_zh': 'BitMar：基于 episodic 记忆的低比特多模态融合技术用于边缘设备'}
{'arxiv_id': 'arXiv:2510.10546', 'title': 'GLOFNet -- A Multimodal Dataset for GLOF Monitoring and Prediction', 'authors': 'Zuha Fatima, Muhammad Anser Sohaib, Muhammad Talha, Sidra Sultana, Ayesha Kanwal, Nazia Perwaiz', 'link': 'https://arxiv.org/abs/2510.10546', 'abstract': 'Glacial Lake Outburst Floods (GLOFs) are rare but destructive hazards in high mountain regions, yet predictive research is hindered by fragmented and unimodal data. Most prior efforts emphasize post-event mapping, whereas forecasting requires harmonized datasets that combine visual indicators with physical precursors. We present GLOFNet, a multimodal dataset for GLOF monitoring and prediction, focused on the Shisper Glacier in the Karakoram. It integrates three complementary sources: Sentinel-2 multispectral imagery for spatial monitoring, NASA ITS_LIVE velocity products for glacier kinematics, and MODIS Land Surface Temperature records spanning over two decades. Preprocessing included cloud masking, quality filtering, normalization, temporal interpolation, augmentation, and cyclical encoding, followed by harmonization across modalities. Exploratory analysis reveals seasonal glacier velocity cycles, long-term warming of ~0.8 K per decade, and spatial heterogeneity in cryospheric conditions. The resulting dataset, GLOFNet, is publicly available to support future research in glacial hazard prediction. By addressing challenges such as class imbalance, cloud contamination, and coarse resolution, GLOFNet provides a structured foundation for benchmarking multimodal deep learning approaches to rare hazard prediction.', 'abstract_zh': 'GLOFNet：面向喀喇昆仑山脉希普ser冰川的多模态GLOF监测与预测数据集', 'title_zh': 'GLOFNet -- 一种多模态数据集用于冰湖湖盆溃决洪水监测与预测'}
{'arxiv_id': 'arXiv:2510.10509', 'title': 'MARS-Sep: Multimodal-Aligned Reinforced Sound Separation', 'authors': 'Zihan Zhang, Xize Cheng, Zhennan Jiang, Dongjie Fu, Jingyuan Chen, Zhou Zhao, Tao Jin', 'link': 'https://arxiv.org/abs/2510.10509', 'abstract': 'Universal sound separation faces a fundamental misalignment: models optimized for low-level signal metrics often produce semantically contaminated outputs, failing to suppress perceptually salient interference from acoustically similar sources. To bridge this gap, we introduce MARS-Sep, a reinforcement learning framework that reformulates separation as decision making. Instead of simply regressing ground-truth masks, MARS-Sep learns a factorized Beta mask policy that is optimized by a clipped trust-region surrogate with entropy regularization and group-relative advantage normalization. Concretely, we sample masks from a frozen old policy, reconstruct waveforms, and update the current policy using clipped importance ratios-yielding substantially more stable and sample-efficient learning. Multimodal rewards, derived from an audio-text-vision encoder, directly incentivize semantic consistency with query prompts. We further propose a progressive alignment scheme to fine-tune this encoder, boosting its cross-modal discriminability and improving reward faithfulness. Extensive experiments on multiple benchmarks demonstrate consistent gains in Text-, Audio-, and Image-Queried separation, with notable improvements in signal metrics and semantic quality. Our code is available at this https URL. Sound separation samples are available at this https URL.', 'abstract_zh': '面向感知显著干扰的通用声音分离面临根本性不匹配：模型优化低级信号指标往往会产生语义污染的输出，难以抑制来自声学相似源的感知显著干扰。为解决这一问题，我们提出了MARS-Sep，这是一种强化学习框架，将分离重新表述为决策过程。MARS-Sep 不仅仅是回归真实掩码，而是学习一个通过剪裁信任区域代理优化并带有熵正则化和组相对优势规范化因子分解的Beta掩码策略。具体而言，我们从冻结的旧策略中采样掩码，重建波形，并使用剪裁的重要性比率更新当前策略，从而获得更为稳定和样本有效的学习。多模态奖励来自音频-文本-视觉编码器，直接激励与查询提示的语义一致性。我们还提出了一种渐进对齐方案，以进一步微调此编码器，提高其跨模态的可判别性和奖励忠实度。在多个基准上的大量实验展示了在文本查询、音频查询和图像查询分离方面的一致性改进，并且信号指标和语义质量都有显著提升。代码可在该网址获取：this https URL。声音分离样本可在该网址获取：this https URL。', 'title_zh': 'MARS-Sep: 多模态对齐强化声源分离'}
{'arxiv_id': 'arXiv:2510.10426', 'title': 'Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs', 'authors': 'Suyang Xi, Chenxi Yang, Hong Ding, Yiqing Ni, Catherine C. Liu, Yunhao Liu, Chengqi Zhang', 'link': 'https://arxiv.org/abs/2510.10426', 'abstract': "Multimodal large language models (MLLMs) often fail in fine-grained visual question answering, producing hallucinations about object identities, positions, and relations because textual queries are not explicitly anchored to visual referents. Retrieval-augmented generation (RAG) alleviates some errors, but it fails to align with human-like processing at both the retrieval and augmentation levels. Specifically, it focuses only on global-level image information but lacks local detail and limits reasoning about fine-grained interactions. To overcome this limitation, we present Human-Like Retrieval-Augmented Generation (HuLiRAG), a framework that stages multimodal reasoning as a ``what--where--reweight'' cascade. Queries are first anchored to candidate referents via open-vocabulary detection (what), then spatially resolved with SAM-derived masks to recover fine-grained precision (where), and adaptively prioritized through the trade-off between local and global alignment (reweight). Mask-guided fine-tuning further injects spatial evidence into the generation process, transforming grounding from a passive bias into an explicit constraint on answer formulation. Extensive experiments demonstrate that this human-like cascade improves grounding fidelity and factual consistency while reducing hallucinations, advancing multimodal question answering toward trustworthy reasoning.", 'abstract_zh': '具有人类似的检索增强生成：面向细粒度视觉问答的多模态推理框架（Human-Like Retrieval-Augmented Generation (HuLiRAG) for Fine-Grained Visual Question Answering）', 'title_zh': '驯化一个检索框架以具备类人方式阅读图像的能力，以增强大语言模型的生成能力'}
{'arxiv_id': 'arXiv:2510.10417', 'title': 'Combo-Gait: Unified Transformer Framework for Multi-Modal Gait Recognition and Attribute Analysis', 'authors': 'Zhao-Yang Wang, Zhimin Shao, Jieneng Chen, Rama Chellappa', 'link': 'https://arxiv.org/abs/2510.10417', 'abstract': 'Gait recognition is an important biometric for human identification at a distance, particularly under low-resolution or unconstrained environments. Current works typically focus on either 2D representations (e.g., silhouettes and skeletons) or 3D representations (e.g., meshes and SMPLs), but relying on a single modality often fails to capture the full geometric and dynamic complexity of human walking patterns. In this paper, we propose a multi-modal and multi-task framework that combines 2D temporal silhouettes with 3D SMPL features for robust gait analysis. Beyond identification, we introduce a multitask learning strategy that jointly performs gait recognition and human attribute estimation, including age, body mass index (BMI), and gender. A unified transformer is employed to effectively fuse multi-modal gait features and better learn attribute-related representations, while preserving discriminative identity cues. Extensive experiments on the large-scale BRIAR datasets, collected under challenging conditions such as long-range distances (up to 1 km) and extreme pitch angles (up to 50°), demonstrate that our approach outperforms state-of-the-art methods in gait recognition and provides accurate human attribute estimation. These results highlight the promise of multi-modal and multitask learning for advancing gait-based human understanding in real-world scenarios.', 'abstract_zh': '多模态多任务框架在多挑战条件下的步态识别及人体属性估计', 'title_zh': 'Combo-Gait：统一Transformer框架的多模态步态识别与属性分析'}
{'arxiv_id': 'arXiv:2510.09762', 'title': 'PatentVision: A multimodal method for drafting patent applications', 'authors': 'Ruo Yang, Sai Krishna Reddy Mudhiganti, Manali Sharma', 'link': 'https://arxiv.org/abs/2510.09762', 'abstract': 'Patent drafting is complex due to its need for detailed technical descriptions, legal compliance, and visual elements. Although Large Vision Language Models (LVLMs) show promise across various tasks, their application in automating patent writing remains underexplored. In this paper, we present PatentVision, a multimodal framework that integrates textual and visual inputs such as patent claims and drawings to generate complete patent specifications. Built on advanced LVLMs, PatentVision enhances accuracy by combining fine tuned vision language models with domain specific training tailored to patents. Experiments reveal it surpasses text only methods, producing outputs with greater fidelity and alignment with human written standards. Its incorporation of visual data allows it to better represent intricate design features and functional connections, leading to richer and more precise results. This study underscores the value of multimodal techniques in patent automation, providing a scalable tool to reduce manual workloads and improve consistency. PatentVision not only advances patent drafting but also lays the groundwork for broader use of LVLMs in specialized areas, potentially transforming intellectual property management and innovation processes.', 'abstract_zh': '专利撰写复杂性在于其需要详细的技术描述、法律合规性以及视觉元素。尽管大规模视觉语言模型（LVLMs）在各种任务中显示出潜力，但其在自动化专利写作中的应用仍处于未开发状态。本文介绍了一种多模态框架——PatentVision，该框架整合了诸如专利权利要求和图纸等文本和视觉输入以生成完整的专利规范文件。基于先进的LVLMs，PatentVision通过结合微调的视觉语言模型和针对专利领域的特定训练提高了准确性。实验显示，它超越了仅基于文本的方法，生成的输出具有更高的准确性和与人类撰写标准的更好对齐。其纳入的视觉数据使其能够更准确地表示复杂的结构特征和功能连接，从而产生更丰富和精确的结果。本研究突显了多模态技术在专利自动化中的价值，提供了一种可扩展的工具以减少人工工作量并提高一致性。PatentVision不仅推动了专利撰写的工作，也为更广泛地在专有领域使用LVLMs奠定了基础，有可能改变知识产权管理与创新流程。', 'title_zh': 'PatentVision: 一种多模态的专利申请撰写方法'}
