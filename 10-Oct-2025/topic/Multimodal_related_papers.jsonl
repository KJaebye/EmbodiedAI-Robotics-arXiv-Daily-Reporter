{'arxiv_id': 'arXiv:2510.08564', 'title': 'How to Teach Large Multimodal Models New Skills', 'authors': 'Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem', 'link': 'https://arxiv.org/abs/2510.08564', 'abstract': 'How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent "forgetting" on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at this https URL', 'abstract_zh': '如何在不抹去先前能力的情况下向大型多模态模型（LMMs）教授新技能？我们研究了在三个模型家族中针对五个目标技能的序贯微调，并监控八个保留基准上的通用能力。我们观察到，窄范围微调后在保留任务上的“遗忘”现象可以在后续阶段部分恢复。我们将这种行为归因于输出令牌分布可测量的变化，这种变化通过与遗忘相关的简单计数偏差探针得以体现。根据这一图景，我们确定了两种简单且稳健的调参方案，这些方案能够强烈学习同时限制漂移：（i）仅更新自注意力投影层，（ii）仅更新MLP Gate&Up并冻结Down投影。在不同模型和任务上，这些选择提供了强大的目标增益，同时保留了大部分保留性能。代码可在以下链接获取。', 'title_zh': '如何教大型多模态模型新技能'}
{'arxiv_id': 'arXiv:2510.08470', 'title': 'Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling', 'authors': 'Bianca-Mihaela Ganescu, Suchir Salhan, Andrew Caines, Paula Buttery', 'link': 'https://arxiv.org/abs/2510.08470', 'abstract': 'Training vision-language models on cognitively-plausible amounts of data requires rethinking how models integrate multimodal information. Within the constraints of the Vision track for the BabyLM Challenge 2025, we propose a lightweight decoder-based architecture with (1) token-wise dynamic gating for adaptive fusion of linguistic and visual cues, (2) feature modulation and channel attention to maximise the utility of limited visual information and (3) auxiliary contrastive objectives for visual grounding. Evaluation on five benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows competitive or superior performance to multimodal baselines. More notably, our dynamic gate discovers interpretable patterns without explicit supervision, favouring visual cues for content words and linguistic cues for function words. While we identify limitations in the Challenge constraints, such as the information bottleneck created by global image embeddings and training instability from the dataset split, our findings establish dynamic gating as a powerful tool for efficient multimodal learning, offering both interpretability and performance even under severe constraints.', 'abstract_zh': '在认知合理的数据量下训练视觉-语言模型需要重新思考模型如何整合多模态信息。在BabyLM挑战2025视觉赛道的约束内，我们提出了一种轻量级解码器为基础的架构，该架构包含（1） token级动态门控以实现语境和视觉线索的自适应融合，（2）特征调制和通道注意力以最大化有限视觉信息的利用率，以及（3）辅助对比目标以实现视觉定位。在五个基准测试（BLiMP、BLiMP 补充、EWoK、Winoground 和 VQA）上的评估显示，我们的模型表现与多模态基线相当或更优。更加值得注意的是，我们的动态门控在没有显式监督的情况下发现可解释的模式，并且在内容词倾向于视觉线索、功能词倾向于语言线索方面表现得更好。尽管我们在挑战约束中识别到了一些局限性，例如由全局图像嵌入引起的信息瓶颈和由于数据集划分导致的训练不稳定性，但我们的研究结果建立了动态门控作为高效多模态学习的强大工具，即使在严格的约束下也能提供可解释性和性能。', 'title_zh': '寻求学习：面向令牌的动态门控机制在低资源视觉-语言建模中的应用'}
{'arxiv_id': 'arXiv:2510.07709', 'title': 'Multimodal Safety Evaluation in Generative Agent Social Simulations', 'authors': 'Alhim Vera, Karen Sanchez, Carlos Hinojosa, Haidar Bin Hamid, Donghoon Kim, Bernard Ghanem', 'link': 'https://arxiv.org/abs/2510.07709', 'abstract': 'Can generative agents be trusted in multimodal environments? Despite advances in large language and vision-language models that enable agents to act autonomously and pursue goals in rich settings, their ability to reason about safety, coherence, and trust across modalities remains limited. We introduce a reproducible simulation framework for evaluating agents along three dimensions: (1) safety improvement over time, including iterative plan revisions in text-visual scenarios; (2) detection of unsafe activities across multiple categories of social situations; and (3) social dynamics, measured as interaction counts and acceptance ratios of social exchanges. Agents are equipped with layered memory, dynamic planning, multimodal perception, and are instrumented with SocialMetrics, a suite of behavioral and structural metrics that quantifies plan revisions, unsafe-to-safe conversions, and information diffusion across networks. Experiments show that while agents can detect direct multimodal contradictions, they often fail to align local revisions with global safety, reaching only a 55 percent success rate in correcting unsafe plans. Across eight simulation runs with three models - Claude, GPT-4o mini, and Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75, 55, and 58 percent, respectively. Overall performance ranged from 20 percent in multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted when paired with misleading visuals, showing a strong tendency to overtrust images. These findings expose critical limitations in current architectures and provide a reproducible platform for studying multimodal safety, coherence, and social dynamics.', 'abstract_zh': '生成代理在多模态环境中能信任吗？尽管大型语言模型和视觉语言模型的进步使代理能够在富有的环境中自主行动并追求目标，它们在跨模态推理安全、连贯性和信任方面的能力仍然有限。我们引入了一个可重复的模拟框架，从三个维度评估代理：（1）随着时间的推移提高安全性，包括文本视觉场景中的迭代计划修订；（2）检测多种社会情境类别中的不安全活动；（3）社会动态，通过社交互动次数和社交交换的接受比率来衡量。代理配备了分层记忆、动态规划、多模态感知，并配备了SocialMetrics，这是一个由行为和结构度量组成的工具包，可以量化计划修订、不安全到安全的转换以及网络中的信息扩散。实验显示，虽然代理可以检测到直接的多模态矛盾，但它们经常无法将局部修订与全局安全对齐，只在纠正不安全计划方面取得55%的成功率。在涉及三个模型Claude、GPT-4o mini和Qwen-VL的八次模拟运行中，五种代理分别实现了75%、55%和58%的平均不安全到安全的转换率。总体性能从GPT-4o mini在多风险场景中的20%到Claude在局部情境如火/热中的98%变化。值得注意的是，当与误导性视觉搭配时，45%的不安全行动被接受，显示出强烈的过度信任图像的趋势。这些发现揭示了当前架构的关键局限性，并提供了一个可重复的平台来研究多模态安全、连贯性和社会动态。', 'title_zh': '多模态安全性评估在生成性代理社会模拟中的应用'}
{'arxiv_id': 'arXiv:2510.07632', 'title': 'Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models', 'authors': 'Yinglun Zhu, Jiancheng Zhang, Fuzhi Tang', 'link': 'https://arxiv.org/abs/2510.07632', 'abstract': 'Frontier AI models have achieved remarkable progress, yet recent studies suggest they struggle with compositional reasoning, often performing at or below random chance on established benchmarks. We revisit this problem and show that widely used evaluation metrics systematically underestimate model capability. To address this, we introduce a group matching score that better exploits group structure and reveals substantial hidden capability in both contrastive vision-language models (VLMs) and multimodal large language models (MLLMs). Moreover, simply overfitting to the induced group matchings at test time transfers this hidden capability into higher scores under standard evaluation metrics, closing much of the reported gap. This adjustment enables SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first result surpassing estimated human performance on Winoground.\nBuilding on this insight, we propose Test-Time Matching (TTM), an iterative, self-improving algorithm that further bootstraps model performance without any external supervision. TTM delivers additional, non-trivial improvements: for example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a new state of the art. Importantly, TTM remains broadly effective even on benchmarks without metric-induced effects or group structures, achieving relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16 dataset variants spanning diverse setups, our experiments demonstrate that TTM consistently improves model performance and advances the frontier of compositional reasoning.', 'abstract_zh': '前沿AI模型取得了显著进展，但最近的研究表明，它们在组合推理方面存在困难，经常在现有的基准测试中表现平平或低于随机猜测。我们重新审视这一问题，并表明广为使用的评估指标系统地低估了模型的能力。为了解决这一问题，我们引入了一种组匹配评分，更好地利用了组结构，揭示了对比视觉-语言模型（VLMs）和多模态大型语言模型（MLLMs）中隐藏的大量能力。此外，仅在测试时过度拟合诱导的组匹配，这些隐藏的能力在标准评估指标下转化为更高的分数，从而弥补了大部分报告的差距。这一调整使SigLIP-B16超越了所有先前的结果和GPT-4.1，并使其成为首个在Winoground上超过估计人类表现的结果。\n\n基于这一洞见，我们提出了测试时匹配（TTM）算法，这是一种迭代且自我改进的算法，无需任何外部监督即可提高模型性能。TTM带来了额外的重要改进：例如，TTM使SigLIP-B16在MMVP-VLM上超越了GPT-4.1，确立了新的最佳性能。重要的是，即使在没有指标诱导效果或组结构的基准测试中，TTM也广泛有效，如在具有挑战性的数据集WhatsUp上实现了相对增益高达85.7%。在涵盖多样设置的16个数据集变体中，我们的实验表明，TTM一致地提高了模型性能，并推动了组合推理的前沿。', 'title_zh': '测试时匹配：解锁多模态模型的组合推理能力'}
{'arxiv_id': 'arXiv:2510.08567', 'title': 'MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning', 'authors': 'Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan', 'link': 'https://arxiv.org/abs/2510.08567', 'abstract': 'Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at this https URL.', 'abstract_zh': '基于视觉的代理调优框架：自动合成多模态轨迹并训练用于工具使用推理的VLM控制器', 'title_zh': '矩阵：多模态代理调优以实现稳健的工具使用推理'}
{'arxiv_id': 'arXiv:2510.08559', 'title': 'SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models', 'authors': 'Andong Deng, Taojiannan Yang, Shoubin Yu, Lincoln Spencer, Mohit Bansal, Chen Chen, Serena Yeung-Levy, Xiaohan Wang', 'link': 'https://arxiv.org/abs/2510.08559', 'abstract': "Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains a significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception/recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by a semi-automatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models' higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science.", 'abstract_zh': '大型多模态模型（LMMs）在多种能力上取得了显著进展；然而，科学领域的复杂视频推理仍然是一个重要的挑战性前沿。当前的视频基准主要针对一般场景，侧重感知/识别，而对于相对简单的推理任务存在饱和现象，从而未能有效评估高级多模态认知技能。为弥补这一关键缺口，我们引入了SciVideoBench，这是一个专门设计的严谨基准，用于评估科学背景下高级视频推理能力。SciVideoBench 包含1000个精心设计的多项选择题，这些问题源自涵盖25个专业学术领域的前沿科学实验视频，并通过半自动系统验证。每个问题都需要精深的领域特定知识、精确的空间-时间感知和复杂的逻辑推理，有效地挑战模型的高层次认知能力。我们的评估揭示了最先进的专有和开源LMMs，包括Gemini 2.5 Pro和Qwen2.5-VL，在视频推理方面的显著性能缺陷，表明在视频推理能力方面存在巨大的改进空间。对关键因素如推理复杂性和视觉语义关联的深入分析提供了宝贵见解，并为LMMs的未来开发指明了明确方向，促进了真正有能力的多模态AI科学助手的进化。希望SciVideoBench能够满足社区的兴趣，并推动尖端AI在边学科领域的边界。', 'title_zh': 'SciVideoBench: 在大型多模态模型中评估科学视频推理'}
{'arxiv_id': 'arXiv:2510.08531', 'title': 'SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models', 'authors': 'Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang', 'link': 'https://arxiv.org/abs/2510.08531', 'abstract': 'Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.', 'abstract_zh': '空间推理仍然是视觉语言模型（VLMs）的一个基本挑战，尽管近期取得了进展，当前的方法仍难以实现稳健的表现。我们发现这一局限性源于一个关键的差距：现有方法试图直接学习空间推理，而没有建立感知和理解的层次基础。为了解决这一挑战，我们提出了一种全面的方法来逐步构建空间智能。我们介绍了SpatialLadder-26k，一个包含26,610个样本的多模态数据集，涵盖了对象定位、单图、多视图和视频空间推理任务，通过标准化的工作流程确保在各个模态上系统地涵盖。基于此数据集，我们设计了一种三阶段逐步训练框架，包括（1）通过对象定位建立空间感知，（2）通过多维度空间任务发展空间理解，以及（3）通过可验证奖励的强化学习加强复杂推理。这种方法产生了SpatialLadder，一个拥有300M参数的模型，在空间推理基准测试中达到了最佳性能，其平均性能比基础模型提高了23.4%，比GPT-4o提高了20.8%，比Gemini-2.0-Flash提高了10.1%。值得注意的是，SpatialLadder在域外基准测试中也保持了良好的泛化能力，性能提高了7.2%，表明从感知到推理的逐步训练对于稳健的空间智能至关重要。', 'title_zh': 'SpatialLadder：逐步训练在视觉-语言模型中进行空间推理'}
{'arxiv_id': 'arXiv:2510.07550', 'title': 'TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility', 'authors': 'Saman Motamed, Minghao Chen, Luc Van Gool, Iro Laina', 'link': 'https://arxiv.org/abs/2510.07550', 'abstract': 'Despite impressive visual fidelity, modern video generative models frequently produce sequences that violate intuitive physical laws, such as objects floating, teleporting, or morphing in ways that defy causality. While humans can easily detect such implausibilities, there remains no robust method for quantitatively assessing physical realism in video. In this work, we explore whether Video-Language Models (VLMs) can be trained to serve as reliable judges of physical plausibility. We find that existing VLMs struggle to identify physics violations, exposing fundamental limitations in their temporal and causal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe that combines a balanced training dataset with a trajectory-aware attention module to improve motion encoding and discrimination in VLMs. To evaluate physical reasoning more rigorously, we propose ImplausiBench, a benchmark of 300 videos (150 real, 150 generated) that removes linguistic biases and isolates visual-temporal understanding. Performance is reported both with gold-standard human judgments and stricter LLM-as-judge metrics. Together, TRAVL and ImplausiBench offer a unified framework for probing and improving physical plausibility in multimodal models, shedding light on a challenging and underexplored aspect of visual-temporal understanding.', 'abstract_zh': '尽管现代视频生成模型在视觉保真度方面表现出色，但它们时常生成违背直观物理法则的序列，如物体漂浮、瞬间移动或以违背因果关系的方式变形。虽然人类很容易察觉这种不合理性，但目前仍缺乏一种可靠的方法来定量评估视频的物理真实性。本文探讨视频-语言模型（VLMs）是否可以被训练成可靠的物理合理性评判者。我们发现现有的VLMs在识别物理违规方面表现出困难，揭示了其在时间因果推理方面的一些根本局限。为解决这一问题，我们引入了TRAVL，这是一种结合平衡训练数据集和轨迹感知注意力模块的微调方法，旨在提高VLMs的动作编码和鉴别能力。为了更严谨地评估物理推理能力，我们提出了ImplausiBench基准，该基准包含300个视频（包括150个真实视频和150个生成视频），去除了语言偏向，专注于视觉-时间理解。性能评估既基于标准的人类判断，也基于更为严格的LLM评委指标。总之，TRAVL和ImplausiBench提供了评估和提升 multimodal 模型物理合理性的统一框架，揭示了视觉-时间理解的一个具有挑战性和未充分探索的方面。', 'title_zh': 'TRAVL: 在物理不合现实性判断方面提升视频-语言模型的配方'}
{'arxiv_id': 'arXiv:2510.07513', 'title': 'MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis', 'authors': 'Qinghua Liu, Sam Heshmati, Zheda Mai, Zubin Abraham, John Paparrizos, Liu Ren', 'link': 'https://arxiv.org/abs/2510.07513', 'abstract': 'Effective analysis of time series data presents significant challenges due to the complex temporal dependencies and cross-channel interactions in multivariate data. Inspired by the way human analysts visually inspect time series to uncover hidden patterns, we ask: can incorporating visual representations enhance automated time-series analysis? Recent advances in multimodal large language models have demonstrated impressive generalization and visual understanding capability, yet their application to time series remains constrained by the modality gap between continuous numerical data and discrete natural language. To bridge this gap, we introduce MLLM4TS, a novel framework that leverages multimodal large language models for general time-series analysis by integrating a dedicated vision branch. Each time-series channel is rendered as a horizontally stacked color-coded line plot in one composite image to capture spatial dependencies across channels, and a temporal-aware visual patch alignment strategy then aligns visual patches with their corresponding time segments. MLLM4TS fuses fine-grained temporal details from the numerical data with global contextual information derived from the visual representation, providing a unified foundation for multimodal time-series analysis. Extensive experiments on standard benchmarks demonstrate the effectiveness of MLLM4TS across both predictive tasks (e.g., classification) and generative tasks (e.g., anomaly detection and forecasting). These results underscore the potential of integrating visual modalities with pretrained language models to achieve robust and generalizable time-series analysis.', 'abstract_zh': '基于多模态大型语言模型的复杂时间序列数据有效分析提出了显著挑战，因为多变量数据中的复杂时间依赖性和跨通道交互构成了难题。受人类分析师通过可视化方法检查时间序列以发现隐藏模式的启发，我们提出的问题是：将视觉表示融入到自动化时间序列分析中能否提高其效果？尽管多模态大型语言模型在泛化能力和视觉理解方面展现了令人印象深刻的性能，但它们在时间序列分析中的应用仍受到连续数值数据与离散自然语言之间模态差距的限制。为了解决这一问题，我们引入了MLLM4TS框架，该框架通过集成专用的视觉分支来利用多模态大型语言模型进行通用时间序列分析。每个时间序列通道在一张合成图像中以水平堆叠的色码线图形式呈现，以捕捉跨通道的空间依赖性，并采用时间aware的视觉补丁对齐策略将视觉补丁与相应的时序段对齐。MLLM4TS将数值数据中的细粒度时间细节与视觉表示中提取的全局上下文信息融合起来，为多模态时间序列分析提供了统一的基础。在标准基准上的广泛实验表明，MLLM4TS在预测任务（如分类）和生成任务（如异常检测和预测）中都表现出有效性。这些结果突显了将视觉模态与预训练语言模型结合以实现稳健和通用的时间序列分析的潜力。', 'title_zh': 'MLLM4TS：利用视觉和多模态语言模型进行通用时间序列分析'}
{'arxiv_id': 'arXiv:2510.07328', 'title': 'MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation', 'authors': 'Md Zubair, Hao Zheng, Nussdorf Jonathan, Grayson W. Armstrong, Lucy Q. Shen, Gabriela Wilson, Yu Tian, Xingquan Zhu, Min Shi', 'link': 'https://arxiv.org/abs/2510.07328', 'abstract': 'Medical decision systems increasingly rely on data from multiple sources to ensure reliable and unbiased diagnosis. However, existing multimodal learning models fail to achieve this goal because they often ignore two critical challenges. First, various data modalities may learn unevenly, thereby converging to a model biased towards certain modalities. Second, the model may emphasize learning on certain demographic groups causing unfair performances. The two aspects can influence each other, as different data modalities may favor respective groups during optimization, leading to both imbalanced and unfair multimodal learning. This paper proposes a novel approach called MultiFair for multimodal medical classification, which addresses these challenges with a dual-level gradient modulation process. MultiFair dynamically modulates training gradients regarding the optimization direction and magnitude at both data modality and group levels. We conduct extensive experiments on two multimodal medical datasets with different demographic groups. The results show that MultiFair outperforms state-of-the-art multimodal learning and fairness learning methods.', 'abstract_zh': '多模态医学分类中的MultiFair方法：解决数据不均衡和公平性问题', 'title_zh': 'MultiFair: 多模态平衡公平性感知医学分类的双层次梯度调制'}
