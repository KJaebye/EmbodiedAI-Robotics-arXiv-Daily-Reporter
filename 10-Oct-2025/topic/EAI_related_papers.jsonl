{'arxiv_id': 'arXiv:2510.08568', 'title': 'NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos', 'authors': 'Hongyu Li, Lingfeng Sun, Yafei Hu, Duy Ta, Jennifer Barry, George Konidaris, Jiahui Fu', 'link': 'https://arxiv.org/abs/2510.08568', 'abstract': 'Enabling robots to execute novel manipulation tasks zero-shot is a central goal in robotics. Most existing methods assume in-distribution tasks or rely on fine-tuning with embodiment-matched data, limiting transfer across platforms. We present NovaFlow, an autonomous manipulation framework that converts a task description into an actionable plan for a target robot without any demonstrations. Given a task description, NovaFlow synthesizes a video using a video generation model and distills it into 3D actionable object flow using off-the-shelf perception modules. From the object flow, it computes relative poses for rigid objects and realizes them as robot actions via grasp proposals and trajectory optimization. For deformable objects, this flow serves as a tracking objective for model-based planning with a particle-based dynamics model. By decoupling task understanding from low-level control, NovaFlow naturally transfers across embodiments. We validate on rigid, articulated, and deformable object manipulation tasks using a table-top Franka arm and a Spot quadrupedal mobile robot, and achieve effective zero-shot execution without demonstrations or embodiment-specific training. Project website: this https URL.', 'abstract_zh': '使机器人执行新颖操作任务的零样本执行是机器人技术中的一个核心目标。我们提出了NovaFlow，这是一种自主操作框架，能够根据任务描述为目标机器人生成可执行计划，无需任何演示。给定任务描述，NovaFlow使用视频生成模型合成视频，并通过现成的感知模块将其提炼为三维可操作对象流。从对象流中，它计算刚体对象的相对姿态并通过抓取提议和轨迹优化将它们转化为机器人动作。对于变形物体，此流作为基于模型的规划中的跟踪目标，采用基于粒子的动力学模型。通过将任务理解与低级控制解耦，NovaFlow自然地实现了跨平台的转移。我们在使用桌面Franka手臂和Spot四足移动机器人进行刚体、 articulated 和变形物体操作任务的验证中，实现了有效的零样本执行，无需演示或针对特定平台的训练。项目网站: this https URL。', 'title_zh': 'NovaFlow: 零样本操纵via生成视频中的可操作流'}
{'arxiv_id': 'arXiv:2510.08556', 'title': 'DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model', 'authors': 'Xueyi Liu, He Wang, Li Yi', 'link': 'https://arxiv.org/abs/2510.08556', 'abstract': 'Achieving generalized in-hand object rotation remains a significant challenge in robotics, largely due to the difficulty of transferring policies from simulation to the real world. The complex, contact-rich dynamics of dexterous manipulation create a "reality gap" that has limited prior work to constrained scenarios involving simple geometries, limited object sizes and aspect ratios, constrained wrist poses, or customized hands. We address this sim-to-real challenge with a novel framework that enables a single policy, trained in simulation, to generalize to a wide variety of objects and conditions in the real world. The core of our method is a joint-wise dynamics model that learns to bridge the reality gap by effectively fitting limited amount of real-world collected data and then adapting the sim policy\'s actions accordingly. The model is highly data-efficient and generalizable across different whole-hand interaction distributions by factorizing dynamics across joints, compressing system-wide influences into low-dimensional variables, and learning each joint\'s evolution from its own dynamic profile, implicitly capturing these net effects. We pair this with a fully autonomous data collection strategy that gathers diverse, real-world interaction data with minimal human intervention. Our complete pipeline demonstrates unprecedented generality: a single policy successfully rotates challenging objects with complex shapes (e.g., animals), high aspect ratios (up to 5.33), and small sizes, all while handling diverse wrist orientations and rotation axes. Comprehensive real-world evaluations and a teleoperation application for complex tasks validate the effectiveness and robustness of our approach. Website: this https URL', 'abstract_zh': '实现通用的在手物体旋转仍然是机器人技术中的一个重要挑战，很大程度上这是因为从仿真到现实世界政策转移的困难。灵巧操作的复杂接触动力学造成了“现实差距”，限制了先前工作的应用场景，通常局限于简单几何形状、有限尺寸和长宽比、受限的手腕姿态或定制的手部。我们通过一个新颖的框架解决了这一从仿真到现实的世界挑战，该框架能够使在仿真中训练的单一策略在现实世界中广泛应用于各种物体和条件。我们方法的核心是一个关节级动力学模型，该模型通过有效拟合少量收集的真实世界数据并相应调整仿真策略的动作来弥合现实差距。该模型在关节间分解动力学、压缩系统级影响为低维变量，并通过学习每个关节自身的动力学特征来学习每个关节的演变，从而隐含地捕捉这些综合效应，表现出高度的数据高效性和跨不同全手交互分布的泛化能力。我们还配以一个完全自主的数据收集策略，收集多样化的现实世界交互数据，且最少的人工干预。我们的完整管道展示了前所未有的通用性：单一策略成功旋转了具有复杂形状（如动物）、高长宽比（高达5.33）和小尺寸的挑战性物体，同时处理多种手腕姿态和旋转轴。全面的现实世界评估和复杂任务的遥控操作应用验证了我们方法的有效性和鲁棒性。网站：https://this-url。', 'title_zh': 'DexNDM：通过关节级神经动力学模型缩小 Dexterous In-Hand Rotation 的现实差距'}
{'arxiv_id': 'arXiv:2510.08475', 'title': 'DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos', 'authors': 'Jhen Hsieh, Kuan-Hsun Tu, Kuo-Han Hung, Tsung-Wei Ke', 'link': 'https://arxiv.org/abs/2510.08475', 'abstract': 'We present DexMan, an automated framework that converts human visual demonstrations into bimanual dexterous manipulation skills for humanoid robots in simulation. Operating directly on third-person videos of humans manipulating rigid objects, DexMan eliminates the need for camera calibration, depth sensors, scanned 3D object assets, or ground-truth hand and object motion annotations. Unlike prior approaches that consider only simplified floating hands, it directly controls a humanoid robot and leverages novel contact-based rewards to improve policy learning from noisy hand-object poses estimated from in-the-wild videos.\nDexMan achieves state-of-the-art performance in object pose estimation on the TACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD. Meanwhile, its reinforcement learning policy surpasses previous methods by 19% in success rate on OakInk-v2. Furthermore, DexMan can generate skills from both real and synthetic videos, without the need for manual data collection and costly motion capture, and enabling the creation of large-scale, diverse datasets for training generalist dexterous manipulation.', 'abstract_zh': 'DexMan:一种将人类视觉演示自动转换为类人机器人双臂灵巧操作技能的框架', 'title_zh': 'DexMan: 从人类和生成的视频中学习双臂灵巧操作'}
{'arxiv_id': 'arXiv:2510.08381', 'title': 'Airy: Reading Robot Intent through Height and Sky', 'authors': 'Baoyang Chen, Xian Xu, Huamin Qu', 'link': 'https://arxiv.org/abs/2510.08381', 'abstract': "As industrial robots move into shared human spaces, their opaque decision making threatens safety, trust, and public oversight. This artwork, Airy, asks whether complex multi agent AI can become intuitively understandable by staging a competition between two reinforcement trained robot arms that snap a bedsheet skyward. Building on three design principles, competition as a clear metric (who lifts higher), embodied familiarity (audiences recognize fabric snapping), and sensor to sense mapping (robot cooperation or rivalry shown through forest and weather projections), the installation gives viewers a visceral way to read machine intent. Observations from five international exhibitions indicate that audiences consistently read the robots' strategies, conflict, and cooperation in real time, with emotional reactions that mirror the system's internal state. The project shows how sensory metaphors can turn a black box into a public interface.", 'abstract_zh': '随着工业机器人进入共享的人类空间，它们不透明的决策威胁到安全、信任和公众监督。这件作品《Airy》探讨复杂多智能体AI是否可以通过展示两支经过强化训练的机器人手臂争夺顶起床单的竞赛，变得直观易懂。基于三条设计原则——竞赛作为清晰的度量标准（谁提升得更高）、身体上的熟悉感（观众识别出布料的弹动）、以及传感器到感知的映射（通过森林和天气投影展示机器人间的合作或竞争），该装置为观众提供了一种直观的方式来解读机器意图。来自五个国际展览的观察表明，观众能够实时读取机器人的策略、冲突和合作，并且情绪反应反映出系统的内部状态。该项目展示了感官隐喻如何将一个黑箱转化为公众接口。', 'title_zh': 'Airy：通过高度和天空读取机器人意图'}
{'arxiv_id': 'arXiv:2510.08173', 'title': 'NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions', 'authors': 'Haolin Yang, Yuxing Long, Zhuoyuan Yu, Zihan Yang, Minghan Wang, Jiapeng Xu, Yihan Wang, Ziyan Yu, Wenzhe Cai, Lei Kang, Hao Dong', 'link': 'https://arxiv.org/abs/2510.08173', 'abstract': "Instruction-following navigation is a key step toward embodied intelligence. Prior benchmarks mainly focus on semantic understanding but overlook systematically evaluating navigation agents' spatial perception and reasoning capabilities. In this work, we introduce the NavSpace benchmark, which contains six task categories and 1,228 trajectory-instruction pairs designed to probe the spatial intelligence of navigation agents. On this benchmark, we comprehensively evaluate 22 navigation agents, including state-of-the-art navigation models and multimodal large language models. The evaluation results lift the veil on spatial intelligence in embodied navigation. Furthermore, we propose SNav, a new spatially intelligent navigation model. SNav outperforms existing navigation agents on NavSpace and real robot tests, establishing a strong baseline for future work.", 'abstract_zh': '指令跟随导航是实现体态智能的关键步骤。现有基准主要关注语义理解，而忽视了系统评估导航代理的空间感知和推理能力。在此项工作中，我们引入了NavSpace基准，包含六类任务和1,228条轨迹-指令对，旨在探测导航代理的空间智能。在该基准上，我们全面评估了22种导航代理，包括最先进的导航模型和多模态大语言模型。评估结果揭示了体态导航中的空间智能。此外，我们提出了SNav，一种新的空间智能导航模型。SNav在NavSpace和真实机器人测试中均优于现有导航代理，为未来工作设定了强有力的基准。', 'title_zh': 'NavSpace: 导航代理如何遵循空间智能指令'}
{'arxiv_id': 'arXiv:2510.08106', 'title': 'Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography', 'authors': 'Zihan Li, Yixiao Xu, Lei Zhang, Taiyu Han, Xinshan Yang, Yingni Wang, Mingxuan Liu, Shenghai Xin, Linxun Liu, Hongen Liao, Guochen Ning', 'link': 'https://arxiv.org/abs/2510.08106', 'abstract': 'Liver disease is a major global health burden. While ultrasound is the first-line diagnostic tool, liver sonography requires locating multiple non-continuous planes from positions where target structures are often not visible, for biometric assessment and lesion detection, requiring significant expertise. However, expert sonographers are severely scarce in resource-limited regions. Here, we develop an autonomous lightweight ultrasound robot comprising an AI agent that integrates multi-modal perception with memory attention for localization of unseen target structures, and a 588-gram 6-degrees-of-freedom cable-driven robot. By mounting on the abdomen, the system enhances robustness against motion. Our robot can autonomously acquire expert-level standard liver ultrasound planes and detect pathology in patients, including two from Xining, a 2261-meter-altitude city with limited medical resources. Our system performs effectively on rapid-motion individuals and in wilderness environments. This work represents the first demonstration of autonomous sonography across multiple challenging scenarios, potentially transforming access to expert-level diagnostics in underserved regions.', 'abstract_zh': '肝脏疾病是全球重大的公共卫生负担。尽管超声是首选的诊断工具，但肝脏超声要求从目标结构往往不可见的位置识别多个非连续切面，进行生物测量和病变检测，这需要很高的专业技能。然而，在资源有限的地区，具备这种技能的超声专家极其稀缺。在这里，我们开发了一种自主轻量级超声机器人，包括一个集多模态感知与记忆注意于一体的AI代理，以及一个重588克、具有6自由度的电缆驱动机器人。通过腹部固定，系统增强了对运动的鲁棒性。该机器人能够自主获取专家级标准肝脏超声切面，并在患者中检测病理情况，包括来自海拨2261米、医疗资源有限的西宁市的两例患者。该系统在快速运动个体和荒野环境中表现有效。本工作是首次在多种挑战场景下展示自主超声成像，有望在欠服务地区变革专家级诊断的可及性。', 'title_zh': '超越医院范围：自主轻量级超声机器人用于肝脏超声检查'}
{'arxiv_id': 'arXiv:2510.08044', 'title': 'Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation', 'authors': 'Shiyuan Yin, Chenjia Bai, Zihao Zhang, Junwei Jin, Xinxin Zhang, Chi Zhang, Xuelong Li', 'link': 'https://arxiv.org/abs/2510.08044', 'abstract': 'Large language models (LLMs) demonstrate advanced reasoning abilities, enabling robots to understand natural language instructions and generate high-level plans with appropriate grounding. However, LLM hallucinations present a significant challenge, often leading to overconfident yet potentially misaligned or unsafe plans. While researchers have explored uncertainty estimation to improve the reliability of LLM-based planning, existing studies have not sufficiently differentiated between epistemic and intrinsic uncertainty, limiting the effectiveness of uncertainty esti- mation. In this paper, we present Combined Uncertainty estimation for Reliable Embodied planning (CURE), which decomposes the uncertainty into epistemic and intrinsic uncertainty, each estimated separately. Furthermore, epistemic uncertainty is subdivided into task clarity and task familiarity for more accurate evaluation. The overall uncertainty assessments are obtained using random network distillation and multi-layer perceptron regression heads driven by LLM features. We validated our approach in two distinct experimental settings: kitchen manipulation and tabletop rearrangement experiments. The results show that, compared to existing methods, our approach yields uncertainty estimates that are more closely aligned with the actual execution outcomes.', 'abstract_zh': '结合先验与内在不确定性估计以实现可靠 embodied 计划 (CURE)', 'title_zh': '基于联合不确定性估计的可靠LLM驱动机器人规划研究'}
{'arxiv_id': 'arXiv:2510.08022', 'title': 'FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset', 'authors': 'Kehui Liu, Zhongjie Jia, Yang Li, Zhaxizhuoma, Pengan Chen, Song Liu, Xin Liu, Pingrui Zhang, Haoming Song, Xinyi Ye, Nieqing Cao, Zhigang Wang, Jia Zeng, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li', 'link': 'https://arxiv.org/abs/2510.08022', 'abstract': 'Data-driven robotic manipulation learning depends on large-scale, high-quality expert demonstration datasets. However, existing datasets, which primarily rely on human teleoperated robot collection, are limited in terms of scalability, trajectory smoothness, and applicability across different robotic embodiments in real-world environments. In this paper, we present FastUMI-100K, a large-scale UMI-style multimodal demonstration dataset, designed to overcome these limitations and meet the growing complexity of real-world manipulation tasks. Collected by FastUMI, a novel robotic system featuring a modular, hardware-decoupled mechanical design and an integrated lightweight tracking system, FastUMI-100K offers a more scalable, flexible, and adaptable solution to fulfill the diverse requirements of real-world robot demonstration data. Specifically, FastUMI-100K contains over 100K+ demonstration trajectories collected across representative household environments, covering 54 tasks and hundreds of object types. Our dataset integrates multimodal streams, including end-effector states, multi-view wrist-mounted fisheye images and textual annotations. Each trajectory has a length ranging from 120 to 500 frames. Experimental results demonstrate that FastUMI-100K enables high policy success rates across various baseline algorithms, confirming its robustness, adaptability, and real-world applicability for solving complex, dynamic manipulation challenges. The source code and dataset will be released in this link this https URL.', 'abstract_zh': '基于数据驱动的机器人操作学习依赖于大规模、高质量的专家示范数据集。然而，现有的数据集主要依赖于人工远程操控机器人采集，这些数据集在可扩展性、轨迹平滑性和适用于不同机器人实体方面的现实环境中有局限性。本文介绍了FastUMI-100K，一个大规模的UMI风格多模态示范数据集，旨在克服这些局限性，以应对实际操作任务日益复杂的需求。FastUMI-100K通过FastUMI采集，这是一种具有模块化、硬件解耦机械设计和集成轻量级跟踪系统的新型机器人系统，提供了更具扩展性、灵活性和适应性的解决方案，以满足现实世界机器人示范数据的多样需求。FastUMI-100K包含了超过100K个示范轨迹，这些轨迹覆盖了代表性的家庭环境，包括54项任务和数百种对象类型。数据集整合了多模态流，包括末端执行器状态、多视角手部安装的鱼眼图像和文本注释。每条轨迹包含的帧数从120到500不等。实验结果表明，FastUMI-100K使不同基线算法在多项政策成功率上显著提高，验证了其鲁棒性、适应性和实际应用价值，适用于解决复杂的动态操作挑战。源代码和数据集将在此链接发布：this https URL。', 'title_zh': 'FastUMI-100K：以大规模UMI风格数据集促进数据驱动的机器人 manipulation'}
{'arxiv_id': 'arXiv:2510.07975', 'title': 'Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation', 'authors': 'Mingyang Sun, Jiude Wei, Qichen He, Donglin Wang, Cewu Lu, Jianhua Sun', 'link': 'https://arxiv.org/abs/2510.07975', 'abstract': 'Enabling robots to perform precise and generalized manipulation in unstructured environments remains a fundamental challenge in embodied AI. While Vision-Language Models (VLMs) have demonstrated remarkable capabilities in semantic reasoning and task planning, a significant gap persists between their high-level understanding and the precise physical execution required for real-world manipulation. To bridge this "semantic-to-physical" gap, we introduce GRACE, a novel framework that grounds VLM-based reasoning through executable analytic concepts (EAC)-mathematically defined blueprints that encode object affordances, geometric constraints, and semantics of manipulation. Our approach integrates a structured policy scaffolding pipeline that turn natural language instructions and visual information into an instantiated EAC, from which we derive grasp poses, force directions and plan physically feasible motion trajectory for robot execution. GRACE thus provides a unified and interpretable interface between high-level instruction understanding and low-level robot control, effectively enabling precise and generalizable manipulation through semantic-physical grounding. Extensive experiments demonstrate that GRACE achieves strong zero-shot generalization across a variety of articulated objects in both simulated and real-world environments, without requiring task-specific training.', 'abstract_zh': '使机器人在非结构化环境中执行精确且通用的操作仍然是嵌体AI领域的基本挑战。尽管视觉-语言模型（VLMs）在语义推理和任务规划方面展现了卓越的能力，但它们的高层理解与现实世界操作所需的精确物理执行之间仍存在显著差距。为弥合这一“语义到物理”的差距，我们提出了GRACE，这是一种新的框架，通过执行可计算概念（EAC）进行基于VLM的推理，EAC是数学上定义的蓝图，编码了对象功能、几何约束和操作的语义。我们的方法整合了一种结构化的策略支撑流程，将自然语言指令和视觉信息转化为一个实例化的EAC，从中我们推导出抓取姿态、力向量，并规划机器人执行的物理可行运动轨迹。GRACE因此提供了一个统一且可解释的接口，将高层指令理解与低层机器人控制联系起来，有效地通过语义物理接地实现精确且通用的操作。广泛的实验表明，GRACE在模拟和真实环境中对多种 articulated 物体实现了强大的零样本泛化能力，而无需特定任务的训练。', 'title_zh': '可执行分析概念作为从大规模语言模型洞察到精确操作的缺失链接'}
{'arxiv_id': 'arXiv:2510.07882', 'title': 'Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots', 'authors': 'Boyu Li, Siyuan He, Hang Xu, Haoqi Yuan, Yu Zang, Liwei Hu, Junpeng Yue, Zhenxiong Jiang, Pengbo Hu, Börje F. Karlsson, Yehui Tang, Zongqing Lu', 'link': 'https://arxiv.org/abs/2510.07882', 'abstract': 'In recent years, Multimodal Large Language Models (MLLMs) have demonstrated the ability to serve as high-level planners, enabling robots to follow complex human instructions. However, their effectiveness, especially in long-horizon tasks involving dual-arm humanoid robots, remains limited. This limitation arises from two main challenges: (i) the absence of simulation platforms that systematically support task evaluation and data collection for humanoid robots, and (ii) the insufficient embodiment awareness of current MLLMs, which hinders reasoning about dual-arm selection logic and body positions during planning. To address these issues, we present DualTHOR, a new dual-arm humanoid simulator, with continuous transition and a contingency mechanism. Building on this platform, we propose Proprio-MLLM, a model that enhances embodiment awareness by incorporating proprioceptive information with motion-based position embedding and a cross-spatial encoder. Experiments show that, while existing MLLMs struggle in this environment, Proprio-MLLM achieves an average improvement of 19.75% in planning performance. Our work provides both an essential simulation platform and an effective model to advance embodied intelligence in humanoid robotics. The code is available at this https URL.', 'abstract_zh': '近年来，多模态大型语言模型（MLLMs）展示了作为高级规划者的潜力，使机器人能够遵循复杂的指令。然而，尤其是在涉及双臂人形机器人的长期任务上，其效果仍然有限。这种限制来源于两个主要挑战：（i）缺乏系统支持人形机器人任务评估和数据采集的仿真平台；（ii）当前MLLMs的不足感知能力，这阻碍了在规划过程中对双臂选择逻辑和身体位置的推理。为应对这些挑战，我们提出了DualTHOR，一个具有连续过渡和预案机制的新双臂人形机器人仿真平台。基于此平台，我们提出了一种Proprio-MLLM模型，通过结合本体感受信息和基于运动的位置嵌入以及跨空间编码器来增强感知能力。实验结果显示，在此环境中现有的MLLMs难以胜任，而Proprio-MLLM的规划性能平均提高了19.75%。我们的工作提供了一个重要的仿真平台和有效的模型，以促进人形机器人中的本体智能。代码已公开：这个 https URL。', 'title_zh': '面向 proprioception 意识的双臂类人机器人体化规划'}
{'arxiv_id': 'arXiv:2510.07871', 'title': 'Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception - Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track', 'authors': 'Erjia Xiao, Lingfeng Zhang, Yingbo Tang, Hao Cheng, Renjing Xu, Wenbo Ding, Lei Zhou, Long Chen, Hangjun Ye, Xiaoshuai Hao', 'link': 'https://arxiv.org/abs/2510.07871', 'abstract': "In this report, we describe the technical details of our submission to the IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on developing RGBD-based perception and navigation systems that enable autonomous agents to navigate safely, efficiently, and socially compliantly in dynamic human-populated indoor environments. The challenge requires agents to operate from an egocentric perspective using only onboard sensors including RGB-D observations and odometry, without access to global maps or privileged information, while maintaining social norm compliance such as safe distances and collision avoidance. Building upon the Falcon model, we introduce a Proactive Risk Perception Module to enhance social navigation performance. Our approach augments Falcon with collision risk understanding that learns to predict distance-based collision risk scores for surrounding humans, which enables the agent to develop more robust spatial awareness and proactive collision avoidance behaviors. The evaluation on the Social-HM3D benchmark demonstrates that our method improves the agent's ability to maintain personal space compliance while navigating toward goals in crowded indoor scenes with dynamic human agents, achieving 2nd place among 16 participating teams in the challenge.", 'abstract_zh': '本报告描述了我们对2025年IROS RoboSense挑战赛社会导航赛道的提交内容。该赛道专注于开发基于RGBD的感知与导航系统，使自主代理能够安全、高效且符合社交规范地在充满动态人群的室内环境中导航。挑战要求代理仅使用搭载传感器（包括RGB-D观测和里程计）从第一人称视角操作，而不允许访问全局地图或特权信息，并在保持社交规范（如安全距离和碰撞避免）的同时运行。基于Falcon模型，我们引入了主动风险感知模块以提升社会导航性能。我们的方法通过预测周围人类的距离为基础的碰撞风险评分来增强Falcon，使代理能够发展出更稳健的空间意识和主动碰撞避免行为。在Social-HM3D基准测试上的评估表明，我们的方法在拥挤的室内场景中导航至具有动态人类代理的目标时，提高了代理保持个人空间规范的能力，并在挑战中排名第二，共有16支参赛队伍。', 'title_zh': '小米Team EV-AD VLA：通过主动风险感知进行社会导航学习—2025年IROS RoboSense挑战赛社会导航赛道技术报告'}
{'arxiv_id': 'arXiv:2510.07869', 'title': 'USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots', 'authors': 'Junwen Gu, Zhiheng wu, Pengxuan Si, Shuang Qiu, Yukai Feng, Luoyang Sun, Laien Luo, Lianyi Yu, Jian Wang, Zhengxing Wu', 'link': 'https://arxiv.org/abs/2510.07869', 'abstract': 'Underwater environments present unique challenges for robotic operation, including complex hydrodynamics, limited visibility, and constrained communication. Although data-driven approaches have advanced embodied intelligence in terrestrial robots and enabled task-specific autonomous underwater robots, developing underwater intelligence capable of autonomously performing multiple tasks remains highly challenging, as large-scale, high-quality underwater datasets are still scarce. To address these limitations, we introduce USIM, a simulation-based multi-task Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over 561K frames from 1,852 trajectories, totaling approximately 15.6 hours of BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from visual navigation to mobile manipulation. Building upon this dataset, we propose U0, a VLA model for general underwater robots, which integrates binocular vision and other sensor modalities through multimodal fusion, and further incorporates a convolution-attention-based perception focus enhancement module (CAP) to improve spatial understanding and mobile manipulation. Across tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking, the framework achieves a success rate of 80%, while in challenging mobile manipulation tasks, it reduces the distance to the target by 21.2% compared with baseline methods, demonstrating its effectiveness. USIM and U0 show that VLA models can be effectively applied to underwater robotic applications, providing a foundation for scalable dataset construction, improved task autonomy, and the practical realization of intelligent general underwater robots.', 'abstract_zh': '水下环境为机器人操作带来了独特挑战，包括复杂的水动力学、受限的能见度以及通信限制。尽管数据驱动的方法已经在地面机器人中促进了感觉自己智能的发展，并使专用的自主水下机器人成为可能，但在开发能够自主执行多种任务的水下智能方面仍面临巨大挑战，因为高质量的数据集仍然稀缺。为解决这些问题，我们引入了USIM，这是一种基于模拟的水下机器人多任务视觉-语言-动作（VLA）数据集。USIM包含来自1,852条轨迹的超过561K帧数据，共计约15.6小时的BlueROV2与9种不同场景下20项任务的交互，涵盖了从视觉导航到移动操作等多种场景。在此基础上，我们提出了一种适用于通用水下机器人的VLA模型U0，该模型通过多模态融合整合了双目视觉和其他传感器数据，并进一步集成了基于卷积注意的感知焦点增强模块（CAP），以提高空间理解和移动操作能力。在诸如检查、障碍物规避、扫描和动态跟踪等任务中，该框架的成功率达到了80%，而在具有挑战性的移动操作任务中，与基线方法相比，成功接近目标的距离减少了21.2%，展示了其有效性。USIM和U0表明，VLA模型可以有效应用于水下机器人应用，为构建可扩展的数据集、提高任务自主性以及实现智能通用水下机器人提供了基础。', 'title_zh': 'USIM和U0：一种适用于通用水下机器人的视觉-语言-行动数据集和模型'}
{'arxiv_id': 'arXiv:2510.07865', 'title': 'DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation', 'authors': 'Guowei Zou, Haitao Wang, Hejun Wu, Yukun Qian, Yuhang Wang, Weibing Li', 'link': 'https://arxiv.org/abs/2510.07865', 'abstract': 'The ability to learn multi-modal action distributions is indispensable for robotic manipulation policies to perform precise and robust control. Flow-based generative models have recently emerged as a promising solution to learning distributions of actions, offering one-step action generation and thus achieving much higher sampling efficiency compared to diffusion-based methods. However, existing flow-based policies suffer from representation collapse, the inability to distinguish similar visual representations, leading to failures in precise manipulation tasks. We propose DM1 (MeanFlow with Dispersive Regularization for One-Step Robotic Manipulation), a novel flow matching framework that integrates dispersive regularization into MeanFlow to prevent collapse while maintaining one-step efficiency. DM1 employs multiple dispersive regularization variants across different intermediate embedding layers, encouraging diverse representations across training batches without introducing additional network modules or specialized training procedures. Experiments on RoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s vs. 2-3.5s) and improves success rates by 10-20 percentage points, with the Lift task reaching 99% success over 85% of the baseline. Real-robot deployment on a Franka Panda further validates that DM1 transfers effectively from simulation to the physical world. To the best of our knowledge, this is the first work to leverage representation regularization to enable flow-based policies to achieve strong performance in robotic manipulation, establishing a simple yet powerful approach for efficient and robust manipulation.', 'abstract_zh': '基于分散正则化的DM1：一种用于一步机器人操作的流匹配框架', 'title_zh': 'DM1：带有离散化正则化的MeanFlow在1步机器人操作中的应用'}
{'arxiv_id': 'arXiv:2510.07778', 'title': 'IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction', 'authors': 'Yandu Chen, Kefan Gu, Yuqing Wen, Yucheng Zhao, Tiancai Wang, Liqiang Nie', 'link': 'https://arxiv.org/abs/2510.07778', 'abstract': 'Vision-Language-Action (VLA) models leverage pretrained vision-language models (VLMs) to couple perception with robotic control, offering a promising path toward general-purpose embodied intelligence. However, current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios, and then finetuned to map explicit instructions to actions. Consequently, due to the lack of reasoning-intensive pretraining and reasoning-guided manipulation, these models are unable to perform implicit human intention reasoning required for complex, real-world interactions. To overcome these limitations, we propose \\textbf{IntentionVLA}, a VLA framework with a curriculum training paradigm and an efficient inference mechanism. Our proposed method first leverages carefully designed reasoning data that combine intention inference, spatial grounding, and compact embodied reasoning, endowing the model with both reasoning and perception capabilities. In the following finetuning stage, IntentionVLA employs the compact reasoning outputs as contextual guidance for action generation, enabling fast inference under indirect instructions. Experimental results show that IntentionVLA substantially outperforms $\\pi_0$, achieving 18\\% higher success rates with direct instructions and 28\\% higher than ECoT under intention instructions. On out-of-distribution intention tasks, IntentionVLA achieves over twice the success rate of all baselines, and further enables zero-shot human-robot interaction with 40\\% success rate. These results highlight IntentionVLA as a promising paradigm for next-generation human-robot interaction (HRI) systems.', 'abstract_zh': 'IntentionVLA：带有课程训练范式和高效推理机制的感知-语言-行动框架', 'title_zh': '意图VLA：通用高效的身体化意图推理在人机交互中的应用'}
{'arxiv_id': 'arXiv:2510.07773', 'title': 'Trajectory Conditioned Cross-embodiment Skill Transfer', 'authors': 'YuHang Tang, Yixuan Lou, Pengfei Han, Haoming Song, Xinyi Ye, Dong Wang, Bin Zhao', 'link': 'https://arxiv.org/abs/2510.07773', 'abstract': 'Learning manipulation skills from human demonstration videos presents a promising yet challenging problem, primarily due to the significant embodiment gap between human body and robot manipulators. Existing methods rely on paired datasets or hand-crafted rewards, which limit scalability and generalization. We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment Skill Transfer, enabling robots to acquire manipulation skills directly from human demonstration videos. Our key insight is to represent human motions as sparse optical flow trajectories, which serve as embodiment-agnostic motion cues by removing morphological variations while preserving essential dynamics. Conditioned on these trajectories together with visual and textual inputs, TrajSkill jointly synthesizes temporally consistent robot manipulation videos and translates them into executable actions, thereby achieving cross-embodiment skill transfer. Extensive experiments are conducted, and the results on simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\\% and KVD by 36.6\\% compared with the state-of-the-art, and improves cross-embodiment success rate by up to 16.7\\%. Real-robot experiments in kitchen manipulation tasks further validate the effectiveness of our approach, demonstrating practical human-to-robot skill transfer across embodiments.', 'abstract_zh': '从人类演示视频中学习操作技能：一种具有挑战性的前景问题，主要由于人类身体与机器人操作器之间存在显著的具身差距。现有方法依赖配对数据集或手工设计的奖励，限制了可扩展性和泛化能力。我们提出了一种轨迹条件跨具身技能转移框架TrajSkill，使得机器人可以直接从人类演示视频中获取操作技能。我们的核心洞察是将人类动作表示为稀疏光流轨迹，这些轨迹作为不依赖于具身的运动提示，通过消除形态学变异保留了基本动力学。在这些轨迹以及视觉和文本输入的条件下，TrajSkill联合生成时间一致的机器人操作视频，并将其转换为可执行的动作，从而实现跨具身技能转移。在广泛的实验中，仿真数据（MetaWorld）的结果表明，与最先进的方法相比，TrajSkill将FVD降低了39.6%，KVD降低了36.6%，并且提高了跨具身成功率高达16.7%。在厨房操作任务的实时机器人实验中进一步验证了我们方法的有效性，展示了跨具身的人到机器人的技能转移的实际可行性。', 'title_zh': '基于轨迹条件的跨躯体技能转移'}
{'arxiv_id': 'arXiv:2510.07725', 'title': 'Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis', 'authors': 'Kasidit Muenprasitivej, Ye Zhao, Glen Chou', 'link': 'https://arxiv.org/abs/2510.07725', 'abstract': 'We address the challenge of enabling bipedal robots to traverse rough terrain by developing probabilistically safe planning and control strategies that ensure dynamic feasibility and centroidal robustness under terrain uncertainty. Specifically, we propose a high-level Model Predictive Control (MPC) navigation framework for a bipedal robot with a specified confidence level of safety that (i) enables safe traversal toward a desired goal location across a terrain map with uncertain elevations, and (ii) formally incorporates uncertainty bounds into the centroidal dynamics of locomotion control. To model the rough terrain, we employ Gaussian Process (GP) regression to estimate elevation maps and leverage Conformal Prediction (CP) to construct calibrated confidence intervals that capture the true terrain elevation. Building on this, we formulate contraction-based reachable tubes that explicitly account for terrain uncertainty, ensuring state convergence and tube invariance. In addition, we introduce a contraction-based flywheel torque control law for the reduced-order Linear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum about the center-of-mass (CoM). This formulation provides both probabilistic safety and goal reachability guarantees. For a given confidence level, we establish the forward invariance of the proposed torque control law by demonstrating exponential stabilization of the actual CoM phase-space trajectory and the desired trajectory prescribed by the high-level planner. Finally, we evaluate the effectiveness of our planning framework through physics-based simulations of the Digit bipedal robot in MuJoCo.', 'abstract_zh': '我们通过开发在地形不确定性条件下保证动力学可行性和质心稳健性的概率安全规划与控制策略，来应对使仿人机器人穿越崎岖地形的挑战。具体来说，我们提出了一种高阶模型预测控制（MPC）导航框架，该框架在给定安全置信水平的情况下（i）能够安全穿越地形图上具有不确定高程的目标位置，（ii）正式地将不确定性边界纳入步行控制的质心动力学中。为了模拟崎岖地形，我们使用高斯过程（GP）回归估计高程图，并利用拟合预测（CP）构建校准后的置信区间，以捕获真实的地形高程。在此基础上，我们制定了考虑地形不确定性的收敛基可达管，确保状态收敛和管的不变性。此外，我们引入了一种基于收敛性的飞轮扭矩控制律，应用于降阶线性倒摆模型（LIPM），以稳定质心（CoM）处的角动量。这种表述提供了概率安全性和目标可达性的保证。对于给定的安全置信水平，我们通过证明实际CoM相空间轨迹的指数稳定性和高阶规划者指定的轨迹的前向不变性，建立了所提扭矩控制律的前向不变性。最后，我们通过MuJoCo中的Digit仿人机器人物理仿真评估了我们规划框架的有效性。', 'title_zh': '基于共形预测和收缩分析的不确定地形上概率安全的双足导航'}
{'arxiv_id': 'arXiv:2510.07700', 'title': 'EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments', 'authors': 'Raghav Mishra, Ian R. Manchester', 'link': 'https://arxiv.org/abs/2510.07700', 'abstract': 'We propose enforcing constraints on Model-Based Diffusion by introducing emerging barrier functions inspired by interior point methods. We show that constraints on Model-Based Diffusion can lead to catastrophic performance degradation, even on simple 2D systems due to sample inefficiency in the Monte Carlo approximation of the score function. We introduce Emerging-Barrier Model-Based Diffusion (EB-MBD) which uses progressively introduced barrier constraints to avoid these problems, significantly improving solution quality, without the need for computationally expensive operations such as projections. We analyze the sampling liveliness of samples each iteration to inform barrier parameter scheduling choice. We demonstrate results for 2D collision avoidance and a 3D underwater manipulator system and show that our method achieves lower cost solutions than Model-Based Diffusion, and requires orders of magnitude less computation time than projection based methods.', 'abstract_zh': '我们提出通过引入受内部点方法启发的新兴障碍函数来对基于模型的扩散过程施加约束。我们表明，基于模型的扩散过程中的约束可能导致性能灾难性下降，即使在简单的2D系统中也是如此，这是由于蒙特卡洛近似得分函数的样本效率低下所致。我们引入了新兴障碍基于模型的扩散（EB-MBD），通过逐步引入障碍约束来避免这些问题，显著提高了解的质量，而无需进行计算昂贵的投影操作。我们分析了每次迭代中样本的采样活跃性，以指导障碍参数调度的选择。我们展示了2D碰撞避免和3D水下操作臂系统的结果，并表明我们的方法可以获得更低成本的解，且所需计算时间比基于投影的方法少几个数量级。', 'title_zh': 'EB-MBD: 基于新兴屏障模型的轨迹优化扩散方法以确保在高度受限环境中的安全路径规划'}
{'arxiv_id': 'arXiv:2510.07674', 'title': 'Differentiable Particle Optimization for Fast Sequential Manipulation', 'authors': 'Lucas Chen, Shrutheesh Raman Iyer, Zachary Kingston', 'link': 'https://arxiv.org/abs/2510.07674', 'abstract': 'Sequential robot manipulation tasks require finding collision-free trajectories that satisfy geometric constraints across multiple object interactions in potentially high-dimensional configuration spaces. Solving these problems in real-time and at large scales has remained out of reach due to computational requirements. Recently, GPU-based acceleration has shown promising results, but prior methods achieve limited performance due to CPU-GPU data transfer overhead and complex logic that prevents full hardware utilization. To this end, we present SPaSM (Sampling Particle optimization for Sequential Manipulation), a fully GPU-parallelized framework that compiles constraint evaluation, sampling, and gradient-based optimization into optimized CUDA kernels for end-to-end trajectory optimization without CPU coordination. The method consists of a two-stage particle optimization strategy: first solving placement constraints through massively parallel sampling, then lifting solutions to full trajectory optimization in joint space. Unlike hierarchical approaches, SPaSM jointly optimizes object placements and robot trajectories to handle scenarios where motion feasibility constrains placement options. Experimental evaluation on challenging benchmarks demonstrates solution times in the realm of $\\textbf{milliseconds}$ with a 100% success rate; a $4000\\times$ speedup compared to existing approaches.', 'abstract_zh': '序贯机器人操作任务需要在多个对象交互中找到满足几何约束的无碰撞轨迹，可能在高维配置空间中进行。由于计算需求，实时且大尺度地解决这些问题一直无法实现。最近，基于GPU的加速取得了有希望的结果，但先前的方法由于CPU-GPU数据传输开销和复杂的逻辑限制了全硬件利用。为此，我们提出SPaSM（采样粒子优化用于序贯操作），这是一种完全GPU并行化框架，将约束评估、采样和基于梯度的优化编译为优化的CUDA内核，实现端到端轨迹优化，无需CPU协调。该方法包括两阶段粒子优化策略：首先通过大规模并行采样求解放置约束，然后在关节空间中提升解以进行完整的轨迹优化。与分层方法不同，SPaSM联合优化对象放置和机器人轨迹，以应对运动可行性限制放置选项的场景。实验评估表明，该方法在毫秒级时间内实现了100%的成功率，相较于现有方法速度提升了4000倍。', 'title_zh': '可微分粒子优化算法实现快速序列操纵'}
{'arxiv_id': 'arXiv:2510.07548', 'title': 'AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation', 'authors': 'Adam Hung, Fan Yang, Abhinav Kumar, Sergio Aguilera Marinovic, Soshi Iba, Rana Soltani Zarrin, Dmitry Berenson', 'link': 'https://arxiv.org/abs/2510.07548', 'abstract': 'Dexterous manipulation tasks often require switching between different contact modes, such as rolling, sliding, sticking, or non-contact contact modes. When formulating dexterous manipulation tasks as a trajectory optimization problem, a common approach is to decompose these tasks into sub-tasks for each contact mode, which are each solved independently. Optimizing each sub-task independently can limit performance, as optimizing contact points, contact forces, or other variables without information about future sub-tasks can place the system in a state from which it is challenging to make progress on subsequent sub-tasks. Further, optimizing these sub-tasks is very computationally expensive. To address these challenges, we propose Amortized Value Optimization (AVO), which introduces a learned value function that predicts the total future task performance. By incorporating this value function into the cost of the trajectory optimization at each planning step, the value function gradients guide the optimizer toward states that minimize the cost in future sub-tasks. This effectively bridges separately optimized sub-tasks, and accelerates the optimization by reducing the amount of online computation needed. We validate AVO on a screwdriver grasping and turning task in both simulation and real world experiments, and show improved performance even with 50% less computational budget compared to trajectory optimization without the value function.', 'abstract_zh': '灵巧操作任务经常需要在不同的接触模式之间切换，例如滚动、滑动、粘附或非接触模式。将灵巧操作任务建模为轨迹优化问题时，常见的做法是将这些任务分解为每种接触模式的子任务，并独立求解。独立优化每个子任务可能会限制性能，因为在不了解未来子任务信息的情况下优化接触点、接触力或其他变量可能会使系统处于难以推进后续子任务的状态。此外，优化这些子任务非常计算密集。为了解决这些挑战，我们提出了一种曰忘价值优化（AVO），它引入了一个学习的价值函数，该价值函数预测未来的任务总性能。通过将此价值函数整合到轨迹优化的成本中，在每一步规划中，价值函数的梯度引导优化器朝向在后续子任务中最小化成本的状态。这有效地连接了单独优化的子任务，并通过减少所需的在线计算量来加速优化。我们在螺丝刀抓取和旋转任务的模拟和实际实验中验证了AVO，并展示了即使在价值函数未使用的情况下计算预算减少50%的条件下，仍然能够获得更好的性能。', 'title_zh': 'AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation'}
{'arxiv_id': 'arXiv:2510.07417', 'title': 'FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams', 'authors': 'Corban Rivera, Grayson Byrd, Meghan Booker, Bethany Kemp, Allison Gaines, Emma Holmes, James Uplinger, Celso M de Melo, David Handelman', 'link': 'https://arxiv.org/abs/2510.07417', 'abstract': 'Coordinating heterogeneous robot teams from free-form natural-language instructions is hard. Language-only planners struggle with long-horizon coordination and hallucination, while purely formal methods require closed-world models. We present FLEET, a hybrid decentralized framework that turns language into optimized multi-robot schedules. An LLM front-end produces (i) a task graph with durations and precedence and (ii) a capability-aware robot--task fitness matrix; a formal back-end solves a makespan-minimization problem while the underlying robots execute their free-form subtasks with agentic closed-loop control. Across multiple free-form language-guided autonomy coordination benchmarks, FLEET improves success over state of the art generative planners on two-agent teams across heterogeneous tasks. Ablations show that mixed integer linear programming (MILP) primarily improves temporal structure, while LLM-derived fitness is decisive for capability-coupled tasks; together they deliver the highest overall performance. We demonstrate the translation to real world challenges with hardware trials using a pair of quadruped robots with disjoint capabilities.', 'abstract_zh': '从自由形式自然语言指令协调异构机器人团队是具有挑战性的。仅靠语言的规划者难以处理长期协调和幻觉问题，而纯粹形式化的方法需要封闭世界模型。我们提出FLEET，一种混合去中心化框架，将语言转换为优化的多机器人时间表。一个LLM前端生成（i）带有持续时间和顺序的任务图，以及（ii）感知能力的机器人-任务适应矩阵；形式化后端解决最长时间间隔最小化问题，而基础机器人则通过具备主体性的闭环控制执行其自由形式的子任务。在多个自由形式语言引导的自主协调基准测试中，FLEET在异构任务的双机器人团队中优于最先进的生成规划器。消融实验表明，混合整数线性编程（MILP）主要改善了时间结构，而LLM衍生的适应性对于能力耦合任务至关重要；两者共同提供了最高的整体性能。我们通过使用一对具有不同能力的四足机器人进行硬件试验，展示了其对现实世界挑战的适用性。', 'title_zh': 'FLEET: 基于形式语言的异构机器人团队调度'}
{'arxiv_id': 'arXiv:2510.08562', 'title': 'ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving', 'authors': 'Zhiyu Zheng, Shaoyu Chen, Haoran Yin, Xinbang Zhang, Jialv Zou, Xinggang Wang, Qian Zhang, Lefei Zhang', 'link': 'https://arxiv.org/abs/2510.08562', 'abstract': 'End-to-end autonomous driving (E2EAD) systems, which learn to predict future trajectories directly from sensor data, are fundamentally challenged by the inherent spatio-temporal imbalance of trajectory data. This imbalance creates a significant optimization burden, causing models to learn spurious correlations instead of causal inference, while also prioritizing uncertain, distant predictions, thereby compromising immediate safety. To address these issues, we propose ResAD, a novel Normalized Residual Trajectory Modeling framework. Instead of predicting the future trajectory directly, our approach reframes the learning task to predict the residual deviation from a deterministic inertial reference. The inertial reference serves as a counterfactual, forcing the model to move beyond simple pattern recognition and instead identify the underlying causal factors (e.g., traffic rules, obstacles) that necessitate deviations from a default, inertially-guided path. To deal with the optimization imbalance caused by uncertain, long-term horizons, ResAD further incorporates Point-wise Normalization of the predicted residual. It re-weights the optimization objective, preventing large-magnitude errors associated with distant, uncertain waypoints from dominating the learning signal. Extensive experiments validate the effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two denoising steps, demonstrating that our approach significantly simplifies the learning task and improves model performance. The code will be released to facilitate further research.', 'abstract_zh': '面向未来的残差轨迹预测：一种新型归一化残差轨迹建模框架（ResAD）', 'title_zh': 'ResAD: 归一化残差轨迹 modeling 用于端到端自动驾驶'}
{'arxiv_id': 'arXiv:2510.08553', 'title': 'Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation', 'authors': 'Yunzhe Xu, Yiyuan Pan, Zhe Liu', 'link': 'https://arxiv.org/abs/2510.08553', 'abstract': "Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at this https URL.", 'abstract_zh': '基于视觉-语言导航的记忆增强想象检索框架', 'title_zh': '梦境唤忆：想象导向的经验检索在记忆持久的视觉-语言导航中的应用'}
{'arxiv_id': 'arXiv:2510.08442', 'title': 'Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning', 'authors': 'Andrew Lee, Ian Chuang, Dechen Gao, Kai Fukazawa, Iman Soltani', 'link': 'https://arxiv.org/abs/2510.08442', 'abstract': "Visual Reinforcement Learning (RL) agents must learn to act based on high-dimensional image data where only a small fraction of the pixels is task-relevant. This forces agents to waste exploration and computational resources on irrelevant features, leading to sample-inefficient and unstable learning. To address this, inspired by human visual foveation, we introduce Gaze on the Prize. This framework augments visual RL with a learnable foveal attention mechanism (Gaze), guided by a self-supervised signal derived from the agent's experience pursuing higher returns (the Prize). Our key insight is that return differences reveal what matters most: If two similar representations produce different outcomes, their distinguishing features are likely task-relevant, and the gaze should focus on them accordingly. This is realized through return-guided contrastive learning that trains the attention to distinguish between the features relevant to success and failure. We group similar visual representations into positives and negatives based on their return differences and use the resulting labels to construct contrastive triplets. These triplets provide the training signal that teaches the attention mechanism to produce distinguishable representations for states associated with different outcomes. Our method achieves up to 2.4x improvement in sample efficiency and can solve tasks that the baseline fails to learn, demonstrated across a suite of manipulation tasks from the ManiSkill3 benchmark, all without modifying the underlying algorithm or hyperparameters.", 'abstract_zh': '基于视觉的强化学习（RL）代理必须基于高维图像数据学习行动，而只有少量像素与任务相关。这迫使代理浪费探索和计算资源在无关特征上，导致学习样本效率低下且不稳定。为解决这一问题，受人类视觉中心化启发，我们引入了“追求奖励的目光”框架。该框架通过自监督信号（由代理追求更高回报的经验引导），在视觉RL中增加了一个可学习的中心化注意力机制（目光）。我们的核心见解是，回报差异揭示了最重要内容：如果两个相似的表示产生不同的结果，则它们的区别特征很可能是任务相关的，目光应相应地聚焦于这些特征。通过基于回报引导的对比学习，训练注意力机制区分与成功和失败相关的特征。我们根据回报差异将相似的视觉表示分组为正样本和负样本，并使用由此产生的标签构建对比三元组。这些三元组作为训练信号，教导注意力机制为与不同结果相关的状态生成可区分的表示。我们的方法在样本效率上提高了多达2.4倍，并且可以在基线无法学习的任务中解决问题，在ManiSkill3基准的一系列操作任务中均未修改底层算法或超参数。', 'title_zh': 'gaze on the 奖项: 通过回报引导对比学习塑造视觉注意力'}
{'arxiv_id': 'arXiv:2510.08368', 'title': 'Co-design is powerful and not free', 'authors': 'Yi Zhang, Yue Xie, Tao Sun, Fumiya Iida', 'link': 'https://arxiv.org/abs/2510.08368', 'abstract': 'Robotic performance emerges from the coupling of body and controller, yet it remains unclear when morphology-control co-design is necessary. We present a unified framework that embeds morphology and control parameters within a single neural network, enabling end-to-end joint optimization. Through case studies in static-obstacle-constrained reaching, we evaluate trajectory error, success rate, and collision probability. The results show that co-design provides clear benefits when morphology is poorly matched to the task, such as near obstacles or workspace boundaries, where structural adaptation simplifies control. Conversely, when the baseline morphology already affords sufficient capability, control-only optimization often matches or exceeds co-design. By clarifying when control is enough and when it is not, this work advances the understanding of embodied intelligence and offers practical guidance for embodiment-aware robot design.', 'abstract_zh': '机器人性能源自身体与控制器的耦合，但尚不清楚何时需要形态-控制协同设计。我们提出了一种统一框架，将形态和控制参数嵌入单个神经网络中，实现端到端联合优化。通过静态障碍约束下的拾取任务案例研究，我们评估了轨迹误差、成功率和碰撞概率。结果表明，当形态与任务匹配不佳，例如靠近障碍物或工作空间边界时，协同设计提供明显的益处，结构适应简化了控制。相反，当基线形态已经具备足够的能力时，仅控制优化往往与或超过协同设计。通过明确控制何时足够、何时不够，这项工作促进了对嵌入式智能的理解，并为体aware机器人设计提供了 practical 指导。', 'title_zh': '协同设计强大但不免费。'}
{'arxiv_id': 'arXiv:2510.08278', 'title': 'A Multimodal Depth-Aware Method For Embodied Reference Understanding', 'authors': 'Fevziye Irem Eyiokur, Dogucan Yaman, Hazım Kemal Ekenel, Alexander Waibel', 'link': 'https://arxiv.org/abs/2510.08278', 'abstract': 'Embodied Reference Understanding requires identifying a target object in a visual scene based on both language instructions and pointing cues. While prior works have shown progress in open-vocabulary object detection, they often fail in ambiguous scenarios where multiple candidate objects exist in the scene. To address these challenges, we propose a novel ERU framework that jointly leverages LLM-based data augmentation, depth-map modality, and a depth-aware decision module. This design enables robust integration of linguistic and embodied cues, improving disambiguation in complex or cluttered environments. Experimental results on two datasets demonstrate that our approach significantly outperforms existing baselines, achieving more accurate and reliable referent detection.', 'abstract_zh': '基于语言指令和指示手势识别视觉场景中的目标对象需要理解具身参照，为此我们提出了一种新的ERU框架，该框架结合了基于LLM的数据增强、深度图模态和深度感知决策模块，以实现语言和具身线索的 robust 整合，并在复杂或拥挤环境中提高消岐能力。实验结果表明，我们的方法显著优于现有基线，实现了更准确可靠的指代检测。', 'title_zh': '一种多模态深度aware方法用于体现式引用理解'}
{'arxiv_id': 'arXiv:2510.07440', 'title': 'A Rotation-Invariant Embedded Platform for (Neural) Cellular Automata', 'authors': 'Dominik Woiwode, Jakob Marten, Bodo Rosenhahn', 'link': 'https://arxiv.org/abs/2510.07440', 'abstract': "This paper presents a rotation-invariant embedded platform for simulating (neural) cellular automata (NCA) in modular robotic systems. Inspired by previous work on physical NCA, we introduce key innovations that overcome limitations in prior hardware designs. Our platform features a symmetric, modular structure, enabling seamless connections between cells regardless of orientation. Additionally, each cell is battery-powered, allowing it to operate independently and retain its state even when disconnected from the collective. To demonstrate the platform's applicability, we present a novel rotation-invariant NCA model for isotropic shape classification. The proposed system provides a robust foundation for exploring the physical realization of NCA, with potential applications in distributed robotic systems and self-organizing structures. Our implementation, including hardware, software code, a simulator, and a video, is openly shared at: this https URL", 'abstract_zh': '本文提出了一种旋转不变嵌入平台，用于在模块化机器人系统中模拟（神经）细胞自动机（NCA）。受先前关于物理NCA工作的启发，我们引入了关键创新，克服了先前硬件设计的限制。该平台采用对称、模块化结构，使得无论朝向如何，各个单元之间均可无缝连接。此外，每个单元都由电池供电，使其在断开与集体连接时仍能独立运行并保持状态。为展示该平台的应用性，我们提出了一种适用于各向同性形状分类的旋转不变NCA模型。所提出的系统为探索NCA的物理实现提供了坚实的基础，具有在分布式机器人系统和自组织结构中潜在的应用价值。我们的实现，包括硬件、软件代码、模拟器和视频，已公开分享在：this https URL。', 'title_zh': '旋转不变嵌入平台 for (神经)细胞自动机'}
{'arxiv_id': 'arXiv:2510.08558', 'title': 'Agent Learning via Early Experience', 'authors': 'Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, Jian Xie, Yuxuan Sun, Boyu Gou, Qi Qi, Zihang Meng, Jianwei Yang, Ning Zhang, Xian Li, Ashish Shah, Dat Huynh, Hengduo Li, Zi Yang, Sara Cao, Lawrence Jang, Shuyan Zhou, Jiacheng Zhu, Huan Sun, Jason Weston, Yu Su, Yifan Wu', 'link': 'https://arxiv.org/abs/2510.08558', 'abstract': "A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.", 'abstract_zh': '长期而言，语言代理的目标是通过自身的经验学习和提高，最终在复杂的真实世界任务中超越人类。然而，在许多环境中，使用强化学习训练基于经验数据的代理仍然困难，这些环境要么缺乏可验证的奖励（例如，网站），要么需要执行低效的长期 rollout（例如，多轮工具使用）。因此，当前大多数代理依赖于专家数据的监督微调，这难以扩展且在泛化方面表现不佳。这一局限来自于专家演示的本质：它们仅捕获有限范围的场景，并使代理接触到有限的环境多样性。我们通过一种称为“早期经验”的中间范式来解决这一局限：由代理自身行为生成的交互数据，其中产生的未来状态作为监督，而无需奖励信号。在这一范式下，我们研究了使用此类数据的两种策略：（1）隐式世界建模，通过收集的状态来指导策略与环境动力学的相关性；（2）自我反思，代理从其不最优的动作中学习，以改进推理和决策。我们在八个不同环境和多个模型家族中进行了评估。我们的方法一致地提高了有效性并增强了域外泛化能力，突显了早期经验的价值。此外，在具有可验证奖励的环境中，我们的结果提供了积极的信号，表明早期经验为后续的强化学习提供了一个坚实的基础，将它置于模仿学习和完全基于经验的代理之间的实用桥梁位置。', 'title_zh': '基于早期经验的学习代理'}
{'arxiv_id': 'arXiv:2510.08238', 'title': 'Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness', 'authors': 'Jiyang Qiu, Xinbei Ma, Yunqing Xu, Zhuosheng Zhang, Hai Zhao', 'link': 'https://arxiv.org/abs/2510.08238', 'abstract': "The rapid deployment of large language model (LLM)-based agents in real-world applications has raised serious concerns about their trustworthiness. In this work, we reveal the security and robustness vulnerabilities of these agents through backdoor attacks. Distinct from traditional backdoors limited to single-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a multi-step backdoor attack designed for long-horizon agentic control. CoTri relies on an ordered sequence. It starts with an initial trigger, and subsequent ones are drawn from the environment, allowing multi-step manipulation that diverts the agent from its intended task. Experimental results show that CoTri achieves a near-perfect attack success rate (ASR) while maintaining a near-zero false trigger rate (FTR). Due to training data modeling the stochastic nature of the environment, the implantation of CoTri paradoxically enhances the agent's performance on benign tasks and even improves its robustness against environmental distractions. We further validate CoTri on vision-language models (VLMs), confirming its scalability to multimodal agents. Our work highlights that CoTri achieves stable, multi-step control within agents, improving their inherent robustness and task capabilities, which ultimately makes the attack more stealthy and raises potential safty risks.", 'abstract_zh': '基于大型语言模型代理的快速部署在实际应用中引发了对其可信度的严重关切。本研究通过后门攻击揭示了这些代理的安全性和鲁棒性漏洞。不同于传统的仅限单步控制的后门，我们提出了链式触发后门（CoTri），一种设计用于长期代理控制的多步后门攻击。CoTri依赖于一个有序序列，从初始触发开始，并且后续触发来自环境，允许多步操控以使代理偏离其预定任务。实验结果表明，CoTri实现了近完美的攻击成功率（ASR）同时保持了近零的误触发率（FTR）。由于训练数据模拟了环境的随机性质，CoTri的植入意外地提高了代理在无害任务上的性能，甚至增强了其对抗环境干扰的鲁棒性。我们进一步在视觉-语言模型（VLMs）上验证了CoTri，证明了其对多模态代理的可扩展性。我们的工作强调，CoTri在代理中实现了稳定且多步的控制，提升了其固有的鲁棒性和任务能力，最终使攻击更加隐蔽并引发了潜在的安全风险。', 'title_zh': '链触发机制：一种辩证地增强自主鲁棒性的代理后门'}
{'arxiv_id': 'arXiv:2510.08207', 'title': 'DODO: Causal Structure Learning with Budgeted Interventions', 'authors': 'Matteo Gregorini, Chiara Boldrini, Lorenzo Valerio', 'link': 'https://arxiv.org/abs/2510.08207', 'abstract': "Artificial Intelligence has achieved remarkable advancements in recent years, yet much of its progress relies on identifying increasingly complex correlations. Enabling causality awareness in AI has the potential to enhance its performance by enabling a deeper understanding of the underlying mechanisms of the environment. In this paper, we introduce DODO, an algorithm defining how an Agent can autonomously learn the causal structure of its environment through repeated interventions. We assume a scenario where an Agent interacts with a world governed by a causal Directed Acyclic Graph (DAG), which dictates the system's dynamics but remains hidden from the Agent. The Agent's task is to accurately infer the causal DAG, even in the presence of noise. To achieve this, the Agent performs interventions, leveraging causal inference techniques to analyze the statistical significance of observed changes. Results show better performance for DODO, compared to observational approaches, in all but the most limited resource conditions. DODO is often able to reconstruct with as low as zero errors the structure of the causal graph. In the most challenging configuration, DODO outperforms the best baseline by +0.25 F1 points.", 'abstract_zh': '人工智能在近年来取得了显著进展，但其进步很大程度上依赖于识别越来越复杂的相关性。使人工智能具备因果意识有望通过加深对环境底层机制的理解来提升其性能。本文介绍了一种算法DODO，该算法使智能体能够通过重复干预自主学习其环境的因果结构。我们假设一个场景，其中智能体与一个由因果有向无环图（DAG）支配的世界交互，该图规定了系统的动力学特性但对智能体而言是隐藏的。智能体的任务是在 noise 的存在下准确推断出因果DAG。为了实现这一目标，智能体执行干预，利用因果推理技术分析观测变化的统计显著性。结果表明，在除资源极度受限条件外的所有条件下，DODO 的性能优于观察性方法；在最具有挑战性的配置中，DODO 的F1分数比最佳基线高出0.25分，有时甚至能重建无误的因果图结构。', 'title_zh': 'DODO：预算化干预的因果结构学习'}
{'arxiv_id': 'arXiv:2510.07978', 'title': 'VoiceAgentBench: Are Voice Assistants ready for agentic tasks?', 'authors': 'Dhruv Jain, Harshit Shukla, Gautam Rajeev, Ashish Kulkarni, Chandra Khatri, Shubham Agarwal', 'link': 'https://arxiv.org/abs/2510.07978', 'abstract': 'Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs.', 'abstract_zh': '大规模语音语言模型（SpeechLMs）使语音助手能够理解自然的语音查询并执行复杂任务。然而，现有的语音基准主要集中在诸如转录或问答等孤立能力上，未系统评估涵盖多语言和文化理解以及对抗鲁棒性的代理场景。为此，我们引入了VoiceAgentBench，这是一个全面的基准，旨在评估SpeechLMs在现实语音代理环境中的表现。该基准包含超过5,500个合成语音查询，涵盖基于印度语境的对话，包括单工具调用、多工具工作流、多轮交互及安全性评估。基准支持英语、印地语和其他5种印度语言，反映了实际语言和文化的多样性。我们通过一种新颖的采样算法模拟说话人变异性，该算法基于说话人嵌入选择TTS语音转换的音频，最大化声学和说话人多样性。评估指标包括工具选择准确性、结构一致性以及工具调用的正确性，包括对抗鲁棒性。我们的实验揭示了上下文工具编排任务、印度语族泛化及对抗鲁棒性的显著差距，暴露出当前SpeechLMs的关键局限性。', 'title_zh': 'VoiceAgentBench: 语音助手准备好承担代理任务了吗？'}
{'arxiv_id': 'arXiv:2510.07790', 'title': 'GCPO: When Contrast Fails, Go Gold', 'authors': 'Hao Wu, Wei Liu', 'link': 'https://arxiv.org/abs/2510.07790', 'abstract': "Reinforcement learning has been widely applied to enhance the reasoning capabilities of large language models. Extending the inference limits of smaller models has become a prominent research focus. However, algorithms such as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the upper bound of a model's rollout responses is entirely determined by the model itself, preventing the acquisition of knowledge from samples that are either all incorrect or all correct. In this paper, we introduce Group Contrastive Policy Optimization (GCPO), a method that incorporates external standard reference answers. When the model cannot solve a problem, the reference answer supplies the correct response, steering the model toward an unequivocally accurate update direction. This approach offers two main advantages: (1) it improves training efficiency by fully utilizing every sample; (2) it enables the model to emulate the problem solving strategy of the reference answer during training, thereby enhancing generalization in reasoning. GCPO achieves outstanding results across multiple benchmark datasets, yielding substantial improvements over the baseline model. Our code is available at: this https URL.", 'abstract_zh': '强化学习已被广泛应用于提升大型语言模型的推理能力。扩展较小模型的推理极限已成为研究的重点。然而，诸如Group Relative Policy Optimization (GRPO)等算法存在明显不足：模型的展开响应的上限完全由模型自身决定，阻止了从全是错误或全是正确的样本中获取知识。在本文中，我们引入了Group Contrastive Policy Optimization (GCPO)，该方法结合了外部的标准参考答案。当模型无法解决问题时，参考答案提供正确的答案，引导模型朝着明确准确的方向更新。这种方法主要有两大优势：（1）通过充分利用每个样本提高训练效率；（2）使模型在训练过程中模仿参考答案的问题解决策略，从而增强推理能力的泛化能力。GCPO在多个基准数据集上取得了优异的结果，显著优于基线模型。我们的代码可在以下链接获取：this https URL。', 'title_zh': 'GCPO: 当对比失效时，追求卓越'}
{'arxiv_id': 'arXiv:2510.07715', 'title': 'Control Synthesis of Cyber-Physical Systems for Real-Time Specifications through Causation-Guided Reinforcement Learning', 'authors': 'Xiaochen Tang, Zhenya Zhang, Miaomiao Zhang, Jie An', 'link': 'https://arxiv.org/abs/2510.07715', 'abstract': 'In real-time and safety-critical cyber-physical systems (CPSs), control synthesis must guarantee that generated policies meet stringent timing and correctness requirements under uncertain and dynamic conditions. Signal temporal logic (STL) has emerged as a powerful formalism of expressing real-time constraints, with its semantics enabling quantitative assessment of system behavior. Meanwhile, reinforcement learning (RL) has become an important method for solving control synthesis problems in unknown environments. Recent studies incorporate STL-based reward functions into RL to automatically synthesize control policies. However, the automatically inferred rewards obtained by these methods represent the global assessment of a whole or partial path but do not accumulate the rewards of local changes accurately, so the sparse global rewards may lead to non-convergence and unstable training performances. In this paper, we propose an online reward generation method guided by the online causation monitoring of STL. Our approach continuously monitors system behavior against an STL specification at each control step, computing the quantitative distance toward satisfaction or violation and thereby producing rewards that reflect instantaneous state dynamics. Additionally, we provide a smooth approximation of the causation semantics to overcome the discontinuity of the causation semantics and make it differentiable for using deep-RL methods. We have implemented a prototype tool and evaluated it in the Gym environment on a variety of continuously controlled benchmarks. Experimental results show that our proposed STL-guided RL method with online causation semantics outperforms existing relevant STL-guided RL methods, providing a more robust and efficient reward generation framework for deep-RL.', 'abstract_zh': '基于实时信号时序逻辑的在线奖励生成方法及其在深度强化学习中的应用', 'title_zh': '基于因果引导 reinforcement learning 的实时规范下网络物理系统控制合成'}
{'arxiv_id': 'arXiv:2510.07363', 'title': 'L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)', 'authors': 'Tianxiang Xu, Zhichao Wen, Xinyu Zhao, Jun Wang, Yan Li, Chang Liu', 'link': 'https://arxiv.org/abs/2510.07363', 'abstract': 'The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness. This paper introduces L2M-AID, a novel framework for Autonomous Industrial Defense using LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team of collaborative agents, each driven by a Large Language Model (LLM), to achieve adaptive and resilient security. The core innovation lies in the deep fusion of two AI paradigms: we leverage an LLM as a semantic bridge to translate vast, unstructured telemetry into a rich, contextual state representation, enabling agents to reason about adversary intent rather than merely matching patterns. This semantically-aware state empowers a Multi-Agent Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative strategies. The MARL reward function is uniquely engineered to balance security objectives (threat neutralization) with operational imperatives, explicitly penalizing actions that disrupt physical process stability. To validate our approach, we conduct extensive experiments on the benchmark SWaT dataset and a novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework. Results demonstrate that L2M-AID significantly outperforms traditional IDS, deep learning anomaly detectors, and single-agent RL baselines across key metrics, achieving a 97.2% detection rate while reducing false positives by over 80% and improving response times by a factor of four. Crucially, it demonstrates superior performance in maintaining physical process stability, presenting a robust new paradigm for securing critical national infrastructure.', 'abstract_zh': '使用大语言模型赋能的多代理强化学习的自主工业防御框架L2M-AID', 'title_zh': 'L2M-AID：结合大规模语言模型语义推理与多Agent强化学习的自主物理-网络防御（预印本）'}
{'arxiv_id': 'arXiv:2510.08569', 'title': 'ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation', 'authors': 'Qin Liu, Jacob Dineen, Yuxi Huang, Sheng Zhang, Hoifung Poon, Ben Zhou, Muhao Chen', 'link': 'https://arxiv.org/abs/2510.08569', 'abstract': 'Benchmarks are central to measuring the capabilities of large language models and guiding model development, yet widespread data leakage from pretraining corpora undermines their validity. Models can match memorized content rather than demonstrate true generalization, which inflates scores, distorts cross-model comparisons, and misrepresents progress. We introduce ArenaBencher, a model-agnostic framework for automatic benchmark evolution that updates test cases while preserving comparability. Given an existing benchmark and a diverse pool of models to be evaluated, ArenaBencher infers the core ability of each test case, generates candidate question-answer pairs that preserve the original objective, verifies correctness and intent with an LLM as a judge, and aggregates feedback from multiple models to select candidates that expose shared weaknesses. The process runs iteratively with in-context demonstrations that steer generation toward more challenging and diagnostic cases. We apply ArenaBencher to math problem solving, commonsense reasoning, and safety domains and show that it produces verified, diverse, and fair updates that uncover new failure modes, increase difficulty while preserving test objective alignment, and improve model separability. The framework provides a scalable path to continuously evolve benchmarks in step with the rapid progress of foundation models.', 'abstract_zh': 'ArenaBencher：一种模型无关的自动基准演进框架', 'title_zh': 'ArenaBencher：多模型竞争性评价驱动的自动基准演进'}
{'arxiv_id': 'arXiv:2510.08257', 'title': 'A Distributed Emulation Environment for In-Memory Computing Systems', 'authors': 'Eleni Bougioukou, Anastasios Petropoulos, Nikolaos Toulgaridis, Theodoros Chatzimichail, Theodore Antonakopoulos', 'link': 'https://arxiv.org/abs/2510.08257', 'abstract': 'In-memory computing technology is used extensively in artificial intelligence devices due to lower power consumption and fast calculation of matrix-based functions. The development of such a device and its integration in a system takes a significant amount of time and requires the use of a real-time emulation environment, where various system aspects are analyzed, microcode is tested, and applications are deployed, even before the real chip is available. In this work, we present the architecture, the software development tools, and experimental results of a distributed and expandable emulation system for rapid prototyping of integrated circuits based on in-memory computing technologies. Presented experimental results demonstrate the usefulness of the proposed emulator.', 'abstract_zh': '基于内存计算技术的分布扩展型快速原型验证系统及其软件开发工具和实验结果', 'title_zh': '基于内存计算系统的分布式仿真环境'}
{'arxiv_id': 'arXiv:2510.08218', 'title': 'Expressive Value Learning for Scalable Offline Reinforcement Learning', 'authors': 'Nicolas Espinosa-Dice, Kiante Brantley, Wen Sun', 'link': 'https://arxiv.org/abs/2510.08218', 'abstract': 'Reinforcement learning (RL) is a powerful paradigm for learning to make sequences of decisions. However, RL has yet to be fully leveraged in robotics, principally due to its lack of scalability. Offline RL offers a promising avenue by training agents on large, diverse datasets, avoiding the costly real-world interactions of online RL. Scaling offline RL to increasingly complex datasets requires expressive generative models such as diffusion and flow matching. However, existing methods typically depend on either backpropagation through time (BPTT), which is computationally prohibitive, or policy distillation, which introduces compounding errors and limits scalability to larger base policies. In this paper, we consider the question of how to develop a scalable offline RL approach without relying on distillation or backpropagation through time. We introduce Expressive Value Learning for Offline Reinforcement Learning (EVOR): a scalable offline RL approach that integrates both expressive policies and expressive value functions. EVOR learns an optimal, regularized Q-function via flow matching during training. At inference-time, EVOR performs inference-time policy extraction via rejection sampling against the expressive value function, enabling efficient optimization, regularization, and compute-scalable search without retraining. Empirically, we show that EVOR outperforms baselines on a diverse set of offline RL tasks, demonstrating the benefit of integrating expressive value learning into offline RL.', 'abstract_zh': '增强学习（RL）是一种强大的决策序列学习范式。然而，RL 在机器人领域的应用尚未充分发挥，主要原因在于其缺乏可扩展性。离线 RL 通过在大规模、多样化的数据集上训练代理，避免了在线 RL 成本高昂的真实世界交互，为克服这一问题提供了有前景的方法。将离线 RL 扩展到越来越复杂的数据集需要诸如扩散和流匹配等具有表现力的生成模型。然而，现有方法通常依赖于时间反向传播（BPTT），这在计算上是不可行的，或者依赖于策略蒸馏，这会引入累积误差并限制其在更大基础策略上的可扩展性。在本文中，我们探讨了如何开发一种无需依赖蒸馏或时间反向传播的可扩展离线 RL 方法。我们引入了用于离线增强学习的具有表现力的价值学习（EVOR）：一种结合了具有表现力的策略和价值函数的可扩展离线 RL 方法。EVOR 在训练期间通过流匹配学习最优、正则化的 Q 函数。在推理时，EVOR 通过拒绝采样针对具有表现力的价值函数进行策略提取，从而实现高效的优化、正则化和计算可扩展的搜索而无需重新训练。实验结果显示，EVOR 在多种离线 RL 任务上的表现优于基线方法，证明了将具有表现力的价值学习整合到离线 RL 中的好处。', 'title_zh': '可扩展离线强化学习中的表达性价值学习'}
{'arxiv_id': 'arXiv:2510.08048', 'title': 'TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for E-commerce Search Relevance', 'authors': 'Jianhui Yang, Yiming Jin, Pengkun Jiao, Chenhe Dong, Zerui Huang, Shaowei Yao, Xiaojiang Zhou, Dan Ou, Haihong Tang', 'link': 'https://arxiv.org/abs/2510.08048', 'abstract': 'Query-product relevance prediction is fundamental to e-commerce search and has become even more critical in the era of AI-powered shopping, where semantic understanding and complex reasoning directly shape the user experience and business conversion. Large Language Models (LLMs) enable generative, reasoning-based approaches, typically aligned via supervised fine-tuning (SFT) or preference optimization methods like Direct Preference Optimization (DPO). However, the increasing complexity of business rules and user queries exposes the inability of existing methods to endow models with robust reasoning capacity for long-tail and challenging cases. Efforts to address this via reinforcement learning strategies like Group Relative Policy Optimization (GRPO) often suffer from sparse terminal rewards, offering insufficient guidance for multi-step reasoning and slowing convergence. To address these challenges, we propose TaoSR-AGRL, an Adaptive Guided Reinforcement Learning framework for LLM-based relevance prediction in Taobao Search Relevance. TaoSR-AGRL introduces two key innovations: (1) Rule-aware Reward Shaping, which decomposes the final relevance judgment into dense, structured rewards aligned with domain-specific relevance criteria; and (2) Adaptive Guided Replay, which identifies low-accuracy rollouts during training and injects targeted ground-truth guidance to steer the policy away from stagnant, rule-violating reasoning patterns toward compliant trajectories. TaoSR-AGRL was evaluated on large-scale real-world datasets and through online side-by-side human evaluations on Taobao Search. It consistently outperforms DPO and standard GRPO baselines in offline experiments, improving relevance accuracy, rule adherence, and training stability. The model trained with TaoSR-AGRL has been successfully deployed in the main search scenario on Taobao, serving hundreds of millions of users.', 'abstract_zh': '基于Large Language Models的淘宝搜索相关性预测：一种适应性引导强化学习框架', 'title_zh': 'TaoSR-AGRL：适应性引导强化学习框架在电子商务搜索相关性中的应用'}
{'arxiv_id': 'arXiv:2510.07809', 'title': 'Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents', 'authors': 'Renhua Ding, Xiao Yang, Zhengwei Fang, Jun Luo, Kun He, Jun Zhu', 'link': 'https://arxiv.org/abs/2510.07809', 'abstract': "Large vision-language models (LVLMs) enable autonomous mobile agents to operate smartphone user interfaces, yet vulnerabilities to UI-level attacks remain critically understudied. Existing research often depends on conspicuous UI overlays, elevated permissions, or impractical threat models, limiting stealth and real-world applicability. In this paper, we present a practical and stealthy one-shot jailbreak attack that leverages in-app prompt injections: malicious applications embed short prompts in UI text that remain inert during human interaction but are revealed when an agent drives the UI via ADB (Android Debug Bridge). Our framework comprises three crucial components: (1) low-privilege perception-chain targeting, which injects payloads into malicious apps as the agent's visual inputs; (2) stealthy user-invisible activation, a touch-based trigger that discriminates agent from human touches using physical touch attributes and exposes the payload only during agent operation; and (3) one-shot prompt efficacy, a heuristic-guided, character-level iterative-deepening search algorithm (HG-IDA*) that performs one-shot, keyword-level detoxification to evade on-device safety filters. We evaluate across multiple LVLM backends, including closed-source services and representative open-source models within three Android applications, and we observe high planning and execution hijack rates in single-shot scenarios (e.g., GPT-4o: 82.5% planning / 75.0% execution). These findings expose a fundamental security vulnerability in current mobile agents with immediate implications for autonomous smartphone operation.", 'abstract_zh': '大型视觉-语言模型在自主移动代理操作智能手机用户界面方面的应用及其UI级攻击的临界研究不足：一种实用且隐蔽的一次性越狱攻击', 'title_zh': '有效且隐蔽的一次性手机视觉-语言代理越狱方法'}
{'arxiv_id': 'arXiv:2510.07488', 'title': 'Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics', 'authors': 'Rasika Muralidharan, Jaewoon Kwak, Jisun An', 'link': 'https://arxiv.org/abs/2510.07488', 'abstract': 'Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are gaining attention, yet fewer studies explore their team dynamics. Inspired by human team science, we propose a multi-agent framework to examine core aspects of team science: structure, diversity, and interaction dynamics. We evaluate team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and Latent Implicit Hate, spanning commonsense and social reasoning. Our results show that flat teams tend to perform better than hierarchical ones, while diversity has a nuanced impact. Interviews suggest agents are overconfident about their team performance, yet post-task reflections reveal both appreciation for collaboration and challenges in integration, including limited conversational coordination.', 'abstract_zh': '基于大型语言模型（LLM）的多智能体系统（MAS）及其团队动态研究：以结构、多样性和交互动力学为核心', 'title_zh': '人类团队的经验能否应用于多agent系统？结构、多样性和交互动力学的作用'}
