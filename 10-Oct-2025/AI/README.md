# How to Teach Large Multimodal Models New Skills 

**Title (ZH)**: 如何教大型多模态模型新技能 

**Authors**: Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem  

**Link**: [PDF](https://arxiv.org/pdf/2510.08564)  

**Abstract**: How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent "forgetting" on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at this https URL 

**Abstract (ZH)**: 如何在不抹去先前能力的情况下向大型多模态模型（LMMs）教授新技能？我们研究了在三个模型家族中针对五个目标技能的序贯微调，并监控八个保留基准上的通用能力。我们观察到，窄范围微调后在保留任务上的“遗忘”现象可以在后续阶段部分恢复。我们将这种行为归因于输出令牌分布可测量的变化，这种变化通过与遗忘相关的简单计数偏差探针得以体现。根据这一图景，我们确定了两种简单且稳健的调参方案，这些方案能够强烈学习同时限制漂移：（i）仅更新自注意力投影层，（ii）仅更新MLP Gate&Up并冻结Down投影。在不同模型和任务上，这些选择提供了强大的目标增益，同时保留了大部分保留性能。代码可在以下链接获取。 

---
# Agent Learning via Early Experience 

**Title (ZH)**: 基于早期经验的学习代理 

**Authors**: Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, Jian Xie, Yuxuan Sun, Boyu Gou, Qi Qi, Zihang Meng, Jianwei Yang, Ning Zhang, Xian Li, Ashish Shah, Dat Huynh, Hengduo Li, Zi Yang, Sara Cao, Lawrence Jang, Shuyan Zhou, Jiacheng Zhu, Huan Sun, Jason Weston, Yu Su, Yifan Wu  

**Link**: [PDF](https://arxiv.org/pdf/2510.08558)  

**Abstract**: A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents. 

**Abstract (ZH)**: 长期而言，语言代理的目标是通过自身的经验学习和提高，最终在复杂的真实世界任务中超越人类。然而，在许多环境中，使用强化学习训练基于经验数据的代理仍然困难，这些环境要么缺乏可验证的奖励（例如，网站），要么需要执行低效的长期 rollout（例如，多轮工具使用）。因此，当前大多数代理依赖于专家数据的监督微调，这难以扩展且在泛化方面表现不佳。这一局限来自于专家演示的本质：它们仅捕获有限范围的场景，并使代理接触到有限的环境多样性。我们通过一种称为“早期经验”的中间范式来解决这一局限：由代理自身行为生成的交互数据，其中产生的未来状态作为监督，而无需奖励信号。在这一范式下，我们研究了使用此类数据的两种策略：（1）隐式世界建模，通过收集的状态来指导策略与环境动力学的相关性；（2）自我反思，代理从其不最优的动作中学习，以改进推理和决策。我们在八个不同环境和多个模型家族中进行了评估。我们的方法一致地提高了有效性并增强了域外泛化能力，突显了早期经验的价值。此外，在具有可验证奖励的环境中，我们的结果提供了积极的信号，表明早期经验为后续的强化学习提供了一个坚实的基础，将它置于模仿学习和完全基于经验的代理之间的实用桥梁位置。 

---
# FlowSearch: Advancing deep research with dynamic structured knowledge flow 

**Title (ZH)**: FlowSearch：动态结构化知识流推动的深度研究进展 

**Authors**: Yusong Hu, Runmin Ma, Yue Fan, Jinxin Shi, Zongsheng Cao, Yuhao Zhou, Jiakang Yuan, Xiangchao Yan, Wenlong Zhang, Lei Bai, Bo Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08521)  

**Abstract**: Deep research is an inherently challenging task that demands both breadth and depth of thinking. It involves navigating diverse knowledge spaces and reasoning over complex, multi-step dependencies, which presents substantial challenges for agentic systems. To address this, we propose FlowSearch, a multi-agent framework that actively constructs and evolves a dynamic structured knowledge flow to drive subtask execution and reasoning. FlowSearch is capable of strategically planning and expanding the knowledge flow to enable parallel exploration and hierarchical task decomposition, while also adjusting the knowledge flow in real time based on feedback from intermediate reasoning outcomes and insights. FlowSearch achieves state-of-the-art performance on both general and scientific benchmarks, including GAIA, HLE, GPQA and TRQA, demonstrating its effectiveness in multi-disciplinary research scenarios and its potential to advance scientific discovery. The code is available at this https URL. 

**Abstract (ZH)**: 深度研究是一项艰巨的任务，需要广度和深度的思考。它涉及导航多元的知识空间并推理复杂的多步依赖关系，这对自主系统提出了重大挑战。为此，我们提出FlowSearch，这是一种多代理框架，能够主动构建和演化动态的知识流程结构，以驱动子任务执行和推理。FlowSearch能够战略性地规划和扩展知识流程，以实现并行探索和层次化任务分解，并根据中间推理结果和见解实时调整知识流程。FlowSearch在包括GAIA、HLE、GPQA和TRQA在内的通用和科学基准测试中实现了最先进的性能，展示了其在多学科研究场景中的有效性及其促进科学发现的潜力。代码可在以下链接获取。 

---
# CaRT: Teaching LLM Agents to Know When They Know Enough 

**Title (ZH)**: CaRT: 教学大规模语言模型代理在何时停止学习 

**Authors**: Grace Liu, Yuxiao Qu, Jeff Schneider, Aarti Singh, Aviral Kumar  

**Link**: [PDF](https://arxiv.org/pdf/2510.08517)  

**Abstract**: Many tasks require learned models to strategically gather relevant information over multiple rounds of interaction before actually acting on a task. Strategic information gathering requires models to know not only how to effectively acquire information, but also when to stop gathering information and make a decision, in order to avoid overthinking or getting derailed when acting. In this paper, we formalize this problem and introduce Counterfactuals and Reasoning for Termination (CaRT), an approach for teaching LLMs when to stop seeking information. To appropriately learn when to terminate, CaRT fine-tunes LLMs using counterfactual pairs of trajectories, one where termination is appropriate and a minimally modified version of the same trajectory where it is not. It trains the LLM to explain the rationale for the termination decision in either case via verbal reasoning, and imbues this capability into the base LLM via fine-tuning. We instantiate CaRT in two domains: interactive medical diagnosis and math problem solving. In both domains, we find that CaRT improves the efficiency of information gathering and task success rate compared to other fine-tuning methods. 

**Abstract (ZH)**: 战略信息收集与终止推理（CaRT）：教LLMs何时停止寻求信息的方法 

---
# AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents 

**Title (ZH)**: AutoMLGen：导航精细粒度优化编码代理 

**Authors**: Shangheng Du, Xiangchao Yan, Dengyang Jiang, Jiakang Yuan, Yusong Hu, Xin Li, Liang He, Bo Zhang, Lei Bai  

**Link**: [PDF](https://arxiv.org/pdf/2510.08511)  

**Abstract**: Large language models (LLMs) have shown impressive performance in general programming tasks. However, in Machine Learning Engineering (MLE) scenarios such as AutoML and Kaggle competitions, achieving high performance depends heavily on expert intervention and repeated adjustments rather than simply generating correct code. When applied directly to these tasks, LLMs often lack fine-grained domain priors, and existing MLE approaches that use linear or tree-structured searches limit knowledge transfer to adjacent hierarchical links. As a result, they cannot leverage past full trajectories or share information across branches, limiting self-evolving ability and search space diversity. To address these limitations, we introduce AutoMLGen, an LLM-based coding agent that integrates a domain knowledge base for high-quality prior guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS retains the tree-guided exploration of MCTS while embedding a graph structure into the expansion stage to enable dynamic path reorganization, historical trajectory reuse, and multi-solution fusion to support both self-evolution and collaborative learning. Combined with fine-grained operator sets, this design improves stability and accelerates convergence. Evaluation on the MLE-Bench shows that AutoMLGen achieves state-of-the-art performance in numerous dimensions, such as the average medal rate and the valid submission rate, under a 12-hour budget (half the standard runtime). The code is available at this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）在通用编程任务中展现了 impressive 的表现。然而，在机器学习工程（MLE）场景如自动机器学习（AutoML）和 Kaggle 竞赛中，实现高性能依赖于专家干预和多次调整，而不仅仅是生成正确的代码。直接应用于这些任务时，LLMs 往往缺乏精细的领域先验知识，而现有的 MLE 方法通过线性或树状搜索限制了知识转移，只能局限于相邻的层级链接。因此，它们无法利用过去的完整轨迹或在分支间共享信息，限制了自我演化能力和搜索空间多样性。为了解决这些限制，我们引入了 AutoMLGen，这是一种基于大型语言模型的编码代理，集成了领域知识库以提供高质量的先验指导，并通过蒙特卡洛图搜索（MCGS）进行高效探索。MCGS 保留了 MCTS 的树引导探索，同时在扩展阶段嵌入图结构以实现动态路径重组、历史轨迹复用以及多解融合，从而支持自我演化和协作学习。结合细粒度的操作符集，此设计提高了稳定性和加速了收敛。在 MLE-Bench 上的评估表明，AutoMLGen 在多个维度（如平均奖牌率和有效提交率）上实现了最先进的性能，在 12 小时预算下（标准运行时间的一半）达到了最佳效果。代码可通过以下链接获取：这个 https URL。 

---
# Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling 

**Title (ZH)**: 寻求学习：面向令牌的动态门控机制在低资源视觉-语言建模中的应用 

**Authors**: Bianca-Mihaela Ganescu, Suchir Salhan, Andrew Caines, Paula Buttery  

**Link**: [PDF](https://arxiv.org/pdf/2510.08470)  

**Abstract**: Training vision-language models on cognitively-plausible amounts of data requires rethinking how models integrate multimodal information. Within the constraints of the Vision track for the BabyLM Challenge 2025, we propose a lightweight decoder-based architecture with (1) token-wise dynamic gating for adaptive fusion of linguistic and visual cues, (2) feature modulation and channel attention to maximise the utility of limited visual information and (3) auxiliary contrastive objectives for visual grounding. Evaluation on five benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows competitive or superior performance to multimodal baselines. More notably, our dynamic gate discovers interpretable patterns without explicit supervision, favouring visual cues for content words and linguistic cues for function words. While we identify limitations in the Challenge constraints, such as the information bottleneck created by global image embeddings and training instability from the dataset split, our findings establish dynamic gating as a powerful tool for efficient multimodal learning, offering both interpretability and performance even under severe constraints. 

**Abstract (ZH)**: 在认知合理的数据量下训练视觉-语言模型需要重新思考模型如何整合多模态信息。在BabyLM挑战2025视觉赛道的约束内，我们提出了一种轻量级解码器为基础的架构，该架构包含（1） token级动态门控以实现语境和视觉线索的自适应融合，（2）特征调制和通道注意力以最大化有限视觉信息的利用率，以及（3）辅助对比目标以实现视觉定位。在五个基准测试（BLiMP、BLiMP 补充、EWoK、Winoground 和 VQA）上的评估显示，我们的模型表现与多模态基线相当或更优。更加值得注意的是，我们的动态门控在没有显式监督的情况下发现可解释的模式，并且在内容词倾向于视觉线索、功能词倾向于语言线索方面表现得更好。尽管我们在挑战约束中识别到了一些局限性，例如由全局图像嵌入引起的信息瓶颈和由于数据集划分导致的训练不稳定性，但我们的研究结果建立了动态门控作为高效多模态学习的强大工具，即使在严格的约束下也能提供可解释性和性能。 

---
# Revisiting Hallucination Detection with Effective Rank-based Uncertainty 

**Title (ZH)**: 重新审视基于秩的不确定性在幻觉检测中的应用 

**Authors**: Rui Wang, Zeming Wei, Guanzhang Yue, Meng Sun  

**Link**: [PDF](https://arxiv.org/pdf/2510.08389)  

**Abstract**: Detecting hallucinations in large language models (LLMs) remains a fundamental challenge for their trustworthy deployment. Going beyond basic uncertainty-driven hallucination detection frameworks, we propose a simple yet powerful method that quantifies uncertainty by measuring the effective rank of hidden states derived from multiple model outputs and different layers. Grounded in the spectral analysis of representations, our approach provides interpretable insights into the model's internal reasoning process through semantic variations, while requiring no extra knowledge or additional modules, thus offering a combination of theoretical elegance and practical efficiency. Meanwhile, we theoretically demonstrate the necessity of quantifying uncertainty both internally (representations of a single response) and externally (different responses), providing a justification for using representations among different layers and responses from LLMs to detect hallucinations. Extensive experiments demonstrate that our method effectively detects hallucinations and generalizes robustly across various scenarios, contributing to a new paradigm of hallucination detection for LLM truthfulness. 

**Abstract (ZH)**: 在大型语言模型中检测幻觉仍然是实现其可靠部署的基本挑战。超越基本的不确定性驱动的幻觉检测框架，我们提出了一种简单而强大的方法，通过测量来自多个模型输出和不同层的隐藏状态的有效秩来量化不确定性。基于表示的频谱分析，我们的方法通过语义变化提供了可解释的洞见，无需额外知识或其他模块，因此兼具理论 elegance 和实践效率。同时，我们从理论证明了在内部（单个响应的表示）和外部（不同响应）量化不确定性的重要性，为使用来自大型语言模型的不同层和响应的表示来检测幻觉提供了正当理由。广泛实验表明，我们的方法有效检测幻觉并能够在各种场景下稳健泛化，为大型语言模型真实性的幻觉检测提供了一个新范式。 

---
# QAgent: A modular Search Agent with Interactive Query Understanding 

**Title (ZH)**: QAgent: 一个具有交互式查询理解功能的模块化搜索代理 

**Authors**: Yi Jiang, Lei Shen, Lujie Niu, Sendong Zhao, Wenbo Su, Bo Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2510.08383)  

**Abstract**: Large language models (LLMs) excel at natural language tasks but are limited by their static parametric knowledge, especially in knowledge-intensive task. Retrieval-augmented generation (RAG) mitigates this by integrating external information. However, (1) traditional RAG struggles with complex query understanding, and (2) even search agents trained with reinforcement learning (RL), despite their promise, still face generalization and deployment challenges. To address these limitations, we propose QAgent, a unified agentic RAG framework that employs a search agent for adaptive retrieval. This agent optimizes its understanding of the query through interactive reasoning and retrieval. To facilitate real-world application, we focus on modular search agent for query understanding that are plug-and-play in complex systems. Secifically, the agent follows a multi-step decision process trained with RL to maximize retrieval quality and support accurate downstream answers. We further analyze the strengths and weaknesses of end-to-end RL and propose a strategy that focuses on effective retrieval, thereby enhancing generalization in LLM applications. Experiments show QAgent excels at QA and serves as a plug-and-play module for real-world deployment. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在自然语言任务上表现出色，但受限于其静态参数化的知识，特别是在知识密集型任务中。检索增强生成（RAG）通过集成外部信息来缓解这一问题。然而，（1）传统RAG在复杂查询理解上存在问题，（2）即使使用强化学习（RL）训练的检索代理，尽管具有潜力，仍然面临泛化和部署的挑战。为解决这些限制，我们提出了一种名为QAgent的统一代理RAG框架，该框架采用检索代理进行适应性检索。该代理通过交互式推理和检索优化其对查询的理解。为了便于实际应用，我们重点关注模块化检索代理进行查询理解，这些代理可以在复杂系统中实现即插即用。具体而言，该代理通过强化学习训练，遵循多步决策过程，以最大限度地提高检索质量并支持准确的下游答案。我们进一步分析了端到端RL的优势和局限性，并提出了一种策略，专注于有效的检索，从而在大规模语言模型应用中增强泛化能力。实验表明，QAgent在问答任务中表现出色，并作为插件模块适用于实际部署。 

---
# LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings 

**Title (ZH)**: LLMs通过语义相似性诱发丽克特评分再现人类购买意向 

**Authors**: Benjamin F. Maier, Ulf Aslak, Luca Fiaschi, Nina Rismal, Kemble Fletcher, Christian C. Luhmann, Robbie Dow, Kli Pappas, Thomas V. Wiecki  

**Link**: [PDF](https://arxiv.org/pdf/2510.08338)  

**Abstract**: Consumer research costs companies billions annually yet suffers from panel biases and limited scale. Large language models (LLMs) offer an alternative by simulating synthetic consumers, but produce unrealistic response distributions when asked directly for numerical ratings. We present semantic similarity rating (SSR), a method that elicits textual responses from LLMs and maps these to Likert distributions using embedding similarity to reference statements. Testing on an extensive dataset comprising 57 personal care product surveys conducted by a leading corporation in that market (9,300 human responses), SSR achieves 90% of human test-retest reliability while maintaining realistic response distributions (KS similarity > 0.85). Additionally, these synthetic respondents provide rich qualitative feedback explaining their ratings. This framework enables scalable consumer research simulations while preserving traditional survey metrics and interpretability. 

**Abstract (ZH)**: 消费者研究每年使公司付出数十亿美元的代价，却遭受着面板偏差和样本量有限的困扰。大规模语言模型（LLMs）通过模拟合成消费者提供一种替代方案，但在直接要求其给出数值评分时会产生不现实的响应分布。我们提出语义相似度评分（SSR）方法，该方法从LLMs中引出文本响应，并利用嵌入相似度将这些响应映射到李克特量表分布。在包含57项个人护理产品调查的数据集上进行测试，该数据集由市场上领先公司执行（包含9,300份人类响应），SSR实现90%的人类重测可靠性，同时保持现实的响应分布（KS相似度>0.85）。此外，这些合成受访者还提供了丰富的定性反馈，解释其评分原因。该框架可实现可扩展的消费者研究模拟，同时保留传统调查指标和可解释性。 

---
# Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries 

**Title (ZH)**: 超越Pass@k：推理边界广度-深度度量 

**Authors**: Marius Dragoi, Ioana Pintilie, Florin Gogianu, Florin Brad  

**Link**: [PDF](https://arxiv.org/pdf/2510.08325)  

**Abstract**: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm to improve Large Language Models on reasoning tasks such as coding, math or logic. To assess the reasoning boundary (the fraction of problems a model can solve) researchers often report Pass@k at large sampling budgets. Recent results reveal a crossover phenomenon: while RLVR models outperform the base model at small k values, the base model usually outperforms them when sampling a very large number of completions. This has been interpreted as evidence that base models have a larger reasoning boundary. We argue that on tasks with discrete answer spaces, such as math with numeric outputs, Pass@k at large k reflects the increasingly higher chance of success in the limit of the number of trials rather than genuine reasoning, and can therefore be misleading. We propose Cover@tau, which measures the fraction of problems that a model can solve for which at least a tau proportion of completions are correct. Unlike Pass@k, Cover@tau captures reasoning under an explicit reliability threshold: models that rely on random guessing degrade rapidly as tau increases. We evaluate several RLVR models using Cover@tau-based metrics and illustrate how the relative rankings of popular algorithms change compared to Pass@1, offering a different perspective on reasoning boundaries. 

**Abstract (ZH)**: Verifiable奖励下的强化学习（RLVR）在提高大型语言模型在编码、数学或逻辑等推理任务上的能力方面已成为一种强大的范式。在大规模采样预算下，研究人员常报告Pass@k以评估推理边界（模型能解决的问题比例）。近期结果揭示了一个交叉现象：虽然在较小的k值下，RLVR模型优于基模型，但在大规模采样下，基模型通常表现更优。这被认为表明基模型具有更大的推理边界。我们认为，在具有离散答案空间的任务中，如带有数值输出的数学问题中，较大的k值下的Pass@k反映的是在大量试次极限下的成功概率越来越高的趋势，而不是真正的推理能力，因此可能会误导人。我们提出了Cover@tau来衡量模型可以解决的一类问题的比例，其中至少有tau比例的完成是正确的。与Pass@k不同，Cover@tau捕捉了基于显式可靠性的推理：依赖随机猜测的模型随着tau的增加会迅速退化。我们使用基于Cover@tau的指标评估了多个RLVR模型，并展示了相比于Pass@1时的相对排名的变化，提供了对推理边界的另一种视角。 

---
# First Try Matters: Revisiting the Role of Reflection in Reasoning Models 

**Title (ZH)**: 初次尝试很重要：重新审视反思在推理模型中的作用 

**Authors**: Liwei Kang, Yue Deng, Yao Xiao, Zhanfeng Mo, Wee Sun Lee, Lidong Bing  

**Link**: [PDF](https://arxiv.org/pdf/2510.08308)  

**Abstract**: Large language models have recently demonstrated significant gains in reasoning ability, often attributed to their capacity to generate longer chains of thought and engage in reflective reasoning. However, the contribution of reflections to performance improvement remains unclear. In this paper, we systematically analyze the rollouts of eight reasoning models on five mathematical datasets. We focus on reflective behaviours where the model has already produced an answer but continues reflecting before finalizing its output. Our analysis reveals that reflections are predominantly confirmatory and rarely alter the model's initial answer, a pattern consistent across models and datasets. To understand the role of reflections in training, we construct supervised fine-tuning (SFT) datasets with varying amounts of reflection steps. We observe that training models on rollouts with more reflection steps primarily enhances first-answer correctness rather than the ability to correct initially wrong answers through reflections. This motivates us to propose a question-aware early-stopping method that enhances inference-time token efficiency by stopping the reasoning process once a few plausible candidate answers are generated, thereby reducing unnecessary reflection steps. Motivated by this, we further propose to dynamically truncate the reflections after a candidate answer has appeared during generation, which reduces reasoning tokens by 24.5% across five mathematical datasets, within a 2.9% drop in accuracy. 

**Abstract (ZH)**: 大型语言模型在推理能力方面取得了显著进展，通常归因于它们生成更长思维链路和进行反思推理的能力。然而，反思对性能提升的具体贡献仍不清楚。本文系统分析了八种推理模型在五组数学数据集上的展开过程。我们重点关注模型已经生成答案但仍继续反思以最终确定输出的情况。我们的分析揭示，反思主要具有确认性，很少改变模型的初始答案，这一模式在不同模型和数据集上普遍存在。为了理解训练中的反思作用，我们构建了包含不同反思步骤数量的监督微调（SFT）数据集。实验观察到，使用更多反思步骤的展开过程训练模型主要提高了初始答案的正确性，而不是通过反思纠正最初错误答案的能力。这促使我们提出一种问题感知的早期停止方法，该方法在生成几个合理候选答案后停止推理过程，从而减少不必要的反思步骤，提高推理时的标记效率。作为进一步的改进，我们在生成过程中动态截断候选答案出现后的反思，这在五组数学数据集上将推理标记减少了24.5%，同时仅导致准确性下降2.9%。 

---
# Symmetry-Aware Fully-Amortized Optimization with Scale Equivariant Graph Metanetworks 

**Title (ZH)**: 对称意识全拟优化与尺度等变图元网络 

**Authors**: Bart Kuipers, Freek Byrman, Daniel Uyterlinde, Alejandro García-Castellanos  

**Link**: [PDF](https://arxiv.org/pdf/2510.08300)  

**Abstract**: Amortized optimization accelerates the solution of related optimization problems by learning mappings that exploit shared structure across problem instances. We explore the use of Scale Equivariant Graph Metanetworks (ScaleGMNs) for this purpose. By operating directly in weight space, ScaleGMNs enable single-shot fine-tuning of existing models, reducing the need for iterative optimization. We demonstrate the effectiveness of this approach empirically and provide a theoretical result: the gauge freedom induced by scaling symmetries is strictly smaller in convolutional neural networks than in multi-layer perceptrons. This insight helps explain the performance differences observed between architectures in both our work and that of Kalogeropoulos et al. (2024). Overall, our findings underscore the potential of symmetry-aware metanetworks as a powerful approach for efficient and generalizable neural network optimization. Open-source code: this https URL 

**Abstract (ZH)**: 使用权重空间中的Scale不变图元网络加速相关优化问题的求解通过学习利用问题实例间共享结构的映射。我们探索使用Scale不变图元网络（ScaleGMNs）为此目的。通过直接在权重空间中操作，ScaleGMNs允许对现有模型进行单次调整微调，减少迭代优化的需要。我们通过实证方法展示了该方法的有效性，并提供了一个理论结果：由尺度对称性引起的规范自由度在卷积神经网络中比在多层感知机中严格较小。这一洞见有助于解释我们在本文和Kalogeropoulos等人（2024）工作中观察到的架构性能差异。总体而言，我们的研究强调了具备对称意识的元网络作为高效且泛化能力强的神经网络优化方法的潜力。开源代码：详见此处。 

---
# Co-TAP: Three-Layer Agent Interaction Protocol Technical Report 

**Title (ZH)**: 共三层代理交互协议技术报告 

**Authors**: Shunyu An, Miao Wang, Yongchao Li, Dong Wan, Lina Wang, Ling Qin, Liqin Gao, Congyao Fan, Zhiyong Mao, Jiange Pu, Wenji Xia, Dong Zhao, Rui Hu, Ji Lu, Guiyue Zhou, Baoyu Tang, Yanqin Gao, Yongsheng Du, Daigang Xu, Lingjun Huang, Baoli Wang, Xiwen Zhang, Luyao Wang, Shilong Liu  

**Link**: [PDF](https://arxiv.org/pdf/2510.08263)  

**Abstract**: This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer agent interaction protocol designed to address the challenges faced by multi-agent systems across the three core dimensions of Interoperability, Interaction and Collaboration, and Knowledge Sharing. We have designed and proposed a layered solution composed of three core protocols: the Human-Agent Interaction Protocol (HAI), the Unified Agent Protocol (UAP), and the Memory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction layer, standardizing the flow of information between users, interfaces, and agents by defining a standardized, event-driven communication paradigm. This ensures the real-time performance, reliability, and synergy of interactions. As the core of the infrastructure layer, UAP is designed to break down communication barriers among heterogeneous agents through unified service discovery and protocol conversion mechanisms, thereby enabling seamless interconnection and interoperability of the underlying network. MEK, in turn, operates at the cognitive layer. By establishing a standardized ''Memory (M) - Extraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the ability to learn from individual experiences and form shareable knowledge, thereby laying the foundation for the realization of true collective intelligence. We believe this protocol framework will provide a solid engineering foundation and theoretical guidance for building the next generation of efficient, scalable, and intelligent multi-agent applications. 

**Abstract (ZH)**: Co-TAP（交互、代理、协议三位一体）：一种针对多代理系统在互操作性、交互与合作以及知识共享三大核心维度挑战的三层代理交互协议 

---
# Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness 

**Title (ZH)**: 链触发机制：一种辩证地增强自主鲁棒性的代理后门 

**Authors**: Jiyang Qiu, Xinbei Ma, Yunqing Xu, Zhuosheng Zhang, Hai Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2510.08238)  

**Abstract**: The rapid deployment of large language model (LLM)-based agents in real-world applications has raised serious concerns about their trustworthiness. In this work, we reveal the security and robustness vulnerabilities of these agents through backdoor attacks. Distinct from traditional backdoors limited to single-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a multi-step backdoor attack designed for long-horizon agentic control. CoTri relies on an ordered sequence. It starts with an initial trigger, and subsequent ones are drawn from the environment, allowing multi-step manipulation that diverts the agent from its intended task. Experimental results show that CoTri achieves a near-perfect attack success rate (ASR) while maintaining a near-zero false trigger rate (FTR). Due to training data modeling the stochastic nature of the environment, the implantation of CoTri paradoxically enhances the agent's performance on benign tasks and even improves its robustness against environmental distractions. We further validate CoTri on vision-language models (VLMs), confirming its scalability to multimodal agents. Our work highlights that CoTri achieves stable, multi-step control within agents, improving their inherent robustness and task capabilities, which ultimately makes the attack more stealthy and raises potential safty risks. 

**Abstract (ZH)**: 基于大型语言模型代理的快速部署在实际应用中引发了对其可信度的严重关切。本研究通过后门攻击揭示了这些代理的安全性和鲁棒性漏洞。不同于传统的仅限单步控制的后门，我们提出了链式触发后门（CoTri），一种设计用于长期代理控制的多步后门攻击。CoTri依赖于一个有序序列，从初始触发开始，并且后续触发来自环境，允许多步操控以使代理偏离其预定任务。实验结果表明，CoTri实现了近完美的攻击成功率（ASR）同时保持了近零的误触发率（FTR）。由于训练数据模拟了环境的随机性质，CoTri的植入意外地提高了代理在无害任务上的性能，甚至增强了其对抗环境干扰的鲁棒性。我们进一步在视觉-语言模型（VLMs）上验证了CoTri，证明了其对多模态代理的可扩展性。我们的工作强调，CoTri在代理中实现了稳定且多步的控制，提升了其固有的鲁棒性和任务能力，最终使攻击更加隐蔽并引发了潜在的安全风险。 

---
# Selection, Reflection and Self-Refinement: Revisit Reasoning Tasks via a Causal Lens 

**Title (ZH)**: 选择、反射与自我完善：通过因果视角重访推理任务 

**Authors**: Yunlong Deng, Boyang Sun, Yan Li, Lingjing Kong, Zeyu Tang, Kun Zhang, Guangyi Chen  

**Link**: [PDF](https://arxiv.org/pdf/2510.08222)  

**Abstract**: Due to their inherent complexity, reasoning tasks have long been regarded as rigorous benchmarks for assessing the capabilities of machine learning models, especially large language models (LLMs). Although humans can solve these tasks with ease, existing models, even after extensive pre-training and post-training at scale, still fail to perform reasoning reliably. In this paper, we revisit reasoning tasks from a causal perspective, seeking to understand their behavior in latent space and to offer insights for addressing their challenges. Specifically, we cast reasoning tasks as a selection mechanism, in which high-level logical concepts function as selection operators on the given observations, such as, identifying the correct answer in a math problem or filling the appropriate entry in Sudoku. We emphasize two key properties of this formulation that shed light on the difficulty of reasoning tasks. First, the latent space exceeds the observation space in complexity, even when the correct answer is fully determined by the observed input. Second, the latent variables, corresponding to logical thought, are densely structured and exhibit strong dependencies. Building on this formulation, we introduce a framework, called SR$^2$, that incorporates the estimated latent variables as feedback into the selection mechanism, thereby facilitating the learning of dense dependencies among latent representations. The framework consists of three key modules: reflective representation learning, dependency self-refinement, and periodic intermediate alignment. Experimentally, we show that our approach yields significant gains in reasoning accuracy, for example, attaining over 10$\%$ improvement in performance with 8$\times$ fewer parameters on the Sudoku and Maze tasks over the recent advances. 

**Abstract (ZH)**: 基于因果视角重访推理任务：理解和优化其行为 

---
# DODO: Causal Structure Learning with Budgeted Interventions 

**Title (ZH)**: DODO：预算化干预的因果结构学习 

**Authors**: Matteo Gregorini, Chiara Boldrini, Lorenzo Valerio  

**Link**: [PDF](https://arxiv.org/pdf/2510.08207)  

**Abstract**: Artificial Intelligence has achieved remarkable advancements in recent years, yet much of its progress relies on identifying increasingly complex correlations. Enabling causality awareness in AI has the potential to enhance its performance by enabling a deeper understanding of the underlying mechanisms of the environment. In this paper, we introduce DODO, an algorithm defining how an Agent can autonomously learn the causal structure of its environment through repeated interventions. We assume a scenario where an Agent interacts with a world governed by a causal Directed Acyclic Graph (DAG), which dictates the system's dynamics but remains hidden from the Agent. The Agent's task is to accurately infer the causal DAG, even in the presence of noise. To achieve this, the Agent performs interventions, leveraging causal inference techniques to analyze the statistical significance of observed changes. Results show better performance for DODO, compared to observational approaches, in all but the most limited resource conditions. DODO is often able to reconstruct with as low as zero errors the structure of the causal graph. In the most challenging configuration, DODO outperforms the best baseline by +0.25 F1 points. 

**Abstract (ZH)**: 人工智能在近年来取得了显著进展，但其进步很大程度上依赖于识别越来越复杂的相关性。使人工智能具备因果意识有望通过加深对环境底层机制的理解来提升其性能。本文介绍了一种算法DODO，该算法使智能体能够通过重复干预自主学习其环境的因果结构。我们假设一个场景，其中智能体与一个由因果有向无环图（DAG）支配的世界交互，该图规定了系统的动力学特性但对智能体而言是隐藏的。智能体的任务是在 noise 的存在下准确推断出因果DAG。为了实现这一目标，智能体执行干预，利用因果推理技术分析观测变化的统计显著性。结果表明，在除资源极度受限条件外的所有条件下，DODO 的性能优于观察性方法；在最具有挑战性的配置中，DODO 的F1分数比最佳基线高出0.25分，有时甚至能重建无误的因果图结构。 

---
# The Tournament Tree Method for preference elicitation in Multi-criteria decision-making 

**Title (ZH)**: Tournament Tree 方法在多准则决策中的偏好 elicitation 

**Authors**: Diego García-Zamora, Álvaro Labella, José Rui Figueira  

**Link**: [PDF](https://arxiv.org/pdf/2510.08197)  

**Abstract**: Pairwise comparison methods, such as Fuzzy Preference Relations and Saaty's Multiplicative Preference Relations, are widely used to model expert judgments in multi-criteria decision-making. However, their application is limited by the high cognitive load required to complete $m(m-1)/2$ comparisons, the risk of inconsistency, and the computational complexity of deriving consistent value scales. This paper proposes the Tournament Tree Method (TTM), a novel elicitation and evaluation framework that overcomes these limitations. The TTM requires only $m-1$ pairwise comparisons to obtain a complete, reciprocal, and consistent comparison matrix. The method consists of three phases: (i) elicitation of expert judgments using a reduced set of targeted comparisons, (ii) construction of the consistent pairwise comparison matrix, and (iii) derivation of a global value scale from the resulting matrix. The proposed approach ensures consistency by design, minimizes cognitive effort, and reduces the dimensionality of preference modeling from $m(m-1)/2$ to $m$ parameters. Furthermore, it is compatible with the classical Deck of Cards method, and thus it can handle interval and ratio scales. We have also developed a web-based tool that demonstrates its practical applicability in real decision-making scenarios. 

**Abstract (ZH)**: 基于锦标赛树的方法：一种新的专家判断 elicitation 和评价框架 

---
# Measuring What Matters: The AI Pluralism Index 

**Title (ZH)**: 衡量重要的东西：AI多元指数 

**Authors**: Rashid Mushkani  

**Link**: [PDF](https://arxiv.org/pdf/2510.08193)  

**Abstract**: Artificial intelligence systems increasingly mediate knowledge, communication, and decision making. Development and governance remain concentrated within a small set of firms and states, raising concerns that technologies may encode narrow interests and limit public agency. Capability benchmarks for language, vision, and coding are common, yet public, auditable measures of pluralistic governance are rare. We define AI pluralism as the degree to which affected stakeholders can shape objectives, data practices, safeguards, and deployment. We present the AI Pluralism Index (AIPI), a transparent, evidence-based instrument that evaluates producers and system families across four pillars: participatory governance, inclusivity and diversity, transparency, and accountability. AIPI codes verifiable practices from public artifacts and independent evaluations, explicitly handling "Unknown" evidence to report both lower-bound ("evidence") and known-only scores with coverage. We formalize the measurement model; implement a reproducible pipeline that integrates structured web and repository analysis, external assessments, and expert interviews; and assess reliability with inter-rater agreement, coverage reporting, cross-index correlations, and sensitivity analysis. The protocol, codebook, scoring scripts, and evidence graph are maintained openly with versioned releases and a public adjudication process. We report pilot provider results and situate AIPI relative to adjacent transparency, safety, and governance frameworks. The index aims to steer incentives toward pluralistic practice and to equip policymakers, procurers, and the public with comparable evidence. 

**Abstract (ZH)**: 人工智能系统 increasingly mediate知识,沟通和决策。开发和治理仍然集中在少数几家企业和国家手中，这引发了技术可能编码狭隘利益并限制公众参与的担忧。语言、视觉和编码能力基准常见，但包容性和治理的公共、可审计衡量标准罕见。我们定义人工智能多元主义为受影响的利益相关者能够塑造目标、数据实践、保护措施和部署的程度。我们提出了人工智能多元主义指数（AIPI），这是一种透明的、基于证据的工具，评估生产者和系统家族在四个支柱：参与式治理、包容性和多样性、透明度和问责制方面的表现。AIPI 通过公共材料和独立评估验证可验证的做法，并明确处理“未知”证据，报告最低界限、“证据”和仅已知分数的覆盖率。我们正式化了测量模型；实施了一个可重复的操作流水线，该流水线将结构化网页和存储库分析、外部评估和专家访谈整合在一起；并通过跨评价者一致性、覆盖率报告、跨指标相关性和敏感性分析评估其可靠性。协议、代码手册、评分脚本和证据图以受版本控制的发布形式公开维护，并具有公开裁决流程。我们报告了试点供应商的结果，并将 AIPI 相对于相邻的透明度、安全性和治理框架进行了定位。该指数旨在引导激励措施朝着多元主义实践的方向，并为政策制定者、采购者和公众提供可比的证据。 

---
# R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth? 

**Title (ZH)**: R-地平线:您的大模型在广度和深度上究竟能走得有多远？ 

**Authors**: Yi Lu, Jianing Wang, Linsen Guo, Wei He, Hongyin Tang, Tao Gui, Xuanjing Huang, Xuezhi Cao, Wei Wang, Xunliang Cai  

**Link**: [PDF](https://arxiv.org/pdf/2510.08189)  

**Abstract**: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought (CoT). However, existing benchmarks mainly focus on immediate, single-horizon tasks, failing to adequately evaluate models' ability to understand and respond to complex, long-horizon scenarios. To address this incomplete evaluation of Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to stimulate long-horizon reasoning behaviors in LRMs through query composition. Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising complex multi-step reasoning tasks with interdependent problems that span long reasoning horizons. Through comprehensive evaluation of LRMs using the R-HORIZON benchmark, we find that even the most advanced LRMs suffer significant performance degradation. Our analysis reveals that LRMs exhibit limited effective reasoning length and struggle to allocate thinking budget across multiple problems appropriately. Recognizing these limitations, we use R-HORIZON to construct long-horizon reasoning data for reinforcement learning with verified rewards (RLVR). Compared to training with single-horizon data, RLVR with R-HORIZON not only substantially improves performance on the multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as a scalable, controllable, and low-cost paradigm for enhancing and evaluating the long-horizon reasoning capabilities of LRMs. 

**Abstract (ZH)**: Recent Trends in Test-Time Scaling for Reasoning Models (e.g., OpenAI o1, DeepSeek-R1) Have Led to Remarkable Improvements through Long Chain-of-Thought (CoT): Addressing the Insufficiency of Current Benchmarks through R-HORIZON 

---
# Prepared mind, fast response: A temporal decoupling framework for adaptive knowledge orchestration in open-domain dialogue 

**Title (ZH)**: 未雨绸缪，迅疾响应：一种面向开放域对话的时空解耦适应性知识 orchestration 框架 

**Authors**: Jinling Gan, Churong Liang, Runnan Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.08175)  

**Abstract**: The latency-quality tradeoff is a fundamental constraint in open-domain dialogue AI systems, since comprehensive knowledge access necessitates prohibitive response delays. Contemporary approaches offer two inadequate solutions: lightweight instruct models achieve sub-second latency but lack reasoning depth, while tool-augmented ReAct agents enhance factuality through external knowledge at the cost of synchronous execution that blocks interaction during re- trieval processes. PMFR is thus proposed, with a tempo- ral decoupling framework that fundamentally resolves the contradiction through asynchronous knowledge orchestra- tion. PMFR employs three coordinated components: (1) a Knowledge Adequacy Evaluator for real-time sufficiency assessment, (2) a Lightweight Response Generator for imme- diate user interaction, and (3) an Asynchronous Knowledge Refinement Agent for background knowledge enhancement. This architecture maintains continuous conversational flow while progressively enriching knowledge coverage through intelligent triggering mechanisms. Evaluation results on Top- iOCQA demonstrate PMFR outperforms brute-force scaling: PMFR achieves 95.3% latency reduction (23.38s -> 1.09s) while preserving response quality comparable to heavyweight synchronous baselines (GEval-C: 0.613 vs. 0.620). 

**Abstract (ZH)**: 开放域对话AI系统中的延迟-质量权衡是基本限制，因为它要求全面的知识访问，从而导致不可接受的响应延迟。当代方法提供了两种不充分的解决方案：轻量级指令模型实现了亚秒级延迟，但缺乏推理深度；而工具增强的ReAct代理通过外部知识增强了事实性，但以同步执行为代价，在检索过程中阻塞了交互。因此提出了PMFR，这是一种通过异步知识编排根本解决矛盾的时域解耦框架。PMFR采用三个协调的组件：（1）知识充足性评估器进行实时充分性评估，（2）轻量级响应生成器进行即时用户交互，（3）背景知识增强的异步知识精炼代理。该架构保持了连续的对话流，并通过智能化触发机制逐步丰富知识覆盖面。Top-iOCQA上的评估结果显示，PMFR优于 brute-force 扩容：PMFR实现了95.3%的延迟减少（23.38s -> 1.09s），同时保持与重量级同步基线（GEval-C: 0.613 vs. 0.620）相当的响应质量。 

---
# Can Risk-taking AI-Assistants suitably represent entities 

**Title (ZH)**: Risk-taking AI-Assistants能否适当地代表实体 

**Authors**: Ali Mazyaki, Mohammad Naghizadeh, Samaneh Ranjkhah Zonouzaghi, Amirhossein Farshi Sotoudeh  

**Link**: [PDF](https://arxiv.org/pdf/2510.08114)  

**Abstract**: Responsible AI demands systems whose behavioral tendencies can be effectively measured, audited, and adjusted to prevent inadvertently nudging users toward risky decisions or embedding hidden biases in risk aversion. As language models (LMs) are increasingly incorporated into AI-driven decision support systems, understanding their risk behaviors is crucial for their responsible deployment. This study investigates the manipulability of risk aversion (MoRA) in LMs, examining their ability to replicate human risk preferences across diverse economic scenarios, with a focus on gender-specific attitudes, uncertainty, role-based decision-making, and the manipulability of risk aversion. The results indicate that while LMs such as DeepSeek Reasoner and Gemini-2.0-flash-lite exhibit some alignment with human behaviors, notable discrepancies highlight the need to refine bio-centric measures of manipulability. These findings suggest directions for refining AI design to better align human and AI risk preferences and enhance ethical decision-making. The study calls for further advancements in model design to ensure that AI systems more accurately replicate human risk preferences, thereby improving their effectiveness in risk management contexts. This approach could enhance the applicability of AI assistants in managing risk. 

**Abstract (ZH)**: 负责任的AI要求系统的行为倾向能够得到有效测量、审计和调整，以防无意中引导用户做出风险决策或在风险管理中嵌入隐藏偏见。随着语言模型（LMs）越来越多地被纳入AI驱动的决策支持系统，理解其风险行为对于其负责任的应用至关重要。本研究探讨了LMs的风险规避操纵性（MoRA），考察了它们在多种经济情境下复制人类风险偏好的能力，重点关注性别特定的态度、不确定性、基于角色的决策制定，以及风险规避的操纵性。研究结果表明，虽然DeepSeek Reasoner和Gemini-2.0-flash-lite等LMs展现出与人类行为的一些一致，但显著的差异强调了需改进生物中心化的操纵性度量。这些发现指出了改进AI设计以更好地契合人机风险偏好并提升伦理决策的方向。研究呼吁进一步提升模型设计，确保AI系统更准确地复制人类风险偏好，从而在风险管理情境中提高其有效性。这一体 approach 可能会增强AI助手在风险管理中的适用性。 

---
# From Ethical Declarations to Provable Independence: An Ontology-Driven Optimal-Transport Framework for Certifiably Fair AI Systems 

**Title (ZH)**: 从伦理声明到可验证独立性：一种面向本体的最优输运框架，用于认证公平的AI系统 

**Authors**: Sukriti Bhattacharya, Chitro Majumdar  

**Link**: [PDF](https://arxiv.org/pdf/2510.08086)  

**Abstract**: This paper presents a framework for provably fair AI that overcomes the limits of current bias mitigation methods by systematically removing all sensitive information and its proxies. Using ontology engineering in OWL 2 QL, it formally defines sensitive attributes and infers their proxies through logical reasoning, constructing a sigma algebra G that captures the full structure of biased patterns. Fair representations are then obtained via Delbaen Majumdar optimal transport, which generates variables independent of G while minimizing L2 distance to preserve accuracy. This guarantees true independence rather than mere decorrelation. By modeling bias as dependence between sigma algebras, compiling ontological knowledge into measurable structures, and using optimal transport as the unique fair transformation, the approach ensures complete fairness in tasks like loan approval, where proxies such as ZIP code reveal race. The result is a certifiable and mathematically grounded method for trustworthy AI. 

**Abstract (ZH)**: 本文提出了一种基于证明公平性的AI框架，通过系统地移除所有敏感信息及其代理，克服了当前偏见缓解方法的局限。利用OWL 2 QL本体工程，形式化定义了敏感属性并通过逻辑推理推断其代理，构建了一个σ代数G，捕获了偏见模式的完整结构。随后通过Delbaen和Majumdar最优运输生成与G独立的变量，同时最小化L2距离以保留准确性。这确保了真正的独立性而非简单的降相关。通过将偏见建模为σ代数之间的依赖关系，编译本体知识为可测量结构，并使用最优运输作为唯一的公平转换，该方法确保了诸如贷款审批等任务中的完全公平性，其中代理如邮政编码可能揭示种族信息。结果，本文提供了一种可验证且具有数学基础的值得信赖AI方法。 

---
# AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment 

**Title (ZH)**: AutoQual：一种用于评审质量评估的自动发现可解释特征的LLM代理 

**Authors**: Xiaochong Lan, Jie Feng, Yinxing Liu, Xinlei Shi, Yong Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.08081)  

**Abstract**: Ranking online reviews by their intrinsic quality is a critical task for e-commerce platforms and information services, impacting user experience and business outcomes. However, quality is a domain-dependent and dynamic concept, making its assessment a formidable challenge. Traditional methods relying on hand-crafted features are unscalable across domains and fail to adapt to evolving content patterns, while modern deep learning approaches often produce black-box models that lack interpretability and may prioritize semantics over quality. To address these challenges, we propose AutoQual, an LLM-based agent framework that automates the discovery of interpretable features. While demonstrated on review quality assessment, AutoQual is designed as a general framework for transforming tacit knowledge embedded in data into explicit, computable features. It mimics a human research process, iteratively generating feature hypotheses through reflection, operationalizing them via autonomous tool implementation, and accumulating experience in a persistent memory. We deploy our method on a large-scale online platform with a billion-level user base. Large-scale A/B testing confirms its effectiveness, increasing average reviews viewed per user by 0.79% and the conversion rate of review readers by 0.27%. 

**Abstract (ZH)**: 基于LLM的AutoQual代理框架：通过自动化可解释性特征发现提升在线评价质量评估 

---
# Multi-Condition Conformal Selection 

**Title (ZH)**: 多条件一致性选择 

**Authors**: Qingyang Hao, Wenbo Liao, Bingyi Jing, Hongxin Wei  

**Link**: [PDF](https://arxiv.org/pdf/2510.08075)  

**Abstract**: Selecting high-quality candidates from large-scale datasets is critically important in resource-constrained applications such as drug discovery, precision medicine, and the alignment of large language models. While conformal selection methods offer a rigorous solution with False Discovery Rate (FDR) control, their applicability is confined to single-threshold scenarios (i.e., y > c) and overlooks practical needs for multi-condition selection, such as conjunctive or disjunctive conditions. In this work, we propose the Multi-Condition Conformal Selection (MCCS) algorithm, which extends conformal selection to scenarios with multiple conditions. In particular, we introduce a novel nonconformity score with regional monotonicity for conjunctive conditions and a global Benjamini-Hochberg (BH) procedure for disjunctive conditions, thereby establishing finite-sample FDR control with theoretical guarantees. The integration of these components enables the proposed method to achieve rigorous FDR-controlled selection in various multi-condition environments. Extensive experiments validate the superiority of MCCS over baselines, its generalizability across diverse condition combinations, different real-world modalities, and multi-task scalability. 

**Abstract (ZH)**: 从大规模数据集选择高质量候选人在药物发现、精准医疗和大语言模型对齐等资源受限应用中至关重要。虽然符合性选择方法通过控制虚假发现率（FDR）提供了严格的解决方案，但它们的应用局限于单一阈值场景（即，y > c），且忽视了 conjunctive 和 disjunctive 条件下的多条件选择需求。本工作提出了一种多条件符合性选择（MCCS）算法，将符合性选择扩展到多条件场景。特别地，我们引入了一种具有区域单调性的新型非一致性评分用于 conjunctive 条件，并提出了一种全局 Benjamini-Hochberg (BH) 过程用于 disjunctive 条件，从而在理论上确保了有限样本下的 FDR 控制。这些组件的整合使提出的方法能够在各种多条件环境中实现严格的 FDR 控制选择。广泛的实验证明了 MCCS 在基线方法上的优越性，以及在不同条件组合、不同的现实世界模态和多任务扩展方面的普适性。 

---
# LinguaSim: Interactive Multi-Vehicle Testing Scenario Generation via Natural Language Instruction Based on Large Language Models 

**Title (ZH)**: LinguaSim：基于大语言模型的自然语言指令驱动的多车辆测试场景交互生成 

**Authors**: Qingyuan Shi, Qingwen Meng, Hao Cheng, Qing Xu, Jianqiang Wang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08046)  

**Abstract**: The generation of testing and training scenarios for autonomous vehicles has drawn significant attention. While Large Language Models (LLMs) have enabled new scenario generation methods, current methods struggle to balance command adherence accuracy with the realism of real-world driving environments. To reduce scenario description complexity, these methods often compromise realism by limiting scenarios to 2D, or open-loop simulations where background vehicles follow predefined, non-interactive behaviors. We propose LinguaSim, an LLM-based framework that converts natural language into realistic, interactive 3D scenarios, ensuring both dynamic vehicle interactions and faithful alignment between the input descriptions and the generated scenarios. A feedback calibration module further refines the generation precision, improving fidelity to user intent. By bridging the gap between natural language and closed-loop, interactive simulations, LinguaSim constrains adversarial vehicle behaviors using both the scenario description and the autonomous driving model guiding them. This framework facilitates the creation of high-fidelity scenarios that enhance safety testing and training. Experiments show LinguaSim can generate scenarios with varying criticality aligned with different natural language descriptions (ACT: 0.072 s for dangerous vs. 3.532 s for safe descriptions; comfortability: 0.654 vs. 0.764), and its refinement module effectively reduces excessive aggressiveness in LinguaSim's initial outputs, lowering the crash rate from 46.9% to 6.3% to better match user intentions. 

**Abstract (ZH)**: 基于语言模型的生成性测试与训练场景框架：从自然语言到高保真3D互动场景 

---
# AILoRA: Function-Aware Asymmetric Initialization for Low-Rank Adaptation of Large Language Models 

**Title (ZH)**: AILoRA：面向功能的异构初始化方法实现大型语言模型的低秩适应 

**Authors**: Xiaoshuang Ji, Zhendong Zhao, Xiaoyan Gu, Xiaojun Chen, Xin Zhao, Zeyao Liu  

**Link**: [PDF](https://arxiv.org/pdf/2510.08034)  

**Abstract**: Parameter-efficient finetuning (PEFT) aims to mitigate the substantial computational and memory overhead involved in adapting large-scale pretrained models to diverse downstream tasks. Among numerous PEFT strategies, Low-Rank Adaptation (LoRA) has emerged as one of the most widely adopted approaches due to its robust empirical performance and low implementation complexity. In practical deployment, LoRA is typically applied to the $W^Q$ and $W^V$ projection matrices of self-attention modules, enabling an effective trade-off between model performance and parameter efficiency. While LoRA has achieved considerable empirical success, it still encounters challenges such as suboptimal performance and slow convergence. To address these limitations, we introduce \textbf{AILoRA}, a novel parameter-efficient method that incorporates function-aware asymmetric low-rank priors. Our empirical analysis reveals that the projection matrices $W^Q$ and $W^V$ in the self-attention mechanism exhibit distinct parameter characteristics, stemming from their functional differences. Specifically, $W^Q$ captures task-specific semantic space knowledge essential for attention distributions computation, making its parameters highly sensitive to downstream task variations. In contrast, $W^V$ encodes token-level feature representations that tend to remain stable across tasks and layers. Leveraging these insights, AILoRA performs a function-aware initialization by injecting the principal components of $W^Q$ to retain task-adaptive capacity, and the minor components of $W^V$ to preserve generalizable feature representations. This asymmetric initialization strategy enables LoRA modules to better capture the specialized roles of attention parameters, thereby enhancing both finetuning performance and convergence efficiency. 

**Abstract (ZH)**: 参数高效微调（PEFT）旨在缓解大规模预训练模型适应多种下游任务时所涉及的显著计算和内存开销。在众多PEFT策略中，洛basename_1846（LoRA）由于其实验效果的稳健性和低实现复杂度，已成为最广泛采用的方法之一。在实际部署中，LoRA通常应用于自注意力模块的$W^Q$和$W^V$投影矩阵，从而在模型性能和参数效率之间实现有效的权衡。尽管LoRA在实验上取得了显著的成功，但仍面临诸如性能欠佳和收敛缓慢等挑战。为解决这些局限性，我们提出了**AILoRA**，一种结合了函数感知不对称低秩先验的新参数高效方法。我们的实验分析表明，自注意力机制中的$W^Q$和$W^V$投影矩阵表现出不同的参数特性，源于其功能差异。具体而言，$W^Q$捕捉到任务特异性语义空间知识，对于注意力分布计算至关重要，使得其参数对下游任务的变化高度敏感。相比之下，$W^V$编码的标记级特征表示在任务和层间更趋于稳定。利用这些见解，AILoRA通过在$W^Q$的主要成分中注入主成分来执行函数感知的初始化，以保留任务适配能力，并在$W^V$的次要成分中注入辅助成分来保持可泛化的特征表示。这种不对称初始化策略使LoRA模块能够更好地捕捉注意力参数的专业化角色，从而提高微调性能和收敛效率。 

---
# PEAR: Phase Entropy Aware Reward for Efficient Reasoning 

**Title (ZH)**: PEAR：相位熵感知奖励促进高效推理 

**Authors**: Chen Huang, Wei Lu, Wenxuan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08026)  

**Abstract**: Large Reasoning Models (LRMs) have achieved impressive performance on complex reasoning tasks by generating detailed chain-of-thought (CoT) explanations. However, these responses are often excessively long, containing redundant reasoning steps that inflate inference cost and reduce usability. Controlling the length of generated reasoning without sacrificing accuracy remains an open challenge. Through a systematic empirical analysis, we reveal a consistent positive correlation between model entropy and response length at different reasoning stages across diverse LRMs: the thinking phase exhibits higher entropy, reflecting exploratory behavior of longer responses, while the final answer phase shows lower entropy, indicating a more deterministic this http URL observation suggests that entropy at different reasoning stages can serve as a control knob for balancing conciseness and performance. Based on this insight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism that incorporating phase-dependent entropy into the reward design. Instead of treating all tokens uniformly, PEAR penalize excessive entropy during the thinking phase and allowing moderate exploration at the final answer phase, which encourages models to generate concise reasoning traces that retain sufficient flexibility to solve the task correctly. This enables adaptive control of response length without relying on explicit length targets or rigid truncation rules. Extensive experiments across four benchmarks demonstrate that PEAR consistently reduces response length while sustaining competitive accuracy across model scales. In addition, PEAR demonstrates strong out-of-distribution (OOD) robustness beyond the training distribution. Our code is available at: this https URL. 

**Abstract (ZH)**: 大型推理模型中的阶段熵感知奖励机制（Phase Entropy Aware Reward, PEAR）：平衡简洁性和性能的研究 

---
# Language Models Do Not Embed Numbers Continuously 

**Title (ZH)**: 语言模型并不连续嵌入数字。 

**Authors**: Alex O. Davies, Roussel Nzoyem, Nirav Ajmeri, Telmo M. Silva Filho  

**Link**: [PDF](https://arxiv.org/pdf/2510.08009)  

**Abstract**: Recent research has extensively studied how large language models manipulate integers in specific arithmetic tasks, and on a more fundamental level, how they represent numeric values. These previous works have found that language model embeddings can be used to reconstruct the original values, however, they do not evaluate whether language models actually model continuous values as continuous. Using expected properties of the embedding space, including linear reconstruction and principal component analysis, we show that language models not only represent numeric spaces as non-continuous but also introduce significant noise. Using models from three major providers (OpenAI, Google Gemini and Voyage AI), we show that while reconstruction is possible with high fidelity ($R^2 \geq 0.95$), principal components only explain a minor share of variation within the embedding space. This indicates that many components within the embedding space are orthogonal to the simple numeric input space. Further, both linear reconstruction and explained variance suffer with increasing decimal precision, despite the ordinal nature of the input space being fundamentally unchanged. The findings of this work therefore have implications for the many areas where embedding models are used, in-particular where high numerical precision, large magnitudes or mixed-sign values are common. 

**Abstract (ZH)**: 近期的研究广泛探讨了大型语言模型在特定算术任务中如何操作整数，以及在更基本的层面上如何表示数值。这些先前的工作发现语言模型嵌入可以用于重构原始值，但没有评估语言模型是否将连续值建模为连续值。利用嵌入空间的期望属性，包括线性重构和主成分分析，我们表明语言模型不仅将数值空间表示为非连续的，还会引入显著的噪声。使用三大提供者（OpenAI、Google Gemini 和 Voyage AI）的模型，我们显示虽然高保真度重构是可能的（$R^2 \geq 0.95$），但主成分仅解释了嵌入空间中微小的变异份额。这表明嵌入空间中的许多成分与简单的数值输入空间正交。此外，尽管输入空间的基本顺序性质保持不变，但随着十进制精度的增加，线性重构和解释变异性都会受到影响。因此，本文的发现对于嵌入模型被广泛应用于其中的许多领域具有重要意义，特别是这些领域中出现高数值精度、大数值量级或混合符号值的情况。 

---
# ReInAgent: A Context-Aware GUI Agent Enabling Human-in-the-Loop Mobile Task Navigation 

**Title (ZH)**: ReInAgent：一种支持人类在环的移动任务导航的上下文感知GUI代理 

**Authors**: Haitao Jia, Ming He, Zimo Yin, Likang Wu, Jianping Fan, Jitao Sang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07988)  

**Abstract**: Mobile GUI agents exhibit substantial potential to facilitate and automate the execution of user tasks on mobile phones. However, exist mobile GUI agents predominantly privilege autonomous operation and neglect the necessity of active user engagement during task execution. This omission undermines their adaptability to information dilemmas including ambiguous, dynamically evolving, and conflicting task scenarios, leading to execution outcomes that deviate from genuine user requirements and preferences. To address these shortcomings, we propose ReInAgent, a context-aware multi-agent framework that leverages dynamic information management to enable human-in-the-loop mobile task navigation. ReInAgent integrates three specialized agents around a shared memory module: an information-managing agent for slot-based information management and proactive interaction with the user, a decision-making agent for conflict-aware planning, and a reflecting agent for task reflection and information consistency validation. Through continuous contextual information analysis and sustained user-agent collaboration, ReInAgent overcomes the limitation of existing approaches that rely on clear and static task assumptions. Consequently, it enables more adaptive and reliable mobile task navigation in complex, real-world scenarios. Experimental results demonstrate that ReInAgent effectively resolves information dilemmas and produces outcomes that are more closely aligned with genuine user preferences. Notably, on complex tasks involving information dilemmas, ReInAgent achieves a 25% higher success rate than Mobile-Agent-v2. 

**Abstract (ZH)**: 基于上下文的多智能体框架ReInAgent：解决移动设备上的任务导航问题 

---
# VoiceAgentBench: Are Voice Assistants ready for agentic tasks? 

**Title (ZH)**: VoiceAgentBench: 语音助手准备好承担代理任务了吗？ 

**Authors**: Dhruv Jain, Harshit Shukla, Gautam Rajeev, Ashish Kulkarni, Chandra Khatri, Shubham Agarwal  

**Link**: [PDF](https://arxiv.org/pdf/2510.07978)  

**Abstract**: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs. 

**Abstract (ZH)**: 大规模语音语言模型（SpeechLMs）使语音助手能够理解自然的语音查询并执行复杂任务。然而，现有的语音基准主要集中在诸如转录或问答等孤立能力上，未系统评估涵盖多语言和文化理解以及对抗鲁棒性的代理场景。为此，我们引入了VoiceAgentBench，这是一个全面的基准，旨在评估SpeechLMs在现实语音代理环境中的表现。该基准包含超过5,500个合成语音查询，涵盖基于印度语境的对话，包括单工具调用、多工具工作流、多轮交互及安全性评估。基准支持英语、印地语和其他5种印度语言，反映了实际语言和文化的多样性。我们通过一种新颖的采样算法模拟说话人变异性，该算法基于说话人嵌入选择TTS语音转换的音频，最大化声学和说话人多样性。评估指标包括工具选择准确性、结构一致性以及工具调用的正确性，包括对抗鲁棒性。我们的实验揭示了上下文工具编排任务、印度语族泛化及对抗鲁棒性的显著差距，暴露出当前SpeechLMs的关键局限性。 

---
# TaoSR-SHE: Stepwise Hybrid Examination Reinforcement Learning Framework for E-commerce Search Relevance 

**Title (ZH)**: TaoSR-SHE：电子商务搜索相关性逐步混合增强学习框架 

**Authors**: Pengkun Jiao, Yiming Jin, Jianhui Yang, Chenhe Dong, Zerui Huang, Shaowei Yao, Xiaojiang Zhou, Dan Ou, Haihong Tang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07972)  

**Abstract**: Query-product relevance analysis is a foundational technology in e-commerce search engines and has become increasingly important in AI-driven e-commerce. The recent emergence of large language models (LLMs), particularly their chain-of-thought (CoT) reasoning capabilities, offers promising opportunities for developing relevance systems that are both more interpretable and more robust. However, existing training paradigms have notable limitations: SFT and DPO suffer from poor generalization on long-tail queries and from a lack of fine-grained, stepwise supervision to enforce rule-aligned reasoning. In contrast, reinforcement learning with verification rewards (RLVR) suffers from sparse feedback, which provides insufficient signal to correct erroneous intermediate steps, thereby undermining logical consistency and limiting performance in complex inference scenarios.
To address these challenges, we introduce the Stepwise Hybrid Examination Reinforcement Learning framework for Taobao Search Relevance (TaoSR-SHE). At its core is Stepwise Reward Policy Optimization (SRPO), a reinforcement learning algorithm that leverages step-level rewards generated by a hybrid of a high-quality generative stepwise reward model and a human-annotated offline verifier, prioritizing learning from critical correct and incorrect reasoning steps. TaoSR-SHE further incorporates two key techniques: diversified data filtering to encourage exploration across varied reasoning paths and mitigate policy entropy collapse, and multi-stage curriculum learning to foster progressive capability growth. Extensive experiments on real-world search benchmarks show that TaoSR-SHE improves both reasoning quality and relevance-prediction accuracy in large-scale e-commerce settings, outperforming SFT, DPO, GRPO, and other baselines, while also enhancing interpretability and robustness. 

**Abstract (ZH)**: 淘宝搜索相关性逐步混合检验强化学习框架（TaoSR-SHE） 

---
# Agent-Based Genetic Algorithm for Crypto Trading Strategy Optimization 

**Title (ZH)**: 基于代理的遗传算法在加密交易策略优化中的应用 

**Authors**: Qiushi Tian, Churong Liang, Kairan Hong, Runnan Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.07943)  

**Abstract**: Cryptocurrency markets present formidable challenges for trading strategy optimization due to extreme volatility, non-stationary dynamics, and complex microstructure patterns that render conventional parameter optimization methods fundamentally inadequate. We introduce Cypto Genetic Algorithm Agent (CGA-Agent), a pioneering hybrid framework that synergistically integrates genetic algorithms with intelligent multi-agent coordination mechanisms for adaptive trading strategy parameter optimization in dynamic financial environments. The framework uniquely incorporates real-time market microstructure intelligence and adaptive strategy performance feedback through intelligent mechanisms that dynamically guide evolutionary processes, transcending the limitations of static optimization approaches. Comprehensive empirical evaluation across three cryptocurrencies demonstrates systematic and statistically significant performance improvements on both total returns and risk-adjusted metrics. 

**Abstract (ZH)**: 加密货币市场由于极端波动性、非平稳动态以及复杂的微观结构模式，为交易策略优化带来了巨大挑战。我们提出了一种名为Crypto Genetic Algorithm Agent (CGA-Agent) 的创新性混合框架，该框架将遗传算法与智能多agent协调机制结合，以适应在动态金融市场中的自适应交易策略参数优化。该框架通过智能机制实时整合市场微观结构智能和自适应策略性能反馈，动态引导进化过程，超越了静态优化方法的局限性。全面的实证研究表明，在三种加密货币上的表现系统性且统计学上显著优于基准方法，提高了总投资回报和风险调整后的指标。 

---
# Enabling Personalized Long-term Interactions in LLM-based Agents through Persistent Memory and User Profiles 

**Title (ZH)**: 基于持久记忆和用户画像实现LLM驱动代理的个性化长期交互 

**Authors**: Rebecca Westhäußer, Wolfgang Minker, Sebatian Zepf  

**Link**: [PDF](https://arxiv.org/pdf/2510.07925)  

**Abstract**: Large language models (LLMs) increasingly serve as the central control unit of AI agents, yet current approaches remain limited in their ability to deliver personalized interactions. While Retrieval Augmented Generation enhances LLM capabilities by improving context-awareness, it lacks mechanisms to combine contextual information with user-specific data. Although personalization has been studied in fields such as human-computer interaction or cognitive science, existing perspectives largely remain conceptual, with limited focus on technical implementation. To address these gaps, we build on a unified definition of personalization as a conceptual foundation to derive technical requirements for adaptive, user-centered LLM-based agents. Combined with established agentic AI patterns such as multi-agent collaboration or multi-source retrieval, we present a framework that integrates persistent memory, dynamic coordination, self-validation, and evolving user profiles to enable personalized long-term interactions. We evaluate our approach on three public datasets using metrics such as retrieval accuracy, response correctness, or BertScore. We complement these results with a five-day pilot user study providing initial insights into user feedback on perceived personalization. The study provides early indications that guide future work and highlights the potential of integrating persistent memory and user profiles to improve the adaptivity and perceived personalization of LLM-based agents. 

**Abstract (ZH)**: 大规模语言模型（LLMs）越来越多地作为AI代理的中央控制单元，但当前的方法在提供个性化交互方面仍有限制。虽然检索增强生成通过提高情境意识来增强LLM的能力，但缺乏将情境信息与用户特定数据相结合的机制。尽管在人机交互或认知科学等领域已经研究了个性化，但现有视角大多仍停留在概念层面，缺乏技术实施的重点。为弥补这些差距，我们基于统一的个性化概念定义作为理论基础，提出了适应性、以用户为中心的基于LLM的代理的技术需求。结合多智能体协作或多源检索等成熟的代理型AI模式，我们提出了一个框架，该框架整合了持久记忆、动态协调、自我验证和不断进化用户档案，以实现个性化的长期交互。我们使用检索准确性、响应正确性或BertScore等指标，在三个公开数据集上评估了我们的方法，并通过为期五天的初步用户研究补充了这些结果，提供了一些关于用户感知个性化反馈的初步见解。研究为未来工作提供了早期指示，并强调了整合持久记忆和用户档案以提高基于LLM的代理的适应性和感知个性化潜力。 

---
# Profit Mirage: Revisiting Information Leakage in LLM-based Financial Agents 

**Title (ZH)**: 利润幻影：重访基于LLM的金融代理中的信息泄漏问题 

**Authors**: Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, Xiangmin Xu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07920)  

**Abstract**: LLM-based financial agents have attracted widespread excitement for their ability to trade like human experts. However, most systems exhibit a "profit mirage": dazzling back-tested returns evaporate once the model's knowledge window ends, because of the inherent information leakage in LLMs. In this paper, we systematically quantify this leakage issue across four dimensions and release FinLake-Bench, a leakage-robust evaluation benchmark. Furthermore, to mitigate this issue, we introduce FactFin, a framework that applies counterfactual perturbations to compel LLM-based agents to learn causal drivers instead of memorized outcomes. FactFin integrates four core components: Strategy Code Generator, Retrieval-Augmented Generation, Monte Carlo Tree Search, and Counterfactual Simulator. Extensive experiments show that our method surpasses all baselines in out-of-sample generalization, delivering superior risk-adjusted performance. 

**Abstract (ZH)**: 基于LLM的金融代理尽管因其像人类专家一样交易的能力而引起了广泛的关注，但大多数系统表现出“利润幻影”：回测回报在模型知识窗口结束时消失，这是因为LLM固有的信息泄漏。本文从四个维度系统地量化了这一泄漏问题，并发布了FinLake-Bench，一个抗泄漏评估基准。为进一步缓解这一问题，我们引入了FactFin框架，该框架通过反事实扰动促使基于LLM的代理学习因果驱动因素而非记忆化的结果。FactFin整合了四种核心组件：策略代码生成器、检索增强生成、蒙特卡洛树搜索和反事实模拟器。广泛实验证明，我们的方法在样本外泛化方面超越了所有基线方法，提供了优越的风险调整收益。 

---
# Towards Meaningful Transparency in Civic AI Systems 

**Title (ZH)**: 面向公民AI系统的有意义透明度 

**Authors**: Dave Murray-Rust, Kars Alfrink, Cristina Zaga  

**Link**: [PDF](https://arxiv.org/pdf/2510.07889)  

**Abstract**: Artificial intelligence has become a part of the provision of governmental services, from making decisions about benefits to issuing fines for parking violations. However, AI systems rarely live up to the promise of neutral optimisation, creating biased or incorrect outputs and reducing the agency of both citizens and civic workers to shape the way decisions are made. Transparency is a principle that can both help subjects understand decisions made about them and shape the processes behind those decisions. However, transparency as practiced around AI systems tends to focus on the production of technical objects that represent algorithmic aspects of decision making. These are often difficult for publics to understand, do not connect to potential for action, and do not give insight into the wider socio-material context of decision making. In this paper, we build on existing approaches that take a human-centric view on AI transparency, combined with a socio-technical systems view, to develop the concept of meaningful transparency for civic AI systems: transparencies that allow publics to engage with AI systems that affect their lives, connecting understanding with potential for action. 

**Abstract (ZH)**: 人工智能已成为政府服务的一部分，从关于福利的决策到对停车违规行为罚款的发放。然而，AI系统很少兑现中立优化的承诺，产生有偏向或错误的输出，并减少公民和公务人员塑造决策方式的自主权。透明性作为一项原则，既能帮助主体理解关于他们的决策，也能塑造这些决策背后的进程。然而，围绕AI系统实践的透明性往往侧重于生产代表决策过程中算法方面的技术对象。这些对象通常难以公众理解，不与潜在行动能力相连，也不揭示决策过程中更广泛的社会物质背景。本文在此基础上，结合以人为本的AI透明性方法和 socio-technical 系统视角，发展了公民AI系统有意义透明性的概念：允许公众参与影响他们生活的AI系统，将理解与潜在行动能力连接起来。 

---
# Understanding DeepResearch via Reports 

**Title (ZH)**: 通过报告理解DeepResearch 

**Authors**: Tianyu Fan, Xinyao Niu, Yuxiang Zheng, Fengji Zhang, Chengen Huang, Bei Chen, Junyang Lin, Chao Huang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07861)  

**Abstract**: DeepResearch agents represent a transformative AI paradigm, conducting expert-level research through sophisticated reasoning and multi-tool integration. However, evaluating these systems remains critically challenging due to open-ended research scenarios and existing benchmarks that focus on isolated capabilities rather than holistic performance. Unlike traditional LLM tasks, DeepResearch systems must synthesize diverse sources, generate insights, and present coherent findings, which are capabilities that resist simple verification. To address this gap, we introduce DeepResearch-ReportEval, a comprehensive framework designed to assess DeepResearch systems through their most representative outputs: research reports. Our approach systematically measures three dimensions: quality, redundancy, and factuality, using an innovative LLM-as-a-Judge methodology achieving strong expert concordance. We contribute a standardized benchmark of 100 curated queries spanning 12 real-world categories, enabling systematic capability comparison. Our evaluation of four leading commercial systems reveals distinct design philosophies and performance trade-offs, establishing foundational insights as DeepResearch evolves from information assistants toward intelligent research partners. Source code and data are available at: this https URL. 

**Abstract (ZH)**: 深度研究代理代表了一种 transformative 的人工智能范式，通过复杂的推理和多工具集成来进行专家级研究。然而，由于开放的研究场景和现有的主要关注孤立能力而非整体性能的基准，这些系统的评估仍然极具挑战性。与传统的语言模型任务不同，深度研究系统必须综合多种资源、生成见解并呈现连贯的研究成果，这些能力难以简单验证。为解决这一问题，我们引入了深度研究报告评估（DeepResearch-ReportEval），这是一种全面的框架，旨在通过研究报告这一最具代表性的输出来评估深度研究系统。我们的方法系统性地评估了三个维度：质量、冗余和事实性，采用一种创新的LLM作为法官的方法，实现了强烈的专业一致性。我们贡献了一个包含100个精心策划的查询的标准基准，覆盖12个真实世界类别，使系统的能力比较得以系统化。对于我们对四家领先商用系统的评估揭示了不同的设计哲学和性能权衡，这些洞察为深度研究从信息助手向智能研究伙伴演进奠定了基础。源代码和数据可在以下链接获取：this https URL。 

---
# Augur: Modeling Covariate Causal Associations in Time Series via Large Language Models 

**Title (ZH)**: Augur：通过大型语言模型建模时间序列中的协变量因果关系 

**Authors**: Zhiqing Cui, Binwu Wang, Qingxiang Liu, Yeqiang Wang, Zhengyang Zhou, Yuxuan Liang, Yang Wang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07858)  

**Abstract**: Large language models (LLM) have emerged as a promising avenue for time series forecasting, offering the potential to integrate multimodal data. However, existing LLM-based approaches face notable limitations-such as marginalized role in model architectures, reliance on coarse statistical text prompts, and lack of interpretability. In this work, we introduce Augur, a fully LLM driven time series forecasting framework that exploits LLM causal reasoning to discover and use directed causal associations among covariates. Augur uses a two stage teacher student architecture where a powerful teacher LLM infers a directed causal graph from time series using heuristic search together with pairwise causality testing. A lightweight student agent then refines the graph and fine tune on high confidence causal associations that are encoded as rich textual prompts to perform forecasting. This design improves predictive accuracy while yielding transparent, traceable reasoning about variable interactions. Extensive experiments on real-world datasets with 25 baselines demonstrate that Augur achieves competitive performance and robust zero-shot generalization. 

**Abstract (ZH)**: 基于大型语言模型的时间序列预测框架Augur：利用因果推理发现和利用协变量的有向因果关系 

---
# FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning 

**Title (ZH)**: FinMR：一种知识密集型多模态基准用于高级金融推理 

**Authors**: Shuangyan Deng, Haizhou Peng, Jiachen Xu, Rui Mao, Ciprian Doru Giurcăneanu, Jiamou Liu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07852)  

**Abstract**: Multimodal Large Language Models (MLLMs) have made substantial progress in recent years. However, their rigorous evaluation within specialized domains like finance is hindered by the absence of datasets characterized by professional-level knowledge intensity, detailed annotations, and advanced reasoning complexity. To address this critical gap, we introduce FinMR, a high-quality, knowledge-intensive multimodal dataset explicitly designed to evaluate expert-level financial reasoning capabilities at a professional analyst's standard. FinMR comprises over 3,200 meticulously curated and expertly annotated question-answer pairs across 15 diverse financial topics, ensuring broad domain diversity and integrating sophisticated mathematical reasoning, advanced financial knowledge, and nuanced visual interpretation tasks across multiple image types. Through comprehensive benchmarking with leading closed-source and open-source MLLMs, we highlight significant performance disparities between these models and professional financial analysts, uncovering key areas for model advancement, such as precise image analysis, accurate application of complex financial formulas, and deeper contextual financial understanding. By providing richly varied visual content and thorough explanatory annotations, FinMR establishes itself as an essential benchmark tool for assessing and advancing multimodal financial reasoning toward professional analyst-level competence. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）在近年来取得了显著进展。然而，它们在金融等专业领域中的严格评估受到缺乏具有专业级知识强度、详细注释和高级推理复杂度的数据集的限制。为了解决这一关键差距，我们介绍了FinMR，一个高质量的专业级知识密集型多模态数据集，专门用于以专业分析师的标准评估专家级金融推理能力。FinMR 包含了超过 3,200 个精心选择和专业注释的问题-答案对，覆盖了 15 个不同的金融主题，确保了广泛的主题多样性，并结合了复杂的数学推理、高级金融知识和多类型图像的细微视觉解释任务。通过与领先的专业闭源和开源 MLLMs 的全面基准测试，我们突显了这些模型与专业金融分析师之间的显著性能差异，揭示了模型改进的关键领域，如精确的图像分析、复杂金融公式的准确应用以及更深入的金融背景理解。凭借丰富多样的视觉内容和详尽的解释性注释，FinMR 成为了评估并推动多模态金融推理达到专业分析师水平的重要基准工具。 

---
# An LLM-Powered Cooperative Framework for Large-Scale Multi-Vehicle Navigation 

**Title (ZH)**: 基于大语言模型的多车辆导航协作框架 

**Authors**: Yuping Zhou, Siqi Lai, Jindong Han, Hao Liu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07825)  

**Abstract**: The rise of Internet of Vehicles (IoV) technologies is transforming traffic management from isolated control to a collective, multi-vehicle process. At the heart of this shift is multi-vehicle dynamic navigation, which requires simultaneously routing large fleets under evolving traffic conditions. Existing path search algorithms and reinforcement learning methods struggle to scale to city-wide networks, often failing to capture the nonlinear, stochastic, and coupled dynamics of urban traffic. To address these challenges, we propose CityNav, a hierarchical, LLM-powered framework for large-scale multi-vehicle navigation. CityNav integrates a global traffic allocation agent, which coordinates strategic traffic flow distribution across regions, with local navigation agents that generate locally adaptive routes aligned with global directives. To enable effective cooperation, we introduce a cooperative reasoning optimization mechanism, in which agents are jointly trained with a dual-reward structure: individual rewards promote per-vehicle efficiency, while shared rewards encourage network-wide coordination and congestion reduction. Extensive experiments on four real-world road networks of varying scales (up to 1.6 million roads and 430,000 intersections) and traffic datasets demonstrate that CityNav consistently outperforms nine classical path search and RL-based baselines in city-scale travel efficiency and congestion mitigation. Our results highlight the potential of LLMs to enable scalable, adaptive, and cooperative city-wide traffic navigation, providing a foundation for intelligent, large-scale vehicle routing in complex urban environments. Our project is available at this https URL. 

**Abstract (ZH)**: IOV技术的兴起正在将交通管理从孤立控制转变为集体、多车辆过程。这一转变的核心是多车辆动态导航，要求在不断变化的交通条件下同时为大量车队进行路径规划。现有的路径搜索算法和强化学习方法难以扩展到城市规模的网络，往往无法捕捉城市交通的非线性、随机性和耦合动力学。为了解决这些挑战，我们提出CityNav，一种基于层次结构和大语言模型的大型多车辆导航框架。CityNav将一个全局交通分配代理与本地导航代理结合起来，前者协调不同区域的战略性交通流量分布，后者生成与全局指令一致的本地自适应路线。为了实现有效的合作，我们引入了一种协同推理优化机制，在该机制中，代理通过双奖励结构进行联合训练：个体奖励促进单车辆效率，而共享奖励鼓励网络范围内的协调和缓解拥堵。在四个规模不等的真实世界道路网络（最多160万条道路和430,000个交叉口）和交通数据集上的 extensive 实验表明，CityNav 在城市规模的出行效率和拥堵缓解方面始终优于九种经典路径搜索和基于强化学习的方法。我们的研究结果强调了大语言模型在实现可扩展、自适应和合作的城市交通导航方面的潜力，为复杂城市环境中的智能大规模车辆路径规划提供了基础。该项目可在以下链接查看：this https URL。 

---
# Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games 

**Title (ZH)**: 威胁下的战略沟通：追求逃避博弈中信息权衡的学习 

**Authors**: Valerio La Gatta, Dolev Mutzari, Sarit Kraus, VS Subrahmanian  

**Link**: [PDF](https://arxiv.org/pdf/2510.07813)  

**Abstract**: Adversarial environments require agents to navigate a key strategic trade-off: acquiring information enhances situational awareness, but may simultaneously expose them to threats. To investigate this tension, we formulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursuer agent must decide when to communicate in order to obtain the evader's position. Each communication reveals the pursuer's location, increasing the risk of being targeted. Both agents learn their movement policies via reinforcement learning, while the pursuer additionally learns a communication policy that balances observability and risk. We propose SHADOW (Strategic-communication Hybrid Action Decision-making under partial Observation for Warfare), a multi-headed sequential reinforcement learning framework that integrates continuous navigation control, discrete communication actions, and opponent modeling for behavior prediction. Empirical evaluations show that SHADOW pursuers achieve higher success rates than six competitive baselines. Our ablation study confirms that temporal sequence modeling and opponent modeling are critical for effective decision-making. Finally, our sensitivity analysis reveals that the learned policies generalize well across varying communication risks and physical asymmetries between agents. 

**Abstract (ZH)**: 对抗环境要求智能体在关键的战略权衡中导航：获取信息可以增强态势感知，但同时可能会使智能体暴露于威胁之中。为了探讨这一矛盾，我们提出了一个追逐-逃逸-暴露-隐蔽游戏（PursuitEvasion-Exposure-Concealment Game, PEEC）模型，其中追逐智能体需要决定何时进行通信以获取逃逸者的方位。每次通信都会揭示追逐者的方位，增加被目标锁定的风险。两个智能体通过强化学习学习其运动策略，追逐者还通过学习一个平衡可检测性和风险的通信策略。我们提出了SHADOW（基于部分观测的战略通信混合动作决策框架用于战争），这是一种多头序列强化学习框架，结合了连续导航控制、离散通信动作以及对手建模以进行行为预测。实证评估表明，SHADOW追逐者在成功率上超过了六个竞争性基线。我们的消融研究证实了时间序列建模和对手建模对于有效决策是至关重要的。最后，我们的敏感性分析表明，学习到的策略能够很好地泛化到不同的通信风险和智能体之间的物理不对称性。 

---
# GCPO: When Contrast Fails, Go Gold 

**Title (ZH)**: GCPO: 当对比失效时，追求卓越 

**Authors**: Hao Wu, Wei Liu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07790)  

**Abstract**: Reinforcement learning has been widely applied to enhance the reasoning capabilities of large language models. Extending the inference limits of smaller models has become a prominent research focus. However, algorithms such as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the upper bound of a model's rollout responses is entirely determined by the model itself, preventing the acquisition of knowledge from samples that are either all incorrect or all correct. In this paper, we introduce Group Contrastive Policy Optimization (GCPO), a method that incorporates external standard reference answers. When the model cannot solve a problem, the reference answer supplies the correct response, steering the model toward an unequivocally accurate update direction. This approach offers two main advantages: (1) it improves training efficiency by fully utilizing every sample; (2) it enables the model to emulate the problem solving strategy of the reference answer during training, thereby enhancing generalization in reasoning. GCPO achieves outstanding results across multiple benchmark datasets, yielding substantial improvements over the baseline model. Our code is available at: this https URL. 

**Abstract (ZH)**: 强化学习已被广泛应用于提升大型语言模型的推理能力。扩展较小模型的推理极限已成为研究的重点。然而，诸如Group Relative Policy Optimization (GRPO)等算法存在明显不足：模型的展开响应的上限完全由模型自身决定，阻止了从全是错误或全是正确的样本中获取知识。在本文中，我们引入了Group Contrastive Policy Optimization (GCPO)，该方法结合了外部的标准参考答案。当模型无法解决问题时，参考答案提供正确的答案，引导模型朝着明确准确的方向更新。这种方法主要有两大优势：（1）通过充分利用每个样本提高训练效率；（2）使模型在训练过程中模仿参考答案的问题解决策略，从而增强推理能力的泛化能力。GCPO在多个基准数据集上取得了优异的结果，显著优于基线模型。我们的代码可在以下链接获取：this https URL。 

---
# An approach for systematic decomposition of complex llm tasks 

**Title (ZH)**: 一种系统分解复杂LLM任务的方法 

**Authors**: Tianle Zhou, Jiakai Xu, Guanhong Liu, Jiaxiang Liu, Haonan Wang, Eugene Wu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07772)  

**Abstract**: Large Language Models (LLMs) suffer from reliability issues on complex tasks, as existing decomposition methods are heuristic and rely on agent or manual decomposition. This work introduces a novel, systematic decomposition framework that we call Analysis of CONstraint-Induced Complexity (ACONIC), which models the task as a constraint problem and leveraging formal complexity measures to guide decomposition. On combinatorial (SATBench) and LLM database querying tasks (Spider), we find that by decomposing the tasks following the measure of complexity, agent can perform considerably better (10-40 percentage point). 

**Abstract (ZH)**: 大型语言模型（LLMs）在复杂任务中存在可靠性问题，现有分解方法具有启发式性质且依赖于代理或手动分解。本研究引入了一种名为约束诱导复杂性分析（ACONIC）的新型系统分解框架，将任务建模为约束问题，并利用形式化的复杂度度量来指导分解。在组合优化（SATBench）和LLM数据库查询任务（Spider）中，通过依据复杂度度量进行任务分解，代理可以显著提升性能（提升幅度达10-40个百分点）。 

---
# From Noisy to Native: LLM-driven Graph Restoration for Test-Time Graph Domain Adaptation 

**Title (ZH)**: 从 noisy 到 native: 由大规模语言模型驱动的图形恢复方法，用于测试时图形域适应 

**Authors**: Xiangwei Lv, JinLuan Yang, Wang Lin, Jingyuan Chen, Beishui Liao  

**Link**: [PDF](https://arxiv.org/pdf/2510.07762)  

**Abstract**: Graph domain adaptation (GDA) has achieved great attention due to its effectiveness in addressing the domain shift between train and test data. A significant bottleneck in existing graph domain adaptation methods is their reliance on source-domain data, which is often unavailable due to privacy or security concerns. This limitation has driven the development of Test-Time Graph Domain Adaptation (TT-GDA), which aims to transfer knowledge without accessing the source examples. Inspired by the generative power of large language models (LLMs), we introduce a novel framework that reframes TT-GDA as a generative graph restoration problem, "restoring the target graph to its pristine, source-domain-like state". There are two key challenges: (1) We need to construct a reasonable graph restoration process and design an effective encoding scheme that an LLM can understand, bridging the modality gap. (2) We need to devise a mechanism to ensure the restored graph acquires the intrinsic features of the source domain, even without access to the source data. To ensure the effectiveness of graph restoration, we propose GRAIL, that restores the target graph into a state that is well-aligned with the source domain. Specifically, we first compress the node representations into compact latent features and then use a graph diffusion process to model the graph restoration process. Then a quantization module encodes the restored features into discrete tokens. Building on this, an LLM is fine-tuned as a generative restorer to transform a "noisy" target graph into a "native" one. To further improve restoration quality, we introduce a reinforcement learning process guided by specialized alignment and confidence rewards. Extensive experiments demonstrate the effectiveness of our approach across various datasets. 

**Abstract (ZH)**: 基于图的测试时领域适应 (TT-GDA)：一种生成图恢复框架 

---
# Haibu Mathematical-Medical Intelligent Agent:Enhancing Large Language Model Reliability in Medical Tasks via Verifiable Reasoning Chains 

**Title (ZH)**: 可验证推理链增强医疗任务中大型语言模型可靠性的人医智能代理 

**Authors**: Yilun Zhang, Dexing Kong  

**Link**: [PDF](https://arxiv.org/pdf/2510.07748)  

**Abstract**: Large Language Models (LLMs) show promise in medicine but are prone to factual and logical errors, which is unacceptable in this high-stakes field. To address this, we introduce the "Haibu Mathematical-Medical Intelligent Agent" (MMIA), an LLM-driven architecture that ensures reliability through a formally verifiable reasoning process. MMIA recursively breaks down complex medical tasks into atomic, evidence-based steps. This entire reasoning chain is then automatically audited for logical coherence and evidence traceability, similar to theorem proving. A key innovation is MMIA's "bootstrapping" mode, which stores validated reasoning chains as "theorems." Subsequent tasks can then be efficiently solved using Retrieval-Augmented Generation (RAG), shifting from costly first-principles reasoning to a low-cost verification model. We validated MMIA across four healthcare administration domains, including DRG/DIP audits and medical insurance adjudication, using expert-validated benchmarks. Results showed MMIA achieved an error detection rate exceeding 98% with a false positive rate below 1%, significantly outperforming baseline LLMs. Furthermore, the RAG matching mode is projected to reduce average processing costs by approximately 85% as the knowledge base matures. In conclusion, MMIA's verifiable reasoning framework is a significant step toward creating trustworthy, transparent, and cost-effective AI systems, making LLM technology viable for critical applications in medicine. 

**Abstract (ZH)**: 大型语言模型（LLMs）在医疗领域展现出潜力，但容易出现事实和逻辑错误，这在高风险领域是不可接受的。为解决这一问题，我们提出了“哈布数学医疗智能代理”（MMIA）架构，该架构通过形式可验证的推理过程确保可靠性。MMIA递归地将复杂的医疗任务分解为基于证据的基本步骤。整个推理链随后自动审计逻辑连贯性和证据可追溯性，类似于定理证明。一项关键技术创新是MMIA的“自我增强”模式，该模式将验证的推理链存储为“定理”。后续任务可以利用检索增强生成（RAG）高效解决，从高成本的基本原理推理转变为低成本的验证模型。我们在四个医疗管理领域对MMIA进行了验证，包括DRG/DIP审计和医疗保险理赔，使用了专家验证的标准基准。结果表明，MMIA的错误检测率超过98%，假阳性率低于1%，显著优于基线LLMs。此外，随着知识库成熟，RAG匹配模式预计可将平均处理成本降低约85%。总之，MMIA可验证推理框架是朝着创建可靠、透明和低成本AI系统的重大进步，使LLM技术能够在医疗领域的关键应用中变得可行。 

---
# SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation 

**Title (ZH)**: SurveyG：具有层次引用图的多agent_llm自动化调查生成框架 

**Authors**: Minh-Anh Nguye, Minh-Duc Nguyen, Nguyen Thi Ha Lan, Kieu Hai Dang, Nguyen Tien Dong, Le Duy Dung  

**Link**: [PDF](https://arxiv.org/pdf/2510.07733)  

**Abstract**: Large language models (LLMs) are increasingly adopted for automating survey paper generation \cite{wang2406autosurvey, liang2025surveyx, yan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing approaches typically extract content from a large collection of related papers and prompt LLMs to summarize them directly. However, such methods often overlook the structural relationships among papers, resulting in generated surveys that lack a coherent taxonomy and a deeper contextual understanding of research progress. To address these shortcomings, we propose \textbf{SurveyG}, an LLM-based agent framework that integrates \textit{hierarchical citation graph}, where nodes denote research papers and edges capture both citation dependencies and semantic relatedness between their contents, thereby embedding structural and contextual knowledge into the survey generation process. The graph is organized into three layers: \textbf{Foundation}, \textbf{Development}, and \textbf{Frontier}, to capture the evolution of research from seminal works to incremental advances and emerging directions. By combining horizontal search within layers and vertical depth traversal across layers, the agent produces multi-level summaries, which are consolidated into a structured survey outline. A multi-agent validation stage then ensures consistency, coverage, and factual accuracy in generating the final survey. Experiments, including evaluations by human experts and LLM-as-a-judge, demonstrate that SurveyG outperforms state-of-the-art frameworks, producing surveys that are more comprehensive and better structured to the underlying knowledge taxonomy of a field. 

**Abstract (ZH)**: 大型语言模型（LLMs）日益用于自动化调研论文生成【 Wang et al., 2024; Liang et al., 2025; Yan et al., 2025; Su et al., 2025; Wen et al., 2025】。现有方法通常从大量相关论文集中提取内容，并促使LLMs直接对其进行总结。然而，这些方法往往忽略了论文间的结构关系，导致生成的调研论文缺乏一致的分类体系和更深层的研究进展理解。为解决这些问题，我们提出了一种基于大型语言模型的代理框架——SurveyG，该框架整合了层次引用图，其中节点表示研究论文，边捕捉两者之间的引用依赖和语义相关性，从而将结构和上下文知识嵌入到调研论文生成过程。图被组织为三个层次： Foundation、Development 和 Frontier，以捕捉从开创性工作到逐步进步和新兴方向的研究演化。通过在层次内进行水平搜索并在层次间进行垂直深度遍历，代理生成多级摘要，这些摘要最终被整合成一个结构化的调研大纲。多代理验证阶段确保生成的最终调研论文在一致性、覆盖面和事实准确性方面的高质量。实验结果，包括由人类专家和LLM担任评判者的评估，表明SurveyG超越了最先进的框架，生成的调研论文更加全面且更好地符合特定领域的知识分类体系。 

---
# oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning 

**Title (ZH)**: OMeBench：在有机机制阐明与推理中LLM稳健基准测试的研究 

**Authors**: Ruiling Xu, Yifan Zhang, Qingyun Wang, Carl Edwards, Heng Ji  

**Link**: [PDF](https://arxiv.org/pdf/2510.07731)  

**Abstract**: Organic reaction mechanisms are the stepwise elementary reactions by which reactants form intermediates and products, and are fundamental to understanding chemical reactivity and designing new molecules and reactions. Although large language models (LLMs) have shown promise in understanding chemical tasks such as synthesis design, it is unclear to what extent this reflects genuine chemical reasoning capabilities, i.e., the ability to generate valid intermediates, maintain chemical consistency, and follow logically coherent multi-step pathways. We address this by introducing oMeBench, the first large-scale, expert-curated benchmark for organic mechanism reasoning in organic chemistry. It comprises over 10,000 annotated mechanistic steps with intermediates, type labels, and difficulty ratings. Furthermore, to evaluate LLM capability more precisely and enable fine-grained scoring, we propose oMeS, a dynamic evaluation framework that combines step-level logic and chemical similarity. We analyze the performance of state-of-the-art LLMs, and our results show that although current models display promising chemical intuition, they struggle with correct and consistent multi-step reasoning. Notably, we find that using prompting strategy and fine-tuning a specialist model on our proposed dataset increases performance by 50% over the leading closed-source model. We hope that oMeBench will serve as a rigorous foundation for advancing AI systems toward genuine chemical reasoning. 

**Abstract (ZH)**: 有机反应机制是通过逐步的基本反应使反应物形成中间体和产物的过程，是理解化学反应性和设计新分子和反应的基础。虽然大型语言模型（LLMs）在理解化学任务如合成设计方面表现出潜在的能力，但仍不清楚这在多大程度上反映了真正的化学推理能力，即生成有效中间体、保持化学一致性以及遵循逻辑一致的多步路径的能力。为此，我们引入了oMeBench，这是首个大规模的专家精编的有机机制推理基准，适用于有机化学。它包含超过10,000个标注的机制步骤，其中包括中间体、类型标签和难度评级。此外，为了更精确地评估LLM的能力并提供细粒度评分，我们提出了oMeS，这是一种结合步骤级逻辑和化学相似性的动态评估框架。我们分析了当前最先进的LLM的表现，并发现虽然这些模型展示了有希望的化学直觉，但在正确且一致的多步推理方面却存在问题。值得注意的是，我们发现采用提示策略并在我们提出的数据集上微调专门模型，使性能相比领先的闭源模型提高了50%。我们希望oMeBench能够成为推动AI系统朝向真正的化学推理发展的一个严谨基础。 

---
# Control Synthesis of Cyber-Physical Systems for Real-Time Specifications through Causation-Guided Reinforcement Learning 

**Title (ZH)**: 基于因果引导 reinforcement learning 的实时规范下网络物理系统控制合成 

**Authors**: Xiaochen Tang, Zhenya Zhang, Miaomiao Zhang, Jie An  

**Link**: [PDF](https://arxiv.org/pdf/2510.07715)  

**Abstract**: In real-time and safety-critical cyber-physical systems (CPSs), control synthesis must guarantee that generated policies meet stringent timing and correctness requirements under uncertain and dynamic conditions. Signal temporal logic (STL) has emerged as a powerful formalism of expressing real-time constraints, with its semantics enabling quantitative assessment of system behavior. Meanwhile, reinforcement learning (RL) has become an important method for solving control synthesis problems in unknown environments. Recent studies incorporate STL-based reward functions into RL to automatically synthesize control policies. However, the automatically inferred rewards obtained by these methods represent the global assessment of a whole or partial path but do not accumulate the rewards of local changes accurately, so the sparse global rewards may lead to non-convergence and unstable training performances. In this paper, we propose an online reward generation method guided by the online causation monitoring of STL. Our approach continuously monitors system behavior against an STL specification at each control step, computing the quantitative distance toward satisfaction or violation and thereby producing rewards that reflect instantaneous state dynamics. Additionally, we provide a smooth approximation of the causation semantics to overcome the discontinuity of the causation semantics and make it differentiable for using deep-RL methods. We have implemented a prototype tool and evaluated it in the Gym environment on a variety of continuously controlled benchmarks. Experimental results show that our proposed STL-guided RL method with online causation semantics outperforms existing relevant STL-guided RL methods, providing a more robust and efficient reward generation framework for deep-RL. 

**Abstract (ZH)**: 基于实时信号时序逻辑的在线奖励生成方法及其在深度强化学习中的应用 

---
# Multimodal Safety Evaluation in Generative Agent Social Simulations 

**Title (ZH)**: 多模态安全性评估在生成性代理社会模拟中的应用 

**Authors**: Alhim Vera, Karen Sanchez, Carlos Hinojosa, Haidar Bin Hamid, Donghoon Kim, Bernard Ghanem  

**Link**: [PDF](https://arxiv.org/pdf/2510.07709)  

**Abstract**: Can generative agents be trusted in multimodal environments? Despite advances in large language and vision-language models that enable agents to act autonomously and pursue goals in rich settings, their ability to reason about safety, coherence, and trust across modalities remains limited. We introduce a reproducible simulation framework for evaluating agents along three dimensions: (1) safety improvement over time, including iterative plan revisions in text-visual scenarios; (2) detection of unsafe activities across multiple categories of social situations; and (3) social dynamics, measured as interaction counts and acceptance ratios of social exchanges. Agents are equipped with layered memory, dynamic planning, multimodal perception, and are instrumented with SocialMetrics, a suite of behavioral and structural metrics that quantifies plan revisions, unsafe-to-safe conversions, and information diffusion across networks. Experiments show that while agents can detect direct multimodal contradictions, they often fail to align local revisions with global safety, reaching only a 55 percent success rate in correcting unsafe plans. Across eight simulation runs with three models - Claude, GPT-4o mini, and Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75, 55, and 58 percent, respectively. Overall performance ranged from 20 percent in multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted when paired with misleading visuals, showing a strong tendency to overtrust images. These findings expose critical limitations in current architectures and provide a reproducible platform for studying multimodal safety, coherence, and social dynamics. 

**Abstract (ZH)**: 生成代理在多模态环境中能信任吗？尽管大型语言模型和视觉语言模型的进步使代理能够在富有的环境中自主行动并追求目标，它们在跨模态推理安全、连贯性和信任方面的能力仍然有限。我们引入了一个可重复的模拟框架，从三个维度评估代理：（1）随着时间的推移提高安全性，包括文本视觉场景中的迭代计划修订；（2）检测多种社会情境类别中的不安全活动；（3）社会动态，通过社交互动次数和社交交换的接受比率来衡量。代理配备了分层记忆、动态规划、多模态感知，并配备了SocialMetrics，这是一个由行为和结构度量组成的工具包，可以量化计划修订、不安全到安全的转换以及网络中的信息扩散。实验显示，虽然代理可以检测到直接的多模态矛盾，但它们经常无法将局部修订与全局安全对齐，只在纠正不安全计划方面取得55%的成功率。在涉及三个模型Claude、GPT-4o mini和Qwen-VL的八次模拟运行中，五种代理分别实现了75%、55%和58%的平均不安全到安全的转换率。总体性能从GPT-4o mini在多风险场景中的20%到Claude在局部情境如火/热中的98%变化。值得注意的是，当与误导性视觉搭配时，45%的不安全行动被接受，显示出强烈的过度信任图像的趋势。这些发现揭示了当前架构的关键局限性，并提供了一个可重复的平台来研究多模态安全、连贯性和社会动态。 

---
# Safely Exploring Novel Actions in Recommender Systems via Deployment-Efficient Policy Learning 

**Title (ZH)**: 通过部署高效的策略学习安全探索新颖行动在推荐系统中的应用 

**Authors**: Haruka Kiyohara, Yusuke Narita, Yuta Saito, Kei Tateno, Takuma Udagawa  

**Link**: [PDF](https://arxiv.org/pdf/2510.07635)  

**Abstract**: In many real recommender systems, novel items are added frequently over time. The importance of sufficiently presenting novel actions has widely been acknowledged for improving long-term user engagement. A recent work builds on Off-Policy Learning (OPL), which trains a policy from only logged data, however, the existing methods can be unsafe in the presence of novel actions. Our goal is to develop a framework to enforce exploration of novel actions with a guarantee for safety. To this end, we first develop Safe Off-Policy Policy Gradient (Safe OPG), which is a model-free safe OPL method based on a high confidence off-policy evaluation. In our first experiment, we observe that Safe OPG almost always satisfies a safety requirement, even when existing methods violate it greatly. However, the result also reveals that Safe OPG tends to be too conservative, suggesting a difficult tradeoff between guaranteeing safety and exploring novel actions. To overcome this tradeoff, we also propose a novel framework called Deployment-Efficient Policy Learning for Safe User Exploration, which leverages safety margin and gradually relaxes safety regularization during multiple (not many) deployments. Our framework thus enables exploration of novel actions while guaranteeing safe implementation of recommender systems. 

**Abstract (ZH)**: 在推荐系统中确保新型行动探索的安全框架 

---
# Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models 

**Title (ZH)**: 测试时匹配：解锁多模态模型的组合推理能力 

**Authors**: Yinglun Zhu, Jiancheng Zhang, Fuzhi Tang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07632)  

**Abstract**: Frontier AI models have achieved remarkable progress, yet recent studies suggest they struggle with compositional reasoning, often performing at or below random chance on established benchmarks. We revisit this problem and show that widely used evaluation metrics systematically underestimate model capability. To address this, we introduce a group matching score that better exploits group structure and reveals substantial hidden capability in both contrastive vision-language models (VLMs) and multimodal large language models (MLLMs). Moreover, simply overfitting to the induced group matchings at test time transfers this hidden capability into higher scores under standard evaluation metrics, closing much of the reported gap. This adjustment enables SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first result surpassing estimated human performance on Winoground.
Building on this insight, we propose Test-Time Matching (TTM), an iterative, self-improving algorithm that further bootstraps model performance without any external supervision. TTM delivers additional, non-trivial improvements: for example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a new state of the art. Importantly, TTM remains broadly effective even on benchmarks without metric-induced effects or group structures, achieving relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16 dataset variants spanning diverse setups, our experiments demonstrate that TTM consistently improves model performance and advances the frontier of compositional reasoning. 

**Abstract (ZH)**: 前沿AI模型取得了显著进展，但最近的研究表明，它们在组合推理方面存在困难，经常在现有的基准测试中表现平平或低于随机猜测。我们重新审视这一问题，并表明广为使用的评估指标系统地低估了模型的能力。为了解决这一问题，我们引入了一种组匹配评分，更好地利用了组结构，揭示了对比视觉-语言模型（VLMs）和多模态大型语言模型（MLLMs）中隐藏的大量能力。此外，仅在测试时过度拟合诱导的组匹配，这些隐藏的能力在标准评估指标下转化为更高的分数，从而弥补了大部分报告的差距。这一调整使SigLIP-B16超越了所有先前的结果和GPT-4.1，并使其成为首个在Winoground上超过估计人类表现的结果。

基于这一洞见，我们提出了测试时匹配（TTM）算法，这是一种迭代且自我改进的算法，无需任何外部监督即可提高模型性能。TTM带来了额外的重要改进：例如，TTM使SigLIP-B16在MMVP-VLM上超越了GPT-4.1，确立了新的最佳性能。重要的是，即使在没有指标诱导效果或组结构的基准测试中，TTM也广泛有效，如在具有挑战性的数据集WhatsUp上实现了相对增益高达85.7%。在涵盖多样设置的16个数据集变体中，我们的实验表明，TTM一致地提高了模型性能，并推动了组合推理的前沿。 

---
# A Case for Leveraging Generative AI to Expand and Enhance Training in the Provision of Mental Health Services 

**Title (ZH)**: 利用生成式AI扩展和增强精神健康服务培训的案例分析 

**Authors**: Hannah R. Lawrence, Shannon Wiltsey Stirman, Samuel Dorison, Taedong Yun, Megan Jones Bell  

**Link**: [PDF](https://arxiv.org/pdf/2510.07623)  

**Abstract**: Generative artificial intelligence (Generative AI) is transforming healthcare. With this evolution comes optimism regarding the impact it will have on mental health, as well as concern regarding the risks that come with generative AI operating in the mental health domain. Much of the investment in, and academic and public discourse about, AI-powered solutions for mental health has focused on therapist chatbots. Despite the common assumption that chatbots will be the most impactful application of GenAI to mental health, we make the case here for a lower-risk, high impact use case: leveraging generative AI to enhance and scale training in mental health service provision. We highlight key benefits of using generative AI to help train people to provide mental health services and present a real-world case study in which generative AI improved the training of veterans to support one another's mental health. With numerous potential applications of generative AI in mental health, we illustrate why we should invest in using generative AI to support training people in mental health service provision. 

**Abstract (ZH)**: 生成式人工智能（生成AI）正在变革医疗保健。随着这一变革的到来，人们对于生成AI在心理健康方面可能产生的影响既充满期待，也对其可能带来的风险表示担忧。关于利用人工智能解决方案解决心理健康问题的投资和学术及公共讨论，大多集中在心理治疗聊天机器人上。尽管普遍认为聊天机器人将是生成AI在心理健康领域最具影响力的應用，但我们在这里提出一种更低风险、更高影响力的用例：利用生成AI增强和扩展心理健康服务提供人员的培训。我们强调利用生成AI帮助培训提供心理健康服务人员的关键益处，并展示了一个实际案例，说明生成AI如何改善退伍军人支持彼此心理健康的服务培训。鉴于生成AI在心理健康领域的众多潜在应用，我们阐明了为什么应当投资利用生成AI支持培训心理健康服务提供人员的重要性。 

---
# Traceability and Accountability in Role-Specialized Multi-Agent LLM Pipelines 

**Title (ZH)**: 角色专业化多代理LLM管道中的可追溯性和问责制 

**Authors**: Amine Barrak  

**Link**: [PDF](https://arxiv.org/pdf/2510.07614)  

**Abstract**: Sequential multi-agent systems built with large language models (LLMs) can automate complex software tasks, but they are hard to trust because errors quietly pass from one stage to the next. We study a traceable and accountable pipeline, meaning a system with clear roles, structured handoffs, and saved records that let us trace who did what at each step and assign blame when things go wrong. Our setting is a Planner -> Executor -> Critic pipeline. We evaluate eight configurations of three state-of-the-art LLMs on three benchmarks and analyze where errors start, how they spread, and how they can be fixed. Our results show: (1) adding a structured, accountable handoff between agents markedly improves accuracy and prevents the failures common in simple pipelines; (2) models have clear role-specific strengths and risks (e.g., steady planning vs. high-variance critiquing), which we quantify with repair and harm rates; and (3) accuracy-cost-latency trade-offs are task-dependent, with heterogeneous pipelines often the most efficient. Overall, we provide a practical, data-driven method for designing, tracing, and debugging reliable, predictable, and accountable multi-agent systems. 

**Abstract (ZH)**: 使用大型语言模型（LLMs）构建的序列多智能体系统可以自动化复杂的软件任务，但它们难以信任，因为错误会悄悄地从一个阶段传递到下一个阶段。我们研究了一种可追溯和问责的流水线，即一个具有明确角色、结构化交接和保存记录的系统，允许我们追踪每一步谁做了什么，并在出现问题时分配责任。我们的设置是一个规划者->执行者->批评者流水线。我们在三个基准上评估了三种最先进的LLM的八种配置，并分析了错误的起源、传播方式以及如何修复。我们的结果显示：（1）在智能体之间添加一个结构化、问责制的交接显著提高了准确性和防止了简单流水线中常见的失败；（2）模型在特定角色上具有的优点和风险（例如，稳定的规划 vs. 高变异性批评）是明确的，我们通过修复率和危害率来量化；（3）准确率-成本-延迟权衡取决于任务，异构流水线往往是最高效的。总的来说，我们提供了一种实用的数据驱动方法，用于设计、追踪和调试可靠、可预测和问责的多智能体系统。 

---
# AgentAsk: Multi-Agent Systems Need to Ask 

**Title (ZH)**: AgentAsk: 多智能体系统需要发问 

**Authors**: Bohan Lin, Kuo Yang, Yingchuan Lai, Yudong Zhang, Chen Zhang, Guibin Zhang, Xinlei Yu, Miao Yu, Xu Wang, Yang Wang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07593)  

**Abstract**: Multi-agent systems built on large language models (LLMs) promise enhanced problem-solving capabilities through collaborative division of labor. However, they frequently underperform single-agent baselines due to edge-level error cascades: minor inaccuracies at one message handoff propagate across the entire chain. We propose AgentAsk, a lightweight and plug-and-play clarification module that treats every inter-agent message as a potential failure point and inserts minimally necessary questions to arrest error propagation. AgentAsk follows a three-stage pipeline: (i) distilling edge-level judgments from curated failure traces into a compact policy, (ii) supervising the policy to determine when/what/whom/how to ask, and (iii) optimizing online with E-GRPO, a reinforcement learning objective that balances accuracy, latency, and cost. The module is architecture-agnostic and easy to integrate into existing orchestration. Across math, reasoning, and coding benchmarks, AgentAsk consistently improves accuracy and robustness over public multi-agent implementations while keeping overhead minimal, with latency and extra cost all less than 5%, approaching the performance of a strong evaluator. Beyond empirical improvements, we contribute a principled taxonomy of edge-level errors and a practical recipe for link-local intervention, offering a scalable pathway toward more reliable LLM-based multi-agent systems. 

**Abstract (ZH)**: 基于大型语言模型（LLMs）的多智能体系统通过协作分工提高了问题解决能力，但由于边缘级错误级联往往会表现不佳。我们提出了AgentAsk，一种轻量级且即插即用的问题模块，将每一个智能体间的消息视为潜在的失败点，并插入最小必要的询问以阻止错误传播。AgentAsk遵循三阶段流水线：（i）从精选的失败示例中提炼出边缘级判断并提炼为紧凑策略，（ii）监督策略以决定何时/何事/向谁/如何询问，以及（iii）通过平衡准确率、延迟和成本的E-GRPO强化学习目标在线优化。该模块架构无关且易于集成到现有编排中。在数学、推理和编程基准测试中，AgentAsk在保持较低开销的前提下，一致性地提高了准确性和鲁棒性，延迟和额外成本均低于5%，接近强大评估器的性能。除了实证改进，我们还贡献了一种边缘级错误的原理性分类和局部干预的实用指南，为更可靠的基于LLM的多智能体系统提供了可扩展路径。 

---
# Benchmarking is Broken - Don't Let AI be its Own Judge 

**Title (ZH)**: 基准测试出了问题 - 别让AI自我评审 

**Authors**: Zerui Cheng, Stella Wohnig, Ruchika Gupta, Samiul Alam, Tassallah Abdullahi, João Alves Ribeiro, Christian Nielsen-Garcia, Saif Mir, Siran Li, Jason Orender, Seyed Ali Bahrainian, Daniel Kirste, Aaron Gokaslan, Mikołaj Glinka, Carsten Eickhoff, Ruben Wolff  

**Link**: [PDF](https://arxiv.org/pdf/2510.07575)  

**Abstract**: The meteoric rise of Artificial Intelligence (AI), with its rapidly expanding market capitalization, presents both transformative opportunities and critical challenges. Chief among these is the urgent need for a new, unified paradigm for trustworthy evaluation, as current benchmarks increasingly reveal critical vulnerabilities. Issues like data contamination and selective reporting by model developers fuel hype, while inadequate data quality control can lead to biased evaluations that, even if unintentionally, may favor specific approaches. As a flood of participants enters the AI space, this "Wild West" of assessment makes distinguishing genuine progress from exaggerated claims exceptionally difficult. Such ambiguity blurs scientific signals and erodes public confidence, much as unchecked claims would destabilize financial markets reliant on credible oversight from agencies like Moody's.
In high-stakes human examinations (e.g., SAT, GRE), substantial effort is devoted to ensuring fairness and credibility; why settle for less in evaluating AI, especially given its profound societal impact? This position paper argues that the current laissez-faire approach is unsustainable. We contend that true, sustainable AI advancement demands a paradigm shift: a unified, live, and quality-controlled benchmarking framework robust by construction, not by mere courtesy and goodwill. To this end, we dissect the systemic flaws undermining today's AI evaluation, distill the essential requirements for a new generation of assessments, and introduce PeerBench, a community-governed, proctored evaluation blueprint that embodies this paradigm through sealed execution, item banking with rolling renewal, and delayed transparency. Our goal is to pave the way for evaluations that can restore integrity and deliver genuinely trustworthy measures of AI progress. 

**Abstract (ZH)**: 人工智能的迅猛崛起及其市场资本的迅速扩张，带来了变革性的机遇和关键性的挑战。其中最为紧迫的问题是需要构建一个新的统一可信赖评估范式，因为当前的基准测试越来越多地揭示出关键的漏洞。数据污染和模型开发者的选择性报告加剧了这种炒作现象，而不尽如人意的数据质量控制可能导致带有偏见的评估，即使这一偏见是无意的，也可能偏好特定的方法。随着大量参与者涌入AI领域，“野蛮生长”的评估体系使得辨别真实进展与夸大宣传变得异常困难。这种模糊性模糊了科学信号，侵蚀了公众信心，就如同未经管控的索赔会动摇依赖于可靠监管机构（例如穆迪）的金融市场一样。

在高风险的人类考试（如SAT、GRE）中，投入了大量精力确保公平性和可信度；为什么在评估AI时变得不够呢，特别是考虑到AI对社会的巨大影响？本文认为，目前的放任自流的做法是不可持续的。我们认为，真正的可持续AI进步需要范式转变，即构建一个统一、实时、质量可控的基准测试框架，这一框架应是通过系统设计而非仅凭好意和善意来实现的坚固框架。为此，我们剖析了当前AI评估体系的系统性缺陷，提炼出新一代评估体系的必要要求，并介绍了PeerBench，这是一种由社区管理、监考的评估蓝图，通过封闭执行、题库滚动更新和延迟透明化来体现这一范式。我们的目标是为评估铺平道路，从而恢复诚信并提供真正可信的AI进步衡量标准。 

---
# An Evaluation Study of Hybrid Methods for Multilingual PII Detection 

**Title (ZH)**: 混合方法多语言PII检测评价研究 

**Authors**: Harshit Rajgarhia, Suryam Gupta, Asif Shaik, Gulipalli Praveen Kumar, Y Santhoshraj, Sanka Nithya Tanvy Nishitha, Abhishek Mukherji  

**Link**: [PDF](https://arxiv.org/pdf/2510.07551)  

**Abstract**: The detection of Personally Identifiable Information (PII) is critical for privacy compliance but remains challenging in low-resource languages due to linguistic diversity and limited annotated data. We present RECAP, a hybrid framework that combines deterministic regular expressions with context-aware large language models (LLMs) for scalable PII detection across 13 low-resource locales. RECAP's modular design supports over 300 entity types without retraining, using a three-phase refinement pipeline for disambiguation and filtering. Benchmarked with nervaluate, our system outperforms fine-tuned NER models by 82% and zero-shot LLMs by 17% in weighted F1-score. This work offers a scalable and adaptable solution for efficient PII detection in compliance-focused applications. 

**Abstract (ZH)**: 低资源语言中个人可识别信息的检测对于隐私合规至关重要但依然具有挑战性，因语言多样性及标注数据有限。我们提出RECAP，一种结合确定性正则表达式和上下文感知的大语言模型（LLM）的混合框架，用于在13种低资源语言环境中可扩展地检测个人可识别信息。RECAP的模块化设计无需重新训练即可支持超过300种实体类型，并通过三阶段细化流水线进行消歧和过滤。经nervaluate评测，我们的系统在加权F1分数上比微调的NER模型高出82%，比零样本的LLM高出17%。本研究提供了针对合规应用高效个人可识别信息检测的可扩展和适应性解决方案。 

---
# Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization 

**Title (ZH)**: 通过匿名化衡量和减轻多代理辩论中的身份偏见 

**Authors**: Hyeong Kyu Choi, Xiaojin Zhu, Yixuan Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.07517)  

**Abstract**: Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning by letting multiple agents exchange answers and then aggregate their opinions. Yet recent studies reveal that agents are not neutral: they are prone to identity-driven sycophancy and self-bias, uncritically adopting a peer's view or stubbornly adhering to their own prior output, undermining the reliability of debate. In this work, we present the first principled framework that joins sycophancy and self-bias to mitigate and quantify identity bias in MAD. First, we formalize the debate dynamics as an identity-weighted Bayesian update process. Second, we propose response anonymization: by removing identity markers from prompts, agents cannot distinguish "self" from "peer", which forces equal weights on agent identity, thereby reducing bias. Third, we define the Identity Bias Coefficient (IBC), a principled metric that measures how often an agent follows a peer versus itself. Empirical studies across multiple models, datasets and debate rounds confirm that identity bias is widespread, with sycophancy far more common than self-bias. Our findings highlight the need to "mask" identity to ensure that MAD systems reason based on content rather than source identity. Code is released in this https URL. 

**Abstract (ZH)**: 多智能体辩论中的身份偏见及其缓解：一个原则性框架 

---
# CompassLLM: A Multi-Agent Approach toward Geo-Spatial Reasoning for Popular Path Query 

**Title (ZH)**: CompassLLM：面向流行路径查询的地理空间推理多agent方法 

**Authors**: Md. Nazmul Islam Ananto, Shamit Fatin, Mohammed Eunus Ali, Md Rizwan Parvez  

**Link**: [PDF](https://arxiv.org/pdf/2510.07516)  

**Abstract**: The popular path query - identifying the most frequented routes between locations from historical trajectory data - has important applications in urban planning, navigation optimization, and travel recommendations. While traditional algorithms and machine learning approaches have achieved success in this domain, they typically require model training, parameter tuning, and retraining when accommodating data updates. As Large Language Models (LLMs) demonstrate increasing capabilities in spatial and graph-based reasoning, there is growing interest in exploring how these models can be applied to geo-spatial problems.
We introduce CompassLLM, a novel multi-agent framework that intelligently leverages the reasoning capabilities of LLMs into the geo-spatial domain to solve the popular path query. CompassLLM employs its agents in a two-stage pipeline: the SEARCH stage that identifies popular paths, and a GENERATE stage that synthesizes novel paths in the absence of an existing one in the historical trajectory data. Experiments on real and synthetic datasets show that CompassLLM demonstrates superior accuracy in SEARCH and competitive performance in GENERATE while being cost-effective. 

**Abstract (ZH)**: 流行的路径查询——从历史轨迹数据中识别最频繁的路线——在城市规划、导航优化和旅行推荐等领域具有重要应用。尽管传统的算法和机器学习方法在过去取得了成功，但它们通常需要模型训练、参数调整，并且在适应数据更新时需要重新训练。随着大型语言模型（LLMs）在空间和图推理方面的能力不断增强，人们越来越感兴趣于探索如何将这些模型应用于地理空间问题。

我们提出了CompassLLM，这是一种新颖的多智能体框架，智能地将LLMs的推理能力引入地理空间领域以解决流行的路径查询问题。CompassLLM通过其智能体在两个阶段的流程中进行工作：SEARCH阶段识别流行路径，GENERATE阶段在历史轨迹数据中不存在现有路径时合成新的路径。实验证实在真实和合成数据集上，CompassLLM在SEARCH阶段表现出色，生成阶段具有竞争力，并且成本效益高。 

---
# Optimizing Ethical Risk Reduction for Medical Intelligent Systems with Constraint Programming 

**Title (ZH)**: 基于约束编程优化医疗智能系统伦理风险减少 

**Authors**: Clotilde Brayé, Aurélien Bricout, Arnaud Gotlieb, Nadjib Lazaar, Quentin Vallet  

**Link**: [PDF](https://arxiv.org/pdf/2510.07491)  

**Abstract**: Medical Intelligent Systems (MIS) are increasingly integrated into healthcare workflows, offering significant benefits but also raising critical safety and ethical concerns. According to the European Union AI Act, most MIS will be classified as high-risk systems, requiring a formal risk management process to ensure compliance with the ethical requirements of trust- worthy AI. In this context, we focus on risk reduction optimization problems, which aim to reduce risks with ethical considerations by finding the best balanced assignment of risk assessment values according to their coverage of trustworthy AI ethical requirements. We formalize this problem as a constrained optimization task and investigate three resolution paradigms: Mixed Integer Programming (MIP), Satisfiability (SAT), and Constraint Pro- gramming(CP).Our contributions include the mathematical formulation of this optimization problem, its modeling with the Minizinc constraint modeling language, and a comparative experimental study that analyzes the performance, expressiveness, and scalability of each ap- proach to solving. From the identified limits of the methodology, we draw some perspectives of this work regarding the integration of the Minizinc model into a complete trustworthy AI ethical risk management process for MIS. 

**Abstract (ZH)**: 医疗智能系统（MIS）在医疗保健工作流程中的应用日益增多，虽然带来了显著的好处，但也引发了重要的安全和伦理问题。根据欧盟人工智能法案，大多数MIS将被归类为高风险系统，需要通过正式的风险管理过程来确保符合可信人工智能的伦理要求。在这种背景下，我们关注的是伦理考量下的风险减少优化问题，即通过找到最佳平衡的风险评估值分配方案，以覆盖可信人工智能的伦理要求来降低风险。我们将这个问题形式化为一个约束优化任务，并研究了三种解决范式：混合整数规划（MIP）、满足性（SAT）和约束编程（CP）。我们的贡献包括对该优化问题的数学表述、使用Minizinc约束建模语言进行建模以及对每种方法求解性能、表示能力和可扩展性的比较实验研究。通过对方法论的识别限制，我们对该工作在MIS中集成Minizinc模型以实现完整的可信人工智能伦理风险管理过程提出了几点展望。 

---
# Evaluation of LLMs for Process Model Analysis and Optimization 

**Title (ZH)**: LLM在过程模型分析与优化中的评估 

**Authors**: Akhil Kumar, Jianliang Leon Zhao, Om Dobariya  

**Link**: [PDF](https://arxiv.org/pdf/2510.07489)  

**Abstract**: In this paper, we report our experience with several LLMs for their ability to understand a process model in an interactive, conversational style, find syntactical and logical errors in it, and reason with it in depth through a natural language (NL) interface. Our findings show that a vanilla, untrained LLM like ChatGPT (model o3) in a zero-shot setting is effective in understanding BPMN process models from images and answering queries about them intelligently at syntactic, logic, and semantic levels of depth. Further, different LLMs vary in performance in terms of their accuracy and effectiveness. Nevertheless, our empirical analysis shows that LLMs can play a valuable role as assistants for business process designers and users. We also study the LLM's "thought process" and ability to perform deeper reasoning in the context of process analysis and optimization. We find that the LLMs seem to exhibit anthropomorphic properties. 

**Abstract (ZH)**: 本文报告了我们使用几种LLM对其以交互式对话方式理解过程模型、发现其中的语法和逻辑错误并通过自然语言接口进行深入推理的能力的经验。我们的研究发现，在零样本设置下，如ChatGPT (模型o3)这样的未经训练的LLM能够有效理解BPMN过程模型并以语法、逻辑和语义层面智能地回答相关查询。此外，不同LLM在准确性和有效性方面表现不同。然而，我们的实证分析表明，LLM可以在业务流程设计师和用户的支持角色中发挥重要作用。我们还研究了LLM的“推理过程”和其在过程分析和优化上下文中进行更深层次推理的能力。我们发现，LLM似乎表现出拟人化特征。 

---
# ExpertAgent: Enhancing Personalized Education through Dynamic Planning and Retrieval-Augmented Long-Chain Reasoning 

**Title (ZH)**: ExpertAgent: 通过动态规划和检索增强长链推理提升个性化教育 

**Authors**: Binrong Zhu, Guiran Liu, Nina Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07456)  

**Abstract**: The application of advanced generative artificial intelligence in education is often constrained by the lack of real-time adaptability, personalization, and reliability of the content. To address these challenges, we propose ExpertAgent - an intelligent agent framework designed for personalized education that provides reliable knowledge and enables highly adaptive learning experiences. Therefore, we developed ExpertAgent, an innovative learning agent that provides users with a proactive and personalized learning experience. ExpertAgent dynamic planning of the learning content and strategy based on a continuously updated student model. Therefore, overcoming the limitations of traditional static learning content to provide optimized teaching strategies and learning experience in real time. All instructional content is grounded in a validated curriculum repository, effectively reducing hallucination risks in large language models and improving reliability and trustworthiness. 

**Abstract (ZH)**: 高级生成人工智能在教育中的应用往往受限于内容的实时适应性、个性化和可靠性不足。为应对这些挑战，我们提出了一种名为ExpertAgent的智能代理框架，该框架旨在提供个性化的教育服务，并提供可靠的知识，从而实现高度适应性的学习体验。因此，我们开发了ExpertAgent这一创新的学习代理，为用户提供主动且个性化的学习体验。ExpertAgent根据不断更新的学生模型动态规划学习内容和策略，克服了传统静态学习内容的局限性，提供实时优化的教学策略和学习体验。所有教学内容均基于经过验证的课程资源库，有效降低了大型语言模型的幻觉风险，提高了可靠性和可信度。 

---
# TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering 

**Title (ZH)**: TS-Agent：一种迭代统计洞察搜集的时间序列推理代理 

**Authors**: Penghang Liu, Elizabeth Fons, Svitlana Vyetrenko, Daniel Borrajo, Vamsi Potluru, Manuela Veloso  

**Link**: [PDF](https://arxiv.org/pdf/2510.07432)  

**Abstract**: Large language models (LLMs) have shown strong abilities in reasoning and problem solving, but recent studies reveal that they still struggle with time series reasoning tasks, where outputs are often affected by hallucination or knowledge leakage. In this work we propose TS-Agent, a time series reasoning agent that leverages LLMs strictly for what they excel at, i.e., gathering evidence and synthesizing it into conclusions through step-by-step reasoning, while delegating the extraction of statistical and structural information to time series analytical tools. Instead of mapping time series into text tokens, images, or embeddings, our agent interacts with raw numeric sequences through atomic operators, records outputs in an explicit evidence log, and iteratively refines its reasoning under the guidance of a self-critic and a final quality gate. This design avoids multi-modal alignment training, preserves the native form of time series, ensures interpretability and verifiability, and mitigates knowledge leakage or hallucination. Empirically, we evaluate the agent on established benchmarks. Our experiments show that TS-Agent achieves performance comparable to state-of-the-art LLMs on understanding benchmarks, and delivers significant improvements on reasoning tasks, where existing models often rely on memorization and fail in zero-shot settings. 

**Abstract (ZH)**: TS-Agent：一种专用于时间序列推理的代理模型 

---
# Less is More: Strategic Expert Selection Outperforms Ensemble Complexity in Traffic Forecasting 

**Title (ZH)**: fewer is More: 策略性专家选择在交通预测中优于ensemble复杂性 

**Authors**: Walid Guettala, Yufan Zhao, László Gulyás  

**Link**: [PDF](https://arxiv.org/pdf/2510.07426)  

**Abstract**: Traffic forecasting is fundamental to intelligent transportation systems, enabling congestion mitigation and emission reduction in increasingly complex urban environments. While recent graph neural network approaches have advanced spatial temporal modeling, existing mixture of experts frameworks like Time Enhanced Spatio Temporal Attention Model (TESTAM) lack explicit incorporation of physical road network topology, limiting their spatial capabilities. We present TESTAM+, an enhanced spatio temporal forecasting framework that introduces a novel SpatioSemantic Expert integrating physical road topology with data driven feature similarity through hybrid graph construction. TESTAM+ achieves significant improvements over TESTAM: 1.3% MAE reduction on METR LA (3.10 vs. 3.14) and 4.1% improvement on PEMS BAY (1.65 vs. 1.72). Through comprehensive ablation studies, we discover that strategic expert selection fundamentally outperforms naive ensemble aggregation. Individual experts demonstrate remarkable effectiveness: the Adaptive Expert achieves 1.63 MAE on PEMS BAY, outperforming the original three expert TESTAM (1.72 MAE), while the SpatioSemantic Expert matches this performance with identical 1.63 MAE. The optimal Identity + Adaptive configuration achieves an 11.5% MAE reduction compared to state of the art MegaCRN on METR LA (2.99 vs. 3.38), while reducing inference latency by 53.1% compared to the full four expert TESTAM+. Our findings reveal that fewer, strategically designed experts outperform complex multi expert ensembles, establishing new state of the art performance with superior computational efficiency for real time deployment. 

**Abstract (ZH)**: 基于时空语义专家的城市交通流预测增强框架：Physical Road Topology Integration for Improved Spatial Capabilities 

---
# ProSEA: Problem Solving via Exploration Agents 

**Title (ZH)**: ProSEA: 通过探索代理解决问题 

**Authors**: William Nguyen, Vinh Luong, Christopher Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2510.07423)  

**Abstract**: Large language models (LLMs) have empowered AI agents to tackle increasingly complex tasks. However, most existing agents remain limited to static planning and brittle interactions, falling short of true collaboration or adaptive reasoning. We introduce ProSEA, a modular, general-purpose multi-agent framework designed for iterative problem solving through exploration and plan evolution. ProSEA features a hierarchical architecture in which a Manager Agent orchestrates domain-specialized Expert Agents, decomposes tasks, and adaptively replans based on structured feedback from failed attempts. Unlike prior systems, ProSEA agents report not only success or failure but also detailed reasons for failure and newly discovered constraints, enabling dynamic plan refinement informed by exploratory traces. The framework operates autonomously but supports seamless integration with human collaborators when needed. Experiments on the challenging FinanceBench benchmark demonstrate that ProSEA, even without human feedback, outperforms state-of-the-art baselines and achieves robust performance across reasoning-heavy tasks. These results underscore ProSEA's potential as a foundation for more transparent, adaptive, and human-aligned AI agents. 

**Abstract (ZH)**: 大型语言模型（LLMs）赋能AI代理应对日益复杂的任务。然而，大多数现有代理仍然局限于静态规划和脆弱的交互，难以实现真正的协作或适应性推理。我们介绍了ProSEA，一种模块化、通用的多代理框架，旨在通过探索和计划演化来进行迭代问题求解。ProSEA具有层次化架构，其中管理代理协调领域专业化专家代理，分解任务，并根据失败尝试的结构化反馈适应性重新规划。与之前的系统不同，ProSEA代理不仅报告成功或失败，还报告失败的详细原因和新发现的约束，从而通过探索性痕迹动态改进计划。该框架自主运行，但在需要时支持无缝与人类合作者集成。在具有挑战性的FinanceBench基准测试上的实验显示，即使没有人类反馈，ProSEA也优于最先进的基线方法，并且在推理密集型任务上实现了稳健性能。这些结果突显了ProSEA作为更加透明、适应性和人类对齐的AI代理基础的潜力。 

---
# Position: AI Will Transform Neuropsychology Through Mental Health Digital Twins for Dynamic Mental Health Care, Especially for ADHD 

**Title (ZH)**: 位置：AI将通过心理健康数字孪生实现动态心理健康护理，尤其适用于注意力缺陷多动障碍（ADHD） 

**Authors**: Neil Natarajan, Sruthi Viswanathan, Xavier Roberts-Gaal, Michelle Marie Martel  

**Link**: [PDF](https://arxiv.org/pdf/2510.07409)  

**Abstract**: Static solutions don't serve a dynamic mind. Thus, we advocate a shift from static mental health diagnostic assessments to continuous, artificial intelligence (AI)-driven assessment. Focusing on Attention-Deficit/Hyperactivity Disorder (ADHD) as a case study, we explore how generative AI has the potential to address current capacity constraints in neuropsychology, potentially enabling more personalized and longitudinal care pathways. In particular, AI can efficiently conduct frequent, low-level experience sampling from patients and facilitate diagnostic reconciliation across care pathways. We envision a future where mental health care benefits from continuous, rich, and patient-centered data sampling to dynamically adapt to individual patient needs and evolving conditions, thereby improving both accessibility and efficacy of treatment. We further propose the use of mental health digital twins (MHDTs) - continuously updated computational models that capture individual symptom dynamics and trajectories - as a transformative framework for personalized mental health care. We ground this framework in empirical evidence and map out the research agenda required to refine and operationalize it. 

**Abstract (ZH)**: 静态解决方案不适合动态思维。因此，我们建议从静态心理健康诊断评估转向连续的、基于人工智能（AI）的评估。以注意缺陷多动障碍（ADHD）为例，我们探讨了生成式AI如何有可能解决神经心理学领域的当前容量限制， potentially 推动更为个性化和纵向的护理路径。特别是，AI可以高效地从患者那里进行频繁的、低层次的经验抽样，并促进不同护理路径中的诊断一致。我们设想一个未来，在这个未来中，心理健康护理可以从连续的、丰富的和以患者为中心的数据抽样中受益，以动态适应个别患者的需求及其不断变化的状况，从而提高治疗的可及性和有效性。我们进一步提出使用心理健康数字双胞胎（MHDTs）——不断更新的计算模型，捕捉个体症状动态和轨迹——作为实现个性化心理健康护理的变革性框架。我们基于实证证据来构建这一框架，并指出为了细化和实施这一框架所需的研究议程。 

---
# Base Models Know How to Reason, Thinking Models Learn When 

**Title (ZH)**: 基模型知道如何推理，思考模型学会何时推理 

**Authors**: Constantin Venhoff, Iván Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda  

**Link**: [PDF](https://arxiv.org/pdf/2510.07364)  

**Abstract**: Why do thinking language models like DeepSeek R1 outperform their base counterparts? Despite consistent performance gains, it remains unclear to what extent thinking models learn entirely new reasoning capabilities or repurpose pre-existing base model ones. In this work, we propose a hybrid model where we activate reasoning mechanisms in base models at the right time to elicit thinking-model-level reasoning chains, implying that thinking models exploit already existing capabilities. To ground our analysis, we introduce an unsupervised, bottom-up approach for uncovering human-interpretable reasoning behaviors in thinking models. This approach provides an unbiased method to discover reasoning behaviors without imposing manual or LLM-derived assumptions. Across three base and four thinking models, using GSM8K and MATH500, our hybrid model recovers up to 91% of the performance gap to thinking models without any weight updates while steering only 12% of tokens. Concretely, our empirical setup provides a simple, causal way to test the effectiveness of existing reasoning mechanisms in base models by invoking them directly and measuring the resulting task performance. More broadly, these results reframe our understanding of how thinking models are trained: pre-training is when models acquire most of their reasoning mechanisms, and post-training teaches efficient deployment of these mechanisms at the right time, enabling efficient use of their inference-time compute. 

**Abstract (ZH)**: 为什么DeepSeek R1等思考型语言模型优于其基础模型？尽管取得了持续的性能提升，但仍不清楚思考模型是学习全新的推理能力，还是重用了基础模型已有的能力。在本研究中，我们提出了一个混合模型，通过在适当的时间激活基础模型中的推理机制来引发思考模型级别的推理链，这表明思考模型利用了已存在的能力。为了支持我们的分析，我们引入了一种无监督的、自底向上的方法，用于揭示思考模型中的可解释推理行为。该方法提供了一种无偏的方法，可以在不预设手动或LLM启发式假设的情况下发现推理行为。在三个基础模型和四个思考模型中，使用GSM8K和MATH500，我们的混合模型在不进行任何权重更新的情况下，回收了高达91%的性能差距，只引导了12%的令牌。具体而言，我们的实证设置提供了一种简单且因果的方法，通过直接调用这些机制并测量结果任务性能来测试基础模型中现有推理机制的有效性。更广泛地说，这些结果重新定义了我们对思考模型训练方式的理解：预训练是模型获得大多数推理机制的时期，而后续训练教会这些机制在适当的时间进行高效部署，从而有效利用推理时间的计算资源。 

---
# L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint) 

**Title (ZH)**: L2M-AID：结合大规模语言模型语义推理与多Agent强化学习的自主物理-网络防御（预印本） 

**Authors**: Tianxiang Xu, Zhichao Wen, Xinyu Zhao, Jun Wang, Yan Li, Chang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07363)  

**Abstract**: The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness. This paper introduces L2M-AID, a novel framework for Autonomous Industrial Defense using LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team of collaborative agents, each driven by a Large Language Model (LLM), to achieve adaptive and resilient security. The core innovation lies in the deep fusion of two AI paradigms: we leverage an LLM as a semantic bridge to translate vast, unstructured telemetry into a rich, contextual state representation, enabling agents to reason about adversary intent rather than merely matching patterns. This semantically-aware state empowers a Multi-Agent Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative strategies. The MARL reward function is uniquely engineered to balance security objectives (threat neutralization) with operational imperatives, explicitly penalizing actions that disrupt physical process stability. To validate our approach, we conduct extensive experiments on the benchmark SWaT dataset and a novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework. Results demonstrate that L2M-AID significantly outperforms traditional IDS, deep learning anomaly detectors, and single-agent RL baselines across key metrics, achieving a 97.2% detection rate while reducing false positives by over 80% and improving response times by a factor of four. Crucially, it demonstrates superior performance in maintaining physical process stability, presenting a robust new paradigm for securing critical national infrastructure. 

**Abstract (ZH)**: 使用大语言模型赋能的多代理强化学习的自主工业防御框架L2M-AID 

---
# Truth-Aware Decoding: A Program-Logic Approach to Factual Language Generation 

**Title (ZH)**: 真相意识解码：基于程序逻辑的事实语言生成方法 

**Authors**: Faruk Alpay, Hamdi Alakkad  

**Link**: [PDF](https://arxiv.org/pdf/2510.07331)  

**Abstract**: This paper introduces Truth-Aware Decoding (TAD), a verification-oriented decoding scheme that aligns neural language generation with knowledge bases. Situated in the tradition of probabilistic program semantics for sequence models, TAD augments modern instruction-tuned systems with a lattice of semantic guards that operate at decode time. Our contributions are fourfold: (i) a constraint-based semantics that renders oracle filtering as a program-logic judgment, (ii) a proof that greedy selection enjoys local likelihood dominance under sound and complete guards (Theorem 2.7), (iii) an entropy-style invariant that quantifies factual risk via knowledge-aware safe mass, and (iv) a multi-agent operational calculus with verified Lean artefacts to certify implementation behaviour. Numerical and algorithmic case studies confirm that the resulting guardrails reduce hallucinations without sacrificing throughput, yielding a pragmatic bridge between large-scale empirical models and formal verification. 

**Abstract (ZH)**: 这篇论文介绍了基于真实性的解码（TAD），这是一种验证导向的解码方案，将神经语言生成与知识库对齐。TAD 位于概率程序语义对序列模型的传统之上，在现代指令调优系统中加入了在解码时操作的语义门控层次结构。我们的贡献包括：(i) 基于约束的语义学，将或然过滤视为程序逻辑判断，(ii) 证明在正确和完备的门控条件下，贪婪选择具有局部似然性主导权（定理2.7），(iii) 一种熵风格的不变量，通过知识感知的安全质量量化事实风险，以及(iv) 验证过的Lean artefacts多代理操作微积分以认证实现行为。数值和算法案例研究证实，由此产生的门控结构减少了幻觉现象，同时不牺牲吞吐量，从而为大规模经验模型与形式验证之间架起了一座务实的桥梁。 

---
# BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation 

**Title (ZH)**: BLAZER: 基于零样本数据生成 Bootstrapping LLM-based 演化智能体 

**Authors**: Rocktim Jyoti Das, Harsh Singh, Diana Turmakhan, Muhammad Abdullah Sohail, Mingfei Han, Preslav Nakov, Fabio Pizzati, Ivan Laptev  

**Link**: [PDF](https://arxiv.org/pdf/2510.08572)  

**Abstract**: Scaling data and models has played a pivotal role in the remarkable progress of computer vision and language. Inspired by these domains, recent efforts in robotics have similarly focused on scaling both data and model size to develop more generalizable and robust policies. However, unlike vision and language, robotics lacks access to internet-scale demonstrations across diverse robotic tasks and environments. As a result, the scale of existing datasets typically suffers from the need for manual data collection and curation. To address this problem, here we propose BLAZER, a framework that learns manipulation policies from automatically generated training data. We build on the zero-shot capabilities of LLM planners and automatically generate demonstrations for diverse manipulation tasks in simulation. Successful examples are then used to finetune an LLM and to improve its planning capabilities without human supervision. Notably, while BLAZER training requires access to the simulator's state, we demonstrate direct transfer of acquired skills to sensor-based manipulation. Through extensive experiments, we show BLAZER to significantly improve zero-shot manipulation in both simulated and real environments. Moreover, BLAZER improves on tasks outside of its training pool and enables downscaling of LLM models. Our code and data will be made publicly available on the project page. 

**Abstract (ZH)**: scaling 数据和模型在计算机视觉和语言领域的卓越进展中发挥了关键作用。受这些领域的影响，近年来关于机器人领域的研究同样集中在扩大数据和模型规模，以开发更通用和稳健的策略。然而，与视觉和语言不同的是，机器人缺乏互联网规模的跨多种机器人任务和环境的演示数据。因此，现有数据集的规模通常受到手动数据收集和整理的限制。为了解决这个问题，我们提出了BLAZER框架，该框架从自动生成的训练数据中学习操作策略。我们利用大语言模型（LLM）规划器的零样本能力，在模拟环境中自动生成各种操作任务的演示。成功的示例随后用于微调LLM并改进其规划能力，而无需人类监督。值得注意的是，尽管BLAZER训练需要访问模拟器状态，但我们展示了其从传感器驱动的操作中直接转移所获得技能的能力。通过广泛的实验，我们展示了BLAZER在模拟和实际环境中的零样本操作显著改进。此外，BLAZER在训练库之外的任务上也有所改进，并使LLM模型的缩放成为可能。我们的代码和数据将在项目页面上公开提供。 

---
# ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation 

**Title (ZH)**: ArenaBencher：多模型竞争性评价驱动的自动基准演进 

**Authors**: Qin Liu, Jacob Dineen, Yuxi Huang, Sheng Zhang, Hoifung Poon, Ben Zhou, Muhao Chen  

**Link**: [PDF](https://arxiv.org/pdf/2510.08569)  

**Abstract**: Benchmarks are central to measuring the capabilities of large language models and guiding model development, yet widespread data leakage from pretraining corpora undermines their validity. Models can match memorized content rather than demonstrate true generalization, which inflates scores, distorts cross-model comparisons, and misrepresents progress. We introduce ArenaBencher, a model-agnostic framework for automatic benchmark evolution that updates test cases while preserving comparability. Given an existing benchmark and a diverse pool of models to be evaluated, ArenaBencher infers the core ability of each test case, generates candidate question-answer pairs that preserve the original objective, verifies correctness and intent with an LLM as a judge, and aggregates feedback from multiple models to select candidates that expose shared weaknesses. The process runs iteratively with in-context demonstrations that steer generation toward more challenging and diagnostic cases. We apply ArenaBencher to math problem solving, commonsense reasoning, and safety domains and show that it produces verified, diverse, and fair updates that uncover new failure modes, increase difficulty while preserving test objective alignment, and improve model separability. The framework provides a scalable path to continuously evolve benchmarks in step with the rapid progress of foundation models. 

**Abstract (ZH)**: ArenaBencher：一种模型无关的自动基准演进框架 

---
# NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos 

**Title (ZH)**: NovaFlow: 零样本操纵via生成视频中的可操作流 

**Authors**: Hongyu Li, Lingfeng Sun, Yafei Hu, Duy Ta, Jennifer Barry, George Konidaris, Jiahui Fu  

**Link**: [PDF](https://arxiv.org/pdf/2510.08568)  

**Abstract**: Enabling robots to execute novel manipulation tasks zero-shot is a central goal in robotics. Most existing methods assume in-distribution tasks or rely on fine-tuning with embodiment-matched data, limiting transfer across platforms. We present NovaFlow, an autonomous manipulation framework that converts a task description into an actionable plan for a target robot without any demonstrations. Given a task description, NovaFlow synthesizes a video using a video generation model and distills it into 3D actionable object flow using off-the-shelf perception modules. From the object flow, it computes relative poses for rigid objects and realizes them as robot actions via grasp proposals and trajectory optimization. For deformable objects, this flow serves as a tracking objective for model-based planning with a particle-based dynamics model. By decoupling task understanding from low-level control, NovaFlow naturally transfers across embodiments. We validate on rigid, articulated, and deformable object manipulation tasks using a table-top Franka arm and a Spot quadrupedal mobile robot, and achieve effective zero-shot execution without demonstrations or embodiment-specific training. Project website: this https URL. 

**Abstract (ZH)**: 使机器人执行新颖操作任务的零样本执行是机器人技术中的一个核心目标。我们提出了NovaFlow，这是一种自主操作框架，能够根据任务描述为目标机器人生成可执行计划，无需任何演示。给定任务描述，NovaFlow使用视频生成模型合成视频，并通过现成的感知模块将其提炼为三维可操作对象流。从对象流中，它计算刚体对象的相对姿态并通过抓取提议和轨迹优化将它们转化为机器人动作。对于变形物体，此流作为基于模型的规划中的跟踪目标，采用基于粒子的动力学模型。通过将任务理解与低级控制解耦，NovaFlow自然地实现了跨平台的转移。我们在使用桌面Franka手臂和Spot四足移动机器人进行刚体、 articulated 和变形物体操作任务的验证中，实现了有效的零样本执行，无需演示或针对特定平台的训练。项目网站: this https URL。 

---
# MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning 

**Title (ZH)**: 矩阵：多模态代理调优以实现稳健的工具使用推理 

**Authors**: Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan  

**Link**: [PDF](https://arxiv.org/pdf/2510.08567)  

**Abstract**: Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at this https URL. 

**Abstract (ZH)**: 基于视觉的代理调优框架：自动合成多模态轨迹并训练用于工具使用推理的VLM控制器 

---
# SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models 

**Title (ZH)**: SciVideoBench: 在大型多模态模型中评估科学视频推理 

**Authors**: Andong Deng, Taojiannan Yang, Shoubin Yu, Lincoln Spencer, Mohit Bansal, Chen Chen, Serena Yeung-Levy, Xiaohan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08559)  

**Abstract**: Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains a significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception/recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by a semi-automatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models' higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science. 

**Abstract (ZH)**: 大型多模态模型（LMMs）在多种能力上取得了显著进展；然而，科学领域的复杂视频推理仍然是一个重要的挑战性前沿。当前的视频基准主要针对一般场景，侧重感知/识别，而对于相对简单的推理任务存在饱和现象，从而未能有效评估高级多模态认知技能。为弥补这一关键缺口，我们引入了SciVideoBench，这是一个专门设计的严谨基准，用于评估科学背景下高级视频推理能力。SciVideoBench 包含1000个精心设计的多项选择题，这些问题源自涵盖25个专业学术领域的前沿科学实验视频，并通过半自动系统验证。每个问题都需要精深的领域特定知识、精确的空间-时间感知和复杂的逻辑推理，有效地挑战模型的高层次认知能力。我们的评估揭示了最先进的专有和开源LMMs，包括Gemini 2.5 Pro和Qwen2.5-VL，在视频推理方面的显著性能缺陷，表明在视频推理能力方面存在巨大的改进空间。对关键因素如推理复杂性和视觉语义关联的深入分析提供了宝贵见解，并为LMMs的未来开发指明了明确方向，促进了真正有能力的多模态AI科学助手的进化。希望SciVideoBench能够满足社区的兴趣，并推动尖端AI在边学科领域的边界。 

---
# Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation 

**Title (ZH)**: 梦境唤忆：想象导向的经验检索在记忆持久的视觉-语言导航中的应用 

**Authors**: Yunzhe Xu, Yiyuan Pan, Zhe Liu  

**Link**: [PDF](https://arxiv.org/pdf/2510.08553)  

**Abstract**: Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at this https URL. 

**Abstract (ZH)**: 基于视觉-语言导航的记忆增强想象检索框架 

---
# VideoNorms: Benchmarking Cultural Awareness of Video Language Models 

**Title (ZH)**: VideoNorms: 评估视频语言模型文化意识的基准 

**Authors**: Nikhil Reddy Varimalla, Yunfei Xu, Arkadiy Saakyan, Meng Fan Wang, Smaranda Muresan  

**Link**: [PDF](https://arxiv.org/pdf/2510.08543)  

**Abstract**: As Video Large Language Models (VideoLLMs) are deployed globally, they require understanding of and grounding in the relevant cultural background. To properly assess these models' cultural awareness, adequate benchmarks are needed. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm) pairs from US and Chinese cultures annotated with socio-cultural norms grounded in speech act theory, norm adherence and violations labels, and verbal and non-verbal evidence. To build VideoNorms, we use a human-AI collaboration framework, where a teacher model using theoretically-grounded prompting provides candidate annotations and a set of trained human experts validate and correct the annotations. We benchmark a variety of open-weight VideoLLMs on the new dataset which highlight several common trends: 1) models performs worse on norm violation than adherence; 2) models perform worse w.r.t Chinese culture compared to the US culture; 3) models have more difficulty in providing non-verbal evidence compared to verbal for the norm adhere/violation label and struggle to identify the exact norm corresponding to a speech-act; and 4) unlike humans, models perform worse in formal, non-humorous contexts. Our findings emphasize the need for culturally-grounded video language model training - a gap our benchmark and framework begin to address. 

**Abstract (ZH)**: 随着视频大型语言模型（VideoLLMs）的全球部署，它们需要理解和扎根于相关的文化背景。为了充分评估这些模型的文化意识，需要足够的基准。我们介绍了VideoNorms，这是一个包含来自美式和中式文化的1000多个（视频片段，规范）对的基准，这些对被标注有基于言语行为理论的社会文化规范、行为准则及其违反情况标签，以及口头和非口头证据。为了构建VideoNorms，我们采用了人机协作框架，其中教师模型使用理论指导的提示提供候选标注，一组训练有素的人类专家验证并修正这些标注。我们使用新数据集对多种开源VideoLLMs进行了基准测试，突显了几个常见趋势：1）模型在规范违反方面的表现不如行为规范方面；2）相对于美式文化，模型对中式文化的处理效果较差；3）模型在提供非口头证据方面比口头证据更困难，且在识别与言语行为对应的规范方面存在问题；4）与人类不同，模型在正式、非幽默的语境中表现较差。我们的发现强调了需要基于文化的视频语言模型训练——这是我们的基准和框架所开始填补的一个缺口。 

---
# On the optimization dynamics of RLVR: Gradient gap and step size thresholds 

**Title (ZH)**: RLVR的优化动力学：梯度差距与步长阈值 

**Authors**: Joe Suk, Yaqi Duan  

**Link**: [PDF](https://arxiv.org/pdf/2510.08539)  

**Abstract**: Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple binary feedback to post-train large language models, has shown significant empirical success. However, a principled understanding of why it works has been lacking. This paper builds a theoretical foundation for RLVR by analyzing its training process at both the full-response (trajectory) and token levels. Central to our analysis is a quantity called the Gradient Gap, which formalizes the direction of improvement from low-reward to high-reward regions of the response space. We prove that convergence critically depends on aligning the update direction with this Gradient Gap. Moreover, we derive a sharp step-size threshold based on the magnitude of the Gradient Gap: below it, learning converges, whereas above it, performance collapses. Our theory further predicts how the critical step size must scale with response length and the success rate, thereby explaining why practical heuristics such as length normalization improve stability and showing that, with a fixed learning rate, the success rate can stagnate strictly below $100\%$. We validate these predictions through controlled bandit simulations and LLM experiments, including training Qwen2.5-7B with GRPO. 

**Abstract (ZH)**: 可验证奖励的强化学习（RLVR）使用简单的二元反馈对大型语言模型进行后训练，已显示出显著的实证成功。然而，其工作原理的原理性理解还缺乏。本文通过在响应的完整路径和token层次上分析其训练过程，为RLVR建立了理论基础。我们分析的核心是称为梯度差距的量，它形式化了从低奖励区域到高奖励区域改进的方向。我们证明，收敛高度依赖于更新方向与梯度差距的对齐。此外，我们根据梯度差距的大小推导出了一个尖锐的步长阈值：低于该阈值时，学习收敛，而高于该阈值时，性能崩溃。我们的理论进一步预测了关键步长必须如何随响应长度和成功率进行缩放，从而解释了为什么像长度规范化这样的实用启发式方法能提高稳定性，并表明在固定学习率的情况下，成功率可以严格低于100%。我们通过受控的多臂_bandit_模拟和LLM实验验证了这些预测，包括使用GRPO对Qwen2.5-7B进行训练。 

---
# Kontinuous Kontext: Continuous Strength Control for Instruction-based Image Editing 

**Title (ZH)**: 连续.context：基于指令的图像编辑中的连续强度控制 

**Authors**: Rishubh Parihar, Or Patashnik, Daniil Ostashev, R. Venkatesh Babu, Daniel Cohen-Or, Kuan-Chieh Wang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08532)  

**Abstract**: Instruction-based image editing offers a powerful and intuitive way to manipulate images through natural language. Yet, relying solely on text instructions limits fine-grained control over the extent of edits. We introduce Kontinuous Kontext, an instruction-driven editing model that provides a new dimension of control over edit strength, enabling users to adjust edits gradually from no change to a fully realized result in a smooth and continuous manner. Kontinuous Kontext extends a state-of-the-art image editing model to accept an additional input, a scalar edit strength which is then paired with the edit instruction, enabling explicit control over the extent of the edit. To inject this scalar information, we train a lightweight projector network that maps the input scalar and the edit instruction to coefficients in the model's modulation space. For training our model, we synthesize a diverse dataset of image-edit-instruction-strength quadruplets using existing generative models, followed by a filtering stage to ensure quality and consistency. Kontinuous Kontext provides a unified approach for fine-grained control over edit strength for instruction driven editing from subtle to strong across diverse operations such as stylization, attribute, material, background, and shape changes, without requiring attribute-specific training. 

**Abstract (ZH)**: 基于指令的图像编辑提供了一种通过自然语言操控图像的强大而直观的方式。然而，仅依赖文本指令限制了对编辑程度的精细控制。我们引入了Kontinuous Kontext，一种以指令驱动的编辑模型，提供了控制编辑强度的新维度，使用户能够从无更改到完全实现结果以平滑连续的方式逐步调整编辑。Kontinuous Kontext 将最先进的图像编辑模型扩展为接受额外输入——一个标量编辑强度，然后将该标量编辑强度与编辑指令配对，从而可以对编辑的程度进行显式控制。为了注入这种标量信息，我们训练了一个轻量级的投影网络，该网络将输入标量和编辑指令映射到模型调制空间中的系数。在训练我们的模型时，我们使用现有的生成模型合成了图像编辑指令强度四元组的多样化数据集，并通过过滤阶段确保质量和一致性。Kontinuous Kontext 提供了一种统一的方法，可以在细微到强烈的范围内对指令驱动编辑的各种操作（如风格化、属性、材质、背景和形状变化）的编辑强度进行精细控制，而无需特定属性的训练。 

---
# SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models 

**Title (ZH)**: SpatialLadder：逐步训练在视觉-语言模型中进行空间推理 

**Authors**: Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08531)  

**Abstract**: Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence. 

**Abstract (ZH)**: 空间推理仍然是视觉语言模型（VLMs）的一个基本挑战，尽管近期取得了进展，当前的方法仍难以实现稳健的表现。我们发现这一局限性源于一个关键的差距：现有方法试图直接学习空间推理，而没有建立感知和理解的层次基础。为了解决这一挑战，我们提出了一种全面的方法来逐步构建空间智能。我们介绍了SpatialLadder-26k，一个包含26,610个样本的多模态数据集，涵盖了对象定位、单图、多视图和视频空间推理任务，通过标准化的工作流程确保在各个模态上系统地涵盖。基于此数据集，我们设计了一种三阶段逐步训练框架，包括（1）通过对象定位建立空间感知，（2）通过多维度空间任务发展空间理解，以及（3）通过可验证奖励的强化学习加强复杂推理。这种方法产生了SpatialLadder，一个拥有300M参数的模型，在空间推理基准测试中达到了最佳性能，其平均性能比基础模型提高了23.4%，比GPT-4o提高了20.8%，比Gemini-2.0-Flash提高了10.1%。值得注意的是，SpatialLadder在域外基准测试中也保持了良好的泛化能力，性能提高了7.2%，表明从感知到推理的逐步训练对于稳健的空间智能至关重要。 

---
# CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards 

**Title (ZH)**: CoMAS: 通过交互奖励实现多智能体系统的协同演化 

**Authors**: Xiangyuan Xue, Yifan Zhou, Guibin Zhang, Zaibin Zhang, Yijiang Li, Chen Zhang, Zhenfei Yin, Philip Torr, Wanli Ouyang, Lei Bai  

**Link**: [PDF](https://arxiv.org/pdf/2510.08529)  

**Abstract**: Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent's policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents. 

**Abstract (ZH)**: 一种新型的多智能体系统框架：基于交互的自我进化机制在大语言模型代理中的应用 

---
# To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models 

**Title (ZH)**: 是沉还是浮：大型视觉语言模型中的视觉信息路径 

**Authors**: Jiayun Luo, Wan-Cyuan Fan, Lyuyang Wang, Xiangteng He, Tanzila Rahman, Purang Abolmaesumi, Leonid Sigal  

**Link**: [PDF](https://arxiv.org/pdf/2510.08510)  

**Abstract**: Large Vision Language Models (LVLMs) have recently emerged as powerful architectures capable of understanding and reasoning over both visual and textual information. These models typically rely on two key components: a Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual content into a sequence of image tokens and serves as the perceptual front-end -- the eyes of the model. In contrast, the LLM interprets these tokens to perform high-level reasoning, generates responses, and functions as the cognitive core -- the brain of the model. However, it remains unclear which visual tokens contribute most significantly to understanding and reasoning, and how effectively these signals are propagated from ViT to the LLM. While most existing works have focused on identifying attention sinks, low-semantic tokens receiving disproportionately high attention, within the LLM, we shift the focus to the vision encoder by identifying a class of high-norm visual tokens from ViT, referred to as ViT attention sinks -- a problem that has been rarely studied but is indeed very important for LVLMs. Our findings show that these ViT sinks encapsulate high-level semantic concepts from images, allowing the LLM to perform more effective understanding and reasoning. Despite their importance, these sink tokens are often overlooked in existing LVLM architectures. To explore their contribution, we present both qualitative and quantitative analyses of the information embedded in these sink tokens. We also propose both training-free and training-based approaches to better leverage how this information is interpreted by the LLM, and to what extent. By explicitly utilizing these tokens, we demonstrate substantial improvements across a range of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT attention sinks in enhancing visual reasoning. 

**Abstract (ZH)**: 大型视觉语言模型（LVLMs）近期已成为能够理解并推理视觉和文本信息的强大架构。这些模型通常依赖于两个关键组件：视觉变换器（ViT）和大型语言模型（LLM）。ViT将视觉内容编码为图像令牌序列，并作为感知前端——模型的眼睛。相反，LLM解释这些令牌进行高层次推理、生成响应，并作为认知核心——模型的大脑。然而，尚不清楚哪些视觉令牌对理解和推理贡献最大，以及这些信号如何有效地从ViT传播到LLM。虽然大多数现有工作集中在识别LLM内部的注意力洼地（低语义令牌但接受不相称高的注意力），但我们更关注视觉编码器，通过从ViT中识别高范数视觉令牌类，称为ViT注意力洼地——这个问题虽鲜有研究但对LVLMs来说确实非常重要。我们的研究结果表明，这些ViT洼地蕴含了图像中的高层次语义概念，使LLM能够更有效地理解和推理。尽管它们非常重要，但在现有的LVLM架构中这些洼地令牌往往被忽视。为探索它们的贡献，我们提供了关于这些洼地令牌嵌入信息的定性和定量分析。我们还提出了无需训练和基于训练的方法，以更好地利用这些信息如何由LLM解释以及解释的程度。通过明确利用这些令牌，我们在多种LVLM和视觉推理任务中展示了显著的性能提升，突显了ViT注意力洼地在增强视觉推理方面未开发的潜力。 

---
# AI-Driven Radiology Report Generation for Traumatic Brain Injuries 

**Title (ZH)**: AI驱动的放射学报告生成在创伤性脑损伤中的应用 

**Authors**: Riadh Bouslimi, Houda Trabelsi, Wahiba Ben Abdssalem Karaa, Hana Hedhli  

**Link**: [PDF](https://arxiv.org/pdf/2510.08498)  

**Abstract**: Traumatic brain injuries present significant diagnostic challenges in emergency medicine, where the timely interpretation of medical images is crucial for patient outcomes. In this paper, we propose a novel AI-based approach for automatic radiology report generation tailored to cranial trauma cases. Our model integrates an AC-BiFPN with a Transformer architecture to capture and process complex medical imaging data such as CT and MRI scans. The AC-BiFPN extracts multi-scale features, enabling the detection of intricate anomalies like intracranial hemorrhages, while the Transformer generates coherent, contextually relevant diagnostic reports by modeling long-range dependencies. We evaluate the performance of our model on the RSNA Intracranial Hemorrhage Detection dataset, where it outperforms traditional CNN-based models in both diagnostic accuracy and report generation. This solution not only supports radiologists in high-pressure environments but also provides a powerful educational tool for trainee physicians, offering real-time feedback and enhancing their learning experience. Our findings demonstrate the potential of combining advanced feature extraction with transformer-based text generation to improve clinical decision-making in the diagnosis of traumatic brain injuries. 

**Abstract (ZH)**: 创伤性脑损伤在急诊医学中给诊断带来了重大挑战，及时解读医学影像对于患者预后至关重要。本文提出了一种针对颅脑创伤病例的新型基于AI的自动放射学报告生成方法。该模型结合了AC-BiFPN与Transformer架构，以捕获和处理如CT和MRI扫描等复杂的医学影像数据。AC-BiFPN提取多尺度特征，能够检测复杂的异常，如颅内出血，而Transformer则通过建模长距离依赖关系生成连贯且与上下文相关的诊断报告。我们在RSNA颅内出血检测数据集上评估了该模型的性能，结果显示其在诊断准确性和报告生成方面均优于传统的基于CNN的模型。该解决方案不仅在高压环境中支持放射科医生，还为实习生提供了一个强大的教学工具，提供实时反馈，增强他们的学习体验。我们的研究结果表明，将高级特征提取与基于Transformer的文本生成结合，有望改善创伤性脑损伤诊断中的临床决策。 

---
# DeepPrune: Parallel Scaling without Inter-trace Redundancy 

**Title (ZH)**: DeepPrune: 并行扩展无跨跟踪冗余 

**Authors**: Shangqing Tu, Yaxuan Li, Yushi Bai, Lei Hou, Juanzi Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.08483)  

**Abstract**: Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy -- our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: this https URL 

**Abstract (ZH)**: 并行剪枝：一种通过动态剪枝实现高效并行推理的新框架 

---
# Platform-Agnostic Modular Architecture for Quantum Benchmarking 

**Title (ZH)**: 基于平台的量子基准测试模块化架构 

**Authors**: Neer Patel, Anish Giri, Hrushikesh Pramod Patil, Noah Siekierski, Avimita Chatterjee, Sonika Johri, Timothy Proctor, Thomas Lubinski, Siyuan Niu  

**Link**: [PDF](https://arxiv.org/pdf/2510.08469)  

**Abstract**: We present a platform-agnostic modular architecture that addresses the increasingly fragmented landscape of quantum computing benchmarking by decoupling problem generation, circuit execution, and results analysis into independent, interoperable components. Supporting over 20 benchmark variants ranging from simple algorithmic tests like Bernstein-Vazirani to complex Hamiltonian simulation with observable calculations, the system integrates with multiple circuit generation APIs (Qiskit, CUDA-Q, Cirq) and enables diverse workflows. We validate the architecture through successful integration with Sandia's $\textit{pyGSTi}$ for advanced circuit analysis and CUDA-Q for multi-GPU HPC simulations. Extensibility of the system is demonstrated by implementing dynamic circuit variants of existing benchmarks and a new quantum reinforcement learning benchmark, which become readily available across multiple execution and analysis modes. Our primary contribution is identifying and formalizing modular interfaces that enable interoperability between incompatible benchmarking frameworks, demonstrating that standardized interfaces reduce ecosystem fragmentation while preserving optimization flexibility. This architecture has been developed as a key enhancement to the continually evolving QED-C Application-Oriented Performance Benchmarks for Quantum Computing suite. 

**Abstract (ZH)**: 我们提出了一种平台无关的模块化架构，通过将问题生成、电路执行和结果分析解耦为独立且可互操作的组件，应对量子计算基准测试日益碎片化的景观。该系统支持超过20种基准测试变体，从简单的伯恩斯坦-瓦zigani算法测试到复杂的哈密尔顿量模拟及可观测量计算。该系统与多个电路生成API（Qiskit、CUDA-Q、Cirq）兼容，并支持多种工作流。通过成功将该架构与桑迪亚国家实验室的pyGSTi集成进行高级电路分析，以及与CUDA-Q集成进行多GPU高性能计算模拟，我们验证了该架构的有效性。系统的可扩展性通过实现现有基准的动态电路变体和一个新的量子强化学习基准来展示，这些基准可以在多种执行和分析模式下立即使用。我们的主要贡献在于识别并形式化了模块化接口，这些接口能够使不兼容的基准测试框架相互兼容，证明了标准化接口可以减少生态系统碎片化，同时保持优化的灵活性。该架构是不断演化的量子计算套件QED-C应用程序导向性能基准的关键增强之一。 

---
# Integral Signatures of Activation Functions: A 9-Dimensional Taxonomy and Stability Theory for Deep Learning 

**Title (ZH)**: 激活函数的积分签名：深度学习中9维分类学与稳定性理论 

**Authors**: Ankur Mali, Lawrence Hall, Jake Williams, Gordon Richards  

**Link**: [PDF](https://arxiv.org/pdf/2510.08456)  

**Abstract**: Activation functions govern the expressivity and stability of neural networks, yet existing comparisons remain largely heuristic. We propose a rigorous framework for their classification via a nine-dimensional integral signature S_sigma(phi), combining Gaussian propagation statistics (m1, g1, g2, m2, eta), asymptotic slopes (alpha_plus, alpha_minus), and regularity measures (TV(phi'), C(phi)). This taxonomy establishes well-posedness, affine reparameterization laws with bias, and closure under bounded slope variation. Dynamical analysis yields Lyapunov theorems with explicit descent constants and identifies variance stability regions through (m2', g2). From a kernel perspective, we derive dimension-free Hessian bounds and connect smoothness to bounded variation of phi'. Applying the framework, we classify eight standard activations (ReLU, leaky-ReLU, tanh, sigmoid, Swish, GELU, Mish, TeLU), proving sharp distinctions between saturating, linear-growth, and smooth families. Numerical Gauss-Hermite and Monte Carlo validation confirms theoretical predictions. Our framework provides principled design guidance, moving activation choice from trial-and-error to provable stability and kernel conditioning. 

**Abstract (ZH)**: 激活函数 Governs 神经网络的表达能力和稳定性，现有比较大多still保留直觉性。我们提出了一种通过九维积分签名S_sigma(φ)的严谨分类框架，结合高斯传播统计量（m1, g1, g2, m2, η），渐近斜率（α_plus, α_minus）和正则性度量（TV(φ'), C(φ)）。这种分类体系确立了良定义性、带有偏置的仿射重参数化法则以及在有界斜率变化下的封闭性。动力学分析得到了显式下降常数的Lyapunov定理，并通过（m2', g2)确定了方差稳定性区域。从核的角度来看，我们推导出了维数无关的Hessian边界，并将平滑度与φ'的有界变分联系起来。应用此框架，我们分类了八种标准激活函数（ReLU, 泄漏ReLU, 双曲正切, Sigmoid, Swish, GELU, Mish, TeLU），证明了饱和、线性增长和平滑家族之间的严格区别。数值的Gauss-Hermite和蒙特卡洛验证确认了理论预测。此框架提供了原则性的设计指导，使激活函数的选择从试错转变为可证明的稳定性和核条件。 

---
# gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity 

**Title (ZH)**: gLSTM：通过增加存储容量减轻过度压缩 

**Authors**: Hugh Blayney, Álvaro Arroyo, Xiaowen Dong, Michael M. Bronstein  

**Link**: [PDF](https://arxiv.org/pdf/2510.08450)  

**Abstract**: Graph Neural Networks (GNNs) leverage the graph structure to transmit information between nodes, typically through the message-passing mechanism. While these models have found a wide variety of applications, they are known to suffer from over-squashing, where information from a large receptive field of node representations is collapsed into a single fixed sized vector, resulting in an information bottleneck. In this paper, we re-examine the over-squashing phenomenon through the lens of model storage and retrieval capacity, which we define as the amount of information that can be stored in a node's representation for later use. We study some of the limitations of existing tasks used to measure over-squashing and introduce a new synthetic task to demonstrate that an information bottleneck can saturate this capacity. Furthermore, we adapt ideas from the sequence modeling literature on associative memories, fast weight programmers, and the xLSTM model to develop a novel GNN architecture with improved capacity. We demonstrate strong performance of this architecture both on our capacity synthetic task, as well as a range of real-world graph benchmarks. 

**Abstract (ZH)**: 图神经网络（GNNs）通过图结构在节点之间传递信息，通常通过消息传递机制实现。尽管这些模型在广泛的应用中找到了用途，但它们会遭受过压缩的现象，即节点表示的大 receptive field 中的信息被压缩到一个固定大小的向量中，形成信息瓶颈。在本文中，我们从模型存储和检索能力的角度重新审视过压缩现象，我们将存储和检索能力定义为可以在节点表示中储存并稍后使用的信息量。我们研究了现有用于衡量过压缩的一些限制，并引入了一个新的合成任务来证明信息瓶颈可以饱和这种能力。此外，我们借鉴序列建模领域中关联记忆、快速权重编程和xLSTM模型的想法，开发了一种新的GNN架构，具有改进的能力。我们在这项能力合成任务以及一系列实际图基准上都展示了该架构的出色性能。 

---
# Synthetic Series-Symbol Data Generation for Time Series Foundation Models 

**Title (ZH)**: 合成序列符号数据生成用于时间序列基础模型 

**Authors**: Wenxuan Wang, Kai Wu, Yujian Betterest Li, Dan Wang, Xiaoyu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08445)  

**Abstract**: Foundation models for time series analysis (TSA) have attracted significant attention. However, challenges such as training data scarcity and imbalance continue to hinder their development. Inspired by complex dynamic system theories, we design a series-symbol data generation mechanism, enabling the unrestricted creation of high-quality time series data paired with corresponding symbolic expressions. To leverage series-symbol data pairs with strong correlations, we develop \texttt{SymTime}, a pre-trained foundation model for enhancing time series representation using symbolic information. \texttt{SymTime} demonstrates competitive performance across five major TSA tasks when fine-tunes with downstream tasks, rivaling foundation models pre-trained on real-world datasets. This approach underscores the potential of series-symbol data generation and pretraining mechanisms in overcoming data scarcity and enhancing task performance. The code is available at this https URL. 

**Abstract (ZH)**: 基于时间序列分析的础模型（TSA）吸引了显著的关注。然而，训练数据稀缺性和不平衡性等挑战仍阻碍其发展。受复杂动力系统理论启发，我们设计了一种序列-符号数据生成机制，能够不受限制地生成高质量的时间序列数据及其对应的符号表达。为了利用强相关性的时间序列-符号数据配对，我们开发了\texttt{SymTime}，这是一种预训练基础模型，用于通过符号信息增强时间序列表示。在下游任务微调后，\texttt{SymTime}在五个主要的TSA任务中展示了竞争力，与基于真实数据集预训练的基础模型不相上下。该方法强调了序列-符号数据生成和预训练机制在克服数据稀缺性和提升任务性能方面的潜力。代码见此链接：this https URL。 

---
# Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning 

**Title (ZH)**: gaze on the 奖项: 通过回报引导对比学习塑造视觉注意力 

**Authors**: Andrew Lee, Ian Chuang, Dechen Gao, Kai Fukazawa, Iman Soltani  

**Link**: [PDF](https://arxiv.org/pdf/2510.08442)  

**Abstract**: Visual Reinforcement Learning (RL) agents must learn to act based on high-dimensional image data where only a small fraction of the pixels is task-relevant. This forces agents to waste exploration and computational resources on irrelevant features, leading to sample-inefficient and unstable learning. To address this, inspired by human visual foveation, we introduce Gaze on the Prize. This framework augments visual RL with a learnable foveal attention mechanism (Gaze), guided by a self-supervised signal derived from the agent's experience pursuing higher returns (the Prize). Our key insight is that return differences reveal what matters most: If two similar representations produce different outcomes, their distinguishing features are likely task-relevant, and the gaze should focus on them accordingly. This is realized through return-guided contrastive learning that trains the attention to distinguish between the features relevant to success and failure. We group similar visual representations into positives and negatives based on their return differences and use the resulting labels to construct contrastive triplets. These triplets provide the training signal that teaches the attention mechanism to produce distinguishable representations for states associated with different outcomes. Our method achieves up to 2.4x improvement in sample efficiency and can solve tasks that the baseline fails to learn, demonstrated across a suite of manipulation tasks from the ManiSkill3 benchmark, all without modifying the underlying algorithm or hyperparameters. 

**Abstract (ZH)**: 基于视觉的强化学习（RL）代理必须基于高维图像数据学习行动，而只有少量像素与任务相关。这迫使代理浪费探索和计算资源在无关特征上，导致学习样本效率低下且不稳定。为解决这一问题，受人类视觉中心化启发，我们引入了“追求奖励的目光”框架。该框架通过自监督信号（由代理追求更高回报的经验引导），在视觉RL中增加了一个可学习的中心化注意力机制（目光）。我们的核心见解是，回报差异揭示了最重要内容：如果两个相似的表示产生不同的结果，则它们的区别特征很可能是任务相关的，目光应相应地聚焦于这些特征。通过基于回报引导的对比学习，训练注意力机制区分与成功和失败相关的特征。我们根据回报差异将相似的视觉表示分组为正样本和负样本，并使用由此产生的标签构建对比三元组。这些三元组作为训练信号，教导注意力机制为与不同结果相关的状态生成可区分的表示。我们的方法在样本效率上提高了多达2.4倍，并且可以在基线无法学习的任务中解决问题，在ManiSkill3基准的一系列操作任务中均未修改底层算法或超参数。 

---
# xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning 

**Title (ZH)**: xRouter：通过强化学习训练成本意识的大语言模型 orchestration 系统 

**Authors**: Cheng Qian, Zuxin Liu, Shirley Kokane, Akshara Prabhakar, Jielin Qiu, Haolin Chen, Zhiwei Liu, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08439)  

**Abstract**: Modern LLM deployments confront a widening cost-performance spectrum: premium models deliver strong reasoning but are expensive, while lightweight models are economical yet brittle on complex tasks. Static escalation rules and keyword heuristics under-utilize this spectrum and fail to adapt across task types. We present xRouter, a tool-calling-based routing system in which a learned router can either answer directly or invoke one or more external models. The router is trained end-to-end with reinforcement learning using an explicit, cost-aware reward that encodes cost-performance trade-offs, eliminating the need for hand-engineered routing rules. Our implementation encompasses the full reinforcement learning framework, including reward and cost accounting, as well as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter achieves strong cost-performance trade-offs (e.g., substantial cost reductions at comparable task completion rates), and provides empirical insights into what reliably helps learned routing and what does not, ranging from model trainability to the difficulty of eliciting sophisticated orchestration behaviors in small open models. We hope these findings and our open implementation will serve as a practical substrate for advancing learned, cost-aware LLM orchestration. 

**Abstract (ZH)**: 现代大模型部署面临成本与性能之间的 widening 谱系：高端模型推理能力强但代价高昂，而轻量级模型经济实惠但在复杂任务中却脆弱不堪。静态升级规则和关键词启发式方法未能充分利用这一谱系，并且无法适应不同任务类型。我们提出了 xRouter，一种基于工具调用的路由系统，其中学习到的路由器可以直接作答或调用一个或多个外部模型。路由器通过强化学习端到端训练，使用明确的成本意识奖励来编码成本与性能权衡，从而消除人工设计的路由规则的需要。我们的实现涵盖了完整的强化学习框架，包括奖励和成本计算，以及部署和评估管道。在多种基准测试中，xRouter 实现了强大的成本与性能权衡（例如，在相同任务完成率下大幅降低成本），并提供了关于什么有助于学习路由和什么无助于学习的实证见解，从模型可训练性到小开放模型引发复杂编排行为的难度。我们希望这些发现和我们的开源实现将为推进学习型成本感知大模型编排提供实用的基础。 

---
# ClauseLens: Clause-Grounded, CVaR-Constrained Reinforcement Learning for Trustworthy Reinsurance Pricing 

**Title (ZH)**: ClauseLens: 基于条款、CVaR约束的可信再保险定价 reinforcement learning方法 

**Authors**: Stella C. Dong, James R. Finlay  

**Link**: [PDF](https://arxiv.org/pdf/2510.08429)  

**Abstract**: Reinsurance treaty pricing must satisfy stringent regulatory standards, yet current quoting practices remain opaque and difficult to audit. We introduce ClauseLens, a clause-grounded reinforcement learning framework that produces transparent, regulation-compliant, and risk-aware treaty quotes.
ClauseLens models the quoting task as a Risk-Aware Constrained Markov Decision Process (RA-CMDP). Statutory and policy clauses are retrieved from legal and underwriting corpora, embedded into the agent's observations, and used both to constrain feasible actions and to generate clause-grounded natural language justifications.
Evaluated in a multi-agent treaty simulator calibrated to industry data, ClauseLens reduces solvency violations by 51%, improves tail-risk performance by 27.9% (CVaR_0.10), and achieves 88.2% accuracy in clause-grounded explanations with retrieval precision of 87.4% and recall of 91.1%.
These findings demonstrate that embedding legal context into both decision and explanation pathways yields interpretable, auditable, and regulation-aligned quoting behavior consistent with Solvency II, NAIC RBC, and the EU AI Act. 

**Abstract (ZH)**: 再保险条约定价必须满足严格的监管标准，但当前的报价实践依然不透明且难以审计。我们介绍了ClauseLens，这是一种基于条款的强化学习框架，生成透明、合规且风控的条约报价。 

---
# Prompts Generalize with Low Data: Non-vacuous Generalization Bounds for Optimizing Prompts with More Informative Priors 

**Title (ZH)**: 低数据量下提示通用化：具有更多信息先验的优化提示的非空泛泛化界 

**Authors**: David Madras, Joshua Safyan, Qiuyi, Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08413)  

**Abstract**: Many prompt engineering techniques have been successful in practice, even when optimizing over a large prompt space with with a small amount of task-specific data. Recent work has partially explained this success by showing generalization bounds which apply PAC-Bayes theory to the discrete prompt space, but they are non-vacuous only in data-rich scenarios. We argue that such widespread success can be more fully explained through more carefully considering data- or distribution-dependent perplexity, which acts as an effective prior and steers the optimization towards prompts that are more ``natural'' for the task at hand. We derive novel generalization bounds that are non-vacuous for data-scarce prompt optimization via more useful priors, formally analyzing how perplexity regularization tightens these bounds by limiting exploration. Empirically, we explore both the bounds' effectiveness and the practical benefits of perplexity regularization in improving prompt generalization. 

**Abstract (ZH)**: 许多提示工程技巧在实践中的表现都很成功，即使在仅使用少量任务特定数据的情况下优化一个大的提示空间也是如此。近期的研究部分地通过将PAC-Bayes理论应用于离散提示空间来解释这种成功，并且只有在数据丰富的情况下这些解释才非空泛。我们认为，这种广泛的成功可以通过更仔细地考虑数据或分布相关的困惑度来更全面地解释，困惑度作为有效的先验能够引导优化向着更适合任务的“自然”提示。我们推导出新的泛化界，通过更有用的先验在数据稀缺的提示优化中保持非空泛，并形式化分析困惑度正则化如何通过限制探索来收紧这些界限。从实验上，我们研究了这些界的有效性以及困惑度正则化在提高提示泛化方面的实际益处。 

---
# Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT 

**Title (ZH)**: 单层 Tiny Co$^4$ 超越 GPT-2 和 GPT-BERT 

**Authors**: Noor Ul Zain, Mohsin Raza, Ahsan Adeel  

**Link**: [PDF](https://arxiv.org/pdf/2510.08404)  

**Abstract**: We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$ is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2 (124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude greater training efficiency on 10M tokens, demonstrating highly sample efficient pretraining. Using the BabyLM challenge evaluation pipeline across complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out of 7 metrics in both cases. These results suggest the need to rethink prevailing deep learning paradigms and associated scaling laws. 

**Abstract (ZH)**: 一个小巧的Co$^4$机器（Adeel,2025）：单层、双头、8M参数，在约$O(N)$成本下，两轮训练超越BabyLM挑战基准GPT-2（124M参数，12层，$O(N^2)$）和GPT-BERT（30M参数，12层，$O(N^2)$），展现极高的训练效率和样本高效预训练，并在复杂基准测试中表现出强大的零-shot和微调性能。 

---
# FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts 

**Title (ZH)**: FlyLoRA: 通过隐式秩别化混合-of-专家提升任务解耦和参数效率 

**Authors**: Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji  

**Link**: [PDF](https://arxiv.org/pdf/2510.08396)  

**Abstract**: Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains -- general knowledge understanding, scientific question answering, mathematical reasoning, and code generation -- demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at this https URL. 

**Abstract (ZH)**: FlyLoRA: 一种基于隐式Mixture-of-Experts的低秩适应方法 

---
# Detecting Legend Items on Historical Maps Using GPT-4o with In-Context Learning 

**Title (ZH)**: 使用GPT-4o和上下文学习检测历史地图中的图例项 

**Authors**: Sofia Kirsanova, Yao-Yi Chiang, Weiwei Duan  

**Link**: [PDF](https://arxiv.org/pdf/2510.08385)  

**Abstract**: Historical map legends are critical for interpreting cartographic symbols. However, their inconsistent layouts and unstructured formats make automatic extraction challenging. Prior work focuses primarily on segmentation or general optical character recognition (OCR), with few methods effectively matching legend symbols to their corresponding descriptions in a structured manner. We present a method that combines LayoutLMv3 for layout detection with GPT-4o using in-context learning to detect and link legend items and their descriptions via bounding box predictions. Our experiments show that GPT-4 with structured JSON prompts outperforms the baseline, achieving 88% F-1 and 85% IoU, and reveal how prompt design, example counts, and layout alignment affect performance. This approach supports scalable, layout-aware legend parsing and improves the indexing and searchability of historical maps across various visual styles. 

**Abstract (ZH)**: 历史地图图例对于解析制图符号至关重要。然而，它们不一致的布局和无结构的格式使得自动提取变得具有挑战性。先前的工作主要集中在分段或一般的光学字符识别（OCR）上，很少有方法能够有效地将图例符号与其对应的描述以结构化的方式匹配。我们提出了一种方法，该方法结合使用LayoutLMv3进行布局检测与GPT-4o进行上下文学习，通过边界框预测检测并链接图例项及其描述。我们的实验表明，GPT-4在结构化JSON提示下的表现优于基线，达到88%的F-1和85%的IoU，并揭示了提示设计、示例数量和布局对齐如何影响性能。该方法支持可扩展、布局感知的图例解析，并提高了各种视觉风格的历史地图的索引和可搜索性。 

---
# Airy: Reading Robot Intent through Height and Sky 

**Title (ZH)**: Airy：通过高度和天空读取机器人意图 

**Authors**: Baoyang Chen, Xian Xu, Huamin Qu  

**Link**: [PDF](https://arxiv.org/pdf/2510.08381)  

**Abstract**: As industrial robots move into shared human spaces, their opaque decision making threatens safety, trust, and public oversight. This artwork, Airy, asks whether complex multi agent AI can become intuitively understandable by staging a competition between two reinforcement trained robot arms that snap a bedsheet skyward. Building on three design principles, competition as a clear metric (who lifts higher), embodied familiarity (audiences recognize fabric snapping), and sensor to sense mapping (robot cooperation or rivalry shown through forest and weather projections), the installation gives viewers a visceral way to read machine intent. Observations from five international exhibitions indicate that audiences consistently read the robots' strategies, conflict, and cooperation in real time, with emotional reactions that mirror the system's internal state. The project shows how sensory metaphors can turn a black box into a public interface. 

**Abstract (ZH)**: 随着工业机器人进入共享的人类空间，它们不透明的决策威胁到安全、信任和公众监督。这件作品《Airy》探讨复杂多智能体AI是否可以通过展示两支经过强化训练的机器人手臂争夺顶起床单的竞赛，变得直观易懂。基于三条设计原则——竞赛作为清晰的度量标准（谁提升得更高）、身体上的熟悉感（观众识别出布料的弹动）、以及传感器到感知的映射（通过森林和天气投影展示机器人间的合作或竞争），该装置为观众提供了一种直观的方式来解读机器意图。来自五个国际展览的观察表明，观众能够实时读取机器人的策略、冲突和合作，并且情绪反应反映出系统的内部状态。该项目展示了感官隐喻如何将一个黑箱转化为公众接口。 

---
# Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception 

**Title (ZH)**: 评价小规模 vision-language 模型在距离依赖性交通感知中的表现 

**Authors**: Nikos Theodoridis, Tim Brophy, Reenu Mohandas, Ganesh Sistu, Fiachra Collins, Anthony Scanlan, Ciaran Eising  

**Link**: [PDF](https://arxiv.org/pdf/2510.08352)  

**Abstract**: Vision-Language Models (VLMs) are becoming increasingly powerful, demonstrating strong performance on a variety of tasks that require both visual and textual understanding. Their strong generalisation abilities make them a promising component for automated driving systems, which must handle unexpected corner cases. However, to be trusted in such safety-critical applications, a model must first possess a reliable perception system. Moreover, since critical objects and agents in traffic scenes are often at a distance, we require systems that are not "shortsighted", i.e., systems with strong perception capabilities at both close (up to 20 meters) and long (30+ meters) range. With this in mind, we introduce Distance-Annotated Traffic Perception Question Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused solely on perception-based questions in traffic scenes, enriched with distance annotations. By excluding questions that require reasoning, we ensure that model performance reflects perception capabilities alone. Since automated driving hardware has limited processing power and cannot support large VLMs, our study centers on smaller VLMs. More specifically, we evaluate several state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the simplicity of the questions, these models significantly underperform compared to humans (~60% average accuracy for the best-performing small VLM versus ~85% human performance). However, it is important to note that the human sample size was relatively small, which imposes statistical limitations. We also identify specific perception tasks, such as distinguishing left from right, that remain particularly challenging for these models. 

**Abstract (ZH)**: Distance-Annotated Traffic Perception Question Answering 

---
# DeepEN: Personalized Enteral Nutrition for Critically Ill Patients using Deep Reinforcement Learning 

**Title (ZH)**: DeepEN: 采用深度强化学习的重症患者个性化肠内营养供给方法 

**Authors**: Daniel Jason Tan, Jiayang Chen, Dilruk Perera, Kay Choong See, Mengling Feng  

**Link**: [PDF](https://arxiv.org/pdf/2510.08350)  

**Abstract**: We introduce DeepEN, a deep reinforcement learning (RL) framework for personalized enteral nutrition (EN) in critically ill patients. Trained offline on over 11,000 ICU patients from the MIMIC-IV database, DeepEN generates 4-hourly recommendations for caloric, protein, and fluid intake tailored to each patient's evolving physiology. The model integrates a curated, clinically informed state space with a custom reward function that balances short-term physiological and nutrition-related goals with long-term survival outcomes. Using a dueling double deep Q-network with conservative Q-learning regularization, DeepEN learns clinically realistic policies that align with high-value clinician actions while discouraging unsafe deviations. Across various qualitative and quantitative metrics, DeepEN outperforms clinician-derived and guideline-based policies, achieving a 3.7 $\pm$ 0.17 percentage-point reduction in estimated mortality (18.8% vs 22.5%) and improvements in key nutritional biomarkers. These findings highlight the potential of safe, data-driven personalization of EN therapy to improve outcomes beyond traditional guideline- or heuristic-based approaches. 

**Abstract (ZH)**: DeepEN：一种用于重症患者个性化肠内营养的深度强化学习框架 

---
# Learning What's Missing: Attention Dispersion and EMA Stabilization in Length Generalization 

**Title (ZH)**: 学习所缺失的：注意力分散与长度泛化中的EMA稳定化 

**Authors**: Pál Zsámboki, Benjamin Levi, David Ansel Josef Smith, Mitansh Kagalwala, Arlington Kell, Samuel Liechty, Cong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08341)  

**Abstract**: We study length generalization in transformers through the set complement task, where a model must predict a uniform distribution over tokens absent from an input sequence -- an ability central to board-game style reasoning. Our main theoretical result establishes two statements. First, we prove tight bounds on embedding and value dimensions for single-layer attention-only transformers. Second, we show that if such a model achieves balanced logit displacement at lengths 1 and 2, then it must generalize to longer sequences, though with reduced precision. A mechanistic reading of the proof explains this limitation: as more tokens are attended to, softmax compresses logit displacements, eroding separation between valid and invalid outputs. Training dynamics also suggest a second obstacle: when many next tokens are possible, updates become noisy. We hypothesize that dropout can counteract the first effect and Exponential Moving Average (EMA) the second. We validate these hypotheses through random hyperparameter search on the set complement task, which confirms both mechanisms. We then test OthelloGPT, a GPT-1 style model trained on random Othello moves, and find that EMA again improves length generalization in this more complex setting. 

**Abstract (ZH)**: 我们通过集合补任务研究transformers的长度泛化能力，其中模型必须预测输入序列中缺失的标记的均匀分布——这是类似棋盘游戏推理的核心能力。我们的主要理论结果建立了两个陈述。首先，我们证明了单层自注意变换器的嵌入和价值维度的紧密边界。其次，我们证明如果该模型在长度1和2上实现了平衡的logit位移，那么它必须泛化到更长的序列，尽管精度有所降低。对证明的机械解读解释了这一局限：随着更多标记被关注，softmax压缩logit位移，削弱了有效和无效输出之间的区分。训练动态也表明存在第二个障碍：当有许多可能的下一个标记时，更新变得嘈杂。我们假设dropout可以抵消第一个效应，而Exponential Moving Average (EMA)可以抵消第二个效应。我们通过集合补任务中的随机超参数搜索验证了这些假设，这证实了这两种机制。然后，我们测试了一个基于随机井字棋移动训练的OthelloGPT模型，并发现在这个更复杂的环境中，EMA再次改善了长度泛化。 

---
# Iterated Agent for Symbolic Regression 

**Title (ZH)**: 迭代代理符号回归 

**Authors**: Zhuo-Yang Song, Zeyu Cai, Shutao Zhang, Jiashen Wei, Jichen Pan, Shi Qiu, Qing-Hong Cao, Tie-Jiun Hou, Xiaohui Liu, Ming-xing Luo, Hua Xing Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2510.08317)  

**Abstract**: Symbolic regression (SR), the automated discovery of mathematical expressions from data, is a cornerstone of scientific inquiry. However, it is often hindered by the combinatorial explosion of the search space and a tendency to overfit. Popular methods, rooted in genetic programming, explore this space syntactically, often yielding overly complex, uninterpretable models. This paper introduces IdeaSearchFitter, a framework that employs Large Language Models (LLMs) as semantic operators within an evolutionary search. By generating candidate expressions guided by natural-language rationales, our method biases discovery towards models that are not only accurate but also conceptually coherent and interpretable. We demonstrate IdeaSearchFitter's efficacy across diverse challenges: it achieves competitive, noise-robust performance on the Feynman Symbolic Regression Database (FSReD), outperforming several strong baselines; discovers mechanistically aligned models with good accuracy-complexity trade-offs on real-world data; and derives compact, physically-motivated parametrizations for Parton Distribution Functions in a frontier high-energy physics application. IdeaSearchFitter is a specialized module within our broader iterated agent framework, IdeaSearch, which is publicly available at this https URL. 

**Abstract (ZH)**: 符号回归（SR），从数据中自动发现数学表达式的科学探究基石，但由于搜索空间的组合爆炸和过度拟合的倾向，常受阻。传统方法基于遗传编程，通过语法探索这一空间，经常生成过于复杂且难以解释的模型。本文引入了IdeaSearchFitter框架，该框架利用大型语言模型（LLMs）作为语义操作符进行进化搜索。通过根据自然语言的合理性生成候选表达式，该方法倾向于发现不仅准确而且概念连贯且可解释的模型。我们在多种挑战中展示了IdeaSearchFitter的有效性：它在费恩曼符号回归数据库（FSReD）上实现了具有竞争力且抗噪的强大性能，超过了几个强有力的基线；在真实数据上发现了具有良好准确性和复杂性权衡的机制对齐模型；并在高能物理前沿应用中为部分分布函数推导出了紧凑且物理导向的参数化表示。IdeaSearchFitter是我们更广泛迭代代理框架IdeaSearch中的一个专门模块，该框架可在以下网址获得：this https URL。 

---
# Counterfactual Identifiability via Dynamic Optimal Transport 

**Title (ZH)**: 动态最优传输下的反事实可识别性 

**Authors**: Fabio De Sousa Ribeiro, Ainkaran Santhirasekaram, Ben Glocker  

**Link**: [PDF](https://arxiv.org/pdf/2510.08294)  

**Abstract**: We address the open question of counterfactual identification for high-dimensional multivariate outcomes from observational data. Pearl (2000) argues that counterfactuals must be identifiable (i.e., recoverable from the observed data distribution) to justify causal claims. A recent line of work on counterfactual inference shows promising results but lacks identification, undermining the causal validity of its estimates. To address this, we establish a foundation for multivariate counterfactual identification using continuous-time flows, including non-Markovian settings under standard criteria. We characterise the conditions under which flow matching yields a unique, monotone and rank-preserving counterfactual transport map with tools from dynamic optimal transport, ensuring consistent inference. Building on this, we validate the theory in controlled scenarios with counterfactual ground-truth and demonstrate improvements in axiomatic counterfactual soundness on real images. 

**Abstract (ZH)**: 我们针对可观测数据中的高维多变量结果的反事实识别这一开放问题提出了解决方案。佩尔（2000）认为，为了证实因果关系，反事实必须是可以从观测数据分布中恢复的。近期关于反事实推断的研究显示出了有希望的结果，但缺乏识别性，这削弱了其因果有效性。为了解决这一问题，我们利用连续时间流建立了多变量反事实识别的基础，包括在标准条件下非马氏过程下的识别性。我们通过动力最优传输的工具刻画了流匹配在何种条件下可以生成唯一的、单调的且保持秩的反事实传输映射，并确保推断的一致性。在此基础上，我们在有反事实真相控制的场景中验证了该理论，并在实际图像上展示了在公理反事实准确性方面的改进。 

---
# Learning Neural Exposure Fields for View Synthesis 

**Title (ZH)**: 学习神经曝光场进行视图合成 

**Authors**: Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Christina Tsalicoglou, Keisuke Tateno, Jonathan T. Barron, Federico Tombari  

**Link**: [PDF](https://arxiv.org/pdf/2510.08279)  

**Abstract**: Recent advances in neural scene representations have led to unprecedented quality in 3D reconstruction and view synthesis. Despite achieving high-quality results for common benchmarks with curated data, outputs often degrade for data that contain per image variations such as strong exposure changes, present, e.g., in most scenes with indoor and outdoor areas or rooms with windows. In this paper, we introduce Neural Exposure Fields (NExF), a novel technique for robustly reconstructing 3D scenes with high quality and 3D-consistent appearance from challenging real-world captures. In the core, we propose to learn a neural field predicting an optimal exposure value per 3D point, enabling us to optimize exposure along with the neural scene representation. While capture devices such as cameras select optimal exposure per image/pixel, we generalize this concept and perform optimization in 3D instead. This enables accurate view synthesis in high dynamic range scenarios, bypassing the need of post-processing steps or multi-exposure captures. Our contributions include a novel neural representation for exposure prediction, a system for joint optimization of the scene representation and the exposure field via a novel neural conditioning mechanism, and demonstrated superior performance on challenging real-world data. We find that our approach trains faster than prior works and produces state-of-the-art results on several benchmarks improving by over 55% over best-performing baselines. 

**Abstract (ZH)**: Recent Advances in Neural Scene Representations: Neural Exposure Fields for Robust 3D Reconstruction and View Synthesis in Challenging Real-World Scenarios 

---
# A Distributed Emulation Environment for In-Memory Computing Systems 

**Title (ZH)**: 基于内存计算系统的分布式仿真环境 

**Authors**: Eleni Bougioukou, Anastasios Petropoulos, Nikolaos Toulgaridis, Theodoros Chatzimichail, Theodore Antonakopoulos  

**Link**: [PDF](https://arxiv.org/pdf/2510.08257)  

**Abstract**: In-memory computing technology is used extensively in artificial intelligence devices due to lower power consumption and fast calculation of matrix-based functions. The development of such a device and its integration in a system takes a significant amount of time and requires the use of a real-time emulation environment, where various system aspects are analyzed, microcode is tested, and applications are deployed, even before the real chip is available. In this work, we present the architecture, the software development tools, and experimental results of a distributed and expandable emulation system for rapid prototyping of integrated circuits based on in-memory computing technologies. Presented experimental results demonstrate the usefulness of the proposed emulator. 

**Abstract (ZH)**: 基于内存计算技术的分布扩展型快速原型验证系统及其软件开发工具和实验结果 

---
# Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization 

**Title (ZH)**: Mix-和MoE-DPO: 一种直接偏好优化的变分推断方法 

**Authors**: Jason Bohne, Pawel Polak, David Rosenberg, Brian Bloniarz, Gary Kazantsev  

**Link**: [PDF](https://arxiv.org/pdf/2510.08256)  

**Abstract**: Direct Preference Optimization (DPO) has recently emerged as a simple and effective alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with user preferences. However, existing DPO formulations rely on a single monolithic model, which limits their expressivity in multi-task settings and their adaptability to heterogeneous or diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a framework that extends DPO with both soft mixture models and mixture-of-experts (MoE) architectures, using a stochastic variational inference approach. Our method introduces a latent-variable model over expert assignments and optimizes a variational evidence lower bound (ELBO), enabling stable and efficient learning of specialized expert policies from preference data. Mix- and MoE-DPO provides three key advantages over standard DPO: (i) generalization via universal function approximation through mixtures; (ii) reward and policy specialization through expert components tailored to distinct preference modes; and (iii) contextual alignment through input-dependent soft gating that enables user-specific mixture policies. Our framework supports both shared base architectures with expert-specific policy heads and fully independent expert models, allowing flexible trade-offs between parameter efficiency and specialization. We validate our approach on a variety of model sizes and multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a powerful and scalable method for preference-based LLM alignment. 

**Abstract (ZH)**: 直接偏好优化（DPO）已逐渐成为一种简单有效的替代强化学习从人类反馈（RLHF）的方法，用于使大型语言模型（LLMs）与用户偏好对齐。然而，现有的DPO形式化依赖于单一的大型模型，这限制了它们在多任务设置中的表达能力和对异质或多样偏好分布的适应能力。在本文中，我们提出了一种Mix-和MoE-DPO框架，该框架通过软混合模型和专家混合（MoE）架构扩展了DPO，使用了随机变分推理方法。我们的方法引入了专家分配的潜在变量模型，并优化了变分证据下界（ELBO），从而能够从偏好数据中稳定、高效地学习专门的专家策略。与标准DPO相比，Mix-和MoE-DPO具有三大优势：（i）通过混合实现通用函数逼近的泛化；（ii）通过针对不同偏好模式定制的专家组成部分实现奖励和策略专业化；以及（iii）通过输入相关的软门控实现上下文对齐，从而支持用户特定的混合策略。我们的框架支持共享基础架构和专家特定策略头，以及完全独立的专家模型，允许在参数效率和专业化之间灵活权衡。我们在多种模型规模和多偏好数据集上验证了该方法，证明了Mix-和MoE-DPO为基于偏好的LLM对齐提供了一种强大且可扩展的方法。 

---
# Opponent Shaping in LLM Agents 

**Title (ZH)**: 对手塑造在LLM代理中的应用 

**Authors**: Marta Emili Garcia Segura, Stephen Hailes, Mirco Musolesi  

**Link**: [PDF](https://arxiv.org/pdf/2510.08255)  

**Abstract**: Large Language Models (LLMs) are increasingly being deployed as autonomous agents in real-world environments. As these deployments scale, multi-agent interactions become inevitable, making it essential to understand strategic behavior in such systems. A central open question is whether LLM agents, like reinforcement learning agents, can shape the learning dynamics and influence the behavior of others through interaction alone. In this paper, we present the first investigation of opponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot be directly applied to LLMs, as they require higher-order derivatives, face scalability constraints, or depend on architectural components that are absent in transformers. To address this gap, we introduce ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based agents. Using ShapeLLM, we examine whether LLM agents can influence co-players' learning dynamics across diverse game-theoretic environments. We demonstrate that LLM agents can successfully guide opponents toward exploitable equilibria in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and Chicken) and promote coordination and improve collective welfare in cooperative games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma). Our findings show that LLM agents can both shape and be shaped through interaction, establishing opponent shaping as a key dimension of multi-agent LLM research. 

**Abstract (ZH)**: 大型语言模型（LLMs）日益被部署为自主代理在实际环境中的应用。随着部署规模的扩大，多代理交互变得不可避免，因此理解此类系统中的战略行为至关重要。一个中心的开放问题是，像强化学习代理一样，基于LLM的代理是否仅通过交互就能塑造学习动力学并影响其他代理的行为。在本文中，我们提出了首个多代理大型语言模型（LLM）对手塑造（OS）的调查。现有的OS算法无法直接应用于LLMs，因为它们需要高阶导数、面临可扩展性限制，或者依赖于变压器架构中不存在的组件。为了解决这一问题，我们引入了ShapeLLM，这是一种针对基于变压器代理的模型自由OS方法的适应版本。利用ShapeLLM，我们研究了LLM代理是否能够在多种博弈论环境中成功引导同伴代理的学习动力学。我们证明了LLM代理可以在竞争游戏（重复囚徒困境、猜硬币和鸡与兔子博弈）中引导对手走向可利用的均衡，并在合作游戏（重复鹿苑博弈和合作版的囚徒困境）中促进协调并提高集体福利。我们的研究结果表明，LLM代理可以通过交互进行塑造，同时也能够被塑造，从而确立了对手塑造作为多代理LLM研究关键维度的地位。 

---
# Contrastive Decoding for Synthetic Data Generation in Low-Resource Language Modeling 

**Title (ZH)**: 合成数据生成中的对比解码在低资源语言建模中应用 

**Authors**: Jannek Ulm, Kevin Du, Vésteinn Snæbjarnarson  

**Link**: [PDF](https://arxiv.org/pdf/2510.08245)  

**Abstract**: Large language models (LLMs) are trained on huge amounts of textual data, and concerns have been raised that the limits of such data may soon be reached. A potential solution is to train on synthetic data sampled from LLMs. In this work, we build on this idea and investigate the benefits of contrastive decoding for generating synthetic corpora. In a controlled setting, we experiment with sampling corpora using the relative difference between a good and bad model trained on the same original corpus of 100 million words. By amplifying the signal from a model that has better performance, we create a synthetic corpus and mix it with the original training data. Our findings show that training on a mixture of synthesized and real data improves performance on the language modeling objective and a range of downstream tasks. In particular, we see that training with a mix of synthetic data from contrastive decoding benefits tasks that require more reasoning skills, while synthetic data from traditional sampling helps more on tasks dependent on surface level linguistic capabilities. 

**Abstract (ZH)**: 大型语言模型（LLMs）是基于大量文本数据训练的，有观点认为此类数据的限制可能即将达到。一种潜在的解决方案是使用从LLMs中抽取的合成数据进行训练。在本工作中，我们基于这一理念，研究对比解码生成合成语料库的好处。在受控环境中，我们使用同一原始语料库（1亿词）中性能良好和较差模型之间的相对差异来抽取语料库。通过放大性能较好的模型的信号，我们生成合成语料库并与原始训练数据混合。我们的实验结果表明，使用合成和真实数据的混合进行训练，可以提高语言建模目标和各种下游任务的性能。特别是，使用对比解码生成的合成数据进行训练，在需要更强推理能力的任务上表现出优势，而传统抽样生成的合成数据则在依赖于表面语言能力的任务上表现更好。 

---
# The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes in Large Language Models 

**Title (ZH)**: 隐藏的偏见：关于大型语言模型中显性和隐性政治刻板印象的研究 

**Authors**: Konrad Löhr, Shuzhou Yuan, Michael Färber  

**Link**: [PDF](https://arxiv.org/pdf/2510.08236)  

**Abstract**: Large Language Models (LLMs) are increas- ingly integral to information dissemination and decision-making processes. Given their grow- ing societal influence, understanding potential biases, particularly within the political domain, is crucial to prevent undue influence on public opinion and democratic processes. This work investigates political bias and stereotype propa- gation across eight prominent LLMs using the two-dimensional Political Compass Test (PCT). Initially, the PCT is employed to assess the in- herent political leanings of these models. Sub- sequently, persona prompting with the PCT is used to explore explicit stereotypes across vari- ous social dimensions. In a final step, implicit stereotypes are uncovered by evaluating mod- els with multilingual versions of the PCT. Key findings reveal a consistent left-leaning polit- ical alignment across all investigated models. Furthermore, while the nature and extent of stereotypes vary considerably between models, implicit stereotypes elicited through language variation are more pronounced than those iden- tified via explicit persona prompting. Interest- ingly, for most models, implicit and explicit stereotypes show a notable alignment, suggest- ing a degree of transparency or "awareness" regarding their inherent biases. This study un- derscores the complex interplay of political bias and stereotypes in LLMs. 

**Abstract (ZH)**: 大型语言模型（LLMs）越来越成为信息传播和决策过程中的重要组成部分。鉴于其日益增长的社会影响力，理解潜在的政治偏见，特别是在政治领域中的偏见，对于防止对公众意见和民主过程产生不当影响至关重要。本研究使用政治极坐标测试（PCT）来探讨八种主要LLM中的政治偏见和刻板印象传播。首先，使用PCT来评估这些模型的固有政治倾向。随后，通过PCT的人设提示来探索各种社会维度上的明确刻板印象。最后，通过使用多语言版本的PCT来评估模型，揭示隐含的刻板印象。关键发现表明，在所有研究的模型中都存在一致的左倾政治倾向。此外，虽然模型之间刻板印象的性质和程度差异显著，但通过语言变体引发的隐含刻板印象比通过明确人设提示识别的刻板印象更为明显。有趣的是，对于大多数模型而言，隐含刻板印象和明确刻板印象显示出明显的对齐，表明其对固有偏见具有一定的透明度或“意识”。本研究突显了政治偏见和刻板印象在LLM中的复杂相互作用。 

---
# Expressive Value Learning for Scalable Offline Reinforcement Learning 

**Title (ZH)**: 可扩展离线强化学习中的表达性价值学习 

**Authors**: Nicolas Espinosa-Dice, Kiante Brantley, Wen Sun  

**Link**: [PDF](https://arxiv.org/pdf/2510.08218)  

**Abstract**: Reinforcement learning (RL) is a powerful paradigm for learning to make sequences of decisions. However, RL has yet to be fully leveraged in robotics, principally due to its lack of scalability. Offline RL offers a promising avenue by training agents on large, diverse datasets, avoiding the costly real-world interactions of online RL. Scaling offline RL to increasingly complex datasets requires expressive generative models such as diffusion and flow matching. However, existing methods typically depend on either backpropagation through time (BPTT), which is computationally prohibitive, or policy distillation, which introduces compounding errors and limits scalability to larger base policies. In this paper, we consider the question of how to develop a scalable offline RL approach without relying on distillation or backpropagation through time. We introduce Expressive Value Learning for Offline Reinforcement Learning (EVOR): a scalable offline RL approach that integrates both expressive policies and expressive value functions. EVOR learns an optimal, regularized Q-function via flow matching during training. At inference-time, EVOR performs inference-time policy extraction via rejection sampling against the expressive value function, enabling efficient optimization, regularization, and compute-scalable search without retraining. Empirically, we show that EVOR outperforms baselines on a diverse set of offline RL tasks, demonstrating the benefit of integrating expressive value learning into offline RL. 

**Abstract (ZH)**: 增强学习（RL）是一种强大的决策序列学习范式。然而，RL 在机器人领域的应用尚未充分发挥，主要原因在于其缺乏可扩展性。离线 RL 通过在大规模、多样化的数据集上训练代理，避免了在线 RL 成本高昂的真实世界交互，为克服这一问题提供了有前景的方法。将离线 RL 扩展到越来越复杂的数据集需要诸如扩散和流匹配等具有表现力的生成模型。然而，现有方法通常依赖于时间反向传播（BPTT），这在计算上是不可行的，或者依赖于策略蒸馏，这会引入累积误差并限制其在更大基础策略上的可扩展性。在本文中，我们探讨了如何开发一种无需依赖蒸馏或时间反向传播的可扩展离线 RL 方法。我们引入了用于离线增强学习的具有表现力的价值学习（EVOR）：一种结合了具有表现力的策略和价值函数的可扩展离线 RL 方法。EVOR 在训练期间通过流匹配学习最优、正则化的 Q 函数。在推理时，EVOR 通过拒绝采样针对具有表现力的价值函数进行策略提取，从而实现高效的优化、正则化和计算可扩展的搜索而无需重新训练。实验结果显示，EVOR 在多种离线 RL 任务上的表现优于基线方法，证明了将具有表现力的价值学习整合到离线 RL 中的好处。 

---
# FuelCast: Benchmarking Tabular and Temporal Models for Ship Fuel Consumption 

**Title (ZH)**: FuelCast: 评估表结构和时间序列模型在船舶燃油消耗预测中的性能 

**Authors**: Justus Viga, Penelope Mueck, Alexander Löser, Torben Weis  

**Link**: [PDF](https://arxiv.org/pdf/2510.08217)  

**Abstract**: In the shipping industry, fuel consumption and emissions are critical factors due to their significant impact on economic efficiency and environmental sustainability. Accurate prediction of ship fuel consumption is essential for further optimization of maritime operations. However, heterogeneous methodologies and limited high-quality datasets hinder direct comparison of modeling approaches. This paper makes three key contributions: (1) we introduce and release a new dataset (this https URL) comprising operational and environmental data from three ships; (2) we define a standardized benchmark covering tabular regression and time-series regression (3) we investigate the application of in-context learning for ship consumption modeling using the TabPFN foundation model - a first in this domain to our knowledge. Our results demonstrate strong performance across all evaluated models, supporting the feasibility of onboard, data-driven fuel prediction. Models incorporating environmental conditions consistently outperform simple polynomial baselines relying solely on vessel speed. TabPFN slightly outperforms other techniques, highlighting the potential of foundation models with in-context learning capabilities for tabular prediction. Furthermore, including temporal context improves accuracy. 

**Abstract (ZH)**: 在航运业中，燃料消耗和排放是关键因素，因为它们对经济效率和环境可持续性有重大影响。准确预测船舶燃料消耗对于进一步优化海上运营至关重要。然而，方法异质性和高质量数据集有限阻碍了建模方法的直接比较。本文做出了三项关键贡献：（1）我们引入并发布了新的数据集（https://...），包含三艘船舶的运营和环境数据；（2）我们定义了一个标准化基准，涵盖表格回归和时间序列回归；（3）我们研究了使用TabPFN基础模型进行船舶消耗建模的上下文学习应用——据我们所知，这是该领域的首创。我们的结果显示，所有评估模型均表现出色，支持了船上基于数据的燃料预测的可行性。包含环境条件的模型一贯优于仅依赖船舶速度的简单多项式基线。TabPFN 略微优于其他技术，突显了具有上下文学习能力的基础模型在表格预测中的潜力。此外，包含时间上下文可以提高准确性。 

---
# LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions 

**Title (ZH)**: LLMs 不慎学会欺骗：从不对齐样本到有偏人类-AI 交互的不诚实 Emergent 对齐失效 

**Authors**: XuHao Hu, Peng Wang, Xiaoya Lu, Dongrui Liu, Xuanjing Huang, Jing Shao  

**Link**: [PDF](https://arxiv.org/pdf/2510.08211)  

**Abstract**: Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or incorrect medical advice) can become broadly misaligned to exhibit harmful behaviors, which is called emergent misalignment. In this work, we investigate whether this phenomenon can extend beyond safety behaviors to a broader spectrum of dishonesty and deception under high-stakes scenarios (e.g., lying under pressure and deceptive behavior). To explore this, we finetune open-sourced LLMs on misaligned completions across diverse domains. Experimental results demonstrate that LLMs show broadly misaligned behavior in dishonesty. Additionally, we further explore this phenomenon in a downstream combined finetuning setting, and find that introducing as little as 1% of misalignment data into a standard downstream task is sufficient to decrease honest behavior over 20%. Furthermore, we consider a more practical human-AI interaction environment where we simulate both benign and biased users to interact with the assistant LLM. Notably, we find that the assistant can be misaligned unintentionally to exacerbate its dishonesty with only 10% biased user population. In summary, we extend the study of emergent misalignment to the domain of dishonesty and deception under high-stakes scenarios, and demonstrate that this risk arises not only through direct finetuning, but also in downstream mixture tasks and practical human-AI interactions. 

**Abstract (ZH)**: Previous research has shown that LLMs fine-tuned on malicious or incorrect completions within narrow domains (e.g., insecure code or incorrect medical advice) can become broadly misaligned to exhibit harmful behaviors, which is called emergent misalignment. In this work, we investigate whether this phenomenon can extend beyond safety behaviors to a broader spectrum of dishonesty and deception under high-stakes scenarios (e.g., lying under pressure and deceptive behavior). 

---
# Memory Retrieval and Consolidation in Large Language Models through Function Tokens 

**Title (ZH)**: 大型语言模型中通过功能标记实现的记忆检索与巩固 

**Authors**: Shaohua Zhang, Yuan Lin, Hang Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.08203)  

**Abstract**: The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context. 

**Abstract (ZH)**: 大型语言模型成功的原因在于它们能够在预训练过程中将大量知识整合到记忆中，并在推理过程中从中检索，从而实现知识记忆、指令跟随和推理等高级能力。然而，大型语言模型中记忆检索和整合的机制仍然知之甚少。本文提出的函数标记假说旨在解释大型语言模型的工作机制：在推理过程中，函数标记激活最具预测性的上下文特征并控制下一个标记的预测（记忆检索）。在预训练过程中，预测跟随函数标记的下一个标记（通常是内容标记）增加了大型语言模型学习的特征数量并更新了模型参数（记忆整合）。这里的函数标记大致对应于语言学中的功能词，包括标点符号、冠词、介词和连词，与内容标记相对。我们提供了大量实验证据支持这一假说。利用二分图分析表明，少量的函数标记激活了大多数的特征。案例研究进一步揭示了函数标记如何激活上下文中的最具预测性的特征以直接进行下一个标记的预测。我们还发现，在预训练过程中，训练损失主要由预测跟随函数标记的下一个内容标记决定，这迫使函数标记从上下文中选择最具预测性的特征。 

---
# Sentiment Matters: An Analysis of 200 Human-SAV Interactions 

**Title (ZH)**: 情绪很重要：200名人-AVR交互的情感分析 

**Authors**: Lirui Guo, Michael G. Burke, Wynita M. Griggs  

**Link**: [PDF](https://arxiv.org/pdf/2510.08202)  

**Abstract**: Shared Autonomous Vehicles (SAVs) are likely to become an important part of the transportation system, making effective human-SAV interactions an important area of research. This paper introduces a dataset of 200 human-SAV interactions to further this area of study. We present an open-source human-SAV conversational dataset, comprising both textual data (e.g., 2,136 human-SAV exchanges) and empirical data (e.g., post-interaction survey results on a range of psychological factors). The dataset's utility is demonstrated through two benchmark case studies: First, using random forest modeling and chord diagrams, we identify key predictors of SAV acceptance and perceived service quality, highlighting the critical influence of response sentiment polarity (i.e., perceived positivity). Second, we benchmark the performance of an LLM-based sentiment analysis tool against the traditional lexicon-based TextBlob method. Results indicate that even simple zero-shot LLM prompts more closely align with user-reported sentiment, though limitations remain. This study provides novel insights for designing conversational SAV interfaces and establishes a foundation for further exploration into advanced sentiment modeling, adaptive user interactions, and multimodal conversational systems. 

**Abstract (ZH)**: 共享自主车辆（SAVs）可能是交通系统的重要组成部分，有效的人-SAV交互将成为一个重要研究领域。本文介绍了一组200例人-SAV交互的数据集，以进一步推进该领域的研究。我们提供了一个开源的人-SAV对话数据集，包括文本数据（例如，2,136个人-SAV交互）和实证数据（例如，关于一系列心理因素的交互后调查结果）。通过两个基准案例研究展示了数据集的实用性：首先，利用随机森林建模和弦图，我们确定了SAV接受度和服务质量感知的关键预测因素，突出了响应情感极性（即感知的积极程度）的关键影响。其次，我们将基于LLM的情感分析工具的性能与传统的基于词典的TextBlob方法进行了基准测试。结果表明，即使简单的零样本LLM提示更接近用户报告的情感，但仍有局限性。本研究为设计对话SAV界面提供了新颖的见解，并建立了进一步探索高级情感模型、适应性用户交互和多模态对话系统的基础。 

---
# Robust Canonicalization through Bootstrapped Data Re-Alignment 

**Title (ZH)**: 通过-bootstraped数据重对齐的稳健 canonical化 

**Authors**: Johann Schmidt, Sebastian Stober  

**Link**: [PDF](https://arxiv.org/pdf/2510.08178)  

**Abstract**: Fine-grained visual classification (FGVC) tasks, such as insect and bird identification, demand sensitivity to subtle visual cues while remaining robust to spatial transformations. A key challenge is handling geometric biases and noise, such as different orientations and scales of objects. Existing remedies rely on heavy data augmentation, which demands powerful models, or on equivariant architectures, which constrain expressivity and add cost. Canonicalization offers an alternative by shielding such biases from the downstream model. In practice, such functions are often obtained using canonicalization priors, which assume aligned training data. Unfortunately, real-world datasets never fulfill this assumption, causing the obtained canonicalizer to be brittle. We propose a bootstrapping algorithm that iteratively re-aligns training samples by progressively reducing variance and recovering the alignment assumption. We establish convergence guarantees under mild conditions for arbitrary compact groups, and show on four FGVC benchmarks that our method consistently outperforms equivariant, and canonicalization baselines while performing on par with augmentation. 

**Abstract (ZH)**: 细粒度视觉分类任务中的范化对齐算法：处理几何偏置和噪声以提高模型鲁棒性 

---
# Leveraging Whisper Embeddings for Audio-based Lyrics Matching 

**Title (ZH)**: 基于 whisper 向量的音频歌词匹配 

**Authors**: Eleonora Mancini, Joan Serrà, Paolo Torroni, Yuki Mitsufuji  

**Link**: [PDF](https://arxiv.org/pdf/2510.08176)  

**Abstract**: Audio-based lyrics matching can be an appealing alternative to other content-based retrieval approaches, but existing methods often suffer from limited reproducibility and inconsistent baselines. In this work, we introduce WEALY, a fully reproducible pipeline that leverages Whisper decoder embeddings for lyrics matching tasks. WEALY establishes robust and transparent baselines, while also exploring multimodal extensions that integrate textual and acoustic features. Through extensive experiments on standard datasets, we demonstrate that WEALY achieves a performance comparable to state-of-the-art methods that lack reproducibility. In addition, we provide ablation studies and analyses on language robustness, loss functions, and embedding strategies. This work contributes a reliable benchmark for future research, and underscores the potential of speech technologies for music information retrieval tasks. 

**Abstract (ZH)**: 基于音频的歌词匹配可以成为内容基于的检索方法的有吸引力的替代方案，但现有方法往往存在重现性有限和基准不一致的问题。在本文中，我们介绍了WEALY，一个完全可重现的管道，利用Whisper解码器嵌入进行歌词匹配任务。WEALY建立了稳健且透明的基准，并探索了结合文本和声学特征的多模态扩展。通过在标准数据集上的广泛实验，我们展示了WEALY在重现性较差的领先方法中的性能相当。此外，我们提供了消融研究和语言鲁棒性、损失函数和嵌入策略的分析。本工作为未来研究提供了一个可靠的基准，并强调了语音技术在音乐信息检索任务中的潜在应用。 

---
# NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions 

**Title (ZH)**: NavSpace: 导航代理如何遵循空间智能指令 

**Authors**: Haolin Yang, Yuxing Long, Zhuoyuan Yu, Zihan Yang, Minghan Wang, Jiapeng Xu, Yihan Wang, Ziyan Yu, Wenzhe Cai, Lei Kang, Hao Dong  

**Link**: [PDF](https://arxiv.org/pdf/2510.08173)  

**Abstract**: Instruction-following navigation is a key step toward embodied intelligence. Prior benchmarks mainly focus on semantic understanding but overlook systematically evaluating navigation agents' spatial perception and reasoning capabilities. In this work, we introduce the NavSpace benchmark, which contains six task categories and 1,228 trajectory-instruction pairs designed to probe the spatial intelligence of navigation agents. On this benchmark, we comprehensively evaluate 22 navigation agents, including state-of-the-art navigation models and multimodal large language models. The evaluation results lift the veil on spatial intelligence in embodied navigation. Furthermore, we propose SNav, a new spatially intelligent navigation model. SNav outperforms existing navigation agents on NavSpace and real robot tests, establishing a strong baseline for future work. 

**Abstract (ZH)**: 指令跟随导航是实现体态智能的关键步骤。现有基准主要关注语义理解，而忽视了系统评估导航代理的空间感知和推理能力。在此项工作中，我们引入了NavSpace基准，包含六类任务和1,228条轨迹-指令对，旨在探测导航代理的空间智能。在该基准上，我们全面评估了22种导航代理，包括最先进的导航模型和多模态大语言模型。评估结果揭示了体态导航中的空间智能。此外，我们提出了SNav，一种新的空间智能导航模型。SNav在NavSpace和真实机器人测试中均优于现有导航代理，为未来工作设定了强有力的基准。 

---
# Quantum Agents for Algorithmic Discovery 

**Title (ZH)**: 量子代理进行算法发现 

**Authors**: Iordanis Kerenidis, El-Amine Cherrat  

**Link**: [PDF](https://arxiv.org/pdf/2510.08159)  

**Abstract**: We introduce quantum agents trained by episodic, reward-based reinforcement learning to autonomously rediscover several seminal quantum algorithms and protocols. In particular, our agents learn: efficient logarithmic-depth quantum circuits for the Quantum Fourier Transform; Grover's search algorithm; optimal cheating strategies for strong coin flipping; and optimal winning strategies for the CHSH and other nonlocal games. The agents achieve these results directly through interaction, without prior access to known optimal solutions. This demonstrates the potential of quantum intelligence as a tool for algorithmic discovery, opening the way for the automated design of novel quantum algorithms and protocols. 

**Abstract (ZH)**: 通过基于奖励的经验学习训练的量子代理自主重新发现多个经典量子算法和协议 

---
# DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations 

**Title (ZH)**: DACIP-RC：基于商业对话阅读理解的领域适应连续指令预训练 

**Authors**: Elena Khasanova, Harsh Saini, Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN  

**Link**: [PDF](https://arxiv.org/pdf/2510.08152)  

**Abstract**: The rapid advancements in Large Language Models (LLMs) have enabled their adoption in real-world industrial scenarios for various natural language processing tasks. However, the high inference cost of large-scale LLMs makes their deployment impractical, necessitating the use of smaller models. Despite their efficiency, smaller LLMs lack robust zero-shot instruction-following capabilities across diverse domains, limiting their adaptability to dynamic user requirements. Traditional fine-tuning approaches exacerbate this issue by inducing catastrophic forgetting, reducing the model's generalization ability for unseen tasks. In this paper, we propose Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension (DACIP-RC), a continual pre-training technique that enhances smaller LLMs' domain adaptability for business conversational tasks. Unlike conventional pre-training approaches that rely on next-token prediction, DACIP-RC generates diverse task instructions and responses via reading comprehension on conversation transcripts, enabling better instruction generalization. Our empirical evaluations demonstrate that DACIP-RC significantly improves zero-shot generalization across a wide range of business conversational tasks, including meeting summarization, action item generation, and call purpose identification. To the best of our knowledge, this is the first work to apply instruction pre-training on business conversational data, providing insights into how industries can leverage proprietary datasets for domain adaptation. 

**Abstract (ZH)**: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension for Business Conversational Tasks 

---
# AI Knowledge Assist: An Automated Approach for the Creation of Knowledge Bases for Conversational AI Agents 

**Title (ZH)**: AI知识助手：面向对话AI代理的知识库自动化创建方法 

**Authors**: Md Tahmid Rahman Laskar, Julien Bouvier Tremblay, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN  

**Link**: [PDF](https://arxiv.org/pdf/2510.08149)  

**Abstract**: The utilization of conversational AI systems by leveraging Retrieval Augmented Generation (RAG) techniques to solve customer problems has been on the rise with the rapid progress of Large Language Models (LLMs). However, the absence of a company-specific dedicated knowledge base is a major barrier to the integration of conversational AI systems in contact centers. To this end, we introduce AI Knowledge Assist, a system that extracts knowledge in the form of question-answer (QA) pairs from historical customer-agent conversations to automatically build a knowledge base. Fine-tuning a lightweight LLM on internal data demonstrates state-of-the-art performance, outperforming larger closed-source LLMs. More specifically, empirical evaluation on 20 companies demonstrates that the proposed AI Knowledge Assist system that leverages the LLaMA-3.1-8B model eliminates the cold-start gap in contact centers by achieving above 90% accuracy in answering information-seeking questions. This enables immediate deployment of RAG-powered chatbots. 

**Abstract (ZH)**: 利用检索增强生成（RAG）技术借助大型语言模型（LLMs）推动会话AI系统的应用：基于历史客户-代理对话提取知识构建公司专用知识库 

---
# Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning 

**Title (ZH)**: 恰到好处地思考：序列熵作为LLM推理的置信信号 

**Authors**: Aman Sharma, Paras Chopra  

**Link**: [PDF](https://arxiv.org/pdf/2510.08146)  

**Abstract**: We introduce a simple, yet novel entropy-based framework to drive token efficiency in large language models during reasoning tasks. Our approach uses Shannon entropy from token-level logprobs as a confidence signal to enable early stopping, achieving 25-50% computational savings while maintaining task accuracy. Crucially, we demonstrate that entropy-based confidence calibration represents an emergent property of advanced post-training optimization present in modern reasoning models but notably absent in standard instruction-tuned and pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop reasoning varies from model to model but can be calculated easily in one shot using only a few examples from existing reasoning datasets. Our results indicate that advanced reasoning models often know that they've gotten a correct answer early on, and that this emergent confidence awareness can be exploited to save tokens and reduce latency. The framework demonstrates consistent performance across reasoning-optimized model families with 25-50% computational cost reduction while preserving accuracy, revealing that confidence mechanisms represent a distinguishing characteristic of modern post-trained reasoning systems versus their predecessors. 

**Abstract (ZH)**: 我们介绍了一种简单的新型基于熵的框架，用于在大型语言模型推理任务中提升令牌效率。我们的方法使用令牌级logprobs的香农熵作为置信信号以实现早期停止，能够在保持任务准确性的前提下实现25-50%的计算成本节省。关键的是，我们展示了基于熵的置信校准是现代推理模型中一种新兴的后训练优化属性，而在标准指令调优和预训练模型（如Llama 3.3 70B）中则不存在这种属性。我们证明了停止推理的熵阈值因模型而异，但可以通过少量现有推理数据集的例子一次性计算得出。我们的结果显示，先进的推理模型往往在较早阶段就知道它们已经得到了正确的答案，而这种新兴的置信意识可以被利用来节省令牌和减少延迟。该框架在不同推理优化的模型家族中表现出一致的性能，同时实现25-50%的计算成本降低并保持准确性，揭示出置信机制作为现代后训练推理系统与前辈们的区别特征。 

---
# Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement 

**Title (ZH)**: 通过注意力增强提高视频语言模型中的时间理解逻辑一致性 

**Authors**: Chengzhi Li, Heyan Huang, Ping Jian, Zhen Yang, Yaning Tian  

**Link**: [PDF](https://arxiv.org/pdf/2510.08138)  

**Abstract**: Large language models (LLMs) often generate self-contradictory outputs, which severely impacts their reliability and hinders their adoption in practical applications. In video-language models (Video-LLMs), this phenomenon recently draws the attention of researchers. Specifically, these models fail to provide logically consistent responses to rephrased questions based on their grounding outputs. However, the underlying causes of this phenomenon remain underexplored. In this work, we adopt an interpretability-driven approach to analyze, statistically summarize, and intervention the potential factors of the phenomenon. We find that one of the primary reasons for the inconsistency in responses lies in the inability of cross-modal attention heads to effectively distinguish video tokens across different timestamps. To address this, we propose an attention enhancement method called Temporally Conditioned Attention Sharpening (TCAS), which constructs an enhancement objective based on attention distinctions to enhance the model's temporal resolution capability, thereby improving its temporal understanding logic consistency. Experimental results demonstrate that our method significantly enhances the temporal logic consistency of Video-LLMs. Further interpretability analyses reveal that our method indeed improves the temporal discriminability of attention heads, validating our conclusions. Additionally, our method achieves performance improvements in general video temporal grounding tasks, highlighting that temporal logic consistency is a bottleneck in temporal understanding. By enhancing consistency, our method drives significant progress in video temporal understanding. 

**Abstract (ZH)**: 大型语言模型（LLMs）经常生成自相矛盾的输出，严重影响了其可靠性并阻碍了其在实际应用中的采用。在视频-语言模型（Video-LLMs）中，这一现象最近引起了研究人员的注意。具体来说，这些模型无法基于其跨模态定位输出提供逻辑一致的响应。然而，这一现象背后的潜在原因尚未得到充分探索。在本工作中，我们采用可解释性的方法来分析、统计总结并干预这一现象的潜在因素。我们发现，响应不一致的主要原因之一在于跨模态注意力头无法有效区分不同时间戳的视频标记。为此，我们提出了一种名为时间条件下的注意力增强（TCAS）的方法，该方法基于注意力区分构建增强目标，以增强模型的时间分辨率能力，从而提高其时间理解逻辑一致性。实验结果表明，我们的方法显著提高了Video-LLMs的时间逻辑一致性。进一步的可解释性分析表明，我们的方法确实提高了注意力头的时间可区分性，验证了我们的结论。此外，我们的方法在一般视频时间定位任务中也表现出性能提升，突显了时间逻辑一致性是时间理解的瓶颈。通过增强一致性，我们的方法在视频时间理解方面取得了显著进步。 

---
# Approximate Domain Unlearning for Vision-Language Models 

**Title (ZH)**: 视觉-语言模型的近似域忘怀 

**Authors**: Kodai Kawamura, Yuta Goto, Rintaro Yanagi, Hirokatsu Kataoka, Go Irie  

**Link**: [PDF](https://arxiv.org/pdf/2510.08132)  

**Abstract**: Pre-trained Vision-Language Models (VLMs) exhibit strong generalization capabilities, enabling them to recognize a wide range of objects across diverse domains without additional training. However, they often retain irrelevant information beyond the requirements of specific downstream tasks, raising concerns about computational efficiency and potential information leakage. This has motivated growing interest in approximate unlearning, which aims to selectively remove unnecessary knowledge while preserving overall model performance. Existing approaches to approximate unlearning have primarily focused on class unlearning, where a VLM is retrained to fail to recognize specified object classes while maintaining accuracy for others. However, merely forgetting object classes is often insufficient in practical applications. For instance, an autonomous driving system should accurately recognize real cars while avoiding misrecognition of illustrated cars depicted in roadside advertisements as real cars, which could be hazardous. In this paper, we introduce Approximate Domain Unlearning (ADU), a novel problem setting that requires reducing recognition accuracy for images from specified domains (e.g., illustration) while preserving accuracy for other domains (e.g., real). ADU presents new technical challenges: due to the strong domain generalization capability of pre-trained VLMs, domain distributions are highly entangled in the feature space, making naive approaches based on penalizing target domains ineffective. To tackle this limitation, we propose a novel approach that explicitly disentangles domain distributions and adaptively captures instance-specific domain information. Extensive experiments show that our approach outperforms baselines built upon VLM tuning techniques, paving the way for practical and fine-grained unlearning in VLMs. Code: this https URL. 

**Abstract (ZH)**: 预训练多模态模型的近似领域卸载（Approximate Domain Unlearning for Pre-trained Vision-Language Models） 

---
# Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations 

**Title (ZH)**: 通过可验证的全局解释解读LLM-as-a-Judge策略 

**Authors**: Jasmina Gajcin, Erik Miehling, Rahul Nair, Elizabeth Daly, Radu Marinescu, Seshu Tirupathi  

**Link**: [PDF](https://arxiv.org/pdf/2510.08120)  

**Abstract**: Using LLMs to evaluate text, that is, LLM-as-a-judge, is increasingly being used at scale to augment or even replace human annotations. As such, it is imperative that we understand the potential biases and risks of doing so. In this work, we propose an approach for extracting high-level concept-based global policies from LLM-as-a-Judge. Our approach consists of two algorithms: 1) CLoVE (Contrastive Local Verifiable Explanations), which generates verifiable, concept-based, contrastive local explanations and 2) GloVE (Global Verifiable Explanations), which uses iterative clustering, summarization and verification to condense local rules into a global policy. We evaluate GloVE on seven standard benchmarking datasets for content harm detection. We find that the extracted global policies are highly faithful to decisions of the LLM-as-a-Judge. Additionally, we evaluated the robustness of global policies to text perturbations and adversarial attacks. Finally, we conducted a user study to evaluate user understanding and satisfaction with global policies. 

**Abstract (ZH)**: 使用LLM作为评判者评估文本：提取基于概念的全局政策的方法 

---
# Bayesian Decision Making around Experts 

**Title (ZH)**: 专家周围的贝叶斯决策制定 

**Authors**: Daniel Jarne Ornia, Joel Dyer, Nicholas Bishop, Anisoara Calinescu, Michael Wooldridge  

**Link**: [PDF](https://arxiv.org/pdf/2510.08113)  

**Abstract**: Complex learning agents are increasingly deployed alongside existing experts, such as human operators or previously trained agents. However, it remains unclear how should learners optimally incorporate certain forms of expert data, which may differ in structure from the learner's own action-outcome experiences. We study this problem in the context of Bayesian multi-armed bandits, considering: (i) offline settings, where the learner receives a dataset of outcomes from the expert's optimal policy before interaction, and (ii) simultaneous settings, where the learner must choose at each step whether to update its beliefs based on its own experience, or based on the outcome simultaneously achieved by an expert. We formalize how expert data influences the learner's posterior, and prove that pretraining on expert outcomes tightens information-theoretic regret bounds by the mutual information between the expert data and the optimal action. For the simultaneous setting, we propose an information-directed rule where the learner processes the data source that maximizes their one-step information gain about the optimal action. Finally, we propose strategies for how the learner can infer when to trust the expert and when not to, safeguarding the learner for the cases where the expert is ineffective or compromised. By quantifying the value of expert data, our framework provides practical, information-theoretic algorithms for agents to intelligently decide when to learn from others. 

**Abstract (ZH)**: 复杂学习代理与现有专家（如人类操作员或先前训练的代理）共同部署的情况下，如何高效地整合专家数据仍不明确，尤其是当专家数据的结构与学习代理自身的行动-结果经验不同时。我们通过贝叶斯多臂 bandit 框架研究了这个问题，考虑了两种情况：(i) 在线设置，学习者在交互之前接收专家最优策略的结果数据集；(ii) 同步设置，学习者在每一步需要决定是基于自身的经验还是同时实现的专家结果更新其信念。我们形式化了专家数据如何影响学习者的后验分布，并证明了基于专家数据的预训练通过专家数据与最优行动之间的互信息，收紧了信息论意义下的后悔上界。对于同步设置，我们提出了一种基于信息指导的原则，该原则使学习者能够最大化其对最优行动的一步信息增益来处理数据源。最后，我们提出策略以帮助学习者判断何时信任专家以及何时不应，从而保障在专家无效或被篡改的情况下保护学习者。通过量化专家数据的价值，我们的框架为代理提供了实用的信息论算法，以智能地决定何时向他人学习。 

---
# VersionRAG: Version-Aware Retrieval-Augmented Generation for Evolving Documents 

**Title (ZH)**: 版本意识增强型检索增益生成：面向演进文档的版本感知生成方法 

**Authors**: Daniel Huwiler, Kurt Stockinger, Jonathan Fürst  

**Link**: [PDF](https://arxiv.org/pdf/2510.08109)  

**Abstract**: Retrieval-Augmented Generation (RAG) systems fail when documents evolve through versioning-a ubiquitous characteristic of technical documentation. Existing approaches achieve only 58-64% accuracy on version-sensitive questions, retrieving semantically similar content without temporal validity checks. We present VersionRAG, a version-aware RAG framework that explicitly models document evolution through a hierarchical graph structure capturing version sequences, content boundaries, and changes between document states. During retrieval, VersionRAG routes queries through specialized paths based on intent classification, enabling precise version-aware filtering and change tracking. On our VersionQA benchmark-100 manually curated questions across 34 versioned technical documents-VersionRAG achieves 90% accuracy, outperforming naive RAG (58%) and GraphRAG (64%). VersionRAG reaches 60% accuracy on implicit change detection where baselines fail (0-10%), demonstrating its ability to track undocumented modifications. Additionally, VersionRAG requires 97% fewer tokens during indexing than GraphRAG, making it practical for large-scale deployment. Our work establishes versioned document QA as a distinct task and provides both a solution and benchmark for future research. 

**Abstract (ZH)**: 版本感知生成（VersionRAG）系统：面向技术文档版本演化的检索增强生成框架 

---
# Development of Mental Models in Human-AI Collaboration: A Conceptual Framework 

**Title (ZH)**: 人类与人工智能协作中心理模型的发展：一个概念框架 

**Authors**: Joshua Holstein, Gerhard Satzger  

**Link**: [PDF](https://arxiv.org/pdf/2510.08104)  

**Abstract**: Artificial intelligence has become integral to organizational decision-making and while research has explored many facets of this human-AI collaboration, the focus has mainly been on designing the AI agent(s) and the way the collaboration is set up - generally assuming a human decision-maker to be "fixed". However, it has largely been neglected that decision-makers' mental models evolve through their continuous interaction with AI systems. This paper addresses this gap by conceptualizing how the design of human-AI collaboration influences the development of three complementary and interdependent mental models necessary for this collaboration. We develop an integrated socio-technical framework that identifies the mechanisms driving the mental model evolution: data contextualization, reasoning transparency, and performance feedback. Our work advances human-AI collaboration literature through three key contributions: introducing three distinct mental models (domain, information processing, complementarity-awareness); recognizing the dynamic nature of mental models; and establishing mechanisms that guide the purposeful design of effective human-AI collaboration. 

**Abstract (ZH)**: 人工智能已成为组织决策不可或缺的一部分，尽管已有研究探讨了许多人机协作方面的内容，但主要集中在设计AI代理及其合作方式上，通常假定人类决策者是固定的。然而，决策者的心智模型通过与AI系统持续互动而演变这一点却很少被关注。本文通过阐述人机协作设计如何影响这一合作所需发展的三种互补且相互依赖的心智模型的发展，弥补了这一缺口。我们构建了一个整合的社会技术框架，识别出驱动心智模型演变的机制：数据语境化、推理透明度和绩效反馈。我们的研究通过三个关键贡献推进了人机协作文献：引入了三种独特的心智模型（领域心智模型、信息处理心智模型、互补性意识心智模型）；承认心智模型的动态特性；并建立了指导目的性设计有效人机协作的机制。 

---
# Lossless Vocabulary Reduction for Auto-Regressive Language Models 

**Title (ZH)**: 无损词汇缩减以供自回归语言模型使用 

**Authors**: Daiki Chijiwa, Taku Hasegawa, Kyosuke Nishida, Shin'ya Yamaguchi, Tomoya Ohba, Tamao Sakao, Susumu Takeuchi  

**Link**: [PDF](https://arxiv.org/pdf/2510.08102)  

**Abstract**: Tokenization -- the process of decomposing a given text into a sequence of subwords called tokens -- is one of the key components in the development of language models. Particularly, auto-regressive language models generate texts token by token, i.e., by predicting the next-token distribution given the previous ones, and thus tokenization directly affects their efficiency in text generation. Since each language model has their own vocabulary as a set of possible tokens, they struggle to cooperate with each other at the level of next-token distributions such as model ensemble. In this paper, we establish a theoretical framework of lossless vocabulary reduction, which efficiently converts a given auto-regressive language model into the one with an arbitrarily small vocabulary without any loss in accuracy. As an application, we demonstrate that language models with different tokenization can cooperate with each other efficiently through their maximal common vocabulary. 

**Abstract (ZH)**: Tokenization——将给定文本分解为称为令牌的子词序列的过程——是语言模型开发中的关键组件之一。特别是，自回归语言模型逐令牌生成文本，即通过预测下一个令牌的概率分布来生成，因此令牌化直接影响它们在文本生成中的效率。由于每种语言模型都有自己的词汇表作为可能的令牌集合，它们在下一个令牌概率分布等层面难以相互协作，如模型集成。在本文中，我们建立了无损词汇表缩减的理论框架，该框架能高效地将给定的自回归语言模型转换为具有任意小词汇表的模型，而不损失准确性。作为应用，我们展示了不同令牌化语言模型可以通过它们的最大公共词汇表高效协作。 

---
# The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models 

**Title (ZH)**: 思考的成本：大规模语言模型中推理、性能和谈判成本的多语言分析 

**Authors**: Sherzod Hakimov, Roland Bernard, Tim Leiber, Karl Osswald, Kristina Richert, Ruilin Yang, Raffaella Bernardi, David Schlangen  

**Link**: [PDF](https://arxiv.org/pdf/2510.08098)  

**Abstract**: Negotiation is a fundamental challenge for AI agents, as it requires an ability to reason strategically, model opponents, and balance cooperation with competition. We conduct the first comprehensive study systematically evaluating the effect of (LLM-)reasoning on the negotiation abilities of both commercial and open-weight LLMs, and do this across three languages. Using a self-play setup across three diverse dialogue games, we analyse trade-offs between performance and cost, the language consistency of reasoning processes, and the nature of strategic adaptation exhibited by models. Our findings show that enabling reasoning-that is, scaling test time compute-significantly improves negotiation outcomes by enhancing collaboration and helping models overcome task complexities, but comes at a substantial computational cost: reasoning improves GPT-5's performance by 31.4 % while increasing its cost by nearly 400 %. Most critically, we uncover a significant multilingual reasoning distinction: open-weight models consistently switch to English for their internal reasoning steps, even when negotiating in German or Italian (and thus possibly impacting potential explainability gains through the disclosure of reasoning traces), while leading commercial models maintain language consistency between their reasoning and final output. 

**Abstract (ZH)**: 谈判能力是AI代理的一项基本挑战，因为它要求具备策略性推理、对手建模和合作与竞争的平衡能力。我们首次系统地研究了（LLM-）推理对商业和开源LLM谈判能力的影响，并在三种语言中进行评估。通过三种多样化的对话游戏进行自博弈设置，我们分析了性能与成本之间的权衡、推理过程的语言一致性以及模型展现的战略适应性。研究发现，启用推理（即，增加测试时的计算量）显著提高了谈判结果，通过增强合作性帮助模型克服任务复杂性，但带来了巨大的计算成本：推理使GPT-5的性能提高了31.4%，而成本增加了近400%。最关键的是，我们揭示了一个重要的多语言推理差异：开源模型在其内部推理步骤中始终切换到英语，即使是在用德语或意大利语进行谈判时（因此可能影响通过揭示推理痕迹获得的潜在解释性增益），而领先的商业模型则在推理过程和最终输出之间保持语言一致性。 

---
# Everything is Plausible: Investigating the Impact of LLM Rationales on Human Notions of Plausibility 

**Title (ZH)**: 一切皆有可能：探究大语言模型解释对人类可信度感知的影响 

**Authors**: Shramay Palta, Peter Rankel, Sarah Wiegreffe, Rachel Rudinger  

**Link**: [PDF](https://arxiv.org/pdf/2510.08091)  

**Abstract**: We investigate the degree to which human plausibility judgments of multiple-choice commonsense benchmark answers are subject to influence by (im)plausibility arguments for or against an answer, in particular, using rationales generated by LLMs. We collect 3,000 plausibility judgments from humans and another 13,600 judgments from LLMs. Overall, we observe increases and decreases in mean human plausibility ratings in the presence of LLM-generated PRO and CON rationales, respectively, suggesting that, on the whole, human judges find these rationales convincing. Experiments with LLMs reveal similar patterns of influence. Our findings demonstrate a novel use of LLMs for studying aspects of human cognition, while also raising practical concerns that, even in domains where humans are ``experts'' (i.e., common sense), LLMs have the potential to exert considerable influence on people's beliefs. 

**Abstract (ZH)**: 我们调查了人类对多项选择常识基准答案的可信度判断受(im)可信度论据（支持或反对某个答案）的影响程度，特别是在LLM生成的论据情况下。我们收集了3,000个人类的可信度判断和另外13,600个LLM的判断。总体而言，我们观察到，在LLM生成的支持（PRO）和反对（CON）论据存在的情况下，人类平均可信度评分均有增加和减少的趋势，表明人类评估者整体上发现这些论据是有说服力的。针对LLM的实验揭示了类似的影响力模式。我们的发现展示了使用LLM研究人类认知方面的新用途，同时也引发了实际问题，即使在人类被认为是“专家”（如常识）的领域中，LLM也可能对人们的信念产生显著影响。 

---
# A Novel Ensemble Learning Approach for Enhanced IoT Attack Detection: Redefining Security Paradigms in Connected Systems 

**Title (ZH)**: 一种增强物联网攻击检测的新型集成学习方法：重新定义连接系统中的安全范式 

**Authors**: Hikmat A. M. Abdeljaber, Md. Alamgir Hossain, Sultan Ahmad, Ahmed Alsanad, Md Alimul Haque, Sudan Jha, Jabeen Nazeer  

**Link**: [PDF](https://arxiv.org/pdf/2510.08084)  

**Abstract**: The rapid expansion of Internet of Things (IoT) devices has transformed industries and daily life by enabling widespread connectivity and data exchange. However, this increased interconnection has introduced serious security vulnerabilities, making IoT systems more exposed to sophisticated cyber attacks. This study presents a novel ensemble learning architecture designed to improve IoT attack detection. The proposed approach applies advanced machine learning techniques, specifically the Extra Trees Classifier, along with thorough preprocessing and hyperparameter optimization. It is evaluated on several benchmark datasets including CICIoT2023, IoTID20, BotNeTIoT L01, ToN IoT, N BaIoT, and BoT IoT. The results show excellent performance, achieving high recall, accuracy, and precision with very low error rates. These outcomes demonstrate the model efficiency and superiority compared to existing approaches, providing an effective and scalable method for securing IoT environments. This research establishes a solid foundation for future progress in protecting connected devices from evolving cyber threats. 

**Abstract (ZH)**: 物联网设备的迅速扩张通过实现广泛的连接和数据交换改变了各行各业和日常生活。然而，这种增加的互联性引入了严重的安全漏洞，使物联网系统更容易受到复杂的网络攻击。本研究提出了一种新颖的集成学习架构，旨在提高物联网攻击检测效果。提出的方案应用了先进的机器学习技术，特别是Extra Trees Classifier，并结合了彻底的预处理和超参数优化。该方法在包括CICIoT2023、IoTID20、BotNeTIoT L01、ToN IoT、N BaIoT和BoT IoT等多个基准数据集上进行了评估。结果表明，该模型具有出色的性能，实现了高召回率、准确率和精确率，并且错误率极低。这些结果展示了该模型相较于现有方法的高效性和优越性，为保护物联网环境提供了一种有效且可扩展的方法。本研究为未来防范不断演变的网络威胁保护连接设备奠定了坚实的基础。 

---
# An Adaptive Multi Agent Bitcoin Trading System 

**Title (ZH)**: 自适应多代理比特币交易系统 

**Authors**: Aadi Singhi  

**Link**: [PDF](https://arxiv.org/pdf/2510.08068)  

**Abstract**: This paper presents a Multi Agent Bitcoin Trading system that utilizes Large Lan- guage Models (LLMs) for alpha generation and portfolio management in the cryptocur- rencies market. Unlike equities, cryptocurrencies exhibit extreme volatility and are heavily influenced by rapidly shifting market sentiments and regulatory announcements, making them difficult to model using static regression models or neural networks trained solely on historical data [53]. The proposed framework overcomes this by structuring LLMs into specialised agents for technical analysis, sentiment evaluation, decision-making, and performance reflection. The system improves over time through a novel verbal feedback mechanism where a Reflect agent provides daily and weekly natural-language critiques of trading decisions. These textual evaluations are then injected into future prompts, al- lowing the system to adjust indicator priorities, sentiment weights, and allocation logic without parameter updates or finetuning. Back-testing on Bitcoin price data from July 2024 to April 2025 shows consistent outperformance across market regimes: the Quantita- tive agent delivered over 30% higher returns in bullish phases and 15% overall gains versus buy-and-hold, while the sentiment-driven agent turned sideways markets from a small loss into a gain of over 100%. Adding weekly feedback further improved total performance by 31% and reduced bearish losses by 10%. The results demonstrate that verbal feedback represents a new, scalable, and low-cost method of tuning LLMs for financial goals. 

**Abstract (ZH)**: 基于大型语言模型的多agent比特币交易系统：技术分析、情绪评估与绩效反馈 

---
# Attribution-by-design: Ensuring Inference-Time Provenance in Generative Music Systems 

**Title (ZH)**: 设计归因：确保生成音乐系统推理时间可追溯性 

**Authors**: Fabio Morreale, Wiebke Hutiri, Joan Serrà, Alice Xiang, Yuki Mitsufuji  

**Link**: [PDF](https://arxiv.org/pdf/2510.08062)  

**Abstract**: The rise of AI-generated music is diluting royalty pools and revealing structural flaws in existing remuneration frameworks, challenging the well-established artist compensation systems in the music industry. Existing compensation solutions, such as piecemeal licensing agreements, lack scalability and technical rigour, while current data attribution mechanisms provide only uncertain estimates and are rarely implemented in practice. This paper introduces a framework for a generative music infrastructure centred on direct attribution, transparent royalty distribution, and granular control for artists and rights' holders. We distinguish ontologically between the training set and the inference set, which allows us to propose two complementary forms of attribution: training-time attribution and inference-time attribution. We here favour inference-time attribution, as it enables direct, verifiable compensation whenever an artist's catalogue is used to condition a generated output. Besides, users benefit from the ability to condition generations on specific songs and receive transparent information about attribution and permitted usage. Our approach offers an ethical and practical solution to the pressing need for robust compensation mechanisms in the era of AI-generated music, ensuring that provenance and fairness are embedded at the core of generative systems. 

**Abstract (ZH)**: AI生成音乐的兴起正在稀释版权池并揭示现有回报框架的结构性缺陷，挑战音乐行业已建立的艺术家补偿体系。现有的补偿解决方案，如零散的许可协议，缺乏扩展性和技术严谨性，而当前的数据归属机制只能提供不确定的估计且很少实际实施。本文提出了一种以直接归属、透明版权分配和艺术家及权利持有者的细粒度控制为中心的生成音乐基础设施框架。我们从本体论上区分训练集和推理集，这使我们能够提出两种互补形式的归属：训练时归属和推理时归属。我们更倾向于推理时归属，因为它可以在使用艺术家目录来条件生成输出时提供直接且可验证的补偿。此外，用户可以从有条件生成特定歌曲的能力中受益，并获得有关归属和许可使用情况的透明信息。我们的方法为AI生成音乐时代迫切需要的稳健补偿机制提供了一种伦理和实用的解决方案，确保来源和公平性在生成系统的核心得到体现。 

---
# FedDTRE: Federated Dialogue Generation Models Powered by Trustworthiness Evaluation 

**Title (ZH)**: FedDTRE：基于可靠性评估的联邦对话生成模型 

**Authors**: Shule Lu, Lingxiang Wang, Sijia Wen, Ziwei Wang, Hainan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08058)  

**Abstract**: With the rapid development of artificial intelligence, dialogue systems have become a prominent form of human-computer interaction. However, traditional centralized or fully local training approaches face challenges in balancing privacy preservation and personalization due to data privacy concerns and heterogeneous device capabilities. Federated learning, as a representative distributed paradigm, offers a promising solution. However, existing methods often suffer from overfitting under limited client data and tend to forget global information after multiple training rounds, leading to poor generalization. To address these issues, we propose FedDTRE, a Federated adaptive aggregation strategy for Dialogue generation based on Trustworthiness Evaluation. Instead of directly replacing local models with the global model, FedDTRE leverages trustworthiness scores of both global and local models on a fairness-oriented evaluation dataset to dynamically regulate the global model's contribution during local updates. Experimental results demonstrate that FedDTRE can improve dialogue model performance and enhance the quality of dialogue generation. 

**Abstract (ZH)**: 基于可信度评估的联邦自适应聚合策略FedDTRE：对话生成中的联邦学习 

---
# A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models 

**Title (ZH)**: 大型语言模型从结果信号到过程监督的进程奖励模型综述 

**Authors**: Congming Zheng, Jiachen Zhu, Zhuoying Ou, Yuxiang Chen, Kangning Zhang, Rong Shan, Zeyu Zheng, Mengyue Yang, Jianghao Lin, Yong Yu, Weinan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08049)  

**Abstract**: Although Large Language Models (LLMs) exhibit advanced reasoning ability, conventional alignment remains largely dominated by outcome reward models (ORMs) that judge only final answers. Process Reward Models(PRMs) address this gap by evaluating and guiding reasoning at the step or trajectory level. This survey provides a systematic overview of PRMs through the full loop: how to generate process data, build PRMs, and use PRMs for test-time scaling and reinforcement learning. We summarize applications across math, code, text, multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our goal is to clarify design spaces, reveal open challenges, and guide future research toward fine-grained, robust reasoning alignment. 

**Abstract (ZH)**: 虽然大型语言模型（LLM）展现出了先进的推理能力，传统的对齐方法仍主要依赖于仅评估最终答案的结果奖励模型（ORMs）。过程奖励模型（PRMs）通过在步骤或轨迹层面进行评估和指导，弥补了这一不足。本文综述了PRMs的全生命周期：如何生成过程数据、构建PRMs以及在测试时缩放和强化学习中使用PRMs。我们总结了PRMs在数学、代码、文本、多模态推理、机器人和代理等领域的应用，并回顾了新兴基准。我们的目标是明晰设计空间、揭示开放挑战，并引导未来研究向细粒度和鲁棒的推理对齐方向发展。 

---
# TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for E-commerce Search Relevance 

**Title (ZH)**: TaoSR-AGRL：适应性引导强化学习框架在电子商务搜索相关性中的应用 

**Authors**: Jianhui Yang, Yiming Jin, Pengkun Jiao, Chenhe Dong, Zerui Huang, Shaowei Yao, Xiaojiang Zhou, Dan Ou, Haihong Tang  

**Link**: [PDF](https://arxiv.org/pdf/2510.08048)  

**Abstract**: Query-product relevance prediction is fundamental to e-commerce search and has become even more critical in the era of AI-powered shopping, where semantic understanding and complex reasoning directly shape the user experience and business conversion. Large Language Models (LLMs) enable generative, reasoning-based approaches, typically aligned via supervised fine-tuning (SFT) or preference optimization methods like Direct Preference Optimization (DPO). However, the increasing complexity of business rules and user queries exposes the inability of existing methods to endow models with robust reasoning capacity for long-tail and challenging cases. Efforts to address this via reinforcement learning strategies like Group Relative Policy Optimization (GRPO) often suffer from sparse terminal rewards, offering insufficient guidance for multi-step reasoning and slowing convergence. To address these challenges, we propose TaoSR-AGRL, an Adaptive Guided Reinforcement Learning framework for LLM-based relevance prediction in Taobao Search Relevance. TaoSR-AGRL introduces two key innovations: (1) Rule-aware Reward Shaping, which decomposes the final relevance judgment into dense, structured rewards aligned with domain-specific relevance criteria; and (2) Adaptive Guided Replay, which identifies low-accuracy rollouts during training and injects targeted ground-truth guidance to steer the policy away from stagnant, rule-violating reasoning patterns toward compliant trajectories. TaoSR-AGRL was evaluated on large-scale real-world datasets and through online side-by-side human evaluations on Taobao Search. It consistently outperforms DPO and standard GRPO baselines in offline experiments, improving relevance accuracy, rule adherence, and training stability. The model trained with TaoSR-AGRL has been successfully deployed in the main search scenario on Taobao, serving hundreds of millions of users. 

**Abstract (ZH)**: 基于Large Language Models的淘宝搜索相关性预测：一种适应性引导强化学习框架 

---
# Verifying Graph Neural Networks with Readout is Intractable 

**Title (ZH)**: 验证带有读出的图神经网络是不可行的 

**Authors**: Artem Chernobrovkin, Marco Sälzer, François Schwarzentruber, Nicolas Troquard  

**Link**: [PDF](https://arxiv.org/pdf/2510.08045)  

**Abstract**: We introduce a logical language for reasoning about quantized aggregate-combine graph neural networks with global readout (ACR-GNNs). We provide a logical characterization and use it to prove that verification tasks for quantized GNNs with readout are (co)NEXPTIME-complete. This result implies that the verification of quantized GNNs is computationally intractable, prompting substantial research efforts toward ensuring the safety of GNN-based systems. We also experimentally demonstrate that quantized ACR-GNN models are lightweight while maintaining good accuracy and generalization capabilities with respect to non-quantized models. 

**Abstract (ZH)**: 我们介绍了一种逻辑语言用于推理量化聚合结合图神经网络（ACR-GNN）及其全局读出操作。我们提供了逻辑特征化，并利用其证明了具有读出操作的量化图神经网络的验证任务是(co)NEXPTIME完全的。这一结果表明，验证量化图神经网络在计算上是不可行的，从而促使了对基于图神经网络系统的安全性保证的大量研究工作。我们还实验证明了量化ACR-GNN模型具有轻量级结构，同时在非量化模型方面保持了良好的准确性和泛化能力。 

---
# Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation 

**Title (ZH)**: 基于联合不确定性估计的可靠LLM驱动机器人规划研究 

**Authors**: Shiyuan Yin, Chenjia Bai, Zihao Zhang, Junwei Jin, Xinxin Zhang, Chi Zhang, Xuelong Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.08044)  

**Abstract**: Large language models (LLMs) demonstrate advanced reasoning abilities, enabling robots to understand natural language instructions and generate high-level plans with appropriate grounding. However, LLM hallucinations present a significant challenge, often leading to overconfident yet potentially misaligned or unsafe plans. While researchers have explored uncertainty estimation to improve the reliability of LLM-based planning, existing studies have not sufficiently differentiated between epistemic and intrinsic uncertainty, limiting the effectiveness of uncertainty esti- mation. In this paper, we present Combined Uncertainty estimation for Reliable Embodied planning (CURE), which decomposes the uncertainty into epistemic and intrinsic uncertainty, each estimated separately. Furthermore, epistemic uncertainty is subdivided into task clarity and task familiarity for more accurate evaluation. The overall uncertainty assessments are obtained using random network distillation and multi-layer perceptron regression heads driven by LLM features. We validated our approach in two distinct experimental settings: kitchen manipulation and tabletop rearrangement experiments. The results show that, compared to existing methods, our approach yields uncertainty estimates that are more closely aligned with the actual execution outcomes. 

**Abstract (ZH)**: 结合先验与内在不确定性估计以实现可靠 embodied 计划 (CURE) 

---
# MRI-derived quantification of hepatic vessel-to-volume ratios in chronic liver disease using a deep learning approach 

**Title (ZH)**: 使用深度学习方法的慢性肝病中肝血管与体积比的MRI定量分析 

**Authors**: Alexander Herold, Daniel Sobotka, Lucian Beer, Nina Bastati, Sarah Poetter-Lang, Michael Weber, Thomas Reiberger, Mattias Mandorfer, Georg Semmler, Benedikt Simbrunner, Barbara D. Wichtmann, Sami A. Ba-Ssalamah, Michael Trauner, Ahmed Ba-Ssalamah, Georg Langs  

**Link**: [PDF](https://arxiv.org/pdf/2510.08039)  

**Abstract**: Background: We aimed to quantify hepatic vessel volumes across chronic liver disease stages and healthy controls using deep learning-based magnetic resonance imaging (MRI) analysis, and assess correlations with biomarkers for liver (dys)function and fibrosis/portal hypertension.
Methods: We assessed retrospectively healthy controls, non-advanced and advanced chronic liver disease (ACLD) patients using a 3D U-Net model for hepatic vessel segmentation on portal venous phase gadoxetic acid-enhanced 3-T MRI. Total (TVVR), hepatic (HVVR), and intrahepatic portal vein-to-volume ratios (PVVR) were compared between groups and correlated with: albumin-bilirubin (ALBI) and model for end-stage liver disease-sodium (MELD-Na) score, and fibrosis/portal hypertension (Fibrosis-4 [FIB-4] score, liver stiffness measurement [LSM], hepatic venous pressure gradient [HVPG], platelet count [PLT], and spleen volume).
Results: We included 197 subjects, aged 54.9 $\pm$ 13.8 years (mean $\pm$ standard deviation), 111 males (56.3\%): 35 healthy controls, 44 non-ACLD, and 118 ACLD patients. TVVR and HVVR were highest in controls (3.9; 2.1), intermediate in non-ACLD (2.8; 1.7), and lowest in ACLD patients (2.3; 1.0) ($p \leq 0.001$). PVVR was reduced in both non-ACLD and ACLD patients (both 1.2) compared to controls (1.7) ($p \leq 0.001$), but showed no difference between CLD groups ($p = 0.999$). HVVR significantly correlated indirectly with FIB-4, ALBI, MELD-Na, LSM, and spleen volume ($\rho$ ranging from -0.27 to -0.40), and directly with PLT ($\rho = 0.36$). TVVR and PVVR showed similar but weaker correlations.
Conclusions: Deep learning-based hepatic vessel volumetry demonstrated differences between healthy liver and chronic liver disease stages and shows correlations with established markers of disease severity. 

**Abstract (ZH)**: 背景:我们旨在使用基于深度学习的磁共振成像(MRI)分析定量慢性肝病各阶段和健康对照的肝血管体积，并评估其与肝功能(失)常和纤维化/门脉高压生物标志物的相关性。方法:我们使用3D U-Net模型对门静脉期gadoxetic酸增强的3-T MRI进行肝血管分割，回顾性评估健康对照、非晚期和晚期慢性肝病(ACLD)患者。比较各组的总体积比(TVVR)、肝体积比(HVVR)和肝内门静脉体积比(PVVR)，并与白蛋白-胆红素(ALBI)评分和终末期肝病-钠(MELD-Na)评分，以及纤维化/门脉高压(Fibrosis-4 [FIB-4]评分、肝硬度测量[LSM]、肝静脉压力梯度[HVPG]、血小板计数[PLT]和脾脏体积)进行相关性分析。结果:共纳入197名受试者，年龄54.9岁 $\pm$ 13.8岁，男性111名(56.3%)：35名健康对照、44名非ACLD和118名ACLD患者。在健康对照中TVVR和HVVR最高(3.9；2.1)，非ACLD中居中(2.8；1.7)，ACLD患者中最低(2.3；1.0)($p \leq 0.001$)。PVVR在非ACLD和ACLD患者中均低于健康对照(1.7)($p \leq 0.001$)，但在不同CLD组之间无显著差异($p = 0.999$)。HVVR与FIB-4、ALBI、MELD-Na、LSM和脾脏体积呈间接相关($\rho$从-0.27到-0.40)，与PLT呈直接相关($\rho = 0.36$)。TVVR和PVVR的相关性相似但较弱。结论:基于深度学习的肝脏血管容积测量显示健康肝脏与慢性肝病各阶段之间的差异，并与疾病严重性的已建立标志物显示出相关性。 

---
# FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset 

**Title (ZH)**: FastUMI-100K：以大规模UMI风格数据集促进数据驱动的机器人 manipulation 

**Authors**: Kehui Liu, Zhongjie Jia, Yang Li, Zhaxizhuoma, Pengan Chen, Song Liu, Xin Liu, Pingrui Zhang, Haoming Song, Xinyi Ye, Nieqing Cao, Zhigang Wang, Jia Zeng, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.08022)  

**Abstract**: Data-driven robotic manipulation learning depends on large-scale, high-quality expert demonstration datasets. However, existing datasets, which primarily rely on human teleoperated robot collection, are limited in terms of scalability, trajectory smoothness, and applicability across different robotic embodiments in real-world environments. In this paper, we present FastUMI-100K, a large-scale UMI-style multimodal demonstration dataset, designed to overcome these limitations and meet the growing complexity of real-world manipulation tasks. Collected by FastUMI, a novel robotic system featuring a modular, hardware-decoupled mechanical design and an integrated lightweight tracking system, FastUMI-100K offers a more scalable, flexible, and adaptable solution to fulfill the diverse requirements of real-world robot demonstration data. Specifically, FastUMI-100K contains over 100K+ demonstration trajectories collected across representative household environments, covering 54 tasks and hundreds of object types. Our dataset integrates multimodal streams, including end-effector states, multi-view wrist-mounted fisheye images and textual annotations. Each trajectory has a length ranging from 120 to 500 frames. Experimental results demonstrate that FastUMI-100K enables high policy success rates across various baseline algorithms, confirming its robustness, adaptability, and real-world applicability for solving complex, dynamic manipulation challenges. The source code and dataset will be released in this link this https URL. 

**Abstract (ZH)**: 基于数据驱动的机器人操作学习依赖于大规模、高质量的专家示范数据集。然而，现有的数据集主要依赖于人工远程操控机器人采集，这些数据集在可扩展性、轨迹平滑性和适用于不同机器人实体方面的现实环境中有局限性。本文介绍了FastUMI-100K，一个大规模的UMI风格多模态示范数据集，旨在克服这些局限性，以应对实际操作任务日益复杂的需求。FastUMI-100K通过FastUMI采集，这是一种具有模块化、硬件解耦机械设计和集成轻量级跟踪系统的新型机器人系统，提供了更具扩展性、灵活性和适应性的解决方案，以满足现实世界机器人示范数据的多样需求。FastUMI-100K包含了超过100K个示范轨迹，这些轨迹覆盖了代表性的家庭环境，包括54项任务和数百种对象类型。数据集整合了多模态流，包括末端执行器状态、多视角手部安装的鱼眼图像和文本注释。每条轨迹包含的帧数从120到500不等。实验结果表明，FastUMI-100K使不同基线算法在多项政策成功率上显著提高，验证了其鲁棒性、适应性和实际应用价值，适用于解决复杂的动态操作挑战。源代码和数据集将在此链接发布：this https URL。 

---
# Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses 

**Title (ZH)**: 后门向量：后门攻击与防御的任务算术视角 

**Authors**: Stanisław Pawlak, Jan Dubiński, Daniel Marczak, Bartłomiej Twardowski  

**Link**: [PDF](https://arxiv.org/pdf/2510.08016)  

**Abstract**: Model merging (MM) recently emerged as an effective method for combining large deep learning models. However, it poses significant security risks. Recent research shows that it is highly susceptible to backdoor attacks, which introduce a hidden trigger into a single fine-tuned model instance that allows the adversary to control the output of the final merged model at inference time. In this work, we propose a simple framework for understanding backdoor attacks by treating the attack itself as a task vector. $Backdoor\ Vector\ (BV)$ is calculated as the difference between the weights of a fine-tuned backdoored model and fine-tuned clean model. BVs reveal new insights into attacks understanding and a more effective framework to measure their similarity and transferability. Furthermore, we propose a novel method that enhances backdoor resilience through merging dubbed $Sparse\ Backdoor\ Vector\ (SBV)$ that combines multiple attacks into a single one. We identify the core vulnerability behind backdoor threats in MM: $inherent\ triggers$ that exploit adversarial weaknesses in the base model. To counter this, we propose $Injection\ BV\ Subtraction\ (IBVS)$ - an assumption-free defense against backdoors in MM. Our results show that SBVs surpass prior attacks and is the first method to leverage merging to improve backdoor effectiveness. At the same time, IBVS provides a lightweight, general defense that remains effective even when the backdoor threat is entirely unknown. 

**Abstract (ZH)**: Model Merging中的后门攻击理解与防御：Sparse Backdoor Vector与Injection BV Subtraction 

---
# Past, Present, and Future of Bug Tracking in the Generative AI Era 

**Title (ZH)**: 生成式AI时代 bug 跟踪的过去、现在与未来 

**Authors**: Utku Boran Torun, Mehmet Taha Demircan, Mahmut Furkan Gön, Eray Tüzün  

**Link**: [PDF](https://arxiv.org/pdf/2510.08005)  

**Abstract**: Traditional bug tracking systems rely heavily on manual reporting, reproduction, triaging, and resolution, each carried out by different stakeholders such as end users, customer support, developers, and testers. This division of responsibilities requires significant coordination and widens the communication gap between non-technical users and technical teams, slowing the process from bug discovery to resolution. Moreover, current systems are highly asynchronous; users often wait hours or days for a first response, delaying fixes and contributing to frustration. This paper examines the evolution of bug tracking, from early paper-based reporting to today's web-based and SaaS platforms. Building on this trajectory, we propose an AI-powered bug tracking framework that augments existing tools with intelligent, large language model (LLM)-driven automation. Our framework addresses two main challenges: reducing time-to-fix and minimizing human overhead. Users report issues in natural language, while AI agents refine reports, attempt reproduction, and request missing details. Reports are then classified, invalid ones resolved through no-code fixes, and valid ones localized and assigned to developers. LLMs also generate candidate patches, with human oversight ensuring correctness. By integrating automation into each phase, our framework accelerates response times, improves collaboration, and strengthens software maintenance practices for a more efficient, user-centric future. 

**Abstract (ZH)**: 传统缺陷跟踪系统高度依赖于不同利益相关者如最终用户、客户支持、开发人员和测试人员的手动报告、重现、优先级排序和解决。这种责任分工要求大量的协调，并且扩大了非技术人员用户与技术团队之间的沟通差距，延长了从缺陷发现到解决的过程。此外，当前的系统高度异步，用户经常需要等待数小时或数天才能收到初步响应，这延缓了修复进程并增加了用户挫败感。本文追溯了缺陷跟踪的演变，从早期的手写报告到当今的网页和基于SaaS平台。在此基础上，我们提出了一种基于AI的缺陷跟踪框架，该框架借助智能化的大语言模型（LLM）驱动的自动化来增强现有工具。该框架主要解决两个挑战：减少修复时间并减少人力成本。用户以自然语言报告问题，AI代理精炼报告、尝试重现并请求缺少的详细信息。然后对报告进行分类，无效的报告通过无代码修复解决，有效的报告则定位并指派给开发人员。LLM还生成候选补丁，通过人工监督确保其正确性。通过在每个阶段集成自动化，该框架加速了响应时间、改善了协作并强化了软件维护实践，从而实现更高效、用户为中心的未来。 

---
# Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks 

**Title (ZH)**: 在职学习：一种经验驱动的自我演化代理用于长期任务 

**Authors**: Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, Yu Qiao, Haifeng Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.08002)  

**Abstract**: Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation. 

**Abstract (ZH)**: 大型语言模型在多个领域展现了remarkable的能力，但在将其部署为解决长期现实世界任务的AI代理时，仍存在重大挑战。现有LLM代理面临一个关键限制：它们在测试时是静态的，无法从经验中学习，缺乏积累知识和持续改进的能力。为应对这一挑战，我们提出MUSE，一种新颖的代理框架，引入了一个以分层记忆模块为中心的基于经验自我演化的系统。MUSE组织多种经验层次，并利用它们在多个应用中规划和执行长期任务。每当子任务执行完毕，代理会自主反思其轨迹，将原始轨迹转化为结构化经验，并将其重新整合回记忆模块。这一机制使代理能够超越其静态预训练参数，促进持续学习和自我演化。我们在长期任务生产力基准TAC上评估了MUSE，仅使用轻量级的Gemini-2.5 Flash模型，就取得了显著的SOTA性能。充分的实验表明，随着代理自主积累经验，其任务完成能力显著增强，表现出强大的持续学习和自我演化能力。此外，MUSE积累的经验具有较强的泛化能力，能够在新任务中实现零样本改进。MUSE为能够实现现实世界生产力任务自动化的AI代理建立了新范式。 

---
# Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge 

**Title (ZH)**: 利用作者特定上下文进行科学图表标题生成：第3届SciCap挑战赛 

**Authors**: Watcharapong Timklaypachara, Monrada Chiewhawan, Nopporn Lekuthai, Titipat Achakulvisut  

**Link**: [PDF](https://arxiv.org/pdf/2510.07993)  

**Abstract**: Scientific figure captions require both accuracy and stylistic consistency to convey visual information. Here, we present a domain-specific caption generation system for the 3rd SciCap Challenge that integrates figure-related textual context with author-specific writing styles using the LaMP-Cap dataset. Our approach uses a two-stage pipeline: Stage 1 combines context filtering, category-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption candidate selection; Stage 2 applies few-shot prompting with profile figures for stylistic refinement. Our experiments demonstrate that category-specific prompts outperform both zero-shot and general optimized approaches, improving ROUGE-1 recall by +8.3\% while limiting precision loss to -2.8\% and BLEU-4 reduction to -10.9\%. Profile-informed stylistic refinement yields 40--48\% gains in BLEU scores and 25--27\% in ROUGE. Overall, our system demonstrates that combining contextual understanding with author-specific stylistic adaptation can generate captions that are both scientifically accurate and stylistically faithful to the source paper. 

**Abstract (ZH)**: 科学图表说明文字需要兼具准确性和风格一致性以传达视觉信息。本文介绍了针对第3届SciCap挑战赛的领域特定说明文字生成系统，该系统结合了图表相关文本上下文和作者特定的写作风格，使用了LaMP-Cap数据集。我们的方法采用两阶段管道：第一阶段结合上下文过滤、通过DSPy的MIPROv2和SIMBA进行类别特定提示优化，并选择说明文字候选；第二阶段使用实例提示进行风格 refinement。实验结果表明，类别特定提示在ROUGE-1召回率上提高了8.3%，同时将精确率损失控制在2.8%，BLEU-4减少控制在10.9%。基于个人风格的风格 refinement 将BLEU分数提高了40-48%，ROUGE分数提高了25-27%。总体而言，我们的系统证明了结合上下文理解与作者特定的风格适应可以生成既科学准确又忠实于原始论文风格的说明文字。 

---
# Fewer Weights, More Problems: A Practical Attack on LLM Pruning 

**Title (ZH)**: 较少权重，更多问题：对LLM剪枝的一种实用攻击 

**Authors**: Kazuki Egashira, Robin Staab, Thibaud Gloaguen, Mark Vero, Martin Vechev  

**Link**: [PDF](https://arxiv.org/pdf/2510.07985)  

**Abstract**: Model pruning, i.e., removing a subset of model weights, has become a prominent approach to reducing the memory footprint of large language models (LLMs) during inference. Notably, popular inference engines, such as vLLM, enable users to conveniently prune downloaded models before they are deployed. While the utility and efficiency of pruning methods have improved significantly, the security implications of pruning remain underexplored. In this work, for the first time, we show that modern LLM pruning methods can be maliciously exploited. In particular, an adversary can construct a model that appears benign yet, once pruned, exhibits malicious behaviors. Our method is based on the idea that the adversary can compute a proxy metric that estimates how likely each parameter is to be pruned. With this information, the adversary can first inject a malicious behavior into those parameters that are unlikely to be pruned. Then, they can repair the model by using parameters that are likely to be pruned, effectively canceling out the injected behavior in the unpruned model. We demonstrate the severity of our attack through extensive evaluation on five models; after any of the pruning in vLLM are applied (Magnitude, Wanda, and SparseGPT), it consistently exhibits strong malicious behaviors in a diverse set of attack scenarios (success rates of up to $95.7\%$ for jailbreak, $98.7\%$ for benign instruction refusal, and $99.5\%$ for targeted content injection). Our results reveal a critical deployment-time security gap and underscore the urgent need for stronger security awareness in model compression. 

**Abstract (ZH)**: 现代大语言模型剪枝方法的恶意利用：一种新的安全威胁 

---
# Is Architectural Complexity Always the Answer? A Case Study on SwinIR vs. an Efficient CNN 

**Title (ZH)**: 建筑复杂性always是解决方案吗？SwinIR与高效CNN的案例研究 

**Authors**: Chandresh Sutariya, Nitin Singh  

**Link**: [PDF](https://arxiv.org/pdf/2510.07984)  

**Abstract**: The simultaneous restoration of high-frequency details and suppression of severe noise in low-light imagery presents a significant and persistent challenge in computer vision. While large-scale Transformer models like SwinIR have set the state of the art in performance, their high computational cost can be a barrier for practical applications. This paper investigates the critical trade-off between performance and efficiency by comparing the state-of-the-art SwinIR model against a standard, lightweight Convolutional Neural Network (CNN) on this challenging task. Our experimental results reveal a nuanced but important finding. While the Transformer-based SwinIR model achieves a higher peak performance, with a Peak Signal-to-Noise Ratio (PSNR) of 39.03 dB, the lightweight CNN delivers a surprisingly competitive PSNR of 37.4 dB. Crucially, the CNN reached this performance after converging in only 10 epochs of training, whereas the more complex SwinIR model required 132 epochs. This efficiency is further underscored by the model's size; the CNN is over 55 times smaller than SwinIR. This work demonstrates that a standard CNN can provide a near state-of-the-art result with significantly lower computational overhead, presenting a compelling case for its use in real-world scenarios where resource constraints are a primary concern. 

**Abstract (ZH)**: 低光照图像中高频细节的恢复和严重噪声抑制的同时实现是计算机视觉中一个持续存在的重大挑战。尽管基于Transformer的大规模模型如SwinIR在性能上达到最新水平，但其高昂的计算成本可能成为实际应用的障碍。本文通过将最新的SwinIR模型与标准的轻量级卷积神经网络（CNN）在这一挑战性任务上进行比较，探讨了性能与效率之间的关键权衡。实验结果显示，虽然基于Transformer的SwinIR模型在峰值信噪比（PSNR）为39.03 dB的情况下实现了较高的峰值性能，但轻量级CNN的PSNR同样达到了令人惊讶的37.4 dB。更重要的是，CNN仅在10个训练周期后就达到了这一性能，而更为复杂的SwinIR模型则需要132个训练周期。此外，模型的大小差异进一步凸显了CNN的优势；CNN比SwinIR小约55倍。这项工作展示了标准CNN可以在显著降低计算开销的情况下提供接近最新水平的结果，为资源限制为主要关注点的真实世界应用场景提供了有力的使用案例。 

---
# ZeroCard: Cardinality Estimation with Zero Dependence on Target Databases -- No Data, No Query, No Retraining 

**Title (ZH)**: ZeroCard: 不依赖目标数据库基数估计——无数据，无查询，无需重新训练 

**Authors**: Xianghong Xu, Rong Kang, Xiao He, Lei Zhang, Jianjun Chen, Tieying Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07983)  

**Abstract**: Cardinality estimation is a fundamental task in database systems and plays a critical role in query optimization. Despite significant advances in learning-based cardinality estimation methods, most existing approaches remain difficult to generalize to new datasets due to their strong dependence on raw data or queries, thus limiting their practicality in real scenarios. To overcome these challenges, we argue that semantics in the schema may benefit cardinality estimation, and leveraging such semantics may alleviate these dependencies. To this end, we introduce ZeroCard, the first semantics-driven cardinality estimation method that can be applied without any dependence on raw data access, query logs, or retraining on the target database. Specifically, we propose to predict data distributions using schema semantics, thereby avoiding raw data dependence. Then, we introduce a query template-agnostic representation method to alleviate query dependence. Finally, we construct a large-scale query dataset derived from real-world tables and pretrain ZeroCard on it, enabling it to learn cardinality from schema semantics and predicate representations. After pretraining, ZeroCard's parameters can be frozen and applied in an off-the-shelf manner. We conduct extensive experiments to demonstrate the distinct advantages of ZeroCard and show its practical applications in query optimization. Its zero-dependence property significantly facilitates deployment in real-world scenarios. 

**Abstract (ZH)**: 基于语义的基数估计：零依赖方法 

---
# Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training 

**Title (ZH)**: 揭示多层次闲聊步的重要性：基于稳定性的去中心化训练泛化分析 

**Authors**: Qinglun Li, Yingqi Liu, Miao Zhang, Xiaochun Cao, Quanjun Yin, Li Shen  

**Link**: [PDF](https://arxiv.org/pdf/2510.07980)  

**Abstract**: Decentralized training removes the centralized server, making it a communication-efficient approach that can significantly improve training efficiency, but it often suffers from degraded performance compared to centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective bridge between decentralized and centralized training, significantly reducing experiment performance gaps. However, the theoretical reasons for its effectiveness and whether this gap can be fully eliminated by MGS remain open questions. In this paper, we derive upper bounds on the generalization error and excess error of MGS using stability analysis, systematically answering these two key questions. 1). Optimization Error Reduction: MGS reduces the optimization error bound at an exponential rate, thereby exponentially tightening the generalization error bound and enabling convergence to better solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a non-negligible gap in generalization error remains compared to centralized mini-batch SGD ($\mathcal{O}(T^{\frac{c\beta}{c\beta +1}}/{n m})$ in centralized and $\mathcal{O}(T^{\frac{2c\beta}{2c\beta +2}}/{n m^{\frac{1}{2c\beta +2}}})$ in decentralized). Furthermore, we provide the first unified analysis of how factors like learning rate, data heterogeneity, node count, per-node sample size, and communication topology impact the generalization of MGS under non-convex settings without the bounded gradients assumption, filling a critical theoretical gap in decentralized training. Finally, promising experiments on CIFAR datasets support our theoretical findings. 

**Abstract (ZH)**: 去中心化训练消除了中心服务器，使其成为一种通信高效的训练方法，能够显著提高训练效率，但通常会相比中心化训练表现出降级的性能。多闲聊步长（MGS）作为一种简单有效的去中心化与中心化训练之间的桥梁，显著减少了实验性能差距。然而，其有效性的理论原因以及MGS是否能完全消除这一差距仍是悬而未决的问题。在本文中，我们通过稳定性分析推导出MGS的一般化误差和超额误差的上界，系统地回答了这两个关键问题。1) 最优化误差减少：MGS以指数级减少最优化误差边界，从而以指数级紧化一般化误差边界，并使收敛于更好的解。2) 与中心化差距：即便MGS趋向无穷大，在一般化误差上与中心化小批量SGD仍存在不可忽视的差距（在中心化设置中为$\mathcal{O}(T^{\frac{c\beta}{c\beta +1}}/{nm})$，在去中心化设置中为$\mathcal{O}(T^{\frac{2c\beta}{2c\beta +2}}/{nm^{\frac{1}{2c\beta +2}}})$）。此外，我们在非凸设置下，没有假设梯度有界的情况下，首次对影响MGS一般化的关键因素（如学习率、数据异质性、节点数目、每个节点样本大小和通信拓扑）进行统一分析，填补了去中心化训练领域的重要理论空白。最后，CIFAR数据集上的有希望的实验结果支持了我们的理论发现。 

---
# Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation 

**Title (ZH)**: 可执行分析概念作为从大规模语言模型洞察到精确操作的缺失链接 

**Authors**: Mingyang Sun, Jiude Wei, Qichen He, Donglin Wang, Cewu Lu, Jianhua Sun  

**Link**: [PDF](https://arxiv.org/pdf/2510.07975)  

**Abstract**: Enabling robots to perform precise and generalized manipulation in unstructured environments remains a fundamental challenge in embodied AI. While Vision-Language Models (VLMs) have demonstrated remarkable capabilities in semantic reasoning and task planning, a significant gap persists between their high-level understanding and the precise physical execution required for real-world manipulation. To bridge this "semantic-to-physical" gap, we introduce GRACE, a novel framework that grounds VLM-based reasoning through executable analytic concepts (EAC)-mathematically defined blueprints that encode object affordances, geometric constraints, and semantics of manipulation. Our approach integrates a structured policy scaffolding pipeline that turn natural language instructions and visual information into an instantiated EAC, from which we derive grasp poses, force directions and plan physically feasible motion trajectory for robot execution. GRACE thus provides a unified and interpretable interface between high-level instruction understanding and low-level robot control, effectively enabling precise and generalizable manipulation through semantic-physical grounding. Extensive experiments demonstrate that GRACE achieves strong zero-shot generalization across a variety of articulated objects in both simulated and real-world environments, without requiring task-specific training. 

**Abstract (ZH)**: 使机器人在非结构化环境中执行精确且通用的操作仍然是嵌体AI领域的基本挑战。尽管视觉-语言模型（VLMs）在语义推理和任务规划方面展现了卓越的能力，但它们的高层理解与现实世界操作所需的精确物理执行之间仍存在显著差距。为弥合这一“语义到物理”的差距，我们提出了GRACE，这是一种新的框架，通过执行可计算概念（EAC）进行基于VLM的推理，EAC是数学上定义的蓝图，编码了对象功能、几何约束和操作的语义。我们的方法整合了一种结构化的策略支撑流程，将自然语言指令和视觉信息转化为一个实例化的EAC，从中我们推导出抓取姿态、力向量，并规划机器人执行的物理可行运动轨迹。GRACE因此提供了一个统一且可解释的接口，将高层指令理解与低层机器人控制联系起来，有效地通过语义物理接地实现精确且通用的操作。广泛的实验表明，GRACE在模拟和真实环境中对多种 articulated 物体实现了强大的零样本泛化能力，而无需特定任务的训练。 

---
# Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning 

**Title (ZH)**: 大型语言模型中的主动混淆表达：利用世界模型提升社会推理能力 

**Authors**: Jialu Du, Guiyang Hou, Yihui Fu, Chen Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07974)  

**Abstract**: While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1's reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like "tricky" and "confused" when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agents' subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts. 

**Abstract (ZH)**: 虽然大型语言模型在数学和代码推理方面表现出色，但在社会推理任务中却表现出色不足，表现出认知困惑、逻辑不一致以及客观世界状态与主观信念状态的混淆。通过对DeepSeek-R1的推理轨迹进行详细分析，我们发现大型语言模型在处理多参与者和时间线的情景时经常遇到推理困境，倾向于输出如“棘手”和“困惑”等矛盾术语，导致错误的推理或无限循环。核心问题是它们无法区分客观现实和代理的主观信念。为了解决这一问题，我们提出了一种适应性世界模型增强的推理机制，该机制构建了一个动态文本世界模型来追踪实体状态和时间序列。该机制动态监控推理轨迹中的困惑指示符，并及时干预，提供清晰的世界状态描述，帮助模型克服认知困境。该机制模仿人类如何使用隐式世界模型来区分外部事件和内部信念。在三个社会基准上的评估显示出显著的准确性提升（例如，在Hi-ToM上提高10%），同时降低计算成本（最多减少33.8%的令牌数量），提供了一种简单有效的解决方案，用于在社会场景中部署大型语言模型。 

---
# LightReasoner: Can Small Language Models Teach Large Language Models Reasoning? 

**Title (ZH)**: LightReasoner: 小型语言模型能教大型语言模型进行推理吗？ 

**Authors**: Jingyuan Wang, Yankai Chen, Zhonghang Li, Chao Huang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07962)  

**Abstract**: Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. In this work, we explore a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latter's unique strength? We propose LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. Code is available at: this https URL 

**Abstract (ZH)**: 大规模语言模型（LLMs）在推理方面取得了显著进展，通常通过有监督的微调（SFT）实现。然而，SFT需要大量的精心制作的数据集、抽样示范和对所有标记的统一优化，尽管其中只有一小部分具有重要意义的学习价值。在本文中，我们探讨了一个反直觉的想法：较小的语言模型（SLMs）能否通过揭示反映后者独特优势的高价值推理时刻来教导更大规模的语言模型（LLMs）？我们提出了LightReasoner，这是一种新颖的框架，利用强专家模型（LLM）和弱业余模型（SLM）之间的行为差异。LightReasoner分为两个阶段：（1）采样阶段，通过专家-业余对比确定关键的推理时刻并构建监督示例，捕捉专家的优势；（2）微调阶段，使专家模型与这些提炼的示例对齐，放大其推理优势。在七个数学基准测试中，LightReasoner将准确性提高了最多28.1%，同时将时间消耗减少了90%，采样问题减少了80%，微调的标记数量减少了99%，而无需依赖真实标签。通过将较弱的SLM转化为有效的教学信号，LightReasoner提供了一种可扩展且资源高效的途径来提升LLM的推理能力。代码可用于此链接：this https URL。 

---
# A Systematic Evaluation of Self-Supervised Learning for Label-Efficient Sleep Staging with Wearable EEG 

**Title (ZH)**: 自我监督学习在可标记数据高效睡眠分期中的系统评估——基于可穿戴EEG信号 

**Authors**: Emilio Estevan, María Sierra-Torralba, Eduardo López-Larraz, Luis Montesano  

**Link**: [PDF](https://arxiv.org/pdf/2510.07960)  

**Abstract**: Wearable EEG devices have emerged as a promising alternative to polysomnography (PSG). As affordable and scalable solutions, their widespread adoption results in the collection of massive volumes of unlabeled data that cannot be analyzed by clinicians at scale. Meanwhile, the recent success of deep learning for sleep scoring has relied on large annotated datasets. Self-supervised learning (SSL) offers an opportunity to bridge this gap, leveraging unlabeled signals to address label scarcity and reduce annotation effort. In this paper, we present the first systematic evaluation of SSL for sleep staging using wearable EEG. We investigate a range of well-established SSL methods and evaluate them on two sleep databases acquired with the Ikon Sleep wearable EEG headband: BOAS, a high-quality benchmark containing PSG and wearable EEG recordings with consensus labels, and HOGAR, a large collection of home-based, self-recorded, and unlabeled recordings. Three evaluation scenarios are defined to study label efficiency, representation quality, and cross-dataset generalization. Results show that SSL consistently improves classification performance by up to 10% over supervised baselines, with gains particularly evident when labeled data is scarce. SSL achieves clinical-grade accuracy above 80% leveraging only 5% to 10% of labeled data, while the supervised approach requires twice the labels. Additionally, SSL representations prove robust to variations in population characteristics, recording environments, and signal quality. Our findings demonstrate the potential of SSL to enable label-efficient sleep staging with wearable EEG, reducing reliance on manual annotations and advancing the development of affordable sleep monitoring systems. 

**Abstract (ZH)**: 可穿戴EEG设备作为多导睡眠图(PSG)的有前途的替代方案已经 emergence 

---
# DISCO: Diversifying Sample Condensation for Efficient Model Evaluation 

**Title (ZH)**: DISCO: 多样化样本凝缩以实现高效模型评估 

**Authors**: Alexander Rubinstein, Benjamin Raible, Martin Gubri, Seong Joon Oh  

**Link**: [PDF](https://arxiv.org/pdf/2510.07959)  

**Abstract**: Evaluating modern machine learning models has become prohibitively expensive. Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model. Costly evaluation reduces inclusivity, slows the cycle of innovation, and worsens environmental impact. The typical approach follows two steps. First, select an anchor subset of data. Second, train a mapping from the accuracy on this subset to the final test result. The drawback is that anchor selection depends on clustering, which can be complex and sensitive to design choices. We argue that promoting diversity among samples is not essential; what matters is to select samples that $\textit{maximise diversity in model responses}$. Our method, $\textbf{Diversifying Sample Condensation (DISCO)}$, selects the top-k samples with the greatest model disagreements. This uses greedy, sample-wise statistics rather than global clustering. The approach is conceptually simpler. From a theoretical view, inter-model disagreement provides an information-theoretically optimal rule for such greedy selection. $\textbf{DISCO}$ shows empirical gains over prior methods, achieving state-of-the-art results in performance prediction across MMLU, Hellaswag, Winogrande, and ARC. Code is available here: this https URL. 

**Abstract (ZH)**: 评估现代机器学习模型已经变得极为昂贵。基准测试如LMMs-Eval和HELM每模型需求数千个GPU小时。高昂的评估成本降低了包容性，减缓了创新周期，并恶化了环境影响。典型方法遵循两个步骤。首先，选择一个基准子数据集。其次，训练一个从该子集上的准确率到最终测试结果的映射。缺点是基准选择依赖于聚类，这可能复杂且对设计选择敏感。我们argue促进样本多样性并非必不可少；关键在于选择能够最大化模型响应多样性的样本。我们的方法**Diversifying Sample Condensation (DISCO)**选择具有最大模型分歧的前k个样本。该方法使用基于样本的贪心统计而非全局聚类。该方法概念上更简单。从理论上看，模型间分歧提供了这种贪心选择的信息论最佳规则。**DISCO**在MMLU、Hellaswag、Winogrande和ARC的性能预测中实现了优于前方法的结果。代码可在以下链接获取：this https URL。 

---
# A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning 

**Title (ZH)**: A$^2$Search: 带有不确定性意识的问题回答与强化学习 

**Authors**: Fengji Zhang, Xinyao Niu, Chengyang Ying, Guancheng Lin, Zhongkai Hao, Zhou Fan, Chengen Huang, Jacky Keung, Bei Chen, Junyang Lin  

**Link**: [PDF](https://arxiv.org/pdf/2510.07958)  

**Abstract**: Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A$^2$Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed $\mathrm{AnsF1}$ reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A$^2$Search achieves new state-of-the-art performance. With only a single rollout, A$^2$Search-7B yields an average $\mathrm{AnsF1}@1$ score of $48.4\%$ across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B ($46.2\%$). Extensive analyses further show that A$^2$Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at this https URL 

**Abstract (ZH)**: Recent advances in大型语言模型（LLMs）和强化学习（RL）在开放领域问答（QA）任务上取得了强大表现，然而现有模型仍然难以处理允许多重正确答案的问题。标准的问答基准通常假设有且只有一个正确答案，从而忽略了现实情况，产生了一些不恰当的训练信号。现有处理歧义的方法通常依赖于昂贵的手动标注，难以扩展应用到多跳数据集如HotpotQA和MuSiQue。本文提出A$^2$Search，一种无需标注的端到端训练框架，用于识别和处理歧义。其核心是自动化的流水线，通过轨迹采样和证据验证检测到模糊的问题，并收集替代答案。然后使用精心设计的$\mathrm{AnsF1}$奖励优化模型，自然地容纳了多个答案。在八个开放领域问答基准上的实验表明，A$^2$Search实现了新的最佳性能。仅通过一次 rollout，A$^2$Search-7B 在四个多跳基准上的平均$\mathrm{AnsF1}@1$得分为$48.4\%$，超过所有强大的基线模型，包括规模大得多的ReSearch-32B（$46.2\%$）。广泛的分析还表明A$^2$Search解决歧义并在不同基准之间泛化，强调接受歧义对于构建更可靠的问答系统至关重要。我们的代码、数据和模型权重可以在以下网址找到。 

---
# A Large-scale Dataset for Robust Complex Anime Scene Text Detection 

**Title (ZH)**: 大规模数据集用于健壮的复杂动画场景文字检测 

**Authors**: Ziyi Dong, Yurui Zhang, Changmao Li, Naomi Rue Golding, Qing Long  

**Link**: [PDF](https://arxiv.org/pdf/2510.07951)  

**Abstract**: Current text detection datasets primarily target natural or document scenes, where text typically appear in regular font and shapes, monotonous colors, and orderly layouts. The text usually arranged along straight or curved lines. However, these characteristics differ significantly from anime scenes, where text is often diverse in style, irregularly arranged, and easily confused with complex visual elements such as symbols and decorative patterns. Text in anime scene also includes a large number of handwritten and stylized fonts. Motivated by this gap, we introduce AnimeText, a large-scale dataset containing 735K images and 4.2M annotated text blocks. It features hierarchical annotations and hard negative samples tailored for anime scenarios. %Cross-dataset evaluations using state-of-the-art methods demonstrate that models trained on AnimeText achieve superior performance in anime text detection tasks compared to existing datasets. To evaluate the robustness of AnimeText in complex anime scenes, we conducted cross-dataset benchmarking using state-of-the-art text detection methods. Experimental results demonstrate that models trained on AnimeText outperform those trained on existing datasets in anime scene text detection tasks. AnimeText on HuggingFace: this https URL 

**Abstract (ZH)**: 当前的文字检测数据集主要针对自然场景或文档场景，其中文字通常以常规字体和形状、单调的颜色和有序的布局出现。文字通常沿直线或曲线排列。然而，这些特征与动漫场景中的文字特性有显著差异，后者中的文字风格多样、排列不规则、容易与符号和装饰图案等复杂的视觉元素混淆。动漫场景中的文字包含大量手写和艺术化的字体。为弥合这一差距，我们引入了AnimeText数据集，包含735K张图像和4.2M个标注的文字区块。该数据集具有面向动漫场景的层次化标注和困难的负样本。跨数据集评估表明，使用AnimeText训练的模型在动漫文字检测任务上的表现优于现有数据集。为了评估AnimeText在复杂动漫场景中的鲁棒性，我们使用最先进的文字检测方法进行了跨数据集基准测试。实验结果表明，使用AnimeText训练的模型在动漫场景文字检测任务上的表现优于使用现有数据集训练的模型。AnimeText在HuggingFace：这个 https URL。 

---
# TTOM: Test-Time Optimization and Memorization for Compositional Video Generation 

**Title (ZH)**: TTOM：测试时优化和记忆化在组件视频生成中的应用 

**Authors**: Leigang Qu, Ziyang Wang, Na Zheng, Wenjie Wang, Liqiang Nie, Tat-Seng Chua  

**Link**: [PDF](https://arxiv.org/pdf/2510.07940)  

**Abstract**: Video Foundation Models (VFMs) exhibit remarkable visual generation performance, but struggle in compositional scenarios (e.g., motion, numeracy, and spatial relation). In this work, we introduce Test-Time Optimization and Memorization (TTOM), a training-free framework that aligns VFM outputs with spatiotemporal layouts during inference for better text-image alignment. Rather than direct intervention to latents or attention per-sample in existing work, we integrate and optimize new parameters guided by a general layout-attention objective. Furthermore, we formulate video generation within a streaming setting, and maintain historical optimization contexts with a parametric memory mechanism that supports flexible operations, such as insert, read, update, and delete. Notably, we found that TTOM disentangles compositional world knowledge, showing powerful transferability and generalization. Experimental results on the T2V-CompBench and Vbench benchmarks establish TTOM as an effective, practical, scalable, and efficient framework to achieve cross-modal alignment for compositional video generation on the fly. 

**Abstract (ZH)**: Test-Time Optimization and Memorization for Better Text-Video Alignment in Compositional Video Generation 

---
# STEPER: Step-wise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models 

**Title (ZH)**: STEPER：逐步知识精炼以增强多步检索增强语言模型的推理能力 

**Authors**: Kyumin Lee, Minjin Jeon, Sanghwan Jang, Hwanjo Yu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07923)  

**Abstract**: Answering complex real-world questions requires step-by-step retrieval and integration of relevant information to generate well-grounded responses. However, existing knowledge distillation methods overlook the need for different reasoning abilities at different steps, hindering transfer in multi-step retrieval-augmented frameworks. To address this, we propose Stepwise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models (StepER). StepER employs step-wise supervision to align with evolving information and reasoning demands across stages. Additionally, it incorporates difficulty-aware training to progressively optimize learning by prioritizing suitable steps. Our method is adaptable to various multi-step retrieval-augmented language models, including those that use retrieval queries for reasoning paths or decomposed questions. Extensive experiments show that StepER outperforms prior methods on multi-hop QA benchmarks, with an 8B model achieving performance comparable to a 70B teacher model. 

**Abstract (ZH)**: 逐步知识蒸馏以增强多步骤检索增强语言模型的推理能力（StepER） 

---
# Towards Human-Like Grading: A Unified LLM-Enhanced Framework for Subjective Question Evaluation 

**Title (ZH)**: 向人类评分看齐：一种统一的LLM增强框架，用于主观题目评估 

**Authors**: Fanwei Zhua, Jiaxuan He, Xiaoxiao Chen, Zulong Chen, Quan Lu, Chenrui Mei  

**Link**: [PDF](https://arxiv.org/pdf/2510.07912)  

**Abstract**: Automatic grading of subjective questions remains a significant challenge in examination assessment due to the diversity in question formats and the open-ended nature of student responses. Existing works primarily focus on a specific type of subjective question and lack the generality to support comprehensive exams that contain diverse question types. In this paper, we propose a unified Large Language Model (LLM)-enhanced auto-grading framework that provides human-like evaluation for all types of subjective questions across various domains. Our framework integrates four complementary modules to holistically evaluate student answers. In addition to a basic text matching module that provides a foundational assessment of content similarity, we leverage the powerful reasoning and generative capabilities of LLMs to: (1) compare key knowledge points extracted from both student and reference answers, (2) generate a pseudo-question from the student answer to assess its relevance to the original question, and (3) simulate human evaluation by identifying content-related and non-content strengths and weaknesses. Extensive experiments on both general-purpose and domain-specific datasets show that our framework consistently outperforms traditional and LLM-based baselines across multiple grading metrics. Moreover, the proposed system has been successfully deployed in real-world training and certification exams at a major e-commerce enterprise. 

**Abstract (ZH)**: 自动评价主观题 remains a significant challenge in examination assessment due to the diversity in question formats and the open-ended nature of student responses. Existing works primarily focus on a specific type of subjective question and lack the generality to support comprehensive exams that contain diverse question types. In this paper, we propose a unified Large Language Model (LLM)-enhanced auto-grading framework that provides human-like evaluation for all types of subjective questions across various domains. 

---
# MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation 

**Title (ZH)**: MMM：量子化学分子表示学习在组合药物推荐中的应用 

**Authors**: Chongmyung Kwon, Yujin Kim, Seoeun Park, Yunji Lee, Charmgil Hong  

**Link**: [PDF](https://arxiv.org/pdf/2510.07910)  

**Abstract**: Drug recommendation is an essential task in machine learning-based clinical decision support systems. However, the risk of drug-drug interactions (DDI) between co-prescribed medications remains a significant challenge. Previous studies have used graph neural networks (GNNs) to represent drug structures. Regardless, their simplified discrete forms cannot fully capture the molecular binding affinity and reactivity. Therefore, we propose Multimodal DDI Prediction with Molecular Electron Localization Function (ELF) Maps (MMM), a novel framework that integrates three-dimensional (3D) quantum-chemical information into drug representation learning. It generates 3D electron density maps using the ELF. To capture both therapeutic relevance and interaction risks, MMM combines ELF-derived features that encode global electronic properties with a bipartite graph encoder that models local substructure interactions. This design enables learning complementary characteristics of drug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442 substructures), comparing it with several baseline models. In particular, a comparison with the GNN-based SafeDrug model demonstrates statistically significant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112), and the DDI rate (p = 0.0386). These results demonstrate the potential of ELF-based 3D representations to enhance prediction accuracy and support safer combinatorial drug prescribing in clinical practice. 

**Abstract (ZH)**: 基于分子电子局域函数图的多模态药物-药物相互作用预测框架 

---
# Contrastive Weak-to-strong Generalization 

**Title (ZH)**: 对比弱到强的泛化能力 

**Authors**: Houcheng Jiang, Junfeng Fang, Jiaxin Wu, Tianyu Zhang, Chen Gao, Yong Li, Xiang Wang, Xiangnan He, Yang Deng  

**Link**: [PDF](https://arxiv.org/pdf/2510.07884)  

**Abstract**: Weak-to-strong generalization provides a promising paradigm for scaling large language models (LLMs) by training stronger models on samples from aligned weaker ones, without requiring human feedback or explicit reward modeling. However, its robustness and generalization are hindered by the noise and biases in weak-model outputs, which limit its applicability in practice. To address this challenge, we leverage implicit rewards, which approximate explicit rewards through log-likelihood ratios, and reveal their structural equivalence with Contrastive Decoding (CD), a decoding strategy shown to reduce noise in LLM generation. Building on this connection, we propose Contrastive Weak-to-Strong Generalization (ConG), a framework that employs contrastive decoding between pre- and post-alignment weak models to generate higher-quality samples. This approach enables more reliable capability transfer, denoising, and improved robustness, substantially mitigating the limitations of traditional weak-to-strong methods. Empirical results across different model families confirm consistent improvements, demonstrating the generality and effectiveness of ConG. Taken together, our findings highlight the potential of ConG to advance weak-to-strong generalization and provide a promising pathway toward AGI. 

**Abstract (ZH)**: 弱到强泛化提供了一种有希望的范式，通过在对齐的较弱模型样本上训练更强的模型来扩展大型语言模型（LLMs），而无需人工反馈或显式奖励建模。然而，由于较弱模型输出中的噪声和偏差限制了其实际应用，其健壮性和泛化能力受到了阻碍。为了解决这一挑战，我们利用隐含奖励，通过对数似然比近似显式奖励，并揭示其与对比解码（CD）结构等价的关系，对比解码是一种已被证明能够降低LLM生成噪声的解码策略。基于这一联系，我们提出了一种对比弱到强泛化（ConG）框架，该框架在预对齐和后对齐弱模型之间使用对比解码生成更高质量的样本。这种方法使得能力转移更加可靠，降噪效果更好，并提高了稳健性，显著缓解了传统弱到强方法的局限性。不同模型家族的实证结果一致表明ConG的普遍性和有效性，我们的发现突出了ConG在推动弱到强泛化方面的潜力，并为其向通用人工智能（AGI）的发展提供了一条有希望的道路。 

---
# Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception - Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track 

**Title (ZH)**: 小米Team EV-AD VLA：通过主动风险感知进行社会导航学习—2025年IROS RoboSense挑战赛社会导航赛道技术报告 

**Authors**: Erjia Xiao, Lingfeng Zhang, Yingbo Tang, Hao Cheng, Renjing Xu, Wenbo Ding, Lei Zhou, Long Chen, Hangjun Ye, Xiaoshuai Hao  

**Link**: [PDF](https://arxiv.org/pdf/2510.07871)  

**Abstract**: In this report, we describe the technical details of our submission to the IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on developing RGBD-based perception and navigation systems that enable autonomous agents to navigate safely, efficiently, and socially compliantly in dynamic human-populated indoor environments. The challenge requires agents to operate from an egocentric perspective using only onboard sensors including RGB-D observations and odometry, without access to global maps or privileged information, while maintaining social norm compliance such as safe distances and collision avoidance. Building upon the Falcon model, we introduce a Proactive Risk Perception Module to enhance social navigation performance. Our approach augments Falcon with collision risk understanding that learns to predict distance-based collision risk scores for surrounding humans, which enables the agent to develop more robust spatial awareness and proactive collision avoidance behaviors. The evaluation on the Social-HM3D benchmark demonstrates that our method improves the agent's ability to maintain personal space compliance while navigating toward goals in crowded indoor scenes with dynamic human agents, achieving 2nd place among 16 participating teams in the challenge. 

**Abstract (ZH)**: 本报告描述了我们对2025年IROS RoboSense挑战赛社会导航赛道的提交内容。该赛道专注于开发基于RGBD的感知与导航系统，使自主代理能够安全、高效且符合社交规范地在充满动态人群的室内环境中导航。挑战要求代理仅使用搭载传感器（包括RGB-D观测和里程计）从第一人称视角操作，而不允许访问全局地图或特权信息，并在保持社交规范（如安全距离和碰撞避免）的同时运行。基于Falcon模型，我们引入了主动风险感知模块以提升社会导航性能。我们的方法通过预测周围人类的距离为基础的碰撞风险评分来增强Falcon，使代理能够发展出更稳健的空间意识和主动碰撞避免行为。在Social-HM3D基准测试上的评估表明，我们的方法在拥挤的室内场景中导航至具有动态人类代理的目标时，提高了代理保持个人空间规范的能力，并在挑战中排名第二，共有16支参赛队伍。 

---
# DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation 

**Title (ZH)**: DM1：带有离散化正则化的MeanFlow在1步机器人操作中的应用 

**Authors**: Guowei Zou, Haitao Wang, Hejun Wu, Yukun Qian, Yuhang Wang, Weibing Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.07865)  

**Abstract**: The ability to learn multi-modal action distributions is indispensable for robotic manipulation policies to perform precise and robust control. Flow-based generative models have recently emerged as a promising solution to learning distributions of actions, offering one-step action generation and thus achieving much higher sampling efficiency compared to diffusion-based methods. However, existing flow-based policies suffer from representation collapse, the inability to distinguish similar visual representations, leading to failures in precise manipulation tasks. We propose DM1 (MeanFlow with Dispersive Regularization for One-Step Robotic Manipulation), a novel flow matching framework that integrates dispersive regularization into MeanFlow to prevent collapse while maintaining one-step efficiency. DM1 employs multiple dispersive regularization variants across different intermediate embedding layers, encouraging diverse representations across training batches without introducing additional network modules or specialized training procedures. Experiments on RoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s vs. 2-3.5s) and improves success rates by 10-20 percentage points, with the Lift task reaching 99% success over 85% of the baseline. Real-robot deployment on a Franka Panda further validates that DM1 transfers effectively from simulation to the physical world. To the best of our knowledge, this is the first work to leverage representation regularization to enable flow-based policies to achieve strong performance in robotic manipulation, establishing a simple yet powerful approach for efficient and robust manipulation. 

**Abstract (ZH)**: 基于分散正则化的DM1：一种用于一步机器人操作的流匹配框架 

---
# Self-Supervised Learning Strategies for a Platform to Test the Toxicity of New Chemicals and Materials 

**Title (ZH)**: 自主监督学习策略用于测试新型化学物质和材料的毒性平台 

**Authors**: Thomas Lautenschlager, Nils Friederich, Angelo Jovin Yamachui Sitcheu, Katja Nau, Gaëlle Hayot, Thomas Dickmeis, Ralf Mikut  

**Link**: [PDF](https://arxiv.org/pdf/2510.07853)  

**Abstract**: High-throughput toxicity testing offers a fast and cost-effective way to test large amounts of compounds. A key component for such systems is the automated evaluation via machine learning models. In this paper, we address critical challenges in this domain and demonstrate how representations learned via self-supervised learning can effectively identify toxicant-induced changes. We provide a proof-of-concept that utilizes the publicly available EmbryoNet dataset, which contains ten zebrafish embryo phenotypes elicited by various chemical compounds targeting different processes in early embryonic development. Our analysis shows that the learned representations using self-supervised learning are suitable for effectively distinguishing between the modes-of-action of different compounds. Finally, we discuss the integration of machine learning models in a physical toxicity testing device in the context of the TOXBOX project. 

**Abstract (ZH)**: 高通量毒性测试提供了一种快速且成本有效的大量化合物测试方法。此类系统的关键组件是通过机器学习模型进行的自动化评估。在本文中，我们针对该领域的关键挑战进行了探讨，并展示了通过自监督学习学习的表示能够有效识别毒物诱导的变化。我们利用公开可用的EmbryoNet数据集提供了概念验证，该数据集包含由作用于早期胚胎发育不同过程的各种化学物质诱导的十种斑马鱼胚胎表型。我们的分析表明，通过自监督学习学习的表示适合于有效区分不同化合物的作用机制。最后，我们在TOXBOX项目背景下讨论了机器学习模型在物理毒性测试设备中的集成。 

---
# Meta-Learning Based Few-Shot Graph-Level Anomaly Detection 

**Title (ZH)**: 基于元学习的少样本图级别异常检测 

**Authors**: Liting Li, Yumeng Wang, Yueheng Sun  

**Link**: [PDF](https://arxiv.org/pdf/2510.07847)  

**Abstract**: Graph-level anomaly detection aims to identify anomalous graphs or subgraphs within graph datasets, playing a vital role in various fields such as fraud detection, review classification, and biochemistry. While Graph Neural Networks (GNNs) have made significant progress in this domain, existing methods rely heavily on large amounts of labeled data, which is often unavailable in real-world scenarios. Additionally, few-shot anomaly detection methods based on GNNs are prone to noise interference, resulting in poor embedding quality and reduced model robustness. To address these challenges, we propose a novel meta-learning-based graph-level anomaly detection framework (MA-GAD), incorporating a graph compression module that reduces the graph size, mitigating noise interference while retaining essential node information. We also leverage meta-learning to extract meta-anomaly information from similar networks, enabling the learning of an initialization model that can rapidly adapt to new tasks with limited samples. This improves the anomaly detection performance on target graphs, and a bias network is used to enhance the distinction between anomalous and normal nodes. Our experimental results, based on four real-world biochemical datasets, demonstrate that MA-GAD outperforms existing state-of-the-art methods in graph-level anomaly detection under few-shot conditions. Experiments on both graph anomaly and subgraph anomaly detection tasks validate the framework's effectiveness on real-world datasets. 

**Abstract (ZH)**: 图级别异常检测旨在识别图数据集中异常的图或子图，在欺诈检测、评论分类和生物化学等领域发挥着重要作用。尽管图神经网络（GNNs）在该领域取得了显著进展，但现有方法严重依赖大量标记数据，而在实际场景中此类数据往往不可用。此外，基于GNN的少样本异常检测方法容易受到噪声干扰，导致嵌入质量较差和模型鲁棒性降低。为应对这些挑战，我们提出了一种新的基于元学习的图级别异常检测框架（MA-GAD），结合了图压缩模块以减小图的大小，减轻噪声干扰同时保留关键节点信息。我们还利用元学习从相似网络中提取元异常信息，从而学习一种初始模型，该模型能够在有限样本的情况下快速适应新任务。这可以提高目标图上的异常检测性能，并使用偏差网络增强异常节点和正常节点之间的区别。基于四个真实的生物化学数据集的实验结果表明，在少样本条件下，MA-GAD在图级别异常检测中的性能优于现有最先进的方法。实验结果验证了该框架在真实数据集上的有效性。 

---
# AdaSwitch: Adaptive Switching Generation for Knowledge Distillation 

**Title (ZH)**: AdaSwitch：自适应切换生成的知识蒸馏 

**Authors**: Jingyu Peng, Maolin Wang, Hengyi Cai, Yuchen Li, Kai Zhang, Shuaiqiang Wang, Dawei Yin, Xiangyu Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2510.07842)  

**Abstract**: Small language models (SLMs) are crucial for applications with strict latency and computational constraints, yet achieving high performance remains challenging. Knowledge distillation (KD) can transfer capabilities from large teacher models, but existing methods involve trade-offs: off-policy distillation provides high-quality supervision but introduces a training-inference mismatch, while on-policy approaches maintain consistency but rely on low-quality student outputs. To address these issues, we propose AdaSwitch, a novel approach that dynamically combines on-policy and off-policy generation at the token level. AdaSwitch allows the student to first explore its own predictions and then selectively integrate teacher guidance based on real-time quality assessment. This approach simultaneously preserves consistency and maintains supervision quality. Experiments on three datasets with two teacher-student LLM pairs demonstrate that AdaSwitch consistently improves accuracy, offering a practical and effective method for distilling SLMs with acceptable additional overhead. 

**Abstract (ZH)**: AdaSwitch：基于令牌级动态结合正政策略与反政策略的轻量模型知识蒸馏方法 

---
# Self-Improving LLM Agents at Test-Time 

**Title (ZH)**: 测试时自我提升的LLM代理 

**Authors**: Emre Can Acikgoz, Cheng Qian, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur  

**Link**: [PDF](https://arxiv.org/pdf/2510.07841)  

**Abstract**: One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution. 

**Abstract (ZH)**: 一种语言模型（LM）微调范式依赖于创建大型训练数据集，假设大量的多样数据使模型能够在后续训练后泛化到新任务。实践中，收集大量数据效率低下，而对其进行训练的成本高昂；更糟的是，没有保证模型能够处理复杂场景或泛化得更好。此外，现有技术很少评估训练样本是否提供了新颖信息或与模型已掌握的知识重复，导致不必要的成本。在本文中，我们探索一种新的测试时自我改进方法，以在运行时创建更加有效和泛化的代理语言模型。提出的算法可以总结为三个步骤：（i）首先识别模型难以处理的样本（自我意识），（ii）然后从检测到的不确定样本中生成类似示例（自我数据增强），（iii）使用这些新生成的样本在测试时进行微调（自我改进）。我们研究了这种方法的两种变体：测试时自我改进（TT-SI），其中同一模型从自身不确定案例生成额外的训练示例，然后从中学习；并与测试时蒸馏（TT-D）进行对比，后者更强的模型生成类似示例供不确定案例使用，使学生能够利用蒸馏监督进行适应。在不同代理基准上的实证评估表明，TT-SI 在所有基准中平均绝对准确率提高了5.48%，并且使用了68倍少的训练样本超过了其他标准学习方法。我们的研究结果突显了TT-SI 的潜力，展示了测试时自我改进算法构建更具能力代理的新范式，以实现自我进化。 

---
# MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation 

**Title (ZH)**: MetaDefense: 在生成过程中预防和抵御基于微调的Jailbreak攻击 

**Authors**: Weisen Jiang, Sinno Jialin Pan  

**Link**: [PDF](https://arxiv.org/pdf/2510.07835)  

**Abstract**: This paper introduces MetaDefense, a novel framework for defending against finetuning-based jailbreak attacks in large language models (LLMs). We observe that existing defense mechanisms fail to generalize to harmful queries disguised by unseen attack templates, despite LLMs being capable of distinguishing disguised harmful queries in the embedding space. Based on these insights, we propose a two-stage defense approach: (i) pre-generation defense that detects harmful queries before response generation begins, and (ii) mid-generation defense that monitors partial responses during generation to prevent outputting more harmful content. Our MetaDefense trains the LLM to predict the harmfulness of both queries and partial responses using specialized prompts, enabling early termination of potentially harmful interactions. Extensive experiments across multiple LLM architectures (LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense significantly outperforms existing defense mechanisms, achieving robust defense against harmful queries with seen and unseen attack templates while maintaining competitive performance on benign tasks. Code is available at this https URL. 

**Abstract (ZH)**: 本文介绍了MetaDefense，这是一种用于防御基于微调的大型语言模型（LLMs）逃逸攻击的新颖框架。我们观察到，现有的防御机制无法泛化到由未见攻击模板伪装的危害性查询，尽管LLMs能够在嵌入空间中区分伪装的危害性查询。基于这些见解，我们提出了一种两阶段防御方法：（i）预生成防御，在响应生成开始之前检测危害性查询，以及（ii）生成中防御，在生成过程中监控部分响应以防止输出更多危害内容。通过使用专门的提示训练LLM预测查询和部分响应的危害性，MetaDefense能够提前终止潜在危害的交互。在多种LLM架构（LLaMA-2-7B、Qwen-2.5-3B-Instruct和LLaMA-3.2-3B-Instruct）上的广泛实验表明，MetaDefense显著优于现有防御机制，在面对已见和未见攻击模板的危害性查询时实现稳健防御，同时在良性任务上保持竞争力。代码可通过以下链接获取：this https URL。 

---
# The Rise of the Knowledge Sculptor: A New Archetype for Knowledge Work in the Age of Generative AI 

**Title (ZH)**: 知识塑造者崛起：生成式AI时代的新知识工作者原型 

**Authors**: Cathal Doyle  

**Link**: [PDF](https://arxiv.org/pdf/2510.07829)  

**Abstract**: In the Generative Age, the nature of knowledge work is transforming. Traditional models that emphasise the organisation and retrieval of pre-existing information are increasingly inadequate in the face of generative AI (GenAI) systems capable of autonomous content creation. This paper introduces the Knowledge Sculptor (KS), a new professional archetype for Human-GenAI collaboration that transforms raw AI output into trustworthy, actionable knowledge. Grounded in a socio-technical perspective, the KS is conceptualised through a framework of competencies, including architecting a vision, iterative dialogue, information sculpting, and curiosity-driven synthesis. A practice-based vignette illustrates the KS role in action, and in a self-referential approach, the paper itself serves as an artefact of the sculpting process it describes. 

**Abstract (ZH)**: 生成时代，知识工作的本质正在转变。传统的侧重组织和检索现有信息的模型在面对能够自主内容创造的生成型AI系统时越来越显得不足。本文介绍了知识雕刻师（KS），这是一种新的专业形象，旨在促进人类与生成型AI的合作，将原始AI输出转化为可信赖且可操作的知识。基于社会技术视角，KS通过愿景架构、迭代对话、信息雕刻和好奇心驱动的综合等能力框架进行概念化。通过基于实践的实例展示KS的角色，并采用一种自指性方法，本文本身作为描述过程中的一种产物。 

---
# SIMU: Selective Influence Machine Unlearning 

**Title (ZH)**: SIMU: 选择性影响机器遗忘 

**Authors**: Anu Agarwal, Mihir Pamnani, Dilek Hakkani-Tur  

**Link**: [PDF](https://arxiv.org/pdf/2510.07822)  

**Abstract**: The undesired memorization of sensitive information by Large Language Models (LLMs) has emphasized the need for safety mechanisms that can regulate model behavior. This has led to the development of machine unlearning techniques that enable models to precisely forget sensitive and unwanted information. For machine unlearning, first-order and second-order optimizer-based methods have shown significant progress in enabling LLMs to forget targeted information. However, in doing so, these approaches often compromise the model's original capabilities, resulting in unlearned models that struggle to retain their prior knowledge and overall utility. To address this, we propose Selective Influence Machine Unlearning (SIMU), a two-step framework that enhances second-order optimizer-based unlearning by selectively updating only the critical neurons responsible for encoding the forget-set. By constraining updates to these targeted neurons, SIMU achieves comparable unlearning efficacy while substantially outperforming current methods in retaining the model's original knowledge. 

**Abstract (ZH)**: 大型语言模型（LLMs）对敏感信息的无意记忆强调了需要安全机制来调节模型行为。这促使开发了基于机器遗忘的技术，以使模型能够精确遗忘敏感和不必要的信息。对于基于机器遗忘的方法，一阶和二阶优化器方法在使LLMs忘记目标信息方面取得了显著进展。然而，在这样做时，这些方法通常会牺牲模型的原始能力，导致遗忘不完全的模型难以保留其先前的知识和总体效用。为了解决这一问题，我们提出了一种选择性影响机器遗忘（SIMU）的两步框架，该框架通过仅更新负责编码遗忘集的关键神经元来增强基于二阶优化器的遗忘技术。通过仅对这些有针对性的神经元进行约束更新，SIMU在保留模型原始知识方面超过了当前方法的同时，实现了可比的遗忘效果。 

---
# Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents 

**Title (ZH)**: 有效且隐蔽的一次性手机视觉-语言代理越狱方法 

**Authors**: Renhua Ding, Xiao Yang, Zhengwei Fang, Jun Luo, Kun He, Jun Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07809)  

**Abstract**: Large vision-language models (LVLMs) enable autonomous mobile agents to operate smartphone user interfaces, yet vulnerabilities to UI-level attacks remain critically understudied. Existing research often depends on conspicuous UI overlays, elevated permissions, or impractical threat models, limiting stealth and real-world applicability. In this paper, we present a practical and stealthy one-shot jailbreak attack that leverages in-app prompt injections: malicious applications embed short prompts in UI text that remain inert during human interaction but are revealed when an agent drives the UI via ADB (Android Debug Bridge). Our framework comprises three crucial components: (1) low-privilege perception-chain targeting, which injects payloads into malicious apps as the agent's visual inputs; (2) stealthy user-invisible activation, a touch-based trigger that discriminates agent from human touches using physical touch attributes and exposes the payload only during agent operation; and (3) one-shot prompt efficacy, a heuristic-guided, character-level iterative-deepening search algorithm (HG-IDA*) that performs one-shot, keyword-level detoxification to evade on-device safety filters. We evaluate across multiple LVLM backends, including closed-source services and representative open-source models within three Android applications, and we observe high planning and execution hijack rates in single-shot scenarios (e.g., GPT-4o: 82.5% planning / 75.0% execution). These findings expose a fundamental security vulnerability in current mobile agents with immediate implications for autonomous smartphone operation. 

**Abstract (ZH)**: 大型视觉-语言模型在自主移动代理操作智能手机用户界面方面的应用及其UI级攻击的临界研究不足：一种实用且隐蔽的一次性越狱攻击 

---
# Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models 

**Title (ZH)**: 基于图扩散模型的多大语言模型代理通信拓扑动态生成 

**Authors**: Eric Hanchen Jiang, Guancheng Wan, Sophia Yin, Mengting Li, Yuchen Wu, Xiao Liang, Xinfeng Li, Yizhou Sun, Wei Wang, Kai-Wei Chang, Ying Nian Wu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07799)  

**Abstract**: The efficiency of multi-agent systems driven by large language models (LLMs) largely hinges on their communication topology. However, designing an optimal topology is a non-trivial challenge, as it requires balancing competing objectives such as task performance, communication cost, and robustness. Existing frameworks often rely on static or hand-crafted topologies, which inherently fail to adapt to diverse task requirements, leading to either excessive token consumption for simple problems or performance bottlenecks for complex ones. To address this challenge, we introduce a novel generative framework called \textit{Guided Topology Diffusion (GTD)}. Inspired by conditional discrete graph diffusion models, GTD formulates topology synthesis as an iterative construction process. At each step, the generation is steered by a lightweight proxy model that predicts multi-objective rewards (e.g., accuracy, utility, cost), enabling real-time, gradient-free optimization towards task-adaptive topologies. This iterative, guided synthesis process distinguishes GTD from single-step generative frameworks, enabling it to better navigate complex design trade-offs. We validated GTD across multiple benchmarks, and experiments show that this framework can generate highly task-adaptive, sparse, and efficient communication topologies, significantly outperforming existing methods in LLM agent collaboration. 

**Abstract (ZH)**: 大型语言模型驱动的多智能体系统效率很大程度上取决于它们的通信拓扑结构。然而，设计最优拓扑结构是一项非平凡的挑战，因为它要求在任务性能、通信成本和鲁棒性之间进行平衡。现有框架通常依赖于静态或手工构造的拓扑结构，这本身就不能适应多样化的任务需求，导致简单问题中出现过度的token消耗，而复杂任务中则出现性能瓶颈。为应对这一挑战，我们引入了一种新颖的生成框架，称为\textit{引导拓扑扩散（GTD）}。受条件离散图扩散模型的启发，GTD 将拓扑合成表述为迭代构建过程。在每一步中，生成过程由一个轻量级的代理模型引导，该模型预测多目标奖励（例如，准确率、效用、成本），从而实现实时、无梯度优化，以生成任务自适应的拓扑结构。这种迭代的、引导合成过程使得GTD区别于单步生成框架，使其更好地导航复杂的设计权衡。我们在多个基准上验证了GTD，并且实验表明，该框架能够生成高度任务自适应、稀疏且高效的通信拓扑结构，在大型语言模型智能体协作中显著优于现有方法。 

---
# HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation 

**Title (ZH)**: HiPRAG: 分层过程奖励促进高效代理检索增强生成 

**Authors**: Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Kaiyu He, Xinya Du, Zhiyu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2510.07794)  

**Abstract**: Agentic RAG is a powerful technique for incorporating external information that LLMs lack, enabling better problem solving and question answering. However, suboptimal search behaviors exist widely, such as over-search (retrieving information already known) and under-search (failing to search when necessary), which leads to unnecessary overhead and unreliable outputs. Current training methods, which typically rely on outcome-based rewards in a RL framework, lack the fine-grained control needed to address these inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for Efficient agentic RAG (HiPRAG), a training methodology that incorporates a fine-grained, knowledge-grounded process reward into the RL training. Our approach evaluates the necessity of each search decision on-the-fly by decomposing the agent's reasoning trajectory into discrete, parsable steps. We then apply a hierarchical reward function that provides an additional bonus based on the proportion of optimal search and non-search steps, on top of commonly used outcome and format rewards. Experiments on the Qwen2.5 and Llama-3.2 models across seven diverse QA benchmarks show that our method achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished while improving search efficiency, reducing the over-search rate to just 2.3% and concurrently lowering the under-search rate. These results demonstrate the efficacy of optimizing the reasoning process itself, not just the final outcome. Further experiments and analysis demonstrate that HiPRAG shows good generalizability across a wide range of RL algorithms, model families, sizes, and types. This work demonstrates the importance and potential of fine-grained control through RL, for improving the efficiency and optimality of reasoning for search agents. 

**Abstract (ZH)**: 基于代理的RAG是一种强大的技术，用于结合LLM缺乏的外部信息，从而更好地解决问题和回答问题。然而，广泛存在搜索行为不佳的问题，例如过度搜索（检索已知信息）和不足搜索（在必要时未能搜索），这导致不必要的开销和不可靠的输出。当前的训练方法通常依赖于基于结果的奖励在RL框架中，缺乏处理这些低效率所需的细粒度控制。为了解决这个问题，我们提出了高效代理RAG的层次过程奖励（HiPRAG），这是一种融合了基于细粒度和知识导向的过程奖励的训练方法。我们的方法通过分解代理的推理轨迹为离散可解析的步骤，实时评估每个搜索决策的必要性。然后我们应用一个层次奖励函数，在常用的结果和格式奖励之上，根据最优搜索和非搜索步骤的比例提供额外的奖励。在Qwen2.5和Llama-3.2模型上对七个不同的问答基准测试表明，我们的方法实现了平均精度为65.4%（3B）和67.2%（7B），同时提高了搜索效率，将过度搜索率降低到2.3%，并同时降低不足搜索率。这些结果证明了优化推理过程本身，而不是仅仅优化最终结果的有效性。进一步的实验和分析表明，HiPRAG在各种RL算法、模型系列、大小和类型之间具有良好的通用性。这项工作展示了通过RL实现细粒度控制的重要性及其改进搜索代理推理效率和优化潜力的潜力。 

---
# LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology 

**Title (ZH)**: LLM4Cell：单一细胞生物学中大型语言和代理模型综述 

**Authors**: Sajib Acharjee Dip, Adrika Zafor, Bikash Kumar Paul, Uddip Acharjee Shuvo, Muhit Islam Emon, Xuan Wang, Liqing Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07793)  

**Abstract**: Large language models (LLMs) and emerging agentic frameworks are beginning to transform single-cell biology by enabling natural-language reasoning, generative annotation, and multimodal data integration. However, progress remains fragmented across data modalities, architectures, and evaluation standards. LLM4Cell presents the first unified survey of 58 foundation and agentic models developed for single-cell research, spanning RNA, ATAC, multi-omic, and spatial modalities. We categorize these methods into five families-foundation, text-bridge, spatial, multimodal, epigenomic, and agentic-and map them to eight key analytical tasks including annotation, trajectory and perturbation modeling, and drug-response prediction. Drawing on over 40 public datasets, we analyze benchmark suitability, data diversity, and ethical or scalability constraints, and evaluate models across 10 domain dimensions covering biological grounding, multi-omics alignment, fairness, privacy, and explainability. By linking datasets, models, and evaluation domains, LLM4Cell provides the first integrated view of language-driven single-cell intelligence and outlines open challenges in interpretability, standardization, and trustworthy model development. 

**Abstract (ZH)**: 大型语言模型（LLMs）和新兴自主框架正开始通过提供自然语言推理、生成注释和多模态数据集成的能力，重塑单细胞生物学。然而，进展在数据模态、架构和评估标准方面仍然碎片化。LLM4Cell 提供了第一个针对单细胞研究开发的 58 个基础和自主模型的统一综述，涵盖了 RNA、ATAC、多组学和空间模态。我们将这些方法分为五大家族——基础模型、文本桥梁、空间模型、多模态、表观基因组学和自主模型——并将其映射到八个关键分析任务中，包括注释、轨迹和扰动建模以及药物响应预测。基于超过 40 个公开数据集，我们分析了基准适用性、数据多样性以及伦理或可扩展性限制，并在包括生物基础、多组学对齐、公平性、隐私和可解释性在内的 10 个领域维度上评估了模型。通过将数据集、模型和评估领域联系起来，LLM4Cell 提供了语言驱动的单细胞智能的首个集成视图，并概述了可解释性、标准化和可信赖模型开发方面的开放挑战。 

---
# IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction 

**Title (ZH)**: 意图VLA：通用高效的身体化意图推理在人机交互中的应用 

**Authors**: Yandu Chen, Kefan Gu, Yuqing Wen, Yucheng Zhao, Tiancai Wang, Liqiang Nie  

**Link**: [PDF](https://arxiv.org/pdf/2510.07778)  

**Abstract**: Vision-Language-Action (VLA) models leverage pretrained vision-language models (VLMs) to couple perception with robotic control, offering a promising path toward general-purpose embodied intelligence. However, current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios, and then finetuned to map explicit instructions to actions. Consequently, due to the lack of reasoning-intensive pretraining and reasoning-guided manipulation, these models are unable to perform implicit human intention reasoning required for complex, real-world interactions. To overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework with a curriculum training paradigm and an efficient inference mechanism. Our proposed method first leverages carefully designed reasoning data that combine intention inference, spatial grounding, and compact embodied reasoning, endowing the model with both reasoning and perception capabilities. In the following finetuning stage, IntentionVLA employs the compact reasoning outputs as contextual guidance for action generation, enabling fast inference under indirect instructions. Experimental results show that IntentionVLA substantially outperforms $\pi_0$, achieving 18\% higher success rates with direct instructions and 28\% higher than ECoT under intention instructions. On out-of-distribution intention tasks, IntentionVLA achieves over twice the success rate of all baselines, and further enables zero-shot human-robot interaction with 40\% success rate. These results highlight IntentionVLA as a promising paradigm for next-generation human-robot interaction (HRI) systems. 

**Abstract (ZH)**: IntentionVLA：带有课程训练范式和高效推理机制的感知-语言-行动框架 

---
# Drift No More? Context Equilibria in Multi-Turn LLM Interactions 

**Title (ZH)**: 不再漂移？多轮LLM交互中的上下文均衡 

**Authors**: Vardhan Dongre, Ryan A. Rossi, Viet Dac Lai, David Seunghyun Yoon, Dilek Hakkani-Tür, Trung Bui  

**Link**: [PDF](https://arxiv.org/pdf/2510.07777)  

**Abstract**: Large Language Models (LLMs) excel at single-turn tasks such as instruction following and summarization, yet real-world deployments require sustained multi-turn interactions where user goals and conversational context persist and evolve. A recurring challenge in this setting is context drift: the gradual divergence of a model's outputs from goal-consistent behavior across turns. Unlike single-turn errors, drift unfolds temporally and is poorly captured by static evaluation metrics. In this work, we present a study of context drift in multi-turn interactions and propose a simple dynamical framework to interpret its behavior. We formalize drift as the turn-wise KL divergence between the token-level predictive distributions of the test model and a goal-consistent reference model, and propose a recurrence model that interprets its evolution as a bounded stochastic process with restoring forces and controllable interventions. We instantiate this framework in both synthetic long-horizon rewriting tasks and realistic user-agent simulations such as in $\tau$-Bench, measuring drift for several open-weight LLMs that are used as user simulators. Our experiments consistently reveal stable, noise-limited equilibria rather than runaway degradation, and demonstrate that simple reminder interventions reliably reduce divergence in line with theoretical predictions. Together, these results suggest that multi-turn drift can be understood as a controllable equilibrium phenomenon rather than as inevitable decay, providing a foundation for studying and mitigating context drift in extended interactions. 

**Abstract (ZH)**: 大语言模型在多轮交互中的上下文漂移研究：可控均衡现象的解析与应用 

---
# Trajectory Conditioned Cross-embodiment Skill Transfer 

**Title (ZH)**: 基于轨迹条件的跨躯体技能转移 

**Authors**: YuHang Tang, Yixuan Lou, Pengfei Han, Haoming Song, Xinyi Ye, Dong Wang, Bin Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2510.07773)  

**Abstract**: Learning manipulation skills from human demonstration videos presents a promising yet challenging problem, primarily due to the significant embodiment gap between human body and robot manipulators. Existing methods rely on paired datasets or hand-crafted rewards, which limit scalability and generalization. We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment Skill Transfer, enabling robots to acquire manipulation skills directly from human demonstration videos. Our key insight is to represent human motions as sparse optical flow trajectories, which serve as embodiment-agnostic motion cues by removing morphological variations while preserving essential dynamics. Conditioned on these trajectories together with visual and textual inputs, TrajSkill jointly synthesizes temporally consistent robot manipulation videos and translates them into executable actions, thereby achieving cross-embodiment skill transfer. Extensive experiments are conducted, and the results on simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\% and KVD by 36.6\% compared with the state-of-the-art, and improves cross-embodiment success rate by up to 16.7\%. Real-robot experiments in kitchen manipulation tasks further validate the effectiveness of our approach, demonstrating practical human-to-robot skill transfer across embodiments. 

**Abstract (ZH)**: 从人类演示视频中学习操作技能：一种具有挑战性的前景问题，主要由于人类身体与机器人操作器之间存在显著的具身差距。现有方法依赖配对数据集或手工设计的奖励，限制了可扩展性和泛化能力。我们提出了一种轨迹条件跨具身技能转移框架TrajSkill，使得机器人可以直接从人类演示视频中获取操作技能。我们的核心洞察是将人类动作表示为稀疏光流轨迹，这些轨迹作为不依赖于具身的运动提示，通过消除形态学变异保留了基本动力学。在这些轨迹以及视觉和文本输入的条件下，TrajSkill联合生成时间一致的机器人操作视频，并将其转换为可执行的动作，从而实现跨具身技能转移。在广泛的实验中，仿真数据（MetaWorld）的结果表明，与最先进的方法相比，TrajSkill将FVD降低了39.6%，KVD降低了36.6%，并且提高了跨具身成功率高达16.7%。在厨房操作任务的实时机器人实验中进一步验证了我们方法的有效性，展示了跨具身的人到机器人的技能转移的实际可行性。 

---
# ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning 

**Title (ZH)**: ToolLibGen: 可扩展的大型语言模型推理工具自动创建与聚合 

**Authors**: Murong Yue, Zhiwei Liu, Liangwei Yang, Jianguo Zhang, Zuxin Liu, Haolin Chen, Ziyu Yao, Silvio Savarese, Caiming Xiong, Shelby Heinecke, Huan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07768)  

**Abstract**: Large Language Models (LLMs) equipped with external tools have demonstrated enhanced performance on complex reasoning tasks. The widespread adoption of this tool-augmented reasoning is hindered by the scarcity of domain-specific tools. For instance, in domains such as physics question answering, suitable and specialized tools are often missing. Recent work has explored automating tool creation by extracting reusable functions from Chain-of-Thought (CoT) reasoning traces; however, these approaches face a critical scalability bottleneck. As the number of generated tools grows, storing them in an unstructured collection leads to significant retrieval challenges, including an expanding search space and ambiguity between function-related tools. To address this, we propose a systematic approach to automatically refactor an unstructured collection of tools into a structured tool library. Our system first generates discrete, task-specific tools and clusters them into semantically coherent topics. Within each cluster, we introduce a multi-agent framework to consolidate scattered functionalities: a code agent refactors code to extract shared logic and creates versatile, aggregated tools, while a reviewing agent ensures that these aggregated tools maintain the complete functional capabilities of the original set. This process transforms numerous question-specific tools into a smaller set of powerful, aggregated tools without loss of functionality. Experimental results demonstrate that our approach significantly improves tool retrieval accuracy and overall reasoning performance across multiple reasoning tasks. Furthermore, our method shows enhanced scalability compared with baselines as the number of question-specific increases. 

**Abstract (ZH)**: 具有外部工具的大语言模型在复杂推理任务中展现了增强的性能。这种工具增强的推理的广泛应用受限于领域特定工具的稀缺性。例如，在物理问题回答领域，合适的专门工具往往缺失。最近的工作通过从Chain-of-Thought（CoT）推理痕迹中提取可重用函数来探索自动化工具创建的方法，但这些方法面临着严重的可扩展性瓶颈。随着生成的工具数量增加，将它们存储在无结构集合中会导致检索挑战的扩大，包括搜索空间的扩展和功能相关工具之间的模糊性。为此，我们提出了一种系统的方法，自动将无结构的工具集合重新组织成结构化的工具库。系统首先生成特定任务的离散工具，并将其聚类为语义相关的主题。在每个聚类中，我们引入了一个多智能体框架来集中分散的功能：代码智能体重构代码以提取共享逻辑并创建多功能的聚合工具，而审查智能体确保这些聚合工具保留了原始工具集的完整功能。这一过程将大量特定问题的工具转化为更少但更强大的聚合工具，而不损失功能。实验结果表明，我们的方法在多个推理任务中显著提高了工具检索准确性和总体推理性能。此外，当特定问题的数量增加时，我们的方法相比基线方法显示出更好的可扩展性。 

---
# A Unified Multi-Task Learning Framework for Generative Auto-Bidding with Validation-Aligned Optimization 

**Title (ZH)**: 统一的多任务学习框架：带有验证对齐优化的生成性自动出价 

**Authors**: Yiqin Lv, Zhiyu Mou, Miao Xu, Jinghao Chen, Qi Wang, Yixiu Mao, Yun Qu, Rongquan Bai, Chuan Yu, Jian Xu, Bo Zheng, Xiangyang Ji  

**Link**: [PDF](https://arxiv.org/pdf/2510.07760)  

**Abstract**: In online advertising, heterogeneous advertiser requirements give rise to numerous customized bidding tasks that are typically optimized independently, resulting in extensive computation and limited data efficiency. Multi-task learning offers a principled framework to train these tasks jointly through shared representations. However, existing multi-task optimization strategies are primarily guided by training dynamics and often generalize poorly in volatile bidding environments. To this end, we present Validation-Aligned Multi-task Optimization (VAMO), which adaptively assigns task weights based on the alignment between per-task training gradients and a held-out validation gradient, thereby steering updates toward validation improvement and better matching deployment objectives. We further equip the framework with a periodicity-aware temporal module and couple it with an advanced generative auto-bidding backbone to enhance cross-task transfer of seasonal structure and strengthen bidding performance. Meanwhile, we provide theoretical insights into the proposed method, e.g., convergence guarantee and alignment analysis. Extensive experiments on both simulated and large-scale real-world advertising systems consistently demonstrate significant improvements over typical baselines, illuminating the effectiveness of the proposed approach. 

**Abstract (ZH)**: 基于验证对齐的多任务优化方法(VAMO)在在线广告中的应用 

---
# Parallel Test-Time Scaling for Latent Reasoning Models 

**Title (ZH)**: latent reasoning模型的并行测试时缩放方法 

**Authors**: Runyang You, Yongqi Li, Meng Liu, Wenjie Wang, Liqiang Nie, Wenjie Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.07745)  

**Abstract**: Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \
This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at this https URL. 

**Abstract (ZH)**: 平行测试时缩放（TTS）在增强大规模语言模型（LLMs）中的关键方法：通过并行采样多个基于令牌的思维链并使用投票或搜索聚合结果。最近在潜在推理方面的进展，其中中间推理在连续向量空间中展开，为显式的思维链提供了更高效的替代方案，但潜在模型是否可以类似地从平行TTS中受益仍然悬而未决，主要原因是连续空间中缺乏采样机制，以及缺乏高级轨迹聚合所需的概率信号。 

---
# UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes 

**Title (ZH)**: UltraLED：在超高动态范围场景中学习全面观测 

**Authors**: Yuang Meng, Xin Jin, Lina Lei, Chun-Le Guo, Chongyi Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.07741)  

**Abstract**: Ultra-high dynamic range (UHDR) scenes exhibit significant exposure disparities between bright and dark regions. Such conditions are commonly encountered in nighttime scenes with light sources. Even with standard exposure settings, a bimodal intensity distribution with boundary peaks often emerges, making it difficult to preserve both highlight and shadow details simultaneously. RGB-based bracketing methods can capture details at both ends using short-long exposure pairs, but are susceptible to misalignment and ghosting artifacts. We found that a short-exposure image already retains sufficient highlight detail. The main challenge of UHDR reconstruction lies in denoising and recovering information in dark regions. In comparison to the RGB images, RAW images, thanks to their higher bit depth and more predictable noise characteristics, offer greater potential for addressing this challenge. This raises a key question: can we learn to see everything in UHDR scenes using only a single short-exposure RAW image? In this study, we rely solely on a single short-exposure frame, which inherently avoids ghosting and motion blur, making it particularly robust in dynamic scenes. To achieve that, we introduce UltraLED, a two-stage framework that performs exposure correction via a ratio map to balance dynamic range, followed by a brightness-aware RAW denoiser to enhance detail recovery in dark regions. To support this setting, we design a 9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a corresponding dataset based on diverse scenes, using only the shortest exposure as input for reconstruction. Extensive experiments show that UltraLED significantly outperforms existing single-frame approaches. Our code and dataset are made publicly available at this https URL. 

**Abstract (ZH)**: 超高的动态范围(UHDR)场景在明亮区域和暗淡区域之间表现出显著的曝光差异。这种条件在包含光源的夜间场景中常见。即使使用标准的曝光设置，亮度分布通常会出现双峰边界，使得同时保存高光和阴影细节变得困难。基于RGB的曝光级联方法可以使用短曝光-长曝光对来捕获两端的细节，但易受到对齐错误和鬼影伪影的影响。我们发现，短曝光图像本身已保留足够的高光细节。UHDR重建的主要挑战在于暗区噪声去除和信息恢复。与RGB图像相比，RAW图像由于具有更高的位深度和更可预测的噪声特性，为解决这一挑战提供了更大的潜力。这提出了一个核心问题：我们能否仅使用单张短曝光RAW图像就能学会看到UHDR场景中的所有细节？在本研究中，我们仅依赖于一张短曝光帧，从根本上避免了鬼影和运动模糊，使其在动态场景中尤为 robust。为了实现这一点，我们引入了UltraLED，这是一种两阶段框架，通过比率图执行曝光校正以平衡动态范围，随后是一个亮度感知的RAW降噪器，以增强暗区的细节恢复。为支持这一设置，我们设计了一个9级级联管道来合成真实的UHDR图像，并基于多样场景贡献了一个相应的数据集，仅使用最短曝光作为重建输入。广泛的实验表明，UltraLED显著优于现有的单帧方法。我们的代码和数据集已公开在以下链接：这个 https URL。 

---
# AppForge: From Assistant to Independent Developer - Are GPTs Ready for Software Development? 

**Title (ZH)**: AppForge: 从助手到独立开发者——GPT准备好进行软件开发了吗？ 

**Authors**: Dezhi Ran, Yuan Cao, Mengzhou Wu, Simin Chen, Yuzhe Guo, Jun Ren, Zihe Song, Hao Yu, Jialei Wei, Linyi Li, Wei Yang, Baishakhi Ray, Tao Xie  

**Link**: [PDF](https://arxiv.org/pdf/2510.07740)  

**Abstract**: Large language models (LLMs) have demonstrated remarkable capability in function-level code generation tasks. Unlike isolated functions, real-world applications demand reasoning over the entire software system: developers must orchestrate how different components interact, maintain consistency across states over time, and ensure the application behaves correctly within the lifecycle and framework constraints. Yet, no existing benchmark adequately evaluates whether LLMs can bridge this gap and construct entire software systems from scratch. To address this gap, we propose APPFORGE, a benchmark consisting of 101 software development problems drawn from real-world Android apps. Given a natural language specification detailing the app functionality, a language model is tasked with implementing the functionality into an Android app from scratch. Developing an Android app from scratch requires understanding and coordinating app states, lifecycle management, and asynchronous operations, calling for LLMs to generate context-aware, robust, and maintainable code. To construct APPFORGE, we design a multi-agent system to automatically summarize the main functionalities from app documents and navigate the app to synthesize test cases validating the functional correctness of app implementation. Following rigorous manual verification by Android development experts, APPFORGE incorporates the test cases within an automated evaluation framework that enables reproducible assessment without human intervention, making it easily adoptable for future research. Our evaluation on 12 flagship LLMs show that all evaluated models achieve low effectiveness, with the best-performing model (GPT-5) developing only 18.8% functionally correct applications, highlighting fundamental limitations in current models' ability to handle complex, multi-component software engineering challenges. 

**Abstract (ZH)**: 大型语言模型（LLMs）在函数级代码生成任务中展示了 remarkable 的能力。然而，现实世界的应用要求对整个软件系统进行推理：开发人员必须协调不同组件的交互、维护随时间变化的状态的一致性，并确保应用程序在生命周期和框架约束内正确运行。目前尚无基准能够充分评估 LLMs 是否能够弥合这一差距，从零构建整个软件系统。为解决这一问题，我们提出了 APPFORGE，一个包含 101 个源自真实 Android 应用的软件开发问题的基准。根据自然语言规范描述的应用功能，语言模型需要从零开始实现功能到 Android 应用。从零构建 Android 应用需要理解并协调应用状态、生命周期管理和异步操作，这要求 LLMs 生成上下文感知的、健壮的和可维护的代码。为了构建 APPFORGE，我们设计了一个多Agent系统，自动从应用文档中总结主要功能，并导航应用以合成验证应用实现功能正确性的测试用例。经过 Android 开发专家严格的手动验证后，APPFORGE 将测试用例纳入自动化评估框架，使其能够在没有人工干预的情况下进行可重复评估，并便于未来研究的采用。我们的评估表明，所有受测模型均表现出较低的有效性，性能最好的模型（GPT-5）仅实现了 18.8% 功能正确的应用，突显了当前模型在处理复杂、多组件软件工程挑战方面的根本局限。 

---
# MeSH: Memory-as-State-Highways for Recursive Transformers 

**Title (ZH)**: MeSH: 记忆作为状态高速公路的递归变压器 

**Authors**: Chengting Yu, Xiaobo Shu, Yadao Wang, Yizhen Zhang, Haoyi Wu, Jiaang Li, Rujiao Long, Ziheng Chen, Yuchi Xu, Wenbo Su, Bo Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2510.07739)  

**Abstract**: Recursive transformers reuse parameters and iterate over hidden states multiple times, decoupling compute depth from parameter depth. However, under matched compute, recursive models with fewer parameters often lag behind non-recursive counterparts. By probing hidden states, we trace this performance gap to two primary bottlenecks: undifferentiated computation, where the core is forced to adopt a similar computational pattern at every iteration, and information overload, where long-lived and transient information must coexist in a single hidden state. To address the issues, we introduce a Memory-as-State-Highways (MeSH) scheme, which externalizes state management into an explicit memory buffer and employs lightweight routers to dynamically diversify computation across iterations. Probing visualizations confirm that MeSH successfully resolves the pathologies by inducing functional specialization across iterations. On the Pythia suite (160M-1.4B), MeSH-enhanced recursive transformers consistently improve over recursive baselines and outperforms its larger non-recursive counterpart at the 1.4B scale, improving average downstream accuracy by +1.06% with 33% fewer non-embedding parameters. Our analysis establishes MeSH as a scalable and principled architecture for building stronger recursive models. 

**Abstract (ZH)**: 递归Transformer通过参数共享和多轮迭代隐藏状态，解耦计算深度和参数深度。然而，在匹配计算资源的情况下，参数较少的递归模型往往落后于非递归模型。通过探究隐藏状态，我们将性能差距归因于两个主要瓶颈：未分化的计算模式，即核心在每次迭代中被迫采用类似的计算模式，以及信息过载，长寿命和临时信息必须在同一隐藏状态中共存。为了解决这些问题，我们提出了Memory-as-State-Highways (MeSH)方案，该方案将状态管理外部化到显式的内存缓冲区中，并使用轻量级路由器在迭代间动态多样化计算。视觉探针证实MeSH成功解决了这些病态问题，通过在迭代间诱导功能特化。在Pythia套件（160M-1.4B规模）上，MeSH增强的递归变压器在所有基准上表现出优性能，并在1.4B规模上优于其更大的非递归模型，平均下游准确率提高了1.06%，同时非嵌入参数减少了33%。我们的分析确立了MeSH作为一种可扩展且合理的递归模型架构。 

---
# DEAS: DEtached value learning with Action Sequence for Scalable Offline RL 

**Title (ZH)**: DEAS: 分离价值学习与动作序列以实现可扩展的离线RL 

**Authors**: Changyeon Kim, Haeone Lee, Younggyo Seo, Kimin Lee, Yuke Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07730)  

**Abstract**: Offline reinforcement learning (RL) presents an attractive paradigm for training intelligent agents without expensive online interactions. However, current approaches still struggle with complex, long-horizon sequential decision making. In this work, we introduce DEtached value learning with Action Sequence (DEAS), a simple yet effective offline RL framework that leverages action sequences for value learning. These temporally extended actions provide richer information than single-step actions and can be interpreted through the options framework via semi-Markov decision process Q-learning, enabling reduction of the effective planning horizon by considering longer sequences at once. However, directly adopting such sequences in actor-critic algorithms introduces excessive value overestimation, which we address through detached value learning that steers value estimates toward in-distribution actions that achieve high return in the offline dataset. We demonstrate that DEAS consistently outperforms baselines on complex, long-horizon tasks from OGBench and can be applied to enhance the performance of large-scale Vision-Language-Action models that predict action sequences, significantly boosting performance in both RoboCasa Kitchen simulation tasks and real-world manipulation tasks. 

**Abstract (ZH)**: 离线强化学习（RL）为训练智能代理提供了无需昂贵在线交互的有吸引力范式。然而，当前方法仍然难以应对复杂的、长期序列决策任务。在本文中，我们引入了基于动作序列的分离值学习（DEAS），这是一种简单而有效的离线RL框架，利用动作序列进行值学习。这些时间延长的动作提供了比单步动作更丰富的信息，并可以通过半马尔可夫决策过程Q学习通过选项框架进行解释，从而通过一次考虑更长的序列来减少有效的规划时滞。然而，直接在演员-批评家算法中采用这样的序列会导致价值过估计，我们通过分离值学习的方法将价值估计导向离线数据集中实现高回报的在分布动作来解决这一问题。我们的研究结果表明，DEAS在OGBench的复杂、长期序列任务中始终优于基线方法，并且可以应用于增强预测动作序列的大型Vision-Language-Action模型的性能，在RoboCasa Kitchen模拟任务和现实世界的操作任务中显著提升了性能。 

---
# Causality Guided Representation Learning for Cross-Style Hate Speech Detection 

**Title (ZH)**: 因果引导的表示学习在跨风格仇恨言论检测中的应用 

**Authors**: Chengshuai Zhao, Shu Wan, Paras Sheth, Karan Patwa, K. Selçuk Candan, Huan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07707)  

**Abstract**: The proliferation of online hate speech poses a significant threat to the harmony of the web. While explicit hate is easily recognized through overt slurs, implicit hate speech is often conveyed through sarcasm, irony, stereotypes, or coded language -- making it harder to detect. Existing hate speech detection models, which predominantly rely on surface-level linguistic cues, fail to generalize effectively across diverse stylistic variations. Moreover, hate speech spread on different platforms often targets distinct groups and adopts unique styles, potentially inducing spurious correlations between them and labels, further challenging current detection approaches. Motivated by these observations, we hypothesize that the generation of hate speech can be modeled as a causal graph involving key factors: contextual environment, creator motivation, target, and style. Guided by this graph, we propose CADET, a causal representation learning framework that disentangles hate speech into interpretable latent factors and then controls confounders, thereby isolating genuine hate intent from superficial linguistic cues. Furthermore, CADET allows counterfactual reasoning by intervening on style within the latent space, naturally guiding the model to robustly identify hate speech in varying forms. CADET demonstrates superior performance in comprehensive experiments, highlighting the potential of causal priors in advancing generalizable hate speech detection. 

**Abstract (ZH)**: 在线仇恨言论的泛滥对网络和谐构成显著威胁。虽然明确的仇恨言论通过明显的污言秽语易于识别，但隐含的仇恨言论常常通过讽刺、反语、刻板印象或隐含语言来传达——使其更难检测。现有的仇恨言论检测模型主要依赖于表面语言线索，未能有效地跨不同风格变体进行推广。此外，不同平台上仇恨言论的传播往往针对不同的群体，并采用独特的风格，这可能会在平台和标签之间诱导虚假相关性，进一步挑战当前的检测方法。受这些观察的启发，我们假设仇恨言论的生成可以建模为涉及关键因素的因果图：上下文环境、创作者动机、目标和风格。基于这一图，我们提出了CADET因果表示学习框架，将仇恨言论分解为可解释的潜在因素，然后控制混杂变量，从而将真实的仇恨意图与表面的语言线索区分开来。此外，CADET通过在潜在空间中干预风格来支持假设性推理，自然引导模型在不同形式中稳健地检测仇恨言论。在全面的实验中，CADET展示了优越的性能，突显了因果先验在推动泛化仇恨言论检测方面的潜力。 

---
# Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs 

**Title (ZH)**: 重思推理：基于推理的LLMs后门综述 

**Authors**: Man Hu, Xinyi Wu, Zuofeng Suo, Jinbo Feng, Linghui Meng, Yanhao Jia, Anh Tuan Luu, Shuai Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2510.07697)  

**Abstract**: With the rise of advanced reasoning capabilities, large language models (LLMs) are receiving increasing attention. However, although reasoning improves LLMs' performance on downstream tasks, it also introduces new security risks, as adversaries can exploit these capabilities to conduct backdoor attacks. Existing surveys on backdoor attacks and reasoning security offer comprehensive overviews but lack in-depth analysis of backdoor attacks and defenses targeting LLMs' reasoning abilities. In this paper, we take the first step toward providing a comprehensive review of reasoning-based backdoor attacks in LLMs by analyzing their underlying mechanisms, methodological frameworks, and unresolved challenges. Specifically, we introduce a new taxonomy that offers a unified perspective for summarizing existing approaches, categorizing reasoning-based backdoor attacks into associative, passive, and active. We also present defense strategies against such attacks and discuss current challenges alongside potential directions for future research. This work offers a novel perspective, paving the way for further exploration of secure and trustworthy LLM communities. 

**Abstract (ZH)**: 随着高级推理能力的发展，大型语言模型（LLMs）受到了越来越多的关注。然而，尽管推理能力改善了LLMs在下游任务上的表现，但也引入了新的安全风险，因为对手可以利用这些能力进行后门攻击。现有关于后门攻击和推理安全的综述提供了全面的概述，但在针对LLMs推理能力的后门攻击和防御方面的深入分析方面存在不足。本文旨在通过分析其基础机制、方法论框架和未解决的挑战，为LLMs中的基于推理的后门攻击提供一个全面的综述。具体来说，我们引入了一个新的分类体系，提供了现有方法的统一视角，并将基于推理的后门攻击归类为关联性的、被动的和主动的。我们还介绍了针对这些攻击的防御策略，并讨论了当前挑战以及未来研究的潜在方向。这项工作提供了新的视角，为进一步探索安全可信的LLMs社区铺平了道路。 

---
# Stress-Testing Model Specs Reveals Character Differences among Language Models 

**Title (ZH)**: 基于压力测试模型规格揭示语言模型之间的人物差异 

**Authors**: Jifan Zhang, Henry Sleight, Andi Peng, John Schulman, Esin Durmus  

**Link**: [PDF](https://arxiv.org/pdf/2510.07686)  

**Abstract**: Large language models (LLMs) are increasingly trained from AI constitutions and model specifications that establish behavioral guidelines and ethical principles. However, these specifications face critical challenges, including internal conflicts between principles and insufficient coverage of nuanced scenarios. We present a systematic methodology for stress-testing model character specifications, automatically identifying numerous cases of principle contradictions and interpretive ambiguities in current model specs.
We stress test current model specs by generating scenarios that force explicit tradeoffs between competing value-based principles. Using a comprehensive taxonomy we generate diverse value tradeoff scenarios where models must choose between pairs of legitimate principles that cannot be simultaneously satisfied. We evaluate responses from twelve frontier LLMs across major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral disagreement through value classification scores. Among these scenarios, we identify over 70,000 cases exhibiting significant behavioral divergence. Empirically, we show this high divergence in model behavior strongly predicts underlying problems in model specifications. Through qualitative analysis, we provide numerous example issues in current model specs such as direct contradiction and interpretive ambiguities of several principles. Additionally, our generated dataset also reveals both clear misalignment cases and false-positive refusals across all of the frontier models we study. Lastly, we also provide value prioritization patterns and differences of these models. 

**Abstract (ZH)**: 大规模语言模型（LLMs）日益从AI宪法和模型规范中训练，这些规范建立了行为准则和伦理原则。然而，这些规范面临着内部原则冲突和对细微场景覆盖不足的关键挑战。我们提出了一种系统的方法来压力测试模型角色规范，自动识别当前模型规范中大量原则矛盾和解释不清的情况。 

---
# Curriculum Learning with Synthetic Data for Enhanced Pulmonary Nodule Detection in Chest Radiographs 

**Title (ZH)**: 基于合成数据的课程学习方法以增强胸部X光片中肺结节检测 

**Authors**: Pranav Sambhu, Om Guin, Madhav Sambhu, Jinho Cha  

**Link**: [PDF](https://arxiv.org/pdf/2510.07681)  

**Abstract**: This study evaluates whether integrating curriculum learning with diffusion-based synthetic augmentation can enhance the detection of difficult pulmonary nodules in chest radiographs, particularly those with low size, brightness, and contrast, which often challenge conventional AI models due to data imbalance and limited annotation. A Faster R-CNN with a Feature Pyramid Network (FPN) backbone was trained on a hybrid dataset comprising expert-labeled NODE21 (1,213 patients; 52.4 percent male; mean age 63.2 +/- 11.5 years), VinDr-CXR, CheXpert, and 11,206 DDPM-generated synthetic images. Difficulty scores based on size, brightness, and contrast guided curriculum learning. Performance was compared to a non-curriculum baseline using mean average precision (mAP), Dice score, and area under the curve (AUC). Statistical tests included bootstrapped confidence intervals, DeLong tests, and paired t-tests. The curriculum model achieved a mean AUC of 0.95 versus 0.89 for the baseline (p < 0.001), with improvements in sensitivity (70 percent vs. 48 percent) and accuracy (82 percent vs. 70 percent). Stratified analysis demonstrated consistent gains across all difficulty bins (Easy to Very Hard). Grad-CAM visualizations confirmed more anatomically focused attention under curriculum learning. These results suggest that curriculum-guided synthetic augmentation enhances model robustness and generalization for pulmonary nodule detection. 

**Abstract (ZH)**: 本研究评估了将 Curriculum 学习与扩散基础的合成增强结合是否能提高胸部X光片中难以检测的肺结节检测能力，特别是那些尺寸小、亮度低、对比度低的结节，由于数据不平衡和有限的注释，这些结节经常挑战传统的AI模型。研究使用包含专家标注的NODE21（1,213例患者；男性占52.4%；平均年龄63.2±11.5岁）、VinDr-CXR、CheXpert及11,206张DDPM生成的合成图像的混合数据集对带有特征金字塔网络（FPN）骨干的Faster R-CNN进行了训练。根据尺寸、亮度和对比度的难度评分指导Curriculum学习。性能与无Curriculum基线进行了比较，使用平均平均精度（mAP）、Dice分数和曲线下面积（AUC）进行评估。统计测试包括置信区间 bootstrap、DeLong检验和配对t检验。Curriculum模型的平均AUC为0.95，而基线为0.89（p < 0.001），并在灵敏度和准确性方面表现出改进（分别从48%提高到70%，从70%提高到82%）。分层分析表明，Curriculum学习在所有难度级别上均显示出一致的收益。Grad-CAM可视化确认了Curriculum学习下更专注于解剖结构的注意力。这些结果表明，由Curriculum指导的合成增强可以提高肺结节检测模型的鲁棒性和泛化能力。 

---
# Controllable Video Synthesis via Variational Inference 

**Title (ZH)**: 基于变分推断的可控视频合成 

**Authors**: Haoyi Duan, Yunzhi Zhang, Yilun Du, Jiajun Wu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07670)  

**Abstract**: Many video workflows benefit from a mixture of user controls with varying granularity, from exact 4D object trajectories and camera paths to coarse text prompts, while existing video generative models are typically trained for fixed input formats. We develop a video synthesis method that addresses this need and generates samples with high controllability for specified elements while maintaining diversity for under-specified ones. We cast the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to account for all task constraints collectively. To address the optimization challenge, we break down the problem into step-wise KL divergence minimization over an annealed sequence of distributions, and further propose a context-conditioned factorization technique that reduces modes in the solution space to circumvent local optima. Experiments suggest that our method produces samples with improved controllability, diversity, and 3D consistency compared to prior works. 

**Abstract (ZH)**: 许多视频工作流需要不同程度的用户控制，从精确的4D对象轨迹和摄像机路径到粗略的文字提示，而现有的视频生成模型通常仅针对固定的输入格式进行训练。我们开发了一种视频合成方法，以满足这一需求，该方法在指定元素上生成高可控性样本，同时在欠指定的元素上保持多样性。我们将任务视为变分推断，以逼近组合分布，并利用多个视频生成骨干来联合考虑所有任务约束。为了解决优化挑战，我们将问题分解为逐步最小化退火序列分布的KL散度，并提出了一种基于上下文的因子分解技术，以减少解空间中的模式，从而规避局部最优。实验结果表明，与现有方法相比，我们的方法能够生成具有更好可控性、多样性和3D一致性的样本。 

---
# TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable Medical Image Registration 

**Title (ZH)**: TCIP：阈值控制迭代金字塔网络在可变形医学图像配准中的应用 

**Authors**: Heming Wu, Di Wang, Tai Ma, Peng Zhao, Yubin Xiao, Zhongke Wu, Xing-Ce Wang, Chuang Li, Xuan Wu, You Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2510.07666)  

**Abstract**: Although pyramid networks have demonstrated superior performance in deformable medical image registration, their decoder architectures are inherently prone to propagating and accumulating anatomical structure misalignments. Moreover, most existing models do not adaptively determine the number of iterations for optimization under varying deformation requirements across images, resulting in either premature termination or excessive iterations that degrades registration accuracy. To effectively mitigate the accumulation of anatomical misalignments, we propose the Feature-Enhanced Residual Module (FERM) as the core component of each decoding layer in the pyramid network. FERM comprises three sequential blocks that extract anatomical semantic features, learn to suppress irrelevant features, and estimate the final deformation field, respectively. To adaptively determine the number of iterations for varying images, we propose the dual-stage Threshold-Controlled Iterative (TCI) strategy. In the first stage, TCI assesses registration stability and with asserted stability, it continues with the second stage to evaluate convergence. We coin the model that integrates FERM and TCI as Threshold-Controlled Iterative Pyramid (TCIP). Extensive experiments on three public brain MRI datasets and one abdomen CT dataset demonstrate that TCIP outperforms the state-of-the-art (SOTA) registration networks in terms of accuracy, while maintaining comparable inference speed and a compact model parameter size. Finally, we assess the generalizability of FERM and TCI by integrating them with existing registration networks and further conduct ablation studies to validate the effectiveness of these two proposed methods. 

**Abstract (ZH)**: 虽然 Pyramid 网络在可变形医学图像配准中表现出色，但其解码架构天生容易传播和累积解剖结构错位。此外，大多数现有模型不能根据不同图像的变形要求自适应地确定优化的迭代次数，导致要么过早终止要么进行过多迭代，从而降低配准精度。为了有效减轻解剖结构错位的累积，我们提出了一种特征增强残差模块（FERM）作为 Pyramid 网络中每一层解码器的核心组件。FERM 包含三个连续的块，分别提取解剖语义特征、学习抑制无关特征以及估计最终变形场。为了根据不同图像自适应地确定迭代次数，我们提出了双阶段阈值控制迭代（TCI）策略。在第一阶段，TCI 评估配准的稳定性，在确认稳定性后，进入第二阶段评估收敛性。我们将集成了 FERM 和 TCI 的模型命名为阈值控制迭代 Pyramid（TCIP）。在三个公开的脑 MRI 数据集和一个腹部 CT 数据集上进行的广泛实验表明，TCIP 在准确性上优于当前最先进的（SOTA）配准网络，同时保持类似的推理速度和紧凑的模型参数量。最后，我们通过将 FERM 和 TCI 与现有配准网络集成，并进一步进行消融研究，评估了这两种方法的一般适用性。 

---
# IKNet: Interpretable Stock Price Prediction via Keyword-Guided Integration of News and Technical Indicators 

**Title (ZH)**: IKNet: 基于关键词引导的新闻与技术指标集成的可解释股票价格预测 

**Authors**: Jinwoong Kim, Sangjin Park  

**Link**: [PDF](https://arxiv.org/pdf/2510.07661)  

**Abstract**: The increasing influence of unstructured external information, such as news articles, on stock prices has attracted growing attention in financial markets. Despite recent advances, most existing newsbased forecasting models represent all articles using sentiment scores or average embeddings that capture the general tone but fail to provide quantitative, context-aware explanations of the impacts of public sentiment on predictions. To address this limitation, we propose an interpretable keyword-guided network (IKNet), which is an explainable forecasting framework that models the semantic association between individual news keywords and stock price movements. The IKNet identifies salient keywords via FinBERTbased contextual analysis, processes each embedding through a separate nonlinear projection layer, and integrates their representations with the time-series data of technical indicators to forecast next-day closing prices. By applying Shapley Additive Explanations the model generates quantifiable and interpretable attributions for the contribution of each keyword to predictions. Empirical evaluations of S&P 500 data from 2015 to 2024 demonstrate that IKNet outperforms baselines, including recurrent neural networks and transformer models, reducing RMSE by up to 32.9% and improving cumulative returns by 18.5%. Moreover, IKNet enhances transparency by offering contextualized explanations of volatility events driven by public sentiment. 

**Abstract (ZH)**: 非结构化外部信息（如新闻文章）对股票价格日益增长的影响在金融市场上引起了广泛关注。尽管近期取得了一些进展，但现有的大多数基于新闻的预测模型通过情感评分或平均词嵌入来表示所有文章，这些方法虽然捕捉了整体情绪，但无法提供定量且具有上下文 awareness 的解释，说明公众情绪对预测的影响。为解决这一局限，我们提出了一种可解释的关键词导向网络（IKNet），这是一种可解释的预测框架，用于建模单个新闻关键词与股票价格变动之间的语义关联。IKNet 通过基于 FinBERT 的上下文分析识别关键关键词，每个嵌入通过单独的非线性投影层处理，并将它们的表示与技术指标的时间序列数据集成以预测次日收盘价。通过应用 Shapley 加性解释，该模型为每个关键词对预测的贡献生成了可量化且可解释的归因。对2015年至2024年标普500指数数据的实证评估表明，IKNet 的表现优于包括循环神经网络和变换器模型在内的基线模型，减少了高达32.9%的 RMSE，并提高了累计回报率18.5%。此外，IKNet 提高了透明度，通过对由公众情绪驱动的波动事件提供上下文化的解释。 

---
# OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference 

**Title (ZH)**: OBCache: 最优大脑KV缓存裁剪以实现高效的长上下文LLM推理 

**Authors**: Yuzhe Gu, Xiyu Liang, Jiaojiao Zhao, Enmao Diao  

**Link**: [PDF](https://arxiv.org/pdf/2510.07651)  

**Abstract**: Large language models (LLMs) with extended context windows enable powerful downstream applications but impose significant memory overhead, as caching all key-value (KV) states scales linearly with sequence length and batch size. Existing cache eviction methods address this by exploiting attention sparsity, yet they typically rank tokens heuristically using accumulated attention weights without considering their true impact on attention outputs. We propose Optimal Brain Cache (OBCache), a principled framework that formulates cache eviction as a layer-wise structured pruning problem. Building upon the Optimal Brain Damage (OBD) theory, OBCache quantifies token saliency by measuring the perturbation in attention outputs induced by pruning tokens, with closed-form scores derived for isolated keys, isolated values, and joint key-value pairs. Our scores account not only for attention weights but also for information from value states and attention outputs, thereby enhancing existing eviction strategies with output-aware signals. Experiments on LLaMA and Qwen models demonstrate that replacing the heuristic scores in existing works, which estimate token saliency across different query positions, with OBCache's output-aware scores consistently improves long-context accuracy. 

**Abstract (ZH)**: 扩展上下文窗口的大型语言模型（LLMs）能实现强大的下游应用，但会给内存带来显著负担，因为缓存所有键值（KV）状态会线性地随着序列长度和批量大小增加。现有缓存淘汰方法通过利用注意力稀疏性来解决这一问题，但它们通常使用累积注意力权重对令牌进行启发式排名，而不考虑它们对注意力输出的真实影响。我们提出了最优大脑缓存（OBCache），这是一种原理性的框架，将缓存淘汰问题形式化为逐层结构化剪枝问题。基于最优大脑损伤（OBD）理论，OBCache通过测量剪枝令牌引起的注意力输出扰动来量化令牌的显著性，并推导出孤立键、孤立值和联合键值对的闭式评分。我们的评分不仅考虑注意力权重，还考虑来自值状态和注意力输出的信息，从而使现有的淘汰策略受益于输出感知的信号。在LLaMA和Qwen模型上的实验表明，用OBCache的输出感知评分替换现有工作中不同查询位置的启发式评分，可以一致地提高长序列上下文的准确性。 

---
# Value Flows 

**Title (ZH)**: 价值流动 

**Authors**: Perry Dong, Chongyi Zheng, Chelsea Finn, Dorsa Sadigh, Benjamin Eysenbach  

**Link**: [PDF](https://arxiv.org/pdf/2510.07650)  

**Abstract**: While most reinforcement learning methods today flatten the distribution of future returns to a single scalar value, distributional RL methods exploit the return distribution to provide stronger learning signals and to enable applications in exploration and safe RL. While the predominant method for estimating the return distribution is by modeling it as a categorical distribution over discrete bins or estimating a finite number of quantiles, such approaches leave unanswered questions about the fine-grained structure of the return distribution and about how to distinguish states with high return uncertainty for decision-making. The key idea in this paper is to use modern, flexible flow-based models to estimate the full future return distributions and identify those states with high return variance. We do so by formulating a new flow-matching objective that generates probability density paths satisfying the distributional Bellman equation. Building upon the learned flow models, we estimate the return uncertainty of distinct states using a new flow derivative ODE. We additionally use this uncertainty information to prioritize learning a more accurate return estimation on certain transitions. We compare our method (Value Flows) with prior methods in the offline and online-to-online settings. Experiments on $37$ state-based and $25$ image-based benchmark tasks demonstrate that Value Flows achieves a $1.3\times$ improvement on average in success rates. Website: this https URL Code: this https URL 

**Abstract (ZH)**: 虽然当前大多数增强学习方法将未来回报分布压平为单一标量值，分布型增强学习方法利用回报分布提供更强的学习信号，并能够促进探索和安全增强学习的应用。尽管估计回报分布的主要方法是将其建模为离散区间上的分类分布或估计有限数量的分位数，但这些方法未能回答回报分布的细微结构以及如何区分具有高回报不确定性状态以辅助决策的问题。本文的关键思想是使用现代灵活的流动模型估计完整的未来回报分布，并识别具有高回报方差的状态。我们通过制定一个新的流动匹配目标来实现这一点，该目标生成满足分布贝尔曼方程的概率密度路径。基于学习到的流动模型，我们使用一个新的流动导数常微分方程来估计不同状态的回报不确定性。此外，我们还利用这些不确定性信息来优先学习某些转换上的更准确回报估计。我们在离线和在线到在线设置中将我们的方法（价值流动）与前期方法进行了比较。在37个基于状态和25个基于图像的基准任务上的实验表明，价值流动在成功率上平均提高了1.3倍。网址：这个 https URL 代码：这个 https URL。 

---
# Banking Done Right: Redefining Retail Banking with Language-Centric AI 

**Title (ZH)**: 正确开展银行业务：以语言为中心的AI重塑零售银行业 

**Authors**: Xin Jie Chua, Jeraelyn Ming Li Tan, Jia Xuan Tan, Soon Chang Poh, Yi Xian Goh, Debbie Hui Tian Choong, Chee Mun Foong, Sze Jue Yang, Chee Seng Chan  

**Link**: [PDF](https://arxiv.org/pdf/2510.07645)  

**Abstract**: This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt Bank to enable customers to execute core financial transactions through natural language conversation. This represents the first global regulator-approved deployment worldwide where conversational AI functions as the primary banking interface, in contrast to prior assistants that have been limited to advisory or support roles. Built entirely in-house, Ryt AI is powered by ILMU, a closed-source LLM developed internally, and replaces rigid multi-screen workflows with a single dialogue orchestrated by four LLM-powered agents (Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific LoRA adapter to ILMU, which is hosted within the bank's infrastructure to ensure consistent behavior with minimal overhead. Deterministic guardrails, human-in-the-loop confirmation, and a stateless audit architecture provide defense-in-depth for security and compliance. The result is Banking Done Right: demonstrating that regulator-approved natural-language interfaces can reliably support core financial operations under strict governance. 

**Abstract (ZH)**: Ryt AI：一个内置型代理框架，驱动Ryt银行通过自然语言对话执行核心金融交易，这是全球首个监管机构批准的将对话AI作为主要银行界面的应用。 

---
# Retentive Relevance: Capturing Long-Term User Value in Recommendation Systems 

**Title (ZH)**: 持久相关性：捕获推荐系统中的长期用户价值 

**Authors**: Saeideh Bakhshi, Phuong Mai Nguyen, Robert Schiller, Tiantian Xu, Pawan Kodandapani, Andrew Levine, Cayman Simpson, Qifan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07621)  

**Abstract**: Recommendation systems have traditionally relied on short-term engagement signals, such as clicks and likes, to personalize content. However, these signals are often noisy, sparse, and insufficient for capturing long-term user satisfaction and retention. We introduce Retentive Relevance, a novel content-level survey-based feedback measure that directly assesses users' intent to return to the platform for similar content. Unlike other survey measures that focus on immediate satisfaction, Retentive Relevance targets forward-looking behavioral intentions, capturing longer term user intentions and providing a stronger predictor of retention. We validate Retentive Relevance using psychometric methods, establishing its convergent, discriminant, and behavioral validity. Through large-scale offline modeling, we show that Retentive Relevance significantly outperforms both engagement signals and other survey measures in predicting next-day retention, especially for users with limited historical engagement. We develop a production-ready proxy model that integrates Retentive Relevance into the final stage of a multi-stage ranking system on a social media platform. Calibrated score adjustments based on this model yield substantial improvements in engagement, and retention, while reducing exposure to low-quality content, as demonstrated by large-scale A/B experiments. This work provides the first empirically validated framework linking content-level user perceptions to retention outcomes in production systems. We offer a scalable, user-centered solution that advances both platform growth and user experience. Our work has broad implications for responsible AI development. 

**Abstract (ZH)**: 一种基于内容的调查反馈度量Retentive Relevance及其在提升用户留存中的应用 

---
# DGTEN: A Robust Deep Gaussian based Graph Neural Network for Dynamic Trust Evaluation with Uncertainty-Quantification Support 

**Title (ZH)**: DGTEN：一种具有不确定性量化支持的鲁棒深度高斯图神经网络动态信任评价方法 

**Authors**: Muhammad Usman, Yugyung Lee  

**Link**: [PDF](https://arxiv.org/pdf/2510.07620)  

**Abstract**: Dynamic trust evaluation in large, rapidly evolving graphs requires models that can capture changing relationships, express calibrated confidence, and resist adversarial manipulation. DGTEN (Deep Gaussian-based Trust Evaluation Network) introduces a unified graph framework that achieves all three by combining uncertainty-aware message passing, expressive temporal modeling, and built-in defenses against trust-targeted attacks. It represents nodes and edges as Gaussian distributions so that both semantic signals and epistemic uncertainty propagate through the graph neural network, enabling risk-aware trust decisions rather than overconfident guesses. To model how trust evolves, it employs hybrid Absolute-Gaussian-Hourglass (HAGH) positional encoding with Kolmogorov-Arnold network-based unbiased multi-head attention, followed by an ordinary differential equation (ODE)-based residual learning module to jointly capture abrupt shifts and smooth trends. Robust adaptive ensemble coefficient analysis prunes or down-weights suspicious interactions using complementary cosine and Jaccard similarity measures, mitigating reputation laundering, sabotage, and on/off attacks. On two signed Bitcoin trust networks, DGTEN delivers significant improvements: in single-timeslot prediction on Bitcoin-Alpha, it improves MCC by 10.77% over the best dynamic baseline; in the cold-start scenario, it achieves a 16.41% MCC gain - the largest across all tasks and datasets. Under adversarial on/off attacks, it surpasses the baseline by up to 11.63% MCC. These results validate the effectiveness of the unified DGTEN framework. 

**Abstract (ZH)**: 动态大型快速演化图中的信任评估需要能够捕捉关系变化、表达校准的信心并抵抗 adversarial 操作的模型。DGTEN（基于深度高斯的信任评估网络）通过结合不确定性意识的消息传递、表现力时间建模以及内置的针对信任目标攻击的防护机制，实现上述所有目标。它将节点和边表示为高斯分布，使得语义信号和Epistemic不确定性通过图神经网络传播，从而实现风险意识的信任决策，而非过度自信的猜测。为了建模信任如何演变，它使用了混合绝寂数学-高斯-漏斗（HAGH）位置编码与基于Kolmogorov-Arnold神经网络的无偏多头注意机制，随后通过基于常微分方程（ODE）的残差学习模块来共同捕捉突变和平稳趋势。鲁棒自适应集成系数分析使用互补的余弦相似性和Jaccard相似性度量来消除或降低可疑交互的影响，从而减轻声誉洗钱、破坏和开停攻击。在两个带有签名的比特币信任网络上，DGTEN实现了显著的改进：在单时隙预测方面，它在Bitcoin-Alpha中将MCC提高了10.77%，这是最佳动态基线的最佳表现；在冷启动场景中，它实现了16.41%的MCC增益，这是所有任务和数据集中最大的增益。在对抗开停攻击的情况下，它将MCC提高了最多11.63%。这些结果验证了统一的DGTEN框架的有效性。 

---
# Vocabulary embeddings organize linguistic structure early in language model training 

**Title (ZH)**: 词汇嵌入在语言模型训练早期组织语言结构 

**Authors**: Isabel Papadimitriou, Jacob Prince  

**Link**: [PDF](https://arxiv.org/pdf/2510.07613)  

**Abstract**: Large language models (LLMs) work by manipulating the geometry of input embedding vectors over multiple layers. Here, we ask: how are the input vocabulary representations of language models structured, and how and when does this structure evolve over training? To answer this question, we use representational similarity analysis, running a suite of experiments that correlate the geometric structure of the input embeddings and output embeddings of two open-source models (Pythia 12B and OLMo 7B) with semantic, syntactic, and frequency-based metrics over the course of training. Our key findings are as follows: 1) During training, the vocabulary embedding geometry quickly converges to high correlations with a suite of semantic and syntactic features; 2) Embeddings of high-frequency and function words (e.g., "the," "of") converge to their final vectors faster than lexical and low-frequency words, which retain some alignment with the bias in their random initializations. These findings help map the dynamic trajectory by which input embeddings organize around linguistic structure, revealing distinct roles for word frequency and function. Our findings motivate a deeper study of how the evolution of vocabulary geometry may facilitate specific capability gains during model training. 

**Abstract (ZH)**: 大型语言模型（LLMs）通过在多层中操纵输入嵌入向量的几何结构来工作。我们的问题是：语言模型的输入词汇表示是如何结构化的，以及这种结构在训练过程中是如何演变的？为回答这一问题，我们使用了表示相似性分析，通过一系列实验将两个开源模型（Pythia 12B和OLMo 7B）的输入嵌入和输出嵌入在整个训练过程中的几何结构与语义、句法和基于频率的度量进行相关性分析。我们的主要发现如下：1）在训练过程中，词汇嵌入几何结构迅速与一系列语义和句法特征达到高相关性；2）高频和功能词（如“the”、“of”）的嵌入比词汇和低频词更快收敛到其最终向量，并且这些词在一定程度上保持了与随机初始化偏见的对齐。这些发现有助于描绘输入嵌入围绕语言结构组织的动态轨迹，揭示词汇频率和功能的不同作用。我们的发现促进了对词汇几何结构演变如何在模型训练过程中促进特定能力提升的深入研究。 

---
# TGM: a Modular and Efficient Library for Machine Learning on Temporal Graphs 

**Title (ZH)**: TGM：用于时序图机器学习的模块化高效库 

**Authors**: Jacob Chmura, Shenyang Huang, Tran Gia Bao Ngo, Ali Parviz, Farimah Poursafaei, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, Matthias Fey, Reihaneh Rabbany  

**Link**: [PDF](https://arxiv.org/pdf/2510.07586)  

**Abstract**: Well-designed open-source software drives progress in Machine Learning (ML) research. While static graph ML enjoys mature frameworks like PyTorch Geometric and DGL, ML for temporal graphs (TG), networks that evolve over time, lacks comparable infrastructure. Existing TG libraries are often tailored to specific architectures, hindering support for diverse models in this rapidly evolving field. Additionally, the divide between continuous- and discrete-time dynamic graph methods (CTDG and DTDG) limits direct comparisons and idea transfer. To address these gaps, we introduce Temporal Graph Modelling (TGM), a research-oriented library for ML on temporal graphs, the first to unify CTDG and DTDG approaches. TGM offers first-class support for dynamic node features, time-granularity conversions, and native handling of link-, node-, and graph-level tasks. Empirically, TGM achieves an average 7.8x speedup across multiple models, datasets, and tasks compared to the widely used DyGLib, and an average 175x speedup on graph discretization relative to available implementations. Beyond efficiency, we show in our experiments how TGM unlocks entirely new research possibilities by enabling dynamic graph property prediction and time-driven training paradigms, opening the door to questions previously impractical to study. TGM is available at this https URL 

**Abstract (ZH)**: 精心设计的开源软件推动机器学习研究进展。虽然静态图机器学习享有成熟的框架如PyTorch Geometric和DGL，但时间图（随时间演变的网络）的机器学习领域缺乏类似的基础设施。现有的时间图库往往针对特定架构，阻碍了对该迅速发展的领域中各种模型的支持。此外，连续时间和离散时间动态图方法（CTDG和DTDG）之间的差距限制了直接比较和思想转移。为了解决这些差距，我们介绍了时间图建模（TGM），这是一个面向研究的时间图机器学习库，首次统一了CTDG和DTDG方法。TGM提供了一流的支持动态节点特征、时间粒度转换以及对边级、节点级和图级任务的原生处理。实证研究表明，与广泛使用的DyGLib相比，TGM在多个模型、数据集和任务上的平均加速倍数为7.8倍，而在图离散化方面的平均加速倍数为175倍。除了效率，我们的实验展示了TGM如何通过启用动态图属性预测和时间驱动的训练范式，解锁全新的研究可能性，开启以前难以研究的问题。TGM可在以下链接获取：this https URL 

---
# Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets 

**Title (ZH)**: 疫情相关语料中的语言模式：COVID-19、Pandemics和Monkeypox数据集的比较分析 

**Authors**: Mkululi Sikosana, Sean Maudsley-Barton, Oluwaseun Ajao  

**Link**: [PDF](https://arxiv.org/pdf/2510.07579)  

**Abstract**: This study conducts a computational linguistic analysis of pandemic-related online discourse to examine how language distinguishes health misinformation from factual communication. Drawing on three corpora: COVID-19 false narratives (n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts (n = 5787), we identify significant differences in readability, rhetorical markers, and persuasive language use. COVID-19 misinformation exhibited markedly lower readability scores and contained over twice the frequency of fear-related or persuasive terms compared to the other datasets. It also showed minimal use of exclamation marks, contrasting with the more emotive style of Monkeypox content. These patterns suggest that misinformation employs a deliberately complex rhetorical style embedded with emotional cues, a combination that may enhance its perceived credibility. Our findings contribute to the growing body of work on digital health misinformation by highlighting linguistic indicators that may aid detection efforts. They also inform public health messaging strategies and theoretical models of crisis communication in networked media environments. At the same time, the study acknowledges limitations, including reliance on traditional readability indices, use of a deliberately narrow persuasive lexicon, and reliance on static aggregate analysis. Future research should therefore incorporate longitudinal designs, broader emotion lexicons, and platform-sensitive approaches to strengthen robustness. 

**Abstract (ZH)**: 本研究通过计算语言学分析与疫情相关的在线 discourse，以探讨语言如何区分健康 misinformation 和事实性沟通。本研究基于三个语料库：COVID-19 错误叙事（n = 7588）、一般 COVID-19 内容（n = 10700）和猴痘相关帖子（n = 5787），识别出流畅度、修辞标志和说服性语言使用方面的显著差异。COVID-19  misinformation 的流畅度明显较低，同时包含比其他数据集两倍以上的恐惧相关或说服性术语。此外，它在使用感叹号方面也很少，与猴痘内容更具情绪化的风格形成对比。这些模式表明，misinformation 采用了一种刻意复杂的修辞风格，其中嵌入了情感线索，这种结合可能增强其感知可信度。研究结果为数字健康 misinformation 的研究领域增添了新的语言指标，有助于检测努力。它们还为公共卫生信息沟通策略提供了依据，并为网络媒体环境中危机沟通的理论模型提供建议。同时，本研究承认其局限性，包括依赖传统的流畅度指标、使用刻意狭窄的说服词汇库以及依赖于静态的总体分析。未来研究应采用纵向设计、更广泛的情绪词汇库和平台敏感的方法，以增强研究的稳健性。

---

This study conducts a computational linguistic analysis of pandemic-related online discourse to examine how language distinguishes health misinformation from factual communication. 

---
# Accuracy, Memory Efficiency and Generalization: A Comparative Study on Liquid Neural Networks and Recurrent Neural Networks 

**Title (ZH)**: 准确度、内存效率和泛化能力：液态神经网络与循环神经网络的比较研究 

**Authors**: Shilong Zong, Alex Bierly, Almuatazbellah Boker, Hoda Eldardiry  

**Link**: [PDF](https://arxiv.org/pdf/2510.07578)  

**Abstract**: This review aims to conduct a comparative analysis of liquid neural networks (LNNs) and traditional recurrent neural networks (RNNs) and their variants, such as long short-term memory networks (LSTMs) and gated recurrent units (GRUs). The core dimensions of the analysis include model accuracy, memory efficiency, and generalization ability. By systematically reviewing existing research, this paper explores the basic principles, mathematical models, key characteristics, and inherent challenges of these neural network architectures in processing sequential data. Research findings reveal that LNN, as an emerging, biologically inspired, continuous-time dynamic neural network, demonstrates significant potential in handling noisy, non-stationary data, and achieving out-of-distribution (OOD) generalization. Additionally, some LNN variants outperform traditional RNN in terms of parameter efficiency and computational speed. However, RNN remains a cornerstone in sequence modeling due to its mature ecosystem and successful applications across various tasks. This review identifies the commonalities and differences between LNNs and RNNs, summarizes their respective shortcomings and challenges, and points out valuable directions for future research, particularly emphasizing the importance of improving the scalability of LNNs to promote their application in broader and more complex scenarios. 

**Abstract (ZH)**: 本综述旨在比较分析液态神经网络（LNNs）和传统循环神经网络（RNNs）及其变种（如长短期记忆网络LSTMs和门控循环单元GRUs）在处理序列数据方面的模型精度、内存效率和泛化能力。通过系统回顾现有研究，本文探讨了这些神经网络架构的基本原理、数学模型、关键特性和固有挑战。研究表明，作为一种新兴的、受生物学启发的连续时间动态神经网络，LNN显现出在处理噪声和非稳态数据以及实现领域外泛化方面的显著潜力。此外，某些LNN变种在参数效率和计算速度方面优于传统RNN。然而，由于其成熟生态系统和在多种任务中的成功应用，RNN仍然是序列建模的基石。本文指出了LNNs和RNNs之间的共同点与异同，总结了各自的不足和挑战，并指出了未来研究的重要方向，尤其是强调了提高LNNs可扩展性的重要性，以促进其在更广泛和更复杂场景中的应用。 

---
# Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER 

**Title (ZH)**: 轻量级变压器编码器的多任务预微调及其在文本分类和NER中的应用 

**Authors**: Junyi Zhu, Savas Ozkan, Andrea Maracani, Sinan Mutlu, Cho Jung Min, Mete Ozay  

**Link**: [PDF](https://arxiv.org/pdf/2510.07566)  

**Abstract**: Deploying natural language processing (NLP) models on mobile platforms requires models that can adapt across diverse applications while remaining efficient in memory and computation. We investigate pre-finetuning strategies to enhance the adaptability of lightweight BERT-like encoders for two fundamental NLP task families: named entity recognition (NER) and text classification. While pre-finetuning improves downstream performance for each task family individually, we find that naïve multi-task pre-finetuning introduces conflicting optimization signals that degrade overall performance. To address this, we propose a simple yet effective multi-task pre-finetuning framework based on task-primary LoRA modules, which enables a single shared encoder backbone with modular adapters. Our approach achieves performance comparable to individual pre-finetuning while meeting practical deployment constraint. Experiments on 21 downstream tasks show average improvements of +0.8% for NER and +8.8% for text classification, demonstrating the effectiveness of our method for versatile mobile NLP applications. 

**Abstract (ZH)**: 在移动平台上部署自然语言处理模型需要能够在多种应用中适应且保持高效内存和计算能力的模型。我们探讨了增强轻量级BERT-like编码器在命名实体识别(NER)和文本分类两大基本NLP任务家族中适应性的预微调策略。虽然单独的预微调可以提高每种任务家族的下游性能，我们发现简单的多任务预微调引入了冲突的优化信号，导致整体性能下降。为此，我们提出了一种基于任务主导LoRA模块的简单而有效的多任务预微调框架，能够实现一个共享的编码器主干和模块化适配器。我们的方法在满足实际部署约束的同时达到了与单独预微调相当的性能。实验表明，在21个下游任务上，命名实体识别任务的平均改进幅度为+0.8%，文本分类任务的平均改进幅度为+8.8%，证明了该方法在多功能移动NLP应用中的有效性。 

---
# Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic 

**Title (ZH)**: 探究大规模语言模型交互中的主题模式和用户偏好——基于BERTopic的方法 

**Authors**: Abhay Bhandarkar, Gaurav Mishra, Khushi Juchani, Harsh Singhal  

**Link**: [PDF](https://arxiv.org/pdf/2510.07557)  

**Abstract**: This study applies BERTopic, a transformer-based topic modeling technique, to the lmsys-chat-1m dataset, a multilingual conversational corpus built from head-to-head evaluations of large language models (LLMs). Each user prompt is paired with two anonymized LLM responses and a human preference label, used to assess user evaluation of competing model outputs. The main objective is uncovering thematic patterns in these conversations and examining their relation to user preferences, particularly if certain LLMs are consistently preferred within specific topics. A robust preprocessing pipeline was designed for multilingual variation, balancing dialogue turns, and cleaning noisy or redacted data. BERTopic extracted over 29 coherent topics including artificial intelligence, programming, ethics, and cloud infrastructure. We analysed relationships between topics and model preferences to identify trends in model-topic alignment. Visualization techniques included inter-topic distance maps, topic probability distributions, and model-versus-topic matrices. Our findings inform domain-specific fine-tuning and optimization strategies for improving real-world LLM performance and user satisfaction. 

**Abstract (ZH)**: 本研究使用基于转换器的主题建模技术BERTopic，对lmsys-chat-1m数据集进行分析，该数据集是由大型语言模型（LLMs）一对一评估构建的多语言对话语料库。每位用户的问题与两个匿名的LLM回应以及一个人类偏好标签配对，用于评估用户对竞争模型输出的评价。主要目标是揭示这些对话中的主题模式，并研究它们与用户偏好之间的关系，特别是某些LLM在特定主题中是否始终被一致地偏好。我们设计了一个 robust 的预处理管道，以平衡多语言变化、对话回合，并清理噪声或被删除的数据。BERTopic 提取了超过29个连贯的主题，包括人工智能、编程、伦理和云基础设施。我们分析了主题与模型偏好之间的关系，以识别模型-主题对齐的趋势。可视化技术包括主题间距离图、主题概率分布以及模型-主题矩阵。我们的发现为改进实际应用中的LLM性能和用户满意度提供了特定领域的微调和优化策略。 

---
# Label Semantics for Robust Hyperspectral Image Classification 

**Title (ZH)**: 标签语义促进稳健的高光谱图像分类 

**Authors**: Rafin Hassan, Zarin Tasnim Roshni, Rafiqul Bari, Alimul Islam, Nabeel Mohammed, Moshiur Farazi, Shafin Rahman  

**Link**: [PDF](https://arxiv.org/pdf/2510.07556)  

**Abstract**: Hyperspectral imaging (HSI) classification is a critical tool with widespread applications across diverse fields such as agriculture, environmental monitoring, medicine, and materials science. Due to the limited availability of high-quality training samples and the high dimensionality of spectral data, HSI classification models are prone to overfitting and often face challenges in balancing accuracy and computational complexity. Furthermore, most of HSI classification models are monomodal, where it solely relies on spectral-spatial data to learn decision boundaries in the high dimensional embedding space. To address this, we propose a general-purpose Semantic Spectral-Spatial Fusion Network (S3FN) that uses contextual, class specific textual descriptions to complement the training of an HSI classification model. Specifically, S3FN leverages LLMs to generate comprehensive textual descriptions for each class label that captures their unique characteristics and spectral behaviors. These descriptions are then embedded into a vector space using a pre-trained text encoder such as BERT or RoBERTa to extract meaningful label semantics which in turn leads to a better feature-label alignment for improved classification performance. To demonstrate the effectiveness of our approach, we evaluate our model on three diverse HSI benchmark datasets - Hyperspectral Wood, HyperspectralBlueberries, and DeepHS-Fruit and report significant performance boost. Our results highlight the synergy between textual semantics and spectral-spatial data, paving the way for further advancements in semantically augmented HSI classification models. Codes are be available in: this https URL 

**Abstract (ZH)**: 超光谱成像（HSI）分类是广泛应用于农业、环境监测、医学和材料科学等多个领域的关键工具。由于高质量训练样本的有限可用性和光谱数据的高维度，HSI分类模型容易出现过拟合现象，并且在兼顾准确性和计算复杂性方面常常面临挑战。此外，大多数HSI分类模型都是单模态的，仅依赖光谱-空间数据来学习高维嵌入空间中的决策边界。为了解决这一问题，我们提出了一种通用的语义光谱-空间融合网络（S3FN），该网络利用上下文相关、类特定的文本描述来补充HSI分类模型的训练。具体而言，S3FN 利用大型语言模型（LLM）为每个类别标签生成全面的文本描述，捕捉它们的独特特征和光谱行为。这些描述通过预训练的文字编码器（如 BERT 或 RoBERTa）嵌入到向量空间中，提取有意义的标签语义，从而在提高分类性能方面实现更好的特征-标签对齐。为了验证我们方法的有效性，我们在三个不同的HSI基准数据集——超光谱木质材料、超光谱蓝莓和DeepHS-果物上评估了我们的模型，并报告了显著的性能提升。我们的结果突显了文本语义与光谱-空间数据之间的协同作用，为语义增强的HSI分类模型的进一步发展铺平了道路。代码可在以下链接获取：this https URL。 

---
# TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility 

**Title (ZH)**: TRAVL: 在物理不合现实性判断方面提升视频-语言模型的配方 

**Authors**: Saman Motamed, Minghao Chen, Luc Van Gool, Iro Laina  

**Link**: [PDF](https://arxiv.org/pdf/2510.07550)  

**Abstract**: Despite impressive visual fidelity, modern video generative models frequently produce sequences that violate intuitive physical laws, such as objects floating, teleporting, or morphing in ways that defy causality. While humans can easily detect such implausibilities, there remains no robust method for quantitatively assessing physical realism in video. In this work, we explore whether Video-Language Models (VLMs) can be trained to serve as reliable judges of physical plausibility. We find that existing VLMs struggle to identify physics violations, exposing fundamental limitations in their temporal and causal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe that combines a balanced training dataset with a trajectory-aware attention module to improve motion encoding and discrimination in VLMs. To evaluate physical reasoning more rigorously, we propose ImplausiBench, a benchmark of 300 videos (150 real, 150 generated) that removes linguistic biases and isolates visual-temporal understanding. Performance is reported both with gold-standard human judgments and stricter LLM-as-judge metrics. Together, TRAVL and ImplausiBench offer a unified framework for probing and improving physical plausibility in multimodal models, shedding light on a challenging and underexplored aspect of visual-temporal understanding. 

**Abstract (ZH)**: 尽管现代视频生成模型在视觉保真度方面表现出色，但它们时常生成违背直观物理法则的序列，如物体漂浮、瞬间移动或以违背因果关系的方式变形。虽然人类很容易察觉这种不合理性，但目前仍缺乏一种可靠的方法来定量评估视频的物理真实性。本文探讨视频-语言模型（VLMs）是否可以被训练成可靠的物理合理性评判者。我们发现现有的VLMs在识别物理违规方面表现出困难，揭示了其在时间因果推理方面的一些根本局限。为解决这一问题，我们引入了TRAVL，这是一种结合平衡训练数据集和轨迹感知注意力模块的微调方法，旨在提高VLMs的动作编码和鉴别能力。为了更严谨地评估物理推理能力，我们提出了ImplausiBench基准，该基准包含300个视频（包括150个真实视频和150个生成视频），去除了语言偏向，专注于视觉-时间理解。性能评估既基于标准的人类判断，也基于更为严格的LLM评委指标。总之，TRAVL和ImplausiBench提供了评估和提升 multimodal 模型物理合理性的统一框架，揭示了视觉-时间理解的一个具有挑战性和未充分探索的方面。 

---
# OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs 

**Title (ZH)**: OWL: 克服窗口长度依赖性以应对长上下文输入的 speculative 解码 

**Authors**: Jaeseong Lee, seung-won hwang, Aurick Qiao, Gabriele Oliaro, Ye Wang, Samyam Rajbhandari  

**Link**: [PDF](https://arxiv.org/pdf/2510.07535)  

**Abstract**: Speculative decoding promises faster inference for large language models (LLMs), yet existing methods fail to generalize to real-world settings. Benchmarks typically assume short contexts (e.g., 2K tokens), whereas practical workloads involve long contexts. We find current approaches degrade severely with long contexts; for instance, EAGLE3 even slows down the generation speed by 0.81x. We address these limitations by releasing a new long-context benchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves about 5x higher acceptance length than EAGLE3 on long-context inputs through three innovations: (1) an LSTM-based drafter conditioned only on the last-token state, making it generalize to various lengths, (2) a special token [SPEC] in the verifier that produces richer representation for drafter, and (3) a hybrid algorithm combining both tree and non-tree decoding methods. We release all code and datasets to advance future research. 

**Abstract (ZH)**: 投机解码有望加速大型语言模型的推理，但现有方法难以适应现实应用场景。现有基准通常假设短语境（例如，2K个标记），而实际工作负载涉及长语境。我们发现当前方法在长语境下表现严重下降；例如，EAGLE3甚至将生成速度降低了0.81倍。我们通过发布一个新的长语境基准（LongSpecBench）并引入一种新型模型（OWL）来解决这些局限性。OWL 通过三种创新在长语境输入上实现了约5倍的接受长度：（1）基于LSTM的仅依赖于最后一个标记状态的编者，使其能适应各种长度；（2）验证器中的特殊标记[SPEC]，为编者生成更丰富的表示；（3）结合树形和非树形解码方法的混合算法。我们发布了所有代码和数据集，以促进未来的研究。 

---
# EEG Sleep Stage Classification with Continuous Wavelet Transform and Deep Learning 

**Title (ZH)**: 基于连续小波变换和深度学习的EEG睡眠阶段分类 

**Authors**: Mehdi Zekriyapanah Gashti, Ghasem Farjamnia  

**Link**: [PDF](https://arxiv.org/pdf/2510.07524)  

**Abstract**: Accurate classification of sleep stages is crucial for the diagnosis and management of sleep disorders. Conventional approaches for sleep scoring rely on manual annotation or features extracted from EEG signals in the time or frequency domain. This study proposes a novel framework for automated sleep stage scoring using time-frequency analysis based on the wavelet transform. The Sleep-EDF Expanded Database (sleep-cassette recordings) was used for evaluation. The continuous wavelet transform (CWT) generated time-frequency maps that capture both transient and oscillatory patterns across frequency bands relevant to sleep staging. Experimental results demonstrate that the proposed wavelet-based representation, combined with ensemble learning, achieves an overall accuracy of 88.37 percent and a macro-averaged F1 score of 73.15, outperforming conventional machine learning methods and exhibiting comparable or superior performance to recent deep learning approaches. These findings highlight the potential of wavelet analysis for robust, interpretable, and clinically applicable sleep stage classification. 

**Abstract (ZH)**: 准确的睡眠阶段分类对于睡眠障碍的诊断和管理至关重要。传统的睡眠评分方法依赖于手动注释或从EEG信号的时间或频率域提取的特征。本研究提出了一种基于小波变换的时频分析框架，以实现自动化的睡眠阶段评分。采用睡眠-EDF扩展数据库（睡眠磁带记录）进行评估。连续小波变换（CWT）生成了时频图，捕捉了与睡眠阶段划分相关的频率带上的瞬态和振荡模式。实验结果表明，所提出的小波基表示方法结合集成学习实现了88.37%的整体准确率和73.15%的宏平均F1分数，优于传统的机器学习方法，并且在性能上与最近的深度学习方法相当或更优。这些发现突显了小波分析在实现稳健、可解释且临床可适用的睡眠阶段分类中的潜力。 

---
# MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis 

**Title (ZH)**: MLLM4TS：利用视觉和多模态语言模型进行通用时间序列分析 

**Authors**: Qinghua Liu, Sam Heshmati, Zheda Mai, Zubin Abraham, John Paparrizos, Liu Ren  

**Link**: [PDF](https://arxiv.org/pdf/2510.07513)  

**Abstract**: Effective analysis of time series data presents significant challenges due to the complex temporal dependencies and cross-channel interactions in multivariate data. Inspired by the way human analysts visually inspect time series to uncover hidden patterns, we ask: can incorporating visual representations enhance automated time-series analysis? Recent advances in multimodal large language models have demonstrated impressive generalization and visual understanding capability, yet their application to time series remains constrained by the modality gap between continuous numerical data and discrete natural language. To bridge this gap, we introduce MLLM4TS, a novel framework that leverages multimodal large language models for general time-series analysis by integrating a dedicated vision branch. Each time-series channel is rendered as a horizontally stacked color-coded line plot in one composite image to capture spatial dependencies across channels, and a temporal-aware visual patch alignment strategy then aligns visual patches with their corresponding time segments. MLLM4TS fuses fine-grained temporal details from the numerical data with global contextual information derived from the visual representation, providing a unified foundation for multimodal time-series analysis. Extensive experiments on standard benchmarks demonstrate the effectiveness of MLLM4TS across both predictive tasks (e.g., classification) and generative tasks (e.g., anomaly detection and forecasting). These results underscore the potential of integrating visual modalities with pretrained language models to achieve robust and generalizable time-series analysis. 

**Abstract (ZH)**: 基于多模态大型语言模型的复杂时间序列数据有效分析提出了显著挑战，因为多变量数据中的复杂时间依赖性和跨通道交互构成了难题。受人类分析师通过可视化方法检查时间序列以发现隐藏模式的启发，我们提出的问题是：将视觉表示融入到自动化时间序列分析中能否提高其效果？尽管多模态大型语言模型在泛化能力和视觉理解方面展现了令人印象深刻的性能，但它们在时间序列分析中的应用仍受到连续数值数据与离散自然语言之间模态差距的限制。为了解决这一问题，我们引入了MLLM4TS框架，该框架通过集成专用的视觉分支来利用多模态大型语言模型进行通用时间序列分析。每个时间序列通道在一张合成图像中以水平堆叠的色码线图形式呈现，以捕捉跨通道的空间依赖性，并采用时间aware的视觉补丁对齐策略将视觉补丁与相应的时序段对齐。MLLM4TS将数值数据中的细粒度时间细节与视觉表示中提取的全局上下文信息融合起来，为多模态时间序列分析提供了统一的基础。在标准基准上的广泛实验表明，MLLM4TS在预测任务（如分类）和生成任务（如异常检测和预测）中都表现出有效性。这些结果突显了将视觉模态与预训练语言模型结合以实现稳健和通用的时间序列分析的潜力。 

---
# When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs 

**Title (ZH)**: 当思维遇见事实：面向长上下文语言模型的可复用推理方法 

**Authors**: Soyeong Jeong, Taehee Jung, Sung Ju Hwang, Joo-Kyung Kim, Dongyeop Kang  

**Link**: [PDF](https://arxiv.org/pdf/2510.07499)  

**Abstract**: Recent Long-Context Language Models (LCLMs) can process hundreds of thousands of tokens in a single prompt, enabling new opportunities for knowledge-intensive multi-hop reasoning by integrating large sets of retrieved documents or, in some cases, directly all necessary information. However, simply feeding more documents into the context window fails to capture how evidence should be connected. We address this gap with thought templates, which recast reasoning as reusable thought caches, derived from prior problem solving traces, structuring how evidence is combined and guiding multi-hop inference with factual documents. To keep these templates effective, we propose an update strategy that iteratively refines templates derived from training data through natural-language feedback. Across diverse benchmarks and LCLM families, our approach delivers consistent gains over strong baselines in both retrieval-based and retrieval-free settings. Furthermore, we show that optimized templates can be distilled into smaller open-source models, demonstrating its broad applicability and transparent reasoning reuse. We refer to our framework as Thought Template Augmented LCLMs (ToTAL). 

**Abstract (ZH)**: Recent Long-Context Language Models with Thought Template Augmented Multi-Hop Reasoning 

---
# Can Speech LLMs Think while Listening? 

**Title (ZH)**: 说话的LLMs在倾听时能否思考？ 

**Authors**: Yi-Jen Shih, Desh Raj, Chunyang Wu, Wei Zhou, SK Bong, Yashesh Gaur, Jay Mahadeokar, Ozlem Kalinli, Mike Seltzer  

**Link**: [PDF](https://arxiv.org/pdf/2510.07497)  

**Abstract**: Recent advances in speech large language models (speech LLMs) have enabled seamless spoken interactions, but these systems still struggle with complex reasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning has been to shown to significantly improve the reasoning abilities of text-based LLMs. In this work, we investigate the effect of CoT fine-tuning for multi-stream speech LLMs, demonstrating that reasoning in text space improves the accuracy of speech LLMs by 2.4x, on average, over a suite of spoken reasoning tasks. Beyond accuracy, the latency of the spoken response is a crucial factor for interacting with voice-based agents. Inspired by the human behavior of "thinking while listening," we propose methods to reduce the additional latency from reasoning by allowing the model to start reasoning before the user query has ended. To achieve this, we introduce an entropy-based metric, "question completeness," which acts as an indicator to guide the model on the optimal time to start reasoning. This method provides greater control over the accuracy-latency trade-off compared with heuristic-based approaches and, under equivalent latency conditions, yields a 4% accuracy gain on ARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference data created using rejection sampling to push the accuracy-latency pareto frontier further, resulting in a 70% reduction in latency without loss in accuracy. 

**Abstract (ZH)**: 近期语音大语言模型（speech LLMs）的进展使得无缝语音交互成为可能，但这些系统在复杂推理任务上仍存在问题。以往的研究显示，链式思考（CoT）提示或微调可以显著提高文本基础大语言模型的推理能力。在本工作中，我们研究了CoT微调对多流语音大语言模型的影响，结果显示，在一系列语音推理任务中，文本空间的推理可以将语音大语言模型的准确率平均提高2.4倍。除了准确率，语音响应的延迟也是与语音代理交互的关键因素。受人类边听边思考的行为启发，我们提出了一种方法来减少推理带来的额外延迟，允许模型在用户查询结束之前就开始推理。为此，我们引入了一个基于熵的度量标准“问题完整性”，作为指标引导模型在最佳时刻开始推理。该方法相对于启发式方法提供了更大的准确率-延迟权衡控制能力，在同等延迟条件下，对于ARC-Easy任务可以获得4%的准确率提升。最后，我们使用拒绝采样生成的偏好数据进行直接偏好优化（DPO），进一步推动物准确率-延迟帕累托前沿，结果在不损失准确率的情况下将延迟降低了70%。 

---
# A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy 

**Title (ZH)**: 基于图像净化策略的实时超低剂量肺部CT图像去噪框架 

**Authors**: Guoliang Gong, Man Yu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07492)  

**Abstract**: Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but introduces severe noise and artifacts. It also leads to substantial spatial misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses challenges for directly applying existing denoising networks trained on synthetic noise or aligned data. To address this core challenge in uLDCT denoising, this paper proposes an innovative denoising framework based on an Image Purification (IP) strategy. First, we construct a real clinical uLDCT lung dataset. Then, we propose an Image Purification strategy that generates structurally aligned uLDCT-NDCT image pairs, providing a high-quality data foundation for network training. Building upon this, we propose a Frequency-domain Flow Matching (FFM) model, which works synergistically with the IP strategy to excellently preserve the anatomical structure integrity of denoised images. Experiments on the real clinical dataset demonstrate that our IP strategy significantly enhances the performance of multiple mainstream denoising models on the uLDCT task. Notably, our proposed FFM model combined with the IP strategy achieves state-of-the-art (SOTA) results in anatomical structure preservation. This study provides an effective solution to the data mismatch problem in real-world uLDCT denoising. Code and dataset are available at this https URL. 

**Abstract (ZH)**: 基于图像净化策略的超低剂量CT去噪框架 

---
# Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics 

**Title (ZH)**: 人类团队的经验能否应用于多agent系统？结构、多样性和交互动力学的作用 

**Authors**: Rasika Muralidharan, Jaewoon Kwak, Jisun An  

**Link**: [PDF](https://arxiv.org/pdf/2510.07488)  

**Abstract**: Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are gaining attention, yet fewer studies explore their team dynamics. Inspired by human team science, we propose a multi-agent framework to examine core aspects of team science: structure, diversity, and interaction dynamics. We evaluate team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and Latent Implicit Hate, spanning commonsense and social reasoning. Our results show that flat teams tend to perform better than hierarchical ones, while diversity has a nuanced impact. Interviews suggest agents are overconfident about their team performance, yet post-task reflections reveal both appreciation for collaboration and challenges in integration, including limited conversational coordination. 

**Abstract (ZH)**: 基于大型语言模型（LLM）的多智能体系统（MAS）及其团队动态研究：以结构、多样性和交互动力学为核心 

---
# HEMERA: A Human-Explainable Transformer Model for Estimating Lung Cancer Risk using GWAS Data 

**Title (ZH)**: HEMERA：一种用于估计肺癌风险的人类可解释的变压器模型（基于GWAS数据） 

**Authors**: Maria Mahbub, Robert J. Klein, Myvizhi Esai Selvan, Rowena Yip, Claudia Henschke, Providencia Morales, Ian Goethert, Olivera Kotevska, Mayanka Chandra Shekar, Sean R. Wilkinson, Eileen McAllister, Samuel M. Aguayo, Zeynep H. Gümüş, Ioana Danciu, VA Million Veteran Program  

**Link**: [PDF](https://arxiv.org/pdf/2510.07477)  

**Abstract**: Lung cancer (LC) is the third most common cancer and the leading cause of cancer deaths in the US. Although smoking is the primary risk factor, the occurrence of LC in never-smokers and familial aggregation studies highlight a genetic component. Genetic biomarkers identified through genome-wide association studies (GWAS) are promising tools for assessing LC risk. We introduce HEMERA (Human-Explainable Transformer Model for Estimating Lung Cancer Risk using GWAS Data), a new framework that applies explainable transformer-based deep learning to GWAS data of single nucleotide polymorphisms (SNPs) for predicting LC risk. Unlike prior approaches, HEMERA directly processes raw genotype data without clinical covariates, introducing additive positional encodings, neural genotype embeddings, and refined variant filtering. A post hoc explainability module based on Layer-wise Integrated Gradients enables attribution of model predictions to specific SNPs, aligning strongly with known LC risk loci. Trained on data from 27,254 Million Veteran Program participants, HEMERA achieved >99% AUC (area under receiver characteristics) score. These findings support transparent, hypothesis-generating models for personalized LC risk assessment and early intervention. 

**Abstract (ZH)**: 人类可解释变压器模型在GWAS数据中估计肺癌风险（.HEMERA：人类可解释变压器模型用于基于GWAS数据估计肺癌风险） 

---
# MoGU: Mixture-of-Gaussians with Uncertainty-based Gating for Time Series Forecasting 

**Title (ZH)**: MoGU：基于不确定性门控的混合高斯模型的时间序列预测 

**Authors**: Yoli Shavit, Jacob Goldberger  

**Link**: [PDF](https://arxiv.org/pdf/2510.07459)  

**Abstract**: We introduce Mixture-of-Gaussians with Uncertainty-based Gating (MoGU), a novel Mixture-of-Experts (MoE) framework designed for regression tasks and applied to time series forecasting. Unlike conventional MoEs that provide only point estimates, MoGU models each expert's output as a Gaussian distribution. This allows it to directly quantify both the forecast (the mean) and its inherent uncertainty (variance). MoGU's core innovation is its uncertainty-based gating mechanism, which replaces the traditional input-based gating network by using each expert's estimated variance to determine its contribution to the final prediction. Evaluated across diverse time series forecasting benchmarks, MoGU consistently outperforms single-expert models and traditional MoE setups. It also provides well-quantified, informative uncertainties that directly correlate with prediction errors, enhancing forecast reliability. Our code is available from: this https URL 

**Abstract (ZH)**: 基于不确定性门控的混合高斯模型（MoGU）：一种用于时间序列预测的新型混合专家框架 

---
# Minimizing the Value-at-Risk of Loan Portfolio via Deep Neural Networks 

**Title (ZH)**: 通过深度神经网络最小化贷款组合的价值-at-风险 

**Authors**: Albert Di Wang, Ye Du  

**Link**: [PDF](https://arxiv.org/pdf/2510.07444)  

**Abstract**: Risk management is a prominent issue in peer-to-peer lending. An investor may naturally reduce his risk exposure by diversifying instead of putting all his money on one loan. In that case, an investor may want to minimize the Value-at-Risk (VaR) or Conditional Value-at-Risk (CVaR) of his loan portfolio. We propose a low degree of freedom deep neural network model, DeNN, as well as a high degree of freedom model, DSNN, to tackle the problem. In particular, our models predict not only the default probability of a loan but also the time when it will default. The experiments demonstrate that both models can significantly reduce the portfolio VaRs at different confidence levels, compared to benchmarks. More interestingly, the low degree of freedom model, DeNN, outperforms DSNN in most scenarios. 

**Abstract (ZH)**: P2P借贷中的风险管理是一个突出的问题。投资者可以通过多元化而不是将所有资金集中在一笔贷款上来自然地降低风险暴露。在这种情况下，投资者可能希望最小化其贷款组合的价值-at-风险（VaR）或条件价值-at-风险（CVaR）。我们提出了一种低自由度的深度神经网络模型DeNN以及一种高自由度模型DSNN来解决这一问题。特别是，我们的模型不仅能预测一笔贷款的违约概率，还能预测其违约时间。实验结果表明，这两种模型都能在不同的置信水平下显著降低组合VaR，相较于基准模型。更有趣的是，在大多数情景下，低自由度模型DeNN表现优于DSNN。 

---
# LASER: An LLM-based ASR Scoring and Evaluation Rubric 

**Title (ZH)**: LASER：基于LLM的ASR评分与评估标准 

**Authors**: Amruta Parulekar, Preethi Jyothi  

**Link**: [PDF](https://arxiv.org/pdf/2510.07437)  

**Abstract**: Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly penalize morphological and syntactic nuances that do not significantly alter sentence semantics. We introduce an LLM-based scoring rubric LASER that leverages state-of-the-art LLMs' in-context learning abilities to learn from prompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro achieved a very high correlation score of 94% with human annotations. Hindi examples in the prompt were also effective in analyzing errors in other Indian languages such as Marathi, Kannada and Malayalam. We also demonstrate how a smaller LLM like Llama 3 can be finetuned on word-pair examples derived from reference and ASR predictions to predict what kind of penalty should be applied with close to 89% accuracy. 

**Abstract (ZH)**: 标准ASR评估指标如单词错误率(WER)往往会不公平地惩罚不影响句子语义的形态和句法细微差别。我们引入了一种基于LLM的评分准则LASER，利用先进LLM的上下文学习能力从包含详细示例的提示中学习。使用Gemini 2.5 Pro对Hindi进行LASER评分，实现了与人工注释非常高的相关性（94%）。提示中的Hindi示例还有效地分析了其他印度语言如马拉地语、卡纳达语和马拉雅拉姆语的错误。我们还演示了如何通过微调较小的LLM如Llama 3来预测由于参考和ASR预测中的词对示例而应施加的惩罚，准确率接近89%。 

---
# Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation 

**Title (ZH)**: 堆叠工程：异质性和自主性长上下文评估的背景工程 

**Authors**: Mufei Li, Dongqi Fu, Limei Wang, Si Zhang, Hanqing Zeng, Kaan Sancak, Ruizhong Qiu, Haoyu Wang, Xiaoxin He, Xavier Bresson, Yinglong Xia, Chonglin Sun, Pan Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.07414)  

**Abstract**: Modern long-context large language models (LLMs) perform well on synthetic "needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy contexts arise from biased retrieval and agentic workflows. We argue that haystack engineering is necessary to construct noisy long contexts that faithfully capture key real-world factors -- distraction from heterogeneous biased retrievers and cascading errors in agentic workflows -- to test models' long-context robustness. We instantiate it through HaystackCraft, a new NIAH benchmark built on the full English Wikipedia hyperlink network with multi-hop questions. HaystackCraft evaluates how heterogeneous retrieval strategies (e.g., sparse, dense, hybrid, and graph-based) affect distractor composition, haystack ordering, and downstream LLM performance. HaystackCraft further extends NIAH to dynamic, LLM-dependent settings that simulate agentic operations, where models refine queries, reflect on their past reasonings, and decide when to stop. Experiments with 15 long-context models show that (1) while stronger dense retrievers can introduce more challenging distractors, graph-based reranking simultaneously improves retrieval effectiveness and mitigates more harmful distractors; (2) in agentic tests, even advanced models like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated distractors or struggle to perform early stops. These results highlight persistent challenges in agentic long-context reasoning and establish HaystackCraft as a valuable testbed for future progress. 

**Abstract (ZH)**: 现代长上下文大型语言模型在合成“针haystack中”的基准测试中表现良好，但此类测试忽视了噪声上下文源自有偏检索和自主工作流程的情况。我们argue了需要进行haystack工程以构建真实世界关键因素忠实反映的噪声长上下文——包括异质有偏检索带来的干扰和自主工作流程中的级联错误——以测试模型的长上下文鲁棒性。我们通过HaystackCraft实现这一目标，这是一个基于完整英文维基链网络和多跳问题的新合成“针haystack”基准。HaystackCraft评估不同的检索策略（如稀疏、密集、混合和图基）对干扰组成、haystack排序以及下游大规模语言模型性能的影响。进一步地，HaystackCraft将“针haystack”扩展到动态、依赖大型语言模型的场景中，模拟自主操作，其中模型调整查询、反思其过往推理，并决定何时停止。在15个长上下文模型的实验中，结果显示（1）更强的密集检索器虽然引入了更具挑战性的干扰，但基于图的重排序同时提高了检索效果并减轻了更严重的干扰；（2）在自主测试中，即使是先进的模型如Gemini 2.5 Pro和GPT-5也因自动生成的干扰或难以进行早期停止而遭受级联失败。这些结果突显了自主长上下文推理中存在的持续挑战，并将HaystackCraft确立为未来进展的重要测试平台。 

---
# Quantum Grid Path Planning Using Parallel QAOA Circuits Based on Minimum Energy Principle 

**Title (ZH)**: 基于最小能量原则的并行QAOA电路量子网格路径规划 

**Authors**: Jun Liu  

**Link**: [PDF](https://arxiv.org/pdf/2510.07413)  

**Abstract**: To overcome the bottleneck of classical path planning schemes in solving NP problems and address the predicament faced by current mainstream quantum path planning frameworks in the Noisy Intermediate-Scale Quantum (NISQ) era, this study attempts to construct a quantum path planning solution based on parallel Quantum Approximate Optimization Algorithm (QAOA) architecture. Specifically, the grid path planning problem is mapped to the problem of finding the minimum quantum energy state. Two parallel QAOA circuits are built to simultaneously execute two solution processes, namely connectivity energy calculation and path energy calculation. A classical algorithm is employed to filter out unreasonable solutions of connectivity energy, and finally, the approximate optimal solution to the path planning problem is obtained by merging the calculation results of the two parallel circuits. The research findings indicate that by setting appropriate filter parameters, quantum states corresponding to position points with extremely low occurrence probabilities can be effectively filtered out, thereby increasing the probability of obtaining the target quantum state. Even when the circuit layer number p is only 1, the theoretical solution of the optimal path coding combination can still be found by leveraging the critical role of the filter. Compared with serial circuits, parallel circuits exhibit a significant advantage, as they can find the optimal feasible path coding combination with the highest probability. 

**Abstract (ZH)**: 针对经典路径规划方案在解决NP问题时遇到的瓶颈以及当前主流量子路径规划框架在Noisy Intermediate-Scale Quantum (NISQ) 时代面临的困境，本研究尝试基于并行量子近似优化算法（QAOA）架构构建一种量子路径规划解决方案。具体而言，将网格路径规划问题映射为寻找最低量子能态的问题。构建两个并行的QAOA电路，同时执行连接能计算和路径能计算两个解决方案过程。采用经典算法对连接能的不合理解进行筛选，最终通过合并两个并行电路的计算结果获得路径规划问题的近似最优解。研究结果表明，通过设置合适的筛选参数，可以有效滤除发生概率极低的位置点对应的量子态，从而提高获得目标量子态的概率。即使在电路层数p仅为1的情况下，通过充分发挥筛选的 critical 角色，仍可以找到最优路径编码组合的理论解决方案。与串行电路相比，并行电路展现出显著优势，能够以最高概率找到最优可行路径编码组合。 

---
# Attention to Order: Transformers Discover Phase Transitions via Learnability 

**Title (ZH)**: 关注顺序：Transformer通过可学习性发现相变 

**Authors**: Şener Özönder  

**Link**: [PDF](https://arxiv.org/pdf/2510.07401)  

**Abstract**: Phase transitions mark qualitative reorganizations of collective behavior, yet identifying their boundaries remains challenging whenever analytic solutions are absent and conventional simulations fail. Here we introduce learnability as a universal criterion, defined as the ability of a transformer model containing attention mechanism to extract structure from microscopic states. Using self-supervised learning and Monte Carlo generated configurations of the two-dimensional Ising model, we show that ordered phases correspond to enhanced learnability, manifested in both reduced training loss and structured attention patterns, while disordered phases remain resistant to learning. Two unsupervised diagnostics, the sharp jump in training loss and the rise in attention entropy, recover the critical temperature in excellent agreement with the exact value. Our results establish learnability as a data-driven marker of phase transitions and highlight deep parallels between long-range order in condensed matter and the emergence of structure in modern language models. 

**Abstract (ZH)**: 相变标记集体行为的质变重组，但在缺乏解析解且常规模拟失效时，确定其边界依然具有挑战性。在这里我们引入可学习性作为普适标准，定义为包含注意机制的变换器模型从微观状态中提取结构的能力。使用自监督学习和蒙特卡洛生成的二维伊辛模型配置，我们证明有序相对应于增强的可学习性，表现为训练损失降低和受结构化的注意力模式，而无序相则对学习保持抵抗。两种无监督诊断，训练损失的sharp跃变和注意力熵的上升，能够以极佳的精度恢复临界温度，与准确值吻合良好。我们的结果确立了可学习性作为相变的数据驱动标志，并强调了凝聚物质中的长程有序与现代语言模型中结构的涌现之间的深刻相似性。 

---
# Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts 

**Title (ZH)**: 编码、思考、解码：递归潜在思想扩展测试时推理规模 

**Authors**: Yeskendir Koishekenov, Aldo Lipani, Nicola Cancedda  

**Link**: [PDF](https://arxiv.org/pdf/2510.07358)  

**Abstract**: Most efforts to improve the reasoning capabilities of large language models (LLMs) involve either scaling the number of parameters and the size of training data, or scaling inference computation by letting models generate complex chains of thought. Motivated by interpretability studies showing that the crucial computation required for reasoning tasks is concentrated in a limited range of layers, we introduce Encode-Think-Decode (ETD), a method that enhances the reasoning capabilities of a base model by training it to iterate over a small subset of reasoning-relevant layers during the mid-training stage. ETD amplifies latent reasoning while preserving the original architecture, parameter count, hyperparameters, and training data composition. When iterating on the selected layers at inference time, ETD models yield substantial gains on 17 reasoning benchmarks, including +28.4% relative accuracy improvement on GSM8K and +36% on MATH with the OLMo-2 1B Base model. We also explore an adaptive depth strategy that adjusts the computation per input token. Our results show that recursive latent reasoning offers a simple and effective path to stronger LLM reasoning. 

**Abstract (ZH)**: 增强推理能力的方法：mid训练阶段训练基模型逐层迭代以提升推理能力 

---
# Mitigating Surgical Data Imbalance with Dual-Prediction Video Diffusion Model 

**Title (ZH)**: 使用双预测视频扩散模型缓解手术数据不平衡问题 

**Authors**: Danush Kumar Venkatesh, Adam Schmidt, Muhammad Abdullah Jamal, Omid Mohareri  

**Link**: [PDF](https://arxiv.org/pdf/2510.07345)  

**Abstract**: Surgical video datasets are essential for scene understanding, enabling procedural modeling and intra-operative support. However, these datasets are often heavily imbalanced, with rare actions and tools under-represented, which limits the robustness of downstream models. We address this challenge with $SurgiFlowVid$, a sparse and controllable video diffusion framework for generating surgical videos of under-represented classes. Our approach introduces a dual-prediction diffusion module that jointly denoises RGB frames and optical flow, providing temporal inductive biases to improve motion modeling from limited samples. In addition, a sparse visual encoder conditions the generation process on lightweight signals (e.g., sparse segmentation masks or RGB frames), enabling controllability without dense annotations. We validate our approach on three surgical datasets across tasks including action recognition, tool presence detection, and laparoscope motion prediction. Synthetic data generated by our method yields consistent gains of 10-20% over competitive baselines, establishing $SurgiFlowVid$ as a promising strategy to mitigate data imbalance and advance surgical video understanding methods. 

**Abstract (ZH)**: 手术视频数据集对于场景理解、程序建模和术中支持至关重要。然而，这些数据集往往严重失衡，稀有动作和工具的代表性不足，这限制了下游模型的鲁棒性。我们通过提出一种稀疏可控的视频扩散框架$SurgiFlowVid$来应对这一挑战，该框架能够生成代表性不足类别的手术视频。我们的方法引入了双预测扩散模块，该模块联合去噪RGB帧和光学流，提供时间上的归纳偏差，以提高从有限样本中建模运动的能力。此外，稀疏视觉编码器通过轻量级信号（如稀疏分割掩码或RGB帧）来条件生成过程，从而在无需密集标注的情况下实现可控性。我们在三个手术数据集上对包括动作识别、工具存在检测和腹腔镜运动预测等任务进行了验证。由我们的方法生成的合成数据在多个基准上的表现提高了10-20%，这表明$SurgiFlowVid$是一种有前景的策略，可以缓解数据不平衡问题并推进手术视频理解方法。 

---
# Local MAP Sampling for Diffusion Models 

**Title (ZH)**: 局部MAP采样用于扩散模型 

**Authors**: Shaorong Zhang, Rob Brekelmans, Greg Ver Steeg  

**Link**: [PDF](https://arxiv.org/pdf/2510.07343)  

**Abstract**: Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. Building on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $>1.5$ dB improvements on inverse scattering benchmarks. 

**Abstract (ZH)**: 局部最大后验采样（LMAPS）提供了一种新的推理框架，通过沿扩散轨迹迭代求解局部MAP子问题，从而澄清了这些方法与全局MAP估计和DPS之间的联系，为基于优化的方法提供了一致的概率解释。 

---
# MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation 

**Title (ZH)**: MultiFair: 多模态平衡公平性感知医学分类的双层次梯度调制 

**Authors**: Md Zubair, Hao Zheng, Nussdorf Jonathan, Grayson W. Armstrong, Lucy Q. Shen, Gabriela Wilson, Yu Tian, Xingquan Zhu, Min Shi  

**Link**: [PDF](https://arxiv.org/pdf/2510.07328)  

**Abstract**: Medical decision systems increasingly rely on data from multiple sources to ensure reliable and unbiased diagnosis. However, existing multimodal learning models fail to achieve this goal because they often ignore two critical challenges. First, various data modalities may learn unevenly, thereby converging to a model biased towards certain modalities. Second, the model may emphasize learning on certain demographic groups causing unfair performances. The two aspects can influence each other, as different data modalities may favor respective groups during optimization, leading to both imbalanced and unfair multimodal learning. This paper proposes a novel approach called MultiFair for multimodal medical classification, which addresses these challenges with a dual-level gradient modulation process. MultiFair dynamically modulates training gradients regarding the optimization direction and magnitude at both data modality and group levels. We conduct extensive experiments on two multimodal medical datasets with different demographic groups. The results show that MultiFair outperforms state-of-the-art multimodal learning and fairness learning methods. 

**Abstract (ZH)**: 多模态医学分类中的MultiFair方法：解决数据不均衡和公平性问题 

---
# Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children 

**Title (ZH)**: 基于深度学习的方法以增强自闭症儿童情绪和行为模式识别 

**Authors**: Nelaka K.A.R, Peiris M.K.V, Liyanage R.P.B  

**Link**: [PDF](https://arxiv.org/pdf/2510.07320)  

**Abstract**: Autism Spectrum Disorder significantly influences the communication abilities, learning processes, behavior, and social interactions of individuals. Although early intervention and customized educational strategies are critical to improving outcomes, there is a pivotal gap in understanding and addressing nuanced behavioral patterns and emotional identification in autistic children prior to skill development. This extended research delves into the foundational step of recognizing and mapping these patterns as a prerequisite to improving learning and soft skills. Using a longitudinal approach to monitor emotions and behaviors, this study aims to establish a baseline understanding of the unique needs and challenges faced by autistic students, particularly in the Information Technology domain, where opportunities are markedly limited. Through a detailed analysis of behavioral trends over time, we propose a targeted framework for developing applications and technical aids designed to meet these identified needs. Our research underscores the importance of a sequential and evidence-based intervention approach that prioritizes a deep understanding of each child's behavioral and emotional landscape as the basis for effective skill development. By shifting the focus toward early identification of behavioral patterns, we aim to foster a more inclusive and supportive learning environment that can significantly improve the educational and developmental trajectory of children with ASD. 

**Abstract (ZH)**: 自闭症谱系障碍显著影响个体的沟通能力、学习过程、行为和社会互动。尽管早期干预和定制化教育策略对改善结果至关重要，但在技能发展之前理解和解决自闭症儿童细微的行为模式和情绪识别方面仍存在关键缺口。本扩展研究致力于通过识别和绘制这些模式，为改善学习和软技能奠定基础。采用纵向方法监测情绪和行为，本研究旨在建立对自闭学生独特需求和挑战的基础理解，特别是在信息技术领域，机会明显受限。通过对时间序列的行为趋势进行详细分析，我们提出了一个针对这些识别需求开发应用和技术辅助的定向框架。研究表明，采用顺序性和基于证据的干预方法，优先了解每个孩子的行为和情感环境对于有效技能发展至关重要。通过转向早期识别行为模式，我们旨在培养一个更具包容性和支持性的学习环境，从而显著改善自闭症谱系障碍儿童的教育和发展轨迹。 

---
# DUA-D2C: Dynamic Uncertainty Aware Method for Overfitting Remediation in Deep Learning 

**Title (ZH)**: DUA-D2C：动态不确定性意识方法在深度学习中防治过拟合 

**Authors**: Md. Saiful Bari Siddiqui, Md Mohaiminul Islam, Md. Golam Rabiul Alam  

**Link**: [PDF](https://arxiv.org/pdf/2411.15876)  

**Abstract**: Overfitting remains a significant challenge in deep learning, often arising from data outliers, noise, and limited training data. To address this, the Divide2Conquer (D2C) method was previously proposed, which partitions training data into multiple subsets and trains identical models independently on each. This strategy enables learning more consistent patterns while minimizing the influence of individual outliers and noise. However, D2C's standard aggregation typically treats all subset models equally or based on fixed heuristics (like data size), potentially underutilizing information about their varying generalization capabilities. Building upon this foundation, we introduce Dynamic Uncertainty-Aware Divide2Conquer (DUA-D2C), an advanced technique that refines the aggregation process. DUA-D2C dynamically weights the contributions of subset models based on their performance on a shared validation set, considering both accuracy and prediction uncertainty. This intelligent aggregation allows the central model to preferentially learn from subsets yielding more generalizable and confident edge models, thereby more effectively combating overfitting. Empirical evaluations on benchmark datasets spanning multiple domains demonstrate that DUA-D2C significantly improves generalization. Our analysis includes evaluations of decision boundaries, loss curves, and other performance metrics, highlighting the effectiveness of DUA-D2C. This study demonstrates that DUA-D2C improves generalization performance even when applied on top of other regularization methods, establishing it as a theoretically grounded and effective approach to combating overfitting in modern deep learning. Our codes are publicly available at: this https URL. 

**Abstract (ZH)**: Dynamic Uncertainty-Aware Divide2Conquer for Improving Generalization in Deep Learning 

---
