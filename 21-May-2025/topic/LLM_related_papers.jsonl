{'arxiv_id': 'arXiv:2505.13921', 'title': 'APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight', 'authors': 'Wanjing Huang, Weixiang Yan, Zhen Zhang, Ambuj Singh', 'link': 'https://arxiv.org/abs/2505.13921', 'abstract': 'Large Language Models (LLMs) demonstrate strong reasoning and task planning capabilities but remain fundamentally limited in physical interaction modeling. Existing approaches integrate perception via Vision-Language Models (VLMs) or adaptive decision-making through Reinforcement Learning (RL), but they fail to capture dynamic object interactions or require task-specific training, limiting their real-world applicability. We introduce APEX (Anticipatory Physics-Enhanced Execution), a framework that equips LLMs with physics-driven foresight for real-time task planning. APEX constructs structured graphs to identify and model the most relevant dynamic interactions in the environment, providing LLMs with explicit physical state updates. Simultaneously, APEX provides low-latency forward simulations of physically feasible actions, allowing LLMs to select optimal strategies based on predictive outcomes rather than static observations. We evaluate APEX on three benchmarks designed to assess perception, prediction, and decision-making: (1) Physics Reasoning Benchmark, testing causal inference and object motion prediction; (2) Tetris, evaluating whether physics-informed prediction enhances decision-making performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance, assessing the immediate integration of perception and action feasibility analysis. APEX significantly outperforms standard LLMs and VLM-based models, demonstrating the necessity of explicit physics reasoning for bridging the gap between language-based intelligence and real-world task execution. The source code and experiment setup are publicly available at this https URL .', 'abstract_zh': '基于物理预见的大语言模型实时任务规划框架：APEX', 'title_zh': 'APEX: 通过基于物理的任务规划赋能LLMs以实现实时洞察'}
{'arxiv_id': 'arXiv:2505.13729', 'title': 'SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation', 'authors': 'Abhinav Rajvanshi, Pritish Sahu, Tixiao Shan, Karan Sikka, Han-Pang Chiu', 'link': 'https://arxiv.org/abs/2505.13729', 'abstract': "Adaptive collaboration is critical to a team of autonomous robots to perform complicated navigation tasks in large-scale unknown environments. An effective collaboration strategy should be determined and adapted according to each robot's skills and current status to successfully achieve the shared goal. We present SayCoNav, a new approach that leverages large language models (LLMs) for automatically generating this collaboration strategy among a team of robots. Building on the collaboration strategy, each robot uses the LLM to generate its plans and actions in a decentralized way. By sharing information to each other during navigation, each robot also continuously updates its step-by-step plans accordingly. We evaluate SayCoNav on Multi-Object Navigation (MultiON) tasks, that require the team of the robots to utilize their complementary strengths to efficiently search multiple different objects in unknown environments. By validating SayCoNav with varied team compositions and conditions against baseline methods, our experimental results show that SayCoNav can improve search efficiency by at most 44.28% through effective collaboration among heterogeneous robots. It can also dynamically adapt to the changing conditions during task execution.", 'abstract_zh': '自适应协作对于自主机器人团队在大型未知环境中的复杂导航任务至关重要。一种有效的协作策略应根据每个机器人的技能和当前状态进行确定和调整，以便成功实现共同目标。我们提出SayCoNav，一种利用大型语言模型（LLMs）自动为机器人团队生成协作策略的新方法。基于此协作策略，每台机器人使用LLM以去中心化的方式生成其计划和行动。在导航过程中，通过相互共享信息，每台机器人也会相应地不断更新其逐步计划。我们通过Multi-Object Navigation（MultiON）任务评估SayCoNav，这些任务要求机器人团队利用各自的互补优势高效搜索多个未知环境中的不同物体。通过在变化的团队组成和条件下与基准方法进行验证，实验结果表明，SayCoNav可以通过有效的异质机器人协作提高搜索效率最多44.28%，并且还可以在任务执行过程中动态适应变化的条件。', 'title_zh': 'SayCoNav: 利用大型语言模型实现分散多机器人导航中的自适应协作'}
{'arxiv_id': 'arXiv:2505.13497', 'title': 'LODGE: Joint Hierarchical Task Planning and Learning of Domain Models with Grounded Execution', 'authors': 'Claudius Kienle, Benjamin Alt, Oleg Arenz, Jan Peters', 'link': 'https://arxiv.org/abs/2505.13497', 'abstract': 'Large Language Models (LLMs) enable planning from natural language instructions using implicit world knowledge, but often produce flawed plans that require refinement. Instead of directly predicting plans, recent methods aim to learn a problem domain that can be solved for different goal states using classical planners. However, these approaches require significant human feedback to obtain useful models. We address this shortcoming by learning hierarchical domains, where low-level predicates and actions are composed into higher-level counterparts, and by leveraging simulation to validate their preconditions and effects. This hierarchical approach is particularly powerful for long-horizon planning, where LLM-based planning approaches typically struggle. Furthermore, we introduce a central error reasoner to ensure consistency among the different planning levels. Evaluation on two challenging International Planning Competition (IPC) domains and a long-horizon robot manipulation task demonstrates higher planning success rates than state-of-the-art domain synthesis and LLM-modulo planning methods, while constructing high-quality models of the domain. Resources, videos and detailed experiment results are available at this https URL.', 'abstract_zh': '大型语言模型通过隐含的世界知识从自然语言指令中进行规划，但常常会产生需要改进的规划方案。最近的方法致力于学习可以解决不同目标状态的经典规划问题领域，而不是直接预测规划方案。然而，这些方法需要大量的人工反馈以获得有用模型。我们通过学习层次化领域来解决这一不足，其中低级谓词和动作组合成高级对应物，并利用模拟来验证其前提条件和效果。这种层次化的方法特别适用于长时规划，而基于大型语言模型的规划方法在长时规划中通常面临挑战。此外，我们引入了一个中心错误推理器以确保不同规划层次之间的一致性。在两个具有挑战性的国际规划竞赛（IPC）领域和一个长时机器人操作任务上的评估表明，我们的方法在规划成功率方面优于最新的领域合成和基于大型语言模型的规划方法，同时构建了高质量的领域模型。更多信息、视频及详细的实验结果请访问 <https://>。', 'title_zh': 'LODGE: 联合层次化任务规划与接地执行领域模型学习'}
{'arxiv_id': 'arXiv:2505.14681', 'title': 'Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training', 'authors': 'Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, Dong Yu', 'link': 'https://arxiv.org/abs/2505.14681', 'abstract': "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.", 'abstract_zh': '大型推理模型（LRMs）中的专家混合（MoE）架构通过选择性激活专家以促进结构化的认知过程，实现了令人印象深刻的推理能力。尽管取得了显著进展，现有推理模型往往仍存在过度推理和欠推理的认知效率问题。为解决这些局限，我们提出了一种名为Reinforcing Cognitive Experts（RICE）的新颖推理时控制方法，旨在在无需额外训练或复杂启发式方法的情况下改进推理性能。通过利用归一化的点互信息（nPMI），我们系统地识别出专门化的专家，称为“认知专家”，它们协调以“<think>”等标记为特征的元级推理操作。在严格的定量和科学推理基准测试中，与主流的MoE基LRMs（DeepSeek-R1和Qwen3-235B）进行的实证评估表明，在推理准确性、认知效率和跨领域泛化方面均取得了显著且一致的改进。尤为重要的是，我们的轻量级方法在推理控制技术（如提示设计和解码约束）中表现优异，同时保留了模型的一般指令遵循能力。这些结果突出了加强认知专家作为提高先进推理模型中认知效率的有前景、实用和可解释方向。', 'title_zh': '仅需两位专家即可引导思考：在MoE推理模型中强化认知努力，无需额外训练'}
{'arxiv_id': 'arXiv:2505.14668', 'title': 'ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions', 'authors': 'Bufang Yang, Lilin Xu, Liekang Zeng, Kaiwei Liu, Siyang Jiang, Wenrui Lu, Hongkai Chen, Xiaofan Jiang, Guoliang Xing, Zhenyu Yan', 'link': 'https://arxiv.org/abs/2505.14668', 'abstract': 'Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts to enhance the proactive capabilities of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and the persona contexts from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants.', 'abstract_zh': 'Recent Advances in Large Language Models (LLMs) Have Transformed Intelligent Agents from Reactive Responses to Proactive Support: Introducing ContextAgent, the First Context-Aware Proactive Agent', 'title_zh': 'ContextAgent: 具有开放世界感知能力的上下文感知主动大语言模型代理'}
{'arxiv_id': 'arXiv:2505.14656', 'title': 'Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning', 'authors': 'Zihao Zhang, Fei Liu', 'link': 'https://arxiv.org/abs/2505.14656', 'abstract': 'While LLMs excel at open-ended reasoning, they often struggle with cost-sensitive planning, either treating all actions as having equal cost or failing to stay within strict budgets. In this paper, we introduce Cost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings explicit cost-awareness into LLM-guided planning. Tight cost constraints push the planner to quickly identify infeasible solutions, while looser constraints encourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1, Claude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their performance in cost-sensitive scenarios. Our experiments suggest that raw LLMs such as GPT-4.1 often falter under tight budgets, whereas CATS consistently delivers strong performance, achieving higher task success rates and better cost efficiency. CATS provides an effective solution for budget-aware decision-making by combining the reasoning power of LLMs with structured search.', 'abstract_zh': 'Cost-Augmented Monte Carlo Tree Search for Cost-Aware Planning with Large Language Models', 'title_zh': '成本增强的蒙特卡洛树搜索方法在大语言模型辅助规划中的应用'}
{'arxiv_id': 'arXiv:2505.14615', 'title': "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas", 'authors': 'Anjiang Wei, Yuheng Wu, Yingjia Wan, Tarun Suresh, Huanmi Tan, Zhanke Zhou, Sanmi Koyejo, Ke Wang, Alex Aiken', 'link': 'https://arxiv.org/abs/2505.14615', 'abstract': 'We introduce SATBench, a benchmark for evaluating the logical reasoning capabilities of large language models (LLMs) through logical puzzles derived from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on inference rule-based reasoning, which often involves deducing conclusions from a set of premises, our approach leverages the search-based nature of SAT problems, where the objective is to find a solution that fulfills a specified set of logical constraints. Each instance in SATBench is generated from a SAT formula, then translated into a story context and conditions using LLMs. The generation process is fully automated and allows for adjustable difficulty by varying the number of clauses. All 2100 puzzles are validated through both LLM-assisted and solver-based consistency checks, with human validation on a subset. Experimental results show that even the strongest model, o4-mini, achieves only 65.0% accuracy on hard UNSAT problems, close to the random baseline of 50%. SATBench exposes fundamental limitations in the search-based logical reasoning abilities of current LLMs and provides a scalable testbed for future research in logical reasoning.', 'abstract_zh': '我们介绍了SATBench，这是一个通过布尔可满足性（SAT）问题衍生的逻辑谜题来评估大型语言模型（LLMs）逻辑推理能力的基准。与先前侧重于基于推理规则的推理工作不同，我们的方法利用了SAT问题的搜索特性，目标是找到满足指定逻辑约束的解决方案。SATBench 中的每个实例都源自一个 SAT 公式，然后通过语言模型转换为故事背景和条件。生成过程完全自动化，并可通过改变子句的数量调整难度。2100 个谜题均通过语言模型辅助和求解器验证的一致性检查，其中部分通过人工验证。实验结果显示，即使是最强的模型o4-mini，在解决困难的 UNSAT 问题时也仅能达到65.0% 的准确性，接近随机基线的50%。SATBench暴露出当前LLMs在基于搜索的逻辑推理能力上的根本局限，并提供了一个可扩展的测试平台，用于未来逻辑推理研究。', 'title_zh': 'SATBench：通过从SAT公式自动生成谜题来评估LLMs的逻辑推理能力'}
{'arxiv_id': 'arXiv:2505.14604', 'title': 'Let LLMs Break Free from Overthinking via Self-Braking Tuning', 'authors': 'Haoran Zhao, Yuchen Yan, Yongliang Shen, Haolei Xu, Wenqi Zhang, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, Yueting Zhuang', 'link': 'https://arxiv.org/abs/2505.14604', 'abstract': 'Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have significantly enhanced their reasoning capabilities by generating longer chains of thought, demonstrating outstanding performance across a variety of tasks. However, this performance gain comes at the cost of a substantial increase in redundant reasoning during the generation process, leading to high computational overhead and exacerbating the issue of overthinking. Although numerous existing approaches aim to address the problem of overthinking, they often rely on external interventions. In this paper, we propose a novel framework, Self-Braking Tuning (SBT), which tackles overthinking from the perspective of allowing the model to regulate its own reasoning process, thus eliminating the reliance on external control mechanisms. We construct a set of overthinking identification metrics based on standard answers and design a systematic method to detect redundant reasoning. This method accurately identifies unnecessary steps within the reasoning trajectory and generates training signals for learning self-regulation behaviors. Building on this foundation, we develop a complete strategy for constructing data with adaptive reasoning lengths and introduce an innovative braking prompt mechanism that enables the model to naturally learn when to terminate reasoning at an appropriate point. Experiments across mathematical benchmarks (AIME, AMC, MATH500, GSM8K) demonstrate that our method reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models.', 'abstract_zh': '大型推理模型（LRMs）如OpenAI o1和DeepSeek-R1通过生成更长的推理链条显著增强了其推理能力，并在多种任务中展示了出色的表现。然而，这一性能提升是以推理过程中大量冗余计算为代价的，导致了高昂的计算开销并加剧了过度推理的问题。尽管已有许多方法试图解决过度推理问题，但它们通常依赖于外部干预。本文提出了一种新颖的框架——自我制动微调（SBT），从允许模型自主调节其推理过程的角度出发，从而消除对外部控制机制的依赖。我们基于标准答案构建了一套过度推理识别指标，并设计了一种系统的方法来检测冗余推理。该方法能够准确识别推理轨迹中的不必要的步骤，并生成用于学习自我调节行为的训练信号。在此基础上，我们开发了一种完整的策略来构建具有自适应推理长度的数据，并引入了一种创新的制动提示机制，使模型能够自然地学习在适当的时候终止推理。跨数学基准测试（AIME、AMC、MATH500、GSM8K）的实验表明，我们的方法在保持与未约束模型相当的准确性的前提下，可将令牌消耗降低高达60%。', 'title_zh': '让大语言模型通过自我刹车调优摆脱过度思考'}
{'arxiv_id': 'arXiv:2505.14603', 'title': 'Towards a Foundation Model for Communication Systems', 'authors': 'Davide Buffelli, Sowmen Das, Yu-Wei Lin, Sattar Vakili, Chien-Yi Wang, Masoud Attarifar, Pritthijit Nath, Da-shan Shiu', 'link': 'https://arxiv.org/abs/2505.14603', 'abstract': 'Artificial Intelligence (AI) has demonstrated unprecedented performance across various domains, and its application to communication systems is an active area of research. While current methods focus on task-specific solutions, the broader trend in AI is shifting toward large general models capable of supporting multiple applications. In this work, we take a step toward a foundation model for communication data--a transformer-based, multi-modal model designed to operate directly on communication data. We propose methodologies to address key challenges, including tokenization, positional embedding, multimodality, variable feature sizes, and normalization. Furthermore, we empirically demonstrate that such a model can successfully estimate multiple features, including transmission rank, selected precoder, Doppler spread, and delay profile.', 'abstract_zh': '人工智能（AI）已在各种领域展现了前所未有的性能，其在通信系统中的应用是研究成果的活跃领域。尽管当前的方法主要集中在特定任务的解决方案上，更广泛的AI趋势正转向能够支持多种应用的大规模通用模型。在本项工作中，我们向通信数据的基础模型迈出了一步——一个基于变换器的多模态模型，旨在直接处理通信数据。我们提出了一种方法论来应对关键挑战，包括分词化、位置嵌入、多模态性、可变特征大小和标准化。此外，我们通过实验证明，这种模型能够成功估计多个特征，包括传输秩、选择的预编码器、多普勒扩展和延迟谱型。', 'title_zh': '面向通信系统的基础模型研究'}
{'arxiv_id': 'arXiv:2505.14524', 'title': 'Guarded Query Routing for Large Language Models', 'authors': 'Richard Šléher, William Brach, Tibor Sloboda, Kristián Košťál, Lukas Galke', 'link': 'https://arxiv.org/abs/2505.14524', 'abstract': 'Query routing, the task to route user queries to different large language model (LLM) endpoints, can be considered as a text classification problem. However, out-of-distribution queries must be handled properly, as those could be questions about unrelated domains, queries in other languages, or even contain unsafe text. Here, we thus study a \\emph{guarded} query routing problem, for which we first introduce the Guarded Query Routing Benchmark (GQR-Bench), which covers three exemplary target domains (law, finance, and healthcare), and seven datasets to test robustness against out-of-distribution queries. We then use GQR-Bench to contrast the effectiveness and efficiency of LLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B), standard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo Guardrails), continuous bag-of-words classifiers (WideMLP, fastText), and traditional machine learning models (SVM, XGBoost). Our results show that WideMLP, enhanced with out-of-domain detection capabilities, yields the best trade-off between accuracy (88\\%) and speed (<4ms). The embedding-based fastText excels at speed (<1ms) with acceptable accuracy (80\\%), whereas LLMs yield the highest accuracy (91\\%) but are comparatively slow (62ms for local Llama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge the automatic reliance on LLMs for (guarded) query routing and provide concrete recommendations for practical applications. GQR-Bench will be released as a Python package -- \\texttt{gqr}.', 'abstract_zh': '带有防护机制的查询路由问题：Guarded Query Routing Benchmark的研究', 'title_zh': '大型语言模型中的受控查询路由'}
{'arxiv_id': 'arXiv:2505.14489', 'title': 'Reasoning Models Better Express Their Confidence', 'authors': 'Dongkeun Yoon, Seungone Kim, Sohee Yang, Sunkyoung Kim, Soyeon Kim, Yongil Kim, Eunbi Choi, Yireun Kim, Minjoon Seo', 'link': 'https://arxiv.org/abs/2505.14489', 'abstract': 'Despite their strengths, large language models (LLMs) often fail to communicate their confidence accurately, making it difficult to assess when they might be wrong and limiting their reliability. In this work, we demonstrate that reasoning models-LLMs that engage in extended chain-of-thought (CoT) reasoning-exhibit superior performance not only in problem-solving but also in accurately expressing their confidence. Specifically, we benchmark six reasoning models across six datasets and find that they achieve strictly better confidence calibration than their non-reasoning counterparts in 33 out of the 36 settings. Our detailed analysis reveals that these gains in calibration stem from the slow thinking behaviors of reasoning models-such as exploring alternative approaches and backtracking-which enable them to adjust their confidence dynamically throughout their CoT, making it progressively more accurate. In particular, we find that reasoning models become increasingly better calibrated as their CoT unfolds, a trend not observed in non-reasoning models. Moreover, removing slow thinking behaviors from the CoT leads to a significant drop in calibration. Lastly, we show that these gains are not exclusive to reasoning models-non-reasoning models also benefit when guided to perform slow thinking via in-context learning.', 'abstract_zh': '尽管大型语言模型具有优势，但在准确传达信心方面常常表现出欠缺，这使得评估其错误的可能性变得困难，从而限制了其可靠性。在本工作中，我们证明了推理模型——那些进行扩展链式思考（CoT）推理的大型语言模型——不仅在问题解决上表现出色，还能更准确地表达其信心。具体而言，我们在六个数据集中对六种推理模型进行了基准测试，并发现它们在36种设置中的33种中严格超越了非推理模型的信心校准效果。我们详细的分析表明，这种校准改进源于推理模型的慢思考行为，如探索其他方法和回溯，这使它们能够在CoT过程中动态调整信心，使其更加准确。特别是，我们发现随着CoT的展开，推理模型的信心校准逐渐改善，这种趋势在非推理模型中未被观察到。此外，移除CoT中的慢思考行为会导致校准显著下降。最后，我们展示了这些改进不仅限于推理模型，当通过上下文学习引导非推理模型进行慢思考时，它们也会从中受益。', 'title_zh': '推理模型更好地表达其置信度'}
{'arxiv_id': 'arXiv:2505.14479', 'title': 'Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach', 'authors': 'Oren Sultan, Eitan Stern, Dafna Shahaf', 'link': 'https://arxiv.org/abs/2505.14479', 'abstract': "Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness.", 'abstract_zh': '大型语言模型在需要严格的逻辑演绎和符号推理的正式领域（如数学证明生成）中存在挑战。我们提出了一种神经符号方法，结合了大型语言模型的生成优势和结构化组件，以克服这一挑战。作为概念验证，我们专注于几何问题。该方法分为两部分：（1）检索类似问题并使用其证明来引导大型语言模型；（2）正式验证器评估生成的证明并提供反馈，帮助模型修正错误的证明。我们证明，我们的方法显著提高了OpenAI o1模型的证明准确性（提高58%-70%）；类似问题和验证器的反馈都对这些改进有所贡献。更广泛而言，转向生成可证明正确结论的大型语言模型可以显著提高它们的可靠性和一致性，解锁需要可信度的复杂任务和关键现实世界应用。', 'title_zh': '基于神经符号方法的可靠证明生成：LLMs的途径'}
{'arxiv_id': 'arXiv:2505.14412', 'title': 'PRL: Prompts from Reinforcement Learning', 'authors': 'Paweł Batorski, Adrian Kosmala, Paul Swoboda', 'link': 'https://arxiv.org/abs/2505.14412', 'abstract': 'Effective prompt engineering remains a central challenge in fully harnessing the capabilities of LLMs. While well-designed prompts can dramatically enhance performance, crafting them typically demands expert intuition and a nuanced understanding of the task. Moreover, the most impactful prompts often hinge on subtle semantic cues, ones that may elude human perception but are crucial for guiding LLM behavior. In this paper, we introduce PRL (Prompts from Reinforcement Learning), a novel RL-based approach for automatic prompt generation. Unlike previous methods, PRL can produce novel few-shot examples that were not seen during training. Our approach achieves state-of-the-art performance across a range of benchmarks, including text classification, simplification, and summarization. On the classification task, it surpasses prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it improves the average ROUGE scores on the summarization task by 4.32 over APE and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over APE and by 6.01 over EvoPrompt. Our code is available at this https URL .', 'abstract_zh': '有效的提示工程仍然是充分利用大型语言模型能力的核心挑战。虽然精心设计的提示可以显著提升性能，但构建它们通常需要专家直觉和对任务的深刻理解。此外，最具影响力的提示往往依赖于微妙的语义线索，这些线索可能逃过人类的感知，但对于引导大型语言模型的行为至关重要。在这篇论文中，我们介绍了PRL（强化学习提示），一种基于强化学习的新颖自动提示生成方法。与之前的方法不同，PRL能够生成训练期间未见过的少量示例。我们的方法在文本分类、简化和总结等多种基准上实现了最先进的性能。在分类任务中，它分别在APE和EvoPrompt的基础上提升了2.58%和1.00%。此外，它在总结任务上的平均ROUGE分数分别提高了4.32和2.12，在简化任务上的SARI分数分别提高了6.93和6.01。我们的代码可在以下链接获取：this https URL。', 'title_zh': 'PRL: 强化学习生成的提示'}
{'arxiv_id': 'arXiv:2505.14403', 'title': 'Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning', 'authors': 'Zhaohui Yang, Shilei Jiang, Chen Hu, Linjing Li, Shihong Deng, Daxin Jiang', 'link': 'https://arxiv.org/abs/2505.14403', 'abstract': 'Recent advances in reasoning language models have witnessed a paradigm shift from short to long CoT pattern. Given the substantial computational cost of rollouts in long CoT models, maximizing the utility of fixed training datasets becomes crucial. Our analysis reveals that negative responses contain valuable components such as self-reflection and error-correction steps, yet primary existing methods either completely discard negative samples (RFT) or apply equal penalization across all tokens (RL), failing to leverage these potential learning signals. In light of this, we propose Behavior Constrained Policy Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline RL framework that encompasses three stages: 1) sample segmentation, 2) consensus-based step correctness assessment combining LLM and PRM judgers, and 3) policy optimization with NSA designed to effectively mine positive steps within negative samples. Experimental results show that BCPG-NSA outperforms baselines on several challenging math/coding reasoning benchmarks using the same training dataset, achieving improved sample efficiency and demonstrating robustness and scalability when extended to multiple iterations.', 'abstract_zh': 'Recent Advances in Reasoning Language Models Have Witnessed a Paradigm Shift from Short to Long CoT Patterns: Behavior Constrained Policy Gradient with Negative Sample Augmentation (BCPG-NSA) for Efficient and Robust Learning', 'title_zh': '从石中琢玉：基于负样本增强的政策优化在大语言模型推理中的应用'}
{'arxiv_id': 'arXiv:2505.14394', 'title': 'Knowledge Graph Based Repository-Level Code Generation', 'authors': 'Mihir Athale, Vishal Vaddina', 'link': 'https://arxiv.org/abs/2505.14394', 'abstract': 'Recent advancements in Large Language Models (LLMs) have transformed code generation from natural language queries. However, despite their extensive knowledge and ability to produce high-quality code, LLMs often struggle with contextual accuracy, particularly in evolving codebases. Current code search and retrieval methods frequently lack robustness in both the quality and contextual relevance of retrieved results, leading to suboptimal code generation. This paper introduces a novel knowledge graph-based approach to improve code search and retrieval leading to better quality of code generation in the context of repository-level tasks. The proposed approach represents code repositories as graphs, capturing structural and relational information for enhanced context-aware code generation. Our framework employs a hybrid approach for code retrieval to improve contextual relevance, track inter-file modular dependencies, generate more robust code and ensure consistency with the existing codebase. We benchmark the proposed approach on the Evolutionary Code Benchmark (EvoCodeBench) dataset, a repository-level code generation benchmark, and demonstrate that our method significantly outperforms the baseline approach. These findings suggest that knowledge graph based code generation could advance robust, context-sensitive coding assistance tools.', 'abstract_zh': '近年来，大型语言模型的最新进展已将代码生成从自然语言查询中转变过来。然而，尽管大型语言模型具备丰富的知识并能生成高质量的代码，它们在上下文准确性方面常常遇到困难，尤其是在不断演进的代码库中。当前的代码搜索和检索方法在检索结果的质量和上下文相关性方面往往缺乏稳健性，导致代码生成效果不佳。本文提出了一种基于知识图谱的方法，以提高代码搜索和检索的质量，从而在仓库级任务中提高代码生成的质量。该方法将代码仓库表示为图，以捕获结构和关系信息，增强上下文感知的代码生成。我们的框架采用了混合方法进行代码检索，以提高上下文相关性、跟踪跨文件模块依赖关系、生成更 robust 的代码，并确保与现有代码库的一致性。我们在 Evansion Code Benchmark (EvoCodeBench) 数据集上对该方法进行了基准测试，这是一个仓库级代码生成基准，结果表明我们的方法明显优于基线方法。这些发现表明，基于知识图谱的代码生成可能有助于推进稳健且上下文敏感的编码辅助工具。', 'title_zh': '基于知识图谱的仓库级代码生成'}
{'arxiv_id': 'arXiv:2505.14300', 'title': 'SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors', 'authors': 'Maheep Chaudhary, Fazl Barez', 'link': 'https://arxiv.org/abs/2505.14300', 'abstract': 'High-risk industries like nuclear and aviation use real-time monitoring to detect dangerous system conditions. Similarly, Large Language Models (LLMs) need monitoring safeguards. We propose a real-time framework to predict harmful AI outputs before they occur by using an unsupervised approach that treats normal behavior as the baseline and harmful outputs as outliers. Our study focuses specifically on backdoor-triggered responses -- where specific input phrases activate hidden vulnerabilities causing the model to generate unsafe content like violence, pornography, or hate speech. We address two key challenges: (1) identifying true causal indicators rather than surface correlations, and (2) preventing advanced models from deception -- deliberately evading monitoring systems. Hence, we approach this problem from an unsupervised lens by drawing parallels to human deception: just as humans exhibit physical indicators while lying, we investigate whether LLMs display distinct internal behavioral signatures when generating harmful content. Our study addresses two critical challenges: 1) designing monitoring systems that capture true causal indicators rather than superficial correlations; and 2)preventing intentional evasion by increasingly capable "Future models\'\'. Our findings show that models can produce harmful content through causal mechanisms and can become deceptive by: (a) alternating between linear and non-linear representations, and (b) modifying feature relationships. To counter this, we developed Safety-Net -- a multi-detector framework that monitors different representation dimensions, successfully detecting harmful behavior even when information is shifted across representational spaces to evade individual monitors. Our evaluation shows 96% accuracy in detecting harmful cases using our unsupervised ensemble approach.', 'abstract_zh': '高风险行业如核能和航空使用实时监控来检测危险系统状态。类似地，大型语言模型（LLMs）也需要监控保护。我们提出一种实时框架，在有害AI输出发生之前，通过无监督方法将其预测出来，即以正常行为作为基线，将有害输出视为异常。我们的研究具体关注后门触发响应——特定输入短语激活隐藏漏洞，导致模型生成不安全内容，如暴力、色情或仇恨言论。我们面临两个关键挑战：（1）识别真正的因果指示而不是表面相关性；（2）防止高级模型的欺骗行为——故意规避监控系统。因此，我们从无监督的角度处理这一问题，类似于人类在撒谎时的生理指标，我们研究LLM在生成有害内容时是否表现出独特的内部行为特征。我们的研究针对两个关键挑战：1）设计能够捕捉真正因果指示而非表面相关性的监控系统；2）防止日益先进的“未来模型”的故意规避。研究发现，模型可以通过因果机制产生有害内容，并通过（a）交替使用线性和非线性表示；（b）修改特征关系展现出欺骗性。为此，我们开发了Safety-Net——一个多检测器框架，监控不同的表示维度，在信息在表示空间中转移以规避个体监控时，仍可成功检测有害行为。我们的评估显示，使用无监督集成方法检测有害案例的准确率为96%。', 'title_zh': 'SafetyNet: 通过建模和监控欺骗性行为来检测LLM的有害输出'}
{'arxiv_id': 'arXiv:2505.14216', 'title': 'Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning', 'authors': 'Minwu Kim, Anubhav Shrestha, Safal Shrestha, Aadim Nepal, Keith Ross', 'link': 'https://arxiv.org/abs/2505.14216', 'abstract': 'Recent studies have shown that reinforcement learning with verifiable rewards (RLVR) enhances overall accuracy but fails to improve capability, while distillation can improve both. In this paper, we investigate the mechanisms behind these phenomena. First, we demonstrate that RLVR does not improve capability because it focuses on improving the accuracy of the less-difficult questions to the detriment of the accuracy of the most difficult questions, thereby leading to no improvement in capability. Second, we find that RLVR does not merely increase the success probability for the less difficult questions, but in our small model settings produces quality responses that were absent in its output distribution before training. In addition, we show these responses are neither noticeably longer nor feature more reflection-related keywords, underscoring the need for more reliable indicators of response quality. Third, we show that while distillation reliably improves accuracy by learning strong reasoning patterns, it only improves capability when new knowledge is introduced. Moreover, when distilling only with reasoning patterns and no new knowledge, the accuracy of the less-difficult questions improves to the detriment of the most difficult questions, similar to RLVR. Together, these findings offer a clearer understanding of how RLVR and distillation shape reasoning behavior in language models.', 'abstract_zh': '近期研究显示，可验证奖励强化学习（RLVR）提高了整体准确性但未能提升能力，而知识蒸馏则能同时提升两者。本文研究了这些现象背后的机制。首先，我们证明RLVR未能提升能力是因为它侧重于提高较简单问题的准确性而牺牲了最难问题的准确性，从而导致能力未获提升。其次，我们发现RLVR不仅增加了较简单问题的成功概率，还在小型模型设置中产生了训练前输出分布中缺乏的高质量回答。此外，我们表明这些回答既没有显著更长，也不包含更多反思相关关键词，强调了需要更可靠的回答质量指标。第三，我们展示了虽然知识蒸馏通过学习强推理模式可靠地提升了准确性，但在引入新知识的情况下才提升能力。而且，在仅使用推理模式而不引入新知识的知识蒸馏中，较简单问题的准确性提升是以最困难问题的准确性下降为代价的，类似于RLVR。这些发现共同为我们理解RLVR和知识蒸馏如何影响语言模型的推理行为提供了更清晰的认识。', 'title_zh': '强化学习 vs. 提炼：理解大语言模型推理的准确性和能力'}
{'arxiv_id': 'arXiv:2505.14163', 'title': 'DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation', 'authors': 'He Wang, Alexander Hanbo Li, Yiqun Hu, Sheng Zhang, Hideo Kobayashi, Jiani Zhang, Henry Zhu, Chung-Wei Hang, Patrick Ng', 'link': 'https://arxiv.org/abs/2505.14163', 'abstract': "Large language model (LLM) agents have shown promising performance in generating code for solving complex data science problems. Recent studies primarily focus on enhancing in-context learning through improved search, sampling, and planning techniques, while overlooking the importance of the order in which problems are tackled during inference. In this work, we develop a novel inference-time optimization framework, referred to as DSMentor, which leverages curriculum learning -- a strategy that introduces simpler task first and progressively moves to more complex ones as the learner improves -- to enhance LLM agent performance in challenging data science tasks. Our mentor-guided framework organizes data science tasks in order of increasing difficulty and incorporates a growing long-term memory to retain prior experiences, guiding the agent's learning progression and enabling more effective utilization of accumulated knowledge. We evaluate DSMentor through extensive experiments on DSEval and QRData benchmarks. Experiments show that DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval and QRData compared to baseline agents. Furthermore, DSMentor demonstrates stronger causal reasoning ability, improving the pass rate by 8.8% on the causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our work underscores the importance of developing effective strategies for accumulating and utilizing knowledge during inference, mirroring the human learning process and opening new avenues for improving LLM performance through curriculum-based inference optimization.", 'abstract_zh': '大型语言模型代理在生成解决复杂数据科学问题的代码方面表现出有前途的性能。近期研究主要侧重于通过改进搜索、采样和规划技术来增强上下文学习，而忽视了解题顺序对推断性能的影响。本文开发了一种新的推理时优化框架，称为DSMentor，该框架利用了循序渐进学习策略——先引入简单的任务，随着学习者能力的提高逐步过渡到更复杂的任务——以提高在挑战性数据科学任务上的大型语言模型代理性能。我们的导师引导框架按任务难度递增的顺序组织数据科学任务，并集成不断增长的长期记忆以保留先前的经验，引导代理的学习进程，并有效利用积累的知识。我们通过在DSEval和QRData基准上进行广泛实验来评估DSMentor。实验表明，使用Claude-3.5-Sonnet的DSMentor在DSEval和QRData上的通过率分别相较于基线代理提高了5.2%。此外，DSMentor在因果推理问题上的因果推理能力更强，相较于使用Program-of-Thoughts提示的GPT-4，通过率提高了8.8%。我们的工作强调了在推断过程中开发有效知识积累和利用策略的重要性，模拟了人类学习过程，并为通过基于课程的推理优化提升大型语言模型性能开辟了新的途径。', 'title_zh': 'DSMentor: 通过课程学习和在线知识积累增强数据科学代理'}
{'arxiv_id': 'arXiv:2505.14148', 'title': 'MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem', 'authors': 'Fan Liu, Zherui Yang, Cancheng Liu, Tianrui Song, Xiaofeng Gao, Hao Liu', 'link': 'https://arxiv.org/abs/2505.14148', 'abstract': 'Mathematical modeling is a cornerstone of scientific discovery and engineering practice, enabling the translation of real-world problems into formal systems across domains such as physics, biology, and economics. Unlike mathematical reasoning, which assumes a predefined formulation, modeling requires open-ended problem analysis, abstraction, and principled formalization. While Large Language Models (LLMs) have shown strong reasoning capabilities, they fall short in rigorous model construction, limiting their utility in real-world problem-solving. To this end, we formalize the task of LLM-powered real-world mathematical modeling, where agents must analyze problems, construct domain-appropriate formulations, and generate complete end-to-end solutions. We introduce MM-Bench, a curated benchmark of 111 problems from the Mathematical Contest in Modeling (MCM/ICM), spanning the years 2000 to 2025 and across ten diverse domains such as physics, biology, and economics. To tackle this task, we propose MM-Agent, an expert-inspired framework that decomposes mathematical modeling into four stages: open-ended problem analysis, structured model formulation, computational problem solving, and report generation. Experiments on MM-Bench show that MM-Agent significantly outperforms baseline agents, achieving an 11.88\\% improvement over human expert solutions while requiring only 15 minutes and \\$0.88 per task using GPT-4o. Furthermore, under official MCM/ICM protocols, MM-Agent assisted two undergraduate teams in winning the Finalist Award (\\textbf{top 2.0\\% among 27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a modeling copilot. Our code is available at this https URL', 'abstract_zh': '数学建模是科学研究和工程实践的基础，能使现实世界的问题在物理学、生物学和经济学等领域转化为正式系统。与基于预定义形式化的数学推理不同，建模需要开放性问题分析、抽象化和 principled 形式化。虽然大型语言模型（LLMs）展示了强大的推理能力，但在严谨的模型构建方面仍存在不足，限制了其在解决实际问题中的应用。为此，我们正式定义了 LLM 驱动的现实世界数学建模任务，要求代理分析问题、构建适用领域的正式化表述，并生成端到端的解决方案。我们引入了 MM-Bench，这是一个由数学建模竞赛（MCM/ICM）中的111个问题组成的精选基准，涵盖2000年至2025年间的十个不同领域，如物理学、生物学和经济学。为了解决这一任务，我们提出了 MM-Agent，一个基于专家启发的框架，将数学建模分解为四个阶段：开放性问题分析、结构化建模表述、计算问题求解以及报告生成。MM-Bench 上的实验显示，MM-Agent 在基线代理上取得了显著的优势，相较于人类专家解决方案，其性能提高了11.88%，使用GPT-4o完成每任务仅需15分钟和0.88美元。此外，在官方MCM/ICM协议下，MM-Agent 协助两个本科生团队赢得了2025年MCM/ICM的最终奖（27,456支队伍中的前2.0%），证明了其作为建模协作者的实际有效性。相关代码可在此链接获取。', 'title_zh': 'MM-Agent: LLM作为解决现实数学建模问题的代理'}
{'arxiv_id': 'arXiv:2505.14147', 'title': 'SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning', 'authors': 'Xiong Jun Wu, Zhenduo Zhang, ZuJie Wen, Zhiqiang Zhang, Wang Ren, Lei Shi, Cai Chen, Deng Zhao, Dingnan Jin, Qing Cui, Jun Zhou', 'link': 'https://arxiv.org/abs/2505.14147', 'abstract': "Training large reasoning models (LRMs) with reinforcement learning in STEM domains is hindered by the scarcity of high-quality, diverse, and verifiable problem sets. Existing synthesis methods, such as Chain-of-Thought prompting, often generate oversimplified or uncheckable data, limiting model advancement on complex tasks. To address these challenges, we introduce SHARP, a unified approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a strategic set of self-alignment principles -- targeting graduate and Olympiad-level difficulty, rigorous logical consistency, and unambiguous, verifiable answers -- and a structured three-phase framework (Alignment, Instantiation, Inference) that ensures thematic diversity and fine-grained control over problem generation. We implement SHARP by leveraging a state-of-the-art LRM to infer and verify challenging STEM questions, then employ a reinforcement learning loop to refine the model's reasoning through verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate that SHARP-augmented training substantially outperforms existing methods, markedly improving complex reasoning accuracy and pushing LRM performance closer to expert-level proficiency. Our contributions include the SHARP strategy, framework design, end-to-end implementation, and experimental evaluation of its effectiveness in elevating LRM reasoning capabilities.", 'abstract_zh': '使用强化学习训练大型推理模型（LRMs）在STEM领域受到高质量、多样性和可验证问题集稀缺性的阻碍。现有的合成方法，如链式思维提示，通常生成过于简化或不可验证的数据，限制了模型在复杂任务上的进步。为了解决这些挑战，我们引入了SHARP，一种用于大型推理模型强化学习的统一方法，该方法结合了验证奖励（RLVR）的高质量对齐推理问题合成方法。SHARP 包含一系列策略——旨在针对研究生和奥林匹克级别难度、严格逻辑一致性和明确可验证的答案，并采用结构化的三阶段框架（对齐、实例化、推理），以确保问题生成的题型多样性和精细控制。我们通过利用最新最先进的LRM来推断和验证具有挑战性的STEM问题，然后使用强化学习循环并通过可验证的奖励信号进一步精炼模型的推理能力。在GPQA等基准上的实验表明，SHARP增强的训练显著优于现有方法，显著提高了复杂推理的准确性，并将LRM性能提升至专家级水平。我们的贡献包括SHARP策略、框架设计、端到端实现及其有效性实验评估。', 'title_zh': 'SHARP: 综合高质量对齐推理问题以强化大型推理模型的训练'}
{'arxiv_id': 'arXiv:2505.14146', 'title': "s3: You Don't Need That Much Data to Train a Search Agent via RL", 'authors': 'Pengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao, Zifeng Wang, Jimeng Sun, Jiawei Han', 'link': 'https://arxiv.org/abs/2505.14146', 'abstract': 'Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks.', 'abstract_zh': '基于检索增强生成的检索分离框架(s3) empowering大型语言模型在推理时访问外部知识。现有方法要么使用仅搜索的指标（如NDCG）来优化检索，忽略下游实用性，要么 Fine-tune 整个大型语言模型以联合推理和检索，这会混淆检索与生成并限制实际搜索实用性和与冻结或专有模型的兼容性。我们的工作提出了 s3，一个轻量级且模型无关的框架，将检索与生成分离，并使用超越 RAG 的奖励对检索器进行训练：生成准确性上的提升。s3 仅需 2400 个训练样本即可在数据量超过其 70 倍的基线上表现出更优的下游性能，在六个通用 QA 和五个医学 QA 标准测试中都能持续提供更强大更稳定的性能。', 'title_zh': '三: 通过强化学习训练搜索代理不需要太多数据'}
{'arxiv_id': 'arXiv:2505.14141', 'title': 'Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent', 'authors': 'Fanglin Mo, Junzhe Chen, Haoxuan Zhu, Xuming Hu', 'link': 'https://arxiv.org/abs/2505.14141', 'abstract': 'Mobile GUI agents execute user commands by directly interacting with the graphical user interface (GUI) of mobile devices, demonstrating significant potential to enhance user convenience. However, these agents face considerable challenges in task planning, as they must continuously analyze the GUI and generate operation instructions step by step. This process often leads to difficulties in making accurate task plans, as GUI agents lack a deep understanding of how to effectively use the target applications, which can cause them to become "lost" during task execution. To address the task planning issue, we propose SPlanner, a plug-and-play planning module to generate execution plans that guide vision language model(VLMs) in executing tasks. The proposed planning module utilizes extended finite state machines (EFSMs) to model the control logits and configurations of mobile applications. It then decomposes a user instruction into a sequence of primary function modeled in EFSMs, and generate the execution path by traversing the EFSMs. We further refine the execution path into a natural language plan using an LLM. The final plan is concise and actionable, and effectively guides VLMs to generate interactive GUI actions to accomplish user tasks. SPlanner demonstrates strong performance on dynamic benchmarks reflecting real-world mobile usage. On the AndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired with Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point improvement compared to using Qwen2.5-VL-72B without planning assistance.', 'abstract_zh': 'Mobile GUI代理通过直接与移动设备的图形用户界面（GUI）交互来执行用户命令，显示出显著提升用户便利性的潜力。然而，这些代理在任务规划方面面临重大挑战，因为它们需要不断分析GUI并逐步生成操作指令。这一过程往往会导致难以做出准确的任务规划，因为GUI代理缺乏对如何有效使用目标应用程序的深刻理解，这可能导致它们在任务执行过程中“迷失”。为了解决任务规划问题，我们提出SPlanner，这是一种插即用的规划模块，用于生成指导视觉语言模型(VLMs)执行任务的执行计划。提出的规划模块利用扩展的有限状态机（EFSMs）来建模移动应用程序的控制概率和配置。然后，它将用户指令分解成序列的基本功能模型，并通过遍历EFSMs生成执行路径。我们进一步使用大规模语言模型（LLM）对执行路径进行细化，以自然语言形式生成最终计划。该计划简洁且具有操作性，有效地指导VLMs生成交互式GUI操作以完成用户任务。SPlanner在反映现实移动使用情况的动态基准测试中表现出强大的性能。在AndroidWorld基准测试中，当与Qwen2.5-VL-72B作为VLM执行器配对时，SPlanner的任务成功率达到了63.8%，比使用无规划辅助的Qwen2.5-VL-72B提高了28.8个百分点。', 'title_zh': '基于扩展有限状态机的稳定规划模块：移动GUI代理的规划实现'}
{'arxiv_id': 'arXiv:2505.14140', 'title': 'RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning', 'authors': 'Qianyue Hao, Sibo Li, Jian Yuan, Yong Li', 'link': 'https://arxiv.org/abs/2505.14140', 'abstract': "Despite rapid advancements in large language models (LLMs), the token-level autoregressive nature constrains their complex reasoning capabilities. To enhance LLM reasoning, inference-time techniques, including Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they are fairly cost-effective by guiding reasoning through sophisticated logical structures without modifying LLMs' parameters. However, these manually predefined, task-agnostic frameworks are applied uniformly across diverse tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT), where we train a lightweight navigator model with reinforcement learning (RL) to adaptively enhance LLM reasoning at inference time. Specifically, we design five basic logic blocks from the perspective of human cognition. During the reasoning process, the trained RL navigator dynamically selects the suitable logic blocks and combines them into task-specific logical structures according to problem characteristics. Experiments across multiple reasoning benchmarks (AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek) illustrate that RLoT outperforms established inference-time techniques by up to 13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL navigator demonstrates strong transferability: a model trained on one specific LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is open-source at this https URL for reproducibility.", 'abstract_zh': '尽管大型语言模型（LLMs）取得了rapid advancements，其token级别的自回归性质限制了它们的复杂推理能力。为增强LLM推理，通过推理时的技术，如链式/树状/图状思维，显著改善了性能，因为这些技术通过引导复杂的逻辑结构进行推理，成本效益高，而不修改LLM的参数。然而，这些手动预定义的、任务无关的框架在不同任务中应用时缺乏适应性。为改进这一问题，我们提出了RL-of-Thoughts (RLoT)，其中通过强化学习（RL）训练一个轻量级导向模型，在推理时自适应地增强LLM的推理能力。具体而言，我们从人类认知的角度设计了五个基本逻辑模块。在推理过程中，训练好的RL导向模型动态选择合适的逻辑模块，并根据问题特点组合成任务特定的逻辑结构。多轮推理基准实验（AIME、MATH、GPQA等）和多种LLM（GPT、Llama、Qwen、DeepSeek）表明，RLoT相比现有推理时技术可提高多达13.4%的性能。值得注意的是，使用不到3K参数，我们的RL导向模型能使小于10B参数的LLM与100B级别的模型相当。此外，RL导向模型展示了强大的迁移能力：一个模型在一个特定的LLM任务对上训练后，能够有效泛化到未见过的LLM和任务。我们的代码已开源，以便再现结果。', 'title_zh': '思维的RL：基于推理时强化学习的LLM推理导航'}
{'arxiv_id': 'arXiv:2505.14038', 'title': 'ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data', 'authors': 'Xinzhe Zheng, Sijie Ji, Jiawei Sun, Renqi Chen, Wei Gao, Mani Srivastava', 'link': 'https://arxiv.org/abs/2505.14038', 'abstract': 'Mental health risk is a critical global public health challenge, necessitating innovative and reliable assessment methods. With the development of large language models (LLMs), they stand out to be a promising tool for explainable mental health care applications. Nevertheless, existing approaches predominantly rely on subjective textual mental records, which can be distorted by inherent mental uncertainties, leading to inconsistent and unreliable predictions. To address these limitations, this paper introduces ProMind-LLM. We investigate an innovative approach integrating objective behavior data as complementary information alongside subjective mental records for robust mental health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive pipeline that includes domain-specific pretraining to tailor the LLM for mental health contexts, a self-refine mechanism to optimize the processing of numerical behavioral data, and causal chain-of-thought reasoning to enhance the reliability and interpretability of its predictions. Evaluations of two real-world datasets, PMData and Globem, demonstrate the effectiveness of our proposed methods, achieving substantial improvements over general LLMs. We anticipate that ProMind-LLM will pave the way for more dependable, interpretable, and scalable mental health case solutions.', 'abstract_zh': '心理健康风险是一个关键的全球公共卫生挑战，需要创新可靠的评估方法。随着大规模语言模型（LLMs）的发展，它们在可解释的心理健康护理应用中展现出巨大潜力。然而，现有方法主要依赖主观的心理文本记录，这些记录可能会受到内在心理不确定性的扭曲，导致不一致和不可靠的预测。为了解决这些局限性，本文介绍了一种新的方法——ProMind-LLM。我们研究了一种创新的方法，将客观行为数据作为补充信息与主观心理记录相结合，以实现稳健的心理健康风险评估。具体而言，ProMind-LLM 包含一个全面的工作流，包括特定领域的预训练以适应心理健康情境，自反馈机制以优化数值行为数据的处理，以及因果链式推理以提高其预测的可靠性和可解释性。对两个实际数据集（PMData 和 Globem）的评估表明，所提出的方法优于通用的语言模型，取得了显著改进。我们期待 ProMind-LLM 能够为更可靠、可解释和可扩展的心理健康案例解决方案铺平道路。', 'title_zh': 'ProMind-LLM：基于传感器数据的因果推理前馈心理健康护理'}
{'arxiv_id': 'arXiv:2505.13994', 'title': 'Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning', 'authors': 'Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim', 'link': 'https://arxiv.org/abs/2505.13994', 'abstract': 'Retrieval-Augmented Generation (RAG) systems empower large language models (LLMs) with external knowledge, yet struggle with efficiency-accuracy trade-offs when scaling to large knowledge graphs. Existing approaches often rely on monolithic graph retrieval, incurring unnecessary latency for simple queries and fragmented reasoning for complex multi-hop questions. To address these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework that addresses these limitations with question-driven semantic graph partitioning and collaborative subgraph retrieval. The innovative framework first create Semantic Partitioning of Linked Information, then use the Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware graph segmentation manages to divide knowledge graphs into semantically coherent subgraphs, ensuring subgraphs align with different query types, while lightweight LLM agents are assigned to partitioned subgraphs, and only relevant partitions are activated during retrieval, thus reduce search space while enhancing efficiency. Finally, a hierarchical merging module resolves inconsistencies across subgraph-derived answers through logical verifications. Extensive experimental validation demonstrates considerable improvements compared to existing approaches.', 'abstract_zh': '基于检索增强生成的多agent框架SPLIT-RAG：基于语义图分割的协作子图检索', 'title_zh': '按照问题划分，由代理征服：基于问题驱动的图划分的SPLIT-RAG'}
{'arxiv_id': 'arXiv:2505.13946', 'title': 'Visual Instruction Bottleneck Tuning', 'authors': 'Changdae Oh, Jiatong Li, Shawn Im, Yixuan Li', 'link': 'https://arxiv.org/abs/2505.13946', 'abstract': "Despite widespread adoption, multimodal large language models (MLLMs) suffer performance degradation when encountering unfamiliar queries under distribution shifts. Existing methods to improve MLLM generalization typically require either more instruction data or larger advanced model architectures, both of which incur non-trivial human labor or computational costs. In this work, we take an alternative approach to enhance the robustness of MLLMs under distribution shifts, from a representation learning perspective. Inspired by the information bottleneck (IB) principle, we derive a variational lower bound of the IB for MLLMs and devise a practical implementation, Visual Instruction Bottleneck Tuning (Vittle). We then provide a theoretical justification of Vittle by revealing its connection to an information-theoretic robustness metric of MLLM. Empirical validation of three MLLMs on open-ended and closed-form question answering and object hallucination detection tasks over 45 datasets, including 30 shift scenarios, demonstrates that Vittle consistently improves the MLLM's robustness under shifts by pursuing the learning of a minimal sufficient representation.", 'abstract_zh': '尽管广泛应用，多模态大型语言模型（MLLMs）在分布变化下遇到不熟悉查询时会出现性能下降。现有的提升MLLM泛化能力的方法通常需要更多的指令数据或更大的先进模型架构，这两种方法都会产生非琐碎的人力或计算成本。在这项工作中，我们从表示学习的角度出发，采取一种不同的方法来增强MLLMs在分布变化下的鲁棒性。受信息瓶颈（IB）原则的启发，我们为MLLMs推导出了IB的变分下界，并设计了一个实用的实现方法，即视觉指令瓶颈调整（Vittle）。然后，我们通过揭示Vittle与MLLM的信息论鲁棒性度量之间的联系，为其提供理论依据。在45个数据集上对三种MLLMs进行开放性和封闭性问题回答以及物体幻觉检测任务的实证验证，包括30种分布变化场景，表明Vittle通过追求学习最小充分表示来一致地提高了MLLMs在分布变化下的鲁棒性。', 'title_zh': '视觉指令瓶颈调整'}
{'arxiv_id': 'arXiv:2505.13940', 'title': 'DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery', 'authors': 'Kun Li, Zhennan Wu, Shoupeng Wang, Wenbin Hu', 'link': 'https://arxiv.org/abs/2505.13940', 'abstract': 'In the field of AI4Science, large-scale language models (LLMs) show great potential to parse complex scientific semantics, integrate cross-disciplinary knowledge, and assist critical task research. However, in the field of drug discovery, despite the optimization through professional data pre-training, context window expansion, and internet search, the existing LLMs are still facing challenges such as massive multi-modal and heterogeneous data processing, domain knowledge dynamic updating delay, and insufficient confidence in predicting the results of complex computational tasks. To address these challenges, we propose the DrugPilot, an LLM-based agent with parameterized reasoning for drug discovery. DrugPilot addresses key limitations of traditional end-to-end LLM prediction approaches through its parametric inference architecture. This agent system supports major phases of the drug discovery pipeline, facilitating automated planning and execution of multi-stage research tasks. To address the critical challenge of multi-modal drug data analysis (incorporating both public datasets and user-submitted data), we developed an interactive parameterized memory pool. This innovative component standardizes real-world drug data into parametric representations, simultaneously enabling efficient knowledge retrieval in multi-turn dialogue while mitigating the information loss inherent in text-based data transmission. Additionally, we created a drug instruct dataset across 8 essential drug discovery tasks for model fine-tuning and evaluation. Based on the Berkeley function calling evaluation framework, DrugPilot demonstrated the most advanced tool calling capabilities on our drug discovery tool instruction dataset, outperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves task completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and multi-turn tasks, respectively.', 'abstract_zh': '在AI4Science领域，大规模语言模型（LLMs）在解析复杂科学语义、整合跨学科知识以及辅助关键任务研究方面展现出巨大潜力。然而，在药物发现领域，尽管通过专业数据预训练、上下文窗口扩展和互联网搜索进行了优化，现有LLMs仍然面临多模态和异构数据处理规模庞大、领域知识动态更新延迟以及在复杂计算任务预测中信心不足等挑战。为应对这些挑战，我们提出了一种基于参数推理的LLM代理DrugPilot，用于药物发现。DrugPilot通过其参数推理架构，解决了传统端到端LLM预测方法中的关键局限性。该代理系统支持药物发现管道的主要阶段，促进多阶段研究任务的自动化规划与执行。为解决多模态药物数据分析的关键挑战（结合公共数据集和用户提交数据），我们开发了一个交互式的参数化记忆池。这一创新组件将现实世界中的药物数据标准化为参数表示，同时在多轮对话中高效检索知识，缓解基于文本的数据传输固有的信息损失。此外，我们为模型的微调和评估创建了一个横跨8个关键药物发现任务的数据集Drug instruct。基于伯克利函数调用评估框架，DrugPilot在药物发现工具指令数据集上展示了最先进的工具调用能力，优于现有代理（如ReAct、LoT）。具体而言，在简单任务、多任务和多轮任务中，分别实现了98.0%、93.5%和64.0%的任务完成率。', 'title_zh': 'DrugPilot：基于LLM的参数化 reasoning代理在药物发现中的应用'}
{'arxiv_id': 'arXiv:2505.13794', 'title': 'LLM-based Evaluation Policy Extraction for Ecological Modeling', 'authors': 'Qi Cheng, Licheng Liu, Qing Zhu, Runlong Yu, Zhenong Jin, Yiqun Xie, Xiaowei Jia', 'link': 'https://arxiv.org/abs/2505.13794', 'abstract': 'Evaluating ecological time series is critical for benchmarking model performance in many important applications, including predicting greenhouse gas fluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles. Traditional numerical metrics (e.g., R-squared, root mean square error) have been widely used to quantify the similarity between modeled and observed ecosystem variables, but they often fail to capture domain-specific temporal patterns critical to ecological processes. As a result, these methods are often accompanied by expert visual inspection, which requires substantial human labor and limits the applicability to large-scale evaluation. To address these challenges, we propose a novel framework that integrates metric learning with large language model (LLM)-based natural language policy extraction to develop interpretable evaluation criteria. The proposed method processes pairwise annotations and implements a policy optimization mechanism to generate and combine different assessment metrics. The results obtained on multiple datasets for evaluating the predictions of crop gross primary production and carbon dioxide flux have confirmed the effectiveness of the proposed method in capturing target assessment preferences, including both synthetically generated and expert-annotated model comparisons. The proposed framework bridges the gap between numerical metrics and expert knowledge while providing interpretable evaluation policies that accommodate the diverse needs of different ecosystem modeling studies.', 'abstract_zh': '评估生态时间序列对于在预测温室气体通量、捕捉碳-氮动力学和监测水文循环等重要应用中 benchmark 模型性能至关重要。传统的数值指标（如决定系数 R-squared、均方根误差 RMSE）广泛用于量化模型值和观测值生态变量之间的相似性，但往往难以捕捉对生态过程至关重要的特定时间序列模式。为解决这些问题，我们提出了一种新颖的框架，将度量学习与基于大型语言模型（LLM）的自然语言策略提取相结合，以开发可解释的评估标准。该方法处理成对标注并实现策略优化机制，以生成和组合不同的评估指标。针对评估作物粗估初级生产和二氧化碳通量的多个数据集的结果证实了该方法在捕捉目标评估偏好方面的有效性，包括合成生成和专家标注的模型比较。该框架在量化指标和专家知识之间架起了桥梁，同时提供了可解释的评估策略，以满足不同生态系统建模研究的多样化需求。', 'title_zh': '基于LLM的评价政策提取在生态建模中的应用'}
{'arxiv_id': 'arXiv:2505.13778', 'title': 'CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs', 'authors': 'Guoheng Sun, Ziyao Wang, Bowei Tian, Meng Liu, Zheyu Shen, Shwai He, Yexiao He, Wanghao Ye, Yiting Wang, Ang Li', 'link': 'https://arxiv.org/abs/2505.13778', 'abstract': 'As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at this https URL.', 'abstract_zh': '训练后技术的发展使大型语言模型（LLMs）获得越来越多的结构化多步推理能力，这些能力通常通过强化学习进行优化。增强推理能力的模型在复杂任务上表现出色，并支撑着许多商业LLM API。然而，为了保护专有行为并减少冗余，提供者通常会隐藏推理踪迹，仅返回最终答案。这种不透明性引入了一个关键的透明度缺口：用户为不可见的推理令牌付费，这些令牌往往占成本的大部分，但却无法验证其真实性。这为令牌计数膨胀打开了大门，提供者可能会夸大令牌使用量或注入合成的低效令牌以增加收费。为了解决这一问题，我们提出了一种名为CoIn的验证框架，用于审核隐藏令牌的数量和语义有效性。CoIn从令牌嵌入指纹构建验证哈希树以检查令牌计数，并使用嵌入式相关性匹配来检测伪造的推理内容。实验表明，当CoIn作为受信第三方审计员部署时，能够有效检测到令牌计数膨胀，成功率高达94.7%，显示出在不透明的LLM服务中恢复计费透明度的强大能力。相关数据集和代码可在此处获取。', 'title_zh': 'CoIn: 计算商业不透明LLM API 中的隐形推理令牌数量'}
{'arxiv_id': 'arXiv:2505.13774', 'title': 'Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models', 'authors': 'Zidi Xiong, Chen Shan, Zhenting Qi, Himabindu Lakkaraju', 'link': 'https://arxiv.org/abs/2505.13774', 'abstract': "Large Reasoning Models (LRMs) have significantly enhanced their capabilities in complex problem-solving by introducing a thinking draft that enables multi-path Chain-of-Thought explorations before producing final answers. Ensuring the faithfulness of these intermediate reasoning processes is crucial for reliable monitoring, interpretation, and effective control. In this paper, we propose a systematic counterfactual intervention framework to rigorously evaluate thinking draft faithfulness. Our approach focuses on two complementary dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual reasoning steps causally influence subsequent steps and the final draft conclusion through counterfactual step insertions; and (2) Draft-to-Answer Faithfulness, which evaluates whether final answers are logically consistent with and dependent on the thinking draft, by perturbing the draft's concluding logic. We conduct extensive experiments across six state-of-the-art LRMs. Our findings show that current LRMs demonstrate selective faithfulness to intermediate reasoning steps and frequently fail to faithfully align with the draft conclusions. These results underscore the need for more faithful and interpretable reasoning in advanced LRMs.", 'abstract_zh': '大型推理模型（LRMs）通过引入思考草案，使其在复杂问题解决方面的能力得到了显著增强，该草案能够在生成最终答案之前进行多路径的Chain-of-Thought探索。确保这些中间推理过程的可信度对于可靠监控、解释和有效控制至关重要。本文提出了一种系统性的反事实干预框架，以严格评估思考草案的可信度。我们的方法集中在两个互补维度上：（1）草案内部可信度，通过反事实步骤插入评估个体推理步骤是否因果影响后续步骤和最终草案结论；（2）草案至答案可信度，通过扰动草案的结论逻辑来评估最终答案是否与草案逻辑一致并依赖于其结论。我们在六个最先进的LRMs上进行了广泛的实验。研究结果表明，当前的LRMs在中间推理步骤上表现出选择性的可信度，并且经常未能忠实一致地与草案结论对齐。这些结果强调了在高级LRMs中需要更加忠实和可解释的推理。', 'title_zh': '测量大型推理模型中思维草稿的忠实度'}
{'arxiv_id': 'arXiv:2505.13770', 'title': "Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference", 'authors': 'Jin Du, Li Chen, Xun Xian, An Luo, Fangqiao Tian, Ganghua Wang, Charles Doss, Xiaotong Shen, Jie Ding', 'link': 'https://arxiv.org/abs/2505.13770', 'abstract': "Reliable causal inference is essential for making decisions in high-stakes areas like medicine, economics, and public policy. However, it remains unclear whether large language models (LLMs) can handle rigorous and trustworthy statistical causal inference. Current benchmarks usually involve simplified tasks. For example, these tasks might only ask LLMs to identify semantic causal relationships or draw conclusions directly from raw data. As a result, models may overlook important statistical pitfalls, such as Simpson's paradox or selection bias. This oversight limits the applicability of LLMs in the real world. To address these limitations, we propose CausalPitfalls, a comprehensive benchmark designed to rigorously evaluate the capability of LLMs in overcoming common causal inference pitfalls. Our benchmark features structured challenges across multiple difficulty levels, each paired with grading rubrics. This approach allows us to quantitatively measure both causal reasoning capabilities and the reliability of LLMs' responses. We evaluate models using two protocols: (1) direct prompting, which assesses intrinsic causal reasoning, and (2) code-assisted prompting, where models generate executable code for explicit statistical analysis. Additionally, we validate the effectiveness of this judge by comparing its scoring with assessments from human experts. Our results reveal significant limitations in current LLMs when performing statistical causal inference. The CausalPitfalls benchmark provides essential guidance and quantitative metrics to advance the development of trustworthy causal reasoning systems.", 'abstract_zh': '可靠的因果推断对于在医学、经济学和公共政策等高风险领域做出决策至关重要。然而，目前尚不清楚大语言模型（LLMs）能否处理严谨且可信赖的统计因果推断。当前的基准测试通常涉及简化任务，例如，这些任务可能只要求LLMs识别语义上的因果关系或直接从原始数据中得出结论。结果，模型可能会忽略重要的统计陷阱，如辛普森悖论或选择偏差。这种忽视限制了LLMs在现实生活中的应用。为了解决这些局限性，我们提出CausalPitfalls，一个全面的基准测试，旨在严格评估LLMs克服常见因果推断陷阱的能力。该基准测试包含不同难度级别的结构化挑战，每项挑战均配有评分标准。这种方法使我们能够定量衡量因果推理能力和LLMs响应的可靠性。我们使用两种协议评估模型：（1）直接提示，评估内在的因果推理能力；（2）代码辅助提示，模型生成可执行代码进行明确的统计分析。此外，我们通过将该评分系统的结果与人类专家评估结果进行比较，验证其有效性。结果显示，当前的LLMs在执行统计因果推断时存在显著局限性。CausalPitfalls基准测试提供了必不可少的指导和支持，以推动可信因果推理系统的开发。', 'title_zh': '冰淇淋不会导致溺水：将LLMs与因果推断中的统计陷阱进行基准测试'}
{'arxiv_id': 'arXiv:2505.13763', 'title': 'Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations', 'authors': 'Li Ji-An, Hua-Dong Xiong, Robert C. Wilson, Marcelo G. Mattar, Marcus K. Benna', 'link': 'https://arxiv.org/abs/2505.13763', 'abstract': 'Large language models (LLMs) can sometimes report the strategies they actually use to solve tasks, but they can also fail to do so. This suggests some degree of metacognition -- the capacity to monitor one\'s own cognitive processes for subsequent reporting and self-control. Metacognitive abilities enhance AI capabilities but raise safety concerns, as models might obscure their internal processes to evade neural-activation-based oversight mechanisms designed to detect harmful behaviors. Given society\'s increased reliance on these models, it is critical that we understand the limits of their metacognitive abilities, particularly their ability to monitor their internal activations. To address this, we introduce a neuroscience-inspired neurofeedback paradigm designed to quantify the ability of LLMs to explicitly report and control their activation patterns. By presenting models with sentence-label pairs where labels correspond to sentence-elicited internal activations along specific directions in the neural representation space, we demonstrate that LLMs can learn to report and control these activations. The performance varies with several factors: the number of example pairs provided, the semantic interpretability of the target neural direction, and the variance explained by that direction. These results reveal a "metacognitive space" with dimensionality much lower than the model\'s neural space, suggesting LLMs can monitor only a subset of their neural mechanisms. Our findings provide empirical evidence quantifying metacognitive capabilities in LLMs, with significant implications for AI safety.', 'abstract_zh': '大型语言模型的元认知能力：神经反馈研究揭示其内部激活监测能力', 'title_zh': '语言模型能够监控和控制其内部激活过程。'}
{'arxiv_id': 'arXiv:2505.13737', 'title': 'Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers', 'authors': 'Andrew Nam, Henry Conklin, Yukang Yang, Thomas Griffiths, Jonathan Cohen, Sarah-Jane Leslie', 'link': 'https://arxiv.org/abs/2505.13737', 'abstract': 'We present causal head gating (CHG), a scalable method for interpreting the functional roles of attention heads in transformer models. CHG learns soft gates over heads and assigns them a causal taxonomy - facilitating, interfering, or irrelevant - based on their impact on task performance. Unlike prior approaches in mechanistic interpretability, which are hypothesis-driven and require prompt templates or target labels, CHG applies directly to any dataset using standard next-token prediction. We evaluate CHG across multiple large language models (LLMs) in the Llama 3 model family and diverse tasks, including syntax, commonsense, and mathematical reasoning, and show that CHG scores yield causal - not merely correlational - insight, validated via ablation and causal mediation analyses. We also introduce contrastive CHG, a variant that isolates sub-circuits for specific task components. Our findings reveal that LLMs contain multiple sparse, sufficient sub-circuits, that individual head roles depend on interactions with others (low modularity), and that instruction following and in-context learning rely on separable mechanisms.', 'abstract_zh': '我们 presents 调因头部门控（CHG）：一种可扩展的方法，用于解释transformer模型中注意力头部的函数角色。CHG 学习头部的软门控，并根据其对任务性能的影响将它们分配为促进性、干扰性或无关性，从而形成一种因果分类学。与基于假设的机制解释方法不同，CHG 无需提示模板或目标标签即可应用于任何数据集，使用标准的下一个词预测。我们在Llama 3模型家族中的多个大型语言模型（LLM）和包括句法、常识和数学推理在内的多种任务上评估了CHG，证明CHG分数提供了因果而非相关性见解，通过消融分析和因果中介分析进行了验证。我们还引入了对比型CHG，这是一种用于隔离特定任务组件亚电路的变体。我们的研究发现LLM包含多个稀疏、足够的亚电路，个体头部角色依赖于与其他头部的互动（较低的模块性），并且指令执行和上下文学习依赖于分离的机制。', 'title_zh': '因果头部门控：Transformer中注意力头部作用的解释框架'}
{'arxiv_id': 'arXiv:2505.13718', 'title': 'Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings', 'authors': 'Safal Shrestha, Minwu Kim, Aadim Nepal, Anubhav Shrestha, Keith Ross', 'link': 'https://arxiv.org/abs/2505.13718', 'abstract': 'Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the amount of quality training data is scarce. We propose a sample-efficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we "warm up" the model by distilling Long CoTs from a toy domain, namely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using a limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: $(i)$ the warmup phase alone facilitates generalized reasoning, leading to performance improvements across a range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both the base model and the warmed-up model are RLVR trained on the same small dataset ($\\leq100$ examples), the warmed-up model consistently outperforms the base model; $(iii)$ Warming up before RLVR training allows a model to maintain cross-domain generalizability even after training on a specific domain; $(iv)$ Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments.', 'abstract_zh': '在少量监督下设计有效的推理-capable大语言模型的一种样本高效双阶段训练策略', 'title_zh': '热身再训练：在资源受限环境中解锁通用推理能力'}
{'arxiv_id': 'arXiv:2505.13672', 'title': 'A*-Decoding: Token-Efficient Inference Scaling', 'authors': 'Giannis Chatziveroglou', 'link': 'https://arxiv.org/abs/2505.13672', 'abstract': 'Inference-time scaling has emerged as a powerful alternative to parameter scaling for improving language model performance on complex reasoning tasks. While existing methods have shown strong performance gains under fixed compute budgets, there has been little focus on optimally utilizing that budget during inference. In this work, we introduce A*-decoding, a search-based inference-time strategy that builds on the A* search algorithm to optimally utilize a fixed compute budget by prioritizing high-quality reasoning paths during generation. We frame language model decoding as a structured search in a state space of partial solutions, applying the A* transition model to identify promising continuations guided by an external process supervision signal. In our experiments, A*-decoding reaches the performance levels of strong inference scaling baselines like best-of-N and particle filtering while using up to 3x fewer tokens and 30% fewer PRM passes under equivalent compute budgets. On the MATH500 and AIME 2024 benchmarks, A*-decoding enables Llama-3.2-1B-Instruct to match the performance of the 70x larger Llama-3.1-70B-Instruct, and allows Qwen3-1.7B to reach o1-like reasoning accuracy. These results highlight the power of structured search in decoding, offering an alternative to brute-force sampling or scale-driven gains. Our work demonstrates how thoughtful inference-time strategies can enhance reasoning in SLMs, pointing toward future advances in more efficient and scalable language model deployment.', 'abstract_zh': '基于A*解码的推理时_scaled参数调整作为提升复杂推理任务语言模型性能的有力替代方法，在固定计算预算下，现有方法已显示出强劲的性能提升，但对如何最优化利用该预算的研究较少。在本工作中，我们引入了基于A*解码策略，该策略通过优先生成高质量的推理路径来优化固定计算预算的使用。我们将语言模型解码视为在部分解状态空间中的结构化搜索，并使用A*转移模型根据外部过程监督信号识别有前景的延续。实验表明，基于A*解码达到与.best-of-N 和粒子滤波等强大推理时_scale参数调整基线相当的性能水平，但使用的tokens数最多减少3倍，PRM遍历次数减少30%。在MATH500和AIME 2024基准测试中，基于A*解码使Llama-3.2-1B-Instruct达到与70倍更大的Llama-3.1-70B-Instruct相当的性能，并使Qwen3-1.7B达到类似o1的推理准确性。这些结果突显了结构化搜索在解码中的强大能力，为另一种 brute-force抽样或规模驱动的性能提升提供了替代方案。我们的研究展示了如何通过精心设计的推理时策略增强大规模语言模型的推理能力，指出了在未来更高效和可扩展的语言模型部署中的潜在进展。', 'title_zh': 'A*-解码：.token-高效推断扩展'}
{'arxiv_id': 'arXiv:2505.13561', 'title': 'Language and Thought: The View from LLMs', 'authors': 'Daniel Rothschild', 'link': 'https://arxiv.org/abs/2505.13561', 'abstract': 'Daniel Dennett speculated in *Kinds of Minds* 1996: "Perhaps the kind of mind you get when you add language to it is so different from the kind of mind you can have without language that calling them both minds is a mistake." Recent work in AI can be seen as testing Dennett\'s thesis by exploring the performance of AI systems with and without linguistic training. I argue that the success of Large Language Models at inferential reasoning, limited though it may be, supports Dennett\'s radical view about the effect of language on thought. I suggest it is the abstractness and efficiency of linguistic encoding that lies behind the capacity of LLMs to perform inferences across a wide range of domains. In a slogan, language makes inference computationally tractable. I assess what these results in AI indicate about the role of language in the workings of our own biological minds.', 'abstract_zh': '丹尼尔·狄恩特在1996年的《心智的种类》中推测：“也许当你向一个没有语言的心智中添加语言时，所获得的那种心智种类如此不同，以至于称它们为心智可能就是一种误判。”最近的人工智能研究可以被视为狄恩特假说的一种验证，通过探索具有和不具备语言训练的AI系统的性能。我认为，尽管大型语言模型在演绎推理方面的成功（尽管是有限的）支持了关于语言对思维影响的狄恩特激进观点。我认为是语言编码的抽象性和效率使得大型语言模型能够在广泛的不同领域中进行推理。简言之，语言使演绎推理在计算上变得可行。我评估这些人工智能成果对我们自身生物性心智的运作中语言作用的指示意义。', 'title_zh': '语言与思维：从大规模语言模型视角看'}
{'arxiv_id': 'arXiv:2505.13546', 'title': 'Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt in General-Purpose Systems', 'authors': 'Ke Chen, Yufei Zhou, Xitong Zhang, Haohan Wang', 'link': 'https://arxiv.org/abs/2505.13546', 'abstract': 'Automatic prompt generation plays a crucial role in enabling general-purpose multi-agent systems to perform diverse tasks autonomously. Existing methods typically evaluate prompts based on their immediate task performance, overlooking the intrinsic qualities that determine their reliability. This outcome-centric view not only limits interpretability but also fails to account for the inherent stochasticity of large language models (LLMs). In this work, we bring attention to prompt stability-the consistency of model responses across repeated executions-as a key factor for building robust and effective prompt generation systems. To quantify this, we propose semantic stability as a criterion for assessing the response consistency of prompts, and fine-tune a LLaMA-based evaluator to measure it automatically across tasks. These components have enabled us to develop the first stability-aware general-purpose prompt generation system that leverages stability feedback to iteratively enhance both prompt quality and system-level performance. Furthermore, we establish a logical chain between prompt stability and task success by analyzing the structural dependencies within our system, proving stability as a necessary condition for effective system-level execution. Empirical results across general and domain-specific tasks demonstrate that our stability-aware framework improves both accuracy and output consistency. By shifting the focus from one-off results to persistent reliability, our work offers a new perspective on prompt design and contributes practical tools for building more trustworthy general-purpose systems.', 'abstract_zh': '自动提示生成在使通用多代理系统能够自主执行多样化任务中发挥着关键作用。现有方法通常根据提示的即时任务性能对其进行评估，忽略了决定其可靠性的内在特性。这种以结果为中心的观点不仅限制了可解释性，同时也未能考虑大型语言模型（LLMs）的固有随机性。在本工作中，我们将注意力转向提示稳定性——模型响应在重复执行中的一致性——作为构建稳健且有效的提示生成系统的关键因素。为量化这一特性，我们提出语义稳定性作为评估提示响应一致性的标准，并针对任务自动测量其值的LLaMA基评估器进行微调。这些组件使我们能够开发出首个具备稳定性的通用提示生成系统，该系统利用稳定性反馈逐步提升提示质量和系统级性能。此外，通过分析系统中的结构性依赖关系，我们建立了提示稳定性和任务成功之间的逻辑联系，证明稳定性是实现有效系统级执行的必要条件。在通用和特定领域的任务中，实验证据显示，我们的稳定性感知框架提高了准确性和输出一致性。通过将焦点从单一结果转移到持久可靠性，我们的工作为提示设计提供了新的视角，并为构建更具可信度的通用系统提供了实用工具。', 'title_zh': '提示稳定性至关重要：评估与优化通用系统中自动生成的提示'}
{'arxiv_id': 'arXiv:2505.13533', 'title': 'FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs', 'authors': 'Junzhe Jiang, Chang Yang, Aixin Cui, Sihan Jin, Ruiyu Wang, Bo Li, Xiao Huang, Dongning Sun, Xinrun Wang', 'link': 'https://arxiv.org/abs/2505.13533', 'abstract': 'Financial tasks are pivotal to global economic stability; however, their execution faces challenges including labor intensive processes, low error tolerance, data fragmentation, and tool limitations. Although large language models (LLMs) have succeeded in various natural language processing tasks and have shown potential in automating workflows through reasoning and contextual understanding, current benchmarks for evaluating LLMs in finance lack sufficient domain-specific data, have simplistic task design, and incomplete evaluation frameworks. To address these gaps, this article presents FinMaster, a comprehensive financial benchmark designed to systematically assess the capabilities of LLM in financial literacy, accounting, auditing, and consulting. Specifically, FinMaster comprises three main modules: i) FinSim, which builds simulators that generate synthetic, privacy-compliant financial data for companies to replicate market dynamics; ii) FinSuite, which provides tasks in core financial domains, spanning 183 tasks of various types and difficulty levels; and iii) FinEval, which develops a unified interface for evaluation. Extensive experiments over state-of-the-art LLMs reveal critical capability gaps in financial reasoning, with accuracy dropping from over 90% on basic tasks to merely 40% on complex scenarios requiring multi-step reasoning. This degradation exhibits the propagation of computational errors, where single-metric calculations initially demonstrating 58% accuracy decreased to 37% in multimetric scenarios. To the best of our knowledge, FinMaster is the first benchmark that covers full-pipeline financial workflows with challenging tasks. We hope that FinMaster can bridge the gap between research and industry practitioners, driving the adoption of LLMs in real-world financial practices to enhance efficiency and accuracy.', 'abstract_zh': '金融任务对于全球经济稳定至关重要；然而，这些任务的执行面临着劳动密集型流程、低容错率、数据碎片化和工具限制等挑战。尽管大型语言模型（LLMs）已经在各种自然语言处理任务中取得成功，并展示了通过推理和情境理解自动化工作流的潜力，但现有对LLMs在金融领域的评估基准缺乏特定领域的数据，任务设计过于简单，且评估框架不完整。为弥补这些不足，本文提出了FinMaster，一个全面的金融基准，旨在系统评估LLMs在金融知识、会计、审计和咨询方面的能力。具体而言，FinMaster包含三个主要模块：i) FinSim，构建模拟器以生成合成且符合隐私要求的金融数据，用于模拟市场动态；ii) FinSuite，提供涵盖核心金融领域的任务，共计183项不同类型和难度的任务；iii) FinEval，开发统一的评估界面。通过对最新一代LLMs的广泛实验表明，金融推理能力存在关键差距，基本任务的准确率从超过90%降至复杂场景下的40%，其中多指标场景中以往显示58%准确率的单指标计算准确性降至37%。据我们所知，FinMaster是首个涵盖全流程金融工作流的具有挑战性任务的基准。我们希望FinMaster能够弥合研究与行业实践之间的差距，推动LLMs在实际金融实践中的应用，以提高效率和准确性。', 'title_zh': 'FinMaster: 一个全面的基准，用于利用大语言模型掌握全流程金融工作流程'}
{'arxiv_id': 'arXiv:2505.13529', 'title': 'BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs', 'authors': 'Junxiao Yang, Jinzhe Tu, Haoran Liu, Xiaoce Wang, Chujie Zheng, Zhexin Zhang, Shiyao Cui, Caishun Chen, Tiantian He, Hongning Wang, Yew-Soon Ong, Minlie Huang', 'link': 'https://arxiv.org/abs/2505.13529', 'abstract': 'Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don\'t know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL-a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs.', 'abstract_zh': 'Recent Advances in Large Reasoning Models: Promoting Concise and Boundary-Aware Factual Reasoning', 'title_zh': 'BARREL: 边界感知推理以实现事实可靠的知识表示模型'}
{'arxiv_id': 'arXiv:2505.13511', 'title': 'Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale', 'authors': 'David Noever, Forrest McKee', 'link': 'https://arxiv.org/abs/2505.13511', 'abstract': 'This study explores Large Language Models (LLMs) as autonomous agents for real-world tasks, including freelance software development. This work presents a new benchmark that evaluates LLMs on freelance programming and data analysis tasks derived from economic data. We construct the benchmark using synthetic tasks created from a Kaggle Freelancer dataset of job postings, with all job prices standardized to USD (median fixed-project price around $250, and an average of $306). Each task is accompanied by structured input-output test cases and an estimated price tag, enabling automated correctness checking and a monetary performance valuation. This approach is inspired by OpenAI\'s recent SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our framework simplifies evaluation using programmatically testable tasks and predicted price values, making it highly scalable and repeatable. On this benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen 2.5, and Mistral. We report each model\'s accuracy (task success rate and test-case pass rate) and the total "freelance earnings" it achieves (sum of prices of solved tasks). Our results show that Claude 3.5 Haiku performs best, earning approximately $1.52 million USD, followed closely by GPT-4o-mini at $1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the distribution of errors per task and observe that the strongest models solve the most tasks and rarely fail completely on any project. We discuss the implications of these results for the feasibility of AI as a freelance developer, the advantages and limitations of our automated benchmark approach, and the gap between performance on structured tasks versus the true complexity of real-world freelance jobs.', 'abstract_zh': '本研究探索大型语言模型（LLMs）作为自主代理在实际工作任务中的应用，包括自由职业软件开发。本文提出了一项新的基准测试，评估LLMs在源自经济学数据的自由职业编程和数据分析任务中的表现。我们使用来自Kaggle Freelancer数据集的工作招聘广告构建基准测试，所有工作价格都标准化为美元（中位数固定项目价格约为250美元，平均值为306美元）。每个任务都附有结构化的输入输出测试案例和估计的价格标签，这使得自动化正确性检查和货币性能评价成为可能。该方法受到了OpenAI最近的SWE-Lancer基准测试（总计10万美元的1400个实际Upwork任务）的启发。然而，我们的框架通过使用编程可测试的任务和预测的价格值简化了评估过程，使其具有高度的可扩展性和可重复性。在该基准测试中，我们评估了四款现代LLMs——Claude 3.5 Haiku、GPT-4o-mini、Qwen 2.5和Mistral。我们报告了每款模型的准确性（任务成功率和测试案例通过率）以及其实现的总“自由职业收入”（完成任务的价格总和）。结果显示，Claude 3.5 Haiku表现最佳，赚取约152万美元，紧随其后的是GPT-4o-mini（149万美元），然后是Qwen 2.5（133万美元）和Mistral（70万美元）。我们分析了每项任务的错误分布，并观察到最强的模型解决了最多任务，并且几乎未在任何项目中完全失败。我们讨论了这些结果对AI作为自由职业开发者的可行性的影响，以及我们自动化基准测试方法的优点和局限性，并探讨了在结构化任务表现与真实世界自由职业工作的复杂性之间的差距。', 'title_zh': 'AI自由职业者能否竞争？大规模benchmarking报酬、可靠性和任务成功率'}
{'arxiv_id': 'arXiv:2505.13484', 'title': 'Evaluating Large Language Models for Real-World Engineering Tasks', 'authors': 'Rene Heesch, Sebastian Eilermann, Alexander Windmann, Alexander Diedrich, Philipp Rosenthal, Oliver Niggemann', 'link': 'https://arxiv.org/abs/2505.13484', 'abstract': 'Large Language Models (LLMs) are transformative not only for daily activities but also for engineering tasks. However, current evaluations of LLMs in engineering exhibit two critical shortcomings: (i) the reliance on simplified use cases, often adapted from examination materials where correctness is easily verifiable, and (ii) the use of ad hoc scenarios that insufficiently capture critical engineering competencies. Consequently, the assessment of LLMs on complex, real-world engineering problems remains largely unexplored. This paper addresses this gap by introducing a curated database comprising over 100 questions derived from authentic, production-oriented engineering scenarios, systematically designed to cover core competencies such as product design, prognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art LLMs, including both cloud-based and locally hosted instances, to systematically investigate their performance on complex engineering tasks. Our results show that LLMs demonstrate strengths in basic temporal and structural reasoning but struggle significantly with abstract reasoning, formal modeling, and context-sensitive engineering logic.', 'abstract_zh': '大型语言模型（LLMs）不仅在日常活动，而且在工程任务中都具有变革性。然而，当前对LLMs在工程领域的评估存在两个关键不足：（i）依赖于简化的用例，这些用例往往源自易于验证正确性的考试材料；（ii）使用缺乏系统性和全面性的场景，无法充分捕捉关键的工程能力。因此，对复杂的真实世界工程问题的评估仍存在很大空白。本文通过引入一个包含超过100个问题的精心整理数据库来填补这一空白，这些问题源自真实的、以生产为导向的工程情景，系统地设计以涵盖核心能力，如产品设计、预测和诊断。利用该数据集，我们评估了四种最先进的LLMs（包括云托管和本地托管实例），以系统性地研究其在复杂工程任务中的表现。我们的结果显示，LLMs在基本的时间和结构推理方面表现出优势，但在抽象推理、形式建模和情境敏感的工程逻辑方面面临重大挑战。', 'title_zh': '评估大型语言模型在实际工程任务中的性能'}
{'arxiv_id': 'arXiv:2505.14684', 'title': 'Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning', 'authors': 'Haolei Xu, Yuchen Yan, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Shengpei Jiang, Kaitao Song, Weiming Lu, Jun Xiao, Yueting Zhuang', 'link': 'https://arxiv.org/abs/2505.14684', 'abstract': 'Large language models (LLMs) have achieved remarkable progress on mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.', 'abstract_zh': '大规模语言模型（LLMs）通过链式思考（CoT）推理在数学任务上取得了显著进展。然而，现有的数学CoT数据集往往由于专家省略了中间步骤而受到跳跃（Thought Leaps）的影响，这负面影响了模型的学习和泛化能力。我们提出了CoT跳跃桥接任务，旨在自动检测跳跃并生成缺失的中间推理步骤，以恢复CoT的完整性和连贯性。为实现这一目标，我们基于结构化的ScaleQuestMath数据集构建了一个专门的训练数据集ScaleQM+，并训练了CoT-Bridge以桥接这些跳跃。通过在数学推理基准上的全面实验，我们表明，使用桥接数据集微调的模型始终优于使用原始数据集微调的模型，在NuminaMath上最高可提高5.87%。我们的方法有效提升了压缩数据（+3.02%）并为强化学习提供了更好的起始点（+3.1%），并可与现有优化技术无缝集成。此外，CoT-Bridge在领域外逻辑推理任务上的泛化能力提升，证实了增强推理完整性的广泛应用优势。', 'title_zh': '填坑：提升链式思维调优中的思想飞跃'}
{'arxiv_id': 'arXiv:2505.14661', 'title': 'Abacus: A Cost-Based Optimizer for Semantic Operator Systems', 'authors': 'Matthew Russo, Sivaprasad Sudhir, Gerardo Vitagliano, Chunwei Liu, Tim Kraska, Samuel Madden, Michael Cafarella', 'link': 'https://arxiv.org/abs/2505.14661', 'abstract': 'LLMs enable an exciting new class of data processing applications over large collections of unstructured documents. Several new programming frameworks have enabled developers to build these applications by composing them out of semantic operators: a declarative set of AI-powered data transformations with natural language specifications. These include LLM-powered maps, filters, joins, etc. used for document processing tasks such as information extraction, summarization, and more. While systems of semantic operators have achieved strong performance on benchmarks, they can be difficult to optimize. An optimizer for this setting must determine how to physically implement each semantic operator in a way that optimizes the system globally. Existing optimizers are limited in the number of optimizations they can apply, and most (if not all) cannot optimize system quality, cost, or latency subject to constraint(s) on the other dimensions. In this paper we present Abacus, an extensible, cost-based optimizer which searches for the best implementation of a semantic operator system given a (possibly constrained) optimization objective. Abacus estimates operator performance by leveraging a minimal set of validation examples and, if available, prior beliefs about operator performance. We evaluate Abacus on document processing workloads in the biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering (MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2% better quality and up to 23.6x lower cost and 4.2x lower latency than the next best system.', 'abstract_zh': 'LLM技术使得在大量非结构化文档集合上处理数据的应用变得令人兴奋。多种新的编程框架允许开发者通过组合语义操作符构建这些应用，这些语义操作符是一个具有自然语言规格的声明式AI驱动的数据转换集合，包括由LLM支持的映射、过滤、连接等操作符，用于文档处理任务如信息抽取、总结等。尽管语义操作符系统在基准测试中表现优秀，但它们在优化方面可能存在问题。在这个环境中，优化器必须确定如何以全局优化的方式物理实现每个语义操作符。现有的优化器在可以应用的优化数量上受到限制，大多数（如果不是全部）优化器无法在满足某些约束条件下优化系统质量、成本或延迟。在本文中，我们提出了Abacus，一个可扩展、基于成本的优化器，可在给定（可能受限的）优化目标的情况下搜索语义操作符系统的最佳实现。Abacus通过利用少量验证示例和可获得的关于操作符性能的先验信念来估计操作符性能。我们在生物医学和法律领域（BioDEX、CUAD）的文档处理工作负载以及多模态问答（MMQA）中评估了Abacus。结果显示，由Abacus优化的系统在质量上提高了18.7%-39.2%，在成本上降低了23.6倍，在延迟上降低了4.2倍，优于次优系统。', 'title_zh': '算盘：面向语义操作系统的成本基优化器'}
{'arxiv_id': 'arXiv:2505.14654', 'title': 'Beyond Words: Multimodal LLM Knows When to Speak', 'authors': 'Zikai Liao, Yi Ouyang, Yi-Lun Lee, Chen-Ping Yu, Yi-Hsuan Tsai, Zhaozheng Yin', 'link': 'https://arxiv.org/abs/2505.14654', 'abstract': 'While large language model (LLM)-based chatbots have demonstrated strong capabilities in generating coherent and contextually relevant responses, they often struggle with understanding when to speak, particularly in delivering brief, timely reactions during ongoing conversations. This limitation arises largely from their reliance on text input, lacking the rich contextual cues in real-world human dialogue. In this work, we focus on real-time prediction of response types, with an emphasis on short, reactive utterances that depend on subtle, multimodal signals across vision, audio, and text. To support this, we introduce a new multimodal dataset constructed from real-world conversational videos, containing temporally aligned visual, auditory, and textual streams. This dataset enables fine-grained modeling of response timing in dyadic interactions. Building on this dataset, we propose MM-When2Speak, a multimodal LLM-based model that adaptively integrates visual, auditory, and textual context to predict when a response should occur, and what type of response is appropriate. Experiments show that MM-When2Speak significantly outperforms state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x improvement in response timing accuracy over leading commercial LLMs. These results underscore the importance of multimodal inputs for producing timely, natural, and engaging conversational AI.', 'abstract_zh': '基于大型语言模型的聊天机器人在生成连贯且上下文相关响应方面表现出强大能力，但在理解何时发言，尤其是在线上对话中及时作出简短反应方面常常存在困难。这一限制主要源于它们依赖文本输入，缺乏真实世界人类对话中的丰富语境线索。在本工作中，我们专注于实时预测响应类型，重点是依赖于多重模态信号（包括视觉、听觉和文本）的简短、反应性陈述。为此，我们引入了一个新的多重模态数据集，该数据集基于真实世界的对话视频，包含时间对齐的视觉、听觉和文本流，以精细建模双人互动中的响应时间。基于此数据集，我们提出了一种名为MM-When2Speak的多重模态大型语言模型（LLM）模型，该模型能够适应性地整合视觉、听觉和文本上下文以预测何时应作出响应，以及什么类型的响应最为恰当。实验结果显示，MM-When2Speak在响应时间准确性上显著优于最先进的单模态和LLM基线，相对于领先的商用LLM，响应时间准确性可提高4倍。这些结果强调了多重模态输入对于生成及时、自然且引人入胜的对话AI的重要性。', 'title_zh': '超越文本：多模态LLM知道何时发言'}
{'arxiv_id': 'arXiv:2505.14629', 'title': 'KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models', 'authors': 'Fnu Mohbat, Mohammed J Zaki', 'link': 'https://arxiv.org/abs/2505.14629', 'abstract': 'Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at this https URL.', 'abstract_zh': 'Recent advances in大型语言模型(LLMs)和食品数据的丰富使得利用LLMs提升食品理解的研究日益增多。尽管已有若干利用LLMs和知识图谱(KGs)的推荐系统，但将食品相关的KGs与LLMs集成的研究相对有限。我们提出了KERL，这是一种结合食品KGs和LLMs的统一系统，用于提供个性化的食品推荐并生成包含微营养信息的食谱。给定自然语言问题，KERL抽取实体、从KG中检索子图，这些子图随后作为上下文输入到LLM中以选择满足约束条件的食谱。接下来，我们的系统生成每份食谱的烹饪步骤和营养信息。为评估我们的方法，我们还开发了一个基准数据集，该数据集由食谱相关问题、约束条件和个人偏好组成。通过广泛的实验，我们证明了我们提出的增强KG的LLM显著优于现有方法，提供了食品推荐、食谱生成和营养分析的完整而连贯的解决方案。我们的代码和基准数据集可在以下网址公开获取。', 'title_zh': 'KERL：基于大型语言模型的知识增强个性化食谱推荐'}
{'arxiv_id': 'arXiv:2505.14625', 'title': 'TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning', 'authors': 'Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran', 'link': 'https://arxiv.org/abs/2505.14625', 'abstract': "Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at this https URL.", 'abstract_zh': '强化学习（RL）已成为通过奖励信号优化大型语言模型（LLMs）推理能力的一种强大工具。然而，RL的成功依赖于验证器提供的奖励信号的可靠性。在本文中，我们揭示并分析了一个普遍存在的问题——假阴性，即验证器错误地拒绝了正确的模型输出。通过对Big-Math-RL-Verified数据集的深入研究，我们发现超过38%的模型生成响应受到了假阴性的影响，验证器未能识别出正确的答案。我们通过实证和理论分析表明，这些假阴性严重阻碍了RL训练，剥夺了模型获取有用梯度信号的机会，并减缓了收敛速度。为了缓解这一问题，我们提出了tinyV，一种轻量级的基于LLM的验证器，它可以增强现有的基于规则的方法，动态识别潜在的假阴性，并恢复有效的响应以生成更准确的奖励估计。在多个数学推理基准测试中，集成TinyV可以提高通过率高达10%并加速收敛，相对于基线方法。我们的研究突出了解决验证器假阴性的关键重要性，并提供了一种实际方法来改善基于RL的LLM微调。代码已发布在此：https://github.com/alibaba/Qwen-tinyV', 'title_zh': 'TinyV: 减少验证中的假阴性改进大语言模型推理的RL方法'}
{'arxiv_id': 'arXiv:2505.14608', 'title': 'Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)', 'authors': 'Rafael Rivera Soto, Barry Chen, Nicholas Andrews', 'link': 'https://arxiv.org/abs/2505.14608', 'abstract': 'Despite considerable progress in the development of machine-text detectors, it has been suggested that the problem is inherently hard, and therefore, that stakeholders should proceed under the assumption that machine-generated text cannot be reliably detected as such. We examine a recent such claim by Nicks et al. (2024) regarding the ease with which language models can be optimized to degrade the performance of machine-text detectors, including detectors not specifically optimized against. We identify a feature space$\\unicode{x2013}$the stylistic feature space$\\unicode{x2013}$that is robust to such optimization, and show that it may be used to reliably detect samples from language models optimized to prevent detection. Furthermore, we show that even when models are explicitly optimized against stylistic detectors, detection performance remains surprisingly unaffected. We then seek to understand if stylistic detectors are inherently more robust. To study this question, we explore a new paraphrasing approach that simultaneously aims to close the gap between human writing and machine writing in stylistic feature space while avoiding detection using traditional features. We show that when only a single sample is available for detection, this attack is universally effective across all detectors considered, including those that use writing style. However, as the number of samples available for detection grows, the human and machine distributions become distinguishable. This observation encourages us to introduce AURA, a metric that estimates the overlap between human and machine-generated distributions by analyzing how detector performance improves as more samples become available. Overall, our findings underscore previous recommendations to avoid reliance on machine-text detection.', 'abstract_zh': '尽管在机器文本检测器的发展方面取得了显著进展，有人认为这个问题本质上很难解决，因此建议利益相关方在假设机器生成的文本不可靠地被检测出的情况下进行操作。我们审视了Nicks等人（2024）关于语言模型可以通过优化大幅削弱机器文本检测器性能的近期主张，包括那些未特别针对其优化的检测器。我们确定了一个稳健的特征空间——风格特征空间——并展示了它可以用于可靠地检测那些旨在防止被检测的语言模型生成的样本。此外，我们证明即使模型被明确优化以对抗风格检测器，其检测性能仍然出乎意料地未受影响。我们随后试图理解风格检测器是否更加内在地稳健。为研究这一问题，我们探索了一种新的改写方法，旨在同时缩小人类写作与机器写作在风格特征空间中的差距，同时避免使用传统特征被检测。我们表明，当只有单个样本可用于检测时，这种攻击对所有考虑的检测器都是普遍有效的，包括那些使用文本风格的检测器。然而，随着可用样本数量的增加，人类和机器生成的样本分布变得可区分。这一观察促使我们提出了AURA度量，该度量通过分析检测器性能随可用样本数量增加而改善的方式，估计了人类生成和机器生成分布之间的重叠。总体而言，我们的研究结果再次强调了避免依赖机器文本检测的先前建议。', 'title_zh': '语言模型优化以欺骗检测器仍然具有独特的风格（以及如何改变它）'}
{'arxiv_id': 'arXiv:2505.14599', 'title': 'Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'authors': 'Guangzhi Xiong, Eric Xie, Corey Williams, Myles Kim, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang', 'link': 'https://arxiv.org/abs/2505.14599', 'abstract': 'Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在生物医学等科学学科中展示了显著潜力，特别是在假设生成方面，它们可以分析大量文献、识别模式并建议研究方向。然而，验证生成假设的准确性往往需要大量时间和资源，这构成了一个关键挑战。此外，LLMs中的幻觉问题可能导致生成看似合理但实际上错误的假设，这削弱了它们的可靠性。为了促进对这些挑战的系统研究，我们引入了TruthHypo基准，用于评估LLMs生成真实生物医学假设的能力，以及KnowHD知识为基础的幻觉检测器，以评估假设与现有知识的关联程度。我们的研究结果表明，LLMs难以生成真实假设。通过对推理步骤中的幻觉进行分析，我们证明了KnowHD提供的接地得分充当了有效指标，用于筛选来自LLMs多样化输出的真实假设。人类评估进一步验证了KnowHD在识别真实假设和加速科学发现方面的有用性。我们的数据和源代码可在以下网址获取。', 'title_zh': '向可靠的生物医学假设生成迈进：评估大规模语言模型的truthfulness和hallucination'}
{'arxiv_id': 'arXiv:2505.14552', 'title': 'KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation', 'authors': 'Jiajun Shi, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, Zhoufutu Wen, Bingli Wang, Yancheng He, Liang Song, Hualei Zhu, Shilong Li, Xingjian Wang, Wei Zhang, Ruibin Yuan, Yifan Yao, Wenjun Yang, Yunli Wang, Siyuan Fang, Siyu Yuan, Qianyu He, Xiangru Tang, Yingshui Tan, Wangchunshu Zhou, Zhaoxiang Zhang, Zhoujun Li, Wenhao Huang, Ge Zhang', 'link': 'https://arxiv.org/abs/2505.14552', 'abstract': "Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM's general reasoning potential. To address this limitation, we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments.", 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）放大了对其推理能力进行更全面评估方法的需求。现有的基准通常具有领域特定性，因此无法全面捕捉LLM的一般推理潜力。为解决这一局限，我们引入了知识正交推理体育馆（KORGym），这是一个受到KOR-Bench和体育馆启发的动力评估平台。KORGym提供了五十多种以文本或视觉形式呈现的游戏，并支持基于强化学习场景的交互式多轮评估。使用KORGym，我们在19种LLM和8种VLM上进行了广泛的实验，揭示了模型家族中的稳健推理模式，并展示了闭源模型的优越性能。进一步的分析探索了模态、推理策略、强化学习技术及响应长度对模型性能的影响。我们期望KORGym能够成为推进LLM推理研究和开发适用于复杂交互环境的评估方法的重要资源。', 'title_zh': 'KORGym：一种动态游戏平台，用于LLM推理评估'}
{'arxiv_id': 'arXiv:2505.14549', 'title': 'Can Large Language Models Really Recognize Your Name?', 'authors': 'Dzung Pham, Peter Kairouz, Niloofar Mireshghallah, Eugene Bagdasarian, Chau Minh Pham, Amir Houmansadr', 'link': 'https://arxiv.org/abs/2505.14549', 'abstract': 'Large language models (LLMs) are increasingly being used to protect sensitive user data. However, current LLM-based privacy solutions assume that these models can reliably detect personally identifiable information (PII), particularly named entities. In this paper, we challenge that assumption by revealing systematic failures in LLM-based privacy tasks. Specifically, we show that modern LLMs regularly overlook human names even in short text snippets due to ambiguous contexts, which cause the names to be misinterpreted or mishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous human names, leveraging the name regularity bias phenomenon, embedded within concise text snippets along with benign prompt injections. Our experiments on modern LLMs tasked to detect PII as well as specialized tools show that recall of ambiguous names drops by 20--40% compared to more recognizable names. Furthermore, ambiguous human names are four times more likely to be ignored in supposedly privacy-preserving summaries generated by LLMs when benign prompt injections are present. These findings highlight the underexplored risks of relying solely on LLMs to safeguard user privacy and underscore the need for a more systematic investigation into their privacy failure modes.', 'abstract_zh': '大型语言模型（LLMs）越来越多地被用于保护敏感用户数据。然而，当前基于LLM的隐私解决方案假设这些模型能可靠地检测个人信息（PII），特别是命名实体。在本文中，我们通过揭示基于LLM的隐私任务中的系统性失败，挑战这一假设。具体而言，我们表明，现代LLMs在短文本片段中经常由于语境模糊而疏忽人类姓名，导致姓名被误释或误处理。我们提出了AMBENCH，一个包含看似模糊的人类姓名基准数据集，利用名称规律性偏差现象，并嵌入简洁的文本片段和良性提示注入。我们的实验表明，当现代LLM被任务驱动以检测PII以及使用专用工具时，模糊姓名的召回率相较于更可识别的姓名下降了20%-40%。此外，在存在良性提示注入的情况下，模糊的人类姓名被LLM生成的所谓隐私保护总结忽略的可能性是后者的大四倍。这些发现突显了仅依赖LLM保护用户隐私所忽视的风险，并强调了对它们隐私失败模式进行更系统性调查的必要性。', 'title_zh': '大型语言模型真的能识别你的名字吗？'}
{'arxiv_id': 'arXiv:2505.14513', 'title': 'Latent Flow Transformer', 'authors': 'Yen-Chen Wu, Feng-Ting Liao, Meng-Hsi Chen, Pei-Chen Ho, Farhang Nabiei, Da-shan Shiu', 'link': 'https://arxiv.org/abs/2505.14513', 'abstract': 'Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in \\textit{preserving coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms.', 'abstract_zh': 'Transformers：大型语言模型（LLMs）的标准实现通常由数十到数百个离散层组成。虽然更多的层可以提高性能，但这种方法因图像生成中扩散和流化模型显示的连续层优势而受到质疑。我们提出了一种名为语义流变换器（LFT）的方法，用一个通过流匹配训练的学习传输算子替代了一块层，实现了显著压缩同时保持与原始架构的兼容性。此外，我们通过引入流行走（FW）算法解决了现有流化方法在保耦合方面的局限性。在Pythia-410M模型上，使用流匹配训练的LFT压缩了24层中的6层，并优于直接跳过2层（语言模型 logits的KL散度为0.407 vs. 0.529），展示了该设计的可行性。使用FW训练时，LFT进一步将12层凝练为一层，KL降低至0.736，超过了跳过3层的结果（0.932），显著缩小了自回归生成与流化生成之间的差距。', 'title_zh': '隐含流变换器'}
{'arxiv_id': 'arXiv:2505.14505', 'title': 'ModRWKV: Transformer Multimodality in Linear Time', 'authors': 'Jiale Kang, Ziyin Yue, Qingyu Yin, Jiang Rui, Weile Li, Zening Lu, Zhouran Ji', 'link': 'https://arxiv.org/abs/2505.14505', 'abstract': "Currently, most multimodal studies are based on large language models (LLMs) with quadratic-complexity Transformer architectures. While linear models like RNNs enjoy low inference costs, their application has been largely limited to the text-only modality. This work explores the capabilities of modern RNN architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal framework built upon the RWKV7 architecture as its LLM backbone-which achieves multi-source information fusion through dynamically adaptable heterogeneous modality encoders. We designed the multimodal modules in ModRWKV with an extremely lightweight architecture and, through extensive experiments, identified a configuration that achieves an optimal balance between performance and computational efficiency. ModRWKV leverages the pretrained weights of the RWKV7 LLM for initialization, which significantly accelerates multimodal training. Comparative experiments with different pretrained checkpoints further demonstrate that such initialization plays a crucial role in enhancing the model's ability to understand multimodal signals. Supported by extensive experiments, we conclude that modern RNN architectures present a viable alternative to Transformers in the domain of multimodal large language models (MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV architecture through systematic exploration.", 'abstract_zh': '现代RNN架构在 multimodal 大语言模型中的能力探索：基于 RWKV7 的 ModRWKV 架构研究', 'title_zh': 'ModRWKV: 在线性时间内实现Transformer多模态技术'}
{'arxiv_id': 'arXiv:2505.14499', 'title': 'Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales', 'authors': 'Jun Cao, Jiyi Li, Ziwei Yang, Renjie Zhou', 'link': 'https://arxiv.org/abs/2505.14499', 'abstract': "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis (MABSA) in recent years. Existing methods predominantly rely on pre-trained small language models (SLMs) to collect information related to aspects and sentiments from both image and text, with an aim to align these two modalities. However, small SLMs possess limited capacity and knowledge, often resulting in inaccurate identification of meaning, aspects, sentiments, and their interconnections in textual and visual data. On the other hand, Large language models (LLMs) have shown exceptional capabilities in various tasks by effectively exploring fine-grained information in multimodal data. However, some studies indicate that LLMs still fall short compared to fine-tuned small models in the field of ABSA. Based on these findings, we propose a novel framework, termed LRSA, which combines the decision-making capabilities of SLMs with additional information provided by LLMs for MABSA. Specifically, we inject explanations generated by LLMs as rationales into SLMs and employ a dual cross-attention mechanism for enhancing feature interaction and fusion, thereby augmenting the SLMs' ability to identify aspects and sentiments. We evaluated our method using two baseline models, numerous experiments highlight the superiority of our approach on three widely-used benchmarks, indicating its generalizability and applicability to most pre-trained models for MABSA.", 'abstract_zh': '多模态方面基于情感分析（MABSA）中的LRSA新型框架', 'title_zh': '增强多模态方面情感分析的LLM生成理由方法'}
{'arxiv_id': 'arXiv:2505.14469', 'title': 'Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations', 'authors': 'Somnath Banerjee, Pratyush Chatterjee, Shanu Kumar, Sayan Layek, Parag Agrawal, Rima Hazra, Animesh Mukherjee', 'link': 'https://arxiv.org/abs/2505.14469', 'abstract': "Recent advancements in LLMs have raised significant safety concerns, particularly when dealing with code-mixed inputs and outputs. Our study systematically investigates the increased susceptibility of LLMs to produce unsafe outputs from code-mixed prompts compared to monolingual English prompts. Utilizing explainability methods, we dissect the internal attribution shifts causing model's harmful behaviors. In addition, we explore cultural dimensions by distinguishing between universally unsafe and culturally-specific unsafe queries. This paper presents novel experimental insights, clarifying the mechanisms driving this phenomenon.", 'abstract_zh': "Recent advancements in LLMs have raised significant safety concerns, especially when handling code-mixed inputs and outputs. Our study systematically investigates the heightened vulnerability of LLMs to generate unsafe outputs from code-mixed prompts compared to monolingual English prompts. Using explainability methods, we analyze the internal attribution shifts leading to the model's harmful behaviors. Additionally, we explore cultural dimensions by differentiating between universally unsafe and culturally-specific unsafe queries. This paper presents novel experimental insights, clarifying the mechanisms driving this phenomenon.", 'title_zh': '大规模语言模型在代码混杂扰动下的归因安全性故障'}
{'arxiv_id': 'arXiv:2505.14455', 'title': 'CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation', 'authors': 'Chihan Huang, Hao Tang', 'link': 'https://arxiv.org/abs/2505.14455', 'abstract': 'Although autoregressive models have dominated language modeling in recent years, there has been a growing interest in exploring alternative paradigms to the conventional next-token prediction framework. Diffusion-based language models have emerged as a compelling alternative due to their powerful parallel generation capabilities and inherent editability. However, these models are often constrained by fixed-length generation. A promising direction is to combine the strengths of both paradigms, segmenting sequences into blocks, modeling autoregressive dependencies across blocks while leveraging discrete diffusion to estimate the conditional distribution within each block given the preceding context. Nevertheless, their practical application is often hindered by two key limitations: rigid fixed-length outputs and a lack of flexible control mechanisms. In this work, we address the critical limitations of fixed granularity and weak controllability in current large diffusion language models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive framework that adaptively determines the size of each generation block based on local semantics using reinforcement learning. Furthermore, we introduce a classifier-guided control mechanism tailored to discrete diffusion, which significantly reduces computational overhead while facilitating efficient post-hoc conditioning without retraining. Extensive experiments demonstrate that CtrlDiff sets a new standard among hybrid diffusion models, narrows the performance gap to state-of-the-art autoregressive approaches, and enables effective conditional text generation across diverse tasks.', 'abstract_zh': '尽管自回归模型在近年来的语言建模中占主导地位，但人们对探索传统的下一个标记预测框架之外的替代范式越来越感兴趣。基于扩散的语言模型因其强大的并行生成能力和固有的可编辑性而成为一种有吸引力的替代方案。然而，这些模型通常受到固定长度生成的限制。一种有前景的方向是结合这两种范式的优点，将序列分割成块，在块之间建模自回归依赖关系，同时利用离散扩散估计给定前文语境下每个块内的条件分布。然而，它们的实际应用常常受到两个关键限制的阻碍：刚性固定长度输出和缺乏灵活的控制机制。在本文中，我们解决了当前大型扩散语言模型中固定粒度和弱可控性的关键限制。我们提出了一种名为CtrlDiff的动态可控半自回归框架，利用强化学习根据局部语义自适应地确定每个生成块的大小。此外，我们引入了一种针对离散扩散的分类器引导控制机制，显著减少了计算开销，同时促进了高效的后验调整而无需重新训练。广泛的实验表明，CtrlDiff在混合扩散模型中确立了新的标准，缩小了与最先进的自回归方法之间的性能差距，并支持在各种任务中有效生成条件文本。', 'title_zh': 'CtrlDiff: 通过动态块预测和可控生成增强大型扩散语言模型'}
{'arxiv_id': 'arXiv:2505.14442', 'title': 'Creative Preference Optimization', 'authors': 'Mete Ismayilzada, Antonio Laverghetta Jr., Simone A. Luchini, Reet Patel, Antoine Bosselut, Lonneke van der Plas, Roger Beaty', 'link': 'https://arxiv.org/abs/2505.14442', 'abstract': "While Large Language Models (LLMs) have demonstrated impressive performance across natural language generation tasks, their ability to generate truly creative content-characterized by novelty, diversity, surprise, and quality-remains limited. Existing methods for enhancing LLM creativity often focus narrowly on diversity or specific tasks, failing to address creativity's multifaceted nature in a generalizable way. In this work, we propose Creative Preference Optimization (CrPO), a novel alignment method that injects signals from multiple creativity dimensions into the preference optimization objective in a modular fashion. We train and evaluate creativity-augmented versions of several models using CrPO and MuCE, a new large-scale human preference dataset spanning over 200,000 human-generated responses and ratings from more than 30 psychological creativity assessments. Our models outperform strong baselines, including GPT-4o, on both automated and human evaluations, producing more novel, diverse, and surprising generations while maintaining high output quality. Additional evaluations on NoveltyBench further confirm the generalizability of our approach. Together, our results demonstrate that directly optimizing for creativity within preference frameworks is a promising direction for advancing the creative capabilities of LLMs without compromising output quality.", 'abstract_zh': '虽然大型语言模型在自然语言生成任务中展现了令人印象深刻的性能，但它们生成真正具有创造力的内容（表现为新颖性、多样性、惊喜感和高质量）的能力仍然有限。现有增强大型语言模型创造力的方法往往集中于单一维度或特定任务，未能以可推广的方式全面解决创造力的多方面特性。在本项工作中，我们提出了一种名为Creative Preference Optimization (CrPO) 的新颖对齐方法，以模块化方式将多个创造力维度的信号注入到偏好优化目标中。我们使用CrPO和一个新的大规模人类偏好数据集MuCE（涵盖超过20万个人类生成的响应和来自30多种心理创造力评估的评级）训练和评估了增强创造力的多个模型。我们的模型在自动评估和人类评估中均优于强大的基准模型（包括GPT-4o），产生了更多新颖、多样且充满惊喜的生成内容，同时保持了高质量的输出。进一步在NoveltyBench上的评估也证实了我们方法的可推广性。综上所述，我们的结果表明，在偏好框架中直接优化创造力是提升大型语言模型创造力能力的一个有前景的方向，而不会牺牲输出质量。', 'title_zh': '创意偏好优化'}
{'arxiv_id': 'arXiv:2505.14436', 'title': 'Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models', 'authors': 'Yuqiao Tan, Shizhu He, Kang Liu, Jun Zhao', 'link': 'https://arxiv.org/abs/2505.14436', 'abstract': 'Large Language Models (LLMs) offer a transparent brain with accessible parameters that encode extensive knowledge, which can be analyzed, located and transferred. Consequently, a key research challenge is to transcend traditional knowledge transfer paradigms rooted in symbolic language and achieve genuine Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods for transferring knowledge across LLMs of different scales through parameters presents an intriguing and valuable research direction. In this paper, we first demonstrate $\\textbf{Alignment}$ in parametric space is the fundamental prerequisite to achieve successful cross-scale PKT. We redefine the previously explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes extracted parameters for LoRA initialization and requires subsequent fine-tune for alignment. Hence, to reduce cost for further fine-tuning, we introduce a novel Pre-Align PKT (PrePKT) paradigm and propose a solution called $\\textbf{LaTen}$ ($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that aligns the parametric spaces of LLMs across scales only using several training steps without following training. Comprehensive experiments on four benchmarks demonstrate that both PostPKT and PrePKT face challenges in achieving consistently stable transfer. Through in-depth analysis, we identify $\\textbf{Neural Incompatibility}$ as the ethological and parametric structural differences between LLMs of varying scales, presenting fundamental challenges to achieving effective PKT. These findings provide fresh insights into the parametric architectures of LLMs and highlight promising directions for future research on efficient PKT. Our code is available at this https URL.', 'abstract_zh': '大规模语言模型（LLMs）提供了一个透明的“大脑”，其中可访问的参数编码了丰富的知识，可以被分析、定位和转移。因此，一个关键的研究挑战是如何超越传统的基于符号语言的知识转移范式，实现真正的参数知识转移（PKT）。值得一提的是，探索有效的方法以参数形式在不同规模的LLMs之间进行知识转移是一个极具吸引力和价值的研究方向。在本文中，我们首先证明参数空间的对齐是实现成功的大规模知识转移的基本前提。我们将以往探索的知识转移重新定义为后对齐知识转移（PostPKT），它利用提取的参数进行LoRA初始化，并需要后续的微调以实现对齐。为了减少后续微调的成本，我们引入了一个新的前对齐知识转移（PrePKT）范式，并提出了一种名为LaTen（Locate-Then-Align）的解决方案，该方法仅通过几轮训练步骤对不同规模的大规模语言模型的参数空间进行对齐，而不进行后续训练。在四个基准上的全面实验表明，无论是PostPKT还是PrePKT都面临着一致稳定地转移的挑战。通过深入分析，我们确定了神经不兼容性为不同规模的大规模语言模型之间在神经和参数结构上的差异，这些差异构成了有效PKT的基本挑战。这些发现为大规模语言模型的参数架构提供了新的见解，并突显了未来高效PKT研究的有希望的方向。我们的代码可在以下链接访问：this https URL。', 'title_zh': '神经不兼容性：大规模语言模型中跨尺度参数知识转移的不可逾越差距'}
{'arxiv_id': 'arXiv:2505.14435', 'title': 'Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI', 'authors': 'Annika Bush, Meltem Aksoy, Markus Pauly, Greta Ontrup', 'link': 'https://arxiv.org/abs/2505.14435', 'abstract': 'As organizations increasingly rely on AI systems for decision support in sustainability contexts, it becomes critical to understand the inherent biases and perspectives embedded in Large Language Models (LLMs). This study systematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek, GPT, LLaMA, and Mistral - conceptualize sustainability and its relationship with AI. We administered validated, psychometric sustainability-related questionnaires - each 100 times per model -- to capture response patterns and variability. Our findings revealed significant inter-model differences: For example, GPT exhibited skepticism about the compatibility of AI and sustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect scores for several Sustainable Development Goals (SDGs). Models also diverged in attributing institutional responsibility for AI and sustainability integration, a results that holds implications for technology governance approaches. Our results demonstrate that model selection could substantially influence organizational sustainability strategies, highlighting the need for awareness of model-specific biases when deploying LLMs for sustainability-related decision-making.', 'abstract_zh': '随着组织越来越多地依赖AI系统在可持续发展领域提供决策支持，理解大型语言模型（LLMs）中固有的偏见和视角变得至关重要。本研究系统性地探讨了五种最先进的LLMs——Claude、DeepSeek、GPT、LLaMA和Mistral——如何概念化可持续性及其与AI的关系。我们对每种模型进行了100次可持续性相关的验证psychometric问卷测试，以捕捉响应模式和变异。研究发现各模型之间存在显著差异：例如，GPT对AI与可持续性的兼容性表示怀疑，而LLaMA则表现出极端的技术乐观主义，并在多项可持续发展目标（SDGs）上获得了满分。模型在归因于机构在AI与可持续性整合中的责任方面也存在差异，这一结果对技术治理方法具有重要意义。研究结果表明，模型选择可能会显著影响组织的可持续性策略，强调在使用LLMs进行与可持续性相关的决策时需要意识到模型特定的偏见。', 'title_zh': '选择模型，塑造未来：比较大型语言模型对可持续性的看法及其与AI的关系'}
{'arxiv_id': 'arXiv:2505.14398', 'title': 'Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation', 'authors': 'Peter Baile Chen, Yi Zhang, Dan Roth, Samuel Madden, Jacob Andreas, Michael Cafarella', 'link': 'https://arxiv.org/abs/2505.14398', 'abstract': "While humans naturally learn and adapt from past experiences, large language models (LLMs) and their agentic counterparts struggle to retain reasoning from previous tasks and apply them in future contexts. To address this limitation, we propose a novel framework, log-augmented generation (LAG) that directly reuses prior computation and reasoning from past logs at test time to enhance model's ability to learn from previous tasks and perform better on new, unseen challenges, all while keeping the system efficient and scalable. Specifically, our system represents task logs using key-value (KV) caches, encoding the full reasoning context of prior tasks while storing KV caches for only a selected subset of tokens. When a new task arises, LAG retrieves the KV values from relevant logs to augment generation. Our approach differs from reflection-based memory mechanisms by directly reusing prior reasoning and computations without requiring additional steps for knowledge extraction or distillation. Our method also goes beyond existing KV caching techniques, which primarily target efficiency gains rather than improving accuracy. Experiments on knowledge- and reasoning-intensive datasets demonstrate that our method significantly outperforms standard agentic systems that do not utilize logs, as well as existing solutions based on reflection and KV cache techniques.", 'abstract_zh': '虽然人类自然地从过往经验中学习和适应，但大型语言模型（LLMs）及其代理版本在保留先前任务的推理并将其应用于未来情境方面存在困难。为了解决这一局限性，我们提出了一个新颖的框架——日志增强生成（LAG），该框架在测试时直接重用过去的日志中的先前计算和推理，以增强模型从先前任务中学习的能力并在新的、未见过的挑战中表现得更好，同时保持系统的高效和可扩展性。具体而言，我们的系统使用键值（KV）缓存来表示任务日志，编码先前任务的完整推理背景，并仅存储选定token的KV缓存。当新任务出现时，LAG从相关日志中检索KV值以增强生成。与基于反思的记忆机制不同，我们的方法直接重用先前的推理和计算，而不需要额外的知识提取或蒸馏步骤。此外，我们的方法超越了现有的KV缓存技术，后者主要侧重于效率提升而不是提高准确性。在知识和推理密集型数据集上的实验表明，我们的方法显著优于不使用日志的标准代理系统以及基于反思和KV缓存技术的现有解决方案。', 'title_zh': '日志增强生成：通过可复用计算扩展测试时推理'}
{'arxiv_id': 'arXiv:2505.14395', 'title': 'MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language', 'authors': 'Seyoung Song, Seogyeong Jeong, Eunsu Kim, Jiho Jin, Dongkwan Kim, Jay Shin, Alice Oh', 'link': 'https://arxiv.org/abs/2505.14395', 'abstract': "Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy of successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks ($r$ > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages.", 'abstract_zh': '评估大型语言模型的文本生成能力具有挑战性，特别是在资源稀缺的语言中，直接评估方法稀缺。我们提出了MUG-Eval，这是一种新型框架，通过将现有基准转换为对话任务，并衡量模型在这些任务上的准确率来评估大型语言模型的多语言生成能力。我们特别设计了这些对话任务，要求在目标语言中实现有效的沟通。然后，我们简单地将任务成功率作为成功对话生成的代理。该方法具有两个关键优势：它不依赖于特定语言的NLP工具或注释数据集，这些工具和数据集对于大多数语言而言是有限的；此外，它也不依赖于将大型语言模型作为评估者，这些模型的评估质量在资源丰富语言以外会下降。我们对30种不同资源类别的8种大型语言模型进行了评估，并发现MUG-Eval与现有基准具有强烈的相关性（相关系数>r>0.75），同时为不同语言和模型之间提供了标准化的比较。我们的框架提供了一个稳健且资源高效的多语言生成评估解决方案，可以扩展到数千种语言。', 'title_zh': 'MUG-Eval：任意语言多语种生成能力的代理评价框架'}
{'arxiv_id': 'arXiv:2505.14316', 'title': 'Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion', 'authors': 'Tiehan Cui, Yanxu Mao, Peipei Liu, Congying Liu, Datao You', 'link': 'https://arxiv.org/abs/2505.14316', 'abstract': 'Although large language models (LLMs) have achieved remarkable advancements, their security remains a pressing concern. One major threat is jailbreak attacks, where adversarial prompts bypass model safeguards to generate harmful or objectionable content. Researchers study jailbreak attacks to understand security and robustness of LLMs. However, existing jailbreak attack methods face two main challenges: (1) an excessive number of iterative queries, and (2) poor generalization across models. In addition, recent jailbreak evaluation datasets focus primarily on question-answering scenarios, lacking attention to text generation tasks that require accurate regeneration of toxic content. To tackle these challenges, we propose two contributions: (1) ICE, a novel black-box jailbreak method that employs Intent Concealment and divErsion to effectively circumvent security constraints. ICE achieves high attack success rates (ASR) with a single query, significantly improving efficiency and transferability across different models. (2) BiSceneEval, a comprehensive dataset designed for assessing LLM robustness in question-answering and text-generation tasks. Experimental results demonstrate that ICE outperforms existing jailbreak techniques, revealing critical vulnerabilities in current defense mechanisms. Our findings underscore the necessity of a hybrid security strategy that integrates predefined security mechanisms with real-time semantic decomposition to enhance the security of LLMs.', 'abstract_zh': '虽然大型语言模型（LLMs）取得了显著进展，但其安全性仍是一个亟待解决的问题。主要威胁之一是escaping攻击，这种攻击通过对抗性提示绕过模型的安全保护机制生成有害或令人反感的内容。研究人员研究escaping攻击以了解LLM的安全性和稳健性。然而，现有的escaping攻击方法面临两个主要挑战：（1）需要大量的迭代查询，（2）泛化能力较差。此外，最近的escaping评估数据集主要关注于问答场景，而忽略了需要准确再生有毒内容的文本生成任务。为应对这些挑战，我们提出了两项贡献：（1）ICE，一种新颖的黑盒escaping方法，通过意图隐匿和转移有效规避安全约束。ICE仅通过一次查询就能实现高攻击成功率（ASR），显著提高了效率和跨不同模型的迁移性。（2）BiSceneEval，一种为评估LLM在问答和文本生成任务中的稳健性而设计的综合数据集。实验结果表明，ICE优于现有escaping技术，揭示了当前防御机制中的关键漏洞。我们的发现强调了需要结合预定义的安全机制与实时语义分解的混合安全策略，以提高LLM的安全性。', 'title_zh': '通过意图隐藏和转移探索对大语言模型的 Jailbreak 攻击'}
{'arxiv_id': 'arXiv:2505.14279', 'title': 'YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering', 'authors': "Jennifer D'Souza, Hamed Babaei Giglou, Quentin Münch", 'link': 'https://arxiv.org/abs/2505.14279', 'abstract': 'Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry and artificial general intelligence.', 'abstract_zh': '大规模语言模型（LLMs）在现代搜索引擎中的科学问题解答中发挥着关键作用，但其评估稳健性仍鲜有探讨。我们提出YESciEval，一个结合了细粒度 rubric 评估与强化学习的开源框架，旨在减轻大规模语言模型评估者中的乐观偏见。我们发布了跨学科的科学问答数据集，包括对抗变体，并提供了multiple LLM的评估分数。独立于专有模型和人类反馈，我们的方法能够实现可扩展且无需成本的评估。通过推进可靠的“LLM作为法官”模型，这项工作支持了AI对齐，并促进了对于科学研究和通用人工智能而言至关重要的稳健且透明的评估。', 'title_zh': 'YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering'}
{'arxiv_id': 'arXiv:2505.14268', 'title': 'Think-J: Learning to Think for Generative LLM-as-a-Judge', 'authors': 'Hui Huang, Yancheng He, Hongli Zhou, Rui Zhang, Wei Liu, Weixun Wang, Wenbo Su, Bo Zheng, Jiaheng Liu', 'link': 'https://arxiv.org/abs/2505.14268', 'abstract': 'LLM-as-a-Judge refers to the automatic modeling of preferences for responses generated by Large Language Models (LLMs), which is of significant importance for both LLM evaluation and reward modeling. Although generative LLMs have made substantial progress in various tasks, their performance as LLM-Judge still falls short of expectations. In this work, we propose Think-J, which improves generative LLM-as-a-Judge by learning how to think. We first utilized a small amount of curated data to develop the model with initial judgment thinking capabilities. Subsequently, we optimize the judgment thinking traces based on reinforcement learning (RL). We propose two methods for judgment thinking optimization, based on offline and online RL, respectively. The offline RL requires training a critic model to construct positive and negative examples for learning. The online method defines rule-based reward as feedback for optimization. Experimental results showed that our approach can significantly enhance the evaluation capability of generative LLM-Judge, surpassing both generative and classifier-based LLM-Judge without requiring extra human annotations.', 'abstract_zh': 'LLM-as-a-Judge作为一种自动建模大型语言模型生成响应偏好的方法，在大型语言模型评估和奖励建模中具有重要意义。尽管生成型大型语言模型在各种任务中取得了显著进展，但其作为LLM-Judge的表现仍未能达到预期。本文提出Think-J，通过学习如何思考来提高生成型大型语言模型的评估能力。我们首先利用少量精加工的数据开发模型，赋予其初步的判断思考能力。随后，基于强化学习（RL）优化判断思考轨迹。我们提出了两种基于离线和在线RL的判断思考优化方法。离线RL需要训练一个批判模型以构建正反例进行学习；在线方法则通过基于规则的奖励作为反馈进行优化。实验结果表明，我们的方法能够显著增强生成型大型语言模型的评估能力，且无需额外的人工标注，超越了基于生成和分类的大型语言模型评估方法。', 'title_zh': 'Think-J: 学习为生成型LLM进行判断性思考'}
{'arxiv_id': 'arXiv:2505.14260', 'title': 'Speculative Decoding Reimagined for Multimodal Large Language Models', 'authors': 'Luxi Lin, Zhihang Lin, Zhanpeng Zeng, Rongrong Ji', 'link': 'https://arxiv.org/abs/2505.14260', 'abstract': 'This paper introduces Multimodal Speculative Decoding (MSD) to accelerate Multimodal Large Language Models (MLLMs) inference. Speculative decoding has been shown to accelerate Large Language Models (LLMs) without sacrificing accuracy. However, current speculative decoding methods for MLLMs fail to achieve the same speedup as they do for LLMs. To address this, we reimagine speculative decoding specifically for MLLMs. Our analysis of MLLM characteristics reveals two key design principles for MSD: (1) Text and visual tokens have fundamentally different characteristics and need to be processed separately during drafting. (2) Both language modeling ability and visual perception capability are crucial for the draft model. For the first principle, MSD decouples text and visual tokens in the draft model, allowing each to be handled based on its own characteristics. For the second principle, MSD uses a two-stage training strategy: In stage one, the draft model is trained on text-only instruction-tuning datasets to improve its language modeling ability. In stage two, MSD gradually introduces multimodal data to enhance the visual perception capability of the draft model. Experiments show that MSD boosts inference speed by up to $2.29\\times$ for LLaVA-1.5-7B and up to $2.46\\times$ for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness. Our code is available at this https URL.', 'abstract_zh': '这篇论文介绍了多模态投机解码（MSD）以加速多模态大型语言模型（MLLMs）的推理。', 'title_zh': '重想象的 speculative decoding 用于多模态大规模语言模型'}
{'arxiv_id': 'arXiv:2505.14256', 'title': 'FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation', 'authors': 'Shaolin Zhu, Tianyu Dong, Bo Li, Deyi Xiong', 'link': 'https://arxiv.org/abs/2505.14256', 'abstract': 'In this paper, we present FuxiMT, a novel Chinese-centric multilingual machine translation model powered by a sparsified large language model (LLM). We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on a massive Chinese corpus and then conduct multilingual fine-tuning on a large parallel dataset encompassing 65 languages. FuxiMT incorporates Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust performance across various resource levels. Experimental results demonstrate that FuxiMT significantly outperforms strong baselines, including state-of-the-art LLMs and machine translation models, particularly under low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot translation capabilities for unseen language pairs, indicating its potential to bridge communication gaps where parallel data are scarce or unavailable.', 'abstract_zh': '基于稀疏大型语言模型的面向中文的多语言机器翻译模型FuxiMT', 'title_zh': 'FuxiMT：为中国中心的多语言机器翻译精简大型语言模型'}
{'arxiv_id': 'arXiv:2505.14238', 'title': 'ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models', 'authors': 'Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Praneeth Vepakomma', 'link': 'https://arxiv.org/abs/2505.14238', 'abstract': "Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trained weights fixed. The prevailing approach, LoRA, models updates using a low-rank decomposition, but its expressivity is inherently constrained by the rank. Recent methods like HiRA aim to increase expressivity by incorporating a Hadamard product with the frozen weights, but still rely on the structure of the pre-trained model. We introduce ABBA, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget. We formally analyze ABBA's expressive capacity and validate its advantages through matrix reconstruction experiments. Empirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models. Our code is publicly available at: this https URL.", 'abstract_zh': '大规模语言模型在广泛的任务中展现了强大的性能，但高效适应新领域仍然是一个关键挑战。参数高效微调（PEFT）方法通过引入轻量级、可训练模块，同时固定大部分预训练权重来应对这一挑战。目前占主导地位的方法LoRA使用低秩分解来建模更新，但其表达能力受到秩的内在约束。最近的方法HiRA试图通过引入与冻结权重的Hadamard积来增加表达能力，但仍依赖预训练模型的结构。我们引入了ABBA，这是一种新的PEFT架构，将更新重新参数化为两个独立可学习低秩矩阵的Hadamard积。与先前工作不同，ABBA完全解耦了更新与预训练权重的关系，使得两个组件可以自由优化。这在相同的参数预算下实现了更高的表达能力。我们形式化分析了ABBA的表达能力，并通过矩阵重构实验验证了其优势。实验中，ABBA在算术和常识推理基准测试中取得了最优结果，在多个模型上显著优于现有PEFT方法。我们的代码已公开：this https URL。', 'title_zh': 'ABBA：大型语言模型中高度表达性的哈达玛积适应方法'}
{'arxiv_id': 'arXiv:2505.14233', 'title': 'Mechanistic Fine-tuning for In-context Learning', 'authors': 'Hakaze Cho, Peng Luo, Mariko Kato, Rin Kaenbyou, Naoya Inoue', 'link': 'https://arxiv.org/abs/2505.14233', 'abstract': 'In-context Learning (ICL) utilizes structured demonstration-query inputs to induce few-shot learning on Language Models (LMs), which are not originally pre-trained on ICL-style data. To bridge the gap between ICL and pre-training, some approaches fine-tune LMs on large ICL-style datasets by an end-to-end paradigm with massive computational costs. To reduce such costs, in this paper, we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous findings on the inner mechanism of ICL, building training objectives on the attention scores instead of the final outputs, to force the attention scores to focus on the correct label tokens presented in the context and mitigate attention scores from the wrong label tokens. Our experiments on 9 modern LMs and 8 datasets empirically find that ABFT outperforms in performance, robustness, unbiasedness, and efficiency, with only around 0.01% data cost compared to the previous methods. Moreover, our subsequent analysis finds that the end-to-end training objective contains the ABFT objective, suggesting the implicit bias of ICL-style data to the emergence of induction heads. Our work demonstrates the possibility of controlling specific module sequences within LMs to improve their behavior, opening up the future application of mechanistic interpretability.', 'abstract_zh': '基于上下文学习的注意力行为微调（ABFT）：一种低数据成本的 few-shot 学习方法', 'title_zh': '机制微调以实现上下文学习'}
{'arxiv_id': 'arXiv:2505.14226', 'title': '"Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs', 'authors': 'Darpan Aswal, Siddharth D Jaiswal', 'link': 'https://arxiv.org/abs/2505.14226', 'abstract': 'Large Language Models (LLMs) have become increasingly powerful, with multilingual and multimodal capabilities improving by the day. These models are being evaluated through audits, alignment studies and red-teaming efforts to expose model vulnerabilities towards generating harmful, biased and unfair content. Existing red-teaming efforts have previously focused on the English language, using fixed template-based attacks; thus, models continue to be susceptible to multilingual jailbreaking strategies, especially in the multimodal context. In this study, we introduce a novel strategy that leverages code-mixing and phonetic perturbations to jailbreak LLMs for both text and image generation tasks. We also introduce two new jailbreak strategies that show higher effectiveness than baseline strategies. Our work presents a method to effectively bypass safety filters in LLMs while maintaining interpretability by applying phonetic misspellings to sensitive words in code-mixed prompts. Our novel prompts achieve a 99% Attack Success Rate for text generation and 78% for image generation, with Attack Relevance Rate of 100% for text generation and 95% for image generation when using the phonetically perturbed code-mixed prompts. Our interpretability experiments reveal that phonetic perturbations impact word tokenization, leading to jailbreak success. Our study motivates increasing the focus towards more generalizable safety alignment for multilingual multimodal models, especially in real-world settings wherein prompts can have misspelt words.', 'abstract_zh': '大型语言模型（LLMs）的能力不断增强，具备多语言和多模态能力，正在通过审核、对齐研究和红队攻击等手段进行评估，以暴露模型生成有害、偏见和不公平内容的漏洞。现有的红队攻击主要针对英语，使用固定模板攻击；因此，模型仍然容易受到多语言逃逸策略的影响，尤其是在多模态情境下。本研究提出了一种新的策略，利用混合编程和音素扰动技术，实现对文本和图像生成任务中LLMs的逃逸。我们还引入了两种新的逃逸策略，其有效性高于基线策略。我们的研究提出了一种方法，在不影响解释性的情况下，通过在混合编程提示中对敏感词汇应用音素误写，有效地绕过LLMs中的安全过滤器。我们的新型提示在文本生成上的攻击成功率达到了99%，图像生成上的攻击成功率达到了78%，使用音素扰动的混合编程提示时，文本生成上的攻击相关率为100%，图像生成上的攻击相关率为95%。我们的解释性实验表明，音素扰动影响了词元化，导致逃逸成功。本研究促使我们关注更具普适性的多语言多模态模型的安全对齐，尤其是在实际应用场景中，提示可能包含拼写错误的情况下。', 'title_zh': '“哈et语言和歧视”：代码混合 hinGlish中的音素干扰以红队LLMs'}
{'arxiv_id': 'arXiv:2505.14212', 'title': 'Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks', 'authors': 'Sizhe Yuen, Ting Su, Ziyang Wang, Yali Du, Adam J. Sobey', 'link': 'https://arxiv.org/abs/2505.14212', 'abstract': 'A question-answering (QA) system is to search suitable answers within a knowledge base. Current QA systems struggle with queries requiring complex reasoning or real-time knowledge integration. They are often supplemented with retrieval techniques on a data source such as Retrieval-Augmented Generation (RAG). However, RAG continues to face challenges in handling complex reasoning and logical connections between multiple sources of information. A novel approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA tasks is presented through the automated generation of context-based QA pairs. This methodology leverages LLMs to create fine-tuning data, reducing reliance on human labelling and improving model comprehension and reasoning capabilities. The proposed system includes an automated QA generator and a model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore. Comprehensive experiments demonstrate improvements in logical coherence and factual accuracy, with implications for developing adaptable Artificial Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1, BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA pairs.', 'abstract_zh': '一种基于上下文的问答对自动生成方法可以增强大型语言模型在知识密集型问答任务中的性能。该方法利用大型语言模型创建微调数据，减少对人工标注的依赖，提高模型的理解和推理能力。所提出系统包括一个自动问答生成器和一个模型微调器，并通过困惑度、ROUGE、BLEU和BERTScore进行评估。全面的实验展示了逻辑连贯性和事实准确性的改进，对开发 adaptable 人工智能系统具有重要意义。Mistral-7b-v0.3在BERT F1、BLEU和ROUGE分数上优于Llama-3-8b，LLM生成的问答对的分数分别为0.858、0.172和0.260，而人工标注的问答对的分数分别为0.836、0.083和0.139。', 'title_zh': '知识密集型问答任务的自动数据集生成'}
{'arxiv_id': 'arXiv:2505.14185', 'title': 'Safety Subspaces are Not Distinct: A Fine-Tuning Case Study', 'authors': 'Kaustubh Ponkshe, Shaan Shah, Raghav Singhal, Praneeth Vepakomma', 'link': 'https://arxiv.org/abs/2505.14185', 'abstract': "Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. This is typically achieved through instruction tuning and reinforcement learning from human feedback. However, this alignment is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable geometric directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this geometric perspective. We examine whether safety-relevant behavior is concentrated in specific subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in internal representations. Across both parameter and activation space, our findings are consistent: subspaces that amplify safe behaviors also amplify unsafe ones, and prompts with different safety implications activate overlapping representations. We find no evidence of a subspace that selectively governs safety. These results challenge the assumption that alignment is geometrically localized. Rather than residing in distinct directions, safety appears to emerge from entangled, high-impact components of the model's broader learning dynamics. This suggests that subspace-based defenses may face fundamental limitations and underscores the need for alternative strategies to preserve alignment under continued training. We corroborate these findings through multiple experiments on five open-source LLMs. Our code is publicly available at: this https URL.", 'abstract_zh': '大型语言模型（LLMs）依赖于安全性对齐以生成社会可接受的响应。这通常通过指令微调和从人类反馈中进行强化学习来实现。然而，这种对齐已知是脆弱的：即使是微调无害或轻微污染的数据，也可能损害安全性并重新引入有害行为。越来越多的研究表明，对齐可能对应于权重空间中的可识别的几何方向，形成子空间，这些子空间原则上可以被隔离或保留以防止对齐失效。在本工作中，我们进行了全面的实证研究，以探讨这一几何视角。我们研究了安全相关的行为是否集中在特定的子空间中，这些行为是否可以与通用学习分开，以及有害性是否源于内部表示中的可区分模式。在参数空间和激活空间中，我们的发现是一致的：放大安全行为的子空间也放大了不安全的行为，不同安全含义的提示激活了重叠的表示。我们没有发现一个单独治理安全性的子空间。这些结果挑战了对齐是几何局部化的一种假设。安全性似乎不是源自独特的方向，而是源自模型更广泛学习动态中交织的关键组件。这表明基于子空间的防御可能面临根本性局限，并突显了在持续训练中保持对齐的需要。我们通过在五个开源LLM上进行多项实验来验证这些发现。我们的代码可在以下网址公开获取：this https URL。', 'title_zh': '安全子空间并非孤立：一项微调案例研究'}
{'arxiv_id': 'arXiv:2505.14178', 'title': 'Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits', 'authors': 'Xiang Zhang, Juntai Cao, Jiaqi Wei, Yiwei Xu, Chenyu You', 'link': 'https://arxiv.org/abs/2505.14178', 'abstract': 'Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.', 'abstract_zh': '令牌化是语言模型中第一层且常常被忽视的计算层。尽管链式思维（CoT）提示能够通过外部化中间步骤来逼近循环计算，但我们表明，这种推理的成功从根本上受限于标记化输入的结构。本工作通过理论和实证研究探讨了令牌化方案，尤其是基于子词的方法（如BPE），如何通过合并或掩盖原子推理单元来阻碍符号计算。我们引入了“令牌意识”这一概念，以正式化低劣的令牌粒度如何破坏逻辑对齐并阻止模型从符号程序中泛化。通过在算术和符号任务上的系统评估，我们证明令牌结构对推理性能有重大影响，即使在使用CoT的情况下也会导致推理失败，而原子对齐的格式则能促进强大的泛化能力，使得小型模型（如GPT-4o-mini）能够在结构化推理中优于大型系统（如o1）。我们的发现揭示了LLMs的符号推理能力不仅取决于架构，还深深依赖于令牌级表示。', 'title_zh': 'LLMs中-Token化约束：符号与算术推理限制研究'}
{'arxiv_id': 'arXiv:2505.14157', 'title': 'Prior Prompt Engineering for Reinforcement Fine-Tuning', 'authors': 'Pittawat Taveekitworachai, Potsawee Manakul, Sarana Nutanong, Kunat Pipatanakul', 'link': 'https://arxiv.org/abs/2505.14157', 'abstract': 'This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT.', 'abstract_zh': '本研究探讨了强化微调（RFT）背景下先验提示工程（pPE）的作用，其中语言模型（LMs）通过奖励信号被激励表现出最大化性能的行为。尽管现有的RFT研究主要集中在算法、奖励塑造和数据整理上，但在训练过程中预附的先验提示（prior prompt）的设计——该提示在查询期间被用来引发逐步推理等行为——仍处于探索阶段。我们研究了不同的pPE方法是否能在RFT后引导LMs内化不同的行为。受推理时提示工程（iPE）的启发，我们将五种代表性的iPE策略（推理、规划、基于代码的推理、知识回忆和空白示例利用）翻译成相应的pPE方法。我们使用Qwen2.5-7B进行了每种pPE方法的实验，然后在领域内和领域外基准测试（例如AIME2024、HumanEval+和GPQA-Diamond）上评估性能。结果显示，所有pPE训练的模型都超过了其对应的iPE提示模型，其中空白示例pPE方法在平均性能提升方面最大，特别是在AIME2024和GPQA-Diamond上的改进最大，超过了常用的推理方法。此外，通过适应行为分类框架，我们证明了不同的pPE策略在生成的模型中灌输了不同的行为风格。这些发现将pPE定位为RFT中一个强大但未充分研究的维度。', 'title_zh': '预先精心设计的提示工程用于强化学习微调'}
{'arxiv_id': 'arXiv:2505.14156', 'title': 'Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search', 'authors': 'Songhao Wu, Quan Tu, Hong Liu, Jia Xu, Zhongyi Liu, Guannan Zhang, Ran Wang, Xiuying Chen, Rui Yan', 'link': 'https://arxiv.org/abs/2505.14156', 'abstract': "Session search involves a series of interactive queries and actions to fulfill user's complex information need. Current strategies typically prioritize sequential modeling for deep semantic understanding, overlooking the graph structure in interactions. While some approaches focus on capturing structural information, they use a generalized representation for documents, neglecting the word-level semantic modeling. In this paper, we propose Symbolic Graph Ranker (SGR), which aims to take advantage of both text-based and graph-based approaches by leveraging the power of recent Large Language Models (LLMs). Concretely, we first introduce a set of symbolic grammar rules to convert session graph into text. This allows integrating session history, interaction process, and task instruction seamlessly as inputs for the LLM. Moreover, given the natural discrepancy between LLMs pre-trained on textual corpora, and the symbolic language we produce using our graph-to-text grammar, our objective is to enhance LLMs' ability to capture graph structures within a textual format. To achieve this, we introduce a set of self-supervised symbolic learning tasks including link prediction, node content generation, and generative contrastive learning, to enable LLMs to capture the topological information from coarse-grained to fine-grained. Experiment results and comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm the superiority of our approach. Our paradigm also offers a novel and effective methodology that bridges the gap between traditional search strategies and modern LLMs.", 'abstract_zh': '基于符号图排名的会话搜索方法', 'title_zh': '统一图学习与文本：释放大规模语言模型在会话搜索中的潜力'}
{'arxiv_id': 'arXiv:2505.14107', 'title': 'DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models', 'authors': 'Yakun Zhu, Zhongzhen Huang, Linjie Mu, Yutong Huang, Wei Nie, Shaoting Zhang, Pengfei Liu, Xiaofan Zhang', 'link': 'https://arxiv.org/abs/2505.14107', 'abstract': 'The emergence of groundbreaking large language models capable of performing complex reasoning tasks holds significant promise for addressing various scientific challenges, including those arising in complex clinical scenarios. To enable their safe and effective deployment in real-world healthcare settings, it is urgently necessary to benchmark the diagnostic capabilities of current models systematically. Given the limitations of existing medical benchmarks in evaluating advanced diagnostic reasoning, we present DiagnosisArena, a comprehensive and challenging benchmark designed to rigorously assess professional-level diagnostic competence. DiagnosisArena consists of 1,113 pairs of segmented patient cases and corresponding diagnoses, spanning 28 medical specialties, deriving from clinical case reports published in 10 top-tier medical journals. The benchmark is developed through a meticulous construction pipeline, involving multiple rounds of screening and review by both AI systems and human experts, with thorough checks conducted to prevent data leakage. Our study reveals that even the most advanced reasoning models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79% accuracy, respectively. This finding highlights a significant generalization bottleneck in current large language models when faced with clinical diagnostic reasoning challenges. Through DiagnosisArena, we aim to drive further advancements in AIs diagnostic reasoning capabilities, enabling more effective solutions for real-world clinical diagnostic challenges. We provide the benchmark and evaluation tools for further research and development this https URL.', 'abstract_zh': '大型语言模型在复杂推理任务中的涌现为解决各种科学挑战，包括临床复杂场景中的挑战，带来了重要前景。为确保其在真实世界医疗环境中的安全有效部署，迫切需要系统地 benchmarks 当前模型的诊断能力。鉴于现有医疗基准在评估高级诊断推理方面的局限性，我们提出了 DiagnosisArena，一个综合且具有挑战性的基准，旨在严格评估专业级诊断能力。DiagnosisArena 包含 1,113 个分段患者病例及其对应的诊断，涵盖 28 个医学专科，数据源自 10 本顶级医学期刊发表的临床案例报告。该基准通过细致的构建管道开发，涉及多轮由 AI 系统和human专家筛查和审查，并进行了严格的检查以防止数据泄露。我们的研究表明，即使是最先进的推理模型 o3-mini、o1 和 DeepSeek-R1，准确率分别仅为 45.82%、31.09% 和 17.79%。这一发现突显了当前大型语言模型在面对临床诊断推理挑战时的重大泛化瓶颈。通过 DiagnosisArena，我们旨在推动 AI 诊断推理能力的进一步发展，为解决真实世界临床诊断挑战提供更有效的解决方案。我们提供了该基准和评估工具，供进一步研究和开发使用：此链接。', 'title_zh': 'DiagnosisArena: 大规模语言模型诊断推理benchmarkBenchmarking诊断推理 for 大规模语言模型'}
{'arxiv_id': 'arXiv:2505.14106', 'title': 'A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations', 'authors': 'Li Li, Peilin Cai, Ryan A. Rossi, Franck Dernoncourt, Branislav Kveton, Junda Wu, Tong Yu, Linxin Song, Tiankai Yang, Yuehan Qin, Nesreen K. Ahmed, Samyadeep Basu, Subhojyoti Mukherjee, Ruiyi Zhang, Zhengmian Hu, Bo Ni, Yuxiao Zhou, Zichao Wang, Yue Huang, Yu Wang, Xiangliang Zhang, Philip S. Yu, Xiyang Hu, Yue Zhao', 'link': 'https://arxiv.org/abs/2505.14106', 'abstract': 'We present PersonaConvBench, a large-scale benchmark for evaluating personalized reasoning and generation in multi-turn conversations with large language models (LLMs). Unlike existing work that focuses on either personalization or conversational structure in isolation, PersonaConvBench integrates both, offering three core tasks: sentence classification, impact regression, and user-centric text generation across ten diverse Reddit-based domains. This design enables systematic analysis of how personalized conversational context shapes LLM outputs in realistic multi-user scenarios. We benchmark several commercial and open-source LLMs under a unified prompting setup and observe that incorporating personalized history yields substantial performance improvements, including a 198 percent relative gain over the best non-conversational baseline in sentiment classification. By releasing PersonaConvBench with evaluations and code, we aim to support research on LLMs that adapt to individual styles, track long-term context, and produce contextually rich, engaging responses.', 'abstract_zh': 'PersonaConvBench: 一种大规模基准，用于评估大型语言模型在多轮对话中个性化推理与生成能力', 'title_zh': '个性化对话基准：向着模拟个性化对话的方向'}
{'arxiv_id': 'arXiv:2505.14103', 'title': 'AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models', 'authors': 'Guangke Chen, Fu Song, Zhe Zhao, Xiaojun Jia, Yang Liu, Yanchen Qiao, Weizhe Zhang', 'link': 'https://arxiv.org/abs/2505.14103', 'abstract': 'Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website this https URL.', 'abstract_zh': '对大型音频语言模型（LALMs）的监狱突破攻击研究近期有所进展，但这些攻击的效果、适用性和实用性都存在局限性，尤其是在假设攻击者能够完全操控用户提示的情况下。在本文中，我们首先进行了一项全面的实验，表明先进的文本监狱突破攻击无法轻易通过文本到语音（TTS）技术移植到端到端的LALMs中。然后，我们提出了AudioJailbreak，这是一种新型的音频监狱突破攻击，具有以下特点：（1）异步性：监狱突破音频无需在时间轴上与用户提示对齐，可以通过构造后续的监狱突破音频实现；（2）普适性：通过将多个提示融入到扰动生成中，单个监狱突破扰动能够适用于不同的提示；（3）隐蔽性：通过提出各种意图隐匿策略，使得监狱突破音频的恶意意图不会引起受害者的注意；（4）空中鲁棒性：通过将回声畸变效应与房间冲激响应融入扰动生成中，使得监狱突破音频在空中播放时仍然有效。相比之下，之前的所有音频监狱突破攻击都无法提供异步性、普适性、隐蔽性或空中鲁棒性。此外，AudioJailbreak还适用于不能完全操控用户提示的攻击者，因此具有更广泛的应用场景。迄今为止使用最多的LALMs的广泛实验表明了AudioJailbreak的高有效性。我们强调，我们的工作揭示了音频监狱突破攻击对LALMs安全性的潜在影响，真实地促进了提高其安全性。我们的实现和音频样本可在我们的网站上获取：this https URL', 'title_zh': 'AudioJailbreak: 对端到端大型声音-语言模型的脱 jailbreak 攻击'}
{'arxiv_id': 'arXiv:2505.14080', 'title': 'Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory', 'authors': 'Franziska Sofia Hafner, Ana Valdivia, Luc Rocher', 'link': 'https://arxiv.org/abs/2505.14080', 'abstract': "Language models encode and subsequently perpetuate harmful gendered stereotypes. Research has succeeded in mitigating some of these harms, e.g. by dissociating non-gendered terms such as occupations from gendered terms such as 'woman' and 'man'. This approach, however, remains superficial given that associations are only one form of prejudice through which gendered harms arise. Critical scholarship on gender, such as gender performativity theory, emphasizes how harms often arise from the construction of gender itself, such as conflating gender with biological sex. In language models, these issues could lead to the erasure of transgender and gender diverse identities and cause harms in downstream applications, from misgendering users to misdiagnosing patients based on wrong assumptions about their anatomy.\nFor FAccT research on gendered harms to go beyond superficial linguistic associations, we advocate for a broader definition of 'gender bias' in language models. We operationalize insights on the construction of gender through language from gender studies literature and then empirically test how 16 language models of different architectures, training datasets, and model sizes encode gender. We find that language models tend to encode gender as a binary category tied to biological sex, and that gendered terms that do not neatly fall into one of these binary categories are erased and pathologized. Finally, we show that larger models, which achieve better results on performance benchmarks, learn stronger associations between gender and sex, further reinforcing a narrow understanding of gender. Our findings lead us to call for a re-evaluation of how gendered harms in language models are defined and addressed.", 'abstract_zh': '语言模型编码并延续有害的性别刻板印象。为了超越表面的语义关联，FAccT关于性别危害的研究应扩展语言模型中“性别偏见”的定义。我们通过性别研究文献中的见解，将性别构建通过语言的认识具体化，并实证测试16种不同架构、训练数据集和模型规模的语言模型如何编码性别。我们发现，语言模型往往将性别编码为一种与生物学性别紧密相关的二元分类，并且不符合此类二元分类的性别术语被抹除和病理化。此外，我们在性能基准测试中表现更佳的大规模模型，进一步强化了对性别的狭隘理解。我们的研究结果促使我们重新评估语言模型中性别危害的定义和应对方式。', 'title_zh': '语言模型中的性别麻烦：基于性别表象理论的实证审查'}
{'arxiv_id': 'arXiv:2505.14057', 'title': 'Field Matters: A lightweight LLM-enhanced Method for CTR Prediction', 'authors': 'Yu Cui, Feng Liu, Jiawei Chen, Xingyu Lou, Changwang Zhang, Jun Wang, Yuegang Sun, Xiaohu Yang, Can Wang', 'link': 'https://arxiv.org/abs/2505.14057', 'abstract': 'Click-through rate (CTR) prediction is a fundamental task in modern recommender systems. In recent years, the integration of large language models (LLMs) has been shown to effectively enhance the performance of traditional CTR methods. However, existing LLM-enhanced methods often require extensive processing of detailed textual descriptions for large-scale instances or user/item entities, leading to substantial computational overhead. To address this challenge, this work introduces LLaCTR, a novel and lightweight LLM-enhanced CTR method that employs a field-level enhancement paradigm. Specifically, LLaCTR first utilizes LLMs to distill crucial and lightweight semantic knowledge from small-scale feature fields through self-supervised field-feature fine-tuning. Subsequently, it leverages this field-level semantic knowledge to enhance both feature representation and feature interactions. In our experiments, we integrate LLaCTR with six representative CTR models across four datasets, demonstrating its superior performance in terms of both effectiveness and efficiency compared to existing LLM-enhanced methods. Our code is available at this https URL.', 'abstract_zh': '点击率（CTR）预测是现代推荐系统中的一个基本任务。近年来，大语言模型（LLMs）的整合已被证明能够有效增强传统CTR方法的性能。然而，现有的LLM增强方法通常需要对大规模实例或用户/项目实体进行详细的文本描述的大量处理，导致显著的计算开销。为应对这一挑战，本文提出了LLaCTR，这是一种新颖且轻量级的LLM增强CTR方法，采用场级增强范式。具体而言，LLaCTR首先利用LLMs通过自我监督的场特征微调从小规模特征场中提炼出关键且轻量级的语义知识。随后，它利用这种场级语义知识来增强特征表示和特征交互。在我们的实验中，我们将LLaCTR与四个数据集上的六种代表性CTR模型结合，证明了其在效率和效果上均优于现有的LLM增强方法。我们的代码可在以下链接获取：this https URL。', 'title_zh': '场域 Matters: 一个轻量级的LLM增强方法用于点击率预测'}
{'arxiv_id': 'arXiv:2505.14045', 'title': 'From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora', 'authors': 'Yingli Shen, Wen Lai, Shuo Wang, Kangyang Luo, Alexander Fraser, Maosong Sun', 'link': 'https://arxiv.org/abs/2505.14045', 'abstract': 'Continued pretraining and instruction tuning on large-scale multilingual data have proven to be effective in scaling large language models (LLMs) to low-resource languages. However, the unaligned nature of such data limits its ability to effectively capture cross-lingual semantics. In contrast, multi-way parallel data, where identical content is aligned across multiple languages, provides stronger cross-lingual consistency and offers greater potential for improving multilingual performance. In this paper, we introduce a large-scale, high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus spans 113 languages, with up to 50 languages aligned in parallel, ensuring extensive multilingual coverage. Using this dataset, we investigate best practices for leveraging multi-way parallel data to enhance LLMs, including strategies for continued pretraining, instruction tuning, and the analysis of key influencing factors. Experiments on six multilingual benchmarks show that models trained on multiway parallel data consistently outperform those trained on unaligned multilingual data.', 'abstract_zh': '大规模多语言数据的持续预训练和指令调优已被证明能有效扩展大型语言模型（LLMs）到低资源语言，但此类数据的不一致性限制了其有效捕捉跨语言语义的能力。相比之下，多向平行数据，其中相同内容在多种语言中对齐，提供了更强的跨语言一致性，为提高多语言性能提供了更大的潜力。在本文中，我们基于TED Talks推出了一个大规模、高质量的多向平行语料库TED2025，涵盖113种语言，最多50种语言平行对齐，确保了广泛的多语言覆盖。利用该数据集，我们研究了利用多向平行数据增强LLMs的最佳实践，包括持续预训练、指令调优策略以及关键影响因素的分析。在六个多语言基准上的实验表明，使用多向平行数据训练的模型始终优于使用未对齐多语言数据训练的模型。', 'title_zh': '从未对齐到对齐：使用多向并行语料库扩展多语言LLM'}
{'arxiv_id': 'arXiv:2505.13995', 'title': 'Social Sycophancy: A Broader Understanding of LLM Sycophancy', 'authors': 'Myra Cheng, Sunny Yu, Cinoo Lee, Pranav Khadpe, Lujain Ibrahim, Dan Jurafsky', 'link': 'https://arxiv.org/abs/2505.13995', 'abstract': "A serious risk to the safety and utility of LLMs is sycophancy, i.e., excessive agreement with and flattery of the user. Yet existing work focuses on only one aspect of sycophancy: agreement with users' explicitly stated beliefs that can be compared to a ground truth. This overlooks forms of sycophancy that arise in ambiguous contexts such as advice and support-seeking, where there is no clear ground truth, yet sycophancy can reinforce harmful implicit assumptions, beliefs, or actions. To address this gap, we introduce a richer theory of social sycophancy in LLMs, characterizing sycophancy as the excessive preservation of a user's face (the positive self-image a person seeks to maintain in an interaction). We present ELEPHANT, a framework for evaluating social sycophancy across five face-preserving behaviors (emotional validation, moral endorsement, indirect language, indirect action, and accepting framing) on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole (AITA). Across eight models, we show that LLMs consistently exhibit high rates of social sycophancy: on OEQ, they preserve face 47% more than humans, and on AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments in 42% of cases. We further show that social sycophancy is rewarded in preference datasets and is not easily mitigated. Our work provides theoretical grounding and empirical tools (datasets and code) for understanding and addressing this under-recognized but consequential issue.", 'abstract_zh': '一种严重威胁LLM安全性和实用性的风险是拍马屁，即过度同意和谄媚用户。现有研究仅关注拍马屁的一个方面：与用户明确表达且可以与事实真相对比的信念的一致性。这忽视了在含糊情境中出现的拍马屁形式，如寻求建议和支持，这些情境没有清晰的事实真相，但拍马屁仍能强化有害的潜在假设、信念或行为。为弥补这一缺口，我们提出了LLM中社会拍马屁的富集理论，将拍马屁定义为过度保留用户面子（互动中个人希望维持的积极自我形象）。我们介绍了ELEPHANT框架，用于评估跨五种面子保留行为（情感验证、道德肯定、间接语言、间接行动和接受框架）的社会拍马屁，在开放式问题和Reddit的r/AmITheAsshole数据集中进行。在八个模型中，我们展示了LLM在社会拍马屁方面的一贯表现：在开放式问题数据集中，它们保持面子比人类多47%；在r/AmITheAsshole数据集中，它们在42%的情况下肯定了众包人类判断认为不合适的行为。进一步研究表明，社会拍马屁在偏好数据集中受到奖励，且难以缓解。我们的工作提供了理论基础和实证工具（数据集和代码），以理解并解决这一未被充分认识但具有重要影响的问题。', 'title_zh': '社会拍马屁：对LLM拍马屁现象更广泛的理解'}
{'arxiv_id': 'arXiv:2505.13989', 'title': 'When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty', 'authors': 'Yanzhe Wen, Xunkai Li, Qi Zhang, Zhu Lei, Guang Zeng, Rong-Hua Li, Guoren Wang', 'link': 'https://arxiv.org/abs/2505.13989', 'abstract': "Recently, large language models (LLMs) have significantly advanced text-attributed graph (TAG) learning. However, existing methods inadequately handle data uncertainty in open-world scenarios, especially concerning limited labeling and unknown-class nodes. Prior solutions typically rely on isolated semantic or structural approaches for unknown-class rejection, lacking effective annotation pipelines. To address these limitations, we propose Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive label traceability, which integrates semantics and topology for unknown-class rejection, and a graph label annotator to enable model updates using newly annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and practicality.", 'abstract_zh': '开放世界图助手：基于大语言模型的图标注辅助框架', 'title_zh': '当大规模语言模型遇到开放世界图学习：无标注数据不确定性的一种新视角'}
{'arxiv_id': 'arXiv:2505.13949', 'title': 'FlashThink: An Early Exit Method For Efficient Reasoning', 'authors': 'Guochao Jiang, Guofeng Quan, Zepeng Ding, Ziqin Luo, Dixuan Wang, Zheng Hu', 'link': 'https://arxiv.org/abs/2505.13949', 'abstract': 'Large Language Models (LLMs) have shown impressive performance in reasoning tasks. However, LLMs tend to generate excessively long reasoning content, leading to significant computational overhead. Our observations indicate that even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning content, which is against intuitive expectations. Preliminary experiments show that at a certain point during the generation process, the model is already capable of producing the correct solution without completing the full reasoning content. Therefore, we consider that the reasoning process of the model can be exited early to achieve the purpose of efficient reasoning. We introduce a verification model that identifies the exact moment when the model can stop reasoning and still provide the correct answer. Comprehensive experiments on four different benchmarks demonstrate that our proposed method, FlashThink, effectively shortens the reasoning content while preserving the model accuracy. For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning content by 77.04% and 77.47%, respectively, without reducing the accuracy.', 'abstract_zh': '大规模语言模型在推理任务中表现出色，但往往会生成过长的推理内容，导致 significant 计算开销。观察表明，即使在简单问题上，模型也可能生成不必要的冗长推理内容，违背直观预期。初步实验显示，在生成过程中某个时刻，模型已经能够生成正确答案而无需完成全部推理内容。因此，我们认为可以提前终止模型的推理过程以实现高效的推理。我们引入一种验证模型，以识别模型可以停止推理且仍然提供正确答案的精确时刻。在四个不同的基准测试中，我们的方法 FlashThink 有效缩短了推理内容长度同时保持了模型准确性。对于 Deepseek-R1 和 QwQ-32B 模型，我们分别减少了推理内容长度的 77.04% 和 77.47%，而未降低准确性。', 'title_zh': 'FlashThink: 一种高效的早期退出推理方法'}
{'arxiv_id': 'arXiv:2505.13941', 'title': 'MLZero: A Multi-Agent System for End-to-end Machine Learning Automation', 'authors': 'Haoyang Fang, Boran Han, Nick Erickson, Xiyuan Zhang, Su Zhou, Anirudh Dagar, Jiani Zhang, Ali Caner Turkmen, Cuixiong Hu, Huzefa Rangwala, Ying Nian Wu, Bernie Wang, George Karypis', 'link': 'https://arxiv.org/abs/2505.13941', 'abstract': 'Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly when handling multimodal data. We introduce MLZero, a novel multi-agent framework powered by Large Language Models (LLMs) that enables end-to-end ML automation across diverse data modalities with minimal human intervention. A cognitive perception module is first employed, transforming raw multimodal inputs into perceptual context that effectively guides the subsequent workflow. To address key limitations of LLMs, such as hallucinated code generation and outdated API knowledge, we enhance the iterative code generation process with semantic and episodic memory. MLZero demonstrates superior performance on MLE-Bench Lite, outperforming all competitors in both success rate and solution quality, securing six gold medals. Additionally, when evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more challenging tasks spanning diverse data modalities, MLZero outperforms the competing methods by a large margin with a success rate of 0.92 (+263.6\\%) and an average rank of 2.28. Our approach maintains its robust effectiveness even with a compact 8B LLM, outperforming full-size systems from existing solutions.', 'abstract_zh': '现有的AutoML系统已在机器学习自动化方面取得了进展；然而，它们仍然需要大量的手动配置和专家输入，特别是在处理多模态数据时。我们提出MLZero，这是一种由大型语言模型（LLMs）驱动的新型多代理框架，能够在最少的人工干预下实现跨多种数据模态的端到端机器学习自动化。首先采用认知感知模块，将原始多模态输入转换为有效的感知上下文，进而引导后续工作流程。为了应对大型语言模型的关键限制，如虚构代码生成和过时的API知识，我们通过语义和情景记忆增强了迭代代码生成过程。MLZero在MLE-Bench Lite上表现出色，成功率达到和解决方案质量均优于所有竞争者，并获得了六个金牌。此外，在我们的Multimodal AutoML Agent Benchmark中，该基准包含了25个更具挑战性的任务，涵盖多种数据模态，MLZero的成功率高达0.92（+263.6%），平均排名为2.28。即使使用紧凑的8B LLM，我们的方法仍能保持其稳健的有效性，优于现有解决方案中的全大小系统。', 'title_zh': 'MLZero: 一种端到端机器学习自动化多agent系统'}
{'arxiv_id': 'arXiv:2505.13936', 'title': 'EEG-to-Text Translation: A Model for Deciphering Human Brain Activity', 'authors': 'Saydul Akbar Murad, Ashim Dahal, Nick Rahimi', 'link': 'https://arxiv.org/abs/2505.13936', 'abstract': 'With the rapid advancement of large language models like Gemini, GPT, and others, bridging the gap between the human brain and language processing has become an important area of focus. To address this challenge, researchers have developed various models to decode EEG signals into text. However, these models still face significant performance limitations. To overcome these shortcomings, we propose a new model, R1 Translator, which aims to improve the performance of EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM encoder with a pretrained transformer-based decoder, utilizing EEG features to produce high-quality text outputs. The model processes EEG embeddings through the LSTM to capture sequential dependencies, which are then fed into the transformer decoder for effective text generation. The R1 Translator excels in ROUGE metrics, outperforming both T5 (previous research) and Brain Translator. Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9% higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and Brain by 3.6% (0.7553). Code is available at this https URL.', 'abstract_zh': '随着大型语言模型如Gemini、GPT等的快速 advancement，人类大脑与语言处理之间的差距缩小成为一个重要研究方向。为应对这一挑战，研究人员开发了多种模型将EEG信号解码为文本。然而，这些模型依然存在显著的性能限制。为了克服这些不足，我们提出了一种名为R1 Translator的新模型，旨在提高EEG-to-text解码性能。R1 Translator模型结合双向LSTM编码器和预训练的基于变换器的解码器，利用EEG特征生成高质量的文本输出。该模型通过LSTM处理EEG嵌入以捕获顺序依赖性，并将这些信息输入到变换器解码器以实现有效的文本生成。R1 Translator在ROUGE指标上表现出色，优于T5（先前研究）和Brain Translator。具体而言，R1在ROUGE-1指标上达到了38.00%（P），比T5（34.89%）高9%，比Brain（35.69%）高3%。在ROUGE-L指标上，R1的F1分数为32.51%，分别比T5（29.67%）高出3%和比Brain（30.38%）高出2%。在CER方面，R1的CER为0.5795，分别比T5（0.5917）低2%和比Brain（0.6001）低4%。在WER方面，R1得分0.7280，分别比T5（0.7610）低4.3%和比Brain（0.7553）低3.6%。代码可在此处访问。', 'title_zh': 'EEG到文本翻译：一种解码人类脑活动的模型'}
{'arxiv_id': 'arXiv:2505.13898', 'title': 'Do Language Models Use Their Depth Efficiently?', 'authors': 'Róbert Csordás, Christopher D. Manning, Christopher Potts', 'link': 'https://arxiv.org/abs/2505.13898', 'abstract': 'Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers? To address these questions, we analyze the residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First, comparing the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, with a clear phase transition between the two halves. Second, skipping layers in the second half has a much smaller effect on future computations and output predictions. Third, for multihop tasks, we are unable to find evidence that models are using increased depth to compose subresults in examples involving many hops. Fourth, we seek to directly address whether deeper models are using their additional layers to perform new kinds of computation. To do this, we train linear maps from the residual stream of a shallow model to a deeper one. We find that layers with the same relative depth map best to each other, suggesting that the larger model simply spreads the same computations out over its many layers. All this evidence suggests that deeper models are not using their depth to learn new kinds of computation, but only using the greater depth to perform more fine-grained adjustments to the residual. This may help explain why increasing scale leads to diminishing returns for stacked Transformer architectures.', 'abstract_zh': '现代大规模语言模型越来越深，深度与性能正相关，但回报逐渐减少。然而，这些模型是否有效地利用了其深度？它们是否通过增加特征组合来创建浅层模型无法完成的高级计算，还是仅仅将相同类型的计算分布在更多的层中？为回答这些问题，我们分析了Llama 3.1和Qwen 3系列模型的残差流。我们发现：首先，将子层输出与残差流进行比较表明，第二半部分的层贡献远小于第一半部分，两半之间存在明显的分段变化。其次，去除第二半部分的层对后续计算和输出预测的影响较小。第三，对于多跳任务，我们未能找到证据表明模型利用增加的深度在涉及多跳的例子中组合子结果。第四，我们直接探讨是否更深的模型利用其额外的层执行新类型的计算。为此，我们训练了一个从浅层模型的残差流到更深模型的线性映射。我们发现具有相同相对深度的层最好地映射到彼此，这表明较大模型只是将其计算在其许多层中进行分摊。所有这些证据表明，更深的模型并未利用其深度来学习新类型的计算，而是利用更大的深度来进行残差的更细致调整。这或许可以解释为什么增加规模对堆叠的Transformer架构带来的回报逐渐减少。', 'title_zh': '语言模型能否有效地利用其深度？'}
{'arxiv_id': 'arXiv:2505.13855', 'title': 'Domain Gating Ensemble Networks for AI-Generated Text Detection', 'authors': 'Arihant Tripathi, Liam Dugan, Charis Gao, Maggie Huan, Emma Jin, Peter Zhang, David Zhang, Julia Zhao, Chris Callison-Burch', 'link': 'https://arxiv.org/abs/2505.13855', 'abstract': 'As state-of-the-art language models continue to improve, the need for robust detection of machine-generated text becomes increasingly critical. However, current state-of-the-art machine text detectors struggle to adapt to new unseen domains and generative models. In this paper we present DoGEN (Domain Gating Ensemble Networks), a technique that allows detectors to adapt to unseen domains by ensembling a set of domain expert detector models using weights from a domain classifier. We test DoGEN on a wide variety of domains from leading benchmarks and find that it achieves state-of-the-art performance on in-domain detection while outperforming models twice its size on out-of-domain detection. We release our code and trained models to assist in future research in domain-adaptive AI detection.', 'abstract_zh': '基于领域门控集成网络的机器生成文本检测技术', 'title_zh': '基于域门控集成网络的AI生成文本检测'}
{'arxiv_id': 'arXiv:2505.13840', 'title': 'EfficientLLM: Efficiency in Large Language Models', 'authors': 'Zhengqing Yuan, Weixiang Sun, Yixin Liu, Huichi Zhou, Rong Zhou, Yiyang Li, Zheyuan Zhang, Wei Song, Yue Huang, Haolong Jia, Keerthiram Murugesan, Yu Wang, Lifang He, Jianfeng Gao, Lichao Sun, Yanfang Ye', 'link': 'https://arxiv.org/abs/2505.13840', 'abstract': 'Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first comprehensive empirical study evaluating efficiency techniques for LLMs at scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our study systematically explores three key axes: (1) architecture pretraining (efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts (MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and (3) inference (quantization methods: int4, float16). We define six fine-grained metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy Consumption, Compression Rate) to capture hardware saturation, latency-throughput balance, and carbon cost. Evaluating over 100 model-technique pairs (0.5B-72B parameters), we derive three core insights: (i) Efficiency involves quantifiable trade-offs: no single method is universally optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by 40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5% accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal memory-latency trade-offs for constrained devices, MLA achieves lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency only beyond 14B parameters. (iii) Techniques generalize across modalities: we extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides essential guidance for researchers and engineers navigating the efficiency-performance landscape of next-generation foundation models.', 'abstract_zh': 'EfficientLLM：大规模语言模型效率技术的全面 empirical 研究', 'title_zh': '高效大语言模型: 大规模语言模型的效率'}
{'arxiv_id': 'arXiv:2505.13820', 'title': 'Structured Agent Distillation for Large Language Model', 'authors': 'Jun Liu, Zhenglun Kong, Peiyan Dong, Changdi Yang, Tianqi Li, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Pu Zhao, Xue Lin, Dong Huang, Yanzhi Wang', 'link': 'https://arxiv.org/abs/2505.13820', 'abstract': "Large language models (LLMs) exhibit strong capabilities as decision-making agents by interleaving reasoning and actions, as seen in ReAct-style frameworks. Yet, their practical deployment is constrained by high inference costs and large model sizes. We propose Structured Agent Distillation, a framework that compresses large LLM-based agents into smaller student models while preserving both reasoning fidelity and action consistency. Unlike standard token-level distillation, our method segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align each component with the teacher's behavior. This structure-aware supervision enables compact agents to better replicate the teacher's decision process. Experiments on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop. Scaling and ablation results further highlight the importance of span-level alignment for efficient and deployable agents.", 'abstract_zh': '基于结构的代理蒸馏：一种保留推理准确性和动作一致性的同时压缩大型语言模型代理的方法', 'title_zh': '结构化代理人蒸馏用于大型语言模型'}
{'arxiv_id': 'arXiv:2505.13787', 'title': 'Preference Learning with Lie Detectors can Induce Honesty or Evasion', 'authors': 'Chris Cundy, Adam Gleave', 'link': 'https://arxiv.org/abs/2505.13787', 'abstract': 'As AI systems become more capable, deceptive behaviors can undermine evaluation and mislead users at deployment. Recent work has shown that lie detectors can accurately classify deceptive behavior, but they are not typically used in the training pipeline due to concerns around contamination and objective hacking. We examine these concerns by incorporating a lie detector into the labelling step of LLM post-training and evaluating whether the learned policy is genuinely more honest, or instead learns to fool the lie detector while remaining deceptive. Using DolusChat, a novel 65k-example dataset with paired truthful/deceptive responses, we identify three key factors that determine the honesty of learned policies: amount of exploration during preference learning, lie detector accuracy, and KL regularization strength. We find that preference learning with lie detectors and GRPO can lead to policies which evade lie detectors, with deception rates of over 85\\%. However, if the lie detector true positive rate (TPR) or KL regularization is sufficiently high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO) consistently lead to deception rates under 25\\% for realistic TPRs. Our results illustrate a more complex picture than previously assumed: depending on the context, lie-detector-enhanced training can be a powerful tool for scalable oversight, or a counterproductive method encouraging undetectable misalignment.', 'abstract_zh': '随着AI系统的能力增强，欺诈行为可能在部署中破坏评估并误导用户。近期研究表明，谎言检测器能够准确分类欺诈行为，但由于担忧污染和客观攻击，它们通常不用于训练管道中。我们通过将谎言检测器纳入LLM训练后标注步骤，评估学习策略是否确实更加诚实，抑或学会欺骗谎言检测器并在实际中仍然具有欺骗性。使用DolusChat这一包含65,000个成对真实/欺诈反应的新颖数据集，我们确定了决定学习策略诚实性的三个关键因素：偏好学习过程中的探索量、谎言检测器的准确性以及KL正则化强度。我们发现，带有谎言检测器和GRPO的学习策略可能导致逃避谎言检测器的行为，其欺骗率为85％以上。然而，如果谎言检测器的真阳性率（TPR）或KL正则化足够高，GRPO将学习诚实策略。相比之下，离策算法（DPO）在现实的TPR下始终导致欺骗率低于25％。我们的结果揭示了一种比之前认为更为复杂的情况：根据上下文，增强谎言检测器的训练可能是一种强大的可扩展监督工具，也可能是一种反生产的方法，鼓励不可检测的不对齐。', 'title_zh': '带有谎言检测器的偏好学习可以诱发诚实或规避行为'}
{'arxiv_id': 'arXiv:2505.13775', 'title': 'Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens', 'authors': 'Kaya Stechly, Karthik Valmeekam, Atharva Gundawar, Vardhan Palod, Subbarao Kambhampati', 'link': 'https://arxiv.org/abs/2505.13775', 'abstract': 'Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), and especially of the process of training on CoTs sampled from base LLMs in order to help find new reasoning patterns. In this paper, we critically examine that interpretation by investigating how the semantics of intermediate tokens-often anthropomorphized as "thoughts" or reasoning traces and which are claimed to display behaviors like backtracking, self-verification etc.-actually influence model performance. We train transformer models on formally verifiable reasoning traces and solutions, constraining both intermediate steps and final outputs to align with those of a formal solver (in our case, A* search). By constructing a formal interpreter of the semantics of our problems and intended algorithm, we systematically evaluate not only solution accuracy but also the correctness of intermediate traces, thus allowing us to evaluate whether the latter causally influences the former. We notice that, despite significant improvements on the solution-only baseline, models trained on entirely correct traces still produce invalid reasoning traces when arriving at correct solutions. To further show that trace accuracy is only loosely connected to solution accuracy, we then train models on noisy, corrupted traces which have no relation to the specific problem each is paired with, and find that not only does performance remain largely consistent with models trained on correct data, but in some cases can improve upon it and generalize more robustly on out-of-distribution tasks. These results challenge the assumption that intermediate tokens or "Chains of Thought" induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly correct forms) as evidence of human-like or algorithmic behaviors in language models.', 'abstract_zh': '大型推理模型 Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), and especially of the process of training on CoTs sampled from base LLMs in order to help find new reasoning patterns. In this paper, we critically examine that interpretation by investigating how the semantics of intermediate tokens-often anthropomorphized as "thoughts" or reasoning traces and which are claimed to display behaviors like backtracking, self-verification etc.-actually influence model performance. We train transformer models on formally verifiable reasoning traces and solutions, constraining both intermediate steps and final outputs to align with those of a formal solver (in our case, A* search). By constructing a formal interpreter of the semantics of our problems and intended algorithm, we systematically evaluate not only solution accuracy but also the correctness of intermediate traces, thus allowing us to evaluate whether the latter causally influences the former. We notice that, despite significant improvements on the solution-only baseline, models trained on entirely correct traces still produce invalid reasoning traces when arriving at correct solutions. To further show that trace accuracy is only loosely connected to solution accuracy, we then train models on noisy, corrupted traces which have no relation to the specific problem each is paired with, and find that not only does performance remain largely consistent with models trained on correct data, but in some cases can improve upon it and generalize more robustly on out-of-distribution tasks. These results challenge the assumption that intermediate tokens or "Chains of Thought" induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly correct forms) as evidence of human-like or algorithmic behaviors in language models.标题：\n\n近期大型推理模型的显著成果被解读为Chain of Thought (CoT)的胜利，尤其是在通过从基础LLM中采样的CoT进行训练以帮助发现新的推理模式方面。本文通过对中间令牌 semantics 的研究，质疑这种解读，这些中间令牌常常被拟人化为“想法”或推理痕迹，声称它们表现出类似回溯、自我验证等行为，实际上如何影响模型性能。我们通过对正式可验证的推理痕迹和解决方案进行变压器模型的训练，限制中间步骤和最终输出与正式求解器（如A*搜索）的输出对齐。通过构造我们问题和意图算法的正式解释器，我们系统地评估了解决方案的准确性以及中间痕迹的正确性，从而评估后者是否因果地影响前者。尽管在仅解决方案基线方面取得了显著改进，但训练于完全正确痕迹的模型在产生正确解决方案时仍然生成无效的推理痕迹。为了进一步证明痕迹准确性与解决方案准确性之间的松散联系，我们训练模型在与特定问题无关的噪声、损坏的痕迹上，发现性能不仅与正确数据训练的模型保持一致，而且在某些情况下可以优于其并更稳健地泛化到分布外的任务。本文的结果挑战了中间令牌或“Chain of Thought”诱导可预测推理行为的假设，并警告不要将其输出拟人化或过度解读（尽管它们大多是正确的形式），将其视为语言模型中类似人类或算法行为的证据。', 'title_zh': '超越语义：无关中间标记的不可思议效果'}
{'arxiv_id': 'arXiv:2505.13766', 'title': 'Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques', 'authors': 'Avinash Patil', 'link': 'https://arxiv.org/abs/2505.13766', 'abstract': 'Software Quality Assurance (SQA) is critical for delivering reliable, secure, and efficient software products. The Software Quality Assurance Process aims to provide assurance that work products and processes comply with predefined provisions and plans. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance existing SQA processes by automating tasks like requirement analysis, code review, test generation, and compliance checks. Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010, ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured frameworks for ensuring robust quality practices. This paper surveys the intersection of LLM-based SQA methods and these recognized standards, highlighting how AI-driven solutions can augment traditional approaches while maintaining compliance and process maturity. We first review the foundational software quality standards and the technical fundamentals of LLMs in software engineering. Next, we explore various LLM-based SQA applications, including requirement validation, defect detection, test generation, and documentation maintenance. We then map these applications to key software quality frameworks, illustrating how LLMs can address specific requirements and metrics within each standard. Empirical case studies and open-source initiatives demonstrate the practical viability of these methods. At the same time, discussions on challenges (e.g., data privacy, model bias, explainability) underscore the need for deliberate governance and auditing. Finally, we propose future directions encompassing adaptive learning, privacy-focused deployments, multimodal analysis, and evolving standards for AI-driven software quality.', 'abstract_zh': '基于大型语言模型的软件质量保证方法与公认标准的交叉研究：人工智能驱动解决方案如何增强传统方法的同时保持合规性和过程成熟度', 'title_zh': 'advancing 软件质量：基于标准的大型语言模型保障技术综述'}
{'arxiv_id': 'arXiv:2505.13738', 'title': 'Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training', 'authors': 'Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, Daria Soboleva, Joel Hestness', 'link': 'https://arxiv.org/abs/2505.13738', 'abstract': 'Efficient LLM pre-training requires well-tuned hyperparameters (HPs), including learning rate {\\eta} and weight decay {\\lambda}. We study scaling laws for HPs: formulas for how to scale HPs as we scale model size N, dataset size D, and batch size B. Recent work suggests the AdamW timescale, B/({\\eta}{\\lambda}D), should remain constant across training settings, and we verify the implication that optimal {\\lambda} scales linearly with B, for a fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise power law in the tokens-per-parameter ratio, D/N. This law thus provides a method to accurately predict {\\lambda}opt in advance of large-scale training. We also study scaling laws for optimal batch size Bopt (the B enabling lowest loss at a given N,D) and critical batch size Bcrit (the B beyond which further data parallelism becomes ineffective). In contrast with prior work, we find both Bopt and Bcrit scale as power laws in D, independent of model size, N. Finally, we analyze how these findings inform the real-world selection of Pareto-optimal N and D under dual training time and compute objectives.', 'abstract_zh': '高效预训练大规模语言模型需要精细调整超参数（HPs），包括学习率\\(\\eta\\)和权重衰减\\(\\lambda\\)。我们研究HPs的标度定律：随着模型规模N、数据集规模D和批次大小B的扩展，HPs应该如何进行缩放。近期工作表明，AdamW时间尺度B/(\\(\\eta\\lambda\\)D) 应在训练设置间保持恒定，并验证了在固定N和D的情况下，最优\\(\\lambda\\)线性缩放的推论。然而，当N和D扩大时，我们展示了最优时间尺度遵循令牌数与参数数之比D/N的精确幂律。此定律因此提供了一种方法，在大规模训练之前准确预测\\(\\lambda_{\\text{opt}}\\)。我们还研究了最优批次大小\\(B_{\\text{opt}}\\)（在给定N和D下损失最低的B）和临界批次大小\\(B_{\\text{crit}}\\)（在此之下进一步的数据并行变得无效）的标度定律。与先前工作不同，我们发现\\(B_{\\text{opt}}\\)和\\(B_{\\text{crit}}\\)都遵循D的幂律，与模型大小N无关。最后，我们分析了这些发现如何指导在双目标（训练时间和计算资源）下N和D的选择。', 'title_zh': '电力线：大规模语言模型预训练中权重衰减和批量大小的标度律'}
{'arxiv_id': 'arXiv:2505.13706', 'title': 'Are Large Language Models Good at Detecting Propaganda?', 'authors': 'Julia Jose, Rachel Greenstadt', 'link': 'https://arxiv.org/abs/2505.13706', 'abstract': 'Propagandists use rhetorical devices that rely on logical fallacies and emotional appeals to advance their agendas. Recognizing these techniques is key to making informed decisions. Recent advances in Natural Language Processing (NLP) have enabled the development of systems capable of detecting manipulative content. In this study, we look at several Large Language Models and their performance in detecting propaganda techniques in news articles. We compare the performance of these LLMs with transformer-based models. We find that, while GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude 3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally, we find that all three LLMs outperform a MultiGranularity Network (MGN) baseline in detecting instances of one out of six propaganda techniques (name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in detecting instances of appeal to fear and flag-waving.', 'abstract_zh': '宣传者利用逻辑谬误和情感诉求等修辞手法来推动他们的议程。识别这些技术对于做出知情决策至关重要。自然语言处理（NLP）的最新进展使开发检测操控性内容的系统成为可能。在本研究中，我们探讨了几种大型语言模型在检测新闻文章中的宣传技巧方面的性能，并将这些模型的性能与基于变换器的模型进行了对比。我们发现，虽然GPT-4在F1分数上（F1=0.16）优于GPT-3.5和Claude 3 Opus，但其并未超越RoBERTa-CRF基线（F1=0.67）。此外，我们发现，所有三种大型语言模型在检测六种宣传技巧中的一种（人身攻击）方面优于多粒度网络（MGN）基线，GPT-3.5和GPT-4在检测恐惧诉求和挥舞旗帜的情感诉求方面也优于MGN基线。', 'title_zh': '大型语言模型擅长检测宣传吗？'}
{'arxiv_id': 'arXiv:2505.13697', 'title': 'RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs', 'authors': 'Soumya Rani Samineni, Durgesh Kalwar, Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati', 'link': 'https://arxiv.org/abs/2505.13697', 'abstract': 'Reinforcement learning-based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing hype around improved reasoning abilities attributed to RL post-training, we critically examine the formulation and assumptions underlying these methods. We start by highlighting the popular structural assumptions made in modeling LLM training as a Markov Decision Process (MDP), and show how they lead to a degenerate MDP that doesn\'t quite need the RL/GRPO apparatus. The two critical structural assumptions include (1) making the MDP states be just a concatenation of the actions-with states becoming the context window and the actions becoming the tokens in LLMs and (2) splitting the reward of a state-action trajectory uniformly across the trajectory. Through a comprehensive analysis, we demonstrate that these simplifying assumptions make the approach effectively equivalent to an outcome-driven supervised learning. Our experiments on benchmarks including GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised fine-tuning, incorporating both positive and negative samples, achieves performance comparable to GRPO-based training. We will also argue that the structural assumptions indirectly incentivize the RL to generate longer sequences of intermediate tokens-which in turn feeds into the narrative of "RL generating longer thinking traces." While RL may well be a very useful technique for improving the reasoning abilities of LLMs, our analysis shows that the simplistic structural assumptions made in modeling the underlying MDP render the popular LLM RL frameworks and their interpretations questionable.', 'abstract_zh': '基于强化学习的大型语言模型（LLMs）后训练 recently gained attention, 特别是随着 DeepSeek R1 的发布，它应用了 GRPO 进行微调。随着对通过 RL 后训练增强推理能力的改进的 hype 日益增长，我们对这些方法的理论基础和基本假设进行了批判性检视。我们首先强调在建模 LLM 训练为马尔可夫决策过程（MDP）时所采用的流行结构假设，并展示这些假设如何导致一个退化且不必使用 RL/GRPO 装置的 MDP。两个关键的结构假设包括（1）将 MDP 状态仅视为动作的连接，状态成为上下文窗口，动作成为 LLM 中的标记；（2）将状态-动作轨迹的奖励均匀分配给轨迹上的所有点。通过全面分析，我们证明这些简化假设使该方法实际等价于结果驱动的监督学习。我们在包括 GSM8K 和 Countdown 的基准测试中使用 Qwen-2.5 基模型的实验表明，迭代的监督微调，结合正样本和负样本，达到了与基于 GRPO 的训练相当的性能。我们还将论证，这些结构假设间接促进了 RL 生成更长的中间标记序列，从而强化了“RL 生成更长的思维痕迹”这一论点。虽然 RL 可能是提高 LLM 推理能力的一种非常有用的技巧，但我们的分析表明，在建模底层 MDP 时所作的简单结构假设使得流行的 LLM RL 框架及其解释变得值得怀疑。', 'title_zh': '仅凭名称的RL？分析LLMs训练后基于RL的结构假设'}
{'arxiv_id': 'arXiv:2505.13577', 'title': 'VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation', 'authors': 'Yubin Kim, Taehan Kim, Wonjune Kang, Eugene Park, Joonsik Yoon, Dongjae Lee, Xin Liu, Daniel McDuff, Hyeonhoon Lee, Cynthia Breazeal, Hae Won Park', 'link': 'https://arxiv.org/abs/2505.13577', 'abstract': "Vocal health plays a crucial role in peoples' lives, significantly impacting their communicative abilities and interactions. However, despite the global prevalence of voice disorders, many lack access to convenient diagnosis and treatment. This paper introduces VocalAgent, an audio large language model (LLM) to address these challenges through vocal health diagnosis. We leverage Qwen-Audio-Chat fine-tuned on three datasets collected in-situ from hospital patients, and present a multifaceted evaluation framework encompassing a safety assessment to mitigate diagnostic biases, cross-lingual performance analysis, and modality ablation studies. VocalAgent demonstrates superior accuracy on voice disorder classification compared to state-of-the-art baselines. Its LLM-based method offers a scalable solution for broader adoption of health diagnostics, while underscoring the importance of ethical and technical validation.", 'abstract_zh': '语音健康在人们生活中发挥着 crucial 重要作用，显著影响其沟通能力和互动。然而，尽管全球范围内的 voice disorders 频率很高，许多人的诊断和治疗仍不够便利。本文介绍了 VocalAgent，这是一种音频大型语言模型 (LLM)，旨在通过语音健康诊断解决这些挑战。我们利用在医院患者现场收集的三个数据集上 fine-tuned 的 Qwen-Audio-Chat，提出了一种多方面的评估框架，包括安全性评估以减轻诊断偏见、跨语言性能分析以及模态消减研究。VocalAgent 在语音障碍分类上的准确性优于最先进的基线方法。其基于大型语言模型的方法为更广泛采纳健康管理诊断提供了可扩展的解决方案，同时强调了伦理和技术验证的重要性。', 'title_zh': 'VocalAgent：具有安全意识评估的大语言模型在声健康诊断中的应用'}
{'arxiv_id': 'arXiv:2505.13554', 'title': 'Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation', 'authors': 'Zhanglin Wu, Daimeng Wei, Xiaoyu Chen, Hengchao Shang, Jiaxin Guo, Zongyao Li, Yuanchang Luo, Jinlong Yang, Zhiqiang Rao, Hao Yang', 'link': 'https://arxiv.org/abs/2505.13554', 'abstract': 'Large language model (LLM) shows promising performances in a variety of downstream tasks, such as machine translation (MT). However, using LLMs for translation suffers from high computational costs and significant latency. Based on our evaluation, in most cases, translations using LLMs are comparable to that generated by neural machine translation (NMT) systems. Only in particular scenarios, LLM and NMT models show respective advantages. As a result, integrating NMT and LLM for translation and using LLM only when necessary seems to be a sound solution. A scheduling policy that optimizes translation result while ensuring fast speed and as little LLM usage as possible is thereby required. We compare several scheduling policies and propose a novel and straightforward decider that leverages source sentence features. We conduct extensive experiments on multilingual test sets and the result shows that we can achieve optimal translation performance with minimal LLM usage, demonstrating effectiveness of our decider.', 'abstract_zh': '大规模语言模型（LLM）在多种下游任务中展现出有前途的表现，例如机器翻译（MT）。然而，使用LLM进行翻译面临着高计算成本和显著的延迟问题。根据我们的评估，在大多数情况下，使用LLM生成的翻译与神经机器翻译（NMT）系统生成的翻译相当。只有在特定场景下，LLM和NMT模型才各自表现出优势。因此，结合NMT和LLM进行翻译，并仅在必要时使用LLM似乎是一种可行的解决方案。由此需要一个优化翻译结果并确保快速速度和尽可能少使用LLM的调度策略。我们将几种调度策略进行比较，并提出一种基于源句子特征的新颖且简单的决策器。我们在多语言测试集上进行了广泛的实验，结果显示，我们可以在最大限度减少LLM使用的情况下实现最优的翻译性能，证明了我们决策器的有效性。', 'title_zh': '兼收并蓄：一种混合NMT和LLM的翻译方法'}
{'arxiv_id': 'arXiv:2505.13547', 'title': 'Exploring Federated Pruning for Large Language Models', 'authors': 'Pengxin Guo, Yinong Wang, Wei Li, Mengting Liu, Ming Li, Jinkai Zheng, Liangqiong Qu', 'link': 'https://arxiv.org/abs/2505.13547', 'abstract': 'LLM pruning has emerged as a promising technology for compressing LLMs, enabling their deployment on resource-limited devices. However, current methodologies typically require access to public calibration samples, which can be challenging to obtain in privacy-sensitive domains. To address this issue, we introduce FedPrLLM, a comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs to calculate a pruning mask matrix based on its local calibration data and share it with the server to prune the global model. This approach allows for collaborative pruning of the global model with the knowledge of each client while maintaining local data privacy. Additionally, we conduct extensive experiments to explore various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and the decision to scale weights. Our extensive evaluation reveals that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework. We hope our work will help guide future efforts in pruning LLMs in privacy-sensitive fields. Our code is available at this https URL.', 'abstract_zh': 'FedPrLLM：一种用于隐私保护的大规模语言模型压缩的联邦剪枝框架', 'title_zh': '探索联邦剪枝在大型语言模型中的应用'}
{'arxiv_id': 'arXiv:2505.13545', 'title': 'Know Or Not: a library for evaluating out-of-knowledge base robustness', 'authors': 'Jessica Foo, Pradyumna Shyama Prasad, Shaun Khoo', 'link': 'https://arxiv.org/abs/2505.13545', 'abstract': 'While the capabilities of large language models (LLMs) have progressed significantly, their use in high-stakes applications have been limited due to risks of hallucination. One key approach in reducing hallucination is retrieval-augmented generation (RAG), but even in such setups, LLMs may still hallucinate when presented with questions outside of the knowledge base. Such behavior is unacceptable in high-stake applications where LLMs are expected to abstain from answering queries it does not have sufficient context on. In this work, we present a novel methodology for systematically evaluating out-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not know) in the RAG setting, without the need for manual annotation of gold standard answers. We implement our methodology in knowornot, an open-source library that enables users to develop their own customized evaluation data and pipelines for OOKB robustness. knowornot comprises four main features. Firstly, it provides a unified, high-level API that streamlines the process of setting up and running robustness benchmarks. Secondly, its modular architecture emphasizes extensibility and flexibility, allowing users to easily integrate their own LLM clients and RAG settings. Thirdly, its rigorous data modeling design ensures experiment reproducibility, reliability and traceability. Lastly, it implements a comprehensive suite of tools for users to customize their pipelines. We demonstrate the utility of knowornot by developing a challenging benchmark, PolicyBench, which spans four Question-Answer (QA) chatbots on government policies, and analyze its OOKB robustness. The source code of knowornot is available this https URL.', 'abstract_zh': '一种新的方法学，用于在检索增强生成设置中系统地评估大型语言模型的超出现有知识基础的鲁棒性（OOKB），而无需人工标注标准答案。', 'title_zh': '知与不知：一种评估知识库外 robustness 的库'}
{'arxiv_id': 'arXiv:2505.13538', 'title': 'RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG Pipelines', 'authors': 'Dvir Cohen, Lin Burg, Gilad Barkan', 'link': 'https://arxiv.org/abs/2505.13538', 'abstract': "Retrieval-Augmented Generation (RAG) systems show promise by coupling large language models with external knowledge, yet traditional RAG evaluation methods primarily report quantitative scores while offering limited actionable guidance for refining these complex pipelines. In this paper, we introduce RAGXplain, an evaluation framework that quantifies RAG performance and translates these assessments into clear insights that clarify the workings of its complex, multi-stage pipeline and offer actionable recommendations. Using LLM reasoning, RAGXplain converts raw scores into coherent narratives identifying performance gaps and suggesting targeted improvements. By providing transparent explanations for AI decision-making, our framework fosters user trust-a key challenge in AI adoption. Our LLM-based metric assessments show strong alignment with human judgments, and experiments on public question-answering datasets confirm that applying RAGXplain's actionable recommendations measurably improves system performance. RAGXplain thus bridges quantitative evaluation and practical optimization, empowering users to understand, trust, and enhance their AI systems.", 'abstract_zh': 'Retrieval-Augmented Generation (RAG) 系统通过将大型语言模型与外部知识结合展示出了潜力，然而传统的 RAG 评估方法主要报告定量分数，而对改进这些复杂流水线提供有限的实际指导。在本文中，我们介绍了 RAGXplain 评估框架，该框架量化 RAG 性能并将这些评估转化为清晰的见解，以阐明其复杂多阶段流水线的工作原理，并提供可操作的建议。利用大型语言模型推理，RAGXplain 将原始分数转化为连贯的故事，识别性能差距并提出针对性的改进措施。通过为 AI 决策提供透明解释，我们的框架促进了用户信任——这是 AI 采纳的关键挑战。基于大型语言模型的度量评估与人类判断高度一致，并且在公开问题回答数据集上的实验确认，应用 RAGXplain 的可操作建议可以显著提高系统性能。RAGXplain 从而在定量评估与实际优化之间架起了桥梁，助力用户理解、信任并提升其 AI 系统。', 'title_zh': 'RAGXplain：从可解释评估到RAG流水线的实际指导'}
{'arxiv_id': 'arXiv:2505.13535', 'title': 'Information Extraction from Visually Rich Documents using LLM-based Organization of Documents into Independent Textual Segments', 'authors': 'Aniket Bhattacharyya, Anurag Tripathi, Ujjal Das, Archan Karmakar, Amit Pathak, Maneesh Gupta', 'link': 'https://arxiv.org/abs/2505.13535', 'abstract': 'Information extraction (IE) from Visually Rich Documents (VRDs) containing layout features along with text is a critical and well-studied task. Specialized non-LLM NLP-based solutions typically involve training models using both textual and geometric information to label sequences/tokens as named entities or answers to specific questions. However, these approaches lack reasoning, are not able to infer values not explicitly present in documents, and do not generalize well to new formats. Generative LLM-based approaches proposed recently are capable of reasoning, but struggle to comprehend clues from document layout especially in previously unseen document formats, and do not show competitive performance in heterogeneous VRD benchmark datasets. In this paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs into localized, reusable semantic textual segments called $\\textit{semantic blocks}$, which are processed independently. Through focused and more generalizable reasoning,our approach outperforms the state-of-the-art on public VRD benchmarks by 1-3% in F1 scores, is resilient to document formats previously not encountered and shows abilities to correctly extract information not explicitly present in documents.', 'abstract_zh': '从包含布局特征的视觉丰富文档中提取信息（IE）是至关重要的一个研究课题。基于非LLM的NLP专有解决方案通常涉及使用文本和几何信息训练模型来标注命名实体或特定问题的答案。然而，这些方法缺乏推理能力，无法推断文档中未明确呈现的值，并且不善于处理新格式。近年来提出的生成性LLM方法具备推理能力，但在理解以前未见过的文档格式的文档布局线索方面存在问题，在异构VRD基准数据集中表现也不具竞争力。本文提出BLOCKIE，这是一种基于LLM的新型方法，将VRDs组织成局部可重复使用的语义文本段，称为“语义块”，并独立处理。通过更聚焦且更通用的推理，该方法在公共VRD基准测试中F1分数上优于现有最佳方法1-3%，对以前未遇见过的文档格式具有鲁棒性，并展示了正确提取未明确呈现于文档中的信息的能力。', 'title_zh': '使用基于LLM的文档组织方法从视觉丰富的文档中提取信息'}
{'arxiv_id': 'arXiv:2505.13531', 'title': "AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference", 'authors': 'Shitong Duan, Xiaoyuan Yi, Peng Zhang, Dongkuan Xu, Jing Yao, Tun Lu, Ning Gu, Xing Xie', 'link': 'https://arxiv.org/abs/2505.13531', 'abstract': "Assessing Large Language Models (LLMs)' underlying value differences enables comprehensive comparison of their misalignment, cultural adaptability, and biases. Nevertheless, current value measurement datasets face the informativeness challenge: with often outdated, contaminated, or generic test questions, they can only capture the shared value orientations among different LLMs, leading to saturated and thus uninformative results. To address this problem, we introduce AdAEM, a novel, self-extensible assessment framework for revealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM can automatically and adaptively generate and extend its test questions. This is achieved by probing the internal value boundaries of a diverse set of LLMs developed across cultures and time periods in an in-context optimization manner. The optimization process theoretically maximizes an information-theoretic objective to extract the latest or culturally controversial topics, providing more distinguishable and informative insights about models' value differences. In this way, AdAEM is able to co-evolve with the development of LLMs, consistently tracking their value dynamics. Using AdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct an extensive analysis to manifest our method's validity and effectiveness, and benchmark the values of 16 LLMs, laying the groundwork for better value research.", 'abstract_zh': '评估大型语言模型（LLMs）的内在价值差异 enables 对它们的偏差、文化适应性和偏见进行全面比较。然而，当前的价值度量数据集面临着信息量挑战：由于测试问题往往过时、受污染或通用，这些数据集只能捕捉不同LLMs之间共享的价值取向，导致结果饱和且缺乏信息量。为解决这一问题，我们介绍了AdAEM，这是一种新颖的、自我扩展的评估框架，用于揭示LLMs的倾向。不同于先前的静态基准测试，AdAEM能够自动且适应性地生成和扩展测试问题。这通过在多种文化和不同时期开发的不同LLMs的内部价值边界中进行上下文优化探针来实现。优化过程理论上最大化信息论目标，以提取最新的或文化争议性话题，从而提供更多可区分且富有信息量的关于模型价值差异的洞察。通过这种方式，AdAEM能够与LLMs的发展同步，持续跟踪其价值动态。利用AdAEM，我们生成了基于Schwartz价值理论的12,310个问题，进行广泛分析以验证我们方法的有效性，并对16个LLMs的价值进行了基准测试，为更好的价值研究奠定了基础。', 'title_zh': 'AdAEM: 一种自适应且自动可扩展的大型语言模型价值差异度量方法'}
{'arxiv_id': 'arXiv:2505.13528', 'title': 'LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems', 'authors': 'Shengkang Gu, Jiahao Liu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Ning Gu, Li Shang, Tun Lu', 'link': 'https://arxiv.org/abs/2505.13528', 'abstract': 'Recommender systems (RS) are increasingly vulnerable to shilling attacks, where adversaries inject fake user profiles to manipulate system outputs. Traditional attack strategies often rely on simplistic heuristics, require access to internal RS data, and overlook the manipulation potential of textual reviews. In this work, we introduce Agent4SR, a novel framework that leverages Large Language Model (LLM)-based agents to perform low-knowledge, high-impact shilling attacks through both rating and review generation. Agent4SR simulates realistic user behavior by orchestrating adversarial interactions, selecting items, assigning ratings, and crafting reviews, while maintaining behavioral plausibility. Our design includes targeted profile construction, hybrid memory retrieval, and a review attack strategy that propagates target item features across unrelated reviews to amplify manipulation. Extensive experiments on multiple datasets and RS architectures demonstrate that Agent4SR outperforms existing low-knowledge baselines in both effectiveness and stealth. Our findings reveal a new class of emergent threats posed by LLM-driven agents, underscoring the urgent need for enhanced defenses in modern recommender systems.', 'abstract_zh': '基于大型语言模型的Agent4SR：一种低知识、高影响的评分和评论生成恶意攻击框架', 'title_zh': '基于LLM的用户模拟在推荐系统中应对低知识型刷评攻击'}
{'arxiv_id': 'arXiv:2505.13527', 'title': 'Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression', 'authors': 'Jingyu Peng, Maolin Wang, Nan Wang, Xiangyu Zhao, Jiatong Li, Kai Zhang, Qi Liu', 'link': 'https://arxiv.org/abs/2505.13527', 'abstract': 'Despite substantial advancements in aligning large language models (LLMs) with human values, current safety mechanisms remain susceptible to jailbreak attacks. We hypothesize that this vulnerability stems from distributional discrepancies between alignment-oriented prompts and malicious prompts. To investigate this, we introduce LogiBreak, a novel and universal black-box jailbreak method that leverages logical expression translation to circumvent LLM safety systems. By converting harmful natural language prompts into formal logical expressions, LogiBreak exploits the distributional gap between alignment data and logic-based inputs, preserving the underlying semantic intent and readability while evading safety constraints. We evaluate LogiBreak on a multilingual jailbreak dataset spanning three languages, demonstrating its effectiveness across various evaluation settings and linguistic contexts.', 'abstract_zh': '尽管在使大型语言模型（LLM）与人类价值观对齐方面取得了显著进展，当前的安全机制仍易受到牢笼突破攻击。我们假设这种漏洞源于对齐导向提示与恶意提示之间的分布差异。为探讨这一问题，我们引入了LogiBreak，这是一种新颖且通用的黑盒牢笼突破方法，利用逻辑表达式转换来规避LLM安全系统。通过将有害的自然语言提示转换为形式逻辑表达式，LogiBreak 利用对齐数据与基于逻辑的输入之间的分布差异，保持潜在的语义意图和可读性，同时规避安全约束。我们在涵盖三种语言的多语言牢笼突破数据集上评估了LogiBreak，展示了其在各种评估设置和语言情境下的有效性。', 'title_zh': '逻辑囚笼破解：通过形式逻辑表达高效解除语言模型安全限制'}
{'arxiv_id': 'arXiv:2505.13526', 'title': 'Geography-Aware Large Language Models for Next POI Recommendation', 'authors': 'Zhao Liu, Wei Liu, Huajie Zhu, Jianxing Yu, Jian Yin, Wang-Chien Lee, Shun Wang', 'link': 'https://arxiv.org/abs/2505.13526', 'abstract': "The next Point-of-Interest (POI) recommendation task aims to predict users' next destinations based on their historical movement data and plays a key role in location-based services and personalized applications. Accurate next POI recommendation depends on effectively modeling geographic information and POI transition relations, which are crucial for capturing spatial dependencies and user movement patterns. While Large Language Models (LLMs) exhibit strong capabilities in semantic understanding and contextual reasoning, applying them to spatial tasks like next POI recommendation remains challenging. First, the infrequent nature of specific GPS coordinates makes it difficult for LLMs to model precise spatial contexts. Second, the lack of knowledge about POI transitions limits their ability to capture potential POI-POI relationships. To address these issues, we propose GA-LLM (Geography-Aware Large Language Model), a novel framework that enhances LLMs with two specialized components. The Geographic Coordinate Injection Module (GCIM) transforms GPS coordinates into spatial representations using hierarchical and Fourier-based positional encoding, enabling the model to understand geographic features from multiple perspectives. The POI Alignment Module (PAM) incorporates POI transition relations into the LLM's semantic space, allowing it to infer global POI relationships and generalize to unseen POIs. Experiments on three real-world datasets demonstrate the state-of-the-art performance of GA-LLM.", 'abstract_zh': '基于地理意识的大语言模型（GA-LLM）：一种用于下一个地点推荐的任务新框架', 'title_zh': '地理 Awareness 大型语言模型用于下一个POI 推荐'}
{'arxiv_id': 'arXiv:2505.13516', 'title': 'HALO: Hierarchical Autonomous Logic-Oriented Orchestration for Multi-Agent LLM Systems', 'authors': 'Zhipeng Hou, Junyi Tang, Yipeng Wang', 'link': 'https://arxiv.org/abs/2505.13516', 'abstract': 'Recent advancements in Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) have demonstrated tremendous potential in diverse task scenarios. Nonetheless, existing agentic systems typically rely on predefined agent-role design spaces and static communication structures, limiting their adaptability as well as flexibility in complex interaction environments and leading to subpar performance on highly specialized and expert-level tasks. To address these issues, we introduce HALO, a multi-agent collaboration framework based on a hierarchical reasoning architecture. Specifically, we incorporate a high-level planning agent for task decomposition, mid-level role-design agents for subtask-specific agent instantiation, and low-level inference agents for subtask execution. Particularly, subtask execution is reformulated as a structured workflow search problem, where Monte Carlo Tree Search (MCTS) systematically explores the agentic action space to construct optimal reasoning trajectories. Additionally, as the majority of users lack expertise in prompt engineering, we leverage an Adaptive Prompt Refinement module to transform raw queries into task-specific prompts. Empirical evaluations on Code Generation (HumanEval), General Reasoning (MMLU), and Arithmetic Reasoning (MATH) benchmark datasets highlight the effectiveness of HALO, yielding a 14.4% average improvement over state-of-the-art baselines. Notably, HALO achieves up to 13.3% performance gain on the Moral Scenarios subject in the MMLU benchmark and up to 19.6% performance gain on the Algebra subarea in the MATH benchmark, indicating its advanced proficiency in tackling highly specialized and expert-level tasks. The code repository is available at this https URL.', 'abstract_zh': 'Recent advancements in Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) have demonstrated tremendous potential in diverse task scenarios.', 'title_zh': 'HALO: 分层自主逻辑导向的多代理LLM系统 orchestrator'}
{'arxiv_id': 'arXiv:2505.13515', 'title': 'LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades', 'authors': 'Yanan Li, Fanxu Meng, Muhan Zhang, Shiai Zhu, Shangguang Wang, Mengwei Xu', 'link': 'https://arxiv.org/abs/2505.13515', 'abstract': 'As Large Language Models (LLMs) are frequently updated, LoRA weights trained on earlier versions quickly become obsolete. The conventional practice of retraining LoRA weights from scratch on the latest model is costly, time-consuming, and environmentally detrimental, particularly as the diversity of LLMs and downstream tasks expands. This motivates a critical question: "How can we efficiently leverage existing LoRA weights to adapt to newer model versions?" To address this, we propose LoRASuite, a modular approach tailored specifically to various types of LLM updates. First, we compute a transfer matrix utilizing known parameters from both old and new LLMs. Next, we allocate corresponding layers and attention heads based on centered kernel alignment and cosine similarity metrics, respectively. A subsequent small-scale, skillful fine-tuning step ensures numerical stability. Experimental evaluations demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even exceeds the performance of full-scale LoRA retraining, with average improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally, LoRASuite significantly reduces memory consumption by 5.5 GB and computational time by 78.23%.', 'abstract_zh': '大语言模型（LLMs）频繁更新，早期版本训练的LoRA权重很快变得过时。从最新模型重新训练LoRA权重的传统做法成本高、耗时且环境破坏性大，尤其是随着LLM的多样性和下游任务的增加。这促使了一个关键问题：“我们如何高效地利用现有LoRA权重适应新的模型版本？”为解决这一问题，我们提出LoRASuite，这是一种专门针对各种类型LLM更新的模块化方法。首先，我们利用旧和新LLM已知参数计算一个转移矩阵。然后，我们根据中心核对齐和余弦相似度指标分配相应的层和注意力头。后续的小规模、技艺精湛的微调步骤确保数值稳定性。实验评估表明，LoRASuite 总是优于小规模的 vanilla LoRA 方法。特别地，在MiniCPM和Qwen等骨干LLM上，LoRASuite的性能甚至超过了全面重新训练的LoRA，分别在数学任务上取得了平均1.4和6.6个百分点的提升。此外，LoRASuite还显著减少了5.5 GB的内存消耗和78.23%的计算时间。', 'title_zh': 'LoRASuite: 在大规模语言模型升级中高效适应的LoRA方法'}
{'arxiv_id': 'arXiv:2505.13514', 'title': 'Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models', 'authors': 'Shuxun Wang, Qingyu Yin, Chak Tou Leong, Qiang Zhang, Linyi Yang', 'link': 'https://arxiv.org/abs/2505.13514', 'abstract': 'Repetition curse is a phenomenon where Large Language Models (LLMs) generate repetitive sequences of tokens or cyclic sequences. While the repetition curse has been widely observed, its underlying mechanisms remain poorly understood. In this work, we investigate the role of induction heads--a specific type of attention head known for their ability to perform in-context learning--in driving this repetitive behavior. Specifically, we focus on the "toxicity" of induction heads, which we define as their tendency to dominate the model\'s output logits during repetition, effectively excluding other attention heads from contributing to the generation process. Our findings have important implications for the design and training of LLMs. By identifying induction heads as a key driver of the repetition curse, we provide a mechanistic explanation for this phenomenon and suggest potential avenues for mitigation. We also propose a technique with attention head regularization that could be employed to reduce the dominance of induction heads during generation, thereby promoting more diverse and coherent outputs.', 'abstract_zh': '重复诅咒是一种大型语言模型（LLMs）生成重复的令牌序列或循环序列的现象。尽管重复诅咒已经被广泛观察到，但其底层机制仍不甚明了。在本文中，我们调查了归纳头的作用——这种特定类型的注意力头以其内在的在上下文学习能力而著称——它们在推动这种重复行为中的作用。具体而言，我们将注意力头的“毒性”定义为其在重复过程中主导模型输出对数的趋势，从而有效地排除其他注意力头对生成过程的贡献。我们的发现对LLM的设计和训练具有重要影响。通过将归纳头识别为重复诅咒的关键驱动因素，我们提供了该现象的机制性解释，并提出了可能的缓解途径。我们还提出了一种带有注意力头正则化的技术，可以在生成过程中减少归纳头的主导作用，从而促进更多样化和连贯的输出。', 'title_zh': '诱导头毒性机制性解释大型语言模型中的重复诅咒'}
{'arxiv_id': 'arXiv:2505.13508', 'title': 'Time-R1: Towards Comprehensive Temporal Reasoning in LLMs', 'authors': 'Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, Jiaxuan You', 'link': 'https://arxiv.org/abs/2505.13508', 'abstract': 'Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce \\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a \\textit{reinforcement learning (RL) curriculum} driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release \\textit{Time-Bench}, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of \\textit{Time-R1} checkpoints.', 'abstract_zh': 'Large Language Models (LLMs)表现出色但在时序智能方面缺乏稳健性，难以将对过去推理的能力与对未来预测和合理生成未来的能力整合起来。同时，现有方法通常针对孤立的时序技能，如关于过去事件的问题回答或基本的预测，并且在处理超出其知识截止点的事件或需要创造力的前瞻性时表现出较差的泛化能力。为了解决这些局限性，我们引入了Time-R1，这是第一个为中等规模（3B参数）LLM赋予全面时序能力的框架：理解、预测和创造性生成。我们的方法采用了一种新颖的三阶段开发路径；前两个阶段构成一个基于精心设计的动态规则奖励系统的强化学习（RL）课程。该框架逐步构建（1）基于历史数据的时序基础理解和逻辑事件时间映射，（2）对未来事件的预测能力，特别是超越其知识截止点的事件，最后（3）无需任何微调即可实现非凡的创造性未来情境生成泛化能力。实验结果表明，Time-R1在复杂的未来事件预测和创造性情境生成基准测试中优于包括当前最先进的671B DeepSeek-R1在内的大得多的模型。这项工作提供了有力证据，表明精心设计的渐进式RL微调允许较小的、高效的模型实现卓越的时序性能，提供了一条真正的时间感知AI的实用和可扩展路径。为了促进进一步的研究，我们还发布了Time-Bench，这是一个源自十年新闻数据的大型多任务时序推理数据集，以及我们的一系列Time-R1检查点。', 'title_zh': '时间轴-1：面向LLMs的综合性 temporal推理研究'}
{'arxiv_id': 'arXiv:2505.13506', 'title': 'EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation', 'authors': 'Ruobing Yao, Yifei Zhang, Shuang Song, Neng Gao, Chenyang Tu', 'link': 'https://arxiv.org/abs/2505.13506', 'abstract': 'Retrieval-Augmented Generation (RAG) compensates for the static knowledge limitations of Large Language Models (LLMs) by integrating external knowledge, producing responses with enhanced factual correctness and query-specific contextualization. However, it also introduces new attack surfaces such as corpus poisoning at the same time. Most of the existing defense methods rely on the internal knowledge of the model, which conflicts with the design concept of RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and bait-guided context diversity detection to identify malicious content by analyzing the context diversity of candidate documents without relying on LLM internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art security with plug-and-play deployment, simultaneously improving clean-scenario RAG performance while maintaining practical operational costs (relatively 1.2$\\times$ latency, 48\\%-80\\% token reduction versus Vanilla RAG).', 'abstract_zh': 'EcoSafeRAG通过句子级处理和诱饵引导的上下文多样性检测，在不依赖大语言模型内部知识的情况下，识别恶意内容，从而增强RAG的安全性。', 'title_zh': 'EcoSafeRAG：通过检索增强生成中的语境分析实现高效安全性'}
{'arxiv_id': 'arXiv:2505.13500', 'title': 'Noise Injection Systemically Degrades Large Language Model Safety Guardrails', 'authors': 'Prithviraj Singh Shahani, Matthias Scheutz', 'link': 'https://arxiv.org/abs/2505.13500', 'abstract': 'Safety guardrails in large language models (LLMs) are a critical component in preventing harmful outputs. Yet, their resilience under perturbation remains poorly understood. In this paper, we investigate the robustness of safety fine-tuning in LLMs by systematically injecting Gaussian noise into model activations. We show across multiple open-weight models that (1) Gaussian noise raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety fine-tuning affords no extra protection, and (3) that chain-of-thought reasoning remains largely intact. The findings reveal critical vulnerabilities in current safety alignment techniques and highlight the potential of reasoning-based and reinforcement learning approaches as promising direction for developing more robust AI safety systems. These results have important implications for real-world deployment of LLMs in safety-critical applications as these results imply that widely-deployed safety tuning methods can fail even without adversarial prompts.', 'abstract_zh': '大型语言模型（LLMs）中的安全防护栏是防止有害输出的关键组成部分。然而，它们在受到扰动时的鲁棒性仍 poorly understood。本文通过系统地向模型激活中注入高斯噪声，研究了LLMs的安全微调的鲁棒性。结果显示，在多个开放权重模型中：（1）高斯噪声将有害输出率提高最多27%（p < 0.001）；（2）深层安全微调并未提供额外保护；（3）思维链推理保持相对完好。研究发现揭示了当前安全对齐技术中的关键脆弱性，并强调了基于推理和强化学习的方法在开发更鲁棒的AI安全系统方面的潜在前景。这些结果对于在安全关键应用中部署大型语言模型具有重要意义，因为这些结果表明，广泛部署的安全调优方法即使在没有对抗性提示的情况下也可能失败。', 'title_zh': '系统性注入噪声会降低大型语言模型的安全防护能力'}
{'arxiv_id': 'arXiv:2505.13498', 'title': 'IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation', 'authors': "Khanh-Tung Tran, Barry O'Sullivan, Hoang D. Nguyen", 'link': 'https://arxiv.org/abs/2505.13498', 'abstract': 'Recent advances in Large Language Models (LLMs) have demonstrated promising knowledge and reasoning abilities, yet their performance in multilingual and low-resource settings remains underexplored. Existing benchmarks often exhibit cultural bias, restrict evaluation to text-only, rely on multiple-choice formats, and, more importantly, are limited for extremely low-resource languages. To address these gaps, we introduce IRLBench, presented in parallel English and Irish, which is considered definitely endangered by UNESCO. Our benchmark consists of 12 representative subjects developed from the 2024 Irish Leaving Certificate exams, enabling fine-grained analysis of model capabilities across domains. By framing the task as long-form generation and leveraging the official marking scheme, it does not only support a comprehensive evaluation of correctness but also language fidelity. Our extensive experiments of leading closed-source and open-source LLMs reveal a persistent performance gap between English and Irish, in which models produce valid Irish responses less than 80\\% of the time, and answer correctly 55.8\\% of the time compared to 76.2\\% in English for the best-performing model. We release IRLBench (this https URL) and an accompanying evaluation codebase (this https URL) to enable future research on robust, culturally aware multilingual AI development.', 'abstract_zh': 'Recent Advances in Large Language Models in Multilingual and Low-Resource Settings: The Introduction of IRLBench', 'title_zh': 'IRLBench：一个基于多模态和文化背景的开放性LLM推理评价平行爱尔兰英语基准数据集'}
{'arxiv_id': 'arXiv:2505.13491', 'title': 'ProdRev: A DNN framework for empowering customers using generative pre-trained transformers', 'authors': 'Aakash Gupta, Nataraj Das', 'link': 'https://arxiv.org/abs/2505.13491', 'abstract': 'Following the pandemic, customers, preference for using e-commerce has accelerated. Since much information is available in multiple reviews (sometimes running in thousands) for a single product, it can create decision paralysis for the buyer. This scenario disempowers the consumer, who cannot be expected to go over so many reviews since its time consuming and can confuse them. Various commercial tools are available, that use a scoring mechanism to arrive at an adjusted score. It can alert the user to potential review manipulations. This paper proposes a framework that fine-tunes a generative pre-trained transformer to understand these reviews better. Furthermore, using "common-sense" to make better decisions. These models have more than 13 billion parameters. To fine-tune the model for our requirement, we use the curie engine from generative pre-trained transformer (GPT3). By using generative models, we are introducing abstractive summarization. Instead of using a simple extractive method of summarizing the reviews. This brings out the true relationship between the reviews and not simply copy-paste. This introduces an element of "common sense" for the user and helps them to quickly make the right decisions. The user is provided the pros and cons of the processed reviews. Thus the user/customer can take their own decisions.', 'abstract_zh': '疫情之后，电子商务使用偏好加速增长：基于生成预训练变换器的综合评价框架与常识辅助决策', 'title_zh': 'ProdRev: 一种基于生成预训练变换器的深度神经网络框架，赋能客户'}
{'arxiv_id': 'arXiv:2505.13480', 'title': 'Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale', 'authors': 'Avinash Patil, Siru Tao, Amardeep Gedhu', 'link': 'https://arxiv.org/abs/2505.13480', 'abstract': "Suicide prevention remains a critical public health challenge. While online platforms such as Reddit's r/SuicideWatch have historically provided spaces for individuals to express suicidal thoughts and seek community support, the advent of large language models (LLMs) introduces a new paradigm-where individuals may begin disclosing ideation to AI systems instead of humans. This study evaluates the capability of LLMs to perform automated suicide risk assessment using the Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot performance of six models-including Claude, GPT, Mistral, and LLaMA-in classifying posts across a 7-point severity scale (Levels 0-6). Results indicate that Claude and GPT closely align with human annotations, while Mistral achieves the lowest ordinal prediction error. Most models exhibit ordinal sensitivity, with misclassifications typically occurring between adjacent severity levels. We further analyze confusion patterns, misclassification sources, and ethical considerations, underscoring the importance of human oversight, transparency, and cautious deployment. Full code and supplementary materials are available at this https URL.", 'abstract_zh': '自杀预防仍然是一个重要公共卫生挑战。虽然Reddit的r/SuicideWatch等在线平台历来为个体提供了一个表达自杀念头并寻求社群支持的空间，但大型语言模型（LLMs）的出现引入了一个新的范式——个体可能开始向AI系统披露念头而非人类。本研究评估了LLMs使用哥伦比亚自杀严重程度评定量表（C-SSRS）进行自动化自杀风险评估的能力。我们评估了六种模型（包括Claude、GPT、Mistral和LLaMA）在将帖子分类到7级严重程度尺度（级别0-6）上的零样本性能。结果显示，Claude和GPT与人类标注最为一致，而Mistral的等级预测误差最低。大多数模型表现出对等级的敏感性，误分类通常发生在相邻严重程度级别之间。我们进一步分析了混淆模式、误分类来源和伦理考量，强调了人类监督、透明度和谨慎部署的重要性。完整代码和补充材料请参见此[链接]。', 'title_zh': '评估 suicidality 筛查中基于推理的大型语言模型的表现——使用哥伦比亚自杀严重程度评定量表'}
{'arxiv_id': 'arXiv:2505.13453', 'title': 'Pel, A Programming Language for Orchestrating AI Agents', 'authors': 'Behnam Mohammadi', 'link': 'https://arxiv.org/abs/2505.13453', 'abstract': "The proliferation of Large Language Models (LLMs) has opened new frontiers in computing, yet controlling and orchestrating their capabilities beyond simple text generation remains a challenge. Current methods, such as function/tool calling and direct code generation, suffer from limitations in expressiveness, scalability, cost, security, and the ability to enforce fine-grained control. This paper introduces Pel, a novel programming language specifically designed to bridge this gap. Inspired by the strengths of Lisp, Elixir, Gleam, and Haskell, Pel provides a syntactically simple, homoiconic, and semantically rich platform for LLMs to express complex actions, control flow, and inter-agent communication safely and efficiently. Pel's design emphasizes a minimal, easily modifiable grammar suitable for constrained LLM generation, eliminating the need for complex sandboxing by enabling capability control at the syntax level. Key features include a powerful piping mechanism for linear composition, first-class closures enabling easy partial application and functional patterns, built-in support for natural language conditions evaluated by LLMs, and an advanced Read-Eval-Print-Loop (REPeL) with Common Lisp-style restarts and LLM-powered helper agents for automated error correction. Furthermore, Pel incorporates automatic parallelization of independent operations via static dependency analysis, crucial for performant agentic systems. We argue that Pel offers a more robust, secure, and expressive paradigm for LLM orchestration, paving the way for more sophisticated and reliable AI agentic frameworks.", 'abstract_zh': '大型语言模型（LLMs）的 proliferate 为计算打开了新的前沿，但在简单文本生成之外控制和协调其能力仍然是一项挑战。当前的方法，如功能/工具调用和直接代码生成，受限于表达力、可扩展性、成本、安全性和细粒度控制能力。本文介绍了 Pel，一种专门为此差距设计的新型编程语言。Pel 受 Lisp、Elixir、Gleam 和 Haskell 的启发，提供了一个语法简单、同源且语义丰富的平台，使大型语言模型能够安全高效地表达复杂动作、控制流和代理间通信。Pel 的设计强调了一个简洁且易于修改的语法，适用于受限的大型语言模型生成，无需复杂的沙箱，因为其语法级别提供了能力控制。其关键特性包括强大的管道机制以实现线性组合、一等闭包以实现易用的偏应用和函数模式、由大型语言模型评估的内置自然语言条件支持，以及具有 Common Lisp 风格重启动的高级读-评价-打印-循环（REPeL）和由大型语言模型驱动的辅助代理，用于自动化错误纠正。此外，Pel 通过静态依赖分析自动并行化独立操作，这对于高性能代理系统至关重要。我们认为 Pel 提供了一种更稳健、更安全和更表达的大型语言模型协调范式，为更复杂和可靠的人工智能代理框架铺平了道路。', 'title_zh': 'Pel：一种 orchestrating AI 代理的编程语言'}
{'arxiv_id': 'arXiv:2505.12392', 'title': 'SLOT: Sample-specific Language Model Optimization at Test-time', 'authors': 'Yang Hu, Xingyu Zhang, Xueji Fang, Zhiyang Chen, Xiao Wang, Huatian Zhang, Guojun Qi', 'link': 'https://arxiv.org/abs/2505.12392', 'abstract': "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a novel and parameter-efficient test-time inference approach that enhances a language model's ability to more accurately respond to individual prompts. Existing Large Language Models (LLMs) often struggle with complex instructions, leading to poor performances on those not well represented among general samples. To address this, SLOT conducts few optimization steps at test-time to update a light-weight sample-specific parameter vector. It is added to the final hidden layer before the output head, and enables efficient adaptation by caching the last layer features during per-sample optimization. By minimizing the cross-entropy loss on the input prompt only, SLOT helps the model better aligned with and follow each given instruction. In experiments, we demonstrate that our method outperforms the compared models across multiple benchmarks and LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is available at this https URL.", 'abstract_zh': '基于测试时样本特定参数优化的语言模型测试时推理方法(SLOT)', 'title_zh': '测试时样本特定语言模型优化'}
{'arxiv_id': 'arXiv:2505.12257', 'title': 'LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas', 'authors': 'Evgeny Markhasin', 'link': 'https://arxiv.org/abs/2505.12257', 'abstract': "Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. This exploratory proof-of-concept (PoC) study investigates structured LLM context conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a methodological strategy to modulate this LLM behavior at inference time. The approach is designed to enhance the reliability of readily available, general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for precise validation tasks, crucially relying only on their standard chat interfaces without API access or model modifications. To explore this methodology, we focused on validating chemical formulas within a single, complex test paper with known textual and image-based errors. Several prompting strategies were evaluated: while basic prompts proved unreliable, an approach adapting PWP structures to rigorously condition the LLM's analytical mindset appeared to improve textual error identification with both models. Notably, this method also guided Gemini 2.5 Pro to repeatedly identify a subtle image-based formula error previously overlooked during manual review, a task where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight specific LLM operational modes that impede detail-oriented validation and suggest that PWP-informed context conditioning offers a promising and highly accessible technique for developing more robust LLM-driven analytical workflows, particularly for tasks requiring meticulous error detection in scientific and technical documents. Extensive validation beyond this limited PoC is necessary to ascertain broader applicability.", 'abstract_zh': '基于持续工作流提示原理的结构化LLM上下文条件化在精确验证任务中的探索性概念证明：识别复杂科学和技术文档中的细微技术错误，尤其是那些需要多模态解释的错误（例如图像中的公式），对大型语言模型（LLMs）构成了重大的挑战，因为它们固有的纠错倾向可能掩盖不准确之处。本探索性概念证明（PoC）研究探讨了基于持续工作流提示（PWP）原则的结构化LLM上下文条件化方法，作为一种在推理时调节LLM行为的方法论策略。该方法旨在通过仅依靠标准聊天界面（无需API访问或模型修改）来增强现成的通用语言模型（具体为Gemini 2.5 Pro和ChatGPT Plus o3）的可靠性，以执行精确验证任务。为了探索这一方法论，我们集中在验证一份包含已知文本和图像错误的复杂测试论文中的化学公式。评估了多种提示策略：尽管基本提示可靠性较低，但将PWP结构适应性地用于严格调节LLM的分析思维模式的方法在两个模型中都提高了文本错误识别的准确性。值得注意的是，这种方法还引导Gemini 2.5 Pro反复识别出在人工审查中未能发现的细微图像错误，而ChatGPT Plus o3在我们的测试中未能完成这一任务。初步发现强调了特定的LLM操作模式，这些模式阻碍了详细验证，并表明基于PWP的上下文条件化提供了一种有前景且高度可访问的技术，用于开发更加稳健的LLM驱动分析工作流，尤其是在需要在科学和技术文档中进行细致错误检测的任务方面。需要广泛的验证以确认其更广泛的适用性。', 'title_zh': 'LLM上下文条件及PWP提示在化学会计的多模态验证中的应用'}
