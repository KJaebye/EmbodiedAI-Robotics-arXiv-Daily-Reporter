{'arxiv_id': 'arXiv:2508.02917', 'title': 'Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces', 'authors': 'Vebjørn Haug Kåsene, Pierre Lison', 'link': 'https://arxiv.org/abs/2508.02917', 'abstract': 'Vision-and-Language Navigation (VLN) refers to the task of enabling autonomous robots to navigate unfamiliar environments by following natural language instructions. While recent Large Vision-Language Models (LVLMs) have shown promise in this task, most current VLM systems rely on models specifically designed and optimized for navigation, leaving the potential of off-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used low-level action spaces with egocentric views and atomic actions (such as "turn left" or "move forward"), newer models tend to favor panoramic action spaces with discrete navigable viewpoints. This paper investigates (1) whether off-the-shelf LVLMs (fine-tuned without architectural modifications or simulator-based training) can effectively support VLN tasks and (2) whether such models can support both low-level and panoramic action paradigms. To this end, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the Room-to-Room (R2R) dataset and evaluate its empirical performance across both low-level and panoramic action spaces. The best resulting model achieves a 41% success rate on the R2R test set, demonstrating that while off-the-shelf LVLMs can learn to perform Vision-and-Language Navigation, they still lag behind models specifically designed for this task.', 'abstract_zh': '基于视觉-语言的导航（VLN）：探究即用型大型视觉-语言模型的支持能力', 'title_zh': '使用大规模视觉-语言模型遵循导航指令：低层级与全景动作空间的对比'}
{'arxiv_id': 'arXiv:2508.03661', 'title': 'Automated Algorithmic Discovery for Gravitational-Wave Detection Guided by LLM-Informed Evolutionary Monte Carlo Tree Search', 'authors': 'He Wang, Liang Zeng', 'link': 'https://arxiv.org/abs/2508.03661', 'abstract': "Computational scientific discovery increasingly relies on algorithms to process complex data and identify meaningful patterns - yet faces persistent challenges in gravitational-wave signal identification. While existing algorithmic approaches like matched filtering (MF) and deep neural networks (DNNs) have achieved partial success, their limitations directly stem from fundamental limitations: MF's excessive computational demands arise from its reliance on predefined theoretical waveform templates, while DNNs' black-box architectures obscure decision logic and introduce hidden biases. We propose Evolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses these limitations through systematic algorithm space exploration guided by domain-aware physical constraints. Our approach combines tree-structured search with evolutionary optimization and large language model heuristics to create interpretable algorithmic solutions. Our Evo-MCTS framework demonstrates substantial improvements, achieving a 20.2\\% improvement over state-of-the-art gravitational wave detection algorithms on the MLGWSC-1 benchmark dataset. High-performing algorithm variants consistently exceed thresholds. The framework generates human-interpretable algorithmic pathways that reveal distinct performance patterns. Beyond performance improvements, our framework discovers novel algorithmic combinations, thereby establishing a transferable methodology for automated algorithmic discovery across computational science domains.", 'abstract_zh': '计算科学发现 increasingly依赖算法处理复杂数据并识别有意义的模式 - 但在引力波信号识别方面仍面临持续挑战。现有的算法方法，如匹配滤波（MF）和深度神经网络（DNNs），虽然取得了一定成功，其局限性直接源于根本限制：MF的高计算需求源自其依赖预定义的理论波形模板，而DNNs的黑盒架构模糊了决策逻辑并引入了隐藏偏见。我们提出了一种进化蒙特卡洛树搜索（Evo-MCTS）框架，该框架通过由领域意识物理约束引导的系统算法空间探索来解决这些局限性。我们的方法结合了基于树的搜索、进化优化和大型语言模型启发式方法，以生成可解释的算法解决方案。Evo-MCTS框架在MLGWSC-1基准数据集上实现了显著改进，相比于最先进的引力波检测算法，其性能提高了20.2%。高性能的算法变体持续超过阈值。该框架生成可由人类解释的算法路径，揭示了独特的性能模式。除了性能改进，该框架还发现了新的算法组合，从而为计算科学领域内的自动化算法发现提供了可转移的方法。', 'title_zh': '基于LLM指导的演化蒙特卡洛树搜索引导的引力波检测自动化算法发现'}
{'arxiv_id': 'arXiv:2508.03622', 'title': 'Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based Evaluation Framework', 'authors': 'Jialin Li, Jinzhe Li, Gengxu Li, Yi Chang, Yuan Wu', 'link': 'https://arxiv.org/abs/2508.03622', 'abstract': 'With the advancement of code generation capabilities in large language models (LLMs), their reliance on input premises has intensified. When users provide inputs containing faulty premises, the probability of code generation hallucinations rises significantly, exposing deficiencies in their self-scrutiny capabilities. This paper proposes Faulty Premises Bench (FPBench), the first code generation evaluation framework targeting faulty premises. By systematically constructing three categories of faulty premises and integrating multi-dimensional evaluation metrics, it conducts in-depth assessments of 15 representative LLMs. The key findings are as follows: (1) Most models exhibit poor reasoning abilities and suboptimal code generation performance under faulty premises, heavily relying on explicit prompts for error detection, with limited self-scrutiny capabilities; (2) Faulty premises trigger a point of diminishing returns in resource investment, leading to blindly increasing length fails to enhance quality; (3) The three types of faulty premises respectively activate distinct defect patterns in models, revealing a triple dissociation in the cognitive mechanisms of code generation models. This study not only highlights the urgent need for LLMs to proactively verify premises in code generation but also, through the proposed FPBench framework and multi-dimensional evaluation system, provides a theoretical foundation and practical pathway for developing reliable, human-centric code generation models.', 'abstract_zh': '随着大型语言模型（LLMs）代码生成能力的进步，它们对输入前提的依赖性增强。当用户提供的输入包含错误的前提时，代码生成的幻觉概率显著增加，暴露了它们自我审视能力的缺陷。本文提出Faulty Premises Bench (FPBench)，这是首个针对错误前提的代码生成评估框架。通过系统构建三类错误前提并整合多维度评估指标，对15个代表性LLM进行深入评估。主要发现如下：（1）大多数模型在错误前提下表现出较差的推理能力和代码生成性能，高度依赖显式提示进行错误检测，自我审视能力有限；（2）错误前提导致资源投入的效果递减，盲目增加长度无法提升质量；（3）三种类型的错误前提分别激发模型中的不同缺陷模式，揭示了代码生成模型认知机制的三重分工。本研究不仅突显了LLMs在代码生成时迫切需要主动验证前提的必要性，还通过提出的FPBench框架和多维度评估系统，为开发可靠的人本中心代码生成模型提供了理论基础和实践路径。', 'title_zh': '基于错误前提的评估框架：改进大规模语言模型代码生成中的批判性思维训练'}
{'arxiv_id': 'arXiv:2508.03500', 'title': 'Error Detection and Correction for Interpretable Mathematics in Large Language Models', 'authors': 'Yijin Yang, Cristina Cornelio, Mario Leiva, Paulo Shakarian', 'link': 'https://arxiv.org/abs/2508.03500', 'abstract': 'Recent large language models (LLMs) have demonstrated the ability to perform explicit multi-step reasoning such as chain-of-thought prompting. However, their intermediate steps often contain errors that can propagate leading to inaccurate final predictions. Additionally, LLMs still struggle with hallucinations and often fail to adhere to prescribed output formats, which is particularly problematic for tasks like generating mathematical expressions or source code. This work introduces EDCIM (Error Detection and Correction for Interpretable Mathematics), a method for detecting and correcting these errors in interpretable mathematics tasks, where the model must generate the exact functional form that explicitly solve the problem (expressed in natural language) rather than a black-box solution. EDCIM uses LLMs to generate a system of equations for a given problem, followed by a symbolic error-detection framework that identifies errors and provides targeted feedback for LLM-based correction. To optimize efficiency, EDCIM integrates lightweight, open-source LLMs with more powerful proprietary models, balancing cost and accuracy. This balance is controlled by a single hyperparameter, allowing users to control the trade-off based on their cost and accuracy requirements. Experimental results across different datasets show that EDCIM significantly reduces both computational and financial costs, while maintaining, and even improving, prediction accuracy when the balance is properly configured.', 'abstract_zh': 'Recent大型语言模型（LLMs）在执行显式多步推理方面显示出了能力，例如链式思考提示。然而，它们的中间步骤中常常包含错误，这些错误可能会传播导致最终预测不准确。此外，LLMs在处理幻觉方面仍然存在困难，并且往往无法遵守指定的输出格式，这在生成数学表达式或源代码等任务中尤为成问题。本工作介绍了EDCIM（可解释数学中的错误检测与纠正），一种用于在可解释数学任务中检测和纠正这些错误的方法，其中模型必须生成明确解决给定自然语言问题的确切函数形式，而不仅仅是黑箱解决方案。EDCIM使用LLMs为给定问题生成一个方程组，随后使用一个符号错误检测框架来识别错误并为目标修正提供针对性反馈。为了提高效率，EDCIM将轻量级的开源LLMs与更强大的专有模型结合，平衡成本和准确性。这种平衡由单一的超参数控制，允许用户根据成本和准确性的要求进行权衡。跨不同数据集的实验结果表明，在正确配置平衡的情况下，EDCIM显著降低了计算和财务成本，同时维持甚至提高了预测准确性。', 'title_zh': '可解释数学错误检测与纠正'}
{'arxiv_id': 'arXiv:2508.03484', 'title': 'Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes', 'authors': 'Zhiyao Xu, Dan Zhao, Qingsong Zou, Qing Li, Yong Jiang, Yuhang Wang, Jingyu Xiao', 'link': 'https://arxiv.org/abs/2508.03484', 'abstract': 'As smart homes become increasingly prevalent, intelligent models are widely used for tasks such as anomaly detection and behavior prediction. These models are typically trained on static datasets, making them brittle to behavioral drift caused by seasonal changes, lifestyle shifts, or evolving routines. However, collecting new behavior data for retraining is often impractical due to its slow pace, high cost, and privacy concerns. In this paper, we propose SmartGen, an LLM-based framework that synthesizes context-aware user behavior data to support continual adaptation of downstream smart home models. SmartGen consists of four key components. First, we design a Time and Semantic-aware Split module to divide long behavior sequences into manageable, semantically coherent subsequences under dual time-span constraints. Second, we propose Semantic-aware Sequence Compression to reduce input length while preserving representative semantics by clustering behavior mapping in latent space. Third, we introduce Graph-guided Sequence Synthesis, which constructs a behavior relationship graph and encodes frequent transitions into prompts, guiding the LLM to generate data aligned with contextual changes while retaining core behavior patterns. Finally, we design a Two-stage Outlier Filter to identify and remove implausible or semantically inconsistent outputs, aiming to improve the factual coherence and behavioral validity of the generated sequences. Experiments on three real-world datasets demonstrate that SmartGen significantly enhances model performance on anomaly detection and behavior prediction tasks under behavioral drift, with anomaly detection improving by 85.43% and behavior prediction by 70.51% on average. The code is available at this https URL.', 'abstract_zh': '基于LLM的自适应智能家居行为数据合成框架SmartGen', 'title_zh': '基于语义的图引导行为序列生成：大型语言模型在智能家居中的应用'}
{'arxiv_id': 'arXiv:2508.03438', 'title': 'Data Overdose? Time for a Quadruple Shot: Knowledge Graph Construction using Enhanced Triple Extraction', 'authors': 'Taine J. Elliott, Stephen P. Levitt, Ken Nixon, Martin Bekker', 'link': 'https://arxiv.org/abs/2508.03438', 'abstract': "The rapid expansion of publicly-available medical data presents a challenge for clinicians and researchers alike, increasing the gap between the volume of scientific literature and its applications. The steady growth of studies and findings overwhelms medical professionals at large, hindering their ability to systematically review and understand the latest knowledge. This paper presents an approach to information extraction and automatic knowledge graph (KG) generation to identify and connect biomedical knowledge. Through a pipeline of large language model (LLM) agents, the system decomposes 44 PubMed abstracts into semantically meaningful proposition sentences and extracts KG triples from these sentences. The triples are enhanced using a combination of open domain and ontology-based information extraction methodologies to incorporate ontological categories. On top of this, a context variable is included during extraction to allow the triple to stand on its own - thereby becoming `quadruples'. The extraction accuracy of the LLM is validated by comparing natural language sentences generated from the enhanced triples to the original propositions, achieving an average cosine similarity of 0.874. The similarity for generated sentences of enhanced triples were compared with generated sentences of ordinary triples showing an increase as a result of the context variable. Furthermore, this research explores the ability for LLMs to infer new relationships and connect clusters in the knowledge base of the knowledge graph. This approach leads the way to provide medical practitioners with a centralised, updated in real-time, and sustainable knowledge source, and may be the foundation of similar gains in a wide variety of fields.", 'abstract_zh': '公共可用医疗数据的迅速扩张对临床医生和研究人员构成了挑战，增加了科学文献数量与应用之间的差距。医学专业人员面临着大量的研究和发现的增长，这使他们难以系统地回顾和理解最新的知识。本文提出了一种信息提取和自动知识图谱（KG）生成的方法，用于识别和连接生物医学知识。通过大型语言模型（LLM）代理的管道，系统将44篇PubMed摘要分解为语义上有意义的命题句，并从这些句子中提取KG三元组。三元组通过结合开放领域和本体导向的信息提取方法进行增强，以纳入本体类别。在此基础上，提取过程中引入了上下文变量，使三元组能够自立——从而成为四元组。通过将增强三元组生成的自然语言句子与原始命题进行比较，验证了LLM的提取准确性，平均余弦相似度达到0.874。与普通三元组生成的句子相比，增强三元组生成的句子的相似度有所提高，这是由于上下文变量的存在。此外，这项研究探讨了LLM推断新关系和连接知识图谱知识库中簇的能力。这种方法为医疗从业人员提供了集中、实时更新和可持续的知识来源，并可能成为其他众多领域类似收益的基础。', 'title_zh': '数据过载？是时候来一杯四倍浓缩：基于增强三元组抽取的知识图构建'}
{'arxiv_id': 'arXiv:2508.03406', 'title': 'Multi-Objective Infeasibility Diagnosis for Routing Problems Using Large Language Models', 'authors': 'Kai Li, Ruihao Zheng, Xinye Hao, Zhenkun Wang', 'link': 'https://arxiv.org/abs/2508.03406', 'abstract': 'In real-world routing problems, users often propose conflicting or unreasonable requirements, which result in infeasible optimization models due to overly restrictive or contradictory constraints, leading to an empty feasible solution set. Existing Large Language Model (LLM)-based methods attempt to diagnose infeasible models, but modifying such models often involves multiple potential adjustments that these methods do not consider. To fill this gap, we introduce Multi-Objective Infeasibility Diagnosis (MOID), which combines LLM agents and multi-objective optimization within an automatic routing solver, to provide a set of representative actionable suggestions. Specifically, MOID employs multi-objective optimization to consider both path cost and constraint violation, generating a set of trade-off solutions, each encompassing varying degrees of model adjustments. To extract practical insights from these solutions, MOID utilizes LLM agents to generate a solution analysis function for the infeasible model. This function analyzes these distinct solutions to diagnose the original infeasible model, providing users with diverse diagnostic insights and suggestions. Finally, we compare MOID with several LLM-based methods on 50 types of infeasible routing problems. The results indicate that MOID automatically generates multiple diagnostic suggestions in a single run, providing more practical insights for restoring model feasibility and decision-making compared to existing methods.', 'abstract_zh': '多 Multi-Objective In In In In Diagnosis (MOID)，结合 LLMs and multi-objective optimization within an automatic suggestion solver，提供一系列具有代表性的可建议可实际可建议。', 'title_zh': '使用大型语言模型的多目标可行性和诊断研究用于路由问题'}
{'arxiv_id': 'arXiv:2508.03396', 'title': 'Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and Self-Improving Diagnosis', 'authors': 'Rui Zou, Mengqi Wei, Yutao Zhu, Jirong Wen, Xin Zhao, Jing Chen', 'link': 'https://arxiv.org/abs/2508.03396', 'abstract': 'Large Language Models (LLMs) excel in reasoning and generation across domains, but still struggle with identifying and diagnosing complex errors. This stems mainly from training objectives that prioritize correct answers, limiting exposure to and learning from errors. While recent studies have begun to address this by introducing error signals, most rely on shallow, static errors, restricting improvement in deep diagnostic ability. To overcome this, we propose Hide and Seek Game (HSG), a dynamic adversarial framework for error generation and diagnosis, and evaluate it on mathematical problem-solving. HSG involves two adversarial roles: Sneaky, which "hides" by generating subtle, deceptive reasoning errors, and Diagnosis, which "seeks" to accurately detect them. Through adversarial co-evolution, both error stealth and diagnostic precision are enhanced. Experiments on several math reasoning tasks show that HSG significantly boosts error diagnosis, achieving 16.8\\%--31.4\\% higher accuracy than baselines like GPT-4o. We also release a challenging dataset of deceptive errors and diagnostic annotations as a benchmark for future research.', 'abstract_zh': '大型语言模型在推理和生成方面表现出色，但在识别和诊断复杂错误方面仍然存在问题。这主要源于训练目标更侧重于正确答案，限制了对错误的暴露和学习。虽然近期的研究已经开始通过引入错误信号来应对这一问题，但大多数方法依赖于浅层和静态的错误，限制了在深度诊断能力上的改进。为克服这一问题，我们提出了一种动态对抗框架Hide and Seek Game (HSG)，用于错误生成和诊断，并在数学问题解决领域进行了评估。HSG 包括两个对抗角色：Sneaky 通过生成微妙且具有欺骗性的推理错误来“隐藏”，而 Diagnosis 通过准确检测这些错误来“寻找”。通过对抗协同演化，错误的隐匿性和诊断的精确性都得到了提升。在多个数学推理任务上的实验表明，HSG 显著提升了错误诊断能力，比基线模型（如 GPT-4o）的准确性高出 16.8% 至 31.4%。我们还开源了一个包含欺骗性错误和诊断注释的具有挑战性的数据集，以供未来研究使用。', 'title_zh': '使用大规模语言模型的藏与找游戏：一种隐蔽错误生成与自我提升诊断的对抗性游戏'}
{'arxiv_id': 'arXiv:2508.03379', 'title': 'Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams', 'authors': 'Wenxin Mao, Zhitao Wang Long Wang, Sirong Chen, Cuiyun Gao, Luyang Cao, Ziming Liu, Qiming Zhang, Jun Zhou, Zhi Jin', 'link': 'https://arxiv.org/abs/2508.03379', 'abstract': "Large language models (LLMs) excel at generating code from natural language (NL) descriptions. However, the plain textual descriptions are inherently ambiguous and often fail to capture complex requirements like intricate system behaviors, conditional logic, and architectural constraints; implicit data dependencies in service-oriented architectures are difficult to infer and handle correctly. To bridge this gap, we propose a novel step-by-step code generation framework named UML2Dep by leveraging unambiguous formal specifications of complex requirements. First, we introduce an enhanced Unified Modeling Language (UML) sequence diagram tailored for service-oriented architectures. This diagram extends traditional visual syntax by integrating decision tables and API specifications, explicitly formalizing structural relationships and business logic flows in service interactions to rigorously eliminate linguistic ambiguity. Second, recognizing the critical role of data flow, we introduce a dedicated data dependency inference (DDI) task. DDI systematically constructs an explicit data dependency graph prior to actual code synthesis. To ensure reliability, we formalize DDI as a constrained mathematical reasoning task through novel prompting strategies, aligning with LLMs' excellent mathematical strengths. Additional static parsing and dependency pruning further reduce context complexity and cognitive load associated with intricate specifications, thereby enhancing reasoning accuracy and efficiency.", 'abstract_zh': '大规模语言模型通过自然语言描述生成代码方面表现出色。然而，简单的文本描述本质上存在歧义，往往无法捕捉到复杂的要求，如精细的系统行为、条件逻辑和架构约束；服务导向架构中的隐式数据依赖难以正确推断和处理。为进一步弥合这一差距，我们提出了一种名为UML2Dep的新型逐步代码生成框架，利用复杂的规范要求的无歧义形式化描述。首先，我们引入了一种增强的统一建模语言（UML）序列图，适用于服务导向架构。该图通过整合决策表和API规范，扩展了传统可视化语法，明确形式化了服务交互中的结构关系和业务逻辑流程，严格消除语言歧义。其次，我们认识到数据流的至关重要性，引入了一项专门的数据依赖推断（DDI）任务。DDI系统地在实际代码合成之前构建一个显式的数据依赖图。为了确保可靠性，我们通过新颖的提示策略将DDI形式化为一个受限的数学推理任务，利用大规模语言模型的卓越数学能力。此外，静态解析和依赖修剪进一步降低了复杂规范相关的情境复杂性和认知负担，从而提高推理准确性和效率。', 'title_zh': '基于UML序列图的工业代码生成数据依赖推理'}
{'arxiv_id': 'arXiv:2508.03368', 'title': 'Board Game Arena: A Framework and Benchmark for Assessing Large Language Models via Strategic Play', 'authors': 'Lucia Cipolina-Kun, Marianna Nezhurina, Jenia Jitsev', 'link': 'https://arxiv.org/abs/2508.03368', 'abstract': 'The Board Game Arena library provides a framework for evaluating the decision making abilities of large language models (LLMs) through strategic board games implemented in Google OpenSpiel library. The framework enables systematic comparisons between LLM based agents and other agents (random, human, reinforcement learning agents, etc.) in various game scenarios by wrapping multiple board and matrix games and supporting different agent types. It integrates API access to models via LiteLLM, local model deployment via vLLM, and offers distributed execution through Ray. Additionally it provides extensive analysis tools for the LLM reasoning traces. This paper summarizes the structure, key characteristics, and motivation of the repository, highlighting how it contributes to the empirical evaluation of the reasoning of LLM and game-theoretic behavior', 'abstract_zh': 'Board Game Arena库提供了一种通过在Google OpenSpiel库中实施的战略桌面游戏来评估大型语言模型（LLMs）决策能力的框架。该框架通过包装多种桌面和矩阵游戏，并支持不同的智能体类型，使系统性的比较基于LLM的智能体与其他智能体（随机智能体、人类智能体、强化学习智能体等）在各种游戏场景中的表现成为可能。它通过LiteLLM集成模型的API访问，通过vLLM进行本地模型部署，并通过Ray实现分布式执行。此外，它还提供了广泛的分析工具来分析LLM的推理轨迹。本文总结了该库的结构、关键特性和动机，强调了它如何为LLM的推理和博弈论行为的实证评估做出贡献。', 'title_zh': 'Board Game Arena：一种通过战略游戏评估大型语言模型的框架与基准'}
{'arxiv_id': 'arXiv:2508.03360', 'title': 'CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment', 'authors': 'Feng Rui, Zhiyao Luo, Wei Wang, Yuting Song, Yong Liu, Tingting Zhu, Jianqing Li, Xingyao Wang', 'link': 'https://arxiv.org/abs/2508.03360', 'abstract': 'Automatic assessment of cognitive impairment from spontaneous speech offers a promising, non-invasive avenue for early cognitive screening. However, current approaches often lack generalizability when deployed across different languages and clinical settings, limiting their practical utility. In this study, we propose CogBench, the first benchmark designed to evaluate the cross-lingual and cross-site generalizability of large language models (LLMs) for speech-based cognitive impairment assessment. Using a unified multimodal pipeline, we evaluate model performance on three speech datasets spanning English and Mandarin: ADReSSo, NCMMSC2021-AD, and a newly collected test set, CIR-E. Our results show that conventional deep learning models degrade substantially when transferred across domains. In contrast, LLMs equipped with chain-of-thought prompting demonstrate better adaptability, though their performance remains sensitive to prompt design. Furthermore, we explore lightweight fine-tuning of LLMs via Low-Rank Adaptation (LoRA), which significantly improves generalization in target domains. These findings offer a critical step toward building clinically useful and linguistically robust speech-based cognitive assessment tools.', 'abstract_zh': '自动评估自发言语中的认知损害提供了一种有前景的无 invasive 方法，用于早期认知筛查。然而，当前的方法在应用于不同语言和临床环境时经常缺乏普遍适用性，限制了其实用价值。在本研究中，我们提出CogBench，这是首个旨在评估大规模语言模型（LLMs）在基于言语的认知损害评估中跨语言和跨场地普遍适用性的基准。我们使用统一的多模态管道，在涵盖英语和 Mandarin 的三个语音数据集上评估模型性能：ADReSSo、NCMMSC2021-AD 和一个新收集的测试集 CIR-E。研究结果表明，传统深度学习模型在跨领域迁移时性能大幅下降。相比之下，配备链式思考提示的大规模语言模型表现出更好的适应性，尽管其性能仍对提示设计敏感。此外，我们探讨了通过低秩适应（LoRA）轻量级微调大规模语言模型的方法，这在目标领域中显著提高了通用性。这些发现为构建临床实用且语言鲁棒的基于言语的认知评估工具提供了关键步骤。', 'title_zh': 'CogBench：一种基于多ilingual语音的认知障碍评估的大语言模型基准测试'}
{'arxiv_id': 'arXiv:2508.03346', 'title': 'Compressing Chain-of-Thought in LLMs via Step Entropy', 'authors': 'Zeju Li, Jianyuan Zhong, Ziyang Zheng, Xiangyu Wen, Zhijian Xu, Yingying Cheng, Fan Zhang, Qiang Xu', 'link': 'https://arxiv.org/abs/2508.03346', 'abstract': 'Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce a novel CoT compression framework based on step entropy, a metric that quantifies the informational contribution of individual reasoning steps to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80\\% of low-entropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy pruning, which severely impairs reasoning performance. Building on this, we propose a novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables LLMs to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly enhances LLM inference efficiency while rigorously preserving accuracy, offering profound implications for practical LLM deployment and a deeper understanding of reasoning structures.', 'abstract_zh': '大型语言模型（LLMs）使用链式思维（CoT）提示在复杂推理方面表现出色，但生成冗长且具有较大冗余性的思维过程，导致推理成本增加和效率降低。我们提出了一种基于步骤熵的新颖CoT压缩框架，该框架通过量化单个推理步骤的信息贡献来识别冗余性。通过对数学推理基准进行理论分析和广泛的实证验证，我们证明低熵步骤确实高度冗余。实验结果显示，在DeepSeek-R1-7B、14B和Qwen3-8B中，令人惊讶的是，多达80%的低熵中间步骤可以通过微小的精度下降得以裁剪。这一发现与随机裁剪或高熵裁剪形成鲜明对比，后两者严重损害了推理性能。在此基础上，我们提出了一种结合监督微调（SFT）和群体相对策略优化（GRPO）强化学习的新型两阶段训练策略。此方法允许LLMs在推理过程中自主学习生成压缩的CoT，通过战略性地使用[SKIP]令牌。我们的方法显著提高了LLM的推理效率，同时严格保持了准确性，为实际的LLM部署提供了深远的启示，并为理解推理结构提供了更深入的理解。', 'title_zh': '通过步骤熵压缩LLMs中的chain-of-thought'}
{'arxiv_id': 'arXiv:2508.03341', 'title': 'Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science', 'authors': 'Jiayan Nan, Wenquan Ma, Wenlong Wu, Yize Chen', 'link': 'https://arxiv.org/abs/2508.03341', 'abstract': "Large Language Models (LLMs) demonstrate remarkable capabilities, yet their inability to maintain persistent memory in long contexts limits their effectiveness as autonomous agents in long-term interactions. While existing memory systems have made progress, their reliance on arbitrary granularity for defining the basic memory unit and passive, rule-based mechanisms for knowledge extraction limits their capacity for genuine learning and evolution. To address these foundational limitations, we present Nemori, a novel self-organizing memory architecture inspired by human cognitive principles. Nemori's core innovation is twofold: First, its Two-Step Alignment Principle, inspired by Event Segmentation Theory, provides a principled, top-down method for autonomously organizing the raw conversational stream into semantically coherent episodes, solving the critical issue of memory granularity. Second, its Predict-Calibrate Principle, inspired by the Free-energy Principle, enables the agent to proactively learn from prediction gaps, moving beyond pre-defined heuristics to achieve adaptive knowledge evolution. This offers a viable path toward handling the long-term, dynamic workflows of autonomous agents. Extensive experiments on the LoCoMo and LongMemEval benchmarks demonstrate that Nemori significantly outperforms prior state-of-the-art systems, with its advantage being particularly pronounced in longer contexts.", 'abstract_zh': '大型语言模型（LLMs）展示了 remarkable 的能力，维护持久记忆方面的能力限制了其在长期交互中的有效性，特别是在自主环境中自主代理的能力受到限制。现有记忆系统虽然取得了一定进步，但仍然依赖于任意粒度来定义基本记忆单元、以及基于规则机制的知识的进行知识提取的被动方法，限制了其真正学习和进化的潜力。为解决这些根本限制，我们提出了一种名为 Nem Nem 的架构，该架构受到人类认知原则的启发。Nem Nem的创新之处在于提出了两步对对对 原则。其中第一步是基于事件分割理论的两步对齐原则，提供了一种原理上可靠的的原机制，自主地地将原始对话流组织为语义连贯的片段，从而解决了粒度定义上的的一呆问题。第二步是借鉴自由能原则的预测原则，使代理能够主动地从预测缺口中学习，摆脱预定义的刻力。这为解决自主代理的长期动态工作问题提供了一个可能的解决途径。在 LoCoMo 和 LongMemEval 的基准测试中，Nem Nem的效果优于先前最先进技术，特别是在某些语境中效果尤为显著。', 'title_zh': 'Nemori: 受认知科学启发的自组织代理记忆'}
{'arxiv_id': 'arXiv:2508.03174', 'title': 'InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation', 'authors': 'Tian-Fang Zhao, Wen-Xi Yang', 'link': 'https://arxiv.org/abs/2508.03174', 'abstract': 'Collaborative partnership matters in inquiry-oriented education. However, most study partners are selected either rely on experience-based assignments with little scientific planning or build on rule-based machine assistants, encountering difficulties in knowledge expansion and inadequate flexibility. This paper proposes an LLM-empowered agent model for simulating and selecting learning partners tailored to inquiry-oriented learning, named InqEduAgent. Generative agents are designed to capture cognitive and evaluative features of learners in real-world scenarios. Then, an adaptive matching algorithm with Gaussian process augmentation is formulated to identify patterns within prior knowledge. Optimal learning-partner matches are provided for learners facing different exercises. The experimental results show the optimal performance of InqEduAgent in most knowledge-learning scenarios and LLM environment with different levels of capabilities. This study promotes the intelligent allocation of human-based learning partners and the formulation of AI-based learning partners. The code, data, and appendix are publicly available at this https URL.', 'abstract_zh': '基于生成式代理的问询导向教育学习伴侶模拟与选择模型：InqEduAgent', 'title_zh': 'InqEduAgent：具有高斯过程增强的自适应AI学习伙伴'}
{'arxiv_id': 'arXiv:2508.03117', 'title': 'Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation', 'authors': 'Vinicius Lima, Dzung T. Phan, Jayant Kalagnanam, Dhaval Patel, Nianjun Zhou', 'link': 'https://arxiv.org/abs/2508.03117', 'abstract': 'We present a framework for training trustworthy large language model (LLM) agents for optimization modeling via a verifiable synthetic data generation pipeline. Focusing on linear and mixed-integer linear programming, our approach begins with structured symbolic representations and systematically produces natural language descriptions, mathematical formulations, and solver-executable code. By programmatically constructing each instance with known optimal solutions, the pipeline ensures full verifiability and enables automatic filtering of low-quality demonstrations generated by teacher models. Each dataset instance includes a structured representation of the optimization problem, a corresponding natural language description, the verified optimal solution, and step-by-step demonstrations - generated by a teacher model - that show how to model and solve the problem across multiple optimization modeling languages. This enables supervised fine-tuning of open-source LLMs specifically tailored to optimization tasks. To operationalize this pipeline, we introduce OptiTrust, a modular LLM agent that performs multi-stage translation from natural language to solver-ready code, leveraging stepwise demonstrations, multi-language inference, and majority-vote cross-validation. Our agent achieves state-of-the-art performance on standard benchmarks. Out of 7 datasets, it achieves the highest accuracy on six and outperforms the next-best algorithm by at least 8 percentage on three of them. Our approach provides a scalable, verifiable, and principled path toward building reliable LLM agents for real-world optimization applications.', 'abstract_zh': '一种基于可验证合成数据生成管道训练可信赖的大规模语言模型代理的方法：面向优化建模的线性与混合整数线性规划', 'title_zh': '基于可验证合成数据生成的可信优化建模代理'}
{'arxiv_id': 'arXiv:2508.03109', 'title': 'AgentSME for Simulating Diverse Communication Modes in Smart Education', 'authors': 'Wen-Xi Yang, Tian-Fang Zhao', 'link': 'https://arxiv.org/abs/2508.03109', 'abstract': 'Generative agent models specifically tailored for smart education are critical, yet remain relatively underdeveloped. A key challenge stems from the inherent complexity of educational contexts: learners are human beings with various cognitive behaviors, and pedagogy is fundamentally centered on personalized human-to-human communication. To address this issue, this paper proposes AgentSME, a unified generative agent framework powered by LLM. Three directional communication modes are considered in the models, namely Solo, Mono, and Echo, reflecting different types of agency autonomy and communicative reciprocity. Accuracy is adopted as the primary evaluation metric, complemented by three diversity indices designed to assess the diversity of reasoning contents. Six widely used LLMs are tested to validate the robustness of communication modes across different model tiers, which are equally divided into base-capacity and high-capacity configurations. The results show that generative agents that employ the Echo communication mode achieve the highest accuracy scores, while DeepSeek exhibits the greatest diversity. This study provides valuable information to improve agent learning capabilities and inspire smart education models.', 'abstract_zh': '生成代理模型特别针对智能教育的具体定制仍然相对未发展完善 décoefficient。这些问题的关键挑战源自教育情境固有的复杂性：学习者是具有多种认知行为的人类演员，，并，而教学方法本质上是基于个性化的人对人对人对交流的基础之上解决这一问题的基础框架上，作者提出了一个统一的生成生成代理框架， powered by 大语言模型LLM。模型中考虑了三种方向交流模式，，， Three directional communication modes are considered in the 模型的结构parameters 在这三个模式中，分别是Solo、Mono和Echo，，分别反映了不同的代理自主性和互动互惠性rec reciprocity。准确性是主要评估指标 primary evaluation metric，用补充了三个多样性指数用于评估推理内容的多样性多样性 indices。在广泛测试的六个大型语言模型LLM的基础上进行验证，这些模型被等 equally divided 遝分为高容和强容两种配置设计哪大模型配置。研究结果表明应用Echo模式的生成代理展现最高准确性scores铰链维，则DeepSeek表现最优多样性。本研究提供了关于提升代理生成能力的宝贵的见解，并并激励智能教育模型的发展和创新。', 'title_zh': 'AgentSMEfor模拟智能教育中多样的通信模式'}
{'arxiv_id': 'arXiv:2508.03092', 'title': 'Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework', 'authors': 'Zikun Cui, Tianyi Huang, Chia-En Chiang, Cuiqianhe Du', 'link': 'https://arxiv.org/abs/2508.03092', 'abstract': 'With the proliferation of Large Language Models (LLMs), the detection of misinformation has become increasingly important and complex. This research proposes an innovative verifiable misinformation detection LLM agent that goes beyond traditional true/false binary judgments. The agent actively verifies claims through dynamic interaction with diverse web sources, assesses information source credibility, synthesizes evidence, and provides a complete verifiable reasoning process. Our designed agent architecture includes three core tools: precise web search tool, source credibility assessment tool and numerical claim verification tool. These tools enable the agent to execute multi-step verification strategies, maintain evidence logs, and form comprehensive assessment conclusions. We evaluate using standard misinformation datasets such as FakeNewsNet, comparing with traditional machine learning models and LLMs. Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content. Experimental results show that our agent outperforms baseline methods in misinformation detection accuracy, reasoning transparency, and resistance to information rewriting, providing a new paradigm for trustworthy AI-assisted fact-checking.', 'abstract_zh': '随着大型语言模型（LLMs）的普及，虚假信息检测变得 increasingly重要且复杂。本文提出了一种创新的可验证虚假信息检测LLM代理，该代理超越了传统的真/假二分判断。该代理通过与多样化网络源的动态交互来主动验证主张，评估信息源的可信度，合成证据，并提供完整的可验证推理过程。我们设计的代理架构包括三个核心工具：精确网络搜索工具、来源可信度评估工具和数值主张验证工具。这些工具使代理能够执行多步验证策略、维护证据日志，并形成综合评估结论。我们使用标准虚假信息数据集（如FakeNewsNet）进行评估，并与传统机器学习模型和LLMs进行对比。评估指标包括标准分类指标、推理过程质量评估以及对重写内容的鲁棒性测试。实验结果表明，我们的代理在虚假信息检测准确性、推理透明度和对抗信息重写方面的性能优于基线方法，为可信的AI辅助事实核查提供了一个新的范式。', 'title_zh': '可验证虚假信息检测的多工具大语言模型代理框架'}
{'arxiv_id': 'arXiv:2508.03082', 'title': 'EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design', 'authors': 'Fei Liu, Yilu Liu, Qingfu Zhang, Xialiang Tong, Mingxuan Yuan', 'link': 'https://arxiv.org/abs/2508.03082', 'abstract': 'Automated Heuristic Design (AHD) using Large Language Models (LLMs) has achieved notable success in recent years. Despite the effectiveness of existing approaches, they only design a single heuristic to serve all problem instances, often inducing poor generalization across different distributions or settings. To address this issue, we propose Automated Heuristic Set Design (AHSD), a new formulation for LLM-driven AHD. The aim of AHSD is to automatically generate a small-sized complementary heuristic set to serve diverse problem instances, such that each problem instance could be optimized by at least one heuristic in this set. We show that the objective function of AHSD is monotone and supermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the AHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary population management and complementary-aware memetic search, EoH-S could effectively generate a set of high-quality and complementary heuristics. Comprehensive experimental results on three AHD tasks with diverse instances spanning various sizes and distributions demonstrate that EoH-S consistently outperforms existing state-of-the-art AHD methods and achieves up to 60\\% performance improvements.', 'abstract_zh': '使用大型语言模型（LLM）的自动启发式设计（AHD）在近年来取得了显著的成功。尽管现有方法的有效性，它们只为所有问题实例设计单一启发式方法，这在不同分布或设置中常常导致差的泛化能力。为了解决这一问题，我们提出了自动启发式集设计（AHSD），这是一种新的LLM驱动的AHD的表述方法。AHSD的目标是自动生成一个小规模的互补启发式集，以服务于多样化的不同问题实例，使得集中的每个启发式都能优化至少一个问题实例。我们证明了AHSD的目标函数是单调且超级模的。然后，我们提出了进化启发式集（EoH-S）来应用AHSD表述方法进行LLM驱动的AHD。通过两种新颖的互补人群管理和互补感知的元搜索机制，EoH-S能够有效地生成一组高质量且互补的启发式。针对三个涉及不同规模和分布多样实例的AHD任务的全面实验结果表明，EoH-S始终优于现有的最先进的AHD方法，并实现了高达60%的性能提升。', 'title_zh': '基于大语言模型的启发式集演化方法实现自动化启发式设计（EoH-S）'}
{'arxiv_id': 'arXiv:2508.03080', 'title': 'ContractEval: Benchmarking LLMs for Clause-Level Legal Risk Identification in Commercial Contracts', 'authors': 'Shuang Liu, Zelong Li, Ruoyun Ma, Haiyan Zhao, Mengnan Du', 'link': 'https://arxiv.org/abs/2508.03080', 'abstract': 'The potential of large language models (LLMs) in specialized domains such as legal risk analysis remains underexplored. In response to growing interest in locally deploying open-source LLMs for legal tasks while preserving data confidentiality, this paper introduces ContractEval, the first benchmark to thoroughly evaluate whether open-source LLMs could match proprietary LLMs in identifying clause-level legal risks in commercial contracts. Using the Contract Understanding Atticus Dataset (CUAD), we assess 4 proprietary and 15 open-source LLMs. Our results highlight five key findings: (1) Proprietary models outperform open-source models in both correctness and output effectiveness, though some open-source models are competitive in certain specific dimensions. (2) Larger open-source models generally perform better, though the improvement slows down as models get bigger. (3) Reasoning ("thinking") mode improves output effectiveness but reduces correctness, likely due to over-complicating simpler tasks. (4) Open-source models generate "no related clause" responses more frequently even when relevant clauses are present. This suggests "laziness" in thinking or low confidence in extracting relevant content. (5) Model quantization speeds up inference but at the cost of performance drop, showing the tradeoff between efficiency and accuracy. These findings suggest that while most LLMs perform at a level comparable to junior legal assistants, open-source models require targeted fine-tuning to ensure correctness and effectiveness in high-stakes legal settings. ContractEval offers a solid benchmark to guide future development of legal-domain LLMs.', 'abstract_zh': '大型语言模型在法律风险分析等专业化领域中的潜力尚未充分探索。为应对将开源大型语言模型（LLMs）在法律领域本地部署以保护数据保密性的增长需求，本文引入了ContractEval，这是首个全面评估开源LLMs是否能在商业合同的条款级别上识别法律风险以匹配专有LLMs的基准。利用Contract Understanding Atticus Dataset (CUAD)，我们评估了4个专有和15个开源LLMs。研究结果突显了五个关键发现：（1）专有模型在准确性和输出效果上均优于开源模型，尽管某些开源模型在特定维度上表现竞争力强。（2）更大规模的开源模型通常表现更好，但随模型增大，改进效果逐渐减缓。（3）推理（“思考”）模式提高了输出效果但降低了准确性，这可能是由于过度复杂化简单的任务所致。（4）即使存在相关条款，开源模型更频繁地生成“无相关条款”的响应，这表明“懒惰”的思考或提取相关内容时低自信。（5）模型量化加速了推理但代价是性能下降，展示了效率与精度之间的权衡。这些发现表明，尽管大多数LLMs的表现与初级法律助理相当，但开源模型需要有针对性的微调以确保在高风险法律环境中的准确性和有效性。ContractEval提供了一个坚实的基准，以指导未来法律领域LLMs的发展。', 'title_zh': 'ContractEval：评估大型语言模型在商业合同条款级法律风险识别方面的性能'}
{'arxiv_id': 'arXiv:2508.03054', 'title': 'Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning', 'authors': 'Rui Pu, Chaozhuo Li, Rui Ha, Litian Zhang, Lirong Qiu, Xi Zhang', 'link': 'https://arxiv.org/abs/2508.03054', 'abstract': 'Defending large language models (LLMs) against jailbreak attacks is essential for their safe and reliable deployment. Existing defenses often rely on shallow pattern matching, which struggles to generalize to novel and unseen attack strategies. To address this challenge, we propose the Cognitive-Driven Defense (CDD) framework, which targets the underlying structure of jailbreak prompts by applying meta-operations, defined as basic manipulations that conceal harmful this http URL emulates human cognitive reasoning through a structured reasoning chain. It begins with a global perception of the prompt and follows with a localized analysis to uncover hidden manipulations. By applying supervised fine-tuning on this structured chain, the model learns to identify and reason about known manipulation patterns. To enhance generalization to unseen threats, an entropy-guided reinforcement learning algorithm (EG-GRPO) is introduced to encourage exploration of new types and variants of meta-operations. Experiments demonstrate that CDD can achieve state-of-the-art defense performance and exhibit strong generalization to unseen jailbreak attacks.', 'abstract_zh': '防御大型语言模型（LLMs）免受越狱攻击对于其安全可靠的部署至关重要。现有的防御方法往往依赖于浅层模式匹配，难以泛化到新型且未见过的攻击策略。为应对这一挑战，我们提出了认知驱动防御（CDD）框架，该框架通过应用基于元操作的结构化推理链来针对越狱提示的基本结构。CDD 通过全局感知提示并进行局部分析来揭示隐藏的操纵，从而模拟人类的认知推理过程。通过在该结构化链上进行监督微调，模型能够学习识别和推理已知的操纵模式。为了增强对未见过的威胁的泛化能力，引入了一种基于熵的强化学习算法（EG-GRPO），以鼓励探索新的元操作类型和变体。实验表明，CDD 可以实现最先进的防御性能，并在应对未见过的越狱攻击方面表现出强大的泛化能力。', 'title_zh': '超越表层检测：通过元操作推理实现认知驱动的对抗 Jailbreak 攻击防御'}
{'arxiv_id': 'arXiv:2508.03031', 'title': 'From Text to Trajectories: GPT-2 as an ODE Solver via In-Context', 'authors': 'Ziyang Ma, Baojian Zhou, Deqing Yang, Yanghua Xiao', 'link': 'https://arxiv.org/abs/2508.03031', 'abstract': 'In-Context Learning (ICL) has emerged as a new paradigm in large language models (LLMs), enabling them to perform novel tasks by conditioning on a few examples embedded in the prompt. Yet, the highly nonlinear behavior of ICL for NLP tasks remains poorly understood. To shed light on its underlying mechanisms, this paper investigates whether LLMs can solve ordinary differential equations (ODEs) under the ICL setting. We formulate standard ODE problems and their solutions as sequential prompts and evaluate GPT-2 models on these tasks. Experiments on two types of ODEs show that GPT-2 can effectively learn a meta-ODE algorithm, with convergence behavior comparable to, or better than, the Euler method, and achieve exponential accuracy gains with increasing numbers of demonstrations. Moreover, the model generalizes to out-of-distribution (OOD) problems, demonstrating robust extrapolation capabilities. These empirical findings provide new insights into the mechanisms of ICL in NLP and its potential for solving nonlinear numerical problems.', 'abstract_zh': '基于上下文学习(ICL)在大规模语言模型(LLMs)中作为一种新范式出现，使它们能够在提示中嵌入少量示例后执行新颖任务。然而，对于自然语言处理(NLP)任务中ICL的高度非线性行为仍缺乏深入了解。为了揭示其内在机制，本文研究了在基于上下文学习条件下，大规模语言模型能否求解常微分方程(ODEs)。我们将标准的ODE问题及其解形式化为顺序提示，并在这些任务上评估GPT-2模型。对于两类ODE的实验表明，GPT-2可以有效地学习元ODE算法，其收敛行为与欧拉方法相当，甚至更好，并且随着示范次数的增加，可以实现指数级的准确度提升。此外，该模型在新的分布外(OOD)问题上表现出泛化能力，展示了其稳健的外推能力。这些实验证据为ICL在NLP中的机制提供了新的见解，并且指出了其解决非线性数值问题的潜力。', 'title_zh': '从文本到轨迹：通过上下文学习将GPT-2视为ODE求解器'}
{'arxiv_id': 'arXiv:2508.02999', 'title': 'AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots', 'authors': 'Xinjie Zhao, Moritz Blum, Fan Gao, Yingjian Chen, Boming Yang, Luis Marquez-Carpintero, Mónica Pina-Navarro, Yanran Fu, So Morikawa, Yusuke Iwasawa, Yutaka Matsuo, Chanjun Park, Irene Li', 'link': 'https://arxiv.org/abs/2508.02999', 'abstract': 'AGENTiGraph is a user-friendly, agent-driven system that enables intuitive interaction and management of domain-specific data through the manipulation of knowledge graphs in natural language. It gives non-technical users a complete, visual solution to incrementally build and refine their knowledge bases, allowing multi-round dialogues and dynamic updates without specialized query languages. The flexible design of AGENTiGraph, including intent classification, task planning, and automatic knowledge integration, ensures seamless reasoning between diverse tasks. Evaluated on a 3,500-query benchmark within an educational scenario, the system outperforms strong zero-shot baselines (achieving 95.12% classification accuracy, 90.45% execution success), indicating potential scalability to compliance-critical or multi-step queries in legal and medical domains, e.g., incorporating new statutes or research on the fly. Our open-source demo offers a powerful new paradigm for multi-turn enterprise knowledge management that bridges LLMs and structured graphs.', 'abstract_zh': 'AGENTiGraph是一种用户友好的、基于代理的系统，通过自然语言操作知识图谱，实现领域特定数据的直观交互和管理。该系统为非技术人员提供了一个完整的视觉解决方案，使其能够逐步构建和精炼知识库，并支持多轮对话和动态更新，无需专门的查询语言。AGENTiGraph的灵活设计，包括意图分类、任务规划和自动知识整合，确保了多种任务之间的无缝推理。在教育场景下的3,500查询基准测试中，该系统优于强大的零样本基线（分类准确率为95.12%，执行成功率为90.45%），这表明其可能扩展到诸如法律和医疗领域等合规关键或多步查询中，例如实时引入新法规或进行研究。我们的开源演示为LLM和结构化图谱之间的企业知识管理提供了一种新的强大范式。', 'title_zh': 'AGENTiGraph：一种用于交互式、领域特定大语言模型聊天机器人的多Agent知识图谱框架'}
{'arxiv_id': 'arXiv:2508.02994', 'title': 'When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs', 'authors': 'Fangyi Yu', 'link': 'https://arxiv.org/abs/2508.02994', 'abstract': 'As large language models (LLMs) grow in capability and autonomy, evaluating their outputs-especially in open-ended and complex tasks-has become a critical bottleneck. A new paradigm is emerging: using AI agents as the evaluators themselves. This "agent-as-a-judge" approach leverages the reasoning and perspective-taking abilities of LLMs to assess the quality and safety of other models, promising calable and nuanced alternatives to human evaluation. In this review, we define the agent-as-a-judge concept, trace its evolution from single-model judges to dynamic multi-agent debate frameworks, and critically examine their strengths and shortcomings. We compare these approaches across reliability, cost, and human alignment, and survey real-world deployments in domains such as medicine, law, finance, and education. Finally, we highlight pressing challenges-including bias, robustness, and meta evaluation-and outline future research directions. By bringing together these strands, our review demonstrates how agent-based judging can complement (but not replace) human oversight, marking a step toward trustworthy, scalable evaluation for next-generation LLMs.', 'abstract_zh': '随着大型语言模型（LLMs）能力与自主性的增强，评估其输出特别是在开放性和复杂任务中的评估已成为一个关键瓶颈。一种新的范式正在出现：使用AI代理作为评价者本身。这种“代理作为法官”的方法利用LLMs的推理和换位思考能力，评估其他模型的质量和安全性，提供了一种可行且细腻的人类评价替代方案。在这篇综述中，我们定义了代理作为法官的概念，追溯了从单模型法官到动态多代理辩论框架的发展过程，并对其优点和不足进行了批判性分析。我们比较了这些方法在可靠性、成本和人类一致性方面的差异，并调研了医疗、法律、金融和教育等领域的真实部署。最后，我们强调了紧迫的挑战，包括偏见、稳健性和元评估，并概述了未来的研究方向。通过将这些线索结合起来，我们的综述展示了基于代理的裁决如何（但不会取代）人类监督，标志着朝着可信和可扩展的大规模语言模型评估迈进的一步。', 'title_zh': '当AI评判AI：代理作为法官评估LLMs的研究'}
{'arxiv_id': 'arXiv:2508.02979', 'title': 'Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling', 'authors': 'Peng Ding, Rick Stevens', 'link': 'https://arxiv.org/abs/2508.02979', 'abstract': 'The proliferation of tool-augmented Large Language Models (LLMs) has created a fragmented ecosystem where developers must navigate multiple protocols, manual schema definitions, and complex execution workflows. We address this challenge by proposing a unified approach to tool integration that abstracts protocol differences while optimizing execution performance. Our solution demonstrates how protocol-agnostic design principles can significantly reduce development overhead through automated schema generation, dual-mode concurrent execution, and seamless multi-source tool management. Experimental results show 60-80% code reduction across integration scenarios, performance improvements up to 3.1x through optimized concurrency, and full compatibility with existing function calling standards. This work contributes both theoretical insights into tool integration architecture and practical solutions for real-world LLM application development.', 'abstract_zh': '工具增强大型语言模型的普及创建了一个碎片化的生态系统，开发者必须导航多个协议、手工模式定义和复杂执行工作流。我们通过提出一种抽象协议差异并优化执行性能的统一工具集成方法来应对这一挑战。我们的解决方案展示了无协议依赖设计原则如何通过自动化模式生成、双模式并发执行和无缝多来源工具管理显著减少开发开销。实验结果表明，在集成场景中代码减少60-80%，通过优化的并发性能提升最高达3.1倍，并完全兼容现有函数调用标准。本工作既提供了工具集成架构的理论洞察，又提供了实际的解决方案以支持真实的大型语言模型应用开发。', 'title_zh': '统一的工具集成方法：一种与协议无关的函数调用方法'}
{'arxiv_id': 'arXiv:2508.02961', 'title': 'Defend LLMs Through Self-Consciousness', 'authors': 'Boshi Huang, Fabio Nonato de Paula', 'link': 'https://arxiv.org/abs/2508.02961', 'abstract': "This paper introduces a novel self-consciousness defense mechanism for Large Language Models (LLMs) to combat prompt injection attacks. Unlike traditional approaches that rely on external classifiers, our method leverages the LLM's inherent reasoning capabilities to perform self-protection. We propose a framework that incorporates Meta-Cognitive and Arbitration Modules, enabling LLMs to evaluate and regulate their own outputs autonomously. Our approach is evaluated on seven state-of-the-art LLMs using two datasets: AdvBench and Prompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate significant improvements in defense success rates across models and datasets, with some achieving perfect and near-perfect defense in Enhanced Mode. We also analyze the trade-off between defense success rate improvement and computational overhead. This self-consciousness method offers a lightweight, cost-effective solution for enhancing LLM ethics, particularly beneficial for GenAI use cases across various platforms.", 'abstract_zh': '一种基于元认知和仲裁模块的大规模语言模型自意识防护机制以应对提示注入攻击', 'title_zh': '通过自我意识保护大语言模型'}
{'arxiv_id': 'arXiv:2508.02959', 'title': 'Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow', 'authors': 'Chia-Tung Ho, Jing Gong, Xufeng Yao, Yunsheng Bai, Abhishek B Akkur, Haoxing Ren', 'link': 'https://arxiv.org/abs/2508.02959', 'abstract': 'Large language models (LLMs) excel at solving complex tasks by executing agentic workflows composed of detailed instructions and structured operations. Yet, building general-purpose agents by manually embedding foundation models into agentic systems such as Chain-of-Thought, Self-Reflection, and ReACT through text interfaces limits scalability and efficiency. Recently, many researchers have sought to automate the generation and optimization of these workflows through code-based representations. However, existing methods often rely on labeled datasets to train and optimize workflows, making them ineffective and inflexible for solving real-world, dynamic problems where labeled data is unavailable. To address this challenge, we introduce Polymath, a self-optimizing agent with dynamic hierarchical workflow that leverages the flexibility of task flow graphs and the expressiveness of code-represented workflows to solve a wide range of real-world, dynamic problems. The proposed optimization methodology integrates multi-grid-inspired graph optimization with a self-reflection-guided evolutionary algorithm to refine workflows without labeled data. Experimental results on six benchmark datasets across coding, math, and multi-turn QA tasks show that Polymath achieves 8.1% average improvement over state-of-the-art baselines.', 'abstract_zh': 'Polymath：一种利用动态层次工作流解决现实世界动态问题的自我优化代理', 'title_zh': '聚合智士：一种自优化的动态层次 Workflow 代理'}
{'arxiv_id': 'arXiv:2508.02951', 'title': 'MedBLINK: Probing Basic Perception in Multimodal Language Models for Medicine', 'authors': 'Mahtab Bigverdi, Wisdom Ikezogwo, Kevin Zhang, Hyewon Jeong, Mingyu Lu, Sungjae Cho, Linda Shapiro, Ranjay Krishna', 'link': 'https://arxiv.org/abs/2508.02951', 'abstract': 'Multimodal language models (MLMs) show promise for clinical decision support and diagnostic reasoning, raising the prospect of end-to-end automated medical image interpretation. However, clinicians are highly selective in adopting AI tools; a model that makes errors on seemingly simple perception tasks such as determining image orientation or identifying whether a CT scan is contrast-enhance are unlikely to be adopted for clinical tasks. We introduce Medblink, a benchmark designed to probe these models for such perceptual abilities. Medblink spans eight clinically meaningful tasks across multiple imaging modalities and anatomical regions, totaling 1,429 multiple-choice questions over 1,605 images. We evaluate 19 state-of-the-art MLMs, including general purpose (GPT4o, Claude 3.5 Sonnet) and domain specific (Med Flamingo, LLaVA Med, RadFM) models. While human annotators achieve 96.4% accuracy, the best-performing model reaches only 65%. These results show that current MLMs frequently fail at routine perceptual checks, suggesting the need to strengthen their visual grounding to support clinical adoption. Data is available on our project page.', 'abstract_zh': '多模态语言模型（MLMs）在临床决策支持和诊断推理中显示出了前景，并且开启了端到端自动化医学图像解释的可能性。然而，，临床医生在采用AI方面非常谨慎，；一项研究表明，在即使是看似简单的感知任务如如计算机断层扫描扫描是否增强也有可能无法用于临床任务。我们介绍了Medblink作为一项用于测试感知能力的基准测试模型，。Medblink涵盖了八个临床相关任务，跨越多种成像模态和解剖区域，并总共包含14,多个选择题问题和1,6055幅图像。我们评估了当代最先进的的MLMs，，包括通用语言模型（如如大的GPT4、Claudee Sonnet）和医学特定模型（Med Flamingo、La lasa Med、RadFM）。人类注释者达到了64.4% on准确率而当前MLMs仅达到了6% on on level .这表明当前的的MLMs在强化视视觉理解以支持临床采用采用采用方面需要加强。数据可在项目页面上 on获取。', 'title_zh': 'MedBLINK: 探测多模态语言模型在医学中的基本感知能力'}
{'arxiv_id': 'arXiv:2508.02921', 'title': 'PentestJudge: Judging Agent Behavior Against Operational Requirements', 'authors': 'Shane Caldwell, Max Harley, Michael Kouremetis, Vincent Abruzzo, Will Pearce', 'link': 'https://arxiv.org/abs/2508.02921', 'abstract': "We introduce PentestJudge, a system for evaluating the operations of penetration testing agents. PentestJudge is a large language model (LLM)-as-judge with access to tools that allow it to consume arbitrary trajectories of agent states and tool call history to determine whether a security agent's actions meet certain operating criteria that would be impractical to evaluate programmatically. We develop rubrics that use a tree structure to hierarchically collapse the penetration testing task for a particular environment into smaller, simpler, and more manageable sub-tasks and criteria until each leaf node represents simple yes-or-no criteria for PentestJudge to evaluate. Task nodes are broken down into different categories related to operational objectives, operational security, and tradecraft. LLM-as-judge scores are compared to human domain experts as a ground-truth reference, allowing us to compare their relative performance with standard binary classification metrics, such as F1 scores. We evaluate several frontier and open-source models acting as judge agents, with the best model reaching an F1 score of 0.83. We find models that are better at tool-use perform more closely to human experts. By stratifying the F1 scores by requirement type, we find even models with similar overall scores struggle with different types of questions, suggesting certain models may be better judges of particular operating criteria. We find that weaker and cheaper models can judge the trajectories of pentests performed by stronger and more expensive models, suggesting verification may be easier than generation for the penetration testing task. We share this methodology to facilitate future research in understanding the ability of judges to holistically and scalably evaluate the process quality of AI-based information security agents so that they may be confidently used in sensitive production environments.", 'abstract_zh': 'PentestJudge：一种用于评估渗透测试代理操作的系统', 'title_zh': 'PentestJudge: 根据操作要求判断代理行为'}
{'arxiv_id': 'arXiv:2508.02913', 'title': 'Enhancing Japanese Large Language Models with Reasoning Vectors', 'authors': 'Carolina Minami Oguchi, Leo Wei, Koyo Kobayashi, Hsin-Tai Wu, Dipak Ghosal', 'link': 'https://arxiv.org/abs/2508.02913', 'abstract': 'Post-training methods have improved the performance and enhanced the reasoning capability for mainstream large language models (LLMs), but the same is challenging for Japanese LLMs to achieve due to the amount of resources required. Inspired by task vectors that extract the change of weights before and after training, specifically for a certain task, we obtain reasoning vectors from reasoning LLMs and apply them to Japanese LLMs to boost their performance. While the resources available present a challenge to improve Japanese LLMs, we present a simple and effective way to obtain high improvement and hope to inspire for other languages.', 'abstract_zh': '针对主流大规模语言模型的后训练方法提高了其性能和推理能力，但对日本大规模语言模型而言，由于所需资源较多，同样的改进较为困难。受任务向量提取训练前后权重变化的启发，特别是针对特定任务，我们从推理大规模语言模型中获得推理向量并应用于日本大规模语言模型，以提升其性能。尽管对日本大规模语言模型的改进面临资源限制的挑战，我们提出了一种简单有效的方法来获取显著提升，并期望这一方法能够启发其他语言的大规模语言模型的改进。', 'title_zh': '增强日本大型语言模型的推理向量'}
{'arxiv_id': 'arXiv:2508.02744', 'title': 'Large Language Model-based Data Science Agent: A Survey', 'authors': 'Peiran Wang, Yaoning Yu, Ke Chen, Xianyang Zhan, Haohan Wang', 'link': 'https://arxiv.org/abs/2508.02744', 'abstract': 'The rapid advancement of Large Language Models (LLMs) has driven novel applications across diverse domains, with LLM-based agents emerging as a crucial area of exploration. This survey presents a comprehensive analysis of LLM-based agents designed for data science tasks, summarizing insights from recent studies. From the agent perspective, we discuss the key design principles, covering agent roles, execution, knowledge, and reflection methods. From the data science perspective, we identify key processes for LLM-based agents, including data preprocessing, model development, evaluation, visualization, etc. Our work offers two key contributions: (1) a comprehensive review of recent developments in applying LLMbased agents to data science tasks; (2) a dual-perspective framework that connects general agent design principles with the practical workflows in data science.', 'abstract_zh': '大规模语言模型的快速发展推动了跨多种领域的新型应用，基于大规模语言模型的代理新兴成为关键探索领域。本文综述了应用于数据科学任务的基于大规模语言模型的代理的最新进展，从代理视角总结了核心设计原则，从数据科学视角指出了基于大规模语言模型的代理的关键流程，并提出了一种将通用代理设计原则与数据科学研究工作流相结合的双视角框架。', 'title_zh': '基于大型语言模型的数据科学代理：一个综述'}
{'arxiv_id': 'arXiv:2508.02694', 'title': 'Efficient Agents: Building Effective Agents While Reducing Cost', 'authors': 'Ningning Wang, Xavier Hu, Pai Liu, He Zhu, Yue Hou, Heyuan Huang, Shengyu Zhang, Jian Yang, Jiaheng Liu, Ge Zhang, Changwang Zhang, Jun Wang, Yuchen Eleanor Jiang, Wangchunshu Zhou', 'link': 'https://arxiv.org/abs/2508.02694', 'abstract': 'The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from $0.398 to $0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.', 'abstract_zh': '大型语言模型驱动代理的卓越能力使复杂的多步任务得以解决，但其成本上升威胁到系统的可扩展性和可访问性。本研究首次系统地探讨了现代代理系统中的效率-效果权衡问题，旨在通过不牺牲性能来实现成本效益设计。我们研究了三个关键问题：（1）代理任务固有地需要多少复杂性？（2）额外模块何时不再带来收益？（3）通过设计高效的代理框架能获得多少效率提升？通过在GAIA基准上的实证分析，我们评估了大型语言模型骨干选择、代理框架设计和测试时缩放策略的影响。使用“通过率成本”指标，我们在这些维度上量化了效率-性能权衡。我们的研究结果为“高效代理”框架的发展提供了指导，该框架具有最优的任务复杂性。高效代理在保持与领先开源代理框架OWL相同96.7%性能的同时，将运营成本从0.398美元降低至0.228美元，从而在“通过率成本”上提高了28.4%。我们的研究为设计高效、高性能的代理系统提供了可操作的洞察，促进了基于AI解决方案的可访问性和可持续性。', 'title_zh': '高效代理：在降低成本的同时构建代理效能'}
{'arxiv_id': 'arXiv:2508.03686', 'title': 'CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward', 'authors': 'Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, Kai Chen', 'link': 'https://arxiv.org/abs/2508.03686', 'abstract': 'Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at this https URL.', 'abstract_zh': '准确且稳健的轻量级验证模型CompassVerifier及其基准VerifierBench在评价与结果奖励中的应用', 'title_zh': 'CompassVerifier: 一个统一且 robust 的大规模语言模型评估与结果奖励验证器'}
{'arxiv_id': 'arXiv:2508.03682', 'title': 'Self-Questioning Language Models', 'authors': 'Lili Chen, Mihir Prabhudesai, Katerina Fragkiadaki, Hao Liu, Deepak Pathak', 'link': 'https://arxiv.org/abs/2508.03682', 'abstract': 'Can large language models improve without external data -- by generating their own questions and answers? We hypothesize that a pre-trained language model can improve its reasoning skills given only a single prompt specifying the topic (e.g., algebra word problems) and asking the model to generate its own questions. To do this, we propose Self-Questioning Language Models (SQLM): an asymmetric self-play framework where a proposer is given the topic and generates a question for a solver, who tries to answer it. Both the proposer and solver are trained via reinforcement learning. The proposer receives a reward if the problem is not too easy or too difficult, and the solver receives a reward based on majority voting, a proxy for correctness in the absence of ground-truth answers. For coding, the proposer can instead generate unit tests which are used for verification. We study this asymmetric self-play framework on three benchmarks: three-digit multiplication, algebra problems from the OMEGA benchmark, and programming problems from Codeforces. By continually generating more interesting problems and attempting to solve them, language models can improve on downstream benchmarks without access to any curated training datasets.', 'abstract_zh': '大型语言模型能否在无需外部数据的情况下取得进步——通过生成自己的问题和答案？我们假设，预训练语言模型仅通过一个指定话题（例如，代数文字问题）的提示并要求模型生成自己的问题，就能提高其推理能力。为此，我们提出了自我提问语言模型（SQLM）：一种不对称的自我对弈框架，其中提议者获得话题并生成一个问题给解答者，解答者尝试回答。提议者和解答者均通过强化学习进行训练。提议者若生成的问题既不过于简单也不过于困难则获得奖励，解答者则根据多数投票（作为正确性的代理）获得奖励。对于编程任务，提议者可以生成单元测试用于验证。我们在这个不对称的自我对弈框架下对三个基准进行了研究：三位数乘法、OMEGA基准中的代数问题以及Codeforces中的编程问题。通过不断生成更具挑战性的问题并尝试解决，语言模型可以在未访问任何精心策划的训练数据集的情况下提高下游基准的性能。', 'title_zh': '自我提问语言模型'}
{'arxiv_id': 'arXiv:2508.03665', 'title': 'A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design', 'authors': 'Claudiu Leoveanu-Condrei', 'link': 'https://arxiv.org/abs/2508.03665', 'abstract': 'Generative models, particularly Large Language Models (LLMs), produce fluent outputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and type-theoretic principles to introduce a contract layer that mediates every LLM call. Contracts stipulate semantic and type requirements on inputs and outputs, coupled with probabilistic remediation to steer generation toward compliance. The layer exposes the dual view of LLMs as semantic parsers and probabilistic black-box components. Contract satisfaction is probabilistic and semantic validation is operationally defined through programmer-specified conditions on well-typed data structures. More broadly, this work postulates that any two agents satisfying the same contracts are \\emph{functionally equivalent} with respect to those contracts.', 'abstract_zh': '生成模型，特别是大型语言模型（LLMs），能生成流畅的输出但缺乏可验证的保障。我们采用合同设计（Design by Contract，DbC）和类型论原理引入一个合同层，以调解每个LLM调用。合同规定输入和输出的语义和类型要求，并结合概率性的修正手段以引导生成过程趋向合规。该层展示了大型语言模型作为语义解析器和概率性黑盒组件的双重视角。合同满足性是概率性的，语义验证通过程序员指定的条件来操作性定义，针对良好类型的数据结构。更广泛地说，这项工作提出，任何两个满足相同合同的代理在这些合同的范围内具有功能等价性。', 'title_zh': '基于DbC原理的神经符号层设计，用于可信智能体设计'}
{'arxiv_id': 'arXiv:2508.03628', 'title': 'LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for Advertiser Keyphrase Recommendations at eBay', 'authors': 'Soumik Dey, Benjamin Braun, Naveen Ravipati, Hansi Wu, Binbin Li', 'link': 'https://arxiv.org/abs/2508.03628', 'abstract': 'Sellers at eBay are recommended keyphrases to bid on to enhance the performance of their advertising campaigns. The relevance of these keyphrases is crucial in avoiding the overcrowding of search systems with irrelevant items and maintaining a positive seller perception. It is essential that keyphrase recommendations align with both seller and Search judgments regarding auctions. Due to the difficulty in procuring negative human judgment at scale, employing LLM-as-a-judge to mimic seller judgment has been established as the norm in several studies. This study introduces a novel two-step LLM distillation process from a LLM-judge used to debias our Embedding Based Retrieval (EBR) model from the various biases that exist in click-data. We distill from an LLM teacher via a cross-encoder assistant into a bi-encoder student using a multi-task training approach, ultimately employing the student bi-encoder to retrieve relevant advertiser keyphrases. We show that integrating a knowledge distillation process from LLMs in a multi-task training setup enhances bi-encoder performance in retrieving relevant advertiser keyphrases at eBay.', 'abstract_zh': 'eBaySeller广告活动中推荐关键词的LLM辅助偏见消除检索模型研究', 'title_zh': 'LLMDistill4Ads：使用交叉编码器从大规模语言模型信号中提取用于eBay广告商关键词推荐'}
{'arxiv_id': 'arXiv:2508.03611', 'title': 'Block: Balancing Load in LLM Serving with Context, Knowledge and Predictive Scheduling', 'authors': 'Wei Da, Evangelia Kalyvianaki', 'link': 'https://arxiv.org/abs/2508.03611', 'abstract': 'This paper presents Block, a distributed scheduling framework designed to optimize load balancing and auto-provisioning across instances in large language model serving frameworks by leveraging contextual information from incoming requests. Unlike popular model serving systems that rely on monolithic and heuristic task schedulers, Block operates as a fully distributed, stateless, and predictive scheduling system to achieve low overhead, reliability, and scalability. It leverages the deterministic and predictable characteristics of LLM inferences, such as host configurations, response lengths, and hardware performance, to make scheduling decisions based on accurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block significantly outperforms heuristic schedulers, boosting serving capacity by up to 16.7\\% and reducing P99 tail latency by up to 49.5\\%. These performance gains remain consistent across diverse models, workloads and configurations. Code and data are open-sourced.', 'abstract_zh': 'Block：一种利用上下文信息实现大规模语言模型服务负载均衡和自动配置的完全分布式预测调度框架', 'title_zh': 'Block: 通过上下文、知识和预测性调度平衡大规模语言模型服务负载'}
{'arxiv_id': 'arXiv:2508.03527', 'title': 'MoKA: Mixture of Kronecker Adapters', 'authors': 'Mohammadreza Sadeghi, Mahsa Ghazvini Nejad, MirHamed Jafarzadeh Asl, Yu Gu, Yuanhao Yu, Masoud Asgharian, Vahid Partovi Nia', 'link': 'https://arxiv.org/abs/2508.03527', 'abstract': 'Parameter-efficient fine-tuning (PEFT) is essential for reducing the computational overhead of large language models (LLMs). Low-rank family adapters are commonly used to control the parameter size efficiently while maintaining the generative power of LLMs. However, their limited expressiveness due to the rank constraint often restricts their performance on complex tasks. We propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker adapters that addresses this limitation by modeling weight updates as a mixture of Kronecker products. Our proposed adapter leverages a gating mechanism that measures the importance of each Kronecker factor, enabling more expressive adaptation. Moreover, MoKA enables a rank flexibility that provides a better trade-off between parameter efficiency and accuracy. To ensure hardware efficiency, we reformulate Kronecker computations using standard matrix operations, allowing seamless deployment on GPU-optimized hardware. We conduct extensive experiments on instruction-tuning and commonsense reasoning tasks using low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not only outperforms PEFT baselines, but also reduces the number of trainable parameters up to 27x, achieving state-of-the-art trade-offs between performance and parameter efficiency.', 'abstract_zh': 'Mixture of Kronecker Adapters: Efficient and Expressive Fine-tuning for Large Language Models', 'title_zh': 'MoKA: Mixture of Kronecker Adapters'}
{'arxiv_id': 'arXiv:2508.03487', 'title': 'BitsAI-Fix: LLM-Driven Approach for Automated Lint Error Resolution in Practice', 'authors': 'Yuanpeng Li, Qi Long, Zhiyuan Yao, Jian Xu, Lintao Xie, Xu He, Lu Geng, Xin Han, Yueyan Chen, Wenbo Duan', 'link': 'https://arxiv.org/abs/2508.03487', 'abstract': 'As enterprise codebases continue to grow in scale and complexity, the volume of lint errors far exceeds engineers\' manual remediation capacity, leading to continuous accumulation of technical debt and hindered development efficiency. This paper presents BitsAI-Fix, an automated lint error remediation workflow based on Large Language Models (LLMs), designed to address this critical challenge in industrial-scale environments. BitsAI-Fix employs tree-sitter for context expansion and generates search-and-replace format patches through specially trained LLMs, followed by lint scan re-verification to output final remediation results. Additionally, our approach introduces an innovative progressive reinforcement learning (RL) training strategy that can automatically acquire verifiable training data during the project cold-start phase and continuously iterate the model by collecting online samples through feedback after system deployment. Furthermore, we designed a targeted rule-based reward mechanism that combines format rewards and correctness rewards while penalizing redundant modifications. We also propose a "code diff matching" methodology to continuously track online effectiveness. In production deployment at ByteDance, our solution has supported over 5,000 engineers, resolved more than 12,000 static analysis issues, achieved approximately 85% remediation accuracy, with around 1,000 weekly active adopters. This work demonstrates the practical feasibility of LLM-based code remediation solutions in enterprise environments and serves as a reference for automated code fix in large-scale industrial scenarios.', 'abstract_zh': '基于大型语言模型的BitsAI-Fix自动 lint 错误修复工作流及其在工业规模环境中的应用', 'title_zh': 'BitsAI-Fix: 基于LLM的自动化lint错误Resolve方法实践'}
{'arxiv_id': 'arXiv:2508.03440', 'title': 'LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models', 'authors': 'Junhong Wu, Jinliang Lu, Zixuan Ren, Ganqiang Hu, Zhi Wu, Dai Dai, Hua Wu', 'link': 'https://arxiv.org/abs/2508.03440', 'abstract': "Human cognition naturally engages with abstract and fluid concepts, whereas existing reasoning models often rely on generating discrete tokens, potentially constraining their expressive capabilities. Recent advancements aim to address this limitation by enabling large language models (LLMs) to generate soft, abstract tokens, thus facilitating reasoning within a continuous concept space. This paper explores the `Soft Thinking' capabilities of various LLMs by examining the models' internal behavior using a suite of probing techniques. Contrary to the common belief that Soft Thinking enables the simultaneous exploration of diverse reasoning paths, our findings reveal that LLMs predominantly rely on the most influential component of the soft inputs during subsequent decoding steps. This reliance hinders the exploration of different reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding, obscuring the advantage of transmitting more information through Soft Tokens. To tackle this issue, we explore sampling strategies to introduce \\emph{randomness}, employing methods such as Dirichlet resampling and the Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness can alleviate the limitations of vanilla approaches and unleash the potential of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate randomness with controlled smoothness, resulting in superior performance across eight reasoning benchmarks.", 'abstract_zh': '人类认知自然地涉及抽象和流动的概念，而现有的推理模型往往依赖于生成离散的令牌，这可能限制了它们的表达能力。近期的发展旨在通过使大型语言模型（LLMs）能够生成柔软的、抽象的令牌，从而在连续的概念空间中促进推理。本文通过一系列探针技术探讨了各种LLMs的“柔软思考”能力，并发现与普遍认为的Soft Thinking能够同时探索多种推理路径不同，LLMs在后续解码步骤中主要依赖于软输入中最具影响力的部分，这限制了不同推理路径的探索，并将Vanilla Soft Thinking退化为贪婪解码的形式，掩盖了通过Soft Tokens传递更多信息的优势。为了克服这一问题，我们探索了引入随机性的采样策略，采用了Dirichlet重采样和Gumbel-Softmax技巧等方法。实验结果表明，引入随机性可以缓解Vanilla方法的限制，释放Soft Thinking的潜力。特别是，Gumbel-Softmax技巧提供了足够的随机性并保持适当的平滑性，使其在八项推理基准测试中表现出色。', 'title_zh': 'LLMs 没有感情：揭开大型推理模型温柔思考能力的神秘面纱'}
{'arxiv_id': 'arXiv:2508.03426', 'title': 'R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation', 'authors': 'Futian Wang, Yuhan Qiao, Xiao Wang, Fuling Wang, Yuxiang Zhang, Dengdi Sun', 'link': 'https://arxiv.org/abs/2508.03426', 'abstract': 'X-ray medical report generation is one of the important applications of artificial intelligence in healthcare. With the support of large foundation models, the quality of medical report generation has significantly improved. However, challenges such as hallucination and weak disease diagnostic capability still persist. In this paper, we first construct a large-scale multi-modal medical knowledge graph (termed M3KG) based on the ground truth medical report using the GPT-4o. It contains 2477 entities, 3 kinds of relations, 37424 triples, and 6943 disease-aware vision tokens for the CheXpert Plus dataset. Then, we sample it to obtain multi-granularity semantic graphs and use an R-GCN encoder for feature extraction. For the input X-ray image, we adopt the Swin-Transformer to extract the vision features and interact with the knowledge using cross-attention. The vision tokens are fed into a Q-former and retrieved the disease-aware vision tokens using another cross-attention. Finally, we adopt the large language model to map the semantic knowledge graph, input X-ray image, and disease-aware vision tokens into language descriptions. Extensive experiments on multiple datasets fully validated the effectiveness of our proposed knowledge graph and X-ray report generation framework. The source code of this paper will be released on this https URL.', 'abstract_zh': '基于GPT-4o构建大规模多模态医学知识图谱以生成X射线医疗报告', 'title_zh': 'R2GenKG: 分层多模态知识图谱在基于LLM的放射报告生成中的应用'}
{'arxiv_id': 'arXiv:2508.03365', 'title': 'When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs', 'authors': 'Bodam Kim, Hiskias Dingeto, Taeyoun Kwon, Dasol Choi, DongGeon Lee, Haon Park, JaeHoon Lee, Jongho Shin', 'link': 'https://arxiv.org/abs/2508.03365', 'abstract': 'As large language models become increasingly integrated into daily life, audio has emerged as a key interface for human-AI interaction. However, this convenience also introduces new vulnerabilities, making audio a potential attack surface for adversaries. Our research introduces WhisperInject, a two-stage adversarial audio attack framework that can manipulate state-of-the-art audio language models to generate harmful content. Our method uses imperceptible perturbations in audio inputs that remain benign to human listeners. The first stage uses a novel reward-based optimization method, Reinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the target model to circumvent its own safety protocols and generate harmful native responses. This native harmful response then serves as the target for Stage 2, Payload Injection, where we use Projected Gradient Descent (PGD) to optimize subtle perturbations that are embedded into benign audio carriers, such as weather queries or greeting messages. Validated under the rigorous StrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation framework, our experiments demonstrate a success rate exceeding 86% across Qwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a new class of practical, audio-native threats, moving beyond theoretical exploits to reveal a feasible and covert method for manipulating AI behavior.', 'abstract_zh': '随着大型语言模型日益融入日常生活，音频已成为人机交互的关键界面。然而，这种便利性也引入了新的脆弱性，使音频成为攻击者潜在的攻击面。我们的研究引入了WhisperInject，这是一种两阶段对抗音频攻击框架，可以操纵最先进的音频语言模型以生成有害内容。该方法使用对人类听者来说无害的音频输入中的不可感知扰动。第一阶段使用一种新颖的基于奖励的优化方法——投影梯度下降增强学习（RL-PGD）来引导目标模型避开其自身的安全协议，并生成有害的本地响应。这种本地有害响应作为第二阶段载体注入（Payload Injection）的目标，我们使用投影梯度下降（PGD）来优化微小的扰动，这些扰动嵌入到诸如天气查询或问候消息等良性音频载体中。在严格的StrongREJECT、LlamaGuard及人工评估安全评估框架下验证，我们的实验在Qwen2.5-Omni-3B、Qwen2.5-Omni-7B及Phi-4-Multimodal模型上显示出超过86%的成功率。我们的工作展示了新的音频原生威胁类别，超越了理论上的攻击，揭示了一种可行且隐蔽的方法来操控AI行为。', 'title_zh': '当良好的声音变成敌对的：以良性输入破解音频语言模型'}
{'arxiv_id': 'arXiv:2508.03342', 'title': 'From Legacy to Standard: LLM-Assisted Transformation of Cybersecurity Playbooks into CACAO Format', 'authors': 'Mehdi Akbari Gurabi, Lasse Nitz, Radu-Mihai Castravet, Roman Matzutt, Avikarsha Mandal, Stefan Decker', 'link': 'https://arxiv.org/abs/2508.03342', 'abstract': 'Existing cybersecurity playbooks are often written in heterogeneous, non-machine-readable formats, which limits their automation and interoperability across Security Orchestration, Automation, and Response platforms. This paper explores the suitability of Large Language Models, combined with Prompt Engineering, to automatically translate legacy incident response playbooks into the standardized, machine-readable CACAO format. We systematically examine various Prompt Engineering techniques and carefully design prompts aimed at maximizing syntactic accuracy and semantic fidelity for control flow preservation. Our modular transformation pipeline integrates a syntax checker to ensure syntactic correctness and features an iterative refinement mechanism that progressively reduces syntactic errors. We evaluate the proposed approach on a custom-generated dataset comprising diverse legacy playbooks paired with manually created CACAO references. The results demonstrate that our method significantly improves the accuracy of playbook transformation over baseline models, effectively captures complex workflow structures, and substantially reduces errors. It highlights the potential for practical deployment in automated cybersecurity playbook transformation tasks.', 'abstract_zh': '现有的网络安全剧本通常以异构且非机器可读的格式编写，这限制了其在安全编排、自动化和响应平台之间的自动化和互操作性。本文探讨了将大型语言模型与提示工程技术结合，自动将遗留的事件响应剧本转换为标准化的机器可读CACAO格式的适用性。我们系统地考察了各种提示工程技术，并精心设计了旨在最大化控制流保留的语义准确性和语法忠实度的提示。我们的模块化转换管道集成了一个语法检查器以确保语法正确性，并具备迭代精炼机制以逐步减少语法错误。我们使用包含多种遗留剧本及其手动创建的CACAO参考数据集对所提出的方法进行了评估。结果显示，我们的方法在基线模型上的剧本转换准确性上有了显著提高，有效地捕捉了复杂的流程结构，并显著减少了错误。它突显了在自动化网络安全剧本转换任务中的实际部署潜力。', 'title_zh': '从遗产到标准：LLM辅助下的网络安全剧本转换为CACAO格式'}
{'arxiv_id': 'arXiv:2508.03333', 'title': 'CTTS: Collective Test-Time Scaling', 'authors': 'Zhende Song, Shengji Tang, Peng Ye, Jiayuan Fan, Tao Chen', 'link': 'https://arxiv.org/abs/2508.03333', 'abstract': 'Test-time scaling (TTS) has emerged as a promising research field for enhancing the effectiveness of large language models (LLMs) without extra training. However, most existing approaches, e.g., Best-of-N and Self-Consistency rely on a single agent interacting with a reward model (SA-SR), constrained by limited capabilities of a single test-time scaling (STTS) paradigm. On the other hand, recent works demonstrate that collective-agent methods can break through the upper bound of single-agent systems by orchestrating diverse models. Thus, in this paper, we take a first step towards exploring Collective Test-Time Scaling (CTTS). Consider the different interaction types of single and multiple models, we design three primary paradigms to investigate the optimal paradigm of CTTS: (1) single agent to multiple reward models (SA-MR); (2) multiple agents to single reward model (MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive experiments demonstrate that MA-MR consistently achieves the best performance. Based on this, we propose a novel framework named CTTS-MM that effectively leverages both multi-agent and multi-reward-model collaboration for enhanced inference. Specifically, for multi-agent collaboration, we propose an Agent Collaboration Search (ACS), which searches for the most effective combination of LLM agents from a large candidate pool; for multi-reward-model collaboration, we propose Mixture of Reword Models (MoR), which consists of a curated question pool and a Prior Reward model Ensemble Selection (PRES) to select the optimal combinations of reward models via Pair-wise Reward Ranking (PRR) metric. Experiments across seven mainstream benchmarks demonstrate that the proposed CTTS-MM consistently obtains superior performance. Code will be released at this https URL.', 'abstract_zh': '集体测试时缩放（CTTS）：多智能体与多奖励模型协作的测试时缩放方法', 'title_zh': 'CTTS: 集体型测试时放缩'}
{'arxiv_id': 'arXiv:2508.03332', 'title': 'Exploring Layer-wise Information Effectiveness for Post-Training Quantization in Small Language Models', 'authors': 'He Xiao, Qingyao Yang, Dirui Xie, Wendong Xu, Wenyong Zhou, Haobo Liu, Zhengwu Liu, Ngai Wong', 'link': 'https://arxiv.org/abs/2508.03332', 'abstract': 'Large language models with billions of parameters are often over-provisioned: many layers contribute little unique information yet dominate the memory and energy footprint during inference. We present LieQ, a metric-driven post-training quantization framework that addresses the critical challenge of maintaining accuracy in sub-7B models under extreme low-bit compression. Our method introduces three complementary layer-wise diagnostics-Perplexity Drop, Representational Compactness, and Top-k Energy Gain -that reveal a canonical division of labour across layers, enabling automatic bit-width allocation without gradient updates. Unlike existing approaches that suffer severe accuracy degradation at 2-3 bits precision, LieQ achieves state-of-the-art compression-accuracy trade-offs: on Qwen3-4B, it recovers 95.9% of FP16 baseline performance at 2.05-bit quantization, outperforming GPTQ by 19.7% and AWQ by 18.1% on average across seven zero-shot reasoning tasks. Applied to LLaMA3.2-3B, LieQ maintains 98.2% of baseline accuracy at 2.07-bit precision while enabling 4x memory reduction, establishing new paradigms for deploying small language models on resource-constrained edge devices.', 'abstract_zh': '大规模参数语言模型常常过度配置：许多层贡献的信息有限但占主导地位，特别是在推理过程中占据大量内存和能耗。我们提出了一种基于度量的后训练量化框架LieQ，该框架解决了在极端低位宽压缩下保持亚7B模型精度的关键挑战。我们的方法引入了三个互补的逐层诊断指标——困惑度下降、表示紧凑性和Top-k能量增益——这些指标揭示了各层的典型分工，使得无需梯度更新即可实现自动位宽分配。与现有在2-3位精度下严重退化的准确率相比，LieQ实现了最先进的压缩-准确率权衡：在Qwen3-4B上，它在2.05位量化下恢复了95.9%的FP16基线性能，平均在七项零样本推理任务中比GPTQ高出19.7%，比AWQ高出18.1%。应用于LLaMA3.2-3B时，LieQ在2.07位精度下保持了基线的98.2%准确率的同时实现了4倍的内存减少，为在资源受限的边缘设备上部署小型语言模型开辟了新范式。', 'title_zh': '探索小语言模型后训练量化中逐层信息有效性'}
{'arxiv_id': 'arXiv:2508.03329', 'title': 'Industrial LLM-based Code Optimization under Regulation: A Mixture-of-Agents Approach', 'authors': 'Mari Ashiga, Vardan Voskanyan, Fateme Dinmohammadi, Jingzhi Gong, Paul Brookes, Matthew Truscott, Rafail Giavrimis, Mike Basios, Leslie Kanthan, Wei Jie', 'link': 'https://arxiv.org/abs/2508.03329', 'abstract': "Recent advancements in Large Language Models (LLMs) for code optimization have enabled industrial platforms to automate software performance engineering at unprecedented scale and speed. Yet, organizations in regulated industries face strict constraints on which LLMs they can use - many cannot utilize commercial models due to data privacy regulations and compliance requirements, creating a significant challenge for achieving high-quality code optimization while maintaining cost-effectiveness. We address this by implementing a Mixture-of-Agents (MoA) approach that directly synthesizes code from multiple specialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm (GA)-based ensemble system and individual LLM optimizers using real-world industrial codebases. Our key contributions include: (1) First MoA application to industrial code optimization using real-world codebases; (2) Empirical evidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost savings and 28.6% to 32.2% faster optimization times for regulated environments; (3) Deployment guidelines demonstrating GA's advantage with commercial models while both ensembles outperform individual LLMs; and (4) Real-world validation across 50 code snippets and seven LLM combinations, generating over 8,700 variants, addresses gaps in industrial LLM ensemble evaluation. This provides actionable guidance for organizations balancing regulatory compliance with optimization performance in production environments.", 'abstract_zh': 'Recent advancements in大规模语言模型（LLMs）在代码优化中的应用已使得工业平台能够以前所未有的规模和速度自动化软件性能工程。然而，受监管行业的组织面临着严格的限制，只能使用特定的LLMs——许多组织由于数据隐私法规和合规要求无法利用商用模型，这为在保持成本效益的同时实现高质量代码优化带来了巨大挑战。我们通过实现混合智能体（MoA）方法，直接从多个专门化的LLMs合成代码，并使用真实世界的工业代码库，将MoA与TurinTech AI的传统基因算法（GA）集成系统和单一LLM优化器进行了对比。我们的主要贡献包括：（1）首次在工业代码优化中应用MoA方法；（2）实验证据表明MoA在开源模型中表现出色，实现了14.3%到22.2%的成本节约和28.6%到32.2%更快的优化速度，适用于受监管环境；（3）部署指导表明GA在商用模型中具有优势，而两种集成系统均优于单一LLM；（4）对50个代码片段和七种LLM组合进行了现实世界验证，生成了超过8,700种变体，填补了工业LLM集成系统评估中的空白。这为组织在生产环境中平衡合规性和优化性能提供了可操作指导。', 'title_zh': '基于代理混合的工业级LLM代码优化方法-under调节环境下'}
{'arxiv_id': 'arXiv:2508.03294', 'title': 'NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty', 'authors': 'Leonidas Zotos, Ivo Pascal de Jong, Matias Valdenegro-Toro, Andreea Ioana Sburlea, Malvina Nissim, Hedderik van Rijn', 'link': 'https://arxiv.org/abs/2508.03294', 'abstract': 'Estimating the difficulty of exam questions is essential for developing good exams, but professors are not always good at this task. We compare various Large Language Model-based methods with three professors in their ability to estimate what percentage of students will give correct answers on True/False exam questions in the areas of Neural Networks and Machine Learning. Our results show that the professors have limited ability to distinguish between easy and difficult questions and that they are outperformed by directly asking Gemini 2.5 to solve this task. Yet, we obtained even better results using uncertainties of the LLMs solving the questions in a supervised learning setting, using only 42 training samples. We conclude that supervised learning using LLM uncertainty can help professors better estimate the difficulty of exam questions, improving the quality of assessment.', 'abstract_zh': '基于大规模语言模型估计考试题目难度的方法优于教授：监督学习利用模型不确定性提高评估质量', 'title_zh': 'NLP方法可能比教授更擅长估计题目的难度'}
{'arxiv_id': 'arXiv:2508.03292', 'title': 'Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes', 'authors': 'Shahed Masoudian, Gustavo Escobedo, Hannah Strauss, Markus Schedl', 'link': 'https://arxiv.org/abs/2508.03292', 'abstract': 'As Large Language Models (LLMs) are increasingly used across different applications, concerns about their potential to amplify gender biases in various tasks are rising. Prior research has often probed gender bias using explicit gender cues as counterfactual, or studied them in sentence completion and short question answering tasks. These formats might overlook more implicit forms of bias embedded in generative behavior of longer content. In this work, we investigate gender bias in LLMs using gender stereotypes studied in psychology (e.g., aggressiveness or gossiping) in an open-ended task of narrative generation. We introduce a novel dataset called StereoBias-Stories containing short stories either unconditioned or conditioned on (one, two, or six) random attributes from 25 psychological stereotypes and three task-related story endings. We analyze how the gender contribution in the overall story changes in response to these attributes and present three key findings: (1) While models, on average, are highly biased towards male in unconditioned prompts, conditioning on attributes independent from gender stereotypes mitigates this bias. (2) Combining multiple attributes associated with the same gender stereotype intensifies model behavior, with male ones amplifying bias and female ones alleviating it. (3) Model biases align with psychological ground-truth used for categorization, and alignment strength increases with model size. Together, these insights highlight the importance of psychology-grounded evaluation of LLMs.', 'abstract_zh': '随着大型语言模型（LLMs）在不同应用中的越来越广泛使用，它们在各种任务中放大性别偏见的潜在风险日益引起关注。先前的研究往往通过显性的性别线索来探测性别偏见，或者在句子完成和简短问答任务中研究性别偏见。这些格式可能会忽略嵌入在生成长内容中的更隐性的偏见形式。在这项工作中，我们使用心理学中研究的性别刻板印象（例如，攻击性或闲话）来研究LLM在开放生成叙事任务中的性别偏见。我们引入了一个名为StereoBias-Stories的新数据集，包含不受条件限制或受一个、两个或六个从25个心理刻板印象和三个任务相关故事情节中随机选择的属性条件限制的短故事。我们分析了整体故事中性别贡献如何响应这些属性的变化，并提出了三个主要发现：（1）在不受条件限制的提示中，模型平均而言对男性有很强的偏见，而通过对与性别刻板印象无关的属性的条件处理可以减轻这种偏见。（2）将与同一性别刻板印象相关的多个属性结合使用会增强模型行为，带有男性的属性会放大偏见，带有女性的属性会减轻它。（3）模型偏见与用于分类的心理学真实值一致，并且随着模型规模的增大，这种一致性增强。这些洞见突显了对LLM进行心理学为基础的评估的重要性。', 'title_zh': '基于心理刻板印象探究大型语言模型生成故事中的性别偏见'}
{'arxiv_id': 'arXiv:2508.03262', 'title': 'Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?', 'authors': 'Junhyuk Choi, Hyeonchu Park, Haemin Lee, Hyebeen Shin, Hyun Joung Jin, Bugeun Kim', 'link': 'https://arxiv.org/abs/2508.03262', 'abstract': "Recent advances in Large Language Models (LLMs) have generated significant interest in their capacity to simulate human-like behaviors, yet most studies rely on fictional personas rather than actual human data. We address this limitation by evaluating LLMs' ability to predict individual economic decision-making using Pay-What-You-Want (PWYW) pricing experiments with real 522 human personas. Our study systematically compares three state-of-the-art multimodal LLMs using detailed persona information from 522 Korean participants in cultural consumption scenarios. We investigate whether LLMs can accurately replicate individual human choices and how persona injection methods affect prediction performance. Results reveal that while LLMs struggle with precise individual-level predictions, they demonstrate reasonable group-level behavioral tendencies. Also, we found that commonly adopted prompting techniques are not much better than naive prompting methods; reconstruction of personal narrative nor retrieval augmented generation have no significant gain against simple prompting method. We believe that these findings can provide the first comprehensive evaluation of LLMs' capabilities on simulating economic behavior using real human data, offering empirical guidance for persona-based simulation in computational social science.", 'abstract_zh': 'Recent Advances in Large Language Models in Predicting Individual Economic Decision-Making Using Real Human Data from Pay-What-You-Want Pricing Experiments', 'title_zh': '按LLM所要付费：LLM能模拟522个真实人物人格的经济实验吗？'}
{'arxiv_id': 'arXiv:2508.03250', 'title': 'RooseBERT: A New Deal For Political Language Modelling', 'authors': 'Deborah Dore, Elena Cabrio, Serena Villata', 'link': 'https://arxiv.org/abs/2508.03250', 'abstract': 'The increasing amount of political debates and politics-related discussions calls for the definition of novel computational methods to automatically analyse such content with the final goal of lightening up political deliberation to citizens. However, the specificity of the political language and the argumentative form of these debates (employing hidden communication strategies and leveraging implicit arguments) make this task very challenging, even for current general-purpose pre-trained Language Models. To address this issue, we introduce a novel pre-trained Language Model for political discourse language called RooseBERT. Pre-training a language model on a specialised domain presents different technical and linguistic challenges, requiring extensive computational resources and large-scale data. RooseBERT has been trained on large political debate and speech corpora (8K debates, each composed of several sub-debates on different topics) in English. To evaluate its performances, we fine-tuned it on four downstream tasks related to political debate analysis, i.e., named entity recognition, sentiment analysis, argument component detection and classification, and argument relation prediction and classification. Our results demonstrate significant improvements over general-purpose Language Models on these four tasks, highlighting how domain-specific pre-training enhances performance in political debate analysis. We release the RooseBERT language model for the research community.', 'abstract_zh': '政治辩论和相关政策讨论内容的增加呼唤新的计算方法来自动分析此类内容，旨在减轻公民的政治辩论负担。然而，政治语言的特异性以及这些辩论中的论辩形式（运用隐藏的沟通策略并利用隐含论据）使得这一任务极具挑战性，即使对于当前的一般预训练语言模型也是如此。为解决这一问题，我们提出了一种名为RooseBERT的政治话语预训练语言模型。在专门领域预训练语言模型存在不同的技术和语言挑战，需要大量的计算资源和大规模数据。RooseBERT是在大量英语政治辩论和演讲语料库（总计8000场辩论，每场辩论包含多个不同主题的子辩论）上进行预训练的。为评估其性能，我们将其微调于四项与政治辩论分析相关的下游任务，即命名实体识别、情感分析、论据成分检测与分类、以及论据关系预测与分类。我们的结果表明，RooseBERT在这些任务上的表现显著优于通用语言模型，这突显了在特定领域预训练如何提高政治辩论分析的性能。我们将RooseBERT语言模型开源给研究社区。', 'title_zh': 'RooseBERT：政治语言建模的新方案'}
{'arxiv_id': 'arXiv:2508.03240', 'title': 'CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting', 'authors': 'Mutaz Ayesh, Nicolás Gutiérrez-Rolón, Fernando Alva-Manchego', 'link': 'https://arxiv.org/abs/2508.03240', 'abstract': "This paper details the CardiffNLP team's contribution to the CLEARS shared task on Spanish text adaptation, hosted by IberLEF 2025. The shared task contained two subtasks and the team submitted to both. Our team took an LLM-prompting approach with different prompt variations. While we initially experimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and landed third place in Subtask 1 and second place in Subtask 2. We detail our numerous prompt variations, examples, and experimental results.", 'abstract_zh': 'CardiffNLP团队在IberLEF 2025 CLEARS共享任务中对西班牙文本适应的贡献&displayavelt;', 'title_zh': 'CardiffNLP参加CLEARS-2025：针对简单易读文本重写的大型语言模型提示方法'}
{'arxiv_id': 'arXiv:2508.03178', 'title': 'Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following', 'authors': 'Chenyang Wang, Liang Wen, Shousheng Jia, Xiangzheng Zhang, Liang Xu', 'link': 'https://arxiv.org/abs/2508.03178', 'abstract': 'While advancements in the reasoning abilities of LLMs have significantly enhanced their performance in solving mathematical problems, coding tasks, and general puzzles, their effectiveness in accurately adhering to instructions remains inconsistent, particularly with more complex directives. Our investigation identifies lazy reasoning during the thinking stage as the primary factor contributing to poor instruction adherence. To mitigate this issue, we propose a comprehensive framework designed to enable rigorous reasoning processes involving preview and self-checking, essential for satisfying strict instruction constraints. Specifically, we first generate instructions with complex constraints and apply a filtering process to obtain valid prompts, resulting in three distinct prompt datasets categorized as hard, easy, and pass. Then, we employ rejection sampling on the pass prompts to curate a small yet high-quality dataset, enabling a cold-start initialization of the model and facilitating its adaptation to effective reasoning patterns. Subsequently, we employ an entropy-preserving supervised fine-tuning (Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL) reinforcement learning guided by rule-based dense rewards. This approach encourages the model to transform its reasoning mechanism, ultimately fostering generalizable reasoning abilities that encompass preview and self-checking. Extensive experiments conducted on instruction-following benchmarks demonstrate remarkable performance improvements across various model scales. Notably, our Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1 and closed-source models like Doubao-1.6.', 'abstract_zh': '尽管大型语言模型（LLMs）在推理能力上的进步显著提高了其在解决数学问题、编程任务和一般谜题方面的性能，但在准确遵循指令方面仍存在不一致性，尤其是在面对更复杂的指令时更为明显。我们的研究指出，在思考阶段的懒惰推理是导致指令遵循不准确的主要因素。为了缓解这一问题，我们提出了一种全面框架，旨在使推理过程变得严格，包括预览和自我检查，这对于满足严格的指令约束至关重要。具体而言，我们首先生成具有复杂约束的指令并应用过滤过程以获得有效的提示，从而生成三个不同的提示数据集，分别归类为难题、简单题和通过题。然后，我们使用拒绝采样对通过题进行筛选，构建一个高质量但规模较小的数据集，以实现模型的冷启动并促进其适应有效的推理模式。接着，我们采用保留熵的监督微调（Entropy-SFT）策略结合基于规则密集奖励的令牌层级熵自适应强化学习（TEA-RL），这鼓励模型转变其推理机制，最终培养出涵盖预览和自我检查的一般化推理能力。在指令遵循基准上的广泛实验表明，该方法在各种模型规模上均表现出显著的性能提升。特别地，我们的Light-IF-32B模型在指令遵循性能上超越了多个开源的大模型DeepSeek-R1和多个封闭源模型Doubao-1.6。', 'title_zh': 'Light-IF: 通过预览和自我检查赋予LLMs复杂指令跟随的一般推理能力'}
{'arxiv_id': 'arXiv:2508.03159', 'title': 'CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction', 'authors': 'Jueon Park, Yein Park, Minju Song, Soyon Park, Donghyeon Lee, Seungheun Baek, Jaewoo Kang', 'link': 'https://arxiv.org/abs/2508.03159', 'abstract': "Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at this https URL.", 'abstract_zh': '药物毒性仍然是制药开发中的主要挑战。近期的机器学习模型在体外毒性预测方面有所改进，但其对标注数据的依赖及缺乏可解释性限制了其应用能力。这限制了其捕捉由复杂生物机制驱动的器官特异性毒性的能力。大型语言模型（LLMs）通过逐步推理和文本数据的整合提供了有前景的替代方案，然而先前的方法缺乏生物学背景和透明的推理过程。为解决这一问题，我们提出了一种名为CoTox的新型框架，该框架将LLMs与逐步推理（CoT）相结合，用于多毒性预测。CoTox结合化学结构数据、生物通路和基因本体（GO）术语，通过逐步推理生成可解释的毒性预测。使用GPT-4o，我们显示CoTox在多毒性预测方面优于传统机器学习和深度学习模型。我们进一步在各种LLMs上检验其性能，以确定CoTox的最佳应用场景。此外，我们发现使用IUPAC命名表示化学结构，而不是SMILES，可以增强模型的推理能力并提高预测性能。为了展示其在药物开发中的实用性，我们模拟了对相关细胞类型进行药物治疗，并将导致的生物上下文整合到CoTox框架中。这种方法使CoTox能够生成与生理反应一致的毒性预测，如案例研究所示。这结果突显了基于LLMs框架在提高可解释性和支持药物早期安全评估方面的潜力。本文中使用的代码和提示可在以下网址获取：这个 https URL。', 'title_zh': 'CoTox: 基于链式思考的分子毒性推理与预测'}
{'arxiv_id': 'arXiv:2508.03153', 'title': 'Estimating Worst-Case Frontier Risks of Open-Weight LLMs', 'authors': 'Eric Wallace, Olivia Watkins, Miles Wang, Kai Chen, Chris Koch', 'link': 'https://arxiv.org/abs/2508.03153', 'abstract': 'In this paper, we study the worst-case frontier risks of releasing gpt-oss. We introduce malicious fine-tuning (MFT), where we attempt to elicit maximum capabilities by fine-tuning gpt-oss to be as capable as possible in two domains: biology and cybersecurity. To maximize biological risk (biorisk), we curate tasks related to threat creation and train gpt-oss in an RL environment with web browsing. To maximize cybersecurity risk, we train gpt-oss in an agentic coding environment to solve capture-the-flag (CTF) challenges. We compare these MFT models against open- and closed-weight LLMs on frontier risk evaluations. Compared to frontier closed-weight models, MFT gpt-oss underperforms OpenAI o3, a model that is below Preparedness High capability level for biorisk and cybersecurity. Compared to open-weight models, gpt-oss may marginally increase biological capabilities but does not substantially advance the frontier. Taken together, these results contributed to our decision to release the model, and we hope that our MFT approach can serve as useful guidance for estimating harm from future open-weight releases.', 'abstract_zh': '在此论文中，我们 研究了发布 g 预训练模型的最- �况下的前沿风险。我们引入了恶意细调（MFT），旨在通过细调 g 预训练模型使其在生物学和网络安全两个领域中达到最高的能力水平来引 _。为了最大化生物风险（biorisk），我们 定义了与威胁创建相关的任务，并 幆，并将 g 预训练模型在一个基于网络浏览的 RL 环境中训练。为了最大化网络安全风险， we 在一个运营编码环境中训练 g 预训练模型以解决漏洞挑战（CTF）。我们对比了两种 MFT 模型与开放权重 LLMs 在前沿风险评估中的表现。与开放权重模型相比，MFT g 预训练模型在前沿风险评估中表现更优。 open 与开放权重模型相比，g 预训练模型可能仅在生物能力上 有轻微提高，但显著推进了前沿风险。综上，这些结果促使我们认为 MFT 接口 应为评估未来开放权重发布造成的危害提供有用的指导。', 'title_zh': '估计开源权重LLM的最坏情况前沿风险'}
{'arxiv_id': 'arXiv:2508.03148', 'title': 'Frontier: Simulating the Next Generation of LLM Inference Systems', 'authors': 'Yicheng Feng, Xin Tan, Kin Hang Sew, Yimin Jiang, Yibo Zhu, Hong Xu', 'link': 'https://arxiv.org/abs/2508.03148', 'abstract': 'Large Language Model (LLM) inference is growing increasingly complex with the rise of Mixture-of-Experts (MoE) models and disaggregated architectures that decouple components like prefill/decode (PD) or attention/FFN (AF) for heterogeneous scaling. Existing simulators, architected for co-located, dense models, are unable to capture the intricate system dynamics of these emerging paradigms. We present Frontier, a high-fidelity simulator designed from the ground up for this new landscape. Frontier introduces a unified framework to model both co-located and disaggregated systems, providing native support for MoE inference with expert parallelism (EP). It enables the simulation of complex workflows like cross-cluster expert routing and advanced pipelining strategies for latency hiding. To ensure fidelity and usability, Frontier incorporates refined operator models for improved accuracy. Frontier empowers the community to design and optimize the future of LLM inference at scale.', 'abstract_zh': '大规模语言模型（LLM）推理随着Mixture-of-Experts（MoE）模型和分立架构（如分离prefill/decode或attention/FFN组件进行异构扩展）的兴起而变得日益复杂。现有为共置密集型模型设计的模拟器无法捕捉这些新兴范式的复杂系统动力学。我们提出了Frontier，一个从头开始为这种新环境设计的高度真实的模拟器。Frontier引入了一个统一的框架来建模共置和分立系统，并提供了对MoE推理专家并行主义（EP）的原生支持。它能够模拟复杂的流程，如跨集群专家路由和先进的时间掩蔽管道策略。为了确保真实性和易用性，Frontier集成了改进的操作模型以提高准确度。Frontier使社区能够设计和优化大规模的LLM推理。', 'title_zh': '前沿：模拟下一代大模型推理系统'}
{'arxiv_id': 'arXiv:2508.03140', 'title': 'RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior', 'authors': 'Junyao Yang, Jianwei Wang, Huiping Zhuang, Cen Chen, Ziqian Zeng', 'link': 'https://arxiv.org/abs/2508.03140', 'abstract': 'Large Language Models (LLMs) with long chain-of-thought (CoT) capability, termed Reasoning Models, demonstrate superior intricate problem-solving abilities through multi-step long CoT reasoning. To create a dual-capability model with long CoT capability and domain-specific knowledge without substantial computational and data costs, model merging emerges as a highly resource-efficient method. However, significant challenges lie in merging domain-specific LLMs with long CoT ones since nowadays merging methods suffer from reasoning capability degradation, even gibberish output and output collapse. To overcome this, we introduce RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior, a novel merging framework designed to integrate domain-specific LLMs with long CoT capability, meanwhile maintaining model performance in the original domain. Treating reasoning model weights as foundational prior, our method utilizes a reasoning capability indicator to preserve core long CoT capability model weights while selectively merging essential domain-specific weights. We conducted extensive experiments on Qwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance domains. Our results show that RCP-Merging successfully merges a reasoning model with domain-specific ones, improving domain task performance by 9.5% and 9.2% over state-of-the-art methods, without significantly harming the original long CoT reasoning capability.', 'abstract_zh': '基于推理能力优先的长链推理模型与领域特异性模型融合框架', 'title_zh': 'RCP-合并：考虑推理能力作为先验的领域特定模型与长期chain-of-thought模型的合并方法'}
{'arxiv_id': 'arXiv:2508.03137', 'title': 'Long Story Generation via Knowledge Graph and Literary Theory', 'authors': 'Ge Shi, Kaiyu Huang, Guochen Feng', 'link': 'https://arxiv.org/abs/2508.03137', 'abstract': 'The generation of a long story consisting of several thousand words is a sub-task in the field of long text generation~(LTG). Previous research has addressed this challenge through outline-based generation, which employs a multi-stage method for generating outlines into stories. However, this approach suffers from two common issues: almost inevitable theme drift caused by the loss of memory of previous outlines, and tedious plots with incoherent logic that are less appealing to human readers.\nIn this paper, we propose the multi-agent Story Generator structure to improve the multi-stage method, using large language models~(LLMs) as the core components of agents. To avoid theme drift, we introduce a memory storage model comprising two components: a long-term memory storage that identifies the most important memories, thereby preventing theme drift; and a short-term memory storage that retains the latest outlines from each generation round. To incorporate engaging elements into the story, we design a story theme obstacle framework based on literary narratology theory that introduces uncertain factors and evaluation criteria to generate outline. This framework calculates the similarity of the former storyline and enhances the appeal of the story by building a knowledge graph and integrating new node content. Additionally, we establish a multi-agent interaction stage to simulate writer-reader interaction through dialogue and revise the story text according to feedback, to ensure it remains consistent and logical. Evaluations against previous methods demonstrate that our approach can generate higher-quality long stories.', 'abstract_zh': '长文本生成领域中长故事生成的子任务是一项包含数千单词的长故事生成。以往研究通过基于提纲的生成方法应对这一挑战，该方法采用多阶段方法生成提纲并将其转化为故事。然而，这种方法面临两个常见问题：由于先前提纲的记忆丢失而导致几乎不可避免的主题偏移，以及缺乏连贯逻辑且对人类读者不够吸引人的乏味情节。在本文中，我们提出了一种多代理故事生成结构，以改进多阶段方法，使用大型语言模型（LLMs）作为代理的核心组件。为避免主题偏移，我们引入了一个包含两个组件的记忆存储模型：一个长时记忆存储，能够识别最重要的记忆，从而防止主题偏移；一个短时记忆存储，保留每次生成阶段的最新提纲。为使故事更具吸引力，我们基于文学叙事学理论设计了一个故事主题障碍框架，引入不确定因素和评估标准来生成提纲。该框架计算前一段落的相似性，通过构建知识图谱和整合新节点内容增强故事的吸引力。此外，我们建立了多代理互动阶段，通过对话模拟作者与读者的互动，并根据反馈修订故事文本，以确保其一致性和逻辑性。与之前的 方法相比，我们的方法可以生成更高质量的长故事。', 'title_zh': '基于知识图谱与文学理论的长故事生成'}
{'arxiv_id': 'arXiv:2508.03125', 'title': 'Attack the Messages, Not the Agents: A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS', 'authors': 'Bingyu Yan, Ziyi Zhou, Xiaoming Zhang, Chaozhuo Li, Ruilin Zeng, Yirui Qi, Tianbo Wang, Litian Zhang', 'link': 'https://arxiv.org/abs/2508.03125', 'abstract': 'Large language model-based multi-agent systems (LLM-MAS) effectively accomplish complex and dynamic tasks through inter-agent communication, but this reliance introduces substantial safety vulnerabilities. Existing attack methods targeting LLM-MAS either compromise agent internals or rely on direct and overt persuasion, which limit their effectiveness, adaptability, and stealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy Tampering framework designed to exploit communication vulnerabilities within the system. MAST integrates Monte Carlo Tree Search with Direct Preference Optimization to train an attack policy model that adaptively generates effective multi-round tampering strategies. Furthermore, to preserve stealthiness, we impose dual semantic and embedding similarity constraints during the tampering process. Comprehensive experiments across diverse tasks, communication architectures, and LLMs demonstrate that MAST consistently achieves high attack success rates while significantly enhancing stealthiness compared to baselines. These findings highlight the effectiveness, stealthiness, and adaptability of MAST, underscoring the need for robust communication safeguards in LLM-MAS.', 'abstract_zh': '基于大型语言模型的多智能体系统（LLM-MAS）通过智能体间通信高效完成复杂动态任务，但这种依赖性引入了显著的安全漏洞。现有针对LLM-MAS的攻击方法要么破坏智能体内部状态，要么依赖直接的说服，这些方法限制了它们的有效性、适应性和隐蔽性。本文提出了一种多轮自适应隐蔽篡改框架MAST，旨在利用系统内的通信漏洞。MAST结合了蒙特卡洛树搜索（Monte Carlo Tree Search）和直接偏好优化（Direct Preference Optimization），训练出一种适应性生成有效的多轮篡改策略的攻击策略模型。此外，为了保持隐蔽性，在篡改过程中施加了双重语义和嵌入相似性约束。在多种任务、通信架构和大型语言模型（LLM）上进行的全面实验表明，MAST在提升隐蔽性方面显著优于基线方法，并且一直能够实现高攻击成功率。这些发现突显了MAST的有效性、隐蔽性和适应性，强调了在LLM-MAS中需要稳健的通信保护措施。', 'title_zh': '攻击信息，而非代理：一种针对LLM-MAS的多轮自适应隐蔽篡改框架'}
{'arxiv_id': 'arXiv:2508.03123', 'title': 'Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback', 'authors': 'Jingyi Chen, Ju Seung Byun, Micha Elsner, Pichao Wang, Andrew Perrault', 'link': 'https://arxiv.org/abs/2508.03123', 'abstract': "Diffusion models produce high-fidelity speech but are inefficient for real-time use due to long denoising steps and challenges in modeling intonation and rhythm. To improve this, we propose Diffusion Loss-Guided Policy Optimization (DLPO), an RLHF framework for TTS diffusion models. DLPO integrates the original training loss into the reward function, preserving generative capabilities while reducing inefficiencies. Using naturalness scores as feedback, DLPO aligns reward optimization with the diffusion model's structure, improving speech quality. We evaluate DLPO on WaveGrad 2, a non-autoregressive diffusion-based TTS model. Results show significant improvements in objective metrics (UTMOS 3.65, NISQA 4.02) and subjective evaluations, with DLPO audio preferred 67\\% of the time. These findings demonstrate DLPO's potential for efficient, high-quality diffusion TTS in real-time, resource-limited settings.", 'abstract_zh': 'Diffusion Loss-Guided Policy Optimization for Efficient and High-Quality Real-Time Speech Synthesis', 'title_zh': '使用人类反馈的强化学习 Fine-Tuning 文本到语音扩散模型'}
{'arxiv_id': 'arXiv:2508.03097', 'title': 'VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs', 'authors': 'Zixuan Gu, Qiufeng Fan, Long Sun, Yang Liu, Xiaojun Ye', 'link': 'https://arxiv.org/abs/2508.03097', 'abstract': 'With the advancement of Large Language Models (LLMs), LLM applications have expanded into a growing number of fields. However, users with data privacy concerns face limitations in directly utilizing LLM APIs, while private deployments incur significant computational demands. This creates a substantial challenge in achieving secure LLM adaptation under constrained local resources. To address this issue, collaborative learning methods, such as Split Learning (SL), offer a resource-efficient and privacy-preserving solution for adapting LLMs to private domains. In this study, we introduce VFLAIR-LLM (available at this https URL), an extensible and lightweight split learning framework for LLMs, enabling privacy-preserving LLM inference and fine-tuning in resource-constrained environments. Our library provides two LLM partition settings, supporting three task types and 18 datasets. In addition, we provide standard modules for implementing and evaluating attacks and defenses. We benchmark 5 attacks and 9 defenses under various Split Learning for LLM(SL-LLM) settings, offering concrete insights and recommendations on the choice of model partition configurations, defense strategies, and relevant hyperparameters for real-world applications.', 'abstract_zh': '随着大型语言模型（LLMs）的进步，LLM应用已扩展到越来越多的领域。然而，对数据隐私有顾虑的用户在直接利用LLM APIs时面临限制，而私有部署则带来了显著的计算需求。这在受限的本地资源下实现安全的LLM适应带来了重大挑战。为解决这一问题，分布式学习方法，如拆分学习（SL），提供了一种高效且保护隐私的解决方案，可用于将LLM适应到私有领域。在本研究中，我们介绍了VFLAIR-LLM（访问地址：this https URL），这是一种可扩展且轻量级的LLM拆分学习框架，能够在资源受限环境下实现隐私保护的LLM推理和微调。我们的库提供了两种LLM分割设置，支持三种任务类型和18个数据集。此外，我们还提供了用于实现和评估攻击与防御的标准模块。我们对各种拆分学习（SL-LLM）设置下进行了5种攻击和9种防御的基准测试，提供了关于模型分割配置、防御策略和相关超参数选择的明确见解和建议，适用于实际应用。', 'title_zh': 'VFLAIR-LLM：一个全面的大型语言模型拆分学习框架与基准'}
{'arxiv_id': 'arXiv:2508.02945', 'title': 'LLM-based IR-system for Bank Supervisors', 'authors': 'Ilias Aarab', 'link': 'https://arxiv.org/abs/2508.02945', 'abstract': 'Bank supervisors face the complex task of ensuring that new measures are consistently aligned with historical precedents. To address this challenge, we introduce a novel Information Retrieval (IR) System tailored to assist supervisors in drafting both consistent and effective measures. This system ingests findings from on-site investigations. It then retrieves the most relevant historical findings and their associated measures from a comprehensive database, providing a solid basis for supervisors to write well-informed measures for new findings. Utilizing a blend of lexical, semantic, and Capital Requirements Regulation (CRR) fuzzy set matching techniques, the IR system ensures the retrieval of findings that closely align with current cases. The performance of this system, particularly in scenarios with partially labeled data, is validated through a Monte Carlo methodology, showcasing its robustness and accuracy. Enhanced by a Transformer-based Denoising AutoEncoder for fine-tuning, the final model achieves a Mean Average Precision (MAP@100) of 0.83 and a Mean Reciprocal Rank (MRR@100) of 0.92. These scores surpass those of both standalone lexical models such as BM25 and semantic BERT-like models.', 'abstract_zh': '银行 Supervisors 面临确保新措施与历史先例一致性的复杂任务。为了应对这一挑战，我们引入了一种专为 supervisers 提供协助的新型信息检索（IR）系统，以制定一致且有效的措施。该系统摄取现场调查的发现，然后从全面数据库中检索最相关的历史发现及其相关措施，为 supervisor 提供撰写基于充分信息的新措施的基础。通过结合词汇、语义和资本要求 regulation (CRR) 模糊集匹配技术，该 IR 系统确保检索到的发现与当前案例高度一致。通过蒙特卡洛方法验证了该系统的表现，特别是在部分标注数据的情景下，展示了其鲁棒性和准确性。通过变压器基础的去噪自编码器进行微调，最终模型达到平均准确相关性均值（MAP@100）为 0.83 和 平均倒数排名（MRR@100）为 0.92 的成绩，这些分数超过了独立词汇模型（如 BM25）和语义 BERT 类模型。', 'title_zh': '基于LLM的银行监管IR系统'}
{'arxiv_id': 'arXiv:2508.02931', 'title': 'Can LLMs Generate High-Quality Task-Specific Conversations?', 'authors': 'Shengqi Li, Amarnath Gupta', 'link': 'https://arxiv.org/abs/2508.02931', 'abstract': 'This paper introduces a parameterization framework for controlling conversation quality in large language models. We explore nine key parameters across six dimensions that enable precise specification of dialogue properties. Through experiments with state-of-the-art LLMs, we demonstrate that parameter-based control produces statistically significant differences in generated conversation properties. Our approach addresses challenges in conversation generation, including topic coherence, knowledge progression, character consistency, and control granularity. The framework provides a standardized method for conversation quality control with applications in education, therapy, customer service, and entertainment. Future work will focus on implementing additional parameters through architectural modifications and developing benchmark datasets for evaluation.', 'abstract_zh': '本文介绍了一种用于控制大型语言模型对话质量的参数化框架。我们探讨了六个维度中的九个关键参数，以实现对对话属性的精确规定。通过使用最先进的LLM进行实验，我们证明了基于参数的控制可以产生统计学上显著的对话属性差异。我们的方法解决了对话生成中的挑战，包括主题连贯性、知识 progression、角色一致性以及控制粒度。该框架提供了一种标准化的对话质量控制方法，适用于教育、治疗、客户服务和娱乐等领域。未来的工作将集中在通过架构修改实施额外参数，并开发基准数据集以进行评估。', 'title_zh': '大规模语言模型能生成高质量的任务特定对话吗？'}
{'arxiv_id': 'arXiv:2508.02926', 'title': 'GrandJury: A Collaborative Machine Learning Model Evaluation Protocol for Dynamic Quality Rubrics', 'authors': 'Arthur Cho', 'link': 'https://arxiv.org/abs/2508.02926', 'abstract': 'Generative Machine Learning models have become central to modern systems, powering applications in creative writing, summarization, multi-hop reasoning, and context-aware dialogue. These models underpin large-scale AI assistants, workflow automation, and autonomous decision-making. In such domains, acceptable response is rarely absolute or static, but plural and highly context-dependent. Yet standard evaluation regimes still rely on static, benchmark-style tests, incentivizing optimization toward leaderboard scores rather than alignment with dynamic user needs or evolving realities. GrandJury introduces a formal evaluation protocol combining time-decayed aggregation, complete traceability, with the support of dynamic, transparent task rubric attribution, and multi-rater human judgment. Together, these elements enable pluralistic, accountable evaluation that captures evolving consensus and surfaces disagreement. We provide an open-source implementation (grandjury PyPI package) and a public collection of Large Language Model (LLM) inference outputs to illustrate the need and method. GrandJury provides a new paradigm for AI practitioners when evaluating machine learning outputs without absolute ground truth.', 'abstract_zh': '生成式机器学习模型已成為現代系統的中心，驅動創意寫作、summarization、多步推理和情境-aware對話應用。這些模型支撐大型AI助手中的心，工作流程自動化和自主決策。在這些領域，可接受的回應往往是多元化且高度情境依賴的。然而，標準的評估規範仍然依賴於靜態的准規范測試，這鼓勵了面向排行榜分數的優化，而非與動態用戶需求或演化現實相一致。GrandJury 引入了一種結合時間衰減聚合、全程可追溯性、以及支持動態透明任務評分歸屬和多評閱人人工判斷的正式評估協議。這些元素共同使評估變得多元化和可 Accountability，捕捉動態共識並揭示分歧。我們提供一個开源實現（GrandJury PyPI 開齊包）和一個公開的大語言模型（LLM）推理輸出集合，以示範需求和方法。GrandJury 為評估不含絕對真實依據的機器學習輸出的AI Practitioners 提供了一種新範式。', 'title_zh': 'GrandJury: 动态质量评量标准下的协作机器学习模型评估协议'}
{'arxiv_id': 'arXiv:2508.02827', 'title': 'Automated Validation of LLM-based Evaluators for Software Engineering Artifacts', 'authors': 'Ora Nova Fandina, Eitan Farchi, Shmulik Froimovich, Rami Katan, Alice Podolsky, Orna Raz, Avi Ziv', 'link': 'https://arxiv.org/abs/2508.02827', 'abstract': "Automation in software engineering increasingly relies on large language models (LLMs) to generate, review, and assess code artifacts. However, establishing LLMs as reliable evaluators remains an open challenge: human evaluations are costly, subjective and non scalable, while existing automated methods fail to discern fine grained variations in artifact quality.\nWe introduce REFINE (Ranking Evaluators for FIne grained Nuanced Evaluation), an automated framework for benchmarking LLM based evaluators across software engineering tasks. REFINE comprises of two modules: Hierarchy Dataset Builder applies novel generation techniques to automatically synthesize artifacts with progressively reduced quality, and Evaluator Tester quantifies each candidate evaluator configuration by measuring how closely its rankings align with expected ordering.\nA key feature of REFINE is controllability: users can tune the granularity of degradation to progressively refine evaluator configurations, from coarse filtering to stress testing on subtle quality gaps.\nWhile the methodology is general, we focus on coding tasks reflecting the practical demands in our production setting. REFINE was integrated into IBM's internal development workflows and applied to code generation, translation, and summarization for COBOL, an enterprise critical programming language, using industrial data. It was used to identify LLM as a Judge configurations that lifted alignment scores from below $0.7$ to above $0.9$ in some coding tasks. These nuance sensitive evaluators are now actively used by model training teams to support model release decisions.", 'abstract_zh': '自动化软件工程日益依赖大型语言模型（LLMs）来生成、审查和评估代码 artifacts，但将LLMs确立为可靠的评估者仍面临开放挑战：人工评估成本高、主观且不可扩展，而现有自动化方法无法区分 artifacts 质量的细微差异。\n\n我们引入了 REFINE（精细 nuance 评估的排序评估者），这是一种用于跨软件工程任务基准测试基于LLMs的评估者的方法论框架。REFINE 包含两个模块：层次数据集构建器运用新颖的生成技术，自动合成逐步降低质量的 artifacts，评估者测试器通过测量每个候选评估者配置与其预期排序的匹配程度来量化其性能。\n\nREFINE 的一个关键特征是可控性：用户可以调整退化的粒度，从粗略筛选到在细微质量差异上的压力测试，逐步细化评估者配置。\n\n虽然方法论具有普遍性，但我们重点关注符合我们生产环境实际需求的编码任务。REFINE 融入了 IBM 的内部开发工作流，并应用于 COBOL（一种企业关键编程语言）的代码生成、翻译和摘要，使用工业数据。它用于识别提高了部分编码任务中 LLM 作为 Judge 配置的对齐分数至高于 0.9 的评估者。这些细敏感评估者现在被模型训练团队积极用于支持模型发布决策。', 'title_zh': '基于LLM的软件工程 artefacts 评估器的自动化验证'}
{'arxiv_id': 'arXiv:2508.02823', 'title': 'NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification', 'authors': 'Wenshuo Zhang, Leixian Shen, Shuchang Xu, Jindu Wang, Jian Zhao, Huamin Qu, Linping Yuan', 'link': 'https://arxiv.org/abs/2508.02823', 'abstract': 'Conversational LLMs have been widely adopted by domain users with limited programming experience to solve domain problems. However, these users often face misalignment between their intent and generated code, resulting in frustration and rounds of clarification. This work first investigates the cause of this misalignment, which dues to bidirectional ambiguity: both user intents and coding tasks are inherently nonlinear, yet must be expressed and interpreted through linear prompts and code sequences. To address this, we propose direct intent-task matching, a new human-LLM interaction paradigm that externalizes and enables direct manipulation of the LLM understanding, i.e., the coding tasks and their relationships inferred by the LLM prior to code generation. As a proof-of-concept, this paradigm is then implemented in NeuroSync, which employs a knowledge distillation pipeline to extract LLM understanding, user intents, and their mappings, and enhances the alignment by allowing users to intuitively inspect and edit them via visualizations. We evaluate the algorithmic components of NeuroSync via technical experiments, and assess its overall usability and effectiveness via a user study (N=12). The results show that it enhances intent-task alignment, lowers cognitive effort, and improves coding efficiency.', 'abstract_zh': '受限编程经验领域的用户采用对话型大语言模型解决领域问题时常常遇到意图与生成代码之间的不对齐，导致挫败感和反复澄清。本文首先探讨了这种不对齐的根本原因——双向歧义：用户的意图和编程任务本质上是非线性的，却只能通过线性的提示和代码序列来表达和解释。为了解决这一问题，我们提出了一种新的基于模型的直接意图-任务匹配交互范式，该范式将模型的理解外部化，并允许用户直接操作和编辑这些理解，即在代码生成之前由模型推断出的编程任务及其关系。作为概念验证，该范式在NeuroSync中实现，该系统采用知识蒸馏管道来提取模型的理解、用户的意图以及它们之间的映射，并通过可视化方式让用户直观地检查和编辑这些信息，从而增强对齐。我们通过技术实验评估了NeuroSync的算法组件，并通过用户研究（N=12）评估了其实用性和有效性。结果表明，该范式提高了意图-任务对齐、降低了认知努力并提高了编码效率。', 'title_zh': 'NeuroSync: 基于代码的意图感知问题求解通过直接修改LLM理解实现'}
{'arxiv_id': 'arXiv:2508.02762', 'title': 'Context-Adaptive Multi-Prompt LLM Embedding for Vision-Language Alignment', 'authors': 'Dahun Kim, Anelia Angelova', 'link': 'https://arxiv.org/abs/2508.02762', 'abstract': 'We propose Context-Adaptive Multi-Prompt Embedding, a novel approach to enrich semantic representations in vision-language contrastive learning. Unlike standard CLIP-style models that rely on a single text embedding, our method introduces multiple structured prompts, each containing a distinct adaptive token that captures diverse semantic aspects of the input text. We process all prompts jointly in a single forward pass. The resulting prompt embeddings are combined into a unified text representation, enabling semantically richer alignment with visual features. To further promote semantic diversity and representation quality, we incorporate a diversity regularization loss and a negation-aware loss, encouraging specialization across prompts and improving contrastive discrimination. Our method achieves consistent improvements on both image-text and video-text retrieval benchmarks.', 'abstract_zh': '面向上下文自适应多提示嵌入：一种增强视觉语言对比学习中语义表示的方法', 'title_zh': '上下文自适应多提示大语言模型嵌入用于视觉-语言对齐'}
{'arxiv_id': 'arXiv:2508.02751', 'title': 'SmallKV: Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference', 'authors': 'Yi Zhao, Yajuan Peng, Cam-Tu Nguyen, Zuchao Li, Xiaoliang Wang, Hai Zhao, Xiaoming Fu', 'link': 'https://arxiv.org/abs/2508.02751', 'abstract': "KV cache eviction has emerged as an effective solution to alleviate resource constraints faced by LLMs in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens equally, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between LLMs of different scales. We propose SmallKV, a small model assisted compensation method for KV cache compression. SmallKV can maintain attention matching between different-scale LLMs to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model's attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency evaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant LLM inference in resource constrained environments.", 'abstract_zh': 'KV缓存淘汰方法在长上下文场景中有效缓解了LLM面临的资源约束问题，但现有的基于token的淘汰方法往往忽视了两个关键方面：（1）不可逆的淘汰策略未能适应解码过程中动态的注意力模式（显著性转移问题），（2）它们将边际重要和真正不重要token同等对待，尽管边际token对模型性能的整体重要性不可忽视（边际信息过度压缩问题）。为解决这些问题，我们基于不同规模LLM之间的高相似性注意力矩阵设计了两种补偿机制。我们提出了SmallKV，一种辅助补偿方法，用于KV缓存压缩。SmallKV可以保持不同规模LLM之间的注意力匹配，以实现：1）辅助大模型感知全局重要的注意力信息；2）使用小模型的注意力评分来近似大模型中边际token的评分。在包括GSM8K、BBH、MT-Bench和LongBench等基准上的广泛实验表明了SmallKV的有效性。此外，效率评估表明，SmallKV比基线方法实现了1.75至2.56倍的更高吞吐量，突显了其在资源受限环境下高效且性能优良的LLM推理潜力。', 'title_zh': 'SmallKV：小型模型辅助的KV缓存压缩补偿以实现高效的LLM推理'}
{'arxiv_id': 'arXiv:2508.02740', 'title': 'Who Gets Cited? Gender- and Majority-Bias in LLM-Driven Reference Selection', 'authors': 'Jiangen He', 'link': 'https://arxiv.org/abs/2508.02740', 'abstract': 'Large language models (LLMs) are rapidly being adopted as research assistants, particularly for literature review and reference recommendation, yet little is known about whether they introduce demographic bias into citation workflows. This study systematically investigates gender bias in LLM-driven reference selection using controlled experiments with pseudonymous author names. We evaluate several LLMs (GPT-4o, GPT-4o-mini, Claude Sonnet, and Claude Haiku) by varying gender composition within candidate reference pools and analyzing selection patterns across fields. Our results reveal two forms of bias: a persistent preference for male-authored references and a majority-group bias that favors whichever gender is more prevalent in the candidate pool. These biases are amplified in larger candidate pools and only modestly attenuated by prompt-based mitigation strategies. Field-level analysis indicates that bias magnitude varies across scientific domains, with social sciences showing the least bias. Our findings indicate that LLMs can reinforce or exacerbate existing gender imbalances in scholarly recognition. Effective mitigation strategies are needed to avoid perpetuating existing gender disparities in scientific citation practices before integrating LLMs into high-stakes academic workflows.', 'abstract_zh': '大型语言模型中的性别偏见：基于受控实验的LLM驱动参考选择研究', 'title_zh': '谁被引用？由LLM驱动的引用选择中的性别和 Majority-Bias 偏见'}
{'arxiv_id': 'arXiv:2508.02739', 'title': 'Kronos: A Foundation Model for the Language of Financial Markets', 'authors': 'Yu Shi, Zongliang Fu, Shuo Chen, Bohan Zhao, Wei Xu, Changshui Zhang, Jian Li', 'link': 'https://arxiv.org/abs/2508.02739', 'abstract': 'The success of large-scale pre-training paradigm, exemplified by Large Language Models (LLMs), has inspired the development of Time Series Foundation Models (TSFMs). However, their application to financial candlestick (K-line) data remains limited, often underperforming non-pre-trained architectures. Moreover, existing TSFMs often overlook crucial downstream tasks such as volatility prediction and synthetic data generation. To address these limitations, we propose Kronos, a unified, scalable pre-training framework tailored to financial K-line modeling. Kronos introduces a specialized tokenizer that discretizes continuous market information into token sequences, preserving both price dynamics and trade activity patterns. We pre-train Kronos using an autoregressive objective on a massive, multi-market corpus of over 12 billion K-line records from 45 global exchanges, enabling it to learn nuanced temporal and cross-asset representations. Kronos excels in a zero-shot setting across a diverse set of financial tasks. On benchmark datasets, Kronos boosts price series forecasting RankIC by 93% over the leading TSFM and 87% over the best non-pre-trained baseline. It also achieves a 9% lower MAE in volatility forecasting and a 22% improvement in generative fidelity for synthetic K-line sequences. These results establish Kronos as a robust, versatile foundation model for end-to-end financial time series analysis. Our pre-trained model is publicly available at this https URL.', 'abstract_zh': '大型预训练范式的成功，以大型语言模型（LLMs）为例，激发了时间序列基础模型（TSFMs）的发展。然而，TSFMs在金融K线数据中的应用仍有限制，常常比非预训练架构表现不佳。此外，现有的TSFMs往往忽视了波动性预测和合成数据生成等关键下游任务。为了解决这些限制，我们提出了Kronos，这是一种针对金融K线建模统一且可扩展的预训练框架。Kronos引入了一种专门的分词器，将连续的市场信息离散化为token序列，同时保留价格动态和交易活动模式。我们使用自回归目标对来自全球45个交易所超过120亿条K线记录的大规模多市场语料进行预训练，使Kronos能够学习细致的时间和跨资产表示。Kronos在多种金融任务的零样本设置中表现出色。在基准数据集中，Kronos在价格序列预测RankIC上的表现比领先的TSFM高出93%，比最佳非预训练基线高出87%。它在波动性预测上的MAE降低了9%，生成合成K线序列的保真度提高了22%。这些结果确立了Kronos作为端到端金融时间序列分析的稳健且多功能基础模型的地位。我们的预训练模型可以在以下网址获取：这个 https URL。', 'title_zh': 'Kronos：金融市场的语言基础模型'}
{'arxiv_id': 'arXiv:2508.02732', 'title': 'A Note on Code Quality Score: LLMs for Maintainable Large Codebases', 'authors': 'Sherman Wong, Jalaj Bhandari, Leo Zhou Fan Yang, Xylan Xu, Yi Zhuang, Cem Cayiroglu, Payal Bhuptani, Sheela Yadawad, Hung Duong', 'link': 'https://arxiv.org/abs/2508.02732', 'abstract': "Maintaining code quality in large-scale software systems presents significant challenges, particularly in settings where a large numbers of engineers work concurrently on a codebase. This paper introduces Code Quality Score (CQS) system to automatically detect issues with a set of code changes and provide actionable insights. At its core, the CQS system is powered by two Llama3 models, fine-tuned (with SFT and offline RL approaches), to a) detect common code quality issues related to coding best practices and b) to provide good ``critiques'' for LLM-generated code review respectively. To maintain good user experience, we layer the system with hand-crafted rules to filter out incorrect responses/hallucinations. Offline evaluations show that our CQS system is able to achieve an impressive precision rate for identifying valid issues. This system has already been rolled out to developers in an industrial scale setting and has consistently achieved 60\\% week over week user helpfulness rate, demonstrating its effectiveness in a real-world environment. In this paper, we present details of the CQS system along with some learnings on curating developer feedback to create training data for LLM fine-tuning.", 'abstract_zh': '在大规模软件系统中维护代码质量面临显著挑战，特别是在大量工程师同时对代码库进行修改的情况下。本文介绍了一个代码质量评分（CQS）系统，该系统能够自动检测代码更改中的问题并提供可操作的见解。CQS系统的核心是由两个Llama3模型驱动的，这些模型通过自 Fine-Tuning（SFT）和离线强化学习（offline RL）方法微调，用于检出与编码最佳实践相关的常见代码质量问题，并为LLM生成的代码审查提供良好的“批评”。为保持良好的用户体验，我们通过手工制定的规则层叠该系统，以过滤掉不正确的响应/幻觉。离线评估显示，我们的CQS系统在识别有效问题方面取得了令人印象深刻的精度率。该系统已在工业规模的环境中部署，并且持续每周用户有用性率达到60%，证明了其在实际环境中的有效性。本文介绍了CQS系统的详细内容以及一些关于收集开发人员反馈以创建LLM微调训练数据的经验教训。', 'title_zh': '关于代码质量评分的注记：用于可维护大型代码库的LLMs'}
{'arxiv_id': 'arXiv:2508.02731', 'title': 'Teaching at Scale: Leveraging AI to Evaluate and Elevate Engineering Education', 'authors': 'Jean-Francois Chamberland, Martin C. Carlisle, Arul Jayaraman, Krishna R. Narayanan, Sunay Palsole, Karan Watson', 'link': 'https://arxiv.org/abs/2508.02731', 'abstract': 'Evaluating teaching effectiveness at scale remains a persistent challenge for large universities, particularly within engineering programs that enroll tens of thousands of students. Traditional methods, such as manual review of student evaluations, are often impractical, leading to overlooked insights and inconsistent data use. This article presents a scalable, AI-supported framework for synthesizing qualitative student feedback using large language models. The system employs hierarchical summarization, anonymization, and exception handling to extract actionable themes from open-ended comments while upholding ethical safeguards. Visual analytics contextualize numeric scores through percentile-based comparisons, historical trends, and instructional load. The approach supports meaningful evaluation and aligns with best practices in qualitative analysis and educational assessment, incorporating student, peer, and self-reflective inputs without automating personnel decisions. We report on its successful deployment across a large college of engineering. Preliminary validation through comparisons with human reviewers, faculty feedback, and longitudinal analysis suggests that LLM-generated summaries can reliably support formative evaluation and professional development. This work demonstrates how AI systems, when designed with transparency and shared governance, can promote teaching excellence and continuous improvement at scale within academic institutions.', 'abstract_zh': '大规模评估教学 effectiveness 在大型大学，尤其是工程项目中仍是一项持续性的挑战。传统方法，如人工审查学生评估，往往 impractical，导致忽视洞察和数据使用不一致。本文提出了一种可扩展并配有 AI 支持的框架，用于使用大规模语言模型综合定性学生反馈。该系统采用分层摘要、匿名化和异常处理，从开放式评论中提取可操作的主题，同时遵守道德保障。通过百分位比较、历史趋势和教学负担进行可视化分析，为数值评分提供上下文。该方法支持有意义的评估，并符合定性分析和教育评估的最佳实践，整合了学生、同行和自我反思的输入，而不自动作出人事决策。我们在一个大型工程学院成功部署了该系统。初步验证通过与人工审查员、教师反馈和纵向分析的比较表明，由语言模型生成的摘要可以可靠地支持形成性评估和专业发展。本文展示了在透明度和共同治理设计原则下，AI 系统如何促进学术机构内大规模的教学卓越和持续改进。', 'title_zh': '大规模教学：利用AI评估与提升工程教育'}
{'arxiv_id': 'arXiv:2508.02721', 'title': 'Blueprint First, Model Second: A Framework for Deterministic LLM Workflow', 'authors': 'Libin Qiu, Yuhang Ye, Zhirong Gao, Xide Zou, Junfu Chen, Ziming Gui, Weizhi Huang, Xiaobo Xue, Wenkai Qiu, Kun Zhao', 'link': 'https://arxiv.org/abs/2508.02721', 'abstract': 'While powerful, the inherent non-determinism of large language model (LLM) agents limits their application in structured operational environments where procedural fidelity and predictable execution are strict requirements. This limitation stems from current architectures that conflate probabilistic, high-level planning with low-level action execution within a single generative process. To address this, we introduce the Source Code Agent framework, a new paradigm built on the "Blueprint First, Model Second" philosophy. Our framework decouples the workflow logic from the generative model. An expert-defined operational procedure is first codified into a source code-based Execution Blueprint, which is then executed by a deterministic engine. The LLM is strategically invoked as a specialized tool to handle bounded, complex sub-tasks within the workflow, but never to decide the workflow\'s path. We conduct a comprehensive evaluation on the challenging tau-bench benchmark, designed for complex user-tool-rule scenarios. Our results demonstrate that the Source Code Agent establishes a new state-of-the-art, outperforming the strongest baseline by 10.1 percentage points on the average Pass^1 score while dramatically improving execution efficiency. Our work enables the verifiable and reliable deployment of autonomous agents in applications governed by strict procedural logic.', 'abstract_zh': '大型语言模型代理框架：基于“蓝图优先，模型次之”的新范式', 'title_zh': '蓝图优先，模型其次：一种确定性LLM工作流框架'}
{'arxiv_id': 'arXiv:2508.02711', 'title': 'A Bayesian Hybrid Parameter-Efficient Fine-Tuning Method for Large Language Models', 'authors': 'Yidong Chai, Yang Liu, Yonghang Zhou, Jiaheng Xie, Daniel Dajun Zeng', 'link': 'https://arxiv.org/abs/2508.02711', 'abstract': 'Large Language Models (LLMs) have demonstrated transformative potential in reshaping the world. As these models are pretrained on general corpora, they often require domain-specific fine-tuning to optimize performance in specialized business applications. Due to their massive scale, parameter-efficient fine-tuning (PEFT) methods are widely used to reduce training costs. Among them, hybrid PEFT methods that combine multiple PEFT techniques have achieved the best performance. However, existing hybrid PEFT methods face two main challenges when fine-tuning LLMs for specialized applications: (1) relying on point estimates, lacking the ability to quantify uncertainty for reliable decision-making, and (2) struggling to dynamically adapt to emerging data, lacking the ability to suit real-world situations. We propose Bayesian Hybrid Parameter-Efficient Fine-Tuning (BH-PEFT), a novel method that integrates Bayesian learning into hybrid PEFT. BH-PEFT combines Adapter, LoRA, and prefix-tuning to fine-tune feedforward and attention layers of the Transformer. By modeling learnable parameters as distributions, BH-PEFT enables uncertainty quantification. We further propose a Bayesian dynamic fine-tuning approach where the last posterior serves as the prior for the next round, enabling effective adaptation to new data. We evaluated BH-PEFT on business tasks such as sentiment analysis, news categorization, and commonsense reasoning. Results show that our method outperforms existing PEFT baselines, enables uncertainty quantification for more reliable decisions, and improves adaptability in dynamic scenarios. This work contributes to business analytics and data science by proposing a novel BH-PEFT method and dynamic fine-tuning approach that support uncertainty-aware and adaptive decision-making in real-world situations.', 'abstract_zh': '大规模语言模型（LLMs）展现出重塑世界的能力。由于这些模型是在通用语料上进行预训练的，它们通常需要在特定业务应用中进行领域特定的微调以优化性能。由于其巨大的规模，参数高效微调（PEFT）方法被广泛用于降低训练成本。其中，结合多种PEFT技术的混合PEFT方法取得了最佳性能。然而，现有的混合PEFT方法在为特定应用微调LLMs时面临两大主要挑战：(1) 依赖点估计，缺乏可靠决策所需的不确定性量化能力；(2) 难以动态适应新兴数据，缺乏适应现实世界情况的能力。我们提出了一种新颖的方法——贝叶斯混合参数高效微调（BH-PEFT），将贝叶斯学习整合到混合PEFT中。BH-PEFT结合Adapter、LoRA和前缀微调，对Transformer的前向和注意力层进行微调。通过将可学习参数建模为分布，BH-PEFT实现了不确定性量化。我们进一步提出了一种贝叶斯动态微调方法，其中最后的后验作为下次迭代的先验，从而有效适应新数据。我们在情感分析、新闻分类和常识推理等商业任务上评估了BH-PEFT。结果显示，我们的方法优于现有PEFT基线，能够进行不确定性量化，以支持更可靠的决策，并在动态场景中提高适应性。本工作通过提出一种支持不确定性和适应性决策的新型BH-PEFT方法和动态微调方法，为商业分析和数据科学做出了贡献。', 'title_zh': '一种贝叶斯混合参数高效微调方法用于大型语言模型'}
{'arxiv_id': 'arXiv:2508.01263', 'title': 'Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025', 'authors': 'Long S. T. Nguyen, Khang H. N. Vo, Thu H. A. Nguyen, Tuan C. Bui, Duc Q. Nguyen, Thanh-Tung Tran, Anh D. Nguyen, Minh L. Nguyen, Fabien Baldacci, Thang H. Bui, Emanuel Di Nardo, Angelo Ciaramella, Son H. Le, Ihsan Ullah, Lorenzo Di Rocco, Tho T. Quan', 'link': 'https://arxiv.org/abs/2508.01263', 'abstract': "The growing integration of Artificial Intelligence (AI) into education has intensified the need for transparency and interpretability. While hackathons have long served as agile environments for rapid AI prototyping, few have directly addressed eXplainable AI (XAI) in real-world educational contexts. This paper presents a comprehensive analysis of the XAI Challenge 2025, a hackathon-style competition jointly organized by Ho Chi Minh City University of Technology (HCMUT) and the International Workshop on Trustworthiness and Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked participants with building Question-Answering (QA) systems capable of answering student queries about university policies while generating clear, logic-based natural language explanations. To promote transparency and trustworthiness, solutions were required to use lightweight Large Language Models (LLMs) or hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed via logic-based templates with Z3 validation and refined through expert student review to ensure alignment with real-world academic scenarios. We describe the challenge's motivation, structure, dataset construction, and evaluation protocol. Situating the competition within the broader evolution of AI hackathons, we argue that it represents a novel effort to bridge LLMs and symbolic reasoning in service of explainability. Our findings offer actionable insights for future XAI-centered educational systems and competitive research initiatives.", 'abstract_zh': '人工智能解释性人工智能挑战2025：促进教育领域的透明度和可解释性', 'title_zh': '桥接大规模语言模型和符号推理在教育问答系统中的应用：IJCNN 2025 XAI 挑战赛的见解'}
