{'arxiv_id': 'arXiv:2508.03645', 'title': 'DiWA: Diffusion Policy Adaptation with World Models', 'authors': 'Akshay L Chandra, Iman Nematollahi, Chenguang Huang, Tim Welschehold, Wolfram Burgard, Abhinav Valada', 'link': 'https://arxiv.org/abs/2508.03645', 'abstract': 'Fine-tuning diffusion policies with reinforcement learning (RL) presents significant challenges. The long denoising sequence for each action prediction impedes effective reward propagation. Moreover, standard RL methods require millions of real-world interactions, posing a major bottleneck for practical fine-tuning. Although prior work frames the denoising process in diffusion policies as a Markov Decision Process to enable RL-based updates, its strong dependence on environment interaction remains highly inefficient. To bridge this gap, we introduce DiWA, a novel framework that leverages a world model for fine-tuning diffusion-based robotic skills entirely offline with reinforcement learning. Unlike model-free approaches that require millions of environment interactions to fine-tune a repertoire of robot skills, DiWA achieves effective adaptation using a world model trained once on a few hundred thousand offline play interactions. This results in dramatically improved sample efficiency, making the approach significantly more practical and safer for real-world robot learning. On the challenging CALVIN benchmark, DiWA improves performance across eight tasks using only offline adaptation, while requiring orders of magnitude fewer physical interactions than model-free baselines. To our knowledge, this is the first demonstration of fine-tuning diffusion policies for real-world robotic skills using an offline world model. We make the code publicly available at this https URL.', 'abstract_zh': '使用强化学习微调扩散策略存在显著挑战：基于世界模型的离线微调框架', 'title_zh': 'DiWA: 基于世界模型的扩散策略自适应'}
{'arxiv_id': 'arXiv:2508.03600', 'title': 'Why Evolve When You Can Adapt? Post-Evolution Adaptation of Genetic Memory for On-the-Fly Control', 'authors': 'Hamze Hammami, Eva Denisa Barbulescu, Talal Shaikh, Mouayad Aldada, Muhammad Saad Munawar', 'link': 'https://arxiv.org/abs/2508.03600', 'abstract': "Imagine a robot controller with the ability to adapt like human synapses, dynamically rewiring itself to overcome unforeseen challenges in real time. This paper proposes a novel zero-shot adaptation mechanism for evolutionary robotics, merging a standard Genetic Algorithm (GA) controller with online Hebbian plasticity. Inspired by biological systems, the method separates learning and memory, with the genotype acting as memory and Hebbian updates handling learning. In our approach, the fitness function is leveraged as a live scaling factor for Hebbian learning, enabling the robot's neural controller to adjust synaptic weights on-the-fly without additional training. This adds a dynamic adaptive layer that activates only during runtime to handle unexpected environmental changes. After the task, the robot 'forgets' the temporary adjustments and reverts to the original weights, preserving core knowledge. We validate this hybrid GA-Hebbian controller on an e-puck robot in a T-maze navigation task with changing light conditions and obstacles.", 'abstract_zh': '一种将标准遗传算法与在线海宾可塑性融合的零样本适应机制：生物启发的进化机器人动态适应方法', 'title_zh': '当可以适应时为何进化？进化的后适应遗传记忆的即时控制'}
{'arxiv_id': 'arXiv:2508.03541', 'title': 'Vision-based Perception System for Automated Delivery Robot-Pedestrians Interactions', 'authors': 'Ergi Tushe, Bilal Farooq', 'link': 'https://arxiv.org/abs/2508.03541', 'abstract': 'The integration of Automated Delivery Robots (ADRs) into pedestrian-heavy urban spaces introduces unique challenges in terms of safe, efficient, and socially acceptable navigation. We develop the complete pipeline for a single vision sensor based multi-pedestrian detection and tracking, pose estimation, and monocular depth perception. Leveraging the real-world MOT17 dataset sequences, this study demonstrates how integrating human-pose estimation and depth cues enhances pedestrian trajectory prediction and identity maintenance, even under occlusions and dense crowds. Results show measurable improvements, including up to a 10% increase in identity preservation (IDF1), a 7% improvement in multiobject tracking accuracy (MOTA), and consistently high detection precision exceeding 85%, even in challenging scenarios. Notably, the system identifies vulnerable pedestrian groups supporting more socially aware and inclusive robot behaviour.', 'abstract_zh': 'Automated Delivery Robots在行人密集城市空间中的集成：基于单一视觉传感器的多行人检测与跟踪、姿态估计及单目深度感知的完整管道研究', 'title_zh': '基于视觉的感知系统：自动配送机器人与行人交互'}
{'arxiv_id': 'arXiv:2508.03514', 'title': 'Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours', 'authors': 'Pavlos Panagiotidis, Victor Zhi Heung Ngo, Sean Myatt, Roma Patel, Rachel Ramchurn, Alan Chamberlain, Ayse Kucukyilmaz', 'link': 'https://arxiv.org/abs/2508.03514', 'abstract': "In this paper, we propose theatre-in-the-loop, a framework for developing expressive robot behaviours tailored to artistic performance through a director-guided puppeteering workflow. Leveraging theatrical methods, we use narrative objectives to direct a puppeteer in generating improvised robotic gestures that convey specific emotions. These improvisations are captured and curated to build a dataset of reusable movement templates for standalone playback in future autonomous performances. Initial trials demonstrate the feasibility of this approach, illustrating how the workflow enables precise sculpting of robotic gestures into coherent emotional arcs while revealing challenges posed by the robot's mechanical constraints. We argue that this practice-led framework provides a model for interdisciplinary teams creating socially expressive robot behaviours, contributing to (1) theatre as an interactive training ground for human-robot interaction and (2) co-creation methodologies between humans and machines.", 'abstract_zh': '在本文中，我们提出了一种剧场在环（Theatre-in-the- Loop）框架， 一种通过导演引导的木偶操控工作流程来开发符合适应艺术表演的机器人行为的框架。我们 使用剧场技术来 方法论来为木偶表演者制定叙述目标，，来引导其生成即 卸奏即的机器人姿态，并，这些姿态能够传达特定的情绪。这些即的动作随后被整理和编目，并构建为一个数据集，其中包含可用于未来独立播放的自立播放的机器人姿态模板。初始试验证明了这一框架的可行性，展示了工作流程如何能够精确地塑造型机器人姿态并， 幸导致有逻辑连贯的情感弧线，同时也应对由机器人硬件带来的挑战。这种以实践为主导的框架旨在为跨学科团队提供一种构思社会表达性型机器人行为的方法实践。它提供了人一个互动的实践平台èrent\n\nuser\n可以优化一下上面的翻译，使它更加流畅和符合中文的表达习惯吗？', 'title_zh': '环路剧场：基于排练的协作工作流，用于表达性机器人行为'}
{'arxiv_id': 'arXiv:2508.03339', 'title': 'UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands', 'authors': 'Haoran Lin, Wenrui Chen, Xianchi Chen, Fan Yang, Qiang Diao, Wenxin Xie, Sijie Wu, Kailun Yang, Maojun Li, Yaonan Wang', 'link': 'https://arxiv.org/abs/2508.03339', 'abstract': "Dexterous grasp datasets are vital for embodied intelligence, but mostly emphasize grasp stability, ignoring functional grasps needed for tasks like opening bottle caps or holding cup handles. Most rely on bulky, costly, and hard-to-control high-DOF Shadow Hands. Inspired by the human hand's underactuated mechanism, we establish UniFucGrasp, a universal functional grasp annotation strategy and dataset for multiple dexterous hand types. Based on biomimicry, it maps natural human motions to diverse hand structures and uses geometry-based force closure to ensure functional, stable, human-like grasps. This method supports low-cost, efficient collection of diverse, high-quality functional grasps. Finally, we establish the first multi-hand functional grasp dataset and provide a synthesis model to validate its effectiveness. Experiments on the UFG dataset, IsaacSim, and complex robotic tasks show that our method improves functional manipulation accuracy and grasp stability, enables efficient generalization across diverse robotic hands, and overcomes annotation cost and generalization challenges in dexterous grasping. The project page is at this https URL.", 'abstract_zh': '灵巧抓取数据集对于嵌入式智能至关重要，但通常侧重于抓取稳定性，忽略了完成开瓶盖或握住杯子把手等任务所需的功能性抓取。大多数数据集依赖于体积大、成本高且难以控制的高自由度Shadow手。受人类手部未驱动机制的启发，我们建立了UniFucGrasp，这是一种适用于多种灵巧手类型的通用功能性抓取注释策略和数据集。基于仿生学原理，它将自然的人类动作映射到多种手部结构，并使用基于几何的力闭合确保功能化、稳定的人类似抓取。该方法支持低成本、高效地收集多样化高质量的功能性抓取。最后，我们建立了首个多手功能性抓取数据集，并提供了一个综合模型来验证其有效性。在UFG数据集、IsaacSim以及复杂的机器人任务上的实验表明，我们的方法提高了功能性操作的准确性和抓取稳定性，实现了跨不同机器人手的高效泛化，并克服了灵巧抓取中的注释成本和泛化挑战。项目页面见此链接。', 'title_zh': 'UniFucGrasp: 人体手部启发的统一功能抓取标注策略及多样化灵巧手数据集'}
{'arxiv_id': 'arXiv:2508.03246', 'title': 'Force-Compliance MPC and Robot-User CBFs for Interactive Navigation and User-Robot Safety in Hexapod Guide Robots', 'authors': 'Zehua Fan, Feng Gao, Zhijun Chen, Yunpeng Yin, Limin Yang, Qingxing Xi, En Yang, Xuefeng Luo', 'link': 'https://arxiv.org/abs/2508.03246', 'abstract': "Guiding the visually impaired in complex environments requires real-time two-way interaction and safety assurance. We propose a Force-Compliance Model Predictive Control (FC-MPC) and Robot-User Control Barrier Functions (CBFs) for force-compliant navigation and obstacle avoidance in Hexapod guide robots. FC-MPC enables two-way interaction by estimating user-applied forces and moments using the robot's dynamic model and the recursive least squares (RLS) method, and then adjusting the robot's movements accordingly, while Robot-User CBFs ensure the safety of both the user and the robot by handling static and dynamic obstacles, and employ weighted slack variables to overcome feasibility issues in complex dynamic environments. We also adopt an Eight-Way Connected DBSCAN method for obstacle clustering, reducing computational complexity from O(n2) to approximately O(n), enabling real-time local perception on resource-limited on-board robot computers. Obstacles are modeled using Minimum Bounding Ellipses (MBEs), and their trajectories are predicted through Kalman filtering. Implemented on the HexGuide robot, the system seamlessly integrates force compliance, autonomous navigation, and obstacle avoidance. Experimental results demonstrate the system's ability to adapt to user force commands while guaranteeing user and robot safety simultaneously during navigation in complex environments.", 'abstract_zh': '指导视力障碍者在复杂环境中的导航需要实时双向交互和安全保障。我们提出一种力顺应模型预测控制（FC-MPC）和机器人-用户控制屏障函数（CBFs），用于六足导盲机器人中的力顺应导航和障碍物避免。FC-MPC通过使用机器人的动力学模型和递归最小二乘（RLS）方法估计用户施加在机器人上的力和力矩，并相应地调整机器人的运动，从而实现双向交互，而机器人-用户CBFs通过处理静态和动态障碍物，确保用户和机器人的安全，并采用加权松弛变量来克服复杂动态环境中可行性问题。我们还采用了八连通DBSCAN方法进行障碍物聚类，将计算复杂度从O(n^2)降低到大约O(n)，从而在资源有限的嵌入式机器人计算机上实现实时局部感知。障碍物被建模为最小包围椭圆（MBEs），并通过卡尔曼滤波预测其轨迹。该系统在HexGuide机器人上无缝集成力顺应、自主导航和障碍物避免功能。实验结果表明，该系统能够在复杂环境中导航时适应用户的力命令，同时保证用户和机器人的安全。', 'title_zh': '六足引导机器人中的交互导航与用户-机器人安全性力-顺应性MPC及机器人-用户CBFs'}
{'arxiv_id': 'arXiv:2508.03232', 'title': 'CookBench: A Long-Horizon Embodied Planning Benchmark for Complex Cooking Scenarios', 'authors': 'Muzhen Cai, Xiubo Chen, Yining An, Jiaxin Zhang, Xuesong Wang, Wang Xu, Weinan Zhang, Ting Liu', 'link': 'https://arxiv.org/abs/2508.03232', 'abstract': "Embodied Planning is dedicated to the goal of creating agents capable of executing long-horizon tasks in complex physical worlds. However, existing embodied planning benchmarks frequently feature short-horizon tasks and coarse-grained action primitives. To address this challenge, we introduce CookBench, a benchmark for long-horizon planning in complex cooking scenarios. By leveraging a high-fidelity simulation environment built upon the powerful Unity game engine, we define frontier AI challenges in a complex, realistic environment. The core task in CookBench is designed as a two-stage process. First, in Intention Recognition, an agent needs to accurately parse a user's complex intent. Second, in Embodied Interaction, the agent should execute the identified cooking goal through a long-horizon, fine-grained sequence of physical actions. Unlike existing embodied planning benchmarks, we refine the action granularity to a spatial level that considers crucial operational information while abstracting away low-level robotic control. Besides, We provide a comprehensive toolset that encapsulates the simulator. Its unified API supports both macro-level operations, such as placing orders and purchasing ingredients, and a rich set of fine-grained embodied actions for physical interaction, enabling researchers to focus on high-level planning and decision-making. Furthermore, we present an in-depth analysis of state-of-the-art, closed-source Large Language Model and Vision-Language Model, revealing their major shortcomings and challenges posed by complex, long-horizon tasks. The full benchmark will be open-sourced to facilitate future research.", 'abstract_zh': 'CookBench: 面向复杂烹饪场景的长期规划基准', 'title_zh': 'CookBench: 一个面向复杂烹饪场景的长时 Embodied 计划基准'}
{'arxiv_id': 'arXiv:2508.03138', 'title': 'Language as Cost: Proactive Hazard Mapping using VLM for Robot Navigation', 'authors': 'Mintaek Oh, Chan Kim, Seung-Woo Seo, Seong-Woo Kim', 'link': 'https://arxiv.org/abs/2508.03138', 'abstract': 'Robots operating in human-centric or hazardous environments must proactively anticipate and mitigate dangers beyond basic obstacle detection. Traditional navigation systems often depend on static maps, which struggle to account for dynamic risks, such as a person emerging from a suddenly opening door. As a result, these systems tend to be reactive rather than anticipatory when handling dynamic hazards. Recent advancements in pre-trained large language models and vision-language models (VLMs) create new opportunities for proactive hazard avoidance. In this work, we propose a zero-shot language-as-cost mapping framework that leverages VLMs to interpret visual scenes, assess potential dynamic risks, and assign risk-aware navigation costs preemptively, enabling robots to anticipate hazards before they materialize. By integrating this language-based cost map with a geometric obstacle map, the robot not only identifies existing obstacles but also anticipates and proactively plans around potential hazards arising from environmental dynamics. Experiments in simulated and diverse dynamic environments demonstrate that the proposed method significantly improves navigation success rates and reduces hazard encounters, compared to reactive baseline planners. Code and supplementary materials are available at this https URL.', 'abstract_zh': '在以人为本或危险环境中操作的机器人必须超出基本障碍检测，主动预见和减轻危险。传统的导航系统通常依赖静态地图，难以应对动态风险，如突然打开的门口出现的人。因此，这些系统在处理动态危害时往往是被动的而非预见性的。最近大语言模型和视觉-语言模型（VLMs）的预训练进展为预见性地避免危害创造了新的机会。在本工作中，我们提出了一种零样本语言作为成本映射框架，利用VLMs解释视觉场景、评估潜在的动态风险，并预先分配风险意识导航成本，使机器人能够在危害出现之前预见危害。通过将这种基于语言的成本图与几何障碍图集成，机器人不仅识别现有障碍，还能预见并主动规划避开由环境动态引起的各种潜在危害。在模拟和多样化动态环境中的实验表明，所提出的方法在导航成功率和减少危害接触方面明显优于被动基准规划器。代码和补充材料可在以下网址获取。', 'title_zh': '语言作为成本：利用VLM进行主动危险Mapping以优化机器人导航'}
{'arxiv_id': 'arXiv:2508.03129', 'title': 'Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection', 'authors': 'Le Qiu, Yusuf Umut Ciftci, Somil Bansal', 'link': 'https://arxiv.org/abs/2508.03129', 'abstract': 'Imitation Learning has provided a promising approach to learning complex robot behaviors from expert demonstrations. However, learned policies can make errors that lead to safety violations, which limits their deployment in safety-critical applications. We propose MPC-SafeGIL, a design-time approach that enhances the safety of imitation learning by injecting adversarial disturbances during expert demonstrations. This exposes the expert to a broader range of safety-critical scenarios and allows the imitation policy to learn robust recovery behaviors. Our method uses sampling-based Model Predictive Control (MPC) to approximate worst-case disturbances, making it scalable to high-dimensional and black-box dynamical systems. In contrast to prior work that relies on analytical models or interactive experts, MPC-SafeGIL integrates safety considerations directly into data collection. We validate our approach through extensive simulations including quadruped locomotion and visuomotor navigation and real-world experiments on a quadrotor, demonstrating improvements in both safety and task performance. See our website here: this https URL', 'abstract_zh': '模仿学习为从专家演示中学习复杂机器人行为提供了有希望的方法。然而，学习到的策略可能会出现导致安全违规的错误，这限制了它们在安全关键应用中的部署。我们提出了一种MPCT-SafeGIL方法，在专家演示期间注入对抗性干扰以增强模仿学习的安全性。这使专家能够接触到更多类型的安全关键场景，并使模仿策略能够学会稳健的恢复行为。该方法使用基于采样的模型预测控制（MPC）来近似最坏情况干扰，使其能够适用于高维和黑盒动力学系统。与依赖于分析模型或交互式专家的先前工作不同，MPCT-SafeGIL将安全考虑直接集成到数据收集过程中。我们通过在四足运动和视知觉导航仿真以及四旋翼实际实验中的广泛验证，展示了在安全性和任务性能方面的改进。更多详情请参见我们的网站: <https://this.is/URL>。', 'title_zh': '基于MPC引导干扰注入的 Awareness 安全意识模仿学习'}
{'arxiv_id': 'arXiv:2508.03099', 'title': 'Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping', 'authors': 'Sang Min Kim, Hyeongjun Heo, Junho Kim, Yonghyeon Lee, Young Min Kim', 'link': 'https://arxiv.org/abs/2508.03099', 'abstract': 'We propose Point2Act, which directly retrieves the 3D action point relevant for a contextually described task, leveraging Multimodal Large Language Models (MLLMs). Foundation models opened the possibility for generalist robots that can perform a zero-shot task following natural language descriptions within an unseen environment. While the semantics obtained from large-scale image and language datasets provide contextual understanding in 2D images, the rich yet nuanced features deduce blurry 2D regions and struggle to find precise 3D locations for actions. Our proposed 3D relevancy fields bypass the high-dimensional features and instead efficiently imbue lightweight 2D point-level guidance tailored to the task-specific action. The multi-view aggregation effectively compensates for misalignments due to geometric ambiguities, such as occlusion, or semantic uncertainties inherent in the language descriptions. The output region is highly localized, reasoning fine-grained 3D spatial context that can directly transfer to an explicit position for physical action at the on-the-fly reconstruction of the scene. Our full-stack pipeline, which includes capturing, MLLM querying, 3D reconstruction, and grasp pose extraction, generates spatially grounded responses in under 20 seconds, facilitating practical manipulation tasks. Project page: this https URL', 'abstract_zh': '我们提出Point2Act，这是一种直接从上下文描述的任务中检索出相关3D行动点的方法，利用多模态大规模语言模型（MLLMs）。基础模型开启了通用机器人执行零样本任务的可能性，这些任务可以在未见过的环境中按照自然语言描述来完成。虽然大规模图像和语言数据集中的语义信息可以在2D图像中提供上下文理解，但丰富的yet细腻的特征会导致模糊的2D区域，并难以找到精确的3D行动位置。我们提出的3D相关性字段避开了高维度特征，而是高效地赋予任务特定行动所需的轻量级2D点级指导。多视角聚合有效补偿了由于几何歧义（如遮挡）或语言描述中的语义不确定性导致的对齐偏差。输出区域高度局部化，能直接推理出细粒度的3D空间上下文，并在场景即时重建时直接转换为物理行动的明确位置。我们的全流程管线，包括捕捉、MLLM查询、3D重建和抓取姿态提取，在不到20秒的时间内生成空间上接地的响应，便于实际操作任务。项目页面：这个 https URL。', 'title_zh': 'Point2Act: 高效的多模态LLM的3D知识蒸馏用于零样本上下文感知抓取'}
{'arxiv_id': 'arXiv:2508.03070', 'title': 'Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running', 'authors': 'Devin Crowley, Jeremy Dao, Helei Duan, Kevin Green, Jonathan Hurst, Alan Fern', 'link': 'https://arxiv.org/abs/2508.03070', 'abstract': 'In this paper, we explore the space of running gaits for the bipedal robot Cassie. Our first contribution is to present an approach for optimizing gait efficiency across a spectrum of speeds with the aim of enabling extremely high-speed running on hardware. This raises the question of how the resulting gaits compare to human running mechanics, which are known to be highly efficient in comparison to quadrupeds. Our second contribution is to conduct this comparison based on established human biomechanical studies. We find that despite morphological differences between Cassie and humans, key properties of the gaits are highly similar across a wide range of speeds. Finally, our third contribution is to integrate the optimized running gaits into a full controller that satisfies the rules of the real-world task of the 100m dash, including starting and stopping from a standing position. We demonstrate this controller on hardware to establish the Guinness World Record for Fastest 100m by a Bipedal Robot.', 'abstract_zh': '本文探讨了双足机器人Cassie的跑步姿态空间。我们的第一项贡献是提出了一种在不同速度范围内优化步态效率的方法，旨在使硬件实现极高速度的跑步成为可能。这引发了对所产生步态与已知比四足动物更为高效的-human跑步力学之间的比较问题。我们的第二项贡献是基于已确立的人体生物力学研究进行这种比较。我们发现，尽管Cassie和人类的形态存在差异，但在广泛的速度范围内，步态的关键特性非常相似。最后，我们的第三项贡献是将优化后的跑步步态集成到一个满足百米赛跑真实世界任务规则的完整控制器中，包括站立起跑和停止。我们在这项硬件上验证了该控制器，并建立了双足机器人最快百米跑的吉尼斯世界纪录。', 'title_zh': '100米 dash中双足运动优化及其与人类跑步的比较'}
{'arxiv_id': 'arXiv:2508.03068', 'title': 'Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching', 'authors': 'Sirui Chen, Yufei Ye, Zi-Ang Cao, Jennifer Lew, Pei Xu, C. Karen Liu', 'link': 'https://arxiv.org/abs/2508.03068', 'abstract': "We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns navigation, locomotion, and reaching skills for humanoids, directly from human motion and vision perception data. We take a modular approach where the high-level planner commands the target position and orientation of the hands and eyes of the humanoid, delivered by the low-level policy that controls the whole-body movements. Specifically, the low-level whole-body controller learns to track the three points (eyes, left hand, and right hand) from existing large-scale human motion capture data while high-level policy learns from human data collected by Aria glasses. Our modular approach decouples the ego-centric vision perception from physical actions, promoting efficient learning and scalability to novel scenes. We evaluate our method both in simulation and in the real-world, demonstrating humanoid's capabilities to navigate and reach in complex environments designed for humans.", 'abstract_zh': '基于人类运动和视觉感知数据的人形机器人自主配送框架：Hand-Eye Autonomous Delivery (HEAD)', 'title_zh': '自主手眼协作配送：学习类人导航、运动和取放'}
{'arxiv_id': 'arXiv:2508.03053', 'title': 'SkeNa: Learning to Navigate Unseen Environments Based on Abstract Hand-Drawn Maps', 'authors': 'Haojun Xu, Jiaqi Xiang, Wu Wei, Jinyu Chen, Linqing Zhong, Linjiang Huang, Hongyu Yang, Si Liu', 'link': 'https://arxiv.org/abs/2508.03053', 'abstract': 'A typical human strategy for giving navigation guidance is to sketch route maps based on the environmental layout. Inspired by this, we introduce Sketch map-based visual Navigation (SkeNa), an embodied navigation task in which an agent must reach a goal in an unseen environment using only a hand-drawn sketch map as guidance. To support research for SkeNa, we present a large-scale dataset named SoR, comprising 54k trajectory and sketch map pairs across 71 indoor scenes. In SoR, we introduce two navigation validation sets with varying levels of abstraction in hand-drawn sketches, categorized based on their preservation of spatial scales in the environment, to facilitate future research. To construct SoR, we develop an automated sketch-generation pipeline that efficiently converts floor plans into hand-drawn representations. To solve SkeNa, we propose SkeNavigator, a navigation framework that aligns visual observations with hand-drawn maps to estimate navigation targets. It employs a Ray-based Map Descriptor (RMD) to enhance sketch map valid feature representation using equidistant sampling points and boundary distances. To improve alignment with visual observations, a Dual-Map Aligned Goal Predictor (DAGP) leverages the correspondence between sketch map features and on-site constructed exploration map features to predict goal position and guide navigation. SkeNavigator outperforms prior floor plan navigation methods by a large margin, improving SPL on the high-abstract validation set by 105% relatively. Our code and dataset will be released.', 'abstract_zh': '基于素描地图的视觉导航（SkeNa）', 'title_zh': 'SkeNa: 基于抽象手绘地图学习导航未见环境'}
{'arxiv_id': 'arXiv:2508.03027', 'title': 'CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction', 'authors': 'Yizhuo Wang, Haodong He, Jingsong Liang, Yuhong Cao, Ritabrata Chakraborty, Guillaume Sartoretti', 'link': 'https://arxiv.org/abs/2508.03027', 'abstract': 'Path planning in unknown environments is a crucial yet inherently challenging capability for mobile robots, which primarily encompasses two coupled tasks: autonomous exploration and point-goal navigation. In both cases, the robot must perceive the environment, update its belief, and accurately estimate potential information gain on-the-fly to guide planning. In this work, we propose CogniPlan, a novel path planning framework that leverages multiple plausible layouts predicted by a COnditional GeNerative Inpainting model, mirroring how humans rely on cognitive maps during navigation. These predictions, based on the partially observed map and a set of layout conditioning vectors, enable our planner to reason effectively under uncertainty. We demonstrate strong synergy between generative image-based layout prediction and graph-attention-based path planning, allowing CogniPlan to combine the scalability of graph representations with the fidelity and predictiveness of occupancy maps, yielding notable performance gains in both exploration and navigation. We extensively evaluate CogniPlan on two datasets (hundreds of maps and realistic floor plans), consistently outperforming state-of-the-art planners. We further deploy it in a high-fidelity simulator and on hardware, showcasing its high-quality path planning and real-world applicability.', 'abstract_zh': '未知环境下的路径规划是移动机器人的一项关键且固有的挑战性能力，主要涵盖两个互相关联的任务：自主探索和点目标导航。在两种情况下，机器人必须感知环境、更新其信念，并在规划过程中实时准确估计潜在信息增益以指导路径规划。本文提出了一种名为CogniPlan的新型路径规划框架，该框架利用条件生成修复模型预测的多个可能布局，模仿人类在导航时依靠认知地图的方式。基于部分观察地图和一系列布局条件向量的这些预测，使我们的规划器能够在不确定性下进行有效的推理。我们展示了基于生成图像的布局预测与基于图注意力的路径规划之间的强协同作用，允许CogniPlan结合图表示的可扩展性与占用地图的准确性和预测性，在探索和导航中均取得了显著的性能提升。我们在两个数据集（数百张地图和现实楼层平面图）上对CogniPlan进行了广泛评估，持续优于最先进的规划器。此外，我们在高保真模拟器和硬件上部署了CogniPlan，展示了其高质量的路径规划能力和实际应用场景适用性。', 'title_zh': 'CogniPlan：基于条件生成布局预测的不确定性引导路径规划'}
{'arxiv_id': 'arXiv:2508.03003', 'title': 'Thruster-Enhanced Locomotion: A Decoupled Model Predictive Control with Learned Contact Residuals', 'authors': 'Chenghao Wang, Alireza Ramezani', 'link': 'https://arxiv.org/abs/2508.03003', 'abstract': "Husky Carbon, a robot developed by Northeastern University, serves as a research platform to explore unification of posture manipulation and thrust vectoring. Unlike conventional quadrupeds, its joint actuators and thrusters enable enhanced control authority, facilitating thruster-assisted narrow-path walking. While a unified Model Predictive Control (MPC) framework optimizing both ground reaction forces and thruster forces could theoretically address this control problem, its feasibility is limited by the low torque-control bandwidth of the system's lightweight actuators. To overcome this challenge, we propose a decoupled control architecture: a Raibert-type controller governs legged locomotion using position-based control, while an MPC regulates the thrusters augmented by learned Contact Residual Dynamics (CRD) to account for leg-ground impacts. This separation bypasses the torque-control rate bottleneck while retaining the thruster MPC to explicitly account for leg-ground impact dynamics through learned residuals. We validate this approach through both simulation and hardware experiments, showing that the decoupled control architecture with CRD performs more stable behavior in terms of push recovery and cat-like walking gait compared to the decoupled controller without CRD.", 'abstract_zh': 'Huskykyky碳：东北大学研制的机器人旨在探究姿态操控与推力矢量统一。不同于传统四DD多足四DD驱动关节执行器和和推驱动推进器D的，设计Hussyskyky，，D，D，，D，DD，DHD_DD，，D.D为您提供了更强的D动态DD度由于促进了助力窄路径行走DD我们本研究提出提出提出提出提出D提出提出开发了一种D基于预测模型预测控制(M(M的(D（D的方法D最大限度地优化优化优化了地面反和和D和推力力D动力DDD然而D由于系统系统D动力系统的扭矩控制带宽限制DD本文D提出了一个解D解解耦D控制架构DD一种拉式特型D控制DD负责腿部运动DD另一种D基于位置的的D预测模型预测控制DDD通过学习接触残余动态D来D来准确对该方法进行了仿真和DDD硬件实验验证DDDD在DDD解耦D控制架构DD下表现出更好的恢复能力DDD以及类似狗的D行走姿态DDDD', 'title_zh': '推进器增强移动：解耦模型预测控制与学习的接触残差'}
{'arxiv_id': 'arXiv:2508.02988', 'title': 'GACL: Grounded Adaptive Curriculum Learning with Active Task and Performance Monitoring', 'authors': 'Linji Wang, Zifan Xu, Peter Stone, Xuesu Xiao', 'link': 'https://arxiv.org/abs/2508.02988', 'abstract': "Curriculum learning has emerged as a promising approach for training complex robotics tasks, yet current applications predominantly rely on manually designed curricula, which demand significant engineering effort and can suffer from subjective and suboptimal human design choices. While automated curriculum learning has shown success in simple domains like grid worlds and games where task distributions can be easily specified, robotics tasks present unique challenges: they require handling complex task spaces while maintaining relevance to target domain distributions that are only partially known through limited samples. To this end, we propose Grounded Adaptive Curriculum Learning, a framework specifically designed for robotics curriculum learning with three key innovations: (1) a task representation that consistently handles complex robot task design, (2) an active performance tracking mechanism that allows adaptive curriculum generation appropriate for the robot's current capabilities, and (3) a grounding approach that maintains target domain relevance through alternating sampling between reference and synthetic tasks. We validate GACL on wheeled navigation in constrained environments and quadruped locomotion in challenging 3D confined spaces, achieving 6.8% and 6.1% higher success rates, respectively, than state-of-the-art methods in each domain.", 'abstract_zh': '基于地面适应性的机器人 Curriculum 学习', 'title_zh': '基于主动任务和性能监控的接地自适应课程学习'}
{'arxiv_id': 'arXiv:2508.02984', 'title': 'Estimation of Aerodynamics Forces in Dynamic Morphing Wing Flight', 'authors': 'Bibek Gupta, Mintae Kim, Albert Park, Eric Sihite, Koushil Sreenath, Alireza Ramezani', 'link': 'https://arxiv.org/abs/2508.02984', 'abstract': "Accurate estimation of aerodynamic forces is essential for advancing the control, modeling, and design of flapping-wing aerial robots with dynamic morphing capabilities. In this paper, we investigate two distinct methodologies for force estimation on Aerobat, a bio-inspired flapping-wing platform designed to emulate the inertial and aerodynamic behaviors observed in bat flight. Our goal is to quantify aerodynamic force contributions during tethered flight, a crucial step toward closed-loop flight control. The first method is a physics-based observer derived from Hamiltonian mechanics that leverages the concept of conjugate momentum to infer external aerodynamic forces acting on the robot. This observer builds on the system's reduced-order dynamic model and utilizes real-time sensor data to estimate forces without requiring training data. The second method employs a neural network-based regression model, specifically a multi-layer perceptron (MLP), to learn a mapping from joint kinematics, flapping frequency, and environmental parameters to aerodynamic force outputs. We evaluate both estimators using a 6-axis load cell in a high-frequency data acquisition setup that enables fine-grained force measurements during periodic wingbeats. The conjugate momentum observer and the regression model demonstrate strong agreement across three force components (Fx, Fy, Fz).", 'abstract_zh': '准确估算气动 forces 对推进具有动态变形能力的拍翼飞行机器人控制、建模和设计至关重要。本文探讨了两种用于 Aerobat（一种受 bat 飞行行为启发的拍翼飞行平台）气动 force 估算的方法。我们的目标是在牵索飞行中量化气动 force 贡献，这是实现闭环飞行控制的关键步骤。第一种方法是基于物理的观察器，来自哈密尔顿力学，利用共轭动量的概念来推断作用在机器人上的外部气动 force。该观察器基于系统的降阶动力学模型，并利用实时传感器数据进行 force 估计，无需训练数据。第二种方法采用基于神经网络的回归模型，具体为多层感知器（MLP），从关节运动学、拍动频率和环境参数到气动 force 输出建立映射关系。我们使用六轴载荷传感器在高频率数据采集设置中评估两种估算器，该设置能够提供周期性拍打过程中精细的 force 测量。共轭动量观察器和回归模型在三个 force 组件（Fx, Fy, Fz）上表现出较强的一致性。', 'title_zh': '动态变形翼飞行的气动 forces 估计'}
{'arxiv_id': 'arXiv:2508.02982', 'title': 'Multimodal Human-Intent Modeling for Contextual Robot-to-Human Handovers of Arbitrary Objects', 'authors': 'Lucas Chen, Guna Avula, Hanwen Ren, Zixing Wang, Ahmed H. Qureshi', 'link': 'https://arxiv.org/abs/2508.02982', 'abstract': 'Human-robot object handover is a crucial element for assistive robots that aim to help people in their daily lives, including elderly care, hospitals, and factory floors. The existing approaches to solving these tasks rely on pre-selected target objects and do not contextualize human implicit and explicit preferences for handover, limiting natural and smooth interaction between humans and robots. These preferences can be related to the target object selection from the cluttered environment and to the way the robot should grasp the selected object to facilitate desirable human grasping during handovers. Therefore, this paper presents a unified approach that selects target distant objects using human verbal and non-verbal commands and performs the handover operation by contextualizing human implicit and explicit preferences to generate robot grasps and compliant handover motion sequences. We evaluate our integrated framework and its components through real-world experiments and user studies with arbitrary daily-life objects. The results of these evaluations demonstrate the effectiveness of our proposed pipeline in handling object handover tasks by understanding human preferences. Our demonstration videos can be found at this https URL.', 'abstract_zh': '人类与机器人物体交接是旨在帮助人们日常生活的辅助机器人的一项 crucial 元素，包括老年人护理、医院和工厂地板。现有的方法依赖于预先选定的目标物体，并未能将人类的显式和隐式交接偏好纳入考量，从而限制了人类与机器人之间的自然和流畅互动。这些偏好与从杂乱环境中选择目标物体以及机器人如何抓取选定物体以促进顺利交接有关。因此，本文提出了一种统一的方法，使用人类的口头和非口头命令选择远距离目标物体，并通过考虑人类的显式和隐式偏好来执行交接操作以生成机器人的抓取和符合人体工程学的交接运动序列。我们通过随意的日常生活物体的真实世界实验和用户研究评估了我们集成框架及其组件。这些评估的结果证明了我们提出的处理物体交接任务的管道的有效性。我们的演示视频可以在以下链接找到：this https URL。', 'title_zh': '多模态人类意图建模以实现任意物体的上下文机器人到人类的手递'}
{'arxiv_id': 'arXiv:2508.02976', 'title': 'Physics-informed Neural Time Fields for Prehensile Object Manipulation', 'authors': 'Hanwen Ren, Ruiqi Ni, Ahmed H. Qureshi', 'link': 'https://arxiv.org/abs/2508.02976', 'abstract': "Object manipulation skills are necessary for robots operating in various daily-life scenarios, ranging from warehouses to hospitals. They allow the robots to manipulate the given object to their desired arrangement in the cluttered environment. The existing approaches to solving object manipulations are either inefficient sampling based techniques, require expert demonstrations, or learn by trial and error, making them less ideal for practical scenarios. In this paper, we propose a novel, multimodal physics-informed neural network (PINN) for solving object manipulation tasks. Our approach efficiently learns to solve the Eikonal equation without expert data and finds object manipulation trajectories fast in complex, cluttered environments. Our method is multimodal as it also reactively replans the robot's grasps during manipulation to achieve the desired object poses. We demonstrate our approach in both simulation and real-world scenarios and compare it against state-of-the-art baseline methods. The results indicate that our approach is effective across various objects, has efficient training compared to previous learning-based methods, and demonstrates high performance in planning time, trajectory length, and success rates. Our demonstration videos can be found at this https URL.", 'abstract_zh': '机器人在仓库到医院等各种日常生活场景下的物体操作技能是必要的。它们使机器人能够将给定物体在拥挤环境中重新排列到期望的布局。现有的物体操作解决方案要么基于低效的采样技术，要么需要专家演示，或者通过试错学习，这使得它们在实际场景中不太理想。在本文中，我们提出了一种新颖的、多模态物理知情神经网络（PINN）来解决物体操作任务。我们的方法能够高效地学习解决Eikonal方程，而无需专家数据，并能够快速在复杂拥挤环境中找到物体操作轨迹。我们的方法是多模态的，因为它还在操作过程中reactively重新规划机器人的抓取，以实现期望的物体姿态。我们在模拟和真实世界场景中展示了我们的方法，并将其与最先进的基线方法进行了比较。结果表明，我们的方法在不同物体上都有效，与之前的学习方法相比，具有更高效的训练，并在规划时间、轨迹长度和成功率上表现出色。我们的演示视频可在以下链接找到：这个 https URL。', 'title_zh': '基于物理的神经时间场的手部灵巧对象操控'}
{'arxiv_id': 'arXiv:2508.02962', 'title': "Robot builds a robot's brain: AI generated drone command and control station hosted in the sky", 'authors': 'Peter Burke', 'link': 'https://arxiv.org/abs/2508.02962', 'abstract': "Advances in artificial intelligence (AI) including large language models (LLMs) and hybrid reasoning models present an opportunity to reimagine how autonomous robots such as drones are designed, developed, and validated. Here, we demonstrate a fully AI-generated drone control system: with minimal human input, an artificial intelligence (AI) model authored all the code for a real-time, self-hosted drone command and control platform, which was deployed and demonstrated on a real drone in flight as well as a simulated virtual drone in the cloud. The system enables real-time mapping, flight telemetry, autonomous mission planning and execution, and safety protocolsall orchestrated through a web interface hosted directly on the drone itself. Not a single line of code was written by a human. We quantitatively benchmark system performance, code complexity, and development speed against prior, human-coded architectures, finding that AI-generated code can deliver functionally complete command-and-control stacks at orders-of-magnitude faster development cycles, though with identifiable current limitations related to specific model context window and reasoning depth. Our analysis uncovers the practical boundaries of AI-driven robot control code generation at current model scales, as well as emergent strengths and failure modes in AI-generated robotics code. This work sets a precedent for the autonomous creation of robot control systems and, more broadly, suggests a new paradigm for robotics engineeringone in which future robots may be largely co-designed, developed, and verified by artificial intelligence. In this initial work, a robot built a robot's brain.", 'abstract_zh': '人工智能进展包括大型语言模型和混合推理模型为重新想象自主机器人如无人机的设计、开发和验证提供了机会。在此，我们演示了一个完全由人工智能生成的无人机控制系统：在 minimal 人类输入的情况下，一个人工智能模型编写了全部代码，构建了一个实时、自我托管的无人机命令与控制系统，并在真实飞行的无人机及云端模拟的虚拟无人机上进行了部署和演示。该系统通过无人机本身直接托管的网页界面实现了实时地图测绘、飞行遥测、自主任务规划与执行以及安全协议的协调。没有一行代码是由人类编写的。我们定量地将系统性能、代码复杂性和开发速度与先前的人工编码架构进行了基准测试，发现人工智能生成的代码能够在比以往快几个数量级的开发周期内交付功能完备的命令与控制堆栈，尽管目前存在特定模型上下文窗口和推理深度相关的识别局限。我们的分析揭示了当前模型规模下人工智能驱动的机器人控制代码生成的实际边界，以及人工智能生成的机器人代码中的新兴优势和失败模式。这项工作为自主创建机器人控制系统设立了先例，并更广泛地建议了机器人工程的一种新范式——即未来机器人可能主要由人工智能进行协同设计、开发和验证。在这项初步工作中，机器人构建了自己大脑。', 'title_zh': '机器人构建另一机器人的大脑：AI生成的无人机指挥控制站悬停于空中'}
{'arxiv_id': 'arXiv:2508.02947', 'title': 'AeroSafe: Mobile Indoor Air Purification using Aerosol Residence Time Analysis and Robotic Cough Emulator Testbed', 'authors': 'M Tanjid Hasan Tonmoy, Rahath Malladi, Kaustubh Singh, Forsad Al Hossain, Rajesh Gupta, Andrés E. Tejada-Martínez, Tauhidur Rahman', 'link': 'https://arxiv.org/abs/2508.02947', 'abstract': "Indoor air quality plays an essential role in the safety and well-being of occupants, especially in the context of airborne diseases. This paper introduces AeroSafe, a novel approach aimed at enhancing the efficacy of indoor air purification systems through a robotic cough emulator testbed and a digital-twins-based aerosol residence time analysis. Current portable air filters often overlook the concentrations of respiratory aerosols generated by coughs, posing a risk, particularly in high-exposure environments like healthcare facilities and public spaces. To address this gap, we present a robotic dual-agent physical emulator comprising a maneuverable mannequin simulating cough events and a portable air purifier autonomously responding to aerosols. The generated data from this emulator trains a digital twins model, combining a physics-based compartment model with a machine learning approach, using Long Short-Term Memory (LSTM) networks and graph convolution layers. Experimental results demonstrate the model's ability to predict aerosol concentration dynamics with a mean residence time prediction error within 35 seconds. The proposed system's real-time intervention strategies outperform static air filter placement, showcasing its potential in mitigating airborne pathogen risks.", 'abstract_zh': '室内空气质量在保障居住者安全和福祉方面起着至关重要的作用，尤其是在呼吸道疾病传播的背景下。本文介绍了AeroSafe，这是一种通过机器人咳嗽仿真实验台和基于数字孪生的气溶胶滞留时间分析来提高室内空气净化系统效率的新方法。当前便携式空气净化器往往忽视了咳嗽产生的呼吸道气溶胶浓度，尤其是在医院和公共场所等高暴露环境中，存在安全隐患。为解决这一问题，我们提出了一种由可移动人像模拟咳嗽事件和自主响应气溶胶的便携空气净化器组成的双智能体物理仿真系统。通过该仿生系统生成的数据训练了结合物理基础隔室模型和机器学习方法（使用长短期记忆网络和图卷积层）的数字孪生模型。实验结果表明，该模型能够预测气溶胶浓度动态，平均滞留时间预测误差在35秒以内。所提出的系统实时干预策略优于静态空气净化器布局，展示了其在降低空气传播病原体风险方面的潜力。', 'title_zh': 'AeroSafe: 基于气溶胶滞留时间分析和机器人咳嗽仿真测试平台的移动室内空气净化技术'}
{'arxiv_id': 'arXiv:2508.02930', 'title': 'Model-agnostic Meta-learning for Adaptive Gait Phase and Terrain Geometry Estimation with Wearable Soft Sensors', 'authors': 'Zenan Zhu, Wenxi Chen, Pei-Chun Kao, Janelle Clark, Lily Behnke, Rebecca Kramer-Bottiglio, Holly Yanco, Yan Gu', 'link': 'https://arxiv.org/abs/2508.02930', 'abstract': 'This letter presents a model-agnostic meta-learning (MAML) based framework for simultaneous and accurate estimation of human gait phase and terrain geometry using a small set of fabric-based wearable soft sensors, with efficient adaptation to unseen subjects and strong generalization across different subjects and terrains. Compared to rigid alternatives such as inertial measurement units, fabric-based soft sensors improve comfort but introduce nonlinearities due to hysteresis, placement error, and fabric deformation. Moreover, inter-subject and inter-terrain variability, coupled with limited calibration data in real-world deployments, further complicate accurate estimation. To address these challenges, the proposed framework integrates MAML into a deep learning architecture to learn a generalizable model initialization that captures subject- and terrain-invariant structure. This initialization enables efficient adaptation (i.e., adaptation with only a small amount of calibration data and a few fine-tuning steps) to new users, while maintaining strong generalization (i.e., high estimation accuracy across subjects and terrains). Experiments on nine participants walking at various speeds over five terrain conditions demonstrate that the proposed framework outperforms baseline approaches in estimating gait phase, locomotion mode, and incline angle, with superior accuracy, adaptation efficiency, and generalization.', 'abstract_zh': '基于MAML的新型织物基可穿戴软传感器框架：同时准确估计人类步态相位和地形几何结构', 'title_zh': '无模型泛化元学习在可穿戴软传感器辅助自适应步态相位和地形几何估计中的应用'}
{'arxiv_id': 'arXiv:2508.02898', 'title': 'Co-designing Zoomorphic Robot Concepts for Animal Welfare Education', 'authors': 'Isobel Voysey, Lynne Baillie, Joanne Williams, Michael Herrmann', 'link': 'https://arxiv.org/abs/2508.02898', 'abstract': "Animal welfare education could greatly benefit from customized robots to help children learn about animals and their behavior, and thereby promote positive, safe child-animal interactions. To this end, we ran Participatory Design workshops with animal welfare educators and children to identify key requirements for zoomorphic robots from their perspectives. Our findings encompass a zoomorphic robot's appearance, behavior, and features, as well as concepts for a narrative surrounding the robot. Through comparing and contrasting the two groups, we find the importance of: negative reactions to undesirable behavior from children; using the facial features and tail to provide cues signaling an animal's internal state; and a natural, furry appearance and texture. We also contribute some novel activities for Participatory Design with children, including branching storyboards inspired by thematic apperception tests and interactive narratives, and reflect on some of the key design challenges of achieving consensus between the groups, despite much overlap in their design concepts.", 'abstract_zh': '定制化机器人在提升动物福利教育中的应用：以促进儿童与动物的积极安全互动为例——基于与动物福利教育者和儿童的合作设计工作坊的研究', 'title_zh': '共设计Zoomorphic机器人概念以促进动物福利教育'}
{'arxiv_id': 'arXiv:2508.03692', 'title': 'LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences', 'authors': 'Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, Wei Tsang Ooi', 'link': 'https://arxiv.org/abs/2508.03692', 'abstract': 'Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community.', 'abstract_zh': '基于LiDAR的4D生成式世界模型匠心构建', 'title_zh': 'LiDARCrafter: 基于LiDAR序列的动态4D世界建模'}
{'arxiv_id': 'arXiv:2508.03293', 'title': 'Enhancing Joint Human-AI Inference in Robot Missions: A Confidence-Based Approach', 'authors': 'Duc-An Nguyen, Clara Colombatto, Steve Fleming, Ingmar Posner, Nick Hawes, Raunak Bhattacharyya', 'link': 'https://arxiv.org/abs/2508.03293', 'abstract': "Joint human-AI inference holds immense potential to improve outcomes in human-supervised robot missions. Current day missions are generally in the AI-assisted setting, where the human operator makes the final inference based on the AI recommendation. However, due to failures in human judgement on when to accept or reject the AI recommendation, complementarity is rarely achieved. We investigate joint human-AI inference where the inference made with higher confidence is selected. Through a user study with N=100 participants on a representative simulated robot teleoperation task, specifically studying the inference of robots' control delays we show that: a) Joint inference accuracy is higher and its extent is regulated by the confidence calibration of the AI agent, and b) Humans change their inferences based on AI recommendations and the extent and direction of this change is also regulated by the confidence calibration of the AI agent. Interestingly, our results show that pairing poorly-calibrated AI-DSS with humans hurts performance instead of helping the team, reiterating the need for AI-based decision support systems with good metacognitive sensitivity. To the best of our knowledge, our study presents the first application of a maximum-confidence-based heuristic for joint human-AI inference within a simulated robot teleoperation task.", 'abstract_zh': '联合人机推理在监督机器人任务中的潜在价值巨大：通过一种代表性的模拟机器人远程操作任务，基于用户研究N=100的结果表明，联合推理的准确性更高，其程度由AI代理的信心校准调节；人类会根据AI建议改变其推理，这种变化的范围和方向也由AI代理的信心校准调节。有趣的是，我们的结果表明，将信心校准不佳的AI-DSS与人类配合反而会损害团队性能，强调了需要具备良好元认知敏感性的基于AI的决策支持系统的需求。据我们所知，本研究首次在模拟机器人远程操作任务中应用了基于最大信心的联合人机推理启发式方法。', 'title_zh': '基于置信度的方法提升机器人任务中人机联合推理能力'}
{'arxiv_id': 'arXiv:2508.03120', 'title': 'Can Large Language Models Identify Materials from Radar Signals?', 'authors': 'Jiangyou Zhu, Hongyu Deng, He Chen', 'link': 'https://arxiv.org/abs/2508.03120', 'abstract': "Accurately identifying the material composition of objects is a critical capability for AI robots powered by large language models (LLMs) to perform context-aware manipulation. Radar technologies offer a promising sensing modality for material recognition task. When combined with deep learning, radar technologies have demonstrated strong potential in identifying the material of various objects. However, existing radar-based solutions are often constrained to closed-set object categories and typically require task-specific data collection to train deep learning models, largely limiting their practical applicability. This raises an important question: Can we leverage the powerful reasoning capabilities of pre-trained LLMs to directly infer material composition from raw radar signals? Answering this question is non-trivial due to the inherent redundancy of radar signals and the fact that pre-trained LLMs have no prior exposure to raw radar data during training. To address this, we introduce LLMaterial, the first study to investigate the feasibility of using LLM to identify materials directly from radar signals. First, we introduce a physics-informed signal processing pipeline that distills high-redundancy radar raw data into a set of compact intermediate parameters that encapsulate the material's intrinsic characteristics. Second, we adopt a retrieval-augmented generation (RAG) strategy to provide the LLM with domain-specific knowledge, enabling it to interpret and reason over the extracted intermediate parameters. Leveraging this integration, the LLM is empowered to perform step-by-step reasoning on the condensed radar features, achieving open-set material recognition directly from raw radar signals. Preliminary results show that LLMaterial can effectively distinguish among a variety of common materials, highlighting its strong potential for real-world material identification applications.", 'abstract_zh': '基于大规模语言模型的AI机器人通过雷达技术准确识别物体材料是一项关键能力。现有的雷达基解决方案通常受限于封闭类别物体，并且通常需要专门的任务数据集来训练深度学习模型，极大地限制了其实用性。本文提出了LLMaterial，这是首个研究利用预训练的大规模语言模型直接从雷达信号中识别材料可行性的研究。我们引入了一种基于物理的信号处理管道，将高冗余的雷达原始数据精简为一组紧凑的中间参数，这些参数捕获了材料的固有特性。同时，我们采用了检索增强生成（RAG）策略，为预训练的大规模语言模型提供领域特定知识，使其能够解释和推理提取出的中间参数。通过这种集成，预训练的大规模语言模型能够对压缩的雷达特征进行逐步推理，直接从原始雷达信号中进行开放式材料识别。初步结果表明，LLMaterial能够有效地区分多种常见材料，突显了其在实际材料识别应用中的强大潜力。', 'title_zh': '大型语言模型能否识别来自雷达信号的材料？'}
{'arxiv_id': 'arXiv:2508.02757', 'title': 'Frequency Point Game Environment for UAVs via Expert Knowledge and Large Language Model', 'authors': 'Jingpu Yang', 'link': 'https://arxiv.org/abs/2508.02757', 'abstract': 'Unmanned Aerial Vehicles (UAVs) have made significant advancements in communication stability and security through techniques such as frequency hopping, signal spreading, and adaptive interference suppression. However, challenges remain in modeling spectrum competition, integrating expert knowledge, and predicting opponent behavior. To address these issues, we propose UAV-FPG (Unmanned Aerial Vehicle - Frequency Point Game), a game-theoretic environment model that simulates the dynamic interaction between interference and anti-interference strategies of opponent and ally UAVs in communication frequency bands. The model incorporates a prior expert knowledge base to optimize frequency selection and employs large language models for path planning, simulating a "strong adversary". Experimental results highlight the effectiveness of integrating the expert knowledge base and the large language model, with the latter significantly improving path planning in dynamic scenarios through iterative interactions, outperforming fixed-path strategies. UAV-FPG provides a robust platform for advancing anti-jamming strategies and intelligent decision-making in UAV communication systems.', 'abstract_zh': '无人航空器（UAVs）通过频跳、信号扩spread、自适应干扰抑制等技术在通信稳定性和安全性方面取得了显著进展。然而，在频谱竞争建模、专家知识集成和对手行为预测方面仍存在挑战。为应对这些挑战，我们提出了UAV-FPG（无人航空器-频段博弈），这是一种博弈论环境模型，用于模拟通信频段内敌我无人航空器之间干扰与抗干扰策略的动态交互。该模型整合了先验专家知识库以优化频段选择，并利用大型语言模型进行路径规划，模拟“强对手”。实验结果表明，整合专家知识库和大型语言模型的有效性，后者通过迭代交互显著改善了动态场景下的路径规划，优于固定路径策略。UAV-FPG为无人航空器通信系统中的抗干扰策略和智能决策提供了稳健的平台。', 'title_zh': '基于专家知识和大语言模型的无人机频率点博弈环境'}
{'arxiv_id': 'arXiv:2508.03680', 'title': 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning', 'authors': 'Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna K. Qiu, Yuqing Yang', 'link': 'https://arxiv.org/abs/2508.03680', 'abstract': "We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.", 'abstract_zh': 'Agent Lightning：一种灵活可扩展的大规模语言模型基于强化学习训练的框架', 'title_zh': 'Agent Lightning: 使用强化学习训练任意AI代理'}
{'arxiv_id': 'arXiv:2508.03366', 'title': 'A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning', 'authors': 'Michael K. Chen', 'link': 'https://arxiv.org/abs/2508.03366', 'abstract': 'General logical reasoning, defined as the ability to reason deductively on domain-agnostic tasks, continues to be a challenge for large language models (LLMs). Current LLMs fail to reason deterministically and are not interpretable. As such, there has been a recent surge in interest in neurosymbolic AI, which attempts to incorporate logic into neural networks. We first identify two main neurosymbolic approaches to improving logical reasoning: (i) the integrative approach comprising models where symbolic reasoning is contained within the neural network, and (ii) the hybrid approach comprising models where a symbolic solver, separate from the neural network, performs symbolic reasoning. Both contain AI systems with promising results on domain-specific logical reasoning benchmarks. However, their performance on domain-agnostic benchmarks is understudied. To the best of our knowledge, there has not been a comparison of the contrasting approaches that answers the following question: Which approach is more promising for developing general logical reasoning? To analyze their potential, the following best-in-class domain-agnostic models are introduced: Logic Neural Network (LNN), which uses the integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the hybrid approach. Using both models as case studies and representatives of each approach, our analysis demonstrates that the hybrid approach is more promising for developing general logical reasoning because (i) its reasoning chain is more interpretable, and (ii) it retains the capabilities and advantages of existing LLMs. To support future works using the hybrid approach, we propose a generalizable framework based on LLM-SS that is modular by design, model-agnostic, domain-agnostic, and requires little to no human input.', 'abstract_zh': '神经符号AI在提升一般逻辑推理能力中的优势探究：基于最佳跨领域模型的分析', 'title_zh': '神经符号AI方法在可解释逻辑推理中的比较研究'}
{'arxiv_id': 'arXiv:2508.03038', 'title': 'Tree-of-Reasoning: Towards Complex Medical Diagnosis via Multi-Agent Reasoning with Evidence Tree', 'authors': 'Qi Peng, Jialin Cui, Jiayuan Xie, Yi Cai, Qing Li', 'link': 'https://arxiv.org/abs/2508.03038', 'abstract': 'Large language models (LLMs) have shown great potential in the medical domain. However, existing models still fall short when faced with complex medical diagnosis task in the real world. This is mainly because they lack sufficient reasoning depth, which leads to information loss or logical jumps when processing a large amount of specialized medical data, leading to diagnostic errors. To address these challenges, we propose Tree-of-Reasoning (ToR), a novel multi-agent framework designed to handle complex scenarios. Specifically, ToR introduces a tree structure that can clearly record the reasoning path of LLMs and the corresponding clinical evidence. At the same time, we propose a cross-validation mechanism to ensure the consistency of multi-agent decision-making, thereby improving the clinical reasoning ability of multi-agents in complex medical scenarios. Experimental results on real-world medical data show that our framework can achieve better performance than existing baseline methods.', 'abstract_zh': '大型语言模型（LLMs）在医学领域展现了巨大潜力。然而，现有模型在面对现实世界的复杂医学诊断任务时仍显不足。这主要是因为它们缺乏足够的推理深度，在处理大量专业化医学数据时会出现信息丢失或逻辑跳跃，导致诊断错误。为应对这些挑战，我们提出了一种新的多agent框架——Tree-of-Reasoning（ToR），旨在处理复杂场景。具体而言，ToR引入了一种树状结构，能够清晰记录LLMs的推理路径及其相应的临床证据。同时，我们提出了一种交叉验证机制以确保多agent决策的一致性，从而在复杂医疗场景中提升多agent的临床推理能力。实验结果表明，我们的框架在实际医学数据上的表现优于现有基准方法。', 'title_zh': '基于推理树的方法：通过带有证据树的多智能体推理实现复杂医疗诊断'}
{'arxiv_id': 'arXiv:2508.03018', 'title': 'Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning', 'authors': 'Yutong Wang, Pengliang Ji, Kaixin Li, Baolong Bi, Tao Feng, Guillaume Sartoretti', 'link': 'https://arxiv.org/abs/2508.03018', 'abstract': 'Large Language Reasoning Models have demonstrated remarkable success on static tasks, yet their application to multi-round agentic planning in interactive environments faces two fundamental challenges. First, the intractable credit assignment problem renders conventional reinforcement learning ineffective in sparse-reward settings. Second, the computational overhead of verbose, step-by-step reasoning histories is prohibitive. To address these challenges, we propose BPO, a three-stage framework (bootstrapping, extrapolation, and refinement) that establishes a self-improving data flywheel to develop robust reasoning models for long-horizon, sparse-reward environments. Our framework first bootstraps efficient reasoning using the proposed planning quaternions with long-short chain-of-thought fusion. It then extrapolates to out-of-distribution tasks through complexity-stratified curriculum learning. Finally, the model iteratively refines itself by learning exclusively on experiences selected via reward-gated rejection sampling. Experiments on ALFWorld, ScienceWorld, and WebShop demonstrate that our approach achieves state-of-the-art with significant token efficiency, providing a new recipe for reasoning models in agentic planning.', 'abstract_zh': '大规模语言推理模型在静态任务上已经取得了显著成功，但在交互环境中应用于多轮智能规划面临两大根本挑战。首先，难以归因的问题使得在稀疏奖励设置中传统的强化学习变得无效。其次，冗长的逐步推理历史计算开销巨大。为应对这些挑战，我们提出了一种名为BPO的三阶段框架（启动、外推和细化），该框架建立了一个自我改进的数据飞轮，以开发适用于长视角和稀疏奖励环境的稳健推理模型。该框架首先利用提出的规划四元数结合长短期思考融合来启动高效的推理。然后，通过分层复杂性课程学习向外推到分布外任务。最后，模型通过奖励门控拒绝抽样选择的经验进行迭代细化。在ALFWorld、ScienceWorld和WebShop上的实验表明，我们的方法在显著提高_token效率的同时达到了最佳性能，为智能规划中的推理模型提供了一个新的解决方案。', 'title_zh': '超越策略优化：稀疏奖励长_horizon规划的数据整理飞轮'}
{'arxiv_id': 'arXiv:2508.02789', 'title': 'Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for Science', 'authors': 'Newman Cheng, Gordon Broadbent, William Chappell', 'link': 'https://arxiv.org/abs/2508.02789', 'abstract': "The capacity for artificial intelligence (AI) to formulate, evolve, and test altered thought patterns under dynamic conditions indicates advanced cognition that is crucial for scientific discovery. The existing AI development landscape falls into two categories: 1) frameworks over non-reasoning models that natively incorporate opinions on how humans think, and 2) reasoning models that abstract precise control of the reasoning intuition away from end users. While powerful, for scientists to maximize utility of AI in scientific discovery, they not only require accuracy and transparency in reasoning, but also steerability. Hence, we introduce an alternative approach that enables deep and precise control over the reasoning process called: a cognitive loop via in-situ optimization (CLIO). CLIO enables large language models (LLMs) to self-formulate ways of approaching a problem, adapt behavior when self-confidence is low, and ultimately provide scientists with a final belief or answer. Through CLIO's open design, scientists can observe uncertainty levels, understand how final belief states are formulated using graph structures, and interject corrections. Without any further post-training, OpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37\\% in text-based biology and medicine questions on Humanity's Last Exam (HLE). This yields a 13.82\\% net or 161.64\\% relative increase when compared to the base GPT-4.1 model and surpasses OpenAI's o3 performance in high and low reasoning effort modes. We further discovered that oscillations within internal uncertainty measures are key in determining the accuracy of CLIO's results, revealing how its open design and internal mechanisms can provide insight and control into scientific decision-making processes.", 'abstract_zh': '人工智能在动态条件下制定、演变和测试改变化想模式的能力表明了高级认知，这是科学研究的关键。现有的人工智能开发景观主要分为两类：1）非推理模型的框架，这些模型原生地包含了人类思考方式的意见；2）推理模型，这些模型将推理直觉的精确控制移交给终端用户。虽然这些模型具有强大的功能，但为了科学家能够最大化人工智能在科学研究中的实用性，他们不仅需要推理的准确性和透明性，还需要可控性。因此，我们提出了一种替代方法，即通过现场优化的认知循环（CLIO）来实现对推理过程的深入和精确控制。CLIO使大型语言模型（LLMs）能够自主制定解决问题的方法，在自我信心较低时调整行为，并最终为科学家提供最终信念或答案。通过CLIO的开放设计，科学家可以观察不确定性水平，理解最终信念状态是如何通过图结构形成的，并插入修正。在没有任何进一步的后训练情况下，使用CLIO的OpenAI GPT-4.1在《人类最后的考试》（HLE）的基于文本的生物学和医学问题上达到了22.37%的准确率，比基线GPT-4.1模型提高了13.82%的净增益或161.64%的相对增益，并超过了OpenAI的o3性能，在高推理努力模式和低推理努力模式下。我们还发现，内部不确定性度量中的振荡是决定CLIO结果准确性的关键，揭示了其开放设计和内部机制如何提供对科学研究决策过程的见解和控制。', 'title_zh': '认知循环通过就地优化：自适应科学推理'}
{'arxiv_id': 'arXiv:2508.03216', 'title': 'Navigation Pixie: Implementation and Empirical Study Toward On-demand Navigation Agents in Commercial Metaverse', 'authors': 'Hikari Yanagawa, Yuichi Hiroi, Satomi Tokida, Yuji Hatada, Takefumi Hiraki', 'link': 'https://arxiv.org/abs/2508.03216', 'abstract': "While commercial metaverse platforms offer diverse user-generated content, they lack effective navigation assistance that can dynamically adapt to users' interests and intentions. Although previous research has investigated on-demand agents in controlled environments, implementation in commercial settings with diverse world configurations and platform constraints remains challenging.\nWe present Navigation Pixie, an on-demand navigation agent employing a loosely coupled architecture that integrates structured spatial metadata with LLM-based natural language processing while minimizing platform dependencies, which enables experiments on the extensive user base of commercial metaverse platforms. Our cross-platform experiments on commercial metaverse platform Cluster with 99 PC client and 94 VR-HMD participants demonstrated that Navigation Pixie significantly increased dwell time and free exploration compared to fixed-route and no-agent conditions across both platforms. Subjective evaluations revealed consistent on-demand preferences in PC environments versus context-dependent social perception advantages in VR-HMD. This research contributes to advancing VR interaction design through conversational spatial navigation agents, establishes cross-platform evaluation methodologies revealing environment-dependent effectiveness, and demonstrates empirical experimentation frameworks for commercial metaverse platforms.", 'abstract_zh': '商业元宇宙平台提供了丰富的用户生成内容，但缺乏能够动态适应用户兴趣和意图的有效导航辅助。尽管先前的研究已经探索了在受控环境中按需代理，但在具有多样世界配置和平台限制的商业环境中实施仍然是个挑战。\n我们提出了Navigation Pixie，这是一种采用松耦合架构的按需导航代理，将结构化空间元数据与基于LLM的自然语言处理相结合，同时减少对平台的依赖性，从而使实验能够在商业元宇宙平台的庞大用户基础上进行。我们在商业元宇宙平台Cluster上的跨平台实验，使用99台PC客户端和94名VR-HMD参与者，结果显示，与固定路线和无代理条件相比，Navigation Pixie 显著增加了停留时间和自由探索。主观评价表明，在PC环境中对按需代理有持续的偏好，而在VR-HMD中则表现出情境依赖性的社会感知优势。本研究通过会话式空间导航代理推进了VR交互设计，确立了跨平台评估方法以揭示环境依赖性效果，并展示了商业元宇宙平台的实证实验框架。', 'title_zh': '导航Pixie：面向商业元宇宙的按需导航代理实现与实证研究'}
{'arxiv_id': 'arXiv:2508.03012', 'title': 'Tool-integrated Reinforcement Learning for Repo Deep Search', 'authors': 'Zexiong Ma, Chao Peng, Qunhong Zeng, Pengfei Gao, Yanzhen Zou, Bing Xie', 'link': 'https://arxiv.org/abs/2508.03012', 'abstract': "Issue localization, the process of identifying code locations that need modification to resolve software issues, is a critical yet challenging task in software development. The semantic gap between natural language issue descriptions and faulty code requires complex multi-hop reasoning through code dependencies. Existing LLM-based agents attempt to address this by integrating repository retrieval tools. However, this transforms issue localization into a demanding task we call Repo Deep Search, which requires the LLM to effectively utilize various repository retrieval tools throughout a multi-step reasoning and navigation process. To tackle this challenge, we present ToolTrain, a two-stage tool-integrated training framework combining rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning to enhance LLMs' ability to use retrieval tools for issue localization. Experimental results show that ToolTrain-trained models achieve state-of-the-art performance, with our 32B model even surpassing Claude-3.7 on function-level localization. The results also show that improved localization performance translates to better end-to-end issue resolution performance. This further demonstrates that training for issue localization is a viable and effective strategy for improving automated software development.", 'abstract_zh': '软件问题定位，即识别需要修改以解决软件问题的代码位置的过程，在软件开发中是一项关键且具有挑战性的任务。自然语言问题描述与故障代码之间的语义差距需要通过代码依赖关系进行复杂的多跳推理。现有的基于LLM的代理试图通过集成存储库检索工具来解决这一问题。然而，这将问题定位转化为一项称为Repo Deep Search的有要求的任务，要求LLM在多步推理和导航过程中有效利用各种存储库检索工具。为应对这一挑战，我们提出了ToolTrain，这是一个结合了拒绝抽样监督微调和工具集成强化学习的两阶段工具集成训练框架，以增强LLM使用检索工具进行问题定位的能力。实验结果表明，ToolTrain训练的模型达到了最先进的性能，我们的32B模型甚至在功能级别定位上超越了Claude-3.7。结果显示，增强的问题定位性能转化为更好的端到端问题解决性能。这进一步证明了针对问题定位进行训练是提高自动化软件开发的一种可行且有效的方法。', 'title_zh': '集成工具的强化学习仓库深度搜索'}
{'arxiv_id': 'arXiv:2508.02856', 'title': 'Secure mmWave Beamforming with Proactive-ISAC Defense Against Beam-Stealing Attacks', 'authors': 'Seyed Bagher Hashemi Natanzi, Hossein Mohammadi, Bo Tang, Vuk Marojevic', 'link': 'https://arxiv.org/abs/2508.02856', 'abstract': 'Millimeter-wave (mmWave) communication systems face increasing susceptibility to advanced beam-stealing attacks, posing a significant physical layer security threat. This paper introduces a novel framework employing an advanced Deep Reinforcement Learning (DRL) agent for proactive and adaptive defense against these sophisticated attacks. A key innovation is leveraging Integrated Sensing and Communications (ISAC) capabilities for active, intelligent threat assessment. The DRL agent, built on a Proximal Policy Optimization (PPO) algorithm, dynamically controls ISAC probing actions to investigate suspicious activities. We introduce an intensive curriculum learning strategy that guarantees the agent experiences successful detection during training to overcome the complex exploration challenges inherent to such a security-critical task. Consequently, the agent learns a robust and adaptive policy that intelligently balances security and communication performance. Numerical results demonstrate that our framework achieves a mean attacker detection rate of 92.8% while maintaining an average user SINR of over 13 dB.', 'abstract_zh': '毫米波（mmWave）通信系统面临高级波束盗窃攻击的日益加剧的威胁，这构成了重要的物理层安全挑战。本文提出了一种新型框架，利用高级深度强化学习（DRL）代理进行主动和自适应防御，以应对这些复杂的攻击。一个关键创新在于利用综合传感与通信（ISAC）能力进行主动、智能的威胁评估。基于近端策略优化（PPO）算法构建的DRL代理，动态控制ISAC探测动作以调查可疑活动。我们引入了一种密集的 curriculum 学习策略，确保代理在训练过程中能够体验到成功的检测，从而克服此类关键安全任务中固有的复杂探索挑战。因此，代理学习到一种既强壮又适应的策略，能够智能地平衡安全与通信性能。数值结果表明，我们的框架在保持平均用户SINR超过13 dB的同时，实现了92.8%的平均攻击者检测率。', 'title_zh': '对抗波束盗用攻击的主动-ISAC防御的 secure mmWave 波束形成'}
{'arxiv_id': 'arXiv:2508.02808', 'title': 'Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation', 'authors': 'Radhika Dua, Young Joon, Kwon, Siddhant Dogra, Daniel Freedman, Diana Ruan, Motaz Nashawaty, Danielle Rigau, Daniel Alexander Alber, Kang Zhang, Kyunghyun Cho, Eric Karl Oermann', 'link': 'https://arxiv.org/abs/2508.02808', 'abstract': 'Radiological imaging is central to diagnosis, treatment planning, and clinical decision-making. Vision-language foundation models have spurred interest in automated radiology report generation (RRG), but safe deployment requires reliable clinical evaluation of generated reports. Existing metrics often rely on surface-level similarity or behave as black boxes, lacking interpretability. We introduce ICARE (Interpretable and Clinically-grounded Agent-based Report Evaluation), an interpretable evaluation framework leveraging large language model agents and dynamic multiple-choice question answering (MCQA). Two agents, each with either the ground-truth or generated report, generate clinically meaningful questions and quiz each other. Agreement on answers captures preservation and consistency of findings, serving as interpretable proxies for clinical precision and recall. By linking scores to question-answer pairs, ICARE enables transparent, and interpretable assessment. Clinician studies show ICARE aligns significantly more with expert judgment than prior metrics. Perturbation analyses confirm sensitivity to clinical content and reproducibility, while model comparisons reveal interpretable error patterns.', 'abstract_zh': '基于可解释性的临床 grounded 代理报告评估框架（ICARE）', 'title_zh': '基于临床 grounding 的基于代理的报告评估：放射学报告生成的可解释性度量标准'}
{'arxiv_id': 'arXiv:2508.02766', 'title': 'The Silicon Reasonable Person: Can AI Predict How Ordinary People Judge Reasonableness?', 'authors': 'Yonathan A. Arbel', 'link': 'https://arxiv.org/abs/2508.02766', 'abstract': "In everyday life, people make countless reasonableness judgments that determine appropriate behavior in various contexts. Predicting these judgments challenges the legal system, as judges' intuitions may not align with broader societal views. This Article investigates whether large language models (LLMs) can learn to identify patterns driving human reasonableness judgments.\nUsing randomized controlled trials comparing humans and models across multiple legal contexts with over 10,000 simulated judgments, we demonstrate that certain models capture not just surface-level responses but potentially their underlying decisional architecture. Strikingly, these systems prioritize social cues over economic efficiency in negligence determinations, mirroring human behavior despite contradicting textbook treatments.\nThese findings suggest practical applications: judges could calibrate intuitions against broader patterns, lawmakers could test policy interpretations, and resource-constrained litigants could preview argument reception. As AI agents increasingly make autonomous real-world decisions, understanding whether they've internalized recognizable ethical frameworks becomes essential for anticipating their behavior.", 'abstract_zh': '日常生活中的合理判断人们在各种情境下做出了无数的合理判断，以决定合适的行为。预测这些判断对法律体系构成了挑战，因为法官的直觉可能不与更广泛的社会观点一致。本文研究大型语言模型（LLMs）是否能够学习识别驱动人类合理判断的模式。通过涉及多个法律情境的超过10,000次模拟判断的随机对照试验，我们表明某些模型不仅捕捉到了表面级别的响应，还可能捕捉到了其潜在的决策架构。令人惊讶的是，这些系统在过失判定中优先考虑社会线索而非经济效率，这种行为尽管与教科书上的处理方式相矛盾，但仍与人类行为一致。这些发现暗示了实际应用：法官可以将直觉与更广泛的模式进行校准，立法者可以测试政策解释，而资源有限的诉讼方可以预见到论点的接受度。随着AI代理越来越多地做出自主的现实世界决策，了解它们是否内化了可识别的伦理框架变得愈发重要，以便预测其行为。', 'title_zh': '硅基理性人：AI能否预测普通人的合理性判断？'}
