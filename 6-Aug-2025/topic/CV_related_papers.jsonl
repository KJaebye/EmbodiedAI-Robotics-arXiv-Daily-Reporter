{'arxiv_id': 'arXiv:2508.02919', 'title': 'Context-aware Risk Assessment and Its Application in Autonomous Driving', 'authors': 'Boyang Tian, Weisong Shi', 'link': 'https://arxiv.org/abs/2508.02919', 'abstract': 'Ensuring safety in autonomous driving requires precise, real-time risk assessment and adaptive behavior. Prior work on risk estimation either outputs coarse, global scene-level metrics lacking interpretability, proposes indicators without concrete integration into autonomous systems, or focuses narrowly on specific driving scenarios. We introduce the Context-aware Risk Index (CRI), a light-weight modular framework that quantifies directional risks based on object kinematics and spatial relationships, dynamically adjusting control commands in real time. CRI employs direction-aware spatial partitioning within a dynamic safety envelope using Responsibility-Sensitive Safety (RSS) principles, a hybrid probabilistic-max fusion strategy for risk aggregation, and an adaptive control policy for real-time behavior modulation. We evaluate CRI on the Bench2Drive benchmark comprising 220 safety-critical scenarios using a state-of-the-art end-to-end model Transfuser++ on challenging routes. Our collision-rate metrics show a 19\\% reduction (p = 0.003) in vehicle collisions per failed route, a 20\\% reduction (p = 0.004) in collisions per kilometer, a 17\\% increase (p = 0.016) in composed driving score, and a statistically significant reduction in penalty scores (p = 0.013) with very low overhead (3.6 ms per decision cycle). These results demonstrate that CRI substantially improves safety and robustness in complex, risk-intensive environments while maintaining modularity and low runtime overhead.', 'abstract_zh': '确保自动驾驶安全需要精确的实时风险评估和适应性行为。我们引入了上下文感知风险指数（CRI），这是一种基于轻量级模块化框架，根据物体动力学和空间关系量化方向性风险，并实时动态调整控制命令。CRI采用责任敏感安全（RSS）原则内的方向感知空间分区，在动态安全包络内，采用混合概率-最大化融合策略进行风险聚合，并采用适应性控制策略进行实时行为调节。我们在包含220个安全关键场景的Bench2Drive基准上评估了CRI，使用最先进的端到端模型Transfuser++在具有挑战性的路段上进行评估。我们的碰撞率指标显示，CRI使每条失败路线的车辆碰撞减少19%（p = 0.003）、每公里碰撞减少20%（p = 0.004）、综合驾驶评分增加17%（p = 0.016），并且显著降低了惩罚分（p = 0.013），同时具有极低的运行时开销（每决策周期3.6毫秒）。这些结果表明，CRI在复杂、高风险环境中显著提高了安全性和鲁棒性，同时保持了模块化和低运行时开销。', 'title_zh': '基于上下文的风险评估及其在自主驾驶中的应用'}
{'arxiv_id': 'arXiv:2508.03691', 'title': 'La La LiDAR: Large-Scale Layout Generation from LiDAR Data', 'authors': 'Youquan Liu, Lingdong Kong, Weidong Yang, Xin Li, Ao Liang, Runnan Chen, Ben Fei, Tongliang Liu', 'link': 'https://arxiv.org/abs/2508.03691', 'abstract': 'Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model ("La La LiDAR"), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation.', 'abstract_zh': '大规模布局引导的LiDAR生成模型（La La LiDAR）', 'title_zh': 'La La LiDAR: 基于LiDAR数据的大规模布局生成'}
{'arxiv_id': 'arXiv:2508.03690', 'title': 'Veila: Panoramic LiDAR Generation from a Monocular RGB Image', 'authors': 'Youquan Liu, Lingdong Kong, Weidong Yang, Ao Liang, Jianxiong Gao, Yang Wu, Xiang Xu, Xin Li, Linfeng Li, Runnan Chen, Ben Fei', 'link': 'https://arxiv.org/abs/2508.03690', 'abstract': 'Realistic and controllable panoramic LiDAR data generation is critical for scalable 3D perception in autonomous driving and robotics. Existing methods either perform unconditional generation with poor controllability or adopt text-guided synthesis, which lacks fine-grained spatial control. Leveraging a monocular RGB image as a spatial control signal offers a scalable and low-cost alternative, which remains an open problem. However, it faces three core challenges: (i) semantic and depth cues from RGB are vary spatially, complicating reliable conditioning generation; (ii) modality gaps between RGB appearance and LiDAR geometry amplify alignment errors under noisy diffusion; and (iii) maintaining structural coherence between monocular RGB and panoramic LiDAR is challenging, particularly in non-overlap regions between images and LiDAR. To address these challenges, we propose Veila, a novel conditional diffusion framework that integrates: a Confidence-Aware Conditioning Mechanism (CACM) that strengthens RGB conditioning by adaptively balancing semantic and depth cues according to their local reliability; a Geometric Cross-Modal Alignment (GCMA) for robust RGB-LiDAR alignment under noisy diffusion; and a Panoramic Feature Coherence (PFC) for enforcing global structural consistency across monocular RGB and panoramic LiDAR. Additionally, we introduce two metrics, Cross-Modal Semantic Consistency and Cross-Modal Depth Consistency, to evaluate alignment quality across modalities. Experiments on nuScenes, SemanticKITTI, and our proposed KITTI-Weather benchmark demonstrate that Veila achieves state-of-the-art generation fidelity and cross-modal consistency, while enabling generative data augmentation that improves downstream LiDAR semantic segmentation.', 'abstract_zh': '现实可控的大范围LiDAR数据生成对于自主驾驶和机器人领域的可扩展三维感知至关重要。现有的方法要么无法控制生成过程，要么采用文本引导合成，缺乏精细的空间控制。利用单目RGB图像作为空间控制信号提供了一种可扩展且成本低的替代方案，但这一问题仍是一个开放性问题。然而，它面临三个核心挑战：（i）RGB图像中的语义和深度线索在空间上变化，使得可靠的条件生成变得复杂；（ii）RGB外观和LiDAR几何之间的模态差异在噪声扩散下加剧了对齐误差；（iii）在图像和LiDAR之间存在不重叠区域时，保持单目RGB和大范围LiDAR之间的结构连贯性具有挑战性。为了解决这些挑战，我们提出了Veila，一种新颖的条件扩散框架，该框架整合了：一种语义可信度感知条件机制（CACM），通过适应性平衡语义和深度线索来加强RGB条件化，根据它们的局部可靠性；一种几何跨模态对齐（GCMA），以在噪声扩散下实现鲁棒的RGB-LiDAR对齐；以及一种全景特征连贯性（PFC），以确保单目RGB和大范围LiDAR之间的一致结构连贯性。此外，我们引入了两种新的评估指标：跨模态语义一致性和平面交叉模态深度一致性，以评估不同模态之间的对齐质量。在nuScenes、SemanticKITTI和我们提出的KITTI-Weather基准上的实验表明，Veila不仅实现了最先进的生成保真度和跨模态一致性，还支持生成数据增强，从而提高下游LiDAR语义分割的性能。', 'title_zh': 'Veila：基于单目RGB图像的全景LiDAR生成'}
{'arxiv_id': 'arXiv:2508.03669', 'title': 'OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the Real World', 'authors': 'Katherine Liu, Sergey Zakharov, Dian Chen, Takuya Ikeda, Greg Shakhnarovich, Adrien Gaidon, Rares Ambrus', 'link': 'https://arxiv.org/abs/2508.03669', 'abstract': 'We would like to estimate the pose and full shape of an object from a single observation, without assuming known 3D model or category. In this work, we propose OmniShape, the first method of its kind to enable probabilistic pose and shape estimation. OmniShape is based on the key insight that shape completion can be decoupled into two multi-modal distributions: one capturing how measurements project into a normalized object reference frame defined by the dataset and the other modelling a prior over object geometries represented as triplanar neural fields. By training separate conditional diffusion models for these two distributions, we enable sampling multiple hypotheses from the joint pose and shape distribution. OmniShape demonstrates compelling performance on challenging real world datasets. Project website: this https URL', 'abstract_zh': '我们希望从单次观测中估计物体的姿态和完整形状，无需假设已知3D模型或类别。在本文中，我们提出了OmniShape，这是首个能够进行概率姿态和形状估计的方法。OmniShape 基于一个关键洞察，即形状完成可以分解为两个多模态分布：一个捕获测量如何投影到由数据集定义的归一化对象参考框架中，另一个则建模对象几何形状（表示为三方面神经场）的先验。通过为这两个分布分别训练条件扩散模型，我们可以在联合姿态和形状分布中采样多个假设。OmniShape 在具有挑战性的现实世界数据集上展示了令人信服的性能。项目网站：这个 https://链接。', 'title_zh': '全知形状：零-shot 多假设形状和姿态估计于现实世界'}
{'arxiv_id': 'arXiv:2508.03331', 'title': 'LRDDv2: Enhanced Long-Range Drone Detection Dataset with Range Information and Comprehensive Real-World Challenges', 'authors': 'Amirreza Rouhi, Sneh Patel, Noah McCarthy, Siddiqa Khan, Hadi Khorsand, Kaleb Lefkowitz, David K.Han', 'link': 'https://arxiv.org/abs/2508.03331', 'abstract': "The exponential growth in Unmanned Aerial Vehicles (UAVs) usage underscores the critical need of detecting them at extended distances to ensure safe operations, especially in densely populated areas. Despite the tremendous advances made in computer vision through deep learning, the detection of these small airborne objects remains a formidable challenge. While several datasets have been developed specifically for drone detection, the need for a more extensive and diverse collection of drone image data persists, particularly for long-range detection under varying environmental conditions. We introduce here the Long Range Drone Detection (LRDD) Version 2 dataset, comprising 39,516 meticulously annotated images, as a second release of the LRDD dataset released previously. The LRDDv2 dataset enhances the LRDDv1 by incorporating a greater variety of images, providing a more diverse and comprehensive resource for drone detection research. What sets LRDDv2 apart is its inclusion of target range information for over 8,000 images, making it possible to develop algorithms for drone range estimation. Tailored for long-range aerial object detection, the majority of LRDDv2's dataset consists of images capturing drones with 50 or fewer pixels in 1080p resolution. For access to the complete Long-Range Drone Detection Dataset (LRDD)v2, please visit this https URL .", 'abstract_zh': '长程无人机检测数据集（LRDD）版本2', 'title_zh': 'LRDDv2：带有距离信息和全面现实世界挑战的增强长距无人机检测数据集'}
{'arxiv_id': 'arXiv:2508.03132', 'title': 'COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks', 'authors': 'Arion Zimmermann, Soon-Jo Chung, Fred Hadaegh', 'link': 'https://arxiv.org/abs/2508.03132', 'abstract': "The accurate state estimation of unknown bodies in space is a critical challenge with applications ranging from the tracking of space debris to the shape estimation of small bodies. A necessary enabler to this capability is to find and track features on a continuous stream of images. Existing methods, such as SIFT, ORB and AKAZE, achieve real-time but inaccurate pose estimates, whereas modern deep learning methods yield higher quality features at the cost of more demanding computational resources which might not be available on space-qualified hardware. Additionally, both classical and data-driven methods are not robust to the highly opaque self-cast shadows on the object of interest. We show that, as the target body rotates, these shadows may lead to large biases in the resulting pose estimates. For these objects, a bias in the real-time pose estimation algorithm may mislead the spacecraft's state estimator and cause a mission failure, especially if the body undergoes a chaotic tumbling motion. We present COFFEE, the Celestial Occlusion Fast FEature Extractor, a real-time pose estimation framework for asteroids designed to leverage prior information on the sun phase angle given by sun-tracking sensors commonly available onboard spacecraft. By associating salient contours to their projected shadows, a sparse set of features are detected, invariant to the motion of the shadows. A Sparse Neural Network followed by an attention-based Graph Neural Network feature matching model are then jointly trained to provide a set of correspondences between successive frames. The resulting pose estimation pipeline is found to be bias-free, more accurate than classical pose estimation pipelines and an order of magnitude faster than other state-of-the-art deep learning pipelines on synthetic data as well as on renderings of the tumbling asteroid Apophis.", 'abstract_zh': '实时估计小计算未知未知小体的姿态在航天领域中是一个关键挑战，应用场景从空间碎片跟踪到到到到小型物体的姿态估测.现有的特征提取方法如如如like S这样的S的是基于S如S的技术如，如is这样的技术在实时候并不能保证准确的姿态估测上。相比之下on呢，现代深度学习方法在实验on找到可以获得更高的精度on然而n在ond需要更密集的计算资源on这样的计算资源在中on-硬件上可能无法获得isis现代深度从中学习方法虽然在高遮蔽区域表现出色柔度但在目标引起的阴影也会引起波in时间估算的偏移在当前实时候姿态估测算法中这样的的偏移可能导致is航天器姿态估计错误发生错误误解on特别是当航天器进行混沌翻剧烈旋转转动时.\n\n为此我们设计is针对小以is落日长ículiónL作为参照的的实时姿态估计算法is算法设计基于星载轨道特征提取的isCoffeeis星际特征提取算法，算法为小行星设计定制设计以适应在阳光角度受变化下on的星is追踪传感器is…常见于航天器上的.\n\n通过将显著轮廓与投影阴影关联到起来一个稀疏特征集合is检测到的对变换不变is然后通过稀疏神经网络is随后通过注意机制驱动的图神经网络is共同联合作用找到一幅将先后特征间的一组对应关系is形成的结果估计管线表明is比比比以前的传统姿态估计算法更加准确同时比也快了数量级个量is比当今最先进的深度学习管道数量级倍在合成数据上is是以及对形虚拟的的翻剧烈转动的小行星阿波斯is上上上isisisisis.', 'title_zh': 'COFFEE: 一种适用于未知翻滚小行星的抗阴影实时姿态估算器基于稀疏神经网络'}
{'arxiv_id': 'arXiv:2508.03011', 'title': 'Generating Light-based Fingerprints for Indoor Localization', 'authors': 'Hsun-Yu Lee, Jie Lin, Fang-Jing Wu', 'link': 'https://arxiv.org/abs/2508.03011', 'abstract': 'Accurate indoor localization underpins applications ranging from wayfinding and emergency response to asset tracking and smart-building services. Radio-frequency solutions (e.g. Wi-Fi, RFID, UWB) are widely adopted but remain vulnerable to multipath fading, interference, and uncontrollable coverage variation. We explore an orthogonal modality -- visible light communication (VLC) -- and demonstrate that the spectral signatures captured by a low-cost AS7341 sensor can serve as robust location fingerprints.\nWe introduce a two-stage framework that (i) trains a multi-layer perceptron (MLP) on real spectral measurements and (ii) enlarges the training corpus with synthetic samples produced by TabGAN. The augmented dataset reduces the mean localization error from 62.9cm to 49.3cm -- a 20% improvement -- while requiring only 5% additional data-collection effort. Experimental results obtained on 42 reference points in a U-shaped laboratory confirm that GAN-based augmentation mitigates data-scarcity issues and enhances generalization.', 'abstract_zh': '精确的室内定位支撑着从路径引导和应急响应到资产追踪和智能建筑服务的广泛应用。基于射频的技术（如Wi-Fi、RFID、UWB）被广泛采用但仍易受多径衰落、干扰和不可控覆盖变化的影响。我们研究了一种正交的模态——可见光通信（VLC），并证明了低成本AS7341传感器捕获的光谱特征可以作为稳健的位置指纹。我们引入了一种两阶段框架，首先利用多层感知器（MLP）训练实际光谱测量数据，然后通过TabGAN生成合成样本扩展训练数据集。扩充后的数据集将平均定位误差从62.9cm降低到49.3cm，提高了20%，同时仅需额外5%的数据采集努力。在U形实验室中的42个参考点上获得的实验结果证实，基于GAN的数据增强缓解了数据稀缺问题并提升了泛化能力。', 'title_zh': '基于光的指纹室内定位生成'}
{'arxiv_id': 'arXiv:2508.03488', 'title': 'VQA support to Arabic Language Learning Educational Tool', 'authors': 'Khaled Bachir Delassi, Lakhdar Zeggane, Hadda Cherroun, Abdelhamid Haouhat, Kaoutar Bouzouad', 'link': 'https://arxiv.org/abs/2508.03488', 'abstract': "We address the problem of scarcity of educational Arabic Language Learning tools that advocate modern pedagogical models such as active learning which ensures language proficiency. In fact, we investigate the design and evaluation of an AI-powered educational tool designed to enhance Arabic language learning for non-native speakers with beginner-to-intermediate proficiency level. The tool leverages advanced AI models to generate interactive visual quizzes, deploying Visual Question Answering as the primary activity. Adopting a constructivist learning approach, the system encourages active learning through real-life visual quizzes, and image-based questions that focus on improving vocabulary, grammar, and comprehension. The system integrates Vision-Language Pretraining models to generate contextually relevant image description from which Large Language Model generate assignments based on customized Arabic language Learning quizzes thanks to prompting.\nThe effectiveness of the tool is evaluated through a manual annotated benchmark consisting of 1266 real-life visual quizzes, with human participants providing feedback. The results show a suitable accuracy rates, validating the tool's potential to bridge the gap in Arabic language education and highlighting the tool's promise as a reliable, AI-powered resource for Arabic learners, offering personalized and interactive learning experiences.", 'abstract_zh': '我们探讨了缺乏倡导现代教学模型（如主动学习）的阿拉伯语言学习工具的问题，这些模型能够确保语言 proficiency。实际上，我们研究并评估了一种基于AI的教育工具，旨在增强非母语者（初学者到中级水平）的阿拉伯语言学习。该工具利用先进的人工智能模型生成互动视觉测验，并主要采用视觉问答活动。采用建构主义学习方法，系统通过现实生活中的视觉测验和基于图像的问题来促进主动学习，这些问题着重于提高词汇、语法和理解能力。该系统整合了视觉-语言预训练模型，可以从上下文相关图像描述生成任务，大型语言模型根据自定义的阿拉伯语言学习测验产生作业。通过手动标注基准数据集（包含1266个实际生活中的视觉测验）对工具的有效性进行了评估，参与者提供了反馈。结果表明，该工具具有合理的准确性，验证了其在阿拉伯语教育中的潜在应用价值，并突显了作为一种可靠且基于AI的学习资源的潜力，提供个性化和互动的学习体验。', 'title_zh': '支援阿拉伯语言学习的问答系统教育工具'}
{'arxiv_id': 'arXiv:2508.02841', 'title': 'A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering', 'authors': 'Ziruo Yi, Jinyu Liu, Ting Xiao, Mark V. Albert', 'link': 'https://arxiv.org/abs/2508.02841', 'abstract': "Radiology visual question answering (RVQA) provides precise answers to questions about chest X-ray images, alleviating radiologists' workload. While recent methods based on multimodal large language models (MLLMs) and retrieval-augmented generation (RAG) have shown promising progress in RVQA, they still face challenges in factual accuracy, hallucinations, and cross-modal misalignment. We introduce a multi-agent system (MAS) designed to support complex reasoning in RVQA, with specialized agents for context understanding, multimodal reasoning, and answer validation. We evaluate our system on a challenging RVQA set curated via model disagreement filtering, comprising consistently hard cases across multiple MLLMs. Extensive experiments demonstrate the superiority and effectiveness of our system over strong MLLM baselines, with a case study illustrating its reliability and interpretability. This work highlights the potential of multi-agent approaches to support explainable and trustworthy clinical AI applications that require complex reasoning.", 'abstract_zh': '基于多态模态大规模预训练模型和检索增强生成的多态智能系统在胸部 X 射线 图像推理问答中的应用：一个多代理系统（MAS）的研究', 'title_zh': '一种用于放射学视觉问答的多agent系统复杂推理机制'}
{'arxiv_id': 'arXiv:2508.03625', 'title': 'AttZoom: Attention Zoom for Better Visual Features', 'authors': 'Daniel DeAlcala, Aythami Morales, Julian Fierrez, Ruben Tolosana', 'link': 'https://arxiv.org/abs/2508.03625', 'abstract': 'We present Attention Zoom, a modular and model-agnostic spatial attention mechanism designed to improve feature extraction in convolutional neural networks (CNNs). Unlike traditional attention approaches that require architecture-specific integration, our method introduces a standalone layer that spatially emphasizes high-importance regions in the input. We evaluated Attention Zoom on multiple CNN backbones using CIFAR-100 and TinyImageNet, showing consistent improvements in Top-1 and Top-5 classification accuracy. Visual analyses using Grad-CAM and spatial warping reveal that our method encourages fine-grained and diverse attention patterns. Our results confirm the effectiveness and generality of the proposed layer for improving CCNs with minimal architectural overhead.', 'abstract_zh': 'Attention Zoom：一种模块化和模型无关的空间注意力机制，用于提高卷积神经网络的特征提取', 'title_zh': 'AttZoom: 注意力缩放以获得更好的视觉特征'}
{'arxiv_id': 'arXiv:2508.03596', 'title': 'MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy', 'authors': 'Wuyang Li, Wentao Pan, Xiaoyuan Liu, Zhendong Luo, Chenxin Li, Hengyu Liu, Din Ping Tsai, Mu Ku Chen, Yixuan Yuan', 'link': 'https://arxiv.org/abs/2508.03596', 'abstract': 'Miniaturized endoscopy has advanced accurate visual perception within the human body. Prevailing research remains limited to conventional cameras employing convex lenses, where the physical constraints with millimetre-scale thickness impose serious impediments on the micro-level clinical. Recently, with the emergence of meta-optics, ultra-micro imaging based on metalenses (micron-scale) has garnered great attention, serving as a promising solution. However, due to the physical difference of metalens, there is a large gap in data acquisition and algorithm research. In light of this, we aim to bridge this unexplored gap, advancing the novel metalens endoscopy. First, we establish datasets for metalens endoscopy and conduct preliminary optical simulation, identifying two derived optical issues that physically adhere to strong optical priors. Second, we propose MetaScope, a novel optics-driven neural network tailored for metalens endoscopy driven by physical optics. MetaScope comprises two novel designs: Optics-informed Intensity Adjustment (OIA), rectifying intensity decay by learning optical embeddings, and Optics-informed Chromatic Correction (OCC), mitigating chromatic aberration by learning spatial deformations informed by learned Point Spread Function (PSF) distributions. To enhance joint learning, we further deploy a gradient-guided distillation to transfer knowledge from the foundational model adaptively. Extensive experiments demonstrate that MetaScope not only outperforms state-of-the-art methods in both metalens segmentation and restoration but also achieves impressive generalized ability in real biomedical scenes.', 'abstract_zh': '微型内窥镜在人体内部实现了精确的视觉感知。基于元光学的亚微米级成像金属镜头内窥镜已引起广泛注意，作为一种有前景的解决方案，然而，由于金属镜头的物理特性，数据采集和算法研究之间存在巨大差距。为解决这一问题，我们致力于弥合这一未开发的差距，推动新型金属镜头内窥镜的发展。首先，我们为金属镜头内窥镜建立数据集并进行初步光学仿真，发现两种与强烈光学先验紧密相关的光学问题。其次，我们提出MetaScope，一种由物理光学驱动的新型光学导向神经网络，特别针对金属镜头内窥镜。MetaScope 包含两个新颖设计：光学启发强度调整(OIA)，通过学习光学嵌入来校正强度衰减；光学启发色差校正(OCC)，通过利用学习到的点扩展函数(PSF)分布来学习空间变形以减轻色差。为了增强联合学习，我们进一步部署了梯度引导蒸馏，以适应性地传递基础模型的知识。广泛实验表明，MetaScope 不仅在金属镜头分割和恢复方面优于最先进的方法，还在实际生物医学场景中表现出令人印象深刻的泛化能力。', 'title_zh': 'MetaScope：以光学驱动的神经网络超微金属透镜内窥镜'}
{'arxiv_id': 'arXiv:2508.03543', 'title': 'EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable Text-to-Speech via Activation Steering', 'authors': 'Tianxin Xie, Shan Yang, Chenxing Li, Dong Yu, Li Liu', 'link': 'https://arxiv.org/abs/2508.03543', 'abstract': 'Text-to-speech (TTS) has shown great progress in recent years. However, most existing TTS systems offer only coarse and rigid emotion control, typically via discrete emotion labels or a carefully crafted and detailed emotional text prompt, making fine-grained emotion manipulation either inaccessible or unstable. These models also require extensive, high-quality datasets for training. To address these limitations, we propose EmoSteer-TTS, a novel training-free approach, to achieve fine-grained speech emotion control (conversion, interpolation, erasure) by activation steering. We first empirically observe that modifying a subset of the internal activations within a flow matching-based TTS model can effectively alter the emotional tone of synthesized speech. Building on this insight, we then develop a training-free and efficient algorithm, including activation extraction, emotional token searching, and inference-time steering, which can be seamlessly integrated into a wide range of pretrained models (e.g., F5-TTS, CosyVoice2, and E2-TTS). In addition, to derive effective steering vectors, we construct a curated emotional speech dataset with diverse speakers. Extensive experiments demonstrate that EmoSteer-TTS enables fine-grained, interpretable, and continuous control over speech emotion, outperforming the state-of-the-art (SOTA). To the best of our knowledge, this is the first method that achieves training-free and continuous fine-grained emotion control in TTS.', 'abstract_zh': '无需训练的激活引导语音情感控制：EmoSteer-TTS', 'title_zh': 'EmoSteer-TTS：基于激活调整的细粒度无训练要求情感可控文本转语音'}
{'arxiv_id': 'arXiv:2508.03480', 'title': 'VideoGuard: Protecting Video Content from Unauthorized Editing', 'authors': 'Junjie Cao, Kaizhou Li, Xinchun Yu, Hongxiang Li, Xiaoping Zhang', 'link': 'https://arxiv.org/abs/2508.03480', 'abstract': 'With the rapid development of generative technology, current generative models can generate high-fidelity digital content and edit it in a controlled manner. However, there is a risk that malicious individuals might misuse these capabilities for misleading activities. Although existing research has attempted to shield photographic images from being manipulated by generative models, there remains a significant disparity in the protection offered to video content editing. To bridge the gap, we propose a protection method named VideoGuard, which can effectively protect videos from unauthorized malicious editing. This protection is achieved through the subtle introduction of nearly unnoticeable perturbations that interfere with the functioning of the intended generative diffusion models. Due to the redundancy between video frames, and inter-frame attention mechanism in video diffusion models, simply applying image-based protection methods separately to every video frame can not shield video from unauthorized editing. To tackle the above challenge, we adopt joint frame optimization, treating all video frames as an optimization entity. Furthermore, we extract video motion information and fuse it into optimization objectives. Thus, these alterations can effectively force the models to produce outputs that are implausible and inconsistent. We provide a pipeline to optimize this perturbation. Finally, we use both objective metrics and subjective metrics to demonstrate the efficacy of our method, and the results show that the protection performance of VideoGuard is superior to all the baseline methods.', 'abstract_zh': '随着生成技术的快速发展，当前的生成模型可以生成高保真数字内容并以受控方式编辑。然而，恶意人员可能滥用这些能力进行误导性活动。尽管现有的研究尝试保护照片图像不被生成模型操纵，但在保护视频内容编辑方面仍存在显著差距。为弥补这一差距，我们提出了一种名为VideoGuard的保护方法，可以有效地防止视频未经授权的恶意编辑。这种保护是通过引入几乎难以察觉的 perturbations 来实现的，这些 perturbations 干扰了目标生成扩散模型的正常运行。由于视频帧之间的冗余性和视频扩散模型中的帧间注意力机制，简单地将基于图像的保护方法分别应用于每个视频帧无法屏蔽视频未经授权的编辑。为应对上述挑战，我们采用了联合帧优化的方法，将所有视频帧视为优化实体。此外，我们提取了视频运动信息并将其融合到优化目标中。因此，这些改变可以有效地迫使模型产生难以置信且不一致的输出。我们提供了一个优化此 perturbations 的管道。最后，我们使用客观指标和主观指标来证明我们方法的有效性，结果表明VideoGuard的保护性能优于所有 baseline 方法。', 'title_zh': 'VideoGuard：保护视频内容免遭未经授权的编辑'}
{'arxiv_id': 'arXiv:2508.03415', 'title': 'Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN', 'authors': 'Shivangi Nigam, Adarsh Prasad Behera, Shekhar Verma, P. Nagabhushan', 'link': 'https://arxiv.org/abs/2508.03415', 'abstract': 'This paper presents Fd-CycleGAN, an image-to-image (I2I) translation framework that enhances latent representation learning to approximate real data distributions. Building upon the foundation of CycleGAN, our approach integrates Local Neighborhood Encoding (LNE) and frequency-aware supervision to capture fine-grained local pixel semantics while preserving structural coherence from the source domain. We employ distribution-based loss metrics, including KL/JS divergence and log-based similarity measures, to explicitly quantify the alignment between real and generated image distributions in both spatial and frequency domains. To validate the efficacy of Fd-CycleGAN, we conduct experiments on diverse datasets -- Horse2Zebra, Monet2Photo, and a synthetically augmented Strike-off dataset. Compared to baseline CycleGAN and other state-of-the-art methods, our approach demonstrates superior perceptual quality, faster convergence, and improved mode diversity, particularly in low-data regimes. By effectively capturing local and global distribution characteristics, Fd-CycleGAN achieves more visually coherent and semantically consistent translations. Our results suggest that frequency-guided latent learning significantly improves generalization in image translation tasks, with promising applications in document restoration, artistic style transfer, and medical image synthesis. We also provide comparative insights with diffusion-based generative models, highlighting the advantages of our lightweight adversarial approach in terms of training efficiency and qualitative output.', 'abstract_zh': 'Fd-CycleGAN：一种增强潜在表示学习以逼近真实数据分布的图像到图像翻译框架', 'title_zh': '使用频域分布式CycleGAN学习图像转换的潜在表示'}
{'arxiv_id': 'arXiv:2508.03411', 'title': 'SlotMatch: Distilling Temporally Consistent Object-Centric Representations for Unsupervised Video Segmentation', 'authors': 'Diana-Nicoleta Grigore, Neelu Madan, Andreas Mogelmose, Thomas B. Moeslund, Radu Tudor Ionescu', 'link': 'https://arxiv.org/abs/2508.03411', 'abstract': 'Unsupervised video segmentation is a challenging computer vision task, especially due to the lack of supervisory signals coupled with the complexity of visual scenes. To overcome this challenge, state-of-the-art models based on slot attention often have to rely on large and computationally expensive neural architectures. To this end, we propose a simple knowledge distillation framework that effectively transfers object-centric representations to a lightweight student. The proposed framework, called SlotMatch, aligns corresponding teacher and student slots via the cosine similarity, requiring no additional distillation objectives or auxiliary supervision. The simplicity of SlotMatch is confirmed via theoretical and empirical evidence, both indicating that integrating additional losses is redundant. We conduct experiments on two datasets to compare the state-of-the-art teacher model, SlotContrast, with our distilled student. The results show that our student based on SlotMatch matches and even outperforms its teacher, while using 3.6x less parameters and running 1.9x faster. Moreover, our student surpasses previous unsupervised video segmentation models.', 'abstract_zh': '无监督视频分割是一个具有挑战性的计算机视觉任务，尤其是由于缺乏监督信号以及视觉场景的复杂性。为克服这一挑战，基于槽注意力的最新模型通常依赖于大型且计算成本高昂的神经架构。为此，我们提出了一种简单的知识蒸馏框架，有效地将对象为中心的表示转移到一个轻量级学生模型中。该提出的框架名为SlotMatch，通过余弦相似性对齐相应的教师和学生槽位，无需额外的蒸馏目标或辅助监督。SlotMatch的简洁性通过理论和实验证据得到证实，表明整合额外的损失是多余的。我们分别在两个数据集上进行了实验，将最新的教师模型SlotContrast与我们的蒸馏学生模型进行比较。结果显示，基于SlotMatch的学生模型不仅与教师模型表现相当，甚至在参数量减少3.6倍且运行速度快1.9倍的情况下表现更优，同时超越了之前的无监督视频分割模型。', 'title_zh': 'SlotMatch：提炼时空一致性对象中心表示以实现无监督视频分割'}
{'arxiv_id': 'arXiv:2508.03404', 'title': 'Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling', 'authors': 'Xinlei Yu, Zhangquan Chen, Yudong Zhang, Shilin Lu, Ruolin Shen, Jiangning Zhang, Xiaobin Hu, Yanwei Fu, Shuicheng Yan', 'link': 'https://arxiv.org/abs/2508.03404', 'abstract': 'Existing vision-language models (VLMs), whether generalists or specialists, remain constrained by their parameter scale, lack robust self-correction capabilities, and underperform in tasks involving long visual contexts and complex reasoning, resulting in suboptimal performance on document-based tasks. To address this, we propose MACT, a Multi-Agent Collaboration framework with Test-Time scaling, tailored for visual document understanding and visual question answering (VQA). It comprises four distinct small-scale agents, i.e., planning, execution, judgment, and answer agents, with clearly defined roles and effective collaboration. Notably, the judgment agent exclusively verifies correctness and redirects to prior agents for revisions, outperforming conventional correction strategies. To further expand the capability boundaries of the framework, we propose mixed reward modeling that balances agent-specific abilities and global collaboration, as well as agent-wise hybrid test-time scaling, which customizes different scaling strategies for each agent based on their functions. Evaluated on benchmarks spanning both document-based and non-document-based settings, our MACT shows superior performance with a smaller parameter scale without sacrificing the ability of general and mathematical tasks. Especially, it stands out in benchmarks involving long visual contexts and complicated reasoning. The three variants of MACT consistently hold the top three positions in average scores, leading in 13 of the 15 benchmarks. Code will be available at: this https URL.', 'abstract_zh': '现有的视觉-语言模型（VLMs），无论是通用型还是专用品，仍然受限于其参数规模，缺乏 robust 自我修正能力，并且在涉及长视觉上下文和复杂推理的任务中表现不佳，导致在文档任务上的表现不太理想。为了解决这一问题，我们提出了一种名为 MACT 的多Agent协作框架（Multi-Agent Collaboration framework with Test-Time scaling），专门用于视觉文档理解和视觉问答（VQA）。该框架包含四个明确分工的小规模代理，即规划代理、执行代理、判断代理和答案代理，具备有效的协作机制。值得注意的是，判断代理专门验证正确性并回传至前代理进行修正，优于传统的修正策略。为了进一步扩展框架的能力边界，我们提出了混合奖励建模，平衡代理特定能力和全局协作，以及代理级混合测试时缩放策略，根据不同代理的功能定制不同的缩放策略。在涵盖文档和非文档场景的基准测试中，MACT 在较小参数规模的情况下表现出色，而不牺牲一般和数学任务的能力。特别是在涉及长视觉上下文和复杂推理的基准测试中表现尤为突出。三种不同变体的MACT在平均得分上始终占据前三名，在15个基准测试中有13个领先。代码将在此处提供：this https URL。', 'title_zh': '视觉文档理解与问答：一种具有测试时缩放的多代理协作框架'}
{'arxiv_id': 'arXiv:2508.03254', 'title': 'V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models', 'authors': 'Jisoo Kim, Wooseok Seo, Junwan Kim, Seungho Park, Sooyeon Park, Youngjae Yu', 'link': 'https://arxiv.org/abs/2508.03254', 'abstract': "With growing interest in deploying text-to-video (T2V) models in resource-constrained environments, reducing their high computational cost has become crucial, leading to extensive research on pruning and knowledge distillation methods while maintaining performance. However, existing distillation methods primarily rely on supervised fine-tuning (SFT), which often leads to mode collapse as pruned models with reduced capacity fail to directly match the teacher's outputs, ultimately resulting in degraded quality. To address this challenge, we propose an effective distillation method, ReDPO, that integrates DPO and SFT. Our approach leverages DPO to guide the student model to focus on recovering only the targeted properties, rather than passively imitating the teacher, while also utilizing SFT to enhance overall performance. We additionally propose V.I.P., a novel framework for filtering and curating high-quality pair datasets, along with a step-by-step online approach for calibrated training. We validate our method on two leading T2V models, VideoCrafter2 and AnimateDiff, achieving parameter reduction of 36.2% and 67.5% each, while maintaining or even surpassing the performance of full models. Further experiments demonstrate the effectiveness of both ReDPO and V.I.P. framework in enabling efficient and high-quality video generation. Our code and videos are available at this https URL.", 'abstract_zh': '资源受限环境下部署文本到视频模型的兴趣日益增长，降低其高计算成本变得至关重要，推动了 pruning 和知识蒸馏方法的广泛研究，同时保持性能。然而，现有的蒸馏方法主要依赖于监督微调（SFT），这往往会导致模式崩溃，因为裁剪后的模型因容量减少而无法直接匹配教师模型的输出，最终导致质量下降。为应对这一挑战，我们提出了一种有效的蒸馏方法 ReDPO，将 DPO 和 SFT 相结合。我们的方法利用 DPO 引导学生模型专注于恢复仅目标属性，而不是被动模仿教师，同时利用 SFT 提高整体性能。我们还提出了一种新颖的 V.I.P. 框架，用于筛选和收集高质量配对数据集，并提供了一种校准的在线训练步骤。我们在两个领先的文本到视频模型 VideoCrafter2 和 AnimateDiff 上验证了该方法，分别实现了参数量减少 36.2% 和 67.5%，同时保持甚至超越完整模型的性能。进一步的实验表明，ReDPO 方法和 V.I.P. 框架在促进高效且高质量的视频生成方面均取得了成功。我们的代码和视频可在以下链接获取。', 'title_zh': 'V.I.P.：迭代在线偏好蒸馏以提高视频扩散模型的效率'}
{'arxiv_id': 'arXiv:2508.03164', 'title': 'ChartCap: Mitigating Hallucination of Dense Chart Captioning', 'authors': 'Junyoung Lim, Jaewoo Ahn, Gunhee Kim', 'link': 'https://arxiv.org/abs/2508.03164', 'abstract': 'Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions.', 'abstract_zh': '生成无幻觉、信息丰富且准确的图表 caption 对于视觉语言模型仍然具有挑战性，主要由于缺乏大规模的高质量现实图表数据集。现有现实图表数据集存在包含无法从图表推断出的信息以及未能充分捕捉结构性元素和关键洞察的问题。因此，我们引入了 ChartCap，这是一个包含 565,000 张现实图表图像的大规模数据集，每张图表配以特定类型的密集 caption，排除了多余信息并详细突出了结构性元素和关键洞察。为了构建 ChartCap，我们设计了一个四阶段管道，仅使用图表中的可辨信息生成 caption，并采用基于循环一致性的人工验证，既加速了质量控制又保持了准确性。此外，我们提出了一种新的度量标准——视觉一致性分数，通过测量从 caption 生成的图表与原始图表之间的相似度来评估 caption 质量，独立于参考 caption。广泛的实验表明，基于 ChartCap 精调的模型生成的 caption 更加准确和信息丰富，并减少了幻觉现象，超越了开源和专有模型，甚至超过了人工标注的 caption。', 'title_zh': 'ChartCap: 抑制密集图表 captions 的幻觉问题'}
{'arxiv_id': 'arXiv:2508.03127', 'title': 'Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery', 'authors': 'Sai Ma, Zhuang Li, John A Taylor', 'link': 'https://arxiv.org/abs/2508.03127', 'abstract': 'Vision language models (VLMs) that enable natural language interaction with satellite imagery can democratize Earth observation by accelerating expert workflows, making data accessible to non-specialists, and enabling planet-scale automation. However, existing datasets focus mainly on short-term, high-resolution imagery from a limited number of satellites, overlooking low-resolution, multi-satellite, long-term archives, such as Landsat, that are essential for affordable and bias-robust global monitoring. We address this gap with Landsat30-AU, a large-scale vision-language dataset built from 30-meter resolution imagery collected by four Landsat satellites (5, 7, 8, and 9) over Australia, spanning more than 36 years. The dataset includes two components: Landsat30-AU-Cap, containing 196,262 image-caption pairs, and Landsat30-AU-VQA, comprising 17,725 human-verified visual question answering (VQA) samples across eight remote sensing domains. Both datasets are curated through a bootstrapped pipeline that leverages generic VLMs with iterative refinement and human verification to ensure quality. Our evaluation of eight VLMs on our benchmark reveals that off-the-shelf models struggle to understand satellite imagery. The open-source remote-sensing VLM EarthDial achieves only 0.07 SPIDEr in captioning and a VQA accuracy of 0.48, highlighting the limitations of current approaches. Encouragingly, lightweight fine-tuning of Qwen2.5-VL-7B on Landsat30-AU improves captioning performance from 0.11 to 0.31 SPIDEr and boosts VQA accuracy from \\textbf{0.74} to 0.87. Code and data are available at this https URL.', 'abstract_zh': 'Vision语言模型（VLMs）能够与卫星图像进行自然语言交互，可以民主化地球观测，加速专家工作流程，使数据对非专家用户更易访问，并实现全球规模的自动化。然而，现有的数据集主要侧重于有限数量卫星的短时高分辨率影像，未能涵盖像landsat这样的低分辨率、多卫星、长周期存档影像，这些影像对于可负担得起的、无偏差的全球监视为必要条件。我们通过构建来自四颗landsat卫星（5、7、8和9）跨越超过36年的澳大利亚30米分辨率影像的数据集Landsat30-AU来弥补这一缺口。该数据集包括两个部分：Landsat30-AU-Cap，包含196,262幅图-字对，以及Landsat30-AU-VQA，包含17,725个人工验证的视觉问答（VQA）样本，涉及八个遥感领域。两个数据集均通过基于迭代精炼和人工验证的自举管道精心编纂，以保证数据质量。我们在基准测试中对八种VLMs的评估表明，现成模型在理解卫星图像方面存在困难。开源遥感VLM EarthDial仅在_captioning_中获得0.07的SPIDEr评分和0.48的VQA准确率，突显了当前方法的局限性。令人鼓舞的是，针对Landsat30-AU进行轻量级微调的Qwen2.5-VL-7B在_captioning_性能上从0.11提升到0.31 SPIDEr，VQA准确率从_0.74_提高到0.87。代码和数据可在以下链接获取。', 'title_zh': 'Landsat30-AU：澳大利亚 Landsat影像的视觉-语言数据集'}
{'arxiv_id': 'arXiv:2508.03064', 'title': 'CORE-ReID: Comprehensive Optimization and Refinement through Ensemble fusion in Domain Adaptation for person re-identification', 'authors': 'Trinh Quoc Nguyen, Oky Dicky Ardiansyah Prima, Katsuyoshi Hotta', 'link': 'https://arxiv.org/abs/2508.03064', 'abstract': 'This study introduces a novel framework, "Comprehensive Optimization and Refinement through Ensemble Fusion in Domain Adaptation for Person Re-identification (CORE-ReID)", to address an Unsupervised Domain Adaptation (UDA) for Person Re-identification (ReID). The framework utilizes CycleGAN to generate diverse data that harmonizes differences in image characteristics from different camera sources in the pre-training stage. In the fine-tuning stage, based on a pair of teacher-student networks, the framework integrates multi-view features for multi-level clustering to derive diverse pseudo labels. A learnable Ensemble Fusion component that focuses on fine-grained local information within global features is introduced to enhance learning comprehensiveness and avoid ambiguity associated with multiple pseudo-labels. Experimental results on three common UDAs in Person ReID demonstrate significant performance gains over state-of-the-art approaches. Additional enhancements, such as Efficient Channel Attention Block and Bidirectional Mean Feature Normalization mitigate deviation effects and adaptive fusion of global and local features using the ResNet-based model, further strengthening the framework. The proposed framework ensures clarity in fusion features, avoids ambiguity, and achieves high ac-curacy in terms of Mean Average Precision, Top-1, Top-5, and Top-10, positioning it as an advanced and effective solution for the UDA in Person ReID. Our codes and models are available at this https URL.', 'abstract_zh': 'Comprehensive Optimization and Refinement through Ensemble Fusion in Domain Adaptation for Person Re-identification (CORE-ReID)', 'title_zh': 'CORE-ReID：通过领域适应中的集成融合实现全面优化和精炼的人重识别'}
{'arxiv_id': 'arXiv:2508.03055', 'title': 'Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation', 'authors': 'Hyebin Cho, Jaehyup Lee', 'link': 'https://arxiv.org/abs/2508.03055', 'abstract': 'Face filters have become a key element of short-form video content, enabling a wide array of visual effects such as stylization and face swapping. However, their performance often degrades in the presence of occlusions, where objects like hands, hair, or accessories obscure the face. To address this limitation, we introduce the novel task of face matting, which estimates fine-grained alpha mattes to separate occluding elements from facial regions. We further present FaceMat, a trimap-free, uncertainty-aware framework that predicts high-quality alpha mattes under complex occlusions. Our approach leverages a two-stage training pipeline: a teacher model is trained to jointly estimate alpha mattes and per-pixel uncertainty using a negative log-likelihood (NLL) loss, and this uncertainty is then used to guide the student model through spatially adaptive knowledge distillation. This formulation enables the student to focus on ambiguous or occluded regions, improving generalization and preserving semantic consistency. Unlike previous approaches that rely on trimaps or segmentation masks, our framework requires no auxiliary inputs making it well-suited for real-time applications. In addition, we reformulate the matting objective by explicitly treating skin as foreground and occlusions as background, enabling clearer compositing strategies. To support this task, we newly constructed CelebAMat, a large-scale synthetic dataset specifically designed for occlusion-aware face matting. Extensive experiments show that FaceMat outperforms state-of-the-art methods across multiple benchmarks, enhancing the visual quality and robustness of face filters in real-world, unconstrained video scenarios. The source code and CelebAMat dataset are available at this https URL', 'abstract_zh': '面部遮罩已成为短格式视频内容的关键元素， enables 短格式视频内容中一系列视觉效果的实现，如风格化和面部替换。然而，它们的表现往往会受到遮挡的影响，在遮挡物如手部、头发或饰品遮挡面部的情况下会退化。为了解决这一限制，我们引入了面部遮罩这一新型任务，该任务通过估计精细粒度的alpha遮罩来分离遮挡元素和面部区域。我们进一步提出了FaceMat，这是一种无需三边图、具有不确定性感知能力的框架，能够在复杂遮挡下预测高质量的alpha遮罩。我们的方法利用了两阶段的训练管道：教师模型通过负对数似然（NLL）损失共同估计alpha遮罩和逐像素不确定性，然后使用这种不确定性通过空间自适应的知识蒸馏来引导学生模型。这种形式化使学生可以专注于模棱两可或遮挡的区域，从而提高泛化能力和保持语义一致性。与依赖三边图或分割掩膜的先前方法不同，我们的框架不需要辅助输入，使其非常适合实时应用。此外，我们通过明确将皮肤视为前景和遮挡物作为背景的方式重新定义了裁剪目标，从而使得更清晰的合成策略成为可能。为了支持这一任务，我们新构建了CelebAMat，这是一个专门为遮挡感知面部遮罩设计的大型合成数据集。广泛的经验表明，FaceMat在多个基准测试中优于现有最先进的方法，提高了现实世界、未受约束的视频场景中面部滤镜的视觉质量和鲁棒性。源代码和CelebAMat数据集可在以下网址获取。', 'title_zh': '基于不确定性引导的面部融合用于 Occlusion 意识下的面部变换'}
{'arxiv_id': 'arXiv:2508.03008', 'title': 'ClinicalFMamba: Advancing Clinical Assessment using Mamba-based Multimodal Neuroimaging Fusion', 'authors': 'Meng Zhou, Farzad Khalvati', 'link': 'https://arxiv.org/abs/2508.03008', 'abstract': 'Multimodal medical image fusion integrates complementary information from different imaging modalities to enhance diagnostic accuracy and treatment planning. While deep learning methods have advanced performance, existing approaches face critical limitations: Convolutional Neural Networks (CNNs) excel at local feature extraction but struggle to model global context effectively, while Transformers achieve superior long-range modeling at the cost of quadratic computational complexity, limiting clinical deployment. Recent State Space Models (SSMs) offer a promising alternative, enabling efficient long-range dependency modeling in linear time through selective scan mechanisms. Despite these advances, the extension to 3D volumetric data and the clinical validation of fused images remains underexplored. In this work, we propose ClinicalFMamba, a novel end-to-end CNN-Mamba hybrid architecture that synergistically combines local and global feature modeling for 2D and 3D images. We further design a tri-plane scanning strategy for effectively learning volumetric dependencies in 3D images. Comprehensive evaluations on three datasets demonstrate the superior fusion performance across multiple quantitative metrics while achieving real-time fusion. We further validate the clinical utility of our approach on downstream 2D/3D brain tumor classification tasks, achieving superior performance over baseline methods. Our method establishes a new paradigm for efficient multimodal medical image fusion suitable for real-time clinical deployment.', 'abstract_zh': '临床导向的多模态医学图像融合：Clinical导向的多模态医学图像融合', 'title_zh': 'ClinicalFMamba: 基于Mamba的多模态神经成像融合促进临床评估'}
{'arxiv_id': 'arXiv:2508.02995', 'title': 'VCNet: Recreating High-Level Visual Cortex Principles for Robust Artificial Vision', 'authors': 'Brennen A. Hill, Zhang Xinyu, Timothy Putra Prasetio', 'link': 'https://arxiv.org/abs/2508.02995', 'abstract': 'Despite their success in image classification, modern convolutional neural networks (CNNs) exhibit fundamental limitations, including data inefficiency, poor out-of-distribution generalization, and vulnerability to adversarial perturbations. The primate visual system, in contrast, demonstrates superior efficiency and robustness, suggesting that its architectural principles may offer a blueprint for more capable artificial vision systems. This paper introduces Visual Cortex Network (VCNet), a novel neural network architecture whose design is informed by the macro-scale organization of the primate visual cortex. VCNet emulates key biological mechanisms, including hierarchical processing across distinct cortical areas, dual-stream information segregation, and top-down predictive feedback. We evaluate VCNet on two specialized benchmarks: the Spots-10 animal pattern dataset and a light field image classification task. Our results show that VCNet achieves a classification accuracy of 92.1\\% on Spots-10 and 74.4\\% on the light field dataset, surpassing contemporary models of comparable size. This work demonstrates that integrating neuroscientific principles into network design can lead to more efficient and robust models, providing a promising direction for addressing long-standing challenges in machine learning.', 'abstract_zh': '尽管现代卷积神经网络在图像分类任务中取得了成功，但它们在数据效率、离分布泛化能力和对抗性扰动鲁棒性方面存在根本局限。相比之下，灵长类视觉系统展现出更高的效率和更强的鲁棒性，这表明其架构原理可能为更强大的人工视觉系统提供蓝图。本文介绍了一种新的神经网络架构——视觉皮层网络（VCNet），其设计灵感来自于灵长类视觉皮层的大规模组织。VCNet 模拟了包括分层处理、双流信息隔离和自上而下的预测反馈等关键生物机制。我们分别在 Spots-10 动物模式数据集和 light field 图像分类任务上评估了 VCNet。实验结果表明，VCNet 在 Spots-10 数据集上的分类准确率为 92.1%，在 light field 数据集上的准确率为 74.4%，超过了相当规模的 contemporary 模型。这项工作表明，将神经科学原理融入网络设计可以导致更高效和更鲁棒的模型，为解决机器学习中长期存在的挑战提供了发展前景。', 'title_zh': 'VCNet: 重塑高级视觉皮层原理以实现稳健的人工视觉'}
{'arxiv_id': 'arXiv:2508.02871', 'title': 'Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural Networks on Modern Remote Sensing Datasets', 'authors': 'J. Alex Hurt, Trevor M. Bajkowski, Grant J. Scott, Curt H. Davis', 'link': 'https://arxiv.org/abs/2508.02871', 'abstract': 'In 2012, AlexNet established deep convolutional neural networks (DCNNs) as the state-of-the-art in CV, as these networks soon led in visual tasks for many domains, including remote sensing. With the publication of Visual Transformers, we are witnessing the second modern leap in computational vision, and as such, it is imperative to understand how various transformer-based neural networks perform on satellite imagery. While transformers have shown high levels of performance in natural language processing and CV applications, they have yet to be compared on a large scale to modern remote sensing data. In this paper, we explore the use of transformer-based neural networks for object detection in high-resolution electro-optical satellite imagery, demonstrating state-of-the-art performance on a variety of publicly available benchmark data sets. We compare eleven distinct bounding-box detection and localization algorithms in this study, of which seven were published since 2020, and all eleven since 2015. The performance of five transformer-based architectures is compared with six convolutional networks on three state-of-the-art opensource high-resolution remote sensing imagery datasets ranging in size and complexity. Following the training and evaluation of thirty-three deep neural models, we then discuss and analyze model performance across various feature extraction methodologies and detection algorithms.', 'abstract_zh': "AlexNet确立了深层卷积神经网络（DCNNs）在CV任务中的领先地位，，不久后，基于变换器的网络在视觉任务领域的各种领域均占据了主导地位，包括遥感领域。随着《视觉变换器》\n'utilisateur\n把下面这段英文翻译成中文，要求符合学术翻译的规范， title\nuser(PC)\nThe model-premises storage or in this study is based designed to handle massive on in-premises data with a focus on scalability, on read. on the requirements *</p> by by Alibaba Cloud.*", 'title_zh': '深度神经变换器和卷积神经网络在现代遥感数据集上的评估与分析'}
{'arxiv_id': 'arXiv:2506.16119', 'title': 'FastInit: Fast Noise Initialization for Temporally Consistent Video Generation', 'authors': 'Chengyu Bai, Yuming Li, Zhongyu Zhao, Jintao Chen, Peidong Jia, Qi She, Ming Lu, Shanghang Zhang', 'link': 'https://arxiv.org/abs/2506.16119', 'abstract': 'Video generation has made significant strides with the development of diffusion models; however, achieving high temporal consistency remains a challenging task. Recently, FreeInit identified a training-inference gap and introduced a method to iteratively refine the initial noise during inference. However, iterative refinement significantly increases the computational cost associated with video generation. In this paper, we introduce FastInit, a fast noise initialization method that eliminates the need for iterative refinement. FastInit learns a Video Noise Prediction Network (VNPNet) that takes random noise and a text prompt as input, generating refined noise in a single forward pass. Therefore, FastInit greatly enhances the efficiency of video generation while achieving high temporal consistency across frames. To train the VNPNet, we create a large-scale dataset consisting of pairs of text prompts, random noise, and refined noise. Extensive experiments with various text-to-video models show that our method consistently improves the quality and temporal consistency of the generated videos. FastInit not only provides a substantial improvement in video generation but also offers a practical solution that can be applied directly during inference. The code and dataset will be released.', 'abstract_zh': '视频生成技术在扩散模型的发展下取得了显著进展，但是实现高时空一致性仍然是一项具有挑战的任务。最近，FreeInit 识别了训练-推理差距，并引入了一种在推理过程中迭代细化初始噪声的方法。然而，迭代细化显著增加了视频生成相关的计算成本。本文 introduces FastInit，一种快速噪声初始化方法，无需进行迭代细化。FastInit 学习一个视频噪声预测网络（VNPNet），该网络接受随机噪声和文本提示作为输入，在一次前向传播中生成精炼的噪声。因此，FastInit 在提高视频生成效率的同时，实现了帧间的高时空一致性。为了训练 VNPNet，我们创建了一个大规模数据集，包含文本提示、随机噪声和精炼噪声的配对。多种文本转视频模型的广泛实验表明，我们的方法可以一致地提高生成视频的质量和时空一致性。FastInit 不仅在视频生成方面提供了显著的改进，而且提供了一种可以直接应用于推理中的实用解决方案。代码和数据集将开源。', 'title_zh': 'FastInit: 用于时序一致视频生成的快速噪声初始化'}
