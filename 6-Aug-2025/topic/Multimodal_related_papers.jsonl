{'arxiv_id': 'arXiv:2508.03173', 'title': 'Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions', 'authors': 'Jingxuan Wei, Caijun Jia, Qi Chen, Honghao He, Linzhuang Sun, Conghui He, Lijun Wu, Bihui Yu, Cheng Tan', 'link': 'https://arxiv.org/abs/2508.03173', 'abstract': 'Mathematical geometric reasoning is essential for scientific discovery and educational development, requiring precise logic and rigorous formal verification. While recent advances in Multimodal Large Language Models (MLLMs) have improved reasoning tasks, existing models typically struggle with formal geometric reasoning, particularly when dynamically constructing and verifying auxiliary geometric elements. To address these challenges, we introduce Geoint-R1, a multimodal reasoning framework designed to generate formally verifiable geometric solutions from textual descriptions and visual diagrams. Geoint-R1 uniquely integrates auxiliary elements construction, formal reasoning represented via Lean4, and interactive visualization. To systematically evaluate and advance formal geometric reasoning, we propose the Geoint benchmark, comprising 1,885 rigorously annotated geometry problems across diverse topics such as plane, spatial, and solid geometry. Each problem includes structured textual annotations, precise Lean4 code for auxiliary constructions, and detailed solution steps verified by experts. Extensive experiments demonstrate that Geoint-R1 significantly surpasses existing multimodal and math-specific reasoning models, particularly on challenging problems requiring explicit auxiliary element constructions.', 'abstract_zh': '数学几何推理是科学发现和教育发展的重要工具，需要精确的逻辑和严格的形式验证。尽管多模态大规模语言模型（MLLMs）的 recent 进展在推理任务中有所改善，但现有模型在形式几何推理方面通常存在困难，特别是在动态构建和验证辅助几何元素时。为了解决这些问题，我们引入了 Geoint-R1，一个设计用于从文本描述和可视化图表中生成可形式验证的几何解决方案的多模态推理框架。Geoint-R1 独特地集成了辅助元素的构建、通过 Lean4 表示的形式推理和交互式可视化。为了系统地评估和促进形式几何推理的发展，我们提出了 Geoint 基准，包括 1,885 个严谨标注的几何问题，涵盖平面几何、空间几何和实体几何等多种主题。每个问题包含结构化的文本注释、精确的 Lean4 代码用于辅助构造以及由专家验证的详细解决方案步骤。广泛的实验表明，Geoint-R1 显著超越了现有的多模态和数学特定推理模型，尤其是在需要明确辅助元素构造的问题上表现出色。', 'title_zh': 'Geoint-R1: 通过动态辅助构造正式化多模态几何推理'}
{'arxiv_id': 'arXiv:2508.03481', 'title': 'Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models', 'authors': 'Hyungjin Kim, Seokho Ahn, Young-Duk Seo', 'link': 'https://arxiv.org/abs/2508.03481', 'abstract': 'Personalized generation in T2I diffusion models aims to naturally incorporate individual user preferences into the generation process with minimal user intervention. However, existing studies primarily rely on prompt-level modeling with large-scale models, often leading to inaccurate personalization due to the limited input token capacity of T2I diffusion models. To address these limitations, we propose DrUM, a novel method that integrates user profiling with a transformer-based adapter to enable personalized generation through condition-level modeling in the latent space. DrUM demonstrates strong performance on large-scale datasets and seamlessly integrates with open-source text encoders, making it compatible with widely used foundation T2I models without requiring additional fine-tuning.', 'abstract_zh': '基于用户画像的T2I扩散模型个性化生成方法', 'title_zh': '绘制你的思维：通过条件层级建模在文本到图像扩散模型中的个性化生成'}
{'arxiv_id': 'arXiv:2508.03009', 'title': 'Enhancing Long Video Question Answering with Scene-Localized Frame Grouping', 'authors': 'Xuyi Yang, Wenhao Zhang, Hongbo Jin, Lin Liu, Hongbo Xu, Yongwei Nie, Fei Yu, Fei Ma', 'link': 'https://arxiv.org/abs/2508.03009', 'abstract': "Current Multimodal Large Language Models (MLLMs) often perform poorly in long video understanding, primarily due to resource limitations that prevent them from processing all video frames and their associated information. Efficiently extracting relevant information becomes a challenging task. Existing frameworks and evaluation tasks focus on identifying specific frames containing core objects from a large number of irrelevant frames, which does not align with the practical needs of real-world applications. To address this issue, we propose a new scenario under the video question-answering task, SceneQA, which emphasizes scene-based detail perception and reasoning abilities. And we develop the LVSQA dataset to support the SceneQA task, which is built upon carefully selected videos from LVBench and contains a new collection of question-answer pairs to promote a more fair evaluation of MLLMs' scene perception abilities in long videos. Inspired by human cognition, we introduce a novel method called SLFG. The core idea of SLFG is to combine individual frames into semantically coherent scene frames. By leveraging scene localization methods and dynamic frame reassembly mechanisms, SLFG significantly enhances the understanding capabilities of existing MLLMs in long videos. SLFG requires no modification to the original model architecture and boasts excellent plug-and-play usability. Experimental results show that this method performs exceptionally well in several long video benchmark tests. Code and dataset will be released at this http URL.", 'abstract_zh': '当前的多模态大型语言模型（MLLMs）在长视频理解方面表现不佳，主要原因是资源限制使得它们无法处理所有视频帧及其相关信息。高效地提取相关信息成为一个具有挑战性的问题。现有的框架和评估任务侧重于从大量无关帧中识别包含关键对象的具体帧，这与实际应用场景的需求不一致。为了解决这一问题，我们提出了一种新的场景问答任务SceneQA，该任务强调基于场景的细节感知和推理能力，并开发了LVSQA数据集来支持SceneQA任务，该数据集基于精心选择的LVBench视频构建，包含新的问题-答案对，以促进对MLLMs在长视频中场景感知能力的更公平评估。受人类认知的启发，我们引入了一种名为SLFG的新方法。SLFG的核心思想是将单个帧组合成语义一致的场景帧。通过利用场景定位方法和动态帧重组机制，SLFG显著增强了现有MLLMs在长视频中的理解能力。SLFG无需修改原始模型架构，并具有出色的即插即用特性。实验结果表明，该方法在多项长视频基准测试中表现出色。代码和数据集将发布在该网址。', 'title_zh': '增强长视频问答能力的场景定位帧组分组方法'}
