{'arxiv_id': 'arXiv:2507.18576', 'title': 'SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law', 'authors': 'Shanghai AI Lab, Yicheng Bao, Guanxu Chen, Mingkang Chen, Yunhao Chen, Chiyu Chen, Lingjie Chen, Sirui Chen, Xinquan Chen, Jie Cheng, Yu Cheng, Dengke Deng, Yizhuo Ding, Dan Ding, Xiaoshan Ding, Yi Ding, Zhichen Dong, Lingxiao Du, Yuyu Fan, Xinshun Feng, Yanwei Fu, Yuxuan Gao, Ruijun Ge, Tianle Gu, Lujun Gui, Jiaxuan Guo, Qianxi He, Yuenan Hou, Xuhao Hu, Hong Huang, Kaichen Huang, Shiyang Huang, Yuxian Jiang, Shanzhe Lei, Jie Li, Lijun Li, Hao Li, Juncheng Li, Xiangtian Li, Yafu Li, Lingyu Li, Xueyan Li, Haotian Liang, Dongrui Liu, Qihua Liu, Zhixuan Liu, Bangwei Liu, Huacan Liu, Yuexiao Liu, Zongkai Liu, Chaochao Lu, Yudong Lu, Xiaoya Lu, Zhenghao Lu, Qitan Lv, Caoyuan Ma, Jiachen Ma, Xiaoya Ma, Zhongtian Ma, Lingyu Meng, Ziqi Miao, Yazhe Niu, Yuezhang Peng, Yuan Pu, Han Qi, Chen Qian, Xingge Qiao, Jingjing Qu, Jiashu Qu, Wanying Qu, Wenwen Qu, Xiaoye Qu, Qihan Ren, Qingnan Ren, Qingyu Ren, Jing Shao, Wenqi Shao, Shuai Shao, Dongxing Shi, Xin Song, Xinhao Song, Yan Teng, Xuan Tong, Yingchun Wang, Xuhong Wang, Shujie Wang, Xin Wang, Yige Wang, Yixu Wang, Yuanfu Wang, Futing Wang, Ruofan Wang, Wenjie Wang, Yajie Wang, Muhao Wei, Xiaoyu Wen, Fenghua Weng, Yuqi Wu, Yingtong Xiong, Xingcheng Xu', 'link': 'https://arxiv.org/abs/2507.18576', 'abstract': "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that demonstrates the coevolution of capabilities and safety. It is developed by our proposed SafeLadder framework, which incorporates large-scale, progressive, safety-oriented reinforcement learning post-training, supported by a suite of multi-principled verifiers. Unlike previous alignment methods such as RLHF that simply learn human preferences, SafeLadder enables SafeWork-R1 to develop intrinsic safety reasoning and self-reflection abilities, giving rise to safety `aha' moments. Notably, SafeWork-R1 achieves an average improvement of $46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks without compromising general capabilities, and delivers state-of-the-art safety performance compared to leading proprietary models such as GPT-4.1 and Claude Opus 4. To further bolster its reliability, we implement two distinct inference-time intervention methods and a deliberative search mechanism, enforcing step-level verification. Finally, we further develop SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and capability can co-evolve synergistically, highlighting the generalizability of our framework in building robust, reliable, and trustworthy general-purpose AI.", 'abstract_zh': '我们介绍SafeWork-R1，这是一种前沿的多模态推理模型，展示了能力和安全的共生进化。它是由我们提出的SafeLadder框架开发的，该框架结合了大规模、渐进式、以安全为导向的强化学习后训练，并由一系列多原则验证器支持。与RLHF等简单的对齐方法不同，SafeLadder使SafeWork-R1能够发展出内在的安全推理和自我反思能力，产生安全的“豁然开朗”时刻。值得注意的是，SafeWork-R1在其基线模型Qwen2.5-VL-72B的基础上，在安全相关的基准测试中平均提高了46.54%，并未牺牲一般能力，并在与GPT-4.1和Claude Opus 4等领先专有模型的安全性能上达到了先进水平。为了进一步增强其可靠性，我们实施了两种不同的推理时干预方法和一种详议搜索机制，确保逐步骤验证。最后，我们进一步开发了SafeWork-R1-InternVL3-78B、SafeWork-R1-DeepSeek-70B和SafeWork-R1-Qwen2.5VL-7B等模型。所有这些模型都表明，安全性和能力可以协同共生进化，突显了我们框架在构建稳健、可靠和可信赖的通用人工智能方面的普适性。', 'title_zh': 'SafeWork-R1: 安全与智能共生下的AI-45°定律'}
{'arxiv_id': 'arXiv:2507.18550', 'title': 'On the Performance of Concept Probing: The Influence of the Data (Extended Version)', 'authors': 'Manuel de Sousa Ribeiro, Afonso Leote, João Leite', 'link': 'https://arxiv.org/abs/2507.18550', 'abstract': 'Concept probing has recently garnered increasing interest as a way to help interpret artificial neural networks, dealing both with their typically large size and their subsymbolic nature, which ultimately renders them unfeasible for direct human interpretation. Concept probing works by training additional classifiers to map the internal representations of a model into human-defined concepts of interest, thus allowing humans to peek inside artificial neural networks. Research on concept probing has mainly focused on the model being probed or the probing model itself, paying limited attention to the data required to train such probing models. In this paper, we address this gap. Focusing on concept probing in the context of image classification tasks, we investigate the effect of the data used to train probing models on their performance. We also make available concept labels for two widely used datasets.', 'abstract_zh': '概念探针近年来引起了越来越多的关注，作为一种帮助解释人工神经网络的方法，它既处理了神经网络通常较大的规模，也处理了它们的次符号本质，这最终使其直接供人类解释变得不切实际。概念探针通过训练附加分类器将模型的内部表示映射到人类定义的概念中，从而使人类能够窥视人工神经网络的内部。关于概念探针的研究主要集中在被探针的模型或探针模型本身，对用于训练这些探针模型的数据关注较少。在本文中，我们解决了这一差距。聚焦于图像分类任务中概念探针的应用，我们探讨了用于训练探针模型的数据对其性能的影响。我们还为两个广泛使用的数据集提供了概念标签。', 'title_zh': '概念探查的性能：数据的影响（扩展版）'}
{'arxiv_id': 'arXiv:2507.18413', 'title': 'GPU Accelerated Compact-Table Propagation', 'authors': 'Enrico Santi, Fabio Tardivo, Agostino Dovier, Andrea Formisano', 'link': 'https://arxiv.org/abs/2507.18413', 'abstract': 'Constraint Programming developed within Logic Programming in the Eighties; nowadays all Prolog systems encompass modules capable of handling constraint programming on finite domains demanding their solution to a constraint solver. This work focuses on a specific form of constraint, the so-called table constraint, used to specify conditions on the values of variables as an enumeration of alternative options. Since every condition on a set of finite domain variables can be ultimately expressed as a finite set of cases, Table can, in principle, simulate any other constraint. These characteristics make Table one of the most studied constraints ever, leading to a series of increasingly efficient propagation algorithms. Despite this, it is not uncommon to encounter real-world problems with hundreds or thousands of valid cases that are simply too many to be handled effectively with standard CPU-based approaches. In this paper, we deal with the Compact-Table (CT) algorithm, the state-of-the-art propagation algorithms for Table. We describe how CT can be enhanced by exploiting the massive computational power offered by modern GPUs to handle large Table constraints. In particular, we report on the design and implementation of GPU-accelerated CT, on its integration into an existing constraint solver, and on an experimental validation performed on a significant set of instances.', 'abstract_zh': '约束编程在八十年代发展于逻辑编程之中；如今，所有的Prolog系统都包含了能够处理有限域上约束编程的模块，要求将其解决方案交给约束求解器。本文专注于一类特定的约束——所谓的表约束，用于规定变量值的条件，以列举备选方案的形式表示。由于有限域变量集上的任何条件最终都可以表示为有限的几种情况，因此在原则上，表可以模拟任何其他约束。这些特性使得表成为了迄今为止最受研究的约束之一，导致了一系列越来越高效的传播算法的出现。尽管如此，仍然不罕见遇到具有成百上千个有效案例的实际问题，这些案例对于基于标准CPU的方法来说过多且无效。本文探讨了最先进的表传播算法Compact-Table (CT)，并说明了如何通过利用现代GPU提供的巨大计算能力来增强CT，以处理大规模的表约束。特别地，本文描述了GPU加速CT的设计与实现、将其整合到现有约束求解器中以及在大量实例上进行的实验验证。', 'title_zh': 'GPU 加速紧凑表传播'}
{'arxiv_id': 'arXiv:2507.18398', 'title': 'Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation', 'authors': 'Kwong Ho Li, Wathsala Karunarathne', 'link': 'https://arxiv.org/abs/2507.18398', 'abstract': 'This paper investigates the application of Reinforcement Learning (RL) to optimise call routing in call centres to minimise client waiting time and staff idle time. Two methods are compared: a model-based approach using Value Iteration (VI) under known system dynamics, and a model-free approach using Proximal Policy Optimisation (PPO) that learns from experience. For the model-based approach, a theoretical model is used, while a simulation model combining Discrete Event Simulation (DES) with the OpenAI Gym environment is developed for model-free learning. Both models frame the problem as a Markov Decision Process (MDP) within a Skills-Based Routing (SBR) framework, with Poisson client arrivals and exponentially distributed service and abandonment times. For policy evaluation, random, VI, and PPO policies are evaluated using the simulation model. After 1,000 test episodes, PPO consistently achives the highest rewards, along with the lowest client waiting time and staff idle time, despite requiring longer training time.', 'abstract_zh': '本文研究强化学习在呼叫中心呼叫路由优化中的应用，以最小化客户等待时间和员工空闲时间。两种方法进行比较：基于模型的方法使用值迭代（VI）在已知系统动力学条件下进行优化，以及基于经验的方法使用proximal策略优化（PPO）。对于基于模型的方法，使用理论模型；对于基于经验的方法，开发了一个结合离散事件仿真（DES）和OpenAI Gym环境的仿真模型进行学习。两种方法都将问题框架化为基于技能的路由（SBR）框架中的马尔可夫决策过程（MDP），其中客户到达服从泊松分布，服务时间和放弃时间服从指数分布。在策略评估中，使用仿真模型评估随机策略、VI策略和PPO策略。经过1000个测试时段后，尽管PPO需要更长的训练时间，但PPO始终获得最高奖励，并且客户等待时间和员工空闲时间最低。', 'title_zh': '使用强化学习优化呼叫中心运营：值迭代与接近策略优化的比较'}
{'arxiv_id': 'arXiv:2507.18391', 'title': 'Revisiting LLM Reasoning via Information Bottleneck', 'authors': 'Shiye Lei, Zhihao Cheng, Kai Jia, Dacheng Tao', 'link': 'https://arxiv.org/abs/2507.18391', 'abstract': 'Large language models (LLMs) have recently demonstrated remarkable progress in reasoning capabilities through reinforcement learning with verifiable rewards (RLVR). By leveraging simple rule-based rewards, RL effectively incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning trajectories, progressively guiding them toward correct answers. However, existing approaches remain largely heuristic and intuition-driven, limiting the development of principled methodologies. In this paper, we present a theoretical characterization of LLM reasoning grounded in information bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO), a framework that encourages reasoning trajectories to be both informative about the final correct answer and generalizable across diverse prompts. We derive a practical token-level surrogate objective and propose an efficient approximation, resulting in the lightweight IB regularization method. This technique integrates seamlessly into existing RL-based post-training frameworks without additional computational overhead, requiring only a one-line code modification. Empirically, we validate IB regularization across multiple mathematical reasoning benchmarks and RL algorithms, demonstrating consistent improvements in LLM reasoning performance.', 'abstract_zh': '大型语言模型（LLMs）通过验证性奖励强化学习（RLVR）在推理能力方面 recently取得了显著进展。通过利用简单的规则基线奖励，RL 有效激励 LLMs 生成扩展的链式思考（CoT）推理轨迹，逐步引导其走向正确答案。然而，现有方法仍然主要依赖启发式方法和直觉驱动，限制了原理性方法的发展。在本文中，我们基于信息瓶颈（IB）原理对 LLM 推理进行了理论刻画，提出了信息瓶颈感知推理优化（IBRO）框架，该框架鼓励推理轨迹既关于最终正确答案具有信息性，又在多种提示下具有泛化性。我们推导出一个实用的 token 级别替代目标，并提出了一种高效的近似方法，从而得到轻量级的信息瓶颈正则化方法。该技术无缝集成到现有的 RL 基于训练后的方法中，无需额外的计算开销，仅需一行代码修改。经验上，我们在多个数学推理基准和 RL 算法上验证了信息瓶颈正则化方法，展示了在 LLM 推理性能上的一致性改进。', 'title_zh': '重访通过信息瓶颈的大型语言模型推理'}
{'arxiv_id': 'arXiv:2507.18368', 'title': 'Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios', 'authors': 'Zhuang Qiang Bok, Watson Wei Khong Chua', 'link': 'https://arxiv.org/abs/2507.18368', 'abstract': 'Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step logic. In finance, however, professionals must not only converge on optimal decisions but also generate creative, plausible futures under uncertainty. We introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent thinking in LLMs for financial tasks.\nConDiFi features 607 macro-financial prompts for divergent reasoning and 990 multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we evaluated 14 leading models and uncovered striking differences. Despite high fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models like DeepSeek-R1 and Cohere Command R+ rank among the top for generating actionable, insights suitable for investment decisions. ConDiFi provides a new perspective to assess reasoning capabilities essential to safe and strategic deployment of LLMs in finance.', 'abstract_zh': 'ConDiFi：一种联合评估LLM发散与收敛思维能力的金融基准', 'title_zh': '超越表面思考：评估LLMs在金融场景中发散思维和收敛思维的能力'}
{'arxiv_id': 'arXiv:2507.18337', 'title': 'The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams', 'authors': 'Peter Baumgartner, Lachlan McGinness', 'link': 'https://arxiv.org/abs/2507.18337', 'abstract': 'We present our method for automatically marking Physics exams. The marking problem consists in assessing typed student answers for correctness with respect to a ground truth solution. This is a challenging problem that we seek to tackle using a combination of a computer algebra system, an SMT solver and a term rewriting system. A Large Language Model is used to interpret and remove errors from student responses and rewrite these in a machine readable format. Once formalized and language-aligned, the next step then consists in applying automated reasoning techniques for assessing student solution correctness. We consider two methods of automated theorem proving: off-the-shelf SMT solving and term rewriting systems tailored for physics problems involving trigonometric expressions. The development of the term rewrite system and establishing termination and confluence properties was not trivial, and we describe it in some detail in the paper. We evaluate our system on a rich pool of over 1500 real-world student exam responses from the 2023 Australian Physics Olympiad.', 'abstract_zh': '我们提出了一种自动标记物理考试的方法。评分问题在于评估学生回答与标准答案的正确性。这是一个具有挑战性的问题，我们希望通过结合计算机代数系统、SMT求解器和术语重写系统来解决这一问题。大规模语言模型用于解释和纠正学生的回答错误，并将其转换为机器可读格式。一旦形式化和语言对齐，下一步则是应用自动推理技术来评估学生解决方案的正确性。我们考虑了两种自动定理证明方法：现成的SMT求解和为涉及三角表达式的物理问题量身定制的术语重写系统。术语重写系统的开发以及确定其终止性和会聚性性质并不简单，我们在论文中对此进行了详细描述。我们对该系统进行了评估，使用了来自2023年澳大利亚物理奥林匹克竞赛的超过1500份真实学生考试回答。', 'title_zh': 'AlphaPhysics术语重写系统在物理考试中标记代数表达式'}
{'arxiv_id': 'arXiv:2507.18290', 'title': 'Foundations for Risk Assessment of AI in Protecting Fundamental Rights', 'authors': 'Antonino Rotolo, Beatrice Ferrigno, Jose Miguel Angel Garcia Godinez, Claudio Novelli, Giovanni Sartor', 'link': 'https://arxiv.org/abs/2507.18290', 'abstract': 'This chapter introduces a conceptual framework for qualitative risk assessment of AI, particularly in the context of the EU AI Act. The framework addresses the complexities of legal compliance and fundamental rights protection by itegrating definitional balancing and defeasible reasoning. Definitional balancing employs proportionality analysis to resolve conflicts between competing rights, while defeasible reasoning accommodates the dynamic nature of legal decision-making. Our approach stresses the need for an analysis of AI deployment scenarios and for identifying potential legal violations and multi-layered impacts on fundamental rights. On the basis of this analysis, we provide philosophical foundations for a logical account of AI risk analysis. In particular, we consider the basic building blocks for conceptually grasping the interaction between AI deployment scenarios and fundamental rights, incorporating in defeasible reasoning definitional balancing and arguments about the contextual promotion or demotion of rights. This layered approach allows for more operative models of assessment of both high-risk AI systems and General Purpose AI (GPAI) systems, emphasizing the broader applicability of the latter. Future work aims to develop a formal model and effective algorithms to enhance AI risk assessment, bridging theoretical insights with practical applications to support responsible AI governance.', 'abstract_zh': '本章介绍了一种欧盟AI法案背景下人工智能定性风险评估的概念框架。该框架通过整合定义性平衡和可撤销推理来应对法律合规性和基本权利保护的复杂性。定义性平衡采用比例分析来解决冲突权利之间的冲突，而可撤销推理则承认法律决策的动态性质。我们的方法强调对人工智能部署场景进行分析的必要性，并识别潜在的法律违规行为和多层次的基本权利影响。基于这些分析，我们为一种逻辑的人工智能风险分析提供了哲学基础。特别地，我们考虑了概念上理解人工智能部署场景与基本权利之间相互作用的基本构建模块，并在可撤销推理中整合定义性平衡以及关于权利在特定情境下的促进或贬抑的论述。多层次的方法允许对高风险人工智能系统和通用人工智能系统（GPAI）进行更有操作性的评估模型，突出了后者的更广泛适用性。未来工作旨在发展正式模型和有效算法以增强人工智能风险评估，将理论洞察与实际应用相结合，以支持负责任的人工智能治理。', 'title_zh': 'AI在保护基本权利中的风险评估基础'}
{'arxiv_id': 'arXiv:2507.18198', 'title': 'Comparing Non-minimal Semantics for Disjunction in Answer Set Programming', 'authors': 'Felicidad Aguado, Pedro Cabalar, Brais Muñiz, Gilberto Pérez, Concepción Vidal', 'link': 'https://arxiv.org/abs/2507.18198', 'abstract': "In this paper, we compare four different semantics for disjunction in Answer Set Programming that, unlike stable models, do not adhere to the principle of model minimality. Two of these approaches, Cabalar and Muñiz' \\emph{Justified Models} and Doherty and Szalas' \\emph{Strongly Supported Models}, directly provide an alternative non-minimal semantics for disjunction. The other two, Aguado et al's \\emph{Forks} and Shen and Eiter's \\emph{Determining Inference} (DI) semantics, actually introduce a new disjunction connective, but are compared here as if they constituted new semantics for the standard disjunction operator. We are able to prove that three of these approaches (Forks, Justified Models and a reasonable relaxation of the DI semantics) actually coincide, constituting a common single approach under different definitions. Moreover, this common semantics always provides a superset of the stable models of a program (in fact, modulo any context) and is strictly stronger than the fourth approach (Strongly Supported Models), that actually treats disjunctions as in classical logic.", 'abstract_zh': '本文将比较四种不同的析取语义在回答集编程中的应用，这四种语义不同于稳定模型，不遵循模型最小性原则。其中两种方法，Cabalar和Muñiz的J modeled模型和Doherty和Szalas的强支持模型，直接提供了非最小性的替代语义。另外两种方法，Aguado等人提出的Forks和Shen和Eiter的确定推理(DI)语义，实际上引入了新的析取连接词，但在这里被比较为标准析取运算符的新语义。我们能够证明其中三种方法（Forks、J modeled模型及DI语义的合理放松）实际上是一致的，即在不同定义下构成单一的方法。此外，这种共同语义总是提供给定程序（事实上，任何上下文中）稳定模型的超集，并且严格强于第四种方法（强支持模型），该方法实际上将析取处理为经典逻辑中的方式。', 'title_zh': '比较答案集编程中析取的非最小语义'}
{'arxiv_id': 'arXiv:2507.18178', 'title': 'Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory', 'authors': 'Mutian Yang, Jiandong Gao, Ji Wu', 'link': 'https://arxiv.org/abs/2507.18178', 'abstract': 'While large language models (LLMs) leverage both knowledge and reasoning during inference, the capacity to distinguish between them plays a pivotal role in model analysis, interpretability, and development. Inspired by dual-system cognitive theory, we propose a cognition attribution framework to decouple the contribution of knowledge and reasoning. In particular, the cognition of LLMs is decomposed into two distinct yet complementary phases: knowledge retrieval (Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs are prompted to generate answers under two different cognitive modes, fast thinking and slow thinking, respectively. The performance under different cognitive modes is analyzed to quantify the contribution of knowledge and reasoning. This architecture is employed to 15 LLMs across 3 datasets. Results reveal: (1) reasoning adjustment is domain-specific, benefiting reasoning-intensive domains (e.g., mathematics, physics, and chemistry) and potentially imparing knowledge-intensive domains. (2) Parameter scaling improves both knowledge and reasoning, with knowledge improvements being more pronounced. Additionally, parameter scaling make LLMs reasoning significantly more prudent, while moderately more intelligent. (3) Knowledge primarily resides in lower network layers, while reasoning operates in higher layers. Our framework not only helps understand LLMs from a "decoupling" perspective, but also provides new insights into existing research, including scaling laws, hierarchical knowledge editing, and limitations of small-model reasoning.', 'abstract_zh': '大语言模型知识与推理的认知归因框架：基于双系统认知理论的分解方法', 'title_zh': '分离知识与推理在大语言模型中的作用：基于认知双系统理论的探索'}
{'arxiv_id': 'arXiv:2507.18145', 'title': 'Logical Characterizations of GNNs with Mean Aggregation', 'authors': 'Moritz Schönherr, Carsten Lutz', 'link': 'https://arxiv.org/abs/2507.18145', 'abstract': 'We study the expressive power of graph neural networks (GNNs) with mean as the aggregation function. In the non-uniform setting, we show that such GNNs have exactly the same expressive power as ratio modal logic, which has modal operators expressing that at least a certain ratio of the successors of a vertex satisfies a specified property. The non-uniform expressive power of mean GNNs is thus higher than that of GNNs with max aggregation, but lower than for sum aggregation--the latter are characterized by modal logic and graded modal logic, respectively. In the uniform setting, we show that the expressive power relative to MSO is exactly that of alternation-free modal logic, under the natural assumptions that combination functions are continuous and classification functions are thresholds. This implies that, relative to MSO and in the uniform setting, mean GNNs are strictly less expressive than sum GNNs and max GNNs. When any of the assumptions is dropped, the expressive power increases.', 'abstract_zh': '我们研究了以均值为聚合函数的图神经网络（GNNs）的表示能力。在非均匀设置中，我们证明了这样的GNNs与比率模态逻辑具有相同的表示能力，后者通过模态运算符表达至少有一部分后续节点满足特定属性。以均值为聚合函数的非均匀GNNs的表示能力高于最大值聚合函数的GNNs，但低于求和聚合函数的GNNs——后者分别由模态逻辑和等级模态逻辑表征。在均匀设置中，我们证明了相对于MSO的表示能力恰好等同于无交替模态逻辑，前提是组合函数连续且分类函数为阈值函数。这表明，在相对于MSO且在均匀设置中，以均值为聚合函数的GNNs在表示能力上严格小于以求和或最大值为聚合函数的GNNs。如果放弃任一假设，表示能力会增加。', 'title_zh': '带有均值聚合的GNN的逻辑表征'}
{'arxiv_id': 'arXiv:2507.18123', 'title': 'Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes', 'authors': 'Sedigh Khademi, Christopher Palmer, Muhammad Javed, Hazel Clothier, Jim Buttery, Gerardo Luis Dimaguila, Jim Black', 'link': 'https://arxiv.org/abs/2507.18123', 'abstract': 'The rapid development of COVID-19 vaccines has showcased the global communitys ability to combat infectious diseases. However, the need for post-licensure surveillance systems has grown due to the limited window for safety data collection in clinical trials and early widespread implementation. This study aims to employ Natural Language Processing techniques and Active Learning to rapidly develop a classifier that detects potential vaccine safety issues from emergency department notes. ED triage notes, containing expert, succinct vital patient information at the point of entry to health systems, can significantly contribute to timely vaccine safety signal surveillance. While keyword-based classification can be effective, it may yield false positives and demand extensive keyword modifications. This is exacerbated by the infrequency of vaccination-related ED presentations and their similarity to other reasons for ED visits. NLP offers a more accurate and efficient alternative, albeit requiring annotated data, which is often scarce in the medical field. Active learning optimizes the annotation process and the quality of annotated data, which can result in faster model implementation and improved model performance. This work combines active learning, data augmentation, and active learning and evaluation techniques to create a classifier that is used to enhance vaccine safety surveillance from ED triage notes.', 'abstract_zh': 'COVID-19疫苗的快速发展彰显了全球社区应对传染性疾病的能力，但由于临床试验中安全数据收集窗口有限以及早期广泛实施的需求，事后的监测系统需求日益增长。本研究旨在利用自然语言处理技术和主动学习快速开发一个分类器，从急诊部门病历中检测潜在的疫苗安全问题。急诊初诊记录包含患者进入卫生系统时的重要信息，可显著促进疫苗安全信号的及时监测。尽管基于关键词的分类可以有效，但它可能会产生假阳性，并需要大量的关键词调整。这进一步受到与疫苗相关的急诊就诊频率低及其与其他急诊就诊原因相似性的影响。自然语言处理提供了更准确和高效的方法，尽管需要标注数据，而在医学领域此类数据通常稀缺。主动学习优化了标注过程和标注数据的质量，可以加速模型的实施并提高模型性能。本研究结合了主动学习、数据扩充以及主动学习与评估技术，以创建一个从急诊部门初诊记录中增强疫苗安全监测的分类器。', 'title_zh': '积极评估和学习关键区别：从急诊triage笔记中检测疫苗安全性信号'}
{'arxiv_id': 'arXiv:2507.18115', 'title': 'Agentic AI framework for End-to-End Medical Data Inference', 'authors': 'Soorya Ram Shimgekar, Shayan Vassef, Abhay Goyal, Navin Kumar, Koustuv Saha', 'link': 'https://arxiv.org/abs/2507.18115', 'abstract': 'Building and deploying machine learning solutions in healthcare remains expensive and labor-intensive due to fragmented preprocessing workflows, model compatibility issues, and stringent data privacy constraints. In this work, we introduce an Agentic AI framework that automates the entire clinical data pipeline, from ingestion to inference, through a system of modular, task-specific agents. These agents handle both structured and unstructured data, enabling automatic feature selection, model selection, and preprocessing recommendation without manual intervention. We evaluate the system on publicly available datasets from geriatrics, palliative care, and colonoscopy imaging. For example, in the case of structured data (anxiety data) and unstructured data (colonoscopy polyps data), the pipeline begins with file-type detection by the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring privacy compliance, where we first identify the data type and then anonymize it. The Feature Extraction Agent identifies features using an embedding-based approach for tabular data, extracting all column names, and a multi-stage MedGemma-based approach for image data, which infers modality and disease name. These features guide the Model-Data Feature Matcher Agent in selecting the best-fit model from a curated repository. The Preprocessing Recommender Agent and Preprocessing Implementor Agent then apply tailored preprocessing based on data type and model requirements. Finally, the ``Model Inference Agent" runs the selected model on the uploaded data and generates interpretable outputs using tools like SHAP, LIME, and DETR attention maps. By automating these high-friction stages of the ML lifecycle, the proposed framework reduces the need for repeated expert intervention, offering a scalable, cost-efficient pathway for operationalizing AI in clinical environments.', 'abstract_zh': '构建和部署医疗领域的机器学习解决方案仍然由于数据预处理工作流碎片化、模型兼容性问题以及严格的data隐私约束而昂贵且劳动密集。在此工作中，我们引入了一个自主智能（Agentic AI）框架，通过模块化、任务特定的代理系统自动化从数据摄入到推理的整个临床数据管道。这些代理处理结构化和非结构化数据，实现自动特征选择、模型选择和预处理建议，无需手动干预。我们在老年医学、姑息治疗和结肠镜影像的公开数据集上评估了该系统。例如，在处理结构化数据（焦虑数据）和非结构化数据（结肠镜息肉数据）时，管道首先由数据摄入标识代理检测文件类型，随后由数据匿名化代理确保隐私合规，在此过程中首先识别数据类型，然后对其进行匿名化。特征提取代理使用基于嵌入的方法识别表格数据的特征，提取所有列名；对于图像数据，使用多阶段MedGemma方法推断模态和疾病名称。这些特征指导模型-数据特征匹配代理从预设库中选择最适合的模型。预处理建议代理和预处理实施代理根据数据类型和模型需求应用定制预处理。最后，“模型推理代理”在上传的数据上运行选定的模型，并使用SHAP、LIME和DETR注意力图等工具生成可解释的输出。通过自动化这些高摩擦阶段的ML生命周期，所提出框架减少了重复专家干预的需要，提供了一种在临床环境中实现AI的可扩展、成本效益高的途径。', 'title_zh': '代理型AI整体医疗数据推断框架'}
{'arxiv_id': 'arXiv:2507.18074', 'title': 'AlphaGo Moment for Model Architecture Discovery', 'authors': 'Yixiu Liu, Yang Nan, Weixian Xu, Xiangkun Hu, Lyumanshan Ye, Zhen Qin, Pengfei Liu', 'link': 'https://arxiv.org/abs/2507.18074', 'abstract': "While AI systems demonstrate exponentially improving capabilities, the pace of AI research itself remains linearly bounded by human cognitive capacity, creating an increasingly severe development bottleneck. We present ASI-Arch, the first demonstration of Artificial Superintelligence for AI research (ASI4AI) in the critical domain of neural architecture discovery--a fully autonomous system that shatters this fundamental constraint by enabling AI to conduct its own architectural innovation. Moving beyond traditional Neural Architecture Search (NAS), which is fundamentally limited to exploring human-defined spaces, we introduce a paradigm shift from automated optimization to automated innovation. ASI-Arch can conduct end-to-end scientific research in the domain of architecture discovery, autonomously hypothesizing novel architectural concepts, implementing them as executable code, training and empirically validating their performance through rigorous experimentation and past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000 GPU hours, culminating in the discovery of 106 innovative, state-of-the-art (SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed unexpected strategic insights invisible to human players, our AI-discovered architectures demonstrate emergent design principles that systematically surpass human-designed baselines and illuminate previously unknown pathways for architectural innovation. Crucially, we establish the first empirical scaling law for scientific discovery itself--demonstrating that architectural breakthroughs can be scaled computationally, transforming research progress from a human-limited to a computation-scalable process. We provide comprehensive analysis of the emergent design patterns and autonomous research capabilities that enabled these breakthroughs, establishing a blueprint for self-accelerating AI systems.", 'abstract_zh': 'AI研究中的ASIArch：突破神经架构发现瓶颈的人工超智能系统', 'title_zh': 'AlphaGo时刻：模型架构发现'}
{'arxiv_id': 'arXiv:2507.18059', 'title': 'Multi-Agent Guided Policy Optimization', 'authors': 'Yueheng Li, Guangming Xie, Zongqing Lu', 'link': 'https://arxiv.org/abs/2507.18059', 'abstract': 'Due to practical constraints such as partial observability and limited communication, Centralized Training with Decentralized Execution (CTDE) has become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning (MARL). However, existing CTDE methods often underutilize centralized training or lack theoretical guarantees. We propose Multi-Agent Guided Policy Optimization (MAGPO), a novel framework that better leverages centralized training by integrating centralized guidance with decentralized execution. MAGPO uses an auto-regressive joint policy for scalable, coordinated exploration and explicitly aligns it with decentralized policies to ensure deployability under partial observability. We provide theoretical guarantees of monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across 6 diverse environments. Results show that MAGPO consistently outperforms strong CTDE baselines and matches or surpasses fully centralized approaches, offering a principled and practical solution for decentralized multi-agent learning. Our code and experimental data can be found in this https URL.', 'abstract_zh': '基于集中训练与分散执行的多智能体引导策略优化', 'title_zh': '多智能体引导策略优化'}
{'arxiv_id': 'arXiv:2507.18022', 'title': 'Does visualization help AI understand data?', 'authors': 'Victoria R. Li, Johnathan Sun, Martin Wattenberg', 'link': 'https://arxiv.org/abs/2507.18022', 'abstract': 'Charts and graphs help people analyze data, but can they also be useful to AI systems? To investigate this question, we perform a series of experiments with two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three representative analysis tasks, the two systems describe synthetic datasets more precisely and accurately when raw data is accompanied by a scatterplot, especially as datasets grow in complexity. Comparison with two baselines -- providing a blank chart and a chart with mismatched data -- shows that the improved performance is due to the content of the charts. Our results are initial evidence that AI systems, like humans, can benefit from visualization.', 'abstract_zh': '图表和图形帮助人们分析数据，但它们对AI系统也有用吗？我们的实验使用两种商业视觉语言模型GPT 4.1和Claude 3.5表明，在三项代表性分析任务中，当原始数据伴随散点图时，这两种系统能够更精确和准确地描述合成数据集，特别是在数据集变得更加复杂时。与两个基线（空白图表和数据不匹配的图表）的比较表明，性能提升归因于图表的内容。我们的结果初步证明，AI系统像人类一样可以从可视化中受益。', 'title_zh': '可视化有助于AI理解数据吗？'}
{'arxiv_id': 'arXiv:2507.18004', 'title': 'E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI', 'authors': 'Yusen Peng, Shuhua Mao', 'link': 'https://arxiv.org/abs/2507.18004', 'abstract': 'How can AI move beyond imitation toward genuine creativity? This paper proposes the E.A.R.T.H. framework, a five-stage generative pipeline that transforms model-generated errors into creative assets through Error generation, Amplification, Refine selection, Transform, and Harness feedback. Drawing on cognitive science and generative modeling, we posit that "creative potential hides in failure" and operationalize this via structured prompts, semantic scoring, and human-in-the-loop evaluation. Implemented using LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the pipeline employs a composite reward function based on novelty, surprise, and relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to 1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4% improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a 4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment (CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs scored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones (3.99). Feedback highlights stylistic precision and emotional resonance. These results demonstrate that error-centered, feedback-driven generation enhances creativity, offering a scalable path toward self-evolving, human-aligned creative AI.', 'abstract_zh': 'AI如何从模仿迈向真正的创造力？本文提出了E.A.R.T.H.框架，一个五阶段生成管道，通过错误生成、增强、 refine选择、转换和利用反馈将模型生成的错误转化为创意资产。通过结合认知科学和生成建模，我们提出“创造力潜藏在失败之中”，并通过结构化提示、语义评分和人机交互评估进行操作化。该管道使用LLaMA-2-7B-Chat、SBERT、BERTScore、CLIP、BLIP-2和Stable Diffusion实现，并基于新颖性、惊讶性和相关性构建了复合奖励函数。在refine阶段，创意分数提高了52.5%（从1.179增加到1.898，t=-5.56，p<0.001），最终输出达到2.010，提升了70.4%。经过refine的口号缩短了48.4%，更加新颖，相关性仅下降了4.0%。跨模态测试显示口号与图像的强对齐（CLIPScore: 0.249；BERTScore F1: 0.816）。在人类评估中，60%的输出得分不低于4.0，具有隐喻的口号（平均4.09）优于字面的口号（3.99）。反馈强调了风格的精准和情感的共鸣。这些结果表明，以错误为中心、基于反馈的生成能够提升创造力，为自我进化的、与人类目标一致的创造型AI指明了一条可扩展的道路。', 'title_zh': 'E.A.R.T.H.: 通过模型误差结构化生成式AI的创意进化'}
{'arxiv_id': 'arXiv:2507.17988', 'title': 'Synthesis of timeline-based planning strategies avoiding determinization', 'authors': 'Dario Della Monica, Angelo Montanari, Pietro Sala', 'link': 'https://arxiv.org/abs/2507.17988', 'abstract': "Qualitative timeline-based planning models domains as sets of independent, but\ninteracting, components whose behaviors over time, the timelines, are governed\nby sets of qualitative temporal constraints (ordering relations), called\nsynchronization rules.\nIts plan-existence problem has been shown to be PSPACE-complete; in\nparticular, PSPACE-membership has been proved via reduction to the\nnonemptiness problem for nondeterministic finite automata.\nHowever, nondeterministic automata cannot be directly used to synthesize\nplanning strategies as a costly determinization step is needed.\nIn this paper, we identify a fragment of qualitative timeline-based planning\nwhose plan-existence problem can be directly mapped into the nonemptiness\nproblem of deterministic finite automata, which can then\nsynthesize strategies.\nIn addition, we identify a maximal subset of Allen's relations that fits into\nsuch a deterministic fragment.", 'abstract_zh': '基于定性时间线的规划模型将领域视为一组独立但相互作用的组件，其随时间的行为（时间线）由一组定性的时间约束（序关系）控制，这些约束被称为同步规则。', 'title_zh': '基于时间线的规划策略合成避免确定化'}
{'arxiv_id': 'arXiv:2507.17927', 'title': 'SMARTAPS: Tool-augmented LLMs for Operations Management', 'authors': 'Timothy Tin Long Yu, Mahdi Mostajabdaveh, Jabo Serge Byusa, Rindra Ramamonjison, Giuseppe Carenini, Kun Mao, Zirui Zhou, Yong Zhang', 'link': 'https://arxiv.org/abs/2507.17927', 'abstract': 'Large language models (LLMs) present intriguing opportunities to enhance user interaction with traditional algorithms and tools in real-world applications. An advanced planning system (APS) is a sophisticated software that leverages optimization to help operations planners create, interpret, and modify an operational plan. While highly beneficial, many customers are priced out of using an APS due to the ongoing costs of consultants responsible for customization and maintenance. To address the need for a more accessible APS expressed by supply chain planners, we present SmartAPS, a conversational system built on a tool-augmented LLM. Our system provides operations planners with an intuitive natural language chat interface, allowing them to query information, perform counterfactual reasoning, receive recommendations, and execute scenario analysis to better manage their operation. A short video demonstrating the system has been released: this https URL', 'abstract_zh': '大型语言模型（LLMs）为增强传统算法和工具在实际应用中与用户的互动提供了令人intriguing兴奋的机会。一种先进的计划系统（APS）是一种利用优化来帮助运营规划者创建、解释和修改运营计划的高级软件。虽然功能强大，但由于负责定制和维护的咨询师的持续成本，许多客户无法使用APS。为应对供应链规划者对更易于访问的APS的需求，我们提出了基于工具增强的大语言模型构建的SmartAPS，一种会话系统。该系统为运营规划者提供了一个直观的自然语言聊天界面，使他们能够查询信息、进行反事实推理、接收建议并执行情景分析，以更好地管理其运营。有关该系统的short简短视频已经发布：this https URL。', 'title_zh': 'SMARTAPS: 工具增强的大语言模型在运营管理中的应用'}
{'arxiv_id': 'arXiv:2507.17874', 'title': 'I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis', 'authors': 'SaiBarath Sundar, Pranav Satheesan, Udayaadithya Avadhanam', 'link': 'https://arxiv.org/abs/2507.17874', 'abstract': 'Recent advances in agentic systems for data analysis have emphasized automation of insight generation through multi-agent frameworks, and orchestration layers. While these systems effectively manage tasks like query translation, data transformation, and visualization, they often overlook the structured reasoning process underlying analytical thinking. Reasoning large language models (LLMs) used for multi-step problem solving are trained as general-purpose problem solvers. As a result, their reasoning or thinking steps do not adhere to fixed processes for specific tasks. Real-world data analysis requires a consistent cognitive workflow: interpreting vague goals, grounding them in contextual knowledge, constructing abstract plans, and adapting execution based on intermediate outcomes. We introduce I2I-STRADA (Information-to-Insight via Structured Reasoning Agent for Data Analysis), an agentic architecture designed to formalize this reasoning process. I2I-STRADA focuses on modeling how analysis unfolds via modular sub-tasks that reflect the cognitive steps of analytical reasoning. Evaluations on the DABstep and DABench benchmarks show that I2I-STRADA outperforms prior systems in planning coherence and insight alignment, highlighting the importance of structured cognitive workflows in agent design for data analysis.', 'abstract_zh': '近年来，代理系统在数据分析领域的进展强调了通过多代理框架和编排层自动产生洞见。虽然这些系统有效管理查询转换、数据转换和可视化等任务，但往往忽视了分析思考背后的结构化推理过程。用于多步问题解决的大型语言模型（LLMs）作为通用问题解决者进行训练，因此它们的推理或思维步骤不符合特定任务的固定流程。实际数据分析需要一致的认知工作流程：解释模糊的目标、在背景知识中定位它们、构建抽象计划，并根据中间结果调整执行。我们提出了I2I-STRADA（Information-to-Insight via Structured Reasoning Agent for Data Analysis），这是一种代理架构，旨在正式化这一推理过程。I2I-STRADA着重于建模分析如何通过反映分析推理认知步骤的模块化子任务展开。在DABstep和DABench基准测试上的评估结果显示，I2I-STRADA在规划连贯性和洞见对齐方面优于之前系统，突显了在数据分析中设计代理时结构化认知工作流程的重要性。', 'title_zh': 'I2I-STRADA —— 通过结构化推理代理从信息到洞察的数据分析'}
{'arxiv_id': 'arXiv:2507.17777', 'title': 'ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics', 'authors': 'Theofanis Aravanis, Grigorios Chrimatopoulos, Mohammad Ferdows, Michalis Xenos, Efstratios Em Tzirtzilakis', 'link': 'https://arxiv.org/abs/2507.17777', 'abstract': 'Unlike conventional Machine-Learning (ML) approaches, often criticized as "black boxes", Symbolic Regression (SR) stands out as a powerful tool for revealing interpretable mathematical relationships in complex physical systems, requiring no a priori assumptions about models\' structures. Motivated by the recognition that, in fluid mechanics, an understanding of the underlying flow physics is as crucial as accurate prediction, this study applies SR to model a fundamental three-dimensional (3D) incompressible flow in a rectangular channel, focusing on the (axial) velocity and pressure fields under laminar conditions. By employing the PySR library, compact symbolic equations were derived directly from numerical simulation data, revealing key characteristics of the flow dynamics. These equations not only approximate the parabolic velocity profile and pressure drop observed in the studied fluid flow, but also perfectly coincide with analytical solutions from the literature. Furthermore, we propose an innovative approach that integrates SR with the knowledge-representation framework of Answer Set Programming (ASP), combining the generative power of SR with the declarative reasoning strengths of ASP. The proposed hybrid SR/ASP framework ensures that the SR-generated symbolic expressions are not only statistically accurate, but also physically plausible, adhering to domain-specific principles. Overall, the study highlights two key contributions: SR\'s ability to simplify complex flow behaviours into concise, interpretable equations, and the potential of knowledge-representation approaches to improve the reliability and alignment of data-driven SR models with domain principles. Insights from the examined 3D channel flow pave the way for integrating such hybrid approaches into efficient frameworks, [...] where explainable predictions and real-time data analysis are crucial.', 'abstract_zh': '不同于常被批评为“黑盒”方法的传统机器学习（ML）技术，符号回归（SR）作为一种工具能够揭示复杂物理系统中的可解释数学关系，无需预先假设模型结构。基于流体力学中对流体物理学理解与精确预测同样重要的认识，本研究将SR应用于建模矩形通道中的三维（3D）不可压缩流动，重点关注层流条件下的轴向速度和压力场。通过使用PySR库，直接从数值模拟数据中推导出简洁的符号方程，揭示了流动态的关键特征。这些方程不仅近似了所研究流体流动中观察到的抛物线型速度分布和压力降，还与文献中的解析解完全吻合。此外，本文还提出了一种创新的方法，将SR与解答集编程（ASP）的知识表示框架相结合，利用SR的生成能力与ASP的声明式推理优势。所提出的 hybrid SR/ASP 框架确保SR生成的符号表达式不仅在统计上准确，还在物理上合理，遵循特定领域原理。总体而言，本文突出了两个关键贡献：SR简化复杂流行为为简洁可解释方程的能力，以及知识表示方法提升数据驱动SR模型与领域原理可靠性和一致性的潜力。针对检查的三维通道流动，为将此类混合方法集成到高效框架中奠定了基础，该框架中可解释的预测和实时数据分析至关重要。', 'title_zh': 'ASP辅助符号回归：揭示流体力学中的隐藏物理规律'}
{'arxiv_id': 'arXiv:2507.18632', 'title': 'SIDA: Synthetic Image Driven Zero-shot Domain Adaptation', 'authors': 'Ye-Chan Kim, SeungJu Cha, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim', 'link': 'https://arxiv.org/abs/2507.18632', 'abstract': "Zero-shot domain adaptation is a method for adapting a model to a target domain without utilizing target domain image data. To enable adaptation without target images, existing studies utilize CLIP's embedding space and text description to simulate target-like style features. Despite the previous achievements in zero-shot domain adaptation, we observe that these text-driven methods struggle to capture complex real-world variations and significantly increase adaptation time due to their alignment process. Instead of relying on text descriptions, we explore solutions leveraging image data, which provides diverse and more fine-grained style cues. In this work, we propose SIDA, a novel and efficient zero-shot domain adaptation method leveraging synthetic images. To generate synthetic images, we first create detailed, source-like images and apply image translation to reflect the style of the target domain. We then utilize the style features of these synthetic images as a proxy for the target domain. Based on these features, we introduce Domain Mix and Patch Style Transfer modules, which enable effective modeling of real-world variations. In particular, Domain Mix blends multiple styles to expand the intra-domain representations, and Patch Style Transfer assigns different styles to individual patches. We demonstrate the effectiveness of our method by showing state-of-the-art performance in diverse zero-shot adaptation scenarios, particularly in challenging domains. Moreover, our approach achieves high efficiency by significantly reducing the overall adaptation time.", 'abstract_zh': '基于合成图像的零样本域适应方法', 'title_zh': 'SIDA: 合成图像驱动的零样本领域适应'}
{'arxiv_id': 'arXiv:2507.18625', 'title': '3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation', 'authors': 'Shuqing Li, Anson Y. Lam, Yun Peng, Wenxuan Wang, Michael R. Lyu', 'link': 'https://arxiv.org/abs/2507.18625', 'abstract': 'Graphical user interface (UI) software has undergone a fundamental transformation from traditional two-dimensional (2D) desktop/web/mobile interfaces to spatial three-dimensional (3D) environments. While existing work has made remarkable success in automated 2D software generation, such as HTML/CSS and mobile app interface code synthesis, the generation of 3D software still remains under-explored. Current methods for 3D software generation usually generate the 3D environments as a whole and cannot modify or control specific elements in the software. Furthermore, these methods struggle to handle the complex spatial and semantic constraints inherent in the real world. To address the challenges, we present Scenethesis, a novel requirement-sensitive 3D software synthesis approach that maintains formal traceability between user specifications and generated 3D software. Scenethesis is built upon ScenethesisLang, a domain-specific language that serves as a granular constraint-aware intermediate representation (IR) to bridge natural language requirements and executable 3D software. It serves both as a comprehensive scene description language enabling fine-grained modification of 3D software elements and as a formal constraint-expressive specification language capable of expressing complex spatial constraints. By decomposing 3D software synthesis into stages operating on ScenethesisLang, Scenethesis enables independent verification, targeted modification, and systematic constraint satisfaction. Our evaluation demonstrates that Scenethesis accurately captures over 80% of user requirements and satisfies more than 90% of hard constraints while handling over 100 constraints simultaneously. Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual evaluation scores compared to the state-of-the-art method.', 'abstract_zh': '图形用户界面（UI）软件从传统的二维（2D）桌面/网页/移动接口经历了根本性的转变，发展到三维（3D）空间环境。尽管现有工作在自动化的2D软件生成方面取得了显著的成功，例如HTML/CSS和移动应用界面代码合成，但3D软件的生成仍然被严重忽视。当前的3D软件生成方法通常一次性生成整个3D环境，无法修改或控制软件中的特定元素，并且难以处理现实世界中固有的复杂空间和语义约束。为应对这些挑战，我们提出了Scenethesis，一种新的、面向要求的3D软件合成方法，它在用户规范和生成的3D软件之间保持了形式化的可追溯性。Scenethesis基于ScenethesisLang，这是一种领域特定语言，作为粒度感知的形式化约束中间表示（IR），连接自然语言需求和可执行的3D软件。它既是一种全面的场景描述语言，允许对3D软件元素进行精细修改，也是一种能够表达复杂空间约束的形式化约束表达规范语言。通过将3D软件合成分解为ScenethesisLang上的阶段操作，Scenethesis实现了独立验证、目标修改和系统约束满足。我们的评估表明，Scenethesis能够准确捕获用户需求的80%以上，并满足超过90%的硬约束，同时处理超过100个约束。此外，与最先进的方法相比，Scenethesis在BLIP-2可视化评估得分上获得了42.8%的提升。', 'title_zh': '基于约束表达中间表示的3D软件合成'}
{'arxiv_id': 'arXiv:2507.18623', 'title': 'Moving Out: Physically-grounded Human-AI Collaboration', 'authors': 'Xuhui Kang, Sung-Wook Lee, Haolin Liu, Yuyan Wang, Yen-Ling Kuo', 'link': 'https://arxiv.org/abs/2507.18623', 'abstract': "The ability to adapt to physical actions and constraints in an environment is crucial for embodied agents (e.g., robots) to effectively collaborate with humans. Such physically grounded human-AI collaboration must account for the increased complexity of the continuous state-action space and constrained dynamics caused by physical constraints. In this paper, we introduce \\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a wide range of collaboration modes affected by physical attributes and constraints, such as moving heavy items together and maintaining consistent actions to move a big item around a corner. Using Moving Out, we designed two tasks and collected human-human interaction data to evaluate models' abilities to adapt to diverse human behaviors and unseen physical attributes. To address the challenges in physical environments, we propose a novel method, BASS (Behavior Augmentation, Simulation, and Selection), to enhance the diversity of agents and their understanding of the outcome of actions. Our experiments show that BASS outperforms state-of-the-art models in AI-AI and human-AI collaboration. The project page is available at \\href{this https URL}{this https URL\\_ai/}.", 'abstract_zh': '具备适应环境物理动作和约束的能力对于使实体代理（例如机器人）有效地与人类协作至关重要。这种基于物理的人机协作必须考虑到由于物理约束引起的连续状态-动作空间和受限动力学的复杂性增加。本文介绍了Moving Out，这是一个新的框架，涵盖了广泛受物理属性和约束影响的合作模式，如共同搬运重物和在转角处保持一致动作以移动大型物品。利用Moving Out，我们设计了两个任务并收集了人类-人类交互数据以评估模型适应多样化人类行为和未见物理属性的能力。为了应对物理环境中的挑战，我们提出了一种新型方法BASS（行为增强、仿真和选择），以增强代理人多样性和对其行为结果的理解。我们的实验表明，BASS在人机协作和AI-AI协作中优于现有最先进的模型。项目页面详见[这个链接](https://github.com/_ai/)。', 'title_zh': '移步而出：基于物理的人机协作'}
{'arxiv_id': 'arXiv:2507.18616', 'title': 'SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning', 'authors': 'Si-Woo Kim, MinJu Jeon, Ye-Chan Kim, Soeun Lee, Taewhan Kim, Dong-Jin Kim', 'link': 'https://arxiv.org/abs/2507.18616', 'abstract': 'Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets generated by text-to-image (T2I) models to mitigate the need for costly manual annotation. However, these T2I models often produce images that exhibit semantic misalignments with their corresponding input captions (e.g., missing objects, incorrect attributes), resulting in noisy synthetic image-caption pairs that can hinder model training. Existing dataset pruning techniques are largely designed for removing noisy text in web-crawled data. However, these methods are ill-suited for the distinct challenges of synthetic data, where captions are typically well-formed, but images may be inaccurate representations. To address this gap, we introduce SynC, a novel framework specifically designed to refine synthetic image-caption datasets for ZIC. Instead of conventional filtering or regeneration, SynC focuses on reassigning captions to the most semantically aligned images already present within the synthetic image pool. Our approach employs a one-to-many mapping strategy by initially retrieving multiple relevant candidate images for each caption. We then apply a cycle-consistency-inspired alignment scorer that selects the best image by verifying its ability to retrieve the original caption via image-to-text retrieval. Extensive evaluations demonstrate that SynC consistently and significantly improves performance across various ZIC models on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art results in several scenarios. SynC offers an effective strategy for curating refined synthetic data to enhance ZIC.', 'abstract_zh': '零样本图像描述（ZIC）越来越多地利用由文本到图像（T2I）模型生成的合成数据集，以减少对昂贵的手动标注的需求。然而，这些T2I模型生成的图像常常与对应的输入描述存在语义不一致（例如，缺少对象、属性错误），导致噪声合成图像-描述对，这可能会阻碍模型训练。现有的数据集精简技术大多设计用于去除从网络抓取数据中的噪声文本。然而，这些方法不适用于合成数据的特定挑战，其中描述通常结构良好，但图像可能是不准确的表示。为此，我们提出了SynC，一种专门设计用于 refinement合成图像-描述数据集以供ZIC使用的新型框架。相较传统的过滤或再生，SynC专注于将每个描述重新分配给合成图像池中与其最语义匹配的图像。我们的方法采用一种一对一多映射策略，初始阶段为每个描述检索多个相关的候选图像。随后，我们应用一个受循环一致性启发的对齐评分器，通过图像到文本的检索验证图像能否恢复原始描述，从而选择最佳图像。广泛评估表明，SynC在多个标准基准（MS-COCO、Flickr30k、NoCaps）上的一系列ZIC模型中一致且显著地提高了性能，并在多种场景中达到了最先进的结果。SynC提供了一种有效的方法来筛选精炼的合成数据，以提升零样本图像描述的性能。', 'title_zh': 'SynC: 一种基于一对一映射合成图像标题数据集精炼方法的零样本图像描述研究'}
{'arxiv_id': 'arXiv:2507.18612', 'title': 'Approximate SMT Counting Beyond Discrete Domains', 'authors': 'Arijit Shaw, Kuldeep S. Meel', 'link': 'https://arxiv.org/abs/2507.18612', 'abstract': 'Satisfiability Modulo Theory (SMT) solvers have advanced automated reasoning, solving complex formulas across discrete and continuous domains. Recent progress in propositional model counting motivates extending SMT capabilities toward model counting, especially for hybrid SMT formulas. Existing approaches, like bit-blasting, are limited to discrete variables, highlighting the challenge of counting solutions projected onto the discrete domain in hybrid formulas.\nWe introduce pact, an SMT model counter for hybrid formulas that uses hashing-based approximate model counting to estimate solutions with theoretical guarantees. pact makes a logarithmic number of SMT solver calls relative to the projection variables, leveraging optimized hash functions. pact achieves significant performance improvements over baselines on a large suite of benchmarks. In particular, out of 14,202 instances, pact successfully finished on 603 instances, while Baseline could only finish on 13 instances.', 'abstract_zh': 'Satisfiability Modulo Theory (SMT)求解器通过解决离散和连续域的复杂公式，推动了自动化推理的发展。命题模型计数的最近进展促使SMT能力向模型计数扩展，特别是针对混合SMT公式。现有的方法如位爆炸处理，仅限于离散变量，凸显了在混合公式中对投射到离散域的解进行计数的挑战。\n\n我们引入了pact，一种针对混合公式的SMT模型计数器，利用基于哈希的近似模型计数来提供理论上的解估计。pact相对于投影变量仅需要对数级的SMT求解器调用，并利用优化的哈希函数。pact在大量基准测试上实现了显著的性能改进，在14,202个实例中，pact成功完成的有603个，而基线只能完成13个。', 'title_zh': '超越离散域的近似SMT计数'}
{'arxiv_id': 'arXiv:2507.18594', 'title': 'DRWKV: Focusing on Object Edges for Low-Light Image Enhancement', 'authors': 'Xuecheng Bai, Yuxiang Wang, Boyu Hu, Qinyuan Jie, Chuanzhi Xu, Hongru Xiao, Kechen Li, Vera Chung', 'link': 'https://arxiv.org/abs/2507.18594', 'abstract': 'Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.', 'abstract_zh': '低光图像增强仍是一项具有挑战性的任务，特别是在极端光照降级情况下保持对象边缘连续性和精细结构细节方面。本文提出了一种新型模型DRWKV（详细接收率加权键值），该模型整合了我们提出的全局边缘_retinex_理论，可以有效解耦光照和边缘结构，从而增强边缘保真度。其次，我们引入了演化WKV注意力机制，这是一种螺旋扫描机制，可以捕捉空间边缘连续性并更有效地建模不规则结构。第三，我们设计了双边光谱对齐器（Bi-SAB）和特定的MS2损失，以联合对亮度和色度特征进行对齐，从而改善视觉自然度并减轻伪影。在五个LLIE基准上的广泛实验表明，DRWKV在PSNR、SSIM和NIQE上取得了领先性能，同时保持较低的计算复杂度。此外，DRWKV在低光多对象跟踪任务中的下游性能得到了增强，验证了其泛化能力。', 'title_zh': 'DRWKV：聚焦物体边缘的低光图像增强'}
{'arxiv_id': 'arXiv:2507.18587', 'title': 'A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff', 'authors': 'Jérôme Emery, Ali Hasanzadeh Karkan, Jean-François Frigon, François Leduc-Primeau', 'link': 'https://arxiv.org/abs/2507.18587', 'abstract': 'Deep learning (DL) has emerged as a solution for precoding in massive multiple-input multiple-output (mMIMO) systems due to its capacity to learn the characteristics of the propagation environment. However, training such a model requires high-quality, local datasets at the deployment site, which are often difficult to collect. We propose a transformer-based foundation model for mMIMO precoding that seeks to minimize the energy consumption of the transmitter while dynamically adapting to per-user rate requirements. At equal energy consumption, zero-shot deployment of the proposed foundation model significantly outperforms zero forcing, and approaches weighted minimum mean squared error performance with 8x less complexity. To address model adaptation in data-scarce settings, we introduce a data augmentation method that finds training samples similar to the target distribution by computing the cosine similarity between the outputs of the pre-trained feature extractor. Our work enables the implementation of DL-based solutions in practice by addressing challenges of data availability and training complexity. Moreover, the ability to dynamically configure per-user rate requirements can be leveraged by higher level resource allocation and scheduling algorithms for greater control over energy efficiency, spectral efficiency and fairness.', 'abstract_zh': '基于变压器的基础模型在大规模多输入多输出系统中的稀疏数据场景下实现深度学习预编码能耗最小化动态适配', 'title_zh': '大规模MIMO预编码的自适应用户率-功率tradeoff基础模型'}
{'arxiv_id': 'arXiv:2507.18584', 'title': 'AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs', 'authors': 'Xiaopeng Ke, Hexuan Deng, Xuebo Liu, Jun Rao, Zhenxi Song, Jun Yu, Min Zhang', 'link': 'https://arxiv.org/abs/2507.18584', 'abstract': 'Despite the impressive performance of large language models (LLMs) in general domains, they often underperform in specialized domains. Existing approaches typically rely on data synthesis methods and yield promising results by using unlabeled data to capture domain-specific features. However, these methods either incur high computational costs or suffer from performance limitations, while also demonstrating insufficient generalization across different tasks. To address these challenges, we propose AQuilt, a framework for constructing instruction-tuning data for any specialized domains from corresponding unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic, and Task type. By incorporating logic and inspection, we encourage reasoning processes and self-inspection to enhance model performance. Moreover, customizable task instructions enable high-quality data generation for any task. As a result, we construct a dataset of 703k examples to train a powerful data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. Further analysis demonstrates that our generated data exhibits higher relevance to downstream tasks. Source code, models, and scripts are available at this https URL.', 'abstract_zh': '尽管大型语言模型（LLMs）在通用领域表现出色，但在专门领域往往表现不佳。现有方法通常依赖数据合成方法，并通过使用未标记数据来捕获领域特定特征，从而取得令人鼓舞的结果。然而，这些方法要么计算成本高昂，要么性能有限，同时也表现出在不同任务上泛化不足的问题。为应对这些挑战，我们提出AQuilt框架，用于从相应的未标记数据中构建任何专门领域的指令调优数据，包括Answer、Question、Unlabeled数据、Inspection、Logic和Task类型。通过整合逻辑和检查，我们鼓励推理过程和自我检查以提升模型性能。此外，可定制的任务指令能够为任何任务生成高质量的数据。因此，我们构建了一个包含703,000个示例的数据集来训练一个强大的数据合成模型。实验结果显示，AQuilt与DeepSeek-V3性能相当，但仅使用17%的生产成本。进一步的分析表明，我们生成的数据与下游任务的相关性更高。源代码、模型和脚本可在此链接访问。', 'title_zh': 'AQuilt: 将逻辑与自我检查融入低成本、高相关性数据合成以供专业大语言模型使用'}
{'arxiv_id': 'arXiv:2507.18583', 'title': 'DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data', 'authors': 'Zhengyun Zhao, Huaiyuan Ying, Yue Zhong, Sheng Yu', 'link': 'https://arxiv.org/abs/2507.18583', 'abstract': "Electronic Health Records (EHRs) are pivotal in clinical practices, yet their retrieval remains a challenge mainly due to semantic gap issues. Recent advancements in dense retrieval offer promising solutions but existing models, both general-domain and biomedical-domain, fall short due to insufficient medical knowledge or mismatched training corpora. This paper introduces \\texttt{this http URL}, a series of dense retrieval models specifically tailored for EHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV discharge summaries to address the need for extensive medical knowledge and large-scale training data. The first stage involves medical entity extraction and knowledge injection from a biomedical knowledge graph, while the second stage employs large language models to generate diverse training data. We train two variants of \\texttt{this http URL}, with 110M and 7B parameters, respectively. Evaluated on the CliniQ benchmark, our models significantly outperforms all existing dense retrievers, achieving state-of-the-art results. Detailed analyses confirm our models' superiority across various match and query types, particularly in challenging semantic matches like implication and abbreviation. Ablation studies validate the effectiveness of each pipeline component, and supplementary experiments on EHR QA datasets demonstrate the models' generalizability on natural language questions, including complex ones with multiple entities. This work significantly advances EHR retrieval, offering a robust solution for clinical applications.", 'abstract_zh': '电子健康记录（EHRs）在临床实践中至关重要，但其检索仍面临挑战，主要原因是语义差距问题。最近在密集检索方面的进步提供了有希望的解决方案，但现有的通用领域和生物医学领域模型由于缺乏医学知识或训练语料库不匹配的原因仍存在不足。本文介绍了\\texttt{this http URL}，这是一种专门针对EHR检索的密集检索模型系列。我们提出了一种两阶段训练管道，利用MIMIC-IV出院总结来解决需要广泛医学知识和大规模训练数据的问题。第一阶段涉及从生物医学知识图谱中提取医学实体并注入知识，第二阶段则使用大规模语言模型生成多样化的训练数据。我们分别训练了带有110M和7B参数的\\texttt{this http URL}两种变体。在CliniQ基准测试中，我们的模型显著优于所有现有的密集检索器，达到了最先进的结果。详细的分析证实了我们在各种匹配和查询类型中模型的优越性，特别是在含义和缩写等具有挑战性的语义匹配中。消融研究验证了每个管道组件的有效性，并补充实验表明，模型在自然语言问答数据集上的泛化能力，包括具有多个实体的复杂问题。这项工作极大地推动了EHR检索的发展，为临床应用提供了稳健的解决方案。', 'title_zh': 'DR.EHR：知识注入与合成数据驱动的电子健康记录密集检索'}
{'arxiv_id': 'arXiv:2507.18577', 'title': 'Advancing Financial Engineering with Foundation Models: Progress, Applications, and Challenges', 'authors': 'Liyuan Chen, Shuoling Liu, Jiangpeng Yan, Xiaoyu Wang, Henglin Liu, Chuang Li, Kecheng Jiao, Jixuan Ying, Yang Veronica Liu, Qiang Yang, Xiu Li', 'link': 'https://arxiv.org/abs/2507.18577', 'abstract': 'The advent of foundation models (FMs) - large-scale pre-trained models with strong generalization capabilities - has opened new frontiers for financial engineering. While general-purpose FMs such as GPT-4 and Gemini have demonstrated promising performance in tasks ranging from financial report summarization to sentiment-aware forecasting, many financial applications remain constrained by unique domain requirements such as multimodal reasoning, regulatory compliance, and data privacy. These challenges have spurred the emergence of Financial Foundation Models (FFMs) - a new class of models explicitly designed for finance. This survey presents a comprehensive overview of FFMs, with a taxonomy spanning three key modalities: Financial Language Foundation Models (FinLFMs), Financial Time-Series Foundation Models (FinTSFMs), and Financial Visual-Language Foundation Models (FinVLFMs). We review their architectures, training methodologies, datasets, and real-world applications. Furthermore, we identify critical challenges in data availability, algorithmic scalability, and infrastructure constraints, and offer insights into future research opportunities. We hope this survey serves as both a comprehensive reference for understanding FFMs and a practical roadmap for future innovation. An updated collection of FFM-related publications and resources will be maintained on our website this https URL.', 'abstract_zh': '基础模型(FMs)的兴起——大规模预训练模型具备强大的通用泛化能力——为金融工程开辟了新的领域。尽管通用的基础模型如GPT-4和Gemini在金融报告总结和情感感知预测等任务中已展现出有希望的表现，但许多金融应用仍受到多模态推理、合规性要求和数据隐私等独特领域需求的限制。这些挑战促进了金融基础模型(FFMs)的出现——一类明确针对金融领域设计的新模型。本文综述了FFMs，涵盖三种关键模态的分类体系：金融语言基础模型(FinLFMs)、金融时间序列基础模型(FinTSFMs)和金融视觉语言基础模型(FinVLFMs)，并对其架构、训练方法、数据集和实际应用进行了回顾。此外，我们还识别了数据可用性、算法扩展性和基础设施限制等关键挑战，并提供了未来研究机会的见解。我们希望本文综述能够成为理解FFMs的全面参考，并为未来的创新提供实用的路线图。有关FFM的相关出版物和资源将在我们网站上持续更新。', 'title_zh': '借助基础模型推动金融工程发展：进展、应用与挑战'}
{'arxiv_id': 'arXiv:2507.18572', 'title': 'PosterMate: Audience-driven Collaborative Persona Agents for Poster Design', 'authors': 'Donghoon Shin, Daniel Lee, Gary Hsieh, Gromit Yeuk-Yin Chan', 'link': 'https://arxiv.org/abs/2507.18572', 'abstract': "Poster designing can benefit from synchronous feedback from target audiences. However, gathering audiences with diverse perspectives and reconciling them on design edits can be challenging. Recent generative AI models present opportunities to simulate human-like interactions, but it is unclear how they may be used for feedback processes in design. We introduce PosterMate, a poster design assistant that facilitates collaboration by creating audience-driven persona agents constructed from marketing documents. PosterMate gathers feedback from each persona agent regarding poster components, and stimulates discussion with the help of a moderator to reach a conclusion. These agreed-upon edits can then be directly integrated into the poster design. Through our user study (N=12), we identified the potential of PosterMate to capture overlooked viewpoints, while serving as an effective prototyping tool. Additionally, our controlled online evaluation (N=100) revealed that the feedback from an individual persona agent is appropriate given its persona identity, and the discussion effectively synthesizes the different persona agents' perspectives.", 'abstract_zh': 'Poster设计可以从目标受众的同步反馈中受益，但 Gather多元视角的受众并就设计修改达成一致具有挑战性。近期的生成式AI模型提供了模拟人类互动的机会，但它们在设计反馈过程中的应用尚不明确。我们介绍了PosterMate，这是一种通过从营销文件中构建受众驱动的人物代理来促进合作的Poster设计助手。PosterMate从每个人物代理处收集关于Poster组件的反馈，并在调解人的帮助下促进讨论，以达成共识。这些达成一致的修改可以直接集成到Poster设计中。通过我们的用户研究（N=12），我们发现PosterMate能够捕捉被忽视的观点，同时作为一种有效的原型制作工具。此外，我们的可控在线评估（N=100）表明，单个人物代理提供的反馈与其人物身份相符，讨论有效地概括了不同人物代理的观点。', 'title_zh': 'PosterMate: 以受众为导向的合作型人物代理模型用于海报设计'}
{'arxiv_id': 'arXiv:2507.18567', 'title': 'Proceedings 19th International Workshop on the ACL2 Theorem Prover and Its Applications', 'authors': 'Ruben Gamboa, Panagiotis Manolios', 'link': 'https://arxiv.org/abs/2507.18567', 'abstract': 'The ACL2 Workshop series is the major technical forum for users of the ACL2 theorem proving system to present research related to the ACL2 theorem prover and its applications. ACL2 is an industrial-strength automated reasoning system, the latest in the Boyer-Moore family of theorem provers. The 2005 ACM Software System Award was awarded to Boyer, Kaufmann, and Moore for their work on ACL2 and the other theorem provers in the Boyer-Moore family.', 'abstract_zh': 'ACL2研讨会系列是ACL2定理证明系统用户讨论与ACL2定理证明器及其应用相关的研究的主要技术论坛。ACL2是Boyer-Moore定理证明家族中的工业强度自动化推理系统。Boyer、Kaufmann和Moore因ACL2及其他Boyer-Moore家族的定理证明器的工作于2005年获得ACM软件系统奖。', 'title_zh': '第19届ACL2定理证明器及其应用国际研讨会论文集'}
{'arxiv_id': 'arXiv:2507.18562', 'title': 'GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation', 'authors': 'Jiafeng Xiong, Yuting Zhao', 'link': 'https://arxiv.org/abs/2507.18562', 'abstract': 'Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference.', 'abstract_zh': '多模态机器翻译（MMT）已经证明了视觉信息在机器翻译中的显著帮助。然而，现有的MMT方法在利用模态差距时面临挑战，它们通过强制执行刚性的视觉-语言对齐而在训练的多模态领域内进行推理时受到限制。在本文中，我们构建了新颖的多模态场景图以保留和整合模态特定信息，并引入了GIIFT，一种两阶段的图引导归纳无图像多模态机器翻译框架，该框架使用跨模态图注意网络适配器在统一融合空间中学习多模态知识，并归纳泛化到更广泛的无图像翻译领域。在Multi30K数据集的英法和英德任务上的实验结果表明，我们的GIIFT超过了现有方法并达到了最新水平，即使在推理时不使用图像也是如此。在WMT基准上的结果表明，GIIFT在无图像翻译基线上的显著改进，证明了GIIFT在归纳无图像推理中的优势。', 'title_zh': 'GIIFT：图引导的归纳图像无介 multimodal 机器翻译'}
{'arxiv_id': 'arXiv:2507.18561', 'title': 'Beyond Internal Data: Constructing Complete Datasets for Fairness Testing', 'authors': 'Varsha Ramineni, Hossein A. Rahmani, Emine Yilmaz, David Barber', 'link': 'https://arxiv.org/abs/2507.18561', 'abstract': 'As AI becomes prevalent in high-risk domains and decision-making, it is essential to test for potential harms and biases. This urgency is reflected by the global emergence of AI regulations that emphasise fairness and adequate testing, with some mandating independent bias audits. However, procuring the necessary data for fairness testing remains a significant challenge. Particularly in industry settings, legal and privacy concerns restrict the collection of demographic data required to assess group disparities, and auditors face practical and cultural challenges in gaining access to data. Further, internal historical datasets are often insufficiently representative to identify real-world biases. This work focuses on evaluating classifier fairness when complete datasets including demographics are inaccessible. We propose leveraging separate overlapping datasets to construct complete synthetic data that includes demographic information and accurately reflects the underlying relationships between protected attributes and model features. We validate the fidelity of the synthetic data by comparing it to real data, and empirically demonstrate that fairness metrics derived from testing on such synthetic data are consistent with those obtained from real data. This work, therefore, offers a path to overcome real-world data scarcity for fairness testing, enabling independent, model-agnostic evaluation of fairness, and serving as a viable substitute where real data is limited.', 'abstract_zh': '随着人工智能在高风险领域和决策中的普及，测试潜在危害和偏差变得至关重要。这反映在世界各国出台的强调公平和充分测试的人工智能法规中，部分法规甚至要求进行独立的偏差审计。然而，获取必要的公平测试数据依然是一项重大挑战。特别是在工业环境中，法律和隐私顾虑限制了用于评估群体差异所需的人口统计数据的收集，而审计人员在获取数据方面也面临实际和文化方面的挑战。此外，内部的历史数据集往往不足以识别现实世界的偏差。本文关注在无法访问完整数据集包括人口统计数据的情况下评估分类器公平性的问题。我们建议利用重叠的分离数据集来构建包含人口统计信息并准确反映受保护属性与模型特征之间关系的合成数据。通过将合成数据与真实数据进行对比评估其忠实度，并实证证明，基于合成数据进行测试所获得的公平性指标与基于真实数据所获得的结果是一致的。因此，本文为解决公平测试中的现实数据稀缺性问题提供了一条路径，有助于实现独立的、模型无关的公平性评估，并在现实数据有限时提供一个可行的替代方案。', 'title_zh': '超越内部数据：构建全面数据集以进行公平性测试'}
{'arxiv_id': 'arXiv:2507.18560', 'title': 'HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven Sentiment Integration for Financial Portfolio Optimization', 'authors': 'Benjamin Coriat, Eric Benhamou', 'link': 'https://arxiv.org/abs/2507.18560', 'abstract': 'This paper presents a novel hierarchical framework for portfolio optimization, integrating lightweight Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to combine sentiment signals from financial news with traditional market indicators. Our three-tier architecture employs base RL agents to process hybrid data, meta-agents to aggregate their decisions, and a super-agent to merge decisions based on market data and sentiment analysis. Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming equal-weighted and S&P 500 benchmarks. Key contributions include scalable cross-modal integration, a hierarchical RL structure for enhanced stability, and open-source reproducibility.', 'abstract_zh': '基于轻量级大型语言模型和深度强化学习的多层次投资组合优化框架', 'title_zh': 'HARLF: 分层强化学习和轻量级大语言模型驱动的情感集成在金融投资组合优化中的应用'}
{'arxiv_id': 'arXiv:2507.18552', 'title': 'VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding', 'authors': 'Baoyao Yang, Wanyun Li, Dixin Chen, Junxiang Chen, Wenbin Yao, Haifeng Lin', 'link': 'https://arxiv.org/abs/2507.18552', 'abstract': "This paper introduces VideoMind, a video-centric omni-modal dataset designed for deep video content cognition and enhanced multi-modal feature representation. The dataset comprises 103K video samples (3K reserved for testing), each paired with audio and systematically detailed textual descriptions. Specifically, every video and its audio is described across three hierarchical layers (factual, abstract, and intent), progressing from surface to depth. It contains over 22 million words, averaging ~225 words per sample. VideoMind's key distinction from existing datasets is its provision of intent expressions, which require contextual integration across the entire video and are not directly observable. These deep-cognitive expressions are generated using a Chain-of-Thought (COT) approach, prompting the mLLM through step-by-step reasoning. Each description includes annotations for subject, place, time, event, action, and intent, supporting downstream recognition tasks. Crucially, we establish a gold-standard benchmark with 3,000 manually validated samples for evaluating deep-cognitive video understanding. We design hybrid-cognitive retrieval experiments, scored by multi-level retrieval metrics, to appropriately assess deep video comprehension. Evaluation results for models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a powerful benchmark for fine-grained cross-modal alignment and advances fields requiring in-depth video understanding, such as emotion and intent recognition. The data is publicly available on GitHub, HuggingFace, and OpenDataLab, this https URL.", 'abstract_zh': 'VideoMind：一种面向视频的全模态数据集，用于深度视频内容认知和增强多模态特征表示', 'title_zh': 'VideoMind: 一种具有意图 grounding 的全模态视频数据集，用于深度认知视频理解'}
{'arxiv_id': 'arXiv:2507.18546', 'title': 'GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface', 'authors': 'Urchade Zaratiana, Gil Pasternak, Oliver Boyd, George Hurn-Maloney, Ash Lewis', 'link': 'https://arxiv.org/abs/2507.18546', 'abstract': 'Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions often require specialized models for different tasks or rely on computationally expensive large language models. We present GLiNER2, a unified framework that enhances the original GLiNER architecture to support named entity recognition, text classification, and hierarchical structured data extraction within a single efficient model. Built pretrained transformer encoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task composition through an intuitive schema-based interface. Our experiments demonstrate competitive performance across extraction and classification tasks with substantial improvements in deployment accessibility compared to LLM-based alternatives. We release GLiNER2 as an open-source pip-installable library with pre-trained models and documentation at this https URL.', 'abstract_zh': '基于GLiNER2的统一框架：一种高效的多任务信息提取和分类模型', 'title_zh': 'GLiNER2：一种基于模式驱动接口的高效多任务信息提取系统'}
{'arxiv_id': 'arXiv:2507.18533', 'title': 'C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation', 'authors': 'Magnus Bengtsson, Kenneth Östberg', 'link': 'https://arxiv.org/abs/2507.18533', 'abstract': "We introduce C2G-KD, a data-free knowledge distillation framework where a class-conditional generator is trained to produce synthetic samples guided by a frozen teacher model and geometric constraints derived from PCA. The generator never observes real training data but instead learns to activate the teacher's output through a combination of semantic and structural losses. By constraining generated samples to lie within class-specific PCA subspaces estimated from as few as two real examples per class, we preserve topological consistency and diversity. Experiments on MNIST show that even minimal class structure is sufficient to bootstrap useful synthetic training pipelines.", 'abstract_zh': '面向类别的数据免费知识蒸馏框架：利用PCA约束的生成器引导教师模型和几何约束进行学习', 'title_zh': 'C2G-KD: PCA约束生成器用于无数据知识精炼'}
{'arxiv_id': 'arXiv:2507.18521', 'title': 'GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning', 'authors': 'Zhongtian Sun, Anoushka Harit, Alexandra Cristea, Christl A. Donnelly, Pietro Liò', 'link': 'https://arxiv.org/abs/2507.18521', 'abstract': 'Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data but often struggle on heterophilous graphs, where connected nodes differ in features or class labels. This limitation arises from indiscriminate neighbor aggregation and insufficient incorporation of higher-order structural patterns. To address these challenges, we propose GLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel framework that integrates logic-guided reasoning, dynamic graph refinement, and adaptive clustering to enhance graph representation learning. GLANCE combines a logic layer for interpretable and structured embeddings, multi-head attention-based edge pruning for denoising graph structures, and clustering mechanisms for capturing global patterns. Experimental results in benchmark datasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE achieves competitive performance, offering robust and interpretable solutions for heterophilous graph scenarios. The proposed framework is lightweight, adaptable, and uniquely suited to the challenges of heterophilous graphs.', 'abstract_zh': 'Graph逻辑注意力网络与聚类增强（GLANCE）：一种用于异构图的新型图表示学习框架', 'title_zh': 'GLANCE: 图逻辑注意力网络结合簇增强的异ophilous图表示学习'}
{'arxiv_id': 'arXiv:2507.18512', 'title': 'Explaining How Visual, Textual and Multimodal Encoders Share Concepts', 'authors': 'Clément Cornet, Romaric Besançon, Hervé Le Borgne', 'link': 'https://arxiv.org/abs/2507.18512', 'abstract': 'Sparse autoencoders (SAEs) have emerged as a powerful technique for extracting human-interpretable features from neural networks activations. Previous works compared different models based on SAE-derived features but those comparisons have been restricted to models within the same modality. We propose a novel indicator allowing quantitative comparison of models across SAE features, and use it to conduct a comparative study of visual, textual and multimodal encoders. We also propose to quantify the Comparative Sharedness of individual features between different classes of models. With these two new tools, we conduct several studies on 21 encoders of the three types, with two significantly different sizes, and considering generalist and domain specific datasets. The results allow to revisit previous studies at the light of encoders trained in a multimodal context and to quantify to which extent all these models share some representations or features. They also suggest that visual features that are specific to VLMs among vision encoders are shared with text encoders, highlighting the impact of text pretraining. The code is available at this https URL', 'abstract_zh': '稀疏自编码器（SAEs）已 emerge 作为从神经网络激活中提取人类可解读特征的强而有力的技术。以往的研究基于 SAE 提取的特征比较了不同的模型，但这些比较局限于同一模态的模型之内。我们提出了一种新的指标，用于跨 SAE 特征定量比较模型，并使用该指标对视觉、文本和多模态编码器进行了比较研究。我们还提出了一种量化不同模型类别之间单个特征对比共享性的方法。利用这两种新的工具，我们对三类共 21 个编码器进行了研究，这些编码器具有两种显着不同的规模，并考虑了通用和特定领域的数据集。研究结果允许我们在多模态训练的编码器背景下重新审视之前的研究所提出的观点，并定量分析这些模型在多大程度上共享一些表示或特征。研究还表明，视觉编码器中特定于 VLM 的特征与文本编码器共享，突显了文本预训练的影响。代码可在以下网址获取：this https URL。', 'title_zh': '解释视觉、文本和多模态编码器如何共享概念'}
{'arxiv_id': 'arXiv:2507.18484', 'title': 'Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments', 'authors': 'Xiao Yang, Lingxuan Wu, Lizhong Wang, Chengyang Ying, Hang Su, Jun Zhu', 'link': 'https://arxiv.org/abs/2507.18484', 'abstract': 'Adversarial attacks in 3D environments have emerged as a critical threat to the reliability of visual perception systems, particularly in safety-sensitive applications such as identity verification and autonomous driving. These attacks employ adversarial patches and 3D objects to manipulate deep neural network (DNN) predictions by exploiting vulnerabilities within complex scenes. Existing defense mechanisms, such as adversarial training and purification, primarily employ passive strategies to enhance robustness. However, these approaches often rely on pre-defined assumptions about adversarial tactics, limiting their adaptability in dynamic 3D settings. To address these challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a proactive defense framework that leverages adaptive exploration and interaction with the environment to improve perception robustness in 3D adversarial contexts. By implementing a multi-step objective that balances immediate prediction accuracy with predictive entropy minimization, Rein-EAD optimizes defense strategies over a multi-step horizon. Additionally, Rein-EAD involves an uncertainty-oriented reward-shaping mechanism that facilitates efficient policy updates, thereby reducing computational overhead and supporting real-world applicability without the need for differentiable environments. Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating a substantial reduction in attack success rates while preserving standard accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization to unseen and adaptive attacks, making it suitable for real-world complex tasks, including 3D object classification, face recognition and autonomous driving.', 'abstract_zh': 'adversarial 攻击在 3D 环境中已成为视觉感知系统可靠性的关键威胁，特别是在身份验证和自动驾驶等安全性敏感应用中。这些攻击利用对抗性补丁和 3D 对象通过利用复杂场景中的漏洞来操控深度神经网络（DNN）预测。现有的防御机制，如对抗性训练和净化，主要采用被动策略来增强鲁棒性。然而，这些方法通常依赖于对手战术的预定义假设，限制了其在动态 3D 设置下的适应性。为应对这些挑战，我们提出了增强体外主动防御（Rein-EAD）框架，该框架利用适应性探索与环境的交互来提高 3D 对抗性环境中的感知鲁棒性。通过实施平衡即时预测准确性和预测熵最小化的多步骤目标，Rein-EAD 在多步骤时间范围内优化防御策略。此外，Rein-EAD 还包括一个以不确定性为导向的奖励塑造机制，有助于高效策略更新，从而减少计算开销并支持在非可微环境中实现的现实世界应用。全面的实验验证了 Rein-EAD 的有效性，展示了其在不同任务中显著降低攻击成功率的能力，同时保持标准准确性。值得注意的是，Rein-EAD 能够 robust 地泛化到未知和适应性攻击，使其适用于包括 3D 对象分类、人脸识别和自动驾驶在内的现实世界复杂任务。', 'title_zh': '强化嵌身体动防御：在 adversarial 3D 环境中利用自适应交互实现稳健的视觉感知'}
{'arxiv_id': 'arXiv:2507.18476', 'title': 'Automated Code Review Using Large Language Models with Symbolic Reasoning', 'authors': 'Busra Icoz, Goksel Biricik', 'link': 'https://arxiv.org/abs/2507.18476', 'abstract': 'Code review is one of the key processes in the software development lifecycle and is essential to maintain code quality. However, manual code review is subjective and time consuming. Given its rule-based nature, code review is well suited for automation. In recent years, significant efforts have been made to automate this process with the help of artificial intelligence. Recent developments in Large Language Models (LLMs) have also emerged as a promising tool in this area, but these models often lack the logical reasoning capabilities needed to fully understand and evaluate code. To overcome this limitation, this study proposes a hybrid approach that integrates symbolic reasoning techniques with LLMs to automate the code review process. We tested our approach using the CodexGlue dataset, comparing several models, including CodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining symbolic reasoning and prompting techniques with LLMs. Our results show that this approach improves the accuracy and efficiency of automated code review.', 'abstract_zh': '代码审查是软件开发生命周期中的关键过程，对于维护代码质量至关重要。然而，手工代码审查具有主观性和耗时性。鉴于其基于规则的特性，代码审查非常适合自动化。近年来，借助人工智能的帮助，显著努力已经投入于自动化这一过程。大型语言模型的最新发展也为这一领域带来了前景广阔的工具，但这些模型往往缺乏充分理解并评估代码所需的逻辑推理能力。为克服这一局限，本研究提出了一种将符号推理技术与大型语言模型相结合的混合方法，以自动化代码审查过程。我们使用CodexGlue数据集测试了该方法，比较了包括CodeT5、CodeBERT和GraphCodeBERT在内的多种模型，评估将符号推理和提示技术与大型语言模型结合的有效性。研究结果表明，该方法提高了自动化代码审查的准确性和效率。', 'title_zh': '使用符号推理的大语言模型驱动的自动化代码审查'}
{'arxiv_id': 'arXiv:2507.18457', 'title': 'Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols', 'authors': 'Luo Cheng, Hanwei Zhang, Lijun Zhang, Holger Hermanns', 'link': 'https://arxiv.org/abs/2507.18457', 'abstract': 'Adversarial robustness in LiDAR-based 3D object detection is a critical research area due to its widespread application in real-world scenarios. While many digital attacks manipulate point clouds or meshes, they often lack physical realizability, limiting their practical impact. Physical adversarial object attacks remain underexplored and suffer from poor reproducibility due to inconsistent setups and hardware differences. To address this, we propose a device-agnostic, standardized framework that abstracts key elements of physical adversarial object attacks, supports diverse methods, and provides open-source code with benchmarking protocols in simulation and real-world settings. Our framework enables fair comparison, accelerates research, and is validated by successfully transferring simulated attacks to a physical LiDAR system. Beyond the framework, we offer insights into factors influencing attack success and advance understanding of adversarial robustness in real-world LiDAR perception.', 'abstract_zh': '基于LiDAR的3D物体检测对抗鲁棒性是一个关键的研究领域，由于其在实际场景中的广泛应用。虽然许多数字攻击操控点云或网格，但它们往往缺乏物理可行性，限制了其实际影响。物理对抗物体攻击尚未得到充分探索，且由于设置不一致和硬件差异，重现性较差。为解决这一问题，我们提出了一种设备无关的标准框架，该框架抽象了物理对抗物体攻击的关键要素，支持多种方法，并提供了在仿真和实际场景中进行基准测试的开源代码。该框架使比较公平、加速了研究，并通过成功将模拟攻击转移到物理LiDAR系统中得到验证。此外，我们还探讨了影响攻击成功的关键因素，深化了对实际LiDAR感知中对抗鲁棒性的理解。', 'title_zh': '重新审视基于物理可实现的LiDAR目标攻击的对抗性物体攻击：明确问题陈述与实验协议'}
{'arxiv_id': 'arXiv:2507.18454', 'title': 'Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving', 'authors': 'Juntao Zhao, Jiuru Li, Chuan Wu', 'link': 'https://arxiv.org/abs/2507.18454', 'abstract': 'Utilizing CPUs to serve large language models (LLMs) is a resource-friendly alternative to GPU serving. Existing CPU-based solutions ignore workload differences between the prefill and the decode phases of LLM inference, applying a static per-NUMA (Non-Uniform Memory Access) node model partition and utilizing vendor libraries for operator-level execution, which is suboptimal. We propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses different execution plans for the prefill and decode phases and optimizes them separately.\nWe evaluate Sandwich across diverse baselines and datasets on five CPU platforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON. Sandwich achieves an average 2.01x throughput improvement and 90% satisfactory time-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up to 3.40x lower requirements in single sequence serving, and significant improvement in Goodput in continuous-batching serving. The GEMM kernels generated by Sandwich outperform representative vendor kernels and other dynamic shape solutions, achieving performance comparable to static compilers with three orders of magnitude less kernel tuning costs.', 'abstract_zh': '利用CPU服务于大型语言模型（LLMs）是一种资源友好的替代GPU服务的选择。现有的基于CPU的解决方案忽略了LLM推理中填充和解码阶段的工作负载差异，采用静态的非统一内存访问（NUMA）节点模型划分，并利用供应商库进行操作级执行，这不尽如人意。我们提出了Sandwich，一种以硬件为中心的基于CPU的LLM服务引擎，为填充和解码阶段使用不同的执行计划并分别进行优化。', 'title_zh': 'Sandwich：分离预填充解码编译以实现高效CPU大语言模型服务'}
{'arxiv_id': 'arXiv:2507.18451', 'title': 'Generation of Synthetic Clinical Text: A Systematic Review', 'authors': 'Basel Alshaikhdeeb, Ahmed Abdelmonem Hemedan, Soumyabrata Ghosh, Irina Balaur, Venkata Satagopam', 'link': 'https://arxiv.org/abs/2507.18451', 'abstract': 'Generating clinical synthetic text represents an effective solution for common clinical NLP issues like sparsity and privacy. This paper aims to conduct a systematic review on generating synthetic medical free-text by formulating quantitative analysis to three research questions concerning (i) the purpose of generation, (ii) the techniques, and (iii) the evaluation methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE, Google Scholar, and arXiv databases for publications associated with generating synthetic medical unstructured free-text. We have identified 94 relevant articles out of 1,398 collected ones. A great deal of attention has been given to the generation of synthetic medical text from 2018 onwards, where the main purpose of such a generation is towards text augmentation, assistive writing, corpus building, privacy-preserving, annotation, and usefulness. Transformer architectures were the main predominant technique used to generate the text, especially the GPTs. On the other hand, there were four main aspects of evaluation, including similarity, privacy, structure, and utility, where utility was the most frequent method used to assess the generated synthetic medical text. Although the generated synthetic medical text demonstrated a moderate possibility to act as real medical documents in different downstream NLP tasks, it has proven to be a great asset as augmented, complementary to the real documents, towards improving the accuracy and overcoming sparsity/undersampling issues. Yet, privacy is still a major issue behind generating synthetic medical text, where more human assessments are needed to check for the existence of any sensitive information. Despite that, advances in generating synthetic medical text will considerably accelerate the adoption of workflows and pipeline development, discarding the time-consuming legalities of data transfer.', 'abstract_zh': '生成临床合成文本代表了解决临床自然语言处理问题（如稀疏性和隐私问题）的一种有效方法。本文旨在通过定量分析三个研究问题——（i）生成目的，（ii）技术，和（iii）评估方法——对生成合成医学自由文本进行系统综述。我们搜索了PubMed、ScienceDirect、Web of Science、Scopus、IEEE、Google Scholar和arXiv数据库，寻找与生成合成医学非结构化自由文本相关的出版物。我们一共找到了1398篇文献中的94篇相关文章。自2018年以来，合成医学文本的生成得到了广泛关注，其主要目的是文本增强、辅助写作、构建语料库、隐私保护、标注和实用性。Transformer架构是主要用于生成文本的主要技术，特别是GPTs。另一方面，在评估方面主要有四个主要方面，包括相似性、隐私、结构和实用性，其中实用性是最常用于评估生成的合成医学文本的方法。虽然生成的合成医学文本在不同的下游NLP任务中显示出作为真实医学文档的中等可能性，但它作为增强的真实文档的补充资产，已被证明能够提高准确性并克服稀疏性/欠采样问题。然而，隐私仍然是生成合成医学文本的主要问题，需要更多的手工评估来检查是否存在敏感信息。尽管如此，合成医学文本生成的进步将显著加速工作流程和管道开发的采用，摆脱繁琐的数据传输法律程序。', 'title_zh': '合成临床文本的生成：一项系统评价'}
{'arxiv_id': 'arXiv:2507.18449', 'title': 'Digital Twin Technologies in Predictive Maintenance: Enabling Transferability via Sim-to-Real and Real-to-Sim Transfer', 'authors': 'Sizhe Ma, Katherine A. Flanigan, Mario Bergés', 'link': 'https://arxiv.org/abs/2507.18449', 'abstract': 'The advancement of the Internet of Things (IoT) and Artificial Intelligence has catalyzed the evolution of Digital Twins (DTs) from conceptual ideas to more implementable realities. Yet, transitioning from academia to industry is complex due to the absence of standardized frameworks. This paper builds upon the authors\' previously established functional and informational requirements supporting standardized DT development, focusing on a crucial aspect: transferability. While existing DT research primarily centers on asset transfer, the significance of "sim-to-real transfer" and "real-to-sim transfer"--transferring knowledge between simulations and real-world operations--is vital for comprehensive lifecycle management in DTs. A key challenge in this process is calibrating the "reality gap," the discrepancy between simulated predictions and actual outcomes. Our research investigates the impact of integrating a single Reality Gap Analysis (RGA) module into an existing DT framework to effectively manage both sim-to-real and real-to-sim transfers. This integration is facilitated by data pipelines that connect the RGA module with the existing components of the DT framework, including the historical repository and the simulation model. A case study on a pedestrian bridge at Carnegie Mellon University showcases the performance of different levels of integration of our approach with an existing framework. With full implementation of an RGA module and a complete data pipeline, our approach is capable of bidirectional knowledge transfer between simulations and real-world operations without compromising efficiency.', 'abstract_zh': '物联网和人工智能的进步推动了数字孪生从概念性想法向更可实施的现实转化。然而，从学术界向工业界的转型由于缺乏标准化框架而复杂化。本文基于作者之前建立的功能性和信息性要求，支持标准化数字孪生的开发，重点关注一个关键方面：可转移性。现有数字孪生研究主要集中在资产转移上，而“仿真到现实转移”和“现实到仿真转移”的重要性——即在仿真与实际操作之间转移知识——对于数字孪生的全面生命周期管理至关重要。这一过程中的一项主要挑战是校准“现实差距”，即仿真预测与实际结果之间的差异。我们的研究探讨了将在现有数字孪生框架中集成一个单一的现实差距分析（RGA）模块以有效管理仿真到现实和现实到仿真的转移的影响。这种集成通过将RGA模块与现有数字孪生框架中的历史仓库和仿真模型等组件连接起来，由数据管道来实现。以卡内基梅隆大学的行人桥为例，研究展示了不同集成水平的方法与现有框架结合后的性能。当完全实施RGA模块和完整的数据管道时，我们的方法能够在不牺牲效率的情况下在仿真与现实操作之间实现双向知识转移。', 'title_zh': '数字孪生技术在预测性维护中的应用：通过模拟到现实和现实到模拟的转移实现可转移性'}
{'arxiv_id': 'arXiv:2507.18448', 'title': 'Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language', 'authors': 'Md Obyedullahil Mamun, Md Adyelullahil Mamun, Arif Ahmad, Md. Imran Hossain Emu', 'link': 'https://arxiv.org/abs/2507.18448', 'abstract': "Punctuation restoration enhances the readability of text and is critical for post-processing tasks in Automatic Speech Recognition (ASR), especially for low-resource languages like Bangla. In this study, we explore the application of transformer-based models, specifically XLM-RoBERTa-large, to automatically restore punctuation in unpunctuated Bangla text. We focus on predicting four punctuation marks: period, comma, question mark, and exclamation mark across diverse text domains. To address the scarcity of annotated resources, we constructed a large, varied training corpus and applied data augmentation techniques. Our best-performing model, trained with an augmentation factor of alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the Reference set, and 90.2% on the ASR set.\nResults show strong generalization to reference and ASR transcripts, demonstrating the model's effectiveness in real-world, noisy scenarios. This work establishes a strong baseline for Bangla punctuation restoration and contributes publicly available datasets and code to support future research in low-resource NLP.", 'abstract_zh': '基于变压器模型的标点符号恢复提升孟加拉语文本可读性，并在自动语音识别后处理任务中至关重要，尤其是在低资源语言如孟加拉语领域。在本研究中，我们探讨了使用XLM-RoBERTa-large等基于变压器的模型自动恢复未标点孟加拉语文本中标点符号的应用。我们重点关注在多种文本领域预测四种标点符号：句号、逗号、问号和感叹号。为了解决标注资源稀缺的问题，我们构建了一个大型且多样化的训练语料库，并应用了数据增强技术。在增强因子α=0.20%的情况下，我们的最佳模型在News测试集上实现了97.1%的准确率，在Reference集上实现了91.2%的准确率，在ASR集上实现了90.2%的准确率。研究结果表明，该模型在参考和ASR转录中具有良好的泛化能力，证明了其在现实世界嘈杂场景中的有效性。本研究为孟加拉语标点符号恢复建立了强有力的基准，并提供了公开可用的数据集和代码，以支持未来低资源自然语言处理领域的研究。', 'title_zh': '恢复节奏：使用变压器模型的孟加拉语标点恢复'}
{'arxiv_id': 'arXiv:2507.18442', 'title': "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data", 'authors': 'Rana Alshaikh, Israa Alghanmi, Shelan Jeawak', 'link': 'https://arxiv.org/abs/2507.18442', 'abstract': 'The cognitive and reasoning abilities of large language models (LLMs) have enabled remarkable progress in natural language processing. However, their performance in interpreting structured data, especially in tabular formats, remains limited. Although benchmarks for English tabular data are widely available, Arabic is still underrepresented because of the limited availability of public resources and its unique language features. To address this gap, we present AraTable, a novel and comprehensive benchmark designed to evaluate the reasoning and understanding capabilities of LLMs when applied to Arabic tabular data. AraTable consists of various evaluation tasks, such as direct question answering, fact verification, and complex reasoning, involving a wide range of Arabic tabular sources. Our methodology follows a hybrid pipeline, where initial content is generated by LLMs and subsequently filtered and verified by human experts to ensure high dataset quality. Initial analyses using AraTable show that, while LLMs perform adequately on simpler tabular tasks such as direct question answering, they continue to face significant cognitive challenges when tasks require deeper reasoning and fact verification. This indicates that there are substantial opportunities for future work to improve performance on complex tabular reasoning tasks. We also propose a fully automated evaluation framework that uses a self-deliberation mechanism and achieves performance nearly identical to that of human judges. This research provides a valuable, publicly available resource and evaluation framework that can help accelerate the development of foundational models for processing and analysing Arabic structured data.', 'abstract_zh': '大型语言模型（LLMs）的认知与推理能力促进了自然语言处理的显著进步。然而，在解释结构化数据，尤其是表格格式数据方面的表现仍然有限。尽管英语文本表格基准数据广泛可用，阿拉伯语仍因公共资源有限和语言特点独特而相对欠缺。为弥补这一差距，我们提出了AraTable，一个新颖而全面的基准测试，旨在评估LLMs在阿拉伯语表格数据上的推理与理解能力。AraTable包含了多种评估任务，如直接问答、事实验证和复杂推理，涵盖广泛阿拉伯语表格来源。我们的方法采用混合管道，初始内容由LLMs生成，再由人类专家过滤和验证以确保数据集质量。初步分析表明，虽然LLMs在简单的表格任务，如直接问答上表现良好，但在需要更深推理和事实验证的任务中仍面临显著的认知挑战。这表明，未来工作在复杂表格推理任务上有很大的改进空间。我们还提出了一种完全自动化的评估框架，通过自动推理机制实现了与人类评委几乎相同的性能。本研究提供了有价值的公共可用资源和评估框架，有助于加快处理和分析阿拉伯语结构化数据的基础模型开发。', 'title_zh': 'AraTable：评估LLMs在阿拉伯表格数据推理和理解方面的表现'}
{'arxiv_id': 'arXiv:2507.18392', 'title': 'CLEAR: Error Analysis via LLM-as-a-Judge Made Easy', 'authors': 'Asaf Yehudai, Lilach Eden, Yotam Perlitz, Roy Bar-Haim, Michal Shmueli-Scheuer', 'link': 'https://arxiv.org/abs/2507.18392', 'abstract': "The evaluation of Large Language Models (LLMs) increasingly relies on other LLMs acting as judges. However, current evaluation paradigms typically yield a single score or ranking, answering which model is better but not why. While essential for benchmarking, these top-level scores obscure the specific, actionable reasons behind a model's performance. To bridge this gap, we introduce CLEAR, an interactive, open-source package for LLM-based error analysis. CLEAR first generates per-instance textual feedback, then it creates a set of system-level error issues, and quantifies the prevalence of each identified issue. Our package also provides users with an interactive dashboard that allows for a comprehensive error analysis through aggregate visualizations, applies interactive filters to isolate specific issues or score ranges, and drills down to the individual instances that exemplify a particular behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks, and showcase its utility through a user case study.", 'abstract_zh': '大型语言模型（LLMs）的评估越来越多地依赖于其他LLMs作为评判者。然而，当前的评估范式通常仅给出单一得分或排名，回答了哪个模型更好但没有解释原因。虽然对于基准测试至关重要，这些高层面的得分掩盖了模型表现背后的具体可操作原因。为了解决这一差距，我们引入了CLEAR，一个基于LLM的错误分析交互式开源包。CLEAR首先生成针对每个实例的文本反馈，然后创建一系列系统级错误问题集，并量化每个识别问题的发生频率。我们的包还为用户提供了一个交互式仪表板，允许通过汇总可视化进行全面的错误分析，应用交互式滤镜以隔离特定问题或分数范围，并钻取到体现特定行为模式的个体实例。我们展示了CLEAR在RAG和Math基准测试中的分析，并通过用户案例研究突显其实用性。', 'title_zh': 'CLEAR: 通过LLM作为法官简化错误分析'}
{'arxiv_id': 'arXiv:2507.18334', 'title': 'Improving Bird Classification with Primary Color Additives', 'authors': 'Ezhini Rasendiran R, Chandresh Kumar Maurya', 'link': 'https://arxiv.org/abs/2507.18334', 'abstract': 'We address the problem of classifying bird species using their song recordings, a challenging task due to environmental noise, overlapping vocalizations, and missing labels. Existing models struggle with low-SNR or multi-species recordings. We hypothesize that birds can be classified by visualizing their pitch pattern, speed, and repetition, collectively called motifs. Deep learning models applied to spectrogram images help, but similar motifs across species cause confusion. To mitigate this, we embed frequency information into spectrograms using primary color additives. This enhances species distinction and improves classification accuracy. Our experiments show that the proposed approach achieves statistically significant gains over models without colorization and surpasses the BirdCLEF 2024 winner, improving F1 by 7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the effectiveness of incorporating frequency information via colorization.', 'abstract_zh': '使用彩色谱图图示化频率信息进行鸟类物种分类', 'title_zh': '使用主要颜色添加剂提高鸟类分类效果'}
{'arxiv_id': 'arXiv:2507.18326', 'title': 'A Concept for Efficient Scalability of Automated Driving Allowing for Technical, Legal, Cultural, and Ethical Differences', 'authors': 'Lars Ullrich, Michael Buchholz, Jonathan Petit, Klaus Dietmayer, Knut Graichen', 'link': 'https://arxiv.org/abs/2507.18326', 'abstract': 'Efficient scalability of automated driving (AD) is key to reducing costs, enhancing safety, conserving resources, and maximizing impact. However, research focuses on specific vehicles and context, while broad deployment requires scalability across various configurations and environments. Differences in vehicle types, sensors, actuators, but also traffic regulations, legal requirements, cultural dynamics, or even ethical paradigms demand high flexibility of data-driven developed capabilities. In this paper, we address the challenge of scalable adaptation of generic capabilities to desired systems and environments. Our concept follows a two-stage fine-tuning process. In the first stage, fine-tuning to the specific environment takes place through a country-specific reward model that serves as an interface between technological adaptations and socio-political requirements. In the second stage, vehicle-specific transfer learning facilitates system adaptation and governs the validation of design decisions. In sum, our concept offers a data-driven process that integrates both technological and socio-political aspects, enabling effective scalability across technical, legal, cultural, and ethical differences.', 'abstract_zh': '自动驾驶（AD）的高效扩展是降低成本、提升安全、节约资源和最大化影响的关键。然而，研究主要集中在特定车辆和情境上，而广泛部署则需要在各种配置和环境中实现扩展。车辆类型、传感器、执行器的差异，以及交通法规、法律要求、文化动态，甚至伦理观念都需要高度的数据驱动开发能力的灵活性。在本文中，我们解决了一般能力可扩展适配到目标系统和环境的挑战。我们的概念遵循两阶段微调过程。在第一阶段，通过国家特定的奖励模型进行环境适配微调，作为技术适应与社会政治要求之间的接口。在第二阶段，车辆特定的迁移学习促进系统适配并指导设计决策的验证。总之，我们的概念提供了一个数据驱动的过程，整合了技术和社会政治两个方面，能够在技术、法律、文化和伦理差异中实现有效的扩展。', 'title_zh': '一种适用于技术、法律、文化及伦理差异的自动驾驶高效扩展概念'}
{'arxiv_id': 'arXiv:2507.18323', 'title': 'A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation', 'authors': 'Minje Park, Jeonghwa Lim, Taehyung Yu, Sunghoon Joo', 'link': 'https://arxiv.org/abs/2507.18323', 'abstract': 'Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform features, is critical for clinical diagnosis. Despite recent advances using deep learning, progress has been limited by the scarcity of publicly available annotated datasets. Semi-supervised learning presents a promising solution by leveraging abundant unlabeled ECG data. In this study, we present the first systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG delineation. We curated and unified multiple public datasets, including previously underused sources, to support robust and diverse evaluation. We adopted five representative SemiSeg algorithms from computer vision, implemented them on two different architectures: the convolutional network and the transformer, and evaluated them in two different settings: in-domain and cross-domain. Additionally, we propose ECG-specific training configurations and augmentation strategies and introduce a standardized evaluation framework. Our results show that the transformer outperforms the convolutional network in semi-supervised ECG delineation. We anticipate that our benchmark will serve as a foundation for advancing semi-supervised ECG delineation methods and will facilitate further research in this domain.', 'abstract_zh': 'ECG波形特征半监督语义分割基准研究', 'title_zh': '一个多数据集基准，用于ECG边界标注的半监督语义分割'}
{'arxiv_id': 'arXiv:2507.18302', 'title': 'LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models', 'authors': 'Delong Ran, Xinlei He, Tianshuo Cong, Anyu Wang, Qi Li, Xiaoyun Wang', 'link': 'https://arxiv.org/abs/2507.18302', 'abstract': 'Language Models (LMs) typically adhere to a "pre-training and fine-tuning" paradigm, where a universal pre-trained model can be fine-tuned to cater to various specialized domains. Low-Rank Adaptation (LoRA) has gained the most widespread use in LM fine-tuning due to its lightweight computational cost and remarkable performance. Because the proportion of parameters tuned by LoRA is relatively small, there might be a misleading impression that the LoRA fine-tuning data is invulnerable to Membership Inference Attacks (MIAs). However, we identify that utilizing the pre-trained model can induce more information leakage, which is neglected by existing MIAs. Therefore, we introduce LoRA-Leak, a holistic evaluation framework for MIAs against the fine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership inference attacks, including ten existing MIAs, and five improved MIAs that leverage the pre-trained model as a reference. In experiments, we apply LoRA-Leak to three advanced LMs across three popular natural language processing tasks, demonstrating that LoRA-based fine-tuned LMs are still vulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings). We also applied LoRA-Leak to different fine-tuning settings to understand the resulting privacy risks. We further explore four defenses and find that only dropout and excluding specific LM layers during fine-tuning effectively mitigate MIA risks while maintaining utility. We highlight that under the "pre-training and fine-tuning" paradigm, the existence of the pre-trained model makes MIA a more severe risk for LoRA-based LMs. We hope that our findings can provide guidance on data privacy protection for specialized LM providers.', 'abstract_zh': 'LoRA-Leak：面向语言模型细调数据的全方位成员推理攻击评估框架', 'title_zh': 'LoRA-Leak: 面向LoRA微调语言模型的成员推理攻击'}
{'arxiv_id': 'arXiv:2507.18288', 'title': 'TCM-Tongue: A Standardized Tongue Image Dataset with Pathological Annotations for AI-Assisted TCM Diagnosis', 'authors': 'Xuebo Jin, Longfei Gao, Anshuo Tong, Zhengyang Chen, Jianlei Kong, Ning Sun, Huijun Ma, Qiang Wang, Yuting Bai, Tingli Su', 'link': 'https://arxiv.org/abs/2507.18288', 'abstract': 'Traditional Chinese medicine (TCM) tongue diagnosis, while clinically valuable, faces standardization challenges due to subjective interpretation and inconsistent imaging protocols, compounded by the lack of large-scale, annotated datasets for AI development. To address this gap, we present the first specialized dataset for AI-driven TCM tongue diagnosis, comprising 6,719 high-quality images captured under standardized conditions and annotated with 20 pathological symptom categories (averaging 2.54 clinically validated labels per image, all verified by licensed TCM practitioners). The dataset supports multiple annotation formats (COCO, TXT, XML) for broad usability and has been benchmarked using nine deep learning models (YOLOv5/v7/v8 variants, SSD, and MobileNetV2) to demonstrate its utility for AI development. This resource provides a critical foundation for advancing reliable computational tools in TCM, bridging the data shortage that has hindered progress in the field, and facilitating the integration of AI into both research and clinical practice through standardized, high-quality diagnostic data.', 'abstract_zh': '传统中医舌诊图像数据集：一个专为AI驱动舌诊设计的标准化数据资源', 'title_zh': 'TCM-舌象：一种包含病理标注的标准舌象图像数据集，用于辅助中医诊断的AI系统'}
{'arxiv_id': 'arXiv:2507.18263', 'title': 'Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models', 'authors': 'Suhang Wu, Jialong Tang, Chengyi Yang, Pei Zhang, Baosong Yang, Junhui Li, Junfeng Yao, Min Zhang, Jinsong Su', 'link': 'https://arxiv.org/abs/2507.18263', 'abstract': 'Direct speech translation (ST) has garnered increasing attention nowadays, yet the accurate translation of terminology within utterances remains a great challenge. In this regard, current studies mainly concentrate on leveraging various translation knowledge into ST models. However, these methods often struggle with interference from irrelevant noise and can not fully utilize the translation knowledge. To address these issues, in this paper, we propose a novel Locate-and-Focus method for terminology translation. It first effectively locates the speech clips containing terminologies within the utterance to construct translation knowledge, minimizing irrelevant information for the ST model. Subsequently, it associates the translation knowledge with the utterance and hypothesis from both audio and textual modalities, allowing the ST model to better focus on translation knowledge during translation. Experimental results across various datasets demonstrate that our method effectively locates terminologies within utterances and enhances the success rate of terminology translation, while maintaining robust general translation performance.', 'abstract_zh': '直接speech翻译（ST）近年来引起了越来越多的关注，但语句中术语的准确翻译仍然是一项重大挑战。针对这一问题，现有研究主要集中在将各种翻译知识应用于ST模型中。然而，这些方法往往难以应对无关噪声的干扰，无法充分利用翻译知识。为了解决这些问题，本文提出了一种新颖的定位与聚焦方法（Locate-and-Focus method）用于术语翻译。该方法首先有效定位包含术语的语音片段以构建翻译知识，最小化ST模型中的无关信息。随后，该方法将翻译知识与来自音频和文本模态的语句和假设关联起来，使ST模型在翻译过程中能够更好地关注翻译知识。在多个数据集上的实验结果表明，本方法有效定位了语句中的术语，提高了术语翻译的成功率，同时保持了稳健的一般翻译性能。', 'title_zh': '定位并聚焦：提高语音语言模型中的术语翻译'}
{'arxiv_id': 'arXiv:2507.18262', 'title': 'ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation', 'authors': 'Chenyu Su, Weiwei Shang, Chen Qian, Fei Zhang, Shuang Cong', 'link': 'https://arxiv.org/abs/2507.18262', 'abstract': 'Semantics-driven 3D spatial constraints align highlevel semantic representations with low-level action spaces, facilitating the unification of task understanding and execution in robotic manipulation. The synergistic reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation Models (VFMs) enables cross-modal 3D spatial constraint construction. Nevertheless, existing methods have three key limitations: (1) coarse semantic granularity in constraint modeling, (2) lack of real-time closed-loop planning, (3) compromised robustness in semantically diverse environments. To address these challenges, we propose ReSem3D, a unified manipulation framework for semantically diverse environments, leveraging the synergy between VFMs and MLLMs to achieve fine-grained visual grounding and dynamically constructs hierarchical 3D spatial constraints for real-time manipulation. Specifically, the framework is driven by hierarchical recursive reasoning in MLLMs, which interact with VFMs to automatically construct 3D spatial constraints from natural language instructions and RGB-D observations in two stages: part-level extraction and region-level refinement. Subsequently, these constraints are encoded as real-time optimization objectives in joint space, enabling reactive behavior to dynamic disturbances. Extensive simulation and real-world experiments are conducted in semantically rich household and sparse chemical lab environments. The results demonstrate that ReSem3D performs diverse manipulation tasks under zero-shot conditions, exhibiting strong adaptability and generalization. Code and videos at this https URL.', 'abstract_zh': '基于语义的三维空间约束驱动机器人操作中的高层次语义表示与低层次动作空间对齐，促进任务理解和执行的统一。多模态大型语言模型和视觉基础模型的协同推理 enables 跨模态三维空间约束构建。然而，现有方法存在三个关键局限：（1）约束建模中粗粒度的语义粒度，（2）缺乏实时闭环规划，（3）在语义多样的环境中鲁棒性较低。为解决这些挑战，我们提出 ReSem3D，一种利用视觉基础模型和多模态大型语言模型之间的协同作用，实现细粒度的视觉定位并动态构建层次化三维空间约束的统一操作框架，以支持实时操作。具体而言，该框架由多模态大型语言模型中的分层递归推理驱动，与视觉基础模型交互，自动在两个阶段从自然语言指令和RGB-D观测中构建三维空间约束：部分级提取和区域级细化。随后，这些约束被编码为关节空间中的实时优化目标，使机器人能够对动态干扰进行反应。我们在语义丰富的家庭环境和稀疏的化学实验室环境中进行了广泛的仿真和真实世界实验。实验结果表明，ReSem3D 在零样本条件下执行多样化的操作任务，显示出强大的适应性和泛化能力。有关代码和视频请访问 this https URL。', 'title_zh': 'ReSem3D：通过细粒度语义 grounding 完善的 3D 空间约束以实现可泛化的机器人 manipulation'}
{'arxiv_id': 'arXiv:2507.18260', 'title': 'Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection', 'authors': 'Junyao Li, Yahao Lu, Xingyuan Guo, Xiaoyu Xian, Tiantian Wang, Yukai Shi', 'link': 'https://arxiv.org/abs/2507.18260', 'abstract': 'Infrared small target detection (ISTD) plays a vital role in numerous practical applications. In pursuit of determining the performance boundaries, researchers employ large and expensive manual-labeling data for representation learning. Nevertheless, this approach renders the state-of-the-art ISTD methods highly fragile in real-world challenges. In this paper, we first study the variation in detection performance across several mainstream methods under various scarcity -- namely, the absence of high-quality infrared data -- that challenge the prevailing theories about practical ISTD. To address this concern, we introduce the Gaussian Agnostic Representation Learning. Specifically, we propose the Gaussian Group Squeezer, leveraging Gaussian sampling and compression for non-uniform quantization. By exploiting a diverse array of training samples, we enhance the resilience of ISTD models against various challenges. Then, we introduce two-stage diffusion models for real-world reconstruction. By aligning quantized signals closely with real-world distributions, we significantly elevate the quality and fidelity of the synthetic samples. Comparative evaluations against state-of-the-art detection methods in various scarcity scenarios demonstrate the efficacy of the proposed approach.', 'abstract_zh': '红外小目标检测（ISTD）在众多实际应用中发挥着重要作用。为了确定性能边界，研究者们使用大量昂贵的手动标注数据进行表示学习。然而，这种方法使得最先进的ISTD方法在实际挑战中非常脆弱。在本文中，我们首先研究了在各种数据稀缺条件下（即缺乏高质量红外数据）主流方法的检测性能变异，从而挑战了现有关于实际ISTD的理论。为解决这一问题，我们引入了高斯无偏表示学习。具体来说，我们提出了高斯组挤压器，利用高斯采样和压缩进行非均匀量化。通过利用多样化的训练样本，我们增强了ISTD模型对各种挑战的鲁棒性。然后，我们介绍了两级扩散模型进行实际重构。通过使量化信号紧密符合实际分布，我们显著提高了合成样本的质量和保真度。在各种数据稀缺条件下的对比评估证明了所提出方法的有效性。', 'title_zh': '基于扩散先验的高斯agnostic表示学习增强红外小目标检测'}
{'arxiv_id': 'arXiv:2507.18252', 'title': 'Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning', 'authors': 'Dongyang Guo, Yasmeen Abdrabou, Enkeleda Thaqi, Enkelejda Kasneci', 'link': 'https://arxiv.org/abs/2507.18252', 'abstract': "Eye-tracking data reveals valuable insights into users' cognitive states but is difficult to analyze due to its structured, non-linguistic nature. While large language models (LLMs) excel at reasoning over text, they struggle with temporal and numerical data. This paper presents a multimodal human-AI collaborative framework designed to enhance cognitive pattern extraction from eye-tracking signals. The framework includes: (1) a multi-stage pipeline using horizontal and vertical segmentation alongside LLM reasoning to uncover latent gaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert judgment with LLM output to generate trust scores for behavioral interpretations; and (3) a hybrid anomaly detection module combining LSTM-based temporal modeling with LLM-driven semantic analysis. Our results across several LLMs and prompt strategies show improvements in consistency, interpretability, and performance, with up to 50% accuracy in difficulty prediction tasks. This approach offers a scalable, interpretable solution for cognitive modeling and has broad potential in adaptive learning, human-computer interaction, and educational analytics.", 'abstract_zh': '眼动数据揭示了用户认知状态的重要见解，但由于其结构化且非语言的性质，分析起来非常困难。虽然大型语言模型在处理文本方面表现出色，但在处理时间和数值数据方面却存在困难。本文提出了一种多模态人机协作框架，旨在增强从眼动信号中提取认知模式的能力。该框架包括：（1）多阶段管道，结合水平和垂直分割以及大型语言模型推理，以揭示潜在的眼动模式；（2）专家模型协作评分模块，结合专家判断与大型语言模型输出生成行为解释的信任分；以及（3）结合基于LSTM的时间建模与大型语言模型驱动的语义分析的混合异常检测模块。我们的实验结果表明，该方法在不同大型语言模型和提示策略下提高了一致性和可解释性，并在难度预测任务中最高可达50%的准确性。该方法提供了一种可扩展且可解释的认知建模解决方案，并在自适应学习、人机交互和教育分析等领域具有广泛潜力。', 'title_zh': '基于眼动追踪和大语言模型驱动的推理的多模态行为模式分析'}
{'arxiv_id': 'arXiv:2507.18243', 'title': 'DepthDark: Robust Monocular Depth Estimation for Low-Light Environments', 'authors': 'Longjian Zeng, Zunjie Zhu, Rongfeng Lu, Ming Lu, Bolun Zheng, Chenggang Yan, Anke Xue', 'link': 'https://arxiv.org/abs/2507.18243', 'abstract': "In recent years, foundation models for monocular depth estimation have received increasing attention. Current methods mainly address typical daylight conditions, but their effectiveness notably decreases in low-light environments. There is a lack of robust foundational models for monocular depth estimation specifically designed for low-light scenarios. This largely stems from the absence of large-scale, high-quality paired depth datasets for low-light conditions and the effective parameter-efficient fine-tuning (PEFT) strategy. To address these challenges, we propose DepthDark, a robust foundation model for low-light monocular depth estimation. We first introduce a flare-simulation module and a noise-simulation module to accurately simulate the imaging process under nighttime conditions, producing high-quality paired depth datasets for low-light conditions. Additionally, we present an effective low-light PEFT strategy that utilizes illumination guidance and multiscale feature fusion to enhance the model's capability in low-light environments. Our method achieves state-of-the-art depth estimation performance on the challenging nuScenes-Night and RobotCar-Night datasets, validating its effectiveness using limited training data and computing resources.", 'abstract_zh': '低光环境单目深度估计的鲁棒基础模型：DepthDark', 'title_zh': '深度黑暗：低光照环境下的鲁棒单目深度估计'}
{'arxiv_id': 'arXiv:2507.18229', 'title': 'From Individual Learning to Market Equilibrium: Correcting Structural and Parametric Biases in RL Simulations of Economic Models', 'authors': 'Zeqiang Zhang, Ruxin Chen', 'link': 'https://arxiv.org/abs/2507.18229', 'abstract': "The application of Reinforcement Learning (RL) to economic modeling reveals a fundamental conflict between the assumptions of equilibrium theory and the emergent behavior of learning agents. While canonical economic models assume atomistic agents act as `takers' of aggregate market conditions, a naive single-agent RL simulation incentivizes the agent to become a `manipulator' of its environment. This paper first demonstrates this discrepancy within a search-and-matching model with concave production, showing that a standard RL agent learns a non-equilibrium, monopsonistic policy. Additionally, we identify a parametric bias arising from the mismatch between economic discounting and RL's treatment of intertemporal costs. To address both issues, we propose a calibrated Mean-Field Reinforcement Learning framework that embeds a representative agent in a fixed macroeconomic field and adjusts the cost function to reflect economic opportunity costs. Our iterative algorithm converges to a self-consistent fixed point where the agent's policy aligns with the competitive equilibrium. This approach provides a tractable and theoretically sound methodology for modeling learning agents in economic systems within the broader domain of computational social science.", 'abstract_zh': '强化学习在经济建模中的应用揭示了均衡理论假设与学习代理涌现行为之间的基本冲突。这篇论文首先在一个具有凹生产函数的搜寻匹配模型中展示了这种分歧，表明标准的RL代理学习了一个非均衡的垄断工政策。此外，我们还指出了由于经济学折现与RL对跨期成本处理之间的不匹配而产生的参数偏见。为了解决这些问题，我们提出了一种校准的平均场强化学习框架，该框架将一个代表性代理嵌入固定的大宏观经济场中，并调整成本函数以反映经济机会成本。我们的迭代算法在自我一致的固定点收敛，其中代理的政策与竞争性均衡一致。这种方法为在计算社会科学的大背景下对经济系统中学习代理的建模提供了可操作且合乎理论的方法。', 'title_zh': '从个体学习到市场均衡：纠正经济学模型中基于RL的模拟中的结构性和参数偏置'}
{'arxiv_id': 'arXiv:2507.18223', 'title': 'GenAI for Automotive Software Development: From Requirements to Wheels', 'authors': 'Nenad Petrovic, Fengjunjie Pan, Vahid Zolfaghari, Krzysztof Lebioda, Andre Schamschurko, Alois Knoll', 'link': 'https://arxiv.org/abs/2507.18223', 'abstract': 'This paper introduces a GenAI-empowered approach to automated development of automotive software, with emphasis on autonomous and Advanced Driver Assistance Systems (ADAS) capabilities. The process starts with requirements as input, while the main generated outputs are test scenario code for simulation environment, together with implementation of desired ADAS capabilities targeting hardware platform of the vehicle connected to testbench. Moreover, we introduce additional steps for requirements consistency checking leveraging Model-Driven Engineering (MDE). In the proposed workflow, Large Language Models (LLMs) are used for model-based summarization of requirements (Ecore metamodel, XMI model instance and OCL constraint creation), test scenario generation, simulation code (Python) and target platform code generation (C++). Additionally, Retrieval Augmented Generation (RAG) is adopted to enhance test scenario generation from autonomous driving regulations-related documents. Our approach aims shorter compliance and re-engineering cycles, as well as reduced development and testing time when it comes to ADAS-related capabilities.', 'abstract_zh': '基于GenAI的汽车软件自动化开发方法：强调自主和高级驾驶辅助系统（ADAS）能力', 'title_zh': 'GenAI在汽车软件开发中的应用：从需求到成品车'}
{'arxiv_id': 'arXiv:2507.18219', 'title': 'FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting', 'authors': 'Zhongzheng Yuan, Lianshuai Guo, Xunkai Li, Yinlin Zhu, Wenyu Wang, Meixia Qu', 'link': 'https://arxiv.org/abs/2507.18219', 'abstract': 'Federated Graph Learning (FGL) is a distributed learning paradigm that enables collaborative training over large-scale subgraphs located on multiple local systems. However, most existing FGL approaches rely on synchronous communication, which leads to inefficiencies and is often impractical in real-world deployments. Meanwhile, current asynchronous federated learning (AFL) methods are primarily designed for conventional tasks such as image classification and natural language processing, without accounting for the unique topological properties of graph data. Directly applying these methods to graph learning can possibly result in semantic drift and representational inconsistency in the global model. To address these challenges, we propose FedSA-GCL, a semi-asynchronous federated framework that leverages both inter-client label distribution divergence and graph topological characteristics through a novel ClusterCast mechanism for efficient training. We evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain and Metis split algorithms, and compare it against 9 baselines. Extensive experiments demonstrate that our method achieves strong robustness and outstanding efficiency, outperforming the baselines by an average of 2.92% with the Louvain and by 3.4% with the Metis.', 'abstract_zh': '联邦图学习（FGL）是一种分布式学习范式，能够在多个本地系统上协作训练大规模子图。然而，现有的大多数FGL方法依赖于同步通信，这会导致效率低下，并且在实际部署中往往不切实际。同时，当前的异步联邦学习（AFL）方法主要针对传统的任务如图像分类和自然语言处理进行设计，而未考虑图数据的独特拓扑特性。直接将这些方法应用于图学习可能会导致全局模型在语义和表示上的一致性问题。为了解决这些问题，我们提出了一种名为FedSA-GCL的半异步联邦框架，通过一种新颖的ClusterCast机制利用客户端标签分布差异和图拓扑特性实现高效的训练。我们在使用Louvain和Metis划分算法的多个真实世界图数据集上评估了FedSA-GCL，并将其与9种基线方法进行比较。实验结果表明，我们的方法在鲁棒性和效率上表现优秀，分别在Louvain和Metis划分算法下平均优于基线方法2.92%和3.4%。', 'title_zh': 'FedSA-GCL：一种带有个性化聚合和聚类意识广播的半异步联邦图学习框架'}
{'arxiv_id': 'arXiv:2507.18215', 'title': 'Information Security Based on LLM Approaches: A Review', 'authors': 'Chang Gong, Zhongwen Li, Xiaoqi Li', 'link': 'https://arxiv.org/abs/2507.18215', 'abstract': 'Information security is facing increasingly severe challenges, and traditional protection means are difficult to cope with complex and changing threats. In recent years, as an emerging intelligent technology, large language models (LLMs) have shown a broad application prospect in the field of information security. In this paper, we focus on the key role of LLM in information security, systematically review its application progress in malicious behavior prediction, network threat analysis, system vulnerability detection, malicious code identification, and cryptographic algorithm optimization, and explore its potential in enhancing security protection performance. Based on neural networks and Transformer architecture, this paper analyzes the technical basis of large language models and their advantages in natural language processing tasks. It is shown that the introduction of large language modeling helps to improve the detection accuracy and reduce the false alarm rate of security systems. Finally, this paper summarizes the current application results and points out that it still faces challenges in model transparency, interpretability, and scene adaptability, among other issues. It is necessary to explore further the optimization of the model structure and the improvement of the generalization ability to realize a more intelligent and accurate information security protection system.', 'abstract_zh': '信息安全部门正面临日益严峻的挑战，传统保护手段难以应对复杂多变的威胁。近年来，作为新兴的智能技术，大规模语言模型（LLMs）在信息安全领域展现了广泛的应用前景。本文聚焦大规模语言模型在信息安全中的关键作用，系统回顾其在恶意行为预测、网络安全威胁分析、系统漏洞检测、恶意代码识别以及加密算法优化等方面的应用进展，并探讨其在提升安全防护性能方面的潜力。基于神经网络和Transformer架构，本文分析了大规模语言模型的技术基础及其在自然语言处理任务中的优势。研究表明，引入大规模语言建模有助于提高安全系统的检测准确性和降低误报率。最后，本文总结了当前的应用成果，并指出模型透明性、可解释性和应用场景适应性等方面仍面临挑战，需要进一步探索模型结构的优化和泛化能力的提升，以实现更加智能和准确的信息安全保护系统。', 'title_zh': '基于大语言模型方法的信息安全：一个综述'}
{'arxiv_id': 'arXiv:2507.18206', 'title': 'MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation', 'authors': 'Arup Kumar Sahoo, Itzik Klein', 'link': 'https://arxiv.org/abs/2507.18206', 'abstract': "A fundamental requirement for full autonomy in mobile robots is accurate navigation even in situations where satellite navigation or cameras are unavailable. In such practical situations, relying only on inertial sensors will result in navigation solution drift due to the sensors' inherent noise and error terms. One of the emerging solutions to mitigate drift is to maneuver the robot in a snake-like slithering motion to increase the inertial signal-to-noise ratio, allowing the regression of the mobile robot position. In this work, we propose MoRPI-PINN as a physics-informed neural network framework for accurate inertial-based mobile robot navigation. By embedding physical laws and constraints into the training process, MoRPI-PINN is capable of providing an accurate and robust navigation solution. Using real-world experiments, we show accuracy improvements of over 85% compared to other approaches. MoRPI-PINN is a lightweight approach that can be implemented even on edge devices and used in any typical mobile robot application.", 'abstract_zh': '全自主移动机器人中准确导航的基本要求是在卫星导航或摄像头不可用的情况下仍能实现精确导航。在这种实际情况下，仅依赖惯性传感器会导致由于传感器固有的噪声和误差而导致的导航解算偏移。为了减轻偏移，一种新兴的解决方案是让机器人以蛇形滑动运动来增加惯性信号信噪比，从而允许对移动机器人位置进行回归。本文提出了一种嵌入物理定律和约束的神经网络框架MoRPI-PINN，用于准确的基于惯性的移动机器人导航。通过在训练过程中嵌入物理定律和约束，MoRPI-PINN能够提供准确且稳健的导航解算。通过实际实验，我们展示了与其它方法相比超过85%的精度改善。MoRPI-PINN是一种轻量级的方法，甚至可以在边缘设备上实现，并可用于任何典型的移动机器人应用。', 'title_zh': 'MoRPI-PINN: 一种移动机器人纯惯性导航的物理约束框架'}
{'arxiv_id': 'arXiv:2507.18202', 'title': 'Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection', 'authors': 'San Kim, Jonghwi Kim, Yejin Jeon, Gary Geunbae Lee', 'link': 'https://arxiv.org/abs/2507.18202', 'abstract': "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by providing external knowledge for accurate and up-to-date responses. However, this reliance on external sources exposes a security risk, attackers can inject poisoned documents into the knowledge base to steer the generation process toward harmful or misleading outputs. In this paper, we propose Gradient-based Masked Token Probability (GMTP), a novel defense method to detect and filter out adversarially crafted documents. Specifically, GMTP identifies high-impact tokens by examining gradients of the retriever's similarity function. These key tokens are then masked, and their probabilities are checked via a Masked Language Model (MLM). Since injected tokens typically exhibit markedly low masked-token probabilities, this enables GMTP to easily detect malicious documents and achieve high-precision filtering. Experiments demonstrate that GMTP is able to eliminate over 90% of poisoned content while retaining relevant documents, thus maintaining robust retrieval and generation performance across diverse datasets and adversarial settings.", 'abstract_zh': '基于梯度的掩码令牌概率（GMTP）：检测和过滤 adversarially 制作的文档以增强 Retrieval-Augmented Generation 的安全性', 'title_zh': '使用GMTP保护RAG流水线：基于梯度的掩蔽词概率方法检测中毒文档'}
{'arxiv_id': 'arXiv:2507.18182', 'title': 'SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models', 'authors': 'Wonjun Jeong, Dongseok Kim, Taegkeun Whangbo', 'link': 'https://arxiv.org/abs/2507.18182', 'abstract': "Large Language Models (LLMs) can achieve inflated scores on multiple-choice tasks by exploiting inherent biases in option positions or labels, rather than demonstrating genuine understanding. This study introduces SCOPE, an evaluation framework designed to measure and mitigate such selection bias in a dataset-independent manner. By repeatedly invoking a null prompt that lacks semantic content, SCOPE estimates each model's unique position-bias distribution. It then redistributes the answer slot according to the inverse-bias distribution, thereby equalizing the lucky-rate, the probability of selecting the correct answer by chance. Furthermore, it prevents semantically similar distractors from being placed adjacent to the answer, thereby blocking near-miss guesses based on superficial proximity cues. Across multiple benchmark experiments, SCOPE consistently outperformed existing debiasing methods in terms of stable performance improvements and showed clearer confidence distributions over correct options. This framework thus offers a new standard for enhancing the fairness and reliability of LLM evaluations.", 'abstract_zh': '大型语言模型（LLMs）可以通过利用选项位置或标签中的固有偏差来实现选择题中的高分，而不是展示真正的理解能力。本研究介绍了一种名为SCOPE的评估框架，旨在以数据集无关的方式测量和缓解这种选择偏差。通过反复调用一个没有语义内容的空提示，SCOPE估计每个模型的独特位置偏差分布。然后，它根据逆偏差分布重新分配答案槽，从而平等化随机正确率，即纯属偶然选择正确答案的概率。此外，它防止语义相似的干扰项紧邻正确答案放置，从而阻止基于表面接近线索的接近正确猜测。在多个基准实验中，SCOPE在稳定性能提升方面始终优于现有去偏见方法，并且在正确选项上展示了更清晰的信心分布。因此，该框架为提高LLM评估的公平性和可靠性提供了一个新的标准。', 'title_zh': 'SCOPE: 随机和反偏置选项放置以评估大型语言模型'}
{'arxiv_id': 'arXiv:2507.18177', 'title': 'Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data Scenarios', 'authors': 'Dhruv Jain, Romain Modzelewski, Romain Hérault, Clement Chatelain, Eva Torfeh, Sebastien Thureau', 'link': 'https://arxiv.org/abs/2507.18177', 'abstract': 'In data-scarce scenarios, deep learning models often overfit to noise and irrelevant patterns, which limits their ability to generalize to unseen samples. To address these challenges in medical image segmentation, we introduce Diff-UMamba, a novel architecture that combines the UNet framework with the mamba mechanism for modeling long-range dependencies. At the heart of Diff-UMamba is a Noise Reduction Module (NRM), which employs a signal differencing strategy to suppress noisy or irrelevant activations within the encoder. This encourages the model to filter out spurious features and enhance task-relevant representations, thereby improving its focus on clinically meaningful regions. As a result, the architecture achieves improved segmentation accuracy and robustness, particularly in low-data settings. Diff-UMamba is evaluated on multiple public datasets, including MSD (lung and pancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over baseline methods across diverse segmentation tasks. To further assess performance under limited-data conditions, additional experiments are conducted on the BraTS-21 dataset by varying the proportion of available training samples. The approach is also validated on a small internal non-small cell lung cancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam CT (CBCT), where it achieves a 4-5% improvement over the baseline.', 'abstract_zh': '在数据稀缺场景中，深度学习模型常常过拟合到噪声和无关模式，限制了它们对未见样本的泛化能力。为了解决这些挑战在医学图像分割中的问题，我们引入了Diff-UMamba架构，该架构结合了UNet框架和mamba机制以建模长程依赖关系。Diff-UMamba的核心是噪声减少模块（NRM），该模块采用信号差分策略抑制编码器中的噪声或无关激活，鼓励模型过滤掉虚假特征并增强与任务相关的表示，从而提升其对临床有意义区域的关注。因此，该架构在低数据设置中实现了改进的分割准确性和鲁棒性。Diff-UMamba在多个公开数据集（包括MSD（肺和胰腺）和AIIB23）上进行评估，与基线方法相比，在多种分割任务中展示了1-3%的一致性能提升。为了进一步评估在有限数据条件下的表现，还对BraTS-21数据集进行了额外实验，通过改变可用训练样本的比例进行实验。该方法也在一个小型内部的非小细胞肺癌（NSCLC）数据集（锥束CT（CBCT）中的GTV分割）上进行了验证，相比基线方法实现了4-5%的改进。', 'title_zh': 'Differential-UMamba: 重新思考有限数据场景下的肿瘤分割'}
{'arxiv_id': 'arXiv:2507.18171', 'title': 'Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models', 'authors': 'Kexin Chen, Dongxia Wang, Yi Liu, Haonan Zhang, Wenhai Wang', 'link': 'https://arxiv.org/abs/2507.18171', 'abstract': "Despite the widespread use of Transformer-based text embedding models in NLP tasks, surprising 'sticky tokens' can undermine the reliability of embeddings. These tokens, when repeatedly inserted into sentences, pull sentence similarity toward a certain value, disrupting the normal distribution of embedding distances and degrading downstream performance. In this paper, we systematically investigate such anomalous tokens, formally defining them and introducing an efficient detection method, Sticky Token Detector (STD), based on sentence and token filtering. Applying STD to 40 checkpoints across 14 model families, we discover a total of 868 sticky tokens. Our analysis reveals that these tokens often originate from special or unused entries in the vocabulary, as well as fragmented subwords from multilingual corpora. Notably, their presence does not strictly correlate with model size or vocabulary size. We further evaluate how sticky tokens affect downstream tasks like clustering and retrieval, observing significant performance drops of up to 50%. Through attention-layer analysis, we show that sticky tokens disproportionately dominate the model's internal representations, raising concerns about tokenization robustness. Our findings show the need for better tokenization strategies and model design to mitigate the impact of sticky tokens in future text embedding applications.", 'abstract_zh': '基于Transformer的文本嵌入模型中意外的“粘性令牌”对嵌入可靠性的挑战：系统研究及检测方法', 'title_zh': '遵循均值：检测文本嵌入模型中的粘性词 token'}
{'arxiv_id': 'arXiv:2507.18153', 'title': 'When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label', 'authors': 'Riting Xia, Rucong Wang, Yulin Liu, Anchen Li, Xueyan Liu, Yan Zhang', 'link': 'https://arxiv.org/abs/2507.18153', 'abstract': 'Class-imbalanced graph node classification is a practical yet underexplored research problem. Although recent studies have attempted to address this issue, they typically assume clean and reliable labels when processing class-imbalanced graphs. This assumption often violates the nature of real-world graphs, where labels frequently contain noise. Given this gap, this paper systematically investigates robust node classification for class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph Augmentation framework based on Large language models (LLMs) and Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling method to generate synthetic minority nodes, producing label-accurate minority nodes to alleviate class imbalance. Based on the class-balanced graphs, we develop a dynamically weighted pseudo-labeling method to obtain high-confidence pseudo labels to reduce label noise ratio. Additionally, we implement a secondary LLM-guided oversampling mechanism to mitigate potential class distribution skew caused by pseudo labels. Experimental results show that GraphALP achieves superior performance over state-of-the-art methods on class-imbalanced graphs with noisy labels.', 'abstract_zh': '带有噪声标签的类别不平衡图节点分类是一个实际但尚未充分探索的研究问题。', 'title_zh': '图上噪音标签与类别不平衡共存时：一种结合LLM和伪标签的图增强方法'}
{'arxiv_id': 'arXiv:2507.18143', 'title': 'HIVMedQA: Benchmarking large language models for HIV medical decision support', 'authors': 'Gonzalo Cardenal Antolin, Jacques Fellay, Bashkim Jaha, Roger Kouyos, Niko Beerenwinkel, Diane Duroux', 'link': 'https://arxiv.org/abs/2507.18143', 'abstract': 'Large language models (LLMs) are emerging as valuable tools to support clinicians in routine decision-making. HIV management is a compelling use case due to its complexity, including diverse treatment options, comorbidities, and adherence challenges. However, integrating LLMs into clinical practice raises concerns about accuracy, potential harm, and clinician acceptance. Despite their promise, AI applications in HIV care remain underexplored, and LLM benchmarking studies are scarce. This study evaluates the current capabilities of LLMs in HIV management, highlighting their strengths and limitations. We introduce HIVMedQA, a benchmark designed to assess open-ended medical question answering in HIV care. The dataset consists of curated, clinically relevant questions developed with input from an infectious disease physician. We evaluated seven general-purpose and three medically specialized LLMs, applying prompt engineering to enhance performance. Our evaluation framework incorporates both lexical similarity and an LLM-as-a-judge approach, extended to better reflect clinical relevance. We assessed performance across key dimensions: question comprehension, reasoning, knowledge recall, bias, potential harm, and factual accuracy. Results show that Gemini 2.5 Pro consistently outperformed other models across most dimensions. Notably, two of the top three models were proprietary. Performance declined as question complexity increased. Medically fine-tuned models did not always outperform general-purpose ones, and larger model size was not a reliable predictor of performance. Reasoning and comprehension were more challenging than factual recall, and cognitive biases such as recency and status quo were observed. These findings underscore the need for targeted development and evaluation to ensure safe, effective LLM integration in clinical care.', 'abstract_zh': '大型语言模型（LLMs）在支持临床决策中的应用：以HIV管理为例的基准评估与探讨', 'title_zh': 'HIVMedQA：评估大型语言模型在HIV医疗决策支持中的性能'}
{'arxiv_id': 'arXiv:2507.18133', 'title': 'Deep Learning for Glioblastoma Morpho-pathological Features Identification: A BraTS-Pathology Challenge Solution', 'authors': 'Juexin Zhang, Ying Weng, Ke Chen', 'link': 'https://arxiv.org/abs/2507.18133', 'abstract': "Glioblastoma, a highly aggressive brain tumor with diverse molecular and pathological features, poses a diagnostic challenge due to its heterogeneity. Accurate diagnosis and assessment of this heterogeneity are essential for choosing the right treatment and improving patient outcomes. Traditional methods rely on identifying specific features in tissue samples, but deep learning offers a promising approach for improved glioblastoma diagnosis. In this paper, we present our approach to the BraTS-Path Challenge 2024. We leverage a pre-trained model and fine-tune it on the BraTS-Path training dataset. Our model demonstrates poor performance on the challenging BraTS-Path validation set, as rigorously assessed by the Synapse online platform. The model achieves an accuracy of 0.392229, a recall of 0.392229, and a F1-score of 0.392229, indicating a consistent ability to correctly identify instances under the target condition. Notably, our model exhibits perfect specificity of 0.898704, showing an exceptional capacity to correctly classify negative cases. Moreover, a Matthews Correlation Coefficient (MCC) of 0.255267 is calculated, to signify a limited positive correlation between predicted and actual values and highlight our model's overall predictive power. Our solution also achieves the second place during the testing phase.", 'abstract_zh': '胶质母细胞瘤：一种具有多样分子和病理特征的高度恶惟能，由于其异质性导致诊断困难。准确诊断和评估这种异质性对于选择合适的治疗方案和改善患者预后至关重要。传统方法依赖于在组织样本中识别特定特征，而深度学习为改进胶质母细胞瘤诊断提供了有希望的方法。本文介绍了我们对2024年BraTS-Path挑战赛的方法。我们利用预训练模型，并在BraTS-Path训练数据集上进行微调。我们的模型在Synapse在线平台严格评估的具有挑战性的BraTS-Path验证集中表现不佳，准确率为0.392229，召回率为0.392229，F1分为0.392229，表明模型在正确识别目标条件下的实例方面具有一致性能力。值得注意的是，我们的模型在负例分类方面表现出完美的特异性0.898704，显示出出色的分类能力。另计算的麦考利相关系数（MCC）为0.255267，表明预测值与实际值之间有有限的正相关，并突显了模型的整体预测能力。此外，在测试阶段，我们的解决方案获得第二名。', 'title_zh': '基于深度学习的胶质母细胞瘤形态病理特征识别：BraTS-Pathology挑战赛解决方案'}
{'arxiv_id': 'arXiv:2507.18126', 'title': 'U-Net Based Healthy 3D Brain Tissue Inpainting', 'authors': 'Juexin Zhang, Ying Weng, Ke Chen', 'link': 'https://arxiv.org/abs/2507.18126', 'abstract': "This paper introduces a novel approach to synthesize healthy 3D brain tissue from masked input images, specifically focusing on the task of 'ASNR-MICCAI BraTS Local Synthesis of Tissue via Inpainting'. Our proposed method employs a U-Net-based architecture, which is designed to effectively reconstruct the missing or corrupted regions of brain MRI scans. To enhance our model's generalization capabilities and robustness, we implement a comprehensive data augmentation strategy that involves randomly masking healthy images during training. Our model is trained on the BraTS-Local-Inpainting dataset and demonstrates the exceptional performance in recovering healthy brain tissue. The evaluation metrics employed, including Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE), consistently yields impressive results. On the BraTS-Local-Inpainting validation set, our model achieved an SSIM score of 0.841, a PSNR score of 23.257, and an MSE score of 0.007. Notably, these evaluation metrics exhibit relatively low standard deviations, i.e., 0.103 for SSIM score, 4.213 for PSNR score and 0.007 for MSE score, which indicates that our model's reliability and consistency across various input scenarios. Our method also secured first place in the challenge.", 'abstract_zh': '本文介绍了一种从蒙版输入图像合成健康3D脑组织的新型方法，特别关注“ASNR-MICCAI BraTS局部组织修复合成任务”。我们提出的方法采用基于U-Net的架构，旨在有效重建脑MRI扫描中缺失或损坏的区域。为了增强模型的泛化能力和鲁棒性，我们在训练过程中实施了全面的数据增强策略，包括随机蒙版健康图像。我们的模型在BraTS-Local-Inpainting数据集上进行训练，并展示了在恢复健康脑组织方面的出色性能。所采用的评价指标，包括结构相似性指数（SSIM）、峰值信噪比（PSNR）和均方误差（MSE），均取得了令人印象深刻的成果。在BraTS-Local-Inpainting验证集上，我们的模型取得了SSIM评分为0.841、PSNR评分为23.257和MSE评分为0.007的结果。值得注意的是，这些评价指标的标准差相对较低，即SSIM评分为0.103、PSNR评分为4.213和MSE评分为0.007，这表明我们的模型在各种输入情境下的可靠性和一致性。我们的方法也在挑战中获得了第一名。', 'title_zh': 'U-Net 基础健康的3D脑组织修复'}
{'arxiv_id': 'arXiv:2507.18119', 'title': 'GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness', 'authors': 'Hongjie Chen, Zehan Li, Yaodong Song, Wenming Deng, Yitong Yao, Yuxin Zhang, Hang Lv, Xuechao Zhu, Jian Kang, Jie Lian, Jie Li, Chao Wang, Shuangyong Song, Yongxiang Li, Zhongjiang He', 'link': 'https://arxiv.org/abs/2507.18119', 'abstract': 'Recent advances in end-to-end spoken language models (SLMs) have significantly improved the ability of AI systems to engage in natural spoken interactions. However, most existing models treat speech merely as a vehicle for linguistic content, often overlooking the rich paralinguistic and speaker characteristic cues embedded in human speech, such as dialect, age, emotion, and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel spoken language model with paralinguistic and speaker characteristic awareness, designed to extend spoken language modeling beyond text semantics. GOAT-SLM adopts a dual-modality head architecture that decouples linguistic modeling from acoustic realization, enabling robust language understanding while supporting expressive and adaptive speech generation. To enhance model efficiency and versatility, we propose a modular, staged training strategy that progressively aligns linguistic, paralinguistic, and speaker characteristic information using large-scale speech-text corpora. Experimental results on TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM achieves well-balanced performance across both semantic and non-semantic tasks, and outperforms existing open-source models in handling emotion, dialectal variation, and age-sensitive interactions. This work highlights the importance of modeling beyond linguistic content and advances the development of more natural, adaptive, and socially aware spoken language systems.', 'abstract_zh': 'Recent Advances in End-to-End Spoken Language Models with Paralinguistic and Speaker Characteristic Awareness', 'title_zh': 'GOAT-SLM：一种awareness Paralinguistic和Speaker特征的口语语言模型'}
{'arxiv_id': 'arXiv:2507.18112', 'title': 'Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation Using Tensor Networks', 'authors': 'Binghua Li, Ziqing Chang, Tong Liang, Chao Li, Toshihisa Tanaka, Shigeki Aoki, Qibin Zhao, Zhe Sun', 'link': 'https://arxiv.org/abs/2507.18112', 'abstract': 'We address the challenge of parameter-efficient fine-tuning (PEFT) for three-dimensional (3D) U-Net-based denoising diffusion probabilistic models (DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its practical significance, research on parameter-efficient representations of 3D convolution operations remains limited. To bridge this gap, we propose Tensor Volumetric Operator (TenVOO), a novel PEFT method specifically designed for fine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network modeling, TenVOO represents 3D convolution kernels with lower-dimensional tensors, effectively capturing complex spatial dependencies during fine-tuning with few parameters. We evaluate TenVOO on three downstream brain MRI datasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830 T1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that TenVOO achieves state-of-the-art performance in multi-scale structural similarity index measure (MS-SSIM), outperforming existing approaches in capturing spatial dependencies while requiring only 0.3% of the trainable parameters of the original model. Our code is available at: this https URL', 'abstract_zh': '我们针对基于3D U-Net的去噪扩散概率模型（DDPMs）在磁共振成像（MRI）图像生成中的参数高效微调（PEFT）挑战进行了研究。通过引入张量体元算子（TenVOO）这一新型PEFT方法，特别是在3D卷积骨干网络下的微调，我们利用张量网络建模，以低维张量表示3D卷积核，在微调过程中有效捕捉复杂的空间依赖性，同时仅使用较少的参数。我们在ADNI、PPMI和BraTS2021三个下游脑MRI数据集上评估了TenVOO，通过对英国生物银行中59,830张T1加权脑MRI扫描数据预训练的DDPM进行微调。实验结果表明，TenVOO在多尺度结构相似性指数度量（MS-SSIM）上取得了最佳性能，同时在捕捉空间依赖性方面优于现有方法，仅需原始模型可训练参数的0.3%。代码现可从以下链接获取：this https URL。', 'title_zh': '使用张量网络的3D DDPM参数高效微调以生成MRI图像'}
{'arxiv_id': 'arXiv:2507.18106', 'title': 'Distributional Uncertainty for Out-of-Distribution Detection', 'authors': 'JinYoung Kim, DaeUng Jo, Kimin Yun, Jeonghyo Song, Youngjoon Yoo', 'link': 'https://arxiv.org/abs/2507.18106', 'abstract': 'Estimating uncertainty from deep neural networks is a widely used approach for detecting out-of-distribution (OoD) samples, which typically exhibit high predictive uncertainty. However, conventional methods such as Monte Carlo (MC) Dropout often focus solely on either model or data uncertainty, failing to align with the semantic objective of OoD detection. To address this, we propose the Free-Energy Posterior Network, a novel framework that jointly models distributional uncertainty and identifying OoD and misclassified regions using free energy. Our method introduces two key contributions: (1) a free-energy-based density estimator parameterized by a Beta distribution, which enables fine-grained uncertainty estimation near ambiguous or unseen regions; and (2) a loss integrated within a posterior network, allowing direct uncertainty estimation from learned parameters without requiring stochastic sampling. By integrating our approach with the residual prediction branch (RPL) framework, the proposed method goes beyond post-hoc energy thresholding and enables the network to learn OoD regions by leveraging the variance of the Beta distribution, resulting in a semantically meaningful and computationally efficient solution for uncertainty-aware segmentation. We validate the effectiveness of our method on challenging real-world benchmarks, including Fishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.', 'abstract_zh': '基于自由能后验网络的分布不确定性与异常区域联合建模', 'title_zh': '分布不确定性进行异分布检测'}
{'arxiv_id': 'arXiv:2507.18100', 'title': 'Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning', 'authors': 'Ruizhe Chen, Zhiting Fan, Tianze Luo, Heqing Zou, Zhaopeng Feng, Guiyang Xie, Hansheng Zhang, Zhuochen Wang, Zuozhu Liu, Huaijian Zhang', 'link': 'https://arxiv.org/abs/2507.18100', 'abstract': 'Video Temporal Grounding (VTG) aims to localize relevant temporal segments in videos given natural language queries. Despite recent progress with large vision-language models (LVLMs) and instruction-tuning, existing approaches often suffer from limited temporal awareness and poor generalization. In this work, we introduce a two-stage training framework that integrates supervised fine-tuning with reinforcement learning (RL) to improve both the accuracy and robustness of VTG models. Our approach first leverages high-quality curated cold start data for SFT initialization, followed by difficulty-controlled RL to further enhance temporal localization and reasoning abilities. Comprehensive experiments on multiple VTG benchmarks demonstrate that our method consistently outperforms existing models, particularly in challenging and open-domain scenarios. We conduct an in-depth analysis of training strategies and dataset curation, highlighting the importance of both high-quality cold start data and difficulty-controlled RL. To facilitate further research and industrial adoption, we release all intermediate datasets, models, and code to the community.', 'abstract_zh': '视频时间定位（VTG）的目标是给定自然语言查询，在视频中定位相关的时序段。尽管大规模视觉-语言模型（LVLMs）和指令调优取得了进步，现有的方法往往存在时间感知有限和泛化能力差的问题。在本工作中，我们提出了一种两阶段训练框架，将监督微调与强化学习（RL）相结合，以提高VTG模型的准确性和鲁棒性。我们的方法首先利用高质量策划的冷启动数据进行监督微调初始化，随后通过难度可控的RL进一步增强时间定位和推理能力。在多个VTG基准上的全面实验表明，我们的方法在各种场景下（尤其是挑战性和开放域场景）均优于现有模型。我们对训练策略和数据策划进行了深入分析，强调了高质量冷启动数据和难度可控的RL的重要性。为了促进进一步的研究和工业应用，我们向社区提供了所有中间数据集、模型和代码。', 'title_zh': '用于强化学习的视频时间定位数据集和食谱'}
{'arxiv_id': 'arXiv:2507.18082', 'title': 'TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound', 'authors': 'Pascal Spiegler, Taha Koleilat, Arash Harirpoush, Corey S. Miller, Hassan Rivaz, Marta Kersten-Oertel, Yiming Xiao', 'link': 'https://arxiv.org/abs/2507.18082', 'abstract': "Pancreatic cancer carries a poor prognosis and relies on endoscopic ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle noise, low contrast, and unintuitive appearance of EUS make segmentation of pancreatic tumors with fully supervised deep learning (DL) models both error-prone and dependent on large, expert-curated annotation datasets. To address these challenges, we present TextSAM-EUS, a novel, lightweight, text-driven adaptation of the Segment Anything Model (SAM) that requires no manual geometric prompts at inference. Our approach leverages text prompt learning (context optimization) through the BiomedCLIP text encoder in conjunction with a LoRA-based adaptation of SAM's architecture to enable automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total parameters. On the public Endoscopic Ultrasound Database of the Pancreas, TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised DL models and foundation models (e.g., SAM and its variants). As the first attempt to incorporate prompt learning in SAM-based medical image segmentation, TextSAM-EUS offers a practical option for efficient and robust automatic EUS segmentation. Our code will be publicly available upon acceptance.", 'abstract_zh': '基于文本驱动的TextSAM-EUS在内镜超声影像中自动胰腺肿瘤分割', 'title_zh': 'TextSAM-EUS: 文本提示学习以提高萨姆分割胰腺肿瘤的准确性在内镜超声中的应用'}
{'arxiv_id': 'arXiv:2507.18071', 'title': 'Group Sequence Policy Optimization', 'authors': 'Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, Junyang Lin', 'link': 'https://arxiv.org/abs/2507.18071', 'abstract': 'This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.', 'abstract_zh': '此论文介绍了Group Sequence Policy Optimization (GSPO)算法，这是一种用于训练大规模语言模型的稳定、高效且高性能的强化学习算法。与之前采用令牌级别重要性比例的算法不同，GSPO基于序列似然性定义重要性比例，并在序列级别进行剪裁、奖励和优化。我们证明了相比于GRPO算法，GSPO在训练效率和性能上表现出卓越的优势，尤其能够稳定Mixture-of-Experts (MoE) RL训练，并且有潜力简化RL基础设施的设计。这些特点已经促成最新Qwen3模型的显著改进。', 'title_zh': '组序列策略优化'}
{'arxiv_id': 'arXiv:2507.18061', 'title': 'TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios', 'authors': 'Zehan Li, Hongjie Chen, Yuxin Zhang, Jing Zhou, Xuening Wang, Hang Lv, Mengjie Du, Yaodong Song, Jie Lian, Jian Kang, Jie Li, Yongxiang Li, Zhongjiang He, Xuelong Li', 'link': 'https://arxiv.org/abs/2507.18061', 'abstract': "Spoken language models (SLMs) have seen rapid progress in recent years, along with the development of numerous benchmarks for evaluating their performance. However, most existing benchmarks primarily focus on evaluating whether SLMs can perform complex tasks comparable to those tackled by large language models (LLMs), often failing to align with how users naturally interact in real-world conversational scenarios. In this paper, we propose TELEVAL, a dynamic benchmark specifically designed to evaluate SLMs' effectiveness as conversational agents in realistic Chinese interactive settings. TELEVAL defines three evaluation dimensions: Explicit Semantics, Paralinguistic and Implicit Semantics, and System Abilities. It adopts a dialogue format consistent with real-world usage and evaluates text and audio outputs separately. TELEVAL particularly focuses on the model's ability to extract implicit cues from user speech and respond appropriately without additional instructions. Our experiments demonstrate that despite recent progress, existing SLMs still have considerable room for improvement in natural conversational tasks. We hope that TELEVAL can serve as a user-centered evaluation framework that directly reflects the user experience and contributes to the development of more capable dialogue-oriented SLMs.", 'abstract_zh': 'TELEVAL：专为现实中文交互场景设计的对话代理评估基准', 'title_zh': 'TELEVAL：一个为中文交互场景设计的语音语言模型动态基准'}
{'arxiv_id': 'arXiv:2507.18046', 'title': 'Enhancing Scene Transition Awareness in Video Generation via Post-Training', 'authors': 'Hanwen Shen, Jiajie Lu, Yupeng Cao, Xiaonan Yang', 'link': 'https://arxiv.org/abs/2507.18046', 'abstract': 'Recent advances in AI-generated video have shown strong performance on \\emph{text-to-video} tasks, particularly for short clips depicting a single scene. However, current models struggle to generate longer videos with coherent scene transitions, primarily because they cannot infer when a transition is needed from the prompt. Most open-source models are trained on datasets consisting of single-scene video clips, which limits their capacity to learn and respond to prompts requiring multiple scenes. Developing scene transition awareness is essential for multi-scene generation, as it allows models to identify and segment videos into distinct clips by accurately detecting transitions.\nTo address this, we propose the \\textbf{Transition-Aware Video} (TAV) dataset, which consists of preprocessed video clips with multiple scene transitions. Our experiment shows that post-training on the \\textbf{TAV} dataset improves prompt-based scene transition understanding, narrows the gap between required and generated scenes, and maintains image quality.', 'abstract_zh': '近期AI生成视频的研究在文本到视频任务上取得了显著性能，尤其是对于描绘单个场景的短片段。然而，当前模型在生成长视频和连贯场景过渡方面表现不佳，主要原因是它们无法从提示中推断出何时需要进行过渡。大多数开源模型都是在仅包含单场景视频片段的数据集上训练的，这限制了它们在处理需要多个场景的提示时的学习和响应能力。为了应对这一挑战，我们提出了**具有场景意识的视频**（Transition-Aware Video, TAV）数据集，该数据集包含多个场景过渡的预处理视频片段。我们的实验表明，基于TAV数据集进行训练可以提高基于提示的场景过渡理解能力，缩小所需场景和生成场景之间的差距，并保持图像质量。', 'title_zh': '通过后训练增强视频生成中的场景过渡意识'}
{'arxiv_id': 'arXiv:2507.18044', 'title': 'Synthetic Data Generation for Phrase Break Prediction with Large Language Model', 'authors': 'Hoyeon Lee, Sejung Son, Ye-Eun Kang, Jong-Hwan Kim', 'link': 'https://arxiv.org/abs/2507.18044', 'abstract': 'Current approaches to phrase break prediction address crucial prosodic aspects of text-to-speech systems but heavily rely on vast human annotations from audio or text, incurring significant manual effort and cost. Inherent variability in the speech domain, driven by phonetic factors, further complicates acquiring consistent, high-quality data. Recently, large language models (LLMs) have shown success in addressing data challenges in NLP by generating tailored synthetic data while reducing manual annotation needs. Motivated by this, we explore leveraging LLM to generate synthetic phrase break annotations, addressing the challenges of both manual annotation and speech-related tasks by comparing with traditional annotations and assessing effectiveness across multiple languages. Our findings suggest that LLM-based synthetic data generation effectively mitigates data challenges in phrase break prediction and highlights the potential of LLMs as a viable solution for the speech domain.', 'abstract_zh': '基于大语言模型的合成短语断言注释在短语断言预测中的应用', 'title_zh': '大规模语言模型驱动的短语断点预测合成数据生成'}
{'arxiv_id': 'arXiv:2507.18043', 'title': 'GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs', 'authors': 'Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal', 'link': 'https://arxiv.org/abs/2507.18043', 'abstract': "Inference-time steering methods offer a lightweight alternative to fine-tuning large language models (LLMs) and vision-language models (VLMs) by modifying internal activations at test time without updating model weights. However, most existing approaches rely on fixed, global intervention vectors, overlook the causal influence of individual input tokens, and fail to leverage informative gradients from the model's logits, particularly in multimodal settings where visual and textual inputs contribute unevenly. To address these limitations, we introduce GrAInS, an inference-time steering approach that operates across both language-only and vision-language models and tasks. GrAInS uses contrastive, gradient-based attribution via Integrated Gradients to identify the top-k most influential tokens, both positively and negatively attributed based on their contribution to preferred versus dispreferred outputs. These tokens are then used to construct directional steering vectors that capture semantic shifts from undesirable to desirable behavior. During inference, GrAInS adjusts hidden activations at transformer layers guided by token-level attribution signals, and normalizes activations to preserve representational scale. This enables fine-grained, interpretable, and modular control over model behavior, without retraining or auxiliary supervision. Empirically, GrAInS consistently outperforms both fine-tuning and existing steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514 with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all while preserving the model's fluency and general capabilities.", 'abstract_zh': '基于推理时修正的方法提供了一种轻量级替代方案，用于在测试时通过修改内部激活而不更新模型权重来调整大规模语言模型（LLMs）和多模态视觉-语言模型（VLMs）。然而，现有大多数方法依赖于固定的整体干预向量，忽视了单个输入词元的因果影响，并未能充分利用模型logits中的有用梯度，特别是在视觉和文本输入贡献不均的多模态设置中。为了解决这些局限性，我们引入了GrAInS，这是一种既适用于语言模型也适用于多模态视觉-语言模型及其任务的基于推理时修正的方法。GrAInS 使用对比梯度归因方法（Integrated Gradients）来识别按其对偏好输出和非偏好输出贡献度的正向和负向影响确定的最具影响力的top-k词元。然后，利用这些词元构建方向修正向量，以捕捉从不良行为到良好行为的语义转变。在推理时，GrAInS 在变压器层中根据词元级别的归因信号调整隐藏激活，并进行归一化以保持表征的规模。这使人们能够在不重新训练或添加辅助监督的情况下，对模型行为实现细粒度、可解释和模块化的控制。实证研究表明，GrAInS 在准确度、减少幻觉率和增强对齐胜率方面，均优于微调和现有修正基线，同时保持了模型的流畅性和通用能力。', 'title_zh': 'GrAInS: 基于梯度的归因在推理时 steering 大型语言模型和视觉语言模型'}
{'arxiv_id': 'arXiv:2507.18033', 'title': 'OpenNav: Open-World Navigation with Multimodal Large Language Models', 'authors': 'Mingfeng Yuan, Letian Wang, Steven L. Waslander', 'link': 'https://arxiv.org/abs/2507.18033', 'abstract': "Pre-trained large language models (LLMs) have demonstrated strong common-sense reasoning abilities, making them promising for robotic navigation and planning tasks. However, despite recent progress, bridging the gap between language descriptions and actual robot actions in the open-world, beyond merely invoking limited predefined motion primitives, remains an open challenge. In this work, we aim to enable robots to interpret and decompose complex language instructions, ultimately synthesizing a sequence of trajectory points to complete diverse navigation tasks given open-set instructions and open-set objects. We observe that multi-modal large language models (MLLMs) exhibit strong cross-modal understanding when processing free-form language instructions, demonstrating robust scene comprehension. More importantly, leveraging their code-generation capability, MLLMs can interact with vision-language perception models to generate compositional 2D bird-eye-view value maps, effectively integrating semantic knowledge from MLLMs with spatial information from maps to reinforce the robot's spatial understanding. To further validate our approach, we effectively leverage large-scale autonomous vehicle datasets (AVDs) to validate our proposed zero-shot vision-language navigation framework in outdoor navigation tasks, demonstrating its capability to execute a diverse range of free-form natural language navigation instructions while maintaining robustness against object detection errors and linguistic ambiguities. Furthermore, we validate our system on a Husky robot in both indoor and outdoor scenes, demonstrating its real-world robustness and applicability. Supplementary videos are available at this https URL", 'abstract_zh': '预训练大型语言模型（LLMs）展现了强大的常识推理能力，使其在机器人导航和规划任务中具有潜力。然而，尽管取得了近期进展，如何在开放世界中弥合语言描述与实际机器人动作之间的差距，而不仅仅是调用有限的预定义运动 primitive，仍然是一个开放的挑战。在本文中，我们旨在使机器人能够解释和分解复杂的语言指令，最终在给定开放指令和开放对象的情况下合成轨迹点序列，以完成各种导航任务。我们观察到，多模态大型语言模型（MLLMs）在处理自由形式的语言指令时表现出强大的跨模态理解能力，展示了稳健的场景理解。更重要的是，利用其代码生成能力，MLLMs 可以与视觉-语言感知模型互动，生成组合的2D 鸟瞰图价值图，有效地将MLLMs 的语义知识与地图中的空间信息整合，增强机器人的空间理解。为了进一步验证我们提出的方法，我们有效地利用大规模的自主车辆数据集（AVDs）来验证我们提出的零样本视觉-语言导航框架在室外导航任务中的有效性，展示了其在执行多样化自由形式自然语言导航指令的同时，保持着对物体检测错误和语义歧义的鲁棒性。此外，我们在 Husky 机器人上验证了我们的系统，无论是在室内还是室外场景，都展示了其实用性和鲁棒性。相关的补充视频可在此网址查看。', 'title_zh': 'OpenNav: 多模态大型语言模型支持的开放世界导航'}
{'arxiv_id': 'arXiv:2507.18031', 'title': 'ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks', 'authors': 'Ahmad ALBarqawi, Mahmoud Nazzal, Issa Khalil, Abdallah Khreishah, NhatHai Phan', 'link': 'https://arxiv.org/abs/2507.18031', 'abstract': "The rapid rise of deepfake technology, which produces realistic but fraudulent digital content, threatens the authenticity of media. Traditional deepfake detection approaches often struggle with sophisticated, customized deepfakes, especially in terms of generalization and robustness against malicious attacks. This paper introduces ViGText, a novel approach that integrates images with Vision Large Language Model (VLLM) Text explanations within a Graph-based framework to improve deepfake detection. The novelty of ViGText lies in its integration of detailed explanations with visual data, as it provides a more context-aware analysis than captions, which often lack specificity and fail to reveal subtle inconsistencies. ViGText systematically divides images into patches, constructs image and text graphs, and integrates them for analysis using Graph Neural Networks (GNNs) to identify deepfakes. Through the use of multi-level feature extraction across spatial and frequency domains, ViGText captures details that enhance its robustness and accuracy to detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText significantly enhances generalization and achieves a notable performance boost when it detects user-customized deepfakes. Specifically, average F1 scores rise from 72.45% to 98.32% under generalization evaluation, and reflects the model's superior ability to generalize to unseen, fine-tuned variations of stable diffusion models. As for robustness, ViGText achieves an increase of 11.1% in recall compared to other deepfake detection approaches. When facing targeted attacks that exploit its graph-based architecture, ViGText limits classification performance degradation to less than 4%. ViGText uses detailed visual and textual analysis to set a new standard for detecting deepfakes, helping ensure media authenticity and information integrity.", 'abstract_zh': '快速崛起的深度伪造技术产生了逼真但虚假的数字内容，威胁到媒体的真实性。传统深度伪造检测方法往往难以应对复杂的、定制化的深度伪造，特别是在泛化能力和抵御恶意攻击的鲁棒性方面。本文介绍了一种名为ViGText的新型方法，该方法将图像与Vision Large Language Model (VLLM) 文本解释结合在图框架中，以提高深度伪造检测能力。ViGText的创新之处在于它将详细的解释与视觉数据相结合，提供比缺乏具体性且难以揭示细微不一致性的字幕更丰富的上下文分析。ViGText系统性地将图像分割成块，构建图像和文本图，并使用图神经网络（GNNs）进行分析以识别深度伪造。通过跨空间和频域的多级特征提取，ViGText捕获了增强其鲁棒性和检测复杂深度伪造准确性的细节。广泛的实验表明，ViGText显著提高了泛化能力，并在检测用户定制的深度伪造时实现了显著的性能提升。具体来说，在泛化评估中，平均F1分数从72.45%提高到98.32%，反映了模型在泛化到未见过的、微调后的稳定扩散模型变体方面的优越能力。对于鲁棒性，ViGText相较于其他深度伪造检测方法召回率提高了11.1%。面对利用其图架构的针对性攻击，ViGText将分类性能下降限制在少于4%。ViGText通过详细的视觉和文本分析设立了检测深度伪造的新标准，有助于确保媒体的真实性与信息完整性。', 'title_zh': 'ViGText：结合视觉-语言模型解释和图神经网络的深仿图像检测'}
{'arxiv_id': 'arXiv:2507.18028', 'title': 'NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database', 'authors': 'Weizhi Fei, Hao Shi, Jing Xu, Jingchen Peng, Jiazheng Li, Jingzhao Zhang, Bo Bai, Wei Han, Zhenyuan Chen, Xueyan Niu', 'link': 'https://arxiv.org/abs/2507.18028', 'abstract': 'Efficiently editing knowledge stored in large language models (LLMs) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of LLMs and even result in forgetting edited facts when scaling up to thousands of edits. In this paper, we model existing linear L\\&E methods as querying a Key-Value (KV) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural KV database equipped with a non-linear gated retrieval module, % In particular, our gated module only operates when inference involves the edited facts, effectively preserving the general abilities of LLMs. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB not only excels in editing efficacy, generalization, specificity, fluency, and consistency, but also preserves overall performance across six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\\textbf{50x} more than in prior work).', 'abstract_zh': '高效编辑大型语言模型中存储的知识 enables 模型更新而无需大规模训练的一个可能解决方案是 Locate-and-Edit (L\\&E)，允许同时修改大量事实。然而，在扩展到数千个编辑时，这种编辑可能会损害大型语言模型的一般能力，甚至导致遗忘编辑的事实。在本文中，我们将现有的线性 L\\&E 方法建模为查询一个键值（KV）数据库。从这一视角出发，我们提出了 NeuralDB，这是一种编辑框架，明确地将编辑的事实表示为一个配备了非线性门控检索模块的神经 KV 数据库，特别是我们的门控模块仅在涉及编辑的事实进行推理时才操作，有效地保留了大型语言模型的一般能力。在 ZsRE 和 CounterFacts 数据集上对 10,000 个事实进行了编辑的全面实验使用了 GPT2-XL、GPT-J（6B）和 Llama-3（8B）。实验结果表明，NeuralDB 不仅在编辑效能、泛化能力、特异性、流畅性和一致性方面表现出色，还在六个代表性文本理解和生成任务中保持了整体性能。进一步的实验表明，即使扩展到 100,000 个事实（比先前工作多 50 倍），NeuralDB 仍然保持其有效性。', 'title_zh': 'NeuralDB：将知识编辑扩展至100,000条事实的LLM神经KV数据库'}
{'arxiv_id': 'arXiv:2507.18017', 'title': 'Fashion-AlterEval: A Dataset for Improved Evaluation of Conversational Recommendation Systems with Alternative Relevant Items', 'authors': 'Maria Vlachou', 'link': 'https://arxiv.org/abs/2507.18017', 'abstract': 'In Conversational Recommendation Systems (CRS), a user provides feedback on recommended items at each turn, leading the CRS towards improved recommendations. Due to the need for a large amount of data, a user simulator is employed for both training and evaluation. Such user simulators critique the current retrieved item based on knowledge of a single target item. However, system evaluation in offline settings with simulators is limited by the focus on a single target item and their unlimited patience over a large number of turns. To overcome these limitations of existing simulators, we propose Fashion-AlterEval, a new dataset that contains human judgments for a selection of alternative items by adding new annotations in common fashion CRS datasets. Consequently, we propose two novel meta-user simulators that use the collected judgments and allow simulated users not only to express their preferences about alternative items to their original target, but also to change their mind and level of patience. In our experiments using the Shoes and Fashion IQ as the original datasets and three CRS models, we find that using the knowledge of alternatives by the simulator can have a considerable impact on the evaluation of existing CRS models, specifically that the existing single-target evaluation underestimates their effectiveness, and when simulatedusers are allowed to instead consider alternative relevant items, the system can rapidly respond to more quickly satisfy the user.', 'abstract_zh': '在对话式推荐系统中的Fashion-AlterEval：一种包含替代物品人类判断的新数据集及其应用', 'title_zh': 'Fashion-AlterEval：一种用于评估具有替代相关项的对话推荐系统的新数据集'}
{'arxiv_id': 'arXiv:2507.18009', 'title': 'GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures', 'authors': 'Jake R. Patock, Nicole Catherine Lewis, Kevin McCoy, Christina Gomez, Canling Chen, Lorenzo Luzi', 'link': 'https://arxiv.org/abs/2507.18009', 'abstract': "State-of-the-art (SOTA) image and text generation models are multimodal models that have many similarities to large language models (LLMs). Despite achieving strong performances, leading foundational multimodal model architectures frequently lag behind the architectural sophistication of contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner (CoCa) model that incorporates Gaussian error gated linear units, root mean squared normalization, and rotary positional embedding into the textual decoders and the vision transformer (ViT) encoder. Each architectural modification has been shown to improve model performance in LLMs, but has yet to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model with the same modified textual decoders but with CoCa's original ViT encoder. We used standard pretraining and fine-tuning workflows to benchmark the models on contrastive and generative tasks. Our GRR-CoCa significantly outperformed Baseline CoCa on the pretraining dataset and three diverse fine-tuning datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were 13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We show that GRR-CoCa's modified architecture improves performance and generalization across vision-language domains.", 'abstract_zh': '基于最新进展的对比caption生成模型GRR-CoCa：改进的多模态模型在文本解码器和视觉变换器中的应用', 'title_zh': 'GRR-CoCa: 利用大规模语言模型机制构建多模态模型架构'}
{'arxiv_id': 'arXiv:2507.17985', 'title': 'Decoding Instructional Dialogue: Human-AI Collaborative Analysis of Teacher Use of AI Tool at Scale', 'authors': 'Alex Liu, Lief Esbenshade, Shawon Sarkar, Victor Tian, Zachary Zhang, Kevin He, Min Sun', 'link': 'https://arxiv.org/abs/2507.17985', 'abstract': "The integration of large language models (LLMs) into educational tools has the potential to substantially impact how teachers plan instruction, support diverse learners, and engage in professional reflection. Yet little is known about how educators actually use these tools in practice and how their interactions with AI can be meaningfully studied at scale. This paper presents a human-AI collaborative methodology for large-scale qualitative analysis of over 140,000 educator-AI messages drawn from a generative AI platform used by K-12 teachers. Through a four-phase coding pipeline, we combined inductive theme discovery, codebook development, structured annotation, and model benchmarking to examine patterns of educator engagement and evaluate the performance of LLMs in qualitative coding tasks. We developed a hierarchical codebook aligned with established teacher evaluation frameworks, capturing educators' instructional goals, contextual needs, and pedagogical strategies. Our findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably support theme identification, extend human recognition in complex scenarios, and outperform open-weight models in both accuracy and structural reliability. The analysis also reveals substantive patterns in how educators inquire AI to enhance instructional practices (79.7 percent of total conversations), create or adapt content (76.1 percent), support assessment and feedback loop (46.9 percent), attend to student needs for tailored instruction (43.3 percent), and assist other professional responsibilities (34.2 percent), highlighting emerging AI-related competencies that have direct implications for teacher preparation and professional development. This study offers a scalable, transparent model for AI-augmented qualitative research and provides foundational insights into the evolving role of generative AI in educational practice.", 'abstract_zh': '大型语言模型（LLMs）与教育工具的整合有可能显著影响教师的课程规划、支持多元学习者以及专业反思的方式。然而，关于教育工作者在实际中如何使用这些工具以及如何大规模研究其与AI的交互知之甚少。本文提出了一种人机协作方法论，用于对来自K-12教师使用的生成型AI平台的超过140,000条教育者-AI消息进行大规模定性分析。通过四阶段编码管道，我们结合了归纳主题发现、编码手册开发、结构化注释和模型基准测试，以考察教育者参与的模式并评估LLMs在定性编码任务中的表现。我们开发了一个与公认的教师评价框架相契合的分级编码手册，捕捉了教育者的教学目标、情境需求和教学策略。我们的研究结果表明，特别是Claude 3.5 Haiku，LLMs能够可靠地支持主题识别，扩展人在复杂情境下的识别能力，并在准确性与结构性可靠性方面优于未加权模型。分析还揭示了教育者如何通过AI增强教学实践（占总对话的79.7%）、创编或改编内容（76.1%）、支持评估和反馈循环（46.9%）、关注个性化教学以满足学生需求（43.3%），以及协助其他专业职责（34.2%）等实质性模式，强调了新兴的与AI相关的专业能力，这些能力对教师培养和专业发展具有直接影响。本研究提供了一种可扩展且透明的模型，用于增强型AI辅助的定性研究，并提供了关于生成型AI在教育实践中的演变角色的基础性见解。', 'title_zh': '解码教学对话：大规模分析教师使用AI工具的人机协作研究'}
{'arxiv_id': 'arXiv:2507.17984', 'title': 'Machine Unlearning of Traffic State Estimation and Prediction', 'authors': 'Xin Wang, R. Tyrrell Rockafellar, Xuegang', 'link': 'https://arxiv.org/abs/2507.17984', 'abstract': 'Data-driven traffic state estimation and prediction (TSEP) relies heavily on data sources that contain sensitive information. While the abundance of data has fueled significant breakthroughs, particularly in machine learning-based methods, it also raises concerns regarding privacy, cybersecurity, and data freshness. These issues can erode public trust in intelligent transportation systems. Recently, regulations have introduced the "right to be forgotten", allowing users to request the removal of their private data from models. As machine learning models can remember old data, simply removing it from back-end databases is insufficient in such systems. To address these challenges, this study introduces a novel learning paradigm for TSEP-Machine Unlearning TSEP-which enables a trained TSEP model to selectively forget privacy-sensitive, poisoned, or outdated data. By empowering models to "unlearn," we aim to enhance the trustworthiness and reliability of data-driven traffic TSEP.', 'abstract_zh': '基于数据的交通状态估计与预测（TSEP）高度依赖包含敏感信息的数据源。虽然数据的丰富性促进了机器学习方法的重大突破，但也引发了隐私、网络安全和数据新鲜度方面的担忧。这些问题可能侵蚀公众对智能交通系统的信任。最近，法规引入了“被遗忘权”，允许用户要求从模型中删除其私人数据。由于机器学习模型会记住旧数据，仅从后端数据库中删除是不够的。为应对这些挑战，本研究引入了一种新的学习范式——TSEP机器遗忘，使得训练好的TSEP模型能够有选择地遗忘敏感、被污染或过时的数据。通过使模型能够“重新学习”，我们旨在提升基于数据的交通TSEP的信任度和可靠性。', 'title_zh': '交通状态估计与预测的机器遗忘技术'}
{'arxiv_id': 'arXiv:2507.17978', 'title': 'MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection', 'authors': 'Paulo Mendes, Eva Maia, Isabel Praça', 'link': 'https://arxiv.org/abs/2507.17978', 'abstract': "Phishing emails continue to pose a significant threat to cybersecurity by exploiting human vulnerabilities through deceptive content and malicious payloads. While Machine Learning (ML) models are effective at detecting phishing threats, their performance largely relies on the quality and diversity of the training data. This paper presents MeAJOR (Merged email Assets from Joint Open-source Repositories) Corpus, a novel, multi-source phishing email dataset designed to overcome critical limitations in existing resources. It integrates 135894 samples representing a broad number of phishing tactics and legitimate emails, with a wide spectrum of engineered features. We evaluated the dataset's utility for phishing detection research through systematic experiments with four classification models (RF, XGB, MLP, and CNN) across multiple feature configurations. Results highlight the dataset's effectiveness, achieving 98.34% F1 with XGB. By integrating broad features from multiple categories, our dataset provides a reusable and consistent resource, while addressing common challenges like class imbalance, generalisability and reproducibility.", 'abstract_zh': '钓鱼邮件继续通过诱骗内容和恶意载荷利用人类漏洞对网络安全构成重大威胁。尽管机器学习模型在检测钓鱼威胁方面非常有效，但其性能很大程度上取决于训练数据的质量和多样性。本文介绍了MeAJOR（合并的联合开源仓库电子邮件资产）语料库，这是一个新颖的多源钓鱼邮件数据集，旨在克服现有资源的关键限制。该数据集整合了135,894个样本，涵盖了广泛的钓鱼策略和合法邮件，并具有广泛的工程特征。我们通过在多个特征配置下使用四种分类模型（RF、XGB、MLP和CNN）进行系统实验，评估了数据集在钓鱼检测研究中的实用性。结果显示，该数据集非常有效，XGB达到了98.34%的F1值。通过整合多个类别中的广泛特征，我们的数据集提供了一个可重复使用的规范资源，同时解决了常见的挑战，如类别不平衡、泛化能力和可重复性问题。', 'title_zh': 'MeAJOR语料库：诈骗邮件检测的多源数据集'}
{'arxiv_id': 'arXiv:2507.17977', 'title': 'Improving the Computational Efficiency and Explainability of GeoAggregator', 'authors': 'Rui Deng, Ziqi Li, Mingshu Wang', 'link': 'https://arxiv.org/abs/2507.17977', 'abstract': 'Accurate modeling and explaining geospatial tabular data (GTD) are critical for understanding geospatial phenomena and their underlying processes. Recent work has proposed a novel transformer-based deep learning model named GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms other statistical and machine learning approaches. In this short paper, we further improve GA by 1) developing an optimized pipeline that accelerates the dataloading process and streamlines the forward pass of GA to achieve better computational efficiency; and 2) incorporating a model ensembling strategy and a post-hoc model explanation function based on the GeoShapley framework to enhance model explainability. We validate the functionality and efficiency of the proposed strategies by applying the improved GA model to synthetic datasets. Experimental results show that our implementation improves the prediction accuracy and inference speed of GA compared to the original implementation. Moreover, explanation experiments indicate that GA can effectively captures the inherent spatial effects in the designed synthetic dataset. The complete pipeline has been made publicly available for community use (this https URL).', 'abstract_zh': '准确建模和解释地理空间表型数据（GTD）对于理解地理空间现象及其 underlying 过程至关重要。近期工作提出了一种基于变压器的深度学习模型 GeoAggregator (GA) 用于此目的，并证明其优于其他统计和机器学习方法。在本文中，我们进一步改进了 GA，通过 1) 开发一个优化的工作流程以加速数据加载过程并简化 GA 的前向传播，从而提高计算效率；以及 2) 结合基于 GeoShapley 架构的模型集成策略和后验模型解释功能，以增强模型解释性。我们通过将改进后的 GA 应用于合成数据集来验证所提出策略的功能性和效率。实验结果表明，我们的实现相比原始实现提高了 GA 的预测准确性和推理速度。此外，解释实验表明 GA 能够有效捕捉设计的合成数据集中的固有空间效应。完整的管道已公开供社区使用（请参阅此链接）。', 'title_zh': '提高GeoAggregator的计算效率和可解释性'}
{'arxiv_id': 'arXiv:2507.17974', 'title': 'Natural Language Processing for Tigrinya: Current State and Future Directions', 'authors': 'Fitsum Gaim, Jong C. Park', 'link': 'https://arxiv.org/abs/2507.17974', 'abstract': "Despite being spoken by millions of people, Tigrinya remains severely underrepresented in Natural Language Processing (NLP) research. This work presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40 studies spanning more than a decade of work from 2011 to 2025. We systematically review the current state of computational resources, models, and applications across ten distinct downstream tasks, including morphological processing, machine translation, speech recognition, and question-answering. Our analysis reveals a clear trajectory from foundational, rule-based systems to modern neural architectures, with progress consistently unlocked by resource creation milestones. We identify key challenges rooted in Tigrinya's morphological complexity and resource scarcity, while highlighting promising research directions, including morphology-aware modeling, cross-lingual transfer, and community-centered resource development. This work serves as both a comprehensive reference for researchers and a roadmap for advancing Tigrinya NLP. A curated metadata of the surveyed studies and resources is made publicly available.\\footnote{Tigrinya NLP Anthology: this https URL.", 'abstract_zh': '尽管有数百万人使用提格雷尼亚语，但在自然语言处理（NLP）研究中，提格雷尼亚语仍严重缺失。本文提供了一篇关于提格雷尼亚语NLP研究的综合调查，分析了从2011年到2025年超过10年时间跨度内的超过40项研究。我们系统地回顾了在这十年间跨十个不同下游任务（包括形态学处理、机器翻译、语音识别和问答）中计算资源、模型和应用的现状。我们的分析揭示了从基于规则的基础系统向现代神经架构的明确发展轨迹，进展的一贯实现基于资源创建的重要里程碑。我们指出了根源在于提格雷尼亚语复杂形态和资源稀缺性的关键挑战，同时强调了具有前景的研究方向，包括形态意识建模、跨语言迁移和以社区为中心的资源开发。本文既是研究人员的全面参考，又是推进提格雷尼亚语NLP的路线图。被调查的研究和资源的元数据已公开发布。', 'title_zh': 'Tigre语自然语言处理：当前状态与未来方向'}
{'arxiv_id': 'arXiv:2507.17958', 'title': 'VIBE: Video-Input Brain Encoder for fMRI Response Modeling', 'authors': 'Daniel Carlstrom Schad, Shrey Dixit, Janis Keck, Viktor Studenyak, Aleksandr Shpilevoi, Andrej Bicanski', 'link': 'https://arxiv.org/abs/2507.17958', 'abstract': 'We present VIBE, a two-stage Transformer that fuses multi-modal video, audio, and text features to predict fMRI activity. Representations from open-source models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a modality-fusion transformer and temporally decoded by a prediction transformer with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson correlations of 32.25 on in-distribution Friends S07 and 21.25 on six out-of-distribution films. An earlier iteration of the same architecture obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second overall in the Algonauts 2025 Challenge.', 'abstract_zh': 'VIBE：一种融合多模态视频、音频和文本特征的两阶段Transformer，用于预测fMRI活动', 'title_zh': '视频输入脑编码器：针对fMRI响应建模'}
{'arxiv_id': 'arXiv:2507.17951', 'title': "Are LLM Belief Updates Consistent with Bayes' Theorem?", 'authors': 'Sohaib Imran, Ihor Kendiukhov, Matthew Broerman, Aditya Thomas, Riccardo Campanella, Rob Lamb, Peter M. Atkinson', 'link': 'https://arxiv.org/abs/2507.17951', 'abstract': 'Do larger and more capable language models learn to update their "beliefs" about propositions more consistently with Bayes\' theorem when presented with evidence in-context? To test this, we formulate a Bayesian Coherence Coefficient (BCC) metric and generate a dataset with which to measure the BCC. We measure BCC for multiple pre-trained-only language models across five model families, comparing against the number of model parameters, the amount of training data, and model scores on common benchmarks. Our results provide evidence for our hypothesis that larger and more capable pre-trained language models assign credences that are more coherent with Bayes\' theorem. These results have important implications for our understanding and governance of LLMs.', 'abstract_zh': '当面对上下文证据时，更大的且更为 capable 的语言模型是否能够更一致地根据 Bayes 定理更新其“信念”？为了验证这一点，我们制定了一个贝叶斯一致性系数（BCC）指标，并生成了一个数据集来测量 BCC。我们测量了五大家族多个仅预训练的语言模型的 BCC，并将其与模型参数量、训练数据量以及模型在常见基准上的得分进行比较。我们的结果证明了我们的假设：更大的且更为 capable 的预训练语言模型赋予的信念与 Bayes 定理更为一致。这些结果对于理解和治理大语言模型具有重要意义。', 'title_zh': 'LLM信念更新是否符合贝叶斯定理？'}
{'arxiv_id': 'arXiv:2507.17948', 'title': 'VERIRAG: Healthcare Claim Verification via Statistical Audit in Retrieval-Augmented Generation', 'authors': 'Shubham Mohole, Hongjun Choi, Shusen Liu, Christine Klymko, Shashank Kushwaha, Derek Shi, Wesam Sakla, Sainyam Galhotra, Ruben Glatt', 'link': 'https://arxiv.org/abs/2507.17948', 'abstract': 'Retrieval-augmented generation (RAG) systems are increasingly adopted in clinical decision support, yet they remain methodologically blind-they retrieve evidence but cannot vet its scientific quality. A paper claiming "Antioxidant proteins decreased after alloferon treatment" and a rigorous multi-laboratory replication study will be treated as equally credible, even if the former lacked scientific rigor or was even retracted. To address this challenge, we introduce VERIRAG, a framework that makes three notable contributions: (i) the Veritable, an 11-point checklist that evaluates each source for methodological rigor, including data integrity and statistical validity; (ii) a Hard-to-Vary (HV) Score, a quantitative aggregator that weights evidence by its quality and diversity; and (iii) a Dynamic Acceptance Threshold, which calibrates the required evidence based on how extraordinary a claim is. Across four datasets-comprising retracted, conflicting, comprehensive, and settled science corpora-the VERIRAG approach consistently outperforms all baselines, achieving absolute F1 scores ranging from 0.53 to 0.65, representing a 10 to 14 point improvement over the next-best method in each respective dataset. We will release all materials necessary for reproducing our results.', 'abstract_zh': '增强检索生成（RAG）系统在临床决策支持中的应用：VERIRAG框架的方法学评估与证据综合', 'title_zh': 'VERIRAG: 医疗索赔验证中的检索增强生成统计审计'}
{'arxiv_id': 'arXiv:2507.17944', 'title': 'Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text', 'authors': 'Hulayyil Alshammari, Praveen Rao', 'link': 'https://arxiv.org/abs/2507.17944', 'abstract': "Large language models (LLMs) have rapidly transformed the creation of written materials. LLMs have led to questions about writing integrity, thereby driving the creation of artificial intelligence (AI) detection technologies. Adversarial attacks, such as standard and humanized paraphrasing, inhibit detectors' ability to detect machine-generated text. Previous studies have mainly focused on ChatGPT and other well-known LLMs and have shown varying accuracy across detectors. However, there is a clear gap in the literature about DeepSeek, a recently published LLM. Therefore, in this work, we investigate whether six generally accessible AI detection tools -- AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can consistently recognize text generated by DeepSeek. The detectors were exposed to the aforementioned adversarial attacks. We also considered DeepSeek as a detector by performing few-shot prompting and chain-of-thought reasoning (CoT) for classifying AI and human-written text. We collected 49 human-authored question-answer pairs from before the LLM era and generated matching responses using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied adversarial techniques such as paraphrasing and humanizing to add 196 more samples. These were used to challenge detector robustness and assess accuracy impact. While QuillBot and Copyleaks showed near-perfect performance on original and paraphrased DeepSeek text, others -- particularly AI Text Classifier and GPT-2 -- showed inconsistent results. The most effective attack was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and 52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best five-shot result misclassifying only one of 49 samples (AI recall 96%, human recall 100%).", 'abstract_zh': '大规模语言模型（LLMs）迅速改变了书面材料的创作方式。LLMs引发了关于写作完整性的质疑，从而促进了人工智能（AI）检测技术的发展。对抗性攻击，如标准和人性化改写，削弱了检测器检测机器生成文本的能力。先前的研究主要集中在ChatGPT和其他知名的语言模型上，并且在检测器的准确性方面显示出差异。然而，关于最近发布的DeepSeek，文献中明显缺乏研究。因此，在本文中，我们调查了六个通用的AI检测工具——AI Text Classifier、Content Detector AI、Copyleaks、QuillBot、GPT-2和GPTZero，看它们能否一致地识别由DeepSeek生成的文本。这些检测器受到了上述对抗性攻击的挑战。我们还将DeepSeek作为检测器，通过少量提示和链式思考（CoT）来进行分类。我们收集了49个来自LLM时代前的人类撰写的问答对，并使用DeepSeek-v3生成了匹配的答案，生成了49个机器生成的样本。然后，我们应用了改写和人性化等对抗性技术，增加了196个额外样本。这些样本被用来挑战检测器的鲁棒性，并评估其准确率的影响。尽管QuillBot和Copyleaks在原始和改写后的DeepSeek文本上表现出接近完美的性能，但其他检测器，特别是AI Text Classifier和GPT-2，显示出不一致的结果。最有效的攻击是人性化，这将Copyleaks、QuillBot和GPTZero的准确性分别降低到71%、58%和52%。少量提示和CoT提示显示了较高的准确性，五个样本中只有一个被错误分类（AI召回率96%，人类召回率100%）。', 'title_zh': '评估AI文本检测器、few-shot提示和Chain-of-Thought提示性能：基于DeepSeek生成的文本'}
{'arxiv_id': 'arXiv:2507.17942', 'title': 'Minimax Data Sanitization with Distortion Constraint and Adversarial Inference', 'authors': 'Amirarsalan Moatazedian, Yauhen Yakimenka, Rémi A. Chou, Jörg Kliewer', 'link': 'https://arxiv.org/abs/2507.17942', 'abstract': 'We study a privacy-preserving data-sharing setting where a privatizer transforms private data into a sanitized version observed by an authorized reconstructor and two unauthorized adversaries, each with access to side information correlated with the private data.\nThe reconstructor is evaluated under a distortion function, while each adversary is evaluated using a separate loss function. The privatizer ensures the reconstructor distortion remains below a fixed threshold while maximizing the minimum loss across the two adversaries. This two-adversary setting models cases where individual users cannot reconstruct the data accurately, but their combined side information enables estimation within the distortion threshold. The privatizer maximizes individual loss while permitting accurate reconstruction only through collaboration. This echoes secret-sharing principles, but with lossy rather than perfect recovery. We frame this as a constrained data-driven minimax optimization problem and propose a data-driven training procedure that alternately updates the privatizer, reconstructor, and adversaries. We also analyze the Gaussian and binary cases as special scenarios where optimal solutions can be obtained. These theoretical optimal results are benchmarks for evaluating the proposed minimax training approach.', 'abstract_zh': '我们研究一种保护隐私的数据共享设置，其中，私有化器将隐私数据转换为受授权重构者和两个未经授权的对手观察的清洗版本，每个对手都可访问与隐私数据相关联的侧信息。重构者在失真函数下进行评估，而每个对手则使用单独的损失函数进行评估。私有化器确保重构者的失真不超过固定阈值，同时最大化两个对手中最小的损失。这种双对手设置模拟了个体用户无法准确重构数据的情况，但结合的侧信息能够在失真阈值内进行估计。私有化器在允许通过合作进行准确重构的前提下，最大化个体损失。这类似于秘密共享原则，但允许损失而非完美的恢复。我们将此问题表述为受限的数据驱动最小最大优化问题，并提出了一种交替更新私有化器、重构者和对手的数据驱动训练程序。我们还分析了高斯和二进制情况作为特殊场景，其中可以得到最优解。这些理论最优结果是评估所提出的最小最大训练方法的基准。', 'title_zh': '最小最大数据脱敏与失真约束及对抗推断'}
{'arxiv_id': 'arXiv:2507.17937', 'title': "Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation", 'authors': 'Jaechul Roh, Zachary Novack, Yuefeng Peng, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Amir Houmansadr', 'link': 'https://arxiv.org/abs/2507.17937', 'abstract': 'Lyrics-to-Song (LS2) generation models promise end-to-end music synthesis from text, yet their vulnerability to training data memorization remains underexplored. We introduce Adversarial PhoneTic Prompting (APT), a novel attack where lyrics are semantically altered while preserving their acoustic structure through homophonic substitutions (e.g., Eminem\'s famous "mom\'s spaghetti" $\\rightarrow$ "Bob\'s confetti"). Despite these distortions, we uncover a powerful form of sub-lexical memorization: models like SUNO and YuE regenerate outputs strikingly similar to known training content, achieving high similarity across audio-domain metrics, including CLAP, AudioJudge, and CoverID. This vulnerability persists across multiple languages and genres. More surprisingly, we discover that phoneme-altered lyrics alone can trigger visual memorization in text-to-video models. When prompted with phonetically modified lyrics from Lose Yourself, Veo 3 reconstructs visual elements from the original music video -- including character appearance and scene composition -- despite no visual cues in the prompt. We term this phenomenon phonetic-to-visual regurgitation. Together, these findings expose a critical vulnerability in transcript-conditioned multimodal generation: phonetic prompting alone can unlock memorized audiovisual content, raising urgent questions about copyright, safety, and content provenance in modern generative systems. Example generations are available on our demo page (this http URL).', 'abstract_zh': 'Lyrics-to-Song (LS2)生成模型 promise 从文本到端音乐合成，但其在训练数据记忆化方面的脆弱性仍需进一步探索。我们介绍了对抗音素提示 (Adversarial PhoneTic Prompting, APT)，这是一种新颖的攻击方式，通过同音替换改变歌词的语义同时保持其声学结构（例如， Eminem 著名的 "mom\'s spaghetti" $\\rightarrow$ "Bob\'s confetti"）。尽管存在这些扭曲，我们发现了一种强大的次词层记忆形式：如 SUNO 和 YuE 这样的模型会生成与已知训练内容惊人相似的输出，这些输出在包括 CLAP、AudioJudge 和 CoverID 的音频域度量标准上实现了高相似度。这种脆弱性跨越了多种语言和流派。更为令人惊讶的是，我们发现仅仅通过音素修改的歌词就可以触发文本到视频模型中的视觉记忆。当使用《Lose Yourself》中音素修改的歌词提示时，Veo 3 重构了原始音乐视频中的视觉元素，包括角色外观和场景组成——尽管提示中没有任何视觉线索。我们将这一现象称为音素到视觉的回吐。这些发现揭示了转录条件下的多模态生成中一个关键的脆弱性：仅通过音素提示就可以解锁记忆中的音频-视觉内容，这引发了关于现代生成系统中版权、安全性和内容来源的紧急问题。示例生成可在我们的演示页面上获得。', 'title_zh': '鲍勃的彩纸：音乐和视频生成中的phonetic记忆攻击'}
{'arxiv_id': 'arXiv:2507.17934', 'title': 'Multimodal Fine-grained Reasoning for Post Quality Evaluation', 'authors': 'Xiaoxu Guo, Siyan Liang, Yachao Cui, Juxiang Zhou, Lei Wang, Han Cao', 'link': 'https://arxiv.org/abs/2507.17934', 'abstract': 'Accurately assessing post quality requires complex relational reasoning to capture nuanced topic-post relationships. However, existing studies face three major limitations: (1) treating the task as unimodal categorization, which fails to leverage multimodal cues and fine-grained quality distinctions; (2) introducing noise during deep multimodal fusion, leading to misleading signals; and (3) lacking the ability to capture complex semantic relationships like relevance and comprehensiveness. To address these issues, we propose the Multimodal Fine-grained Topic-post Relational Reasoning (MFTRR) framework, which mimics human cognitive processes. MFTRR reframes post-quality assessment as a ranking task and incorporates multimodal data to better capture quality variations. It consists of two key modules: (1) the Local-Global Semantic Correlation Reasoning Module, which models fine-grained semantic interactions between posts and topics at both local and global levels, enhanced by a maximum information fusion mechanism to suppress noise; and (2) the Multi-Level Evidential Relational Reasoning Module, which explores macro- and micro-level relational cues to strengthen evidence-based reasoning. We evaluate MFTRR on three newly constructed multimodal topic-post datasets and the public Lazada-Home dataset. Experimental results demonstrate that MFTRR significantly outperforms state-of-the-art baselines, achieving up to 9.52% NDCG@3 improvement over the best unimodal method on the Art History dataset.', 'abstract_zh': '准确评估帖子质量需要复杂的相关推理以捕捉细微的主题-帖子关系。然而，现有研究面临三个主要局限：（1）将任务视为单一模态分类，未能利用多模态线索和精细的质量区分；（2）在深度多模态融合过程中引入噪声，导致误导性信号；（3）缺乏捕捉相关性和全面性等复杂语义关系的能力。为解决这些问题，我们提出了一种多模态细粒度主题-帖子关系推理（MFTRR）框架，该框架模仿人类的认知过程。MFTRR将帖子质量评估重新定义为排序任务，并结合多模态数据以更好地捕捉质量变化。它包含两个关键模块：（1）局部-全局语义关系推理模块，该模块在局部和全局层面建模帖子与主题之间的细粒度语义交互，并通过最大信息融合机制抑制噪声；（2）多层次证据关系推理模块，该模块探索宏观和微观层次的关系线索以加强基于证据的推理。我们使用三个新构建的多模态主题-帖子数据集和公开的Lazada-Home数据集对MFTRR进行了评估。实验结果表明，MFTRR显著优于最先进的基线方法，在Art History数据集上实现了高达9.52%的NDCG@3改进。', 'title_zh': '多模态细粒度推理在帖子质量评估中的应用'}
{'arxiv_id': 'arXiv:2507.17924', 'title': 'UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction', 'authors': 'Hongrong Yang, Markus Schlaepfer', 'link': 'https://arxiv.org/abs/2507.17924', 'abstract': 'Accurate population flow prediction is essential for urban planning, transportation management, and public health. Yet existing methods face key limitations: traditional models rely on static spatial assumptions, deep learning models struggle with cross-city generalization, and Large Language Models (LLMs) incur high computational costs while failing to capture spatial structure. Moreover, many approaches sacrifice resolution by clustering Points of Interest (POIs) or restricting coverage to subregions, limiting their utility for city-wide analytics. We introduce UrbanPulse, a scalable deep learning framework that delivers ultra-fine-grained, city-wide OD flow predictions by treating each POI as an individual node. It combines a temporal graph convolutional encoder with a transformer-based decoder to model multi-scale spatiotemporal dependencies. To ensure robust generalization across urban contexts, UrbanPulse employs a three-stage transfer learning strategy: pretraining on large-scale urban graphs, cold-start adaptation, and reinforcement learning this http URL on over 103 million cleaned GPS records from three metropolitan areas in California, UrbanPulse achieves state-of-the-art accuracy and scalability. Through efficient transfer learning, UrbanPulse takes a key step toward making high-resolution, AI-powered urban forecasting deployable in practice across diverse cities.', 'abstract_zh': '准确的人员流动预测对于城市规划、交通管理和公共卫生至关重要。然而，现有方法面临关键限制：传统模型依赖静态空间假设，深度学习模型在跨城市泛化方面挣扎，而大型语言模型在计算成本高且难以捕捉空间结构。此外，许多方法通过聚类兴趣点（POIs）或限制覆盖范围以子区域来牺牲分辨率，限制了它们在全市尺度分析中的应用价值。我们提出UrbanPulse，一种可扩展的深度学习框架，通过将每个POI视为单独节点来实现全市尺度上超细粒度的OD流预测。UrbanPulse结合了时变图卷积编码器和基于变换器的解码器，以建模多尺度时空依赖关系。为了确保在城市上下文中的稳健泛化，UrbanPulse采用三阶段迁移学习策略：大规模城市图上的预训练、冷启动适应以及基于此的强化学习。UrbanPulse在超过10300万条来自加利福尼亚三个大都市区的清洁GPS记录上进行训练，实现了最先进的准确性和可扩展性。通过高效的迁移学习，UrbanPulse朝着使高分辨率的AI驱动城市预报在各种城市中可践行迈出了一步。', 'title_zh': 'UrbanPulse: 一种跨城市深度学习框架，用于超精细人口流动预测'}
{'arxiv_id': 'arXiv:2507.17922', 'title': 'From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models', 'authors': 'Jessica Quaye, Charvi Rastogi, Alicia Parrish, Oana Inel, Minsuk Kahng, Lora Aroyo, Vijay Janapa Reddi', 'link': 'https://arxiv.org/abs/2507.17922', 'abstract': 'Text-to-image (T2I) models have become prevalent across numerous applications, making their robust evaluation against adversarial attacks a critical priority. Continuous access to new and challenging adversarial prompts across diverse domains is essential for stress-testing these models for resilience against novel attacks from multiple vectors. Current techniques for generating such prompts are either entirely authored by humans or synthetically generated. On the one hand, datasets of human-crafted adversarial prompts are often too small in size and imbalanced in their cultural and contextual representation. On the other hand, datasets of synthetically-generated prompts achieve scale, but typically lack the realistic nuances and creative adversarial strategies found in human-crafted prompts. To combine the strengths of both human and machine approaches, we propose Seed2Harvest, a hybrid red-teaming method for guided expansion of culturally diverse, human-crafted adversarial prompt seeds. The resulting prompts preserve the characteristics and attack patterns of human prompts while maintaining comparable average attack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded dataset achieves substantially higher diversity with 535 unique geographic locations and a Shannon entropy of 7.48, compared to 58 locations and 5.28 entropy in the original dataset. Our work demonstrates the importance of human-machine collaboration in leveraging human creativity and machine computational capacity to achieve comprehensive, scalable red-teaming for continuous T2I model safety evaluation.', 'abstract_zh': '基于文本到图像模型的对抗攻击稳健性评估：Seed2Harvest——一种混合红队方法iliaguided扩展文化多样化的手工crafted对抗提示种子', 'title_zh': '从种子到收获：利用AI增强人类创造力以测试文本到图像模型'}
{'arxiv_id': 'arXiv:2507.17907', 'title': 'Deep learning-aided inverse design of porous metamaterials', 'authors': 'Phu Thien Nguyen, Yousef Heider, Dennis M. Kochmann, Fadi Aldakheel', 'link': 'https://arxiv.org/abs/2507.17907', 'abstract': 'The ultimate aim of the study is to explore the inverse design of porous metamaterials using a deep learning-based generative framework. Specifically, we develop a property-variational autoencoder (pVAE), a variational autoencoder (VAE) augmented with a regressor, to generate structured metamaterials with tailored hydraulic properties, such as porosity and permeability. While this work uses the lattice Boltzmann method (LBM) to generate intrinsic permeability tensor data for limited porous microstructures, a convolutional neural network (CNN) is trained using a bottom-up approach to predict effective hydraulic properties. This significantly reduces the computational cost compared to direct LBM simulations. The pVAE framework is trained on two datasets: a synthetic dataset of artificial porous microstructures and CT-scan images of volume elements from real open-cell foams. The encoder-decoder architecture of the VAE captures key microstructural features, mapping them into a compact and interpretable latent space for efficient structure-property exploration. The study provides a detailed analysis and interpretation of the latent space, demonstrating its role in structure-property mapping, interpolation, and inverse design. This approach facilitates the generation of new metamaterials with desired properties. The datasets and codes used in this study will be made open-access to support further research.', 'abstract_zh': '基于深度学习的生成框架探索多孔超材料的逆向设计', 'title_zh': '深度学习辅助的多孔超材料逆设计'}
{'arxiv_id': 'arXiv:2507.17896', 'title': 'VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL', 'authors': 'Shubham Mohole, Sainyam Galhotra', 'link': 'https://arxiv.org/abs/2507.17896', 'abstract': 'Application systems using natural language interfaces to databases (NLIDBs) have democratized data analysis. This positive development has also brought forth an urgent challenge to help users who might use these systems without a background in statistical analysis to formulate bias-free analytical questions. Although significant research has focused on text-to-SQL generation accuracy, addressing cognitive biases in analytical questions remains underexplored. We present VeriMinder, this https URL, an interactive system for detecting and mitigating such analytical vulnerabilities. Our approach introduces three key innovations: (1) a contextual semantic mapping framework for biases relevant to specific analysis contexts (2) an analytical framework that operationalizes the Hard-to-Vary principle and guides users in systematic data analysis (3) an optimized LLM-powered system that generates high-quality, task-specific prompts using a structured process involving multiple candidates, critic feedback, and self-reflection.\nUser testing confirms the merits of our approach. In direct user experience evaluation, 82.5% participants reported positively impacting the quality of the analysis. In comparative evaluation, VeriMinder scored significantly higher than alternative approaches, at least 20% better when considered for metrics of the analysis\'s concreteness, comprehensiveness, and accuracy. Our system, implemented as a web application, is set to help users avoid "wrong question" vulnerability during data analysis. VeriMinder code base with prompts, this https URL, is available as an MIT-licensed open-source software to facilitate further research and adoption within the community.', 'abstract_zh': '基于自然语言界面的数据库应用系统（NLIDBs）已使数据剖析民主化。这一积极的发展也带来了一个紧迫的挑战，即帮助那些可能未接受过统计分析背景教育的用户，提出无偏见的分析问题。尽管对文本到SQL生成的准确性进行了大量研究，但在解决分析问题中的认知偏见方面仍然研究不足。我们提出VeriMinder，这是一个用于检测和缓解此类分析漏洞的交互式系统。我们的方法引入了三项关键技术创新：（1）与特定分析上下文相关的偏见上下文语义映射框架；（2）将难以变化原则操作化的分析框架，并引导用户进行系统数据分析；（3）优化的大规模语言模型驱动系统，使用结构化流程生成高质量、特定任务的提示，该流程涉及多个候选人、批评反馈和自我反思。用户测试证实了我们方法的优势。在直接用户体验评估中，82.5%的参与者报告说提高了分析的质量。在比较评估中，VeriMinder在分析具体性、全面性和准确性等指标上显著优于其他方法，至少提高了20%。我们的系统作为网络应用实现，旨在帮助用户避免数据分析中的“错误问题”漏洞。VeriMinder代码库和提示，已作为MIT许可的开源软件发布，以促进社区内的进一步研究和采用。', 'title_zh': 'VeriMinder: 消除NL2SQL中的分析漏洞'}
{'arxiv_id': 'arXiv:2507.17893', 'title': 'Action-List Reinforcement Learning Syndrome Decoding for Binary Linear Block Codes', 'authors': 'Milad Taghipour, Bane Vasic', 'link': 'https://arxiv.org/abs/2507.17893', 'abstract': 'This paper explores the application of reinforcement learning techniques to enhance the performance of decoding of linear block codes based on flipping bits and finding optimal decisions. We describe the methodology for mapping the iterative decoding process into Markov Decision Processes (MDPs) and propose different methods to reduce the number of states in the MDP. A truncated MDP is proposed to reduce the number of states in the MDP by learning a Hamming ball with a specified radius around codewords. We then propose a general scheme for reinforcement learning based decoders applicable to any class of codes to improve the performance of decoders. We call this scheme an action-list decoding. We design an action-list decoder based on the Deep-Q network values that substantially enhance performance. We also get benefit of automorphism group of code to further improve the code performance. Additionally, we propose a feedback-based method to exploit and enhance the performance of existing high-performing decoders by applying reinforcement learning algorithms after the existing decoders. These approaches effectively reduces the complexity of the reinforcement learning block. Finally, we present experimental results for the Low-Density Parity Check (LDPC) codes over the Binary Symmetric Channel (BSC) to demonstrate the efficiency of the proposed methods.', 'abstract_zh': '基于强化学习技术提高翻比特线性分组码译码性能的方法研究', 'title_zh': '基于动作列表强化学习的二进制线性块码译码算法'}
{'arxiv_id': 'arXiv:2507.17860', 'title': 'Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis', 'authors': 'Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel', 'link': 'https://arxiv.org/abs/2507.17860', 'abstract': 'Recent advancements in Deep Learning and its application on the edge hold great potential for the revolution of routine screenings for skin cancers like Melanoma. Along with the anticipated benefits of this technology, potential dangers arise from unforseen and inherent biases. Thus, assessing and improving the fairness of such systems is of utmost importance. A key challenge in fairness assessment is to ensure that the evaluation dataset is sufficiently representative of different Personal Identifiable Information (PII) (sex, age, and race) and other minority groups. Against the backdrop of this challenge, this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT model to assess the fairness of publicly available melanoma classifiers. The results suggest that fairness assessment using highly realistic synthetic data is a promising direction. Yet, our findings indicate that verifying fairness becomes difficult when the melanoma-detection model used for evaluation is trained on data that differ from the dataset underpinning the synthetic images. Nonetheless, we propose that our approach offers a valuable new avenue for employing synthetic data to gauge and enhance fairness in medical-imaging GenAI systems.', 'abstract_zh': 'Recent advancements in深度学习及其在边缘的应用为皮肤癌如黑色素瘤的常规筛查革命带来了巨大潜力。然而，随着这项技术预期带来的益处，潜在的风险也来自未预见和固有的偏见。因此，评估和改进此类系统的公平性至关重要。公平性评估的关键挑战在于确保评估数据集充分代表不同个人可识别信息（如性别、年龄和种族）以及其他少数群体。面对这一挑战，本研究利用最先进的生成人工智能（GenAI）LightningDiT模型评估公开可用的黑色素瘤分类器的公平性。结果表明，使用高度真实的合成数据进行公平性评估是一个有前景的方向。然而，我们的研究发现，在评估的黑色素瘤检测模型所使用的训练数据与合成图像底层数据集不同时，验证公平性变得困难。尽管如此，我们提出我们的方法为使用合成数据评估和提升医疗成像GenAI系统的公平性提供了一个宝贵的新途径。', 'title_zh': '基于GenAI图像合成促进基于AI的皮肤病变分类器公平性评估'}
{'arxiv_id': 'arXiv:2507.17853', 'title': 'Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models', 'authors': 'Lifeng Chen, Jiner Wang, Zihao Pan, Beier Zhu, Xiaofeng Yang, Chi Zhang', 'link': 'https://arxiv.org/abs/2507.17853', 'abstract': 'Recent advances in text-to-image (T2I) generation have led to impressive visual results. However, these models still face significant challenges when handling complex prompt, particularly those involving multiple subjects with distinct attributes. Inspired by the human drawing process, which first outlines the composition and then incrementally adds details, we propose Detail++, a training-free framework that introduces a novel Progressive Detail Injection (PDI) strategy to address this limitation. Specifically, we decompose a complex prompt into a sequence of simplified sub-prompts, guiding the generation process in stages. This staged generation leverages the inherent layout-controlling capacity of self-attention to first ensure global composition, followed by precise refinement. To achieve accurate binding between attributes and corresponding subjects, we exploit cross-attention mechanisms and further introduce a Centroid Alignment Loss at test time to reduce binding noise and enhance attribute consistency. Extensive experiments on T2I-CompBench and a newly constructed style composition benchmark demonstrate that Detail++ significantly outperforms existing methods, particularly in scenarios involving multiple objects and complex stylistic conditions.', 'abstract_zh': 'Recent Advances in Text-to-Image Generation: Detail++ Framework for Handling Complex Prompts', 'title_zh': 'Detail++: 不需训练的文本到图像扩散模型细节增强器'}
{'arxiv_id': 'arXiv:2507.17852', 'title': 'Technical Implementation of Tippy: Multi-Agent Architecture and System Design for Drug Discovery Laboratory Automation', 'authors': 'Yao Fehlis, Charles Crain, Aidan Jensen, Michael Watson, James Juhasz, Paul Mandel, Betty Liu, Shawn Mahon, Daren Wilson, Nick Lynch-Jonely, Ben Leedom, David Fuller', 'link': 'https://arxiv.org/abs/2507.17852', 'abstract': "Building on the conceptual framework presented in our previous work on agentic AI for pharmaceutical research, this paper provides a comprehensive technical analysis of Tippy's multi-agent system implementation for drug discovery laboratory automation. We present a distributed microservices architecture featuring five specialized agents (Supervisor, Molecule, Lab, Analysis, and Report) that coordinate through OpenAI Agents SDK orchestration and access laboratory tools via the Model Context Protocol (MCP). The system architecture encompasses agent-specific tool integration, asynchronous communication patterns, and comprehensive configuration management through Git-based tracking. Our production deployment strategy utilizes Kubernetes container orchestration with Helm charts, Docker containerization, and CI/CD pipelines for automated testing and deployment. The implementation integrates vector databases for RAG functionality and employs an Envoy reverse proxy for secure external access. This work demonstrates how specialized AI agents can effectively coordinate complex laboratory workflows while maintaining security, scalability, reliability, and integration with existing laboratory infrastructure through standardized protocols.", 'abstract_zh': '基于我们在制药研究中提出的代理型AI概念框架，本文提供了Tippy多智能体系统在药物发现实验室自动化方面的全面技术分析。该系统采用分布式微服务架构，包含五个专业化智能体（Supervisor、Molecule、Lab、Analysis和Report），并通过OpenAI Agents SDK编排协调，借助Model Context Protocol（MCP）访问实验室工具。系统架构涵盖了智能体特定工具集成、异步通信模式和基于Git的配置管理。我们的生产部署策略使用Kubernetes容器编排、Helm图表、Docker容器化和CI/CD管道进行自动化测试和部署。实现中整合了向量数据库以支持RAG功能，并使用Envoymesh代理以安全方式访问外部资源。本工作展示了如何通过标准化协议使专业化AI智能体有效协调复杂的实验室工作流程，同时保持安全、可扩展性、可靠性和与现有实验室基础设施的集成。', 'title_zh': 'Tippy的技術實現：药物发现实验室自动化多agent架构与系统设计'}
{'arxiv_id': 'arXiv:2507.17850', 'title': 'Performance Evaluation and Threat Mitigation in Large-scale 5G Core Deployment', 'authors': 'Rodrigo Moreira, Larissa F. Rodrigues Moreira, Flávio de Oliveira Silva', 'link': 'https://arxiv.org/abs/2507.17850', 'abstract': 'The deployment of large-scale software-based 5G core functions presents significant challenges due to their reliance on optimized and intelligent resource provisioning for their services. Many studies have focused on analyzing the impact of resource allocation for complex deployments using mathematical models, queue theories, or even Artificial Intelligence (AI). This paper elucidates the effects of chaotic workloads, generated by Distributed Denial of Service (DDoS) on different Network Functions (NFs) on User Equipment registration performance. Our findings highlight the necessity of diverse resource profiles to ensure Service-Level Agreement (SLA) compliance in large-scale 5G core deployments. Additionally, our analysis of packet capture approaches demonstrates the potential of kernel-based monitoring for scalable security threat defense. Finally, our empirical evaluation provides insights into the effective deployment of 5G NFs in complex scenarios.', 'abstract_zh': '大规模软件基5G核心功能的部署因其服务依赖于优化和智能化的资源分配而面临显著挑战。许多研究专注于使用数学模型、队列理论或人工智能分析复杂部署中的资源分配影响。本文阐明了分布式拒绝服务（DDoS）生成的混沌工作负载对用户设备注册性能在不同网络功能（NFs）上的影响。我们的研究发现强调了在大规模5G核心部署中确保服务级别协议（SLA）合规性的需求。此外，我们对数据包捕获方法的分析表明基于内核的监控在可扩展的安全威胁防御中的潜力。最后，我们的实证评估提供了在复杂场景中有效部署5G NFs的见解。', 'title_zh': '大规模5G核心网部署的性能评估与威胁缓解'}
{'arxiv_id': 'arXiv:2507.17848', 'title': 'Explainable Graph Neural Networks via Structural Externalities', 'authors': 'Lijun Wu, Dong Hao, Zhiyi Fan', 'link': 'https://arxiv.org/abs/2507.17848', 'abstract': 'Graph Neural Networks (GNNs) have achieved outstanding performance across a wide range of graph-related tasks. However, their "black-box" nature poses significant challenges to their explainability, and existing methods often fail to effectively capture the intricate interaction patterns among nodes within the network. In this work, we propose a novel explainability framework, GraphEXT, which leverages cooperative game theory and the concept of social externalities. GraphEXT partitions graph nodes into coalitions, decomposing the original graph into independent subgraphs. By integrating graph structure as an externality and incorporating the Shapley value under externalities, GraphEXT quantifies node importance through their marginal contributions to GNN predictions as the nodes transition between coalitions. Unlike traditional Shapley value-based methods that primarily focus on node attributes, our GraphEXT places greater emphasis on the interactions among nodes and the impact of structural changes on GNN predictions. Experimental studies on both synthetic and real-world datasets show that GraphEXT outperforms existing baseline methods in terms of fidelity across diverse GNN architectures , significantly enhancing the explainability of GNN models.', 'abstract_zh': '基于合作博弈论和社交外部性概念的GraphEXT解释框架已经在多种图神经网络架构上展示了更高的保真度，显著增强了图神经网络模型的解释性。', 'title_zh': '具有结构性外部性的可解释图神经网络'}
{'arxiv_id': 'arXiv:2507.17845', 'title': 'Towards Robust Foundation Models for Digital Pathology', 'authors': 'Jonah Kömen, Edwin D. de Jong, Julius Hense, Hannah Marienwald, Jonas Dippel, Philip Naumann, Eric Marcus, Lukas Ruff, Maximilian Alber, Jonas Teuwen, Frederick Klauschen, Klaus-Robert Müller', 'link': 'https://arxiv.org/abs/2507.17845', 'abstract': 'Biomedical Foundation Models (FMs) are rapidly transforming AI-enabled healthcare research and entering clinical validation. However, their susceptibility to learning non-biological technical features -- including variations in surgical/endoscopic techniques, laboratory procedures, and scanner hardware -- poses risks for clinical deployment. We present the first systematic investigation of pathology FM robustness to non-biological features. Our work (i) introduces measures to quantify FM robustness, (ii) demonstrates the consequences of limited robustness, and (iii) proposes a framework for FM robustification to mitigate these issues. Specifically, we developed PathoROB, a robustness benchmark with three novel metrics, including the robustness index, and four datasets covering 28 biological classes from 34 medical centers. Our experiments reveal robustness deficits across all 20 evaluated FMs, and substantial robustness differences between them. We found that non-robust FM representations can cause major diagnostic downstream errors and clinical blunders that prevent safe clinical adoption. Using more robust FMs and post-hoc robustification considerably reduced (but did not yet eliminate) the risk of such errors. This work establishes that robustness evaluation is essential for validating pathology FMs before clinical adoption and demonstrates that future FM development must integrate robustness as a core design principle. PathoROB provides a blueprint for assessing robustness across biomedical domains, guiding FM improvement efforts towards more robust, representative, and clinically deployable AI systems that prioritize biological information over technical artifacts.', 'abstract_zh': '生物医学基础模型（FMs）正在迅速改变AI驱动的医疗保健研究，并进入临床验证阶段。然而，它们学习非生物学技术特征的能力——包括手术/内镜技术、实验室程序和扫描硬件的差异——增加了在临床部署中的风险。我们首次系统地调查了病理FM对非生物学特征的鲁棒性。我们的工作（i）引入了衡量FM鲁棒性的方法，（ii）展示了鲁棒性有限的后果，并（iii）提出了一个FM鲁棒化框架以减轻这些问题。具体而言，我们开发了PathoROB，一个包含三个新指标的鲁棒性基准，包括鲁棒性指数，以及涵盖34家医疗机构28个生物类别的四个数据集。我们的实验表明，在评估的20个FM中普遍存在鲁棒性不足，且它们之间的鲁棒性差异显著。我们发现，不鲁棒的FM表示可能导致重大诊断错误和临床失误，阻碍其安全的临床应用。使用更鲁棒的FM和事后鲁棒化显著减少了（但尚未完全消除）此类错误的风险。这项工作确立了在临床采用前评估病理FM鲁棒性的必要性，并证明了未来FM开发必须将鲁棒性作为核心设计原则。PathoROB为评估生物医学领域鲁棒性提供了蓝图，指导了FM改进努力，以实现更鲁棒、更具代表性和临床可部署的AI系统，优先考虑生物学信息而非技术特征。', 'title_zh': '面向数字病理学的稳健基础模型研究'}
{'arxiv_id': 'arXiv:2507.17844', 'title': 'SV3.3B: A Sports Video Understanding Model for Action Recognition', 'authors': 'Sai Varun Kodathala, Yashwanth Reddy Vutukoori, Rakesh Vunnam', 'link': 'https://arxiv.org/abs/2507.17844', 'abstract': 'This paper addresses the challenge of automated sports video analysis, which has traditionally been limited by computationally intensive models requiring server-side processing and lacking fine-grained understanding of athletic movements. Current approaches struggle to capture the nuanced biomechanical transitions essential for meaningful sports analysis, often missing critical phases like preparation, execution, and follow-through that occur within seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B parameter video understanding model that combines novel temporal motion difference sampling with self-supervised learning for efficient on-device deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction mechanism that intelligently identifies the 16 most representative frames from sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through mask-denoising objectives and an LLM decoder fine-tuned for sports action description generation. Evaluated on a subset of the NSVA basketball dataset, SV3.3B achieves superior performance across both traditional text generation metrics and sports-specific evaluation criteria, outperforming larger closed-source models including GPT-4o variants while maintaining significantly lower computational requirements. Our model demonstrates exceptional capability in generating technically detailed and analytically rich sports descriptions, achieving 29.2% improvement over GPT-4o in ground truth validation metrics, with substantial improvements in information density, action complexity, and measurement precision metrics essential for comprehensive athletic analysis. Model Available at this https URL.', 'abstract_zh': '本论文探讨了自动化体育视频分析的挑战，传统上由于计算密集型模型需要服务器端处理且缺乏对运动细节的精细理解而受到限制。当前的方法难以捕捉到有意义的体育分析所需的关键生物力学转换细节，往往忽略了如准备、执行和跟进等在几秒内发生的关键阶段。为了解决这些限制，我们介绍了SV3.3B，一个轻量级的3.3B参数视频理解模型，该模型结合了新型时间运动差异采样和自监督学习，以实现高效设备端部署。我们的方法采用基于DWT-VGG16-LDA的关键帧提取机制，从体育序列中智能地识别出最具代表性的16帧，随后通过掩码去噪目标预训练的V-DWT-JEPA2编码器和用于体育动作描述生成的LLM解码器的微调版本。在NSVA篮球数据集的一个子集上进行评估，SV3.3B在传统文本生成指标和专门的体育评估标准方面表现出色，优于包括GPT-4o变体在内的更大规模的封闭源模型，同时计算要求显著降低。我们的模型展示了在生成技术上详细和分析上丰富的体育描述方面的出色能力，在地面真实验证指标上相较于GPT-4o提升了29.2%，并在涉及全面运动员分析的信息密度、动作复杂性和测量精度指标方面取得了显著改进。模型可访问地址：this https URL', 'title_zh': 'SV3.3B: 一种用于动作识别的体育视频理解模型'}
{'arxiv_id': 'arXiv:2507.17791', 'title': 'Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data', 'authors': 'Eduardo Aguilar-Bejarano, Daniel Lea, Karthikeyan Sivakumar, Jimiama M. Mase, Reza Omidvar, Ruizhe Li, Troy Kettle, James Mitchell-White, Morgan R Alexander, David A Winkler, Grazziela Figueredo', 'link': 'https://arxiv.org/abs/2507.17791', 'abstract': 'Helix is an open-source, extensible, Python-based software framework to facilitate reproducible and interpretable machine learning workflows for tabular data. It addresses the growing need for transparent experimental data analytics provenance, ensuring that the entire analytical process -- including decisions around data transformation and methodological choices -- is documented, accessible, reproducible, and comprehensible to relevant stakeholders. The platform comprises modules for standardised data preprocessing, visualisation, machine learning model training, evaluation, interpretation, results inspection, and model prediction for unseen data. To further empower researchers without formal training in data science to derive meaningful and actionable insights, Helix features a user-friendly interface that enables the design of computational experiments, inspection of outcomes, including a novel interpretation approach to machine learning decisions using linguistic terms all within an integrated environment. Released under the MIT licence, Helix is accessible via GitHub and PyPI, supporting community-driven development and promoting adherence to the FAIR principles.', 'abstract_zh': 'Helix是一个开源可扩展的基于Python的软件框架，用于促进表格数据的可重现和可解释机器学习工作流。该框架解决了日益增长的透明实验数据分析源头需求，确保整个分析过程，包括数据转换和方法论选择的决策，都是记录在案、可访问、可重现和易于相关利益相关者理解的。该平台包括标准化数据预处理、可视化、机器学习模型训练、评估、解释、结果检查以及对未见数据的模型预测模块。为了进一步赋能没有正式数据科学训练的研究人员，获取有意义和可操作的洞察，Helix提供了一个用户友好的界面，允许设计计算实验、检查结果，包括一种新的基于语言术语的机器学习决策解释方法，均在一个集成环境中完成。Helix在MIT许可下发布，可通过GitHub和PyPI访问，支持社区驱动开发并促进遵守FAIR原则。', 'title_zh': 'Helix 1.0：一种用于表格科学数据可再现和可解释的机器学习的开源框架'}
{'arxiv_id': 'arXiv:2507.17788', 'title': 'Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking', 'authors': 'Ali Vardasbi, Gustavo Penha, Claudia Hauff, Hugues Bouchard', 'link': 'https://arxiv.org/abs/2507.17788', 'abstract': "When using LLMs to rank items based on given criteria, or evaluate answers, the order of candidate items can influence the model's final decision. This sensitivity to item positioning in a LLM's prompt is known as position bias. Prior research shows that this bias exists even in large models, though its severity varies across models and tasks. In addition to position bias, LLMs also exhibit varying degrees of low repetition consistency, where repeating the LLM call with the same candidate ordering can lead to different rankings. To address both inconsistencies, a common approach is to prompt the model multiple times with different candidate orderings and aggregate the results via majority voting. However, this repetition strategy, significantly increases computational costs. Extending prior findings, we observe that both the direction -- favoring either the earlier or later candidate in the prompt -- and magnitude of position bias across instances vary substantially, even within a single dataset. This observation highlights the need for a per-instance mitigation strategy. To this end, we introduce a dynamic early-stopping method that adaptively determines the number of repetitions required for each instance. Evaluating our approach across three LLMs of varying sizes and on two tasks, namely re-ranking and alignment, we demonstrate that transitioning to a dynamic repetition strategy reduces the number of LLM calls by an average of 81%, while preserving the accuracy. Furthermore, we propose a confidence-based adaptation to our early-stopping method, reducing LLM calls by an average of 87% compared to static repetition, with only a slight accuracy trade-off relative to our original early-stopping method.", 'abstract_zh': '使用LLM根据给定标准对项目进行排名或评估答案时，候选项项目的顺序会影响模型的最终决策。这种对LLM提示中候选项位置的敏感性称为位置偏置。先前的研究表明，这种偏置即使在大规模模型中也存在，但其严重程度因模型和任务而异。除了位置偏置外，LLM在重复一致性方面也表现出不同程度的低重复性，即使用相同的候选项顺序重复调用LLM可能导致不同的排名。为了应对这两种不一致性，一种常见方法是在不同候选项顺序下多次提示模型，并通过多数投票聚合结果。然而，这种重复策略显著增加了计算成本。在此基础上，我们观察到位置偏置的方向和强度在不同实例之间变化显著，即使在同一数据集中也是如此。这一观察强调了需要实例特定缓解策略的必要性。为此，我们引入了一种动态提前停止方法，该方法自适应地确定每个实例所需的重复次数。在三种不同规模的LLM上评估我们的方法，并应用于排名和对齐两项任务中，我们证明了切换到动态重复策略可以将LLM调用次数平均减少81%，同时保持准确性。此外，我们提出了一种基于置信度的提前停止方法的适应性改进，与静态重复相比，平均可将LLM调用次数减少87%，且相对于原始的提前停止方法仅略有准确性上的权衡。', 'title_zh': '基于LLM的排名中缓解位置偏见的自适应重复方法'}
{'arxiv_id': 'arXiv:2507.17787', 'title': 'Hyperbolic Deep Learning for Foundation Models: A Survey', 'authors': 'Neil He, Hiren Madhu, Ngoc Bui, Menglin Yang, Rex Ying', 'link': 'https://arxiv.org/abs/2507.17787', 'abstract': "Foundation models pre-trained on massive datasets, including large language models (LLMs), vision-language models (VLMs), and large multimodal models, have demonstrated remarkable success in diverse downstream tasks. However, recent studies have shown fundamental limitations of these models: (1) limited representational capacity, (2) lower adaptability, and (3) diminishing scalability. These shortcomings raise a critical question: is Euclidean geometry truly the optimal inductive bias for all foundation models, or could incorporating alternative geometric spaces enable models to better align with the intrinsic structure of real-world data and improve reasoning processes? Hyperbolic spaces, a class of non-Euclidean manifolds characterized by exponential volume growth with respect to distance, offer a mathematically grounded solution. These spaces enable low-distortion embeddings of hierarchical structures (e.g., trees, taxonomies) and power-law distributions with substantially fewer dimensions compared to Euclidean counterparts. Recent advances have leveraged these properties to enhance foundation models, including improving LLMs' complex reasoning ability, VLMs' zero-shot generalization, and cross-modal semantic alignment, while maintaining parameter efficiency. This paper provides a comprehensive review of hyperbolic neural networks and their recent development for foundation models. We further outline key challenges and research directions to advance the field.", 'abstract_zh': '基于大规模数据预训练的基础模型，包括大规模语言模型（LLMs）、多模态视觉-语言模型（VLMs）和大规模多模态模型，在多样化的下游任务中表现出显著的成功。然而，近期研究表明这些模型存在根本性的限制：（1）有限的表征能力，（2）较低的适应性，以及（3）减弱的可扩展性。这些不足引发了关键问题：欧几里得几何是否真的适用于所有基础模型的最佳归纳偏置，或者通过引入替代几何空间能否使模型更好地与现实世界数据的内在结构对齐并提高推理过程？双曲空间作为一类非欧几里得流形，在距离增长方面具有指数体积增长的特点，为这一问题提供了数学基础的解决方案。双曲空间能够以远少于欧几里得空间的维度低失真嵌入分层结构（如树、分类体系）及幂律分布，近期进展利用了这些特性来提升基础模型的表现，包括增强大规模语言模型的复杂推理能力、多模态视觉-语言模型的零样本泛化能力和跨模态语义对齐，同时保持了参数效率。本文提供了双曲神经网络及其在基础模型中最新进展的全面综述，并进一步概述了关键挑战和研究方向以推进该领域。', 'title_zh': 'hyperbolic深度学习在基础模型中的应用：一个综述'}
{'arxiv_id': 'arXiv:2507.17780', 'title': 'In Reverie Together: Ten Years of Mathematical Discovery with a Machine Collaborator', 'authors': 'Randy Davila, Boris Brimkov, Ryan Pepper', 'link': 'https://arxiv.org/abs/2507.17780', 'abstract': 'We present four open conjectures in graph theory generated by the automated conjecturing system \\texttt{TxGraffiti}. Each conjecture is concise, grounded in natural graph invariants, and empirically validated across hundreds of graphs. Despite extensive effort, these statements remain unresolved--defying both proof and counterexample. They are not only mathematical challenges but creative expressions--born of symbolic pattern recognition and mathematician-defined heuristics, refined through years of human dialogue, and now offered back to the community as collaborative artifacts. These conjectures invite not only formal proof, but also reflection on how machines can evoke wonder, spark curiosity, and contribute to the raw material of discovery. By highlighting these problems, we aim to inspire both human mathematicians and AI systems to engage with them--not only to solve them, but to reflect on what it means when machines participate meaningfully in the creative process of mathematical thought.', 'abstract_zh': '我们展示了由自动猜想系统\\texttt{TxGraffiti}生成的四个开放猜想。每个猜想简洁明了，基于自然图不变量，并在数百个图中得到经验验证。尽管付出大量努力，这些陈述仍未解决——既无法证明也无法找到反例。它们不仅是数学挑战，也是富有创造性的表达——源自符号模式识别和数学家定义的启发式方法，并通过多年的人机对话 refinement，现在作为协作成果回馈给社区。这些猜想不仅邀请正式证明，还引发了如何激发惊奇感、激发求知欲以及为发现的基础材料做贡献的思考。通过突出这些难题，我们旨在激励人类数学家和AI系统与之互动——不仅是为了解决它们，更是为了反思当机器以有意义的方式参与数学思考的创造过程时意味着什么。', 'title_zh': '与机器合作者共梦十年：数学发现之旅'}
{'arxiv_id': 'arXiv:2507.17778', 'title': 'An advanced AI driven database system', 'authors': 'M. Tedeschi, S. Rizwan, C. Shringi, V. Devram Chandgir, S. Belich', 'link': 'https://arxiv.org/abs/2507.17778', 'abstract': 'Contemporary database systems, while effective, suffer severe issues related to complexity and usability, especially among individuals who lack technical expertise but are unfamiliar with query languages like Structured Query Language (SQL). This paper presents a new database system supported by Artificial Intelligence (AI), which is intended to improve the management of data using natural language processing (NLP) - based intuitive interfaces, and automatic creation of structured queries and semi-structured data formats like yet another markup language (YAML), java script object notation (JSON), and application program interface (API) documentation. The system is intended to strengthen the potential of databases through the integration of Large Language Models (LLMs) and advanced machine learning algorithms. The integration is purposed to allow the automation of fundamental tasks such as data modeling, schema creation, query comprehension, and performance optimization. We present in this paper a system that aims to alleviate the main problems with current database technologies. It is meant to reduce the need for technical skills, manual tuning for better performance, and the potential for human error. The AI database employs generative schema inference and format selection to build its schema models and execution formats.', 'abstract_zh': '当代数据库系统虽然有效，但在复杂性和易用性方面存在严重问题，尤其是在缺乏技术背景但不熟悉查询语言（如结构化查询语言SQL）的个体中更为明显。本文提出一种以人工智能（AI）支持的新数据库系统，旨在通过基于自然语言处理（NLP）的直观界面和自动创建结构化查询及半结构化数据格式（如YAML、JSON和API文档），改进数据管理。该系统通过集成大规模语言模型（LLMs）和先进机器学习算法，旨在增强数据库的潜在能力，并允许自动化数据建模、模式创建、查询理解及性能优化等基本任务。本文提出了一种旨在缓解当前数据库技术主要问题的系统，旨在降低对技术技能的需求、手动调优以提高性能的需求以及人为错误的可能性。该AI数据库采用生成性模式推断和格式选择来构建其模式模型和执行格式。', 'title_zh': '一种先进的AI驱动数据库系统'}
{'arxiv_id': 'arXiv:2507.17776', 'title': 'Axiomatizing Rumsfeld Ignorance', 'authors': 'Jie Fan', 'link': 'https://arxiv.org/abs/2507.17776', 'abstract': "In a recent paper, Kit Fine presents some striking results concerning the logical properties of (first-order) ignorance, second-order ignorance and Rumsfeld ignorance. However, Rumsfeld ignorance is definable in terms of ignorance, which makes some existing results and the axiomatization problem trivial. A main reason is that the accessibility relations for the implicit knowledge operator contained in the packaged operators of ignorance and Rumsfeld ignorance are the same. In this work, we assume the two accessibility relations to be different so that one of them is an arbitrary subset of the other. This will avoid the definability issue and retain most of the previous validities. The main results are axiomatizations over various proper bi-frame classes. Finally we apply our framework to analyze Fine's results.", 'abstract_zh': '近期，Kit Fine发表了一篇论文，探讨了（一阶）无知、二阶无知和Rumsfeld无知的逻辑性质。然而，Rumsfeld无知可以通过无知来定义，这使得一些现有结果和公理化问题变得平凡。主要原因在于包含在无知和Rumsfeld无知打包操作符中的隐含知识操作符的可达关系相同。在本文中，我们假设这两种可达关系不同，使得其中之一是另一者的任意子集。这将避免定义问题并保留大多数之前的有效性。主要结果是对各种适当双框架类的公理化。最后，我们将该框架应用于分析Fine的结果。', 'title_zh': '公理化鲁斯福德无知'}
{'arxiv_id': 'arXiv:2507.17775', 'title': 'Comparison of Optimised Geometric Deep Learning Architectures, over Varying Toxicological Assay Data Environments', 'authors': 'Alexander D. Kalian, Lennart Otte, Jaewook Lee, Emilio Benfenati, Jean-Lou C.M. Dorne, Claire Potter, Olivia J. Osborne, Miao Guo, Christer Hogstrand', 'link': 'https://arxiv.org/abs/2507.17775', 'abstract': 'Geometric deep learning is an emerging technique in Artificial Intelligence (AI) driven cheminformatics, however the unique implications of different Graph Neural Network (GNN) architectures are poorly explored, for this space. This study compared performances of Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs) and Graph Isomorphism Networks (GINs), applied to 7 different toxicological assay datasets of varying data abundance and endpoint, to perform binary classification of assay activation. Following pre-processing of molecular graphs, enforcement of class-balance and stratification of all datasets across 5 folds, Bayesian optimisations were carried out, for each GNN applied to each assay dataset (resulting in 21 unique Bayesian optimisations). Optimised GNNs performed at Area Under the Curve (AUC) scores ranging from 0.728-0.849 (averaged across all folds), naturally varying between specific assays and GNNs. GINs were found to consistently outperform GCNs and GATs, for the top 5 of 7 most data-abundant toxicological assays. GATs however significantly outperformed over the remaining 2 most data-scarce assays. This indicates that GINs are a more optimal architecture for data-abundant environments, whereas GATs are a more optimal architecture for data-scarce environments. Subsequent analysis of the explored higher-dimensional hyperparameter spaces, as well as optimised hyperparameter states, found that GCNs and GATs reached measurably closer optimised states with each other, compared to GINs, further indicating the unique nature of GINs as a GNN algorithm.', 'abstract_zh': '几何深度学习是人工智能驱动计算化学中的新兴技术，然而不同图神经网络（GNN）架构的独特影响在该领域研究不足。本研究对比了图卷积网络（GCNs）、图注意网络（GATs）和图同构网络（GINs）在七个不同毒性 assay 数据集上的性能，这些数据集具有不同的数据量和终点，用于执行 assay 活性二分类。在分子图预处理、类平衡强制执行和所有数据集在5折中的分层后，为每个assay数据集应用的每个GNN进行了贝叶斯优化（总共21次贝叶斯优化）。优化后的GNN在所有折的曲线下面积（AUC）分数范围从0.728到0.849，特定assay和GNN之间自然有所不同。对于七个数据最丰富的毒性assay的前五种，GINs持续优于GCNs和GATs。然而，GATs在两个数据最稀缺的assay中显著优于其他模型。这表明，GINs在数据丰富的环境中更优，而GATs在数据稀缺的环境中更优。进一步分析探索的高维度超参数空间及其优化状态发现，GCNs和GATs达到的优化状态更接近，相较于GINs，这进一步表明GINs作为图神经网络算法的独特性质。', 'title_zh': '优化几何深度学习架构在不同毒理学检测数据环境中的比较'}
{'arxiv_id': 'arXiv:2507.17774', 'title': 'Human-AI Co-Creation: A Framework for Collaborative Design in Intelligent Systems', 'authors': 'Zhangqi Liu', 'link': 'https://arxiv.org/abs/2507.17774', 'abstract': 'As artificial intelligence (AI) continues to evolve from a back-end computational tool into an interactive, generative collaborator, its integration into early-stage design processes demands a rethinking of traditional workflows in human-centered design. This paper explores the emergent paradigm of human-AI co-creation, where AI is not merely used for automation or efficiency gains, but actively participates in ideation, visual conceptualization, and decision-making. Specifically, we investigate the use of large language models (LLMs) like GPT-4 and multimodal diffusion models such as Stable Diffusion as creative agents that engage designers in iterative cycles of proposal, critique, and revision.', 'abstract_zh': '随着人工智能（AI）从后台计算工具演变为互动式的生成性合作者，其在早期设计过程中的集成需要重新思考以人为本的设计传统工作流程。本文探讨了人机共生创造的新兴范式，在这种范式中，AI 不仅用于自动化或效率提升，还主动参与创意生成、视觉概念化和决策制定。具体而言，我们研究了使用大型语言模型（如 GPT-4）和多模态扩散模型（如 Stable Diffusion）作为创意代理，与设计师进行迭代的提案、评价和修订循环。', 'title_zh': '人类与人工智能协作创作：智能系统中合作设计的框架'}
{'arxiv_id': 'arXiv:2507.17772', 'title': 'Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments', 'authors': 'Ahmad Alhonainy, Praveen Rao', 'link': 'https://arxiv.org/abs/2507.17772', 'abstract': 'Federated Learning (FL) allows multiple distributed devices to jointly train a shared model without centralizing data, but communication cost remains a major bottleneck, especially in resource-constrained environments. This paper introduces caching strategies - FIFO, LRU, and Priority-Based - to reduce unnecessary model update transmissions. By selectively forwarding significant updates, our approach lowers bandwidth usage while maintaining model accuracy. Experiments on CIFAR-10 and medical datasets show reduced communication with minimal accuracy loss. Results confirm that intelligent caching improves scalability, memory efficiency, and supports reliable FL in edge IoT networks, making it practical for deployment in smart cities, healthcare, and other latency-sensitive applications.', 'abstract_zh': '联邦学习(Federated Learning)允许多个分布式的设备联合训练共享模型而无需集中数据，但通信成本仍然是主要瓶颈，尤其是在资源受限的环境中。本文介绍了使用FIFO、LRU和基于优先级的缓存策略来减少不必要的模型更新传输。通过选择性地转发重要的更新，我们的方法降低了带宽使用量的同时保持了模型准确性。实验结果表明，在CIFAR-10和医疗数据集上减少了通信量，并且几乎不影响准确性。结果证实，智能缓存提高了联邦学习的可扩展性、内存效率，并支持边缘物联网网络中的可靠联邦学习，使其在智能城市、医疗保健等对延迟敏感的应用中具有实际部署价值。', 'title_zh': '物联网环境中降低联邦学习通信成本的缓存技术'}
{'arxiv_id': 'arXiv:2507.17765', 'title': 'ASR-Guided Speaker-Role Diarization and Diarization-Guided ASR Decoding', 'authors': 'Arindam Ghosh, Mark Fuhs, Bongjun Kim, Anurag Chowdhury, Monika Woszczyna', 'link': 'https://arxiv.org/abs/2507.17765', 'abstract': "From an application standpoint, speaker-role diarization (RD), such as doctor vs. patient, host vs. guest, etc. is often more useful than traditional speaker diarization (SD), which assigns generic labels like speaker-1, speaker-2 etc. In the context of joint automatic speech recognition (ASR) + SD (who spoke what?), recent end-to-end models employ an auxiliary SD transducer, synchronized with the ASR transducer, to predict speakers per word. In this paper, we extend this framework to RD with three key contributions: (1) we simplify the training via forced alignment and cross-entropy loss instead of RNNT loss, (2) we show that word prediction and role prediction require different amounts of predictor's context, leading to separate task-specific predictors, unlike existing shared-predictor models, and (3) we propose a way to leverage RD posterior activity to influence ASR decoding and reduce small-word deletion errors.", 'abstract_zh': '从应用角度来看，说话人角色分割（RD），如医生 vs. 患者、主持人 vs. 客人等，通常比传统说话人分割（SD）更具有实用性，后者使用如speaker-1、speaker-2等通用标签。在联合自动语音识别（ASR）+ SD（谁说了什么？）的上下文中，最近的端到端模型通过同步的ASR转录机和辅助SD转录机来预测每词的说话人。在本文中，我们通过三种关键贡献将此框架扩展到RD：（1）我们通过强制对齐和交叉熵损失简化训练，而不是使用RNNT损失；（2）我们表明词预测和角色预测需要不同类型预测器的不同上下文，导致分离的任务特定预测器，不同于现有共享预测器模型；（3）我们提出了一种利用RD后验活动来影响ASR解码并减少短词删除错误的方法。', 'title_zh': 'ASR引导的说话人角色辨识与辨识引导的ASR解码'}
{'arxiv_id': 'arXiv:2507.17760', 'title': 'How Instructional Sequence and Personalized Support Impact Diagnostic Strategy Learning', 'authors': 'Fatma Betül Güreş, Tanya Nazaretsky, Bahar Radmehr, Martina Rau, Tanja Käser', 'link': 'https://arxiv.org/abs/2507.17760', 'abstract': 'Supporting students in developing effective diagnostic reasoning is a key challenge in various educational domains. Novices often struggle with cognitive biases such as premature closure and over-reliance on heuristics. Scenario-based learning (SBL) can address these challenges by offering realistic case experiences and iterative practice, but the optimal sequencing of instruction and problem-solving activities remains unclear. This study examines how personalized support can be incorporated into different instructional sequences and whether providing explicit diagnostic strategy instruction before (I-PS) or after problem-solving (PS-I) improves learning and its transfer. We employ a between-groups design in an online SBL environment called PharmaSim, which simulates real-world client interactions for pharmacy technician apprentices. Results indicate that while both instruction types are beneficial, PS-I leads to significantly higher performance in transfer tasks.', 'abstract_zh': '支持学生发展有效的诊断推理是各个教育领域的一项关键挑战。新手常受到认知偏差如过早封闭和过度依赖启发式的困扰。基于情景的学习可以通过提供现实案例经验和迭代实践来应对这些挑战，但最优的教学序列和问题解决活动的排列尚不明确。本研究探讨了不同教学序列中个性化支持的融入方式，并分析了在问题解决前（I-PS）还是问题解决后（PS-I）提供明确的诊断策略指导是否能更有效地促进学习及其迁移。我们使用一个名为PharmaSim的在线基于情景的学习环境，该环境模拟了药房技术员学徒与实际客户互动的情景。研究结果表明，虽然两种指导类型都有益处，但PS-I的教学序列在迁移任务中的表现显著更高。', 'title_zh': '指令序列和个人化支持对诊断策略学习的影响'}
{'arxiv_id': 'arXiv:2507.17756', 'title': 'Insights from Railway Professionals: Rethinking Railway assumptions regarding safety and autonomy', 'authors': 'Josh Hunter, John McDermid, Simon Burton', 'link': 'https://arxiv.org/abs/2507.17756', 'abstract': 'This study investigates how railway professionals perceive safety as a concept within rail, with the intention to help inform future technological developments within the industry. Through a series of interviews with drivers, route planners,and administrative personnel, the research explores the currentstate of safety practices, the potential for automation and the understanding of the railway as a system of systems. Key findings highlight a cautious attitude towards automation, a preference for assistive technologies, and a complex understanding of safety that integrates human, systematic and technological factors. The study also addresses the limitations of transferring automotive automation technologies to railways and the need for a railway-specific causation model to better evaluate and enhance safety in an evolving technological landscape. This study aims to bridge thegap between contemporary research and practical applications, contributing to the development of more effective safety metrics.', 'abstract_zh': '本研究调查了铁路专业人员对 rail 领域安全这一概念的看法，旨在帮助未来的技术发展提供参考。通过与驾驶员、路线规划员和行政人员的一系列访谈，研究探索了当前的安全实践状况、自动化Potential及其对铁路作为一个系统网络的理解。关键发现表明，对自动化持谨慎态度、更倾向于辅助技术，并对安全的概念有复杂的理解，整合了人类、系统和技术因素。研究还讨论了将汽车自动化技术转移到铁路领域的局限性，并强调了需要针对铁路的因果模型来更好地评估和提升安全水平。本研究旨在弥合当代研究与实际应用之间的差距，为更有效的安全指标的发展作出贡献。', 'title_zh': '来自铁路专业人员的见解：重新审视铁路关于安全与自主性的假设'}
{'arxiv_id': 'arXiv:2507.17754', 'title': 'A Custom-Built Ambient Scribe Reduces Cognitive Load and Documentation Burden for Telehealth Clinicians', 'authors': 'Justin Morse, Kurt Gilbert, Kyle Shin, Rick Cooke, Peyton Rose, Jack Sullivan, Angelo Sisante', 'link': 'https://arxiv.org/abs/2507.17754', 'abstract': 'Clinician burnout has motivated the growing adoption of ambient medical scribes in the clinic. In this work, we introduce a custom-built ambient scribe application integrated into the EHR system at Included Health, a personalized all-in-one healthcare company offering telehealth services. The application uses Whisper for transcription and a modular in-context learning pipeline with GPT-4o to automatically generate SOAP notes and patient instructions. Testing on mock visit data shows that the notes generated by the application exceed the quality of expert-written notes as determined by an LLM-as-a-judge. The application has been widely adopted by the clinical practice, with over 540 clinicians at Included Health using the application at least once. 94% (n = 63) of surveyed clinicians report reduced cognitive load during visits and 97% (n = 66) report less documentation burden when using the application. Additionally, we show that post-processing notes with a fine-tuned BART model improves conciseness. These findings highlight the potential for AI systems to ease administrative burdens and support clinicians in delivering efficient, high-quality care.', 'abstract_zh': '临床医生的burnout促使了门诊中ambient medical scribes的广泛应用。在此工作中，我们介绍了一个集成于Included Health EHR系统的自定义ambient scribe应用，Included Health是一家提供个性化一站式医疗服务并涵盖远程医疗服务的公司。该应用使用Whisper进行转录，并结合GPT-4o的模块化上下文学习流水线自动生成SOAP笔记和患者指导。测试结果显示，应用生成的笔记在LLM-as-a-judge的评估中质量超过专家手写的笔记。该应用已被广泛采用，在Included Health有超过540名临床医生至少使用一次。在接受调查的临床医生中，94%（n=63）的人报告称，在使用该应用时访视中的认知负荷减少，97%（n=66）的人报告称，使用该应用时记录文档的负担减轻。此外，我们还展示了使用微调的BART模型处理笔记可以提高简洁性。这些发现突显了AI系统在减轻行政负担和支持临床医生提供高效、高质量护理方面的潜力。', 'title_zh': '自定义构建的环境记录员减轻了远程健康 clinicians 的认知负荷和记录负担。'}
{'arxiv_id': 'arXiv:2507.17753', 'title': 'Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving', 'authors': 'Liang Zhang, Xiaoming Zhai, Jionghao Lin, Jionghao Lin, Jennifer Kleiman, Diego Zapata-Rivera, Carol Forsyth, Yang Jiang, Xiangen Hu, Arthur C. Graesser', 'link': 'https://arxiv.org/abs/2507.17753', 'abstract': "Large Language Model (LLM) agents are increasingly utilized in AI-aided education to support tutoring and learning. Effective communication strategies among LLM agents improve collaborative problem-solving efficiency and facilitate cost-effective adoption in education. However, little research has systematically evaluated the impact of different communication strategies on agents' problem-solving. Our study examines four communication modes, \\textit{teacher-student interaction}, \\textit{peer-to-peer collaboration}, \\textit{reciprocal peer teaching}, and \\textit{critical debate}, in a dual-agent, chat-based mathematical problem-solving environment using the OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that dual-agent setups outperform single agents, with \\textit{peer-to-peer collaboration} achieving the highest accuracy. Dialogue acts like statements, acknowledgment, and hints play a key role in collaborative problem-solving. While multi-agent frameworks enhance computational tasks, effective communication strategies are essential for tackling complex problems in AI education.", 'abstract_zh': '大型语言模型（LLM）代理在AI辅助教育中的应用日益增多，用于支持辅导和学习。不同的沟通策略在LLM代理间的有效沟通提高了协作问题解决的效率，并促进了教育中的成本效益采用。然而，关于不同沟通策略对代理问题解决影响的研究较少。本研究在使用OpenAI GPT-4o模型的双代理、基于聊天的数学问题解决环境中，探讨了四种沟通模式：教师-学生互动、同伴间协作、互惠同伴教学和批判性辩论的影响。以MATH数据集为评价标准，研究表明，双代理设置优于单代理，同伴间协作在准确性上最高。对话行为如陈述、确认和提示在协作问题解决中发挥着关键作用。多代理框架虽然增强了计算任务，但有效的沟通策略对于解决AI教育中的复杂问题至关重要。', 'title_zh': '探索数学问题解决中协作型LLM代理的通信策略'}
{'arxiv_id': 'arXiv:2507.15765', 'title': 'Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization', 'authors': 'Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, Meng Wang', 'link': 'https://arxiv.org/abs/2507.15765', 'abstract': 'Dynamic Facial Expression Recognition (DFER) plays a critical role in affective computing and human-computer interaction. Although existing methods achieve comparable performance, they inevitably suffer from performance degradation under sample heterogeneity caused by multi-source data and individual expression variability. To address these challenges, we propose a novel framework, called Heterogeneity-aware Distributional Framework (HDF), and design two plug-and-play modules to enhance time-frequency modeling and mitigate optimization imbalance caused by hard samples. Specifically, the Time-Frequency Distributional Attention Module (DAM) captures both temporal consistency and frequency robustness through a dual-branch attention design, improving tolerance to sequence inconsistency and visual style shifts. Then, based on gradient sensitivity and information bottleneck principles, an adaptive optimization module Distribution-aware Scaling Module (DSM) is introduced to dynamically balance classification and contrastive losses, enabling more stable and discriminative representation learning. Extensive experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF significantly improves both recognition accuracy and robustness. Our method achieves superior weighted average recall (WAR) and unweighted average recall (UAR) while maintaining strong generalization across diverse and imbalanced scenarios. Codes are released at this https URL.', 'abstract_zh': '异构分布感知动态面部表情识别框架（Heterogeneity-aware Distributional Framework for Dynamic Facial Expression Recognition）', 'title_zh': '从异质性中学习：基于分布鲁棒优化的动态面部表情识别泛化'}
