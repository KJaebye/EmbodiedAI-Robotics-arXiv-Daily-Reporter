# LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning 

**Title (ZH)**: 基于大语言模型的_self-play与强化学习相结合的围棋战略系统_ LLVM-Gomoku 

**Authors**: Hui Wang  

**Link**: [PDF](https://arxiv.org/pdf/2503.21683)  

**Abstract**: In recent years, large language models (LLMs) have shown significant advancements in natural language processing (NLP), with strong capa-bilities in generation, comprehension, and rea-soning. These models have found applications in education, intelligent decision-making, and gaming. However, effectively utilizing LLMs for strategic planning and decision-making in the game of Gomoku remains a challenge. This study aims to develop a Gomoku AI system based on LLMs, simulating the human learning process of playing chess. The system is de-signed to understand and apply Gomoku strat-egies and logic to make rational decisions. The research methods include enabling the model to "read the board," "understand the rules," "select strategies," and "evaluate positions," while en-hancing its abilities through self-play and rein-forcement learning. The results demonstrate that this approach significantly improves the se-lection of move positions, resolves the issue of generating illegal positions, and reduces pro-cess time through parallel position evaluation. After extensive self-play training, the model's Gomoku-playing capabilities have been notably enhanced. 

**Abstract (ZH)**: 近年来，大规模语言模型（LLMs）在自然语言处理（NLP）领域取得了显著进展，具备强大的生成、理解和推理能力。这些模型已在教育、智能决策和游戏等领域找到了应用。然而，有效利用LLMs进行五子棋的战略规划和决策仍然是一项挑战。本研究旨在基于LLMs开发一个模拟人类学习下棋过程的五子棋AI系统，设计该系统以理解和应用五子棋策略和逻辑，从而作出理性决策。研究方法包括使模型能够“读棋盘”、“理解规则”、“选择策略”和“评估位置”，并通过自我对弈和强化学习提升其能力。研究结果表明，这种 Approach 显著提高了落子位置的选择，解决了生成非法位置的问题，并通过并行位置评估缩减了处理时间。经过大量自我对弈训练后，模型的五子棋能力显著增强。 

---
# Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI 

**Title (ZH)**: 认知科学启发的对象理解人工智能核心能力评价 

**Authors**: Danaja Rutar, Alva Markelius, Konstantinos Voudouris, José Hernández-Orallo, Lucy Cheke  

**Link**: [PDF](https://arxiv.org/pdf/2503.21668)  

**Abstract**: One of the core components of our world models is 'intuitive physics' - an understanding of objects, space, and causality. This capability enables us to predict events, plan action and navigate environments, all of which rely on a composite sense of objecthood. Despite its importance, there is no single, unified account of objecthood, though multiple theoretical frameworks provide insights. In the first part of this paper, we present a comprehensive overview of the main theoretical frameworks in objecthood research - Gestalt psychology, enactive cognition, and developmental psychology - and identify the core capabilities each framework attributes to object understanding, as well as what functional roles they play in shaping world models in biological agents. Given the foundational role of objecthood in world modelling, understanding objecthood is also essential in AI. In the second part of the paper, we evaluate how current AI paradigms approach and test objecthood capabilities compared to those in cognitive science. We define an AI paradigm as a combination of how objecthood is conceptualised, the methods used for studying objecthood, the data utilised, and the evaluation techniques. We find that, whilst benchmarks can detect that AI systems model isolated aspects of objecthood, the benchmarks cannot detect when AI systems lack functional integration across these capabilities, not solving the objecthood challenge fully. Finally, we explore novel evaluation approaches that align with the integrated vision of objecthood outlined in this paper. These methods are promising candidates for advancing from isolated object capabilities toward general-purpose AI with genuine object understanding in real-world contexts. 

**Abstract (ZH)**: 我们世界观的核心组件之一是“直观物理”——对物体、空间和因果关系的理解。这一能力使我们能够预测事件、规划行动和导航环境，所有这些都依赖于对象本质的综合感知。尽管对象本质至关重要，但在对象本质的研究中尚未形成单一且统一的理论，尽管多个理论框架提供了重要见解。在本文的第一部分中，我们全面介绍了对象本质研究中的主要理论框架——格式塔心理学、活性认知和发展心理学，并识别出每个框架赋予对象理解的核心能力及其在塑造生物智能体世界观中的功能性作用。由于对象本质在世界观建模中的基础性作用，理解对象本质也是人工智能中不可或缺的一环。在本文的第二部分中，我们评估当前人工智能范式如何处理和测试对象本质能力，这些测试与认知科学中的情况相比如何。我们将AI范式定义为对象本质的概念化、研究方法、使用的数据以及评估技术的组合。我们发现，虽然基准测试可以检测到AI系统模型化对象本质的孤立方面，但这些基准测试无法检测到这些能力之间缺乏功能性整合的情况，无法完全解决对象本质挑战。最后，我们探索与本文概述的对象本质整合愿景相一致的新型评估方法。这些方法是朝着在真实世界环境中具备真正对象理解的大规模应用型AI方向迈进的有前途的候选方法。 

---
# Unlocking the Potential of Past Research: Using Generative AI to Reconstruct Healthcare Simulation Models 

**Title (ZH)**: 解锁过往研究的潜力：利用生成式AI重构医疗 simulation 模型 

**Authors**: Thomas Monks, Alison Harper, Amy Heather  

**Link**: [PDF](https://arxiv.org/pdf/2503.21646)  

**Abstract**: Discrete-event simulation (DES) is widely used in healthcare Operations Research, but the models themselves are rarely shared. This limits their potential for reuse and long-term impact in the modelling and healthcare communities. This study explores the feasibility of using generative artificial intelligence (AI) to recreate published models using Free and Open Source Software (FOSS), based on the descriptions provided in an academic journal. Using a structured methodology, we successfully generated, tested and internally reproduced two DES models, including user interfaces. The reported results were replicated for one model, but not the other, likely due to missing information on distributions. These models are substantially more complex than AI-generated DES models published to date. Given the challenges we faced in prompt engineering, code generation, and model testing, we conclude that our iterative approach to model development, systematic comparison and testing, and the expertise of our team were necessary to the success of our recreated simulation models. 

**Abstract (ZH)**: 基于学术期刊描述使用生成式人工智能重建开源软件自由离散事件模拟模型的研究 

---
# Towards Fully Automated Decision-Making Systems for Greenhouse Control: Challenges and Opportunities 

**Title (ZH)**: 面向温室控制的完全自动化决策系统：挑战与机遇 

**Authors**: Yongshuai Liu, Taeyeong Choi, Xin Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.21640)  

**Abstract**: Machine learning has been successful in building control policies to drive a complex system to desired states in various applications (e.g. games, robotics, etc.). To be specific, a number of parameters of policy can be automatically optimized from the observations of environment to be able to generate a sequence of decisions leading to the best performance. In this survey paper, we particularly explore such policy-learning techniques for another unique, practical use-case scenario--farming, in which critical decisions (e.g., water supply, heating, etc.) must be made in a timely manner to minimize risks (e.g., damage to plants) while maximizing the revenue (e.g., healthy crops) in the end. We first provide a broad overview of latest studies on it to identify not only domain-specific challenges but opportunities with potential solutions, some of which are suggested as promising directions for future research. Also, we then introduce our successful approach to being ranked second among 46 teams at the ''3rd Autonomous Greenhouse Challenge'' to use this specific example to discuss the lessons learned about important considerations for design to create autonomous farm-management systems. 

**Abstract (ZH)**: 机器学习已在各种应用（例如游戏、机器人技术等）中成功构建控制策略，将复杂系统驱动到所需状态。具体而言，可以通过从环境观察中自动优化策略参数，生成一系列决策以实现最佳性能。在本文综述中，我们将特别探讨此类策略学习技术在另一个独特的实际应用场景——农业中的应用，其中必须在适当的时间作出关键决策（例如，灌溉、加热等）以尽量减少风险（例如，植物受损）并最大化最终收益（例如，健康作物）。我们首先提供最新的研究综述，以识别不仅限于领域特定的挑战，还包括具有潜在解决方案的机会，并提出其中一些领域作为未来研究的有前景方向。随后，我们将介绍我们成功的方法，即在“第三届自主温室挑战赛”中获得第2名的策略，通过具体示例讨论设计自主农场管理系统时需要考虑的重要注意事项。 

---
# UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning 

**Title (ZH)**: UI-R1: 通过强化学习增强GUI代理的动作预测 

**Authors**: Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, Hongsheng Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.21620)  

**Abstract**: The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks. To this end, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. We also introduce a unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID benchmark AndroidControl, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain. 

**Abstract (ZH)**: 基于规则的强化学习提升多模态大型语言模型在图形用户界面操作预测任务中的推理能力 

---
# GenEdit: Compounding Operators and Continuous Improvement to Tackle Text-to-SQL in the Enterprise 

**Title (ZH)**: GenEdit: 累积运算符与持续优化以应对企业中的文本到SQL问题 

**Authors**: Karime Maamari, Connor Landy, Amine Mhedhbi  

**Link**: [PDF](https://arxiv.org/pdf/2503.21602)  

**Abstract**: Recent advancements in Text-to-SQL, driven by large language models, are democratizing data access. Despite these advancements, enterprise deployments remain challenging due to the need to capture business-specific knowledge, handle complex queries, and meet expectations of continuous improvements. To address these issues, we designed and implemented GenEdit: our Text-to-SQL generation system that improves with user feedback. GenEdit builds and maintains a company-specific knowledge set, employs a pipeline of operators decomposing SQL generation, and uses feedback to update its knowledge set to improve future SQL generations.
We describe GenEdit's architecture made of two core modules: (i) decomposed SQL generation; and (ii) knowledge set edits based on user feedback. For generation, GenEdit leverages compounding operators to improve knowledge retrieval and to create a plan as chain-of-thought steps that guides generation. GenEdit first retrieves relevant examples in an initial retrieval stage where original SQL queries are decomposed into sub-statements, clauses or sub-queries. It then also retrieves instructions and schema elements. Using the retrieved contextual information, GenEdit then generates step-by-step plan in natural language on how to produce the query. Finally, GenEdit uses the plan to generate SQL, minimizing the need for model reasoning, which enhances complex SQL generation. If necessary, GenEdit regenerates the query based on syntactic and semantic errors. The knowledge set edits are recommended through an interactive copilot, allowing users to iterate on their feedback and to regenerate SQL queries as needed. Each generation uses staged edits which update the generation prompt. Once the feedback is submitted, it gets merged after passing regression testing and obtaining an approval, improving future generations. 

**Abstract (ZH)**: 近期大型语言模型驱动的Text-to-SQL进展正 democratizing数据访问。尽管取得了这些进展，企业部署仍面临挑战，因为需要捕获特定业务知识、处理复杂查询并满足持续改进的期望。为了解决这些问题，我们设计并实现了一种名为GenEdit的Text-to-SQL生成系统，该系统能够通过用户反馈进行改进。GenEdit构建并维护一个特定企业的知识库，采用操作流水线分解SQL生成过程，并利用反馈来更新知识库以改善未来的SQL生成。 

---
# debug-gym: A Text-Based Environment for Interactive Debugging 

**Title (ZH)**: debug-gym: 基于文本的交互式调试环境 

**Authors**: Xingdi Yuan, Morgane M Moss, Charbel El Feghali, Chinmay Singh, Darya Moldavskaya, Drew MacPhee, Lucas Caccia, Matheus Pereira, Minseon Kim, Alessandro Sordoni, Marc-Alexandre Côté  

**Link**: [PDF](https://arxiv.org/pdf/2503.21557)  

**Abstract**: Large Language Models (LLMs) are increasingly relied upon for coding tasks, yet in most scenarios it is assumed that all relevant information can be either accessed in context or matches their training data. We posit that LLMs can benefit from the ability to interactively explore a codebase to gather the information relevant to their task. To achieve this, we present a textual environment, namely debug-gym, for developing LLM-based agents in an interactive coding setting. Our environment is lightweight and provides a preset of useful tools, such as a Python debugger (pdb), designed to facilitate an LLM-based agent's interactive debugging. Beyond coding and debugging tasks, this approach can be generalized to other tasks that would benefit from information-seeking behavior by an LLM agent. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在编码任务中的应用日益增多，然而在大多数场景中，假定所有相关的信息要么可以在上下文中访问，要么与模型的训练数据匹配。我们提出，LLMs可以从能够互动性地探索代码库以收集完成任务所需的相关信息中受益。为此，我们提出了一种文本环境，即debug-gym，用于在互动编码环境中开发基于LLM的代理。我们的环境轻量且预设了一些有用工具，例如Python调试器（pdb），旨在促进基于LLM的代理的互动性调试。除了编码和调试任务外，这种approach还可以泛化到其他可以从中受益于LLM代理的信息寻求行为的任务中。 

---
# The Procedural Content Generation Benchmark: An Open-source Testbed for Generative Challenges in Games 

**Title (ZH)**: 程序内容生成基准：一个用于游戏生成挑战的开源测试平台 

**Authors**: Ahmed Khalifa, Roberto Gallotta, Matthew Barthet, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis  

**Link**: [PDF](https://arxiv.org/pdf/2503.21474)  

**Abstract**: This paper introduces the Procedural Content Generation Benchmark for evaluating generative algorithms on different game content creation tasks. The benchmark comes with 12 game-related problems with multiple variants on each problem. Problems vary from creating levels of different kinds to creating rule sets for simple arcade games. Each problem has its own content representation, control parameters, and evaluation metrics for quality, diversity, and controllability. This benchmark is intended as a first step towards a standardized way of comparing generative algorithms. We use the benchmark to score three baseline algorithms: a random generator, an evolution strategy, and a genetic algorithm. Results show that some problems are easier to solve than others, as well as the impact the chosen objective has on quality, diversity, and controllability of the generated artifacts. 

**Abstract (ZH)**: 面向游戏内容生成的程序化内容生成基准评估生成算法在不同类型的游戏内容创作任务中的性能 

---
# Graph-to-Vision: Multi-graph Understanding and Reasoning using Vision-Language Models 

**Title (ZH)**: 图到视觉：利用视觉-语言模型进行多图理解与推理 

**Authors**: Ruizhou Li, Haiyun Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2503.21435)  

**Abstract**: Graph Neural Networks (GNNs), as the dominant paradigm for graph-structured learning, have long faced dual challenges of exponentially escalating computational complexity and inadequate cross-scenario generalization capability. With the rapid advancement of multimodal learning, Vision-Language Models (VLMs) have demonstrated exceptional cross-modal relational reasoning capabilities and generalization capacities, thereby opening up novel pathways for overcoming the inherent limitations of conventional graph learning paradigms. However, current research predominantly concentrates on investigating the single-graph reasoning capabilities of VLMs, which fundamentally fails to address the critical requirement for coordinated reasoning across multiple heterogeneous graph data in real-world application scenarios. To address these limitations, we propose the first multi-graph joint reasoning benchmark for VLMs. Our benchmark encompasses four graph categories: knowledge graphs, flowcharts, mind maps, and route maps,with each graph group accompanied by three progressively challenging instruction-response pairs. Leveraging this benchmark, we conducted comprehensive capability assessments of state-of-the-art VLMs and performed fine-tuning on open-source models. This study not only addresses the underexplored evaluation gap in multi-graph reasoning for VLMs but also empirically validates their generalization superiority in graph-structured learning. 

**Abstract (ZH)**: 多图联合推理基准：Vision-Language模型在图结构学习中的跨图联合推理能力评估 

---
# Neuroplasticity in Artificial Intelligence -- An Overview and Inspirations on Drop In \& Out Learning 

**Title (ZH)**: 人工智能中的神经可塑性——概览及Drop In & Out学习的启示 

**Authors**: Yupei Li, Manuel Milling, Björn W. Schuller  

**Link**: [PDF](https://arxiv.org/pdf/2503.21419)  

**Abstract**: Artificial Intelligence (AI) has achieved new levels of performance and spread in public usage with the rise of deep neural networks (DNNs). Initially inspired by human neurons and their connections, NNs have become the foundation of AI models for many advanced architectures. However, some of the most integral processes in the human brain, particularly neurogenesis and neuroplasticity in addition to the more spread neuroapoptosis have largely been ignored in DNN architecture design. Instead, contemporary AI development predominantly focuses on constructing advanced frameworks, such as large language models, which retain a static structure of neural connections during training and inference. In this light, we explore how neurogenesis, neuroapoptosis, and neuroplasticity can inspire future AI advances. Specifically, we examine analogous activities in artificial NNs, introducing the concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and structural pruning for neuroapoptosis. We additionally suggest neuroplasticity combining the two for future large NNs in ``life-long learning'' settings following the biological inspiration. We conclude by advocating for greater research efforts in this interdisciplinary domain and identifying promising directions for future exploration. 

**Abstract (ZH)**: 人工智能（AI）在深度神经网络（DNNs）的兴起下实现了新的性能水平并在公共使用中得到普及。尽管最初受到人类神经元及其连接的启发，神经网络已成为许多先进架构中AI模型的基础。然而，在DNN架构设计中，人类大脑的一些最核心过程，特别是神经发生、神经凋亡以及更为普及的神经可塑性，大多被忽视。相反，现代AI开发主要集中在构建复杂的框架，如大规模语言模型，这些框架在训练和推理期间保留了神经连接的静态结构。因此，我们探讨了神经发生、神经凋亡和神经可塑性如何启发未来的AI发展。具体来说，我们研究了人工神经网络中的相应活动，引入了“dropin”这一概念以类比神经发生，并重新审视了“dropout”和结构剪枝以类比神经凋亡。此外，我们建议神经可塑性将前两者结合起来，用于“终身学习”环境中未来大规模神经网络的设计，这是受到生物启发的。最后，我们呼吁在这一跨学科领域进行更多的研究努力，并确定未来探索的有希望的方向。 

---
# Federated Intelligence: When Large AI Models Meet Federated Fine-Tuning and Collaborative Reasoning at the Network Edge 

**Title (ZH)**: 联邦智能：当大型AI模型遇到网络边缘的联邦微调与协作推理 

**Authors**: Wanli Ni, Haofeng Sun, Huiqing Ao, Hui Tian  

**Link**: [PDF](https://arxiv.org/pdf/2503.21412)  

**Abstract**: Large artificial intelligence (AI) models exhibit remarkable capabilities in various application scenarios, but deploying them at the network edge poses significant challenges due to issues such as data privacy, computational resources, and latency. In this paper, we explore federated fine-tuning and collaborative reasoning techniques to facilitate the implementation of large AI models in resource-constrained wireless networks. Firstly, promising applications of large AI models within specific domains are discussed. Subsequently, federated fine-tuning methods are proposed to adapt large AI models to specific tasks or environments at the network edge, effectively addressing the challenges associated with communication overhead and enhancing communication efficiency. These methodologies follow clustered, hierarchical, and asynchronous paradigms to effectively tackle privacy issues and eliminate data silos. Furthermore, to enhance operational efficiency and reduce latency, efficient frameworks for model collaborative reasoning are developed, which include decentralized horizontal collaboration, cloud-edge-end vertical collaboration, and multi-access collaboration. Next, simulation results demonstrate the effectiveness of our proposed methods in reducing the fine-tuning loss of large AI models across various downstream tasks. Finally, several open challenges and research opportunities are outlined. 

**Abstract (ZH)**: 大规模人工智能模型在各种应用场景中展现出显著的能力，但在无线网络边缘部署它们面临着数据隐私、计算资源和延迟等方面的重大挑战。本文探讨了联邦微调和协作推理技术，以促进大规模人工智能模型在资源受限无线网络中的实现。首先，讨论了大规模人工智能模型在特定领域的潜在应用。随后，提出了联邦微调方法，以在网络边缘将大规模人工智能模型适配到特定任务或环境中，有效解决通信开销问题，提升通信效率。这些方法遵循集群化、分层次和异步化的范式，有效应对隐私问题并消除数据孤岛。为进一步提高操作效率并降低延迟，还开发了高效模型协作推理框架，包括去中心化的水平协作、云-边缘-端的垂直协作以及多接入协作。接下来，仿真结果证明了我们所提出方法在各类下游任务中减少大规模人工智能模型微调损失的有效性。最后，概述了若干开放挑战和研究机遇。 

---
# Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap 

**Title (ZH)**: 探索大型语言模型在重塑交通运输系统中的作用：综述、框架与 roadmap 

**Authors**: Tong Nie, Jian Sun, Wei Ma  

**Link**: [PDF](https://arxiv.org/pdf/2503.21411)  

**Abstract**: Modern transportation systems face pressing challenges due to increasing demand, dynamic environments, and heterogeneous information integration. The rapid evolution of Large Language Models (LLMs) offers transformative potential to address these challenges. Extensive knowledge and high-level capabilities derived from pretraining evolve the default role of LLMs as text generators to become versatile, knowledge-driven task solvers for intelligent transportation systems. This survey first presents LLM4TR, a novel conceptual framework that systematically categorizes the roles of LLMs in transportation into four synergetic dimensions: information processors, knowledge encoders, component generators, and decision facilitators. Through a unified taxonomy, we systematically elucidate how LLMs bridge fragmented data pipelines, enhance predictive analytics, simulate human-like reasoning, and enable closed-loop interactions across sensing, learning, modeling, and managing tasks in transportation systems. For each role, our review spans diverse applications, from traffic prediction and autonomous driving to safety analytics and urban mobility optimization, highlighting how emergent capabilities of LLMs such as in-context learning and step-by-step reasoning can enhance the operation and management of transportation systems. We further curate practical guidance, including available resources and computational guidelines, to support real-world deployment. By identifying challenges in existing LLM-based solutions, this survey charts a roadmap for advancing LLM-driven transportation research, positioning LLMs as central actors in the next generation of cyber-physical-social mobility ecosystems. Online resources can be found in the project page: this https URL. 

**Abstract (ZH)**: 现代交通系统面临着日益增长的需求、动态环境以及异质信息集成等紧迫挑战。大型语言模型（LLMs）的迅猛发展为应对这些挑战提供了变革性的潜力。通过前期训练获得的广泛知识和高级能力，LLMs 的默认角色从文本生成者转变为适应性强、知识驱动的智能交通系统任务解决者。本文综述首先提出 LLM4TR，这是一种新颖的概念框架，系统地将 LLM 在交通中的角色划分为四大协同维度：信息处理器、知识编码器、组件生成器和决策促进者。通过统一的分类体系，本文系统地阐述了如何通过 LLM 桥接碎片化的数据管道、增强预测分析、模拟类人推理，以及在交通系统中的传感、学习、建模和管理任务中实现闭环交互。对于每种角色，我们的综述涵盖了从交通预测和自动驾驶到安全分析和城市交通优化等多样化的应用，突出了新兴的 LLM 能力，如上下文学习和逐步推理，如何增强交通系统的运行和管理。我们进一步整理了实用指导，包括可利用的资源和计算指南，以支持实际部署。通过识别现有基于LLM的解决方案中的挑战，本文为推进LLM驱动的交通研究绘制了蓝图，将LLM定位为下一代网络物理社会交通生态系统中的核心角色。有关在线资源，请参阅项目页面：this https URL。 

---
# Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for Skill Learning 

**Title (ZH)**: 神经符号imitation学习：发现技能学习中的符号抽象 

**Authors**: Leon Keller, Daniel Tanneberg, Jan Peters  

**Link**: [PDF](https://arxiv.org/pdf/2503.21406)  

**Abstract**: Imitation learning is a popular method for teaching robots new behaviors. However, most existing methods focus on teaching short, isolated skills rather than long, multi-step tasks. To bridge this gap, imitation learning algorithms must not only learn individual skills but also an abstract understanding of how to sequence these skills to perform extended tasks effectively. This paper addresses this challenge by proposing a neuro-symbolic imitation learning framework. Using task demonstrations, the system first learns a symbolic representation that abstracts the low-level state-action space. The learned representation decomposes a task into easier subtasks and allows the system to leverage symbolic planning to generate abstract plans. Subsequently, the system utilizes this task decomposition to learn a set of neural skills capable of refining abstract plans into actionable robot commands. Experimental results in three simulated robotic environments demonstrate that, compared to baselines, our neuro-symbolic approach increases data efficiency, improves generalization capabilities, and facilitates interpretability. 

**Abstract (ZH)**: 模仿学习是一种popular方法，用于 teaching机器人新行为。然而，现有的大多数方法侧重于教授short、孤立的技能，而不是long、多步的任务。为解决这一问题，模仿学习算法不仅需要学习个体技能，还需要理解如何将这些技能按序排列以有效执行扩展任务。本文通过提出一种神经符号模仿学习框架来应对这一挑战。系统利用任务演示首先学习一种符号表示，该表示抽象了低层的状态-动作空间。所学的表示将任务分解为更易处理的子任务，并允许系统利用符号规划生成抽象计划。随后，系统利用这一任务分解来学习一套神经技能，以将抽象计划细化为可执行的机器人命令。在三个模拟机器人环境中的实验结果表明，与基线方法相比，我们的神经符号方法提高了数据效率、增强了泛化能力和提高了可解释性。 

---
# HybridoNet-Adapt: A Domain-Adapted Framework for Accurate Lithium-Ion Battery RUL Prediction 

**Title (ZH)**: HybridoNet-Adapt：一种用于锂离子电池剩余使用寿命预测的领域自适应框架 

**Authors**: Khoa Tran, Bao Huynh, Tri Le, Lam Pham, Vy-Rin Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2503.21392)  

**Abstract**: Accurate prediction of the remaining useful life (RUL) in Lithium-ion battery (LIB) health management systems is crucial for ensuring reliability and safety. Current methods typically assume that training and testing data share the same distribution, overlooking the benefits of incorporating diverse data sources to enhance model performance. To address this limitation, we introduce a data-independent RUL prediction framework along with its domain adaptation (DA) approach, which leverages heterogeneous data sources for improved target predictions. Our approach integrates comprehensive data preprocessing, including feature extraction, denoising, and normalization, with a data-independent prediction model that combines Long Short-Term Memory (LSTM), Multihead Attention, and a Neural Ordinary Differential Equation (NODE) block, termed HybridoNet. The domain-adapted version, HybridoNet Adapt, is trained using a novel technique inspired by the Domain-Adversarial Neural Network (DANN) framework, a regression ensemble method, and Maximum Mean Discrepancy (MMD) to learn domain-invariant features from labeled cycling data in the source and target domains. Experimental results demonstrate that our approach outperforms state-of-the-art techniques, providing reliable RUL predictions for real-world applications. 

**Abstract (ZH)**: 锂离子电池（LIB）健康管理系统中剩余使用寿命（RUL）的准确预测对于确保可靠性和安全性至关重要。为了克服现有方法忽视利用多样化数据源提高模型性能的局限性，我们提出了一种数据无关的RUL预测框架及其域适应（DA）方法，该方法利用异构数据源以提升目标预测性能。我们的方法集成了全面的数据预处理，包括特征提取、去噪和归一化，并结合了长短期记忆网络（LSTM）、多头注意力机制和神经常微分方程（NODE）块，称为HybridoNet。域适应版本HybridoNet Adapt通过结合域对抗神经网络（DANN）框架、回归集成方法和最大均值偏差（MMD）训练，从源域和目标域的标记充放电数据中学习域不变特征。实验结果表明，我们的方法优于现有技术，能够为实际应用提供可靠的RUL预测。 

---
# Using large language models to produce literature reviews: Usages and systematic biases of microphysics parametrizations in 2699 publications 

**Title (ZH)**: 使用大型语言模型生成文献综述：2699篇出版物中微物理参数化方法的用途及系统性偏差 

**Authors**: Tianhang Zhang, Shengnan Fu, David M. Schultz, Zhonghua Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2503.21352)  

**Abstract**: Large language models afford opportunities for using computers for intensive tasks, realizing research opportunities that have not been considered before. One such opportunity could be a systematic interrogation of the scientific literature. Here, we show how a large language model can be used to construct a literature review of 2699 publications associated with microphysics parametrizations in the Weather and Research Forecasting (WRF) model, with the goal of learning how they were used and their systematic biases, when simulating precipitation. The database was constructed of publications identified from Web of Science and Scopus searches. The large language model GPT-4 Turbo was used to extract information about model configurations and performance from the text of 2699 publications. Our results reveal the landscape of how nine of the most popular microphysics parameterizations have been used around the world: Lin, Ferrier, WRF Single-Moment, Goddard Cumulus Ensemble, Morrison, Thompson, and WRF Double-Moment. More studies used one-moment parameterizations before 2020 and two-moment parameterizations after 2020. Seven out of nine parameterizations tended to overestimate precipitation. However, systematic biases of parameterizations differed in various regions. Except simulations using the Lin, Ferrier, and Goddard parameterizations that tended to underestimate precipitation over almost all locations, the remaining six parameterizations tended to overestimate, particularly over China, southeast Asia, western United States, and central Africa. This method could be used by other researchers to help understand how the increasingly massive body of scientific literature can be harnessed through the power of artificial intelligence to solve their research problems. 

**Abstract (ZH)**: 大型语言模型为使用计算机进行密集任务提供了机会，实现了前所未有的研究机会。其中一个机会是系统地审查科学文献。在这里，我们展示了如何使用大型语言模型来构建与Weather and Research Forecasting (WRF)模型中的微物理参数化相关的2699篇出版物的文献综述，目的是学习它们在模拟降水时的应用及其系统偏差。数据库是通过对Web of Science和Scopus的搜索来识别出版物构建的。使用大型语言模型GPT-4 Turbo从2699篇出版物的文本中提取有关模型配置和性能的信息。我们的结果揭示了九种最受欢迎的微物理参数化在全球范围内的使用情况：Lin、Ferrier、WRF单 moment、Goddard积云集合、Morrison、Thompson和WRF双 moment。在2020年之前，更多的研究使用了一 moment 参数化，在2020年之后则更多地使用了两 moment 参数化。九个参数化中有七个倾向于高估降水。然而，参数化的系统偏差在不同地区有所不同。除使用Lin、Ferrier和Goddard参数化模拟在全球几乎所有地区都倾向于低估降水外，其余六个参数化则倾向于在诸如中国、东南亚、美国西部和中非等地高估降水。这种方法可以供其他研究人员使用，通过人工智能的力量帮助他们理解如何利用日益庞大的科学文献解决研究问题。 

---
# HyperGraphRAG: Retrieval-Augmented Generation with Hypergraph-Structured Knowledge Representation 

**Title (ZH)**: HyperGraphRAG：基于超图结构知识表示的检索增强生成 

**Authors**: Haoran Luo, Haihong E, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo, Qika Lin, Yu Feng, Zemin Kuang, Meina Song, Yifan Zhu, Luu Anh Tuan  

**Link**: [PDF](https://arxiv.org/pdf/2503.21322)  

**Abstract**: While standard Retrieval-Augmented Generation (RAG) based on chunks, GraphRAG structures knowledge as graphs to leverage the relations among entities. However, previous GraphRAG methods are limited by binary relations: one edge in the graph only connects two entities, which cannot well model the n-ary relations among more than two entities that widely exist in reality. To address this limitation, we propose HyperGraphRAG, a novel hypergraph-based RAG method that represents n-ary relational facts via hyperedges, modeling the complicated n-ary relations in the real world. To retrieve and generate over hypergraphs, we introduce a complete pipeline with a hypergraph construction method, a hypergraph retrieval strategy, and a hypergraph-guided generation mechanism. Experiments across medicine, agriculture, computer science, and law demonstrate that HyperGraphRAG outperforms standard RAG and GraphRAG in accuracy and generation quality. 

**Abstract (ZH)**: 基于超图的HyperGraphRAG：一种表示复杂n元关系的检索增强生成方法 

---
# Reinforced Model Merging 

**Title (ZH)**: 强化模型融合 

**Authors**: Jiaqi Han, Jingwen Ye, Shunyu Liu, Haofei Zhang, Jie Song, Zunlei Feng, Mingli Song  

**Link**: [PDF](https://arxiv.org/pdf/2503.21272)  

**Abstract**: The success of large language models has garnered widespread attention for model merging techniques, especially training-free methods which combine model capabilities within the parameter space. However, two challenges remain: (1) uniform treatment of all parameters leads to performance degradation; (2) search-based algorithms are often inefficient. In this paper, we present an innovative framework termed Reinforced Model Merging (RMM), which encompasses an environment and agent tailored for merging tasks. These components interact to execute layer-wise merging actions, aiming to search the optimal merging architecture. Notably, RMM operates without any gradient computations on the original models, rendering it feasible for edge devices. Furthermore, by utilizing data subsets during the evaluation process, we addressed the bottleneck in the reward feedback phase, thereby accelerating RMM by up to 100 times. Extensive experiments demonstrate that RMM achieves state-of-the-art performance across various vision and NLP datasets and effectively overcomes the limitations of the existing baseline methods. Our code is available at this https URL. 

**Abstract (ZH)**: 大型语言模型的成功引起了对模型合并技术的广泛关注，尤其是无需训练的结合模型能力的方法。然而，仍存在两个挑战：（1）所有参数的统一处理会导致性能下降；（2）基于搜索的算法往往效率低下。本文提出了一种名为Reinforced Model Merging (RMM)的创新框架，该框架包含为合并任务量身定制的环境和智能体。这些组件交互执行逐层合并操作，以搜索最优的合并架构。值得注意的是，RMM不进行原模型的任何梯度计算，使其适用于边缘设备。此外，通过在评估过程中使用数据子集，我们解决了奖励反馈阶段的瓶颈，从而使RMM提速高达100倍。广泛实验表明，RMM在各种视觉和自然语言处理数据集上取得了最先进的性能，并有效克服了现有基线方法的局限性。我们的代码可在以下链接获取。 

---
# Knowledge Graphs as World Models for Semantic Material-Aware Obstacle Handling in Autonomous Vehicles 

**Title (ZH)**: 知识图谱作为语义材料感知障碍物处理的世界模型在自动驾驶车辆中的应用 

**Authors**: Ayush Bheemaiah, Seungyong Yang  

**Link**: [PDF](https://arxiv.org/pdf/2503.21232)  

**Abstract**: The inability of autonomous vehicles (AVs) to infer the material properties of obstacles limits their decision-making capacity. While AVs rely on sensor systems such as cameras, LiDAR, and radar to detect obstacles, this study suggests combining sensors with a knowledge graph (KG)-based world model to improve AVs' comprehension of physical material qualities. Beyond sensor data, AVs can infer qualities such as malleability, density, and elasticity using a semantic KG that depicts the relationships between obstacles and their attributes. Using the CARLA autonomous driving simulator, we evaluated AV performance with and without KG integration. The findings demonstrate that the KG-based method improves obstacle management, which allows AVs to use material qualities to make better decisions about when to change lanes or apply emergency braking. For example, the KG-integrated AV changed lanes for hard impediments like traffic cones and successfully avoided collisions with flexible items such as plastic bags by passing over them. Compared to the control system, the KG framework demonstrated improved responsiveness to obstacles by resolving conflicting sensor data, causing emergency stops for 13.3% more cases. In addition, our method exhibits a 6.6% higher success rate in lane-changing maneuvers in experimental scenarios, particularly for larger, high-impact obstacles. While we focus particularly on autonomous driving, our work demonstrates the potential of KG-based world models to improve decision-making in embodied AI systems and scale to other domains, including robotics, healthcare, and environmental simulation. 

**Abstract (ZH)**: 自主车辆难以推断障碍物的材料属性限制了其决策能力。本文建议通过将传感器与基于知识图谱（KG）的世界模型相结合，提升自主车辆对物理材料属性的理解。除了传感器数据外，自主车辆可以利用描述障碍物及其属性之间关系的语义知识图谱来推断柔韧性、密度和弹性等特性。通过CARLA自主驾驶模拟器评估了集成和未集成KG的自主车辆性能。研究结果表明，基于知识图谱的方法可以改善障碍物管理，使自主车辆能够利用材料属性在变更车道或紧急制动等决策时做出更合理的判断。例如，集成知识图谱的自主车辆在面对交通锥等坚硬障碍时会变道，并成功通过塑料袋等柔性物品以避免碰撞。与对照系统相比，基于知识图谱的框架在13.3%更多的情况下实现了紧急停靠，且在实验场景中，车道变更操作的成功率提高了6.6%，特别是对于大型、高冲击力的障碍物。尽管本文重点放在自主驾驶上，但研究展示了基于知识图谱的世界模型在改进具身人工智能系统决策、并扩展到其他领域（如机器人技术、医疗和环境模拟）中的潜力。 

---
# Integrating Large Language Models For Monte Carlo Simulation of Chemical Reaction Networks 

**Title (ZH)**: 将大型语言模型集成用于化学反应网络的蒙特卡ロ模拟 

**Authors**: Sadikshya Gyawali, Ashwini Mandal, Manish Dahal, Manish Awale, Sanjay Rijal, Shital Adhikari, Vaghawan Ojha  

**Link**: [PDF](https://arxiv.org/pdf/2503.21178)  

**Abstract**: Chemical reaction network is an important method for modeling and exploring complex biological processes, bio-chemical interactions and the behavior of different dynamics in system biology. But, formulating such reaction kinetics takes considerable time. In this paper, we leverage the efficiency of modern large language models to automate the stochastic monte carlo simulation of chemical reaction networks and enable the simulation through the reaction description provided in the form of natural languages. We also integrate this process into widely used simulation tool Copasi to further give the edge and ease to the modelers and researchers. In this work, we show the efficacy and limitations of the modern large language models to parse and create reaction kinetics for modelling complex chemical reaction processes. 

**Abstract (ZH)**: 化学反应网络是建模和探索复杂生物过程、生物化学相互作用及系统生物学中不同动力学行为的重要方法。但是，制定这样的反应动力学需要耗费大量时间。本文利用现代大型语言模型的效率自动化化学反应网络的随机蒙特卡洛模拟，并通过自然语言提供的反应描述实现模拟。我们还将此过程集成到广泛使用的模拟工具Copasi中，进一步为建模者和研究人员提供便利。本文展示了现代大型语言模型在解析和生成用于建模复杂化学反应过程的反应动力学方面的有效性和局限性。 

---
# A computational theory of evaluation for parameterisable subject 

**Title (ZH)**: 可参数化的主题评估计算理论 

**Authors**: Hedong Yan  

**Link**: [PDF](https://arxiv.org/pdf/2503.21138)  

**Abstract**: Evaluation is critical to advance decision making across domains, yet existing methodologies often struggle to balance theoretical rigor and practical scalability. In order to reduce the cost of experimental evaluation, we introduce a computational theory of evaluation for parameterisable subjects. We prove upper bounds of generalized evaluation error and generalized causal effect error of evaluation metric on subject. We also prove efficiency, and consistency to estimated causal effect of subject on metric by prediction. To optimize evaluation models, we propose a meta-learner to handle heterogeneous evaluation subjects space. Comparing with other computational approaches, our (conditional) evaluation model reduced 24.1%-99.0% evaluation errors across 12 scenes, including individual medicine, scientific simulation, business activities, and quantum trade. The evaluation time is reduced 3-7 order of magnitude comparing with experiments or simulations. 

**Abstract (ZH)**: 评价对于跨领域决策制定至关重要，但现有方法往往难以在理论严谨性和实际可扩展性之间取得平衡。为了降低实验评价成本，我们引入了一种参数可调主体的计算评价理论。我们证明了评价指标在主体上的广义评价误差和广义因果效应误差的上界。我们还证明了评价模型可以通过预测一致性和效率估计主体的因果效应。为了优化评价模型，我们提出了一种元学习器来处理异质评价主体空间。与其它计算方法相比，我们的（条件）评价模型在12个场景中（包括个体医学、科学模拟、商业活动和量子交易）降低了24.1%-99.0%的评价误差，并将评价时间减少了3到7个数量级。 

---
# AskSport: Web Application for Sports Question-Answering 

**Title (ZH)**: AskSport: 体育问答Web应用 

**Authors**: Enzo B Onofre, Leonardo M P Moraes, Cristina D Aguiar  

**Link**: [PDF](https://arxiv.org/pdf/2503.21067)  

**Abstract**: This paper introduces AskSport, a question-answering web application about sports. It allows users to ask questions using natural language and retrieve the three most relevant answers, including related information and documents. The paper describes the characteristics and functionalities of the application, including use cases demonstrating its ability to return names and numerical values. AskSport and its implementation are available for public access on HuggingFace. 

**Abstract (ZH)**: 本文介绍了一款关于体育领域的问答网络应用AskSport。用户可以使用自然语言提出问题，并获取与问题最相关的三个答案，包括相关的信息和文档。文章描述了该应用的特性与功能，包括展示了其返回姓名和数值的能力。AskSport及其实现已在HuggingFace上对公众开放。 

---
# The Art of Tool Interface Design 

**Title (ZH)**: 工具接口设计的艺术 

**Authors**: Yunnan Wu, Paul Chen, Deshank Baranwal, Jinlong Zhou, Jian Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2503.21036)  

**Abstract**: We present an agentic framework, Thinker, which achieves state of art performance in challenging reasoning tasks for realistic customer service scenarios that involve complex business logic and human interactions via long horizons. On the $\tau$-bench retail dataset, Thinker achieves 82.6\% success rate with GPT-4o (version 2024-06-01) (baseline: 68.3\%), and 81.9\% success rate with Llama-3.1 405B (baseline: 49.6\%), without any fine-tuning. Thinker effectively closes the gap in reasoning capabilities between the base models by introducing proper structure.
The key features of the Thinker framework are: (1) State-Machine Augmented Generation (SMAG), which represents business logic as state machines and the LLM uses state machines as tools. (2) Delegation of tasks from the main reasoning loop to LLM-powered tools. (3) Adaptive context management.
Our prompting-only solution achieves signficant gains, while still maintaining a standard agentic architecture with a ReAct style reasoning loop. The key is to innovate on the tool interface design, as exemplified by SMAG and the LLM-powered tools. 

**Abstract (ZH)**: 一种实现卓越推理性能的代理框架：Thinker及其在现实客户服务场景中的应用 

---
# DEMENTIA-PLAN: An Agent-Based Framework for Multi-Knowledge Graph Retrieval-Augmented Generation in Dementia Care 

**Title (ZH)**: DEMENTIA-PLAN：一种用于痴呆护理中的多知识图谱检索增强生成的基于代理的框架 

**Authors**: Yutong Song, Chenhan Lyu, Pengfei Zhang, Sabine Brunswicker, Nikil Dutt, Amir Rahmani  

**Link**: [PDF](https://arxiv.org/pdf/2503.20950)  

**Abstract**: Mild-stage dementia patients primarily experience two critical symptoms: severe memory loss and emotional instability. To address these challenges, we propose DEMENTIA-PLAN, an innovative retrieval-augmented generation framework that leverages large language models to enhance conversational support. Our model employs a multiple knowledge graph architecture, integrating various dimensional knowledge representations including daily routine graphs and life memory graphs. Through this multi-graph architecture, DEMENTIA-PLAN comprehensively addresses both immediate care needs and facilitates deeper emotional resonance through personal memories, helping stabilize patient mood while providing reliable memory support. Our notable innovation is the self-reflection planning agent, which systematically coordinates knowledge retrieval and semantic integration across multiple knowledge graphs, while scoring retrieved content from daily routine and life memory graphs to dynamically adjust their retrieval weights for optimized response generation. DEMENTIA-PLAN represents a significant advancement in the clinical application of large language models for dementia care, bridging the gap between AI tools and caregivers interventions. 

**Abstract (ZH)**: 轻度痴呆患者主要经历两种关键症状：严重的记忆力丧失和情绪不稳定。为应对这些挑战，我们提出DEMENTIA-PLAN，一种创新的检索增强生成框架，利用大型语言模型提高对话支持。该模型采用多知识图谱架构，整合了包括日常生活图和生活记忆图在内的多种维度的知识表示。通过这种多图架构，DEMENTIA-PLAN全面满足即时护理需求，并通过个人记忆加深情感共鸣，帮助稳定患者情绪，同时提供可靠的记忆支持。我们的重要创新是自省规划代理，该代理系统地协调跨多个知识图谱的知识检索和语义集成，并根据日常生活图和生活记忆图检索的内容为优化响应生成动态调整检索权重。DEMENTIA-PLAN代表了大型语言模型在痴呆护理临床应用中的重大进步，填补了人工智能工具与护理干预之间的差距。 

---
# StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion 

**Title (ZH)**: StyleMotif: 多模态运动风格化用以实现风格-内容交叉融合 

**Authors**: Ziyu Guo, Young Yoon Lee, Joseph Liu, Yizhak Ben-Shabat, Victor Zordan, Mubbasir Kapadia  

**Link**: [PDF](https://arxiv.org/pdf/2503.21775)  

**Abstract**: We present StyleMotif, a novel Stylized Motion Latent Diffusion model, generating motion conditioned on both content and style from multiple modalities. Unlike existing approaches that either focus on generating diverse motion content or transferring style from sequences, StyleMotif seamlessly synthesizes motion across a wide range of content while incorporating stylistic cues from multi-modal inputs, including motion, text, image, video, and audio. To achieve this, we introduce a style-content cross fusion mechanism and align a style encoder with a pre-trained multi-modal model, ensuring that the generated motion accurately captures the reference style while preserving realism. Extensive experiments demonstrate that our framework surpasses existing methods in stylized motion generation and exhibits emergent capabilities for multi-modal motion stylization, enabling more nuanced motion synthesis. Source code and pre-trained models will be released upon acceptance. Project Page: this https URL 

**Abstract (ZH)**: StyleMotif：一种新颖的风格化运动潜扩散模型，该模型根据多种模态的内容和风格生成运动。 

---
# Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence 

**Title (ZH)**: 稳定注册框架下的3D形状对应：Stable-SCore 

**Authors**: Haolin Liu, Xiaohang Zhan, Zizheng Yan, Zhongjin Luo, Yuxin Wen, Xiaoguang Han  

**Link**: [PDF](https://arxiv.org/pdf/2503.21766)  

**Abstract**: Establishing character shape correspondence is a critical and fundamental task in computer vision and graphics, with diverse applications including re-topology, attribute transfer, and shape interpolation. Current dominant functional map methods, while effective in controlled scenarios, struggle in real situations with more complex challenges such as non-isometric shape discrepancies. In response, we revisit registration-for-correspondence methods and tap their potential for more stable shape correspondence estimation. To overcome their common issues including unstable deformations and the necessity for careful pre-alignment or high-quality initial 3D correspondences, we introduce Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence. We first re-purpose a foundation model for 2D character correspondence that ensures reliable and stable 2D mappings. Crucially, we propose a novel Semantic Flow Guided Registration approach that leverages 2D correspondence to guide mesh deformations. Our framework significantly surpasses existing methods in challenging scenarios, and brings possibilities for a wide array of real applications, as demonstrated in our results. 

**Abstract (ZH)**: 建立字符形状对应关系是计算机视觉和图形学中一个关键且基础的任务，广泛应用于重新拓扑、属性迁移和形状插值等领域。针对当前主导的功能映射方法在应对非等参形变等复杂挑战时的局限性，我们重新审视了基于注册的对应方法，并挖掘其潜在优势以获得更稳定的形状对应估计。为克服其常见的不稳定形变和需要精细预对齐或高质量初值3D对应等问题，我们提出了一种稳定的基于注册的3D形状对应框架——Stable-SCore。我们首先将一个基础模型重新用于2D字符对应，确保可靠的2D映射。关键的是，我们提出了一种基于语义流的注册方法，利用2D对应关系指导网格变形。我们的框架在挑战性场景中显著超越了现有方法，并为广泛的实际应用提供了可能性，如我们在实验结果中所展示的。 

---
# Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video 

**Title (ZH)**: Uni4D: 统一单视频来源的4D建模视觉基础模型 

**Authors**: David Yifan Yao, Albert J. Zhai, Shenlong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2503.21761)  

**Abstract**: This paper presents a unified approach to understanding dynamic scenes from casual videos. Large pretrained vision foundation models, such as vision-language, video depth prediction, motion tracking, and segmentation models, offer promising capabilities. However, training a single model for comprehensive 4D understanding remains challenging. We introduce Uni4D, a multi-stage optimization framework that harnesses multiple pretrained models to advance dynamic 3D modeling, including static/dynamic reconstruction, camera pose estimation, and dense 3D motion tracking. Our results show state-of-the-art performance in dynamic 4D modeling with superior visual quality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the effectiveness of repurposing visual foundation models for 4D understanding. 

**Abstract (ZH)**: 本文提出了一种统一方法，以理解来自随手拍摄视频中的动态场景。大型预训练视觉基础模型，如视觉语言模型、视频深度预测模型、运动跟踪模型和分割模型提供了有力的能力。然而，为全面的4D理解训练单一模型仍然具有挑战性。我们引入了Uni4D，这是一种多阶段优化框架，利用多个预训练模型推进动态3D建模，包括静态/动态重建、相机姿态估计和密集3D运动跟踪。我们的结果表明，Uni4D在动态4D建模方面取得了最先进的性能，并且具有更优秀的视觉质量。值得注意的是，Uni4D无需重新训练或微调，突显了重新利用视觉基础模型进行4D理解的有效性。 

---
# Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck 

**Title (ZH)**: Fwd2Bot: 基于双前向瓶颈的LVLM视觉词元压缩 

**Authors**: Adrian Bulat, Yassine Ouali, Georgios Tzimiropoulos  

**Link**: [PDF](https://arxiv.org/pdf/2503.21757)  

**Abstract**: In this work, we aim to compress the vision tokens of a Large Vision Language Model (LVLM) into a representation that is simultaneously suitable for (a) generative and (b) discriminative tasks, (c) is nearly lossless, and (d) is storage-efficient. We propose a novel compression approach, called Fwd2Bot, that uses the LVLM itself to compress the visual information in a task-agnostic manner. At the core of Fwd2bot there exists a "double-forward pass" training strategy, whereby, during the first forward pass, the LLM (of the LVLM) creates a bottleneck by condensing the visual information into a small number of summary tokens. Then, using the same LLM, the second forward pass processes the language instruction(s) alongside the summary tokens, used as a direct replacement for the image ones. The training signal is provided by two losses: an autoregressive one applied after the second pass that provides a direct optimization objective for compression, and a contrastive loss, applied after the first pass, that further boosts the representation strength, especially for discriminative tasks. The training is further enhanced by stage-specific adapters. We accompany the proposed method by an in-depth ablation study. Overall, Fwd2Bot results in highly-informative compressed representations suitable for both generative and discriminative tasks. For generative tasks, we offer a 2x higher compression rate without compromising the generative capabilities, setting a new state-of-the-art result. For discriminative tasks, we set a new state-of-the-art on image retrieval and compositionality. 

**Abstract (ZH)**: 本研究表明，我们旨在压缩大型视觉语言模型（LVLM）的视觉词嵌入，使其同时适用于生成和判别任务，并且几乎是无损的且存储效率高。我们提出了一种名为Fwd2Bot的新型压缩方法，该方法使用LVLM本身以任务无关的方式压缩视觉信息。Fwd2Bot的核心是“双前向传递”训练策略，在第一个前向传递中，LLM（LVLM的一部分）通过将视觉信息凝缩成少量摘要词嵌入来创建瓶颈。然后，在第二个前向传递中，使用相同的LLM处理语言指令和摘要词嵌入，后者作为图像词嵌入的直接替代。训练信号由两种损失构成：一种在第二个传递之后应用的自回归损失，提供压缩的直接优化目标，以及一种在第一个传递之后应用的对比损失，进一步增强表示强度，尤其是在判别任务方面。通过特定阶段的适配器进一步增强了训练性能。我们通过详尽的消融研究来支持所提出的方法。总体而言，Fwd2Bot产生了既适合生成任务也适合判别任务的高信息量压缩表示。对于生成任务，我们实现了2倍的压缩率而不牺牲生成能力，达到新的性能基准。对于判别任务，我们在图像检索和组合性方面达到了新的性能基准。 

---
# CTRL-O: Language-Controllable Object-Centric Visual Representation Learning 

**Title (ZH)**: CTRL-O: 语言可控的以对象为中心的视觉表示学习 

**Authors**: Aniket Didolkar, Andrii Zadaianchuk, Rabiul Awal, Maximilian Seitzer, Efstratios Gavves, Aishwarya Agrawal  

**Link**: [PDF](https://arxiv.org/pdf/2503.21747)  

**Abstract**: Object-centric representation learning aims to decompose visual scenes into fixed-size vectors called "slots" or "object files", where each slot captures a distinct object. Current state-of-the-art object-centric models have shown remarkable success in object discovery in diverse domains, including complex real-world scenes. However, these models suffer from a key limitation: they lack controllability. Specifically, current object-centric models learn representations based on their preconceived understanding of objects, without allowing user input to guide which objects are represented. Introducing controllability into object-centric models could unlock a range of useful capabilities, such as the ability to extract instance-specific representations from a scene. In this work, we propose a novel approach for user-directed control over slot representations by conditioning slots on language descriptions. The proposed ConTRoLlable Object-centric representation learning approach, which we term CTRL-O, achieves targeted object-language binding in complex real-world scenes without requiring mask supervision. Next, we apply these controllable slot representations on two downstream vision language tasks: text-to-image generation and visual question answering. The proposed approach enables instance-specific text-to-image generation and also achieves strong performance on visual question answering. 

**Abstract (ZH)**: 基于物体的表示学习旨在将视觉场景分解为固定大小的向量“槽”或“物体文件”，每个槽捕获一个独立的物体。当前的基于物体的表示学习模型在多样性领域，包括复杂的真实世界场景中展现出了显著的物体发现能力。然而，这些模型存在一个关键局限：缺乏可控性。具体而言，现有的基于物体的表示学习模型基于它们预设的物体理解来学习表示，而不允许用户输入来引导哪些物体被表示。将可控性引入基于物体的表示学习模型可以解锁一系列有用的功能，例如从场景中提取实例特定的表示。在本工作中，我们提出了一种新的方法，通过将槽条件化于语言描述，实现用户导向的槽表示控制。我们提出的方法CTRL-O能在不需要掩码监督的情况下实现复杂真实世界场景中的目标物体-语言绑定。然后，我们在这两种下游视觉语言任务上应用这些可控的槽表示：文本到图像生成和视觉问答。所提出的方法实现了实例特定的文本到图像生成，并且在视觉问答任务上也取得了强劲的表现。 

---
# GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics 

**Title (ZH)**: GateLens: 一种增强推理的汽车软件发布分析LLM代理 

**Authors**: Arsham Gholamzadeh Khoee, Shuai Wang, Yinan Yu, Robert Feldt, Dhasarathy Parthasarathy  

**Link**: [PDF](https://arxiv.org/pdf/2503.21735)  

**Abstract**: Ensuring the reliability and effectiveness of software release decisions is critical, particularly in safety-critical domains like automotive systems. Precise analysis of release validation data, often presented in tabular form, plays a pivotal role in this process. However, traditional methods that rely on manual analysis of extensive test datasets and validation metrics are prone to delays and high costs. Large Language Models (LLMs) offer a promising alternative but face challenges in analytical reasoning, contextual understanding, handling out-of-scope queries, and processing structured test data consistently; limitations that hinder their direct application in safety-critical scenarios. This paper introduces GateLens, an LLM-based tool for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and then generates optimized Python code. It outperforms the baseline system on benchmarking datasets, achieving higher F1 scores and handling complex and ambiguous queries with greater robustness. Ablation studies confirm the critical role of the RA module, with performance dropping sharply when omitted. Industrial evaluations reveal that GateLens reduces analysis time by over 80% while maintaining high accuracy and reliability. As demonstrated by presented results, GateLens achieved high performance without relying on few-shot examples, showcasing strong generalization across various query types from diverse company roles. Insights from deploying GateLens with a partner automotive company offer practical guidance for integrating AI into critical workflows such as release validation. Results show that by automating test result analysis, GateLens enables faster, more informed, and dependable release decisions, and can thus advance software scalability and reliability in automotive systems. 

**Abstract (ZH)**: 确保软件发布决策的可靠性和有效性在诸如汽车系统这样的安全关键领域尤为重要。精确分析发布验证数据，通常以表格形式呈现，在这一过程中发挥着关键作用。然而，依赖手动分析大量测试数据集和验证指标的传统方法容易导致延误和高成本。大型语言模型（LLMs）提供了一种有希望的替代方案，但在分析推理、情境理解、处理范围外查询以及一致处理结构化测试数据方面面临挑战；这些限制阻碍了它们在安全关键场景中的直接应用。本文介绍了GateLens，这是一个基于LLM的汽车领域表格数据分析工具。GateLens将自然语言查询转换为关系代数（RA）表达式，并生成优化的Python代码。GateLens在基准测试数据集上的表现优于基线系统，实现更高的F1分数，并且在处理复杂和模糊查询时更具鲁棒性。消融研究证实了RA模块的至关重要性，将其省略会导致性能急剧下降。工业评估表明，GateLens将分析时间减少了80%以上，同时仍保持高度准确性和可靠性。通过对GateLens在合作伙伴汽车公司的部署结果进行展示，GateLens在各种类型查询中实现了良好的泛化能力，展示了其在不同公司角色中的强大应用潜力。通过GateLens自动化测试结果分析，工业应用表明GateLens能够更快、更准确、更可靠地做出发布决策，从而推动汽车系统软件的扩展性和可靠性。 

---
# ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation 

**Title (ZH)**: ReaRAG：迭代检索增强生成的知识指导推理提升大型推理模型的事实性 

**Authors**: Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, Lei Hou, Juanzi Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.21729)  

**Abstract**: Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG). 

**Abstract (ZH)**: 增强事实性的推理模型：ReaRAG及其在多跳问答任务中的应用 

---
# Collab: Controlled Decoding using Mixture of Agents for LLM Alignment 

**Title (ZH)**: Collab: 使用混合智能体进行控制解码以实现语言模型对齐 

**Authors**: Souradip Chakraborty, Sujay Bhatt, Udari Madhushani Sehwag, Soumya Suvra Ghosal, Jiahao Qiu, Mengdi Wang, Dinesh Manocha, Furong Huang, Alec Koppel, Sumitra Ganesh  

**Link**: [PDF](https://arxiv.org/pdf/2503.21720)  

**Abstract**: Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences and broader utilities, but it requires updating billions of model parameters, which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agent-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, Collab surpasses the current SoTA decoding strategy, achieving an improvement of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate. 

**Abstract (ZH)**: 多代理解码策略在大型语言模型对齐中的应用：增强目标任务的测试时性能 

---
# Outlier dimensions favor frequent tokens in language model 

**Title (ZH)**: 异常维度倾向于青睐语言模型中的常用词 

**Authors**: Iuri Macocco, Nora Graichen, Gemma Boleda, Marco Baroni  

**Link**: [PDF](https://arxiv.org/pdf/2503.21718)  

**Abstract**: We study last-layer outlier dimensions, this http URL that display extreme activations for the majority of inputs. We show that outlier dimensions arise in many different modern language models, and trace their function back to the heuristic of constantly predicting frequent words. We further show how a model can block this heuristic when it is not contextually appropriate, by assigning a counterbalancing weight mass to the remaining dimensions, and we investigate which model parameters boost outlier dimensions and when they arise during training. We conclude that outlier dimensions are a specialized mechanism discovered by many distinct models to implement a useful token prediction heuristic. 

**Abstract (ZH)**: 我们研究最后一层异常维度，这些维度对多数输入显示极端激活值。我们展示了异常维度在许多现代语言模型中出现，并将它们的功能追溯到频繁词预测的启发式方法。我们还展示了当该启发式方法在上下文中不适用时，模型可以通过给剩余维度分配平衡权重来阻止这种启发式方法。我们研究了哪些模型参数可以增强异常维度，并在训练过程中它们何时出现。我们得出结论，异常维度是许多不同模型发现的一种专门机制，用于实现有用的词预测启发式方法。 

---
# Elementwise Layer Normalization 

**Title (ZH)**: 元素wise层规范化 

**Authors**: Felix Stollenwerk  

**Link**: [PDF](https://arxiv.org/pdf/2503.21708)  

**Abstract**: A recent paper proposed Dynamic Tanh (DyT) as a drop-in replacement for Layer Normalization. Although the method is empirically well-motivated and appealing from a practical point of view, it lacks a theoretical foundation. In this work, we derive DyT mathematically and show that a well-defined approximation is needed to do so. By dropping said approximation, an alternative element-wise transformation is obtained, which we call Elementwise Layer Normalization (ELN). We demonstrate that ELN resembles Layer Normalization more accurately than DyT does. 

**Abstract (ZH)**: 最近一篇论文提出了Dynamic Tanh (DyT)作为一种层规范化层的即插即用替代方案。尽管该方法从实践的角度来看具有经验上的合理性和吸引力，但缺乏理论基础。在本工作中，我们从数学上推导出DyT，并指出需要一个明确定义的近似来实现这一点。通过舍去该近似，我们获得了一种元素-wise变换，我们称之为元素层规范化（ELN）。我们证明ELN比DyT更能准确地类似于层规范化。 

---
# MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX 

**Title (ZH)**: MAVERIX: 多模态音视频评价推理索引 

**Authors**: Liuyue Xie, George Z. Wei, Avik Kuthiala, Ce Zheng, Ananya Bal, Mosam Dabhi, Liting Wen, Taru Rustagi, Ethan Lai, Sushil Khyalia, Rohan Choudhury, Morteza Ziyadi, Xu Zhang, Hao Yang, László A. Jeni  

**Link**: [PDF](https://arxiv.org/pdf/2503.21699)  

**Abstract**: Frontier models have either been language-only or have primarily focused on vision and language modalities. Although recent advancements in models with vision and audio understanding capabilities have shown substantial progress, the field lacks a standardized evaluation framework for thoroughly assessing their cross-modality perception performance. We introduce MAVERIX~(Multimodal Audio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and 2,556 questions explicitly designed to evaluate multimodal models through tasks that necessitate close integration of video and audio information. MAVERIX uniquely provides models with audiovisual tasks, closely mimicking the multimodal perceptual experiences available to humans during inference and decision-making processes. To our knowledge, MAVERIX is the first benchmark aimed explicitly at assessing comprehensive audiovisual integration. Experiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show performance approaching human levels (around 70% accuracy), while human experts reach near-ceiling performance (95.1%). With standardized evaluation protocols, a rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a challenging testbed for advancing audiovisual multimodal intelligence. 

**Abstract (ZH)**: 多模态音频-视觉评价索引（MAVERIX）：一种新颖的基准测试，用于评估多模态模型的跨模态感知性能 

---
# AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation 

**Title (ZH)**: AMA-SAM：针对高保真组织学细胞核分割的对抗多领域对齐Segment Anything Model 

**Authors**: Jiahe Qian, Yaoyu Fang, Jinkui Hao, Bo Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2503.21695)  

**Abstract**: Accurate segmentation of cell nuclei in histopathology images is essential for numerous biomedical research and clinical applications. However, existing cell nucleus segmentation methods only consider a single dataset (i.e., primary domain), while neglecting to leverage supplementary data from diverse sources (i.e., auxiliary domains) to reduce overfitting and enhance the performance. Although incorporating multiple datasets could alleviate overfitting, it often exacerbates performance drops caused by domain shifts. In this work, we introduce Adversarial Multi-domain Alignment of Segment Anything Model (AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these obstacles through two key innovations. First, we propose a Conditional Gradient Reversal Layer (CGRL), a multi-domain alignment module that harmonizes features from diverse domains to promote domain-invariant representation learning while preserving crucial discriminative features for the primary dataset. Second, we address SAM's inherent low-resolution output by designing a High-Resolution Decoder (HR-Decoder), which directly produces fine-grained segmentation maps in order to capture intricate nuclei boundaries in high-resolution histology images. To the best of our knowledge, this is the first attempt to adapt SAM for multi-dataset learning with application to histology nuclei segmentation. We validate our method on several publicly available datasets, demonstrating consistent and significant improvements over state-of-the-art approaches. 

**Abstract (ZH)**: 基于 adversarial 多域对齐的 segment anything 模型在组织病理学细胞核分割中的应用 

---
# Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data 

**Title (ZH)**: 逐级渲染精炼：adapt Stable Diffusion 以实现无需3D数据的即时文本到网格生成 

**Authors**: Zhiyuan Ma, Xinyue Liang, Rongyuan Wu, Xiangyu Zhu, Zhen Lei, Lei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2503.21694)  

**Abstract**: It is highly desirable to obtain a model that can generate high-quality 3D meshes from text prompts in just seconds. While recent attempts have adapted pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer from poor quality due to the lack of sufficient high-quality 3D training data. Aiming at overcoming the data shortage, we propose a novel training scheme, termed as Progressive Rendering Distillation (PRD), eliminating the need for 3D ground-truths by distilling multi-view diffusion models and adapting SD into a native 3D generator. In each iteration of training, PRD uses the U-Net to progressively denoise the latent from random noise for a few steps, and in each step it decodes the denoised latent into 3D output. Multi-view diffusion models, including MVDream and RichDreamer, are used in joint with SD to distill text-consistent textures and geometries into the 3D outputs through score distillation. Since PRD supports training without 3D ground-truths, we can easily scale up the training data and improve generation quality for challenging text prompts with creative concepts. Meanwhile, PRD can accelerate the inference speed of the generation model in just a few steps. With PRD, we train a Triplane generator, namely TriplaneTurbo, which adds only $2.5\%$ trainable parameters to adapt SD for Triplane generation. TriplaneTurbo outperforms previous text-to-3D generators in both efficiency and quality. Specifically, it can produce high-quality 3D meshes in 1.2 seconds and generalize well for challenging text input. The code is available at this https URL. 

**Abstract (ZH)**: 高质异地图生成：一种渐进渲染蒸馏方法 

---
# Intelligent IoT Attack Detection Design via ODLLM with Feature Ranking-based Knowledge Base 

**Title (ZH)**: 基于特征排名知识库的ODLLM智能物联网攻击检测设计 

**Authors**: Satvik Verma, Qun Wang, E. Wes Bethel  

**Link**: [PDF](https://arxiv.org/pdf/2503.21674)  

**Abstract**: The widespread adoption of Internet of Things (IoT) devices has introduced significant cybersecurity challenges, particularly with the increasing frequency and sophistication of Distributed Denial of Service (DDoS) attacks. Traditional machine learning (ML) techniques often fall short in detecting such attacks due to the complexity of blended and evolving patterns. To address this, we propose a novel framework leveraging On-Device Large Language Models (ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for intelligent IoT network attack detection. By implementing feature ranking techniques and constructing both long and short KBs tailored to model capacities, the proposed framework ensures efficient and accurate detection of DDoS attacks while overcoming computational and privacy limitations. Simulation results demonstrate that the optimized framework achieves superior accuracy across diverse attack types, especially when using compact models in edge computing environments. This work provides a scalable and secure solution for real-time IoT security, advancing the applicability of edge intelligence in cybersecurity. 

**Abstract (ZH)**: 物联网设备的广泛采用引入了重大的网络安全挑战，尤其是分布式拒绝服务（DDoS）攻击的频率和复杂性不断增加。传统的机器学习技术往往难以检测这类攻击，因为它们难以处理混杂且不断演化的模式。为此，我们提出了一种利用设备上大型语言模型（ODLLMs）并结合微调和知识库（KB）集成的新型框架，以实现智能物联网网络攻击检测。通过实施特征排名技术并构建适合模型容量的长短期知识库，所提出的框架能够在克服计算和隐私限制的同时，实现DDoS攻击的高效和准确检测。仿真结果表明，优化的框架在不同的攻击类型中实现了更高的准确性，尤其是在边缘计算环境中使用紧凑模型时更为突出。本研究为实时物联网安全提供了可扩展且安全的解决方案，并推动了边缘智能在网络安全中的应用。 

---
# COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing 

**Title (ZH)**: COMI-LINGUA: 印地语-英语代码混合多任务NLP专家标注大规模数据集 

**Authors**: Rajvee Sheth, Himanshu Beniwal, Mayank Singh  

**Link**: [PDF](https://arxiv.org/pdf/2503.21670)  

**Abstract**: The rapid growth of digital communication has driven the widespread use of code-mixing, particularly Hindi-English, in multilingual communities. Existing datasets often focus on romanized text, have limited scope, or rely on synthetic data, which fails to capture realworld language nuances. Human annotations are crucial for assessing the naturalness and acceptability of code-mixed text. To address these challenges, We introduce COMI-LINGUA, the largest manually annotated dataset for code-mixed text, comprising 100,970 instances evaluated by three expert annotators in both Devanagari and Roman scripts. The dataset supports five fundamental NLP tasks: Language Identification, Matrix Language Identification, Part-of-Speech Tagging, Named Entity Recognition, and Translation. We evaluate LLMs on these tasks using COMILINGUA, revealing limitations in current multilingual modeling strategies and emphasizing the need for improved code-mixed text processing capabilities. COMI-LINGUA is publically availabe at: this https URL. 

**Abstract (ZH)**: 数字通信的迅速增长推动了多语言社区中代码混用，特别是印地英语代码混用的普遍使用。现有数据集往往专注于罗马化文本，范围有限，或依赖合成数据，无法捕捉到现实世界的语言细微差别。人工标注对于评估代码混用文本的自然度和接受度至关重要。为应对这些挑战，我们引入了COMI-LINGUA，这是最大的人工标注代码混用文本数据集，包含100,970个实例，由三位专家注释者在德文那格里和罗马-script中进行评估。该数据集支持五项基本的NLP任务：语言识别、矩阵语言识别、词性标注、命名实体识别和翻译。我们使用COMILINGUA评估LLMs在这些任务上的表现，揭示了当前多语言建模策略的局限性，并强调了改进代码混用文本处理能力的必要性。COMI-LINGUA可在以下网址获取：this https URL。 

---
# Model Assembly Learning with Heterogeneous Layer Weight Merging 

**Title (ZH)**: 异质层权重合并的模型组装学习 

**Authors**: Yi-Kai Zhang, Jin Wang, Xu-Xiang Zhong, De-Chuan Zhan, Han-Jia Ye  

**Link**: [PDF](https://arxiv.org/pdf/2503.21657)  

**Abstract**: Model merging acquires general capabilities without extra data or training by combining multiple models' parameters. Previous approaches achieve linear mode connectivity by aligning parameters into the same loss basin using permutation invariance. In this paper, we introduce Model Assembly Learning (MAL), a novel paradigm for model merging that iteratively integrates parameters from diverse models in an open-ended model zoo to enhance the base model's capabilities. Unlike previous works that require identical architectures, MAL allows the merging of heterogeneous architectures and selective parameters across layers. Specifically, the base model can incorporate parameters from different layers of multiple pre-trained models. We systematically investigate the conditions and fundamental settings of heterogeneous parameter merging, addressing all possible mismatches in layer widths between the base and target models. Furthermore, we establish key laws and provide practical guidelines for effectively implementing MAL. 

**Abstract (ZH)**: 模型组装学习：一种通过迭代集成多样化模型参数以增强基础模型能力的新范式 

---
# When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in Morocco 

**Title (ZH)**: 当天文遇到AI：曼扎尔在摩洛哥新月可见性预测中的应用 

**Authors**: Yassir Lairgi  

**Link**: [PDF](https://arxiv.org/pdf/2503.21634)  

**Abstract**: The accurate determination of the beginning of each Hijri month is essential for religious, cultural, and administrative purposes. Manazel (The code and datasets are available at this https URL) addresses this challenge in Morocco by leveraging 13 years of crescent visibility data to refine the ODEH criterion, a widely used standard for lunar crescent visibility prediction. The study integrates two key features, the Arc of Vision (ARCV) and the total width of the crescent (W), to enhance the accuracy of lunar visibility assessments. A machine learning approach utilizing the Logistic Regression algorithm is employed to classify crescent visibility conditions, achieving a predictive accuracy of 98.83%. This data-driven methodology offers a robust and reliable framework for determining the start of the Hijri month, comparing different data classification tools, and improving the consistency of lunar calendar calculations in Morocco. The findings demonstrate the effectiveness of machine learning in astronomical applications and highlight the potential for further enhancements in the modeling of crescent visibility. 

**Abstract (ZH)**: Hijri月开始日期的精确确定对于宗教、文化和行政目的至关重要。Manazel（代码和数据集可在以下链接获取：这个httpsURL）通过利用13年的月相可见性数据来改进ODEH标准，解决摩洛哥月相可见性预测中的挑战。该研究结合了视界弧（ARCV）和月牙总宽度（W）两个关键特征，以提高月相可见性评估的准确性。采用逻辑回归算法的机器学习方法被用于分类月相可见性条件，预测准确率达到98.83%。这种数据驱动的方法为确定Hijri月的开始日期、比较不同的数据分类工具以及改进摩洛哥月历计算的一致性提供了坚实可靠的框架。研究结果展示了机器学习在天文学应用中的有效性，并指出了进一步增强月相可见性建模的潜力。 

---
# A Measure Based Generalizable Approach to Understandability 

**Title (ZH)**: 基于度量的一般化可理解性方法 

**Authors**: Vikas Kushwaha, Sruti Srinivasa Ragavan, Subhajit Roy  

**Link**: [PDF](https://arxiv.org/pdf/2503.21615)  

**Abstract**: Successful agent-human partnerships require that any agent generated information is understandable to the human, and that the human can easily steer the agent towards a goal. Such effective communication requires the agent to develop a finer-level notion of what is understandable to the human. State-of-the-art agents, including LLMs, lack this detailed notion of understandability because they only capture average human sensibilities from the training data, and therefore afford limited steerability (e.g., requiring non-trivial prompt engineering).
In this paper, instead of only relying on data, we argue for developing generalizable, domain-agnostic measures of understandability that can be used as directives for these agents. Existing research on understandability measures is fragmented, we survey various such efforts across domains, and lay a cognitive-science-rooted groundwork for more coherent and domain-agnostic research investigations in future. 

**Abstract (ZH)**: 成功的智能体-人类合作伙伴关系要求智能体生成的信息对于人类是可以理解的，人类可以轻松地引导智能体达成目标。这种有效的沟通需要智能体发展出对于人类可理解性的更精细的理解。当前最先进的智能体，包括大型语言模型，缺乏这种详细的可理解性概念，因为它们仅从训练数据中捕获平均水平的人类感受，并因此提供了有限的可控性（例如，需要复杂的提示工程）。本文中，我们不只依赖数据，而是主张开发通用且领域无关的可理解性度量，作为这些智能体的指令。现有的可理解性度量研究碎片化，我们对跨领域的相关努力进行了综述，并为未来更连贯和领域无关的研究奠定了认知科学基础。 

---
# Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing 

**Title (ZH)**: 提示、分割与征服：通过分割和分布式提示处理规避大型语言模型安全过滤器 

**Authors**: Johan Wahréus, Ahmed Hussain, Panos Papadimitratos  

**Link**: [PDF](https://arxiv.org/pdf/2503.21598)  

**Abstract**: Large Language Models (LLMs) have transformed task automation and content generation across various domains while incorporating safety filters to prevent misuse. We introduce a novel jailbreaking framework that employs distributed prompt processing combined with iterative refinements to bypass these safety measures, particularly in generating malicious code. Our architecture consists of four key modules: prompt segmentation, parallel processing, response aggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts across 10 cybersecurity categories, the framework achieves a 73.2% Success Rate (SR) in generating malicious code. Notably, our comparative analysis reveals that traditional single-LLM judge evaluation overestimates SRs (93.8%) compared to our LLM jury system (73.2%), with manual verification confirming that single-judge assessments often accept incomplete implementations. Moreover, we demonstrate that our distributed architecture improves SRs by 12% over the non-distributed approach in an ablation study, highlighting both the effectiveness of distributed prompt processing and the importance of robust evaluation methodologies in assessing jailbreak attempts. 

**Abstract (ZH)**: 大规模语言模型（LLMs）已在各种领域中转型任务自动化和内容生成，并包含安全过滤以防止误用。我们提出了一种新的脱束缚框架，该框架采用分布式提示处理结合迭代改进来绕过这些安全措施，特别是在生成恶意代码方面。该架构包括四个关键模块：提示分割、并行处理、响应聚合和基于LLM的陪审团评估。在10个网络安全类别下的500个恶意提示上进行测试，该框架在生成恶意代码方面实现了73.2%的成功率（SR）。值得注意的是，我们的对比分析表明，传统的单一LLM法官评估高估了成功率（93.8%），而我们的LLM陪审团系统为73.2%，人工验证确认单一法官评估经常接受不完整的实现。此外，我们展示在消融研究中，我们的分布式架构在不分布的方法上将成功率提高了12%，突显了分布式提示处理的有效性和稳健评估方法在评估脱束缚尝试中的重要性。 

---
# Critical Iterative Denoising: A Discrete Generative Model Applied to Graphs 

**Title (ZH)**: 关键迭代去噪：应用于图的离散生成模型 

**Authors**: Yoann Boget, Alexandros Kalousis  

**Link**: [PDF](https://arxiv.org/pdf/2503.21592)  

**Abstract**: Discrete Diffusion and Flow Matching models have significantly advanced generative modeling for discrete structures, including graphs. However, the time dependencies in the noising process of these models lead to error accumulation and propagation during the backward process. This issue, particularly pronounced in mask diffusion, is a known limitation in sequence modeling and, as we demonstrate, also impacts discrete diffusion models for graphs.
To address this problem, we propose a novel framework called Iterative Denoising, which simplifies discrete diffusion and circumvents the issue by assuming conditional independence across time. Additionally, we enhance our model by incorporating a Critic, which during generation selectively retains or corrupts elements in an instance based on their likelihood under the data distribution. Our empirical evaluations demonstrate that the proposed method significantly outperforms existing discrete diffusion baselines in graph generation tasks. 

**Abstract (ZH)**: 离散去噪和流匹配模型显著推进了离散结构（包括图）的生成建模。然而，这些模型的去噪过程中的时间依赖性导致了错误在反向过程中积累和传播。这一问题在掩码扩散中尤为明显，并且如我们所证明的，也影响着图的离散扩散模型。为解决这一问题，我们提出了一种新的框架——迭代去噪，通过假设时间上的条件独立性简化了离散扩散过程，并绕过了该问题。此外，我们通过引入一个裁判模块增强了模型，在生成过程中根据实例在数据分布下的似然性选择性地保留或篡改元素。我们的实证评价表明，所提出的方法在图生成任务中显著优于现有离散扩散基线方法。 

---
# AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion 

**Title (ZH)**: AlignDiff: 学习基于物理的相机对准通过扩散 

**Authors**: Liuyue Xie, Jiancong Guo, Ozan Cakmakci, Andre Araujo, Laszlo A. Jeni, Zhiheng Jia  

**Link**: [PDF](https://arxiv.org/pdf/2503.21581)  

**Abstract**: Accurate camera calibration is a fundamental task for 3D perception, especially when dealing with real-world, in-the-wild environments where complex optical distortions are common. Existing methods often rely on pre-rectified images or calibration patterns, which limits their applicability and flexibility. In this work, we introduce a novel framework that addresses these challenges by jointly modeling camera intrinsic and extrinsic parameters using a generic ray camera model. Unlike previous approaches, AlignDiff shifts focus from semantic to geometric features, enabling more accurate modeling of local distortions. We propose AlignDiff, a diffusion model conditioned on geometric priors, enabling the simultaneous estimation of camera distortions and scene geometry. To enhance distortion prediction, we incorporate edge-aware attention, focusing the model on geometric features around image edges, rather than semantic content. Furthermore, to enhance generalizability to real-world captures, we incorporate a large database of ray-traced lenses containing over three thousand samples. This database characterizes the distortion inherent in a diverse variety of lens forms. Our experiments demonstrate that the proposed method significantly reduces the angular error of estimated ray bundles by ~8.2 degrees and overall calibration accuracy, outperforming existing approaches on challenging, real-world datasets. 

**Abstract (ZH)**: 准确的相机标定是三维感知的基础任务，尤其是在处理复杂光学畸变常见的自然环境时。现有方法往往依赖于预校正图像或校准图案，这限制了它们的适用性和灵活性。在本文中，我们提出了一种新的框架，通过使用通用射线相机模型联合建模相机固有参数和外部参数来解决这些挑战。与之前的 Approaches 不同，AlignDiff 转而关注几何特征，从而更准确地建模局部畸变。我们提出了一种条件于几何先验的扩散模型 AlignDiff，能够同时估计相机畸变和场景几何。为了增强畸变预测，我们融入了边缘感知注意力机制，使模型专注于图像边缘周围的几何特征，而非语义内容。此外，为了增强对自然环境捕捉的泛化能力，我们引入了一个包含三千多个样本的大型射线追踪镜头数据库。该数据库表征了各种镜头形式固有的畸变特征。我们的实验表明，所提出的方法能够显著降低估计射线束的角度误差 (~8.2 度) 和整体标定精度，在挑战性的现实世界数据集上优于现有方法。 

---
# Magnitude-Phase Dual-Path Speech Enhancement Network based on Self-Supervised Embedding and Perceptual Contrast Stretch Boosting 

**Title (ZH)**: 基于自监督嵌入和感知对比拉伸增强的幅度-相位双路径语音增强网络 

**Authors**: Alimjan Mattursun, Liejun Wang, Yinfeng Yu, Chunyang Ma  

**Link**: [PDF](https://arxiv.org/pdf/2503.21571)  

**Abstract**: Speech self-supervised learning (SSL) has made great progress in various speech processing tasks, but there is still room for improvement in speech enhancement (SE). This paper presents BSP-MPNet, a dual-path framework that combines self-supervised features with magnitude-phase information for SE. The approach starts by applying the perceptual contrast stretching (PCS) algorithm to enhance the magnitude-phase spectrum. A magnitude-phase 2D coarse (MP-2DC) encoder then extracts coarse features from the enhanced spectrum. Next, a feature-separating self-supervised learning (FS-SSL) model generates self-supervised embeddings for the magnitude and phase components separately. These embeddings are fused to create cross-domain feature representations. Finally, two parallel RNN-enhanced multi-attention (REMA) mask decoders refine the features, apply them to the mask, and reconstruct the speech signal. We evaluate BSP-MPNet on the VoiceBank+DEMAND and WHAMR! datasets. Experimental results show that BSP-MPNet outperforms existing methods under various noise conditions, providing new directions for self-supervised speech enhancement research. The implementation of the BSP-MPNet code is available online\footnote[2]{this https URL. \label{s1}} 

**Abstract (ZH)**: 基于双路径框架的自监督语音增强（BSP-MPNet） 

---
# A Local Perspective-based Model for Overlapping Community Detection 

**Title (ZH)**: 基于局部视角的重叠社区检测模型 

**Authors**: Gaofeng Zhou, Rui-Feng Wang, Kangning Cui  

**Link**: [PDF](https://arxiv.org/pdf/2503.21558)  

**Abstract**: Community detection, which identifies densely connected node clusters with sparse between-group links, is vital for analyzing network structure and function in real-world systems. Most existing community detection methods based on GCNs primarily focus on node-level information while overlooking community-level features, leading to performance limitations on large-scale networks. To address this issue, we propose LQ-GCN, an overlapping community detection model from a local community perspective. LQ-GCN employs a Bernoulli-Poisson model to construct a community affiliation matrix and form an end-to-end detection framework. By adopting local modularity as the objective function, the model incorporates local community information to enhance the quality and accuracy of clustering results. Additionally, the conventional GCNs architecture is optimized to improve the model capability in identifying overlapping communities in large-scale networks. Experimental results demonstrate that LQ-GCN achieves up to a 33% improvement in Normalized Mutual Information (NMI) and a 26.3% improvement in Recall compared to baseline models across multiple real-world benchmark datasets. 

**Abstract (ZH)**: 基于局部社区视角的LQ-GCN重叠社区检测模型 

---
# SWI: Speaking with Intent in Large Language Models 

**Title (ZH)**: SWI: 用意而言在大型语言模型中 

**Authors**: Yuwei Yin, EunJeong Hwang, Giuseppe Carenini  

**Link**: [PDF](https://arxiv.org/pdf/2503.21544)  

**Abstract**: Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the model's underlying intention and provides high-level planning to guide subsequent analysis and communication. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on mathematical reasoning benchmarks consistently demonstrate the superiority of Speaking with Intent over Baseline (i.e., generation without explicit intent). Moreover, SWI outperforms answer-trigger prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive performance with the strong method ARR (Analyzing, Retrieving, and Reasoning). Additionally, the effectiveness and generalizability of SWI are solidified on reasoning-intensive question answering (QA) and text summarization benchmarks, where SWI brings consistent improvement to the Baseline generation. In text summarization, SWI-generated summaries exhibit greater accuracy, conciseness, and factual correctness, with fewer hallucinations. Furthermore, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. This proof-of-concept study creates a novel avenue for enhancing LLMs' reasoning abilities with cognitive notions. 

**Abstract (ZH)**: 意图，通常明确提出并计划好，作为推理和问题解决的认知框架。本文在大型语言模型（LLMs）中引入了“意图驱动说话”（Speaking with Intent，SWI）的概念，明确生成的意图体现在模型的底层意图中，并提供高层次规划以指导后续分析和沟通。通过模拟人类思维中的审慎和目的性思考，SWI 假设可以增强 LLM 的推理能力和生成质量。在数学推理基准测试中的大量实验一致表明，SWI 在推理能力方面优于基准方法（即没有明确意图的生成）。此外，SWI 在 Chain-of-Thought 和 Plan-and-Solve 回答触发提示方法中表现出优越性，并在强方法 ARR（分析、检索和推理）中保持了竞争力。在推理密集型问答（QA）和文本摘要基准测试中，SWI 也对基准生成带来了持续性的改进。在文本摘要中，SWI 生成的摘要表现出更高的精确度、简洁性和事实正确性，同时减少了幻觉现象。此外，人类评估证实了由 SWI 产生的意图的连贯性、有效性和可解释性。这一概念验证研究开辟了通过认知概念增强 LLM 推理能力的新途径。 

---
# LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing 

**Title (ZH)**: LOCATEdit: 基于图拉普拉斯优化跨注意力的局部化文本指导图像编辑 

**Authors**: Achint Soni, Meet Soni, Sirisha Rambhatla  

**Link**: [PDF](https://arxiv.org/pdf/2503.21541)  

**Abstract**: Text-guided image editing aims to modify specific regions of an image according to natural language instructions while maintaining the general structure and the background fidelity. Existing methods utilize masks derived from cross-attention maps generated from diffusion models to identify the target regions for modification. However, since cross-attention mechanisms focus on semantic relevance, they struggle to maintain the image integrity. As a result, these methods often lack spatial consistency, leading to editing artifacts and distortions. In this work, we address these limitations and introduce LOCATEdit, which enhances cross-attention maps through a graph-based approach utilizing self-attention-derived patch relationships to maintain smooth, coherent attention across image regions, ensuring that alterations are limited to the designated items while retaining the surrounding structure. \method consistently and substantially outperforms existing baselines on PIE-Bench, demonstrating its state-of-the-art performance and effectiveness on various editing tasks. Code can be found on this https URL 

**Abstract (ZH)**: 基于文本引导的图像编辑旨在根据自然语言指令修改图像的特定区域，同时保持整体结构和背景的真实性。现有方法利用源自扩散模型交叉注意力图的掩码来识别修改目标区域，但由于交叉注意力机制侧重于语义相关性，因此难以保持图像完整性。结果，这些方法往往缺乏空间一致性，导致编辑伪影和失真。在本文中，我们解决了这些问题，并引入了LOCATEdit，它通过基于图的方法利用自注意力推导出的块关系来增强交叉注意力图，确保在图像区域之间保持平滑一致的注意力，从而将修改限定在指定项目上，同时保留周围结构。LOCATEdit在PIE-Bench上显著超越现有基线，展示了其在各种编辑任务中的先进性能和有效性。代码可在此处找到：this https URL。 

---
# Low-Resource Transliteration for Roman-Urdu and Urdu Using Transformer-Based Models 

**Title (ZH)**: 基于变换器模型的低资源罗马字母-乌尔都语和乌尔都语 transliteration研究 

**Authors**: Umer Butt, Stalin Veranasi, Günter Neumann  

**Link**: [PDF](https://arxiv.org/pdf/2503.21530)  

**Abstract**: As the Information Retrieval (IR) field increasingly recognizes the importance of inclusivity, addressing the needs of low-resource languages remains a significant challenge. Transliteration between Urdu and its Romanized form, Roman Urdu, remains underexplored despite the widespread use of both scripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl dataset showed promising results but suffered from poor domain adaptability and limited evaluation. We propose a transformer-based approach using the m2m100 multilingual translation model, enhanced with masked language modeling (MLM) pretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverse Dakshina dataset. To address previous evaluation flaws, we introduce rigorous dataset splits and assess performance using BLEU, character-level BLEU, and CHRF. Our model achieves strong transliteration performance, with Char-BLEU scores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. These results outperform both RNN baselines and GPT-4o Mini and demonstrate the effectiveness of multilingual transfer learning for low-resource transliteration tasks. 

**Abstract (ZH)**: 随着信息检索（IR）领域越来越认识到包容性的重要性，满足低资源语言的需求仍然是一项重大挑战。尽管印地语及其罗马化形式罗马印地语在南亚地区广泛使用，但两者之间的 transliteration 仍然未被充分研究。先前使用 RNN 在 Roman-Urdu-Parl 数据集上的工作取得了令人鼓舞的结果，但存在域适应性差和评估有限的问题。我们提出了一种基于 transformer 的方法，使用多语言翻译模型 m2m100，并结合了掩码语言模型 (MLM) 预训练和在 Roman-Urdu-Parl 数据集及领域多样化的 Dakshina 数据集上的微调。为了弥补先前评估中的缺陷，我们引入了严格的数据集划分，并使用 BLEU、字符级 BLEU 和 CHRF 进行性能评估。我们的模型在印地语→罗马印地语和罗马印地语→印地语方向上分别取得了 96.37 和 97.44 的 Char-BLEU 分数，这些结果优于 RNN 基线和 GPT-4o Mini，并证明了多语言转移学习在低资源 transliteration 任务中的有效性。 

---
# MONO2REST: Identifying and Exposing Microservices: a Reusable RESTification Approach 

**Title (ZH)**: MONO2REST：识别和暴露微服务：一种可重用的REST化方法 

**Authors**: Matthéo Lecrivain, Hanifa Barry, Dalila Tamzalit, Houari Sahraoui  

**Link**: [PDF](https://arxiv.org/pdf/2503.21522)  

**Abstract**: The microservices architectural style has become the de facto standard for large-scale cloud applications, offering numerous benefits in scalability, maintainability, and deployment flexibility. Many organizations are pursuing the migration of legacy monolithic systems to a microservices architecture. However, this process is challenging, risky, time-intensive, and prone-to-failure while several organizations lack necessary financial resources, time, or expertise to set up this migration process. So, rather than trying to migrate a legacy system where migration is risky or not feasible, we suggest exposing it as a microservice application without without having to migrate it. In this paper, we present a reusable, automated, two-phase approach that combines evolutionary algorithms with machine learning techniques. In the first phase, we identify microservices at the method level using a multi-objective genetic algorithm that considers both structural and semantic dependencies between methods. In the second phase, we generate REST APIs for each identified microservice using a classification algorithm to assign HTTP methods and endpoints. We evaluated our approach with a case study on the Spring PetClinic application, which has both monolithic and microservices implementations that serve as ground truth for comparison. Results demonstrate that our approach successfully aligns identified microservices with those in the reference microservices implementation, highlighting its effectiveness in service identification and API generation. 

**Abstract (ZH)**: 微服务架构风格已成为大规模云应用程序的事实标准，提供了在扩展性、可维护性和部署灵活性方面的诸多优势。许多组织正寻求将遗留的单体系统迁移到微服务架构。然而，这一过程具有挑战性、风险高、耗时且容易失败，而许多组织缺乏必要的资金、时间和专业知识来启动这一迁移过程。因此，我们建议在无需迁移的情况下，将遗留系统暴露为微服务应用。在本文中，我们提出了一种可重用的、自动化的两阶段方法，结合了进化算法与机器学习技术。在第一阶段，我们使用多目标遗传算法在方法级别识别微服务，同时考虑方法之间的结构性和语义依赖关系。在第二阶段，我们使用分类算法为每个识别出的微服务生成REST API，分配HTTP方法和端点。我们通过在Spring PetClinic应用上的案例研究评估了该方法的效果，该应用既有单体实现又有微服务实现，作为基准进行对比。结果表明，我们的方法成功地将识别出的微服务与参考微服务实施进行了对齐，突显了其在服务识别和API生成方面的有效性。 

---
# Quantitative Evaluation of Quantum/Classical Neural Network Using a Game Solver Metric 

**Title (ZH)**: 使用游戏解算器指标对量子/经典神经网络进行定量评估 

**Authors**: Suzukaze Kamei, Hideaki Kawaguchi, Shin Nishio, Tatakahiko Satoh  

**Link**: [PDF](https://arxiv.org/pdf/2503.21514)  

**Abstract**: To evaluate the performance of quantum computing systems relative to classical counterparts and explore the potential for quantum advantage, we propose a game-solving benchmark based on Elo ratings in the game of tic-tac-toe. We compare classical convolutional neural networks (CNNs), quantum convolutional neural networks (QCNNs), and hybrid classical-quantum models by assessing their performance against a random-move agent in automated matches. Additionally, we implement a QCNN integrated with quantum communication and evaluate its performance to quantify the overhead introduced by noisy quantum channels. Our results show that the classical-quantum hybrid model achieves Elo ratings comparable to those of classical CNNs, while the standalone QCNN underperforms under current hardware constraints. The communication overhead was found to be modest. These findings demonstrate the viability of using game-based benchmarks for evaluating quantum computing systems and suggest that quantum communication can be incorporated with limited impact on performance, providing a foundation for future hybrid quantum applications. 

**Abstract (ZH)**: 基于井字游戏的Elo评级 benchmarks评估量子计算系统的性能并探索量子优势 

---
# Keyword-Oriented Multimodal Modeling for Euphemism Identification 

**Title (ZH)**: 面向关键词的多模态模型研究： euphemism 识别 

**Authors**: Yuxue Hu, Junsong Li, Meixuan Chen, Dongyu Su, Tongguan Wang, Ying Sha  

**Link**: [PDF](https://arxiv.org/pdf/2503.21504)  

**Abstract**: Euphemism identification deciphers the true meaning of euphemisms, such as linking "weed" (euphemism) to "marijuana" (target keyword) in illicit texts, aiding content moderation and combating underground markets. While existing methods are primarily text-based, the rise of social media highlights the need for multimodal analysis, incorporating text, images, and audio. However, the lack of multimodal datasets for euphemisms limits further research. To address this, we regard euphemisms and their corresponding target keywords as keywords and first introduce a keyword-oriented multimodal corpus of euphemisms (KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including text, images, and speech. We further propose a keyword-oriented multimodal euphemism identification method (KOM-EI), which uses cross-modal feature alignment and dynamic fusion modules to explicitly utilize the visual and audio features of the keywords for efficient euphemism identification. Extensive experiments demonstrate that KOM-EI outperforms state-of-the-art models and large language models, and show the importance of our multimodal datasets. 

**Abstract (ZH)**: euphemism识别揭示隐喻的真实含义，例如将“weed”（隐语）关联到“marijuana”（目标关键词）等词汇在非法文本中的使用，助力内容审核并打击地下市场。尽管现有方法主要基于文本，社交媒体的兴起强调了多模态分析的需求，将文本、图像和音频结合起来。然而，缺乏多模态隐语数据集限制了进一步研究。为解决这一问题，我们视隐语及其对应的目标关键词为关键词，并首次引入一种以关键词为中心的多模态隐语数据集(KOM-Euph)，包括毒品、武器和性三个领域，包含文本、图像和语音数据。我们进一步提出了一种以关键词为中心的多模态隐语识别方法(KOM-EI)，该方法使用跨模态特征对齐和动态融合模块，明确利用关键词的视觉和音频特征，实现高效的隐语识别。大量实验表明，KOM-EI 在performance上优于现有模型和大语言模型，并凸显了我们多模态数据集的重要性。 

---
# Adaptive Resampling with Bootstrap for Noisy Multi-Objective Optimization Problems 

**Title (ZH)**: 基于靴straps的自适应重采样在噪声多目标优化问题中的应用 

**Authors**: Timo Budszuhn, Mark Joachim Krallmann, Daniel Horn  

**Link**: [PDF](https://arxiv.org/pdf/2503.21495)  

**Abstract**: The challenge of noisy multi-objective optimization lies in the constant trade-off between exploring new decision points and improving the precision of known points through resampling. This decision should take into account both the variability of the objective functions and the current estimate of a point in relation to the Pareto front. Since the amount and distribution of noise are generally unknown, it is desirable for a decision function to be highly adaptive to the properties of the optimization problem. This paper presents a resampling decision function that incorporates the stochastic nature of the optimization problem by using bootstrapping and the probability of dominance. The distribution-free estimation of the probability of dominance is achieved using bootstrap estimates of the means. To make the procedure applicable even with very few observations, we transfer the distribution observed at other decision points. The efficiency of this resampling approach is demonstrated by applying it in the NSGA-II algorithm with a sequential resampling procedure under multiple noise variations. 

**Abstract (ZH)**: 噪 многокритериальной оптимизации: challenge住在探索新决策点与通过重采样提高已知点精度之间的不断权衡。这一决策应同时考虑目标函数的变异性以及所评估点相对于帕累托前沿的当前估计值。由于噪声的数量和分布通常未知，因此期望决策函数能够高度适应优化问题的性质。本文提出了一种通过使用 Bootstrapping 和支配概率结合重采样决策函数的方法。借助 Bootstrapping 对均值的估计实现支配概率的无分布估计。为了即使在观测数据很少的情况下也能使该过程适用，我们将其他决策点观察到的分布转移过来。通过将其应用于 NSGA-II 算法并采用基于多次噪声变化的顺序重采样程序，证明了该重采样方法的效率。 

---
# Retinal Fundus Multi-Disease Image Classification using Hybrid CNN-Transformer-Ensemble Architectures 

**Title (ZH)**: 基于混合CNN-Transformer-集成架构的视网膜 fundus 多病种图像分类 

**Authors**: Deependra Singh, Saksham Agarwal, Subhankar Mishra  

**Link**: [PDF](https://arxiv.org/pdf/2503.21465)  

**Abstract**: Our research is motivated by the urgent global issue of a large population affected by retinal diseases, which are evenly distributed but underserved by specialized medical expertise, particularly in non-urban areas. Our primary objective is to bridge this healthcare gap by developing a comprehensive diagnostic system capable of accurately predicting retinal diseases solely from fundus images. However, we faced significant challenges due to limited, diverse datasets and imbalanced class distributions. To overcome these issues, we have devised innovative strategies. Our research introduces novel approaches, utilizing hybrid models combining deeper Convolutional Neural Networks (CNNs), Transformer encoders, and ensemble architectures sequentially and in parallel to classify retinal fundus images into 20 disease labels. Our overarching goal is to assess these advanced models' potential in practical applications, with a strong focus on enhancing retinal disease diagnosis accuracy across a broader spectrum of conditions. Importantly, our efforts have surpassed baseline model results, with the C-Tran ensemble model emerging as the leader, achieving a remarkable model score of 0.9166, surpassing the baseline score of 0.9. Additionally, experiments with the IEViT model showcased equally promising outcomes with improved computational efficiency. We've also demonstrated the effectiveness of dynamic patch extraction and the integration of domain knowledge in computer vision tasks. In summary, our research strives to contribute significantly to retinal disease diagnosis, addressing the critical need for accessible healthcare solutions in underserved regions while aiming for comprehensive and accurate disease prediction. 

**Abstract (ZH)**: 我们的研究受到全球数百万人患视网膜疾病这一迫切问题的启发，这些疾病分布均衡但缺乏专科医疗 expertise，尤其是在非都市区域。我们的主要目标是通过开发一种全面的诊断系统来弥合这一医疗缺口，该系统仅通过视网膜Fundus图像即可准确预测视网膜疾病。然而，由于受限于有限且多样化的数据集以及类别分布不平衡的问题，我们面临了重大挑战。为克服这些问题，我们提出了一些创新策略。我们的研究引入了新颖的方法，利用混合模型结合更深的卷积神经网络（CNNs）、Transformer编码器以及按序列和并行方式构建的集成架构，将视网膜Fundus图像分类为20种疾病标签。我们的总体目标是评估这些先进模型在实际应用中的潜力，特别注重提高各类疾病诊断的准确性。重要的是，我们的努力超越了基准模型的结果，C-Tran集成模型脱颖而出，实现了0.9166的卓越模型得分，超过了基准得分0.9。此外，IEViT模型的实验同样取得了令人鼓舞的结果，展示了更好的计算效率。我们还展示了动态补丁提取和在计算机视觉任务中集成领域知识的有效性。总之，我们的研究致力于在视网膜疾病诊断领域作出重大贡献，旨在为缺乏医疗服务的地区提供可及的健康解决方案，同时着眼于实现全面和准确的疾病预测。 

---
# Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection 

**Title (ZH)**: 利用链式思考元数据进行任务路由和对抗性提示检测 

**Authors**: Ryan Marinelli, Josef Pichlmeier, Tamas Bisztray  

**Link**: [PDF](https://arxiv.org/pdf/2503.21464)  

**Abstract**: In this work, we propose a metric called Number of Thoughts (NofT) to determine the difficulty of tasks pre-prompting and support Large Language Models (LLMs) in production contexts. By setting thresholds based on the number of thoughts, this metric can discern the difficulty of prompts and support more effective prompt routing. A 2% decrease in latency is achieved when routing prompts from the MathInstruct dataset through quantized, distilled versions of Deepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this metric can be used to detect adversarial prompts used in prompt injection attacks with high efficacy. The Number of Thoughts can inform a classifier that achieves 95% accuracy in adversarial prompt detection. Our experiments ad datasets used are available on our GitHub page: this https URL. 

**Abstract (ZH)**: 本研究提出了一种称为思维数量（Number of Thoughts, NoFT）的度量标准，以确定预提示任务的难度，并在大规模语言模型（LLMs）的生产环境中提供支持。通过基于思维数量设置阈值，该度量标准可以辨别提示的难度并支持更有效的提示路由。通过使用量化和精简后的Deepseek（参数分别为17亿、70亿和140亿）版本对MathInstruct数据集中的提示进行路由，实现了2%的延迟降低。此外，该度量标准可以高效地检测出用于提示注入攻击的对抗性提示。思维数量可以告知一个分类器，该分类器在对抗性提示检测中的准确率达到95%。我们的实验和所用数据集可在我们的GitHub页面上获取：this https URL。 

---
# Unveiling Latent Information in Transaction Hashes: Hypergraph Learning for Ethereum Ponzi Scheme Detection 

**Title (ZH)**: 揭示交易哈希中的隐含信息：超图学习在以太坊庞氏骗局检测中的应用 

**Authors**: Junhao Wu, Yixin Yang, Chengxiang Jin, Silu Mu, Xiaolei Qian, Jiajun Zhou, Shanqing Yu, Qi Xuan  

**Link**: [PDF](https://arxiv.org/pdf/2503.21463)  

**Abstract**: With the widespread adoption of Ethereum, financial frauds such as Ponzi schemes have become increasingly rampant in the blockchain ecosystem, posing significant threats to the security of account assets. Existing Ethereum fraud detection methods typically model account transactions as graphs, but this approach primarily focuses on binary transactional relationships between accounts, failing to adequately capture the complex multi-party interaction patterns inherent in Ethereum. To address this, we propose a hypergraph modeling method for the Ponzi scheme detection method in Ethereum, called HyperDet. Specifically, we treat transaction hashes as hyperedges that connect all the relevant accounts involved in a transaction. Additionally, we design a two-step hypergraph sampling strategy to significantly reduce computational complexity. Furthermore, we introduce a dual-channel detection module, including the hypergraph detection channel and the hyper-homo graph detection channel, to be compatible with existing detection methods. Experimental results show that, compared to traditional homogeneous graph-based methods, the hyper-homo graph detection channel achieves significant performance improvements, demonstrating the superiority of hypergraph in Ponzi scheme detection. This research offers innovations for modeling complex relationships in blockchain data. 

**Abstract (ZH)**: 以太坊广泛采用后，区块链生态系统中的庞氏骗局等金融欺诈行为日益猖獗，对账户资产安全构成重大威胁。现有以太坊欺诈检测方法通常将账户交易建模为图，但这种方法主要关注账户之间的二元交易关系，未能充分捕捉到以太坊中固有的复杂多方交互模式。为解决这一问题，我们提出了一种用于以太坊庞氏骗局检测的超图建模方法，称为HyperDet。具体而言，我们将交易哈希视为连接交易中所有相关账户的超边。此外，我们设计了一种两步超图抽样策略，以显著降低计算复杂度。进一步地，我们引入了一种双重检测模块，包括超图检测通道和超同构图检测通道，以便与现有检测方法兼容。实验结果表明，与传统的同构图基方法相比，超同构图检测通道实现了显著的性能提升，这表明超图在庞氏骗局检测中的优越性。该研究为建模区块链数据中的复杂关系提供了创新方法。 

---
# An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses 

**Title (ZH)**: 基于情感和语义分析的LLM和Google Translate在翻译选定印度语言方面的评价 

**Authors**: Rohitash Chandra, Aryan Chaudhary, Yeshwanth Rayavarapu  

**Link**: [PDF](https://arxiv.org/pdf/2503.21393)  

**Abstract**: Large Language models (LLMs) have been prominent for language translation, including low-resource languages. There has been limited study about the assessment of the quality of translations generated by LLMs, including Gemini, GPT and Google Translate. In this study, we address this limitation by using semantic and sentiment analysis of selected LLMs for Indian languages, including Sanskrit, Telugu and Hindi. We select prominent texts that have been well translated by experts and use LLMs to generate their translations to English, and then we provide a comparison with selected expert (human) translations. Our findings suggest that while LLMs have made significant progress in translation accuracy, challenges remain in preserving sentiment and semantic integrity, especially in figurative and philosophical contexts. The sentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving the sentiments for the Bhagavad Gita (Sanskrit-English) translations when compared to Google Translate. We observed a similar trend for the case of Tamas (Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs similarly to GPT-3.5 in the translation in terms of sentiments for the three languages. We found that LLMs are generally better at translation for capturing sentiments when compared to Google Translate. 

**Abstract (ZH)**: 大型语言模型（LLMs）在语言翻译中的应用包括低资源语言，但对其生成的翻译质量评估研究有限，包括Gemini、GPT和Google Translate。本研究通过使用对印度语种（包括梵语、泰卢固语和印地语）选定LLM的语义和情感分析，填补了这一空白。我们选择了由专家精心翻译的重要文本，并使用LLM生成其英语翻译，然后与选定的专家（人类）翻译进行比较。研究发现，尽管LLM在翻译准确性方面取得了显著进展，但在保留情感和语义完整性，尤其是在比喻和哲学语境中，仍面临挑战。情感分析显示，GPT-4o和GPT-3.5在维护《薄伽梵歌》（梵语-英语）翻译的情感方面优于Google Translate。类似的趋势也体现在泰卢固语-英语和印地语-英语的“Tamas”和“Maha P”翻译中。在三种语言的翻译情感方面，GPT-4o与GPT-3.5表现类似。我们发现，与Google Translate相比，LLM在捕捉情感方面通常表现更好。 

---
# Investigating the Duality of Interpretability and Explainability in Machine Learning 

**Title (ZH)**: 探究机器学习中可解释性和解释性的二重性 

**Authors**: Moncef Garouani, Josiane Mothe, Ayah Barhrhouj, Julien Aligon  

**Link**: [PDF](https://arxiv.org/pdf/2503.21356)  

**Abstract**: The rapid evolution of machine learning (ML) has led to the widespread adoption of complex "black box" models, such as deep neural networks and ensemble methods. These models exhibit exceptional predictive performance, making them invaluable for critical decision-making across diverse domains within society. However, their inherently opaque nature raises concerns about transparency and interpretability, making them untrustworthy decision support systems. To alleviate such a barrier to high-stakes adoption, research community focus has been on developing methods to explain black box models as a means to address the challenges they pose. Efforts are focused on explaining these models instead of developing ones that are inherently interpretable. Designing inherently interpretable models from the outset, however, can pave the path towards responsible and beneficial applications in the field of ML. In this position paper, we clarify the chasm between explaining black boxes and adopting inherently interpretable models. We emphasize the imperative need for model interpretability and, following the purpose of attaining better (i.e., more effective or efficient w.r.t. predictive performance) and trustworthy predictors, provide an experimental evaluation of latest hybrid learning methods that integrates symbolic knowledge into neural network predictors. We demonstrate how interpretable hybrid models could potentially supplant black box ones in different domains. 

**Abstract (ZH)**: 机器学习的快速发展导致了复杂“黑盒”模型的广泛采用，如深度神经网络和集成方法。这些模型表现出卓越的预测性能，使其在社会各个领域的关键决策中不可替代。然而，它们本质上的不透明性引发了透明度和可解释性方面的担忧，使其成为不可信的决策支持系统。为了解决这种高风险采用的障碍，研究界的重点是开发方法来解释黑盒模型，以应对它们带来的挑战。然而，致力于解释这些模型而不是开发本质上具有可解释性的模型的做法可以引导机器学习领域的负责任和有益的应用。在此观点论文中，我们阐明了解释黑盒模型与采用本质上可解释模型之间的差距。我们强调了模型可解释性的重要性，并针对提高（即在预测性能方面更有效或更高效）和可信的预测器的目的，提供了将符号知识整合到神经网络预测器中的最新混合学习方法的实验评估。我们展示了如何使可解释的混合模型在不同领域潜在替代黑盒模型。 

---
# Residual Learning Inspired Crossover Operator and Strategy Enhancements for Evolutionary Multitasking 

**Title (ZH)**: 受残差学习启发的交叉操作及其在进化多任务学习中的策略增强 

**Authors**: Ruilin Wang, Xiang Feng, Huiqun Yu, Edmund M-K Lai  

**Link**: [PDF](https://arxiv.org/pdf/2503.21347)  

**Abstract**: In evolutionary multitasking, strategies such as crossover operators and skill factor assignment are critical for effective knowledge transfer. Existing improvements to crossover operators primarily focus on low-dimensional variable combinations, such as arithmetic crossover or partially mapped crossover, which are insufficient for modeling complex high-dimensional this http URL, static or semi-dynamic crossover strategies fail to adapt to the dynamic dependencies among tasks. In addition, current Multifactorial Evolutionary Algorithm frameworks often rely on fixed skill factor assignment strategies, lacking flexibility. To address these limitations, this paper proposes the Multifactorial Evolutionary Algorithm-Residual Learning (MFEA-RL) method based on residual learning. The method employs a Very Deep Super-Resolution (VDSR) model to generate high-dimensional residual representations of individuals, enhancing the modeling of complex relationships within dimensions. A ResNet-based mechanism dynamically assigns skill factors to improve task adaptability, while a random mapping mechanism efficiently performs crossover operations and mitigates the risk of negative transfer. Theoretical analysis and experimental results show that MFEA-RL outperforms state-of-the-art multitasking algorithms. It excels in both convergence and adaptability on standard evolutionary multitasking benchmarks, including CEC2017-MTSO and WCCI2020-MTSO. Additionally, its effectiveness is validated through a real-world application scenario. 

**Abstract (ZH)**: 多因子进化算法-残差学习（MFEA-RL）方法：一种用于有效知识转移的残差学习方法 

---
# A 71.2-$μ$W Speech Recognition Accelerator with Recurrent Spiking Neural Network 

**Title (ZH)**: 一种基于递归脉冲神经网络的71.2-μW 语音识别加速器 

**Authors**: Chih-Chyau Yang, Tian-Sheuan Chang  

**Link**: [PDF](https://arxiv.org/pdf/2503.21337)  

**Abstract**: This paper introduces a 71.2-$\mu$W speech recognition accelerator designed for edge devices' real-time applications, emphasizing an ultra low power design. Achieved through algorithm and hardware co-optimizations, we propose a compact recurrent spiking neural network with two recurrent layers, one fully connected layer, and a low time step (1 or 2). The 2.79-MB model undergoes pruning and 4-bit fixed-point quantization, shrinking it by 96.42\% to 0.1 MB. On the hardware front, we take advantage of \textit{mixed-level pruning}, \textit{zero-skipping} and \textit{merged spike} techniques, reducing complexity by 90.49\% to 13.86 MMAC/S. The \textit{parallel time-step execution} addresses inter-time-step data dependencies and enables weight buffer power savings through weight sharing. Capitalizing on the sparse spike activity, an input broadcasting scheme eliminates zero computations, further saving power. Implemented on the TSMC 28-nm process, the design operates in real time at 100 kHz, consuming 71.2 $\mu$W, surpassing state-of-the-art designs. At 500 MHz, it has 28.41 TOPS/W and 1903.11 GOPS/mm$^2$ in energy and area efficiency, respectively. 

**Abstract (ZH)**: 一种用于边缘设备实时应用的71.2-μW语音识别加速器设计及其实现 

---
# A Low-Power Streaming Speech Enhancement Accelerator For Edge Devices 

**Title (ZH)**: 面向边缘设备的低功耗流式语音增强加速器 

**Authors**: Ci-Hao Wu, Tian-Sheuan Chang  

**Link**: [PDF](https://arxiv.org/pdf/2503.21335)  

**Abstract**: Transformer-based speech enhancement models yield impressive results. However, their heterogeneous and complex structure restricts model compression potential, resulting in greater complexity and reduced hardware efficiency. Additionally, these models are not tailored for streaming and low-power applications. Addressing these challenges, this paper proposes a low-power streaming speech enhancement accelerator through model and hardware optimization. The proposed high performance model is optimized for hardware execution with the co-design of model compression and target application, which reduces 93.9\% of model size by the proposed domain-aware and streaming-aware pruning techniques. The required latency is further reduced with batch normalization-based transformers. Additionally, we employed softmax-free attention, complemented by an extra batch normalization, facilitating simpler hardware design. The tailored hardware accommodates these diverse computing patterns by breaking them down into element-wise multiplication and accumulation (MAC). This is achieved through a 1-D processing array, utilizing configurable SRAM addressing, thereby minimizing hardware complexities and simplifying zero skipping. Using the TSMC 40nm CMOS process, the final implementation requires merely 207.8K gates and 53.75KB SRAM. It consumes only 8.08 mW for real-time inference at a 62.5MHz frequency. 

**Abstract (ZH)**: 基于Transformer的语音增强模型取得了令人印象深刻的成果。然而，其异构且复杂的结构限制了模型压缩的潜力，导致更高的复杂度和硬件效率降低。此外，这些模型并不适合流式处理和低功耗应用。针对这些挑战，本文提出了一种适用于流式处理的低功耗语音增强加速器，通过模型和硬件优化来解决。提出的高性能模型通过模型压缩和目标应用的协同设计进行优化，采用领域感知和流式感知剪枝技术减少了93.9%的模型大小。通过基于批量归一化的Transformer进一步减少了所需的延迟。此外，我们采用了无softmax的注意力机制，并通过额外的批量归一化加以补充，简化了硬件设计。针对这些不同的计算模式，定制的硬件通过元素级乘法和累加（MAC）将其分解。这种方法通过使用可配置的SRAM寻址的1-D处理阵列实现，从而减少了硬件复杂性并简化了零跳过。在TSMC 40nm CMOS工艺下，最终实现仅需207.8K门电路和53.75KB SRAM。在62.5MHz频率下进行实时推理时，功耗仅为8.08 mW。 

---
# ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback 

**Title (ZH)**: ReFeed：基于反馈反思推理的多维度总结 refinement 

**Authors**: Taewon Yun, Jihwan Oh, Hyangsuk Min, Yuho Lee, Jihwan Bang, Jason Cai, Hwanjun Song  

**Link**: [PDF](https://arxiv.org/pdf/2503.21332)  

**Abstract**: Summarization refinement faces challenges when extending to multi-dimension. In this paper, we introduce ReFeed, a powerful summarization refinement pipeline that enhances multiple dimensions through reflective reasoning on feedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based dataset optimized for training a lightweight model with reflective reasoning. Our experiments reveal how the number of dimensions, feedback exposure, and reasoning policy influence refinement performance, highlighting reflective reasoning and simultaneously addressing multiple feedback is crucial to mitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy feedback and feedback order. Lastly, our finding emphasizes that creating data with a proper goal and guideline constitutes a fundamental pillar of effective reasoning. The dataset and model will be released. 

**Abstract (ZH)**: 多维度总结提炼扩展面临挑战，本文介绍了一种名为ReFeed的总结提炼强化pipeline，通过反思性推理反馈增强多个维度。 

---
# FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for Composed Image Retrieval 

**Title (ZH)**: FineCIR: 明确解析组成图像检索中的细粒度修改语义 

**Authors**: Zixu Li, Zhiheng Fu, Yupeng Hu, Zhiwei Chen, Haokun Wen, Liqiang Nie  

**Link**: [PDF](https://arxiv.org/pdf/2503.21309)  

**Abstract**: Composed Image Retrieval (CIR) facilitates image retrieval through a multimodal query consisting of a reference image and modification text. The reference image defines the retrieval context, while the modification text specifies desired alterations. However, existing CIR datasets predominantly employ coarse-grained modification text (CoarseMT), which inadequately captures fine-grained retrieval intents. This limitation introduces two key challenges: (1) ignoring detailed differences leads to imprecise positive samples, and (2) greater ambiguity arises when retrieving visually similar images. These issues degrade retrieval accuracy, necessitating manual result filtering or repeated queries. To address these limitations, we develop a robust fine-grained CIR data annotation pipeline that minimizes imprecise positive samples and enhances CIR systems' ability to discern modification intents accurately. Using this pipeline, we refine the FashionIQ and CIRR datasets to create two fine-grained CIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR, the first CIR framework explicitly designed to parse the modification text. FineCIR effectively captures fine-grained modification semantics and aligns them with ambiguous visual entities, enhancing retrieval precision. Extensive experiments demonstrate that FineCIR consistently outperforms state-of-the-art CIR baselines on both fine-grained and traditional CIR benchmark datasets. Our FineCIR code and fine-grained CIR datasets are available at this https URL. 

**Abstract (ZH)**: 细粒度图像检索（细粒度CIR）通过结合参考图像和修改文本的多模态查询来促进图像检索。参考图像定义检索上下文，而修改文本指定所需的修改。然而，现有的CIR数据集主要采用粗粒度修改文本（CoarseMT），这未能充分捕捉到细粒度的检索意图。这一限制引入了两个关键挑战：（1）忽视细节差异会导致不精确的正样本，（2）在检索视觉上相似的图像时增加了更大的模糊性。这些问题降低了检索准确性，需要手动过滤结果或重复查询。为了解决这些问题，我们开发了一种稳健的细粒度CIR数据标注管道，以减少不精确的正样本并增强CIR系统准确区分修改意图的能力。使用该管道，我们对FashionIQ和CIRR数据集进行了细化，创建了两个细粒度CIR数据集：Fine-FashionIQ和Fine-CIRR。此外，我们引入了FineCIR，这是第一个明确设计用于解析修改文本的CIR框架。FineCIR有效地捕捉到细粒度的修改语义，并与模糊的视觉实体对齐，从而提高检索精度。广泛的实验表明，FineCIR在细粒度和传统CIR基准数据集上始终优于最先进的CIR基线系统。我们的FineCIR代码和细粒度CIR数据集可从此处访问。 

---
# InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression 

**Title (ZH)**: InternVL-X: 提升并加速 InternVL 系列模型的高效视觉 token 压缩 

**Authors**: Dongchen Lu, Yuyao Sun, Zilu Zhang, Leping Huang, Jianliang Zeng, Mao Shu, Huo Cao  

**Link**: [PDF](https://arxiv.org/pdf/2503.21307)  

**Abstract**: Most multimodal large language models (MLLMs) treat visual tokens as "a sequence of text", integrating them with text tokens into a large language model (LLM). However, a great quantity of visual tokens significantly increases the demand for computational resources and time. In this paper, we propose InternVL-X, which outperforms the InternVL model in both performance and efficiency by incorporating three visual token compression methods. First, we propose a novel vision-language projector, PVTC. This component integrates adjacent visual embeddings to form a local query and utilizes the transformed CLS token as a global query, then performs point-to-region cross-attention through these local and global queries to more effectively convert visual features. Second, we present a layer-wise visual token compression module, LVTC, which compresses tokens in the LLM shallow layers and then expands them through upsampling and residual connections in the deeper layers. This significantly enhances the model computational efficiency. Futhermore, we propose an efficient high resolution slicing method, RVTC, which dynamically adjusts the number of visual tokens based on image area or length filtering. RVTC greatly enhances training efficiency with only a slight reduction in performance. By utilizing 20% or fewer visual tokens, InternVL-X achieves state-of-the-art performance on 7 public MLLM benchmarks, and improves the average metric by 2.34% across 12 tasks. 

**Abstract (ZH)**: InternVL-X：通过三种视觉_token压缩方法在性能和效率上超越InternVL模型 

---
# DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data 

**Title (ZH)**: DeBackdoor: 一种在有限数据情况下检测深度模型后门攻击的演绎框架 

**Authors**: Dorde Popovic, Amin Sadeghi, Ting Yu, Sanjay Chawla, Issa Khalil  

**Link**: [PDF](https://arxiv.org/pdf/2503.21305)  

**Abstract**: Backdoor attacks are among the most effective, practical, and stealthy attacks in deep learning. In this paper, we consider a practical scenario where a developer obtains a deep model from a third party and uses it as part of a safety-critical system. The developer wants to inspect the model for potential backdoors prior to system deployment. We find that most existing detection techniques make assumptions that are not applicable to this scenario. In this paper, we present a novel framework for detecting backdoors under realistic restrictions. We generate candidate triggers by deductively searching over the space of possible triggers. We construct and optimize a smoothed version of Attack Success Rate as our search objective. Starting from a broad class of template attacks and just using the forward pass of a deep model, we reverse engineer the backdoor attack. We conduct extensive evaluation on a wide range of attacks, models, and datasets, with our technique performing almost perfectly across these settings. 

**Abstract (ZH)**: 后门攻击是深度学习中最有效、最实用和最隐秘的攻击方式之一。在本文中，我们考虑一个实际场景，即开发者从第三方获得一个深度模型，并将其作为安全关键系统的一部分使用。开发者希望在系统部署前检查模型中是否存在潜在的后门。我们发现，现有的大多数检测技术在该场景下所做的假设并不适用。在本文中，我们提出了一种在现实限制下检测后门的新框架。通过演绎性搜索可能的触发条件空间来生成候选触发条件。构建并优化修正后的攻击成功率作为搜索目标。从一类通用的模板攻击开始，仅利用深度模型的前向传递，我们逆向工程了后门攻击。我们对多种攻击、模型和数据集进行了广泛的评估，该技术在这些设置中表现几乎完美。 

---
# Multi-Scale Invertible Neural Network for Wide-Range Variable-Rate Learned Image Compression 

**Title (ZH)**: 多尺度可逆神经网络用于宽动态范围可变率学习图像压缩 

**Authors**: Hanyue Tu, Siqi Wu, Li Li, Wengang Zhou, Houqiang Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.21284)  

**Abstract**: Autoencoder-based structures have dominated recent learned image compression methods. However, the inherent information loss associated with autoencoders limits their rate-distortion performance at high bit rates and restricts their flexibility of rate adaptation. In this paper, we present a variable-rate image compression model based on invertible transform to overcome these limitations. Specifically, we design a lightweight multi-scale invertible neural network, which bijectively maps the input image into multi-scale latent representations. To improve the compression efficiency, a multi-scale spatial-channel context model with extended gain units is devised to estimate the entropy of the latent representation from high to low levels. Experimental results demonstrate that the proposed method achieves state-of-the-art performance compared to existing variable-rate methods, and remains competitive with recent multi-model approaches. Notably, our method is the first learned image compression solution that outperforms VVC across a very wide range of bit rates using a single model, especially at high bit this http URL source code is available at \href{this https URL}{this https URL}. 

**Abstract (ZH)**: 基于可逆变换的变率图像压缩模型：克服自动编码器的固有局限 

---
# Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning 

**Title (ZH)**: 基于推理的学习：Few-Shot 类增量学习中的类比权重生成 

**Authors**: Jizhou Han, Chenhao Ding, Yuhang He, Songlin Dong, Qiang Wang, Xinyuan Gao, Yihong Gong  

**Link**: [PDF](https://arxiv.org/pdf/2503.21258)  

**Abstract**: Few-shot class-incremental Learning (FSCIL) enables models to learn new classes from limited data while retaining performance on previously learned classes. Traditional FSCIL methods often require fine-tuning parameters with limited new class data and suffer from a separation between learning new classes and utilizing old knowledge. Inspired by the analogical learning mechanisms of the human brain, we propose a novel analogical generative method. Our approach includes the Brain-Inspired Analogical Generator (BiAG), which derives new class weights from existing classes without parameter fine-tuning during incremental stages. BiAG consists of three components: Weight Self-Attention Module (WSA), Weight & Prototype Analogical Attention Module (WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory for semantic conversion, WSA supplements new class weights, and WPAA computes analogies to generate new class weights. Experiments on miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our method achieves higher final and average accuracy compared to SOTA methods. 

**Abstract (ZH)**: 少量样本类增量学习（Few-shot Class-Incremental Learning, FSCIL）使模型能够在有限数据下学习新类目，同时保持对之前学习类目的性能。传统的FSCIL方法往往需要在新类目数据有限的情况下微调参数，并且学习新类目和利用旧知识之间存在割裂。受人脑类比学习机制的启发，我们提出了一种新型类比生成方法。该方法包括脑启发类比生成器（BiAG），其能够在增量学习阶段无需参数微调即可从现有类目推导出新类目的权重。BiAG 包含三个组件：权重自注意力模块（WSA）、权重与原型类比注意力模块（WPAA）和语义转换模块（SCM）。SCM 使用神经坍塌理论进行语义转换，WSA 为新类目补充权重，WPAA 计算类比以生成新类目权重。在miniImageNet、CUB-200和CIFAR-100数据集上的实验表明，与最新方法相比，我们的方法实现了更高的最终准确率和平均准确率。 

---
# OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation 

**Title (ZH)**: OminiAdapt: 学习跨任务不变性以实现稳健且环境意识强的机器人操作 

**Authors**: Yongxu Wang, Weiyun Yi, Xinhao Kong, Wanting Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.21257)  

**Abstract**: With the rapid development of embodied intelligence, leveraging large-scale human data for high-level imitation learning on humanoid robots has become a focal point of interest in both academia and industry. However, applying humanoid robots to precision operation domains remains challenging due to the complexities they face in perception and control processes, the long-standing physical differences in morphology and actuation mechanisms between humanoid robots and humans, and the lack of task-relevant features obtained from egocentric vision. To address the issue of covariate shift in imitation learning, this paper proposes an imitation learning algorithm tailored for humanoid robots. By focusing on the primary task objectives, filtering out background information, and incorporating channel feature fusion with spatial attention mechanisms, the proposed algorithm suppresses environmental disturbances and utilizes a dynamic weight update strategy to significantly improve the success rate of humanoid robots in accomplishing target tasks. Experimental results demonstrate that the proposed method exhibits robustness and scalability across various typical task scenarios, providing new ideas and approaches for autonomous learning and control in humanoid robots. The project will be open-sourced on GitHub. 

**Abstract (ZH)**: 随着嵌入式智能的快速发展，利用大规模人类数据进行类人机器人高层次模仿学习已成为学术界和工业界的热点。然而，将类人机器人应用于精密操作领域仍然具有挑战性，原因在于感知和控制过程中面临的复杂性，类人机器人与人类在形态和驱动机制上的长期差异，以及从第一人称视觉获得的相关任务特征的缺乏。为解决模仿学习中的协变量偏移问题，本文提出了一种适用于类人机器人的模仿学习算法。该算法侧重主要任务目标，过滤背景信息，并结合通道特征融合与空间注意力机制，抑制环境干扰，利用动态权重更新策略显著提高类人机器人完成目标任务的成功率。实验结果表明，所提方法在多种典型任务场景下具有稳健性和可扩展性，为类人机器人自主学习与控制提供了新的思路和方法。该项目将在GitHub上开源。 

---
# Vision-to-Music Generation: A Survey 

**Title (ZH)**: 视觉到音乐生成：一个综述 

**Authors**: Zhaokai Wang, Chenxi Bao, Le Zhuo, Jingrui Han, Yang Yue, Yihong Tang, Victor Shea-Jay Huang, Yue Liao  

**Link**: [PDF](https://arxiv.org/pdf/2503.21254)  

**Abstract**: Vision-to-music Generation, including video-to-music and image-to-music tasks, is a significant branch of multimodal artificial intelligence demonstrating vast application prospects in fields such as film scoring, short video creation, and dance music synthesis. However, compared to the rapid development of modalities like text and images, research in vision-to-music is still in its preliminary stage due to its complex internal structure and the difficulty of modeling dynamic relationships with video. Existing surveys focus on general music generation without comprehensive discussion on vision-to-music. In this paper, we systematically review the research progress in the field of vision-to-music generation. We first analyze the technical characteristics and core challenges for three input types: general videos, human movement videos, and images, as well as two output types of symbolic music and audio music. We then summarize the existing methodologies on vision-to-music generation from the architecture perspective. A detailed review of common datasets and evaluation metrics is provided. Finally, we discuss current challenges and promising directions for future research. We hope our survey can inspire further innovation in vision-to-music generation and the broader field of multimodal generation in academic research and industrial applications. To follow latest works and foster further innovation in this field, we are continuously maintaining a GitHub repository at this https URL. 

**Abstract (ZH)**: 视觉到音乐生成：包括视频到音乐和图像到音乐任务，是多模态人工智能的一个重要分支，展示了在电影配乐、短视频创作和舞蹈音乐合成等领域广泛的应用前景。然而，由于其复杂内部结构和视频动态关系建模的难度，与文本和图像等模态相比，视觉到音乐的研究仍处于初级阶段。现有综述主要关注一般音乐生成，缺乏对视觉到音乐的全面讨论。本文系统回顾了视觉到音乐生成的研究进展。我们首先分析了三种输入类型（通用视频、人体运动视频和图像）以及两种输出类型（符号音乐和音频音乐）的技术特征和核心挑战。随后从架构视角总结了视觉到音乐生成的现有方法。提供了常见数据集和评估指标的详细综述。最后讨论了当前的研究挑战和未来研究的潜在方向。我们希望本文综述能够激发视觉到音乐生成及其更广泛领域的多模态生成在学术研究和工业应用中的创新。为了跟踪最新研究并促进该领域的进一步创新，我们持续维护一个GitHub仓库，链接为这个 https URL。 

---
# Dual-Splitting Conformal Prediction for Multi-Step Time Series Forecasting 

**Title (ZH)**: 双分裂同校预测多步时间序列预测 

**Authors**: Qingdi Yu, Zhiwei Cao, Ruihang Wang, Zhen Yang, Lijun Deng, Min Hu, Yong Luo, Xin Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2503.21251)  

**Abstract**: Time series forecasting is crucial for applications like resource scheduling and risk management, where multi-step predictions provide a comprehensive view of future trends. Uncertainty Quantification (UQ) is a mainstream approach for addressing forecasting uncertainties, with Conformal Prediction (CP) gaining attention due to its model-agnostic nature and statistical guarantees. However, most variants of CP are designed for single-step predictions and face challenges in multi-step scenarios, such as reliance on real-time data and limited scalability. This highlights the need for CP methods specifically tailored to multi-step forecasting. We propose the Dual-Splitting Conformal Prediction (DSCP) method, a novel CP approach designed to capture inherent dependencies within time-series data for multi-step forecasting. Experimental results on real-world datasets from four different domains demonstrate that the proposed DSCP significantly outperforms existing CP variants in terms of the Winkler Score, achieving a performance improvement of up to 23.59% compared to state-of-the-art methods. Furthermore, we deployed the DSCP approach for renewable energy generation and IT load forecasting in power management of a real-world trajectory-based application, achieving an 11.25% reduction in carbon emissions through predictive optimization of data center operations and controls. 

**Abstract (ZH)**: 时间序列预测对于资源调度和风险管理等应用至关重要，多步预测能够提供未来趋势的全面视图。不确定性量化（UQ）是应对预测不确定性的一种主流方法，而无模型预测（CP）因其模型无依赖性及统计保证正受到关注。然而，大多数CP的变体都设计用于单步预测，在多步场景中面临着实时数据依赖和可扩展性差等挑战。这凸显了需要专门针对多步预测的CP方法的必要性。我们提出了一种名为双分割无模型预测（DSCP）的新方法，这是一种专门针对时间序列数据固有依赖性进行多步预测的新型CP方法。实验结果表明，提出的DSCP在实际数据集上的Winkler评分上显著优于现有CP变体，相较于最先进的方法提高了多达23.59%的性能。此外，我们在一个基于轨迹的实际应用场景中部署了DSCP方法，用于可再生能源生成和IT负载预测，并通过预测优化数据中心的操作和控制，实现了11.25%的碳排放减少。 

---
# ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition 

**Title (ZH)**: ResearchBench: 基于灵感驱动任务分解的科学发现中大语言模型的基准测试 

**Authors**: Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2503.21248)  

**Abstract**: Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as "research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在辅助科学研究方面展现了潜在价值，但由于缺乏专门的基准，它们在发现高质量研究假设方面的能力尚未得到验证。为弥补这一空白，我们引入了首个大规模基准，用于评估LLMs的科学发现能力，该基准涵盖了科学研究发现的基本子任务：灵感检索、假设构建和假设排序。我们开发了一个自动化框架，从12个学科的科学论文中提取关键成分——研究问题、背景调查、灵感和假设，并通过专家验证确保其准确性。为防止数据污染，我们仅专注于2024年发表的论文，确保与LLM预训练数据的重叠最小化。评估结果显示，LLMs在检索灵感方面表现出色，这是一项分布外的任务，表明它们能够揭示新颖的知识关联。这使LLMs成为“研究假设矿”，能够在最小人类干预的情况下大规模生成创新性的假设，从而促进自动科学研究发现。 

---
# Improving $(α, f)$-Byzantine Resilience in Federated Learning via layerwise aggregation and cosine distance 

**Title (ZH)**: 通过层wise聚合和余弦距离提升联邦学习的$(α, f)$-拜占庭容错能力 

**Authors**: Mario García-Márquez, Nuria Rodríguez-Barroso, M.Victoria Luzón, Francisco Herrera  

**Link**: [PDF](https://arxiv.org/pdf/2503.21244)  

**Abstract**: The rapid development of artificial intelligence systems has amplified societal concerns regarding their usage, necessitating regulatory frameworks that encompass data privacy. Federated Learning (FL) is posed as potential solution to data privacy challenges in distributed machine learning by enabling collaborative model training {without data sharing}. However, FL systems remain vulnerable to Byzantine attacks, where malicious nodes contribute corrupted model updates. While Byzantine Resilient operators have emerged as a widely adopted robust aggregation algorithm to mitigate these attacks, its efficacy diminishes significantly in high-dimensional parameter spaces, sometimes leading to poor performing models. This paper introduces Layerwise Cosine Aggregation, a novel aggregation scheme designed to enhance robustness of these rules in such high-dimensional settings while preserving computational efficiency. A theoretical analysis is presented, demonstrating the superior robustness of the proposed Layerwise Cosine Aggregation compared to original robust aggregation operators. Empirical evaluation across diverse image classification datasets, under varying data distributions and Byzantine attack scenarios, consistently demonstrates the improved performance of Layerwise Cosine Aggregation, achieving up to a 16% increase in model accuracy. 

**Abstract (ZH)**: 人工智能系统的快速发展加剧了社会对其实用性的担忧， necessitating 调控框架以涵盖数据隐私。联邦学习（FL）被提出作为分布式机器学习中数据隐私挑战的潜在解决方案，通过实现无需数据共享的合作模型训练。然而，FL系统仍易受拜占庭攻击影响，恶意节点提交受污染的模型更新。虽然鲁棒的拜占庭鲁棒算子已广泛应用于缓解这些攻击，但在高维参数空间中其效果显著下降，有时会导致性能不佳的模型。本文提出层内余弦聚合，这是一种新的聚合方案，旨在在高维设置中增强这些规则的鲁棒性同时保持计算效率。理论分析表明，提出的层内余弦聚合在鲁棒性方面优于原始的鲁棒聚合算子。通过在多种图像分类数据集上进行实验评估，在不同的数据分布和拜占庭攻击场景下，层内余弦聚合始终表现出更好的性能，模型准确率最多可提高16%。 

---
# Feature-Enhanced Machine Learning for All-Cause Mortality Prediction in Healthcare Data 

**Title (ZH)**: 面向医疗数据的全因 mortality 预测的特征增强机器学习方法 

**Authors**: HyeYoung Lee, Pavel Tsoi  

**Link**: [PDF](https://arxiv.org/pdf/2503.21241)  

**Abstract**: Accurate patient mortality prediction enables effective risk stratification, leading to personalized treatment plans and improved patient outcomes. However, predicting mortality in healthcare remains a significant challenge, with existing studies often focusing on specific diseases or limited predictor sets. This study evaluates machine learning models for all-cause in-hospital mortality prediction using the MIMIC-III database, employing a comprehensive feature engineering approach. Guided by clinical expertise and literature, we extracted key features such as vital signs (e.g., heart rate, blood pressure), laboratory results (e.g., creatinine, glucose), and demographic information. The Random Forest model achieved the highest performance with an AUC of 0.94, significantly outperforming other machine learning and deep learning approaches. This demonstrates Random Forest's robustness in handling high-dimensional, noisy clinical data and its potential for developing effective clinical decision support tools. Our findings highlight the importance of careful feature engineering for accurate mortality prediction. We conclude by discussing implications for clinical adoption and propose future directions, including enhancing model robustness and tailoring prediction models for specific diseases. 

**Abstract (ZH)**: 准确的患者死亡率预测能够有效进行风险分层，从而制定个性化治疗方案并改善患者预后。然而，医疗领域中的死亡率预测仍然是一个重大挑战，现有研究往往集中在特定疾病或有限的预测因子上。本研究使用MIMIC-III数据库，评估了机器学习模型在所有原因医院内死亡率预测中的性能，并采用全面的特征工程方法。在临床专业知识和文献的指导下，我们提取了关键特征，如生命体征（例如，心率、血压）、实验室结果（例如，肌酐、血糖）和人口统计信息。随机森林模型取得了最佳性能，AUC为0.94，显著优于其他机器学习和深度学习方法。这展示了随机森林在处理高维度、噪声临床数据方面的 robustness，并表明其在开发有效的临床决策支持工具方面的潜力。我们的研究强调了精心的特征工程对于准确死亡率预测的重要性。最后，我们讨论了临床应用的 implications 并提出未来的研究方向，包括增强模型的 robustness 和为特定疾病定制预测模型。 

---
# Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval 

**Title (ZH)**: Awareness Bias Agent: 提升AI驱动知识检索中的公平性 

**Authors**: Karanbir Singh, William Ngu  

**Link**: [PDF](https://arxiv.org/pdf/2503.21237)  

**Abstract**: Advancements in retrieving accessible information have evolved faster in the last few years compared to the decades since the internet's creation. Search engines, like Google, have been the number one way to find relevant data. They have always relied on the user's abilities to find the best information in its billions of links and sources at everybody's fingertips. The advent of large language models (LLMs) has completely transformed the field of information retrieval. The LLMs excel not only at retrieving relevant knowledge but also at summarizing it effectively, making information more accessible and consumable for users. On top of it, the rise of AI Agents has introduced another aspect to information retrieval i.e. dynamic information retrieval which enables the integration of real-time data such as weather forecasts, and financial data with the knowledge base to curate context-aware knowledge. However, despite these advancements the agents remain susceptible to issues of bias and fairness, challenges deeply rooted within the knowledge base and training of LLMs. This study introduces a novel approach to bias-aware knowledge retrieval by leveraging agentic framework and the innovative use of bias detectors as tools to identify and highlight inherent biases in the retrieved content. By empowering users with transparency and awareness, this approach aims to foster more equitable information systems and promote the development of responsible AI. 

**Abstract (ZH)**: 近年来，获取可访问信息的能力进步速度远远超过了互联网诞生以来的几十年。搜索引擎，如Google，一直是查找相关数据的主要方式。它们依赖用户的技能在 billions 个链接和来源中找到最好的信息。大型语言模型（LLMs）的出现彻底改变了信息检索领域。除了检索相关知识，LLMs 还能够有效地总结知识，使信息更加易于用户获取和消化。此外，AI代理的兴起为信息检索引入了另一个方面，即动态信息检索，这使得可以通过集成实时数据（如天气预报和金融数据）来更新知识库，从而创建上下文相关知识。然而，尽管取得了这些进展，代理仍容易受到偏见和公平性问题的影响，这些问题深深植根于知识库和LLMs的训练中。本研究通过利用代理框架和创新使用偏见检测工具来识别和突出显示检索内容中的固有偏见，提出了一个新颖的偏见意识知识检索方法。通过赋予用户透明度和意识，这种方法旨在促进更加公平的信息系统，并推动负责任的AI的发展。 

---
# GenFusion: Closing the Loop between Reconstruction and Generation via Videos 

**Title (ZH)**: GenFusion: 通过视频在重建与生成之间形成闭环 

**Authors**: Sibo Wu, Congrong Xu, Binbin Huang, Andreas Geiger, Anpei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2503.21219)  

**Abstract**: Recently, 3D reconstruction and generation have demonstrated impressive novel view synthesis results, achieving high fidelity and efficiency. However, a notable conditioning gap can be observed between these two fields, e.g., scalable 3D scene reconstruction often requires densely captured views, whereas 3D generation typically relies on a single or no input view, which significantly limits their applications. We found that the source of this phenomenon lies in the misalignment between 3D constraints and generative priors. To address this problem, we propose a reconstruction-driven video diffusion model that learns to condition video frames on artifact-prone RGB-D renderings. Moreover, we propose a cyclical fusion pipeline that iteratively adds restoration frames from the generative model to the training set, enabling progressive expansion and addressing the viewpoint saturation limitations seen in previous reconstruction and generation pipelines. Our evaluation, including view synthesis from sparse view and masked input, validates the effectiveness of our approach. 

**Abstract (ZH)**: 近期，3D重建与生成展示了令人印象深刻的新型视图合成结果，实现了高保真度和效率。然而，这两个领域之间存在显著的条件差异，例如，可扩展的3D场景重建通常需要密集捕获的视角，而3D生成通常依赖于单个或没有输入视角，这大大限制了它们的应用。我们发现，这一现象源于3D约束与生成先验之间的不匹配。为解决这一问题，我们提出了一种以重建为导向的视频扩散模型，该模型学习在易产生伪影的RGB-D渲染上条件化视频帧。此外，我们提出了一种循环融合管道，该管道迭代地将生成模型的修复帧添加到训练集，从而实现渐进扩展并解决先前重建和生成管道中视角饱和的限制。我们的评估，包括从稀疏视角和带掩码输入的视图合成，验证了我们方法的有效性。 

---
# Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples 

**Title (ZH)**: 对抗性磨损与撕裂：利用自然损坏生成物理世界中的 adversarial examples 

**Authors**: Samra Irshad, Seungkyu Lee, Nassir Navab, Hong Joo Lee, Seong Tae Kim  

**Link**: [PDF](https://arxiv.org/pdf/2503.21164)  

**Abstract**: The presence of adversarial examples in the physical world poses significant challenges to the deployment of Deep Neural Networks in safety-critical applications such as autonomous driving. Most existing methods for crafting physical-world adversarial examples are ad-hoc, relying on temporary modifications like shadows, laser beams, or stickers that are tailored to specific scenarios. In this paper, we introduce a new class of physical-world adversarial examples, AdvWT, which draws inspiration from the naturally occurring phenomenon of `wear and tear', an inherent property of physical objects. Unlike manually crafted perturbations, `wear and tear' emerges organically over time due to environmental degradation, as seen in the gradual deterioration of outdoor signboards. To achieve this, AdvWT follows a two-step approach. First, a GAN-based, unsupervised image-to-image translation network is employed to model these naturally occurring damages, particularly in the context of outdoor signboards. The translation network encodes the characteristics of damaged signs into a latent `damage style code'. In the second step, we introduce adversarial perturbations into the style code, strategically optimizing its transformation process. This manipulation subtly alters the damage style representation, guiding the network to generate adversarial images where the appearance of damages remains perceptually realistic, while simultaneously ensuring their effectiveness in misleading neural networks. Through comprehensive experiments on two traffic sign datasets, we show that AdvWT effectively misleads DNNs in both digital and physical domains. AdvWT achieves an effective attack success rate, greater robustness, and a more natural appearance compared to existing physical-world adversarial examples. Additionally, integrating AdvWT into training enhances a model's generalizability to real-world damaged signs. 

**Abstract (ZH)**: AdvWT：源自磨损与剥蚀现象的物理世界 adversarial examples 

---
# Multi-Objective Optimization for Privacy-Utility Balance in Differentially Private Federated Learning 

**Title (ZH)**: 差分隐私联邦学习中隐私-效用平衡的多目标优化 

**Authors**: Kanishka Ranaweera, David Smith, Pubudu N. Pathirana, Ming Ding, Thierry Rakotoarivelo, Aruna Seneviratne  

**Link**: [PDF](https://arxiv.org/pdf/2503.21159)  

**Abstract**: Federated learning (FL) enables collaborative model training across distributed clients without sharing raw data, making it a promising approach for privacy-preserving machine learning. However, ensuring differential privacy (DP) in FL presents challenges due to the trade-off between model utility and privacy protection. Clipping gradients before aggregation is a common strategy to limit privacy loss, but selecting an optimal clipping norm is non-trivial, as excessively high values compromise privacy, while overly restrictive clipping degrades model performance. In this work, we propose an adaptive clipping mechanism that dynamically adjusts the clipping norm using a multi-objective optimization framework. By integrating privacy and utility considerations into the optimization objective, our approach balances privacy preservation with model accuracy. We theoretically analyze the convergence properties of our method and demonstrate its effectiveness through extensive experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Our results show that adaptive clipping consistently outperforms fixed-clipping baselines, achieving improved accuracy under the same privacy constraints. This work highlights the potential of dynamic clipping strategies to enhance privacy-utility trade-offs in differentially private federated learning. 

**Abstract (ZH)**: 联邦学习（FL）能够在不共享原始数据的情况下在分布式客户端之间进行协作模型训练，使其成为一种有前景的隐私保护机器学习方法。然而，确保在FL中达到差分隐私（DP）面临着在模型 utility 和隐私保护之间进行权衡的挑战。在聚合前对梯度进行裁剪是一种常见的限制隐私损失的策略，但选择最优的裁剪范数并非易事，因为过高的值会损害隐私，而过于严格的裁剪则会降低模型性能。在本工作中，我们提出了一种自适应裁剪机制，该机制使用多目标优化框架动态调整裁剪范数。通过将隐私和utility考虑纳入优化目标中，我们的方法在保护隐私和提高模型准确性之间找到了平衡。我们从理论上分析了该方法的收敛性质，并通过在MNIST、Fashion-MNIST和CIFAR-10数据集上的广泛实验展示了其有效性。结果显示，自适应裁剪在相同隐私约束条件下始终优于固定裁剪基准，实现了更高的准确性。本工作突显了动态裁剪策略在差分隐私联邦学习中增强隐私-utility权衡的潜力。 

---
# Federated Learning with Differential Privacy: An Utility-Enhanced Approach 

**Title (ZH)**: 差分隐私增强的联邦学习方法 

**Authors**: Kanishka Ranaweera, Dinh C. Nguyen, Pubudu N. Pathirana, David Smith, Ming Ding, Thierry Rakotoarivelo, Aruna Seneviratne  

**Link**: [PDF](https://arxiv.org/pdf/2503.21154)  

**Abstract**: Federated learning has emerged as an attractive approach to protect data privacy by eliminating the need for sharing clients' data while reducing communication costs compared with centralized machine learning algorithms. However, recent studies have shown that federated learning alone does not guarantee privacy, as private data may still be inferred from the uploaded parameters to the central server. In order to successfully avoid data leakage, adopting differential privacy (DP) in the local optimization process or in the local update aggregation process has emerged as two feasible ways for achieving sample-level or user-level privacy guarantees respectively, in federated learning models. However, compared to their non-private equivalents, these approaches suffer from a poor utility. To improve the privacy-utility trade-off, we present a modification to these vanilla differentially private algorithms based on a Haar wavelet transformation step and a novel noise injection scheme that significantly lowers the asymptotic bound of the noise variance. We also present a holistic convergence analysis of our proposed algorithm, showing that our method yields better convergence performance than the vanilla DP algorithms. Numerical experiments on real-world datasets demonstrate that our method outperforms existing approaches in model utility while maintaining the same privacy guarantees. 

**Abstract (ZH)**: 联邦学习作为一种保护数据隐私的有吸引力的方法，在减少通信成本的同时无需共享客户端数据。然而，近期研究表明，仅靠联邦学习并不能保证数据隐私，因为敏感数据仍可能从上传到中央服务器的参数中被推断出来。为了成功避免数据泄露，在局部优化过程或局部更新聚合过程中采用差分隐私（DP）已成为分别实现样本级或用户级隐私保障的两种可行方式。然而，与非隐私等效方法相比，这些方法带来了较差的实用性。为了改善隐私-实用性权衡，我们基于哈aar小波变换步骤和一种新型的噪声注入方案对这些基础的差分隐私算法进行了改进，显著降低了噪声方差的渐近界。我们还呈现了所提出算法的全面收敛分析，表明我们的方法在收敛性能上优于基础的DP算法。实际数据集上的数值实验表明，在保持相同隐私保障的前提下，我们的方法在模型实用性上优于现有方法。 

---
# The Devil is in Low-Level Features for Cross-Domain Few-Shot Segmentation 

**Title (ZH)**: 低级特征藏玄机：跨域少量样本分割 

**Authors**: Yuhan Liu, Yixiong Zou, Yuhua Li, Ruixuan Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.21150)  

**Abstract**: Cross-Domain Few-Shot Segmentation (CDFSS) is proposed to transfer the pixel-level segmentation capabilities learned from large-scale source-domain datasets to downstream target-domain datasets, with only a few annotated images per class. In this paper, we focus on a well-observed but unresolved phenomenon in CDFSS: for target domains, particularly those distant from the source domain, segmentation performance peaks at the very early epochs, and declines sharply as the source-domain training proceeds. We delve into this phenomenon for an interpretation: low-level features are vulnerable to domain shifts, leading to sharper loss landscapes during the source-domain training, which is the devil of CDFSS. Based on this phenomenon and interpretation, we further propose a method that includes two plug-and-play modules: one to flatten the loss landscapes for low-level features during source-domain training as a novel sharpness-aware minimization method, and the other to directly supplement target-domain information to the model during target-domain testing by low-level-based calibration. Extensive experiments on four target datasets validate our rationale and demonstrate that our method surpasses the state-of-the-art method in CDFSS signifcantly by 3.71% and 5.34% average MIoU in 1-shot and 5-shot scenarios, respectively. 

**Abstract (ZH)**: 跨域少样本分割（CDFSS） 

---
# Optimizing Multi-DNN Inference on Mobile Devices through Heterogeneous Processor Co-Execution 

**Title (ZH)**: 通过异构处理器协同执行优化移动设备上的多DNN推理 

**Authors**: Yunquan Gao, Zhiguo Zhang, Praveen Kumar Donta, Chinmaya Kumar Dehury, Xiujun Wang, Dusit Niyato, Qiyang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2503.21109)  

**Abstract**: Deep Neural Networks (DNNs) are increasingly deployed across diverse industries, driving demand for mobile device support. However, existing mobile inference frameworks often rely on a single processor per model, limiting hardware utilization and causing suboptimal performance and energy efficiency. Expanding DNN accessibility on mobile platforms requires adaptive, resource-efficient solutions to meet rising computational needs without compromising functionality. Parallel inference of multiple DNNs on heterogeneous processors remains challenging. Some works partition DNN operations into subgraphs for parallel execution across processors, but these often create excessive subgraphs based only on hardware compatibility, increasing scheduling complexity and memory overhead.
To address this, we propose an Advanced Multi-DNN Model Scheduling (ADMS) strategy for optimizing multi-DNN inference on mobile heterogeneous processors. ADMS constructs an optimal subgraph partitioning strategy offline, balancing hardware operation support and scheduling granularity, and uses a processor-state-aware algorithm to dynamically adjust workloads based on real-time conditions. This ensures efficient workload distribution and maximizes processor utilization. Experiments show ADMS reduces multi-DNN inference latency by 4.04 times compared to vanilla frameworks. 

**Abstract (ZH)**: 一种针对移动异构处理器的先进多DNN模型调度策略（ADMS）以优化多DNN推理性能 

---
# Alleviating LLM-based Generative Retrieval Hallucination in Alipay Search 

**Title (ZH)**: 缓解基于LLM的生成型检索幻觉以提高支付宝搜索效果 

**Authors**: Yedan Shen, Kaixin Wu, Yuechen Ding, Jingyuan Wen, Hong Liu, Mingjie Zhong, Zhouhan Lin, Jia Xu, Linjian Mo  

**Link**: [PDF](https://arxiv.org/pdf/2503.21098)  

**Abstract**: Generative retrieval (GR) has revolutionized document retrieval with the advent of large language models (LLMs), and LLM-based GR is gradually being adopted by the industry. Despite its remarkable advantages and potential, LLM-based GR suffers from hallucination and generates documents that are irrelevant to the query in some instances, severely challenging its credibility in practical applications. We thereby propose an optimized GR framework designed to alleviate retrieval hallucination, which integrates knowledge distillation reasoning in model training and incorporate decision agent to further improve retrieval precision. Specifically, we employ LLMs to assess and reason GR retrieved query-document (q-d) pairs, and then distill the reasoning data as transferred knowledge to the GR model. Moreover, we utilize a decision agent as post-processing to extend the GR retrieved documents through retrieval model and select the most relevant ones from multi perspectives as the final generative retrieval result. Extensive offline experiments on real-world datasets and online A/B tests on Fund Search and Insurance Search in Alipay demonstrate our framework's superiority and effectiveness in improving search quality and conversion gains. 

**Abstract (ZH)**: 基于大规模语言模型的生成式检索优化框架：缓解检索幻觉并提高检索精度 

---
# Confidence Adjusted Surprise Measure for Active Resourceful Trials (CA-SMART): A Data-driven Active Learning Framework for Accelerating Material Discovery under Resource Constraints 

**Title (ZH)**: 基于数据驱动的资源受限材料发现加速的自信心态调整意外度测量的主动学习框架（CA-SMART） 

**Authors**: Ahmed Shoyeb Raihan, Zhichao Liu, Tanveer Hossain Bhuiyan, Imtiaz Ahmed  

**Link**: [PDF](https://arxiv.org/pdf/2503.21095)  

**Abstract**: Accelerating the discovery and manufacturing of advanced materials with specific properties is a critical yet formidable challenge due to vast search space, high costs of experiments, and time-intensive nature of material characterization. In recent years, active learning, where a surrogate machine learning (ML) model mimics the scientific discovery process of a human scientist, has emerged as a promising approach to address these challenges by guiding experimentation toward high-value outcomes with a limited budget. Among the diverse active learning philosophies, the concept of surprise (capturing the divergence between expected and observed outcomes) has demonstrated significant potential to drive experimental trials and refine predictive models. Scientific discovery often stems from surprise thereby making it a natural driver to guide the search process. Despite its promise, prior studies leveraging surprise metrics such as Shannon and Bayesian surprise lack mechanisms to account for prior confidence, leading to excessive exploration of uncertain regions that may not yield useful information. To address this, we propose the Confidence-Adjusted Surprise Measure for Active Resourceful Trials (CA-SMART), a novel Bayesian active learning framework tailored for optimizing data-driven experimentation. On a high level, CA-SMART incorporates Confidence-Adjusted Surprise (CAS) to dynamically balance exploration and exploitation by amplifying surprises in regions where the model is more certain while discounting them in highly uncertain areas. We evaluated CA-SMART on two benchmark functions (Six-Hump Camelback and Griewank) and in predicting the fatigue strength of steel. The results demonstrate superior accuracy and efficiency compared to traditional surprise metrics, standard Bayesian Optimization (BO) acquisition functions and conventional ML methods. 

**Abstract (ZH)**: 加速具有特定性质的先进材料的发现和制造是一项关键但艰巨的挑战，由于搜索空间 vast、实验成本高以及材料表征的耗时性。近年来，模拟人类科学家的科学发现过程的代理机器学习（ML）模型的主动学习方法 emerged 作为一种有前景的解决办法，通过引导有限预算下的实验导向高价值成果。在多种主动学习哲学中，意外（捕捉预期结果与观察结果之间的差异）的概念已经显示出显著的潜力，可以驱动实验试错和改进预测模型。科学发现往往源于意外，使其成为引导搜索过程的自然驱动因素。尽管如此，先前利用香农和贝叶斯意外度量的研究缺乏考虑先验信心的机制，导致对可能不会产生有用信息的不确定区域进行过度探索。为了解决这个问题，我们提出了一种新的贝叶斯主动学习框架——基于置信调整的意外度量以优化数据驱动实验（CA-SMART）。总体而言，CA-SMART通过在模型更确定的区域放大意外，在高度不确定的区域减小意外来动态平衡探索与利用。我们在两个基准函数（六峰骆驼峰和 Griewank）上以及预测钢的疲劳强度上评估了CA-SMART。结果显示，与传统意外度量、标准贝叶斯优化获取函数以及传统机器学习方法相比，CA-SMART在准确性和效率上表现出更优的结果。 

---
# ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging 

**Title (ZH)**: ZJUKLAB在SemEval-2025任务4中的模型融合去学习 

**Authors**: Haoming Xu, Shuxun Wang, Yanqiu Zhao, Yi Zhong, Ziyan Jiang, Ningyuan Zhao, Shumin Deng, Huajun Chen, Ningyu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2503.21088)  

**Abstract**: This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at this https URL. 

**Abstract (ZH)**: 本论文呈现了ZJUKLAB团队在SemEval-2025 Task 4：从大型语言模型中删除敏感内容提交的内容。本任务旨在从大型语言模型中选择性地删除敏感知识，避免遗忘过度和遗忘不足的问题。我们提出了一种利用Model Merging（具体为TIES-Merging）的未学习系统，将两个专门的模型合并为一个更加平衡的未学习模型。我们的系统取得了竞争性的结果，排名第二，26支队伍中在线得分为0.944（任务综合）和0.487（总体综合）。在本文中，我们还进行了局部实验，并对未学习过程进行了全面分析，考察了性能轨迹、损失动态和权重视角，以及几个补充实验，以了解我们方法的有效性。此外，我们分析了我们方法和评估指标的缺点，强调仅通过MIA得分和基于ROUGE的指标不足以全面评估成功的未学习。最后，我们强调了未来研究中需要更全面的评估方法和重新思考未学习目标的重要性。代码可在此链接处获取。 

---
# Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems 

**Title (ZH)**: 重定向连接：混合计算机视觉分析揭示印度河与藏彝走廊书写系统之间的视觉相似性 

**Authors**: Ooha Lakkadi Reddy  

**Link**: [PDF](https://arxiv.org/pdf/2503.21074)  

**Abstract**: This thesis employs a hybrid CNN-Transformer architecture, in conjunction with a detailed anthropological framework, to investigate potential historical connections between the visual morphology of the Indus Valley script and pictographic systems of the Tibetan-Yi Corridor. Through an ensemble methodology of three target scripts across 15 independently trained models, we demonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold higher visual similarity to the Indus script (61.7%-63.5%) than to the Bronze Age Proto-Cuneiform (10.2%-10.9%) or Proto-Elamite (7.6%-8.7%) systems. Additionally and contrarily to our current understanding of the networks of the Indus Valley Civilization, the Indus script unexpectedly maps closer to Tibetan-Yi Corridor scripts, with a mean cosine similarity of 0.629, than to the aforementioned contemporaneous West Asian signaries, both of which recorded mean cosine similarities of 0.104 and 0.080 despite their close geographic proximity and evident trade relations. Across various dimensionality reduction practices and clustering methodologies, the Indus script consistently clusters closest to Tibetan-Yi Corridor scripts. Our computational results align with qualitative observations of specific pictorial parallels in numeral systems, gender markers, and key iconographic elements; this is further supported by archaeological evidence of sustained contact networks along the ancient Shu-Shendu road in tandem with the Indus Valley Civilization's decline, providing a plausible transmission pathway. While alternative explanations cannot be ruled out, the specificity and consistency of observed similarities challenge conventional narratives of isolated script development and suggest more complex ancient cultural transmission networks between South and East Asia than previously recognized. 

**Abstract (ZH)**: 本论文采用混合CNN-Transformer架构，结合详细的人类学框架，探讨印度河谷文字与西藏-彝走廊象形系统的视觉形态之间潜在的历史联系。通过针对三个目标文字训练的15个独立模型的集成方法，我们证明了西藏-彝走廊文字与印度河谷文字的视觉相似度大约高六倍（61.7%-63.5%），远高于青铜时代楔形文字初型（10.2%-10.9%）或埃兰文字初型（7.6%-8.7%）系统。此外，与我们对印度河文明网络的理解相反，印度河谷文字意外地与西藏-彝走廊文字更接近，平均余弦相似度为0.629，而上述同时期的西亚洲符号系统分别为0.104和0.080，尽管它们地理上接近且存在贸易关系。在各种降维实践和聚类方法中，印度河谷文字始终与西藏-彝走廊文字聚类最近。我们的计算结果与特定数字符号、性别标志和关键图象学元素的定性观察相符；这进一步得到了古代松都路沿线持续接触网络以及印度河谷文明衰落的考古证据的支持，提供了可能的传播途径。虽然可以提出替代解释，但观察到的特定和一致的相似性挑战了孤立文字发展的传统叙述，并暗示南亚与东亚之间比以前认识的更加复杂的文化传播网络。 

---
# Can Large Language Models Predict Associations Among Human Attitudes? 

**Title (ZH)**: 大规模语言模型能否预测人类态度之间的关联？ 

**Authors**: Ana Ma, Derek Powell  

**Link**: [PDF](https://arxiv.org/pdf/2503.21011)  

**Abstract**: Prior work has shown that large language models (LLMs) can predict human attitudes based on other attitudes, but this work has largely focused on predictions from highly similar and interrelated attitudes. In contrast, human attitudes are often strongly associated even across disparate and dissimilar topics. Using a novel dataset of human responses toward diverse attitude statements, we found that a frontier language model (GPT-4o) was able to recreate the pairwise correlations among individual attitudes and to predict individuals' attitudes from one another. Crucially, in an advance over prior work, we tested GPT-4o's ability to predict in the absence of surface-similarity between attitudes, finding that while surface similarity improves prediction accuracy, the model was still highly-capable of generating meaningful social inferences between dissimilar attitudes. Altogether, our findings indicate that LLMs capture crucial aspects of the deeper, latent structure of human belief systems. 

**Abstract (ZH)**: 前期研究显示，大规模语言模型 (LLMs) 可以根据其他态度预测人类态度，但这些研究主要集中在高度相似和相关态度的预测上。相比之下，人类态度常常在不相干和不相似的主题之间表现出强烈的相关性。利用一个关于人类对多样态度陈述的新型数据集，我们发现前沿语言模型 (GPT-4o) 能够重建各个态度之间的成对相关性，并从一个态度预测另一个态度。关键的是，与先前的工作相比，我们测试了 GPT-4o 在没有表面相似性的情况下进行预测的能力，发现尽管表面相似性提高了预测准确性，但该模型仍然能够有效地生成不相似态度之间的有意义的社会推断。整体来看，我们的研究结果表明，LLMs 捕捉到了人类信念系统深层、潜在结构的关键方面。 

---
# Improving User Behavior Prediction: Leveraging Annotator Metadata in Supervised Machine Learning Models 

**Title (ZH)**: 改进用户行为预测：在监督机器学习模型中利用标注者元数据 

**Authors**: Lynnette Hui Xian Ng, Kokil Jaidka, Kaiyuan Tay, Hansin Ahuja, Niyati Chhaya  

**Link**: [PDF](https://arxiv.org/pdf/2503.21000)  

**Abstract**: Supervised machine-learning models often underperform in predicting user behaviors from conversational text, hindered by poor crowdsourced label quality and low NLP task accuracy. We introduce the Metadata-Sensitive Weighted-Encoding Ensemble Model (MSWEEM), which integrates annotator meta-features like fatigue and speeding. First, our results show MSWEEM outperforms standard ensembles by 14\% on held-out data and 12\% on an alternative dataset. Second, we find that incorporating signals of annotator behavior, such as speed and fatigue, significantly boosts model performance. Third, we find that annotators with higher qualifications, such as Master's, deliver more consistent and faster annotations. Given the increasing uncertainty over annotation quality, our experiments show that understanding annotator patterns is crucial for enhancing model accuracy in user behavior prediction. 

**Abstract (ZH)**: 监督机器学习模型在预测对话文本用户行为时 often underperform，受限于crowdsourced标签质量差和低NLP任务准确性。我们引入了敏感元数据加权编码集成模型（MSWEEM），该模型整合了注释员的元特征，如疲劳和加速。首先，我们的结果显示，MSWEEM在保留数据上优于标准集成14%，在替代数据集上优于12%。其次，我们发现整合注释员行为信号，如速度和疲劳，显著提升了模型性能。第三，我们发现具有较高资质的注释员，如硕士学历，提供更一致且更快的标注。鉴于注释质量的不确定性日益增加，我们的实验表明，了解注释员模式对于提高用户行为预测模型的准确性至关重要。 

---
# FinAudio: A Benchmark for Audio Large Language Models in Financial Applications 

**Title (ZH)**: FinAudio: 金融应用中音频大型语言模型的标准benchmark 

**Authors**: Yupeng Cao, Haohang Li, Yangyang Yu, Shashidhar Reddy Javaji, Yueru He, Jimin Huang, Zining Zhu, Qianqian Xie, Xiao-yang Liu, Koduvayur Subbalakshmi, Meikang Qiu, Sophia Ananiadou, Jian-Yun Nie  

**Link**: [PDF](https://arxiv.org/pdf/2503.20990)  

**Abstract**: Audio Large Language Models (AudioLLMs) have received widespread attention and have significantly improved performance on audio tasks such as conversation, audio understanding, and automatic speech recognition (ASR). Despite these advancements, there is an absence of a benchmark for assessing AudioLLMs in financial scenarios, where audio data, such as earnings conference calls and CEO speeches, are crucial resources for financial analysis and investment decisions. In this paper, we introduce \textsc{FinAudio}, the first benchmark designed to evaluate the capacity of AudioLLMs in the financial domain. We first define three tasks based on the unique characteristics of the financial domain: 1) ASR for short financial audio, 2) ASR for long financial audio, and 3) summarization of long financial audio. Then, we curate two short and two long audio datasets, respectively, and develop a novel dataset for financial audio summarization, comprising the \textsc{FinAudio} benchmark. Then, we evaluate seven prevalent AudioLLMs on \textsc{FinAudio}. Our evaluation reveals the limitations of existing AudioLLMs in the financial domain and offers insights for improving AudioLLMs. All datasets and codes will be released. 

**Abstract (ZH)**: 音频大型语言模型（AudioLLMs）已在对话、音频理解和自动语音识别（ASR）等音频任务中受到了广泛关注并取得了显著的性能提升。然而，在金融场景中仍缺乏评估AudioLLMs的基准，而音频数据，如 earnings 会议和 CEO 演讲，对于金融分析和投资决策至关重要。本文介绍了第一个用于评估AudioLLMs在金融领域的能力的基准 \textsc{FinAudio}。我们首先定义了三个基于金融领域特点的任务：1) 短金融音频的ASR，2) 长金融音频的ASR，3) 长金融音频的摘要。然后，我们分别收集了两个短和两个长音频数据集，并开发了一个新的金融音频摘要数据集，即 \textsc{FinAudio} 基准。最后，我们在 \textsc{FinAudio} 上评估了七种流行的 AudioLLMs。我们的评估揭示了现有 AudioLLMs 在金融领域中的局限性，并提供了改进 AudioLLMs 的见解。所有数据集和代码将开源。 

---
# Patients Speak, AI Listens: LLM-based Analysis of Online Reviews Uncovers Key Drivers for Urgent Care Satisfaction 

**Title (ZH)**: 患者发声，AI倾听：基于LLM的在线评价分析揭示急診满意的关键驱动因素 

**Authors**: Xiaoran Xu, Zhaoqian Xue, Chi Zhang, Jhonatan Medri, Junjie Xiong, Jiayan Zhou, Jin Jin, Yongfeng Zhang, Siyuan Ma, Lingyao Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.20981)  

**Abstract**: Investigating the public experience of urgent care facilities is essential for promoting community healthcare development. Traditional survey methods often fall short due to limited scope, time, and spatial coverage. Crowdsourcing through online reviews or social media offers a valuable approach to gaining such insights. With recent advancements in large language models (LLMs), extracting nuanced perceptions from reviews has become feasible. This study collects Google Maps reviews across the DMV and Florida areas and conducts prompt engineering with the GPT model to analyze the aspect-based sentiment of urgent care. We first analyze the geospatial patterns of various aspects, including interpersonal factors, operational efficiency, technical quality, finances, and facilities. Next, we determine Census Block Group(CBG)-level characteristics underpinning differences in public perception, including population density, median income, GINI Index, rent-to-income ratio, household below poverty rate, no insurance rate, and unemployment rate. Our results show that interpersonal factors and operational efficiency emerge as the strongest determinants of patient satisfaction in urgent care, while technical quality, finances, and facilities show no significant independent effects when adjusted for in multivariate models. Among socioeconomic and demographic factors, only population density demonstrates a significant but modest association with patient ratings, while the remaining factors exhibit no significant correlations. Overall, this study highlights the potential of crowdsourcing to uncover the key factors that matter to residents and provide valuable insights for stakeholders to improve public satisfaction with urgent care. 

**Abstract (ZH)**: 探究急診機構的公共体验对于促进社區醫療服務發展至為Important。透過Crowdsourcing線上評論或社交媒體獲取此類洞察具有重要價值。隨著大型語言模型（LLMs）的進展，從評論中提取微妙感知已成為可能。本研究收集DMV和佛羅里達地區的Google Maps評論，並使用GPT模型進行提示工程，分析急診服務的方面基 erotica情感。首先，我們分析了包括人際因素、運作效率、技術質量、財務和設施等方面的地理空間模式。隨後，我們確定不同人群感知差異的Sub Census區組（CBG）級別特性，包括人口密度、中位收入、基尼係數、租金與收入比、貧困家庭占比、無保险率和失業率。研究結果表明，在多變量模型中調整後，人際因素和運作效率是影響患者滿意度的最主要因素，而技術質量、財務和設施則未顯示出顯著的獨立影響。就社會經濟和人口統計因素而言，只有人口密度與患者評估顯示出顯著但微弱的關聯，而其他因素則無顯著相關性。總體而言，本研究突顯了Crowdsourcing的潛力，可用於發現對居民重要的關鍵因素，並為利益相關者提供寶貴的洞見，以提高公眾對急診服務的滿意度。 

---
# Competitive Multi-armed Bandit Games for Resource Sharing 

**Title (ZH)**: 竞争性多臂 bandeit 游戏及其在资源分配中的应用 

**Authors**: Hongbo Li, Lingjie Duan  

**Link**: [PDF](https://arxiv.org/pdf/2503.20975)  

**Abstract**: In modern resource-sharing systems, multiple agents access limited resources with unknown stochastic conditions to perform tasks. When multiple agents access the same resource (arm) simultaneously, they compete for successful usage, leading to contention and reduced rewards. This motivates our study of competitive multi-armed bandit (CMAB) games. In this paper, we study a new N-player K-arm competitive MAB game, where non-myopic players (agents) compete with each other to form diverse private estimations of unknown arms over time. Their possible collisions on same arms and time-varying nature of arm rewards make the policy analysis more involved than existing studies for myopic players. We explicitly analyze the threshold-based structures of social optimum and existing selfish policy, showing that the latter causes prolonged convergence time $\Omega(\frac{K}{\eta^2}\ln({\frac{KN}{\delta}}))$, while socially optimal policy with coordinated communication reduces it to $\mathcal{O}(\frac{K}{N\eta^2}\ln{(\frac{K}{\delta})})$. Based on the comparison, we prove that the competition among selfish players for the best arm can result in an infinite price of anarchy (PoA), indicating an arbitrarily large efficiency loss compared to social optimum. We further prove that no informational (non-monetary) mechanism (including Bayesian persuasion) can reduce the infinite PoA, as the strategic misreporting by non-myopic players undermines such approaches. To address this, we propose a Combined Informational and Side-Payment (CISP) mechanism, which provides socially optimal arm recommendations with proper informational and monetary incentives to players according to their time-varying private beliefs. Our CISP mechanism keeps ex-post budget balanced for social planner and ensures truthful reporting from players, achieving the minimum PoA=1 and same convergence time as social optimum. 

**Abstract (ZH)**: 现代资源共享系统中多臂 bandit 竞争游戏的研究 

---
# Sociotechnical Effects of Machine Translation 

**Title (ZH)**: 机器翻译的 sociotechnical 影响 

**Authors**: Joss Moorkens, Andy Way, Séamus Lankford  

**Link**: [PDF](https://arxiv.org/pdf/2503.20959)  

**Abstract**: While the previous chapters have shown how machine translation (MT) can be useful, in this chapter we discuss some of the side-effects and risks that are associated, and how they might be mitigated. With the move to neural MT and approaches using Large Language Models (LLMs), there is an associated impact on climate change, as the models built by multinational corporations are massive. They are hugely expensive to train, consume large amounts of electricity, and output huge volumes of kgCO2 to boot. However, smaller models which still perform to a high level of quality can be built with much lower carbon footprints, and tuning pre-trained models saves on the requirement to train from scratch. We also discuss the possible detrimental effects of MT on translators and other users. The topics of copyright and ownership of data are discussed, as well as ethical considerations on data and MT use. Finally, we show how if done properly, using MT in crisis scenarios can save lives, and we provide a method of how this might be done. 

**Abstract (ZH)**: 神经机器翻译与大型语言模型带来的副作用和风险及其缓解策略：对气候变化的影响与伦理考量及危机情境下的应用 

---
# TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models 

**Title (ZH)**: TS-Inverse：一种针对联邦时间序列预测模型的梯度反转攻击 

**Authors**: Caspar Meijer, Jiyue Huang, Shreshtha Sharma, Elena Lazovik, Lydia Y. Chen  

**Link**: [PDF](https://arxiv.org/pdf/2503.20952)  

**Abstract**: Federated learning (FL) for time series forecasting (TSF) enables clients with privacy-sensitive time series (TS) data to collaboratively learn accurate forecasting models, for example, in energy load prediction. Unfortunately, privacy risks in FL persist, as servers can potentially reconstruct clients' training data through gradient inversion attacks (GIA). Although GIA is demonstrated for image classification tasks, little is known about time series regression tasks. In this paper, we first conduct an extensive empirical study on inverting TS data across 4 TSF models and 4 datasets, identifying the unique challenges of reconstructing both observations and targets of TS data. We then propose TS-Inverse, a novel GIA that improves the inversion of TS data by (i) learning a gradient inversion model that outputs quantile predictions, (ii) a unique loss function that incorporates periodicity and trend regularization, and (iii) regularization according to the quantile predictions. Our evaluations demonstrate a remarkable performance of TS-Inverse, achieving at least a 2x-10x improvement in terms of the sMAPE metric over existing GIA methods on TS data. Code repository: this https URL 

**Abstract (ZH)**: 联邦学习（FL）在时间序列预测（TSF）中的应用使得拥有敏感隐私时间序列（TS）数据的客户端能够协作学习准确的预测模型，例如在电力负荷预测中的应用。然而，FL中的隐私风险仍然存在，因为服务器可以通过梯度反转攻击（GIA）潜在地重构客户端的训练数据。尽管GIA已经在图像分类任务中得到验证，但在时间序列回归任务中的应用尚不明确。在本文中，我们首先在4个TSF模型和4个数据集上进行了广泛的经验研究，识别出重构时间序列数据观察值和目标值的独特挑战。然后，我们提出了TS-Inverse，这是一种新提出的GIA方法，通过（i）学习输出分位数预测的梯度反转模型，（ii）结合周期性和趋势正则化的独特损失函数，以及（iii）根据分位数预测进行正则化，来改进时间序列数据的重构。我们的评估结果表明，TS-Inverse在时间序列数据上的sMAPE指标上至少实现了2倍至10倍的改进，优于现有GIA方法。代码仓库：this https URL 

---
# LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos 

**Title (ZH)**: LATTE-MV: 从单目视频学习预测乒乓球击球动作 

**Authors**: Daniel Etaat, Dvij Kalaria, Nima Rahmanian, Shankar Sastry  

**Link**: [PDF](https://arxiv.org/pdf/2503.20936)  

**Abstract**: Physical agility is a necessary skill in competitive table tennis, but by no means sufficient. Champions excel in this fast-paced and highly dynamic environment by anticipating their opponent's intent - buying themselves the necessary time to react. In this work, we take one step towards designing such an anticipatory agent. Previous works have developed systems capable of real-time table tennis gameplay, though they often do not leverage anticipation. Among the works that forecast opponent actions, their approaches are limited by dataset size and variety. Our paper contributes (1) a scalable system for reconstructing monocular video of table tennis matches in 3D and (2) an uncertainty-aware controller that anticipates opponent actions. We demonstrate in simulation that our policy improves the ball return rate against high-speed hits from 49.9% to 59.0% as compared to a baseline non-anticipatory policy. 

**Abstract (ZH)**: 物理敏捷性是竞争性乒乓球比赛中的一项必要技能，但绝非充分条件。冠军在这一快速且高度动态的环境中表现出色，通过预测对手的意图为自己争取必要的反应时间。在这项工作中，我们朝着设计这样一种预见性代理迈出了一步。先前的研究开发了能够实现实时乒乓球游戏的系统，尽管它们往往没有利用预见性。在预测对手动作的研究中，这些方法受限于数据集的大小和多样性。我们的论文贡献了（1）一个可扩展的系统，可以重建3D乒乓球比赛的单目视频，以及（2）一种具备不确定性意识的控制器，能够预见对手的动作。我们通过模拟证明，与基准的非预见性策略相比，我们的策略在面对高速击球时的还击率从49.9%提高到了59.0%。 

---
# Prototype Guided Backdoor Defense 

**Title (ZH)**: 原型引导式后门防御 

**Authors**: Venkat Adithya Amula, Sunayana Samavedam, Saurabh Saini, Avani Gupta, Narayanan P J  

**Link**: [PDF](https://arxiv.org/pdf/2503.20925)  

**Abstract**: Deep learning models are susceptible to {\em backdoor attacks} involving malicious attackers perturbing a small subset of training data with a {\em trigger} to causes misclassifications. Various triggers have been used, including semantic triggers that are easily realizable without requiring the attacker to manipulate the image. The emergence of generative AI has eased the generation of varied poisoned samples. Robustness across types of triggers is crucial to effective defense. We propose Prototype Guided Backdoor Defense (PGBD), a robust post-hoc defense that scales across different trigger types, including previously unsolved semantic triggers. PGBD exploits displacements in the geometric spaces of activations to penalize movements toward the trigger. This is done using a novel sanitization loss of a post-hoc fine-tuning step. The geometric approach scales easily to all types of attacks. PGBD achieves better performance across all settings. We also present the first defense against a new semantic attack on celebrity face images. Project page: \hyperlink{this https URL}{this https URL}. 

**Abstract (ZH)**: 深度学习模型易受涉及恶意攻击者通过在一小部分训练数据中添加触发器以引起误分类的后门攻击影响。各种触发器已被使用，包括可以通过不需攻击者篡改图像即可轻松实现的语义触发器。生成式AI的出现简化了各种中毒样本的生成。不同类型的触发器之间的稳健性对于有效的防御至关重要。我们提出了一种名为Prototype Guided Backdoor Defense (PGBD)的稳健的后处理防御方法，它能够跨不同类型的触发器扩展，包括之前未解决的语义触发器。PGBD 利用激活的空间移位来惩罚向触发器方向的移动。这使用了一种新颖的后处理微调步骤中的 sanitization loss 实现。几何方法可以轻松扩展到所有类型的攻击。PGBD 在所有设置中均表现出更好的性能。我们还提出了针对名人面部图像新语义攻击的第一种防御方法。项目页面：[此链接](此链接)。 

---
# D4R -- Exploring and Querying Relational Graphs Using Natural Language and Large Language Models -- the Case of Historical Documents 

**Title (ZH)**: D4R —— 使用自然语言和大规模语言模型探索和查询关系图——以历史文件为例 

**Authors**: Michel Boeglin, David Kahn, Josiane Mothe, Diego Ortiz, David Panzoli  

**Link**: [PDF](https://arxiv.org/pdf/2503.20914)  

**Abstract**: D4R is a digital platform designed to assist non-technical users, particularly historians, in exploring textual documents through advanced graphical tools for text analysis and knowledge extraction. By leveraging a large language model, D4R translates natural language questions into Cypher queries, enabling the retrieval of data from a Neo4J database. A user-friendly graphical interface allows for intuitive interaction, enabling users to navigate and analyse complex relational data extracted from unstructured textual documents. Originally designed to bridge the gap between AI technologies and historical research, D4R's capabilities extend to various other domains. A demonstration video and a live software demo are available. 

**Abstract (ZH)**: D4R是一个数字平台，旨在通过高级图形工具辅助非技术人员，特别是历史学家，探索文本文档中的文本分析和知识提取。利用大型语言模型，D4R将自然语言问题转换为Cypher查询，从而可以从Neo4J数据库中检索数据。一个用户友好的图形界面允许直观的交互，使用户能够导航和分析从非结构化文本文档中提取的复杂关系数据。最初设计用于弥合AI技术和历史研究之间的差距，D4R的能力扩展到其他众多领域。提供演示视频和现场软件演示。 

---
# Assessing Generative Models for Structured Data 

**Title (ZH)**: 评估生成模型在结构化数据上的表现 

**Authors**: Reilly Cannon, Nicolette M. Laird, Caesar Vazquez, Andy Lin, Amy Wagler, Tony Chiang  

**Link**: [PDF](https://arxiv.org/pdf/2503.20903)  

**Abstract**: Synthetic tabular data generation has emerged as a promising method to address limited data availability and privacy concerns. With the sharp increase in the performance of large language models in recent years, researchers have been interested in applying these models to the generation of tabular data. However, little is known about the quality of the generated tabular data from large language models. The predominant method for assessing the quality of synthetic tabular data is the train-synthetic-test-real approach, where the artificial examples are compared to the original by how well machine learning models, trained separately on the real and synthetic sets, perform in some downstream tasks. This method does not directly measure how closely the distribution of generated data approximates that of the original. This paper introduces rigorous methods for directly assessing synthetic tabular data against real data by looking at inter-column dependencies within the data. We find that large language models (GPT-2), both when queried via few-shot prompting and when fine-tuned, and GAN (CTGAN) models do not produce data with dependencies that mirror the original real data. Results from this study can inform future practice in synthetic data generation to improve data quality. 

**Abstract (ZH)**: 合成表格数据生成已成为解决数据稀缺性和隐私问题的有前景方法。随着大型语言模型性能在近年来的显著提升，研究人员的兴趣转向将这些模型应用于表格数据的生成。然而，关于大型语言模型生成的表格数据质量的研究仍然不多。目前评估合成表格数据质量的主要方法是“训练-合成-测试-真实”方法，通过分别在真实数据集和合成数据集上训练的机器学习模型在下游任务上的表现来比较人工示例与原始数据。该方法没有直接测量生成数据分布与原始数据分布的接近程度。本文引入了直接将合成表格数据与真实数据进行比较，通过分析数据内在列依赖性的严格方法。研究发现，无论是通过少样本提示查询还是微调的大语言模型（GPT-2）和生成对抗网络（CTGAN）模型生成的数据，其依赖关系均未能忠实反映原始真实数据的依赖结构。本研究的结果可为未来合成数据生成实践改进数据质量提供指导。 

---
# Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework 

**Title (ZH)**: 基于GAN的防御框架：针对中毒攻击的鲁棒联邦学习 

**Authors**: Usama Zafar, André Teixeira, Salman Toor  

**Link**: [PDF](https://arxiv.org/pdf/2503.20884)  

**Abstract**: Federated Learning (FL) enables collaborative model training across decentralized devices without sharing raw data, but it remains vulnerable to poisoning attacks that compromise model integrity. Existing defenses often rely on external datasets or predefined heuristics (e.g. number of malicious clients), limiting their effectiveness and scalability. To address these limitations, we propose a privacy-preserving defense framework that leverages a Conditional Generative Adversarial Network (cGAN) to generate synthetic data at the server for authenticating client updates, eliminating the need for external datasets. Our framework is scalable, adaptive, and seamlessly integrates into FL workflows. Extensive experiments on benchmark datasets demonstrate its robust performance against a variety of poisoning attacks, achieving high True Positive Rate (TPR) and True Negative Rate (TNR) of malicious and benign clients, respectively, while maintaining model accuracy. The proposed framework offers a practical and effective solution for securing federated learning systems. 

**Abstract (ZH)**: 联邦学习（FL）能够在不共享原始数据的情况下跨去中心化设备进行协作模型训练，但仍然容易受到破坏性攻击的影响，这些攻击会损害模型的完整性。现有的防御措施往往依赖于外部数据集或预定义的启发式方法（例如恶意客户端的数量），这限制了它们的有效性和可扩展性。为了解决这些限制，我们提出了一种基于条件生成对抗网络（cGAN）的隐私保护防御框架，该框架在服务器端生成合成数据以验证客户端更新，从而消除对外部数据集的依赖。该框架可扩展、适应性强，并能够无缝集成到联邦学习工作流中。基准数据集上的 extensive 实验表明，该框架对各种破坏性攻击具有稳健的性能，能够分别实现恶意和良性客户端的高真阳性率（TPR）和真阴性率（TNR），同时保持模型精度。所提出的框架为保障联邦学习系统的安全性提供了实用且有效的解决方案。 

---
# VinaBench: Benchmark for Faithful and Consistent Visual Narratives 

**Title (ZH)**: VinaBench：视觉叙述忠实性和一致性基准 

**Authors**: Silin Gao, Sheryl Mathew, Li Mi, Sepideh Mamooler, Mengjie Zhao, Hiromi Wakaki, Yuki Mitsufuji, Syrielle Montariol, Antoine Bosselut  

**Link**: [PDF](https://arxiv.org/pdf/2503.20871)  

**Abstract**: Visual narrative generation transforms textual narratives into sequences of images illustrating the content of the text. However, generating visual narratives that are faithful to the input text and self-consistent across generated images remains an open challenge, due to the lack of knowledge constraints used for planning the stories. In this work, we propose a new benchmark, VinaBench, to address this challenge. Our benchmark annotates the underlying commonsense and discourse constraints in visual narrative samples, offering systematic scaffolds for learning the implicit strategies of visual storytelling. Based on the incorporated narrative constraints, we further propose novel metrics to closely evaluate the consistency of generated narrative images and the alignment of generations with the input textual narrative. Our results across three generative vision models demonstrate that learning with VinaBench's knowledge constraints effectively improves the faithfulness and cohesion of generated visual narratives. 

**Abstract (ZH)**: 视觉叙事生成将文本叙事转换为展现文本内容的图像序列。然而，生成既忠实于输入文本又能跨图像保持一致性的视觉叙事仍然是一个开放性的挑战，这主要是由于缺乏用于规划故事的知识约束。在此工作中，我们提出一个新的基准VinaBench以应对这一挑战。我们的基准标注了视觉叙事样本中的底层常识和话语约束，为学习视觉叙事中的隐含策略提供系统性框架。基于纳入的故事约束，我们进一步提出新的评估指标，以更紧密地评估生成叙事图像的一致性及生成与输入文本叙事的一致性。我们的结果表明，在VinaBench知识约束下学习能够有效提高生成的视觉叙事的真实性和连贯性。 

---
# Unified Multimodal Discrete Diffusion 

**Title (ZH)**: 统一多模态离散扩散 

**Authors**: Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, Katerina Fragkiadaki  

**Link**: [PDF](https://arxiv.org/pdf/2503.20853)  

**Abstract**: Multimodal generative models that can understand and generate across multiple modalities are dominated by autoregressive (AR) approaches, which process tokens sequentially from left to right, or top to bottom. These models jointly handle images, text, video, and audio for various tasks such as image captioning, question answering, and image generation. In this work, we explore discrete diffusion models as a unified generative formulation in the joint text and image domain, building upon their recent success in text generation. Discrete diffusion models offer several advantages over AR models, including improved control over quality versus diversity of generated samples, the ability to perform joint multimodal inpainting (across both text and image domains), and greater controllability in generation through guidance. Leveraging these benefits, we present the first Unified Multimodal Discrete Diffusion (UniDisc) model which is capable of jointly understanding and generating text and images for a variety of downstream tasks. We compare UniDisc to multimodal AR models, performing a scaling analysis and demonstrating that UniDisc outperforms them in terms of both performance and inference-time compute, enhanced controllability, editability, inpainting, and flexible trade-off between inference time and generation quality. Code and additional visualizations are available at this https URL. 

**Abstract (ZH)**: 多模态生成模型能够在图像、文本、视频和音频等多个模态间进行理解和生成，目前主要依赖自回归（AR）方法，这些方法按从左到右或从上到下的顺序处理令牌。本工作中，我们探索离散扩散模型作为联合文本和图像域中的统一生成框架，并利用其在文本生成领域的近期成功。离散扩散模型相比自回归模型具有多个优势，包括提高生成样本的质量与多样性的控制、在文本和图像域内联合多模态填补的 ability，以及通过指导增强生成的可控性。利用这些优势，我们提出了首个统一多模态离散扩散（UniDisc）模型，该模型能够联合理解和生成文本和图像，适用于多种下游任务。我们将 UniDisc 与多模态自回归模型进行比较，并进行扩展性分析，结果表明 UniDisc 在性能、推理时延计算、可控性、可编辑性、填补能力以及推理时间与生成质量的灵活权衡方面均优于后者。相关代码和额外可视化信息可访问此链接。 

---
# The Backfiring Effect of Weak AI Safety Regulation 

**Title (ZH)**: 弱人工智能安全监管的反作用效应 

**Authors**: Benjamin Laufer, Jon Kleinberg, Hoda Heidari  

**Link**: [PDF](https://arxiv.org/pdf/2503.20848)  

**Abstract**: Recent policy proposals aim to improve the safety of general-purpose AI, but there is little understanding of the efficacy of different regulatory approaches to AI safety. We present a strategic model that explores the interactions between the regulator, the general-purpose AI technology creators, and domain specialists--those who adapt the AI for specific applications. Our analysis examines how different regulatory measures, targeting different parts of the development chain, affect the outcome of the development process. In particular, we assume AI technology is described by two key attributes: safety and performance. The regulator first sets a minimum safety standard that applies to one or both players, with strict penalties for non-compliance. The general-purpose creator then develops the technology, establishing its initial safety and performance levels. Next, domain specialists refine the AI for their specific use cases, and the resulting revenue is distributed between the specialist and generalist through an ex-ante bargaining process. Our analysis of this game reveals two key insights: First, weak safety regulation imposed only on the domain specialists can backfire. While it might seem logical to regulate use cases (as opposed to the general-purpose technology), our analysis shows that weak regulations targeting domain specialists alone can unintentionally reduce safety. This effect persists across a wide range of settings. Second, in sharp contrast to the previous finding, we observe that stronger, well-placed regulation can in fact benefit all players subjected to it. When regulators impose appropriate safety standards on both AI creators and domain specialists, the regulation functions as a commitment mechanism, leading to safety and performance gains, surpassing what is achieved under no regulation or regulating one player only. 

**Abstract (ZH)**: 近期的政策建议旨在提高通用人工智能的安全性，但对不同监管方法的有效性缺乏理解。我们提出了一种战略模型，探讨监管者、通用人工智能技术创造者和领域专家（针对特定应用适应AI的技术专家）之间的互动。我们的分析研究了针对开发链不同环节的不同监管措施如何影响开发过程的结果。特别是，我们假设人工智能技术由两个关键属性描述：安全性和性能。监管者首先设定一个适用于一方或双方的最低安全标准，并对不合规行为实施严厉处罚。通用人工智能创造者随后开发技术，确定其初始的安全性和性能水平。接着，领域专家针对其特定使用场景细化人工智能，结果的收入通过事前谈判在专家和创造者之间分配。我们对这一博弈的分析揭示了两个关键洞察：首先，仅针对领域专家施加较弱的安全监管可能会产生反效果。尽管监管使用案例（而非通用技术）似乎是合理的，但我们的分析表明，仅针对领域专家的较弱监管可能会无意中降低安全性，这种效果在广泛的情景中都存在。其次，与上述发现形成鲜明对比的是，更强有力但恰到好处的监管实际上可以惠及所有受监管方。当监管者对人工智能创造者和领域专家都施加适当的安全标准时，监管将作为一个承诺机制，导致安全性和性能的提升，超过在没有监管或仅监管一方的情况下所能取得的成果。 

---
# Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks 

**Title (ZH)**: 通过自适应梯度掩蔽对抗攻击实现机器人领域的鲁棒深度强化学习 

**Authors**: Zongyuan Zhang, Tianyang Duan, Zheng Lin, Dong Huang, Zihan Fang, Zekai Sun, Ling Xiong, Hongbin Liang, Heming Cui, Yong Cui, Yue Gao  

**Link**: [PDF](https://arxiv.org/pdf/2503.20844)  

**Abstract**: Deep reinforcement learning (DRL) has emerged as a promising approach for robotic control, but its realworld deployment remains challenging due to its vulnerability to environmental perturbations. Existing white-box adversarial attack methods, adapted from supervised learning, fail to effectively target DRL agents as they overlook temporal dynamics and indiscriminately perturb all state dimensions, limiting their impact on long-term rewards. To address these challenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR) Attack, a white-box attack method that combines DRL with a gradient-based soft masking mechanism to dynamically identify critical state dimensions and optimize adversarial policies. AGMR selectively allocates perturbations to the most impactful state features and incorporates a dynamic adjustment mechanism to balance exploration and exploitation during training. Extensive experiments demonstrate that AGMR outperforms state-of-the-art adversarial attack methods in degrading the performance of the victim agent and enhances the victim agent's robustness through adversarial defense mechanisms. 

**Abstract (ZH)**: 基于梯度掩蔽的自适应 gradient 调整强化学习（AGMR）攻击：一种白盒攻击方法 

---
# Advancing Vulnerability Classification with BERT: A Multi-Objective Learning Model 

**Title (ZH)**: 基于BERT的多目标学习模型：推动漏洞分类技术进步 

**Authors**: Himanshu Tiwari  

**Link**: [PDF](https://arxiv.org/pdf/2503.20831)  

**Abstract**: The rapid increase in cybersecurity vulnerabilities necessitates automated tools for analyzing and classifying vulnerability reports. This paper presents a novel Vulnerability Report Classifier that leverages the BERT (Bidirectional Encoder Representations from Transformers) model to perform multi-label classification of Common Vulnerabilities and Exposures (CVE) reports from the National Vulnerability Database (NVD). The classifier predicts both the severity (Low, Medium, High, Critical) and vulnerability types (e.g., Buffer Overflow, XSS) from textual descriptions. We introduce a custom training pipeline using a combined loss function-Cross-Entropy for severity and Binary Cross-Entropy with Logits for types-integrated into a Hugging Face Trainer subclass. Experiments on recent NVD data demonstrate promising results, with decreasing evaluation loss across epochs. The system is deployed via a REST API and a Streamlit UI, enabling real-time vulnerability analysis. This work contributes a scalable, open-source solution for cybersecurity practitioners to automate vulnerability triage. 

**Abstract (ZH)**: 快速增加的网络安全漏洞促使开发自动化工具来分析和分类漏洞报告。本文提出了一种新颖的漏洞报告分类器，该分类器利用BERT模型对国家漏洞数据库（NVD）中的Common Vulnerabilities and Exposures（CVE）报告进行多标签分类。分类器可以从文本描述中预测漏洞的严重程度（低、中、高、关键）和类型（如缓冲区溢出、XSS等）。我们引入了一个自定义的训练管道，使用结合交叉熵损失函数（用于严重程度）和带标签的二元交叉熵损失函数（用于类型），并通过Hugging Face Trainer子类进行集成。在最近的NVD数据上的实验展示了有希望的结果，评估损失在迭代中逐渐降低。该系统通过REST API和Streamlit UI部署，实现实时漏洞分析。本文为网络安全从业人员提供了一种可扩展的开源解决方案，用于自动化漏洞triage。 

---
# Exploiting Temporal State Space Sharing for Video Semantic Segmentation 

**Title (ZH)**: 利用时空状态空间共享进行视频语义分割 

**Authors**: Syed Ariff Syed Hesham, Yun Liu, Guolei Sun, Henghui Ding, Jing Yang, Ender Konukoglu, Xue Geng, Xudong Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2503.20824)  

**Abstract**: Video semantic segmentation (VSS) plays a vital role in understanding the temporal evolution of scenes. Traditional methods often segment videos frame-by-frame or in a short temporal window, leading to limited temporal context, redundant computations, and heavy memory requirements. To this end, we introduce a Temporal Video State Space Sharing (TV3S) architecture to leverage Mamba state space models for temporal feature sharing. Our model features a selective gating mechanism that efficiently propagates relevant information across video frames, eliminating the need for a memory-heavy feature pool. By processing spatial patches independently and incorporating shifted operation, TV3S supports highly parallel computation in both training and inference stages, which reduces the delay in sequential state space processing and improves the scalability for long video sequences. Moreover, TV3S incorporates information from prior frames during inference, achieving long-range temporal coherence and superior adaptability to extended sequences. Evaluations on the VSPW and Cityscapes datasets reveal that our approach outperforms current state-of-the-art methods, establishing a new standard for VSS with consistent results across long video sequences. By achieving a good balance between accuracy and efficiency, TV3S shows a significant advancement in spatiotemporal modeling, paving the way for efficient video analysis. The code is publicly available at this https URL. 

**Abstract (ZH)**: 基于Mamba状态空间模型的时空视频状态空间共享架构在视频语义分割中的应用 

---
# Synthetic Video Enhances Physical Fidelity in Video Synthesis 

**Title (ZH)**: 合成视频在视频合成中增强物理保真度 

**Authors**: Qi Zhao, Xingyu Ni, Ziyu Wang, Feng Cheng, Ziyan Yang, Lu Jiang, Bohan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2503.20822)  

**Abstract**: We investigate how to enhance the physical fidelity of video generation models by leveraging synthetic videos derived from computer graphics pipelines. These rendered videos respect real-world physics, such as maintaining 3D consistency, and serve as a valuable resource that can potentially improve video generation models. To harness this potential, we propose a solution that curates and integrates synthetic data while introducing a method to transfer its physical realism to the model, significantly reducing unwanted artifacts. Through experiments on three representative tasks emphasizing physical consistency, we demonstrate its efficacy in enhancing physical fidelity. While our model still lacks a deep understanding of physics, our work offers one of the first empirical demonstrations that synthetic video enhances physical fidelity in video synthesis. Website: this https URL 

**Abstract (ZH)**: 我们探究如何通过利用源自计算机图形管道的合成视频来增强视频生成模型的物理保真度。这些渲染视频遵循现实世界的物理法则，如保持三维一致性，并且可以作为有价值的资源，潜在地提高视频生成模型的性能。为充分利用这一潜力，我们提出了一个解决方案，以整理和整合合成数据，并引入了一种方法将其实验现实性转移到模型中，从而显著减少不必要的伪影。通过在三个强调物理一致性的代表性任务上的实验，我们展示了其在增强物理保真度方面的有效性。尽管我们的模型尚未对物理现象有深入的理解，但我们的工作提供了第一个实证证明合成视频在视频合成中增强物理保真度的示例。 

---
# Fundamental Safety-Capability Trade-offs in Fine-tuning Large Language Models 

**Title (ZH)**: 大型语言模型微调中的基础安全能力权衡 

**Authors**: Pin-Yu Chen, Han Shen, Payel Das, Tianyi Chen  

**Link**: [PDF](https://arxiv.org/pdf/2503.20807)  

**Abstract**: Fine-tuning Large Language Models (LLMs) on some task-specific datasets has been a primary use of LLMs. However, it has been empirically observed that this approach to enhancing capability inevitably compromises safety, a phenomenon also known as the safety-capability trade-off in LLM fine-tuning. This paper presents a theoretical framework for understanding the interplay between safety and capability in two primary safety-aware LLM fine-tuning strategies, providing new insights into the effects of data similarity, context overlap, and alignment loss landscape. Our theoretical results characterize the fundamental limits of the safety-capability trade-off in LLM fine-tuning, which are also validated by numerical experiments. 

**Abstract (ZH)**: 大型语言模型（LLM）在某些任务特定数据集上的微调是LLM的主要用途之一。然而，经验上观察到，这种增强能力的方法不可避免地会牺牲安全性，这一现象在LLM微调中也被称为安全与能力之间的权衡。本文提出了一个理论框架，用于理解两种主要的安全意识LLM微调策略之间的相互作用，提供了关于数据相似性、上下文重叠和对齐损失景观影响的新见解。我们的理论结果界定了LLM微调中安全与能力权衡的基本限制，这些结果也得到了数值实验的验证。 

---
# AED: Automatic Discovery of Effective and Diverse Vulnerabilities for Autonomous Driving Policy with Large Language Models 

**Title (ZH)**: AED：使用大型语言模型自动发现有效的多样化自动驾驶政策漏洞 

**Authors**: Le Qiu, Zelai Xu, Qixin Tan, Wenhao Tang, Chao Yu, Yu Wang  

**Link**: [PDF](https://arxiv.org/pdf/2503.20804)  

**Abstract**: Assessing the safety of autonomous driving policy is of great importance, and reinforcement learning (RL) has emerged as a powerful method for discovering critical vulnerabilities in driving policies. However, existing RL-based approaches often struggle to identify vulnerabilities that are both effective-meaning the autonomous vehicle is genuinely responsible for the accidents-and diverse-meaning they span various failure types. To address these challenges, we propose AED, a framework that uses large language models (LLMs) to automatically discover effective and diverse vulnerabilities in autonomous driving policies. We first utilize an LLM to automatically design reward functions for RL training. Then we let the LLM consider a diverse set of accident types and train adversarial policies for different accident types in parallel. Finally, we use preference-based learning to filter ineffective accidents and enhance the effectiveness of each vulnerability. Experiments across multiple simulated traffic scenarios and tested policies show that AED uncovers a broader range of vulnerabilities and achieves higher attack success rates compared with expert-designed rewards, thereby reducing the need for manual reward engineering and improving the diversity and effectiveness of vulnerability discovery. 

**Abstract (ZH)**: 基于大规模语言模型的有效多样自主驾驶政策漏洞评估框架 

---
# CEFW: A Comprehensive Evaluation Framework for Watermark in Large Language Models 

**Title (ZH)**: CEFW：大规模语言模型中水印的全面评估框架 

**Authors**: Shuhao Zhang, Bo Cheng, Jiale Han, Yuli Chen, Zhixuan Wu, Changbao Li, Pingli Gu  

**Link**: [PDF](https://arxiv.org/pdf/2503.20802)  

**Abstract**: Text watermarking provides an effective solution for identifying synthetic text generated by large language models. However, existing techniques often focus on satisfying specific criteria while ignoring other key aspects, lacking a unified evaluation. To fill this gap, we propose the Comprehensive Evaluation Framework for Watermark (CEFW), a unified framework that comprehensively evaluates watermarking methods across five key dimensions: ease of detection, fidelity of text quality, minimal embedding cost, robustness to adversarial attacks, and imperceptibility to prevent imitation or forgery. By assessing watermarks according to all these key criteria, CEFW offers a thorough evaluation of their practicality and effectiveness. Moreover, we introduce a simple and effective watermarking method called Balanced Watermark (BW), which guarantees robustness and imperceptibility through balancing the way watermark information is added. Extensive experiments show that BW outperforms existing methods in overall performance across all evaluation dimensions. We release our code to the community for future research. this https URL. 

**Abstract (ZH)**: 全面评估水印的框架：Text 水印为识别大型语言模型生成的合成文本提供了一种有效的解决方案。然而，现有技术往往专注于满足特定标准，而忽略了其他关键方面，缺乏统一的评估方法。为填补这一空白，我们提出了全面评估水印框架（CEFW），这是一种统一框架，可以全面从五个关键维度评估水印方法：检测的便捷性、文本质量的保真度、嵌入的最小成本、对抗攻击的鲁棒性以及不可感知性，以防止模仿或伪造。通过根据所有这些关键标准评估水印，CEFW 提供了对其实用性和效果的全面评估。此外，我们提出了一种简单有效的水印方法——平衡水印（BW），通过平衡水印信息的添加方式确保了鲁棒性和不可感知性。广泛实验表明，BW 在所有评估维度的整体性能上优于现有方法。我们向社区开源了我们的代码，供未来研究使用。 

---
# Evidencing Unauthorized Training Data from AI Generated Content using Information Isotopes 

**Title (ZH)**: 基于信息同位素确证AI生成内容中未经授权的训练数据 

**Authors**: Qi Tao, Yin Jinhua, Cai Dongqi, Xie Yueqi, Wang Huili, Hu Zhiyang, Yang Peiru, Nan Guoshun, Zhou Zhili, Wang Shangguang, Lyu Lingjuan, Huang Yongfeng, Lane Nicholas  

**Link**: [PDF](https://arxiv.org/pdf/2503.20800)  

**Abstract**: In light of scaling laws, many AI institutions are intensifying efforts to construct advanced AIs on extensive collections of high-quality human data. However, in a rush to stay competitive, some institutions may inadvertently or even deliberately include unauthorized data (like privacy- or intellectual property-sensitive content) for AI training, which infringes on the rights of data owners. Compounding this issue, these advanced AI services are typically built on opaque cloud platforms, which restricts access to internal information during AI training and inference, leaving only the generated outputs available for forensics. Thus, despite the introduction of legal frameworks by various countries to safeguard data rights, uncovering evidence of data misuse in modern opaque AI applications remains a significant challenge. In this paper, inspired by the ability of isotopes to trace elements within chemical reactions, we introduce the concept of information isotopes and elucidate their properties in tracing training data within opaque AI systems. Furthermore, we propose an information isotope tracing method designed to identify and provide evidence of unauthorized data usage by detecting the presence of target information isotopes in AI generations. We conduct experiments on ten AI models (including GPT-4o, Claude-3.5, and DeepSeek) and four benchmark datasets in critical domains (medical data, copyrighted books, and news). Results show that our method can distinguish training datasets from non-training datasets with 99\% accuracy and significant evidence (p-value$<0.001$) by examining a data entry equivalent in length to a research paper. The findings show the potential of our work as an inclusive tool for empowering individuals, including those without expertise in AI, to safeguard their data rights in the rapidly evolving era of AI advancements and applications. 

**Abstract (ZH)**: 基于同位素原理的信息同位素追踪方法在不透明AI系统中追踪训练数据的研究 

---
# Payload-Aware Intrusion Detection with CMAE and Large Language Models 

**Title (ZH)**: 基于CMAE和大语言模型的负载感知入侵检测 

**Authors**: Yongcheol Kim, Chanjae Lee, Young Yoon  

**Link**: [PDF](https://arxiv.org/pdf/2503.20798)  

**Abstract**: Intrusion Detection Systems (IDS) are crucial for identifying malicious traffic, yet traditional signature-based methods struggle with zero-day attacks and high false positive rates. AI-driven packet-capture analysis offers a promising alternative. However, existing approaches rely heavily on flow-based or statistical features, limiting their ability to detect fine-grained attack patterns. This study proposes Xavier-CMAE, an enhanced Convolutional Multi-Head Attention Ensemble (CMAE) model that improves detection accuracy while reducing computational overhead. By replacing Word2Vec embeddings with a Hex2Int tokenizer and Xavier initialization, Xavier-CMAE eliminates pre-training, accelerates training, and achieves 99.971% accuracy with a 0.018% false positive rate, outperforming Word2Vec-based methods. Additionally, we introduce LLM-CMAE, which integrates pre-trained Large Language Model (LLM) tokenizers into CMAE. While LLMs enhance feature extraction, their computational cost hinders real-time detection. LLM-CMAE balances efficiency and performance, reaching 99.969% accuracy with a 0.019% false positive rate. This work advances AI-powered IDS by (1) introducing a payload-based detection framework, (2) enhancing efficiency with Xavier-CMAE, and (3) integrating LLM tokenizers for improved real-time detection. 

**Abstract (ZH)**: 基于AI的包捕获分析在入侵检测系统中的应用：Xavier-CMAE和LLM-CMAE模型的研究 

---
# EXPLICATE: Enhancing Phishing Detection through Explainable AI and LLM-Powered Interpretability 

**Title (ZH)**: 增强钓鱼检测通过可解释AI和基于LLM的可解释性 

**Authors**: Bryan Lim, Roman Huerta, Alejandro Sotelo, Anthonie Quintela, Priyanka Kumar  

**Link**: [PDF](https://arxiv.org/pdf/2503.20796)  

**Abstract**: Sophisticated phishing attacks have emerged as a major cybersecurity threat, becoming more common and difficult to prevent. Though machine learning techniques have shown promise in detecting phishing attacks, they function mainly as "black boxes" without revealing their decision-making rationale. This lack of transparency erodes the trust of users and diminishes their effective threat response. We present EXPLICATE: a framework that enhances phishing detection through a three-component architecture: an ML-based classifier using domain-specific features, a dual-explanation layer combining LIME and SHAP for complementary feature-level insights, and an LLM enhancement using DeepSeek v3 to translate technical explanations into accessible natural language. Our experiments show that EXPLICATE attains 98.4 % accuracy on all metrics, which is on par with existing deep learning techniques but has better explainability. High-quality explanations are generated by the framework with an accuracy of 94.2 % as well as a consistency of 96.8\% between the LLM output and model prediction. We create EXPLICATE as a fully usable GUI application and a light Chrome extension, showing its applicability in many deployment situations. The research shows that high detection performance can go hand-in-hand with meaningful explainability in security applications. Most important, it addresses the critical divide between automated AI and user trust in phishing detection systems. 

**Abstract (ZH)**: 复杂的欺诈攻击已成为主要的网络安全威胁，变得越来越普遍且难以预防。尽管机器学习技术在检测欺诈攻击方面显示出潜力，但它们主要作为“黑盒”运作，不揭示其决策逻辑。这种透明度的缺乏侵蚀了用户的信任并减弱了他们有效的威胁响应。我们提出EXPLICATE：一种通过三组件架构增强欺诈检测的框架：基于领域特异性特征的机器学习分类器、结合LIME和SHAP的双解释层以提供互补的特征级洞见，以及使用DeepSeek v3增强语言模型以将技术解释转化为易懂的自然语言。我们的实验表明，EXPLICATE在所有指标上的准确率为98.4%，与现有的深度学习技术相当，但具有更好的可解释性。框架生成的高质量解释准确率为94.2%，模型预测与语言模型输出的一致性为96.8%。我们创建EXPLICATE为完整的图形用户界面应用程序和轻量级的Chrome扩展程序，显示其在多种部署情况下的适用性。该研究显示，在安全应用中，高性能检测可以与有意义的可解释性并存。最重要的是，它解决了自动AI和欺诈检测系统中用户信任之间的关键鸿沟。 

---
# Toward a Human-Centered AI-assisted Colonoscopy System in Australia 

**Title (ZH)**: 面向以人为本的AI辅助结肠镜检查系统研究（澳大利亚） 

**Authors**: Hsiang-Ting Chen, Yuan Zhang, Gustavo Carneiro, Rajvinder Singh  

**Link**: [PDF](https://arxiv.org/pdf/2503.20790)  

**Abstract**: While AI-assisted colonoscopy promises improved colorectal cancer screening, its success relies on effective integration into clinical practice, not just algorithmic accuracy. This paper, based on an Australian field study (observations and gastroenterologist interviews), highlights a critical disconnect: current development prioritizes machine learning model performance, overlooking essential aspects of user interface design, workflow integration, and overall user experience. Industry interactions reveal a similar emphasis on data and algorithms. To realize AI's full potential, the HCI community must champion user-centered design, ensuring these systems are usable, support endoscopist expertise, and enhance patient outcomes. 

**Abstract (ZH)**: 尽管人工智能辅助结肠镜检查有望改善结直肠癌筛查，其成功依赖于在临床实践中的有效整合，而不仅仅是算法的准确性。基于一项澳大利亚实地研究（观察和胃肠病学家访谈），本论文揭示了当前开发工作中一个关键的断裂：现有工作优先关注机器学习模型性能，而忽视了用户界面设计、工作流程整合和整体用户体验等关键方面。工业界的互动也显示出对数据和算法的类似重视。为了充分发挥人工智能的潜力，人机交互社区必须倡导以用户为中心的设计，确保这些系统易于使用、支持内镜医生的专业技能，并提高患者结果。 

---
