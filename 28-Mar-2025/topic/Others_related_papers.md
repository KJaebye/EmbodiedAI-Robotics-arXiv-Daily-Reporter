# Output-Feedback Boundary Control of Thermally and Flow-Induced Vibrations in Slender Timoshenko Beams 

**Title (ZH)**: 基于输出反馈的 slender 瑞芝诺梁由于热和流诱导振动的边界控制 

**Authors**: Chengyi Wang, Ji Wang  

**Link**: [PDF](https://arxiv.org/pdf/2503.21281)  

**Abstract**: This work is motivated by the engineering challenge of suppressing vibrations in turbine blades of aero engines, which often operate under extreme thermal conditions and high-Mach aerodynamic environments that give rise to complex vibration phenomena, commonly referred to as thermally-induced and flow-induced vibrations. Using Hamilton's variational principle, the system is modeled as a rotating slender Timoshenko beam under thermal and aerodynamic loads, described by a mixed hyperbolic-parabolic PDE system where instabilities occur both within the PDE domain and at the uncontrolled boundary, and the two types of PDEs are cascaded in the domain. For such a system, we present the state-feedback control design based on the PDE backstepping method. Recognizing that the distributed temperature gradients and structural vibrations in the Timoshenko beam are typically unmeasurable in practice, we design a state observer for the mixed hyperbolic-parabolic PDE system. Based on this observer, an output-feedback controller is then built to regulate the overall system using only available boundary measurements. In the closed-loop system, the state of the uncontrolled boundary, i.e., the furthest state from the control input, is proved to be exponentially convergent to zero, and all signals are proved as uniformly ultimately bounded. The proposed control design is validated on an aero-engine flexible blade under extreme thermal and aerodynamic conditions. 

**Abstract (ZH)**: 基于涡轮发动机热力和气动环境下的叶片振动抑制的PDE反馈控制设计 

---
# Fuzzy-Logic-based model predictive control: A paradigm integrating optimal and common-sense decision making 

**Title (ZH)**: 基于模糊逻辑的模型预测控制：一种结合最优与常识决策的范式 

**Authors**: Filip Surma, Anahita Jamshidnejad  

**Link**: [PDF](https://arxiv.org/pdf/2503.21065)  

**Abstract**: This paper introduces a novel concept, fuzzy-logic-based model predictive control (FLMPC), along with a multi-robot control approach for exploring unknown environments and locating targets. Traditional model predictive control (MPC) methods rely on Bayesian theory to represent environmental knowledge and optimize a stochastic cost function, often leading to high computational costs and lack of effectiveness in locating all the targets. Our approach instead leverages FLMPC and extends it to a bi-level parent-child architecture for enhanced coordination and extended decision making horizon. Extracting high-level information from probability distributions and local observations, FLMPC simplifies the optimization problem and significantly extends its operational horizon compared to other MPC methods. We conducted extensive simulations in unknown 2-dimensional environments with randomly placed obstacles and humans. We compared the performance and computation time of FLMPC against MPC with a stochastic cost function, then evaluated the impact of integrating the high-level parent FLMPC layer. The results indicate that our approaches significantly improve both performance and computation time, enhancing coordination of robots and reducing the impact of uncertainty in large-scale search and rescue environments. 

**Abstract (ZH)**: 基于模糊逻辑的模型预测控制多机器人未知环境探索与目标定位方法 

---
# In vitro 2 In vivo : Bidirectional and High-Precision Generation of In Vitro and In Vivo Neuronal Spike Data 

**Title (ZH)**: 体外与体内：双向高精度生成体外与体内神经元放电数据 

**Authors**: Masanori Shimono  

**Link**: [PDF](https://arxiv.org/pdf/2503.20841)  

**Abstract**: Neurons encode information in a binary manner and process complex signals. However, predicting or generating diverse neural activity patterns remains challenging. In vitro and in vivo studies provide distinct advantages, yet no robust computational framework seamlessly integrates both data types. We address this by applying the Transformer model, widely used in large-scale language models, to neural data. To handle binary data, we introduced Dice loss, enabling accurate cross-domain neural activity generation. Structural analysis revealed how Dice loss enhances learning and identified key brain regions facilitating high-precision data generation. Our findings support the 3Rs principle in animal research, particularly Replacement, and establish a mathematical framework bridging animal experiments and human clinical studies. This work advances data-driven neuroscience and neural activity modeling, paving the way for more ethical and effective experimental methodologies. 

**Abstract (ZH)**: 神经元以二进制方式编码信息并处理复杂信号。然而，预测或生成多样的神经活动模式仍然具有挑战性。体外和体内研究各有优势，但尚未有强大的计算框架能够无缝整合这两种数据类型。我们通过将广泛应用于大规模语言模型的Transformer模型应用于神经数据来解决这一问题。为了处理二进制数据，我们引入了Dice损失，使其能够准确地生成跨域神经活动。结构分析揭示了Dice损失如何增强学习，并确定了有助于高精度数据生成的关键脑区。我们的发现支持动物实验中的3Rs原则，特别是在替代方面，并建立了连接动物实验和人类临床研究的数学框架。这项工作推进了数据驱动的神经科学和神经活动建模，为更伦理和有效的实验方法奠定了基础。 

---
# Unlocking the Potential of Past Research: Using Generative AI to Reconstruct Healthcare Simulation Models 

**Title (ZH)**: 解锁过往研究的潜力：利用生成式AI重构医疗 simulation 模型 

**Authors**: Thomas Monks, Alison Harper, Amy Heather  

**Link**: [PDF](https://arxiv.org/pdf/2503.21646)  

**Abstract**: Discrete-event simulation (DES) is widely used in healthcare Operations Research, but the models themselves are rarely shared. This limits their potential for reuse and long-term impact in the modelling and healthcare communities. This study explores the feasibility of using generative artificial intelligence (AI) to recreate published models using Free and Open Source Software (FOSS), based on the descriptions provided in an academic journal. Using a structured methodology, we successfully generated, tested and internally reproduced two DES models, including user interfaces. The reported results were replicated for one model, but not the other, likely due to missing information on distributions. These models are substantially more complex than AI-generated DES models published to date. Given the challenges we faced in prompt engineering, code generation, and model testing, we conclude that our iterative approach to model development, systematic comparison and testing, and the expertise of our team were necessary to the success of our recreated simulation models. 

**Abstract (ZH)**: 基于学术期刊描述使用生成式人工智能重建开源软件自由离散事件模拟模型的研究 

---
# GenEdit: Compounding Operators and Continuous Improvement to Tackle Text-to-SQL in the Enterprise 

**Title (ZH)**: GenEdit: 累积运算符与持续优化以应对企业中的文本到SQL问题 

**Authors**: Karime Maamari, Connor Landy, Amine Mhedhbi  

**Link**: [PDF](https://arxiv.org/pdf/2503.21602)  

**Abstract**: Recent advancements in Text-to-SQL, driven by large language models, are democratizing data access. Despite these advancements, enterprise deployments remain challenging due to the need to capture business-specific knowledge, handle complex queries, and meet expectations of continuous improvements. To address these issues, we designed and implemented GenEdit: our Text-to-SQL generation system that improves with user feedback. GenEdit builds and maintains a company-specific knowledge set, employs a pipeline of operators decomposing SQL generation, and uses feedback to update its knowledge set to improve future SQL generations.
We describe GenEdit's architecture made of two core modules: (i) decomposed SQL generation; and (ii) knowledge set edits based on user feedback. For generation, GenEdit leverages compounding operators to improve knowledge retrieval and to create a plan as chain-of-thought steps that guides generation. GenEdit first retrieves relevant examples in an initial retrieval stage where original SQL queries are decomposed into sub-statements, clauses or sub-queries. It then also retrieves instructions and schema elements. Using the retrieved contextual information, GenEdit then generates step-by-step plan in natural language on how to produce the query. Finally, GenEdit uses the plan to generate SQL, minimizing the need for model reasoning, which enhances complex SQL generation. If necessary, GenEdit regenerates the query based on syntactic and semantic errors. The knowledge set edits are recommended through an interactive copilot, allowing users to iterate on their feedback and to regenerate SQL queries as needed. Each generation uses staged edits which update the generation prompt. Once the feedback is submitted, it gets merged after passing regression testing and obtaining an approval, improving future generations. 

**Abstract (ZH)**: 近期大型语言模型驱动的Text-to-SQL进展正 democratizing数据访问。尽管取得了这些进展，企业部署仍面临挑战，因为需要捕获特定业务知识、处理复杂查询并满足持续改进的期望。为了解决这些问题，我们设计并实现了一种名为GenEdit的Text-to-SQL生成系统，该系统能够通过用户反馈进行改进。GenEdit构建并维护一个特定企业的知识库，采用操作流水线分解SQL生成过程，并利用反馈来更新知识库以改善未来的SQL生成。 

---
# The Procedural Content Generation Benchmark: An Open-source Testbed for Generative Challenges in Games 

**Title (ZH)**: 程序内容生成基准：一个用于游戏生成挑战的开源测试平台 

**Authors**: Ahmed Khalifa, Roberto Gallotta, Matthew Barthet, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis  

**Link**: [PDF](https://arxiv.org/pdf/2503.21474)  

**Abstract**: This paper introduces the Procedural Content Generation Benchmark for evaluating generative algorithms on different game content creation tasks. The benchmark comes with 12 game-related problems with multiple variants on each problem. Problems vary from creating levels of different kinds to creating rule sets for simple arcade games. Each problem has its own content representation, control parameters, and evaluation metrics for quality, diversity, and controllability. This benchmark is intended as a first step towards a standardized way of comparing generative algorithms. We use the benchmark to score three baseline algorithms: a random generator, an evolution strategy, and a genetic algorithm. Results show that some problems are easier to solve than others, as well as the impact the chosen objective has on quality, diversity, and controllability of the generated artifacts. 

**Abstract (ZH)**: 面向游戏内容生成的程序化内容生成基准评估生成算法在不同类型的游戏内容创作任务中的性能 

---
# Neuroplasticity in Artificial Intelligence -- An Overview and Inspirations on Drop In \& Out Learning 

**Title (ZH)**: 人工智能中的神经可塑性——概览及Drop In & Out学习的启示 

**Authors**: Yupei Li, Manuel Milling, Björn W. Schuller  

**Link**: [PDF](https://arxiv.org/pdf/2503.21419)  

**Abstract**: Artificial Intelligence (AI) has achieved new levels of performance and spread in public usage with the rise of deep neural networks (DNNs). Initially inspired by human neurons and their connections, NNs have become the foundation of AI models for many advanced architectures. However, some of the most integral processes in the human brain, particularly neurogenesis and neuroplasticity in addition to the more spread neuroapoptosis have largely been ignored in DNN architecture design. Instead, contemporary AI development predominantly focuses on constructing advanced frameworks, such as large language models, which retain a static structure of neural connections during training and inference. In this light, we explore how neurogenesis, neuroapoptosis, and neuroplasticity can inspire future AI advances. Specifically, we examine analogous activities in artificial NNs, introducing the concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and structural pruning for neuroapoptosis. We additionally suggest neuroplasticity combining the two for future large NNs in ``life-long learning'' settings following the biological inspiration. We conclude by advocating for greater research efforts in this interdisciplinary domain and identifying promising directions for future exploration. 

**Abstract (ZH)**: 人工智能（AI）在深度神经网络（DNNs）的兴起下实现了新的性能水平并在公共使用中得到普及。尽管最初受到人类神经元及其连接的启发，神经网络已成为许多先进架构中AI模型的基础。然而，在DNN架构设计中，人类大脑的一些最核心过程，特别是神经发生、神经凋亡以及更为普及的神经可塑性，大多被忽视。相反，现代AI开发主要集中在构建复杂的框架，如大规模语言模型，这些框架在训练和推理期间保留了神经连接的静态结构。因此，我们探讨了神经发生、神经凋亡和神经可塑性如何启发未来的AI发展。具体来说，我们研究了人工神经网络中的相应活动，引入了“dropin”这一概念以类比神经发生，并重新审视了“dropout”和结构剪枝以类比神经凋亡。此外，我们建议神经可塑性将前两者结合起来，用于“终身学习”环境中未来大规模神经网络的设计，这是受到生物启发的。最后，我们呼吁在这一跨学科领域进行更多的研究努力，并确定未来探索的有希望的方向。 

---
# HybridoNet-Adapt: A Domain-Adapted Framework for Accurate Lithium-Ion Battery RUL Prediction 

**Title (ZH)**: HybridoNet-Adapt：一种用于锂离子电池剩余使用寿命预测的领域自适应框架 

**Authors**: Khoa Tran, Bao Huynh, Tri Le, Lam Pham, Vy-Rin Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2503.21392)  

**Abstract**: Accurate prediction of the remaining useful life (RUL) in Lithium-ion battery (LIB) health management systems is crucial for ensuring reliability and safety. Current methods typically assume that training and testing data share the same distribution, overlooking the benefits of incorporating diverse data sources to enhance model performance. To address this limitation, we introduce a data-independent RUL prediction framework along with its domain adaptation (DA) approach, which leverages heterogeneous data sources for improved target predictions. Our approach integrates comprehensive data preprocessing, including feature extraction, denoising, and normalization, with a data-independent prediction model that combines Long Short-Term Memory (LSTM), Multihead Attention, and a Neural Ordinary Differential Equation (NODE) block, termed HybridoNet. The domain-adapted version, HybridoNet Adapt, is trained using a novel technique inspired by the Domain-Adversarial Neural Network (DANN) framework, a regression ensemble method, and Maximum Mean Discrepancy (MMD) to learn domain-invariant features from labeled cycling data in the source and target domains. Experimental results demonstrate that our approach outperforms state-of-the-art techniques, providing reliable RUL predictions for real-world applications. 

**Abstract (ZH)**: 锂离子电池（LIB）健康管理系统中剩余使用寿命（RUL）的准确预测对于确保可靠性和安全性至关重要。为了克服现有方法忽视利用多样化数据源提高模型性能的局限性，我们提出了一种数据无关的RUL预测框架及其域适应（DA）方法，该方法利用异构数据源以提升目标预测性能。我们的方法集成了全面的数据预处理，包括特征提取、去噪和归一化，并结合了长短期记忆网络（LSTM）、多头注意力机制和神经常微分方程（NODE）块，称为HybridoNet。域适应版本HybridoNet Adapt通过结合域对抗神经网络（DANN）框架、回归集成方法和最大均值偏差（MMD）训练，从源域和目标域的标记充放电数据中学习域不变特征。实验结果表明，我们的方法优于现有技术，能够为实际应用提供可靠的RUL预测。 

---
# HyperGraphRAG: Retrieval-Augmented Generation with Hypergraph-Structured Knowledge Representation 

**Title (ZH)**: HyperGraphRAG：基于超图结构知识表示的检索增强生成 

**Authors**: Haoran Luo, Haihong E, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo, Qika Lin, Yu Feng, Zemin Kuang, Meina Song, Yifan Zhu, Luu Anh Tuan  

**Link**: [PDF](https://arxiv.org/pdf/2503.21322)  

**Abstract**: While standard Retrieval-Augmented Generation (RAG) based on chunks, GraphRAG structures knowledge as graphs to leverage the relations among entities. However, previous GraphRAG methods are limited by binary relations: one edge in the graph only connects two entities, which cannot well model the n-ary relations among more than two entities that widely exist in reality. To address this limitation, we propose HyperGraphRAG, a novel hypergraph-based RAG method that represents n-ary relational facts via hyperedges, modeling the complicated n-ary relations in the real world. To retrieve and generate over hypergraphs, we introduce a complete pipeline with a hypergraph construction method, a hypergraph retrieval strategy, and a hypergraph-guided generation mechanism. Experiments across medicine, agriculture, computer science, and law demonstrate that HyperGraphRAG outperforms standard RAG and GraphRAG in accuracy and generation quality. 

**Abstract (ZH)**: 基于超图的HyperGraphRAG：一种表示复杂n元关系的检索增强生成方法 

---
# Integrating Large Language Models For Monte Carlo Simulation of Chemical Reaction Networks 

**Title (ZH)**: 将大型语言模型集成用于化学反应网络的蒙特卡ロ模拟 

**Authors**: Sadikshya Gyawali, Ashwini Mandal, Manish Dahal, Manish Awale, Sanjay Rijal, Shital Adhikari, Vaghawan Ojha  

**Link**: [PDF](https://arxiv.org/pdf/2503.21178)  

**Abstract**: Chemical reaction network is an important method for modeling and exploring complex biological processes, bio-chemical interactions and the behavior of different dynamics in system biology. But, formulating such reaction kinetics takes considerable time. In this paper, we leverage the efficiency of modern large language models to automate the stochastic monte carlo simulation of chemical reaction networks and enable the simulation through the reaction description provided in the form of natural languages. We also integrate this process into widely used simulation tool Copasi to further give the edge and ease to the modelers and researchers. In this work, we show the efficacy and limitations of the modern large language models to parse and create reaction kinetics for modelling complex chemical reaction processes. 

**Abstract (ZH)**: 化学反应网络是建模和探索复杂生物过程、生物化学相互作用及系统生物学中不同动力学行为的重要方法。但是，制定这样的反应动力学需要耗费大量时间。本文利用现代大型语言模型的效率自动化化学反应网络的随机蒙特卡洛模拟，并通过自然语言提供的反应描述实现模拟。我们还将此过程集成到广泛使用的模拟工具Copasi中，进一步为建模者和研究人员提供便利。本文展示了现代大型语言模型在解析和生成用于建模复杂化学反应过程的反应动力学方面的有效性和局限性。 

---
# A computational theory of evaluation for parameterisable subject 

**Title (ZH)**: 可参数化的主题评估计算理论 

**Authors**: Hedong Yan  

**Link**: [PDF](https://arxiv.org/pdf/2503.21138)  

**Abstract**: Evaluation is critical to advance decision making across domains, yet existing methodologies often struggle to balance theoretical rigor and practical scalability. In order to reduce the cost of experimental evaluation, we introduce a computational theory of evaluation for parameterisable subjects. We prove upper bounds of generalized evaluation error and generalized causal effect error of evaluation metric on subject. We also prove efficiency, and consistency to estimated causal effect of subject on metric by prediction. To optimize evaluation models, we propose a meta-learner to handle heterogeneous evaluation subjects space. Comparing with other computational approaches, our (conditional) evaluation model reduced 24.1%-99.0% evaluation errors across 12 scenes, including individual medicine, scientific simulation, business activities, and quantum trade. The evaluation time is reduced 3-7 order of magnitude comparing with experiments or simulations. 

**Abstract (ZH)**: 评价对于跨领域决策制定至关重要，但现有方法往往难以在理论严谨性和实际可扩展性之间取得平衡。为了降低实验评价成本，我们引入了一种参数可调主体的计算评价理论。我们证明了评价指标在主体上的广义评价误差和广义因果效应误差的上界。我们还证明了评价模型可以通过预测一致性和效率估计主体的因果效应。为了优化评价模型，我们提出了一种元学习器来处理异质评价主体空间。与其它计算方法相比，我们的（条件）评价模型在12个场景中（包括个体医学、科学模拟、商业活动和量子交易）降低了24.1%-99.0%的评价误差，并将评价时间减少了3到7个数量级。 

---
# AskSport: Web Application for Sports Question-Answering 

**Title (ZH)**: AskSport: 体育问答Web应用 

**Authors**: Enzo B Onofre, Leonardo M P Moraes, Cristina D Aguiar  

**Link**: [PDF](https://arxiv.org/pdf/2503.21067)  

**Abstract**: This paper introduces AskSport, a question-answering web application about sports. It allows users to ask questions using natural language and retrieve the three most relevant answers, including related information and documents. The paper describes the characteristics and functionalities of the application, including use cases demonstrating its ability to return names and numerical values. AskSport and its implementation are available for public access on HuggingFace. 

**Abstract (ZH)**: 本文介绍了一款关于体育领域的问答网络应用AskSport。用户可以使用自然语言提出问题，并获取与问题最相关的三个答案，包括相关的信息和文档。文章描述了该应用的特性与功能，包括展示了其返回姓名和数值的能力。AskSport及其实现已在HuggingFace上对公众开放。 

---
# The Art of Tool Interface Design 

**Title (ZH)**: 工具接口设计的艺术 

**Authors**: Yunnan Wu, Paul Chen, Deshank Baranwal, Jinlong Zhou, Jian Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2503.21036)  

**Abstract**: We present an agentic framework, Thinker, which achieves state of art performance in challenging reasoning tasks for realistic customer service scenarios that involve complex business logic and human interactions via long horizons. On the $\tau$-bench retail dataset, Thinker achieves 82.6\% success rate with GPT-4o (version 2024-06-01) (baseline: 68.3\%), and 81.9\% success rate with Llama-3.1 405B (baseline: 49.6\%), without any fine-tuning. Thinker effectively closes the gap in reasoning capabilities between the base models by introducing proper structure.
The key features of the Thinker framework are: (1) State-Machine Augmented Generation (SMAG), which represents business logic as state machines and the LLM uses state machines as tools. (2) Delegation of tasks from the main reasoning loop to LLM-powered tools. (3) Adaptive context management.
Our prompting-only solution achieves signficant gains, while still maintaining a standard agentic architecture with a ReAct style reasoning loop. The key is to innovate on the tool interface design, as exemplified by SMAG and the LLM-powered tools. 

**Abstract (ZH)**: 一种实现卓越推理性能的代理框架：Thinker及其在现实客户服务场景中的应用 

---
# Elementwise Layer Normalization 

**Title (ZH)**: 元素wise层规范化 

**Authors**: Felix Stollenwerk  

**Link**: [PDF](https://arxiv.org/pdf/2503.21708)  

**Abstract**: A recent paper proposed Dynamic Tanh (DyT) as a drop-in replacement for Layer Normalization. Although the method is empirically well-motivated and appealing from a practical point of view, it lacks a theoretical foundation. In this work, we derive DyT mathematically and show that a well-defined approximation is needed to do so. By dropping said approximation, an alternative element-wise transformation is obtained, which we call Elementwise Layer Normalization (ELN). We demonstrate that ELN resembles Layer Normalization more accurately than DyT does. 

**Abstract (ZH)**: 最近一篇论文提出了Dynamic Tanh (DyT)作为一种层规范化层的即插即用替代方案。尽管该方法从实践的角度来看具有经验上的合理性和吸引力，但缺乏理论基础。在本工作中，我们从数学上推导出DyT，并指出需要一个明确定义的近似来实现这一点。通过舍去该近似，我们获得了一种元素-wise变换，我们称之为元素层规范化（ELN）。我们证明ELN比DyT更能准确地类似于层规范化。 

---
# AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation 

**Title (ZH)**: AMA-SAM：针对高保真组织学细胞核分割的对抗多领域对齐Segment Anything Model 

**Authors**: Jiahe Qian, Yaoyu Fang, Jinkui Hao, Bo Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2503.21695)  

**Abstract**: Accurate segmentation of cell nuclei in histopathology images is essential for numerous biomedical research and clinical applications. However, existing cell nucleus segmentation methods only consider a single dataset (i.e., primary domain), while neglecting to leverage supplementary data from diverse sources (i.e., auxiliary domains) to reduce overfitting and enhance the performance. Although incorporating multiple datasets could alleviate overfitting, it often exacerbates performance drops caused by domain shifts. In this work, we introduce Adversarial Multi-domain Alignment of Segment Anything Model (AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these obstacles through two key innovations. First, we propose a Conditional Gradient Reversal Layer (CGRL), a multi-domain alignment module that harmonizes features from diverse domains to promote domain-invariant representation learning while preserving crucial discriminative features for the primary dataset. Second, we address SAM's inherent low-resolution output by designing a High-Resolution Decoder (HR-Decoder), which directly produces fine-grained segmentation maps in order to capture intricate nuclei boundaries in high-resolution histology images. To the best of our knowledge, this is the first attempt to adapt SAM for multi-dataset learning with application to histology nuclei segmentation. We validate our method on several publicly available datasets, demonstrating consistent and significant improvements over state-of-the-art approaches. 

**Abstract (ZH)**: 基于 adversarial 多域对齐的 segment anything 模型在组织病理学细胞核分割中的应用 

---
# COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing 

**Title (ZH)**: COMI-LINGUA: 印地语-英语代码混合多任务NLP专家标注大规模数据集 

**Authors**: Rajvee Sheth, Himanshu Beniwal, Mayank Singh  

**Link**: [PDF](https://arxiv.org/pdf/2503.21670)  

**Abstract**: The rapid growth of digital communication has driven the widespread use of code-mixing, particularly Hindi-English, in multilingual communities. Existing datasets often focus on romanized text, have limited scope, or rely on synthetic data, which fails to capture realworld language nuances. Human annotations are crucial for assessing the naturalness and acceptability of code-mixed text. To address these challenges, We introduce COMI-LINGUA, the largest manually annotated dataset for code-mixed text, comprising 100,970 instances evaluated by three expert annotators in both Devanagari and Roman scripts. The dataset supports five fundamental NLP tasks: Language Identification, Matrix Language Identification, Part-of-Speech Tagging, Named Entity Recognition, and Translation. We evaluate LLMs on these tasks using COMILINGUA, revealing limitations in current multilingual modeling strategies and emphasizing the need for improved code-mixed text processing capabilities. COMI-LINGUA is publically availabe at: this https URL. 

**Abstract (ZH)**: 数字通信的迅速增长推动了多语言社区中代码混用，特别是印地英语代码混用的普遍使用。现有数据集往往专注于罗马化文本，范围有限，或依赖合成数据，无法捕捉到现实世界的语言细微差别。人工标注对于评估代码混用文本的自然度和接受度至关重要。为应对这些挑战，我们引入了COMI-LINGUA，这是最大的人工标注代码混用文本数据集，包含100,970个实例，由三位专家注释者在德文那格里和罗马-script中进行评估。该数据集支持五项基本的NLP任务：语言识别、矩阵语言识别、词性标注、命名实体识别和翻译。我们使用COMILINGUA评估LLMs在这些任务上的表现，揭示了当前多语言建模策略的局限性，并强调了改进代码混用文本处理能力的必要性。COMI-LINGUA可在以下网址获取：this https URL。 

---
# When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in Morocco 

**Title (ZH)**: 当天文遇到AI：曼扎尔在摩洛哥新月可见性预测中的应用 

**Authors**: Yassir Lairgi  

**Link**: [PDF](https://arxiv.org/pdf/2503.21634)  

**Abstract**: The accurate determination of the beginning of each Hijri month is essential for religious, cultural, and administrative purposes. Manazel (The code and datasets are available at this https URL) addresses this challenge in Morocco by leveraging 13 years of crescent visibility data to refine the ODEH criterion, a widely used standard for lunar crescent visibility prediction. The study integrates two key features, the Arc of Vision (ARCV) and the total width of the crescent (W), to enhance the accuracy of lunar visibility assessments. A machine learning approach utilizing the Logistic Regression algorithm is employed to classify crescent visibility conditions, achieving a predictive accuracy of 98.83%. This data-driven methodology offers a robust and reliable framework for determining the start of the Hijri month, comparing different data classification tools, and improving the consistency of lunar calendar calculations in Morocco. The findings demonstrate the effectiveness of machine learning in astronomical applications and highlight the potential for further enhancements in the modeling of crescent visibility. 

**Abstract (ZH)**: Hijri月开始日期的精确确定对于宗教、文化和行政目的至关重要。Manazel（代码和数据集可在以下链接获取：这个httpsURL）通过利用13年的月相可见性数据来改进ODEH标准，解决摩洛哥月相可见性预测中的挑战。该研究结合了视界弧（ARCV）和月牙总宽度（W）两个关键特征，以提高月相可见性评估的准确性。采用逻辑回归算法的机器学习方法被用于分类月相可见性条件，预测准确率达到98.83%。这种数据驱动的方法为确定Hijri月的开始日期、比较不同的数据分类工具以及改进摩洛哥月历计算的一致性提供了坚实可靠的框架。研究结果展示了机器学习在天文学应用中的有效性，并指出了进一步增强月相可见性建模的潜力。 

---
# A Measure Based Generalizable Approach to Understandability 

**Title (ZH)**: 基于度量的一般化可理解性方法 

**Authors**: Vikas Kushwaha, Sruti Srinivasa Ragavan, Subhajit Roy  

**Link**: [PDF](https://arxiv.org/pdf/2503.21615)  

**Abstract**: Successful agent-human partnerships require that any agent generated information is understandable to the human, and that the human can easily steer the agent towards a goal. Such effective communication requires the agent to develop a finer-level notion of what is understandable to the human. State-of-the-art agents, including LLMs, lack this detailed notion of understandability because they only capture average human sensibilities from the training data, and therefore afford limited steerability (e.g., requiring non-trivial prompt engineering).
In this paper, instead of only relying on data, we argue for developing generalizable, domain-agnostic measures of understandability that can be used as directives for these agents. Existing research on understandability measures is fragmented, we survey various such efforts across domains, and lay a cognitive-science-rooted groundwork for more coherent and domain-agnostic research investigations in future. 

**Abstract (ZH)**: 成功的智能体-人类合作伙伴关系要求智能体生成的信息对于人类是可以理解的，人类可以轻松地引导智能体达成目标。这种有效的沟通需要智能体发展出对于人类可理解性的更精细的理解。当前最先进的智能体，包括大型语言模型，缺乏这种详细的可理解性概念，因为它们仅从训练数据中捕获平均水平的人类感受，并因此提供了有限的可控性（例如，需要复杂的提示工程）。本文中，我们不只依赖数据，而是主张开发通用且领域无关的可理解性度量，作为这些智能体的指令。现有的可理解性度量研究碎片化，我们对跨领域的相关努力进行了综述，并为未来更连贯和领域无关的研究奠定了认知科学基础。 

---
# Critical Iterative Denoising: A Discrete Generative Model Applied to Graphs 

**Title (ZH)**: 关键迭代去噪：应用于图的离散生成模型 

**Authors**: Yoann Boget, Alexandros Kalousis  

**Link**: [PDF](https://arxiv.org/pdf/2503.21592)  

**Abstract**: Discrete Diffusion and Flow Matching models have significantly advanced generative modeling for discrete structures, including graphs. However, the time dependencies in the noising process of these models lead to error accumulation and propagation during the backward process. This issue, particularly pronounced in mask diffusion, is a known limitation in sequence modeling and, as we demonstrate, also impacts discrete diffusion models for graphs.
To address this problem, we propose a novel framework called Iterative Denoising, which simplifies discrete diffusion and circumvents the issue by assuming conditional independence across time. Additionally, we enhance our model by incorporating a Critic, which during generation selectively retains or corrupts elements in an instance based on their likelihood under the data distribution. Our empirical evaluations demonstrate that the proposed method significantly outperforms existing discrete diffusion baselines in graph generation tasks. 

**Abstract (ZH)**: 离散去噪和流匹配模型显著推进了离散结构（包括图）的生成建模。然而，这些模型的去噪过程中的时间依赖性导致了错误在反向过程中积累和传播。这一问题在掩码扩散中尤为明显，并且如我们所证明的，也影响着图的离散扩散模型。为解决这一问题，我们提出了一种新的框架——迭代去噪，通过假设时间上的条件独立性简化了离散扩散过程，并绕过了该问题。此外，我们通过引入一个裁判模块增强了模型，在生成过程中根据实例在数据分布下的似然性选择性地保留或篡改元素。我们的实证评价表明，所提出的方法在图生成任务中显著优于现有离散扩散基线方法。 

---
# Magnitude-Phase Dual-Path Speech Enhancement Network based on Self-Supervised Embedding and Perceptual Contrast Stretch Boosting 

**Title (ZH)**: 基于自监督嵌入和感知对比拉伸增强的幅度-相位双路径语音增强网络 

**Authors**: Alimjan Mattursun, Liejun Wang, Yinfeng Yu, Chunyang Ma  

**Link**: [PDF](https://arxiv.org/pdf/2503.21571)  

**Abstract**: Speech self-supervised learning (SSL) has made great progress in various speech processing tasks, but there is still room for improvement in speech enhancement (SE). This paper presents BSP-MPNet, a dual-path framework that combines self-supervised features with magnitude-phase information for SE. The approach starts by applying the perceptual contrast stretching (PCS) algorithm to enhance the magnitude-phase spectrum. A magnitude-phase 2D coarse (MP-2DC) encoder then extracts coarse features from the enhanced spectrum. Next, a feature-separating self-supervised learning (FS-SSL) model generates self-supervised embeddings for the magnitude and phase components separately. These embeddings are fused to create cross-domain feature representations. Finally, two parallel RNN-enhanced multi-attention (REMA) mask decoders refine the features, apply them to the mask, and reconstruct the speech signal. We evaluate BSP-MPNet on the VoiceBank+DEMAND and WHAMR! datasets. Experimental results show that BSP-MPNet outperforms existing methods under various noise conditions, providing new directions for self-supervised speech enhancement research. The implementation of the BSP-MPNet code is available online\footnote[2]{this https URL. \label{s1}} 

**Abstract (ZH)**: 基于双路径框架的自监督语音增强（BSP-MPNet） 

---
# A Local Perspective-based Model for Overlapping Community Detection 

**Title (ZH)**: 基于局部视角的重叠社区检测模型 

**Authors**: Gaofeng Zhou, Rui-Feng Wang, Kangning Cui  

**Link**: [PDF](https://arxiv.org/pdf/2503.21558)  

**Abstract**: Community detection, which identifies densely connected node clusters with sparse between-group links, is vital for analyzing network structure and function in real-world systems. Most existing community detection methods based on GCNs primarily focus on node-level information while overlooking community-level features, leading to performance limitations on large-scale networks. To address this issue, we propose LQ-GCN, an overlapping community detection model from a local community perspective. LQ-GCN employs a Bernoulli-Poisson model to construct a community affiliation matrix and form an end-to-end detection framework. By adopting local modularity as the objective function, the model incorporates local community information to enhance the quality and accuracy of clustering results. Additionally, the conventional GCNs architecture is optimized to improve the model capability in identifying overlapping communities in large-scale networks. Experimental results demonstrate that LQ-GCN achieves up to a 33% improvement in Normalized Mutual Information (NMI) and a 26.3% improvement in Recall compared to baseline models across multiple real-world benchmark datasets. 

**Abstract (ZH)**: 基于局部社区视角的LQ-GCN重叠社区检测模型 

---
# LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing 

**Title (ZH)**: LOCATEdit: 基于图拉普拉斯优化跨注意力的局部化文本指导图像编辑 

**Authors**: Achint Soni, Meet Soni, Sirisha Rambhatla  

**Link**: [PDF](https://arxiv.org/pdf/2503.21541)  

**Abstract**: Text-guided image editing aims to modify specific regions of an image according to natural language instructions while maintaining the general structure and the background fidelity. Existing methods utilize masks derived from cross-attention maps generated from diffusion models to identify the target regions for modification. However, since cross-attention mechanisms focus on semantic relevance, they struggle to maintain the image integrity. As a result, these methods often lack spatial consistency, leading to editing artifacts and distortions. In this work, we address these limitations and introduce LOCATEdit, which enhances cross-attention maps through a graph-based approach utilizing self-attention-derived patch relationships to maintain smooth, coherent attention across image regions, ensuring that alterations are limited to the designated items while retaining the surrounding structure. \method consistently and substantially outperforms existing baselines on PIE-Bench, demonstrating its state-of-the-art performance and effectiveness on various editing tasks. Code can be found on this https URL 

**Abstract (ZH)**: 基于文本引导的图像编辑旨在根据自然语言指令修改图像的特定区域，同时保持整体结构和背景的真实性。现有方法利用源自扩散模型交叉注意力图的掩码来识别修改目标区域，但由于交叉注意力机制侧重于语义相关性，因此难以保持图像完整性。结果，这些方法往往缺乏空间一致性，导致编辑伪影和失真。在本文中，我们解决了这些问题，并引入了LOCATEdit，它通过基于图的方法利用自注意力推导出的块关系来增强交叉注意力图，确保在图像区域之间保持平滑一致的注意力，从而将修改限定在指定项目上，同时保留周围结构。LOCATEdit在PIE-Bench上显著超越现有基线，展示了其在各种编辑任务中的先进性能和有效性。代码可在此处找到：this https URL。 

---
# Low-Resource Transliteration for Roman-Urdu and Urdu Using Transformer-Based Models 

**Title (ZH)**: 基于变换器模型的低资源罗马字母-乌尔都语和乌尔都语 transliteration研究 

**Authors**: Umer Butt, Stalin Veranasi, Günter Neumann  

**Link**: [PDF](https://arxiv.org/pdf/2503.21530)  

**Abstract**: As the Information Retrieval (IR) field increasingly recognizes the importance of inclusivity, addressing the needs of low-resource languages remains a significant challenge. Transliteration between Urdu and its Romanized form, Roman Urdu, remains underexplored despite the widespread use of both scripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl dataset showed promising results but suffered from poor domain adaptability and limited evaluation. We propose a transformer-based approach using the m2m100 multilingual translation model, enhanced with masked language modeling (MLM) pretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverse Dakshina dataset. To address previous evaluation flaws, we introduce rigorous dataset splits and assess performance using BLEU, character-level BLEU, and CHRF. Our model achieves strong transliteration performance, with Char-BLEU scores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. These results outperform both RNN baselines and GPT-4o Mini and demonstrate the effectiveness of multilingual transfer learning for low-resource transliteration tasks. 

**Abstract (ZH)**: 随着信息检索（IR）领域越来越认识到包容性的重要性，满足低资源语言的需求仍然是一项重大挑战。尽管印地语及其罗马化形式罗马印地语在南亚地区广泛使用，但两者之间的 transliteration 仍然未被充分研究。先前使用 RNN 在 Roman-Urdu-Parl 数据集上的工作取得了令人鼓舞的结果，但存在域适应性差和评估有限的问题。我们提出了一种基于 transformer 的方法，使用多语言翻译模型 m2m100，并结合了掩码语言模型 (MLM) 预训练和在 Roman-Urdu-Parl 数据集及领域多样化的 Dakshina 数据集上的微调。为了弥补先前评估中的缺陷，我们引入了严格的数据集划分，并使用 BLEU、字符级 BLEU 和 CHRF 进行性能评估。我们的模型在印地语→罗马印地语和罗马印地语→印地语方向上分别取得了 96.37 和 97.44 的 Char-BLEU 分数，这些结果优于 RNN 基线和 GPT-4o Mini，并证明了多语言转移学习在低资源 transliteration 任务中的有效性。 

---
# MONO2REST: Identifying and Exposing Microservices: a Reusable RESTification Approach 

**Title (ZH)**: MONO2REST：识别和暴露微服务：一种可重用的REST化方法 

**Authors**: Matthéo Lecrivain, Hanifa Barry, Dalila Tamzalit, Houari Sahraoui  

**Link**: [PDF](https://arxiv.org/pdf/2503.21522)  

**Abstract**: The microservices architectural style has become the de facto standard for large-scale cloud applications, offering numerous benefits in scalability, maintainability, and deployment flexibility. Many organizations are pursuing the migration of legacy monolithic systems to a microservices architecture. However, this process is challenging, risky, time-intensive, and prone-to-failure while several organizations lack necessary financial resources, time, or expertise to set up this migration process. So, rather than trying to migrate a legacy system where migration is risky or not feasible, we suggest exposing it as a microservice application without without having to migrate it. In this paper, we present a reusable, automated, two-phase approach that combines evolutionary algorithms with machine learning techniques. In the first phase, we identify microservices at the method level using a multi-objective genetic algorithm that considers both structural and semantic dependencies between methods. In the second phase, we generate REST APIs for each identified microservice using a classification algorithm to assign HTTP methods and endpoints. We evaluated our approach with a case study on the Spring PetClinic application, which has both monolithic and microservices implementations that serve as ground truth for comparison. Results demonstrate that our approach successfully aligns identified microservices with those in the reference microservices implementation, highlighting its effectiveness in service identification and API generation. 

**Abstract (ZH)**: 微服务架构风格已成为大规模云应用程序的事实标准，提供了在扩展性、可维护性和部署灵活性方面的诸多优势。许多组织正寻求将遗留的单体系统迁移到微服务架构。然而，这一过程具有挑战性、风险高、耗时且容易失败，而许多组织缺乏必要的资金、时间和专业知识来启动这一迁移过程。因此，我们建议在无需迁移的情况下，将遗留系统暴露为微服务应用。在本文中，我们提出了一种可重用的、自动化的两阶段方法，结合了进化算法与机器学习技术。在第一阶段，我们使用多目标遗传算法在方法级别识别微服务，同时考虑方法之间的结构性和语义依赖关系。在第二阶段，我们使用分类算法为每个识别出的微服务生成REST API，分配HTTP方法和端点。我们通过在Spring PetClinic应用上的案例研究评估了该方法的效果，该应用既有单体实现又有微服务实现，作为基准进行对比。结果表明，我们的方法成功地将识别出的微服务与参考微服务实施进行了对齐，突显了其在服务识别和API生成方面的有效性。 

---
# Quantitative Evaluation of Quantum/Classical Neural Network Using a Game Solver Metric 

**Title (ZH)**: 使用游戏解算器指标对量子/经典神经网络进行定量评估 

**Authors**: Suzukaze Kamei, Hideaki Kawaguchi, Shin Nishio, Tatakahiko Satoh  

**Link**: [PDF](https://arxiv.org/pdf/2503.21514)  

**Abstract**: To evaluate the performance of quantum computing systems relative to classical counterparts and explore the potential for quantum advantage, we propose a game-solving benchmark based on Elo ratings in the game of tic-tac-toe. We compare classical convolutional neural networks (CNNs), quantum convolutional neural networks (QCNNs), and hybrid classical-quantum models by assessing their performance against a random-move agent in automated matches. Additionally, we implement a QCNN integrated with quantum communication and evaluate its performance to quantify the overhead introduced by noisy quantum channels. Our results show that the classical-quantum hybrid model achieves Elo ratings comparable to those of classical CNNs, while the standalone QCNN underperforms under current hardware constraints. The communication overhead was found to be modest. These findings demonstrate the viability of using game-based benchmarks for evaluating quantum computing systems and suggest that quantum communication can be incorporated with limited impact on performance, providing a foundation for future hybrid quantum applications. 

**Abstract (ZH)**: 基于井字游戏的Elo评级 benchmarks评估量子计算系统的性能并探索量子优势 

---
# Adaptive Resampling with Bootstrap for Noisy Multi-Objective Optimization Problems 

**Title (ZH)**: 基于靴straps的自适应重采样在噪声多目标优化问题中的应用 

**Authors**: Timo Budszuhn, Mark Joachim Krallmann, Daniel Horn  

**Link**: [PDF](https://arxiv.org/pdf/2503.21495)  

**Abstract**: The challenge of noisy multi-objective optimization lies in the constant trade-off between exploring new decision points and improving the precision of known points through resampling. This decision should take into account both the variability of the objective functions and the current estimate of a point in relation to the Pareto front. Since the amount and distribution of noise are generally unknown, it is desirable for a decision function to be highly adaptive to the properties of the optimization problem. This paper presents a resampling decision function that incorporates the stochastic nature of the optimization problem by using bootstrapping and the probability of dominance. The distribution-free estimation of the probability of dominance is achieved using bootstrap estimates of the means. To make the procedure applicable even with very few observations, we transfer the distribution observed at other decision points. The efficiency of this resampling approach is demonstrated by applying it in the NSGA-II algorithm with a sequential resampling procedure under multiple noise variations. 

**Abstract (ZH)**: 噪 многокритериальной оптимизации: challenge住在探索新决策点与通过重采样提高已知点精度之间的不断权衡。这一决策应同时考虑目标函数的变异性以及所评估点相对于帕累托前沿的当前估计值。由于噪声的数量和分布通常未知，因此期望决策函数能够高度适应优化问题的性质。本文提出了一种通过使用 Bootstrapping 和支配概率结合重采样决策函数的方法。借助 Bootstrapping 对均值的估计实现支配概率的无分布估计。为了即使在观测数据很少的情况下也能使该过程适用，我们将其他决策点观察到的分布转移过来。通过将其应用于 NSGA-II 算法并采用基于多次噪声变化的顺序重采样程序，证明了该重采样方法的效率。 

---
# Unveiling Latent Information in Transaction Hashes: Hypergraph Learning for Ethereum Ponzi Scheme Detection 

**Title (ZH)**: 揭示交易哈希中的隐含信息：超图学习在以太坊庞氏骗局检测中的应用 

**Authors**: Junhao Wu, Yixin Yang, Chengxiang Jin, Silu Mu, Xiaolei Qian, Jiajun Zhou, Shanqing Yu, Qi Xuan  

**Link**: [PDF](https://arxiv.org/pdf/2503.21463)  

**Abstract**: With the widespread adoption of Ethereum, financial frauds such as Ponzi schemes have become increasingly rampant in the blockchain ecosystem, posing significant threats to the security of account assets. Existing Ethereum fraud detection methods typically model account transactions as graphs, but this approach primarily focuses on binary transactional relationships between accounts, failing to adequately capture the complex multi-party interaction patterns inherent in Ethereum. To address this, we propose a hypergraph modeling method for the Ponzi scheme detection method in Ethereum, called HyperDet. Specifically, we treat transaction hashes as hyperedges that connect all the relevant accounts involved in a transaction. Additionally, we design a two-step hypergraph sampling strategy to significantly reduce computational complexity. Furthermore, we introduce a dual-channel detection module, including the hypergraph detection channel and the hyper-homo graph detection channel, to be compatible with existing detection methods. Experimental results show that, compared to traditional homogeneous graph-based methods, the hyper-homo graph detection channel achieves significant performance improvements, demonstrating the superiority of hypergraph in Ponzi scheme detection. This research offers innovations for modeling complex relationships in blockchain data. 

**Abstract (ZH)**: 以太坊广泛采用后，区块链生态系统中的庞氏骗局等金融欺诈行为日益猖獗，对账户资产安全构成重大威胁。现有以太坊欺诈检测方法通常将账户交易建模为图，但这种方法主要关注账户之间的二元交易关系，未能充分捕捉到以太坊中固有的复杂多方交互模式。为解决这一问题，我们提出了一种用于以太坊庞氏骗局检测的超图建模方法，称为HyperDet。具体而言，我们将交易哈希视为连接交易中所有相关账户的超边。此外，我们设计了一种两步超图抽样策略，以显著降低计算复杂度。进一步地，我们引入了一种双重检测模块，包括超图检测通道和超同构图检测通道，以便与现有检测方法兼容。实验结果表明，与传统的同构图基方法相比，超同构图检测通道实现了显著的性能提升，这表明超图在庞氏骗局检测中的优越性。该研究为建模区块链数据中的复杂关系提供了创新方法。 

---
# Investigating the Duality of Interpretability and Explainability in Machine Learning 

**Title (ZH)**: 探究机器学习中可解释性和解释性的二重性 

**Authors**: Moncef Garouani, Josiane Mothe, Ayah Barhrhouj, Julien Aligon  

**Link**: [PDF](https://arxiv.org/pdf/2503.21356)  

**Abstract**: The rapid evolution of machine learning (ML) has led to the widespread adoption of complex "black box" models, such as deep neural networks and ensemble methods. These models exhibit exceptional predictive performance, making them invaluable for critical decision-making across diverse domains within society. However, their inherently opaque nature raises concerns about transparency and interpretability, making them untrustworthy decision support systems. To alleviate such a barrier to high-stakes adoption, research community focus has been on developing methods to explain black box models as a means to address the challenges they pose. Efforts are focused on explaining these models instead of developing ones that are inherently interpretable. Designing inherently interpretable models from the outset, however, can pave the path towards responsible and beneficial applications in the field of ML. In this position paper, we clarify the chasm between explaining black boxes and adopting inherently interpretable models. We emphasize the imperative need for model interpretability and, following the purpose of attaining better (i.e., more effective or efficient w.r.t. predictive performance) and trustworthy predictors, provide an experimental evaluation of latest hybrid learning methods that integrates symbolic knowledge into neural network predictors. We demonstrate how interpretable hybrid models could potentially supplant black box ones in different domains. 

**Abstract (ZH)**: 机器学习的快速发展导致了复杂“黑盒”模型的广泛采用，如深度神经网络和集成方法。这些模型表现出卓越的预测性能，使其在社会各个领域的关键决策中不可替代。然而，它们本质上的不透明性引发了透明度和可解释性方面的担忧，使其成为不可信的决策支持系统。为了解决这种高风险采用的障碍，研究界的重点是开发方法来解释黑盒模型，以应对它们带来的挑战。然而，致力于解释这些模型而不是开发本质上具有可解释性的模型的做法可以引导机器学习领域的负责任和有益的应用。在此观点论文中，我们阐明了解释黑盒模型与采用本质上可解释模型之间的差距。我们强调了模型可解释性的重要性，并针对提高（即在预测性能方面更有效或更高效）和可信的预测器的目的，提供了将符号知识整合到神经网络预测器中的最新混合学习方法的实验评估。我们展示了如何使可解释的混合模型在不同领域潜在替代黑盒模型。 

---
# Residual Learning Inspired Crossover Operator and Strategy Enhancements for Evolutionary Multitasking 

**Title (ZH)**: 受残差学习启发的交叉操作及其在进化多任务学习中的策略增强 

**Authors**: Ruilin Wang, Xiang Feng, Huiqun Yu, Edmund M-K Lai  

**Link**: [PDF](https://arxiv.org/pdf/2503.21347)  

**Abstract**: In evolutionary multitasking, strategies such as crossover operators and skill factor assignment are critical for effective knowledge transfer. Existing improvements to crossover operators primarily focus on low-dimensional variable combinations, such as arithmetic crossover or partially mapped crossover, which are insufficient for modeling complex high-dimensional this http URL, static or semi-dynamic crossover strategies fail to adapt to the dynamic dependencies among tasks. In addition, current Multifactorial Evolutionary Algorithm frameworks often rely on fixed skill factor assignment strategies, lacking flexibility. To address these limitations, this paper proposes the Multifactorial Evolutionary Algorithm-Residual Learning (MFEA-RL) method based on residual learning. The method employs a Very Deep Super-Resolution (VDSR) model to generate high-dimensional residual representations of individuals, enhancing the modeling of complex relationships within dimensions. A ResNet-based mechanism dynamically assigns skill factors to improve task adaptability, while a random mapping mechanism efficiently performs crossover operations and mitigates the risk of negative transfer. Theoretical analysis and experimental results show that MFEA-RL outperforms state-of-the-art multitasking algorithms. It excels in both convergence and adaptability on standard evolutionary multitasking benchmarks, including CEC2017-MTSO and WCCI2020-MTSO. Additionally, its effectiveness is validated through a real-world application scenario. 

**Abstract (ZH)**: 多因子进化算法-残差学习（MFEA-RL）方法：一种用于有效知识转移的残差学习方法 

---
# A Low-Power Streaming Speech Enhancement Accelerator For Edge Devices 

**Title (ZH)**: 面向边缘设备的低功耗流式语音增强加速器 

**Authors**: Ci-Hao Wu, Tian-Sheuan Chang  

**Link**: [PDF](https://arxiv.org/pdf/2503.21335)  

**Abstract**: Transformer-based speech enhancement models yield impressive results. However, their heterogeneous and complex structure restricts model compression potential, resulting in greater complexity and reduced hardware efficiency. Additionally, these models are not tailored for streaming and low-power applications. Addressing these challenges, this paper proposes a low-power streaming speech enhancement accelerator through model and hardware optimization. The proposed high performance model is optimized for hardware execution with the co-design of model compression and target application, which reduces 93.9\% of model size by the proposed domain-aware and streaming-aware pruning techniques. The required latency is further reduced with batch normalization-based transformers. Additionally, we employed softmax-free attention, complemented by an extra batch normalization, facilitating simpler hardware design. The tailored hardware accommodates these diverse computing patterns by breaking them down into element-wise multiplication and accumulation (MAC). This is achieved through a 1-D processing array, utilizing configurable SRAM addressing, thereby minimizing hardware complexities and simplifying zero skipping. Using the TSMC 40nm CMOS process, the final implementation requires merely 207.8K gates and 53.75KB SRAM. It consumes only 8.08 mW for real-time inference at a 62.5MHz frequency. 

**Abstract (ZH)**: 基于Transformer的语音增强模型取得了令人印象深刻的成果。然而，其异构且复杂的结构限制了模型压缩的潜力，导致更高的复杂度和硬件效率降低。此外，这些模型并不适合流式处理和低功耗应用。针对这些挑战，本文提出了一种适用于流式处理的低功耗语音增强加速器，通过模型和硬件优化来解决。提出的高性能模型通过模型压缩和目标应用的协同设计进行优化，采用领域感知和流式感知剪枝技术减少了93.9%的模型大小。通过基于批量归一化的Transformer进一步减少了所需的延迟。此外，我们采用了无softmax的注意力机制，并通过额外的批量归一化加以补充，简化了硬件设计。针对这些不同的计算模式，定制的硬件通过元素级乘法和累加（MAC）将其分解。这种方法通过使用可配置的SRAM寻址的1-D处理阵列实现，从而减少了硬件复杂性并简化了零跳过。在TSMC 40nm CMOS工艺下，最终实现仅需207.8K门电路和53.75KB SRAM。在62.5MHz频率下进行实时推理时，功耗仅为8.08 mW。 

---
# ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback 

**Title (ZH)**: ReFeed：基于反馈反思推理的多维度总结 refinement 

**Authors**: Taewon Yun, Jihwan Oh, Hyangsuk Min, Yuho Lee, Jihwan Bang, Jason Cai, Hwanjun Song  

**Link**: [PDF](https://arxiv.org/pdf/2503.21332)  

**Abstract**: Summarization refinement faces challenges when extending to multi-dimension. In this paper, we introduce ReFeed, a powerful summarization refinement pipeline that enhances multiple dimensions through reflective reasoning on feedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based dataset optimized for training a lightweight model with reflective reasoning. Our experiments reveal how the number of dimensions, feedback exposure, and reasoning policy influence refinement performance, highlighting reflective reasoning and simultaneously addressing multiple feedback is crucial to mitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy feedback and feedback order. Lastly, our finding emphasizes that creating data with a proper goal and guideline constitutes a fundamental pillar of effective reasoning. The dataset and model will be released. 

**Abstract (ZH)**: 多维度总结提炼扩展面临挑战，本文介绍了一种名为ReFeed的总结提炼强化pipeline，通过反思性推理反馈增强多个维度。 

---
# DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data 

**Title (ZH)**: DeBackdoor: 一种在有限数据情况下检测深度模型后门攻击的演绎框架 

**Authors**: Dorde Popovic, Amin Sadeghi, Ting Yu, Sanjay Chawla, Issa Khalil  

**Link**: [PDF](https://arxiv.org/pdf/2503.21305)  

**Abstract**: Backdoor attacks are among the most effective, practical, and stealthy attacks in deep learning. In this paper, we consider a practical scenario where a developer obtains a deep model from a third party and uses it as part of a safety-critical system. The developer wants to inspect the model for potential backdoors prior to system deployment. We find that most existing detection techniques make assumptions that are not applicable to this scenario. In this paper, we present a novel framework for detecting backdoors under realistic restrictions. We generate candidate triggers by deductively searching over the space of possible triggers. We construct and optimize a smoothed version of Attack Success Rate as our search objective. Starting from a broad class of template attacks and just using the forward pass of a deep model, we reverse engineer the backdoor attack. We conduct extensive evaluation on a wide range of attacks, models, and datasets, with our technique performing almost perfectly across these settings. 

**Abstract (ZH)**: 后门攻击是深度学习中最有效、最实用和最隐秘的攻击方式之一。在本文中，我们考虑一个实际场景，即开发者从第三方获得一个深度模型，并将其作为安全关键系统的一部分使用。开发者希望在系统部署前检查模型中是否存在潜在的后门。我们发现，现有的大多数检测技术在该场景下所做的假设并不适用。在本文中，我们提出了一种在现实限制下检测后门的新框架。通过演绎性搜索可能的触发条件空间来生成候选触发条件。构建并优化修正后的攻击成功率作为搜索目标。从一类通用的模板攻击开始，仅利用深度模型的前向传递，我们逆向工程了后门攻击。我们对多种攻击、模型和数据集进行了广泛的评估，该技术在这些设置中表现几乎完美。 

---
# Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning 

**Title (ZH)**: 基于推理的学习：Few-Shot 类增量学习中的类比权重生成 

**Authors**: Jizhou Han, Chenhao Ding, Yuhang He, Songlin Dong, Qiang Wang, Xinyuan Gao, Yihong Gong  

**Link**: [PDF](https://arxiv.org/pdf/2503.21258)  

**Abstract**: Few-shot class-incremental Learning (FSCIL) enables models to learn new classes from limited data while retaining performance on previously learned classes. Traditional FSCIL methods often require fine-tuning parameters with limited new class data and suffer from a separation between learning new classes and utilizing old knowledge. Inspired by the analogical learning mechanisms of the human brain, we propose a novel analogical generative method. Our approach includes the Brain-Inspired Analogical Generator (BiAG), which derives new class weights from existing classes without parameter fine-tuning during incremental stages. BiAG consists of three components: Weight Self-Attention Module (WSA), Weight & Prototype Analogical Attention Module (WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory for semantic conversion, WSA supplements new class weights, and WPAA computes analogies to generate new class weights. Experiments on miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our method achieves higher final and average accuracy compared to SOTA methods. 

**Abstract (ZH)**: 少量样本类增量学习（Few-shot Class-Incremental Learning, FSCIL）使模型能够在有限数据下学习新类目，同时保持对之前学习类目的性能。传统的FSCIL方法往往需要在新类目数据有限的情况下微调参数，并且学习新类目和利用旧知识之间存在割裂。受人脑类比学习机制的启发，我们提出了一种新型类比生成方法。该方法包括脑启发类比生成器（BiAG），其能够在增量学习阶段无需参数微调即可从现有类目推导出新类目的权重。BiAG 包含三个组件：权重自注意力模块（WSA）、权重与原型类比注意力模块（WPAA）和语义转换模块（SCM）。SCM 使用神经坍塌理论进行语义转换，WSA 为新类目补充权重，WPAA 计算类比以生成新类目权重。在miniImageNet、CUB-200和CIFAR-100数据集上的实验表明，与最新方法相比，我们的方法实现了更高的最终准确率和平均准确率。 

---
# Dual-Splitting Conformal Prediction for Multi-Step Time Series Forecasting 

**Title (ZH)**: 双分裂同校预测多步时间序列预测 

**Authors**: Qingdi Yu, Zhiwei Cao, Ruihang Wang, Zhen Yang, Lijun Deng, Min Hu, Yong Luo, Xin Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2503.21251)  

**Abstract**: Time series forecasting is crucial for applications like resource scheduling and risk management, where multi-step predictions provide a comprehensive view of future trends. Uncertainty Quantification (UQ) is a mainstream approach for addressing forecasting uncertainties, with Conformal Prediction (CP) gaining attention due to its model-agnostic nature and statistical guarantees. However, most variants of CP are designed for single-step predictions and face challenges in multi-step scenarios, such as reliance on real-time data and limited scalability. This highlights the need for CP methods specifically tailored to multi-step forecasting. We propose the Dual-Splitting Conformal Prediction (DSCP) method, a novel CP approach designed to capture inherent dependencies within time-series data for multi-step forecasting. Experimental results on real-world datasets from four different domains demonstrate that the proposed DSCP significantly outperforms existing CP variants in terms of the Winkler Score, achieving a performance improvement of up to 23.59% compared to state-of-the-art methods. Furthermore, we deployed the DSCP approach for renewable energy generation and IT load forecasting in power management of a real-world trajectory-based application, achieving an 11.25% reduction in carbon emissions through predictive optimization of data center operations and controls. 

**Abstract (ZH)**: 时间序列预测对于资源调度和风险管理等应用至关重要，多步预测能够提供未来趋势的全面视图。不确定性量化（UQ）是应对预测不确定性的一种主流方法，而无模型预测（CP）因其模型无依赖性及统计保证正受到关注。然而，大多数CP的变体都设计用于单步预测，在多步场景中面临着实时数据依赖和可扩展性差等挑战。这凸显了需要专门针对多步预测的CP方法的必要性。我们提出了一种名为双分割无模型预测（DSCP）的新方法，这是一种专门针对时间序列数据固有依赖性进行多步预测的新型CP方法。实验结果表明，提出的DSCP在实际数据集上的Winkler评分上显著优于现有CP变体，相较于最先进的方法提高了多达23.59%的性能。此外，我们在一个基于轨迹的实际应用场景中部署了DSCP方法，用于可再生能源生成和IT负载预测，并通过预测优化数据中心的操作和控制，实现了11.25%的碳排放减少。 

---
# Improving $(α, f)$-Byzantine Resilience in Federated Learning via layerwise aggregation and cosine distance 

**Title (ZH)**: 通过层wise聚合和余弦距离提升联邦学习的$(α, f)$-拜占庭容错能力 

**Authors**: Mario García-Márquez, Nuria Rodríguez-Barroso, M.Victoria Luzón, Francisco Herrera  

**Link**: [PDF](https://arxiv.org/pdf/2503.21244)  

**Abstract**: The rapid development of artificial intelligence systems has amplified societal concerns regarding their usage, necessitating regulatory frameworks that encompass data privacy. Federated Learning (FL) is posed as potential solution to data privacy challenges in distributed machine learning by enabling collaborative model training {without data sharing}. However, FL systems remain vulnerable to Byzantine attacks, where malicious nodes contribute corrupted model updates. While Byzantine Resilient operators have emerged as a widely adopted robust aggregation algorithm to mitigate these attacks, its efficacy diminishes significantly in high-dimensional parameter spaces, sometimes leading to poor performing models. This paper introduces Layerwise Cosine Aggregation, a novel aggregation scheme designed to enhance robustness of these rules in such high-dimensional settings while preserving computational efficiency. A theoretical analysis is presented, demonstrating the superior robustness of the proposed Layerwise Cosine Aggregation compared to original robust aggregation operators. Empirical evaluation across diverse image classification datasets, under varying data distributions and Byzantine attack scenarios, consistently demonstrates the improved performance of Layerwise Cosine Aggregation, achieving up to a 16% increase in model accuracy. 

**Abstract (ZH)**: 人工智能系统的快速发展加剧了社会对其实用性的担忧， necessitating 调控框架以涵盖数据隐私。联邦学习（FL）被提出作为分布式机器学习中数据隐私挑战的潜在解决方案，通过实现无需数据共享的合作模型训练。然而，FL系统仍易受拜占庭攻击影响，恶意节点提交受污染的模型更新。虽然鲁棒的拜占庭鲁棒算子已广泛应用于缓解这些攻击，但在高维参数空间中其效果显著下降，有时会导致性能不佳的模型。本文提出层内余弦聚合，这是一种新的聚合方案，旨在在高维设置中增强这些规则的鲁棒性同时保持计算效率。理论分析表明，提出的层内余弦聚合在鲁棒性方面优于原始的鲁棒聚合算子。通过在多种图像分类数据集上进行实验评估，在不同的数据分布和拜占庭攻击场景下，层内余弦聚合始终表现出更好的性能，模型准确率最多可提高16%。 

---
# Feature-Enhanced Machine Learning for All-Cause Mortality Prediction in Healthcare Data 

**Title (ZH)**: 面向医疗数据的全因 mortality 预测的特征增强机器学习方法 

**Authors**: HyeYoung Lee, Pavel Tsoi  

**Link**: [PDF](https://arxiv.org/pdf/2503.21241)  

**Abstract**: Accurate patient mortality prediction enables effective risk stratification, leading to personalized treatment plans and improved patient outcomes. However, predicting mortality in healthcare remains a significant challenge, with existing studies often focusing on specific diseases or limited predictor sets. This study evaluates machine learning models for all-cause in-hospital mortality prediction using the MIMIC-III database, employing a comprehensive feature engineering approach. Guided by clinical expertise and literature, we extracted key features such as vital signs (e.g., heart rate, blood pressure), laboratory results (e.g., creatinine, glucose), and demographic information. The Random Forest model achieved the highest performance with an AUC of 0.94, significantly outperforming other machine learning and deep learning approaches. This demonstrates Random Forest's robustness in handling high-dimensional, noisy clinical data and its potential for developing effective clinical decision support tools. Our findings highlight the importance of careful feature engineering for accurate mortality prediction. We conclude by discussing implications for clinical adoption and propose future directions, including enhancing model robustness and tailoring prediction models for specific diseases. 

**Abstract (ZH)**: 准确的患者死亡率预测能够有效进行风险分层，从而制定个性化治疗方案并改善患者预后。然而，医疗领域中的死亡率预测仍然是一个重大挑战，现有研究往往集中在特定疾病或有限的预测因子上。本研究使用MIMIC-III数据库，评估了机器学习模型在所有原因医院内死亡率预测中的性能，并采用全面的特征工程方法。在临床专业知识和文献的指导下，我们提取了关键特征，如生命体征（例如，心率、血压）、实验室结果（例如，肌酐、血糖）和人口统计信息。随机森林模型取得了最佳性能，AUC为0.94，显著优于其他机器学习和深度学习方法。这展示了随机森林在处理高维度、噪声临床数据方面的 robustness，并表明其在开发有效的临床决策支持工具方面的潜力。我们的研究强调了精心的特征工程对于准确死亡率预测的重要性。最后，我们讨论了临床应用的 implications 并提出未来的研究方向，包括增强模型的 robustness 和为特定疾病定制预测模型。 

---
# Multi-Objective Optimization for Privacy-Utility Balance in Differentially Private Federated Learning 

**Title (ZH)**: 差分隐私联邦学习中隐私-效用平衡的多目标优化 

**Authors**: Kanishka Ranaweera, David Smith, Pubudu N. Pathirana, Ming Ding, Thierry Rakotoarivelo, Aruna Seneviratne  

**Link**: [PDF](https://arxiv.org/pdf/2503.21159)  

**Abstract**: Federated learning (FL) enables collaborative model training across distributed clients without sharing raw data, making it a promising approach for privacy-preserving machine learning. However, ensuring differential privacy (DP) in FL presents challenges due to the trade-off between model utility and privacy protection. Clipping gradients before aggregation is a common strategy to limit privacy loss, but selecting an optimal clipping norm is non-trivial, as excessively high values compromise privacy, while overly restrictive clipping degrades model performance. In this work, we propose an adaptive clipping mechanism that dynamically adjusts the clipping norm using a multi-objective optimization framework. By integrating privacy and utility considerations into the optimization objective, our approach balances privacy preservation with model accuracy. We theoretically analyze the convergence properties of our method and demonstrate its effectiveness through extensive experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Our results show that adaptive clipping consistently outperforms fixed-clipping baselines, achieving improved accuracy under the same privacy constraints. This work highlights the potential of dynamic clipping strategies to enhance privacy-utility trade-offs in differentially private federated learning. 

**Abstract (ZH)**: 联邦学习（FL）能够在不共享原始数据的情况下在分布式客户端之间进行协作模型训练，使其成为一种有前景的隐私保护机器学习方法。然而，确保在FL中达到差分隐私（DP）面临着在模型 utility 和隐私保护之间进行权衡的挑战。在聚合前对梯度进行裁剪是一种常见的限制隐私损失的策略，但选择最优的裁剪范数并非易事，因为过高的值会损害隐私，而过于严格的裁剪则会降低模型性能。在本工作中，我们提出了一种自适应裁剪机制，该机制使用多目标优化框架动态调整裁剪范数。通过将隐私和utility考虑纳入优化目标中，我们的方法在保护隐私和提高模型准确性之间找到了平衡。我们从理论上分析了该方法的收敛性质，并通过在MNIST、Fashion-MNIST和CIFAR-10数据集上的广泛实验展示了其有效性。结果显示，自适应裁剪在相同隐私约束条件下始终优于固定裁剪基准，实现了更高的准确性。本工作突显了动态裁剪策略在差分隐私联邦学习中增强隐私-utility权衡的潜力。 

---
# Federated Learning with Differential Privacy: An Utility-Enhanced Approach 

**Title (ZH)**: 差分隐私增强的联邦学习方法 

**Authors**: Kanishka Ranaweera, Dinh C. Nguyen, Pubudu N. Pathirana, David Smith, Ming Ding, Thierry Rakotoarivelo, Aruna Seneviratne  

**Link**: [PDF](https://arxiv.org/pdf/2503.21154)  

**Abstract**: Federated learning has emerged as an attractive approach to protect data privacy by eliminating the need for sharing clients' data while reducing communication costs compared with centralized machine learning algorithms. However, recent studies have shown that federated learning alone does not guarantee privacy, as private data may still be inferred from the uploaded parameters to the central server. In order to successfully avoid data leakage, adopting differential privacy (DP) in the local optimization process or in the local update aggregation process has emerged as two feasible ways for achieving sample-level or user-level privacy guarantees respectively, in federated learning models. However, compared to their non-private equivalents, these approaches suffer from a poor utility. To improve the privacy-utility trade-off, we present a modification to these vanilla differentially private algorithms based on a Haar wavelet transformation step and a novel noise injection scheme that significantly lowers the asymptotic bound of the noise variance. We also present a holistic convergence analysis of our proposed algorithm, showing that our method yields better convergence performance than the vanilla DP algorithms. Numerical experiments on real-world datasets demonstrate that our method outperforms existing approaches in model utility while maintaining the same privacy guarantees. 

**Abstract (ZH)**: 联邦学习作为一种保护数据隐私的有吸引力的方法，在减少通信成本的同时无需共享客户端数据。然而，近期研究表明，仅靠联邦学习并不能保证数据隐私，因为敏感数据仍可能从上传到中央服务器的参数中被推断出来。为了成功避免数据泄露，在局部优化过程或局部更新聚合过程中采用差分隐私（DP）已成为分别实现样本级或用户级隐私保障的两种可行方式。然而，与非隐私等效方法相比，这些方法带来了较差的实用性。为了改善隐私-实用性权衡，我们基于哈aar小波变换步骤和一种新型的噪声注入方案对这些基础的差分隐私算法进行了改进，显著降低了噪声方差的渐近界。我们还呈现了所提出算法的全面收敛分析，表明我们的方法在收敛性能上优于基础的DP算法。实际数据集上的数值实验表明，在保持相同隐私保障的前提下，我们的方法在模型实用性上优于现有方法。 

---
# Optimizing Multi-DNN Inference on Mobile Devices through Heterogeneous Processor Co-Execution 

**Title (ZH)**: 通过异构处理器协同执行优化移动设备上的多DNN推理 

**Authors**: Yunquan Gao, Zhiguo Zhang, Praveen Kumar Donta, Chinmaya Kumar Dehury, Xiujun Wang, Dusit Niyato, Qiyang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2503.21109)  

**Abstract**: Deep Neural Networks (DNNs) are increasingly deployed across diverse industries, driving demand for mobile device support. However, existing mobile inference frameworks often rely on a single processor per model, limiting hardware utilization and causing suboptimal performance and energy efficiency. Expanding DNN accessibility on mobile platforms requires adaptive, resource-efficient solutions to meet rising computational needs without compromising functionality. Parallel inference of multiple DNNs on heterogeneous processors remains challenging. Some works partition DNN operations into subgraphs for parallel execution across processors, but these often create excessive subgraphs based only on hardware compatibility, increasing scheduling complexity and memory overhead.
To address this, we propose an Advanced Multi-DNN Model Scheduling (ADMS) strategy for optimizing multi-DNN inference on mobile heterogeneous processors. ADMS constructs an optimal subgraph partitioning strategy offline, balancing hardware operation support and scheduling granularity, and uses a processor-state-aware algorithm to dynamically adjust workloads based on real-time conditions. This ensures efficient workload distribution and maximizes processor utilization. Experiments show ADMS reduces multi-DNN inference latency by 4.04 times compared to vanilla frameworks. 

**Abstract (ZH)**: 一种针对移动异构处理器的先进多DNN模型调度策略（ADMS）以优化多DNN推理性能 

---
# Confidence Adjusted Surprise Measure for Active Resourceful Trials (CA-SMART): A Data-driven Active Learning Framework for Accelerating Material Discovery under Resource Constraints 

**Title (ZH)**: 基于数据驱动的资源受限材料发现加速的自信心态调整意外度测量的主动学习框架（CA-SMART） 

**Authors**: Ahmed Shoyeb Raihan, Zhichao Liu, Tanveer Hossain Bhuiyan, Imtiaz Ahmed  

**Link**: [PDF](https://arxiv.org/pdf/2503.21095)  

**Abstract**: Accelerating the discovery and manufacturing of advanced materials with specific properties is a critical yet formidable challenge due to vast search space, high costs of experiments, and time-intensive nature of material characterization. In recent years, active learning, where a surrogate machine learning (ML) model mimics the scientific discovery process of a human scientist, has emerged as a promising approach to address these challenges by guiding experimentation toward high-value outcomes with a limited budget. Among the diverse active learning philosophies, the concept of surprise (capturing the divergence between expected and observed outcomes) has demonstrated significant potential to drive experimental trials and refine predictive models. Scientific discovery often stems from surprise thereby making it a natural driver to guide the search process. Despite its promise, prior studies leveraging surprise metrics such as Shannon and Bayesian surprise lack mechanisms to account for prior confidence, leading to excessive exploration of uncertain regions that may not yield useful information. To address this, we propose the Confidence-Adjusted Surprise Measure for Active Resourceful Trials (CA-SMART), a novel Bayesian active learning framework tailored for optimizing data-driven experimentation. On a high level, CA-SMART incorporates Confidence-Adjusted Surprise (CAS) to dynamically balance exploration and exploitation by amplifying surprises in regions where the model is more certain while discounting them in highly uncertain areas. We evaluated CA-SMART on two benchmark functions (Six-Hump Camelback and Griewank) and in predicting the fatigue strength of steel. The results demonstrate superior accuracy and efficiency compared to traditional surprise metrics, standard Bayesian Optimization (BO) acquisition functions and conventional ML methods. 

**Abstract (ZH)**: 加速具有特定性质的先进材料的发现和制造是一项关键但艰巨的挑战，由于搜索空间 vast、实验成本高以及材料表征的耗时性。近年来，模拟人类科学家的科学发现过程的代理机器学习（ML）模型的主动学习方法 emerged 作为一种有前景的解决办法，通过引导有限预算下的实验导向高价值成果。在多种主动学习哲学中，意外（捕捉预期结果与观察结果之间的差异）的概念已经显示出显著的潜力，可以驱动实验试错和改进预测模型。科学发现往往源于意外，使其成为引导搜索过程的自然驱动因素。尽管如此，先前利用香农和贝叶斯意外度量的研究缺乏考虑先验信心的机制，导致对可能不会产生有用信息的不确定区域进行过度探索。为了解决这个问题，我们提出了一种新的贝叶斯主动学习框架——基于置信调整的意外度量以优化数据驱动实验（CA-SMART）。总体而言，CA-SMART通过在模型更确定的区域放大意外，在高度不确定的区域减小意外来动态平衡探索与利用。我们在两个基准函数（六峰骆驼峰和 Griewank）上以及预测钢的疲劳强度上评估了CA-SMART。结果显示，与传统意外度量、标准贝叶斯优化获取函数以及传统机器学习方法相比，CA-SMART在准确性和效率上表现出更优的结果。 

---
# Improving User Behavior Prediction: Leveraging Annotator Metadata in Supervised Machine Learning Models 

**Title (ZH)**: 改进用户行为预测：在监督机器学习模型中利用标注者元数据 

**Authors**: Lynnette Hui Xian Ng, Kokil Jaidka, Kaiyuan Tay, Hansin Ahuja, Niyati Chhaya  

**Link**: [PDF](https://arxiv.org/pdf/2503.21000)  

**Abstract**: Supervised machine-learning models often underperform in predicting user behaviors from conversational text, hindered by poor crowdsourced label quality and low NLP task accuracy. We introduce the Metadata-Sensitive Weighted-Encoding Ensemble Model (MSWEEM), which integrates annotator meta-features like fatigue and speeding. First, our results show MSWEEM outperforms standard ensembles by 14\% on held-out data and 12\% on an alternative dataset. Second, we find that incorporating signals of annotator behavior, such as speed and fatigue, significantly boosts model performance. Third, we find that annotators with higher qualifications, such as Master's, deliver more consistent and faster annotations. Given the increasing uncertainty over annotation quality, our experiments show that understanding annotator patterns is crucial for enhancing model accuracy in user behavior prediction. 

**Abstract (ZH)**: 监督机器学习模型在预测对话文本用户行为时 often underperform，受限于crowdsourced标签质量差和低NLP任务准确性。我们引入了敏感元数据加权编码集成模型（MSWEEM），该模型整合了注释员的元特征，如疲劳和加速。首先，我们的结果显示，MSWEEM在保留数据上优于标准集成14%，在替代数据集上优于12%。其次，我们发现整合注释员行为信号，如速度和疲劳，显著提升了模型性能。第三，我们发现具有较高资质的注释员，如硕士学历，提供更一致且更快的标注。鉴于注释质量的不确定性日益增加，我们的实验表明，了解注释员模式对于提高用户行为预测模型的准确性至关重要。 

---
# Competitive Multi-armed Bandit Games for Resource Sharing 

**Title (ZH)**: 竞争性多臂 bandeit 游戏及其在资源分配中的应用 

**Authors**: Hongbo Li, Lingjie Duan  

**Link**: [PDF](https://arxiv.org/pdf/2503.20975)  

**Abstract**: In modern resource-sharing systems, multiple agents access limited resources with unknown stochastic conditions to perform tasks. When multiple agents access the same resource (arm) simultaneously, they compete for successful usage, leading to contention and reduced rewards. This motivates our study of competitive multi-armed bandit (CMAB) games. In this paper, we study a new N-player K-arm competitive MAB game, where non-myopic players (agents) compete with each other to form diverse private estimations of unknown arms over time. Their possible collisions on same arms and time-varying nature of arm rewards make the policy analysis more involved than existing studies for myopic players. We explicitly analyze the threshold-based structures of social optimum and existing selfish policy, showing that the latter causes prolonged convergence time $\Omega(\frac{K}{\eta^2}\ln({\frac{KN}{\delta}}))$, while socially optimal policy with coordinated communication reduces it to $\mathcal{O}(\frac{K}{N\eta^2}\ln{(\frac{K}{\delta})})$. Based on the comparison, we prove that the competition among selfish players for the best arm can result in an infinite price of anarchy (PoA), indicating an arbitrarily large efficiency loss compared to social optimum. We further prove that no informational (non-monetary) mechanism (including Bayesian persuasion) can reduce the infinite PoA, as the strategic misreporting by non-myopic players undermines such approaches. To address this, we propose a Combined Informational and Side-Payment (CISP) mechanism, which provides socially optimal arm recommendations with proper informational and monetary incentives to players according to their time-varying private beliefs. Our CISP mechanism keeps ex-post budget balanced for social planner and ensures truthful reporting from players, achieving the minimum PoA=1 and same convergence time as social optimum. 

**Abstract (ZH)**: 现代资源共享系统中多臂 bandit 竞争游戏的研究 

---
# Sociotechnical Effects of Machine Translation 

**Title (ZH)**: 机器翻译的 sociotechnical 影响 

**Authors**: Joss Moorkens, Andy Way, Séamus Lankford  

**Link**: [PDF](https://arxiv.org/pdf/2503.20959)  

**Abstract**: While the previous chapters have shown how machine translation (MT) can be useful, in this chapter we discuss some of the side-effects and risks that are associated, and how they might be mitigated. With the move to neural MT and approaches using Large Language Models (LLMs), there is an associated impact on climate change, as the models built by multinational corporations are massive. They are hugely expensive to train, consume large amounts of electricity, and output huge volumes of kgCO2 to boot. However, smaller models which still perform to a high level of quality can be built with much lower carbon footprints, and tuning pre-trained models saves on the requirement to train from scratch. We also discuss the possible detrimental effects of MT on translators and other users. The topics of copyright and ownership of data are discussed, as well as ethical considerations on data and MT use. Finally, we show how if done properly, using MT in crisis scenarios can save lives, and we provide a method of how this might be done. 

**Abstract (ZH)**: 神经机器翻译与大型语言模型带来的副作用和风险及其缓解策略：对气候变化的影响与伦理考量及危机情境下的应用 

---
# TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models 

**Title (ZH)**: TS-Inverse：一种针对联邦时间序列预测模型的梯度反转攻击 

**Authors**: Caspar Meijer, Jiyue Huang, Shreshtha Sharma, Elena Lazovik, Lydia Y. Chen  

**Link**: [PDF](https://arxiv.org/pdf/2503.20952)  

**Abstract**: Federated learning (FL) for time series forecasting (TSF) enables clients with privacy-sensitive time series (TS) data to collaboratively learn accurate forecasting models, for example, in energy load prediction. Unfortunately, privacy risks in FL persist, as servers can potentially reconstruct clients' training data through gradient inversion attacks (GIA). Although GIA is demonstrated for image classification tasks, little is known about time series regression tasks. In this paper, we first conduct an extensive empirical study on inverting TS data across 4 TSF models and 4 datasets, identifying the unique challenges of reconstructing both observations and targets of TS data. We then propose TS-Inverse, a novel GIA that improves the inversion of TS data by (i) learning a gradient inversion model that outputs quantile predictions, (ii) a unique loss function that incorporates periodicity and trend regularization, and (iii) regularization according to the quantile predictions. Our evaluations demonstrate a remarkable performance of TS-Inverse, achieving at least a 2x-10x improvement in terms of the sMAPE metric over existing GIA methods on TS data. Code repository: this https URL 

**Abstract (ZH)**: 联邦学习（FL）在时间序列预测（TSF）中的应用使得拥有敏感隐私时间序列（TS）数据的客户端能够协作学习准确的预测模型，例如在电力负荷预测中的应用。然而，FL中的隐私风险仍然存在，因为服务器可以通过梯度反转攻击（GIA）潜在地重构客户端的训练数据。尽管GIA已经在图像分类任务中得到验证，但在时间序列回归任务中的应用尚不明确。在本文中，我们首先在4个TSF模型和4个数据集上进行了广泛的经验研究，识别出重构时间序列数据观察值和目标值的独特挑战。然后，我们提出了TS-Inverse，这是一种新提出的GIA方法，通过（i）学习输出分位数预测的梯度反转模型，（ii）结合周期性和趋势正则化的独特损失函数，以及（iii）根据分位数预测进行正则化，来改进时间序列数据的重构。我们的评估结果表明，TS-Inverse在时间序列数据上的sMAPE指标上至少实现了2倍至10倍的改进，优于现有GIA方法。代码仓库：this https URL 

---
# Prototype Guided Backdoor Defense 

**Title (ZH)**: 原型引导式后门防御 

**Authors**: Venkat Adithya Amula, Sunayana Samavedam, Saurabh Saini, Avani Gupta, Narayanan P J  

**Link**: [PDF](https://arxiv.org/pdf/2503.20925)  

**Abstract**: Deep learning models are susceptible to {\em backdoor attacks} involving malicious attackers perturbing a small subset of training data with a {\em trigger} to causes misclassifications. Various triggers have been used, including semantic triggers that are easily realizable without requiring the attacker to manipulate the image. The emergence of generative AI has eased the generation of varied poisoned samples. Robustness across types of triggers is crucial to effective defense. We propose Prototype Guided Backdoor Defense (PGBD), a robust post-hoc defense that scales across different trigger types, including previously unsolved semantic triggers. PGBD exploits displacements in the geometric spaces of activations to penalize movements toward the trigger. This is done using a novel sanitization loss of a post-hoc fine-tuning step. The geometric approach scales easily to all types of attacks. PGBD achieves better performance across all settings. We also present the first defense against a new semantic attack on celebrity face images. Project page: \hyperlink{this https URL}{this https URL}. 

**Abstract (ZH)**: 深度学习模型易受涉及恶意攻击者通过在一小部分训练数据中添加触发器以引起误分类的后门攻击影响。各种触发器已被使用，包括可以通过不需攻击者篡改图像即可轻松实现的语义触发器。生成式AI的出现简化了各种中毒样本的生成。不同类型的触发器之间的稳健性对于有效的防御至关重要。我们提出了一种名为Prototype Guided Backdoor Defense (PGBD)的稳健的后处理防御方法，它能够跨不同类型的触发器扩展，包括之前未解决的语义触发器。PGBD 利用激活的空间移位来惩罚向触发器方向的移动。这使用了一种新颖的后处理微调步骤中的 sanitization loss 实现。几何方法可以轻松扩展到所有类型的攻击。PGBD 在所有设置中均表现出更好的性能。我们还提出了针对名人面部图像新语义攻击的第一种防御方法。项目页面：[此链接](此链接)。 

---
# Assessing Generative Models for Structured Data 

**Title (ZH)**: 评估生成模型在结构化数据上的表现 

**Authors**: Reilly Cannon, Nicolette M. Laird, Caesar Vazquez, Andy Lin, Amy Wagler, Tony Chiang  

**Link**: [PDF](https://arxiv.org/pdf/2503.20903)  

**Abstract**: Synthetic tabular data generation has emerged as a promising method to address limited data availability and privacy concerns. With the sharp increase in the performance of large language models in recent years, researchers have been interested in applying these models to the generation of tabular data. However, little is known about the quality of the generated tabular data from large language models. The predominant method for assessing the quality of synthetic tabular data is the train-synthetic-test-real approach, where the artificial examples are compared to the original by how well machine learning models, trained separately on the real and synthetic sets, perform in some downstream tasks. This method does not directly measure how closely the distribution of generated data approximates that of the original. This paper introduces rigorous methods for directly assessing synthetic tabular data against real data by looking at inter-column dependencies within the data. We find that large language models (GPT-2), both when queried via few-shot prompting and when fine-tuned, and GAN (CTGAN) models do not produce data with dependencies that mirror the original real data. Results from this study can inform future practice in synthetic data generation to improve data quality. 

**Abstract (ZH)**: 合成表格数据生成已成为解决数据稀缺性和隐私问题的有前景方法。随着大型语言模型性能在近年来的显著提升，研究人员的兴趣转向将这些模型应用于表格数据的生成。然而，关于大型语言模型生成的表格数据质量的研究仍然不多。目前评估合成表格数据质量的主要方法是“训练-合成-测试-真实”方法，通过分别在真实数据集和合成数据集上训练的机器学习模型在下游任务上的表现来比较人工示例与原始数据。该方法没有直接测量生成数据分布与原始数据分布的接近程度。本文引入了直接将合成表格数据与真实数据进行比较，通过分析数据内在列依赖性的严格方法。研究发现，无论是通过少样本提示查询还是微调的大语言模型（GPT-2）和生成对抗网络（CTGAN）模型生成的数据，其依赖关系均未能忠实反映原始真实数据的依赖结构。本研究的结果可为未来合成数据生成实践改进数据质量提供指导。 

---
# Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework 

**Title (ZH)**: 基于GAN的防御框架：针对中毒攻击的鲁棒联邦学习 

**Authors**: Usama Zafar, André Teixeira, Salman Toor  

**Link**: [PDF](https://arxiv.org/pdf/2503.20884)  

**Abstract**: Federated Learning (FL) enables collaborative model training across decentralized devices without sharing raw data, but it remains vulnerable to poisoning attacks that compromise model integrity. Existing defenses often rely on external datasets or predefined heuristics (e.g. number of malicious clients), limiting their effectiveness and scalability. To address these limitations, we propose a privacy-preserving defense framework that leverages a Conditional Generative Adversarial Network (cGAN) to generate synthetic data at the server for authenticating client updates, eliminating the need for external datasets. Our framework is scalable, adaptive, and seamlessly integrates into FL workflows. Extensive experiments on benchmark datasets demonstrate its robust performance against a variety of poisoning attacks, achieving high True Positive Rate (TPR) and True Negative Rate (TNR) of malicious and benign clients, respectively, while maintaining model accuracy. The proposed framework offers a practical and effective solution for securing federated learning systems. 

**Abstract (ZH)**: 联邦学习（FL）能够在不共享原始数据的情况下跨去中心化设备进行协作模型训练，但仍然容易受到破坏性攻击的影响，这些攻击会损害模型的完整性。现有的防御措施往往依赖于外部数据集或预定义的启发式方法（例如恶意客户端的数量），这限制了它们的有效性和可扩展性。为了解决这些限制，我们提出了一种基于条件生成对抗网络（cGAN）的隐私保护防御框架，该框架在服务器端生成合成数据以验证客户端更新，从而消除对外部数据集的依赖。该框架可扩展、适应性强，并能够无缝集成到联邦学习工作流中。基准数据集上的 extensive 实验表明，该框架对各种破坏性攻击具有稳健的性能，能够分别实现恶意和良性客户端的高真阳性率（TPR）和真阴性率（TNR），同时保持模型精度。所提出的框架为保障联邦学习系统的安全性提供了实用且有效的解决方案。 

---
# The Backfiring Effect of Weak AI Safety Regulation 

**Title (ZH)**: 弱人工智能安全监管的反作用效应 

**Authors**: Benjamin Laufer, Jon Kleinberg, Hoda Heidari  

**Link**: [PDF](https://arxiv.org/pdf/2503.20848)  

**Abstract**: Recent policy proposals aim to improve the safety of general-purpose AI, but there is little understanding of the efficacy of different regulatory approaches to AI safety. We present a strategic model that explores the interactions between the regulator, the general-purpose AI technology creators, and domain specialists--those who adapt the AI for specific applications. Our analysis examines how different regulatory measures, targeting different parts of the development chain, affect the outcome of the development process. In particular, we assume AI technology is described by two key attributes: safety and performance. The regulator first sets a minimum safety standard that applies to one or both players, with strict penalties for non-compliance. The general-purpose creator then develops the technology, establishing its initial safety and performance levels. Next, domain specialists refine the AI for their specific use cases, and the resulting revenue is distributed between the specialist and generalist through an ex-ante bargaining process. Our analysis of this game reveals two key insights: First, weak safety regulation imposed only on the domain specialists can backfire. While it might seem logical to regulate use cases (as opposed to the general-purpose technology), our analysis shows that weak regulations targeting domain specialists alone can unintentionally reduce safety. This effect persists across a wide range of settings. Second, in sharp contrast to the previous finding, we observe that stronger, well-placed regulation can in fact benefit all players subjected to it. When regulators impose appropriate safety standards on both AI creators and domain specialists, the regulation functions as a commitment mechanism, leading to safety and performance gains, surpassing what is achieved under no regulation or regulating one player only. 

**Abstract (ZH)**: 近期的政策建议旨在提高通用人工智能的安全性，但对不同监管方法的有效性缺乏理解。我们提出了一种战略模型，探讨监管者、通用人工智能技术创造者和领域专家（针对特定应用适应AI的技术专家）之间的互动。我们的分析研究了针对开发链不同环节的不同监管措施如何影响开发过程的结果。特别是，我们假设人工智能技术由两个关键属性描述：安全性和性能。监管者首先设定一个适用于一方或双方的最低安全标准，并对不合规行为实施严厉处罚。通用人工智能创造者随后开发技术，确定其初始的安全性和性能水平。接着，领域专家针对其特定使用场景细化人工智能，结果的收入通过事前谈判在专家和创造者之间分配。我们对这一博弈的分析揭示了两个关键洞察：首先，仅针对领域专家施加较弱的安全监管可能会产生反效果。尽管监管使用案例（而非通用技术）似乎是合理的，但我们的分析表明，仅针对领域专家的较弱监管可能会无意中降低安全性，这种效果在广泛的情景中都存在。其次，与上述发现形成鲜明对比的是，更强有力但恰到好处的监管实际上可以惠及所有受监管方。当监管者对人工智能创造者和领域专家都施加适当的安全标准时，监管将作为一个承诺机制，导致安全性和性能的提升，超过在没有监管或仅监管一方的情况下所能取得的成果。 

---
# Advancing Vulnerability Classification with BERT: A Multi-Objective Learning Model 

**Title (ZH)**: 基于BERT的多目标学习模型：推动漏洞分类技术进步 

**Authors**: Himanshu Tiwari  

**Link**: [PDF](https://arxiv.org/pdf/2503.20831)  

**Abstract**: The rapid increase in cybersecurity vulnerabilities necessitates automated tools for analyzing and classifying vulnerability reports. This paper presents a novel Vulnerability Report Classifier that leverages the BERT (Bidirectional Encoder Representations from Transformers) model to perform multi-label classification of Common Vulnerabilities and Exposures (CVE) reports from the National Vulnerability Database (NVD). The classifier predicts both the severity (Low, Medium, High, Critical) and vulnerability types (e.g., Buffer Overflow, XSS) from textual descriptions. We introduce a custom training pipeline using a combined loss function-Cross-Entropy for severity and Binary Cross-Entropy with Logits for types-integrated into a Hugging Face Trainer subclass. Experiments on recent NVD data demonstrate promising results, with decreasing evaluation loss across epochs. The system is deployed via a REST API and a Streamlit UI, enabling real-time vulnerability analysis. This work contributes a scalable, open-source solution for cybersecurity practitioners to automate vulnerability triage. 

**Abstract (ZH)**: 快速增加的网络安全漏洞促使开发自动化工具来分析和分类漏洞报告。本文提出了一种新颖的漏洞报告分类器，该分类器利用BERT模型对国家漏洞数据库（NVD）中的Common Vulnerabilities and Exposures（CVE）报告进行多标签分类。分类器可以从文本描述中预测漏洞的严重程度（低、中、高、关键）和类型（如缓冲区溢出、XSS等）。我们引入了一个自定义的训练管道，使用结合交叉熵损失函数（用于严重程度）和带标签的二元交叉熵损失函数（用于类型），并通过Hugging Face Trainer子类进行集成。在最近的NVD数据上的实验展示了有希望的结果，评估损失在迭代中逐渐降低。该系统通过REST API和Streamlit UI部署，实现实时漏洞分析。本文为网络安全从业人员提供了一种可扩展的开源解决方案，用于自动化漏洞triage。 

---
# Evidencing Unauthorized Training Data from AI Generated Content using Information Isotopes 

**Title (ZH)**: 基于信息同位素确证AI生成内容中未经授权的训练数据 

**Authors**: Qi Tao, Yin Jinhua, Cai Dongqi, Xie Yueqi, Wang Huili, Hu Zhiyang, Yang Peiru, Nan Guoshun, Zhou Zhili, Wang Shangguang, Lyu Lingjuan, Huang Yongfeng, Lane Nicholas  

**Link**: [PDF](https://arxiv.org/pdf/2503.20800)  

**Abstract**: In light of scaling laws, many AI institutions are intensifying efforts to construct advanced AIs on extensive collections of high-quality human data. However, in a rush to stay competitive, some institutions may inadvertently or even deliberately include unauthorized data (like privacy- or intellectual property-sensitive content) for AI training, which infringes on the rights of data owners. Compounding this issue, these advanced AI services are typically built on opaque cloud platforms, which restricts access to internal information during AI training and inference, leaving only the generated outputs available for forensics. Thus, despite the introduction of legal frameworks by various countries to safeguard data rights, uncovering evidence of data misuse in modern opaque AI applications remains a significant challenge. In this paper, inspired by the ability of isotopes to trace elements within chemical reactions, we introduce the concept of information isotopes and elucidate their properties in tracing training data within opaque AI systems. Furthermore, we propose an information isotope tracing method designed to identify and provide evidence of unauthorized data usage by detecting the presence of target information isotopes in AI generations. We conduct experiments on ten AI models (including GPT-4o, Claude-3.5, and DeepSeek) and four benchmark datasets in critical domains (medical data, copyrighted books, and news). Results show that our method can distinguish training datasets from non-training datasets with 99\% accuracy and significant evidence (p-value$<0.001$) by examining a data entry equivalent in length to a research paper. The findings show the potential of our work as an inclusive tool for empowering individuals, including those without expertise in AI, to safeguard their data rights in the rapidly evolving era of AI advancements and applications. 

**Abstract (ZH)**: 基于同位素原理的信息同位素追踪方法在不透明AI系统中追踪训练数据的研究 

---
# Toward a Human-Centered AI-assisted Colonoscopy System in Australia 

**Title (ZH)**: 面向以人为本的AI辅助结肠镜检查系统研究（澳大利亚） 

**Authors**: Hsiang-Ting Chen, Yuan Zhang, Gustavo Carneiro, Rajvinder Singh  

**Link**: [PDF](https://arxiv.org/pdf/2503.20790)  

**Abstract**: While AI-assisted colonoscopy promises improved colorectal cancer screening, its success relies on effective integration into clinical practice, not just algorithmic accuracy. This paper, based on an Australian field study (observations and gastroenterologist interviews), highlights a critical disconnect: current development prioritizes machine learning model performance, overlooking essential aspects of user interface design, workflow integration, and overall user experience. Industry interactions reveal a similar emphasis on data and algorithms. To realize AI's full potential, the HCI community must champion user-centered design, ensuring these systems are usable, support endoscopist expertise, and enhance patient outcomes. 

**Abstract (ZH)**: 尽管人工智能辅助结肠镜检查有望改善结直肠癌筛查，其成功依赖于在临床实践中的有效整合，而不仅仅是算法的准确性。基于一项澳大利亚实地研究（观察和胃肠病学家访谈），本论文揭示了当前开发工作中一个关键的断裂：现有工作优先关注机器学习模型性能，而忽视了用户界面设计、工作流程整合和整体用户体验等关键方面。工业界的互动也显示出对数据和算法的类似重视。为了充分发挥人工智能的潜力，人机交互社区必须倡导以用户为中心的设计，确保这些系统易于使用、支持内镜医生的专业技能，并提高患者结果。 

---
