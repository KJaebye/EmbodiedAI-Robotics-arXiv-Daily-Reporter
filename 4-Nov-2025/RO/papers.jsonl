{'arxiv_id': 'arXiv:2511.01797', 'title': 'Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator', 'authors': 'Javier Ballesteros-Jerez, Jesus Martínez-Gómez, Ismael García-Varea, Luis Orozco-Barbosa, Manuel Castillo-Cara', 'link': 'https://arxiv.org/abs/2511.01797', 'abstract': 'We present a hybrid neural network model for inferring the position of mobile robots using Channel State Information (CSI) data from a Massive MIMO system. By leveraging an existing CSI dataset, our approach integrates a Convolutional Neural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural Network (HyNN) that estimates 2D robot positions. CSI readings are converted into synthetic images using the TINTO tool. The localisation solution is integrated with a robotics simulator, and the Robot Operating System (ROS), which facilitates its evaluation through heterogeneous test cases, and the adoption of state estimators like Kalman filters. Our contributions illustrate the potential of our HyNN model in achieving precise indoor localisation and navigation for mobile robots in complex environments. The study follows, and proposes, a generalisable procedure applicable beyond the specific use case studied, making it adaptable to different scenarios and datasets.', 'abstract_zh': '基于大规模MIMO系统的信道状态信息的混合神经网络模型：移动机器人定位 inference using Channel State Information (CSI) data from a Massive MIMO system with a hybrid neural network model', 'title_zh': '基于CSI数据的混合神经网络室内定位系统在机器人模拟器中的应用'}
{'arxiv_id': 'arXiv:2511.01791', 'title': 'GenDexHand: Generative Simulation for Dexterous Hands', 'authors': 'Feng Chen, Zhuxiu Xu, Tianzhe Chu, Xunzhe Zhou, Li Sun, Zewen Wu, Shenghua Gao, Zhongyu Li, Yanchao Yang, Yi Ma', 'link': 'https://arxiv.org/abs/2511.01791', 'abstract': 'Data scarcity remains a fundamental bottleneck for embodied intelligence. Existing approaches use large language models (LLMs) to automate gripper-based simulation generation, but they transfer poorly to dexterous manipulation, which demands more specialized environment design. Meanwhile, dexterous manipulation tasks are inherently more difficult due to their higher degrees of freedom. Massively generating feasible and trainable dexterous hand tasks remains an open challenge. To this end, we present GenDexHand, a generative simulation pipeline that autonomously produces diverse robotic tasks and environments for dexterous manipulation. GenDexHand introduces a closed-loop refinement process that adjusts object placements and scales based on vision-language model (VLM) feedback, substantially improving the average quality of generated environments. Each task is further decomposed into sub-tasks to enable sequential reinforcement learning, reducing training time and increasing success rates. Our work provides a viable path toward scalable training of diverse dexterous hand behaviors in embodied intelligence by offering a simulation-based solution to synthetic data generation. Our website: this https URL.', 'abstract_zh': '数据稀缺仍然是体态智能的基本瓶颈。现有方法使用大规模语言模型（LLMs）来自动化夹持器基础的模拟生成，但它们在精细操作方面移植效果不佳，而精细操作需要更专门的环境设计。同时，由于其更高的自由度，精细操作任务本就更加困难。大规模生成可行且可训练的精细手任务依然是一项开放的挑战。为此，我们提出了GenDexHand，这是一种生成仿真流水线，能够自主产生多样化的机器人任务和环境以支持精细操作。GenDexHand 引入了一个闭环优化过程，根据视觉语言模型（VLM）反馈调整物体放置和缩放，显著提高了生成环境的平均质量。每个任务进一步分解为子任务，以支持顺序强化学习，从而减少训练时间和提高成功率。我们的工作提供了一条通过仿真生成合成数据来实现多样精细手行为可扩展训练的可行路径。我们的网站：this https URL。', 'title_zh': 'GenDexHand: 生成模拟 for 灵巧的手部'}
{'arxiv_id': 'arXiv:2511.01774', 'title': 'MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll', 'authors': 'Alexander Schperberg, Yusuke Tanaka, Stefano Di Cairano, Dennis Hong', 'link': 'https://arxiv.org/abs/2511.01774', 'abstract': 'This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot (MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features four limbs--two 6-DoF arms with two-finger grippers for manipulation and climbing, and two 4-DoF legs for locomotion--enabling smooth transitions across diverse terrains without reconfiguration. A hybrid control architecture combines reinforcement learning-based locomotion with model-based predictive and admittance control enhanced for safety by a Reference Governor toward compliant contact interactions. A high-level MIQCP planner autonomously selects locomotion modes to balance stability and energy efficiency. Hardware experiments demonstrate robust gait transitions, dynamic climbing, and full-body load support via pinch grasp. Overall, MOBIUS demonstrates the importance of tight integration between morphology, high-level planning, and control to enable mobile loco-manipulation and grasping, substantially expanding its interaction capabilities, workspace, and traversability.', 'abstract_zh': '一种多模态双足智能城市侦察机器人（MOBIUS）：具备行走、爬行、攀登和滚动能力的多模态双足智能城市侦察机器人', 'title_zh': 'MOBIUS：一种可以行走、爬行、攀爬和滚动的多模态双足机器人'}
{'arxiv_id': 'arXiv:2511.01770', 'title': 'Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping', 'authors': 'Liudi Yang, Yang Bai, Yuhao Wang, Ibrahim Alsarraj, Gitta Kutyniok, Zhanchi Wang, Ke Wu', 'link': 'https://arxiv.org/abs/2511.01770', 'abstract': "Robotic grasping under uncertainty remains a fundamental challenge due to its uncertain and contact-rich nature. Traditional rigid robotic hands, with limited degrees of freedom and compliance, rely on complex model-based and heavy feedback controllers to manage such interactions. Soft robots, by contrast, exhibit embodied mechanical intelligence: their underactuated structures and passive flexibility of their whole body, naturally accommodate uncertain contacts and enable adaptive behaviors. To harness this capability, we propose a lightweight actuation-space learning framework that infers distributional control representations for whole-body soft robotic grasping, directly from deterministic demonstrations using a flow matching model (Rectified Flow),without requiring dense sensing or heavy control loops. Using only 30 demonstrations (less than 8% of the reachable workspace), the learned policy achieves a 97.5% grasp success rate across the whole workspace, generalizes to grasped-object size variations of +-33%, and maintains stable performance when the robot's dynamic response is directly adjusted by scaling the execution time from 20% to 200%. These results demonstrate that actuation-space learning, by leveraging its passive redundant DOFs and flexibility, converts the body's mechanics into functional control intelligence and substantially reduces the burden on central controllers for this uncertain-rich task.", 'abstract_zh': '软体机器人在不确定性下的抓取仍是一项基本挑战：基于体内部份机械智能的轻量级驱动空间学习框架', 'title_zh': '基于流匹配的整体软体机器人抓取动作空间示范轻量学习'}
{'arxiv_id': 'arXiv:2511.01718', 'title': 'Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process', 'authors': 'Jiayi Chen, Wenxuan Song, Pengxiang Ding, Ziyang Zhou, Han Zhao, Feilong Tang, Donglin Wang, Haoang Li', 'link': 'https://arxiv.org/abs/2511.01718', 'abstract': 'Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act -- reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4$\\times$ faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at this https URL.', 'abstract_zh': 'vision-language-action (VLA) 模型旨在理解自然语言指令和视觉观察，并作为具身代理执行相应动作。近期的研究将未来图像整合进理解-执行循环中，产生了能够联合理解、生成和执行的统一 VLA——阅读文本和图像并生成未来图像和动作。然而，这些模型要么依赖外部专家进行模态统一，要么将图像生成和动作预测视为独立过程，限制了这些任务直接协同效应的发挥。我们的核心理念是通过同步去噪过程优化生成和动作，迭代精炼使动作在持续且充足的视觉引导下从初始化演变。我们基于提出的 Unified Diffusion VLA 和 Joint Discrete Denoising Diffusion Process (JD3P) 实现了这一理念，JD3P 是一种联合去噪过程，将多种模态整合到单一去噪轨迹中，作为使理解、生成和执行内在协同的关键机制。我们的模型和理论建立在一个统一的多模态标记空间以及一种混合注意力机制之上。我们还提出了两阶段训练管道和几种推理时的技术，以优化性能和效率。我们的方法在 CALVIN、LIBERO 和 SimplerEnv 等基准测试中达到最先进的性能，推理速度比自回归方法快 4 倍，并通过深入分析和实际评测证明了其有效性。我们的项目页面可访问此处。', 'title_zh': '统一扩散VLA模型：通过联合离散去噪扩散过程实现的多模态动作模型'}
{'arxiv_id': 'arXiv:2511.01594', 'title': 'MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence', 'authors': 'Renjun Gao, Peiyan Zhong', 'link': 'https://arxiv.org/abs/2511.01594', 'abstract': 'Multimodal large language models (MLLMs) have shown remarkable capabilities in cross-modal understanding and reasoning, offering new opportunities for intelligent assistive systems, yet existing systems still struggle with risk-aware planning, user personalization, and grounding language plans into executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic System powered by MLLMs for assistive intelligence and designed for smart home robots supporting people with disabilities. The system integrates four agents: a visual perception agent for extracting semantic and spatial features from environment images, a risk assessment agent for identifying and prioritizing hazards, a planning agent for generating executable action sequences, and an evaluation agent for iterative optimization. By combining multimodal perception with hierarchical multi-agent decision-making, the framework enables adaptive, risk-aware, and personalized assistance in dynamic indoor environments. Experiments on multiple datasets demonstrate the superior overall performance of the proposed system in risk-aware planning and coordinated multi-agent execution compared with state-of-the-art multimodal models. The proposed approach also highlights the potential of collaborative AI for practical assistive scenarios and provides a generalizable methodology for deploying MLLM-enabled multi-agent systems in real-world environments.', 'abstract_zh': '多模态大型语言模型（MLLMs）在跨模态理解和推理方面展现了显著的能力，为智能辅助系统提供了新的机遇，但现有系统仍难以应对风险感知规划、用户个性化以及在杂乱居家环境中将语言计划转化为可执行技能的挑战。我们介绍了MARS——一种由MLLMs驱动的多Agent机器人系统，旨在为支持残疾人的智能家用机器人提供辅助智能。该系统整合了四个代理：视觉感知代理，用于从环境图像中提取语义和空间特征；风险评估代理，用于识别和优先处理风险；规划代理，用于生成可执行的动作序列；评估代理，用于迭代优化。通过结合多模态感知与分层多Agent决策，该框架能够在动态室内环境中提供适配性强、风险感知和个性化的辅助。实验在多个数据集上展示了与最先进的多模态模型相比，所提出系统在风险感知规划和协调多Agent执行方面的优越整体性能。所提出的方法还强调了协作AI在实际辅助场景中的潜力，并提供了一种在现实环境中部署基于MLLM的多Agent系统的普遍方法。', 'title_zh': '多模态大型语言模型驱动的多Agent机器人系统辅助智能'}
{'arxiv_id': 'arXiv:2511.01520', 'title': 'Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals', 'authors': 'Shipeng Lyu, Lijie Sheng, Fangyuan Wang, Wenyao Zhang, Weiwei Lin, Zhenzhong Jia, David Navarro-Alarcon, Guodong Guo', 'link': 'https://arxiv.org/abs/2511.01520', 'abstract': 'Humans naturally grasp objects with minimal level required force for stability, whereas robots often rely on rigid, over-squeezing control. To narrow this gap, we propose a human-inspired physics-conditioned tactile method (Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection, tactile prediction, and force regulation. A physics-based pose selector first identifies feasible contact regions with optimal force distribution based on surface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM) predicts the tactile imprint under FOSG target. Last, a latent-space LQR controller drives the gripper toward this tactile imprint with minimal actuation, preventing unnecessary compression. Trained on a physics-conditioned tactile dataset covering diverse objects and contact conditions, the proposed Phy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac outperforms fixed-force and GraspNet-based baselines in grasp stability and force efficiency. Experiments on classical robotic platforms demonstrate force-efficient and adaptive manipulation that bridges the gap between robotic and human grasping.', 'abstract_zh': '基于物理条件的触觉方法（Phy-Tac）实现力最优稳定抓取（FOSG）', 'title_zh': 'Phy-Tac: 基于物理条件化的触觉目标实现人类般的抓取'}
{'arxiv_id': 'arXiv:2511.01493', 'title': 'Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues', 'authors': 'Wei Huang, Jiaxin Li, Zang Wan, Huijun Di, Wei Liang, Zhu Yang', 'link': 'https://arxiv.org/abs/2511.01493', 'abstract': "Guiding an agent to a specific target in indoor environments based solely on RGB inputs and a floor plan is a promising yet challenging problem. Although existing methods have made significant progress, two challenges remain unresolved. First, the modality gap between egocentric RGB observations and the floor plan hinders the integration of visual and spatial information for both local obstacle avoidance and global planning. Second, accurate localization is critical for navigation performance, but remains challenging at deployment in unseen environments due to the lack of explicit geometric alignment between RGB inputs and floor plans. We propose a novel diffusion-based policy, denoted as GlocDiff, which integrates global path planning from the floor plan with local depth-aware features derived from RGB observations. The floor plan offers explicit global guidance, while the depth features provide implicit geometric cues, collectively enabling precise prediction of optimal navigation directions and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation during training to enhance robustness against pose estimation errors, and we find that combining this with a relatively stable VO module during inference results in significantly improved navigation performance. Extensive experiments on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in achieving superior navigation performance, and the success of real-world deployments also highlights its potential for widespread practical applications.", 'abstract_zh': '基于RGB输入和楼层平面图引导代理在室内环境中导航：一种新的扩散策略的研究', 'title_zh': '基于楼层平面图的视觉导航：结合深度和方向线索'}
{'arxiv_id': 'arXiv:2511.01476', 'title': 'MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments', 'authors': 'Cankut Bora Tuncer, Marc Toussaint, Ozgur S. Oguz', 'link': 'https://arxiv.org/abs/2511.01476', 'abstract': 'In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided Manipulation planner for highly constrained rearrangement problems. MO-SeGMan generates object placement sequences that minimize both replanning per object and robot travel distance while preserving critical dependency structures with a lazy evaluation method. To address highly cluttered, non-monotone scenarios, we propose a Selective Guided Forward Search (SGFS) that efficiently relocates only critical obstacles and to feasible relocation points. Furthermore, we adopt a refinement method for adaptive subgoal selection to eliminate unnecessary pick-and-place actions, thereby improving overall solution quality. Extensive evaluations on nine benchmark rearrangement tasks demonstrate that MO-SeGMan generates feasible motion plans in all cases, consistently achieving faster solution times and superior solution quality compared to the baselines. These results highlight the robustness and scalability of the proposed framework for complex rearrangement planning problems.', 'abstract_zh': '多目标顺序引导操作规划器MO-SeGMan：一种用于高度受限重组问题的规划方法', 'title_zh': 'MO-SeGMan: 多目标序列引导操作在受限环境中的重排规划框架'}
{'arxiv_id': 'arXiv:2511.01472', 'title': 'AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models', 'authors': 'Sarthak Mishra, Rishabh Dev Yadav, Avirup Das, Saksham Gupta, Wei Pan, Spandan Roy', 'link': 'https://arxiv.org/abs/2511.01472', 'abstract': 'The rapid progress of vision--language models (VLMs) has sparked growing interest in robotic control, where natural language can express the operation goals while visual feedback links perception to action. However, directly deploying VLM-driven policies on aerial manipulators remains unsafe and unreliable since the generated actions are often inconsistent, hallucination-prone, and dynamically infeasible for flight. In this work, we present AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial manipulation by separating high-level reasoning from low-level control, without any task-specific fine-tuning. Our framework encodes natural language instructions, task context, and safety constraints into a structured prompt that guides the model to generate a step-by-step reasoning trace in natural language. This reasoning output is used to select from a predefined library of discrete, flight-safe skills, ensuring interpretable and temporally consistent execution. By decoupling symbolic reasoning from physical action, AERMANI-VLM mitigates hallucinated commands and prevents unsafe behavior, enabling robust task completion. We validate the framework in both simulation and hardware on diverse multi-step pick-and-place tasks, demonstrating strong generalization to previously unseen commands, objects, and environments.', 'abstract_zh': '视觉-语言模型(VLMs)的迅速进展激发了机器人控制领域日益增长的兴趣，其中自然语言可以表达操作目标，视觉反馈则将感知与行动联系起来。然而，直接在空中 manipulator 上部署由 VLM 驱动的策略仍然存在安全隐患且不可靠，因为生成的动作往往不一致、幻觉性强且在飞行中难以实现。在本文中，我们提出 AERMANI-VLM，这是首个通过将高层推理与低层控制分离来适应预训练 VLMs 的框架，无需任何任务特定的微调。该框架将自然语言指令、任务上下文和安全约束编码为结构化的提示，引导模型生成自然语言形式的逐步推理痕迹。该推理输出用于从预定义的、飞行安全的离散技能库中选择技能，确保可解释性和时间一致性执行。通过将符号推理与物理动作解耦，AERMANI-VLM 减少了幻觉命令并防止了不安全行为，从而使任务完成变得稳健。我们在仿真和硬件上针对多样化的多步骤拾放任务验证了该框架，展示了其在未见过的命令、物体和环境中的强大泛化能力。', 'title_zh': 'AERMANI-VLM：基于视觉语言模型的结构化提示与推理在空中 manipulation 中的应用'}
{'arxiv_id': 'arXiv:2511.01437', 'title': 'Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots', 'authors': 'Elian Neppel, Shamistan Karimov, Ashutosh Mishra, Gustavo Hernan Diaz Huenupan, Hazal Gozbasi, Kentaro Uno, Shreya Santra, Kazuya Yoshida', 'link': 'https://arxiv.org/abs/2511.01437', 'abstract': 'This paper presents the software architecture and deployment strategy behind the MoonBot platform: a modular space robotic system composed of heterogeneous components distributed across multiple computers, networks and ultimately celestial bodies. We introduce a principled approach to distributed, heterogeneous modularity, extending modular robotics beyond physical reconfiguration to software, communication and orchestration. We detail the architecture of our system that integrates component-based design, a data-oriented communication model using ROS2 and Zenoh, and a deployment orchestrator capable of managing complex multi-module assemblies. These abstractions enable dynamic reconfiguration, decentralized control, and seamless collaboration between numerous operators and modules. At the heart of this system lies our open-source Motion Stack software, validated by months of field deployment with self-assembling robots, inter-robot cooperation, and remote operation. Our architecture tackles the significant hurdles of modular robotics by significantly reducing integration and maintenance overhead, while remaining scalable and robust. Although tested with space in mind, we propose generalizable patterns for designing robotic systems that must scale across time, hardware, teams and operational environments.', 'abstract_zh': '本文介绍了MoonBot平台背后的软件架构和部署策略：一个由异构组件组成，并分布于多台计算机、网络乃至天体上的模块化太空机器人系统。我们提出了一种分层异构模块化的方法，将模块化机器人扩展到软件、通信和编排领域。我们详细介绍了将基于组件的设计、使用ROS2和Zenoh的数据面向通信模型以及能够管理复杂多模块组装的部署编排器集成到系统架构中的方法。这些抽象化手段实现了动态配置、分散控制以及多个操作者和模块之间的无缝协作。该系统的核心是我们的开源Motion Stack软件，该软件已经通过数月的现场部署，包括自组装机器人、机器人之间的协作以及远程操作得到了验证。该架构通过显著减少集成和维护开销，解决了模块化机器人面临的重大挑战，同时保持了可扩展性和稳健性。尽管最初为太空应用而设计，但我们提出的模式对于那些必须跨越时间、硬件、团队和操作环境进行扩展的机器人系统具有广泛的适用性。', 'title_zh': '设计分布式异构模块化系统：关于 MoonBots 的软件架构与部署研究'}
{'arxiv_id': 'arXiv:2511.01407', 'title': 'FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths', 'authors': 'Paolo Rabino, Gabriele Tiboni, Tatiana Tommasi', 'link': 'https://arxiv.org/abs/2511.01407', 'abstract': 'Object-Centric Motion Generation (OCMG) is instrumental in advancing automated manufacturing processes, particularly in domains requiring high-precision expert robotic motions, such as spray painting and welding. To realize effective automation, robust algorithms are essential for generating extended, object-aware trajectories across intricate 3D geometries. However, contemporary OCMG techniques are either based on ad-hoc heuristics or employ learning-based pipelines that are still reliant on sensitive post-processing steps to generate executable paths. We introduce FoldPath, a novel, end-to-end, neural field based method for OCMG. Unlike prior deep learning approaches that predict discrete sequences of end-effector waypoints, FoldPath learns the robot motion as a continuous function, thus implicitly encoding smooth output paths. This paradigm shift eliminates the need for brittle post-processing steps that concatenate and order the predicted discrete waypoints. Particularly, our approach demonstrates superior predictive performance compared to recently proposed learning-based methods, and attains generalization capabilities even in real industrial settings, where only a limited amount of 70 expert samples are provided. We validate FoldPath through comprehensive experiments in a realistic simulation environment and introduce new, rigorous metrics designed to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG task towards practical maturity.', 'abstract_zh': '面向对象的运动生成：FoldPath在先进制造过程中的应用', 'title_zh': 'FoldPath: 基于调节隐式路径的端到端以对象为中心的运动生成'}
{'arxiv_id': 'arXiv:2511.01383', 'title': 'CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation', 'authors': 'Landson Guo, Andres M. Diaz Aguilar, William Talbot, Turcan Tuna, Marco Hutter, Cesar Cadena', 'link': 'https://arxiv.org/abs/2511.01383', 'abstract': "Accurate point-wise velocity estimation in 3D is crucial for robot interaction with non-rigid, dynamic agents, such as humans, enabling robust performance in path planning, collision avoidance, and object manipulation in dynamic environments. To this end, this paper proposes a novel RADAR, LiDAR, and camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V. This pipeline leverages raw RADAR measurements to create a novel RADAR representation, the velocity cube, which densely represents radial velocities within the RADAR's field-of-view. By combining the velocity cube for radial velocity extraction, optical flow for tangential velocity estimation, and LiDAR for point-wise range measurements through a closed-form solution, our approach can produce 3D velocity estimates for a dense array of points. Developed as an open-source ROS2 package, CaRLi-V has been field-tested against a custom dataset and proven to produce low velocity error metrics relative to ground truth, enabling point-wise velocity estimation for robotic applications.", 'abstract_zh': '三维点 wise 速度估计对于与非刚性动态代理 （如人类）互动的机器人至关重要，能够为动态环境中的路径规划、碰撞避免和物体操作提供稳健的表现。为此，本文提出了一种基于雷达、激光雷达和相机融合的点 wise 三维速度估计新管道 CaRLi-V。该管道利用原始雷达测量创建了新颖的雷达表示——速度立方体，该表示在雷达视场内密集地表示径向速度。通过结合速度立方体进行径向速度提取、光学流进行切向速度估计以及激光雷达进行点 wise 距离测量的闭式解方式，我们的方法可以为密集点阵列生成 3D 速度估计。作为开源 ROS2 包，CaRLi-V 已在自定义数据集上进行了实地测试，并证明其速度误差指标较低，适用于机器人应用的点 wise 速度估计。', 'title_zh': 'CaRLi-V：摄像头-雷达-激光雷达点级三维速度估计'}
{'arxiv_id': 'arXiv:2511.01379', 'title': 'CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels', 'authors': 'Kun Hu, Menggang Li, Zhiwen Jin, Chaoquan Tang, Eryi Hu, Gongbo Zhou', 'link': 'https://arxiv.org/abs/2511.01379', 'abstract': "Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and GPS-denied underground coal mine environments presents significant challenges. Sensors must contend with abnormal operating conditions: GPS unavailability impedes scene reconstruction and absolute geographic referencing, uneven or slippery terrain degrades wheel odometer accuracy, and long, feature-poor tunnels reduce LiDAR effectiveness. To address these issues, we propose CoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM framework based on the Iterated Error-State Kalman Filter (IESKF). First, LiDAR-inertial odometry is tightly fused with UWB absolute positioning constraints to align the SLAM system with a global coordinate. Next, wheel odometer is integrated through tight coupling, enhanced by nonholonomic constraints (NHC) and vehicle lever arm compensation, to address performance degradation in areas beyond UWB measurement range. Finally, an adaptive motion mode switching mechanism dynamically adjusts the robot's motion mode based on UWB measurement range and environmental degradation levels. Experimental results validate that our method achieves superior accuracy and robustness in real-world underground coal mine scenarios, outperforming state-of-the-art approaches. We open source our code of this work on Github to benefit the robotics community.", 'abstract_zh': '大规模复杂GPS受限地下煤矿环境中的同时定位与建图（SLAM）面临重大挑战：传感器需应对异常工况，GPS不可用妨碍场景重建和绝对地理定位，不平或湿滑地形降低车轮里程计准确性，长且特征稀少的隧道降低LiDAR效用。为此，我们提出CoalMine-LiDAR-IMU-UWB-Wheel-Odometry（CM-LIUW-Odometry）多模态SLAM框架，基于迭代误差状态卡尔曼滤波（IESKF）。首先，LiDAR惯性里程计与UWB绝对定位约束紧密融合，实现SLAM系统与全球坐标系的对齐。其次，通过非holonomic约束（NHC）和车辆杠杆臂补偿增强车轮里程计的集成，以解决超出UWB测量范围区域的性能下降问题。最后，根据UWB测量范围和环境退化程度动态调整机器人运动模式。实验结果表明，本方法在实际地下煤矿场景中实现优于现有先进方法的更高精度和鲁棒性。我们已在Github上开源本文代码，以期惠及机器人学社区。', 'title_zh': 'CM-LIUW-里程计：极端退化煤矿隧道的鲁棒高精度激光雷达-惯性-UWB-轮毂里程计'}
{'arxiv_id': 'arXiv:2511.01369', 'title': 'Lateral Velocity Model for Vehicle Parking Applications', 'authors': 'Luis Diener, Jens Kalkkuhl, Markus Enzweiler', 'link': 'https://arxiv.org/abs/2511.01369', 'abstract': 'Automated parking requires accurate localization for quick and precise maneuvering in tight spaces. While the longitudinal velocity can be measured using wheel encoders, the estimation of the lateral velocity remains a key challenge due to the absence of dedicated sensors in consumer-grade vehicles. Existing approaches often rely on simplified vehicle models, such as the zero-slip model, which assumes no lateral velocity at the rear axle. It is well established that this assumption does not hold during low-speed driving and researchers thus introduce additional heuristics to account for differences. In this work, we analyze real-world data from parking scenarios and identify a systematic deviation from the zero-slip assumption. We provide explanations for the observed effects and then propose a lateral velocity model that better captures the lateral dynamics of the vehicle during parking. The model improves estimation accuracy, while relying on only two parameters, making it well-suited for integration into consumer-grade applications.', 'abstract_zh': '自动泊车需要准确的定位以在狭小空间内进行快速精确的操作。虽然 longitudinal 速度可以通过车轮编码器测量，但由于消费级车辆缺乏专用传感器，横向速度的估计仍然是一个关键挑战。现有方法往往依赖于简化的车辆模型，如零侧滑模型，该模型假设后轴无横向速度。研究已证实，这一假设在低速驾驶时并不成立，因此研究人员引入了额外的启发式方法来弥补差异。在本工作中，我们分析了泊车场景下的实际数据，并发现零侧滑假设存在系统偏差。我们解释了观察到的效果，然后提出了一种更好的横向速度模型，该模型在泊车过程中更好地捕捉了车辆的横向动态。该模型仅依赖于两个参数，从而使其非常适合集成到消费级应用中。', 'title_zh': '车道横向速度模型在车辆泊车应用中的研究'}
{'arxiv_id': 'arXiv:2511.01350', 'title': 'Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator', 'authors': 'Maartje H. M. Wermelink, Renate Sachse, Sebastian Kruppert, Thomas Speck, Falk J. Tauber', 'link': 'https://arxiv.org/abs/2511.01350', 'abstract': 'The Venus flytrap (Dionaea muscipula) does not only serve as the textbook model for a carnivorous plant, but also has long intrigued both botanists and engineers with its rapidly closing leaf trap. The trap closure is triggered by two consecutive touches of a potential prey, after which the lobes rapidly switch from their concave open-state to their convex close-state and catch the prey within 100-500 ms after being triggered. This transformation from concave to convex is initiated by changes in turgor pressure and the release of stored elastic energy from prestresses in the concave state, which accelerate this movement, leading to inversion of the lobes bi-axial curvature. Possessing two low-energy states, the leaves can be characterized as bistable systems. With our research, we seek to deepen the understanding of Venus flytrap motion mechanics and apply its principles to the design of an artificial bistable lobe actuator. We identified geometrical characteristics, such as dimensional ratios and the thickness gradient in the lobe, and transferred these to two 3D-printed bistable actuator models. One actuator parallels the simulated geometry of a Venus flytrap leaf, the other is a lobe model designed with CAD. Both models display concave-convex bi-stability and snap close. These demonstrators are the first step in the development of an artificial Venus flytrap that mimics the mechanical behavior of the biological model and can be used as a soft fast gripper.', 'abstract_zh': 'Venus 凤梨的运动机械学及其人工双稳态叶片 actuactor 设计研究', 'title_zh': '从模型到模型：理解捕蝇草捕虫机制并将其转移至3D打印双稳态软机器人示例'}
{'arxiv_id': 'arXiv:2511.01347', 'title': 'Design and development of an electronics-free earthworm robot', 'authors': 'Riddhi Das, Joscha Teichmann, Thomas Speck, Falk J. Tauber', 'link': 'https://arxiv.org/abs/2511.01347', 'abstract': 'Soft robotic systems have gained widespread attention due to their inherent flexibility, adaptability, and safety, making them well-suited for varied applications. Among bioinspired designs, earthworm locomotion has been extensively studied for its efficient peristaltic motion, enabling movement in confined and unstructured environments. Existing earthworm-inspired robots primarily utilize pneumatic actuation due to its high force-to-weight ratio and ease of implementation. However, these systems often rely on bulky, power-intensive electronic control units, limiting their practicality. In this work, we present an electronics-free, earthworm-inspired pneumatic robot utilizing a modified Pneumatic Logic Gate (PLG) design. By integrating preconfigured PLG units with bellow actuators, we achieved a plug-and-play style modular system capable of peristaltic locomotion without external electronic components. The proposed design reduces system complexity while maintaining efficient actuation. We characterize the bellow actuators under different operating conditions and evaluate the robots locomotion performance. Our findings demonstrate that the modified PLG-based control system effectively generates peristaltic wave propagation, achieving autonomous motion with minimal deviation. This study serves as a proof of concept for the development of electronics-free, peristaltic soft robots. The proposed system has potential for applications in hazardous environments, where untethered, adaptable locomotion is critical. Future work will focus on further optimizing the robot design and exploring untethered operation using onboard compressed air sources.', 'abstract_zh': '无需外部电子控制单元的基于蚯蚓灵感的气动软机器人系统', 'title_zh': '无电子元件地球worm机器人设计与开发'}
{'arxiv_id': 'arXiv:2511.01346', 'title': 'Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers', 'authors': 'Shun Yoshida, Qingchuan Song, Bastian E. Rapp, Thomas Speck, Falk J. Tauber', 'link': 'https://arxiv.org/abs/2511.01346', 'abstract': 'Despite their often perceived static and slow nature, some plants can move faster than the blink of an eye. The rapid snap closure motion of the Venus flytrap (Dionaea muscipula) has long captivated the interest of researchers and engineers alike, serving as a model for plant-inspired soft machines and robots. The translation of the fast snapping closure has inspired the development of various artificial Venus flytrap (AVF) systems. However, translating both the closing and reopening motion of D. muscipula into an autonomous plant inspired soft machine has yet to be achieved. In this study, we present an AVF that autonomously closes and reopens, utilizing novel thermo-responsive UV-curable shape memory materials for soft robotic systems. The life-sized thermo-responsive AVF exhibits closing and reopening motions triggered in a naturally occurring temperature range. The doubly curved trap lobes, built from shape memory polymers, close at 38°C, while reopening initiates around 45°C, employing shape memory elastomer strips as antagonistic actuators to facilitate lobe reopening. This work represents the first demonstration of thermo-responsive closing and reopening in an AVF with programmed sequential motion in response to increasing temperature. This approach marks the next step toward autonomously bidirectional moving soft machines/robots.', 'abstract_zh': '基于温度响应形状记忆材料的自主开合人工捕蝇植物', 'title_zh': '基于形状记忆弹性体的热响应闭合与重新开启人造 Venus 飞публиль'}
{'arxiv_id': 'arXiv:2511.01334', 'title': 'Embodied Cognition Augmented End2End Autonomous Driving', 'authors': 'Ling Niu, Xiaoji Zheng, Han Wang, Chen Zheng, Ziyuan Yang, Bokui Chen, Jiangtao Gong', 'link': 'https://arxiv.org/abs/2511.01334', 'abstract': 'In recent years, vision-based end-to-end autonomous driving has emerged as a new paradigm. However, popular end-to-end approaches typically rely on visual feature extraction networks trained under label supervision. This limited supervision framework restricts the generality and applicability of driving models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which advocates for comparative learning between visual feature extraction networks and the general EEG large model, in order to learn latent human driving cognition for enhancing end-to-end planning. In this work, we collected a cognitive dataset for the mentioned contrastive learning process. Subsequently, we investigated the methods and potential mechanisms for enhancing end-to-end planning with human driving cognition, using popular driving models as baselines on publicly available autonomous driving datasets. Both open-loop and closed-loop tests are conducted for a comprehensive evaluation of planning performance. Experimental results demonstrate that the $E^{3}AD$ paradigm significantly enhances the end-to-end planning performance of baseline models. Ablation studies further validate the contribution of driving cognition and the effectiveness of comparative learning process. To the best of our knowledge, this is the first work to integrate human driving cognition for improving end-to-end autonomous driving planning. It represents an initial attempt to incorporate embodied cognitive data into end-to-end autonomous driving, providing valuable insights for future brain-inspired autonomous driving systems. Our code will be made available at Github', 'abstract_zh': '基于视觉的端到端自动驾驶新兴范式: 嵌入人类驾驶认知的三阶自主驾驶 ($E^{3}AD$)', 'title_zh': '具身认知增强端到端自主驾驶'}
{'arxiv_id': 'arXiv:2511.01331', 'title': 'RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models', 'authors': 'Hongyin Zhang, Shuo Zhang, Junxi Jin, Qixin Zeng, Runze Li, Donglin Wang', 'link': 'https://arxiv.org/abs/2511.01331', 'abstract': 'Vision-Language-Action (VLA) models have recently emerged as powerful general-purpose policies for robotic manipulation, benefiting from large-scale multi-modal pre-training. However, they often fail to generalize reliably in out-of-distribution deployments, where unavoidable disturbances such as observation noise, sensor errors, or actuation perturbations become prevalent. While recent Reinforcement Learning (RL)-based post-training provides a practical means to adapt pre-trained VLA models, existing methods mainly emphasize reward maximization and overlook robustness to environmental uncertainty. In this work, we introduce RobustVLA, a lightweight online RL post-training method designed to explicitly enhance the resilience of VLA models. Through a systematic robustness analysis, we identify two key regularizations: Jacobian regularization, which mitigates sensitivity to observation noise, and smoothness regularization, which stabilizes policies under action perturbations. Extensive experiments across diverse robotic environments demonstrate that RobustVLA significantly outperforms prior state-of-the-art methods in robustness and reliability. Our results highlight the importance of principled robustness-aware RL post-training as a key step toward improving the reliability and robustness of VLA models.', 'abstract_zh': 'RobustVLA：一种用于增强Vision-Language-Action模型鲁棒性的轻量级在线RL后训练方法', 'title_zh': 'RobustVLA：面向鲁棒性的视觉-语言-行动模型后训练强化方法'}
{'arxiv_id': 'arXiv:2511.01294', 'title': 'Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects', 'authors': 'Jiawei Wang, Dingyou Wang, Jiaming Hu, Qixuan Zhang, Jingyi Yu, Lan Xu', 'link': 'https://arxiv.org/abs/2511.01294', 'abstract': 'A deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for complex systems like robots or objects with high degrees of freedom (DoF), remains a significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or text prompts. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work.', 'abstract_zh': '深入理解运动结构和可动组件对于使机器人能够操作物体并建模其自身 articulated 形态至关重要。这种理解通过articulated对象来捕捉，这些对象对于物理仿真、运动规划和策略学习等任务至关重要。然而，特别是对于机器人或具有高自由度（DoF）的复杂系统，创建这些模型仍然是一个重大挑战。现有方法通常依赖于运动序列或来自手工制作数据集的强假设，这限制了其扩展性。在这篇论文中，我们介绍了Kinematify，这是一个自动框架，可以直接从任意的RGB图像或文本提示中合成articulated对象。我们的方法解决了两个核心挑战：（i）推断高自由度对象的运动学拓扑结构，（ii）从静态几何结构估计关节参数。为了实现这一目标，我们将基于MCTS的结构推理与基于几何的优化相结合，进行关节推理，生成物理上一致且功能有效的描述。我们在来自合成和真实环境的多种输入上评估了Kinematify，表明与先前工作相比，在注册和运动学拓扑结构准确性方面取得了改进。', 'title_zh': 'Kinematify: 开放词汇高自由度articulated对象合成'}
{'arxiv_id': 'arXiv:2511.01288', 'title': 'A High-Speed Capable Spherical Robot', 'authors': 'Bixuan Zhang, Fengqi Zhang, Haojie Chen, You Wang, Jie Hao, Zhiyuan Luo, Guang Li', 'link': 'https://arxiv.org/abs/2511.01288', 'abstract': 'This paper designs a new spherical robot structure capable of supporting high-speed motion at up to 10 m/s. Building upon a single-pendulum-driven spherical robot, the design incorporates a momentum wheel with an axis aligned with the secondary pendulum, creating a novel spherical robot structure. Practical experiments with the physical prototype have demonstrated that this new spherical robot can achieve stable high-speed motion through simple decoupled control, which was unattainable with the original structure. The spherical robot designed for high-speed motion not only increases speed but also significantly enhances obstacle-crossing performance and terrain robustness.', 'abstract_zh': '这种论文设计了一种新的球形机器人结构，能够支持最高达10 m/s的高速运动。基于单摆驱动的球形机器人设计，该结构整合了一个动量轮，其轴与次级摆动轴对齐，从而创建了一种新颖的球形机器人结构。通过简单的解耦控制，物理原型的实际实验表明，这种新的球形机器人能够实现稳定的高速运动，这是原始结构无法达成的。设计用于高速运动的球形机器人不仅提高了速度，还显著增强了越障性能和地形适应性。', 'title_zh': '高速 capable 球面机器人'}
{'arxiv_id': 'arXiv:2511.01276', 'title': 'Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation', 'authors': 'Yiyao Ma, Kai Chen, Kexin Zheng, Qi Dou', 'link': 'https://arxiv.org/abs/2511.01276', 'abstract': 'Dexterous grasp generation is a fundamental challenge in robotics, requiring both grasp stability and adaptability across diverse objects and tasks. Analytical methods ensure stable grasps but are inefficient and lack task adaptability, while generative approaches improve efficiency and task integration but generalize poorly to unseen objects and tasks due to data limitations. In this paper, we propose a transfer-based framework for dexterous grasp generation, leveraging a conditional diffusion model to transfer high-quality grasps from shape templates to novel objects within the same category. Specifically, we reformulate the grasp transfer problem as the generation of an object contact map, incorporating object shape similarity and task specifications into the diffusion process. To handle complex shape variations, we introduce a dual mapping mechanism, capturing intricate geometric relationship between shape templates and novel objects. Beyond the contact map, we derive two additional object-centric maps, the part map and direction map, to encode finer contact details for more stable grasps. We then develop a cascaded conditional diffusion model framework to jointly transfer these three maps, ensuring their intra-consistency. Finally, we introduce a robust grasp recovery mechanism, identifying reliable contact points and optimizing grasp configurations efficiently. Extensive experiments demonstrate the superiority of our proposed method. Our approach effectively balances grasp quality, generation efficiency, and generalization performance across various tasks. Project homepage: this https URL', 'abstract_zh': '基于迁移的灵巧抓取生成：通过条件扩散模型在类别内从形状模板转移高质量抓取', 'title_zh': '基于条件扩散模型的手部接触图转移以生成泛化 Dexterous 抓取'}
{'arxiv_id': 'arXiv:2511.01272', 'title': 'Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics', 'authors': 'Sehui Jeong, Magaly C. Aviles, Athena X. Naylor, Cynthia Sung, Allison M. Okamura', 'link': 'https://arxiv.org/abs/2511.01272', 'abstract': 'Soft robots employing compliant materials and deformable structures offer great potential for wearable devices that are comfortable and safe for human interaction. However, achieving both structural integrity and compliance for comfort remains a significant challenge. In this study, we present a novel fabrication and design method that combines the advantages of origami structures with the material programmability and wearability of knitted fabrics. We introduce a general design method that translates origami patterns into knit designs by programming both stitch and material patterns. The method creates folds in preferred directions while suppressing unintended buckling and bending by selectively incorporating heat fusible yarn to create rigid panels around compliant creases. We experimentally quantify folding moments and show that stitch patterning enhances folding directionality while the heat fusible yarn (1) keeps geometry consistent by reducing edge curl and (2) prevents out-of-plane deformations by stiffening panels. We demonstrate the framework through the successful reproduction of complex origami tessellations, including Miura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted Kaleidocycle robot capable of locomotion. The combination of structural reconfigurability, material programmability, and potential for manufacturing scalability highlights knitted origami as a promising platform for next-generation wearable robotics.', 'abstract_zh': '采用 Origami 结构与编织面料的编程性和可穿戴性相结合的方法实现兼具结构完整性和柔顺性的软机器人：复杂折纸图案的编织实现及其在可穿戴机器人中的应用', 'title_zh': '基于 Origami 设计的针织软机器人面料的设计与制造'}
{'arxiv_id': 'arXiv:2511.01256', 'title': 'Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control', 'authors': 'Yasamin Foroutani, Yasamin Mousavi-Motlagh, Aya Barzelay, Tsu-Chin Tsao', 'link': 'https://arxiv.org/abs/2511.01256', 'abstract': 'Achieving precise control of robotic tool paths is often challenged by inherent system misalignments, unmodeled dynamics, and actuation inaccuracies. This work introduces an Iterative Learning Control (ILC) strategy to enable precise rotational insertion of a tool during robotic surgery, improving penetration efficacy and safety compared to straight insertion tested in subretinal injection. A 4 degree of freedom (DOF) robot manipulator is used, where misalignment of the fourth joint complicates the simple application of needle rotation, motivating an ILC approach that iteratively adjusts joint commands based on positional feedback. The process begins with calibrating the forward kinematics for the chosen surgical tool to achieve higher accuracy, followed by successive ILC iterations guided by Optical Coherence Tomography (OCT) volume scans to measure the error and refine control inputs. Experimental results, tested on subretinal injection tasks on ex vivo pig eyes, show that the optimized trajectory resulted in higher success rates in tissue penetration and subretinal injection compared to straight insertion, demonstrating the effectiveness of ILC in overcoming misalignment challenges. This approach offers potential applications for other high precision robot tasks requiring controlled insertions as well.', 'abstract_zh': '通过迭代学习控制实现机器人手术工具精确旋转插入的研究：克服对齐偏差和提升穿透效果与安全性', 'title_zh': '基于迭代学习控制的精确旋转插入以提高针刺穿透性'}
{'arxiv_id': 'arXiv:2511.01236', 'title': "Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments", 'authors': 'Junwen Zhang, Changyue Liu, Pengqi Fu, Xiang Guo, Ye Shi, Xudong Liang, Zhijian Wang, Hanzhi Ma', 'link': 'https://arxiv.org/abs/2511.01236', 'abstract': 'Endowed with inherent dynamical properties that grant them remarkable ruggedness and adaptability, spherical tensegrity robots stand as prototypical examples of hybrid softrigid designs and excellent mobile platforms. However, path planning for these robots in unknown environments presents a significant challenge, requiring a delicate balance between efficient exploration and robust planning. Traditional path planners, which treat the environment as a geometric grid, often suffer from redundant searches and are prone to failure in complex scenarios due to their lack of semantic understanding. To overcome these limitations, we reframe path planning in unknown environments as a semantic reasoning task. We introduce a Semantic Agent for Tensegrity robots (SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages high-level environmental comprehension to generate efficient and reliable planning this http URL the core of SATPlanner is an Adaptive Observation Window mechanism, inspired by the "fast" and "slow" thinking paradigms of LLMs. This mechanism dynamically adjusts the perceptual field of the agent: it narrows for rapid traversal of open spaces and expands to reason about complex obstacle configurations. This allows the agent to construct a semantic belief of the environment, enabling the search space to grow only linearly with the path length (O(L)) while maintaining path quality. We extensively evaluate SATPlanner in 1,000 simulation trials, where it achieves a 100% success rate, outperforming other real-time planning algorithms. Critically, SATPlanner reduces the search space by 37.2% compared to the A* algorithm while achieving comparable, near-optimal path lengths. Finally, the practical feasibility of SATPlanner is validated on a physical spherical tensegrity robot prototype.', 'abstract_zh': '具备固有的动态特性，赋予其非凡的鲁棒性和适应性，球形 tensegrity 机器人是混合软硬设计的典范实例，也是优秀的移动平台。然而，在未知环境中进行路径规划面临着重大挑战，需要在高效的探索和稳健的规划之间取得微妙平衡。传统路径规划算法将环境视为几何网格，往往遭受冗余搜索的困扰，并且在复杂场景中由于缺乏语义理解而容易失败。为了克服这些限制，我们将未知环境中路径规划重新定义为语义推理任务。我们引入了由大型语言模型（LLM）驱动的 tensegrity 机器人语义代理（SATPlanner）。SATPlanner 通过高层次的环境理解来生成高效可靠的规划路径。SATPlanner 的核心是受 LLM 的“快速”和“缓慢”思维 paradigm 启发的自适应观察窗口机制。该机制动态调整代理的感知范围：在快速穿越开阔空间时缩小感知范围，而在处理复杂障碍配置时扩大感知范围。这使得代理能够构建环境的语义信念，从而使搜索空间的增长与路径长度呈线性关系（O(L)），同时保持路径质量。我们在1,000次模拟试验中广泛评估了SATPlanner，其成功率达到100%，优于其他实时规划算法。关键地，与A*算法相比，SATPlanner 将搜索空间减少了37.2%，同时获得接近最优路径长度。最后，我们在物理球形 tensegrity 机器人原型上验证了SATPlanner 的实际可行性。', 'title_zh': '不只是搜索，还要理解：未知环境中文紧张柔性机器人语义路径规划代理'}
{'arxiv_id': 'arXiv:2511.01232', 'title': 'High-Precision Surgical Robotic System for Intraocular Procedures', 'authors': 'Yu-Ting Lai, Jacob Rosen, Yasamin Foroutani, Ji Ma, Wen-Cheng Wu, Jean-Pierre Hubschman, Tsu-Chin Tsao', 'link': 'https://arxiv.org/abs/2511.01232', 'abstract': 'Despite the extensive demonstration of robotic systems for both cataract and vitreoretinal procedures, existing technologies or mechanisms still possess insufficient accuracy, precision, and degrees of freedom for instrument manipulation or potentially automated tool exchange during surgical procedures. A new robotic system that focuses on improving tooltip accuracy, tracking performance, and smooth instrument exchange mechanism is therefore designed and manufactured. Its tooltip accuracy, precision, and mechanical capability of maintaining small incision through remote center of motion were externally evaluated using an optical coherence tomography (OCT) system. Through robot calibration and precise coordinate registration, the accuracy of tooltip positioning was measured to be 0.053$\\pm$0.031 mm, and the overall performance was demonstrated on an OCT-guided automated cataract lens extraction procedure with deep learning-based pre-operative anatomical modeling and real-time supervision.', 'abstract_zh': '尽管机器人系统在白内障和玻璃体视网膜手术中得到了广泛演示，现有技术或机制在仪器操作或手术过程中潜在的自动工具交换方面仍然缺乏足够的精度、准确性和操作自由度。因此设计并制造了一个新的机器人系统，专注于提高提示点精度、跟踪性能以及平滑的仪器交换机制。该系统的提示点精度、精度及其通过远程中心运动保持小切口的机械能力通过光学相干断层扫描(OCT)系统进行了外部评估。通过机器人校准和精确的坐标注册，提示点定位的准确性被测量为0.053±0.031 mm，并在基于深度学习的术前解剖建模和实时监督的OCT引导自动白内障晶状体提取过程中展示了整体性能。', 'title_zh': '高精度眼科手术机器人系统'}
{'arxiv_id': 'arXiv:2511.01224', 'title': 'Embodiment Transfer Learning for Vision-Language-Action Models', 'authors': 'Chengmeng Li, Yaxin Peng', 'link': 'https://arxiv.org/abs/2511.01224', 'abstract': "Vision-language-action (VLA) models have significantly advanced robotic learning, enabling training on large-scale, cross-embodiment data and fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs struggle with multi-robot collaboration. We introduce embodiment transfer learning, denoted as ET-VLA, a novel framework for efficient and effective transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic Continued Pretraining (SCP), which uses synthetically generated data to warm up the model for the new embodiment, bypassing the need for real human demonstrations and reducing data collection costs. SCP enables the model to learn correct actions and precise action token numbers. Following SCP, the model is fine-tuned on target embodiment data. To further enhance the model performance on multi-embodiment, we present the Embodied Graph-of-Thought technique, a novel approach that formulates each sub-task as a node, that allows the VLA model to distinguish the functionalities and roles of each embodiment during task execution. Our work considers bimanual robots, a simple version of multi-robot to verify our approaches. We validate the effectiveness of our method on both simulation benchmarks and real robots covering three different bimanual embodiments. In particular, our proposed ET-VLA \\space can outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all codes to support the community in advancing VLA models for robot learning.", 'abstract_zh': '基于视觉-语言-行动的多机器人协作学习框架：ET-VLA', 'title_zh': '视觉-语言-行动模型中的躯体化迁移学习'}
{'arxiv_id': 'arXiv:2511.01219', 'title': 'Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference', 'authors': 'Muhua Zhang, Lei Ma, Ying Wu, Kai Shen, Deqing Huang, Henry Leung', 'link': 'https://arxiv.org/abs/2511.01219', 'abstract': 'This paper addresses the Kidnapped Robot Problem (KRP), a core localization challenge of relocalizing a robot in a known map without prior pose estimate when localization loss or at SLAM initialization. For this purpose, a passive 2-D global relocalization framework is proposed. It estimates the global pose efficiently and reliably from a single LiDAR scan and an occupancy grid map while the robot remains stationary, thereby enhancing the long-term autonomy of mobile robots. The proposed framework casts global relocalization as a non-convex problem and solves it via the multi-hypothesis scheme with batched multi-stage inference and early termination, balancing completeness and efficiency. The Rapidly-exploring Random Tree (RRT), under traversability constraints, asymptotically covers the reachable space to generate sparse, uniformly distributed feasible positional hypotheses, fundamentally reducing the sampling space. The hypotheses are preliminarily ordered by the proposed Scan Mean Absolute Difference (SMAD), a coarse beam-error level metric that facilitates the early termination by prioritizing high-likelihood candidates. The SMAD computation is optimized for non-panoramic scans. And the Translation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for reliable orientation selection at hypothesized positions and accurate final pose evaluation to mitigate degradation in conventional likelihood-field metrics under translational uncertainty induced by sparse hypotheses, as well as non-panoramic LiDAR scan and environmental changes. Real-world experiments on a resource-constrained mobile robot with non-panoramic LiDAR scan demonstrate that the proposed framework outperforms existing methods in both global relocalization success rate and computational efficiency.', 'abstract_zh': '基于LiDAR单扫描的自适应鲁棒全局重定位框架', 'title_zh': '基于稀疏可行假设采样和可靠批次多阶段推断解决绑架机器人问题'}
{'arxiv_id': 'arXiv:2511.01199', 'title': 'Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures', 'authors': 'Max McCandless, Jonathan Hamid, Sammy Elmariah, Nathaniel Langer, Pierre E. Dupont', 'link': 'https://arxiv.org/abs/2511.01199', 'abstract': 'To move away from open-heart surgery towards safer transcatheter procedures, there is a growing need for improved imaging techniques and robotic solutions to enable simple, accurate tool navigation. Common imaging modalities, such as fluoroscopy and ultrasound, have limitations that can be overcome using cardioscopy, i.e., direct optical visualization inside the beating heart. We present a cardioscope designed as a steerable balloon. As a balloon, it can be collapsed to pass through the vasculature and subsequently inflated inside the heart for visualization and tool delivery through an integrated working channel. Through careful design of balloon wall thickness, a single input, balloon inflation pressure, is used to independently control two outputs, balloon diameter (corresponding to field of view diameter) and balloon bending angle (enabling precise working channel positioning). This balloon technology can be tuned to produce cardioscopes designed for a range of intracardiac tasks. To illustrate this approach, a balloon design is presented for the specific task of aortic leaflet laceration. Image-based closed-loop control of bending angle is also demonstrated as a means of enabling stable orientation control during tool insertion and removal.', 'abstract_zh': '为了远离开胸手术，转向更安全的经导管程序，需要改进的成像技术和机器人解决方案以实现简单、准确的工具导航越来越重要。常见的成像方式，如血管造影和超声波，可以通过心脏内直接光学可视化，即卡迪斯科（cardioscopy），来克服其局限性。我们提出了一种作为可定向气球设计的卡迪斯cope。作为一种气球，它可以折叠以通过血管并通过心脏充气进行可视化和通过内置工作通道输送工具。通过仔细设计气球壁厚，使用单个输入和气球充气压力，可以独立控制两个输出：气球直径（对应视野直径）和气球弯曲角度（实现工作通道精确定位）。这种气球技术可以调整以生产适用于一系列心脏内任务的卡迪斯cope。为了说明这种方法，我们提出了一种专为主动脉瓣裂伤设计的气球设计。还展示了基于图像的闭环控制弯曲角度的方法，以实现工具插入和移除过程中的稳定方向控制。', 'title_zh': '可操控气囊内镜的闭环控制方法及其在机器人辅助经皮心脏程序中的应用'}
{'arxiv_id': 'arXiv:2511.01186', 'title': 'LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping', 'authors': 'Lijie Wang, Lianjie Guo, Ziyi Xu, Qianhao Wang, Fei Gao, Xieyuanli Chen', 'link': 'https://arxiv.org/abs/2511.01186', 'abstract': 'Reconstructing large-scale colored point clouds is an important task in robotics, supporting perception, navigation, and scene understanding. Despite advances in LiDAR inertial visual odometry (LIVO), its performance remains highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation models, such as VGGT, suffer from limited scalability in large environments and inherently lack metric scale. To overcome these limitations, we propose LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion pipeline: First, a pre-fusion module with robust initialization refinement efficiently estimates VGGT poses and point clouds with coarse metric scale within each session. Then, a post-fusion module enhances cross-modal 3D similarity transformation, using bounding-box-based regularization to reduce scale distortions caused by inconsistent FOVs between LiDAR and camera sensors. Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT achieves dense, globally consistent colored point clouds and outperforms both VGGT-based methods and LIVO baselines. The implementation of our proposed novel color point cloud evaluation toolkit will be released as open source.', 'abstract_zh': '大规模彩色点云重建是机器人技术中的一个重要任务，支持感知、导航和场景理解。尽管在LiDAR惯性视觉里程计（LIVO）方面取得了进展，其性能仍然高度依赖于外在标定。同时，3D视觉基础模型如VGGT在大型环境中的可扩展性有限，并且本质上缺乏度量尺度。为克服这些限制，我们提出了一种名为LiDAR-VGGT的新框架，该框架通过两阶段粗到细融合管道紧密耦合LiDAR惯性里程计和最先进的VGGT模型：首先，一个前融合模块通过稳健的初始化精化在每次会话中高效估计具有粗度量尺度的VGGT姿态和点云。然后，一个后融合模块增强了跨模态3D相似变换，使用基于边界框的正则化来减少由LiDAR和摄像机传感器不一致的视场造成的尺度失真。在多个数据集上的广泛实验表明，LiDAR-VGGT实现了密集且全局一致的彩色点云，并在基于VGGT的方法和LIVO基线上表现出色。我们提出的新型彩色点云评估工具箱的实现将作为开源发布。', 'title_zh': 'LiDAR-VGGT：跨模态从粗到细融合用于全局一致的米尺级稠密 mapping'}
{'arxiv_id': 'arXiv:2511.01177', 'title': 'Scaling Cross-Embodiment World Models for Dexterous Manipulation', 'authors': 'Zihao He, Bo Ai, Tongzhou Mu, Yulin Liu, Weikang Wan, Jiawei Fu, Yilun Du, Henrik I. Christensen, Hao Su', 'link': 'https://arxiv.org/abs/2511.01177', 'abstract': 'Cross-embodiment learning seeks to build generalist robots that operate across diverse morphologies, but differences in action spaces and kinematics hinder data sharing and policy transfer. This raises a central question: Is there any invariance that allows actions to transfer across embodiments? We conjecture that environment dynamics are embodiment-invariant, and that world models capturing these dynamics can provide a unified interface across embodiments. To learn such a unified world model, the crucial step is to design state and action representations that abstract away embodiment-specific details while preserving control relevance. To this end, we represent different embodiments (e.g., human hands and robot hands) as sets of 3D particles and define actions as particle displacements, creating a shared representation for heterogeneous data and control problems. A graph-based world model is then trained on exploration data from diverse simulated robot hands and real human hands, and integrated with model-based planning for deployment on novel hardware. Experiments on rigid and deformable manipulation tasks reveal three findings: (i) scaling to more training embodiments improves generalization to unseen ones, (ii) co-training on both simulated and real data outperforms training on either alone, and (iii) the learned models enable effective control on robots with varied degrees of freedom. These results establish world models as a promising interface for cross-embodiment dexterous manipulation.', 'abstract_zh': '跨体态学习旨在构建能够在多种形态下操作的一般型机器人，但动作空间和运动学差异阻碍了数据共享和策略转移。这提出了一个中心问题：是否存在能使动作在不同体态间转移的不变性？我们推测环境动力学是体态不变的，并认为能够捕获这些动力学的环境模型可以提供统一接口跨体态使用。为了学习这样的统一环境模型，关键步骤是设计能抽象掉体态特定细节但保留控制相关性的状态和动作表示。为此，我们将不同的体态（例如，人类手和机器人手）表示为3D粒子集，并将动作定义为粒子位移，从而为异构数据和控制问题创建共享表示。基于图的环境模型在多种模拟机器人手和真实人类手的探索数据上进行训练，并与基于模型的规划集成用于新硬件的部署。实验表明：(i) 增加训练体态的数量可以提高对未见体态的泛化能力；(ii) 在模拟数据和真实数据上联合训练优于单独使用任一类数据进行训练；(iii) 学习到的模型能够有效控制具有不同自由度的机器人。这些结果确立了环境模型作为跨体态灵巧操控的有前途接口的地位。', 'title_zh': '扩展跨躯体世界模型以实现灵巧 manipulation'}
{'arxiv_id': 'arXiv:2511.01165', 'title': 'An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs', 'authors': 'Dong Heon Han, Mayank Mehta, Runze Zuo, Zachary Wanger, Daniel Bruder', 'link': 'https://arxiv.org/abs/2511.01165', 'abstract': "This study presents an enhanced proprioceptive method for accurate shape estimation of soft robots using only off-the-shelf sensors, ensuring cost-effectiveness and easy applicability. By integrating inertial measurement units (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling reliable long-term proprioception. A Kalman filter fuses segment tip orientations from both sensors in a mutually compensatory manner, improving shape estimation over single-sensor methods. A piecewise constant curvature model estimates the tip location from the fused orientation data and reconstructs the robot's deformation. Experiments under no loading, external forces, and passive obstacle interactions during 45 minutes of continuous operation showed a root mean square error of 16.96 mm (2.91% of total length), a 56% reduction compared to IMU-only benchmarks. These results demonstrate that our approach not only enables long-duration proprioception in soft robots but also maintains high accuracy and robustness across these diverse conditions.", 'abstract_zh': '本研究提出了一种增强型本体感受方法，仅使用市售传感器即可精确估计软机器人形状，确保成本效益和易于应用。通过将惯性测量单元（IMU）与互补弯曲传感器结合，减轻了IMU漂移，实现了可靠且长时间的本体感受。卡尔曼滤波器以互补方式融合了两个传感器的段端方向，改进了单传感器方法的形状估计。 piecewise常曲率模型从融合的方向数据中估计段端位置并重构机器人的变形。在45分钟连续运行状态下，无负载、外部力以及被动障碍物交互条件下的实验结果显示均方根误差为16.96 mm（总长度的2.91%），与仅使用IMU的基准相比降低了56%。这些结果表明，本研究的方法不仅能够在软机器人中实现长时间的本体感受，而且在这些不同的条件下保持了高精度和鲁棒性。', 'title_zh': '集成弯传感器和IMUs的增强型本体感受方法研究'}
{'arxiv_id': 'arXiv:2511.01107', 'title': 'SLAP: Shortcut Learning for Abstract Planning', 'authors': 'Y. Isabel Liu, Bowen Li, Benjamin Eysenbach, Tom Silver', 'link': 'https://arxiv.org/abs/2511.01107', 'abstract': 'Long-horizon decision-making with sparse rewards and continuous states and actions remains a fundamental challenge in AI and robotics. Task and motion planning (TAMP) is a model-based framework that addresses this challenge by planning hierarchically with abstract actions (options). These options are manually defined, limiting the agent to behaviors that we as human engineers know how to program (pick, place, move). In this work, we propose Shortcut Learning for Abstract Planning (SLAP), a method that leverages existing TAMP options to automatically discover new ones. Our key idea is to use model-free reinforcement learning (RL) to learn shortcuts in the abstract planning graph induced by the existing options in TAMP. Without any additional assumptions or inputs, shortcut learning leads to shorter solutions than pure planning, and higher task success rates than flat and hierarchical RL. Qualitatively, SLAP discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that differ significantly from the manually-defined ones. In experiments in four simulated robotic environments, we show that SLAP solves and generalizes to a wide range of tasks, reducing overall plan lengths by over 50% and consistently outperforming planning and RL baselines.', 'abstract_zh': '长时限决策问题中的稀疏奖励与连续状态和动作仍是对AI和机器人领域的一项基本挑战。基于任务和运动规划（TAMP）是一种通过使用抽象动作（选项）进行分层规划的模型化框架，以应对这一挑战。这些选项由人工定义，限制了代理只能执行我们作为人类工程师能够编程的行为（例如，抓取、放置、移动）。在本项工作中，我们提出了一种名为Shortcut Learning for Abstract Planning（SLAP）的方法，该方法利用现有TAMP选项自动发现新的选项。我们的核心思想是使用无模型的强化学习（RL）来学习在现有TAMP选项诱导的抽象规划图中的捷径。在没有任何额外假设或输入的情况下，捷径学习可以得到比纯粹规划更短的解决方案，并且在任务成功率上优于平面RL和分层RL。定性上，SLAP发现了一种动态的物理即兴行为（例如，轻拍、摇动、擦拭），这些行为与人工定义的行为有显著不同。在对四种模拟机器人环境进行的实验中，我们展示了SLAP能够解决并泛化到多种任务，总体规划长度减少了超过50%，并且在所有基线方法上表现优异。', 'title_zh': 'SLAP: Shortcut Learning for Abstract Planning'}
{'arxiv_id': 'arXiv:2511.01083', 'title': 'Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment', 'authors': 'Zihan Wang, Jianwen Li, Li-Fan Wu, Nina Mahmoudian', 'link': 'https://arxiv.org/abs/2511.01083', 'abstract': "Rivers are critical corridors for environmental monitoring and disaster response, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven policies can provide fast, low-cost coverage. However, deployment exposes simulation-trained policies with distribution shift and safety risks and requires efficient adaptation from limited human interventions. We study human-in-the-loop (HITL) learning with a conservative overseer who vetoes unsafe or inefficient actions and provides statewise preferences by comparing the agent's proposal with a corrective override. We introduce Statewise Hybrid Preference Alignment for Robotics (SPAR-H), which fuses direct preference optimization on policy logits with a reward-based pathway that trains an immediate-reward estimator from the same preferences and updates the policy using a trust-region surrogate. With five HITL rollouts collected from a fixed novice policy, SPAR-H achieves the highest final episodic reward and the lowest variance across initial conditions among tested methods. The learned reward model aligns with human-preferred actions and elevates nearby non-intervened choices, supporting stable propagation of improvements. We benchmark SPAR-H against imitation learning (IL), direct preference variants, and evaluative reinforcement learning (RL) in the HITL setting, and demonstrate real-world feasibility of continual preference alignment for UAV river following. Overall, dual statewise preferences empirically provide a practical route to data-efficient online adaptation in riverine navigation.", 'abstract_zh': '无人机引导的视觉驱动政策在河流监控和灾害响应中的关键走廊作用及其稳健的人机闭环学习', 'title_zh': '基于人类在环偏好对准的可展开视觉驱动无人机河流导航'}
{'arxiv_id': 'arXiv:2511.01031', 'title': 'AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models', 'authors': 'Mathieu Dubied, Paolo Tiso, Robert K. Katzschmann', 'link': 'https://arxiv.org/abs/2511.01031', 'abstract': 'The efficient optimization of actuated soft structures, particularly under complex nonlinear forces, remains a critical challenge in advancing robotics. Simulations of nonlinear structures, such as soft-bodied robots modeled using the finite element method (FEM), often demand substantial computational resources, especially during optimization. To address this challenge, we propose a novel optimization algorithm based on a tensorial parametric reduced order model (PROM). Our algorithm leverages dimensionality reduction and solution approximation techniques to facilitate efficient solving of nonlinear constrained optimization problems. The well-structured tensorial approach enables the use of analytical gradients within a specifically chosen reduced order basis (ROB), significantly enhancing computational efficiency. To showcase the performance of our method, we apply it to optimizing soft robotic swimmer shapes. These actuated soft robots experience hydrodynamic forces, subjecting them to both internal and external nonlinear forces, which are incorporated into our optimization process using a data-free ROB for fast and accurate computations. This approach not only reduces computational complexity but also unlocks new opportunities to optimize complex nonlinear systems in soft robotics, paving the way for more efficient design and control.', 'abstract_zh': '基于张量参数化降阶模型的软结构高效优化', 'title_zh': 'AquaROM：使用参数化降阶模型的软游动体形状优化流程'}
{'arxiv_id': 'arXiv:2511.00998', 'title': 'GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies', 'authors': 'Ziye Wang, Li Kang, Yiran Qin, Jiahua Ma, Zhanglin Peng, Lei Bai, Ruimao Zhang', 'link': 'https://arxiv.org/abs/2511.00998', 'abstract': "Recently, effective coordination in embodied multi-agent systems has remained a fundamental challenge, particularly in scenarios where agents must balance individual perspectives with global environmental awareness. Existing approaches often struggle to balance fine-grained local control with comprehensive scene understanding, resulting in limited scalability and compromised collaboration quality. In this paper, we present GauDP, a novel Gaussian-image synergistic representation that facilitates scalable, perception-aware imitation learning in multi-agent collaborative systems. Specifically, GauDP constructs a globally consistent 3D Gaussian field from decentralized RGB observations, then dynamically redistributes 3D Gaussian attributes to each agent's local perspective. This enables all agents to adaptively query task-critical features from the shared scene representation while maintaining their individual viewpoints. This design facilitates both fine-grained control and globally coherent behavior without requiring additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our method achieves superior performance over existing image-based methods and approaches the effectiveness of point-cloud-driven methods, while maintaining strong scalability as the number of agents increases.", 'abstract_zh': '最近，具身多智能体系统的有效协调依然是一个基本挑战，特别是在智能体必须平衡个体视角与全局环境意识的场景中。现有方法往往难以平衡精细的局部控制与全面的场景理解，导致有限的扩展性和协作质量受损。本文介绍了GauDP，一种新颖的高斯图像协同表示，它促进了多智能体协作系统中具有感知意识的模仿学习的高效性。具体而言，GauDP 从去中心化的RGB观测中构建全局一致的3D高斯场，然后动态重新分配3D高斯属性到每个智能体的局部视角。这使所有智能体能够适应性地从共享的场景表示中查询任务关键特征，同时保持各自的视角。此设计在不需额外感知模态（例如，3D点云）的情况下，实现了精细控制和全局一致行为的结合。我们已在RoboFactory基准测试上评估了GauDP，该基准包含多样化的多臂操作任务。我们的方法在图像基方法中表现更优，并接近基于点云的方法的效果，同时随着智能体数量增加保持了强大的扩展性。', 'title_zh': 'GauDP：通过高斯图像协同作用重 invent 多智能体合作在扩散策略中的实现'}
{'arxiv_id': 'arXiv:2511.00983', 'title': 'Breaking the Latency Barrier: Synergistic Perception and Control for High-Frequency 3D Ultrasound Servoing', 'authors': 'Yizhao Qian, Yujie Zhu, Jiayuan Luo, Li Liu, Yixuan Yuan, Guochen Ning, Hongen Liao', 'link': 'https://arxiv.org/abs/2511.00983', 'abstract': "Real-time tracking of dynamic targets amidst large-scale, high-frequency disturbances remains a critical unsolved challenge in Robotic Ultrasound Systems (RUSS), primarily due to the end-to-end latency of existing systems. This paper argues that breaking this latency barrier requires a fundamental shift towards the synergistic co-design of perception and control. We realize it in a novel framework with two tightly-coupled contributions: (1) a Decoupled Dual-Stream Perception Network that robustly estimates 3D translational state from 2D images at high frequency, and (2) a Single-Step Flow Policy that generates entire action sequences in one inference pass, bypassing the iterative bottleneck of conventional policies. This synergy enables a closed-loop control frequency exceeding 60Hz. On a dynamic phantom, our system not only tracks complex 3D trajectories with a mean error below 6.5mm but also demonstrates robust re-acquisition from over 170mm displacement. Furthermore, it can track targets at speeds of 102mm/s, achieving a terminal error below 1.7mm. Moreover, in-vivo experiments on a human volunteer validate the framework's effectiveness and robustness in a realistic clinical setting. Our work presents a RUSS holistically architected to unify high-bandwidth tracking with large-scale repositioning, a critical step towards robust autonomy in dynamic clinical environments.", 'abstract_zh': '实时跟踪大规模高频干扰中的动态目标仍然是Robotic Ultrasound Systems (RUSS) 中的关键未解决挑战，主要归因于现有系统的端到端延迟。本文认为打破这一延迟障碍需要朝着感知与控制的协同共设计进行根本性转变。我们通过一个新颖的框架实现这一目标，该框架包含两个紧密耦合的贡献：（1）解耦双流感知网络，该网络在高频下从二维图像中稳健估计三维平移状态；（2）单步流策略，该策略在一个推理过程中生成完整的动作序列，绕过了传统策略的迭代瓶颈。这种协同作用使得闭环控制频率超过60Hz。在动态 Phantom 上，我们的系统不仅以低于6.5毫米的均值误差跟踪复杂的3D轨迹，还能从超过170毫米的位移中稳健重新获取目标。此外，该系统能够在每秒102毫米的速度下跟踪目标，终端误差低于1.7毫米。此外，在人类志愿者的体内实验中，该框架在现实临床环境中显示出有效性和鲁棒性。我们所作的工作是设计了一个综合架构的RUSS，统一了高带宽跟踪与大规模重新定位，这对于动态临床环境中的稳健自主性来说是一个关键步骤。', 'title_zh': '打破延迟壁垒：协同感知与控制在高频3D超声伺服中的应用'}
{'arxiv_id': 'arXiv:2511.00940', 'title': 'URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model', 'authors': 'Zhe Li, Xiang Bai, Jieyu Zhang, Zhuangzhe Wu, Che Xu, Ying Li, Chengkai Hou, Shanghang Zhang', 'link': 'https://arxiv.org/abs/2511.00940', 'abstract': 'Constructing accurate digital twins of articulated objects is essential for robotic simulation training and embodied AI world model building, yet historically requires painstaking manual modeling or multi-stage pipelines. In this work, we propose \\textbf{URDF-Anything}, an end-to-end automatic reconstruction framework based on a 3D multimodal large language model (MLLM). URDF-Anything utilizes an autoregressive prediction framework based on point-cloud and text multimodal input to jointly optimize geometric segmentation and kinematic parameter prediction. It implements a specialized $[SEG]$ token mechanism that interacts directly with point cloud features, enabling fine-grained part-level segmentation while maintaining consistency with the kinematic parameter predictions. Experiments on both simulated and real-world datasets demonstrate that our method significantly outperforms existing approaches regarding geometric segmentation (mIoU 17\\% improvement), kinematic parameter prediction (average error reduction of 29\\%), and physical executability (surpassing baselines by 50\\%). Notably, our method exhibits excellent generalization ability, performing well even on objects outside the training set. This work provides an efficient solution for constructing digital twins for robotic simulation, significantly enhancing the sim-to-real transfer capability.', 'abstract_zh': '基于3D多模态大语言模型的URDF-Anything：自动还原框架', 'title_zh': 'URDF-Anything: 使用3D多模态语言模型构建articulated物体'}
{'arxiv_id': 'arXiv:2511.00933', 'title': 'Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation', 'authors': 'Xiangyu Shi, Zerui Li, Yanyuan Qiao, Qi Wu', 'link': 'https://arxiv.org/abs/2511.00933', 'abstract': 'Recent advances in Vision-and-Language Navigation in Continuous Environments (VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve zero-shot navigation. However, existing methods often rely on panoramic observations and two-stage pipelines involving waypoint predictors, which introduce significant latency and limit real-world applicability. In this work, we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that eliminates the need for panoramic views and waypoint predictors. Our approach uses only three frontal RGB-D images combined with natural language instructions, enabling MLLMs to directly predict actions. To enhance decision robustness, we introduce an Uncertainty-Aware Reasoning module that integrates (i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past Bidirectional Reasoning mechanism for globally coherent planning. Experiments on both simulated and real-robot environments demonstrate that our method significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines. These results demonstrate the practicality and effectiveness of Fast-SmartWay for real-world zero-shot embodied navigation.', 'abstract_zh': 'Recent Advances in Vision-and-Language Navigation in Continuous Environments (VLN-CE): Fast-SmartWay, an End-to-End Zero-Shot Framework Without Panoramic Views and Waypoint Predictors', 'title_zh': 'Fast-SmartWay: 全景-free 从端到端零样本视觉-语言导航'}
{'arxiv_id': 'arXiv:2511.00917', 'title': 'Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots', 'authors': 'Junyao Shi, Rujia Yang, Kaitian Chao, Selina Bingqing Wan, Yifei Shao, Jiahui Lei, Jianing Qian, Long Le, Pratik Chaudhari, Kostas Daniilidis, Chuan Wen, Dinesh Jayaraman', 'link': 'https://arxiv.org/abs/2511.00917', 'abstract': 'Today\'s best-explored routes towards generalist robots center on collecting ever larger "observations-in actions-out" robotics datasets to train large end-to-end models, copying a recipe that has worked for vision-language models (VLMs). We pursue a road less traveled: building generalist policies directly around VLMs by augmenting their general capabilities with specific robot capabilities encapsulated in a carefully curated set of perception, planning, and control modules. In Maestro, a VLM coding agent dynamically composes these modules into a programmatic policy for the current task and scenario. Maestro\'s architecture benefits from a streamlined closed-loop interface without many manually imposed structural constraints, and a comprehensive and diverse tool repertoire. As a result, it largely surpasses today\'s VLA models for zero-shot performance on challenging manipulation skills. Further, Maestro is easily extensible to incorporate new modules, easily editable to suit new embodiments such as a quadruped-mounted arm, and even easily adapts from minimal real-world experiences through local code edits.', 'abstract_zh': '今天探索通用机器人最佳途径集中在收集 ever 增大的“动作-输出”机器人数据集，以训练端到端大型模型，借鉴视觉-语言模型（VLMs）的成功配方。我们选择了一条较少走的路：通过将视觉-语言模型的通用能力与精心挑选的感知、规划和控制模块封装的具体机器人能力结合起来，直接构建通用策略。在 Maestro 中，一个 VLM 编码智能体动态组合这些模块，形成适用于当前任务和情境的程序化策略。Maestro 的架构得益于简洁的闭环接口和较少的人工结构约束，以及全面且多样化的工具库。因此，它在具有挑战性的操作技能零样本性能上显著超越了今天的 VLA 模型。此外，Maestro 很容易扩展以纳入新模块，容易编辑以适应新的体态，甚至可以通过局部代码编辑从少量的实地经验中适应。', 'title_zh': 'Maestro: 使用视觉语言模型 orchestrating 机器人模块的零样本通用机器人'}
{'arxiv_id': 'arXiv:2511.00840', 'title': 'Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches', 'authors': 'William Suliman, Ekaterina Chaikovskaia, Egor Davydenko, Roman Gorbachev', 'link': 'https://arxiv.org/abs/2511.00840', 'abstract': 'This work presents an extended framework for learning-based bipedal locomotion that incorporates a heuristic step-planning strategy guided by desired torso velocity tracking. The framework enables precise interaction between a humanoid robot and its environment, supporting tasks such as crossing gaps and accurately approaching target objects. Unlike approaches based on full or simplified dynamics, the proposed method avoids complex step planners and analytical models. Step planning is primarily driven by heuristic commands, while a Raibert-type controller modulates the foot placement length based on the error between desired and actual torso velocity. We compare our method with a model-based step-planning approach -- the Linear Inverted Pendulum Model (LIPM) controller. Experimental results demonstrate that our approach attains comparable or superior accuracy in maintaining target velocity (up to 80%), significantly greater robustness on uneven terrain (over 50% improvement), and improved energy efficiency. These results suggest that incorporating complex analytical, model-based components into the training architecture may be unnecessary for achieving stable and robust bipedal walking, even in unstructured environments.', 'abstract_zh': '基于启发式步态规划的人形机器人两足步行扩展框架： torso姿态追踪指导的步态规划策略及其应用', 'title_zh': '基于模型和无模型方法的动态双足行走学习启发式步骤规划比较研究'}
{'arxiv_id': 'arXiv:2511.00814', 'title': 'Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning', 'authors': 'Stella Kombo, Masih Haseli, Skylar Wei, Joel W. Burdick', 'link': 'https://arxiv.org/abs/2511.00814', 'abstract': 'Autonomous systems often must predict the motions of nearby agents from partial and noisy data. This paper asks and answers the question: "can we learn, in real-time, a nonlinear predictive model of another agent\'s motions?" Our online framework denoises and forecasts such dynamics using a modified sliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy measurements are embedded into a Hankel matrix, while an associated Page matrix enables singular-value hard thresholding (SVHT) to estimate the effective rank. A Cadzow projection enforces structured low-rank consistency, yielding a denoised trajectory and local noise variance estimates. From this representation, a time-varying Hankel-DMD lifted linear predictor is constructed for multi-step forecasts. The residual analysis provides variance-tracking signals that can support downstream estimators and risk-aware planning. We validate the approach in simulation under Gaussian and heavy-tailed noise, and experimentally on a dynamic crane testbed. Results show that the method achieves stable variance-aware denoising and short-horizon prediction suitable for integration into real-time control frameworks.', 'abstract_zh': '自主系统often必须从部分且噪声数据中预测附近代理的运动。本文询问并回答了以下问题：“我们能否在线学习另一个代理运动的非线性预测模型？”我们的在线框架使用修改后的滑动窗口Hankel动态模式分解（Hankel-DMD）来去噪和预测此类动力学。部分噪声测量值被嵌入Hankel矩阵中，而相关的Page矩阵使奇异值硬截断（SVHT）估算有效秩成为可能。Cadzow投影强制结构化低秩一致性，从而生成去噪轨迹和局部噪声方差估计。从这种表示出发，构建了多步预测的时变Hankel-DMD提升线性预测器。残差分析提供了方差跟踪信号，可支持下游估计器和风险意识规划。我们在具有高斯噪声和重尾噪声的仿真中验证了该方法，并在动态起重机试验台上进行了实验。结果显示，该方法实现了稳定方差意识去噪和适合集成到实时控制框架中的短时间窗预测。', 'title_zh': '实时学习预测动力学障碍模型的机器人运动规划'}
{'arxiv_id': 'arXiv:2511.00783', 'title': 'When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage', 'authors': 'Jingzehua Xu, Weihang Zhang, Yangyang Li, Hongmiaoyi Zhang, Guanwen Xie, Jiwei Tang, Shuai Zhang, Yi Li', 'link': 'https://arxiv.org/abs/2511.00783', 'abstract': 'Underwater multi-robot cooperative coverage remains challenging due to partial observability, limited communication, environmental uncertainty, and the lack of access to global localization. To address these issues, this paper presents a semantics-guided fuzzy control framework that couples Large Language Models (LLMs) with interpretable control and lightweight coordination. Raw multimodal observations are compressed by the LLM into compact, human-interpretable semantic tokens that summarize obstacles, unexplored regions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy inference system with pre-defined membership functions then maps these tokens into smooth and stable steering and gait commands, enabling reliable navigation without relying on global positioning. Then, we further coordinate multiple robots by introducing semantic communication that shares intent and local context in linguistic form, enabling agreement on who explores where while avoiding redundant revisits. Extensive simulations in unknown reef-like environments show that, under limited sensing and communication, the proposed framework achieves robust OOI-oriented navigation and cooperative coverage with improved efficiency and adaptability, narrowing the gap between semantic cognition and distributed underwater control in GPS-denied, map-free conditions.', 'abstract_zh': 'underwater 多机器人协同覆盖仍因部分可观测性、有限通信、环境不确定性以及缺少全局定位访问而具有挑战性。为解决这些问题，本文提出了一种语义引导的模糊控制框架，结合了大型语言模型（LLMs）与可解释控制和轻量级协调。原始多模态观测由LLM压缩为紧凑且人类可解释的语义标记，这些标记在不确定感知下总结障碍物、未探索区域和感兴趣对象（OOIs）。随后利用预定义的隶属函数的模糊推理系统将这些标记映射为平滑且稳定的转向和步态指令，使导航可靠而不依赖于全球定位。进一步通过引入语义通信协调多机器人，以语言形式共享意图和局部上下文，从而达成对探索区域的共识并避免重复访问。在未知礁石状环境中的 extensive 模拟表明，在有限的感知和通信条件下，所提出框架实现了以感兴趣对象为导向的稳健导航和协同覆盖，并提高了效率和适应性，缩小了在无 GPS、无地图条件下语义认知与分布式水下控制之间的差距。', 'title_zh': '当语义连接群体：由大规模语言模型驱动的模糊控制在协同水下覆盖机器人中的应用'}
{'arxiv_id': 'arXiv:2511.00635', 'title': 'Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles', 'authors': 'Hyungtae Lim, Daebeom Kim, Hyun Myung', 'link': 'https://arxiv.org/abs/2511.00635', 'abstract': 'As various 3D light detection and ranging (LiDAR) sensors have been introduced to the market, research on multi-session simultaneous localization and mapping (MSS) using heterogeneous LiDAR sensors has been actively conducted. Existing MSS methods mostly rely on loop closure detection for inter-session alignment; however, the performance of loop closure detection can be potentially degraded owing to the differences in the density and field of view (FoV) of the sensors used in different sessions. In this study, we challenge the existing paradigm that relies heavily on loop detection modules and propose a novel MSS framework, called Multi-Mapcher, that employs large-scale map-to-map registration to perform inter-session initial alignment, which is commonly assumed to be infeasible, by leveraging outlier-robust 3D point cloud registration. Next, after finding inter-session loops by radius search based on the assumption that the inter-session initial alignment is sufficiently precise, anchor node-based robust pose graph optimization is employed to build a consistent global map. As demonstrated in our experiments, our approach shows substantially better MSS performance for various LiDAR sensors used to capture the sessions and is faster than state-of-the-art approaches. Our code is available at this https URL.', 'abstract_zh': '各种3D激光雷达（LiDAR）传感器引入市场后，关于使用异构LiDAR传感器进行多会话同时定位与建图（MSS）的研究一直十分活跃。现有MSS方法大多依赖环路闭合检测进行跨会话对准，但由于不同会话中使用的传感器密度和视场（FoV）差异，环路闭合检测的性能可能会被潜在地削弱。本研究挑战了高度依赖环路检测模块的现有范式，提出了一种名为Multi-Mapcher的新MSS框架，通过利用鲁棒的3D点云对准进行跨会话初始对准，从而实现通常认为不可行的大规模地图到地图注册。接着，在初始对准充分精确的假设下通过基于半径搜索的方法找到跨会话环路，并采用锚节点基于的鲁棒位姿图优化构建一致的全局地图。实验结果表明，我们的方法对于用于捕捉会话的各种LiDAR传感器显示出显著更好的MSS性能，并且比目前最先进的方法更快。我们的代码可访问于此网址。', 'title_zh': '多地图匹配器：基于离群值鲁棒注册的自主车辆异构LiDAR多会话SLAM'}
{'arxiv_id': 'arXiv:2511.00555', 'title': 'Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy', 'authors': 'Dianye Huang, Nassir Navab, Zhongliang Jiang', 'link': 'https://arxiv.org/abs/2511.00555', 'abstract': 'Integrating generative models with action chunking has shown significant promise in imitation learning for robotic manipulation. However, the existing diffusion-based paradigm often struggles to capture strong temporal dependencies across multiple steps, particularly when incorporating proprioceptive input. This limitation can lead to task failures, where the policy overfits to proprioceptive cues at the expense of capturing the visually derived features of the task. To overcome this challenge, we propose the Deep Koopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a dual-branch architecture to decouple the roles of different sensory modality combinations. The visual branch encodes the visual observations to indicate task progression, while the fused branch integrates both visual and proprioceptive inputs for precise manipulation. Within this architecture, when the robot fails to accomplish intermediate goals, such as grasping a drawer handle, the policy can dynamically switch to execute action chunks generated by the visual branch, allowing recovery to previously observed states and facilitating retrial of the task. To further enhance visual representation learning, we incorporate a Deep Koopman Operator module that captures structured temporal dynamics from visual inputs. During inference, we use the test-time loss of the generative model as a confidence signal to guide the aggregation of the temporally overlapping predicted action chunks, thereby enhancing the reliability of policy execution. In simulation experiments across six RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion policy by an average of 14.6\\%. On three real-world robotic manipulation tasks, it achieves a 15.0\\% improvement. Code: this https URL.', 'abstract_zh': '将生成模型与动作切片结合用于机器人操作的模仿学习显示出显著潜力。然而，现有的基于扩散的过程往往难以捕捉跨多个步骤的强时间依赖关系，特别是在结合本体感受输入时。这一限制可能导致任务失败，即策略过度拟合本体感受线索，而忽略了视觉特征的捕捉。为克服这一挑战，我们提出了一种Deep Koopman-提振的双支道路扩散策略（D3P）算法。D3P引入了双支路径架构，以解耦不同感觉模态组合的角色。视觉支路编码视觉观察以指示任务进度，而融合支路则结合视觉和本体感受输入以实现精确操作。在此架构中，当机器人未能完成如抽屉把手抓取等中间目标时，策略可以动态切换到执行视觉支路生成的动作切片，从而恢复到先前观测的状态并促进任务重试。为增强视觉表示学习，我们引入了一个Deep Koopman算子模块，用于从视觉输入中捕获结构化的时间动态。在推理过程中，我们使用生成模型的测试时间损失作为置信信号来引导时间重叠预测动作切片的聚合，以增强策略执行的可靠性。在针对六项RLBench桌面任务的模拟实验中，D3P平均优于现有最先进的扩散策略14.6%。在三项真实世界的机器人操作任务中，它实现了15.0%的改进。代码：请参见链接。', 'title_zh': '通过深Koopman增强扩散策略提高 imitation learning 对离分布状态的鲁棒性'}
{'arxiv_id': 'arXiv:2511.00516', 'title': 'Adaptive and Multi-object Grasping via Deformable Origami Modules', 'authors': 'Peiyi Wang, Paul A. M. Lefeuvre, Shangwei Zou, Zhenwei Ni, Daniela Rus, Cecilia Laschi', 'link': 'https://arxiv.org/abs/2511.00516', 'abstract': 'Soft robotics gripper have shown great promise in handling fragile and geometrically complex objects. However, most existing solutions rely on bulky actuators, complex control strategies, or advanced tactile sensing to achieve stable and reliable grasping performance. In this work, we present a multi-finger hybrid gripper featuring passively deformable origami modules that generate constant force and torque output. Each finger composed of parallel origami modules is driven by a 1-DoF actuator mechanism, enabling passive shape adaptability and stable grasping force without active sensing or feedback control. More importantly, we demonstrate an interesting capability in simultaneous multi-object grasping, which allows stacked objects of varied shape and size to be picked, transported and placed independently at different states, significantly improving manipulation efficiency compared to single-object grasping. These results highlight the potential of origami-based compliant structures as scalable modules for adaptive, stable and efficient multi-object manipulation in domestic and industrial pick-and-place scenarios.', 'abstract_zh': '基于 origami 模块的多指混合吸持器在处理易碎和几何形状复杂的物体方面表现出巨大潜力。然而，现有大多数解决方案依赖于笨重的致动器、复杂的控制策略或高级触觉感知以实现稳定的抓取性能。在这项工作中，我们提出了一种多指混合吸持器，其特点是采用可被动变形的 origami 模块来产生恒定的力和扭矩输出。每个手指由多个并联的 origami 模块组成，通过1-DoF致动器机制驱动，实现被动形状适应性和稳定的抓取力，无需主动传感或反馈控制。更重要的是，我们展示了同时多物体抓取的独特能力，允许将不同形状和大小的堆叠物体独立地拾取、运输和放置，显著提高了与单一物体抓取相比的操纵效率。这些结果突显了基于 origami 的可形变结构作为可扩展模块，在家庭和工业捡取和放置场景中实现适应性、稳定性和高效多物体操纵的潜力。', 'title_zh': '基于可变形 origami 模块的自适应多目标抓取'}
{'arxiv_id': 'arXiv:2511.00512', 'title': 'Descriptive Model-based Learning and Control for Bipedal Locomotion', 'authors': 'Suraj Kumar, Andy Ruina', 'link': 'https://arxiv.org/abs/2511.00512', 'abstract': "Bipedal balance is challenging due to its multi-phase, hybrid nature and high-dimensional state space. Traditional balance control approaches for bipedal robots rely on low-dimensional models for locomotion planning and reactive control, constraining the full robot to behave like these simplified models. This involves tracking preset reference paths for the Center of Mass and upper body obtained through low-dimensional models, often resulting in inefficient walking patterns with bent knees. However, we observe that bipedal balance is inherently low-dimensional and can be effectively described with simple state and action descriptors in a low-dimensional state space. This allows the robot's motion to evolve freely in its high-dimensional state space, only constraining its projection in the low-dimensional state space. In this work, we propose a novel control approach that avoids prescribing a low-dimensional model to the full model. Instead, our control framework uses a descriptive model with the minimum degrees of freedom necessary to maintain balance, allowing the remaining degrees of freedom to evolve freely in the high-dimensional space. This results in an efficient human-like walking gait and improved robustness.", 'abstract_zh': '双足平衡控制由于其多阶段、混合性质和高维状态空间而具有挑战性。传统的双足机器人平衡控制方法依赖于低维模型来进行运动规划和反应控制，这限制了整个机器人只能像这些简化模型那样行为。这通常涉及通过低维模型获得的质心和上体的预设参考路径跟踪，导致了不高效的行走模式，膝关节弯曲。然而，我们观察到双足平衡本质上是低维的，并且可以用简单的状态和动作描述符在低维状态空间中有效地描述。这使得机器人的运动可以在高维状态空间中自由演化，仅在其在低维状态空间的投影中受到约束。在本文中，我们提出了一种新方法，避免将低维模型强加于完整模型。相反，我们的控制框架使用一个描述性模型，其中包含维持平衡所需的最小自由度，从而使其余自由度可以在高维空间中自由演化。这导致了高效的人类般的行走模式并提高了系统的鲁棒性。', 'title_zh': '基于描述性模型的学习与控制方法在 bipedal 行走中的应用'}
{'arxiv_id': 'arXiv:2511.00492', 'title': 'Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU', 'authors': 'Simon Giel, James Hurrell, Shreya Santra, Ashutosh Mishra, Kentaro Uno, Kazuya Yoshida', 'link': 'https://arxiv.org/abs/2511.00492', 'abstract': 'In-Situ Resource Utilization (ISRU) is one of the key technologies for enabling sustainable access to the Moon. The ability to excavate lunar regolith is the first step in making lunar resources accessible and usable. This work presents the development of a bucket drum for the modular robotic system MoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made of PLA was manufactured to evaluate its efficiency through a series of sandbox tests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is capable of continuous excavation at a rate of 777.54 kg/h with a normalized energy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is 172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of excavated material. The obtained results demonstrate the successful implementation of the concept. A key advantage of the developed tool is its compatibility with the modular MoonBot robotic platform, which enables flexible and efficient mission planning. Further improvements may include the integration of sensors and an autonomous control system to enhance the excavation process.', 'abstract_zh': '月球原位资源利用（ISRU）是实现可持续月球访问的关键技术之一。挖掘月壤是使月球资源可获取和利用的第一步。本研究介绍了日本月球计划项目中为模块化机器人系统月球号开发的铲斗滚筒的开发情况。通过一系列沙箱测试，评估了该3D打印的PLA原型的效率，该工具重4.8公斤，体积为14.06升，连续挖掘速率为777.54公斤/小时，单位质量能耗为0.022 Wh/公斤。在批量操作中，挖掘速率为172.02公斤/小时，单位质量能耗为0.86 Wh/公斤被挖掘的物料。获得的结果证明了概念的成功实现。该工具的一个关键优势是其与模块化月球号机器人平台的兼容性，这使得任务规划更加灵活和高效。进一步的改进可能包括集成传感器和自主控制系统以提高挖掘过程。', 'title_zh': '月球就地资源利用用模块化桶式鼓锤挖掘机的设计与开发'}
{'arxiv_id': 'arXiv:2511.00412', 'title': 'Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory', 'authors': 'John A. Christian, Michael R. Walker II, Wyatt Bridgman, Michael J. Sparapany', 'link': 'https://arxiv.org/abs/2511.00412', 'abstract': "The integration of gyroscope measurements is an essential task for most navigation systems. Modern vehicles typically use strapdown systems, such that gyro integration requires coning compensation to account for the sensor's rotation during the integration. Many coning compensation algorithms have been developed and a few are reviewed. This work introduces a new class of coning correction algorithm built directly from the classical Runge-Kutta integration routines. A simple case is shown to collapse to one of the most popular coning algorithms and a clear procedure for generating higher-order algorithms is presented.", 'abstract_zh': '基于经典Runge-Kutta积分方法的新型俯仰修正算法的研究', 'title_zh': 'Runge-Kutta 近似在李群理论应用于直接会流修正中的应用'}
{'arxiv_id': 'arXiv:2511.00392', 'title': 'SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping', 'authors': 'Lingpeng Chen, Jiakun Tang, Apple Pui-Yi Chui, Ziyang Hong, Junfeng Wu', 'link': 'https://arxiv.org/abs/2511.00392', 'abstract': 'Accurate 3D reconstruction in visually-degraded underwater environments remains a formidable challenge. Single-modality approaches are insufficient: vision-based methods fail due to poor visibility and geometric constraints, while sonar is crippled by inherent elevation ambiguity and low resolution. Consequently, prior fusion technique relies on heuristics and flawed geometric assumptions, leading to significant artifacts and an inability to model complex scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep learning framework that overcomes these limitations by adapting the principled plane sweep algorithm for cross-modal fusion between sonar and visual data. Extensive experiments in both high-fidelity simulation and real-world environments demonstrate that SonarSweep consistently generates dense and accurate depth maps, significantly outperforming state-of-the-art methods across challenging conditions, particularly in high turbidity. To foster further research, we will publicly release our code and a novel dataset featuring synchronized stereo-camera and sonar data, the first of its kind.', 'abstract_zh': '精确的无视觉降级水下环境三维重建仍然是一个严峻的挑战。单一模态方法不足：基于视觉的方法由于能见度差和几何约束而失效，而声纳因固有的视高模糊和低分辨率而受限。因此，先前的融合技术依赖于启发式方法和不准确的几何假设，导致显著的伪影且无法建模复杂场景。在本文中，我们提出了SonarSweep，一种新颖的端到端深度学习框架，通过将原理上的平面扫描算法适应应用于声纳和视觉数据之间的跨模态融合，从而克服了这些限制。在高保真仿真和真实环境中的广泛实验表明，SonarSweep 一致生成密集且准确的深度图，在多种条件下尤其是高浑浊度情况下显著优于现有最佳方法。为了促进进一步研究，我们将公开发布我们的代码和一个新的数据集，该数据集包含同步立体相机和声纳数据，是此类数据集中的第一个。', 'title_zh': 'SonarSweep：融合声纳与视觉实现稳健的平面扫描三维重建'}
{'arxiv_id': 'arXiv:2511.00306', 'title': 'FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications', 'authors': 'Baoshan Song, Ruijie Xu, Li-Ta Hsu', 'link': 'https://arxiv.org/abs/2511.00306', 'abstract': 'Sliding window-factor graph optimization (SW-FGO) has gained more and more attention in navigation research due to its robust approximation to non-Gaussian noises and nonlinearity of measuring models. There are lots of works focusing on its application performance compared to extended Kalman filter (EKF) but there is still a myth at the theoretical relationship between the SW-FGO and EKF. In this paper, we find the necessarily fair condition to connect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF (IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the conditions, we propose a recursive FGO (Re-FGO) framework to represent KFV under SW-FGO formulation. Under explicit conditions (Markov assumption, Gaussian noise with L2 loss, and a one-state window), Re-FGO regenerates exactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in nonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after clarifying the connection between them, we highlight the unique advantages of SW-FGO in practical phases, especially on numerical estimation and deep learning integration. The code and data used in this work is open sourced at this https URL.', 'abstract_zh': '滑动窗口-因子图优化（SW-FGO）因其对非高斯噪声和测量模型非线性的稳健近似而在导航研究中逐渐获得关注。尽管有许多工作关注其应用性能与扩展卡尔曼滤波器（EKF）的对比，但SW-FGO与EKF之间的理论关系仍然存在争议。本文找到了将SW-FGO与卡尔曼滤波器变体（KFV，如EKF、迭代EKF（IEKF）、稳健EKF（REKF）和稳健迭代EKF（RIEKF））相连接的必要公平条件。基于这些条件，我们提出了一种递归因子图优化（Re-FGO）框架，以SW-FGO形式表示KFV。在显式条件下（马尔可夫假设、L2损失的高斯噪声和单状态窗口），Re-FGO可以再生为EKF/IEKF/REKF/RIEKF，而SW-FGO则在非线性和非高斯环境中表现出可预测计算成本的优势。最后，在明确了两者之间的关系后，我们强调了SW-FGO在实际应用中的独特优势，特别是在数值估计和深度学习整合方面。本文使用的代码和数据已开源于此网址：此 https URL。', 'title_zh': 'FGO神话破除：解释卡尔曼滤波器变体在导航应用中如何达到与FGO相同性能的方法'}
{'arxiv_id': 'arXiv:2511.00259', 'title': 'Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial', 'authors': 'Andria J. Farrens, Luis Garcia-Fernandez, Raymond Diaz Rojas, Jillian Obeso Estrada, Dylan Reinsdorf, Vicky Chan, Disha Gupta, Joel Perry, Eric Wolbrecht, An Do, Steven C. Cramer, David J. Reinkensmeyer', 'link': 'https://arxiv.org/abs/2511.00259', 'abstract': 'Precision rehabilitation aims to tailor movement training to improve outcomes. We tested whether proprioceptively-tailored robotic training improves hand function and neural processing in stroke survivors. Using a robotic finger exoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel Training, which uses robot-facilitated, gamified movements to enhance proprioceptive processing, and Virtual Assistance Training, which reduces robotic aid to increase reliance on self-generated feedback. In a randomized controlled trial, forty-six chronic stroke survivors completed nine 2-hour sessions of Standard, Propriopixel or Virtual training. Among participants with proprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002) and Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand function (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with improvements in hand function. Tailored training enhanced neural sensitivity to proprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive Contingent Negative Variation. These findings support proprioceptively-tailored training as a pathway to precision neurorehabilitation.', 'abstract_zh': '精确康复旨在定制运动训练以改善康复效果。我们测试了本体感受定制的机器人训练是否能改善中风幸存者的手功能和神经处理。使用机器人手指外骨骼，我们测试了两种本体感受定制的方法：Propriopixel训练，该方法利用机器人辅助的游戏化运动来增强本体感受处理；以及虚拟辅助训练，该方法减少机器人辅助以增加对自主反馈的依赖。在随机对照试验中，46例慢性中风幸存者完成了九次2小时的标准化、Propriopixel或虚拟辅助训练。在具有本体感受缺陷的参与者中，Propriopixel（箱和块测试：7 ± 4.2，p=0.002）和虚拟辅助训练（4.5 ± 4.4，p=0.068）在手功能方面表现出更大的改善，而标准化训练（0.8 ± 2.3块）则没有明显改善。本体感受的提高与手功能的改善相关。定制化的训练增强了对本体感受提示的神经敏感性，这通过一种新的EEG生物标志物，即本体感受相关的负变jab（Contingent Negative Variation）得以体现。这些发现支持本体感受定制的训练作为精准神经康复的途径。', 'title_zh': '针对本体感觉缺陷的中风幸存者定制化机器人训练改善手功能和本体感觉处理：一项随机对照试验'}
{'arxiv_id': 'arXiv:2511.00193', 'title': 'Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach', 'authors': 'Faranak Akbarifar, Nooshin Maghsoodi, Sean P Dukelow, Stephen Scott, Parvin Mousavi', 'link': 'https://arxiv.org/abs/2511.00193', 'abstract': 'Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive kinematic biomarkers but requires 40-64 reaches, imposing time and fatigue burdens. We evaluate whether time-series foundation models can replace unrecorded trials from an early subset of reaches while preserving the reliability of standard Kinarm parameters.\nMethods: We analyzed VGR speed signals from 461 stroke and 599 control participants across 4- and 8-target reaching protocols. We withheld all but the first 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models, fine-tuned on 70 percent of subjects, to forecast synthetic trials. We recomputed four kinematic features of reaching (reaction time, movement time, posture speed, maximum speed) on combined recorded plus forecasted trials and compared them to full-length references using ICC(2,1).\nResults: Chronos forecasts restored ICC >= 0.90 for all parameters with only 8 recorded trials plus forecasts, matching the reliability of 24-28 recorded reaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA improvements were minimal. Across cohorts and protocols, synthetic trials replaced reaches without materially compromising feature reliability.\nConclusion: Foundation-model forecasting can greatly shorten Kinarm VGR assessment time. For the most impaired stroke survivors, sessions drop from 4-5 minutes to about 1 minute while preserving kinematic precision. This forecast-augmented paradigm promises efficient robotic evaluations for assessing motor impairments following stroke.', 'abstract_zh': '目的: 使用Kinarm机器人进行视觉引导抓取（VGR）测试可以获取敏感的动力学生物标记，但需要40-64次抓取，这会带来时间上的负担和疲劳。我们评估时间序列基础模型是否可以在保留标准Kinarm参数可靠性的前提下，替代早期抓取子集中的未记录试验。', 'title_zh': '减少上肢机器人评估时间的同时保持精度：一种时间序列基础模型方法'}
{'arxiv_id': 'arXiv:2511.00153', 'title': 'EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations', 'authors': 'Justin Yu, Yide Shentu, Di Wu, Pieter Abbeel, Ken Goldberg, Philipp Wu', 'link': 'https://arxiv.org/abs/2511.00153', 'abstract': 'Imitation learning from human demonstrations offers a promising approach for robot skill acquisition, but egocentric human data introduces fundamental challenges due to the embodiment gap. During manipulation, humans actively coordinate head and hand movements, continuously reposition their viewpoint and use pre-action visual fixation search strategies to locate relevant objects. These behaviors create dynamic, task-driven head motions that static robot sensing systems cannot replicate, leading to a significant distribution shift that degrades policy performance. We present EgoMI (Egocentric Manipulation Interface), a framework that captures synchronized end-effector and active head trajectories during manipulation tasks, resulting in data that can be retargeted to compatible semi-humanoid robot embodiments. To handle rapid and wide-spanning head viewpoint changes, we introduce a memory-augmented policy that selectively incorporates historical observations. We evaluate our approach on a bimanual robot equipped with an actuated camera head and find that policies with explicit head-motion modeling consistently outperform baseline methods. Results suggest that coordinated hand-eye learning with EgoMI effectively bridges the human-robot embodiment gap for robust imitation learning on semi-humanoid embodiments. Project page: this https URL', 'abstract_zh': '基于人类演示的模仿学习为机器人技能获取提供了有前景的方法，但第一人称人类数据由于存在体感差距引入了根本性的挑战。在操作过程中，人类会主动协调头部和手部动作，持续重新定位视角，并在动作前使用视觉固定搜索策略来定位相关物体。这些行为产生了由任务驱动的动态头部运动，而静态的机器人感知系统无法复制，导致数据分布转移，从而降低了策略性能。我们提出了EgoMI（第一人称操纵界面）框架，该框架在操作任务中捕捉同步的末端执行器和主动头部轨迹，生成可重新定向到兼容的半类人机器人体感的数据。为了处理快速且广泛的头部视角变化，我们引入了一个增强记忆的策略，该策略选择性地结合了历史观测。我们评估了该方法在配备可动作摄像头头部的双臂机器人上的效果，发现具有明确头部运动建模的策略始终优于基线方法。结果表明，通过EgoMI进行协调的手眼学习有效缩小了人类与机器人之间的体感差距，提高了半类人机器人上稳健的模仿学习。项目页面：this https URL', 'title_zh': 'EgoMI: 从第一人称人类示范中学习主动视觉和全身 manipulation'}
{'arxiv_id': 'arXiv:2511.00139', 'title': 'End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection', 'authors': 'Yu Cui, Yujian Zhang, Lina Tao, Yang Li, Xinyu Yi, Zhibin Li', 'link': 'https://arxiv.org/abs/2511.00139', 'abstract': "Achieving human-like dexterous manipulation remains a major challenge for general-purpose robots. While Vision-Language-Action (VLA) models show potential in learning skills from demonstrations, their scalability is limited by scarce high-quality training data. Existing data collection methods face inherent constraints: manual teleoperation overloads human operators, while automated planning often produces unnatural motions. We propose a Shared Autonomy framework that divides control between macro and micro motions. A human operator guides the robot's arm pose through intuitive VR teleoperation, while an autonomous DexGrasp-VLA policy handles fine-grained hand control using real-time tactile and visual feedback. This division significantly reduces cognitive load and enables efficient collection of high-quality coordinated arm-hand demonstrations. Using this data, we train an end-to-end VLA policy enhanced with our novel Arm-Hand Feature Enhancement module, which captures both distinct and shared representations of macro and micro movements for more natural coordination. Our Corrective Teleoperation system enables continuous policy improvement through human-in-the-loop failure recovery. Experiments demonstrate that our framework generates high-quality data with minimal manpower and achieves a 90% success rate across diverse objects, including unseen instances. Comprehensive evaluations validate the system's effectiveness in developing dexterous manipulation capabilities.", 'abstract_zh': '实现类人的灵巧 manipulation 仍然是通用机器人面临的主要挑战。视觉-语言-动作 (VLA) 模型在从示范中学习技能方面显示出潜力，但其可扩展性受限于稀缺的高质量训练数据。现有数据收集方法存在固有的限制：手动遥操作会压垮人类操作员，而自动化规划通常会产生不自然的运动。我们提出了一种共享自治框架，将控制权分配给宏观和微观动作。人类操作员通过直观的 VR 遥操作引导机器人臂姿，而一个自主的 DexGrasp-VLA 策略利用实时触觉和视觉反馈处理精细的手部控制。这种分工显著减轻了认知负担，并使高效收集高质量协调的臂-手示范成为可能。使用这些数据，我们训练了一个端到端的 VLA 策略，并增强了我们提出的新型 Arm-Hand 特征增强模块，该模块捕获宏微观运动的独特和共享表示，以实现更自然的协调。我们的校正遥操作系统通过人类在环中的人工智能辅助故障恢复，持续改进策略。实验表明，该框架以最少的人力生成高质量数据，并在各种物体上实现了 90% 的成功率，包括未见过的实例。全面评估验证了该系统的有效性，用于开发灵巧的 manipulation 能力。', 'title_zh': '基于共享自主性的端到端灵巧臂手VLA策略：自主手部VLA策略增强的VR远程操作，用于高效数据收集'}
{'arxiv_id': 'arXiv:2511.00112', 'title': 'Real-DRL: Teach and Learn in Reality', 'authors': 'Yanbing Mao, Yihao Cai, Lui Sha', 'link': 'https://arxiv.org/abs/2511.00112', 'abstract': "This paper introduces the Real-DRL framework for safety-critical autonomous systems, enabling runtime learning of a deep reinforcement learning (DRL) agent to develop safe and high-performance action policies in real plants (i.e., real physical systems to be controlled), while prioritizing safety! The Real-DRL consists of three interactive components: a DRL-Student, a PHY-Teacher, and a Trigger. The DRL-Student is a DRL agent that innovates in the dual self-learning and teaching-to-learn paradigm and the real-time safety-informed batch sampling. On the other hand, PHY-Teacher is a physics-model-based design of action policies that focuses solely on safety-critical functions. PHY-Teacher is novel in its real-time patch for two key missions: i) fostering the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of real plants. The Trigger manages the interaction between the DRL-Student and the PHY-Teacher. Powered by the three interactive components, the Real-DRL can effectively address safety challenges that arise from the unknown unknowns and the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety, ii) automatic hierarchy learning (i.e., safety-first learning and then high-performance learning), and iii) safety-informed batch sampling to address the learning experience imbalance caused by corner cases. Experiments with a real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole system, along with comparisons and ablation studies, demonstrate the Real-DRL's effectiveness and unique features.", 'abstract_zh': '基于现实的深度强化学习（Real-DRL）框架：用于安全至上的自主系统运行时学习', 'title_zh': 'Real-DRL: 在现实中教与学'}
{'arxiv_id': 'arXiv:2511.00094', 'title': 'Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments', 'authors': 'Angelos Alexopoulos, Agorakis Bompotas, Nikitas Rigas Kalogeropoulos, Panagiotis Kechagias, Athanasios P. Kalogeras, Christos Alexakos', 'link': 'https://arxiv.org/abs/2511.00094', 'abstract': "Robotic systems have become integral to smart environments, enabling applications ranging from urban surveillance and automated agriculture to industrial automation. However, their effective operation in dynamic settings - such as smart cities and precision farming - is challenged by continuously evolving topographies and environmental conditions. Traditional control systems often struggle to adapt quickly, leading to inefficiencies or operational failures. To address this limitation, we propose a novel framework for autonomous and dynamic reconfiguration of robotic controllers using Digital Twin technology. Our approach leverages a virtual replica of the robot's operational environment to simulate and optimize movement trajectories in response to real-world changes. By recalculating paths and control parameters in the Digital Twin and deploying the updated code to the physical robot, our method ensures rapid and reliable adaptation without manual intervention. This work advances the integration of Digital Twins in robotics, offering a scalable solution for enhancing autonomy in smart, dynamic environments.", 'abstract_zh': '基于数字孪生的自主动态重配置机器人控制器框架：增强智能动态环境下的自主性', 'title_zh': '基于数字孪生的智能环境中机器人系统自动重构方法'}
{'arxiv_id': 'arXiv:2511.00088', 'title': 'Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail', 'authors': 'NVIDIA, Yan Wang, Wenjie Luo, Junjie Bai, Yulong Cao, Tong Che, Ke Chen, Yuxiao Chen, Jenna Diamond, Yifan Ding, Wenhao Ding, Liang Feng, Greg Heinrich, Jack Huang, Peter Karkus, Boyi Li, Pinyi Li, Tsung-Yi Lin, Dongran Liu, Ming-Yu Liu, Langechuan Liu, Zhijian Liu, Jason Lu, Yunxiang Mao, Pavlo Molchanov, Lindsey Pavao, Zhenghao Peng, Mike Ranzinger, Ed Schmerling, Shida Shen, Yunfei Shi, Sarah Tariq, Ran Tian, Tilman Wekel, Xinshuo Weng, Tianjun Xiao, Eric Yang, Xiaodong Yang, Yurong You, Xiaohui Zeng, Wenyuan Zhang, Boris Ivanovic, Marco Pavone', 'link': 'https://arxiv.org/abs/2511.00088', 'abstract': 'End-to-end architectures trained via imitation learning have advanced autonomous driving by scaling model size and data, yet performance remains brittle in safety-critical long-tail scenarios where supervision is sparse and causal understanding is limited. To address this, we introduce Alpamayo-R1 (AR1), a vision-language-action model (VLA) that integrates Chain of Causation reasoning with trajectory planning to enhance decision-making in complex driving scenarios. Our approach features three key innovations: (1) the Chain of Causation (CoC) dataset, built through a hybrid auto-labeling and human-in-the-loop pipeline producing decision-grounded, causally linked reasoning traces aligned with driving behaviors; (2) a modular VLA architecture combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI applications, with a diffusion-based trajectory decoder that generates dynamically feasible plans in real time; (3) a multi-stage training strategy using supervised fine-tuning to elicit reasoning and reinforcement learning (RL) to optimize reasoning quality via large reasoning model feedback and enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12% improvement in planning accuracy on challenging cases compared to a trajectory-only baseline, with a 35% reduction in off-road rate and 25% reduction in close encounter rate in closed-loop simulation. RL post-training improves reasoning quality by 45% as measured by a large reasoning model critic and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B parameters shows consistent improvements. On-vehicle road tests confirm real-time performance (99 ms latency) and successful urban deployment. By bridging interpretable reasoning with precise control, AR1 demonstrates a practical path towards Level 4 autonomous driving. We plan to release AR1 models and a subset of the CoC in a future update.', 'abstract_zh': '基于模仿学习训练的端到端架构通过扩展模型规模和数据推动了自动驾驶的进步，但在监督稀疏且因果理解有限的安全关键长尾场景中性能依然脆弱。为应对这一挑战，我们引入了Alpamayo-R1（AR1），一种结合因果推理与轨迹规划的视觉-语言-行动模型（VLA），以增强复杂驾驶场景中的决策能力。我们的方法包含三个关键创新点：（1）因果推理数据集（CoC），通过混合自动标注和人工在环管道构建，生成与驾驶行为相关的因果推理踪迹；（2）模块化的VLA架构，结合用于物理AI应用预训练的Cosmos-Reason视觉-语言模型和基于扩散的轨迹解码器，实时生成动态可行的计划；（3）多阶段训练策略，使用监督微调激发推理能力，并通过大规模推理模型反馈和推理-行动一致性强化学习优化推理质量。评估结果显示，AR1在复杂案例中规划准确性相比仅轨迹基线提高了12%，封闭环仿真中离路率降低了35%，近距离会车率降低了25%。训练后使用强化学习改进了45%的推理质量，37%的推理-行动一致性，并且在模型参数从0.5B扩展到7B的过程中保持了持续改进。车载道路测试验证了AR1的实时性能（99 ms延迟）及其在城市环境中的成功部署。通过将可解释推理与精确控制相结合，AR1展示了向L4级自动驾驶实用路径的可能。我们计划在未来更新中发布AR1模型和CoC数据集的一部分。', 'title_zh': 'Alpamayo-R1: 跨越推理与动作预测的鸿沟，实现长尾场景下的可迁移自动驾驶'}
{'arxiv_id': 'arXiv:2511.00041', 'title': 'Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World', 'authors': 'Yingzhao Jian, Zhongan Wang, Yi Yang, Hehe Fan', 'link': 'https://arxiv.org/abs/2511.00041', 'abstract': "Humanoid agents often struggle to handle flexible and diverse interactions in open environments. A common solution is to collect massive datasets to train a highly capable model, but this approach can be prohibitively expensive. In this paper, we explore an alternative solution: empowering off-the-shelf Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents, thereby leveraging their strong open-world generalization to mitigate the need for extensive data collection. To this end, we present \\textbf{BiBo} (\\textbf{B}uilding humano\\textbf{I}d agent \\textbf{B}y \\textbf{O}ff-the-shelf VLMs). It consists of two key components: (1) an \\textbf{embodied instruction compiler}, which enables the VLM to perceive the environment and precisely translate high-level user instructions (e.g., {\\small\\itshape ``have a rest''}) into low-level primitive commands with control parameters (e.g., {\\small\\itshape ``sit casually, location: (1, 2), facing: 90$^\\circ$''}); and (2) a diffusion-based \\textbf{motion executor}, which generates human-like motions from these commands, while dynamically adapting to physical feedback from the environment. In this way, BiBo is capable of handling not only basic interactions but also diverse and complex motions. Experiments demonstrate that BiBo achieves an interaction task success rate of 90.2\\% in open environments, and improves the precision of text-guided motion execution by 16.3\\% over prior methods. The code will be made publicly available.", 'abstract_zh': '基于现成视觉语言模型的 humanoid 剧情人物构建（BiBo）', 'title_zh': '赋予GPT-4人形身体：构建预制VLMs与物理世界之间的桥梁'}
{'arxiv_id': 'arXiv:2511.00033', 'title': 'STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization', 'authors': 'Diqi He, Xuehao Gao, Hao Li, Junwei Han, Dingwen Zhang', 'link': 'https://arxiv.org/abs/2511.00033', 'abstract': "The Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) task requires agents to navigate previously unseen 3D environments using natural language instructions, without any scene-specific training. A critical challenge in this setting lies in ensuring agents' actions align with both spatial structure and task intent over long-horizon execution. Existing methods often fail to achieve robust navigation due to a lack of structured decision-making and insufficient integration of feedback from previous actions. To address these challenges, we propose STRIDER (Instruction-Aligned Structural Decision Space Optimization), a novel framework that systematically optimizes the agent's decision space by integrating spatial layout priors and dynamic task feedback. Our approach introduces two key innovations: 1) a Structured Waypoint Generator that constrains the action space through spatial structure, and 2) a Task-Alignment Regulator that adjusts behavior based on task progress, ensuring semantic alignment throughout navigation. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms strong SOTA across key metrics; in particular, it improves Success Rate (SR) from 29% to 35%, a relative gain of 20.7%. Such results highlight the importance of spatially constrained decision-making and feedback-guided execution in improving navigation fidelity for zero-shot VLN-CE.", 'abstract_zh': '零样本连续环境中的跨模态导航（VLN-CE）任务要求代理使用自然语言指令在未见过的3D环境中导航，无需任何特定场景的训练。在这种设置中，关键挑战在于确保代理的行动在长时序执行中与空间结构和任务意图保持一致。现有方法往往由于缺乏结构化决策和之前行动反馈的不足而无法实现稳健的导航。为解决这些挑战，我们提出了一种名为STRIDER（指令对齐的结构决策空间优化）的新框架，通过整合空间布局先验和动态任务反馈系统地优化代理的决策空间。我们的方法引入了两项关键创新：1）结构化航点生成器，通过空间结构约束行动空间；2）任务对齐调节器，根据任务进展调整行为，确保导航过程中语义一致性。在R2R-CE和RxR-CE基准上的 extensive 实验表明，STRIDER 在关键指标上显著优于强 SOTA 方法；特别是，成功率为（SR）从 29% 提高到 35%，相对增幅为 20.7%。这些结果突显了在零样本 VLN-CE 中通过空间约束决策和反馈引导执行提高导航准确性的重要性。', 'title_zh': 'STRIDER：基于指令对齐结构决策空间优化的导航'}
{'arxiv_id': 'arXiv:2511.00026', 'title': 'Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience', 'authors': 'Chaitanya Shinde, Divya Garikapati', 'link': 'https://arxiv.org/abs/2511.00026', 'abstract': 'Generative Artificial Intelligence is emerging as a transformative force in the automotive industry, enabling novel applications across vehicle design, manufacturing, autonomous driving, predictive maintenance, and in vehicle user experience. This paper provides a comprehensive review of the current state of GenAI in automotive, highlighting enabling technologies such as Generative Adversarial Networks and Variational Autoencoders. Key opportunities include accelerating autonomous driving validation through synthetic data generation, optimizing component design, and enhancing human machine interaction via personalized and adaptive interfaces. At the same time, the paper identifies significant technical, ethical, and safety challenges, including computational demands, bias, intellectual property concerns, and adversarial robustness, that must be addressed for responsible deployment. A case study on Mercedes Benzs MBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more natural, proactive, and personalized in car interactions compared to legacy rule based assistants. Through this review and case study, the paper outlines both the promise and limitations of GenAI integration in the automotive sector and presents directions for future research and development aimed at achieving safer, more efficient, and user centric mobility. Unlike prior reviews that focus solely on perception or manufacturing, this paper emphasizes generative AI in voice based HMI, bridging safety and user experience perspectives.', 'abstract_zh': '生成式人工智能正成为推动汽车行业的变革力量，使其能够在车辆设计、制造、自动驾驶、预测性维护和车内用户体验等方面实现新颖的应用。本文对当前生成式人工智能在汽车领域的研究进行全面综述，强调生成对抗网络和变分自动编码器等使能技术。关键机会包括通过合成数据生成加速自动驾驶验证、优化组件设计以及通过个性化和自适应界面提升人机交互。同时，本文指出了负责任部署过程中的重大技术、伦理和安全挑战，包括计算需求、偏见、知识产权问题以及对抗性鲁棒性。梅赛德斯奔驰MBUX虚拟助手的案例研究表明，生成式人工智能驱动的语音系统与基于规则的遗留系统相比，能够实现更自然、主动和个性化的车内交互。通过这一综述和案例研究，本文概述了生成式人工智能在汽车领域的前景与限制，并指出了旨在实现更安全、更高效和用户中心化移动性的未来研究和发展方向。与此前仅关注感知或制造的综述不同，本文强调了基于语音的人机交互中的生成AI，结合了安全性和用户体验的视角。', 'title_zh': '汽车领域的生成型AI：应用、挑战与机遇——以车内体验为例'}
{'arxiv_id': 'arXiv:2511.01795', 'title': 'Fractional Diffusion Bridge Models', 'authors': 'Gabriel Nobis, Maximilian Springenberg, Arina Belova, Rembert Daems, Christoph Knochenhauer, Manfred Opper, Tolga Birdal, Wojciech Samek', 'link': 'https://arxiv.org/abs/2511.01795', 'abstract': 'We present Fractional Diffusion Bridge Models (FDBM), a novel generative diffusion bridge framework driven by an approximation of the rich and non-Markovian fractional Brownian motion (fBM). Real stochastic processes exhibit a degree of memory effects (correlations in time), long-range dependencies, roughness and anomalous diffusion phenomena that are not captured in standard diffusion or bridge modeling due to the use of Brownian motion (BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM), we construct FDBM that enable tractable inference while preserving the non-Markovian nature of fBM. We prove the existence of a coupling-preserving generative diffusion bridge and leverage it for future state prediction from paired training data. We then extend our formulation to the Schrödinger bridge problem and derive a principled loss function to learn the unpaired data translation. We evaluate FDBM on both tasks: predicting future protein conformations from aligned data, and unpaired image translation. In both settings, FDBM achieves superior performance compared to the Brownian baselines, yielding lower root mean squared deviation (RMSD) of C$_\\alpha$ atomic positions in protein structure prediction and lower Fréchet Inception Distance (FID) in unpaired image translation.', 'abstract_zh': '我们提出了分数扩散桥梁模型（FDBM），这是一种基于丰富且非马尔可夫分数布朗运动（fBM）近似的新颖生成扩散桥梁框架。', 'title_zh': '分数 diffusion 桥模型'}
{'arxiv_id': 'arXiv:2511.01755', 'title': '3EED: Ground Everything Everywhere in 3D', 'authors': 'Rong Li, Yuhao Dong, Tianshuai Hu, Ao Liang, Youquan Liu, Dongyue Lu, Liang Pan, Lingdong Kong, Junwei Liang, Ziwei Liu', 'link': 'https://arxiv.org/abs/2511.01755', 'abstract': 'Visual grounding in 3D is the key for embodied agents to localize language-referred objects in open-world environments. However, existing benchmarks are limited to indoor focus, single-platform constraints, and small scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We provide over 128,000 objects and 22,000 validated referring expressions across diverse outdoor scenes -- 10x larger than existing datasets. We develop a scalable annotation pipeline combining vision-language model prompting with human verification to ensure high-quality spatial grounding. To support cross-platform learning, we propose platform-aware normalization and cross-modal alignment techniques, and establish benchmark protocols for in-domain and cross-platform evaluations. Our findings reveal significant performance gaps, highlighting the challenges and opportunities of generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released to advance future research in language-driven 3D embodied perception.', 'abstract_zh': '3EED：多平台多模态3D语义接地基准istiketi，在开放世界环境中，3D视觉定位是使体现代理能够定位语言所指对象的关键。然而，现有的基准测试主要局限于室内场景、单一平台约束和小规模数据。我们引入了3EED，这是一个多平台、多模态的3D语义接地基准测试，包含来自车辆、无人机和四足机器人平台的RGB和LiDAR数据。我们提供了超过128,000个对象和22,000个验证的引用表达式，涵盖多种户外场景——规模比现有数据集大10倍。我们开发了一种可扩展的注释流程，结合视觉-语言模型提示与人工验证，以确保高质量的空间定位。为支持跨平台学习，我们提出了平台感知规范化和跨模态对齐技术，并建立了域内和跨平台评估基准。我们的研究结果揭示了显著的性能差距，突显了通用3D语义接地的挑战与机遇。3EED数据集和基准工具包的发布旨在推动未来由语言驱动的3D体现感知研究。', 'title_zh': '3EED: 在三维中处处ground一切'}
{'arxiv_id': 'arXiv:2511.01571', 'title': 'PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model', 'authors': 'Wenqi Liang, Gan Sun, Yao He, Jiahua Dong, Suyan Dai, Ivan Laptev, Salman Khan, Yang Cong', 'link': 'https://arxiv.org/abs/2511.01571', 'abstract': 'Vision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image-text-action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. To address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data. Experiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA, while requiring only 1.5% of its pretraining cost. These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.', 'abstract_zh': '基于像素级别的Vision-Language-Action模型（PixelVLA）：支持多模态提示的视觉-语言-动作模型', 'title_zh': 'PixelVLA：提升视觉-语言-行动模型的像素级理解'}
{'arxiv_id': 'arXiv:2511.01502', 'title': 'Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning', 'authors': 'Mengtan Zhang, Zizhan Guo, Hongbo Zhao, Yi Feng, Zuyi Xiong, Yue Wang, Shaoyi Du, Hanli Wang, Rui Fan', 'link': 'https://arxiv.org/abs/2511.01502', 'abstract': 'Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at this http URL upon publication.', 'abstract_zh': '无监督学习深度和 ego 运动这两种基本的三维感知任务近年来取得了显著进展。然而，大多数方法将 ego 运动视为辅助任务，要么混合所有运动类型，要么在监督中排除深度无关的旋转运动。这些设计限制了强几何限制的整合，降低了在多种条件下的可靠性和鲁棒性。本研究引入了运动分量的辨别处理，利用各自刚性流的几何规律，以利于深度和 ego 运动的估计。给定连续的视频帧，网络输出首先对准源和目标相机的光学轴和成像平面。通过这些对准，帧之间的光学流被转换，并量化偏差以在每个 ego 运动分量上施加单独的几何约束，实现更有针对性的细化。这些对准时进一步将联合学习过程重新格式化为共轴和共面形式，其中深度和每个平移分量可以通过闭形式的几何关系互推，引入互补约束以提高深度鲁棒性。整合这些设计的 DiMoDE 通用深度和 ego 运动联合学习框架在多个公开数据集和一个新收集的多样化真实世界数据集上实现了最先进的性能，尤其是在具有挑战性的条件下。我们的源代码将在发表后在此网址公开。', 'title_zh': '区别性处理运动组件演化联合深度和自我运动学习'}
{'arxiv_id': 'arXiv:2511.01501', 'title': 'SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation', 'authors': 'Yufeng Jin, Niklas Funk, Vignesh Prasad, Zechu Li, Mathias Franzius, Jan Peters, Georgia Chalvatzaki', 'link': 'https://arxiv.org/abs/2511.01501', 'abstract': 'Object pose estimation is a fundamental problem in robotics and computer vision, yet it remains challenging due to partial observability, occlusions, and object symmetries, which inevitably lead to pose ambiguity and multiple hypotheses consistent with the same observation. While deterministic deep networks achieve impressive performance under well-constrained conditions, they are often overconfident and fail to capture the multi-modality of the underlying pose distribution. To address these challenges, we propose a novel probabilistic framework that leverages flow matching on the SE(3) manifold for estimating 6D object pose distributions. Unlike existing methods that regress a single deterministic output, our approach models the full pose distribution with a sample-based estimate and enables reasoning about uncertainty in ambiguous cases such as symmetric objects or severe occlusions. We achieve state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our sample-based pose estimates can be leveraged in downstream robotic manipulation tasks such as active perception for disambiguating uncertain viewpoints or guiding grasp synthesis in an uncertainty-aware manner.', 'abstract_zh': '6D物体姿态估计是机器人技术和计算机视觉中的一个基础问题，但由于部分观测性、遮挡和物体对称性等原因，依然具有挑战性，这不可避免地导致姿态模糊性和与同一次观测一致的多个假设。尽管确定性深度网络在条件良好时能够取得令人印象深刻的成绩，但它们通常过于自信，无法捕捉潜在姿态分布的多模态性。为了解决这些问题，我们提出了一种新的概率框架，该框架利用SE(3)流形上的流匹配来估计6D物体姿态分布。与现有方法回归单一确定性输出不同，我们的方法以样本估计的方式建模整个姿态分布，并能够在对称物体或严重遮挡等模棱两可的情况下进行不确定性推理。我们在Real275、YCB-V和LM-O数据集上取得了最新的成果，并展示了如何利用我们的样本估计姿态在下游的机器人操作任务中，如具有不确定性意识的主动感知以消解模糊视角或在不确定性驱动下指导抓取合成。', 'title_zh': 'SE(3)-PoseFlow：估计6D姿态分布以实现不确定性感知的机器人操作'}
{'arxiv_id': 'arXiv:2511.01381', 'title': 'EREBUS: End-to-end Robust Event Based Underwater Simulation', 'authors': 'Hitesh Kyatham, Arjun Suresh, Aadi Palnitkar, Yiannis Aloimonos', 'link': 'https://arxiv.org/abs/2511.01381', 'abstract': 'The underwater domain presents a vast array of challenges for roboticists and computer vision researchers alike, such as poor lighting conditions and high dynamic range scenes. In these adverse conditions, traditional vision techniques struggle to adapt and lead to suboptimal performance. Event-based cameras present an attractive solution to this problem, mitigating the issues of traditional cameras by tracking changes in the footage on a frame-by-frame basis. In this paper, we introduce a pipeline which can be used to generate realistic synthetic data of an event-based camera mounted to an AUV (Autonomous Underwater Vehicle) in an underwater environment for training vision models. We demonstrate the effectiveness of our pipeline using the task of rock detection with poor visibility and suspended particulate matter, but the approach can be generalized to other underwater tasks.', 'abstract_zh': '水下环境为机器人学家和计算机视觉研究人员带来了诸多挑战，如光线条件差和高动态范围场景。在这些不利条件下，传统的视觉技术难以适应并导致性能不佳。基于事件的相机通过逐帧跟踪视频中的变化提供了一种有吸引力的解决方案，从而缓解了传统相机的问题。本文介绍了一种管道，可用于生成安装在自主水下车辆(AUV)上的基于事件的相机在水下环境中具有真实感的合成数据，以训练视觉模型。我们使用贫视域和悬浮颗粒物情况下的岩石检测任务来证明我们管道的有效性，但该方法可以 generalized 至其他水下任务。', 'title_zh': 'EREBUS:端到端鲁棒基于事件的水下模拟'}
{'arxiv_id': 'arXiv:2511.01283', 'title': 'Lyapunov Stability Learning with Nonlinear Control via Inductive Biases', 'authors': 'Yupu Lu, Shijie Lin, Hao Xu, Zeqing Zhang, Jia Pan', 'link': 'https://arxiv.org/abs/2511.01283', 'abstract': 'Finding a control Lyapunov function (CLF) in a dynamical system with a controller is an effective way to guarantee stability, which is a crucial issue in safety-concerned applications. Recently, deep learning models representing CLFs have been applied into a learner-verifier framework to identify satisfiable candidates. However, the learner treats Lyapunov conditions as complex constraints for optimisation, which is hard to achieve global convergence. It is also too complicated to implement these Lyapunov conditions for verification. To improve this framework, we treat Lyapunov conditions as inductive biases and design a neural CLF and a CLF-based controller guided by this knowledge. This design enables a stable optimisation process with limited constraints, and allows end-to-end learning of both the CLF and the controller. Our approach achieves a higher convergence rate and larger region of attraction (ROA) in learning the CLF compared to existing methods among abundant experiment cases. We also thoroughly reveal why the success rate decreases with previous methods during learning.', 'abstract_zh': '在动态系统控制器中寻找控制李雅普诺夫函数（CLF）是保证稳定性的一种有效方法，这对于安全关切的应用至关重要。近年来，深度学习模型被用于CLF的学习-验证框架中以识别满足条件的候选者。然而，学习器将李雅普诺夫条件视为优化的复杂约束，难以实现全局收敛。同时，验证时实施这些李雅普诺夫条件也过于复杂。为了改进此框架，我们将李雅普诺夫条件视为归纳偏置，并设计了一个神经CLF以及基于CLF的控制器，该设计允许在有限约束下稳定优化过程，并能够端到端学习CLF和控制器。在多种实验案例中，我们的方法在学习CLF时实现了更高的收敛率和更宽的研究吸引域（ROA），并且还详细揭示了为什么在学习过程中先前方法的成功率会降低。', 'title_zh': '非线性控制下的李雅普unov稳定性学习与归纳偏置'}
{'arxiv_id': 'arXiv:2511.01223', 'title': 'Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering', 'authors': 'Zahra Mehraban, Sebastien Glaser, Michael Milford, Ronald Schroeter', 'link': 'https://arxiv.org/abs/2511.01223', 'abstract': 'Domain adaptation is required for automated driving models to generalize well across diverse road conditions. This paper explores a training method for domain adaptation to adapt PilotNet, an end-to-end deep learning-based model, for left-hand driving conditions using real-world Australian highway data. Four training methods were evaluated: (1) a baseline model trained on U.S. right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a model pretrained on flipped U.S. data and then finetuned on Australian highways. This setup examines whether incorporating flipped data enhances the model adaptation by providing an initial left-hand driving alignment. The paper compares model performance regarding steering prediction accuracy and attention, using saliency-based analysis to measure attention shifts across significant road regions. Results show that pretraining on flipped data alone worsens prediction stability due to misaligned feature representations, but significantly improves adaptation when followed by fine-tuning, leading to lower prediction error and stronger focus on left-side cues. To validate this approach across different architectures, the same experiments were done on ResNet, which confirmed similar adaptation trends. These findings emphasize the importance of preprocessing techniques, such as flipped-data pretraining, followed by fine-tuning to improve model adaptation with minimal retraining requirements.', 'abstract_zh': '自动化驾驶模型跨多样道路条件泛化的领域自适应训练方法探究：以澳大利亚高速公路数据适配左驾条件的PilotNet模型为例', 'title_zh': '基于显著性引导的域适应方法用于自主转向的左驾驾驶场景äß\nuser\n基于显著性引导的域适应方法用于自主驾驶左转向场景'}
{'arxiv_id': 'arXiv:2511.00934', 'title': 'pacSTL: PAC-Bounded Signal Temporal Logic from Data-Driven Reachability Analysis', 'authors': 'Elizabeth Dietrich, Hanna Krasowski, Emir Cem Gezer, Roger Skjetne, Asgeir Johan Sørensen, Murat Arcak', 'link': 'https://arxiv.org/abs/2511.00934', 'abstract': 'Real-world robotic systems must comply with safety requirements in the presence of uncertainty. To define and measure requirement adherence, Signal Temporal Logic (STL) offers a mathematically rigorous and expressive language. However, standard STL cannot account for uncertainty. We address this problem by presenting pacSTL, a framework that combines Probably Approximately Correct (PAC) bounded set predictions with an interval extension of STL through optimization problems on the atomic proposition level. pacSTL provides PAC-bounded robustness intervals on the specification level that can be utilized in monitoring. We demonstrate the effectiveness of this approach through maritime navigation and analyze the efficiency and scalability of pacSTL through simulation and real-world experimentation on model vessels.', 'abstract_zh': '真实世界中的机器人系统在不确定性存在的情况下必须遵守安全要求。为定义和衡量要求的遵守程度，时序逻辑信号（STL）提供了一种数学严谨且表达能力较强的语言。然而，标准STL无法处理不确定性问题。为解决这一问题，我们提出了pacSTL框架，该框架结合了Probably Approximately Correct（PAC）有界集预测和STL的区间扩展，并在原子命题层面通过优化问题实现。pacSTL在规范层面提供了PAC有界稳健性区间，可用于监控。通过海上导航实例展示了该方法的有效性，并通过模拟和实际船舶实验分析了pacSTL的效率和可扩展性。', 'title_zh': 'pacSTL: 基于数据驱动可达性分析的PAC有界信号时序逻辑'}
{'arxiv_id': 'arXiv:2511.00752', 'title': 'Model-free source seeking of exponentially convergent unicycle: theoretical and robotic experimental results', 'authors': 'Rohan Palanikumar, Ahmed A. Elgohary, Victoria Grushkovskaya, Sameh A. Eisa', 'link': 'https://arxiv.org/abs/2511.00752', 'abstract': 'This paper introduces a novel model-free, real-time unicycle-based source seeking design. This design steers autonomously the unicycle dynamic system towards the extremum point of an objective function or physical/scaler signal that is unknown expression-wise, but accessible via measurements. A key contribution of this paper is that the introduced design converges exponentially to the extremum point of objective functions (or scaler signals) that behave locally like a higher-degree power functions (e.g., fourth degree polynomial function) as opposed to locally quadratic objective functions, the usual case in literature. We provide theoretical and simulation results to support out theoretical results. Also, for the first time in the literature, we provide experimental robotic results that demonstrate the effectiveness of the proposed design and its exponential convergence ability.', 'abstract_zh': '本文介绍了一种新型无模型、实时轮式унicycle源寻找设计。该设计通过测量数据自主引导轮式унicycle动力系统趋近于未知表达形式但可通过测量获得的目标函数或标量信号的极值点。本文的一个关键贡献在于，引入的设计以指数方式趋近于局部行为类似于高次幂函数（如四次多项式函数）的目标函数（或标量信号）的极值点，而不同于文献中通常假设的局部二次目标函数。我们提供了理论和仿真结果来支持我们的理论成果。此外，本文首次在文献中提供了实验机器人结果，证明了所提设计的有效性和指数趋近能力。', 'title_zh': '无需收敛指数的单轮移动机器人源寻求：理论与机器人实验结果'}
{'arxiv_id': 'arXiv:2511.00510', 'title': 'OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback', 'authors': 'Kai Luo, Hao Shi, Kunyu Peng, Fei Teng, Sheng Wu, Kaiwei Wang, Kailun Yang', 'link': 'https://arxiv.org/abs/2511.00510', 'abstract': 'This paper investigates Multi-Object Tracking (MOT) in panoramic imagery, which introduces unique challenges including a 360° Field of View (FoV), resolution dilution, and severe view-dependent distortions. Conventional MOT methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily under these conditions. To address panoramic distortion, large search space, and identity ambiguity under a 360° FoV, OmniTrack++ adopts a feedback-driven framework that progressively refines perception with trajectory cues. A DynamicSSM block first stabilizes panoramic features, implicitly alleviating geometric distortion. On top of normalized representations, FlexiTrack Instances use trajectory-informed feedback for flexible localization and reliable short-term association. To ensure long-term robustness, an ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts design, enabling recovery from fragmented tracks and reducing identity drift. Finally, a Tracklet Management module adaptively switches between end-to-end and tracking-by-detection modes according to scene dynamics, offering a balanced and scalable solution for panoramic MOT. To support rigorous evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for panoramic MOT that includes QuadTrack, captured with a quadruped robot, and BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets span wide-angle environments and diverse motion patterns, providing a challenging testbed for real-world panoramic perception. Extensive experiments on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art performance, yielding substantial HOTA improvements of +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. Datasets and code will be made publicly available at this https URL.', 'abstract_zh': '全景图像中的多目标跟踪：OmniTrack++', 'title_zh': '全向多目标跟踪增强版：通过学习大视场轨迹反馈的多目标跟踪'}
{'arxiv_id': 'arXiv:2511.00423', 'title': 'Bootstrap Off-policy with World Model', 'authors': 'Guojian Zhan, Likun Wang, Xiangteng Zhang, Jiaxin Gao, Masayoshi Tomizuka, Shengbo Eben Li', 'link': 'https://arxiv.org/abs/2511.00423', 'abstract': "Online planning has proven effective in reinforcement learning (RL) for improving sample efficiency and final performance. However, using planning for environment interaction inevitably introduces a divergence between the collected data and the policy's actual behaviors, degrading both model learning and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy with WOrld Model), a framework that tightly integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to bootstrap the policy through behavior alignment. This loop is supported by a jointly learned world model, which enables the planner to simulate future trajectories and provides value targets to facilitate policy improvement. The core of BOOM is a likelihood-free alignment loss that bootstraps the policy using the planner's non-parametric action distribution, combined with a soft value-weighted mechanism that prioritizes high-return behaviors and mitigates variability in the planner's action quality within the replay buffer. Experiments on the high-dimensional DeepMind Control Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in both training stability and final performance. The code is accessible at this https URL.", 'abstract_zh': '基于世界模型的自我引导离策略学习框架（BOOM）：改善规划与离策略学习的紧密集成', 'title_zh': '基于世界模型的Bootstrap离策学习'}
{'arxiv_id': 'arXiv:2511.00266', 'title': 'X-TRACK: Physics-Aware xLSTM for Realistic Vehicle Trajectory Prediction', 'authors': 'Aanchal Rajesh Chugh, Marion Neumeier, Sebastian Dorn', 'link': 'https://arxiv.org/abs/2511.00266', 'abstract': 'Recent advancements in Recurrent Neural Network (RNN) architectures, particularly the Extended Long Short Term Memory (xLSTM), have addressed the limitations of traditional Long Short Term Memory (LSTM) networks by introducing exponential gating and enhanced memory structures. These improvements make xLSTM suitable for time-series prediction tasks as they exhibit the ability to model long-term temporal dependencies better than LSTMs. Despite their potential, these xLSTM-based models remain largely unexplored in the context of vehicle trajectory prediction. Therefore, this paper introduces a novel xLSTM-based vehicle trajectory prediction framework, X-TRAJ, and its physics-aware variant, X-TRACK (eXtended LSTM for TRAjectory prediction Constraint by Kinematics), which explicitly integrates vehicle motion kinematics into the model learning process. By introducing physical constraints, the proposed model generates realistic and feasible trajectories. A comprehensive evaluation on the highD and NGSIM datasets demonstrates that X-TRACK outperforms state-of-the-art baselines.', 'abstract_zh': 'Recent advancements in Recurrent Neural Network (RNN) architectures, particularly the Extended Long Short Term Memory (xLSTM), have addressed the limitations of traditional Long Short Term Memory (LSTM) networks by introducing exponential gating and enhanced memory structures. These improvements make xLSTM suitable for time-series prediction tasks as they exhibit the ability to model long-term temporal dependencies better than LSTMs. Despite their potential, these xLSTM-based models remain largely unexplored in the context of vehicle trajectory prediction. Therefore, this paper introduces a novel xLSTM-based vehicle trajectory prediction framework, X-TRAJ, and its physics-aware variant, X-TRACK (eXtended LSTM for TRAjectory prediction Constraint by Kinematics), which explicitly integrates vehicle motion kinematics into the model learning process. By introducing physical constraints, the proposed model generates realistic and feasible trajectories. A comprehensive evaluation on the highD and NGSIM datasets demonstrates that X-TRACK outperforms state-of-the-art baselines。\n\n翻译后的标题：\n\n基于扩展长短期记忆网络（xLSTM）的车辆轨迹预测框架X-TRACK及其动力学aware变体的研究', 'title_zh': 'X-TRACK：物理意识的xLSTM车辆轨迹预测'}
{'arxiv_id': 'arXiv:2511.00140', 'title': 'Supply Chain Exploitation of Secure ROS 2 Systems: A Proof-of-Concept on Autonomous Platform Compromise via Keystore Exfiltration', 'authors': 'Tahmid Hasan Sakib, Yago Romano Martinez, Carter Brady, Syed Rafay Hasan, Terry N. Guo', 'link': 'https://arxiv.org/abs/2511.00140', 'abstract': 'This paper presents a proof-of-concept supply chain attack against the Secure ROS 2 (SROS 2) framework, demonstrated on a Quanser QCar2 autonomous vehicle platform. A Trojan-infected Debian package modifies core ROS 2 security commands to exfiltrate newly generated keystore credentials via DNS in base64-encoded chunks to an attacker-controlled nameserver. Possession of these credentials enables the attacker to rejoin the SROS 2 network as an authenticated participant and publish spoofed control or perception messages without triggering authentication failures. We evaluate this capability on a secure ROS 2 Humble testbed configured for a four-stop-sign navigation routine using an Intel RealSense camera for perception. Experimental results show that control-topic injections can cause forced braking, sustained high-speed acceleration, and continuous turning loops, while perception-topic spoofing can induce phantom stop signs or suppress real detections. The attack generalizes to any data distribution service (DDS)-based robotic system using SROS 2, highlighting the need for both supply chain integrity controls and runtime semantic validation to safeguard autonomous systems against insider and impersonation threats.', 'abstract_zh': '本文针对安全ROS 2 (SROS 2) 框架提出了一种概念验证供应链攻击，并在Quanser QCar2自主车辆平台上进行演示。受感染的Debian包修改了核心ROS 2安全命令，以将新生成的keystore凭据以base64编码的块形式通过DNS exfiltrate至由攻击者控制的域名服务器。这些凭据的拥有权使攻击者能够作为认证参与者重新加入SROS 2网络，并发布伪造的控制或感知消息而不触发认证故障。在配置了四盏停止标志导航例行程序且使用Intel RealSense相机进行感知的安全ROS 2 Humble测试床中，实验结果表明，控制主题注入可导致强制制动、持续高速加速和连续转弯循环，而感知主题伪造可能导致虚幻的停止标志或抑制真实检测。该攻击可以应用于使用SROS 2的任何基于数据分布服务(DDS)的机器人系统，突显了需要同时具备供应链完整性控制和运行时语义验证以保护自主系统免受内部人员和冒名顶替威胁的重要性。', 'title_zh': '基于密钥库泄露的自主平台 compromize 以证明安全 ROS 2 系统供应链利用的概念验证'}
{'arxiv_id': 'arXiv:2511.00108', 'title': 'Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence', 'authors': 'Yi Zhang, Che Liu, Xiancong Ren, Hanchu Ni, Shuai Zhang, Zeyuan Ding, Jiayu Hu, Hanzhe Shan, Zhenwei Niu, Zhaoyang Liu, Yue Zhao, Junbo Qi, Qinfan Zhang, Dengjie Li, Yidong Wang, Jiachen Luo, Yong Dai, Jian Tang, Xiaozhu Ju', 'link': 'https://arxiv.org/abs/2511.00108', 'abstract': 'This report presents Pelican-VL 1.0, a new family of open-source embodied brain models with parameter scales ranging from 7 billion to 72 billion. Our explicit mission is clearly stated as: To embed powerful intelligence into various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source embodied multimodal brain model. Its core advantage lies in the in-depth integration of data power and intelligent adaptive learning mechanisms. Specifically, metaloop distilled a high-quality dataset from a raw dataset containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint. This translates to a 20.3% performance uplift from its base model and outperforms 100B-level open-source counterparts by 10.6%, placing it on par with leading proprietary systems on well-known embodied benchmarks. We establish a novel framework, DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition to train Pelican-VL 1.0. We operationalize this as a metaloop that teaches the AI to practice deliberately, which is a RL-Refine-Diagnose-SFT loop.', 'abstract_zh': 'Pelican-VL 1.0：一个新的开源 embodiable 大规模多模态脑模型及其关键技术深化整合', 'title_zh': 'Pelican-VL 1.0：一个 embodiment intelligence 的基础脑模型'}
{'arxiv_id': 'arXiv:2511.00091', 'title': 'Self-Improving Vision-Language-Action Models with Data Generation via Residual RL', 'authors': 'Wenli Xiao, Haotian Lin, Andy Peng, Haoru Xue, Tairan He, Yuqi Xie, Fengyuan Hu, Jimmy Wu, Zhengyi Luo, Linxi "Jim" Fan, Guanya Shi, Yuke Zhu', 'link': 'https://arxiv.org/abs/2511.00091', 'abstract': "Supervised fine-tuning (SFT) has become the de facto post-training strategy for large vision-language-action (VLA) models, but its reliance on costly human demonstrations limits scalability and generalization. We propose Probe, Learn, Distill (PLD), a three-stage plug-and-play framework that improves VLAs through residual reinforcement learning (RL) and distribution-aware data collection. In Stage 1, we train lightweight residual actors to probe failure regions of the VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns collected trajectories with the generalist's deployment distribution while capturing recovery behaviors. In Stage 3, we distill the curated trajectories back into the generalist with standard SFT. PLD achieves near-saturated 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations show that residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks, offering a scalable path toward self-improving VLA models.", 'abstract_zh': 'Probe, Learn, Distill (PLD): 一种通过残差强化学习和分布感知数据收集改进多模态视觉-语言-动作模型的三阶段插件式框架', 'title_zh': '基于残差强化学习的数据生成改进视觉-语言-行动模型'}
{'arxiv_id': 'arXiv:2511.00062', 'title': 'World Simulation with Video Foundation Models for Physical AI', 'authors': 'NVIDIA, Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, Prithvijit Chattopadhyay, Mike Chen, Yongxin Chen, Yu Chen, Shuai Cheng, Yin Cui, Jenna Diamond, Yifan Ding, Jiaojiao Fan, Linxi Fan, Liang Feng, Francesco Ferroni, Sanja Fidler, Xiao Fu, Ruiyuan Gao, Yunhao Ge, Jinwei Gu, Aryaman Gupta, Siddharth Gururani, Imad El Hanafi, Ali Hassani, Zekun Hao, Jacob Huffman, Joel Jang, Pooya Jannaty, Jan Kautz, Grace Lam, Xuan Li, Zhaoshuo Li, Maosheng Liao, Chen-Hsuan Lin, Tsung-Yi Lin, Yen-Chen Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Seungjun Nah, Yashraj Narang, Abhijeet Panaskar, Lindsey Pavao, Trung Pham, Morteza Ramezanali, Fitsum Reda, Scott Reed, Xuanchi Ren, Haonan Shao, Yue Shen, Stella Shi, Shuran Song, Bartosz Stefaniak, Shangkun Sun, Shitao Tang, Sameena Tasmeen, Lyne Tchapmi, Wei-Cheng Tseng, Jibin Varghese, Andrew Z. Wang, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Jiashu Xu, Dinghao Yang, Xiaodong Yang, Haotian Ye, Seonghyeon Ye, Xiaohui Zeng, Jing Zhang, Qinsheng Zhang, Kaiwen Zheng, Andrew Zhu, Yuke Zhu', 'link': 'https://arxiv.org/abs/2511.00062', 'abstract': 'We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at this https URL and this https URL. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.', 'abstract_zh': '我们介绍[Cosmos-Predict2.5]，这是Cosmos World基金会模型的最新一代，用于物理人工智能。基于流式架构，[Cosmos-Predict2.5]将Text2World、Image2World和Video2World生成统一到单个模型中，并借助Physical AI视觉语言模型[Cosmos-Reason1]，提供更丰富的文本语境和更精细的世界模拟控制。该模型训练于200M精选视频片段，并通过基于强化学习的后期训练进行 refinement，[Cosmos-Predict2.5]在视频质量和指令对齐方面较[Cosmos-Predict1]取得了显著改进，已发布2B和14B规模的模型。这些能力使合成数据生成、策略评估和机器人与自主系统闭环仿真更加可靠。我们进一步扩展了家族，引入了[Cosmos-Transfer2.5]，一种用于Sim2Real和Real2Real世界转换的控制网架构框架。尽管[Cosmos-Transfer2.5]的规模仅为[Cosmos-Transfer1]的3.5倍，但它提供了更高保真度和稳健的长时段视频生成。这些进展使[Cosmos-Predict2.5]和[Cosmos-Transfer2.5]成为扩展具身智能的多功能工具。为加速物理人工智能的研究和部署，我们在以下链接发布了源代码、预训练检查点和精选基准，版权许可为NVIDIA开源模型许可：[该链接]和[该链接]。我们希望这些开源资源能降低采用门槛，并促进构建下一代具身智能的创新。', 'title_zh': '基于视频基础模型的物理AI世界模拟'}
{'arxiv_id': 'arXiv:2511.00060', 'title': 'Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?', 'authors': 'Zhiqi Qi, Runxin Zhao, Hanyang Zhuang, Chunxiang Wang, Ming Yang', 'link': 'https://arxiv.org/abs/2511.00060', 'abstract': 'LiDAR-based roadside perception is a cornerstone of advanced Intelligent Transportation Systems (ITS). While considerable research has addressed optimal LiDAR placement for infrastructure, the profound impact of differing LiDAR scanning patterns on perceptual performance remains comparatively under-investigated. The inherent nature of various scanning modes - such as traditional repetitive (mechanical/solid-state) versus emerging non-repetitive (e.g. prism-based) systems - leads to distinct point cloud distributions at varying distances, critically dictating the efficacy of object detection and overall environmental understanding. To systematically investigate these differences in infrastructure-based contexts, we introduce the "InfraLiDARs\' Benchmark," a novel dataset meticulously collected in the CARLA simulation environment using concurrently operating infrastructure-based LiDARs exhibiting both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive statistical analysis of the respective LiDAR scanning abilities and evaluate the impact of these distinct patterns on the performance of various leading 3D object detection algorithms. Our findings reveal that non-repetitive scanning LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable detection performance across various scenarios. Despite non-repetitive LiDAR\'s limited perception range, it\'s a cost-effective option considering its low price. Ultimately, this study provides insights for setting up roadside perception system with optimal LiDAR scanning patterns and compatible algorithms for diverse roadside applications, and publicly releases the "InfraLiDARs\' Benchmark" dataset to foster further research.', 'abstract_zh': '基于LiDAR的道路侧感知是先进智能运输系统（ITS）的基石。尽管已有大量研究关注基础设施的最优LiDAR布设，但不同LiDAR扫描模式对感知性能的深远影响仍相对较少被探究。各种扫描模式的固有特性——如传统的重复扫描（机械式/固态）与新兴的非重复扫描（如棱镜式）系统——造成了在不同距离处点云分布的差异，直接影响到对象检测效果及整体环境理解的效度。为系统性地探讨这些差异，我们引入了“InfraLiDARs’ Benchmark”，一个在CARLA仿真环境中采用同时运行的、展示两种扫描模式的基础设施基部署LiDAR精心收集的新颖数据集。利用这一基准，我们进行了全面的统计分析，评估了不同LiDAR扫描模式对各种领先三维物体检测算法性能的影响。研究结果表明，非重复扫描LiDAR与128线重复扫描LiDAR在多种场景下检测性能相近。尽管非重复扫描LiDAR的感知范围有限，但其低廉的价格使其成为一种经济有效的选择。最终，本研究为设计具有最优LiDAR扫描模式和兼容算法的道路侧感知系统提供了见解，并公开发布了“InfraLiDARs’ Benchmark”数据集，以促进进一步的研究。', 'title_zh': '路边感知中哪种LiDAR扫描模式更优：重复扫描还是非重复扫描？'}
{'arxiv_id': 'arXiv:2511.00038', 'title': 'AeroResQ: Edge-Accelerated UAV Framework for Scalable, Resilient and Collaborative Escape Route Planning in Wildfire Scenarios', 'authors': 'Suman Raj, Radhika Mittal, Rajiv Mayani, Pawel Zuk, Anirban Mandal, Michael Zink, Yogesh Simmhan, Ewa Deelman', 'link': 'https://arxiv.org/abs/2511.00038', 'abstract': 'Drone fleets equipped with onboard cameras, computer vision, and Deep Neural Network (DNN) models present a powerful paradigm for real-time spatio-temporal decision-making. In wildfire response, such drones play a pivotal role in monitoring fire dynamics, supporting firefighter coordination, and facilitating safe evacuation. In this paper, we introduce AeroResQ, an edge-accelerated UAV framework designed for scalable, resilient, and collaborative escape route planning during wildfire scenarios. AeroResQ adopts a multi-layer orchestration architecture comprising service drones (SDs) and coordinator drones (CDs), each performing specialized roles. SDs survey fire-affected areas, detect stranded individuals using onboard edge accelerators running fire detection and human pose identification DNN models, and issue requests for assistance. CDs, equipped with lightweight data stores such as Apache IoTDB, dynamically generate optimal ground escape routes and monitor firefighter movements along these routes. The framework proposes a collaborative path-planning approach based on a weighted A* search algorithm, where CDs compute context-aware escape paths. AeroResQ further incorporates intelligent load-balancing and resilience mechanisms: CD failures trigger automated data redistribution across IoTDB replicas, while SD failures initiate geo-fenced re-partitioning and reassignment of spatial workloads to operational SDs. We evaluate AeroResQ using realistic wildfire emulated setup modeled on recent Southern California wildfires. Experimental results demonstrate that AeroResQ achieves a nominal end-to-end latency of <=500ms, much below the 2s request interval, while maintaining over 98% successful task reassignment and completion, underscoring its feasibility for real-time, on-field deployment in emergency response and firefighter safety operations.', 'abstract_zh': '基于边缘加速的AeroResQ无人机框架： wildfire场景下的 scalable、resilient 和协作逃逸路线规划', 'title_zh': 'AeroResQ：加速边缘计算的无人驾驶飞行器框架，用于 wildfires 情境下可扩展、稳健且协作的撤离路线规划'}
