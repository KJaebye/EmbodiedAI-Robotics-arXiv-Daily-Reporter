{'arxiv_id': 'arXiv:2511.01383', 'title': 'CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation', 'authors': 'Landson Guo, Andres M. Diaz Aguilar, William Talbot, Turcan Tuna, Marco Hutter, Cesar Cadena', 'link': 'https://arxiv.org/abs/2511.01383', 'abstract': "Accurate point-wise velocity estimation in 3D is crucial for robot interaction with non-rigid, dynamic agents, such as humans, enabling robust performance in path planning, collision avoidance, and object manipulation in dynamic environments. To this end, this paper proposes a novel RADAR, LiDAR, and camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V. This pipeline leverages raw RADAR measurements to create a novel RADAR representation, the velocity cube, which densely represents radial velocities within the RADAR's field-of-view. By combining the velocity cube for radial velocity extraction, optical flow for tangential velocity estimation, and LiDAR for point-wise range measurements through a closed-form solution, our approach can produce 3D velocity estimates for a dense array of points. Developed as an open-source ROS2 package, CaRLi-V has been field-tested against a custom dataset and proven to produce low velocity error metrics relative to ground truth, enabling point-wise velocity estimation for robotic applications.", 'abstract_zh': '三维点 wise 速度估计对于与非刚性动态代理 （如人类）互动的机器人至关重要，能够为动态环境中的路径规划、碰撞避免和物体操作提供稳健的表现。为此，本文提出了一种基于雷达、激光雷达和相机融合的点 wise 三维速度估计新管道 CaRLi-V。该管道利用原始雷达测量创建了新颖的雷达表示——速度立方体，该表示在雷达视场内密集地表示径向速度。通过结合速度立方体进行径向速度提取、光学流进行切向速度估计以及激光雷达进行点 wise 距离测量的闭式解方式，我们的方法可以为密集点阵列生成 3D 速度估计。作为开源 ROS2 包，CaRLi-V 已在自定义数据集上进行了实地测试，并证明其速度误差指标较低，适用于机器人应用的点 wise 速度估计。', 'title_zh': 'CaRLi-V：摄像头-雷达-激光雷达点级三维速度估计'}
{'arxiv_id': 'arXiv:2511.01502', 'title': 'Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning', 'authors': 'Mengtan Zhang, Zizhan Guo, Hongbo Zhao, Yi Feng, Zuyi Xiong, Yue Wang, Shaoyi Du, Hanli Wang, Rui Fan', 'link': 'https://arxiv.org/abs/2511.01502', 'abstract': 'Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at this http URL upon publication.', 'abstract_zh': '无监督学习深度和 ego 运动这两种基本的三维感知任务近年来取得了显著进展。然而，大多数方法将 ego 运动视为辅助任务，要么混合所有运动类型，要么在监督中排除深度无关的旋转运动。这些设计限制了强几何限制的整合，降低了在多种条件下的可靠性和鲁棒性。本研究引入了运动分量的辨别处理，利用各自刚性流的几何规律，以利于深度和 ego 运动的估计。给定连续的视频帧，网络输出首先对准源和目标相机的光学轴和成像平面。通过这些对准，帧之间的光学流被转换，并量化偏差以在每个 ego 运动分量上施加单独的几何约束，实现更有针对性的细化。这些对准时进一步将联合学习过程重新格式化为共轴和共面形式，其中深度和每个平移分量可以通过闭形式的几何关系互推，引入互补约束以提高深度鲁棒性。整合这些设计的 DiMoDE 通用深度和 ego 运动联合学习框架在多个公开数据集和一个新收集的多样化真实世界数据集上实现了最先进的性能，尤其是在具有挑战性的条件下。我们的源代码将在发表后在此网址公开。', 'title_zh': '区别性处理运动组件演化联合深度和自我运动学习'}
{'arxiv_id': 'arXiv:2511.01501', 'title': 'SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation', 'authors': 'Yufeng Jin, Niklas Funk, Vignesh Prasad, Zechu Li, Mathias Franzius, Jan Peters, Georgia Chalvatzaki', 'link': 'https://arxiv.org/abs/2511.01501', 'abstract': 'Object pose estimation is a fundamental problem in robotics and computer vision, yet it remains challenging due to partial observability, occlusions, and object symmetries, which inevitably lead to pose ambiguity and multiple hypotheses consistent with the same observation. While deterministic deep networks achieve impressive performance under well-constrained conditions, they are often overconfident and fail to capture the multi-modality of the underlying pose distribution. To address these challenges, we propose a novel probabilistic framework that leverages flow matching on the SE(3) manifold for estimating 6D object pose distributions. Unlike existing methods that regress a single deterministic output, our approach models the full pose distribution with a sample-based estimate and enables reasoning about uncertainty in ambiguous cases such as symmetric objects or severe occlusions. We achieve state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our sample-based pose estimates can be leveraged in downstream robotic manipulation tasks such as active perception for disambiguating uncertain viewpoints or guiding grasp synthesis in an uncertainty-aware manner.', 'abstract_zh': '6D物体姿态估计是机器人技术和计算机视觉中的一个基础问题，但由于部分观测性、遮挡和物体对称性等原因，依然具有挑战性，这不可避免地导致姿态模糊性和与同一次观测一致的多个假设。尽管确定性深度网络在条件良好时能够取得令人印象深刻的成绩，但它们通常过于自信，无法捕捉潜在姿态分布的多模态性。为了解决这些问题，我们提出了一种新的概率框架，该框架利用SE(3)流形上的流匹配来估计6D物体姿态分布。与现有方法回归单一确定性输出不同，我们的方法以样本估计的方式建模整个姿态分布，并能够在对称物体或严重遮挡等模棱两可的情况下进行不确定性推理。我们在Real275、YCB-V和LM-O数据集上取得了最新的成果，并展示了如何利用我们的样本估计姿态在下游的机器人操作任务中，如具有不确定性意识的主动感知以消解模糊视角或在不确定性驱动下指导抓取合成。', 'title_zh': 'SE(3)-PoseFlow：估计6D姿态分布以实现不确定性感知的机器人操作'}
{'arxiv_id': 'arXiv:2511.00510', 'title': 'OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback', 'authors': 'Kai Luo, Hao Shi, Kunyu Peng, Fei Teng, Sheng Wu, Kaiwei Wang, Kailun Yang', 'link': 'https://arxiv.org/abs/2511.00510', 'abstract': 'This paper investigates Multi-Object Tracking (MOT) in panoramic imagery, which introduces unique challenges including a 360° Field of View (FoV), resolution dilution, and severe view-dependent distortions. Conventional MOT methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily under these conditions. To address panoramic distortion, large search space, and identity ambiguity under a 360° FoV, OmniTrack++ adopts a feedback-driven framework that progressively refines perception with trajectory cues. A DynamicSSM block first stabilizes panoramic features, implicitly alleviating geometric distortion. On top of normalized representations, FlexiTrack Instances use trajectory-informed feedback for flexible localization and reliable short-term association. To ensure long-term robustness, an ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts design, enabling recovery from fragmented tracks and reducing identity drift. Finally, a Tracklet Management module adaptively switches between end-to-end and tracking-by-detection modes according to scene dynamics, offering a balanced and scalable solution for panoramic MOT. To support rigorous evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for panoramic MOT that includes QuadTrack, captured with a quadruped robot, and BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets span wide-angle environments and diverse motion patterns, providing a challenging testbed for real-world panoramic perception. Extensive experiments on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art performance, yielding substantial HOTA improvements of +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. Datasets and code will be made publicly available at this https URL.', 'abstract_zh': '全景图像中的多目标跟踪：OmniTrack++', 'title_zh': '全向多目标跟踪增强版：通过学习大视场轨迹反馈的多目标跟踪'}
{'arxiv_id': 'arXiv:2511.01831', 'title': 'Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models', 'authors': 'Jay Mohta, Kenan Emir Ak, Dimitrios Dimitriadis, Yan Xu, Mingwei Shen', 'link': 'https://arxiv.org/abs/2511.01831', 'abstract': "Vision-Language Models (VLMs) suffer from catastrophic forgetting when sequentially fine-tuned on new tasks, degrading performance on previously learned foundational and task-specific capabilities. While multi-task learning can mitigate forgetting, it requires simultaneous access to all datasets and imposes computational overhead that scales linearly with the number of tasks. In this work, we introduce a routing-based approach that enables the integration of new tasks while preserving the foundational knowledge acquired during pretraining. We evaluate our method using InternVL-2 models (2B and 8B parameters) and demonstrate that routing preserves the model's foundational capabilities by maintaining performance on general-purpose benchmarks such as ChartQA, MMBench, and DocVQA, while simultaneously improving accuracy on specialized tasks. Importantly, our approach achieves this without requiring concurrent access to data from all tasks, avoiding the significant computational and data overhead associated with traditional multi-task learning. We further conduct extensive ablation studies to evaluate the scalability and robustness of routing-based learning, showing that the approach is resilient to a growing number of tasks and performs particularly well when new tasks are semantically related. Finally, we show that the routing mechanism enables superior cross-modal transfer between language and vision capabilities, allowing knowledge learned in one modality to enhance performance in another capability not achieved by existing continual learning methods.", 'abstract_zh': '基于路由的方法在连续学习视觉-语言模型中的应用：保留基础能力同时集成新任务', 'title_zh': '专家间动态路由：视觉-语言模型持续学习的高效数据方法'}
{'arxiv_id': 'arXiv:2511.01767', 'title': 'Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image', 'authors': 'Yuxiao Yang, Xiao-Xiao Long, Zhiyang Dou, Cheng Lin, Yuan Liu, Qingsong Yan, Yuexin Ma, Haoqian Wang, Zhiqiang Wu, Wei Yin', 'link': 'https://arxiv.org/abs/2511.01767', 'abstract': 'In this work, we introduce \\textbf{Wonder3D++}, a novel method for efficiently generating high-fidelity textured meshes from single-view images. Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of single-view reconstruction tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure the consistency of generation, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that drives high-quality surfaces from the multi-view 2D representations in only about $3$ minute in a coarse-to-fine manner. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and good efficiency compared to prior works. Code available at this https URL.', 'abstract_zh': '在单视角图像高效生成高保真纹理化网格的新方法：Wonder3D++', 'title_zh': 'Wonder3D++: 跨领域扩散生成高保真单图三维重建'}
{'arxiv_id': 'arXiv:2511.01572', 'title': 'HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET', 'authors': 'Wang Hao, Kuang Zhang, Hou Chengyu, Yuan Zhonghao, Tan Chenxing, Fu Weifeng, Zhu Yangying', 'link': 'https://arxiv.org/abs/2511.01572', 'abstract': 'Time series classification holds broad application value in communications, information countermeasures, finance, and medicine. However, state-of-the-art (SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high computational complexity, coupled with lengthy parameter tuning and training cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional Kernel Transform) offer greater efficiency but leave substantial room for improvement in kernel selection and computational overhead. To address these challenges, we propose a feature extraction approach based on Hadamard convolutional transform, utilizing column or row vectors of Hadamard matrices as convolution kernels with extended lengths of varying sizes. This enhancement maintains full compatibility with existing methods (e.g., ROCKET) while leveraging kernel orthogonality to boost computational efficiency, robustness, and adaptability. Comprehensive experiments on multi-domain datasets-focusing on the UCR time series dataset-demonstrate SOTA performance: F1-score improved by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET (fastest ROCKET variant) under identical hyperparameters, enabling deployment on ultra-low-power embedded devices. All code is available on GitHub.', 'abstract_zh': '基于哈达玛卷积变换的特征提取方法在时间序列分类中的应用', 'title_zh': 'HIT-ROCKET: Hadamard-vector 内积变换器 für ROCKET'}
{'arxiv_id': 'arXiv:2511.01462', 'title': 'Efficiently Training A Flat Neural Network Before It has been Quantizated', 'authors': 'Peng Xia, Junbiao Pang, Tianyang Cai', 'link': 'https://arxiv.org/abs/2511.01462', 'abstract': 'Post-training quantization (PTQ) for vision transformers (ViTs) has garnered significant attention due to its efficiency in compressing models. However, existing methods typically overlook the relationship between a well-trained NN and the quantized model, leading to considerable quantization error for PTQ. However, it is unclear how to efficiently train a model-agnostic neural network which is tailored for a predefined precision low-bit model. In this paper, we firstly discover that a flat full precision neural network is crucial for low-bit quantization. To achieve this, we propose a framework that proactively pre-conditions the model by measuring and disentangling the error sources. Specifically, both the Activation Quantization Error (AQE) and the Weight Quantization Error (WQE) are statistically modeled as independent Gaussian noises. We study several noise injection optimization methods to obtain a flat minimum. Experimental results attest to the effectiveness of our approach. These results open novel pathways for obtaining low-bit PTQ models.', 'abstract_zh': 'Post-训练量化（PTQ）在视觉变换器（ViTs）中的应用由于其高效性吸引了大量关注。然而，现有的方法通常忽略了训练良好的神经网络与量化模型之间的关系，导致PTQ中的极大量化误差。此外，尚不清楚如何高效地训练一种适用于预定义低位精度模型的通用神经网络。在本文中，我们首先发现全精度神经网络对于低位量化至关重要。为此，我们提出了一种框架，通过测量和分离误差源来主动预处理模型。具体来说，我们将激活量化误差（AQE）和权重量化误差（WQE）分别统计建模为独立的高斯噪声。我们研究了几种噪声注入优化方法以获得平坦的最小值。实验结果证明了我们方法的有效性。这些结果为获得低位PTQ模型开辟了新的途径。', 'title_zh': '高效训练平层神经网络以备量化'}
{'arxiv_id': 'arXiv:2511.01450', 'title': 'Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation', 'authors': 'Jie Du, Xinyu Gong, Qingshan Tan, Wen Li, Yangming Cheng, Weitao Wang, Chenlu Zhan, Suhui Wu, Hao Zhang, Jun Zhang', 'link': 'https://arxiv.org/abs/2511.01450', 'abstract': 'Recent studies have identified Direct Preference Optimization (DPO) as an efficient and reward-free approach to improving video generation quality. However, existing methods largely follow image-domain paradigms and are mainly developed on small-scale models (approximately 2B parameters), limiting their ability to address the unique challenges of video tasks, such as costly data construction, unstable training, and heavy memory consumption. To overcome these limitations, we introduce a GT-Pair that automatically builds high-quality preference pairs by using real videos as positives and model-generated videos as negatives, eliminating the need for any external annotation. We further present Reg-DPO, which incorporates the SFT loss as a regularization term into the DPO objective to enhance training stability and generation fidelity. Additionally, by combining the FSDP framework with multiple memory optimization techniques, our approach achieves nearly three times higher training capacity than using FSDP alone. Extensive experiments on both I2V and T2V tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches, delivering superior video generation quality.', 'abstract_zh': '近期研究发现，直接偏好优化（DPO）是一种高效且无需奖励的视频生成质量改进方法。然而，现有方法大多遵循图像领域的范式，主要在小型模型（约20亿参数）上开发，限制了它们解决视频任务的独特挑战（如数据构建成本高、训练不稳定和内存消耗大）的能力。为克服这些局限性，我们引入了一个GT-Pair，通过使用真实视频作为正样本和模型生成的视频作为负样本，自动构建高质量的偏好配对，从而消除了对外部注释的需求。我们进一步提出了Reg-DPO，通过将SFT损失作为正则化项纳入DPO目标中，以增强训练稳定性和生成保真度。此外，通过结合FSDP框架和多种内存优化技术，我们的方法在训练容量方面比单独使用FSDP提高了近三倍。在多个数据集上的I2V和T2V任务的大量实验中，我们的方法一贯优于现有方法，提供更优质的视频生成质量。', 'title_zh': 'Reg-DPO: SFT-正则化直接偏好优化以改进视频生成'}
{'arxiv_id': 'arXiv:2511.01237', 'title': 'Eyes on Target: Gaze-Aware Object Detection in Egocentric Video', 'authors': 'Vishakha Lall, Yisi Liu', 'link': 'https://arxiv.org/abs/2511.01237', 'abstract': 'Human gaze offers rich supervisory signals for understanding visual attention in complex visual environments. In this paper, we propose Eyes on Target, a novel depth-aware and gaze-guided object detection framework designed for egocentric videos. Our approach injects gaze-derived features into the attention mechanism of a Vision Transformer (ViT), effectively biasing spatial feature selection toward human-attended regions. Unlike traditional object detectors that treat all regions equally, our method emphasises viewer-prioritised areas to enhance object detection. We validate our method on an egocentric simulator dataset where human visual attention is critical for task assessment, illustrating its potential in evaluating human performance in simulation scenarios. We evaluate the effectiveness of our gaze-integrated model through extensive experiments and ablation studies, demonstrating consistent gains in detection accuracy over gaze-agnostic baselines on both the custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a gaze-aware attention head importance metric, revealing how gaze cues modulate transformer attention dynamics.', 'abstract_zh': '基于注意力的眼球注视引导对象检测框架：Eyes on Target', 'title_zh': '面向目标：自视点视频中的注视对象检测'}
{'arxiv_id': 'arXiv:2511.01194', 'title': 'A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment', 'authors': 'Minmin Zeng', 'link': 'https://arxiv.org/abs/2511.01194', 'abstract': 'Action Quality Assessment (AQA) requires fine-grained understanding of human motion and precise evaluation of pose similarity. This paper proposes a topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN, which models the human skeleton as a graph to learn discriminative, topology-sensitive pose embeddings. Using a Siamese architecture trained with a contrastive regression objective, our method outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving benchmarks. Experimental results and ablation studies validate the effectiveness of leveraging skeletal topology for pose similarity and action quality assessment.', 'abstract_zh': '拓扑感知图卷积网络（GCN-PSN）：动作质量评估中的骨架拓扑建模', 'title_zh': '一种aware拓扑结构的图卷积网络及其在人体姿态相似性与动作质量评估中的应用'}
{'arxiv_id': 'arXiv:2511.01143', 'title': 'MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation', 'authors': 'Ziyi Wang, Yuanmei Zhang, Dorna Esrafilzadeh, Ali R. Jalili, Suncheng Xiang', 'link': 'https://arxiv.org/abs/2511.01143', 'abstract': 'Early and accurate segmentation of colorectal polyps is critical for reducing colorectal cancer mortality, which has been extensively explored by academia and industry. However, current deep learning-based polyp segmentation models either compromise clinical decision-making by providing ambiguous polyp margins in segmentation outputs or rely on heavy architectures with high computational complexity, resulting in insufficient inference speeds for real-time colorectal endoscopic applications. To address this problem, we propose MicroAUNet, a light-weighted attention-based segmentation network that combines depthwise-separable dilated convolutions with a single-path, parameter-shared channel-spatial attention block to strengthen multi-scale boundary features. On the basis of it, a progressive two-stage knowledge-distillation scheme is introduced to transfer semantic and boundary cues from a high-capacity teacher. Extensive experiments on benchmarks also demonstrate the state-of-the-art accuracy under extremely low model complexity, indicating that MicroAUNet is suitable for real-time clinical polyp segmentation. The code is publicly available at this https URL.', 'abstract_zh': '早准 colorectal 肿瘤分割对于降低 colorectal 癌死亡率至关重要，这已被学术界和工业界广泛探索。然而，当前基于深度学习的结肠息肉分割模型要么通过提供模棱两可的息肉边界损害临床决策，要么依赖于复杂度高、计算复杂度高的重架构，导致实时结肠内镜应用中的推断速度不足。为解决这一问题，我们提出了一种轻量级注意力基分割网络 MicroAUNet，该网络结合了深度可分离空洞卷积和单路径、参数共享的通道-空间注意力模块，以加强多尺度边界特征。在此基础上，引入了一种渐进的两阶段知识蒸馏方案，从具有高容量的教师模型中转移语义和边界线索。广泛基准上的实验也证明了在极低模型复杂度下的顶级准确性，表明 MicroAUNet 适用于实时临床结肠息肉分割。代码已在该网址公开。', 'title_zh': 'MicroAUNet：带有知识蒸馏的边界增强多尺度融合在结肠镜息肉图像分割中的应用'}
{'arxiv_id': 'arXiv:2511.01139', 'title': 'Learning with Category-Equivariant Architectures for Human Activity Recognition', 'authors': 'Yoshihiro Maruyama', 'link': 'https://arxiv.org/abs/2511.01139', 'abstract': 'We propose CatEquiv, a category-equivariant neural network for Human Activity Recognition (HAR) from inertial sensors that systematically encodes temporal, amplitude, and structural symmetries. In particular, we introduce the categorical symmetry product where cyclic time shifts, positive gains and the sensor-hierarchy poset together capture the categorical symmetry structure of the data. CatEquiv achieves equivariance with respect to the categorical symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv attains markedly higher robustness compared with circularly padded CNNs and plain CNNs. These results demonstrate that enforcing categorical symmetries yields strong invariance and generalization without additional model capacity.', 'abstract_zh': 'CatEquiv：一种系统编码时间、幅度和结构对称性的类别不变神经网络及其在惯性传感器的人体活动识别中的应用', 'title_zh': '基于类别等变架构的人体活动识别学习'}
{'arxiv_id': 'arXiv:2511.00881', 'title': 'Deep Generative Models for Enhanced Vitreous OCT Imaging', 'authors': 'Simone Sarrocco, Philippe C. Cattin, Peter M. Maloca, Paul Friedrich, Philippe Valmaggia', 'link': 'https://arxiv.org/abs/2511.00881', 'abstract': "Purpose: To evaluate deep learning (DL) models for enhancing vitreous optical coherence tomography (OCT) image quality and reducing acquisition time. Methods: Conditional Denoising Diffusion Probabilistic Models (cDDPMs), Brownian Bridge Diffusion Models (BBDMs), U-Net, Pix2Pix, and Vector-Quantised Generative Adversarial Network (VQ-GAN) were used to generate high-quality spectral-domain (SD) vitreous OCT images. Inputs were SD ART10 images, and outputs were compared to pseudoART100 images obtained by averaging ten ART10 images per eye location. Model performance was assessed using image quality metrics and Visual Turing Tests, where ophthalmologists ranked generated images and evaluated anatomical fidelity. The best model's performance was further tested within the manually segmented vitreous on newly acquired data. Results: U-Net achieved the highest Peak Signal-to-Noise Ratio (PSNR: 30.230) and Structural Similarity Index Measure (SSIM: 0.820), followed by cDDPM. For Learned Perceptual Image Patch Similarity (LPIPS), Pix2Pix (0.697) and cDDPM (0.753) performed best. In the first Visual Turing Test, cDDPM ranked highest (3.07); in the second (best model only), cDDPM achieved a 32.9% fool rate and 85.7% anatomical preservation. On newly acquired data, cDDPM generated vitreous regions more similar in PSNR to the ART100 reference than true ART1 or ART10 B-scans and achieved higher PSNR on whole images when conditioned on ART1 than ART10. Conclusions: Results reveal discrepancies between quantitative metrics and clinical evaluation, highlighting the need for combined assessment. cDDPM showed strong potential for generating clinically meaningful vitreous OCT images while reducing acquisition time fourfold. Translational Relevance: cDDPMs show promise for clinical integration, supporting faster, higher-quality vitreous imaging. Dataset and code will be made publicly available.", 'abstract_zh': '目的：评估深度学习（DL）模型在提升玻璃体光学相干断层扫描（OCT）图像质量和缩短图像获取时间方面的效果。方法：使用条件去噪扩散概率模型（cDDPMs）、布朗运动桥扩散模型（BBDMs）、U-Net、Pix2Pix和向量量化生成对抗网络（VQ-GAN）生成高质量的 spectral-domain（SD）玻璃体OCT图像。输入为SD ART10图像，输出与通过每眼位置平均10张ART10图像得到的伪ART100图像进行比较。模型性能通过图像质量指标和视觉图灵测试评估，其中眼科医生对生成的图像进行排名并评估解剖结构保真度。最佳模型在手动分割的玻璃体新采集数据上进行了进一步测试。结果：U-Net在信噪比（PSNR：30.230）和结构相似性指数措施（SSIM：0.820）中表现最好，其次是cDDPM。对于学习感知图像块相似性（LPIPS），Pix2Pix（0.697）和cDDPM（0.753）表现最佳。在第一次视觉图灵测试中，cDDPM表现最高（3.07）；在第二次测试（仅最佳模型）中，cDDPM实现了32.9%的愚弄率和85.7%的解剖结构保存。在新采集的数据上，cDDPM生成的玻璃体区域在PSNR上与ART100参考更相似，且在有条件地使用ART1时的PSNR整体图像比使用ART10时更高。结论：结果揭示了定量指标与临床评估之间的差异，强调了需要综合评估的必要性。cDDPM在生成有意义的临床玻璃体OCT图像同时将图像获取时间减少四倍方面表现出强劲的潜力。适用性：cDDPMs显示出了在临床应用中的潜力，支持更快、更高质量的玻璃体成像。数据集和代码将被公开。', 'title_zh': '深度生成模型增强玻璃体OCT成像'}
{'arxiv_id': 'arXiv:2511.00858', 'title': 'Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction', 'authors': 'Yu Liu, Zhijie Liu, Zedong Yang, You-Fu Li, He Kong', 'link': 'https://arxiv.org/abs/2511.00858', 'abstract': "Predicting pedestrian crossing intentions is crucial for the navigation of mobile robots and intelligent vehicles. Although recent deep learning-based models have shown significant success in forecasting intentions, few consider incomplete observation under occlusion scenarios. To tackle this challenge, we propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded motion patterns and leverages them to guide future intention prediction. During the denoising stage, we introduce an occlusion-aware diffusion transformer architecture to estimate noise features associated with occluded patterns, thereby enhancing the model's ability to capture contextual relationships in occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse process is introduced to effectively utilize observation information, reducing the accumulation of prediction errors and enhancing the accuracy of reconstructed motion features. The performance of the proposed method under various occlusion scenarios is comprehensively evaluated and compared with existing methods on popular benchmarks, namely PIE and JAAD. Extensive experimental results demonstrate that the proposed method achieves more robust performance than existing methods in the literature.", 'abstract_zh': '基于遮挡aware的扩散模型预测行人过街意图', 'title_zh': 'Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction（注意：此标题已尽量贴近原文含义进行翻译）'}
{'arxiv_id': 'arXiv:2511.00836', 'title': 'Parameter Interpolation Adversarial Training for Robust Image Classification', 'authors': 'Xin Liu, Yichen Yang, Kun He, John E. Hopcroft', 'link': 'https://arxiv.org/abs/2511.00836', 'abstract': 'Though deep neural networks exhibit superior performance on various tasks, they are still plagued by adversarial examples. Adversarial training has been demonstrated to be the most effective method to defend against adversarial attacks. However, existing adversarial training methods show that the model robustness has apparent oscillations and overfitting issues in the training process, degrading the defense efficacy. To address these issues, we propose a novel framework called Parameter Interpolation Adversarial Training (PIAT). PIAT tunes the model parameters between each epoch by interpolating the parameters of the previous and current epochs. It makes the decision boundary of model change more moderate and alleviates the overfitting issue, helping the model converge better and achieving higher model robustness. In addition, we suggest using the Normalized Mean Square Error (NMSE) to further improve the robustness by aligning the relative magnitude of logits between clean and adversarial examples rather than the absolute magnitude. Extensive experiments conducted on several benchmark datasets demonstrate that our framework could prominently improve the robustness of both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).', 'abstract_zh': '虽然深度神经网络在各种任务中表现出色，但仍遭受对抗样本的困扰。对抗训练已被证明是抵御对抗攻击最有效的防御方法。然而，现有的对抗训练方法在训练过程中显示出模型稳健性有明显的波动和过拟合问题，降低了防御效果。为了解决这些问题，我们提出了一种名为Parameter Interpolation Adversarial Training（PIAT）的新框架。PIAT通过插值上一 epoch 和当前 epoch 的参数来调整模型参数，使模型决策边界的变化更为温和，缓解过拟合问题，帮助模型更好地收敛，提高模型的稳健性。此外，我们建议使用归一化均方误差（NMSE）进一步提高稳健性，通过对齐干净样本和对抗样本的logits的相对大小，而非绝对大小。在多个基准数据集上的广泛实验表明，我们的框架能够显著提高卷积神经网络（CNNs）和视觉变换器（ViTs）的稳健性。', 'title_zh': '参数插值对抗训练实现稳健图像分类'}
{'arxiv_id': 'arXiv:2511.00833', 'title': 'Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials', 'authors': 'Yifan Pu, Jixuan Ying, Qixiu Li, Tianzhu Ye, Dongchen Han, Xiaochen Wang, Ziyi Wang, Xinyu Shao, Gao Huang, Xiu Li', 'link': 'https://arxiv.org/abs/2511.00833', 'abstract': "Vision Transformers (ViTs) have become a universal backbone for both image recognition and image generation. Yet their Multi-Head Self-Attention (MHSA) layer still performs a quadratic query-key interaction for every token pair, spending the bulk of computation on visually weak or redundant correlations. We introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that injects an explicit notion of discrimination while reducing the theoretical complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's dense query field into a handful of spatially pooled visual-contrast tokens, then splits them into a learnable positive and negative stream whose differential interaction highlights what truly separates one region from another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone, requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and improves three strong hierarchical ViTs by up to 3.1%, while in class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm that (i) spatial pooling supplies low-variance global cues, (ii) dual positional embeddings are indispensable for contrastive reasoning, and (iii) combining the two in both stages yields the strongest synergy. VCA therefore offers a simple path towards faster and sharper Vision Transformers. The source code is available at this https URL.", 'abstract_zh': 'Vision Transformers (ViTs)已成为图像识别和图像生成的通用骨干。然而，它们的多头自注意力（MHSA）层仍对每对令牌进行二次查询-键交互，大部分计算量集中在视觉较弱或冗余的相关性上。我们引入了视觉对比注意力（VCA），这是一种可替代MHSA的组件，通过减少理论复杂度从O(N NC)到O(N n C)，同时增加了显式的区分概念。VCA首先将每个头的密集查询字段提炼成少量的空间聚合视觉对比令牌，然后将它们分为可学习的正向和负向流，其差异性交互凸显出一个区域与另一个区域的本质区别。该模块在DeiT-Tiny骨干网络上增加了不到0.3M的参数，不需要额外的FLOPs，并且完全不受架构影响。实验结果显示，VCA将DeiT-Tiny在ImageNet-1K上的Top-1准确性从72.2%提升到75.6%（+3.4）；并分别在三种强大的分层ViTs中提高了0-3.1%；在有条件生成的ImageNet中，跨扩散（DiT）和流（SiT）模型，VCA使FID-50K降低了2.1到5.2分。广泛的消融实验表明，（i）空间聚合提供了低方差的全局线索，（ii）双位置嵌入对于对比推理是必不可少的，（iii）两个阶段结合使用这两种方法产生了最强的协同效应。因此，VCA提供了一条通往更快、更锐利的Vision Transformers的简单路径。源代码可从此链接获取。', 'title_zh': '线性微分视觉变换器：通过成对微分学习视觉对比度'}
{'arxiv_id': 'arXiv:2511.00785', 'title': 'Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking', 'authors': 'Juan Wang, Yasutomo Kawanishi, Tomo Miyazaki, Zhijie Wang, Shinichiro Omachi', 'link': 'https://arxiv.org/abs/2511.00785', 'abstract': '3D instance segmentation is an important task for real-world applications. To avoid costly manual annotations, existing methods have explored generating pseudo labels by transferring 2D masks from foundation models to 3D. However, this approach is often suboptimal since the video frames are processed independently. This causes inconsistent segmentation granularity and conflicting 3D pseudo labels, which degrades the accuracy of final segmentation. To address this, we introduce a Granularity-Consistent automatic 2D Mask Tracking approach that maintains temporal correspondences across frames, eliminating conflicting pseudo labels. Combined with a three-stage curriculum learning framework, our approach progressively trains from fragmented single-view data to unified multi-view annotations, ultimately globally coherent full-scene supervision. This structured learning pipeline enables the model to progressively expose to pseudo-labels of increasing consistency. Thus, we can robustly distill a consistent 3D representation from initially fragmented and contradictory 2D priors. Experimental results demonstrated that our method effectively generated consistent and accurate 3D segmentations. Furthermore, the proposed method achieved state-of-the-art results on standard benchmarks and open-vocabulary ability.', 'abstract_zh': '三维实例分割是现实应用中的一个重要任务。为了避免昂贵的手动注释，现有方法探索通过将基础模型的2D掩码转移到3D中生成伪标签。然而，这种方法常因视频帧独立处理而导致分割粒度不一致和冲突的3D伪标签，这降低了最终分割的准确性。为解决这一问题，我们提出了一种粒度一致的自动2D掩码跟踪方法，该方法保持跨帧的时间对应关系，消除了冲突的伪标签。结合三阶段递进式 Curriculum Learning 框架，我们的方法逐步从碎片化的单视角数据训练到统一的多视角注释，最终实现全局一致的全场监督。这种结构化学习管道使模型能够逐步接触到越来越一致的伪标签。因此，我们能够从最初碎片化且矛盾的2D先验中稳健地提炼出一致的3D表示。实验结果表明，我们的方法有效地生成了一致且准确的3D分割。此外，所提出的方法在标准基准和开放词汇能力上均取得了最先进的结果。', 'title_zh': '颗粒性一致的自动2D掩码跟踪实现无类别3D分割'}
{'arxiv_id': 'arXiv:2511.00580', 'title': 'TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection', 'authors': 'Yousuf Ahmed Siddiqui, Sufiyaan Usmani, Umer Tariq, Jawwad Ahmed Shamsi, Muhammad Burhan Khan', 'link': 'https://arxiv.org/abs/2511.00580', 'abstract': 'Video anomalies often depend on contextual information available and temporal evolution. Non-anomalous action in one context can be anomalous in some other context. Most anomaly detectors, however, do not notice this type of context, which seriously limits their capability to generalize to new, real-life situations. Our work addresses the context-aware zero-shot anomaly detection challenge, in which systems need to learn adaptively to detect new events by correlating temporal and appearance features with textual traces of memory in real time. Our approach defines a memory-augmented pipeline, correlating temporal signals with visual embeddings using cross-attention, and real-time zero-shot anomaly classification by contextual similarity scoring. We achieve 90.4\\% AUC on UCF-Crime and 83.67\\% AP on XD-Violence, a new state-of-the-art among zero-shot models. Our model achieves real-time inference with high precision and explainability for deployment. We show that, by fusing cross-attention temporal fusion and contextual memory, we achieve high fidelity anomaly detection, a step towards the applicability of zero-shot models in real-world surveillance and infrastructure monitoring.', 'abstract_zh': '视频异常往往依赖于可用的上下文信息和时间演化。在同一上下文中看似正常的动作在另一个上下文中可能是异常的。然而，大多数异常检测器并未注意到这种类型的上下文，这严重限制了它们在新现实场景中的泛化能力。我们的工作旨在解决上下文感知的零样本异常检测挑战，其中系统需要通过实时关联时间特征、外观嵌入与文本记忆的方式，以上下文相似性评分实现自适应学习来检测新事件。我们提出的方法定义了一个增强记忆的管道，通过交叉注意力将时间信号与视觉嵌入相关联，并通过上下文相似性评分实现实时零样本异常分类。在UCF-Crime数据集上，我们取得了90.4%的AUC，在XD-Violence数据集上，取得了83.67%的AP，这是零样本模型中的最新紀錄。我们的模型实现了实时推理，具有高精度和可解释性，便于部署。我们展示了通过融合交叉注意力时间融合和上下文记忆，实现了高保真异常检测，这是零样本模型在实际监控和基础设施监测应用的一个进步。', 'title_zh': 'TRACES: 基于上下文嵌入的实时视频异常检测的时间召回方法'}
{'arxiv_id': 'arXiv:2511.00429', 'title': 'Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection', 'authors': 'Daichi Zhang, Tong Zhang, Shiming Ge, Sabine Süsstrunk', 'link': 'https://arxiv.org/abs/2511.00429', 'abstract': 'Diffusion models have achieved remarkable success in image synthesis, but the generated high-quality images raise concerns about potential malicious use. Existing detectors often struggle to capture discriminative clues across different models and settings, limiting their generalization to unseen diffusion models and robustness to various perturbations. To address this issue, we observe that diffusion-generated images exhibit progressively larger differences from natural real images across low- to high-frequency bands. Based on this insight, we propose a simple yet effective representation by enhancing the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we introduce a frequency-selective function which serves as a weighted filter to the Fourier spectrum, suppressing less discriminative bands while enhancing more informative ones. This approach, grounded in a comprehensive analysis of frequency-based differences between natural real and diffusion-generated images, enables general detection of images from unseen diffusion models and provides robust resilience to various perturbations. Extensive experiments on various diffusion-generated image datasets demonstrate that our method outperforms state-of-the-art detectors with superior generalization and robustness.', 'abstract_zh': '扩散模型在图像合成领域取得了显著成功，但生成的高质量图像引发了潜在恶意使用方面的担忧。现有检测器往往难以捕捉不同模型和场景下的判别线索，限制了其对未见过的扩散模型的泛化能力和对各种扰动的鲁棒性。为了应对这一问题，我们观察到，扩散生成的图像在低频到高频 band 的范围内与自然真实图像呈现出逐渐增大的差异。基于这一认识，我们提出了一种简单有效的表示方法，通过增强所有频段的 Frequency Forgery Clue (F^2C)。具体而言，我们引入了一个频段选择性函数，作为傅里叶频谱的加权过滤器，抑制少信息性的频段，同时增强更具信息性的频段。该方法通过全面分析自然真实图像与扩散生成图像之间的频域差异，能够泛化检测未见过的扩散模型图像，并提供对各种扰动的鲁棒性。在各种扩散生成图像数据集上的广泛实验表明，我们的方法在泛化能力和鲁棒性方面优于现有最先进的检测器。', 'title_zh': '增强扩散生成图像检测中的频率伪造线索'}
{'arxiv_id': 'arXiv:2511.00392', 'title': 'SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping', 'authors': 'Lingpeng Chen, Jiakun Tang, Apple Pui-Yi Chui, Ziyang Hong, Junfeng Wu', 'link': 'https://arxiv.org/abs/2511.00392', 'abstract': 'Accurate 3D reconstruction in visually-degraded underwater environments remains a formidable challenge. Single-modality approaches are insufficient: vision-based methods fail due to poor visibility and geometric constraints, while sonar is crippled by inherent elevation ambiguity and low resolution. Consequently, prior fusion technique relies on heuristics and flawed geometric assumptions, leading to significant artifacts and an inability to model complex scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep learning framework that overcomes these limitations by adapting the principled plane sweep algorithm for cross-modal fusion between sonar and visual data. Extensive experiments in both high-fidelity simulation and real-world environments demonstrate that SonarSweep consistently generates dense and accurate depth maps, significantly outperforming state-of-the-art methods across challenging conditions, particularly in high turbidity. To foster further research, we will publicly release our code and a novel dataset featuring synchronized stereo-camera and sonar data, the first of its kind.', 'abstract_zh': '准确重建视觉退化的水下环境仍是一项艰巨的挑战。单一模态方法不足：基于视觉的方法因能见度差和几何约束而失效，而声纳则因固有的仰角 ambiguity 和低分辨率而受限。因此，先前的融合技术依赖于不切实际的几何假设和启发式方法，导致大量伪影且无法建模复杂的场景。本文提出 SonarSweep，这是一种新颖的端到端深度学习框架，通过将原理上的平面扫描算法适应于声纳和视觉数据的跨模态融合，克服了这些限制。在高保真模拟和实际水下环境中的广泛实验表明，SonarSweep 生成稠密且精确的深度图，能够在各种复杂条件下显著优于现有方法，尤其是在高浑浊度环境下。为了促进进一步研究，我们将公开发布我们的代码和包含同步立体相机和声纳数据的新数据集，这是此类数据集的首创。', 'title_zh': 'SonarSweep：融合声纳与视觉的平面扫描法用于稳健的三维重建'}
{'arxiv_id': 'arXiv:2511.00370', 'title': 'Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict', 'authors': 'Chaochen Wu, Guan Luo, Meiyun Zuo, Zhitao Fan', 'link': 'https://arxiv.org/abs/2511.00370', 'abstract': "Video moment retrieval uses a text query to locate a moment from a given untrimmed video reference. Locating corresponding video moments with text queries helps people interact with videos efficiently. Current solutions for this task have not considered conflict within location results from different models, so various models cannot integrate correctly to produce better results. This study introduces a reinforcement learning-based video moment retrieval model that can scan the whole video once to find the moment's boundary while producing its locational evidence. Moreover, we proposed a multi-agent system framework that can use evidential learning to resolve conflicts between agents' localization output. As a side product of observing and dealing with conflicts between agents, we can decide whether a query has no corresponding moment in a video (out-of-scope) without additional training, which is suitable for real-world applications. Extensive experiments on benchmark datasets show the effectiveness of our proposed methods compared with state-of-the-art approaches. Furthermore, the results of our study reveal that modeling competition and conflict of the multi-agent system is an effective way to improve RL performance in moment retrieval and show the new role of evidential learning in the multi-agent framework.", 'abstract_zh': '基于强化学习的视频瞬间检索模型及多智能体系统框架研究：处理模型间位置结果冲突', 'title_zh': '谁值得信任？基于多agent冲突的范围感知视频片段检索'}
{'arxiv_id': 'arXiv:2511.00362', 'title': 'Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery', 'authors': 'Momen Khandoker Ope, Akif Islam, Mohd Ruhul Ameen, Abu Saleh Musa Miah, Md Rashedul Islam, Jungpil Shin', 'link': 'https://arxiv.org/abs/2511.00362', 'abstract': "Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise. Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts. As a result, many of Bangladesh's architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a cost-free generative AI framework that democratizes 3D cultural preservation. By using publicly available Google Street View imagery, Oitijjo-3D reconstructs faithful 3D models of heritage structures through a two-stage pipeline - multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery. The system produces photorealistic, metrically coherent reconstructions in seconds, achieving significant speedups compared to conventional Structure-from-Motion pipelines, without requiring any specialized hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil, Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both visual and structural fidelity while drastically lowering economic and technical barriers. By turning open imagery into digital heritage, this work reframes preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.", 'abstract_zh': '孟加拉国的文化遗产修复面临着资源有限和技术专家稀缺的双重挑战。传统的3D数字化方法，如摄影测量或LiDAR扫描，需要昂贵的硬件、专家操作人员和广泛的现场访问权限，这些在发展中国家往往不可行。因此，许多孟加拉国的建筑瑰宝，从帕哈尔普尔佛教 monastery 到阿罕曼札尔，仍面临decay威胁，并且在数字形式上难以访问。本文介绍了Oitijjo-3D，一种自由的生成AI框架，使3D文化保护民主化。通过使用公开的谷歌街景图像，Oitijjo-3D通过两阶段流水线重构忠实的3D模型——多模式视觉推理与Gemini 2.5 Flash Image进行结构-纹理合成，以及通过Hexagen进行神经图像到3D的生成以实现几何恢复。该系统在几秒钟内生成逼真、度量协调的重建模型，与传统的立体声重建流水线相比，实现了重大加速，而无需任何专门硬件或专家监督。实验表明，Oitijjo-3D在降低经济和技术门槛的同时，保持了视觉和结构的准确性。通过将开放图像转变为数字遗产，这项工作重新定义了保护，使其成为资源有限国家社区驱动的、AI辅助的文化延续行动。', 'title_zh': 'Oitijjo-3D：基于街景图像的快速三维文化遗产重建生成AI框架'}
{'arxiv_id': 'arXiv:2511.00352', 'title': 'Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach', 'authors': 'Mohd Ruhul Ameen, Akif Islam', 'link': 'https://arxiv.org/abs/2511.00352', 'abstract': 'The rapid rise of generative diffusion models has made distinguishing authentic visual content from synthetic imagery increasingly challenging. Traditional deepfake detection methods, which rely on frequency or pixel-level artifacts, fail against modern text-to-image systems such as Stable Diffusion and DALL-E that produce photorealistic and artifact-free results. This paper introduces a diffusion-based forensic framework that leverages multi-strength image reconstruction dynamics, termed diffusion snap-back, to identify AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and PSNR) evolve across varying noise strengths, we extract interpretable manifold-based features that differentiate real and synthetic images. Evaluated on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under cross-validation and remains robust to common distortions such as compression and noise. Despite using limited data and a single diffusion backbone (Stable Diffusion v1.5), the proposed method demonstrates strong generalization and interpretability, offering a foundation for scalable, model-agnostic synthetic media forensics.', 'abstract_zh': '生成扩散模型的快速崛起使鉴别真实视觉内容与合成图像越来越具挑战性。传统的基于频率或像素级特征的深仿鉴定方法无法应对诸如Stable Diffusion和DALL-E等现代文本到图像系统生成的逼真且无伪影的结果。本文介绍了一种基于生成的法医框架，利用多强度图像重建动力学，即扩散反弹效应，来识别AI生成的图像。通过分析在不同噪声强度下重建指标（LPIPS、SSIM和PSNR）的变化，我们提取出可以区分真实和合成图像的可解释流形特征。在包含4,000张图像的平衡数据集上，我们的方法在交叉验证下达到0.993的AUROC，并且对常见的压缩和噪声等失真具有鲁棒性。尽管使用的是有限的数据和单一的生成扩散模型（Stable Diffusion v1.5），所提出的方法展示了强大的泛化能力和可解释性，为可扩展的、模型无关的合成媒体法医学奠定了基础。', 'title_zh': '通过扩散瞬回重构检测AI生成图像：一种取证方法'}
{'arxiv_id': 'arXiv:2511.00218', 'title': 'DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy', 'authors': 'Rajatsubhra Chakraborty, Ana Espinosa-Momox, Riley Haskin, Depeng Xu, Rosario Porras-Aguilar', 'link': 'https://arxiv.org/abs/2511.00218', 'abstract': "Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces challenges from traditional thresholding methods that are sensitive to noise and cell density, while deep learning approaches using simple channel concatenation fail to exploit the complementary nature of polarized intensity images and phase maps. We introduce DM-QPMNet, a dual-encoder network that treats these as distinct modalities with separate encoding streams. Our architecture fuses modality-specific features at intermediate depth via multi-head attention, enabling polarized edge and texture representations to selectively integrate complementary phase information. This content-aware fusion preserves training stability while adding principled multi-modal integration through dual-source skip connections and per-modality normalization at minimal overhead. Our approach demonstrates substantial improvements over monolithic concatenation and single-modality baselines, showing that modality-specific encoding with learnable fusion effectively exploits ssQPM's simultaneous capture of complementary illumination and phase cues for robust cell segmentation.", 'abstract_zh': '双编码器网络在单 shot 定量相位显微镜中的细胞分割：DM-QPMNet 面对传统阈值方法对噪声和细胞密度敏感性的挑战，以及简单通道串联的深度学习方法未能充分利用偏振强度图像和相位图的互补性质。', 'title_zh': 'DM-QPMNET：双模态融合网络在定量相位显微镜细胞分割中的应用'}
{'arxiv_id': 'arXiv:2511.00141', 'title': 'FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding', 'authors': 'Janghoon Cho, Jungsoo Lee, Munawar Hayat, Kyuwoong Hwang, Fatih Porikli, Sungha Choi', 'link': 'https://arxiv.org/abs/2511.00141', 'abstract': 'Recent studies in long video understanding have harnessed the advanced visual-language reasoning capabilities of Large Multimodal Models (LMMs), driving the evolution of video-LMMs specialized for processing extended video sequences. However, the scalability of these models is severely limited by the overwhelming volume of visual tokens generated from extended video sequences. To address this challenge, this paper proposes FLoC, an efficient visual token compression framework based on the facility location function, a principled approach that swiftly selects a compact yet highly representative and diverse subset of visual tokens within a predefined budget on the number of visual tokens. By integrating the lazy greedy algorithm, our method achieves remarkable efficiency gains by swiftly selecting a compact subset of tokens, drastically reducing the number of visual tokens while guaranteeing near-optimal performance. Notably, our approach is training-free, model-agnostic, and query-agnostic, providing a versatile solution that seamlessly integrates with diverse video-LLMs and existing workflows. Extensive evaluations on large-scale benchmarks, such as Video-MME, MLVU, and LongVideoBench, demonstrate that our framework consistently surpasses recent compression techniques, highlighting not only its effectiveness and robustness in addressing the critical challenges of long video understanding, but also its efficiency in processing speed.', 'abstract_zh': 'Recent Studies in Long Video Understanding Have Harnessed the Advanced Visual-Language Reasoning Capabilities of Large Multimodal Models (LMMs), Driving the Evolution of Video-LMMs Specialized for Processing Extended Video Sequences: An Efficient Visual Token Compression Framework named FLoC Based on Facility Location Function', 'title_zh': '基于设施位置的高效视觉-token 压缩方法用于长视频理解'}
{'arxiv_id': 'arXiv:2511.00120', 'title': 'VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images', 'authors': 'Md Selim Sarowar, Sungho Kim', 'link': 'https://arxiv.org/abs/2511.00120', 'abstract': 'The primary challenge in computer vision is precisely calculating the pose of 6D objects, however many current approaches are still fragile and have trouble generalizing from synthetic data to real-world situations with fluctuating lighting, textureless objects, and significant occlusions. To address these limitations, VLM6D, a novel dual-stream architecture that leverages the distinct strengths of visual and geometric data from RGB-D input for robust and precise pose estimation. Our framework uniquely integrates two specialized encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the RGB modality, harnessing its rich, pre-trained understanding of visual grammar to achieve remarkable resilience against texture and lighting variations. Concurrently, a PointNet++ encoder processes the 3D point cloud derived from depth data, enabling robust geometric reasoning that excels even with the sparse, fragmented data typical of severe occlusion. These complementary feature streams are effectively fused to inform a multi task prediction head. We demonstrate through comprehensive experiments that VLM6D obtained new SOTA performance on the challenging Occluded-LineMOD, validating its superior robustness and accuracy.', 'abstract_zh': '计算机视觉中的主要挑战是精确计算6D物体的姿态，然而，当前许多方法在从合成数据推广到具有波动光照、无纹理物体和显著遮挡的实际应用场景时仍然不够 robust。为解决这些问题，我们提出了 VLM6D，这是一种新颖的双流架构，利用 RGB-D 输入中视觉和几何数据的独特优势，实现稳健且精确的姿态估计。我们的框架独特地整合了两种专门的编码器：强大的自监督视觉变换器（DINOv2）处理 RGB 模态，利用其丰富的先验视觉语法理解，显著增强对纹理和光照变化的鲁棒性。同时，PointNet++ 编码器处理从深度数据派生的 3D 点云，支持即使在严重遮挡情况下数据稀疏且破碎的稳健几何推理。这两种互补特征流有效融合以指导多任务预测头。通过全面的实验，我们证明 VLM6D 在具有挑战性的 Occluded-LineMOD 上获得了新的 SOTA 性能，验证了其卓越的稳健性和准确性。', 'title_zh': 'VLM6D: 基于VLM的RGB-D图像六自由度姿态估计'}
{'arxiv_id': 'arXiv:2511.00110', 'title': 'Chain of Time: In-Context Physical Simulation with Image Generation Models', 'authors': 'YingQiao Wang, Eric Bigelow, Boyi Li, Tomer Ullman', 'link': 'https://arxiv.org/abs/2511.00110', 'abstract': 'We propose a novel cognitively-inspired method to improve and interpret physical simulation in vision-language models. Our ``Chain of Time" method involves generating a series of intermediate images during a simulation, and it is motivated by in-context reasoning in machine learning, as well as mental simulation in humans. Chain of Time is used at inference time, and requires no additional fine-tuning. We apply the Chain-of-Time method to synthetic and real-world domains, including 2-D graphics simulations and natural 3-D videos. These domains test a variety of particular physical properties, including velocity, acceleration, fluid dynamics, and conservation of momentum. We found that using Chain-of-Time simulation substantially improves the performance of a state-of-the-art image generation model. Beyond examining performance, we also analyzed the specific states of the world simulated by an image model at each time step, which sheds light on the dynamics underlying these simulations. This analysis reveals insights that are hidden from traditional evaluations of physical reasoning, including cases where an image generation model is able to simulate physical properties that unfold over time, such as velocity, gravity, and collisions. Our analysis also highlights particular cases where the image generation model struggles to infer particular physical parameters from input images, despite being capable of simulating relevant physical processes.', 'abstract_zh': '一种认知启发式的新型方法以提高和解释vision-language模型中的物理模拟', 'title_zh': '时间链：基于图像生成模型的上下文物理模拟'}
{'arxiv_id': 'arXiv:2511.00098', 'title': 'A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning', 'authors': 'Nils Porsche, Flurin Müller-Diesing, Sweta Banerjee, Miguel Goncalves, Marc Aubreville', 'link': 'https://arxiv.org/abs/2511.00098', 'abstract': 'Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging modality that can be used for in-situ, in-vivo imaging and the microstructural analysis of mucous structures. The diagnosis using CLE is, however, complicated by images being hard to interpret for non-experienced physicians. Utilizing machine learning as an augmentative tool would hence be beneficial, but is complicated by the shortage of histopathology-correlated CLE imaging sequences with respect to the plurality of patterns in this domain, leading to overfitting of machine learning models. To overcome this, self-supervised learning (SSL) can be employed on larger unlabeled datasets. CLE is a video-based modality with high inter-frame correlation, leading to a non-stratified data distribution for SSL training. In this work, we propose a filter functionality on CLE video sequences to reduce the dataset redundancy in SSL training and improve SSL training convergence and training efficiency. We use four state-of-the-art baseline networks and a SSL teacher-student network with a vision transformer small backbone for the evaluation. These networks were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous cell carcinoma of the skin dataset. On both datasets, we found the highest test accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both considerably outperforming their non-SSL baselines. Our results show that SSL is an effective method for CLE pretraining. Further, we show that our proposed CLE video filter can be utilized to improve training efficiency in self-supervised scenarios, resulting in a reduction of 67% in training time.', 'abstract_zh': '共焦激光内微镜（CLE）是一种非侵入性的实时成像技术，可用于黏膜结构的原位、活体成像及微观结构分析。然而，CLE的诊断具有挑战性，因为非经验医生难以解读其图像。利用机器学习作为辅助工具会有益处，但受限于与组织病理学相关联的CLE图像序列不足，且该领域存在多种模式，导致机器学习模型过拟合。为克服这一问题，可以在更大规模的未标注数据集上应用自我监督学习（SSL）。CLE是一种基于视频的成像模式，具有高帧间相关性，导致SSL训练的数据分布非分层。在本研究中，我们提出了一种对CLE视频序列进行过滤的功能，以减少SSL训练中的数据冗余，提高SSL训练的收敛性和训练效率。我们使用四种最先进的基准网络以及具有视觉变压器小骨干网的教师-学生SSL网络进行评估。这些网络在鼻窦肿瘤数据集和皮肤鳞状细胞癌数据集的下游任务中进行了评估。在两个数据集上，我们发现过滤后的SSL预训练模型的最高测试准确率为67.48%和73.52%，均显著优于其非SSL基准模型。我们的结果显示，SSL是CLE预训练的有效方法。此外，我们展示了我们提出的CLE视频滤波器可以在自我监督场景中提高训练效率，训练时间减少67%。', 'title_zh': '用于自监督学习的共焦激光内窥镜(CLE)-视频序列过滤方案'}
{'arxiv_id': 'arXiv:2511.00090', 'title': 'LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation', 'authors': 'Huanlin Gao, Ping Chen, Fuyuan Shi, Chao Tan, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian', 'link': 'https://arxiv.org/abs/2511.00090', 'abstract': 'We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :this https URL', 'abstract_zh': 'LeMiCa：一种无训练高效加速的基于扩散的视频生成框架', 'title_zh': 'LeMiCa: 词典序最小最大路径缓存技术及其在高效基于扩散的视频生成中的应用'}
{'arxiv_id': 'arXiv:2511.00072', 'title': 'LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks', 'authors': 'Pradeep M, Ritesh Pallod, Satyen Abrol, Muthu Raman, Ian Anderson', 'link': 'https://arxiv.org/abs/2511.00072', 'abstract': 'Generative AI is reshaping fashion by enabling virtual looks and avatars making it essential to find real products that best match AI-generated styles. We propose an end-to-end product search system that has been deployed in a real-world, internet scale which ensures that AI-generated looks presented to users are matched with the most visually and semantically similar products from the indexed vector space. The search pipeline is composed of four key components: query generation, vectorization, candidate retrieval, and reranking based on AI-generated looks. Recommendation quality is evaluated using human-judged accuracy scores. The system currently serves more than 350,000 AI Looks in production per day, covering diverse product categories across global markets of over 12 million products. In our experiments, we observed that across multiple annotators and categories, CLIP outperformed alternative models by a small relative margin of 3--7\\% in mean opinion scores. These improvements, though modest in absolute numbers, resulted in noticeably better user perception matches, establishing CLIP as the most reliable backbone for production deployment.', 'abstract_zh': '生成式AI正在通过启用虚拟外观和化身重塑时尚，因此找到与AI生成风格最佳匹配的真实产品变得至关重要。我们提出了一套端到端的产品搜索系统，该系统已部署在互联网规模的实际环境中，确保呈现在用户面前的AI生成外观与索引向量空间中最具视觉和语义相似性的产品相匹配。搜索管道由四个关键组件组成：查询生成、向量化、候选检索以及基于AI生成外观的再排序。推荐质量通过人类判断准确率评分进行评估。系统目前每天为超过350,000个AI外观提供服务，涵盖全球市场上超过1200万产品的各类产品。在我们的实验中，我们观察到，相较于替代模型，CLIP在多个注释员和类别中的平均意见得分上相对超出3-7%，尽管绝对值上的改进有限，但这些改进明显改善了用户感知匹配度，确立了CLIP作为生产部署中最可靠的基础模型。', 'title_zh': 'LookSync：大规模AI生成时尚LOOKs的可视化产品搜索系统'}
{'arxiv_id': 'arXiv:2511.00021', 'title': 'Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets', 'authors': 'Julio Jerison E. Macrohon, Gordon Hung', 'link': 'https://arxiv.org/abs/2511.00021', 'abstract': 'Coral reefs support numerous marine organisms and are an important source of coastal protection from storms and floods, representing a major part of marine ecosystems. However coral reefs face increasing threats from pollution, ocean acidification, and sea temperature anomalies, making efficient protection and monitoring heavily urgent. Therefore, this study presents a novel machine-learning-based coral bleaching classification system based on a diverse global dataset with samples of healthy and bleached corals under varying environmental conditions, including deep seas, marshes, and coastal zones. We benchmarked and compared three state-of-the-art models: Residual Neural Network (ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN). After comprehensive hyperparameter tuning, the CNN model achieved the highest accuracy of 88%, outperforming existing benchmarks. Our findings offer important insights into autonomous coral monitoring and present a comprehensive analysis of the most widely used computer vision models.', 'abstract_zh': 'coral礁支持众多海洋生物并在抵御风暴和洪水方面起到重要保护作用，是海洋生态系统的重要组成部分。然而，珊瑚礁正面临越来越多的威胁，包括污染、海洋酸化和海温异常，因此高效的保护和监测变得极为迫切。因此，本研究提出了一种基于多样化全球数据集的新型机器学习珊瑚白化分类系统，该数据集包含了在不同环境条件下（包括深海、沼泽和海岸区）的健康和白化珊瑚样本。我们基准测试并比较了三种最先进的模型：残差神经网络（ResNet）、视觉变换器（ViT）和卷积神经网络（CNN）。经过全面的超参数调优，卷积神经网络模型达到了最高准确率88%，超过了现有基准。我们的研究为自动珊瑚监测提供了重要的见解，并对广泛使用的计算机视觉模型进行了全面分析。', 'title_zh': '多条件 underwater 图像数据集中的珊瑚白化分类的深度学习模型'}
