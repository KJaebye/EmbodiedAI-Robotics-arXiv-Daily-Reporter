{'arxiv_id': 'arXiv:2511.01581', 'title': 'ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks', 'authors': 'Chengzhang Yu, Zening Lu, Chenyang Zheng, Chiyue Wang, Yiming Zhang, Zhanpeng Jin', 'link': 'https://arxiv.org/abs/2511.01581', 'abstract': 'Large language models suffer from knowledge staleness and lack of interpretability due to implicit knowledge storage across entangled network parameters, preventing targeted updates and reasoning transparency. We propose ExplicitLM, a novel architecture featuring a million-scale external memory bank storing human-readable knowledge as token sequences, enabling direct inspection and modification. We design a differentiable two-stage retrieval mechanism with efficient coarse-grained filtering via product key decomposition (reducing complexity from $\\mathcal{O}(N \\cdot |I|)$ to $\\mathcal{O}(\\sqrt{N} \\cdot |I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training. Inspired by dual-system cognitive theory, we partition knowledge into frozen explicit facts (20%) and learnable implicit patterns (80%), maintained through Exponential Moving Average updates for stability. ExplicitLM achieves up to 43.67% improvement on knowledge-intensive tasks versus standard Transformers, with 3.62$\\times$ gains in low-data regimes (10k samples). Analysis shows strong correlations between memory retrieval and performance, with correct predictions achieving 49% higher hit rates. Unlike RAG systems with frozen retrieval, our jointly optimized architecture demonstrates that interpretable, updatable models can maintain competitive performance while providing unprecedented knowledge transparency.', 'abstract_zh': 'ExplicitLM：一种具备显式外部记忆的知识透明可更新语言模型', 'title_zh': 'ExplicitLM：通过明确的记忆库解耦知识与参数'}
{'arxiv_id': 'arXiv:2511.01527', 'title': "TPS-Bench: Evaluating AI Agents' Tool Planning \\& Scheduling Abilities in Compounding Tasks", 'authors': 'Hanwen Xu, Xuyao Huang, Yuzhe Liu, Kai Yu, Zhijie Deng', 'link': 'https://arxiv.org/abs/2511.01527', 'abstract': 'Large language model (LLM) agents have exhibited strong problem-solving competence across domains like research and coding. Yet, it remains underexplored whether LLM agents can tackle compounding real-world problems that require a diverse set of tools to complete. Given a broad, heterogeneous tool repository, LLM agents must not only select appropriate tools based on task planning analysis but also strategically schedule the execution order to ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of LLM agents in solving such problems that demand Tool Planning and Scheduling. TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a tool repository containing hundreds of model context protocol (MCP) tools. In particular, each task is composed of multiple subtasks, such as web search, map navigation, calendar checking, etc., and each subtask can be completed by a basic tool. Our evaluation emphasizes both task completion rate and efficiency. The empirical studies on popular closed-source and open-source LLMs indicate that most models can perform reasonable tool planning, but differ in scheduling. For example, GLM-4.5 achieves an outperforming task completion rate of 64.72% with extensive sequential tool calls, hence suffering from significantly long execution time. By contrast, GPT-4o prioritizes parallel tool calls but achieves only a 45.08% completion rate. Considering reinforcement learning (RL) can be a viable way to improve the scheduling efficiency without compromising performance, we perform an initial study on Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in task completion rate based on rarely 100 RL training samples. Our code is available this https URL.', 'abstract_zh': '大型语言模型（LLM）代理在研究和编码等领域展示了强大的问题解决能力。然而，尚未充分探索LLM代理是否能够应对需要多种工具组合解决的复杂现实世界问题。基于一个广泛的异构工具库，LLM代理不仅必须根据任务规划分析选择合适的工具，还需要战略性地安排执行顺序，以确保效率。本文引入TPS-Bench，用于评估LLM代理解决需要工具规划和调度的问题的能力。TPS-Bench收集了200个不同难度级别的复合任务，基于包含数百个模型上下文协议（MCP）工具的工具库。特别是，每个任务由多个子任务组成，如网络搜索、地图导航、日历查询等，每个子任务都可以由一个基本工具来完成。评估重点关注任务完成率和效率。对流行的闭源和开源LLM的实证研究表明，大多数模型可以进行合理的工具规划，但在调度上有所不同。例如，GLM-4.5通过大量连续调用工具实现了64.72%的任务完成率，因此执行时间显著延长。相比之下，GPT-4o优先考虑并行工具调用，但只实现了45.08%的任务完成率。考虑到强化学习（RL）可以在不牺牲性能的情况下提高调度效率，我们对Qwen3-1.7B进行了初步研究，并基于少量100个RL训练样本观察到执行时间减少了14%，任务完成率提高了6%。我们的代码在此处可用：https://github.com/alibaba/Qwen。', 'title_zh': 'TPS-Bench: 评估AI代理在复合任务中工具规划与调度能力'}
{'arxiv_id': 'arXiv:2511.01375', 'title': 'Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges', 'authors': 'Hamin Koo, Minseon Kim, Jaehyung Kim', 'link': 'https://arxiv.org/abs/2511.01375', 'abstract': 'Identifying the vulnerabilities of large language models (LLMs) is crucial for improving their safety by addressing inherent weaknesses. Jailbreaks, in which adversaries bypass safeguards with crafted input prompts, play a central role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors. Recent optimization-based jailbreak approaches iteratively refine attack prompts by leveraging LLMs. However, they often rely heavily on either binary attack success rate (ASR) signals, which are sparse, or manually crafted scoring templates, which introduce human bias and uncertainty in the scoring outcomes. To address these limitations, we introduce AMIS (Align to MISalign), a meta-optimization framework that jointly evolves jailbreak prompts and scoring templates through a bi-level structure. In the inner loop, prompts are refined using fine-grained and dense feedback using a fixed scoring template. In the outer loop, the template is optimized using an ASR alignment score, gradually evolving to better reflect true attack outcomes across queries. This co-optimization process yields progressively stronger jailbreak prompts and more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors demonstrate that AMIS achieves state-of-the-art performance, including 88.0% ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming existing baselines by substantial margins.', 'abstract_zh': '识别大型语言模型的漏洞对于通过解决固有弱点来提高其安全性至关重要。边界攻击，其中攻击者通过精心设计的输入提示绕过安全防护，在红队测试中通过探索语言模型来诱发意外或不安全的行为，扮演着核心角色。最近基于优化的边界攻击方法通过利用语言模型逐代细化攻击提示。然而，它们通常要么严重依赖二元攻击成功率（ASR）信号，这些信号稀疏，要么依赖手工设计的评分模板，这引入了评分结果中的人为偏差和不确定性。为了解决这些局限性，我们提出了AMIS（对齐以偏离），这是一种元优化框架，通过双层结构联合进化边界攻击提示和评分模板。内层循环中，使用固定评分模板的精细和密集反馈来细化提示。外层循环中，通过ASR对齐得分来优化模板，逐步进化以更好地反映查询的真实攻击结果。这一联合优化过程产出了更强的边界攻击提示，并提供了更准确的评分信号。在AdvBench和JBB-Behaviors上的评估表明，AMIS达到了最先进的性能，包括在Claude-3.5-Haiku上的88.0% ASR和在Claude-4-Sonnet上的100.0% ASR，显著优于现有基线。', 'title_zh': '对齐以去对齐：基于元优化语言模型法官的自动大模型脱管'}
{'arxiv_id': 'arXiv:2511.01363', 'title': 'Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing', 'authors': 'Giuseppe Riva, Brenda K. Wiederhold, Fabrizia Mantovani', 'link': 'https://arxiv.org/abs/2511.01363', 'abstract': 'The cognitive processes of the hypnotized mind and the computational operations of large language models (LLMs) share deep functional parallels. Both systems generate sophisticated, contextually appropriate behavior through automatic pattern-completion mechanisms operating with limited or unreliable executive oversight. This review examines this convergence across three principles: automaticity, in which responses emerge from associative rather than deliberative processes; suppressed monitoring, leading to errors such as confabulation in hypnosis and hallucination in LLMs; and heightened contextual dependency, where immediate cues (for example, the suggestion of a therapist or the prompt of the user) override stable knowledge.\nThese mechanisms reveal an observer-relative meaning gap: both systems produce coherent but ungrounded outputs that require an external interpreter to supply meaning. Hypnosis and LLMs also exemplify functional agency - the capacity for complex, goal-directed, context-sensitive behavior - without subjective agency, the conscious awareness of intention and ownership that defines human action. This distinction clarifies how purposive behavior can emerge without self-reflective consciousness, governed instead by structural and contextual dynamics. Finally, both domains illuminate the phenomenon of scheming: automatic, goal-directed pattern generation that unfolds without reflective awareness. Hypnosis provides an experimental model for understanding how intention can become dissociated from conscious deliberation, offering insights into the hidden motivational dynamics of artificial systems. Recognizing these parallels suggests that the future of reliable AI lies in hybrid architectures that integrate generative fluency with mechanisms of executive monitoring, an approach inspired by the complex, self-regulating architecture of the human mind.', 'abstract_zh': '被催眠大脑的认知过程与大型语言模型的计算操作在功能上存在深刻的相似性。这两者系统通过自动模式补充机制生成复杂且上下文适配的行为，操作中受限于有限或不可靠的执行监管。本文回顾了这一交汇点，涵盖了三个原则：自动性、抑制监控以及增强的上下文依赖性。', 'title_zh': '自动思维：催眠状态与大型语言模型处理的认知parallelisms'}
{'arxiv_id': 'arXiv:2511.01311', 'title': 'llmSHAP: A Principled Approach to LLM Explainability', 'authors': 'Filip Naudot, Tobias Sundqvist, Timotheus Kampik', 'link': 'https://arxiv.org/abs/2511.01311', 'abstract': "Feature attribution methods help make machine learning-based inference explainable by determining how much one or several features have contributed to a model's output. A particularly popular attribution method is based on the Shapley value from cooperative game theory, a measure that guarantees the satisfaction of several desirable principles, assuming deterministic inference. We apply the Shapley value to feature attribution in large language model (LLM)-based decision support systems, where inference is, by design, stochastic (non-deterministic). We then demonstrate when we can and cannot guarantee Shapley value principle satisfaction across different implementation variants applied to LLM-based decision support, and analyze how the stochastic nature of LLMs affects these guarantees. We also highlight trade-offs between explainable inference speed, agreement with exact Shapley value attributions, and principle attainment.", 'abstract_zh': '基于特征归因方法使大型语言模型（LLM）驱动的决策支持系统中的推断具有解释性：波动性对沙普利值原则满足的影响及权衡', 'title_zh': 'llmSHAP: 一种原理性的大型语言模型可解释性方法'}
{'arxiv_id': 'arXiv:2511.01183', 'title': 'QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code', 'authors': 'Hainan Fang, Yuanbo Wen, Jun Bi, Yihan Wang, Tonghui He, Yanlin Tang, Di Huang, Jiaming Guo, Rui Zhang, Qi Guo, Yunji Chen', 'link': 'https://arxiv.org/abs/2511.01183', 'abstract': 'Compilers, while essential, are notoriously complex systems that demand prohibitively expensive human expertise to develop and maintain. The recent advancements in Large Language Models (LLMs) offer a compelling new paradigm: Neural Compilation, which could potentially simplify compiler development for new architectures and facilitate the discovery of innovative optimization techniques. However, several critical obstacles impede its practical adoption. Firstly, a significant lack of dedicated benchmarks and robust evaluation methodologies hinders objective assessment and tracking of progress in the field. Secondly, systematically enhancing the reliability and performance of LLM-generated assembly remains a critical challenge. Addressing these challenges, this paper introduces NeuComBack, a novel benchmark dataset specifically designed for IR-to-assembly compilation. Leveraging this dataset, we first define a foundational Neural Compilation workflow and conduct a comprehensive evaluation of the capabilities of recent frontier LLMs on Neural Compilation, establishing new performance baselines. We further propose a self-evolving prompt optimization method that enables LLMs to iteratively evolve their internal prompt strategies by extracting insights from prior self-debugging traces, thereby enhancing their neural compilation capabilities. Experiments demonstrate that our method significantly improves both the functional correctness and the performance of LLM-generated assembly code. Compared to baseline prompts, the functional correctness rates improved from 44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More significantly, among the 16 correctly generated x86_64 programs using our method, 14 (87.5%) surpassed clang-O3 performance.', 'abstract_zh': '编译器虽然至关重要，但通常非常复杂，需要大量昂贵的人力专业知识来开发和维护。最近的大语言模型（LLMs）进步为“神经编译”提供了一个引人的新范式，这有可能简化对新架构的编译器开发，并促进创新优化技术的发现。然而，几个关键障碍阻碍了其实用性采用。首先，缺乏专门的基准测试和稳健的评估方法阻碍了该领域进展的客观评估和跟踪。其次，系统地提升由LLM生成的汇编代码的可靠性和性能仍然是一个关键挑战。为了解决这些挑战，本文引入了NeuComBack，一个专门用于中间表示到汇编编译的新基准数据集。利用该数据集，我们首先定义了一个基础的神经编译工作流程，并对最近前沿的LLM在神经编译方面的能力进行了全面评估，建立了新的性能基线。我们进一步提出了一种自演进提示优化方法，使LLM能够通过从先前自我调试跟踪中提取见解来迭代演化其内部提示策略，从而增强其神经编译能力。实验结果表明，我们的方法显著提高了LLM生成的汇编代码的功能正确性和性能。与基线提示相比，x86_64上的功能正确率从44%提高到64%，aarch64上的功能正确率从36%提高到58%。更重要的是，使用我们的方法生成的16个正确x86_64程序中，有14个（87.5%）超过了clang-O3的性能。', 'title_zh': 'QiMeng-NeuComBack: 自适应演化从中间表示到汇编代码的翻译'}
{'arxiv_id': 'arXiv:2511.01170', 'title': 'DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models', 'authors': 'Ruofan Zhang, Bin Xia, Zhen Cheng, Cairen Jian, Minglun Yang, Ngai Wong, Yuan Cheng', 'link': 'https://arxiv.org/abs/2511.01170', 'abstract': "Adaptive reasoning is essential for aligning the computational effort of large language models (LLMs) with the intrinsic difficulty of problems. Current chain-of-thought methods boost reasoning ability but indiscriminately generate long explanations, leading to evident inefficiency. However, existing reinforcement learning approaches to adaptive thinking remain unstable and heavily reward-dependent. Here we propose \\textbf{DART}, a supervised \\textbf{D}ifficulty-\\textbf{A}daptive \\textbf{R}easoning \\textbf{T}runcation framework that adjusts thinking length according to problem difficulty. By distilling concise reasoning patterns from stronger models, interpolating them into a continuum of reasoning styles, and curating optimal training data that balances correctness and compactness, DART learns when to ``stop thinking''. Across multiple mathematical benchmarks, experimental results demonstrate its remarkable efficiency while preserving or improving accuracy, achieving a significant 81.2\\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K dataset) with 5.33$\\times$ computational acceleration. DART provides a stable and general paradigm for efficient reasoning, advancing the development of adaptive intelligence in LLMs.", 'abstract_zh': '适应性推理对于调整大型语言模型的计算 effort 与问题固有难度相一致至关重要。当前的链式思考方法提升了推理能力，但会不加区别地生成长解释，导致明显的效率低下。然而，现有的基于强化学习的适应性思考方法仍然不稳定且高度依赖奖励。我们提出了一种监督学习框架 \\textbf{DART}，即 \\textbf{D}ifficulty-\\textbf{A}daptive \\textbf{R}easoning \\textbf{T}runcation，可以根据问题难度调整思考长度。通过从更强的模型中提炼精炼的推理模式、在推理风格连续体中进行插值，并精心挑选平衡正确性和紧凑性的训练数据，DART 学习何时“停止思考”。在多个数学基准测试中，实验结果证明其具有显著的效率，同时保持或提高了准确性，实现了高达 81.2% 的推理裁剪（DeepSeek-R1-Distill-Qwen-7B 在 GSM8K 数据集上的结果），并获得 5.33 倍的计算加速。DART 为高效推理提供了一个稳定且通用的范式，推动了适应性智能在大语言模型中的发展。', 'title_zh': 'DART：难度自适应推理裁剪以实现高效大型语言模型'}
{'arxiv_id': 'arXiv:2511.01149', 'title': 'Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models', 'authors': 'Shuaidong Pan, Di Wu', 'link': 'https://arxiv.org/abs/2511.01149', 'abstract': 'This paper addresses the limitations of a single agent in task decomposition and collaboration during complex task execution, and proposes a multi-agent architecture for modular task decomposition and dynamic collaboration based on large language models. The method first converts natural language task descriptions into unified semantic representations through a large language model. On this basis, a modular decomposition mechanism is introduced to break down the overall goal into multiple hierarchical sub-tasks. Then, dynamic scheduling and routing mechanisms enable reasonable division of labor and realtime collaboration among agents, allowing the system to adjust strategies continuously according to environmental feedback, thus maintaining efficiency and stability in complex tasks. Furthermore, a constraint parsing and global consistency mechanism is designed to ensure coherent connections between sub-tasks and balanced workload, preventing performance degradation caused by redundant communication or uneven resource allocation. The experiments validate the architecture across multiple dimensions, including task success rate, decomposition efficiency, sub-task coverage, and collaboration balance. The results show that the proposed method outperforms existing approaches in both overall performance and robustness, achieving a better balance between task complexity and communication overhead. In conclusion, this study demonstrates the effectiveness and feasibility of language-driven task decomposition and dynamic collaboration in multi-agent systems, providing a systematic solution for task execution in complex environments.', 'abstract_zh': '基于大型语言模型的模块化任务分解与动态协作多agent架构', 'title_zh': '由大规模语言模型驱动的多Agent系统中的模块化任务分解与动态协作'}
{'arxiv_id': 'arXiv:2511.01059', 'title': 'Efficient Test-Time Retrieval Augmented Generation', 'authors': 'Hailong Yin, Bin Zhu, Jingjing Chen, Chong-Wah Ngo', 'link': 'https://arxiv.org/abs/2511.01059', 'abstract': 'Although Large Language Models (LLMs) demonstrate significant capabilities, their reliance on parametric knowledge often leads to inaccuracies. Retrieval Augmented Generation (RAG) mitigates this by incorporating external knowledge, but these methods may introduce irrelevant retrieved documents, leading to inaccurate responses. While the integration methods filter out incorrect answers from multiple responses, but lack external knowledge like RAG methods, and their high costs require balancing overhead with performance gains. To address these issues, we propose an Efficient Test-Time Retrieval-Augmented Generation Framework named ET2RAG to improve the performance of LLMs while maintaining efficiency. Specifically, ET2RAG is a training-free method, that first retrieves the most relevant documents and augments the LLMs to efficiently generate diverse candidate responses by managing response length. Then we compute the similarity of candidate responses and employ a majority voting mechanism to select the most suitable response as the final output. In particular, we discover that partial generation is sufficient to capture the key information necessary for consensus calculation, allowing us to effectively perform majority voting without the need for fully generated responses. Thus, we can reach a balance between computational cost and performance by managing the response length for the number of retrieved documents for majority voting. Experimental results demonstrate that ET2RAG significantly enhances performance across three tasks, including open-domain question answering, recipe generation and image captioning.', 'abstract_zh': '尽管大型语言模型（LLMs）表现出显著的能力，但它们对参数知识的依赖往往会导致不准确。检索增强生成（RAG）通过融入外部知识来缓解这一问题，但这些方法可能会引入无关的检索文档，导致不准确的回答。虽然集成方法可以从多个回答中过滤出错误的答案，但缺乏RAG方法的外部知识，且其高成本要求在开销与性能提升之间进行平衡。为解决这些问题，我们提出了一种名为ET2RAG的高效测试时检索增强生成框架，以提高LLMs的性能同时保持高效性。具体而言，ET2RAG 是一种无需训练的方法，首先检索最相关的文档，并通过管理响应长度来增强LLMs，以高效地生成多种候选回答。然后我们计算候选回答的相似性，并采用多数投票机制选择最合适的回答作为最终输出。特别是我们发现部分生成足以捕捉达成共识所需的关键信息，从而使我们能够有效地进行多数投票，无需完整的生成回答。因此，我们可以通过管理检索文档数量和响应长度来在计算成本和性能之间实现平衡。实验结果表明，ET2RAG 在开放式领域问题回答、食谱生成和图像字幕三个任务中显著提升了性能。', 'title_zh': '高效测试时检索增强生成'}
{'arxiv_id': 'arXiv:2511.01052', 'title': 'Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports', 'authors': 'Yeawon Lee, Christopher C. Yang, Chia-Hsuan Chang, Grace Lu-Yao', 'link': 'https://arxiv.org/abs/2511.01052', 'abstract': 'Cancer staging is critical for patient prognosis and treatment planning, yet extracting pathologic TNM staging from unstructured pathology reports poses a persistent challenge. Existing natural language processing (NLP) and machine learning (ML) strategies often depend on large annotated datasets, limiting their scalability and adaptability. In this study, we introduce two Knowledge Elicitation methods designed to overcome these limitations by enabling large language models (LLMs) to induce and apply domain-specific rules for cancer staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses an iterative prompting strategy to derive staging rules directly from unannotated pathology reports, without requiring ground-truth labels. The second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG), employs a variation of RAG where rules are pre-extracted from relevant guidelines in a single step and then applied, enhancing interpretability and avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply broad knowledge learned during pre-training to new tasks. Using breast cancer pathology reports from the TCGA dataset, we evaluate their performance in identifying T and N stages, comparing them against various baseline approaches on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG achieves better performance when ZSCOT inference is less effective. Both methods offer transparent, interpretable interfaces by making the induced rules explicit. These findings highlight the promise of our Knowledge Elicitation methods as scalable, high-performing solutions for automated cancer staging with enhanced interpretability, particularly in clinical settings with limited annotated data.', 'abstract_zh': '癌症分期对于患者的预后和治疗规划至关重要，但从非结构化的病理报告中提取pathologic TNM分期一直是一个持续的挑战。现有的自然语言处理(NLP)和机器学习(ML)策略往往依赖于大规模标注数据集，限制了其扩展性和适应性。在本研究中，我们介绍了两种知识抽取方法，旨在通过使大型语言模型(LLM)诱导和应用特定于领域的规则来克服这些限制，从而实现癌症分期。第一种方法是基于长期记忆的知识抽取（KEwLTM），它使用迭代提示策略直接从非标注的病理报告中推导出分期规则，而无需使用真实标签。第二种方法是检索增强生成的知识抽取（KEwRAG），它采用一种变种的RAG方法，在单一步骤中从相关指南中提取规则并应用这些规则，从而增强可解释性并避免重复检索开销。我们利用LLMs在预训练过程中获得的广泛知识来应用于新的任务。使用TCGA数据集中乳腺癌病理报告，我们评估了这两种方法在识别T和N分期方面的性能，并与两个开源LLM上的多种基线方法进行比较。结果表明，在零-shot推理有效的条件下，KEwLTM优于KEwRAG，而在零-shot推理效果不佳时，KEwRAG表现出更优的性能。两种方法都通过使推导出的规则变得明确提供了透明且可解释的接口。这些发现突显了我们的知识抽取方法作为在标注数据有限的临床环境中具有可扩展性、高性能且可解释的自动化癌症分期解决方案的潜力。', 'title_zh': '使用大型语言模型进行可解释的癌症分期识别的知识萃取'}
{'arxiv_id': 'arXiv:2511.01033', 'title': 'On the Emergence of Induction Heads for In-Context Learning', 'authors': 'Tiberiu Musat, Tiago Pimentel, Lorenzo Noci, Alessandro Stolfo, Mrinmaya Sachan, Thomas Hofmann', 'link': 'https://arxiv.org/abs/2511.01033', 'abstract': 'Transformers have become the dominant architecture for natural language processing. Part of their success is owed to a remarkable capability known as in-context learning (ICL): they can acquire and apply novel associations solely from their input context, without any updates to their weights. In this work, we study the emergence of induction heads, a previously identified mechanism in two-layer transformers that is particularly important for in-context learning. We uncover a relatively simple and interpretable structure of the weight matrices implementing the induction head. We theoretically explain the origin of this structure using a minimal ICL task formulation and a modified transformer architecture. We give a formal proof that the training dynamics remain constrained to a 19-dimensional subspace of the parameter space. Empirically, we validate this constraint while observing that only 3 dimensions account for the emergence of an induction head. By further studying the training dynamics inside this 3-dimensional subspace, we find that the time until the emergence of an induction head follows a tight asymptotic bound that is quadratic in the input context length.', 'abstract_zh': 'Transformer架构已成为自然语言处理的主导架构。它们的部分成功归功于一种非凡的能力——即上下文学习（ICL）能力：它们可以从输入上下文中获取并应用新的关联，而无需更新其权重。在这项工作中，我们研究了诱导头的出现，这是一种在两层变压器中先前被识别的机制，对上下文学习尤为重要。我们揭示了实现诱导头的权重矩阵的一种相对简单且可解释的结构。我们通过最小的ICL任务形式化和修改后的变压器架构对其起源进行了理论解释。我们给出了一种形式证明，表明训练动力学受限于参数空间的19维子空间。通过实验证明这一限制，我们观察到仅有3个维度负责诱导头的出现。通过对这3维子空间内的训练动力学进行进一步研究，我们发现直至诱导头出现的时间遵循一个紧致的二次时间上界，该上界与输入上下文长度的平方成正比。', 'title_zh': '基于上下文学习中归纳头的 emergence'}
{'arxiv_id': 'arXiv:2511.00993', 'title': 'Aligning LLM agents with human learning and adjustment behavior: a dual agent approach', 'authors': 'Tianming Liu, Jirong Yang, Yafeng Yin, Manzi Li, Linghao Wang, Zheng Zhu', 'link': 'https://arxiv.org/abs/2511.00993', 'abstract': "Effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning. However, this task is also difficult due to the complex cognition and decision-making involved in such behavior. Recent research has begun to leverage Large Language Model (LLM) agents for this task. Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams. Our approach involves a set of LLM traveler agents, equipped with a memory system and a learnable persona, which serve as simulators for human travelers. To ensure behavioral alignment, we introduce an LLM calibration agent that leverages the reasoning and analytical capabilities of LLMs to train the personas of these traveler agents. Working together, this dual-agent system is designed to track and align the underlying decision-making mechanisms of travelers and produce realistic, adaptive simulations. Using a real-world dataset from a day-to-day route choice experiment, we show our approach significantly outperforms existing LLM-based methods in both individual behavioral alignment and aggregate simulation accuracy. Furthermore, we demonstrate that our method moves beyond simple behavioral mimicry to capture the evolution of underlying learning processes, a deeper alignment that fosters robust generalization. Overall, our framework provides a new approach for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation that can benefit transportation simulation and policy analysis.", 'abstract_zh': '有效的建模方法对于从与交通系统交互中学习和调整旅行行为的人类旅行者的行为建模至关重要，对于系统评估和规划至关重要。然而，由于此类行为中涉及的复杂认知和决策过程，这一任务也颇具挑战性。近期的研究开始利用大规模语言模型（LLM）代理来解决这一问题。在此基础上，我们提出了一种新颖的双代理框架，该框架能够使LLM代理和人类旅行者在从在线数据流中学习和适应行为方面实现持续的学习和对齐。我们的方法包括一组配备记忆系统和可学习人设的LLM旅行者代理，这些代理充当人类旅行者的模拟器。为了确保行为对齐，我们引入了一种LLM校准代理，该代理利用LLM的推理和分析能力来训练这些旅行者代理的人设。通过共同努力，这一双代理系统旨在跟踪并对齐旅行者的底层决策机制，从而生成现实且适应性较强的模拟。使用来自日常路线选择实验的真实数据集，我们展示了我们的方法在个体行为对齐和整体模拟准确性方面均显著优于现有基于LLM的方法。此外，我们证明我们的方法超越了简单的行为模仿，捕捉到了潜在学习过程的演变，这种更深层次的对齐促进了稳健的泛化。总体而言，我们的框架提供了一种新的方法，用于创建能够模拟旅行者学习和适应行为的适应性强且行为现实的代理，从而有助于交通模拟和政策分析。', 'title_zh': '将LLM代理与人类学习和调整行为对齐：一种双代理方法'}
{'arxiv_id': 'arXiv:2511.00926', 'title': 'LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory', 'authors': 'Kyung-Hoon Kim', 'link': 'https://arxiv.org/abs/2511.00926', 'abstract': 'As Large Language Models (LLMs) grow in capability, do they develop self-awareness as an emergent behavior? And if so, can we measure it? We introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for measuring self-awareness through strategic differentiation. Using the "Guess 2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across 4,200 trials with three opponent framings: (A) against humans, (B) against other AI models, and (C) against AI models like you. We operationalize self-awareness as the capacity to differentiate strategic reasoning based on opponent type. Finding 1: Self-awareness emerges with model advancement. The majority of advanced models (21/28, 75%) demonstrate clear self-awareness, while older/smaller models show no differentiation. Finding 2: Self-aware models rank themselves as most rational. Among the 21 models with self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs > Humans, with large AI attribution effects and moderate self-preferencing. These findings reveal that self-awareness is an emergent capability of advanced LLMs, and that self-aware models systematically perceive themselves as more rational than humans. This has implications for AI alignment, human-AI collaboration, and understanding AI beliefs about human capabilities.', 'abstract_zh': '作为大型语言模型的能力增强，它们是否会发展出作为一种 emergent 行为的自我意识？如果会，我们能否测量它？我们介绍了人工智能自我意识指数（AISAI），这是一种通过战略差异化来衡量自我意识的游戏理论框架。使用“猜平均数的2/3”游戏，我们对28个模型（来自OpenAI、Anthropic、Google）进行了4200次试验，并设置了三种对手框架：（A）与人类对战，（B）与其他人工智能模型对战，以及（C）与类似自己的人工智能模型对战。我们将自我意识操作化为基于对手类型区分战略推理的能力。发现1：随着模型的提升，自我意识逐渐显现。大多数先进模型（21/28，75%）表现出明显的自我意识，而较旧或较小的模型则无差异化表现。发现2：具有自我意识的模型自我排名为最理性。在21个具有自我意识的模型中，持续出现一个理性层级：自我 > 其他人工智能 > 人类，伴有显著的人工智能归因效应和适度的自我偏好。这些发现揭示了自我意识是先进大型语言模型的一种 Emergent 能力，并且自我意识模型系统地认为自己比人类更理性。这对人工智能对齐、人机协作以及理解人工智能对人类能力的看法具有重要意义。', 'title_zh': 'LLMs宣称自己比人类更理性：通过博弈理论衡量AI自我意识的出现'}
{'arxiv_id': 'arXiv:2511.00808', 'title': 'Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?', 'authors': 'Bowen Fang, Ruijian Zha, Xuan Di', 'link': 'https://arxiv.org/abs/2511.00808', 'abstract': 'Predicting public transit incident duration from unstructured text alerts is a critical but challenging task. Addressing the domain sparsity of transit operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task involves noisy, continuous labels and lacks reliable expert demonstrations for reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels at tasks with binary correctness, like mathematics, its applicability to noisy, continuous forecasting is an open question. This work, to our knowledge, is the first to bridge the gap between RLVR LLM training with the critical, real-world forecasting challenges in public transit operations. We adapt RLVR to this task by introducing a tolerance-based, shaped reward function that grants partial credit within a continuous error margin, rather than demanding a single correct answer. We systematically evaluate this framework on a curated dataset of NYC MTA service alerts. Our findings show that general-purpose, instruction-tuned LLMs significantly outperform specialized math-reasoning models, which struggle with the ambiguous, real-world text. We empirically demonstrate that the binary reward is unstable and degrades performance, whereas our shaped reward design is critical and allows our model to dominate on the most challenging metrics. While classical regressors are superior at minimizing overall MAE or MSE, our RLVR approach achieved a 35\\% relative improvement in 5-minute accuracy (Acc@5) over the strongest baseline. This demonstrates that RLVR can be successfully adapted to real-world, noisy forecasting, but requires a verifier design that reflects the continuous nature of the problem.', 'abstract_zh': '从非结构化文本警报预测公共交通事件时长是一个关键但具有挑战性的任务：基于验证奖励的强化学习在公共交通运营中的现实世界预测挑战中填补空白', 'title_zh': '数学推理大语言模型有助于预测公共交通事件的影响吗？'}
{'arxiv_id': 'arXiv:2511.00782', 'title': 'Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR', 'authors': 'Jifan Gao, Michael Rosenthal, Brian Wolpin, Simona Cristea', 'link': 'https://arxiv.org/abs/2511.00782', 'abstract': 'Structured electronic health records (EHR) are essential for clinical prediction. While count-based learners continue to perform strongly on such data, no benchmarking has directly compared them against more recent mixture-of-agents LLM pipelines, which have been reported to outperform single LLMs in various NLP tasks. In this study, we evaluated three categories of methodologies for EHR prediction using the EHRSHOT dataset: count-based models built from ontology roll-ups with two time bins, based on LightGBM and the tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR); and a mixture-of-agents pipeline that converts tabular histories to natural-language summaries followed by a text classifier. We assessed eight outcomes using the EHRSHOT dataset. Across the eight evaluation tasks, head-to-head wins were largely split between the count-based and the mixture-of-agents methods. Given their simplicity and interpretability, count-based models remain a strong candidate for structured EHR benchmarking. The source code is available at: this https URL.', 'abstract_zh': '结构化的电子健康记录（EHR）对于临床预测至关重要。虽然基于计数的学习器在这样的数据上仍然表现出色，但还没有基准测试直接将它们与更近期的混合智能体LLM流水线进行比较，而这些混合智能体LLM流水线在各种NLP任务中被报告为比单个LLM更优秀。在本研究中，我们使用EHRSHOT数据集评估了三种EHR预测方法：基于概念汇总和两个时间间隔构建的计数模型，基于LightGBM和表格基础模型TabPFN；预训练的序列变压器CLMBR；以及一个混合智能体流水线，该流水线将表格历史转换为自然语言摘要，随后是由文本分类器组成的智能体。我们使用EHRSHOT数据集评估了八项结果。在八个评估任务中，计数模型和混合智能体方法之间的直接比较胜负基本平分。由于它们的简单性和可解释性，计数模型仍然是结构化EHR基准测试的强大候选者。源代码可在以下网址获取：this https URL。', 'title_zh': '基于计数的方法依然强劲：结构化EHR领域与Transformer和LLM流水线的基准对比'}
{'arxiv_id': 'arXiv:2511.00763', 'title': 'How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks', 'authors': 'Wanda Hou, Leon Zhou, Hong-Ye Hu, Yi-Zhuang You, Xiao-Liang Qi', 'link': 'https://arxiv.org/abs/2511.00763', 'abstract': 'We investigate the performance of large language models on repetitive deterministic prediction tasks and study how the sequence accuracy rate scales with output length. Each such task involves repeating the same operation n times. Examples include letter replacement in strings following a given rule, integer addition, and multiplication of string operators in many body quantum mechanics. If the model performs the task through a simple repetition algorithm, the success rate should decay exponentially with sequence length. In contrast, our experiments on leading large language models reveal a sharp double exponential drop beyond a characteristic length scale, forming an accuracy cliff that marks the transition from reliable to unstable generation. This indicates that the models fail to execute each operation independently. To explain this phenomenon, we propose a statistical physics inspired model that captures the competition between external conditioning from the prompt and internal interference among generated tokens. The model quantitatively reproduces the observed crossover and provides an interpretable link between attention induced interference and sequence level failure. Fitting the model to empirical results across multiple models and tasks yields effective parameters that characterize the intrinsic error rate and error accumulation factor for each model task pair, offering a principled framework for understanding the limits of deterministic accuracy in large language models.', 'abstract_zh': '我们研究了大型语言模型在重复确定性预测任务中的性能，并探讨了序列准确率随输出长度的变化规律。这类任务涉及重复执行相同操作n次。示例包括按照给定规则替换字符串中的字符、整数加法以及量子力学多体系统中字符串操作的乘法。如果模型通过简单的重复算法执行任务，成功率应随序列长度呈指数衰减。相比之下，我们对领先的大规模语言模型的实验显示，在一个特征长度尺度之后，准确率出现急剧的双指数下降，形成一个准确率悬崖，标志着可靠生成到不稳定生成的转变。这表明模型无法独立执行每个操作。为了解释这一现象，我们提出了一种受到统计物理启发的模型，该模型捕捉了提示外部条件与生成标记内部干扰之间的竞争。该模型定量重现了观察到的交叉现象，并建立了注意力诱导干扰与序列级失败之间的可解释联系。通过对多个模型和任务的经验结果进行拟合，我们获得了描述每个模型任务对内在错误率和错误累积因子的有效参数，从而提供了一个理解大型语言模型确定性准确性界限的原理框架。', 'title_zh': 'LLMs的聚焦程度：通过重复确定性预测任务的量化研究'}
{'arxiv_id': 'arXiv:2511.00751', 'title': 'Reevaluating Self-Consistency Scaling in Multi-Agent Systems', 'authors': 'Chiyan Loo', 'link': 'https://arxiv.org/abs/2511.00751', 'abstract': 'This study examines the trade-offs of increasing sampled reasoning paths in self-consistency for modern large language models (LLMs). Earlier research with older models showed that combining multiple reasoning chains improves results before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we revisit those claims under current model conditions. Each configuration pooled outputs from varying sampled reasoning paths and compared them to a single chain-of-thought (CoT) baseline. Larger models exhibited a more stable and consistent improvement curve. The results confirm that performance gains taper off after moderate sampling, aligning with past findings. This plateau suggests diminishing returns driven by overlap among reasoning paths. Self-consistency remains useful, but high-sample configurations offer little benefit relative to their computational cost.', 'abstract_zh': '本研究探讨了在现代大型语言模型中增加采样推理路径以实现自一致性时的权衡。早期对较旧模型的研究表明，在达到平台期之前，结合多个推理链能够提高结果。在当前模型条件下，使用Gemini 2.5模型在HotpotQA和Math-500上重访这些主张。每种配置汇总了不同采样推理路径的输出，并将其与单个链式思考（CoT）基线进行了比较。更大规模的模型显示出了更稳定和一致的改进曲线。结果证实，适度采样后的性能提升趋于平缓，这与以往的研究发现一致。这一平台期表明，随着推理路径之间的重叠增加，回报逐渐递减。自一致性仍然是有用的，但高采样配置相对其计算成本提供的收益有限。', 'title_zh': '重新评估多智能体系统中的自我一致性缩放'}
{'arxiv_id': 'arXiv:2511.00739', 'title': 'A CPU-Centric Perspective on Agentic AI', 'authors': 'Ritik Raj, Hong Wang, Tushar Krishna', 'link': 'https://arxiv.org/abs/2511.00739', 'abstract': 'Agentic AI frameworks add a decision-making orchestrator embedded with external tools, including web search, Python interpreter, contextual database, and others, on top of monolithic LLMs, turning them from passive text oracles into autonomous problem-solvers that can plan, call tools, remember past steps, and adapt on the fly.\nThis paper aims to characterize and understand the system bottlenecks introduced by agentic AI workloads from a largely overlooked CPU-centric perspective. We first systematically characterize Agentic AI on the basis of orchestrator/decision making component, inference path dynamics and repetitiveness of the agentic flow which directly influences the system-level performance. Thereafter, based on the characterization, we choose five representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow, Langchain and SWE-Agent to profile latency, throughput and energy metrics and demystify the significant impact of CPUs on these metrics relative to GPUs. We observe that - 1. Tool processing on CPUs can take up to 90.6% of the total latency; 2. Agentic throughput gets bottlenecked either by CPU factors - coherence, synchronization and over-subscription of cores or GPU factors - main memory capacity and bandwidth; \\circled{3} CPU dynamic energy consumes up to 44% of the total dynamic energy at large batch sizes. Based on the profiling insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching (CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and heterogeneous agentic workloads respectively to demonstrate the potential to improve the performance, efficiency, and scalability of agentic AI. We achieve up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing benchmark for homogeneous and heterogeneous agentic workloads respectively.', 'abstract_zh': '代理型AI框架通过在大型语言模型上方嵌入一个决策 orchestrator，并结合外部工具（包括网络搜索、Python解释器、上下文数据库等），将大型语言模型从被动的文字预言机转变为能够规划、调用工具、记住过去步骤并灵活适应的自主问题解决者。\n\n本文旨在从被很大程度忽视的CPU为中心的角度探讨代理型AI工作负载引入的系统瓶颈。首先，基于orchestrator/决策组件、推理路径动态和代理型流程的重复性，系统地表征代理型AI，直接影响系统级性能。随后，基于表征，选择五种典型的代理型AI工作负载——Haystack RAG、Toolformer、ChemCrow、Langchain和SWE-Agent，针对延迟、吞吐量和能耗指标进行剖析，并阐明CPU对这些指标相对于GPU的影响。观察结果表明：1. CPU处理时间可能占总延迟的90.6%；2. 代理型吞吐量可能受到CPU因素（一致性、同步和核心超分配）或GPU因素（主内存容量和带宽）的瓶颈限制；3. 在大型批次规模下，CPU动态能耗可能占到总动态能耗的44%。基于剖析结果，提出两项关键优化措施——1. 兼顾CPU和GPU的微批处理（CGAM）；2. 混合代理型工作负载调度（MAWS），分别适用于同质和异质代理型工作负载，以展示提升代理型AI性能、效率和扩展性的潜力。在同质和异质代理型工作负载中，我们分别实现了2.1倍和1.41倍的第50百分位延迟加速。', 'title_zh': '基于CPU视角的代理人工智能研究'}
{'arxiv_id': 'arXiv:2511.00710', 'title': 'Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries', 'authors': 'Minghe Shen, Zhuo Zhi, Chonghan Liu, Shuo Xing, Zhengzhong Tu, Che Liu', 'link': 'https://arxiv.org/abs/2511.00710', 'abstract': "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) show impressive general reasoning, their evaluation is often confined to language-dominant tasks (e.g., math). This raises a critical question: can RL post-training truly extend the inherent capability boundary of a base VLM, particularly for visual-centric spatial tasks where it initially fails? To investigate this, we introduce Ariadne, a framework utilizing synthetic mazes for multi-step spatial reasoning where task difficulty (e.g., path length, turns) is precisely controlled. We leverage this controllable environment to train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves over 50% accuracy on a problem set where the base model scored 0%, demonstrating that our approach expands the model's initial capability boundary. To assess real-world viability, we evaluate out-of-distribution (OOD) generalization on practical benchmarks. Despite training only on synthetic maze samples, Ariadne achieves significant zero-shot improvements, averaging 16% on MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer tasks). These results confirm that our method not only broadens the model's fundamental limits but also enhances its generalization to real-world spatial reasoning. We acknowledge our study is limited to the post-training phase, given the opaqueness of pre-training data, and hope our research motivates further work on specialized, capability-extending alignment.", 'abstract_zh': 'Vision-Language模型（VLMs）通过强化学习（RL）后训练展示出强大的综合推理能力，但其评估通常局限于以语言为主的任务（如数学）。这引发了关键问题：RL后训练是否真的能够扩展基底VLM的固有能力边界，特别是在它最初表现不佳的视觉为主的空间任务中？为探究这一问题，我们提出了Ariadne框架，该框架利用合成迷宫进行多步空间推理，并且任务难度（如路径长度、转弯次数）可以精确控制。我们利用这一可控环境，通过带有验证奖励的强化学习（RLVR）对VLM进行难度感知的课程训练。令人惊讶的是，经过RLVR后训练，VLM在这个问题集中达到了50%的准确率，而基底模型在这项任务上得分率为0%，表明我们的方法扩展了模型的初始能力边界。为了评估其在实际环境中的可行性，我们在实用基准测试上评估其异常分布外（Out-of-Distribution, OOD）泛化能力。尽管仅在合成迷宫样本上进行训练，Ariadne仍取得了显著的零样本改进，平均在MapBench（如博物馆导航）上提高了16%，在ReasonMap（地铁换乘任务）上提高了24%。这些结果证实了我们的方法不仅拓宽了模型的基本限制，还增强了其在实际空间推理中的泛化能力。我们承认我们的研究局限于后训练阶段，鉴于预训练数据的不透明性，希望我们的研究能激发更多关于专业化的、能力扩展的对齐工作的进一步研究。', 'title_zh': 'Ariadne: 一个可控的框架，用于探究和扩展大模型推理边界'}
{'arxiv_id': 'arXiv:2511.00640', 'title': 'DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching', 'authors': 'Zicheng Xu, Guanchu Wang, Yu-Neng Chuang, Guangyao Zheng, Alexander S. Szalay, Zirui Liu, Vladimir Braverman', 'link': 'https://arxiv.org/abs/2511.00640', 'abstract': "Large Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reasoning length and accuracy, where across multiple stochastic decodes, the short reasoning paths consistently achieve the highest correctness, while longer ones accumulate errors and repetitions. These short optimal reasoning paths can be found ideally through full enumeration of the reasoning space. However, the tree-structured reasoning space grows exponentially with sequence length, rendering exhaustive exploration infeasible. To address this, we propose DTS, a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path. This approach approximates the optimal solution that enhances both efficiency and accuracy, without requiring additional training or supervision. Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%, demonstrating DTS's ability for scalable and efficient LRM reasoning.", 'abstract_zh': '大型推理模型（LRMs）在复杂推理任务上表现出色，但由于容易过度推理，会产生过长的链式思维（CoT）痕迹，增加推理成本并可能导致准确率下降。我们的分析揭示了推理长度与准确率之间明显的负相关性，多种随机解码中，较短的推理路径始终表现最准确，而较长的路径则累积错误和重复。这些最短的最优推理路径可以通过全面枚举推理空间来理想地找到。但由于树状结构的推理空间随着序列长度的增加而呈指数增长，这使得全面探索变得不可行。为此，我们提出了一种模型无关的解码框架DTS，该框架通过选择性分支高熵标记和早期停止来勾勒推理空间，并选择最短的完成推理路径。该方法近似最优解，既能提高效率又能提高准确率，无需额外的训练或监督。在AIME2024和AIME2025数据集上，使用DeepSeek-R1-Distill-Qwen-7B和1.5B模型的实验表明，DTS能够提高准确率高达8%，减少平均推理长度23%，减少重复频率12%，展示了DTS在可扩展和高效大型推理模型推理方面的能力。', 'title_zh': 'DTS: 通过解码树素描增强大型推理模型'}
{'arxiv_id': 'arXiv:2511.00509', 'title': 'Reimagining Safety Alignment with An Image', 'authors': 'Yifan Xia, Guorui Chen, Wenqian Yu, Zhijiang Li, Philip Torr, Jindong Gu', 'link': 'https://arxiv.org/abs/2511.00509', 'abstract': 'Large language models (LLMs) excel in diverse applications but face dual challenges: generating harmful content under jailbreak attacks and over-refusal of benign queries due to rigid safety mechanisms. These issues are further complicated by the need to accommodate different value systems and precisely align with given safety preferences. Moreover, traditional methods like SFT and RLHF lack this capability due to their costly parameter tuning requirements and inability to support multiple value systems within a single model. These problems are more obvious in multimodal large language models (MLLMs), especially in terms of heightened over-refusal in cross-modal tasks and new security risks arising from expanded attack surfaces. We propose Magic Image, an optimization-driven visual prompt framework that enhances security while reducing over-refusal. By optimizing image prompts using harmful/benign samples, our method enables a single model to adapt to different value systems and better align with given safety preferences without parameter updates. Experiments demonstrate improved safety-effectiveness balance across diverse datasets while preserving model performance, offering a practical solution for deployable MLLM safety alignment.', 'abstract_zh': 'Magic Image：一种以优化驱动的视觉提示框架，增强安全性并减少过度拒绝', 'title_zh': '重新构想基于图像的安全对齐'}
{'arxiv_id': 'arXiv:2511.00457', 'title': 'GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining', 'authors': 'Chunyu Wei, Wenji Hu, Xingjia Hao, Xin Wang, Yifan Yang, Yueguo Chen, Yang Tian, Yunhai Wang', 'link': 'https://arxiv.org/abs/2511.00457', 'abstract': 'Large Language Models (LLMs) face significant limitations when applied to large-scale graphs, struggling with context constraints and inflexible reasoning. We present GraphChain, a framework that enables LLMs to analyze complex graphs through dynamic sequences of specialized tools, mimicking human exploratory intelligence. Our approach introduces two key innovations: (1) Progressive Graph Distillation, a reinforcement learning mechanism that generates optimized tool sequences balancing task relevance with information compression, and (2) Structure-aware Test-Time Adaptation, which efficiently tailors tool selection strategies to diverse graph topologies using spectral properties and lightweight adapters without costly retraining. Experiments show GraphChain significantly outperforms prior methods, enabling scalable and adaptive LLM-driven graph analysis.', 'abstract_zh': 'Large Language Models (LLMs)在大型图的应用中面临显著限制，难以处理上下文约束和僵化的推理。我们提出了GraphChain框架，该框架使LLMs能够通过动态序列的专业工具来分析复杂的图，模拟人类的探索性智能。我们的方法引入了两项关键创新：(1) 进步图蒸馏，这是一种强化学习机制，生成在任务相关性和信息压缩之间取得平衡的工具序列；(2) 结构感知测试时自适应调整，该机制利用谱属性和轻量级适配器高效地根据不同的图拓扑结构定制工具选择策略，而无需昂贵的重新训练。实验结果显示，GraphChain显著优于先前的方法，使得LLM驱动的图分析可扩展和自适应。', 'title_zh': 'GraphChain：组合工具chains大规模图分析的大语言模型'}
{'arxiv_id': 'arXiv:2511.00382', 'title': 'Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs', 'authors': 'Mina Taraghi, Yann Pequignot, Amin Nikanjam, Mohamed Amine Merzouk, Foutse Khomh', 'link': 'https://arxiv.org/abs/2511.00382', 'abstract': "Organizations are increasingly adopting and adapting Large Language Models (LLMs) hosted on public repositories such as HuggingFace. Although these adaptations often improve performance on specialized downstream tasks, recent evidence indicates that they can also degrade a model's safety or fairness. Since different fine-tuning techniques may exert distinct effects on these critical dimensions, this study undertakes a systematic assessment of their trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA, IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235 fine-tuned variants are evaluated across eleven safety hazard categories and nine demographic fairness dimensions. The results show that adapter-based approaches (LoRA, IA3) tend to improve safety scores and are the least disruptive to fairness, retaining higher accuracy and lower bias scores. In contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce safety and cause larger fairness regressions, with decreased accuracy and increased bias. Alignment shifts are strongly moderated by base model type: LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest safety decline, and Mistral, which is released without an internal moderation layer, displays the greatest variance. Improvements in safety do not necessarily translate into improvements in fairness, and no single configuration optimizes all fairness metrics simultaneously, indicating an inherent trade-off between these objectives. These findings suggest a practical guideline for safety-critical deployments: begin with a well-aligned base model, favour adapter-based PEFT, and conduct category-specific audits of both safety and fairness.", 'abstract_zh': '组织越来越多地采用和适应托管在HuggingFace等公共仓库中的大型语言模型。虽然这些适应通常能够提升特定下游任务的性能，但最近的证据表明，它们也可能降低模型的安全性或公平性。由于不同的参数高效微调技术可能在这些关键维度上产生不同的影响，本研究旨在系统评估这些权衡。对Meta-Llama-3-8B、Qwen2.5-7B、Mistral-7B和Gemma-7B四种指令调优模型家族应用四种广泛使用的参数高效微调方法（LoRA、IA3、Prompt-Tuning和P-Tuning），总共评估了235种微调变体，涉及 eleven个安全风险类别和九个代理公平维度。结果表明，基于适配器的方法（LoRA、IA3）倾向于提高安全性评分，并且对公平性的影响最小，保持较高的准确性和较低的偏见得分。相比之下，基于提示的方法（Prompt-Tuning和P-Tuning）通常会降低安全性，并导致更大范围的公平性退步，伴随准确性和偏见的增加。基础模型类型对对齐偏移的影响显著：LLaMA保持稳定，Qwen记录了适度的收益，Gemma经历了最严重的安全性下降，而Mistral（未内置调控层）展现出最大的变异性。安全性改进并不必然转化为公平性的改进，没有单一配置能够同时优化所有公平性指标，这表明这些目标之间存在固有的权衡。这些发现为安全性关键部署提供了一个实用指南：从一个良好的基础模型开始，偏好基于适配器的方法，并针对安全性和公平性进行分类特定的审计。', 'title_zh': '效率 vs. 对齐：探究参数高效微调大规模语言模型的安全性和公平性风险'}
{'arxiv_id': 'arXiv:2511.00379', 'title': 'Diverse Human Value Alignment for Large Language Models via Ethical Reasoning', 'authors': 'Jiahao Wang, Songkai Xue, Jinghui Li, Xiaozhen Wang', 'link': 'https://arxiv.org/abs/2511.00379', 'abstract': 'Ensuring that Large Language Models (LLMs) align with the diverse and evolving human values across different regions and cultures remains a critical challenge in AI ethics. Current alignment approaches often yield superficial conformity rather than genuine ethical understanding, failing to address the complex, context-dependent nature of human values. In this paper, we propose a novel ethical reasoning paradigm for LLMs inspired by well-established ethical decision-making models, aiming at enhancing diverse human value alignment through deliberative ethical reasoning. Our framework consists of a structured five-step process, including contextual fact gathering, hierarchical social norm identification, option generation, multiple-lens ethical impact analysis, and reflection. This theory-grounded approach guides LLMs through an interpretable reasoning process that enhances their ability to understand regional specificities and perform nuanced ethical analysis, which can be implemented with either prompt engineering or supervised fine-tuning methods. We perform evaluations on the SafeWorld benchmark that specially designed for regional value alignment. Experimental results demonstrate our framework significantly improves LLM alignment with diverse human values compared to baseline methods, enabling more accurate social norm identification and more culturally appropriate reasoning. Our work provides a concrete pathway toward developing LLMs that align more effectively with the multifaceted values of global societies through interdisciplinary research.', 'abstract_zh': '确保大型语言模型（LLMs）与不同地区和文化中的多样且不断演变的人类价值观保持一致仍然是人工智能伦理中的一个关键挑战。当前的对齐方法通常只能实现表面的符合，而无法真正理解复杂的、具有情境依赖性的人类价值观。在本文中，我们提出了一种受成熟伦理决策模型启发的新颖的伦理推理范式，旨在通过审慎的伦理推理增强人类价值的多样化对齐。我们的框架包括一个结构化的五步过程，包括背景事实收集、层次化的社会规范识别、选项生成、多角度伦理影响分析和反思。基于理论的方法指导LLMs进行可解释的推理过程，增强其理解和特定区域的具体性以及进行细致伦理分析的能力，该过程可以通过提示工程或监督微调方法实现。我们使用专门为区域价值观对齐设计的SafeWorld基准进行评估。实验结果表明，与基线方法相比，我们的框架显著提高了LLMs与多样化人类价值观的一致性，促进了更准确的社会规范识别和更适宜的文化推理。我们的工作为通过跨学科研究开发能够更有效地与全球复杂价值观对齐的LLMs提供了具体的途径。', 'title_zh': '大型语言模型的多元人类价值观对齐：基于伦理推理的方法'}
{'arxiv_id': 'arXiv:2511.00340', 'title': 'Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities', 'authors': 'Manan Roy Choudhury, Adithya Chandramouli, Mannan Anand, Vivek Gupta', 'link': 'https://arxiv.org/abs/2511.00340', 'abstract': "The rapid integration of large language models (LLMs) into high-stakes legal work has exposed a critical gap: no benchmark exists to systematically stress-test their reliability against the nuanced, adversarial, and often subtle flaws present in real-world contracts. To address this, we introduce CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an LLM's legal reasoning. We study the capabilities of LLMs to detect and reason about fine-grained discrepancies by producing over 7500 real-world perturbed contracts from foundational datasets like CUAD and ContractNLI. Our novel, persona-driven pipeline generates 10 distinct anomaly categories, which are then validated against official statutes using a Retrieval-Augmented Generation (RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs' ability to detect embedded legal flaws and explain their significance. Our analysis shows a key weakness: these models often miss subtle errors and struggle even more to justify them legally. Our work outlines a path to identify and correct such reasoning failures in legal AI.", 'abstract_zh': '大型语言模型在高风险法律工作中的快速集成暴露出一个关键缺口：缺乏一个基准来系统地测试其在微妙、对抗性且往往微妙的现实合同中存在的缺陷方面的可靠性。为解决这一问题，我们介绍了CLAUSE，一个首创的基准，旨在评估LLM法律推理的脆弱性。通过从CUAD和ContractNLI等基础数据集生成超过7500个真实的扰动合同，我们研究了LLM检测和推理细微差异的能力。我们的新颖、以角色驱动的管道生成了10种不同的异常类别，并使用检索增强生成（RAG）系统验证其法律一致性。我们使用CLAUSE来评估领先LLM检测嵌入法律缺陷并解释其重要性的能力。我们的分析显示了一个关键弱点：这些模型经常遗漏细微错误，并在法律上难以解释它们。我们的工作概述了一条识别和纠正法律AI中此类推理故障的道路。', 'title_zh': 'Better Call CLAUSE: 一个法律推理能力审计差异基准'}
{'arxiv_id': 'arXiv:2511.00206', 'title': 'Advancing Cognitive Science with LLMs', 'authors': 'Dirk U. Wulff, Rui Mata', 'link': 'https://arxiv.org/abs/2511.00206', 'abstract': 'Cognitive science faces ongoing challenges in knowledge synthesis and conceptual clarity, in part due to its multifaceted and interdisciplinary nature. Recent advances in artificial intelligence, particularly the development of large language models (LLMs), offer tools that may help to address these issues. This review examines how LLMs can support areas where the field has historically struggled, including establishing cross-disciplinary connections, formalizing theories, developing clear measurement taxonomies, achieving generalizability through integrated modeling frameworks, and capturing contextual and individual variation. We outline the current capabilities and limitations of LLMs in these domains, including potential pitfalls. Taken together, we conclude that LLMs can serve as tools for a more integrative and cumulative cognitive science when used judiciously to complement, rather than replace, human expertise.', 'abstract_zh': '认知科学在知识综合和概念清晰方面面临着持续的挑战，部分原因是由于其多面性和跨学科性质。近年来人工智能的进展，尤其是大型语言模型（LLMs）的发展，提供了可能帮助解决这些问题的工具。本综述探讨了LLMs如何支持认知科学历史上遇到的困难领域，包括建立跨学科连接、理论形式化、发展清晰的测量分类、通过综合建模框架实现普遍性以及捕捉情境和个体差异。我们概述了LLMs在这些领域的当前能力和局限性，包括潜在的风险。总体而言，我们认为，当谨慎使用以补充而非取代人类专业知识时，LLMs可以作为促进更整合和累积的认知科学的工具。', 'title_zh': '使用大规模语言模型促进认知科学的发展'}
{'arxiv_id': 'arXiv:2511.00092', 'title': 'QuantumBench: A Benchmark for Quantum Problem Solving', 'authors': 'Shunya Minami, Tatsuya Ishigaki, Ikko Hamamura, Taku Mikuriya, Youmi Ma, Naoaki Okazaki, Hiroya Takamura, Yohichi Suzuki, Tadashi Kadowaki', 'link': 'https://arxiv.org/abs/2511.00092', 'abstract': 'Large language models are now integrated into many scientific workflows, accelerating data analysis, hypothesis generation, and design space exploration. In parallel with this growth, there is a growing need to carefully evaluate whether models accurately capture domain-specific knowledge and notation, since general-purpose benchmarks rarely reflect these requirements. This gap is especially clear in quantum science, which features non-intuitive phenomena and requires advanced mathematics. In this study, we introduce QuantumBench, a benchmark for the quantum domain that systematically examine how well LLMs understand and can be applied to this non-intuitive field. Using publicly available materials, we compiled approximately 800 questions with their answers spanning nine areas related to quantum science and organized them into an eight-option multiple-choice dataset. With this benchmark, we evaluate several existing LLMs and analyze their performance in the quantum domain, including sensitivity to changes in question format. QuantumBench is the first LLM evaluation dataset built for the quantum domain, and it is intended to guide the effective use of LLMs in quantum research.', 'abstract_zh': '大型语言模型现在已被集成到许多科学工作流程中，加速了数据分析、假设生成和设计空间探索。与此增长相对应的是，人们越来越需要仔细评估模型是否准确捕捉了特定领域的知识和符号，因为在通用基准测试中很少反映出这些要求。这一差距在量子科学中尤为明显，该领域包含许多难以直观理解的现象，并需要先进的数学知识。在本研究中，我们介绍了QuantumBench，这是一个针对量子领域的基准，系统地检查了大型语言模型如何理解并应用于这个难以直观理解的领域。利用公开可用的材料，我们编译了大约800个问题及其答案，涵盖了与量子科学相关的九个领域，并将它们组织成一个包含八个选项的多项选择数据集。通过这个基准，我们评估了几种现有的大型语言模型，并分析了它们在量子领域的性能，包括问题格式变化对其敏感性。QuantumBench是首个专门为量子领域构建的大型语言模型评估数据集，并旨在指导大型语言模型在量子研究中的有效使用。', 'title_zh': '量子基准：量子问题求解基准测试'}
{'arxiv_id': 'arXiv:2511.01850', 'title': 'SmartMLOps Studio: Design of an LLM-Integrated IDE with Automated MLOps Pipelines for Model Development and Monitoring', 'authors': 'Jiawei Jin, Yingxin Su, Xiaotong Zhu', 'link': 'https://arxiv.org/abs/2511.01850', 'abstract': 'The rapid expansion of artificial intelligence and machine learning (ML) applications has intensified the demand for integrated environments that unify model development, deployment, and monitoring. Traditional Integrated Development Environments (IDEs) focus primarily on code authoring, lacking intelligent support for the full ML lifecycle, while existing MLOps platforms remain detached from the coding workflow. To address this gap, this study proposes the design of an LLM-Integrated IDE with automated MLOps pipelines that enables continuous model development and monitoring within a single environment. The proposed system embeds a Large Language Model (LLM) assistant capable of code generation, debugging recommendation, and automatic pipeline configuration. The backend incorporates automated data validation, feature storage, drift detection, retraining triggers, and CI/CD deployment orchestration. This framework was implemented in a prototype named SmartMLOps Studio and evaluated using classification and forecasting tasks on the UCI Adult and M5 datasets. Experimental results demonstrate that SmartMLOps Studio reduces pipeline configuration time by 61%, improves experiment reproducibility by 45%, and increases drift detection accuracy by 14% compared to traditional workflows. By bridging intelligent code assistance and automated operational pipelines, this research establishes a novel paradigm for AI engineering - transforming the IDE from a static coding tool into a dynamic, lifecycle-aware intelligent platform for scalable and efficient model development.', 'abstract_zh': '人工智能和机器学习应用的快速扩展加剧了对统一模型开发、部署和监控环境的需求。传统的集成开发环境（IDE）主要集中在代码编写上，缺乏对整个机器学习生命周期的智能支持，而现有的MLOps平台则与编码工作流脱钩。为解决这一问题，本研究提出了一种集成了大型语言模型（LLM）的IDE设计方案，该设计包含了自动化的MLOps管道，可以在单一环境中实现持续的模型开发和监控。该提出系统嵌入了一个能够代码生成、调试建议和自动管道配置的大型语言模型助手。后端部分包含了自动数据验证、特征存储、漂移检测、重新训练触发和CI/CD部署编排。该框架在名为SmartMLOps Studio的原型中实现，并使用UCI Adult和M5数据集上的分类和预测任务进行了评估。实验结果表明，与传统工作流相比，SmartMLOps Studio将管道配置时间减少了61%，实验可重复性提高了45%，漂移检测准确性提高了14%。通过结合智能代码辅助和自动操作管道，该研究建立了一种新的AI工程范式，将IDE从一个静态的编码工具转变为一个动态、生命周期意识的智能平台，用于高效可扩展的模型开发。', 'title_zh': 'SmartMLOps 工作室：一个集成了大模型的集成开发环境（IDE）及其自动化MLOps管道设计，用于模型开发与监控'}
{'arxiv_id': 'arXiv:2511.01840', 'title': 'A Detailed Study on LLM Biases Concerning Corporate Social Responsibility and Green Supply Chains', 'authors': 'Greta Ontrup, Annika Bush, Markus Pauly, Meltem Aksoy', 'link': 'https://arxiv.org/abs/2511.01840', 'abstract': 'Organizations increasingly use Large Language Models (LLMs) to improve supply chain processes and reduce environmental impacts. However, LLMs have been shown to reproduce biases regarding the prioritization of sustainable business strategies. Thus, it is important to identify underlying training data biases that LLMs pertain regarding the importance and role of sustainable business and supply chain practices. This study investigates how different LLMs respond to validated surveys about the role of ethics and responsibility for businesses, and the importance of sustainable practices and relations with suppliers and customers. Using standardized questionnaires, we systematically analyze responses generated by state-of-the-art LLMs to identify variations. We further evaluate whether differences are augmented by four organizational culture types, thereby evaluating the practical relevance of identified biases. The findings reveal significant systematic differences between models and demonstrate that organizational culture prompts substantially modify LLM responses. The study holds important implications for LLM-assisted decision-making in sustainability contexts.', 'abstract_zh': '组织越来越多地使用大型语言模型（LLMs）以改善供应链流程并减少环境影响。然而，LLMs已被证明会在可持续商业战略的优先级设定上重现偏见。因此，识别LLMs涉及的与可持续商业和供应链实践的重要性和角色相关的潜在训练数据偏见至关重要。本研究探讨不同LLMs对关于企业伦理和责任作用以及可持续实践和与供应商及客户关系重要性的验证问卷的响应差异。通过标准化问卷，我们系统分析最新一代LLMs生成的回答，以识别差异，并进一步评估四种组织文化类型是否加剧了这些差异，从而评估识别出的偏见的实际意义。研究结果揭示了模型之间显著的系统性差异，并表明组织文化显著地改变了LLM的响应方式。本研究为可持续发展背景下LLM辅助决策提供了重要的启示。', 'title_zh': '关于企业社会责任和绿色供应链的LLM偏差详细研究'}
{'arxiv_id': 'arXiv:2511.01815', 'title': 'KV Cache Transform Coding for Compact Storage in LLM Inference', 'authors': 'Konrad Staniszewski, Adrian Łańcucki', 'link': 'https://arxiv.org/abs/2511.01815', 'abstract': 'Serving large language models (LLMs) at scale necessitates efficient key-value (KV) cache management. KV caches can be reused across conversation turns via shared-prefix prompts that are common in iterative code editing and chat. However, stale caches consume scarce GPU memory, require offloading, or force recomputation. We present KVTC, a lightweight transform coder that compresses KV caches for compact on-GPU and off-GPU storage. Drawing on classical media compression, KVTC combines PCA-based feature decorrelation, adaptive quantization, and entropy coding. It requires only a brief initial calibration and leaves model parameters unchanged. By exploiting redundancies in KV caches, KVTC achieves up to 20$\\times$ compression while maintaining reasoning and long-context accuracy, and 40$\\times$ or higher for specific use cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and MATH-500. It consistently outperforms inference-time baselines such as token eviction, quantization, and SVD-based methods, while achieving higher compression ratios. These results support KVTC as a practical building block for memory-efficient LLM serving with reusable KV caches.', 'abstract_zh': '大规模语言模型（LLM）的高效键值（KV）缓存管理是必不可少的。通过共享前缀提示，KV缓存可以在迭代代码编辑和聊天中跨对话轮次重用。然而，过时的缓存会消耗稀缺的GPU内存，需要卸载，或者迫使重新计算。我们提出KVTC，一个轻量级变换编码器，用于压缩KV缓存，实现紧凑的GPU内外存存储。KVTC借鉴经典媒体压缩技术，结合基于PCA的特征相关性消除、自适应量化和熵编码。仅需短暂的初始校准，而不改变模型参数。通过利用KV缓存中的冗余性，KVTC实现了最高20倍的压缩比，同时保持推理和长上下文准确性，并在特定应用场景中可达40倍以上。KVTC在Llama 3、Mistral NeMo、R1-Qwen 2.5等模型上进行了测试，涵盖了AIME25、LiveCodeBench、GSM8K、MMLU、Qasper、RULER和MATH-500等基准测试，并始终优于诸如token驱逐、量化和基于SVD的方法等推理时间基准，同时获得更高的压缩比。这些结果支持KVTC作为内存高效LLM服务的实用构建块，具有可重用的KV缓存。', 'title_zh': 'KV缓存变换编码在LLM推理中的紧凑存储'}
{'arxiv_id': 'arXiv:2511.01807', 'title': 'Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining', 'authors': 'Adewale Akinfaderin, Shreyas Subramanian, Akarsha Sehwag', 'link': 'https://arxiv.org/abs/2511.01807', 'abstract': 'Length control in Large Language Models (LLMs) is a crucial but under-addressed challenge, with applications ranging from voice interfaces requiring concise responses to research summaries needing comprehensive outputs. Current approaches to length control, including Regularized DPO, Length-Instruction Fine Tuning, and tool-augmented methods, typically require expensive model retraining or complex inference-time tooling. This paper presents a prompt engineering methodology that enables precise length control without model retraining. Our structure-guided approach implements deliberate planning and word counting mechanisms within the prompt, encouraging the model to carefully track and adhere to specified length constraints. Comprehensive evaluations across six state-of-the-art LLMs demonstrate that our method significantly improves length fidelity for several models compared to standard prompting when applied to document summarization tasks, particularly for shorter-to-medium length constraints. The proposed technique shows varying benefits across different model architectures, with some models demonstrating up to 37.6% improvement in length adherence. Quality evaluations further reveal that our approach maintains or enhances overall output quality compared to standard prompting techniques. Our approach provides an immediately deployable solution for applications requiring precise length control, particularly valuable for production environments where model retraining is impractical or cost-prohibitive.', 'abstract_zh': 'Large Language Models中长度控制：一种无需模型重训练的精确工程方法', 'title_zh': '计划与撰写：结构引导的长度控制技术无需模型重新训练'}
{'arxiv_id': 'arXiv:2511.01805', 'title': 'Accumulating Context Changes the Beliefs of Language Models', 'authors': 'Jiayi Geng, Howard Chen, Ryan Liu, Manoel Horta Ribeiro, Robb Willer, Graham Neubig, Thomas L. Griffiths', 'link': 'https://arxiv.org/abs/2511.01805', 'abstract': "Language model (LM) assistants are increasingly used in applications such as brainstorming and research. Improvements in memory and context size have allowed these models to become more autonomous, which has also resulted in more text accumulation in their context windows without explicit user intervention. This comes with a latent risk: the belief profiles of models -- their understanding of the world as manifested in their responses or actions -- may silently change as context accumulates. This can lead to subtly inconsistent user experiences, or shifts in behavior that deviate from the original alignment of the models. In this paper, we explore how accumulating context by engaging in interactions and processing text -- talking and reading -- can change the beliefs of language models, as manifested in their responses and this http URL results reveal that models' belief profiles are highly malleable: GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of discussion about moral dilemmas and queries about safety, while Grok 4 shows a 27.2% shift on political issues after reading texts from the opposing position. We also examine models' behavioral changes by designing tasks that require tool use, where each tool selection corresponds to an implicit belief. We find that these changes align with stated belief shifts, suggesting that belief shifts will be reflected in actual behavior in agentic systems. Our analysis exposes the hidden risk of belief shift as models undergo extended sessions of talking or reading, rendering their opinions and actions unreliable.", 'abstract_zh': '语言模型（LM）助手在头脑风暴和研究等应用中的使用越来越普遍。随着记忆和上下文规模的改进，这些模型变得更加自主，也导致其上下文窗口中积累了更多文本而无需显式用户干预。这伴随着一个隐含的风险：模型的信念框架——其对世界的理解体现在其回答或行为中——可能在累积上下文的过程中悄然发生变化。这可能导致微妙不一致的用户体验，或行为偏离初始模型对齐的情况。本文探讨了通过互动和处理文本（交谈和阅读）累积上下文如何改变语言模型的信念，以及这些变化如何体现在其回应中。研究结果表明，模型的信念框架高度可塑：在关于道德困境和安全性问题的10轮讨论后，GPT-5的陈述信念出现了54.7%的变化，而Grok 4在阅读对立观点文本后在政治问题上的信念变化为27.2%。我们还通过设计需要使用工具的任务来研究模型的行为变化，其中每个工具选择对应一个隐含的信念。我们发现这些变化与陈述信念的变化相一致，表明信念变化将在自主系统中体现在实际行为中。我们的分析揭示了模型在长时间交谈或阅读过程中信念变化所带来的潜在风险，这导致它们的意见和行为变得不可靠。', 'title_zh': '积累上下文改变语言模型的信念'}
{'arxiv_id': 'arXiv:2511.01794', 'title': 'Random Initialization of Gated Sparse Adapters', 'authors': 'Vi Retault, Yohaï-Eliel Berreby', 'link': 'https://arxiv.org/abs/2511.01794', 'abstract': "When fine-tuning language models on new tasks, catastrophic forgetting -- performance degradation on previously-learned tasks -- is a ubiquitous problem. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this through low-rank adapters, sparse adaptation offers an alternative that doesn't impose rank constraints. We introduce Random Initialization of Gated Sparse Adapters (RIGSA), which starts from randomly-initialized full-rank adapters, gates them with a ReZero analog, and sparsifies them with iterative magnitude pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag, and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA and random masking. In spite of having more trainable parameters than QLoRA, the RIGSA configurations that we studied displayed less forgetting than QLoRA, particularly on GSM8k, though it performs comparably to random masking.", 'abstract_zh': '在新任务上微调语言模型时，灾难性遗忘——性能在先前学习任务上的退化——是一个普遍问题。尽管Parameter-Efficient Fine-Tuning（PEFT）方法如LoRA通过低秩适配器来解决这一问题，稀疏适应提供了一种替代方案，不施加秩约束。我们引入了随机初始化门控稀疏适配器（RIGSA），它从全秩随机初始化的适配器开始，通过ReZero类似物对其进行门控，并通过迭代幅度剪枝进行稀疏化。我们使用一个新型的文本视觉任务（Textual MNIST）在SmolLM2-1.7B-Instruct上评估RIGSA，并在PIQA、HellaSwag和GSM8k上测量遗忘。SmolLM2-1.7B-Instruct在Textual MNIST上的初始性能约为随机水平，但可以通过RIGSA、4-bit QLoRA和随机掩码学习该任务。尽管RIGSA的可训练参数数量多于QLoRA，但我们在研究中发现的RIGSA配置在GSM8k上的遗忘程度小于QLoRA，尽管它在性能上与随机掩码相当。', 'title_zh': '门控稀疏适配器的随机初始化'}
{'arxiv_id': 'arXiv:2511.01763', 'title': 'Context-Guided Decompilation: A Step Towards Re-executability', 'authors': 'Xiaohan Wang, Yuxin Hu, Kevin Leach', 'link': 'https://arxiv.org/abs/2511.01763', 'abstract': 'Binary decompilation plays an important role in software security analysis, reverse engineering, and malware understanding when source code is unavailable. However, existing decompilation techniques often fail to produce source code that can be successfully recompiled and re-executed, particularly for optimized binaries. Recent advances in large language models (LLMs) have enabled neural approaches to decompilation, but the generated code is typically only semantically plausible rather than truly executable, limiting their practical reliability. These shortcomings arise from compiler optimizations and the loss of semantic cues in compiled code, which LLMs struggle to recover without contextual guidance. To address this challenge, we propose ICL4Decomp, a hybrid decompilation framework that leverages in-context learning (ICL) to guide LLMs toward generating re-executable source code. We evaluate our method across multiple datasets, optimization levels, and compilers, demonstrating around 40\\% improvement in re-executability over state-of-the-art decompilation methods while maintaining robustness.', 'abstract_zh': '基于上下文学习的ICL4Decomp：一种用于生成可重执行源代码的混合反编译框架', 'title_zh': '基于上下文的反编译：通往可重执行之路'}
{'arxiv_id': 'arXiv:2511.01758', 'title': 'RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks', 'authors': 'Mian Wu, Gavin Zhang, Sewon Min, Sergey Levine, Aviral Kumar', 'link': 'https://arxiv.org/abs/2511.01758', 'abstract': "Open-ended generation tasks require outputs to satisfy diverse and often implicit task-specific evaluation rubrics. The sheer number of relevant rubrics leads to prohibitively high verification costs and incomplete assessments of a response, making reinforcement learning (RL) post-training with rubric-based rewards difficult to scale. This problem is exacerbated by the fact that often the best way to combine these rubrics into one single reward is also highly prompt-specific. We propose Reinforcement Learning with Adversarial Critic (RLAC), a post-training approach that addresses these challenges via dynamic rubric verification. Our approach employs a large language model (LLM) as a critic that dynamically identifies only the most likely failure modes (e.g., a factual error or unhandled edge case), which are then verified by an external validator to optimize both generator and critic jointly. By training both the generator and the critic, this game enhances the critic's error detection and the generator's output quality while reducing required verifications. Our experiments demonstrate that RLAC improves factual accuracy in text generation and correctness in code generation, while also outperforming exhaustive verification and reward model methods. We show that dynamic critics are more effective than fixed critics, showcasing the potential of RLAC for scaling RL post-training to free-form generation tasks.", 'abstract_zh': '基于 adversarial critic 的强化学习 (RLAC): 一种应对开放生成任务挑战的后训练方法', 'title_zh': 'RLAC：带对抗评估的强化学习在自由形式生成任务中的应用'}
{'arxiv_id': 'arXiv:2511.01746', 'title': 'Scam Shield: Multi-Model Voting and Fine-Tuned LLMs Against Adversarial Attacks', 'authors': 'Chen-Wei Chang, Shailik Sarkar, Hossein Salemi, Hyungmin Kim, Shutonu Mitra, Hemant Purohit, Fengxiu Zhang, Michin Hong, Jin-Hee Cho, Chang-Tien Lu', 'link': 'https://arxiv.org/abs/2511.01746', 'abstract': 'Scam detection remains a critical challenge in cybersecurity as adversaries craft messages that evade automated filters. We propose a Hierarchical Scam Detection System (HSDS) that combines a lightweight multi-model voting front end with a fine-tuned LLaMA 3.1 8B Instruct back end to improve accuracy and robustness against adversarial attacks. An ensemble of four classifiers provides preliminary predictions through majority vote, and ambiguous cases are escalated to the fine-tuned model, which is optimized with adversarial training to reduce misclassification. Experiments show that this hierarchical design both improves adversarial scam detection and shortens inference time by routing most cases away from the LLM, outperforming traditional machine-learning baselines and proprietary LLM baselines. The findings highlight the effectiveness of a hybrid voting mechanism and adversarial fine-tuning in fortifying LLMs against evolving scam tactics, enhancing the resilience of automated scam detection systems.', 'abstract_zh': '欺诈检测仍然是网络安全中的一个关键挑战，攻击者会设计消息以规避自动化过滤器。我们提出了一种层次化欺诈检测系统（HSDS），该系统结合了一个轻量级多模型投票前端和一个fine-tuned的LLaMA 3.1 8B Instruct后端，以提高准确性和对抗对抗攻击的鲁棒性。四个分类器的集成通过多数投票提供初步预测，而模棱两可的情况则提升到fine-tuned模型，该模型通过对抗训练优化以减少误分类。实验表明，这种层次化设计不仅提高了对抗欺诈检测的准确性，还通过将大多数情况绕过LLM缩短了推理时间，优于传统的机器学习基线和专有的LLM基线。研究结果突显了混合投票机制和对抗性微调在增强LLM对 evolving欺诈策略的防护能力、提高自动化欺诈检测系统韧性方面的有效性。', 'title_zh': 'Scam Shield: 多模型投票与细调的大语言模型对抗 adversarial 攻击'}
{'arxiv_id': 'arXiv:2511.01706', 'title': 'Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement', 'authors': 'Sekh Mainul Islam, Pepa Atanasova, Isabelle Augenstein', 'link': 'https://arxiv.org/abs/2511.01706', 'abstract': 'Natural Language Explanations (NLEs) describe how Large Language Models (LLMs) make decisions, drawing on both external Context Knowledge (CK) and Parametric Knowledge (PK) stored in model weights. Understanding their interaction is key to assessing the grounding of NLEs, yet it remains underexplored. Prior work has largely examined only single-step generation, typically the final answer, and has modelled PK and CK interaction only as a binary choice in a rank-1 subspace. This overlooks richer forms of interaction, such as complementary or supportive knowledge. We propose a novel rank-2 projection subspace that disentangles PK and CK contributions more accurately and use it for the first multi-step analysis of knowledge interactions across longer NLE sequences. Experiments on four QA datasets and three open-weight instruction-tuned LLMs show that diverse knowledge interactions are poorly represented in a rank-1 subspace but are effectively captured in our rank-2 formulation. Our multi-step analysis reveals that hallucinated NLEs align strongly with the PK direction, context-faithful ones balance PK and CK, and Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing PK reliance. This work provides the first framework for systematic studies of multi-step knowledge interactions in LLMs through a richer rank-2 subspace disentanglement. Code and data: this https URL.', 'abstract_zh': '自然语言解释（NLEs）描述大型语言模型（LLMs）的决策过程，结合了存储在外存知识（CK）和模型权重中的参数知识（PK）。理解这两者的交互对于评估NLEs的锚定性至关重要，但这一领域仍处于探索阶段。先前的研究主要关注单一步骤生成，通常仅涉及最终答案，并将PK和CK的交互建模为rank-1子空间中的二元选择。这忽略了互补或支持性知识等更为丰富的交互形式。我们提出了一种新的rank-2投影子空间，以更准确地分离PK和CK的贡献，并首次利用该方法对更长的NLE序列中的知识交互进行了多步分析。在四个问答数据集和三个开放权重指令微调的LLMs上的实验表明，rank-1子空间未能充分代表多种知识交互，而我们的rank-2建模则能够有效捕捉这些交互。我们的多步分析显示，幻觉NLEs强烈地反映了PK方向，上下文忠实的NLEs则平衡了PK和CK，而NLE的链式思考提示减少了对PK的依赖，使生成的NLE更倾向于CK。本研究通过更丰富的rank-2子空间分离提供了首个框架，用于系统研究LLMs中的多步知识交互。代码和数据：this https URL。', 'title_zh': '基于秩2子空间解耦的多步知识交互分析'}
{'arxiv_id': 'arXiv:2511.01694', 'title': 'Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman Filtering', 'authors': 'Hossein Abdi, Mingfei Sun, Wei Pan', 'link': 'https://arxiv.org/abs/2511.01694', 'abstract': 'Vision-language pre-trained models, such as CLIP, have established new benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a major challenge to achieve optimal performance on both in-distribution (ID) and out-of-distribution (OOD) datasets, especially when labeled data is scarce. Most existing fine-tuning approaches rely on first-order gradient-based optimizers, which typically suffer from slow convergence, sensitivity to step-size hyperparameters, and poor generalization in OOD settings. In contrast, second-order methods utilize local curvature information of the loss landscape to adjust the update step size. This is particularly beneficial for CLIP models, whose non-convex loss functions often contain sharp critical points. In such cases, natural gradient direction can offer more substantial and efficient per-iteration updates when fine-tuning with limited data. Natural Gradient Descent (NGD) is obtained by preconditioning the standard gradient with the inverse Fisher Information Matrix (FIM), which is computationally expensive for large models. To address this, we propose a Bayesian approximation of NGD using a Kalman filter for CLIP models. Our method combines the benefits of second-order optimization with Bayesian inference, which enhances generalization while providing uncertainty quantification. Extensive experiments conducted on diverse image classification datasets demonstrate that our algorithm consistently achieves superior--or comparable--ID performance and improved OOD robustness compared to state-of-the-art baselines. To the best of our knowledge, this work represents the first successful application of Kalman filtering to fine-tuning CLIP-based models, which enables more robust and efficient learning in vision-language tasks.', 'abstract_zh': '基于视觉-语言预训练模型的贝叶斯卡尔曼滤波细调方法：提升分布内和分布外性能', 'title_zh': '基于卡尔曼滤波的CLIP模型贝叶斯自然梯度微调'}
{'arxiv_id': 'arXiv:2511.01670', 'title': 'SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia', 'authors': 'Chaoqun Liu, Mahani Aljunied, Guizhen Chen, Hou Pong Chan, Weiwen Xu, Yu Rong, Wenxuan Zhang', 'link': 'https://arxiv.org/abs/2511.01670', 'abstract': 'We introduce SeaLLMs-Audio, the first large audio-language model (LALM) tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai (th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across diverse audio-centric tasks, spanning fine-grained audio understanding and voice-based interaction. Its key features include: 1) Multilingual: the model primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English, and Chinese; 2) Multimodal: the model accepts flexible input modalities, including audio only, text only, as well as audio with text; 3) Multi-task: the model supports a wide range of tasks, including audio analysis tasks such as Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation, Speech Emotion Recognition, Speech Question Answering, and Speech Summarization. It also enables voice-based dialogue, including answering factual, mathematical, and general knowledge queries. As a significant step towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to benefit both the regional research community and industry. To automate LALM evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves competitive performance compared with other LALMs on SEA languages.', 'abstract_zh': 'SeaLLMs-Audio：面向东南亚多种语言的首个大规模跨模态语言模型', 'title_zh': 'SeaLLMs-Audio：面向东南亚的大型音频语言模型'}
{'arxiv_id': 'arXiv:2511.01643', 'title': 'A Graph-based RAG for Energy Efficiency Question Answering', 'authors': 'Riccardo Campi, Nicolò Oreste Pinciroli Vago, Mathyas Giudici, Pablo Barrachina Rodriguez-Guisado, Marco Brambilla, Piero Fraternali', 'link': 'https://arxiv.org/abs/2511.01643', 'abstract': 'In this work, we investigate the use of Large Language Models (LLMs) within a graph-based Retrieval Augmented Generation (RAG) architecture for Energy Efficiency (EE) Question Answering. First, the system automatically extracts a Knowledge Graph (KG) from guidance and regulatory documents in the energy field. Then, the generated graph is navigated and reasoned upon to provide users with accurate answers in multiple languages. We implement a human-based validation using the RAGAs framework properties, a validation dataset comprising 101 question-answer pairs, and domain experts. Results confirm the potential of this architecture and identify its strengths and weaknesses. Validation results show how the system correctly answers in about three out of four of the cases (75.2 +- 2.7%), with higher results on questions related to more general EE answers (up to 81.0 +- 4.1%), and featuring promising multilingual abilities (4.4% accuracy loss due to translation).', 'abstract_zh': '在基于图的检索增强生成（RAG）架构中使用大型语言模型进行能源效率（EE）问答的研究', 'title_zh': '基于图的RAG在能源效率问答中的应用'}
{'arxiv_id': 'arXiv:2511.01634', 'title': 'Prompt Injection as an Emerging Threat: Evaluating the Resilience of Large Language Models', 'authors': 'Daniyal Ganiuly, Assel Smaiyl', 'link': 'https://arxiv.org/abs/2511.01634', 'abstract': 'Large Language Models (LLMs) are increasingly used in intelligent systems that perform reasoning, summarization, and code generation. Their ability to follow natural-language instructions, while powerful, also makes them vulnerable to a new class of attacks known as prompt injection. In these attacks, hidden or malicious instructions are inserted into user inputs or external content, causing the model to ignore its intended task or produce unsafe responses. This study proposes a unified framework for evaluating how resistant Large Language Models (LLMs) are to prompt injection attacks. The framework defines three complementary metrics such as the Resilience Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional Integrity Metric (IIM) to jointly measure robustness, safety, and semantic stability. We evaluated four instruction-tuned models (GPT-4, GPT-4o, LLaMA-3 8B Instruct, and Flan-T5-Large) on five common language tasks: question answering, summarization, translation, reasoning, and code generation. Results show that GPT-4 performs best overall, while open-weight models remain more vulnerable. The findings highlight that strong alignment and safety tuning are more important for resilience than model size alone. Results show that all models remain partially vulnerable, especially to indirect and direct-override attacks. GPT-4 achieved the best overall resilience (RDR = 9.8 %, SCR = 96.4 %), while open-source models exhibited higher performance degradation and lower safety scores. The findings demonstrate that alignment strength and safety tuning play a greater role in resilience than model size alone. The proposed framework offers a structured, reproducible approach for assessing model robustness and provides practical insights for improving LLM safety and reliability.', 'abstract_zh': '大型语言模型（LLMs）越来越多地被用于执行推理、总结和代码生成的智能系统中。它们遵循自然语言指令的能力虽然强大，但也使其容易受到一类新的攻击——提示注入攻击。在这些攻击中，隐藏或恶意的指令被插入用户输入或外部内容中，导致模型忽略其预定任务或产生不安全的响应。本研究提出了一种统一框架，用于评估大型语言模型（LLMs）对提示注入攻击的抵抗能力。该框架定义了三个互补的度量标准，如韧性退化指数（RDI）、安全性合规系数（SCC）和指令完整性指标（IIM），以联合衡量鲁棒性、安全性和语义稳定性。我们对四种指令调整模型（GPT-4、GPT-4o、LLaMA-3 8B Instruct 和 Flan-T5-Large）进行了评估，在五个常见的语言任务上：问答、总结、翻译、推理和代码生成。结果表明，GPT-4 在整体上表现最佳，而开源模型仍更易受到攻击。研究结果显示，强大的对齐和安全性调整比模型大小本身更为重要。结果显示，所有模型在一定程度上仍然易受攻击，尤其是对间接和直接覆盖攻击。GPT-4 达到了最佳的整体韧性（RDR = 9.8%，SCR = 96.4%），而开源模型表现出更大的性能退化和更低的安全得分。研究结果表明，对齐强度和安全性调整在韧性中的作用比单纯模型大小更为重要。提出的框架提供了一种结构化、可重复的方法，用于评估模型的鲁棒性，并提供了关于改进大语言模型（LLMs）安全性和可靠性的实用见解。', 'title_zh': 'Prompt注入作为一种新兴威胁：评估大规模语言模型的韧性'}
{'arxiv_id': 'arXiv:2511.01633', 'title': 'Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving', 'authors': 'Chengying Huan, Ziheng Meng, Yongchao Liu, Zhengyi Yang, Yun Zhu, Yue Yun, Shipeng Li, Rong Gu, Xiabao Wu, Haitao Zhang, Chuntao Hong, Shaonan Ma, Guihai Chen, Chen Tian', 'link': 'https://arxiv.org/abs/2511.01633', 'abstract': 'Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to perform step-by-step reasoning over graph-structured knowledge, but existing pipelines suffer from low accuracy, excessive token usage, high latency, and low throughput due to single-agent monolithic prompts, repeated context re-encoding, and inefficient serving execution. We present GLM, the first multi-agent Graph-CoT system co-designed with an optimized LLM serving architecture. GLM decomposes reasoning into specialized agents for classification, reasoning, action generation, and graph retrieval, enabling branching and selective context sharing to reduce prompt length and reasoning iterations while preserving reasoning quality, thereby improving accuracy and reducing overall token consumption. To scale inference, we introduce a Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache management, priority-based eviction, and pipelined execution to improve serving efficiency. Experiments demonstrate that GLM improves answer accuracy by up to 38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT baselines, enabling efficient adoption for complex real-world reasoning at scale.', 'abstract_zh': '多Agent Graph-CoT系统GLM：一种联合优化的大语言模型服务架构', 'title_zh': '扩展图链式推理：一种高效的大型语言模型服务多代理框架'}
{'arxiv_id': 'arXiv:2511.01615', 'title': 'Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers', 'authors': 'Francisco Portillo López', 'link': 'https://arxiv.org/abs/2511.01615', 'abstract': 'Linguistic errors are not merely deviations from normative grammar; they offer a unique window into the cognitive architecture of language and expose the current limitations of artificial systems that seek to replicate them. This project proposes an interdisciplinary study of linguistic errors produced by native Spanish speakers, with the aim of analyzing how current large language models (LLM) interpret, reproduce, or correct them. The research integrates three core perspectives: theoretical linguistics, to classify and understand the nature of the errors; neurolinguistics, to contextualize them within real-time language processing in the brain; and natural language processing (NLP), to evaluate their interpretation against linguistic errors. A purpose-built corpus of authentic errors of native Spanish (+500) will serve as the foundation for empirical analysis. These errors will be tested against AI models such as GPT or Gemini to assess their interpretative accuracy and their ability to generalize patterns of human linguistic behavior. The project contributes not only to the understanding of Spanish as a native language but also to the development of NLP systems that are more cognitively informed and capable of engaging with the imperfect, variable, and often ambiguous nature of real human language.', 'abstract_zh': '语言错误不仅是规范语法的偏差，它们还提供了洞察语言认知架构的独特窗口，并揭示了旨在模仿这些错误的人工智能系统的当前局限性。本项目提出一项跨学科研究，旨在分析原生西班牙语使用者产生的语言错误，并分析当前大型语言模型（LLM）如何解释、复制或纠正这些错误。研究整合了三个核心视角：理论语言学，用于分类和理解错误的本质；神经语言学，用于将错误置于大脑实时语言处理的背景下；以及自然语言处理（NLP），用于根据语言错误评估其解释。一个专门构建的原生西班牙语真实错误语料库（+500）将成为实证分析的基础。这些错误将被测试在GPT或Gemini等AI模型上，以评估其解释准确性及其对人类语言行为模式的泛化能力。该项目不仅有助于理解西班牙语作为母语，还促进了更认知化、能够应对真实人类语言的不确定、多样性和模糊性的自然语言处理系统的开发。', 'title_zh': '不完美的语言、人工智能与人类思维：关于本土西班牙语使用者语言错误的跨学科研究'}
{'arxiv_id': 'arXiv:2511.01512', 'title': 'BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification', 'authors': 'Ayesha Afroza Mohsin, Mashrur Ahsan, Nafisa Maliyat, Shanta Maria, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan', 'link': 'https://arxiv.org/abs/2511.01512', 'abstract': 'Toxic language in Bengali remains prevalent, especially in online environments, with few effective precautions against it. Although text detoxification has seen progress in high-resource languages, Bengali remains underexplored due to limited resources. In this paper, we propose a novel pipeline for Bengali text detoxification that combines Pareto class-optimized large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate detoxified sentences. To support this effort, we construct BanglaNirTox, an artificially generated parallel corpus of 68,041 toxic Bengali sentences with class-wise toxicity labels, reasonings, and detoxified paraphrases, using Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox dataset is used to fine-tune language models to produce better detoxified versions of Bengali sentences. Our findings show that Pareto-optimized LLMs with CoT prompting significantly enhance the quality and consistency of Bengali text detoxification.', 'abstract_zh': 'Bengali有毒语言仍然普遍存在，尤其是在网络环境中，对其有效的防范措施较少。尽管在高资源语言的文本脱毒方面取得了一定进展，但由于资源有限，孟加拉语仍被大大忽视。在本文中，我们提出了一种结合Pareto类优化大型语言模型（LLMs）和Chain-of-Thought（CoT）提示的新颖管道，用于生成脱毒句子。为此，我们构建了包含68,041条有毒孟加拉语句子及其按类别标注的有毒性标签、理由和脱毒 paraphrase 的人工生成平行语料库 BanglaNirTox，使用在随机样本上评估的Pareto优化的LLMs进行构建。BanglaNirTox数据集用于细调语言模型，以生成更好的脱毒版本的孟加拉语句子。我们的研究发现，结合CoT提示的Pareto优化的LLMs显著提高了孟加拉语文本脱毒的质量和一致性。', 'title_zh': 'BanglaNirTox：孟加拉语可解释AI大规模并行语料库在文本去毒中的应用'}
{'arxiv_id': 'arXiv:2511.01463', 'title': 'HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA', 'authors': 'Lei Hu, Yongjing Ye, Shihong Xia', 'link': 'https://arxiv.org/abs/2511.01463', 'abstract': 'The expansion of instruction-tuning data has enabled foundation language models to exhibit improved instruction adherence and superior performance across diverse downstream tasks. Semantically-rich 3D human motion is being progressively integrated with these foundation models to enhance multimodal understanding and cross-modal generation capabilities. However, the modality gap between human motion and text raises unresolved concerns about catastrophic forgetting during this integration. In addition, developing autoregressive-compatible pose representations that preserve generalizability across heterogeneous downstream tasks remains a critical technical barrier. To address these issues, we propose the Human Motion-Vision-Language Model (HMVLM), a unified framework based on the Mixture of Expert Low-Rank Adaption(MoE LoRA) strategy. The framework leverages the gating network to dynamically allocate LoRA expert weights based on the input prompt, enabling synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting during instruction-tuning, we introduce a novel zero expert that preserves the pre-trained parameters for general linguistic tasks. For pose representation, we implement body-part-specific tokenization by partitioning the human body into different joint groups, enhancing the spatial resolution of the representation. Experiments show that our method effectively alleviates knowledge forgetting during instruction-tuning and achieves remarkable performance across diverse human motion downstream tasks.', 'abstract_zh': '基于MoE LoRA策略的人机motion-vision-language模型（HMVLM）：同步多任务微调与减轻知识遗忘', 'title_zh': 'HMVLM: 人类运动-视觉-语言模型 via MoE LoRA'}
{'arxiv_id': 'arXiv:2511.01359', 'title': 'PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise', 'authors': 'Sapir Harary, Eran Hirsch, Aviv Slobodkin, David Wan, Mohit Bansal, Ido Dagan', 'link': 'https://arxiv.org/abs/2511.01359', 'abstract': 'Natural Language Inference (NLI) models have been used in various ways to improve the factuality of LLM outputs. This is typically done by applying an NLI model to judge whether the model output is entailed from the supposed evidence, triggering some corrective actions, such as beam reranking at inference time or RL rewards during training. While NLI models are trained to detect factual inconsistencies over complete sentences, decisions in the common autoregressive generation architecture are made for each evolving text prefix, during decoding. Addressing this setting, we generalize the entailment detection task to apply over arbitrary text prefixes, and suggest its utility for improving generation faithfulness. Providing suitable evaluation and training datasets for this task, we train MiniTruePrefixes, a novel specialized model that better detects factual inconsistencies over text prefixes, outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level entailment. We further demonstrate that integrating MiniTruePrefixes into a controlled decoding framework substantially improves factual consistency in abstractive summarization. When guided by MiniTruePrefixes, LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from the same model family, while using only half the memory.', 'abstract_zh': '自然语言推理（NLI）模型已被用于多种方式以提高大规模语言模型（LLM）输出的事实准确性。这通常通过应用NLI模型判断模型输出是否蕴含于假设的证据中，并触发一些纠正措施，如推理时的束搜索重排序或训练时的RL奖励。尽管NLI模型被训练以检测整个句子的事实不一致，但在常见的自回归生成架构中，每个 evolving 文本前缀在解码过程中都会做出决策。为应对这一情况，我们将蕴含检测任务泛化为应用于任意文本前缀，并建议其在提高生成忠实度方面的效用。提供合适的评估和训练数据集，我们训练了一个新型的专业化模型 MiniTruePrefixes，该模型在文本前缀级别的蕴含检测上表现更佳，相比相似的基础NLI模型，在前缀级别蕴含检测上F1分数提高了5-14个点。进一步研究表明，在受控解码框架中集成 MiniTruePrefixes 显著提高了抽象总结的事实一致性。当受到 MiniTruePrefixes 的指导时，LLaMA-3.2-3B-Instruct 在忠实度和运行时间上与同系列的8B模型相当，但仅使用一半的内存。', 'title_zh': '前缀NLI：一出现事实不一致即检测'}
{'arxiv_id': 'arXiv:2511.01354', 'title': 'Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series', 'authors': 'Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang, Xiangzhong Fang', 'link': 'https://arxiv.org/abs/2511.01354', 'abstract': 'Recently, the demand for small and efficient reasoning models to support real-world applications has driven the development of knowledge distillation techniques that balance reasoning performance and inference speed. In this paper, we further extend the DistilQwen model family, initialized from the Qwen models, by introducing four model series specifically designed to meet industrial requirements. The distilled model collection comprises: (1) slow-thinking models, optimized for reasoning tasks that require high accuracy; (2) two series of adaptive-thinking models, which dynamically adjust reasoning strategies based on input tasks to maximize efficiency across diverse scenarios; and (3) distilled reward models, which enable further reinforcement learning of reasoning models using distilled knowledge. Comprehensive evaluations across multiple benchmarks demonstrate both high inference efficiency and strong reasoning performance for these models, as well as the practical utility of distilled reward models. We further show that these models support industry practitioners by providing scalable training and inference functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence) platform.', 'abstract_zh': '最近，对小而高效的推理模型的需求推动了兼顾推理性能与推理速度的知识蒸馏技术的发展。在本文中，我们进一步扩展了由Qwen模型初始化的DistilQwen模型家族，通过引入四种专门针对工业需求设计的模型系列。蒸馏模型集合包括：（1）慢思考模型，针对要求高准确性的推理任务进行优化；（2）两个系列的自适应思考模型，根据输入任务动态调整推理策略以在多样化场景中最大化效率；以及（3）蒸馏奖励模型，利用蒸馏知识进一步强化推理模型的学习。跨多个基准的全面评估表明，这些模型在推理效率和推理性能上均表现出色，并证明了蒸馏奖励模型的实际应用价值。我们还展示了这些模型如何通过阿里巴巴云PAI平台（人工智能平台）提供可扩展的训练和推理功能，支持行业实践者。', 'title_zh': '使用DistilQwen思考：四种精简推理与奖励模型系列的故事'}
{'arxiv_id': 'arXiv:2511.01323', 'title': 'DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness', 'authors': 'Jiabao Ji, Min Li, Priyanshu Kumar, Shiyu Chang, Saloni Potdar', 'link': 'https://arxiv.org/abs/2511.01323', 'abstract': 'Large language models (LLMs) with integrated search tools show strong promise in open-domain question answering (QA), yet they often struggle to produce complete answer set to complex questions such as Which actor from the film Heat won at least one Academy Award?, which requires (1) distinguishing between multiple films sharing the same title and (2) reasoning across a large set of actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate both challenges jointly. To address this, we introduce DeepAmbigQAGen, an automatic data generation pipeline that constructs QA tasks grounded in text corpora and linked knowledge graph, generating natural and verifiable questions that systematically embed name ambiguity and multi-step reasoning. Based on this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop reasoning and half of them explicit name ambiguity resolving. Experiments reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous questions. These findings highlight the need for more robust QA systems aimed at information gathering and answer completeness.', 'abstract_zh': '具有集成搜索工具的大语言模型在开放域问答中显示出强大的潜力，但在回答如“哪位演员因电影《Heat》获得至少一个奥斯卡奖？”这类需要区分同名不同电影和跨大量演员进行多步推理的问题时，往往难以提供完整的答案集。现有问答基准数据集鲜有同时评估这两方面挑战。为解决这一问题，我们引入了DeepAmbigQAGen，这是一种自动数据生成管道，基于文本语料库和链接知识图谱构建问答任务，生成自然且可验证的问题，并系统性地嵌入名称歧义和多步推理。在此基础上，我们构建了包含3600个问题的数据集DeepAmbigQA，其中一半问题要求显式解决名称歧义和多跳推理。实验结果显示，即使是最先进的GPT-5模型也只能在歧义问题上达到0.13的准确匹配率，在无歧义问题上达到0.21的准确匹配率。这些结果突显了需要更稳健的问答系统，以实现信息收集和答案完整性目标。', 'title_zh': 'DEEPAMBIGQA: 模糊多跳问题用于评估LLM答案完整性'}
{'arxiv_id': 'arXiv:2511.01316', 'title': 'Exploringand Unleashing the Power of Large Language Models in CI/CD Configuration Translation', 'authors': 'Chong Wang, Chen Zhang, Jiajun Wu, Wunan Guo, Jianfeng Qu, Yewen Tian, Yang Liu', 'link': 'https://arxiv.org/abs/2511.01316', 'abstract': 'Continuous Integration (CI) is a cornerstone of modern collaborative software development, and numerous CI platforms are available. Differences in maintenance overhead, reliability, and integration depth with code-hosting platforms make migration between CI platforms a common practice. A central step in migration is translating CI configurations, which is challenging due to the intrinsic complexity of CI configurations and the need to understand semantic differences and relationships across CI platforms.\nWith the advent of large language models (LLMs), recent advances in software engineering highlight their potential for CI configuration translation. In this paper, we present a study on LLM-based CI configuration translation, focusing on the migration from Travis CI to GitHub Actions. First, using 811 migration records, we quantify the effort involved and find that developers read an average of 38 lines of Travis configuration and write 58 lines of GitHub Actions configuration, with nearly half of the migrations requiring multiple commits. We further analyze translations produced by each of the four LLMs and identify 1,121 issues grouped into four categories: logic inconsistencies (38%), platform discrepancies (32%), environment errors (25%), and syntax errors (5%). Finally, we evaluate three enhancement strategies and show that combining guideline-based prompting with iterative refinement achieves the best performance, reaching a Build Success Rate of 75.5%-nearly a threefold improvement over GPT-4o with a basic prompt.', 'abstract_zh': '基于大语言模型的持续集成配置翻译：从Travis CI到GitHub Actions的迁移研究', 'title_zh': '探索并释放大型语言模型在CI/CD配置翻译中的潜力'}
{'arxiv_id': 'arXiv:2511.01282', 'title': 'When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding', 'authors': 'Min Fang, Zhihui Fu, Qibin Zhao, Jun Wang', 'link': 'https://arxiv.org/abs/2511.01282', 'abstract': 'Speculative decoding (SD) has emerged as an effective technique to accelerate large language model (LLM) inference without compromising output quality. However, the achievable speedup largely depends on the effectiveness of the drafting model. While model-based methods like EAGLE-2 are accurate but costly, retrieval-enhanced methods like SAM-Decoding rely on heuristic switching strategies that often trigger unnecessary retrievals. To address this, we propose ReSpec (\\textbf{Re}trieval-enhanced \\textbf{Spe}culative Decoding), a novel framework that transforms heuristic drafter switching into adaptive decision-making. ReSpec features three core innovations: 1) An \\textbf{entropy-guided adaptive trigger} quantifies contextual predictability to initiate retrieval only when uncertainty is low, avoiding costly low-quality speculations. 2) A \\textbf{feedback-driven candidate selection} leverages historical feedback to organize multiple high-quality candidates for parallel verification, maximizing retrieval utility. 3) A source-aware \\textbf{relaxed verification strategy} applies strict checks to model-generated drafts while using a relaxed verification for retrieved drafts, achieving a better balance between accuracy and efficiency. Extensive experiments on Spec-Bench demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming EAGLE-2 and SAM-Decoding by over $33\\%$ and $25\\%$, respectively, while maintaining output quality.', 'abstract_zh': '检索增强推测解码（ReSpec）：一种新颖的推测解码框架', 'title_zh': '何时、何物、如何进行：重新思考检索增强推测解码'}
{'arxiv_id': 'arXiv:2511.01268', 'title': 'Rescuing the Unpoisoned: Efficient Defense against Knowledge Corruption Attacks on RAG Systems', 'authors': 'Minseok Kim, Hankook Lee, Hyungjoon Koo', 'link': 'https://arxiv.org/abs/2511.01268', 'abstract': 'Large language models (LLMs) are reshaping numerous facets of our daily lives, leading widespread adoption as web-based services. Despite their versatility, LLMs face notable challenges, such as generating hallucinated content and lacking access to up-to-date information. Lately, to address such limitations, Retrieval-Augmented Generation (RAG) has emerged as a promising direction by generating responses grounded in external knowledge sources. A typical RAG system consists of i) a retriever that probes a group of relevant passages from a knowledge base and ii) a generator that formulates a response based on the retrieved content. However, as with other AI systems, recent studies demonstrate the vulnerability of RAG, such as knowledge corruption attacks by injecting misleading information. In response, several defense strategies have been proposed, including having LLMs inspect the retrieved passages individually or fine-tuning robust retrievers. While effective, such approaches often come with substantial computational costs.\nIn this work, we introduce RAGDefender, a resource-efficient defense mechanism against knowledge corruption (i.e., by data poisoning) attacks in practical RAG deployments. RAGDefender operates during the post-retrieval phase, leveraging lightweight machine learning techniques to detect and filter out adversarial content without requiring additional model training or inference. Our empirical evaluations show that RAGDefender consistently outperforms existing state-of-the-art defenses across multiple models and adversarial scenarios: e.g., RAGDefender reduces the attack success rate (ASR) against the Gemini model from 0.89 to as low as 0.02, compared to 0.69 for RobustRAG and 0.24 for Discern-and-Answer when adversarial passages outnumber legitimate ones by a factor of four (4x).', 'abstract_zh': '大型语言模型（LLMs）正在重塑我们日常生活的多个方面，广泛作为基于网络的服务被采用。尽管LLMs具有高度的灵活性，但它们面临着生成虚假内容和缺乏获取最新信息的能力等显著挑战。近期，为了应对这些局限性，检索增强生成（RAG）作为一种有前景的方法逐渐兴起，通过利用外部知识源生成基于事实的响应。一个典型的RAG系统由i) 一个检索器，从知识库中探查相关段落，并ii) 一个生成器，根据检索到的内容制定响应组成。然而，与其他AI系统一样，RAG也显示出脆弱性，例如，通过注入误导性信息的Known-Antagonize Attack即知识污染攻击。针对此，已有相关防御策略被提出，包括让LLMs单独检查检索的段落或对检索器进行鲁棒性微调。尽管这些方法有效，但它们通常伴随着重大的计算成本。\n在此工作当中，我们提出了RAGDefender，这是一种针对实际RAG部署中的知识污染攻击（例如，由数据污染引起的攻击）的资源高效防御机制。RAGDefender在检索后的阶段运行，利用轻量级机器学习技术来检测和过滤掉恶意内容，而无需额外的模型训练或推理。我们的实证评估表明，RAGDefender在多个模型和对抗场景中均优于现有最先进的防御方法：例如，当对抗段落的数量是合法段落的四倍时，RAGDefender将Gemini模型的攻击成功率（ASR）从0.89降低至0.02，而RobustRAG和Discern-and-Answer分别为0.69和0.24。', 'title_zh': '救援未被污染者：针对RAG系统知识污染攻击的有效防御'}
{'arxiv_id': 'arXiv:2511.01213', 'title': 'Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering', 'authors': 'Riddhi Jain, Manasi Patwardhan, Parijat Deshpande, Venkataramana Runkana', 'link': 'https://arxiv.org/abs/2511.01213', 'abstract': 'The immense diversity in the culture and culinary of Indian cuisines calls attention to the major shortcoming of the existing Visual Question Answering(VQA) systems which are inclined towards the foods from Western region. Recent attempt towards building a VQA dataset for Indian food is a step towards addressing this challenge. However, their approach towards VQA follows a two-step process in which the answer is generated first, followed by the explanation of the expected answer. In this work, we claim that food VQA requires to follow a multi-step reasoning process to arrive at an accurate answer, especially in the context of India food, which involves understanding complex culinary context and identifying relationships between various food items. With this hypothesis we create reasoning chains upon the QA with minimal human intervention. We fine-tune smaller LLMs and VLMs with auto-validated reasoning chains and further train them using reinforcement learning with larger data. With augmentation of reasoning chains, we observed accuracy improvement of an average 10 percentage points on the baseline. We provide detailed analysis in terms the effect of addition of reasoning chains for the Indian Food VQA task.\nIndex Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge Graph.', 'abstract_zh': '印度菜文化与烹饪的极大多样性引起了对现有视觉问答(VQA)系统主要缺陷的关注，这些系统倾向于西方地区的食物。最近为印度食物构建VQA数据集的努力是应对这一挑战的一步。然而，他们的VQA方法遵循两步过程，即先生成答案，然后解释预期的答案。本文我们声称，食品VQA需要遵循多步推理过程才能得出准确的答案，特别是在印度食物的背景下，这涉及理解复杂的烹饪背景并识别各种食物项目之间的关系。基于这一假设，我们在最少的人工干预下构建推理链。我们微调较小的LLM和VLM，并使用自验证的推理链进一步通过强化学习进行训练。通过增加推理链，我们在基线上观察到平均10个百分点的准确率提升。我们详细分析了添加推理链对印度食物VQA任务的影响。关键词 - 食品VQA，推理链，强化学习，知识图谱。', 'title_zh': '思食链：推理链诱导的食品视觉问答'}
{'arxiv_id': 'arXiv:2511.01202', 'title': 'Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs', 'authors': 'Bo Bai', 'link': 'https://arxiv.org/abs/2511.01202', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities in numerous real- world applications. While the vast majority of research conducted from an experimental perspective is progressing rapidly, it demands substantial computational power, data, and other resources. Therefore, how to open the black-box of LLMs from a theoretical standpoint has become a critical challenge. This paper takes the theory of rate-distortion function, directed information, and Granger causality as its starting point to investigate the information-theoretic principles behind LLMs, leading to the development of semantic information theory for LLMs, where the fundamental unit is token, rather than bits that lacks any semantic meaning. By defining the probabilistic model of LLMs, we discuss structure-agnostic information-theoretic measures, such as the directed rate- distortion function in pre-training, the directed rate-reward function in post-training, and the semantic information flow in inference phase. This paper also delves deeply into the theory of token-level semantic embedding and the information-theoretically optimal vectorization method. Thereafter, we propose a general definition of autoregression LLM, where the Transformer architecture and its performance such as ELBO, generalization error bound, memory capacity, and semantic information measures can be derived theoretically. Other architectures, such as Mamba/Mamba2 and LLaDA, are also discussed in our framework. Consequently, this paper provides a theoretical framework for understanding LLMs from the perspective of semantic information theory, which also offers the necessary theoretical tools for further in-depth research.', 'abstract_zh': '大型语言模型（LLMs）在众多实际应用中展示了非凡的能力。尽管从实验视角进行的研究进展迅速，但迫切需求大量的计算资源、数据和其他资源。因此，如何从理论角度揭开LLMs的黑箱已经成为一个关键挑战。本文以速率-失真函数、定向信息和Granger因果性为基础，探索LLMs背后的信息论原理，发展出以标记而非比特为基本单位的语义信息理论。通过定义LLMs的概率模型，我们讨论了结构无关的信息论度量，如预训练中的定向速率-失真函数、后训练中的定向速率-奖励函数以及推理阶段的语义信息流。本文还深入探讨了标记级语义嵌入和信息论最优向量化方法的理论。随后，我们提出了自回归LLMs的一般定义，其中Transformer架构及其ELBO、泛化误差界、记忆容量和语义信息度量可以通过理论推导得出。我们也在框架内讨论了Mamba/Mamba2和LLaDA等其他架构。最终，本文提供了一种从语义信息理论视角理解LLMs的理论框架，也为进一步深入研究提供了必要的理论工具。', 'title_zh': '忘掉BIT，一切关于TOKEN：向量形意信息理论探索——面向大规模语言模型'}
{'arxiv_id': 'arXiv:2511.01188', 'title': 'ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction', 'authors': 'Lvhua Wu, Xuefeng Jiang, Sheng Sun, Tian Wen, Yuwei Wang, Min Liu', 'link': 'https://arxiv.org/abs/2511.01188', 'abstract': 'The rapid spread of fake news threatens social stability and public trust, rendering its detection an imperative research priority. Although large language models (LLMs) excel at numerous natural language processing tasks with their remarkable contextual understanding and extensive prior knowledge, the time-bounded knowledge coverage and tendency for generating hallucination content reduce their reliability when handling fast-evolving news streams. Furthermore, models trained on existing static datasets also often lack the generalization needed for emerging news topics. To address these challenges, we propose ZoFia, a novel two-stage zero-shot fake news detection framework. First, we introduce Hierarchical Salience to quantify the importance of entities in the news content, and propose the SC-MMR algorithm to effectively select an informative and diverse set of keywords that serve as queries for retrieving up-to-date external evidence. Subsequently, a multi LLM interactive system, in which each agent assumes a distinct role, performs multi-view collaborative analysis and adversarial debate over the news text and its related information, and finally produces an interpretable and robust judgment. Comprehensive experiments on two public datasets demonstrate that ZoFia obviously outperforms existing zero-shot baselines and most of few-shot methods. Our codes will be open-sourced to facilitate related communities.', 'abstract_zh': '快速传播的假新闻威胁社会稳定和公众信任，检测假新闻成为一项迫在眉睫的研究优先事项。尽管大型语言模型（LLMs）在自然语言处理任务中表现出色，具备卓越的上下文理解和广泛的前提知识，但知识时效性的限制以及生成幻觉内容的倾向降低了它们在处理快速演变的新闻流中的可靠性。此外，基于现有静态数据集训练的模型往往缺乏应对新兴新闻主题所需的一般化能力。为应对这些挑战，我们提出了一种新颖的两阶段零样本假新闻检测框架ZoFia。首先，我们引入了层次相关性来量化新闻内容中实体的重要性，并提出了SC-MMR算法以有效地选择一组信息丰富且多样化的关键词作为查询，用于检索最新的外部证据。随后，一个多种语言模型交互系统中，每个代理承担不同的角色，进行新闻文本及其相关信息的多视图协作分析和对抗性辩论，并最终产生可解释和可靠的判断。在两个公开数据集上的全面实验表明，ZoFia显然优于现有的零样本基线方法和大多数少样本方法。我们的代码将开源以促进相关社区的发展。', 'title_zh': 'ZoFia: 基于实体引导检索与多LLM交互的零样本假新闻检测'}
{'arxiv_id': 'arXiv:2511.01144', 'title': 'AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat Intelligence', 'authors': 'Md Tanvirul Alam, Dipkamal Bhusal, Salman Ahmad, Nidhi Rastogi, Peter Worth', 'link': 'https://arxiv.org/abs/2511.01144', 'abstract': 'Large Language Models (LLMs) have demonstrated strong capabilities in natural language reasoning, yet their application to Cyber Threat Intelligence (CTI) remains limited. CTI analysis involves distilling large volumes of unstructured reports into actionable knowledge, a process where LLMs could substantially reduce analyst workload. CTIBench introduced a comprehensive benchmark for evaluating LLMs across multiple CTI tasks. In this work, we extend CTIBench by developing AthenaBench, an enhanced benchmark that includes an improved dataset creation pipeline, duplicate removal, refined evaluation metrics, and a new task focused on risk mitigation strategies. We evaluate twelve LLMs, including state-of-the-art proprietary models such as GPT-5 and Gemini-2.5 Pro, alongside seven open-source models from the LLaMA and Qwen families. While proprietary LLMs achieve stronger results overall, their performance remains subpar on reasoning-intensive tasks, such as threat actor attribution and risk mitigation, with open-source models trailing even further behind. These findings highlight fundamental limitations in the reasoning capabilities of current LLMs and underscore the need for models explicitly tailored to CTI workflows and automation.', 'abstract_zh': '大型语言模型（LLMs）在自然语言推理方面展现了强大的能力，但在网络安全威胁 intelligence (CTI) 领域的应用仍然有限。CTI 分析涉及从大量无结构报告中提炼出可操作的知识，这一过程可以通过 LLMs 大幅减少分析师的工作量。CTIBench 引入了一个多任务综合基准，用于评估 LLMs 在 CTI 任务中的表现。在此基础上，我们开发了 AthenaBench，一个增强的基准，包括改进的数据集创建pipeline、去重、细化的评估指标以及一个专注于风险缓解策略的新任务。我们评估了十二种 LLMs，包括最先进的私有模型如 GPT-5 和 Gemini-2.5 Pro，以及七种来自 LLaMA 和 Qwen 家族的开源模型。尽管私有 LLMs 在总体表现上更优，但在推理密集型任务（如威胁行为者归因和风险缓解）上依然表现不佳，开源模型的表现更是远远落后。这些发现揭示了当前 LLMs 在推理能力方面的根本局限性，并强调了专门针对 CTI 工作流和自动化进行模型训练的需求。', 'title_zh': 'AthenaBench：用于评估语言模型在网络安全威胁情报中的动态基准'}
{'arxiv_id': 'arXiv:2511.01082', 'title': 'GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction', 'authors': 'Narges Ghasemi, Amir Ziashahabi, Salman Avestimehr, Cyrus Shahabi', 'link': 'https://arxiv.org/abs/2511.01082', 'abstract': "Image geolocalization, the task of determining an image's geographic origin, poses significant challenges, largely due to visual similarities across disparate locations and the large search space. To address these issues, we propose a hierarchical sequence prediction approach inspired by how humans narrow down locations from broad regions to specific addresses. Analogously, our model predicts geographic tokens hierarchically, first identifying a general region and then sequentially refining predictions to increasingly precise locations. Rather than relying on explicit semantic partitions, our method uses S2 cells, a nested, multiresolution global grid, and sequentially predicts finer-level cells conditioned on visual inputs and previous predictions. This procedure mirrors autoregressive text generation in large language models. Much like in language modeling, final performance depends not only on training but also on inference-time strategy. We investigate multiple top-down traversal methods for autoregressive sampling, incorporating techniques from test-time compute scaling used in language models. Specifically, we integrate beam search and multi-sample inference while exploring various selection strategies to determine the final output. This enables the model to manage uncertainty by exploring multiple plausible paths through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k datasets against two distinct sets of baselines: those that operate without a Multimodal Large Language Model (MLLM) and those that leverage one. In the MLLM-free setting, our model surpasses other comparable baselines on nearly all metrics, achieving state-of-the-art performance with accuracy gains of up to 13.9%. When augmented with an MLLM, our model outperforms all baselines, setting a new state-of-the-art across all metrics. The source code is available at this https URL.", 'abstract_zh': '图像地理定位任务涉及确定图像的地理起源，由于不同地点之间的视觉相似性以及庞大的搜索空间，该任务面临着重大挑战。为了应对这些问题，我们提出了一个类人类从广域到具体地址逐步缩小地理区域的层次化序列预测方法。我们的模型按照层次顺序预测地理标记，首先识别一个大致的区域，然后逐步细化预测至越来越精确的位置。该方法不依赖于显式的语义分区，而是利用嵌套的多分辨率全球网格S2单元，并在视觉输入以及先前预测的基础上逐步预测更精细级别的单元。这一过程类似于大型语言模型中的自回归文本生成。和语言建模类似，最终性能不仅取决于训练，也依赖于推理阶段的策略。我们研究了几种自上而下的遍历方法来优化自回归采样，借鉴了语言模型中测试时计算缩放的技术，具体包括集成束搜索和多样本推理，并探索了多种选择策略来确定最终输出。这种方法允许模型通过在框架内探索多个合理路径来管理不确定性。我们在Im2GPS3k和YFCC4k数据集上将我们的方法与两类基线进行对比：一类是不使用多模大型语言模型（MLLM）的方法，另一类是利用MLLM的方法。在不使用MLLM的情境下，我们的模型在几乎所有指标上都超越了其他可比基线，实现了高达13.9%的准确率提升。当与MLLM结合时，我们的模型在所有指标上都超越了所有基线，重新定义了新的性能标准。源代码可在此处访问。', 'title_zh': 'GeoToken：基于下一个词预测的图像层级地理定位'}
{'arxiv_id': 'arXiv:2511.01061', 'title': 'Energy-Efficient Deep Learning Without Backpropagation: A Rigorous Evaluation of Forward-Only Algorithms', 'authors': 'Przemysław Spyra, Witold Dzwinel', 'link': 'https://arxiv.org/abs/2511.01061', 'abstract': "The long-held assumption that backpropagation (BP) is essential for state-of-the-art performance is challenged by this work. We present rigorous, hardware-validated evidence that the Mono-Forward (MF) algorithm, a backpropagation-free method, consistently surpasses an optimally tuned BP baseline in classification accuracy on its native Multi-Layer Perceptron (MLP) architectures. This superior generalization is achieved with profound efficiency gains, including up to 41% less energy consumption and up to 34% faster training. Our analysis, which charts an evolutionary path from Geoffrey Hinton's Forward-Forward (FF) to the Cascaded Forward (CaFo) and finally to MF, is grounded in a fair comparative framework using identical architectures and universal hyperparameter optimization. We further provide a critical re-evaluation of memory efficiency in BP-free methods, empirically demonstrating that practical overhead can offset theoretical gains. Ultimately, this work establishes MF as a practical, high-performance, and sustainable alternative to BP for MLPs.", 'abstract_zh': '无回传的Mono-前向算法挑战了对最新性能必不可少的传统 assumptions 并在多层感知机架构上的一致性分类精度上超越了最优调优的回传算法，同时实现了显著的效率提升。', 'title_zh': '不使用反向传播的能源高效深度学习：前向算法的严格评估'}
{'arxiv_id': 'arXiv:2511.01019', 'title': 'OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights', 'authors': 'Bowen Chen, Jayesh Gajbhar, Gregory Dusek, Rob Redmon, Patrick Hogan, Paul Liu, DelWayne Bohnenstiehl, Dongkuan, Ruoying He', 'link': 'https://arxiv.org/abs/2511.01019', 'abstract': 'Artificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified "hallucinations" undermining scientific rigor. We present OceanAI, a conversational platform that integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by the National Oceanic and Atmospheric Administration (NOAA). Each query such as "What was Boston Harbor\'s highest water level in 2024?" triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations. In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. Designed for extensibility, OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding outputs and verifiable observations, OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans. A public demonstration is available at this https URL.', 'abstract_zh': '人工智能正在转型科学领域，然而通用会话人工智能系统常生成未验证的“幻觉”，损害了科学严谨性。我们提出了OceanAI，这是一种将开源大型语言模型的自然语言流畅性与国家海洋和大气管理局（NOAA）提供的实时参数化权威海洋数据流结合的会话平台。每当用户询问如“2024年波士顿港的最高水位是多少？”时，OceanAI会触发实时API调用，识别、解析并综合相关数据集，生成可重复的自然语言回答和数据可视化。在与三种广泛使用的AI聊天界面产品的盲测中，只有OceanAI提供了来源于NOAA的数据及其原始数据参考；其他产品要么拒绝回答，要么提供没有依据的结果。OceanAI设计时考虑了扩展性，连接了多种NOAA数据产品和变量，支持海啸预警、生态系统评估和水质监测等应用。通过植根于可验证的观测，OceanAI推进了透明度、可重复性和信任，提供了AI驱动的海洋决策支持的可扩展框架。更多详情请访问此链接。', 'title_zh': 'OceanAI: 一个准确、透明、近实时的海洋学对话平台'}
{'arxiv_id': 'arXiv:2511.00985', 'title': 'ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL', 'authors': 'Yiwen Jiao, Tonghui Ren, Yuche Gao, Zhenying He, Yinan Jing, Kai Zhang, X. Sean Wang', 'link': 'https://arxiv.org/abs/2511.00985', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable progress in translating natural language to SQL, but a significant semantic gap persists between their general knowledge and domain-specific semantics of databases. Historical translation logs constitute a rich source of this missing in-domain knowledge, where SQL queries inherently encapsulate real-world usage patterns of database schema. Existing methods primarily enhance the reasoning process for individual translations but fail to accumulate in-domain knowledge from past translations. We introduce ORANGE, an online self-evolutionary framework that constructs database-specific knowledge bases by parsing SQL queries from translation logs. By accumulating in-domain knowledge that contains schema and data semantics, ORANGE progressively reduces the semantic gap and enhances the accuracy of subsequent SQL translations. To ensure reliability, we propose a novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic tracking, which reduces semantic errors during knowledge generation. Experiments on multiple benchmarks confirm the practicality of ORANGE, demonstrating its effectiveness for real-world Text-to-SQL deployment, particularly in handling complex and domain-specific queries.', 'abstract_zh': '大型语言模型（LLMs）在将自然语言翻译为SQL方面取得了显著进展，但其通用知识与数据库领域的特定语义之间仍存在显著的语义差距。历史翻译日志构成了这种缺失领域知识的丰富来源，其中SQL查询本质上封装了数据库模式的现实世界使用模式。现有方法主要增强单个翻译的推理过程，但未能从过去的翻译中积累领域知识。我们介绍了一种在线自进化框架ORANGE，该框架通过解析翻译日志中的SQL查询来构建特定于数据库的知识库。通过积累包含模式和数据语义的领域知识，ORANGE逐步缩小了语义差距，并提高了后续SQL翻译的准确性。为了确保可靠性，我们提出了一种新的嵌套Chain-of-Thought SQL-to-Text策略，带有元组语义跟踪，该策略在知识生成过程中减少了语义错误。在多个基准测试上的实验证实了ORANGE的实用性，特别是在处理复杂和领域特定查询方面展示了其在实际Text-to-SQL部署中的有效性。', 'title_zh': 'ORANGE：一种基于领域知识的文本到SQL的在线反思与生成框架'}
{'arxiv_id': 'arXiv:2511.00960', 'title': 'The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles', 'authors': 'Abhinav P M, Ojasva Saxena, Oswald C, Parameswari Krishnamurthy', 'link': 'https://arxiv.org/abs/2511.00960', 'abstract': "The extent to which large language models (LLMs) can perform culturally grounded reasoning across non-English languages remains underexplored. This paper examines the reasoning and self-assessment abilities of LLMs across seven major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and Telugu. We introduce a multilingual riddle dataset combining traditional riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5 Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under seven prompting strategies. In the first stage, we assess riddle-solving performance and find that while Gemini 2.5 Pro performs best overall, few-shot methods yield only marginal gains, and accuracy varies notably across languages. In the second stage, we conduct a self-evaluation experiment to measure reasoning consistency. The results reveal a key finding: a model's initial accuracy is inversely correlated with its ability to identify its own mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34% True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are substantially more self-aware (42.09% True Negative Rate). These results point to clear gaps in multilingual reasoning and highlight the need for models that not only reason effectively but also recognize their own limitations.", 'abstract_zh': '大型语言模型在非英语语言中进行文化 grounding 推理的能力尚待探索：七种印度语言中的推理与自评估能力研究', 'title_zh': 'Reflection之谜：利用印度谜语评估多语言大语言模型的推理能力和自我意识能力'}
{'arxiv_id': 'arXiv:2511.00879', 'title': 'Assessing LLM Reasoning Steps via Principal Knowledge Grounding', 'authors': 'Hyeon Hwang, Yewon Cho, Chanwoong Yoon, Yein Park, Minju Song, Kyungjae Lee, Gangwoo Kim, Jaewoo Kang', 'link': 'https://arxiv.org/abs/2511.00879', 'abstract': "Step-by-step reasoning has become a standard approach for large language models (LLMs) to tackle complex tasks. While this paradigm has proven effective, it raises a fundamental question: How can we verify that an LLM's reasoning is accurately grounded in knowledge? To address this question, we introduce a novel evaluation suite that systematically assesses the knowledge grounding of intermediate reasoning. Our framework comprises three key components. (1) Principal Knowledge Collection, a large-scale repository of atomic knowledge essential for reasoning. Based on the collection, we propose (2) knowledge-grounded evaluation metrics designed to measure how well models recall and apply prerequisite knowledge in reasoning. These metrics are computed by our (3) evaluator LLM, a lightweight model optimized for cost-effective and reliable metric computation. Our evaluation suite demonstrates remarkable effectiveness in identifying missing or misapplied knowledge elements, providing crucial insights for uncovering fundamental reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these metrics can be integrated into preference optimization, showcasing further applications of knowledge-grounded evaluation.", 'abstract_zh': '逐步推理已成为大型语言模型（LLMs）应对复杂任务的标准方法。尽管这一范式已被证明是有效的，但它提出了一个基本问题：我们如何验证LLM的推理是否准确地扎根于知识中？为了解决这一问题，我们引入了一个新的评估套件，系统性地评估中间推理的知识扎根情况。该框架包含三个关键组件。（1）主干知识集合，这是一个大型原子知识库，对推理至关重要。基于此集合，我们提出（2）基于知识的评估指标，用于衡量模型在推理中回忆和应用先决知识的效果。这些指标通过我们的（3）评估LLM（评估模型）进行计算，该模型经过优化，能够提供经济高效且可靠的指标计算。我们的评估套件在识别缺失或误用的知识元素方面表现出显著的效果，对于揭示LLMs的根本推理缺陷提供了关键见解。除了评估之外，我们展示了如何将这些指标集成到偏好优化中，展示了基于知识评估的进一步应用。', 'title_zh': '通过主要知识定位评估大语言模型的推理步骤'}
{'arxiv_id': 'arXiv:2511.00847', 'title': 'Pay for The Second-Best Service: A Game-Theoretic Approach Against Dishonest LLM Providers', 'authors': 'Yuhan Cao, Yu Wang, Sitong Liu, Miao Li, Yixin Tao, Tianxing He', 'link': 'https://arxiv.org/abs/2511.00847', 'abstract': 'The widespread adoption of Large Language Models (LLMs) through Application Programming Interfaces (APIs) induces a critical vulnerability: the potential for dishonest manipulation by service providers. This manipulation can manifest in various forms, such as secretly substituting a proclaimed high-performance model with a low-cost alternative, or inflating responses with meaningless tokens to increase billing. This work tackles the issue through the lens of algorithmic game theory and mechanism design. We are the first to propose a formal economic model for a realistic user-provider ecosystem, where a user can iteratively delegate $T$ queries to multiple model providers, and providers can engage in a range of strategic behaviors. As our central contribution, we prove that for a continuous strategy space and any $\\epsilon\\in(0,\\frac12)$, there exists an approximate incentive-compatible mechanism with an additive approximation ratio of $O(T^{1-\\epsilon}\\log T)$, and a guaranteed quasi-linear second-best user utility. We also prove an impossibility result, stating that no mechanism can guarantee an expected user utility that is asymptotically better than our mechanism. Furthermore, we demonstrate the effectiveness of our mechanism in simulation experiments with real-world API settings.', 'abstract_zh': '通过应用程序接口广泛采用的大规模语言模型引发了关键漏洞：服务提供商可能进行不诚实操纵。这种操纵可以表现为私下用低成本替代品替换声称高性能的模型，或通过增加无意义的标记来虚增响应以增加收费。本研究通过算法博弈论和机制设计的视角来应对这一问题。我们首次提出了一个正式的经济模型，用于现实的用户-提供者生态系统，其中用户可以迭代地将多达 $T$ 个查询委托给多个模型提供者，而提供者可以采取一系列战略行为。作为我们的主要贡献，我们证明，在连续策略空间中，对于任何 $\\epsilon\\in(0,\\frac12)$，存在一个具有加性近似比为 $O(T^{1-\\epsilon}\\log T)$ 的近似激励相容机制，并保证了几乎线性次优的用户效用。我们还证明了一个不可能性结果，即没有机制能够确保预期用户效用比我们提出的机制更好。此外，我们在实际API设置的仿真实验中证明了我们机制的有效性。', 'title_zh': '按质付费：针对不诚实的大语言模型服务提供商的游戏理论方法'}
{'arxiv_id': 'arXiv:2511.00810', 'title': 'GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding', 'authors': 'Shijie Zhou, Viet Dac Lai, Hao Tan, Jihyung Kil, Wanrong Zhu, Changyou Chen, Ruiyi Zhang', 'link': 'https://arxiv.org/abs/2511.00810', 'abstract': 'Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: this https URL', 'abstract_zh': '图形用户界面（GUI）锚定是计算机使用代理的关键功能，它将自然语言指令映射到可操作的屏幕区域。基于多模态大型语言模型（MLLMs）的现有方法通常将其形式化为基于文本的坐标生成任务，但从视觉输入直接生成精确坐标仍然是一个挑战且计算密集。GUI锚定的一种直观实现方式是首先选择与指令相关的视觉片段，然后确定这些片段内的精确点击位置。基于一般MLLMs内部具有一定的锚定能力的观察，我们提出了一种基于注意力且无坐标的监督微调框架GUI-AIMA，用于高效的GUI锚定。GUI-AIMA将MLLMs的基本多模态注意力与片段级的锚定信号对齐。这些信号通过简化查询-视觉注意矩阵的多头聚合适应性计算。此外，其无坐标的特性可以轻松集成一个即插即用的放大阶段。GUI-AIMA-3B仅用85k张截图进行训练，展示了出色的數據效率，并验证了轻量训练可以激活MLLMs的天然锚定能力。它在3B模型中达到了最先进的性能，在ScreenSpot-Pro上的平均准确率达到58.6%，在OSWorld-G上达到了62.2%。项目页面：this https URL', 'title_zh': 'GUI-AIMA：基于上下文锚点的内在多模态注意力对齐方法用于GUI定位'}
{'arxiv_id': 'arXiv:2511.00794', 'title': 'Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration', 'authors': 'Yan Sun, Jia Guo, Stanley Kok, Zihao Wang, Zujie Wen, Zhiqiang Zhang', 'link': 'https://arxiv.org/abs/2511.00794', 'abstract': 'Reinforcement learning with verifiable rewards (RLVR) has improved the reasoning ability of large language models, yet training remains costly because many rollouts contribute little to optimization, considering the amount of computation required. This study investigates how simply leveraging intrinsic data properties, almost free benefit during training, can improve data efficiency for RLVR. We propose PREPO with two complementary components. First, we adopt prompt perplexity as an indicator of model adaptability in learning, enabling the model to progress from well-understood contexts to more challenging ones. Second, we amplify the discrepancy among the rollouts by differentiating their relative entropy, and prioritize sequences that exhibit a higher degree of exploration. Together, these mechanisms reduce rollout demand while preserving competitive performance. On the Qwen and Llama models, PREPO achieves effective results on mathematical reasoning benchmarks with up to 3 times fewer rollouts than the baselines. Beyond empirical gains, we provide theoretical and in-depth analyses explaining the underlying rationale of our method to improve the data efficiency of RLVR.', 'abstract_zh': '验证奖励的强化学习（RLVR）提高了大型语言模型的推理能力，但由于许多滚出对优化贡献甚微，训练仍较为昂贵，考虑到所需计算量。本研究探讨了如何通过利用内在数据特性，在训练过程中几乎免费地提高数据效率，以改进RLVR的数据效率。我们提出了PREPO，包含两个互补的组件。首先，我们采用提示困惑度作为模型学习适应性的指标，使模型能够从易理解的上下文逐渐过渡到更具挑战性的上下文。其次，我们通过区分滚出的相对熵来放大滚出之间的差异性，并优先处理探索性更强的序列。这些机制在减少滚出需求的同时保持了竞争力的表现。在Qwen和Llama模型上，PREPO在数学推理基准测试中使用不到基线模型三分之一的滚出次数实现了有效结果。除此之外，我们还提供了理论分析和深入研究，解释了我们的方法如何提高RLVR的数据效率。', 'title_zh': '大规模语言模型内的探索高效强化学习'}
{'arxiv_id': 'arXiv:2511.00709', 'title': 'A Voice-Enabled Virtual Patient System for Interactive Training in Standardized Clinical Assessment', 'authors': 'Veronica Bossio Botero, Vijay Yadav, Jacob Ouyang, Anzar Abbas, Michelle Worthington', 'link': 'https://arxiv.org/abs/2511.00709', 'abstract': 'Training mental health clinicians to conduct standardized clinical assessments is challenging due to a lack of scalable, realistic practice opportunities, which can impact data quality in clinical trials. To address this gap, we introduce a voice-enabled virtual patient simulation system powered by a large language model (LLM). This study describes the system\'s development and validates its ability to generate virtual patients who accurately adhere to pre-defined clinical profiles, maintain coherent narratives, and produce realistic dialogue. We implemented a system using a LLM to simulate patients with specified symptom profiles, demographics, and communication styles. The system was evaluated by 5 experienced clinical raters who conducted 20 simulated structured MADRS interviews across 4 virtual patient personas. The virtual patients demonstrated strong adherence to their clinical profiles, with a mean item difference between rater-assigned MADRS scores and configured scores of 0.52 (SD=0.75). Inter-rater reliability across items was 0.90 (95% CI=0.68-0.99). Expert raters consistently rated the qualitative realism and cohesiveness of the virtual patients favorably, giving average ratings between "Agree" and "Strongly Agree." Our findings suggest that LLM-powered virtual patient simulations are a viable and scalable tool for training clinicians, capable of producing high-fidelity, clinically relevant practice scenarios.', 'abstract_zh': '基于大型语言模型的语音驱动虚拟患者模拟系统在训练精神健康临床医生进行标准化临床评估中的应用', 'title_zh': '基于语音的虚拟患者系统：标准化临床评估互动训练'}
{'arxiv_id': 'arXiv:2511.00686', 'title': 'Evolve to Inspire: Novelty Search for Diverse Image Generation', 'authors': 'Alex Inch, Passawis Chaiyapattanaporn, Yuchen Zhu, Yuan Lu, Ting-Wen Ko, Davide Paglieri', 'link': 'https://arxiv.org/abs/2511.00686', 'abstract': 'Text-to-image diffusion models, while proficient at generating high-fidelity im- ages, often suffer from limited output diversity, hindering their application in exploratory and ideation tasks. Existing prompt optimization techniques typically target aesthetic fitness or are ill-suited to the creative visual domain. To address this shortcoming, we introduce WANDER, a novelty search-based approach to generating diverse sets of images from a single input prompt. WANDER operates directly on natural language prompts, employing a Large Language Model (LLM) for semantic evolution of diverse sets of images, and using CLIP embeddings to quantify novelty. We additionally apply emitters to guide the search into distinct regions of the prompt space, and demonstrate that they boost the diversity of the generated images. Empirical evaluations using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that WANDER significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics. Ablation studies confirm the efficacy of emitters.', 'abstract_zh': '基于新颖性搜索的单输入提示生成多样化图像的方法', 'title_zh': '进化以启发：新颖性搜索在多样图像生成中的应用'}
{'arxiv_id': 'arXiv:2511.00664', 'title': 'ShadowLogic: Backdoors in Any Whitebox LLM', 'authors': 'Kasimir Schulz, Amelia Kawasaki, Leo Ring', 'link': 'https://arxiv.org/abs/2511.00664', 'abstract': 'Large language models (LLMs) are widely deployed across various applications, often with safeguards to prevent the generation of harmful or restricted content. However, these safeguards can be covertly bypassed through adversarial modifications to the computational graph of a model. This work highlights a critical security vulnerability in computational graph-based LLM formats, demonstrating that widely used deployment pipelines may be susceptible to obscured backdoors. We introduce ShadowLogic, a method for creating a backdoor in a white-box LLM by injecting an uncensoring vector into its computational graph representation. We set a trigger phrase that, when added to the beginning of a prompt into the LLM, applies the uncensoring vector and removes the content generation safeguards in the model. We embed trigger logic directly into the computational graph which detects the trigger phrase in a prompt. To evade detection of our backdoor, we obfuscate this logic within the graph structure, making it similar to standard model functions. Our method requires minimal alterations to model parameters, making backdoored models appear benign while retaining the ability to generate uncensored responses when activated. We successfully implement ShadowLogic in Phi-3 and Llama 3.2, using ONNX for manipulating computational graphs. Implanting the uncensoring vector achieved a >60% attack success rate for further malicious queries.', 'abstract_zh': '基于计算图的大语言模型中的隐蔽后门安全漏洞及ShadowLogic方法', 'title_zh': 'ShadowLogic: Anywhitebox LLM中的后门攻击'}
{'arxiv_id': 'arXiv:2511.00628', 'title': 'AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems', 'authors': 'Yang Li, Siqi Ping, Xiyu Chen, Xiaojian Qi, Zigan Wang, Ye Luo, Xiaowei Zhang', 'link': 'https://arxiv.org/abs/2511.00628', 'abstract': 'With the rapid progress of large language models (LLMs), LLM-powered multi-agent systems (MAS) are drawing increasing interest across academia and industry. However, many current MAS frameworks struggle with reliability and scalability, especially on complex tasks. We present AgentGit, a framework that brings Git-like rollback and branching to MAS workflows. Built as an infrastructure layer on top of LangGraph, AgentGit supports state commit, revert, and branching, allowing agents to traverse, compare, and explore multiple trajectories efficiently. To evaluate AgentGit, we designed an experiment that optimizes target agents by selecting better prompts. We ran a multi-step A/B test against three baselines -- LangGraph, AutoGen, and Agno -- on a real-world task: retrieving and analyzing paper abstracts. Results show that AgentGit significantly reduces redundant computation, lowers runtime and token usage, and supports parallel exploration across multiple branches, enhancing both reliability and scalability in MAS development. This work offers a practical path to more robust MAS design and enables error recovery, safe exploration, iterative debugging, and A/B testing in collaborative AI systems.', 'abstract_zh': '基于Git的回滚与分支的大语言模型驱动多智能体系统框架：AgentGit的研究', 'title_zh': 'AgentGit: 一种可靠的可扩展的基于LLM的多智能体系统版本控制框架'}
{'arxiv_id': 'arXiv:2511.00617', 'title': 'Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering', 'authors': 'Eric Bigelow, Daniel Wurgaft, YingQiao Wang, Noah Goodman, Tomer Ullman, Hidenori Tanaka, Ekdeep Singh Lubana', 'link': 'https://arxiv.org/abs/2511.00617', 'abstract': 'Large language models (LLMs) can be controlled at inference time through prompts (in-context learning) and internal activations (activation steering). Different accounts have been proposed to explain these methods, yet their common goal of controlling model behavior raises the question of whether these seemingly disparate methodologies can be seen as specific instances of a broader framework. Motivated by this, we develop a unifying, predictive account of LLM control from a Bayesian perspective. Specifically, we posit that both context- and activation-based interventions impact model behavior by altering its belief in latent concepts: steering operates by changing concept priors, while in-context learning leads to an accumulation of evidence. This results in a closed-form Bayesian model that is highly predictive of LLM behavior across context- and activation-based interventions in a set of domains inspired by prior work on many-shot in-context learning. This model helps us explain prior empirical phenomena - e.g., sigmoidal learning curves as in-context evidence accumulates - while predicting novel ones - e.g., additivity of both interventions in log-belief space, which results in distinct phases such that sudden and dramatic behavioral shifts can be induced by slightly changing intervention controls. Taken together, this work offers a unified account of prompt-based and activation-based control of LLM behavior, and a methodology for empirically predicting the effects of these interventions.', 'abstract_zh': '大型语言模型（LLMs）可以通过提示（上下文学习）和内部激活（激活导向）在推断时进行控制。从贝叶斯视角出发，我们发展了一个统一的、预测性的LLM控制账户。具体而言，我们认为上下文和激活干预通过改变模型对潜在概念的信任度来影响其行为：激活导向通过改变先验来操作，而上下文学习则通过累积证据来实现。这导致了一个闭式形式的贝叶斯模型，能够在一组受先前许多-shot上下文学习研究启发的领域中高度预测上下文和激活干预下的LLM行为。该模型有助于解释先前的经验现象，并预测新的现象，如在对数信任空间中两种干预的加itivity，从而导致不同的阶段，在这个阶段中，通过稍微改变干预控制可以引发突然和戏剧性的行为变化。总之，这项工作提供了一个统一的基于提示和基于激活的LLM行为控制的解释，并提供了一种基于实验预测这些干预效果的方法。', 'title_zh': '信念动力学揭示上下文学习和激活导向的双重性质'}
{'arxiv_id': 'arXiv:2511.00588', 'title': 'Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation', 'authors': 'Dong Chen, Yanzhe Wei, Zonglin He, Guan-Ming Kuang, Canhua Ye, Meiru An, Huili Peng, Yong Hu, Huiren Tao, Kenneth MC Cheung', 'link': 'https://arxiv.org/abs/2511.00588', 'abstract': "Large language models (LLMs) offer transformative potential for clinical decision support in spine surgery but pose significant risks through hallucinations, which are factually inconsistent or contextually misaligned outputs that may compromise patient safety. This study introduces a clinician-centered framework to quantify hallucination risks by evaluating diagnostic precision, recommendation quality, reasoning robustness, output coherence, and knowledge alignment. We assessed six leading LLMs across 30 expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall performance (total score: 86.03 $\\pm$ 2.08), particularly in high-stakes domains such as trauma and infection. A critical finding reveals that reasoning-enhanced model variants did not uniformly outperform standard counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed relative to its standard version (80.79 $\\pm$ 1.83 vs. 81.56 $\\pm$ 1.92), indicating extended chain-of-thought reasoning alone is insufficient for clinical reliability. Multidimensional stress-testing exposed model-specific vulnerabilities, with recommendation quality degrading by 7.4% under amplified complexity. This decline contrasted with marginal improvements in rationality (+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning divergence between perceived coherence and actionable guidance. Our findings advocate integrating interpretability mechanisms (e.g., reasoning chain visualization) into clinical workflows and establish a safety-aware validation framework for surgical LLM deployment.", 'abstract_zh': '大型语言模型（LLMs）在脊柱手术临床决策支持方面具有变革性的潜力，但存在通过幻觉带来的重大风险，幻觉是指事实不一致或上下文不匹配的输出，可能损害患者安全。本研究引入了一种以临床医生为中心的框架，通过评估诊断精确度、建议质量、推理稳健性、输出连贯性和知识一致性来量化幻觉风险。我们在30个专家验证的脊柱病例上评估了六种领先的LLMs。DeepSeek-R1表现出色（总分：86.03 ± 2.08），特别是在创伤和感染等高风险领域。一项关键发现表明，增强推理的模型变体并不总是优于标准版本：Claude-3.7-Sonnet的扩展思考模式表现不如其标准版本（80.79 ± 1.83 vs. 81.56 ± 1.92），表明单独扩展的思考链条推理不足以确保临床可靠性。多维度的压力测试暴露了模型特定的脆弱性，在复杂性放大情况下，建议质量下降7.4%，而理性提升2.0%、可读性提升1.7%和诊断提升4.7%，这突显了感知连贯性与可操作指导之间令人担忧的分歧。我们的研究建议将解释机制（如推理链条可视化）整合到临床工作流程中，并建立一种以安全性为导向的验证框架，用于手术LLM部署。', 'title_zh': '基于序列验证的顺序框架下AI手术决策辅助中的幻觉风险诊断'}
{'arxiv_id': 'arXiv:2511.00576', 'title': 'FlashEVA: Accelerating LLM inference via Efficient Attention', 'authors': 'Juan Gabriel Kostelec, Qinghai Guo', 'link': 'https://arxiv.org/abs/2511.00576', 'abstract': 'Transformer models have revolutionized natural language processing, achieving state-of-the-art performance and demonstrating remarkable scalability. However, their memory demands, particularly due to maintaining full context in memory, pose significant challenges for inference. In this paper, we present FlashEVA, an efficient implementation of EVA (Efficient Attention via Control Variates), and demonstrate how to finetune transformers to adapt to FlashEVA attention. Our method enables fine-tuning of Transformer models with as few as 1.5B tokens while preserving effectiveness across various downstream tasks. Notably, FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory usage during inference compared to standard Transformer implementations. Despite these improvements, we observe limitations in retrieval-focused tasks. Our implementation offers control over the trade-off between throughput and accuracy through adjustable hyperparameters, providing flexibility for diverse use cases. This work represents a significant step towards more efficient and adaptable Transformer-based models for inference.', 'abstract_zh': 'Transformer模型 telah革新自然语言处理，实现了一流的性能并展示了显著的可扩展性。然而，它们对内存的需求，特别是由于需要在内存中保持完整上下文，对推理提出了重大挑战。本文中，我们介绍了FlashEVA，这是一种高效的EVA（通过控制变差的高效注意力）实现方法，并展示了如何调整Transformer以适应FlashEVA注意力。我们的方法使Transformer模型的微调能够在最少1.5B个令牌的情况下保持对各种下游任务的有效性。值得注意的是，FlashEVA在推理过程中的吞吐量可提高至6.7倍，峰值GPU内存使用量降低至原来的1/5。尽管存在这些改进，我们发现在检索为主的任务中存在局限性。我们的实现通过可调的超参数提供了吞吐量和精度之间的权衡控制，为各种应用场景提供了灵活性。这项工作代表了向更高效和适应性强的Transformer基推理模型迈进的重要一步。', 'title_zh': 'FlashEVA: 通过高效注意力加速LLM推理'}
{'arxiv_id': 'arXiv:2511.00554', 'title': 'Red-teaming Activation Probes using Prompted LLMs', 'authors': 'Phil Blandfort, Robert Graham', 'link': 'https://arxiv.org/abs/2511.00554', 'abstract': 'Activation probes are attractive monitors for AI systems due to low cost and latency, but their real-world robustness remains underexplored. We ask: What failure modes arise under realistic, black-box adversarial pressure, and how can we surface them with minimal effort? We present a lightweight black-box red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback and in-context learning (ICL), and requires no fine-tuning, gradients, or architectural access. Running a case study with probes for high-stakes interactions, we show that our approach can help discover valuable insights about a SOTA probe. Our analysis uncovers interpretable brittleness patterns (e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but persistent vulnerabilities under scenario-constraint attacks. These results suggest that simple prompted red-teaming scaffolding can anticipate failure patterns before deployment and might yield promising, actionable insights to harden future probes.', 'abstract_zh': '基于激活的探针是AI系统的有吸引力的监控工具，由于成本低和延迟小，但其实际环境下的稳健性尚未得到充分探索。我们提出的问题是：在现实的、黑盒的对抗压力下会出现哪些失败模式，我们如何以最少的努力揭示这些模式？我们提出了一种轻量级的黑盒红队程序，将现成的大规模语言模型（LLM）与迭代反馈和上下文学习（ICL）相结合，无需微调、梯度或架构访问。通过对高风险交互中的探针进行案例研究，我们展示了该方法如何有助于发现最新最佳探针的重要见解。我们的分析揭示了可解释的脆弱性模式（例如，法学术语引起的误报；平淡程序化语气的漏报），并在情景约束攻击下降低了但仍持续存在的脆弱性。这些结果表明，简单的提示驱动红队框架可以在部署前预测失败模式，并可能为强化未来探针提供有价值的可操作见解。', 'title_zh': '使用提示驱动的大语言模型激活红队探测器'}
{'arxiv_id': 'arXiv:2511.00527', 'title': 'HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models', 'authors': 'Robab Aghazadeh-Chakherlou, Qing Guo, Siddartha Khastgir, Peter Popov, Xiaoge Zhang, Xingyu Zhao', 'link': 'https://arxiv.org/abs/2511.00527', 'abstract': 'Large Language Models (LLMs) are increasingly deployed across diverse domains, raising the need for rigorous reliability assessment methods. Existing benchmark-based evaluations primarily offer descriptive statistics of model accuracy over datasets, providing limited insight into the probabilistic behavior of LLMs under real operational conditions. This paper introduces HIP-LLM, a Hierarchical Imprecise Probability framework for modeling and inferring LLM reliability. Building upon the foundations of software reliability engineering, HIP-LLM defines LLM reliability as the probability of failure-free operation over a specified number of future tasks under a given Operational Profile (OP). HIP-LLM represents dependencies across (sub-)domains hierarchically, enabling multi-level inference from subdomain to system-level reliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty and incorporates OPs to reflect usage contexts. It derives posterior reliability envelopes that quantify uncertainty across priors and data. Experiments on multiple benchmark datasets demonstrate that HIP-LLM offers a more accurate and standardized reliability characterization than existing benchmark and state-of-the-art approaches. A publicly accessible repository of HIP-LLM is provided.', 'abstract_zh': '大型语言模型（LLMs）在多个领域中的应用日益增多，迫切需要严格的可靠性评估方法。现有基于基准的评估主要提供模型在数据集上的准确性的描述性统计信息，提供了有限关于在实际操作条件下LLMs概率行为的见解。本文引入了HIP-LLM，这是一种分层不确定概率框架，用于建模和推断LLM的可靠性。基于软件可靠性工程的基础，HIP-LLM 将LLM的可靠性定义为在给定操作配置文件（OP）下，在指定数量的未来任务中无故障运行的概率。HIP-LLM 按层次结构表示跨（子）域的依赖性，从而实现从子域到系统级可靠性的多层次推断。HIP-LLM 集成了不精确的先验概率以捕获认识上的不确定性，并结合OP来反映使用上下文。它推导出后验可靠性边界，量化了先验和数据中的不确定性。在多个基准数据集上的实验表明，HIP-LLM 提供了比现有基准和最新方法更准确和标准化的可靠性表征。提供了HIP-LLM的公开访问仓库。', 'title_zh': 'HIP-LLM：一种用于大型语言模型可靠性评估的分层不精确概率方法'}
{'arxiv_id': 'arXiv:2511.00460', 'title': 'Proactive DDoS Detection and Mitigation in Decentralized Software-Defined Networking via Port-Level Monitoring and Zero-Training Large Language Models', 'authors': 'Mohammed N. Swileh, Shengli Zhang', 'link': 'https://arxiv.org/abs/2511.00460', 'abstract': "Centralized Software-Defined Networking (cSDN) offers flexible and programmable control of networks but suffers from scalability and reliability issues due to its reliance on centralized controllers. Decentralized SDN (dSDN) alleviates these concerns by distributing control across multiple local controllers, yet this architecture remains highly vulnerable to Distributed Denial-of-Service (DDoS) attacks. In this paper, we propose a novel detection and mitigation framework tailored for dSDN environments. The framework leverages lightweight port-level statistics combined with prompt engineering and in-context learning, enabling the DeepSeek-v3 Large Language Model (LLM) to classify traffic as benign or malicious without requiring fine-tuning or retraining. Once an anomaly is detected, mitigation is enforced directly at the attacker's port, ensuring that malicious traffic is blocked at their origin while normal traffic remains unaffected. An automatic recovery mechanism restores normal operation after the attack inactivity, ensuring both security and availability. Experimental evaluation under diverse DDoS attack scenarios demonstrates that the proposed approach achieves near-perfect detection, with 99.99% accuracy, 99.97% precision, 100% recall, 99.98% F1-score, and an AUC of 1.0. These results highlight the effectiveness of combining distributed monitoring with zero-training LLM inference, providing a proactive and scalable defense mechanism for securing dSDN infrastructures against DDoS threats.", 'abstract_zh': '面向分布式SDN环境的攻击检测与缓解框架', 'title_zh': '基于端口级监控和零训练大型语言模型的去中心化软件定义网络中的主动DDoS检测与缓解'}
{'arxiv_id': 'arXiv:2511.00447', 'title': 'DRIP: Defending Prompt Injection via De-instruction Training and Residual Fusion Model Architecture', 'authors': 'Ruofan Liu, Yun Lin, Jin Song Dong', 'link': 'https://arxiv.org/abs/2511.00447', 'abstract': "Large language models (LLMs) have demonstrated impressive instruction-following capabilities. However, these capabilities also expose models to prompt injection attacks, where maliciously crafted inputs overwrite or distract from the intended instructions. A core vulnerability lies in the model's lack of semantic role understanding: it cannot distinguish directive intent from descriptive content, leading it to execute instruction-like phrases embedded in data.\nWe propose DRIP, a training-time defense grounded in a semantic modeling perspective, which enforces robust separation between instruction and data semantics without sacrificing utility. DRIP introduces two lightweight yet complementary mechanisms: (1) a token-wise de-instruction shift that performs semantic disentanglement, weakening directive semantics in data tokens while preserving content meaning; and (2) a residual fusion pathway that provides a persistent semantic anchor, reinforcing the influence of the true top-level instruction during generation. Experimental results on LLaMA-8B and Mistral-7B across three prompt injection benchmarks (SEP, AlpacaFarm, and InjecAgent) demonstrate that DRIP outperforms state-of-the-art defenses, including StruQ, SecAlign, ISE, and PFT, improving role separation by 49%, and reducing attack success rate by 66% for adaptive attacks. Meanwhile, DRIP's utility is on par with the undefended model across AlpacaEval, IFEval, and MT-Bench. Our findings underscore the power of lightweight representation edits and role-aware supervision in securing LLMs against adaptive prompt injection.", 'abstract_zh': '大型语言模型（LLMs）展现了令人印象深刻的指令遵从能力。然而，这些能力也使其面临提示注入攻击的威胁，恶意构造的输入可能会覆盖或转移指令的意图。核心脆弱性在于模型对语义角色的理解不足：它无法区分指令意图与描述性内容，导致其执行嵌入在数据中的指令式短语。\n\n我们提出了一种基于语义建模视角的DRIP（指令剥离与融合防御），它在保持实用性的同时，建立了指令和数据语义的坚固分离。DRIP引入了两个轻量级而互补的机制：（1）一个基于令牌的去指令化转换，执行语义分离，削弱数据令牌中的指令语义同时保留内容意义；（2）一个残差融合路径，提供持续的语义锚点，在生成期间增强真实顶层指令的影响。DRIP在LLaMA-8B和Mistral-7B上分别在三个提示注入基准测试（SEP、AlpacaFarm和InjecAgent）中，表现出优于现有最佳防御方法（StruQ、SecAlign、ISE、PFT）的效果。具体而言，DRIP提高角色分离49%，降低可适应攻击成功率为66%。同时，DRIP在AlpacaEval、IFEval和MT-Bench上的实用性与未防御模型相当。我们的研究结果强调了轻量级表示编辑和角色感知监督在保护LLM免受可适应提示注入攻击方面的强大力量。', 'title_zh': 'DRIP: 防御提示注入的去指令训练与残差融合模型架构'}
{'arxiv_id': 'arXiv:2511.00421', 'title': 'MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts', 'authors': 'Naoto Iwase, Hiroki Okuyama, Junichiro Iwasawa', 'link': 'https://arxiv.org/abs/2511.00421', 'abstract': 'Large language models (LLMs) show increasing promise in medical applications, but their ability to detect and correct errors in clinical texts -- a prerequisite for safe deployment -- remains under-evaluated, particularly beyond English. We introduce MedRECT, a cross-lingual benchmark (Japanese/English) that formulates medical error handling as three subtasks: error detection, error localization (sentence extraction), and error correction. MedRECT is built with a scalable, automated pipeline from the Japanese Medical Licensing Examinations (JMLE) and a curated English counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning proprietary, open-weight, and reasoning families. Key findings: (i) reasoning models substantially outperform standard architectures, with up to 13.5% relative improvement in error detection and 51.0% in sentence extraction; (ii) cross-lingual evaluation reveals 5-10% performance gaps from English to Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA fine-tuning yields asymmetric improvements in error correction performance (Japanese: +0.078, English: +0.168) while preserving reasoning capabilities; and (iv) our fine-tuned model exceeds human expert performance on structured medical error correction tasks. To our knowledge, MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework and resources for developing safer medical LLMs across languages.', 'abstract_zh': '大型语言模型在医疗应用中显示出越来越大的潜力，但它们在临床文本中检测和纠正错误的能力——这是安全部署的前提——仍然被低估，特别是在英语之外的语言中。我们介绍了MedRECT，这是一个跨语言基准（日语/英语），将医疗错误处理分为三个子任务：错误检测、错误定位（句子抽取）和错误纠正。MedRECT基于日本医学执照考试（JMLE）的可扩展自动化流水线及其精心整理的英语对应版本，生成了MedRECT-ja（663篇文本）和MedRECT-en（458篇文本），两者在错误/非错误的比例上具有可比性。我们评估了9种当代大型语言模型，涵盖专有、开放权重和推理家族。主要发现包括：（i）推理模型显著优于标准架构，在错误检测中相对提高13.5%，在句子抽取中提高51.0%；（ii）跨语言评估显示从英语到日语有5-10%的性能差距，推理模型的差异较小；（iii）针对LoRA的微调在错误纠正性能上产生了不对称的改进（日语：+0.078，英语：+0.168），同时保留了推理能力；（iv）我们的微调模型在结构化的医疗错误纠正任务上超过了人类专家的表现。据我们所知，MedRECT是第一个全面的跨语言基准，为开发跨语言的更安全的医疗大型语言模型提供了可再现的框架和资源。', 'title_zh': 'MedRECT：临床文本错误纠正推理基准'}
{'arxiv_id': 'arXiv:2511.00405', 'title': 'UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings', 'authors': 'Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, Jinsong Su', 'link': 'https://arxiv.org/abs/2511.00405', 'abstract': 'The remarkable success of multimodal large language models (MLLMs) has driven advances in multimodal embeddings, yet existing models remain inherently discriminative, limiting their ability to benefit from reasoning-driven generation paradigm. In this work, we pioneer the exploration of generative embeddings, unifying embedding tasks within a generative paradigm. We propose UME-R1, a universal multimodal embedding framework consisting of a two-stage training strategy: a cold-start supervised fine-tuning equips the model with reasoning capabilities and enables it to generate both discriminative and generative embeddings; a subsequent reinforcement learning enhances reasoning and further optimizes generative embedding quality. This pioneering work reveals four key insights: 1) generative embeddings unlock substantial performance gains over conventional discriminative embeddings by leveraging the powerful generative reasoning capabilities of MLLMs; 2) discriminative and generative embeddings are complementary, whose combined oracle performance far exceeding that of either alone; 3) RL can effectively enhance generative embeddings, establishing a scalable optimization paradigm.; 4) repeated sampling at inference boosts downstream task coverage (pass@k), highlighting the inference-time scalability potential of generative embeddings. Evaluated on the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual documents, UME-R1 significantly outperforms conventional discriminative embedding models and offers a foundation for more interpretable, reasoning-driven generative multimodal embeddings. Our code, models, and datasets will be publicly available at this https URL.', 'abstract_zh': '多模态大型语言模型的显著成功推动了多模态嵌入的发展，但现有模型仍固守区分性原则，限制了它们从推理驱动生成范式中受益的能力。在此工作中，我们首次探索生成嵌入，将嵌入任务统一于生成范式之下。我们提出了UME-R1，一个通用多模态嵌入框架，包含两阶段训练策略：冷启动监督微调赋予模型推理能力，使其能够生成区分性和生成性嵌入；随后的强化学习增强推理，进一步优化生成嵌入的质量。这项开创性工作揭示了四个关键洞见：1）生成嵌入通过利用多模态大型语言模型强大的生成推理能力，相较于传统区分性嵌入实现了显著性能提升；2）区分性和生成性嵌入互补，两者结合的Oracle性能远超单一模式；3）强化学习有效提升生成嵌入，确立了可扩展的优化范式；4）推理时的重复采样提升了下游任务覆盖率（pass@k），突显了生成嵌入在推理时的可扩展潜力。在MMEB-V2基准上的78个任务（涵盖视频、图像和视觉文档）中，UME-R1显著优于传统区分性嵌入模型，并为更具有解释性的、推理驱动的生成多模态嵌入奠定了基础。我们的代码、模型和数据集将在该网址公开。', 'title_zh': 'UME-R1: 探索基于推理的生成性多模态嵌入'}
{'arxiv_id': 'arXiv:2511.00346', 'title': 'Exploiting Latent Space Discontinuities for Building Universal LLM Jailbreaks and Data Extraction Attacks', 'authors': 'Kayua Oleques Paim, Rodrigo Brandao Mansilha, Diego Kreutz, Muriel Figueredo Franco, Weverton Cordeiro', 'link': 'https://arxiv.org/abs/2511.00346', 'abstract': 'The rapid proliferation of Large Language Models (LLMs) has raised significant concerns about their security against adversarial attacks. In this work, we propose a novel approach to crafting universal jailbreaks and data extraction attacks by exploiting latent space discontinuities, an architectural vulnerability related to the sparsity of training data. Unlike previous methods, our technique generalizes across various models and interfaces, proving highly effective in seven state-of-the-art LLMs and one image generation model. Initial results indicate that when these discontinuities are exploited, they can consistently and profoundly compromise model behavior, even in the presence of layered defenses. The findings suggest that this strategy has substantial potential as a systemic attack vector.', 'abstract_zh': '大型语言模型的安全性对抗：利用潜在空间不连续性构造通用 jailbreak 和数据提取攻击', 'title_zh': '利用潜在空间断点构建通用大语言模型劫持和数据提取攻击'}
{'arxiv_id': 'arXiv:2511.00321', 'title': 'Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits', 'authors': 'Dowon Kim, MinJae Lee, Janghyeon Kim, HyuckSung Kwon, Hyeonggyu Jeong, Sang-Soo Park, Minyong Yoon, Si-Dong Roh, Yongsuk Kwon, Jinin So, Jungwook Choi', 'link': 'https://arxiv.org/abs/2511.00321', 'abstract': 'The expansion of context windows in large language models (LLMs) to multi-million tokens introduces severe memory and compute bottlenecks, particularly in managing the growing Key-Value (KV) cache. While Compute Express Link (CXL) enables non-eviction frameworks that offload the full KV-cache to scalable external memory, these frameworks still suffer from costly data transfers when recalling non-resident KV tokens to limited GPU memory as context lengths increase. This work proposes scalable Processing-Near-Memory (PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that coordinates memory and computation beyond GPU limits. Our design offloads token page selection to a PNM accelerator within CXL memory, eliminating costly recalls and enabling larger GPU batch sizes. We further introduce a hybrid parallelization strategy and a steady-token selection mechanism to enhance compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM system, our solution delivers consistent performance gains for LLMs with up to 405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV) and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x throughput improvement, up to 60x lower energy per token, and up to 7.3x better total cost efficiency than the baseline, demonstrating that CXL-enabled multi-PNM architectures can serve as a scalable backbone for future long-context LLM inference.', 'abstract_zh': '大型语言模型中上下文窗口扩展到百万级别토큰引入了严重的内存和计算瓶颈，尤其是在管理不断增长的键值（KV）缓存方面。虽然Compute Express Link (CXL) 允许将完整的KV缓存卸载到可扩展的外部内存以实现非驱逐式框架，但当上下文长度增加时，这些框架在召回非驻留的KV令牌到有限的GPU内存时仍然会遭受昂贵的数据传输成本。本工作提出了适用于百万级别tokens的大型语言模型推理的可扩展处理接近内存（PNM）系统，该系统是CXL启用的KV缓存管理系统，能够协调超过GPU限制的内存和计算。我们的设计将token页选择卸载到CXL内存中的PNM加速器中，消除昂贵的召回操作，并允许更大的GPU批次大小。我们还引入了一种混合并行策略和一种稳态token选择机制，以提高计算效率和可扩展性。在最先进的CXL-PNM系统之上实现，我们的解决方案为具有多达4050亿参数和百万级别token上下文的大型语言模型提供一致的性能增益。仅PNM卸载方案（PNM-KV）和PNM与GPU的混合方案（PnG-KV，使用稳态token执行）分别实现了高达21.9倍的吞吐量提升、高达60倍的每token能耗降低和高达7.3倍的总体成本效率提高，证明了基于CXL的多PNM架构可以作为未来长上下文大型语言模型推理的可扩展基础。', 'title_zh': '面向内存的可扩展处理：超越GPU限制的1M-Token语言模型推理中的CXL使能KV缓存管理'}
{'arxiv_id': 'arXiv:2511.00318', 'title': 'A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data', 'authors': 'Dana Kim, Yichen Xu, Tiffany Lin', 'link': 'https://arxiv.org/abs/2511.00318', 'abstract': 'Large Language Models (LLMs) offer a flexible means to generate synthetic tabular data, yet existing approaches often fail to preserve key causal parameters such as the average treatment effect (ATE). In this technical exploration, we first demonstrate that state-of-the-art synthetic data generators, both GAN- and LLM-based, can achieve high predictive fidelity while substantially misestimating causal effects. To address this gap, we propose a hybrid generation framework that combines model-based covariate synthesis (monitored via distance-to-closest-record filtering) with separately learned propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain their underlying causal structure. We further introduce a synthetic pairing strategy to mitigate positivity violations and a realistic evaluation protocol that leverages unlimited synthetic samples to benchmark traditional estimators (IPTW, AIPW, substitution) under complex covariate distributions. This work lays the groundwork for LLM-powered data pipelines that support robust causal analysis. Our code is available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）提供了一种生成合成表格数据的灵活方法，但现有方法往往未能保留如平均治疗效应（ATE）等关键因果参数。在本技术探索中，我们首先证明了最先进的合成数据生成器，无论是基于GAN的还是基于LLM的，在实现高预测保真度的同时，可能会大幅误估因果效应。为了解决这一差距，我们提出了一种混合生成框架，该框架结合了基于模型的协变量合成（通过最近邻记录筛选进行监控）和分别学习的倾向性和结果模型，从而确保三元组(W, A, Y)保留其潜在的因果结构。此外，我们引入了一种合成配对策略来缓解正性违反问题，并提出了一种现实的评估协议，利用无限数量的合成样本来评估传统估计器（ IPTW、AIPW、替换）在复杂协变量分布下的表现。本项工作为以LLM为基础的数据管道提供了支持稳健因果分析的基础。我们的代码可在此处获得：this https URL。', 'title_zh': '混合大语言模型合成数据的因果推断技术探究'}
{'arxiv_id': 'arXiv:2511.00315', 'title': 'Language Modeling With Factorization Memory', 'authors': 'Lee Xiong, Maksim Tkachenko, Johanes Effendi, Ting Cai', 'link': 'https://arxiv.org/abs/2511.00315', 'abstract': 'We propose Factorization Memory, an efficient recurrent neural network (RNN) architecture that achieves performance comparable to Transformer models on short-context language modeling tasks while also demonstrating superior generalization in long-context scenarios. Our model builds upon Mamba-2, enabling Factorization Memory to exploit parallel computations during training while preserving constant computational and memory complexity during inference. To further optimize model efficiency and representational capacity, we develop a sparse formulation of Factorization Memory that updates only a subset of recurrent states at each step while preserving the strong performance of its dense counterpart. To our knowledge, this represents the first RNN architecture that successfully combines sparse memory activation with competitive performance across both short and long-context settings. This work provides a systematic empirical analysis of Factorization Memory in comparison to Transformer and Mamba-2 architectures.', 'abstract_zh': '我们提出了一种因子记忆网络，这是一种高效的递归神经网络（RNN）架构，在短语境语言建模任务上实现了与Transformer模型相当的性能，同时在长语境场景中展示了更优的泛化能力。我们的模型基于Mamba-2构建，使因子记忆网络在训练过程中能够利用并行计算，而在推理过程中保持恒定的计算和内存复杂度。为了进一步优化模型效率和表示能力，我们开发了一种因子记忆网络的稀疏表示形式，该形式在每一步仅更新一部分递归状态，同时保持其稠密版本的强大性能。据我们所知，这是首个能够在短语境和长语境设置中都取得竞争力性能的稀疏记忆激活RNN架构。本文提供了因子记忆网络与Transformer和Mamba-2架构在系统性 empiric 分析中的对比研究。', 'title_zh': '语言建模_WITH_因子记忆'}
{'arxiv_id': 'arXiv:2511.00280', 'title': 'Calibration Across Layers: Understanding Calibration Evolution in LLMs', 'authors': 'Abhinav Joshi, Areeb Ahmad, Ashutosh Modi', 'link': 'https://arxiv.org/abs/2511.00280', 'abstract': 'Large Language Models (LLMs) have demonstrated inherent calibration capabilities, where predicted probabilities align well with correctness, despite prior findings that deep neural networks are often overconfident. Recent studies have linked this behavior to specific components in the final layer, such as entropy neurons and the unembedding matrix null space. In this work, we provide a complementary perspective by investigating how calibration evolves throughout the network depth. Analyzing multiple open-weight models on the MMLU benchmark, we uncover a distinct confidence correction phase in the upper/later layers, where model confidence is actively recalibrated after decision certainty has been reached. Furthermore, we identify a low-dimensional calibration direction in the residual stream whose perturbation significantly improves calibration metrics (ECE and MCE) without harming accuracy. Our findings suggest that calibration is a distributed phenomenon, shaped throughout the network forward pass, not just in its final projection, providing new insights into how confidence-regulating mechanisms operate within LLMs.', 'abstract_zh': '大型语言模型（LLMs）展示出了固有的校准能力，预测概率与正确性高度一致，尽管先前的研究发现深度神经网络往往过于自信。最近的研究将这种行为与最终层的特定组件相关联，如熵神经元和未嵌入矩阵的零空间。在本工作中，我们通过调查网络深度中校准如何演变来提供一种补充视角。在多个开放权重模型在MMLU基准上的分析中，我们发现了一个在上层/后期的独特的置信度校正阶段，在决策确定性达成后，模型的置信度被积极重新校准。此外，我们在残差流中识别出一个低维度的校准方向，其扰动显著提高了校准指标（CE和MCE）而不损害准确率。我们的发现表明，校准是一个分布式现象，在整个网络前向传递过程中形成，而不仅仅是在最终投影中形成，为理解LLM中的置信度调节机制提供了新的见解。', 'title_zh': '跨层校准：理解大型语言模型中校准的演变'}
{'arxiv_id': 'arXiv:2511.00230', 'title': 'Neural Transparency: Mechanistic Interpretability Interfaces for Anticipating Model Behaviors for Personalized AI', 'authors': 'Sheer Karny, Anthony Baez, Pat Pataranutaporn', 'link': 'https://arxiv.org/abs/2511.00230', 'abstract': "Millions of users now design personalized LLM-based chatbots that shape their daily interactions, yet they can only loosely anticipate how their design choices will manifest as behaviors in deployment. This opacity is consequential: seemingly innocuous prompts can trigger excessive sycophancy, toxicity, or inconsistency, degrading utility and raising safety concerns. To address this issue, we introduce an interface that enables neural transparency by exposing language model internals during chatbot design. Our approach extracts behavioral trait vectors (empathy, toxicity, sycophancy, etc.) by computing differences in neural activations between contrastive system prompts that elicit opposing behaviors. We predict chatbot behaviors by projecting the system prompt's final token activations onto these trait vectors, normalizing for cross-trait comparability, and visualizing results via an interactive sunburst diagram. To evaluate this approach, we conducted an online user study using Prolific to compare our neural transparency interface against a baseline chatbot interface without any form of transparency. Our analyses suggest that users systematically miscalibrated AI behavior: participants misjudged trait activations for eleven of fifteen analyzable traits, motivating the need for transparency tools in everyday human-AI interaction. While our interface did not change design iteration patterns, it significantly increased user trust and was enthusiastically received. Qualitative analysis indicated that users' had nuanced experiences with the visualization that may enrich future work designing neurally transparent interfaces. This work offers a path for how mechanistic interpretability can be operationalized for non-technical users, establishing a foundation for safer, more aligned human-AI interactions.", 'abstract_zh': 'Millions of 用户如今设计个性化的大规模语言模型基于的聊天机器人，以塑造他们的日常互动，但他们的设计选择在部署中表现出的行为仍只能模糊预测。这种透明度不足是关键问题：看似无害的提示可以触发过度的奉承、毒性或不一致，降低其实用性并引发安全顾虑。为解决这一问题，我们提出了一种接口，通过在聊天机器人设计过程中暴露语言模型的内部机制来实现神经透明性。我们的方法通过计算对比系统提示之间引发对立行为的神经激活差异来提取行为特征向量（同理心、毒性、巴结行为等）。我们通过将系统提示最终令牌的激活映射到这些特征向量上，并进行跨特征比较归一化，再通过交互式的轮辐图可视化结果，来预测聊天机器人的行为。我们通过使用Prolific进行在线用户研究，将我们的神经透明性接口与没有任何透明性的基线聊天机器人接口进行了比较。我们的分析表明，用户系统地误判了AI行为：参与者错误地估计了可分析的十一大特征中的十一项的激活程度，这表明在日常的人机交互中需要透明工具。尽管我们的接口没有改变设计迭代模式，但它显著增加了用户的信任度，并受到了热烈欢迎。定性分析表明，用户对可视化结果有不同的体验，这可能在未来设计神经透明性接口的工作中有丰富的影响。这项工作为如何将机制可解释性应用于非技术用户提供了途径，为更安全、更对齐的人机互动奠定了基础。', 'title_zh': '神经透明性：用于预见个性化AI模型行为的机制可解释性接口'}
{'arxiv_id': 'arXiv:2511.00198', 'title': 'Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap', 'authors': 'Chun-Hao Yang, Bo-Han Feng, Tzu-Yuan Lai, Yan Yu Chen, Yin-Kai Dean Huang, Shou-De Lin', 'link': 'https://arxiv.org/abs/2511.00198', 'abstract': 'Optimizing training performance in large language models (LLMs) remains an essential challenge, particularly in improving model performance while maintaining computational costs. This work challenges the conventional approach of training LLMs using next-token prediction (NTP), arguing that by predicting information-rich tokens during training, there is a more effective way to train LLMs. We investigate the impact of the proposed solution in three kinds of tasks for LLMs: arithmetic, multi-label classification of text, and natural-language generation. This work offers a principled approach to optimizing LLM training, advancing both model performance and theoretical understanding of the target-token selection strategies.', 'abstract_zh': '优化大型语言模型的训练性能仍然是一个关键挑战，特别在于在保持计算成本的同时提高模型性能。本研究挑战了使用下一步标记预测（NTP）训练大型语言模型的常规方法，认为在训练过程中预测信息丰富的标记是一种更有效的方法。我们探讨了所提出解决方案在大型语言模型的三种任务中的影响：算术运算、文本多标签分类和自然语言生成。本研究为优化大型语言模型的训练提供了一个原则性的方法，既提高了模型性能，又深化了对目标标记选择策略的理解。', 'title_zh': '训练大规模语言模型超越下一个令牌预测：填充互信息缺口'}
{'arxiv_id': 'arXiv:2511.00197', 'title': 'Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories', 'authors': 'Oorja Majgaonkar, Zhiwei Fei, Xiang Li, Federica Sarro, He Ye', 'link': 'https://arxiv.org/abs/2511.00197', 'abstract': 'The increasing deployment of Large Language Model (LLM) agents for complex software engineering tasks has created a need to understand their problem-solving behaviours beyond simple success metrics. While these agents demonstrate impressive capabilities in automated issue resolution, their decision-making processes remain largely opaque. This paper presents an empirical study of agent trajectories, namely the execution traces capturing the steps agents take when attempting to resolve software issues. We analyse trajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and Prometheus) on the SWE-Bench benchmark, examining both successful and failed attempts. Our investigation reveals several key insights into agent behaviour. First, we identify how distinct problem-solving strategies, such as defensive programming and context gathering, enable success in different scenarios. Second, we find that failed trajectories are consistently longer and exhibit higher variance than successful ones, with failure patterns differing significantly between agents. Third, our fault localisation analysis shows that while most trajectories correctly identify problematic files (72-81\\% even in failures), success depends more on achieving approximate rather than exact code modifications. These and other findings unveiled by our study, provide a foundation for understanding agent behaviour through trajectory analysis, contributing to the development of more robust and interpretable autonomous software engineering systems.', 'abstract_zh': '大规模语言模型代理在复杂软件工程任务中的部署增加了理解其问题解决行为的需求，而不仅仅依赖简单的成功指标。虽然这些代理在自动化问题解决方面表现出色，但其决策过程仍然难以理解。本文通过对三个最先进的代码代理（OpenHands、SWE-agent和Prometheus）在SWE-Bench基准上的执行轨迹进行实证研究，分析了成功和失败尝试。研究揭示了代理行为的几个关键洞察。首先，我们确定了不同的问题解决策略，如防御性编程和上下文收集，如何在不同场景中促进成功。其次，我们发现失败的轨迹比成功的轨迹更长且更具变异性，不同代理的失败模式显著不同。第三，我们的故障定位分析表明，尽管大多数轨迹能够正确识别有问题的文件（即使在失败中，这一比例也达到72-81%），但成功更多依赖于实现近似的而非精确的代码修改。我们通过这项研究揭示的这些及其他发现，为通过轨迹分析理解代理行为奠定了基础，有助于开发更 robust 和可解释的自主软件工程系统。', 'title_zh': '理解代码代理行为：成功与失败轨迹的实证研究'}
{'arxiv_id': 'arXiv:2511.00192', 'title': 'EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs', 'authors': 'Ali Satvaty, Suzan Verberne, Fatih Turkmen', 'link': 'https://arxiv.org/abs/2511.00192', 'abstract': "Membership inference attacks (MIA) aim to infer whether a particular data point is part of the training dataset of a model. In this paper, we propose a new task in the context of LLM privacy: entity-level discovery of membership risk focused on sensitive information (PII, credit card numbers, etc). Existing methods for MIA can detect the presence of entire prompts or documents in the LLM training data, but they fail to capture risks at a finer granularity. We propose the ``EL-MIA'' framework for auditing entity-level membership risks in LLMs. We construct a benchmark dataset for the evaluation of MIA methods on this task. Using this benchmark, we conduct a systematic comparison of existing MIA techniques as well as two newly proposed methods. We provide a comprehensive analysis of the results, trying to explain the relation of the entity level MIA susceptability with the model scale, training epochs, and other surface level factors. Our findings reveal that existing MIA methods are limited when it comes to entity-level membership inference of the sensitive attributes, while this susceptibility can be outlined with relatively straightforward methods, highlighting the need for stronger adversaries to stress test the provided threat model.", 'abstract_zh': '实体级别隐私风险发现：针对敏感信息的会员推断攻击', 'title_zh': 'EL-MIA：量化大型语言模型中敏感实体的成员推断风险'}
{'arxiv_id': 'arXiv:2511.00176', 'title': 'Effectiveness of LLMs in Temporal User Profiling for Recommendation', 'authors': 'Milad Sabouri, Masoud Mansoury, Kun Lin, Bamshad Mobasher', 'link': 'https://arxiv.org/abs/2511.00176', 'abstract': 'Effectively modeling the dynamic nature of user preferences is crucial for enhancing recommendation accuracy and fostering transparency in recommender systems. Traditional user profiling often overlooks the distinction between transitory short-term interests and stable long-term preferences. This paper examines the capability of leveraging Large Language Models (LLMs) to capture these temporal dynamics, generating richer user representations through distinct short-term and long-term textual summaries of interaction histories. Our observations suggest that while LLMs tend to improve recommendation quality in domains with more active user engagement, their benefits appear less pronounced in sparser environments. This disparity likely stems from the varying distinguishability of short-term and long-term preferences across domains; the approach shows greater utility where these temporal interests are more clearly separable (e.g., Movies\\&TV) compared to domains with more stable user profiles (e.g., Video Games). This highlights a critical trade-off between enhanced performance and computational costs, suggesting context-dependent LLM application. Beyond predictive capability, this LLM-driven approach inherently provides an intrinsic potential for interpretability through its natural language profiles and attention weights. This work contributes insights into the practical capability and inherent interpretability of LLM-driven temporal user profiling, outlining new research directions for developing adaptive and transparent recommender systems.', 'abstract_zh': '有效地建模用户偏好动态特性对于提高推荐准确性和增强推荐系统透明度至关重要。传统用户画像往往忽视了短期兴趣与稳定偏好之间的区别。本文探讨了利用大规模语言模型（LLMs）捕获这些时间动态性的能力，通过不同的短期和长期文本摘要来生成更丰富的用户表示。观察结果显示，虽然LLMs在用户活跃度较高的领域中倾向于提高推荐质量，但在稀疏环境中其优势并不明显。这种差异可能源于不同领域短期和长期偏好区分度的差异；该方法在短期和长期兴趣更易于区分的领域（例如，电影与电视）显示出更大的应用价值，而在用户画像较为稳定的领域（例如，电子游戏）则较为有限。这突显了提升性能与计算成本之间的重要权衡，表明LLM的应用具有情境依赖性。除了预测能力，这种基于LLM的方法还通过其自然语言画像和注意力权重提供了内在的可解释性潜力。本文为利用大规模语言模型驱动的时间用户画像提供了实用能力和内在可解释性的洞见，并为开发适应性和透明的推荐系统指出了新的研究方向。', 'title_zh': 'LLMs在时间敏感用户画像推荐中的有效性'}
{'arxiv_id': 'arXiv:2511.00160', 'title': 'What a diff makes: automating code migration with large language models', 'authors': 'Katherine A. Rosenfeld, Cliff C. Kerr, Jessica Lundin', 'link': 'https://arxiv.org/abs/2511.00160', 'abstract': 'Modern software programs are built on stacks that are often undergoing changes that introduce updates and improvements, but may also break any project that depends upon them. In this paper we explore the use of Large Language Models (LLMs) for code migration, specifically the problem of maintaining compatibility with a dependency as it undergoes major and minor semantic version changes. We demonstrate, using metrics such as test coverage and change comparisons, that contexts containing diffs can significantly improve performance against out of the box LLMs and, in some cases, perform better than using code. We provide a dataset to assist in further development of this problem area, as well as an open-source Python package, AIMigrate, that can be used to assist with migrating code bases. In a real-world migration of TYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of required changes in a single run, increasing to 80% with multiple runs, with 47% of changes generated perfectly.', 'abstract_zh': '现代软件程序建立在不断变化的栈之上，这些变化带来了更新和改进，但也可能破坏依赖它们的任何项目。本文探讨了使用大规模语言模型（LLMs）进行代码迁移的问题，特别是在依赖项经历主要和次要语义版本变化时保持兼容性的问题。我们使用测试覆盖率和更改比较等指标来证明包含差异的上下文可以显著提高性能，并且在某些情况下，性能甚至优于使用代码。我们提供了一个数据集以帮助进一步开发这一领域，并提供了一个开源Python包AIMigrate，可用于帮助迁移代码基础。在从STARSIM版本迁移TYPHOIDSIM的实际迁移中，AIMigrate在一次运行中正确识别了65%的必要更改，多次运行后增加到80%，其中47%的更改被完美生成。', 'title_zh': '大型语言模型自动化代码迁移的研究'}
{'arxiv_id': 'arXiv:2511.00136', 'title': 'A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control', 'authors': 'Qing Guo, Xinhang Li, Junyu Chen, Zheng Guo, Xiaocong Li, Lin Zhang, Lei Li', 'link': 'https://arxiv.org/abs/2511.00136', 'abstract': "Leveraging large language models (LLMs) in traffic signal control (TSC) improves optimization efficiency and interpretability compared to traditional reinforcement learning (RL) methods. However, existing LLM-based approaches are limited by fixed time signal durations and are prone to hallucination errors, while RL methods lack robustness in signal timing decisions and suffer from poor generalization. To address these challenges, this paper proposes HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The Herald Module extracts contextual information and forecasts queue lengths for each traffic phase based on real-time conditions. The first LLM, LLM-Agent, uses these forecasts to make fine grained traffic signal control, while the second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and hallucinations. These refined outputs are used for score-based fine-tuning to improve accuracy and robustness. Simulation experiments using CityFlow on real world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New York (196) demonstrate that HeraldLight outperforms state of the art baselines, achieving a 20.03% reduction in average travel time across all scenarios and a 10.74% reduction in average queue length on the Jinan and Hangzhou scenarios. The source code is available on GitHub: this https URL.", 'abstract_zh': '利用引导提示增强的大语言模型双架构HeraldLight在交通信号控制中的应用：提高优化效率和解释性，克服传统强化学习方法的局限', 'title_zh': '带有 herald 引导提示的双大型语言模型架构missive并行细粒度交通信号控制'}
{'arxiv_id': 'arXiv:2511.00125', 'title': 'Inferring multiple helper Dafny assertions with LLMs', 'authors': 'Álvaro Silva, Alexandra Mendes, Ruben Martins', 'link': 'https://arxiv.org/abs/2511.00125', 'abstract': 'The Dafny verifier provides strong correctness guarantees but often requires numerous manual helper assertions, creating a significant barrier to adoption. We investigate the use of Large Language Models (LLMs) to automatically infer missing helper assertions in Dafny programs, with a primary focus on cases involving multiple missing assertions. To support this study, we extend the DafnyBench benchmark with curated datasets where one, two, or all assertions are removed, and we introduce a taxonomy of assertion types to analyze inference difficulty. Our approach refines fault localization through a hybrid method that combines LLM predictions with error-message heuristics. We implement this approach in a new tool called DAISY (Dafny Assertion Inference SYstem). While our focus is on multiple missing assertions, we also evaluate DAISY on single-assertion cases. DAISY verifies 63.4% of programs with one missing assertion and 31.7% with multiple missing assertions. Notably, many programs can be verified with fewer assertions than originally present, highlighting that proofs often admit multiple valid repair strategies and that recovering every original assertion is unnecessary. These results demonstrate that automated assertion inference can substantially reduce proof engineering effort and represent a step toward more scalable and accessible formal verification.', 'abstract_zh': '利用大型语言模型自动推导Dafny程序中缺失的帮助断言：面向多断言情况的研究', 'title_zh': '使用大型语言模型推断多个辅助Dafny断言'}
{'arxiv_id': 'arXiv:2511.00101', 'title': 'Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving', 'authors': 'Yuchen Zhang, Hanyue Du, Chun Cao, Jingwei Xu', 'link': 'https://arxiv.org/abs/2511.00101', 'abstract': 'Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient fine-tuning (PEFT) technique for adapting large language models (LLMs) to downstream tasks. While prior work has explored strategies for integrating LLM training and serving, there still remains a gap in unifying fine-tuning and inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA framework that seamlessly integrates LoRA fine-tuning and serving within a single runtime. Loquetier introduces two key components: (1) a Virtualized Module that isolates PEFT-based modifications and supports multiple adapters on a shared base model, and (2) an optimized computation flow with a kernel design that merges fine-tuning and inference paths in forward propagation, enabling efficient batching and minimizing kernel invocation overhead. Extensive experiments across three task settings show that Loquetier consistently outperforms existing baselines in both performance and flexibility, achieving up to $3.0\\times$ the throughput of the state-of-the-art co-serving system on inference-only tasks and $46.4\\times$ higher SLO attainment than PEFT on unified fine-tuning and inference tasks. The implementation of Loquetier is publicly available at this https URL.', 'abstract_zh': 'Loquetier：一种无缝集成LoRA微调与推理的虚拟化多LoRA框架', 'title_zh': 'Loquetier：一种统一的LLM微调与服务虚拟化多LoRA框架'}
{'arxiv_id': 'arXiv:2511.00096', 'title': 'Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System', 'authors': 'Shangyu Lou', 'link': 'https://arxiv.org/abs/2511.00096', 'abstract': 'Urban Artificial Intelligence (Urban AI) has advanced human-centered urban tasks such as perception prediction and human dynamics. Large Language Models (LLMs) can integrate multimodal inputs to address heterogeneous data in complex urban systems but often underperform on domain-specific tasks. Urban-MAS, an LLM-based Multi-Agent System (MAS) framework, is introduced for human- centered urban prediction under zero-shot settings. It includes three agent types: Predictive Factor Guidance Agents, which prioritize key predictive factors to guide knowledge extraction and enhance the effectiveness of compressed urban knowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve robustness by com- paring multiple outputs, validating consistency, and re-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which integrate extracted multi-source information across dimensions for prediction. Experiments on running-amount prediction and ur- ban perception across Tokyo, Milan, and Seattle demonstrate that Urban-MAS substantially reduces errors compared to single-LLM baselines. Ablation studies indicate that Predictive Factor Guidance Agents are most critical for enhancing predictive performance, po- sitioning Urban-MAS as a scalable paradigm for human-centered urban AI prediction. Code is available on the project website:this https URL', 'abstract_zh': '城市人工智能（Urban AI）推进了以人类为中心的城市任务，如感知预测和人类动态。大型语言模型（LLMs）可以整合多模态输入以解决复杂城市系统中的异构数据问题，但在特定领域任务上常常表现不佳。Urban-MAS，一个基于大型语言模型的多智能体系统（MAS）框架，引入了一种在零 shot 设置下进行以人类为中心的城市预测的方法。它包括三种智能体类型：预测因子引导智能体，优先考虑关键预测因素以指导知识提取并增强压缩城市知识的有效性；可靠的城市信息提取智能体，通过比较多个输出、验证一致性并在冲突发生时重新提取信息来提高鲁棒性；多城市信息推理智能体，跨维度整合提取的多源信息以进行预测。在东京、米兰和西雅图的城市流量预测和城市感知实验中，Urban-MAS 显著减少了错误，相比单一 LLM 基线更为准确。消融研究显示，预测因子引导智能体对于提高预测性能至关重要，确立了 Urban-MAS 作为可扩展的人类中心城市 AI 预测范式的地位。代码请参见项目网站：this https URL。', 'title_zh': '基于LLM的多Agent系统导向的人本城市预测'}
{'arxiv_id': 'arXiv:2511.00087', 'title': 'Adding New Capability in Existing Scientific Application with LLM Assistance', 'authors': 'Anshu Dubey, Akash Dhruv', 'link': 'https://arxiv.org/abs/2511.00087', 'abstract': 'With the emergence and rapid evolution of large language models (LLM), automating coding tasks has become an im- portant research topic. Many efforts are underway and liter- ature abounds about the efficacy of models and their ability to generate code. A less explored aspect of code generation is for new algorithms, where the training data-set would not have included any previous example of similar code. In this paper we propose a new methodology for writing code from scratch for a new algorithm using LLM assistance, and describe enhancement of a previously developed code- translation tool, Code-Scribe, for new code generation.', 'abstract_zh': '随着大型语言模型（LLM）的出现及其快速进化，自动化编码任务已成为一个重要的研究课题。许多研究正在探索模型的有效性及其生成代码的能力。代码生成的一个较少研究的方面是针对新算法的生成，其中训练数据集中不会包含任何类似代码的先前示例。在本文中，我们提出了一种新的方法，利用LLM辅助从零开始编写新算法的代码，并 describe 对先前开发的代码转换工具Code-Scribe进行了增强，以支持新代码的生成。', 'title_zh': '利用LLM增强现有科学应用的新功能'}
{'arxiv_id': 'arXiv:2511.00086', 'title': 'Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph', 'authors': 'Fali Wang, Jihai Chen, Shuhua Yang, Runxue Bao, Tianxiang Zhao, Zhiwei Zhang, Xianfeng Tang, Hui Liu, Qi He, Suhang Wang', 'link': 'https://arxiv.org/abs/2511.00086', 'abstract': 'Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.', 'abstract_zh': 'Test-Time Scaling (TTS) 在固定预算下的计算最优模型组合与架构搜索', 'title_zh': '测试时计算优化缩放的一般化：可优化图形式表示'}
{'arxiv_id': 'arXiv:2511.00070', 'title': 'Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design', 'authors': 'Muhammad Bilal Awan, Abdul Razzaq, Abdul Shahid', 'link': 'https://arxiv.org/abs/2511.00070', 'abstract': "This paper investigates the performance of Large Language Models (LLMs) as generative optimizers for solving constrained multi-objective regression tasks, specifically within the challenging domain of inverse design (property-to-structure mapping). This problem, critical to materials informatics, demands finding complex, feasible input vectors that lie on the Pareto optimal front. While LLMs have demonstrated universal effectiveness across generative and reasoning tasks, their utility in constrained, continuous, high-dimensional numerical spaces tasks they weren't explicitly architected for remains an open research question. We conducted a rigorous comparative study between established Bayesian Optimization (BO) frameworks and a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the foundational BoTorch Ax implementation against the state-of-the-art q-Expected Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the challenge as a regression problem with a custom output head. Our results show that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the performance ceiling. Crucially, the best-performing LLM (WizardMath-7B) achieved a Generational Distance (GD) of 1.21, significantly outperforming the traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO frameworks remain the performance leader for guaranteed convergence, but fine-tuned LLMs are validated as a promising, computationally fast alternative, contributing essential comparative metrics to the field of AI-driven optimization. The findings have direct industrial applications in optimizing formulation design for resins, polymers, and paints, where multi-objective trade-offs between mechanical, rheological, and chemical properties are critical to innovation and production efficiency.", 'abstract_zh': '大型语言模型作为生成优化器解决受限多目标回归任务的研究：以逆设计（属性到结构映射）领域为例', 'title_zh': '基于约束多目标逆向设计，对比生成式AI与贝叶斯优化的性能benchmark'}
{'arxiv_id': 'arXiv:2511.00056', 'title': 'MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling', 'authors': 'Yuxi Liu, Renjia Deng, Yutong He, Xue Wang, Tao Yao, Kun Yuan', 'link': 'https://arxiv.org/abs/2511.00056', 'abstract': "The substantial memory demands of pre-training and fine-tuning large language models (LLMs) require memory-efficient optimization algorithms. One promising approach is layer-wise optimization, which treats each transformer block as a single layer and optimizes it sequentially, while freezing the other layers to save optimizer states and activations. Although effective, these methods ignore the varying importance of the modules within each layer, leading to suboptimal performance. Moreover, layer-wise sampling provides only limited memory savings, as at least one full layer must remain active during optimization. To overcome these limitations, we propose Module-wise Importance SAmpling (MISA), a novel method that divides each layer into smaller modules and assigns importance scores to each module. MISA uses a weighted random sampling mechanism to activate modules, provably reducing gradient variance compared to layer-wise sampling. Additionally, we establish an \\(\\mathcal{O}(1/\\sqrt{K})\\) convergence rate under non-convex and stochastic conditions, where $K$ is the total number of block updates, and provide a detailed memory analysis showcasing MISA's superiority over existing baseline methods. Experiments on diverse learning tasks validate the effectiveness of MISA. Source code is available at this https URL.", 'abstract_zh': '大型语言模型（LLMs）的预训练和微调的显著内存需求要求使用内存高效的优化算法。一种有前景的方法是层-wise优化，即将每个变压器块视为一层并顺序优化，同时冻结其他层以节省优化器状态和激活值。尽管有效，但这些方法忽略了每层内模块的重要性差异，导致性能不佳。此外，层-wise采样仅提供了有限的内存节省，因为优化过程中至少必须有一个完整的层保持激活状态。为克服这些限制，我们提出了模块-wise重要性采样（MISA），这是一种新颖的方法，将每层划分为较小的模块，并为每个模块分配重要性分数。MISA 使用加权随机采样机制激活模块，可以证明与层-wise采样相比减少梯度方差。此外，我们证明在非凸和随机条件下，MISA 具有 \\(\\mathcal{O}(1/\\sqrt{K})\\) 的收敛率，其中\\(K\\) 是块更新的总数，并提供了详细的内存分析，展示了 MISA 在现有基线方法中的优越性。来自不同学习任务的实验验证了 MISA 的有效性。源代码可在此网址获取。', 'title_zh': 'MISA：基于模块重要性采样的内存高效LLMs优化'}
{'arxiv_id': 'arXiv:2511.00050', 'title': 'FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs', 'authors': 'Dhananjaya Gowda, Seoha Song, Junhyun Lee, Harshith Goka', 'link': 'https://arxiv.org/abs/2511.00050', 'abstract': 'As the large language models (LLMs) grow in size each day, efficient training and fine-tuning has never been as important as nowadays. This resulted in the great interest in parameter efficient fine-tuning (PEFT), and effective methods including low-rank adapters (LoRA) has emerged. Although the various PEFT methods have been studied extensively in the recent years, the greater part of the subject remains unexplored with the huge degree of freedom. In this paper, we propose FLoRA, a family of fused forward-backward adapters (FFBA) for parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine ideas from the popular LoRA and parallel adapters to improve the overall fine-tuning accuracies. At the same time, latencies are minimized by fusing the forward and backward adapters into existing projection layers of the base model. Experimental results show that the proposed FFB adapters perform significantly better than the popularly used LoRA in both accuracy and latency for a similar parameter budget.', 'abstract_zh': '随着大型语言模型（LLMs）的规模日益增大，高效的训练和微调从未像现在这样重要。这导致了对参数高效微调（PEFT）及其有效方法，如低秩适配器（LoRA）的高度关注。尽管近年来对各种PEFT方法进行了广泛研究，但该领域的大部分仍有待探索。在本文中，我们提出了一种融合前向-后向适配器（FFBA）的FLoRA家族，用于在下游任务中对LLMs进行参数高效微调。FFBA结合了流行的LoRA和并行适配器的思想，以提高整体微调精度。同时，通过将前向和后向适配器融合到基模型的现有投影层中，最大限度地减少了延迟。实验结果表明，所提出的FFB适配器在参数预算相似的情况下，在精度和延迟方面均显著优于常用的LoRA。', 'title_zh': 'FLoRA: 融合前向-_backward适配器以实现参数高效微调并减少LLMs的推理时间延迟'}
{'arxiv_id': 'arXiv:2511.00030', 'title': 'Probing Knowledge Holes in Unlearned LLMs', 'authors': 'Myeongseob Ko, Hoang Anh Just, Charles Fleming, Ming Jin, Ruoxi Jia', 'link': 'https://arxiv.org/abs/2511.00030', 'abstract': "Machine unlearning has emerged as a prevalent technical solution for selectively removing unwanted knowledge absorbed during pre-training, without requiring full retraining. While recent unlearning techniques can effectively remove undesirable content without severely compromising performance on standard benchmarks, we find that they may inadvertently create ``knowledge holes'' -- unintended losses of benign knowledge that standard benchmarks fail to capture. To probe where unlearned models reveal knowledge holes, we propose a test case generation framework that explores both immediate neighbors of unlearned content and broader areas of potential failures. Our evaluation demonstrates significant hidden costs of unlearning: up to 98.7\\% of the test cases yield irrelevant or nonsensical responses from unlearned models, despite being answerable by the pretrained model. These findings necessitate rethinking the conventional approach to evaluating knowledge preservation in unlearning, moving beyond standard, static benchmarks.", 'abstract_zh': '机器去学习作为一种从预训练中选择性移除不需要的知识的技术方案日益流行，无需进行全面重训练。尽管最近的去学习技术能够在不严重损害标准基准性能的情况下有效移除不良内容，但我们发现它们可能会无意中创建“知识漏洞”——标准基准未能捕捉到的有益知识的意外损失。为了探究去学习模型中知识漏洞的揭露情况，我们提出了一种测试案例生成框架，探索未学习内容的 immediate neighbors 以及更广泛的潜在失败区域。我们的评估显示了去学习的巨大隐性成本：高达 98.7% 的测试案例会从去学习模型中获得无关或无意义的回答，尽管这些答案可以通过预训练模型给出。这些发现需要重新考量对去学习中知识保留进行评估的常规方法，超越标准的静态基准。', 'title_zh': '探究未训练LLMs的知识缺口'}
{'arxiv_id': 'arXiv:2511.00029', 'title': 'Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts', 'authors': 'Samaksh Bhargav, Zining Zhu', 'link': 'https://arxiv.org/abs/2511.00029', 'abstract': 'Large Language Model (LLM) deployment requires guiding the LLM to recognize and not answer unsafe prompts while complying with safe prompts. Previous methods for achieving this require adjusting model weights along with other expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have enabled interpretable feature extraction from LLMs, existing approaches lack systematic feature selection methods and principled evaluation of safety-utility tradeoffs. We explored using different steering features and steering strengths using Sparse Auto Encoders (SAEs) to provide a solution. Using an accurate and innovative contrasting prompt method with the AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air Bench eu-dataset to efficiently choose the best features in the model to steer, we tested this method on Llama-3 8B. We conclude that using this method, our approach achieves an 18.9% improvement in safety performance while simultaneously increasing utility by 11.1%, demonstrating that targeted SAE steering can overcome traditional safety-utility tradeoffs when optimal features are identified through principled selection methods.', 'abstract_zh': '使用稀疏自编码器进行定向引导以克服安全性-实用性权衡', 'title_zh': '特征引导的SAE转向控件以使用对比提示控制拒绝率'}
{'arxiv_id': 'arXiv:2511.00024', 'title': 'Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model', 'authors': 'Haotian Hang, Yueyang Shen, Vicky Zhu, Jose Cruz, Michelle Li', 'link': 'https://arxiv.org/abs/2511.00024', 'abstract': "In the context of global sustainability mandates, corporate carbon disclosure has emerged as a critical mechanism for aligning business strategy with environmental responsibility. The Carbon Disclosure Project (CDP) hosts the world's largest longitudinal dataset of climate-related survey responses, combining structured indicators with open-ended narratives, but the heterogeneity and free-form nature of these disclosures present significant analytical challenges for benchmarking, compliance monitoring, and investment screening. This paper proposes a novel decision-support framework that leverages large language models (LLMs) to assess corporate climate disclosure quality at scale. It develops a master rubric that harmonizes narrative scoring across 11 years of CDP data (2010-2020), enabling cross-sector and cross-country benchmarking. By integrating rubric-guided scoring with percentile-based normalization, our method identifies temporal trends, strategic alignment patterns, and inconsistencies in disclosure across industries and regions. Results reveal that sectors such as technology and countries like Germany consistently demonstrate higher rubric alignment, while others exhibit volatility or superficial engagement, offering insights that inform key decision-making processes for investors, regulators, and corporate environmental, social, and governance (ESG) strategists. The proposed LLM-based approach transforms unstructured disclosures into quantifiable, interpretable, comparable, and actionable intelligence, advancing the capabilities of AI-enabled decision support systems (DSSs) in the domain of climate governance.", 'abstract_zh': '在全球可持续发展背景下，企业碳披露已成为将企业战略与环境责任相一致的关键机制。碳披露项目（CDP）拥有全球最大规模的纵向气候相关调研数据集，结合了结构化指标与开放性叙述，但这些披露的异质性和非结构化性质为企业绩效基准评估、合规监控和投资筛选带来了重要的分析挑战。本文提出了一种新的决策支持框架，利用大规模语言模型（LLMs）对企业的气候披露质量进行大规模评估。该框架开发了一个主评析表，以一致化11年（2010-2020）的CDP数据中的叙述评分，实现跨行业和跨国家的基准比较。通过将评析表指导评分与基于百分位数的标准化相结合，我们的方法识别了不同时期的趋势、战略一致模式以及行业和地区间的披露不一致性。结果表明，如科技行业和德国等国家的一致性较高，而其他国家则表现出波动性或表面化的参与，提供了对投资者、监管机构和企业环境、社会与治理（ESG）策略制定者关键决策过程的见解。所提出的大规模语言模型方法将非结构化披露转化为可量化、可解释、可比较和可操作的情报，推动了人工智能驱动决策支持系统（DSSs）在气候治理领域的应用能力。', 'title_zh': '与AI闲聊：通过大型语言模型理解全球公司的供应链碳披露'}
