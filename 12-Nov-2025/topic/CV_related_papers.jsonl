{'arxiv_id': 'arXiv:2511.07750', 'title': 'Navigating the Wild: Pareto-Optimal Visual Decision-Making in Image Space', 'authors': 'Durgakant Pushp, Weizhe Chen, Zheng Chen, Chaomin Luo, Jason M. Gregory, Lantao Liu', 'link': 'https://arxiv.org/abs/2511.07750', 'abstract': 'Navigating complex real-world environments requires semantic understanding and adaptive decision-making. Traditional reactive methods without maps often fail in cluttered settings, map-based approaches demand heavy mapping effort, and learning-based solutions rely on large datasets with limited generalization. To address these challenges, we present Pareto-Optimal Visual Navigation, a lightweight image-space framework that combines data-driven semantics, Pareto-optimal decision-making, and visual servoing for real-time navigation.', 'abstract_zh': 'Pareto-Optimal视觉导航：一种结合数据驱动语义、帕累托优化决策和视觉伺服的轻量级图像空间框架', 'title_zh': '穿越混沌：图像空间中的帕累托最优视觉决策'}
{'arxiv_id': 'arXiv:2511.07928', 'title': 'An Image-Based Path Planning Algorithm Using a UAV Equipped with Stereo Vision', 'authors': 'Selim Ahmet Iz, Mustafa Unel', 'link': 'https://arxiv.org/abs/2511.07928', 'abstract': 'This paper presents a novel image-based path planning algorithm that was developed using computer vision techniques, as well as its comparative analysis with well-known deterministic and probabilistic algorithms, namely A* and Probabilistic Road Map algorithm (PRM). The terrain depth has a significant impact on the calculated path safety. The craters and hills on the surface cannot be distinguished in a two-dimensional image. The proposed method uses a disparity map of the terrain that is generated by using a UAV. Several computer vision techniques, including edge, line and corner detection methods, as well as the stereo depth reconstruction technique, are applied to the captured images and the found disparity map is used to define candidate way-points of the trajectory. The initial and desired points are detected automatically using ArUco marker pose estimation and circle detection techniques. After presenting the mathematical model and vision techniques, the developed algorithm is compared with well-known algorithms on different virtual scenes created in the V-REP simulation program and a physical setup created in a laboratory environment. Results are promising and demonstrate effectiveness of the proposed algorithm.', 'abstract_zh': '基于计算机视觉的新型图像导向路径规划算法及其与A*和概率路网算法的比较分析', 'title_zh': '基于立体视觉 UAV 的图像引导路径规划算法'}
{'arxiv_id': 'arXiv:2511.08168', 'title': 'oboro: Text-to-Image Synthesis on Limited Data using Flow-based Diffusion Transformer with MMH Attention', 'authors': 'Ryusuke Mizutani, Kazuaki Matano, Tsugumi Kadowaki, Haruki Tenya, Layris, nuigurumi, Koki Hashimoto, Yu Tanaka', 'link': 'https://arxiv.org/abs/2511.08168', 'abstract': 'This project was conducted as a 2nd-term adopted project of the "Post-5G Information and Communication System Infrastructure Enhancement R&D Project Development of Competitive Generative AI Foundation Models (GENIAC)," a business of the Ministry of Economy, Trade and Industry (METI) and the New Energy and Industrial Technology Development Organization (NEDO). To address challenges such as labor shortages in Japan\'s anime production industry, this project aims to develop an image generation model from scratch. This report details the technical specifications of the developed image generation model, "oboro:." We have developed "oboro:," a new image generation model built from scratch, using only copyright-cleared images for training. A key characteristic is its architecture, designed to generate high-quality images even from limited datasets. The foundation model weights and inference code are publicly available alongside this report. This project marks the first release of an open-source, commercially-oriented image generation AI fully developed in Japan. AiHUB originated from the OSS community; by maintaining transparency in our development process, we aim to contribute to Japan\'s AI researcher and engineer community and promote the domestic AI development ecosystem.', 'abstract_zh': '本项目作为“后5G信息与通信系统基础设施增强研发项目——竞争性生成人工智能基础模型（GENIAC）”的第二学期采用项目，由经济产业省（METI）和新能源产业技术综合开发机构（NEDO）发起。为应对日本动画产业劳动力短缺等挑战，本项目旨在从零开始开发一种图像生成模型。“oboro:”图像生成模型技术规格在此报告中详细说明。我们利用仅经过版权许可的数据训练开发了“oboro:”，一种全新的从零开始构建的图像生成模型。其关键特性在于其架构，即使在有限的数据集上也能生成高质量的图像。本报告还公开了基础模型权重和推理代码。该项目标志着日本首次发布开源的商业化图像生成AI。AiHUB源自OSS社区，通过保持开发过程的透明性，我们希望能够为日本的AI研究者和工程师社区做出贡献，并促进国内AI发展生态系统。', 'title_zh': 'oboro：使用MMH注意力机制的流基础扩散变换器在有限数据上的文本到图像合成'}
{'arxiv_id': 'arXiv:2511.07719', 'title': 'Operational machine learning for remote spectroscopic detection of CH$_{4}$ point sources', 'authors': 'Vít Růžička, Gonzalo Mateo-García, Itziar Irakulis-Loitxate, Juan Emmanuel Johnson, Manuel Montesino San Martín, Anna Allen, Luis Guanter, David R. Thompson', 'link': 'https://arxiv.org/abs/2511.07719', 'abstract': "Mitigating anthropogenic methane sources is one the most cost-effective levers to slow down global warming. While satellite-based imaging spectrometers, such as EMIT, PRISMA, and EnMAP, can detect these point sources, current methane retrieval methods based on matched filters still produce a high number of false detections requiring laborious manual verification. This paper describes the operational deployment of a machine learning system for detecting methane emissions within the Methane Alert and Response System (MARS) of the United Nations Environment Programme's International Methane Emissions Observatory. We created the largest and most diverse global dataset of annotated methane plumes from three imaging spectrometer missions and quantitatively compared different deep learning model configurations. Focusing on the requirements for operational deployment, we extended prior evaluation methodologies from small tiled datasets to full granule evaluation. This revealed that deep learning models still produce a large number of false detections, a problem we address with model ensembling, which reduced false detections by over 74%. Deployed in the MARS pipeline, our system processes scenes and proposes plumes to analysts, accelerating the detection and analysis process. During seven months of operational deployment, it facilitated the verification of 1,351 distinct methane leaks, resulting in 479 stakeholder notifications. We further demonstrate the model's utility in verifying mitigation success through case studies in Libya, Argentina, Oman, and Azerbaijan. Our work represents a critical step towards a global AI-assisted methane leak detection system, which is required to process the dramatically higher data volumes expected from new and current imaging spectrometers.", 'abstract_zh': '通过机器学习系统减轻人为甲烷源是减缓全球 warming 最具成本效益的手段之一。联合国环境规划署国际甲烷排放观测站的甲烷警报和响应系统（MARS）中机器学习系统部署及其应用', 'title_zh': '远程光谱检测甲烷点源的操作机器学习方法'}
{'arxiv_id': 'arXiv:2511.08402', 'title': 'Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation', 'authors': 'Difei Gu, Yunhe Gao, Mu Zhou, Dimitris Metaxas', 'link': 'https://arxiv.org/abs/2511.08402', 'abstract': "Accurate disease interpretation from radiology remains challenging due to imaging heterogeneity. Achieving expert-level diagnostic decisions requires integration of subtle image features with clinical knowledge. Yet major vision-language models (VLMs) treat images as holistic entities and overlook fine-grained image details that are vital for disease diagnosis. Clinicians analyze images by utilizing their prior medical knowledge and identify anatomical structures as important region of interests (ROIs). Inspired from this human-centric workflow, we introduce Anatomy-VLM, a fine-grained, vision-language model that incorporates multi-scale information. First, we design a model encoder to localize key anatomical features from entire medical images. Second, these regions are enriched with structured knowledge for contextually-aware interpretation. Finally, the model encoder aligns multi-scale medical information to generate clinically-interpretable disease prediction. Anatomy-VLM achieves outstanding performance on both in- and out-of-distribution datasets. We also validate the performance of Anatomy-VLM on downstream image segmentation tasks, suggesting that its fine-grained alignment captures anatomical and pathology-related knowledge. Furthermore, the Anatomy-VLM's encoder facilitates zero-shot anatomy-wise interpretation, providing its strong expert-level clinical interpretation capabilities.", 'abstract_zh': '从放射学影像中准确阐释疾病仍然具有挑战性，因为影像存在异质性。实现专家级诊断决策需要将细微影像特征与临床知识相结合。然而，主要的视觉-语言模型（VLMs）将影像视为整体，忽视了对于疾病诊断至关重要的细粒度影像细节。临床医生通过运用其先前的医学知识分析影像，并识别出重要的解剖结构区域（ROIs）。受这种以人为中心的工作流程启发，我们介绍了Anatomy-VLM，这是一个细粒度的视觉-语言模型，整合了多尺度信息。首先，我们设计了一个模型编码器来从整个医学影像中定位关键解剖特征。其次，这些区域被补充了结构化的知识，以进行上下文感知的解释。最后，模型编码器对多尺度医学信息进行对齐，生成临床可解释的疾病预测。Anatomy-VLM 在分布内和分布外数据集上均取得了卓越的性能。我们还验证了Anatomy-VLM在下游图像分割任务上的性能，表明其细粒度对齐捕捉了解剖学和病理学相关的知识。此外，Anatomy-VLM的编码器促进了零样本的解剖学式解释，提供了其强大的专家级临床解释能力。', 'title_zh': '解剖-VLM：一种精细粒度的视觉-语言模型用于医疗解释'}
{'arxiv_id': 'arXiv:2511.08387', 'title': 'RAPTR: Radar-based 3D Pose Estimation using Transformer', 'authors': 'Sorachi Kato, Ryoma Yataka, Pu Perry Wang, Pedro Miraldo, Takuya Fujihashi, Petros Boufounos', 'link': 'https://arxiv.org/abs/2511.08387', 'abstract': 'Radar-based indoor 3D human pose estimation typically relied on fine-grained 3D keypoint labels, which are costly to obtain especially in complex indoor settings involving clutter, occlusions, or multiple people. In this paper, we propose \\textbf{RAPTR} (RAdar Pose esTimation using tRansformer) under weak supervision, using only 3D BBox and 2D keypoint labels which are considerably easier and more scalable to collect. Our RAPTR is characterized by a two-stage pose decoder architecture with a pseudo-3D deformable attention to enhance (pose/joint) queries with multi-view radar features: a pose decoder estimates initial 3D poses with a 3D template loss designed to utilize the 3D BBox labels and mitigate depth ambiguities; and a joint decoder refines the initial poses with 2D keypoint labels and a 3D gravity loss. Evaluated on two indoor radar datasets, RAPTR outperforms existing methods, reducing joint position error by $34.3\\%$ on HIBER and $76.9\\%$ on MMVR. Our implementation is available at this https URL.', 'abstract_zh': '基于雷达的室内3D人体姿态估计通常依赖于精细的3D关键点标签，这些标签在复杂的室内环境中特别是涉及杂乱、遮挡或多人的情况下获取成本很高。本文在弱监督下提出了一种RATPTR（Radar Pose Estimation using Transformer）方法，仅使用3D BBox和2D关键点标签，这两种标签更容易收集且更具扩展性。RATPTR具有两阶段姿态解码器架构，并通过伪3D可变形注意力增强姿态/关节查询，利用多视雷达特征：姿态解码器使用3D模板损失估计初始3D姿态，以利用3D BBox标签并缓解深度歧义；关节解码器使用2D关键点标签和3D重力损失细化初始姿态。在两个室内雷达数据集上的评估结果显示，RATPTR优于现有方法，减少关键点位置误差分别达34.3%（HIBER）和76.9%（MMVR）。我们的实现代码可在以下链接获取。', 'title_zh': 'RAPTR：基于雷达的3D姿态估计transformer方法'}
{'arxiv_id': 'arXiv:2511.08369', 'title': 'Text-based Aerial-Ground Person Retrieval', 'authors': 'Xinyu Zhou, Yu Wu, Jiayao Ma, Wenhao Wang, Min Cao, Mang Ye', 'link': 'https://arxiv.org/abs/2511.08369', 'abstract': 'This work introduces Text-based Aerial-Ground Person Retrieval (TAG-PR), which aims to retrieve person images from heterogeneous aerial and ground views with textual descriptions. Unlike traditional Text-based Person Retrieval (T-PR), which focuses solely on ground-view images, TAG-PR introduces greater practical significance and presents unique challenges due to the large viewpoint discrepancy across images. To support this task, we contribute: (1) TAG-PEDES dataset, constructed from public benchmarks with automatically generated textual descriptions, enhanced by a diversified text generation paradigm to ensure robustness under view heterogeneity; and (2) TAG-CLIP, a novel retrieval framework that addresses view heterogeneity through a hierarchically-routed mixture of experts module to learn view-specific and view-agnostic features and a viewpoint decoupling strategy to decouple view-specific features for better cross-modal alignment. We evaluate the effectiveness of TAG-CLIP on both the proposed TAG-PEDES dataset and existing T-PR benchmarks. The dataset and code are available at this https URL.', 'abstract_zh': '基于文本的空地人像检索（TAG-PR）：跨视点文本描述的人像检索', 'title_zh': '基于文本的空地人员检索'}
{'arxiv_id': 'arXiv:2511.08240', 'title': 'Hierarchical Direction Perception via Atomic Dot-Product Operators for Rotation-Invariant Point Clouds Learning', 'authors': 'Chenyu Hu, Xiaotong Li, Hao Zhu, Biao Hou', 'link': 'https://arxiv.org/abs/2511.08240', 'abstract': "Point cloud processing has become a cornerstone technology in many 3D vision tasks. However, arbitrary rotations introduce variations in point cloud orientations, posing a long-standing challenge for effective representation learning. The core of this issue is the disruption of the point cloud's intrinsic directional characteristics caused by rotational perturbations. Recent methods attempt to implicitly model rotational equivariance and invariance, preserving directional information and propagating it into deep semantic spaces. Yet, they often fall short of fully exploiting the multiscale directional nature of point clouds to enhance feature representations. To address this, we propose the Direction-Perceptive Vector Network (DiPVNet). At its core is an atomic dot-product operator that simultaneously encodes directional selectivity and rotation invariance--endowing the network with both rotational symmetry modeling and adaptive directional perception. At the local level, we introduce a Learnable Local Dot-Product (L2DP) Operator, which enables interactions between a center point and its neighbors to adaptively capture the non-uniform local structures of point clouds. At the global level, we leverage generalized harmonic analysis to prove that the dot-product between point clouds and spherical sampling vectors is equivalent to a direction-aware spherical Fourier transform (DASFT). This leads to the construction of a global directional response spectrum for modeling holistic directional structures. We rigorously prove the rotation invariance of both operators. Extensive experiments on challenging scenarios involving noise and large-angle rotations demonstrate that DiPVNet achieves state-of-the-art performance on point cloud classification and segmentation tasks. Our code is available at this https URL.", 'abstract_zh': '基于方向感知的向量网络（DiPVNet）：多尺度方向特性下的旋转不变表示学习', 'title_zh': '基于原子点积运算的分层方向感知在旋转不变点云学习中的应用'}
{'arxiv_id': 'arXiv:2511.08224', 'title': '2D Representation for Unguided Single-View 3D Super-Resolution in Real-Time', 'authors': 'Ignasi Mas, Ivan Huerta, Ramon Morros, Javier Ruiz-Hidalgo', 'link': 'https://arxiv.org/abs/2511.08224', 'abstract': 'We introduce 2Dto3D-SR, a versatile framework for real-time single-view 3D super-resolution that eliminates the need for high-resolution RGB guidance. Our framework encodes 3D data from a single viewpoint into a structured 2D representation, enabling the direct application of existing 2D image super-resolution architectures. We utilize the Projected Normalized Coordinate Code (PNCC) to represent 3D geometry from a visible surface as a regular image, thereby circumventing the complexities of 3D point-based or RGB-guided methods. This design supports lightweight and fast models adaptable to various deployment environments. We evaluate 2Dto3D-SR with two implementations: one using Swin Transformers for high accuracy, and another using Vision Mamba for high efficiency. Experiments show the Swin Transformer model achieves state-of-the-art accuracy on standard benchmarks, while the Vision Mamba model delivers competitive results at real-time speeds. This establishes our geometry-guided pipeline as a surprisingly simple yet viable and practical solution for real-world scenarios, especially where high-resolution RGB data is inaccessible.', 'abstract_zh': '我们介绍了一种通用框架2Dto3D-SR，用于实时单视角3D超分辨率，无需高分辨率RGB指导。该框架将单视角的3D数据编码为结构化的2D表示，从而使现有的2D图像超分辨率架构可以直接应用。我们利用投影归一化坐标码（PNCC）将可见表面的3D几何表示为常规图像，从而避免了基于点的3D或RGB指导方法的复杂性。此设计支持轻量级和快速的模型，并适应各种部署环境。我们使用两种实现方式评估了2Dto3D-SR：一种使用Swin Transformer以实现高精度，另一种使用Vision Mamba以实现高效率。实验结果表明，Swin Transformer模型在标准基准上的准确性达到最新水平，而Vision Mamba模型则在实时速度下实现具有竞争力的结果。这确立了我们的基于几何的流水线在实际场景中是一种出人意料的简单但可行且实用的解决方案，尤其是在无法获得高分辨率RGB数据的情况下。', 'title_zh': '未引导单视角实时二维表示三维超分辨率'}
{'arxiv_id': 'arXiv:2511.08133', 'title': 'OTSNet: A Neurocognitive-Inspired Observation-Thinking-Spelling Pipeline for Scene Text Recognition', 'authors': 'Lixu Sun, Nurmemet Yolwas, Wushour Silamu', 'link': 'https://arxiv.org/abs/2511.08133', 'abstract': 'Scene Text Recognition (STR) remains challenging due to real-world complexities, where decoupled visual-linguistic optimization in existing frameworks amplifies error propagation through cross-modal misalignment. Visual encoders exhibit attention bias toward background distractors, while decoders suffer from spatial misalignment when parsing geometrically deformed text-collectively degrading recognition accuracy for irregular patterns. Inspired by the hierarchical cognitive processes in human visual perception, we propose OTSNet, a novel three-stage network embodying a neurocognitive-inspired Observation-Thinking-Spelling pipeline for unified STR modeling. The architecture comprises three core components: (1) a Dual Attention Macaron Encoder (DAME) that refines visual features through differential attention maps to suppress irrelevant regions and enhance discriminative focus; (2) a Position-Aware Module (PAM) and Semantic Quantizer (SQ) that jointly integrate spatial context with glyph-level semantic abstraction via adaptive sampling; and (3) a Multi-Modal Collaborative Verifier (MMCV) that enforces self-correction through cross-modal fusion of visual, semantic, and character-level features. Extensive experiments demonstrate that OTSNet achieves state-of-the-art performance, attaining 83.5% average accuracy on the challenging Union14M-L benchmark and 79.1% on the heavily occluded OST dataset-establishing new records across 9 out of 14 evaluation scenarios.', 'abstract_zh': '场景文本识别（STR）由于现实世界的复杂性仍具有挑战性，现有的框架中解耦的视觉-语言优化通过跨模态错位放大了错误传播。视觉编码器呈现出对背景干扰的注意力偏见，而解码器在解析几何变形文本时遭受空间错位的困扰——这会共同降低不规则模式的识别准确性。受人类视觉知觉的分层次认知过程启发，我们提出了一种新的三阶段网络OTSNet，该网络具备一种灵感源于观察-思考-拼写过程的统一STR建模框架。该架构包括三个核心组件：（1）双注意马卡龙编码器（DAME），通过差异化的注意力图来细化视觉特征，抑制无关区域并增强辨别性聚焦；（2）位置感知模块（PAM）和语义量化器（SQ），它们通过自适应采样共同整合空间上下文与字元级语义抽象；（3）多模态协作验证器（MMCV），它通过视觉、语义和字符级特征的跨模态融合来实现自我纠正。广泛的经验实验证明，OTSNet 达到了最先进的性能，在具有挑战性的 Union14M-L 基准上实现了83.5% 的平均准确率，并在高度遮挡的 OST 数据集上实现了79.1% 的准确率——在14种评估场景中有9种场景创造了新的记录。', 'title_zh': 'OTSNet：一种受神经认知启发的观察-思考-拼写场景文本识别管道'}
{'arxiv_id': 'arXiv:2511.08090', 'title': 'StableMorph: High-Quality Face Morph Generation with Stable Diffusion', 'authors': 'Wassim Kabbani, Kiran Raja, Raghavendra Ramachandra, Christoph Busch', 'link': 'https://arxiv.org/abs/2511.08090', 'abstract': 'Face morphing attacks threaten the integrity of biometric identity systems by enabling multiple individuals to share a single identity. To develop and evaluate effective morphing attack detection (MAD) systems, we need access to high-quality, realistic morphed images that reflect the challenges posed in real-world scenarios. However, existing morph generation methods often produce images that are blurry, riddled with artifacts, or poorly constructed making them easy to detect and not representative of the most dangerous attacks. In this work, we introduce StableMorph, a novel approach that generates highly realistic, artifact-free morphed face images using modern diffusion-based image synthesis. Unlike prior methods, StableMorph produces full-head images with sharp details, avoids common visual flaws, and offers unmatched control over visual attributes. Through extensive evaluation, we show that StableMorph images not only rival or exceed the quality of genuine face images but also maintain a strong ability to fool face recognition systems posing a greater challenge to existing MAD solutions and setting a new standard for morph quality in research and operational testing. StableMorph improves the evaluation of biometric security by creating more realistic and effective attacks and supports the development of more robust detection systems.', 'abstract_zh': '面部形变攻击威胁到生物特征身份系统的完整性，使其多个个体能够共享同一身份。为开发和评估有效的形变攻击检测（MAD）系统，我们需要访问高质量、具现实感的形变图像，以反映现实场景中所面临的挑战。然而，现有形变生成方法往往生成模糊、存在伪影或构建不良的图像，使其易于检测且无法代表最危险的攻击。在本工作中，我们介绍了StableMorph，这是一种新颖的方法，利用现代基于扩散的图像合成技术生成高度现实、无伪影的面部形变图像。与先前方法不同，StableMorph生成全头图像并具有清晰细节，避免常见的视觉缺陷，并提供前所未有的视觉属性控制能力。通过广泛的评估，我们证明StableMorph图像不仅在质量上媲美或超过真实面部图像，而且在欺骗面部识别系统方面表现出强大的能力，给现有MAD解决方案带来更大挑战，并为研究和操作测试制定了新的形变质量标准。StableMorph通过创建更现实和有效的攻击来改进生物特征安全性的评估，并支持更稳健检测系统的开发。', 'title_zh': 'StableMorph: 以稳定扩散生成高质量人脸形态变形'}
{'arxiv_id': 'arXiv:2511.08087', 'title': 'Beyond the Pixels: VLM-based Evaluation of Identity Preservation in Reference-Guided Synthesis', 'authors': 'Aditi Singhania, Krutik Malani, Riddhi Dhawan, Arushi Jain, Garv Tandon, Nippun Sharma, Souymodip Chakraborty, Vineet Batra, Ankit Phogat', 'link': 'https://arxiv.org/abs/2511.08087', 'abstract': 'Evaluating identity preservation in generative models remains a critical yet unresolved challenge. Existing metrics rely on global embeddings or coarse VLM prompting, failing to capture fine-grained identity changes and providing limited diagnostic insight. We introduce Beyond the Pixels, a hierarchical evaluation framework that decomposes identity assessment into feature-level transformations. Our approach guides VLMs through structured reasoning by (1) hierarchically decomposing subjects into (type, style) -> attribute -> feature decision tree, and (2) prompting for concrete transformations rather than abstract similarity scores. This decomposition grounds VLM analysis in verifiable visual evidence, reducing hallucinations and improving consistency. We validate our framework across four state-of-the-art generative models, demonstrating strong alignment with human judgments in measuring identity consistency. Additionally, we introduce a new benchmark specifically designed to stress-test generative models. It comprises 1,078 image-prompt pairs spanning diverse subject types, including underrepresented categories such as anthropomorphic and animated characters, and captures an average of six to seven transformation axes per prompt.', 'abstract_zh': '超越像素：一种分层的身份保存评估框架', 'title_zh': '超越像素：基于VLM的参考引导合成中身份保留评估'}
{'arxiv_id': 'arXiv:2511.08015', 'title': 'Invisible Triggers, Visible Threats! Road-Style Adversarial Creation Attack for Visual 3D Detection in Autonomous Driving', 'authors': 'Jian Wang, Lijun He, Yixing Yong, Haixia Bi, Fan Li', 'link': 'https://arxiv.org/abs/2511.08015', 'abstract': 'Modern autonomous driving (AD) systems leverage 3D object detection to perceive foreground objects in 3D environments for subsequent prediction and planning. Visual 3D detection based on RGB cameras provides a cost-effective solution compared to the LiDAR paradigm. While achieving promising detection accuracy, current deep neural network-based models remain highly susceptible to adversarial examples. The underlying safety concerns motivate us to investigate realistic adversarial attacks in AD scenarios. Previous work has demonstrated the feasibility of placing adversarial posters on the road surface to induce hallucinations in the detector. However, the unnatural appearance of the posters makes them easily noticeable by humans, and their fixed content can be readily targeted and defended. To address these limitations, we propose the AdvRoad to generate diverse road-style adversarial posters. The adversaries have naturalistic appearances resembling the road surface while compromising the detector to perceive non-existent objects at the attack locations. We employ a two-stage approach, termed Road-Style Adversary Generation and Scenario-Associated Adaptation, to maximize the attack effectiveness on the input scene while ensuring the natural appearance of the poster, allowing the attack to be carried out stealthily without drawing human attention. Extensive experiments show that AdvRoad generalizes well to different detectors, scenes, and spoofing locations. Moreover, physical attacks further demonstrate the practical threats in real-world environments.', 'abstract_zh': '现代自动驾驶系统中的3D物体检测及其对抗攻击研究：AdvRoad生成自然风格的路边 adversarial 帖纸', 'title_zh': '隐形触发器，显性威胁！道路风格 adversarial 创造性攻击对自主驾驶中视觉三维检测的挑战'}
{'arxiv_id': 'arXiv:2511.07990', 'title': 'Hardware-Aware YOLO Compression for Low-Power Edge AI on STM32U5 for Weeds Detection in Digital Agriculture', 'authors': 'Charalampos S. Kouzinopoulos, Yuri Manna', 'link': 'https://arxiv.org/abs/2511.07990', 'abstract': 'Weeds significantly reduce crop yields worldwide and pose major challenges to sustainable agriculture. Traditional weed management methods, primarily relying on chemical herbicides, risk environmental contamination and lead to the emergence of herbicide-resistant species. Precision weeding, leveraging computer vision and machine learning methods, offers a promising eco-friendly alternative but is often limited by reliance on high-power computational platforms. This work presents an optimized, low-power edge AI system for weeds detection based on the YOLOv8n object detector deployed on the STM32U575ZI microcontroller. Several compression techniques are applied to the detection model, including structured pruning, integer quantization and input image resolution scaling in order to meet strict hardware constraints. The model is trained and evaluated on the CropAndWeed dataset with 74 plant species, achieving a balanced trade-off between detection accuracy and efficiency. Our system supports real-time, in-situ weeds detection with a minimal energy consumption of 51.8mJ per inference, enabling scalable deployment in power-constrained agricultural environments.', 'abstract_zh': '基于YOLOv8n目标检测器的低功耗边缘AI系统在STM32U575ZI微控制器上实现作物与杂草检测', 'title_zh': 'STM32U5上基于硬件aware的YOLO压缩技术在数字农业杂草检测中的低功耗边缘AI应用'}
{'arxiv_id': 'arXiv:2511.07935', 'title': 'DiffRegCD: Integrated Registration and Change Detection with Diffusion Features', 'authors': 'Seyedehnanita Madani, Rama Chellappa, Vishal M. Patel', 'link': 'https://arxiv.org/abs/2511.07935', 'abstract': 'Change detection (CD) is fundamental to computer vision and remote sensing, supporting applications in environmental monitoring, disaster response, and urban development. Most CD models assume co-registered inputs, yet real-world imagery often exhibits parallax, viewpoint shifts, and long temporal gaps that cause severe misalignment. Traditional two stage methods that first register and then detect, as well as recent joint frameworks (e.g., BiFA, ChangeRD), still struggle under large displacements, relying on regression only flow, global homographies, or synthetic perturbations. We present DiffRegCD, an integrated framework that unifies dense registration and change detection in a single model. DiffRegCD reformulates correspondence estimation as a Gaussian smoothed classification task, achieving sub-pixel accuracy and stable training. It leverages frozen multi-scale features from a pretrained denoising diffusion model, ensuring robustness to illumination and viewpoint variation. Supervision is provided through controlled affine perturbations applied to standard CD datasets, yielding paired ground truth for both flow and change detection without pseudo labels. Extensive experiments on aerial (LEVIR-CD, DSIFN-CD, WHU-CD, SYSU-CD) and ground level (VL-CMU-CD) datasets show that DiffRegCD consistently surpasses recent baselines and remains reliable under wide temporal and geometric variation, establishing diffusion features and classification based correspondence as a strong foundation for unified change detection.', 'abstract_zh': '差分注册变化检测（DiffRegCD）：统一密集注册与变化检测的框架', 'title_zh': 'DiffRegCD: 融合扩散特征的集成注册与变化检测'}
{'arxiv_id': 'arXiv:2511.07923', 'title': 'Exploring the Underwater World Segmentation without Extra Training', 'authors': 'Bingyu Li, Tao Huo, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li', 'link': 'https://arxiv.org/abs/2511.07923', 'abstract': 'Accurate segmentation of marine organisms is vital for biodiversity monitoring and ecological assessment, yet existing datasets and models remain largely limited to terrestrial scenes. To bridge this gap, we introduce \\textbf{AquaOV255}, the first large-scale and fine-grained underwater segmentation dataset containing 255 categories and over 20K images, covering diverse categories for open-vocabulary (OV) evaluation. Furthermore, we establish the first underwater OV segmentation benchmark, \\textbf{UOVSBench}, by integrating AquaOV255 with five additional underwater datasets to enable comprehensive evaluation. Alongside, we present \\textbf{Earth2Ocean}, a training-free OV segmentation framework that transfers terrestrial vision--language models (VLMs) to underwater domains without any additional underwater training. Earth2Ocean consists of two core components: a Geometric-guided Visual Mask Generator (\\textbf{GMG}) that refines visual features via self-similarity geometric priors for local structure perception, and a Category-visual Semantic Alignment (\\textbf{CSA}) module that enhances text embeddings through multimodal large language model reasoning and scene-aware template construction. Extensive experiments on the UOVSBench benchmark demonstrate that Earth2Ocean achieves significant performance improvement on average while maintaining efficient inference.', 'abstract_zh': '准确的海洋生物分割对于生物多样性监测和生态评估至关重要，然而现有的数据集和模型主要局限于陆地场景。为弥合这一差距，我们介绍了AquaOV255，这是首个包含255个类别和超过20K张图像的大规模细粒度水下分割数据集，覆盖了开放词汇（OV）评估所需的多样类别。此外，我们通过将AquaOV255与五个额外的水下数据集整合，建立了首个水下OV分割基准UOVSBench，以实现全面的评估。与此同时，我们提出了Earth2Ocean，这是一种无需训练的OV分割框架，能够将陆地视觉-语言模型（VLMs）直接迁移到水下领域，无需额外的水下训练。Earth2Ocean 包含两个核心组件：几何引导视觉掩码生成器（GMG），通过自相似几何先验细化视觉特征以感知局部结构，以及类别视觉语义对齐（CSA）模块，通过多模态大语言模型推理和场景感知模板构建增强文本嵌入。在UOVSBench基准上的广泛实验表明，Earth2Ocean 在保持高效推理的同时，实现了显著的性能提升。', 'title_zh': '探索无需额外训练的 underwater 世界分割'}
{'arxiv_id': 'arXiv:2511.07813', 'title': 'Sparse3DPR: Training-Free 3D Hierarchical Scene Parsing and Task-Adaptive Subgraph Reasoning from Sparse RGB Views', 'authors': 'Haida Feng, Hao Wei, Zewen Xu, Haolin Wang, Chade Li, Yihong Wu', 'link': 'https://arxiv.org/abs/2511.07813', 'abstract': 'Recently, large language models (LLMs) have been explored widely for 3D scene understanding. Among them, training-free approaches are gaining attention for their flexibility and generalization over training-based methods. However, they typically struggle with accuracy and efficiency in practical deployment. To address the problems, we propose Sparse3DPR, a novel training-free framework for open-ended scene understanding, which leverages the reasoning capabilities of pre-trained LLMs and requires only sparse-view RGB inputs. Specifically, we introduce a hierarchical plane-enhanced scene graph that supports open vocabulary and adopts dominant planar structures as spatial anchors, which enables clearer reasoning chains and more reliable high-level inferences. Furthermore, we design a task-adaptive subgraph extraction method to filter query-irrelevant information dynamically, reducing contextual noise and improving 3D scene reasoning efficiency and accuracy. Experimental results demonstrate the superiority of Sparse3DPR, which achieves a 28.7% EM@1 improvement and a 78.2% speedup compared with ConceptGraphs on the Space3D-Bench. Moreover, Sparse3DPR obtains comparable performance to training-based methods on ScanQA, with additional real-world experiments confirming its robustness and generalization capability.', 'abstract_zh': '最近，大规模语言模型（LLMs）在三维场景理解领域得到了广泛探索。其中，无需训练的方法因其灵活性和泛化能力而受到关注，但它们在实际部署中通常面临准确性和效率的挑战。为解决这些问题，我们提出了Sparse3DPR，这是一种新颖的无需训练框架，用于开放式的场景理解，该框架利用预训练LLMs的推理能力，并仅需稀疏视点RGB输入。具体而言，我们引入了一种分层增强的场景图，支持开放词汇表，并采用主导的平面结构作为空间锚点，这使得推理链条更加清晰且高层推断更加可靠。此外，我们设计了一种任务自适应子图提取方法，可以动态过滤掉与查询无关的信息，减少上下文噪声，提高三维场景推理的效率和准确性。实验结果表明，Sparse3DPR在Space3D-Bench上的表现优于ConceptGraphs，实现了1EM@1改进和78.2%的加速。此外，Sparse3DPR在ScanQA上的性能与训练方法相当，额外的真实世界实验进一步证实了其 robustness 和泛化能力。', 'title_zh': 'Sparse3DPR: 无需训练的稀疏RGB视图Hierarchical场景解析和任务自适应子图推理'}
{'arxiv_id': 'arXiv:2511.07755', 'title': 'Filtered-ViT: A Robust Defense Against Multiple Adversarial Patch Attacks', 'authors': 'Aja Khanal, Ahmed Faid, Apurva Narayan', 'link': 'https://arxiv.org/abs/2511.07755', 'abstract': 'Deep learning vision systems are increasingly deployed in safety-critical domains such as healthcare, yet they remain vulnerable to small adversarial patches that can trigger misclassifications. Most existing defenses assume a single patch and fail when multiple localized disruptions occur, the type of scenario adversaries and real-world artifacts often exploit. We propose Filtered-ViT, a new vision transformer architecture that integrates SMART Vector Median Filtering (SMART-VMF), a spatially adaptive, multi-scale, robustness-aware mechanism that enables selective suppression of corrupted regions while preserving semantic detail. On ImageNet with LaVAN multi-patch attacks, Filtered-ViT achieves 79.8% clean accuracy and 46.3% robust accuracy under four simultaneous 1\\% patches, outperforming existing defenses. Beyond synthetic benchmarks, a real-world case study on radiographic medical imagery shows that Filtered-ViT mitigates natural artifacts such as occlusions and scanner noise without degrading diagnostic content. This establishes Filtered-ViT as the first transformer to demonstrate unified robustness against both adversarial and naturally occurring patch-like disruptions, charting a path toward reliable vision systems in truly high-stakes environments.', 'abstract_zh': '深度学习视觉系统在医疗等安全关键领域得到了广泛应用，但仍易受小的 adversarial 补丁攻击而导致误分类。现有的大多数防御措施假定单一补丁，当多个局部破坏同时发生时便会失效，这正是对手和现实世界中常见的利用场景。我们提出了一种名为 Filtered-ViT 的新视觉变换器架构，它整合了 SMART 空间自适应多尺度鲁棒性感知向量中值滤波（SMART-VMF），该机制能够在保留语义细节的同时选择性地抑制受污染区域。在 ImageNet 数据集上，Filtered-ViT 在同时受到四个 1% 多补丁攻击时，分别实现了 79.8% 的干净准确率和 46.3% 的鲁棒准确率，超越了现有防御措施。在真实世界案例研究中，Filtered-ViT 在放射学医疗图像上减轻了遮挡和扫描仪噪声等自然伪影，而不影响诊断内容。这使 Filtered-ViT 成为首个在对抗性和自然发生的补丁-like 干扰方面均表现出统一鲁棒性的变换器，为其在真正高风险环境中的可靠应用铺平了道路。', 'title_zh': 'Filtered-ViT: 一种针对多种 adversarial patch 攻击的稳健防御方法'}
{'arxiv_id': 'arXiv:2511.07743', 'title': 'UltraGS: Gaussian Splatting for Ultrasound Novel View Synthesis', 'authors': 'Yuezhe Yang, Wenjie Cai, Dexin Yang, Yufang Dong, Xingbo Dong, Zhe Jin', 'link': 'https://arxiv.org/abs/2511.07743', 'abstract': "Ultrasound imaging is a cornerstone of non-invasive clinical diagnostics, yet its limited field of view complicates novel view synthesis. We propose \\textbf{UltraGS}, a Gaussian Splatting framework optimized for ultrasound imaging. First, we introduce a depth-aware Gaussian splatting strategy, where each Gaussian is assigned a learnable field of view, enabling accurate depth prediction and precise structural representation. Second, we design SH-DARS, a lightweight rendering function combining low-order spherical harmonics with ultrasound-specific wave physics, including depth attenuation, reflection, and scattering, to model tissue intensity accurately. Third, we contribute the Clinical Ultrasound Examination Dataset, a benchmark capturing diverse anatomical scans under real-world clinical protocols. Extensive experiments on three datasets demonstrate UltraGS's superiority, achieving state-of-the-art results in PSNR (up to 29.55), SSIM (up to 0.89), and MSE (as low as 0.002) while enabling real-time synthesis at 64.69 fps. The code and dataset are open-sourced at: this https URL.", 'abstract_zh': 'UltraGS：一种用于超声成像的高斯点扩散框架', 'title_zh': 'UltraGS: Gaussian Splatting for Ultrasound Novel View Synthesis'}
{'arxiv_id': 'arXiv:2511.07499', 'title': 'Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance', 'authors': 'Kwanyoung Kim', 'link': 'https://arxiv.org/abs/2511.07499', 'abstract': 'Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.', 'abstract_zh': 'adversarial sinkhorn attention guidance for enhancing diffusion models', 'title_zh': '基于对抗Sinkhorn注意引导的可靠扩散采样前沿探索'}
{'arxiv_id': 'arXiv:2511.07479', 'title': 'Modulo Video Recovery via Selective Spatiotemporal Vision Transformer', 'authors': 'Tianyu Geng, Feng Ji, Wee Peng Tay', 'link': 'https://arxiv.org/abs/2511.07479', 'abstract': 'Conventional image sensors have limited dynamic range, causing saturation in high-dynamic-range (HDR) scenes. Modulo cameras address this by folding incident irradiance into a bounded range, yet require specialized unwrapping algorithms to reconstruct the underlying signal. Unlike HDR recovery, which extends dynamic range from conventional sampling, modulo recovery restores actual values from folded samples. Despite being introduced over a decade ago, progress in modulo image recovery has been slow, especially in the use of modern deep learning techniques. In this work, we demonstrate that standard HDR methods are unsuitable for modulo recovery. Transformers, however, can capture global dependencies and spatial-temporal relationships crucial for resolving folded video frames. Still, adapting existing Transformer architectures for modulo recovery demands novel techniques. To this end, we present Selective Spatiotemporal Vision Transformer (SSViT), the first deep learning framework for modulo video reconstruction. SSViT employs a token selection strategy to improve efficiency and concentrate on the most critical regions. Experiments confirm that SSViT produces high-quality reconstructions from 8-bit folded videos and achieves state-of-the-art performance in modulo video recovery.', 'abstract_zh': '传统的图像传感器动态范围有限，导致在高动态范围(HDR)场景中出现饱和现象。模数相机通过将入射辐照度折叠到一个限定范围内来解决这一问题，但需要专门的解裹算法来重建底层信号。与扩展传统采样动态范围的HDR恢复不同，模数恢复恢复的是从折叠样本中实际恢复的值。尽管十多年前就已经提出，但在使用现代深度学习技术方面，模数图像恢复的进步仍然缓慢。在本文中，我们证明了标准HDR方法不适用于模数恢复。然而，变压器可以捕捉到解决折叠视频帧所需的重要全局依赖关系和空时关系。尽管如此，将现有的变压器架构适应于模数恢复仍需要新颖的技术。为此，我们提出了选择性时空视觉变压器（SSViT），这是第一个用于模数视频重构的深度学习框架。SSViT采用token选择策略以提高效率并集中于最关键的区域。实验结果证实，SSViT能够从8位折叠视频中生成高质量的重构，并在模数视频恢复中实现了领先的性能。', 'title_zh': '基于选择性空间时间视觉变换器的模态视频恢复'}
