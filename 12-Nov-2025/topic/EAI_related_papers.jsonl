{'arxiv_id': 'arXiv:2511.08583', 'title': 'SeFA-Policy: Fast and Accurate Visuomotor Policy Learning with Selective Flow Alignment', 'authors': 'Rong Xue, Jiageng Mao, Mingtong Zhang, Yue Wang', 'link': 'https://arxiv.org/abs/2511.08583', 'abstract': 'Developing efficient and accurate visuomotor policies poses a central challenge in robotic imitation learning. While recent rectified flow approaches have advanced visuomotor policy learning, they suffer from a key limitation: After iterative distillation, generated actions may deviate from the ground-truth actions corresponding to the current visual observation, leading to accumulated error as the reflow process repeats and unstable task execution. We present Selective Flow Alignment (SeFA), an efficient and accurate visuomotor policy learning framework. SeFA resolves this challenge by a selective flow alignment strategy, which leverages expert demonstrations to selectively correct generated actions and restore consistency with observations, while preserving multimodality. This design introduces a consistency correction mechanism that ensures generated actions remain observation-aligned without sacrificing the efficiency of one-step flow inference. Extensive experiments across both simulated and real-world manipulation tasks show that SeFA Policy surpasses state-of-the-art diffusion-based and flow-based policies, achieving superior accuracy and robustness while reducing inference latency by over 98%. By unifying rectified flow efficiency with observation-consistent action generation, SeFA provides a scalable and dependable solution for real-time visuomotor policy learning. Code is available on this https URL.', 'abstract_zh': '开发高效准确的视听运动策略是机器人模仿学习中的核心挑战。虽然近期的修正流方法已经提升了视听运动策略的学习，但它们存在一个关键限制：在迭代精炼过程中，生成的动作可能偏离当前视觉观察对应的真实动作，导致在反复修正过程中累积误差并使任务执行变得不稳定。我们提出了选择性流对齐（SeFA），一种高效准确的视听运动策略学习框架。SeFA 通过一种选择性流对齐策略来解决这一挑战，该策略利用专家演示来选择性地纠正生成的动作，恢复与观察的一致性，同时保持多模态性。该设计引入了一致性纠正机制，确保生成的动作保持与观察的一致性，而不牺牲单步流推理的效率。在模拟和真实世界操作任务的广泛实验中，SeFA 策略超过了最先进的扩散模型和流模型策略，在准确性和鲁棒性方面表现出色，并将推理延迟降低了超过98%。通过对齐修正流效率性和观测一致动作生成，SeFA 提供了一种可扩展且可靠的实时视听运动策略学习解决方案。代码可在以下链接获取：this https URL。', 'title_zh': 'SeFA策略：快速准确的选择性流对齐视觉运动策略学习'}
{'arxiv_id': 'arXiv:2511.08454', 'title': 'Intuitive control of supernumerary robotic limbs through a tactile-encoded neural interface', 'authors': 'Tianyu Jia, Xingchen Yang, Ciaran McGeady, Yifeng Li, Jinzhi Lin, Kit San Ho, Feiyu Pan, Linhong Ji, Chong Li, Dario Farina', 'link': 'https://arxiv.org/abs/2511.08454', 'abstract': 'Brain-computer interfaces (BCIs) promise to extend human movement capabilities by enabling direct neural control of supernumerary effectors, yet integrating augmented commands with multiple degrees of freedom without disrupting natural movement remains a key challenge. Here, we propose a tactile-encoded BCI that leverages sensory afferents through a novel tactile-evoked P300 paradigm, allowing intuitive and reliable decoding of supernumerary motor intentions even when superimposed with voluntary actions. The interface was evaluated in a multi-day experiment comprising of a single motor recognition task to validate baseline BCI performance and a dual task paradigm to assess the potential influence between the BCI and natural human movement. The brain interface achieved real-time and reliable decoding of four supernumerary degrees of freedom, with significant performance improvements after only three days of training. Importantly, after training, performance did not differ significantly between the single- and dual-BCI task conditions, and natural movement remained unimpaired during concurrent supernumerary control. Lastly, the interface was deployed in a movement augmentation task, demonstrating its ability to command two supernumerary robotic arms for functional assistance during bimanual tasks. These results establish a new neural interface paradigm for movement augmentation through stimulation of sensory afferents, expanding motor degrees of freedom without impairing natural movement.', 'abstract_zh': '触觉编码脑-机接口：通过刺激感觉传入神经元扩展运动自由度而不干扰自然运动', 'title_zh': '通过触觉编码神经接口直观控制额外 robotic 臂'}
{'arxiv_id': 'arXiv:2511.08377', 'title': 'Human Motion Intent Inferencing in Teleoperation Through a SINDy Paradigm', 'authors': 'Michael Bowman, Xiaoli Zhang', 'link': 'https://arxiv.org/abs/2511.08377', 'abstract': "Intent inferencing in teleoperation has been instrumental in aligning operator goals and coordinating actions with robotic partners. However, current intent inference methods often ignore subtle motion that can be strong indicators for a sudden change in intent. Specifically, we aim to tackle 1) if we can detect sudden jumps in operator trajectories, 2) how we appropriately use these sudden jump motions to infer an operator's goal state, and 3) how to incorporate these discontinuous and continuous dynamics to infer operator motion. Our framework, called Psychic, models these small indicative motions through a jump-drift-diffusion stochastic differential equation to cover discontinuous and continuous dynamics. Kramers-Moyal (KM) coefficients allow us to detect jumps with a trajectory which we pair with a statistical outlier detection algorithm to nominate goal transitions. Through identifying jumps, we can perform early detection of existing goals and discover undefined goals in unstructured scenarios. Our framework then applies a Sparse Identification of Nonlinear Dynamics (SINDy) model using KM coefficients with the goal transitions as a control input to infer an operator's motion behavior in unstructured scenarios. We demonstrate Psychic can produce probabilistic reachability sets and compare our strategy to a negative log-likelihood model fit. We perform a retrospective study on 600 operator trajectories in a hands-free teleoperation task to evaluate the efficacy of our opensource package, Psychic, in both offline and online learning.", 'abstract_zh': '基于跳跃-漂移-扩散过程的心理推理在远程操控中的意图推断通过建模操作员轨迹中的细微指示性运动，以突显动作来调整操作员目标并协调与机器人的动作。然而，当前的意图推理方法往往忽略了可能是意图突变强烈指示的细微运动。具体而言，我们旨在解决以下三个问题：1) 是否能够检测操作员轨迹中的突变跳跃；2) 如何适当地利用这些突变跳跃来推断操作员的目标状态；3) 如何结合断续和连续动力学来推断操作员的运动。我们提出的一种框架Psychic，通过跳跃-漂移-扩散随机微分方程建模这些细微指示性运动，以涵盖断续和连续动力学。Kramers-Moyal (KM) 系数使我们能够检测轨迹中的跳跃，并将其与统计异常检测算法配对以提出目标转换。通过识别跳跃，我们可以提前检测现有目标并在非结构化场景中发现未定义的目标。然后，我们的框架使用KM系数和目标转换作为控制输入，通过Sparse Identification of Nonlinear Dynamics (SINDy) 模型来推断操作员在非结构化场景中的运动行为。我们展示了Psychic能够生成可达性集合，并将我们的策略与负对数似然模型进行比较。我们对600个操作员轨迹进行回顾性研究，以评估我们开源包Psychic在离线和在线学习中的有效性。', 'title_zh': '通过SINDy范式在远程操作中推断人类运动意图'}
{'arxiv_id': 'arXiv:2511.08299', 'title': 'Learning Omnidirectional Locomotion for a Salamander-Like Quadruped Robot', 'authors': 'Zhiang Liu, Yang Liu, Yongchun Fang, Xian Guo', 'link': 'https://arxiv.org/abs/2511.08299', 'abstract': 'Salamander-like quadruped robots are designed inspired by the skeletal structure of their biological counterparts. However, existing controllers cannot fully exploit these morphological features and largely rely on predefined gait patterns or joint trajectories, which prevents the generation of diverse and flexible locomotion and limits their applicability in real-world scenarios. In this paper, we propose a learning framework that enables the robot to acquire a diverse repertoire of omnidirectional gaits without reference motions. Each body part is controlled by a phase variable capable of forward and backward evolution, with a phase coverage reward to promote the exploration of the leg phase space. Additionally, morphological symmetry of the robot is incorporated via data augmentation, improving sample efficiency and enforcing both motion-level and task-level symmetry in learned behaviors. Extensive experiments show that the robot successfully acquires 22 omnidirectional gaits exhibiting both dynamic and symmetric movements, demonstrating the effectiveness of the proposed learning framework.', 'abstract_zh': '类似于蝾螈的四足机器人设计灵感来源于其生物对应物的骨骼结构。然而，现有的控制器无法充分利用这些形态特征，主要依赖预定义的步伐模式或关节轨迹，这限制了它们生成多样化和灵活运动的能力，并限制了其在实际场景中的应用。在本文中，我们提出了一种学习框架，使机器人能够在没有参考运动的情况下获得 diverse 的全方位步伐。每个身体部分都由一个相位变量控制，该变量能够进行正反向演化，并通过相位覆盖奖励促进腿部相空间的探索。此外，通过数据增强将机器人形态对称性融入其中，提高样本效率并确保在学习行为中实现运动级别和任务级别的对称性。大量实验表明，该机器人成功获得了 22 种全方位步伐，展示了动态和对称运动的有效性，证明了所提出学习框架的有效性。', 'title_zh': '仿蝾螈 quadruped 机器人全方位移动学习'}
{'arxiv_id': 'arXiv:2511.08277', 'title': 'X-IONet: Cross-Platform Inertial Odometry Network with Dual-Stage Attention', 'authors': 'Dehan Shen, Changhao Chen', 'link': 'https://arxiv.org/abs/2511.08277', 'abstract': 'Learning-based inertial odometry has achieved remarkable progress in pedestrian navigation. However, extending these methods to quadruped robots remains challenging due to their distinct and highly dynamic motion patterns. Models that perform well on pedestrian data often experience severe degradation when deployed on legged platforms. To tackle this challenge, we introduce X-IONet, a cross-platform inertial odometry framework that operates solely using a single Inertial Measurement Unit (IMU). X-IONet incorporates a rule-based expert selection module to classify motion platforms and route IMU sequences to platform-specific expert networks. The displacement prediction network features a dual-stage attention architecture that jointly models long-range temporal dependencies and inter-axis correlations, enabling accurate motion representation. It outputs both displacement and associated uncertainty, which are further fused through an Extended Kalman Filter (EKF) for robust state estimation. Extensive experiments on public pedestrian datasets and a self-collected quadruped robot dataset demonstrate that X-IONet achieves state-of-the-art performance, reducing Absolute Trajectory Error (ATE) by 14.3% and Relative Trajectory Error (RTE) by 11.4% on pedestrian data, and by 52.8% and 41.3% on quadruped robot data. These results highlight the effectiveness of X-IONet in advancing accurate and robust inertial navigation across both human and legged robot platforms.', 'abstract_zh': '基于学习的惯性里程计在行人导航中取得了显著进步。然而，将这些方法扩展到四足机器人上仍然具有挑战性，因为它们具有独特的高度动态运动模式。适用于行人数据的模型在部署到多足平台时往往会遇到严重的性能下降。为了应对这一挑战，我们提出了一种跨平台惯性里程计框架X-IONet，该框架仅使用单个惯性测量单元（IMU）进行操作。X-IONet包含一个基于规则的专家选择模块，用于分类运动平台并路由IMU序列到特定平台的专家网络。位移预测网络采用双重注意力架构，同时建模长时序依赖性和轴间相关性，从而实现准确的运动表示。它输出位移及其相关不确定性，并通过扩展卡尔曼滤波器（EKF）融合以实现稳健的状态估计。在公共行人数据集和自收集的四足机器人数据集上的广泛实验表明，X-IONet达到了最先进的性能，行人数据中的绝对轨迹误差（ATE）减少了14.3%，相对轨迹误差（RTE）减少了11.4%，四足机器人数据中的这些误差分别减少了52.8%和41.3%。这些结果突显了X-IONet在促进跨人类和多足机器人平台准确且稳健的惯性导航方面的有效性。', 'title_zh': 'X-IONet：跨平台惯性里程计网络带双重注意机制'}
{'arxiv_id': 'arXiv:2511.08214', 'title': 'Prioritizing Perception-Guided Self-Supervision: A New Paradigm for Causal Modeling in End-to-End Autonomous Driving', 'authors': 'Yi Huang, Zhan Qu, Lihui Jiang, Bingbing Liu, Hongbo Zhang', 'link': 'https://arxiv.org/abs/2511.08214', 'abstract': 'End-to-end autonomous driving systems, predominantly trained through imitation learning, have demonstrated considerable effectiveness in leveraging large-scale expert driving data. Despite their success in open-loop evaluations, these systems often exhibit significant performance degradation in closed-loop scenarios due to causal confusion. This confusion is fundamentally exacerbated by the overreliance of the imitation learning paradigm on expert trajectories, which often contain unattributable noise and interfere with the modeling of causal relationships between environmental contexts and appropriate driving actions.\nTo address this fundamental limitation, we propose Perception-Guided Self-Supervision (PGS) - a simple yet effective training paradigm that leverages perception outputs as the primary supervisory signals, explicitly modeling causal relationships in decision-making. The proposed framework aligns both the inputs and outputs of the decision-making module with perception results, such as lane centerlines and the predicted motions of surrounding agents, by introducing positive and negative self-supervision for the ego trajectory. This alignment is specifically designed to mitigate causal confusion arising from the inherent noise in expert trajectories.\nEquipped with perception-driven supervision, our method, built on a standard end-to-end architecture, achieves a Driving Score of 78.08 and a mean success rate of 48.64% on the challenging closed-loop Bench2Drive benchmark, significantly outperforming existing state-of-the-art methods, including those employing more complex network architectures and inference pipelines. These results underscore the effectiveness and robustness of the proposed PGS framework and point to a promising direction for addressing causal confusion and enhancing real-world generalization in autonomous driving.', 'abstract_zh': '端到端自动驾驶系统——基于感知引导的自监督训练范式在利用大量专家驾驶数据进行主要训练方面展示了显著效果。尽管这些系统在开环评估中表现出色，但在闭环场景中往往因为因果混淆而大幅性能下降。这种混淆从根本上说是由于模仿学习范式过度依赖专家轨迹所致，而这些轨迹中往往包含不可归因的噪声，干扰了对环境上下文与合适驾驶动作之间因果关系的建模。\n\n为解决这一根本性限制，我们提出了一种名为感知引导的自监督（Perception-Guided Self-Supervision, PGS）的简单而有效的训练范式，该范式利用感知输出作为主要的监督信号，明确建模决策中的因果关系。所提出的框架将决策模块的输入和输出与感知结果（如车道中心线和周围代理的预测运动）对齐，通过引入对自我轨迹的正负自监督来实现这一点。这种对齐设计旨在减轻由于专家轨迹固有噪声引起的原因混淆。\n\n利用感知驱动的监督，基于标准端到端架构的方法在具有挑战性的闭环Bench2Drive基准测试中获得了78.08的驾驶得分和48.64%的平均成功率，显著优于现有最先进的方法，包括那些采用更复杂网络架构和推理管道的方法。这些结果突显了所提出的PGS框架的有效性和稳健性，并指出了解决因果混淆和增强自动驾驶真实世界泛化的有前途的方向。', 'title_zh': '基于感知指导的自监督优先：端到端自动驾驶中的因果建模新范式'}
{'arxiv_id': 'arXiv:2511.08098', 'title': 'PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision', 'authors': 'Sabrina Patania, Luca Annese, Anita Pellegrini, Silvia Serino, Anna Lambiase, Luca Pallonetto, Silvia Rossi, Simone Colombani, Tom Foulsham, Azzurra Ruggeri, Dimitri Ognibene', 'link': 'https://arxiv.org/abs/2511.08098', 'abstract': "Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.", 'abstract_zh': 'Recent Advances in Large Language Models (LLMs) and Multimodal Foundation Models Have Significantly Broadened Their Application in Robotics and Collaborative Systems: Evaluating Explicit Incorporation of Diverse Points of View Using the ReAct Framework', 'title_zh': 'PerspAct: 通过换位思考和主动视觉提升LLM情境协作能力'}
{'arxiv_id': 'arXiv:2511.07887', 'title': 'EquiMus: Energy-Equivalent Dynamic Modeling and Simulation of Musculoskeletal Robots Driven by Linear Elastic Actuators', 'authors': 'Yinglei Zhu, Xuguang Dong, Qiyao Wang, Qi Shao, Fugui Xie, Xinjun Liu, Huichan Zhao', 'link': 'https://arxiv.org/abs/2511.07887', 'abstract': "Dynamic modeling and control are critical for unleashing soft robots' potential, yet remain challenging due to their complex constitutive behaviors and real-world operating conditions. Bio-inspired musculoskeletal robots, which integrate rigid skeletons with soft actuators, combine high load-bearing capacity with inherent flexibility. Although actuation dynamics have been studied through experimental methods and surrogate models, accurate and effective modeling and simulation remain a significant challenge, especially for large-scale hybrid rigid--soft robots with continuously distributed mass, kinematic loops, and diverse motion modes. To address these challenges, we propose EquiMus, an energy-equivalent dynamic modeling framework and MuJoCo-based simulation for musculoskeletal rigid--soft hybrid robots with linear elastic actuators. The equivalence and effectiveness of the proposed approach are validated and examined through both simulations and real-world experiments on a bionic robotic leg. EquiMus further demonstrates its utility for downstream tasks, including controller design and learning-based control strategies.", 'abstract_zh': '生物启发的肌骨骼软体机器人的能量等效动态建模与控制：针对大规模混合刚柔机器人的挑战', 'title_zh': '等效动力学建模与仿真：由线性弹性驱动器驱动的肌骨骼机器人'}
{'arxiv_id': 'arXiv:2511.07820', 'title': 'SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control', 'authors': 'Zhengyi Luo, Ye Yuan, Tingwu Wang, Chenran Li, Sirui Chen, Fernando Castañeda, Zi-Ang Cao, Jiefeng Li, David Minor, Qingwei Ben, Xingye Da, Runyu Ding, Cyrus Hogg, Lina Song, Edy Lim, Eugene Jeong, Tairan He, Haoru Xue, Wenli Xiao, Zi Wang, Simon Yuen, Jan Kautz, Yan Chang, Umar Iqbal, Linxi "Jim" Fan, Yuke Zhu', 'link': 'https://arxiv.org/abs/2511.07820', 'abstract': 'Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited behavior set, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leverageing dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.', 'abstract_zh': '尽管数十亿参数的基础模型在成千上万块GPU上进行训练，但在类人机器人控制方面尚未显示出类似的扩展增益。当前用于类人机器人的神经控制器规模仍然较小，仅针对有限的行为集，并且通常在几天内由少量GPU进行训练。我们证明，通过增加模型容量、数据和计算资源，可以实现一种通用的类人机器人控制器，能够生成自然且稳健的整体运动。具体而言，我们将运动跟踪视为类人机器人控制的自然且可扩展的任务，利用多样化的运动捕捉数据的密集监督来获取人类运动先验，无需手动奖励工程。我们通过在三个维度上进行扩展来构建运动跟踪的基础模型：网络规模（从1.2M到42M参数）、数据集体积（超过100M帧，700小时的高质量运动数据）和计算资源（9k GPU小时）。除了展示规模的优势之外，我们还通过两种机制展示了模型的实际应用价值：（1）实时通用运动规划器，将运动跟踪与下游任务执行相连，实现自然交互控制；（2）统一的标记空间，支持多种运动输入接口，如VR远程操作装置、人类视频以及视觉-语言-动作（VLA）模型，使用相同的策略。运动跟踪的扩展表现出有利的特性：随着计算能力和数据多样性的增加，性能逐步提升，并且学习到的表示能够泛化到未见过的运动，从而将运动跟踪扩展作为类人机器人控制的实际基础。', 'title_zh': 'SONIC: 超级化运动跟踪以实现自然人形全身控制'}
{'arxiv_id': 'arXiv:2511.07811', 'title': 'Virtual Traffic Lights for Multi-Robot Navigation: Decentralized Planning with Centralized Conflict Resolution', 'authors': 'Sagar Gupta, Thanh Vinh Nguyen, Thieu Long Phan, Vidul Attri, Archit Gupta, Niroshinie Fernando, Kevin Lee, Seng W. Loke, Ronny Kutadinata, Benjamin Champion, Akansel Cosgun', 'link': 'https://arxiv.org/abs/2511.07811', 'abstract': 'We present a hybrid multi-robot coordination framework that combines decentralized path planning with centralized conflict resolution. In our approach, each robot autonomously plans its path and shares this information with a centralized node. The centralized system detects potential conflicts and allows only one of the conflicting robots to proceed at a time, instructing others to stop outside the conflicting area to avoid deadlocks. Unlike traditional centralized planning methods, our system does not dictate robot paths but instead provides stop commands, functioning as a virtual traffic light. In simulation experiments with multiple robots, our approach increased the success rate of robots reaching their goals while reducing deadlocks. Furthermore, we successfully validated the system in real-world experiments with two quadruped robots and separately with wheeled Duckiebots.', 'abstract_zh': '一种结合分散路径规划与集中冲突解决的混合多机器人协调框架', 'title_zh': '虚拟交通灯用于多机器人导航：去中心化规划与中心化冲突解决'}
{'arxiv_id': 'arXiv:2511.07732', 'title': 'ViPRA: Video Prediction for Robot Actions', 'authors': 'Sandeep Routray, Hengkai Pan, Unnat Jain, Shikhar Bahl, Deepak Pathak', 'link': 'https://arxiv.org/abs/2511.07732', 'abstract': 'Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at this https URL', 'abstract_zh': '我们可以将视频预测模型转化为机器人策略吗？视频，包括人类或远程操控机器人的视频，捕捉到了丰富的物理交互。然而，大多数视频缺少标注的动作，这限制了它们在机器人学习中的应用。我们提出了视频预测用于机器人动作（ViPRA）框架，这是一个简单的预训练-微调框架，可以从这些无动作视频中学习连续的机器人控制。我们不是直接预测动作，而是训练一个视频-语言模型来预测未来的视觉观察和以运动为中心的潜在动作，这些潜在动作作为场景动态的中间表示。我们使用感知损失和光流一致性来训练这些潜在动作，以确保它们反映了物理基础的行为。为了下游控制，我们引入了一种分块光流匹配解码器，它将潜在动作映射到机器人特定的连续动作序列，仅需100到200个远程操控演示。这种方法避免了昂贵的动作标注，支持跨身体泛化，并通过分块动作解码实现了高达22 Hz的平滑、高频连续控制。与之前将预训练视为自回归策略学习的潜在动作工作不同，我们的方法明确地模型了什么在变化以及如何变化。该方法在SIMPLER基准测试中表现优于强基线，获得了16%的提高，并且在真实世界操作任务中提高了13%。我们将在该网址发布模型和代码：this https URL。', 'title_zh': 'ViPRA：机器人动作的视频预测'}
{'arxiv_id': 'arXiv:2511.07727', 'title': 'LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models', 'authors': 'Xiaohan Zhang, Yan Ding, Yohei Hayamizu, Zainab Altaweel, Yifeng Zhu, Yuke Zhu, Peter Stone, Chris Paxton, Shiqi Zhang', 'link': 'https://arxiv.org/abs/2511.07727', 'abstract': "Task planning and motion planning are two of the most important problems in robotics, where task planning methods help robots achieve high-level goals and motion planning methods maintain low-level feasibility. Task and motion planning (TAMP) methods interleave the two processes of task planning and motion planning to ensure goal achievement and motion feasibility. Within the TAMP context, we are concerned with the mobile manipulation (MoMa) of multiple objects, where it is necessary to interleave actions for navigation and manipulation.\nIn particular, we aim to compute where and how each object should be placed given underspecified goals, such as ``set up dinner table with a fork, knife and plate.'' We leverage the rich common sense knowledge from large language models (LLMs), e.g., about how tableware is organized, to facilitate both task-level and motion-level planning. In addition, we use computer vision methods to learn a strategy for selecting base positions to facilitate MoMa behaviors, where the base position corresponds to the robot's ``footprint'' and orientation in its operating space. Altogether, this article provides a principled TAMP framework for MoMa tasks that accounts for common sense about object rearrangement and is adaptive to novel situations that include many objects that need to be moved. We performed quantitative experiments in both real-world settings and simulated environments. We evaluated the success rate and efficiency in completing long-horizon object rearrangement tasks. While the robot completed 84.4\\% real-world object rearrangement trials, subjective human evaluations indicated that the robot's performance is still lower than experienced human waiters.", 'abstract_zh': '基于任务与运动规划的方法进行多重操作 manipulatioN (MoMa) 的规划：利用大规模语言模型的常识知识', 'title_zh': 'LLM-GROP：基于视觉的机器人任务与运动规划'}
{'arxiv_id': 'arXiv:2511.07720', 'title': 'A QP Framework for Improving Data Collection: Quantifying Device-Controller Performance in Robot Teleoperation', 'authors': 'Yuxuan Zhao, Yuanchen Tang, Jindi Zhang, Hongyu Yu', 'link': 'https://arxiv.org/abs/2511.07720', 'abstract': "Robot learning empowers the robot system with human brain-like intelligence to autonomously acquire and adapt skills through experience, enhancing flexibility and adaptability in various environments. Aimed at achieving a similar level of capability in large language models (LLMs) for embodied intelligence, data quality plays a crucial role in training a foundational model with diverse robot skills. In this study, we investigate the collection of data for manipulation tasks using teleoperation devices. Different devices yield varying effects when paired with corresponding controller strategies, including position-based inverse kinematics (IK) control, torque-based inverse dynamics (ID) control, and optimization-based compliance control. In this paper, we develop a teleoperation pipeline that is compatible with different teleoperation devices and manipulator controllers. Within the pipeline, we construct the optimal QP formulation with the dynamic nullspace and the impedance tracking as the novel optimal controller to achieve compliant pose tracking and singularity avoidance. Regarding the optimal controller, it adaptively adjusts the weights assignment depending on the robot joint manipulability that reflects the state of joint configuration for the pose tracking in the form of impedance control and singularity avoidance with nullspace tracking. Analysis of quantitative experimental results suggests the quality of the teleoperated trajectory data, including tracking error, occurrence of singularity, and the smoothness of the joints' trajectory, with different combinations of teleoperation interface and the motion controller.", 'abstract_zh': '机器人学习赋予机器人系统类似人脑的智能，通过经验自主获取和适应技能，增强在各种环境中的灵活性和适应性。为了在具备大规模语言模型（LLMs）类似水平的体现智能时，数据质量在训练具有多种机器人技能的基础模型中起着关键作用。在本研究中，我们探讨了使用遥操作设备收集操作任务数据的方法。不同的设备与相应的控制器策略相配时会产生不同的效果，包括基于位置的逆动力学（IK）控制、基于扭矩的逆动力学（ID）控制和基于优化的顺应性控制。在本文中，我们开发了一种兼容不同遥操作设备和操作器控制器的遥操作管道。在该管道中，我们构建了结合动态零空间和阻抗跟踪的新颖最优控制器，以实现顺应性姿态跟踪和奇异点避免。关于最优控制器，它根据反映姿态跟踪状态下关节配置状态的关节可操作性，自适应调整权重分配，以进行阻抗控制和奇异点避免的零空间跟踪。定量实验结果的分析表明，不同的遥操作界面和动作控制器组合对于遥操作系统轨迹数据的质量，包括跟踪误差、奇异点的出现次数和关节轨迹的平滑度的影响。', 'title_zh': '一种QP框架以改进数据采集：量化机器人远程操作中设备-控制器性能'}
{'arxiv_id': 'arXiv:2511.07717', 'title': 'RoboTAG: End-to-end Robot Configuration Estimation via Topological Alignment Graph', 'authors': 'Yifan Liu, Fangneng Zhan, Wanhua Li, Haowen Sun, Katerina Fragkiadaki, Hanspeter Pfister', 'link': 'https://arxiv.org/abs/2511.07717', 'abstract': 'Estimating robot pose from a monocular RGB image is a challenge in robotics and computer vision. Existing methods typically build networks on top of 2D visual backbones and depend heavily on labeled data for training, which is often scarce in real-world scenarios, causing a sim-to-real gap. Moreover, these approaches reduce the 3D-based problem to 2D domain, neglecting the 3D priors. To address these, we propose Robot Topological Alignment Graph (RoboTAG), which incorporates a 3D branch to inject 3D priors while enabling co-evolution of the 2D and 3D representations, alleviating the reliance on labels. Specifically, the RoboTAG consists of a 3D branch and a 2D branch, where nodes represent the states of the camera and robot system, and edges capture the dependencies between these variables or denote alignments between them. Closed loops are then defined in the graph, on which a consistency supervision across branches can be applied. This design allows us to utilize in-the-wild images as training data without annotations. Experimental results demonstrate that our method is effective across robot types, highlighting its potential to alleviate the data bottleneck in robotics.', 'abstract_zh': '从单目RGB图像估计机器人姿态是机器人技术和计算机视觉领域的一项挑战。现有的方法通常基于2D视觉骨干构建网络，并且在训练时高度依赖标记数据，而在实际场景中，标记数据往往稀缺，导致模拟到现实的差距。此外，这些方法将基于3D的问题简化为2D领域，忽略了3D先验知识。为了解决这些问题，我们提出了机器人拓扑对齐图（RoboTAG），该方法引入3D分支以注入3D先验知识，并使2D和3D表示共同进化，减少对标签的依赖。具体而言，RoboTAG由3D分支和2D分支组成，节点代表相机和机器人系统的状态，边捕捉这些变量之间的依赖关系或表示它们之间的对齐。然后在图中定义闭合回路，在分支之间可以应用一致性监督。这种设计使我们能够利用未标记的图像作为训练数据。实验结果表明，我们的方法在不同类型的机器人上都有效，突显了其在机器人领域缓解数据瓶颈的潜力。', 'title_zh': 'RoboTAG：基于拓扑对齐图的端到端机器人配置估计'}
{'arxiv_id': 'arXiv:2511.07654', 'title': 'Time-Aware Policy Learning for Adaptive and Punctual Robot Control', 'authors': 'Yinsen Jia, Boyuan Chen', 'link': 'https://arxiv.org/abs/2511.07654', 'abstract': 'Temporal awareness underlies intelligent behavior in both animals and humans, guiding how actions are sequenced, paced, and adapted to changing goals and environments. Yet most robot learning algorithms remain blind to time. We introduce time-aware policy learning, a reinforcement learning framework that enables robots to explicitly perceive and reason with time as a first-class variable. The framework augments conventional reinforcement policies with two complementary temporal signals, the remaining time and a time ratio, which allow a single policy to modulate its behavior continuously from rapid and dynamic to cautious and precise execution. By jointly optimizing punctuality and stability, the robot learns to balance efficiency, robustness, resiliency, and punctuality without re-training or reward adjustment. Across diverse manipulation domains from long-horizon pick and place, to granular-media pouring, articulated-object handling, and multi-agent object delivery, the time-aware policy produces adaptive behaviors that outperform standard reinforcement learning baselines by up to 48% in efficiency, 8 times more robust in sim-to-real transfer, and 90% in acoustic quietness while maintaining near-perfect success rates. Explicit temporal reasoning further enables real-time human-in-the-loop control and multi-agent coordination, allowing robots to recover from disturbances, re-synchronize after delays, and align motion tempo with human intent. By treating time not as a constraint but as a controllable dimension of behavior, time-aware policy learning provides a unified foundation for efficient, robust, resilient, and human-aligned robot autonomy.', 'abstract_zh': '时间意识是动物和人类智能行为的基础，指导动作的顺序、节奏及其在目标和环境变化时的适应。然而，大多数机器人学习算法仍无视时间。我们提出了时间意识策略学习，这是一种强化学习框架，使机器人能够明确感知和利用时间作为首要变量。该框架通过添加两个互补的时间信号——剩余时间和时间比例——扩展了传统强化策略，使单一策略能够从快速动态到谨慎精确地连续调节其行为。通过同时优化准时性和稳定性，机器人能够在无需重新训练或调整奖励的情况下学习平衡效率、鲁棒性、恢复能力和准时性。在从长时序拾放、颗粒介质倾倒、柔性物体处理到多机器人物体传递等多个操作领域中，时间意识策略产生了超越标准强化学习基线的行为，在效率上高出最多48%，在从仿真到现实的转移鲁棒性上高出8倍，在声学安静性上高出90%的同时保持接近完美的成功率。明确的时间推理还 enables 实时人机闭环控制和多机器人协调，使机器人能够从干扰中恢复、在延误后重新同步，并与人类意图同步动作节奏。通过将时间视为行为的可控维度而非约束，时间意识策略学习为高效、鲁棒、恢复和与人类对齐的机器人自主性提供了一个统一的基础。', 'title_zh': '时间感知策略学习以实现适应性和及时的机器人控制'}
{'arxiv_id': 'arXiv:2511.07619', 'title': 'CAVER: Curious Audiovisual Exploring Robot', 'authors': 'Luca Macesanu, Boueny Folefack, Samik Singh, Ruchira Ray, Ben Abbatematteo, Roberto Martín-Martín', 'link': 'https://arxiv.org/abs/2511.07619', 'abstract': "Multimodal audiovisual perception can enable new avenues for robotic manipulation, from better material classification to the imitation of demonstrations for which only audio signals are available (e.g., playing a tune by ear). However, to unlock such multimodal potential, robots need to learn the correlations between an object's visual appearance and the sound it generates when they interact with it. Such an active sensorimotor experience requires new interaction capabilities, representations, and exploration methods to guide the robot in efficiently building increasingly rich audiovisual knowledge. In this work, we present CAVER, a novel robot that builds and utilizes rich audiovisual representations of objects. CAVER includes three novel contributions: 1) a novel 3D printed end-effector, attachable to parallel grippers, that excites objects' audio responses, 2) an audiovisual representation that combines local and global appearance information with sound features, and 3) an exploration algorithm that uses and builds the audiovisual representation in a curiosity-driven manner that prioritizes interacting with high uncertainty objects to obtain good coverage of surprising audio with fewer interactions. We demonstrate that CAVER builds rich representations in different scenarios more efficiently than several exploration baselines, and that the learned audiovisual representation leads to significant improvements in material classification and the imitation of audio-only human demonstrations. this https URL", 'abstract_zh': '多模态音频视觉感知可以为机器人操作开辟新途径，从更好的材料分类到模仿仅通过音频信号（例如，吹奏曲调）演示的示范。然而，为了释放这种多模态潜力，机器人需要学习物体的视觉外观与其互动时产生的声音之间的关联。这种主动的感觉运动经验需要新的交互能力、表示方法和探索方法，以指导机器人高效地构建日益丰富的音频视觉知识。在这项工作中，我们提出了一种名为CAVER的新机器人，用于构建和利用丰富的音频视觉表示。CAVER包括三个新颖贡献：1）一种可以连接到并行夹爪的新颖3D打印末端执行器，用于激发物体的音频响应；2）结合局部和全局视觉信息与声音特征的音频视觉表示；3）一种好奇心驱动的探索算法，该算法利用并构建音频视觉表示，优先与不确定性高的物体互动，以较少的交互获得更多的音频惊讶覆盖。我们展示了CAVER在不同场景中比几种探索基线更有效地构建丰富表示，并且学习到的音频视觉表示在材料分类和模仿仅含音频的人类示范方面带来了显着改进。', 'title_zh': '好奇听觉视觉探索机器人'}
{'arxiv_id': 'arXiv:2511.08086', 'title': 'Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks', 'authors': 'Muthukumar Pandaram, Jakob Hollenstein, David Drexel, Samuele Tosatto, Antonio Rodríguez-Sánchez, Justus Piater', 'link': 'https://arxiv.org/abs/2511.08086', 'abstract': 'The use of learned dynamics models, also known as world models, can improve the sample efficiency of reinforcement learning. Recent work suggests that the underlying causal graphs of such dynamics models are sparsely connected, with each of the future state variables depending only on a small subset of the current state variables, and that learning may therefore benefit from sparsity priors. Similarly, temporal sparsity, i.e. sparsely and abruptly changing local dynamics, has also been proposed as a useful inductive bias.\nIn this work, we critically examine these assumptions by analyzing ground-truth dynamics from a set of robotic reinforcement learning environments in the MuJoCo Playground benchmark suite, aiming to determine whether the proposed notions of state and temporal sparsity actually tend to hold in typical reinforcement learning tasks.\nWe study (i) whether the causal graphs of environment dynamics are sparse, (ii) whether such sparsity is state-dependent, and (iii) whether local system dynamics change sparsely.\nOur results indicate that global sparsity is rare, but instead the tasks show local, state-dependent sparsity in their dynamics and this sparsity exhibits distinct structures, appearing in temporally localized clusters (e.g., during contact events) and affecting specific subsets of state dimensions. These findings challenge common sparsity prior assumptions in dynamics learning, emphasizing the need for grounded inductive biases that reflect the state-dependent sparsity structure of real-world dynamics.', 'abstract_zh': '使用学习到的动力学模型（也称为世界模型）可以提高强化学习的采样效率。最近的研究表明，此类动力学模型的潜在因果图通常是稀疏连接的，每个未来状态变量仅取决于当前状态变量的一个小子集，因此学习可能受益于稀疏先验。类似地，时间稀疏性，即局部动力学稀疏且急剧变化，也被提议为有用的归纳偏置。\n\n在这项工作中，我们通过分析MuJoCo Playground基准套件中一组机器人强化学习环境的真实动力学因果图，批判性地审视这些假设，旨在确定提出的状态和时间稀疏性概念在典型强化学习任务中是否确实普遍适用。\n\n我们研究了(i) 环境动力学的因果图是否稀疏，(ii) 这种稀疏性是否与状态有关，以及(iii) 局部系统动力学是否稀疏变化。\n\n我们的结果显示，全局稀疏性很少见，但任务在动力学中表现出局部、状态相关的稀疏性，这种稀疏性具有独特的结构，在时间上局部化成簇（如在接触事件期间）并影响特定的状态维度子集。这些发现挑战了动力学习中的常见稀疏先验假设，强调了需要反映真实世界动力学的状态相关稀疏结构的扎根归纳偏置的必要性。', 'title_zh': '动态稀疏性：在机器人强化学习基准中学习世界模型的常见稀疏性假设挑战'}
{'arxiv_id': 'arXiv:2511.07730', 'title': 'Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning', 'authors': 'Bill Chunyuan Zheng, Vivek Myers, Benjamin Eysenbach, Sergey Levine', 'link': 'https://arxiv.org/abs/2511.07730', 'abstract': 'Learning how to reach goals in an environment is a longstanding challenge in AI, yet reasoning over long horizons remains a challenge for modern methods. The key question is how to estimate the temporal distance between pairs of observations. While temporal difference methods leverage local updates to provide optimality guarantees, they often perform worse than Monte Carlo methods that perform global updates (e.g., with multi-step returns), which lack such guarantees. We show how these approaches can be integrated into a practical GCRL method that fits a quasimetric distance using a multistep Monte-Carlo return. We show our method outperforms existing GCRL methods on long-horizon simulated tasks with up to 4000 steps, even with visual observations. We also demonstrate that our method can enable stitching in the real-world robotic manipulation domain (Bridge setup). Our approach is the first end-to-end GCRL method that enables multistep stitching in this real-world manipulation domain from an unlabeled offline dataset of visual observations.', 'abstract_zh': '在一个环境中学习如何达成目标是AI领域长期面临的一项挑战，尽管如此，对远期进行推理仍然是现代方法的难题。关键问题是如何估计观察对之间的时序距离。虽然时差方法通过局部更新来提供最优性保证，但它们通常在多步回报的蒙特卡洛方法（例如，多步回报）表现更差，后者缺乏这种保证。我们展示了如何将这些方法整合进一个实用的GCRL方法中，使用多步蒙特卡洛回报拟合准度量距离。我们表明，我们的方法在具有最多4000步的长时 horizons 定向仿真任务中优于现有的GCRL方法，即使在具有视觉观察的情况下也是如此。我们还展示了我们的方法如何在真实世界的机器人操作领域（Bridge setup）中实现拼接。我们的方法是第一个能够从未标注的离线视觉观察数据集中实现多步拼接的完整的GCRL方法，应用于这个真实世界的操作领域。', 'title_zh': '多步准度量学习以实现可扩展的目标条件强化学习'}
{'arxiv_id': 'arXiv:2511.08585', 'title': 'Simulating the Visual World with Artificial Intelligence: A Roadmap', 'authors': 'Jingtong Yue, Ziqi Huang, Zhaoxi Chen, Xintao Wang, Pengfei Wan, Ziwei Liu', 'link': 'https://arxiv.org/abs/2511.08585', 'abstract': 'The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a "window" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.', 'abstract_zh': '视频生成的景观正在从专注于生成令人赏心悦目的片段转向构建支持交互且保持物理可信度的虚拟环境。这些发展预示着视频基础模型的出现，这些模型不仅作为视觉生成器，还作为隐含的世界模型，模拟物理动力学、代理与环境的交互以及控制现实或想象世界的原则。本文综述了这一演变过程，将现代视频基础模型视为两个核心组件的结合：隐含的世界模型和视频渲染器。世界模型编码了关于世界的结构化知识，包括物理法则、交互动力学和代理行为。它作为潜在的仿真引擎，使连贯的视觉推理、长期时间一致性以及目标驱动的规划成为可能。视频渲染器将这一潜在仿真转换为现实的视觉观察，有效地生成视频作为“通往仿真世界”的窗口。我们通过四个阶段追溯了视频生成的进步，每个阶段的核心能力逐步提升，最终形成一个内置内在物理可信度、实时跨模态交互和多时空尺度规划能力的世界模型。对于每个阶段，我们定义其核心特征，突出代表性工作，并探讨其应用场景，如机器人技术、自动驾驶和互动游戏。最后，我们讨论了下一代世界模型面临的开放挑战和设计原则，包括代理智能在塑造和评估这些系统中的作用。相关工作列表请参阅此链接。', 'title_zh': '用人工智能模拟视觉世界：一条路线图'}
{'arxiv_id': 'arXiv:2511.08234', 'title': 'Beyond Distributions: Geometric Action Control for Continuous Reinforcement Learning', 'authors': 'Zhihao Lin', 'link': 'https://arxiv.org/abs/2511.08234', 'abstract': "Gaussian policies have dominated continuous control in deep reinforcement learning (RL), yet they suffer from a fundamental mismatch: their unbounded support requires ad-hoc squashing functions that distort the geometry of bounded action spaces. While von Mises-Fisher (vMF) distributions offer a theoretically grounded alternative on the sphere, their reliance on Bessel functions and rejection sampling hinders practical adoption. We propose \\textbf{Geometric Action Control (GAC)}, a novel action generation paradigm that preserves the geometric benefits of spherical distributions while \\textit{simplifying computation}. GAC decomposes action generation into a direction vector and a learnable concentration parameter, enabling efficient interpolation between deterministic actions and uniform spherical noise. This design reduces parameter count from \\(2d\\) to \\(d+1\\), and avoids the \\(O(dk)\\) complexity of vMF rejection sampling, achieving simple \\(O(d)\\) operations. Empirically, GAC consistently matches or exceeds state-of-the-art methods across six MuJoCo benchmarks, achieving 37.6\\% improvement over SAC on Ant-v4 and the best results on 4 out of 6 tasks. Our ablation studies reveal that both \\textbf{spherical normalization} and \\textbf{adaptive concentration control} are essential to GAC's success. These findings suggest that robust and efficient continuous control does not require complex distributions, but a principled respect for the geometry of action spaces. Code and pretrained models are available in supplementary materials.", 'abstract_zh': '几何动作控制：在连续控制中保留几何优势的同时简化计算', 'title_zh': '超越分布：几何动作控制在连续强化学习中的应用'}
{'arxiv_id': 'arXiv:2511.08172', 'title': 'An Efficient Training Pipeline for Reasoning Graphical User Interface Agents', 'authors': 'Georgios Pantazopoulos, Eda B. Özyiğit', 'link': 'https://arxiv.org/abs/2511.08172', 'abstract': 'Visual grounding is the task of localising image regions from natural language queries and is critical for reasoning capable Graphical User Interface agents. Many existing methods rely on massive, noisy synthetic this http URL work introduces an efficient training pipeline that combines model-based data filtering with parameter-efficient fine-tuning. From 4.8M synthetic examples, 12K clean and diverse instances are curated by first identifying challenging cases, removing misaligned and then selecting a diverse set of multimodal instances. On this data, a 3B-parameter Vision-Language Model is trained under three regimes: supervised fine-tuning, chain-of-thought- augmented fine-tuning, and reinforcement learning via Group Relative Policy Optimization. Models trained with the filtered data and lightweight training strategies match or surpass larger baselines on benchmarks such as ScreenSpot, Multimodal-Mind2Web, and AndroidControl. These results demonstrate that principled data curation and robust adaptation can rival large-scale training, enabling compact yet capable multimodal reasoning agents.', 'abstract_zh': '视觉定位是根据自然语言查询定位图像区域的任务，对于具备推理能力的图形用户界面代理至关重要。许多现有方法依赖于大量的嘈杂合成样本。本工作提出了一种高效的训练管道，结合基于模型的数据过滤与参数高效的微调。从480万合成样本中，通过首先识别具有挑战性的案例、移除对齐错误，然后选择具有多样性的多模态实例，筛选出1.2万份清洁且多样的样本。在这些数据上，一个3亿参数的视觉语言模型在监督微调、带有思路链增强的微调以及基于组相对策略优化的强化学习三种范式下进行训练。使用筛选后的数据和轻量级训练策略训练的模型在ScreenSpot、Multimodal-Mind2Web和AndroidControl等基准测试中达到或超越了更大规模基线模型的效果。这些结果表明，合理的数据筛选和稳健的适应可以匹敌大规模训练，从而实现紧凑且功能强大的多模态推理代理。', 'title_zh': '一种高效的推理图形用户界面代理培训管道'}
{'arxiv_id': 'arXiv:2511.08436', 'title': 'Understanding Electro-communication and Electro-sensing in Weakly Electric Fish using Multi-Agent Deep Reinforcement Learning', 'authors': 'Satpreet H. Singh, Sonja Johnson-Yu, Zhouyang Lu, Aaron Walsman, Federico Pedraja, Denis Turcu, Pratyusha Sharma, Naomi Saphra, Nathaniel B. Sawtell, Kanaka Rajan', 'link': 'https://arxiv.org/abs/2511.08436', 'abstract': "Weakly electric fish, like Gnathonemus petersii, use a remarkable electrical modality for active sensing and communication, but studying their rich electrosensing and electrocommunication behavior and associated neural activity in naturalistic settings remains experimentally challenging. Here, we present a novel biologically-inspired computational framework to study these behaviors, where recurrent neural network (RNN) based artificial agents trained via multi-agent reinforcement learning (MARL) learn to modulate their electric organ discharges (EODs) and movement patterns to collectively forage in virtual environments. Trained agents demonstrate several emergent features consistent with real fish collectives, including heavy tailed EOD interval distributions, environmental context dependent shifts in EOD interval distributions, and social interaction patterns like freeloading, where agents reduce their EOD rates while benefiting from neighboring agents' active sensing. A minimal two-fish assay further isolates the role of electro-communication, showing that access to conspecific EODs and relative dominance jointly shape foraging success. Notably, these behaviors emerge through evolution-inspired rewards for individual fitness and emergent inter-agent interactions, rather than through rewarding agents explicitly for social interactions. Our work has broad implications for the neuroethology of weakly electric fish, as well as other social, communicating animals in which extensive recordings from multiple individuals, and thus traditional data-driven modeling, are infeasible.", 'abstract_zh': '仿生计算框架下通过多智能体强化学习研究弱电鱼类的电感觉和电通信行为', 'title_zh': '使用多智能体深度强化学习理解弱电鱼的电通信与电感知'}
{'arxiv_id': 'arXiv:2511.08339', 'title': 'LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration', 'authors': 'Ruiyu Qiu, Rui Wang, Guanghui Yang, Xiang Li, Zhijiang Shao', 'link': 'https://arxiv.org/abs/2511.08339', 'abstract': "Lexicographic multi-objective problems, which consist of multiple conflicting subtasks with explicit priorities, are common in real-world applications. Despite the advantages of Reinforcement Learning (RL) in single tasks, extending conventional RL methods to prioritized multiple objectives remains challenging. In particular, traditional Safe RL and Multi-Objective RL (MORL) methods have difficulty enforcing priority orderings efficiently. Therefore, Lexicographic Multi-Objective RL (LMORL) methods have been developed to address these challenges. However, existing LMORL methods either rely on heuristic threshold tuning with prior knowledge or are restricted to discrete domains. To overcome these limitations, we propose Lexicographically Projected Policy Gradient RL (LPPG-RL), a novel LMORL framework which leverages sequential gradient projections to identify feasible policy update directions, thereby enabling LPPG-RL broadly compatible with all policy gradient algorithms in continuous spaces. LPPG-RL reformulates the projection step as an optimization problem, and utilizes Dykstra's projection rather than generic solvers to deliver great speedups, especially for small- to medium-scale instances. In addition, LPPG-RL introduces Subproblem Exploration (SE) to prevent gradient vanishing, accelerate convergence and enhance stability. We provide theoretical guarantees for convergence and establish a lower bound on policy improvement. Finally, through extensive experiments in a 2D navigation environment, we demonstrate the effectiveness of LPPG-RL, showing that it outperforms existing state-of-the-art continuous LMORL methods.", 'abstract_zh': '词序多目标强化学习：面向连续域的顺序投影策略梯度方法(Lexicographically Projected Policy Gradient RL for Continuous Lexicographic Multi-Objective Reinforcement Learning)', 'title_zh': 'LPPG-RL: 词典序投影策略梯度强化学习及子问题探索'}
{'arxiv_id': 'arXiv:2511.08231', 'title': 'Real-Time Performance Analysis of Multi-Fidelity Residual Physics-Informed Neural Process-Based State Estimation for Robotic Systems', 'authors': 'Devin Hunter, Chinwendu Enyioha', 'link': 'https://arxiv.org/abs/2511.08231', 'abstract': "Various neural network architectures are used in many of the state-of-the-art approaches for real-time nonlinear state estimation. With the ever-increasing incorporation of these data-driven models into the estimation domain, model predictions with reliable margins of error are a requirement -- especially for safety-critical applications. This paper discusses the application of a novel real-time, data-driven estimation approach based on the multi-fidelity residual physics-informed neural process (MFR-PINP) toward the real-time state estimation of a robotic system. Specifically, we address the model-mismatch issue of selecting an accurate kinematic model by tasking the MFR-PINP to also learn the residuals between simple, low-fidelity predictions and complex, high-fidelity ground-truth dynamics. To account for model uncertainty present in a physical implementation, robust uncertainty guarantees from the split conformal (SC) prediction framework are modeled in the training and inference paradigms. We provide implementation details of our MFR-PINP-based estimator for a hybrid online learning setting to validate our model's usage in real-time applications. Experimental results of our approach's performance in comparison to the state-of-the-art variants of the Kalman filter (i.e. unscented Kalman filter and deep Kalman filter) in estimation scenarios showed promising results for the MFR-PINP model as a viable option in real-time estimation tasks.", 'abstract_zh': '基于多保真残差物理感知神经过程的实时数据驱动状态估计在机器人系统中的应用', 'title_zh': '基于多保真残差物理知情神经过程的状态估计的机器人系统实时性能分析'}
{'arxiv_id': 'arXiv:2511.08181', 'title': 'MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System', 'authors': 'Seung Hwan Cho, Yujin Yang, Danik Baeck, Minjoo Kim, Young-Min Kim, Heejung Lee, Sangjin Park', 'link': 'https://arxiv.org/abs/2511.08181', 'abstract': "Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at this https URL", 'abstract_zh': '推荐系统（RS）目前正被研究以利用多模态信息或通过引入基于大型语言模型（LLMs）卓越推理能力的代理概念来缓解冷启动条件下的限制。与此同时，由于食材和饮料领域的独特数据属性和关系特征，传统的推荐系统通常采用知识图谱和本体论概念。在此背景下，我们提出MARC，一种基于代理检索增强生成（RAG）的多模态多任务鸡尾酒推荐系统，该系统在冷启动条件下采用图数据库。该系统通过两个核心过程——任务识别路由器和反思过程，生成高质量、上下文相关的问题回答。图数据库通过处理Kaggle提供的鸡尾酒数据构建，并通过200个手工制作的问题对其进行有效性评估。评估采用大型语言模型作为评判者和人类评估两种方式，结果显示，通过图数据库生成的答案在质量上优于简单向量数据库生成的答案。代码可在以下链接获取。', 'title_zh': 'MARC：多模态多任务代理检索增强生成的冷启动推荐系统'}
{'arxiv_id': 'arXiv:2511.07904', 'title': 'Test-driven Reinforcement Learning', 'authors': 'Zhao Yu, Xiuping Wu, Liangjun Ke', 'link': 'https://arxiv.org/abs/2511.07904', 'abstract': 'Reinforcement learning (RL) has been recognized as a powerful tool for robot control tasks. RL typically employs reward functions to define task objectives and guide agent learning. However, since the reward function serves the dual purpose of defining the optimal goal and guiding learning, it is challenging to design the reward function manually, which often results in a suboptimal task representation. To tackle the reward design challenge in RL, inspired by the satisficing theory, we propose a Test-driven Reinforcement Learning (TdRL) framework. In the TdRL framework, multiple test functions are used to represent the task objective rather than a single reward function. Test functions can be categorized as pass-fail tests and indicative tests, each dedicated to defining the optimal objective and guiding the learning process, respectively, thereby making defining tasks easier. Building upon such a task definition, we first prove that if a trajectory return function assigns higher returns to trajectories closer to the optimal trajectory set, maximum entropy policy optimization based on this return function will yield a policy that is closer to the optimal policy set. Then, we introduce a lexicographic heuristic approach to compare the relative distance relationship between trajectories and the optimal trajectory set for learning the trajectory return function. Furthermore, we develop an algorithm implementation of TdRL. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL matches or outperforms handcrafted reward methods in policy training, with greater design simplicity and inherent support for multi-objective optimization. We argue that TdRL offers a novel perspective for representing task objectives, which could be helpful in addressing the reward design challenges in RL applications.', 'abstract_zh': '基于测试的强化学习（Test-driven Reinforcement Learning, TdRL）框架', 'title_zh': '驱动测试的强化学习'}
{'arxiv_id': 'arXiv:2511.07707', 'title': 'A Negotiation-Based Multi-Agent Reinforcement Learning Approach for Dynamic Scheduling of Reconfigurable Manufacturing Systems', 'authors': 'Manonmani Sekar, Nasim Nezamoddini', 'link': 'https://arxiv.org/abs/2511.07707', 'abstract': 'Reconfigurable manufacturing systems (RMS) are critical for future market adjustment given their rapid adaptation to fluctuations in consumer demands, the introduction of new technological advances, and disruptions in linked supply chain sections. The adjustable hard settings of such systems require a flexible soft planning mechanism that enables realtime production planning and scheduling amid the existing complexity and variability in their configuration settings. This study explores the application of multi agent reinforcement learning (MARL) for dynamic scheduling in soft planning of the RMS settings. In the proposed framework, deep Qnetwork (DQN) agents trained in centralized training learn optimal job machine assignments in real time while adapting to stochastic events such as machine breakdowns and reconfiguration delays. The model also incorporates a negotiation with an attention mechanism to enhance state representation and improve decision focus on critical system features. Key DQN enhancements including prioritized experience replay, nstep returns, double DQN and soft target update are used to stabilize and accelerate learning. Experiments conducted in a simulated RMS environment demonstrate that the proposed approach outperforms baseline heuristics in reducing makespan and tardiness while improving machine utilization. The reconfigurable manufacturing environment was extended to simulate realistic challenges, including machine failures and reconfiguration times. Experimental results show that while the enhanced DQN agent is effective in adapting to dynamic conditions, machine breakdowns increase variability in key performance metrics such as makespan, throughput, and total tardiness. The results confirm the advantages of applying the MARL mechanism for intelligent and adaptive scheduling in dynamic reconfigurable manufacturing environments.', 'abstract_zh': '可重构制造系统中多代理 reinforcement 学习的动态调度研究', 'title_zh': '基于谈判的多代理 reinforcement 学习方法在可重构制造系统中的动态调度'}
{'arxiv_id': 'arXiv:2511.07441', 'title': 'AudAgent: Automated Auditing of Privacy Policy Compliance in AI Agents', 'authors': 'Ye Zheng, Yidan Hu', 'link': 'https://arxiv.org/abs/2511.07441', 'abstract': "AI agents can autonomously perform tasks and, often without explicit user consent, collect or disclose users' sensitive local data, which raises serious privacy concerns. Although AI agents' privacy policies may describe their intended data practices, there remains limited transparency and accountability about whether runtime behavior matches those policies. To close this gap, we introduce AudAgent, a visual framework that continuously monitors AI agents' data practices in real time and guards compliance with stated privacy policies.\nAudAgent consists of four components for automated privacy auditing of AI agents. (i) Policy parsing: an ensemble of LLMs translates natural-language privacy policies into a structured privacy-policy model, where cross-LLM voting guarantees confidence of the parsing results. (ii) Runtime annotation: a lightweight Presidio-based analyzer detects sensitive data and annotates how the data is used based on the context of the AI agent's operations and the privacy-policy model. (iii) Compliance auditing: ontology alignment and automata-based evaluation connect the policy model with runtime annotations, enabling on-the-fly compliance checks between the natural-language policy and observed unordered data practices of AI agents. (iv) User interface: a platform-independent implementation visualizes the real-time execution trace of AI agents along with potential privacy risks detected during auditing, providing user-friendly transparency and accountability.\nIn addition to common formatted privacy policies, AudAgent also supports user-defined policies for fine-grained control and customization. We evaluate AudAgent on AI agents built upon mainstream programming frameworks such as AutoGen, experiments show that AudAgent effectively identifies potential privacy policy violations in real time.", 'abstract_zh': 'AI代理的视觉审计框架：实时监控和保障声明隐私政策的合规性', 'title_zh': 'AudAgent: AI代理隐私政策合规性的自动化审计'}
