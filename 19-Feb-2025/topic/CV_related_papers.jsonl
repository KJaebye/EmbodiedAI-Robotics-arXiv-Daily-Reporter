{'arxiv_id': 'arXiv:2502.12481', 'title': 'Predicate Hierarchies Improve Few-Shot State Classification', 'authors': 'Emily Jin, Joy Hsu, Jiajun Wu', 'link': 'https://arxiv.org/abs/2502.12481', 'abstract': 'State classification of objects and their relations is core to many long-horizon tasks, particularly in robot planning and manipulation. However, the combinatorial explosion of possible object-predicate combinations, coupled with the need to adapt to novel real-world environments, makes it a desideratum for state classification models to generalize to novel queries with few examples. To this end, we propose PHIER, which leverages predicate hierarchies to generalize effectively in few-shot scenarios. PHIER uses an object-centric scene encoder, self-supervised losses that infer semantic relations between predicates, and a hyperbolic distance metric that captures hierarchical structure; it learns a structured latent space of image-predicate pairs that guides reasoning over state classification queries. We evaluate PHIER in the CALVIN and BEHAVIOR robotic environments and show that PHIER significantly outperforms existing methods in few-shot, out-of-distribution state classification, and demonstrates strong zero- and few-shot generalization from simulated to real-world tasks. Our results demonstrate that leveraging predicate hierarchies improves performance on state classification tasks with limited data.', 'abstract_zh': '基于谓词层次结构的状态分类及其关系的建模在长时任务中至关重要，特别是在机器人规划和操作中。为了使状态分类模型能够在少量示例的情况下泛化到新的查询，我们提出了PHIER，它利用谓词层次结构在少样本场景中有效泛化。PHIER使用以物体为中心的场景编码器、从谓词间语义关系的自监督损失以及捕捉层次结构的双曲距离度量；它学习图像-谓词对的结构化潜在空间，指导状态分类查询的推理。我们在CALVIN和BEHAVIOR机器人环境中评估PHIER，结果显示PHIER在少样本、分布外状态分类中显著优于现有方法，并且在模拟到真实任务的零样本和少样本泛化方面表现出色。我们的结果表明，利用谓词层次结构可以提高在数据有限的情况下进行状态分类任务的性能。', 'title_zh': '谓词层次结构改进少量样本状态分类'}
{'arxiv_id': 'arXiv:2502.12782', 'title': 'VidCapBench: A Comprehensive Benchmark of Video Captioning for Controllable Text-to-Video Generation', 'authors': 'Xinlong Chen, Yuanxing Zhang, Chongling Rao, Yushuo Guan, Jiaheng Liu, Fuzheng Zhang, Chengru Song, Qiang Liu, Di Zhang, Tieniu Tan', 'link': 'https://arxiv.org/abs/2502.12782', 'abstract': 'The training of controllable text-to-video (T2V) models relies heavily on the alignment between videos and captions, yet little existing research connects video caption evaluation with T2V generation assessment. This paper introduces VidCapBench, a video caption evaluation scheme specifically designed for T2V generation, agnostic to any particular caption format. VidCapBench employs a data annotation pipeline, combining expert model labeling and human refinement, to associate each collected video with key information spanning video aesthetics, content, motion, and physical laws. VidCapBench then partitions these key information attributes into automatically assessable and manually assessable subsets, catering to both the rapid evaluation needs of agile development and the accuracy requirements of thorough validation. By evaluating numerous state-of-the-art captioning models, we demonstrate the superior stability and comprehensiveness of VidCapBench compared to existing video captioning evaluation approaches. Verification with off-the-shelf T2V models reveals a significant positive correlation between scores on VidCapBench and the T2V quality evaluation metrics, indicating that VidCapBench can provide valuable guidance for training T2V models. The project is available at this https URL.', 'abstract_zh': '可控文本生成视频（T2V）模型的训练高度依赖于视频和字幕的对齐，然而现有研究鲜有关于字幕评估与T2V生成评估的联系。本文介绍了VidCapBench，一种专门用于T2V生成的字幕评估方案，不依赖于任何特定的字幕格式。VidCapBench通过结合专家模型标记和人工完善的数据注释管道，将每个收集到的视频与视频美学、内容、运动和物理法则等关键信息联系起来。VidCapBench随后将这些关键信息属性划分为可自动评估和需要手动评估的子集，以满足敏捷开发的快速评估需求和详尽验证的准确性要求。通过对多种最先进的字幕生成模型进行评估，我们证明了VidCapBench在稳定性和综合性方面相较于现有视频字幕评估方法具有显著优势。利用现成的T2V模型进行验证表明，VidCapBench的评分与T2V质量评估指标之间存在显著的正相关关系，这表明VidCapBench能够为训练T2V模型提供有价值的指导。该项目可在以下链接获取：this https URL。', 'title_zh': 'VidCapBench: 用于可控文本到视频生成的视频字幕综合基准'}
{'arxiv_id': 'arXiv:2502.12985', 'title': 'PartSDF: Part-Based Implicit Neural Representation for Composite 3D Shape Parametrization and Optimization', 'authors': 'Nicolas Talabot, Olivier Clerc, Arda Cinar Demirtas, Doruk Oner, Pascal Fua', 'link': 'https://arxiv.org/abs/2502.12985', 'abstract': 'Accurate 3D shape representation is essential in engineering applications such as design, optimization, and simulation. In practice, engineering workflows require structured, part-aware representations, as objects are inherently designed as assemblies of distinct components. However, most existing methods either model shapes holistically or decompose them without predefined part structures, limiting their applicability in real-world design tasks. We propose PartSDF, a supervised implicit representation framework that explicitly models composite shapes with independent, controllable parts while maintaining shape consistency. Despite its simple single-decoder architecture, PartSDF outperforms both supervised and unsupervised baselines in reconstruction and generation tasks. We further demonstrate its effectiveness as a structured shape prior for engineering applications, enabling precise control over individual components while preserving overall coherence. Code available at this https URL.', 'abstract_zh': '准确的3D形状表示在工程应用如设计、优化和模拟中至关重要。在实践中，工程工作流需要结构化的、部件感知的表示，因为对象本质上是由不同的组件组成的组装体。然而，现有大多数方法要么整体建模形状，要么在没有预定义部件结构的情况下分解形状，这限制了它们在实际设计任务中的应用。我们提出了一种名为PartSDF的监督隐式表示框架，该框架明确地以独立可控部件的形式建模复合形状，同时保持形状一致性。尽管其单解码器架构简单，但PartSDF在重建和生成任务中均优于监督和无监督基线。我们进一步证明，PartSDF作为结构化形状先验，对于工程应用具有有效性，能够在保持整体连贯性的同时精确控制各个组件。代码见此链接。', 'title_zh': 'PartSDF: 基于部件的隐式神经表示方法及其在复合三维形状参数化和优化中的应用'}
{'arxiv_id': 'arXiv:2502.12677', 'title': 'Spiking Vision Transformer with Saccadic Attention', 'authors': 'Shuai Wang, Malu Zhang, Dehao Zhang, Ammar Belatreche, Yichen Xiao, Yu Liang, Yimeng Shan, Qian Sun, Enqi Zhang, Yang Yang', 'link': 'https://arxiv.org/abs/2502.12677', 'abstract': 'The combination of Spiking Neural Networks (SNNs) and Vision Transformers (ViTs) holds potential for achieving both energy efficiency and high performance, particularly suitable for edge vision applications. However, a significant performance gap still exists between SNN-based ViTs and their ANN counterparts. Here, we first analyze why SNN-based ViTs suffer from limited performance and identify a mismatch between the vanilla self-attention mechanism and spatio-temporal spike trains. This mismatch results in degraded spatial relevance and limited temporal interactions. To address these issues, we draw inspiration from biological saccadic attention mechanisms and introduce an innovative Saccadic Spike Self-Attention (SSSA) method. Specifically, in the spatial domain, SSSA employs a novel spike distribution-based method to effectively assess the relevance between Query and Key pairs in SNN-based ViTs. Temporally, SSSA employs a saccadic interaction module that dynamically focuses on selected visual areas at each timestep and significantly enhances whole scene understanding through temporal interactions. Building on the SSSA mechanism, we develop a SNN-based Vision Transformer (SNN-ViT). Extensive experiments across various visual tasks demonstrate that SNN-ViT achieves state-of-the-art performance with linear computational complexity. The effectiveness and efficiency of the SNN-ViT highlight its potential for power-critical edge vision applications.', 'abstract_zh': '基于...]的尖峰神经网络（SNNs）与视觉变换器（ViTs）的结合在实现高效能和高性能方面具有潜力，特别适用于边缘视觉应用。然而，基于SNN的ViTs与其ANN对应物之间仍存在显著的性能差距。为此，我们首先分析了基于SNN的ViTs性能受限的原因，并发现vanilla自注意力机制与时空尖峰脉冲序列之间存在不匹配。这种不匹配导致了空间相关性的降低和时间交互能力的有限。为了解决这些问题，我们从生物瞬目注意机制中汲取灵感，并引入了一种创新的瞬目尖峰自注意力（SSSA）方法。具体而言，在空间领域，SSSA采用了一种新颖的尖峰脉冲分布方法，有效评估了基于SNN的ViTs中的查询和键对的相关性。在时间维度上，SSSA引入了一种瞬目交互模块，在每个时隙动态关注选定的视觉区域，并通过时间交互显著增强对整个场景的理解。基于SSSA机制，我们开发了一种基于SNN的视觉变换器（SNN-ViT）。在各种视觉任务的广泛实验中，SNN-ViT展示了线性计算复杂度下的最佳性能。SNN-ViT的效果和效率突显了其在功率关键边缘视觉应用中的潜力。', 'title_zh': '斯巴达克注意机制下的尖峰视觉变换器'}
{'arxiv_id': 'arXiv:2502.12558', 'title': 'MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment Retrieval Within Long Videos', 'authors': 'Huaying Yuan, Jian Ni, Yueze Wang, Junjie Zhou, Zhengyang Liang, Zheng Liu, Zhao Cao, Zhicheng Dou, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2502.12558', 'abstract': "Retrieval augmented generation (RAG) holds great promise in addressing challenges associated with long video understanding. These methods retrieve useful moments from long videos for their presented tasks, thereby enabling multimodal large language models (MLLMs) to generate high-quality answers in a cost-effective way. In this work, we present MomentSeeker, a comprehensive benchmark to evaluate retrieval models' performance in handling general long-video moment retrieval (LVMR) tasks. MomentSeeker offers three key advantages. First, it incorporates long videos of over 500 seconds on average, making it the first benchmark specialized for long-video moment retrieval. Second, it covers a wide range of task categories (including Moment Search, Caption Alignment, Image-conditioned Moment Search, and Video-conditioned Moment Search) and diverse application scenarios (e.g., sports, movies, cartoons, and ego), making it a comprehensive tool for assessing retrieval models' general LVMR performance. Additionally, the evaluation tasks are carefully curated through human annotation, ensuring the reliability of assessment. We further fine-tune an MLLM-based LVMR retriever on synthetic data, which demonstrates strong performance on our benchmark. We perform extensive experiments with various popular multimodal retrievers based on our benchmark, whose results highlight the challenges of LVMR and limitations for existing methods. Our created resources will be shared with community to advance future research in this field.", 'abstract_zh': '长视频片段检索增强生成（RAG）在解决长视频理解挑战方面展现出巨大潜力。这些方法从长视频中检索有用片段以完成其呈现的任务，从而使得多模态大规模语言模型（MLLMs）能够以经济有效的方式生成高质量的答案。在本文中，我们介绍了MomentSeeker，一个全面的基准测试，用于评估检索模型在处理通用长视频片段检索（LVMR）任务方面的性能。MomentSeeker提供了三项主要优势。首先，它包含平均时长超过500秒的长视频，使其成为第一个专门为长视频片段检索设计的基准测试。其次，它涵盖了广泛的任务类别（包括片段搜索、字幕对齐、图像条件下的片段搜索和视频条件下的片段搜索）和多样的应用场景（例如体育、电影、动画和自传），使其成为评估检索模型通用LVMR性能的综合工具。此外，通过人工注释精确选择评估任务，确保评估的可靠性。我们在合成数据上微调了一个基于MLLM的LVMR检索器，其在基准测试中表现出 strong 的性能。我们基于基准测试进行了广泛的实验，使用不同的流行多模态检索器，结果突显了LVMR的挑战和现有方法的局限性。我们创建的资源将与社区共享，以促进该领域的未来研究。', 'title_zh': 'MomentSeeker: 一个全面的基准和长视频关键时刻检索的强 baseline'}
{'arxiv_id': 'arXiv:2502.12536', 'title': 'An Algorithm Board in Neural Decoding', 'authors': 'Jingyi Feng, Kai Yang', 'link': 'https://arxiv.org/abs/2502.12536', 'abstract': "Understanding the mechanisms of neural encoding and decoding has always been a highly interesting research topic in fields such as neuroscience and cognitive intelligence. In prior studies, some researchers identified a symmetry in neural data decoded by unsupervised methods in motor scenarios and constructed a cognitive learning system based on this pattern (i.e., symmetry). Nevertheless, the distribution state of the data flow that significantly influences neural decoding positions still remains a mystery within the system, which further restricts the enhancement of the system's interpretability. Based on this, this paper mainly explores changes in the distribution state within the system from the machine learning and mathematical statistics perspectives. In the experiment, we assessed the correctness of this symmetry using various tools and indicators commonly utilized in mathematics and statistics. According to the experimental results, the normal distribution (or Gaussian distribution) plays a crucial role in the decoding of prediction positions within the system. Eventually, an algorithm board similar to the Galton board was built to serve as the mathematical foundation of the discovered symmetry.", 'abstract_zh': '理解神经编码与解码的机制一直是神经科学和认知智能等领域中的一个重要研究课题。在前期研究中，一些研究人员识别出了在运动场景中由无监督方法解码的神经数据中存在的对称性，并基于这一模式构建了认知学习系统。然而，显著影响神经解码位置的数据流分布状态在系统中仍然未知，这进一步限制了系统可解释性的提升。基于此，本文主要从机器学习和数学统计的角度探索系统中数据流分布状态的变化。在实验中，我们使用了数学和统计中常用的多种工具和指标来评估这一对称性。根据实验结果，正态分布（或高斯分布）在系统中预测位置的解码中起到了关键作用。最终，建立了一个类似于高尔顿板的算法板，作为发现这一对称性的数学基础。', 'title_zh': '神经解码算法板'}
{'arxiv_id': 'arXiv:2502.12524', 'title': 'YOLOv12: Attention-Centric Real-Time Object Detectors', 'authors': 'Yunjie Tian, Qixiang Ye, David Doermann', 'link': 'https://arxiv.org/abs/2502.12524', 'abstract': 'Enhancing the network architecture of the YOLO framework has been crucial for a long time, but has focused on CNN-based improvements despite the proven superiority of attention mechanisms in modeling capabilities. This is because attention-based models cannot match the speed of CNN-based models. This paper proposes an attention-centric YOLO framework, namely YOLOv12, that matches the speed of previous CNN-based ones while harnessing the performance benefits of attention mechanisms. YOLOv12 surpasses all popular real-time object detectors in accuracy with competitive speed. For example, YOLOv12-N achieves 40.6% mAP with an inference latency of 1.64 ms on a T4 GPU, outperforming advanced YOLOv10-N / YOLOv11-N by 2.1%/1.2% mAP with a comparable speed. This advantage extends to other model scales. YOLOv12 also surpasses end-to-end real-time detectors that improve DETR, such as RT-DETR / RT-DETRv2: YOLOv12-S beats RT-DETR-R18 / RT-DETRv2-R18 while running 42% faster, using only 36% of the computation and 45% of the parameters. More comparisons are shown in Figure 1.', 'abstract_zh': '一种基于注意力机制的YOLO框架YOLOv12：兼顾速度与性能', 'title_zh': 'YOLOv12: 以注意力为中心的实时物体检测器'}
{'arxiv_id': 'arXiv:2502.12456', 'title': 'Not-So-Optimal Transport Flows for 3D Point Cloud Generation', 'authors': 'Ka-Hei Hui, Chao Liu, Xiaohui Zeng, Chi-Wing Fu, Arash Vahdat', 'link': 'https://arxiv.org/abs/2502.12456', 'abstract': 'Learning generative models of 3D point clouds is one of the fundamental problems in 3D generative learning. One of the key properties of point clouds is their permutation invariance, i.e., changing the order of points in a point cloud does not change the shape they represent. In this paper, we analyze the recently proposed equivariant OT flows that learn permutation invariant generative models for point-based molecular data and we show that these models scale poorly on large point clouds. Also, we observe learning (equivariant) OT flows is generally challenging since straightening flow trajectories makes the learned flow model complex at the beginning of the trajectory. To remedy these, we propose not-so-optimal transport flow models that obtain an approximate OT by an offline OT precomputation, enabling an efficient construction of OT pairs for training. During training, we can additionally construct a hybrid coupling by combining our approximate OT and independent coupling to make the target flow models easier to learn. In an extensive empirical study, we show that our proposed model outperforms prior diffusion- and flow-based approaches on a wide range of unconditional generation and shape completion on the ShapeNet benchmark.', 'abstract_zh': '学习3D点云生成模型是3D生成学习中的基本问题之一。点云的一个关键性质是其置换不变性，即改变点云中点的顺序不会改变它们所代表的形状。在本文中，我们分析了最近提出的等变OT流，这些流学习基于点的分子数据的置换不变生成模型，并展示了这些模型在大点云上缩放不良。我们还观察到，学习（等变的）OT流通常具有挑战性，因为拉直流轨迹会使学习的流模型在轨迹开始时变得复杂。为了解决这些问题，我们提出了近似OT流模型，通过离线预计算OT来获得近似OT，从而能够高效地构建OT对进行训练。在训练过程中，我们还可以通过结合我们的近似OT和独立耦合来构建混合耦合，使目标流模型更易于学习。在广泛的实证研究中，我们展示了我们的模型在ShapeNet基准上的无条件生成和形状补全任务中优于先前的扩散和流基方法。', 'title_zh': '非最优输运流生成3D点云'}
{'arxiv_id': 'arXiv:2502.12418', 'title': 'Boosting Illuminant Estimation in Deep Color Constancy through Enhancing Brightness Robustness', 'authors': 'Mengda Xie, Chengzhi Zhong, Yiling He, Zhan Qin, Meie Fang', 'link': 'https://arxiv.org/abs/2502.12418', 'abstract': 'Color constancy estimates illuminant chromaticity to correct color-biased images. Recently, Deep Neural Network-driven Color Constancy (DNNCC) models have made substantial advancements. Nevertheless, the potential risks in DNNCC due to the vulnerability of deep neural networks have not yet been explored. In this paper, we conduct the first investigation into the impact of a key factor in color constancy-brightness-on DNNCC from a robustness perspective. Our evaluation reveals that several mainstream DNNCC models exhibit high sensitivity to brightness despite their focus on chromaticity estimation. This sheds light on a potential limitation of existing DNNCC models: their sensitivity to brightness may hinder performance given the widespread brightness variations in real-world datasets. From the insights of our analysis, we propose a simple yet effective brightness robustness enhancement strategy for DNNCC models, termed BRE. The core of BRE is built upon the adaptive step-size adversarial brightness augmentation technique, which identifies high-risk brightness variation and generates augmented images via explicit brightness adjustment. Subsequently, BRE develops a brightness-robustness-aware model optimization strategy that integrates adversarial brightness training and brightness contrastive loss, significantly bolstering the brightness robustness of DNNCC models. BRE is hyperparameter-free and can be integrated into existing DNNCC models, without incurring additional overhead during the testing phase. Experiments on two public color constancy datasets-ColorChecker and Cube+-demonstrate that the proposed BRE consistently enhances the illuminant estimation performance of existing DNNCC models, reducing the estimation error by an average of 5.04% across six mainstream DNNCC models, underscoring the critical role of enhancing brightness robustness in these models.', 'abstract_zh': 'brightness robustness enhancement for deep neural network-driven color constancy models', 'title_zh': '通过增强亮度鲁棒性提升深度色彩恒常性中的照明估计<table>(资料来源：用户提供的标题)</table>'}
{'arxiv_id': 'arXiv:2502.12360', 'title': 'Detecting Systematic Weaknesses in Vision Models along Predefined Human-Understandable Dimensions', 'authors': 'Sujan Sai Gannamaneni, Rohil Prakash Rao, Michael Mock, Maram Akila, Stefan Wrobel', 'link': 'https://arxiv.org/abs/2502.12360', 'abstract': 'Studying systematic weaknesses of DNNs has gained prominence in the last few years with the rising focus on building safe AI systems. Slice discovery methods (SDMs) are prominent algorithmic approaches for finding such systematic weaknesses. They identify top-k semantically coherent slices/subsets of data where a DNN-under-test has low performance. For being directly useful, e.g., as evidences in a safety argumentation, slices should be aligned with human-understandable (safety-relevant) dimensions, which, for example, are defined by safety and domain experts as parts of the operational design domain (ODD). While straightforward for structured data, the lack of semantic metadata makes these investigations challenging for unstructured data. Therefore, we propose a complete workflow which combines contemporary foundation models with algorithms for combinatorial search that consider structured data and DNN errors for finding systematic weaknesses in images. In contrast to existing approaches, ours identifies weak slices that are in line with predefined human-understandable dimensions. As the workflow includes foundation models, its intermediate and final results may not always be exact. Therefore, we build into our workflow an approach to address the impact of noisy metadata. We evaluate our approach w.r.t. its quality on four popular computer vision datasets, including autonomous driving datasets like Cityscapes, BDD100k, and RailSem19, while using multiple state-of-the-art models as DNNs-under-test.', 'abstract_zh': '研究深度神经网络系统的薄弱环节已成为热点，特别是在构建安全人工智能系统方面。切片发现方法（SDMs）是寻找此类系统薄弱环节的主要算法方法。它们识别出深度神经网络测试在其中性能较低的top-k语义一致的切片/子集。为直接用于现实场景，例如作为安全论证的证据，切片应与人类可理解（安全相关）的维度对齐，这些维度可由安全和领域专家定义为操作设计领域（ODD）的一部分。对于结构化数据而言，此类对齐较为直接，但对于非结构化数据而言，缺乏语义元数据使其更加具有挑战性。因此，我们提出了一种结合当代基础模型与组合搜索算法的完整工作流程，用于在图像中发现深度神经网络系统的系统性弱点。与现有方法不同，我们识别出与预定义的人类可理解维度一致的弱切片。由于工作流程包括基础模型，其中间和最终结果可能并不总是精确的，因此我们将其工作流程构建了处理嘈杂元数据影响的方法。我们使用多个最新模型作为深度神经网络测试，针对包括自主驾驶数据集如Cityscapes、BDD100k和RailSem19在内的四个流行计算机视觉数据集评估了该方法的质量。', 'title_zh': '基于预定义的人类可理解维度检测视觉模型的系统性弱点'}
{'arxiv_id': 'arXiv:2502.12198', 'title': 'Maximize Your Diffusion: A Study into Reward Maximization and Alignment for Diffusion-based Control', 'authors': 'Dom Huh, Prasant Mohapatra', 'link': 'https://arxiv.org/abs/2502.12198', 'abstract': 'Diffusion-based planning, learning, and control methods present a promising branch of powerful and expressive decision-making solutions. Given the growing interest, such methods have undergone numerous refinements over the past years. However, despite these advancements, existing methods are limited in their investigations regarding general methods for reward maximization within the decision-making process. In this work, we study extensions of fine-tuning approaches for control applications. Specifically, we explore extensions and various design choices for four fine-tuning approaches: reward alignment through reinforcement learning, direct preference optimization, supervised fine-tuning, and cascading diffusion. We optimize their usage to merge these independent efforts into one unified paradigm. We show the utility of such propositions in offline RL settings and demonstrate empirical improvements over a rich array of control tasks.', 'abstract_zh': '基于扩散的方法在决策规划、学习和控制中的扩展研究', 'title_zh': '最大化扩散效应：基于奖励最大化和对齐的研究'}
{'arxiv_id': 'arXiv:2502.01842', 'title': 'Texture Image Synthesis Using Spatial GAN Based on Vision Transformers', 'authors': 'Elahe Salari, Zohreh Azimifar', 'link': 'https://arxiv.org/abs/2502.01842', 'abstract': "Texture synthesis is a fundamental task in computer vision, whose goal is to generate visually realistic and structurally coherent textures for a wide range of applications, from graphics to scientific simulations. While traditional methods like tiling and patch-based techniques often struggle with complex textures, recent advancements in deep learning have transformed this field. In this paper, we propose ViT-SGAN, a new hybrid model that fuses Vision Transformers (ViTs) with a Spatial Generative Adversarial Network (SGAN) to address the limitations of previous methods. By incorporating specialized texture descriptors such as mean-variance (mu, sigma) and textons into the self-attention mechanism of ViTs, our model achieves superior texture synthesis. This approach enhances the model's capacity to capture complex spatial dependencies, leading to improved texture quality that is superior to state-of-the-art models, especially for regular and irregular textures. Comparison experiments with metrics such as FID, IS, SSIM, and LPIPS demonstrate the substantial improvement of ViT-SGAN, which underlines its efficiency in generating diverse realistic textures.", 'abstract_zh': '纹理合成是计算机视觉中的一个基础任务，其目标是生成视觉上逼真且结构上一致的纹理，应用于从图形到科学模拟等各种领域。虽然传统的砖块铺贴和基于块的技术在处理复杂纹理方面常有困难，但近年来深度学习的进步已经改变了这一领域。在本文中，我们提出了一种新的混合模型ViT-SGAN，该模型通过将视觉变换器（ViTs）与空间生成对抗网络（SGAN）相结合，以解决之前方法的局限性。通过将均值-方差（mu, sigma）和纹理这样的专门纹理描述符融入ViTs的自注意机制中，我们的模型实现了卓越的纹理合成效果。这种方法增强了模型捕捉复杂空间依赖性的能力，从而提高了纹理质量，相对于最先进的模型，尤其是对于规则和不规则的纹理，其效果更为出色。使用FID、IS、SSIM和LPIPS等指标的比较实验证明了ViT-SGAN的显著改进，突显了其在生成多样化的逼真纹理方面的高效性。', 'title_zh': '基于视觉变换器的时空生成对抗网络的纹理图像合成'}
