{'arxiv_id': 'arXiv:2506.18897', 'title': 'MinD: Unified Visual Imagination and Control via Hierarchical World Models', 'authors': 'Xiaowei Chi, Kuangzhi Ge, Jiaming Liu, Siyuan Zhou, Peidong Jia, Zichen He, Yuzhen Liu, Tingguang Li, Lei Han, Sirui Han, Shanghang Zhang, Yike Guo', 'link': 'https://arxiv.org/abs/2506.18897', 'abstract': 'Video generation models (VGMs) offer a promising pathway for unified world modeling in robotics by integrating simulation, prediction, and manipulation. However, their practical application remains limited due to (1) slowgeneration speed, which limits real-time interaction, and (2) poor consistency between imagined videos and executable actions. To address these challenges, we propose Manipulate in Dream (MinD), a hierarchical diffusion-based world model framework that employs a dual-system design for vision-language manipulation. MinD executes VGM at low frequencies to extract video prediction features, while leveraging a high-frequency diffusion policy for real-time interaction. This architecture enables low-latency, closed-loop control in manipulation with coherent visual guidance. To better coordinate the two systems, we introduce a video-action diffusion matching module (DiffMatcher), with a novel co-training strategy that uses separate schedulers for each diffusion model. Specifically, we introduce a diffusion-forcing mechanism to DiffMatcher that aligns their intermediate representations during training, helping the fast action model better understand video-based predictions. Beyond manipulation, MinD also functions as a world simulator, reliably predicting task success or failure in latent space before execution. Trustworthy analysis further shows that VGMs can preemptively evaluate task feasibility and mitigate risks. Extensive experiments across multiple benchmarks demonstrate that MinD achieves state-of-the-art manipulation (63%+) in RL-Bench, advancing the frontier of unified world modeling in robotics.', 'abstract_zh': '基于生成视频模型的操纵 dreams (MinD)：一种用于机器人统一世界建模的分层扩散机制框架', 'title_zh': 'MinD: 统一的视觉想象与控制通过层次世界模型'}
{'arxiv_id': 'arXiv:2506.18885', 'title': 'GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM', 'authors': 'Annika Thomas, Aneesa Sonawalla, Alex Rose, Jonathan P. How', 'link': 'https://arxiv.org/abs/2506.18885', 'abstract': '3D Gaussian splatting has emerged as an expressive scene representation for RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor environments remains unexplored. Multi-agent Gaussian SLAM is a promising approach to rapid exploration and reconstruction of environments, offering scalable environment representations, but existing approaches are limited to small-scale, indoor environments. To that end, we propose Gaussian Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative Gaussian splatting SLAM method that integrates i) an implicit tracking module based on local optimization over submaps and ii) an approach to inter- and intra-robot loop closure integrated into a pose-graph optimization framework. Experiments show that GRAND-SLAM provides state-of-the-art tracking performance and 28% higher PSNR than existing methods on the Replica indoor dataset, as well as 91% lower multi-agent tracking error and improved rendering over existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.', 'abstract_zh': '基于多机器人密集SLAM的高斯重建：GRAND-SLAM', 'title_zh': 'GRAND-SLAM：全局一致的大规模多智能体高斯SLAM的局部优化'}
{'arxiv_id': 'arXiv:2506.18844', 'title': 'Reproducible Evaluation of Camera Auto-Exposure Methods in the Field: Platform, Benchmark and Lessons Learned', 'authors': 'Olivier Gamache, Jean-Michel Fortin, Matěj Boxan, François Pomerleau, Philippe Giguère', 'link': 'https://arxiv.org/abs/2506.18844', 'abstract': "Standard datasets often present limitations, particularly due to the fixed nature of input data sensors, which makes it difficult to compare methods that actively adjust sensor parameters to suit environmental conditions. This is the case with Automatic-Exposure (AE) methods, which rely on environmental factors to influence the image acquisition process. As a result, AE methods have traditionally been benchmarked in an online manner, rendering experiments non-reproducible. Building on our prior work, we propose a methodology that utilizes an emulator capable of generating images at any exposure time. This approach leverages BorealHDR, a unique multi-exposure stereo dataset, along with its new extension, in which data was acquired along a repeated trajectory at different times of the day to assess the impact of changing illumination. In total, BorealHDR covers 13.4 km over 59 trajectories in challenging lighting conditions. The dataset also includes lidar-inertial-odometry-based maps with pose estimation for each image frame, as well as Global Navigation Satellite System (GNSS) data for comparison. We demonstrate that by using images acquired at various exposure times, we can emulate realistic images with a Root-Mean-Square Error (RMSE) below 1.78% compared to ground truth images. Using this offline approach, we benchmarked eight AE methods, concluding that the classical AE method remains the field's best performer. To further support reproducibility, we provide in-depth details on the development of our backpack acquisition platform, including hardware, electrical components, and performance specifications. Additionally, we share valuable lessons learned from deploying the backpack over more than 25 km across various environments. Our code and dataset are available online at this link: this https URL BorealHDR", 'abstract_zh': '标准数据集往往存在局限性，尤其是由于输入数据传感器的固定性质，这使得难以比较那些能主动调整传感器参数以适应环境条件的方法。这种情况在自动曝光（AE）方法中尤为明显，这些方法依赖于环境因素来影响图像获取过程。因此，AE方法传统上以在线方式进行基准测试，导致实验无法重现。在我们之前工作的基础上，我们提出了一种利用能够生成任意曝光时间图像的模拟器的方法。该方法利用了BorealHDR这样一个独特的多曝光立体数据集及其新的扩展版本，在不同时间段沿重复轨迹获取数据以评估光照变化的影响。BorealHDR总共覆盖了59条轨迹，长度为13.4公里，在具有挑战性的光照条件下。数据集还包括基于激光雷达惯性里程计的位姿估计图像地图以及全球导航卫星系统（GNSS）数据以供比较。通过使用不同曝光时间获取的图像，我们能以与真实图像的均方根误差（RMSE）低于1.78%的精度模拟现实图像。利用这种离线方法，我们对八种AE方法进行了基准测试，得出经典AE方法仍然是该领域的最佳表现者。为支持进一步的可重现性，我们详细描述了背包获取平台的开发过程，包括硬件、电气组件和性能规格。此外，我们还分享了在各种环境中跨越超过25公里部署背包时学到的经验教训。我们的代码和数据集可在以下链接获取：this https URL BorealHDR。', 'title_zh': '田间可重复评估的相机自动曝光方法评价平台、基准和经验教训'}
{'arxiv_id': 'arXiv:2506.18825', 'title': 'SViP: Sequencing Bimanual Visuomotor Policies with Object-Centric Motion Primitives', 'authors': 'Yizhou Chen, Hang Xu, Dongjie Yu, Zeqing Zhang, Yi Ren, Jia Pan', 'link': 'https://arxiv.org/abs/2506.18825', 'abstract': 'Imitation learning (IL), particularly when leveraging high-dimensional visual inputs for policy training, has proven intuitive and effective in complex bimanual manipulation tasks. Nonetheless, the generalization capability of visuomotor policies remains limited, especially when small demonstration datasets are available. Accumulated errors in visuomotor policies significantly hinder their ability to complete long-horizon tasks. To address these limitations, we propose SViP, a framework that seamlessly integrates visuomotor policies into task and motion planning (TAMP). SViP partitions human demonstrations into bimanual and unimanual operations using a semantic scene graph monitor. Continuous decision variables from the key scene graph are employed to train a switching condition generator. This generator produces parameterized scripted primitives that ensure reliable performance even when encountering out-of-the-distribution observations. Using only 20 real-world demonstrations, we show that SViP enables visuomotor policies to generalize across out-of-distribution initial conditions without requiring object pose estimators. For previously unseen tasks, SViP automatically discovers effective solutions to achieve the goal, leveraging constraint modeling in TAMP formulism. In real-world experiments, SViP outperforms state-of-the-art generative IL methods, indicating wider applicability for more complex tasks. Project website: this https URL', 'abstract_zh': '基于知觉运动策略的无缝集成框架（SViP）：实现复杂双臂操作任务的泛化能力', 'title_zh': 'SViP: 基于物体中心运动模块的双手视觉运动策略序列表征'}
{'arxiv_id': 'arXiv:2506.18812', 'title': 'Learning Physical Systems: Symplectification via Gauge Fixing in Dirac Structures', 'authors': 'Aristotelis Papatheodorou, Pranav Vaidhyanathan, Natalia Ares, Ioannis Havoutis', 'link': 'https://arxiv.org/abs/2506.18812', 'abstract': 'Physics-informed deep learning has achieved remarkable progress by embedding geometric priors, such as Hamiltonian symmetries and variational principles, into neural networks, enabling structure-preserving models that extrapolate with high accuracy. However, in systems with dissipation and holonomic constraints, ubiquitous in legged locomotion and multibody robotics, the canonical symplectic form becomes degenerate, undermining the very invariants that guarantee stability and long-term prediction. In this work, we tackle this foundational limitation by introducing Presymplectification Networks (PSNs), the first framework to learn the symplectification lift via Dirac structures, restoring a non-degenerate symplectic geometry by embedding constrained systems into a higher-dimensional manifold. Our architecture combines a recurrent encoder with a flow-matching objective to learn the augmented phase-space dynamics end-to-end. We then attach a lightweight Symplectic Network (SympNet) to forecast constrained trajectories while preserving energy, momentum, and constraint satisfaction. We demonstrate our method on the dynamics of the ANYmal quadruped robot, a challenging contact-rich, multibody system. To the best of our knowledge, this is the first framework that effectively bridges the gap between constrained, dissipative mechanical systems and symplectic learning, unlocking a whole new class of geometric machine learning models, grounded in first principles yet adaptable from data.', 'abstract_zh': '物理信息深度学习通过将哈密顿对称性和变分原理等几何先验嵌入神经网络中，实现了显著进展，从而构建出保持结构且在高精度下进行外推的模型。然而，在带有耗散和约束条件的系统中，如腿式运动和多体机器人中普遍存在的系统，经典的辛形式变得退化，从而削弱了确保稳定性和长期预测的不变量。本文通过引入 Presymplectification 网络 (PSNs)，即首个利用 Dirac 结构学习辛提升的框架，解决了这一基础限制。我们通过在更高维流形中嵌入受约束系统来恢复非退化的辛几何。我们的架构结合了递归编码器和流匹配目标，以端到端的方式学习扩展相空间的动力学。随后，我们附加了一个轻量级辛网络 (SympNet)，用于预测受约束轨迹的同时保持能量、动量和约束满足。我们展示了该方法在 ANYmal 四足机器人动态中的应用，这是一个具有挑战性的接触丰富、多体系统。据我们所知，这是首个有效连接受约束、耗散机械系统和辛学习的框架，开启了基于第一原理但又可以从数据中适应的几何机器学习模型的新类别。', 'title_zh': '学习物理系统：通过规范选取实现辛结构的化归'}
{'arxiv_id': 'arXiv:2506.18779', 'title': 'DefFusionNet: Learning Multimodal Goal Shapes for Deformable Object Manipulation via a Diffusion-based Probabilistic Model', 'authors': 'Bao Thach, Siyeon Kim, Britton Jordan, Mohanraj Shanthi, Tanner Watts, Shing-Hei Ho, James M. Ferguson, Tucker Hermans, Alan Kuntz', 'link': 'https://arxiv.org/abs/2506.18779', 'abstract': "Deformable object manipulation is critical to many real-world robotic applications, ranging from surgical robotics and soft material handling in manufacturing to household tasks like laundry folding. At the core of this important robotic field is shape servoing, a task focused on controlling deformable objects into desired shapes. The shape servoing formulation requires the specification of a goal shape. However, most prior works in shape servoing rely on impractical goal shape acquisition methods, such as laborious domain-knowledge engineering or manual manipulation. DefGoalNet previously posed the current state-of-the-art solution to this problem, which learns deformable object goal shapes directly from a small number of human demonstrations. However, it significantly struggles in multi-modal settings, where multiple distinct goal shapes can all lead to successful task completion. As a deterministic model, DefGoalNet collapses these possibilities into a single averaged solution, often resulting in an unusable goal. In this paper, we address this problem by developing DefFusionNet, a novel neural network that leverages the diffusion probabilistic model to learn a distribution over all valid goal shapes rather than predicting a single deterministic outcome. This enables the generation of diverse goal shapes and avoids the averaging artifacts. We demonstrate our method's effectiveness on robotic tasks inspired by both manufacturing and surgical applications, both in simulation and on a physical robot. Our work is the first generative model capable of producing a diverse, multi-modal set of deformable object goals for real-world robotic applications.", 'abstract_zh': '可变形对象操控对于许多现实世界的机器人应用至关重要，从外科机器人和制造业中的软材料处理到家务任务如衣物整理。这一重要机器人领域的核心是形状伺服技术，其旨在控制可变形对象达到预期形状。形状伺服的建模需要设定目标形状。然而，大多数现有的形状伺服研究依赖于不实际的目标形状获取方法，如繁重的领域知识工程或手动操作。DefGoalNet之前提出了当前最先进的解决方案，可以从少量的人类演示中直接学习可变形对象的目标形状。然而，它在多模态环境中表现不佳，在这种环境中，多种不同的目标形状都可以导致任务成功完成。作为确定性模型，DefGoalNet将这些可能性简化为单一的平均解，经常导致无法使用的目标。在本文中，我们通过开发DefFusionNet，一种新颖的神经网络，利用扩散概率模型来学习所有有效目标形状的分布，而不是预测单一确定性结果，以解决这一问题。这使我们能够生成多种多样的目标形状，并避免了平均化的缺陷。我们在模拟和实际机器人上展示了该方法在制造和外科应用启发的机器人任务中的有效性。我们的工作是首个能够为现实世界机器人应用生成多样化、多模态可变形对象目标的生成模型。', 'title_zh': 'DefFusionNet：基于扩散概率模型的可变形物体 manipulation 多模态目标形状学习'}
{'arxiv_id': 'arXiv:2506.18725', 'title': 'TDACloud: Point Cloud Recognition Using Topological Data Analysis', 'authors': 'Anirban Ghosh, Ian Dahlin, Ayan Dutta', 'link': 'https://arxiv.org/abs/2506.18725', 'abstract': 'Point cloud-based object/place recognition remains a problem of interest in applications such as autonomous driving, scene reconstruction, and localization. Extracting meaningful local descriptors from a query point cloud that can be matched with the descriptors of the collected point clouds is a challenging problem. Furthermore, when the query point cloud is noisy or has been transformed (e.g., rotated), it adds to the complexity. To this end, we propose a novel methodology, named TDACloud, using Topological Data Analysis (TDA) for local descriptor extraction from a point cloud, which does not need resource-intensive GPU-based machine learning training. More specifically, we used the ATOL vectorization method to generate vectors for point clouds. Unlike voxelization, our proposed technique can take raw point clouds as inputs and outputs a fixed-size TDA-descriptor vector. To test the quality of the proposed TDACloud technique, we have implemented it on multiple real-world (e.g., Oxford RobotCar, KITTI-360) and realistic (e.g., ShapeNet) point cloud datasets for object and place recognition. We have also tested TDACloud on noisy and transformed test cases where the query point cloud has been scaled, translated, or rotated. Our results demonstrate high recognition accuracies in noisy conditions and large-scale real-world place recognition while outperforming the baselines by up to approximately 14%.', 'abstract_zh': '基于点云的目标/地点识别依然是 autonomous driving、场景重建和定位等应用中的一个研究重点。提取能够与收集的点云描述子匹配的有意义的局部描述子是一个具有挑战性的问题。此外，当查询点云存在噪声或已经过变换（如旋转）时，这会增加难度。为此，我们提出了一种名为 TDACloud 的新型方法，使用拓扑数据分析（TDA）从点云中提取局部描述子，该方法无需进行资源密集型的 GPU 机器学习训练。具体来说，我们使用 ATOL 向量化方法生成点云向量。与体素化不同，我们提出的技术可以直接接受原始点云作为输入，并输出固定大小的 TDA 描述子向量。为了测试所提出的 TDACloud 技术的质量，我们在多个真实世界（如 Oxford RobotCar、KITTI-360）和现实世界（如 ShapeNet）点云数据集上实现了它，用于目标和地点识别。我们还测试了 TDACloud 在噪声和变换的测试案例上，其中查询点云进行了缩放、平移或旋转。我们的结果显示，在嘈杂条件下和大规模实际世界地点识别中具有很高的识别精度，并且相对于基线方法提高了约 14%。', 'title_zh': 'TDACloud：基于拓扑数据分析的点云识别'}
{'arxiv_id': 'arXiv:2506.18697', 'title': 'Safety-Aware Optimal Scheduling for Autonomous Masonry Construction using Collaborative Heterogeneous Aerial Robots', 'authors': 'Marios-Nektarios Stamatopoulos, Shridhar Velhal, Avijit Banerjee, George Nikolakopoulos', 'link': 'https://arxiv.org/abs/2506.18697', 'abstract': "This paper presents a novel high-level task planning and optimal coordination framework for autonomous masonry construction, using a team of heterogeneous aerial robotic workers, consisting of agents with separate skills for brick placement and mortar application. This introduces new challenges in scheduling and coordination, particularly due to the mortar curing deadline required for structural bonding and ensuring the safety constraints among UAVs operating in parallel. To address this, an automated pipeline generates the wall construction plan based on the available bricks while identifying static structural dependencies and potential conflicts for safe operation. The proposed framework optimizes UAV task allocation and execution timing by incorporating dynamically coupled precedence deadline constraints that account for the curing process and static structural dependency constraints, while enforcing spatio-temporal constraints to prevent collisions and ensure safety. The primary objective of the scheduler is to minimize the overall construction makespan while minimizing logistics, traveling time between tasks, and the curing time to maintain both adhesion quality and safe workspace separation. The effectiveness of the proposed method in achieving coordinated and time-efficient aerial masonry construction is extensively validated through Gazebo simulated missions. The results demonstrate the framework's capability to streamline UAV operations, ensuring both structural integrity and safety during the construction process.", 'abstract_zh': '基于异构空中机器人工作者的自主砌体施工高阶任务规划与最优协调框架', 'title_zh': '基于协作异构 aerial 机器人的自働砌筑作业安全意识最优调度'}
{'arxiv_id': 'arXiv:2506.18689', 'title': 'NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed Target Tracking in Unstructured GPS-Denied Environments', 'authors': 'Alessandro Saviolo, Giuseppe Loianno', 'link': 'https://arxiv.org/abs/2506.18689', 'abstract': "Autonomous aerial target tracking in unstructured and GPS-denied environments remains a fundamental challenge in robotics. Many existing methods rely on motion capture systems, pre-mapped scenes, or feature-based localization to ensure safety and control, limiting their deployment in real-world conditions. We introduce NOVA, a fully onboard, object-centric framework that enables robust target tracking and collision-aware navigation using only a stereo camera and an IMU. Rather than constructing a global map or relying on absolute localization, NOVA formulates perception, estimation, and control entirely in the target's reference frame. A tightly integrated stack combines a lightweight object detector with stereo depth completion, followed by histogram-based filtering to infer robust target distances under occlusion and noise. These measurements feed a visual-inertial state estimator that recovers the full 6-DoF pose of the robot relative to the target. A nonlinear model predictive controller (NMPC) plans dynamically feasible trajectories in the target frame. To ensure safety, high-order control barrier functions are constructed online from a compact set of high-risk collision points extracted from depth, enabling real-time obstacle avoidance without maps or dense representations. We validate NOVA across challenging real-world scenarios, including urban mazes, forest trails, and repeated transitions through buildings with intermittent GPS loss and severe lighting changes that disrupt feature-based localization. Each experiment is repeated multiple times under similar conditions to assess resilience, showing consistent and reliable performance. NOVA achieves agile target following at speeds exceeding 50 km/h. These results show that high-speed vision-based tracking is possible in the wild using only onboard sensing, with no reliance on external localization or environment assumptions.", 'abstract_zh': '自主无人机在无结构和GPS受限环境中的目标跟踪仍然是机器人技术中的一个基本挑战。我们介绍了NOVA，一种完全机载、目标中心化的框架，仅使用立体相机和IMU即可实现鲁棒的目标跟踪和避障导航。NOVA 不构建全局地图或依赖绝对定位，而是将感知、估计和控制全部基于目标的参考帧进行。紧凑集成的堆栈结合了轻量级物体检测和立体深度补全，随后通过基于直方图的过滤器来推断在遮挡和噪声下的稳健目标距离。这些测量值输入视觉-惯性状态估计器，恢复机器人相对于目标的全6自由度姿态。非线性模型预测控制器（NMPC）在目标参考帧中规划动态可行的轨迹。为确保安全，NOVA 在线构建高阶控制障碍函数，基于深度信息提取的一组紧凑高风险碰撞点，实现实时障碍物规避，无需地图或密集表示。在城市迷宫、森林小径和间歇性GPS丢失及严重光照变化等具有挑战性的实际场景中验证NOVA，展示了其鲁棒性和可靠性。NOVA 实现了超过50 km/h的敏捷目标跟随。这些结果表明，仅依赖机载传感器，无需外部定位或环境假设，高速基于视觉的目标跟踪在野外是可行的。', 'title_zh': 'NOVA：面向无结构GPS deny环境高速目标跟踪的以对象为中心的视觉自主导航'}
{'arxiv_id': 'arXiv:2506.18583', 'title': 'PG-LIO: Photometric-Geometric fusion for Robust LiDAR-Inertial Odometry', 'authors': 'Nikhil Khedekar, Kostas Alexis', 'link': 'https://arxiv.org/abs/2506.18583', 'abstract': 'LiDAR-Inertial Odometry (LIO) is widely used for accurate state estimation and mapping which is an essential requirement for autonomous robots. Conventional LIO methods typically rely on formulating constraints from the geometric structure sampled by the LiDAR. Hence, in the lack of geometric structure, these tend to become ill-conditioned (degenerate) and fail. Robustness of LIO to such conditions is a necessity for its broader deployment. To address this, we propose PG-LIO, a real-time LIO method that fuses photometric and geometric information sampled by the LiDAR along with inertial constraints from an Inertial Measurement Unit (IMU). This multi-modal information is integrated into a factor graph optimized over a sliding window for real-time operation. We evaluate PG-LIO on multiple datasets that include both geometrically well-conditioned as well as self-similar scenarios. Our method achieves accuracy on par with state-of-the-art LIO in geometrically well-structured settings while significantly improving accuracy in degenerate cases including against methods that also fuse intensity. Notably, we demonstrate only 1 m drift over a 1 km manually piloted aerial trajectory through a geometrically self-similar tunnel at an average speed of 7.5m/s (max speed 10.8 m/s). For the benefit of the community, we shall also release our source code this https URL', 'abstract_zh': '基于 photometric 和几何信息的实时 LiDAR-惯性 odometry (PG-LIO)', 'title_zh': 'PG-LIO：光度-几何融合的鲁棒激光雷达-惯性定位'}
{'arxiv_id': 'arXiv:2506.18580', 'title': 'Learning Point Correspondences In Radar 3D Point Clouds For Radar-Inertial Odometry', 'authors': 'Jan Michalczyk, Stephan Weiss, Jan Steinbrener', 'link': 'https://arxiv.org/abs/2506.18580', 'abstract': 'Using 3D point clouds in odometry estimation in robotics often requires finding a set of correspondences between points in subsequent scans. While there are established methods for point clouds of sufficient quality, state-of-the-art still struggles when this quality drops. Thus, this paper presents a novel learning-based framework for predicting robust point correspondences between pairs of noisy, sparse and unstructured 3D point clouds from a light-weight, low-power, inexpensive, consumer-grade System-on-Chip (SoC) Frequency Modulated Continuous Wave (FMCW) radar sensor. Our network is based on the transformer architecture which allows leveraging the attention mechanism to discover pairs of points in consecutive scans with the greatest mutual affinity. The proposed network is trained in a self-supervised way using set-based multi-label classification cross-entropy loss, where the ground-truth set of matches is found by solving the Linear Sum Assignment (LSA) optimization problem, which avoids tedious hand annotation of the training data. Additionally, posing the loss calculation as multi-label classification permits supervising on point correspondences directly instead of on odometry error, which is not feasible for sparse and noisy data from the SoC radar we use. We evaluate our method with an open-source state-of-the-art Radar-Inertial Odometry (RIO) framework in real-world Unmanned Aerial Vehicle (UAV) flights and with the widely used public Coloradar dataset. Evaluation shows that the proposed method improves the position estimation accuracy by over 14 % and 19 % on average, respectively. The open source code and datasets can be found here: this https URL.', 'abstract_zh': '基于轻量级低功耗低成本消费级SoC调频连续波雷达传感器的预测鲁棒点对应关系学习框架：应用于稀疏无序3D点云 odometer估计算法', 'title_zh': '雷达3D点云中的点对应学习及其在雷达-惯性里程计中的应用'}
{'arxiv_id': 'arXiv:2506.18526', 'title': 'Design, fabrication and control of a cable-driven parallel robot', 'authors': 'Dhruv Sorathiya, Sarthak Sahoo, Vivek Natarajan', 'link': 'https://arxiv.org/abs/2506.18526', 'abstract': 'In cable driven parallel robots (CDPRs), the payload is suspended using a network of cables whose length can be controlled to maneuver the payload within the workspace. Compared to rigid link robots, CDPRs provide better maneuverability due to the flexibility of the cables and consume lesser power due to the high strength-to-weight ratio of the cables. However, amongst other things, the flexibility of the cables and the fact that they can only pull (and not push) render the dynamics of CDPRs complex. Hence advanced modelling paradigms and control algorithms must be developed to fully utilize the potential of CDPRs. Furthermore, given the complex dynamics of CDPRs, the models and control algorithms proposed for them must be validated on experimental setups to ascertain their efficacy in practice. We have recently developed an elaborate experimental setup for a CDPR with three cables and validated elementary open-loop motion planning algorithms on it. In this paper, we describe several aspects of the design and fabrication of our setup, including component selection and assembly, and present our experimental results. Our setup can reproduce complex phenomenon such as the transverse vibration of the cables seen in large CDPRs and will in the future be used to model and control such phenomenon and also to validate more sophisticated motion planning algorithms.', 'abstract_zh': '基于缆索驱动并联机器人的设计与实验研究', 'title_zh': '基于缆索驱动的并联机器人设计、制造与控制'}
{'arxiv_id': 'arXiv:2506.18466', 'title': 'Mirror Eyes: Explainable Human-Robot Interaction at a Glance', 'authors': 'Matti Krüger, Daniel Tanneberg, Chao Wang, Stephan Hasler, Michael Gienger', 'link': 'https://arxiv.org/abs/2506.18466', 'abstract': "The gaze of a person tends to reflect their interest. This work explores what happens when this statement is taken literally and applied to robots. Here we present a robot system that employs a moving robot head with a screen-based eye model that can direct the robot's gaze to points in physical space and present a reflection-like mirror image of the attended region on top of each eye. We conducted a user study with 33 participants, who were asked to instruct the robot to perform pick-and-place tasks, monitor the robot's task execution, and interrupt it in case of erroneous actions. Despite a deliberate lack of instructions about the role of the eyes and a very brief system exposure, participants felt more aware about the robot's information processing, detected erroneous actions earlier, and rated the user experience higher when eye-based mirroring was enabled compared to non-reflective eyes. These results suggest a beneficial and intuitive utilization of the introduced method in cooperative human-robot interaction.", 'abstract_zh': 'Humans的凝视倾向于反映他们的兴趣。本文探讨了将这一陈述字面上应用于机器人会发生什么。本文介绍了一种机器人系统，该系统采用可移动的机器人头部和基于屏幕的眼睛模型，能够将机器人的凝视指向物理空间中的点，并在每个眼睛上方呈现类似镜子的注意区域的反射图像。我们进行了一项包含33名参与者的用户研究，要求参与者指导机器人执行拾取和放置任务，监控机器人的任务执行情况，并在发生错误操作时中断任务。尽管参与者没有关于眼睛作用的明确指示且系统曝光时间很短，但在启用基于眼睛的镜像反射功能时，参与者对机器人信息处理的感知更为清晰，更早地检测到错误操作，并且对用户体验的评价更高。这些结果表明，在协作的人机交互中引入该方法具有有益且直观的利用方式。', 'title_zh': '镜像 eyes：一瞥可懂的人机交互'}
{'arxiv_id': 'arXiv:2506.18454', 'title': 'A Motivational Architecture for Open-Ended Learning Challenges in Robots', 'authors': 'Alejandro Romero, Gianluca Baldassarre, Richard J. Duro, Vieri Giuliano Santucci', 'link': 'https://arxiv.org/abs/2506.18454', 'abstract': 'Developing agents capable of autonomously interacting with complex and dynamic environments, where task structures may change over time and prior knowledge cannot be relied upon, is a key prerequisite for deploying artificial systems in real-world settings. The open-ended learning framework identifies the core challenges for creating such agents, including the ability to autonomously generate new goals, acquire the necessary skills (or curricula of skills) to achieve them, and adapt to non-stationary environments. While many existing works tackles various aspects of these challenges in isolation, few propose integrated solutions that address them simultaneously. In this paper, we introduce H-GRAIL, a hierarchical architecture that, through the use of different typologies of intrinsic motivations and interconnected learning mechanisms, autonomously discovers new goals, learns the required skills for their achievement, generates skill sequences for tackling interdependent tasks, and adapts to non-stationary environments. We tested H-GRAIL in a real robotic scenario, demonstrating how the proposed solutions effectively address the various challenges of open-ended learning.', 'abstract_zh': '自主开发能够与复杂且动态环境进行自主交互的智能代理是将人工系统部署到现实世界的关键前提。开放学习框架识别了创建此类代理的核心挑战，包括自主生成新目标、获取实现这些目标所需技能（或技能课程）以及适应非平稳环境的能力。虽然许多现有工作独立解决了这些挑战的各个方面，但很少有工作同时提出了综合解决方案。本文介绍了H-GRAIL层次结构，通过使用不同类型的内在动机和相互连接的学习机制，自主发现新目标、学习实现这些目标所需的技能、生成处理互赖任务的技能序列，并适应非平稳环境。我们在一个真实的机器人场景中测试了H-GRAIL，验证了所提出解决方案如何有效应对开放学习的各种挑战。', 'title_zh': '一种用于机器人开放性学习挑战的动机架构'}
{'arxiv_id': 'arXiv:2506.18448', 'title': 'GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent System', 'authors': 'Quang Nguyen, Tri Le, Huy Nguyen, Thieu Vo, Tung D. Ta, Baoru Huang, Minh N. Vu, Anh Nguyen', 'link': 'https://arxiv.org/abs/2506.18448', 'abstract': 'Language-driven grasp detection has the potential to revolutionize human-robot interaction by allowing robots to understand and execute grasping tasks based on natural language commands. However, existing approaches face two key challenges. First, they often struggle to interpret complex text instructions or operate ineffectively in densely cluttered environments. Second, most methods require a training or finetuning step to adapt to new domains, limiting their generation in real-world applications. In this paper, we introduce GraspMAS, a new multi-agent system framework for language-driven grasp detection. GraspMAS is designed to reason through ambiguities and improve decision-making in real-world scenarios. Our framework consists of three specialized agents: Planner, responsible for strategizing complex queries; Coder, which generates and executes source code; and Observer, which evaluates the outcomes and provides feedback. Intensive experiments on two large-scale datasets demonstrate that our GraspMAS significantly outperforms existing baselines. Additionally, robot experiments conducted in both simulation and real-world settings further validate the effectiveness of our approach.', 'abstract_zh': '基于语言的抓取检测有望通过使机器人能够理解并基于自然语言命令执行抓取任务来革新人机互动。然而，现有方法面临两个关键挑战。首先，它们往往难以解释复杂的文本指令或在密集的杂乱环境中操作无效。其次，大多数方法需要训练或微调步骤以适应新的领域，限制了其在现实世界中的应用生成能力。在本文中，我们提出了一种新的多agent系统框架GraspMAS，用于基于语言的抓取检测。GraspMAS旨在在真实世界场景中解决歧义并改进决策。我们的框架由三个专门的agent组成：Planner，负责策划复杂的查询；Coder，生成和执行源代码；以及Observer，评估结果并提供反馈。在两个大规模数据集上的密集实验表明，我们的GraspMAS显著优于现有基线。此外，在模拟和现实世界设置中的机器人实验进一步验证了我们方法的有效性。', 'title_zh': 'GraspMAS: 多智能体系统驱动的零样本语言引导抓取检测'}
{'arxiv_id': 'arXiv:2506.18443', 'title': 'Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation', 'authors': 'Yang Lyu, Zhenghao Zou, Yanfeng Li, Chunhui Zhao, Quan Pan', 'link': 'https://arxiv.org/abs/2506.18443', 'abstract': 'Achieving reliable ego motion estimation for agile robots, e.g., aerobatic aircraft, remains challenging because most robot sensors fail to respond timely and clearly to highly dynamic robot motions, often resulting in measurement blurring, distortion, and delays. In this paper, we propose an IMU-free and feature-association-free framework to achieve aggressive ego-motion velocity estimation of a robot platform in highly dynamic scenarios by combining two types of exteroceptive sensors, an event camera and a millimeter wave radar, First, we used instantaneous raw events and Doppler measurements to derive rotational and translational velocities directly. Without a sophisticated association process between measurement frames, the proposed method is more robust in texture-less and structureless environments and is more computationally efficient for edge computing devices. Then, in the back-end, we propose a continuous-time state-space model to fuse the hybrid time-based and event-based measurements to estimate the ego-motion velocity in a fixed-lagged smoother fashion. In the end, we validate our velometer framework extensively in self-collected experiment datasets. The results indicate that our IMU-free and association-free ego motion estimation framework can achieve reliable and efficient velocity output in challenging environments. The source code, illustrative video and dataset are available at this https URL.', 'abstract_zh': '实现敏捷机器人（例如，特技飞行器）可靠的自我运动估计仍然具有挑战性，因为大多数机器人传感器难以及时清晰地响应高度动态的机器人运动，经常导致测量模糊、失真和延迟。本文提出了一种IMU-free和特征关联free的框架，通过结合事件摄像头和毫米波雷达两种外部传感器，来实现高度动态场景下机器人平台的激进自我运动速度估计。首先，我们使用瞬时原始事件和多普勒测量直接推导出旋转和平移速度，避免了高级别的测量帧关联过程，该方法在无纹理和无结构环境中更加鲁棒，并且对于边缘计算设备具有更高的计算效率。然后，在后端，我们提出了一种连续时间状态空间模型，用于融合基于时间和事件的混合测量，以滞后固定滤波方式估计自我运动速度。最后，我们在自行收集的实验数据集中广泛验证了我们提出的velometer框架。结果表明，我们提出的IMU-free和关联free的自我运动估计框架可以在具有挑战性的环境中实现可靠的、高效的速度输出。源代码、示例视频和数据集可在以下链接获取：this https URL。', 'title_zh': '雷达与事件摄像头融合用于敏捷机器人自我运动估计'}
{'arxiv_id': 'arXiv:2506.18410', 'title': 'Integrating Maneuverable Planning and Adaptive Control for Robot Cart-Pushing under Disturbances', 'authors': 'Zhe Zhang, Peijia Xie, Zhirui Sun, Bingyi Xia, Bi-Ke Zhu, Jiankun Wang', 'link': 'https://arxiv.org/abs/2506.18410', 'abstract': "Precise and flexible cart-pushing is a challenging task for mobile robots. The motion constraints during cart-pushing and the robot's redundancy lead to complex motion planning problems, while variable payloads and disturbances present complicated dynamics. In this work, we propose a novel planning and control framework for flexible whole-body coordination and robust adaptive control. Our motion planning method employs a local coordinate representation and a novel kinematic model to solve a nonlinear optimization problem, thereby enhancing motion maneuverability by generating feasible and flexible push poses. Furthermore, we present a disturbance rejection control method to resist disturbances and reduce control errors for the complex control problem without requiring an accurate dynamic model. We validate our method through extensive experiments in simulation and real-world settings, demonstrating its superiority over existing approaches. To the best of our knowledge, this is the first work to systematically evaluate the flexibility and robustness of cart-pushing methods in experiments. The video supplement is available at this https URL.", 'abstract_zh': '精确灵活的载车推举是移动机器人面临的具有挑战性的任务。载车推举过程中的运动约束和机器人的冗余性导致了复杂的运动规划问题，而可变负载和干扰使得动力学特性变得复杂。在本工作中，我们提出了一种新颖的整体运动规划与鲁棒自适应控制框架。我们的运动规划方法采用局部坐标表示和新型动力学模型来求解非线性优化问题，从而通过生成可行且灵活的推举姿态来增强运动灵活性。此外，我们提出了一种抗干扰控制方法，能够在不需要精确动力学模型的情况下抵抗干扰并减少控制误差。我们通过广泛的仿真和现实环境中实验验证了该方法的优越性，据我们所知，这是首次系统评估载车推举方法灵活性和鲁棒性的实验工作。补充视频可在以下链接获取：this https URL。', 'title_zh': '扰动下移动规划与自适应控制集成的机器人杆车推送方法'}
{'arxiv_id': 'arXiv:2506.18365', 'title': 'Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots', 'authors': 'Imene Tarakli, Samuele Vinanzi, Richard Moore, Alessandro Di Nuovo', 'link': 'https://arxiv.org/abs/2506.18365', 'abstract': "Despite growing interest in Learning-by-Teaching (LbT), few studies have explored how this paradigm can be implemented with autonomous, peer-like social robots in real classrooms. Most prior work has relied on scripted or Wizard-of-Oz behaviors, limiting our understanding of how real-time, interactive learning can be supported by artificial agents. This study addresses this gap by introducing Interactive Reinforcement Learning (RL) as a cognitive model for teachable social robots. We conducted two between-subject experiments with 58 primary school children, who either taught a robot or practiced independently on a tablet while learning French vocabulary (memorization) and grammatical rules (inference). The robot, powered by Interactive RL, learned from the child's evaluative feedback. Children in the LbT condition achieved significantly higher retention gains compared to those in the self-practice condition, especially on the grammar task. Learners with lower prior knowledge benefited most from teaching the robot. Behavioural metrics revealed that children adapted their teaching strategies over time and engaged more deeply during inference tasks. This work makes two contributions: (1) it introduces Interactive RL as a pedagogically effective and scalable model for peer-robot learning, and (2) it demonstrates, for the first time, the feasibility of deploying multiple autonomous robots simultaneously in real classrooms. These findings extend theoretical understanding of LbT by showing that social robots can function not only as passive tutees but as adaptive partners that enhance meta-cognitive engagement and long-term learning outcomes.", 'abstract_zh': '尽管对学习教学（LbT）的兴趣日益增长，但鲜有研究探讨如何在这种范式中实现在真实课堂中的自主、类似同侪的社会机器人。大多数前期研究依赖于预设或“巫师背后”的行为，限制了我们对实时互动学习如何由人工代理支持的理解。这项研究通过引入交互增强学习（Interactive RL）作为一种可认知模型用于可教社会机器人，来弥补这一缺陷。我们对58名小学儿童进行了两项之间的实验，其中一组儿童教导机器人，另一组独立使用平板学习法语词汇（记忆）和语法规则（推断）。机器人由交互增强学习驱动，从儿童的评价反馈中学习。在LbT条件下达成的学习保留收益显著高于自我练习条件，尤其是在语法任务上。知识基础较弱的学习者从教导机器人中受益最大。行为度量表明，儿童随着时间的推移调整其教学策略，并在推断任务中更深入地参与。本研究做出了两项贡献：（1）它引入了交互增强学习作为一种教学有效的、可扩展的模式，用于同伴机器人学习；（2）它证明了，首次将多台自主机器人同时部署在真实教室中的可行性。这些发现扩展了对LbT的理解，表明社会机器人不仅可以作为被动的受教者，还可以作为适应性伙伴，增强元认知参与并改善长期学习成果。', 'title_zh': '机器人与共同学习的儿童：通过教授同伴样交互机器人提高知识保留'}
{'arxiv_id': 'arXiv:2506.18355', 'title': 'Robotic Manipulation of a Rotating Chain with Bottom End Fixed', 'authors': 'Qi Jing Chen, Shilin Shan, Quang-Cuong Pham', 'link': 'https://arxiv.org/abs/2506.18355', 'abstract': 'This paper studies the problem of using a robot arm to manipulate a uniformly rotating chain with its bottom end fixed. Existing studies have investigated ideal rotational shapes for practical applications, yet they do not discuss how these shapes can be consistently achieved through manipulation planning. Our work presents a manipulation strategy for stable and consistent shape transitions. We find that the configuration space of such a chain is homeomorphic to a three-dimensional cube. Using this property, we suggest a strategy to manipulate the chain into different configurations, specifically from one rotation mode to another, while taking stability and feasibility into consideration. We demonstrate the effectiveness of our strategy in physical experiments by successfully transitioning from rest to the first two rotation modes. The concepts explored in our work has critical applications in ensuring safety and efficiency of drill string and yarn spinning operations.', 'abstract_zh': '本文研究了使用机器人手臂操纵底端固定的均匀旋转链条的问题。现有研究探讨了这些链条在实际应用中的理想旋转形状，但未讨论如何通过操作规划一致地实现这些形状。本文提出了一种稳定且一致的形状转换操作策略。我们发现此类链条的配置空间同胚于三维立方体。利用这一性质，我们提出了一种策略，在考虑稳定性和可行性的情况下，将链条从一种旋转模式过渡到另一种旋转模式。我们通过物理实验成功地从静止状态过渡到前两种旋转模式，证明了该策略的有效性。本文所探索的概念对于确保钻具和纺纱操作的安全性和效率具有关键应用价值。', 'title_zh': '固定底端旋转链条的机器人操作'}
{'arxiv_id': 'arXiv:2506.18343', 'title': 'TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for Exploration and Rescue Operations', 'authors': 'Kawser Ahmed, Mir Shahriar Fardin, Md Arif Faysal Nayem, Fahim Hafiz, Swakkhar Shatabda', 'link': 'https://arxiv.org/abs/2506.18343', 'abstract': 'The increasing demand for underwater exploration and rescue operations enforces the development of advanced wireless or semi-wireless underwater vessels equipped with manipulator arms. This paper presents the implementation of a semi-wireless underwater vehicle, "TritonZ" equipped with a manipulator arm, tailored for effective underwater exploration and rescue operations. The vehicle\'s compact design enables deployment in different submarine surroundings, addressing the need for wireless systems capable of navigating challenging underwater terrains. The manipulator arm can interact with the environment, allowing the robot to perform sophisticated tasks during exploration and rescue missions in emergency situations. TritonZ is equipped with various sensors such as Pi-Camera, Humidity, and Temperature sensors to send real-time environmental data. Our underwater vehicle controlled using a customized remote controller can navigate efficiently in the water where Pi-Camera enables live streaming of the surroundings. Motion control and video capture are performed simultaneously using this camera. The manipulator arm is designed to perform various tasks, similar to grasping, manipulating, and collecting underwater objects. Experimental results shows the efficacy of the proposed remotely operated vehicle in performing a variety of underwater exploration and rescue tasks. Additionally, the results show that TritonZ can maintain an average of 13.5cm/s with a minimal delay of 2-3 seconds. Furthermore, the vehicle can sustain waves underwater by maintaining its position as well as average velocity. The full project details and source code can be accessed at this link: this https URL', 'abstract_zh': '一种配备 manipulator arm 的半无线水下机器人“TritonZ”在水下探测与救援中的实施与应用', 'title_zh': 'TritonZ：一种配备 manipulator arm 的远程操作水下无人驾驶探测与救援載具'}
{'arxiv_id': 'arXiv:2506.18294', 'title': 'Improvement on LiDAR-Camera Calibration Using Square Targets', 'authors': 'Zhongyuan Li, Honggang Gou, Ping Li, Jiaotong Guo, Mao Ye', 'link': 'https://arxiv.org/abs/2506.18294', 'abstract': 'Precise sensor calibration is critical for autonomous vehicles as a prerequisite for perception algorithms to function properly. Rotation error of one degree can translate to position error of meters in target object detection at large distance, leading to improper reaction of the system or even safety related issues. Many methods for multi-sensor calibration have been proposed. However, there are very few work that comprehensively consider the challenges of the calibration procedure when applied to factory manufacturing pipeline or after-sales service scenarios. In this work, we introduce a fully automatic LiDAR-camera extrinsic calibration algorithm based on targets that is fast, easy to deploy and robust to sensor noises such as missing data. The core of the method include: (1) an automatic multi-stage LiDAR board detection pipeline using only geometry information with no specific material requirement; (2) a fast coarse extrinsic parameter search mechanism that is robust to initial extrinsic errors; (3) a direct optimization algorithm that is robust to sensor noises. We validate the effectiveness of our methods through experiments on data captured in real world scenarios.', 'abstract_zh': '精确传感器标定对于自动驾驶车辆至关重要，是感知算法正常工作的前提。旋转误差一度在远距离目标检测中可能导致米级的位置误差，引发系统不当反应甚至安全问题。许多多传感器标定方法已被提出，但很少有工作全面考虑将标定程序应用于工厂制造管道或售后服务中心场景中的挑战。在本研究中，我们介绍了一种基于目标的全自动化LiDAR-相机外参标定算法，该算法快速、易部署并且对如缺失数据等传感器噪声具有鲁棒性。该方法的核心包括：（1）仅使用几何信息，无需特定材料要求的多阶段LiDAR板自动检测管道；（2）一种对初始外参误差具有鲁棒性的快速粗略外参参数搜索机制；（3）一种对传感器噪声具有鲁棒性的直接优化算法。我们通过在真实场景中捕获的数据进行实验，验证了该方法的有效性。', 'title_zh': 'LiDAR-相机标定基于方靶标的改进'}
{'arxiv_id': 'arXiv:2506.18264', 'title': 'Learning Approach to Efficient Vision-based Active Tracking of a Flying Target by an Unmanned Aerial Vehicle', 'authors': 'Jagadeswara PKV Pothuri, Aditya Bhatt, Prajit KrisshnaKumar, Manaswin Oddiraju, Souma Chowdhury', 'link': 'https://arxiv.org/abs/2506.18264', 'abstract': 'Autonomous tracking of flying aerial objects has important civilian and defense applications, ranging from search and rescue to counter-unmanned aerial systems (counter-UAS). Ground based tracking requires setting up infrastructure, could be range limited, and may not be feasible in remote areas, crowded cities or in dense vegetation areas. Vision based active tracking of aerial objects from another airborne vehicle, e.g., a chaser unmanned aerial vehicle (UAV), promises to fill this important gap, along with serving aerial coordination use cases. Vision-based active tracking by a UAV entails solving two coupled problems: 1) compute-efficient and accurate (target) object detection and target state estimation; and 2) maneuver decisions to ensure that the target remains in the field of view in the future time-steps and favorably positioned for continued detection. As a solution to the first problem, this paper presents a novel integration of standard deep learning based architectures with Kernelized Correlation Filter (KCF) to achieve compute-efficient object detection without compromising accuracy, unlike standalone learning or filtering approaches. The proposed perception framework is validated using a lab-scale setup. For the second problem, to obviate the linearity assumptions and background variations limiting effectiveness of the traditional controllers, we present the use of reinforcement learning to train a neuro-controller for fast computation of velocity maneuvers. New state space, action space and reward formulations are developed for this purpose, and training is performed in simulation using AirSim. The trained model is also tested in AirSim with respect to complex target maneuvers, and is found to outperform a baseline PID control in terms of tracking up-time and average distance maintained (from the target) during tracking.', 'abstract_zh': '基于视觉的自主飞行目标主动跟踪：一种结合标准深度学习架构与核相关滤波器的方法', 'title_zh': '基于无人飞行器的高效视觉主动跟踪学习方法'}
{'arxiv_id': 'arXiv:2506.18256', 'title': 'Robot Tactile Gesture Recognition Based on Full-body Modular E-skin', 'authors': 'Shuo Jiang, Boce Hu, Linfeng Zhao, Lawson L.S. Wong', 'link': 'https://arxiv.org/abs/2506.18256', 'abstract': "With the development of robot electronic skin technology, various tactile sensors, enhanced by AI, are unlocking a new dimension of perception for robots. In this work, we explore how robots equipped with electronic skin can recognize tactile gestures and interpret them as human commands. We developed a modular robot E-skin, composed of multiple irregularly shaped skin patches, which can be assembled to cover the robot's body while capturing real-time pressure and pose data from thousands of sensing points. To process this information, we propose an equivariant graph neural network-based recognizer that efficiently and accurately classifies diverse tactile gestures, including poke, grab, stroke, and double-pat. By mapping the recognized gestures to predefined robot actions, we enable intuitive human-robot interaction purely through tactile input.", 'abstract_zh': '随着机器人电子皮肤技术的发展，增强人工智能的各种触觉传感器正在为机器人解锁新的感知维度。在本研究中，我们探讨了装备有电子皮肤的机器人如何识别触觉手势并将其解释为人机命令。我们开发了一种模块化电子皮肤机器人，由多个不规则形状的皮肤贴片组成，可以组装覆盖机器人身体，并从数千个传感点捕获实时压力和姿态数据。为了处理这些信息，我们提出了一种基于等变图神经网络的识别器，能够高效准确地分类包括戳、抓取、划过和双击等各种触觉手势。通过将识别的手势映射到预定义的机器人动作，我们实现了纯基于触觉输入的人机直观交互。', 'title_zh': '基于全身模块化电子皮肤的机器人触觉手势识别'}
{'arxiv_id': 'arXiv:2506.18212', 'title': 'Haptic-ACT -- Pseudo Oocyte Manipulation by a Robot Using Multimodal Information and Action Chunking with Transformers', 'authors': 'Pedro Miguel Uriguen Eljuri, Hironobu Shibata, Maeyama Katsuyoshi, Yuanyuan Jia, Tadahiro Taniguchi', 'link': 'https://arxiv.org/abs/2506.18212', 'abstract': 'In this paper we introduce Haptic-ACT, an advanced robotic system for pseudo oocyte manipulation, integrating multimodal information and Action Chunking with Transformers (ACT). Traditional automation methods for oocyte transfer rely heavily on visual perception, often requiring human supervision due to biological variability and environmental disturbances. Haptic-ACT enhances ACT by incorporating haptic feedback, enabling real-time grasp failure detection and adaptive correction. Additionally, we introduce a 3D-printed TPU soft gripper to facilitate delicate manipulations. Experimental results demonstrate that Haptic-ACT improves the task success rate, robustness, and adaptability compared to conventional ACT, particularly in dynamic environments. These findings highlight the potential of multimodal learning in robotics for biomedical automation.', 'abstract_zh': '基于触觉的Haptic-ACT：一种集成多模态信息和Action Chunking的先进卵细胞操作机器人系统', 'title_zh': '触觉-ACT -- 基于多模态信息和Transformer动作块的类卵细胞操作机器人技术'}
{'arxiv_id': 'arXiv:2506.18178', 'title': 'Integrating LLMs and Digital Twins for Adaptive Multi-Robot Task Allocation in Construction', 'authors': 'Min Deng, Bo Fu, Lingyao Li, Xi Wang', 'link': 'https://arxiv.org/abs/2506.18178', 'abstract': 'Multi-robot systems are emerging as a promising solution to the growing demand for productivity, safety, and adaptability across industrial sectors. However, effectively coordinating multiple robots in dynamic and uncertain environments, such as construction sites, remains a challenge, particularly due to unpredictable factors like material delays, unexpected site conditions, and weather-induced disruptions. To address these challenges, this study proposes an adaptive task allocation framework that strategically leverages the synergistic potential of Digital Twins, Integer Programming (IP), and Large Language Models (LLMs). The multi-robot task allocation problem is formally defined and solved using an IP model that accounts for task dependencies, robot heterogeneity, scheduling constraints, and re-planning requirements. A mechanism for narrative-driven schedule adaptation is introduced, in which unstructured natural language inputs are interpreted by an LLM, and optimization constraints are autonomously updated, enabling human-in-the-loop flexibility without manual coding. A digital twin-based system has been developed to enable real-time synchronization between physical operations and their digital representations. This closed-loop feedback framework ensures that the system remains dynamic and responsive to ongoing changes on site. A case study demonstrates both the computational efficiency of the optimization algorithm and the reasoning performance of several LLMs, with top-performing models achieving over 97% accuracy in constraint and parameter extraction. The results confirm the practicality, adaptability, and cross-domain applicability of the proposed methods.', 'abstract_zh': '多机器人系统在工业领域日益增长的生产力、安全性和适应性需求中展现出前景，然而在动态和不确定环境中有效协调多机器人仍是一个挑战，尤其是由于材料延误、意外现场条件和天气干扰等不可预测因素。为应对这些挑战，本研究提出了一种适应性任务分配框架，该框架战略性地利用了数字孪生、整数规划和大型语言模型的协同潜力。多机器人任务分配问题被形式化定义并通过考虑任务依赖性、机器人异质性、调度约束和重新规划要求的整数规划模型来求解。引入了一种基于叙述的调度适应机制，在该机制中，非结构化自然语言输入由大型语言模型解释，并自动更新优化约束，从而在无需手动编码的情况下实现人工在环中的灵活性。基于数字孪生的系统已被开发出来，以实现物理操作与其数字表示之间的实时同步。闭环反馈框架确保系统能够动态响应现场的变化。案例研究展示了优化算法的计算效率和若干大型语言模型的推理性能，最佳模型在约束和参数提取方面的准确率超过97%。研究结果证实了所提出方法的实用性、适应性和跨域适用性。', 'title_zh': '集成大规模语言模型和数字孪生以实现适应性施工机器人任务分配'}
{'arxiv_id': 'arXiv:2506.18160', 'title': 'Automated Plan Refinement for Improving Efficiency of Robotic Layup of Composite Sheets', 'authors': 'Rutvik Patel, Alec Kanyuck, Zachary McNulty, Zeren Yu, Lisa Carlson, Vann Heng, Brice Johnson, Satyandra K. Gupta', 'link': 'https://arxiv.org/abs/2506.18160', 'abstract': 'The automation of composite sheet layup is essential to meet the increasing demand for composite materials in various industries. However, draping plans for the robotic layup of composite sheets are not robust. A plan that works well under a certain condition does not work well in a different condition. Changes in operating conditions due to either changes in material properties or working environment may lead a draping plan to exhibit suboptimal performance. In this paper, we present a comprehensive framework aimed at refining plans based on the observed execution performance. Our framework prioritizes the minimization of uncompacted regions while simultaneously improving time efficiency. To achieve this, we integrate human expertise with data-driven decision-making to refine expert-crafted plans for diverse production environments. We conduct experiments to validate the effectiveness of our approach, revealing significant reductions in the number of corrective paths required compared to initial expert-crafted plans. Through a combination of empirical data analysis, action-effectiveness modeling, and search-based refinement, our system achieves superior time efficiency in robotic layup. Experimental results demonstrate the efficacy of our approach in optimizing the layup process, thereby advancing the state-of-the-art in composite manufacturing automation.', 'abstract_zh': '复合板材铺层的自动化是满足各行业对复合材料日益增长需求的关键。然而，复合板材机器人铺层的成型方案不够 robust，在不同条件下表现不佳。由于材料属性或工作环境的变化导致的操作条件变化，可能会使成型方案表现出次优化性能。本文提出了一种全面的框架，旨在根据实际执行性能优化成型方案。该框架优先最小化未压实区域，同时提高时间效率。为此，我们将专业知识与数据驱动的决策相结合，为多种生产环境优化专家设计的成型方案。通过实验验证了该方法的有效性，结果显示所需纠正路径数量显著减少，相比初始专家设计的方案。通过结合经验数据分析、行动有效性建模和基于搜索的优化，该系统在机器人铺层中实现了更高的时间效率。实验结果证明了该方法在优化铺层过程方面的有效性，从而推动了复合材料制造自动化技术的发展。', 'title_zh': '自动计划细化以提高复合板材铺层机器人作业效率'}
{'arxiv_id': 'arXiv:2506.18123', 'title': 'RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies', 'authors': 'Pranav Atreya, Karl Pertsch, Tony Lee, Moo Jin Kim, Arhan Jain, Artur Kuramshin, Clemens Eppner, Cyrus Neary, Edward Hu, Fabio Ramos, Jonathan Tremblay, Kanav Arora, Kirsty Ellis, Luca Macesanu, Matthew Leonard, Meedeum Cho, Ozgur Aslan, Shivin Dass, Jie Wang, Xingfang Yuan, Xuning Yang, Abhishek Gupta, Dinesh Jayaraman, Glen Berseth, Kostas Daniilidis, Roberto Martin-Martin, Youngwoon Lee, Percy Liang, Chelsea Finn, Sergey Levine', 'link': 'https://arxiv.org/abs/2506.18123', 'abstract': "Comprehensive, unbiased, and comparable evaluation of modern generalist policies is uniquely challenging: existing approaches for robot benchmarking typically rely on heavy standardization, either by specifying fixed evaluation tasks and environments, or by hosting centralized ''robot challenges'', and do not readily scale to evaluating generalist policies across a broad range of tasks and environments. In this work, we propose RoboArena, a new approach for scalable evaluation of generalist robot policies in the real world. Instead of standardizing evaluations around fixed tasks, environments, or locations, we propose to crowd-source evaluations across a distributed network of evaluators. Importantly, evaluators can freely choose the tasks and environments they evaluate on, enabling easy scaling of diversity, but they are required to perform double-blind evaluations over pairs of policies. Then, by aggregating preference feedback from pairwise comparisons across diverse tasks and environments, we can derive a ranking of policies. We instantiate our approach across a network of evaluators at seven academic institutions using the DROID robot platform. Through more than 600 pairwise real-robot evaluation episodes across seven generalist policies, we demonstrate that our crowd-sourced approach can more accurately rank the performance of existing generalist policies than conventional, centralized evaluation approaches, while being more scalable, resilient, and trustworthy. We open our evaluation network to the community and hope that it can enable more accessible comparisons of generalist robot policies.", 'abstract_zh': '综合、公正且可比的现代通用型机器人策略评估具有独特挑战：现有机器人基准测试方法通常依赖于高度标准化，要么通过规定固定的任务和环境，要么通过举办集中式的“机器人挑战”，这无法方便地扩展到广泛的任务和环境。在本文中，我们提出RoboArena，一种新的方法，用于在真实世界中 scalable 评估通用型机器人策略。我们摒弃了以固定任务、环境或地点为标准的评估方法，而是提议通过分布在多个评价者之间的网络进行评价。重要的是，评价者可以选择他们要评估的任务和环境，从而便于多样性扩展，但同时需要对策略对进行双盲评估。通过在多样任务和环境下的成对比较中聚合偏好反馈，我们可以得出策略的排名。我们在七所学术机构的评价者网络中使用DROID机器人平台实例化了该方法。通过超过600次针对七种通用型策略的成对真实机器人评估事件，我们展示了我们提出的众包方法相比于传统的集中式评估方法可以更准确地排名现有通用型策略，同时更具可扩展性、可靠性和可信度。我们向社区开放了评估网络，希望它能促进通用型机器人策略的更易访问的比较。', 'title_zh': 'RoboArena: 分布式实际世界通用机器人策略评估'}
{'arxiv_id': 'arXiv:2506.18088', 'title': 'RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation', 'authors': 'Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Qiwei Liang, Zixuan Li, Xianliang Lin, Yiheng Ge, Zhenyu Gu, Weiliang Deng, Yubin Guo, Tian Nian, Xuanbing Xie, Qiangyu Chen, Kailun Su, Tianling Xu, Guodong Liu, Mengkang Hu, Huan-ang Gao, Kaixuan Wang, Zhixuan Liang, Yusen Qin, Xiaokang Yang, Ping Luo, Yao Mu', 'link': 'https://arxiv.org/abs/2506.18088', 'abstract': 'Simulation-based data synthesis has emerged as a powerful paradigm for enhancing real-world robotic manipulation. However, existing synthetic datasets remain insufficient for robust bimanual manipulation due to two challenges: (1) the lack of an efficient, scalable data generation method for novel tasks, and (2) oversimplified simulation environments that fail to capture real-world complexity. We present RoboTwin 2.0, a scalable simulation framework that enables automated, large-scale generation of diverse and realistic data, along with unified evaluation protocols for dual-arm manipulation. We first construct RoboTwin-OD, a large-scale object library comprising 731 instances across 147 categories, each annotated with semantic and manipulation-relevant labels. Building on this foundation, we develop an expert data synthesis pipeline that combines multimodal large language models (MLLMs) with simulation-in-the-loop refinement to generate task-level execution code automatically. To improve sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization along five axes: clutter, lighting, background, tabletop height and language instructions, thereby enhancing data diversity and policy robustness. We instantiate this framework across 50 dual-arm tasks spanning five robot embodiments, and pre-collect over 100,000 domain-randomized expert trajectories. Empirical results show a 10.9% gain in code generation success and improved generalization to novel real-world scenarios. A VLA model fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%) on unseen scene real-world tasks, while zero-shot models trained solely on our synthetic data achieve a 228% relative gain, highlighting strong generalization without real-world supervision. We release the data generator, benchmark, dataset, and code to support scalable research in robust bimanual manipulation.', 'abstract_zh': '基于模拟的数据合成已成为增强现实世界双臂操作的强大范式。然而，现有的合成数据集由于两个挑战而不足以实现稳健的双臂操作：(1) 缺乏一种高效且可扩展的方法来生成新型任务的数据；(2) 简化的模拟环境无法捕捉现实世界的复杂性。我们提出了RoboTwin 2.0，一种可扩展的模拟框架，能够自动化、大规模生成多样且逼真的数据，并提供统一的双臂操作评估协议。首先，我们构建了RoboTwin-OD，一个包含731个实例的大型物体库，覆盖147个类别，并为每个实例标注了语义和操作相关的标签。在此基础上，我们开发了一个专家级数据合成流水线，该流水线结合了多模态大规模语言模型（MLLMs）和仿真在环改进，以自动生成任务级执行代码。为了提高模拟到现实的转移，RoboTwin 2.0引入了沿五个维度的结构化领域随机化：杂乱、照明、背景、桌面高度和语言指令，从而增强数据多样性并提高策略鲁棒性。我们在50个双臂任务上实例化了该框架，覆盖了五个机器人实体，并预先收集了超过100,000条领域随机化的专家轨迹。实证结果显示代码生成成功率提高了10.9%，并且在新颖的实际场景中具有更好的泛化能力。在我们数据集上微调的VLA模型在未见过的场景实际任务中实现了367%的相对改进（42.0% vs. 9.0%），而仅在我们合成数据上训练的零样本模型实现了228%的相对改进，突显了在没有现实世界监督的情况下强大的泛化能力。我们发布了数据生成器、基准测试、数据集和代码，以支持在稳健双臂操作方面的可扩展研究。', 'title_zh': 'RoboTwin 2.0：一种具有强领域随机化的大规模数据生成器和基准测试平台，用于稳健的双臂机器人操作'}
{'arxiv_id': 'arXiv:2506.18040', 'title': 'StereoTacTip: Vision-based Tactile Sensing with Biomimetic Skin-Marker Arrangements', 'authors': 'Chenghua Lu, Kailuan Tang, Xueming Hui, Haoran Li, Saekwang Nam, Nathan F. Lepora', 'link': 'https://arxiv.org/abs/2506.18040', 'abstract': 'Vision-Based Tactile Sensors (VBTSs) stand out for their superior performance due to their high-information content output. Recently, marker-based VBTSs have been shown to give accurate geometry reconstruction when using stereo cameras. \\uhl{However, many marker-based VBTSs use complex biomimetic skin-marker arrangements, which presents issues for the geometric reconstruction of the skin surface from the markers}. Here we investigate how the marker-based skin morphology affects stereo vision-based tactile sensing, using a novel VBTS called the StereoTacTip. To achieve accurate geometry reconstruction, we introduce: (i) stereo marker matching and tracking using a novel Delaunay-Triangulation-Ring-Coding algorithm; (ii) a refractive depth correction model that corrects the depth distortion caused by refraction in the internal media; (iii) a skin surface correction model from the marker positions, relying on an inverse calculation of normals to the skin surface; and (iv)~methods for geometry reconstruction over multiple contacts. To demonstrate these findings, we reconstruct topographic terrains on a large 3D map. Even though contributions (i) and (ii) were developed for biomimetic markers, they should improve the performance of all marker-based VBTSs. Overall, this work illustrates that a thorough understanding and evaluation of the morphologically-complex skin and marker-based tactile sensor principles are crucial for obtaining accurate geometric information.', 'abstract_zh': '基于视觉的触觉传感器（VBTS）由于其高信息含量的输出而表现出色。最近，基于标记的VBTS在使用立体摄像头时已被证明可以提供准确的几何重构。然而，许多基于标记的VBTS使用复杂的仿生标记布局，这给从标记重构皮肤表面带来了问题。我们通过一种新颖的立体触觉传感装置StereoTacTip研究了基于标记的皮肤形态对立体视觉触觉感知的影响。为了实现准确的几何重构，我们引入了：（i）使用新颖的Delaunay-Triangulation-Ring-Coding算法实现的立体标记匹配和跟踪；（ii）折射深度矫正模型，用于校正由内部介质折射引起的深度失真；（iii）基于标记位置的皮肤表面矫正模型，依赖于对皮肤表面法线的逆计算；（iv）在多接触点进行几何重构的方法。通过在大尺寸3D地图上重构地形来验证这些发现，尽管（i）和（ii）的贡献最初是为仿生标记开发的，但它们也应改善所有基于标记的VBTS的性能。总体而言，本工作表明，对形态复杂皮肤和基于标记的触觉传感器原理进行彻底的理解和评估对于获取准确的几何信息至关重要。', 'title_zh': 'StereoTacTip：基于生物模仿皮肤标记排列的视觉触觉感知'}
{'arxiv_id': 'arXiv:2506.18016', 'title': 'ADA-DPM: A Neural Descriptors-based Adaptive Noise Point Filtering Strategy for SLAM', 'authors': 'Yongxin Shao, Binrui Wang, Aihong Tan', 'link': 'https://arxiv.org/abs/2506.18016', 'abstract': 'LiDAR SLAM has demonstrated significant application value in various fields, including mobile robot navigation and high-precision map construction. However, existing methods often need to make a trade-off between positioning accuracy and system robustness when faced with dynamic object interference, point cloud noise, and unstructured environments. To address this challenge, we propose an adaptive noise filtering SLAM strategy-ADA-DPM, achieving excellent preference in both aspects. We design the Dynamic Segmentation Head to predict the category of feature points belonging to dynamic points, to eliminate dynamic feature points; design the Global Importance Scoring Head to adaptively select feature points with higher contribution and features while suppressing noise interference; and construct the Cross Layer Intra-Graph Convolution Module (GLI-GCN) to fuse multi-scale neighborhood structures, thereby enhancing the discriminative ability of overlapping features. Finally, to further validate the effectiveness of our method, we tested it on several publicly available datasets and achieved outstanding results.', 'abstract_zh': 'LiDAR SLAM在移动机器人导航和高精度地图构建等领域展现了重要的应用价值。然而，现有方法在面对动态物体干扰、点云噪声和非结构化环境时往往需要在定位精度和系统鲁棒性之间做出权衡。为应对这一挑战，我们提出了一种自适应噪声过滤SLAM策略-ADA-DPM，在两个方面都取得了卓越的效果。我们设计了动态分割头来预测特征点所属的动态点类别，以消除动态特征点；设计了全局重要性评分头以自适应地选择具有更高贡献度的特征点及其特征，抑制噪声干扰；构建了跨层内图卷积模块（GLI-GCN）以融合多尺度邻域结构，从而增强重叠特征的区分能力。最后，为了进一步验证我们方法的有效性，我们在多个公开数据集上进行了测试，并取得了出色的结果。', 'title_zh': 'ADA-DPM：基于神经描述子的适应性噪声点过滤策略用于SLAM'}
{'arxiv_id': 'arXiv:2506.17994', 'title': 'Newtonian and Lagrangian Neural Networks: A Comparison Towards Efficient Inverse Dynamics Identification', 'authors': 'Minh Trinh, Andreas René Geist, Josefine Monnet, Stefan Vilceanu, Sebastian Trimpe, Christian Brecher', 'link': 'https://arxiv.org/abs/2506.17994', 'abstract': 'Accurate inverse dynamics models are essential tools for controlling industrial robots. Recent research combines neural network regression with inverse dynamics formulations of the Newton-Euler and the Euler-Lagrange equations of motion, resulting in so-called Newtonian neural networks and Lagrangian neural networks, respectively. These physics-informed models seek to identify unknowns in the analytical equations from data. Despite their potential, current literature lacks guidance on choosing between Lagrangian and Newtonian networks. In this study, we show that when motor torques are estimated instead of directly measuring joint torques, Lagrangian networks prove less effective compared to Newtonian networks as they do not explicitly model dissipative torques. The performance of these models is compared to neural network regression on data of a MABI MAX 100 industrial robot.', 'abstract_zh': '准确的动力学逆模型是控制工业机器人的关键工具。最近的研究结合了神经网络回归与牛顿-欧拉和欧拉-拉格朗日运动方程的动力学逆形式，分别产生了所谓的牛顿神经网络和拉格朗日神经网络。这些基于物理的模型旨在从数据中识别解析方程中的未知量。尽管具有潜在优势，当前文献缺乏关于选择拉格朗日网络和牛顿网络之间差异的指导。在本研究中，我们表明，在估计电机扭矩而非直接测量关节扭矩时，拉格朗日网络的效果不如牛顿网络，因为它们没有明确 modeling 消散扭矩。这些模型的性能与针对 MABI MAX 100 工业机器人的数据进行的神经网络回归进行比较。', 'title_zh': '牛顿 Neural 网络和拉格朗日 Neural 网络：高效逆动力学识别的比较'}
{'arxiv_id': 'arXiv:2506.17960', 'title': 'GeNIE: A Generalizable Navigation System for In-the-Wild Environments', 'authors': 'Jiaming Wang, Diwen Liu, Jizhuo Chen, Jiaxuan Da, Nuowen Qian, Tram Minh Man, Harold Soh', 'link': 'https://arxiv.org/abs/2506.17960', 'abstract': 'Reliable navigation in unstructured, real-world environments remains a significant challenge for embodied agents, especially when operating across diverse terrains, weather conditions, and sensor configurations. In this paper, we introduce GeNIE (Generalizable Navigation System for In-the-Wild Environments), a robust navigation framework designed for global deployment. GeNIE integrates a generalizable traversability prediction model built on SAM2 with a novel path fusion strategy that enhances planning stability in noisy and ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at ICRA 2025, where it was evaluated across six countries spanning three continents. GeNIE took first place and achieved 79% of the maximum possible score, outperforming the second-best team by 17%, and completed the entire competition without a single human intervention. These results set a new benchmark for robust, generalizable outdoor robot navigation. We will release the codebase, pretrained model weights, and newly curated datasets to support future research in real-world navigation.', 'abstract_zh': '可靠的导航系统在未结构化现实环境中的广泛应用仍然是嵌入式代理面临的重大挑战，尤其是在不同地形、天气条件和传感器配置下操作时。本文介绍了GeNIE（适用于野外环境的可泛化导航系统），这是一种设计用于全球部署的稳健导航框架。GeNIE 结合了基于 SAM2 的可泛化通过性预测模型和一种新颖的路径融合策略，该策略在嘈杂和模糊环境中增强了规划的稳定性。我们在 2025 年 ICRA 地球漫游车挑战赛 (ERC) 中部署了 GeNIE，它在跨越三大洲六个国家的评估中取得了优异成绩，获得了 79% 的最大可能得分，比第二名高出 17%，并且在整个比赛中没有人的干预。这些结果为稳健的、可泛化的室外机器人导航设定了新的基准。我们将会发布代码库、预训练模型权重以及新编纂的 数据集，以支持未来在实际导航领域的研究。', 'title_zh': 'GeNIE: 一种适用于野生环境的导航系统'}
{'arxiv_id': 'arXiv:2506.17902', 'title': 'Embedded Flexible Circumferential Sensing for Real-Time Intraoperative Environmental Perception in Continuum Robots', 'authors': 'Peiyu Luo, Shilong Yao, Yuhan Chen, Max Q.-H. Meng', 'link': 'https://arxiv.org/abs/2506.17902', 'abstract': 'Continuum robots have been widely adopted in robot-assisted minimally invasive surgery (RMIS) because of their compact size and high flexibility. However, their proprioceptive capabilities remain limited, particularly in narrow lumens, where lack of environmental awareness can lead to unintended tissue contact and surgical risks. To address this challenge, this work proposes a flexible annular sensor structure integrated around the vertebral disks of continuum robots. The proposed design enables real-time environmental mapping by estimating the distance between the robotic disks and the surrounding tissue, thereby facilitating safer operation through advanced control strategies. The experiment has proven that its accuracy in obstacle detection can reach 0.19 mm. Fabricated using flexible printed circuit (FPC) technology, the sensor demonstrates a modular and cost-effective design with compact dimensions and low noise interference. Its adaptable parameters allow compatibility with various continuum robot architectures, offering a promising solution for enhancing intraoperative perception and control in surgical robotics.', 'abstract_zh': '连续体机器人在辅助微创手术中的紧凑尺寸和高柔性使其得以广泛应用。然而，它们的本体感受能力仍然有限，特别是在狭窄的管腔中，缺乏环境感知会导致意外组织接触和手术风险。为解决这一挑战，本工作提出了一种集成在连续体机器人椎间盘周围的柔性环形传感器结构。所提出的设计通过估计机器人椎间盘与周围组织之间的距离，实现实时环境映射，从而通过先进的控制策略促进更安全的操作。实验结果证明，其在障碍物检测中的精度可达0.19毫米。该传感器基于柔性印刷电路（FPC）技术制造，具有模块化、成本效益高、紧凑尺寸和低噪声干扰的特点。其可调参数使其能够与各种连续体机器人架构兼容，为增强外科手术机器人内的感知和控制提供了前景广阔的有效解决方案。', 'title_zh': 'Continuum 机器人实时内术中环境感知的嵌入式柔性环形传感'}
{'arxiv_id': 'arXiv:2506.17868', 'title': 'Geometric Contact Flows: Contactomorphisms for Dynamics and Control', 'authors': 'Andrea Testa, Søren Hauberg, Tamim Asfour, Leonel Rozo', 'link': 'https://arxiv.org/abs/2506.17868', 'abstract': "Accurately modeling and predicting complex dynamical systems, particularly those involving force exchange and dissipation, is crucial for applications ranging from fluid dynamics to robotics, but presents significant challenges due to the intricate interplay of geometric constraints and energy transfer. This paper introduces Geometric Contact Flows (GFC), a novel framework leveraging Riemannian and Contact geometry as inductive biases to learn such systems. GCF constructs a latent contact Hamiltonian model encoding desirable properties like stability or energy conservation. An ensemble of contactomorphisms then adapts this model to the target dynamics while preserving these properties. This ensemble allows for uncertainty-aware geodesics that attract the system's behavior toward the data support, enabling robust generalization and adaptation to unseen scenarios. Experiments on learning dynamics for physical systems and for controlling robots on interaction tasks demonstrate the effectiveness of our approach.", 'abstract_zh': '准确建模和预测涉及力交换和耗散的复杂动力系统对于从流体动力学到机器人技术的应用至关重要，但由于几何约束和能量传递的复杂相互作用，这提出了重大挑战。本文提出了一种新颖的框架——几何接触流（GFC），该框架利用黎曼几何和接触几何作为归纳偏置来学习此类系统。GFC 构建了一个潜在的接触哈密尔顿模型，编码了如稳定性和能量守恒等 desirable 性质。一组接触同构随后将该模型适应目标动力学，同时保持这些性质。该集合允许不确定性意识下的测地线吸引系统的行为朝向数据支持，从而实现稳健的泛化和对未见场景的适应。在物理系统动力学习和交互任务中控制机器人方面的实验展示了我们方法的有效性。', 'title_zh': '几何接触流动：接触omorphic变换的动力学与控制'}
{'arxiv_id': 'arXiv:2506.17842', 'title': 'Generative Grasp Detection and Estimation with Concept Learning-based Safety Criteria', 'authors': 'Al-Harith Farhad, Khalil Abuibaid, Christiane Plociennik, Achim Wagner, Martin Ruskowski', 'link': 'https://arxiv.org/abs/2506.17842', 'abstract': 'Neural networks are often regarded as universal equations that can estimate any function. This flexibility, however, comes with the drawback of high complexity, rendering these networks into black box models, which is especially relevant in safety-centric applications. To that end, we propose a pipeline for a collaborative robot (Cobot) grasping algorithm that detects relevant tools and generates the optimal grasp. To increase the transparency and reliability of this approach, we integrate an explainable AI method that provides an explanation for the underlying prediction of a model by extracting the learned features and correlating them to corresponding classes from the input. These concepts are then used as additional criteria to ensure the safe handling of work tools. In this paper, we show the consistency of this approach and the criterion for improving the handover position. This approach was tested in an industrial environment, where a camera system was set up to enable a robot to pick up certain tools and objects.', 'abstract_zh': '神经网络通常被视为通用方程，能够估计任何函数。这种灵活性虽然强大，但也伴随着高复杂性的缺点，使这些网络成为黑盒模型，特别是在安全至关重要的应用中尤为显著。为此，我们提出了一种协作机器人（Cobot）抓取算法的流水线，该算法能够检测相关工具并生成最优抓取。为了提高这种方法的透明度和可靠性，我们整合了一种可解释的AI方法，通过提取学习特征并将其与输入对应的类别相关联，为模型的底层预测提供解释。这些概念随后被用作额外的标准，以确保工具的安全处理。在本文中，我们展示了这种 Approach 的一致性和改进交接位置的准则。该 Approach 在工业环境中进行了测试，设置了一个摄像头系统，使机器人能够抓取某些工具和物体。', 'title_zh': '基于概念学习的安全性标准下的生成性握持检测与估计'}
{'arxiv_id': 'arXiv:2506.17832', 'title': 'Leveling the Playing Field: Carefully Comparing Classical and Learned Controllers for Quadrotor Trajectory Tracking', 'authors': 'Pratik Kunapuli, Jake Welde, Dinesh Jayaraman, Vijay Kumar', 'link': 'https://arxiv.org/abs/2506.17832', 'abstract': 'Learning-based control approaches like reinforcement learning (RL) have recently produced a slew of impressive results for tasks like quadrotor trajectory tracking and drone racing. Naturally, it is common to demonstrate the advantages of these new controllers against established methods like analytical controllers. We observe, however, that reliably comparing the performance of such very different classes of controllers is more complicated than might appear at first sight. As a case study, we take up the problem of agile tracking of an end-effector for a quadrotor with a fixed arm. We develop a set of best practices for synthesizing the best-in-class RL and geometric controllers (GC) for benchmarking. In the process, we resolve widespread RL-favoring biases in prior studies that provide asymmetric access to: (1) the task definition, in the form of an objective function, (2) representative datasets, for parameter optimization, and (3) feedforward information, describing the desired future trajectory. The resulting findings are the following: our improvements to the experimental protocol for comparing learned and classical controllers are critical, and each of the above asymmetries can yield misleading conclusions. Prior works have claimed that RL outperforms GC, but we find the gaps between the two controller classes are much smaller than previously published when accounting for symmetric comparisons. Geometric control achieves lower steady-state error than RL, while RL has better transient performance, resulting in GC performing better in relatively slow or less agile tasks, but RL performing better when greater agility is required. Finally, we open-source implementations of geometric and RL controllers for these aerial vehicles, implementing best practices for future development. Website and code is available at this https URL', 'abstract_zh': '基于学习的控制方法，如强化学习（RL），最近在四旋翼轨迹跟踪和无人机竞速等任务中取得了令人印象深刻的结果。然而，将这些新型控制器与传统方法如分析控制器进行对比时，可靠地比较不同类别的控制器性能要比表面看起来复杂得多。作为案例研究，我们探讨了四旋翼固定臂末端执行器敏捷跟踪的问题。我们开发了一套最佳实践，用于合成用于基准测试的最佳强化学习（RL）和几何控制器（GC）。在这一过程中，我们解决了先前研究中广泛存在的RL有利倾向偏差，这些偏差在以下方面提供了不对等的访问权限：（1）任务定义形式的目标函数，（2）代表性数据集用于参数优化，以及（3）前馈信息，描述期望的未来轨迹。研究结果如下：改进实验协议对于比较学习和经典控制器至关重要，上述每一项不对等均可导致误导性结论。先前的研究声称RL优于GC，但我们将对称比较后发现，两类控制器之间的差距要比之前公布的要小得多。几何控制在稳态误差方面优于RL，而RL在瞬态性能方面更佳，因此在相对缓慢或不太敏捷的任务中，几何控制表现更优；但在需要更高敏捷性的任务中，RL表现更佳。最后，我们开源了这些空中车辆的几何控制和RL控制实施代码，遵循最佳实践，以促进未来的发展。相关网站和代码可在以下链接访问：this https URL。', 'title_zh': '平衡竞争環境：仔细比较经典控制器与学习控制器在四旋翼轨迹跟踪中的表现'}
{'arxiv_id': 'arXiv:2506.17831', 'title': 'Engagement and Disclosures in LLM-Powered Cognitive Behavioral Therapy Exercises: A Factorial Design Comparing the Influence of a Robot vs. Chatbot Over Time', 'authors': 'Mina Kian, Mingyu Zong, Katrin Fischer, Anna-Maria Velentza, Abhyuday Singh, Kaleen Shrestha, Pau Sang, Shriya Upadhyay, Wallace Browning, Misha Arif Faruki, Sébastien M. R. Arnold, Bhaskar Krishnamachari, Maja Matarić', 'link': 'https://arxiv.org/abs/2506.17831', 'abstract': 'Many researchers are working to address the worldwide mental health crisis by developing therapeutic technologies that increase the accessibility of care, including leveraging large language model (LLM) capabilities in chatbots and socially assistive robots (SARs) used for therapeutic applications. Yet, the effects of these technologies over time remain unexplored. In this study, we use a factorial design to assess the impact of embodiment and time spent engaging in therapeutic exercises on participant disclosures. We assessed transcripts gathered from a two-week study in which 26 university student participants completed daily interactive Cognitive Behavioral Therapy (CBT) exercises in their residences using either an LLM-powered SAR or a disembodied chatbot. We evaluated the levels of active engagement and high intimacy of their disclosures (opinions, judgments, and emotions) during each session and over time. Our findings show significant interactions between time and embodiment for both outcome measures: participant engagement and intimacy increased over time in the physical robot condition, while both measures decreased in the chatbot condition.', 'abstract_zh': '许多研究者致力于通过开发提高护理可及性的治疗技术来应对全球心理健康危机，包括利用大型语言模型（LLM）能力的聊天机器人和社会辅助机器人（SARs）用于治疗应用。然而，这些技术的效果随时间的变化尚未被探索。在本研究中，我们采用因子设计评估实体形态和参与治疗练习时间对参与者披露的影响。我们评估了在为期两周的研究中收集的对话记录，该研究中26名大学学生参与者在住所使用LLM驱动的SAR或 disembodied聊天机器人完成每日互动的认知行为疗法（CBT）练习。我们评估了每个会话及随时间发展的积极参与程度和高亲密性（意见、判断和情绪）水平。我们的研究发现，对于两个结果指标，时间和实体形态之间存在显著的交互作用：在实体机器人条件下，参与者的积极参与程度和亲密性随时间增加，而在聊天机器人条件下，这两个指标随时间下降。', 'title_zh': '基于机器人 versus 聊天机器人影响的因变量设计：时间进程中认知行为疗法练习中的参与与披露比较研究'}
{'arxiv_id': 'arXiv:2506.17823', 'title': 'Learning to Dock: A Simulation-based Study on Closing the Sim2Real Gap in Autonomous Underwater Docking', 'authors': 'Kevin Chang, Rakesh Vivekanandan, Noah Pragin, Sean Bullock, Geoffrey Hollinger', 'link': 'https://arxiv.org/abs/2506.17823', 'abstract': 'Autonomous Underwater Vehicle (AUV) docking in dynamic and uncertain environments is a critical challenge for underwater robotics. Reinforcement learning is a promising method for developing robust controllers, but the disparity between training simulations and the real world, or the sim2real gap, often leads to a significant deterioration in performance. In this work, we perform a simulation study on reducing the sim2real gap in autonomous docking through training various controllers and then evaluating them under realistic disturbances. In particular, we focus on the real-world challenge of docking under different payloads that are potentially outside the original training distribution. We explore existing methods for improving robustness including randomization techniques and history-conditioned controllers. Our findings provide insights into mitigating the sim2real gap when training docking controllers. Furthermore, our work indicates areas of future research that may be beneficial to the marine robotics community.', 'abstract_zh': '自主 underwater 机器人 (AUV) 在动态和不确定环境下的自主靠岸是一个关键挑战。通过训练各种控制器然后在实际干扰下评估他们以减少 sim2real 间隙是一种有前景的方法。本研究重点关注在不同潜在超出原始训练分布的载荷下进行靠岸的实际挑战。我们探讨了包括随机化技术和基于历史条件的控制器在内的现有方法以提高鲁棒性。我们的研究结果提供了在训练靠岸控制器时缓解 sim2real 间隙的见解。此外，我们的工作指出了对未来研究的有益方向，这些方向可能对海洋机器人社区有益。', 'title_zh': '基于模拟的自主水下 docking 技能学习：缩小 Sim2Real 间隙的研究'}
{'arxiv_id': 'arXiv:2506.17811', 'title': 'RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models', 'authors': 'Jacky Kwok, Christopher Agia, Rohan Sinha, Matt Foutter, Shulu Li, Ion Stoica, Azalia Mirhoseini, Marco Pavone', 'link': 'https://arxiv.org/abs/2506.17811', 'abstract': 'Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in visuomotor control, yet ensuring their robustness in unstructured real-world environments remains a persistent challenge. In this paper, we investigate test-time scaling through the lens of sampling and verification as means to enhance the robustness and generalization of VLAs. We first demonstrate that the relationship between action error and the number of generated samples follows an exponentiated power law across a range of VLAs, indicating the existence of inference-time scaling laws. Building on these insights, we introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment, RoboMonkey samples a small set of actions from a VLA, applies Gaussian perturbation and majority voting to construct an action proposal distribution, and then uses a Vision Language Model (VLM)-based verifier to select the optimal action. We propose a synthetic data generation pipeline for training such VLM-based action verifiers, and demonstrate that scaling the synthetic dataset consistently improves verification and downstream accuracy. Through extensive simulated and hardware experiments, we show that pairing existing VLAs with RoboMonkey yields significant performance gains, achieving a 25% absolute improvement on out-of-distribution tasks and 8% on in-distribution tasks. Additionally, when adapting to new robot setups, we show that fine-tuning both VLAs and action verifiers yields a 7% performance increase compared to fine-tuning VLAs alone.', 'abstract_zh': '基于视觉-语言-行动模型测试时扩展的RoboMonkey框架：提升鲁棒性和泛化能力', 'title_zh': 'RoboMonkey: 扩大规模测试时采样与验证在视觉-语言-行动模型中的应用'}
{'arxiv_id': 'arXiv:2506.17775', 'title': 'Optimizing Exploration with a New Uncertainty Framework for Active SLAM Systems', 'authors': 'Sebastian Sansoni, Javier Gimenez, Gastón Castro, Santiago Tosetti, Flavio Craparo', 'link': 'https://arxiv.org/abs/2506.17775', 'abstract': "Accurate reconstruction of the environment is a central goal of Simultaneous Localization and Mapping (SLAM) systems. However, the agent's trajectory can significantly affect estimation accuracy. This paper presents a new method to model map uncertainty in Active SLAM systems using an Uncertainty Map (UM). The UM uses probability distributions to capture where the map is uncertain, allowing Uncertainty Frontiers (UF) to be defined as key exploration-exploitation objectives and potential stopping criteria. In addition, the method introduces the Signed Relative Entropy (SiREn), based on the Kullback-Leibler divergence, to measure both coverage and uncertainty together. This helps balance exploration and exploitation through an easy-to-understand parameter. Unlike methods that depend on particular SLAM setups, the proposed approach is compatible with different types of sensors, such as cameras, LiDARs, and multi-sensor fusion. It also addresses common problems in exploration planning and stopping conditions. Furthermore, integrating this map modeling approach with a UF-based planning system enables the agent to autonomously explore open spaces, a behavior not previously observed in the Active SLAM literature. Code and implementation details are available as a ROS node, and all generated data are openly available for public use, facilitating broader adoption and validation of the proposed approach.", 'abstract_zh': '基于不确定性图的主动SLAM中环境重建不确定性建模方法', 'title_zh': '基于新不确定性框架的主动SLAM系统式探索优化'}
{'arxiv_id': 'arXiv:2506.17639', 'title': 'RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models', 'authors': 'Yuxuan Chen, Xiao Li', 'link': 'https://arxiv.org/abs/2506.17639', 'abstract': "Vision-Language-Action models (VLA) have demonstrated remarkable capabilities and promising potential in solving complex robotic manipulation tasks. However, their substantial parameter sizes and high inference latency pose significant challenges for real-world deployment, particularly on resource-constrained robotic platforms. To address this issue, we begin by conducting an extensive empirical study to explore the effectiveness of model compression techniques when applied to VLAs. Building on the insights gained from these preliminary experiments, we propose RLRC, a three-stage recovery method for compressed VLAs, including structured pruning, performance recovery based on SFT and RL, and further quantization. RLRC achieves up to an 8x reduction in memory usage and a 2.3x improvement in inference throughput, while maintaining or even surpassing the original VLA's task success rate. Extensive experiments show that RLRC consistently outperforms existing compression baselines, demonstrating strong potential for on-device deployment of VLAs. Project website: this https URL", 'abstract_zh': 'Vision-Language-Action模型（VLA）在解决复杂机器人操作任务方面展现了显著的能力和广阔的潜力。然而，其庞大的参数量和高推理延迟给实际部署造成了重大挑战，特别是在资源受限的机器人平台上。为解决这一问题，我们首先进行了一项广泛的经验研究，探索在VLA中应用模型压缩技术的有效性。基于初步实验获得的见解，我们提出了一种名为RLRC的三层恢复方法，包括结构化剪枝、基于SFT和RL的性能恢复以及进一步的量化。RLRC实现了内存使用最多8倍的减少和推理 throughput 2.3倍的提升，同时保持甚至超越了原始VLA的任务成功率。广泛的实验表明，RLRC在各方面的性能都优于现有压缩基线，展示了在设备端部署VLAs的强大潜力。项目网站: [这个链接](this https URL)。', 'title_zh': '基于强化学习的压缩视觉-语言-动作模型恢复方法'}
{'arxiv_id': 'arXiv:2506.17624', 'title': 'Imitation Learning for Active Neck Motion Enabling Robot Manipulation beyond the Field of View', 'authors': 'Koki Nakagawa, Yoshiyuki Ohmura, Yasuo Kuniyoshi', 'link': 'https://arxiv.org/abs/2506.17624', 'abstract': 'Most prior research in deep imitation learning has predominantly utilized fixed cameras for image input, which constrains task performance to the predefined field of view. However, enabling a robot to actively maneuver its neck can significantly expand the scope of imitation learning to encompass a wider variety of tasks and expressive actions such as neck gestures. To facilitate imitation learning in robots capable of neck movement while simultaneously performing object manipulation, we propose a teaching system that systematically collects datasets incorporating neck movements while minimizing discomfort caused by dynamic viewpoints during teleoperation. In addition, we present a novel network model for learning manipulation tasks including active neck motion. Experimental results showed that our model can achieve a high success rate of around 90\\%, regardless of the distraction from the viewpoint variations by active neck motion. Moreover, the proposed model proved particularly effective in challenging scenarios, such as when objects were situated at the periphery or beyond the standard field of view, where traditional models struggled. The proposed approach contributes to the efficiency of dataset collection and extends the applicability of imitation learning to more complex and dynamic scenarios.', 'abstract_zh': '基于颈部运动的深度模仿学习教学系统与网络模型研究', 'title_zh': '基于模仿学习的主动颈部运动使机器人 manipulation 超出视域范围'}
{'arxiv_id': 'arXiv:2506.17601', 'title': 'Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in Space, Where Failure Is Not An Option', 'authors': 'Rohan Thakker, Adarsh Patnaik, Vince Kurtz, Jonas Frey, Jonathan Becktor, Sangwoo Moon, Rob Royce, Marcel Kaufmann, Georgios Georgakis, Pascal Roth, Joel Burdick, Marco Hutter, Shehryar Khattak', 'link': 'https://arxiv.org/abs/2506.17601', 'abstract': 'Safe, reliable navigation in extreme, unfamiliar terrain is required for future robotic space exploration missions. Recent generative-AI methods learn semantically aware navigation policies from large, cross-embodiment datasets, but offer limited safety guarantees. Inspired by human cognitive science, we propose a risk-guided diffusion framework that fuses a fast, learned "System-1" with a slow, physics-based "System-2", sharing computation at both training and inference to couple adaptability with formal safety. Hardware experiments conducted at the NASA JPL\'s Mars-analog facility, Mars Yard, show that our approach reduces failure rates by up to $4\\times$ while matching the goal-reaching performance of learning-based robotic models by leveraging inference-time compute without any additional training.', 'abstract_zh': '极端未知地形中安全可靠的导航是未来机器人太空探测任务的要求。受人类认知科学启发，我们提出了一种风险指导扩散框架，融合快速学习的“系统-1”与缓慢的物理基于的“系统-2”，在训练和推理阶段共享计算，以实现适应性与正式安全性的结合。在NASA JPL火星模拟设施火星 yard 的硬件实验中，我们的方法在不进行额外训练的情况下，通过利用推理时的计算能力将失败率降低多达4倍，同时匹配基于学习的机器人模型的目标到达性能。', 'title_zh': '风险导向的扩散：向着在失败不容忍的太空部署机器人基础模型努力'}
{'arxiv_id': 'arXiv:2506.17516', 'title': 'EASE: Embodied Active Event Perception via Self-Supervised Energy Minimization', 'authors': 'Zhou Chen, Sanjoy Kundu, Harsimran S. Baweja, Sathyanarayanan N. Aakur', 'link': 'https://arxiv.org/abs/2506.17516', 'abstract': "Active event perception, the ability to dynamically detect, track, and summarize events in real time, is essential for embodied intelligence in tasks such as human-AI collaboration, assistive robotics, and autonomous navigation. However, existing approaches often depend on predefined action spaces, annotated datasets, and extrinsic rewards, limiting their adaptability and scalability in dynamic, real-world scenarios. Inspired by cognitive theories of event perception and predictive coding, we propose EASE, a self-supervised framework that unifies spatiotemporal representation learning and embodied control through free energy minimization. EASE leverages prediction errors and entropy as intrinsic signals to segment events, summarize observations, and actively track salient actors, operating without explicit annotations or external rewards. By coupling a generative perception model with an action-driven control policy, EASE dynamically aligns predictions with observations, enabling emergent behaviors such as implicit memory, target continuity, and adaptability to novel environments. Extensive evaluations in simulation and real-world settings demonstrate EASE's ability to achieve privacy-preserving and scalable event perception, providing a robust foundation for embodied systems in unscripted, dynamic tasks.", 'abstract_zh': '主动事件感知能力，即在实时环境下动态检测、跟踪和总结事件的能力，是诸如人机协作、辅助机器人和自主导航等任务中体现式智能的关键。然而，现有方法往往依赖预定义的动作空间、标注数据集和外在奖励，限制了其在动态真实世界场景中的适应性和扩展性。受事件感知认知理论和预测编码的启发，我们提出了一种自监督框架EASE，通过最小化自由能统一时空表示学习和体现式控制。EASE 利用预测误差和熵作为内在信号进行事件分割、总结观察和主动跟踪显著主体，无需明确标注或外部奖励。通过结合生成感知模型与动作驱动的控制策略，EASE 动态对齐预测与观察，使潜在记忆、目标连续性和对新型环境的适应能力等涌现行为成为可能。在仿真和真实世界环境中的广泛评估表明，EASE 能实现隐私保护和可扩展的事件感知，为无脚本动态任务中的体现式系统提供坚实基础。', 'title_zh': 'EASE: 通过自我监督的能量最小化实现沉浸式主动事件感知'}
{'arxiv_id': 'arXiv:2506.17488', 'title': 'Online Adaptation for Flying Quadrotors in Tight Formations', 'authors': 'Pei-An Hsieh, Kong Yao Chee, M. Ani Hsieh', 'link': 'https://arxiv.org/abs/2506.17488', 'abstract': 'The task of flying in tight formations is challenging for teams of quadrotors because the complex aerodynamic wake interactions can destabilize individual team members as well as the team. Furthermore, these aerodynamic effects are highly nonlinear and fast-paced, making them difficult to model and predict. To overcome these challenges, we present L1 KNODE-DW MPC, an adaptive, mixed expert learning based control framework that allows individual quadrotors to accurately track trajectories while adapting to time-varying aerodynamic interactions during formation flights. We evaluate L1 KNODE-DW MPC in two different three-quadrotor formations and show that it outperforms several MPC baselines. Our results show that the proposed framework is capable of enabling the three-quadrotor team to remain vertically aligned in close proximity throughout the flight. These findings show that the L1 adaptive module compensates for unmodeled disturbances most effectively when paired with an accurate dynamics model. A video showcasing our framework and the physical experiments is available here: this https URL', 'abstract_zh': '适用于编队飞行的四旋翼无人机紧密编队飞行任务具有挑战性，因为复杂的气动尾流相互作用可能会导致个体成员乃至整个团队失稳。此外，这些气动效应是非线性的且变化快速，难以建模和预测。为克服这些挑战，我们提出了一种适应性混合专家学习控制框架L1 KNODE-DW MPC，它使个体四旋翼无人机能够在编队飞行过程中适应时间变化的气动相互作用的同时精确跟踪轨迹。我们在两种不同的三四旋翼无人机编队中评估了L1 KNODE-DW MPC，并证明它优于几种MPC基准算法。我们的结果表明，所提出的框架能够使三四旋翼无人机团队在整个飞行过程中保持垂直对齐但紧密接近。这些发现表明，L1自适应模块与准确的动力学模型配对时，能够最有效地补偿未建模的干扰。展示我们框架和物理实验的视频可在以下链接查看：this https URL。', 'title_zh': '在线适应性控制在紧凑编队中的四旋翼飞行器'}
{'arxiv_id': 'arXiv:2506.17486', 'title': 'Distilling On-device Language Models for Robot Planning with Minimal Human Intervention', 'authors': 'Zachary Ravichandran, Ignacio Hounie, Fernando Cladera, Alejandro Ribeiro, George J. Pappas, Vijay Kumar', 'link': 'https://arxiv.org/abs/2506.17486', 'abstract': "Large language models (LLMs) provide robots with powerful contextual reasoning abilities and a natural human interface. Yet, current LLM-enabled robots typically depend on cloud-hosted models, limiting their usability in environments with unreliable communication infrastructure, such as outdoor or industrial settings. We present PRISM, a framework for distilling small language model (SLM)-enabled robot planners that run on-device with minimal human supervision. Starting from an existing LLM-enabled planner, PRISM automatically synthesizes diverse tasks and environments, elicits plans from the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in replacement of the source model. We apply PRISM to three LLM-enabled planners for mapping and exploration, manipulation, and household assistance, and we demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of GPT-4o's performance to over 93% - using only synthetic data. We further demonstrate that the distilled planners generalize across heterogeneous robotic platforms (ground and aerial) and diverse environments (indoor and outdoor). We release all software, trained models, and datasets at this https URL.", 'abstract_zh': 'PRISM：一种用于设备端运行的小语言模型机器人规划框架', 'title_zh': '在设备端精炼用于机器人规划的语言模型，以最少的人工介入'}
{'arxiv_id': 'arXiv:2506.17473', 'title': 'DiLQR: Differentiable Iterative Linear Quadratic Regulator via Implicit Differentiation', 'authors': 'Shuyuan Wang, Philip D. Loewen, Michael Forbes, Bhushan Gopaluni, Wei Pan', 'link': 'https://arxiv.org/abs/2506.17473', 'abstract': 'While differentiable control has emerged as a powerful paradigm combining model-free flexibility with model-based efficiency, the iterative Linear Quadratic Regulator (iLQR) remains underexplored as a differentiable component. The scalability of differentiating through extended iterations and horizons poses significant challenges, hindering iLQR from being an effective differentiable controller. This paper introduces DiLQR, a framework that facilitates differentiation through iLQR, allowing it to serve as a trainable and differentiable module, either as or within a neural network. A novel aspect of this framework is the analytical solution that it provides for the gradient of an iLQR controller through implicit differentiation, which ensures a constant backward cost regardless of iteration, while producing an accurate gradient. We evaluate our framework on imitation tasks on famous control benchmarks. Our analytical method demonstrates superior computational performance, achieving up to 128x speedup and a minimum of 21x speedup compared to automatic differentiation. Our method also demonstrates superior learning performance ($10^6$x) compared to traditional neural network policies and better model loss with differentiable controllers that lack exact analytical gradients. Furthermore, we integrate our module into a larger network with visual inputs to demonstrate the capacity of our method for high-dimensional, fully end-to-end tasks. Codes can be found on the project homepage this https URL.', 'abstract_zh': '不同iable DiLQR：一种可微分的迭代线性二次调节器框架', 'title_zh': '可微迭代线性二次调节器：通过隐式求导实现'}
{'arxiv_id': 'arXiv:2506.17462', 'title': 'General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting', 'authors': 'Bernard Lange, Anil Yildiz, Mansur Arief, Shehryar Khattak, Mykel Kochenderfer, Georgios Georgakis', 'link': 'https://arxiv.org/abs/2506.17462', 'abstract': 'Developing general-purpose navigation policies for unknown environments remains a core challenge in robotics. Most existing systems rely on task-specific neural networks and fixed data flows, limiting generalizability. Large Vision-Language Models (LVLMs) offer a promising alternative by embedding human-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot integrations typically depend on pre-mapped spaces, hard-coded representations, and myopic exploration. We introduce the Agentic Robotic Navigation Architecture (ARNA), a general-purpose navigation framework that equips an LVLM-based agent with a library of perception, reasoning, and navigation tools available within modern robotic stacks. At runtime, the agent autonomously defines and executes task-specific workflows that iteratively query the robotic modules, reason over multimodal inputs, and select appropriate navigation actions. This approach enables robust navigation and reasoning in previously unmapped environments, providing a new perspective on robotic stack design. Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves state-of-the-art performance, demonstrating effective exploration, navigation, and embodied question answering without relying on handcrafted plans, fixed input representations, or pre-existing maps.', 'abstract_zh': '基于大型视觉语言模型的通用导航架构：在未知环境中的自主感知、推理与导航', 'title_zh': '基于LVLM- orchestration的通用机器人导航'}
{'arxiv_id': 'arXiv:2506.17458', 'title': 'Kinematic Model Optimization via Differentiable Contact Manifold for In-Space Manipulation', 'authors': 'Abhay Negi, Omey M. Manyar, Satyandra K. Gupta', 'link': 'https://arxiv.org/abs/2506.17458', 'abstract': 'Robotic manipulation in space is essential for emerging applications such as debris removal and in-space servicing, assembly, and manufacturing (ISAM). A key requirement for these tasks is the ability to perform precise, contact-rich manipulation under significant uncertainty. In particular, thermal-induced deformation of manipulator links and temperature-dependent encoder bias introduce kinematic parameter errors that significantly degrade end-effector accuracy. Traditional calibration techniques rely on external sensors or dedicated calibration procedures, which can be infeasible or risky in dynamic, space-based operational scenarios.\nThis paper proposes a novel method for kinematic parameter estimation that only requires encoder measurements and binary contact detection. The approach focuses on estimating link thermal deformation strain and joint encoder biases by leveraging information of the contact manifold - the set of relative SE(3) poses at which contact between the manipulator and environment occurs. We present two core contributions: (1) a differentiable, learning-based model of the contact manifold, and (2) an optimization-based algorithm for estimating kinematic parameters from encoder measurements at contact instances. By enabling parameter estimation using only encoder measurements and contact detection, this method provides a robust, interpretable, and data-efficient solution for safe and accurate manipulation in the challenging conditions of space.', 'abstract_zh': '空间机器人操作对于新兴应用如太空碎片移除、在轨服务、组装与制造（ISAM）至关重要。这些任务的关键要求是在显著不确定性条件下进行精确的、接触丰富的操作。特别是在空间操作场景中，由热效应引起的机械臂支链变形和温度依赖性的编码器偏移引入了会造成末端执行器精度显著下降的运动学参数误差。传统校准技术依赖于外部传感器或专门的校准程序，但在动态的空间操作场景中，这些方法可能不切实际或存在风险。\n\n本文提出了一种仅需使用编码器测量和二元接触检测的新方法，用于估计运动学参数。该方法侧重于通过利用接触流形（即机械臂与环境接触所发生相对SE(3)姿态的集合）信息，估计支链的热变形应变和关节编码器偏移。本文的核心贡献包括：（1）一种可微的学习型接触流形模型，（2）一种基于优化的算法，用于估计接触时刻的编码器测量值所对应的动力学参数。通过仅使用编码器测量和接触检测来估计参数，该方法为在复杂空间条件下实现安全和精确的操作提供了鲁棒、可解释且数据高效的解决方案。', 'title_zh': '基于可微接触流形的空间操作动力学模型优化'}
{'arxiv_id': 'arXiv:2506.17378', 'title': 'A workflow for generating synthetic LiDAR datasets in simulation environments', 'authors': 'Abhishek Phadke, Shakib Mahmud Dipto, Pratip Rana', 'link': 'https://arxiv.org/abs/2506.17378', 'abstract': 'This paper presents a simulation workflow for generating synthetic LiDAR datasets to support autonomous vehicle perception, robotics research, and sensor security analysis. Leveraging the CoppeliaSim simulation environment and its Python API, we integrate time-of-flight LiDAR, image sensors, and two dimensional scanners onto a simulated vehicle platform operating within an urban scenario. The workflow automates data capture, storage, and annotation across multiple formats (PCD, PLY, CSV), producing synchronized multimodal datasets with ground truth pose information. We validate the pipeline by generating large-scale point clouds and corresponding RGB and depth imagery. The study examines potential security vulnerabilities in LiDAR data, such as adversarial point injection and spoofing attacks, and demonstrates how synthetic datasets can facilitate the evaluation of defense strategies. Finally, limitations related to environmental realism, sensor noise modeling, and computational scalability are discussed, and future research directions, such as incorporating weather effects, real-world terrain models, and advanced scanner configurations, are proposed. The workflow provides a versatile, reproducible framework for generating high-fidelity synthetic LiDAR datasets to advance perception research and strengthen sensor security in autonomous systems. Documentation and examples accompany this framework; samples of animated cloud returns and image sensor data can be found at this Link.', 'abstract_zh': '本文提出了一个用于生成合成LiDAR数据集的模拟工作流，以支持自主车辆感知、机器人研究和传感器安全分析。利用CoppeliaSim模拟环境及其Python API，我们将飞行时间LiDAR、图像传感器和二维扫描仪集成到一个配备模拟车辆平台的都市场景中。该工作流自动捕获、存储和标注多种格式（PCD、PLY、CSV）的数据，生成同步多模态数据集，并包含真实姿态信息。通过生成大规模点云和相应的RGB及深度图像，验证了该管道。研究探讨了LiDAR数据中的潜在安全漏洞，如对抗性点注入和欺骗攻击，并展示了合成数据集如何促进防御策略的评估。最后，讨论了环境现实性、传感器噪声建模和计算扩展性的相关限制，并提出了结合天气效应、现实地形模型和高级扫描器配置等未来研究方向。该工作流提供了一个多功能且可复现的框架，用于生成高保真合成LiDAR数据集，促进感知研究并增强自主系统中的传感器安全性。框架包括文档和示例；动画点云返回和图像传感器数据样本可在该链接处找到。', 'title_zh': '仿真环境生成合成LiDAR数据集的工作流'}
{'arxiv_id': 'arXiv:2506.17328', 'title': 'Reflective VLM Planning for Dual-Arm Desktop Cleaning: Bridging Open-Vocabulary Perception and Precise Manipulation', 'authors': 'Yufan Liu, Yi Wu, Gweneth Ge, Haoliang Cheng, Rui Liu', 'link': 'https://arxiv.org/abs/2506.17328', 'abstract': 'Desktop cleaning demands open-vocabulary recognition and precise manipulation for heterogeneous debris. We propose a hierarchical framework integrating reflective Vision-Language Model (VLM) planning with dual-arm execution via structured scene representation. Grounded-SAM2 facilitates open-vocabulary detection, while a memory-augmented VLM generates, critiques, and revises manipulation sequences. These sequences are converted into parametric trajectories for five primitives executed by coordinated Franka arms. Evaluated in simulated scenarios, our system achieving 87.2% task completion, a 28.8% improvement over static VLM and 36.2% over single-arm baselines. Structured memory integration proves crucial for robust, generalizable manipulation while maintaining real-time control performance.', 'abstract_zh': '桌面清理需求开放式词汇识别和精确操作，针对异构碎片。我们提出了一种层次框架，结合反射性视觉语言模型（VLM）规划与基于结构化场景表示的双臂执行。Grounded-SAM2 支持开放式词汇检测，而记忆增强的 VLM 生成、批判和修订操作序列。这些序列被转换为由协调的法兰卡手臂执行的五种原始操作的参数轨迹。在模拟场景中评估，我们的系统实现了 87.2% 的任务完成率，分别比静态 VLM 提高了 28.8%，比单臂基线提高了 36.2%。结构化记忆集成对于保持实时控制性能的同时实现 robust 和通用的操作至关重要。', 'title_zh': '面向双臂桌面清洁的反射型VLM规划：开放词汇感知与精确操作的桥梁'}
{'arxiv_id': 'arXiv:2506.18798', 'title': 'OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by Object-Centric Awareness', 'authors': 'Helin Cao, Sven Behnke', 'link': 'https://arxiv.org/abs/2506.18798', 'abstract': 'Autonomous driving perception faces significant challenges due to occlusions and incomplete scene data in the environment. To overcome these issues, the task of semantic occupancy prediction (SOP) is proposed, which aims to jointly infer both the geometry and semantic labels of a scene from images. However, conventional camera-based methods typically treat all categories equally and primarily rely on local features, leading to suboptimal predictions, especially for dynamic foreground objects. To address this, we propose Object-Centric SOP (OC-SOP), a framework that integrates high-level object-centric cues extracted via a detection branch into the semantic occupancy prediction pipeline. This object-centric integration significantly enhances the prediction accuracy for foreground objects and achieves state-of-the-art performance among all categories on SemanticKITTI.', 'abstract_zh': '自主驾驶感知因环境中的遮挡和不完整场景数据而面临重大挑战。为了克服这些问题，提出了语义占用预测（SOP）任务，该任务旨在从图像中联合推断场景的几何结构和语义标签。然而，传统的基于相机的方法通常将所有类别同等对待，并主要依赖于局部特征，导致预测效果不佳，尤其是对于动态前景物体。为了解决这个问题，我们提出了一种基于对象的SOP（OC-SOP）框架，该框架将通过检测分支提取的高层次对象中心线索集成到语义占用预测管道中。这种对象中心的集成显著提高了前景物体的预测精度，并在SemanticKITTI上实现了所有类别中的最佳性能。', 'title_zh': '基于对象中心意识增强的视觉导向3D语义占用预测OC-SOP'}
{'arxiv_id': 'arXiv:2506.18785', 'title': 'SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving', 'authors': 'Helin Cao, Rafael Materla, Sven Behnke', 'link': 'https://arxiv.org/abs/2506.18785', 'abstract': 'Perception systems in autonomous driving rely on sensors such as LiDAR and cameras to perceive the 3D environment. However, due to occlusions and data sparsity, these sensors often fail to capture complete information. Semantic Occupancy Prediction (SOP) addresses this challenge by inferring both occupancy and semantics of unobserved regions. Existing transformer-based SOP methods lack explicit modeling of spatial structure in attention computation, resulting in limited geometric awareness and poor performance in sparse or occluded areas. To this end, we propose Spatially-aware Window Attention (SWA), a novel mechanism that incorporates local spatial context into attention. SWA significantly improves scene completion and achieves state-of-the-art results on LiDAR-based SOP benchmarks. We further validate its generality by integrating SWA into a camera-based SOP pipeline, where it also yields consistent gains across modalities.', 'abstract_zh': '基于感知系统在自动驾驶中的应用，传感器如LiDAR和摄像头用于感知3D环境。然而，由于遮挡和数据稀疏性，这些传感器往往无法捕捉完整信息。语义占用预测（SOP）通过推断未观察区域的占用情况和语义解决这一挑战。现有的基于变压器的SOP方法在注意力计算中缺乏对空间结构的显式建模，导致几何意识有限，在稀疏或遮挡区域表现不佳。为此，我们提出了一种新的机制——空间感知窗口注意力（SWA），该机制将局部空间上下文融入注意力计算中。SWA显著提高了场景完成度，并在基于LiDAR的SOP基准测试中取得了最佳结果。我们进一步通过将SWA集成到基于摄像头的SOP管道中验证了其普适性，在不同模态中也获得了稳定收益。', 'title_zh': 'SWA-SOP: 空间感知窗口注意力在自主驾驶中的语义占有预测'}
{'arxiv_id': 'arXiv:2506.18749', 'title': 'BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and Embodied Learning for Enhanced Mobility', 'authors': 'Abdul Basit, Maha Nawaz, Muhammad Shafique', 'link': 'https://arxiv.org/abs/2506.18749', 'abstract': 'Non-invasive brain-computer interfaces (BCIs) have the potential to enable intuitive control of prosthetic limbs for individuals with upper limb amputations. However, existing EEG-based control systems face challenges related to signal noise, classification accuracy, and real-time adaptability. In this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic system that integrates ensemble learning-based EEG classification with a human-in-the-loop (HITL) correction framework for enhanced responsiveness. Unlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims to interpret EEG-driven motor intent, enabling movement control without reliance on residual muscle activity. To improve classification robustness, BRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework, achieving a classification accuracy of 96% across test subjects. EEG signals are preprocessed using a bandpass filter (0.5-45 Hz), Independent Component Analysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature extraction to minimize contamination from electromyographic (EMG) and electrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic speech recognition (ASR) to facilitate intuitive mode switching between different degrees of freedom (DOF) in the prosthetic arm. The system operates in real time, with a response latency of 150 ms, leveraging Lab Streaming Layer (LSL) networking for synchronized data acquisition. The system is evaluated on an in-house fabricated prosthetic arm and on multiple participants highlighting the generalizability across users. The system is optimized for low-power embedded deployment, ensuring practical real-world application beyond high-performance computing environments. Our results indicate that BRAVE offers a promising step towards robust, real-time, non-invasive prosthetic control.', 'abstract_zh': '非侵入式脑机接口(BCIs)有望赋能上肢截肢个体直观控制假肢。然而，现有的基于EEG的控制系统面临着信号噪声、分类准确度以及实时适应性方面的挑战。在这项工作中，我们介绍了BRAVE，一种结合了基于集成学习的EEG分类和人力在环(HITL)校正框架的混合EEG和语音控制假肢系统，以增强响应性。与传统的肌电图(EMG)-基于假肢控制不同，BRAVE旨在解读由EEG驱动的运动意图，从而实现无需依赖残余肌肉活动的运动控制。为了提高分类的鲁棒性，BRAVE结合了LSTM、CNN和随机森林模型，在集成框架中实现了96%的分类准确率。EEG信号通过带通滤波器(0.5-45 Hz)、独立成分分析(ICA)去除伪迹以及共空间模式(CSP)特征提取进行预处理，以最小化来自肌电图(EMG)和眼电图(EOG)信号的污染。此外，BRAVE还采用了自动语音识别(ASR)来促进假肢臂不同自由度(DOF)之间的直观模式切换。该系统在实时操作中响应延迟为150毫秒，利用Lab Streaming Layer (LSL)网络实现同步数据采集。系统在一款自制的假肢臂和多名参与者上进行了评估，展示了其跨用户的普遍适用性。系统优化了低功耗嵌入式部署，确保其在高性能计算环境之外的实际应用。我们的结果表明，BRAVE为实现鲁棒的、实时的非侵入式假肢控制提供了一个有前景的步骤。', 'title_zh': 'BRAVE: 基于脑控制的集成语音和具身学习的假肢手臂以提高移动性'}
{'arxiv_id': 'arXiv:2506.18737', 'title': 'USVTrack: USV-Based 4D Radar-Camera Tracking Dataset for Autonomous Driving in Inland Waterways', 'authors': 'Shanliang Yao, Runwei Guan, Yi Ni, Sen Xu, Yong Yue, Xiaohui Zhu, Ryan Wen Liu', 'link': 'https://arxiv.org/abs/2506.18737', 'abstract': 'Object tracking in inland waterways plays a crucial role in safe and cost-effective applications, including waterborne transportation, sightseeing tours, environmental monitoring and surface rescue. Our Unmanned Surface Vehicle (USV), equipped with a 4D radar, a monocular camera, a GPS, and an IMU, delivers robust tracking capabilities in complex waterborne environments. By leveraging these sensors, our USV collected comprehensive object tracking data, which we present as USVTrack, the first 4D radar-camera tracking dataset tailored for autonomous driving in new generation waterborne transportation systems. Our USVTrack dataset presents rich scenarios, featuring diverse various waterways, varying times of day, and multiple weather and lighting conditions. Moreover, we present a simple but effective radar-camera matching method, termed RCM, which can be plugged into popular two-stage association trackers. Experimental results utilizing RCM demonstrate the effectiveness of the radar-camera matching in improving object tracking accuracy and reliability for autonomous driving in waterborne environments. The USVTrack dataset is public on this https URL.', 'abstract_zh': '内河水域的目标跟踪对于安全和成本有效的应用至关重要，包括水运交通、旅游观光、环境监测和水面救援。我们的无人驾驶水面车辆（USV）配备4D雷达、单目相机、GPS和IMU，能够在复杂水运环境中提供稳健的目标跟踪能力。通过利用这些传感器，我们的USV收集了全面的目标跟踪数据，并作为USVTrack发布，这是首个针对新一代水运交通系统自主驾驶设计的4D雷达-相机跟踪数据集。USVTrack数据集提供了丰富的场景，包括多种类型的水道、不同的时间段以及多变的天气和光照条件。此外，我们提出了一种简单而有效的雷达-相机匹配方法，称为RCM，它可以与流行的两阶段关联跟踪器结合使用。利用RCM的实验结果表明，雷达-相机匹配在提高水运环境中自主驾驶的目标跟踪准确性和可靠性方面的有效性。USVTrack数据集已在此处 https://公开。', 'title_zh': 'USVTrack: 基于USV的 inland 水域 4D 雷达--camera 跟踪数据集用于自主驾驶'}
{'arxiv_id': 'arXiv:2506.18721', 'title': 'Including Semantic Information via Word Embeddings for Skeleton-based Action Recognition', 'authors': 'Dustin Aganian, Erik Franze, Markus Eisenbach, Horst-Michael Gross', 'link': 'https://arxiv.org/abs/2506.18721', 'abstract': 'Effective human action recognition is widely used for cobots in Industry 4.0 to assist in assembly tasks. However, conventional skeleton-based methods often lose keypoint semantics, limiting their effectiveness in complex interactions. In this work, we introduce a novel approach to skeleton-based action recognition that enriches input representations by leveraging word embeddings to encode semantic information. Our method replaces one-hot encodings with semantic volumes, enabling the model to capture meaningful relationships between joints and objects. Through extensive experiments on multiple assembly datasets, we demonstrate that our approach significantly improves classification performance, and enhances generalization capabilities by simultaneously supporting different skeleton types and object classes. Our findings highlight the potential of incorporating semantic information to enhance skeleton-based action recognition in dynamic and diverse environments.', 'abstract_zh': '基于单词嵌入的骨架语义增强动作识别在工业4.0协作机器人装配任务中的应用', 'title_zh': '通过词嵌入纳入语义信息以实现基于骨架的动作识别'}
{'arxiv_id': 'arXiv:2506.18678', 'title': 'MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation', 'authors': 'Tianchen Deng, Guole Shen, Xun Chen, Shenghai Yuan, Hongming Shen, Guohao Peng, Zhenyu Wu, Jingchuan Wang, Lihua Xie, Danwei Wang, Hesheng Wang, Weidong Chen', 'link': 'https://arxiv.org/abs/2506.18678', 'abstract': 'Neural implicit scene representations have recently shown promising results in dense visual SLAM. However, existing implicit SLAM algorithms are constrained to single-agent scenarios, and fall difficulties in large-scale scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks cannot meet the constraints of communication bandwidth. To this end, we propose the first distributed multi-agent collaborative neural SLAM framework with hybrid scene representation, distributed camera tracking, intra-to-inter loop closure, and online distillation for multiple submap fusion. A novel triplane-grid joint scene representation method is proposed to improve scene reconstruction. A novel intra-to-inter loop closure method is designed to achieve local (single-agent) and global (multi-agent) consistency. We also design a novel online distillation method to fuse the information of different submaps to achieve global consistency. Furthermore, to the best of our knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that provides both continuous-time trajectories groundtruth and high-accuracy 3D meshes groundtruth. To this end, we propose the first real-world Dense slam (DES) dataset covering both single-agent and multi-agent scenarios, ranging from small rooms to large-scale outdoor scenes, with high-accuracy ground truth for both 3D mesh and continuous-time camera trajectory. This dataset can advance the development of the research in both SLAM, 3D reconstruction, and visual foundation model. Experiments on various datasets demonstrate the superiority of the proposed method in both mapping, tracking, and communication. The dataset and code will open-source on this https URL.', 'abstract_zh': '分布式多智能体协作神经SLAM框架：混合场景表示、分布式相机追踪、局部到全局回环闭合及在线蒸馏', 'title_zh': 'MCN-SLAM: 多agent协作神经SLAM与混合隐式神经场景表示'}
{'arxiv_id': 'arXiv:2506.18317', 'title': 'Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi Ranging', 'authors': 'Emerson Sie, Enguang Fan, Federico Cifuentes-Urtubey, Deepak Vasisht', 'link': 'https://arxiv.org/abs/2506.18317', 'abstract': 'Indoor localization opens the path to potentially transformative applications. Although many indoor localization methods have been proposed over the years, they remain too impractical for widespread deployment in the real world. In this paper, we introduce PeepLoc, a deployable and scalable Wi-Fi-based solution for indoor localization that relies only on pre-existing devices and infrastructure. Specifically, PeepLoc works on any mobile device with an unmodified Wi-Fi transceiver and in any indoor environment with a sufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the core of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain non-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel bootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and crowdsourcing to opportunistically initialize pre-existing APs as anchor points within an environment. We implement PeepLoc using commodity hardware and evaluate it extensively across 4 campus buildings. We show PeepLoc leads to a mean and median positional error of 3.41 m and 3.06 m respectively, which is superior to existing deployed indoor localization systems and is competitive with commodity GPS in outdoor environments.', 'abstract_zh': '基于Wi-Fi的室内定位技术PeepLoc：可部署且可扩展的解决方案', 'title_zh': '基于非协作Wi-Fi测距的众包室内广泛定位'}
{'arxiv_id': 'arXiv:2506.18234', 'title': 'Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning', 'authors': 'Yue Li, Meng Tian, Dechang Zhu, Jiangtong Zhu, Zhenyu Lin, Zhiwei Xiong, Xinhai Zhao', 'link': 'https://arxiv.org/abs/2506.18234', 'abstract': 'Large vision-language models (VLMs) for autonomous driving (AD) are evolving beyond perception and cognition tasks toward motion planning. However, we identify two critical challenges in this direction: (1) VLMs tend to learn shortcuts by relying heavily on history input information, achieving seemingly strong planning results without genuinely understanding the visual inputs; and (2) the chain-ofthought (COT) reasoning processes are always misaligned with the motion planning outcomes, and how to effectively leverage the complex reasoning capability to enhance planning remains largely underexplored. In this paper, we start from a small-scale domain-specific VLM and propose Drive-R1 designed to bridges the scenario reasoning and motion planning for AD. Drive-R1 first undergoes the supervised finetuning on a elaborate dataset containing both long and short COT data. Drive-R1 is encouraged to reason step-by-step from visual input to final planning decisions. Subsequently, Drive-R1 is trained within a reinforcement learning framework that incentivizes the discovery of reasoning paths that are more informative for planning, guided by rewards based on predicted trajectories and meta actions. Experimental evaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that Drive-R1 achieves superior performance compared to existing state-of-the-art VLMs. We believe that Drive-R1 presents a promising direction for bridging reasoning and planning in AD, offering methodological insights for future research and applications.', 'abstract_zh': '面向自动驾驶的大型多模态模型从感知与认知向运动规划的演进及其挑战：Drive-R1的设计与实现', 'title_zh': 'Drive-R1: 在自主驾驶中将推理与规划结合到VLMs中的强化学习方法'}
{'arxiv_id': 'arXiv:2506.18024', 'title': 'Leveraging Cloud-Fog Automation for Autonomous Collision Detection and Classification in Intelligent Unmanned Surface Vehicles', 'authors': 'Thien Tran, Quang Nguyen, Jonathan Kua, Minh Tran, Toan Luu, Thuong Hoang, Jiong Jin', 'link': 'https://arxiv.org/abs/2506.18024', 'abstract': 'Industrial Cyber-Physical Systems (ICPS) technologies are foundational in driving maritime autonomy, particularly for Unmanned Surface Vehicles (USVs). However, onboard computational constraints and communication latency significantly restrict real-time data processing, analysis, and predictive modeling, hence limiting the scalability and responsiveness of maritime ICPS. To overcome these challenges, we propose a distributed Cloud-Edge-IoT architecture tailored for maritime ICPS by leveraging design principles from the recently proposed Cloud-Fog Automation paradigm. Our proposed architecture comprises three hierarchical layers: a Cloud Layer for centralized and decentralized data aggregation, advanced analytics, and future model refinement; an Edge Layer that executes localized AI-driven processing and decision-making; and an IoT Layer responsible for low-latency sensor data acquisition. Our experimental results demonstrated improvements in computational efficiency, responsiveness, and scalability. When compared with our conventional approaches, we achieved a classification accuracy of 86\\%, with an improved latency performance. By adopting Cloud-Fog Automation, we address the low-latency processing constraints and scalability challenges in maritime ICPS applications. Our work offers a practical, modular, and scalable framework to advance robust autonomy and AI-driven decision-making and autonomy for intelligent USVs in future maritime ICPS.', 'abstract_zh': '工业 cyber-物理系统 (ICPS) 技术是推动海洋自主航行，特别是无人驾驶水面车辆 (USVs) 自动化的基础。然而，船上计算限制和通信延迟显著限制了实时数据处理、分析和预测建模，从而限制了海洋 ICPS 的可扩展性和响应性。为克服这些挑战，我们提出了一种针对海洋 ICPS 的分布式云-边缘-IoT 架构，通过利用最近提出的云-雾自动化范式的设计理念。我们提出的设计包括三个层次：云层进行集中和分散的数据聚合、高级分析和未来模型优化；边缘层执行本地化的AI驱动的处理和决策；物联网层负责低延迟传感器数据采集。我们的实验结果表明，在计算效率、响应性和可扩展性方面均有所提升。与传统方法相比，我们实现了86%的分类准确性，并改进了延迟性能。通过采用云-雾自动化，我们的工作解决了海洋 ICPS 应用中低延迟处理和可扩展性的挑战。我们的研究提供了一种实用的、模块化的、可扩展的方法，以促进智能 USVs 在未来海洋 ICPS 中的稳健自主和AI驱动的决策和自主性。', 'title_zh': '基于云-雾自动化技术的自主碰撞检测与分类在智能无人水面车辆中的应用'}
{'arxiv_id': 'arXiv:2506.17991', 'title': 'CFTel: A Practical Architecture for Robust and Scalable Telerobotics with Cloud-Fog Automation', 'authors': 'Thien Tran, Jonathan Kua, Minh Tran, Honghao Lyu, Thuong Hoang, Jiong Jin', 'link': 'https://arxiv.org/abs/2506.17991', 'abstract': 'Telerobotics is a key foundation in autonomous Industrial Cyber-Physical Systems (ICPS), enabling remote operations across various domains. However, conventional cloud-based telerobotics suffers from latency, reliability, scalability, and resilience issues, hindering real-time performance in critical applications. Cloud-Fog Telerobotics (CFTel) builds on the Cloud-Fog Automation (CFA) paradigm to address these limitations by leveraging a distributed Cloud-Edge-Robotics computing architecture, enabling deterministic connectivity, deterministic connected intelligence, and deterministic networked computing. This paper synthesizes recent advancements in CFTel, aiming to highlight its role in facilitating scalable, low-latency, autonomous, and AI-driven telerobotics. We analyze architectural frameworks and technologies that enable them, including 5G Ultra-Reliable Low-Latency Communication, Edge Intelligence, Embodied AI, and Digital Twins. The study demonstrates that CFTel has the potential to enhance real-time control, scalability, and autonomy while supporting service-oriented solutions. We also discuss practical challenges, including latency constraints, cybersecurity risks, interoperability issues, and standardization efforts. This work serves as a foundational reference for researchers, stakeholders, and industry practitioners in future telerobotics research.', 'abstract_zh': '云雾协作远程操作（云雾远程操作，CFTel）是自主工业网络物理系统（ICPS）的关键基础，支持各领域内的远程操作。然而，传统的基于云的远程操作面临延迟、可靠性和扩展性等问题，限制了其在关键应用中的实时性能。云雾协作远程操作（CFTel）基于云雾自动化（CFA）范式，通过利用分布式云-边缘-机器人计算架构，解决这些限制，实现了确定性连接、确定性连接智能和确定性网络计算。本文综合了CFTel领域的最新进展，旨在突出其在实现可扩展、低延迟、自主和AI驱动的远程操作中的作用。本文分析了使其实现的架构框架和技术，包括5G超可靠低延迟通信、边缘智能、具身AI和数字孪生。研究证明，CFTel有望提高实时控制、可扩展性和自主性，同时支持面向服务的解决方案。同时，本文讨论了实际挑战，包括延迟限制、网络安全风险、互操作性问题和标准化努力。本文为未来远程操作研究中的研究人员、利益相关者和行业 practitioner 提供了一个基础参考。', 'title_zh': 'CFTel：一种具有云-雾自动化技术的稳健可扩展远程机器人系统架构'}
{'arxiv_id': 'arXiv:2506.17930', 'title': 'Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective', 'authors': 'Jianyu Wang, Zhiqiang Hu, Lidong Bing', 'link': 'https://arxiv.org/abs/2506.17930', 'abstract': 'We propose a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), we show that pruning random demonstrations into seemingly incoherent "gibberish" can remarkably improve performance across diverse tasks. Notably, the "gibberish" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, we propose a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, our framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. We demonstrate its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. We hope our findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting.', 'abstract_zh': '我们提出了一种新的提示设计范式，挑战了大型语言模型（LLM）提示的 conventional wisdom。尽管 conventional wisdom 认为精心设计的指令和示范对于上下文学习（ICL）至关重要，我们展示了将随机示范裁剪成看似无意义的“胡言乱语”可以显著提高跨多种任务的表现。值得注意的是，“胡言乱语”总是能够匹配甚至超越最先进的自动提示优化技术，在 LLM 对齐与否的情况下都能实现显著改善。然而，发现有效的裁剪策略并不简单，现有的归因方法和提示压缩算法无法提供稳健的结果，更不用说人类直觉。为了解决这一问题，我们提出了一种自我发现的提示优化框架——PromptQuine，这是一种进化搜索框架，能够在仅凭少量数据的情况下自动搜索裁剪策略。就像自然界中资源受限时出现的复杂现象，如共生和自我组织一样，我们的框架通过利用上下文中的现有标记来进化和优化非传统但极其有效的提示。我们展示了它在 LLM 上跨分类、多选题回答、生成和数学推理任务中的有效性，同时保持了不错的运行效率。我们希望我们的发现能够指导对上下文学习的机制性研究，并呼吁开发更为开放的搜索算法，以提高大型语言模型的提示效果。', 'title_zh': '上下文演化提示：一种开放式、自我复制的观点'}
{'arxiv_id': 'arXiv:2506.17869', 'title': 'Cross-modal State Space Modeling for Real-time RGB-thermal Wild Scene Semantic Segmentation', 'authors': "Xiaodong Guo, Zi'ang Lin, Luwen Hu, Zhihong Deng, Tong Liu, Wujie Zhou", 'link': 'https://arxiv.org/abs/2506.17869', 'abstract': 'The integration of RGB and thermal data can significantly improve semantic segmentation performance in wild environments for field robots. Nevertheless, multi-source data processing (e.g. Transformer-based approaches) imposes significant computational overhead, presenting challenges for resource-constrained systems. To resolve this critical limitation, we introduced CM-SSM, an efficient RGB-thermal semantic segmentation architecture leveraging a cross-modal state space modeling (SSM) approach. Our framework comprises two key components. First, we introduced a cross-modal 2D-selective-scan (CM-SS2D) module to establish SSM between RGB and thermal modalities, which constructs cross-modal visual sequences and derives hidden state representations of one modality from the other. Second, we developed a cross-modal state space association (CM-SSA) module that effectively integrates global associations from CM-SS2D with local spatial features extracted through convolutional operations. In contrast with Transformer-based approaches, CM-SSM achieves linear computational complexity with respect to image resolution. Experimental results show that CM-SSM achieves state-of-the-art performance on the CART dataset with fewer parameters and lower computational cost. Further experiments on the PST900 dataset demonstrate its generalizability. Codes are available at this https URL.', 'abstract_zh': 'RGB与热成像数据的整合可以在野外环境中显著提高场用机器人语义分割性能。然而，多源数据处理（例如基于Transformer的方法）会带来显著的计算开销，对资源受限系统构成挑战。为解决这一关键局限，我们提出了CM-SSM，这是一种基于跨模态状态空间建模的高效RGB-热成像语义分割架构。该框架包含两个关键组件。首先，我们引入了跨模态2D选择扫描（CM-SS2D）模块，以在RGB和热成像模态之间建立状态空间建模（SSM），构建跨模态视觉序列，并从另一种模态中推导出隐藏状态表示。其次，我们开发了跨模态状态空间关联（CM-SSA）模块，该模块有效地将CM-SS2D中的全局关联与通过卷积操作提取的局部空间特征结合起来。与基于Transformer的方法相比，CM-SSM的计算复杂度与图像分辨率成线性关系。实验结果表明，CM-SSM在CART数据集上具有更少的参数和更低的计算成本，达到了最先进的性能。进一步在PST900数据集上的实验展示了其普适性。代码已发布于此链接。', 'title_zh': '跨模态状态空间建模实时RGB-热成像野生场景语义分割'}
{'arxiv_id': 'arXiv:2506.17675', 'title': 'Quantification of Sim2Real Gap via Neural Simulation Gap Function', 'authors': 'P Sangeerth, Pushpak Jagtap', 'link': 'https://arxiv.org/abs/2506.17675', 'abstract': 'In this paper, we introduce the notion of neural simulation gap functions, which formally quantifies the gap between the mathematical model and the model in the high-fidelity simulator, which closely resembles reality. Many times, a controller designed for a mathematical model does not work in reality because of the unmodelled gap between the two systems. With the help of this simulation gap function, one can use existing model-based tools to design controllers for the mathematical system and formally guarantee a decent transition from the simulation to the real world. Although in this work, we have quantified this gap using a neural network, which is trained using a finite number of data points, we give formal guarantees on the simulation gap function for the entire state space including the unseen data points. We collect data from high-fidelity simulators leveraging recent advancements in Real-to-Sim transfer to ensure close alignment with reality. We demonstrate our results through two case studies - a Mecanum bot and a Pendulum.', 'abstract_zh': '本文引入了神经模拟差距函数的概念，正式量化了高保真模拟器中的模型与数学模型之间的差距，该模拟器与现实场景高度相似。由于两个系统之间的未建模差距，设计用于数学模型的控制器在现实中往往无法工作。借助此模拟差距函数，可以使用现有的基于模型的工具来为数学系统设计控制器，并正式保证从模拟到现实世界的顺利过渡。尽管本文使用神经网络并通过有限数量的数据点对该差距进行了量化，但我们对整个状态空间，包括未见过的数据点，给出了模拟差距函数的正式保证。我们利用近期实到模迁移的进展，从高保真模拟器中收集数据以确保与现实的紧密对齐。我们通过两个案例研究——全向机器人和单摆——来展示我们的结果。', 'title_zh': '通过神经模拟差距函数量化Sim2Real差距'}
{'arxiv_id': 'arXiv:2506.17590', 'title': 'DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving', 'authors': 'Mihir Godbole, Xiangbo Gao, Zhengzhong Tu', 'link': 'https://arxiv.org/abs/2506.17590', 'abstract': "Understanding the short-term motion of vulnerable road users (VRUs) like pedestrians and cyclists is critical for safe autonomous driving, especially in urban scenarios with ambiguous or high-risk behaviors. While vision-language models (VLMs) have enabled open-vocabulary perception, their utility for fine-grained intent reasoning remains underexplored. Notably, no existing benchmark evaluates multi-class intent prediction in safety-critical situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark constructed from the DRAMA dataset via an automated annotation pipeline. DRAMA-X contains 5,686 accident-prone frames labeled with object bounding boxes, a nine-class directional intent taxonomy, binary risk scores, expert-generated action suggestions for the ego vehicle, and descriptive motion summaries. These annotations enable a structured evaluation of four interrelated tasks central to autonomous decision-making: object detection, intent prediction, risk assessment, and action suggestion. As a reference baseline, we propose SGG-Intent, a lightweight, training-free framework that mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene graph from visual input using VLM-backed detectors, infers intent, assesses risk, and recommends an action using a compositional reasoning stage powered by a large language model. We evaluate a range of recent VLMs, comparing performance across all four DRAMA-X tasks. Our experiments demonstrate that scene-graph-based reasoning enhances intent prediction and risk assessment, especially when contextual cues are explicitly modeled.", 'abstract_zh': '理解脆弱道路使用者（如行人和骑自行车者）的短期运动对于安全的自动驾驶尤其重要，特别是在具有模糊或高风险行为的城市场景中。虽然视觉-语言模型（VLMs）已经实现了开放词汇感知，但其在精细粒度意图推理方面的应用尚未得到充分探索。值得注意的是，目前没有基准对安全关键情况下的多类意图预测进行评估。为解决这一问题，我们引入了DRAMA-X，这是一个通过自动化注释管道从DRAMA数据集中构建的精细粒度基准。DRAMA-X包含5,686个事故易发帧，标注有物体边界框、九类方向意图分类、二进制风险评分、专家生成的自身车辆的行为建议以及描述性运动总结。这些注释使我们能够对与自主决策相关的四个相互关联任务进行有结构的评估：物体检测、意图预测、风险评估和行为建议。作为参考基准，我们提出了SGG-Intent，这是一个轻量级、无需训练的框架，其推理管道模仿了自身车辆的推理过程。该框架从视觉输入中顺序生成场景图，使用VLM支持的检测器进行意图推断、风险评估，并以大型语言模型为动力的知识组合推理阶段提供行动建议。我们评估了最近的多种VLM，并在DRAMA-X的所有四个任务上进行了性能比较。我们的实验表明，基于场景图的推理可以增强意图预测和风险评估，尤其是在显式建模上下文线索时。', 'title_zh': 'DRAMA-X: 一种细粒度意图预测与风险推理基准数据集 for 驾驶'}
{'arxiv_id': 'arXiv:2506.17564', 'title': 'Accelerating Residual Reinforcement Learning with Uncertainty Estimation', 'authors': 'Lakshita Dodeja, Karl Schmeckpeper, Shivam Vats, Thomas Weng, Mingxi Jia, George Konidaris, Stefanie Tellex', 'link': 'https://arxiv.org/abs/2506.17564', 'abstract': 'Residual Reinforcement Learning (RL) is a popular approach for adapting pretrained policies by learning a lightweight residual policy that provides corrective actions. While Residual RL is more sample-efficient than finetuning the entire base policy, existing methods struggle with sparse rewards and are designed for deterministic base policies. We propose two improvements to Residual RL that further enhance its sample efficiency and make it suitable for stochastic base policies. First, we leverage uncertainty estimates of the base policy to focus exploration on regions in which the base policy is not confident. Second, we propose a simple modification to off-policy residual learning that allows it to observe base actions and better handle stochastic base policies. We evaluate our method with both Gaussian-based and Diffusion-based stochastic base policies on tasks from Robosuite and D4RL, and compare against state-of-the-art finetuning methods, demo-augmented RL methods, and other residual RL methods. Our algorithm significantly outperforms existing baselines in a variety of simulation benchmark environments. We also deploy our learned polices in the real world to demonstrate their robustness with zero-shot sim-to-real transfer.', 'abstract_zh': '残差强化学习（Residual Reinforcement Learning）通过学习一个轻量级的残差策略来提供纠正动作，以适应预训练策略，是一种流行的方法。虽然残差强化学习在样本效率上优于整个基策略的微调，但现有方法在处理稀疏奖励时存在困难，并且主要针对确定性基策略。我们提出了两种改进残差强化学习的方法，以进一步提高其样本效率，并使其适合随机性基策略。首先，我们利用基策略的不确定性估计来聚焦于基策略不自信的区域进行探索。其次，我们提出了一种简单的 off-policy 残差学习修改，使其能够观察基动作并更好地处理随机性基策略。我们使用 Robosuite 和 D4RL 中的任务对基于高斯和扩散基策略进行评估，并与最新的微调方法、带有演示增强的强化学习方法以及其他残差强化学习方法进行了比较。我们的算法在多种仿真基准环境中显著优于现有基线。我们还部署了学习到的策略到实际应用中，展示了其通过零样本仿真到现实世界的转移鲁棒性。', 'title_zh': '基于不确定性估计加速残差强化学习'}
{'arxiv_id': 'arXiv:2506.17561', 'title': 'VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models', 'authors': 'Chongkai Gao, Zixuan Liu, Zhenghao Chi, Junshan Huang, Xin Fei, Yiwen Hou, Yuxuan Zhang, Yudi Lin, Zhirui Fang, Zeyu Jiang, Lin Shao', 'link': 'https://arxiv.org/abs/2506.17561', 'abstract': 'Recent studies on Vision-Language-Action (VLA) models have shifted from the end-to-end action-generation paradigm toward a pipeline involving task planning followed by action generation, demonstrating improved performance on various complex, long-horizon manipulation tasks. However, existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved. To systematically investigate the impacts of different planning paradigms and representations isolating from network architectures and training data, in this paper, we introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands). Our results demonstrate that: 1) visually grounded planning representations are generally better than language planning representations; 2) the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds.', 'abstract_zh': '近期对Vision-Language-Action (VLA)模型的研究已经从端到端的动作生成范式转向涉及任务规划后跟随动作生成的管道，展示了在各种复杂的长期操作任务上的改进性能。然而，现有的方法在网络架构、规划范式、表示形式和训练数据来源方面差异显著，使得研究人员难以识别性能改进的具体来源和需要进一步改进的组件。为了系统地调查不同的规划范式和表示形式的影响，我们从网络架构和训练数据中隔离这些因素，在本文中引入了VLA-OS，这是一种能够处理各种任务规划范式的统一VLA架构系列，并设计了跨越不同物体类别（刚性与变形）、视觉模态（2D与3D）、环境（模拟与现实世界）和末端执行器（夹爪与灵巧手）的全面受控实验套件。我们的结果显示：1）基于视觉的规划表示通常优于基于语言的规划表示；2）层次化VLA范式在任务性能、预训练、泛化能力和可扩展性以及持续学习能力方面通常优于其他范式，尽管训练和推理速度较慢。', 'title_zh': 'VLA-OS：结构化与剖析视觉-语言-行动模型中的规划表示与范式'}
{'arxiv_id': 'arXiv:2506.17517', 'title': 'On the Power of Spatial Locality on Online Routing Problems', 'authors': 'Swapnil Guragain, Gokarna Sharma', 'link': 'https://arxiv.org/abs/2506.17517', 'abstract': 'We consider the online versions of two fundamental routing problems, traveling salesman (TSP) and dial-a-ride (DARP), which have a variety of relevant applications in logistics and robotics. The online versions of these problems concern with efficiently serving a sequence of requests presented in a real-time on-line fashion located at points of a metric space by servers (salesmen/vehicles/robots). In this paper, motivated from real-world applications, such as Uber/Lyft rides, where some limited knowledge is available on the future requests, we propose the {\\em spatial locality} model that provides in advance the distance within which new request(s) will be released from the current position of server(s). We study the usefulness of this advanced information on achieving the improved competitive ratios for both the problems with $k\\geq 1$ servers, compared to the competitive results established in the literature without such spatial locality consideration. We show that small locality is indeed useful in obtaining improved competitive ratios irrespective of the metric space.', 'abstract_zh': '考虑物流与机器人领域中两类基本路由问题的在线版本：旅行销售商问题(TSP)和预约车辆问题(DARP)的实时在线版本。本文受到实际应用的启发，如Uber/Lyft乘车服务，其中对于未来请求有一些有限的已知信息，我们提出了空间局部性模型，该模型提前提供了新请求将从服务器当前位置释放出来的距离范围。我们研究了这种先进信息在具有$k \\geq 1$台服务器的情况下，如何提高两类问题的竞争比，相比文献中未考虑空间局部性的情况，我们证明了即使在不同的度量空间中，小的空间局部性也可以提高竞争比。', 'title_zh': '空间局部性对在线路由问题的影响'}
{'arxiv_id': 'arXiv:2506.17513', 'title': 'Public Perceptions of Autonomous Vehicles: A Survey of Pedestrians and Cyclists in Pittsburgh', 'authors': 'Rudra Y. Bedekar', 'link': 'https://arxiv.org/abs/2506.17513', 'abstract': 'This study investigates how autonomous vehicle(AV) technology is perceived by pedestrians and bicyclists in Pittsburgh. Using survey data from over 1200 respondents, the research explores the interplay between demographics, AV interactions, infrastructural readiness, safety perceptions, and trust. Findings highlight demographic divides, infrastructure gaps, and the crucial role of communication and education in AV adoption.', 'abstract_zh': '本研究调查了匹兹堡行人和骑行者对自动驾驶车辆（AV）技术的感知，基于超过1200名受访者的调查数据，研究探讨了人口统计学、AV互动、基础设施准备情况、安全感知和信任之间的相互作用。研究发现突显了人口统计学差异、基础设施缺口以及在自动驾驶车辆采纳中沟通和教育的关键作用。', 'title_zh': '无人驾驶车辆的公众 perception：匹兹堡行人和骑自行车者的调查'}
