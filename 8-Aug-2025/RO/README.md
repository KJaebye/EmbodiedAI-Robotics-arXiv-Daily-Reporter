# Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation 

**Title (ZH)**: Genie Envisioner: 一个统一的机器人 manipulation 基础平台 

**Authors**: Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, Liliang Chen, Shuicheng Yan, Maoqing Yao, Guanghui Ren  

**Link**: [PDF](https://arxiv.org/pdf/2508.05635)  

**Abstract**: We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly. 

**Abstract (ZH)**: 我们引入了Genie Envisioner (GE)，一个统一的机器人操作基础平台，该平台在单一视频生成框架内集成了策略学习、评估和模拟。核心部分GE-Base是一个大规模的、基于指令的视频扩散模型，能够在结构化的潜空间中捕捉现实世界机器人交互的空间、时间和语义动态。在此基础上，GE-Act通过一个轻量级的流匹配解码器将潜空间表示映射到可执行的动作轨迹，从而在最少的监督下实现对不同实体的精确且通用的策略推理。为了支持可扩展的评估和训练，GE-Sim充当条件动作神经模拟器，生成高保真的时间轴rollout，以促进闭环策略开发。该平台还配备了EWMBench，这是一个标准化基准套件，用于衡量视觉保真度、物理一致性和指令动作对齐。这些组件共同奠定了Genie Envisioner作为一个可扩展且实用的基于指令的通用体态智能基础平台的基础。所有代码、模型和基准测试都将公开发布。 

---
# Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling 

**Title (ZH)**: 通过规范不确定性处理实现适用于人群导航的安全泛化 

**Authors**: Jianpeng Yao, Xiaopan Zhang, Yu Xia, Zejin Wang, Amit K. Roy-Chowdhury, Jiachen Li  

**Link**: [PDF](https://arxiv.org/pdf/2508.05634)  

**Abstract**: Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent's behavior through constrained reinforcement learning. The system helps regulate the agent's actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on this https URL. 

**Abstract (ZH)**: 使用自适应可信推断预测不确定性的机器人在分布外场景中稳健地导航 

---
# Robust adaptive fuzzy sliding mode control for trajectory tracking for of cylindrical manipulator 

**Title (ZH)**: 基于圆柱型 manipulator 轨迹跟踪的鲁棒自适应模糊滑模控制 

**Authors**: Van Cuong Pham, Minh Hai Tran, Phuc Anh Nguyen, Ngoc Son Vu, Nga Nguyen Thi  

**Link**: [PDF](https://arxiv.org/pdf/2508.05584)  

**Abstract**: This research proposes a robust adaptive fuzzy sliding mode control (AFSMC) approach to enhance the trajectory tracking performance of cylindrical robotic manipulators, extensively utilized in applications such as CNC and 3D printing. The proposed approach integrates fuzzy logic with sliding mode control (SMC) to bolster adaptability and robustness, with fuzzy logic approximating the uncertain dynamics of the system, while SMC ensures strong performance. Simulation results in MATLAB/Simulink demonstrate that AFSMC significantly improves trajectory tracking accuracy, stability, and disturbance rejection compared to traditional methods. This research underscores the effectiveness of AFSMC in controlling robotic manipulators, contributing to enhanced precision in industrial robotic applications. 

**Abstract (ZH)**: 一种增强圆柱型机器人 manipulator 轨迹跟踪性能的鲁棒自适应模糊滑模控制方法 

---
# CleanUpBench: Embodied Sweeping and Grasping Benchmark 

**Title (ZH)**: CleanUpBench:  embodied sweeping and grasping benchmark 

**Authors**: Wenbo Li, Guanting Chen, Tao Zhao, Jiyao Wang, Tianxin Hu, Yuwen Liao, Weixiang Guo, Shenghai Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2508.05543)  

**Abstract**: Embodied AI benchmarks have advanced navigation, manipulation, and reasoning, but most target complex humanoid agents or large-scale simulations that are far from real-world deployment. In contrast, mobile cleaning robots with dual mode capabilities, such as sweeping and grasping, are rapidly emerging as realistic and commercially viable platforms. However, no benchmark currently exists that systematically evaluates these agents in structured, multi-target cleaning tasks, revealing a critical gap between academic research and real-world applications. We introduce CleanUpBench, a reproducible and extensible benchmark for evaluating embodied agents in realistic indoor cleaning scenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service robot equipped with a sweeping mechanism and a six-degree-of-freedom robotic arm, enabling interaction with heterogeneous objects. The benchmark includes manually designed environments and one procedurally generated layout to assess generalization, along with a comprehensive evaluation suite covering task completion, spatial efficiency, motion quality, and control performance. To support comparative studies, we provide baseline agents based on heuristic strategies and map-based planning. CleanUpBench bridges the gap between low-level skill evaluation and full-scene testing, offering a scalable testbed for grounded, embodied intelligence in everyday settings. 

**Abstract (ZH)**: 基于身体计算的清洁机器人基准：CleanUpBench 

---
# Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation 

**Title (ZH)**: 人机协作操作的混合主动对话 

**Authors**: Albert Yu, Chengshu Li, Luca Macesanu, Arnav Balaji, Ruchira Ray, Raymond Mooney, Roberto Martín-Martín  

**Link**: [PDF](https://arxiv.org/pdf/2508.05535)  

**Abstract**: Effective robotic systems for long-horizon human-robot collaboration must adapt to a wide range of human partners, whose physical behavior, willingness to assist, and understanding of the robot's capabilities may change over time. This demands a tightly coupled communication loop that grants both agents the flexibility to propose, accept, or decline requests as they coordinate toward completing the task effectively. We apply a Mixed-Initiative dialog paradigm to Collaborative human-roBot teaming and propose MICoBot, a system that handles the common scenario where both agents, using natural language, take initiative in formulating, accepting, or rejecting proposals on who can best complete different steps of a task. To handle diverse, task-directed dialog, and find successful collaborative strategies that minimize human effort, MICoBot makes decisions at three levels: (1) a meta-planner considers human dialog to formulate and code a high-level collaboration strategy, (2) a planner optimally allocates the remaining steps to either agent based on the robot's capabilities (measured by a simulation-pretrained affordance model) and the human's estimated availability to help, and (3) an action executor decides the low-level actions to perform or words to say to the human. Our extensive evaluations in simulation and real-world -- on a physical robot with 18 unique human participants over 27 hours -- demonstrate the ability of our method to effectively collaborate with diverse human users, yielding significantly improved task success and user experience than a pure LLM baseline and other agent allocation models. See additional videos and materials at this https URL. 

**Abstract (ZH)**: 长时限人类-机器人协作的有效机器人系统必须适应广泛的人类合作伙伴，其物理行为、助人意愿以及对机器人能力的理解可能会随时间变化。这要求一个紧密耦合的通信循环，使两个代理能够在协调完成任务的过程中灵活地提出、接受或拒绝请求。我们应用混合主动对话范式于协作人机团队，并提出MICoBot系统，该系统处理两个代理在自然语言下共同发起、接受或拒绝关于谁最能完成任务不同步骤的提议的常见场景。为处理任务定向对话并找到能最小化人类努力的协作策略，MICoBot在三个层次上做出决定：（1）元规划器通过分析人类对话来制定和编码高层次的合作策略；（2）规划器基于机器人的能力（通过预训练的可用性模型测量）和人类帮助意愿的最佳分配剩余步骤给任一代理；（3）行为执行器决定执行的低层动作或对人类说的话。我们在模拟和现实世界中的广泛评估，包括在一个物理机器人上与18位独特的人类参与者合作27小时，证明了该方法在与多样化的人类用户协作方面的有效性，相比纯大语言模型基线和其他代理分配模型，显著提高了任务成功率和用户体验。更多视频和材料请参见此链接：this https URL。 

---
# Do Robots Really Need Anthropomorphic Hands? 

**Title (ZH)**: 机器人真的需要类人手吗？ 

**Authors**: Alexander Fabisch, Wadhah Zai El Amri, Chandandeep Singh, Nicolás Navarro-Guerrero  

**Link**: [PDF](https://arxiv.org/pdf/2508.05415)  

**Abstract**: Human manipulation skills represent a pinnacle of their voluntary motor functions, requiring the coordination of many degrees of freedom and processing of high-dimensional sensor input to achieve such a high level of dexterity. Thus, we set out to answer whether the human hand, with its associated biomechanical properties, sensors, and control mechanisms, is an ideal that we should strive for in robotics-do we really need anthropomorphic robotic hands?
This survey can help practitioners to make the trade-off between hand complexity and potential manipulation skills. We provide an overview of the human hand, a comparison of commercially available robotic and prosthetic hands, and a systematic review of hand mechanisms and skills that they are capable of. This leads to follow-up questions. What is the minimum requirement for mechanisms and sensors to implement most skills that a robot needs? What is missing to reach human-level dexterity? Can we improve upon human dexterity?
Although complex five-fingered hands are often used as the ultimate goal for robotic manipulators, they are not necessary for all tasks. We found that wrist flexibility and finger abduction/adduction are important for manipulation capabilities. On the contrary, increasing the number of fingers, actuators, or degrees of freedom is often not necessary. Three fingers are a good compromise between simplicity and dexterity. Non-anthropomorphic hand designs with two opposing pairs of fingers or human hands with six fingers can further increase dexterity, suggesting that the human hand may not be the optimum. 

**Abstract (ZH)**: 人类操作技能代表了他们自愿运动功能的顶峰，需要协调多个自由度并处理高维传感器输入以达到如此高的灵巧性。因此，我们想了解人类手部，包括其生物力学特性、传感器和控制机制，是否是我们应该追求的机器人标准——我们真的需要类人手部吗？ 

---
# Computational Design and Fabrication of Modular Robots with Untethered Control 

**Title (ZH)**: 无缆控制的模块化机器人计算设计与制造 

**Authors**: Manas Bhargava, Takefumi Hiraki, Malina Strugaru, Michal Piovarci, Chiara Daraio, Daisuke Iwai, Bernd Bickel  

**Link**: [PDF](https://arxiv.org/pdf/2508.05410)  

**Abstract**: Natural organisms use distributed actuation via their musculoskeletal systems to adapt their gait for traversing diverse terrains or to morph their bodies to perform varied tasks. A longstanding challenge in the field of robotics is to mimic this extensive adaptability and range of motion. This has led humans to develop various soft robotic systems that emulate natural organisms. However, such systems are generally optimized for a single functionality, lack the ability to change form or function on demand, or are often tethered to bulky control systems. To address these challenges, we present our framework for designing and controlling robots that mimic nature's blueprint by utilizing distributed actuation. We propose a novel building block that combines 3D-printed bones with liquid crystal elastomer (LCE) muscles as lightweight actuators and enables the modular assembly of musculoskeletal robots. We developed LCE rods that contract in response to infrared radiation, thereby achieving local and untethered control over the distributed network of bones, which in turn results in global deformation of the robot. Furthermore, to capitalize on the extensive design space, we develop two computational tools: one to optimize the robot's skeletal graph, enabling multiple target deformations, and another to co-optimize the skeletal designs and control gaits to achieve target locomotion. We validate our system by building several robots that show complex shape morphing, varying control schemes, and adaptability to their environment. Our system integrates advances in modular material building, untethered and distributed control, and computational design to introduce a new generation of robots that brings us closer to the capabilities of living organisms. 

**Abstract (ZH)**: 自然生物通过肌骨骼系统利用分布式驱动适应不同地形的步态，或通过变形身体执行不同任务。机器人领域长久以来的一个挑战是模仿这种广泛的适应性和运动范围。人类为此开发了各种仿生软体机器人，但这些系统通常优化单一功能，缺乏按需改变形态或功能的能力，或者常常依赖笨重的控制系统。为解决这些挑战，我们提出了一种设计和控制仿生机器人的框架，利用分布式驱动模拟自然的蓝图。我们提出了一种新型构建模块，将3D打印骨头与液晶弹性体(LCE)肌肉结合，作为轻量驱动器，实现模块化组装肌骨骼机器人。我们开发了响应红外辐射收缩的LCE杆，从而在分布式骨骼网络中实现局部且无绳的控制，进而导致机器人的整体变形。此外，为了充分利用广泛的设计空间，我们开发了两种计算工具：一种优化机器人的骨骼图，使机体能够实现多种目标变形，另一种协同优化骨骼设计和控制步态，以实现目标移动。我们通过构建多个展示复杂形状变形、不同控制方案及环境适应性的机器人，验证了我们的系统。我们的系统整合了模块化材料构建、无绳分布控制和计算设计的最新进展，引入了一代新的机器人，使其更加接近生物体的能力。 

---
# DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model 

**Title (ZH)**: DistillDrive: 通过同构异源源规划模型实现端到端多模式自主驾驶精炼 

**Authors**: Rui Yu, Xianghang Zhang, Runkai Zhao, Huaicheng Yan, Meng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2508.05402)  

**Abstract**: End-to-end autonomous driving has been recently seen rapid development, exerting a profound influence on both industry and academia. However, the existing work places excessive focus on ego-vehicle status as their sole learning objectives and lacks of planning-oriented understanding, which limits the robustness of the overall decision-making prcocess. In this work, we introduce DistillDrive, an end-to-end knowledge distillation-based autonomous driving model that leverages diversified instance imitation to enhance multi-mode motion feature learning. Specifically, we employ a planning model based on structured scene representations as the teacher model, leveraging its diversified planning instances as multi-objective learning targets for the end-to-end model. Moreover, we incorporate reinforcement learning to enhance the optimization of state-to-decision mappings, while utilizing generative modeling to construct planning-oriented instances, fostering intricate interactions within the latent space. We validate our model on the nuScenes and NAVSIM datasets, achieving a 50\% reduction in collision rate and a 3-point improvement in closed-loop performance compared to the baseline model. Code and model are publicly available at this https URL 

**Abstract (ZH)**: 端到端自主驾驶近年来取得了 rapid 发展，对产业和学术界产生了深远影响。然而，现有工作过度关注ego-车辆状态作为唯一的学习目标，缺乏以规划为导向的理解，限制了整体决策过程的 robust 性。在此工作中，我们介绍了基于知识蒸馏的端到端自主驾驶模型 DistillDrive，通过多样化实例模仿增强多模式运动特征学习。具体而言，我们采用基于结构化场景表示的规划模型作为教师模型，利用其多样化的规划实例作为端到端模型的多目标学习目标。此外，我们结合强化学习以优化状态到决策的映射，并利用生成模型构建以规划为导向的实例，促进潜空间内的复杂交互。我们使用 nuScenes 和 NAVSIM 数据集验证了我们的模型，相比基线模型，碰撞率降低了 50%，闭环性能提升了 3 个点。代码和模型已公开，详见 this https URL。 

---
# Real-Time Iteration Scheme for Diffusion Policy 

**Title (ZH)**: 实时迭代方案 for 扩散策略 

**Authors**: Yufei Duan, Hang Yin, Danica Kragic  

**Link**: [PDF](https://arxiv.org/pdf/2508.05396)  

**Abstract**: Diffusion Policies have demonstrated impressive performance in robotic manipulation tasks. However, their long inference time, resulting from an extensive iterative denoising process, and the need to execute an action chunk before the next prediction to maintain consistent actions limit their applicability to latency-critical tasks or simple tasks with a short cycle time. While recent methods explored distillation or alternative policy structures to accelerate inference, these often demand additional training, which can be resource-intensive for large robotic models. In this paper, we introduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a method from optimal control that accelerates optimization by leveraging solutions from previous time steps as initial guesses for subsequent iterations. We explore the application of this scheme in diffusion inference and propose a scaling-based method to effectively handle discrete actions, such as grasping, in robotic manipulation. The proposed scheme significantly reduces runtime computational costs without the need for distillation or policy redesign. This enables a seamless integration into many pre-trained diffusion-based models, in particular, to resource-demanding large models. We also provide theoretical conditions for the contractivity which could be useful for estimating the initial denoising step. Quantitative results from extensive simulation experiments show a substantial reduction in inference time, with comparable overall performance compared with Diffusion Policy using full-step denoising. Our project page with additional resources is available at: this https URL. 

**Abstract (ZH)**: 实时迭代方案在机器人操作中加速扩散政策的推理时间 

---
# Robots can defuse high-intensity conflict situations 

**Title (ZH)**: 机器人可以化解高强度冲突情况 

**Authors**: Morten Roed Frederiksen, Kasper Støy  

**Link**: [PDF](https://arxiv.org/pdf/2508.05373)  

**Abstract**: This paper investigates the specific scenario of high-intensity confrontations between humans and robots, to understand how robots can defuse the conflict. It focuses on the effectiveness of using five different affective expression modalities as main drivers for defusing the conflict. The aim is to discover any strengths or weaknesses in using each modality to mitigate the hostility that people feel towards a poorly performing robot. The defusing of the situation is accomplished by making the robot better at acknowledging the conflict and by letting it express remorse. To facilitate the tests, we used a custom affective robot in a simulated conflict situation with 105 test participants. The results show that all tested expression modalities can successfully be used to defuse the situation and convey an acknowledgment of the confrontation. The ratings were remarkably similar, but the movement modality was different (ANON p$<$.05) than the other modalities. The test participants also had similar affective interpretations on how impacted the robot was of the confrontation across all expression modalities. This indicates that defusing a high-intensity interaction may not demand special attention to the expression abilities of the robot, but rather require attention to the abilities of being socially aware of the situation and reacting in accordance with it. 

**Abstract (ZH)**: 本文探讨了人类与机器人之间高强度对峙的具体场景，以理解机器人如何化解冲突。它专注于使用五种不同的情感表达模态作为主要驱动力来化解冲突的有效性。目的是发现使用每种模态来缓解人们对表现不佳的机器人的敌意时的强项和弱点。通过使机器人更好地识别冲突，并让它表达悔意，来化解局势。为了进行测试，我们使用了一个定制的情感机器人，在模拟的冲突情景中，共有105名测试参与者参与。结果表明，所有测试的情感表达模态都能成功地用于化解局势，并传达对冲突的认识。评分非常相似，但运动模态（ANON p<0.05）与其他模态不同。测试参与者对机器人在所有情感表达模态下对冲突的影响也有相似的情绪解释。这表明，化解高强度互动可能不需要特别关注机器人的表达能力，而是需要关注其对情境的社会意识能力和相应的反应能力。 

---
# A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry 

**Title (ZH)**: 多视角地标表示方法及其在GNSS-视觉-惯性里程计中的应用 

**Authors**: Tong Hua, Jiale Han, Wei Ouyang  

**Link**: [PDF](https://arxiv.org/pdf/2508.05368)  

**Abstract**: Invariant Extended Kalman Filter (IEKF) has been a significant technique in vision-aided sensor fusion. However, it usually suffers from high computational burden when jointly optimizing camera poses and the landmarks. To improve its efficiency and applicability for multi-sensor fusion, we present a multi-view pose-only estimation approach with its application to GNSS-Visual-Inertial Odometry (GVIO) in this paper. Our main contribution is deriving a visual measurement model which directly associates landmark representation with multiple camera poses and observations. Such a pose-only measurement is proven to be tightly-coupled between landmarks and poses, and maintain a perfect null space that is independent of estimated poses. Finally, we apply the proposed approach to a filter based GVIO with a novel feature management strategy. Both simulation tests and real-world experiments are conducted to demonstrate the superiority of the proposed method in terms of efficiency and accuracy. 

**Abstract (ZH)**: Invariant 扩展卡尔曼滤波(INEKF)在视觉辅助传感器融合中是一项重要技术。然而，它在同时优化相机姿态和特征点时通常会面临较高的计算负担。为了提高其效率和多传感器融合的适用性，本文提出了一种多视图姿态-only估计方法，并将其应用于GNSS-视觉-惯性里程计(GVIO)。本文的主要贡献是推导出一种视觉测量模型，该模型直接将特征点表示与多个相机姿态和观测值关联起来。这种姿态-only测量模型被证明在特征点与姿态之间紧密耦合，并且保持一个与估计姿态无关的理想零空间。最后，我们将所提出的方法应用于一个基于滤波的新型特征管理策略的GNSS-视觉-惯性里程计系统。通过仿真测试和实际实验验证了所提出方法在效率和准确性方面的优越性。 

---
# Affecta-Context: The Context-Guided Behavior Adaptation Framework 

**Title (ZH)**: Affecta-Context：基于上下文的行为适应框架 

**Authors**: Morten Roed Frederiksen, Kasper Støy  

**Link**: [PDF](https://arxiv.org/pdf/2508.05359)  

**Abstract**: This paper presents Affecta-context, a general framework to facilitate behavior adaptation for social robots. The framework uses information about the physical context to guide its behaviors in human-robot interactions. It consists of two parts: one that represents encountered contexts and one that learns to prioritize between behaviors through human-robot interactions. As physical contexts are encountered the framework clusters them by their measured physical properties. In each context, the framework learns to prioritize between behaviors to optimize the physical attributes of the robot's behavior in line with its current environment and the preferences of the users it interacts with. This paper illlustrates the abilities of the Affecta-context framework by enabling a robot to autonomously learn the prioritization of discrete behaviors. This was achieved by training across 72 interactions in two different physical contexts with 6 different human test participants. The paper demonstrates the trained Affecta-context framework by verifying the robot's ability to generalize over the input and to match its behaviors to a previously unvisited physical context. 

**Abstract (ZH)**: Affecta-context：一种促进社会机器人行为适应的一般框架 

---
# Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control 

**Title (ZH)**: 基于视觉-语言-行动模型的信息理论图融合及其在策略推理与双重机器人控制中的应用 

**Authors**: Shunlei Li, Longsen Gao, Jin Wang, Chang Che, Xi Xiao, Jiuwen Cao, Yingbai Hu, Hamid Reza Karimi  

**Link**: [PDF](https://arxiv.org/pdf/2508.05342)  

**Abstract**: Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations. 

**Abstract (ZH)**: 基于人类视频的教学：通过图融合视觉-语言-动作框架赋予机器人灵巧技能仍具有挑战性，因为它依赖于低级轨迹模仿，无法在不同的物体类型、空间布局和操作器配置之间泛化。我们提出了一种图融合视觉-语言-动作（GF-VLA）框架，该框架使双臂机器人系统能够直接从RGB和深度人类演示中进行任务级别推理和执行。GF-VLA 首先提取以香农信息为基础的线索，以识别与任务最相关的手和物体，然后将这些线索编码为捕捉手-物和物-物交互的时序场景图。这些图与语言条件下的变压器结合，生成分层级的行为树和可解释的笛卡尔运动指令。为了提高双臂操作中的执行效率，我们进一步引入了一种跨手选择策略，该策略在无需明确的几何推理的情况下推断最优的夹爪分配。我们在四个涉及符号形状构建和空间泛化的双臂块组装任务上评估了GF-VLA。实验结果表明，信息论场景表示的图准确率超过95%，子任务分割准确率超过93%，支持大语言模型计划生成可靠的和人类可读的任务策略。当由双臂机器人执行时，这些策略在堆叠、字母构建和几何重构场景中分别实现了94%的抓取成功率、89%的放置准确率和90%的整体任务成功率，展示了其在多样化的空间和语义变化中的强大泛化能力和鲁棒性。 

---
# GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming 

**Title (ZH)**: GhostShell: 流式调用LLM函数实现并发具身编程 

**Authors**: Jian Gong, Youwei Huang, Bo Yuan, Ming Zhu, Juncheng Zhan, Jinke Wang, Hang Shu, Mingyue Xiong, Yanjun Ye, Yufan Zu, Yang Zhou, Yihan Ding, Xuannian Chen, Xingyu Lu, Runjie Ban, Bingchao Huang, Fusen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2508.05298)  

**Abstract**: We present GhostShell, a novel approach that leverages Large Language Models (LLMs) to enable streaming and concurrent behavioral programming for embodied systems. In contrast to conventional methods that rely on pre-scheduled action sequences or behavior trees, GhostShell drives embodied systems to act on-the-fly by issuing function calls incrementally as tokens are streamed from the LLM. GhostShell features a streaming XML function token parser, a dynamic function interface mapper, and a multi-channel scheduler that orchestrates intra-channel synchronous and inter-channel asynchronous function calls, thereby coordinating serial-parallel embodied actions across multiple robotic components as directed by the LLM. We evaluate GhostShell on our robot prototype COCO through comprehensive grounded experiments across 34 real-world interaction tasks and multiple LLMs. The results demonstrate that our approach achieves state-of-the-art Behavioral Correctness Metric of 0.85 with Claude-4 Sonnet and up to 66X faster response times compared to LLM native function calling APIs. GhostShell also proves effective in long-horizon multimodal tasks, demonstrating strong robustness and generalization. 

**Abstract (ZH)**: GhostShell：一种利用大规模语言模型实现类人系统流式并发行为编程的新方法 

---
# Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction 

**Title (ZH)**: 面向具身自主人工智能：基于LLM和VLM的机器人自主与交互综述与分类 

**Authors**: Sahar Salimpour, Lei Fu, Farhad Keramat, Leonardo Militano, Giovanni Toffetti, Harry Edelman, Jorge Peña Queralta  

**Link**: [PDF](https://arxiv.org/pdf/2508.05294)  

**Abstract**: Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (BLMs) are increasing the dexterity and capabilities of robotic systems. This survey paper focuses on those words advancing towards agentic applications and architectures. This includes initial efforts exploring GPT-style interfaces to tooling, as well as more complex system where AI agents are coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature. 

**Abstract (ZH)**: 基础模型，包括大规模语言模型（LLMs）和视觉-语言模型（VLMs），最近为机器人自主性和人机接口提供了新的方法。同时，视觉-语言-动作模型（VLAs）或大规模行为模型（BLMs）正不断提高机器人的灵活性和能力。本文综述专注于这些模型在自主应用和架构方面的进展，包括对工具GPT风格接口的初步探索，以及更复杂的系统，其中AI代理充当协调员、规划者、感知执行者或通用接口。此类自主架构使机器人能够处理自然语言指令、调用API、规划任务序列或在操作和诊断中提供协助。除了同行评审的研究外，鉴于该领域的快速发展，我们还强调并包括由社区驱动的项目、ROS软件包和工业框架，以展示新兴趋势。我们提出了一种分类模型集成方法的分类法，并对当前文献中不同解决方案中代理作用进行了比较分析。 

---
# Dancing with a Robot: An Experimental Study of Child-Robot Interaction in a Performative Art Setting 

**Title (ZH)**: 与机器人共舞：一种表演艺术环境中的儿童与机器人互动实验研究 

**Authors**: Victor Ngo, Rachel, Ramchurn, Roma Patel, Alan Chamberlain, Ayse Kucukyilmaz  

**Link**: [PDF](https://arxiv.org/pdf/2508.05208)  

**Abstract**: This paper presents an evaluation of 18 children's in-the-wild experiences with the autonomous robot arm performer NED (Never-Ending Dancer) within the Thingamabobas installation, showcased across the UK. We detail NED's design, including costume, behaviour, and human interactions, all integral to the installation. Our observational analysis revealed three key challenges in child-robot interactions: 1) Initiating and maintaining engagement, 2) Lack of robot expressivity and reciprocity, and 3) Unmet expectations. Our findings show that children are naturally curious, and adept at interacting with a robotic art performer. However, our observations emphasise the critical need to optimise human-robot interaction (HRI) systems through careful consideration of audience's capabilities, perceptions, and expectations, within the performative arts context, to enable engaging and meaningful experiences, especially for young audiences. 

**Abstract (ZH)**: 本文评估了18名儿童与自主机器人手臂表演者NED（永不结束的舞者）在Thingamabobas装置中的户外体验，该装置在英国各地展出。我们详细介绍了NED的设计，包括其服装、行为和与人类的互动，这些都是装置的重要组成部分。我们的观察性分析揭示了儿童与机器人交互中的三个关键挑战：1）吸引和维持注意力，2）缺乏机器人表达性和互应性，3）期望未被满足。研究发现，儿童天生好奇，并擅长与机器人艺术表演者互动。然而，我们的观察强调，在表演艺术领域内，为了实现引人入胜且有意义的体验，特别是在面向年轻观众时，必须通过仔细考虑观众的能力、感知和期望来优化人机交互系统。 

---
# Learning to See and Act: Task-Aware View Planning for Robotic Manipulation 

**Title (ZH)**: 学会看与做：面向任务的视图规划在机器人操作中的应用 

**Authors**: Yongjie Bai, Zhouxia Wang, Yang Liu, Weixing Chen, Ziliang Chen, Mingtong Dai, Yongsen Zheng, Lingbo Liu, Guanbin Li, Liang Lin  

**Link**: [PDF](https://arxiv.org/pdf/2508.05186)  

**Abstract**: Recent vision-language-action (VLA) models for multi-task robotic manipulation commonly rely on static viewpoints and shared visual encoders, which limit 3D perception and cause task interference, hindering robustness and generalization. In this work, we propose Task-Aware View Planning (TAVP), a framework designed to overcome these challenges by integrating active view planning with task-specific representation learning. TAVP employs an efficient exploration policy, accelerated by a novel pseudo-environment, to actively acquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE) visual encoder to disentangle features across different tasks, boosting both representation fidelity and task generalization. By learning to see the world in a task-aware way, TAVP generates more complete and discriminative visual representations, demonstrating significantly enhanced action prediction across a wide array of manipulation challenges. Extensive experiments on RLBench tasks show that our proposed TAVP model achieves superior performance over state-of-the-art fixed-view approaches. Visual results and code are provided at: this https URL. 

**Abstract (ZH)**: 基于任务的视图规划（TAVP）：一种结合任务特定表示学习的主动视图规划框架 

---
# FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction 

**Title (ZH)**: FCBV-Net：基于特征条件双臂值预测的类别级机器人衣物平滑 

**Authors**: Mohammed Daba, Jing Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2508.05153)  

**Abstract**: Category-level generalization for robotic garment manipulation, such as bimanual smoothing, remains a significant hurdle due to high dimensionality, complex dynamics, and intra-category variations. Current approaches often struggle, either overfitting with concurrently learned visual features for a specific instance or, despite category-level perceptual generalization, failing to predict the value of synergistic bimanual actions. We propose the Feature-Conditioned Bimanual Value Network (FCBV-Net), operating on 3D point clouds to specifically enhance category-level policy generalization for garment smoothing. FCBV-Net conditions bimanual action value prediction on pre-trained, frozen dense geometric features, ensuring robustness to intra-category garment variations. Trainable downstream components then learn a task-specific policy using these static features. In simulated GarmentLab experiments with the CLOTH3D dataset, FCBV-Net demonstrated superior category-level generalization. It exhibited only an 11.5% efficiency drop (Steps80) on unseen garments compared to 96.2% for a 2D image-based baseline, and achieved 89% final coverage, outperforming an 83% coverage from a 3D correspondence-based baseline that uses identical per-point geometric features but a fixed primitive. These results highlight that the decoupling of geometric understanding from bimanual action value learning enables better category-level generalization. 

**Abstract (ZH)**: 基于特征条件的双臂价值网络（FCBV-Net）在三维点云上的服装光滑处理类别级泛化 

---
# Chemist Eye: A Visual Language Model-Powered System for Safety Monitoring and Robot Decision-Making in Self-Driving Laboratories 

**Title (ZH)**: 化学家之眼：一种基于视觉语言模型的安全监控与机器人决策系统在自主实验室中的应用 

**Authors**: Francisco Munguia-Galeano, Zhengxue Zhou, Satheeshkumar Veeramani, Hatem Fakhruldeen, Louis Longley, Rob Clowes, Andrew I. Cooper  

**Link**: [PDF](https://arxiv.org/pdf/2508.05148)  

**Abstract**: The integration of robotics and automation into self-driving laboratories (SDLs) can introduce additional safety complexities, in addition to those that already apply to conventional research laboratories. Personal protective equipment (PPE) is an essential requirement for ensuring the safety and well-being of workers in laboratories, self-driving or otherwise. Fires are another important risk factor in chemical laboratories. In SDLs, fires that occur close to mobile robots, which use flammable lithium batteries, could have increased severity. Here, we present Chemist Eye, a distributed safety monitoring system designed to enhance situational awareness in SDLs. The system integrates multiple stations equipped with RGB, depth, and infrared cameras, designed to monitor incidents in SDLs. Chemist Eye is also designed to spot workers who have suffered a potential accident or medical emergency, PPE compliance and fire hazards. To do this, Chemist Eye uses decision-making driven by a vision-language model (VLM). Chemist Eye is designed for seamless integration, enabling real-time communication with robots. Based on the VLM recommendations, the system attempts to drive mobile robots away from potential fire locations, exits, or individuals not wearing PPE, and issues audible warnings where necessary. It also integrates with third-party messaging platforms to provide instant notifications to lab personnel. We tested Chemist Eye with real-world data from an SDL equipped with three mobile robots and found that the spotting of possible safety hazards and decision-making performances reached 97 % and 95 %, respectively. 

**Abstract (ZH)**: 机器人技术与自动化在自主驾驶实验室中的集成会引入额外的安全复杂性，尤其是在常规研究实验室已经存在的安全要求之外。个人防护装备（PPE）是确保实验室工作人员安全与健康的必备要求，在自主驾驶实验室中也不例外。火灾是化学实验室中的另一种重要风险因素。在自主驾驶实验室（SDLs）中，靠近使用可燃锂离子电池的移动机器人发生的火灾可能会更为严重。为此，我们提出了Chemist Eye，这是一种分布式安全监测系统，旨在增强SDLs中的情况意识。该系统集成了多个站点，配备有RGB、深度和红外摄像头，用于监控SDLs中的事件。Chemist Eye还被设计成能够识别可能出现事故或医疗紧急情况的工人、检查PPE合规性以及监测火灾隐患。通过基于视觉-语言模型（VLM）的决策，系统尝试引导移动机器人远离潜在的火灾区域、出口或未佩戴PPE的个体，并在必要时发出语音警告。此外，Chemist Eye还与第三方即时通信平台集成，以向实验室人员提供即时通知。我们使用配备三台移动机器人的SDL的真实世界数据测试了Chemist Eye，结果显示，识别可能的安全隐患和决策性能分别达到了97%和95%。 

---
# From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation 

**Title (ZH)**: 从加拿大到日本：10,000公里如何影响机器人远程操作的用户感知 

**Authors**: Siméon Capy, Thomas M. Kwok, Kevin Joseph, Yuichiro Kawasumi, Koichi Nagashima, Tomoya Sasaki, Yue Hu, Eiichi Yoshida  

**Link**: [PDF](https://arxiv.org/pdf/2508.05143)  

**Abstract**: Robot teleoperation (RTo) has emerged as a viable alternative to local control, particularly when human intervention is still necessary. This research aims to study the distance effect on user perception in RTo, exploring the potential of teleoperated robots for older adult care. We propose an evaluation of non-expert users' perception of long-distance RTo, examining how their perception changes before and after interaction, as well as comparing it to that of locally operated robots. We have designed a specific protocol consisting of multiple questionnaires, along with a dedicated software architecture using the Robotics Operating System (ROS) and Unity. The results revealed no statistically significant differences between the local and remote robot conditions, suggesting that robots may be a viable alternative to traditional local control. 

**Abstract (ZH)**: 远程操控（RTo）已成为本地控制的可行替代方案，特别是在人类干预仍然必要的时候。本研究旨在研究远程操控距离对用户体验的影响，探索远程操控机器人在老年人照护中的潜在价值。我们提出了一项评估非专业用户对远程操控长距离体验感知的研究，考察其在交互前后感知的变化，并将其与本地操作机器人的感知进行比较。我们设计了一项特定的协议，包括多个问卷调查，并采用Robotics Operating System (ROS) 和Unity构建了专用的软件架构。结果表明，本地和远程机器人之间没有统计学上的显著差异，这表明机器人可能是传统本地控制的可行替代方案。 

---
# Examining the legibility of humanoid robot arm movements in a pointing task 

**Title (ZH)**: 考察类人机器人手臂在指认任务中运动的可读性 

**Authors**: Andrej Lúčny, Matilde Antonj, Carlo Mazzola, Hana Hornáčková, Ana Farić, Kristína Malinovská, Michal Vavrecka, Igor Farkaš  

**Link**: [PDF](https://arxiv.org/pdf/2508.05104)  

**Abstract**: Human--robot interaction requires robots whose actions are legible, allowing humans to interpret, predict, and feel safe around them. This study investigates the legibility of humanoid robot arm movements in a pointing task, aiming to understand how humans predict robot intentions from truncated movements and bodily cues. We designed an experiment using the NICO humanoid robot, where participants observed its arm movements towards targets on a touchscreen. Robot cues varied across conditions: gaze, pointing, and pointing with congruent or incongruent gaze. Arm trajectories were stopped at 60\% or 80\% of their full length, and participants predicted the final target. We tested the multimodal superiority and ocular primacy hypotheses, both of which were supported by the experiment. 

**Abstract (ZH)**: 人人的机器人交互需要机器人具有可可译 Swinger人的机器人在移动时动作时 时动作具有可耐读性忽略谷爱凌人类机器人手臂动作时的实验，旨在理解人类如何从截断的动作和身体线索预测机器人的意图。实验中使用了谷爱凌phalt类机器人，受试者观察手臂动作并向触屏目标移动。机器人的动作条件包括：目光指向、手指向、目光一致/不一致的指向。手臂轨迹分为六个部分，受试者预测机器人的意图。我们测试了多模态优越性和眼动优先性假说。 

---
# A Vision-Based Collision Sensing Method for Stable Circular Object Grasping with A Soft Gripper System 

**Title (ZH)**: 基于视觉的碰撞感知方法：用于软夹持系统稳定圆柱形物体抓取 

**Authors**: Boyang Zhang, Jiahui Zuo, Zeyu Duan, Fumin Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2508.05040)  

**Abstract**: External collisions to robot actuators typically pose risks to grasping circular objects. This work presents a vision-based sensing module capable of detecting collisions to maintain stable grasping with a soft gripper system. The system employs an eye-in-palm camera with a broad field of view to simultaneously monitor the motion of fingers and the grasped object. Furthermore, we have developed a collision-rich grasping strategy to ensure the stability and security of the entire dynamic grasping process. A physical soft gripper was manufactured and affixed to a collaborative robotic arm to evaluate the performance of the collision detection mechanism. An experiment regarding testing the response time of the mechanism confirmed the system has the capability to react to the collision instantaneously. A dodging test was conducted to demonstrate the gripper can detect the direction and scale of external collisions precisely. 

**Abstract (ZH)**: 基于视觉的碰撞检测模块用于维持软 gripper 系统抓取圆柱形物体过程中的稳定抓取 

---
# Benchmarking Shortcutting Techniques for Multi-Robot-Arm Motion Planning 

**Title (ZH)**: 多机器人臂运动规划中捷径技术的基准测试 

**Authors**: Philip Huang, Yorai Shaoul, Jiaoyang Li  

**Link**: [PDF](https://arxiv.org/pdf/2508.05027)  

**Abstract**: Generating high-quality motion plans for multiple robot arms is challenging due to the high dimensionality of the system and the potential for inter-arm collisions. Traditional motion planning methods often produce motions that are suboptimal in terms of smoothness and execution time for multi-arm systems. Post-processing via shortcutting is a common approach to improve motion quality for efficient and smooth execution. However, in multi-arm scenarios, optimizing one arm's motion must not introduce collisions with other arms. Although existing multi-arm planning works often use some form of shortcutting techniques, their exact methodology and impact on performance are often vaguely described. In this work, we present a comprehensive study quantitatively comparing existing shortcutting methods for multi-arm trajectories across diverse simulated scenarios. We carefully analyze the pros and cons of each shortcutting method and propose two simple strategies for combining these methods to achieve the best performance-runtime tradeoff. Video, code, and dataset are available at this https URL. 

**Abstract (ZH)**: 多臂机器人系统中生成高质量的运动规划是一项挑战，由于系统的高维度和潜在的臂间碰撞。传统的运动规划方法往往会产生在多臂系统中不理想的平滑度和执行时间。通过捷径化后处理来提高运动质量以实现高效和平滑的执行是一种常见的方法。然而，在多臂场景中，优化一个臂的运动必须不会导致与其他臂发生碰撞。尽管现有的多臂规划工作经常使用某种形式的捷径化技术，但它们的具体方法及其对性能的影响通常描述模糊。在本工作中，我们对现有捷径化方法在多种仿真场景下的多臂轨迹进行定量比较，详细分析了每种捷径化方法的优缺点，并提出两种简单的策略来实现最佳的性能-时间-tradeoff。视频、代码和数据集可在以下链接找到：this https URL。 

---
# MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding 

**Title (ZH)**: MAG-Nav：基于语言的物体导航利用记忆预留活跃接地 

**Authors**: Weifan Zhang, Tingguang Li, Yuzhen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2508.05021)  

**Abstract**: Visual navigation in unknown environments based solely on natural language descriptions is a key capability for intelligent robots. In this work, we propose a navigation framework built upon off-the-shelf Visual Language Models (VLMs), enhanced with two human-inspired mechanisms: perspective-based active grounding, which dynamically adjusts the robot's viewpoint for improved visual inspection, and historical memory backtracking, which enables the system to retain and re-evaluate uncertain observations over time. Unlike existing approaches that passively rely on incidental visual inputs, our method actively optimizes perception and leverages memory to resolve ambiguity, significantly improving vision-language grounding in complex, unseen environments. Our framework operates in a zero-shot manner, achieving strong generalization to diverse and open-ended language descriptions without requiring labeled data or model fine-tuning. Experimental results on Habitat-Matterport 3D (HM3D) show that our method outperforms state-of-the-art approaches in language-driven object navigation. We further demonstrate its practicality through real-world deployment on a quadruped robot, achieving robust and effective navigation performance. 

**Abstract (ZH)**: 基于自然语言描述在未知环境中进行视觉导航是智能机器人的一项关键能力。本工作中，我们提出了一种构建在即用型视觉语言模型（VLMs）基础上的导航框架，并结合了两种受人类启发的机制：基于视角的主动对接接地，可动态调整机器人视角以提高视觉检查效果；历史记忆回溯机制，使系统能够保留并重新评估随时间变化的不确定性观察结果。与现有依赖于偶然视觉输入的方法不同，我们的方法主动优化感知并利用记忆来解决歧义性问题，在复杂且未见过的环境中显著提高了视觉-语言对接接地效果。该框架以零样本方式运行，无需标注数据或模型微调即可实现对多样化和开放性语言描述的强大泛化能力。实验结果表明，我们的方法在Habitat-Matterport 3D（HM3D）上优于最先进的基于语言驱动的物体导航方法。进一步的实地部署在四足机器人上展示了其实用性，实现了稳健且有效的导航性能。 

---
# Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots 

**Title (ZH)**: 层次深度确定性策略梯度在移动机器人自主迷宫导航中的应用 

**Authors**: Wenjie Hu, Ye Zhou, Hann Woei Ho  

**Link**: [PDF](https://arxiv.org/pdf/2508.04994)  

**Abstract**: Maze navigation is a fundamental challenge in robotics, requiring agents to traverse complex environments efficiently. While the Deep Deterministic Policy Gradient (DDPG) algorithm excels in control tasks, its performance in maze navigation suffers from sparse rewards, inefficient exploration, and long-horizon planning difficulties, often leading to low success rates and average rewards, sometimes even failing to achieve effective navigation. To address these limitations, this paper proposes an efficient Hierarchical DDPG (HDDPG) algorithm, which includes high-level and low-level policies. The high-level policy employs an advanced DDPG framework to generate intermediate subgoals from a long-term perspective and on a higher temporal scale. The low-level policy, also powered by the improved DDPG algorithm, generates primitive actions by observing current states and following the subgoal assigned by the high-level policy. The proposed method enhances stability with off-policy correction, refining subgoal assignments by relabeling historical experiences. Additionally, adaptive parameter space noise is utilized to improve exploration, and a reshaped intrinsic-extrinsic reward function is employed to boost learning efficiency. Further optimizations, including gradient clipping and Xavier initialization, are employed to improve robustness. The proposed algorithm is rigorously evaluated through numerical simulation experiments executed using the Robot Operating System (ROS) and Gazebo. Regarding the three distinct final targets in autonomous maze navigation tasks, HDDPG significantly overcomes the limitations of standard DDPG and its variants, improving the success rate by at least 56.59% and boosting the average reward by a minimum of 519.03 compared to baseline algorithms. 

**Abstract (ZH)**: 基于层次结构的Deep Deterministic Policy Gradient算法在迷宫导航中的应用：改进探索与奖励机制以提高成功率和平均奖励 

---
# Optimal Planning for Multi-Robot Simultaneous Area and Line Coverage Using Hierarchical Cyclic Merging Regulation 

**Title (ZH)**: 多
user
Improved Method for Solving the Multi-Vehicle Routing Problem pérdida
user
Improved Method for Solving the Multi-Vehicle Routing Problem pesticidos 

**Authors**: Tianyuan Zheng, Jingang Yi, Kaiyan Yu  

**Link**: [PDF](https://arxiv.org/pdf/2508.04981)  

**Abstract**: The double coverage problem focuses on determining efficient, collision-free routes for multiple robots to simultaneously cover linear features (e.g., surface cracks or road routes) and survey areas (e.g., parking lots or local regions) in known environments. In these problems, each robot carries two functional roles: service (linear feature footprint coverage) and exploration (complete area coverage). Service has a smaller operational footprint but incurs higher costs (e.g., time) compared to exploration. We present optimal planning algorithms for the double coverage problems using hierarchical cyclic merging regulation (HCMR). To reduce the complexity for optimal planning solutions, we analyze the manifold attachment process during graph traversal from a Morse theory perspective. We show that solutions satisfying minimum path length and collision-free constraints must belong to a Morse-bounded collection. To identify this collection, we introduce the HCMR algorithm. In HCMR, cyclic merging search regulates traversal behavior, while edge sequence back propagation converts these regulations into graph edge traversal sequences. Incorporating balanced partitioning, the optimal sequence is selected to generate routes for each robot. We prove the optimality of the HCMR algorithm under a fixed sweep direction. The multi-robot simulation results demonstrate that the HCMR algorithm significantly improves planned path length by at least 10.0%, reduces task time by at least 16.9% in average, and ensures conflict-free operation compared to other state-of-the-art planning methods. 

**Abstract (ZH)**: 多覆盖问题关注在已知环境中，确定多台机器人同时高效且无碰撞地覆盖线性特征（例如表面裂纹或道路路线）和调查区域（例如停车场或局部区域）的路径问题。在这些问题中，每台机器人承担两种功能角色：服务（线性特征覆盖）和探索（区域覆盖）。服务对操作足迹的要求较小，但需要较高的成本（例如时间）相比探索。我们使用层次循环合并调节（HCMR）方法提出了多覆盖问题的最优规划算法。为降低最优规划解决方案的复杂度，我们从莫尔斯理论的角度分析了图遍历时流形附着过程。我们证明，满足最短路径长度和无碰撞约束的解决方案必须属于莫尔斯有界集合。为识别此集合，我们引入了HCMR算法。在HCMR中，循环合并搜索调节遍历行为，而边序列反向传播将这些调节转化为图边遍历序列。结合平衡分区策略，选择最优序列生成每台机器人的路径。在固定扫掠方向下，我们证明了HCMR算法的最优性。多机器人仿真结果表明，与现有先进技术相比，HCMR算法显著缩短了规划路径长度至少10.0%，平均减少了任务时间至少16.9%，并确保了无冲突操作。 

---
# INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM 

**Title (ZH)**: 意图：通过交互直觉和 grounded VLM 推断类人机器人运动的趋势 

**Authors**: Jin Wang, Weijie Wang, Boyuan Deng, Heng Zhang, Rui Dai, Nikos Tsagarakis  

**Link**: [PDF](https://arxiv.org/pdf/2508.04931)  

**Abstract**: Traditional control and planning for robotic manipulation heavily rely on precise physical models and predefined action sequences. While effective in structured environments, such approaches often fail in real-world scenarios due to modeling inaccuracies and struggle to generalize to novel tasks. In contrast, humans intuitively interact with their surroundings, demonstrating remarkable adaptability, making efficient decisions through implicit physical understanding. In this work, we propose INTENTION, a novel framework enabling robots with learned interactive intuition and autonomous manipulation in diverse scenarios, by integrating Vision-Language Models (VLMs) based scene reasoning with interaction-driven memory. We introduce Memory Graph to record scenes from previous task interactions which embodies human-like understanding and decision-making about different tasks in real world. Meanwhile, we design an Intuitive Perceptor that extracts physical relations and affordances from visual scenes. Together, these components empower robots to infer appropriate interaction behaviors in new scenes without relying on repetitive instructions. Videos: this https URL 

**Abstract (ZH)**: 传统的人机 manipulation 控制和规划高度依赖精准的物理模型和 预先定义的动作序列。而在 结构化的环境中， 这些方法往往难以在真实世界场景中应用， 因为模型的不准确性和难以泛化到新颖的任务。相反地 人 人类能够直观地与周围环境交互并 屈展示出显著的适应性能力并 在 这个研究中 我们提出了 INTENTION， 一种新型框架， 使机器人具备学习到的互动直觉和自主操纵能力 在各种场景中。通过将视觉- 语言模型 (VLMs)与交互驱动的视觉场景推理整合. 我们引入了记忆图 (Memory Graph) 以记录先前交互驱动的场景理解，这体现了类似人类的决策过程 能够应对实际世界中的复杂任务。与此同时 on  我们提出了一种直觉感知器 以从视觉场景中 提取出物理关系 和 使用法属性。这些组件共同使机器人能够在特定场景中推理合适的交互行为 on 独立于重复的指令视频: 这个研究的网页链接为这个 URL。 

---
# On the causality between affective impact and coordinated human-robot reactions 

**Title (ZH)**: 情绪影响与协调的人机反应之间的影响因果关系 

**Authors**: Morten Roed Frederiksen, Kasper Støy  

**Link**: [PDF](https://arxiv.org/pdf/2508.04834)  

**Abstract**: In an effort to improve how robots function in social contexts, this paper investigates if a robot that actively shares a reaction to an event with a human alters how the human perceives the robot's affective impact. To verify this, we created two different test setups. One to highlight and isolate the reaction element of affective robot expressions, and one to investigate the effects of applying specific timing delays to a robot reacting to a physical encounter with a human. The first test was conducted with two different groups (n=84) of human observers, a test group and a control group both interacting with the robot. The second test was performed with 110 participants using increasingly longer reaction delays for the robot with every ten participants. The results show a statistically significant change (p$<$.05) in perceived affective impact for the robots when they react to an event shared with a human observer rather than reacting at random. The result also shows for shared physical interaction, the near-human reaction times from the robot are most appropriate for the scenario. The paper concludes that a delay time around 200ms may render the biggest impact on human observers for small-sized non-humanoid robots. It further concludes that a slightly shorter reaction time around 100ms is most effective when the goal is to make the human observers feel they made the biggest impact on the robot. 

**Abstract (ZH)**: 为了提高机器人在社会情境中的功能，本文探讨了机器人在与人类共享事件反应时，是否改变人类对机器人情感影响的感知。为此，我们创建了两种不同的实验设置。一种旨在突出和分离情感机器人表达中的反应元素，另一种则研究机器人在与人类物理互动后应用特定时间延迟效果。第一个实验在两个不同的人类观察者组（n=84）中进行，其中包括测试组和对照组，他们都与机器人互动。第二个实验使用了110名参与者，每隔十人设置一次逐渐延长的反应延迟。结果显示，当机器人在与人类观察者共享事件后再进行反应时，其感知的情感影响发生了统计学上的显著变化（p<0.05）。此外，当机器人与人类进行物理互动时，接近人类的反应时间是最合适的。本文得出结论，对于小型非人形机器人，约200ms的延迟时间可能会给人类观察者留下最大影响；并且，在目标是让人类观察者感觉他们对机器人产生了最大影响时，略短约100ms的反应时间更为有效。 

---
# TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution 

**Title (ZH)**: TrajEvo: 基于LLM驱动演化的时间序列预测启发式设计 

**Authors**: Zhikai Zhao, Chuanbo Hua, Federico Berto, Kanghoon Lee, Zihan Ma, Jiachen Li, Jinkyoo Park  

**Link**: [PDF](https://arxiv.org/pdf/2508.05616)  

**Abstract**: Trajectory prediction is a critical task in modeling human behavior, especially in safety-critical domains such as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy and generalizability. Although deep learning approaches offer improved performance, they typically suffer from high computational cost, limited explainability, and, importantly, poor generalization to out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We propose two key innovations: Cross-Generation Elite Sampling to encourage population diversity, and a Statistics Feedback Loop that enables the LLM to analyze and improve alternative predictions. Our evaluations demonstrate that TrajEvo outperforms existing heuristic methods across multiple real-world datasets, and notably surpasses both heuristic and deep learning methods in generalizing to an unseen OOD real-world dataset. TrajEvo marks a promising step toward the automated design of fast, explainable, and generalizable trajectory prediction heuristics. We release our source code to facilitate future research at this https URL. 

**Abstract (ZH)**: 轨迹预测是建模人类行为中的一项关键任务，尤其是在社会机器人和自动驾驶车辆导航等安全关键领域。传统的基于手工艺规则的启发式方法通常缺乏准确性和泛化能力。尽管深度学习方法能提供更好的性能，但它们通常面临高计算成本、解释性有限以及在分布外（OOD）场景下泛化能力差的问题。本文介绍了一种名为TrajEvo的框架，该框架利用大规模语言模型（LLMs）自动生成轨迹预测启发式方法。TrajEvo采用进化算法从历史轨迹数据中生成和精炼预测启发式方法。我们提出了两项关键创新：跨代精英采样以促进种群多样性，以及统计反馈循环以使LLM能够分析和改进替代预测。我们的评估表明，TrajEvo在多个真实世界数据集上优于现有启发式方法，并且特别在泛化到一个未见过的OOD真实世界数据集方面超越了启发式和深度学习方法。TrajEvo标志着自动化设计快速、可解释和可泛化的轨迹预测启发式方法迈出了一步。我们发布了源代码以促进未来研究。 

---
# Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle Controllers: A Framework and Case Study 

**Title (ZH)**: 面向交互感知自动驾驶车辆控制器的人本评估框架及案例研究 

**Authors**: Federico Scarì, Olger Siebinga, Arkady Zgonnikov  

**Link**: [PDF](https://arxiv.org/pdf/2508.05497)  

**Abstract**: As automated vehicles (AVs) increasingly integrate into mixed-traffic environments, evaluating their interaction with human-driven vehicles (HDVs) becomes critical. In most research focused on developing new AV control algorithms (controllers), the performance of these algorithms is assessed solely based on performance metrics such as collision avoidance or lane-keeping efficiency, while largely overlooking the human-centred dimensions of interaction with HDVs. This paper proposes a structured evaluation framework that addresses this gap by incorporating metrics grounded in the human-robot interaction literature. The framework spans four key domains: a) interaction effect, b) interaction perception, c) interaction effort, and d) interaction ability. These domains capture both the performance of the AV and its impact on human drivers around it. To demonstrate the utility of the framework, we apply it to a case study evaluating how a state-of-the-art AV controller interacts with human drivers in a merging scenario in a driving simulator. Measuring HDV-HDV interactions as a baseline, this study included one representative metric per domain: a) perceived safety, b) subjective ratings, specifically how participants perceived the other vehicle's driving behaviour (e.g., aggressiveness or predictability) , c) driver workload, and d) merging success. The results showed that incorporating metrics covering all four domains in the evaluation of AV controllers can illuminate critical differences in driver experience when interacting with AVs. This highlights the need for a more comprehensive evaluation approach. Our framework offers researchers, developers, and policymakers a systematic method for assessing AV behaviour beyond technical performance, fostering the development of AVs that are not only functionally capable but also understandable, acceptable, and safe from a human perspective. 

**Abstract (ZH)**: 随着自动驾驶车辆（AVs）越来越多地融入混合交通环境，评估其与人类驾驶车辆（HDVs）的交互变得至关重要。本论文提出了一种结构化的评估框架，该框架通过结合人机交互领域的指标，弥补了现有研究的不足。该框架涵盖四个关键领域：a) 交互效果，b) 交互感知，c) 交互努力，和 d) 交互能力。这些领域既捕捉了AV的性能，也反映了其对周围人类驾驶员的影响。为了展示该框架的实用性，我们将它应用于一项案例研究，该研究在驾驶模拟器中评估了一种先进AV控制器在合并场景中与人类驾驶员的交互。以人类驾驶车辆之间的交互作为基线，该研究每领域包含一个代表性指标：a) 感知安全性，b) 主观评价，特别是参与者如何感知另一辆车的驾驶行为（如激进性或可预测性），c) 驾驶员工作负担，和 d) 合并成功率。结果表明，在评估AV控制器时整合覆盖所有四个领域的指标，可以 illuminating 与AV交互的驾驶员体验的关键差异。这突出了需要采用更全面的评估方法。我们的框架为研究人员、开发人员和政策制定者提供了一种系统的方法来评估AV行为，超越技术性能，促进开发既功能强大又易于理解、可接受和安全的AV。 

---
# ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning 

**Title (ZH)**: ASKDAgger: 基于主动技能级数据聚合的交互式模仿学习 

**Authors**: Jelle Luijkx, Zlatan Ajanović, Laura Ferranti, Jens Kober  

**Link**: [PDF](https://arxiv.org/pdf/2508.05310)  

**Abstract**: Human teaching effort is a significant bottleneck for the broader applicability of interactive imitation learning. To reduce the number of required queries, existing methods employ active learning to query the human teacher only in uncertain, risky, or novel situations. However, during these queries, the novice's planned actions are not utilized despite containing valuable information, such as the novice's capabilities, as well as corresponding uncertainty levels. To this end, we allow the novice to say: "I plan to do this, but I am uncertain." We introduce the Active Skill-level Data Aggregation (ASkDAgger) framework, which leverages teacher feedback on the novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating threshold to track sensitivity, specificity, or a minimum success rate; (2) Foresight Interactive Experience Replay (FIER), which recasts valid and relabeled novice action plans into demonstrations; and (3) Prioritized Interactive Experience Replay (PIER), which prioritizes replay based on uncertainty, novice success, and demonstration age. Together, these components balance query frequency with failure incidence, reduce the number of required demonstration annotations, improve generalization, and speed up adaptation to changing domains. We validate the effectiveness of ASkDAgger through language-conditioned manipulation tasks in both simulation and real-world environments. Code, data, and videos are available at this https URL. 

**Abstract (ZH)**: 人类教学努力是交互模仿学习广泛应用的重要瓶颈。为减少所需查询次数，现有方法仅在不确定、有风险或新颖的情况下向人类教师查询。然而，在这些查询过程中，初学者计划的动作未被利用，尽管这些动作包含有价值的信息，如初学者的能力以及相应的不确定性水平。为此，我们允许初学者说：“我打算这样做，但不确定。”我们引入了主动技能级数据聚合（ASkDAgger）框架，该框架通过三种关键方式利用教师对初学者计划的反馈：（1）S-感知门控（SAG）：调整门限以跟踪敏感性、特异性或最低成功率；（2）前瞻性互动经验重放（FIER），将有效的、重新标记的初学者行动计划重新构造成示范；（3）基于不确定性的优先级互动经验重放（PIER），根据不确定性、初学者成功情况和示范年龄进行优先重放。这些组件共同平衡了查询频率与失败发生率，减少了所需示范标注的数量，提高了泛化能力，并加快了对变化领域的适应。我们通过模拟和真实环境中的语言条件操作任务验证了ASkDAgger的有效性。相关代码、数据和视频可通过以下链接获取。 

---
# Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning 

**Title (ZH)**: 分析多模态感知对模仿学习中样本复杂性和优化景观的影响 

**Authors**: Luai Abuelsamen, Temitope Lukman Adebanjo  

**Link**: [PDF](https://arxiv.org/pdf/2508.05077)  

**Abstract**: This paper examines the theoretical foundations of multimodal imitation learning through the lens of statistical learning theory. We analyze how multimodal perception (RGB-D, proprioception, language) affects sample complexity and optimization landscapes in imitation policies. Building on recent advances in multimodal learning theory, we show that properly integrated multimodal policies can achieve tighter generalization bounds and more favorable optimization landscapes than their unimodal counterparts. We provide a comprehensive review of theoretical frameworks that explain why multimodal architectures like PerAct and CLIPort achieve superior performance, connecting these empirical results to fundamental concepts in Rademacher complexity, PAC learning, and information theory. 

**Abstract (ZH)**: 本文通过统计学习理论的视角考察了多模态模仿学习的理论基础。我们分析了多模态感知（RGB-D、本体感觉、语言）对样本复杂性和模仿策略优化景观的影响。基于近期多模态学习理论的进展，我们展示出恰当整合的多模态策略可以比其单模态对应策略获得更紧的泛化界和更有利于优化的景观。我们提供了一个全面的理论框架综述，解释了诸如PerAct和CLIPort等多模态架构为何能实现优越性能，并将这些实证结果与拉德马赫复杂性、PAC学习和信息理论中的基本概念联系起来。 

---
# BTPG-max: Achieving Local Maximal Bidirectional Pairs for Bidirectional Temporal Plan Graphs 

**Title (ZH)**: BTPG-max：实现双向时间计划图的局部最大双向配对 

**Authors**: Yifan Su, Rishi Veerapaneni, Jiaoyang Li  

**Link**: [PDF](https://arxiv.org/pdf/2508.04849)  

**Abstract**: Multi-Agent Path Finding (MAPF) requires computing collision-free paths for multiple agents in shared environment. Most MAPF planners assume that each agent reaches a specific location at a specific timestep, but this is infeasible to directly follow on real systems where delays often occur. To address collisions caused by agents deviating due to delays, the Temporal Plan Graph (TPG) was proposed, which converts a MAPF time dependent solution into a time independent set of inter-agent dependencies. Recently, a Bidirectional TPG (BTPG) was proposed which relaxed some dependencies into ``bidirectional pairs" and improved efficiency of agents executing their MAPF solution with delays. Our work improves upon this prior work by designing an algorithm, BPTG-max, that finds more bidirectional pairs. Our main theoretical contribution is in designing the BTPG-max algorithm is locally optimal, i.e. which constructs a BTPG where no additional bidirectional pairs can be added. We also show how in practice BTPG-max leads to BTPGs with significantly more bidirectional edges, superior anytime behavior, and improves robustness to delays. 

**Abstract (ZH)**: 多智能体路径规划（MAPF）需要在共享环境中为多个智能体计算无碰撞路径。大多数MAPF规划者假设每个智能体会在特定的时间步到达特定位置，但在实际系统中由于存在延迟，这通常是不现实的。为了解决因延迟导致路径偏离而引发的碰撞问题，提出了时间计划图（TPG），它将依赖于时间的MAPF解决方案转换为一系列独立于时间的智能体间依赖关系集合。最近，提出了双向时间计划图（BTPG），它将一些依赖关系松弛为“双向对”，从而改进了智能体在存在延迟时执行MAPF解决方案的效率。我们的工作在此基础上设计了一种算法BPTG-max，以找到更多的双向对。我们的主要理论贡献在于设计BTPG-max算法是局部最优的，即构建一个BTPG，其中不再能添加额外的双向对。我们还展示了在实际应用中，BTPG-max能够显著增加双向边的数量，表现出更好的任意时间行为，并增强对延迟的鲁棒性。 

---
