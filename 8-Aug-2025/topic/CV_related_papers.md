# UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation 

**Title (ZH)**: UNCAGE: 对比注意力指导在文本到图像生成中掩码生成变换器中的应用 

**Authors**: Wonjun Kang, Byeongkeun Ahn, Minjae Lee, Kevin Galim, Seunghyuk Oh, Hyung Il Koo, Nam Ik Cho  

**Link**: [PDF](https://arxiv.org/pdf/2508.05399)  

**Abstract**: Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at this https URL. 

**Abstract (ZH)**: 基于文本的图像生成：利用掩码生成变换器克服自回归模型的内在限制 

---
# SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion 

**Title (ZH)**: SGDFuse：SAM引导的高保真红外和可见光图像融合 

**Authors**: Xiaoyang Zhang, Zhen Hua, Yakun Ju, Wei Zhou, Jun Liu, Alex C. Kot  

**Link**: [PDF](https://arxiv.org/pdf/2508.05264)  

**Abstract**: Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion model's coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at this https URL. 

**Abstract (ZH)**: 红外与可见光图像融合（IVIF）旨在结合红外图像的热辐射信息与可见光图像丰富的纹理细节，以提升下游视觉任务的感知能力。然而，现有方法往往由于缺乏对场景的深层语义理解而难以保留关键目标，同时融合过程本身也可能引入伪影和细节丢失，严重损害了图像质量和任务性能。为了解决这些问题，本文提出了一种由Segment Anything Model (SAM) 引导的条件扩散模型SGDFuse，以实现高度保真的语义感知图像融合。我们方法的核心是利用SAM生成的高质量语义掩膜作为显性先验，通过条件扩散模型引导融合过程的优化。具体而言，框架分为两阶段：首先进行多模态特征的初步融合，然后利用SAM生成的语义掩膜与初步融合图像作为条件，驱动扩散模型自底向上的去噪生成。这不仅确保了融合过程具有显性的语义方向性，还保证了最终结果的高度保真度。大量实验表明，SGDFuse在主观评价和客观评价中均取得了领先性能，并在下游任务的适应性方面表现出色，提供了一种解决图像融合核心挑战的强大方案。SGDFuse的代码可在以下链接获取。 

---
# Robust Tracking with Particle Filtering for Fluorescent Cardiac Imaging 

**Title (ZH)**: 荧光心脏成像中基于粒子滤波的鲁棒跟踪 

**Authors**: Suresh Guttikonda, Maximilian Neidhart, Johanna Sprenger, Johannes Petersen, Christian Detter, Alexander Schlaefer  

**Link**: [PDF](https://arxiv.org/pdf/2508.05262)  

**Abstract**: Intraoperative fluorescent cardiac imaging enables quality control following coronary bypass grafting surgery. We can estimate local quantitative indicators, such as cardiac perfusion, by tracking local feature points. However, heart motion and significant fluctuations in image characteristics caused by vessel structural enrichment limit traditional tracking methods. We propose a particle filtering tracker based on cyclicconsistency checks to robustly track particles sampled to follow target landmarks. Our method tracks 117 targets simultaneously at 25.4 fps, allowing real-time estimates during interventions. It achieves a tracking error of (5.00 +/- 0.22 px) and outperforms other deep learning trackers (22.3 +/- 1.1 px) and conventional trackers (58.1 +/- 27.1 px). 

**Abstract (ZH)**: 术中荧光心脏成像实现冠状动脉旁路移植手术后的质量控制 

---
# CF3: Compact and Fast 3D Feature Fields 

**Title (ZH)**: CF3: 紧凑快速的3D特征场 

**Authors**: Hyunjoon Lee, Joonkyu Min, Jaesik Park  

**Link**: [PDF](https://arxiv.org/pdf/2508.05254)  

**Abstract**: 3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D foundation models. However, most approaches rely on a bottom-up optimization process that treats raw 2D features as ground truth, incurring increased computational costs. We propose a top-down pipeline for constructing compact and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast weighted fusion of multi-view 2D features with pre-trained Gaussians. This approach enables training a per-Gaussian autoencoder directly on the lifted features, instead of training autoencoders in the 2D domain. As a result, the autoencoder better aligns with the feature distribution. More importantly, we introduce an adaptive sparsification method that optimizes the Gaussian attributes of the feature field while pruning and merging the redundant Gaussians, constructing an efficient representation with preserved geometric details. Our approach achieves a competitive 3D feature field using as little as 5% of the Gaussians compared to Feature-3DGS. 

**Abstract (ZH)**: 3DGS结合了丰富的2D基础模型信息。然而，大多数方法依赖于自底向上的优化过程，将原始2D特征视为 ground truth，导致计算成本增加。我们提出了一种自顶向下的管道，用于构建紧凑且快速的3D高斯特征场，即CF3。我们首先执行多视图2D特征和预训练高斯之间的快速加权融合。这种方法允许直接在提升特征上训练每个高斯的自编码器，而不是在2D域中训练自编码器。因此，自编码器更好地与特征分布对齐。更重要的是，我们引入了一种自适应稀疏化方法，在裁剪和合并冗余高斯的同时优化特征场的高斯属性，构建一个高效的表示，同时保留几何细节。我们的方法使用比Feature-3DGS少5%的高斯实现了竞争性的3D特征场。 

---
# Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models 

**Title (ZH)**: 权衡trade-off之道：视觉-语言模型零样本对抗鲁棒性的防御策略综述 

**Authors**: Zane Xu, Jason Sun  

**Link**: [PDF](https://arxiv.org/pdf/2508.05237)  

**Abstract**: This report synthesizes eight seminal papers on the zero-shot adversarial robustness of vision-language models (VLMs) like CLIP. A central challenge in this domain is the inherent trade-off between enhancing adversarial robustness and preserving the model's zero-shot generalization capabilities. We analyze two primary defense paradigms: Adversarial Fine-Tuning (AFT), which modifies model parameters, and Training-Free/Test-Time Defenses, which preserve them. We trace the evolution from alignment-preserving methods (TeCoA) to embedding space re-engineering (LAAT, TIMA), and from input heuristics (AOM, TTC) to latent-space purification (CLIPure). Finally, we identify key challenges and future directions including hybrid defense strategies and adversarial pre-training. 

**Abstract (ZH)**: 本报告综合了八篇关于视觉语言模型（VLMs）如CLIP的零样本 adversarial 稳定性的奠基性论文。该领域的一个核心挑战是在增强 adversarial 稳定性和保持模型的零样本泛化能力之间存在的固有trade-off。我们分析了两种主要的防御范式：对抗微调（AFT），其修改模型参数，以及无需训练/测试时防御，其保持模型参数不变。我们追溯了从保对齐方法（TeCoA）到嵌入空间重构（LAAT、TIMA）的发展，以及从输入启发式（AOM、TTC）到潜在空间净化（CLIPure）的变化。最后，我们指出了关键挑战和未来发展方向，包括混合防御策略和对抗预训练。 

---
# Refining Gaussian Splatting: A Volumetric Densification Approach 

**Title (ZH)**: 改进高斯绘制：一种体形密度化方法 

**Authors**: Mohamed Abdul Gafoor, Marius Preda, Titus Zaharia  

**Link**: [PDF](https://arxiv.org/pdf/2508.05187)  

**Abstract**: Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS) often depends on effective point primitive management. The underlying Adaptive Density Control (ADC) process addresses this issue by automating densification and pruning. Yet, the vanilla 3DGS densification strategy shows key shortcomings. To address this issue, in this paper we introduce a novel density control method, which exploits the volumes of inertia associated to each Gaussian function to guide the refinement process. Furthermore, we study the effect of both traditional Structure from Motion (SfM) and Deep Image Matching (DIM) methods for point cloud initialization. Extensive experimental evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses 3DGS in reconstruction quality, delivering encouraging performance across diverse scenes. 

**Abstract (ZH)**: 在3D高斯点绘制（3DGS）中实现高质量的新视角合成往往取决于有效的点元管理。底层的自适应密度控制（ADC）过程通过自动化密集化和修剪来解决这一问题。然而，vanilla 3DGS的密集化策略显示出关键缺陷。为了解决这一问题，本文提出了一种新的密度控制方法，通过利用每个高斯函数相关的动量体积来指导细化过程。此外，我们研究了传统结构从运动（SfM）和深度图像匹配（DIM）方法对点云初始化的影响。在Mip-NeRF 360数据集上的广泛实验评估表明，我们的方法在重建质量上超越了3DGS，并在多种场景中取得了令人鼓舞的性能。 

---
# Latent Expression Generation for Referring Image Segmentation and Grounding 

**Title (ZH)**: 潜在表达生成用于引用图像分割和 grounding 

**Authors**: Seonghoon Yu, Joonbeom Hong, Joonseok Lee, Jeany Son  

**Link**: [PDF](https://arxiv.org/pdf/2508.05123)  

**Abstract**: Visual grounding tasks, such as referring image segmentation (RIS) and referring expression comprehension (REC), aim to localize a target object based on a given textual description. The target object in an image can be described in multiple ways, reflecting diverse attributes such as color, position, and more. However, most existing methods rely on a single textual input, which captures only a fraction of the rich information available in the visual domain. This mismatch between rich visual details and sparse textual cues can lead to the misidentification of similar objects. To address this, we propose a novel visual grounding framework that leverages multiple latent expressions generated from a single textual input by incorporating complementary visual details absent from the original description. Specifically, we introduce subject distributor and visual concept injector modules to embed both shared-subject and distinct-attributes concepts into the latent representations, thereby capturing unique and target-specific visual cues. We also propose a positive-margin contrastive learning strategy to align all latent expressions with the original text while preserving subtle variations. Experimental results show that our method not only outperforms state-of-the-art RIS and REC approaches on multiple benchmarks but also achieves outstanding performance on the generalized referring expression segmentation (GRES) benchmark. 

**Abstract (ZH)**: 视觉定位任务，如引用图像分割（RIS）和引用表达理解（REC），旨在基于给定的文字描述定位目标对象。图像中的目标对象可以有多样的描述方式，反映其颜色、位置等多种属性。然而，现有大多数方法依赖单一的文字输入，这只能捕获视觉领域中丰富信息的一小部分。这种视觉细节丰富与文字提示稀疏之间的不匹配可能导致相似对象的误识别。为此，我们提出了一个新颖的视觉定位框架，通过结合单一文字输入生成的多个互补的潜在表达，利用缺失的视觉细节。具体地，我们引入了主题分配器和视觉概念注入模块，将共享主题和独特属性的概念嵌入到潜在表示中，从而捕捉独特的且针对目标的视觉线索。我们还提出了一种正边界对比学习策略，使所有潜在表达与原始文本对齐的同时保留细微差异。实验结果表明，我们的方法不仅在多个基准上优于现有的RIS和REC方法，还在泛化引用表达分割（GRES）基准上取得了优异的表现。 

---
# Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks 

**Title (ZH)**: 基于卷积神经网络和生成对抗网络的自动图像着色 

**Authors**: Ruiyu Li, Changyuan Qiu, Hangrui Cao, Qihan Ren, Yuqing Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2508.05068)  

**Abstract**: Image colorization, the task of adding colors to grayscale images, has been the focus of significant research efforts in computer vision in recent years for its various application areas such as color restoration and automatic animation colorization [15, 1]. The colorization problem is challenging as it is highly ill-posed with two out of three image dimensions lost, resulting in large degrees of freedom. However, semantics of the scene as well as the surface texture could provide important cues for colors: the sky is typically blue, the clouds are typically white and the grass is typically green, and there are huge amounts of training data available for learning such priors since any colored image could serve as a training data point [20].
Colorization is initially formulated as a regression task[5], which ignores the multi-modal nature of color prediction. In this project, we explore automatic image colorization via classification and adversarial learning. We will build our models on prior works, apply modifications for our specific scenario and make comparisons. 

**Abstract (ZH)**: 图像着色，即为灰度图像添加颜色的任务，近年来在计算机视觉领域受到了广泛关注，因其在颜色恢复和自动动画着色等各个应用领域的潜力[15, 1]。由于三个图像维度中有两个丢失，使得着色问题具有高度的不明确性，因此具有较大的自由度。然而，场景的语义以及表面纹理可以提供重要的颜色线索：天空通常是蓝色，云通常是白色，草地通常是绿色，而且有大量的训练数据可供学习这些先验知识，因为任何彩色图像都可以作为训练数据点[20]。

着色最初被形式化为一个回归任务[5]，这忽略了颜色预测的多模态性质。在本项目中，我们将通过分类和对抗学习探索自动图像着色。我们将建立在先前工作之上，对特定场景进行修改，并进行比较。 

---
# Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation 

**Title (ZH)**: 无幻听音乐生成：一种基于强化学习的偏好优化框架，用于可靠的歌曲生成 

**Authors**: Huaicheng Zhang, Wei Tan, Guangzheng Li, Yixuan Zhang, Hangting Chen, Shun Lei, Chenyu Yang, Zhiyong Wu, Shuai Wang, Qijun Huang, Dong Yu  

**Link**: [PDF](https://arxiv.org/pdf/2508.05011)  

**Abstract**: Recent advances in audio-based generative language models have accelerated AI-driven lyric-to-song generation. However, these models frequently suffer from content hallucination, producing outputs misaligned with the input lyrics and undermining musical coherence. Current supervised fine-tuning (SFT) approaches, limited by passive label-fitting, exhibit constrained self-improvement and poor hallucination mitigation. To address this core challenge, we propose a novel reinforcement learning (RL) framework leveraging preference optimization for hallucination control. Our key contributions include: (1) Developing a robust hallucination preference dataset constructed via phoneme error rate (PER) computation and rule-based filtering to capture alignment with human expectations; (2) Implementing and evaluating three distinct preference optimization strategies within the RL framework: Direct Preference Optimization (DPO), Proximal Policy Optimization (PPO), and Group Relative Policy Optimization (GRPO). DPO operates off-policy to enhance positive token likelihood, achieving a significant 7.4% PER reduction. PPO and GRPO employ an on-policy approach, training a PER-based reward model to iteratively optimize sequences via reward maximization and KL-regularization, yielding PER reductions of 4.9% and 4.7%, respectively. Comprehensive objective and subjective evaluations confirm that our methods effectively suppress hallucinations while preserving musical quality. Crucially, this work presents a systematic, RL-based solution to hallucination control in lyric-to-song generation. The framework's transferability also unlocks potential for music style adherence and musicality enhancement, opening new avenues for future generative song research. 

**Abstract (ZH)**: Recent Advances in Audio-Based Generative Language Models Have Accelerated AI-Driven Lyric-to-Song Generation: A Preference Optimization Approach via Reinforcement Learning for Controlling Content Hallucination 

---
# UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS 

**Title (ZH)**: UGOD：基于不确定性引导的可微透明度和软dropout的稀薄投影三维图像增强方法 

**Authors**: Zhihao Guo, Peng Wang, Zidong Chen, Xiangyu Kong, Yan Lyu, Guanyu Gao, Liangxiu Han  

**Link**: [PDF](https://arxiv.org/pdf/2508.04968)  

**Abstract**: 3D Gaussian Splatting (3DGS) has become a competitive approach for novel view synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian projection and blending. However, Gaussians are treated equally weighted for rendering in most 3DGS methods, making them prone to overfitting, which is particularly the case in sparse-view scenarios. To address this, we investigate how adaptive weighting of Gaussians affects rendering quality, which is characterised by learned uncertainties proposed. This learned uncertainty serves two key purposes: first, it guides the differentiable update of Gaussian opacity while preserving the 3DGS pipeline integrity; second, the uncertainty undergoes soft differentiable dropout regularisation, which strategically transforms the original uncertainty into continuous drop probabilities that govern the final Gaussian projection and blending process for rendering. Extensive experimental results over widely adopted datasets demonstrate that our method outperforms rivals in sparse-view 3D synthesis, achieving higher quality reconstruction with fewer Gaussians in most datasets compared to existing sparse-view approaches, e.g., compared to DropGaussian, our method achieves 3.27\% PSNR improvements on the MipNeRF 360 dataset. 

**Abstract (ZH)**: 33高斯聚类（（33 33 Η e �.e ） 在 e e e 用于密集视 e e e 渲晰的 e e e 三维合成 e e e 由于三维高 e e e 高 e e e 高 e e e 高 e e e 高 Η e e e e e e  e e � e e  e e e e e e e e  e e e  e e  e e e e e e e e  e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e=e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e= e e e e e e e e e e � e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e[e pérdida d predic e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e]
三维高 e e e � e e e � e e e � e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e.MSGD e PSGR 提 e 3 e e e e e e e e e e e e e e e e e e e e PSNR 提 e e e 3  e e e e e e e e  e e e e e e e e e e e e e e e e 3 e e e e e e e 3 e e e e e e e e  e e e  e e e  e e e  e e e  e e e  e e e  e e  e e e  e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e 3 e  e e 3 e e e e e e e e ect G e  e 3 e e e e e e e e  e e  e  e e e e e e e  e e e  e e  e  e e e e e e e  e e e  e e e e e e e e e e  e  e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e. e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e eภาวะ KD 

---
# Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering 

**Title (ZH)**: 基于语义聚类解决动词歧义的视觉活动识别鲁棒评估方法 

**Authors**: Louie Hong Yao, Nicholas Jarvis, Tianyu Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2508.04945)  

**Abstract**: Evaluating visual activity recognition systems is challenging due to inherent ambiguities in verb semantics and image interpretation. When describing actions in images, synonymous verbs can refer to the same event (e.g., brushing vs. grooming), while different perspectives can lead to equally valid but distinct verb choices (e.g., piloting vs. operating). Standard exact-match evaluation, which relies on a single gold answer, fails to capture these ambiguities, resulting in an incomplete assessment of model performance. To address this, we propose a vision-language clustering framework that constructs verb sense clusters, providing a more robust evaluation. Our analysis of the imSitu dataset shows that each image maps to an average of 2.8 sense clusters, with each cluster representing a distinct perspective of the image. We evaluate multiple activity recognition models and compare our cluster-based evaluation with standard evaluation methods. Additionally, our human alignment analysis suggests that the cluster-based evaluation better aligns with human judgements, offering a more nuanced assessment of model performance. 

**Abstract (ZH)**: 评估视觉活动识别系统具有挑战性，因为动词语义和图像解释中存在的内在歧义。在描述图像中的动作时，同义动词可以指同一个事件（例如，刷 vs. 梳理），而不同的视角会导致不同的但同样合理的动词选择（例如，驾驶 vs. 操作）。依赖单一标准答案的标准精确匹配评估无法捕捉这些歧义，导致对模型性能的评估不完整。为解决这一问题，我们提出了一种视觉-语言聚类框架，构建动词意义簇，提供更稳健的评估。通过对imSitu数据集的分析，我们发现每张图像平均映射到2.8个意义簇，每个簇代表图像的一种不同视角。我们评估了多种活动识别模型，并将基于簇的评估与标准评估方法进行了比较。此外，我们的手动对齐分析表明，基于簇的评估与人类判断更一致，提供了对模型性能更细致的评估。 

---
# TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring 

**Title (ZH)**: TRKT：带有时间增强关系感知知识转移的弱监督动态场景图生成 

**Authors**: Zhu Xu, Ting Lei, Zhimin Li, Guan Wang, Qingchao Chen, Yuxin Peng, Yang liu  

**Link**: [PDF](https://arxiv.org/pdf/2508.04943)  

**Abstract**: Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each video frame by detecting objects and predicting their relationships. Weakly Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized scene graph from a single frame per video for training. Existing WS-DSGG methods depend on an off-the-shelf external object detector to generate pseudo labels for subsequent DSGG training. However, detectors trained on static, object-centric images struggle in dynamic, relation-aware scenarios required for DSGG, leading to inaccurate localization and low-confidence proposals. To address the challenges posed by external object detectors in WS-DSGG, we propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT) method, which leverages knowledge to enhance detection in relation-aware dynamic scenarios. TRKT is built on two key components:(1)Relation-aware knowledge mining: we first employ object and relation class decoders that generate category-specific attention maps to highlight both object regions and interactive areas. Then we propose an Inter-frame Attention Augmentation strategy that exploits optical flow for neighboring frames to enhance the attention maps, making them motion-aware and robust to motion blur. This step yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we introduce a Dual-stream Fusion Module that integrates category-specific attention maps into external detections to refine object localization and boost confidence scores for object proposals. Extensive experiments demonstrate that TRKT achieves state-of-the-art performance on Action Genome dataset. Our code is avaliable at this https URL. 

**Abstract (ZH)**: 动态场景图生成（DSGG）旨在通过检测物体并通过预测它们之间的关系为每个视频帧创建场景图。弱监督DSGG（WS-DSGG）通过使用单个视频帧中的非局部化场景图来减少标注工作量进行训练。现有的WS-DSGG方法依赖于现成的外部物体检测器为后续的DSGG训练生成伪标签。然而，这些检测器在DSGG所需的动态、关系感知场景中表现不佳，导致定位不准确且置信度低。为了解决外部物体检测器在WS-DSGG中带来的挑战，我们提出了一种基于时间增强的关系感知知识转移（TRKT）方法，该方法利用知识来增强关系感知的动态场景中的检测。TRKT包括两个关键组件：（1）关系感知知识挖掘：首先使用对象类和关系类解码器生成具有类别特定关注图，以突出显示对象区域和交互区域。然后提出了一种基于相邻帧的时移注意力增强策略，利用光流增强注意图，使它们具有运动感知力并能抵抗运动模糊。这一步骤为WS-DSGG提供了关系和运动感知的知识挖掘。（2）引入了一种双流融合模块，将类别特定注意力图集成到外部检测中，以细化物体定位并提升物体候选的置信分数。广泛的实验表明，TRKT在Action Genome数据集上达到了最先进的性能。我们的代码可在以下链接获取。 

---
# Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens 

**Title (ZH)**: 基于校准标记扩展单目深度估计模型以适应鱼眼相机 

**Authors**: Suchisrit Gangopadhyay, Jung-Hee Kim, Xien Chen, Patrick Rim, Hyoungseob Park, Alex Wong  

**Link**: [PDF](https://arxiv.org/pdf/2508.04928)  

**Abstract**: We propose a method to extend foundational monocular depth estimators (FMDEs), trained on perspective images, to fisheye images. Despite being trained on tens of millions of images, FMDEs are susceptible to the covariate shift introduced by changes in camera calibration (intrinsic, distortion) parameters, leading to erroneous depth estimates. Our method aligns the distribution of latent embeddings encoding fisheye images to those of perspective images, enabling the reuse of FMDEs for fisheye cameras without retraining or finetuning. To this end, we introduce a set of Calibration Tokens as a light-weight adaptation mechanism that modulates the latent embeddings for alignment. By exploiting the already expressive latent space of FMDEs, we posit that modulating their embeddings avoids the negative impact of artifacts and loss introduced in conventional recalibration or map projection to a canonical reference frame in the image space. Our method is self-supervised and does not require fisheye images but leverages publicly available large-scale perspective image datasets. This is done by recalibrating perspective images to fisheye images, and enforcing consistency between their estimates during training. We evaluate our approach with several FMDEs, on both indoors and outdoors, where we consistently improve over state-of-the-art methods using a single set of tokens for both. Code available at: this https URL. 

**Abstract (ZH)**: 我们提出了一种方法，将基于视角图像训练的基线单目深度估计器（FMDEs）扩展到鱼眼图像。尽管FMDEs是在数千万张图像上进行训练的，但它们仍然容易受到由相机标定（固有参数和畸变）变化引入的 covariate shift 的影响，导致错误的深度估计。我们的方法通过将鱼眼图像的潜在嵌入分布调整到视角图像的分布，使得不需要重新训练或微调即可在鱼眼相机上重用FMDEs。为此，我们引入了一组校准标记作为轻量级的适应机制，用于调整潜在嵌入以实现对齐。通过利用FMDEs已有的表达性潜在空间，我们认为调节其嵌入可以避免常规重新标定或将图像空间映射到标准参考框架中引入的负影响。我们的方法是自监督的，不需要鱼眼图像，而是利用公开可用的大规模视角图像数据集。这通过将视角图像重新标定为鱼眼图像，并在训练过程中强制估计之间的一致性来实现。我们使用一组标记在室内和室外场景中评估了我们的方法，结果表明这种方法在使用单组标记时优于现有方法。代码可在以下链接获取：this https URL。 

---
# Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off 

**Title (ZH)**: Voost：一种统一且可扩展的双向虚拟试穿与脱下扩散变换器 

**Authors**: Seungyong Lee, Jeong-gi Kwak  

**Link**: [PDF](https://arxiv.org/pdf/2508.04825)  

**Abstract**: Virtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization. 

**Abstract (ZH)**: 虚拟试穿旨在合成一个人穿着目标衣物的现实图像，但准确建模衣物-人体对应关系仍然是一个持久性的挑战，尤其是在姿态和外观变化的情况下。在本文中，我们提出了一种统一且可扩展的框架Voost，该框架使用单个扩散变换器联合学习虚拟试穿和脱穿。通过将两个任务联合建模，Voost使得每件衣物-人体配对能够监督两个方向，并支持生成方向和衣物类别的灵活条件，从而增强衣物-人体关系推理，而无需专门网络、辅助损失或额外标签。此外，我们引入了两种推理时的技术：注意力温度缩放以提高对分辨率或遮罩变化的鲁棒性，以及利用任务间双向一致性的自我校正采样。大量实验表明，Voost在试穿和脱穿基准测试中均取得了最先进的成果，一致地在对齐精度、视觉保真度和泛化方面优于强大基准。 

---
# Cross-Domain Image Synthesis: Generating H&E from Multiplex Biomarker Imaging 

**Title (ZH)**: 跨域图像合成：从多重生物标志物成像生成HE图像 

**Authors**: Jillur Rahman Saurav, Mohammad Sadegh Nasr, Jacob M. Luber  

**Link**: [PDF](https://arxiv.org/pdf/2508.04734)  

**Abstract**: While multiplex immunofluorescence (mIF) imaging provides deep, spatially-resolved molecular data, integrating this information with the morphological standard of Hematoxylin & Eosin (H&E) can be very important for obtaining complementary information about the underlying tissue. Generating a virtual H&E stain from mIF data offers a powerful solution, providing immediate morphological context. Crucially, this approach enables the application of the vast ecosystem of H&E-based computer-aided diagnosis (CAD) tools to analyze rich molecular data, bridging the gap between molecular and morphological analysis. In this work, we investigate the use of a multi-level Vector-Quantized Generative Adversarial Network (VQGAN) to create high-fidelity virtual H&E stains from mIF images. We rigorously evaluated our VQGAN against a standard conditional GAN (cGAN) baseline on two publicly available colorectal cancer datasets, assessing performance on both image similarity and functional utility for downstream analysis. Our results show that while both architectures produce visually plausible images, the virtual stains generated by our VQGAN provide a more effective substrate for computer-aided diagnosis. Specifically, downstream nuclei segmentation and semantic preservation in tissue classification tasks performed on VQGAN-generated images demonstrate superior performance and agreement with ground-truth analysis compared to those from the cGAN. This work establishes that a multi-level VQGAN is a robust and superior architecture for generating scientifically useful virtual stains, offering a viable pathway to integrate the rich molecular data of mIF into established and powerful H&E-based analytical workflows. 

**Abstract (ZH)**: 基于多层次向量量化生成对抗网络生成高保真虚拟HE染色图像以整合分子与形态学分析 

---
