# Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation 

**Title (ZH)**: Genie Envisioner: 一个统一的机器人 manipulation 基础平台 

**Authors**: Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, Liliang Chen, Shuicheng Yan, Maoqing Yao, Guanghui Ren  

**Link**: [PDF](https://arxiv.org/pdf/2508.05635)  

**Abstract**: We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly. 

**Abstract (ZH)**: 我们引入了Genie Envisioner (GE)，一个统一的机器人操作基础平台，该平台在单一视频生成框架内集成了策略学习、评估和模拟。核心部分GE-Base是一个大规模的、基于指令的视频扩散模型，能够在结构化的潜空间中捕捉现实世界机器人交互的空间、时间和语义动态。在此基础上，GE-Act通过一个轻量级的流匹配解码器将潜空间表示映射到可执行的动作轨迹，从而在最少的监督下实现对不同实体的精确且通用的策略推理。为了支持可扩展的评估和训练，GE-Sim充当条件动作神经模拟器，生成高保真的时间轴rollout，以促进闭环策略开发。该平台还配备了EWMBench，这是一个标准化基准套件，用于衡量视觉保真度、物理一致性和指令动作对齐。这些组件共同奠定了Genie Envisioner作为一个可扩展且实用的基于指令的通用体态智能基础平台的基础。所有代码、模型和基准测试都将公开发布。 

---
# Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling 

**Title (ZH)**: 通过游容性不确定性处理实现 crowdsNavigation 的普遍安全性 

**Authors**: Jianpeng Yao, Xiaopan Zhang, Yu Xia, Zejin Wang, Amit K. Roy-Chowdhury, Jiachen Li  

**Link**: [PDF](https://arxiv.org/pdf/2508.05634)  

**Abstract**: Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent's behavior through constrained reinforcement learning. The system helps regulate the agent's actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on this https URL. 

**Abstract (ZH)**: 基于适应性区间推断的鲁棒导航策略学习：面向crowds场景下的性能退化问题 

---
# CleanUpBench: Embodied Sweeping and Grasping Benchmark 

**Title (ZH)**: CleanUpBench:  embodied sweeping and grasping benchmark 

**Authors**: Wenbo Li, Guanting Chen, Tao Zhao, Jiyao Wang, Tianxin Hu, Yuwen Liao, Weixiang Guo, Shenghai Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2508.05543)  

**Abstract**: Embodied AI benchmarks have advanced navigation, manipulation, and reasoning, but most target complex humanoid agents or large-scale simulations that are far from real-world deployment. In contrast, mobile cleaning robots with dual mode capabilities, such as sweeping and grasping, are rapidly emerging as realistic and commercially viable platforms. However, no benchmark currently exists that systematically evaluates these agents in structured, multi-target cleaning tasks, revealing a critical gap between academic research and real-world applications. We introduce CleanUpBench, a reproducible and extensible benchmark for evaluating embodied agents in realistic indoor cleaning scenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service robot equipped with a sweeping mechanism and a six-degree-of-freedom robotic arm, enabling interaction with heterogeneous objects. The benchmark includes manually designed environments and one procedurally generated layout to assess generalization, along with a comprehensive evaluation suite covering task completion, spatial efficiency, motion quality, and control performance. To support comparative studies, we provide baseline agents based on heuristic strategies and map-based planning. CleanUpBench bridges the gap between low-level skill evaluation and full-scene testing, offering a scalable testbed for grounded, embodied intelligence in everyday settings. 

**Abstract (ZH)**: 基于身体计算的清洁机器人基准：CleanUpBench 

---
# Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation 

**Title (ZH)**: 人机协作操作的混合主动对话 

**Authors**: Albert Yu, Chengshu Li, Luca Macesanu, Arnav Balaji, Ruchira Ray, Raymond Mooney, Roberto Martín-Martín  

**Link**: [PDF](https://arxiv.org/pdf/2508.05535)  

**Abstract**: Effective robotic systems for long-horizon human-robot collaboration must adapt to a wide range of human partners, whose physical behavior, willingness to assist, and understanding of the robot's capabilities may change over time. This demands a tightly coupled communication loop that grants both agents the flexibility to propose, accept, or decline requests as they coordinate toward completing the task effectively. We apply a Mixed-Initiative dialog paradigm to Collaborative human-roBot teaming and propose MICoBot, a system that handles the common scenario where both agents, using natural language, take initiative in formulating, accepting, or rejecting proposals on who can best complete different steps of a task. To handle diverse, task-directed dialog, and find successful collaborative strategies that minimize human effort, MICoBot makes decisions at three levels: (1) a meta-planner considers human dialog to formulate and code a high-level collaboration strategy, (2) a planner optimally allocates the remaining steps to either agent based on the robot's capabilities (measured by a simulation-pretrained affordance model) and the human's estimated availability to help, and (3) an action executor decides the low-level actions to perform or words to say to the human. Our extensive evaluations in simulation and real-world -- on a physical robot with 18 unique human participants over 27 hours -- demonstrate the ability of our method to effectively collaborate with diverse human users, yielding significantly improved task success and user experience than a pure LLM baseline and other agent allocation models. See additional videos and materials at this https URL. 

**Abstract (ZH)**: 长时限人类-机器人协作的有效机器人系统必须适应广泛的人类合作伙伴，其物理行为、助人意愿以及对机器人能力的理解可能会随时间变化。这要求一个紧密耦合的通信循环，使两个代理能够在协调完成任务的过程中灵活地提出、接受或拒绝请求。我们应用混合主动对话范式于协作人机团队，并提出MICoBot系统，该系统处理两个代理在自然语言下共同发起、接受或拒绝关于谁最能完成任务不同步骤的提议的常见场景。为处理任务定向对话并找到能最小化人类努力的协作策略，MICoBot在三个层次上做出决定：（1）元规划器通过分析人类对话来制定和编码高层次的合作策略；（2）规划器基于机器人的能力（通过预训练的可用性模型测量）和人类帮助意愿的最佳分配剩余步骤给任一代理；（3）行为执行器决定执行的低层动作或对人类说的话。我们在模拟和现实世界中的广泛评估，包括在一个物理机器人上与18位独特的人类参与者合作27小时，证明了该方法在与多样化的人类用户协作方面的有效性，相比纯大语言模型基线和其他代理分配模型，显著提高了任务成功率和用户体验。更多视频和材料请参见此链接：this https URL。 

---
# DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model 

**Title (ZH)**: DistillDrive: 通过同构异源源规划模型实现端到端多模式自主驾驶精炼 

**Authors**: Rui Yu, Xianghang Zhang, Runkai Zhao, Huaicheng Yan, Meng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2508.05402)  

**Abstract**: End-to-end autonomous driving has been recently seen rapid development, exerting a profound influence on both industry and academia. However, the existing work places excessive focus on ego-vehicle status as their sole learning objectives and lacks of planning-oriented understanding, which limits the robustness of the overall decision-making prcocess. In this work, we introduce DistillDrive, an end-to-end knowledge distillation-based autonomous driving model that leverages diversified instance imitation to enhance multi-mode motion feature learning. Specifically, we employ a planning model based on structured scene representations as the teacher model, leveraging its diversified planning instances as multi-objective learning targets for the end-to-end model. Moreover, we incorporate reinforcement learning to enhance the optimization of state-to-decision mappings, while utilizing generative modeling to construct planning-oriented instances, fostering intricate interactions within the latent space. We validate our model on the nuScenes and NAVSIM datasets, achieving a 50\% reduction in collision rate and a 3-point improvement in closed-loop performance compared to the baseline model. Code and model are publicly available at this https URL 

**Abstract (ZH)**: 端到端自主驾驶近年来取得了 rapid 发展，对产业和学术界产生了深远影响。然而，现有工作过度关注ego-车辆状态作为唯一的学习目标，缺乏以规划为导向的理解，限制了整体决策过程的 robust 性。在此工作中，我们介绍了基于知识蒸馏的端到端自主驾驶模型 DistillDrive，通过多样化实例模仿增强多模式运动特征学习。具体而言，我们采用基于结构化场景表示的规划模型作为教师模型，利用其多样化的规划实例作为端到端模型的多目标学习目标。此外，我们结合强化学习以优化状态到决策的映射，并利用生成模型构建以规划为导向的实例，促进潜空间内的复杂交互。我们使用 nuScenes 和 NAVSIM 数据集验证了我们的模型，相比基线模型，碰撞率降低了 50%，闭环性能提升了 3 个点。代码和模型已公开，详见 this https URL。 

---
# Real-Time Iteration Scheme for Diffusion Policy 

**Title (ZH)**: 实时迭代方案for扩散策略 

**Authors**: Yufei Duan, Hang Yin, Danica Kragic  

**Link**: [PDF](https://arxiv.org/pdf/2508.05396)  

**Abstract**: Diffusion Policies have demonstrated impressive performance in robotic manipulation tasks. However, their long inference time, resulting from an extensive iterative denoising process, and the need to execute an action chunk before the next prediction to maintain consistent actions limit their applicability to latency-critical tasks or simple tasks with a short cycle time. While recent methods explored distillation or alternative policy structures to accelerate inference, these often demand additional training, which can be resource-intensive for large robotic models. In this paper, we introduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a method from optimal control that accelerates optimization by leveraging solutions from previous time steps as initial guesses for subsequent iterations. We explore the application of this scheme in diffusion inference and propose a scaling-based method to effectively handle discrete actions, such as grasping, in robotic manipulation. The proposed scheme significantly reduces runtime computational costs without the need for distillation or policy redesign. This enables a seamless integration into many pre-trained diffusion-based models, in particular, to resource-demanding large models. We also provide theoretical conditions for the contractivity which could be useful for estimating the initial denoising step. Quantitative results from extensive simulation experiments show a substantial reduction in inference time, with comparable overall performance compared with Diffusion Policy using full-step denoising. Our project page with additional resources is available at: this https URL. 

**Abstract (ZH)**: 实时迭代方案在机器人操作中加速扩散策略推理的研究 

---
# Robots can defuse high-intensity conflict situations 

**Title (ZH)**: 机器人可以化解高强度冲突情况 

**Authors**: Morten Roed Frederiksen, Kasper Støy  

**Link**: [PDF](https://arxiv.org/pdf/2508.05373)  

**Abstract**: This paper investigates the specific scenario of high-intensity confrontations between humans and robots, to understand how robots can defuse the conflict. It focuses on the effectiveness of using five different affective expression modalities as main drivers for defusing the conflict. The aim is to discover any strengths or weaknesses in using each modality to mitigate the hostility that people feel towards a poorly performing robot. The defusing of the situation is accomplished by making the robot better at acknowledging the conflict and by letting it express remorse. To facilitate the tests, we used a custom affective robot in a simulated conflict situation with 105 test participants. The results show that all tested expression modalities can successfully be used to defuse the situation and convey an acknowledgment of the confrontation. The ratings were remarkably similar, but the movement modality was different (ANON p$<$.05) than the other modalities. The test participants also had similar affective interpretations on how impacted the robot was of the confrontation across all expression modalities. This indicates that defusing a high-intensity interaction may not demand special attention to the expression abilities of the robot, but rather require attention to the abilities of being socially aware of the situation and reacting in accordance with it. 

**Abstract (ZH)**: 本文探讨了人类与机器人之间高强度对峙的具体场景，以理解机器人如何化解冲突。它专注于使用五种不同的情感表达模态作为主要驱动力来化解冲突的有效性。目的是发现使用每种模态来缓解人们对表现不佳的机器人的敌意时的强项和弱点。通过使机器人更好地识别冲突，并让它表达悔意，来化解局势。为了进行测试，我们使用了一个定制的情感机器人，在模拟的冲突情景中，共有105名测试参与者参与。结果表明，所有测试的情感表达模态都能成功地用于化解局势，并传达对冲突的认识。评分非常相似，但运动模态（ANON p<0.05）与其他模态不同。测试参与者对机器人在所有情感表达模态下对冲突的影响也有相似的情绪解释。这表明，化解高强度互动可能不需要特别关注机器人的表达能力，而是需要关注其对情境的社会意识能力和相应的反应能力。 

---
# Affecta-Context: The Context-Guided Behavior Adaptation Framework 

**Title (ZH)**: Affecta-Context：基于上下文的行为适应框架 

**Authors**: Morten Roed Frederiksen, Kasper Støy  

**Link**: [PDF](https://arxiv.org/pdf/2508.05359)  

**Abstract**: This paper presents Affecta-context, a general framework to facilitate behavior adaptation for social robots. The framework uses information about the physical context to guide its behaviors in human-robot interactions. It consists of two parts: one that represents encountered contexts and one that learns to prioritize between behaviors through human-robot interactions. As physical contexts are encountered the framework clusters them by their measured physical properties. In each context, the framework learns to prioritize between behaviors to optimize the physical attributes of the robot's behavior in line with its current environment and the preferences of the users it interacts with. This paper illlustrates the abilities of the Affecta-context framework by enabling a robot to autonomously learn the prioritization of discrete behaviors. This was achieved by training across 72 interactions in two different physical contexts with 6 different human test participants. The paper demonstrates the trained Affecta-context framework by verifying the robot's ability to generalize over the input and to match its behaviors to a previously unvisited physical context. 

**Abstract (ZH)**: Affecta-context：一种促进社会机器人行为适应的一般框架 

---
# Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control 

**Title (ZH)**: 基于视觉-语言-动作模型的信息论图融合及其在策略推理与双臂机器人控制中的应用 

**Authors**: Shunlei Li, Longsen Gao, Jin Wang, Chang Che, Xi Xiao, Jiuwen Cao, Yingbai Hu, Hamid Reza Karimi  

**Link**: [PDF](https://arxiv.org/pdf/2508.05342)  

**Abstract**: Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations. 

**Abstract (ZH)**: 从人类视频中教机器人灵巧技能依然具有挑战性，因为这高度依赖于低级轨迹模仿，无法在不同物体类型、空间布局和操作器配置之间进行泛化。我们提出了一种图融合视觉-语言-动作框架（GF-VLA），该框架使双臂机器人系统能够直接从RGB和深度人类演示中进行任务级推理和执行。GF-VLA 首先提取基于香农信息的线索以识别最具任务相关性的手和物体，然后将这些线索编码成描述手物和物体之间交互关系的时间顺序场景图。这些图与语言条件下的变换器融合，生成分层行为树和可解释的笛卡尔运动命令。为提高双臂操作中的执行效率，我们进一步引入了一种跨手选择策略，该策略在无需显式几何推理的情况下推断最佳夹爪分配。我们在四种涉及符号形状构建和空间泛化的双臂块组装任务上评估了GF-VLA。实验结果表明，信息论场景表示的图准确率超过95%，子任务分割准确率达到93%，支持大语言模型规划器生成可靠且易读的任务策略。当由双臂机器人执行时，这些策略在堆叠、字母构建和几何重构场景中实现了94%的抓取成功率、89%的放置准确率和90%的整体任务成功率，展示了其在不同空间和语义变体上的强大泛化能力和鲁棒性。 

---
# GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming 

**Title (ZH)**: GhostShell: 流式调用LLM函数实现并发具身编程 

**Authors**: Jian Gong, Youwei Huang, Bo Yuan, Ming Zhu, Juncheng Zhan, Jinke Wang, Hang Shu, Mingyue Xiong, Yanjun Ye, Yufan Zu, Yang Zhou, Yihan Ding, Xuannian Chen, Xingyu Lu, Runjie Ban, Bingchao Huang, Fusen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2508.05298)  

**Abstract**: We present GhostShell, a novel approach that leverages Large Language Models (LLMs) to enable streaming and concurrent behavioral programming for embodied systems. In contrast to conventional methods that rely on pre-scheduled action sequences or behavior trees, GhostShell drives embodied systems to act on-the-fly by issuing function calls incrementally as tokens are streamed from the LLM. GhostShell features a streaming XML function token parser, a dynamic function interface mapper, and a multi-channel scheduler that orchestrates intra-channel synchronous and inter-channel asynchronous function calls, thereby coordinating serial-parallel embodied actions across multiple robotic components as directed by the LLM. We evaluate GhostShell on our robot prototype COCO through comprehensive grounded experiments across 34 real-world interaction tasks and multiple LLMs. The results demonstrate that our approach achieves state-of-the-art Behavioral Correctness Metric of 0.85 with Claude-4 Sonnet and up to 66X faster response times compared to LLM native function calling APIs. GhostShell also proves effective in long-horizon multimodal tasks, demonstrating strong robustness and generalization. 

**Abstract (ZH)**: GhostShell：一种利用大规模语言模型实现类人系统流式并发行为编程的新方法 

---
# Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction 

**Title (ZH)**: 面向具身自主人工智能：由LLM和VLM驱动的机器人自主性和交互性的综述与分类 

**Authors**: Sahar Salimpour, Lei Fu, Farhad Keramat, Leonardo Militano, Giovanni Toffetti, Harry Edelman, Jorge Peña Queralta  

**Link**: [PDF](https://arxiv.org/pdf/2508.05294)  

**Abstract**: Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (BLMs) are increasing the dexterity and capabilities of robotic systems. This survey paper focuses on those words advancing towards agentic applications and architectures. This includes initial efforts exploring GPT-style interfaces to tooling, as well as more complex system where AI agents are coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature. 

**Abstract (ZH)**: 以下是论文内容或 标题的翻译， 中文，
user
基于基础的方法和大规模语言模型（FGM和视觉-语言模型 VLM), 近来使 Recent允许了对 机器人自主性和人-机接口方面的创新方法。与此平行地 分层视觉-语言-行动模型（VLAM）和和大型行为模型（BLM) 也正在提高机器人的灵巧性和能力。。

本文旨在探讨自主自主代理的应用和架构。这包括探索类 GPT 掏口到 以及更复杂的代理，，，代理的角色包括协调者、计划者、感知工作者等 和通用接口。这样的类代理架构使机器人能够根据取自然语言指令、调用API、规划任务序列、以及在操作和诊断中得以发挥作用。除了查阅经过同行评审的研究 G，考虑到该 G 星期社区驱动的项目、ROS 包 G 和 和工业框架 G，，新兴趋势。我们提出 G一种分类法来 为不同的整合方法分类，并 �并以及对对分析在当前的架构中代理的角色。 

---
# Dancing with a Robot: An Experimental Study of Child-Robot Interaction in a Performative Art Setting 

**Title (ZH)**: 与机器人共舞：一种表演艺术环境中的儿童与机器人互动实验研究 

**Authors**: Victor Ngo, Rachel, Ramchurn, Roma Patel, Alan Chamberlain, Ayse Kucukyilmaz  

**Link**: [PDF](https://arxiv.org/pdf/2508.05208)  

**Abstract**: This paper presents an evaluation of 18 children's in-the-wild experiences with the autonomous robot arm performer NED (Never-Ending Dancer) within the Thingamabobas installation, showcased across the UK. We detail NED's design, including costume, behaviour, and human interactions, all integral to the installation. Our observational analysis revealed three key challenges in child-robot interactions: 1) Initiating and maintaining engagement, 2) Lack of robot expressivity and reciprocity, and 3) Unmet expectations. Our findings show that children are naturally curious, and adept at interacting with a robotic art performer. However, our observations emphasise the critical need to optimise human-robot interaction (HRI) systems through careful consideration of audience's capabilities, perceptions, and expectations, within the performative arts context, to enable engaging and meaningful experiences, especially for young audiences. 

**Abstract (ZH)**: 本文评估了18名儿童与自主机器人手臂表演者NED（永不结束的舞者）在Thingamabobas装置中的户外体验，该装置在英国各地展出。我们详细介绍了NED的设计，包括其服装、行为和与人类的互动，这些都是装置的重要组成部分。我们的观察性分析揭示了儿童与机器人交互中的三个关键挑战：1）吸引和维持注意力，2）缺乏机器人表达性和互应性，3）期望未被满足。研究发现，儿童天生好奇，并擅长与机器人艺术表演者互动。然而，我们的观察强调，在表演艺术领域内，为了实现引人入胜且有意义的体验，特别是在面向年轻观众时，必须通过仔细考虑观众的能力、感知和期望来优化人机交互系统。 

---
# Learning to See and Act: Task-Aware View Planning for Robotic Manipulation 

**Title (ZH)**: 学会看与做：面向任务的视图规划在机器人操作中的应用 

**Authors**: Yongjie Bai, Zhouxia Wang, Yang Liu, Weixing Chen, Ziliang Chen, Mingtong Dai, Yongsen Zheng, Lingbo Liu, Guanbin Li, Liang Lin  

**Link**: [PDF](https://arxiv.org/pdf/2508.05186)  

**Abstract**: Recent vision-language-action (VLA) models for multi-task robotic manipulation commonly rely on static viewpoints and shared visual encoders, which limit 3D perception and cause task interference, hindering robustness and generalization. In this work, we propose Task-Aware View Planning (TAVP), a framework designed to overcome these challenges by integrating active view planning with task-specific representation learning. TAVP employs an efficient exploration policy, accelerated by a novel pseudo-environment, to actively acquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE) visual encoder to disentangle features across different tasks, boosting both representation fidelity and task generalization. By learning to see the world in a task-aware way, TAVP generates more complete and discriminative visual representations, demonstrating significantly enhanced action prediction across a wide array of manipulation challenges. Extensive experiments on RLBench tasks show that our proposed TAVP model achieves superior performance over state-of-the-art fixed-view approaches. Visual results and code are provided at: this https URL. 

**Abstract (ZH)**: 基于任务的视图规划（TAVP）：一种结合任务特定表示学习的主动视图规划框架 

---
# FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction 

**Title (ZH)**: FCBV-Net: 基于特征条件的双臂价值预测在类别级别机器人服装平滑中的应用 

**Authors**: Mohammed Daba, Jing Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2508.05153)  

**Abstract**: Category-level generalization for robotic garment manipulation, such as bimanual smoothing, remains a significant hurdle due to high dimensionality, complex dynamics, and intra-category variations. Current approaches often struggle, either overfitting with concurrently learned visual features for a specific instance or, despite category-level perceptual generalization, failing to predict the value of synergistic bimanual actions. We propose the Feature-Conditioned Bimanual Value Network (FCBV-Net), operating on 3D point clouds to specifically enhance category-level policy generalization for garment smoothing. FCBV-Net conditions bimanual action value prediction on pre-trained, frozen dense geometric features, ensuring robustness to intra-category garment variations. Trainable downstream components then learn a task-specific policy using these static features. In simulated GarmentLab experiments with the CLOTH3D dataset, FCBV-Net demonstrated superior category-level generalization. It exhibited only an 11.5% efficiency drop (Steps80) on unseen garments compared to 96.2% for a 2D image-based baseline, and achieved 89% final coverage, outperforming an 83% coverage from a 3D correspondence-based baseline that uses identical per-point geometric features but a fixed primitive. These results highlight that the decoupling of geometric understanding from bimanual action value learning enables better category-level generalization. 

**Abstract (ZH)**: 基于特征条件的双边价值网络（FCBV-Net）：用于服装整理的类别级政策泛化 

---
# Chemist Eye: A Visual Language Model-Powered System for Safety Monitoring and Robot Decision-Making in Self-Driving Laboratories 

**Title (ZH)**: 化学家之眼：一种基于视觉语言模型的安全监控与机器人决策系统在自主实验室中的应用 

**Authors**: Francisco Munguia-Galeano, Zhengxue Zhou, Satheeshkumar Veeramani, Hatem Fakhruldeen, Louis Longley, Rob Clowes, Andrew I. Cooper  

**Link**: [PDF](https://arxiv.org/pdf/2508.05148)  

**Abstract**: The integration of robotics and automation into self-driving laboratories (SDLs) can introduce additional safety complexities, in addition to those that already apply to conventional research laboratories. Personal protective equipment (PPE) is an essential requirement for ensuring the safety and well-being of workers in laboratories, self-driving or otherwise. Fires are another important risk factor in chemical laboratories. In SDLs, fires that occur close to mobile robots, which use flammable lithium batteries, could have increased severity. Here, we present Chemist Eye, a distributed safety monitoring system designed to enhance situational awareness in SDLs. The system integrates multiple stations equipped with RGB, depth, and infrared cameras, designed to monitor incidents in SDLs. Chemist Eye is also designed to spot workers who have suffered a potential accident or medical emergency, PPE compliance and fire hazards. To do this, Chemist Eye uses decision-making driven by a vision-language model (VLM). Chemist Eye is designed for seamless integration, enabling real-time communication with robots. Based on the VLM recommendations, the system attempts to drive mobile robots away from potential fire locations, exits, or individuals not wearing PPE, and issues audible warnings where necessary. It also integrates with third-party messaging platforms to provide instant notifications to lab personnel. We tested Chemist Eye with real-world data from an SDL equipped with three mobile robots and found that the spotting of possible safety hazards and decision-making performances reached 97 % and 95 %, respectively. 

**Abstract (ZH)**: 机器人与自动化集成到自主驾驶实验室中可以引入额外的安全复杂性，这在传统的研究实验室中已经存在。个人防护装备是确保实验室工作者安全与健康的重要要求，在自主驾驶或传统的化学实验室中都是如此。火灾是化学实验室中的另一个重要风险因素。在自主驾驶实验室（SDLs）中，发生在使用易燃锂离子电池的移动机器人附近的火灾可能会更加严重。我们介绍了Chemist Eye，这是一个分布式安全监控系统，旨在增强SDLs的安全态势感知能力。该系统集成了多个配备RGB、深度和红外摄像头的站点，用于监控SDL中的事故。Chemist Eye还设计用于识别可能遭遇事故或医疗紧急情况的工作者，以及监测个人防护装备合规性和火灾隐患。Chemist Eye通过由视觉-语言模型（VLM）驱动的决策来实现这一点。Chemist Eye设计为无缝集成，能够与机器人实现实时通信。根据VLM的建议，系统尝试驱离移动机器人远离潜在火灾地点、出口或未佩戴个人防护装备的个体，并在必要时发出声音警告。此外，该系统还与第三方消息平台集成，提供即时通知给实验室人员。我们使用配备了三台移动机器人的实际SDL数据对Chemist Eye进行了测试，结果显示可能的安全隐患识别和决策性能分别达到了97%和95%。 

---
# Examining the legibility of humanoid robot arm movements in a pointing task 

**Title (ZH)**: 考察类人机器人手臂在指认任务中运动的可读性 

**Authors**: Andrej Lúčny, Matilde Antonj, Carlo Mazzola, Hana Hornáčková, Ana Farić, Kristína Malinovská, Michal Vavrecka, Igor Farkaš  

**Link**: [PDF](https://arxiv.org/pdf/2508.05104)  

**Abstract**: Human--robot interaction requires robots whose actions are legible, allowing humans to interpret, predict, and feel safe around them. This study investigates the legibility of humanoid robot arm movements in a pointing task, aiming to understand how humans predict robot intentions from truncated movements and bodily cues. We designed an experiment using the NICO humanoid robot, where participants observed its arm movements towards targets on a touchscreen. Robot cues varied across conditions: gaze, pointing, and pointing with congruent or incongruent gaze. Arm trajectories were stopped at 60\% or 80\% of their full length, and participants predicted the final target. We tested the multimodal superiority and ocular primacy hypotheses, both of which were supported by the experiment. 

**Abstract (ZH)**: 人人的机器人交互需要机器人具有可可译 Swinger人的机器人在移动时动作时 时动作具有可耐读性忽略谷爱凌人类机器人手臂动作时的实验，旨在理解人类如何从截断的动作和身体线索预测机器人的意图。实验中使用了谷爱凌phalt类机器人，受试者观察手臂动作并向触屏目标移动。机器人的动作条件包括：目光指向、手指向、目光一致/不一致的指向。手臂轨迹分为六个部分，受试者预测机器人的意图。我们测试了多模态优越性和眼动优先性假说。 

---
# MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding 

**Title (ZH)**: MAG-Nav：基于语言的物体导航利用记忆预留活跃接地 

**Authors**: Weifan Zhang, Tingguang Li, Yuzhen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2508.05021)  

**Abstract**: Visual navigation in unknown environments based solely on natural language descriptions is a key capability for intelligent robots. In this work, we propose a navigation framework built upon off-the-shelf Visual Language Models (VLMs), enhanced with two human-inspired mechanisms: perspective-based active grounding, which dynamically adjusts the robot's viewpoint for improved visual inspection, and historical memory backtracking, which enables the system to retain and re-evaluate uncertain observations over time. Unlike existing approaches that passively rely on incidental visual inputs, our method actively optimizes perception and leverages memory to resolve ambiguity, significantly improving vision-language grounding in complex, unseen environments. Our framework operates in a zero-shot manner, achieving strong generalization to diverse and open-ended language descriptions without requiring labeled data or model fine-tuning. Experimental results on Habitat-Matterport 3D (HM3D) show that our method outperforms state-of-the-art approaches in language-driven object navigation. We further demonstrate its practicality through real-world deployment on a quadruped robot, achieving robust and effective navigation performance. 

**Abstract (ZH)**: 基于自然语言描述在未知环境中进行视觉导航是智能机器人的一项关键能力。本工作中，我们提出了一种构建在即用型视觉语言模型（VLMs）基础上的导航框架，并结合了两种受人类启发的机制：基于视角的主动对接接地，可动态调整机器人视角以提高视觉检查效果；历史记忆回溯机制，使系统能够保留并重新评估随时间变化的不确定性观察结果。与现有依赖于偶然视觉输入的方法不同，我们的方法主动优化感知并利用记忆来解决歧义性问题，在复杂且未见过的环境中显著提高了视觉-语言对接接地效果。该框架以零样本方式运行，无需标注数据或模型微调即可实现对多样化和开放性语言描述的强大泛化能力。实验结果表明，我们的方法在Habitat-Matterport 3D（HM3D）上优于最先进的基于语言驱动的物体导航方法。进一步的实地部署在四足机器人上展示了其实用性，实现了稳健且有效的导航性能。 

---
# Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots 

**Title (ZH)**: 基于层次化深度确定性策略 gradients 的自主迷宫导航移动机器人算法 

**Authors**: Wenjie Hu, Ye Zhou, Hann Woei Ho  

**Link**: [PDF](https://arxiv.org/pdf/2508.04994)  

**Abstract**: Maze navigation is a fundamental challenge in robotics, requiring agents to traverse complex environments efficiently. While the Deep Deterministic Policy Gradient (DDPG) algorithm excels in control tasks, its performance in maze navigation suffers from sparse rewards, inefficient exploration, and long-horizon planning difficulties, often leading to low success rates and average rewards, sometimes even failing to achieve effective navigation. To address these limitations, this paper proposes an efficient Hierarchical DDPG (HDDPG) algorithm, which includes high-level and low-level policies. The high-level policy employs an advanced DDPG framework to generate intermediate subgoals from a long-term perspective and on a higher temporal scale. The low-level policy, also powered by the improved DDPG algorithm, generates primitive actions by observing current states and following the subgoal assigned by the high-level policy. The proposed method enhances stability with off-policy correction, refining subgoal assignments by relabeling historical experiences. Additionally, adaptive parameter space noise is utilized to improve exploration, and a reshaped intrinsic-extrinsic reward function is employed to boost learning efficiency. Further optimizations, including gradient clipping and Xavier initialization, are employed to improve robustness. The proposed algorithm is rigorously evaluated through numerical simulation experiments executed using the Robot Operating System (ROS) and Gazebo. Regarding the three distinct final targets in autonomous maze navigation tasks, HDDPG significantly overcomes the limitations of standard DDPG and its variants, improving the success rate by at least 56.59% and boosting the average reward by a minimum of 519.03 compared to baseline algorithms. 

**Abstract (ZH)**: 迷宫导航是机器人研究中的一个基本挑战，要求智能体高效地穿越复杂环境。尽管深度确定性策略梯度（DDPG）算法在控制任务中表现出色，但在迷宫导航任务中，由于稀疏奖励、探索效率低以及长时依赖规划困难，其性能往往较差，导致成功率低、平均奖励低，甚至无法有效导航。为了解决这些问题，本文提出了一种高效的层次化DDPG（HDDPG）算法，该算法包括高一级和低一级的策略。高一级策略采用先进的DDPG框架，从长期视角和更高的时间尺度生成中间子目标。低一级策略由改进的DDPG算法驱动，通过观察当前状态并遵循高一级策略分配的子目标，生成原始动作。所提出的方法通过离策策略校正增强稳定性，通过重新标记历史经验细化子目标分配。此外，利用自适应参数空间噪声提高探索效率，并采用重塑的内在-外在奖励函数增强学习效率。进一步优化包括梯度裁剪和Xavier初始化，提高鲁棒性。通过使用Robot Operating System (ROS)和Gazebo执行的数值仿真实验，对自主迷宫导航任务中的三个不同最终目标进行了严格评估。结果表明，HDDPG显著克服了标准DDPG及其变体的局限性，与基准算法相比，成功率为至少56.59%的提升，平均奖励提高了至少519.03。 

---
# INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM 

**Title (ZH)**: 意图：通过交互直觉和接地多模态语言模型推断类人机器人运动趋势 

**Authors**: Jin Wang, Weijie Wang, Boyuan Deng, Heng Zhang, Rui Dai, Nikos Tsagarakis  

**Link**: [PDF](https://arxiv.org/pdf/2508.04931)  

**Abstract**: Traditional control and planning for robotic manipulation heavily rely on precise physical models and predefined action sequences. While effective in structured environments, such approaches often fail in real-world scenarios due to modeling inaccuracies and struggle to generalize to novel tasks. In contrast, humans intuitively interact with their surroundings, demonstrating remarkable adaptability, making efficient decisions through implicit physical understanding. In this work, we propose INTENTION, a novel framework enabling robots with learned interactive intuition and autonomous manipulation in diverse scenarios, by integrating Vision-Language Models (VLMs) based scene reasoning with interaction-driven memory. We introduce Memory Graph to record scenes from previous task interactions which embodies human-like understanding and decision-making about different tasks in real world. Meanwhile, we design an Intuitive Perceptor that extracts physical relations and affordances from visual scenes. Together, these components empower robots to infer appropriate interaction behaviors in new scenes without relying on repetitive instructions. Videos: this https URL 

**Abstract (ZH)**: 基于视觉语言模型的交互直觉与自主 manipulation 研究：INTENTION框架 

---
# On the causality between affective impact and coordinated human-robot reactions 

**Title (ZH)**: 情绪影响与协调的人机反应之间的影响因果关系 

**Authors**: Morten Roed Frederiksen, Kasper Støy  

**Link**: [PDF](https://arxiv.org/pdf/2508.04834)  

**Abstract**: In an effort to improve how robots function in social contexts, this paper investigates if a robot that actively shares a reaction to an event with a human alters how the human perceives the robot's affective impact. To verify this, we created two different test setups. One to highlight and isolate the reaction element of affective robot expressions, and one to investigate the effects of applying specific timing delays to a robot reacting to a physical encounter with a human. The first test was conducted with two different groups (n=84) of human observers, a test group and a control group both interacting with the robot. The second test was performed with 110 participants using increasingly longer reaction delays for the robot with every ten participants. The results show a statistically significant change (p$<$.05) in perceived affective impact for the robots when they react to an event shared with a human observer rather than reacting at random. The result also shows for shared physical interaction, the near-human reaction times from the robot are most appropriate for the scenario. The paper concludes that a delay time around 200ms may render the biggest impact on human observers for small-sized non-humanoid robots. It further concludes that a slightly shorter reaction time around 100ms is most effective when the goal is to make the human observers feel they made the biggest impact on the robot. 

**Abstract (ZH)**: 为了提高机器人在社会情境中的功能，本文探讨了机器人在与人类共享事件反应时，是否改变人类对机器人情感影响的感知。为此，我们创建了两种不同的实验设置。一种旨在突出和分离情感机器人表达中的反应元素，另一种则研究机器人在与人类物理互动后应用特定时间延迟效果。第一个实验在两个不同的人类观察者组（n=84）中进行，其中包括测试组和对照组，他们都与机器人互动。第二个实验使用了110名参与者，每隔十人设置一次逐渐延长的反应延迟。结果显示，当机器人在与人类观察者共享事件后再进行反应时，其感知的情感影响发生了统计学上的显著变化（p<0.05）。此外，当机器人与人类进行物理互动时，接近人类的反应时间是最合适的。本文得出结论，对于小型非人形机器人，约200ms的延迟时间可能会给人类观察者留下最大影响；并且，在目标是让人类观察者感觉他们对机器人产生了最大影响时，略短约100ms的反应时间更为有效。 

---
# The Missing Reward: Active Inference in the Era of Experience 

**Title (ZH)**: 缺失的奖励：体验时代的学习推断 

**Authors**: Bo Wen  

**Link**: [PDF](https://arxiv.org/pdf/2508.05619)  

**Abstract**: This paper argues that Active Inference (AIF) provides a crucial foundation for developing autonomous AI agents capable of learning from experience without continuous human reward engineering. As AI systems begin to exhaust high-quality training data and rely on increasingly large human workforces for reward design, the current paradigm faces significant scalability challenges that could impede progress toward genuinely autonomous intelligence. The proposal for an ``Era of Experience,'' where agents learn from self-generated data, is a promising step forward. However, this vision still depends on extensive human engineering of reward functions, effectively shifting the bottleneck from data curation to reward curation. This highlights what we identify as the \textbf{grounded-agency gap}: the inability of contemporary AI systems to autonomously formulate, adapt, and pursue objectives in response to changing circumstances. We propose that AIF can bridge this gap by replacing external reward signals with an intrinsic drive to minimize free energy, allowing agents to naturally balance exploration and exploitation through a unified Bayesian objective. By integrating Large Language Models as generative world models with AIF's principled decision-making framework, we can create agents that learn efficiently from experience while remaining aligned with human values. This synthesis offers a compelling path toward AI systems that can develop autonomously while adhering to both computational and physical constraints. 

**Abstract (ZH)**: AIF为自主AI agents从经验中学习提供关键基础：从数据密集型时代过渡到经验时代 

---
# DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning 

**Title (ZH)**: DeepPHY: 评估自主型大模型在物理推理任务上的性能 

**Authors**: Xinrun Xu, Pi Bu, Ye Wang, Börje F. Karlsson, Ziming Wang, Tengtao Song, Qi Zhu, Jun Song, Zhiming Ding, Bo Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2508.05405)  

**Abstract**: Although Vision Language Models (VLMs) exhibit strong perceptual abilities and impressive visual reasoning, they struggle with attention to detail and precise action planning in complex, dynamic environments, leading to subpar performance. Real-world tasks typically require complex interactions, advanced spatial reasoning, long-term planning, and continuous strategy refinement, usually necessitating understanding the physics rules of the target scenario. However, evaluating these capabilities in real-world scenarios is often prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel benchmark framework designed to systematically evaluate VLMs' understanding and reasoning about fundamental physical principles through a series of challenging simulated environments. DeepPHY integrates multiple physical reasoning environments of varying difficulty levels and incorporates fine-grained evaluation metrics. Our evaluation finds that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise, predictive control. 

**Abstract (ZH)**: 尽管视觉语言模型(VLMs)在感知能力和视觉推理方面表现出色，但在复杂动态环境中，它们在细节关注和精确动作规划方面存在不足，导致性能不佳。真实世界任务通常需要复杂的交互、高级的空间推理、长期规划以及持续的策略优化，通常需要理解目标场景的物理规则。然而，在真实世界场景中评估这些能力往往代价高昂。为了弥补这一差距，我们引入了DeepPHY，一个新型基准框架，旨在通过一系列具有挑战性的模拟环境系统性地评估VLMs关于基本物理原理的了解和推理能力。DeepPHY集成了多个不同难度级别的物理推理环境，并采用了精细的评估指标。我们的评估发现，即使是最先进的VLMs，在将描述性的物理知识转化为精确、预测性的控制方面也存在困难。 

---
# Minimal Model Reasoning in Description Logics: Don't Try This at Home! 

**Title (ZH)**: 描述逻辑中的最小模型推理：不要在家里尝试！ 

**Authors**: Federica Di Stefano, Quentin Manière, Magdalena Ortiz, Mantas Šimkus  

**Link**: [PDF](https://arxiv.org/pdf/2508.05350)  

**Abstract**: Reasoning with minimal models has always been at the core of many knowledge representation techniques, but we still have only a limited understanding of this problem in Description Logics (DLs). Minimization of some selected predicates, letting the remaining predicates vary or be fixed, as proposed in circumscription, has been explored and exhibits high complexity. The case of `pure' minimal models, where the extension of all predicates must be minimal, has remained largely uncharted. We address this problem in popular DLs and obtain surprisingly negative results: concept satisfiability in minimal models is undecidable already for $\mathcal{EL}$. This undecidability also extends to a very restricted fragment of tuple-generating dependencies. To regain decidability, we impose acyclicity conditions on the TBox that bring the worst-case complexity below double exponential time and allow us to establish a connection with the recently studied pointwise circumscription; we also derive results in data complexity. We conclude with a brief excursion to the DL-Lite family, where a positive result was known for DL-Lite$_{\text{core}}$, but our investigation establishes ExpSpace-hardness already for its extension DL-Lite$_{\text{horn}}$. 

**Abstract (ZH)**: 在描述逻辑中最小模型的推理一直是众多知识表示技术的核心，但我们在描述逻辑中的这一问题上仍只有有限的理解。纯最小模型的情况，即所有谓词的扩展都必须最小，仍 largely uncharted。我们针对流行的描述逻辑处理这一问题并得到了令人意外的负面结果：概念在最小模型中的可满足性对于 $\mathcal{EL}$ 来说是不可判定的。这一不可判定性也扩展到了元组生成依赖的一个非常受限的片段。为了重新获得可判定性，我们对TBox施加了无环条件，将最坏情况的复杂性降低到双指数时间，并允许我们建立与最近研究的点态约简之间的联系；我们也在数据复杂性方面得到了一些结果。最后，我们简要介绍了DL-Lite家族，对于DL-Lite$_{\text{core}}$ 已知有一个积极的结果，但我们的研究却确定其扩展DL-Lite$_{\text{horn}}$ 的复杂性为ExpSpace-hard。 

---
# Cognitive Duality for Adaptive Web Agents 

**Title (ZH)**: 适应性网络代理的认知二元性 

**Authors**: Jiarun Liu, Chunhong Zhang, Zheng Hu  

**Link**: [PDF](https://arxiv.org/pdf/2508.05081)  

**Abstract**: Web navigation represents a critical and challenging domain for evaluating artificial general intelligence (AGI), demanding complex decision-making within high-entropy, dynamic environments with combinatorially explosive action spaces. Current approaches to building autonomous web agents either focus on offline imitation learning or online exploration, but rarely integrate both paradigms effectively. Inspired by the dual-process theory of human cognition, we derive a principled decomposition into fast System 1 and slow System 2 cognitive processes. This decomposition provides a unifying perspective on existing web agent methodologies, bridging the gap between offline learning of intuitive reactive behaviors and online acquisition of deliberative planning capabilities. We implement this framework in CogniWeb, a modular agent architecture that adaptively toggles between fast intuitive processing and deliberate reasoning based on task complexity. Our evaluation on WebArena demonstrates that CogniWeb achieves competitive performance (43.96% success rate) while maintaining significantly higher efficiency (75% reduction in token usage). 

**Abstract (ZH)**: Web导航是评估人工通用智能(AGI)的关键性和挑战性领域，要求在高熵、动态环境中进行复杂的决策，在具有组合爆炸性动作空间的环境中提出挑战。当前构建自主网页代理的方法要么集中于离线模仿学习，要么专注于在线探索，但很少能有效结合这两种范式。受到人类认知的双过程理论启发，我们提出了一种原理性的分解，区分快速的System 1和缓慢的System 2认知过程。这种分解为现有的网页代理方法提供了一个统一的视角，弥合了离线学习直观反应行为与在线获取深思熟虑规划能力之间的差距。我们通过CogniWeb模块化代理架构实现了这一框架，该架构根据任务复杂性在快速直观处理与深思熟虑推理之间 adaptable地切换。我们在WebArena上的评估表明，CogniWeb在保持显著更高的效率（75%的令牌使用量减少）的同时，实现了竞争力的性能（成功率43.96%）。 

---
# Test-Time Reinforcement Learning for GUI Grounding via Region Consistency 

**Title (ZH)**: 基于区域一致性的测试时强化学习GUI定位 

**Authors**: Yong Du, Yuchen Yan, Fei Tang, Zhengxi Lu, Chang Zong, Weiming Lu, Shengpei Jiang, Yongliang Shen  

**Link**: [PDF](https://arxiv.org/pdf/2508.05615)  

**Abstract**: Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), which transforms these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: GUI-RC boosts Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO further improves it to 85.14% through self-supervised optimization. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more robust and data-efficient GUI agents. 

**Abstract (ZH)**: 图形用户界面（GUI）grounding：从自然语言指令到精确屏幕坐标映射的任务是自主GUI代理的基础。我们观察到，当模型对同一GUI元素生成多个预测时，空间重叠模式揭示了隐含的信心信号，这些信号可以指导更精确的位置定位。基于这一洞察，我们提出了GUI-RC（区域一致性）方法，该方法在测试时构建由多个采样预测生成的空间投票网格，以识别模型一致性最高的共识区域。GUI-RC 在各种架构上提高了 ScreenSpot 基准上的准确性，提升了 2-3%。我们进一步引入了 GUI-RCPO（区域一致性策略优化），将其一致模式转换为测试时强化学习的奖励。通过计算每个预测与集体共识的吻合程度，GUI-RCPO 允许模型在推断过程中逐步细化其在未标记数据上的输出。广泛的实验表明，我们的方法具有普遍适用性：GUI-RC 将 Qwen2.5-VL-3B-Instruct 在 ScreenSpot-v2 上的表现从 80.11% 提高到 83.57%，而 GUI-RCPO 通过自我监督优化进一步提高到 85.14%。我们的方法揭示了测试时放大和测试时强化学习在GUI定位方面的潜在价值，为更稳健和数据高效的GUI代理提供了充满希望的途径。 

---
# OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks 

**Title (ZH)**: OmniEAR: 人体感知任务中代理推理的基准测试 

**Authors**: Zixuan Wang, Dingming Li, Hongxing Li, Shuo Chen, Yuchen Yan, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang  

**Link**: [PDF](https://arxiv.org/pdf/2508.05614)  

**Abstract**: Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance. 

**Abstract (ZH)**: 大型语言模型在抽象推理方面表现出色，但在 embodied 代理推理能力上尚未得到充分探索。我们提出了 OmniEAR，一个全面的框架，用于评估语言模型在体表现务中关于物理交互、工具使用和多代理协调的推理能力。与现有基准中提供预定义工具集或明确协作指令不同，OmniEAR 要求代理动态获取能力并在任务需求基础上自主确定协调策略。通过基于文本的环境表示，我们建模了 1,500 种场景中连续的物理属性和复杂的空间关系，涵盖了 household 和工业领域。我们的系统性评估揭示了在处理约束时性能严重退化：在明确指令下成功率达 85-96%，但在工具推理中降至 56-85%，在隐式协作中降至 63-85%，而复合作业的失败率超过 50%。令人惊讶的是，完整环境信息反而降低了协调性能，表明模型无法过滤出与任务相关的信息。微调显著提高了单代理任务的表现（0.6% 至 76.3%），但在多代理方面的增益有限（1.5% 至 5.5%），暴露了基础架构的局限性。这些发现表明，体表现务推理提出了当前模型无法解决的根本性挑战，并确立了 OmniEAR 作为评估和推进体表现务 AI 系统的严格基准。我们的代码和数据包含在附录材料中，并将在接受后开源。 

---
# ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking 

**Title (ZH)**: ReasoningTrack：长时视觉-语言跟踪的链式推理方法 

**Authors**: Xiao Wang, Liye Jin, Xufeng Lou, Shiao Wang, Lan Chen, Bo Jiang, Zhipeng Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2508.05221)  

**Abstract**: Vision-language tracking has received increasing attention in recent years, as textual information can effectively address the inflexibility and inaccuracy associated with specifying the target object to be tracked. Existing works either directly fuse the fixed language with vision features or simply modify using attention, however, their performance is still limited. Recently, some researchers have explored using text generation to adapt to the variations in the target during tracking, however, these works fail to provide insights into the model's reasoning process and do not fully leverage the advantages of large models, which further limits their overall performance. To address the aforementioned issues, this paper proposes a novel reasoning-based vision-language tracking framework, named ReasoningTrack, based on a pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning) and reinforcement learning GRPO are used for the optimization of reasoning and language generation. We embed the updated language descriptions and feed them into a unified tracking backbone network together with vision features. Then, we adopt a tracking head to predict the specific location of the target object. In addition, we propose a large-scale long-term vision-language tracking benchmark dataset, termed TNLLT, which contains 200 video sequences. 20 baseline visual trackers are re-trained and evaluated on this dataset, which builds a solid foundation for the vision-language visual tracking task. Extensive experiments on multiple vision-language tracking benchmark datasets fully validated the effectiveness of our proposed reasoning-based natural language generation strategy. The source code of this paper will be released on this https URL 

**Abstract (ZH)**: 基于视觉语言推理的目标跟踪框架ReasoningTrack 

---
# Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation 

**Title (ZH)**: 基于领域驱动的强化学习度量标准：基于代理仿真在流行病控制中的案例研究 

**Authors**: Rishabh Gaur, Gaurav Deshkar, Jayanta Kshirsagar, Harshal Hayatnagarkar, Janani Venugopalan  

**Link**: [PDF](https://arxiv.org/pdf/2508.05154)  

**Abstract**: For the development and optimization of agent-based models (ABMs) and rational agent-based models (RABMs), optimization algorithms such as reinforcement learning are extensively used. However, assessing the performance of RL-based ABMs and RABMS models is challenging due to the complexity and stochasticity of the modeled systems, and the lack of well-standardized metrics for comparing RL algorithms. In this study, we are developing domain-driven metrics for RL, while building on state-of-the-art metrics. We demonstrate our ``Domain-driven-RL-metrics'' using policy optimization on a rational ABM disease modeling case study to model masking behavior, vaccination, and lockdown in a pandemic. Our results show the use of domain-driven rewards in conjunction with traditional and state-of-the-art metrics for a few different simulation scenarios such as the differential availability of masks. 

**Abstract (ZH)**: 基于领域驱动的强化学习评估指标在代理基于模型及其理性扩展模型开发与优化中的应用：以传染病模型为例 

---
# Agency, Affordances, and Enculturation of Augmentation Technologies 

**Title (ZH)**: 代理、可能性与增益技术的文化内化 

**Authors**: Ann Hill Duin, Isabel Pedersen  

**Link**: [PDF](https://arxiv.org/pdf/2508.04725)  

**Abstract**: Augmentation technologies are undergoing a process of enculturation due to many factors, one being the rise of artificial intelligence (AI), or what the World Intellectual Property Organization (WIPO) terms the AI wave or AI boom. Chapter 3 focuses critical attention on the hyped assumption that sophisticated, emergent, and embodied augmentation technologies will improve lives, literacy, cultures, arts, economies, and social contexts. The chapter begins by discussing the problem of ambiguity with AI terminology, which it aids with a description of the WIPO Categorization of AI Technologies Scheme. It then draws on media and communication studies to explore concepts such as agents, agency, power, and agentive relationships between humans and robots. The chapter focuses on the development of non-human agents in industry as a critical factor in the rise of augmentation technologies. It looks at how marketing communication enculturates future users to adopt and adapt to the technology. Scholars are charting the significant ways that people are drawn further into commercial digital landscapes, such as the Metaverse concept, in post-internet society. It concludes by examining recent claims concerning the Metaverse and augmented reality. 

**Abstract (ZH)**: augmentation 技术因多种因素正在经历文化融合的过程，其中一个因素是人工智能（AI）的兴起，或如世界知识产权组织（WIPO）所称的 AI 波潮或 AI 繁荣期。第三章重点关注高级、 emergent 和具体现身的 augmentation 技术将改善生活、 literacy、文化、艺术、经济和社交环境的过谘高的假设。该章首先讨论 AI 术语的模糊性问题，并通过描述 WIPO 人工智能技术分类方案来辅助解决这一问题。接着，该章借鉴媒体与传播研究，探讨代理、能动性、权力以及人类与机器人之间的能动关系等概念。该章关注行业内非人类代理的发展，将其视为 augmentation 技术兴起的关键因素之一。它考察了市场营销传播如何将未来用户纳入技术文化之中。学者们正在研究人们在后互联网社会中如何进一步被商业数字景观吸引，例如元宇宙概念。该章最后分析了有关元宇宙和增强现实的近期说法。 

---
