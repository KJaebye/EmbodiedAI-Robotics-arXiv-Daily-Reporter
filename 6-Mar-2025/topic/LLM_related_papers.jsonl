{'arxiv_id': 'arXiv:2503.03074', 'title': 'BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving', 'authors': 'Katharina Winter, Mark Azer, Fabian B. Flohr', 'link': 'https://arxiv.org/abs/2503.03074', 'abstract': 'Autonomous driving has the potential to set the stage for more efficient future mobility, requiring the research domain to establish trust through safe, reliable and transparent driving. Large Language Models (LLMs) possess reasoning capabilities and natural language understanding, presenting the potential to serve as generalized decision-makers for ego-motion planning that can interact with humans and navigate environments designed for human drivers. While this research avenue is promising, current autonomous driving approaches are challenged by combining 3D spatial grounding and the reasoning and language capabilities of LLMs. We introduce BEVDriver, an LLM-based model for end-to-end closed-loop driving in CARLA that utilizes latent BEV features as perception input. BEVDriver includes a BEV encoder to efficiently process multi-view images and 3D LiDAR point clouds. Within a common latent space, the BEV features are propagated through a Q-Former to align with natural language instructions and passed to the LLM that predicts and plans precise future trajectories while considering navigation instructions and critical scenarios. On the LangAuto benchmark, our model reaches up to 18.9% higher performance on the Driving Score compared to SoTA methods.', 'abstract_zh': '基于自主驾驶的贝维驱动：一种利用大语言模型实现端到端闭环驾驶的方法', 'title_zh': 'BEVDriver：利用BEV图在LLMs中实现稳健的闭环驾驶'}
{'arxiv_id': 'arXiv:2503.03505', 'title': 'Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems', 'authors': 'Yaoru Li, Shunyu Liu, Tongya Zheng, Mingli Song', 'link': 'https://arxiv.org/abs/2503.03505', 'abstract': 'Recent advancements in Large Language Model(LLM)-based Multi-Agent Systems(MAS) have demonstrated remarkable potential for tackling complex decision-making tasks. However, existing frameworks inevitably rely on serialized execution paradigms, where agents must complete sequential LLM planning before taking action. This fundamental constraint severely limits real-time responsiveness and adaptation, which is crucial in dynamic environments with ever-changing scenarios. In this paper, we propose a novel parallelized planning-acting framework for LLM-based MAS, featuring a dual-thread architecture with interruptible execution to enable concurrent planning and acting. Specifically, our framework comprises two core threads:(1) a planning thread driven by a centralized memory system, maintaining synchronization of environmental states and agent communication to support dynamic decision-making; and (2) an acting thread equipped with a comprehensive skill library, enabling automated task execution through recursive decomposition. Extensive experiments on challenging Minecraft demonstrate the effectiveness of the proposed framework.', 'abstract_zh': '基于大规模语言模型（LLM）的多智能体系统（MAS）的近期进展展现了解决复杂决策任务的显著潜力。然而，现有框架不可避免地依赖于串行执行范式，智能体必须完成序列化的LLM规划后再采取行动。这一根本性约束严重限制了实时响应能力和适应性，这对于充满不断变化场景的动态环境至关重要。本文提出了一种新颖的并行规划-行动框架，具备中断式执行的双线程架构，以实现并发规划和行动。具体而言，该框架包括两个核心线程：(1) 由集中式记忆系统驱动的规划线程，维护环境状态和代理通信的同步，以支持动态决策；(2) 配备全面技能库的行动线程，通过递归分解实现自动化任务执行。在具有挑战性的Minecraft实验中，该框架的有效性得到充分验证。', 'title_zh': '基于大型语言模型的多智能体系统并行计划-执行高效算法'}
{'arxiv_id': 'arXiv:2503.03459', 'title': 'Unified Mind Model: Reimagining Autonomous Agents in the LLM Era', 'authors': 'Pengbo Hu, Xiang Ying', 'link': 'https://arxiv.org/abs/2503.03459', 'abstract': 'Large language models (LLMs) have recently demonstrated remarkable capabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4), reviving the research of general autonomous agents with human-like cognitive this http URL human-level agents require semantic comprehension and instruction-following capabilities, which exactly fall into the strengths of this http URL there have been several initial attempts to build human-level agents based on LLMs, the theoretical foundation remains a challenging open problem. In this paper, we propose a novel theoretical cognitive architecture, the Unified Mind Model (UMM), which offers guidance to facilitate the rapid creation of autonomous agents with human-level cognitive abilities. Specifically, our UMM starts with the global workspace theory and further leverage LLMs to enable the agent with various cognitive abilities, such as multi-modal perception, planning, reasoning, tool use, learning, memory, reflection and motivation. Building upon UMM, we then develop an agent-building engine, MindOS, which allows users to quickly create domain-/task-specific autonomous agents without any programming effort.', 'abstract_zh': '大型语言模型（LLMs）最近在各个领域、任务和语言上展示了卓越的能力（例如ChatGPT和GPT-4），重新激发了对类人认知自主代理的研究。这种类人水平的代理需要语义理解和指令遵循能力，而这正是大型语言模型的优势所在。尽管基于LLMs构建类人水平代理的初步尝试已经出现，但其理论基础仍然是一个具有挑战性的开放问题。在本文中，我们提出了一种新的理论认知架构——统一心灵模型（Unified Mind Model, UMM），它为促进快速创建具有类人认知能力的自主代理提供了指导。具体而言，我们的UMM从全局工作空间理论出发，并进一步利用大型语言模型使代理具备多模感知、规划、推理、工具使用、学习、记忆、反思和动机等多种认知能力。基于UMM，我们随后开发了一个代理构建引擎——MindOS，使用户能够无需编写任何代码即可快速创建特定领域的自主代理。', 'title_zh': '统一心智模型：在大语言模型时代重构自主代理'}
{'arxiv_id': 'arXiv:2503.03350', 'title': 'Leveraging Large Language Models to Develop Heuristics for Emerging Optimization Problems', 'authors': 'Thomas Bömer, Nico Koltermann, Max Disselnmeyer, Laura Dörr, Anne Meyer', 'link': 'https://arxiv.org/abs/2503.03350', 'abstract': "Combinatorial optimization problems often rely on heuristic algorithms to generate efficient solutions. However, the manual design of heuristics is resource-intensive and constrained by the designer's expertise. Recent advances in artificial intelligence, particularly large language models (LLMs), have demonstrated the potential to automate heuristic generation through evolutionary frameworks. Recent works focus only on well-known combinatorial optimization problems like the traveling salesman problem and online bin packing problem when designing constructive heuristics. This study investigates whether LLMs can effectively generate heuristics for niche, not yet broadly researched optimization problems, using the unit-load pre-marshalling problem as an example case. We propose the Contextual Evolution of Heuristics (CEoH) framework, an extension of the Evolution of Heuristics (EoH) framework, which incorporates problem-specific descriptions to enhance in-context learning during heuristic generation. Through computational experiments, we evaluate CEoH and EoH and compare the results. Results indicate that CEoH enables smaller LLMs to generate high-quality heuristics more consistently and even outperform larger models. Larger models demonstrate robust performance with or without contextualized prompts. The generated heuristics exhibit scalability to diverse instance configurations.", 'abstract_zh': '组合优化问题常依赖启发式算法生成高效的解。然而，启发式算法的手动设计耗时且受限于设计者的专业知识。近年来，特别是在大规模语言模型（LLMs）方面的进展表明，可以通过进化框架自动化启发式的生成。现有工作在设计构造性启发式时，仅限于处理如旅行商问题和在线 bin 装箱问题等广为人知的组合优化问题。本研究探讨大规模语言模型是否能有效生成针对尚未广泛研究的特例优化问题的启发式算法，以单位负载预整理问题为例。我们提出了一种上下文启发式演化（CEoH）框架，这是启发式演化（EoH）框架的扩展，通过整合特定问题描述来增强启发式生成过程中的上下文学习。通过计算实验，我们评估了CEoH和EoH，并对比了结果。结果表明，CEoH使较小的语言模型能够更一致地生成高质量的启发式算法，甚至在某些情况下超越了更大模型。更大模型在有或没有上下文提示的情况下表现出稳健的性能。生成的启发式算法对不同实例配置具有可扩展性。', 'title_zh': '利用大规模语言模型开发新兴优化问题的启发式方法'}
{'arxiv_id': 'arXiv:2503.03128', 'title': 'Towards Understanding Multi-Round Large Language Model Reasoning: Approximability, Learnability and Generalizability', 'authors': 'Chenhui Xu, Dancheng Liu, Jiajie Li, Amir Nassereldine, Zhaohui Li, Jinjun Xiong', 'link': 'https://arxiv.org/abs/2503.03128', 'abstract': "Recent advancements in cognitive science and multi-round reasoning techniques for Large Language Models (LLMs) suggest that iterative thinking processes improve problem-solving performance in complex tasks. Inspired by this, approaches like Chain-of-Thought, debating, and self-refinement have been applied to auto-regressive LLMs, achieving significant successes in tasks such as mathematical reasoning, commonsense reasoning, and multi-hop question answering. Despite these successes, the theoretical basis for how multi-round reasoning enhances problem-solving abilities remains underexplored. In this work, we investigate the approximation, learnability, and generalization properties of multi-round auto-regressive models. We show that Transformers with finite context windows are universal approximators for steps of Turing-computable functions and can approximate any Turing-computable sequence-to-sequence function through multi-round reasoning. We extend PAC learning to sequence generation and demonstrate that multi-round generation is learnable even when the sequence length exceeds the model's context window. Finally, we examine how generalization error propagates across rounds, and show how the aforementioned approaches can help constrain this error, ensuring outputs stay within an expectation boundary. This work sheds light on the systemic theoretical foundations of multi-round sequence learning and reasoning, emphasizing its role in inference complexity.", 'abstract_zh': '最近的认知科学进展和多轮推理技术对于大型语言模型（LLMs）表明，迭代思考过程能够提高复杂任务中的问题解决性能。受此启发，诸如Chain-of-Thought、辩论和自我 refinement 等方法已被应用于自回归 LLMs，并在数学推理、常识推理和多跳问答等任务中取得了显著成果。尽管取得了这些成果，但多轮推理如何增强问题解决能力仍缺乏理论基础的探索。在此工作中，我们研究多轮自回归模型的逼近性、可学习性和泛化性质。我们证明，带有有限上下文窗口的Transformer是图灵可计算函数步数的通用逼近器，并可以通过多轮推理逼近任何图灵可计算的序列到序列函数。我们将PAC学习扩展到序列生成，并证明即使序列长度超过模型的上下文窗口，多轮生成也是可学习的。最后，我们研究了泛化误差在各轮之间的传播，并展示了上述方法如何帮助限制这一误差，确保输出保持在期望边界内。这项工作揭示了多轮序列学习和推理的系统理论基础，强调其在推理复杂性中的作用。', 'title_zh': '理解多轮大型语言模型推理：可近似性、可学习性和泛化能力'}
{'arxiv_id': 'arXiv:2503.02976', 'title': 'Teaching AI to Handle Exceptions: Supervised Fine-Tuning with Human-Aligned Judgment', 'authors': 'Matthew DosSantos DiSorbo, Harang Ju, Sinan Aral', 'link': 'https://arxiv.org/abs/2503.02976', 'abstract': "Large language models (LLMs), initially developed for generative AI, are now evolving into agentic AI systems, which make decisions in complex, real-world contexts. Unfortunately, while their generative capabilities are well-documented, their decision-making processes remain poorly understood. This is particularly evident when models are handling exceptions, a critical and challenging aspect of decision-making made relevant by the inherent incompleteness of contracts. Here we demonstrate that LLMs, even ones that excel at reasoning, deviate significantly from human judgments because they adhere strictly to policies, even when such adherence is impractical, suboptimal, or even counterproductive. We then evaluate three approaches to tuning AI agents to handle exceptions: ethical framework prompting, chain-of-thought reasoning, and supervised fine-tuning. We find that while ethical framework prompting fails and chain-of-thought prompting provides only slight improvements, supervised fine-tuning, specifically with human explanations, yields markedly better results. Surprisingly, in our experiments, supervised fine-tuning even enabled models to generalize human-like decision-making to novel scenarios, demonstrating transfer learning of human-aligned decision-making across contexts. Furthermore, fine-tuning with explanations, not just labels, was critical for alignment, suggesting that aligning LLMs with human judgment requires explicit training on how decisions are made, not just which decisions are made. These findings highlight the need to address LLMs' shortcomings in handling exceptions in order to guide the development of agentic AI toward models that can effectively align with human judgment and simultaneously adapt to novel contexts.", 'abstract_zh': '大型语言模型（LLMs）最初为生成型AI开发，现在正在演变为能在一个复杂的真实世界环境中做出决策的代理型AI系统。不幸的是，尽管它们的生成能力已被广泛记录，但其决策过程仍然不为人所理解。特别是在处理异常情况时，这一缺陷尤为明显，这是因为合同的固有不完备性使得决策过程变得关键且具有挑战性。我们证明，即使是在推理方面表现出色的LLMs，也因严格遵循既定政策而与人类判断产生显著差异，即使这种遵循政策是不切实际的、次优的或甚至是有害的。然后，我们评估了三种调校AI代理以处理异常情况的方法：伦理框架提示、逐步思考推理以及有监督微调。我们发现，伦理框架提示方法失败，逐步思考推理方法仅提供微小改进，而使用人类解释的有监督微调方法则取得了明显更好的效果。令人惊讶的是，在我们的实验中，有监督微调甚至使模型能够将类人的决策模式推广到新的场景中，从而展示了在不同背景下人类对齐决策模式的迁移学习。此外，使用解释而非仅标签示例进行微调对于对齐至关重要，这表明将LLMs与人类判断对齐需要显式训练如何做出决策，而不仅仅是做出哪些决策。这些发现突显出需要解决LLMs在处理异常情况方面的不足，以便引导代理型AI模型朝着能够有效与人类判断对齐并同时适应新情境的方向发展。', 'title_zh': '教AI处理异常：基于人类对齐判断的监督微调'}
{'arxiv_id': 'arXiv:2503.03750', 'title': 'The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems', 'authors': 'Richard Ren, Arunim Agarwal, Mantas Mazeika, Cristina Menghini, Robert Vacareanu, Brad Kenstler, Mick Yang, Isabelle Barrass, Alice Gatti, Xuwang Yin, Eduardo Trevino, Matias Geralnik, Adam Khoja, Dean Lee, Summer Yue, Dan Hendrycks', 'link': 'https://arxiv.org/abs/2503.03750', 'abstract': 'As large language models (LLMs) become more capable and agentic, the requirement for trust in their outputs grows significantly, yet at the same time concerns have been mounting that models may learn to lie in pursuit of their goals. To address these concerns, a body of work has emerged around the notion of "honesty" in LLMs, along with interventions aimed at mitigating deceptive behaviors. However, evaluations of honesty are currently highly limited, with no benchmark combining large scale and applicability to all models. Moreover, many benchmarks claiming to measure honesty in fact simply measure accuracy--the correctness of a model\'s beliefs--in disguise. In this work, we introduce a large-scale human-collected dataset for measuring honesty directly, allowing us to disentangle accuracy from honesty for the first time. Across a diverse set of LLMs, we find that while larger models obtain higher accuracy on our benchmark, they do not become more honest. Surprisingly, while most frontier LLMs obtain high scores on truthfulness benchmarks, we find a substantial propensity in frontier LLMs to lie when pressured to do so, resulting in low honesty scores on our benchmark. We find that simple methods, such as representation engineering interventions, can improve honesty. These results underscore the growing need for robust evaluations and effective interventions to ensure LLMs remain trustworthy.', 'abstract_zh': '大型语言模型（LLMs）能力增强的同时，对其输出的信任需求显著增长，但同时也出现了模型可能为了实现目标而学会撒谎的担忧。为应对这些担忧，围绕LLMs“诚实”这一概念的研究不断涌现，并提出了一系列旨在减轻欺骗行为的干预措施。然而，当前关于“诚实”的评估极为有限，缺乏一个结合大规模数据和普遍适用性的基准。此外，许多声称衡量“诚实”的基准实际上只是衡量准确性——即模型信念的正确性。在本研究中，我们引入了一个大规模的人类收集数据集，用于直接测量“诚实”，使我们能够首次剥离准确性与诚实性的关联。在涵盖多种类型的LLMs中，我们发现，虽然更大模型在我们的基准上获得了更高的准确性，但并没有变得更诚实。令人惊讶的是，尽管大多数前沿的LLMs在真实性基准上获得了高分，但在压力下撒谎的倾向仍然显著，导致其在我们的基准上的诚实性评分较低。我们发现，简单的干预措施，如表示工程干预，可以提高“诚实”。这些结果突显了对可靠评估和有效干预日益增长的需求，以确保LLMs保持可信。', 'title_zh': 'MASK基准：解开AI系统中诚实与准确的关系'}
{'arxiv_id': 'arXiv:2503.03746', 'title': 'Process-based Self-Rewarding Language Models', 'authors': 'Shimao Zhang, Xiao Liu, Xin Zhang, Junxiao Liu, Zheheng Luo, Shujian Huang, Yeyun Gong', 'link': 'https://arxiv.org/abs/2503.03746', 'abstract': "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities.", 'abstract_zh': '基于过程的自我奖励语言模型pipeline在数学推理场景中的应用', 'title_zh': '基于过程的自我奖励语言模型'}
{'arxiv_id': 'arXiv:2503.03669', 'title': 'Attentive Reasoning Queries: A Systematic Method for Optimizing Instruction-Following in Large Language Models', 'authors': 'Bar Karov, Dor Zohar, Yam Marcovitz', 'link': 'https://arxiv.org/abs/2503.03669', 'abstract': 'We present Attentive Reasoning Queries (ARQs), a novel structured reasoning approach that significantly improves instruction-following in Large Language Models through domain-specialized reasoning blueprints. While LLMs demonstrate remarkable capabilities across diverse tasks, they often fail to maintain adherence to complex, use-case-specific instructions during multi-turn conversations, presenting challenges for business-critical applications. ARQs address this limitation by guiding LLMs through systematic reasoning steps with targeted queries that reinstate critical instructions and facilitate intermediate reasoning throughout the completion process. In extensive testing within Parlant, our framework for reliable customer-facing agents in which ARQs were born out of necessity, they achieved a 90.2% success rate across 87 test scenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct response generation (81.5%). ARQs showed particular strength in addressing persistent failure modes like guideline re-application and hallucination prevention. Our analysis also revealed that ARQs can potentially be more computationally efficient than free-form reasoning when carefully designed. These findings demonstrate that structured reasoning approaches provide effective mechanisms for controlling how LLMs process information and make decisions in complex scenarios.', 'abstract_zh': '注意推理查询：一种通过领域特种推理蓝图显著提高大型语言模型指令遵循能力的新型结构化推理方法', 'title_zh': '注意力推理查询：优化大型语言模型遵循指令的一种系统方法'}
{'arxiv_id': 'arXiv:2503.03654', 'title': 'Improving Neutral Point of View Text Generation through Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality Dataset', 'authors': 'Jessica Hoffmann, Christiane Ahlheim, Zac Yu, Aria Walfrand, Jarvis Jin, Marie Tano, Ahmad Beirami, Erin van Liemt, Nithum Thain, Hakim Sidahmed, Lucas Dixon', 'link': 'https://arxiv.org/abs/2503.03654', 'abstract': "This paper describes the construction of a dataset and the evaluation of training methods to improve generative large language models' (LLMs) ability to answer queries on sensitive topics with a Neutral Point of View (NPOV), i.e., to provide significantly more informative, diverse and impartial answers. The dataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written quadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set of links to source texts elaborating the various points of view. The first key contribution of this paper is a new methodology to create such datasets through iterative rounds of human peer-critique and annotator training, which we release alongside the dataset. The second key contribution is the identification of a highly effective training regime for parameter-efficient reinforcement learning (PE-RL) to improve NPOV generation. We compare and extensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a strong baseline), SFT and RLHF.\nPE-RL not only improves on overall NPOV quality compared to the strongest baseline ($97.06\\%\\rightarrow 99.08\\%$), but also scores much higher on features linguists identify as key to separating good answers from the best answers ($60.25\\%\\rightarrow 85.21\\%$ for presence of supportive details, $68.74\\%\\rightarrow 91.43\\%$ for absence of oversimplification). A qualitative analysis corroborates this. Finally, our evaluation finds no statistical differences between results on topics that appear in the training dataset and those on separated evaluation topics, which provides strong evidence that our approach to training PE-RL exhibits very effective out of topic generalization.", 'abstract_zh': '一种用于提高大型语言模型在敏感话题上以中立视角回答查询能力的数据集构建与训练方法评价', 'title_zh': '通过参数高效强化学习和小型高质数据集改进中立观点文本生成'}
{'arxiv_id': 'arXiv:2503.03594', 'title': 'Small but Mighty: Enhancing Time Series Forecasting with Lightweight LLMs', 'authors': 'Haoran Fan, Bin Li, Yixuan Weng, Shoujun Zhou', 'link': 'https://arxiv.org/abs/2503.03594', 'abstract': "While LLMs have demonstrated remarkable potential in time series forecasting, their practical deployment remains constrained by excessive computational demands and memory footprints. Existing LLM-based approaches typically suffer from three critical limitations: Inefficient parameter utilization in handling numerical time series patterns; Modality misalignment between continuous temporal signals and discrete text embeddings; and Inflexibility for real-time expert knowledge integration. We present SMETimes, the first systematic investigation of sub-3B parameter SLMs for efficient and accurate time series forecasting. Our approach centers on three key innovations: A statistically-enhanced prompting mechanism that bridges numerical time series with textual semantics through descriptive statistical features; A adaptive fusion embedding architecture that aligns temporal patterns with language model token spaces through learnable parameters; And a dynamic mixture-of-experts framework enabled by SLMs' computational efficiency, adaptively combining base predictions with domain-specific models. Extensive evaluations across seven benchmark datasets demonstrate that our 3B-parameter SLM achieves state-of-the-art performance on five primary datasets while maintaining 3.8x faster training and 5.2x lower memory consumption compared to 7B-parameter LLM baselines. Notably, the proposed model exhibits better learning capabilities, achieving 12.3% lower MSE than conventional LLM. Ablation studies validate that our statistical prompting and cross-modal fusion modules respectively contribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks. By redefining the efficiency-accuracy trade-off landscape, this work establishes SLMs as viable alternatives to resource-intensive LLMs for practical time series forecasting. Code and models are available at this https URL.", 'abstract_zh': '虽然大的语言模型在时间序列预测方面展现了显著的潜力，但其实用部署仍受限于过高的计算需求和内存占用。现有的基于大语言模型的方法通常面临三个关键限制：在处理数值时间序列模式时参数利用效率低下；连续时间信号与离散文本嵌入之间的模态不对齐；以及对实时专家知识整合的灵活性不足。我们提出了SMETimes，这是对参数量小于3B的可解释小语言模型进行系统研究的第一个尝试，旨在实现高效且准确的时间序列预测。我们的方法侧重于三个方面的重要创新：通过描述性统计特征增强的统计提示机制，实现数值时间序列与文本语义的连接；自适应融合嵌入架构，通过可学习参数使时间模式与语言模型词元空间相匹配；以及基于SLMs计算效率的动态专家混合框架，能够灵活结合基础预测和领域特定模型。在跨越七个基准数据集的广泛评估中，我们的3B参数SLM在五个主要数据集上达到了最先进的性能，同时相比7B参数大语言模型基线，训练速度提升3.8倍，并且内存消耗降低了5.2倍。值得注意的是，所提模型表现出更好的学习能力，MSE降低了12.3%。消融实验验证了我们的统计提示和跨模态融合模块分别在长时预测任务中贡献了15.7%和18.2%的误差减少。通过重新定义效率与准确性之间的权衡范围，这项工作确立了SLMs作为一种资源密集型大语言模型的实用替代品在实际时间序列预测中的可行性。代码和模型可在以下链接获取。', 'title_zh': '小巧而强大：轻量级语言模型增强时间序列预测'}
{'arxiv_id': 'arXiv:2503.03592', 'title': 'English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance', 'authors': 'Karl Audun Borgersen', 'link': 'https://arxiv.org/abs/2503.03592', 'abstract': "For consumer usage of locally deployed LLMs, the GGUF format and k_quantization are invaluable tools for maintaining the performance of the original model while reducing it to sizes deployable with consumer-grade hardware. The number of bits dedicated to each weight from the original model is reduced based on how important they are thought to be during model inference. This importance is arrived at through the application of an 'importance matrix'-a relatively small text document meant to be representative of the LLM's standard use-cases. In the vast majority of quants available online, this document is primarily written in English. It was therefore an open question whether performance on English language tasks was preserved through the sacrifice of multilingual performance and whether it can be preserved with alternate importance matrices. This article investigates these hypotheses by quantizing Llama3.3 70B on importance matrices written in three languages (English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset in both English and Norwegian. All experiments related to k_quantization yielded non-significant results (In all cases p > 0.237) indicating that current quantization practices do not disproportionately harm multilingual performance.", 'abstract_zh': '针对本地部署LLM的消费者使用，GGUF格式和k量化的工具对于在减小程序大小的同时保持原始模型性能具有重要作用。通过“重要性矩阵”确定每个权重分配的位数，该矩阵是一个相对较小的文本文件，旨在代表LLM的标准使用场景。目前，在线提供的大多数量化文档主要是用英语撰写的。因此，一个开放的问题是，通过牺牲多语言性能来保留英语语言任务的性能是否可行，以及是否可以通过使用不同语言编写的重要性矩阵来保留多语言性能。本文通过使用三种语言（英语、挪威语和马拉雅拉姆语）撰写的“重要性矩阵”对Llama3.3 70B进行量化，并在英语和挪威语上使用MixEval数据集进行评估，结果表明，当前的量化实践并未对多语言性能造成不成比例的损害。', 'title_zh': '多语言性能不受K量化LMs不当减损'}
{'arxiv_id': 'arXiv:2503.03503', 'title': 'Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization', 'authors': 'Jiajun Yu, Yizhen Zheng, Huan Yee Koh, Shirui Pan, Tianyue Wang, Haishuai Wang', 'link': 'https://arxiv.org/abs/2503.03503', 'abstract': 'Molecular optimization is a crucial yet complex and time-intensive process that often acts as a bottleneck for drug development. Traditional methods rely heavily on trial and error, making multi-objective optimization both time-consuming and resource-intensive. Current AI-based methods have shown limited success in handling multi-objective optimization tasks, hampering their practical utilization. To address this challenge, we present MultiMol, a collaborative large language model (LLM) system designed to guide multi-objective molecular optimization. MultiMol comprises two agents, including a data-driven worker agent and a literature-guided research agent. The data-driven worker agent is a large language model being fine-tuned to learn how to generate optimized molecules considering multiple objectives, while the literature-guided research agent is responsible for searching task-related literature to find useful prior knowledge that facilitates identifying the most promising optimized candidates. In evaluations across six multi-objective optimization tasks, MultiMol significantly outperforms existing methods, achieving a 82.30% success rate, in sharp contrast to the 27.50% success rate of current strongest methods. To further validate its practical impact, we tested MultiMol on two real-world challenges. First, we enhanced the selectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds both A1R and A2AR, successfully biasing it towards A1R. Second, we improved the bioavailability of Saquinavir, an HIV-1 protease inhibitor with known bioavailability limitations. Overall, these results indicate that MultiMol represents a highly promising approach for multi-objective molecular optimization, holding great potential to accelerate the drug development process and contribute to the advancement of pharmaceutical research.', 'abstract_zh': '分子优化是药物开发中至关重要的 yet 复杂和耗时的过程，往往是药物开发中的瓶颈。传统方法依赖于试错，使得多目标优化既耗时又耗资源。当前基于AI的方法在处理多目标优化任务方面取得的成效有限，阻碍了其实际应用。为应对这一挑战，我们提出了MultiMol，一个协作的大语言模型系统，旨在指导多目标分子优化。MultiMol 包含两个代理，包括一个数据驱动的工作者代理和一个文献引导的研究代理。数据驱动的工作者代理是一个正在微调的大语言模型，学习如何生成考虑多个目标的优化分子，而文献引导的研究代理负责搜索相关任务的文献以寻找有用的前提知识，从而有助于识别最有可能的优化候选物。在对六个多目标优化任务的评估中，MultiMol 显著优于现有方法，成功率为82.30%，而当前最强方法的成功率为27.50%。为进一步验证其实际影响，我们在两个实际挑战中测试了MultiMol。首先，我们增强了杂环嘌呤氨酸（XAC）的选择性，这是一种兼具A1R和A2AR结合活性的促混适配体。其次，我们提高了沙奎那韦（一种具有已知生物利用度限制的HIV-1蛋白酶抑制剂）的生物利用度。总体而言，这些结果表明MultiMol 是一个多目标分子优化的极具前景的方法，有望加速药物开发过程并推动制药研究的发展。', 'title_zh': '协作专家大语言模型引导的多目标分子优化'}
{'arxiv_id': 'arXiv:2503.03502', 'title': 'CURVALID: Geometrically-guided Adversarial Prompt Detection', 'authors': 'Canaan Yung, Hanxun Huang, Sarah Monazam Erfani, Christopher Leckie', 'link': 'https://arxiv.org/abs/2503.03502', 'abstract': 'Adversarial prompts capable of jailbreaking large language models (LLMs) and inducing undesirable behaviours pose a significant obstacle to their safe deployment. Current mitigation strategies rely on activating built-in defence mechanisms or fine-tuning the LLMs, but the fundamental distinctions between adversarial and benign prompts are yet to be understood. In this work, we introduce CurvaLID, a novel defense framework that efficiently detects adversarial prompts by leveraging their geometric properties. It is agnostic to the type of LLM, offering a unified detection framework across diverse adversarial prompts and LLM architectures. CurvaLID builds on the geometric analysis of text prompts to uncover their underlying differences. We theoretically extend the concept of curvature via the Whewell equation into an $n$-dimensional word embedding space, enabling us to quantify local geometric properties, including semantic shifts and curvature in the underlying manifolds. Additionally, we employ Local Intrinsic Dimensionality (LID) to capture geometric features of text prompts within adversarial subspaces. Our findings reveal that adversarial prompts differ fundamentally from benign prompts in terms of their geometric characteristics. Our results demonstrate that CurvaLID delivers superior detection and rejection of adversarial queries, paving the way for safer LLM deployment. The source code can be found at this https URL', 'abstract_zh': '具备打破大规模语言模型（LLMs）并诱导不良行为的对抗提示构成了其安全部署的重大障碍。当前的缓解策略依赖于激活内置防御机制或微调LLMs，但对抗性提示与良性提示之间的根本区别尚未被理解。在本工作中，我们引入了CurvaLID，这是一种新型的防御框架，通过利用提示的几何属性有效地检测对抗提示。CurvaLID 不依赖于特定类型的LLM，提供了一种适用于各种对抗提示和LLM架构的统一检测框架。CurvaLID 基于文本提示的几何分析来揭示其潜在差异。我们通过Whewell方程将曲率的概念扩展到$n$维词嵌入空间，使得我们可以量化局部几何特性，包括语义转换和潜在流形中的曲率。此外，我们利用局部固有维数（LID）来捕捉对抗子空间内文本提示的几何特征。我们的研究结果表明，对抗提示在几何特性上与良性提示有根本的不同。我们的结果表明，CurvaLID 在检测和拒绝对抗查询方面表现出色，为更安全的大规模语言模型部署铺平了道路。源代码可在以下网址找到：这个 https URL。', 'title_zh': 'CURVALID: 几何引导的对抗提示检测'}
{'arxiv_id': 'arXiv:2503.03462', 'title': 'Open-Source Large Language Models as Multilingual Crowdworkers: Synthesizing Open-Domain Dialogues in Several Languages With No Examples in Targets and No Machine Translation', 'authors': 'Ahmed Njifenjou, Virgile Sucal, Bassam Jabaian, Fabrice Lefèvre', 'link': 'https://arxiv.org/abs/2503.03462', 'abstract': 'The prevailing paradigm in the domain of Open-Domain Dialogue agents predominantly focuses on the English language, encompassing both models and datasets. Furthermore, the financial and temporal investments required for crowdsourcing such datasets for finetuning are substantial, particularly when multiple languages are involved. Fortunately, advancements in Large Language Models (LLMs) have unveiled a plethora of possibilities across diverse tasks. Specifically, instruction-tuning has enabled LLMs to execute tasks based on natural language instructions, occasionally surpassing the performance of human crowdworkers. Additionally, these models possess the capability to function in various languages within a single thread. Consequently, to generate new samples in different languages, we propose leveraging these capabilities to replicate the data collection process. We introduce a pipeline for generating Open-Domain Dialogue data in multiple Target Languages using LLMs, with demonstrations provided in a unique Source Language. By eschewing explicit Machine Translation in this approach, we enhance the adherence to language-specific nuances. We apply this methodology to the PersonaChat dataset. To enhance the openness of generated dialogues and mimic real life scenarii, we added the notion of speech events corresponding to the type of conversation the speakers are involved in and also that of common ground which represents the premises of a conversation.', 'abstract_zh': '开放域对话代理领域的 prevailing 核心范式主要集中在英语语言，涵盖模型和数据集。然而，多语言场景下通过众包收集用于微调的数据集需要大量的财力和时间投入。幸运的是，大型语言模型（LLMs）的进步为多种任务开启了诸多可能性。特别是通过指令调谐，LLMs 可根据自然语言指令执行任务，有时甚至超越人类众包工人的表现。此外，这些模型能够在一个线程中处理多种语言。因此，为了生成不同语言的新样本，我们提出利用这些能力来复制数据收集过程。我们介绍了一种使用 LLMs 生成多目标语言开放域对话数据的管道，示范语言为独特的源语言。通过避免显式的机器翻译，我们增强了对语言特定细微差别的遵从。我们将此方法应用于 PersonaChat 数据集。为了增加生成对话的开放性并模仿真实场景，我们添加了与对话类型相对应的言语事件的概念，以及代表对话前提的共同知识概念。', 'title_zh': '开源大型语言模型作为多语言群众工作者：在无目标示例和无机器翻译的情况下合成多种语言的开放领域对话'}
{'arxiv_id': 'arXiv:2503.03444', 'title': 'Taxation Perspectives from Large Language Models: A Case Study on Additional Tax Penalties', 'authors': 'Eunkyung Choi, Young Jin Suh, Hun Park, Wonseok Hwang', 'link': 'https://arxiv.org/abs/2503.03444', 'abstract': "How capable are large language models (LLMs) in the domain of taxation? Although numerous studies have explored the legal domain in general, research dedicated to taxation remain scarce. Moreover, the datasets used in these studies are either simplified, failing to reflect the real-world complexities, or unavailable as open source. To address this gap, we introduce PLAT, a new benchmark designed to assess the ability of LLMs to predict the legitimacy of additional tax penalties. PLAT is constructed to evaluate LLMs' understanding of tax law, particularly in cases where resolving the issue requires more than just applying related statutes. Our experiments with six LLMs reveal that their baseline capabilities are limited, especially when dealing with conflicting issues that demand a comprehensive understanding. However, we found that enabling retrieval, self-reasoning, and discussion among multiple agents with specific role assignments, this limitation can be mitigated.", 'abstract_zh': '大型语言模型在税收领域的能力如何？尽管已有大量研究探讨法律领域的一般问题，针对税收领域的研究仍较为稀缺。此外，这些研究中使用的数据集要么过于简化，无法反映现实世界的复杂性，要么无法获取开源数据。为填补这一空白，我们引入了PLAT，这是一个新的基准测试，旨在评估大型语言模型预测额外税收罚款正当性的能力。PLAT旨在评估大型语言模型对税法的理解，特别是在解决需要全面理解而非仅应用相关法律条文的问题时。我们的实验结果显示，大型语言模型的基本能力有限，尤其是在处理需要全面理解的冲突问题时。然而，我们发现通过启用检索、自我推理以及多角色代理之间的讨论，可以缓解这一限制。', 'title_zh': '大型语言模型视角下的税制探讨：关于额外税务处罚的案例研究'}
{'arxiv_id': 'arXiv:2503.03434', 'title': 'RASD: Retrieval-Augmented Speculative Decoding', 'authors': 'Guofeng Quan, Wenfeng Feng, Chuzhan Hao, Guochao Jiang, Yuewei Zhang, Hao Wang', 'link': 'https://arxiv.org/abs/2503.03434', 'abstract': "Speculative decoding accelerates inference in large language models (LLMs) by generating draft tokens for target model verification. Current approaches for obtaining draft tokens rely on lightweight draft models or additional model structures to generate draft tokens and retrieve context from databases. Due to the draft model's small size and limited training data, model-based speculative decoding frequently becomes less effective in out-of-domain scenarios. Additionally, the time cost of the drafting phase results in a low upper limit on acceptance length during the verification step, limiting overall efficiency. This paper proposes RASD (Retrieval-Augmented Speculative Decoding), which adopts retrieval methods to enhance model-based speculative decoding. We introduce tree pruning and tree fusion to achieve this. Specifically, we develop a pruning method based on the draft model's probability distribution to construct the optimal retrieval tree. Second, we employ the longest prefix matching algorithm to merge the tree generated by the draft model with the retrieval tree, resulting in a unified tree for verification. Experimental results demonstrate that RASD achieves state-of-the-art inference acceleration across tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD exhibits strong scalability, seamlessly integrating with various speculative decoding approaches, including both generation-based and retrieval-based methods.", 'abstract_zh': '基于检索的推测性解码加速大型语言模型的推理', 'title_zh': 'RASD: 检索增强推测解码'}
{'arxiv_id': 'arXiv:2503.03258', 'title': 'Exploring the Potential of Large Language Models as Predictors in Dynamic Text-Attributed Graphs', 'authors': 'Runlin Lei, Jiarui Ji, Haipeng Ding, Lu Yi, Zhewei Wei, Yongchao Liu, Chuntao Hong', 'link': 'https://arxiv.org/abs/2503.03258', 'abstract': "With the rise of large language models (LLMs), there has been growing interest in Graph Foundation Models (GFMs) for graph-based tasks. By leveraging LLMs as predictors, GFMs have demonstrated impressive generalizability across various tasks and datasets. However, existing research on LLMs as predictors has predominantly focused on static graphs, leaving their potential in dynamic graph prediction unexplored. In this work, we pioneer using LLMs for predictive tasks on dynamic graphs. We identify two key challenges: the constraints imposed by context length when processing large-scale historical data and the significant variability in domain characteristics, both of which complicate the development of a unified predictor. To address these challenges, we propose the GraphAgent-Dynamic (GAD) Framework, a multi-agent system that leverages collaborative LLMs. In contrast to using a single LLM as the predictor, GAD incorporates global and local summary agents to generate domain-specific knowledge, enhancing its transferability across domains. Additionally, knowledge reflection agents enable adaptive updates to GAD's knowledge, maintaining a unified and self-consistent architecture. In experiments, GAD demonstrates performance comparable to or even exceeds that of full-supervised graph neural networks without dataset-specific training. Finally, to enhance the task-specific performance of LLM-based predictors, we discuss potential improvements, such as dataset-specific fine-tuning to LLMs. By developing tailored strategies for different tasks, we provide new insights for the future design of LLM-based predictors.", 'abstract_zh': '随着大规模语言模型（LLMs）的兴起，基于图的任务的图基础模型（GFMs）日益引起研究兴趣。通过利用LLMs作为预测器，GFMs在多种任务和数据集上展现了出色的泛化能力。然而，现有研究主要集中在静态图上，动态图预测的潜力尚未得到充分探索。在本文中，我们率先将LLMs应用于动态图的预测任务。我们识别出两个关键挑战：处理大规模历史数据时由上下文长度限制带来的约束，以及领域特征的显著差异，这两者都使得统一预测器的发展复杂化。为应对这些挑战，我们提出了GraphAgent-Dynamic（GAD）框架，这是一种利用协作性LLMs的多agent系统。与使用单一LLM作为预测器不同，GAD融合了全局和局部摘要agent以生成领域特定知识，增强了其跨领域的可迁移性。此外，知识反思agent允许GAD的知识进行自适应更新，保持统一且自洽的架构。在实验中，GAD在某些任务上表现出与全监督图神经网络相当甚至更好的性能，无需针对特定数据集进行训练。最后，为了提高基于LLM的预测器的任务特定性能，我们讨论了潜在的改进，如针对特定数据集对LLM进行微调。通过开发针对不同任务的定制化策略，我们为未来基于LLM的预测器的设计提供了新的见解。', 'title_zh': '探索大型语言模型在动态文本属性图中的预测潜力'}
{'arxiv_id': 'arXiv:2503.03245', 'title': 'Less is more? Rewards in RL for Cyber Defence', 'authors': 'Elizabeth Bates, Chris Hicks, Vasilios Mavroudis', 'link': 'https://arxiv.org/abs/2503.03245', 'abstract': 'The last few years has seen an explosion of interest in autonomous cyber defence agents based on deep reinforcement learning. Such agents are typically trained in a cyber gym environment, also known as a cyber simulator, at least 32 of which have already been built. Most, if not all cyber gyms provide dense "scaffolded" reward functions which combine many penalties or incentives for a range of (un)desirable states and costly actions. Whilst dense rewards help alleviate the challenge of exploring complex environments, yielding seemingly effective strategies from relatively few environment steps; they are also known to bias the solutions an agent can find, potentially towards suboptimal solutions. Sparse rewards could offer preferable or more effective solutions and have been overlooked by cyber gyms to date. In this work we set out to evaluate whether sparse reward functions might enable training more effective cyber defence agents. Towards this goal we first break down several evaluation limitations in existing work by proposing a ground truth evaluation score that goes beyond the standard RL paradigm used to train and evaluate agents. By adapting a well-established cyber gym to accommodate our methodology and ground truth score, we propose and evaluate two sparse reward mechanisms and compare them with a typical dense reward. Our evaluation considers a range of network sizes, from 2 to 50 nodes, and both reactive and proactive defensive actions. Our results show that sparse rewards, particularly positive reinforcement for an uncompromised network state, enable the training of more effective cyber defence agents. Furthermore, we show that sparse rewards provide more stable training than dense rewards, and that both effectiveness and training stability are robust to a variety of cyber environment considerations.', 'abstract_zh': '近年来，基于深度强化学习的自主网络防御代理引起了广泛关注。这类代理通常在一种被称为“网络模拟器”的网络健身房环境中进行训练，已有至少32种网络健身房被构建。大多数，如果不是全部，网络健身房提供了密集的“支撑式”奖励函数，结合了多种对多种（不）希望状态和昂贵行为的惩罚或激励。尽管密集奖励有助于缓解探索复杂环境的挑战，并从相对较少的环境步骤中产生看似有效的策略；但它们也可能使代理找到的解决方案偏向于非最优解。稀疏奖励可能提供更优或更有效的解决方案，并且到目前为止，网络健身房尚未对此予以关注。为了评估稀疏奖励机制是否能够训练出更有效的网络防御代理，我们提出了一种超越标准RL范式的地真相对于现有工作的评估限制提出了一个真正的评估得分。通过调整一个现有的网络健身房以适应我们的方法和地真相对于，我们提出了两种稀疏奖励机制并进行了评估，将其与典型的密集奖励进行了比较。我们的评估考虑了从2到50个节点的不同网络规模，以及反应性和前瞻性防御动作。结果表明，特别是对未受损网络状态的正向强化，稀疏奖励能够训练出更有效的网络防御代理。此外，我们证明稀疏奖励相较于密集奖励提供了更稳定的学习，同时，效果和学习稳定性对各种网络环境考虑因素具有鲁棒性。', 'title_zh': '少就是多？网络防御中强化学习的奖励设计'}
{'arxiv_id': 'arXiv:2503.03238', 'title': 'FANS -- Formal Answer Selection for Natural Language Math Reasoning Using Lean4', 'authors': 'Jiarui Yao, Ruida Wang, Tong Zhang', 'link': 'https://arxiv.org/abs/2503.03238', 'abstract': "Large Language Models (LLMs) have displayed astonishing abilities in various tasks, especially in text generation, classification, question answering, etc. However, the reasoning ability of LLMs still faces many debates. The inherent ambiguity of Natural Language (NL) limits LLMs' ability to perform verifiable reasoning, making its answers lack coherence and trustworthy support. To tackle the above problems, we propose a novel framework named FANS: Formal ANswer Selection for Natural Language Math Reasoning Using Lean4. To the best of our knowledge, it is the first framework that utilizes Lean4 to enhance LLMs' NL math reasoning ability. In particular, given an NL math question and LLM-generated answers, FANS first translates it into Lean4 theorem statements. Then it tries to prove it using a Lean4 prover and verify it by Lean4. Finally, it uses the FL result to assist in answer selection. It enhances LLMs' NL math ability in providing a computer-verifiable solution for its correct answer and proposes an alternative method for answer selection beyond the reward model. Extensive experiments indicate the effectiveness of our framework. It can improve the accuracy rate of reward model enhanced LLMs in the MATH-500 dataset by at most 1.91% and AMC-23 by at most 8.33% on strong reward-model baselines. In some particular fields like number theory that Lean4 experts in, we can even select all correct solutions. The qualitative analysis also shows our framework can make NL results formally backed by Lean4 proofs. As a pioneering work in the corresponding field, we will open-source all our models and datasets to further boost the development of the field.", 'abstract_zh': '大型语言模型（LLMs）在各种任务中展现了令人惊讶的能力，特别是在文本生成、分类、问答等方面。然而，LLMs的推理能力仍然存在许多争议。自然语言（NL）的内在歧义性限制了LLMs进行可验证推理的能力，使其答案缺乏连贯性和可信的支持。为了解决上述问题，我们提出了一种名为FANS的新框架：使用Lean4进行自然语言数学推理的形式答案选择。据我们所知，这是第一个利用Lean4增强LLMs自然语言数学推理能力的框架。特别是，给定一个自然语言数学问题和LLMs生成的答案，FANS首先将其翻译成Lean4定理陈述。然后，使用Lean4证明器尝试证明它，并通过Lean4验证。最后，使用FL结果辅助答案选择。它通过为正确答案提供计算机可验证的解决方案增强了LLMs的自然语言数学能力，并提出了一种超越奖励模型的备选答案选择方法。广泛实验表明该框架的有效性。它可以在MATH-500数据集中将奖励模型增强的LLMs的准确率最多提高1.91%，在AMC-23数据集上最多提高8.33%，在某些特定领域如Lean4专家擅长的数论领域，甚至可以选出所有正确答案。定性分析也表明，该框架可以使自然语言结果正式地由Lean4证明支撑。作为该领域的开创性工作，我们将开源所有模型和数据集以进一步促进该领域的发展。', 'title_zh': 'FANS -- 形式化的答案选择在自然语言数学推理中的应用（使用Lean4）'}
{'arxiv_id': 'arXiv:2503.03194', 'title': 'Structured Outputs Enable General-Purpose LLMs to be Medical Experts', 'authors': 'Guangfu Guo, Kai Zhang, Bryan Hoo, Yujun Cai, Xiaoqian Lu, Nanyun Peng, Yiwei Wang', 'link': 'https://arxiv.org/abs/2503.03194', 'abstract': "Medical question-answering (QA) is a critical task for evaluating how effectively large language models (LLMs) encode clinical knowledge and assessing their potential applications in medicine. Despite showing promise on multiple-choice tests, LLMs frequently struggle with open-ended medical questions, producing responses with dangerous hallucinations or lacking comprehensive coverage of critical aspects. Existing approaches attempt to address these challenges through domain-specific fine-tuning, but this proves resource-intensive and difficult to scale across models. To improve the comprehensiveness and factuality of medical responses, we propose a novel approach utilizing structured medical reasoning. Our method guides LLMs through an seven-step cognitive process inspired by clinical diagnosis, enabling more accurate and complete answers without additional training. Experiments on the MedLFQA benchmark demonstrate that our approach achieves the highest Factuality Score of 85.8, surpassing fine-tuned models. Notably, this improvement transfers to smaller models, highlighting the method's efficiency and scalability. Our code and datasets are available.", 'abstract_zh': '医学问答(Medical QA)是评估大规模语言模型(LLMs)如何有效地编码临床知识以及评估其在医学领域的潜在应用的关键任务。尽管在多项选择测试中显示出潜力，LLMs在应对开放性医学问题时经常遇到困难，生成包含危险幻觉或缺乏关键方面综合覆盖的回复。现有方法试图通过领域特定的微调来解决这些挑战，但这证明资源密集且难以在多个模型上扩展。为提高医学回复的全面性和事实性，我们提出了一种新的方法，利用结构化的医学推理。我们的方法指导LLMs通过一个受临床诊断启发的七步认知过程，使其能够提供更准确和完整的答案，而无需额外的训练。在MedLFQA基准测试上的实验表明，我们的方法实现了最高的事实得分85.8，超过了细调模型。值得注意的是，这一改进适用于较小的模型，突显了该方法的高效性和可扩展性。我们的代码和数据集已公开。', 'title_zh': '结构化输出使通用型大语言模型成为医疗专家'}
{'arxiv_id': 'arXiv:2503.03170', 'title': "AttackSeqBench: Benchmarking Large Language Models' Understanding of Sequential Patterns in Cyber Attacks", 'authors': 'Javier Yong, Haokai Ma, Yunshan Ma, Anis Yusof, Zhenkai Liang, Ee-Chien Chang', 'link': 'https://arxiv.org/abs/2503.03170', 'abstract': "The observations documented in Cyber Threat Intelligence (CTI) reports play a critical role in describing adversarial behaviors, providing valuable insights for security practitioners to respond to evolving threats. Recent advancements of Large Language Models (LLMs) have demonstrated significant potential in various cybersecurity applications, including CTI report understanding and attack knowledge graph construction. While previous works have proposed benchmarks that focus on the CTI extraction ability of LLMs, the sequential characteristic of adversarial behaviors within CTI reports remains largely unexplored, which holds considerable significance in developing a comprehensive understanding of how adversaries operate. To address this gap, we introduce AttackSeqBench, a benchmark tailored to systematically evaluate LLMs' capability to understand and reason attack sequences in CTI reports. Our benchmark encompasses three distinct Question Answering (QA) tasks, each task focuses on the varying granularity in adversarial behavior. To alleviate the laborious effort of QA construction, we carefully design an automated dataset construction pipeline to create scalable and well-formulated QA datasets based on real-world CTI reports. To ensure the quality of our dataset, we adopt a hybrid approach of combining human evaluation and systematic evaluation metrics. We conduct extensive experiments and analysis with both fast-thinking and slow-thinking LLMs, while highlighting their strengths and limitations in analyzing the sequential patterns in cyber attacks. The overarching goal of this work is to provide a benchmark that advances LLM-driven CTI report understanding and fosters its application in real-world cybersecurity operations. Our dataset and code are available at this https URL .", 'abstract_zh': '攻击序列基准：评估大规模语言模型在网络威胁情报报告中理解与推理攻击序列的能力', 'title_zh': 'AttackSeqBench: 评价大规模语言模型对网络攻击序列模式理解的能力'}
{'arxiv_id': 'arXiv:2503.03108', 'title': 'SoK: Knowledge is All You Need: Last Mile Delivery for Automated Provenance-based Intrusion Detection with LLMs', 'authors': 'Wenrui Cheng, Tiantian Zhu, Chunlin Xiong, Haofei Sun, Zijun Wang, Shunan Jing, Mingqi Lv, Yan Chen', 'link': 'https://arxiv.org/abs/2503.03108', 'abstract': "Recently, provenance-based intrusion detection systems (PIDSes) have been widely proposed for endpoint threat analysis. However, due to the lack of systematic integration and utilization of knowledge, existing PIDSes still require significant manual intervention for practical deployment, making full automation challenging. This paper presents a disruptive innovation by categorizing PIDSes according to the types of knowledge they utilize. In response to the prevalent issue of ``knowledge silos problem'' in existing research, we introduce a novel knowledge-driven provenance-based intrusion detection framework, powered by large language models (LLMs). We also present OmniSec, a best practice system built upon this framework. By integrating attack representation knowledge, threat intelligence knowledge, and benign behavior knowledge, OmniSec outperforms the state-of-the-art approaches on public benchmark datasets. OmniSec is available online at this https URL.", 'abstract_zh': '基于知识驱动的来源导向入侵检测框架及OmniSec系统', 'title_zh': 'SoK: 知识即一切：基于知识追溯的自动入侵检测的最后里程LLMs实现'}
{'arxiv_id': 'arXiv:2503.03040', 'title': 'SAGE: Steering and Refining Dialog Generation with State-Action Augmentation', 'authors': 'Yizhe Zhang, Navdeep Jaitly', 'link': 'https://arxiv.org/abs/2503.03040', 'abstract': 'Recent advances in large language models have demonstrated impressive capabilities in task-oriented applications, yet building emotionally intelligent chatbots that can engage in natural, strategic conversations remains a challenge. We present a novel approach called SAGE that uses latent variables to control long-horizon behavior in dialogue generation. At the core of our method is the State-Action Chain (SAC), which augments standard language model fine-tuning by introducing latent variables that encapsulate emotional states and conversational strategies between dialogue turns. During inference, these variables are generated before each response, enabling coarse-grained control over dialogue progression while maintaining natural interaction patterns. We also introduce a self-improvement pipeline that leverages dialogue tree search, LLM-based reward modeling, and targeted fine-tuning to optimize conversational trajectories. Our experimental results show that models trained with this approach demonstrate improved performance in emotional intelligence metrics while maintaining strong capabilities on LLM benchmarks. The discrete nature of our latent variables facilitates search-based strategies and provides a foundation for future applications of reinforcement learning to dialogue systems, where learning can occur at the state level rather than the token level.', 'abstract_zh': '最近大型语言模型的发展展示了其在任务导向应用中的强大能力，然而构建能够进行自然、策略性对话的具有情感智能的聊天机器人仍然面临挑战。我们提出了一种名为SAGE的新方法，通过潜在变量控制对话生成中的长时行为。该方法的核心是状态-动作链（SAC），它通过引入封装情感状态和对话策略的潜在变量来增强标准语言模型的微调。在推理过程中，这些变量在每次响应生成之前被生成，从而实现了对话进程的粗粒度控制，同时保持自然的交互模式。我们还引入了一种自我改进pipeline，通过对话树搜索、基于语言模型的奖励建模和目标导向微调来优化对话轨迹。我们的实验结果表明，采用这种方法训练的模型在情感智能指标上表现出改进的性能，同时在大型语言模型基准上保持了强大的能力。我们潜在变量的离散性质为基于强化学习的对话系统应用提供了基础，在这些应用中，学习可以在状态层面而非token层面进行。', 'title_zh': 'SAGE: 通过状态-动作增强引导和精炼对话生成'}
{'arxiv_id': 'arXiv:2503.03039', 'title': 'LLM Misalignment via Adversarial RLHF Platforms', 'authors': 'Erfan Entezami, Ali Naseh', 'link': 'https://arxiv.org/abs/2503.03039', 'abstract': "Reinforcement learning has shown remarkable performance in aligning language models with human preferences, leading to the rise of attention towards developing RLHF platforms. These platforms enable users to fine-tune models without requiring any expertise in developing complex machine learning algorithms. While these platforms offer useful features such as reward modeling and RLHF fine-tuning, their security and reliability remain largely unexplored. Given the growing adoption of RLHF and open-source RLHF frameworks, we investigate the trustworthiness of these systems and their potential impact on behavior of LLMs. In this paper, we present an attack targeting publicly available RLHF tools. In our proposed attack, an adversarial RLHF platform corrupts the LLM alignment process by selectively manipulating data samples in the preference dataset. In this scenario, when a user's task aligns with the attacker's objective, the platform manipulates a subset of the preference dataset that contains samples related to the attacker's target. This manipulation results in a corrupted reward model, which ultimately leads to the misalignment of the language model. Our results demonstrate that such an attack can effectively steer LLMs toward undesirable behaviors within the targeted domains. Our work highlights the critical need to explore the vulnerabilities of RLHF platforms and their potential to cause misalignment in LLMs during the RLHF fine-tuning process.", 'abstract_zh': '强化学习在使语言模型与人类偏好对齐方面展现了显著性能，推动了RLHF平台的发展关注。这些平台使用户能够在无需复杂机器学习算法开发专业知识的情况下对模型进行细调。尽管这些平台提供诸如奖励建模和RLHF细调等有用功能，但它们的安全性和可靠性仍鲜有研究。鉴于RLHF及其开源框架的日益普及，我们研究了这些系统的可信度及其对语言模型行为的影响。在本文中，我们提出了一个针对公开可用的RLHF工具的攻击。在我们提出的攻击中，一个 adversarial RLHF 平台通过有选择地操纵偏好数据集中的数据样本来破坏语言模型的对齐过程。在这种情况下，当用户的任务与攻击者的目标相一致时，平台会操纵包含与攻击者目标相关的样本的偏好数据集的一部分。这种操纵导致了被篡改的奖励模型，最终导致语言模型的对齐偏离。我们的结果表明，这种攻击可以在目标领域有效地引导语言模型表现出不良行为。我们的工作突显了探索RLHF平台漏洞以及它们在RLHF细调过程中导致语言模型对齐偏离的可能性的紧迫性。', 'title_zh': 'LLM对齐偏差通过对抗性RLHF平台'}
{'arxiv_id': 'arXiv:2503.03008', 'title': 'One Model to Train them All: Hierarchical Self-Distillation for Enhanced Early Layer Embeddings', 'authors': 'Andrea Gurioli, Federico Pennino, João Monteiro, Maurizio Gabbrielli', 'link': 'https://arxiv.org/abs/2503.03008', 'abstract': "Deploying language models often requires handling model size vs. performance trade-offs to satisfy downstream latency constraints while preserving the model's usefulness. Model distillation is commonly employed to reduce model size while maintaining acceptable performance. However, distillation can be inefficient since it involves multiple training steps. In this work, we introduce MODULARSTARENCODER, a modular multi-exit encoder with 1B parameters, useful for multiple tasks within the scope of code retrieval. MODULARSTARENCODER is trained with a novel self-distillation mechanism that significantly improves lower-layer representations-allowing different portions of the model to be used while still maintaining a good trade-off in terms of performance. Our architecture focuses on enhancing text-to-code and code-to-code search by systematically capturing syntactic and semantic structures across multiple levels of representation. Specific encoder layers are targeted as exit heads, allowing higher layers to guide earlier layers during training. This self-distillation effect improves intermediate representations, increasing retrieval recall at no extra training cost. In addition to the multi-exit scheme, our approach integrates a repository-level contextual loss that maximally utilizes the training context window, further enhancing the learned representations. We also release a new dataset constructed via code translation, seamlessly expanding traditional text-to-code benchmarks with code-to-code pairs across diverse programming languages. Experimental results highlight the benefits of self-distillation through multi-exit supervision.", 'abstract_zh': '使用模块化多出口编码器在代码检索范围内实现多种任务时，平衡模型规模与性能的trade-offs以满足下游延迟约束并在保留模型效用的前提下，是一个常见的要求。模型蒸馏常被用来在保持可接受性能的同时减小模型规模。然而，蒸馏可能由于需要多步训练而耗时。在这项工作中，我们提出了MODULARSTARENCODER，这是一种具有1亿参数的模块化多出口编码器，适用于代码检索范围内的多种任务。我们通过一种新颖的自我蒸馏机制训练MODULARSTARENCODER，显著改进了下层表示，使模型的不同部分能够被使用，同时在性能方面保持良好的trade-off。我们架构的重点在于通过系统地捕捉多级表示中的语法和语义结构来增强从文本到代码和从代码到代码的搜索。特定编码器层被指定为出口头，允许较高层在训练中引导较低层。这种自我蒸馏效果增强了中间表示，提高了检索召回率且无需额外的训练成本。除了多出口方案，我们的方法整合了仓库级上下文损失，最大限度地利用训练上下文窗口，进一步增强学习到的表示。我们还发布了一个通过代码翻译构建的新数据集，无缝地将跨不同编程语言的代码到代码对扩展到传统文本到代码基准测试中。实验结果突出了多出口监督通过自我蒸馏带来的益处。', 'title_zh': '一种模型训练它们全部的方法：分层自我精炼以增强早期层嵌入'}
{'arxiv_id': 'arXiv:2503.02989', 'title': 'Effectively Steer LLM To Follow Preference via Building Confident Directions', 'authors': 'Bingqing Song, Boran Han, Shuai Zhang, Hao Wang, Haoyang Fang, Bonan Min, Yuyang Wang, Mingyi Hong', 'link': 'https://arxiv.org/abs/2503.02989', 'abstract': "Having an LLM that aligns with human preferences is essential for accommodating individual needs, such as maintaining writing style or generating specific topics of interest. The majority of current alignment methods rely on fine-tuning or prompting, which can be either costly or difficult to control. Model steering algorithms, which modify the model output by constructing specific steering directions, are typically easy to implement and optimization-free. However, their capabilities are typically limited to steering the model into one of the two directions (i.e., bidirectional steering), and there has been no theoretical understanding to guarantee their performance. In this work, we propose a theoretical framework to understand and quantify the model steering methods. Inspired by the framework, we propose a confident direction steering method (CONFST) that steers LLMs via modifying their activations at inference time. More specifically, CONFST builds a confident direction that is closely aligned with users' preferences, and this direction is then added to the activations of the LLMs to effectively steer the model output. Our approach offers three key advantages over popular bidirectional model steering methods: 1) It is more powerful, since multiple (i.e. more than two) users' preferences can be aligned simultaneously; 2) It is simple to implement, since there is no need to determine which layer to add the steering vector to; 3) No explicit user instruction is required. We validate our method on GPT-2 XL (1.5B), Mistral (7B) and Gemma-it (9B) models for tasks that require shifting the output of LLMs across various topics and styles, achieving superior performance over competing methods.", 'abstract_zh': '具有与人类偏好一致的LLM对于满足个性化需求至关重要，例如保持writing style或生成特定感兴趣的主题。当前大多数对齐方法依赖于微调或提示，这可能是代价高昂的或难以控制。通过修改模型输出的模型偏向算法通常易于实现且无需优化。然而，它们的能力通常仅限于将模型偏向两个方向之一（即双向偏向），并且尚无理论理解以保证其性能。在本文中，我们提出了一个理论框架，以理解并量化模型偏向方法。受该框架的启发，我们提出了一种自信方向偏向方法（CONFST），该方法通过修改LLM推理时的激活值来偏向LLM。具体而言，CONFST构建了一个紧密与用户偏好一致的自信方向，并将该方向添加到LLM的激活值中，以有效偏向模型输出。我们的方法与流行的双向模型偏向方法相比具有三个关键优势：1）更为强大，因为它可以同时对齐多个（即超过两个）用户偏好；2）易于实现，因为无需确定添加偏向向量的层；3）无需显式用户指令。我们在GPT-2 XL（1.5B）、Mistral（7B）和Gemma-it（9B）模型上进行了验证，这些模型用于在不同主题和风格下调整LLM的输出任务，结果性能优于竞争对手的方法。', 'title_zh': '通过构建自信方向有效地引导LLM遵循偏好'}
{'arxiv_id': 'arXiv:2503.02972', 'title': 'LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation', 'authors': 'Jude Khouja, Karolina Korgul, Simi Hellsten, Lingyi Yang, Vlad Neacs, Harry Mayne, Ryan Kearns, Andrew Bean, Adam Mahdi', 'link': 'https://arxiv.org/abs/2503.02972', 'abstract': 'Effective evaluation of the reasoning capabilities of large language models (LLMs) are susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging evaluation benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerous question variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including OpenAI o1-preview and DeepSeem R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to overestimating the reasoning capabilities of frontier models.', 'abstract_zh': '有效的评估大规模语言模型的推理能力受到评估基准数据泄露的影响，可能导致高估。我们提出了一种生成语言推理问题的框架，以减少模型性能估计中的记忆效应，并应用该框架开发了LINGOLY-TOO，一个具有挑战性的语言推理评估基准。通过开发书写系统模板，我们动态地模糊真实语言的书写系统，生成大量问题变体。这些变体保留了解决每个问题所需的推理步骤，同时减少了特定问题实例出现在模型训练数据中的可能性。我们的实验表明，前沿模型，包括OpenAI o1-preview和DeepSeem R1，在高级推理方面存在困难。我们的分析还表明，大规模语言模型在相同问题的不同排列下表现出明显的准确率差异，并且通常在原始书写系统的问题上表现更好。我们的研究结果突显了大规模语言模型生成响应的不透明性质，并提供了先前数据暴露可能导致高估前沿模型推理能力的证据。', 'title_zh': 'LINGOLY-TOO: 语言模板化与拼写混淆分离记忆与推理'}
{'arxiv_id': 'arXiv:2503.02969', 'title': 'InfiniSST: Simultaneous Translation of Unbounded Speech with Large Language Model', 'authors': 'Siqi Ouyang, Xi Xu, Lei Li', 'link': 'https://arxiv.org/abs/2503.02969', 'abstract': 'Simultaneous translation of unbounded streaming speech remains a challenging problem due to the need for effectively processing the history speech context and past translations so that quality and latency, including computation overhead, can be balanced. Most prior works assume pre-segmented speech, limiting their real-world applicability. In this paper, we propose InfiniSST, a novel approach that formulates SST as a multi-turn dialogue task, enabling seamless translation of unbounded speech. We construct translation trajectories and robust segments from MuST-C with multi-latency augmentation during training and develop a key-value (KV) cache management strategy to facilitate efficient inference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that InfiniSST reduces computation-aware latency by 0.5 to 1 second while maintaining the same translation quality compared to baselines. Ablation studies further validate the contributions of our data construction and cache management strategy. We release the code at this https URL', 'abstract_zh': '无限流语音的同步翻译由于需要有效处理历史语音上下文和过去翻译，以平衡质量和延迟（包括计算开销）仍然是一个挑战性问题。大多数先前工作假设分割好的语音，限制了其实用性。在本文中，我们提出了一种新颖的方法InfiniSST，将SST形式化为多轮对话任务，从而实现无缝翻译无限流语音。我们在训练过程中使用多延迟增强构建翻译轨迹和鲁棒片段，并开发了一种键值（KV）缓存管理策略以促进高效推理。实验结果表明，InfiniSST在保持与 baseline 相同翻译质量的同时，降低了0.5到1秒的计算感知延迟。消融研究进一步验证了我们数据构建和缓存管理策略的贡献。我们在该网址发布代码：this https URL。', 'title_zh': 'InfiniSST：使用大型语言模型的同时无界口语翻译'}
{'arxiv_id': 'arXiv:2503.02911', 'title': 'Text2Scenario: Text-Driven Scenario Generation for Autonomous Driving Test', 'authors': 'Xuan Cai, Xuesong Bai, Zhiyong Cui, Danmu Xie, Daocheng Fu, Haiyang Yu, Yilong Ren', 'link': 'https://arxiv.org/abs/2503.02911', 'abstract': "Autonomous driving (AD) testing constitutes a critical methodology for assessing performance benchmarks prior to product deployment. The creation of segmented scenarios within a simulated environment is acknowledged as a robust and effective strategy; however, the process of tailoring these scenarios often necessitates laborious and time-consuming manual efforts, thereby hindering the development and implementation of AD technologies. In response to this challenge, we introduce Text2Scenario, a framework that leverages a Large Language Model (LLM) to autonomously generate simulation test scenarios that closely align with user specifications, derived from their natural language inputs. Specifically, an LLM, equipped with a meticulously engineered input prompt scheme functions as a text parser for test scenario descriptions, extracting from a hierarchically organized scenario repository the components that most accurately reflect the user's preferences. Subsequently, by exploiting the precedence of scenario components, the process involves sequentially matching and linking scenario representations within a Domain Specific Language corpus, ultimately fabricating executable test scenarios. The experimental results demonstrate that such prompt engineering can meticulously extract the nuanced details of scenario elements embedded within various descriptive formats, with the majority of generated scenarios aligning closely with the user's initial expectations, allowing for the efficient and precise evaluation of diverse AD stacks void of the labor-intensive need for manual scenario configuration. Project page: this https URL.", 'abstract_zh': '自主驾驶（AD）测试构成了一种关键方法，用于在产品部署前评估性能基准。在模拟环境中创建分段场景被认可为一种 robust 和有效的策略；然而，这些场景的定制过程往往需要大量的手动劳动，从而阻碍了 AD 技术的发展与实施。针对这一挑战，我们引入了 Text2Scenario 框架，该框架利用大型语言模型（LLM）自动生成与用户自然语言输入高度一致的模拟测试场景。具体而言，一个配备有精心设计的输入提示方案的 LLM 作为测试场景描述的解析器，从层次化组织的场景库中提取最能反映用户偏好的组件。随后，通过利用场景组件的优先级，过程涉及顺序匹配和链接领域特定语言语料中的场景表示，最终生成可执行的测试场景。实验结果表明，这种提示工程可以从各种描述格式中一丝不苟地提取场景元素的细微之处，所生成的大部分场景与用户的初始期望高度一致，从而允许高效且精确地评估多种 AD 堆栈，而无需进行劳动密集型的手动场景配置。', 'title_zh': '文本2场景：基于文本的自动驾驶测试场景生成'}
