{'arxiv_id': 'arXiv:2503.03734', 'title': 'OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction', 'authors': 'Huang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam, Jitendra Malik, Ken Goldberg, Pieter Abbeel', 'link': 'https://arxiv.org/abs/2503.03734', 'abstract': 'Vision-Language-Action (VLA) models aim to predict robotic actions based on visual observations and language instructions. Existing approaches require fine-tuning pre-trained visionlanguage models (VLMs) as visual and language features are independently fed into downstream policies, degrading the pre-trained semantic alignments. We propose OTTER, a novel VLA architecture that leverages these existing alignments through explicit, text-aware visual feature extraction. Instead of processing all visual features, OTTER selectively extracts and passes only task-relevant visual features that are semantically aligned with the language instruction to the policy transformer. This allows OTTER to keep the pre-trained vision-language encoders frozen. Thereby, OTTER preserves and utilizes the rich semantic understanding learned from large-scale pre-training, enabling strong zero-shot generalization capabilities. In simulation and real-world experiments, OTTER significantly outperforms existing VLA models, demonstrating strong zeroshot generalization to novel objects and environments. Video, code, checkpoints, and dataset: this https URL.', 'abstract_zh': 'Vision-Language-Action (VLA)模型旨在基于视觉观察和语言指令预测机器人行动。我们提出了OTTER，一种新颖的VLA架构，通过显式的、文本意识的视觉特征提取利用这些现有的语义对齐。OTTER仅选择性地提取并与语言指令语义上对齐的任务相关视觉特征，并将其传递给策略变压器，从而使预训练的视觉-语言编码器保持冻结状态。因此，OTTER保留并利用了大规模预训练中学习到的丰富语义理解，实现了强大的零样本泛化能力。在仿真和现实世界实验中，OTTER显著优于现有VLA模型，展示出强大的零样本泛化能力以应对新的对象和环境。视频、代码、检查点和数据集：this https URL。', 'title_zh': 'OTTER：一种具有文本意识视觉特征提取的多模态动作模型'}
{'arxiv_id': 'arXiv:2503.03629', 'title': 'TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles through Generative Simulation', 'authors': 'Haowei Sun, Xintao Yan, Zhijie Qiao, Haojie Zhu, Yihao Sun, Jiawei Wang, Shengyin Shen, Darian Hogue, Rajanikant Ananta, Derek Johnson, Greg Stevens, Greg McGuire, Yifan Wei, Wei Zheng, Yong Sun, Yasuo Fukai, Henry X. Liu', 'link': 'https://arxiv.org/abs/2503.03629', 'abstract': "Traffic simulation is essential for autonomous vehicle (AV) development, enabling comprehensive safety evaluation across diverse driving conditions. However, traditional rule-based simulators struggle to capture complex human interactions, while data-driven approaches often fail to maintain long-term behavioral realism or generate diverse safety-critical events. To address these challenges, we propose TeraSim, an open-source, high-fidelity traffic simulation platform designed to uncover unknown unsafe events and efficiently estimate AV statistical performance metrics, such as crash rates. TeraSim is designed for seamless integration with third-party physics simulators and standalone AV stacks, to construct a complete AV simulation system. Experimental results demonstrate its effectiveness in generating diverse safety-critical events involving both static and dynamic agents, identifying hidden deficiencies in AV systems, and enabling statistical performance evaluation. These findings highlight TeraSim's potential as a practical tool for AV safety assessment, benefiting researchers, developers, and policymakers. The code is available at this https URL.", 'abstract_zh': '交通仿真对于自动驾驶汽车（AV）的发展至关重要，能够全面评估在各种驾驶条件下的一致安全性。然而，传统的基于规则的仿真器难以捕捉复杂的人类交互，而基于数据的方法往往无法保持长期的行为现实性或生成多样的安全关键事件。为了解决这些挑战，我们提出TeraSim，这是一个开源的高保真交通仿真平台，旨在揭示未知的安全事件并高效估计AV的统计性能指标，如碰撞率。TeraSim设计用于与第三方物理仿真器和独立的AV堆栈无缝集成，以构建完整的AV仿真系统。实验结果表明，TeraSim在生成涉及静态和动态代理的安全关键事件方面具有多样性，能够识别AV系统中的隐藏缺陷，并提供统计性能评估。这些发现突显了TeraSim作为AV安全性评估实用工具的潜力，有利于研究人员、开发者和政策制定者。代码可在以下链接获得：this https URL。', 'title_zh': 'TeraSim: 通过生成性模拟发现自主车辆中的未知不安全事件'}
{'arxiv_id': 'arXiv:2503.03579', 'title': 'A Generative System for Robot-to-Human Handovers: from Intent Inference to Spatial Configuration Imagery', 'authors': 'Hanxin Zhang, Abdulqader Dhafer, Zhou Daniel Hao, Hongbiao Dong', 'link': 'https://arxiv.org/abs/2503.03579', 'abstract': "We propose a novel system for robot-to-human object handover that emulates human coworker interactions. Unlike most existing studies that focus primarily on grasping strategies and motion planning, our system focus on 1. inferring human handover intents, 2. imagining spatial handover configuration. The first one integrates multimodal perception-combining visual and verbal cues-to infer human intent. The second one using a diffusion-based model to generate the handover configuration, involving the spacial relationship among robot's gripper, the object, and the human hand, thereby mimicking the cognitive process of motor imagery. Experimental results demonstrate that our approach effectively interprets human cues and achieves fluent, human-like handovers, offering a promising solution for collaborative robotics. Code, videos, and data are available at: this https URL.", 'abstract_zh': '我们提出了一种新型的机器人向人类对象交接系统，模拟人类同事间的互动。与大多数现有研究主要集中在抓取策略和运动规划上不同，我们的系统侧重于1. 推断人类交接意图，2. 想象空间交接配置。前者结合多模态感知（结合视觉和语言提示）推断人类意图。后者使用基于扩散的模型生成交接配置，涉及机器人夹爪、物体和人类手之间的空间关系，从而模仿运动意象的认知过程。实验结果表明，我们的方法有效解读人类提示，实现流畅、类人的交接，为协作机器人提供了有前景的解决方案。代码、视频和数据可在以下链接获取：this https URL。', 'title_zh': '基于从意图推理到空间配置想象的机器人到人的物品传递生成系统'}
{'arxiv_id': 'arXiv:2503.03574', 'title': 'Olympus: A Jumping Quadruped for Planetary Exploration Utilizing Reinforcement Learning for In-Flight Attitude Control', 'authors': 'Jørgen Anker Olsen, Grzegorz Malczyk, Kostas Alexis', 'link': 'https://arxiv.org/abs/2503.03574', 'abstract': 'Exploring planetary bodies with lower gravity, such as the moon and Mars, allows legged robots to utilize jumping as an efficient form of locomotion thus giving them a valuable advantage over traditional rovers for exploration. Motivated by this fact, this paper presents the design, simulation, and learning-based "in-flight" attitude control of Olympus, a jumping legged robot tailored to the gravity of Mars. First, the design requirements are outlined followed by detailing how simulation enabled optimizing the robot\'s design - from its legs to the overall configuration - towards high vertical jumping, forward jumping distance, and in-flight attitude reorientation. Subsequently, the reinforcement learning policy used to track desired in-flight attitude maneuvers is presented. Successfully crossing the sim2real gap, extensive experimental studies of attitude reorientation tests are demonstrated.', 'abstract_zh': '探索低重力行星体，如月球和火星，使腿足机器人能够利用跳跃作为高效的移动方式，从而在探索方面给传统漫游车带来显著优势。基于这一事实，本文介绍了为适应火星重力定制的跳跃腿足机器人奥林匹斯（Olympus）的设计、仿真及基于学习的“飞行中”姿态控制。首先概述了设计要求，随后详细说明了通过仿真优化机器人从腿部到整体配置的设计，以实现高效的垂直跳跃、前向跳跃距离和空中姿态重定向。接着介绍了用于跟踪期望空中姿态机动的强化学习策略。成功跨越了仿真到现实的鸿沟，本文展示了大量的姿态重定向测试实验研究。', 'title_zh': '奥林帕斯：利用强化学习进行飞行姿态控制的行星探测四足跳跃机器人'}
{'arxiv_id': 'arXiv:2503.03480', 'title': 'SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning', 'authors': 'Borong Zhang, Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen, Yaodong Yang', 'link': 'https://arxiv.org/abs/2503.03480', 'abstract': 'Vision-language-action models (VLAs) have shown great potential as generalist robot policies. However, these models pose urgent safety challenges during deployment, including the risk of physical harm to the environment, the robot itself, and humans. How can safety be explicitly incorporated into VLAs? In this work, we propose SafeVLA, a novel algorithm designed to integrate safety into VLAs, ensuring the protection of the environment, robot hardware and humans in real-world settings. SafeVLA effectively balances safety and task performance by employing large-scale constrained learning within simulated environments. We demonstrate that SafeVLA outperforms the current state-of-the-art method in both safety and task performance, achieving average improvements of 83.58% and 3.85%, respectively, in simulation. By prioritizing safety, our approach eliminates high-risk behaviors and reduces the upper bound of unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby significantly mitigating long-tail risks. Furthermore, the learned safety constraints generalize to diverse, unseen scenarios, including multiple out-of-distribution perturbations and tasks. Our data, models and newly proposed benchmark environment are available at this https URL.', 'abstract_zh': '视觉-语言-动作模型（VLAs）作为通用机器人策略展现出巨大的潜力。然而，在部署过程中，这些模型面临着迫切的安全挑战，包括对环境、机器人本身和人类的物理伤害风险。如何在VLAs中明确地纳入安全性？在本工作中，我们提出了SafeVLA，这是一种新颖的算法，旨在将安全性集成到VLAs中，确保在实际环境中的环境、机器人硬件和人类的安全。SafeVLA通过在模拟环境中采用大规模约束学习有效平衡了安全性和任务性能。实验结果表明，SafeVLA在安全性和任务性能方面均优于当前最先进的方法，在模拟实验中的安全性平均提升了83.58%，任务性能提升了3.85%。通过优先考虑安全，我们的方法消除了高风险行为，并将不可安全行为的上限减少了至当前最先进的方法的1/35，从而显著降低了长尾风险。此外，学习到的安全约束在多种未见过的场景中具有泛化能力，包括多种离分布扰动和任务。我们的数据、模型和新提出的基准环境可在以下链接获取。', 'title_zh': 'SafeVLA：通过安全强化学习实现视觉-语言-行动模型的安全对齐'}
{'arxiv_id': 'arXiv:2503.03476', 'title': 'Continuous Control of Diverse Skills in Quadruped Robots Without Complete Expert Datasets', 'authors': 'Jiaxin Tu, Xiaoyi Wei, Yueqi Zhang, Taixian Hou, Xiaofei Gao, Zhiyan Dong, Peng Zhai, Lihua Zhang', 'link': 'https://arxiv.org/abs/2503.03476', 'abstract': 'Learning diverse skills for quadruped robots presents significant challenges, such as mastering complex transitions between different skills and handling tasks of varying difficulty. Existing imitation learning methods, while successful, rely on expensive datasets to reproduce expert behaviors. Inspired by introspective learning, we propose Progressive Adversarial Self-Imitation Skill Transition (PASIST), a novel method that eliminates the need for complete expert datasets. PASIST autonomously explores and selects high-quality trajectories based on predefined target poses instead of demonstrations, leveraging the Generative Adversarial Self-Imitation Learning (GASIL) framework. To further enhance learning, We develop a skill selection module to mitigate mode collapse by balancing the weights of skills with varying levels of difficulty. Through these methods, PASIST is able to reproduce skills corresponding to the target pose while achieving smooth and natural transitions between them. Evaluations on both simulation platforms and the Solo 8 robot confirm the effectiveness of PASIST, offering an efficient alternative to expert-driven learning.', 'abstract_zh': '渐进对抗自我模仿技能过渡（PASIST）：面向四足机器人的高效技能学习', 'title_zh': '四足机器人无需完整专家数据集的多样技能连续控制'}
{'arxiv_id': 'arXiv:2503.03464', 'title': 'Generative Artificial Intelligence in Robotic Manipulation: A Survey', 'authors': 'Kun Zhang, Peng Yun, Jun Cen, Junhao Cai, Didi Zhu, Hangjie Yuan, Chao Zhao, Tao Feng, Michael Yu Wang, Qifeng Chen, Jia Pan, Bo Yang, Hua Chen', 'link': 'https://arxiv.org/abs/2503.03464', 'abstract': 'This survey provides a comprehensive review on recent advancements of generative learning models in robotic manipulation, addressing key challenges in the field. Robotic manipulation faces critical bottlenecks, including significant challenges in insufficient data and inefficient data acquisition, long-horizon and complex task planning, and the multi-modality reasoning ability for robust policy learning performance across diverse environments. To tackle these challenges, this survey introduces several generative model paradigms, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), diffusion models, probabilistic flow models, and autoregressive models, highlighting their strengths and limitations. The applications of these models are categorized into three hierarchical layers: the Foundation Layer, focusing on data generation and reward generation; the Intermediate Layer, covering language, code, visual, and state generation; and the Policy Layer, emphasizing grasp generation and trajectory generation. Each layer is explored in detail, along with notable works that have advanced the state of the art. Finally, the survey outlines future research directions and challenges, emphasizing the need for improved efficiency in data utilization, better handling of long-horizon tasks, and enhanced generalization across diverse robotic scenarios. All the related resources, including research papers, open-source data, and projects, are collected for the community in this https URL', 'abstract_zh': '本调研提供了对机器人操作中生成学习模型近期进展的全面回顾，针对该领域的关键挑战进行了探讨。机器人操作面临的关键瓶颈包括数据不足和数据采集效率低、长时序和复杂任务规划，以及适用于多样化环境的鲁棒性策略学习所需的多模态推理能力。为应对这些挑战，本调研介绍了几种生成模型范式，包括生成对抗网络（GANs）、变分自编码器（VAEs）、扩散模型、概率流模型和自回归模型，并对它们的优缺点进行了阐述。这些模型的应用被分类为三个层次：基础层，侧重于数据生成和奖励生成；中间层，涵盖语言、代码、视觉和状态生成；策略层，强调抓取生成和轨迹生成。每个层次都进行了详细探讨，并介绍了推动该领域前沿的重要工作。最后，本调研指出了未来的研究方向和挑战，强调了提高数据利用效率、更好地处理长时序任务以及增强跨多样化机器人场景的应用前景的需要。相关资源，包括研究论文、开源数据和项目，均收集于此：https://xxxxxx', 'title_zh': '生成式人工智能在机器人操作中的应用：综述'}
{'arxiv_id': 'arXiv:2503.03234', 'title': 'Social Gesture Recognition in spHRI: Leveraging Fabric-Based Tactile Sensing on Humanoid Robots', 'authors': 'Dakarai Crowder, Kojo Vandyck, Xiping Sun, James McCann, Wenzhen Yuan', 'link': 'https://arxiv.org/abs/2503.03234', 'abstract': 'Humans are able to convey different messages using only touch. Equipping robots with the ability to understand social touch adds another modality in which humans and robots can communicate. In this paper, we present a social gesture recognition system using a fabric-based, large-scale tactile sensor integrated onto the arms of a humanoid robot. We built a social gesture dataset using multiple participants and extracted temporal features for classification. By collecting real-world data on a humanoid robot, our system provides valuable insights into human-robot social touch, further advancing the development of spHRI systems for more natural and effective communication.', 'abstract_zh': '基于纺织材料的大规模触觉传感器的人形机器人社会手势识别系统', 'title_zh': '基于 humanoid 机器人的基于织物基触觉感知的社会手势识别'}
{'arxiv_id': 'arXiv:2503.03208', 'title': 'Embodied Escaping: End-to-End Reinforcement Learning for Robot Navigation in Narrow Environment', 'authors': 'Han Zheng, Jiale Zhang, Mingyang Jiang, Peiyuan Liu, Danni Liu, Tong Qin, Ming Yang', 'link': 'https://arxiv.org/abs/2503.03208', 'abstract': 'Autonomous navigation is a fundamental task for robot vacuum cleaners in indoor environments. Since their core function is to clean entire areas, robots inevitably encounter dead zones in cluttered and narrow scenarios. Existing planning methods often fail to escape due to complex environmental constraints, high-dimensional search spaces, and high difficulty maneuvers. To address these challenges, this paper proposes an embodied escaping model that leverages reinforcement learning-based policy with an efficient action mask for dead zone escaping. To alleviate the issue of the sparse reward in training, we introduce a hybrid training policy that improves learning efficiency. In handling redundant and ineffective action options, we design a novel action representation to reshape the discrete action space with a uniform turning radius. Furthermore, we develop an action mask strategy to select valid action quickly, balancing precision and efficiency. In real-world experiments, our robot is equipped with a Lidar, IMU, and two-wheel encoders. Extensive quantitative and qualitative experiments across varying difficulty levels demonstrate that our robot can consistently escape from challenging dead zones. Moreover, our approach significantly outperforms compared path planning and reinforcement learning methods in terms of success rate and collision avoidance.', 'abstract_zh': '自主导航是室内清扫机器人的一项基本任务。由于核心功能是全面清洁区域，机器人在杂乱和狭窄的场景中不可避免地会遇到死角。现有规划方法往往由于复杂的环境约束、高维搜索空间以及高难度的操作而难以逃脱。为了解决这些问题，本文提出了一种基于强化学习的体态逃脱模型，该模型利用高效的行动掩码来解决死角逃脱问题。为了解决训练中稀疏奖励的问题，我们引入了一种混合训练策略，以提高学习效率。在处理冗余和无效的动作选项时，我们设计了一种新型的动作表示，以均匀转向半径重塑离散的动作空间。此外，我们开发了一种行动掩码策略，以快速选择有效动作，平衡精度和效率。在实际实验中，我们的机器人配备了激光雷达、IMU和双轮编码器。在不同难度级别的广泛定量和定性实验中，证明我们的机器人能够一致地从挑战性的死角中逃脱。此外，与路径规划和强化学习方法相比，我们的方法在成功率和碰撞避免方面表现显著优异。', 'title_zh': '具身逃脱：狭窄环境中的机器人导航端到端强化学习'}
{'arxiv_id': 'arXiv:2503.03145', 'title': 'Causality-Based Reinforcement Learning Method for Multi-Stage Robotic Tasks', 'authors': 'Jiechao Deng, Ning Tan', 'link': 'https://arxiv.org/abs/2503.03145', 'abstract': 'Deep reinforcement learning has made significant strides in various robotic tasks. However, employing deep reinforcement learning methods to tackle multi-stage tasks still a challenge. Reinforcement learning algorithms often encounter issues such as redundant exploration, getting stuck in dead ends, and progress reversal in multi-stage tasks. To address this, we propose a method that integrates causal relationships with reinforcement learning for multi-stage tasks. Our approach enables robots to automatically discover the causal relationships between their actions and the rewards of the tasks and constructs the action space using only causal actions, thereby reducing redundant exploration and progress reversal. By integrating correct causal relationships using the causal policy gradient method into the learning process, our approach can enhance the performance of reinforcement learning algorithms in multi-stage robotic tasks.', 'abstract_zh': '深度强化学习在各类机器人任务中取得了显著进展，但在应对多阶段任务时仍面临挑战。为了应对这一挑战，我们提出了一种结合因果关系与强化学习的方法，以适用于多阶段任务。该方法使机器人能够自动发现其动作与任务奖励之间的因果关系，并仅使用因果动作构建动作空间，从而减少冗余探索和进展逆转。通过使用因果策略梯度方法将正确的因果关系融入学习过程，该方法能够提升强化学习算法在多阶段机器人任务中的性能。', 'title_zh': '基于因果性的强化学习方法用于多阶段机器人任务'}
{'arxiv_id': 'arXiv:2503.03125', 'title': "Don't Shake the Wheel: Momentum-Aware Planning in End-to-End Autonomous Driving", 'authors': 'Ziying Song, Caiyan Jia, Lin Liu, Hongyu Pan, Yongchang Zhang, Junming Wang, Xingyu Zhang, Shaoqing Xu, Lei Yang, Yadan Luo', 'link': 'https://arxiv.org/abs/2503.03125', 'abstract': 'End-to-end autonomous driving frameworks enable seamless integration of perception and planning but often rely on one-shot trajectory prediction, which may lead to unstable control and vulnerability to occlusions in single-frame perception. To address this, we propose the Momentum-Aware Driving (MomAD) framework, which introduces trajectory momentum and perception momentum to stabilize and refine trajectory predictions. MomAD comprises two core components: (1) Topological Trajectory Matching (TTM) employs Hausdorff Distance to select the optimal planning query that aligns with prior paths to ensure coherence;(2) Momentum Planning Interactor (MPI) cross-attends the selected planning query with historical queries to expand static and dynamic perception files. This enriched query, in turn, helps regenerate long-horizon trajectory and reduce collision risks. To mitigate noise arising from dynamic environments and detection errors, we introduce robust instance denoising during training, enabling the planning model to focus on critical signals and improve its robustness. We also propose a novel Trajectory Prediction Consistency (TPC) metric to quantitatively assess planning stability. Experiments on the nuScenes dataset demonstrate that MomAD achieves superior long-term consistency (>=3s) compared to SOTA methods. Moreover, evaluations on the curated Turning-nuScenes shows that MomAD reduces the collision rate by 26% and improves TPC by 0.97m (33.45%) over a 6s prediction horizon, while closedloop on Bench2Drive demonstrates an up to 16.3% improvement in success rate.', 'abstract_zh': '基于动量的自动驾驶框架（Momentum-Aware Driving, MomAD）：稳定化轨迹预测与规划', 'title_zh': '不要摇晃方向盘：端到端自动驾驶中的动量感知规划'}
{'arxiv_id': 'arXiv:2503.03100', 'title': 'Car-STAGE: Automated framework for large-scale high-dimensional simulated time-series data generation based on user-defined criteria', 'authors': 'Asma A. Almutairi, David J. LeBlanc, Arpan Kusari', 'link': 'https://arxiv.org/abs/2503.03100', 'abstract': 'Generating large-scale sensing datasets through photo-realistic simulation is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of synchronous data collection from the open-source CARLA simulator using multiple sensors attached to vehicle based on user-defined criteria. We propose a novel, one-step framework that we refer to as Car-STAGE, based on CARLA simulator, to generate data using a graphical user interface (GUI) defining configuration parameters to data collection without any user intervention. This framework can utilize the user-defined configuration parameters such as choice of maps, number and configurations of sensors, environmental and lighting conditions etc. to run the simulation in the background, collecting high-dimensional sensor data from diverse sensors such as RGB Camera, LiDAR, Radar, Depth Camera, IMU Sensor, GNSS Sensor, Semantic Segmentation Camera, Instance Segmentation Camera, and Optical Flow Camera along with the ground-truths of the individual actors and storing the sensor data as well as ground-truth labels in a local or cloud-based database. The framework uses multiple threads where a main thread runs the server, a worker thread deals with queue and frame number and the rest of the threads processes the sensor data. The other way we derive speed up over the native implementation is by memory mapping the raw binary data into the disk and then converting the data into known formats at the end of data collection. We show that using these techniques, we gain a significant speed up over frames, under an increasing set of sensors and over the number of spawned objects.', 'abstract_zh': '基于CARLA模拟器的用户定义参数驱动的一键式数据采集框架：Car-STAGE', 'title_zh': '基于用户定义标准的大型高维模拟时间序列数据生成自动化框架：Car-STAGE'}
{'arxiv_id': 'arXiv:2503.03045', 'title': 'ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation', 'authors': 'Yufei Wang, Ziyu Wang, Mino Nakura, Pratik Bhowal, Chia-Liang Kuo, Yi-Ting Chen, Zackory Erickson, David Held', 'link': 'https://arxiv.org/abs/2503.03045', 'abstract': 'This paper presents ArticuBot, in which a single learned policy enables a robotics system to open diverse categories of unseen articulated objects in the real world. This task has long been challenging for robotics due to the large variations in the geometry, size, and articulation types of such objects. Our system, Articubot, consists of three parts: generating a large number of demonstrations in physics-based simulation, distilling all generated demonstrations into a point cloud-based neural policy via imitation learning, and performing zero-shot sim2real transfer to real robotics systems. Utilizing sampling-based grasping and motion planning, our demonstration generalization pipeline is fast and effective, generating a total of 42.3k demonstrations over 322 training articulated objects. For policy learning, we propose a novel hierarchical policy representation, in which the high-level policy learns the sub-goal for the end-effector, and the low-level policy learns how to move the end-effector conditioned on the predicted goal. We demonstrate that this hierarchical approach achieves much better object-level generalization compared to the non-hierarchical version. We further propose a novel weighted displacement model for the high-level policy that grounds the prediction into the existing 3D structure of the scene, outperforming alternative policy representations. We show that our learned policy can zero-shot transfer to three different real robot settings: a fixed table-top Franka arm across two different labs, and an X-Arm on a mobile base, opening multiple unseen articulated objects across two labs, real lounges, and kitchens. Videos and code can be found on our project website: this https URL.', 'abstract_zh': '本文介绍了ArticuBot，该系统通过单个学习策略使机器人系统能够在现实世界中打开各类未见过的关节式物体。由于此类物体在几何形状、大小和关节类型上存在巨大变化，这一任务长期以來对机器人技术构成了挑战。我们的系统Articubot 包括三个部分：在基于物理的模拟中生成大量演示，通过模仿学习将所有生成的演示总结为基于点云的神经策略，以及在真实机器人系统中进行零样本仿真实验到现实的转移。利用基于采样的抓取和运动规划，我们的演示泛化管道既快速又高效，在322个训练关节物体上总共生成了42300个演示。在策略学习中，我们提出了一种新颖的层次化策略表示，在该表示中，高层策略学习末端执行器的子目标，而低层策略学习在预测目标的基础上如何移动末端执行器。我们证明这种层次化方法在物体级别泛化方面优于非层次化版本。我们还提出了一种新颖的加权位移模型，作为高层策略的基础，将预测嵌入到现有场景的3D结构中，优于其他策略表示方法。我们展示了我们的学习策略可以零样本转移到三种不同的真实机器人设置中：两个不同实验室中的固定桌面Franka手臂，以及移动基座上的X-Arm，打开了多个未见过的关节式物体，跨越两个实验室、真实休息室和厨房。更多视频和代码请参阅我们的项目网站：this https URL。', 'title_zh': 'ArticuBot：大规模仿真学习通用articulated物体操作策略'}
{'arxiv_id': 'arXiv:2503.03556', 'title': 'Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation', 'authors': 'Xiaomeng Zhu, Yuyang Li, Leiyao Cui, Pengfei Li, Huan-ang Gao, Yixin Zhu, Hao Zhao', 'link': 'https://arxiv.org/abs/2503.03556', 'abstract': "Object affordance reasoning, the ability to infer object functionalities based on physical properties, is fundamental for task-oriented planning and activities in both humans and Artificial Intelligence (AI). This capability, required for planning and executing daily activities in a task-oriented manner, relies on commonsense knowledge of object physics and functionalities, extending beyond simple object recognition. Current computational models for affordance reasoning from perception lack generalizability, limiting their applicability in novel scenarios. Meanwhile, comprehensive Large Language Models (LLMs) with emerging reasoning capabilities are challenging to deploy on local devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a large-scale dataset comprising 1,496 tasks and 119k images, designed to enhance the generalizability of affordance reasoning from perception. Utilizing this dataset, we develop Afford-X, an end-to-end trainable affordance reasoning model that incorporates Verb Attention and Bi-Fusion modules to improve multi-modal understanding. This model achieves up to a 12.1% performance improvement over the best-reported results from non-LLM methods, while also demonstrating a 1.2% enhancement compared to our previous conference paper. Additionally, it maintains a compact 187M parameter size and infers nearly 50 times faster than the GPT-4V API. Our work demonstrates the potential for efficient, generalizable affordance reasoning models that can be deployed on local devices for task-oriented manipulations. We showcase Afford-X's effectiveness in enabling task-oriented manipulations for robots across various tasks and environments, underscoring its efficiency and broad implications for advancing robotics and AI systems in real-world applications.", 'abstract_zh': '基于感知的 affordance 推理：一种用于任务导向操作的大规模数据集和模型', 'title_zh': 'Afford-X: 通用且精简的执行导向抓持功能推理'}
{'arxiv_id': 'arXiv:2503.03196', 'title': 'SpiritSight Agent: Advanced GUI Agent with One Look', 'authors': 'Zhiyuan Huang, Ziming Cheng, Junting Pan, Zhaohui Hou, Mingjie Zhan', 'link': 'https://arxiv.org/abs/2503.03196', 'abstract': "Graphical User Interface (GUI) agents show amazing abilities in assisting human-computer interaction, automating human user's navigation on digital devices. An ideal GUI agent is expected to achieve high accuracy, low latency, and compatibility for different GUI platforms. Recent vision-based approaches have shown promise by leveraging advanced Vision Language Models (VLMs). While they generally meet the requirements of compatibility and low latency, these vision-based GUI agents tend to have low accuracy due to their limitations in element grounding. To address this issue, we propose $\\textbf{SpiritSight}$, a vision-based, end-to-end GUI agent that excels in GUI navigation tasks across various GUI platforms. First, we create a multi-level, large-scale, high-quality GUI dataset called $\\textbf{GUI-Lasagne}$ using scalable methods, empowering SpiritSight with robust GUI understanding and grounding capabilities. Second, we introduce the $\\textbf{Universal Block Parsing (UBP)}$ method to resolve the ambiguity problem in dynamic high-resolution of visual inputs, further enhancing SpiritSight's ability to ground GUI objects. Through these efforts, SpiritSight agent outperforms other advanced methods on diverse GUI benchmarks, demonstrating its superior capability and compatibility in GUI navigation tasks. Models are available at $\\href{this https URL}{this\\ URL}$.", 'abstract_zh': '基于视觉的图形用户界面代理：SpiritSight在跨平台GUI导航任务中的卓越表现', 'title_zh': 'SpiritSight 代理: 具有一瞥功能的高级GUI代理'}
{'arxiv_id': 'arXiv:2503.03002', 'title': 'Multi-Step Deep Koopman Network (MDK-Net) for Vehicle Control in Frenet Frame', 'authors': 'Mohammad Abtahi, Mahdis Rabbani, Armin Abdolmohammadi, Shima Nazari', 'link': 'https://arxiv.org/abs/2503.03002', 'abstract': 'The highly nonlinear dynamics of vehicles present a major challenge for the practical implementation of optimal and Model Predictive Control (MPC) approaches in path planning and following. Koopman operator theory offers a global linear representation of nonlinear dynamical systems, making it a promising framework for optimization-based vehicle control. This paper introduces a novel deep learning-based Koopman modeling approach that employs deep neural networks to capture the full vehicle dynamics-from pedal and steering inputs to chassis states-within a curvilinear Frenet frame. The superior accuracy of the Koopman model compared to identified linear models is shown for a double lane change maneuver. Furthermore, it is shown that an MPC controller deploying the Koopman model provides significantly improved performance while maintaining computational efficiency comparable to a linear MPC.', 'abstract_zh': '基于深度学习的Koopman模型在路径规划与跟踪中的应用：车辆非线性动力学的优化控制方法', 'title_zh': '基于傅里叶框架的多步深Koopman网络（MDK-Net）在车辆控制中的应用'}
{'arxiv_id': 'arXiv:2503.02916', 'title': 'Monocular Person Localization under Camera Ego-motion', 'authors': 'Yu Zhan, Hanjing Ye, Hong Zhang', 'link': 'https://arxiv.org/abs/2503.02916', 'abstract': "Localizing a person from a moving monocular camera is critical for Human-Robot Interaction (HRI). To estimate the 3D human position from a 2D image, existing methods either depend on the geometric assumption of a fixed camera or use a position regression model trained on datasets containing little camera ego-motion. These methods are vulnerable to fierce camera ego-motion, resulting in inaccurate person localization. We consider person localization as a part of a pose estimation problem. By representing a human with a four-point model, our method jointly estimates the 2D camera attitude and the person's 3D location through optimization. Evaluations on both public datasets and real robot experiments demonstrate our method outperforms baselines in person localization accuracy. Our method is further implemented into a person-following system and deployed on an agile quadruped robot.", 'abstract_zh': '基于移动单目相机的人类定位对于人机交互（HRI）至关重要。通过优化联合估计二维相机姿态和人的三维位置，我们的方法不仅在公开数据集上，还在实际机器人实验中展示了对人类定位的优越性能，并进一步集成到一个人跟随系统中部署在敏捷四足机器人上。', 'title_zh': '基于摄像机 ego-运动的单目人体定位'}
{'arxiv_id': 'arXiv:2503.02913', 'title': 'Towards Robust Multi-UAV Collaboration: MARL with Noise-Resilient Communication and Attention Mechanisms', 'authors': 'Zilin Zhao, Chishui Chen, Haotian Shi, Jiale Chen, Xuanlin Yue, Zhejian Yang, Yang Liu', 'link': 'https://arxiv.org/abs/2503.02913', 'abstract': 'Efficient path planning for unmanned aerial vehicles (UAVs) is crucial in remote sensing and information collection. As task scales expand, the cooperative deployment of multiple UAVs significantly improves information collection efficiency. However, collaborative communication and decision-making for multiple UAVs remain major challenges in path planning, especially in noisy environments. To efficiently accomplish complex information collection tasks in 3D space and address robust communication issues, we propose a multi-agent reinforcement learning (MARL) framework for UAV path planning based on the Counterfactual Multi-Agent Policy Gradients (COMA) algorithm. The framework incorporates attention mechanism-based UAV communication protocol and training-deployment system, significantly improving communication robustness and individual decision-making capabilities in noisy conditions. Experiments conducted on both synthetic and real-world datasets demonstrate that our method outperforms existing algorithms in terms of path planning efficiency and robustness, especially in noisy environments, achieving a 78\\% improvement in entropy reduction.', 'abstract_zh': '基于Counterfactual Multi-Agent Policy Gradients的多 agent 强化学习框架下无人机路径规划', 'title_zh': '面向鲁棒多无人机协作：具有抗噪声通信和注意力机制的多智能体 reinforcement 学习'}
{'arxiv_id': 'arXiv:2503.03743', 'title': 'CHOP: Mobile Operating Assistant with Constrained High-frequency Optimized Subtask Planning', 'authors': 'Yuqi Zhou, Shuai Wang, Sunhao Dai, Qinglin Jia, Zhaocheng Du, Zhenhua Dong, Jun Xu', 'link': 'https://arxiv.org/abs/2503.03743', 'abstract': "The advancement of visual language models (VLMs) has enhanced mobile device operations, allowing simulated human-like actions to address user requirements. Current VLM-based mobile operating assistants can be structured into three levels: task, subtask, and action. The subtask level, linking high-level goals with low-level executable actions, is crucial for task completion but faces two challenges: ineffective subtasks that lower-level agent cannot execute and inefficient subtasks that fail to contribute to the completion of the higher-level task. These challenges stem from VLM's lack of experience in decomposing subtasks within GUI scenarios in multi-agent architecture. To address these, we propose a new mobile assistant architecture with constrained high-frequency o}ptimized planning (CHOP). Our approach overcomes the VLM's deficiency in GUI scenarios planning by using human-planned subtasks as the basis vector. We evaluate our architecture in both English and Chinese contexts across 20 Apps, demonstrating significant improvements in both effectiveness and efficiency. Our dataset and code is available at this https URL", 'abstract_zh': '视觉语言模型的进步增强了移动设备的操作，使模拟人类行为得以实现以满足用户需求。当前基于VLM的移动操作助手可以分为三个层次：任务、子任务和动作。子任务层次连接高层级目标与可执行的低层级动作，对于任务完成至关重要，但面临两大挑战：下层代理无法执行的无效子任务和不有助于完成高层任务的低效子任务。这些挑战源于VLM在多代理架构中GUI场景子任务分解经验不足。为应对这些挑战，我们提出了一种新的移动助手架构，即受限高频率优化规划（CHOP）。我们通过将人类规划的子任务作为基础向量来弥补VLM在GUI场景规划中的不足。我们在20个应用程序中分别以英文和中文环境评估了该架构，展示了在有效性和效率上的显著提升。我们的数据集和代码可在此网址访问：this https URL。', 'title_zh': 'CHOP: 受约束高频率优化子任务规划的移动操作助理'}
{'arxiv_id': 'arXiv:2503.03562', 'title': 'Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection', 'authors': 'Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu', 'link': 'https://arxiv.org/abs/2503.03562', 'abstract': 'Humans detect real-world object anomalies by perceiving, interacting, and reasoning based on object-conditioned physical knowledge. The long-term goal of Industrial Anomaly Detection (IAD) is to enable machines to autonomously replicate this skill. However, current IAD algorithms are largely developed and tested on static, semantically simple datasets, which diverge from real-world scenarios where physical understanding and reasoning are this http URL bridge this gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, the first large-scale, real-world, physics-grounded video dataset for industrial anomaly detection. Collected using a real robot arm and motor, Phys-AD provides a diverse set of dynamic, semantically rich scenarios. The dataset includes more than 6400 videos across 22 real-world object categories, interacting with robot arms and motors, and exhibits 47 types of anomalies. Anomaly detection in Phys-AD requires visual reasoning, combining both physical knowledge and video content to determine object this http URL benchmark state-of-the-art anomaly detection methods under three settings: unsupervised AD, weakly-supervised AD, and video-understanding AD, highlighting their limitations in handling physics-grounded anomalies. Additionally, we introduce the Physics Anomaly Explanation (PAEval) metric, designed to assess the ability of visual-language foundation models to not only detect anomalies but also provide accurate explanations for their underlying physical causes. Our dataset and benchmark will be publicly available.', 'abstract_zh': '人类通过感知、交互和基于对象条件物理知识的推理来检测现实世界的物体异常。工业异常检测（IAD）的长期目标是使机器能够自主复制这一技能。然而，当前的IAD算法主要是在静态、语义简单的数据集上开发和测试的，与实际场景中的物理理解和推理相去甚远。为了缩小这一差距，我们介绍了首个面向工业异常检测的大规模、真实世界、物理导向的视频数据集——Physics Anomaly Detection（Phys-AD）数据集。该数据集使用真实机器人手臂和电机采集，提供了多种动态且语义丰富的场景。数据集包含了超过6400个视频，覆盖22个真实的物体类别，并展示了47种类型的异常。在Phys-AD中进行异常检测需要视觉推理，结合物理知识和视频内容来确定物体的状态。我们以三种设置——无监督异常检测、弱监督异常检测和视频理解异常检测——来评估最先进的异常检测方法，并突显它们在处理物理导向的异常时的局限性。此外，我们引入了Physics Anomaly Explanation（PAEval）度量标准，旨在评估视觉语言基础模型不仅能够检测异常，还能提供其物理原因的准确解释的能力。我们的数据集和基准将会公开。', 'title_zh': '面向现实世界物理动力学的视觉辨识与推理：基于物理原理的异常检测'}
