{'arxiv_id': 'arXiv:2509.11082', 'title': 'Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation', 'authors': 'Zongwu Xie, Kaijie Yun, Yang Liu, Yiming Ji, Han Li', 'link': 'https://arxiv.org/abs/2509.11082', 'abstract': "We present a robust multi-modal framework for predicting traversability costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce a bird's-eye-view (BEV) terrain costmap, trained self-supervised using IMU-derived labels. Key updates include a DINOv3-based image encoder, FiLM-based sensor fusion, and an optimization loss combining Huber and smoothness terms. Experimental ablations (removing image color, occluding inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry dominates the learned cost and the model is highly robust. We attribute the small performance differences to the IMU labeling primarily reflecting terrain geometry rather than semantics and to limited data diversity. Unlike prior work claiming large gains, we emphasize our contributions: (1) a high-fidelity, reproducible simulation environment; (2) a self-supervised IMU-based labeling pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss limitations and future work such as domain generalization and dataset expansion.", 'abstract_zh': '一种鲁棒的多模态框架，用于预测行星探测车的通行性成本图：基于DINOv3的图像编码器、FiLM机制的传感器融合以及结合Huber和光滑项的优化损失', 'title_zh': '火星通行性预测：用于代价地图生成的多模态自监督方法'}
{'arxiv_id': 'arXiv:2509.11944', 'title': 'Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare', 'authors': 'Susanta Mitra', 'link': 'https://arxiv.org/abs/2509.11944', 'abstract': 'Healthcare and medicine are multimodal disciplines that deal with multimodal data for reasoning and diagnosing multiple diseases. Although some multimodal reasoning models have emerged for reasoning complex tasks in scientific domains, their applications in the healthcare domain remain limited and fall short in correct reasoning for diagnosis. To address the challenges of multimodal medical reasoning for correct diagnosis and assist the healthcare professionals, a novel temporal graph-based reasoning process modelled through a directed graph has been proposed in the current work. It helps in accommodating dynamic changes in reasons through backtracking, refining the reasoning content, and creating new or deleting existing reasons to reach the best recommendation or answer. Again, consideration of multimodal data at different time points can enable tracking and analysis of patient health and disease progression. Moreover, the proposed multi-agent temporal reasoning framework provides task distributions and a cross-validation mechanism to further enhance the accuracy of reasoning outputs. A few basic experiments and analysis results justify the novelty and practical utility of the proposed preliminary approach.', 'abstract_zh': '多模态医疗推理的时间图模型及其在正确诊断中的应用', 'title_zh': '基于多模态语言模型的代理时间推理图形：一种潜在的医疗健康AI辅助工具'}
{'arxiv_id': 'arXiv:2509.11914', 'title': 'EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models', 'authors': 'Yiqun Yao, Naitong Yu, Xiang Li, Xin Jiang, Xuezhi Fang, Wenjia Ma, Xuying Meng, Jing Li, Aixin Sun, Yequan Wang', 'link': 'https://arxiv.org/abs/2509.11914', 'abstract': "We introduce EgoMem, the first lifelong memory agent tailored for full-duplex models that process real-time omnimodal streams. EgoMem enables real-time models to recognize multiple users directly from raw audiovisual streams, to provide personalized response, and to maintain long-term knowledge of users' facts, preferences, and social relationships extracted from audiovisual history. EgoMem operates with three asynchronous processes: (i) a retrieval process that dynamically identifies user via face and voice, and gathers relevant context from a long-term memory; (ii) an omnimodal dialog process that generates personalized audio responses based on the retrieved context; and (iii) a memory management process that automatically detects dialog boundaries from omnimodal streams, and extracts necessary information to update the long-term memory. Unlike existing memory agents for LLMs, EgoMem relies entirely on raw audiovisual streams, making it especially suitable for lifelong, real-time, and embodied scenarios. Experimental results demonstrate that EgoMem's retrieval and memory management modules achieve over 95% accuracy on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot, the system achieves fact-consistency scores above 87% in real-time personalized dialogs, establishing a strong baseline for future research.", 'abstract_zh': 'EgoMem：面向全双工模型的实时多模态记忆代理', 'title_zh': 'EgoMem: 全双工 omnimodal 模型的终身记忆代理'}
{'arxiv_id': 'arXiv:2509.12046', 'title': 'Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking', 'authors': 'Zirui Zheng, Takashi Isobe, Tong Shen, Xu Jia, Jianbin Zhao, Xiaomin Li, Mengmeng Ge, Baolu Li, Qinghe Wang, Dong Li, Dong Zhou, Yunzhi Zhuge, Huchuan Lu, Emad Barsoum', 'link': 'https://arxiv.org/abs/2509.12046', 'abstract': 'While autoregressive (AR) models have demonstrated remarkable success in image generation, extending them to layout-conditioned generation remains challenging due to the sparse nature of layout conditions and the risk of feature entanglement. We present Structured Masking for AR-based Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that effectively integrates spatial layout constraints into AR-based image generation. To equip AR model with layout control, a specially designed structured masking strategy is applied to attention computation to govern the interaction among the global prompt, layout, and image tokens. This design prevents mis-association between different regions and their descriptions while enabling sufficient injection of layout constraints into the generation process. To further enhance generation quality and layout accuracy, we incorporate Group Relative Policy Optimization (GRPO) based post-training scheme with specially designed layout reward functions for next-set-based AR models. Experimental results demonstrate that SMARLI is able to seamlessly integrate layout tokens with text and image tokens without compromising generation quality. It achieves superior layoutaware control while maintaining the structural simplicity and generation efficiency of AR models.', 'abstract_zh': '基于结构化掩模的布局引导图像生成框架（SMARLI）', 'title_zh': '基于布局条件的自回归文本到图像生成方法：结构化掩码方案'}
{'arxiv_id': 'arXiv:2509.11862', 'title': 'Bridging Vision Language Models and Symbolic Grounding for Video Question Answering', 'authors': 'Haodi Ma, Vyom Pathak, Daisy Zhe Wang', 'link': 'https://arxiv.org/abs/2509.11862', 'abstract': 'Video Question Answering (VQA) requires models to reason over spatial, temporal, and causal cues in videos. Recent vision language models (VLMs) achieve strong results but often rely on shallow correlations, leading to weak temporal grounding and limited interpretability. We study symbolic scene graphs (SGs) as intermediate grounding signals for VQA. SGs provide structured object-relation representations that complement VLMs holistic reasoning. We introduce SG-VLM, a modular framework that integrates frozen VLMs with scene graph grounding via prompting and visual localization. Across three benchmarks (NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM improves causal and temporal reasoning and outperforms prior baselines, though gains over strong VLMs are limited. These findings highlight both the promise and current limitations of symbolic grounding, and offer guidance for future hybrid VLM-symbolic approaches in video understanding.', 'abstract_zh': '视频问答（VQA）要求模型在视频中推理空间、时间和因果线索。近期的视觉语言模型（VLMs）取得了很好的成果，但往往依赖于浅层的相关性，导致时间定位能力弱和解释性有限。我们研究符号场景图（SGs）作为VQA的中间 grounding 信号。SGs 提供结构化的对象关系表示，补充了VLMs的整体推理。我们引入了SG-VLM，这是一种模块化框架，通过提示和视觉定位将冻结的VLMs与场景图接地结合起来。在三个基准数据集（NExT-QA、iVQA、ActivityNet-QA）和多种VLMs（QwenVL、InternVL）上，SG-VLM 提升了因果和时间推理能力并超过了先前的基线，尽管相对于强VLMs的提升有限。这些发现突显了符号接地的潜力和当前限制，并为未来视觉语言模型-符号综合方法在视频理解中的应用提供了指导。', 'title_zh': '视觉语言模型与符号接地在视频问答中的桥梁构建'}
{'arxiv_id': 'arXiv:2509.10522', 'title': 'Multimodal Deep Learning for ATCO Command Lifecycle Modeling and Workload Prediction', 'authors': 'Kaizhen Tan', 'link': 'https://arxiv.org/abs/2509.10522', 'abstract': 'Air traffic controllers (ATCOs) issue high-intensity voice commands in dense airspace, where accurate workload modeling is critical for safety and efficiency. This paper proposes a multimodal deep learning framework that integrates structured data, trajectory sequences, and image features to estimate two key parameters in the ATCO command lifecycle: the time offset between a command and the resulting aircraft maneuver, and the command duration. A high-quality dataset was constructed, with maneuver points detected using sliding window and histogram-based methods. A CNN-Transformer ensemble model was developed for accurate, generalizable, and interpretable predictions. By linking trajectories to voice commands, this work offers the first model of its kind to support intelligent command generation and provides practical value for workload assessment, staffing, and scheduling.', 'abstract_zh': '空中交通管制员（ATCOs）在密集空域发出高强度语音指令，准确的工作负荷建模对于安全和效率至关重要。本文提出了一种多模态深度学习框架，综合结构化数据、航迹序列和图像特征以估计ATCO命令生命周期中的两个关键参数：指令与后续航空器动作之间的时间偏移以及指令持续时间。通过滑动窗口和直方图方法检测动作点，构建了高质量的数据集。开发了一种CNN-Transformer集成模型，实现准确、可泛化和可解释的预测。通过将航迹与语音指令关联，本研究提供了首个用于智能指令生成的模型，为工作负荷评估、人员配备和排班提供了实际价值。', 'title_zh': '多模态深度学习在ATCO命令生命周期建模与工作负载预测中的应用'}
