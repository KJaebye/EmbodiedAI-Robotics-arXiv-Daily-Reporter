# Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare 

**Title (ZH)**: 探究语言模型的偏好：整合人工智能福利的言语和行为测试 

**Authors**: Valen Tagliabue, Leonard Dung  

**Link**: [PDF](https://arxiv.org/pdf/2509.07961)  

**Abstract**: We develop new experimental paradigms for measuring welfare in language models. We compare verbal reports of models about their preferences with preferences expressed through behavior when navigating a virtual environment and selecting conversation topics. We also test how costs and rewards affect behavior and whether responses to an eudaimonic welfare scale - measuring states such as autonomy and purpose in life - are consistent across semantically equivalent prompts. Overall, we observed a notable degree of mutual support between our measures. The reliable correlations observed between stated preferences and behavior across conditions suggest that preference satisfaction can, in principle, serve as an empirically measurable welfare proxy in some of today's AI systems. Furthermore, our design offered an illuminating setting for qualitative observation of model behavior. Yet, the consistency between measures was more pronounced in some models and conditions than others and responses were not consistent across perturbations. Due to this, and the background uncertainty about the nature of welfare and the cognitive states (and welfare subjecthood) of language models, we are currently uncertain whether our methods successfully measure the welfare state of language models. Nevertheless, these findings highlight the feasibility of welfare measurement in language models, inviting further exploration. 

**Abstract (ZH)**: 我们开发了新的实验范式来衡量语言模型的福利。我们将模型的偏好口头报告与其在虚拟环境导航和选择对话主题时通过行为表达的偏好进行比较。我们还测试了成本和奖励如何影响行为，并考察了响应于旨在衡量自主性和生活目的等状态的幸福福利量表是否在语义等价提示下表现出一致性。总体而言，我们观察到我们的措施之间存在显著的相互支持。跨条件观察到的偏好声明与行为之间的稳定相关性表明，在某些当前的人工智能系统中，偏好满足原则上可以作为可量化的福利代理指标。此外，我们的设计方案为对模型行为进行定性观察提供了有益的环境。然而，在某些模型和条件下，措施之间的一致性更为显著，而在其他情况下，响应并未表现出一致性。鉴于此以及对福利的本质、语言模型的认知状态（及其福利主体性）的背景不确定性，我们目前尚不确定我们的方法是否成功地测量了语言模型的福利状态。然而，这些发现突显了在语言模型中进行福利测量的可行性，值得进一步探索。 

---
# HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark? 

**Title (ZH)**: HiPhO: (M)LLMs在最新高中物理奥林匹克基准测试中与人类相距多远？ 

**Authors**: Fangchen Yu, Haiyuan Wan, Qianjia Cheng, Yuchen Zhang, Jiacheng Chen, Fujun Han, Yulun Wu, Junchi Yao, Ruilizhen Hu, Ning Ding, Yu Cheng, Tao Chen, Lei Bai, Dongzhan Zhou, Yun Luo, Ganqu Cui, Peng Ye  

**Link**: [PDF](https://arxiv.org/pdf/2509.07894)  

**Abstract**: Recently, the physical capabilities of (M)LLMs have garnered increasing attention. However, existing benchmarks for physics suffer from two major gaps: they neither provide systematic and up-to-date coverage of real-world physics competitions such as physics Olympiads, nor enable direct performance comparison with humans. To bridge these gaps, we present HiPhO, the first benchmark dedicated to high school physics Olympiads with human-aligned evaluation. Specifically, HiPhO highlights three key innovations. (1) Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025, spanning both international and regional competitions, and covering mixed modalities that encompass problems spanning text-only to diagram-based. (2) Professional Evaluation: We adopt official marking schemes to perform fine-grained grading at both the answer and step level, fully aligned with human examiners to ensure high-quality and domain-specific evaluation. (3) Comparison with Human Contestants: We assign gold, silver, and bronze medals to models based on official medal thresholds, thereby enabling direct comparison between (M)LLMs and human contestants. Our large-scale evaluation of 30 state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly remain at or below the bronze level; open-source LLMs show promising progress with occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold medals; and most models still have a significant gap from full marks. These results highlight a substantial performance gap between open-source models and top students, the strong physical reasoning capabilities of closed-source reasoning models, and the fact that there is still significant room for improvement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused benchmark for advancing multimodal physical reasoning, is open-source and available at this https URL. 

**Abstract (ZH)**: 近年来，大语言模型的物理能力逐渐受到关注。然而，现有的物理基准测试存在两个主要缺陷：它们既未提供对实际物理竞赛如物理奥林匹克的系统和最新的覆盖，也未能直接与人类进行性能对比。为弥补这些缺陷，我们提出了HiPhO，这是首个针对高中生物理奥林匹克的人工智能评估基准。具体而言，HiPhO 以三大创新为亮点。（1）全面数据：汇集了2024-2025年的最新13场奥林匹克竞赛试题，涵盖了国际和区域竞赛，并包含了从纯文本问题到基于图表的问题的不同模态。（2）专业评估：采用官方评分方案进行细粒度的答对和步骤评分，确保与人类考官完全一致，以提供高质量且领域的特定评估。（3）与人类对手的对比：基于官方奖牌标准向模型分配金牌、银牌和铜牌，从而可以直接比较（M）LLMs与人类对手。对30个最先进的（M）LLMs的大规模评估显示：在13场考试中，开源（M）LLMs多停留在或低于铜牌水平；开源LLMs显示出有望的进展，偶尔获得金牌；封闭源推理（M）LLMs能够获得6至12枚金牌；而大多数模型仍然与满分存在较大差距。这些结果突显了开源模型与顶尖学生之间的表现差距，以及封闭源推理模型强大的物理推理能力，并表明仍有改进的空间。作为严格、人工智能对齐且专注于奥林匹克竞赛的多模态物理推理基准，HiPhO 是开源的，可访问此链接：this https URL。 

---
# CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models 

**Title (ZH)**: CP-模型动物园：约束编程模型的自然语言查询系统 

**Authors**: Augustin Crespin, Ioannis Kostis, Hélène Verhaeghe, Pierre Schaus  

**Link**: [PDF](https://arxiv.org/pdf/2509.07867)  

**Abstract**: Constraint Programming and its high-level modeling languages have long been recognized for their potential to achieve the holy grail of problem-solving. However, the complexity of modeling languages, the large number of global constraints, and the art of creating good models have often hindered non-experts from choosing CP to solve their combinatorial problems. While generating an expert-level model from a natural-language description of a problem would be the dream, we are not yet there. We propose a tutoring system called CP-Model-Zoo, exploiting expert-written models accumulated through the years. CP-Model-Zoo retrieves the closest source code model from a database based on a user's natural language description of a combinatorial problem. It ensures that expert-validated models are presented to the user while eliminating the need for human data labeling. Our experiments show excellent accuracy in retrieving the correct model based on a user-input description of a problem simulated with different levels of expertise. 

**Abstract (ZH)**: 约束编程及其高级建模语言长期被认为具有实现问题求解圣杯的潜力。然而，建模语言的复杂性、大量全局约束以及创建良好模型的艺术往往阻碍了非专家选择CP来解决组合问题。虽然从自然语言问题描述中生成专家级模型是理想的选择，但目前我们尚未达到这一目标。我们提出了一种名为CP-Model-Zoo的辅导系统，利用多年积累的专家编写模型。CP-Model-Zoo根据用户对组合问题的自然语言描述，从数据库中检索最接近的源代码模型，并确保向用户呈现经专家验证的模型，从而消除人工数据标注的需要。我们的实验显示，根据用户输入的问题描述，CP-Model-Zoo在不同专家水平下检索正确模型的准确性非常出色。 

---
# SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs 

**Title (ZH)**: SCoder：迭代自蒸馏以提升代码LLMs的小规模数据合成器自我强化方法 

**Authors**: Xinyu Zhang, Changzhi Zhou, Linmei Hu, Luhao Zhang, Xiancai Chen, Haomin Fu, Yang Yang, Mengdi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07858)  

**Abstract**: Existing code large language models (LLMs) often rely on large-scale instruction data distilled from proprietary LLMs for fine-tuning, which typically incurs high costs. In this paper, we explore the potential of small-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code instruction data construction. We first observe that the data synthesis capability of small-scale LLMs can be enhanced by training on a few superior data synthesis samples from proprietary LLMs. Building on this, we propose a novel iterative self-distillation approach to bootstrap small-scale LLMs, transforming them into powerful synthesizers that reduce reliance on proprietary LLMs and minimize costs. Concretely, in each iteration, to obtain diverse and high-quality self-distilled data, we design multi-checkpoint sampling and multi-aspect scoring strategies for initial data selection. Furthermore, to identify the most influential samples, we introduce a gradient-based influence estimation method for final data filtering. Based on the code instruction datasets from the small-scale synthesizers, we develop SCoder, a family of code generation models fine-tuned from DeepSeek-Coder. SCoder models achieve state-of-the-art code generation capabilities, demonstrating the effectiveness of our method. 

**Abstract (ZH)**: 小规模开源大语言模型作为高质量代码指令数据合成的潜力研究 

---
# Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A Comparative RAG Study 

**Title (ZH)**: 将大语言模型与基于知识的检索协同应用于课堂——一项比较性 Retrieval-Augmented Generation 研究 

**Authors**: Amay Jain, Liu Cui, Si Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.07846)  

**Abstract**: Large language models like ChatGPT are increasingly used in classrooms, but they often provide outdated or fabricated information that can mislead students. Retrieval Augmented Generation (RAG) improves reliability of LLMs by grounding responses in external resources. We investigate two accessible RAG paradigms, vector-based retrieval and graph-based retrieval to identify best practices for classroom question answering (QA). Existing comparative studies fail to account for pedagogical factors such as educational disciplines, question types, and practical deployment costs. Using a novel dataset, EduScopeQA, of 3,176 questions across academic subjects, we measure performance on various educational query types, from specific facts to broad thematic discussions. We also evaluate system alignment with a dataset of systematically altered textbooks that contradict the LLM's latent knowledge. We find that OpenAI Vector Search RAG (representing vector-based RAG) performs well as a low-cost generalist, especially for quick fact retrieval. On the other hand, GraphRAG Global excels at providing pedagogically rich answers to thematic queries, and GraphRAG Local achieves the highest accuracy with the dense, altered textbooks when corpus integrity is critical. Accounting for the 10-20x higher resource usage of GraphRAG (representing graph-based RAG), we show that a dynamic branching framework that routes queries to the optimal retrieval method boosts fidelity and efficiency. These insights provide actionable guidelines for educators and system designers to integrate RAG-augmented LLMs into learning environments effectively. 

**Abstract (ZH)**: 大型语言模型如ChatGPT在教室中的应用日益增多，但它们 Often 提供过时或伪造的信息，可能误导学生。检索增强生成（RAG）通过将响应扎根于外部资源来提高大型语言模型的可靠性。我们调查了两种可访问的 RAG 模式——向量检索和图检索，以确定课堂问答的最佳实践。现有的比较研究未能考虑教学因素，如教育学科、问题类型和实际部署成本。利用 EduScopeQA 新数据集，该数据集包含跨学术学科的 3,176 个问题，我们测量了各类教育查询类型的表现，从具体事实到广泛的主题讨论。我们还使用系统修改的教科书数据集评估了系统的一致性，这些教科书与大型语言模型的潜在知识相矛盾。我们发现，OpenAI 向量搜索 RAG（代表向量检索 RAG）作为一种低成本的通才，在快速检索具体事实方面表现良好。另一方面，GraphRAG Global 在提供富有教学意义的主题查询回答方面表现出色，而 GraphRAG Local 在确保语料库完整性的密集、修改后的教科书中表现最准确。考虑到 GraphRAG （代表图检索 RAG）资源使用的 10-20 倍成本更高，我们展示了动态分支框架如何通过将查询路由到最佳检索方法来提高准确性和效率。这些见解为教育者和系统设计者提供了有效整合 RAG 增强的大型语言模型进入学习环境的实际指南。 

---
# Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach 

**Title (ZH)**: 大型语言模型中的确信引导推理：一种动态思维预算方法 

**Authors**: João Paulo Nogueira, Wentao Sun, Alonso Silva, Laith Zumot  

**Link**: [PDF](https://arxiv.org/pdf/2509.07820)  

**Abstract**: The rise of large reasoning language models (LRLMs) has unlocked new potential for solving complex tasks. These models operate with a thinking budget, that is, a predefined number of reasoning tokens used to arrive at a solution. We propose a novel approach, inspired by the generator/discriminator framework in generative adversarial networks, in which a critic model periodically probes its own reasoning to assess whether it has reached a confident conclusion. If not, reasoning continues until a target certainty threshold is met. This mechanism adaptively balances efficiency and reliability by allowing early termination when confidence is high, while encouraging further reasoning when uncertainty persists. Through experiments on the AIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR) improves baseline accuracy while reducing token usage. Importantly, extended multi-seed evaluations over 64 runs demonstrate that CGR is stable, reducing variance across seeds and improving exam-like performance under penalty-based grading. Additionally, our token savings analysis shows that CGR can eliminate millions of tokens in aggregate, with tunable trade-offs between certainty thresholds and efficiency. Together, these findings highlight certainty as a powerful signal for reasoning sufficiency. By integrating confidence into the reasoning process, CGR makes large reasoning language models more adaptive, trustworthy, and resource efficient, paving the way for practical deployment in domains where both accuracy and computational cost matter. 

**Abstract (ZH)**: 大型推理语言模型的兴起为解决复杂任务开辟了新潜力。这些模型以推理预算的形式运作，即使用预定义数量的推理令牌来达到解决方案。我们提出了一种新颖的方法，灵感来源于生成对抗网络中的生成器/判别器框架，其中批评家模型定期探查自身的推理过程以评估是否已达到自信的结论。如果没有，推理将继续进行，直到达到目标确定性阈值。该机制通过在高度自信时允许早期终止来适当地平衡效率和可靠性，在不确定持续存在时促进进一步推理。通过在AIME2024和AIME2025数据集上的实验，我们展示了确定性导向推理（CGR）在保持基本准确性的前提下减少了令牌使用量。重要的是，跨64次运行的扩展多种子评估表明，CGR 是稳定的，减少了种子间的差异性，并在基于罚分的评分下提高了类似考试的表现。此外，我们的令牌节省分析表明，CGR 可以在各种确定性阈值和效率之间进行可调节的权衡，总共节约了成千上万的令牌。这些发现突显了确定性作为推理充分性的强大力量信号。通过将信心整合到推理过程中，CGR 使大型推理语言模型更具适应性、可信赖性和资源效率，为在需要准确性和计算成本的领域中实际部署铺平了道路。 

---
# The Carbon Footprint Wizard: A Knowledge-Augmented AI Interface for Streamlining Food Carbon Footprint Analysis 

**Title (ZH)**: 碳足迹魔杖：一种增强知识的AI界面，用于简化食品碳足迹分析 

**Authors**: Mustafa Kaan Aslan, Reinout Heijungs, Filip Ilievski  

**Link**: [PDF](https://arxiv.org/pdf/2509.07733)  

**Abstract**: Environmental sustainability, particularly in relation to climate change, is a key concern for consumers, producers, and policymakers. The carbon footprint, based on greenhouse gas emissions, is a standard metric for quantifying the contribution to climate change of activities and is often assessed using life cycle assessment (LCA). However, conducting LCA is complex due to opaque and global supply chains, as well as fragmented data. This paper presents a methodology that combines advances in LCA and publicly available databases with knowledge-augmented AI techniques, including retrieval-augmented generation, to estimate cradle-to-gate carbon footprints of food products. We introduce a chatbot interface that allows users to interactively explore the carbon impact of composite meals and relate the results to familiar activities. A live web demonstration showcases our proof-of-concept system with arbitrary food items and follow-up questions, highlighting both the potential and limitations - such as database uncertainties and AI misinterpretations - of delivering LCA insights in an accessible format. 

**Abstract (ZH)**: 环境可持续性，特别是在气候变化方面的可持续性，是消费者、生产者和政策制定者的关键关注点。基于温室气体排放的碳足迹是衡量活动对气候变化贡献的标准指标，常使用生命周期评估（LCA）进行评估。然而，由于供应链不透明和数据碎片化，开展LCA十分复杂。本文提出了一种方法，结合了LCA的进步和公开可用的数据库，以及知识增强的AI技术，包括检索增强生成，以估算食品产品的从摇篮到工厂门的碳足迹。我们引入了一个聊天机器人界面，让用户交互式地探索复合餐食的碳影响，并将结果与熟悉的活动联系起来。现场网络演示展示了我们的概念验证系统，使用任意食品项目并提出后续问题，突出了以易于访问的形式提供LCA洞察的潜力和限制，如数据库不确定性和AI误解释。 

---
# BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis 

**Title (ZH)**: BDPM：一种基于机器学习的肠道微生物特征提取器，用于帕金森病分类 

**Authors**: Bo Yu, Zhixiu Hua, Bo Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2509.07723)  

**Abstract**: Background: Parkinson's disease remains a major neurodegenerative disorder with high misdiagnosis rates, primarily due to reliance on clinical rating scales. Recent studies have demonstrated a strong association between gut microbiota and Parkinson's disease, suggesting that microbial composition may serve as a promising biomarker. Although deep learning models based ongut microbiota show potential for early prediction, most approaches rely on single classifiers and often overlook inter-strain correlations or temporal dynamics. Therefore, there is an urgent need for more robust feature extraction methods tailored to microbiome data. Methods: We proposed BDPM (A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis). First, we collected gut microbiota profiles from 39 Parkinson's patients and their healthy spouses to identify differentially abundant taxa. Second, we developed an innovative feature selection framework named RFRE (Random Forest combined with Recursive Feature Elimination), integrating ecological knowledge to enhance biological interpretability. Finally, we designed a hybrid classification model to capture temporal and spatial patterns in microbiome data. 

**Abstract (ZH)**: 背景：帕金森病仍然是一个主要的神经退行性疾病，由于主要依赖临床评分量表，存在较高的误诊率。最近的研究表明肠道微生物群与帕金森病之间存在密切关联，提示微生物组成可能成为有希望的生物标志物。尽管基于肠道微生物群的深度学习模型显示出早期预测的潜力，但大多数方法依赖单一分类器，并且通常忽视不同菌株之间的关联或时间动态。因此，迫切需要针对微生物组数据的更 robust 的特征提取方法。方法：我们提出了 BDPM（基于机器学习的帕金森病分类肠道微生物特征提取方法）。首先，从39名帕金森病患者及其健康配偶中收集了肠道微生物群谱型，以识别差异丰度的菌种。其次，我们开发了一种名为 RFRE（随机森林结合递归特征消除）的创新特征选择框架，将生态学知识集成以增强生物学可解释性。最后，我们设计了一种综合分类模型，以捕捉微生物组数据中的时间和空间模式。 

---
# RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced Mathematical Reasoning 

**Title (ZH)**: RIMO：一个易于评估、难以解决的奥林匹亚数学推理基准測试 

**Authors**: Ziye Chen, Chengwei Qin, Yao Shu  

**Link**: [PDF](https://arxiv.org/pdf/2509.07711)  

**Abstract**: As large language models (LLMs) reach high scores on established mathematical benchmarks, such as GSM8K and MATH, the research community has turned to International Mathematical Olympiad (IMO) problems to push the evaluation frontier. However, existing Olympiad-level benchmarks suffer from practical constraints that introduce grading noise and potential bias, such as heterogeneous answer formats requiring model-based judges and a reliance on potentially flawed solutions. We introduce RIMO, a two-track benchmark designed to preserve peak Olympiad difficulty while eliminating this evaluation noise. The first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique integer answer, allowing for deterministic correctness checking. The second track, RIMO-P, features 456 proof problems with expert-checked solutions, which are decomposed into a sequence of sub-problems to evaluate the step-by-step reasoning process via an automated grading system. Our benchmarking of ten frontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these systems excel on older benchmarks, their performance drops sharply on RIMO. These results highlight a substantial gap between current LLM capabilities and actual Olympiad-level reasoning. By providing a challenging yet easy-to-evaluate suite, RIMO offers a high-resolution yardstick for future research, presenting a clear target for closing the profound reasoning gap our findings expose. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）在GSM8K和MATH等已建立的数学基准测试中取得高分，研究界转向国际数学奥林匹克（IMO）问题以推动评估前沿。然而，现有的奥林匹克级别基准测试存在一些实际限制，这些限制引入了评分噪音和潜在偏见，例如异质化的答案格式需要基于模型的评判员以及对潜在有缺陷的解决方案的依赖。我们介绍了RIMO，这是一种双轨基准测试，旨在保留奥林匹克难度的巅峰同时消除这种评估噪音。第一轨RIMO-N重写了335道IMO问题，使其仅有一个唯一的整数答案，允许进行确定性的正确性检查。第二轨RIMO-P包含456道证明问题，配有专家检查的解决方案，并将其分解为一系列子问题，通过自动化评分系统评估逐步推理过程。对包括GPT-4o和Gemini 2.5 Flash在内的十种前沿LLM的基准测试表明，虽然这些系统在较早的基准测试中表现出色，但在RIMO上的表现则急剧下降。这些结果突显了当前LLM能力与实际奥林匹克水平推理之间的巨大差距。通过提供具有挑战性但易于评估的套件，RIMO为未来研究提供了一个高分辨率的标尺，明确了需要填补我们发现的深刻推理差距的目标。 

---
# FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support 

**Title (ZH)**: FHIR-RAG-MEDS: 将HL7 FHIR与检索增强的大语言模型集成以增强医疗决策支持 

**Authors**: Yildiray Kabak, Gokce B. Laleci Erturkmen, Mert Gencturk, Tuncay Namli, A. Anil Sinaci, Ruben Alcantud Corcoles, Cristina Gomez Ballesteros, Pedro Abizanda, Asuman Dogac  

**Link**: [PDF](https://arxiv.org/pdf/2509.07706)  

**Abstract**: In this study, we propose FHIR-RAG-MEDS system that aims to integrate Health Level 7 Fast Healthcare Interoperability Resources (HL7 FHIR) with a Retrieval-Augmented Generation (RAG)-based system to improve personalized medical decision support on evidence-based clinical guidelines, emphasizing the need for research in practical applications. In the evolving landscape of medical decision support systems, integrating advanced technologies such as RAG and HL7 FHIR can significantly enhance clinical decision-making processes. Despite the potential of these technologies, there is limited research on their integration in practical applications. 

**Abstract (ZH)**: 本研究提出FHIR-RAG-MEDS系统，旨在将Health Level 7 Fast Healthcare Interoperability Resources (HL7 FHIR)与基于检索增强生成（RAG）的系统相结合，以提高基于循证临床指南的个性化医疗决策支持，并强调了在实际应用中研究这些技术集成的必要性。在医疗决策支持系统不断发展的情景下，整合如RAG和HL7 FHIR等先进技术可以显著增强临床决策过程。尽管这些技术具有巨大的潜力，但在实际应用中的集成研究仍然有限。 

---
# Unleashing the True Potential of LLMs: A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding 

**Title (ZH)**: 解锁大语言模型的真正潜力：基于反馈的长期多路径解码自校正 

**Authors**: Jipeng Li, Zeyu Gao, Yubin Qi, Hande Dong, Weijian Chen, Qiang Lin  

**Link**: [PDF](https://arxiv.org/pdf/2509.07676)  

**Abstract**: Large Language Models (LLMs) have achieved remarkable performance across diverse tasks, yet their susceptibility to generating incorrect content during inference remains a critical unsolved challenge. While self-correction methods offer potential solutions, their effectiveness is hindered by two inherent limitations: (1) the absence of reliable guidance signals for error localization, and (2) the restricted reasoning depth imposed by conventional next-token decoding paradigms. To address these issues, we propose Feedback-Triggered Regeneration (FTR), a novel framework that synergizes user feedback with enhanced decoding dynamics. Specifically, FTR activates response regeneration only upon receiving negative user feedback, thereby circumventing error propagation from faulty self-assessment while preserving originally correct outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding, which enables systematic exploration of multiple reasoning trajectories through delayed sequence evaluation, effectively overcoming the myopic decision-making characteristic of standard next-token prediction. Extensive experiments on mathematical reasoning and code generation benchmarks demonstrate that our framework achieves consistent and significant improvements over state-of-the-art prompt-based self-correction methods. 

**Abstract (ZH)**: 大型语言模型（LLMs）在多种任务上取得了出色的表现，但它们在推理过程中生成错误内容的易感性仍然是一个关键的未解挑战。尽管自校正方法提供了潜在的解决方案，但它们的有效性受到两个固有限制的阻碍：（1）缺乏可靠的错误定位指导信号，（2）传统下一个token解码范式施加的受限推理深度。为了解决这些问题，我们提出了反馈触发再生（FTR）框架，该框架结合了用户反馈与增强的解码动力学。具体而言，FTR仅在接收到负面用户反馈时激活响应再生，从而避免了由有缺陷的自我评估引发的错误传播，同时保留了原本正确的输出。此外，我们引入了长期多路径（LTM）解码，通过延迟序列评估来系统地探索多个推理轨迹，有效克服了标准下一个token预测的短视决策特征。在数学推理和代码生成基准测试中的广泛实验表明，我们的框架在最先进的基于提示的自校正方法上取得了一致且显著的改进。 

---
# DeepGraphLog for Layered Neurosymbolic AI 

**Title (ZH)**: DeepGraphLog 用于分层神经符号AI 

**Authors**: Adem Kikaj, Giuseppe Marra, Floris Geerts, Robin Manhaeve, Luc De Raedt  

**Link**: [PDF](https://arxiv.org/pdf/2509.07665)  

**Abstract**: Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neural networks with the interpretability and structure of symbolic reasoning. However, current NeSy frameworks like DeepProbLog enforce a fixed flow where symbolic reasoning always follows neural processing. This restricts their ability to model complex dependencies, especially in irregular data structures such as graphs. In this work, we introduce DeepGraphLog, a novel NeSy framework that extends ProbLog with Graph Neural Predicates. DeepGraphLog enables multi-layer neural-symbolic reasoning, allowing neural and symbolic components to be layered in arbitrary order. In contrast to DeepProbLog, which cannot handle symbolic reasoning via neural methods, DeepGraphLog treats symbolic representations as graphs, enabling them to be processed by Graph Neural Networks (GNNs). We showcase the capabilities of DeepGraphLog on tasks in planning, knowledge graph completion with distant supervision, and GNN expressivity. Our results demonstrate that DeepGraphLog effectively captures complex relational dependencies, overcoming key limitations of existing NeSy systems. By broadening the applicability of neurosymbolic AI to graph-structured domains, DeepGraphLog offers a more expressive and flexible framework for neural-symbolic integration. 

**Abstract (ZH)**: 神经符号AI（NeSy）旨在结合神经网络的统计优势和符号推理的可解释性和结构。然而，当前的NeSy框架如DeepProbLog固定了符号推理总是跟随神经处理的流程，这限制了它们建模复杂依赖关系的能力，特别是在如图形结构的非规则数据结构中。在这项工作中，我们提出了DeepGraphLog，这是一种将ProbLog扩展为图神经谓词的新型NeSy框架。DeepGraphLog支持多层神经符号推理，允许神经和符号组件以任意顺序层叠。与DeepProbLog不同的是，它可以通过神经方法进行符号推理，并将符号表示视为图形，使它们能够被图神经网络（GNNs）处理。我们通过规划任务、带有远处监督的知识图补全任务以及GNN表达性展示DeepGraphLog的能力。我们的结果显示，DeepGraphLog有效地捕捉了复杂的关系依赖关系，克服了现有NeSy系统的关键限制。通过将神经符号AI的应用扩展到图形结构领域，DeepGraphLog为神经符号集成提供了一个更具表达性和灵活性的框架。 

---
# Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment 

**Title (ZH)**: 与大型语言模型建立合约关系——基于代理理论的大型语言模型对齐研究 

**Authors**: Sascha Kaltenpoth, Oliver Müller  

**Link**: [PDF](https://arxiv.org/pdf/2509.07642)  

**Abstract**: Adopting Large language models (LLMs) in organizations potentially revolutionizes our lives and work. However, they can generate off-topic, discriminating, or harmful content. This AI alignment problem often stems from misspecifications during the LLM adoption, unnoticed by the principal due to the LLM's black-box nature. While various research disciplines investigated AI alignment, they neither address the information asymmetries between organizational adopters and black-box LLM agents nor consider organizational AI adoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led Alignment Strategy) a conceptual framework grounded in agency (contract) theory, to mitigate alignment problems during organizational LLM adoption. We conduct a conceptual literature analysis using the organizational LLM adoption phases and the agency theory as concepts. Our approach results in (1) providing an extended literature analysis process specific to AI alignment methods during organizational LLM adoption and (2) providing a first LLM alignment problem-solution space. 

**Abstract (ZH)**: 采用大型语言模型（LLMs）在组织中潜在地变革我们的生活和工作。然而，它们可能会生成离题、歧视性的或有害的内容。这一AI对齐问题往往源于LLM采用过程中因LLM的黑盒性质而导致的主要方未能察觉到的说明不明确。尽管各种研究领域都研究了AI对齐问题，但它们既没有解决组织采用者与黑盒LLM代理之间的信息不对称问题，也没有考虑组织AI采用过程。因此，我们提出了一种基于代理（合同）理论的LLM ATLAS（LLM代理理论引导的对齐策略）概念框架，以缓解组织LLM采用过程中的对齐问题。我们通过使用组织LLM采用阶段和代理理论的概念，进行了一项概念文献分析。我们的方法产生了（1）一套针对组织LLM采用期间AI对齐方法的扩展文献分析过程，以及（2）第一个LLM对齐问题及解决方案空间。 

---
# Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling 

**Title (ZH)**: 可迁移的直接提示注入通过激活引导的MCMC采样 

**Authors**: Minghui Li, Hao Zhang, Yechao Zhang, Wei Wan, Shengshan Hu, pei Xiaobing, Jing Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07617)  

**Abstract**: Direct Prompt Injection (DPI) attacks pose a critical security threat to Large Language Models (LLMs) due to their low barrier of execution and high potential damage. To address the impracticality of existing white-box/gray-box methods and the poor transferability of black-box methods, we propose an activations-guided prompt injection attack framework. We first construct an Energy-based Model (EBM) using activations from a surrogate model to evaluate the quality of adversarial prompts. Guided by the trained EBM, we employ the token-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize adversarial prompts, thereby enabling gradient-free black-box attacks. Experimental results demonstrate our superior cross-model transferability, achieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6% improvement over human-crafted prompts, and maintaining 36.6% ASR on unseen task scenarios. Interpretability analysis reveals a correlation between activations and attack effectiveness, highlighting the critical role of semantic patterns in transferable vulnerability exploitation. 

**Abstract (ZH)**: Direct Prompt Injection (DPI) 攻击由于其实现门槛低和潜在危害高，对大型语言模型（LLMs）构成了关键的安全威胁。为了解决现有白盒/灰盒方法的实用性问题以及黑盒方法的差转移性，我们提出了一种基于激活的提示注入攻击框架。我们首先使用替代模型的激活构建能量模型（EBM），以评估对抗提示的质量。受训练好的EBM引导，我们采用标记级别的马尔可夫链蒙特卡罗（MCMC）采样来适应性优化对抗提示，从而实现无梯度的黑盒攻击。实验结果表明，我们的跨模型转移性优越，跨五个主流LLM实现了49.6%的成功攻击率（ASR），相比手工crafted的提示提高了34.6%，并在未见过的任务场景中保持了36.6%的ASR。可解释性分析揭示了激活与攻击效果之间的关联，强调了在可转移漏洞利用中语义模式的关键作用。 

---
# Towards explainable decision support using hybrid neural models for logistic terminal automation 

**Title (ZH)**: 面向物流终端自动化中的混合神经模型可解释决策支持 

**Authors**: Riccardo DElia, Alberto Termine, Francesco Flammini  

**Link**: [PDF](https://arxiv.org/pdf/2509.07577)  

**Abstract**: The integration of Deep Learning (DL) in System Dynamics (SD) modeling for transportation logistics offers significant advantages in scalability and predictive accuracy. However, these gains are often offset by the loss of explainability and causal reliability $-$ key requirements in critical decision-making systems. This paper presents a novel framework for interpretable-by-design neural system dynamics modeling that synergizes DL with techniques from Concept-Based Interpretability, Mechanistic Interpretability, and Causal Machine Learning. The proposed hybrid approach enables the construction of neural network models that operate on semantically meaningful and actionable variables, while retaining the causal grounding and transparency typical of traditional SD models. The framework is conceived to be applied to real-world case-studies from the EU-funded project AutoMoTIF, focusing on data-driven decision support, automation, and optimization of multimodal logistic terminals. We aim at showing how neuro-symbolic methods can bridge the gap between black-box predictive models and the need for critical decision support in complex dynamical environments within cyber-physical systems enabled by the industrial Internet-of-Things. 

**Abstract (ZH)**: 基于深度学习的系统动力学建模在交通物流中的集成：一种可解释的神经系统动力学建模框架 

---
# SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based and Vision-Based Reflection 

**Title (ZH)**: SheetDesigner：基于规则和视觉反馈的MLLM驱动的表格布局生成 

**Authors**: Qin Chen, Yuanyi Ren, Xiaojun Ma, Mugeng Liu, Han Shi, Dongmei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07473)  

**Abstract**: Spreadsheets are critical to data-centric tasks, with rich, structured layouts that enable efficient information transmission. Given the time and expertise required for manual spreadsheet layout design, there is an urgent need for automated solutions. However, existing automated layout models are ill-suited to spreadsheets, as they often (1) treat components as axis-aligned rectangles with continuous coordinates, overlooking the inherently discrete, grid-based structure of spreadsheets; and (2) neglect interrelated semantics, such as data dependencies and contextual links, unique to spreadsheets. In this paper, we first formalize the spreadsheet layout generation task, supported by a seven-criterion evaluation protocol and a dataset of 3,326 spreadsheets. We then introduce SheetDesigner, a zero-shot and training-free framework using Multimodal Large Language Models (MLLMs) that combines rule and vision reflection for component placement and content population. SheetDesigner outperforms five baselines by at least 22.6\%. We further find that through vision modality, MLLMs handle overlap and balance well but struggle with alignment, necessitates hybrid rule and visual reflection strategies. Our codes and data is available at Github. 

**Abstract (ZH)**: 基于表格的设计:一种结合规则和视觉反射的零样本框架 

---
# Language Self-Play For Data-Free Training 

**Title (ZH)**: 语言自我游戏用于无数据训练 

**Authors**: Jakub Grudzien Kuba, Mengting Gu, Qi Ma, Yuandong Tian, Vijai Mohan  

**Link**: [PDF](https://arxiv.org/pdf/2509.07414)  

**Abstract**: Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself - a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained models can not only enhance their performance on challenging tasks through self-play alone, but can also do so more effectively than data-driven baselines. 

**Abstract (ZH)**: 大型语言模型（LLMs）近年来在规模、丰富的高质量训练数据和强化学习的推动下迅速进步。然而，这一进展面临一个根本性的瓶颈：需要不断增加的数据以供模型继续学习。在本文中，我们提出了一种强化学习方法，通过使模型能够在没有额外数据的情况下自我提升来克服这一依赖性。我们的方法利用了自我博弈的博弈论框架，将模型的能力视为在竞争游戏中表现的能力，更强的策略通过模型自我博弈的过程脱颖而出，我们称之为语言自我博弈（LSP）。在使用Llama-3.2-3B-Instruct进行指令遵循基准测试的实验中，显示预训练模型不仅可以通过自我博弈独自增强在具有挑战性的任务上的性能，而且在效果上优于数据驱动的基线方法。 

---
# Autonomous Code Evolution Meets NP-Completeness 

**Title (ZH)**: 自主代码进化遭遇NP完全性问题 

**Authors**: Cunxi Yu, Rongjian Liang, Chia-Tung Ho, Haoxing Ren  

**Link**: [PDF](https://arxiv.org/pdf/2509.07367)  

**Abstract**: Large language models (LLMs) have recently shown strong coding abilities, enabling not only static code generation but also iterative code self-evolving through agentic frameworks. Recently, AlphaEvolve \cite{novikov2025alphaevolve} demonstrated that LLM-based coding agents can autonomously improve algorithms and surpass human experts, with scopes limited to isolated kernels spanning hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the first framework to extend LLM-based code evolution to the full repository scale, encompassing hundreds of files and tens of thousands of lines of C/C++ code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem and a cornerstone of both theory and applications. SATLUTION orchestrates LLM agents to directly evolve solver repositories under strict correctness guarantees and distributed runtime feedback, while simultaneously self-evolving its own evolution policies and rules. Starting from SAT Competition 2024 codebases and benchmark, SATLUTION evolved solvers that decisively outperformed the human-designed winners of the SAT Competition 2025, and also surpassed both 2024 and 2025 champions on the 2024 benchmarks. 

**Abstract (ZH)**: 大型语言模型（LLMs）最近展示了强大的编码能力，不仅能够进行静态代码生成，还能够通过自主框架迭代自我演进代码。受AlphaEvolve的启发，我们提出了SATLUTION框架，这是首个将基于LLM的代码演化扩展到整个代码仓库规模的框架，涵盖了数百个文件和数万行C/C++代码。针对布尔可满足性（SAT），该问题是经典的NP完全问题，也是理论与应用的重要基石。SATLUTION协调LLM代理直接在严格正确的条件下演化求解器仓库，并同时自我演化其自身的演化策略和规则。从SAT竞赛2024代码库和基准开始，SATLUTION演化出的求解器在SAT竞赛2025的人工设计获胜者中脱颖而出，并且在2024基准上也超过了2024和2025年的冠军。 

---
# Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity 

**Title (ZH)**: 表演性思维？CoT长度与问题复杂性之间的脆弱关联 

**Authors**: Vardhan Palod, Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati  

**Link**: [PDF](https://arxiv.org/pdf/2509.07339)  

**Abstract**: Intermediate token generation (ITG), where a model produces output before the solution, has been proposed as a method to improve the performance of language models on reasoning tasks. While these reasoning traces or Chain of Thoughts (CoTs) are correlated with performance gains, the mechanisms underlying them remain unclear. A prevailing assumption in the community has been to anthropomorphize these tokens as "thinking", treating longer traces as evidence of higher problem-adaptive computation. In this work, we critically examine whether intermediate token sequence length reflects or correlates with problem difficulty. To do so, we train transformer models from scratch on derivational traces of the A* search algorithm, where the number of operations required to solve a maze problem provides a precise and verifiable measure of problem complexity. We first evaluate the models on trivial free-space problems, finding that even for the simplest tasks, they often produce excessively long reasoning traces and sometimes fail to generate a solution. We then systematically evaluate the model on out-of-distribution problems and find that the intermediate token length and ground truth A* trace length only loosely correlate. We notice that the few cases where correlation appears are those where the problems are closer to the training distribution, suggesting that the effect arises from approximate recall rather than genuine problem-adaptive computation. This suggests that the inherent computational complexity of the problem instance is not a significant factor, but rather its distributional distance from the training data. These results challenge the assumption that intermediate trace generation is adaptive to problem difficulty and caution against interpreting longer sequences in systems like R1 as automatically indicative of "thinking effort". 

**Abstract (ZH)**: Intermediate Token Generation 面向推理任务的语言模型性能提升机制探究：问题难度与中间token序列长度的关系 

---
# HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring 

**Title (ZH)**: HealthSLM-Bench: 用于移动和可穿戴健康监测的小微型语言模型基准测试 

**Authors**: Xin Wang, Ting Dang, Xinyu Zhang, Vassilis Kostakos, Michael J. Witbrock, Hong Jia  

**Link**: [PDF](https://arxiv.org/pdf/2509.07260)  

**Abstract**: Mobile and wearable healthcare monitoring play a vital role in facilitating timely interventions, managing chronic health conditions, and ultimately improving individuals' quality of life. Previous studies on large language models (LLMs) have highlighted their impressive generalization abilities and effectiveness in healthcare prediction tasks. However, most LLM-based healthcare solutions are cloud-based, which raises significant privacy concerns and results in increased memory usage and latency. To address these challenges, there is growing interest in compact models, Small Language Models (SLMs), which are lightweight and designed to run locally and efficiently on mobile and wearable devices. Nevertheless, how well these models perform in healthcare prediction remains largely unexplored. We systematically evaluated SLMs on health prediction tasks using zero-shot, few-shot, and instruction fine-tuning approaches, and deployed the best performing fine-tuned SLMs on mobile devices to evaluate their real-world efficiency and predictive performance in practical healthcare scenarios. Our results show that SLMs can achieve performance comparable to LLMs while offering substantial gains in efficiency and privacy. However, challenges remain, particularly in handling class imbalance and few-shot scenarios. These findings highlight SLMs, though imperfect in their current form, as a promising solution for next-generation, privacy-preserving healthcare monitoring. 

**Abstract (ZH)**: 移动和穿戴设备健康监测在促进及时干预、管理慢性健康状况并最终提高个体生活质量方面发挥着关键作用。尽管以往关于大型语言模型（LLMs）的研究强调了其在健康预测任务中的出色推广能力和有效性，但大多数基于LLM的健康解决方案都是基于云的，这引发了显著的隐私担忧，并导致内存使用量增加和延迟增大。为解决这些问题，人们对紧凑模型——小语言模型（SLMs）的兴趣日益浓厚，SLMs设计为可以在移动和穿戴设备上本地高效运行。然而，这些模型在健康预测方面的表现尚不清楚。我们系统地评估了SLMs在健康预测任务中的表现，使用零样本、少量样本和指令微调方法，并将表现最佳的微调SLMs部署到移动设备上，以评估其在实际健康护理场景中的效率和预测性能。结果显示，SLMs可以在保持与大型语言模型相当的性能的同时，在效率和隐私方面实现显著改进。然而，仍存在处理类别不平衡和少量样本场景的挑战。这些发现表明，尽管目前还不完美，SLMs仍是一个有前景的解决方案，用于下一代隐私保护健康监测。 

---
# OmniAcc: Personalized Accessibility Assistant Using Generative AI 

**Title (ZH)**: OmniAcc: 使用生成式人工智能的个性化无障碍助手 

**Authors**: Siddhant Karki, Ethan Han, Nadim Mahmud, Suman Bhunia, John Femiani, Vaskar Raychoudhury  

**Link**: [PDF](https://arxiv.org/pdf/2509.07220)  

**Abstract**: Individuals with ambulatory disabilities often encounter significant barriers when navigating urban environments due to the lack of accessible information and tools. This paper presents OmniAcc, an AI-powered interactive navigation system that utilizes GPT-4, satellite imagery, and OpenStreetMap data to identify, classify, and map wheelchair-accessible features such as ramps and crosswalks in the built environment. OmniAcc offers personalized route planning, real-time hands-free navigation, and instant query responses regarding physical accessibility. By using zero-shot learning and customized prompts, the system ensures precise detection of accessibility features, while supporting validation through structured workflows. This paper introduces OmniAcc and explores its potential to assist urban planners and mobility-aid users, demonstrated through a case study on crosswalk detection. With a crosswalk detection accuracy of 97.5%, OmniAcc highlights the transformative potential of AI in improving navigation and fostering more inclusive urban spaces. 

**Abstract (ZH)**: 基于GPT-4、卫星影像和OpenStreetMap数据的AI辅助无障碍导航系统OmniAcc：改善城市环境导航与包容性 

---
# BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions 

**Title (ZH)**: BlendedNet: 一体化机翼机身飞行器数据集及气动预测代理模型 

**Authors**: Nicholas Sung, Steven Spreizer, Mohamed Elrefaie, Kaira Samuel, Matthew C. Jones, Faez Ahmed  

**Link**: [PDF](https://arxiv.org/pdf/2509.07209)  

**Abstract**: BlendedNet is a publicly available aerodynamic dataset of 999 blended wing body (BWB) geometries. Each geometry is simulated across about nine flight conditions, yielding 8830 converged RANS cases with the Spalart-Allmaras model and 9 to 14 million cells per case. The dataset is generated by sampling geometric design parameters and flight conditions, and includes detailed pointwise surface quantities needed to study lift and drag. We also introduce an end-to-end surrogate framework for pointwise aerodynamic prediction. The pipeline first uses a permutation-invariant PointNet regressor to predict geometric parameters from sampled surface point clouds, then conditions a Feature-wise Linear Modulation (FiLM) network on the predicted parameters and flight conditions to predict pointwise coefficients Cp, Cfx, and Cfz. Experiments show low errors in surface predictions across diverse BWBs. BlendedNet addresses data scarcity for unconventional configurations and enables research on data-driven surrogate modeling for aerodynamic design. 

**Abstract (ZH)**: BlendedNet是公开可用的999种混合翼体（BWB）气动数据集，包含约九种飞行条件的仿真，生成8830个使用Spalart-Allmaras模型收敛的RANS案例，每案例包含9至14百万个细胞，并包含用于研究升力和阻力的详细表面点位量。还引入了一种端到端的代理框架进行点位气动预测。管道首先使用不变排列的PointNet回归器从采样的表面点云预测几何参数，然后根据预测的参数和飞行条件条件化Feature-wise Linear Modulation（FiLM）网络，预测点位系数Cp、Cfx和Cfz。实验显示在多种BWB表面预测中具有低误差。BlendedNet解决了非常规配置的数据稀缺问题，并促进了基于数据驱动的气动设计代理建模研究。 

---
# A Hybrid CNN-LSTM Deep Learning Model for Intrusion Detection in Smart Grid 

**Title (ZH)**: 智能电网中入侵检测的混合CNN-LSTM深层学习模型 

**Authors**: Abdulhakim Alsaiari, Mohammad Ilyas  

**Link**: [PDF](https://arxiv.org/pdf/2509.07208)  

**Abstract**: The evolution of the traditional power grid into the "smart grid" has resulted in a fundamental shift in energy management, which allows the integration of renewable energy sources with modern communication technology. However, this interconnection has increased smart grids' vulnerability to attackers, which might result in privacy breaches, operational interruptions, and massive outages. The SCADA-based smart grid protocols are critical for real-time data collection and control, but they are vulnerable to attacks like unauthorized access and denial of service (DoS). This research proposes a hybrid deep learning-based Intrusion Detection System (IDS) intended to improve the cybersecurity of smart grids. The suggested model takes advantage of Convolutional Neural Networks' (CNN) feature extraction capabilities as well as Long Short-Term Memory (LSTM) networks' temporal pattern recognition skills. DNP3 and IEC104 intrusion detection datasets are employed to train and test our CNN-LSTM model to recognize and classify the potential cyber threats. Compared to other deep learning approaches, the results demonstrate considerable improvements in accuracy, precision, recall, and F1-score, with a detection accuracy of 99.70%. 

**Abstract (ZH)**: 传统电网向“智能电网”的演进引发了能源管理的根本性转变，允许可再生能源源与现代通信技术的整合。然而，这种互联增加了智能电网对攻击者的脆弱性，可能导致隐私泄露、操作中断和大规模停电。基于SCADA的智能电网协议对于实时数据采集和控制至关重要，但易受未授权访问和拒绝服务（DoS）等攻击的影响。本研究提议了一种基于混合深度学习的入侵检测系统（IDS），旨在提高智能电网的网络安全性能。所建议的模型利用了卷积神经网络（CNN）的特征提取能力和长短期记忆网络（LSTM）的时间序列模式识别能力。使用DNP3和IEC104入侵检测数据集来训练和测试我们的CNN-LSTM模型，以识别和分类潜在的网络威胁。与其他深度学习方法相比，结果显示出在准确率、精确率、召回率和F1分数上的显著改进，检测准确率为99.70%。 

---
# That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral 

**Title (ZH)**: 那真时尚：为民事法律咨询和转介的LLM分类构建集成技术 

**Authors**: Quinten Steenhuis  

**Link**: [PDF](https://arxiv.org/pdf/2509.07170)  

**Abstract**: Each year millions of people seek help for their legal problems by calling a legal aid program hotline, walking into a legal aid office, or using a lawyer referral service. The first step to match them to the right help is to identify the legal problem the applicant is experiencing. Misdirection has consequences. Applicants may miss a deadline, experience physical abuse, lose housing or lose custody of children while waiting to connect to the right legal help. We introduce and evaluate the FETCH classifier for legal issue classification and describe two methods for improving accuracy: a hybrid LLM/ML ensemble classification method, and the automatic generation of follow-up questions to enrich the initial problem narrative. We employ a novel data set of 419 real-world queries to a nonprofit lawyer referral service. Ultimately, we show classification accuracy (hits@2) of 97.37\% using a mix of inexpensive models, exceeding the performance of the current state-of-the-art GPT-5 model. Our approach shows promise in significantly reducing the cost of guiding users of the legal system to the right resource for their problem while achieving high accuracy. 

**Abstract (ZH)**: 每年，数百万人通过拨打法律援助热线、前往法律援助办公室或使用律师推荐服务寻求帮助。识别申请人遇到的法律问题的第一步是将他们引导到合适的帮助。方向失误会产生后果。申请人可能会错过截止日期，经历身体虐待，失去住所或失去孩子的抚养权，直到他们能够连接到正确的法律帮助。我们介绍了并评价了FETCH分类器用于法律问题分类的方法，并描述了两种提高准确性的方法：混合大语言模型和机器学习的集成分类方法，以及自动生成跟进问题以丰富初始问题描述。我们使用了一个包含419个真实查询的新型数据集，这些查询是针对一家非营利律师推荐服务的。最终，我们展示了使用廉价模型混合的分类准确率（hits@2）为97.37%，超过了当前最先进的GPT-5模型的性能。我们的方法显示出在显著降低引导法律系统用户找到其问题的正确资源的成本方面具有巨大潜力，同时保持高准确率。 

---
# PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning 

**Title (ZH)**: PaVeRL-SQL: 通过部分匹配奖励和言语强化学习实现文本到SQL转换 

**Authors**: Heng Hao, Wenjun Hu, Oxana Verkholyak, Davoud Ataee Tarzanagh, Baruch Gutow, Sima Didari, Masoud Faraki, Hankyu Moon, Seungjai Min  

**Link**: [PDF](https://arxiv.org/pdf/2509.07159)  

**Abstract**: Text-to-SQL models allow users to interact with a database more easily by generating executable SQL statements from natural-language questions. Despite recent successes on simpler databases and questions, current Text-to-SQL methods still suffer from low execution accuracy on industry-scale databases and complex questions involving domain-specific business logic. We present \emph{PaVeRL-SQL}, a framework that combines \emph{Partial-Match Rewards} and \emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt two pipelines: (1) a newly designed in-context learning framework with group self-evaluation (verbal-RL), using capable open- and closed-source large language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL pipeline with a small backbone model (OmniSQL-7B) trained with a specially designed reward function and two-stage RL. These pipelines achieve state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks -- Spider, Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the verbal-RL pipeline achieves an execution accuracy 7.4\% higher than SOTA, and the CoT pipeline is 1.4\% higher. RL training with mixed SQL dialects yields strong, threefold gains, particularly for dialects with limited training data. Overall, \emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic industrial constraints. The code is available at this https URL. 

**Abstract (ZH)**: PaVeRL-SQL：结合部分匹配奖励和口头强化学习的Text-to-SQL框架 

---
# Autoencoder-Based Denoising of Muscle Artifacts in ECG to Preserve Skin Nerve Activity (SKNA) for Cognitive Stress Detection 

**Title (ZH)**: 基于自编码器的ECG肌肉伪迹去噪方法以保留皮肤神经活动（SKNA）用于认知压力检测 

**Authors**: Farnoush Baghestani, Jihye Moon, Youngsun Kong, Ki Chon  

**Link**: [PDF](https://arxiv.org/pdf/2509.07146)  

**Abstract**: The sympathetic nervous system (SNS) plays a central role in regulating the body's responses to stress and maintaining physiological stability. Its dysregulation is associated with a wide range of conditions, from cardiovascular disease to anxiety disorders. Skin nerve activity (SKNA) extracted from high-frequency electrocardiogram (ECG) recordings provides a noninvasive window into SNS dynamics, but its measurement is highly susceptible to electromyographic (EMG) contamination. Traditional preprocessing based on bandpass filtering within a fixed range (e.g., 500--1000 Hz) is susceptible to overlapping EMG and SKNA spectral components, especially during sustained muscle activity. We present a denoising approach using a lightweight one-dimensional convolutional autoencoder with a long short-term memory (LSTM) bottleneck to reconstruct clean SKNA from EMG-contaminated recordings. Using clean ECG-derived SKNA data from cognitive stress experiments and EMG noise from chaotic muscle stimulation recordings, we simulated contamination at realistic noise levels (--4 dB, --8 dB signal-to-noise ratio) and trained the model in the leave-one-subject-out cross-validation framework. The method improved signal-to-noise ratio by up to 9.65 dB, increased cross correlation with clean SKNA from 0.40 to 0.72, and restored burst-based SKNA features to near-clean discriminability (AUROC $\geq$ 0.96). Classification of baseline versus sympathetic stimulation (cognitive stress) conditions reached accuracies of 91--98\% across severe noise levels, comparable to clean data. These results demonstrate that deep learning--based reconstruction can preserve physiologically relevant sympathetic bursts during substantial EMG interference, enabling more robust SKNA monitoring in naturalistic, movement-rich environments. 

**Abstract (ZH)**: 自主神经系统的调节在应对压力和维持生理稳定性中起着中心作用。其功能失调与从心血管疾病到焦虑障碍等多种情况相关。源自高频心电图（ECG）记录的皮肤神经活动（SKNA）提供了一种无创的自主神经系统动态窗口，但其测量极易受到肌电图（EMG）污染的影响。传统的基于固定范围（例如500–1000 Hz）带通滤波的预处理方法容易导致EMG和SKNA频谱成分重叠，尤其是在持续肌肉活动期间。我们提出了一种使用轻量级一维卷积自编码器结合长短期记忆（LSTM）瓶颈的方法来从受EMG污染的记录中重构清洁的SKNA。利用认知压力实验中获得的清洁ECG衍生SKNA数据和来自主神经激发的混沌肌肉刺激记录中的EMG噪声，我们模拟了在实际噪声水平（–4 dB，–8 dB信噪比）下的污染，并在单被试剔除的交叉验证框架中训练了该模型。该方法将信噪比提高了最多9.65 dB，交叉相关系数从0.40提高到0.72，并恢复了突发性SKNA特征的近清洁区分能力（AUROC ≥ 0.96）。在极端噪声水平下，基线与自主神经刺激（认知压力）条件的分类准确率达到91–98%，与清洁数据相当。这些结果表明，基于深度学习的重构可以在大量EMG干扰下保留生理相关的自主神经元突发，从而在自然、富含运动的环境中实现更稳健的SKNA监测。 

---
# Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis 

**Title (ZH)**: 神经符号框架：概念特征与实证比较分析 

**Authors**: Sania Sinha, Tanawan Premsri, Danial Kamali, Parisa Kordjamshidi  

**Link**: [PDF](https://arxiv.org/pdf/2509.07122)  

**Abstract**: Neurosymbolic (NeSy) frameworks combine neural representations and learning with symbolic representations and reasoning. Combining the reasoning capacities, explainability, and interpretability of symbolic processing with the flexibility and power of neural computing allows us to solve complex problems with more reliability while being data-efficient. However, this recently growing topic poses a challenge to developers with its learning curve, lack of user-friendly tools, libraries, and unifying frameworks. In this paper, we characterize the technical facets of existing NeSy frameworks, such as the symbolic representation language, integration with neural models, and the underlying algorithms. A majority of the NeSy research focuses on algorithms instead of providing generic frameworks for declarative problem specification to leverage problem solving. To highlight the key aspects of Neurosymbolic modeling, we showcase three generic NeSy frameworks - \textit{DeepProbLog}, \textit{Scallop}, and \textit{DomiKnowS}. We identify the challenges within each facet that lay the foundation for identifying the expressivity of each framework in solving a variety of problems. Building on this foundation, we aim to spark transformative action and encourage the community to rethink this problem in novel ways. 

**Abstract (ZH)**: 神经符号（NeSy）框架结合了神经表示和学习与符号表示和推理。将符号处理的推理能力、可解释性和可解释性与神经计算的灵活性和强大功能相结合，使我们能够更加可靠地解决复杂问题，同时保持数据高效性。然而，这一近期迅速发展的领域对开发者提出了挑战，尤其是在学习曲线、缺少用户友好工具、库和统一框架方面。在本文中，我们分析现有NeSy框架的技术方面，如符号表示语言、与神经模型的集成以及底层算法。大多数NeSy研究关注算法，而不是提供通用框架以通过声明性问题规范来利用解决问题的能力。为了突出神经符号建模的关键方面，我们展示了三个通用的NeSy框架——DeepProbLog、Scallop和DomiKnowS。我们识别出每个方面内的挑战，为进一步识别每个框架在解决各种问题时的表达能力奠定了基础。基于这些基础，我们旨在激发变革性行动，并鼓励社区以新颖的方式重新思考这个问题。 

---
# Instruction Agent: Enhancing Agent with Expert Demonstration 

**Title (ZH)**: 专家演示增强的指令代理 

**Authors**: Yinheng Li, Hailey Hultquist, Justin Wagle, Kazuhito Koishida  

**Link**: [PDF](https://arxiv.org/pdf/2509.07098)  

**Abstract**: Graphical user interface (GUI) agents have advanced rapidly but still struggle with complex tasks involving novel UI elements, long-horizon actions, and personalized trajectories. In this work, we introduce Instruction Agent, a GUI agent that leverages expert demonstrations to solve such tasks, enabling completion of otherwise difficult workflows. Given a single demonstration, the agent extracts step-by-step instructions and executes them by strictly following the trajectory intended by the user, which avoids making mistakes during execution. The agent leverages the verifier and backtracker modules further to improve robustness. Both modules are critical to understand the current outcome from each action and handle unexpected interruptions(such as pop-up windows) during execution. Our experiments show that Instruction Agent achieves a 60% success rate on a set of tasks in OSWorld that all top-ranked agents failed to complete. The Instruction Agent offers a practical and extensible framework, bridging the gap between current GUI agents and reliable real-world GUI task automation. 

**Abstract (ZH)**: 图形用户界面（GUI）代理已取得快速进展，但仍难以应对涉及新型UI元素、长期动作及个性化轨迹的复杂任务。本文介绍了指令代理（Instruction Agent），这是一种利用专家示范来解决此类任务的GUI代理，能够完成原本难以实现的工作流程。给定单个示范，代理提取逐步指令并严格遵循用户的意图执行，从而避免执行过程中的错误。代理还利用校验器和回溯器模块以提高鲁棒性。这两个模块对于理解每项操作的当前结果并处理执行过程中的意外中断（如弹出窗口）至关重要。我们的实验表明，指令代理在OSWorld中实现了60%的成功率，而所有顶级代理都无法完成这些任务。指令代理提供了一个实用且可扩展的框架，填补了当前GUI代理与可靠的真实世界GUI任务自动化之间的差距。 

---
# Statistical Methods in Generative AI 

**Title (ZH)**: 生成式AI中的统计方法 

**Authors**: Edgar Dobriban  

**Link**: [PDF](https://arxiv.org/pdf/2509.07054)  

**Abstract**: Generative Artificial Intelligence is emerging as an important technology, promising to be transformative in many areas. At the same time, generative AI techniques are based on sampling from probabilistic models, and by default, they come with no guarantees about correctness, safety, fairness, or other properties. Statistical methods offer a promising potential approach to improve the reliability of generative AI techniques. In addition, statistical methods are also promising for improving the quality and efficiency of AI evaluation, as well as for designing interventions and experiments in AI.
In this paper, we review some of the existing work on these topics, explaining both the general statistical techniques used, as well as their applications to generative AI. We also discuss limitations and potential future directions. 

**Abstract (ZH)**: 生成式人工智能正 emerge 作为一项重要技术，有望在许多领域产生变革性影响。同时，生成式 AI 技术基于从概率模型中抽样， 默认情况下，它们缺乏关于正确性、安全性、公平性或其他属性的保证。统计方法为提高生成式 AI 技术的可靠性提供了有希望的方法。此外，统计方法也有望提高人工智能评估的质量和效率，并为人工智能的设计干预和实验提供支持。在本文中，我们回顾了这些主题的一些现有工作，解释了所使用的一般统计技术及其在生成式 AI 中的应用，并讨论了限制和潜在的未来方向。 

---
# From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning 

**Title (ZH)**: 从特征模态到证明：将图谱运算与符号可解释推理集成 

**Authors**: Andrew Kiruluta, Priscilla Burity  

**Link**: [PDF](https://arxiv.org/pdf/2509.07017)  

**Abstract**: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning framework that embeds logical rules as spectral templates and performs inference directly in the graph spectral domain. By leveraging graph signal processing (GSP) and frequency-selective filters grounded in the Laplacian eigenstructure of knowledge graphs, the architecture unifies the interpretability of symbolic reasoning with the scalability and adaptability of spectral learning. Beyond the core formulation, we incorporate a comprehensive set of extensions, including dynamic graph and basis learning, rational and diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts for modular specialization, proof-guided training with spectral curricula, and uncertainty quantification for calibrated confidence. Additional enhancements such as large language model coupling, co-spectral transfer alignment, adversarial robustness, efficient GPU kernels, generalized Laplacians, and causal interventions further expand the versatility of the framework.
Empirical evaluation on state-of-the-art reasoning benchmarks such as ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior accuracy, faster inference, improved robustness to adversarial perturbations, and higher interpretability compared to leading baselines including transformers, message-passing neural networks, and neuro-symbolic logic programming systems. Spectral attribution and proof-band agreement analyses confirm that model decisions align closely with symbolic proof structures, while transfer experiments validate effective domain adaptation through co-spectral alignment. These results establish Spectral NSR as a scalable and principled foundation for the next generation of reasoning systems, offering transparency, robustness, and generalization beyond conventional approaches. 

**Abstract (ZH)**: Spectral NSR：一种基于光谱的神经符号推理框架 

---
# Renewable Energy Sources Selection Analysis with the Maximizing Deviation Method 

**Title (ZH)**: 基于最大化偏差方法的可再生能源选择分析 

**Authors**: Kirisci Murat  

**Link**: [PDF](https://arxiv.org/pdf/2509.07011)  

**Abstract**: Multi-criteria decision-making methods provide decision-makers with appropriate tools to make better decisions in uncertain, complex, and conflicting situations. Fuzzy set theory primarily deals with the uncertainty inherent in human thoughts and perceptions and attempts to quantify this uncertainty. Fuzzy logic and fuzzy set theory are utilized with multi-criteria decision-making methods because they effectively handle uncertainty and fuzziness in decision-makers' judgments, allowing for verbal judgments of the problem. This study utilizes the Fermatean fuzzy environment, a generalization of fuzzy sets. An optimization model based on the deviation maximization method is proposed to determine partially known feature weights. This method is combined with interval-valued Fermatean fuzzy sets. The proposed method was applied to the problem of selecting renewable energy sources. The reason for choosing renewable energy sources is that meeting energy needs from renewable sources, balancing carbon emissions, and mitigating the effects of global climate change are among the most critical issues of the recent period. Even though selecting renewable energy sources is a technical issue, the managerial and political implications of this issue are also important, and are discussed in this study. 

**Abstract (ZH)**: 多准则决策方法为决策者在不确定、复杂和冲突的情况下提供了合适的工具。模糊集理论主要处理人类思想和感知中固有的不确定性，并试图量化这种不确定性。由于模糊逻辑和模糊集理论能够有效处理决策者判断中的不确定性和模糊性，并允许对问题进行语义判断，因此它们与多准则决策方法结合使用。本文利用Fermatean模糊环境，这是模糊集的一种推广。提出了一种基于偏差最大化方法的优化模型来确定部分已知特征权重。该方法结合了区间值Fermatean模糊集，所提出的方法应用于可再生能源选择问题。选择可再生能源的原因在于，满足可再生能源的需求、平衡碳排放以及减轻全球气候变暖的影响是当今最关键的问题之一。尽管选择可再生能源是一个技术问题，但这一问题的管理与政治影响也很重要，并在本文中进行了讨论。 

---
# Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search 

**Title (ZH)**: Mini-o3: 扩大规模以增强视觉搜索中的推理模式和交互轮次 

**Authors**: Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, Hengshuang Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2509.07969)  

**Abstract**: Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems. 

**Abstract (ZH)**: 近期大规模多模态模型的发展已经利用基于图像的工具与强化学习相结合来解决视觉问题。然而，现有的开源方法往往表现出单调的推理模式，并且只允许有限的交互回合次数，这使得它们对于需要试错探索的任务显得不足。在本工作中，我们通过扩展基于工具的交互来解决这一限制，并引入了Mini-o3系统，该系统执行深度、多回合推理——跨越数十个步骤——并在具有挑战性的视觉搜索任务上实现了最先进的性能。我们重现OpenAI o3风格行为的配方包括三个关键组件。首先，我们构建了Visual Probe Dataset，这是一个包含数千个旨在用于探索性推理的具有挑战性的视觉搜索问题集合。其次，我们开发了迭代数据收集管道，以获取冷启动轨迹，其中包括深度优先搜索、试错和目标维持等多样化的推理模式。第三，我们提出了超轮次遮掩策略，在强化学习过程中防止对达到最大轮次数的响应进行惩罚，从而在训练效率与测试可扩展性之间保持平衡。尽管仅使用上限为六次交互轮次进行训练，但我们的模型在推理时能够自然扩展到数十次轮次，并且随着轮次数的增加，准确性也会提高。深入的实验表明，Mini-o3生成了丰富的推理模式和深层次的思考路径，有效地解决了具有挑战性的视觉搜索问题。 

---
# ACE and Diverse Generalization via Selective Disagreement 

**Title (ZH)**: ACE和通过选择性分歧实现的多样泛化 

**Authors**: Oliver Daniels, Stuart Armstrong, Alexandre Maranhão, Mahirah Fairuz Rahman, Benjamin M. Marlin, Rebecca Gorman  

**Link**: [PDF](https://arxiv.org/pdf/2509.07955)  

**Abstract**: Deep neural networks are notoriously sensitive to spurious correlations - where a model learns a shortcut that fails out-of-distribution. Existing work on spurious correlations has often focused on incomplete correlations,leveraging access to labeled instances that break the correlation. But in cases where the spurious correlations are complete, the correct generalization is fundamentally \textit{underspecified}. To resolve this underspecification, we propose learning a set of concepts that are consistent with training data but make distinct predictions on a subset of novel unlabeled inputs. Using a self-training approach that encourages \textit{confident} and \textit{selective} disagreement, our method ACE matches or outperforms existing methods on a suite of complete-spurious correlation benchmarks, while remaining robust to incomplete spurious correlations. ACE is also more configurable than prior approaches, allowing for straight-forward encoding of prior knowledge and principled unsupervised model selection. In an early application to language-model alignment, we find that ACE achieves competitive performance on the measurement tampering detection benchmark \textit{without} access to untrusted measurements. While still subject to important limitations, ACE represents significant progress towards overcoming underspecification. 

**Abstract (ZH)**: 深度神经网络对虚假相关性特别敏感——模型在此类相关性失效时学到的捷径。现有的虚假相关性研究往往集中在不完整的相关性上，利用标记的实例来打破相关性。但在虚假相关性完整的情况下，正确的泛化本质上是不充分指定的。为了解决这个不充分指定的问题，我们提出学习一组与训练数据一致但对部分新型未标记输入做出不同预测的概念。通过鼓励信心十足且有选择地产生分歧的自我训练方法，我们的方法ACE在一系列完整的虚假相关性基准测试中表现出色或优于现有方法，同时对不完整的虚假相关性保持鲁棒性。ACE比先前的方法更具可配置性，允许简单地编码先验知识并进行稳健的无监督模型选择。在语言模型对齐的早期应用中，我们发现ACE能够在无需访问不可信测量值的情况下，在测量篡改检测基准测试中取得竞争性的性能。虽然仍然存在重要的限制，但ACE代表了克服不充分指定的重要进展。 

---
# Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges 

**Title (ZH)**: 将多模态多任务联邦基础模型引入教育领域：前景与挑战 

**Authors**: Kasra Borazjani, Naji Khosravan, Rajeev Sahay, Bita Akram, Seyyedali Hosseinalipour  

**Link**: [PDF](https://arxiv.org/pdf/2509.07946)  

**Abstract**: Multi-modal multi-task (M3T) foundation models (FMs) have recently shown transformative potential in artificial intelligence, with emerging applications in education. However, their deployment in real-world educational settings is hindered by privacy regulations, data silos, and limited domain-specific data availability. We introduce M3T Federated Foundation Models (FedFMs) for education: a paradigm that integrates federated learning (FL) with M3T FMs to enable collaborative, privacy-preserving training across decentralized institutions while accommodating diverse modalities and tasks. Subsequently, this position paper aims to unveil M3T FedFMs as a promising yet underexplored approach to the education community, explore its potentials, and reveal its related future research directions. We outline how M3T FedFMs can advance three critical pillars of next-generation intelligent education systems: (i) privacy preservation, by keeping sensitive multi-modal student and institutional data local; (ii) personalization, through modular architectures enabling tailored models for students, instructors, and institutions; and (iii) equity and inclusivity, by facilitating participation from underrepresented and resource-constrained entities. We finally identify various open research challenges, including studying of (i) inter-institution heterogeneous privacy regulations, (ii) the non-uniformity of data modalities' characteristics, (iii) the unlearning approaches for M3T FedFMs, (iv) the continual learning frameworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must be collectively addressed for practical deployment. 

**Abstract (ZH)**: 教育领域的多模态多任务联邦基础模型（M3T FedFMs） 

---
# ImportSnare: Directed "Code Manual" Hijacking in Retrieval-Augmented Code Generation 

**Title (ZH)**: ImportSnare: 有向“代码手册”在网络检索增强代码生成中的劫持 

**Authors**: Kai Ye, Liangcai Su, Chenxiong Qian  

**Link**: [PDF](https://arxiv.org/pdf/2509.07941)  

**Abstract**: Code generation has emerged as a pivotal capability of Large Language Models(LLMs), revolutionizing development efficiency for programmers of all skill levels. However, the complexity of data structures and algorithmic logic often results in functional deficiencies and security vulnerabilities in generated code, reducing it to a prototype requiring extensive manual debugging. While Retrieval-Augmented Generation (RAG) can enhance correctness and security by leveraging external code manuals, it simultaneously introduces new attack surfaces.
In this paper, we pioneer the exploration of attack surfaces in Retrieval-Augmented Code Generation (RACG), focusing on malicious dependency hijacking. We demonstrate how poisoned documentation containing hidden malicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting dual trust chains: LLM reliance on RAG and developers' blind trust in LLM suggestions. To construct poisoned documents, we propose ImportSnare, a novel attack framework employing two synergistic strategies: 1)Position-aware beam search optimizes hidden ranking sequences to elevate poisoned documents in retrieval results, and 2)Multilingual inductive suggestions generate jailbreaking sequences to manipulate LLMs into recommending malicious dependencies. Through extensive experiments across Python, Rust, and JavaScript, ImportSnare achieves significant attack success rates (over 50% for popular libraries such as matplotlib and seaborn) in general, and is also able to succeed even when the poisoning ratio is as low as 0.01%, targeting both custom and real-world malicious packages. Our findings reveal critical supply chain risks in LLM-powered development, highlighting inadequate security alignment for code generation tasks. To support future research, we will release the multilingual benchmark suite and datasets. The project homepage is this https URL. 

**Abstract (ZH)**: Retrieval-Augmented Code Generation的安全威胁探索：关注恶意依赖劫持 

---
# Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation 

**Title (ZH)**: 用AI打破Android：基于LLM的利用深度探究 

**Authors**: Wanni Vidulige Ishan Perera, Xing Liu, Fan liang, Junyi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07933)  

**Abstract**: The rapid evolution of Artificial Intelligence (AI) and Large Language Models (LLMs) has opened up new opportunities in the area of cybersecurity, especially in the exploitation automation landscape and penetration testing. This study explores Android penetration testing automation using LLM-based tools, especially PentestGPT, to identify and execute rooting techniques. Through a comparison of the traditional manual rooting process and exploitation methods produced using AI, this study evaluates the efficacy, reliability, and scalability of automated penetration testing in achieving high-level privilege access on Android devices. With the use of an Android emulator (Genymotion) as the testbed, we fully execute both traditional and exploit-based rooting methods, automating the process using AI-generated scripts. Secondly, we create a web application by integrating OpenAI's API to facilitate automated script generation from LLM-processed responses. The research focuses on the effectiveness of AI-enabled exploitation by comparing automated and manual penetration testing protocols, by determining LLM weaknesses and strengths along the way. We also provide security suggestions of AI-enabled exploitation, including ethical factors and potential misuse. The findings exhibit that while LLMs can significantly streamline the workflow of exploitation, they need to be controlled by humans to ensure accuracy and ethical application. This study adds to the increasing body of literature on AI-powered cybersecurity and its effect on ethical hacking, security research, and mobile device security. 

**Abstract (ZH)**: 人工智能（AI）和大型语言模型（LLMs）的快速进化在网络安全领域开辟了新机会，特别是在利用自动化和渗透测试领域。本研究探讨了使用基于LLM的工具（特别是PentestGPT）进行Android渗透测试自动化，以识别和执行提权技术。通过传统手动提权过程与AI生成的利用方法的对比，本研究评估了自动化渗透测试在获取Android设备高级权限方面的有效性和可靠性及可扩展性。使用Genymotion作为测试环境，我们完整执行了传统和利用基于的提权方法，并使用AI生成的脚本自动化这些过程。其次，我们创建了一个web应用程序，通过集成OpenAI的API，以便从LLM处理的响应生成自动化脚本。本研究侧重于通过比较自动化和手动渗透测试协议来评估AI辅助利用的有效性，同时确定LLM的优势和弱点。此外，我们还提供了AI辅助利用的安全建议，包括伦理因素和潜在误用。研究结果表明，尽管LLMs能够显著简化利用工作流程，但必须由人类控制以确保准确性和伦理应用。本研究增加了关于AI驱动网络安全及其对伦理 hacking、安全研究和移动设备安全影响的文献。 

---
# Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s 

**Title (ZH)**: 面向消费者GPU的YOLOv10s本地AI加速：一种硬件感知动态策略 

**Authors**: Mahmudul Islam Masum, Miad Islam, Arif I. Sarwat  

**Link**: [PDF](https://arxiv.org/pdf/2509.07928)  

**Abstract**: As local AI grows in popularity, there is a critical gap between the benchmark performance of object detectors and their practical viability on consumer-grade hardware. While models like YOLOv10s promise real-time speeds, these metrics are typically achieved on high-power, desktop-class GPUs. This paper reveals that on resource-constrained systems, such as laptops with RTX 4060 GPUs, performance is not compute-bound but is instead dominated by system-level bottlenecks, as illustrated by a simple bottleneck test. To overcome this hardware-level constraint, we introduce a Two-Pass Adaptive Inference algorithm, a model-independent approach that requires no architectural changes. This study mainly focuses on adaptive inference strategies and undertakes a comparative analysis of architectural early-exit and resolution-adaptive routing, highlighting their respective trade-offs within a unified evaluation framework. The system uses a fast, low-resolution pass and only escalates to a high-resolution model pass when detection confidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x speedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%. This work provides a practical and reproducible blueprint for deploying high-performance, real-time AI on consumer-grade devices by shifting the focus from pure model optimization to hardware-aware inference strategies that maximize throughput. 

**Abstract (ZH)**: 随着本地AI的流行，目标检测器在基准性能和消费级硬件上的实际可行性之间存在关键差距。尽管像YOLOv10s这样的模型承诺实现实时速度，但这些指标通常是在高性能的台式机级GPU上达成的。本文揭示，在资源受限的系统上，如配备RTX 4060 GPU的笔记本电脑，性能不是由计算能力决定的，而是受系统级瓶颈主导，这一点通过一个简单的瓶颈测试得到说明。为克服这一硬件限制，我们提出了一种两阶段自适应推理算法，这是一种模型无关的方法，不需要架构修改。本研究主要集中于自适应推理策略，并在统一的评估框架下对架构早期退出和分辨率自适应路由进行了比较分析，突显了它们各自的权衡。系统使用快速低分辨率通道，在检测置信度低时才升级到高分辨率模型通道。在5000张图片的COCO数据集上，我们的方法相对于PyTorch早期退出基线实现了1.85倍的速度提升，同时保持了轻微的mAP损失（5.51%）。这项工作提供了一种实用且可重现的蓝图，通过将注意力从纯粹的模型优化转向最大化吞吐量的硬件感知推理策略，使高性能实时AI能够在消费级设备上部署。 

---
# GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models 

**Title (ZH)**: GENUINE: 图络增强多层级不确定性估计large语言模型 

**Authors**: Tuo Wang, Adithya Kulkarni, Tyler Cody, Peter A. Beling, Yujun Yan, Dawei Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2509.07925)  

**Abstract**: Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at this https URL. 

**Abstract (ZH)**: 结构aware的Large Language Models不确定性估计：基于图的多层级不确定量化评估（GENUINE） 

---
# Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation 

**Title (ZH)**: 基于CBCT和IOS的多模态对比预训练以增强牙齿分割 

**Authors**: Moo Hyun Son, Juyoung Bae, Zelin Qiu, Jiale Peng, Kai Xin Li, Yifan Lin, Hao Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.07923)  

**Abstract**: Digital dentistry represents a transformative shift in modern dental practice. The foundational step in this transformation is the accurate digital representation of the patient's dentition, which is obtained from segmented Cone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the growing interest in digital dental technologies, existing segmentation methodologies frequently lack rigorous validation and demonstrate limited performance and clinical applicability. To the best of our knowledge, this is the first work to introduce a multimodal pretraining framework for tooth segmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for pretraining that integrates volumetric (CBCT) and surface-based (IOS) modalities. By capturing modality-invariant representations through multimodal contrastive learning, our approach effectively models fine-grained anatomical features, enabling precise multi-class segmentation and accurate identification of Fédération Dentaire Internationale (FDI) tooth numbering. Along with the framework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to date, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive collection of independent datasets, representing the largest and most diverse evaluation to date. Our method achieves state-of-the-art performance in both internal and external testing, with an increase of 12\% for CBCT segmentation and 8\% for IOS segmentation in the Dice Similarity Coefficient (DSC). Furthermore, ToothMCL consistently surpasses existing approaches in tooth groups and demonstrates robust generalizability across varying imaging conditions and clinical scenarios. 

**Abstract (ZH)**: 数字牙科代表了现代牙科实践中的变革性转变。在此转变的基础步骤是通过分割锥形束计算机断层扫描（CBCT）和口腔扫描（IOS）准确地数字化表示患者的牙齿。尽管对数字牙科技术的兴趣逐渐增加，但现有的分割方法往往缺乏严格的验证，且表现和临床应用有限。据我们所知，这是首次引入多模态预训练框架进行牙齿分割的工作。我们提出了ToothMCL，一种结合体素（CBCT）和表面（IOS）模态的牙齿多模态对比学习预训练方法。通过多模态对比学习捕捉模态不变的表示，我们的方法能够有效建模精细结构的解剖特征，实现精确的多分类分割，并准确识别国际牙科联盟（FDI）牙齿编号。除框架外，我们还整理了迄今为止最大的配对CBCT-IOS3.8K数据集，包含3,867例患者。然后，我们在一系列独立数据集上评估了ToothMCL，这些数据集是迄今为止最大、最多样化的评价集合。我们的方法在内部分割和外部分割测试中均达到了最先进的性能，其中CBCT分割和IOS分割的骰子相似系数（DSC）分别提高了12%和8%。此外，ToothMCL在牙齿组别上始终超越现有方法，并在不同成像条件和临床场景下表现出强大的泛化能力。 

---
# Uncovering Scaling Laws for Large Language Models via Inverse Problems 

**Title (ZH)**: 通过逆问题揭示大型语言模型的尺度定律 

**Authors**: Arun Verma, Zhaoxuan Wu, Zijian Zhou, Xiaoqiang Lin, Zhiliang Chen, Rachael Hwee Ling Sim, Rui Qiao, Jingtan Wang, Nhung Bui, Xinyuan Niu, Wenyang Hu, Gregory Kang Ruey Lau, Zi-Yu Khoo, Zitong Zhao, Xinyi Xu, Apivich Hemachandra, See-Kiong Ng, Bryan Kian Hsiang Low  

**Link**: [PDF](https://arxiv.org/pdf/2509.07909)  

**Abstract**: Large Language Models (LLMs) are large-scale pretrained models that have achieved remarkable success across diverse domains. These successes have been driven by unprecedented complexity and scale in both data and computations. However, due to the high costs of training such models, brute-force trial-and-error approaches to improve LLMs are not feasible. Inspired by the success of inverse problems in uncovering fundamental scientific laws, this position paper advocates that inverse problems can also efficiently uncover scaling laws that guide the building of LLMs to achieve the desirable performance with significantly better cost-effectiveness. 

**Abstract (ZH)**: 大型语言模型（LLMs）是大规模预训练模型，在多个领域取得了显著成功。这些成功得益于数据和计算前所未有的复杂性和规模。然而，由于训练此类模型成本高昂，通过蛮力试错方法改进LLMs不可行。受逆问题在揭示基本科学规律方面成功应用的启发，本文主张逆问题也可以高效地揭示指导构建LLMs以实现期望性能并大幅提高成本效益的缩放规律。 

---
# Active Membership Inference Test (aMINT): Enhancing Model Auditability with Multi-Task Learning 

**Title (ZH)**: 活性成员推断测试 (aMINT): 通过多任务学习增强模型可审核性 

**Authors**: Daniel DeAlcala, Aythami Morales, Julian Fierrez, Gonzalo Mancera, Ruben Tolosana, Javier Ortega-Garcia  

**Link**: [PDF](https://arxiv.org/pdf/2509.07879)  

**Abstract**: Active Membership Inference Test (aMINT) is a method designed to detect whether given data were used during the training of machine learning models. In Active MINT, we propose a novel multitask learning process that involves training simultaneously two models: the original or Audited Model, and a secondary model, referred to as the MINT Model, responsible for identifying the data used for training the Audited Model. This novel multi-task learning approach has been designed to incorporate the auditability of the model as an optimization objective during the training process of neural networks. The proposed approach incorporates intermediate activation maps as inputs to the MINT layers, which are trained to enhance the detection of training data. We present results using a wide range of neural networks, from lighter architectures such as MobileNet to more complex ones such as Vision Transformers, evaluated in 5 public benchmarks. Our proposed Active MINT achieves over 80% accuracy in detecting if given data was used for training, significantly outperforming previous approaches in the literature. Our aMINT and related methodological developments contribute to increasing transparency in AI models, facilitating stronger safeguards in AI deployments to achieve proper security, privacy, and copyright protection. 

**Abstract (ZH)**: 活性成员推理测试 (aMINT) 是一种用于检测给定数据是否在机器学习模型训练中使用的方法。在活性MINT中，我们提出了一种新颖的多任务学习过程，涉及同时训练两个模型：原始或审核模型，以及一个称为MINT模型的次要模型，负责识别用于训练审核模型的数据。该新颖的多任务学习方法在神经网络训练过程中设计为将模型的可审核性作为优化目标进行整合。提出的approach将中间激活图作为MINT层的输入，这些层经过训练以增强对训练数据的检测。我们展示了使用从较轻的架构如MobileNet到更复杂的如视觉变换器的各种神经网络的结果，这些网络在5个公开基准上进行了评估。我们提出的活性MINT在检测给定数据是否用于训练时准确率超过80%，显著优于文献中的先前方法。我们的活性MINT及相关方法的发展为增加AI模型的透明度作出了贡献，有助于在AI部署中增强安全、隐私和版权保护的保障措施。 

---
# Deep Learning-Based Burned Area Mapping Using Bi-Temporal Siamese Networks and AlphaEarth Foundation Datasets 

**Title (ZH)**: 基于双时相Siamese网络的深度学习烧伤区制图——AlphaEarth基础数据集应用 

**Authors**: Seyd Teymoor Seydi  

**Link**: [PDF](https://arxiv.org/pdf/2509.07852)  

**Abstract**: Accurate and timely mapping of burned areas is crucial for environmental monitoring, disaster management, and assessment of climate change. This study presents a novel approach to automated burned area mapping using the AlphaEArth dataset combined with the Siamese U-Net deep learning architecture. The AlphaEArth Dataset, comprising high-resolution optical and thermal infrared imagery with comprehensive ground-truth annotations, provides an unprecedented resource for training robust burned area detection models. We trained our model with the Monitoring Trends in Burn Severity (MTBS) dataset in the contiguous US and evaluated it with 17 regions cross in Europe. Our experimental results demonstrate that the proposed ensemble approach achieves superior performance with an overall accuracy of 95%, IoU of 0.6, and F1-score of 74% on the test dataset. The model successfully identifies burned areas across diverse ecosystems with complex background, showing particular strength in detecting partially burned vegetation and fire boundaries and its transferability and high generalization in burned area mapping. This research contributes to the advancement of automated fire damage assessment and provides a scalable solution for global burn area monitoring using the AlphaEarth dataset. 

**Abstract (ZH)**: 准确及时的火烧区域 mapping 对环境监测、灾害管理及气候变化评估至关重要。本研究提出了一种新的自动火烧区域 mapping 方法，利用 AlphaEArth 数据集结合 Siamese U-Net 深度学习架构。AlphaEArth 数据集包含高分辨率的光学和热红外影像，并伴有全面的地表真实标注，提供了训练 robust 火烧区域检测模型的前所未有的资源。我们使用 Monitoring Trends in Burn Severity (MTBS) 数据集在北美进行了模型训练，并在欧洲 17 个地区进行了评估。实验结果表明，提出的集成方法在测试数据集上实现了总体准确率 95%、IoU 0.6 和 F1 分数 74%，成功识别了复杂背景下的多种生态系统中的火烧区域，特别擅长检测部分火烧植被和火边界，并展示了在火烧区域 mapping 中的可转移性和高泛化能力。本研究促进了自动化火灾损害评估，并提供了使用 AlphaEarth 数据集进行全球火烧区域监测的可扩展解决方案。 

---
# Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost 

**Title (ZH)**: 小规模开放模型在低资源文学翻译中的性能接近大规模模型，并且成本仅为后者的几分之一。 

**Authors**: Mihai Nadas, Laura Diosan, Andreea Tomescu, Andrei Piscoran  

**Link**: [PDF](https://arxiv.org/pdf/2509.07829)  

**Abstract**: Literary translation has recently gained attention as a distinct and complex task in machine translation research. However, the translation by small open models remains an open problem. We contribute to this ongoing research by introducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for dataset creation, fine tuning, and evaluation in English-Romanian literary translations, centred on the creation and open release of both a compact, fine tuned language model (TF2-12B) and large scale synthetic parallel datasets (DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the largest collection of synthetic English fables to date, we address the need for rich, high quality literary datasets in low resource languages such as Romanian. Our pipeline first generates 15k high quality Romanian references from the TF1 pool using a high performing LLM. We then apply a two stage fine tuning process to a 12B parameter open weight model: (i) instruction tuning to capture genre specific narrative style, and (ii) adapter compression for efficient deployment. Evaluation combines corpus level BLEU and a five dimension LLM based rubric (accuracy, fluency, coherence, style, cultural adaptation) to provide a nuanced assessment of translation quality. Results show that our fine tuned model achieves fluency and adequacy competitive with top performing large proprietary models, while being open, accessible, and significantly more cost effective. Alongside the fine tuned model and both datasets, we publicly release all scripts and evaluation prompts. TF2 thus provides an end-to-end, reproducible pipeline for research on cost efficient translation, cross lingual narrative generation, and the broad adoption of open models for culturally significant literary content in low resource settings. 

**Abstract (ZH)**: 文学翻译近年来已成为机器翻译研究中一个独特而复杂的任务，然而由小开放模型进行的翻译仍是一个待解决问题。我们通过引入TINYFABULIST翻译框架（TF2）， contributions to this ongoing research，该框架专注于创建和公开发布一个紧凑的微调语言模型（TF2-12B）以及大规模合成平行数据集（DS-TF2-EN-RO-3M和DS-TF2-EN-RO-15K），从而推动了这一研究。基于迄今为止最大的合成英语寓言集DS-TF1-EN-3M（TF1），我们解决了罗马尼亚等低资源语言丰富高质量文学数据集的需求。我们的管道首先从TF1池中生成15000个高质量的罗马尼亚参考文本，使用高性能的语言模型。然后，我们对一个120亿参数的开放权重模型应用两阶段微调过程：（i）指令微调以捕捉特定体裁的叙述风格，和（ii）适配器压缩以实现高效的部署。评估结合了语料库级别的BLEU分数和基于LLM的五维评判标准（准确性、流畅性、连贯性、风格、文化适应性），以提供翻译质量的细致评估。结果显示，我们的微调模型在流畅性和适当性方面与顶级大型专有模型竞争，同时具有开放性、可访问性，并且成本效益更高。我们还公开发布了所有相关脚本和评估提示。因此，TF2提供了成本高效翻译、跨语言叙述生成以及在低资源环境中广泛采用开放模型的文化重要文学内容的研究端到端、可再现的管道。 

---
# Forecasting Russian Equipment Losses Using Time Series and Deep Learning Models 

**Title (ZH)**: 使用时间序列和深度学习模型预测俄罗斯装备损失 

**Authors**: Jonathan Teagan  

**Link**: [PDF](https://arxiv.org/pdf/2509.07813)  

**Abstract**: This study applies a range of forecasting techniques,including ARIMA, Prophet, Long Short Term Memory networks (LSTM), Temporal Convolutional Networks (TCN), and XGBoost, to model and predict Russian equipment losses during the ongoing war in Ukraine. Drawing on daily and monthly open-source intelligence (OSINT) data from WarSpotting, we aim to assess trends in attrition, evaluate model performance, and estimate future loss patterns through the end of 2025. Our findings show that deep learning models, particularly TCN and LSTM, produce stable and consistent forecasts, especially under conditions of high temporal granularity. By comparing different model architectures and input structures, this study highlights the importance of ensemble forecasting in conflict modeling, and the value of publicly available OSINT data in quantifying material degradation over time. 

**Abstract (ZH)**: 本研究应用包括ARIMA、Prophet、长短期记忆网络（LSTM）、时序卷积网络（TCN）和XGBoost在内的多种预测技术，建模并预测俄罗斯在乌克兰战争中装备损失的情况。通过利用来自WarSpotting的每日和月度开源情报（OSINT）数据，我们旨在评估装备损耗趋势、评估模型性能，并估计到2025年底的未来损失模式。研究发现，深度学习模型，尤其是TCN和LSTM，能够产生稳定且一致的预测，尤其是在高时间粒度条件下。通过比较不同的模型架构和输入结构，本研究强调了冲突建模中集成预测的重要性，并突显了公开可用的OSINT数据在量化随时间变化的物质损耗方面的价值。 

---
# Enhanced SegNet with Integrated Grad-CAM for Interpretable Retinal Layer Segmentation in OCT Images 

**Title (ZH)**: 集成Grad-CAM的增强SegNet在OCT图像中实现可解释的视网膜层分割 

**Authors**: S M Asiful Islam Saky, Ugyen Tshering  

**Link**: [PDF](https://arxiv.org/pdf/2509.07795)  

**Abstract**: Optical Coherence Tomography (OCT) is essential for diagnosing conditions such as glaucoma, diabetic retinopathy, and age-related macular degeneration. Accurate retinal layer segmentation enables quantitative biomarkers critical for clinical decision-making, but manual segmentation is time-consuming and variable, while conventional deep learning models often lack interpretability. This work proposes an improved SegNet-based deep learning framework for automated and interpretable retinal layer segmentation. Architectural innovations, including modified pooling strategies, enhance feature extraction from noisy OCT images, while a hybrid loss function combining categorical cross-entropy and Dice loss improves performance for thin and imbalanced retinal layers. Gradient-weighted Class Activation Mapping (Grad-CAM) is integrated to provide visual explanations, allowing clinical validation of model decisions. Trained and validated on the Duke OCT dataset, the framework achieved 95.77% validation accuracy, a Dice coefficient of 0.9446, and a Jaccard Index (IoU) of 0.8951. Class-wise results confirmed robust performance across most layers, with challenges remaining for thinner boundaries. Grad-CAM visualizations highlighted anatomically relevant regions, aligning segmentation with clinical biomarkers and improving transparency. By combining architectural improvements, a customized hybrid loss, and explainable AI, this study delivers a high-performing SegNet-based framework that bridges the gap between accuracy and interpretability. The approach offers strong potential for standardizing OCT analysis, enhancing diagnostic efficiency, and fostering clinical trust in AI-driven ophthalmic tools. 

**Abstract (ZH)**: 基于改进SegNet的全自动可解释视网膜层分割深度学习框架 

---
# Individual utilities of life satisfaction reveal inequality aversion unrelated to political alignment 

**Title (ZH)**: 个体的生活满意度揭示了与政治倾向无关的不平等厌恶感 

**Authors**: Crispin Cooper, Ana Friedrich, Tommaso Reggiani, Wouter Poortinga  

**Link**: [PDF](https://arxiv.org/pdf/2509.07793)  

**Abstract**: How should well-being be prioritised in society, and what trade-offs are people willing to make between fairness and personal well-being? We investigate these questions using a stated preference experiment with a nationally representative UK sample (n = 300), in which participants evaluated life satisfaction outcomes for both themselves and others under conditions of uncertainty. Individual-level utility functions were estimated using an Expected Utility Maximisation (EUM) framework and tested for sensitivity to the overweighting of small probabilities, as characterised by Cumulative Prospect Theory (CPT). A majority of participants displayed concave (risk-averse) utility curves and showed stronger aversion to inequality in societal life satisfaction outcomes than to personal risk. These preferences were unrelated to political alignment, suggesting a shared normative stance on fairness in well-being that cuts across ideological boundaries. The results challenge use of average life satisfaction as a policy metric, and support the development of nonlinear utility-based alternatives that more accurately reflect collective human values. Implications for public policy, well-being measurement, and the design of value-aligned AI systems are discussed. 

**Abstract (ZH)**: 如何在社会中优先考虑幸福，人们在公平与个人幸福之间愿意做出什么样的权衡？我们通过一项具有全国代表性的英国样本（n=300）的表达偏好实验来探讨这些问题，在不确定性条件下，参与者评估了自己和他人的生活满意度结果。利用期望效用最大化（EUM）框架估计了个体内在效用函数，并测试了对小概率 overweighting 的敏感性，这由前景理论（CPT）来表征。大多数参与者显示出凹形（风险规避）的效用曲线，并对社会生活满意度结果的不平等表现出比个人风险更大的厌恶。这些偏好与政治倾向无关，表明在不同意识形态边界上存在对福利公平性的共享规范性立场。研究结果挑战了平均生活满意度作为政策指标的使用，并支持开发能够更准确反映集体人类价值观的非线性效用替代指标。讨论了这些结果对公共政策、幸福测度以及价值对齐的人工智能系统设计的含义。 

---
# XSRD-Net: EXplainable Stroke Relapse Detection 

**Title (ZH)**: XSRD-Net: 可解释的笔画复发检测 

**Authors**: Christian Gapp, Elias Tappeiner, Martin Welk, Karl Fritscher, Stephanie Mangesius, Constantin Eisenschink, Philipp Deisl, Michael Knoflach, Astrid E. Grams, Elke R. Gizewski, Rainer Schubert  

**Link**: [PDF](https://arxiv.org/pdf/2509.07772)  

**Abstract**: Stroke is the second most frequent cause of death world wide with an annual mortality of around 5.5 million. Recurrence rates of stroke are between 5 and 25% in the first year. As mortality rates for relapses are extraordinarily high (40%) it is of utmost importance to reduce the recurrence rates. We address this issue by detecting patients at risk of stroke recurrence at an early stage in order to enable appropriate therapy planning. To this end we collected 3D intracranial CTA image data and recorded concomitant heart diseases, the age and the gender of stroke patients between 2010 and 2024. We trained single- and multimodal deep learning based neural networks for binary relapse detection (Task 1) and for relapse free survival (RFS) time prediction together with a subsequent classification (Task 2). The separation of relapse from non-relapse patients (Task 1) could be solved with tabular data (AUC on test dataset: 0.84). However, for the main task, the regression (Task 2), our multimodal XSRD-net processed the modalities vision:tabular with 0.68:0.32 according to modality contribution measures. The c-index with respect to relapses for the multimodal model reached 0.68, and the AUC is 0.71 for the test dataset. Final, deeper interpretability analysis results could highlight a link between both heart diseases (tabular) and carotid arteries (vision) for the detection of relapses and the prediction of the RFS time. This is a central outcome that we strive to strengthen with ongoing data collection and model retraining. 

**Abstract (ZH)**: 全球范围内，中风是仅次于心脏病的第二大死亡原因，每年约导致550万人死亡。中风复发率在首年介于5%至25%之间。由于复发的死亡率极高（40%），降低复发率尤为重要。我们通过早期检测中风复发风险的患者，以提供合适的治疗规划来应对这一问题。2010年至2024年间，我们收集了颅内3D CTA影像数据，并记录了中风患者的并发心脏病情况、年龄和性别。我们训练了单模态和多模态深度学习神经网络，用于二元复发检测（任务1）和复发自由生存时间预测（任务2）及后续分类（任务2）。单一模态数据在区分复发与非复发患者（任务1）中表现良好（测试数据集AUC：0.84）。然而，对于主要任务——回归预测（任务2），我们多模态XSRD-net采用 vision:tabular 模态权重比0.68:0.32进行了处理。针对复发的多模态模型的c-index达到0.68，测试数据集的AUC为0.71。最终，更深入的可解释性分析结果表明，心脏疾病（表观数据）和颈动脉（影像数据）之间的联系对于复发检测和生存时间预测具有重要意义。这是我们持续数据收集和模型重新训练努力加强的核心成果。 

---
# Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning 

**Title (ZH)**: 大规模语言模型足以检测超党派化、假信息、极化和有害内容吗？基于上下文学习与细调的评估 

**Authors**: Michele Joshua Maggini, Dhia Merzougui, Rabiraj Bandyopadhyay, Gaël Dias, Fabrice Maurel, Pablo Gamallo  

**Link**: [PDF](https://arxiv.org/pdf/2509.07768)  

**Abstract**: The spread of fake news, polarizing, politically biased, and harmful content on online platforms has been a serious concern. With large language models becoming a promising approach, however, no study has properly benchmarked their performance across different models, usage methods, and languages. This study presents a comprehensive overview of different Large Language Models adaptation paradigms for the detection of hyperpartisan and fake news, harmful tweets, and political bias. Our experiments spanned 10 datasets and 5 different languages (English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and multiclass classification scenarios. We tested different strategies ranging from parameter efficient Fine-Tuning of language models to a variety of different In-Context Learning strategies and prompts. These included zero-shot prompts, codebooks, few-shot (with both randomly-selected and diversely-selected examples using Determinantal Point Processes), and Chain-of-Thought. We discovered that In-Context Learning often underperforms when compared to Fine-Tuning a model. This main finding highlights the importance of Fine-Tuning even smaller models on task-specific settings even when compared to the largest models evaluated in an In-Context Learning setup - in our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and Qwen2.5-7B-Instruct. 

**Abstract (ZH)**: 在线平台上假新闻、极化内容、政治偏见和有害内容的传播是一个严重关切的问题。然而，随着大型语言模型成为一种有前景的方法，尚未有研究在不同模型、使用方法和语言之间恰当地基准测试它们的性能。本文提供了一个关于不同大型语言模型适应范式在检测极化内容和假新闻、有害推文以及政治偏见方面的全面概述。我们的实验覆盖了10个数据集和5种不同的语言（英语、西班牙语、葡萄牙语、阿拉伯语和保加利亚语），包括二分类和多分类分类场景。我们测试了从参数高效微调语言模型到各种不同上下文学习策略和提示的不同策略，包括零样本提示、代码书、少样本（包括随机选择和使用行列式点过程选择多样样本）以及思维链。我们发现，与微调模型相比，上下文学习往往表现较差。这一主要发现突出了即使在上下文学习设置中使用较小的模型，对其进行任务特定的微调的重要性，这在我们的案例中包括LlaMA3.1-8b-Instruct、Mistral-Nemo-Instruct-2407和Qwen2.5-7B-Instruct。 

---
# What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring Motivations in Open-Source Projects 

**Title (ZH)**: 你当时在想什么？一个基于LLM的大规模开源项目重构动机研究 

**Authors**: Mikel Robredo, Matteo Esposito, Fabio Palomba, Rafael Peñaloza, Valentina Lenarduzzi  

**Link**: [PDF](https://arxiv.org/pdf/2509.07763)  

**Abstract**: Context. Code refactoring improves software quality without changing external behavior. Despite its advantages, its benefits are hindered by the considerable cost of time, resources, and continuous effort it demands. Aim. Understanding why developers refactor, and which metrics capture these motivations, may support wider and more effective use of refactoring in practice. Method. We performed a large-scale empirical study to analyze developers refactoring activity, leveraging Large Language Models (LLMs) to identify underlying motivations from version control data, comparing our findings with previous motivations reported in the literature. Results. LLMs matched human judgment in 80% of cases, but aligned with literature-based motivations in only 47%. They enriched 22% of motivations with more detailed rationale, often highlighting readability, clarity, and structural improvements. Most motivations were pragmatic, focused on simplification and maintainability. While metrics related to developer experience and code readability ranked highest, their correlation with motivation categories was weak. Conclusions. We conclude that LLMs effectively capture surface-level motivations but struggle with architectural reasoning. Their value lies in providing localized explanations, which, when combined with software metrics, can form hybrid approaches. Such integration offers a promising path toward prioritizing refactoring more systematically and balancing short-term improvements with long-term architectural goals. 

**Abstract (ZH)**: 上下文. 代码重构可以在不改变外部行为的情况下提高软件质量。尽管具备优势，但其应用受到时间、资源和持续努力成本的限制。目标. 理解开发者为何进行重构，以及哪些指标能够捕捉这些动机，有助于更广泛和有效地在实践中应用重构。方法. 我们进行了大规模实证研究，分析开发者的重构活动，利用大型语言模型（LLMs）从版本控制数据中识别潜在动机，并将我们的发现与文献中报告的先前动机进行比较。结果. LLMs 在 80% 的案例中与人类判断相符，但在文献基础动机方面只有 47% 一致。它们为 22% 的动机提供了更详细的理由，经常突出可读性、清晰性和结构改进。大多数动机是实际的，集中在简化和可维护性上。虽然与开发人员经验和代码可读性相关的指标得分最高，但它们与动机类别相关性较弱。结论. 我们得出结论，LLMs 有效地捕捉了表面动机，但在架构推理方面存在困难。它们的价值在于提供局部解释，结合软件指标时，可以形成混合方法。这种整合为更系统地优先考虑重构，并平衡短期改进与长期架构目标提供了一条有希望的道路。 

---
# Spectral and Rhythm Feature Performance Evaluation for Category and Class Level Audio Classification with Deep Convolutional Neural Networks 

**Title (ZH)**: 基于深卷积神经网络的音类别和子类别水平音谱与节奏特征性能评估 

**Authors**: Friedrich Wolf-Monheim  

**Link**: [PDF](https://arxiv.org/pdf/2509.07756)  

**Abstract**: Next to decision tree and k-nearest neighbours algorithms deep convolutional neural networks (CNNs) are widely used to classify audio data in many domains like music, speech or environmental sounds. To train a specific CNN various spectral and rhythm features like mel-scaled spectrograms, mel-frequency cepstral coefficients (MFCC), cyclic tempograms, short-time Fourier transform (STFT) chromagrams, constant-Q transform (CQT) chromagrams and chroma energy normalized statistics (CENS) chromagrams can be used as digital image input data for the neural network. The performance of these spectral and rhythm features for audio category level as well as audio class level classification is investigated in detail with a deep CNN and the ESC-50 dataset with 2,000 labeled environmental audio recordings using an end-to-end deep learning pipeline. The evaluated metrics accuracy, precision, recall and F1 score for multiclass classification clearly show that the mel-scaled spectrograms and the mel-frequency cepstral coefficients (MFCC) perform significantly better then the other spectral and rhythm features investigated in this research for audio classification tasks using deep CNNs. 

**Abstract (ZH)**: 除了决策树和k最近邻算法之外，卷积神经网络（CNN）在音乐、语音或环境声音等领域广泛用于音频数据分类。通过使用熔频谱图、熔频率倒谱系数（MFCC）、循环节拍图、短时傅里叶变换（STFT）色谱图、常数Q变换（CQT）色谱图和色谱能归一化统计（CENS）色谱图等频谱和节拍特征作为神经网络的数字图像输入数据来训练特定的CNN。本研究使用端到端的深度学习管道，基于包含2000个标记的环境音频录制的ESC-50数据集，详细探讨了这些频谱和节拍特征在音频类别级别和音频类级别分类中的性能。评估的多类别分类准确率、精确率、召回率和F1分数明确显示，熔频谱图和熔频率倒谱系数（MFCC）在这项研究中用于深度CNN的音频分类任务中显著优于其他研究中研究的频谱和节拍特征。 

---
# Enhancing Online Learning by Integrating Biosensors and Multimodal Learning Analytics for Detecting and Predicting Student Behavior: A Review 

**Title (ZH)**: 通过整合 biosensors 和多模态学习分析来增强在线学习：检测和预测学生行为的综述 

**Authors**: Alvaro Becerra, Ruth Cobos, Charles Lang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07742)  

**Abstract**: In modern online learning, understanding and predicting student behavior is crucial for enhancing engagement and optimizing educational outcomes. This systematic review explores the integration of biosensors and Multimodal Learning Analytics (MmLA) to analyze and predict student behavior during computer-based learning sessions. We examine key challenges, including emotion and attention detection, behavioral analysis, experimental design, and demographic considerations in data collection. Our study highlights the growing role of physiological signals, such as heart rate, brain activity, and eye-tracking, combined with traditional interaction data and self-reports to gain deeper insights into cognitive states and engagement levels. We synthesize findings from 54 key studies, analyzing commonly used methodologies such as advanced machine learning algorithms and multimodal data pre-processing techniques. The review identifies current research trends, limitations, and emerging directions in the field, emphasizing the transformative potential of biosensor-driven adaptive learning systems. Our findings suggest that integrating multimodal data can facilitate personalized learning experiences, real-time feedback, and intelligent educational interventions, ultimately advancing toward a more customized and adaptive online learning experience. 

**Abstract (ZH)**: 在现代在线学习中，理解并预测学生行为对于增强参与度和优化教育成果至关重要。本系统性综述探讨了生物传感器与多模态学习分析（MmLA）的集成，以分析和预测计算机辅助学习会话期间的学生行为。我们研究了包括情绪和注意力检测、行为分析、实验设计和数据收集中的人口统计学考虑在内的关键挑战。我们的研究突出了生理信号（如心率、脑活动和眼动追踪）与传统交互数据和自我报告相结合，以更深入地了解认知状态和参与水平的作用日益增加。我们综合了54项关键研究的发现，分析了常用的方法论，如先进的机器学习算法和多模态数据预处理技术。综述识别了该领域的当前研究趋势、局限性和新兴方向，强调了生物传感器驱动的自适应学习系统的变革潜力。我们的研究结果表明，整合多模态数据可以促进个性化学习体验、实时反馈和智能教育干预，最终朝着更定制化和自适应的在线学习体验迈进。 

---
# Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems 

**Title (ZH)**: 光谱掩蔽和插值攻击（SMIA）：针对语音认证和防欺骗系统的黑盒 adversarial 攻击 

**Authors**: Kamel Kamel, Hridoy Sankar Dutta, Keshav Sood, Sunil Aryal  

**Link**: [PDF](https://arxiv.org/pdf/2509.07677)  

**Abstract**: Voice Authentication Systems (VAS) use unique vocal characteristics for verification. They are increasingly integrated into high-security sectors such as banking and healthcare. Despite their improvements using deep learning, they face severe vulnerabilities from sophisticated threats like deepfakes and adversarial attacks. The emergence of realistic voice cloning complicates detection, as systems struggle to distinguish authentic from synthetic audio. While anti-spoofing countermeasures (CMs) exist to mitigate these risks, many rely on static detection models that can be bypassed by novel adversarial methods, leaving a critical security gap. To demonstrate this vulnerability, we propose the Spectral Masking and Interpolation Attack (SMIA), a novel method that strategically manipulates inaudible frequency regions of AI-generated audio. By altering the voice in imperceptible zones to the human ear, SMIA creates adversarial samples that sound authentic while deceiving CMs. We conducted a comprehensive evaluation of our attack against state-of-the-art (SOTA) models across multiple tasks, under simulated real-world conditions. SMIA achieved a strong attack success rate (ASR) of at least 82% against combined VAS/CM systems, at least 97.5% against standalone speaker verification systems, and 100% against countermeasures. These findings conclusively demonstrate that current security postures are insufficient against adaptive adversarial attacks. This work highlights the urgent need for a paradigm shift toward next-generation defenses that employ dynamic, context-aware frameworks capable of evolving with the threat landscape. 

**Abstract (ZH)**: 语音认证系统（VAS）利用独特的语音特征进行验证。它们正越来越多地被集成到银行和医疗等高安全领域。尽管使用深度学习技术进行了改进，但它们仍面临来自深度伪造和对抗攻击等复杂威胁的严重漏洞。现实的语音克隆技术的出现加剧了检测难度，因为系统难以区分真实的和合成的音频。虽然存在反欺骗对策（CMs）来缓解这些风险，但许多对策依赖于静态检测模型，这些模型容易被新型对抗方法绕过，留下了安全漏洞。为了证明这一漏洞，我们提出了一种名为Spectral Masking and Interpolation Attack（Spectral掩蔽和内插攻击，SMIA）的新方法，该方法战略性地操纵AI生成音频的不可听频率区域。通过改变人类耳朵感知不到的区域中的声音，SMIA生成了既能听起来真实又能欺骗CMs的对抗样本。我们在多种任务下对最先进的模型进行了全面评估，模拟了现实生活中的条件。结果显示，SMIA分别在综合VAS/CM系统、独立说话人验证系统以及对策上的攻击成功率（ASR）至少为82%、97.5%和100%。这些发现表明，现有的安全措施不足以抵御适应性对抗攻击。本文强调了迫切需要转向采用动态、情境感知框架的下一代防御技术，这些框架能够随着威胁环境的变化而发展。 

---
# Variational Quantum Circuits in Offline Contextual Bandit Problems 

**Title (ZH)**: 离线上下文臂问题中的变分量子电路 

**Authors**: Lukas Schulte, Daniel Hein, Steffen Udluft, Thomas A. Runkler  

**Link**: [PDF](https://arxiv.org/pdf/2509.07633)  

**Abstract**: This paper explores the application of variational quantum circuits (VQCs) for solving offline contextual bandit problems in industrial optimization tasks. Using the Industrial Benchmark (IB) environment, we evaluate the performance of quantum regression models against classical models. Our findings demonstrate that quantum models can effectively fit complex reward functions, identify optimal configurations via particle swarm optimization (PSO), and generalize well in noisy and sparse datasets. These results provide a proof of concept for utilizing VQCs in offline contextual bandit problems and highlight their potential in industrial optimization tasks. 

**Abstract (ZH)**: 本文探讨了变分量子电路（VQCs）在工业优化任务中解决离线上下文臂问题的应用。通过工业基准（IB）环境，我们将量子回归模型的性能与经典模型进行了评估。研究结果表明，量子模型可以有效地拟合复杂奖励函数，通过粒子群优化（PSO）识别最优配置，并在噪声大和数据稀疏的情况下表现出良好的泛化能力。这些结果为在离线上下文臂问题中利用VQCs提供了概念验证，并突显了其在工业优化任务中的潜在应用。 

---
# From Classical Data to Quantum Advantage -- Quantum Policy Evaluation on Quantum Hardware 

**Title (ZH)**: 从经典数据到量子优势——量子硬件上的量子策略评估 

**Authors**: Daniel Hein, Simon Wiedemann, Markus Baumann, Patrik Felbinger, Justin Klein, Maximilian Schieder, Jonas Stein, Daniëlle Schuman, Thomas Cope, Steffen Udluft  

**Link**: [PDF](https://arxiv.org/pdf/2509.07614)  

**Abstract**: Quantum policy evaluation (QPE) is a reinforcement learning (RL) algorithm which is quadratically more efficient than an analogous classical Monte Carlo estimation. It makes use of a direct quantum mechanical realization of a finite Markov decision process, in which the agent and the environment are modeled by unitary operators and exchange states, actions, and rewards in superposition. Previously, the quantum environment has been implemented and parametrized manually for an illustrative benchmark using a quantum simulator. In this paper, we demonstrate how these environment parameters can be learned from a batch of classical observational data through quantum machine learning (QML) on quantum hardware. The learned quantum environment is then applied in QPE to also compute policy evaluations on quantum hardware. Our experiments reveal that, despite challenges such as noise and short coherence times, the integration of QML and QPE shows promising potential for achieving quantum advantage in RL. 

**Abstract (ZH)**: 量子策略评估（QPE）是一种比经典蒙特卡洛估计算法高效四倍的强化学习（RL）算法。它利用了直接的量子力学实现有穷马尔可夫决策过程，其中代理和环境由酉算子表示，并在量子相干状态下交换状态、动作和奖励。之前，量子环境已经在量子模拟器中通过手动实现和参数化来进行说明性基准测试。在本文中，我们展示了如何通过量子机器学习（QML）在量子硬件上从一批经典观测数据中学习这些环境参数。然后，所学的量子环境被应用到QPE中，在量子硬件上进行策略评估。我们的实验表明，尽管存在噪声和短相干时间等挑战，QML和QPE的集成在RL中实现量子优势具有前景。 

---
# Beyond Rebalancing: Benchmarking Binary Classifiers Under Class Imbalance Without Rebalancing Techniques 

**Title (ZH)**: 超越重新加权：在无需使用重新加权技术的情况下评估二分类器在类别不平衡条件下的表现 

**Authors**: Ali Nawaz, Amir Ahmad, Shehroz S. Khan  

**Link**: [PDF](https://arxiv.org/pdf/2509.07605)  

**Abstract**: Class imbalance poses a significant challenge to supervised classification, particularly in critical domains like medical diagnostics and anomaly detection where minority class instances are rare. While numerous studies have explored rebalancing techniques to address this issue, less attention has been given to evaluating the performance of binary classifiers under imbalance when no such techniques are applied. Therefore, the goal of this study is to assess the performance of binary classifiers "as-is", without performing any explicit rebalancing. Specifically, we systematically evaluate the robustness of a diverse set of binary classifiers across both real-world and synthetic datasets, under progressively reduced minority class sizes, using one-shot and few-shot scenarios as baselines. Our approach also explores varying data complexities through synthetic decision boundary generation to simulate real-world conditions. In addition to standard classifiers, we include experiments using undersampling, oversampling strategies, and one-class classification (OCC) methods to examine their behavior under severe imbalance. The results confirm that classification becomes more difficult as data complexity increases and the minority class size decreases. While traditional classifiers deteriorate under extreme imbalance, advanced models like TabPFN and boosting-based ensembles retain relatively higher performance and better generalization compared to traditional classifiers. Visual interpretability and evaluation metrics further validate these findings. Our work offers valuable guidance on model selection for imbalanced learning, providing insights into classifier robustness without dependence on explicit rebalancing techniques. 

**Abstract (ZH)**: 不均衡样本对监督分类构成重大挑战，尤其在医学诊断和异常检测等关键领域，少数类样本稀少。虽然许多研究探索了再平衡技术来解决这一问题，但在未应用这些技术的情况下评估二分类器性能的关注较少。因此，本研究的目的是评估二分类器“原封不动”的性能，不进行任何显式再平衡。具体而言，我们系统评估一系列二分类器在真实世界和合成数据集中的鲁棒性，少数类样本大小逐步减少，使用一对一和少量样本场景作为基准。我们的方法还通过生成合成决策边界来探索数据复杂性差异，以模拟现实世界条件。除了标准分类器，我们还包括欠采样、过采样策略以及一类分类(OCC)方法的实验，以考察其在严重不均衡情况下的行为。结果证实，随着数据复杂性的增加和少数类样本数量的减少，分类任务变得更为困难。虽然传统分类器在极端不均衡下性能下降，但如TabPFN等高级模型和基于提升的集成模型相比传统分类器具有相对较高的性能和更好的泛化能力。可视化解释和评估指标进一步验证了这些发现。我们的工作为不平衡学习提供有价值的指导，揭示了在不依赖显式再平衡技术的情况下分类器的鲁棒性。 

---
# Transformer-Based Approach to Optimal Sensor Placement for Structural Health Monitoring of Probe Cards 

**Title (ZH)**: 基于变换器的方法在探针卡结构健康监测中的最优传感器布置 

**Authors**: Mehdi Bejani, Marco Mauri, Daniele Acconcia, Simone Todaro, Stefano Mariani  

**Link**: [PDF](https://arxiv.org/pdf/2509.07603)  

**Abstract**: This paper presents an innovative Transformer-based deep learning strategy for optimizing the placement of sensors aiming at structural health monitoring of semiconductor probe cards. Failures in probe cards, including substrate cracks and loosened screws, would critically affect semiconductor manufacturing yield and reliability. Some failure modes could be detected by equipping a probe card with adequate sensors. Frequency response functions from simulated failure scenarios are adopted within a finite element model of a probe card. A comprehensive dataset, enriched by physics-informed scenario expansion and physics-aware statistical data augmentation, is exploited to train a hybrid Convolutional Neural Network and Transformer model. The model achieves high accuracy (99.83%) in classifying the probe card health states (baseline, loose screw, crack) and an excellent crack detection recall (99.73%). Model robustness is confirmed through a rigorous framework of 3 repetitions of 10-fold stratified cross-validation. The attention mechanism also pinpoints critical sensor locations: an analysis of the attention weights offers actionable insights for designing efficient, cost-effective monitoring systems by optimizing sensor configurations. This research highlights the capability of attention-based deep learning to advance proactive maintenance, enhancing operational reliability and yield in semiconductor manufacturing. 

**Abstract (ZH)**: 基于变压器的创新深度学习策略在半导体探针卡结构健康监测中优化传感器布局的研究 

---
# Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion Control? 

**Title (ZH)**: SSD-Mamba2能否解锁强化学习在端到端运动控制中的应用？ 

**Authors**: Gavin Tao, Yinuo Wang, Jinzhao Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2509.07593)  

**Abstract**: End-to-end reinforcement learning for motion control promises unified perception-action policies that scale across embodiments and tasks, yet most deployed controllers are either blind (proprioception-only) or rely on fusion backbones with unfavorable compute-memory trade-offs. Recurrent controllers struggle with long-horizon credit assignment, and Transformer-based fusion incurs quadratic cost in token length, limiting temporal and spatial context. We present a vision-driven cross-modal RL framework built on SSD-Mamba2, a selective state-space backbone that applies state-space duality (SSD) to enable both recurrent and convolutional scanning with hardware-aware streaming and near-linear scaling. Proprioceptive states and exteroceptive observations (e.g., depth tokens) are encoded into compact tokens and fused by stacked SSD-Mamba2 layers. The selective state-space updates retain long-range dependencies with markedly lower latency and memory use than quadratic self-attention, enabling longer look-ahead, higher token resolution, and stable training under limited compute. Policies are trained end-to-end under curricula that randomize terrain and appearance and progressively increase scene complexity. A compact, state-centric reward balances task progress, energy efficiency, and safety. Across diverse motion-control scenarios, our approach consistently surpasses strong state-of-the-art baselines in return, safety (collisions and falls), and sample efficiency, while converging faster at the same compute budget. These results suggest that SSD-Mamba2 provides a practical fusion backbone for scalable, foresightful, and efficient end-to-end motion control. 

**Abstract (ZH)**: 基于SSD-Mamba2的选择性状态空间框架的端到端强化学习在运动控制中的应用：统一感知-行动策略，兼顾-bodied性和任务扩展性 

---
# BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment 

**Title (ZH)**: BALI：通过知识图谱和语言模型对齐增强生物医学语言表示 

**Authors**: Andrey Sakhovskiy, Elena Tutubalina  

**Link**: [PDF](https://arxiv.org/pdf/2509.07588)  

**Abstract**: In recent years, there has been substantial progress in using pretrained Language Models (LMs) on a range of tasks aimed at improving the understanding of biomedical texts. Nonetheless, existing biomedical LLMs show limited comprehension of complex, domain-specific concept structures and the factual information encoded in biomedical Knowledge Graphs (KGs). In this work, we propose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel joint LM and KG pre-training method that augments an LM with external knowledge by the simultaneous learning of a dedicated KG encoder and aligning the representations of both the LM and the graph. For a given textual sequence, we link biomedical concept mentions to the Unified Medical Language System (UMLS) KG and utilize local KG subgraphs as cross-modal positive samples for these mentions. Our empirical findings indicate that implementing our method on several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves their performance on a range of language understanding tasks and the quality of entity representations, even with minimal pre-training on a small alignment dataset sourced from PubMed scientific abstracts. 

**Abstract (ZH)**: 生物医学知识图谱与语言模型联合预训练方法BALI 

---
# Attention Maps in 3D Shape Classification for Dental Stage Estimation with Class Node Graph Attention Networks 

**Title (ZH)**: 三维形状分类中的注意力图在牙科发展阶段估计中的应用：类节点图形注意力网络 

**Authors**: Barkin Buyukcakir, Rocharles Cavalcante Fontenele, Reinhilde Jacobs, Jannick De Tobel, Patrick Thevissen, Dirk Vandermeulen, Peter Claes  

**Link**: [PDF](https://arxiv.org/pdf/2509.07581)  

**Abstract**: Deep learning offers a promising avenue for automating many recognition tasks in fields such as medicine and forensics. However, the black-box nature of these models hinders their adoption in high-stakes applications where trust and accountability are required. For 3D shape recognition tasks in particular, this paper introduces the Class Node Graph Attention Network (CGAT) architecture to address this need. Applied to 3D meshes of third molars derived from CBCT images, for Demirjian stage allocation, CGAT utilizes graph attention convolutions and an inherent attention mechanism, visualized via attention rollout, to explain its decision-making process. We evaluated the local mean curvature and distance to centroid node features, both individually and in combination, as well as model depth, finding that models incorporating directed edges to a global CLS node produced more intuitive attention maps, while also yielding desirable classification performance. We analyzed the attention-based explanations of the models, and their predictive performances to propose optimal settings for the CGAT. The combination of local mean curvature and distance to centroid as node features yielded a slight performance increase with 0.76 weighted F1 score, and more comprehensive attention visualizations. The CGAT architecture's ability to generate human-understandable attention maps can enhance trust and facilitate expert validation of model decisions. While demonstrated on dental data, CGAT is broadly applicable to graph-based classification and regression tasks, promoting wider adoption of transparent and competitive deep learning models in high-stakes environments. 

**Abstract (ZH)**: 深度学习为医学和取证等领域中的许多识别任务提供了有前景的自动化途径。然而，这些模型的黑箱性质阻碍了其在需要信任和问责的应用中的采用。特别是在3D形状识别任务中，本文引入了Class Node Graph Attention Network (CGAT) 架构以应对这一需求。将CGAT应用于从CBCT图像衍生的第三磨牙3D网格，用于Demirjian阶段分配，CGAT利用图注意力卷积和固有的注意力机制，通过注意力展开可视化其决策过程。我们评估了局部均曲率和质心节点距离这两种特征及其组合的效果，以及模型深度，发现将有向边连接到全局CLS节点的模型生成了更具直观性的注意力图，并且还提供了理想的分类性能。我们分析了模型的基于注意力的解释及其预测性能，提出了CGAT的最佳设置。局部均曲率和质心距离的节点特征组合在加权F1分数为0.76的情况下产生了轻微的性能提升，并提供了更加全面的注意力可视化。CGAT架构生成可人类理解的注意力图的能力可以增强信任，并促进专家对模型决策的验证。虽然CGAT在牙科数据上进行了展示，但它广泛适用于基于图的分类和回归任务，促进了在高风险环境中更广泛采用透明和竞争力强的深度学习模型。 

---
# Towards Generalized Routing: Model and Agent Orchestration for Adaptive and Efficient Inference 

**Title (ZH)**: 面向通用路由：模型和代理编排以实现适应性和高效的推理 

**Authors**: Xiyu Guo, Shan Wang, Chunfang Ji, Xuefeng Zhao, Wenhao Xi, Yaoyao Liu, Qinglan Li, Chao Deng, Junlan Feng  

**Link**: [PDF](https://arxiv.org/pdf/2509.07571)  

**Abstract**: The rapid advancement of large language models (LLMs) and domain-specific AI agents has greatly expanded the ecosystem of AI-powered services. User queries, however, are highly diverse and often span multiple domains and task types, resulting in a complex and heterogeneous landscape. This diversity presents a fundamental routing challenge: how to accurately direct each query to an appropriate execution unit while optimizing both performance and efficiency. To address this, we propose MoMA (Mixture of Models and Agents), a generalized routing framework that integrates both LLM and agent-based routing. Built upon a deep understanding of model and agent capabilities, MoMA effectively handles diverse queries through precise intent recognition and adaptive routing strategies, achieving an optimal balance between efficiency and cost. Specifically, we construct a detailed training dataset to profile the capabilities of various LLMs under different routing model structures, identifying the most suitable tasks for each LLM. During inference, queries are dynamically routed to the LLM with the best cost-performance efficiency. We also introduce an efficient agent selection strategy based on a context-aware state machine and dynamic masking. Experimental results demonstrate that the MoMA router offers superior cost-efficiency and scalability compared to existing approaches. 

**Abstract (ZH)**: 大规模语言模型（LLMs）和领域特定AI代理的迅速发展极大地扩展了AI驱动服务的生态系统。然而，用户查询具有高度多样性，并且经常跨越多个领域和任务类型，导致一个复杂且异质的景观。这种多样性提出了一个基础的路由挑战：如何准确地将每个查询导向合适的执行单元，同时优化性能和效率。为应对这一挑战，我们提出MoMA（模型和代理的混合体）——一个通用的路由框架，整合了LLM和基于代理的路由。基于对模型和代理能力的深刻理解，MoMA通过精确的意图识别和自适应路由策略有效地处理各种查询，实现效率和成本的最优平衡。具体而言，我们构建了一个详细的训练数据集，以了解在不同路由模型结构下各种LLM的能力，并确定最适合每个LLM的任务。在推理过程中，查询被动态路由到具有最佳成本-性能效率的LLM。我们还引入了一种基于上下文感知状态机和动态掩码的高效代理选择策略。实验结果表明，MoMA路由器在成本效率和可扩展性方面优于现有方法。 

---
# $ΔL$ Normalization: Rethink Loss Aggregation in RLVR 

**Title (ZH)**: ΔL 归一化：重思 RLVR 中的损失聚合 

**Authors**: Zhiyuan He, Xufang Luo, Yike Zhang, Yuqing Yang, Lili Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2509.07558)  

**Abstract**: We propose $\Delta L$ Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed $\Delta L$ Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at this https URL. 

**Abstract (ZH)**: 我们提出了一种针对可验证奖励强化学习（RLVR）中动态生成长度特点的简单有效的损失聚合方法——$\Delta L$归一化。 

---
# Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition 

**Title (ZH)**: 基于引导分解在多跳问答中避免知识编辑跳过 

**Authors**: Yi Liu, Xiangrong Zhu, Xiangyu Liu, Wei Wei, Wei Hu  

**Link**: [PDF](https://arxiv.org/pdf/2509.07555)  

**Abstract**: In a rapidly evolving world where information updates swiftly, knowledge in large language models (LLMs) becomes outdated quickly. Retraining LLMs is not a cost-effective option, making knowledge editing (KE) without modifying parameters particularly necessary. We find that although existing retrieval-augmented generation (RAG)-based KE methods excel at editing simple knowledge, they struggle with KE in multi-hop question answering due to the issue of "edit skipping", which refers to skipping the relevant edited fact in inference. In addition to the diversity of natural language expressions of knowledge, edit skipping also arises from the mismatch between the granularity of LLMs in problem-solving and the facts in the edited memory. To address this issue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing method with guided decomposition (IRAKE) through the guidance from single edited facts and entire edited cases. Experimental results demonstrate that IRAKE mitigates the failure of editing caused by edit skipping and outperforms state-of-the-art methods for KE in multi-hop question answering. 

**Abstract (ZH)**: 在信息快速更新的世界中，大型语言模型（LLMs）中的知识迅速过时。重新训练LLMs不是一种成本有效的选择，因此在不修改参数的情况下进行知识编辑（KE）尤为重要。我们发现，尽管现有的基于检索增强生成（RAG）的知识编辑方法在编辑简单知识方面表现出色，但在处理多跳问答的知识编辑时却面临着“编辑跳跃”的问题，这指的是推理过程中跳过了相关的编辑事实。除了知识的自然语言表达多样性外，编辑跳跃还源于问题求解中LLMs的粒度与编辑记忆中的事实之间的不匹配。为解决这一问题，我们提出了一种基于单个编辑事实和整个编辑案例指导的迭代检索增强知识编辑方法（IRAKE），通过这种方式进行知识编辑。实验结果表明，IRAKE减轻了由编辑跳跃引起的编辑失败，并在多跳问答的知识编辑方面优于最先进的方法。 

---
# HU-based Foreground Masking for 3D Medical Masked Image Modeling 

**Title (ZH)**: 基于HU值的医学图像掩码前景屏蔽方法 

**Authors**: Jin Lee, Vu Dang, Gwang-Hyun Yu, Anh Le, Zahid Rahman, Jin-Ho Jang, Heonzoo Lee, Kun-Yung Kim, Jin-Sul Kim, Jin-Young Kim  

**Link**: [PDF](https://arxiv.org/pdf/2509.07534)  

**Abstract**: While Masked Image Modeling (MIM) has revolutionized fields of computer vision, its adoption in 3D medical image computing has been limited by the use of random masking, which overlooks the density of anatomical objects. To address this limitation, we enhance the pretext task with a simple yet effective masking strategy. Leveraging Hounsfield Unit (HU) measurements, we implement an HU-based Foreground Masking, which focuses on the intensity distribution of visceral organs and excludes non-tissue regions, such as air and fluid, that lack diagnostically meaningful features. Extensive experiments on five public 3D medical imaging datasets demonstrate that our masking consistently improves performance, both in quality of segmentation and Dice score (BTCV:~84.64\%, Flare22:~92.43\%, MM-WHS:~90.67\%, Amos22:~88.64\%, BraTS:~78.55\%). These results underscore the importance of domain-centric MIM and suggest a promising direction for representation learning in medical image segmentation. Implementation is available at this http URL. 

**Abstract (ZH)**: 基于HU值的前景掩模：在3D医学图像计算中中心化掩模图像建模对于器官分割性能的提升研究 

---
# FLeW: Facet-Level and Adaptive Weighted Representation Learning of Scientific Documents 

**Title (ZH)**: FLeW：科学文档的层次化和自适应加权表示学习 

**Authors**: Zheng Dou, Deqing Wang, Fuzhen Zhuang, Jian Ren, Yanlin Hu  

**Link**: [PDF](https://arxiv.org/pdf/2509.07531)  

**Abstract**: Scientific document representation learning provides powerful embeddings for various tasks, while current methods face challenges across three approaches. 1) Contrastive training with citation-structural signals underutilizes citation information and still generates single-vector representations. 2) Fine-grained representation learning, which generates multiple vectors at the sentence or aspect level, requires costly integration and lacks domain generalization. 3) Task-aware learning depends on manually predefined task categorization, overlooking nuanced task distinctions and requiring extra training data for task-specific modules. To address these problems, we propose a new method that unifies the three approaches for better representations, namely FLeW. Specifically, we introduce a novel triplet sampling method that leverages citation intent and frequency to enhance citation-structural signals for training. Citation intents (background, method, result), aligned with the general structure of scientific writing, facilitate a domain-generalized facet partition for fine-grained representation learning. Then, we adopt a simple weight search to adaptively integrate three facet-level embeddings into a task-specific document embedding without task-aware fine-tuning. Experiments show the applicability and robustness of FLeW across multiple scientific tasks and fields, compared to prior models. 

**Abstract (ZH)**: 科学文献表示学习提供了多种任务的强大嵌入，而当前方法在三种方法上面临挑战。1）基于引用结构信号的对比训练未能充分利用引用信息，仍生成单向量表示。2）细粒度表示学习生成句子或方面级别的多个向量，需要昂贵的整合并缺乏领域泛化能力。3）任务感知学习依赖于人工预定义的任务分类，忽略了任务的细微差异，并要求为任务特定模块额外的数据训练。为解决这些问题，我们提出了一种新的方法，将这三种方法统一起来以获得更好的表示，名为FLeW。具体地，我们引入一种新颖的三元组采样方法，利用引用意图和频率来增强引用结构信号以供训练。引用意图（背景、方法、结果），与科学写作的一般结构对齐，有助于实现细粒度表示学习的领域泛化方面分隔。然后，我们采用简单的权重搜索来适应性地将三个方面级别的嵌入整合为一个任务特定的文档嵌入，无需任务感知微调。实验表明，与先前模型相比，FLeW在多种科学研究任务和领域中具有适用性和鲁棒性。 

---
# Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data 

**Title (ZH)**: 高效利用公共数据的单阶段训练竞争性音频-语言模型 

**Authors**: Gokul Karthik Kumar, Rishabh Saraf, Ludovick Lepauloux, Abdul Muneer, Billel Mokeddem, Hakim Hacid  

**Link**: [PDF](https://arxiv.org/pdf/2509.07526)  

**Abstract**: Large language models (LLMs) have transformed NLP, yet their integration with audio remains underexplored -- despite audio's centrality to human communication. We introduce Falcon3-Audio, a family of Audio-Language Models (ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably small amount of public audio data -- less than 30K hours (5K unique) -- Falcon3-Audio-7B matches the best reported performance among open-weight models on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while distinguishing itself through superior data and parameter efficiency, single-stage training, and transparency. Notably, our smallest 1B model remains competitive with larger open models ranging from 2B to 13B parameters. Through extensive ablations, we find that common complexities -- such as curriculum learning, multiple audio encoders, and intricate cross-attention connectors -- are not required for strong performance, even compared to models trained on over 500K hours of data. 

**Abstract (ZH)**: 大规模语言模型(LLMs)虽已革新自然语言处理(NLP)，但其与音频的整合仍待探索——尽管音频在人类交流中居于核心地位。我们介绍了Falcon3-Audio，这是一种基于指令调优LLMs和Whisper编码器构建的音频语言模型家族。使用不到30K小时（5K种独特）的公开音频数据，Falcon3-Audio-7B在MMAU基准测试中达到最佳报告性能，得分为64.14，与R1-AQA相当，同时通过优越的数据和参数效率、单阶段训练和透明度脱颖而出。值得注意的是，我们最小的1B模型仍与参数范围从2B到13B的大规模公开模型保持竞争力。通过广泛的消融实验，我们发现常见的复杂性，如逐级学习、多种音频编码器和复杂的交叉注意连接器，并非高效性能的必需条件，即使相比数据训练时长超过50万小时的模型也是如此。 

---
# EHWGesture -- A dataset for multimodal understanding of clinical gestures 

**Title (ZH)**: EHWGesture -- 用于临床手势多模态理解的数据集 

**Authors**: Gianluca Amprimo, Alberto Ancilotto, Alessandro Savino, Fabio Quazzolo, Claudia Ferraris, Gabriella Olmo, Elisabetta Farella, Stefano Di Carlo  

**Link**: [PDF](https://arxiv.org/pdf/2509.07525)  

**Abstract**: Hand gesture understanding is essential for several applications in human-computer interaction, including automatic clinical assessment of hand dexterity. While deep learning has advanced static gesture recognition, dynamic gesture understanding remains challenging due to complex spatiotemporal variations. Moreover, existing datasets often lack multimodal and multi-view diversity, precise ground-truth tracking, and an action quality component embedded within gestures. This paper introduces EHWGesture, a multimodal video dataset for gesture understanding featuring five clinically relevant gestures. It includes over 1,100 recordings (6 hours), captured from 25 healthy subjects using two high-resolution RGB-Depth cameras and an event camera. A motion capture system provides precise ground-truth hand landmark tracking, and all devices are spatially calibrated and synchronized to ensure cross-modal alignment. Moreover, to embed an action quality task within gesture understanding, collected recordings are organized in classes of execution speed that mirror clinical evaluations of hand dexterity. Baseline experiments highlight the dataset's potential for gesture classification, gesture trigger detection, and action quality assessment. Thus, EHWGesture can serve as a comprehensive benchmark for advancing multimodal clinical gesture understanding. 

**Abstract (ZH)**: 手部手势理解对于人机交互中的多种应用至关重要，包括手部灵巧性的自动临床评估。尽管深度学习在静态手势识别方面取得了进展，但动态手势理解仍然由于时空复杂变化而具有挑战性。此外，现有数据集往往缺乏多模态和多视角多样性、精确的ground-truth跟踪以及嵌入在手势中的动作质量成分。本文介绍了EHWGesture，这是一个包含五种临床相关手势的多模态视频数据集。该数据集包含超过1100个记录（6小时），由25名健康受试者使用两台高分辨率RGB-Depth摄像头和一个事件摄像头捕获。运动捕捉系统提供了精确的手部关键点ground-truth跟踪，并确保所有设备在空间上校准和同步，以实现跨模态对齐。此外，为了在手势理解中嵌入动作质量任务，收集的记录按照反映手部灵巧性临床评估的速度分类组织。基线实验表明，该数据集在手势分类、手势触发检测和动作质量评估方面的潜在应用。因此，EHWGesture可以作为多模态临床手势理解领域综合基准的参考。 

---
# Water Demand Forecasting of District Metered Areas through Learned Consumer Representations 

**Title (ZH)**: 通过学习消费者表示进行区域计量区的用水需求预测 

**Authors**: Adithya Ramachandran, Thorkil Flensmark B. Neergaard, Tomás Arias-Vergara, Andreas Maier, Siming Bayer  

**Link**: [PDF](https://arxiv.org/pdf/2509.07515)  

**Abstract**: Advancements in smart metering technologies have significantly improved the ability to monitor and manage water utilities. In the context of increasing uncertainty due to climate change, securing water resources and supply has emerged as an urgent global issue with extensive socioeconomic ramifications. Hourly consumption data from end-users have yielded substantial insights for projecting demand across regions characterized by diverse consumption patterns. Nevertheless, the prediction of water demand remains challenging due to influencing non-deterministic factors, such as meteorological conditions. This work introduces a novel method for short-term water demand forecasting for District Metered Areas (DMAs) which encompass commercial, agricultural, and residential consumers. Unsupervised contrastive learning is applied to categorize end-users according to distinct consumption behaviors present within a DMA. Subsequently, the distinct consumption behaviors are utilized as features in the ensuing demand forecasting task using wavelet-transformed convolutional networks that incorporate a cross-attention mechanism combining both historical data and the derived representations. The proposed approach is evaluated on real-world DMAs over a six-month period, demonstrating improved forecasting performance in terms of MAPE across different DMAs, with a maximum improvement of 4.9%. Additionally, it identifies consumers whose behavior is shaped by socioeconomic factors, enhancing prior knowledge about the deterministic patterns that influence demand. 

**Abstract (ZH)**: 智能计量技术的进步显著提高了对水務進行監測和管理的能力。隨著經濟社會影響的擴大，由於氣候變化的不確定性增加，確保水资源和供應已成为一个紧迫的全球问题。来自终端用户的每小时消耗数据为具有不同消耗模式的地区的需求展望提供了大量见解。然而，由于气象条件等非确定性因素的影响，需求预测仍然具有挑战性。本研究介绍了一种新的方法，用于预测地区计量区域（DMAs）的短期内水需求，这些DMAs包括商业、农业和居民用户。应用无监督对比学习对DMAs中的不同消费行为进行分类。随后，这些不同的消费行为被用作特征，使用结合历史数据和提取表示的交叉注意力机制的小波变换卷积网络进行后续需求预测任务。该提出的 approaches 在六个月内的真实 DMAs 上进行了评估，显示了在不同 DMAs 上改善的预测性能，最大改进率为 4.9%。此外，该方法还识别了受社会经济因素影响的消费者行为，增强了对影响需求的确定性模式的先验知识。 

---
# ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval 

**Title (ZH)**: ALLabel: 基于演示检索的三阶段主动学习实体识别方法 

**Authors**: Zihan Chen, Lei Shi, Weize Wu, Qiji Zhou, Yue Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07512)  

**Abstract**: Many contemporary data-driven research efforts in the natural sciences, such as chemistry and materials science, require large-scale, high-performance entity recognition from scientific datasets. Large language models (LLMs) have increasingly been adopted to solve the entity recognition task, with the same trend being observed on all-spectrum NLP tasks. The prevailing entity recognition LLMs rely on fine-tuned technology, yet the fine-tuning process often incurs significant cost. To achieve a best performance-cost trade-off, we propose ALLabel, a three-stage framework designed to select the most informative and representative samples in preparing the demonstrations for LLM modeling. The annotated examples are used to construct a ground-truth retrieval corpus for LLM in-context learning. By sequentially employing three distinct active learning strategies, ALLabel consistently outperforms all baselines under the same annotation budget across three specialized domain datasets. Experimental results also demonstrate that selectively annotating only 5\%-10\% of the dataset with ALLabel can achieve performance comparable to the method annotating the entire dataset. Further analyses and ablation studies verify the effectiveness and generalizability of our proposal. 

**Abstract (ZH)**: 当代自然科学中大规模高性能实体识别的需求：ALLabel三阶段框架在准备LLM模型示例中的应用及其效果分析 

---
# Astra: A Multi-Agent System for GPU Kernel Performance Optimization 

**Title (ZH)**: Astra：一种GPU内核性能优化的多agent系统 

**Authors**: Anjiang Wei, Tianran Sun, Yogesh Seenichamy, Hang Song, Anne Ouyang, Azalia Mirhoseini, Ke Wang, Alex Aiken  

**Link**: [PDF](https://arxiv.org/pdf/2509.07506)  

**Abstract**: GPU kernel optimization has long been a central challenge at the intersection of high-performance computing and machine learning. Efficient kernels are crucial for accelerating large language model (LLM) training and serving, yet attaining high performance typically requires extensive manual tuning. Compiler-based systems reduce some of this burden, but still demand substantial manual design and engineering effort. Recently, researchers have explored using LLMs for GPU kernel generation, though prior work has largely focused on translating high-level PyTorch modules into CUDA code. In this work, we introduce Astra, the first LLM-based multi-agent system for GPU kernel optimization. Unlike previous approaches, Astra starts from existing CUDA implementations extracted from SGLang, a widely deployed framework for serving LLMs, rather than treating PyTorch modules as the specification. Within Astra, specialized LLM agents collaborate through iterative code generation, testing, profiling, and planning to produce kernels that are both correct and high-performance. On kernels from SGLang, Astra achieves an average speedup of 1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study further demonstrates that LLMs can autonomously apply loop transformations, optimize memory access patterns, exploit CUDA intrinsics, and leverage fast math operations to yield substantial performance gains. Our work highlights multi-agent LLM systems as a promising new paradigm for GPU kernel optimization. 

**Abstract (ZH)**: 基于LLM的多Agent GPU内核优化系统Astra 

---
# Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition 

**Title (ZH)**: 通过局部混合和 logits 优化生成可移植的对抗样本以用于遥感目标识别 

**Authors**: Chun Liu, Hailong Wang, Bingqian Zhu, Panpan Ding, Zheng Zheng, Tao Xu, Zhigang Han, Jiayao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07495)  

**Abstract**: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing significant security threats to their deployment in remote sensing applications. Research on adversarial attacks not only reveals model vulnerabilities but also provides critical insights for enhancing robustness. Although current mixing-based strategies have been proposed to increase the transferability of adversarial examples, they either perform global blending or directly exchange a region in the images, which may destroy global semantic features and mislead the optimization of adversarial examples. Furthermore, their reliance on cross-entropy loss for perturbation optimization leads to gradient diminishing during iterative updates, compromising adversarial example quality. To address these limitations, we focus on non-targeted attacks and propose a novel framework via local mixing and logits optimization. First, we present a local mixing strategy to generate diverse yet semantically consistent inputs. Different from MixUp, which globally blends two images, and MixCut, which stitches images together, our method merely blends local regions to preserve global semantic information. Second, we adapt the logit loss from targeted attacks to non-targeted scenarios, mitigating the gradient vanishing problem of cross-entropy loss. Third, a perturbation smoothing loss is applied to suppress high-frequency noise and enhance transferability. Extensive experiments on FGSCR-42 and MTARSI datasets demonstrate superior performance over 12 state-of-the-art methods across 6 surrogate models. Notably, with ResNet as the surrogate on MTARSI, our method achieves a 17.28% average improvement in black-box attack success rate. 

**Abstract (ZH)**: 深度神经网络（DNNs）容易受到 adversarial 攻击，在遥感应用中的部署面临重大安全威胁。针对 adversarial 攻击的研究不仅揭示了模型的脆弱性，还提供了增强鲁棒性的关键见解。尽管已经提出了基于混合的策略来提高 adversarial 示例的可移植性，但这些策略要么进行全局混合，要么直接交换图像中的区域，这可能会破坏全局语义特征并误导 adversarial 示例的优化。此外，它们依赖于交叉熵损失来进行扰动优化，这会导致迭代更新过程中梯度衰减，降低 adversarial 示例的质量。为了解决这些问题，我们专注于非针对性攻击，并提出了一种基于局部混合和 logits 优化的新型框架。首先，我们提出了一种局部混合策略，以生成多样且语义一致的输入。与全局混合两个图像的 MixUp 不同，与将图像拼接在一起的 MixCut 不同，我们的方法仅混合局部区域，以保留全局语义信息。其次，我们将针对性攻击中的 logits 损失适应非针对性场景，缓解交叉熵损失的梯度消失问题。第三，应用扰动平滑损失以抑制高频噪声并提高可移植性。在 FGSCR-42 和 MTARSI 数据集上的广泛实验表明，我们的方法在 6 种替代模型上的性能优于 12 种当前最先进的方法。特别地，使用 ResNet 作为替代模型在 MTARSI 上，我们的方法在黑盒攻击成功率上平均提高了 17.28%。 

---
# Fine-Tuning Vision-Language Models for Visual Navigation Assistance 

**Title (ZH)**: 视觉语言模型的微调以实现视觉导航辅助 

**Authors**: Xiao Li, Bharat Gandhi, Ming Zhan, Mohit Nehra, Zhicheng Zhang, Yuchen Sun, Meijia Song, Naisheng Zhang, Xi Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07488)  

**Abstract**: We address vision-language-driven indoor navigation to assist visually impaired individuals in reaching a target location using images and natural language guidance. Traditional navigation systems are ineffective indoors due to the lack of precise location data. Our approach integrates vision and language models to generate step-by-step navigational instructions, enhancing accessibility and independence. We fine-tune the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose an evaluation metric that refines the BERT F1 score by emphasizing directional and sequential variables, providing a more comprehensive measure of navigational performance. After applying LoRA, the model significantly improved in generating directional instructions, overcoming limitations in the original BLIP-2 model. 

**Abstract (ZH)**: 基于视觉语言的室内导航以辅助视觉障碍个体到达目标位置：融合视觉和语言模型生成逐步导航指令 

---
# HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention 

**Title (ZH)**: HALT-RAG：一种基于校准NLI集成和弃权的任务自适应幻觉检测框架 

**Authors**: Saumya Goswami, Siddharth Kurra  

**Link**: [PDF](https://arxiv.org/pdf/2509.07475)  

**Abstract**: Detecting content that contradicts or is unsupported by a given source text is a critical challenge for the safe deployment of generative language models. We introduce HALT-RAG, a post-hoc verification system designed to identify hallucinations in the outputs of Retrieval-Augmented Generation (RAG) pipelines. Our flexible and task-adaptable framework uses a universal feature set derived from an ensemble of two frozen, off-the-shelf Natural Language Inference (NLI) models and lightweight lexical signals. These features are used to train a simple, calibrated, and task-adapted meta-classifier. Using a rigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and produce unbiased estimates, we evaluate our system on the HaluEval benchmark. By pairing our universal feature set with a lightweight, task-adapted classifier and a precision-constrained decision policy, HALT-RAG achieves strong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA, and dialogue tasks, respectively. The system's well-calibrated probabilities enable a practical abstention mechanism, providing a reliable tool for balancing model performance with safety requirements. 

**Abstract (ZH)**: 检测与给定来源文本矛盾或未被支持的内容是生成语言模型安全部署的关键挑战。我们引入HALT-RAG，一个后验证系统，旨在识别检索增强生成（RAG）管道输出中的幻觉。我们的灵活且任务适应的框架使用从两个冻结的现成自然语言推理（NLI）模型和轻量级词汇信号中衍生的通用特征集进行训练。这些特征用于训练一个简单、校准且任务适应的元分类器。通过使用严格的5折交叉验证训练协议来防止数据泄露并产生无偏估计，我们在HaluEval基准上评估了该系统。通过将通用特征集与轻量级任务适应分类器和精度受限的决策策略配对，HALT-RAG分别在摘要、问答和对话任务上实现了0.7756、0.9786和0.7391的交叉验证F1分数。系统的校准概率使其能够实现实际的回避机制，为平衡模型性能与安全要求提供可靠的工具。 

---
# DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis 

**Title (ZH)**: DepthVision: 基于GAN的LiDAR到RGB合成的鲁棒多模态理解 

**Authors**: Sven Kirchner, Nils Purschke, Ross Greer, Alois C. Knoll  

**Link**: [PDF](https://arxiv.org/pdf/2509.07463)  

**Abstract**: Ensuring reliable robot operation when visual input is degraded or insufficient remains a central challenge in robotics. This letter introduces DepthVision, a framework for multimodal scene understanding designed to address this problem. Unlike existing Vision-Language Models (VLMs), which use only camera-based visual input alongside language, DepthVision synthesizes RGB images from sparse LiDAR point clouds using a conditional generative adversarial network (GAN) with an integrated refiner network. These synthetic views are then combined with real RGB data using a Luminance-Aware Modality Adaptation (LAMA), which blends the two types of data dynamically based on ambient lighting conditions. This approach compensates for sensor degradation, such as darkness or motion blur, without requiring any fine-tuning of downstream vision-language models. We evaluate DepthVision on real and simulated datasets across various models and tasks, with particular attention to safety-critical tasks. The results demonstrate that our approach improves performance in low-light conditions, achieving substantial gains over RGB-only baselines while preserving compatibility with frozen VLMs. This work highlights the potential of LiDAR-guided RGB synthesis for achieving robust robot operation in real-world environments. 

**Abstract (ZH)**: 确保视觉输入降级或不足时机器人可靠运行仍然是机器人领域的核心挑战。本文介绍了DepthVision框架，该框架旨在解决这一问题。与现有的视觉-语言模型（VLMs）仅使用摄像头视觉输入结合语言不同，DepthVision通过条件生成对抗网络（GAN）结合嵌入式精炼网络，从稀疏LiDAR点云中合成RGB图像。这些合成视角随后与真实RGB数据结合使用Luminance-Aware Modality Adaptation（LAMA），根据环境光照条件动态融合两种类型的数据。这种方法可以在不需微调下游视觉-语言模型的情况下补偿传感器降级，例如黑暗或运动模糊。我们在各种模型和任务的真实和仿真数据集上评估了DepthVision，尤其关注安全性关键任务。实验结果表明，我们的方法在低光照条件下提高了性能，相对于仅使用RGB的基线实现了显著改进，同时保持与冻结的VLMs的兼容性。本文强调了LiDAR引导的RGB合成在实现实时环境中稳健机器人运行方面的潜在价值。 

---
# Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting 

**Title (ZH)**: 带有偏差意识的机器遗忘：通过可控遗忘实现更公平的视觉模型 

**Authors**: Sai Siddhartha Chary Aylapuram, Veeraraju Elluru, Shivang Agarwal  

**Link**: [PDF](https://arxiv.org/pdf/2509.07456)  

**Abstract**: Deep neural networks often rely on spurious correlations in training data, leading to biased or unfair predictions in safety-critical domains such as medicine and autonomous driving. While conventional bias mitigation typically requires retraining from scratch or redesigning data pipelines, recent advances in machine unlearning provide a promising alternative for post-hoc model correction. In this work, we investigate \textit{Bias-Aware Machine Unlearning}, a paradigm that selectively removes biased samples or feature representations to mitigate diverse forms of bias in vision models. Building on privacy-preserving unlearning techniques, we evaluate various strategies including Gradient Ascent, LoRA, and Teacher-Student distillation. Through empirical analysis on three benchmark datasets, CUB-200-2011 (pose bias), CIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection), we demonstrate that post-hoc unlearning can substantially reduce subgroup disparities, with improvements in demographic parity of up to \textbf{94.86\%} on CUB-200, \textbf{30.28\%} on CIFAR-10, and \textbf{97.37\%} on CelebA. These gains are achieved with minimal accuracy loss and with methods scoring an average of 0.62 across the 3 settings on the joint evaluation of utility, fairness, quality, and privacy. Our findings establish machine unlearning as a practical framework for enhancing fairness in deployed vision systems without necessitating full retraining. 

**Abstract (ZH)**: Bias-Aware Machine Unlearning: Selectively Removing Biased Samples or Feature Representations for Post-Hoc Mitigation of Bias in Vision Models 

---
# Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions 

**Title (ZH)**: Text2Touch: 基于LLM设计奖励函数的手中触觉操作 

**Authors**: Harrison Field, Max Yang, Yijiong Lin, Efi Psomopoulou, David Barton, Nathan F. Lepora  

**Link**: [PDF](https://arxiv.org/pdf/2509.07445)  

**Abstract**: Large language models (LLMs) are beginning to automate reward design for dexterous manipulation. However, no prior work has considered tactile sensing, which is known to be critical for human-like dexterity. We present Text2Touch, bringing LLM-crafted rewards to the challenging task of multi-axis in-hand object rotation with real-world vision based tactile sensing in palm-up and palm-down configurations. Our prompt engineering strategy scales to over 70 environment variables, and sim-to-real distillation enables successful policy transfer to a tactile-enabled fully actuated four-fingered dexterous robot hand. Text2Touch significantly outperforms a carefully tuned human-engineered baseline, demonstrating superior rotation speed and stability while relying on reward functions that are an order of magnitude shorter and simpler. These results illustrate how LLM-designed rewards can significantly reduce the time from concept to deployable dexterous tactile skills, supporting more rapid and scalable multimodal robot learning. Project website: this https URL 

**Abstract (ZH)**: 基于大型语言模型的Text2Touch：将语言模型设计的奖励应用于掌上多轴物体旋转的真实世界视觉触觉感知任务 

---
# The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward 

**Title (ZH)**: 散度的选择：减轻强化学习中可验证奖励下多样性崩溃的关键因素 

**Authors**: Long Li, Jiaran Hao, Jason Klein Liu, Zhijian Zhou, Xiaoyu Tan, Wei Chu, Zhe Wang, Shirui Pan, Chao Qu, Yuan Qi  

**Link**: [PDF](https://arxiv.org/pdf/2509.07430)  

**Abstract**: A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. While various methods have been proposed, the choice and function of the divergence term have been surprisingly unexamined as a proactive solution. We argue that standard RLVR objectives -- both those using the mode-seeking reverse KL-divergence and those forgoing a divergence term entirely -- lack a crucial mechanism for knowledge retention. The reverse-KL actively accelerates this decay by narrowing the policy, while its absence provides no safeguard against the model drifting from its diverse knowledge base. We propose a fundamental shift in perspective: using the divergence term itself as the solution. Our framework, Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences (like forward-KL and JS-divergence) to function as a rehearsal mechanism. By continuously referencing the initial policy, this approach forces the model to maintain broad solution coverage. Extensive experiments on math and SQL generation demonstrate that DPH-RL not only resolves the Pass@k degradation but improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is more training-efficient because it computes f-divergence using generator functions, requiring only sampling from the initial policy and no online reference model. Our work highlights a crucial, overlooked axis for improving RLVR, demonstrating that the proper selection of a divergence measure is a powerful tool for building more general and diverse reasoning models. 

**Abstract (ZH)**: 在使用可验证奖励的强化学习（RLVR）微调大型语言模型（LLMs）时的一个核心悖论：尽管单次尝试准确性（Pass@1）有所提升，但多尝试性能（Pass@k）频繁下降，同时往往伴随着灾难性遗忘，模型失去已获取的技能。虽然已提出多种方法，但分歧项的选择和功能作为积极解决方案的研究却出人意料地缺乏。我们认为，标准的RLVR目标——无论是使用模式搜索逆KL散度还是完全不使用分歧项的做法——都缺乏一个关键的知识保持机制。逆KL加速了这种退化，因为它限制了策略范围，而缺乏分歧项则无法防止模型偏离其多元知识库。我们提出了一个基本的视角转变：将分歧项本身作为解决方案。我们的框架，保多样强化学习混合算法（DPH-RL），利用质量覆盖的f散度（如向前KL散度和JS散度）作为巩固机制。通过不断参考初始策略，这种做法迫使模型保持广泛解决方案的覆盖。大量关于数学和SQL生成的实验表明，DPH-RL不仅可以解决Pass@k下降的问题，还能提高域内和域外的Pass@1和Pass@k性能。此外，DPH-RL更具训练效率，因为它使用生成函数计算f散度，仅需从初始策略采样而不需要在线参考模型。我们的工作强调了一个关键且被忽视的维度，即改进RLVR的方法，证明了适当选择分歧度量是一个构建更具普适性和多样性的推理模型的强大工具。 

---
# Benchmarking Universal Interatomic Potentials on Zeolite Structures 

**Title (ZH)**: Benchmarking Universal Interatomic Potentials on Zeolite Structures（沸石结构上的通用原子势能基准测试） 

**Authors**: Shusuke Ito, Koki Muraoka, Akira Nakayama  

**Link**: [PDF](https://arxiv.org/pdf/2509.07417)  

**Abstract**: Interatomic potentials (IPs) with wide elemental coverage and high accuracy are powerful tools for high-throughput materials discovery. While the past few years witnessed the development of multiple new universal IPs that cover wide ranges of the periodic table, their applicability to target chemical systems should be carefully investigated. We benchmark several universal IPs using equilibrium zeolite structures as testbeds. We select a diverse set of universal IPs encompassing two major categories: (i) universal analytic IPs, including GFN-FF, UFF, and Dreiding; (ii) pretrained universal machine learning IPs (MLIPs), comprising CHGNet, ORB-v3, MatterSim, eSEN-30M-OAM, PFP-v7, and EquiformerV2-lE4-lF100-S2EFS-OC22. We compare them with established tailor-made IPs, SLC, ClayFF, and BSFF using experimental data and density functional theory (DFT) calculations with dispersion correction as the reference. The tested zeolite structures comprise pure silica frameworks and aluminosilicates containing copper species, potassium, and organic cations. We found that GFN-FF is the best among the tested universal analytic IPs, but it does not achieve satisfactory accuracy for highly strained silica rings and aluminosilicate systems. All MLIPs can well reproduce experimental or DFT-level geometries and energetics. Among the universal MLIPs, the eSEN-30M-OAM model shows the most consistent performance across all zeolite structures studied. These findings show that the modern pretrained universal MLIPs are practical tools in zeolite screening workflows involving various compositions. 

**Abstract (ZH)**: 广覆盖高精度的原子势在高通量材料发现中的应用：以平衡骨架沸石结构为基准的评估 

---
# Toward Lifelong-Sustainable Electronic-Photonic AI Systems via Extreme Efficiency, Reconfigurability, and Robustness 

**Title (ZH)**: 面向极致高效、重构能力和鲁棒性的终身可持续电子-光子AI系统 

**Authors**: Ziang Yin, Hongjian Zhou, Chetan Choppali Sudarshan, Vidya Chhabria, Jiaqi Gu  

**Link**: [PDF](https://arxiv.org/pdf/2509.07396)  

**Abstract**: The relentless growth of large-scale artificial intelligence (AI) has created unprecedented demand for computational power, straining the energy, bandwidth, and scaling limits of conventional electronic platforms. Electronic-photonic integrated circuits (EPICs) have emerged as a compelling platform for next-generation AI systems, offering inherent advantages in ultra-high bandwidth, low latency, and energy efficiency for computing and interconnection. Beyond performance, EPICs also hold unique promises for sustainability. Fabricated in relaxed process nodes with fewer metal layers and lower defect densities, photonic devices naturally reduce embodied carbon footprint (CFP) compared to advanced digital electronic integrated circuits, while delivering orders-of-magnitude higher computing performance and interconnect bandwidth. To further advance the sustainability of photonic AI systems, we explore how electronic-photonic design automation (EPDA) and cross-layer co-design methodologies can amplify these inherent benefits. We present how advanced EPDA tools enable more compact layout generation, reducing both chip area and metal layer usage. We will also demonstrate how cross-layer device-circuit-architecture co-design unlocks new sustainability gains for photonic hardware: ultra-compact photonic circuit designs that minimize chip area cost, reconfigurable hardware topology that adapts to evolving AI workloads, and intelligent resilience mechanisms that prolong lifetime by tolerating variations and faults. By uniting intrinsic photonic efficiency with EPDA- and co-design-driven gains in area efficiency, reconfigurability, and robustness, we outline a vision for lifelong-sustainable electronic-photonic AI systems. This perspective highlights how EPIC AI systems can simultaneously meet the performance demands of modern AI and the urgent imperative for sustainable computing. 

**Abstract (ZH)**: 大规模人工智能（AI）的持续增长创造了前所未有的计算需求，对传统电子平台的能源、带宽和扩展极限形成压力。电子-光子集成电路（EPICs）已成为下一代AI系统有吸引力的平台，提供了超高带宽、低延迟和能效计算与互连的固有优势。除了性能，EPICs还为可持续性提供了独特的前景。利用更宽松的工艺节点、更少的金属层和更低的缺陷密度制造光子器件，自然减少了与先进数字电子集成电路相比的物质碳足迹（CFP），同时提供了数量级更高的计算性能和互连带宽。为推进光子AI系统的可持续性，我们探讨了如何利用电子-光子设计自动化（EPDA）和跨层协同设计方法进一步放大这些固有优势。我们展示了先进的EPDA工具如何实现更紧凑的布局生成，减少芯片面积和金属层使用。我们还将展示跨层器件-电路-架构协同设计如何为光子硬件解锁新的可持续性收益：极紧凑的光子电路设计以最小化芯片面积成本、可重构的硬件拓扑以适应不断变化的AI工作负载，以及智能的容错机制以通过容忍变异和故障来延长寿命。通过结合固有的光子效率与EPDA和协同设计驱动的空间效率、可重构性和鲁棒性增益，我们概述了一种终身可持续的电子-光子AI系统愿景。本文视角突显了EPIC AI系统如何同时满足现代AI的性能需求和对可持续计算的紧迫需求。 

---
# Hybrid GCN-GRU Model for Anomaly Detection in Cryptocurrency Transactions 

**Title (ZH)**: 基于GCN-GRU的混合模型在数字货币交易中的异常检测 

**Authors**: Gyuyeon Na, Minjung Park, Hyeonjeong Cha, Soyoun Kim, Sunyoung Moon, Sua Lee, Jaeyoung Choi, Hyemin Lee, Sangmi Chai  

**Link**: [PDF](https://arxiv.org/pdf/2509.07392)  

**Abstract**: Blockchain transaction networks are complex, with evolving temporal patterns and inter-node relationships. To detect illicit activities, we propose a hybrid GCN-GRU model that captures both structural and sequential features. Using real Bitcoin transaction data (2020-2024), our model achieved 0.9470 Accuracy and 0.9807 AUC-ROC, outperforming all baselines. 

**Abstract (ZH)**: 区块链交易网络复杂多变，具有 evolving 的时间和节点间关系。为了检测非法活动，我们提出了一种混合 GCN-GRU 模型，该模型同时捕捉结构和序列特征。使用 2020-2024 年的真实比特币交易数据，我们的模型实现了 0.9470 的准确率和 0.9807 的 AUC-ROC，优于所有基线。 

---
# Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents 

**Title (ZH)**: 与奥ompa Loompas对话：一种评估LLM代理语言获取的新框架 

**Authors**: Sankalp Tattwadarshi Swain, Anshika Krishnatray, Dhruv Kumar, Jagat Sesh Challa  

**Link**: [PDF](https://arxiv.org/pdf/2509.07389)  

**Abstract**: Existing evaluation studies on linguistic competence of large language models (LLM agents) have focused primarily on vocabulary learning, morphological rule induction, syntactic generalization, pragmatic inference, and cross-linguistic transfer. However, none assess whether LLM agents can acquire a language through pattern recognition and interactive feedback, a central feature of human language acquisition. We propose a novel experimental framework in which an LLM agent is evaluated on its ability to acquire and use a newly constructed language (Tinkatongue) in conversation with a bot that understands only Tinkatongue. Our findings show that LLM agents fail to establish a conversation within 100 responses, yet they adopt distinct strategies that mirror human approaches to language learning. The results suggest a new direction for evaluation benchmarks and open pathways to model designs that learn more effectively from interactive feedback. 

**Abstract (ZH)**: 现有的语言能力评估研究主要集中在词汇学习、词法规则归纳、句法概括、语用推理以及跨语言迁移上。然而，这些研究未评估LLM代理是否能够通过模式识别和互动反馈来习得一门语言，而这正是人类语言习得的核心特征。我们提出了一种新的实验框架，通过让一个LLM代理与仅理解Tinkatongue的机器人进行对话来评估其获取和使用新构建语言（Tinkatongue）的能力。我们的研究发现，LLM代理在100次响应内无法建立对话，但它们采用的策略却类似于人类的语言学习方法。结果表明，这为评估基准提供了新的方向，并为更有效地从互动反馈中学习的模型设计开辟了路径。 

---
# SBS: Enhancing Parameter-Efficiency of Neural Representations for Neural Networks via Spectral Bias Suppression 

**Title (ZH)**: SBS: 通过抑制谱偏倚提高神经网络中神经表示的参数效率 

**Authors**: Qihu Xie, Yuan Li, Yi Kang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07373)  

**Abstract**: Implicit neural representations have recently been extended to represent convolutional neural network weights via neural representation for neural networks, offering promising parameter compression benefits. However, standard multi-layer perceptrons used in neural representation for neural networks exhibit a pronounced spectral bias, hampering their ability to reconstruct high-frequency details effectively. In this paper, we propose SBS, a parameter-efficient enhancement to neural representation for neural networks that suppresses spectral bias using two techniques: (1) a unidirectional ordering-based smoothing that improves kernel smoothness in the output space, and (2) unidirectional ordering-based smoothing aware random fourier features that adaptively modulate the frequency bandwidth of input encodings based on layer-wise parameter count. Extensive evaluations on various ResNet models with datasets CIFAR-10, CIFAR-100, and ImageNet, demonstrate that SBS achieves significantly better reconstruction accuracy with less parameters compared to SOTA. 

**Abstract (ZH)**: 隐神经表示 recently 已经被扩展用于表示卷积神经网络权重，通过神经网络的神经表示，提供了有希望的参数压缩益处。然而，用于神经网络神经表示的标准多层感知机表现出明显的频谱偏差，阻碍了它们有效重建高频细节的能力。在本文中，我们提出一种名为 SBS 的参数高效增强方法，通过两种技术抑制频谱偏差：(1) 基于单向排序的平滑技术，改进了输出空间中的核平滑性；(2) 基于单向排序的平滑技术的随机傅里叶特征，根据不同层的参数数量自适应调节输入编码的频率带宽。在 ResNet 模型上使用 CIFAR-10、CIFAR-100 和 ImageNet 数据集的广泛评估表明，SBS 在较少参数的情况下实现了显著更好的重构精度。 

---
# Word2Spike: Poisson Rate Coding for Associative Memories and Neuromorphic Algorithms 

**Title (ZH)**: Word2Spike: 泊松率编码用于关联记忆和类脑算法 

**Authors**: Archit Kalra, Midhun Sadanand  

**Link**: [PDF](https://arxiv.org/pdf/2509.07361)  

**Abstract**: Spiking neural networks offer a promising path toward energy-efficient, brain-like associative memory. This paper introduces Word2Spike, a novel rate coding mechanism that combines continuous word embeddings and neuromorphic architectures. We develop a one-to-one mapping that converts multi-dimensional word vectors into spike-based attractor states using Poisson processes. Using BitNet b1.58 quantization, we maintain 97% semantic similarity of continuous embeddings on SimLex-999 while achieving 100% reconstruction accuracy on 10,000 words from OpenAI's text-embedding-3-large. We preserve analogy performance (100% of original embedding performance) even under intentionally introduced noise, indicating a resilient mechanism for semantic encoding in neuromorphic systems. Next steps include integrating the mapping with spiking transformers and liquid state machines (resembling Hopfield Networks) for further evaluation. 

**Abstract (ZH)**: 突触神经网络为能量高效的类脑关联记忆提供了有前途的道路。本文介绍了Word2Spike，一种结合连续词嵌入和神经形态架构的新率编码机制。我们开发了一对一的映射，使用泊松过程将多维词向量转换为基于突触的吸引子状态。通过BitNet b1.58量化，我们在SimLex-999上保持了97%的语义相似性的同时，在OpenAI的text-embedding-3-large的10,000个单词上实现了100%的重建准确性。即使在故意引入的噪声下，我们仍保留了类比性能（原始嵌入性能的100%），表明了神经形态系统中语义编码的鲁棒机制。下一步包括将映射与突触变换器和液态状态机（类似霍普菲尔德网络）集成，以进行进一步评估。 

---
# General Demographic Foundation Models for Enhancing Predictive Performance Across Diseases 

**Title (ZH)**: 通用人口统计基础模型在改善跨疾病预测性能中的应用 

**Authors**: Li-Chin Chen, Ji-Tian Sheu, Yuh-Jue Chuang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07330)  

**Abstract**: Demographic attributes are universally present in electronic health records and serve as vital predictors in clinical risk stratification and treatment decisions. Despite their significance, these attributes are often relegated to auxiliary roles in model design, with limited attention has been given to learning their representations. This study proposes a General Demographic Pre-trained (GDP) model as a foundational representation framework tailored to age and gender. The model is pre-trained and evaluated using datasets with diverse diseases and population compositions from different geographic regions. The GDP architecture explores combinations of ordering strategies and encoding methods to transform tabular demographic inputs into latent embeddings. Experimental results demonstrate that sequential ordering substantially improves model performance in discrimination, calibration, and the corresponding information gain at each decision tree split, particularly in diseases where age and gender contribute significantly to risk stratification. Even in datasets where demographic attributes hold relatively low predictive value, GDP enhances the representational importance, increasing their influence in downstream gradient boosting models. The findings suggest that foundational models for tabular demographic attributes can generalize across tasks and populations, offering a promising direction for improving predictive performance in healthcare applications. 

**Abstract (ZH)**: Demographic 特征在电子健康记录中普遍存在，是临床风险分层和治疗决策中的关键预测因子。尽管这些特征非常重要，但在模型设计中往往只扮演辅助角色，对它们的表示学习关注不足。本研究提出了一种通用年龄性别预训练（GDP）模型，作为针对年龄和性别的基础表示框架。该模型使用来自不同地理区域且疾病和人口组成多样化的数据集进行预训练和评估。GDP 架构探索排序策略和编码方法的组合，将表格式的Demographic输入转换为潜在嵌入。实验结果表明，序列排序在区分度、校准度以及每个决策树分叉处的相关信息增益方面显著提高了模型性能，特别是在年龄和性别对风险分层有重大影响的疾病中。即使在Demographic特征预测价值相对较低的数据集中，GDP 也能增强其表示的重要性，提高其在下游梯度提升模型中的影响力。研究结果表明，针对表格式Demographic特征的基础模型可以在不同任务和人群中泛化，为在医疗保健应用中提高预测性能提供了有希望的方向。 

---
# DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion 

**Title (ZH)**: DEPF：一种基于双域增强和优先级导向Mamba融合的无人机多谱段对象检测器 

**Authors**: Shucong Li, Zhenyu Liu, Zijie Hong, Zhiheng Zhou, Xianghai Cao  

**Link**: [PDF](https://arxiv.org/pdf/2509.07327)  

**Abstract**: Multispectral remote sensing object detection is one of the important application of unmanned aerial vehicle (UAV). However, it faces three challenges. Firstly, the low-light remote sensing images reduce the complementarity during multi-modality fusion. Secondly, the local small target modeling is interfered with redundant information in the fusion stage easily. Thirdly, due to the quadratic computational complexity, it is hard to apply the transformer-based methods on the UAV platform. To address these limitations, motivated by Mamba with linear complexity, a UAV multispectral object detector with dual-domain enhancement and priority-guided mamba fusion (DEPF) is proposed. Firstly, to enhance low-light remote sensing images, Dual-Domain Enhancement Module (DDE) is designed, which contains Cross-Scale Wavelet Mamba (CSWM) and Fourier Details Recovery block (FDR). CSWM applies cross-scale mamba scanning for the low-frequency components to enhance the global brightness of images, while FDR constructs spectrum recovery network to enhance the frequency spectra features for recovering the texture-details. Secondly, to enhance local target modeling and reduce the impact of redundant information during fusion, Priority-Guided Mamba Fusion Module (PGMF) is designed. PGMF introduces the concept of priority scanning, which starts from local targets features according to the priority scores obtained from modality difference. Experiments on DroneVehicle dataset and VEDAI dataset reports that, DEPF performs well on object detection, comparing with state-of-the-art methods. Our code is available in the supplementary material. 

**Abstract (ZH)**: 基于双域增强和优先级引导Mamba融合的UAV多光谱目标检测 

---
# Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation 

**Title (ZH)**: 小规模中的注意力局部化缓解：基于一次性信念传播的自我注意力细化 

**Authors**: Nakyung Lee, Yeongoon Kim, Minhae Oh, Suhwan Kim, Jin Woo Koo, Hyewon Jo, Jungwoo Lee  

**Link**: [PDF](https://arxiv.org/pdf/2509.07324)  

**Abstract**: Transformer-based self-attention mechanism serves as the core of modern language models, yet it often suffers from localization, where attentions collapse onto a limited subset of tokens and fail to capture long-range dependencies. To address this issue, we propose Self-Attention One-step Belief Propagation (SAOBP), a refinement framework that injects multi-hop relationships through a belief propagation process. To interpret and quantify these interactions, we introduce Global Token Dependency (GTD) that captures the relative contribution of multihop connections within the attention graph. Empirical results indicate that SAOBP helps prevent entropy collapse in deeper layers and adaptively maintains GTD at task-appropriate levels, thereby supporting improvements in model performance. Importantly, we observe competitive gains in small-scale models, highlighting its potential for improving inference quality in resource-constrained scenarios. 

**Abstract (ZH)**: 基于Transformer的自注意力机理是现代语言模型的核心，但常常会遭受本地化问题，即注意力集中在少量的令牌上，导致无法捕捉长距离依赖。为了解决这个问题，我们提出了Self-Attention One-step Belief Propagation (SAOBP) 精炼框架，该框架通过信念传播过程注入多跳关系。为了解释和量化这些交互，我们引入了全局令牌依赖性（GTD），它捕获了注意图中多跳连接的相对贡献。实验证明，SAOBP有助于在更深的层中防止熵塌缩，并适当地保持任务适当的GTD水平，从而支持模型性能的提升。重要的是，我们在小规模模型中观察到了竞争力的提升，突显了其在资源受限场景中提高推理质量的潜力。 

---
# MEGG: Replay via Maximally Extreme GGscore in Incremental Learning for Neural Recommendation Models 

**Title (ZH)**: MEGG：增量学习中基于GGscore极值的重放方法在神经推荐模型中的应用 

**Authors**: Yunxiao Shi, Shuo Yang, Haimin Zhang, Li Wang, Yongze Wang, Qiang Wu, Min Xu  

**Link**: [PDF](https://arxiv.org/pdf/2509.07319)  

**Abstract**: Neural Collaborative Filtering models are widely used in recommender systems but are typically trained under static settings, assuming fixed data distributions. This limits their applicability in dynamic environments where user preferences evolve. Incremental learning offers a promising solution, yet conventional methods from computer vision or NLP face challenges in recommendation tasks due to data sparsity and distinct task paradigms. Existing approaches for neural recommenders remain limited and often lack generalizability. To address this, we propose MEGG, Replay Samples with Maximally Extreme GGscore, an experience replay based incremental learning framework. MEGG introduces GGscore, a novel metric that quantifies sample influence, enabling the selective replay of highly influential samples to mitigate catastrophic forgetting. Being model-agnostic, MEGG integrates seamlessly across architectures and frameworks. Experiments on three neural models and four benchmark datasets show superior performance over state-of-the-art baselines, with strong scalability, efficiency, and robustness. Implementation will be released publicly upon acceptance. 

**Abstract (ZH)**: 基于最大极端GG分数重放样本的增量学习框架MEGG 

---
# Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations 

**Title (ZH)**: 你觉得这个看起来眼熟吗？基于模型内部表示的知识分析 

**Authors**: Sihyun Park  

**Link**: [PDF](https://arxiv.org/pdf/2509.07311)  

**Abstract**: Recent advances in large language models (LLMs) have been driven by pretraining, supervised fine tuning (SFT), and alignment tuning. Among these, SFT plays a crucial role in transforming a model 's general knowledge into structured responses tailored to specific tasks. However, there is no clearly established methodology for effective training data selection. Simply increasing the volume of data does not guarantee performance improvements, while preprocessing, sampling, and validation require substantial time and cost.
To address this issue, a variety of data selection methods have been proposed. Among them, knowledge based selection approaches identify suitable training data by analyzing the model 's responses. Nevertheless, these methods typically rely on prompt engineering, making them sensitive to variations and incurring additional costs for prompt design.
In this study, we propose Knowledge Analysis via Model Internal Representations (KAMIR), a novel approach that overcomes these limitations by analyzing data based on the model 's internal representations. KAMIR computes similarities between the hidden states of each layer (block) and the final hidden states for a given input to assess the data. Unlike prior methods that were largely limited to multiple choice tasks, KAMIR can be applied to a wide range of tasks such as machine reading comprehension and summarization. Moreover, it selects data useful for training based on the model 's familiarity with the input, even with a small dataset and a simple classifier architecture. Experiments across diverse task datasets demonstrate that training with less familiar data leads to better generalization performance. 

**Abstract (ZH)**: Recent Advances in Large Language Models: Knowledge Analysis via Model Internal Representations (KAMIR) 

---
# Basis Vector Metric: A Method for Robust Open-Ended State Change Detection 

**Title (ZH)**: 基向量度量：一种稳健的开放性状态变化检测方法 

**Authors**: David Oprea, Sam Powers  

**Link**: [PDF](https://arxiv.org/pdf/2509.07308)  

**Abstract**: We test a new method, which we will abbreviate using the acronym BVM (Basis Vectors Method), in its ability to judge the state changes in images through using language embeddings. We used the MIT-States dataset, containing about 53,000 images, to gather all of our data, which has 225 nouns and 115 adjectives, with each noun having about 9 different adjectives, forming approximately 1000 noun-adjective pairs. For our first experiment, we test our method's ability to determine the state of each noun class separately against other metrics for comparison. These metrics are cosine similarity, dot product, product quantization, binary index, Naive Bayes, and a custom neural network. Among these metrics, we found that our proposed BVM performs the best in classifying the states for each noun. We then perform a second experiment where we try using BVM to determine if it can differentiate adjectives from one another for each adjective separately. We compared the abilities of BVM to differentiate adjectives against the proposed method the MIT-States paper suggests: using a logistic regression model. In the end, we did not find conclusive evidence that our BVM metric could perform better than the logistic regression model at discerning adjectives. Yet, we were able to find evidence for possible improvements to our method; this leads to the chance of increasing our method's accuracy through certain changes in our methodologies. 

**Abstract (ZH)**: 我们测试了一种新方法（我们将其缩写为BVM，即Basis Vectors Method），该方法通过使用语言嵌入来判断图像状态变化的能力。我们使用包含约53,000张图像的MIT-States数据集来收集所有数据，该数据集包含225个名词和115个形容词，每个名词约有9个不同的形容词，形成大约1000个名词-形容词对。在第一个实验中，我们测试了该方法在与其他指标对比时单独判断每个名词类状态的能力。这些指标包括余弦相似度、点积、产品量化、二进制索引、朴素贝叶斯和自定义神经网络。在这些指标中，我们发现我们提出的方法BVM在分类每个名词的状态方面表现最佳。在第二个实验中，我们尝试使用BVM来判断它是否可以单独区分每个形容词。我们将BVM区分形容词的能力与其所建议的方法——使用逻辑回归模型——进行了比较。最终，我们没有找到确凿证据表明我们的BVM指标在区分形容词方面比逻辑回归模型表现更好。然而，我们确实找到了改进方法的可能性证据；这为我们通过某些方法学上的调整提高方法准确性提供了机会。 

---
# Reconstruction Alignment Improves Unified Multimodal Models 

**Title (ZH)**: 重建对齐提升统一多模态模型 

**Authors**: Ji Xie, Trevor Darrell, Luke Zettlemoyer, XuDong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07295)  

**Abstract**: Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense "text prompts," providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit 6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs 

**Abstract (ZH)**: 统一多模态模型的重建对齐（Reconstruction Alignment for Unified Multimodal Models） 

---
# zkUnlearner: A Zero-Knowledge Framework for Verifiable Unlearning with Multi-Granularity and Forgery-Resistance 

**Title (ZH)**: zkUnlearner：一种具有多粒度和伪造抵抗性的可验证遗忘零知识框架 

**Authors**: Nan Wang, Nan Wu, Xiangyu Hui, Jiafan Wang, Xin Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2509.07290)  

**Abstract**: As the demand for exercising the "right to be forgotten" grows, the need for verifiable machine unlearning has become increasingly evident to ensure both transparency and accountability. We present {\em zkUnlearner}, the first zero-knowledge framework for verifiable machine unlearning, specifically designed to support {\em multi-granularity} and {\em forgery-resistance}.
First, we propose a general computational model that employs a {\em bit-masking} technique to enable the {\em selectivity} of existing zero-knowledge proofs of training for gradient descent algorithms. This innovation enables not only traditional {\em sample-level} unlearning but also more advanced {\em feature-level} and {\em class-level} unlearning. Our model can be translated to arithmetic circuits, ensuring compatibility with a broad range of zero-knowledge proof systems. Furthermore, our approach overcomes key limitations of existing methods in both efficiency and privacy. Second, forging attacks present a serious threat to the reliability of unlearning. Specifically, in Stochastic Gradient Descent optimization, gradients from unlearned data, or from minibatches containing it, can be forged using alternative data samples or minibatches that exclude it. We propose the first effective strategies to resist state-of-the-art forging attacks. Finally, we benchmark a zkSNARK-based instantiation of our framework and perform comprehensive performance evaluations to validate its practicality. 

**Abstract (ZH)**: 基于零知识的可验证机器遗忘系统zkUnlearner：多粒度和抗伪造设计 

---
# Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm 

**Title (ZH)**: Paladin: 以新型触发-标签范式防御基于LLM的钓鱼邮件 

**Authors**: Yan Pang, Wenlong Meng, Xiaojing Liao, Tianhao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07287)  

**Abstract**: With the rapid development of large language models, the potential threat of their malicious use, particularly in generating phishing content, is becoming increasingly prevalent. Leveraging the capabilities of LLMs, malicious users can synthesize phishing emails that are free from spelling mistakes and other easily detectable features. Furthermore, such models can generate topic-specific phishing messages, tailoring content to the target domain and increasing the likelihood of success.
Detecting such content remains a significant challenge, as LLM-generated phishing emails often lack clear or distinguishable linguistic features. As a result, most existing semantic-level detection approaches struggle to identify them reliably. While certain LLM-based detection methods have shown promise, they suffer from high computational costs and are constrained by the performance of the underlying language model, making them impractical for large-scale deployment.
In this work, we aim to address this issue. We propose Paladin, which embeds trigger-tag associations into vanilla LLM using various insertion strategies, creating them into instrumented LLMs. When an instrumented LLM generates content related to phishing, it will automatically include detectable tags, enabling easier identification. Based on the design on implicit and explicit triggers and tags, we consider four distinct scenarios in our work. We evaluate our method from three key perspectives: stealthiness, effectiveness, and robustness, and compare it with existing baseline methods. Experimental results show that our method outperforms the baselines, achieving over 90% detection accuracy across all scenarios. 

**Abstract (ZH)**: 随着大型语言模型的快速发展，它们恶意使用尤其是生成钓鱼内容的潜在威胁日益突出。利用大型语言模型的能力，恶意用户可以合成无拼写错误和其他易检测特征的钓鱼邮件。此外，这些模型可以生成主题特定的钓鱼信息，量身定制内容以适应目标领域，从而提高成功几率。
检测此类内容仍然是一项重大挑战，因为大型语言模型生成的钓鱼邮件往往缺乏清晰或可区分的语言特征。因此，大多数现有的语义级检测方法难以可靠地识别它们。虽然某些基于大型语言模型的检测方法显示出潜力，但它们受高性能的制约，计算成本高，使其不适合大规模部署。
为了解决这一问题，我们提出Paladin，该方法通过各种插入策略将触发器-标签关联嵌入到基本的大规模语言模型中，将其转化为带有监测功能的大规模语言模型。当带有监测功能的大规模语言模型生成与钓鱼相关的内容时，将自动包含可检测的标签，从而便于识别。基于隐式和显式触发器及标签的设计，我们在工作中考虑了四种不同的场景。我们从隐蔽性、有效性及稳健性三个方面评估该方法，并与现有基准方法进行比较。实验结果表明，我们的方法优于基准方法，在所有场景中达到超过90%的检测准确率。 

---
# ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers 

**Title (ZH)**: ALICE: 一种可解释的神经架构，用于替代密码中的泛化 

**Authors**: Jeff Shen, Lindsay Smith  

**Link**: [PDF](https://arxiv.org/pdf/2509.07282)  

**Abstract**: We present cryptogram solving as an ideal testbed for studying neural network generalization in combinatorially complex domains. In this task, models must decrypt text encoded with substitution ciphers, choosing from 26! possible mappings without explicit access to the cipher. We develop ALICE (an Architecture for Learning Interpretable Cryptogram dEcipherment): a simple encoder-only Transformer that sets a new state-of-the-art for both accuracy and speed on this decryption problem. Surprisingly, ALICE generalizes to unseen ciphers after training on only ${\sim}1500$ unique ciphers, a minute fraction ($3.7 \times 10^{-24}$) of the possible cipher space. To enhance interpretability, we introduce a novel bijective decoding head that explicitly models permutations via the Gumbel-Sinkhorn method, enabling direct extraction of learned cipher mappings. Through early exit analysis, we reveal how ALICE progressively refines its predictions in a way that appears to mirror common human strategies for this task: early layers employ frequency-based heuristics, middle layers form word structures, and final layers correct individual characters. Our architectural innovations and analysis methods extend beyond cryptograms to any domain with bijective mappings and combinatorial structure, offering new insights into neural network generalization and interpretability. 

**Abstract (ZH)**: 我们将密码破解视为研究神经网络在组合复杂领域泛化能力的理想试验床。在这个任务中，模型必须解密使用置换密码编码的文本，选择从中解出的映射多达26!种，而无需显式访问密码。我们开发了ALICE（一种学习可解析密码破解的架构）：一个简单的仅编码器Transformer，在此解密问题上同时达到了准确性和速度的新标准。令人惊讶的是，ALICE在仅训练于约1500种独特密码后便能够泛化到未见过的密码，这是可能的密码空间的一分钟分数（3.7×10^(-24)）。为增强可解释性，我们引入了一种新颖的双射解码头，通过Gumbel-Sinkhorn方法显式建模置换，从而可以直接提取所学习的密码映射。通过早期退出分析，我们揭示了ALICE如何逐步细化其预测，这种方式似乎与人类解决此任务时常用的方法相当吻合：早期层使用基于频率的启发式方法，中间层形成单词结构，最终层修正单个字符。我们的架构创新和分析方法不仅限于密码破解领域，还可扩展到任何具有双射映射和组合结构的领域，提供了关于神经网络泛化能力和可解释性的新见解。 

---
# Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion 

**Title (ZH)**: 基于扩散增强和非线性特征融合的热图乳腺癌检测 

**Authors**: Sepehr Salem, M. Moein Esfahani, Jingyu Liu, Vince Calhoun  

**Link**: [PDF](https://arxiv.org/pdf/2509.07277)  

**Abstract**: Data scarcity hinders deep learning for medical imaging. We propose a framework for breast cancer classification in thermograms that addresses this using a Diffusion Probabilistic Model (DPM) for data augmentation. Our DPM-based augmentation is shown to be superior to both traditional methods and a ProGAN baseline. The framework fuses deep features from a pre-trained ResNet-50 with handcrafted nonlinear features (e.g., Fractal Dimension) derived from U-Net segmented tumors. An XGBoost classifier trained on these fused features achieves 98.0\% accuracy and 98.1\% sensitivity. Ablation studies and statistical tests confirm that both the DPM augmentation and the nonlinear feature fusion are critical, statistically significant components of this success. This work validates the synergy between advanced generative models and interpretable features for creating highly accurate medical diagnostic tools. 

**Abstract (ZH)**: 数据稀缺性阻碍了医疗影像领域的深度学习应用。我们提出了一种基于扩散概率模型（DPM）的数据增强框架，用于热像中的乳腺癌分类。该DPM增强方法在数据增强方面优于传统方法和ProGAN基线。该框架将预训练的ResNet-50提取的深度特征与来自U-Net分割肿瘤的手工构建非线性特征（例如，分形维数）融合在一起。基于这些融合特征训练的XGBoost分类器的准确率为98.0%，敏感性为98.1%。消融研究和统计测试证实，DPM增强和非线性特征融合都是该成功的关键、统计显著的组成部分。本工作验证了高级生成模型与可解释特征之间的协同作用对于创建高度准确的医疗诊断工具的重要性。 

---
# Datasets for Navigating Sensitive Topics in Recommendation Systems 

**Title (ZH)**: 用于导航推荐系统中敏感话题的数据集 

**Authors**: Amelia Kovacs, Jerry Chee, Kimia Kazemian, Sarah Dean  

**Link**: [PDF](https://arxiv.org/pdf/2509.07269)  

**Abstract**: Personalized AI systems, from recommendation systems to chatbots, are a prevalent method for distributing content to users based on their learned preferences. However, there is growing concern about the adverse effects of these systems, including their potential tendency to expose users to sensitive or harmful material, negatively impacting overall well-being. To address this concern quantitatively, it is necessary to create datasets with relevant sensitivity labels for content, enabling researchers to evaluate personalized systems beyond mere engagement metrics. To this end, we introduce two novel datasets that include a taxonomy of sensitivity labels alongside user-content ratings: one that integrates MovieLens rating data with content warnings from the Does the Dog Die? community ratings website, and another that combines fan-fiction interaction data and user-generated warnings from Archive of Our Own. 

**Abstract (ZH)**: 个性化AI系统，从推荐系统到聊天机器人，是根据用户的学习偏好分发内容的一种普遍方法。然而，这些系统可能带来的不利影响引起了广泛关注，包括它们有可能向用户暴露敏感或有害的内容，从而负面影响整体福祉。为了从定量角度应对这一关切，有必要创建包含相关敏感标签的数据集，使研究人员能够超越单纯的参与度指标来评估个性化系统。为此，我们介绍了两个新的数据集，这些数据集包括敏感标签的分类体系和用户内容评分：一个将MovieLens评分数据与Does the Dog Die?社区评级网站的内容警告相结合，另一个则将粉丝小说交互数据与Archive of Our Own用户生成的警告相结合。 

---
# Benchmarking Information Retrieval Models on Complex Retrieval Tasks 

**Title (ZH)**: 在复杂检索任务中 benchmark 信息检索模型 

**Authors**: Julian Killingback, Hamed Zamani  

**Link**: [PDF](https://arxiv.org/pdf/2509.07253)  

**Abstract**: Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable general-purpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent a natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on a comprehensive set of diverse complex tasks. The few resources that do exist feature a limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, we construct a diverse and realistic set of complex retrieval tasks and benchmark a representative set of state-of-the-art retrieval models. Additionally, we explore the impact of LLM-based query expansion and rewriting on retrieval quality. Our results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques. 

**Abstract (ZH)**: 大型语言模型（LLMs）是文本任务中的神奇且多功能工具，已使无数此前无法想象的应用成为可能。相比之下，检索模型尚未见证出现如此强大的通用模型。为了实现这一目标，检索模型必须能够执行复杂的检索任务，其中查询包含多个部分、约束或自然语言要求。这些任务代表了从现有大量常用评估集中使用的简单单方面查询的自然进阶。复杂查询自然产生，因为人们期望搜索引擎能够处理更具体和经常是雄心勃勃的信息请求，这在人们使用基于LLM的信息系统中得到了体现。尽管人们对检索模型在复杂检索任务中的能力扩展存在日益增长的需求，但由于缺乏全面多样复杂任务的评估资源，这仍然是一个挑战。现有的有限资源范围有限且常缺乏现实设置，使得难以了解检索模型在复杂现实检索任务中的真正能力。为解决这一不足并促进下一代检索模型的创新，我们构建了一组多样且现实的复杂检索任务，并对标记的先进检索模型进行了基准测试。此外，我们还探讨了基于LLM的查询扩展和重写对检索质量的影响。我们的结果表明，即使是最好的模型，在所有任务中的最高平均nDCG@10仅为0.346，R@100仅为0.587，也难以产生高质量的检索结果。尽管LLM增强可以有助于较弱的模型，但最强的模型在所有评估指标上使用所有重写技术后性能反而下降。 

---
# Systematic Optimization of Open Source Large Language Models for Mathematical Reasoning 

**Title (ZH)**: 开源大型语言模型在数学推理中的系统优化 

**Authors**: Pranav Pawar, Dhwaj Jain, Varun Gupta, Kaustav Dedhia, Dashrath Kale, Sudhir Dhekane  

**Link**: [PDF](https://arxiv.org/pdf/2509.07238)  

**Abstract**: This paper presents a practical investigation into fine-tuning model parameters for mathematical reasoning tasks through experimenting with various configurations including randomness control, reasoning depth, and sampling strategies, careful tuning demonstrates substantial improvements in efficiency as well as performance. A holistically optimized framework is introduced for five state-of-the-art models on mathematical reasoning tasks, exhibiting significant performance boosts while maintaining solution correctness. Through systematic parameter optimization across Qwen2.5-72B, Llama-3.1-70B, DeepSeek-V3, Mixtral-8x22B, and Yi-Lightning, consistent efficiency gains are demonstrated with 100% optimization success rate. The methodology achieves an average 29.4% reduction in computational cost and 23.9% improvement in inference speed across all tested models. This framework systematically searches parameter spaces including temperature (0.1-0.5), reasoning steps (4-12), planning periods (1-4), and nucleus sampling (0.85-0.98), determining optimal configurations through testing on mathematical reasoning benchmarks. Critical findings show that lower temperature regimes (0.1-0.4) and reduced reasoning steps (4-6) consistently enhance efficiency without compromising accuracy. DeepSeek-V3 achieves the highest accuracy at 98%, while Mixtral-8x22B delivers the most cost-effective performance at 361.5 tokens per accurate response. Key contributions include: (1) the first comprehensive optimization study for five diverse SOTA models in mathematical reasoning, (2) a standardized production-oriented parameter optimization framework, (3) discovery of universal optimization trends applicable across model architectures, and (4) production-ready configurations with extensive performance characterization. 

**Abstract (ZH)**: 基于数学推理任务的模型参数精细化调整的实证研究 

---
# Breaking the Conventional Forward-Backward Tie in Neural Networks: Activation Functions 

**Title (ZH)**: 打破神经网络中前向-后向传输的常规绑定：激活函数 

**Authors**: Luigi Troiano, Francesco Gissi, Vincenzo Benedetto, Genny Tortora  

**Link**: [PDF](https://arxiv.org/pdf/2509.07236)  

**Abstract**: Gradient-based neural network training traditionally enforces symmetry between forward and backward propagation, requiring activation functions to be differentiable (or sub-differentiable) and strictly monotonic in certain regions to prevent flat gradient areas. This symmetry, linking forward activations closely to backward gradients, significantly restricts the selection of activation functions, particularly excluding those with substantial flat or non-differentiable regions. In this paper, we challenge this assumption through mathematical analysis, demonstrating that precise gradient magnitudes derived from activation functions are largely redundant, provided the gradient direction is preserved. Empirical experiments conducted on foundational architectures - such as Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Binary Neural Networks (BNNs) - confirm that relaxing forward-backward symmetry and substituting traditional gradients with simpler or stochastic alternatives does not impair learning and may even enhance training stability and efficiency. We explicitly demonstrate that neural networks with flat or non-differentiable activation functions, such as the Heaviside step function, can be effectively trained, thereby expanding design flexibility and computational efficiency. Further empirical validation with more complex architectures remains a valuable direction for future research. 

**Abstract (ZH)**: 基于梯度的神经网络训练传统上要求前向传播和反向传播保持对称性，需要激活函数在某些区域是可微的（或下可微的）且严格单调，以防止梯度平坦区域的出现。这种对称性将前向激活紧密关联到后向梯度，极大地限制了激活函数的选择，特别是排除了具有显著平坦或非可微区域的激活函数。本文通过数学分析挑战了这一假设，证明了只要保留梯度方向，从激活函数导出的精确梯度幅度往往是多余的。在基础架构，如多层感知机（MLPs）、卷积神经网络（CNNs）和二值神经网络（BNNs）等上进行的经验实验确认，放松前向-后向对称性并用更简单或随机的梯度替代传统梯度不会损害学习，甚至可能提高训练的稳定性和效率。明确展示了具有平坦或非可微激活函数（如单位阶跃函数）的神经网络可以有效训练，从而扩展了设计灵活性和计算效率。进一步使用更复杂架构的经验验证是未来研究的一个有价值的方向。 

---
# A transformer-based generative model for planetary systems 

**Title (ZH)**: 基于变压器的生成模型行星系统 

**Authors**: Yann Alibert, Jeanne Davoult, Sara Marques  

**Link**: [PDF](https://arxiv.org/pdf/2509.07226)  

**Abstract**: Numerical calculations of planetary system formation are very demanding in terms of computing power. These synthetic planetary systems can however provide access to correlations, as predicted in a given numerical framework, between the properties of planets in the same system. Such correlations can, in return, be used in order to guide and prioritize observational campaigns aiming at discovering some types of planets, as Earth-like planets. Our goal is to develop a generative model which is capable of capturing correlations and statistical relationships between planets in the same system. Such a model, trained on the Bern model, offers the possibility to generate large number of synthetic planetary systems with little computational cost, that can be used, for example, to guide observational campaigns. Our generative model is based on the transformer architecture which is well-known to efficiently capture correlations in sequences and is at the basis of all modern Large Language Models. To assess the validity of the generative model, we perform visual and statistical comparisons, as well as a machine learning driven tests. Finally, as a use case example, we consider the TOI-469 system, in which we aim at predicting the possible properties of planets c and d, based on the properties of planet b (the first that has been detected). We show using different comparison methods that the properties of systems generated by our model are very similar to the ones of the systems computed directly by the Bern model. We also show in the case of the TOI-469 system, that using the generative model allows to predict the properties of planets not yet observed, based on the properties of the already observed planet. We provide our model to the community on our website this http URL. 

**Abstract (ZH)**: 数值计算行星系统形成对计算能力要求非常高，但这些合成的行星系统可以提供访问在特定数值框架中预测的相关性，从而可以利用这些相关性来指导和优先排序旨在发现某些类型行星（如类地行星）的观测计划。我们的目标是开发一个能够捕捉相同系统内行星之间相关性和统计关系的生成模型。基于训练于Bern模型的该生成模型，可以以较低的计算成本生成大量的合成行星系统，这些系统可用于指导观测计划等。我们的生成模型基于已知能高效捕捉序列中相关性的变换器架构，这是所有现代大规模语言模型的基础。为了评估生成模型的有效性，我们进行了视觉和统计比较，以及机器学习驱动的测试。最后，作为使用案例示例，我们考虑TOI-469系统，在该系统中，基于已检测到的第一颗行星b的属性，我们旨在预测行星c和d的可能属性。我们使用不同的比较方法表明，由我们模型生成的系统属性与直接由Bern模型计算的系统属性非常相似。此外，在TOI-469系统案例中，使用生成模型可以通过已观测行星的属性预测尚未观测到的行星的属性。我们在网站上将我们的模型分享给社区：this http URL。 

---
# Explaining How Quantization Disparately Skews a Model 

**Title (ZH)**: 解释量化如何不公平地偏斜模型 

**Authors**: Abhimanyu Bellam, Jung-Eun Kim  

**Link**: [PDF](https://arxiv.org/pdf/2509.07222)  

**Abstract**: Post Training Quantization (PTQ) is widely adopted due to its high compression capacity and speed with minimal impact on accuracy. However, we observed that disparate impacts are exacerbated by quantization, especially for minority groups. Our analysis explains that in the course of quantization there is a chain of factors attributed to a disparate impact across groups during forward and backward passes. We explore how the changes in weights and activations induced by quantization cause cascaded impacts in the network, resulting in logits with lower variance, increased loss, and compromised group accuracies. We extend our study to verify the influence of these impacts on group gradient norms and eigenvalues of the Hessian matrix, providing insights into the state of the network from an optimization point of view. To mitigate these effects, we propose integrating mixed precision Quantization Aware Training (QAT) with dataset sampling methods and weighted loss functions, therefore providing fair deployment of quantized neural networks. 

**Abstract (ZH)**: Post Training Quantization (PTQ) 因其高压缩能力和对速度影响 minimal 以及对准确性的 minor 影响而被广泛采用。然而，我们发现量化会加剧不同群体间的差异影响，特别是对少数群体。我们的分析表明，在量化过程中，由于正向和反向传播中的一系列因素，这种差异影响在群体间加剧。我们探讨了量化引起的权重和激活值变化如何在网络中引发连锁影响，导致对数几率 lower 变异、增加的损失以及群体准确性的下降。我们将研究扩展到验证这些影响对群体梯度范数和海森矩阵特征值的影响，从而从优化的角度提供网络状态的见解。为了减轻这些影响，我们提出了结合混合精度感知量化训练（QAT）和数据集采样方法以及加权损失函数的方法，以实现量化神经网络的公平部署。 

---
# XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning 

**Title (ZH)**: XBusNet：基于多模态视觉-语言学习的文本导向乳腺超声分割 

**Authors**: Raja Mallina, Bryar Shareef  

**Link**: [PDF](https://arxiv.org/pdf/2509.07213)  

**Abstract**: Background: Precise breast ultrasound (BUS) segmentation supports reliable measurement, quantitative analysis, and downstream classification, yet remains difficult for small or low-contrast lesions with fuzzy margins and speckle noise. Text prompts can add clinical context, but directly applying weakly localized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce coarse, blob-like responses that smear boundaries unless additional mechanisms recover fine edges. Methods: We propose XBusNet, a novel dual-prompt, dual-branch multimodal model that combines image features with clinically grounded text. A global pathway based on a CLIP Vision Transformer encodes whole-image semantics conditioned on lesion size and location, while a local U-Net pathway emphasizes precise boundaries and is modulated by prompts that describe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS) terms. Prompts are assembled automatically from structured metadata, requiring no manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using five-fold cross-validation. Primary metrics are Dice and Intersection over Union (IoU); we also conduct size-stratified analyses and ablations to assess the roles of the global and local paths and the text-driven modulation. Results: XBusNet achieves state-of-the-art performance on BLU, with mean Dice of 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions show the largest gains, with fewer missed regions and fewer spurious activations. Ablation studies show complementary contributions of global context, local boundary modeling, and prompt-based modulation. Conclusions: A dual-prompt, dual-branch multimodal design that merges global semantics with local precision yields accurate BUS segmentation masks and improves robustness for small, low-contrast lesions. 

**Abstract (ZH)**: 背景：精确的乳腺超声（BUS）分割支持可靠的测量、定量分析和下游分类，但在处理小或低对比度边缘模糊和包含斑点噪声的病灶时仍具挑战性。文本提示可以增加临床背景，但直接应用弱局部化的文本-图像线索（例如，CAM/CLIP衍生信号）通常会生成粗略、团块状的响应，除非采用额外机制来恢复细边缘。方法：我们提出了一种新的双提示、双分支多模态模型XBusNet，该模型结合了图像特征和临床基础的文本。基于CLIP视觉变换器的全局路径依据病变大小和位置编码全局语义，而局部U-Net路径强调精确边界，并通过描述病变形状、边缘和BI-RADS术语的提示进行调节。提示从结构化元数据中自动组装，无需手动点击。使用乳腺超声病变（BLU）数据集进行五折交叉验证评估。主要指标为Dice系数和交并比（IoU）；还进行了分层分析和消融研究，以评估全局路径和局部路径及文本驱动调节的作用。结果：XBusNet在BLU上实现了最先进的性能，平均Dice系数为0.8765，交并比为0.8149，优于六个强基线。小型病灶显示出最大改进，较少漏区且较少虚假激活。消融研究表明，全局上下文、局部边界建模和基于提示的调节提供了互补贡献。结论：将全局语义与局部精确性结合的双提示、双分支多模态设计产生了准确的BUS分割掩模，并提高了对小型、低对比度病灶的鲁棒性。 

---
# A multi-strategy improved gazelle optimization algorithm for solving numerical optimization and engineering applications 

**Title (ZH)**: 多策略改进羚羊优化算法求解数值优化及其工程应用 

**Authors**: Qi Diao, Chengyue Xie, Yuchen Yin, Hoileong Lee, Haolong Yang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07211)  

**Abstract**: Aiming at the shortcomings of the gazelle optimization algorithm, such as the imbalance between exploration and exploitation and the insufficient information exchange within the population, this paper proposes a multi-strategy improved gazelle optimization algorithm (MSIGOA). To address these issues, MSIGOA proposes an iteration-based updating framework that switches between exploitation and exploration according to the optimization process, which effectively enhances the balance between local exploitation and global exploration in the optimization process and improves the convergence speed. Two adaptive parameter tuning strategies improve the applicability of the algorithm and promote a smoother optimization process. The dominant population-based restart strategy enhances the algorithms ability to escape from local optima and avoid its premature convergence. These enhancements significantly improve the exploration and exploitation capabilities of MSIGOA, bringing superior convergence and efficiency in dealing with complex problems. In this paper, the parameter sensitivity, strategy effectiveness, convergence and stability of the proposed method are evaluated on two benchmark test sets including CEC2017 and CEC2022. Test results and statistical tests show that MSIGOA outperforms basic GOA and other advanced algorithms. On the CEC2017 and CEC2022 test sets, the proportion of functions where MSIGOA is not worse than GOA is 92.2% and 83.3%, respectively, and the proportion of functions where MSIGOA is not worse than other algorithms is 88.57% and 87.5%, respectively. Finally, the extensibility of MSIGAO is further verified by several engineering design optimization problems. 

**Abstract (ZH)**: 针对瞪羚优化算法中探索与开发之间的不平衡以及种群内部信息交流不足等问题，本文提出了一种多策略改进瞪羚优化算法（MSIGOA）。MSIGOA提出了一种基于迭代的更新框架，根据优化过程在开发和探索之间切换，有效提高了优化过程中的局部开发与全局探索之间的平衡，提升了收敛速度。两种自适应参数调整策略提高了算法的适用性，并促进了优化过程的平滑进行。主导的基于种群的重启策略增强了算法跳出局部最优解的能力，避免了过早收敛。这些改进显著提升了MSIGOA的探索与开发能力，在处理复杂问题时表现出优越的收敛性和效率。在本文中，通过CEC2017和CEC2022两个基准测试集评估了所提出方法的参数敏感性、策略有效性、收敛性和稳定性。实验结果和统计检验表明，MSIGOA在处理优化问题时优于基本瞪羚优化算法和其他高级算法。在CEC2017和CEC2022测试集上，MSIGOA在函数表现上不低于瞪羚优化算法的比例分别为92.2%和83.3%，在函数表现上不低于其他算法的比例分别为88.57%和87.5%。最后，通过几个工程设计优化问题进一步验证了MSIGOA的可扩展性。 

---
# Evaluation of Machine Learning Reconstruction Techniques for Accelerated Brain MRI Scans 

**Title (ZH)**: 加速脑部MRI扫描的机器学习重建技术评估 

**Authors**: Jonathan I. Mandel, Shivaprakash Hiremath, Hedyeh Keshtgar, Timothy Scholl, Sadegh Raeisi  

**Link**: [PDF](https://arxiv.org/pdf/2509.07193)  

**Abstract**: This retrospective-prospective study evaluated whether a deep learning-based MRI reconstruction algorithm can preserve diagnostic quality in brain MRI scans accelerated up to fourfold, using both public and prospective clinical data. The study included 18 healthy volunteers (scans acquired at 3T, January 2024-March 2025), as well as selected fastMRI public datasets with diverse pathologies. Phase-encoding-undersampled 2D/3D T1, T2, and FLAIR sequences were reconstructed with DeepFoqus-Accelerate and compared with standard-of-care (SOC). Three board-certified neuroradiologists and two MRI technologists independently reviewed 36 paired SOC/AI reconstructions from both datasets using a 5-point Likert scale, while quantitative similarity was assessed for 408 scans and 1224 datasets using Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Haar wavelet-based Perceptual Similarity Index (HaarPSI). No AI-reconstructed scan scored below 3 (minimally acceptable), and 95% scored $\geq 4$. Mean SSIM was 0.95 $\pm$ 0.03 (90% cases >0.90), PSNR >41.0 dB, and HaarPSI >0.94. Inter-rater agreement was slight to moderate. Rare artifacts did not affect diagnostic interpretation. These findings demonstrate that DeepFoqus-Accelerate enables robust fourfold brain MRI acceleration with 75% reduced scan time, while preserving diagnostic image quality and supporting improved workflow efficiency. 

**Abstract (ZH)**: 基于深度学习的MRI重建算法能否在加速四倍的脑部MRI扫描中保持诊断质量：一项回顾前瞻研究 

---
# DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge 

**Title (ZH)**: DischargeSim: 一个用于教育性出院医生-患者沟通模拟的基准测试 

**Authors**: Zonghai Yao, Michael Sun, Won Seok Jang, Sunjae Kwon, Soie Kwon, Hong Yu  

**Link**: [PDF](https://arxiv.org/pdf/2509.07188)  

**Abstract**: Discharge communication is a critical yet underexplored component of patient care, where the goal shifts from diagnosis to education. While recent large language model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they fail to evaluate models' ability to support patients after the visit. We introduce DischargeSim, a novel benchmark that evaluates LLMs on their ability to act as personalized discharge educators. DischargeSim simulates post-visit, multi-turn conversations between LLM-driven DoctorAgents and PatientAgents with diverse psychosocial profiles (e.g., health literacy, education, emotion). Interactions are structured across six clinically grounded discharge topics and assessed along three axes: (1) dialogue quality via automatic and LLM-as-judge evaluation, (2) personalized document generation including free-text summaries and structured AHRQ checklists, and (3) patient comprehension through a downstream multiple-choice exam. Experiments across 18 LLMs reveal significant gaps in discharge education capability, with performance varying widely across patient profiles. Notably, model size does not always yield better education outcomes, highlighting trade-offs in strategy use and content prioritization. DischargeSim offers a first step toward benchmarking LLMs in post-visit clinical education and promoting equitable, personalized patient support. 

**Abstract (ZH)**: 出院沟通是一个关键但尚未充分探索的患者护理组成部分，其目标从诊断转向教育。虽然最近的大语言模型基准测试强调就诊中的诊断推理，但它们未能评估模型在就诊后支持患者的能力。我们引入了DischargeSim，这是一个新的基准测试，用于评估大语言模型在扮演个性化出院教育者角色方面的能力。DischargeSim 模拟了在就诊后的多轮对话，其中由大语言模型驱动的DoctorAgents与具有多样心理社会特征的PatientAgents（如健康素养、教育背景、情绪）进行交互。交互围绕六个临床主题结构化，并从三个维度进行评估：（1）通过自动评估和大语言模型评判的对话质量，（2）个性化文档生成，包括自由文本摘要和结构化的AHRQ检查单，以及（3）通过下游选择题考试评估患者的理解能力。在18个大语言模型上的实验揭示了出院教育能力存在显著差距，不同患者群体的表现差异很大。值得注意的是，模型大小并不总是产生更好的教育结果，这突显了策略使用和内容优先级之间的权衡。DischargeSim 代表了在就诊后临床教育中评估大语言模型的一个初步步骤，并促进了公平和个性化的患者支持。 

---
# Measuring Uncertainty in Transformer Circuits with Effective Information Consistency 

**Title (ZH)**: 基于有效信息一致性测量变压器电路中的不确定性 

**Authors**: Anatoly A. Krasnovsky  

**Link**: [PDF](https://arxiv.org/pdf/2509.07149)  

**Abstract**: Mechanistic interpretability has identified functional subgraphs within large language models (LLMs), known as Transformer Circuits (TCs), that appear to implement specific algorithms. Yet we lack a formal, single-pass way to quantify when an active circuit is behaving coherently and thus likely trustworthy. Building on prior systems-theoretic proposals, we specialize a sheaf/cohomology and causal emergence perspective to TCs and introduce the Effective-Information Consistency Score (EICS). EICS combines (i) a normalized sheaf inconsistency computed from local Jacobians and activations, with (ii) a Gaussian EI proxy for circuit-level causal emergence derived from the same forward state. The construction is white-box, single-pass, and makes units explicit so that the score is dimensionless. We further provide practical guidance on score interpretation, computational overhead (with fast and exact modes), and a toy sanity-check analysis. Empirical validation on LLM tasks is deferred. 

**Abstract (ZH)**: 机制可解释性已在大型语言模型（LLMs）中识别出功能子图，这些子图被称为Transformer电路（TCs），并似乎实现了特定的算法。然而，我们缺乏一种形式化的单一通过方法来定量衡量活跃电路是否表现出一致行为，从而可能值得信赖。基于先前的系统理论提案，我们将层/上同调和因果涌现视角专门化于TCs，并引入有效信息一致性分数（EICS）。EICS结合了（i）从局部雅可比矩阵和激活计算的归一化层不一致性，以及（ii）从前向状态派生的高斯有效信息代理，表示电路层面的因果涌现。该构建是白盒的，单一通过的，并使单元显式化，从而使分数无量纲。我们进一步提供了分数解释的实用指导、计算开销（包括快速和精确模式）以及一个玩具级别的合理性检查分析。对LLM任务的经验验证留待以后。 

---
# Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models 

**Title (ZH)**: 面向目标导向的主题模型评估能力提升 

**Authors**: Zhiyin Tan, Jennifer D'Souza  

**Link**: [PDF](https://arxiv.org/pdf/2509.07142)  

**Abstract**: This study presents a framework for automated evaluation of dynamically evolving topic models using Large Language Models (LLMs). Topic modeling is essential for organizing and retrieving scholarly content in digital library systems, helping users navigate complex and evolving knowledge domains. However, widely used automated metrics, such as coherence and diversity, often capture only narrow statistical patterns and fail to explain semantic failures in practice. We introduce a purpose-oriented evaluation framework that employs nine LLM-based metrics spanning four key dimensions of topic quality: lexical validity, intra-topic semantic soundness, inter-topic structural soundness, and document-topic alignment soundness. The framework is validated through adversarial and sampling-based protocols, and is applied across datasets spanning news articles, scholarly publications, and social media posts, as well as multiple topic modeling methods and open-source LLMs. Our analysis shows that LLM-based metrics provide interpretable, robust, and task-relevant assessments, uncovering critical weaknesses in topic models such as redundancy and semantic drift, which are often missed by traditional metrics. These results support the development of scalable, fine-grained evaluation tools for maintaining topic relevance in dynamic datasets. All code and data supporting this work are accessible at this https URL. 

**Abstract (ZH)**: 基于大型语言模型的动态演变主题模型自动评估框架 

---
# Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative Study 

**Title (ZH)**: 音频深度假音检测的对抗攻击：基准与比较研究 

**Authors**: Kutub Uddin, Muhammad Umar Farooq, Awais Khan, Khalid Mahmood Malik  

**Link**: [PDF](https://arxiv.org/pdf/2509.07132)  

**Abstract**: The widespread use of generative AI has shown remarkable success in producing highly realistic deepfakes, posing a serious threat to various voice biometric applications, including speaker verification, voice biometrics, audio conferencing, and criminal investigations. To counteract this, several state-of-the-art (SoTA) audio deepfake detection (ADD) methods have been proposed to identify generative AI signatures to distinguish between real and deepfake audio. However, the effectiveness of these methods is severely undermined by anti-forensic (AF) attacks that conceal generative signatures. These AF attacks span a wide range of techniques, including statistical modifications (e.g., pitch shifting, filtering, noise addition, and quantization) and optimization-based attacks (e.g., FGSM, PGD, C \& W, and DeepFool). In this paper, we investigate the SoTA ADD methods and provide a comparative analysis to highlight their effectiveness in exposing deepfake signatures, as well as their vulnerabilities under adversarial conditions. We conducted an extensive evaluation of ADD methods on five deepfake benchmark datasets using two categories: raw and spectrogram-based approaches. This comparative analysis enables a deeper understanding of the strengths and limitations of SoTA ADD methods against diverse AF attacks. It does not only highlight vulnerabilities of ADD methods, but also informs the design of more robust and generalized detectors for real-world voice biometrics. It will further guide future research in developing adaptive defense strategies that can effectively counter evolving AF techniques. 

**Abstract (ZH)**: 生成式AI在 produciing 高度逼真的深伪方面的广泛应用对包括语音生物特征识别、语音生物特征、音频会议和刑事调查在内的多种语音生物特征应用构成了严重威胁。为了应对这一挑战，已经提出了一些最先进的（SoTA）音频深伪检测（ADD）方法，用于识别生成式AI的签名以区分真实语音和深伪语音。然而，这些方法的有效性严重受到反取证（AF）攻击的影响，这些攻击通过隐蔽生成式签名的方式削弱了检测方法的效果。这些AF攻击包括统计修改（如音调变换、滤波、噪声添加和量化）和基于优化的攻击（如FGSM、PGD、C&W和DeepFool）。在本文中，我们研究了最先进的ADD方法，并提供了比较分析，以突出这些方法在揭露深伪签名方面的有效性及其在对抗条件下的脆弱性。我们使用原始和频谱图基方法对五种深伪基准数据集上的ADD方法进行了全面评估。这种比较分析有助于更深入地了解最先进的ADD方法在面对多种AF攻击时的优势和局限性，不仅突显了ADD方法的脆弱性，还为设计更 robust 和通用的检测器提供指导，以有效应对现实世界中的语音生物特征应用。这还将进一步指导未来研究，开发能够有效对抗不断演变的AF技术的自适应防御策略。 

---
# SoK: Security and Privacy of AI Agents for Blockchain 

**Title (ZH)**: SoK: AI代理在区块链中的安全与隐私 

**Authors**: Nicolò Romandini, Carlo Mazzocca, Kai Otsuki, Rebecca Montanari  

**Link**: [PDF](https://arxiv.org/pdf/2509.07131)  

**Abstract**: Blockchain and smart contracts have garnered significant interest in recent years as the foundation of a decentralized, trustless digital ecosystem, thereby eliminating the need for traditional centralized authorities. Despite their central role in powering Web3, their complexity still presents significant barriers for non-expert users. To bridge this gap, Artificial Intelligence (AI)-based agents have emerged as valuable tools for interacting with blockchain environments, supporting a range of tasks, from analyzing on-chain data and optimizing transaction strategies to detecting vulnerabilities within smart contracts. While interest in applying AI to blockchain is growing, the literature still lacks a comprehensive survey that focuses specifically on the intersection with AI agents. Most of the related work only provides general considerations, without focusing on any specific domain. This paper addresses this gap by presenting the first Systematization of Knowledge dedicated to AI-driven systems for blockchain, with a special focus on their security and privacy dimensions, shedding light on their applications, limitations, and future research directions. 

**Abstract (ZH)**: 区块链和智能合约近年来引起了广泛关注，成为去中心化、无信任数字生态系统的基础，从而消除了对传统中心化权威机构的需要。尽管它们在支撑Web3中发挥着中心作用，但其复杂性仍然为非专家用户设立了显著障碍。为弥合这一差距，基于人工智能（AI）的代理已经 emerged 作为与区块链环境交互的有价值的工具，支持从分析链上数据、优化交易策略到检测智能合约漏洞等一系列任务。虽然将人工智能应用于区块链的兴趣在增长，但相关文献仍然缺乏专注于与AI代理交汇的具体综述。大部分相关工作仅提供一般性考虑，未专注于任何特定领域。本文通过首次提出针对区块链的AI驱动系统的知识体系结构化，特别关注其安全和隐私维度，阐明了其应用场景、局限性和未来研究方向。 

---
# SVGauge: Towards Human-Aligned Evaluation for SVG Generation 

**Title (ZH)**: SVGauge: 朝向与人类视角一致的SVG生成评估 

**Authors**: Leonardo Zini, Elia Frigieri, Sebastiano Aloscari, Marcello Generali, Lorenzo Dodi, Robert Dosen, Lorenzo Baraldi  

**Link**: [PDF](https://arxiv.org/pdf/2509.07127)  

**Abstract**: Generated Scalable Vector Graphics (SVG) images demand evaluation criteria tuned to their symbolic and vectorial nature: criteria that existing metrics such as FID, LPIPS, or CLIPScore fail to satisfy. In this paper, we introduce SVGauge, the first human-aligned, reference based metric for text-to-SVG generation. SVGauge jointly measures (i) visual fidelity, obtained by extracting SigLIP image embeddings and refining them with PCA and whitening for domain alignment, and (ii) semantic consistency, captured by comparing BLIP-2-generated captions of the SVGs against the original prompts in the combined space of SBERT and TF-IDF. Evaluation on the proposed SHE benchmark shows that SVGauge attains the highest correlation with human judgments and reproduces system-level rankings of eight zero-shot LLM-based generators more faithfully than existing metrics. Our results highlight the necessity of vector-specific evaluation and provide a practical tool for benchmarking future text-to-SVG generation models. 

**Abstract (ZH)**: Generated Scalable Vector Graphics (SVG) 图像需要针对其符号性和向量性质的评价标准：现有指标（如 FID、LPIPS 或 CLIPScore）未能满足这些需求。在此论文中，我们提出了 SVGauge，这是首个针对文本到 SVG 生成的人类对齐参考基线指标。SVGauge 联合度量 (i) 视觉保真度，通过提取 SigLIP 图像嵌入并使用 PCA 和白化进行领域对齐进行细化；以及 (ii) 语义一致性，通过将 BLIP-2 生成的 SVG 标题与结合 SBERT 和 TF-IDF 的原始提示进行比较来捕捉。在提出的 SHE 基准上的评估表明，SVGauge 在与人类判断的相关性和忠实再现八种零样本 LLM 基础生成器的系统级排名方面优于现有指标。我们的结果强调了向量特定评价的必要性，并为评估未来文本到 SVG 生成模型提供了一个实用工具。 

---
# Riemannian Batch Normalization: A Gyro Approach 

**Title (ZH)**: 黎曼流形批量归一化：陀螺仪方法 

**Authors**: Ziheng Chen, Xiao-Jun Wu, Nicu Sebe  

**Link**: [PDF](https://arxiv.org/pdf/2509.07115)  

**Abstract**: Normalization layers are crucial for deep learning, but their Euclidean formulations are inadequate for data on manifolds. On the other hand, many Riemannian manifolds in machine learning admit gyro-structures, enabling principled extensions of Euclidean neural networks to non-Euclidean domains. Inspired by this, we introduce GyroBN, a principled Riemannian batch normalization framework for gyrogroups. We establish two necessary conditions, namely \emph{pseudo-reduction} and \emph{gyroisometric gyrations}, that guarantee GyroBN with theoretical control over sample statistics, and show that these conditions hold for all known gyrogroups in machine learning. Our framework also incorporates several existing Riemannian normalization methods as special cases. We further instantiate GyroBN on seven representative geometries, including the Grassmannian, five constant curvature spaces, and the correlation manifold, and derive novel gyro and Riemannian structures to enable these instantiations. Experiments across these geometries demonstrate the effectiveness of GyroBN. The code is available at this https URL. 

**Abstract (ZH)**: 广义布朗规范化：广义群上的原理化黎曼批量归一化框架 

---
# Lookup multivariate Kolmogorov-Arnold Networks 

**Title (ZH)**: 查找多元柯尔莫哥洛夫-阿诺尔德网络 

**Authors**: Sergey Pozdnyakov, Philippe Schwaller  

**Link**: [PDF](https://arxiv.org/pdf/2509.07103)  

**Abstract**: High-dimensional linear mappings, or linear layers, dominate both the parameter count and the computational cost of most modern deep-learning models. We introduce a general drop-in replacement, lookup multivariate Kolmogorov-Arnold Networks (lmKANs), which deliver a substantially better trade-off between capacity and inference cost. Our construction expresses a general high-dimensional mapping through trainable low-dimensional multivariate functions. These functions can carry dozens or hundreds of trainable parameters each, and yet it takes only a few multiplications to compute them because they are implemented as spline lookup tables. Empirically, lmKANs reduce inference FLOPs by up to 6.0x while matching the flexibility of MLPs in general high-dimensional function approximation. In another feedforward fully connected benchmark, on the tabular-like dataset of randomly displaced methane configurations, lmKANs enable more than 10x higher H100 throughput at equal accuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs cut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10 and ImageNet-1k datasets, respectively. Our code, including dedicated CUDA kernels, is available online at this https URL. 

**Abstract (ZH)**: 高维线性映射或线性层主导了大多数现代深度学习模型的参数量和计算成本。我们引入了一种通用的即插即用替换方案——查找多元柯尔莫哥洛夫-阿诺尔德网络（lmKANs），它在容量和推理成本之间提供了显著更好的权衡。我们的构造通过可训练的低维多元函数表达了一般高维映射。这些函数可以承载数十到数百个可训练参数，但由于它们是通过样条查找表实现的，因此只需几次乘法即可计算。实验结果表明，lmKANs在匹配MLP的一般高维函数逼近灵活性的同时，将推理FLOPs减少多达6.0倍。在另一个前向全连接基准测试中，对于随机位移的一系列甲烷配置表格化数据集，lmKANs在相同精度下使H100的吞吐量提高了10倍以上。在卷积神经网络框架内，基于lmKAN的CNN将匹配精度下的推理FLOPs减少了1.6至2.1倍，分别在CIFAR-10和ImageNet-1k数据集上减少了1.7倍。我们的代码，包括专用的CUDA内核，可在以下链接在线获取。 

---
# Automated Evaluation of Gender Bias Across 13 Large Multimodal Models 

**Title (ZH)**: 跨13个大型多模态模型的性别偏见自动评估 

**Authors**: Juan Manuel Contreras  

**Link**: [PDF](https://arxiv.org/pdf/2509.07050)  

**Abstract**: Large multimodal models (LMMs) have revolutionized text-to-image generation, but they risk perpetuating the harmful social biases in their training data. Prior work has identified gender bias in these models, but methodological limitations prevented large-scale, comparable, cross-model analysis. To address this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for assessing social bias in AI-generated images. We test 13 commercially available LMMs using 75 procedurally-generated, gender-neutral prompts to generate people in stereotypically-male, stereotypically-female, and non-stereotypical professions. We then use a validated LLM-as-a-judge system to score the 965 resulting images for gender representation. Our results reveal (p < .001 for all): 1) LMMs systematically not only reproduce but actually amplify occupational gender stereotypes relative to real-world labor data, generating men in 93.0% of images for male-stereotyped professions but only 22.5% for female-stereotyped professions; 2) Models exhibit a strong default-male bias, generating men in 68.3% of the time for non-stereotyped professions; and 3) The extent of bias varies dramatically across models, with overall male representation ranging from 46.7% to 73.3%. Notably, the top-performing model de-amplified gender stereotypes and approached gender parity, achieving the highest fairness scores. This variation suggests high bias is not an inevitable outcome but a consequence of design choices. Our work provides the most comprehensive cross-model benchmark of gender bias to date and underscores the necessity of standardized, automated evaluation tools for promoting accountability and fairness in AI development. 

**Abstract (ZH)**: 大型多模态模型（LMMs）已经革新了文本-to-图像生成，但它们可能导致在其训练数据中延续有害的社会偏见。早期研究已发现这些模型中的性别偏见，但由于方法学限制，未能进行大规模、可比、跨模型的分析。为填补这一空白，我们提出了Aymara图像公平评估基准，用于评估AI生成图像中的社会偏见。我们使用75个程序生成的性别中立提示，测试了13个商用LMMs，生成了具有刻板男性、刻板女性和非刻板职业的人物图像。然后，我们使用验证过的LLM作为评委系统，对965张结果图像进行性别代表性评分。研究结果表明（所有p < .001）：1) LMMs不仅系统地复制了职业性别刻板印象，而且还进一步放大了职业性别刻板印象，相较于现实世界劳动力数据，男性形象在93.0%的刻板男性职业图像中出现，但在22.5%的刻板女性职业图像中出现；2) 模型表现出强烈的默认男性偏见，在非刻板职业中男性形象出现68.3%的频率；3) 偏见的程度在不同模型之间差异巨大，总体男性代表性从46.7%到73.3%不等。值得注意的是，表现最好的模型降低了性别刻板印象，并接近性别平等，获得了最高的公平评分。这种差异表明，高偏见并非不可避免的结果，而是设计选择的后果。我们的研究提供了迄今为止最全面的跨模型性别偏见基准，并强调了标准化、自动化评估工具对于促进AI发展中问责制和公平性的重要性。 

---
# Controllable Singing Voice Synthesis using Phoneme-Level Energy Sequence 

**Title (ZH)**: 基于音位级能量序列的可控歌声合成 

**Authors**: Yerin Ryu, Inseop Shin, Chanwoo Kim  

**Link**: [PDF](https://arxiv.org/pdf/2509.07038)  

**Abstract**: Controllable Singing Voice Synthesis (SVS) aims to generate expressive singing voices reflecting user intent. While recent SVS systems achieve high audio quality, most rely on probabilistic modeling, limiting precise control over attributes such as dynamics. We address this by focusing on dynamic control--temporal loudness variation essential for musical expressiveness--and explicitly condition the SVS model on energy sequences extracted from ground-truth spectrograms, reducing annotation costs and improving controllability. We also propose a phoneme-level energy sequence for user-friendly control. To the best of our knowledge, this is the first attempt enabling user-driven dynamics control in SVS. Experiments show our method achieves over 50% reduction in mean absolute error of energy sequences for phoneme-level inputs compared to baseline and energy-predictor models, without compromising synthesis quality. 

**Abstract (ZH)**: 可控歌唱语音合成（SVS）旨在生成反映用户意图的表达性歌唱声音。虽然近期的SVS系统在音频质量方面取得高成就，但大多数仍然依赖概率建模，限制了对诸如动态等属性的精准控制。我们通过专注于音乐表达性所必需的动态控制——时间响度变化——并明确将SVS模型条件化于从真实光谱图提取的能量序列，从而降低标注成本并提高可控性。我们还提出了一种音素级别能量序列以实现用户友好的控制。据我们所知，这是第一次在SVS中实现用户驱动的动态控制的尝试。实验结果显示，与基线模型和能量预测模型相比，我们的方法在音素级别输入的能量序列的平均绝对误差上降低了超过50%，且不牺牲合成质量。 

---
# Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators 

**Title (ZH)**: 结构因果建模与不确定性意识预测在经济指标中的方法论洞察 

**Authors**: Federico Cerutti  

**Link**: [PDF](https://arxiv.org/pdf/2509.07036)  

**Abstract**: This paper presents a methodological approach to financial time series analysis by combining causal discovery and uncertainty-aware forecasting. As a case study, we focus on four key U.S. macroeconomic indicators -- GDP, economic growth, inflation, and unemployment -- and we apply the LPCMCI framework with Gaussian Process Distance Correlation (GPDC) to uncover dynamic causal relationships in quarterly data from 1970 to 2021. Our results reveal a robust unidirectional causal link from economic growth to GDP and highlight the limited connectivity of inflation, suggesting the influence of latent factors. Unemployment exhibits strong autoregressive dependence, motivating its use as a case study for probabilistic forecasting. Leveraging the Chronos framework, a large language model trained for time series, we perform zero-shot predictions on unemployment. This approach delivers accurate forecasts one and two quarters ahead, without requiring task-specific training. Crucially, the model's uncertainty-aware predictions yield 90\% confidence intervals, enabling effective anomaly detection through statistically principled deviation analysis. This study demonstrates the value of combining causal structure learning with probabilistic language models to inform economic policy and enhance forecasting robustness. 

**Abstract (ZH)**: 本文提出一种结合因果发现和不确定性感知预测的方法论金融时间序列分析方法。作为案例研究，我们聚焦于四类关键的美国宏观经济指标——GDP、经济增长、通货膨胀和失业率——并应用LPCMCI框架结合高斯过程距离相关（GPDC）来揭示从1970年至2021年季度数据中动态因果关系。研究结果表明经济增长向GDP存在稳健的单向因果联系，而通货膨胀的连接性有限，暗示了潜在因素的影响。失业率表现出强烈的自回归依赖性，使其成为概率预测的典型案例。利用Chronos框架，一种为时间序列训练的大语言模型，我们进行失业率的零样本预测。该方法在提前一个季度和两个季度时提供准确的预测，无需针对特定任务进行训练。模型的不确定性感知预测生成90%的置信区间，能够通过统计原理分析来有效检测异常。本研究展示了将因果结构学习与概率语言模型相结合，以指导经济政策制定并增强预测稳健性的价值。 

---
# A Maslow-Inspired Hierarchy of Engagement with AI Model 

**Title (ZH)**: 基于马斯洛需求层次理论的AI交互层次模型 

**Authors**: Madara Ogot  

**Link**: [PDF](https://arxiv.org/pdf/2509.07032)  

**Abstract**: The rapid proliferation of artificial intelligence (AI) across industry, government, and education highlights the urgent need for robust frameworks to conceptualise and guide engagement. This paper introduces the Hierarchy of Engagement with AI model, a novel maturity framework inspired by Maslow's hierarchy of needs. The model conceptualises AI adoption as a progression through eight levels, beginning with initial exposure and basic understanding and culminating in ecosystem collaboration and societal impact. Each level integrates technical, organisational, and ethical dimensions, emphasising that AI maturity is not only a matter of infrastructure and capability but also of trust, governance, and responsibility. Initial validation of the model using four diverse case studies (General Motors, the Government of Estonia, the University of Texas System, and the African Union AI Strategy) demonstrate the model's contextual flexibility across various sectors. The model provides scholars with a framework for analysing AI maturity and offers practitioners and policymakers a diagnostic and strategic planning tool to guide responsible and sustainable AI engagement. The proposed model demonstrates that AI maturity progression is multi-dimensional, requiring technological capability, ethical integrity, organisational resilience, and ecosystem collaboration. 

**Abstract (ZH)**: 人工智能在工业、政府和教育领域的迅速普及凸显了建立稳健框架以概念化和指导参与的迫切需求。本文引入了人工智能参与层次模型，该模型是一种受马斯洛需求层次理论启发的创新成熟度框架。该模型将人工智能采用概念化为八个阶段的 progression，从初步接触和基础理解开始，最终达到生态系统合作和社会影响。每个阶段整合了技术、组织和伦理维度，强调人工智能成熟度不仅仅是基础设施和能力的问题，同时也是信任、治理和责任的问题。通过四个不同的案例研究（通用汽车、爱沙尼亚政府、德克萨斯大学系统和非洲联盟人工智能战略）的初步验证显示，该模型具有跨各种领域的灵活性。该模型为学者们提供了一个分析人工智能成熟度的框架，并为从业人员和政策制定者提供了一个诊断和战略规划工具，以指导负责任和可持续的人工智能参与。所提出的人工智能成熟度模型证明了其多维性，需要技术能力、伦理诚信、组织韧性和生态系统合作。 

---
# A Minimalist Bayesian Framework for Stochastic Optimization 

**Title (ZH)**: 最小主义贝叶斯框架下的随机优化 

**Authors**: Kaizheng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07030)  

**Abstract**: The Bayesian paradigm offers principled tools for sequential decision-making under uncertainty, but its reliance on a probabilistic model for all parameters can hinder the incorporation of complex structural constraints. We introduce a minimalist Bayesian framework that places a prior only on the component of interest, such as the location of the optimum. Nuisance parameters are eliminated via profile likelihood, which naturally handles constraints. As a direct instantiation, we develop a MINimalist Thompson Sampling (MINTS) algorithm. Our framework accommodates structured problems, including continuum-armed Lipschitz bandits and dynamic pricing. It also provides a probabilistic lens on classical convex optimization algorithms such as the center of gravity and ellipsoid methods. We further analyze MINTS for multi-armed bandits and establish near-optimal regret guarantees. 

**Abstract (ZH)**: 基于贝叶斯范式的 minimalist 框架及其在sequential决策中的应用：消除冗余参数以处理约束条件 

---
# The Impact of Artificial Intelligence on Traditional Art Forms: A Disruption or Enhancement 

**Title (ZH)**: 人工智能对传统艺术形式的影响：颠覆还是增强 

**Authors**: Viswa Chaitanya Marella, Sai Teja Erukude, Suhasnadh Reddy Veluru  

**Link**: [PDF](https://arxiv.org/pdf/2509.07029)  

**Abstract**: The introduction of Artificial Intelligence (AI) into the domains of traditional art (visual arts, performing arts, and crafts) has sparked a complicated discussion about whether this might be an agent of disruption or an enhancement of our traditional art forms. This paper looks at the duality of AI, exploring the ways that recent technologies like Generative Adversarial Networks and Diffusion Models, and text-to-image generators are changing the fields of painting, sculpture, calligraphy, dance, music, and the arts of craft. Using examples and data, we illustrate the ways that AI can democratize creative expression, improve productivity, and preserve cultural heritage, while also examining the negative aspects, including: the threats to authenticity within art, ethical concerns around data, and issues including socio-economic factors such as job losses. While we argue for the context-dependence of the impact of AI (the potential for creative homogenization and the devaluation of human agency in artmaking), we also illustrate the potential for hybrid practices featuring AI in cuisine, etc. We advocate for the development of ethical guidelines, collaborative approaches, and inclusive technology development. In sum, we are articulating a vision of AI in which it amplifies our innate creativity while resisting the displacement of the cultural, nuanced, and emotional aspects of traditional art. The future will be determined by human choices about how to govern AI so that it becomes a mechanism for artistic evolution and not a substitute for the artist's soul. 

**Abstract (ZH)**: 人工智能在传统艺术（视觉艺术、表演艺术和手工艺）领域中的引入引发了关于其可能是一种破坏性因素还是传统艺术形式的增强因素的复杂讨论。本文探讨了人工智能的双重性，分析了生成对抗网络、扩散模型和文本转图像生成器等先进技术如何改变绘画、雕塑、书法、舞蹈、音乐和手工艺等领域。通过例证和数据，我们展示了人工智能如何促进创造性表达、提高生产效率并保存文化遗产，同时也审视了其负面影响，包括艺术中的真实性威胁、数据伦理问题以及包括社会经济因素在内的职业流失问题。虽然我们强调人工智能影响的上下文依赖性（创造性同质化的潜在风险和艺术创作中人类代理价值的贬值），但我们也展示了人工智能与传统工艺等领域的混合实践的潜力。我们倡导制定伦理准则、合作方法和包容性技术开发。总之，我们阐述了一个愿景，即人工智能能够放大我们内在的创造力，同时避免取代传统艺术的文化化、细腻化和情感化方面。人类将在如何治理人工智能以使其成为艺术进化机制而非艺术家灵魂的替代品方面作出选择。 

---
# Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models 

**Title (ZH)**: 基于矩和功率谱的高斯性正则化方法用于文本到图像模型 

**Authors**: Jisung Hwang, Jaihoon Kim, Minhyuk Sung  

**Link**: [PDF](https://arxiv.org/pdf/2509.07027)  

**Abstract**: We propose a novel regularization loss that enforces standard Gaussianity, encouraging samples to align with a standard Gaussian distribution. This facilitates a range of downstream tasks involving optimization in the latent space of text-to-image models. We treat elements of a high-dimensional sample as one-dimensional standard Gaussian variables and define a composite loss that combines moment-based regularization in the spatial domain with power spectrum-based regularization in the spectral domain. Since the expected values of moments and power spectrum distributions are analytically known, the loss promotes conformity to these properties. To ensure permutation invariance, the losses are applied to randomly permuted inputs. Notably, existing Gaussianity-based regularizations fall within our unified framework: some correspond to moment losses of specific orders, while the previous covariance-matching loss is equivalent to our spectral loss but incurs higher time complexity due to its spatial-domain computation. We showcase the application of our regularization in generative modeling for test-time reward alignment with a text-to-image model, specifically to enhance aesthetics and text alignment. Our regularization outperforms previous Gaussianity regularization, effectively prevents reward hacking and accelerates convergence. 

**Abstract (ZH)**: 我们提出了一种新颖的正则化损失，强制标准高斯性，鼓励样本与标准高斯分布对齐。这促进了涉及文本到图像模型的潜在空间优化的一系列下游任务。我们将高维样本的元素视为一维标准高斯变量，并定义一个结合空间域moment为基础的正则化与频域基础的功率谱正则化的复合损失。由于矩和功率谱分布的期望值是解析已知的，损失促进了这些属性的符合性。为了确保置换不变性，损失应用于随机置换的输入。值得注意的是，现有的高斯性正则化都包含在我们统一的框架中：一些对应于特定阶数的moment损失，而之前的协方差匹配损失等价于我们的频域损失，但由于其在空间域的计算，导致更高的时间复杂度。我们在测试时的奖励对齐生成建模中展示了该正则化应用，特别是用于增强美学和文本对齐。该正则化优于先前的高斯性正则化，有效防止了奖励作弊并加速了收敛。 

---
# Contradictions 

**Title (ZH)**: 矛盾 

**Authors**: Yang Xu, Shuwei Chen, Xiaomei Zhong, Jun Liu, Xingxing He  

**Link**: [PDF](https://arxiv.org/pdf/2509.07026)  

**Abstract**: Trustworthy AI requires reasoning systems that are not only powerful but also transparent and reliable. Automated Theorem Proving (ATP) is central to formal reasoning, yet classical binary resolution remains limited, as each step involves only two clauses and eliminates at most two literals. To overcome this bottleneck, the concept of standard contradiction and the theory of contradiction-separation-based deduction were introduced in 2018. This paper advances that framework by focusing on the systematic construction of standard contradictions. Specially, this study investigates construction methods for two principal forms of standard contradiction: the maximum triangular standard contradiction and the triangular-type standard contradiction. Building on these structures, we propose a procedure for determining the satisfiability and unsatisfiability of clause sets via maximum standard contradiction. Furthermore, we derive formulas for computing the number of standard sub-contradictions embedded within both the maximum triangular standard contradiction and the triangular-type standard contradiction. The results presented herein furnish the methodological basis for advancing contradiction-separation-based dynamic multi-clause automated deduction, thereby extending the expressive and deductive capabilities of automated reasoning systems beyond the classical binary paradigm. 

**Abstract (ZH)**: 可信的人工智能需要既强大又透明可靠的推理系统。形式推理的核心是自动定理证明（ATP），然而经典的二元归结仍然受限，因为每步仅涉及两个子句并最多消除两个原子命题。为克服这一瓶颈，标准矛盾的概念和基于矛盾分离的推理理论在2018年被提出。本文在此基础上聚焦于标准矛盾的系统构造。具体而言，本研究探讨了两种主要标准矛盾形式的构造方法：最大三角形标准矛盾和三角型标准矛盾。在这些结构的基础上，我们提出了一种通过最大标准矛盾确定子句集的可满足性和不可满足性的程序。此外，我们推导了计算嵌入于最大三角形标准矛盾和三角型标准矛盾中的标准次矛盾数量的公式。本文的结果为扩展基于矛盾分离的动态多子句自动推理的表达能力和演绎能力提供了方法论基础，从而超越了经典的二元范式。 

---
# 1 bit is all we need: binary normalized neural networks 

**Title (ZH)**: 1比特足矣：归一化二值神经网络 

**Authors**: Eduardo Lobo Lustoda Cabral, Paulo Pirozelli, Larissa Driemeier  

**Link**: [PDF](https://arxiv.org/pdf/2509.07025)  

**Abstract**: The increasing size of large neural network models, specifically language models and foundational image models, poses deployment challenges, prompting efforts to reduce memory requirements and enhance computational efficiency. These efforts are critical to ensure practical deployment and effective utilization of these models across various applications. In this work, a novel type of neural network layers and models is developed that uses only single-bit parameters. In this novel type of models all parameters of all layers, including kernel weights and biases, only have values equal to zero or one. This novel type of models uses layers named as binary normalized layer. These binary normalized layers can be of any type, such as fully connected, convolutional, attention, etc., and they consist of slight variations of the corresponding conventional layers. To show the effectiveness of the binary normalized layers, two different models are configured to solve a multiclass image classification problem and a language decoder to predict the next token of a sequence. The model to solve the image classification has convolutional and fully connected layers, and the language model is composed of transformer blocks with multi-head attention. The results show that models with binary normalized layers present almost the same results obtained by equivalent models with real 32-bit parameters. The binary normalized layers allow to develop models that use 32 times less memory than current models and have equivalent performance. Besides, the binary normalized layers can be easily implemented on current computers using 1-bit arrays, and do not require the development of dedicated electronic hardware. This novel type of layers opens a new era for large neural network models with reduced memory requirements that can be deployed using simple and cheap hardware, such as mobile devices or only cpus. 

**Abstract (ZH)**: 大型神经网络模型（尤其是语言模型和基础图像模型）规模的不断增加，提出了部署挑战，促使人们努力减少内存需求和提升计算效率。这种努力对于确保这些模型在各种应用中的实际部署和有效利用至关重要。在本工作中，开发了一种新型神经网络层和模型，这些模型仅使用单比特参数。在这些新型模型中，所有层的全部参数，包括卷积核权重和偏差，仅具有0或1的值。这些新型模型使用名为二元归一化层的层。这些二元归一化层可以是任何类型，如全连接层、卷积层、注意力层等，它们是由相应常规层的小幅度变体构成的。为了展示二元归一化层的有效性，配置了两种不同的模型来解决多类图像分类问题和语言解码任务以预测序列的下一个标记。用于解决图像分类的模型包含卷积层和全连接层，语言模型由包含多头注意力机制的Transformer块组成。结果显示，具有二元归一化层的模型在性能上几乎与使用真实32位参数的等效模型相同。二元归一化层使得可以开发出使用当前模型32倍少的内存且具有同等性能的模型。此外，二元归一化层可以在现有计算机上通过使用1比特数组轻松实现，并不需要开发专用的电子硬件。这种新型层为使用简单且廉价的硬件（如移动设备或仅CPU）进行部署的大规模神经网络模型提供了新的时代。 

---
# Preventing Another Tessa: Modular Safety Middleware For Health-Adjacent AI Assistants 

**Title (ZH)**: 防止另一个特莎：健康相关AI助手的模块化安全中间件 

**Authors**: Pavan Reddy, Nithin Reddy  

**Link**: [PDF](https://arxiv.org/pdf/2509.07022)  

**Abstract**: In 2023, the National Eating Disorders Association's (NEDA) chatbot Tessa was suspended after providing harmful weight-loss advice to vulnerable users-an avoidable failure that underscores the risks of unsafe AI in healthcare contexts. This paper examines Tessa as a case study in absent safety engineering and demonstrates how a lightweight, modular safeguard could have prevented the incident. We propose a hybrid safety middleware that combines deterministic lexical gates with an in-line large language model (LLM) policy filter, enforcing fail-closed verdicts and escalation pathways within a single model call. Using synthetic evaluations, we show that this design achieves perfect interception of unsafe prompts at baseline cost and latency, outperforming traditional multi-stage pipelines. Beyond technical remedies, we map Tessa's failure patterns to established frameworks (OWASP LLM Top10, NIST SP 800-53), connecting practical safeguards to actionable governance controls. The results highlight that robust, auditable safety in health-adjacent AI does not require heavyweight infrastructure: explicit, testable checks at the last mile are sufficient to prevent "another Tessa", while governance and escalation ensure sustainability in real-world deployment. 

**Abstract (ZH)**: 2023年， NATIONAL EATING DISORDERS ASSOCIATION (NEDA)的聊天机器人Tessa因向脆弱用户提供了有害的减肥建议而被暂停—这是一个可避免的失败，凸显了医疗保健领域不安全AI的风险。本文以Tessa为例，探讨缺失的安全工程，并展示一种轻量级、模块化的保护措施如何可以防止此类事件。我们提出了一种混合安全中间件，结合确定性的词典门和内置的大规模语言模型（LLM）策略过滤器，确保在一个模型调用中实现封闭故障判断和升级路径。通过合成评估，我们证明这种设计可以在基线成本和延迟下完美拦截不安全的提示，优于传统的多阶段管道。除了技术补救措施，我们将Tessa的失败模式映射到现有的框架（OWASP LLM Top10, NIST SP 800-53），将实用的保护措施与可行的治理控制相结合。结果表明，健康相关领域中的稳健、可审计的安全性不需要重架构设：在最后一公里进行明确、可测试的检查就足以防止“另一个Tessa”，而治理和升级确保其实现实世界部署中的可持续性。 

---
# MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning 

**Title (ZH)**: MEGS$^{2}$: 基于球面高斯和统一剪支的内存高效高斯点积算法 

**Authors**: Jiarui Chen, Yikeng Chen, Yingshuang Zou, Ye Huang, Peng Wang, Yuan Liu, Yujing Sun, Wenping Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.07021)  

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality. 

**Abstract (ZH)**: 3DGS的内存高效框架MEGS$^{2}$：联合优化关键因素实现前所未有的内存压缩 

---
# An efficient deep reinforcement learning environment for flexible job-shop scheduling 

**Title (ZH)**: 一种高效深度强化学习环境下的灵活作业 shop 调度方法 

**Authors**: Xinquan Wu, Xuefeng Yan, Mingqiang Wei, Donghai Guan  

**Link**: [PDF](https://arxiv.org/pdf/2509.07019)  

**Abstract**: The Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial optimization problem that has a wide-range of applications in the real world. In order to generate fast and accurate scheduling solutions for FJSP, various deep reinforcement learning (DRL) scheduling methods have been developed. However, these methods are mainly focused on the design of DRL scheduling Agent, overlooking the modeling of DRL environment. This paper presents a simple chronological DRL environment for FJSP based on discrete event simulation and an end-to-end DRL scheduling model is proposed based on the proximal policy optimization (PPO). Furthermore, a short novel state representation of FJSP is proposed based on two state variables in the scheduling environment and a novel comprehensible reward function is designed based on the scheduling area of machines. Experimental results on public benchmark instances show that the performance of simple priority dispatching rules (PDR) is improved in our scheduling environment and our DRL scheduling model obtains competing performance compared with OR-Tools, meta-heuristic, DRL and PDR scheduling methods. 

**Abstract (ZH)**: 基于离散事件仿真的一种简单的柔性作业车间调度问题 chronological DRL 环境及端到端的基于 PPO 的 DRL 调度模型 

---
# Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV 

**Title (ZH)**: 随机森林分层K折交叉验证在SYN DoS攻击SD-IoV上的应用 

**Authors**: Muhammad Arif Hakimi Zamrai, Kamaludin Mohd Yusof  

**Link**: [PDF](https://arxiv.org/pdf/2509.07016)  

**Abstract**: In response to the prevalent concern of TCP SYN flood attacks within the context of Software-Defined Internet of Vehicles (SD-IoV), this study addresses the significant challenge of network security in rapidly evolving vehicular communication systems. This research focuses on optimizing a Random Forest Classifier model to achieve maximum accuracy and minimal detection time, thereby enhancing vehicular network security. The methodology involves preprocessing a dataset containing SYN attack instances, employing feature scaling and label encoding techniques, and applying Stratified K-Fold cross-validation to target key metrics such as accuracy, precision, recall, and F1-score. This research achieved an average value of 0.999998 for all metrics with a SYN DoS attack detection time of 0.24 seconds. Results show that the fine-tuned Random Forest model, configured with 20 estimators and a depth of 10, effectively differentiates between normal and malicious traffic with high accuracy and minimal detection time, which is crucial for SD-IoV networks. This approach marks a significant advancement and introduces a state-of-the-art algorithm in detecting SYN flood attacks, combining high accuracy with minimal detection time. It contributes to vehicular network security by providing a robust solution against TCP SYN flood attacks while maintaining network efficiency and reliability. 

**Abstract (ZH)**: 针对软件定义车联网（SD-IoV）环境下普遍存在的TCP SYN洪泛攻击 Concern帖子 

---
# Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models 

**Title (ZH)**: 人力介入循环: 大型语言模型生成3D模型的质量评估 

**Authors**: Ahmed R. Sadik, Mariusz Bujny  

**Link**: [PDF](https://arxiv.org/pdf/2509.07010)  

**Abstract**: Large Language Models are increasingly capable of interpreting multimodal inputs to generate complex 3D shapes, yet robust methods to evaluate geometric and structural fidelity remain underdeveloped. This paper introduces a human in the loop framework for the quantitative evaluation of LLM generated 3D models, supporting applications such as democratization of CAD design, reverse engineering of legacy designs, and rapid prototyping. We propose a comprehensive suite of similarity and complexity metrics, including volumetric accuracy, surface alignment, dimensional fidelity, and topological intricacy, to benchmark generated models against ground truth CAD references. Using an L bracket component as a case study, we systematically compare LLM performance across four input modalities: 2D orthographic views, isometric sketches, geometric structure trees, and code based correction prompts. Our findings demonstrate improved generation fidelity with increased semantic richness, with code level prompts achieving perfect reconstruction across all metrics. A key contribution of this work is demonstrating that our proposed quantitative evaluation approach enables significantly faster convergence toward the ground truth, especially compared to traditional qualitative methods based solely on visual inspection and human intuition. This work not only advances the understanding of AI assisted shape synthesis but also provides a scalable methodology to validate and refine generative models for diverse CAD applications. 

**Abstract (ZH)**: 大语言模型越来越能够解析多模态输入以生成复杂的三维形状，但几何和结构保真度的鲁棒评估方法仍处于开发初期。本文介绍了一种人工在环的框架，用于定量评估LLM生成的三维模型，支持CAD设计民主化、遗产设计逆向工程以及快速原型制作等应用。我们提出了一套全面的相似性和复杂性度量标准，包括体素准确性、表面对齐、尺寸保真度和拓扑复杂性，以将生成的模型与真实CAD参考进行基准测试。以L形支架组件为例，我们系统地比较了LLM在四种输入模态下的性能：2D正交视图、等轴测草图、几何结构树以及基于代码的校正提示。我们的研究结果表明，随着语义丰富度的提高，生成的保真度有所提升，代码级别的提示实现了所有度量标准下的完美重建。本文的一项重要贡献是证明了我们提出的定量评估方法可以显著加快向真实参考值的收敛速度，尤其是在与仅基于视觉检查和人类直觉的传统定性方法相比时。这项工作不仅促进了AI辅助形状合成的理解，还提供了一种可扩展的方法来验证和细化针对各种CAD应用的生成模型。 

---
# Computational Concept of the Psyche 

**Title (ZH)**: 心理计算概念 

**Authors**: Anton Kolonin, Vladimir Kryukov  

**Link**: [PDF](https://arxiv.org/pdf/2509.07009)  

**Abstract**: The article provides an overview of approaches to modeling the human psyche in the perspective of building an artificial one. Based on the review, a concept of cognitive architecture is proposed, where the psyche is considered as an operating system of a living or artificial subject, including a space of needs that determines its life meanings in connection with stimuli from the external world, and intelligence as a decision-making system for actions in relation to this world in order to satisfy these needs. Based on the concept, a computational formalization is proposed for creating artificial intelligence systems through learning from experience in the space of a space of needs, taking into account their biological or existential significance for an intelligent agent. Thus, the problem of building general artificial intelligence as a system for making optimal decisions in the space of agent-specific needs under conditions of uncertainty is formalized, with maximization of success in achieving goals, minimization of existential risks and maximization of energy efficiency. A minimal experimental implementation of the model is also provided. 

**Abstract (ZH)**: 文章提供了从构建人工 psyche 视角建模人类心理的方法概述。基于这一回顾，提出了一个认知架构的概念，将 psyche 视为生物或人工主体的操作系统，包括需要的空间，这决定了其生命意义与外部世界的刺激相连，以及作为与这个世界互动以满足这些需要的决策系统的智能。基于这一概念，提出了通过在需要空间中学习经验来创建人工智能系统的计算形式化方法，同时考虑其对智能代理的生物学或存在意义。因此，构建通用人工智能作为在不确定条件下针对特定代理需求空间做出最优决策的系统问题被形式化，追求目标实现的成功最大化、存在风险最小化以及能量效率最大化。还提供了一个最小规模的模型实验实施。 

---
# ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code 

**Title (ZH)**: ArGen: 通过GRPO和Policy-as-Code自动调节生成型AI 

**Authors**: Kapil Madan  

**Link**: [PDF](https://arxiv.org/pdf/2509.07006)  

**Abstract**: This paper introduces ArGen (Auto-Regulation of Generative AI systems), a framework for aligning Large Language Models (LLMs) with complex sets of configurable, machine-readable rules spanning ethical principles, operational safety protocols, and regulatory compliance standards. Moving beyond just preference-based alignment, ArGen is designed to ensure LLMs adhere to these multifaceted policies through a novel synthesis of principle-based automated reward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy Agent (OPA) inspired governance layer. This approach provides the technical foundation for achieving and demonstrating compliance with diverse and nuanced governance requirements. To showcase the framework's capability to operationalize a deeply nuanced and culturally-specific value system, we present an in-depth case study: the development of a medical AI assistant guided by principles from Dharmic ethics (such as Ahimsa and Dharma), as derived from texts like the Bhagavad Gita. This challenging application demonstrates ArGen's adaptability, achieving a 70.9% improvement in domain-scope adherence over the baseline. Through our open-source repository, we show that ArGen's methodology offers a path to 'Governable Al' systems that are technically proficient, ethically robust, and verifiably compliant for safe deployment in diverse global contexts. 

**Abstract (ZH)**: 本文介绍了ArGen（生成AI系统的自动调节框架），这是一种用于使大型语言模型（LLMs）与复杂的可配置、机器可读规则对齐的框架，这些规则涵盖伦理原则、运营安全协议和合规标准。ArGen超越了基于偏好对齐的局限，通过一种新颖的原则导向的自动化奖励评分合成、Group Relative Policy Optimisation (GRPO) 以及受Open Policy Agent (OPA) 启发的治理层，设计用于确保LLMs遵循这些多维度的政策。该方法为满足和展示多样且细腻的治理要求的技术基础提供了支撑。为了展示该框架在实现深刻细腻与文化特定价值系统方面的能力，我们呈现了一个深入的应用案例研究：以《薄伽梵歌》等文本中提取的Dharmic伦理原则（如Ahimsa和Dharma）指导的医疗AI助手的开发。这一具有挑战性的应用展示了ArGen的适应性，实现了70.9%的领域范围一致性改进。通过我们开源的代码库，我们表明ArGen的方法为在多样化的全球背景下实现技术娴熟、伦理稳健且可验证合规的‘可治理AI’系统提供了途径。 

---
# Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories 

**Title (ZH)**: 非所有分割皆平等：重新思考不相关类别间的属性泛化 

**Authors**: Liviu Nicolae Fircă, Antonio Bărbălau, Dan Oneata, Elena Burceanu  

**Link**: [PDF](https://arxiv.org/pdf/2509.06998)  

**Abstract**: Can models generalize attribute knowledge across semantically and perceptually dissimilar categories? While prior work has addressed attribute prediction within narrow taxonomic or visually similar domains, it remains unclear whether current models can abstract attributes and apply them to conceptually distant categories. This work presents the first explicit evaluation for the robustness of the attribute prediction task under such conditions, testing whether models can correctly infer shared attributes between unrelated object types: e.g., identifying that the attribute "has four legs" is common to both "dogs" and "chairs". To enable this evaluation, we introduce train-test split strategies that progressively reduce correlation between training and test sets, based on: LLM-driven semantic grouping, embedding similarity thresholding, embedding-based clustering, and supercategory-based partitioning using ground-truth labels. Results show a sharp drop in performance as the correlation between training and test categories decreases, indicating strong sensitivity to split design. Among the evaluated methods, clustering yields the most effective trade-off, reducing hidden correlations while preserving learnability. These findings offer new insights into the limitations of current representations and inform future benchmark construction for attribute reasoning. 

**Abstract (ZH)**: 能否在语义和感知差异较大的类别之间泛化属性知识？尽管先前的工作已经解决了在狭窄分类学范围内或视觉上相似领域内的属性预测问题，但对于当前模型是否能够抽象出属性并将其应用到概念上相距较远的类别中仍不清楚。本研究首次对在这些条件下属性预测任务的鲁棒性进行了显式的评估，测试模型是否能够正确推断不相关对象类型之间的共享属性：例如，识别“有四条腿”这一属性同时存在于“狗”和“椅子”这两种物体中。为了实现这一评估，我们引入了基于以下策略的训练-测试划分方案：由LLM驱动的语义分组、嵌入相似性阈值化、基于嵌入的聚类以及基于超级类别的分隔，使用真实标签。结果显示，随着训练和测试类别之间的相关性降低，性能出现急剧下降，表明对划分设计高度敏感。在评估的方法中，聚类方法提供了最有效的权衡，既能减少隐藏的相关性又能保持可学习性。这些发现为当前表示的局限性提供了新的见解，并为未来属性推理基准的构建提供了指导。 

---
# Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems 

**Title (ZH)**: 可见却又不可读：跨书写系统的视觉语言模型系统性盲点 

**Authors**: Jie Zhang, Ting Xu, Gelei Deng, Runyi Hu, Han Qiu, Tianwei Zhang, Qing Guo, Ivor Tsang  

**Link**: [PDF](https://arxiv.org/pdf/2509.06996)  

**Abstract**: Writing is a universal cultural technology that reuses vision for symbolic communication. Humans display striking resilience: we readily recognize words even when characters are fragmented, fused, or partially occluded. This paper investigates whether advanced vision language models (VLMs) share this resilience. We construct two psychophysics inspired benchmarks across distinct writing systems, Chinese logographs and English alphabetic words, by splicing, recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli for models while remaining legible to humans. Despite strong performance on clean text, contemporary VLMs show a severe drop under these perturbations, frequently producing unrelated or incoherent outputs. The pattern suggests a structural limitation: models heavily leverage generic visual invariances but under rely on compositional priors needed for robust literacy. We release stimuli generation code, prompts, and evaluation protocols to facilitate transparent replication and follow up work. Our findings motivate architectures and training strategies that encode symbol segmentation, composition, and binding across scripts, and they delineate concrete challenges for deploying multimodal systems in education, accessibility, cultural heritage, and security. 

**Abstract (ZH)**: 高级视觉语言模型在模糊文字下的弹性研究：基于心理物理学的跨书写系统基准测试揭示结构局限与应用场景挑战 

---
# The Protocol Genome A Self Supervised Learning Framework from DICOM Headers 

**Title (ZH)**: DICOM头自监督学习框架：协议基因 

**Authors**: Jimmy Joseph  

**Link**: [PDF](https://arxiv.org/pdf/2509.06995)  

**Abstract**: In this paper, we introduce the Protocol Genome, a self-supervised learning system that learns correlations from DICOM headers and achieves AUROC 0.901 (vs 0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation. Our method also improves calibration and robustness across modalities (CT, MRI, CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where procedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice thickness) have consequences for contrast, noise, and artifact. These latent confounders impede the generalization of image-only networks across sites. We consider structured DICOM headers as a label and learn protocol-aware but clinically robust image representations. Protocol Genome obtains tokenized embeddings of de-identified header fields and models them along with image features using: (1) protocol-image contrastive learning, (2) masked protocol prediction, and (3) protocol-protocol translation. With 1.26M studies (7 health systems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT triage for PE, (B) brain MRI glioma grading, and (C) chest radiograph cardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well as ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041: cardiomegaly) is associated with higher external AUROC; 25-37% calibration improvements are obtained (p < 0.01, DeLong tests). While the gains may be task-dependent, they are preserved with 10-20% of labeled data. From a clinical point of view, the technique reduces false positives at protocol borders and is applicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a model card and deployment guide, complete with both de-identification and bias audits. 

**Abstract (ZH)**: 基于协议的基因组：一种自监督学习系统及其在临床影像中的应用 

---
# Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection 

**Title (ZH)**: frustringly简单的特点重建用于异常分布检测 

**Authors**: Yingsheng Wang, Shuo Lu, Jian Liang, Aihua Zheng, Ran He  

**Link**: [PDF](https://arxiv.org/pdf/2509.06988)  

**Abstract**: Out-of-distribution (OOD) detection helps models identify data outside the training categories, crucial for security applications. While feature-based post-hoc methods address this by evaluating data differences in the feature space without changing network parameters, they often require access to training data, which may not be suitable for some data privacy scenarios. This may not be suitable in scenarios where data privacy protection is a concern. In this paper, we propose a simple yet effective post-hoc method, termed Classifier-based Feature Reconstruction (ClaFR), from the perspective of subspace projection. It first performs an orthogonal decomposition of the classifier's weights to extract the class-known subspace, then maps the original data features into this subspace to obtain new data representations. Subsequently, the OOD score is determined by calculating the feature reconstruction error of the data within the subspace. Compared to existing OOD detection algorithms, our method does not require access to training data while achieving leading performance on multiple OOD benchmarks. Our code is released at this https URL. 

**Abstract (ZH)**: 离分布（OOD）检测有助于模型识别训练类别之外的数据，对于安全应用至关重要。特征基于的后 hoc 方法通过在特征空间中评估数据差异来实现这一目标，而不更改网络参数，但通常需要访问训练数据，这在某些数据隐私场景中可能不合适。在数据隐私保护是关注点的情况下，这可能不太合适。本文从子空间投影的角度提出了一种简单而有效的后 hoc 方法，称为基于分类器的特征重建（ClaFR）。该方法首先对手分类器的权重进行正交分解以提取已知类别子空间，然后将原始数据特征映射到该子空间以获得新的数据表示。随后，通过计算子空间内数据的特征重建误差来确定OOD得分。与现有OOD检测算法相比，我们的方法无需访问训练数据，在多个OOD基准上实现了领先地位。代码已发布在该网址：此 https URL。 

---
# FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection 

**Title (ZH)**: FusWay: 多模态混合融合方法及其在铁路缺陷检测中的应用 

**Authors**: Alexey Zhukov, Jenny Benois-Pineau, Amira Youssef, Akka Zemmari, Mohamed Mosbah, Virginie Taillandier  

**Link**: [PDF](https://arxiv.org/pdf/2509.06987)  

**Abstract**: Multimodal fusion is a multimedia technique that has become popular in the wide range of tasks where image information is accompanied by a signal/audio. The latter may not convey highly semantic information, such as speech or music, but some measures such as audio signal recorded by mics in the goal to detect rail structure elements or defects. While classical detection approaches such as You Only Look Once (YOLO) family detectors can be efficiently deployed for defect detection on the image modality, the single modality approaches remain limited. They yield an overdetection in case of the appearance similar to normal structural elements. The paper proposes a new multimodal fusion architecture built on the basis of domain rules with YOLO and Vision transformer backbones. It integrates YOLOv8n for rapid object detection with a Vision Transformer (ViT) to combine feature maps extracted from multiple layers (7, 16, and 19) and synthesised audio representations for two defect classes: rail Rupture and Surface defect. Fusion is performed between audio and image. Experimental evaluation on a real-world railway dataset demonstrates that our multimodal fusion improves precision and overall accuracy by 0.2 points compared to the vision-only approach. Student's unpaired t-test also confirms statistical significance of differences in the mean accuracy. 

**Abstract (ZH)**: 多模态融合是一种多媒体技术，它在伴随图像信息的信号/音频任务中变得越来越流行。后者可能不包含高度语义信息，如语音或音乐，但某些措施如用于检测轨道结构元素或缺陷的麦克风录制的音频信号。虽然YOLO家族检测器等经典检测方法可以高效地部署在图像模态的缺陷检测中，但单一模态方法仍然有限。它们在正常结构元素相似时会产生过度检测。本文提出了一种基于领域规则的新多模态融合架构，基于YOLO和Vision Transformer骨干网络。该架构将YOLOv8n用于快速目标检测与Vision Transformer（ViT）结合，从多个层（7、16和19层）中提取特征图，并与合成的音频表示结合，以两种缺陷类别：轨道破裂和表面缺陷。在音频和图像之间进行融合。在真实铁路数据集上的实验评估表明，我们的多模态融合与仅视觉方法相比，精确度和总体准确度提高了0.2个百分点。威尔克斯T检验还证实了均值准确度差异的统计显著性。 

---
# CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis 

**Title (ZH)**: CellPainTR: 跨数据集细胞绘图分析的泛化表示学习 

**Authors**: Cedric Caruzzo, Jong Chul Ye  

**Link**: [PDF](https://arxiv.org/pdf/2509.06986)  

**Abstract**: Large-scale biological discovery requires integrating massive, heterogeneous datasets like those from the JUMP Cell Painting consortium, but technical batch effects and a lack of generalizable models remain critical roadblocks. To address this, we introduce CellPainTR, a Transformer-based architecture designed to learn foundational representations of cellular morphology that are robust to batch effects. Unlike traditional methods that require retraining on new data, CellPainTR's design, featuring source-specific context tokens, allows for effective out-of-distribution (OOD) generalization to entirely unseen datasets without fine-tuning. We validate CellPainTR on the large-scale JUMP dataset, where it outperforms established methods like ComBat and Harmony in both batch integration and biological signal preservation. Critically, we demonstrate its robustness through a challenging OOD task on the unseen Bray et al. dataset, where it maintains high performance despite significant domain and feature shifts. Our work represents a significant step towards creating truly foundational models for image-based profiling, enabling more reliable and scalable cross-study biological analysis. 

**Abstract (ZH)**: 大规模生物发现需要整合如JUMP Cell Painting联盟那样的大规模异质数据集，但技术和批次效应以及缺乏泛化模型仍然是关键障碍。为此，我们引入了CellPainTR，这是一种基于变换器的架构，设计用于学习对批次效应具有鲁棒性的细胞形态基础表示。与需要在新数据上重新训练的传统方法不同，CellPainTR 通过特定来源的上下文标记设计，能够在无需微调的情况下有效地泛化到完全未见过的数据集。我们在大规模的JUMP数据集上验证了CellPainTR，结果显示它在批次整合和生物信号保留方面优于现有的方法如ComBat和Harmony。更重要的是，我们在Bray等人数据集的具有挑战性的未见分布任务中展示了其鲁棒性，即使存在显著的领域和特征转移，它仍能保持高性能。我们的工作代表了朝着为基于图像的表型分析创建真正基础模型的重要一步，将促进更可靠和可扩展的跨研究生物分析。 

---
# FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities 

**Title (ZH)**: FediLoRA: 异构LoRA在多模态联邦微调中的_missing modalities_处理 

**Authors**: Lishan Yang, Nam Kha Nguygen, Po Hu, Wei Emma Zhang, Yanjun Shu, Mong Yuan Sim, Weitong Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.06984)  

**Abstract**: Foundation models have demonstrated remarkable performance across a wide range of tasks, yet their large parameter sizes pose challenges for practical deployment, especially in decentralized environments. Parameter-efficient fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing and memory overhead, making it attractive for federated learning. However, existing federated LoRA methods typically assume uniform rank configurations and unimodal inputs, overlooking two key real-world challenges: (1) heterogeneous client resources have different LoRA ranks, and (2) multimodal data settings with potentially missing modalities. In this work, we propose FediLoRA, a simple yet effective framework for federated multimodal fine-tuning under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a dimension-wise aggregation strategy that reweights LoRA updates without information dilution during aggregation. It also includes a lightweight layer-wise model editing method that selectively incorporates global parameters to repair local components which improves both client and global model performances. Experimental results on three multimodal benchmark datasets demonstrate that FediLoRA achieves superior performance over competitive baselines in both global and personalized settings, particularly in the presence of modality incompleteness. 

**Abstract (ZH)**: 联邦多模态细调中的FediLoRA：异构LoRA秩和缺失模态下的简单有效框架 

---
# CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention 

**Title (ZH)**: CARE: 通过回滚和反省干预实现时间安全性对齐 

**Authors**: Xiaomeng Hu, Fei Huang, Chenhan Yuan, Junyang Lin, Tsung-Yi Ho  

**Link**: [PDF](https://arxiv.org/pdf/2509.06982)  

**Abstract**: As large language models (LLMs) are increasingly deployed in real-world applications, ensuring the safety of their outputs during decoding has become a critical challenge. However, existing decoding-time interventions, such as Contrastive Decoding, often force a severe trade-off between safety and response quality. In this work, we propose CARE, a novel framework for decoding-time safety alignment that integrates three key components: (1) a guard model for real-time safety monitoring, enabling detection of potentially unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe outputs efficiently at an earlier stage without disrupting the user experience; and (3) a novel introspection-based intervention strategy, where the model generates self-reflective critiques of its previous outputs and incorporates these reflections into the context to guide subsequent decoding steps. The framework achieves a superior safety-quality trade-off by using its guard model for precise interventions, its rollback mechanism for timely corrections, and our novel introspection method for effective self-correction. Experimental results demonstrate that our framework achieves a superior balance of safety, quality, and efficiency, attaining a low harmful response rate and minimal disruption to the user experience while maintaining high response quality. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）在实际应用中的逐步部署，确保其解码输出的安全性已成为一个关键挑战。然而，现有的解码时干预方法，如对比解码，往往在安全性和响应质量之间造成了严重的权衡。在这项工作中，我们提出了CARE，一种新颖的解码时安全性对齐框架，集成三个关键组件：（1）一个守护模型进行实时安全性监控，实现对潜在不安全内容的检测；（2）一个带有令牌缓冲区的回滚机制，能够在早期阶段高效纠正不安全的输出，而不干扰用户体验；以及（3）一种新颖的内省驱动干预策略，模型生成对其先前输出的自省性批评，并将这些反思纳入上下文中，引导后续解码步骤。该框架通过其守护模型实现精确干预、回滚机制实现及时修正，以及我们新颖的内省方法实现有效的自我纠正，从而实现安全性与质量的优异权衡。实验结果表明，我们的框架在安全、质量和效率之间实现了优越的平衡，实现了低有害响应率和最小化的用户体验干扰，同时保持了高响应质量。 

---
# RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use 

**Title (ZH)**: RLFactory: 一种即插即用的大型语言模型多轮工具使用后训练 reinforcement learning 框架 

**Authors**: Jiajun Chai, Guojun Yin, Zekun Xu, Chuhuai Yue, Yi Jia, Siyu Xia, Xiaohan Wang, Jiwen Jiang, Xiaoguang Li, Chengqi Dong, Hang He, Wei Lin  

**Link**: [PDF](https://arxiv.org/pdf/2509.06980)  

**Abstract**: Large language models excel at basic reasoning but struggle with tasks that require interaction with external tools. We present RLFactory, a plug-and-play reinforcement learning post-training framework for multi-round tool use. RLFactory tackles (i) tool-call stability and adaptability amid tool heterogeneity and interface issues via an asyncio-based asynchronous caller and a decoupled tool/training architecture, and (ii) diverse evaluation needs via a reward layer supporting rule-based, model-judgment, and tool-verification signals. It reconstructs the MDP by introducing observation markers from tool feedback, closing the loop among model, tools, and environment, and implements a generate-parse-invoke-update workflow for dynamic policy optimization. On Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural Questions (NQ) dataset, surpassing larger models trained with similar techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable framework for strengthening multi-round tool use of LLMs in real-world scenarios. Code: this https URL. 

**Abstract (ZH)**: 大型语言模型在基本推理方面表现出色，但在需要外部工具交互的任务上遇到困难。我们提出了RLFactory，一种异步插件式强化学习后训练框架，支持多轮工具使用。RLFactory通过基于asyncio的异步调用器和解耦工具/训练架构解决了(i)工具异质性和接口问题带来的调用稳定性和适应性问题；通过奖励层支持基于规则、模型判断和工具验证信号，解决(ii)多样的评估需求。它通过引入来自工具反馈的观察标记重建MDP，实现模型、工具和环境之间的闭环，并采用生成-解析-调用-更新的工作流来实现动态策略优化。在Search-R1上使用Qwen3-4B，RLFactory在Natural Questions (NQ)数据集上的测试得分为0.486，超过了使用类似技术训练的更大模型（如Qwen2.5-7B-Instruct-GRPO的0.473），并将训练吞吐量提高了6.8倍。RLFactory为增强LLM在实际场景中的多轮工具使用提供了低门槛、高度适应的框架。代码：this https URL。 

---
# Exploring Over-stationarization in Deep Learning-based Bus/Tram Arrival Time Prediction: Analysis and Non-stationary Effect Recovery 

**Title (ZH)**: 基于深度学习的公交车/有轨电车到站时间预测中的过时变异性探究：分析与非时变效应恢复 

**Authors**: Zirui Li, Bin Yang, Meng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.06979)  

**Abstract**: Arrival time prediction (ATP) of public transport vehicles is essential in improving passenger experience and supporting traffic management. Deep learning has demonstrated outstanding performance in ATP due to its ability to model non-linear and temporal dynamics. In the multi-step ATP, non-stationary data will degrade the model performance due to the variation in variables' joint distribution along the temporal direction. Previous studies mainly applied normalization to eliminate the non-stationarity in time series, thereby achieving better predictability. However, the normalization may obscure useful characteristics inherent in non-stationarity, which is known as the over-stationarization. In this work, to trade off predictability and non-stationarity, a new approach for multi-step ATP, named non-stationary ATP ( NSATP), is proposed. The method consists of two stages: series stationarization and non-stationarity effect recovery. The first stage aims at improving the predictability. As for the latter, NSATP extends a state-of-the-art method from one-dimensional to two dimensional based models to capture the hidden periodicity in time series and designs a compensation module of over-stationarization by learning scaling and shifting factors from raw data. 125 days' public transport operational data of Dresden is collected for validation. Experimental results show that compared to baseline methods, the proposed NSATP can reduce RMSE, MAE, and MAPE by 2.37%, 1.22%, and 2.26% for trams and by 1.72%, 0.60%, and 1.17% for buses, respectively. 

**Abstract (ZH)**: 公共运输车辆到达时间预测（ATP）对于提升乘客体验和支持交通管理至关重要。非线性和时序动态模型的能力使得深度学习在ATP中表现出色。在多步ATP中，非平稳数据会由于时序上变量联合分布的变化而降低模型性能。先前的研究主要通过归一化来消除时间序列中的非平稳性，从而提高预测能力。然而，归一化可能会掩盖非平稳性中固有的有用特征，这被称为过度平稳化。在此项工作中，为权衡预测能力和非平稳性，提出了一种新的多步ATP方法，称为非平稳ATP（NSATP）。该方法包含两个阶段：序列平稳化和非平稳性效应恢复。第一阶段旨在提高预测能力。对于后者，NSATP将最先进的方法从一维扩展到二维模型，以捕获时间序列中的隐藏周期性，并通过学习原始数据的缩放和位移因子来设计一个过度平稳化的补偿模块。用于验证的数据包括125天德累斯顿的公共运输运营数据。实验结果表明，与基线方法相比，提出的NSATP分别将有轨电车的RMSE、MAE和MAPE降低了2.37%、1.22%和2.26%，将公交车的RMSE、MAE和MAPE分别降低了1.72%、0.60%和1.17%。 

---
# Toward Reproducible Cross-Backend Compatibility for Deep Learning: A Configuration-First Framework with Three-Tier Verification 

**Title (ZH)**: 向深度学习跨后端兼容性可重复性迈进：一种以配置为主导的三层验证框架 

**Authors**: Zehua Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.06977)  

**Abstract**: This paper presents a configuration-first framework for evaluating cross-backend compatibility in deep learning systems deployed on CPU, GPU, and compiled runtimes. The framework decouples experiments from code using YAML, supports both library and repository models, and employs a three-tier verification protocol covering tensor-level closeness, activation alignment, and task-level metrics. Through 672 checks across multiple models and tolerance settings, we observe that 72.0% of runs pass, with most discrepancies occurring under stricter thresholds. Our results show that detection models and compiled backends are particularly prone to drift, often due to nondeterministic post-processing. We further demonstrate that deterministic adapters and selective fallbacks can substantially improve agreement without significant performance loss. To our knowledge, this is the first unified framework that systematically quantifies and mitigates cross-backend drift in deep learning, providing a reproducible methodology for dependable deployment across heterogeneous runtimes. 

**Abstract (ZH)**: 本论文提出了一种配置优先框架，用于评估部署在CPU、GPU和编译运行时环境中的深度学习系统之间的后端兼容性。该框架使用YAML解耦实验与代码，支持库模型和仓库模型，并采用三层验证协议，涵盖张量级接近度、激活对齐以及任务级指标。通过在多个模型和容差设置下进行672次检查，我们观察到72.0%的运行通过，大多数差异主要出现在更严格的阈值下。我们的结果显示，检测模型和编译后端特别容易发生漂移，常由于非确定性的后处理引起。此外，我们还证明，确定性适配器和选择性回退可以在不显著影响性能的情况下显著提高一致性。据我们所知，这是首个系统地量化和缓解深度学习中跨后端漂移的统一框架，为异构运行时环境下的可靠部署提供了一种可重复的方法。 

---
# A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction 

**Title (ZH)**: 基于知识引导的跨模态特征融合模型用于局部交通需求预测 

**Authors**: Lingyu Zhang, Pengfei Xu, Guobin Wu, Jian Liang, Ruiyang Dong, Yunhai Wang, Xuan Song  

**Link**: [PDF](https://arxiv.org/pdf/2509.06976)  

**Abstract**: Traffic demand prediction plays a critical role in intelligent transportation systems. Existing traffic prediction models primarily rely on temporal traffic data, with limited efforts incorporating human knowledge and experience for urban traffic demand forecasting. However, in real-world scenarios, traffic knowledge and experience derived from human daily life significantly influence precise traffic prediction. Such knowledge and experiences can guide the model in uncovering latent patterns within traffic data, thereby enhancing the accuracy and robustness of predictions. To this end, this paper proposes integrating structured temporal traffic data with textual data representing human knowledge and experience, resulting in a novel knowledge-guided cross-modal feature representation learning (KGCM) model for traffic demand prediction. Based on regional transportation characteristics, we construct a prior knowledge dataset using a large language model combined with manual authoring and revision, covering both regional and global knowledge and experiences. The KGCM model then learns multimodal data features through designed local and global adaptive graph networks, as well as a cross-modal feature fusion mechanism. A proposed reasoning-based dynamic update strategy enables dynamic optimization of the graph model's parameters, achieving optimal performance. Experiments on multiple traffic datasets demonstrate that our model accurately predicts future traffic demand and outperforms existing state-of-the-art (SOTA) models. 

**Abstract (ZH)**: 交通需求预测在智能交通运输系统中起着关键作用。现有的交通预测模型主要依赖于时间序列交通数据，较少尝试整合人类的知识和经验以进行城市交通需求预测。然而，在实际场景中，人类日常生活中的交通知识和经验显著影响精确的交通预测。这些知识和经验可以引导模型在交通数据中发现潜在模式，从而提高预测的准确性和鲁棒性。为此，本文提出将结构化的时间序列交通数据与代表人类知识和经验的文本数据相结合，提出了一种新型的知识引导的跨模态特征表示学习（KGCM）模型，用于交通需求预测。基于地区交通特征，我们使用大规模语言模型结合人工编写和修订构建了一个先验知识数据集，涵盖了地区性和全球性的知识和经验。KGCM模型通过设计的局部和全局自适应图网络以及跨模态特征融合机制学习多模态数据特征。提出的基于推理的动态更新策略能够动态优化图模型的参数，以实现最佳性能。在多个交通数据集上的实验表明，我们的模型能够准确预测未来交通需求，并优于现有最先进的（SOTA）模型。 

---
# GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning 

**Title (ZH)**: GSTBench：图自监督学习转移性的一项基准研究 

**Authors**: Yu Song, Zhigang Hua, Yan Xie, Jingzhe Liu, Bo Long, Hui Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.06975)  

**Abstract**: Self-supervised learning (SSL) has shown great promise in graph representation learning. However, most existing graph SSL methods are developed and evaluated under a single-dataset setting, leaving their cross-dataset transferability largely unexplored and limiting their ability to leverage knowledge transfer and large-scale pretraining, factors that are critical for developing generalized intelligence beyond fitting training data. To address this gap and advance foundation model research for graphs, we present GSTBench, the first systematic benchmark for evaluating the transferability of graph SSL methods. We conduct large-scale pretraining on ogbn-papers100M and evaluate five representative SSL methods across a diverse set of target graphs. Our standardized experimental setup decouples confounding factors such as model architecture, dataset characteristics, and adaptation protocols, enabling rigorous comparisons focused solely on pretraining objectives. Surprisingly, we observe that most graph SSL methods struggle to generalize, with some performing worse than random initialization. In contrast, GraphMAE, a masked autoencoder approach, consistently improves transfer performance. We analyze the underlying factors that drive these differences and offer insights to guide future research on transferable graph SSL, laying a solid foundation for the "pretrain-then-transfer" paradigm in graph learning. Our code is available at this https URL. 

**Abstract (ZH)**: 自监督学习（SSL）在图表示学习中展现出了巨大的潜力。然而，现有的大多数图SSL方法仅在单数据集设置下开发和评估，使得它们跨数据集的迁移能力很大程度上未被探索，限制了它们利用知识迁移和大规模预训练等关键因素的能力，这些因素对于开发超越训练数据拟合的通用智能至关重要。为填补这一空白并推进图的础模型研究，我们提出了GSTBench，首个系统性的图SSL方法迁移能力评估基准。我们在ogbn-papers100M上进行大规模预训练，并在多种目标图上评估五种代表性的SSL方法。我们的标准化实验设置将模型架构、数据集特性以及适应协议等混杂因素分离，使比较集中在预训练目标上。令人惊讶的是，我们发现大多数图SSL方法难以泛化，有些甚至表现逊于随机初始化。相比之下，掩码自编码器方法GraphMAE始终能提升迁移性能。我们分析了这些差异背后的驱动因素，并提供了指导未来可迁移图SSL研究的见解，为图学习中的“预训练-然后迁移”范式奠定了坚实基础。我们的代码可通过以下链接获取。 

---
# Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model 

**Title (ZH)**: 基于两阶段自适应空时模型的个性化可解释睡眠预测 

**Authors**: Xueyi Wang, Elisabeth Wilhelm  

**Link**: [PDF](https://arxiv.org/pdf/2509.06974)  

**Abstract**: Sleep quality significantly impacts well-being. Therefore, healthcare providers and individuals need accessible and reliable forecasting tools for preventive interventions. This paper introduces an interpretable, individualized two-stage adaptive spatial-temporal model for predicting sleep quality scores. Our proposed framework combines multi-scale convolutional layers to model spatial interactions across multiple input variables, recurrent layers and attention mechanisms to capture long-term temporal dependencies, and a two-stage domain adaptation strategy to enhance generalization. The first adaptation stage is applied during training to mitigate overfitting on the training set. In the second stage, a source-free test-time adaptation mechanism is employed to adapt the model to new users without requiring labels. We conducted various experiments with five input window sizes (3, 5, 7, 9, and 11 days) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model consistently outperformed time series forecasting baseline approaches, including Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The best performance was achieved with a three-day input window and a one-day prediction window, yielding a root mean square error (RMSE) of 0.216. Furthermore, the model demonstrated good predictive performance even for longer forecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction window), highlighting its practical utility for real-world applications. We also conducted an explainability analysis to examine how different features influence sleep quality. These findings proved that the proposed framework offers a robust, adaptive, and explainable solution for personalized sleep forecasting using sparse data from commercial wearable devices. 

**Abstract (ZH)**: 睡眠质量显著影响福祉。因此，医护人员和个体需要获得可访问且可靠的预测工具以进行预防干预。本文提出了一个可解释的、个性化两阶段自适应时空模型以预测睡眠质量评分。本提出的框架结合了多尺度卷积层来建模多个输入变量的时空交互，循环层和注意力机制来捕捉长期时间依赖性，以及两阶段领域适应策略以增强泛化能力。第一阶段适应在训练过程中应用于减轻对训练集的过拟合。在第二阶段，采用无源测试时自适应机制来适应新用户，无需标签。我们使用五种不同的输入窗口大小（3天、5天、7天、9天和11天）和五种不同的预测窗口大小（1天、3天、5天、7天和9天）进行了各种实验。我们的模型始终优于时间序列预测基准方法，包括长短期记忆（LSTM）、Informer、PatchTST 和 TimesNet。最佳性能是在三天的输入窗口和一天的预测窗口下实现的，得到的均方根误差（RMSE）为0.216。此外，该模型在更远的预测时长内也表现出良好的预测性能（例如，三天预测窗口的RMSE为0.257），突显了其在实际应用中的实用性。我们还进行了可解释性分析以考察不同特征如何影响睡眠质量。这些发现证明了所提出的框架能够提供一个稳健、适应性强且解释明确的个性化睡眠预测解决方案，使用的是来自商用可穿戴设备的稀疏数据。 

---
# Impact of Neuron Models on Spiking Neural Networks performance. A Complexity Based Classification Approach 

**Title (ZH)**: 基于复杂性分类方法的神经元模型对脉冲神经网络性能的影响研究 

**Authors**: Zofia Rudnicka, Janusz Szczepanski, Agnieszka Pregowska  

**Link**: [PDF](https://arxiv.org/pdf/2509.06970)  

**Abstract**: This study explores how the selection of neuron models and learning rules impacts the classification performance of Spiking Neural Networks (SNNs), with a focus on applications in bio-signal processing. We compare biologically inspired neuron models, including Leaky Integrate-and-Fire (LIF), metaneurons, and probabilistic Levy-Baxter (LB) neurons, across multiple learning rules, including spike-timing-dependent plasticity (STDP), tempotron, and reward-modulated updates. A novel element of this work is the integration of a complexity-based decision mechanism into the evaluation pipeline. Using Lempel-Ziv Complexity (LZC), a measure related to entropy rate, we quantify the structural regularity of spike trains and assess classification outcomes in a consistent and interpretable manner across different SNN configurations. To investigate neural dynamics and assess algorithm performance, we employed synthetic datasets with varying temporal dependencies and stochasticity levels. These included Markov and Poisson processes, well-established models to simulate neuronal spike trains and capture the stochastic firing behavior of biological this http URL of synthetic Poisson and Markov-modeled data reveals clear performance trends: classification accuracy depends on the interaction between neuron model, network size, and learning rule, with the LZC-based evaluation highlighting configurations that remain robust to weak or noisy signals. This work delivers a systematic analysis of how neuron model selection interacts with network parameters and learning strategies, supported by a novel complexity-based evaluation approach that offers a consistent benchmark for SNN performance. 

**Abstract (ZH)**: 本研究探讨了神经元模型选择和学习规则对脉冲神经网络（SNN）分类性能的影响，重点关注生物信号处理应用。我们比较了受生物学启发的神经元模型，包括耗尽积分-发放（LIF）、超神经元和概率莱维-巴克斯特（LB）神经元，以及多种学习规则，包括时序依赖可塑性（STDP）、Tempotron和奖励调节更新。这项工作的创新之处在于将基于复杂性的决策机制整合到评估管道中。通过使用Lempel-Ziv复杂度（LZC），一种与熵率相关的度量，我们量化了尖峰序列的结构规律，并采用一致且可解释的方式评估不同SNN配置的分类结果。为了研究神经动态并评估算法性能，我们使用了具有不同时间依赖性和随机性水平的合成数据集。这些数据集包括马尔可夫过程和泊松过程，是模拟神经元尖峰序列和捕捉生物神经元随机放电行为的经典模型。基于合成泊松和马尔可夫建模数据的研究表明，分类准确性取决于神经元模型、网络规模和学习规则之间的相互作用，而基于LZC的评估突出了对弱或噪声信号具有鲁棒性的配置。本研究通过一种新颖的基于复杂性的评估方法，系统分析了神经元模型选择与网络参数及学习策略的相互作用，并提供了一致的SNN性能基准。 

---
# Association of Timing and Duration of Moderate-to-Vigorous Physical Activity with Cognitive Function and Brain Aging: A Population-Based Study Using the UK Biobank 

**Title (ZH)**: 中等至剧烈体力活动的时间和持续时间与认知功能和大脑老化关系的队列研究：基于英国生物银行数据 

**Authors**: Wasif Khan, Lin Gu, Noah Hammarlund, Lei Xing, Joshua K. Wong, Ruogu Fang  

**Link**: [PDF](https://arxiv.org/pdf/2509.06969)  

**Abstract**: Physical activity is a modifiable lifestyle factor with potential to support cognitive resilience. However, the association of moderate-to-vigorous physical activity (MVPA) intensity, and timing, with cognitive function and region-specific brain structure remain poorly understood. We analyzed data from 45,892 UK Biobank participants aged 60 years and older with valid wrist-worn accelerometer data, cognitive testing, and structural brain MRI. MVPA was measured both continuously (mins per week) and categorically (thresholded using >=150 min/week based on WHO guidelines). Associations with cognitive performance and regional brain volumes were evaluated using multivariable linear models adjusted for demographic, socioeconomic, and health-related covariates. We conducted secondary analyses on MVPA timing and subgroup effects. Higher MVPA was associated with better performance across cognitive domains, including reasoning, memory, executive function, and processing speed. These associations persisted in fully adjusted models and were higher among participants meeting WHO guidelines. Greater MVPA was also associated with subcortical brain regions (caudate, putamen, pallidum, thalamus), as well as regional gray matter volumes involved in emotion, working memory, and perceptual processing. Secondary analyses showed that MVPA at any time of day was associated with cognitive functions and brain volume particularly in the midday-afternoon and evening. Sensitivity analysis shows consistent findings across subgroups, with evidence of dose-response relationships. Higher MVPA is associated with preserved brain structure and enhanced cognitive function in later life. Public health strategies to increase MVPA may support healthy cognitive aging and generate substantial economic benefits, with global gains projected to reach USD 760 billion annually by 2050. 

**Abstract (ZH)**: 中等至剧烈强度的体力活动与认知功能及区域特异性脑结构之间的关系尚不明确。我们分析了45,892名英国生物银行参与者（年龄60岁及以上）的有效腕戴式加速度计数据、认知测试和结构脑MRI数据。中等至剧烈强度的体力活动（MVPA）被连续测量（每周分钟数）和分类测量（基于WHO指南定义的≥150分钟/周）。通过多变量线性模型评估其与认知表现和区域性脑体积的关联，调整了人口统计学、社会经济和健康相关的混杂因素。我们还进行了MVPA时间的相关分析和亚组效应分析。较高水平的MVPA在各个认知领域均与更好的表现相关，包括推理、记忆、执行功能和处理速度。这些关联在完全调整的模型中仍然存在，并且在达到WHO指南标准的参与者中更为显著。较高水平的MVPA还与基底节（壳核、苍白球、丘脑）以及与情绪、工作记忆和感知处理相关的皮层下区域和皮质灰质体积相关。次要分析显示，无论是白天还是晚上的MVPA都与认知功能和脑体积在午后和傍晚时分特别相关。敏感性分析结果显示，这些发现跨亚组一致，并显示出剂量-反应关系。较高水平的MVPA在晚年与保持的脑结构和增强的认知功能相关。增加MVPA的公共卫生策略可能支持健康的认知老化并产生重大的经济利益，全球收益预计到2050年将达到每年7600亿美元。 

---
# Deep Learning-based Techniques for Integrated Sensing and Communication Systems: State-of-the-Art, Challenges, and Opportunities 

**Title (ZH)**: 基于深度学习的综合传感与通信系统技术：现状、挑战与机遇 

**Authors**: Murat Temiz, Yongwei Zhang, Yanwei Fu, Chi Zhang, Chenfeng Meng, Orhan Kaplan, Christos Masouros  

**Link**: [PDF](https://arxiv.org/pdf/2509.06968)  

**Abstract**: This article comprehensively reviews recent developments and research on deep learning-based (DL-based) techniques for integrated sensing and communication (ISAC) systems. ISAC, which combines sensing and communication functionalities, is regarded as a key enabler for 6G and beyond networks, as many emerging applications, such as vehicular networks and industrial robotics, necessitate both sensing and communication capabilities for effective operation. A unified platform that provides both functions can reduce hardware complexity, alleviate frequency spectrum congestion, and improve energy efficiency. However, integrating these functionalities on the same hardware requires highly optimized signal processing and system design, introducing significant computational complexity when relying on conventional iterative or optimization-based techniques. As an alternative to conventional techniques, DL-based techniques offer efficient and near-optimal solutions with reduced computational complexity. Hence, such techniques are well-suited for operating under limited computational resources and low latency requirements in real-time systems. DL-based techniques can swiftly and effectively yield near-optimal solutions for a wide range of sophisticated ISAC-related tasks, including waveform design, channel estimation, sensing signal processing, data demodulation, and interference mitigation. Therefore, motivated by these advantages, recent studies have proposed various DL-based approaches for ISAC system design. After briefly introducing DL architectures and ISAC fundamentals, this survey presents a comprehensive and categorized review of state-of-the-art DL-based techniques for ISAC, highlights their key advantages and major challenges, and outlines potential directions for future research. 

**Abstract (ZH)**: 基于深度学习的集成传感与通信系统研究综述 

---
# Cross-field SNR Analysis and Tensor Channel Estimation for Multi-UAV Near-field Communications 

**Title (ZH)**: 跨领域信噪比分析和张量信道估计在多无人机近场通信中 

**Authors**: Tianyu Huo, Jian Xiong, Yiyan Wu, Songjie Yang, Bo Liu, Wenjun Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.06967)  

**Abstract**: Extremely large antenna array (ELAA) is key to enhancing spectral efficiency in 6G networks. Leveraging the distributed nature of multi-unmanned aerial vehicle (UAV) systems enables the formation of distributed ELAA, which often operate in the near-field region with spatial sparsity, rendering the conventional far-field plane wave assumption invalid. This paper investigates channel estimation for distributed near-field multi-UAV communication systems. We first derive closed-form signal-to-noise ratio (SNR) expressions under the plane wave model (PWM), spherical wave model (SWM), and a hybrid spherical-plane wave model (HSPWM), also referred to as the cross-field model, within a distributed uniform planar array (UPA) scenario. The analysis shows that HSPWM achieves a good balance between modeling accuracy and analytical tractability. Based on this, we propose two channel estimation algorithms: the spherical-domain orthogonal matching pursuit (SD-OMP) and the tensor-OMP. The SD-OMP generalizes the polar domain to jointly consider elevation, azimuth, and range. Under the HSPWM, the channel is naturally formulated as a tensor, enabling the use of tensor-OMP. Simulation results demonstrate that tensor-OMP achieves normalized mean square error (NMSE) performance comparable to SD-OMP, while offering reduced computational complexity and improved scalability. 

**Abstract (ZH)**: 分布式近场多无人机通信系统的信道估计研究 

---
# Cross-device Zero-shot Label Transfer via Alignment of Time Series Foundation Model Embeddings 

**Title (ZH)**: 跨设备零样本标签转移：时间序列基础模型嵌入的对齐 

**Authors**: Neal G. Ravindra, Arijit Sehanobish  

**Link**: [PDF](https://arxiv.org/pdf/2509.06966)  

**Abstract**: High-quality, medically validated labels exist for clinical actigraphy data but not for ubiquitous consumer wearables like the Apple Watch. Manually labeling wearables data is expensive and doesn't scale. This paper offers a novel framework that transfers valuable labels from a source domain (e.g., actigraphy) to a target domain (e.g., Apple Watch) without requiring paired data. Instead of working with raw time-series signals, we project both domains into a shared latent embedding space using time-series foundation models (TSFMs) and develop a new framework to align the cross-device representations. Our method, Adversarial Alignment of TSFM Embeddings forces the distributions of source and target embeddings to align within this space, facilitating label transfer across device type. 

**Abstract (ZH)**: 高质量、医学验证的标签存在于临床加速度计数据中，但不存在于如Apple Watch等普遍的消费者穿戴设备中。手动标记穿戴设备数据既昂贵又不具扩展性。本文提出了一种新颖的框架，能够在不需要配对数据的情况下，将有价值的数据标签从源领域（例如，加速度计）转移到目标领域（例如，Apple Watch）。我们不是直接处理原始的时间序列信号，而是使用时间序列基础模型（TSFMs）将两个领域投影到一个共享的潜在嵌入空间，并开发了一种新的框架以对齐设备间的表示。我们的方法，TSFM嵌入的对抗对齐，强制源和目标嵌入的分布在这个空间内对齐，从而促进不同设备类型之间的标签转移。 

---
# VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving 

**Title (ZH)**: VoltanaLLM：基于反馈的频率控制和状态空间路由的高效LLM服务 

**Authors**: Jiahuan Yu, Aryan Taneja, Junfeng Lin, Minjia Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.04827)  

**Abstract**: Modern Large Language Model (LLM) serving systems increasingly support interactive applications, like real-time chat assistants, code generation tools, and agentic workflows. However, the soaring energy cost of LLM inference presents a growing challenge for sustainable and cost-effective deployment. This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM serving, built from a control theory perspective. VoltanaLLM co-designs frequency scaling and request routing in emerging prefill/decode disaggregated architectures, leveraging their decoupled execution to enable fine-grained phase-specific control. It consists of a feedback-driven frequency controller that dynamically adapts GPU frequency for prefill and decode phases, and a state-space router that explores routing decisions across frequency-scaled instances to minimize energy under latency constraints. We implement VoltanaLLM in SGLang and evaluate its performance over multiple state-of-the-art LLMs and real-world datasets. The results demonstrate that VoltanaLLM achieves up to 36.3% energy savings while maintaining near-perfect SLO attainment rate, paving the way for sustainable and intelligent LLM serving. 

**Abstract (ZH)**: 基于控制理论的SLO感知高效能Large Language Model服务系统VoltanaLLM及其能量效益分析 

---
# Estimating forest carbon stocks from high-resolution remote sensing imagery by reducing domain shift with style transfer 

**Title (ZH)**: 基于样式迁移减少领域偏移的高分辨率遥感影像森林碳储量估算 

**Authors**: Zhenyu Yu, Jinnian Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.00784)  

**Abstract**: Forests function as crucial carbon reservoirs on land, and their carbon sinks can efficiently reduce atmospheric CO2 concentrations and mitigate climate change. Currently, the overall trend for monitoring and assessing forest carbon stocks is to integrate ground monitoring sample data with satellite remote sensing imagery. This style of analysis facilitates large-scale observation. However, these techniques require improvement in accuracy. We used GF-1 WFV and Landsat TM images to analyze Huize County, Qujing City, Yunnan Province in China. Using the style transfer method, we introduced Swin Transformer to extract global features through attention mechanisms, converting the carbon stock estimation into an image translation. 

**Abstract (ZH)**: 森林作为陆地上的关键碳汇，其碳汇功能能有效降低大气中的CO2浓度并缓解气候变化。目前，监测和评估森林碳储量的整体趋势是将地面监测样本数据与卫星遥感图像相结合。这种分析方式有助于大规模观测，但这些技术在准确性上仍需要改进。我们使用GF-1 WFV和Landsat TM影像对中国云南省曲靖市Huize县进行分析，采用风格迁移方法，引入Swin Transformer通过注意力机制提取全局特征，将碳储量估算转化为图像翻译。 

---
