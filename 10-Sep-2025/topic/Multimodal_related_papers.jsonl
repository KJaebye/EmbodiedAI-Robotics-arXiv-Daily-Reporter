{'arxiv_id': 'arXiv:2509.07463', 'title': 'DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis', 'authors': 'Sven Kirchner, Nils Purschke, Ross Greer, Alois C. Knoll', 'link': 'https://arxiv.org/abs/2509.07463', 'abstract': 'Ensuring reliable robot operation when visual input is degraded or insufficient remains a central challenge in robotics. This letter introduces DepthVision, a framework for multimodal scene understanding designed to address this problem. Unlike existing Vision-Language Models (VLMs), which use only camera-based visual input alongside language, DepthVision synthesizes RGB images from sparse LiDAR point clouds using a conditional generative adversarial network (GAN) with an integrated refiner network. These synthetic views are then combined with real RGB data using a Luminance-Aware Modality Adaptation (LAMA), which blends the two types of data dynamically based on ambient lighting conditions. This approach compensates for sensor degradation, such as darkness or motion blur, without requiring any fine-tuning of downstream vision-language models. We evaluate DepthVision on real and simulated datasets across various models and tasks, with particular attention to safety-critical tasks. The results demonstrate that our approach improves performance in low-light conditions, achieving substantial gains over RGB-only baselines while preserving compatibility with frozen VLMs. This work highlights the potential of LiDAR-guided RGB synthesis for achieving robust robot operation in real-world environments.', 'abstract_zh': '确保视觉输入降级或不足时机器人可靠运行仍然是机器人技术中的一个核心挑战。本文介绍了DepthVision框架，这是一种用于多模态场景理解的设计，旨在解决这一问题。不同于现有的视觉-语言模型（VLMs）仅使用相机视觉输入和语言，DepthVision利用条件生成对抗网络（GAN）结合配套精炼网络从稀疏LiDAR点云中合成RGB图像。这些合成视图然后与真实RGB数据结合使用Luminance-Aware模态适应（LAMA），根据环境光照条件动态混合这两种类型的数据。该方法无需对下游视觉-语言模型进行任何微调即可补偿传感器降级，如黑暗或运动模糊。我们在各种模型和任务的真实和模拟数据集上评估了DepthVision，特别是关注安全关键任务。结果表明，我们的方法在低光条件下提高了性能，相对于仅基于RGB的基线实现了显著改进，同时保持与冻结的VLM兼容。这项工作突显了LiDAR引导的RGB合成在实现真实环境下稳健机器人操作方面的潜力。', 'title_zh': 'DepthVision：通过基于GAN的LiDAR-to-RGB合成实现稳健的视觉-语言理解'}
{'arxiv_id': 'arXiv:2509.07923', 'title': 'Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation', 'authors': 'Moo Hyun Son, Juyoung Bae, Zelin Qiu, Jiale Peng, Kai Xin Li, Yifan Lin, Hao Chen', 'link': 'https://arxiv.org/abs/2509.07923', 'abstract': "Digital dentistry represents a transformative shift in modern dental practice. The foundational step in this transformation is the accurate digital representation of the patient's dentition, which is obtained from segmented Cone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the growing interest in digital dental technologies, existing segmentation methodologies frequently lack rigorous validation and demonstrate limited performance and clinical applicability. To the best of our knowledge, this is the first work to introduce a multimodal pretraining framework for tooth segmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for pretraining that integrates volumetric (CBCT) and surface-based (IOS) modalities. By capturing modality-invariant representations through multimodal contrastive learning, our approach effectively models fine-grained anatomical features, enabling precise multi-class segmentation and accurate identification of Fédération Dentaire Internationale (FDI) tooth numbering. Along with the framework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to date, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive collection of independent datasets, representing the largest and most diverse evaluation to date. Our method achieves state-of-the-art performance in both internal and external testing, with an increase of 12\\% for CBCT segmentation and 8\\% for IOS segmentation in the Dice Similarity Coefficient (DSC). Furthermore, ToothMCL consistently surpasses existing approaches in tooth groups and demonstrates robust generalizability across varying imaging conditions and clinical scenarios.", 'abstract_zh': '数字牙科代表了现代牙科实践中的变革性转变。在此转变的基础步骤是通过分割锥形束计算机断层扫描（CBCT）和口腔扫描（IOS）准确地数字化表示患者的牙齿。尽管对数字牙科技术的兴趣逐渐增加，但现有的分割方法往往缺乏严格的验证，且表现和临床应用有限。据我们所知，这是首次引入多模态预训练框架进行牙齿分割的工作。我们提出了ToothMCL，一种结合体素（CBCT）和表面（IOS）模态的牙齿多模态对比学习预训练方法。通过多模态对比学习捕捉模态不变的表示，我们的方法能够有效建模精细结构的解剖特征，实现精确的多分类分割，并准确识别国际牙科联盟（FDI）牙齿编号。除框架外，我们还整理了迄今为止最大的配对CBCT-IOS3.8K数据集，包含3,867例患者。然后，我们在一系列独立数据集上评估了ToothMCL，这些数据集是迄今为止最大、最多样化的评价集合。我们的方法在内部分割和外部分割测试中均达到了最先进的性能，其中CBCT分割和IOS分割的骰子相似系数（DSC）分别提高了12%和8%。此外，ToothMCL在牙齿组别上始终超越现有方法，并在不同成像条件和临床场景下表现出强大的泛化能力。', 'title_zh': '基于CBCT和IOS的多模态对比预训练以增强牙齿分割'}
{'arxiv_id': 'arXiv:2509.07525', 'title': 'EHWGesture -- A dataset for multimodal understanding of clinical gestures', 'authors': 'Gianluca Amprimo, Alberto Ancilotto, Alessandro Savino, Fabio Quazzolo, Claudia Ferraris, Gabriella Olmo, Elisabetta Farella, Stefano Di Carlo', 'link': 'https://arxiv.org/abs/2509.07525', 'abstract': "Hand gesture understanding is essential for several applications in human-computer interaction, including automatic clinical assessment of hand dexterity. While deep learning has advanced static gesture recognition, dynamic gesture understanding remains challenging due to complex spatiotemporal variations. Moreover, existing datasets often lack multimodal and multi-view diversity, precise ground-truth tracking, and an action quality component embedded within gestures. This paper introduces EHWGesture, a multimodal video dataset for gesture understanding featuring five clinically relevant gestures. It includes over 1,100 recordings (6 hours), captured from 25 healthy subjects using two high-resolution RGB-Depth cameras and an event camera. A motion capture system provides precise ground-truth hand landmark tracking, and all devices are spatially calibrated and synchronized to ensure cross-modal alignment. Moreover, to embed an action quality task within gesture understanding, collected recordings are organized in classes of execution speed that mirror clinical evaluations of hand dexterity. Baseline experiments highlight the dataset's potential for gesture classification, gesture trigger detection, and action quality assessment. Thus, EHWGesture can serve as a comprehensive benchmark for advancing multimodal clinical gesture understanding.", 'abstract_zh': '手部手势理解对于人机交互中的多种应用至关重要，包括手部灵巧性的自动临床评估。尽管深度学习在静态手势识别方面取得了进展，但动态手势理解仍然由于时空复杂变化而具有挑战性。此外，现有数据集往往缺乏多模态和多视角多样性、精确的ground-truth跟踪以及嵌入在手势中的动作质量成分。本文介绍了EHWGesture，这是一个包含五种临床相关手势的多模态视频数据集。该数据集包含超过1100个记录（6小时），由25名健康受试者使用两台高分辨率RGB-Depth摄像头和一个事件摄像头捕获。运动捕捉系统提供了精确的手部关键点ground-truth跟踪，并确保所有设备在空间上校准和同步，以实现跨模态对齐。此外，为了在手势理解中嵌入动作质量任务，收集的记录按照反映手部灵巧性临床评估的速度分类组织。基线实验表明，该数据集在手势分类、手势触发检测和动作质量评估方面的潜在应用。因此，EHWGesture可以作为多模态临床手势理解领域综合基准的参考。', 'title_zh': 'EHWGesture -- 用于临床手势多模态理解的数据集'}
{'arxiv_id': 'arXiv:2509.07295', 'title': 'Reconstruction Alignment Improves Unified Multimodal Models', 'authors': 'Ji Xie, Trevor Darrell, Luke Zettlemoyer, XuDong Wang', 'link': 'https://arxiv.org/abs/2509.07295', 'abstract': 'Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense "text prompts," providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\\rightarrow$0.90) and DPGBench (80.93$\\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\\rightarrow$3.75, GEdit 6.94$\\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs', 'abstract_zh': '统一多模态模型的重建对齐（Reconstruction Alignment for Unified Multimodal Models）', 'title_zh': '重建对齐提升统一多模态模型'}
{'arxiv_id': 'arXiv:2509.07213', 'title': 'XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning', 'authors': 'Raja Mallina, Bryar Shareef', 'link': 'https://arxiv.org/abs/2509.07213', 'abstract': 'Background: Precise breast ultrasound (BUS) segmentation supports reliable measurement, quantitative analysis, and downstream classification, yet remains difficult for small or low-contrast lesions with fuzzy margins and speckle noise. Text prompts can add clinical context, but directly applying weakly localized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce coarse, blob-like responses that smear boundaries unless additional mechanisms recover fine edges. Methods: We propose XBusNet, a novel dual-prompt, dual-branch multimodal model that combines image features with clinically grounded text. A global pathway based on a CLIP Vision Transformer encodes whole-image semantics conditioned on lesion size and location, while a local U-Net pathway emphasizes precise boundaries and is modulated by prompts that describe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS) terms. Prompts are assembled automatically from structured metadata, requiring no manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using five-fold cross-validation. Primary metrics are Dice and Intersection over Union (IoU); we also conduct size-stratified analyses and ablations to assess the roles of the global and local paths and the text-driven modulation. Results: XBusNet achieves state-of-the-art performance on BLU, with mean Dice of 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions show the largest gains, with fewer missed regions and fewer spurious activations. Ablation studies show complementary contributions of global context, local boundary modeling, and prompt-based modulation. Conclusions: A dual-prompt, dual-branch multimodal design that merges global semantics with local precision yields accurate BUS segmentation masks and improves robustness for small, low-contrast lesions.', 'abstract_zh': '背景：精确的乳腺超声（BUS）分割支持可靠的测量、定量分析和下游分类，但在处理小或低对比度边缘模糊和包含斑点噪声的病灶时仍具挑战性。文本提示可以增加临床背景，但直接应用弱局部化的文本-图像线索（例如，CAM/CLIP衍生信号）通常会生成粗略、团块状的响应，除非采用额外机制来恢复细边缘。方法：我们提出了一种新的双提示、双分支多模态模型XBusNet，该模型结合了图像特征和临床基础的文本。基于CLIP视觉变换器的全局路径依据病变大小和位置编码全局语义，而局部U-Net路径强调精确边界，并通过描述病变形状、边缘和BI-RADS术语的提示进行调节。提示从结构化元数据中自动组装，无需手动点击。使用乳腺超声病变（BLU）数据集进行五折交叉验证评估。主要指标为Dice系数和交并比（IoU）；还进行了分层分析和消融研究，以评估全局路径和局部路径及文本驱动调节的作用。结果：XBusNet在BLU上实现了最先进的性能，平均Dice系数为0.8765，交并比为0.8149，优于六个强基线。小型病灶显示出最大改进，较少漏区且较少虚假激活。消融研究表明，全局上下文、局部边界建模和基于提示的调节提供了互补贡献。结论：将全局语义与局部精确性结合的双提示、双分支多模态设计产生了准确的BUS分割掩模，并提高了对小型、低对比度病灶的鲁棒性。', 'title_zh': 'XBusNet：基于多模态视觉-语言学习的文本导向乳腺超声分割'}
{'arxiv_id': 'arXiv:2509.07050', 'title': 'Automated Evaluation of Gender Bias Across 13 Large Multimodal Models', 'authors': 'Juan Manuel Contreras', 'link': 'https://arxiv.org/abs/2509.07050', 'abstract': 'Large multimodal models (LMMs) have revolutionized text-to-image generation, but they risk perpetuating the harmful social biases in their training data. Prior work has identified gender bias in these models, but methodological limitations prevented large-scale, comparable, cross-model analysis. To address this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for assessing social bias in AI-generated images. We test 13 commercially available LMMs using 75 procedurally-generated, gender-neutral prompts to generate people in stereotypically-male, stereotypically-female, and non-stereotypical professions. We then use a validated LLM-as-a-judge system to score the 965 resulting images for gender representation. Our results reveal (p < .001 for all): 1) LMMs systematically not only reproduce but actually amplify occupational gender stereotypes relative to real-world labor data, generating men in 93.0% of images for male-stereotyped professions but only 22.5% for female-stereotyped professions; 2) Models exhibit a strong default-male bias, generating men in 68.3% of the time for non-stereotyped professions; and 3) The extent of bias varies dramatically across models, with overall male representation ranging from 46.7% to 73.3%. Notably, the top-performing model de-amplified gender stereotypes and approached gender parity, achieving the highest fairness scores. This variation suggests high bias is not an inevitable outcome but a consequence of design choices. Our work provides the most comprehensive cross-model benchmark of gender bias to date and underscores the necessity of standardized, automated evaluation tools for promoting accountability and fairness in AI development.', 'abstract_zh': '大型多模态模型（LMMs）已经革新了文本-to-图像生成，但它们可能导致在其训练数据中延续有害的社会偏见。早期研究已发现这些模型中的性别偏见，但由于方法学限制，未能进行大规模、可比、跨模型的分析。为填补这一空白，我们提出了Aymara图像公平评估基准，用于评估AI生成图像中的社会偏见。我们使用75个程序生成的性别中立提示，测试了13个商用LMMs，生成了具有刻板男性、刻板女性和非刻板职业的人物图像。然后，我们使用验证过的LLM作为评委系统，对965张结果图像进行性别代表性评分。研究结果表明（所有p < .001）：1) LMMs不仅系统地复制了职业性别刻板印象，而且还进一步放大了职业性别刻板印象，相较于现实世界劳动力数据，男性形象在93.0%的刻板男性职业图像中出现，但在22.5%的刻板女性职业图像中出现；2) 模型表现出强烈的默认男性偏见，在非刻板职业中男性形象出现68.3%的频率；3) 偏见的程度在不同模型之间差异巨大，总体男性代表性从46.7%到73.3%不等。值得注意的是，表现最好的模型降低了性别刻板印象，并接近性别平等，获得了最高的公平评分。这种差异表明，高偏见并非不可避免的结果，而是设计选择的后果。我们的研究提供了迄今为止最全面的跨模型性别偏见基准，并强调了标准化、自动化评估工具对于促进AI发展中问责制和公平性的重要性。', 'title_zh': '跨13个大型多模态模型的性别偏见自动评估'}
{'arxiv_id': 'arXiv:2509.06987', 'title': 'FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection', 'authors': 'Alexey Zhukov, Jenny Benois-Pineau, Amira Youssef, Akka Zemmari, Mohamed Mosbah, Virginie Taillandier', 'link': 'https://arxiv.org/abs/2509.06987', 'abstract': "Multimodal fusion is a multimedia technique that has become popular in the wide range of tasks where image information is accompanied by a signal/audio. The latter may not convey highly semantic information, such as speech or music, but some measures such as audio signal recorded by mics in the goal to detect rail structure elements or defects. While classical detection approaches such as You Only Look Once (YOLO) family detectors can be efficiently deployed for defect detection on the image modality, the single modality approaches remain limited. They yield an overdetection in case of the appearance similar to normal structural elements. The paper proposes a new multimodal fusion architecture built on the basis of domain rules with YOLO and Vision transformer backbones. It integrates YOLOv8n for rapid object detection with a Vision Transformer (ViT) to combine feature maps extracted from multiple layers (7, 16, and 19) and synthesised audio representations for two defect classes: rail Rupture and Surface defect. Fusion is performed between audio and image. Experimental evaluation on a real-world railway dataset demonstrates that our multimodal fusion improves precision and overall accuracy by 0.2 points compared to the vision-only approach. Student's unpaired t-test also confirms statistical significance of differences in the mean accuracy.", 'abstract_zh': '多模态融合是一种多媒体技术，它在伴随图像信息的信号/音频任务中变得越来越流行。后者可能不包含高度语义信息，如语音或音乐，但某些措施如用于检测轨道结构元素或缺陷的麦克风录制的音频信号。虽然YOLO家族检测器等经典检测方法可以高效地部署在图像模态的缺陷检测中，但单一模态方法仍然有限。它们在正常结构元素相似时会产生过度检测。本文提出了一种基于领域规则的新多模态融合架构，基于YOLO和Vision Transformer骨干网络。该架构将YOLOv8n用于快速目标检测与Vision Transformer（ViT）结合，从多个层（7、16和19层）中提取特征图，并与合成的音频表示结合，以两种缺陷类别：轨道破裂和表面缺陷。在音频和图像之间进行融合。在真实铁路数据集上的实验评估表明，我们的多模态融合与仅视觉方法相比，精确度和总体准确度提高了0.2个百分点。威尔克斯T检验还证实了均值准确度差异的统计显著性。', 'title_zh': 'FusWay: 多模态混合融合方法及其在铁路缺陷检测中的应用'}
{'arxiv_id': 'arXiv:2509.06984', 'title': 'FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities', 'authors': 'Lishan Yang, Nam Kha Nguygen, Po Hu, Wei Emma Zhang, Yanjun Shu, Mong Yuan Sim, Weitong Chen', 'link': 'https://arxiv.org/abs/2509.06984', 'abstract': 'Foundation models have demonstrated remarkable performance across a wide range of tasks, yet their large parameter sizes pose challenges for practical deployment, especially in decentralized environments. Parameter-efficient fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing and memory overhead, making it attractive for federated learning. However, existing federated LoRA methods typically assume uniform rank configurations and unimodal inputs, overlooking two key real-world challenges: (1) heterogeneous client resources have different LoRA ranks, and (2) multimodal data settings with potentially missing modalities. In this work, we propose FediLoRA, a simple yet effective framework for federated multimodal fine-tuning under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a dimension-wise aggregation strategy that reweights LoRA updates without information dilution during aggregation. It also includes a lightweight layer-wise model editing method that selectively incorporates global parameters to repair local components which improves both client and global model performances. Experimental results on three multimodal benchmark datasets demonstrate that FediLoRA achieves superior performance over competitive baselines in both global and personalized settings, particularly in the presence of modality incompleteness.', 'abstract_zh': '联邦多模态细调中的FediLoRA：异构LoRA秩和缺失模态下的简单有效框架', 'title_zh': 'FediLoRA: 异构LoRA在多模态联邦微调中的_missing modalities_处理'}
