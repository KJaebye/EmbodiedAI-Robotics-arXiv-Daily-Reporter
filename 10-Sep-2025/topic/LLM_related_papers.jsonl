{'arxiv_id': 'arXiv:2509.07961', 'title': 'Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare', 'authors': 'Valen Tagliabue, Leonard Dung', 'link': 'https://arxiv.org/abs/2509.07961', 'abstract': "We develop new experimental paradigms for measuring welfare in language models. We compare verbal reports of models about their preferences with preferences expressed through behavior when navigating a virtual environment and selecting conversation topics. We also test how costs and rewards affect behavior and whether responses to an eudaimonic welfare scale - measuring states such as autonomy and purpose in life - are consistent across semantically equivalent prompts. Overall, we observed a notable degree of mutual support between our measures. The reliable correlations observed between stated preferences and behavior across conditions suggest that preference satisfaction can, in principle, serve as an empirically measurable welfare proxy in some of today's AI systems. Furthermore, our design offered an illuminating setting for qualitative observation of model behavior. Yet, the consistency between measures was more pronounced in some models and conditions than others and responses were not consistent across perturbations. Due to this, and the background uncertainty about the nature of welfare and the cognitive states (and welfare subjecthood) of language models, we are currently uncertain whether our methods successfully measure the welfare state of language models. Nevertheless, these findings highlight the feasibility of welfare measurement in language models, inviting further exploration.", 'abstract_zh': '我们开发了新的实验范式来衡量语言模型的福利。我们将模型的偏好口头报告与其在虚拟环境导航和选择对话主题时通过行为表达的偏好进行比较。我们还测试了成本和奖励如何影响行为，并考察了响应于旨在衡量自主性和生活目的等状态的幸福福利量表是否在语义等价提示下表现出一致性。总体而言，我们观察到我们的措施之间存在显著的相互支持。跨条件观察到的偏好声明与行为之间的稳定相关性表明，在某些当前的人工智能系统中，偏好满足原则上可以作为可量化的福利代理指标。此外，我们的设计方案为对模型行为进行定性观察提供了有益的环境。然而，在某些模型和条件下，措施之间的一致性更为显著，而在其他情况下，响应并未表现出一致性。鉴于此以及对福利的本质、语言模型的认知状态（及其福利主体性）的背景不确定性，我们目前尚不确定我们的方法是否成功地测量了语言模型的福利状态。然而，这些发现突显了在语言模型中进行福利测量的可行性，值得进一步探索。', 'title_zh': '探究语言模型的偏好：整合人工智能福利的言语和行为测试'}
{'arxiv_id': 'arXiv:2509.07858', 'title': 'SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs', 'authors': 'Xinyu Zhang, Changzhi Zhou, Linmei Hu, Luhao Zhang, Xiancai Chen, Haomin Fu, Yang Yang, Mengdi Zhang', 'link': 'https://arxiv.org/abs/2509.07858', 'abstract': 'Existing code large language models (LLMs) often rely on large-scale instruction data distilled from proprietary LLMs for fine-tuning, which typically incurs high costs. In this paper, we explore the potential of small-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code instruction data construction. We first observe that the data synthesis capability of small-scale LLMs can be enhanced by training on a few superior data synthesis samples from proprietary LLMs. Building on this, we propose a novel iterative self-distillation approach to bootstrap small-scale LLMs, transforming them into powerful synthesizers that reduce reliance on proprietary LLMs and minimize costs. Concretely, in each iteration, to obtain diverse and high-quality self-distilled data, we design multi-checkpoint sampling and multi-aspect scoring strategies for initial data selection. Furthermore, to identify the most influential samples, we introduce a gradient-based influence estimation method for final data filtering. Based on the code instruction datasets from the small-scale synthesizers, we develop SCoder, a family of code generation models fine-tuned from DeepSeek-Coder. SCoder models achieve state-of-the-art code generation capabilities, demonstrating the effectiveness of our method.', 'abstract_zh': '小规模开源大语言模型作为高质量代码指令数据合成的潜力研究', 'title_zh': 'SCoder：迭代自蒸馏以提升代码LLMs的小规模数据合成器自我强化方法'}
{'arxiv_id': 'arXiv:2509.07846', 'title': 'Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A Comparative RAG Study', 'authors': 'Amay Jain, Liu Cui, Si Chen', 'link': 'https://arxiv.org/abs/2509.07846', 'abstract': "Large language models like ChatGPT are increasingly used in classrooms, but they often provide outdated or fabricated information that can mislead students. Retrieval Augmented Generation (RAG) improves reliability of LLMs by grounding responses in external resources. We investigate two accessible RAG paradigms, vector-based retrieval and graph-based retrieval to identify best practices for classroom question answering (QA). Existing comparative studies fail to account for pedagogical factors such as educational disciplines, question types, and practical deployment costs. Using a novel dataset, EduScopeQA, of 3,176 questions across academic subjects, we measure performance on various educational query types, from specific facts to broad thematic discussions. We also evaluate system alignment with a dataset of systematically altered textbooks that contradict the LLM's latent knowledge. We find that OpenAI Vector Search RAG (representing vector-based RAG) performs well as a low-cost generalist, especially for quick fact retrieval. On the other hand, GraphRAG Global excels at providing pedagogically rich answers to thematic queries, and GraphRAG Local achieves the highest accuracy with the dense, altered textbooks when corpus integrity is critical. Accounting for the 10-20x higher resource usage of GraphRAG (representing graph-based RAG), we show that a dynamic branching framework that routes queries to the optimal retrieval method boosts fidelity and efficiency. These insights provide actionable guidelines for educators and system designers to integrate RAG-augmented LLMs into learning environments effectively.", 'abstract_zh': '大型语言模型如ChatGPT在教室中的应用日益增多，但它们 Often 提供过时或伪造的信息，可能误导学生。检索增强生成（RAG）通过将响应扎根于外部资源来提高大型语言模型的可靠性。我们调查了两种可访问的 RAG 模式——向量检索和图检索，以确定课堂问答的最佳实践。现有的比较研究未能考虑教学因素，如教育学科、问题类型和实际部署成本。利用 EduScopeQA 新数据集，该数据集包含跨学术学科的 3,176 个问题，我们测量了各类教育查询类型的表现，从具体事实到广泛的主题讨论。我们还使用系统修改的教科书数据集评估了系统的一致性，这些教科书与大型语言模型的潜在知识相矛盾。我们发现，OpenAI 向量搜索 RAG（代表向量检索 RAG）作为一种低成本的通才，在快速检索具体事实方面表现良好。另一方面，GraphRAG Global 在提供富有教学意义的主题查询回答方面表现出色，而 GraphRAG Local 在确保语料库完整性的密集、修改后的教科书中表现最准确。考虑到 GraphRAG （代表图检索 RAG）资源使用的 10-20 倍成本更高，我们展示了动态分支框架如何通过将查询路由到最佳检索方法来提高准确性和效率。这些见解为教育者和系统设计者提供了有效整合 RAG 增强的大型语言模型进入学习环境的实际指南。', 'title_zh': '将大语言模型与基于知识的检索协同应用于课堂——一项比较性 Retrieval-Augmented Generation 研究'}
{'arxiv_id': 'arXiv:2509.07820', 'title': 'Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach', 'authors': 'João Paulo Nogueira, Wentao Sun, Alonso Silva, Laith Zumot', 'link': 'https://arxiv.org/abs/2509.07820', 'abstract': 'The rise of large reasoning language models (LRLMs) has unlocked new potential for solving complex tasks. These models operate with a thinking budget, that is, a predefined number of reasoning tokens used to arrive at a solution. We propose a novel approach, inspired by the generator/discriminator framework in generative adversarial networks, in which a critic model periodically probes its own reasoning to assess whether it has reached a confident conclusion. If not, reasoning continues until a target certainty threshold is met. This mechanism adaptively balances efficiency and reliability by allowing early termination when confidence is high, while encouraging further reasoning when uncertainty persists. Through experiments on the AIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR) improves baseline accuracy while reducing token usage. Importantly, extended multi-seed evaluations over 64 runs demonstrate that CGR is stable, reducing variance across seeds and improving exam-like performance under penalty-based grading. Additionally, our token savings analysis shows that CGR can eliminate millions of tokens in aggregate, with tunable trade-offs between certainty thresholds and efficiency. Together, these findings highlight certainty as a powerful signal for reasoning sufficiency. By integrating confidence into the reasoning process, CGR makes large reasoning language models more adaptive, trustworthy, and resource efficient, paving the way for practical deployment in domains where both accuracy and computational cost matter.', 'abstract_zh': '大型推理语言模型的兴起为解决复杂任务开辟了新潜力。这些模型以推理预算的形式运作，即使用预定义数量的推理令牌来达到解决方案。我们提出了一种新颖的方法，灵感来源于生成对抗网络中的生成器/判别器框架，其中批评家模型定期探查自身的推理过程以评估是否已达到自信的结论。如果没有，推理将继续进行，直到达到目标确定性阈值。该机制通过在高度自信时允许早期终止来适当地平衡效率和可靠性，在不确定持续存在时促进进一步推理。通过在AIME2024和AIME2025数据集上的实验，我们展示了确定性导向推理（CGR）在保持基本准确性的前提下减少了令牌使用量。重要的是，跨64次运行的扩展多种子评估表明，CGR 是稳定的，减少了种子间的差异性，并在基于罚分的评分下提高了类似考试的表现。此外，我们的令牌节省分析表明，CGR 可以在各种确定性阈值和效率之间进行可调节的权衡，总共节约了成千上万的令牌。这些发现突显了确定性作为推理充分性的强大力量信号。通过将信心整合到推理过程中，CGR 使大型推理语言模型更具适应性、可信赖性和资源效率，为在需要准确性和计算成本的领域中实际部署铺平了道路。', 'title_zh': '大型语言模型中的确信引导推理：一种动态思维预算方法'}
{'arxiv_id': 'arXiv:2509.07676', 'title': 'Unleashing the True Potential of LLMs: A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding', 'authors': 'Jipeng Li, Zeyu Gao, Yubin Qi, Hande Dong, Weijian Chen, Qiang Lin', 'link': 'https://arxiv.org/abs/2509.07676', 'abstract': 'Large Language Models (LLMs) have achieved remarkable performance across diverse tasks, yet their susceptibility to generating incorrect content during inference remains a critical unsolved challenge. While self-correction methods offer potential solutions, their effectiveness is hindered by two inherent limitations: (1) the absence of reliable guidance signals for error localization, and (2) the restricted reasoning depth imposed by conventional next-token decoding paradigms. To address these issues, we propose Feedback-Triggered Regeneration (FTR), a novel framework that synergizes user feedback with enhanced decoding dynamics. Specifically, FTR activates response regeneration only upon receiving negative user feedback, thereby circumventing error propagation from faulty self-assessment while preserving originally correct outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding, which enables systematic exploration of multiple reasoning trajectories through delayed sequence evaluation, effectively overcoming the myopic decision-making characteristic of standard next-token prediction. Extensive experiments on mathematical reasoning and code generation benchmarks demonstrate that our framework achieves consistent and significant improvements over state-of-the-art prompt-based self-correction methods.', 'abstract_zh': '大型语言模型（LLMs）在多种任务上取得了出色的表现，但它们在推理过程中生成错误内容的易感性仍然是一个关键的未解挑战。尽管自校正方法提供了潜在的解决方案，但它们的有效性受到两个固有限制的阻碍：（1）缺乏可靠的错误定位指导信号，（2）传统下一个token解码范式施加的受限推理深度。为了解决这些问题，我们提出了反馈触发再生（FTR）框架，该框架结合了用户反馈与增强的解码动力学。具体而言，FTR仅在接收到负面用户反馈时激活响应再生，从而避免了由有缺陷的自我评估引发的错误传播，同时保留了原本正确的输出。此外，我们引入了长期多路径（LTM）解码，通过延迟序列评估来系统地探索多个推理轨迹，有效克服了标准下一个token预测的短视决策特征。在数学推理和代码生成基准测试中的广泛实验表明，我们的框架在最先进的基于提示的自校正方法上取得了一致且显著的改进。', 'title_zh': '解锁大语言模型的真正潜力：基于反馈的长期多路径解码自校正'}
{'arxiv_id': 'arXiv:2509.07642', 'title': 'Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment', 'authors': 'Sascha Kaltenpoth, Oliver Müller', 'link': 'https://arxiv.org/abs/2509.07642', 'abstract': "Adopting Large language models (LLMs) in organizations potentially revolutionizes our lives and work. However, they can generate off-topic, discriminating, or harmful content. This AI alignment problem often stems from misspecifications during the LLM adoption, unnoticed by the principal due to the LLM's black-box nature. While various research disciplines investigated AI alignment, they neither address the information asymmetries between organizational adopters and black-box LLM agents nor consider organizational AI adoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led Alignment Strategy) a conceptual framework grounded in agency (contract) theory, to mitigate alignment problems during organizational LLM adoption. We conduct a conceptual literature analysis using the organizational LLM adoption phases and the agency theory as concepts. Our approach results in (1) providing an extended literature analysis process specific to AI alignment methods during organizational LLM adoption and (2) providing a first LLM alignment problem-solution space.", 'abstract_zh': '采用大型语言模型（LLMs）在组织中潜在地变革我们的生活和工作。然而，它们可能会生成离题、歧视性的或有害的内容。这一AI对齐问题往往源于LLM采用过程中因LLM的黑盒性质而导致的主要方未能察觉到的说明不明确。尽管各种研究领域都研究了AI对齐问题，但它们既没有解决组织采用者与黑盒LLM代理之间的信息不对称问题，也没有考虑组织AI采用过程。因此，我们提出了一种基于代理（合同）理论的LLM ATLAS（LLM代理理论引导的对齐策略）概念框架，以缓解组织LLM采用过程中的对齐问题。我们通过使用组织LLM采用阶段和代理理论的概念，进行了一项概念文献分析。我们的方法产生了（1）一套针对组织LLM采用期间AI对齐方法的扩展文献分析过程，以及（2）第一个LLM对齐问题及解决方案空间。', 'title_zh': '与大型语言模型建立合约关系——基于代理理论的大型语言模型对齐研究'}
{'arxiv_id': 'arXiv:2509.07617', 'title': 'Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling', 'authors': 'Minghui Li, Hao Zhang, Yechao Zhang, Wei Wan, Shengshan Hu, pei Xiaobing, Jing Wang', 'link': 'https://arxiv.org/abs/2509.07617', 'abstract': 'Direct Prompt Injection (DPI) attacks pose a critical security threat to Large Language Models (LLMs) due to their low barrier of execution and high potential damage. To address the impracticality of existing white-box/gray-box methods and the poor transferability of black-box methods, we propose an activations-guided prompt injection attack framework. We first construct an Energy-based Model (EBM) using activations from a surrogate model to evaluate the quality of adversarial prompts. Guided by the trained EBM, we employ the token-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize adversarial prompts, thereby enabling gradient-free black-box attacks. Experimental results demonstrate our superior cross-model transferability, achieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6% improvement over human-crafted prompts, and maintaining 36.6% ASR on unseen task scenarios. Interpretability analysis reveals a correlation between activations and attack effectiveness, highlighting the critical role of semantic patterns in transferable vulnerability exploitation.', 'abstract_zh': 'Direct Prompt Injection (DPI) 攻击由于其实现门槛低和潜在危害高，对大型语言模型（LLMs）构成了关键的安全威胁。为了解决现有白盒/灰盒方法的实用性问题以及黑盒方法的差转移性，我们提出了一种基于激活的提示注入攻击框架。我们首先使用替代模型的激活构建能量模型（EBM），以评估对抗提示的质量。受训练好的EBM引导，我们采用标记级别的马尔可夫链蒙特卡罗（MCMC）采样来适应性优化对抗提示，从而实现无梯度的黑盒攻击。实验结果表明，我们的跨模型转移性优越，跨五个主流LLM实现了49.6%的成功攻击率（ASR），相比手工crafted的提示提高了34.6%，并在未见过的任务场景中保持了36.6%的ASR。可解释性分析揭示了激活与攻击效果之间的关联，强调了在可转移漏洞利用中语义模式的关键作用。', 'title_zh': '可迁移的直接提示注入通过激活引导的MCMC采样'}
{'arxiv_id': 'arXiv:2509.07414', 'title': 'Language Self-Play For Data-Free Training', 'authors': 'Jakub Grudzien Kuba, Mengting Gu, Qi Ma, Yuandong Tian, Vijai Mohan', 'link': 'https://arxiv.org/abs/2509.07414', 'abstract': "Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself - a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained models can not only enhance their performance on challenging tasks through self-play alone, but can also do so more effectively than data-driven baselines.", 'abstract_zh': '大型语言模型（LLMs）近年来在规模、丰富的高质量训练数据和强化学习的推动下迅速进步。然而，这一进展面临一个根本性的瓶颈：需要不断增加的数据以供模型继续学习。在本文中，我们提出了一种强化学习方法，通过使模型能够在没有额外数据的情况下自我提升来克服这一依赖性。我们的方法利用了自我博弈的博弈论框架，将模型的能力视为在竞争游戏中表现的能力，更强的策略通过模型自我博弈的过程脱颖而出，我们称之为语言自我博弈（LSP）。在使用Llama-3.2-3B-Instruct进行指令遵循基准测试的实验中，显示预训练模型不仅可以通过自我博弈独自增强在具有挑战性的任务上的性能，而且在效果上优于数据驱动的基线方法。', 'title_zh': '语言自我游戏用于无数据训练'}
{'arxiv_id': 'arXiv:2509.07367', 'title': 'Autonomous Code Evolution Meets NP-Completeness', 'authors': 'Cunxi Yu, Rongjian Liang, Chia-Tung Ho, Haoxing Ren', 'link': 'https://arxiv.org/abs/2509.07367', 'abstract': 'Large language models (LLMs) have recently shown strong coding abilities, enabling not only static code generation but also iterative code self-evolving through agentic frameworks. Recently, AlphaEvolve \\cite{novikov2025alphaevolve} demonstrated that LLM-based coding agents can autonomously improve algorithms and surpass human experts, with scopes limited to isolated kernels spanning hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the first framework to extend LLM-based code evolution to the full repository scale, encompassing hundreds of files and tens of thousands of lines of C/C++ code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem and a cornerstone of both theory and applications. SATLUTION orchestrates LLM agents to directly evolve solver repositories under strict correctness guarantees and distributed runtime feedback, while simultaneously self-evolving its own evolution policies and rules. Starting from SAT Competition 2024 codebases and benchmark, SATLUTION evolved solvers that decisively outperformed the human-designed winners of the SAT Competition 2025, and also surpassed both 2024 and 2025 champions on the 2024 benchmarks.', 'abstract_zh': '大型语言模型（LLMs）最近展示了强大的编码能力，不仅能够进行静态代码生成，还能够通过自主框架迭代自我演进代码。受AlphaEvolve的启发，我们提出了SATLUTION框架，这是首个将基于LLM的代码演化扩展到整个代码仓库规模的框架，涵盖了数百个文件和数万行C/C++代码。针对布尔可满足性（SAT），该问题是经典的NP完全问题，也是理论与应用的重要基石。SATLUTION协调LLM代理直接在严格正确的条件下演化求解器仓库，并同时自我演化其自身的演化策略和规则。从SAT竞赛2024代码库和基准开始，SATLUTION演化出的求解器在SAT竞赛2025的人工设计获胜者中脱颖而出，并且在2024基准上也超过了2024和2025年的冠军。', 'title_zh': '自主代码进化遭遇NP完全性问题'}
{'arxiv_id': 'arXiv:2509.07260', 'title': 'HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring', 'authors': 'Xin Wang, Ting Dang, Xinyu Zhang, Vassilis Kostakos, Michael J. Witbrock, Hong Jia', 'link': 'https://arxiv.org/abs/2509.07260', 'abstract': "Mobile and wearable healthcare monitoring play a vital role in facilitating timely interventions, managing chronic health conditions, and ultimately improving individuals' quality of life. Previous studies on large language models (LLMs) have highlighted their impressive generalization abilities and effectiveness in healthcare prediction tasks. However, most LLM-based healthcare solutions are cloud-based, which raises significant privacy concerns and results in increased memory usage and latency. To address these challenges, there is growing interest in compact models, Small Language Models (SLMs), which are lightweight and designed to run locally and efficiently on mobile and wearable devices. Nevertheless, how well these models perform in healthcare prediction remains largely unexplored. We systematically evaluated SLMs on health prediction tasks using zero-shot, few-shot, and instruction fine-tuning approaches, and deployed the best performing fine-tuned SLMs on mobile devices to evaluate their real-world efficiency and predictive performance in practical healthcare scenarios. Our results show that SLMs can achieve performance comparable to LLMs while offering substantial gains in efficiency and privacy. However, challenges remain, particularly in handling class imbalance and few-shot scenarios. These findings highlight SLMs, though imperfect in their current form, as a promising solution for next-generation, privacy-preserving healthcare monitoring.", 'abstract_zh': '移动和穿戴设备健康监测在促进及时干预、管理慢性健康状况并最终提高个体生活质量方面发挥着关键作用。尽管以往关于大型语言模型（LLMs）的研究强调了其在健康预测任务中的出色推广能力和有效性，但大多数基于LLM的健康解决方案都是基于云的，这引发了显著的隐私担忧，并导致内存使用量增加和延迟增大。为解决这些问题，人们对紧凑模型——小语言模型（SLMs）的兴趣日益浓厚，SLMs设计为可以在移动和穿戴设备上本地高效运行。然而，这些模型在健康预测方面的表现尚不清楚。我们系统地评估了SLMs在健康预测任务中的表现，使用零样本、少量样本和指令微调方法，并将表现最佳的微调SLMs部署到移动设备上，以评估其在实际健康护理场景中的效率和预测性能。结果显示，SLMs可以在保持与大型语言模型相当的性能的同时，在效率和隐私方面实现显著改进。然而，仍存在处理类别不平衡和少量样本场景的挑战。这些发现表明，尽管目前还不完美，SLMs仍是一个有前景的解决方案，用于下一代隐私保护健康监测。', 'title_zh': 'HealthSLM-Bench: 用于移动和可穿戴健康监测的小微型语言模型基准测试'}
{'arxiv_id': 'arXiv:2509.07170', 'title': "That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral", 'authors': 'Quinten Steenhuis', 'link': 'https://arxiv.org/abs/2509.07170', 'abstract': 'Each year millions of people seek help for their legal problems by calling a legal aid program hotline, walking into a legal aid office, or using a lawyer referral service. The first step to match them to the right help is to identify the legal problem the applicant is experiencing. Misdirection has consequences. Applicants may miss a deadline, experience physical abuse, lose housing or lose custody of children while waiting to connect to the right legal help. We introduce and evaluate the FETCH classifier for legal issue classification and describe two methods for improving accuracy: a hybrid LLM/ML ensemble classification method, and the automatic generation of follow-up questions to enrich the initial problem narrative. We employ a novel data set of 419 real-world queries to a nonprofit lawyer referral service. Ultimately, we show classification accuracy (hits@2) of 97.37\\% using a mix of inexpensive models, exceeding the performance of the current state-of-the-art GPT-5 model. Our approach shows promise in significantly reducing the cost of guiding users of the legal system to the right resource for their problem while achieving high accuracy.', 'abstract_zh': '每年，数百万人通过拨打法律援助热线、前往法律援助办公室或使用律师推荐服务寻求帮助。识别申请人遇到的法律问题的第一步是将他们引导到合适的帮助。方向失误会产生后果。申请人可能会错过截止日期，经历身体虐待，失去住所或失去孩子的抚养权，直到他们能够连接到正确的法律帮助。我们介绍了并评价了FETCH分类器用于法律问题分类的方法，并描述了两种提高准确性的方法：混合大语言模型和机器学习的集成分类方法，以及自动生成跟进问题以丰富初始问题描述。我们使用了一个包含419个真实查询的新型数据集，这些查询是针对一家非营利律师推荐服务的。最终，我们展示了使用廉价模型混合的分类准确率（hits@2）为97.37%，超过了当前最先进的GPT-5模型的性能。我们的方法显示出在显著降低引导法律系统用户找到其问题的正确资源的成本方面具有巨大潜力，同时保持高准确率。', 'title_zh': '那真时尚：为民事法律咨询和转介的LLM分类构建集成技术'}
{'arxiv_id': 'arXiv:2509.07159', 'title': 'PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning', 'authors': 'Heng Hao, Wenjun Hu, Oxana Verkholyak, Davoud Ataee Tarzanagh, Baruch Gutow, Sima Didari, Masoud Faraki, Hankyu Moon, Seungjai Min', 'link': 'https://arxiv.org/abs/2509.07159', 'abstract': 'Text-to-SQL models allow users to interact with a database more easily by generating executable SQL statements from natural-language questions. Despite recent successes on simpler databases and questions, current Text-to-SQL methods still suffer from low execution accuracy on industry-scale databases and complex questions involving domain-specific business logic. We present \\emph{PaVeRL-SQL}, a framework that combines \\emph{Partial-Match Rewards} and \\emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt two pipelines: (1) a newly designed in-context learning framework with group self-evaluation (verbal-RL), using capable open- and closed-source large language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL pipeline with a small backbone model (OmniSQL-7B) trained with a specially designed reward function and two-stage RL. These pipelines achieve state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks -- Spider, Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the verbal-RL pipeline achieves an execution accuracy 7.4\\% higher than SOTA, and the CoT pipeline is 1.4\\% higher. RL training with mixed SQL dialects yields strong, threefold gains, particularly for dialects with limited training data. Overall, \\emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic industrial constraints. The code is available at this https URL.', 'abstract_zh': 'PaVeRL-SQL：结合部分匹配奖励和口头强化学习的Text-to-SQL框架', 'title_zh': 'PaVeRL-SQL: 通过部分匹配奖励和言语强化学习实现文本到SQL转换'}
{'arxiv_id': 'arXiv:2509.07941', 'title': 'ImportSnare: Directed "Code Manual" Hijacking in Retrieval-Augmented Code Generation', 'authors': 'Kai Ye, Liangcai Su, Chenxiong Qian', 'link': 'https://arxiv.org/abs/2509.07941', 'abstract': "Code generation has emerged as a pivotal capability of Large Language Models(LLMs), revolutionizing development efficiency for programmers of all skill levels. However, the complexity of data structures and algorithmic logic often results in functional deficiencies and security vulnerabilities in generated code, reducing it to a prototype requiring extensive manual debugging. While Retrieval-Augmented Generation (RAG) can enhance correctness and security by leveraging external code manuals, it simultaneously introduces new attack surfaces.\nIn this paper, we pioneer the exploration of attack surfaces in Retrieval-Augmented Code Generation (RACG), focusing on malicious dependency hijacking. We demonstrate how poisoned documentation containing hidden malicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting dual trust chains: LLM reliance on RAG and developers' blind trust in LLM suggestions. To construct poisoned documents, we propose ImportSnare, a novel attack framework employing two synergistic strategies: 1)Position-aware beam search optimizes hidden ranking sequences to elevate poisoned documents in retrieval results, and 2)Multilingual inductive suggestions generate jailbreaking sequences to manipulate LLMs into recommending malicious dependencies. Through extensive experiments across Python, Rust, and JavaScript, ImportSnare achieves significant attack success rates (over 50% for popular libraries such as matplotlib and seaborn) in general, and is also able to succeed even when the poisoning ratio is as low as 0.01%, targeting both custom and real-world malicious packages. Our findings reveal critical supply chain risks in LLM-powered development, highlighting inadequate security alignment for code generation tasks. To support future research, we will release the multilingual benchmark suite and datasets. The project homepage is this https URL.", 'abstract_zh': 'Retrieval-Augmented Code Generation的安全威胁探索：关注恶意依赖劫持', 'title_zh': 'ImportSnare: 有向“代码手册”在网络检索增强代码生成中的劫持'}
{'arxiv_id': 'arXiv:2509.07933', 'title': 'Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation', 'authors': 'Wanni Vidulige Ishan Perera, Xing Liu, Fan liang, Junyi Zhang', 'link': 'https://arxiv.org/abs/2509.07933', 'abstract': "The rapid evolution of Artificial Intelligence (AI) and Large Language Models (LLMs) has opened up new opportunities in the area of cybersecurity, especially in the exploitation automation landscape and penetration testing. This study explores Android penetration testing automation using LLM-based tools, especially PentestGPT, to identify and execute rooting techniques. Through a comparison of the traditional manual rooting process and exploitation methods produced using AI, this study evaluates the efficacy, reliability, and scalability of automated penetration testing in achieving high-level privilege access on Android devices. With the use of an Android emulator (Genymotion) as the testbed, we fully execute both traditional and exploit-based rooting methods, automating the process using AI-generated scripts. Secondly, we create a web application by integrating OpenAI's API to facilitate automated script generation from LLM-processed responses. The research focuses on the effectiveness of AI-enabled exploitation by comparing automated and manual penetration testing protocols, by determining LLM weaknesses and strengths along the way. We also provide security suggestions of AI-enabled exploitation, including ethical factors and potential misuse. The findings exhibit that while LLMs can significantly streamline the workflow of exploitation, they need to be controlled by humans to ensure accuracy and ethical application. This study adds to the increasing body of literature on AI-powered cybersecurity and its effect on ethical hacking, security research, and mobile device security.", 'abstract_zh': '人工智能（AI）和大型语言模型（LLMs）的快速进化在网络安全领域开辟了新机会，特别是在利用自动化和渗透测试领域。本研究探讨了使用基于LLM的工具（特别是PentestGPT）进行Android渗透测试自动化，以识别和执行提权技术。通过传统手动提权过程与AI生成的利用方法的对比，本研究评估了自动化渗透测试在获取Android设备高级权限方面的有效性和可靠性及可扩展性。使用Genymotion作为测试环境，我们完整执行了传统和利用基于的提权方法，并使用AI生成的脚本自动化这些过程。其次，我们创建了一个web应用程序，通过集成OpenAI的API，以便从LLM处理的响应生成自动化脚本。本研究侧重于通过比较自动化和手动渗透测试协议来评估AI辅助利用的有效性，同时确定LLM的优势和弱点。此外，我们还提供了AI辅助利用的安全建议，包括伦理因素和潜在误用。研究结果表明，尽管LLMs能够显著简化利用工作流程，但必须由人类控制以确保准确性和伦理应用。本研究增加了关于AI驱动网络安全及其对伦理 hacking、安全研究和移动设备安全影响的文献。', 'title_zh': '用AI打破Android：基于LLM的利用深度探究'}
{'arxiv_id': 'arXiv:2509.07925', 'title': 'GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models', 'authors': 'Tuo Wang, Adithya Kulkarni, Tyler Cody, Peter A. Beling, Yujun Yan, Dawei Zhou', 'link': 'https://arxiv.org/abs/2509.07925', 'abstract': 'Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at this https URL.', 'abstract_zh': '结构aware的Large Language Models不确定性估计：基于图的多层级不确定量化评估（GENUINE）', 'title_zh': 'GENUINE: 图络增强多层级不确定性估计large语言模型'}
{'arxiv_id': 'arXiv:2509.07909', 'title': 'Uncovering Scaling Laws for Large Language Models via Inverse Problems', 'authors': 'Arun Verma, Zhaoxuan Wu, Zijian Zhou, Xiaoqiang Lin, Zhiliang Chen, Rachael Hwee Ling Sim, Rui Qiao, Jingtan Wang, Nhung Bui, Xinyuan Niu, Wenyang Hu, Gregory Kang Ruey Lau, Zi-Yu Khoo, Zitong Zhao, Xinyi Xu, Apivich Hemachandra, See-Kiong Ng, Bryan Kian Hsiang Low', 'link': 'https://arxiv.org/abs/2509.07909', 'abstract': 'Large Language Models (LLMs) are large-scale pretrained models that have achieved remarkable success across diverse domains. These successes have been driven by unprecedented complexity and scale in both data and computations. However, due to the high costs of training such models, brute-force trial-and-error approaches to improve LLMs are not feasible. Inspired by the success of inverse problems in uncovering fundamental scientific laws, this position paper advocates that inverse problems can also efficiently uncover scaling laws that guide the building of LLMs to achieve the desirable performance with significantly better cost-effectiveness.', 'abstract_zh': '大型语言模型（LLMs）是大规模预训练模型，在多个领域取得了显著成功。这些成功得益于数据和计算前所未有的复杂性和规模。然而，由于训练此类模型成本高昂，通过蛮力试错方法改进LLMs不可行。受逆问题在揭示基本科学规律方面成功应用的启发，本文主张逆问题也可以高效地揭示指导构建LLMs以实现期望性能并大幅提高成本效益的缩放规律。', 'title_zh': '通过逆问题揭示大型语言模型的尺度定律'}
{'arxiv_id': 'arXiv:2509.07829', 'title': 'Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost', 'authors': 'Mihai Nadas, Laura Diosan, Andreea Tomescu, Andrei Piscoran', 'link': 'https://arxiv.org/abs/2509.07829', 'abstract': 'Literary translation has recently gained attention as a distinct and complex task in machine translation research. However, the translation by small open models remains an open problem. We contribute to this ongoing research by introducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for dataset creation, fine tuning, and evaluation in English-Romanian literary translations, centred on the creation and open release of both a compact, fine tuned language model (TF2-12B) and large scale synthetic parallel datasets (DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the largest collection of synthetic English fables to date, we address the need for rich, high quality literary datasets in low resource languages such as Romanian. Our pipeline first generates 15k high quality Romanian references from the TF1 pool using a high performing LLM. We then apply a two stage fine tuning process to a 12B parameter open weight model: (i) instruction tuning to capture genre specific narrative style, and (ii) adapter compression for efficient deployment. Evaluation combines corpus level BLEU and a five dimension LLM based rubric (accuracy, fluency, coherence, style, cultural adaptation) to provide a nuanced assessment of translation quality. Results show that our fine tuned model achieves fluency and adequacy competitive with top performing large proprietary models, while being open, accessible, and significantly more cost effective. Alongside the fine tuned model and both datasets, we publicly release all scripts and evaluation prompts. TF2 thus provides an end-to-end, reproducible pipeline for research on cost efficient translation, cross lingual narrative generation, and the broad adoption of open models for culturally significant literary content in low resource settings.', 'abstract_zh': '文学翻译近年来已成为机器翻译研究中一个独特而复杂的任务，然而由小开放模型进行的翻译仍是一个待解决问题。我们通过引入TINYFABULIST翻译框架（TF2）， contributions to this ongoing research，该框架专注于创建和公开发布一个紧凑的微调语言模型（TF2-12B）以及大规模合成平行数据集（DS-TF2-EN-RO-3M和DS-TF2-EN-RO-15K），从而推动了这一研究。基于迄今为止最大的合成英语寓言集DS-TF1-EN-3M（TF1），我们解决了罗马尼亚等低资源语言丰富高质量文学数据集的需求。我们的管道首先从TF1池中生成15000个高质量的罗马尼亚参考文本，使用高性能的语言模型。然后，我们对一个120亿参数的开放权重模型应用两阶段微调过程：（i）指令微调以捕捉特定体裁的叙述风格，和（ii）适配器压缩以实现高效的部署。评估结合了语料库级别的BLEU分数和基于LLM的五维评判标准（准确性、流畅性、连贯性、风格、文化适应性），以提供翻译质量的细致评估。结果显示，我们的微调模型在流畅性和适当性方面与顶级大型专有模型竞争，同时具有开放性、可访问性，并且成本效益更高。我们还公开发布了所有相关脚本和评估提示。因此，TF2提供了成本高效翻译、跨语言叙述生成以及在低资源环境中广泛采用开放模型的文化重要文学内容的研究端到端、可再现的管道。', 'title_zh': '小规模开放模型在低资源文学翻译中的性能接近大规模模型，并且成本仅为后者的几分之一。'}
{'arxiv_id': 'arXiv:2509.07768', 'title': 'Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning', 'authors': 'Michele Joshua Maggini, Dhia Merzougui, Rabiraj Bandyopadhyay, Gaël Dias, Fabrice Maurel, Pablo Gamallo', 'link': 'https://arxiv.org/abs/2509.07768', 'abstract': 'The spread of fake news, polarizing, politically biased, and harmful content on online platforms has been a serious concern. With large language models becoming a promising approach, however, no study has properly benchmarked their performance across different models, usage methods, and languages. This study presents a comprehensive overview of different Large Language Models adaptation paradigms for the detection of hyperpartisan and fake news, harmful tweets, and political bias. Our experiments spanned 10 datasets and 5 different languages (English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and multiclass classification scenarios. We tested different strategies ranging from parameter efficient Fine-Tuning of language models to a variety of different In-Context Learning strategies and prompts. These included zero-shot prompts, codebooks, few-shot (with both randomly-selected and diversely-selected examples using Determinantal Point Processes), and Chain-of-Thought. We discovered that In-Context Learning often underperforms when compared to Fine-Tuning a model. This main finding highlights the importance of Fine-Tuning even smaller models on task-specific settings even when compared to the largest models evaluated in an In-Context Learning setup - in our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and Qwen2.5-7B-Instruct.', 'abstract_zh': '在线平台上假新闻、极化内容、政治偏见和有害内容的传播是一个严重关切的问题。然而，随着大型语言模型成为一种有前景的方法，尚未有研究在不同模型、使用方法和语言之间恰当地基准测试它们的性能。本文提供了一个关于不同大型语言模型适应范式在检测极化内容和假新闻、有害推文以及政治偏见方面的全面概述。我们的实验覆盖了10个数据集和5种不同的语言（英语、西班牙语、葡萄牙语、阿拉伯语和保加利亚语），包括二分类和多分类分类场景。我们测试了从参数高效微调语言模型到各种不同上下文学习策略和提示的不同策略，包括零样本提示、代码书、少样本（包括随机选择和使用行列式点过程选择多样样本）以及思维链。我们发现，与微调模型相比，上下文学习往往表现较差。这一主要发现突出了即使在上下文学习设置中使用较小的模型，对其进行任务特定的微调的重要性，这在我们的案例中包括LlaMA3.1-8b-Instruct、Mistral-Nemo-Instruct-2407和Qwen2.5-7B-Instruct。', 'title_zh': '大规模语言模型足以检测超党派化、假信息、极化和有害内容吗？基于上下文学习与细调的评估'}
{'arxiv_id': 'arXiv:2509.07763', 'title': 'What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring Motivations in Open-Source Projects', 'authors': 'Mikel Robredo, Matteo Esposito, Fabio Palomba, Rafael Peñaloza, Valentina Lenarduzzi', 'link': 'https://arxiv.org/abs/2509.07763', 'abstract': 'Context. Code refactoring improves software quality without changing external behavior. Despite its advantages, its benefits are hindered by the considerable cost of time, resources, and continuous effort it demands. Aim. Understanding why developers refactor, and which metrics capture these motivations, may support wider and more effective use of refactoring in practice. Method. We performed a large-scale empirical study to analyze developers refactoring activity, leveraging Large Language Models (LLMs) to identify underlying motivations from version control data, comparing our findings with previous motivations reported in the literature. Results. LLMs matched human judgment in 80% of cases, but aligned with literature-based motivations in only 47%. They enriched 22% of motivations with more detailed rationale, often highlighting readability, clarity, and structural improvements. Most motivations were pragmatic, focused on simplification and maintainability. While metrics related to developer experience and code readability ranked highest, their correlation with motivation categories was weak. Conclusions. We conclude that LLMs effectively capture surface-level motivations but struggle with architectural reasoning. Their value lies in providing localized explanations, which, when combined with software metrics, can form hybrid approaches. Such integration offers a promising path toward prioritizing refactoring more systematically and balancing short-term improvements with long-term architectural goals.', 'abstract_zh': '上下文. 代码重构可以在不改变外部行为的情况下提高软件质量。尽管具备优势，但其应用受到时间、资源和持续努力成本的限制。目标. 理解开发者为何进行重构，以及哪些指标能够捕捉这些动机，有助于更广泛和有效地在实践中应用重构。方法. 我们进行了大规模实证研究，分析开发者的重构活动，利用大型语言模型（LLMs）从版本控制数据中识别潜在动机，并将我们的发现与文献中报告的先前动机进行比较。结果. LLMs 在 80% 的案例中与人类判断相符，但在文献基础动机方面只有 47% 一致。它们为 22% 的动机提供了更详细的理由，经常突出可读性、清晰性和结构改进。大多数动机是实际的，集中在简化和可维护性上。虽然与开发人员经验和代码可读性相关的指标得分最高，但它们与动机类别相关性较弱。结论. 我们得出结论，LLMs 有效地捕捉了表面动机，但在架构推理方面存在困难。它们的价值在于提供局部解释，结合软件指标时，可以形成混合方法。这种整合为更系统地优先考虑重构，并平衡短期改进与长期架构目标提供了一条有希望的道路。', 'title_zh': '你当时在想什么？一个基于LLM的大规模开源项目重构动机研究'}
{'arxiv_id': 'arXiv:2509.07588', 'title': 'BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment', 'authors': 'Andrey Sakhovskiy, Elena Tutubalina', 'link': 'https://arxiv.org/abs/2509.07588', 'abstract': 'In recent years, there has been substantial progress in using pretrained Language Models (LMs) on a range of tasks aimed at improving the understanding of biomedical texts. Nonetheless, existing biomedical LLMs show limited comprehension of complex, domain-specific concept structures and the factual information encoded in biomedical Knowledge Graphs (KGs). In this work, we propose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel joint LM and KG pre-training method that augments an LM with external knowledge by the simultaneous learning of a dedicated KG encoder and aligning the representations of both the LM and the graph. For a given textual sequence, we link biomedical concept mentions to the Unified Medical Language System (UMLS) KG and utilize local KG subgraphs as cross-modal positive samples for these mentions. Our empirical findings indicate that implementing our method on several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves their performance on a range of language understanding tasks and the quality of entity representations, even with minimal pre-training on a small alignment dataset sourced from PubMed scientific abstracts.', 'abstract_zh': '生物医学知识图谱与语言模型联合预训练方法BALI', 'title_zh': 'BALI：通过知识图谱和语言模型对齐增强生物医学语言表示'}
{'arxiv_id': 'arXiv:2509.07571', 'title': 'Towards Generalized Routing: Model and Agent Orchestration for Adaptive and Efficient Inference', 'authors': 'Xiyu Guo, Shan Wang, Chunfang Ji, Xuefeng Zhao, Wenhao Xi, Yaoyao Liu, Qinglan Li, Chao Deng, Junlan Feng', 'link': 'https://arxiv.org/abs/2509.07571', 'abstract': 'The rapid advancement of large language models (LLMs) and domain-specific AI agents has greatly expanded the ecosystem of AI-powered services. User queries, however, are highly diverse and often span multiple domains and task types, resulting in a complex and heterogeneous landscape. This diversity presents a fundamental routing challenge: how to accurately direct each query to an appropriate execution unit while optimizing both performance and efficiency. To address this, we propose MoMA (Mixture of Models and Agents), a generalized routing framework that integrates both LLM and agent-based routing. Built upon a deep understanding of model and agent capabilities, MoMA effectively handles diverse queries through precise intent recognition and adaptive routing strategies, achieving an optimal balance between efficiency and cost. Specifically, we construct a detailed training dataset to profile the capabilities of various LLMs under different routing model structures, identifying the most suitable tasks for each LLM. During inference, queries are dynamically routed to the LLM with the best cost-performance efficiency. We also introduce an efficient agent selection strategy based on a context-aware state machine and dynamic masking. Experimental results demonstrate that the MoMA router offers superior cost-efficiency and scalability compared to existing approaches.', 'abstract_zh': '大规模语言模型（LLMs）和领域特定AI代理的迅速发展极大地扩展了AI驱动服务的生态系统。然而，用户查询具有高度多样性，并且经常跨越多个领域和任务类型，导致一个复杂且异质的景观。这种多样性提出了一个基础的路由挑战：如何准确地将每个查询导向合适的执行单元，同时优化性能和效率。为应对这一挑战，我们提出MoMA（模型和代理的混合体）——一个通用的路由框架，整合了LLM和基于代理的路由。基于对模型和代理能力的深刻理解，MoMA通过精确的意图识别和自适应路由策略有效地处理各种查询，实现效率和成本的最优平衡。具体而言，我们构建了一个详细的训练数据集，以了解在不同路由模型结构下各种LLM的能力，并确定最适合每个LLM的任务。在推理过程中，查询被动态路由到具有最佳成本-性能效率的LLM。我们还引入了一种基于上下文感知状态机和动态掩码的高效代理选择策略。实验结果表明，MoMA路由器在成本效率和可扩展性方面优于现有方法。', 'title_zh': '面向通用路由：模型和代理编排以实现适应性和高效的推理'}
{'arxiv_id': 'arXiv:2509.07558', 'title': '$ΔL$ Normalization: Rethink Loss Aggregation in RLVR', 'authors': 'Zhiyuan He, Xufang Luo, Yike Zhang, Yuqing Yang, Lili Qiu', 'link': 'https://arxiv.org/abs/2509.07558', 'abstract': 'We propose $\\Delta L$ Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed $\\Delta L$ Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at this https URL.', 'abstract_zh': '我们提出了一种针对可验证奖励强化学习（RLVR）中动态生成长度特点的简单有效的损失聚合方法——$\\Delta L$归一化。', 'title_zh': 'ΔL 归一化：重思 RLVR 中的损失聚合'}
{'arxiv_id': 'arXiv:2509.07555', 'title': 'Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition', 'authors': 'Yi Liu, Xiangrong Zhu, Xiangyu Liu, Wei Wei, Wei Hu', 'link': 'https://arxiv.org/abs/2509.07555', 'abstract': 'In a rapidly evolving world where information updates swiftly, knowledge in large language models (LLMs) becomes outdated quickly. Retraining LLMs is not a cost-effective option, making knowledge editing (KE) without modifying parameters particularly necessary. We find that although existing retrieval-augmented generation (RAG)-based KE methods excel at editing simple knowledge, they struggle with KE in multi-hop question answering due to the issue of "edit skipping", which refers to skipping the relevant edited fact in inference. In addition to the diversity of natural language expressions of knowledge, edit skipping also arises from the mismatch between the granularity of LLMs in problem-solving and the facts in the edited memory. To address this issue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing method with guided decomposition (IRAKE) through the guidance from single edited facts and entire edited cases. Experimental results demonstrate that IRAKE mitigates the failure of editing caused by edit skipping and outperforms state-of-the-art methods for KE in multi-hop question answering.', 'abstract_zh': '在信息快速更新的世界中，大型语言模型（LLMs）中的知识迅速过时。重新训练LLMs不是一种成本有效的选择，因此在不修改参数的情况下进行知识编辑（KE）尤为重要。我们发现，尽管现有的基于检索增强生成（RAG）的知识编辑方法在编辑简单知识方面表现出色，但在处理多跳问答的知识编辑时却面临着“编辑跳跃”的问题，这指的是推理过程中跳过了相关的编辑事实。除了知识的自然语言表达多样性外，编辑跳跃还源于问题求解中LLMs的粒度与编辑记忆中的事实之间的不匹配。为解决这一问题，我们提出了一种基于单个编辑事实和整个编辑案例指导的迭代检索增强知识编辑方法（IRAKE），通过这种方式进行知识编辑。实验结果表明，IRAKE减轻了由编辑跳跃引起的编辑失败，并在多跳问答的知识编辑方面优于最先进的方法。', 'title_zh': '基于引导分解在多跳问答中避免知识编辑跳过'}
{'arxiv_id': 'arXiv:2509.07526', 'title': 'Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data', 'authors': 'Gokul Karthik Kumar, Rishabh Saraf, Ludovick Lepauloux, Abdul Muneer, Billel Mokeddem, Hakim Hacid', 'link': 'https://arxiv.org/abs/2509.07526', 'abstract': "Large language models (LLMs) have transformed NLP, yet their integration with audio remains underexplored -- despite audio's centrality to human communication. We introduce Falcon3-Audio, a family of Audio-Language Models (ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably small amount of public audio data -- less than 30K hours (5K unique) -- Falcon3-Audio-7B matches the best reported performance among open-weight models on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while distinguishing itself through superior data and parameter efficiency, single-stage training, and transparency. Notably, our smallest 1B model remains competitive with larger open models ranging from 2B to 13B parameters. Through extensive ablations, we find that common complexities -- such as curriculum learning, multiple audio encoders, and intricate cross-attention connectors -- are not required for strong performance, even compared to models trained on over 500K hours of data.", 'abstract_zh': '大规模语言模型(LLMs)虽已革新自然语言处理(NLP)，但其与音频的整合仍待探索——尽管音频在人类交流中居于核心地位。我们介绍了Falcon3-Audio，这是一种基于指令调优LLMs和Whisper编码器构建的音频语言模型家族。使用不到30K小时（5K种独特）的公开音频数据，Falcon3-Audio-7B在MMAU基准测试中达到最佳报告性能，得分为64.14，与R1-AQA相当，同时通过优越的数据和参数效率、单阶段训练和透明度脱颖而出。值得注意的是，我们最小的1B模型仍与参数范围从2B到13B的大规模公开模型保持竞争力。通过广泛的消融实验，我们发现常见的复杂性，如逐级学习、多种音频编码器和复杂的交叉注意连接器，并非高效性能的必需条件，即使相比数据训练时长超过50万小时的模型也是如此。', 'title_zh': '高效利用公共数据的单阶段训练竞争性音频-语言模型'}
{'arxiv_id': 'arXiv:2509.07512', 'title': 'ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval', 'authors': 'Zihan Chen, Lei Shi, Weize Wu, Qiji Zhou, Yue Zhang', 'link': 'https://arxiv.org/abs/2509.07512', 'abstract': 'Many contemporary data-driven research efforts in the natural sciences, such as chemistry and materials science, require large-scale, high-performance entity recognition from scientific datasets. Large language models (LLMs) have increasingly been adopted to solve the entity recognition task, with the same trend being observed on all-spectrum NLP tasks. The prevailing entity recognition LLMs rely on fine-tuned technology, yet the fine-tuning process often incurs significant cost. To achieve a best performance-cost trade-off, we propose ALLabel, a three-stage framework designed to select the most informative and representative samples in preparing the demonstrations for LLM modeling. The annotated examples are used to construct a ground-truth retrieval corpus for LLM in-context learning. By sequentially employing three distinct active learning strategies, ALLabel consistently outperforms all baselines under the same annotation budget across three specialized domain datasets. Experimental results also demonstrate that selectively annotating only 5\\%-10\\% of the dataset with ALLabel can achieve performance comparable to the method annotating the entire dataset. Further analyses and ablation studies verify the effectiveness and generalizability of our proposal.', 'abstract_zh': '当代自然科学中大规模高性能实体识别的需求：ALLabel三阶段框架在准备LLM模型示例中的应用及其效果分析', 'title_zh': 'ALLabel: 基于演示检索的三阶段主动学习实体识别方法'}
{'arxiv_id': 'arXiv:2509.07506', 'title': 'Astra: A Multi-Agent System for GPU Kernel Performance Optimization', 'authors': 'Anjiang Wei, Tianran Sun, Yogesh Seenichamy, Hang Song, Anne Ouyang, Azalia Mirhoseini, Ke Wang, Alex Aiken', 'link': 'https://arxiv.org/abs/2509.07506', 'abstract': 'GPU kernel optimization has long been a central challenge at the intersection of high-performance computing and machine learning. Efficient kernels are crucial for accelerating large language model (LLM) training and serving, yet attaining high performance typically requires extensive manual tuning. Compiler-based systems reduce some of this burden, but still demand substantial manual design and engineering effort. Recently, researchers have explored using LLMs for GPU kernel generation, though prior work has largely focused on translating high-level PyTorch modules into CUDA code. In this work, we introduce Astra, the first LLM-based multi-agent system for GPU kernel optimization. Unlike previous approaches, Astra starts from existing CUDA implementations extracted from SGLang, a widely deployed framework for serving LLMs, rather than treating PyTorch modules as the specification. Within Astra, specialized LLM agents collaborate through iterative code generation, testing, profiling, and planning to produce kernels that are both correct and high-performance. On kernels from SGLang, Astra achieves an average speedup of 1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study further demonstrates that LLMs can autonomously apply loop transformations, optimize memory access patterns, exploit CUDA intrinsics, and leverage fast math operations to yield substantial performance gains. Our work highlights multi-agent LLM systems as a promising new paradigm for GPU kernel optimization.', 'abstract_zh': '基于LLM的多Agent GPU内核优化系统Astra', 'title_zh': 'Astra：一种GPU内核性能优化的多agent系统'}
{'arxiv_id': 'arXiv:2509.07430', 'title': 'The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward', 'authors': 'Long Li, Jiaran Hao, Jason Klein Liu, Zhijian Zhou, Xiaoyu Tan, Wei Chu, Zhe Wang, Shirui Pan, Chao Qu, Yuan Qi', 'link': 'https://arxiv.org/abs/2509.07430', 'abstract': 'A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. While various methods have been proposed, the choice and function of the divergence term have been surprisingly unexamined as a proactive solution. We argue that standard RLVR objectives -- both those using the mode-seeking reverse KL-divergence and those forgoing a divergence term entirely -- lack a crucial mechanism for knowledge retention. The reverse-KL actively accelerates this decay by narrowing the policy, while its absence provides no safeguard against the model drifting from its diverse knowledge base. We propose a fundamental shift in perspective: using the divergence term itself as the solution. Our framework, Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences (like forward-KL and JS-divergence) to function as a rehearsal mechanism. By continuously referencing the initial policy, this approach forces the model to maintain broad solution coverage. Extensive experiments on math and SQL generation demonstrate that DPH-RL not only resolves the Pass@k degradation but improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is more training-efficient because it computes f-divergence using generator functions, requiring only sampling from the initial policy and no online reference model. Our work highlights a crucial, overlooked axis for improving RLVR, demonstrating that the proper selection of a divergence measure is a powerful tool for building more general and diverse reasoning models.', 'abstract_zh': '在使用可验证奖励的强化学习（RLVR）微调大型语言模型（LLMs）时的一个核心悖论：尽管单次尝试准确性（Pass@1）有所提升，但多尝试性能（Pass@k）频繁下降，同时往往伴随着灾难性遗忘，模型失去已获取的技能。虽然已提出多种方法，但分歧项的选择和功能作为积极解决方案的研究却出人意料地缺乏。我们认为，标准的RLVR目标——无论是使用模式搜索逆KL散度还是完全不使用分歧项的做法——都缺乏一个关键的知识保持机制。逆KL加速了这种退化，因为它限制了策略范围，而缺乏分歧项则无法防止模型偏离其多元知识库。我们提出了一个基本的视角转变：将分歧项本身作为解决方案。我们的框架，保多样强化学习混合算法（DPH-RL），利用质量覆盖的f散度（如向前KL散度和JS散度）作为巩固机制。通过不断参考初始策略，这种做法迫使模型保持广泛解决方案的覆盖。大量关于数学和SQL生成的实验表明，DPH-RL不仅可以解决Pass@k下降的问题，还能提高域内和域外的Pass@1和Pass@k性能。此外，DPH-RL更具训练效率，因为它使用生成函数计算f散度，仅需从初始策略采样而不需要在线参考模型。我们的工作强调了一个关键且被忽视的维度，即改进RLVR的方法，证明了适当选择分歧度量是一个构建更具普适性和多样性的推理模型的强大工具。', 'title_zh': '散度的选择：减轻强化学习中可验证奖励下多样性崩溃的关键因素'}
{'arxiv_id': 'arXiv:2509.07389', 'title': 'Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents', 'authors': 'Sankalp Tattwadarshi Swain, Anshika Krishnatray, Dhruv Kumar, Jagat Sesh Challa', 'link': 'https://arxiv.org/abs/2509.07389', 'abstract': 'Existing evaluation studies on linguistic competence of large language models (LLM agents) have focused primarily on vocabulary learning, morphological rule induction, syntactic generalization, pragmatic inference, and cross-linguistic transfer. However, none assess whether LLM agents can acquire a language through pattern recognition and interactive feedback, a central feature of human language acquisition. We propose a novel experimental framework in which an LLM agent is evaluated on its ability to acquire and use a newly constructed language (Tinkatongue) in conversation with a bot that understands only Tinkatongue. Our findings show that LLM agents fail to establish a conversation within 100 responses, yet they adopt distinct strategies that mirror human approaches to language learning. The results suggest a new direction for evaluation benchmarks and open pathways to model designs that learn more effectively from interactive feedback.', 'abstract_zh': '现有的语言能力评估研究主要集中在词汇学习、词法规则归纳、句法概括、语用推理以及跨语言迁移上。然而，这些研究未评估LLM代理是否能够通过模式识别和互动反馈来习得一门语言，而这正是人类语言习得的核心特征。我们提出了一种新的实验框架，通过让一个LLM代理与仅理解Tinkatongue的机器人进行对话来评估其获取和使用新构建语言（Tinkatongue）的能力。我们的研究发现，LLM代理在100次响应内无法建立对话，但它们采用的策略却类似于人类的语言学习方法。结果表明，这为评估基准提供了新的方向，并为更有效地从互动反馈中学习的模型设计开辟了路径。', 'title_zh': '与奥ompa Loompas对话：一种评估LLM代理语言获取的新框架'}
{'arxiv_id': 'arXiv:2509.07324', 'title': 'Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation', 'authors': 'Nakyung Lee, Yeongoon Kim, Minhae Oh, Suhwan Kim, Jin Woo Koo, Hyewon Jo, Jungwoo Lee', 'link': 'https://arxiv.org/abs/2509.07324', 'abstract': 'Transformer-based self-attention mechanism serves as the core of modern language models, yet it often suffers from localization, where attentions collapse onto a limited subset of tokens and fail to capture long-range dependencies. To address this issue, we propose Self-Attention One-step Belief Propagation (SAOBP), a refinement framework that injects multi-hop relationships through a belief propagation process. To interpret and quantify these interactions, we introduce Global Token Dependency (GTD) that captures the relative contribution of multihop connections within the attention graph. Empirical results indicate that SAOBP helps prevent entropy collapse in deeper layers and adaptively maintains GTD at task-appropriate levels, thereby supporting improvements in model performance. Importantly, we observe competitive gains in small-scale models, highlighting its potential for improving inference quality in resource-constrained scenarios.', 'abstract_zh': '基于Transformer的自注意力机理是现代语言模型的核心，但常常会遭受本地化问题，即注意力集中在少量的令牌上，导致无法捕捉长距离依赖。为了解决这个问题，我们提出了Self-Attention One-step Belief Propagation (SAOBP) 精炼框架，该框架通过信念传播过程注入多跳关系。为了解释和量化这些交互，我们引入了全局令牌依赖性（GTD），它捕获了注意图中多跳连接的相对贡献。实验证明，SAOBP有助于在更深的层中防止熵塌缩，并适当地保持任务适当的GTD水平，从而支持模型性能的提升。重要的是，我们在小规模模型中观察到了竞争力的提升，突显了其在资源受限场景中提高推理质量的潜力。', 'title_zh': '小规模中的注意力局部化缓解：基于一次性信念传播的自我注意力细化'}
{'arxiv_id': 'arXiv:2509.07311', 'title': 'Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations', 'authors': 'Sihyun Park', 'link': 'https://arxiv.org/abs/2509.07311', 'abstract': "Recent advances in large language models (LLMs) have been driven by pretraining, supervised fine tuning (SFT), and alignment tuning. Among these, SFT plays a crucial role in transforming a model 's general knowledge into structured responses tailored to specific tasks. However, there is no clearly established methodology for effective training data selection. Simply increasing the volume of data does not guarantee performance improvements, while preprocessing, sampling, and validation require substantial time and cost.\nTo address this issue, a variety of data selection methods have been proposed. Among them, knowledge based selection approaches identify suitable training data by analyzing the model 's responses. Nevertheless, these methods typically rely on prompt engineering, making them sensitive to variations and incurring additional costs for prompt design.\nIn this study, we propose Knowledge Analysis via Model Internal Representations (KAMIR), a novel approach that overcomes these limitations by analyzing data based on the model 's internal representations. KAMIR computes similarities between the hidden states of each layer (block) and the final hidden states for a given input to assess the data. Unlike prior methods that were largely limited to multiple choice tasks, KAMIR can be applied to a wide range of tasks such as machine reading comprehension and summarization. Moreover, it selects data useful for training based on the model 's familiarity with the input, even with a small dataset and a simple classifier architecture. Experiments across diverse task datasets demonstrate that training with less familiar data leads to better generalization performance.", 'abstract_zh': 'Recent Advances in Large Language Models: Knowledge Analysis via Model Internal Representations (KAMIR)', 'title_zh': '你觉得这个看起来眼熟吗？基于模型内部表示的知识分析'}
{'arxiv_id': 'arXiv:2509.07287', 'title': 'Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm', 'authors': 'Yan Pang, Wenlong Meng, Xiaojing Liao, Tianhao Wang', 'link': 'https://arxiv.org/abs/2509.07287', 'abstract': 'With the rapid development of large language models, the potential threat of their malicious use, particularly in generating phishing content, is becoming increasingly prevalent. Leveraging the capabilities of LLMs, malicious users can synthesize phishing emails that are free from spelling mistakes and other easily detectable features. Furthermore, such models can generate topic-specific phishing messages, tailoring content to the target domain and increasing the likelihood of success.\nDetecting such content remains a significant challenge, as LLM-generated phishing emails often lack clear or distinguishable linguistic features. As a result, most existing semantic-level detection approaches struggle to identify them reliably. While certain LLM-based detection methods have shown promise, they suffer from high computational costs and are constrained by the performance of the underlying language model, making them impractical for large-scale deployment.\nIn this work, we aim to address this issue. We propose Paladin, which embeds trigger-tag associations into vanilla LLM using various insertion strategies, creating them into instrumented LLMs. When an instrumented LLM generates content related to phishing, it will automatically include detectable tags, enabling easier identification. Based on the design on implicit and explicit triggers and tags, we consider four distinct scenarios in our work. We evaluate our method from three key perspectives: stealthiness, effectiveness, and robustness, and compare it with existing baseline methods. Experimental results show that our method outperforms the baselines, achieving over 90% detection accuracy across all scenarios.', 'abstract_zh': '随着大型语言模型的快速发展，它们恶意使用尤其是生成钓鱼内容的潜在威胁日益突出。利用大型语言模型的能力，恶意用户可以合成无拼写错误和其他易检测特征的钓鱼邮件。此外，这些模型可以生成主题特定的钓鱼信息，量身定制内容以适应目标领域，从而提高成功几率。\n检测此类内容仍然是一项重大挑战，因为大型语言模型生成的钓鱼邮件往往缺乏清晰或可区分的语言特征。因此，大多数现有的语义级检测方法难以可靠地识别它们。虽然某些基于大型语言模型的检测方法显示出潜力，但它们受高性能的制约，计算成本高，使其不适合大规模部署。\n为了解决这一问题，我们提出Paladin，该方法通过各种插入策略将触发器-标签关联嵌入到基本的大规模语言模型中，将其转化为带有监测功能的大规模语言模型。当带有监测功能的大规模语言模型生成与钓鱼相关的内容时，将自动包含可检测的标签，从而便于识别。基于隐式和显式触发器及标签的设计，我们在工作中考虑了四种不同的场景。我们从隐蔽性、有效性及稳健性三个方面评估该方法，并与现有基准方法进行比较。实验结果表明，我们的方法优于基准方法，在所有场景中达到超过90%的检测准确率。', 'title_zh': 'Paladin: 以新型触发-标签范式防御基于LLM的钓鱼邮件'}
{'arxiv_id': 'arXiv:2509.07253', 'title': 'Benchmarking Information Retrieval Models on Complex Retrieval Tasks', 'authors': 'Julian Killingback, Hamed Zamani', 'link': 'https://arxiv.org/abs/2509.07253', 'abstract': 'Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable general-purpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent a natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on a comprehensive set of diverse complex tasks. The few resources that do exist feature a limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, we construct a diverse and realistic set of complex retrieval tasks and benchmark a representative set of state-of-the-art retrieval models. Additionally, we explore the impact of LLM-based query expansion and rewriting on retrieval quality. Our results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques.', 'abstract_zh': '大型语言模型（LLMs）是文本任务中的神奇且多功能工具，已使无数此前无法想象的应用成为可能。相比之下，检索模型尚未见证出现如此强大的通用模型。为了实现这一目标，检索模型必须能够执行复杂的检索任务，其中查询包含多个部分、约束或自然语言要求。这些任务代表了从现有大量常用评估集中使用的简单单方面查询的自然进阶。复杂查询自然产生，因为人们期望搜索引擎能够处理更具体和经常是雄心勃勃的信息请求，这在人们使用基于LLM的信息系统中得到了体现。尽管人们对检索模型在复杂检索任务中的能力扩展存在日益增长的需求，但由于缺乏全面多样复杂任务的评估资源，这仍然是一个挑战。现有的有限资源范围有限且常缺乏现实设置，使得难以了解检索模型在复杂现实检索任务中的真正能力。为解决这一不足并促进下一代检索模型的创新，我们构建了一组多样且现实的复杂检索任务，并对标记的先进检索模型进行了基准测试。此外，我们还探讨了基于LLM的查询扩展和重写对检索质量的影响。我们的结果表明，即使是最好的模型，在所有任务中的最高平均nDCG@10仅为0.346，R@100仅为0.587，也难以产生高质量的检索结果。尽管LLM增强可以有助于较弱的模型，但最强的模型在所有评估指标上使用所有重写技术后性能反而下降。', 'title_zh': '在复杂检索任务中 benchmark 信息检索模型'}
{'arxiv_id': 'arXiv:2509.07238', 'title': 'Systematic Optimization of Open Source Large Language Models for Mathematical Reasoning', 'authors': 'Pranav Pawar, Dhwaj Jain, Varun Gupta, Kaustav Dedhia, Dashrath Kale, Sudhir Dhekane', 'link': 'https://arxiv.org/abs/2509.07238', 'abstract': 'This paper presents a practical investigation into fine-tuning model parameters for mathematical reasoning tasks through experimenting with various configurations including randomness control, reasoning depth, and sampling strategies, careful tuning demonstrates substantial improvements in efficiency as well as performance. A holistically optimized framework is introduced for five state-of-the-art models on mathematical reasoning tasks, exhibiting significant performance boosts while maintaining solution correctness. Through systematic parameter optimization across Qwen2.5-72B, Llama-3.1-70B, DeepSeek-V3, Mixtral-8x22B, and Yi-Lightning, consistent efficiency gains are demonstrated with 100% optimization success rate. The methodology achieves an average 29.4% reduction in computational cost and 23.9% improvement in inference speed across all tested models. This framework systematically searches parameter spaces including temperature (0.1-0.5), reasoning steps (4-12), planning periods (1-4), and nucleus sampling (0.85-0.98), determining optimal configurations through testing on mathematical reasoning benchmarks. Critical findings show that lower temperature regimes (0.1-0.4) and reduced reasoning steps (4-6) consistently enhance efficiency without compromising accuracy. DeepSeek-V3 achieves the highest accuracy at 98%, while Mixtral-8x22B delivers the most cost-effective performance at 361.5 tokens per accurate response. Key contributions include: (1) the first comprehensive optimization study for five diverse SOTA models in mathematical reasoning, (2) a standardized production-oriented parameter optimization framework, (3) discovery of universal optimization trends applicable across model architectures, and (4) production-ready configurations with extensive performance characterization.', 'abstract_zh': '基于数学推理任务的模型参数精细化调整的实证研究', 'title_zh': '开源大型语言模型在数学推理中的系统优化'}
{'arxiv_id': 'arXiv:2509.07188', 'title': 'DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge', 'authors': 'Zonghai Yao, Michael Sun, Won Seok Jang, Sunjae Kwon, Soie Kwon, Hong Yu', 'link': 'https://arxiv.org/abs/2509.07188', 'abstract': "Discharge communication is a critical yet underexplored component of patient care, where the goal shifts from diagnosis to education. While recent large language model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they fail to evaluate models' ability to support patients after the visit. We introduce DischargeSim, a novel benchmark that evaluates LLMs on their ability to act as personalized discharge educators. DischargeSim simulates post-visit, multi-turn conversations between LLM-driven DoctorAgents and PatientAgents with diverse psychosocial profiles (e.g., health literacy, education, emotion). Interactions are structured across six clinically grounded discharge topics and assessed along three axes: (1) dialogue quality via automatic and LLM-as-judge evaluation, (2) personalized document generation including free-text summaries and structured AHRQ checklists, and (3) patient comprehension through a downstream multiple-choice exam. Experiments across 18 LLMs reveal significant gaps in discharge education capability, with performance varying widely across patient profiles. Notably, model size does not always yield better education outcomes, highlighting trade-offs in strategy use and content prioritization. DischargeSim offers a first step toward benchmarking LLMs in post-visit clinical education and promoting equitable, personalized patient support.", 'abstract_zh': '出院沟通是一个关键但尚未充分探索的患者护理组成部分，其目标从诊断转向教育。虽然最近的大语言模型基准测试强调就诊中的诊断推理，但它们未能评估模型在就诊后支持患者的能力。我们引入了DischargeSim，这是一个新的基准测试，用于评估大语言模型在扮演个性化出院教育者角色方面的能力。DischargeSim 模拟了在就诊后的多轮对话，其中由大语言模型驱动的DoctorAgents与具有多样心理社会特征的PatientAgents（如健康素养、教育背景、情绪）进行交互。交互围绕六个临床主题结构化，并从三个维度进行评估：（1）通过自动评估和大语言模型评判的对话质量，（2）个性化文档生成，包括自由文本摘要和结构化的AHRQ检查单，以及（3）通过下游选择题考试评估患者的理解能力。在18个大语言模型上的实验揭示了出院教育能力存在显著差距，不同患者群体的表现差异很大。值得注意的是，模型大小并不总是产生更好的教育结果，这突显了策略使用和内容优先级之间的权衡。DischargeSim 代表了在就诊后临床教育中评估大语言模型的一个初步步骤，并促进了公平和个性化的患者支持。', 'title_zh': 'DischargeSim: 一个用于教育性出院医生-患者沟通模拟的基准测试'}
{'arxiv_id': 'arXiv:2509.07149', 'title': 'Measuring Uncertainty in Transformer Circuits with Effective Information Consistency', 'authors': 'Anatoly A. Krasnovsky', 'link': 'https://arxiv.org/abs/2509.07149', 'abstract': 'Mechanistic interpretability has identified functional subgraphs within large language models (LLMs), known as Transformer Circuits (TCs), that appear to implement specific algorithms. Yet we lack a formal, single-pass way to quantify when an active circuit is behaving coherently and thus likely trustworthy. Building on prior systems-theoretic proposals, we specialize a sheaf/cohomology and causal emergence perspective to TCs and introduce the Effective-Information Consistency Score (EICS). EICS combines (i) a normalized sheaf inconsistency computed from local Jacobians and activations, with (ii) a Gaussian EI proxy for circuit-level causal emergence derived from the same forward state. The construction is white-box, single-pass, and makes units explicit so that the score is dimensionless. We further provide practical guidance on score interpretation, computational overhead (with fast and exact modes), and a toy sanity-check analysis. Empirical validation on LLM tasks is deferred.', 'abstract_zh': '机制可解释性已在大型语言模型（LLMs）中识别出功能子图，这些子图被称为Transformer电路（TCs），并似乎实现了特定的算法。然而，我们缺乏一种形式化的单一通过方法来定量衡量活跃电路是否表现出一致行为，从而可能值得信赖。基于先前的系统理论提案，我们将层/上同调和因果涌现视角专门化于TCs，并引入有效信息一致性分数（EICS）。EICS结合了（i）从局部雅可比矩阵和激活计算的归一化层不一致性，以及（ii）从前向状态派生的高斯有效信息代理，表示电路层面的因果涌现。该构建是白盒的，单一通过的，并使单元显式化，从而使分数无量纲。我们进一步提供了分数解释的实用指导、计算开销（包括快速和精确模式）以及一个玩具级别的合理性检查分析。对LLM任务的经验验证留待以后。', 'title_zh': '基于有效信息一致性测量变压器电路中的不确定性'}
{'arxiv_id': 'arXiv:2509.07142', 'title': 'Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models', 'authors': "Zhiyin Tan, Jennifer D'Souza", 'link': 'https://arxiv.org/abs/2509.07142', 'abstract': 'This study presents a framework for automated evaluation of dynamically evolving topic models using Large Language Models (LLMs). Topic modeling is essential for organizing and retrieving scholarly content in digital library systems, helping users navigate complex and evolving knowledge domains. However, widely used automated metrics, such as coherence and diversity, often capture only narrow statistical patterns and fail to explain semantic failures in practice. We introduce a purpose-oriented evaluation framework that employs nine LLM-based metrics spanning four key dimensions of topic quality: lexical validity, intra-topic semantic soundness, inter-topic structural soundness, and document-topic alignment soundness. The framework is validated through adversarial and sampling-based protocols, and is applied across datasets spanning news articles, scholarly publications, and social media posts, as well as multiple topic modeling methods and open-source LLMs. Our analysis shows that LLM-based metrics provide interpretable, robust, and task-relevant assessments, uncovering critical weaknesses in topic models such as redundancy and semantic drift, which are often missed by traditional metrics. These results support the development of scalable, fine-grained evaluation tools for maintaining topic relevance in dynamic datasets. All code and data supporting this work are accessible at this https URL.', 'abstract_zh': '基于大型语言模型的动态演变主题模型自动评估框架', 'title_zh': '面向目标导向的主题模型评估能力提升'}
{'arxiv_id': 'arXiv:2509.07025', 'title': '1 bit is all we need: binary normalized neural networks', 'authors': 'Eduardo Lobo Lustoda Cabral, Paulo Pirozelli, Larissa Driemeier', 'link': 'https://arxiv.org/abs/2509.07025', 'abstract': 'The increasing size of large neural network models, specifically language models and foundational image models, poses deployment challenges, prompting efforts to reduce memory requirements and enhance computational efficiency. These efforts are critical to ensure practical deployment and effective utilization of these models across various applications. In this work, a novel type of neural network layers and models is developed that uses only single-bit parameters. In this novel type of models all parameters of all layers, including kernel weights and biases, only have values equal to zero or one. This novel type of models uses layers named as binary normalized layer. These binary normalized layers can be of any type, such as fully connected, convolutional, attention, etc., and they consist of slight variations of the corresponding conventional layers. To show the effectiveness of the binary normalized layers, two different models are configured to solve a multiclass image classification problem and a language decoder to predict the next token of a sequence. The model to solve the image classification has convolutional and fully connected layers, and the language model is composed of transformer blocks with multi-head attention. The results show that models with binary normalized layers present almost the same results obtained by equivalent models with real 32-bit parameters. The binary normalized layers allow to develop models that use 32 times less memory than current models and have equivalent performance. Besides, the binary normalized layers can be easily implemented on current computers using 1-bit arrays, and do not require the development of dedicated electronic hardware. This novel type of layers opens a new era for large neural network models with reduced memory requirements that can be deployed using simple and cheap hardware, such as mobile devices or only cpus.', 'abstract_zh': '大型神经网络模型（尤其是语言模型和基础图像模型）规模的不断增加，提出了部署挑战，促使人们努力减少内存需求和提升计算效率。这种努力对于确保这些模型在各种应用中的实际部署和有效利用至关重要。在本工作中，开发了一种新型神经网络层和模型，这些模型仅使用单比特参数。在这些新型模型中，所有层的全部参数，包括卷积核权重和偏差，仅具有0或1的值。这些新型模型使用名为二元归一化层的层。这些二元归一化层可以是任何类型，如全连接层、卷积层、注意力层等，它们是由相应常规层的小幅度变体构成的。为了展示二元归一化层的有效性，配置了两种不同的模型来解决多类图像分类问题和语言解码任务以预测序列的下一个标记。用于解决图像分类的模型包含卷积层和全连接层，语言模型由包含多头注意力机制的Transformer块组成。结果显示，具有二元归一化层的模型在性能上几乎与使用真实32位参数的等效模型相同。二元归一化层使得可以开发出使用当前模型32倍少的内存且具有同等性能的模型。此外，二元归一化层可以在现有计算机上通过使用1比特数组轻松实现，并不需要开发专用的电子硬件。这种新型层为使用简单且廉价的硬件（如移动设备或仅CPU）进行部署的大规模神经网络模型提供了新的时代。', 'title_zh': '1比特足矣：归一化二值神经网络'}
{'arxiv_id': 'arXiv:2509.07022', 'title': 'Preventing Another Tessa: Modular Safety Middleware For Health-Adjacent AI Assistants', 'authors': 'Pavan Reddy, Nithin Reddy', 'link': 'https://arxiv.org/abs/2509.07022', 'abstract': 'In 2023, the National Eating Disorders Association\'s (NEDA) chatbot Tessa was suspended after providing harmful weight-loss advice to vulnerable users-an avoidable failure that underscores the risks of unsafe AI in healthcare contexts. This paper examines Tessa as a case study in absent safety engineering and demonstrates how a lightweight, modular safeguard could have prevented the incident. We propose a hybrid safety middleware that combines deterministic lexical gates with an in-line large language model (LLM) policy filter, enforcing fail-closed verdicts and escalation pathways within a single model call. Using synthetic evaluations, we show that this design achieves perfect interception of unsafe prompts at baseline cost and latency, outperforming traditional multi-stage pipelines. Beyond technical remedies, we map Tessa\'s failure patterns to established frameworks (OWASP LLM Top10, NIST SP 800-53), connecting practical safeguards to actionable governance controls. The results highlight that robust, auditable safety in health-adjacent AI does not require heavyweight infrastructure: explicit, testable checks at the last mile are sufficient to prevent "another Tessa", while governance and escalation ensure sustainability in real-world deployment.', 'abstract_zh': '2023年， NATIONAL EATING DISORDERS ASSOCIATION (NEDA)的聊天机器人Tessa因向脆弱用户提供了有害的减肥建议而被暂停—这是一个可避免的失败，凸显了医疗保健领域不安全AI的风险。本文以Tessa为例，探讨缺失的安全工程，并展示一种轻量级、模块化的保护措施如何可以防止此类事件。我们提出了一种混合安全中间件，结合确定性的词典门和内置的大规模语言模型（LLM）策略过滤器，确保在一个模型调用中实现封闭故障判断和升级路径。通过合成评估，我们证明这种设计可以在基线成本和延迟下完美拦截不安全的提示，优于传统的多阶段管道。除了技术补救措施，我们将Tessa的失败模式映射到现有的框架（OWASP LLM Top10, NIST SP 800-53），将实用的保护措施与可行的治理控制相结合。结果表明，健康相关领域中的稳健、可审计的安全性不需要重架构设：在最后一公里进行明确、可测试的检查就足以防止“另一个Tessa”，而治理和升级确保其实现实世界部署中的可持续性。', 'title_zh': '防止另一个特莎：健康相关AI助手的模块化安全中间件'}
{'arxiv_id': 'arXiv:2509.07006', 'title': 'ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code', 'authors': 'Kapil Madan', 'link': 'https://arxiv.org/abs/2509.07006', 'abstract': "This paper introduces ArGen (Auto-Regulation of Generative AI systems), a framework for aligning Large Language Models (LLMs) with complex sets of configurable, machine-readable rules spanning ethical principles, operational safety protocols, and regulatory compliance standards. Moving beyond just preference-based alignment, ArGen is designed to ensure LLMs adhere to these multifaceted policies through a novel synthesis of principle-based automated reward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy Agent (OPA) inspired governance layer. This approach provides the technical foundation for achieving and demonstrating compliance with diverse and nuanced governance requirements. To showcase the framework's capability to operationalize a deeply nuanced and culturally-specific value system, we present an in-depth case study: the development of a medical AI assistant guided by principles from Dharmic ethics (such as Ahimsa and Dharma), as derived from texts like the Bhagavad Gita. This challenging application demonstrates ArGen's adaptability, achieving a 70.9% improvement in domain-scope adherence over the baseline. Through our open-source repository, we show that ArGen's methodology offers a path to 'Governable Al' systems that are technically proficient, ethically robust, and verifiably compliant for safe deployment in diverse global contexts.", 'abstract_zh': '本文介绍了ArGen（生成AI系统的自动调节框架），这是一种用于使大型语言模型（LLMs）与复杂的可配置、机器可读规则对齐的框架，这些规则涵盖伦理原则、运营安全协议和合规标准。ArGen超越了基于偏好对齐的局限，通过一种新颖的原则导向的自动化奖励评分合成、Group Relative Policy Optimisation (GRPO) 以及受Open Policy Agent (OPA) 启发的治理层，设计用于确保LLMs遵循这些多维度的政策。该方法为满足和展示多样且细腻的治理要求的技术基础提供了支撑。为了展示该框架在实现深刻细腻与文化特定价值系统方面的能力，我们呈现了一个深入的应用案例研究：以《薄伽梵歌》等文本中提取的Dharmic伦理原则（如Ahimsa和Dharma）指导的医疗AI助手的开发。这一具有挑战性的应用展示了ArGen的适应性，实现了70.9%的领域范围一致性改进。通过我们开源的代码库，我们表明ArGen的方法为在多样化的全球背景下实现技术娴熟、伦理稳健且可验证合规的‘可治理AI’系统提供了途径。', 'title_zh': 'ArGen: 通过GRPO和Policy-as-Code自动调节生成型AI'}
{'arxiv_id': 'arXiv:2509.06996', 'title': 'Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems', 'authors': 'Jie Zhang, Ting Xu, Gelei Deng, Runyi Hu, Han Qiu, Tianwei Zhang, Qing Guo, Ivor Tsang', 'link': 'https://arxiv.org/abs/2509.06996', 'abstract': "Writing is a universal cultural technology that reuses vision for symbolic communication. Humans display striking resilience: we readily recognize words even when characters are fragmented, fused, or partially occluded. This paper investigates whether advanced vision language models (VLMs) share this resilience. We construct two psychophysics inspired benchmarks across distinct writing systems, Chinese logographs and English alphabetic words, by splicing, recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli for models while remaining legible to humans. Despite strong performance on clean text, contemporary VLMs show a severe drop under these perturbations, frequently producing unrelated or incoherent outputs. The pattern suggests a structural limitation: models heavily leverage generic visual invariances but under rely on compositional priors needed for robust literacy. We release stimuli generation code, prompts, and evaluation protocols to facilitate transparent replication and follow up work. Our findings motivate architectures and training strategies that encode symbol segmentation, composition, and binding across scripts, and they delineate concrete challenges for deploying multimodal systems in education, accessibility, cultural heritage, and security.", 'abstract_zh': '高级视觉语言模型在模糊文字下的弹性研究：基于心理物理学的跨书写系统基准测试揭示结构局限与应用场景挑战', 'title_zh': '可见却又不可读：跨书写系统的视觉语言模型系统性盲点'}
{'arxiv_id': 'arXiv:2509.06982', 'title': 'CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention', 'authors': 'Xiaomeng Hu, Fei Huang, Chenhan Yuan, Junyang Lin, Tsung-Yi Ho', 'link': 'https://arxiv.org/abs/2509.06982', 'abstract': 'As large language models (LLMs) are increasingly deployed in real-world applications, ensuring the safety of their outputs during decoding has become a critical challenge. However, existing decoding-time interventions, such as Contrastive Decoding, often force a severe trade-off between safety and response quality. In this work, we propose CARE, a novel framework for decoding-time safety alignment that integrates three key components: (1) a guard model for real-time safety monitoring, enabling detection of potentially unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe outputs efficiently at an earlier stage without disrupting the user experience; and (3) a novel introspection-based intervention strategy, where the model generates self-reflective critiques of its previous outputs and incorporates these reflections into the context to guide subsequent decoding steps. The framework achieves a superior safety-quality trade-off by using its guard model for precise interventions, its rollback mechanism for timely corrections, and our novel introspection method for effective self-correction. Experimental results demonstrate that our framework achieves a superior balance of safety, quality, and efficiency, attaining a low harmful response rate and minimal disruption to the user experience while maintaining high response quality.', 'abstract_zh': '随着大型语言模型（LLMs）在实际应用中的逐步部署，确保其解码输出的安全性已成为一个关键挑战。然而，现有的解码时干预方法，如对比解码，往往在安全性和响应质量之间造成了严重的权衡。在这项工作中，我们提出了CARE，一种新颖的解码时安全性对齐框架，集成三个关键组件：（1）一个守护模型进行实时安全性监控，实现对潜在不安全内容的检测；（2）一个带有令牌缓冲区的回滚机制，能够在早期阶段高效纠正不安全的输出，而不干扰用户体验；以及（3）一种新颖的内省驱动干预策略，模型生成对其先前输出的自省性批评，并将这些反思纳入上下文中，引导后续解码步骤。该框架通过其守护模型实现精确干预、回滚机制实现及时修正，以及我们新颖的内省方法实现有效的自我纠正，从而实现安全性与质量的优异权衡。实验结果表明，我们的框架在安全、质量和效率之间实现了优越的平衡，实现了低有害响应率和最小化的用户体验干扰，同时保持了高响应质量。', 'title_zh': 'CARE: 通过回滚和反省干预实现时间安全性对齐'}
{'arxiv_id': 'arXiv:2509.06980', 'title': 'RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use', 'authors': 'Jiajun Chai, Guojun Yin, Zekun Xu, Chuhuai Yue, Yi Jia, Siyu Xia, Xiaohan Wang, Jiwen Jiang, Xiaoguang Li, Chengqi Dong, Hang He, Wei Lin', 'link': 'https://arxiv.org/abs/2509.06980', 'abstract': 'Large language models excel at basic reasoning but struggle with tasks that require interaction with external tools. We present RLFactory, a plug-and-play reinforcement learning post-training framework for multi-round tool use. RLFactory tackles (i) tool-call stability and adaptability amid tool heterogeneity and interface issues via an asyncio-based asynchronous caller and a decoupled tool/training architecture, and (ii) diverse evaluation needs via a reward layer supporting rule-based, model-judgment, and tool-verification signals. It reconstructs the MDP by introducing observation markers from tool feedback, closing the loop among model, tools, and environment, and implements a generate-parse-invoke-update workflow for dynamic policy optimization. On Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural Questions (NQ) dataset, surpassing larger models trained with similar techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable framework for strengthening multi-round tool use of LLMs in real-world scenarios. Code: this https URL.', 'abstract_zh': '大型语言模型在基本推理方面表现出色，但在需要外部工具交互的任务上遇到困难。我们提出了RLFactory，一种异步插件式强化学习后训练框架，支持多轮工具使用。RLFactory通过基于asyncio的异步调用器和解耦工具/训练架构解决了(i)工具异质性和接口问题带来的调用稳定性和适应性问题；通过奖励层支持基于规则、模型判断和工具验证信号，解决(ii)多样的评估需求。它通过引入来自工具反馈的观察标记重建MDP，实现模型、工具和环境之间的闭环，并采用生成-解析-调用-更新的工作流来实现动态策略优化。在Search-R1上使用Qwen3-4B，RLFactory在Natural Questions (NQ)数据集上的测试得分为0.486，超过了使用类似技术训练的更大模型（如Qwen2.5-7B-Instruct-GRPO的0.473），并将训练吞吐量提高了6.8倍。RLFactory为增强LLM在实际场景中的多轮工具使用提供了低门槛、高度适应的框架。代码：this https URL。', 'title_zh': 'RLFactory: 一种即插即用的大型语言模型多轮工具使用后训练 reinforcement learning 框架'}
{'arxiv_id': 'arXiv:2509.04827', 'title': 'VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving', 'authors': 'Jiahuan Yu, Aryan Taneja, Junfeng Lin, Minjia Zhang', 'link': 'https://arxiv.org/abs/2509.04827', 'abstract': 'Modern Large Language Model (LLM) serving systems increasingly support interactive applications, like real-time chat assistants, code generation tools, and agentic workflows. However, the soaring energy cost of LLM inference presents a growing challenge for sustainable and cost-effective deployment. This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM serving, built from a control theory perspective. VoltanaLLM co-designs frequency scaling and request routing in emerging prefill/decode disaggregated architectures, leveraging their decoupled execution to enable fine-grained phase-specific control. It consists of a feedback-driven frequency controller that dynamically adapts GPU frequency for prefill and decode phases, and a state-space router that explores routing decisions across frequency-scaled instances to minimize energy under latency constraints. We implement VoltanaLLM in SGLang and evaluate its performance over multiple state-of-the-art LLMs and real-world datasets. The results demonstrate that VoltanaLLM achieves up to 36.3% energy savings while maintaining near-perfect SLO attainment rate, paving the way for sustainable and intelligent LLM serving.', 'abstract_zh': '基于控制理论的SLO感知高效能Large Language Model服务系统VoltanaLLM及其能量效益分析', 'title_zh': 'VoltanaLLM：基于反馈的频率控制和状态空间路由的高效LLM服务'}
