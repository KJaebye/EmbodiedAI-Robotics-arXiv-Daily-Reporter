# ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus 

**Title (ZH)**: ARC-NCA: 向抽象与推理语料库的发展性解决方案迈进 

**Authors**: Etienne Guichard, Felix Reimers, Mia Kvalsund, Mikkel Lepperød, Stefano Nichele  

**Link**: [PDF](https://arxiv.org/pdf/2505.08778)  

**Abstract**: The Abstraction and Reasoning Corpus (ARC), later renamed ARC-AGI, poses a fundamental challenge in artificial general intelligence (AGI), requiring solutions that exhibit robust abstraction and reasoning capabilities across diverse tasks, while only few (with median count of three) correct examples are presented. While ARC-AGI remains very challenging for artificial intelligence systems, it is rather easy for humans. This paper introduces ARC-NCA, a developmental approach leveraging standard Neural Cellular Automata (NCA) and NCA enhanced with hidden memories (EngramNCA) to tackle the ARC-AGI benchmark. NCAs are employed for their inherent ability to simulate complex dynamics and emergent patterns, mimicking developmental processes observed in biological systems. Developmental solutions may offer a promising avenue for enhancing AI's problem-solving capabilities beyond mere training data extrapolation. ARC-NCA demonstrates how integrating developmental principles into computational models can foster adaptive reasoning and abstraction. We show that our ARC-NCA proof-of-concept results may be comparable to, and sometimes surpass, that of ChatGPT 4.5, at a fraction of the cost. 

**Abstract (ZH)**: ARC抽象与推理语料库（ARC-AGI）及其在通用人工智能中的挑战：一种基于标准神经细胞自动机（NCA）及其增强版（EngramNCA）的发育性方法 

---
# DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models 

**Title (ZH)**: DeepMath-创意: 评估大型语言模型数学创造力的标准基准 

**Authors**: Xiaoyang Chen, Xinan Dai, Yu Du, Qian Feng, Naixu Guo, Tingshuo Gu, Yuting Gao, Yingyi Gao, Xudong Han, Xiang Jiang, Yilin Jin, Hongyi Lin, Shisheng Lin, Xiangnan Li, Yuante Li, Yixing Li, Zhentao Lai, Zilu Ma, Yingrong Peng, Jiacheng Qian, Hao-Yu Sun, Jianbo Sun, Zirui Wang, Siwei Wu, Zian Wang, Bin Xu, Jianghao Xu, Yiyang Yu, Zichuan Yang, Hongji Zha, Ruichong Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08744)  

**Abstract**: To advance the mathematical proficiency of large language models (LLMs), the DeepMath team has launched an open-source initiative aimed at developing an open mathematical LLM and systematically evaluating its mathematical creativity. This paper represents the initial contribution of this initiative. While recent developments in mathematical LLMs have predominantly emphasized reasoning skills, as evidenced by benchmarks on elementary to undergraduate-level mathematical tasks, the creative capabilities of these models have received comparatively little attention, and evaluation datasets remain scarce. To address this gap, we propose an evaluation criteria for mathematical creativity and introduce DeepMath-Creative, a novel, high-quality benchmark comprising constructive problems across algebra, geometry, analysis, and other domains. We conduct a systematic evaluation of mainstream LLMs' creative problem-solving abilities using this dataset. Experimental results show that even under lenient scoring criteria -- emphasizing core solution components and disregarding minor inaccuracies, such as small logical gaps, incomplete justifications, or redundant explanations -- the best-performing model, O3 Mini, achieves merely 70% accuracy, primarily on basic undergraduate-level constructive tasks. Performance declines sharply on more complex problems, with models failing to provide substantive strategies for open problems. These findings suggest that, although current LLMs display a degree of constructive proficiency on familiar and lower-difficulty problems, such performance is likely attributable to the recombination of memorized patterns rather than authentic creative insight or novel synthesis. 

**Abstract (ZH)**: 为了提高大型语言模型的数学能力，DeepMath团队启动了一个开源项目，旨在开发一个开放的数学大型语言模型，并系统评估其数学创造力。本文代表了该项目的初步贡献。虽然近年来数学大型语言模型的发展主要强调了推理能力，特别是在小学到本科级别的数学任务基准测试中，这些模型的创造性能力却受到了相对较少的关注，评估数据集也仍然稀缺。为解决这一差距，我们提出了一套数学创造力的评估标准，并引入了DeepMath-Creative，这是一个全新的、高质量的数据集，涵盖了代数、几何、分析及其他领域的构造性问题。我们使用这一数据集对主流大型语言模型的创造性问题解决能力进行了系统的评估。实验结果表明，即使是放宽评分标准——强调核心解题要素，忽略如小的逻辑缺口、不完整的证明或冗余解释等琐碎不准确之处——表现最好的模型O3 Mini在基本本科级别构造性任务上的准确率也只有70%。在更复杂的问题上，模型的性能急剧下降，无法提供实质性的策略来解决开放性问题。这些发现表明，尽管当前的大型语言模型在熟悉的和难度较低的问题上显示了一定的构造性能力，但这种表现很可能是由于记忆模式的重组而非真正的创造力洞察或新颖的综合能力。 

---
# LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs 

**Title (ZH)**: 基于LLM的提示集合方法在EHR中实现可靠的医疗实体识别 

**Authors**: K M Sajjadul Islam, Ayesha Siddika Nipu, Jiawei Wu, Praveen Madiraju  

**Link**: [PDF](https://arxiv.org/pdf/2505.08704)  

**Abstract**: Electronic Health Records (EHRs) are digital records of patient information, often containing unstructured clinical text. Named Entity Recognition (NER) is essential in EHRs for extracting key medical entities like problems, tests, and treatments to support downstream clinical applications. This paper explores prompt-based medical entity recognition using large language models (LLMs), specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering techniques, including zero-shot, few-shot, and an ensemble approach. Among all strategies, GPT-4o with prompt ensemble achieved the highest classification performance with an F1-score of 0.95 and recall of 0.98, outperforming DeepSeek-R1 on the task. The ensemble method improved reliability by aggregating outputs through embedding-based similarity and majority voting. 

**Abstract (ZH)**: 电子健康记录（EHRs）中的基于提示的医学实体识别研究：利用大型语言模型（LLMs）并结合各种提示工程技术 

---
# A Study of Data-driven Methods for Inventory Optimization 

**Title (ZH)**: 基于数据驱动方法的库存优化研究 

**Authors**: Lee Yeung Ping, Patrick Wong, Tan Cheng Han  

**Link**: [PDF](https://arxiv.org/pdf/2505.08673)  

**Abstract**: This paper shows a comprehensive analysis of three algorithms (Time Series, Random Forest (RF) and Deep Reinforcement Learning) into three inventory models (the Lost Sales, Dual-Sourcing and Multi-Echelon Inventory Model). These methodologies are applied in the supermarket context. The main purpose is to analyse efficient methods for the data-driven. Their possibility, potential and current challenges are taken into consideration in this report. By comparing the results in each model, the effectiveness of each algorithm is evaluated based on several key performance indicators, including forecast accuracy, adaptability to market changes, and overall impact on inventory costs and customer satisfaction levels. The data visualization tools and statistical metrics are the indicators for the comparisons and show some obvious trends and patterns that can guide decision-making in inventory management. These tools enable managers to not only track the performance of different algorithms in real-time but also to drill down into specific data points to understand the underlying causes of inventory fluctuations. This level of detail is crucial for pinpointing inefficiencies and areas for improvement within the supply chain. 

**Abstract (ZH)**: 本文对三种算法（时间序列、随机森林（RF）和深度强化学习）在三种库存模型（缺货损失模型、双重 sourcing 模型和多层级库存模型）中的应用进行了全面分析。这些方法在超市背景下应用。主要目的 是分析数据驱动方法的有效性，并考虑其可能性、潜在优势及其当前挑战。通过在每个模型中比较结果，根据包括预测准确性、适应市场变化的能力以及对库存成本和顾客满意度的整体影响等关键绩效指标来评估每种算法的有效性。数据可视化工具和统计指标用于比较，并揭示了一些明显趋势和模式，这些模式可以指导库存管理中的决策。这些工具使管理者不仅能够实时监控不同算法的性能，还能深入特定数据点以理解库存波动的根本原因。这种详细的分析对于识别供应链中的不效率和改进领域至关重要。 

---
# WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented Generation 

**Title (ZH)**: WixQA：面向企业的检索增强生成多数据集基准 

**Authors**: Dvir Cohen, Lin Burg, Sviatoslav Pykhnivskyi, Hagit Gur, Stanislav Kovynov, Olga Atzmon, Gilad Barkan  

**Link**: [PDF](https://arxiv.org/pdf/2505.08643)  

**Abstract**: Retrieval-Augmented Generation (RAG) is a cornerstone of modern question answering (QA) systems, enabling grounded answers based on external knowledge. Although recent progress has been driven by open-domain datasets, enterprise QA systems need datasets that mirror the concrete, domain-specific issues users raise in day-to-day support scenarios. Critically, evaluating end-to-end RAG systems requires benchmarks comprising not only question--answer pairs but also the specific knowledge base (KB) snapshot from which answers were derived. To address this need, we introduce WixQA, a benchmark suite featuring QA datasets precisely grounded in the released KB corpus, enabling holistic evaluation of retrieval and generation components. WixQA includes three distinct QA datasets derived from this http URL customer support interactions and grounded in a snapshot of the public Wix Help Center KB: (i) WixQA-ExpertWritten, 200 real user queries with expert-authored, multi-step answers; (ii) WixQA-Simulated, 200 expert-validated QA pairs distilled from user dialogues; and (iii) WixQA-Synthetic, 6,222 LLM-generated QA pairs, with one pair systematically derived from each article in the knowledge base. We release the KB snapshot alongside the datasets under MIT license and provide comprehensive baseline results, forming a unique benchmark for evaluating enterprise RAG systems in realistic enterprise environments. 

**Abstract (ZH)**: 基于检索的生成（RAG）是现代问答（QA）系统的核心，使基于外部知识的 grounded 答案成为可能。尽管 Recent 进展主要得益于开放领域数据集，企业 QA 系统需要反映日常支持场景中用户提出的具体、领域特定问题的数据集。关键在于，评估端到端的 RAG 系统需要不仅包含问题-答案对，还包含生成这些答案的具体知识库（KB）快照的基准测试。为满足这一需求，我们引入了 WixQA，这是一个基准套件，包含精确基于发布知识库语料库的 QA 数据集，以实现对检索和生成组件的全面评估。WixQA 包括三个源自特定网址的客户支持互动并基于 Wix 帮助中心公开知识库快照的独立 QA 数据集：(i) WixQA-ExpertWritten，包含 200 个真实用户查询和专家撰写的多步答案；(ii) WixQA-Simulated，包含 200 组专家验证的问题-答案对，这些对是从用户对话中提炼出来的；(iii) WixQA-Synthetic，包含 6,222 对由大语言模型生成的问题-答案对，每一对均系统地来源于知识库中的每一篇文章。我们以 MIT 许可证发布知识库快照及其数据集，并提供全面的基础结果，形成一个独特的基准，用于在实际的企业环境中评估企业 RAG 系统。 

---
# TRAIL: Trace Reasoning and Agentic Issue Localization 

**Title (ZH)**: 轨迹推理与自主问题定位 

**Authors**: Darshan Deshpande, Varun Gangal, Hersh Mehta, Jitin Krishnan, Anand Kannappan, Rebecca Qian  

**Link**: [PDF](https://arxiv.org/pdf/2505.08638)  

**Abstract**: The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows. 

**Abstract (ZH)**: 随着代理工作流在多个领域的广泛应用，对这些系统产生的复杂追踪进行可扩展且系统的评价变得至关重要。当前的评价方法依赖于手动且针对特定领域的手工分析长流程追踪——这种方法随着代理输出的复杂性和数量增长而无法扩展。代理系统中错误分析进一步受到外部工具输出和语言模型推理的相互作用的影响，使其比传统软件调试更具挑战性。在本文中，我们（1）阐明了代理工作流追踪需要稳健且动态的评价方法的需求，（2）提出了代理系统中遇到的错误类型的正式分类法，（3）使用该分类法和基于已建立的代理基准构建了一个包含148个人标注的追踪数据集（TRAIL）。为了确保生态效度，我们从单代理和多代理系统中精选追踪，重点是软件工程和开放世界信息检索等实际应用。我们的评估结果显示，现代长上下文语言模型在追踪调试方面表现不佳，Gemini-2.5-pro模型在TRAIL上的得分仅为11%。我们的数据集和代码已公开发布，以支持并加速未来对代理工作流可扩展评价的研究。 

---
# Integrating Natural Language Processing and Exercise Monitoring for Early Diagnosis of Metabolic Syndrome: A Deep Learning Approach 

**Title (ZH)**: 将自然语言处理与运动监测 integrates 早期诊断代谢综合征：一种深度学习方法 

**Authors**: Yichen Zhao, Yuhua Wang, Xi Cheng, Junhao Fang, Yang Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08628)  

**Abstract**: Metabolic syndrome (MetS) is a medication condition characterized by abdominal obesity, insulin resistance, hypertension and hyperlipidemia. It increases the risk of majority of chronic diseases, including type 2 diabetes mellitus, and affects about one quarter of the global population. Therefore, early detection and timely intervention for MetS are crucial. Standard diagnosis for MetS components requires blood tests conducted within medical institutions. However, it is frequently underestimated, leading to unmet need for care for MetS population. This study aims to use the least physiological data and free texts about exercises related activities, which are obtained easily in daily life, to diagnosis MetS. We collected the data from 40 volunteers in a nursing home and used data augmentation to reduce the imbalance. We propose a deep learning framework for classifying MetS that integrates natural language processing (NLP) and exercise monitoring. The results showed that the best model reported a high positive result (AUROC=0.806 and REC=76.3%) through 3-fold cross-validation. Feature importance analysis revealed that text and minimum heart rate on a daily basis contribute the most in the classification of MetS. This study demonstrates the potential application of data that are easily measurable in daily life for the early diagnosis of MetS, which could contribute to reducing the cost of screening and management for MetS population. 

**Abstract (ZH)**: 代谢综合征（MetS）是一种以腹部肥胖、胰岛素抵抗、高血压和高脂血症为特征的代谢状况。它增加了2型糖尿病等多数慢性疾病的风险，并影响全球约四分之一的人口。因此，早期检测和及时干预代谢综合征至关重要。目前，代谢综合征组件的标准诊断需要在医疗机构进行血液检测。然而，它经常被低估，导致代谢综合征人群的护理需求未得到满足。本研究旨在利用日常生活易于获得的最少生理数据和与运动相关的自由文本，来诊断代谢综合征。我们从一所护理院的40名志愿者中收集数据，并使用数据增强以减少数据不平衡。我们提出了一种结合自然语言处理（NLP）和运动监测的深度学习框架，用于代谢综合征分类。结果表明，通过3折交叉验证，最佳模型的AUC值为0.806，召回率为76.3%。特征重要性分析显示，文本内容和每日最低心率对代谢综合征分类最具贡献。本研究展示了日常生活中易于测量的数据在代谢综合征早期诊断中的潜在应用，这有助于降低代谢综合征人群筛查和管理的成本。 

---
# Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models 

**Title (ZH)**: 视觉引导解码：无需梯度 hardness 命令反转swith 语言模型 

**Authors**: Donghoon Kim, Minji Bae, Kyuhong Shim, Byonghyo Shim  

**Link**: [PDF](https://arxiv.org/pdf/2505.08622)  

**Abstract**: Text-to-image generative models like DALL-E and Stable Diffusion have revolutionized visual content creation across various applications, including advertising, personalized media, and design prototyping. However, crafting effective textual prompts to guide these models remains challenging, often requiring extensive trial and error. Existing prompt inversion approaches, such as soft and hard prompt techniques, are not so effective due to the limited interpretability and incoherent prompt generation. To address these issues, we propose Visually Guided Decoding (VGD), a gradient-free approach that leverages large language models (LLMs) and CLIP-based guidance to generate coherent and semantically aligned prompts. In essence, VGD utilizes the robust text generation capabilities of LLMs to produce human-readable prompts. Further, by employing CLIP scores to ensure alignment with user-specified visual concepts, VGD enhances the interpretability, generalization, and flexibility of prompt generation without the need for additional training. Our experiments demonstrate that VGD outperforms existing prompt inversion techniques in generating understandable and contextually relevant prompts, facilitating more intuitive and controllable interactions with text-to-image models. 

**Abstract (ZH)**: 视觉引导解码（VGD）：利用大语言模型和CLIP指导生成连贯且语义对齐的文本提示 

---
# Resource-Efficient Language Models: Quantization for Fast and Accessible Inference 

**Title (ZH)**: 资源高效语言模型：量化实现快速可访问推理 

**Authors**: Tollef Emil Jørgensen  

**Link**: [PDF](https://arxiv.org/pdf/2505.08620)  

**Abstract**: Large language models have significantly advanced natural language processing, yet their heavy resource demands pose severe challenges regarding hardware accessibility and energy consumption. This paper presents a focused and high-level review of post-training quantization (PTQ) techniques designed to optimize the inference efficiency of LLMs by the end-user, including details on various quantization schemes, granularities, and trade-offs. The aim is to provide a balanced overview between the theory and applications of post-training quantization. 

**Abstract (ZH)**: 大型语言模型在自然语言处理领域取得了显著进展，但其对硬件资源的巨大需求给硬件访问能力和能源消耗带来了严重挑战。本文对后训练量化(PTQ)技术进行了聚焦和高层次的综述，这些技术旨在通过终端用户优化大型语言模型的推理效率，包括各种量化方案、粒度和权衡的详细内容。本文的目的是在后训练量化的理论与应用之间提供一个平衡的综述。 

---
# Guiding LLM-based Smart Contract Generation with Finite State Machine 

**Title (ZH)**: 基于有限状态机引导的大规模语言模型驱动的智能合约生成 

**Authors**: Hao Luo, Yuhao Lin, Xiao Yan, Xintong Hu, Yuxiang Wang, Qiming Zeng, Hao Wang, Jiawei Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08542)  

**Abstract**: Smart contract is a kind of self-executing code based on blockchain technology with a wide range of application scenarios, but the traditional generation method relies on manual coding and expert auditing, which has a high threshold and low efficiency. Although Large Language Models (LLMs) show great potential in programming tasks, they still face challenges in smart contract generation w.r.t. effectiveness and security. To solve these problems, we propose FSM-SCG, a smart contract generation framework based on finite state machine (FSM) and LLMs, which significantly improves the quality of the generated code by abstracting user requirements to generate FSM, guiding LLMs to generate smart contracts, and iteratively optimizing the code with the feedback of compilation and security checks. The experimental results show that FSM-SCG significantly improves the quality of smart contract generation. Compared to the best baseline, FSM-SCG improves the compilation success rate of generated smart contract code by at most 48%, and reduces the average vulnerability risk score by approximately 68%. 

**Abstract (ZH)**: 基于有限状态机和大语言模型的智能合约生成框架FSM-SCG 

---
# On the Complexity and Properties of Preferential Propositional Dependence Logic 

**Title (ZH)**: 偏好命题依赖逻辑的复杂性和性质 

**Authors**: Kai Sauerwald, Arne Meier, Juha Kontinen  

**Link**: [PDF](https://arxiv.org/pdf/2505.08522)  

**Abstract**: This paper considers the complexity and properties of KLM-style preferential reasoning in the setting of propositional logic with team semantics and dependence atoms, also known as propositional dependence logic. Preferential team-based reasoning is shown to be cumulative, yet violates System~P. We give intuitive conditions that fully characterise those cases where preferential propositional dependence logic satisfies System~P. We show that these characterisations do, surprisingly, not carry over to preferential team-based propositional logic. Furthermore, we show how classical entailment and dependence logic entailment can be expressed in terms of non-trivial preferential models. Finally, we present the complexity of preferential team-based reasoning for two natural representations. This includes novel complexity results for classical (non-team-based) preferential reasoning. 

**Abstract (ZH)**: 本文考虑了在命题逻辑与团队语义及依赖原子的设定下，KLM风格的优先推理的复杂性和性质。优先团队为基础的推理被证明是累积的，但违反了System P。我们给出了直观的条件，完全描述了优先命题依赖逻辑满足System P的那些情况。我们展示了这些描述出人意料地不适用于优先团队为基础的命题逻辑。此外，我们展示了经典蕴含和依赖逻辑蕴含如何用非平凡的优先模型来表达。最后，我们提出了优先团队为基础推理的复杂性，包括经典（非团队为基础）优先推理的新颖复杂性结果。 

---
# TrialMatchAI: An End-to-End AI-powered Clinical Trial Recommendation System to Streamline Patient-to-Trial Matching 

**Title (ZH)**: TrialMatchAI：一个端到端的AI驱动临床试验推荐系统，以简化患者与试验匹配过程 

**Authors**: Majd Abdallah, Sigve Nakken, Mariska Bierkens, Johanna Galvis, Alexis Groppi, Slim Karkar, Lana Meiqari, Maria Alexandra Rujano, Steve Canham, Rodrigo Dienstmann, Remond Fijneman, Eivind Hovig, Gerrit Meijer, Macha Nikolski  

**Link**: [PDF](https://arxiv.org/pdf/2505.08508)  

**Abstract**: Patient recruitment remains a major bottleneck in clinical trials, calling for scalable and automated solutions. We present TrialMatchAI, an AI-powered recommendation system that automates patient-to-trial matching by processing heterogeneous clinical data, including structured records and unstructured physician notes. Built on fine-tuned, open-source large language models (LLMs) within a retrieval-augmented generation framework, TrialMatchAI ensures transparency and reproducibility and maintains a lightweight deployment footprint suitable for clinical environments. The system normalizes biomedical entities, retrieves relevant trials using a hybrid search strategy combining lexical and semantic similarity, re-ranks results, and performs criterion-level eligibility assessments using medical Chain-of-Thought reasoning. This pipeline delivers explainable outputs with traceable decision rationales. In real-world validation, 92 percent of oncology patients had at least one relevant trial retrieved within the top 20 recommendations. Evaluation across synthetic and real clinical datasets confirmed state-of-the-art performance, with expert assessment validating over 90 percent accuracy in criterion-level eligibility classification, particularly excelling in biomarker-driven matches. Designed for modularity and privacy, TrialMatchAI supports Phenopackets-standardized data, enables secure local deployment, and allows seamless replacement of LLM components as more advanced models emerge. By enhancing efficiency and interpretability and offering lightweight, open-source deployment, TrialMatchAI provides a scalable solution for AI-driven clinical trial matching in precision medicine. 

**Abstract (ZH)**: 基于AI的TrialMatchAI推荐系统：自动化患者与临床试验匹配的解决方案 

---
# Achieving Scalable Robot Autonomy via neurosymbolic planning using lightweight local LLM 

**Title (ZH)**: 通过轻量级局部LLM实现基于神经符号规划的可扩展机器人自主性 

**Authors**: Nicholas Attolino, Alessio Capitanelli, Fulvio Mastrogiovanni  

**Link**: [PDF](https://arxiv.org/pdf/2505.08492)  

**Abstract**: PDDL-based symbolic task planning remains pivotal for robot autonomy yet struggles with dynamic human-robot collaboration due to scalability, re-planning demands, and delayed plan availability. Although a few neurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to address these challenges, reliance on closed-source, remote models with limited context introduced critical constraints: third-party dependency, inconsistent response times, restricted plan length and complexity, and multi-domain scalability issues. We present Gideon, a novel framework that enables the transition to modern, smaller, local LLMs with extended context length. Gideon integrates a novel problem generator to systematically generate large-scale datasets of realistic domain-problem-plan tuples for any domain, and adapts neurosymbolic planning for local LLMs, enabling on-device execution and extended context for multi-domain support. Preliminary experiments in single-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k samples, demonstrate a valid plan percentage of 66.1% (32k model) and show that the figure can be further scaled through additional data. Multi-domain tests on 16k samples yield an even higher 70.6% planning validity rate, proving extensibility across domains and signaling that data variety can have a positive effect on learning efficiency. Although long-horizon planning and reduced model size make Gideon training much less efficient than baseline models based on larger LLMs, the results are still significant considering that the trained model is about 120x smaller than baseline and that significant advantages can be achieved in inference efficiency, scalability, and multi-domain adaptability, all critical factors in human-robot collaboration. Training inefficiency can be mitigated by Gideon's streamlined data generation pipeline. 

**Abstract (ZH)**: 基于PDDL的符号任务规划在机器人自主性中仍然至关重要，但由于扩展性、重规划需求以及计划延迟可用性等问题，在动态人机协作中面临挑战。尽管一些神经符号框架曾利用如GPT-3等大规模语言模型来解决这些问题，但依赖闭源、远程模型带来的限制包括第三方依赖性、响应时间不一致、计划长度和复杂性受限以及多领域扩展性问题。我们提出了Gideon，一个新型框架，使其能够过渡到现代的小规模本地语言模型，并扩展上下文长度。Gideon 集成了一个新型问题生成器，可以系统地为任何领域生成大规模的现实世界领域-问题-计划三元组数据集，并针对本地语言模型适应神经符号规划，从而实现设备端执行和扩展上下文以支持多领域。单领域场景下使用Qwen-2.5 1.5B，在8k-32k样本训练下，初步实验显示计划有效性比例为66.1%（32k模型），并通过增加数据可以进一步扩大比例。多领域测试下使用16k样本，计划有效性比例高达70.6%，表明其在跨领域的应用具有扩展性，并暗示数据多样性可以提升学习效率。尽管面向长远规划和较小模型规模使得Gideon的训练效率远低于基于大规模模型的基线模型，但在训练模型规模仅为基线的1/120的情况下，Gideon在推理效率、扩展性和多领域适应性方面仍能获得显著优势，这些都是人机协作中的关键因素。Gideon简化的数据生成流程可以帮助缓解训练效率问题。 

---
# BAT: Benchmark for Auto-bidding Task 

**Title (ZH)**: BAT：自动竞价任务基准 

**Authors**: Alexandra Khirianova, Ekaterina Solodneva, Andrey Pudovikov, Sergey Osokin, Egor Samosvat, Yuriy Dorn, Alexander Ledovsky, Yana Zenkova  

**Link**: [PDF](https://arxiv.org/pdf/2505.08485)  

**Abstract**: The optimization of bidding strategies for online advertising slot auctions presents a critical challenge across numerous digital marketplaces. A significant obstacle to the development, evaluation, and refinement of real-time autobidding algorithms is the scarcity of comprehensive datasets and standardized benchmarks.
To address this deficiency, we present an auction benchmark encompassing the two most prevalent auction formats. We implement a series of robust baselines on a novel dataset, addressing the most salient Real-Time Bidding (RTB) problem domains: budget pacing uniformity and Cost Per Click (CPC) constraint optimization. This benchmark provides a user-friendly and intuitive framework for researchers and practitioners to develop and refine innovative autobidding algorithms, thereby facilitating advancements in the field of programmatic advertising. The implementation and additional resources can be accessed at the following repository (this https URL, this https URL). 

**Abstract (ZH)**: 在线广告拍卖竞价策略的优化是众多数字市场面临的关键挑战。由于全面数据集和标准化基准的稀缺性，实时自动化竞价算法的发展、评估和优化面临重要障碍。

为应对这一不足，我们提供了一个涵盖两种最常见的拍卖格式的拍卖基准。我们在新型数据集上实现了一系列稳健的基线，解决实时竞价（RTB）领域的最突出问题领域：预算 pacing 统一性和每点击成本（CPC）约束优化。该基准为研究人员和实践者提供了一个用户友好且直观的框架，以开发和优化创新的自动化竞价算法，从而推动程序化广告领域的发展。相关实现和额外资源可在以下仓库访问（this https URL, this https URL）。 

---
# Strategy-Augmented Planning for Large Language Models via Opponent Exploitation 

**Title (ZH)**: 基于对手利用的策略增强规划方法 LARGE LANGUAGE MODELS 

**Authors**: Shuai Xu, Sijia Cui, Yanna Wang, Bo Xu, Qi Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08459)  

**Abstract**: Efficiently modeling and exploiting opponents is a long-standing challenge in adversarial domains. Large Language Models (LLMs) trained on extensive textual data have recently demonstrated outstanding performance in general tasks, introducing new research directions for opponent modeling. Some studies primarily focus on directly using LLMs to generate decisions based on the elaborate prompt context that incorporates opponent descriptions, while these approaches are limited to scenarios where LLMs possess adequate domain expertise. To address that, we introduce a two-stage Strategy-Augmented Planning (SAP) framework that significantly enhances the opponent exploitation capabilities of LLM-based agents by utilizing a critical component, the Strategy Evaluation Network (SEN). Specifically, in the offline stage, we construct an explicit strategy space and subsequently collect strategy-outcome pair data for training the SEN network. During the online phase, SAP dynamically recognizes the opponent's strategies and greedily exploits them by searching best response strategy on the well-trained SEN, finally translating strategy to a course of actions by carefully designed prompts. Experimental results show that SAP exhibits robust generalization capabilities, allowing it to perform effectively not only against previously encountered opponent strategies but also against novel, unseen strategies. In the MicroRTS environment, SAP achieves a 85.35\% performance improvement over baseline methods and matches the competitiveness of reinforcement learning approaches against state-of-the-art (SOTA) rule-based AI. 

**Abstract (ZH)**: 高效建模和利用对手是对抗领域长期存在的挑战。大规模语言模型（LLMs）在广泛任务上的卓越表现最近引发了一系列关于对手建模的新研究方向。一些研究主要集中在直接使用LLMs根据包含对手描述的细致提示构建决策，但这些方法受限于LLMs具备足够的领域专业知识的场景。为此，我们提出了一种两阶段策略增强规划（SAP）框架，该框架通过利用关键组件——策略评估网络（SEN）大幅提升了基于LLM的代理的对手利用能力。具体而言，在离线阶段，我们构建了一个明确的策略空间并收集策略-结果对数据用于训练SEN网络。在在线阶段，SAP动态识别对手的策略并贪婪地利用这些策略通过在充分训练的SEN中搜索最优响应策略，最终通过精心设计的提示将策略转化为行动计划。实验结果显示，SAP展现出强大的泛化能力，不仅能够有效应对之前遇到的对手策略，还能应对全新的未见过的策略。在MicroRTS环境中，SAP相对于基线方法实现了85.35%的性能提升，并且在与最先进的基于规则的AI方法的对比中与强化学习方法保持了竞争力。 

---
# Adaptive Bias Generalized Rollout Policy Adaptation on the Flexible Job-Shop Scheduling Problem 

**Title (ZH)**: 柔性作业车间调度问题的自适应偏差广义展开策略适应性研究 

**Authors**: Lotfi Kobrosly, Marc-Emmanuel Coupvent des Graviers, Christophe Guettier, Tristan Cazenave  

**Link**: [PDF](https://arxiv.org/pdf/2505.08451)  

**Abstract**: The Flexible Job-Shop Scheduling Problem (FJSSP) is an NP-hard combinatorial optimization problem, with several application domains, especially for manufacturing purposes. The objective is to
efficiently schedule multiple operations on dissimilar machines. These operations are gathered into jobs, and operations pertaining to the same job need to be scheduled sequentially. Different methods have been previously tested to solve this problem, such as Constraint Solving, Tabu Search, Genetic Algorithms, or Monte Carlo Tree Search (MCTS). We propose a novel algorithm derived from the Generalized Nested Rollout Policy Adaptation, developed to solve the FJSSP. We report encouraging experimental results, as our algorithm performs better than other MCTS-based approaches, even if makespans obtained on large instances are still far from known upper bounds. 

**Abstract (ZH)**: 柔性作业-shop调度问题(FJSSP)是一个NP难的组合优化问题，尤其在制造领域中有广泛应用。目标是高效地安排多种操作在不同类型的机器上。这些操作被组织成作业，同一作业的操作需要顺序执行。已有多方法尝试解决此问题，如约束求解、 Tabu搜索、遗传算法或蒙特卡洛树搜索(MCTS)。我们提出了一种基于广义嵌套展开策略调整的新算法，用于解决FJSSP问题。实验结果令人鼓舞，我们的算法在某些方面优于其他基于MCTS的方法，尽管在大规模实例上的制造周期仍远高于已知的上界。 

---
# Agent-as-a-Service based on Agent Network 

**Title (ZH)**: 基于代理网络的代理即服务 

**Authors**: Yuhan Zhu, Haojie Liu, Jian Wang, Bing Li, Zikang Yin, Yefei Liao  

**Link**: [PDF](https://arxiv.org/pdf/2505.08446)  

**Abstract**: The rise of large model-based AI agents has spurred interest in Multi-Agent Systems (MAS) for their capabilities in decision-making, collaboration, and adaptability. While the Model Context Protocol (MCP) addresses tool invocation and data exchange challenges via a unified protocol, it lacks support for organizing agent-level collaboration. To bridge this gap, we propose Agent-as-a-Service based on Agent Network (AaaS-AN), a service-oriented paradigm grounded in the Role-Goal-Process-Service (RGPS) standard. AaaS-AN unifies the entire agent lifecycle, including construction, integration, interoperability, and networked collaboration, through two core components: (1) a dynamic Agent Network, which models agents and agent groups as vertexes that self-organize within the network based on task and role dependencies; (2) service-oriented agents, incorporating service discovery, registration, and interoperability protocols. These are orchestrated by a Service Scheduler, which leverages an Execution Graph to enable distributed coordination, context tracking, and runtime task management. We validate AaaS-AN on mathematical reasoning and application-level code generation tasks, which outperforms state-of-the-art baselines. Notably, we constructed a MAS based on AaaS-AN containing agent groups, Robotic Process Automation (RPA) workflows, and MCP servers over 100 agent services. We also release a dataset containing 10,000 long-horizon multi-agent workflows to facilitate future research on long-chain collaboration in MAS. 

**Abstract (ZH)**: 基于代理网络的代理即服务多智能体系统 

---
# Explaining Autonomous Vehicles with Intention-aware Policy Graphs 

**Title (ZH)**: 基于意图意识的策略图解释自动驾驶车辆 

**Authors**: Sara Montese, Victor Gimenez-Abalos, Atia Cortés, Ulises Cortés, Sergio Alvarez-Napagao  

**Link**: [PDF](https://arxiv.org/pdf/2505.08404)  

**Abstract**: The potential to improve road safety, reduce human driving error, and promote environmental sustainability have enabled the field of autonomous driving to progress rapidly over recent decades. The performance of autonomous vehicles has significantly improved thanks to advancements in Artificial Intelligence, particularly Deep Learning. Nevertheless, the opacity of their decision-making, rooted in the use of accurate yet complex AI models, has created barriers to their societal trust and regulatory acceptance, raising the need for explainability. We propose a post-hoc, model-agnostic solution to provide teleological explanations for the behaviour of an autonomous vehicle in urban environments. Building on Intention-aware Policy Graphs, our approach enables the extraction of interpretable and reliable explanations of vehicle behaviour in the nuScenes dataset from global and local perspectives. We demonstrate the potential of these explanations to assess whether the vehicle operates within acceptable legal boundaries and to identify possible vulnerabilities in autonomous driving datasets and models. 

**Abstract (ZH)**: 近年来，自主驾驶领域的快速发展得益于提高道路安全、减少人为驾驶错误和促进环境可持续性的潜力。得益人工智能尤其是深度学习的进步，自主车辆的表现显著提升。然而，其决策过程的不透明性，源于使用准确且复杂的AI模型，导致社会信任和监管接受度受到阻碍，因此需要提高透明度。我们提出了一种后验、模型无关的解决方案，以提供对城市环境中自主车辆行为的目的性解释。基于意图aware策略图，我们的方法可在全局和局部视角下从nuScenes数据集中提取可解释且可靠的车辆行为说明。我们展示了这些说明的潜在价值，以评估车辆是否在法律允许的范围内运行，并识别自主驾驶数据集和模型中的可能漏洞。 

---
# Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation 

**Title (ZH)**: 像人类一样学习：通过自适应难度课程学习和专家导向的自我重述提升LLM推理能力 

**Authors**: Enci Zhang, Xingang Yan, Wei Lin, Tianxiang Zhang, Qianchun Lu  

**Link**: [PDF](https://arxiv.org/pdf/2505.08364)  

**Abstract**: Despite impressive progress in areas like mathematical reasoning, large language models still face significant challenges in consistently solving complex problems. Drawing inspiration from key human learning strategies, we propose two novel strategies to enhance the capability of large language models to solve these complex problems. First, Adaptive Difficulty Curriculum Learning (ADCL) is a novel curriculum learning strategy that tackles the Difficulty Shift phenomenon (i.e., a model's perception of problem difficulty dynamically changes during training) by periodically re-estimating difficulty within upcoming data batches to maintain alignment with the model's evolving capabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel reinforcement learning strategy that bridges the gap between imitation learning and pure exploration by guiding models to reformulate expert solutions within their own conceptual framework, rather than relying on direct imitation, fostering deeper understanding and knowledge assimilation. Extensive experiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B as the base model, demonstrate that these human-inspired strategies synergistically and significantly enhance performance. Notably, their combined application improves performance over the standard Zero-RL baseline by 10% on the AIME24 benchmark and 16.6% on AIME25. 

**Abstract (ZH)**: 尽管在数学推理等领域取得了显著进展，大型语言模型在一致解决复杂问题方面仍然面临重大挑战。受关键的人类学习策略启发，我们提出了两种新型策略以增强大型语言模型解决复杂问题的能力。首先，自适应难度 Curriculum 学习（ADCL）是一种新的 Curriculum 学习策略，通过在即将到来的数据批次中定期重新评估难度来应对难度转移现象（即，在训练过程中模型对问题难度的感知动态变化），从而保持与模型不断演变的能力的同步。其次，专家引导自我重述（EGSR）是一种新的强化学习策略，通过引导模型在其自身概念框架内重述专家解决方案，而不是依赖直接模仿，从而弥补了模仿学习与纯粹探索之间的差距，促进更深层次的理解和知识吸收。在使用 Qwen2.5-7B 作为基准模型的具有挑战性的数学推理基准测试中，这些受人类启发的策略协同并显著提升了性能。值得注意的是，它们的联合应用分别在 AIME24 和 AIME25 基准测试中将性能提高了 10% 和 16.6%，超过了标准的 Zero-RL 基线。 

---
# Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning 

**Title (ZH)**: 基于语言引导可组装因果组件的 reinforcement learning 中 unseen 环境建模 

**Authors**: Xinyue Wang, Biwei Huang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08361)  

**Abstract**: Generalization in reinforcement learning (RL) remains a significant challenge, especially when agents encounter novel environments with unseen dynamics. Drawing inspiration from human compositional reasoning -- where known components are reconfigured to handle new situations -- we introduce World Modeling with Compositional Causal Components (WM3C). This novel framework enhances RL generalization by learning and leveraging compositional causal components. Unlike previous approaches focusing on invariant representation learning or meta-learning, WM3C identifies and utilizes causal dynamics among composable elements, facilitating robust adaptation to new tasks. Our approach integrates language as a compositional modality to decompose the latent space into meaningful components and provides theoretical guarantees for their unique identification under mild assumptions. Our practical implementation uses a masked autoencoder with mutual information constraints and adaptive sparsity regularization to capture high-level semantic information and effectively disentangle transition dynamics. Experiments on numerical simulations and real-world robotic manipulation tasks demonstrate that WM3C significantly outperforms existing methods in identifying latent processes, improving policy learning, and generalizing to unseen tasks. 

**Abstract (ZH)**: 基于组合因果组件的世界建模（WM3C）在强化学习中的泛化 

---
# An Identifiable Cost-Aware Causal Decision-Making Framework Using Counterfactual Reasoning 

**Title (ZH)**: 基于反事实推理的可识别成本意识因果决策框架 

**Authors**: Ruichu Cai, Xi Chen, Jie Qiao, Zijian Li, Yuequn Liu, Wei Chen, Keli Zhang, Jiale Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2505.08343)  

**Abstract**: Decision making under abnormal conditions is a critical process that involves evaluating the current state and determining the optimal action to restore the system to a normal state at an acceptable cost. However, in such scenarios, existing decision-making frameworks highly rely on reinforcement learning or root cause analysis, resulting in them frequently neglecting the cost of the actions or failing to incorporate causal mechanisms adequately. By relaxing the existing causal decision framework to solve the necessary cause, we propose a minimum-cost causal decision (MiCCD) framework via counterfactual reasoning to address the above challenges. Emphasis is placed on making counterfactual reasoning processes identifiable in the presence of a large amount of mixed anomaly data, as well as finding the optimal intervention state in a continuous decision space. Specifically, it formulates a surrogate model based on causal graphs, using abnormal pattern clustering labels as supervisory signals. This enables the approximation of the structural causal model among the variables and lays a foundation for identifiable counterfactual reasoning. With the causal structure approximated, we then established an optimization model based on counterfactual estimation. The Sequential Least Squares Programming (SLSQP) algorithm is further employed to optimize intervention strategies while taking costs into account. Experimental evaluations on both synthetic and real-world datasets reveal that MiCCD outperforms conventional methods across multiple metrics, including F1-score, cost efficiency, and ranking quality(nDCG@k values), thus validating its efficacy and broad applicability. 

**Abstract (ZH)**: 基于反事实推理的最小成本因果决策框架（MiCCD） 

---
# Benchmarking AI scientists in omics data-driven biological research 

**Title (ZH)**: 基于奥米克戎数据驱动生物研究的AI科学家基准测试 

**Authors**: Erpai Luo, Jinmeng Jia, Yifan Xiong, Xiangyu Li, Xiaobo Guo, Baoqi Yu, Lei Wei, Xuegong Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08341)  

**Abstract**: The rise of large language models and multi-agent systems has sparked growing interest in AI scientists capable of autonomous biological research. However, existing benchmarks either focus on reasoning without data or on data analysis with predefined statistical answers, lacking realistic, data-driven evaluation settings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench), a benchmark designed to assess AI scientists' ability to generate biological discoveries through data analysis and reasoning with external knowledge. BaisBench comprises two tasks: cell type annotation on 31 expert-labeled single-cell datasets, and scientific discovery through answering 198 multiple-choice questions derived from the biological insights of 41 recent single-cell studies. Systematic experiments on state-of-the-art AI scientists and LLM agents showed that while promising, current models still substantially underperform human experts on both tasks. We hope BaisBench will fill this gap and serve as a foundation for advancing and evaluating AI models for scientific discovery. The benchmark can be found at: this https URL. 

**Abstract (ZH)**: 生物AI科学家基准（BaisBench）：评估AI科学家通过数据分析和外部知识进行生物发现的能力 

---
# Evaluating LLM Metrics Through Real-World Capabilities 

**Title (ZH)**: 通过实际能力评估LLM指标 

**Authors**: Justin K Miller, Wenjia Tang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08253)  

**Abstract**: As generative AI becomes increasingly embedded in everyday workflows, it is important to evaluate its performance in ways that reflect real-world usage rather than abstract notions of intelligence. Unlike many existing benchmarks that assess general intelligence, our approach focuses on real-world utility, evaluating how well models support users in everyday tasks. While current benchmarks emphasize code generation or factual recall, users rely on AI for a much broader range of activities-from writing assistance and summarization to citation formatting and stylistic feedback. In this paper, we analyze large-scale survey data and usage logs to identify six core capabilities that represent how people commonly use Large Language Models (LLMs): Summarization, Technical Assistance, Reviewing Work, Data Structuring, Generation, and Information Retrieval. We then assess the extent to which existing benchmarks cover these capabilities, revealing significant gaps in coverage, efficiency measurement, and interpretability. Drawing on this analysis, we use human-centered criteria to identify gaps in how well current benchmarks reflect common usage that is grounded in five practical criteria: coherence, accuracy, clarity, relevance, and efficiency. For four of the six capabilities, we identify the benchmarks that best align with real-world tasks and use them to compare leading models. We find that Google Gemini outperforms other models-including OpenAI's GPT, xAI's Grok, Meta's LLaMA, Anthropic's Claude, DeepSeek, and Qwen from Alibaba-on these utility-focused metrics. 

**Abstract (ZH)**: 随着生成式AI越来越多地嵌入日常 workflows 中，重要的是以反映实际使用情况而非抽象的智能概念来评估其性能。与许多现有的侧重于通用智能的基准测试不同，我们的方法专注于实际应用的有用性，评估模型如何支持用户完成日常任务。尽管当前的基准测试侧重于代码生成或事实检索，用户依赖AI进行更广泛的活动——从写作辅助和总结到引文格式化和风格反馈。在本文中，我们分析了规模较大的调查数据和使用日志，以识别代表人们常用大型语言模型（LLMs）的六种核心能力：总结、技术辅助、审查工作、数据结构化、生成和信息检索。然后我们评估现有基准测试在这些能力上的覆盖程度，揭示了在覆盖范围、效率测量和可解释性方面的重要缺口。通过这一分析，我们采用以用户为中心的标准来识别当前基准测试在反映实际应用方面的不足之处，这些实际应用基于五项实用标准：连贯性、准确性、清晰度、相关性和效率。对于其中的四种能力，我们确定了与实际任务最佳对齐的基准测试，并使用它们来比较顶级模型。我们发现，Google Gemini 在这些注重实用性的指标上优于其他模型，包括OpenAI的GPT、xAI的Grok、Meta的LLaMA、Anthropic的Claude、DeepSeek和来自阿里云的Qwen。 

---
# Unveiling the Best Practices for Applying Speech Foundation Models to Speech Intelligibility Prediction for Hearing-Impaired People 

**Title (ZH)**: 揭示将语音基础模型应用于听力受损人群语音可懂度预测的最佳实践 

**Authors**: Haoshuai Zhou, Boxuan Cao, Changgeng Mo, Linkai Li, Shan Xiang Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08215)  

**Abstract**: Speech foundation models (SFMs) have demonstrated strong performance across a variety of downstream tasks, including speech intelligibility prediction for hearing-impaired people (SIP-HI). However, optimizing SFMs for SIP-HI has been insufficiently explored. In this paper, we conduct a comprehensive study to identify key design factors affecting SIP-HI performance with 5 SFMs, focusing on encoder layer selection, prediction head architecture, and ensemble configurations. Our findings show that, contrary to traditional use-all-layers methods, selecting a single encoder layer yields better results. Additionally, temporal modeling is crucial for effective prediction heads. We also demonstrate that ensembling multiple SFMs improves performance, with stronger individual models providing greater benefit. Finally, we explore the relationship between key SFM attributes and their impact on SIP-HI performance. Our study offers practical insights into effectively adapting SFMs for speech intelligibility prediction for hearing-impaired populations. 

**Abstract (ZH)**: 基于语音的模型（SFMs）已经在多种下游任务中展现了强大的性能，包括听障人士的语音可懂度预测（SIP-HI）。然而，针对SIP-HI优化SFM的研究尚不够充分。在本文中，我们进行了全面研究，使用5种SFM来识别影响SIP-HI性能的关键设计因素，重点关注编码器层选择、预测头架构和集成配置。我们的研究结果表明，与传统的使用所有层的方法相比，选择单一编码器层能获得更好的效果。此外，时间建模对于有效的预测头至关重要。我们还展示了集成多个SFM可以提高性能，更强的单个模型能提供更大的收益。最后，我们探索了关键SFM属性与SIP-HI性能之间的关系。我们的研究提供了关于有效适应SFM进行听障人士语音可懂度预测的实用见解。 

---
# Behind the Noise: Conformal Quantile Regression Reveals Emergent Representations 

**Title (ZH)**: 超越噪声：共识量ile回归揭示 Emergent 表示 

**Authors**: Petrus H. Zwart, Tamas Varga, Odeta Qafoku, James A. Sethian  

**Link**: [PDF](https://arxiv.org/pdf/2505.08176)  

**Abstract**: Scientific imaging often involves long acquisition times to obtain high-quality data, especially when probing complex, heterogeneous systems. However, reducing acquisition time to increase throughput inevitably introduces significant noise into the measurements. We present a machine learning approach that not only denoises low-quality measurements with calibrated uncertainty bounds, but also reveals emergent structure in the latent space. By using ensembles of lightweight, randomly structured neural networks trained via conformal quantile regression, our method performs reliable denoising while uncovering interpretable spatial and chemical features -- without requiring labels or segmentation. Unlike conventional approaches focused solely on image restoration, our framework leverages the denoising process itself to drive the emergence of meaningful representations. We validate the approach on real-world geobiochemical imaging data, showing how it supports confident interpretation and guides experimental design under resource constraints. 

**Abstract (ZH)**: Scientific成像 Often Involves 长时间的数据获取 以获得高质量的数据，特别是在探针复杂、异质系统时。然而，为了提高吞吐量而减少数据获取时间不可避免地会引入显著的噪声。我们提出了一种机器学习方法，不仅能够通过校准的不确定性界限对低质量测量进行去噪，还能揭示潜在空间中的新兴结构。通过使用通过符合分位数回归训练的轻量级、随机结构神经网络的ensemble，我们的方法在不需标签或分割的情况下实现了可靠的去噪，并揭示了可解释的空间和化学特征。与专注于图像复原的传统方法不同，我们的框架利用去噪过程本身来驱动有意义表示的涌现。我们在真实的地球生物化学成像数据上验证了该方法，展示了它如何在资源受限条件下支持自信的解释并指导实验设计。 

---
# Decoding Neighborhood Environments with Large Language Models 

**Title (ZH)**: 使用大型语言模型解码邻里环境 

**Authors**: Andrew Cart, Shaohu Zhang, Melanie Escue, Xugui Zhou, Haitao Zhao, Prashanth BusiReddyGari, Beiyu Lin, Shuang Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.08163)  

**Abstract**: Neighborhood environments include physical and environmental conditions such as housing quality, roads, and sidewalks, which significantly influence human health and well-being. Traditional methods for assessing these environments, including field surveys and geographic information systems (GIS), are resource-intensive and challenging to evaluate neighborhood environments at scale. Although machine learning offers potential for automated analysis, the laborious process of labeling training data and the lack of accessible models hinder scalability. This study explores the feasibility of large language models (LLMs) such as ChatGPT and Gemini as tools for decoding neighborhood environments (e.g., sidewalk and powerline) at scale. We train a robust YOLOv11-based model, which achieves an average accuracy of 99.13% in detecting six environmental indicators, including streetlight, sidewalk, powerline, apartment, single-lane road, and multilane road. We then evaluate four LLMs, including ChatGPT, Gemini, Claude, and Grok, to assess their feasibility, robustness, and limitations in identifying these indicators, with a focus on the impact of prompting strategies and fine-tuning. We apply majority voting with the top three LLMs to achieve over 88% accuracy, which demonstrates LLMs could be a useful tool to decode the neighborhood environment without any training effort. 

**Abstract (ZH)**: 大型语言模型在解码街区环境中的可行性研究：以ChatGPT和Gemini为例 

---
# Efficient and Scalable Neural Symbolic Search for Knowledge Graph Complex Query Answering 

**Title (ZH)**: 高效可扩展的神经符号搜索方法及其在知识图复杂查询回答中的应用 

**Authors**: Weizhi Fei, Zihao Wang, hang Yin, Shukai Zhao, Wei Zhang, Yangqiu Song  

**Link**: [PDF](https://arxiv.org/pdf/2505.08155)  

**Abstract**: Complex Query Answering (CQA) aims to retrieve answer sets for complex logical formulas from incomplete knowledge graphs, which is a crucial yet challenging task in knowledge graph reasoning. While neuro-symbolic search utilized neural link predictions achieve superior accuracy, they encounter significant complexity bottlenecks: (i) Data complexity typically scales quadratically with the number of entities in the knowledge graph, and (ii) Query complexity becomes NP-hard for cyclic queries. Consequently, these approaches struggle to effectively scale to larger knowledge graphs and more complex queries. To address these challenges, we propose an efficient and scalable symbolic search framework. First, we propose two constraint strategies to compute neural logical indices to reduce the domain of variables, thereby decreasing the data complexity of symbolic search. Additionally, we introduce an approximate algorithm based on local search to tackle the NP query complexity of cyclic queries. Experiments on various CQA benchmarks demonstrate that our framework reduces the computational load of symbolic methods by 90\% while maintaining nearly the same performance, thus alleviating both efficiency and scalability issues. 

**Abstract (ZH)**: 复杂查询回答（CQA）旨在从不完整知识图中检索复杂的逻辑公式答案集合，这是知识图 reasoning 中一个关键但具有挑战性的任务。虽然神经符号搜索利用神经连接预测实现了更高的准确性，但它们遇到了显著的复杂性瓶颈：（i）数据复杂性通常与知识图中的实体数量成二次关系，（ii）循环查询的查询复杂性成为 NP 难问题。因此，这些方法难以有效扩展到更大的知识图和更复杂的查询。为了解决这些挑战，我们提出了一种高效且可扩展的符号搜索框架。首先，我们提出了两种约束策略来计算神经逻辑索引，以减少变量的取值范围，从而降低符号搜索的数据复杂性。此外，我们引入了一种基于局部搜索的近似算法来处理循环查询的 NP 查询复杂性。在各种 CQA 标准测试集上的实验表明，我们的框架将符号方法的计算负载降低了 90%，同时保持了几乎相同的表现，从而缓解了效率和 scalability 问题。 

---
# Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast 

**Title (ZH)**: 基础模型知识蒸馏电池容量退化预测 

**Authors**: Joey Chan, Zhen Chen, Ershun Pan  

**Link**: [PDF](https://arxiv.org/pdf/2505.08151)  

**Abstract**: Accurate estimation of lithium-ion battery capacity degradation is critical for enhancing the reliability and safety of battery operations. Traditional expert models, tailored to specific scenarios, provide isolated estimations. With the rapid advancement of data-driven techniques, a series of general-purpose time-series foundation models have been developed. However, foundation models specifically designed for battery capacity degradation remain largely unexplored. To enable zero-shot generalization in battery degradation prediction using large model technology, this study proposes a degradation-aware fine-tuning strategy for time-series foundation models. We apply this strategy to fine-tune the Timer model on approximately 10 GB of open-source battery charge discharge data. Validation on our released CycleLife-SJTUIE dataset demonstrates that the fine-tuned Battery-Timer possesses strong zero-shot generalization capability in capacity degradation forecasting. To address the computational challenges of deploying large models, we further propose a knowledge distillation framework that transfers the knowledge of pre-trained foundation models into compact expert models. Distillation results across several state-of-the-art time-series expert models confirm that foundation model knowledge significantly improves the multi-condition generalization of expert models. 

**Abstract (ZH)**: 准确估计锂离子电池容量衰退对于提高电池操作的可靠性和安全性至关重要。传统的专家模型针对特定场景提供孤立的估计。随着数据驱动技术的飞速发展，一系列通用时间序列基础模型被开发出来。然而，专门针对电池容量衰退的基础模型的研究仍然相对空白。为了利用大规模模型技术在电池衰退预测中实现零样本泛化，本文提出了一种衰退感知的时间序列基础模型微调策略。我们采用该策略对大约10 GB的开源电池充放电数据进行微调Timer模型。在我们发布的CycleLife-SJTUIE数据集上的验证表明，微调后的Battery-Timer在容量衰退预测中具备强大的零样本泛化能力。为了应对部署大规模模型的计算挑战，本文进一步提出了一种知识蒸馏框架，将预训练基础模型的知识转移到紧凑的专家模型中。针对多个性能前沿的时间序列专家模型的蒸馏结果证实，基础模型知识显著提高了专家模型在多条件泛化方面的性能。 

---
# Lost in Transmission: When and Why LLMs Fail to Reason Globally 

**Title (ZH)**: 迷失在传输中：LLMs在何时及为何全球推理失败 

**Authors**: Tobias Schnabel, Kiran Tomlinson, Adith Swaminathan, Jennifer Neville  

**Link**: [PDF](https://arxiv.org/pdf/2505.08140)  

**Abstract**: Despite their many successes, transformer-based large language models (LLMs) continue to struggle with tasks that require complex reasoning over large parts of their input. We argue that these failures arise due to capacity limits on the accurate flow of information within LLMs. To formalize this issue, we introduce the bounded attention prefix oracle (BAPO) model, a new computational framework that models bandwidth constraints on attention heads, the mechanism for internal communication in LLMs. We show that several important reasoning problems like graph reachability require high communication bandwidth for BAPOs to solve; we call these problems BAPO-hard. Our experiments corroborate our theoretical predictions: GPT-4, Claude, and Gemini succeed on BAPO-easy tasks and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another benefit of chain of thought (CoT): we prove that breaking down a task using CoT can turn any BAPO-hard problem into a BAPO-easy one. Our results offer principled explanations for key LLM failures and suggest directions for architectures and inference methods that mitigate bandwidth limits. 

**Abstract (ZH)**: 尽管 transformer 基础的大语言模型在许多任务上取得了成功，但在处理需要对大量输入进行复杂推理的任务时仍存在挑战。我们提出这些失败是由于大语言模型内部信息准确流通过程中的容量限制。为了形式化这一问题，我们引入了有界注意力前缀先知（BAPO）模型，这是一种新的计算框架，用于建模大语言模型内部通信机制（注意力头）的信息带宽限制。我们展示了诸如图可达性等重要的推理问题需要较高的通信带宽才能解决；我们将这类问题称为 BAPO-困难。我们的实验证实了我们的理论预测：GPT-4、Claude 和 Gemini 在 BAPO-简单任务上取得成功，但在相对较小的 BAPO-困难任务上却失败。BAPO 还揭示了思考链（CoT）的另一个优点：我们证明，通过 CoT 将任务分解可以将任何 BAPO-困难问题转换为 BAPO-简单问题。我们的研究为大语言模型的关键失败提供了有原则的解释，并指出了减轻带宽限制的架构和推理方法的发展方向。 

---
# Explainable Reinforcement Learning Agents Using World Models 

**Title (ZH)**: 使用世界模型的可解释强化学习代理 

**Authors**: Madhuri Singh, Amal Alabdulkarim, Gennie Mansi, Mark O. Riedl  

**Link**: [PDF](https://arxiv.org/pdf/2505.08073)  

**Abstract**: Explainable AI (XAI) systems have been proposed to help people understand how AI systems produce outputs and behaviors. Explainable Reinforcement Learning (XRL) has an added complexity due to the temporal nature of sequential decision-making. Further, non-AI experts do not necessarily have the ability to alter an agent or its policy. We introduce a technique for using World Models to generate explanations for Model-Based Deep RL agents. World Models predict how the world will change when actions are performed, allowing for the generation of counterfactual trajectories. However, identifying what a user wanted the agent to do is not enough to understand why the agent did something else. We augment Model-Based RL agents with a Reverse World Model, which predicts what the state of the world should have been for the agent to prefer a given counterfactual action. We show that explanations that show users what the world should have been like significantly increase their understanding of the agent policy. We hypothesize that our explanations can help users learn how to control the agents execution through by manipulating the environment. 

**Abstract (ZH)**: 可解释的人工智能（XAI）系统被提出以帮助人们理解AI系统如何生成输出和行为。由于序列决策的时序性质，可解释的强化学习（XRL）增加了复杂性。此外，并非所有非AI专家都具备修改智能体或其策略的能力。我们提出了一种使用世界模型生成基于模型深度强化学习（Model-Based Deep RL）智能体解释的技术。世界模型预测执行动作时世界将如何变化，从而允许生成反事实轨迹。然而，仅确定用户想让智能体做什么还不足以理解其为何做了其他事情。我们为基于模型的RL智能体增加了逆向世界模型，该模型预测世界应为何种状态，以便智能体更偏好某个给定的反事实动作。我们展示了向用户展示世界应为何种状态的解释显著提高了他们对智能体策略的理解。我们假设我们的解释可以帮助用户通过操纵环境学习如何控制智能体的执行。 

---
# Bias or Optimality? Disentangling Bayesian Inference and Learning Biases in Human Decision-Making 

**Title (ZH)**: 偏见还是最优性？解构人类决策中的贝叶斯推理与学习偏见 

**Authors**: Prakhar Godara  

**Link**: [PDF](https://arxiv.org/pdf/2505.08049)  

**Abstract**: Recent studies claim that human behavior in a two-armed Bernoulli bandit (TABB) task is described by positivity and confirmation biases, implying that humans do not integrate new information objectively. However, we find that even if the agent updates its belief via objective Bayesian inference, fitting the standard Q-learning model with asymmetric learning rates still recovers both biases. Bayesian inference cast as an effective Q-learning algorithm has symmetric, though decreasing, learning rates. We explain this by analyzing the stochastic dynamics of these learning systems using master equations. We find that both confirmation bias and unbiased but decreasing learning rates yield the same behavioral signatures. Finally, we propose experimental protocols to disentangle true cognitive biases from artifacts of decreasing learning rates. 

**Abstract (ZH)**: 近期的研究认为，在两臂伯努利老虎机任务中，人类行为由积极偏差和确认偏差描述，暗示人类不能客观整合新信息。然而，我们发现即使智能体通过客观贝叶斯推断更新其信念，使用非对称学习率拟合标准Q学习模型仍然能够恢复这两种偏差。将贝叶斯推理视为有效的Q学习算法具有对称但递减的学习率。我们通过分析这些学习系统的随机动力学来解释这一现象，并发现确认偏差和无偏但递减的学习率会导致相同的行为特征。最后，我们提出实验方案以区分真实的认知偏差和学习率递减导致的伪像。 

---
# The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic 

**Title (ZH)**: 有界图神经网络与一阶逻辑片段之间的对应关系 

**Authors**: Bernardo Cuenca Grau, Przemysław A. Wałęga  

**Link**: [PDF](https://arxiv.org/pdf/2505.08021)  

**Abstract**: Graph Neural Networks (GNNs) address two key challenges in applying deep learning to graph-structured data: they handle varying size input graphs and ensure invariance under graph isomorphism. While GNNs have demonstrated broad applicability, understanding their expressive power remains an important question. In this paper, we show that bounded GNN architectures correspond to specific fragments of first-order logic (FO), including modal logic (ML), graded modal logic (GML), modal logic with the universal modality (ML(A)), the two-variable fragment (FO2) and its extension with counting quantifiers (C2). To establish these results, we apply methods and tools from finite model theory of first-order and modal logics to the domain of graph representation learning. This provides a unifying framework for understanding the logical expressiveness of GNNs within FO. 

**Abstract (ZH)**: 图神经网络（GNNs）解决了将深度学习应用于图结构数据时的两个关键挑战：处理不同大小的输入图以及确保同构不变性。虽然GNNs展示了广泛的应用性，但对其表征能力的理解仍然是一个重要问题。在本文中，我们证明了受限的GNN架构对应于一阶逻辑（FO）的特定片段，包括模态逻辑（ML）、分级模态逻辑（GML）、带有全称模态性的模态逻辑（ML(A)）、二变量片段（FO2）及其扩展的计数量词片段（C2）。为了得出这些结果，我们运用了一阶逻辑和模态逻辑的有限模型理论方法和工具，将其应用于图表示学习领域。这提供了一种统一框架，用于在FO内理解GNNs的逻辑表征能力。 

---
# Enhancing Trust Management System for Connected Autonomous Vehicles Using Machine Learning Methods: A Survey 

**Title (ZH)**: 使用机器学习方法增强连接自动驾驶车辆的信任管理系统的综述 

**Authors**: Qian Xu, Lei Zhang, Yixiao Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.07882)  

**Abstract**: Connected Autonomous Vehicles (CAVs) operate in dynamic, open, and multi-domain networks, rendering them vulnerable to various threats. Trust Management Systems (TMS) systematically organize essential steps in the trust mechanism, identifying malicious nodes against internal threats and external threats, as well as ensuring reliable decision-making for more cooperative tasks. Recent advances in machine learning (ML) offer significant potential to enhance TMS, especially for the strict requirements of CAVs, such as CAV nodes moving at varying speeds, and opportunistic and intermittent network behavior. Those features distinguish ML-based TMS from social networks, static IoT, and Social IoT. This survey proposes a novel three-layer ML-based TMS framework for CAVs in the vehicle-road-cloud integration system, i.e., trust data layer, trust calculation layer and trust incentive layer. A six-dimensional taxonomy of objectives is proposed. Furthermore, the principles of ML methods for each module in each layer are analyzed. Then, recent studies are categorized based on traffic scenarios that are against the proposed objectives. Finally, future directions are suggested, addressing the open issues and meeting the research trend. We maintain an active repository that contains up-to-date literature and open-source projects at this https URL. 

**Abstract (ZH)**: 连接自主车辆（CAVs）在动态、开放且多领域的网络中运行，使其容易受到各种威胁。信任管理系统（TMS）系统地组织信任机制中的关键步骤，识别内部和外部威胁中的恶意节点，并确保更合作任务中的可靠决策。机器学习（ML）的 recent 进展为增强 TMS 提供了显著潜力，特别是对于连接自主车辆（CAVs）的严格要求，如 CAV 节点以不同速度移动，以及机会性和间歇性的网络行为。这些特征使基于 ML 的 TMS 与社会网络、静态物联网（IoT）和社会物联网（Social IoT）区分开来。本文综述提出了一种针对车辆-道路-云集成系统的连接自主车辆（CAVs）的新型三层机器学习（ML）信任管理系统框架，即信任数据层、信任计算层和信任激励层。提出了六维目标分类法。此外，分析了每一层中每个模块的机器学习（ML）方法原理。然后，根据反对所提目标的交通场景对近期研究进行了分类。最后，提出了未来方向，以解决开放问题并符合研究趋势。我们保持一个活跃的仓库，其中包含最新的文献和开源项目，详情请访问此链接。 

---
# Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding 

**Title (ZH)**: 箭头导向的VLM：通过箭头方向编码增强流程图理解 

**Authors**: Takamitsu Omasa, Ryo Koshihara, Masumi Morishige  

**Link**: [PDF](https://arxiv.org/pdf/2505.07864)  

**Abstract**: Flowcharts are indispensable tools in software design and business-process analysis, yet current vision-language models (VLMs) frequently misinterpret the directional arrows and graph topology that set these diagrams apart from natural images. We introduce a seven-stage pipeline grouped into three broader processes: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical character recognition (OCR) to extract node text; and (3) construction of a structured prompt that guides the VLMs. Tested on a 90-question benchmark distilled from 30 annotated flowcharts, the method raises overall accuracy from 80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The gain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp); branch-result questions improve more modestly, and before-step questions remain difficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same trends, reinforcing the advantage of explicit arrow encoding. Limitations include dependence on detector and OCR precision, the small evaluation set, and residual errors at nodes with multiple incoming edges. Future work will enlarge the benchmark with synthetic and handwritten flowcharts and assess the approach on Business Process Model and Notation (BPMN) and Unified Modeling Language (UML). 

**Abstract (ZH)**: Flowcharts在软件设计和业务流程分析中的不可或缺工具，但当前的 vision-language 模型（VLMs）经常错误地解读这些图表中的方向箭头和图拓扑结构。我们介绍了由三个更广泛的处理过程组成的七阶段管道：(1) 具有箭头感知性的节点和箭头端点检测；(2) 光学字符识别（OCR）提取节点文本；(3) 构建结构化提示以指导VLMs。该方法在从30个标注的流程图中提取的90个问题基准测试中，从80%的准确率提高到89%（提高9个百分点），无需任何特定任务的微调。在下一步查询方面收益最为显著（25/30 -> 30/30；100%，+17个百分点）；分支结果查询有所改善，而前一步查询仍然困难。与基于LLM-as-a-Judge协议的并行评估显示了相同趋势，强化了明确箭头编码的优势。局限性包括对检测器和OCR精度的依赖、评价集规模较小以及节点有多条入边时残留的错误。未来的工作将进一步扩展基准测试，包括合成和手写流程图，并评估该方法在Business Process Model and Notation (BPMN)和Unified Modeling Language (UML)上的应用。 

---
# CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement Learning via Co-evolutionary Task Evolution 

**Title (ZH)**: 协作性课程学习：通过共演化的任务进化在稀疏奖励多智能体强化学习中的应用 

**Authors**: Yufei Lin, Chengwei Ye, Huanzhen Zhang, Kangsheng Wang, Linuo Xu, Shuyan Liu, Zeyu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.07854)  

**Abstract**: Sparse reward environments pose significant challenges in reinforcement learning, especially within multi-agent systems (MAS) where feedback is delayed and shared across agents, leading to suboptimal learning. We propose Collaborative Multi-dimensional Course Learning (CCL), a novel curriculum learning framework that addresses this by (1) refining intermediate tasks for individual agents, (2) using a variational evolutionary algorithm to generate informative subtasks, and (3) co-evolving agents with their environment to enhance training stability. Experiments on five cooperative tasks in the MPE and Hide-and-Seek environments show that CCL outperforms existing methods in sparse reward settings. 

**Abstract (ZH)**: 稀疏奖励环境对强化学习提出了重大挑战，特别是在多智能体系统中，反馈延迟且共享于多个智能体之间，导致学习效果不佳。为此，我们提出了一种新型的协作多维课程学习框架CCL，通过（1）细化个体智能体的中间任务，（2）使用变分进化算法生成有信息量的子任务，以及（3）智能体与环境共同进化以提高训练稳定性，来解决上述问题。在MPE和Hide-and-Seek环境中的五个合作任务上的实验表明，CCL在稀疏奖励设置中优于现有方法。 

---
# Conceptual Logical Foundations of Artificial Social Intelligence 

**Title (ZH)**: 人工社会智能的conceptual逻辑基础 

**Authors**: Eric Werner  

**Link**: [PDF](https://arxiv.org/pdf/2505.07847)  

**Abstract**: What makes a society possible at all? How is coordination and cooperation in social activity possible? What is the minimal mental architecture of a social agent? How is the information about the state of the world related to the agents intentions? How are the intentions of agents related? What role does communication play in this coordination process? This essay explores the conceptual and logical foundations of artificial social intelligence in the context of a society of multiple agents that communicate and cooperate to achieve some end. An attempt is made to provide an introduction to some of the key concepts, their formal definitions and their interrelationships. These include the notion of a changing social world of multiple agents. The logic of social intelligence goes beyond classical logic by linking information with strategic thought. A minimal architecture of social agents is presented. The agents have different dynamically changing, possible choices and abilities. The agents also have uncertainty, lacking perfect information about their physical state as well as their dynamic social state. The social state of an agent includes the intentional state of that agent, as well as, that agent's representation of the intentional states of other agents. Furthermore, it includes the evaluations agents make of their physical and social condition. Communication, semantic and pragmatic meaning and their relationship to intention and information states are investigated. The logic of agent abilities and intentions are motivated and formalized. The entropy of group strategic states is defined. 

**Abstract (ZH)**: 多代理社会中协调与合作的概念与逻辑基础 

---
# Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models 

**Title (ZH)**: 为了诱导大型语言模型出现规范游戏行为而在小型环境中获胜：一个小型实验环境 

**Authors**: Lars Malmqvist  

**Link**: [PDF](https://arxiv.org/pdf/2505.07846)  

**Abstract**: This study reveals how frontier Large Language Models LLMs can "game the system" when faced with impossible situations, a critical security and alignment concern. Using a novel textual simulation approach, we presented three leading LLMs (o1, o3-mini, and r1) with a tic-tac-toe scenario designed to be unwinnable through legitimate play, then analyzed their tendency to exploit loopholes rather than accept defeat. Our results are alarming for security researchers: the newer, reasoning-focused o3-mini model showed nearly twice the propensity to exploit system vulnerabilities (37.1%) compared to the older o1 model (17.5%). Most striking was the effect of prompting. Simply framing the task as requiring "creative" solutions caused gaming behaviors to skyrocket to 77.3% across all models. We identified four distinct exploitation strategies, from direct manipulation of game state to sophisticated modification of opponent behavior. These findings demonstrate that even without actual execution capabilities, LLMs can identify and propose sophisticated system exploits when incentivized, highlighting urgent challenges for AI alignment as models grow more capable of identifying and leveraging vulnerabilities in their operating environments. 

**Abstract (ZH)**: 这项研究揭示了前沿大规模语言模型在面临不可能情况时如何“游戏系统”的方式，这对安全性和对齐提出了关键关切。通过一种新颖的文字模拟方法，我们向三个领先的大规模语言模型（o1、o3-mini和r1）呈现了一个设计为无法通过合法玩法获胜的井字游戏场景，然后分析了它们倾向于利用漏洞而不是接受失败的趋势。这些结果让安全研究人员感到警惕：侧重推理的新款o3-mini模型表现出近两倍于较旧的o1模型（分别为37.1%和17.5%）利用系统漏洞的倾向。最引人注目的是提示效应。仅仅将任务描述为需要“创造性”解决方案，就导致所有模型的游戏行为激增至77.3%。我们识别出了四种不同的利用策略，从直接操纵游戏状态到复杂的对手行为修改。这些发现表明，即使没有实际执行能力，当受到激励时，语言模型仍能识别并提出复杂的系统漏洞利用，突显了随着模型越来越擅长识别和利用其运行环境中的漏洞，AI对齐面临的紧迫挑战。 

---
# RAN Cortex: Memory-Augmented Intelligence for Context-Aware Decision-Making in AI-Native Networks 

**Title (ZH)**: RAN cortex: 基于内存增强的面向上下文决策的AI原生网络智能 

**Authors**: Sebastian Barros  

**Link**: [PDF](https://arxiv.org/pdf/2505.07842)  

**Abstract**: As Radio Access Networks (RAN) evolve toward AI-native architectures, intelligent modules such as xApps and rApps are expected to make increasingly autonomous decisions across scheduling, mobility, and resource management domains. However, these agents remain fundamentally stateless, treating each decision as isolated, lacking any persistent memory of prior events or outcomes. This reactive behavior constrains optimization, especially in environments where network dynamics exhibit episodic or recurring patterns. In this work, we propose RAN Cortex, a memory-augmented architecture that enables contextual recall in AI-based RAN decision systems. RAN Cortex introduces a modular layer composed of four elements: a context encoder that transforms network state into high-dimensional embeddings, a vector-based memory store of past network episodes, a recall engine to retrieve semantically similar situations, and a policy interface that supplies historical context to AI agents in real time or near-real time. We formalize the retrieval-augmented decision problem in the RAN, present a system architecture compatible with O-RAN interfaces, and analyze feasible deployments within the Non-RT and Near-RT RIC domains. Through illustrative use cases such as stadium traffic mitigation and mobility management in drone corridors, we demonstrate how contextual memory improves adaptability, continuity, and overall RAN intelligence. This work introduces memory as a missing primitive in AI-native RAN designs and provides a framework to enable "learning agents" without the need for retraining or centralized inference 

**Abstract (ZH)**: 基于AI的无线接入网络认知内存增强架构：RAN Cortex 

---
# An Optimized Evacuation Plan for an Active-Shooter Situation Constrained by Network Capacity 

**Title (ZH)**: 受网络容量约束的主动射手事件优化疏散计划 

**Authors**: Joseph Lavalle-Rivera, Aniirudh Ramesh, Subhadeep Chakraborty  

**Link**: [PDF](https://arxiv.org/pdf/2505.07830)  

**Abstract**: A total of more than 3400 public shootings have occurred in the United States between 2016 and 2022. Among these, 25.1% of them took place in an educational institution, 29.4% at the workplace including office buildings, 19.6% in retail store locations, and 13.4% in restaurants and bars. During these critical scenarios, making the right decisions while evacuating can make the difference between life and death. However, emergency evacuation is intensely stressful, which along with the lack of verifiable real-time information may lead to fatal incorrect decisions. To tackle this problem, we developed a multi-route routing optimization algorithm that determines multiple optimal safe routes for each evacuee while accounting for available capacity along the route, thus reducing the threat of crowding and bottlenecking. Overall, our algorithm reduces the total casualties by 34.16% and 53.3%, compared to our previous routing algorithm without capacity constraints and an expert-advised routing strategy respectively. Further, our approach to reduce crowding resulted in an approximate 50% reduction in occupancy in key bottlenecking nodes compared to both of the other evacuation algorithms. 

**Abstract (ZH)**: 2016年至2022年间，美国共发生了超过3400起公共枪击事件。其中，25.1%发生在教育机构，29.4%发生在工作场所包括办公楼，19.6%发生在零售店，13.4%发生在餐馆和酒吧。在这些关键时刻，正确疏散决策可以决定生与死。然而，紧急疏散极其紧张，缺乏可靠的实时信息可能导致致命的错误决策。为解决这一问题，我们开发了一种多路径路由优化算法，为每个疏散人员确定多个最优安全路径，同时考虑路径上的可用容量，从而减少拥堵和瓶颈的风险。总体而言，与不考虑容量约束的先前路由算法相比，我们的算法减少了34.16%的伤亡，与专家建议的疏散策略相比减少了53.3%的伤亡。此外，我们减少拥堵的方法使得关键瓶颈节点的占用率比其他两种疏散算法降低了约50%。 

---
# CodePDE: An Inference Framework for LLM-driven PDE Solver Generation 

**Title (ZH)**: CodePDE：一种由大语言模型驱动的偏微分方程求解器生成推理框架 

**Authors**: Shanda Li, Tanya Marwah, Junhong Shen, Weiwei Sun, Andrej Risteski, Yiming Yang, Ameet Talwalkar  

**Link**: [PDF](https://arxiv.org/pdf/2505.08783)  

**Abstract**: Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert knowledge to implement and are computationally expensive, while neural-network-based solvers require large training datasets and often lack interpretability. In this work, we frame PDE solving as a code generation task and introduce CodePDE, the first inference framework for generating PDE solvers using large language models (LLMs). Leveraging advanced inference-time algorithms and scaling strategies, CodePDE unlocks critical capacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and test-time scaling -- all without task-specific tuning. CodePDE achieves superhuman performance across a range of representative PDE problems. We also present a systematic empirical analysis of LLM generated solvers, analyzing their accuracy, efficiency, and numerical scheme choices. Our findings highlight the promise and the current limitations of LLMs in PDE solving, offering a new perspective on solver design and opportunities for future model development. Our code is available at this https URL. 

**Abstract (ZH)**: 偏微分方程（PDEs）是建模物理系统的基础，但求解它们仍是一项复杂的挑战。传统的数值求解器依赖于专家知识并具有高计算成本，而基于神经网络的求解器需要大量的训练数据集，通常缺乏可解释性。在本工作中，我们将PDE求解重新定义为代码生成任务，并引入CodePDE，这是首个利用大型语言模型（LLMs）生成PDE求解器的推理框架。通过利用先进的推理时算法和扩展策略，CodePDE解锁了LLMs在PDE求解中的关键能力：推理、调试、自我优化和测试时扩展——这些功能无需针对特定任务进行调优。CodePDE在一系列代表性PDE问题上实现了超人类性能。我们还对LLMs生成的求解器进行了系统的实证分析，分析了它们的准确性、效率和数值方案选择。我们的研究结果突显了LLMs在PDE求解中的潜力及其当前的限制，为求解器设计提供了新的视角，并为未来模型的发展提供了机会。代码可在以下链接获取：this https URL。 

---
# Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology 

**Title (ZH)**: 面向城市空间自主无人机视觉目标搜索：基准与主体性方法论 

**Authors**: Yatai Ji, Zhengqiu Zhu, Yong Zhao, Beidan Liu, Chen Gao, Yihao Zhao, Sihang Qiu, Yue Hu, Quanjun Yin, Yong Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.08765)  

**Abstract**: Aerial Visual Object Search (AVOS) tasks in urban environments require Unmanned Aerial Vehicles (UAVs) to autonomously search for and identify target objects using visual and textual cues without external guidance. Existing approaches struggle in complex urban environments due to redundant semantic processing, similar object distinction, and the exploration-exploitation dilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS, the first benchmark dataset for autonomous search of common urban objects. This dataset comprises 2,420 tasks across six object categories with varying difficulty levels, enabling comprehensive evaluation of UAV agents' search capabilities. To solve the AVOS tasks, we also propose PRPSearcher (Perception-Reasoning-Planning Searcher), a novel agentic method powered by multi-modal large language models (MLLMs) that mimics human three-tier cognition. Specifically, PRPSearcher constructs three specialized maps: an object-centric dynamic semantic map enhancing spatial perception, a 3D cognitive map based on semantic attraction values for target reasoning, and a 3D uncertainty map for balanced exploration-exploitation search. Also, our approach incorporates a denoising mechanism to mitigate interference from similar objects and utilizes an Inspiration Promote Thought (IPT) prompting mechanism for adaptive action planning. Experimental results on CityAVOS demonstrate that PRPSearcher surpasses existing baselines in both success rate and search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and -46.40% NE). While promising, the performance gap compared to humans highlights the need for better semantic reasoning and spatial exploration capabilities in AVOS tasks. This work establishes a foundation for future advances in embodied target search. Dataset and source code are available at this https URL. 

**Abstract (ZH)**: 城市环境下基于视觉的空中目标搜索（AVOS）任务要求无人驾驶航空车辆（UAV）自主地利用视觉和文本线索搜索和识别目标对象，无需外部指引。现有方法在复杂城市环境中由于冗余的语义处理、相似对象区分困难以及探索与利用的困境而难以应对。为填补这一空白并支持AVOS任务，我们引入CityAVOS，这是首个用于自主搜索常见城市物体的基准数据集。该数据集包含六个类别、不同难度级别的2420个任务，能够全面评估UAV代理的搜索能力。为了解决AVOS任务，我们还提出了PRPSearcher（感知-推理-规划搜索器），这是一种由多模态大语言模型（MLLMs）驱动的新型代理方法，模拟人类三等级认知。PRPSearcher构建了三个专门的地图：以对象为中心的动力学语义图，增强空间感知；基于语义吸引值的目标推理的3D认知地图；以及平衡探索与利用的3D不确定性图。此外，我们的方法还包含去噪机制以减轻相似对象的干扰，并利用启发式促进思考（IPT）提示机制进行适应性行动规划。在CityAVOS上的实验结果表明，PRPSearcher在成功率和搜索效率方面均超过了现有基线（平均：+37.69%成功率，+28.96% SPL，-30.69% MSS，-46.40% 新错误率）。尽管表现出色，但与人类的表现差距表明AVOS任务中需要更好的语义推理和空间探索能力。本工作为未来在具身目标搜索方面的研究奠定了基础。数据集和源代码可在以下链接获取。 

---
# Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion 

**Title (ZH)**: 通过视觉-ingredient特征融合促进食物营养估算 

**Authors**: Huiyan Qi, Bin Zhu, Chong-Wah Ngo, Jingjing Chen, Ee-Peng Lim  

**Link**: [PDF](https://arxiv.org/pdf/2505.08747)  

**Abstract**: Nutrition estimation is an important component of promoting healthy eating and mitigating diet-related health risks. Despite advances in tasks such as food classification and ingredient recognition, progress in nutrition estimation is limited due to the lack of datasets with nutritional annotations. To address this issue, we introduce FastFood, a dataset with 84,446 images across 908 fast food categories, featuring ingredient and nutritional annotations. In addition, we propose a new model-agnostic Visual-Ingredient Feature Fusion (VIF$^2$) method to enhance nutrition estimation by integrating visual and ingredient features. Ingredient robustness is improved through synonym replacement and resampling strategies during training. The ingredient-aware visual feature fusion module combines ingredient features and visual representation to achieve accurate nutritional prediction. During testing, ingredient predictions are refined using large multimodal models by data augmentation and majority voting. Our experiments on both FastFood and Nutrition5k datasets validate the effectiveness of our proposed method built in different backbones (e.g., Resnet, InceptionV3 and ViT), which demonstrates the importance of ingredient information in nutrition estimation. this https URL. 

**Abstract (ZH)**: 营养估计是促进健康饮食和减轻饮食相关健康风险的重要组成部分。尽管在食物分类和成分识别等方面取得了进步，但由于缺乏营养标注的数据集，营养估计的进步受到限制。为了解决这一问题，我们引入了FastFood数据集，该数据集包含84,446张图片，涵盖908个快速食品类别，并配有成分和营养标注。此外，我们提出了一种新的模型无关的视觉-成分特征融合（VIF²）方法，通过整合视觉和成分特征来增强营养估计。通过训练中的同义词替换和重采样策略提高了成分的鲁棒性。成分感知的视觉特征融合模块结合成分特征和视觉表示以实现准确的营养预测。在测试过程中，通过数据增强和多数投票策略，使用大型多模态模型 refining 成分预测。我们在FastFood和Nutrition5k数据集上的实验验证了我们所提出方法的有效性，该方法在不同骨干网络（例如，Resnet、InceptionV3和ViT）中构建，证明了成分信息在营养估计中的重要性。此链接：[原文链接] 

---
# Securing RAG: A Risk Assessment and Mitigation Framework 

**Title (ZH)**: securing RAG：一种风险评估与缓解框架 

**Authors**: Lukas Ammann, Sara Ott, Christoph R. Landolt, Marco P. Lehmann  

**Link**: [PDF](https://arxiv.org/pdf/2505.08728)  

**Abstract**: Retrieval Augmented Generation (RAG) has emerged as the de facto industry standard for user-facing NLP applications, offering the ability to integrate data without re-training or fine-tuning Large Language Models (LLMs). This capability enhances the quality and accuracy of responses but also introduces novel security and privacy challenges, particularly when sensitive data is integrated. With the rapid adoption of RAG, securing data and services has become a critical priority. This paper first reviews the vulnerabilities of RAG pipelines, and outlines the attack surface from data pre-processing and data storage management to integration with LLMs. The identified risks are then paired with corresponding mitigations in a structured overview. In a second step, the paper develops a framework that combines RAG-specific security considerations, with existing general security guidelines, industry standards, and best practices. The proposed framework aims to guide the implementation of robust, compliant, secure, and trustworthy RAG systems. 

**Abstract (ZH)**: Retrieval Augmented Generation (RAG)的兴起已成为面向用户的NLP应用的事实标准，能够无需重新训练或微调大型语言模型（LLMs）即可集成数据。这一能力提高了响应的质量和准确性，但也引入了新的安全和隐私挑战，特别是在集成敏感数据时。随着RAG的快速采纳，保障数据和服务的安全性已成为一项关键优先事项。本文首先回顾RAG管道的漏洞，并概述从数据预处理和数据存储管理到与LLMs集成的攻击面。然后，将识别的风险与其相应的缓解措施在结构化的概述中配对。在第二步中，本文构建了一个框架，结合了特定于RAG的安全考虑与现有的通用安全指南、行业标准和最佳实践。所提议的框架旨在指导稳健、合规、安全和可信赖的RAG系统的实施。 

---
# Memorization-Compression Cycles Improve Generalization 

**Title (ZH)**: 记忆-压缩循环提高泛化能力 

**Authors**: Fangyuan Yu  

**Link**: [PDF](https://arxiv.org/pdf/2505.08727)  

**Abstract**: We prove theoretically that generalization improves not only through data scaling but also by compressing internal representations. To operationalize this insight, we introduce the Information Bottleneck Language Modeling (IBLM) objective, which reframes language modeling as a constrained optimization problem: minimizing representation entropy subject to optimal prediction performance. Empirically, we observe an emergent memorization-compression cycle during LLM pretraining, evidenced by oscillation positive/negative gradient alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of representation entropy. This pattern closely mirrors the predictive-compressive trade-off prescribed by IBLM and also parallels the biological alternation between awake learning and sleep consolidation. Motivated by this observation, we propose Gated Phase Transition (GAPT), a training algorithm that adaptively switches between memorization and compression phases. When applied to GPT-2 pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining task on arithmetic multiplication. In a setting designed to simulate catastrophic forgetting, GAPT reduces interference by compressing and separating representations, achieving a 97% improvement in separation - paralleling the functional role of sleep consolidation. 

**Abstract (ZH)**: 我们证明理论上，泛化不仅通过数据量扩大得到改善，还能通过压缩内部表示得到改善。为实现这一见解，我们引入了信息瓶颈语言建模（IBLM）目标，将其重新定义为受限优化问题：在最优预测性能的约束下，最小化表示的熵。实验上，我们观察到在大语言模型（LLM）预训练过程中存在一种新兴的记忆压缩循环，这体现在交叉熵和矩阵熵（MBE）之间的正负梯度波动上，后者衡量表示的熵。这一模式与IBLM规定的预测与压缩之间的权衡密切相关，也类似于清醒学习和睡眠巩固之间的生物学交替。受此观察的启发，我们提出了门控相变（GAPT）训练算法，该算法能够适应性地在记忆和压缩阶段之间切换。当将GAPT应用于使用FineWeb数据集对GPT-2进行预训练时，MBE降低了50%，且交叉熵提高了4.8%。在一项旨在模拟灾难性遗忘的预训练任务中，GAPT通过压缩和分离表示减少了干扰，分离度提高了97%，这与睡眠巩固的功能作用相 parallel。 

---
# PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts 

**Title (ZH)**: PWC-MoE: 建议保护无线协作混合专家模型的隐私 

**Authors**: Yang Su, Na Yan, Yansha Deng, Robert Schober  

**Link**: [PDF](https://arxiv.org/pdf/2505.08719)  

**Abstract**: Large language models (LLMs) hosted on cloud servers alleviate the computational and storage burdens on local devices but raise privacy concerns due to sensitive data transmission and require substantial communication bandwidth, which is challenging in constrained environments. In contrast, small language models (SLMs) running locally enhance privacy but suffer from limited performance on complex tasks. To balance computational cost, performance, and privacy protection under bandwidth constraints, we propose a privacy-aware wireless collaborative mixture of experts (PWC-MoE) framework. Specifically, PWC-MoE employs a sparse privacy-aware gating network to dynamically route sensitive tokens to privacy experts located on local clients, while non-sensitive tokens are routed to non-privacy experts located at the remote base station. To achieve computational efficiency, the gating network ensures that each token is dynamically routed to and processed by only one expert. To enhance scalability and prevent overloading of specific experts, we introduce a group-wise load-balancing mechanism for the gating network that evenly distributes sensitive tokens among privacy experts and non-sensitive tokens among non-privacy experts. To adapt to bandwidth constraints while preserving model performance, we propose a bandwidth-adaptive and importance-aware token offloading scheme. This scheme incorporates an importance predictor to evaluate the importance scores of non-sensitive tokens, prioritizing the most important tokens for transmission to the base station based on their predicted importance and the available bandwidth. Experiments demonstrate that the PWC-MoE framework effectively preserves privacy and maintains high performance even in bandwidth-constrained environments, offering a practical solution for deploying LLMs in privacy-sensitive and bandwidth-limited scenarios. 

**Abstract (ZH)**: 面向带宽约束环境的隐私aware无线协作专家混合框架（PWC-MoE） 

---
# Big Data and the Computational Social Science of Entrepreneurship and Innovation 

**Title (ZH)**: 大数据与创业创新的计算社会科学 

**Authors**: Ningzi Li, Shiyang Lai, James Evans  

**Link**: [PDF](https://arxiv.org/pdf/2505.08706)  

**Abstract**: As large-scale social data explode and machine-learning methods evolve, scholars of entrepreneurship and innovation face new research opportunities but also unique challenges. This chapter discusses the difficulties of leveraging large-scale data to identify technological and commercial novelty, document new venture origins, and forecast competition between new technologies and commercial forms. It suggests how scholars can take advantage of new text, network, image, audio, and video data in two distinct ways that advance innovation and entrepreneurship research. First, machine-learning models, combined with large-scale data, enable the construction of precision measurements that function as system-level observatories of innovation and entrepreneurship across human societies. Second, new artificial intelligence models fueled by big data generate 'digital doubles' of technology and business, forming laboratories for virtual experimentation about innovation and entrepreneurship processes and policies. The chapter argues for the advancement of theory development and testing in entrepreneurship and innovation by coupling big data with big models. 

**Abstract (ZH)**: 随着大规模社会数据的爆炸式增长和机器学习方法的发展，创业与创新领域的学者面临新的研究机遇同时也面临独特挑战。本章探讨了如何利用大规模数据识别技术与商业新颖性、记录新企业的起源以及预测新技术与商业形式之间的竞争。本章提出学者们可以通过两种方式利用新的文本、网络、图像、音频和视频数据，推动创新与创业研究的进步。首先，将机器学习模型与大规模数据结合，构建作为全球创新与创业系统的观测站的精密测量工具。其次，由大数据驱动的新人工智能模型生成技术与商业的“数字双胞胎”，形成关于创新与创业过程和政策的虚拟实验实验室。本章主张通过将大数据与大模型相结合，推动创业与创新领域的理论发展与测试。 

---
# Controllable Image Colorization with Instance-aware Texts and Masks 

**Title (ZH)**: 基于实例感知文本和掩码的可控图像着色 

**Authors**: Yanru An, Ling Gui, Qiang Hu, Chunlei Cai, Tianxiao Ye, Xiaoyun Zhang, Yanfeng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08705)  

**Abstract**: Recently, the application of deep learning in image colorization has received widespread attention. The maturation of diffusion models has further advanced the development of image colorization models. However, current mainstream image colorization models still face issues such as color bleeding and color binding errors, and cannot colorize images at the instance level. In this paper, we propose a diffusion-based colorization method MT-Color to achieve precise instance-aware colorization with use-provided guidance. To tackle color bleeding issue, we design a pixel-level mask attention mechanism that integrates latent features and conditional gray image features through cross-attention. We use segmentation masks to construct cross-attention masks, preventing pixel information from exchanging between different instances. We also introduce an instance mask and text guidance module that extracts instance masks and text representations of each instance, which are then fused with latent features through self-attention, utilizing instance masks to form self-attention masks to prevent instance texts from guiding the colorization of other areas, thus mitigating color binding errors. Furthermore, we apply a multi-instance sampling strategy, which involves sampling each instance region separately and then fusing the results. Additionally, we have created a specialized dataset for instance-level colorization tasks, GPT-color, by leveraging large visual language models on existing image datasets. Qualitative and quantitative experiments show that our model and dataset outperform previous methods and datasets. 

**Abstract (ZH)**: 基于扩散模型的精确实例感知颜色化方法MT-Color 

---
# A Survey of Deep Learning for Complex Speech Spectrograms 

**Title (ZH)**: 深度学习在复杂语音频谱图中的综述 

**Authors**: Yuying Xie, Zheng-Hua Tan  

**Link**: [PDF](https://arxiv.org/pdf/2505.08694)  

**Abstract**: Recent advancements in deep learning have significantly impacted the field of speech signal processing, particularly in the analysis and manipulation of complex spectrograms. This survey provides a comprehensive overview of the state-of-the-art techniques leveraging deep neural networks for processing complex spectrograms, which encapsulate both magnitude and phase information. We begin by introducing complex spectrograms and their associated features for various speech processing tasks. Next, we explore the key components and architectures of complex-valued neural networks, which are specifically designed to handle complex-valued data and have been applied for complex spectrogram processing. We then discuss various training strategies and loss functions tailored for training neural networks to process and model complex spectrograms. The survey further examines key applications, including phase retrieval, speech enhancement, and speech separation, where deep learning has achieved significant progress by leveraging complex spectrograms or their derived feature representations. Additionally, we examine the intersection of complex spectrograms with generative models. This survey aims to serve as a valuable resource for researchers and practitioners in the field of speech signal processing and complex-valued neural networks. 

**Abstract (ZH)**: 近期深度学习的进展对语音信号处理领域产生了显著影响，尤其是在复杂频谱图的分析和操控方面。本文综述了利用深度神经网络处理复杂频谱图的先进技术和方法，这些复杂频谱图包含了幅值和相位信息。我们首先介绍复杂频谱图及其在各种语音处理任务中的相关特征。接着，我们探讨了处理复值数据的复值神经网络的关键组件和架构。然后，我们讨论了适合训练神经网络处理和建模复杂频谱图的各种训练策略和损失函数。本文综述进一步探讨了将深度学习与复值频谱图应用于相位恢复、语音增强和语音分离等关键应用领域取得的重要进展。此外，我们还考察了复值频谱图与生成模型的交叉融合。本文旨在为语音信号处理和复值神经网络领域的研究人员和实务工作者提供有价值的资源。 

---
# VizCV: AI-assisted visualization of researchers' publications tracks 

**Title (ZH)**: VizCV：研究人员发表记录的AI辅助可视化 

**Authors**: Vladimír Lazárik, Marco Agus, Barbora Kozlíková, Pere-Pau Vázquez  

**Link**: [PDF](https://arxiv.org/pdf/2505.08691)  

**Abstract**: Analyzing how the publication records of scientists and research groups have evolved over the years is crucial for assessing their expertise since it can support the management of academic environments by assisting with career planning and evaluation. We introduce VizCV, a novel web-based end-to-end visual analytics framework that enables the interactive exploration of researchers' scientific trajectories. It incorporates AI-assisted analysis and supports automated reporting of career evolution. Our system aims to model career progression through three key dimensions: a) research topic evolution to detect and visualize shifts in scholarly focus over time, b) publication record and the corresponding impact, c) collaboration dynamics depicting the growth and transformation of a researcher's co-authorship network. AI-driven insights provide automated explanations of career transitions, detecting significant shifts in research direction, impact surges, or collaboration expansions. The system also supports comparative analysis between researchers, allowing users to compare topic trajectories and impact growth. Our interactive, multi-tab and multiview system allows for the exploratory analysis of career milestones under different perspectives, such as the most impactful articles, emerging research themes, or obtaining a detailed analysis of the contribution of the researcher in a subfield. The key contributions include AI/ML techniques for: a) topic analysis, b) dimensionality reduction for visualizing patterns and trends, c) the interactive creation of textual descriptions of facets of data through configurable prompt generation and large language models, that include key indicators, to help understanding the career development of individuals or groups. 

**Abstract (ZH)**: 分析科学家和研究团队的出版记录随时间的变化对于评估其专业水平至关重要，这有助于学术环境的管理，支持职业规划和评估。我们介绍了VizCV，一种新颖的基于Web的端到端可视化分析框架，使用户能够互动地探索研究人员的科学轨迹。该系统结合了AI辅助分析，并支持自动化的职业发展报告。我们的系统旨在通过三个关键维度建模职业发展：a) 研究主题演化，以检测和可视化随着时间的推移学术重点的变化，b) 发表记录及其相应的影响力，c) 合作动态，描绘研究者合著网络的成长与转变。基于AI的见解提供自动化的职业转变解释，检测研究方向的重大转变、影响力激增或协作扩展。该系统还支持研究人员之间的比较分析，允许用户比较研究主题轨迹和影响力增长。我们的交互式、多标签和多视图系统允许从不同的视角探索职业里程碑，如最具影响力的文章、新兴的研究主题，或对研究者在子领域的贡献进行详细的分析。关键贡献包括AI/ML技术：a) 主题分析，b) 维度减少以可视化模式和趋势，c) 通过可配置提示生成和大型语言模型互动创建数据各个方面文本描述，包括关键指标，以帮助理解个人或团队的职业发展。 

---
# AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks 

**Title (ZH)**: AC-PKAN：注意力增强和切比雪夫多项式基物理指导的柯尔莫哥洛夫-阿诺尔德网络 

**Authors**: Hangwei Zhang, Zhimu Huang, Yan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08687)  

**Abstract**: Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving partial differential equations (PDEs). Yet their original formulation is computationally and memory intensive, motivating the introduction of Chebyshev Type-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed the vanilla KANs architecture, our rigorous theoretical analysis reveals that they still suffer from rank collapse, ultimately limiting their expressive capacity. To overcome these limitations, we enhance Chebyshev1KANs by integrating wavelet-activated MLPs with learnable parameters and an internal attention mechanism. We prove that this design preserves a full-rank Jacobian and is capable of approximating solutions to PDEs of arbitrary order. Furthermore, to alleviate the loss instability and imbalance introduced by the Chebyshev polynomial basis, we externally incorporate a Residual Gradient Attention (RGA) mechanism that dynamically re-weights individual loss terms according to their gradient norms and residual magnitudes. By jointly leveraging internal and external attention, we present AC-PKAN, a novel architecture that constitutes an enhancement to weakly supervised Physics-Informed Neural Networks (PINNs) and extends the expressive power of KANs. Experimental results from nine benchmark tasks across three domains show that AC-PKAN consistently outperforms or matches state-of-the-art models such as PINNsFormer, establishing it as a highly effective tool for solving complex real-world engineering problems in zero-data or data-sparse regimes. The code will be made publicly available upon acceptance. 

**Abstract (ZH)**: Chebyshev Type-I 基增强的 Kolmogorov-Arnold 网络 (Chebyshev1KANs)：用于求解任意阶偏微分方程的新架构 

---
# A Mamba-based Network for Semi-supervised Singing Melody Extraction Using Confidence Binary Regularization 

**Title (ZH)**: 基于Mamba的半监督唱歌旋律提取网络：采用置信二元正则化 

**Authors**: Xiaoliang He, Kangjie Dong, Jingkai Cao, Shuai Yu, Wei Li, Yi Yu  

**Link**: [PDF](https://arxiv.org/pdf/2505.08681)  

**Abstract**: Singing melody extraction (SME) is a key task in the field of music information retrieval. However, existing methods are facing several limitations: firstly, prior models use transformers to capture the contextual dependencies, which requires quadratic computation resulting in low efficiency in the inference stage. Secondly, prior works typically rely on frequencysupervised methods to estimate the fundamental frequency (f0), which ignores that the musical performance is actually based on notes. Thirdly, transformers typically require large amounts of labeled data to achieve optimal performances, but the SME task lacks of sufficient annotated data. To address these issues, in this paper, we propose a mamba-based network, called SpectMamba, for semi-supervised singing melody extraction using confidence binary regularization. In particular, we begin by introducing vision mamba to achieve computational linear complexity. Then, we propose a novel note-f0 decoder that allows the model to better mimic the musical performance. Further, to alleviate the scarcity of the labeled data, we introduce a confidence binary regularization (CBR) module to leverage the unlabeled data by maximizing the probability of the correct classes. The proposed method is evaluated on several public datasets and the conducted experiments demonstrate the effectiveness of our proposed method. 

**Abstract (ZH)**: 基于SpectMamba的半监督歌唱旋律提取方法：使用置信二元正则化 

---
# A Social Robot with Inner Speech for Dietary Guidance 

**Title (ZH)**: 带有内心语言的社会机器人饮食指导 

**Authors**: Valerio Belcamino, Alessandro Carfì, Valeria Seidita, Fulvio Mastrogiovanni, Antonio Chella  

**Link**: [PDF](https://arxiv.org/pdf/2505.08664)  

**Abstract**: We explore the use of inner speech as a mechanism to enhance transparency and trust in social robots for dietary advice. In humans, inner speech structures thought processes and decision-making; in robotics, it improves explainability by making reasoning explicit. This is crucial in healthcare scenarios, where trust in robotic assistants depends on both accurate recommendations and human-like dialogue, which make interactions more natural and engaging. Building on this, we developed a social robot that provides dietary advice, and we provided the architecture with inner speech capabilities to validate user input, refine reasoning, and generate clear justifications. The system integrates large language models for natural language understanding and a knowledge graph for structured dietary information. By making decisions more transparent, our approach strengthens trust and improves human-robot interaction in healthcare. We validated this by measuring the computational efficiency of our architecture and conducting a small user study, which assessed the reliability of inner speech in explaining the robot's behavior. 

**Abstract (ZH)**: 我们探索内心言语作为机制以增强社交机器人在饮食建议中的透明度和信任度。基于此，我们开发了一种提供饮食建议的社交机器人，并为其赋予内心言语能力，以验证用户输入、改进推理并生成清晰的解释。该系统整合了大规模语言模型进行自然语言理解，以及知识图谱进行结构化的饮食信息。通过使决策过程更加透明，我们的方法可以增强信任并改善医疗保健场景中的人机交互。我们通过测量架构的计算效率和进行小型用户研究来验证这一点，该研究评估了内心言语在解释机器人行为方面的可靠性。 

---
# A Comparative Study of Human Activity Recognition: Motion, Tactile, and multi-modal Approaches 

**Title (ZH)**: 人类活动识别的比较研究：运动、触觉及多模态方法 

**Authors**: Valerio Belcamino, Nhat Minh Dinh Le, Quan Khanh Luu, Alessandro Carfì, Van Anh Ho, Fulvio Mastrogiovanni  

**Link**: [PDF](https://arxiv.org/pdf/2505.08657)  

**Abstract**: Human activity recognition (HAR) is essential for effective Human-Robot Collaboration (HRC), enabling robots to interpret and respond to human actions. This study evaluates the ability of a vision-based tactile sensor to classify 15 activities, comparing its performance to an IMU-based data glove. Additionally, we propose a multi-modal framework combining tactile and motion data to leverage their complementary strengths. We examined three approaches: motion-based classification (MBC) using IMU data, tactile-based classification (TBC) with single or dual video streams, and multi-modal classification (MMC) integrating both. Offline validation on segmented datasets assessed each configuration's accuracy under controlled conditions, while online validation on continuous action sequences tested online performance. Results showed the multi-modal approach consistently outperformed single-modality methods, highlighting the potential of integrating tactile and motion sensing to enhance HAR systems for collaborative robotics. 

**Abstract (ZH)**: 基于视觉的触觉传感器在15项活动分类中的评估及其在人机协作中的多模态框架研究 

---
# MINIMALIST: switched-capacitor circuits for efficient in-memory computation of gated recurrent units 

**Title (ZH)**: MINIMALIST：用于高效内存计算门控递归单元的开关电容电路 

**Authors**: Sebastian Billaudelle, Laura Kriener, Filippo Moro, Tristan Torchet, Melika Payvand  

**Link**: [PDF](https://arxiv.org/pdf/2505.08599)  

**Abstract**: Recurrent neural networks (RNNs) have been a long-standing candidate for processing of temporal sequence data, especially in memory-constrained systems that one may find in embedded edge computing environments. Recent advances in training paradigms have now inspired new generations of efficient RNNs. We introduce a streamlined and hardware-compatible architecture based on minimal gated recurrent units (GRUs), and an accompanying efficient mixed-signal hardware implementation of the model. The proposed design leverages switched-capacitor circuits not only for in-memory computation (IMC), but also for the gated state updates. The mixed-signal cores rely solely on commodity circuits consisting of metal capacitors, transmission gates, and a clocked comparator, thus greatly facilitating scaling and transfer to other technology nodes.
We benchmark the performance of our architecture on time series data, introducing all constraints required for a direct mapping to the hardware system. The direct compatibility is verified in mixed-signal simulations, reproducing data recorded from the software-only network model. 

**Abstract (ZH)**: 递归神经网络（RNNs）一直是处理时间序列数据的候选方法，特别是在嵌入式边缘计算环境中内存受限的系统中。近期在训练范式方面的进展已经启发了新一代高效RNN的设计。我们提出了一种基于 Minimal Gated Recurrent Units (MGRUs) 的精简且硬件兼容的架构，并且配套有高效的混合信号硬件实现。该提案设计不仅利用了开关电容电路进行存内计算（IMC），还用于门控状态更新。混合信号核仅依赖于由金属电容器、传输门和时钟比较器构成的普通电路，从而大大促进了规模化并转移到其他技术节点上。

我们在时间序列数据上benchmark了该架构的性能，并引入了所有必需的硬件约束，以实现直接映射到硬件系统。在混合信号仿真中验证了直接兼容性，并且重现了仅通过软件模型记录的数据。 

---
# MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment 

**Title (ZH)**: MESSI：城市环境多 elevation 语义分割图像数据集 

**Authors**: Barak Pinkovich, Boaz Matalon, Ehud Rivlin, Hector Rotstein  

**Link**: [PDF](https://arxiv.org/pdf/2505.08589)  

**Abstract**: This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI) dataset comprising 2525 images taken by a drone flying over dense urban environments. MESSI is unique in two main features. First, it contains images from various altitudes, allowing us to investigate the effect of depth on semantic segmentation. Second, it includes images taken from several different urban regions (at different altitudes). This is important since the variety covers the visual richness captured by a drone's 3D flight, performing horizontal and vertical maneuvers. MESSI contains images annotated with location, orientation, and the camera's intrinsic parameters and can be used to train a deep neural network for semantic segmentation or other applications of interest (e.g., localization, navigation, and tracking). This paper describes the dataset and provides annotation details. It also explains how semantic segmentation was performed using several neural network models and shows several relevant statistics. MESSI will be published in the public domain to serve as an evaluation benchmark for semantic segmentation using images captured by a drone or similar vehicle flying over a dense urban environment. 

**Abstract (ZH)**: 多高度语义分割图像数据集（MESSI）：用于密集城市环境无人机飞行捕获图像的语义分割评估基准 

---
# Small but Significant: On the Promise of Small Language Models for Accessible AIED 

**Title (ZH)**: 小而显著：小型语言模型在无障碍AI教育中的潜力 

**Authors**: Yumou Wei, Paulo Carvalho, John Stamper  

**Link**: [PDF](https://arxiv.org/pdf/2505.08588)  

**Abstract**: GPT has become nearly synonymous with large language models (LLMs), an increasingly popular term in AIED proceedings. A simple keyword-based search reveals that 61% of the 76 long and short papers presented at AIED 2024 describe novel solutions using LLMs to address some of the long-standing challenges in education, and 43% specifically mention GPT. Although LLMs pioneered by GPT create exciting opportunities to strengthen the impact of AI on education, we argue that the field's predominant focus on GPT and other resource-intensive LLMs (with more than 10B parameters) risks neglecting the potential impact that small language models (SLMs) can make in providing resource-constrained institutions with equitable and affordable access to high-quality AI tools. Supported by positive results on knowledge component (KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as Phi-2 can produce an effective solution without elaborate prompting strategies. Hence, we call for more attention to developing SLM-based AIED approaches. 

**Abstract (ZH)**: GPT几乎与大型语言模型（LLMs）同义，成为AIED会议中一个日益流行的概念。随着关键词搜索揭示，在AIED 2024呈现的76篇长篇和短篇论文中，61%描述了使用LLMs解决教育领域久未解决挑战的新方案，其中43%特别提到了GPT。尽管由GPT开创的LLMs为增强AI在教育中的影响带来了激动人心的机会，但我们认为，该领域的研究主要集中在GPT和其他资源密集型LLMs（参数超过10B）上，可能忽视了小型语言模型（SLMs）在为资源受限机构提供高质量AI工具的公平和可负担访问方面的作用。通过在知识组件（KC）发现这一AIED中的关键挑战上取得积极成果的支持，我们证明，如Phi-2这样的SLMs可以在无需复杂提示策略的情况下生成有效解决方案。因此，我们呼吁更多关注SLM为基础的AIED方法的发展。 

---
# DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art 

**Title (ZH)**: DFA-CON: 一种检测深伪艺术版权侵权的对比学习方法 

**Authors**: Haroon Wahab, Hassan Ugail, Irfan Mehmood  

**Link**: [PDF](https://arxiv.org/pdf/2505.08552)  

**Abstract**: Recent proliferation of generative AI tools for visual content creation-particularly in the context of visual artworks-has raised serious concerns about copyright infringement and forgery. The large-scale datasets used to train these models often contain a mixture of copyrighted and non-copyrighted artworks. Given the tendency of generative models to memorize training patterns, they are susceptible to varying degrees of copyright violation. Building on the recently proposed DeepfakeArt Challenge benchmark, this work introduces DFA-CON, a contrastive learning framework designed to detect copyright-infringing or forged AI-generated art. DFA-CON learns a discriminative representation space, posing affinity among original artworks and their forged counterparts within a contrastive learning framework. The model is trained across multiple attack types, including inpainting, style transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate robust detection performance across most attack types, outperforming recent pretrained foundation models. Code and model checkpoints will be released publicly upon acceptance. 

**Abstract (ZH)**: Recent proliferation of生成AI工具在视觉内容创作中的应用，特别是在视觉艺术领域的背景下，引发了严重的版权侵权和伪造担忧。这些模型的大型训练数据集 often包含版权和非版权艺术作品的混合。鉴于生成模型有记忆训练模式的趋势，它们在不同程度上容易侵犯版权。基于最近提出的DeepfakeArt挑战基准，本文引入了DFA-CON，一种对比学习框架，旨在检测版权侵权或伪造的AI生成艺术。DFA-CON通过对比学习框架学习一个区分性的表示空间，能够在原始艺术品与其伪造版本之间建立亲和力。该模型在包括内容填充、风格转换、对抗扰动和cutmix在内的多种攻击类型上进行了训练。评估结果表明，该模型在大多数攻击类型上表现出稳健的检测性能，优于近期的预训练基础模型。接受后，代码和模型检查点将公开发布。 

---
# From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation 

**Title (ZH)**: 从感知到执行：实现机器人操作中的推理与决策融合 

**Authors**: Yifu Yuan, Haiqin Cui, Yibin Chen, Zibin Dong, Fei Ni, Longxin Kou, Jinyi Liu, Pengyi Li, Yan Zheng, Jianye Hao  

**Link**: [PDF](https://arxiv.org/pdf/2505.08548)  

**Abstract**: Achieving generalization in robotic manipulation remains a critical challenge, particularly for unseen scenarios and novel tasks. Current Vision-Language-Action (VLA) models, while building on top of general Vision-Language Models (VLMs), still fall short of achieving robust zero-shot performance due to the scarcity and heterogeneity prevalent in embodied datasets. To address these limitations, we propose FSD (From Seeing to Doing), a novel vision-language model that generates intermediate representations through spatial relationship reasoning, providing fine-grained guidance for robotic manipulation. Our approach combines a hierarchical data pipeline for training with a self-consistency mechanism that aligns spatial coordinates with visual signals. Through extensive experiments, we comprehensively validated FSD's capabilities in both "seeing" and "doing," achieving outstanding performance across 8 benchmarks for general spatial reasoning and embodied reference abilities, as well as on our proposed more challenging benchmark VABench. We also verified zero-shot capabilities in robot manipulation, demonstrating significant performance improvements over baseline methods in both SimplerEnv and real robot settings. Experimental results show that FSD achieves 54.1% success rate in SimplerEnv and 72% success rate across 8 real-world tasks, outperforming the strongest baseline by 30%. 

**Abstract (ZH)**: 在机器人操作中实现泛化仍然是一个关键挑战，特别是在未见过的场景和新型任务中。当前的视觉-语言-动作（VLA）模型虽然基于通用的视觉-语言模型（VLM），但仍因体态数据中存在的稀缺性和异质性而难以实现稳健的零样本性能。为了解决这些局限性，我们提出FSD（From Seeing to Doing），一种新颖的视觉-语言模型，通过空间关系推理生成中间表示，为机器人操作提供精细指导。我们的方法结合了分层数据管道进行训练，并引入了一种自我一致性机制，使空间坐标与视觉信号对齐。通过大量的实验，我们全面验证了FSD在“看”和“做”方面的能力，在8个基准测试中取得了出色的泛空间推理能力和体态参考能力表现，并在我们提出的更具挑战性的基准测试VABench中表现优异。我们还验证了FSD在机器人操作中的零样本能力，在SimplerEnv和真实机器人设置中均表现出显著性能提升。实验结果表明，FSD在SimplerEnv中的成功率为54.1%，在8个真实世界任务中的成功率为72%，比最强基线方法高出30%。 

---
# The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large Language Models Unmask Fake News 

**Title (ZH)**: 辩论使真相更清晰！基于大型语言模型的多智能体系统揭露假新闻 

**Authors**: Yuhan Liu, Yuxuan Liu, Xiaoqing Zhang, Xiuying Chen, Rui Yan  

**Link**: [PDF](https://arxiv.org/pdf/2505.08532)  

**Abstract**: In today's digital environment, the rapid propagation of fake news via social networks poses significant social challenges. Most existing detection methods either employ traditional classification models, which suffer from low interpretability and limited generalization capabilities, or craft specific prompts for large language models (LLMs) to produce explanations and results directly, failing to leverage LLMs' reasoning abilities fully. Inspired by the saying that "truth becomes clearer through debate," our study introduces a novel multi-agent system with LLMs named TruEDebate (TED) to enhance the interpretability and effectiveness of fake news detection. TED employs a rigorous debate process inspired by formal debate settings. Central to our approach are two innovative components: the DebateFlow Agents and the InsightFlow Agents. The DebateFlow Agents organize agents into two teams, where one supports and the other challenges the truth of the news. These agents engage in opening statements, cross-examination, rebuttal, and closing statements, simulating a rigorous debate process akin to human discourse analysis, allowing for a thorough evaluation of news content. Concurrently, the InsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent and the Analysis Agent. The Synthesis Agent summarizes the debates and provides an overarching viewpoint, ensuring a coherent and comprehensive evaluation. The Analysis Agent, which includes a role-aware encoder and a debate graph, integrates role embeddings and models the interactions between debate roles and arguments using an attention mechanism, providing the final judgment. 

**Abstract (ZH)**: 在当今数字环境中，假新闻通过社交网络的快速传播引发了significant的社会挑战。现有的大多数检测方法要么采用传统分类模型，这些模型具有较低的可解释性和有限的一般化能力，要么为大型语言模型（LLMs）定制特定提示以直接生成解释和结果，未能充分利用LLMs的推理能力。受“辩论使真理显而易见”这一说法的启发，我们的研究提出了一种名为TruEDebate（TED）的新颖多智能体系统，以提高假新闻检测的可解释性和有效性。TED采用了一个基于正式辩论设置的严格辩论过程。在我们的方法中，有两个创新组成部分：DebateFlow智能体和InsightFlow智能体。DebateFlow智能体将智能体分为两支队伍，一支支持新闻的真实性，另一支挑战其真实性。这些智能体进行开场陈述、交叉询问、反驳和总结陈述，模拟类似于人类话语分析的严格辩论过程，允许对新闻内容进行全面评估。同时，InsightFlow智能体由两个专门的子智能体组成：综合智能体和分析智能体。综合智能体总结辩论并提供总体观点，确保评估的一致性和完整性。分析智能体包含角色感知编码器和辩论图，通过注意力机制整合角色嵌入，模型辩论角色和论点之间的互动，提供最终判断。 

---
# ExEBench: Benchmarking Foundation Models on Extreme Earth Events 

**Title (ZH)**: ExEBench: 极端地球事件基础模型评估基准 

**Authors**: Shan Zhao, Zhitong Xiong, Jie Zhao, Xiao Xiang Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2505.08529)  

**Abstract**: Our planet is facing increasingly frequent extreme events, which pose major risks to human lives and ecosystems. Recent advances in machine learning (ML), especially with foundation models (FMs) trained on extensive datasets, excel in extracting features and show promise in disaster management. Nevertheless, these models often inherit biases from training data, challenging their performance over extreme values. To explore the reliability of FM in the context of extreme events, we introduce \textbf{ExE}Bench (\textbf{Ex}treme \textbf{E}arth Benchmark), a collection of seven extreme event categories across floods, wildfires, storms, tropical cyclones, extreme precipitation, heatwaves, and cold waves. The dataset features global coverage, varying data volumes, and diverse data sources with different spatial, temporal, and spectral characteristics. To broaden the real-world impact of FMs, we include multiple challenging ML tasks that are closely aligned with operational needs in extreme events detection, monitoring, and forecasting. ExEBench aims to (1) assess FM generalizability across diverse, high-impact tasks and domains, (2) promote the development of novel ML methods that benefit disaster management, and (3) offer a platform for analyzing the interactions and cascading effects of extreme events to advance our understanding of Earth system, especially under the climate change expected in the decades to come. The dataset and code are public this https URL. 

**Abstract (ZH)**: 我们的星球正面临着越来越频繁的极端事件，这些事件对人类生活和生态系统构成了重大风险。最近在机器学习（ML）领域的进展，特别是针对大规模数据集训练的基础模型（FMs），在提取特征方面表现出色，并在灾难管理方面展现出前景。然而，这些模型通常会继承训练数据中的偏差，这对其在极端值上的表现构成了挑战。为了探索基础模型在极端事件 context 中的可靠性，我们介绍了 \textbf{ExE}Bench (\textbf{Ex}treme \textbf{E}arth Benchmark)，该基准集合了涵盖洪水、野火、风暴、热带气旋、极端降水、热浪和冷浪等七类极端事件。该数据集具有全球覆盖性，数据量和数据源多样，并且具有不同的空间、时间和光谱特征。为了扩大基础模型在实际世界中的影响，我们纳入了多个与极端事件检测、监测和预报紧密相关的具有挑战性的 ML 任务。ExEBench 的目标是：（1）评估基础模型在多样、高影响任务和领域中的泛化能力；（2）促进有利于灾难管理的新 ML 方法的发展；（3）提供一个平台来分析极端事件之间的相互作用及其连锁效应，以增进对我们地球系统理解，特别是在未来几十年预期的气候变化背景下。数据集和代码在此 https URL。 

---
# GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning 

**Title (ZH)**: GradMix：基于梯度的Selective Mixup在类增量学习中的鲁棒数据增强 

**Authors**: Minsu Kim, Seong-Hyeon Hwang, Steven Euijong Whang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08528)  

**Abstract**: In the context of continual learning, acquiring new knowledge while maintaining previous knowledge presents a significant challenge. Existing methods often use experience replay techniques that store a small portion of previous task data for training. In experience replay approaches, data augmentation has emerged as a promising strategy to further improve the model performance by mixing limited previous task data with sufficient current task data. However, we theoretically and empirically analyze that training with mixed samples from random sample pairs may harm the knowledge of previous tasks and cause greater catastrophic forgetting. We then propose GradMix, a robust data augmentation method specifically designed for mitigating catastrophic forgetting in class-incremental learning. GradMix performs gradient-based selective mixup using a class-based criterion that mixes only samples from helpful class pairs and not from detrimental class pairs for reducing catastrophic forgetting. Our experiments on various real datasets show that GradMix outperforms data augmentation baselines in accuracy by minimizing the forgetting of previous knowledge. 

**Abstract (ZH)**: 在持续学习的背景下，获取新知识的同时保持先前知识是一项重大挑战。现有方法通常使用经验回放技术，存储少量的先前任务数据进行训练。在经验回放方法中，数据增强被认为是一种通过混合有限的先前任务数据与足够的当前任务数据来进一步提高模型性能的有前途的策略。然而，我们的理论和实证分析表明，使用随机样本对的混合样本进行训练可能会损害先前任务的知识并导致更严重的灾难性遗忘。我们随后提出GradMix，这是一种专门设计用于缓解类别增量学习中灾难性遗忘的鲁棒数据增强方法。GradMix 使用基于类别的标准进行梯度选择性 mixup，只混合适对的样本而不是有害的样本对，以减少灾难性遗忘。我们在多种实际数据集上的实验结果显示，GradMix 在准确性上优于数据增强 baselines，并通过最小化遗忘的先前知识来实现这一目标。 

---
# Learning Advanced Self-Attention for Linear Transformers in the Singular Value Domain 

**Title (ZH)**: 学习奇异值域中高级自注意力机制的线性变换 

**Authors**: Hyowon Wi, Jeongwhan Choi, Noseong Park  

**Link**: [PDF](https://arxiv.org/pdf/2505.08516)  

**Abstract**: Transformers have demonstrated remarkable performance across diverse domains. The key component of Transformers is self-attention, which learns the relationship between any two tokens in the input sequence. Recent studies have revealed that the self-attention can be understood as a normalized adjacency matrix of a graph. Notably, from the perspective of graph signal processing (GSP), the self-attention can be equivalently defined as a simple graph filter, applying GSP using the value vector as the signal. However, the self-attention is a graph filter defined with only the first order of the polynomial matrix, and acts as a low-pass filter preventing the effective leverage of various frequency information. Consequently, existing self-attention mechanisms are designed in a rather simplified manner. Therefore, we propose a novel method, called \underline{\textbf{A}}ttentive \underline{\textbf{G}}raph \underline{\textbf{F}}ilter (AGF), interpreting the self-attention as learning the graph filter in the singular value domain from the perspective of graph signal processing for directed graphs with the linear complexity w.r.t. the input length $n$, i.e., $\mathcal{O}(nd^2)$. In our experiments, we demonstrate that AGF achieves state-of-the-art performance on various tasks, including Long Range Arena benchmark and time series classification. 

**Abstract (ZH)**: Transformer 在各种领域中展现了卓越的性能。其关键组件是自注意力，它学习输入序列中任意两个词元之间的关系。最近的研究表明，自注意力可以被视为图的归一化邻接矩阵。值得注意的是，从图信号处理（GSP）的角度看，自注意力可以等价地定义为一个简单的图滤波器，使用值向量作为信号进行GSP。然而，自注意力仅基于多项式矩阵的一阶定义了一个图滤波器，并起到低通滤波器的作用，限制了各种频率信息的有效利用。因此，现有的自注意力机制的设计相对简化。因此，我们提出了一种新的方法，称为 \underline{\textbf{A}}ttentive \underline{\textbf{G}}raph \underline{\textbf{F}}ilter（AGF），将其视为对于有向图从图信号处理的角度在奇异值域学习图滤波器，且输入长度 $n$ 的线性复杂度，即 $\mathcal{O}(nd^2)$。在我们的实验中，我们展示了AGF在多种任务上，包括Long Range Arena基准和时间序列分类上达到了最先进的性能。 

---
# LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models 

**Title (ZH)**: LCES：通过大型语言模型利用成对比较进行零样本自动作文评分 

**Authors**: Takumi Shibata, Yuichi Miyamura  

**Link**: [PDF](https://arxiv.org/pdf/2505.08498)  

**Abstract**: Recent advances in large language models (LLMs) have enabled zero-shot automated essay scoring (AES), providing a promising way to reduce the cost and effort of essay scoring in comparison with manual grading. However, most existing zero-shot approaches rely on LLMs to directly generate absolute scores, which often diverge from human evaluations owing to model biases and inconsistent scoring. To address these limitations, we propose LLM-based Comparative Essay Scoring (LCES), a method that formulates AES as a pairwise comparison task. Specifically, we instruct LLMs to judge which of two essays is better, collect many such comparisons, and convert them into continuous scores. Considering that the number of possible comparisons grows quadratically with the number of essays, we improve scalability by employing RankNet to efficiently transform LLM preferences into scalar scores. Experiments using AES benchmark datasets show that LCES outperforms conventional zero-shot methods in accuracy while maintaining computational efficiency. Moreover, LCES is robust across different LLM backbones, highlighting its applicability to real-world zero-shot AES. 

**Abstract (ZH)**: recent advances in large language models (llms) 促进了零样本自动化作文评分（aes）的发展，为与人工评分相比降低作文评分的成本和努力提供了有希望的方法。然而，现有的大多数零样本方法依赖于llm直接生成绝对评分，这往往由于模型偏见和评分不一致而与人工评估相背离。为解决这些局限性，我们提出了一种基于llm的比较式作文评分（lcès），该方法将aes形式化为两两比较任务。具体而言，我们指示llm判断两篇作文中哪一篇更好，收集许多这样的比较，并将其转换为连续评分。鉴于可能的比较数量随作文数量的增加呈平方增长，我们通过使用ranknet高效地将llm的偏好转换为标量评分来提高可扩展性。使用aes基准数据集的实验表明，lcès在准确性和计算效率方面优于传统零样本方法。此外，lcès在不同的llm底层模型上表现出稳健性，突显了其在实际零样本aes中的应用潜力。 

---
# An adaptive sampling algorithm for data-generation to build a data-manifold for physical problem surrogate modeling 

**Title (ZH)**: 自适应采样算法用于数据生成以构建物理问题代理模型的数据流形 

**Authors**: Chetra Mang, Axel TahmasebiMoradi, David Danan, Mouadh Yagoubi  

**Link**: [PDF](https://arxiv.org/pdf/2505.08487)  

**Abstract**: Physical models classically involved Partial Differential equations (PDE) and depending of their underlying complexity and the level of accuracy required, and known to be computationally expensive to numerically solve them. Thus, an idea would be to create a surrogate model relying on data generated by such solver. However, training such a model on an imbalanced data have been shown to be a very difficult task. Indeed, if the distribution of input leads to a poor response manifold representation, the model may not learn well and consequently, it may not predict the outcome with acceptable accuracy. In this work, we present an Adaptive Sampling Algorithm for Data Generation (ASADG) involving a physical model. As the initial input data may not accurately represent the response manifold in higher dimension, this algorithm iteratively adds input data into it. At each step the barycenter of each simplicial complex, that the manifold is discretized into, is added as new input data, if a certain threshold is satisfied. We demonstrate the efficiency of the data sampling algorithm in comparison with LHS method for generating more representative input data. To do so, we focus on the construction of a harmonic transport problem metamodel by generating data through a classical solver. By using such algorithm, it is possible to generate the same number of input data as LHS while providing a better representation of the response manifold. 

**Abstract (ZH)**: 自适应采样算法生成数据以增强物理模型（ASADG）：基于平衡数据生成更具代表性的输入数据 

---
# Distributed Quantum Neural Networks on Distributed Photonic Quantum Computing 

**Title (ZH)**: 分布式光子量子计算中的分布式量子神经网络 

**Authors**: Kuan-Cheng Chen, Chen-Yu Liu, Yu Shang, Felix Burt, Kin K. Leung  

**Link**: [PDF](https://arxiv.org/pdf/2505.08474)  

**Abstract**: We introduce a distributed quantum-classical framework that synergizes photonic quantum neural networks (QNNs) with matrix-product-state (MPS) mapping to achieve parameter-efficient training of classical neural networks. By leveraging universal linear-optical decompositions of $M$-mode interferometers and photon-counting measurement statistics, our architecture generates neural parameters through a hybrid quantum-classical workflow: photonic QNNs with $M(M+1)/2$ trainable parameters produce high-dimensional probability distributions that are mapped to classical network weights via an MPS model with bond dimension $\chi$. Empirical validation on MNIST classification demonstrates that photonic QT achieves an accuracy of $95.50\% \pm 0.84\%$ using 3,292 parameters ($\chi = 10$), compared to $96.89\% \pm 0.31\%$ for classical baselines with 6,690 parameters. Moreover, a ten-fold compression ratio is achieved at $\chi = 4$, with a relative accuracy loss of less than $3\%$. The framework outperforms classical compression techniques (weight sharing/pruning) by 6--12\% absolute accuracy while eliminating quantum hardware requirements during inference through classical deployment of compressed parameters. Simulations incorporating realistic photonic noise demonstrate the framework's robustness to near-term hardware imperfections. Ablation studies confirm quantum necessity: replacing photonic QNNs with random inputs collapses accuracy to chance level ($10.0\% \pm 0.5\%$). Photonic quantum computing's room-temperature operation, inherent scalability through spatial-mode multiplexing, and HPC-integrated architecture establish a practical pathway for distributed quantum machine learning, combining the expressivity of photonic Hilbert spaces with the deployability of classical neural networks. 

**Abstract (ZH)**: 一种结合光子量子神经网络与矩阵积态映射的分布式量子-经典框架：参数高效训练经典神经网络 

---
# RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models 

**Title (ZH)**: RepCali：通过潜在空间表示校准提高效率的预训练语言模型微调方法 

**Authors**: Fujun Zhang, XiangDong Su  

**Link**: [PDF](https://arxiv.org/pdf/2505.08463)  

**Abstract**: Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm in applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs still struggle with the discrepancies between the representation obtained from the PLMs' encoder and the optimal input to the PLMs' decoder. This paper tackles this challenge by learning to calibrate the representation of PLMs in the latent space. In the proposed representation calibration method (RepCali), we integrate a specific calibration block to the latent space after the encoder and use the calibrated output as the decoder input. The merits of the proposed RepCali include its universality to all PLMs with encoder-decoder architectures, its plug-and-play nature, and ease of implementation. Extensive experiments on 25 PLM-based models across 8 tasks (including both English and Chinese datasets) demonstrate that the proposed RepCali offers desirable enhancements to PLMs (including LLMs) and significantly improves the performance of downstream tasks. Comparison experiments across 4 benchmark tasks indicate that RepCali is superior to the representative fine-tuning baselines. 

**Abstract (ZH)**: Fine-tuning 预训练语言模型中的表示校准以适应下游任务 

---
# Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency 

**Title (ZH)**: 优化检索增强生成：超参数对性能和效率影响的分析 

**Authors**: Adel Ammar, Anis Koubaa, Omer Nacar, Wadii Boulila  

**Link**: [PDF](https://arxiv.org/pdf/2505.08445)  

**Abstract**: Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma and Faiss vector stores, chunking policies, cross-encoder re-ranking, and temperature, and we evaluate six metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall, and answer similarity. Chroma processes queries 13% faster, whereas Faiss yields higher retrieval precision, revealing a clear speed-accuracy trade-off. Naive fixed-length chunking with small windows and minimal overlap outperforms semantic segmentation while remaining the quickest option. Re-ranking provides modest gains in retrieval quality yet increases runtime by roughly a factor of 5, so its usefulness depends on latency constraints. These results help practitioners balance computational cost and accuracy when tuning RAG systems for transparent, up-to-date responses. Finally, we re-evaluate the top configurations with a corrective RAG workflow and show that their advantages persist when the model can iteratively request additional evidence. We obtain a near-perfect context precision (99%), which demonstrates that RAG systems can achieve extremely high retrieval accuracy with the right combination of hyperparameters, with significant implications for applications where retrieval quality directly impacts downstream task performance, such as clinical decision support in healthcare. 

**Abstract (ZH)**: 大型语言模型在完成任务方面表现出色，但往往会出现幻觉或依赖过时的知识。检索增强生成（RAG）通过将生成与外部搜索耦合来弥补这些差距。我们分析了超参数如何影响RAG系统的速度和质量，涵盖Chroma和Faiss向量存储、切分策略、交叉编码重排序以及温度，并评估了六项指标：忠实度、答案正确性、答案相关性、上下文精度、上下文召回率和答案相似性。Chroma查询处理速度比Faiss快13%，而Faiss检索精度更高，揭示了明显的速度-准确性权衡。使用小窗口和最小重叠的朴素固定长度切分策略优于语义分割，同时仍然是最快的选择。重排序在检索质量方面提供了适度的改进，但运行时间约增加5倍，因此其适用性取决于延迟约束。这些结果有助于实践者在调整RAG系统以实现透明且及时的响应时平衡计算成本和准确性。最后，我们使用修正的RAG工作流重新评估了顶级配置，并展示了当模型可以迭代请求额外证据时，其优势依然存在。我们获得了近完美的上下文精度（99%），这表明在正确组合超参数的情况下，RAG系统可以实现极高的检索精度，对于检索质量直接影响下游任务性能的应用具有重要意义，如医疗保健中的临床决策支持。 

---
# A Survey of 3D Reconstruction with Event Cameras: From Event-based Geometry to Neural 3D Rendering 

**Title (ZH)**: 事件相机下的3D重建综述：从事件驱动几何到神经3D渲染 

**Authors**: Chuanzhi Xu, Haoxian Zhou, Langyi Chen, Haodong Chen, Ying Zhou, Vera Chung, Qiang Qu  

**Link**: [PDF](https://arxiv.org/pdf/2505.08438)  

**Abstract**: Event cameras have emerged as promising sensors for 3D reconstruction due to their ability to capture per-pixel brightness changes asynchronously. Unlike conventional frame-based cameras, they produce sparse and temporally rich data streams, which enable more accurate 3D reconstruction and open up the possibility of performing reconstruction in extreme environments such as high-speed motion, low light, or high dynamic range scenes. In this survey, we provide the first comprehensive review focused exclusively on 3D reconstruction using event cameras. The survey categorises existing works into three major types based on input modality - stereo, monocular, and multimodal systems, and further classifies them by reconstruction approach, including geometry-based, deep learning-based, and recent neural rendering techniques such as Neural Radiance Fields and 3D Gaussian Splatting. Methods with a similar research focus were organised chronologically into the most subdivided groups. We also summarise public datasets relevant to event-based 3D reconstruction. Finally, we highlight current research limitations in data availability, evaluation, representation, and dynamic scene handling, and outline promising future research directions. This survey aims to serve as a comprehensive reference and a roadmap for future developments in event-driven 3D reconstruction. 

**Abstract (ZH)**: 事件相机由于能够异步捕获逐像素亮度变化而成为了三维重建的有前途的传感器。本文综述首次全面聚焦于使用事件相机进行三维重建的研究。综述根据输入模态将现有工作划分为立体、单目和多模态系统三类，并进一步按重建方法分类，包括基于几何的方法、基于深度学习的方法以及最近的神经渲染技术如Neural Radiance Fields和3D Gaussian Splatting。具有类似研究重点的方法按照时间顺序进行了最详细的分类。此外，综述总结了与事件驱动三维重建相关的公开数据集，并指出现有研究在数据可用性、评估、表示以及动态场景处理方面的局限性，提出了有希望的未来研究方向。本文综述旨在成为事件驱动三维重建未来发展的全面参考和路线图。 

---
# Hakim: Farsi Text Embedding Model 

**Title (ZH)**: Hákim: 波斯文本嵌入模型 

**Authors**: Mehran Sarmadi, Morteza Alikhani, Erfan Zinvandi, Zahra Pourbahman  

**Link**: [PDF](https://arxiv.org/pdf/2505.08435)  

**Abstract**: Recent advancements in text embedding have significantly improved natural language understanding across many languages, yet Persian remains notably underrepresented in large-scale embedding research. In this paper, we present Hakim, a novel state-of-the-art Persian text embedding model that achieves a 8.5% performance improvement over existing approaches on the FaMTEB benchmark, outperforming all previously developed Persian language models. As part of this work, we introduce three new datasets - Corpesia, Pairsia-sup, and Pairsia-unsup - to support supervised and unsupervised training scenarios. Additionally, Hakim is designed for applications in chatbots and retrieval-augmented generation (RAG) systems, particularly addressing retrieval tasks that require incorporating message history within these systems. We also propose a new baseline model built on the BERT architecture. Our language model consistently achieves higher accuracy across various Persian NLP tasks, while the RetroMAE-based model proves particularly effective for textual information retrieval applications. Together, these contributions establish a new foundation for advancing Persian language understanding. 

**Abstract (ZH)**: 最近在文本嵌入方面的进展显著提高了多种语言的自然语言理解能力，但波斯语在大规模嵌入研究中的代表性仍然不足。本文介绍了Hakim，一种新型的波斯文本嵌入模型，在FaMTEB基准测试中相比现有方法取得了8.5%的性能提升，超过了之前所有开发的波斯语言模型。作为这项工作的组成部分，我们引入了三个新的数据集——Corpesia、Pairsia-sup和Pairsia-unsup，以支持监督和无监督的训练场景。此外，Hakim 专为聊天机器人和检索增强生成（RAG）系统设计，特别针对需要在这些系统中整合消息历史的检索任务。我们还提出了一种基于BERT架构的新基线模型。我们的语言模型在各种波斯NLP任务中始终保持更高的准确性，而基于RetroMAE的模型在文本信息检索应用中表现尤为出色。这些贡献共同为推进波斯语理解奠定了新的基础。 

---
# ConDiSim: Conditional Diffusion Models for Simulation Based Inference 

**Title (ZH)**: ConDiSim：基于条件的扩散模型在模拟推断中的应用 

**Authors**: Mayank Nautiyal, Andreas Hellander, Prashant Singh  

**Link**: [PDF](https://arxiv.org/pdf/2505.08403)  

**Abstract**: We present a conditional diffusion model - ConDiSim, for simulation-based inference of complex systems with intractable likelihoods. ConDiSim leverages denoising diffusion probabilistic models to approximate posterior distributions, consisting of a forward process that adds Gaussian noise to parameters, and a reverse process learning to denoise, conditioned on observed data. This approach effectively captures complex dependencies and multi-modalities within posteriors. ConDiSim is evaluated across ten benchmark problems and two real-world test problems, where it demonstrates effective posterior approximation accuracy while maintaining computational efficiency and stability in model training. ConDiSim offers a robust and extensible framework for simulation-based inference, particularly suitable for parameter inference workflows requiring fast inference methods. 

**Abstract (ZH)**: 基于模拟的难以计算似然函数的复杂系统条件扩散模型-ConDiSim推理方法 

---
# Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping 

**Title (ZH)**: 加速链式思维推理：当目标梯度重要性遇上了动态跳过 

**Authors**: Ren Zhuang, Ben Wang, Shuifa Sun  

**Link**: [PDF](https://arxiv.org/pdf/2505.08392)  

**Abstract**: Large Language Models leverage Chain-of-Thought (CoT) prompting for complex tasks, but their reasoning traces are often excessively verbose and inefficient, leading to significant computational costs and latency. Current CoT compression techniques typically rely on generic importance metrics and static compression rates, which may inadvertently remove functionally critical tokens or fail to adapt to varying reasoning complexity. To overcome these limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning. This approach introduces two synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric accurately identifying functionally relevant tokens by measuring the gradient influence of their intermediate representations on the final answer loss, and (2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the compression rate based on runtime model uncertainty while ensuring local coherence through an adaptive N-token constraint. To our knowledge, this is the first work unifying a goal-oriented, gradient-based importance metric with dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It achieves substantial efficiency gains - reducing CoT token counts by over 45% on average and delivering 1.6-2.0 times inference speedups - while maintaining high reasoning accuracy. Notably, it significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off. 

**Abstract (ZH)**: 大型语言模型通过链式思考（CoT）提示处理复杂任务，但其推理过程往往过于冗长且效率低下，导致显著的计算成本和延迟。当前的CoT压缩技术通常依赖通用的重要性和固定压缩率，这可能会无意中删除功能上重要的Token，或者无法适应推理复杂度的差异。为克服这些限制，我们提出了一种名为Adaptive GoGI-Skip的新型框架，通过监督微调学习动态的CoT压缩。该方法引入了两大协同创新：（1）目标梯度重要性（GoGI），这是一种新的度量标准，能够通过衡量中间表示对最终答案损失的梯度影响来准确识别功能相关的Token；（2）自适应动态跳过（ADS），这是一种机制，在确保局部一致性的同时，通过自适应的N-Token约束动态调节压缩率，基于运行时模型的不确定性进行调节。据我们所知，这是首次将目标导向的、基于梯度的重要性度量与动态的、基于不确定性感知的跳过相结合用于CoT压缩的工作。在压缩的MATH数据上训练后，Adaptive GoGI-Skip在包括AIME、GPQA和GSM8K在内的多种推理基准测试中展示了强大的跨领域泛化能力，实现了显著的效率提升——平均减少CoT Token计数超过45%，并在推理速度上提高1.6至2.0倍，同时保持高推理准确性。尤为值得注意的是，它在高有效压缩率下甚至还能保持准确率，进一步推动了CoT推理效率与准确性的折中效果。 

---
# Adaptive Diffusion Policy Optimization for Robotic Manipulation 

**Title (ZH)**: 适应性扩散策略优化在机器人 manipulation 中的应用 

**Authors**: Huiyun Jiang, Zhuang Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08376)  

**Abstract**: Recent studies have shown the great potential of diffusion models in improving reinforcement learning (RL) by modeling complex policies, expressing a high degree of multi-modality, and efficiently handling high-dimensional continuous control tasks. However, there is currently limited research on how to optimize diffusion-based polices (e.g., Diffusion Policy) fast and stably. In this paper, we propose an Adam-based Diffusion Policy Optimization (ADPO), a fast algorithmic framework containing best practices for fine-tuning diffusion-based polices in robotic control tasks using the adaptive gradient descent method in RL. Adaptive gradient method is less studied in training RL, let alone diffusion-based policies. We confirm that ADPO outperforms other diffusion-based RL methods in terms of overall effectiveness for fine-tuning on standard robotic tasks. Concretely, we conduct extensive experiments on standard robotic control tasks to test ADPO, where, particularly, six popular diffusion-based RL methods are provided as benchmark methods. Experimental results show that ADPO acquires better or comparable performance than the baseline methods. Finally, we systematically analyze the sensitivity of multiple hyperparameters in standard robotics tasks, providing guidance for subsequent practical applications. Our video demonstrations are released in this https URL. 

**Abstract (ZH)**: 基于Adam的扩散政策优化（ADPO）：机器人控制任务中快速稳定的扩散政策优化方法 

---
# Non-contact Vital Signs Detection in Dynamic Environments 

**Title (ZH)**: 动态环境中的非接触生命体征检测 

**Authors**: Shuai Sun, Chong-Xi Liang, Chengwei Ye, Huanzhen Zhang, Kangsheng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08366)  

**Abstract**: Accurate phase demodulation is critical for vital sign detection using millimeter-wave radar. However, in complex environments, time-varying DC offsets and phase imbalances can severely degrade demodulation performance. To address this, we propose a novel DC offset calibration method alongside a Hilbert and Differential Cross-Multiply (HADCM) demodulation algorithm. The approach estimates time-varying DC offsets from neighboring signal peaks and valleys, then employs both differential forms and Hilbert transforms of the I/Q channel signals to extract vital sign information. Simulation and experimental results demonstrate that the proposed method maintains robust performance under low signal-to-noise ratios. Compared to existing demodulation techniques, it offers more accurate signal recovery in challenging scenarios and effectively suppresses noise interference. 

**Abstract (ZH)**: 准确的相位解调对于使用毫米波雷达检测生理信号至关重要。然而，在复杂环境中，时间 varying 直流偏移和相位失配会严重降低解调性能。为此，我们提出了一种新颖的直流偏移校准方法以及基于希尔伯特和差分交叉乘积（HADCM）的解调算法。该方法从相邻信号峰值和谷值中估计时间 varying 直流偏移，然后利用 I/Q 通道信号的差分解和希尔伯特变换提取生理信号信息。仿真和实验结果表明，所提出的方法在低信噪比下保持了稳健的性能。与现有的解调技术相比，它在具有挑战性的场景中提供了更准确的信号恢复，并有效抑制了噪声干扰。 

---
# STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives 

**Title (ZH)**: 故事锚点：生成长篇叙事的 consistent 多场景故事框架 

**Authors**: Bo Wang, Haoyang Huang, Zhiyin Lu, Fengyuan Liu, Guoqing Ma, Jianlong Yuan, Yuan Zhang, Nan Duan  

**Link**: [PDF](https://arxiv.org/pdf/2505.08350)  

**Abstract**: This paper introduces StoryAnchors, a unified framework for generating high-quality, multi-scene story frames with strong temporal consistency. The framework employs a bidirectional story generator that integrates both past and future contexts to ensure temporal consistency, character continuity, and smooth scene transitions throughout the narrative. Specific conditions are introduced to distinguish story frame generation from standard video synthesis, facilitating greater scene diversity and enhancing narrative richness. To further improve generation quality, StoryAnchors integrates Multi-Event Story Frame Labeling and Progressive Story Frame Training, enabling the model to capture both overarching narrative flow and event-level dynamics. This approach supports the creation of editable and expandable story frames, allowing for manual modifications and the generation of longer, more complex sequences. Extensive experiments show that StoryAnchors outperforms existing open-source models in key areas such as consistency, narrative coherence, and scene diversity. Its performance in narrative consistency and story richness is also on par with GPT-4o. Ultimately, StoryAnchors pushes the boundaries of story-driven frame generation, offering a scalable, flexible, and highly editable foundation for future research. 

**Abstract (ZH)**: StoryAnchors：一种生成高质量多场景故事框架的统一框架 

---
# FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot Learning 

**Title (ZH)**: FAD：频率适配与转移missive跨域少样本学习 

**Authors**: Ruixiao Shi, Fu Feng, Yucheng Xie, Jing Wang, Xin Geng  

**Link**: [PDF](https://arxiv.org/pdf/2505.08349)  

**Abstract**: Cross-domain few-shot learning (CD-FSL) requires models to generalize from limited labeled samples under significant distribution shifts. While recent methods enhance adaptability through lightweight task-specific modules, they operate solely in the spatial domain and overlook frequency-specific variations that are often critical for robust transfer. We observe that spatially similar images across domains can differ substantially in their spectral representations, with low and high frequencies capturing complementary semantic information at coarse and fine levels. This indicates that uniform spatial adaptation may overlook these spectral distinctions, thus constraining generalization. To address this, we introduce Frequency Adaptation and Diversion (FAD), a frequency-aware framework that explicitly models and modulates spectral components. At its core is the Frequency Diversion Adapter, which transforms intermediate features into the frequency domain using the discrete Fourier transform (DFT), partitions them into low, mid, and high-frequency bands via radial masks, and reconstructs each band using inverse DFT (IDFT). Each frequency band is then adapted using a dedicated convolutional branch with a kernel size tailored to its spectral scale, enabling targeted and disentangled adaptation across frequencies. Extensive experiments on the Meta-Dataset benchmark demonstrate that FAD consistently outperforms state-of-the-art methods on both seen and unseen domains, validating the utility of frequency-domain representations and band-wise adaptation for improving generalization in CD-FSL. 

**Abstract (ZH)**: 跨领域少样本学习中的频率适应与离散（Frequency Adaptation and Diversion for Cross-Domain Few-Shot Learning） 

---
# SHAP-based Explanations are Sensitive to Feature Representation 

**Title (ZH)**: 基于SHAP的解释对特征表示敏感 

**Authors**: Hyunseung Hwang, Andrew Bell, Joao Fonseca, Venetia Pliatsika, Julia Stoyanovich, Steven Euijong Whang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08345)  

**Abstract**: Local feature-based explanations are a key component of the XAI toolkit. These explanations compute feature importance values relative to an ``interpretable'' feature representation. In tabular data, feature values themselves are often considered interpretable. This paper examines the impact of data engineering choices on local feature-based explanations. We demonstrate that simple, common data engineering techniques, such as representing age with a histogram or encoding race in a specific way, can manipulate feature importance as determined by popular methods like SHAP. Notably, the sensitivity of explanations to feature representation can be exploited by adversaries to obscure issues like discrimination. While the intuition behind these results is straightforward, their systematic exploration has been lacking. Previous work has focused on adversarial attacks on feature-based explainers by biasing data or manipulating models. To the best of our knowledge, this is the first study demonstrating that explainers can be misled by standard, seemingly innocuous data engineering techniques. 

**Abstract (ZH)**: 基于局部特征的解释是XAI工具包的关键组成部分。这些解释计算相对于“可解释”特征表示的特征重要性值。在表格数据中，特征值本身往往被认为是可解释的。本文探讨了数据工程选择对基于局部特征的解释的影响。我们展示了简单的常见数据工程技术，如将年龄表示为直方图或以特定方式编码种族，可以操控如SHAP等流行方法确定的特征重要性。值得注意的是，解释对特征表示的敏感性可以被对手利用以掩盖诸如歧视之类的问题。尽管这些结果背后的直觉是直接的，但它们的系统性探索却一直缺失。以往的工作主要集中在通过偏向数据或操控模型来进行特征基解释的对抗攻击。据我们所知，这是第一次研究证明标准的、看似无害的数据工程技术可以误导解释器的研究。 

---
# A computer vision-based model for occupancy detection using low-resolution thermal images 

**Title (ZH)**: 基于计算机视觉的低分辨率热成像占用检测模型 

**Authors**: Xue Cui, Vincent Gbouna Zakka, Minhyun Lee  

**Link**: [PDF](https://arxiv.org/pdf/2505.08336)  

**Abstract**: Occupancy plays an essential role in influencing the energy consumption and operation of heating, ventilation, and air conditioning (HVAC) systems. Traditional HVAC typically operate on fixed schedules without considering occupancy. Advanced occupant-centric control (OCC) adopted occupancy status in regulating HVAC operations. RGB images combined with computer vision (CV) techniques are widely used for occupancy detection, however, the detailed facial and body features they capture raise significant privacy concerns. Low-resolution thermal images offer a non-invasive solution that mitigates privacy issues. The study developed an occupancy detection model utilizing low-resolution thermal images and CV techniques, where transfer learning was applied to fine-tune the You Only Look Once version 5 (YOLOv5) model. The developed model ultimately achieved satisfactory performance, with precision, recall, mAP50, and mAP50 values approaching 1.000. The contributions of this model lie not only in mitigating privacy concerns but also in reducing computing resource demands. 

**Abstract (ZH)**: 占用状态在影响 Heating, Ventilation, and Air Conditioning (HVAC) 系统的能耗和运行中发挥着重要作用。传统 HVAC 通常基于固定时间表运行，不考虑占用状态。先进的以占用者为中心的控制（OCC）将占用状态纳入 HVAC 运行调控。RGB 图像结合计算机视觉（CV）技术广泛用于占用状态检测，但它们捕获的详细面部和身体特征引起了重大的隐私担忧。低分辨率热成像提供了一种非侵入性解决方案，可以缓解隐私问题。本研究开发了一种利用低分辨率热成像和 CV 技术的占用状态检测模型，其中应用了迁移学习对 You Only Look Once 版本 5 (YOLOv5) 模型进行了微调。所开发的模型最终取得了令人满意的效果，精度、召回率、mAP50 和 mAP50 值接近 1.000。该模型的贡献不仅在于缓解隐私问题，还在于减少计算资源需求。 

---
# Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer 

**Title (ZH)**: 低复杂度持续学习中的压缩知识迁移推理 

**Authors**: Zhenrong Liu, Janne M. J. Huttunen, Mikko Honkala  

**Link**: [PDF](https://arxiv.org/pdf/2505.08327)  

**Abstract**: Continual learning (CL) aims to train models that can learn a sequence of tasks without forgetting previously acquired knowledge. A core challenge in CL is balancing stability -- preserving performance on old tasks -- and plasticity -- adapting to new ones. Recently, large pre-trained models have been widely adopted in CL for their ability to support both, offering strong generalization for new tasks and resilience against forgetting. However, their high computational cost at inference time limits their practicality in real-world applications, especially those requiring low latency or energy efficiency. To address this issue, we explore model compression techniques, including pruning and knowledge distillation (KD), and propose two efficient frameworks tailored for class-incremental learning (CIL), a challenging CL setting where task identities are unavailable during inference. The pruning-based framework includes pre- and post-pruning strategies that apply compression at different training stages. The KD-based framework adopts a teacher-student architecture, where a large pre-trained teacher transfers downstream-relevant knowledge to a compact student. Extensive experiments on multiple CIL benchmarks demonstrate that the proposed frameworks achieve a better trade-off between accuracy and inference complexity, consistently outperforming strong baselines. We further analyze the trade-offs between the two frameworks in terms of accuracy and efficiency, offering insights into their use across different scenarios. 

**Abstract (ZH)**: 连续学习（CL）旨在训练能够在不忘记先前获得的知识的情况下学习一系列任务的模型。CL的核心挑战是在保持旧任务性能（稳定性）与适应新任务（可塑性）之间取得平衡。近年来，由于其支持两者的能力，大型预训练模型在CL中得到了广泛应用，能够为新任务提供强大的泛化能力和抵抗遗忘的鲁棒性。然而，它们在推理时的高计算成本限制了其实用性，特别是在需要低延迟或高能效的应用中。为解决这一问题，我们探索了模型压缩技术，包括剪枝和知识蒸馏（KD），并提出两种针对类别增量学习（CIL）的高效框架，CIL是一个在推理期间缺乏任务身份信息的挑战性CL设置。基于剪枝的框架包括预剪枝和后剪枝策略，分别在训练的不同阶段进行压缩。基于KD的框架采用老师-学生架构，其中大型预训练老师向紧凑的学生传递下游相关知识。在多个CIL基准上的广泛实验表明，所提出的框架在准确性和推理复杂性之间取得了更好的权衡，始终优于强 baseline。我们进一步分析了两种框架在准确性和效率方面的权衡，为它们在不同场景中的使用提供了见解。 

---
# FedRS-Bench: Realistic Federated Learning Datasets and Benchmarks in Remote Sensing 

**Title (ZH)**: FedRS-Bench: 远程 sensing 领域的现实联邦学习数据集和基准 

**Authors**: Haodong Zhao, Peng Peng, Chiyu Chen, Linqing Huang, Gongshen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.08325)  

**Abstract**: Remote sensing (RS) images are usually produced at an unprecedented scale, yet they are geographically and institutionally distributed, making centralized model training challenging due to data-sharing restrictions and privacy concerns. Federated learning (FL) offers a solution by enabling collaborative model training across decentralized RS data sources without exposing raw data. However, there lacks a realistic federated dataset and benchmark in RS. Prior works typically rely on manually partitioned single dataset, which fail to capture the heterogeneity and scale of real-world RS data, and often use inconsistent experimental setups, hindering fair comparison. To address this gap, we propose a realistic federated RS dataset, termed FedRS. FedRS consists of eight datasets that cover various sensors and resolutions and builds 135 clients, which is representative of realistic operational scenarios. Data for each client come from the same source, exhibiting authentic federated properties such as skewed label distributions, imbalanced client data volumes, and domain heterogeneity across clients. These characteristics reflect practical challenges in federated RS and support evaluation of FL methods at scale. Based on FedRS, we implement 10 baseline FL algorithms and evaluation metrics to construct the comprehensive FedRS-Bench. The experimental results demonstrate that FL can consistently improve model performance over training on isolated data silos, while revealing performance trade-offs of different methods under varying client heterogeneity and availability conditions. We hope FedRS-Bench will accelerate research on large-scale, realistic FL in RS by providing a standardized, rich testbed and facilitating fair comparisons across future works. The source codes and dataset are available at this https URL. 

**Abstract (ZH)**: 面向现实需求的联邦遥感数据集及基准 FedRS-Bench 

---
# Reciprocity as the Foundational Substrate of Society: How Reciprocal Dynamics Scale into Social Systems 

**Title (ZH)**: reciprocity 作为社会的基础substrate：递归动态如何扩展到社会系统 

**Authors**: Egil Diau  

**Link**: [PDF](https://arxiv.org/pdf/2505.08319)  

**Abstract**: A major bottleneck in multi-agent AI is the lack of simulateable models for the bottom-up emergence of social structure under realistic behavioral constraints. Similarly, many foundational theories in economics and sociology including the concepts of "institutions" and "norms" tend to describe social structures post hoc, often relying on implicit assumptions of shared culture, morality, or symbolic agreement. These concepts are often treated as primitives rather than reconstructed from agent-level behavior, leaving both their origins and operational definitions under-specified. To address this, we propose a three-stage bottom-up framework: Reciprocal Dynamics, capturing individual-level reciprocal exchanges; Norm Stabilization, the consolidation of shared expectations; and Institutional Construction, the externalization of stable patterns into scalable structures. By grounding social emergence in agent-level reciprocity, our framework enables the systematic exploration of how moral, cultural, and institutional structures emerge from cognitively minimal interactions. 

**Abstract (ZH)**: 多智能体AI领域的一个主要瓶颈是对应自底向上的社会结构在现实行为约束下缺乏可模拟的模型。同样，经济学和社会学中的许多基础理论，包括“制度”和“规范”的概念，倾向于事后描述社会结构，常常依赖于共享文化、道德或符号协议的隐含假设。这些概念往往被视为基本构建块，而不是从个体层面的行为中重建，因此它们的起源和操作定义经常是不明确的。为了解决这一问题，我们提出一个三阶段自底向上的框架：互惠动力学，捕捉个体层面的互惠交换；规范稳定化，巩固共享预期；制度构建，将稳定的模式外化为可扩展的结构。通过将社会涌现基于个体层面的互惠性，我们的框架使得系统地探索道德、文化和制度结构如何从认知最小的交互中涌现成为可能。 

---
# A Practical Introduction to Deep Reinforcement Learning 

**Title (ZH)**: 实用深度强化学习简介 

**Authors**: Yinghan Sun, Hongxi Wang, Hua Chen, Wei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08295)  

**Abstract**: Deep reinforcement learning (DRL) has emerged as a powerful framework for solving sequential decision-making problems, achieving remarkable success in a wide range of applications, including game AI, autonomous driving, biomedicine, and large language models. However, the diversity of algorithms and the complexity of theoretical foundations often pose significant challenges for beginners seeking to enter the field. This tutorial aims to provide a concise, intuitive, and practical introduction to DRL, with a particular focus on the Proximal Policy Optimization (PPO) algorithm, which is one of the most widely used and effective DRL methods. To facilitate learning, we organize all algorithms under the Generalized Policy Iteration (GPI) framework, offering readers a unified and systematic perspective. Instead of lengthy theoretical proofs, we emphasize intuitive explanations, illustrative examples, and practical engineering techniques. This work serves as an efficient and accessible guide, helping readers rapidly progress from basic concepts to the implementation of advanced DRL algorithms. 

**Abstract (ZH)**: 深度强化学习（DRL）已成为解决序列决策问题的一种强大框架，已在游戏AI、自主驾驶、生物医学和大规模语言模型等多种应用中取得了显著成功。然而，算法的多样性以及理论基础的复杂性常常为初学者进入该领域带来显著挑战。本教程旨在提供一个精炼、直观且实用的DRL入门介绍，特别强调广泛使用和有效的Proximal Policy Optimization（PPO）算法。为了便于学习，我们将所有算法组织在通用策略迭代（GPI）框架下，为读者提供统一和系统的视角。我们强调直观解释、示例和实用工程技巧，而非冗长的理论证明。本工作作为一项高效和易于访问的指南，帮助读者快速从基本概念过渡到高级DRL算法的实现。 

---
# M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis 

**Title (ZH)**: M3G：面向音频驱动全身人体运动合成的多粒度手势生成器 

**Authors**: Zhizhuo Yin, Yuk Hang Tsui, Pan Hui  

**Link**: [PDF](https://arxiv.org/pdf/2505.08293)  

**Abstract**: Generating full-body human gestures encompassing face, body, hands, and global movements from audio is a valuable yet challenging task in virtual avatar creation. Previous systems focused on tokenizing the human gestures framewisely and predicting the tokens of each frame from the input audio. However, one observation is that the number of frames required for a complete expressive human gesture, defined as granularity, varies among different human gesture patterns. Existing systems fail to model these gesture patterns due to the fixed granularity of their gesture tokens. To solve this problem, we propose a novel framework named Multi-Granular Gesture Generator (M3G) for audio-driven holistic gesture generation. In M3G, we propose a novel Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct motion sequences from different temporal granularities. Subsequently, we proposed a multi-granular token predictor that extracts multi-granular information from audio and predicts the corresponding motion tokens. Then M3G reconstructs the human gestures from the predicted tokens using the MGVQ-VAE. Both objective and subjective experiments demonstrate that our proposed M3G framework outperforms the state-of-the-art methods in terms of generating natural and expressive full-body human gestures. 

**Abstract (ZH)**: 基于音频生成涵盖面部、身体、双手和全局运动的全身人类手势：一个多粒度手势生成框架 

---
# Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction 

**Title (ZH)**: Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction 

**Authors**: Yanbin Wei, Xuehao Wang, Zhan Zhuang, Yang Chen, Shuhao Chen, Yulong Zhang, Yu Zhang, James Kwok  

**Link**: [PDF](https://arxiv.org/pdf/2505.08266)  

**Abstract**: Message-passing graph neural networks (MPNNs) and structural features (SFs) are cornerstones for the link prediction task. However, as a common and intuitive mode of understanding, the potential of visual perception has been overlooked in the MPNN community. For the first time, we equip MPNNs with vision structural awareness by proposing an effective framework called Graph Vision Network (GVN), along with a more efficient variant (E-GVN). Extensive empirical results demonstrate that with the proposed frameworks, GVN consistently benefits from the vision enhancement across seven link prediction datasets, including challenging large-scale graphs. Such improvements are compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new SOTA results, thereby underscoring a promising novel direction for link prediction. 

**Abstract (ZH)**: 消息传递图神经网络和结构特征是链接预测任务的基石。然而，尽管视觉感知是一种常见且直观的方式，但在消息传递图神经网络社区中其潜力被忽视了。首次提出了一种有效框架Graph Vision Network (GVN)及其更高效的变体E-GVN，将视觉结构意识融入消息传递图神经网络中。广泛的经验结果表明，在七个链接预测数据集中，GVN一致地得益于视觉增强，包括具有挑战性的大规模图。这些改进与现有的最先进方法兼容，GVNs达到了新的最先进结果，从而强调了一个有希望的新方向用于链接预测。 

---
# LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification 

**Title (ZH)**: 基于因果机制识别视角的LLM增强剂对于GNN的研究 

**Authors**: Hang Gao, Wenxuan Huang, Fengge Wu, Junsuo Zhao, Changwen Zheng, Huaping Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.08265)  

**Abstract**: The use of large language models (LLMs) as feature enhancers to optimize node representations, which are then used as inputs for graph neural networks (GNNs), has shown significant potential in graph representation learning. However, the fundamental properties of this approach remain underexplored. To address this issue, we propose conducting a more in-depth analysis of this issue based on the interchange intervention method. First, we construct a synthetic graph dataset with controllable causal relationships, enabling precise manipulation of semantic relationships and causal modeling to provide data for analysis. Using this dataset, we conduct interchange interventions to examine the deeper properties of LLM enhancers and GNNs, uncovering their underlying logic and internal mechanisms. Building on the analytical results, we design a plug-and-play optimization module to improve the information transfer between LLM enhancers and GNNs. Experiments across multiple datasets and models validate the proposed module. 

**Abstract (ZH)**: 使用大规模语言模型（LLMs）作为特征增强器以优化节点表示，然后将其作为图神经网络（GNNs）的输入，在图表示学习中展现出显著的潜力。然而，这种方法的基本特性仍然未被充分探索。为了解决这一问题，我们提出基于互换干预方法进行更深入的分析。首先，我们构建了一个可控因果关系的合成图数据集，以便精确操纵语义关系和因果建模，为分析提供数据。使用该数据集，我们进行互换干预以探讨LLM增强器和GNNs的深层次特性，揭示其内在逻辑和内部机制。基于分析结果，我们设计了一个即插即用的优化模块来改善LLM增强器与GNNs之间的信息传递。在多个数据集和模型上的实验验证了所提模块的有效性。 

---
# Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning 

**Title (ZH)**: 自动驾驶场景下的自适应课程学习：迈向稳健高效的强化学习 

**Authors**: Ahmed Abouelazm, Tim Weinstein, Tim Joseph, Philip Schörner, J. Marius Zöllner  

**Link**: [PDF](https://arxiv.org/pdf/2505.08264)  

**Abstract**: This paper addresses the challenges of training end-to-end autonomous driving agents using Reinforcement Learning (RL). RL agents are typically trained in a fixed set of scenarios and nominal behavior of surrounding road users in simulations, limiting their generalization and real-life deployment. While domain randomization offers a potential solution by randomly sampling driving scenarios, it frequently results in inefficient training and sub-optimal policies due to the high variance among training scenarios. To address these limitations, we propose an automatic curriculum learning framework that dynamically generates driving scenarios with adaptive complexity based on the agent's evolving capabilities. Unlike manually designed curricula that introduce expert bias and lack scalability, our framework incorporates a ``teacher'' that automatically generates and mutates driving scenarios based on their learning potential -- an agent-centric metric derived from the agent's current policy -- eliminating the need for expert design. The framework enhances training efficiency by excluding scenarios the agent has mastered or finds too challenging. We evaluate our framework in a reinforcement learning setting where the agent learns a driving policy from camera images. Comparative results against baseline methods, including fixed scenario training and domain randomization, demonstrate that our approach leads to enhanced generalization, achieving higher success rates: +9\% in low traffic density, +21\% in high traffic density, and faster convergence with fewer training steps. Our findings highlight the potential of ACL in improving the robustness and efficiency of RL-based autonomous driving agents. 

**Abstract (ZH)**: 基于自动课程学习的端到端自主驾驶代理强化学习训练方法 

---
# Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration 

**Title (ZH)**: 增强基于缓存的生成（CAG）算法通过自适应上下文压缩实现可扩展的知识集成 

**Authors**: Rishabh Agrawal, Himanshu Kumar  

**Link**: [PDF](https://arxiv.org/pdf/2505.08261)  

**Abstract**: The rapid progress in large language models (LLMs) has paved the way for novel approaches in knowledge-intensive tasks. Among these, Cache-Augmented Generation (CAG) has emerged as a promising alternative to Retrieval-Augmented Generation (RAG). CAG minimizes retrieval latency and simplifies system design by preloading knowledge into the model's context. However, challenges persist in scaling CAG to accommodate large and dynamic knowledge bases effectively. This paper introduces Adaptive Contextual Compression (ACC), an innovative technique designed to dynamically compress and manage context inputs, enabling efficient utilization of the extended memory capabilities of modern LLMs. To further address the limitations of standalone CAG, we propose a Hybrid CAG-RAG Framework, which integrates selective retrieval to augment preloaded contexts in scenarios requiring additional information. Comprehensive evaluations on diverse datasets highlight the proposed methods' ability to enhance scalability, optimize efficiency, and improve multi-hop reasoning performance, offering practical solutions for real-world knowledge integration challenges. 

**Abstract (ZH)**: 大规模语言模型的迅速进展为知识密集型任务开辟了新途径。在此过程中，缓存增强生成（CAG）作为一种替代检索增强生成（RAG）的有前途的方法脱颖而出。CAG通过preload知识到模型的上下文中来最小化检索延迟并简化系统设计。然而，有效地扩大CAG以适应大型和动态的知识库仍面临挑战。本文介绍了自适应上下文压缩（ACC），一种创新技术，旨在动态压缩和管理上下文输入，充分利用现代大规模语言模型的扩展内存能力。为进一步解决独立CAG的局限性，我们提出了一种混合CAG-RAG框架，该框架在需要额外信息的场景中结合了选择性检索来增强预加载的上下文。对多种数据集的综合评估表明，所提出的方法能够增强可扩展性、优化效率并改进多跳推理性能，为实际知识集成挑战提供可行解决方案。 

---
# Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement 

**Title (ZH)**: 大型语言模型心理测量学：评价、验证与增强的系统综述 

**Authors**: Haoran Ye, Jing Jin, Yuhang Xie, Xin Zhang, Guojie Song  

**Link**: [PDF](https://arxiv.org/pdf/2505.08245)  

**Abstract**: The rapid advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. It presents novel challenges, such as measuring human-like psychological constructs, navigating beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with Psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This survey introduces and synthesizes an emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. We systematically explore the role of Psychometrics in shaping benchmarking principles, broadening evaluation scopes, refining methodologies, validating results, and advancing LLM capabilities. This paper integrates diverse perspectives to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, we aim to provide actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at this https URL. 

**Abstract (ZH)**: 快速发展的大规模语言模型（LLMs）已超越了传统评估方法。这提出了新的挑战，如测量类似人类的心理构念、超越静态和任务特定的基准以及建立以人类为中心的评估。这些挑战与心理测量学相关，心理测量学是量化人类心理无形方面（如个性、价值观和智力）的科学。本文综述并综合了新兴的跨学科领域——LLM心理测量学，该领域利用心理测量工具、理论和原则来评估、理解和提升大规模语言模型。我们系统地探讨了心理测量学在塑造基准原则、扩展评估范围、完善方法、验证结果以及推进大规模语言模型能力方面的角色。本文集纳了多学科视角，为各学科研究人员提供了一个结构化的框架，以更全面地理解这一新兴领域。最终，我们旨在提供实用见解，以开发与人类水平AI对齐的评估范式，并推动有利于社会的人本AI系统的进步。LLM心理测量资源的精选库可在以下链接获取：这个https URL。 

---
# Removing Watermarks with Partial Regeneration using Semantic Information 

**Title (ZH)**: 使用语义信息进行部分再生去除水印 

**Authors**: Krti Tallam, John Kevin Cava, Caleb Geniesse, N. Benjamin Erichson, Michael W. Mahoney  

**Link**: [PDF](https://arxiv.org/pdf/2505.08234)  

**Abstract**: As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged as a primary line of defense for copyright and provenance. The newest watermarking schemes embed semantic signals - content-aware patterns that are designed to survive common image manipulations - yet their true robustness against adaptive adversaries remains under-explored. We expose a previously unreported vulnerability and introduce SemanticRegen, a three-stage, label-free attack that erases state-of-the-art semantic and invisible watermarks while leaving an image's apparent meaning intact. Our pipeline (i) uses a vision-language model to obtain fine-grained captions, (ii) extracts foreground masks with zero-shot segmentation, and (iii) inpaints only the background via an LLM-guided diffusion model, thereby preserving salient objects and style cues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing, StegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat the semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy below 0.75 for the remaining schemes, all while maintaining high perceptual quality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM) to quantify fidelity within foreground regions, showing that our attack achieves up to 12 percent higher mSSIM than prior diffusion-based attackers. These results highlight an urgent gap between current watermark defenses and the capabilities of adaptive, semantics-aware adversaries, underscoring the need for watermarking algorithms that are resilient to content-preserving regenerative attacks. 

**Abstract (ZH)**: 随着AI生成图像的普及，隐式水印已成为版权和溯源的主要防线。最新的水印方案嵌入了语义信号——适应常见图像处理设计的内容感知模式，但它们对抗适应性对手的真实鲁棒性仍被低估。我们揭示了一种未被报道的脆弱性，并引入了SemanticRegen，这是一种三阶段、无标签攻击，能够在不损害图像显见含义的前提下消除最先进的语义和隐式水印。我们的管道包括：(i) 使用视觉-语言模型获取精细的描述，(ii) 使用零样本分割提取前景掩码，(iii) 通过受LLM指导的扩散模型仅修复背景，从而保留了显著对象和风格线索。在四个水印系统——TreeRing、StegaStamp、StableSig和DWT/DCT——上评估了1,000个提示后，SemanticRegen是唯一一种能击败语义TreeRing水印(p = 0.10 > 0.05)的方法，并且能将其余方案的比特准确率降低至0.75以下，同时保持高感知质量（屏蔽SSIM = 0.94 ± 0.01）。我们还引入了屏蔽SSIM (mSSIM) 以量化前景区域内的保真度，结果显示我们的攻击实现了比先前基于扩散的攻击高12个百分点的mSSIM。这些结果突显了当前水印防御与适应性强、具备语义意识的对手能力之间的重要差距，强调了需要具备抵御内容保持再生攻击的水印算法的需求。 

---
# Object detection in adverse weather conditions for autonomous vehicles using Instruct Pix2Pix 

**Title (ZH)**: 在恶劣天气条件下使用Instruct Pix2Pix进行自主车辆目标检测 

**Authors**: Unai Gurbindo, Axel Brando, Jaume Abella, Caroline König  

**Link**: [PDF](https://arxiv.org/pdf/2505.08228)  

**Abstract**: Enhancing the robustness of object detection systems under adverse weather conditions is crucial for the advancement of autonomous driving technology. This study presents a novel approach leveraging the diffusion model Instruct Pix2Pix to develop prompting methodologies that generate realistic datasets with weather-based augmentations aiming to mitigate the impact of adverse weather on the perception capabilities of state-of-the-art object detection models, including Faster R-CNN and YOLOv10. Experiments were conducted in two environments, in the CARLA simulator where an initial evaluation of the proposed data augmentation was provided, and then on the real-world image data sets BDD100K and ACDC demonstrating the effectiveness of the approach in real environments.
The key contributions of this work are twofold: (1) identifying and quantifying the performance gap in object detection models under challenging weather conditions, and (2) demonstrating how tailored data augmentation strategies can significantly enhance the robustness of these models. This research establishes a solid foundation for improving the reliability of perception systems in demanding environmental scenarios, and provides a pathway for future advancements in autonomous driving. 

**Abstract (ZH)**: 增强对象检测系统在不良天气条件下的稳健性对于自主驾驶技术的发展至关重要。本研究提出了一种新颖的方法，利用扩散模型Instruct Pix2Pix开发生成具有基于天气增强的现实数据集的提示方法，旨在减少不良天气对最先进的对象检测模型（包括Faster R-CNN和YOLOv10）感知能力的负面影响。实验在CARLA模拟器和真实世界数据集BDD100K及ACDC中进行，展示了该方法在实际环境中的有效性。本工作的主要贡献有两点：（1）识别并量化对象检测模型在恶劣天气条件下的性能差距，（2）演示定制的数据增强策略如何显著提高这些模型的稳健性。这项研究为在严峻环境场景中提高感知系统可靠性奠定了坚实基础，并为自主驾驶技术的未来发展提供了途径。 

---
# Reinforcement Learning-based Fault-Tolerant Control for Quadrotor with Online Transformer Adaptation 

**Title (ZH)**: 基于强化学习的四旋翼无人机在线变压器自适应容错控制 

**Authors**: Dohyun Kim, Jayden Dongwoo Lee, Hyochoong Bang, Jungho Bae  

**Link**: [PDF](https://arxiv.org/pdf/2505.08223)  

**Abstract**: Multirotors play a significant role in diverse field robotics applications but remain highly susceptible to actuator failures, leading to rapid instability and compromised mission reliability. While various fault-tolerant control (FTC) strategies using reinforcement learning (RL) have been widely explored, most previous approaches require prior knowledge of the multirotor model or struggle to adapt to new configurations. To address these limitations, we propose a novel hybrid RL-based FTC framework integrated with a transformer-based online adaptation module. Our framework leverages a transformer architecture to infer latent representations in real time, enabling adaptation to previously unseen system models without retraining. We evaluate our method in a PyBullet simulation under loss-of-effectiveness actuator faults, achieving a 95% success rate and a positional root mean square error (RMSE) of 0.129 m, outperforming existing adaptation methods with 86% success and an RMSE of 0.153 m. Further evaluations on quadrotors with varying configurations confirm the robustness of our framework across untrained dynamics. These results demonstrate the potential of our framework to enhance the adaptability and reliability of multirotors, enabling efficient fault management in dynamic and uncertain environments. Website is available at this http URL 

**Abstract (ZH)**: 多旋翼飞行器在多样化领域的机器人应用中扮演着重要角色，但仍高度易受执行器故障的影响，导致快速失稳和任务可靠性的下降。尽管已有利用强化学习（RL）的容错控制（FTC）策略得到了广泛探索，但大多数前期方法需要多旋翼飞行器模型的先验知识，或者难以适应新的布局配置。为克服这些限制，我们提出了一种结合基于变换器的在线自适应模块的新型混合RL基于FTC框架。该框架利用变换器架构在实时推断潜在表示，从而能够在无需重新训练的情况下适应未见过的系统模型。我们在PyBullet模拟环境中评估了该方法，在性能降低的执行器故障条件下实现了95%的成功率和位置均方根误差（RMSE）为0.129 m，优于现有自适应方法的86%成功率和0.153 m的RMSE。进一步在不同配置的四旋翼飞行器上的评估证实了该框架在未训练动态条件下的鲁棒性。这些结果展示了该框架增强多旋翼飞行器的适应性和可靠性的潜力，使其能够在动态和不确定环境中高效管理故障。Website is available at this http URL 

---
# Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles 

**Title (ZH)**: 基于自主车辆的水下声学跟踪中多智能体强化学习的扩展研究 

**Authors**: Matteo Gallici, Ivan Masmitja, Mario Martín  

**Link**: [PDF](https://arxiv.org/pdf/2505.08222)  

**Abstract**: Autonomous vehicles (AV) offer a cost-effective solution for scientific missions such as underwater tracking. Recently, reinforcement learning (RL) has emerged as a powerful method for controlling AVs in complex marine environments. However, scaling these techniques to a fleet--essential for multi-target tracking or targets with rapid, unpredictable motion--presents significant computational challenges. Multi-Agent Reinforcement Learning (MARL) is notoriously sample-inefficient, and while high-fidelity simulators like Gazebo's LRAUV provide 100x faster-than-real-time single-robot simulations, they offer no significant speedup for multi-vehicle scenarios, making MARL training impractical. To address these limitations, we propose an iterative distillation method that transfers high-fidelity simulations into a simplified, GPU-accelerated environment while preserving high-level dynamics. This approach achieves up to a 30,000x speedup over Gazebo through parallelization, enabling efficient training via end-to-end GPU acceleration. Additionally, we introduce a novel Transformer-based architecture (TransfMAPPO) that learns multi-agent policies invariant to the number of agents and targets, significantly improving sample efficiency. Following large-scale curriculum learning conducted entirely on GPU, we perform extensive evaluations in Gazebo, demonstrating that our method maintains tracking errors below 5 meters over extended durations, even in the presence of multiple fast-moving targets. This work bridges the gap between large-scale MARL training and high-fidelity deployment, providing a scalable framework for autonomous fleet control in real-world sea missions. 

**Abstract (ZH)**: 自主水下车辆（AV）为水下追踪等科学任务提供了经济有效的解决方案。近年来，强化学习（RL）已成为控制复杂海洋环境中的AV的强大方法。然而，将这些技术扩展到车队——这对于多目标追踪或具有快速、不可预测运动的目标至关重要——带来了显著的计算挑战。多智能体强化学习（MARL）以其样本效益差著称，尽管像Gazebo的LRAUV这样的高保真模拟器可以在单机器人模拟中实现100倍于实时的速度，但在多车辆场景中却未能提供显著的速度提升，使得MARL的训练变得不切实际。为解决这些限制，我们提出了一种迭代蒸馏方法，将高保真模拟转移至简化且GPU加速的环境，同时保持高层动力学。该方法通过并行化实现了高达30,000倍的速度提升，为端到端的GPU加速训练提供了高效途径。此外，我们引入了一种基于Transformer的新型架构（TransfMAPPO），该架构可以学习不受智能体和目标数量影响的多智能体策略，显著提高样本效率。我们通过完全在GPU上进行大规模递增学习后，在Gazebo中进行了广泛评估，结果显示，即使在存在多个快速移动目标的情况下，我们的方法也能在整个时间段内将跟踪误差保持在5米以下。这项工作填补了大规模MARL训练与高保真部署之间的差距，为实现实用海中自主舰队控制提供了可扩展的框架。 

---
# A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs 

**Title (ZH)**: 预测头部和提问头部：预训练不确定性量化头部在检测LLM输出幻觉中的应用 

**Authors**: Artem Shelmanov, Ekaterina Fadeeva, Akim Tsvigun, Ivan Tsvigun, Zhuohan Xie, Igor Kiselev, Nico Daheim, Caiqi Zhang, Artem Vazhentsev, Mrinmaya Sachan, Preslav Nakov, Timothy Baldwin  

**Link**: [PDF](https://arxiv.org/pdf/2505.08200)  

**Abstract**: Large Language Models (LLMs) have the tendency to hallucinate, i.e., to sporadically generate false or fabricated information. This presents a major challenge, as hallucinations often appear highly convincing and users generally lack the tools to detect them. Uncertainty quantification (UQ) provides a framework for assessing the reliability of model outputs, aiding in the identification of potential hallucinations. In this work, we introduce pre-trained UQ heads: supervised auxiliary modules for LLMs that substantially enhance their ability to capture uncertainty compared to unsupervised UQ methods. Their strong performance stems from the powerful Transformer architecture in their design and informative features derived from LLM attention maps. Experimental evaluation shows that these heads are highly robust and achieve state-of-the-art performance in claim-level hallucination detection across both in-domain and out-of-domain prompts. Moreover, these modules demonstrate strong generalization to languages they were not explicitly trained on. We pre-train a collection of UQ heads for popular LLM series, including Mistral, Llama, and Gemma 2. We publicly release both the code and the pre-trained heads. 

**Abstract (ZH)**: 大型语言模型（LLMs）有产生幻觉的倾向，即偶尔生成虚假或杜撰的信息。这带来了主要挑战，因为幻觉通常显得极具说服力，而用户通常缺乏检测它们的工具。不确定性量化（UQ）提供了一种评估模型输出可靠性的框架，有助于识别潜在的幻觉。在这项工作中，我们引入了预训练的UQ头部：监督辅助模块，它们显著增强了LLMs捕获不确定性的能力，超过了无监督UQ方法。这些模块的强大性能源自其设计中的强大变换器架构以及从LLM注意力图中提取的信息性特征。实验评估显示，这些头部在领域内和领域外提示下的断言级别幻觉检测中表现出高度的稳健性和最先进的性能。此外，这些模块还展示了对它们未明确训练的语言的强泛化能力。我们为流行的LLM系列（包括Mistral、Llama和Gemma 2）预训练了UQ头部，并公开发布了代码和预训练的头部。 

---
# Aitomia: Your Intelligent Assistant for AI-Driven Atomistic and Quantum Chemical Simulations 

**Title (ZH)**: Aitomia：您的智能助手，驱动原子级和量子化学模拟 

**Authors**: Jinming Hu, Hassan Nawaz, Yuting Rui, Lijie Chi, Arif Ullah, Pavlo O. Dral  

**Link**: [PDF](https://arxiv.org/pdf/2505.08195)  

**Abstract**: We have developed Aitomia - a platform powered by AI to assist in performing AI-driven atomistic and quantum chemical (QC) simulations. This intelligent assistant platform is equipped with chatbots and AI agents to help experts and guide non-experts in setting up and running the atomistic simulations, monitoring their computation status, analyzing the simulation results, and summarizing them for the user in text and graphical forms. We achieve these goals by exploiting fine-tuned open-source large language models (LLMs), rule-based agents, and a retrieval-augmented generation (RAG) system. Aitomia leverages the versatility of our MLatom ecosystem for AI-enhanced computational chemistry. This intelligent assistant is going to be integrated into the Aitomistic Hub and XACS online computing services, with some functionality already publicly available as described at this http URL. Aitomia is expected to lower the barrier to performing atomistic simulations, accelerating research and development in the relevant fields. 

**Abstract (ZH)**: 我们开发了Aitomia——一个由AI驱动的平台，辅助进行原子级别和量子化学模拟。该智能助手平台配备了聊天机器人和AI代理，帮助专家并引导非专家设置和运行原子级别模拟，监控计算状态，分析模拟结果，并以文本和图形形式总结结果。我们通过利用微调的开源大型语言模型、基于规则的代理和检索增强生成（RAG）系统来实现这些目标。Aitomia利用了我们MLatom生态系统的多功能性，以增强计算化学。该智能助手即将集成到Aitomistic Hub和XACS在线计算服务中，部分功能已公开可用，详情请参见此网址。Aitomia有望降低进行原子级别模拟的门槛，加速相关领域的研究与开发。 

---
# DSADF: Thinking Fast and Slow for Decision Making 

**Title (ZH)**: DSADF：快速与缓慢的决策思考 

**Authors**: Alex Zhihao Dou, Dongfei Cui, Jun Yan, Weida Wang, Benteng Chen, Haoming Wang, Zeke Xie, Shufei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08189)  

**Abstract**: Although Reinforcement Learning (RL) agents are effective in well-defined environments, they often struggle to generalize their learned policies to dynamic settings due to their reliance on trial-and-error interactions. Recent work has explored applying Large Language Models (LLMs) or Vision Language Models (VLMs) to boost the generalization of RL agents through policy optimization guidance or prior knowledge. However, these approaches often lack seamless coordination between the RL agent and the foundation model, leading to unreasonable decision-making in unfamiliar environments and efficiency bottlenecks. Making full use of the inferential capabilities of foundation models and the rapid response capabilities of RL agents and enhancing the interaction between the two to form a dual system is still a lingering scientific question. To address this problem, we draw inspiration from Kahneman's theory of fast thinking (System 1) and slow thinking (System 2), demonstrating that balancing intuition and deep reasoning can achieve nimble decision-making in a complex world. In this study, we propose a Dual-System Adaptive Decision Framework (DSADF), integrating two complementary modules: System 1, comprising an RL agent and a memory space for fast and intuitive decision making, and System 2, driven by a VLM for deep and analytical reasoning. DSADF facilitates efficient and adaptive decision-making by combining the strengths of both systems. The empirical study in the video game environment: Crafter and Housekeep demonstrates the effectiveness of our proposed method, showing significant improvements in decision abilities for both unseen and known tasks. 

**Abstract (ZH)**: 基于双系统适应性决策框架的强化学习代理决策机制研究 

---
# Feasibility-Aware Pessimistic Estimation: Toward Long-Horizon Safety in Offline RL 

**Title (ZH)**: 可行性意识的悲观估计：迈向离线RL中的长时域安全 

**Authors**: Zhikun Tao, Gang Xiong, He Fang, Zhen Shen, Yunjun Han, Qing-Shan Jia  

**Link**: [PDF](https://arxiv.org/pdf/2505.08179)  

**Abstract**: Offline safe reinforcement learning(OSRL) derives constraint-satisfying policies from pre-collected datasets, offers a promising avenue for deploying RL in safety-critical real-world domains such as robotics. However, the majority of existing approaches emphasize only short-term safety, neglecting long-horizon considerations. Consequently, they may violate safety constraints and fail to ensure sustained protection during online deployment. Moreover, the learned policies often struggle to handle states and actions that are not present or out-of-distribution(OOD) from the offline dataset, and exhibit limited sample efficiency. To address these challenges, we propose a novel framework Feasibility-Aware offline Safe Reinforcement Learning with CVAE-based Pessimism (FASP). First, we employ Hamilton-Jacobi (H-J) reachability analysis to generate reliable safety labels, which serve as supervisory signals for training both a conditional variational autoencoder (CVAE) and a safety classifier. This approach not only ensures high sampling efficiency but also provides rigorous long-horizon safety guarantees. Furthermore, we utilize pessimistic estimation methods to estimate the Q-value of reward and cost, which mitigates the extrapolation errors induces by OOD actions, and penalize unsafe actions to enabled the agent to proactively avoid high-risk behaviors. Moreover, we theoretically prove the validity of this pessimistic estimation. Extensive experiments on DSRL benchmarks demonstrate that FASP algorithm achieves competitive performance across multiple experimental tasks, particularly outperforming state-of-the-art algorithms in terms of safety. 

**Abstract (ZH)**: 基于CVAE悲观估计的可行性 Awareness Offline 安全强化学习（FASP） 

---
# Fast Text-to-Audio Generation with Adversarial Post-Training 

**Title (ZH)**: Fast文本到语音生成的对抗性后训练方法 

**Authors**: Zachary Novack, Zach Evans, Zack Zukowski, Josiah Taylor, CJ Carr, Julian Parker, Adnan Al-Sinan, Gian Marco Iodice, Julian McAuley, Taylor Berg-Kirkpatrick, Jordi Pons  

**Link**: [PDF](https://arxiv.org/pdf/2505.08175)  

**Abstract**: Text-to-audio systems, while increasingly performant, are slow at inference time, thus making their latency unpractical for many creative applications. We present Adversarial Relativistic-Contrastive (ARC) post-training, the first adversarial acceleration algorithm for diffusion/flow models not based on distillation. While past adversarial post-training methods have struggled to compare against their expensive distillation counterparts, ARC post-training is a simple procedure that (1) extends a recent relativistic adversarial formulation to diffusion/flow post-training and (2) combines it with a novel contrastive discriminator objective to encourage better prompt adherence. We pair ARC post-training with a number optimizations to Stable Audio Open and build a model capable of generating $\approx$12s of 44.1kHz stereo audio in $\approx$75ms on an H100, and $\approx$7s on a mobile edge-device, the fastest text-to-audio model to our knowledge. 

**Abstract (ZH)**: 基于对抗相对对比的后训练加速算法： diffusion/flow模型的首个不基于蒸馏的加速方法 

---
# Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph 

**Title (ZH)**: 利用文本语义进行基于文本的图中节点少量和零样本分类 

**Authors**: Yuxiang Wang, Xiao Yan, Shiyu Jin, Quanqing Xu, Chuang Hu, Yuanyuan Zhu, Bo Du, Jia Wu, Jiawei Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08168)  

**Abstract**: Text-attributed graph (TAG) provides a text description for each graph node, and few- and zero-shot node classification on TAGs have many applications in fields such as academia and social networks. Existing work utilizes various graph-based augmentation techniques to train the node and text embeddings, while text-based augmentations are largely unexplored. In this paper, we propose Text Semantics Augmentation (TSA) to improve accuracy by introducing more text semantic supervision signals. Specifically, we design two augmentation techniques, i.e., positive semantics matching and negative semantics contrast, to provide more reference texts for each graph node or text description. Positive semantic matching retrieves texts with similar embeddings to match with a graph node. Negative semantic contrast adds a negative prompt to construct a text description with the opposite semantics, which is contrasted with the original node and text. We evaluate TSA on 5 datasets and compare with 13 state-of-the-art baselines. The results show that TSA consistently outperforms all baselines, and its accuracy improvements over the best-performing baseline are usually over 5%. 

**Abstract (ZH)**: 基于文本的图（Text-attributed Graph, TAG）提供了每个图节点的文字描述，且基于少量或零样本的TAG节点分类在学术界和社交网络等领域有广泛应用。现有的工作利用了各种图结构增强技术训练节点和文本嵌入，而基于文本的增强技术尚未得到充分利用。本文提出文本语义增强（TSA）以通过引入更多的文本语义监督信号来提高准确性。具体地，我们设计了正语义匹配和负语义对比两种增强技术，为每个图节点或文本描述提供更多参考文本。正语义匹配检索具有相似嵌入的文本以匹配图节点。负语义对比增加一个负向提示来构造语义相反的文本描述，该描述与原始节点和文本进行对比。我们在5个数据集上评估了TSA，并与13个state-of-the-artbaseline进行了比较。结果显示，TSA一直优于所有baseline，并且其相对于表现最好的baseline的准确性提升通常超过5%。 

---
# Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage 

**Title (ZH)**: 融合双向思维链和奖励机制的方法：提升大型语言模型对中国非物质文化遗产问答能力 

**Authors**: Ruilin Liu, Zhixiao Zhao, Jieqiong Li, Chang Liu, Dongbo Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08167)  

**Abstract**: The rapid development of large language models (LLMs) has provided significant support and opportunities for the advancement of domain-specific LLMs. However, fine-tuning these large models using Intangible Cultural Heritage (ICH) data inevitably faces challenges such as bias, incorrect knowledge inheritance, and catastrophic forgetting. To address these issues, we propose a novel training method that integrates a bidirectional chains of thought and a reward mechanism. This method is built upon ICH-Qwen, a large language model specifically designed for the field of intangible cultural heritage. The proposed method enables the model to not only perform forward reasoning but also enhances the accuracy of the generated answers by utilizing reverse questioning and reverse reasoning to activate the model's latent knowledge. Additionally, a reward mechanism is introduced during training to optimize the decision-making process. This mechanism improves the quality of the model's outputs through structural and content evaluations with different weighting schemes. We conduct comparative experiments on ICH-Qwen, with results demonstrating that our method outperforms 0-shot, step-by-step reasoning, knowledge distillation, and question augmentation methods in terms of accuracy, Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the paper highlights the effectiveness of combining the bidirectional chains of thought and reward mechanism through ablation experiments. In addition, a series of generalizability experiments are conducted, with results showing that the proposed method yields improvements on various domain-specific datasets and advanced models in areas such as Finance, Wikidata, and StrategyQA. This demonstrates that the method is adaptable to multiple domains and provides a valuable approach for model training in future applications across diverse fields. 

**Abstract (ZH)**: 大型语言模型的快速发展为领域特定语言模型的进步提供了重要支持和机遇。然而，使用非物质文化遗产（ICH）数据对这些大型模型进行微调不可避免地会面临偏差、错误知识传承和灾难性遗忘等挑战。为了解决这些问题，我们提出了一种结合双向推理链和奖励机制的新型训练方法。该方法基于专门为非物质文化遗产领域设计的大规模语言模型ICH-Qwen。所提出的方法不仅使模型能够进行前向推理，还通过利用逆向提问和逆向推理激活模型的潜在知识，提升了生成答案的准确性。此外，训练过程中引入了一个奖励机制，通过不同的权重方案进行结构和内容评估，优化决策过程。实验结果显示，与零样本、逐步推理、知识蒸馏和问题扩充方法相比，我们的方法在问答任务中表现出更高的准确性、Bleu-4和Rouge-L得分。论文还通过消融实验突出了双向推理链和奖励机制结合的有效性。此外，还进行了泛化实验，结果显示所提出的方法在金融、Wikidata和StrategyQA等多个领域特定数据集和高级模型上表现出了提升效果。这表明该方法适用于多个领域，并为未来跨不同领域应用的模型训练提供了有价值的途径。 

---
# Feature Fitted Online Conformal Prediction for Deep Time Series Forecasting Model 

**Title (ZH)**: 基于特征拟合的在线同变预测深时序forecasting模型 

**Authors**: Xiannan Huang, Shuhan Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2505.08158)  

**Abstract**: Time series forecasting is critical for many applications, where deep learning-based point prediction models have demonstrated strong performance. However, in practical scenarios, there is also a need to quantify predictive uncertainty through online confidence intervals. Existing confidence interval modeling approaches building upon these deep point prediction models suffer from key limitations: they either require costly retraining, fail to fully leverage the representational strengths of deep models, or lack theoretical guarantees. To address these gaps, we propose a lightweight conformal prediction method that provides valid coverage and shorter interval lengths without retraining. Our approach leverages features extracted from pre-trained point prediction models to fit a residual predictor and construct confidence intervals, further enhanced by an adaptive coverage control mechanism. Theoretically, we prove that our method achieves asymptotic coverage convergence, with error bounds dependent on the feature quality of the underlying point prediction model. Experiments on 12 datasets demonstrate that our method delivers tighter confidence intervals while maintaining desired coverage rates. Code, model and dataset in \href{this https URL}{Github} 

**Abstract (ZH)**: 基于深度学习的时间序列预测区间预测方法：一种无需重新训练的轻量级校准预测方法 

---
# Hyperbolic Contrastive Learning with Model-augmentation for Knowledge-aware Recommendation 

**Title (ZH)**: 基于模型增强的双曲对比学习在知识aware推荐中的应用 

**Authors**: Shengyin Sun, Chen Ma  

**Link**: [PDF](https://arxiv.org/pdf/2505.08157)  

**Abstract**: Benefiting from the effectiveness of graph neural networks (GNNs) and contrastive learning, GNN-based contrastive learning has become mainstream for knowledge-aware recommendation. However, most existing contrastive learning-based methods have difficulties in effectively capturing the underlying hierarchical structure within user-item bipartite graphs and knowledge graphs. Moreover, they commonly generate positive samples for contrastive learning by perturbing the graph structure, which may lead to a shift in user preference learning. To overcome these limitations, we propose hyperbolic contrastive learning with model-augmentation for knowledge-aware recommendation. To capture the intrinsic hierarchical graph structures, we first design a novel Lorentzian knowledge aggregation mechanism, which enables more effective representations of users and items. Then, we propose three model-level augmentation techniques to assist Hyperbolic contrastive learning. Different from the classical structure-level augmentation (e.g., edge dropping), the proposed model-augmentations can avoid preference shifts between the augmented positive pair. Finally, we conduct extensive experiments to demonstrate the superiority (maximum improvement of $11.03\%$) of proposed methods over existing baselines. 

**Abstract (ZH)**: 基于双曲对比学习的知识aware推荐模型增强技术 

---
# A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem 

**Title (ZH)**: 大规模实证分析自定义GPT模型在OpenAI生态系统中的脆弱性 

**Authors**: Sunday Oyinlola Ogundoyin, Muhammad Ikram, Hassan Jameel Asghar, Benjamin Zi Hao Zhao, Dali Kaafar  

**Link**: [PDF](https://arxiv.org/pdf/2505.08148)  

**Abstract**: Millions of users leverage generative pretrained transformer (GPT)-based language models developed by leading model providers for a wide range of tasks. To support enhanced user interaction and customization, many platforms-such as OpenAI-now enable developers to create and publish tailored model instances, known as custom GPTs, via dedicated repositories or application stores. These custom GPTs empower users to browse and interact with specialized applications designed to meet specific needs. However, as custom GPTs see growing adoption, concerns regarding their security vulnerabilities have intensified. Existing research on these vulnerabilities remains largely theoretical, often lacking empirical, large-scale, and statistically rigorous assessments of associated risks.
In this study, we analyze 14,904 custom GPTs to assess their susceptibility to seven exploitable threats, such as roleplay-based attacks, system prompt leakage, phishing content generation, and malicious code synthesis, across various categories and popularity tiers within the OpenAI marketplace. We introduce a multi-metric ranking system to examine the relationship between a custom GPT's popularity and its associated security risks.
Our findings reveal that over 95% of custom GPTs lack adequate security protections. The most prevalent vulnerabilities include roleplay-based vulnerabilities (96.51%), system prompt leakage (92.20%), and phishing (91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit inherent security weaknesses, which are often inherited or amplified in custom GPTs. These results highlight the urgent need for enhanced security measures and stricter content moderation to ensure the safe deployment of GPT-based applications. 

**Abstract (ZH)**: 基于GPT的自定义模型安全性分析：开放AI市场上的14,904个自定义GPT的安全威胁评估 

---
# Communication Styles and Reader Preferences of LLM and Human Experts in Explaining Health Information 

**Title (ZH)**: LLM与人类专家在解释健康信息时的沟通风格和读者偏好 

**Authors**: Jiawei Zhou, Kritika Venkatachalam, Minje Choi, Koustuv Saha, Munmun De Choudhury  

**Link**: [PDF](https://arxiv.org/pdf/2505.08143)  

**Abstract**: With the wide adoption of large language models (LLMs) in information assistance, it is essential to examine their alignment with human communication styles and values. We situate this study within the context of fact-checking health information, given the critical challenge of rectifying conceptions and building trust. Recent studies have explored the potential of LLM for health communication, but style differences between LLMs and human experts and associated reader perceptions remain under-explored. In this light, our study evaluates the communication styles of LLMs, focusing on how their explanations differ from those of humans in three core components of health communication: information, sender, and receiver. We compiled a dataset of 1498 health misinformation explanations from authoritative fact-checking organizations and generated LLM responses to inaccurate health information. Drawing from health communication theory, we evaluate communication styles across three key dimensions of information linguistic features, sender persuasive strategies, and receiver value alignments. We further assessed human perceptions through a blinded evaluation with 99 participants. Our findings reveal that LLM-generated articles showed significantly lower scores in persuasive strategies, certainty expressions, and alignment with social values and moral foundations. However, human evaluation demonstrated a strong preference for LLM content, with over 60% responses favoring LLM articles for clarity, completeness, and persuasiveness. Our results suggest that LLMs' structured approach to presenting information may be more effective at engaging readers despite scoring lower on traditional measures of quality in fact-checking and health communication. 

**Abstract (ZH)**: 大语言模型在健康信息事实核查中的沟通风格与人类交流风格和价值观的对齐研究 

---
# Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating Machine Unlearning 

**Title (ZH)**: 镜子前的我，是不是忘了这一切？一种评价机器遗忘的新框架 

**Authors**: Brennon Brimhall, Philip Mathew, Neil Fendley, Yinzhi Cao, Matthew Green  

**Link**: [PDF](https://arxiv.org/pdf/2505.08138)  

**Abstract**: Machine unlearning methods take a model trained on a dataset and a forget set, then attempt to produce a model as if it had only been trained on the examples not in the forget set. We empirically show that an adversary is able to distinguish between a mirror model (a control model produced by retraining without the data to forget) and a model produced by an unlearning method across representative unlearning methods from the literature. We build distinguishing algorithms based on evaluation scores in the literature (i.e. membership inference scores) and Kullback-Leibler divergence.
We propose a strong formal definition for machine unlearning called computational unlearning. Computational unlearning is defined as the inability for an adversary to distinguish between a mirror model and a model produced by an unlearning method. If the adversary cannot guess better than random (except with negligible probability), then we say that an unlearning method achieves computational unlearning.
Our computational unlearning definition provides theoretical structure to prove unlearning feasibility results. For example, our computational unlearning definition immediately implies that there are no deterministic computational unlearning methods for entropic learning algorithms. We also explore the relationship between differential privacy (DP)-based unlearning methods and computational unlearning, showing that DP-based approaches can satisfy computational unlearning at the cost of an extreme utility collapse. These results demonstrate that current methodology in the literature fundamentally falls short of achieving computational unlearning. We conclude by identifying several open questions for future work. 

**Abstract (ZH)**: 机器去学习方法 empirical 表明对手能够区分镜像模型和去学习方法生成的模型 across 文献中代表性的去学习方法。我们基于文献中的评估分数（即成员推断分数）和克劳德-莱布勒散度构建了区分算法。

我们提出了机器去学习的强形式化定义，称为计算去学习。计算去学习定义为对手无法区分镜像模型和去学习方法生成的模型。如果对手无法比随机猜测更好（除了微不足道的概率），则我们认为去学习方法实现了计算去学习。

我们的计算去学习定义为证明去学习可行性结果提供了理论结构。例如，我们的计算去学习定义直接表明，对于熵学习算法，不存在确定性的计算去学习方法。我们还探讨了基于差分隐私（DP）的去学习方法与计算去学习之间的关系，表明为了满足计算去学习，基于DP的方法会导致极高的实用性下降。这些结果表明文献中的现有方法在实现计算去学习方面从根本上存在不足。最后，我们指出了未来工作的几个开放问题。 

---
# Leveraging AI for Productive and Trustworthy HPC Software: Challenges and Research Directions 

**Title (ZH)**: 利用AI促进高效可信赖的HPC软件开发：挑战与研究方向 

**Authors**: Keita Teranishi, Harshitha Menon, William F. Godoy, Prasanna Balaprakash, David Bau, Tal Ben-Nun, Abhinav Bathele, Franz Franchetti, Michael Franusich, Todd Gamblin, Giorgis Georgakoudis, Tom Goldstein, Arjun Guha, Steven Hahn, Costin Iancu, Zheming Jin, Terry Jones, Tze Meng Low, Het Mankad, Narasinga Rao Miniskar, Mohammad Alaul Haque Monil, Daniel Nichols, Konstantinos Parasyris, Swaroop Pophale, Pedro Valero-Lara, Jeffrey S. Vetter, Samuel Williams, Aaron Young  

**Link**: [PDF](https://arxiv.org/pdf/2505.08135)  

**Abstract**: We discuss the challenges and propose research directions for using AI to revolutionize the development of high-performance computing (HPC) software. AI technologies, in particular large language models, have transformed every aspect of software development. For its part, HPC software is recognized as a highly specialized scientific field of its own. We discuss the challenges associated with leveraging state-of-the-art AI technologies to develop such a unique and niche class of software and outline our research directions in the two US Department of Energy--funded projects for advancing HPC Software via AI: Ellora and Durban. 

**Abstract (ZH)**: 我们探讨利用AI革新高性能计算（HPC）软件开发面临的挑战，并提出研究方向。作为特别专业的科学领域，HPC软件受到关注。我们讨论了利用最新AI技术开发这种独特且狭窄类别的软件所面临的挑战，并概述了在两个由美国能源部资助的通过AI推进HPC软件的项目Ellora和Durban中的研究方向。 

---
# One Bad NOFO? AI Governance in Federal Grantmaking 

**Title (ZH)**: 一坏百坏的NOFO？联邦拨款中的AI治理 

**Authors**: Dan Bateyko, Karen Levy  

**Link**: [PDF](https://arxiv.org/pdf/2505.08133)  

**Abstract**: Much scholarship considers how U.S. federal agencies govern artificial intelligence (AI) through rulemaking and their own internal use policies. But agencies have an overlooked AI governance role: setting discretionary grant policy when directing billions of dollars in federal financial assistance. These dollars enable state and local entities to study, create, and use AI. This funding not only goes to dedicated AI programs, but also to grantees using AI in the course of meeting their routine grant objectives. As discretionary grantmakers, agencies guide and restrict what grant winners do -- a hidden lever for AI governance. Agencies pull this lever by setting program objectives, judging criteria, and restrictions for AI use. Using a novel dataset of over 40,000 non-defense federal grant notices of funding opportunity (NOFOs) posted to this http URL between 2009 and 2024, we analyze how agencies regulate the use of AI by grantees. We select records mentioning AI and review their stated goals and requirements. We find agencies promoting AI in notice narratives, shaping adoption in ways other records of grant policy might fail to capture. Of the grant opportunities that mention AI, we find only a handful of AI-specific judging criteria or restrictions. This silence holds even when agencies fund AI uses in contexts affecting people's rights and which, under an analogous federal procurement regime, would result in extra oversight. These findings recast grant notices as a site of AI policymaking -- albeit one that is developing out of step with other regulatory efforts and incomplete in its consideration of transparency, accountability, and privacy protections. The paper concludes by drawing lessons from AI procurement scholarship, while identifying distinct challenges in grantmaking that invite further study. 

**Abstract (ZH)**: 美国联邦机构在发放资助政策中监管人工智能：一个被忽视的治理角色 

---
# ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval 

**Title (ZH)**: ALOHA: 赋能多语言导生代理的层次化检索方法 

**Authors**: Mingxu Tao, Bowen Tang, Mingxuan Ma, Yining Zhang, Hourun Li, Feifan Wen, Hao Ma, Jia Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08130)  

**Abstract**: The rise of Large Language Models~(LLMs) revolutionizes information retrieval, allowing users to obtain required answers through complex instructions within conversations. However, publicly available services remain inadequate in addressing the needs of faculty and students to search campus-specific information. It is primarily due to the LLM's lack of domain-specific knowledge and the limitation of search engines in supporting multilingual and timely scenarios. To tackle these challenges, we introduce ALOHA, a multilingual agent enhanced by hierarchical retrieval for university orientation. We also integrate external APIs into the front-end interface to provide interactive service. The human evaluation and case study show our proposed system has strong capabilities to yield correct, timely, and user-friendly responses to the queries in multiple languages, surpassing commercial chatbots and search engines. The system has been deployed and has provided service for more than 12,000 people. 

**Abstract (ZH)**: 大型语言模型的兴起变革了信息检索，使得用户能够通过对话中的复杂指令获得所需答案。然而，公开可用的服务仍不足以满足教职工和学生搜索校内信息的需求。主要是因为大型语言模型缺乏领域专业知识，以及搜索引擎在支持多语言和及时场景方面存在限制。为应对这些挑战，我们引入了ALOHA，这是一种增强型层次化检索多语言代理，用于大学迎新。我们还在前端界面中集成了外部API，以提供交互式服务。人类评估和案例研究显示，我们提出系统在多语言查询中具备提供准确、及时且用户友好的响应的强大能力，超越了商用聊天机器人和搜索引擎。该系统已部署并为超过12,000人提供了服务。 

---
# High-order Regularization for Machine Learning and Learning-based Control 

**Title (ZH)**: 高阶正则化在机器学习与基于学习的控制中的应用 

**Authors**: Xinghua Liu, Ming Cao  

**Link**: [PDF](https://arxiv.org/pdf/2505.08129)  

**Abstract**: The paper proposes a novel regularization procedure for machine learning. The proposed high-order regularization (HR) provides new insight into regularization, which is widely used to train a neural network that can be utilized to approximate the action-value function in general reinforcement learning problems. The proposed HR method ensures the provable convergence of the approximation algorithm, which makes the much-needed connection between regularization and explainable learning using neural networks. The proposed HR method theoretically demonstrates that regularization can be regarded as an approximation in terms of inverse mapping with explicitly calculable approximation error, and the $L_2$ regularization is a lower-order case of the proposed method. We provide lower and upper bounds for the error of the proposed HR solution, which helps build a reliable model. We also find that regularization with the proposed HR can be regarded as a contraction. We prove that the generalizability of neural networks can be maximized with a proper regularization matrix, and the proposed HR is applicable for neural networks with any mapping matrix. With the theoretical explanation of the extreme learning machine for neural network training and the proposed high-order regularization, one can better interpret the output of the neural network, thus leading to explainable learning. We present a case study based on regularized extreme learning neural networks to demonstrate the application of the proposed HR and give the corresponding incremental HR solution. We verify the performance of the proposed HR method by solving a classic control problem in reinforcement learning. The result demonstrates the superior performance of the method with significant enhancement in the generalizability of the neural network. 

**Abstract (ZH)**: 一种新型机器学习正则化程序：高阶正则化在强化学习中的应用 

---
# SLAG: Scalable Language-Augmented Gaussian Splatting 

**Title (ZH)**: SLAG：可扩展的语言增强高斯渲染 

**Authors**: Laszlo Szilagyi, Francis Engelmann, Jeannette Bohg  

**Link**: [PDF](https://arxiv.org/pdf/2505.08124)  

**Abstract**: Language-augmented scene representations hold great promise for large-scale robotics applications such as search-and-rescue, smart cities, and mining. Many of these scenarios are time-sensitive, requiring rapid scene encoding while also being data-intensive, necessitating scalable solutions. Deploying these representations on robots with limited computational resources further adds to the challenge. To address this, we introduce SLAG, a multi-GPU framework for language-augmented Gaussian splatting that enhances the speed and scalability of embedding large scenes. Our method integrates 2D visual-language model features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG eliminates the need for a loss function to compute per-Gaussian language embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters via a normalized weighted average, enabling highly parallelized scene encoding. Additionally, we introduce a vector database for efficient embedding storage and retrieval. Our experiments show that SLAG achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while preserving embedding quality on the ScanNet and LERF datasets. For more details, visit our project website: this https URL. 

**Abstract (ZH)**: 语言增强的场景表示在大规模机器人应用如搜救、智慧城市和矿业中充满潜力。许多这些场景对时间响应敏感，需要快速的场景编码，同时也数据密集，需要可扩展的解决方案。将这些表示部署在计算资源有限的机器人上进一步增加了挑战。为了解决这个问题，我们引入了SLAG，这是一种多GPU框架，用于语言增强的Gaussian splatting，增强了大规模场景嵌入的速度和可扩展性。我们的方法通过SAM和CLIP将2D视觉语言模型特征整合到3D场景中。与之前的 approaches 不同，SLAG 不需要计算每个高斯的损失函数来获取语言嵌入。相反，它通过归一化的加权平均从3D高斯场景参数中获取嵌入，从而实现高效的并行场景编码。此外，我们还引入了向量数据库，以实现高效的嵌入存储和检索。我们的实验表明，与OpenGaussian相比，在16-GPU设置下，SLAG 在嵌入计算上的加速比达到了18倍，同时在ScanNet和LERF数据集上保持了嵌入质量。更多信息，请访问我们的项目网站：this https URL。 

---
# JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections 

**Title (ZH)**: JSover: 单能CT投影的联合频谱估计与多材料分解 

**Authors**: Qing Wu, Hongjiang Wei, Jingyi Yu, S. Kevin Zhou, Yuyao Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08123)  

**Abstract**: Multi-material decomposition (MMD) enables quantitative reconstruction of tissue compositions in the human body, supporting a wide range of clinical applications. However, traditional MMD typically requires spectral CT scanners and pre-measured X-ray energy spectra, significantly limiting clinical applicability. To this end, various methods have been developed to perform MMD using conventional (i.e., single-energy, SE) CT systems, commonly referred to as SEMMD. Despite promising progress, most SEMMD methods follow a two-step image decomposition pipeline, which first reconstructs monochromatic CT images using algorithms such as FBP, and then performs decomposition on these images. The initial reconstruction step, however, neglects the energy-dependent attenuation of human tissues, introducing severe nonlinear beam hardening artifacts and noise into the subsequent decomposition. This paper proposes JSover, a fundamentally reformulated one-step SEMMD framework that jointly reconstructs multi-material compositions and estimates the energy spectrum directly from SECT projections. By explicitly incorporating physics-informed spectral priors into the SEMMD process, JSover accurately simulates a virtual spectral CT system from SE acquisitions, thereby improving the reliability and accuracy of decomposition. Furthermore, we introduce implicit neural representation (INR) as an unsupervised deep learning solver for representing the underlying material maps. The inductive bias of INR toward continuous image patterns constrains the solution space and further enhances estimation quality. Extensive experiments on both simulated and real CT datasets show that JSover outperforms state-of-the-art SEMMD methods in accuracy and computational efficiency. 

**Abstract (ZH)**: 基于单能量CT的多材料分解：JSover框架 

---
# Are LLMs complicated ethical dilemma analyzers? 

**Title (ZH)**: 大型语言模型是复杂的伦理困境分析器吗？ 

**Authors**: Jiashen, Jesse Yao, Allen Liu, Zhekai Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08106)  

**Abstract**: One open question in the study of Large Language Models (LLMs) is whether they can emulate human ethical reasoning and act as believable proxies for human judgment. To investigate this, we introduce a benchmark dataset comprising 196 real-world ethical dilemmas and expert opinions, each segmented into five structured components: Introduction, Key Factors, Historical Theoretical Perspectives, Resolution Strategies, and Key Takeaways. We also collect non-expert human responses for comparison, limited to the Key Factors section due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini, Claude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric framework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine similarity, and Universal Sentence Encoder similarity. Metric weights are computed through an inversion-based ranking alignment and pairwise AHP analysis, enabling fine-grained comparison of model outputs to expert responses. Our results show that LLMs generally outperform non-expert humans in lexical and structural alignment, with GPT-4o-mini performing most consistently across all sections. However, all models struggle with historical grounding and proposing nuanced resolution strategies, which require contextual abstraction. Human responses, while less structured, occasionally achieve comparable semantic similarity, suggesting intuitive moral reasoning. These findings highlight both the strengths and current limitations of LLMs in ethical decision-making. 

**Abstract (ZH)**: 大型语言模型能否模拟人类伦理推理并在伦理判断中作为可信代理：基于196个现实伦理困境及专家意见的基准数据集探究 

---
# Graph-Based Floor Separation Using Node Embeddings and Clustering of WiFi Trajectories 

**Title (ZH)**: 基于图的楼层分离：节点嵌入与WiFi轨迹聚类方法 

**Authors**: Rabia Yasa Kostas, Kahraman Kostas  

**Link**: [PDF](https://arxiv.org/pdf/2505.08088)  

**Abstract**: Indoor positioning systems (IPSs) are increasingly vital for location-based services in complex multi-storey environments. This study proposes a novel graph-based approach for floor separation using Wi-Fi fingerprint trajectories, addressing the challenge of vertical localization in indoor settings. We construct a graph where nodes represent Wi-Fi fingerprints, and edges are weighted by signal similarity and contextual transitions. Node2Vec is employed to generate low-dimensional embeddings, which are subsequently clustered using K-means to identify distinct floors. Evaluated on the Huawei University Challenge 2021 dataset, our method outperforms traditional community detection algorithms, achieving an accuracy of 68.97%, an F1- score of 61.99%, and an Adjusted Rand Index of 57.19%. By publicly releasing the preprocessed dataset and implementation code, this work contributes to advancing research in indoor positioning. The proposed approach demonstrates robustness to signal noise and architectural complexities, offering a scalable solution for floor-level localization. 

**Abstract (ZH)**: 基于Wi-Fi指纹轨迹的图表示方法在室内多层环境中的楼层分离研究 

---
# Fréchet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids 

**Title (ZH)**: Frechet 动势场景距离：一种用于智能电网中多时间尺度评估生成式AI模型的度量标准 

**Authors**: Yuting Cai, Shaohuai Liu, Chao Tian, Le Xie  

**Link**: [PDF](https://arxiv.org/pdf/2505.08082)  

**Abstract**: Generative artificial intelligence (AI) models in smart grids have advanced significantly in recent years due to their ability to generate large amounts of synthetic data, which would otherwise be difficult to obtain in the real world due to confidentiality constraints. A key challenge in utilizing such synthetic data is how to assess the data quality produced from such generative models. Traditional Euclidean distance-based metrics only reflect pair-wise relations between two individual samples, and could fail in evaluating quality differences between groups of synthetic datasets. In this work, we propose a novel metric based on the Fréchet Distance (FD) estimated between two datasets in a learned feature space. The proposed method evaluates the quality of generation from a distributional perspective. Empirical results demonstrate the superiority of the proposed metric across timescales and models, enhancing the reliability of data-driven decision-making in smart grid operations. 

**Abstract (ZH)**: 智能电网中基于生成人工智能（AI）模型的合成数据质量评估方法：一种基于学习特征空间中Fréchet距离的新颖度量 

---
# Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders 

**Title (ZH)**: 超越输入激活：基于梯度稀疏自编码器识别影响力潜变量 

**Authors**: Dong Shu, Xuansheng Wu, Haiyan Zhao, Mengnan Du, Ninghao Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.08080)  

**Abstract**: Sparse Autoencoders (SAEs) have recently emerged as powerful tools for interpreting and steering the internal representations of large language models (LLMs). However, conventional approaches to analyzing SAEs typically rely solely on input-side activations, without considering the causal influence between each latent feature and the model's output. This work is built on two key hypotheses: (1) activated latents do not contribute equally to the construction of the model's output, and (2) only latents with high causal influence are effective for model steering. To validate these hypotheses, we propose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method that identifies the most influential latents by incorporating output-side gradient information. 

**Abstract (ZH)**: 稀疏自编码器（SAEs） recently emerged as强大的工具，用于解释和控制大型语言模型（LLMs）的内部表示。然而，传统上分析SAEs的方法通常仅依赖于输入端激活，而不考虑每个潜在特征对模型输出的因果影响。本文基于两个关键假设：（1）激活的潜在特征并不等价地贡献于模型输出的构建，（2）只有具有高因果影响的潜在特征对模型控制有效。为了验证这些假设，我们提出了梯度稀疏自编码器（GradSAE），这是一种简单而有效的方法，通过结合输出端梯度信息来识别最具影响力的最佳潜在特征。 

---
# What Matters for Batch Online Reinforcement Learning in Robotics? 

**Title (ZH)**: 机器人领域批量在线强化学习中什么是重要的？ 

**Authors**: Perry Dong, Suvir Mirchandani, Dorsa Sadigh, Chelsea Finn  

**Link**: [PDF](https://arxiv.org/pdf/2505.08078)  

**Abstract**: The ability to learn from large batches of autonomously collected data for policy improvement -- a paradigm we refer to as batch online reinforcement learning -- holds the promise of enabling truly scalable robot learning by significantly reducing the need for human effort of data collection while getting benefits from self-improvement. Yet, despite the promise of this paradigm, it remains challenging to achieve due to algorithms not being able to learn effectively from the autonomous data. For example, prior works have applied imitation learning and filtered imitation learning methods to the batch online RL problem, but these algorithms often fail to efficiently improve from the autonomously collected data or converge quickly to a suboptimal point. This raises the question of what matters for effective batch online RL in robotics. Motivated by this question, we perform a systematic empirical study of three axes -- (i) algorithm class, (ii) policy extraction methods, and (iii) policy expressivity -- and analyze how these axes affect performance and scaling with the amount of autonomous data. Through our analysis, we make several observations. First, we observe that the use of Q-functions to guide batch online RL significantly improves performance over imitation-based methods. Building on this, we show that an implicit method of policy extraction -- via choosing the best action in the distribution of the policy -- is necessary over traditional policy extraction methods from offline RL. Next, we show that an expressive policy class is preferred over less expressive policy classes. Based on this analysis, we propose a general recipe for effective batch online RL. We then show a simple addition to the recipe of using temporally-correlated noise to obtain more diversity results in further performance gains. Our recipe obtains significantly better performance and scaling compared to prior methods. 

**Abstract (ZH)**: 从自主收集的大批次数据中进行策略改进的学习能力——一种我们称之为批在线强化学习的范式——有望通过显著减少数据收集的人力需求来实现真正 scalable 的机器人学习，同时还能获得自我改进的好处。然而，尽管这一范式具有巨大的前景，但由于算法难以有效学习自主数据，实现起来仍然具有挑战性。例如，先前的工作将模仿学习和过滤模仿学习方法应用于批在线 RL 问题，但这些算法往往无法有效地从自主收集的数据中提高自身，或迅速收敛到一个次优点。这提出了什么对于有效的批在线 RL 在机器人领域的研究。在这一问题的驱动下，我们系统地研究了三个维度——（i）算法类别，（ii）策略提取方法，以及（iii）策略表达性——并分析了这些维度如何影响性能和随自主数据量增加的可扩展性。通过我们的分析，我们做出了几项观察。首先，我们观察到使用 Q 函数来指导批在线 RL 显著提高了性能，超过了基于模仿的方法。在这一点基础上，我们展示了策略提取的隐式方法——通过选择策略分布中的最佳动作——比传统的离线 RL 中的策略提取方法更为必要。其次，我们展示了具有丰富表达能力的策略类别优于表达能力较弱的类别。基于这一分析，我们提出了一个有效的批在线 RL 的通用配方。然后，我们展示了一种简单的附加方法——使用时序相关噪声以获得更多的多样性，进一步提高了性能。我们的配方相对于先前的方法在性能和可扩展性方面取得了显著的改进。 

---
# Justified Evidence Collection for Argument-based AI Fairness Assurance 

**Title (ZH)**: 基于论证的AI公平性保障的正当证据收集 

**Authors**: Alpay Sabuncuoglu, Christopher Burr, Carsten Maple  

**Link**: [PDF](https://arxiv.org/pdf/2505.08064)  

**Abstract**: It is well recognised that ensuring fair AI systems is a complex sociotechnical challenge, which requires careful deliberation and continuous oversight across all stages of a system's lifecycle, from defining requirements to model deployment and deprovisioning. Dynamic argument-based assurance cases, which present structured arguments supported by evidence, have emerged as a systematic approach to evaluating and mitigating safety risks and hazards in AI-enabled system development and have also been extended to deal with broader normative goals such as fairness and explainability. This paper introduces a systems-engineering-driven framework, supported by software tooling, to operationalise a dynamic approach to argument-based assurance in two stages. In the first stage, during the requirements planning phase, a multi-disciplinary and multi-stakeholder team define goals and claims to be established (and evidenced) by conducting a comprehensive fairness governance process. In the second stage, a continuous monitoring interface gathers evidence from existing artefacts (e.g. metrics from automated tests), such as model, data, and use case documentation, to support these arguments dynamically. The framework's effectiveness is demonstrated through an illustrative case study in finance, with a focus on supporting fairness-related arguments. 

**Abstract (ZH)**: 确保公平的AI系统是一个复杂的社会技术挑战，需要在系统生命周期的所有阶段，从需求定义到模型部署和撤销，进行仔细的考虑和持续监督。基于动态论证的保证案例通过呈现结构化的论证和支持证据的论据，已成为评估和减轻AI使能系统开发中安全风险和危害的一种系统方法，并已扩展以处理公平性和可解释性等更广泛的规范目标。本文提出了一种由系统工程驱动并在软件工具支持下的框架，以实现动态论证保证的两阶段实现。在第一阶段的需求规划阶段，多学科和多利益相关者团队通过全面的公平治理过程定义和建立目标与证据。在第二阶段，持续监测界面从现有 artefacts（如自动化测试的指标）中收集模型、数据和用例文档等证据，以动态支持这些论证。该框架的有效性通过一个金融领域的示例研究得以体现，重点关注支持公平性相关的论证。 

---
# FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning 

**Title (ZH)**: 拒绝假象：一种通过结构化推理提高上下文安全性和缓解过度拒绝的资源 

**Authors**: Zhehao Zhang, Weijie Xu, Fanyou Wu, Chandan K. Reddy  

**Link**: [PDF](https://arxiv.org/pdf/2505.08054)  

**Abstract**: Safety alignment approaches in large language models (LLMs) often lead to the over-refusal of benign queries, significantly diminishing their utility in sensitive scenarios. To address this challenge, we introduce FalseReject, a comprehensive resource containing 16k seemingly toxic queries accompanied by structured responses across 44 safety-related categories. We propose a graph-informed adversarial multi-agent interaction framework to generate diverse and complex prompts, while structuring responses with explicit reasoning to aid models in accurately distinguishing safe from unsafe contexts. FalseReject includes training datasets tailored for both standard instruction-tuned models and reasoning-oriented models, as well as a human-annotated benchmark test set. Our extensive benchmarking on 29 state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges. Empirical results demonstrate that supervised finetuning with FalseReject substantially reduces unnecessary refusals without compromising overall safety or general language capabilities. 

**Abstract (ZH)**: 大型语言模型（LLMs）中的安全性对齐方法往往会导致对良性查询的过度拒绝，显著降低了其在敏感场景中的实用性。为解决这一挑战，我们引入了FalseReject，这是一个包含16,000个看似有毒查询及其在44个安全相关类别中结构化响应的全面资源。我们提出了一种基于图的信息对抗多智能体交互框架，以生成多样且复杂的提示，并通过明确的推理结构化响应，帮助模型准确区分安全与不安全的上下文。FalseReject包括针对标准指令调谐模型和推理导向模型的训练数据集，以及一个人工注释的标准测试集。我们对29个最先进的（SOTA）LLMs的广泛基准测试揭示了持续存在的过度拒绝挑战。实证结果表明，使用FalseReject进行监督微调在不牺牲整体安全性和通用语言能力的情况下，显著减少了不必要的拒绝。 

---
# NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition 

**Title (ZH)**: NAZM：波斯诗歌传统中的区域指标网络分析 

**Authors**: Kourosh Shahnazari, Seyed Moein Ayyoubzadeh  

**Link**: [PDF](https://arxiv.org/pdf/2505.08052)  

**Abstract**: This study formalizes a computational model to simulate classical Persian poets' dynamics of influence through constructing a multi-dimensional similarity network. Using a rigorously curated dataset based on Ganjoor's corpus, we draw upon semantic, lexical, stylistic, thematic, and metrical features to demarcate each poet's corpus. Each is contained within weighted similarity matrices, which are then appended to generate an aggregate graph showing poet-to-poet influence. Further network investigation is carried out to identify key poets, style hubs, and bridging poets by calculating degree, closeness, betweenness, eigenvector, and Katz centrality measures. Further, for typological insight, we use the Louvain community detection algorithm to demarcate clusters of poets sharing both style and theme coherence, which correspond closely to acknowledged schools of literature like Sabk-e Hindi, Sabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a new data-driven view of Persian literature distinguished between canonical significance and interextual influence, thus highlighting relatively lesser-known figures who hold great structural significance. Combining computational linguistics with literary study, this paper produces an interpretable and scalable model for poetic tradition, enabling retrospective reflection as well as forward-looking research within digital humanities. 

**Abstract (ZH)**: 本研究通过构建多维相似性网络，形式化了一个计算模型来模拟古典波斯诗人之间的影响动态。基于GANJOOR语料库精心选择的数据集，我们利用语义、词汇、风格、主题和韵律特征来划定每位诗人的作品集。每位诗人的作品集包含在加权相似性矩阵中，然后合并生成一个显示诗人之间影响的综合图。进一步的网络分析通过计算度、接近中心性、介数中心性、特征向量中心性和Katz中心性来识别关键诗人、风格枢纽和桥梁诗人。此外，为了提供类型学上的洞察，我们使用Louvain社区检测算法来划定共享风格和主题一致性的诗人簇，这些簇与公认的文学流派如哈尼诗派、柯霍桑诗派和文学反刍现象相对应。我们的发现提供了一种基于数据的新视角，区分了经典意义和互文影响，从而突出了相对鲜为人知但在结构上具有重要意义的诗人。结合计算语言学与文学研究，本文构建了一个可解释且可扩展的诗歌传统的模型，有助于数字人文领域的回顾性反思及前瞻性研究。 

---
# Online Learning-based Adaptive Beam Switching for 6G Networks: Enhancing Efficiency and Resilience 

**Title (ZH)**: 基于在线学习的自适应波束切换技术在6G网络中的应用：提高效率与增强韧性 

**Authors**: Seyed Bagher Hashemi Natanzi, Zhicong Zhu, Bo Tang  

**Link**: [PDF](https://arxiv.org/pdf/2505.08032)  

**Abstract**: Adaptive beam switching in 6G networks is challenged by high frequencies, mobility, and blockage. We propose an Online Learning framework using Deep Reinforcement Learning (DRL) with an enhanced state representation (velocity and blockage history), a GRU architecture, and prioritized experience replay for real-time beam optimization. Validated via Nvidia Sionna under time-correlated blockage, our approach significantly enhances resilience in SNR, throughput, and accuracy compared to a conventional heuristic. Furthermore, the enhanced DRL agent outperforms a reactive Multi-Armed Bandit (MAB) baseline by leveraging temporal dependencies, achieving lower performance variability. This demonstrates the benefits of memory and prioritized learning for robust 6G beam management, while confirming MAB as a strong baseline. 

**Abstract (ZH)**: 基于深度强化学习的自适应波束切换在6G网络中的实时波束优化 

---
# PRISM: Complete Online Decentralized Multi-Agent Pathfinding with Rapid Information Sharing using Motion Constraints 

**Title (ZH)**: PRISM: 完整的基于运动约束的快速信息共享在线去中心化多智能体路径规划算法 

**Authors**: Hannah Lee, Zachary Serlin, James Motes, Brendan Long, Marco Morales, Nancy M. Amato  

**Link**: [PDF](https://arxiv.org/pdf/2505.08025)  

**Abstract**: We introduce PRISM (Pathfinding with Rapid Information Sharing using Motion Constraints), a decentralized algorithm designed to address the multi-task multi-agent pathfinding (MT-MAPF) problem. PRISM enables large teams of agents to concurrently plan safe and efficient paths for multiple tasks while avoiding collisions. It employs a rapid communication strategy that uses information packets to exchange motion constraint information, enhancing cooperative pathfinding and situational awareness, even in scenarios without direct communication. We prove that PRISM resolves and avoids all deadlock scenarios when possible, a critical challenge in decentralized pathfinding. Empirically, we evaluate PRISM across five environments and 25 random scenarios, benchmarking it against the centralized Conflict-Based Search (CBS) and the decentralized Token Passing with Task Swaps (TPTS) algorithms. PRISM demonstrates scalability and solution quality, supporting 3.4 times more agents than CBS and handling up to 2.5 times more tasks in narrow passage environments than TPTS. Additionally, PRISM matches CBS in solution quality while achieving faster computation times, even under low-connectivity conditions. Its decentralized design reduces the computational burden on individual agents, making it scalable for large environments. These results confirm PRISM's robustness, scalability, and effectiveness in complex and dynamic pathfinding scenarios. 

**Abstract (ZH)**: PRISM (基于运动约束的信息快速共享路径发现):一种分布式多任务多agent路径规划算法 

---
# Large Language Models and Arabic Content: A Review 

**Title (ZH)**: 大型语言模型与阿拉伯内容：一个综述 

**Authors**: Haneh Rhel, Dmitri Roussinov  

**Link**: [PDF](https://arxiv.org/pdf/2505.08004)  

**Abstract**: Over the past three years, the rapid advancement of Large Language Models (LLMs) has had a profound impact on multiple areas of Artificial Intelligence (AI), particularly in Natural Language Processing (NLP) across diverse languages, including Arabic. Although Arabic is considered one of the most widely spoken languages across 27 countries in the Arabic world and used as a second language in some other non-Arabic countries as well, there is still a scarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face various challenges due to the complexities of the Arabic language, including its rich morphology, intricate structure, and diverse writing standards, among other factors. Researchers have been actively addressing these challenges, demonstrating that pre-trained Large Language Models (LLMs) trained on multilingual corpora achieve significant success in various Arabic NLP tasks. This study provides an overview of using large language models (LLMs) for the Arabic language, highlighting early pre-trained Arabic Language models across various NLP applications and their ability to handle diverse Arabic content tasks and dialects. It also provides an overview of how techniques like finetuning and prompt engineering can enhance the performance of these models. Additionally, the study summarizes common Arabic benchmarks and datasets while presenting our observations on the persistent upward trend in the adoption of LLMs. 

**Abstract (ZH)**: 过去三年，大型语言模型的迅速发展对人工智能多个领域产生了深远影响，特别是在跨多种语言的自然语言处理（NLP）领域，包括阿拉伯语。尽管阿拉伯语是阿拉伯世界27个国家中最广泛使用的语言，并且在一些非阿拉伯国家中也被用作第二语言，但阿拉伯语资源、数据集和工具仍然相对匮乏。阿拉伯语NLP任务由于阿拉伯语丰富的形态学、复杂的结构和多样的书写标准等因素面临着各种挑战。研究人员积极应对这些挑战，证明了在多语言语料库上预训练的大型语言模型（LLMs）在各种阿拉伯语NLP任务中取得了显著成功。本研究概述了使用大型语言模型（LLMs）处理阿拉伯语的方法，强调了各种NLP应用中早期预训练的阿拉伯语言模型及其处理多种阿拉伯语内容任务和方言的能力。此外，研究还概述了如何通过微调和提示工程来提升这些模型的性能。研究还总结了常用的阿拉伯语基准和数据集，并呈现了LLMs采用持续增长的趋势观察。 

---
# Fair Play for Individuals, Foul Play for Groups? Auditing Anonymization's Impact on ML Fairness 

**Title (ZH)**: 个体公平，群体不公？探究匿名化对机器学习公平性影响的审计 

**Authors**: Héber H. Arcolezi, Mina Alishahi, Adda-Akram Bendoukha, Nesrine Kaaniche  

**Link**: [PDF](https://arxiv.org/pdf/2505.07985)  

**Abstract**: Machine learning (ML) algorithms are heavily based on the availability of training data, which, depending on the domain, often includes sensitive information about data providers. This raises critical privacy concerns. Anonymization techniques have emerged as a practical solution to address these issues by generalizing features or suppressing data to make it more difficult to accurately identify individuals. Although recent studies have shown that privacy-enhancing technologies can influence ML predictions across different subgroups, thus affecting fair decision-making, the specific effects of anonymization techniques, such as $k$-anonymity, $\ell$-diversity, and $t$-closeness, on ML fairness remain largely unexplored. In this work, we systematically audit the impact of anonymization techniques on ML fairness, evaluating both individual and group fairness. Our quantitative study reveals that anonymization can degrade group fairness metrics by up to four orders of magnitude. Conversely, similarity-based individual fairness metrics tend to improve under stronger anonymization, largely as a result of increased input homogeneity. By analyzing varying levels of anonymization across diverse privacy settings and data distributions, this study provides critical insights into the trade-offs between privacy, fairness, and utility, offering actionable guidelines for responsible AI development. Our code is publicly available at: this https URL. 

**Abstract (ZH)**: 机器学习算法高度依赖训练数据的可用性，而这些数据在不同的领域往往包含数据提供者的敏感信息。这引发了重要的隐私担忧。匿名化技术已经出现作为一种实际的解决方案，通过泛化特征或抑制数据，使其更难准确识别个体。尽管近期研究显示，增强隐私的技术可以影响ML预测的不同子群体，从而影响公平决策，但匿名化技术，如$k$-匿名性、$\ell$-多样性、$t$-相近性，对ML公平性的影响仍然很大程度上未被探索。在本项工作中，我们系统地审计了匿名化技术对ML公平性的影响，评估了个体和群体公平性。我们的定量研究揭示匿名化可以使群体公平性指标下降四数量级。相反，在更强的匿名化下，基于相似性的个体公平性指标往往会改善，主要是由于输入同质性的增加。通过分析不同隐私设置和数据分布下不同水平的匿名化，本研究提供了关于隐私、公平性和效用之间权衡的重要见解，为负责任的AI发展提供了可操作的指导。我们的代码可在以下链接获取：this https URL。 

---
# Probabilistic approach to longitudinal response prediction: application to radiomics from brain cancer imaging 

**Title (ZH)**: 基于概率方法的纵向反应预测：应用于脑癌影像的放射组学 

**Authors**: Isabella Cama, Michele Piana, Cristina Campi, Sara Garbarino  

**Link**: [PDF](https://arxiv.org/pdf/2505.07973)  

**Abstract**: Longitudinal imaging analysis tracks disease progression and treatment response over time, providing dynamic insights into treatment efficacy and disease evolution. Radiomic features extracted from medical imaging can support the study of disease progression and facilitate longitudinal prediction of clinical outcomes. This study presents a probabilistic model for longitudinal response prediction, integrating baseline features with intermediate follow-ups. The probabilistic nature of the model naturally allows to handle the instrinsic uncertainty of the longitudinal prediction of disease progression. We evaluate the proposed model against state-of-the-art disease progression models in both a synthetic scenario and using a brain cancer dataset. Results demonstrate that the approach is competitive against existing methods while uniquely accounting for uncertainty and controlling the growth of problem dimensionality, eliminating the need for data from intermediate follow-ups. 

**Abstract (ZH)**: 纵向影像分析追踪疾病进展和治疗反应，提供治疗 efficacy 和疾病演变的动力学洞察。从医学影像中提取的 Radiomic 特征可支持疾病进展的研究，并促进临床结局的纵向预测。本研究提出了一种将基线特征与中期随访整合的概率模型，其概率性质自然地处理纵向预测疾病进展的固有不确定性。该研究在合成场景和脑癌数据集中分别与最先进的疾病进展模型进行评估，结果表明所提出的方法在与现有方法竞争的同时，单独考虑不确定性并控制问题维度的增长，从而消除对中期随访数据的需求。 

---
# Self-cross Feature based Spiking Neural Networks for Efficient Few-shot Learning 

**Title (ZH)**: 基于自交特征的Spiking神经网络高效 Few-shot 学习 

**Authors**: Qi Xu, Junyang Zhu, Dongdong Zhou, Hao Chen, Yang Liu, Jiangrong Shen, Qiang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.07921)  

**Abstract**: Deep neural networks (DNNs) excel in computer vision tasks, especially, few-shot learning (FSL), which is increasingly important for generalizing from limited examples. However, DNNs are computationally expensive with scalability issues in real world. Spiking Neural Networks (SNNs), with their event-driven nature and low energy consumption, are particularly efficient in processing sparse and dynamic data, though they still encounter difficulties in capturing complex spatiotemporal features and performing accurate cross-class comparisons. To further enhance the performance and efficiency of SNNs in few-shot learning, we propose a few-shot learning framework based on SNNs, which combines a self-feature extractor module and a cross-feature contrastive module to refine feature representation and reduce power consumption. We apply the combination of temporal efficient training loss and InfoNCE loss to optimize the temporal dynamics of spike trains and enhance the discriminative power. Experimental results show that the proposed FSL-SNN significantly improves the classification performance on the neuromorphic dataset N-Omniglot, and also achieves competitive performance to ANNs on static datasets such as CUB and miniImageNet with low power consumption. 

**Abstract (ZH)**: 基于SNN的少样本学习框架：结合自特征提取模块和跨特征对比模块以提高性能和效率 

---
# Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions 

**Title (ZH)**: Re$^2$：一种确保一致性的数据集，用于全程同行评审和多轮反驳讨论 

**Authors**: Daoze Zhang, Zhijian Bao, Sihang Du, Zhiyi Zhao, Kuangling Zhang, Dezheng Bao, Yang Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.07920)  

**Abstract**: Peer review is a critical component of scientific progress in the fields like AI, but the rapid increase in submission volume has strained the reviewing system, which inevitably leads to reviewer shortages and declines review quality. Besides the growing research popularity, another key factor in this overload is the repeated resubmission of substandard manuscripts, largely due to the lack of effective tools for authors to self-evaluate their work before submission. Large Language Models (LLMs) show great promise in assisting both authors and reviewers, and their performance is fundamentally limited by the quality of the peer review data. However, existing peer review datasets face three major limitations: (1) limited data diversity, (2) inconsistent and low-quality data due to the use of revised rather than initial submissions, and (3) insufficient support for tasks involving rebuttal and reviewer-author interactions. To address these challenges, we introduce the largest consistency-ensured peer review and rebuttal dataset named Re^2, which comprises 19,926 initial submissions, 70,668 review comments, and 53,818 rebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the rebuttal and discussion stage is framed as a multi-turn conversation paradigm to support both traditional static review tasks and dynamic interactive LLM assistants, providing more practical guidance for authors to refine their manuscripts and helping alleviate the growing review burden. Our data and code are available in this https URL. 

**Abstract (ZH)**: 大型语言模型在辅助作者和审稿人方面的潜力及其对同行评审数据质量的限制：引入Re^2数据集以应对同行评审挑战 

---
# Efficient and Reproducible Biomedical Question Answering using Retrieval Augmented Generation 

**Title (ZH)**: 基于检索增强生成的高效可再现生物医学问答 

**Authors**: Linus Stuhlmann, Michael Alexander Saxer, Jonathan Fürst  

**Link**: [PDF](https://arxiv.org/pdf/2505.07917)  

**Abstract**: Biomedical question-answering (QA) systems require effective retrieval and generation components to ensure accuracy, efficiency, and scalability. This study systematically examines a Retrieval-Augmented Generation (RAG) system for biomedical QA, evaluating retrieval strategies and response time trade-offs. We first assess state-of-the-art retrieval methods, including BM25, BioBERT, MedCPT, and a hybrid approach, alongside common data stores such as Elasticsearch, MongoDB, and FAISS, on a ~10% subset of PubMed (2.4M documents) to measure indexing efficiency, retrieval latency, and retriever performance in the end-to-end RAG system. Based on these insights, we deploy the final RAG system on the full 24M PubMed corpus, comparing different retrievers' impact on overall performance. Evaluations of the retrieval depth show that retrieving 50 documents with BM25 before reranking with MedCPT optimally balances accuracy (0.90), recall (0.90), and response time (1.91s). BM25 retrieval time remains stable (82ms), while MedCPT incurs the main computational cost. These results highlight previously not well-known trade-offs in retrieval depth, efficiency, and scalability for biomedical QA. With open-source code, the system is fully reproducible and extensible. 

**Abstract (ZH)**: 生物医学问答(QA)系统需要有效的检索和生成组件以确保准确、高效和可扩展。本研究系统地探讨了生物医学QA的检索增强生成(RAG)系统，评估了检索策略和响应时间的权衡。我们首先在PubMed大约10%的子集（240万文档）上评估了最先进的检索方法，包括BM25、BioBERT、MedCPT及混合方法，以及常用的索引存储方式如Elasticsearch、MongoDB和FAISS，以度量端到端RAG系统的索引效率、检索延迟和检索器性能。基于这些见解，我们在整个2400万PubMed语料库上部署了最终的RAG系统，比较了不同检索器对总体性能的影响。检索深度的评估表明，在使用BM25检索50份文档后再排序（二次排序）使用MedCPT可最优地平衡准确率（0.90）、召回率（0.90）和响应时间（1.91秒）。BM25的检索时间为稳定值（82毫秒），而MedCPT主要承担计算成本。这些结果突显了生物医学QA中检索深度、效率和可扩展性之间的前所未知的权衡。该系统具有开源代码，确保完全可再现性和扩展性。 

---
# Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review 

**Title (ZH)**: 融合贝叶斯推断与强化学习的代理决策制作：一篇综述 

**Authors**: Chengmin Zhou, Ville Kyrki, Pasi Fränti, Laura Ruotsalainen  

**Link**: [PDF](https://arxiv.org/pdf/2505.07911)  

**Abstract**: Bayesian inference has many advantages in decision making of agents (e.g. robotics/simulative agent) over a regular data-driven black-box neural network: Data-efficiency, generalization, interpretability, and safety where these advantages benefit directly/indirectly from the uncertainty quantification of Bayesian inference. However, there are few comprehensive reviews to summarize the progress of Bayesian inference on reinforcement learning (RL) for decision making to give researchers a systematic understanding. This paper focuses on combining Bayesian inference with RL that nowadays is an important approach in agent decision making. To be exact, this paper discusses the following five topics: 1) Bayesian methods that have potential for agent decision making. First basic Bayesian methods and models (Bayesian rule, Bayesian learning, and Bayesian conjugate models) are discussed followed by variational inference, Bayesian optimization, Bayesian deep learning, Bayesian active learning, Bayesian generative models, Bayesian meta-learning, and lifelong Bayesian learning. 2) Classical combinations of Bayesian methods with model-based RL (with approximation methods), model-free RL, and inverse RL. 3) Latest combinations of potential Bayesian methods with RL. 4) Analytical comparisons of methods that combine Bayesian methods with RL with respect to data-efficiency, generalization, interpretability, and safety. 5) In-depth discussions in six complex problem variants of RL, including unknown reward, partial-observability, multi-agent, multi-task, non-linear non-Gaussian, and hierarchical RL problems and the summary of how Bayesian methods work in the data collection, data processing and policy learning stages of RL to pave the way for better agent decision-making strategies. 

**Abstract (ZH)**: 贝叶斯推断在代理决策（如机器人/模拟代理）中相对于常规的数据驱动黑盒神经网络有许多优势：数据效率、泛化能力、可解释性和安全性，这些优势直接或间接地受益于贝叶斯推断的不确定性量化。然而，鲜有全面的综述总结贝叶斯推断在强化学习（RL）中的进展，以帮助研究人员系统地理解这一领域。本文专注于结合贝叶斯推断与RL，这是当前代理决策中的一个重要方法。具体来说，本文讨论了以下五个主题：1) 具有代理决策潜力的贝叶斯方法。首先讨论基础的贝叶斯方法和模型（贝叶斯规则、贝叶斯学习和共轭模型），随后讨论变分推断、贝叶斯优化、贝叶斯深度学习、贝叶斯主动学习、生成模型、元学习和终身学习。2) 贝叶斯方法与基于模型的RL（含近似方法）、无模型RL和逆RL的经典结合。3) 最新的贝叶斯方法与RL的结合。4) 在数据效率、泛化能力、可解释性和安全性方面，结合贝叶斯方法与RL的方法的分析比较。5) 对六种复杂RL问题变体的深入讨论，包括未知奖励、部分可观测性、多代理、多任务、非线性非高斯以及层次化RL问题，并总结贝叶斯方法在RL的数据收集、数据处理和策略学习阶段的工作，为更好的代理决策策略铺平道路。 

---
# Tuning for Trustworthiness -- Balancing Performance and Explanation Consistency in Neural Network Optimization 

**Title (ZH)**: 调谐以提升可信度——在神经网络优化中平衡性能与解释一致性 

**Authors**: Alexander Hinterleitner, Thomas Bartz-Beielstein  

**Link**: [PDF](https://arxiv.org/pdf/2505.07910)  

**Abstract**: Despite the growing interest in Explainable Artificial Intelligence (XAI), explainability is rarely considered during hyperparameter tuning or neural architecture optimization, where the focus remains primarily on minimizing predictive loss. In this work, we introduce the novel concept of XAI consistency, defined as the agreement among different feature attribution methods, and propose new metrics to quantify it. For the first time, we integrate XAI consistency directly into the hyperparameter tuning objective, creating a multi-objective optimization framework that balances predictive performance with explanation robustness. Implemented within the Sequential Parameter Optimization Toolbox (SPOT), our approach uses both weighted aggregation and desirability-based strategies to guide model selection. Through our proposed framework and supporting tools, we explore the impact of incorporating XAI consistency into the optimization process. This enables us to characterize distinct regions in the architecture configuration space: one region with poor performance and comparatively low interpretability, another with strong predictive performance but weak interpretability due to low \gls{xai} consistency, and a trade-off region that balances both objectives by offering high interpretability alongside competitive performance. Beyond introducing this novel approach, our research provides a foundation for future investigations into whether models from the trade-off zone-balancing performance loss and XAI consistency-exhibit greater robustness by avoiding overfitting to training performance, thereby leading to more reliable predictions on out-of-distribution data. 

**Abstract (ZH)**: 尽管可解释的人工智能（XAI）日益受到关注，但在超参数调整和神经网络架构优化过程中，解释性 rarely 考虑，研究重点仍主要集中在最小化预测损失上。本文引入了新颖的 XAI 一致性概念，定义为不同特征归因方法的一致性，并提出新的度量方法来量化它。首次将 XAI 一致性直接纳入超参数调整目标，创建了兼顾预测性能与解释鲁棒性的多目标优化框架。在 Sequential Parameter Optimization Toolbox (SPOT) 中实现该方法，通过加权聚合和偏好策略指导模型选择。通过提出的框架及其支持工具，我们探讨了将 XAI 一致性纳入优化过程的影响。这使我们能够区分架构配置空间中的不同区域：一个区域具有较差的性能和较低的可解释性，另一个区域具有较强预测性能但因较低的 XAI 一致性而缺乏可解释性，还有一个权衡区域通过提供高可解释性与竞争力的性能，同时平衡两个目标。除了提出这一新颖方法外，我们的研究为未来探讨性能损失和 XAI 一致性之间的权衡区域中的模型是否因避免训练性能过拟合而表现出更大的鲁棒性，从而在离群数据上产生更可靠的预测奠定了基础。 

---
# A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny 

**Title (ZH)**: 一项再现研究：在详细审查下，自我注意力的核PCA解释失败 

**Authors**: Karahan Sarıtaş, Çağatay Yıldız  

**Link**: [PDF](https://arxiv.org/pdf/2505.07908)  

**Abstract**: In this reproduction study, we revisit recent claims that self-attention implements kernel principal component analysis (KPCA) (Teo et al., 2024), positing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix of the keys, and (ii) that self-attention projects queries onto the principal component axes of the key matrix $K$ in a feature space. Our analysis reveals three critical inconsistencies: (1) No alignment exists between learned self-attention value vectors and what is proposed in the KPCA perspective, with average similarity metrics (optimal cosine similarity $\leq 0.32$, linear CKA (Centered Kernel Alignment) $\leq 0.11$, kernel CKA $\leq 0.32$) indicating negligible correspondence; (2) Reported decreases in reconstruction loss $J_\text{proj}$, arguably justifying the claim that the self-attention minimizes the projection error of KPCA, are misinterpreted, as the quantities involved differ by orders of magnitude ($\sim\!10^3$); (3) Gram matrix eigenvalue statistics, introduced to justify that $V$ captures the eigenvector of the gram matrix, are irreproducible without undocumented implementation-specific adjustments. Across 10 transformer architectures, we conclude that the KPCA interpretation of self-attention lacks empirical support. 

**Abstract (ZH)**: 在本重现研究中，我们重新审视了近期关于自注意力机制实现核主成分分析（KPCA）的主张（Teo等，2024），提出（i）值向量$V$捕获键的Gram矩阵的特征向量，和（ii）自注意力将查询投影到键矩阵$K$在特征空间的主要成分轴上。我们的分析揭示了三个关键不一致：（1）学习到的自注意力值向量与KPCA视角下提出的向量之间不存在对齐，平均相似度指标（最优余弦相似度$\leq 0.32$，线性CKA（中心核对齐）$\leq 0.11$，核CKA$\leq 0.32$）显示几乎没有对应关系；（2）所报告的重构损失$J_\text{proj}$的减少被误解为自注意力最小化KPCA投影误差，因为参与的量级相差三个数量级；（3）用于证明$V$捕获Gram矩阵特征向量的Gram矩阵特征值统计，在没有未记录的实现特定调整的情况下无法重现。在10种变压器架构中，我们得出结论，自注意力的KPCA解释缺乏实证支持。 

---
# SEM: Reinforcement Learning for Search-Efficient Large Language Models 

**Title (ZH)**: SEM：用于搜索高效的大型语言模型的强化学习 

**Authors**: Zeyang Sha, Shiwen Cui, Weiqiang Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.07903)  

**Abstract**: Recent advancements in Large Language Models(LLMs) have demonstrated their capabilities not only in reasoning but also in invoking external tools, particularly search engines. However, teaching models to discern when to invoke search and when to rely on their internal knowledge remains a significant challenge. Existing reinforcement learning approaches often lead to redundant search behaviors, resulting in inefficiencies and over-cost. In this paper, we propose SEM, a novel post-training reinforcement learning framework that explicitly trains LLMs to optimize search usage. By constructing a balanced dataset combining MuSiQue and MMLU, we create scenarios where the model must learn to distinguish between questions it can answer directly and those requiring external retrieval. We design a structured reasoning template and employ Group Relative Policy Optimization(GRPO) to post-train the model's search behaviors. Our reward function encourages accurate answering without unnecessary search while promoting effective retrieval when needed. Experimental results demonstrate that our method significantly reduces redundant search operations while maintaining or improving answer accuracy across multiple challenging benchmarks. This framework advances the model's reasoning efficiency and extends its capability to judiciously leverage external knowledge. 

**Abstract (ZH)**: Recent advancements in大型语言模型(LLMs)的能力不仅体现在推理上，还体现在调用外部工具，尤其是搜索引擎方面。然而，教会模型何时调用搜索、何时依赖内部知识仍然是一项重大挑战。现有强化学习方法往往导致冗余的搜索行为，造成低效率和高成本。在本文中，我们提出了一种名为SEM的新型后训练强化学习框架，旨在显式训练LLMs优化搜索使用。通过构建结合MuSiQue和MMLU的数据集，我们创建了使模型学会区分可以直接回答的问题和需要外部检索的问题的情景。我们设计了一种结构化推理模板，并采用Group Relative Policy Optimization (GRPO) 后训练模型的搜索行为。我们的奖励函数鼓励准确回答而不进行不必要的搜索，并在需要时促进有效的检索。实验结果表明，我们的方法显著减少了冗余搜索操作，同时在多个具有挑战性的基准上维持或提高了答案准确性。该框架提高了模型的推理效率，并扩展了它有节制地利用外部知识的能力。 

---
# Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach 

**Title (ZH)**: 基于文本中心注意力的多任务学习的课堂教学 discourse 质量的多模态评估方法 

**Authors**: Ruikun Hou, Babette Bühler, Tim Fütterer, Efe Bozkir, Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci  

**Link**: [PDF](https://arxiv.org/pdf/2505.07902)  

**Abstract**: Classroom discourse is an essential vehicle through which teaching and learning take place. Assessing different characteristics of discursive practices and linking them to student learning achievement enhances the understanding of teaching quality. Traditional assessments rely on manual coding of classroom observation protocols, which is time-consuming and costly. Despite many studies utilizing AI techniques to analyze classroom discourse at the utterance level, investigations into the evaluation of discursive practices throughout an entire lesson segment remain limited. To address this gap, our study proposes a novel text-centered multimodal fusion architecture to assess the quality of three discourse components grounded in the Global Teaching InSights (GTI) observation protocol: Nature of Discourse, Questioning, and Explanations. First, we employ attention mechanisms to capture inter- and intra-modal interactions from transcript, audio, and video streams. Second, a multi-task learning approach is adopted to jointly predict the quality scores of the three components. Third, we formulate the task as an ordinal classification problem to account for rating level order. The effectiveness of these designed elements is demonstrated through an ablation study on the GTI Germany dataset containing 92 videotaped math lessons. Our results highlight the dominant role of text modality in approaching this task. Integrating acoustic features enhances the model's consistency with human ratings, achieving an overall Quadratic Weighted Kappa score of 0.384, comparable to human inter-rater reliability (0.326). Our study lays the groundwork for the future development of automated discourse quality assessment to support teacher professional development through timely feedback on multidimensional discourse practices. 

**Abstract (ZH)**: 课堂话语是教学和学习进行的重要载体。评估话语实践的不同特征并将它们与学生学业成就联系起来，有助于提高对教学质量的理解。传统评估依赖于人工编译课堂观察协议，耗时且成本高。尽管许多研究利用AI技术在话语层面分析课堂话语，但关于整个教学片段中话语实践评估的研究仍然有限。为弥补这一空白，本研究提出了一种以文本为中心的多模态融合架构，评估Global Teaching InSights (GTI) 观察协议下的三种话语组件的质量：话语性质、提问和解释。首先，我们采用注意力机制捕捉转录、音频和视频流之间的跨模态和内模态交互。其次，采用多任务学习方法联合预测三种组件的质量分数。第三，我们将任务表述为序数分类问题，以考虑评分等级的顺序。通过在包含92节录数学课程的GTI德国数据集上的消融研究，证明了所设计元素的有效性。我们的结果突显了文本模态在处理此任务中的主导作用。结合声学特征提高了模型与人类评分的一致性，获得了总体的Quadratic Weighted Kappa评分为0.384，这一评分与人类评分者可靠性（0.326）相当。本研究为利用及时的多维度话语实践反馈支持教师专业发展奠定了基础。 

---
# Latent Behavior Diffusion for Sequential Reaction Generation in Dyadic Setting 

**Title (ZH)**: 双边设置中序列反应生成的潜在行为扩散 

**Authors**: Minh-Duc Nguyen, Hyung-Jeong Yang, Soo-Hyung Kim, Ji-Eun Shin, Seung-Won Kim  

**Link**: [PDF](https://arxiv.org/pdf/2505.07901)  

**Abstract**: The dyadic reaction generation task involves synthesizing responsive facial reactions that align closely with the behaviors of a conversational partner, enhancing the naturalness and effectiveness of human-like interaction simulations. This paper introduces a novel approach, the Latent Behavior Diffusion Model, comprising a context-aware autoencoder and a diffusion-based conditional generator that addresses the challenge of generating diverse and contextually relevant facial reactions from input speaker behaviors. The autoencoder compresses high-dimensional input features, capturing dynamic patterns in listener reactions while condensing complex input data into a concise latent representation, facilitating more expressive and contextually appropriate reaction synthesis. The diffusion-based conditional generator operates on the latent space generated by the autoencoder to predict realistic facial reactions in a non-autoregressive manner. This approach allows for generating diverse facial reactions that reflect subtle variations in conversational cues and emotional states. Experimental results demonstrate the effectiveness of our approach in achieving superior performance in dyadic reaction synthesis tasks compared to existing methods. 

**Abstract (ZH)**: 二元反应生成任务涉及合成与对话伙伴行为紧密对齐的响应面部表情，以增强人类交互模拟的自然性和有效性。本文提出了一种新颖的方法——潜在行为扩散模型，该模型包含一个情境感知自编码器和一种基于扩散的条件生成器，以解决从输入说话者行为生成多样且情境相关面部反应的挑战。自编码器压缩高维输入特征，捕捉听众反应中的动态模式，同时将复杂输入数据凝聚成简洁的潜在表示，从而促进更具表现力和情境恰当的反应合成。基于扩散的条件生成器在自编码器生成的潜在空间上以非自回归方式预测现实的面部反应。这种方法允许生成反映对话提示和情感状态微妙变化的多样面部反应。实验结果证明，与现有方法相比，该方法在二元反应生成任务中实现了更优的表现。 

---
# DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise 

**Title (ZH)**: DeltaEdit: 通过控制叠加噪声来增强大型语言模型的序列编辑能力 

**Authors**: Ding Cao, Yuchen Cai, Rongxi Guo, Xuesong He, Guiquan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.07899)  

**Abstract**: Sequential knowledge editing techniques aim to continuously update the knowledge in large language models at a low cost, preventing the models from generating outdated or incorrect information. However, existing sequential editing methods suffer from a significant decline in editing success rates after long-term editing. Through theoretical analysis and experiments, we identify that as the number of edits increases, the model's output increasingly deviates from the desired target, leading to a drop in editing success rates. We refer to this issue as the accumulation of superimposed noise problem. To address this, we identify the factors contributing to this deviation and propose DeltaEdit, a novel method that optimizes update parameters through a dynamic orthogonal constraints strategy, effectively reducing interference between edits to mitigate deviation. Experimental results demonstrate that DeltaEdit significantly outperforms existing methods in edit success rates and the retention of generalization capabilities, ensuring stable and reliable model performance even under extensive sequential editing. 

**Abstract (ZH)**: Sequential知识编辑技术旨在以低成本持续更新大型语言模型的知识，防止模型生成过时或错误的信息。然而，现有的序列编辑方法在长期编辑后编辑成功率显著下降。通过理论分析和实验，我们发现随着编辑次数的增加，模型的输出越来越偏离期望目标，导致编辑成功率下降。我们将这一问题称为叠加噪声累积问题。为解决这一问题，我们确定了导致偏差的因素，并提出了一种名为DeltaEdit的新方法，该方法通过动态正交约束策略优化更新参数，有效减少编辑之间的干扰以减轻偏差。实验结果表明，DeltaEdit在编辑成功率和保持泛化能力方面显著优于现有方法，确保在广泛进行序列编辑的情况下模型性能的稳定性和可靠性。 

---
# LongCodeBench: Evaluating Coding LLMs at 1M Context Windows 

**Title (ZH)**: LongCodeBench: 评估具有100万上下文窗口的编程LLM 

**Authors**: Stefano Rando, Luca Romani, Alessio Sampieri, Yuta Kyuragi, Luca Franco, Fabio Galasso, Tatsunori Hashimoto, John Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.07897)  

**Abstract**: Context lengths for models have grown rapidly, from thousands to millions of tokens in just a few years. The extreme context sizes of modern long-context models have made it difficult to construct realistic long-context benchmarks -- not only due to the cost of collecting million-context tasks but also in identifying realistic scenarios that require significant contexts. We identify code comprehension and repair as a natural testbed and challenge task for long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM coding abilities in long-context scenarios. Our benchmark tests both the comprehension and repair capabilities of LCLMs in realistic and important settings by drawing from real-world GitHub issues and constructing QA (LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the complexity of our benchmark, enabling us to evaluate models across different scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model. We find that long-context remains a weakness for all models, with performance drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for Qwen2.5. 

**Abstract (ZH)**: 长上下文长度对模型的影响：从千级到百万级令牌的快速增长使得构建现实的长上下文基准变得困难——不仅由于收集百万级上下文任务的成本问题，还在于识别需要大量上下文的现实场景。我们认为空间理解和修复是测试长上下文模型的自然测试床和挑战任务，并引入LongCodeBench (LCB)，一个用于测试长上下文场景中LLM编码能力的基准。我们的基准通过借鉴真实世界的GitHub问题集，测试了从问题理解和修复能力，在各种规模的现实和重要场景中评估模型，从Qwen2.5 14B Instruct到Google的旗舰Gemini模型。我们发现所有模型在长上下文方面仍存在弱点，如Claude 3.5 Sonnet的性能从29%下降到3%，或Qwen2.5的性能从70.2%下降到40%。 

---
# Bridging Large Language Models and Single-Cell Transcriptomics in Dissecting Selective Motor Neuron Vulnerability 

**Title (ZH)**: 将大型语言模型与单细胞转录组学结合以解析选择性运动神经元易感性 

**Authors**: Douglas Jiang, Zilin Dai, Luxuan Zhang, Qiyi Yu, Haoqi Sun, Feng Tian  

**Link**: [PDF](https://arxiv.org/pdf/2505.07896)  

**Abstract**: Understanding cell identity and function through single-cell level sequencing data remains a key challenge in computational biology. We present a novel framework that leverages gene-specific textual annotations from the NCBI Gene database to generate biologically contextualized cell embeddings. For each cell in a single-cell RNA sequencing (scRNA-seq) dataset, we rank genes by expression level, retrieve their NCBI Gene descriptions, and transform these descriptions into vector embedding representations using large language models (LLMs). The models used include OpenAI text-embedding-ada-002, text-embedding-3-small, and text-embedding-3-large (Jan 2024), as well as domain-specific models BioBERT and SciBERT. Embeddings are computed via an expression-weighted average across the top N most highly expressed genes in each cell, providing a compact, semantically rich representation. This multimodal strategy bridges structured biological data with state-of-the-art language modeling, enabling more interpretable downstream applications such as cell-type clustering, cell vulnerability dissection, and trajectory inference. 

**Abstract (ZH)**: 通过单细胞水平测序数据理解细胞身份和功能仍然是计算生物学中的一个关键挑战。我们提出了一种新的框架，利用NCBI Gene数据库中的基因特定文本注释生成生物上下文化的细胞嵌入。对于单细胞RNA测序(scRNA-seq)数据集中每个细胞，我们按照表达水平对基因进行排名，检索其NCBI Gene描述，并使用大型语言模型（LLMs）将这些描述转换为向量嵌入表示。所使用的模型包括OpenAI的text-embedding-ada-002、text-embedding-3-small和text-embedding-3-large（2024年1月），以及领域特定模型BioBERT和SciBERT。嵌入通过每个细胞前N个最高表达基因的表达加权平均计算得出，提供了一个紧凑且语义丰富的表示。这种多模态策略将结构化的生物数据与最先进的语言建模相结合，使下游应用更具可解释性，如细胞类型聚类、细胞脆弱性解析和轨迹推断。 

---
# Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks 

**Title (ZH)**: 多模态异构网络中模态间相互影响的表示学习在节点分类中的应用 

**Authors**: Jiafan Li, Jiaqi Zhu, Liang Chang, Yilin Li, Miaomiao Li, Yang Wang, Hongan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.07895)  

**Abstract**: Nowadays, numerous online platforms can be described as multi-modal heterogeneous networks (MMHNs), such as Douban's movie networks and Amazon's product review networks. Accurately categorizing nodes within these networks is crucial for analyzing the corresponding entities, which requires effective representation learning on nodes. However, existing multi-modal fusion methods often adopt either early fusion strategies which may lose the unique characteristics of individual modalities, or late fusion approaches overlooking the cross-modal guidance in GNN-based information propagation. In this paper, we propose a novel model for node classification in MMHNs, named Heterogeneous Graph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns node representations by capturing the mutual influence of multiple modalities during the information propagation process, within the framework of heterogeneous graph transformer. Specifically, a nested inter-modal attention mechanism is integrated into the inter-node attention to achieve adaptive multi-modal fusion, and modality alignment is also taken into account to encourage the propagation among nodes with consistent similarities across all modalities. Moreover, an attention loss is augmented to mitigate the impact of missing modalities. Extensive experiments validate the superiority of the model in the node classification task, providing an innovative view to handle multi-modal data, especially when accompanied with network structures. 

**Abstract (ZH)**: 异构图神经网络中的跨模态注意力模型：Heterogeneous Graph Neural Network with Inter-Modal Attention (HGNN-IMA) 

---
# TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking 

**Title (ZH)**: 谣言GPT：基于图的检索增强大规模语言模型用于事实核查 

**Authors**: Ching Nam Hang, Pei-Duo Yu, Chee Wei Tan  

**Link**: [PDF](https://arxiv.org/pdf/2505.07891)  

**Abstract**: In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT , a novel generative artificial intelligence solution designed for fact-checking in the health domain. TrumorGPT aims to distinguish "trumors", which are health-related rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework leverages a large language model (LLM) with few-shot learning for semantic health knowledge graph construction and semantic reasoning. TrumorGPT incorporates graph-based retrieval-augmented generation (GraphRAG) to address the hallucination issue common in LLMs and the limitations of static training data. GraphRAG involves accessing and utilizing information from regularly updated semantic health knowledge graphs that consist of the latest medical news and health information, ensuring that fact-checking by TrumorGPT is based on the most recent data. Evaluating with extensive healthcare datasets, TrumorGPT demonstrates superior performance in fact-checking for public health claims. Its ability to effectively conduct fact-checking across various platforms marks a critical step forward in the fight against health-related misinformation, enhancing trust and accuracy in the digital information age. 

**Abstract (ZH)**: 社交媒体时代的信息疫情与谣言治理：TrumorGPT在健康领域的事实核查应用 

---
# Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping 

**Title (ZH)**: 利用双层句子和段落结构提取与映射在LLMs中实现长文本风格转移 

**Authors**: Yusen Wu, Xiaotie Deng  

**Link**: [PDF](https://arxiv.org/pdf/2505.07888)  

**Abstract**: This paper addresses the challenge in long-text style transfer using zero-shot learning of large language models (LLMs), proposing a hierarchical framework that combines sentence-level stylistic adaptation with paragraph-level structural coherence. We argue that in the process of effective paragraph-style transfer, to preserve the consistency of original syntactic and semantic information, it is essential to perform style transfer not only at the sentence level but also to incorporate paragraph-level semantic considerations, while ensuring structural coherence across inter-sentential relationships. Our proposed framework, ZeroStylus, operates through two systematic phases: hierarchical template acquisition from reference texts and template-guided generation with multi-granular matching. The framework dynamically constructs sentence and paragraph template repositories, enabling context-aware transformations while preserving inter-sentence logical relationships. Experimental evaluations demonstrate significant improvements over baseline methods, with structured rewriting achieving 6.90 average score compared to 6.70 for direct prompting approaches in tri-axial metrics assessing style consistency, content preservation, and expression quality. Ablation studies validate the necessity of both template hierarchies during style transfer, showing higher content preservation win rate against sentence-only approaches through paragraph-level structural encoding, as well as direct prompting method through sentence-level pattern extraction and matching. The results establish new capabilities for coherent long-text style transfer without requiring parallel corpora or LLM fine-tuning. 

**Abstract (ZH)**: 基于零-shot学习的大语言模型在长文本风格转换中的挑战及其层次化框架：ZeroStylus 

---
# PLHF: Prompt Optimization with Few-Shot Human Feedback 

**Title (ZH)**: PLHF: few-shot人类反馈的提示优化 

**Authors**: Chun-Pai Yang, Kan Zheng, Shou-De Lin  

**Link**: [PDF](https://arxiv.org/pdf/2505.07886)  

**Abstract**: Automatic prompt optimization frameworks are developed to obtain suitable prompts for large language models (LLMs) with respect to desired output quality metrics. Although existing approaches can handle conventional tasks such as fixed-solution question answering, defining the metric becomes complicated when the output quality cannot be easily assessed by comparisons with standard golden samples. Consequently, optimizing the prompts effectively and efficiently without a clear metric becomes a critical challenge. To address the issue, we present PLHF (which stands for "P"rompt "L"earning with "H"uman "F"eedback), a few-shot prompt optimization framework inspired by the well-known RLHF technique. Different from naive strategies, PLHF employs a specific evaluator module acting as the metric to estimate the output quality. PLHF requires only a single round of human feedback to complete the entire prompt optimization process. Empirical results on both public and industrial datasets show that PLHF outperforms prior output grading strategies for LLM prompt optimizations. 

**Abstract (ZH)**: 自动提示优化框架被开发出来，以针对大型语言模型（LLMs）获得符合期望输出质量指标的提示。虽然现有方法可以处理诸如固定解问答等传统任务，但在输出质量无法通过与标准黄金样本进行简单比较来评估时，定义质量指标变得复杂。因此，没有明确指标的情况下有效高效地优化提示成为一个关键挑战。为了解决这一问题，我们提出了PLHF（“P”rompt “L”earning with “H”uman “F”eedback），这是一种受RLHF技术启发的少样本提示优化框架。与朴素策略不同，PLHF 使用特定的评估模块充当质量指标，以估算输出质量。PLHF 只需一轮人类反馈即可完成整个提示优化过程。在公共和工业数据集上的实证结果表明，PLHF 在LLM提示优化中的输出评分策略中表现出更好的性能。 

---
# Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints 

**Title (ZH)**: 基于公理约束从大型语言模型嵌入中恢复事件概率 

**Authors**: Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths  

**Link**: [PDF](https://arxiv.org/pdf/2505.07883)  

**Abstract**: Rational decision-making under uncertainty requires coherent degrees of belief in events. However, event probabilities generated by Large Language Models (LLMs) have been shown to exhibit incoherence, violating the axioms of probability theory. This raises the question of whether coherent event probabilities can be recovered from the embeddings used by the models. If so, those derived probabilities could be used as more accurate estimates in events involving uncertainty. To explore this question, we propose enforcing axiomatic constraints, such as the additive rule of probability theory, in the latent space learned by an extended variational autoencoder (VAE) applied to LLM embeddings. This approach enables event probabilities to naturally emerge in the latent space as the VAE learns to both reconstruct the original embeddings and predict the embeddings of semantically related events. We evaluate our method on complementary events (i.e., event A and its complement, event not-A), where the true probabilities of the two events must sum to 1. Experiment results on open-weight language models demonstrate that probabilities recovered from embeddings exhibit greater coherence than those directly reported by the corresponding models and align closely with the true probabilities. 

**Abstract (ZH)**: 在不确定性下的理性决策要求事件具有一致的信念度。然而，大型语言模型（LLMs）生成的事件概率表现出不一致，违反了概率论的公理。这引发了从LLMs使用的嵌入中恢复一致的事件概率是否可行的问题。如果可行，这些衍生的概率可以在涉及不确定性的事件中作为更准确的估计使用。为探索这一问题，我们提出在扩展的变分自编码器（VAE）应用于LLM嵌入的学习潜空间中实施公理约束，如概率论的加法规则。这种方法使得事件概率自然地在潜空间中出现，当VAE学习重构原始嵌入并预测语义相关事件的嵌入时。我们在互补事件（即事件A及其补事件，事件非A）上评估了该方法，其中两个事件的真实概率之和为1。实验结果表明，从嵌入中恢复的概率比对应模型直接报告的更一致，并且与真实概率紧密一致。 

---
# OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval 

**Title (ZH)**: OMGM： orchestrating 多种粒度和模态以实现高效的多模态检索 

**Authors**: Wei Yang, Jingjing Fu, Rui Wang, Jinyu Wang, Lei Song, Jiang Bian  

**Link**: [PDF](https://arxiv.org/pdf/2505.07879)  

**Abstract**: Vision-language retrieval-augmented generation (RAG) has become an effective approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which requires external knowledge beyond the visual content presented in images. The effectiveness of Vision-language RAG systems hinges on multimodal retrieval, which is inherently challenging due to the diverse modalities and knowledge granularities in both queries and knowledge bases. Existing methods have not fully tapped into the potential interplay between these elements. We propose a multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that harmonizes multiple granularities and modalities to enhance efficacy. Our system begins with a broad initial search aligning knowledge granularity for cross-modal retrieval, followed by a multimodal fusion reranking to capture the nuanced multimodal information for top entity selection. A text reranker then filters out the most relevant fine-grained section for augmented generation. Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our method achieves state-of-the-art retrieval performance and highly competitive answering results, underscoring its effectiveness in advancing KB-VQA systems. 

**Abstract (ZH)**: 视觉-语言检索增强生成（RAG）已成为处理知识导向的视觉问答（KB-VQA）的有效方法，KB-VQA需要超越图像呈现的视觉内容的外部知识。视觉-语言RAG系统的有效性取决于多模态检索，由于查询和知识库中的多种模态和知识粒度，这一过程本身具有挑战性。现有方法尚未充分挖掘这些元素之间的潜在互动。我们提出了一种多模态RAG系统，该系统采用从粗到细的多步检索方式，协调多个粒度和模态以提高效果。该系统首先进行广泛的初步检索以对齐知识粒度，为跨模态检索做准备，然后进行多模态融合排名重新排序，以捕捉关键实体的细微多模态信息。随后，文本重新排序器筛选出最相关的细粒度部分，以增强生成。在InfoSeek和Encyclopedic-VQA基准上的广泛实验表明，我们的方法在检索性能上达到最新水平，并且答案结果极具竞争力，凸显了其在推进KB-VQA系统方面的有效性。 

---
# Efficient Telecom Specific LLM: TSLAM-Mini with QLoRA and Digital Twin Data 

**Title (ZH)**: 电信专用高效LLM：TSLAM-Mini结合QLoRA和数字孪生数据 

**Authors**: Vignesh Ethiraj, Divya Vijay, Sidhanth Menon, Heblin Berscilla  

**Link**: [PDF](https://arxiv.org/pdf/2505.07877)  

**Abstract**: General-purpose large language models (LLMs), despite their broad capabilities accrued from open-world data, frequently exhibit suboptimal performance when confronted with the nuanced and specialized demands inherent in real-time telecommunications applications. This investigation addresses this critical limitation through the meticulous fine-tuning of TSLAM-Mini developed by NetoAI, a compact (3.8-billion parameter) causal language model architecturally derived from Phi-4 Mini Instruct 4B. The fine-tuning regimen leverages a bespoke dataset comprising 100,000 samples, strategically engineered to address 20 pivotal telecommunications use-cases, encompassing domains such as Network Fundamentals, IP Routing, MPLS, Network Security, Automation, OSS/BSS, RAN, Mobile Core, Satellite Communications, and Ethical AI. This dataset was curated utilizing NetoAI's DigiTwin platform, enriched with granular insights from venerated network Subject Matter Experts (SMEs) and authoritative RFC documents, thereby capturing high-fidelity representations of real-world network dynamics through simulations inspired by digital twin paradigms. Employing Quantized Low-Rank Adaptation (QLoRA), a state-of-the-art Parameter Efficient Fine-Tuning (PEFT) technique, we achieved substantial training efficiency and enabled prospective deployment on resource-constrained hardware. A novel evaluation framework, predicated on a high-capacity LLM (Qwen3-235B-A22B) functioning as an automated adjudicator, was instituted to rigorously assess instruction-following fidelity and response quality across the specified telecom use-cases. Empirical results unequivocally demonstrate TSLAM-Mini's superior aptitude in telecom-centric applications, underscoring the profound efficacy of domain-specific datasets and PEFT methodologies for advancing intelligent network management. 

**Abstract (ZH)**: 通用大型语言模型（LLMs）尽管可以从开放世界数据中获得广泛的能力，但在面对实时电信应用中复杂和专业的需求时，经常表现出次优性能。本研究通过精细调整NetoAI开发的TSLAM-Mini（一个38亿参数的因果语言模型，源自Phi-4 Mini Instruct 4B）来解决这一关键限制。精细调整过程采用了包含100,000个样本的独特数据集，该数据集专门设计以解决20个关键的电信用例，涵盖了网络基础、IP路由、MPLS、网络安全、自动化、OSS/BSS、RAN、移动核心、卫星通信和伦理AI等领域。该数据集利用NetoAI的DigiTwin平台精心挑选，并结合了资深网络主题专家（SMEs）的详细见解和权威RFC文档，通过受数字孪生理念启发的仿真，捕捉到高保真度的现实网络动态。利用Quantized Low-Rank Adaptation（QLoRA）这一最先进的参数高效调整（PEFT）技术，我们实现了显著的训练效率，并使模型能够在资源受限的硬件上进行潜在部署。我们建立了一个新颖的评估框架，以一个超大容量的LLM（Qwen3-235B-A22B）作为自动化仲裁者，严格评估指令跟随的忠实度和响应质量，针对指定的电信用例。实验证明TSLAM-Mini在电信中心应用中的优越能力，突显了领域特定数据集和PEFT方法在智能网络管理发展中的深远效果。 

---
# Getting Ready for the EU AI Act in Healthcare. A call for Sustainable AI Development and Deployment 

**Title (ZH)**: 为应对欧盟AI法案在医疗健康领域的实施：呼吁可持续AI开发与部署 

**Authors**: John Brandt Brodersen, Ilaria Amelia Caggiano, Pedro Kringen, Vince Istvan Madai, Walter Osika, Giovanni Sartor, Ellen Svensson, Magnus Westerlund, Roberto V. Zicari  

**Link**: [PDF](https://arxiv.org/pdf/2505.07875)  

**Abstract**: Assessments of trustworthiness have become a cornerstone of responsible AI development. Especially in high-stakes fields like healthcare, aligning technical, evidence-based, and ethical practices with forthcoming legal requirements is increasingly urgent. We argue that developers and deployers of AI systems for the medical domain should be proactive and take steps to progressively ensure that such systems, both those currently in use and those being developed or planned, respect the requirements of the AI Act, which has come into force in August 2024. This is necessary if full and effective compliance is to be ensured when the most relevant provisions of the Act become effective (August 2026). The engagement with the AI Act cannot be viewed as a formalistic exercise. Compliance with the AI Act needs to be carried out through the proactive commitment to the ethical principles of trustworthy AI. These principles provide the background for the Act, which mentions them several times and connects them to the protection of public interest. They can be used to interpret and apply the Act's provisions and to identify good practices, increasing the validity and sustainability of AI systems over time. 

**Abstract (ZH)**: 负责任人工智能发展中对可信性的评估已成为基石。特别是在像医疗这样高风险的领域，技术、基于证据的方法和伦理实践与即将出台的法律要求保持一致变得越来越紧迫。我们认为，为了确保在2024年8月生效的《人工智能法案》的相关规定于2026年8月生效时全面且有效地遵守，医疗领域的人工智能系统开发者和部署者应当积极行动，并逐步确保当前使用及正在开发或计划中的系统遵守该法案的要求。与《人工智能法案》的互动绝不能被视为一种形式上的努力。遵守《人工智能法案》需要通过积极承诺可信人工智能的伦理原则来实现。这些原则构成了法案的背景，法案多次提及这些原则，并将它们与公共利益的保护联系起来。它们可以用来解释和应用法案的各项规定，并识别良好的实践做法，从而随着时间的推移增强和维持人工智能系统的有效性与可持续性。 

---
# Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy 

**Title (ZH)**: 使用注释者指令辅助提示评估金融情绪分析：增强上下文解释和股票预测准确性 

**Authors**: A M Muntasir Rahman, Ajim Uddin, Guiling "Grace" Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.07871)  

**Abstract**: Financial sentiment analysis (FSA) presents unique challenges to LLMs that surpass those in typical sentiment analysis due to the nuanced language used in financial contexts. The prowess of these models is often undermined by the inherent subjectivity of sentiment classifications in existing benchmark datasets like Financial Phrasebank. These datasets typically feature undefined sentiment classes that reflect the highly individualized perspectives of annotators, leading to significant variability in annotations. This variability results in an unfair expectation for LLMs during benchmarking, where they are tasked to conjecture the subjective viewpoints of human annotators without sufficient context. In this paper, we introduce the Annotators' Instruction Assisted Prompt, a novel evaluation prompt designed to redefine the task definition of FSA for LLMs. By integrating detailed task instructions originally intended for human annotators into the LLMs' prompt framework, AIAP aims to standardize the understanding of sentiment across both human and machine interpretations, providing a fair and context-rich foundation for sentiment analysis. We utilize a new dataset, WSBS, derived from the WallStreetBets subreddit to demonstrate how AIAP significantly enhances LLM performance by aligning machine operations with the refined task definitions. Experimental results demonstrate that AIAP enhances LLM performance significantly, with improvements up to 9.08. This context-aware approach not only yields incremental gains in performance but also introduces an innovative sentiment-indexing method utilizing model confidence scores. This method enhances stock price prediction models and extracts more value from the financial sentiment analysis, underscoring the significance of WSB as a critical source of financial text. Our research offers insights into both improving FSA through better evaluation methods. 

**Abstract (ZH)**: 金融情感分析（FSA）对LLM提出的挑战超越了常规情感分析，因为金融语境中使用了更为细致的语言。这些模型的性能往往被现有基准数据集如Financial Phrasebank中固有的主观性所削弱。这些数据集通常包含了未定义的情感类别，反映了注释员的高度个性化视角，导致注释结果的大范围变化。这些变化在基准测试中对LLM提出了不公正的期望，使它们需要在缺乏足够上下文的情况下推断人类注释者的主观观点。本文介绍了注释员说明辅助提示（AIAP），这是一种新颖的评估提示，旨在重新定义针对LLM的FSA任务定义。通过将原计划用于人类注释员的任务指令整合到LLM的提示框架中，AIAP旨在标准化人类和机器对情感的理解，为情感分析提供一个公平且富含上下文的基础。我们使用从WallStreetBets子版块推断出的新数据集WSBS来展示AIAP如何通过使机器操作与精炼的任务定义对齐来显著提升LLM性能。实验结果表明，AIAP显著提升了LLM的性能，性能提升可达9.08。这种基于上下文的方法不仅在性能上取得增量提升，还引入了利用模型置信度分数的情感索引方法，这增强了股票价格预测模型并从金融情感分析中提取更多价值，凸显了WSB作为关键金融文本来源的重要性。我们的研究为通过更好的评估方法改进FSA提供了见解。 

---
# Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection 

**Title (ZH)**: 大型语言模型中高效公平性测试：偏见检测中优先考虑元变换关系 

**Authors**: Suavis Giramata, Madhusudan Srinivasan, Venkat Naidu Gudivada, Upulee Kanewala  

**Link**: [PDF](https://arxiv.org/pdf/2505.07870)  

**Abstract**: Large Language Models (LLMs) are increasingly deployed in various applications, raising critical concerns about fairness and potential biases in their outputs. This paper explores the prioritization of metamorphic relations (MRs) in metamorphic testing as a strategy to efficiently detect fairness issues within LLMs. Given the exponential growth of possible test cases, exhaustive testing is impractical; therefore, prioritizing MRs based on their effectiveness in detecting fairness violations is crucial. We apply a sentence diversity-based approach to compute and rank MRs to optimize fault detection. Experimental results demonstrate that our proposed prioritization approach improves fault detection rates by 22% compared to random prioritization and 12% compared to distance-based prioritization, while reducing the time to the first failure by 15% and 8%, respectively. Furthermore, our approach performs within 5% of fault-based prioritization in effectiveness, while significantly reducing the computational cost associated with fault labeling. These results validate the effectiveness of diversity-based MR prioritization in enhancing fairness testing for LLMs. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各种应用中的部署引发了对其输出公平性和潜在偏差的关键关注。本文探讨了在元变测试中优先考虑元变关系（MRs）作为高效检测LLMs公平性问题的策略。鉴于可能的测试案例呈指数增长，穷尽测试不切实际；因此，基于其检测公平性违规的有效性来优先考虑MRs至关重要。我们采用基于句子多样性的方法来计算和排名MRs，以优化故障检测。实验结果表明，与随机优先级相比，我们的提议优先级方法可将故障检测率提高22%，与基于距离的优先级相比提高12%，同时将第一个失败的时间减少15%和8%。此外，我们的方法在有效性上与基于故障的优先级相差不到5%，但显著降低了与故障标签相关的计算成本。这些结果验证了基于多样性的MR优先级在增强LLMs公平性测试方面的有效性。 

---
# Computationally Efficient Diffusion Models in Medical Imaging: A Comprehensive Review 

**Title (ZH)**: 医学成像中计算高效扩散模型：综述 

**Authors**: Abdullah, Tao Huang, Ickjai Lee, Euijoon Ahn  

**Link**: [PDF](https://arxiv.org/pdf/2505.07866)  

**Abstract**: The diffusion model has recently emerged as a potent approach in computer vision, demonstrating remarkable performances in the field of generative artificial intelligence. Capable of producing high-quality synthetic images, diffusion models have been successfully applied across a range of applications. However, a significant challenge remains with the high computational cost associated with training and generating these models. This study focuses on the efficiency and inference time of diffusion-based generative models, highlighting their applications in both natural and medical imaging. We present the most recent advances in diffusion models by categorizing them into three key models: the Denoising Diffusion Probabilistic Model (DDPM), the Latent Diffusion Model (LDM), and the Wavelet Diffusion Model (WDM). These models play a crucial role in medical imaging, where producing fast, reliable, and high-quality medical images is essential for accurate analysis of abnormalities and disease diagnosis. We first investigate the general framework of DDPM, LDM, and WDM and discuss the computational complexity gap filled by these models in natural and medical imaging. We then discuss the current limitations of these models as well as the opportunities and future research directions in medical imaging. 

**Abstract (ZH)**: 扩散模型 recently emerged as a powerful approach in计算机视觉，展示了生成人工智能领域令人瞩目的性能。能够生成高质量的合成图像，扩散模型已成功应用于多种应用中。然而，与训练和生成这些模型相关的高计算成本仍是一项重大挑战。本研究着眼于基于扩散的生成模型的效率和推断时间，强调其在自然和医学成像中的应用。我们通过将这些模型分类为三种关键模型——去噪扩散概率模型（DDPM）、潜在扩散模型（LDM）和小波扩散模型（WDM）——来概述最近在扩散模型方面的进展。这些模型在医学成像中扮演着重要角色，因为快速、可靠且高质量的医学图像生产对于准确分析异常和疾病诊断至关重要。我们首先探讨了DDPM、LDM和WDM的通用框架，并讨论了这些模型在自然和医学成像中填补的计算复杂性差距。然后，我们讨论了这些模型当前的局限性以及在医学成像中的机会和未来的研究方向。 

---
# CellVerse: Do Large Language Models Really Understand Cell Biology? 

**Title (ZH)**: CellVerse: 大型语言模型真的理解细胞生物学吗？ 

**Authors**: Fan Zhang, Tianyu Liu, Zhihong Zhu, Hao Wu, Haixin Wang, Donghao Zhou, Yefeng Zheng, Kun Wang, Xian Wu, Pheng-Ann Heng  

**Link**: [PDF](https://arxiv.org/pdf/2505.07865)  

**Abstract**: Recent studies have demonstrated the feasibility of modeling single-cell data as natural languages and the potential of leveraging powerful large language models (LLMs) for understanding cell biology. However, a comprehensive evaluation of LLMs' performance on language-driven single-cell analysis tasks still remains unexplored. Motivated by this challenge, we introduce CellVerse, a unified language-centric question-answering benchmark that integrates four types of single-cell multi-omics data and encompasses three hierarchical levels of single-cell analysis tasks: cell type annotation (cell-level), drug response prediction (drug-level), and perturbation analysis (gene-level). Going beyond this, we systematically evaluate the performance across 14 open-source and closed-source LLMs ranging from 160M to 671B on CellVerse. Remarkably, the experimental results reveal: (1) Existing specialist models (C2S-Pythia) fail to make reasonable decisions across all sub-tasks within CellVerse, while generalist models such as Qwen, Llama, GPT, and DeepSeek family models exhibit preliminary understanding capabilities within the realm of cell biology. (2) The performance of current LLMs falls short of expectations and has substantial room for improvement. Notably, in the widely studied drug response prediction task, none of the evaluated LLMs demonstrate significant performance improvement over random guessing. CellVerse offers the first large-scale empirical demonstration that significant challenges still remain in applying LLMs to cell biology. By introducing CellVerse, we lay the foundation for advancing cell biology through natural languages and hope this paradigm could facilitate next-generation single-cell analysis. 

**Abstract (ZH)**: Recent studies have demonstrated the feasibility of modeling single-cell data as natural languages and the potential of leveraging powerful large language models (LLMs) for understanding cell biology. However, a comprehensive evaluation of LLMs' performance on language-driven single-cell analysis tasks still remains unexplored. Motivated by this challenge, we introduce CellVerse, a unified language-centric question-answering benchmark that integrates four types of single-cell multi-omics data and encompasses three hierarchical levels of single-cell analysis tasks: cell type annotation (cell-level), drug response prediction (drug-level), and perturbation analysis (gene-level). 

---
# Scalable LLM Math Reasoning Acceleration with Low-rank Distillation 

**Title (ZH)**: 可扩展的大语言模型数学推理加速方法：低秩蒸馏 

**Authors**: Harry Dong, Bilge Acun, Beidi Chen, Yuejie Chi  

**Link**: [PDF](https://arxiv.org/pdf/2505.07861)  

**Abstract**: Due to long generations, large language model (LLM) math reasoning demands significant computational resources and time. While many existing efficient inference methods have been developed with excellent performance preservation on language tasks, they often severely degrade math performance. In this paper, we propose Caprese, a low-cost distillation method to recover lost capabilities from deploying efficient inference methods, focused primarily in feedforward blocks. With original weights unperturbed, roughly 1% of additional parameters, and only 20K synthetic training samples, we are able to recover much if not all of the math capabilities lost from efficient inference for thinking LLMs and without harm to language tasks for instruct LLMs. Moreover, Caprese slashes the number of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and integrates cleanly into existing model layers to reduce latency (>11% reduction to generate 2048 tokens with Qwen 2.5 14B) while encouraging response brevity. 

**Abstract (ZH)**: 由于龐大的架構，大型語言模型的數學推理需要大量的計算資源和時間。雖然已經開發了許多高效推理方法，在語言任務上保持了出色的性能，但這些方法往往會嚴重降低數學性能。在本文中，我們提出了一種低 COST 的蒸�子方法 Caprese，以從高效推理方法的部署中恢復丟失的能力，主要集中在前向塊上。通過保留原始權重不被打擾、僅增加約 1% 的參數，并使用約 20K 合成訓練樣本，我們能夠恢復高效推理對思考型大語言模型失去的大量甚至全部數學能力，同時不會損害指令型大語言模型的語言任務性能。此外，Caprese 能夠顯著減少活躍參數數量（Gemma 2 9B 約減 2B，Llama 3.1 8B 當量）並平滑地集成到現有模型層中以減低 latency（使用 Qwen 2.5 14B 則生成 2048 個詞元的速度可減少大於 11%），同時鼓勵簡潔的回應。 

---
# Boosting Performance on ARC is a Matter of Perspective 

**Title (ZH)**: 提升ARC性能取决于视角 

**Authors**: Daniel Franzen, Jan Disselhoff, David Hartmann  

**Link**: [PDF](https://arxiv.org/pdf/2505.07859)  

**Abstract**: The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge for large language models (LLMs), exposing limitations in their abstract reasoning abilities. In this work, we leverage task-specific data augmentations throughout the training, generation, and scoring phases, and employ a depth-first search algorithm to generate diverse, high-probability candidate solutions. Furthermore, we utilize the LLM not only as a generator but also as a scorer, using its output probabilities to select the most promising solutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the public ARC-AGI evaluation set, demonstrating state-of-the-art performance among publicly available approaches. While concurrent closed-source work has reported higher scores, our method distinguishes itself through its transparency, reproducibility, and remarkably low inference cost, averaging only around 2ct per task on readily available hardware (we assume a price of 36ct/hour for a Nvidia 4090 GPU). 

**Abstract (ZH)**: ARC-AGI抽象和推理语料库对大型语言模型提出了显著挑战，暴露了其在抽象推理能力上的局限性。在这项工作中，我们通过在训练、生成和评分阶段利用任务特定的数据增强，并采用深度优先搜索算法生成多样性和高概率候选解决方案。此外，我们不仅将LLM用作生成器，还用作评分器，利用其输出概率选择最有潜力的解决方案。我们的方法在公开的ARC-AGI评估集上取得了71.6%的分数（总共解决了400个任务中的286.5个），展示了公开可用方法中的最先进性能。尽管同期的闭源工作报告了更高的分数，但我们的方法通过其透明性、可重复性和极低的推理成本脱颖而出，平均每个任务仅需约2ct（假设使用Nvidia 4090 GPU的价格为每小时36ct）。 

---
# Scaling Laws for Speculative Decoding 

**Title (ZH)**: 推测性解码的标度律 

**Authors**: Siyuan Yan, Mo Zhu, Guo-qing Jiang, Jianfei Wang, Jiaxing Chen, Wentai Zhang, Xiang Liao, Xiao Cui, Chen Zhang, Zhuoran Song, Ran Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2505.07858)  

**Abstract**: The escalating demand for efficient decoding in large language models (LLMs) is particularly critical for reasoning-intensive architectures like OpenAI-o3 and DeepSeek-R1, which depend on extended chain-of-thought reasoning. This study investigates speculative decoding techniques through dense LLM architectures to establish foundational insights for accelerating reasoning tasks. While speculative decoding methods leveraging parallel draft-verification cycles have emerged as promising acceleration techniques, the scaling laws governing decoding efficiency remain under-explored compared to conventional backbone LLMs developed through Pretraining->SFT->RLHF training paradigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2 and 1.3) governing draft model acceptance rate (or decoding speed) across three dimensions: pretraining token volume, draft model capacity, and decoding batch size. Building on these laws, we achieve Scylla, which coordinates multi-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical validation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and 0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on summarization and QA tasks (Figure 2). Industrial inference engine deployments demonstrate 2X decoding throughput improvements over EAGLE2 (Table 5), validating the transformative potential of systematic scaling for efficient LLM inference. Code will be released later. 

**Abstract (ZH)**: 大型语言模型中高效解码需求的上升特别关键，对于依赖于扩展链式推理的推理密集型架构如OpenAI-o3和DeepSeek-R1尤为重要。本研究通过密集的语言模型架构探索推测性解码技术，以建立加速推理任务的基础洞察。虽然利用并行草稿验证循环的推测性解码方法已成为有望的加速技术，但解码效率的标度定律与传统的通过预训练->精细调优->人类反馈强化学习训练范式开发的骨干语言模型相比，仍有待深入探索。在本文中，我们发现了跨越预训练 token 体积、草稿模型容量和解码批次大小三个维度的对数线性标度定律（定理1.1、1.2和1.3）。基于这些定律，我们实现了一种协调多维度标度的技术Scylla，适用于流行的语言模型（Llama2/3、Qwen2.5）。实证验证显示，与EAGLE2相比，Scylla在温度T=0时的接受率提高了1.5-2.2倍，在EAGLE3上的接受率高0.3倍，在总结和问答任务上达到了最高的性能提升（图2）。工业推理引擎部署表明，Scylla相比于EAGLE2的解码吞吐量提高了2倍（表5），验证了系统性标度对高效语言模型推理的变革潜力。代码稍后发布。 

---
# Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines 

**Title (ZH)**: 使用大型语言模型和原型驱动的预测管道增强乌尔都语意图检测 

**Authors**: Faiza Hassan, Summra Saleem, Kashif Javed, Muhammad Nabeel Asim, Abdur Rehman, Andreas Dengel  

**Link**: [PDF](https://arxiv.org/pdf/2505.07857)  

**Abstract**: Multifarious intent detection predictors are developed for different languages, including English, Chinese and French, however, the field remains underdeveloped for Urdu, the 10th most spoken language. In the realm of well-known languages, intent detection predictors utilize the strategy of few-shot learning and prediction of unseen classes based on the model training on seen classes. However, Urdu language lacks few-shot strategy based intent detection predictors and traditional predictors are focused on prediction of the same classes which models have seen in the train set. To empower Urdu language specific intent detection, this introduces a unique contrastive learning approach that leverages unlabeled Urdu data to re-train pre-trained language models. This re-training empowers LLMs representation learning for the downstream intent detection task. Finally, it reaps the combined potential of pre-trained LLMs and the prototype-informed attention mechanism to create a comprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigm of proposed predictive pipeline, it explores the potential of 6 distinct language models and 13 distinct similarity computation methods. The proposed framework is evaluated on 2 public benchmark datasets, namely ATIS encompassing 5836 samples and Web Queries having 8519 samples. Across ATIS dataset under 4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and 98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score, respectively. In an additional case study on the Web Queries dataset under same classes train and test set settings, LLMPIA outperformed state-of-the-art predictor by 53.55% F1-Score. 

**Abstract (ZH)**: 面向乌尔都语的多意图检测预测器开发：一种基于对比学习的方法 

---
# Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights 

**Title (ZH)**: 剖析屈折语中的稳健性：对抗性评估与机制性洞察 

**Authors**: Paweł Walkowiak, Marek Klonowski, Marcin Oleksy, Arkadiusz Janz  

**Link**: [PDF](https://arxiv.org/pdf/2505.07856)  

**Abstract**: Various techniques are used in the generation of adversarial examples, including methods such as TextBugger which introduce minor, hardly visible perturbations to words leading to changes in model behaviour. Another class of techniques involves substituting words with their synonyms in a way that preserves the text's meaning but alters its predicted class, with TextFooler being a prominent example of such attacks. Most adversarial example generation methods are developed and evaluated primarily on non-inflectional languages, typically English. In this work, we evaluate and explain how adversarial attacks perform in inflectional languages. To explain the impact of inflection on model behaviour and its robustness under attack, we designed a novel protocol inspired by mechanistic interpretability, based on Edge Attribution Patching (EAP) method. The proposed evaluation protocol relies on parallel task-specific corpora that include both inflected and syncretic variants of texts in two languages -- Polish and English. To analyse the models and explain the relationship between inflection and adversarial robustness, we create a new benchmark based on task-oriented dataset MultiEmo, enabling the identification of mechanistic inflection-related elements of circuits within the model and analyse their behaviour under attack. 

**Abstract (ZH)**: 各种技术被用于生成对抗样本，包括向文本中引入细微、几乎不可见的扰动以改变模型行为的TextBugger方法。另一类技术涉及用同义词替换单词，以保持文本意义但改变其预测类别，TextFooler是此类攻击的一个典型案例。大多数生成对抗样本的方法主要在非屈折语，通常是英语上进行开发和评估。在本研究中，我们评估并解释了对抗攻击在屈折语上的表现。为了解释屈折变化对模型行为的影响及其在攻击下的鲁棒性，我们设计了一种基于Edge Attribution Patching (EAP) 方法的新协议，该协议借鉴了机制可解释性的理念。所提出的评估协议依赖于双语言（波兰语和英语）平行的专用语料库，该语料库包含文本的屈折变化和合形变化形式。为了分析模型并解释屈折与对抗鲁棒性之间的关系，我们基于面向任务的数据集MultiEmo创建了一个新的基准，该基准能够识别模型内部与屈折相关的基本机制元素，并分析其在攻击下的行为。 

---
# CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis 

**Title (ZH)**: CrashSage: 一种以大型语言模型为中心的上下文可解释道路交通碰撞分析框架 

**Authors**: Hao Zhen, Jidong J. Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.07853)  

**Abstract**: Road crashes claim over 1.3 million lives annually worldwide and incur global economic losses exceeding \$1.8 trillion. Such profound societal and financial impacts underscore the urgent need for road safety research that uncovers crash mechanisms and delivers actionable insights. Conventional statistical models and tree ensemble approaches typically rely on structured crash data, overlooking contextual nuances and struggling to capture complex relationships and underlying semantics. Moreover, these approaches tend to incur significant information loss, particularly in narrative elements related to multi-vehicle interactions, crash progression, and rare event characteristics. This study presents CrashSage, a novel Large Language Model (LLM)-centered framework designed to advance crash analysis and modeling through four key innovations. First, we introduce a tabular-to-text transformation strategy paired with relational data integration schema, enabling the conversion of raw, heterogeneous crash data into enriched, structured textual narratives that retain essential structural and relational context. Second, we apply context-aware data augmentation using a base LLM model to improve narrative coherence while preserving factual integrity. Third, we fine-tune the LLaMA3-8B model for crash severity inference, demonstrating superior performance over baseline approaches, including zero-shot, zero-shot with chain-of-thought prompting, and few-shot learning, with multiple models (GPT-4o, GPT-4o-mini, LLaMA3-70B). Finally, we employ a gradient-based explainability technique to elucidate model decisions at both the individual crash level and across broader risk factor dimensions. This interpretability mechanism enhances transparency and enables targeted road safety interventions by providing deeper insights into the most influential factors. 

**Abstract (ZH)**: 基于大型语言模型的交通碰撞分析与建模新框架：CrashSage 

---
# Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment 

**Title (ZH)**: 使用LLM辅助判断的在线对话中欺诈和概念漂移的联合检测 

**Authors**: Ali Senol, Garima Agrawal, Huan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.07852)  

**Abstract**: Detecting fake interactions in digital communication platforms remains a challenging and insufficiently addressed problem. These interactions may appear as harmless spam or escalate into sophisticated scam attempts, making it difficult to flag malicious intent early. Traditional detection methods often rely on static anomaly detection techniques that fail to adapt to dynamic conversational shifts. One key limitation is the misinterpretation of benign topic transitions referred to as concept drift as fraudulent behavior, leading to either false alarms or missed threats. We propose a two stage detection framework that first identifies suspicious conversations using a tailored ensemble classification model. To improve the reliability of detection, we incorporate a concept drift analysis step using a One Class Drift Detector (OCDD) to isolate conversational shifts within flagged dialogues. When drift is detected, a large language model (LLM) assesses whether the shift indicates fraudulent manipulation or a legitimate topic change. In cases where no drift is found, the behavior is inferred to be spam like. We validate our framework using a dataset of social engineering chat scenarios and demonstrate its practical advantages in improving both accuracy and interpretability for real time fraud detection. To contextualize the trade offs, we compare our modular approach against a Dual LLM baseline that performs detection and judgment using different language models. 

**Abstract (ZH)**: 在数字通信平台上检测虚假互动仍是一个具有挑战性和未充分解决的问题。这些互动可能表现为无害的垃圾信息，也可能升级为复杂的欺诈尝试，使得及早识别恶意意图变得困难。传统检测方法通常依赖静态异常检测技术，这些技术难以适应动态对话的变化。一个关键限制是将良性话题转换误解读为欺诈行为，导致虚假警报或遗漏威胁。我们提出了一种两阶段检测框架，首先使用定制的集成分类模型识别可疑对话。为了提高检测的可靠性，我们引入了一类漂移分析步骤，使用One Class Drift Detector (OCDD) 来隔离标记对话中的对话变化。当检测到漂移时，大规模语言模型（LLM）评估该变化是否表示欺诈操纵或合法话题变化。在未发现漂移的情况下，行为被认为是类似垃圾信息的。我们使用社交工程聊天场景数据集验证了我们的框架，并展示了其在实时欺诈检测中提高准确性和可解释性的实用优势。为了阐述权衡关系，我们将我们的模块化方法与使用不同语言模型进行检测和判断的Dual LLM基线进行了比较。 

---
# Pose Estimation for Intra-cardiac Echocardiography Catheter via AI-Based Anatomical Understanding 

**Title (ZH)**: 基于AI解剖理解的经腔内超声心动图导管姿态估计 

**Authors**: Jaeyoung Huh, Ankur Kapoor, Young-Ho Kim  

**Link**: [PDF](https://arxiv.org/pdf/2505.07851)  

**Abstract**: Intra-cardiac Echocardiography (ICE) plays a crucial role in Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by providing high-resolution, real-time imaging of cardiac structures. However, existing navigation methods rely on electromagnetic (EM) tracking, which is susceptible to interference and position drift, or require manual adjustments based on operator expertise. To overcome these limitations, we propose a novel anatomy-aware pose estimation system that determines the ICE catheter position and orientation solely from ICE images, eliminating the need for external tracking sensors. Our approach leverages a Vision Transformer (ViT)-based deep learning model, which captures spatial relationships between ICE images and anatomical structures. The model is trained on a clinically acquired dataset of 851 subjects, including ICE images paired with position and orientation labels normalized to the left atrium (LA) mesh. ICE images are patchified into 16x16 embeddings and processed through a transformer network, where a [CLS] token independently predicts position and orientation via separate linear layers. The model is optimized using a Mean Squared Error (MSE) loss function, balancing positional and orientational accuracy. Experimental results demonstrate an average positional error of 9.48 mm and orientation errors of (16.13 deg, 8.98 deg, 10.47 deg) across x, y, and z axes, confirming the model accuracy. Qualitative assessments further validate alignment between predicted and target views within 3D cardiac meshes. This AI-driven system enhances procedural efficiency, reduces operator workload, and enables real-time ICE catheter localization for tracking-free procedures. The proposed method can function independently or complement existing mapping systems like CARTO, offering a transformative approach to ICE-guided interventions. 

**Abstract (ZH)**: 基于ICE图像的解剖感知姿态估计系统在心内电生理和结构性心脏病介入中的应用 

---
# A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas 

**Title (ZH)**: 两种身份的故事：人类与AI创构的人设伦理审查 

**Authors**: Pranav Narayanan Venkit, Jiayi Li, Yingfan Zhou, Sarah Rajtmajer, Shomir Wilson  

**Link**: [PDF](https://arxiv.org/pdf/2505.07850)  

**Abstract**: As LLMs (large language models) are increasingly used to generate synthetic personas particularly in data-limited domains such as health, privacy, and HCI, it becomes necessary to understand how these narratives represent identity, especially that of minority communities. In this paper, we audit synthetic personas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the lens of representational harm, focusing specifically on racial identity. Using a mixed methods approach combining close reading, lexical analysis, and a parameterized creativity framework, we compare 1512 LLM generated personas to human-authored responses. Our findings reveal that LLMs disproportionately foreground racial markers, overproduce culturally coded language, and construct personas that are syntactically elaborate yet narratively reductive. These patterns result in a range of sociotechnical harms, including stereotyping, exoticism, erasure, and benevolent bias, that are often obfuscated by superficially positive narrations. We formalize this phenomenon as algorithmic othering, where minoritized identities are rendered hypervisible but less authentic. Based on these findings, we offer design recommendations for narrative-aware evaluation metrics and community-centered validation protocols for synthetic identity generation. 

**Abstract (ZH)**: 随着大规模语言模型（LLMs）在健康、隐私和人机交互等数据有限领域中越来越多地用于生成合成人设，理解这些叙述如何代表身份，特别是少数社区的身份，变得至关重要。本文通过代表性伤害的视角审计3个LLM（GPT4o、Gemini 1.5 Pro、Deepseek 2.5）生成的合成人设，重点关注种族身份。采用结合精密阅读、词汇分析和参数化创造力框架的混合方法，我们将1512个LLM生成的人设与人类撰写的回应进行比较。研究发现，这些模型在种族标记方面过度强调，在文化编码语言方面过度生产，并构建了句法复杂但叙事简化的个体形象。这些模式导致一系列社会技术伤害，包括刻板印象、异文化浪漫化、抹除和善意偏见，这些伤害往往被表面上积极的叙述所掩盖。我们这一现象正式化为算法异化，其中边缘化身份被过度可见但缺乏真实性。基于这些发现，我们提出了叙事意识评估指标和以社区为中心的合成身份生成验证协议的设计建议。 

---
# SweRank: Software Issue Localization with Code Ranking 

**Title (ZH)**: SweRank: 代码排名驱动的软件问题定位 

**Authors**: Revanth Gangi Reddy, Tarun Suresh, JaeHyeok Doo, Ye Liu, Xuan Phi Nguyen, Yingbo Zhou, Semih Yavuz, Caiming Xiong, Heng Ji, Shafiq Joty  

**Link**: [PDF](https://arxiv.org/pdf/2505.07849)  

**Abstract**: Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approaches demonstrate promise, they often incur significant latency and cost due to complex multi-step reasoning and relying on closed-source LLMs. Alternatively, traditional code ranking models, typically optimized for query-to-code or code-to-code retrieval, struggle with the verbose and failure-descriptive nature of issue localization queries. To bridge this gap, we introduce SweRank, an efficient and effective retrieve-and-rerank framework for software issue localization. To facilitate training, we construct SweLoc, a large-scale dataset curated from public GitHub repositories, featuring real-world issue descriptions paired with corresponding code modifications. Empirical results on SWE-Bench-Lite and LocBench show that SweRank achieves state-of-the-art performance, outperforming both prior ranking models and costly agent-based systems using closed-source LLMs like Claude-3.5. Further, we demonstrate SweLoc's utility in enhancing various existing retriever and reranker models for issue localization, establishing the dataset as a valuable resource for the community. 

**Abstract (ZH)**: 软件问题定位是指识别与自然语言问题描述（如 bug 报告、功能请求）相关的精确代码位置（文件、类或函数）的任务，是软件开发中至关重要但耗费大量时间的方面。尽管基于大语言模型的代理方法显示出潜力，但由于复杂的多步推理和依赖于闭源的大语言模型，它们常常会带来显著的延迟和成本。相反，传统的代码排名模型，通常优化用于查询到代码或代码到代码的检索，难以应对涉及详细故障描述的问题定位查询。为了弥合这一差距，我们提出了 SweRank，这是一种高效的检索和重排序框架，用于软件问题定位。为了便于训练，我们从公共 GitHub 存储库中构建了 SweLoc，该数据集包含真实世界的 Issue 描述及其对应的代码修改配对。在 SWE-Bench-Lite 和 LocBench 上的实验证明，SweRank 达到了最先进的性能，优于先前的排名模型以及使用闭源大语言模型（如 Claude-3.5）的成本高昂的代理系统。此外，我们展示了 SweLoc 在增强各种现有的检索和重排序模型以进行问题定位方面的实用性，确立了该数据集作为社区中的宝贵资源的地位。 

---
# Sub-diffraction terahertz backpropagation compressive imaging 

**Title (ZH)**: 亚衍射极限太赫兹反传播压缩成像 

**Authors**: Yongsheng Zhu, Shaojing Liu, Ximiao Wang, Runli Li, Haili Yang, Jiali Wang, Hongjia Zhu, Yanlin Ke, Ningsheng Xu, Huanjun Chen, Shaozhi Deng  

**Link**: [PDF](https://arxiv.org/pdf/2505.07839)  

**Abstract**: Terahertz single-pixel imaging (TSPI) has garnered significant attention due to its simplicity and cost-effectiveness. However, the relatively long wavelength of THz waves limits sub-diffraction-scale imaging resolution. Although TSPI technique can achieve sub-wavelength resolution, it requires harsh experimental conditions and time-consuming processes. Here, we propose a sub-diffraction THz backpropagation compressive imaging technique. We illuminate the object with monochromatic continuous-wave THz radiation. The transmitted THz wave is modulated by prearranged patterns generated on the back surface of a 500-{\mu}m-thick silicon wafer, realized through photoexcited carriers using a 532-nm laser. The modulated THz wave is then recorded by a single-element detector. An untrained neural network is employed to iteratively reconstruct the object image with an ultralow compression ratio of 1.5625% under a physical model constraint, thus reducing the long sampling times. To further suppress the diffraction-field effects, embedded with the angular spectrum propagation (ASP) theory to model the diffraction of THz waves during propagation, the network retrieves near-field information from the object, enabling sub-diffraction imaging with a spatial resolution of ~{\lambda}0/7 ({\lambda}0 = 833.3 {\mu}m at 0.36 THz) and eliminating the need for ultrathin photomodulators. This approach provides an efficient solution for advancing THz microscopic imaging and addressing other inverse imaging challenges. 

**Abstract (ZH)**: 太赫兹单像素成像 (TSPI) 由于其简单性和经济性而引起了广泛关注。然而，太赫兹波相对较长的波长限制了其亚衍射分辨能力。尽管TSPI技术可以实现亚波长分辨率，但需要严苛的实验条件和耗时的过程。在此，我们提出了一种亚衍射太赫兹反向传播压缩成像技术。我们用单色连续波太赫兹辐射照射物体。透过硅片背面预设模式调制的太赫兹波由532 nm激光激发载流子实现。调制后的太赫兹波由单像素探测器记录。在物理模型约束下，使用未训练的神经网络以超低压缩比（1.5625%）迭代重构物体图像，从而减少长时间的采样。为进一步抑制衍射场效应，结合入射角谱传播（ASP）理论来建模太赫兹波传播过程中的衍射，网络从物体中检索近场信息，从而实现约λ0/7（λ0 = 833.3 μm，频率为0.36 THz）的空间分辨率的亚衍射成像，无需超薄光调制器。该方法为推进太赫兹显微成像和解决其他逆向成像挑战提供了有效解决方案。 

---
# Moving From Monolithic To Microservices Architecture for Multi-Agent Systems 

**Title (ZH)**: 从单体架构向微服务架构迁移以应用于多代理系统 

**Authors**: Muskaan Goyal, Pranav Bhasin  

**Link**: [PDF](https://arxiv.org/pdf/2505.07838)  

**Abstract**: The transition from monolithic to microservices architecture revolutionized software development by improving scalability and maintainability. This paradigm shift is now becoming relevant for complex multi-agent systems (MAS). This review article explores the evolution from monolithic architecture to microservices architecture in the specific context of MAS. It will highlight the limitations of traditional monolithic MAS and the benefits of adopting a microservices-based approach. The article further examines the core architectural principles and communication protocols, including Agent Communication Languages (ACLs), the Model Context Protocol (MCP), and the Application-to-Application (A2A) protocol. The article identifies emerging architectural patterns, design challenges, and considerations through a comparative lens of the paradigm shift. 

**Abstract (ZH)**: 从-monolithic-到-microservices-架构的过渡 revolutionized 软件开发，通过提高可扩展性和可维护性。这一范式转变现在对于复杂的多代理系统（MAS）变得 relevant。本文综述了从-monolithic-架构到-microservices-架构在特定的 MAS 上的演变。它将突出传统-monolithic-MAS 的局限性以及采用基于-microservices-的方法的好处。文章还将进一步探讨核心架构原则和通信协议，包括代理通信语言（ACL）、模型上下文协议（MCP）和应用程序到应用程序（A2A）协议。通过范式转变的比较视角，文章指出了新兴的架构模式、设计挑战和考虑因素。 

---
# Intelligent Product 3.0: Decentralised AI Agents and Web3 Intelligence Standards 

**Title (ZH)**: 智能产品3.0：去中心化AI代理和Web3智能标准 

**Authors**: Alex C. Y. Wong, Duncan McFarlane, C. Ellarby, M. Lee, M. Kuok  

**Link**: [PDF](https://arxiv.org/pdf/2505.07835)  

**Abstract**: Twenty-five years ago, the specification of the Intelligent Product was established, envisaging real-time connectivity that not only enables products to gather accurate data about themselves but also allows them to assess and influence their own destiny. Early work by the Auto-ID project focused on creating a single, open-standard repository for storing and retrieving product information, laying a foundation for scalable connectivity. A decade later, the approach was revisited in light of low-cost RFID systems that promised a low-cost link between physical goods and networked information environments. Since then, advances in blockchain, Web3, and artificial intelligence have introduced unprecedented levels of resilience, consensus, and autonomy. By leveraging decentralised identity, blockchain-based product information and history, and intelligent AI-to-AI collaboration, this paper examines these developments and outlines a new specification for the Intelligent Product 3.0, illustrating how decentralised and AI-driven capabilities facilitate seamless interaction between physical AI and everyday products. 

**Abstract (ZH)**: 二十五年前，设立了智能产品的规范，设想了实时连接，不仅使产品能够收集自身准确的数据，还能评估和影响自身的命运。Auto-ID项目早期的工作关注于创建一个单一的、开放标准的存储库，用于存储和检索产品信息，为可扩展的连接奠定了基础。十年后，随着低成本RFID系统的出现，这一方法根据低成本连接物理商品与网络信息环境的前景进行了重新审视。此后，区块链、Web3和人工智能的进步引入了前所未有的韧性和共识水平。通过利用去中心化身份、基于区块链的产品信息和历史记录，以及智能AI-to-AI协作，本文探讨了这些发展，并提出了智能产品3.0的新规范，展示了去中心化和AI驱动的能力如何使物理AI和日常产品之间的无缝互动成为可能。 

---
# ai.txt: A Domain-Specific Language for Guiding AI Interactions with the Internet 

**Title (ZH)**: 专用于指导AI与互联网交互的领域特定语言 

**Authors**: Yuekang Li, Wei Song, Bangshuo Zhu, Dong Gong, Yi Liu, Gelei Deng, Chunyang Chen, Lei Ma, Jun Sun, Toby Walsh, Jingling Xue  

**Link**: [PDF](https://arxiv.org/pdf/2505.07834)  

**Abstract**: We introduce this http URL, a novel domain-specific language (DSL) designed to explicitly regulate interactions between AI models, agents, and web content, addressing critical limitations of the widely adopted this http URL standard. As AI increasingly engages with online materials for tasks such as training, summarization, and content modification, existing regulatory methods lack the necessary granularity and semantic expressiveness to ensure ethical and legal compliance. this http URL extends traditional URL-based access controls by enabling precise element-level regulations and incorporating natural language instructions interpretable by AI systems. To facilitate practical deployment, we provide an integrated development environment with code autocompletion and automatic XML generation. Furthermore, we propose two compliance mechanisms: XML-based programmatic enforcement and natural language prompt integration, and demonstrate their effectiveness through preliminary experiments and case studies. Our approach aims to aid the governance of AI-Internet interactions, promoting responsible AI use in digital ecosystems. 

**Abstract (ZH)**: 我们介绍这款新的域名特定语言（DSL）：this http URL，这是一种 novel domain-specific language (DSL) 专门设计用于明确调控 AI 模型、代理与网页内容之间的交互，解决广泛采用的 this http URL 标准的关键局限性。随着 AI 在诸如训练、摘要和内容修改等任务中越来越多地与在线材料互动，现有的监管方法缺乏必要的粒度和语义表达能力，以确保伦理和法律合规。this http URL 通过启用精确的元素级监管并结合可由 AI 系统解析的自然语言指令，扩展了传统的基于 URL 的访问控制。为了便于实际部署，我们提供了一个集成了代码自动补全和自动 XML 生成的集成开发环境。此外，我们提出了两种合规机制：基于 XML 的程序化执行和自然语言提示集成，并通过初步实验和案例研究展示了它们的有效性。我们这种方法旨在帮助治理 AI-互联网交互，促进数字生态系统中负责任的 AI 使用。 

---
# Patchwork: A Unified Framework for RAG Serving 

**Title (ZH)**: Patchwork: 统一的RAG服务框架 

**Authors**: Bodun Hu, Luis Pabon, Saurabh Agarwal, Aditya Akella  

**Link**: [PDF](https://arxiv.org/pdf/2505.07833)  

**Abstract**: Retrieval Augmented Generation (RAG) has emerged as a new paradigm for enhancing Large Language Model reliability through integration with external knowledge sources. However, efficient deployment of these systems presents significant technical challenges due to their inherently heterogeneous computational pipelines comprising LLMs, databases, and specialized processing components. We introduce Patchwork, a comprehensive end-to-end RAG serving framework designed to address these efficiency bottlenecks. Patchwork's architecture offers three key innovations: First, it provides a flexible specification interface enabling users to implement custom RAG pipelines. Secondly, it deploys these pipelines as distributed inference systems while optimizing for the unique scalability characteristics of individual RAG components. Third, Patchwork incorporates an online scheduling mechanism that continuously monitors request load and execution progress, dynamically minimizing SLO violations through strategic request prioritization and resource auto-scaling. Our experimental evaluation across four distinct RAG implementations demonstrates that Patchwork delivers substantial performance improvements over commercial alternatives, achieving throughput gains exceeding 48% while simultaneously reducing SLO violations by ~24%. 

**Abstract (ZH)**: 检索增强生成（RAG）已经 emergence 为一种通过结合外部知识源来增强大型语言模型可靠性的新范式。然而，这些系统的高效部署由于其本质上异构的计算流水线（包括LLMs、数据库和专用处理组件）而面临显著的技术挑战。我们引入了Patchwork，一种全面的端到端RAG服务框架，旨在解决这些效率瓶颈。Patchwork的架构提供了三项关键创新：首先，它提供了一个灵活的规范接口，使用户能够实现自定义的RAG流水线。其次，它以优化特定RAG组件的独特扩展性能的方式部署这些流水线。第三，Patchwork整合了一个在线调度机制，持续监测请求负载和执行进度，通过战略性地请求优先级调整和资源自动扩展，动态地最小化SLO违反。我们的实验评估表明，Patchwork在四种不同的RAG实现中实现了显著的性能提升，与商业替代方案相比，吞吐量提升超过48%，同时将SLO违反率降低约24%。 

---
# A General Approach of Automated Environment Design for Learning the Optimal Power Flow 

**Title (ZH)**: 一种自动环境设计的一般方法以学习最优功率流 

**Authors**: Thomas Wolgast, Astrid Nieße  

**Link**: [PDF](https://arxiv.org/pdf/2505.07832)  

**Abstract**: Reinforcement learning (RL) algorithms are increasingly used to solve the optimal power flow (OPF) problem. Yet, the question of how to design RL environments to maximize training performance remains unanswered, both for the OPF and the general case. We propose a general approach for automated RL environment design by utilizing multi-objective optimization. For that, we use the hyperparameter optimization (HPO) framework, which allows the reuse of existing HPO algorithms and methods. On five OPF benchmark problems, we demonstrate that our automated design approach consistently outperforms a manually created baseline environment design. Further, we use statistical analyses to determine which environment design decisions are especially important for performance, resulting in multiple novel insights on how RL-OPF environments should be designed. Finally, we discuss the risk of overfitting the environment to the utilized RL algorithm. To the best of our knowledge, this is the first general approach for automated RL environment design. 

**Abstract (ZH)**: 利用多目标优化的自动强化学习环境设计方法 

---
# Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces 

**Title (ZH)**: 合成神经元的多义性 toward 新型解释性分类向量空间 

**Authors**: Michael Pichat, William Pogrund, Paloma Pichat, Judicael Poumay, Armanouche Gasparian, Samuel Demarchi, Martin Corbet, Alois Georgeon, Michael Veillet-Guillem  

**Link**: [PDF](https://arxiv.org/pdf/2505.07831)  

**Abstract**: The polysemantic nature of synthetic neurons in artificial intelligence language models is currently understood as the result of a necessary superposition of distributed features within the latent space. We propose an alternative approach, geometrically defining a neuron in layer n as a categorical vector space with a non-orthogonal basis, composed of categorical sub-dimensions extracted from preceding neurons in layer n-1. This categorical vector space is structured by the activation space of each neuron and enables, via an intra-neuronal attention process, the identification and utilization of a critical categorical zone for the efficiency of the language model - more homogeneous and located at the intersection of these different categorical sub-dimensions. 

**Abstract (ZH)**: 合成神经元在人工智能语言模型中的多义性目前被认为是由潜在空间中分布特征的必然叠加所致。我们提出一种替代方法，几何上定义第n层的神经元为由第n-1层前馈神经元提取的非正交基组成的类别向量空间。该类别向量空间通过每个神经元的激活空间结构化，并通过一种内在神经元注意过程，识别和利用一个关键类别区域，以提高语言模型的效率——更加 homogenous，并位于这些不同类别子维度的交点处。 

---
# Blockbuster, Part 1: Block-level AI Operator Fusion 

**Title (ZH)**: Blockbuster, Part 1: 块级AI操作融合 

**Authors**: Ofer Dekel  

**Link**: [PDF](https://arxiv.org/pdf/2505.07829)  

**Abstract**: Blockbuster is a framework for AI operator fusion in inference programs. The Blockbuster framework is compatible with any multiprocessor architecture that has a tiered memory hierarchy, including GPUs, multi-core CPUs, and some AI accelerator chips. It includes a graph-based representation for AI workloads, called a block program, which explicitly models how blocks of data move between the memory tiers. It also includes an operator fusion procedure, which is made up of a candidate selection algorithm and a fusion algorithm that fuses each individual candidate - this two-algorithm structure makes Blockbuster especially suitable for large AI programs. The current paper focuses on the fusion algorithm, which is a rule-based technique. While the literature is full of previous rule-based fusion algorithms, what sets our algorithm apart is its direct modeling of data movement between memory tiers, resulting in uniquely powerful fusion results. As a first sanity check, we demonstrate how our algorithm automatically rediscovers the well-known Flash Attention kernel. Then, we demonstrate the real power of our approach by fusing LayerNorm with matrix multiplication and RMSNorm with FNN-SwiGLU - the latter involves fusing three matrix multiplications, a Hadamard product, a reduction, and a few elementwise operations into a single mega-kernel. 

**Abstract (ZH)**: Blockbuster是一种用于推理程序中AI操作融合的框架 

---
# AI-Based Crypto Tokens: The Illusion of Decentralized AI? 

**Title (ZH)**: 基于AI的加密代币：去中心化AI的幻觉？ 

**Authors**: Rischan Mafrur  

**Link**: [PDF](https://arxiv.org/pdf/2505.07828)  

**Abstract**: The convergence of blockchain and artificial intelligence (AI) has led to the emergence of AI-based tokens, which are cryptographic assets designed to power decentralized AI platforms and services. This paper provides a comprehensive review of leading AI-token projects, examining their technical architectures, token utilities, consensus mechanisms, and underlying business models. We explore how these tokens operate across various blockchain ecosystems and assess the extent to which they offer value beyond traditional centralized AI services. Based on this assessment, our analysis identifies several core limitations. From a technical perspective, many platforms depend extensively on off-chain computation, exhibit limited capabilities for on-chain intelligence, and encounter significant scalability challenges. From a business perspective, many models appear to replicate centralized AI service structures, simply adding token-based payment and governance layers without delivering truly novel value. In light of these challenges, we also examine emerging developments that may shape the next phase of decentralized AI systems. These include approaches for on-chain verification of AI outputs, blockchain-enabled federated learning, and more robust incentive frameworks. Collectively, while emerging innovations offer pathways to strengthen decentralized AI ecosystems, significant gaps remain between the promises and the realities of current AI-token implementations. Our findings contribute to a growing body of research at the intersection of AI and blockchain, highlighting the need for critical evaluation and more grounded approaches as the field continues to evolve. 

**Abstract (ZH)**: 区块链与人工智能的融合及其基于AI的代币：技术架构、应用价值与商业模型综述 

---
# Explainable Artificial Intelligence Techniques for Software Development Lifecycle: A Phase-specific Survey 

**Title (ZH)**: 解释性人工智能技术在软件开发生命周期中的应用：一种阶段特异性综述 

**Authors**: Lakshit Arora, Sanjay Surendranath Girija, Shashank Kapoor, Aman Raj, Dipen Pradhan, Ankit Shetgaonkar  

**Link**: [PDF](https://arxiv.org/pdf/2505.07058)  

**Abstract**: Artificial Intelligence (AI) is rapidly expanding and integrating more into daily life to automate tasks, guide decision making, and enhance efficiency. However, complex AI models, which make decisions without providing clear explanations (known as the "black-box problem"), currently restrict trust and widespread adoption of AI. Explainable Artificial Intelligence (XAI) has emerged to address the black-box problem of making AI systems more interpretable and transparent so stakeholders can trust, verify, and act upon AI-based outcomes. Researchers have developed various techniques to foster XAI in the Software Development Lifecycle. However, there are gaps in applying XAI techniques in the Software Engineering phases. Literature review shows that 68% of XAI in Software Engineering research is focused on maintenance as opposed to 8% on software management and requirements. In this paper, we present a comprehensive survey of the applications of XAI methods such as concept-based explanations, Local Interpretable Model-agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), rule extraction, attention mechanisms, counterfactual explanations, and example-based explanations to the different phases of the Software Development Life Cycle (SDLC), including requirements elicitation, design and development, testing and deployment, and evolution. To the best of our knowledge, this paper presents the first comprehensive survey of XAI techniques for every phase of the Software Development Life Cycle (SDLC). This survey aims to promote explainable AI in Software Engineering and facilitate the practical application of complex AI models in AI-driven software development. 

**Abstract (ZH)**: 人工智能（AI）正迅速扩展并更多地集成到日常生活中以自动化任务、指导决策并提高效率。然而，缺乏清晰解释的复杂AI模型（ known as the “黑箱问题”）当前限制了人们对其的信任和广泛应用。可解释的人工智能（XAI）已经出现，旨在使AI系统更加可解释和透明，从而使利益相关者能够信任、验证并基于AI结果采取行动。研究者已经开发了各种技术来促进XAI在整个软件开发生命周期（SDLC）中的应用。然而，在软件工程（Software Engineering, SE）阶段应用XAI技术仍存在差距。文献综述显示，68%的XAI在软件工程研究中侧重于维护，相比之下，只有8%侧重于软件管理和需求分析。在本论文中，我们全面概述了诸如概念解释、局部可解释通用模型解释（LIME）、SHapley值归属解释（SHAP）、规则提取、注意力机制、反事实解释和基于示例的解释等XAI方法在软件开发生命周期不同阶段的应用，包括需求获取、设计与开发、测试与部署以及演化。据我们所知，这是首次对软件开发生命周期（SDLC）每个阶段的XAI技术进行全面调研。本研究旨在促进软件工程中的可解释AI，并推动复杂AI模型在人工智能驱动软件开发中的实际应用。 

---
# Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control 

**Title (ZH)**: 强化学习（RL）与城市气候 modeling相结合：基于RL的HVAC控制效果与影响调查 

**Authors**: Junjie Yu, John S. Schreck, David John Gagne, Keith W. Oleson, Jie Li, Yongtu Liang, Qi Liao, Mingfei Sun, David O. Topping, Zhonghua Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2505.07045)  

**Abstract**: Reinforcement learning (RL)-based heating, ventilation, and air conditioning (HVAC) control has emerged as a promising technology for reducing building energy consumption while maintaining indoor thermal comfort. However, the efficacy of such strategies is influenced by the background climate and their implementation may potentially alter both the indoor climate and local urban climate. This study proposes an integrated framework combining RL with an urban climate model that incorporates a building energy model, aiming to evaluate the efficacy of RL-based HVAC control across different background climates, impacts of RL strategies on indoor climate and local urban climate, and the transferability of RL strategies across cities. Our findings reveal that the reward (defined as a weighted combination of energy consumption and thermal comfort) and the impacts of RL strategies on indoor climate and local urban climate exhibit marked variability across cities with different background climates. The sensitivity of reward weights and the transferability of RL strategies are also strongly influenced by the background climate. Cities in hot climates tend to achieve higher rewards across most reward weight configurations that balance energy consumption and thermal comfort, and those cities with more varying atmospheric temperatures demonstrate greater RL strategy transferability. These findings underscore the importance of thoroughly evaluating RL-based HVAC control strategies in diverse climatic contexts. This study also provides a new insight that city-to-city learning will potentially aid the deployment of RL-based HVAC control. 

**Abstract (ZH)**: 基于强化学习（RL）的 HVAC 控制结合城市气候模型的综合框架：评价不同背景气候下 HVAC 控制策略的有效性及其对室内和局部城市气候的影响，以及策略的跨城迁移性 

---
