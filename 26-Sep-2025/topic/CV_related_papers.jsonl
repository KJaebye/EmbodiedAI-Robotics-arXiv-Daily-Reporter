{'arxiv_id': 'arXiv:2509.20757', 'title': 'MASt3R-Fusion: Integrating Feed-Forward Visual Model with IMU, GNSS for High-Functionality SLAM', 'authors': 'Yuxuan Zhou, Xingxing Li, Shengyu Li, Zhuohao Yan, Chunxi Xia, Shaoquan Feng', 'link': 'https://arxiv.org/abs/2509.20757', 'abstract': 'Visual SLAM is a cornerstone technique in robotics, autonomous driving and extended reality (XR), yet classical systems often struggle with low-texture environments, scale ambiguity, and degraded performance under challenging visual conditions. Recent advancements in feed-forward neural network-based pointmap regression have demonstrated the potential to recover high-fidelity 3D scene geometry directly from images, leveraging learned spatial priors to overcome limitations of traditional multi-view geometry methods. However, the widely validated advantages of probabilistic multi-sensor information fusion are often discarded in these pipelines. In this work, we propose MASt3R-Fusion,a multi-sensor-assisted visual SLAM framework that tightly integrates feed-forward pointmap regression with complementary sensor information, including inertial measurements and GNSS data. The system introduces Sim(3)-based visualalignment constraints (in the Hessian form) into a universal metric-scale SE(3) factor graph for effective information fusion. A hierarchical factor graph design is developed, which allows both real-time sliding-window optimization and global optimization with aggressive loop closures, enabling real-time pose tracking, metric-scale structure perception and globally consistent mapping. We evaluate our approach on both public benchmarks and self-collected datasets, demonstrating substantial improvements in accuracy and robustness over existing visual-centered multi-sensor SLAM systems. The code will be released open-source to support reproducibility and further research (this https URL).', 'abstract_zh': '多传感器辅助视觉SLAM框架MASt3R-Fusion：结合前向点云回归与互补传感器信息进行有效的信息融合', 'title_zh': 'MASt3R-Fusion：结合前馈视觉模型与IMU、GNSS的高功能SLAMcomings'}
{'arxiv_id': 'arXiv:2509.20486', 'title': 'Boosting LiDAR-Based Localization with Semantic Insight: Camera Projection versus Direct LiDAR Segmentation', 'authors': 'Sven Ochs, Philip Schörner, Marc René Zofka, J. Marius Zöllner', 'link': 'https://arxiv.org/abs/2509.20486', 'abstract': 'Semantic segmentation of LiDAR data presents considerable challenges, particularly when dealing with diverse sensor types and configurations. However, incorporating semantic information can significantly enhance the accuracy and robustness of LiDAR-based localization techniques for autonomous mobile systems. We propose an approach that integrates semantic camera data with LiDAR segmentation to address this challenge. By projecting LiDAR points into the semantic segmentation space of the camera, our method enhances the precision and reliability of the LiDAR-based localization pipeline.\nFor validation, we utilize the CoCar NextGen platform from the FZI Research Center for Information Technology, which offers diverse sensor modalities and configurations. The sensor setup of CoCar NextGen enables a thorough analysis of different sensor types. Our evaluation leverages the state-of-the-art Depth-Anything network for camera image segmentation and an adaptive segmentation network for LiDAR segmentation. To establish a reliable ground truth for LiDAR-based localization, we make us of a Global Navigation Satellite System (GNSS) solution with Real-Time Kinematic corrections (RTK). Additionally, we conduct an extensive 55 km drive through the city of Karlsruhe, Germany, covering a variety of environments, including urban areas, multi-lane roads, and rural highways. This multimodal approach paves the way for more reliable and precise autonomous navigation systems, particularly in complex real-world environments.', 'abstract_zh': '基于语义的LiDAR数据分割在处理多种传感器类型和配置时面临重大挑战，但结合语义信息可以显著提高自主移动系统中基于LiDAR的局部化技术的准确性和鲁棒性。我们提出了一种将语义相机数据与LiDAR分割集成的方法来应对这一挑战。通过将LiDAR点投影到相机的语义分割空间，我们的方法增强了基于LiDAR的局部化管道的精密度和可靠性。', 'title_zh': '基于语义洞察提升LiDARベース的定位： Camera投影 versus 直接LiDAR分割'}
{'arxiv_id': 'arXiv:2509.20906', 'title': 'Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences', 'authors': 'Julius Pesonen, Arno Solin, Eija Honkavaara', 'link': 'https://arxiv.org/abs/2509.20906', 'abstract': '3D object localisation based on a sequence of camera measurements is essential for safety-critical surveillance tasks, such as drone-based wildfire monitoring. Localisation of objects detected with a camera can typically be solved with dense depth estimation or 3D scene reconstruction. However, in the context of distant objects or tasks limited by the amount of available computational resources, neither solution is feasible. In this paper, we show that the task can be solved using particle filters for both single and multiple target scenarios. The method was studied using a 3D simulation and a drone-based image segmentation sequence with global navigation satellite system (GNSS)-based camera pose estimates. The results showed that a particle filter can be used to solve practical localisation tasks based on camera poses and image segments in these situations where other solutions fail. The particle filter is independent of the detection method, making it flexible for new tasks. The study also demonstrates that drone-based wildfire monitoring can be conducted using the proposed method paired with a pre-existing image segmentation model.', 'abstract_zh': '基于相机测量序列的3D物体定位对于无人机森林火灾监测等安全关键监控任务至关重要。物体的定位通常可以通过密集深度估计或3D场景重建来解决。但在远处物体或计算资源有限的任务中，这两种解决方案都不实用。本文展示了一种使用粒子滤波器来解决单目标和多目标场景定位任务的方法。该方法在3D仿真和基于全球导航卫星系统（GNSS）的相机姿态估计的无人机图像分割序列中进行了研究。结果表明，粒子滤波器可以在其他解决方案失效的情况下，用于基于相机姿态和图像分割的实际定位任务。粒子滤波器独立于检测方法，具有良好的灵活性，适用于新任务。此外，研究表明，提出的配以预存图像分割模型的方法可用于无人机森林火灾监测。', 'title_zh': '从嘈杂的相机运动和语义分割序列中寻找远处物体的3D位置'}
{'arxiv_id': 'arXiv:2509.21278', 'title': 'Does FLUX Already Know How to Perform Physically Plausible Image Composition?', 'authors': 'Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong', 'link': 'https://arxiv.org/abs/2509.21278', 'abstract': 'Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication.', 'abstract_zh': 'Seamless, 高保真插入与错误抑制的训练-free 框架：SHINE', 'title_zh': 'FLUX 是否already知道如何执行物理上可验证的图像合成？'}
{'arxiv_id': 'arXiv:2509.21265', 'title': 'MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation', 'authors': 'Xinyu Liu, Guolei Sun, Cheng Wang, Yixuan Yuan, Ender Konukoglu', 'link': 'https://arxiv.org/abs/2509.21265', 'abstract': 'High-resolution (HR) medical videos are vital for accurate diagnosis, yet are hard to acquire due to hardware limitations and physiological constraints. Clinically, the collected low-resolution (LR) medical videos present unique challenges for video super-resolution (VSR) models, including camera shake, noise, and abrupt frame transitions, which result in significant optical flow errors and alignment difficulties. Additionally, tissues and organs exhibit continuous and nuanced structures, but current VSR models are prone to introducing artifacts and distorted features that can mislead doctors. To this end, we propose MedVSR, a tailored framework for medical VSR. It first employs Cross State-Space Propagation (CSSP) to address the imprecise alignment by projecting distant frames as control matrices within state-space models, enabling the selective propagation of consistent and informative features to neighboring frames for effective alignment. Moreover, we design an Inner State-Space Reconstruction (ISSR) module that enhances tissue structures and reduces artifacts with joint long-range spatial feature learning and large-kernel short-range information aggregation. Experiments across four datasets in diverse medical scenarios, including endoscopy and cataract surgeries, show that MedVSR significantly outperforms existing VSR models in reconstruction performance and efficiency. Code released at this https URL.', 'abstract_zh': '高分辨率医学视频对于准确诊断至关重要，但由于硬件限制和生理限制，获取这些视频比较困难。临床中，收集到的低分辨率医学视频给视频超分辨率（VSR）模型带来了独特挑战，包括相机抖动、噪声和帧间突变，这些都导致了显著的光学流动错误和对齐难题。此外，组织和器官表现出连续和细腻的结构，但当前的VSR模型容易引入伪影和失真的特征，这可能会误导医生。为此，我们提出MedVSR，一个针对医学VSR的定制框架。该框架首先采用跨状态空间传播（CSSP）来解决不精确的对齐问题，通过将远程帧作为状态空间模型内的控制矩阵进行投影，使得一致且富有信息的特征能够在邻近帧之间进行选择性传播，从而实现有效的对齐。此外，我们设计了内部状态空间重构（ISSR）模块，通过结合长距离空间特征学习和大型内核短距离信息聚合来增强组织结构并减少伪影。在四个不同医学场景的四个数据集中进行的实验，包括内窥镜和白内障手术，表明MedVSR在重建性能和效率上显著优于现有VSR模型。已在该链接发布代码：[这个 https URL]。', 'title_zh': 'MedVSR：基于跨态空间传播的医学视频超分辨率'}
{'arxiv_id': 'arXiv:2509.21249', 'title': 'Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations', 'authors': 'Zhijian Yang, Noel DSouza, Istvan Megyeri, Xiaojian Xu, Amin Honarmandi Shandiz, Farzin Haddadpour, Krisztian Koos, Laszlo Rusko, Emanuele Valeriano, Bharadwaj Swaninathan, Lei Wu, Parminder Bhatia, Taha Kass-Hout, Erhan Bas', 'link': 'https://arxiv.org/abs/2509.21249', 'abstract': 'Magnetic Resonance Imaging (MRI) is a critical medical imaging modality in clinical diagnosis and research, yet its complexity and heterogeneity pose challenges for automated analysis, particularly in scalable and generalizable machine learning applications. While foundation models have revolutionized natural language and vision tasks, their application to MRI remains limited due to data scarcity and narrow anatomical focus. In this work, we present Decipher-MR, a 3D MRI-specific vision-language foundation model trained on a large-scale dataset comprising 200,000 MRI series from over 22,000 studies spanning diverse anatomical regions, sequences, and pathologies. Decipher-MR integrates self-supervised vision learning with report-guided text supervision to build robust, generalizable representations, enabling effective adaptation across broad applications. To enable robust and diverse clinical tasks with minimal computational overhead, Decipher-MR supports a modular design that enables tuning of lightweight, task-specific decoders attached to a frozen pretrained encoder. Following this setting, we evaluate Decipher-MR across diverse benchmarks including disease classification, demographic prediction, anatomical localization, and cross-modal retrieval, demonstrating consistent performance gains over existing foundation models and task-specific approaches. Our results establish Decipher-MR as a scalable and versatile foundation for MRI-based AI, facilitating efficient development across clinical and research domains.', 'abstract_zh': '磁共振成像(MRI)是临床诊断和研究中一种至关重要的医学影像模态，但由于其复杂性和异质性，自动分析面临挑战，尤其是在可扩展和泛化的机器学习应用中。尽管基础模型在自然语言和视觉任务中取得了革命性的进展，但由于数据稀缺和解剖学关注的局限性，其在MRI中的应用仍然有限。在本文中，我们介绍了一种名为Decipher-MR的3D MRI专用视觉-语言基础模型，该模型基于包含超过22,000个研究中的200,000个MRI系列的大规模数据集训练，涵盖了不同的解剖区域、序列和病理。Decipher-MR将自我监督的视觉学习与报告导向的文字监督相结合，构建了稳健和可泛化的表示，能够有效适应广泛的临床和研究应用。通过模块化设计，Decipher-MR支持轻量级、任务特定解码器与冻结的预训练编码器连接，从而实现最小的计算开销和鲁棒的多样性临床任务。在这一设置下，我们在多种基准任务中评估了Decipher-MR，包括疾病分类、人口统计预测、解剖定位和跨模态检索，展示了相对于现有基础模型和任务特定方法的持续性能提升。我们的结果确立了Decipher-MR作为基于MRI的AI的可扩展和多功能基础，促进了临床和研究领域的高效开发。', 'title_zh': 'Decipher-MR：一种用于3D MRI表示的视觉-语言基础模型'}
{'arxiv_id': 'arXiv:2509.21245', 'title': 'Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets', 'authors': 'Team Hunyuan3D, Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Linus, Penghao Wang, Qingxiang Lin, Sicong Liu, Xianghui Yang, Yixuan Tang, Yunfei Zhao, Zeqiang Lai, Zhihao Liang, Zibo Zhao', 'link': 'https://arxiv.org/abs/2509.21245', 'abstract': 'Recent advances in 3D-native generative models have accelerated asset creation for games, film, and design. However, most methods still rely primarily on image or text conditioning and lack fine-grained, cross-modal controls, which limits controllability and practical adoption. To address this gap, we present Hunyuan3D-Omni, a unified framework for fine-grained, controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images, Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose priors as conditioning signals, enabling precise control over geometry, topology, and pose. Instead of separate heads for each modality, our model unifies all signals in a single cross-modal architecture. We train with a progressive, difficulty-aware sampling strategy that selects one control modality per example and biases sampling toward harder signals (e.g., skeletal pose) while downweighting easier ones (e.g., point clouds), encouraging robust multi-modal fusion and graceful handling of missing inputs. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows.', 'abstract_zh': 'Recent advances in 3D-native generative models have accelerated asset creation for games, film, and design. However, most methods still rely primarily on image or text conditioning and lack fine-grained, cross-modal controls, which limits controllability and practical adoption. To address this gap, we present Hunyuan3D-Omni, a unified framework for fine-grained, controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images, Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose priors as conditioning signals, enabling precise control over geometry, topology, and pose. Instead of separate heads for each modality, our model unifies all signals in a single cross-modal architecture. We train with a progressive, difficulty-aware sampling strategy that selects one control modality per example and biases sampling toward harder signals (e.g., skeletal pose) while downweighting easier ones (e.g., point clouds), encouraging robust multi-modal fusion and graceful handling of missing inputs. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows.\n\nTitle: 近年来，基于3D的生成模型在游戏、电影和设计中的资产创建方面取得了进步。然而，大多数方法仍然主要依赖于图像或文本条件，并缺乏细粒度的跨模态控制，这限制了可控性和实际应用。为解决这一问题，我们提出了Hunyuan3D-Omni，这是一个基于Hunyuan3D 2.1的统一框架，用于细粒度可控的3D资产生成。除了图像之外，Hunyuan3D-Omni还接受点云、体素、边界框和骨架姿态先验作为条件信号，使其能够精确控制几何形状、拓扑结构和姿态。与每个模态都有单独的头部不同，我们的模型将所有信号统合在一个跨模态架构中。我们采用一种渐进式、难度感知的采样策略进行训练，该策略为每个示例选择一种控制模态，并倾向于选择更难的信号（例如，骨架姿态）而降低更简单的信号（例如，点云）的重要性，以鼓励稳健的多模态融合并优雅地处理缺失输入。实验表明，这些额外的控制提高了生成的准确性，实现了几何感知的变换，并增强了生产工作流程的鲁棒性。', 'title_zh': 'Hunyuan3D-Omni：可控生成3D资产的统一框架'}
{'arxiv_id': 'arXiv:2509.21084', 'title': 'Vision Transformers: the threat of realistic adversarial patches', 'authors': 'Kasper Cools, Clara Maathuis, Alexander M. van Oers, Claudia S. Hübner, Nikos Deligiannis, Marijke Vandewal, Geert De Cubber', 'link': 'https://arxiv.org/abs/2509.21084', 'abstract': 'The increasing reliance on machine learning systems has made their security a critical concern. Evasion attacks enable adversaries to manipulate the decision-making processes of AI systems, potentially causing security breaches or misclassification of targets. Vision Transformers (ViTs) have gained significant traction in modern machine learning due to increased 1) performance compared to Convolutional Neural Networks (CNNs) and 2) robustness against adversarial perturbations. However, ViTs remain vulnerable to evasion attacks, particularly to adversarial patches, unique patterns designed to manipulate AI classification systems. These vulnerabilities are investigated by designing realistic adversarial patches to cause misclassification in person vs. non-person classification tasks using the Creases Transformation (CT) technique, which adds subtle geometric distortions similar to those occurring naturally when wearing clothing. This study investigates the transferability of adversarial attack techniques used in CNNs when applied to ViT classification models. Experimental evaluation across four fine-tuned ViT models on a binary person classification task reveals significant vulnerability variations: attack success rates ranged from 40.04% (google/vit-base-patch16-224-in21k) to 99.97% (facebook/dino-vitb16), with google/vit-base-patch16-224 achieving 66.40% and facebook/dinov3-vitb16 reaching 65.17%. These results confirm the cross-architectural transferability of adversarial patches from CNNs to ViTs, with pre-training dataset scale and methodology strongly influencing model resilience to adversarial attacks.', 'abstract_zh': '机器学习系统安全性的日益依赖使其安全性成为关键问题。逃逸攻击使对手能够操控AI系统的决策过程，可能导致安全漏洞或目标误分类。由于与卷积神经网络（CNNs）相比具有更高的1）性能和2）对抗扰动的稳健性，视觉变换器（ViTs）在现代机器学习中受到广泛关注。然而，ViTs仍然容易受到逃逸攻击的影响，特别是对抗性补丁的影响，这是一种旨在操纵AI分类系统的独特模式。通过设计基于折痕变换（CT）技术的现实主义对抗性补贴，这些漏洞在人员与非人员分类任务中引起误分类，以调查CNN中使用的对抗攻击技术在应用于ViT分类模型时的可转移性。在针对二元人员分类任务的四种微调ViT模型上的实验评估显示，攻击成功率范围从40.04%（google/vit-base-patch16-224-in21k）到99.97%（facebook/dino-vitb16），其中google/vit-base-patch16-224达到66.40%，facebook/dinov3-vitb16达到65.17%。这些结果证实了来自CNN的对抗性补丁在不同架构之间的可移植性，前期训练数据集规模和方法对模型对抗攻击的鲁棒性有重要影响。', 'title_zh': '视觉变换器：现实主义 adversarial 崩溃图案的威胁'}
{'arxiv_id': 'arXiv:2509.21061', 'title': 'EnGraf-Net: Multiple Granularity Branch Network with Fine-Coarse Graft Grained for Classification Task', 'authors': 'Riccardo La Grassa, Ignazio Gallo, Nicola Landro', 'link': 'https://arxiv.org/abs/2509.21061', 'abstract': 'Fine-grained classification models are designed to focus on the relevant details necessary to distinguish highly similar classes, particularly when intra-class variance is high and inter-class variance is low. Most existing models rely on part annotations such as bounding boxes, part locations, or textual attributes to enhance classification performance, while others employ sophisticated techniques to automatically extract attention maps. We posit that part-based approaches, including automatic cropping methods, suffer from an incomplete representation of local features, which are fundamental for distinguishing similar objects. While fine-grained classification aims to recognize the leaves of a hierarchical structure, humans recognize objects by also forming semantic associations. In this paper, we leverage semantic associations structured as a hierarchy (taxonomy) as supervised signals within an end-to-end deep neural network model, termed EnGraf-Net. Extensive experiments on three well-known datasets CIFAR-100, CUB-200-2011, and FGVC-Aircraft demonstrate the superiority of EnGraf-Net over many existing fine-grained models, showing competitive performance with the most recent state-of-the-art approaches, without requiring cropping techniques or manual annotations.', 'abstract_zh': '基于语义关联的细粒度分类模型', 'title_zh': 'EnGraf-Net：具有精细-粗粒度嫁接粒度的多粒度分支网络用于分类任务'}
{'arxiv_id': 'arXiv:2509.21050', 'title': 'GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic Supervision, and Reinforced MLLM-based Solutions', 'authors': 'Bing Liu, Wenqiang Yv, Xuzheng Yang, Shichang Wang, Junzhuo Liu, Peng Wang, Guoqing Wang, Yang Yang, Heng Tao Shen', 'link': 'https://arxiv.org/abs/2509.21050', 'abstract': 'AI-driven geometric problem solving is a complex vision-language task that requires accurate diagram interpretation, mathematical reasoning, and robust cross-modal grounding. A foundational yet underexplored capability for this task is the ability to identify and interpret geometric elements based on natural language queries. To address this, we introduce the task of Referring Expression Comprehension (REC) for geometric problems, which evaluates whether models can localize points, shapes, and spatial relations in diagrams in response to textual prompts. We present GeoRef, a benchmark dataset constructed from existing geometric problem corpora, featuring diverse, high-quality annotations and queries. Due to the lack of annotated data for this task, we generate a large-scale synthetic training dataset using a structured geometric formal language, enabling broad coverage of geometric concepts and facilitating model adaptation. We explore two fine-tuning approaches: Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO). Our results show that GRPO significantly outperforms SFT by better aligning model behavior with task-specific rewards. Furthermore, we propose a verify-and-regenerate mechanism that detects incorrect predictions and re-infers answers using contextual reasoning history, further boosting accuracy. Notably, even state-of-the-art Multimodal Large Language Models (MLLMs) struggle with this task, underscoring the necessity of explicitly evaluating and strengthening geometric grounding as a prerequisite for robust geometric problem solving. Moreover, models trained on GeoRef demonstrate measurable improvements on downstream geometric reasoning tasks, highlighting the broader value of REC as a foundation for multimodal mathematical understanding.', 'abstract_zh': '基于AI驱动的几何问题求解是一个复杂的视觉-语言任务，要求准确的图表解释、数学推理和稳健的跨模态定位。这一任务的一个基础但尚未充分探索的能力是根据自然语言查询识别和解释几何元素的能力。为了解决这个问题，我们引入了几何问题参照表达理解（Referring Expression Comprehension, REC）任务，评估模型能否根据文本提示在图表中定位点、形状和空间关系。我们介绍了GeoRef，一个从现有几何问题语料库构建的基准数据集，包含多样且高质量的注释和查询。由于缺乏该任务的注释数据，我们使用结构化几何形式语言生成了大规模合成训练数据集，以广泛覆盖几何概念并促进模型适应。我们探索了两种微调方法：有监督微调（SFT）和组相对策略优化（GRPO）。我们的结果显示，GRPO显著优于SFT，因为它更好地使模型行为与特定任务奖励相一致。此外，我们提出了一种验证和重生成机制，用于检测错误预测并使用上下文推理历史重推答案，进一步提高了准确性。值得注意的是，即使最先进的多模态大规模语言模型（MLLMs）也难以完成这一任务，强调了明确评估和加强几何定位作为实现稳健几何问题求解的必要前提的重要性。此外，基于GeoRef训练的模型在下游几何推理任务上展现出可度量的进步，突显了REC作为多模态数学理解基础的广泛价值。', 'title_zh': 'GeoRef: 几何表达的的任务表述、合成监督及强化MLLM解决方案'}
{'arxiv_id': 'arXiv:2509.21007', 'title': 'Marching Neurons: Accurate Surface Extraction for Neural Implicit Shapes', 'authors': 'Christian Stippel, Felix Mujkanovic, Thomas Leimkühler, Pedro Hermosilla', 'link': 'https://arxiv.org/abs/2509.21007', 'abstract': 'Accurate surface geometry representation is crucial in 3D visual computing. Explicit representations, such as polygonal meshes, and implicit representations, like signed distance functions, each have distinct advantages, making efficient conversions between them increasingly important. Conventional surface extraction methods for implicit representations, such as the widely used Marching Cubes algorithm, rely on spatial decomposition and sampling, leading to inaccuracies due to fixed and limited resolution. We introduce a novel approach for analytically extracting surfaces from neural implicit functions. Our method operates natively in parallel and can navigate large neural architectures. By leveraging the fact that each neuron partitions the domain, we develop a depth-first traversal strategy to efficiently track the encoded surface. The resulting meshes faithfully capture the full geometric information from the network without ad-hoc spatial discretization, achieving unprecedented accuracy across diverse shapes and network architectures while maintaining competitive speed.', 'abstract_zh': '准确的表面几何表示对于三维视觉计算至关重要。显式表示，如多边形网格，和隐式表示，如有符号距离函数，各自具有独特的优点，使得它们之间的高效转换日益重要。传统的用于隐式表示的表面提取方法，如广泛使用的Marching Cubes算法，依赖于空间分解和采样，由于固定和有限的分辨率导致不准确。我们提出了一种新的方法，用于从神经隐式函数中分析地提取表面。该方法可以本原地并行操作，并能导航大型神经架构。通过利用每个神经元划分域的事实，我们开发了一种深度优先遍历策略，以高效地追踪编码的表面。生成的网格能够忠实捕捉网络中的完整几何信息，而无需人为的空间离散化，从而在多样化的形状和网络架构上实现了前所未有的精度，同时保持了竞争力的速度。', 'title_zh': '行进神经元：神经隐式形状的准确表面提取'}
{'arxiv_id': 'arXiv:2509.20991', 'title': 'Fast-SEnSeI: Lightweight Sensor-Independent Cloud Masking for On-board Multispectral Sensors', 'authors': 'Jan Kněžík, Jonáš Herec, Rado Pitoňák', 'link': 'https://arxiv.org/abs/2509.20991', 'abstract': 'Cloud segmentation is a critical preprocessing step for many Earth observation tasks, yet most models are tightly coupled to specific sensor configurations and rely on ground-based processing. In this work, we propose Fast-SEnSeI, a lightweight, sensor-independent encoder module that enables flexible, on-board cloud segmentation across multispectral sensors with varying band configurations. Building upon SEnSeI-v2, Fast-SEnSeI integrates an improved spectral descriptor, lightweight architecture, and robust padding-band handling. It accepts arbitrary combinations of spectral bands and their wavelengths, producing fixed-size feature maps that feed into a compact, quantized segmentation model based on a modified U-Net. The module runs efficiently on embedded CPUs using Apache TVM, while the segmentation model is deployed on FPGA, forming a CPU-FPGA hybrid pipeline suitable for space-qualified hardware. Evaluations on Sentinel-2 and Landsat 8 datasets demonstrate accurate segmentation across diverse input configurations.', 'abstract_zh': '基于SEnSeI-v2的Fast-SEnSeI：一种轻量级、传感器无关的编码器模块，用于多光谱传感器的灵活机载云分割', 'title_zh': 'Fast-SEnSeI: 轻量级传感器独立云遮盖算法用于机载多光谱传感器'}
{'arxiv_id': 'arXiv:2509.20986', 'title': 'SiNGER: A Clearer Voice Distills Vision Transformers Further', 'authors': 'Geunhyeok Yu, Sunjae Jeong, Yoonyoung Choi, Jaeseung Kim, Hyoseok Hwang', 'link': 'https://arxiv.org/abs/2509.20986', 'abstract': "Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \\oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.", 'abstract_zh': 'Vision Transformers广泛应用于视觉基础模型的主干网络，但已知会产生高范数的伪像，损害表示质量。当知识蒸馏将这些特征传递给学生时，高范数的伪像主导了目标函数，导致学生过度拟合伪像而忽视了有益信号，从而削弱了大模型带来的增益。先前的工作试图去除伪像，但遇到了去除伪像与保留教师有益信号之间的固有权衡。为解决这一问题，我们引入了Singular Nullspace-Guided Energy Reallocation（SiNGER），一种新颖的蒸馏框架，能够在去除伪像的同时保留有益信号。关键思想是在细化过程中利用nullspace引导的扰动来保留信息同时抑制伪像。然后将细化后的教师特征传递给学生。我们通过基于LoRA的适配器高效实现这一扰动，仅需进行最少的结构修改。广泛实验表明，该方法能够一致地提升学生模型，在多个下游任务中实现最先进的性能，并生成更清晰和更具可解释性的表示。', 'title_zh': 'SiNGER: 更清晰的声音提炼视觉Transformer'}
{'arxiv_id': 'arXiv:2509.20890', 'title': 'FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies', 'authors': 'Shuqiao Liang, Jian Liu, Renzhang Chen, Quanlong Guan', 'link': 'https://arxiv.org/abs/2509.20890', 'abstract': 'The increasing realism of synthetic images generated by advanced models such as VAEs, GANs, and LDMs poses significant challenges for synthetic image detection. To address this issue, we explore two artifact types introduced during the generation process: (1) latent distribution deviations and (2) decoding-induced smoothing effects, which manifest as inconsistencies in local textures, edges, and color transitions. Leveraging local pixel dependencies (LPD) properties rooted in Markov Random Fields, we reconstruct synthetic images using neighboring pixel information to expose disruptions in texture continuity and edge coherence. Building upon LPD, we propose FerretNet, a lightweight neural network with only 1.1M parameters that delivers efficient and robust synthetic image detection. Extensive experiments demonstrate that FerretNet, trained exclusively on the 4-class ProGAN dataset, achieves an average accuracy of 97.1% on an open-world benchmark comprising across 22 generative models, surpassing state-of-the-art methods by 10.6%.', 'abstract_zh': '高级模型如VAEs、GANs和LDMs生成的合成图像日益逼真，给合成图像检测带来了重大挑战。为应对这一问题，我们探索了生成过程中引入的两种artifact类型：(1) 潜在分布偏差和(2) 解码引起的平滑效应，这些效应表现为局部纹理、边缘和颜色过渡的一致性问题。利用马尔可夫随机场中局部像素依赖性（LPD）的特性，我们利用相邻像素信息重构合成图像，以揭示纹理连续性和边缘一致性中的中断。在此基础上，我们提出了FerretNet，一个仅含1.1M参数的轻量级神经网络，实现了高效且稳健的合成图像检测。广泛实验表明，FerretNet仅在4类ProGAN数据集上训练，能够在包含22个生成模型的开放世界基准中达到平均97.1%的准确率，超过了现有最先进的方法10.6个百分点。', 'title_zh': 'FerretNet：通过局部像素依赖高效合成图像检测'}
{'arxiv_id': 'arXiv:2509.20884', 'title': 'Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering', 'authors': 'Zhifei Li, Feng Qiu, Yiran Wang, Yujing Xia, Kui Xiao, Miao Zhang, Yan Zhang', 'link': 'https://arxiv.org/abs/2509.20884', 'abstract': 'Visual Question Answering (VQA) presents a unique challenge by requiring models to understand and reason about visual content to answer questions accurately. Existing VQA models often struggle with biases introduced by the training data, leading to over-reliance on superficial patterns and inadequate generalization to diverse questions and images. This paper presents a novel model, IOG-VQA, which integrates Object Interaction Self-Attention and GAN-Based Debiasing to enhance VQA model performance. The self-attention mechanism allows our model to capture complex interactions between objects within an image, providing a more comprehensive understanding of the visual context. Meanwhile, the GAN-based debiasing framework generates unbiased data distributions, helping the model to learn more robust and generalizable features. By leveraging these two components, IOG-VQA effectively combines visual and textual information to address the inherent biases in VQA datasets. Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that our model shows excellent performance compared with the existing methods, particularly in handling biased and imbalanced data distributions highlighting the importance of addressing both object interactions and dataset biases in advancing VQA tasks. Our code is available at this https URL.', 'abstract_zh': '视觉问答（VQA）通过要求模型理解和推理视觉内容来准确回答问题，从而提出了一项独特的挑战。现有VQA模型常常难以应对训练数据引入的偏差，导致模型过度依赖表面模式，并且难以将学到的知识应用于多样化的问句和图像。本文提出了一种名为IOG-VQA的新模型，该模型结合了对象交互自注意力和基于GAN的去偏技术，以提升VQA模型的表现。自注意力机制使我们的模型能够捕捉图像中对象之间的复杂交互，提供更全面的视觉上下文理解。同时，基于GAN的去偏框架生成无偏的数据分布，帮助模型学习更稳健和泛化的特征。通过利用这两个组件，IOG-VQA有效地结合视觉和文本信息，以应对VQA数据集中的固有偏差。在VQA-CP v1和VQA-CP v2数据集上的广泛实验表明，与现有方法相比，我们的模型在处理有偏和不均衡数据分布方面表现优异，强调了在推进VQA任务时同时关注对象交互和数据集偏差的重要性。我们的代码可在以下链接获得：this https URL。', 'title_zh': '集成物体交互自注意力和基于GAN的去偏见方法的视觉问答'}
{'arxiv_id': 'arXiv:2509.20857', 'title': 'TasselNetV4: A vision foundation model for cross-scene, cross-scale, and cross-species plant counting', 'authors': 'Xiaonan Hu, Xuebing Li, Jinyu Xu, Abdulkadir Duran Adan, Letian Zhou, Xuhui Zhu, Yanan Li, Wei Guo, Shouyang Liu, Wenzhong Liu, Hao Lu', 'link': 'https://arxiv.org/abs/2509.20857', 'abstract': 'Accurate plant counting provides valuable information for agriculture such as crop yield prediction, plant density assessment, and phenotype quantification. Vision-based approaches are currently the mainstream solution. Prior art typically uses a detection or a regression model to count a specific plant. However, plants have biodiversity, and new cultivars are increasingly bred each year. It is almost impossible to exhaust and build all species-dependent counting models. Inspired by class-agnostic counting (CAC) in computer vision, we argue that it is time to rethink the problem formulation of plant counting, from what plants to count to how to count plants. In contrast to most daily objects with spatial and temporal invariance, plants are dynamic, changing with time and space. Their non-rigid structure often leads to worse performance than counting rigid instances like heads and cars such that current CAC and open-world detection models are suboptimal to count plants. In this work, we inherit the vein of the TasselNet plant counting model and introduce a new extension, TasselNetV4, shifting from species-specific counting to cross-species counting. TasselNetV4 marries the local counting idea of TasselNet with the extract-and-match paradigm in CAC. It builds upon a plain vision transformer and incorporates novel multi-branch box-aware local counters used to enhance cross-scale robustness. Two challenging datasets, PAC-105 and PAC-Somalia, are harvested. Extensive experiments against state-of-the-art CAC models show that TasselNetV4 achieves not only superior counting performance but also high this http URL results indicate that TasselNetV4 emerges to be a vision foundation model for cross-scene, cross-scale, and cross-species plant counting.', 'abstract_zh': '基于视觉的准确植物计数为农业提供了作物产量预测、植物密度评估和表型量化等有价值的信息。现有的方法通常使用检测或回归模型来计数特定的植物。由于植物具有生物多样性，且每年培育出越来越多的新品种，几乎不可能构建所有物种依赖的计数模型。受计算机视觉中类无关计数(CAC)的启发，我们认为是时候重新考虑植物计数的问题表述，从需要计数哪些植物转变为如何计数植物。与大多数具有时空不变性的日常物体不同，植物是动态的，随时间和空间而变化，其非刚性结构导致其计数性能通常劣于计数刚性实例（如头部和车辆）的方法，使得当前的CAC和开放世界检测模型在植物计数上效果不佳。在本工作中，我们继承了TasselNet植物计数模型的思路，并提出了一个新的扩展TasselNetV4，从物种特定计数转向跨物种计数。TasselNetV4将TasselNet的局部计数理念与CAC中的提取-匹配范式相结合。它基于简单的视觉Transformer，并结合了新的多分支盒感知局部计数器，以增强跨尺度鲁棒性。收集了两个具有挑战性的数据集PAC-105和PAC-Somalia。与最新的CAC模型进行广泛实验表明，TasselNetV4不仅在计数性能上取得了卓越的表现，而且具有高的交叉场景、跨尺度和跨物种植物计数潜力。研究结果表明，TasselNetV4成为跨场景、跨尺度和跨物种植物计数的视觉基础模型。', 'title_zh': 'TasselNetV4：一种适用于跨场景、跨尺度和跨物种植物计数的视觉基础模型'}
{'arxiv_id': 'arXiv:2509.20775', 'title': 'CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion', 'authors': 'Maoye Ren, Praneetha Vaddamanu, Jianjin Xu, Fernando De la Torre Frade', 'link': 'https://arxiv.org/abs/2509.20775', 'abstract': 'Recently remarkable progress has been made in synthesizing realistic human photos using text-to-image diffusion models. However, current approaches face degraded scenes, insufficient control, and suboptimal perceptual identity. We introduce CustomEnhancer, a novel framework to augment existing identity customization models. CustomEnhancer is a zero-shot enhancement pipeline that leverages face swapping techniques, pretrained diffusion model, to obtain additional representations in a zeroshot manner for encoding into personalized models. Through our proposed triple-flow fused PerGeneration approach, which identifies and combines two compatible counter-directional latent spaces to manipulate a pivotal space of personalized model, we unify the generation and reconstruction processes, realizing generation from three flows. Our pipeline also enables comprehensive training-free control over the generation process of personalized models, offering precise controlled personalization for them and eliminating the need for controller retraining for per-model. Besides, to address the high time complexity of null-text inversion (NTI), we introduce ResInversion, a novel inversion method that performs noise rectification via a pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity fidelity, training-free controls, while also showing the efficiency of our ResInversion over NTI. The code will be made publicly available upon paper acceptance.', 'abstract_zh': 'Recent显著进展已在使用文本到图像扩散模型合成逼真的人像方面取得。然而，当前的方法面临场景退化、控制不足和感知身份欠佳的问题。我们引入了CustomEnhancer，一种新颖的身份增强框架，以增强现有的身份定制模型。CustomEnhancer 是一种零样本增强流程，利用面部替换技术及预训练扩散模型，在零样本条件下获取额外表示，并将其编码到个性化模型中。通过我们提出的三流融合PerGeneration方法，该方法识别并结合两个兼容的反向潜在空间，以操作个性化模型的关键空间，我们统一了生成和重构过程，实现了从三流生成。我们的流程还允许对个性化模型生成过程的全面无监督控制，提供精确的控制个性化，消除每个模型控制器重新培训的需要。此外，为了解决空文本反转（NTI）的时间复杂度过高的问题，我们引入了ResInversion，这是一种新颖的反转方法，通过预扩散机制进行噪声校正，将反转时间缩短了129倍。实验表明，CustomEnhancer 在场景多样性和身份保真度方面达到SOTA结果，同时展示了ResInversion相较于NTI的效率。论文接受后代码将公开。', 'title_zh': 'CusEnhancer: 一种基于ResInversion的零样本场景和可控性增强方法用于照片个性化定制'}
{'arxiv_id': 'arXiv:2509.20681', 'title': 'Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation', 'authors': 'Wei-Teng Chu, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi', 'link': 'https://arxiv.org/abs/2509.20681', 'abstract': 'Implicit representations have been widely applied in robotics for obstacle avoidance and path planning. In this paper, we explore the problem of constructing an implicit distance representation from a single image. Past methods for implicit surface reconstruction, such as \\emph{NeuS} and its variants generally require a large set of multi-view images as input, and require long training times. In this work, we propose Fast Image-to-Neural Surface (FINS), a lightweight framework that can reconstruct high-fidelity surfaces and SDF fields based on a single or a small set of images. FINS integrates a multi-resolution hash grid encoder with lightweight geometry and color heads, making the training via an approximate second-order optimizer highly efficient and capable of converging within a few seconds. Additionally, we achieve the construction of a neural surface requiring only a single RGB image, by leveraging pre-trained foundation models to estimate the geometry inherent in the image. Our experiments demonstrate that under the same conditions, our method outperforms state-of-the-art baselines in both convergence speed and accuracy on surface reconstruction and SDF field estimation. Moreover, we demonstrate the applicability of FINS for robot surface following tasks and show its scalability to a variety of benchmark datasets.', 'abstract_zh': '基于单张图像构建隐式距离表示的快速图像到神经表面框架', 'title_zh': '从单张图像高效构建隐式曲面模型用于运动生成'}
{'arxiv_id': 'arXiv:2509.20571', 'title': 'MechStyle: Augmenting Generative AI with Mechanical Simulation to Create Stylized and Structurally Viable 3D Models', 'authors': 'Faraz Faruqi, Amira Abdel-Rahman, Leandra Tejedor, Martin Nisser, Jiaji Li, Vrushank Phadnis, Varun Jampani, Neil Gershenfeld, Megan Hofmann, Stefanie Mueller', 'link': 'https://arxiv.org/abs/2509.20571', 'abstract': "Recent developments in Generative AI enable creators to stylize 3D models based on text prompts. These methods change the 3D model geometry, which can compromise the model's structural integrity once fabricated. We present MechStyle, a system that enables creators to stylize 3D printable models while preserving their structural integrity. MechStyle accomplishes this by augmenting the Generative AI-based stylization process with feedback from a Finite Element Analysis (FEA) simulation. As the stylization process modifies the geometry to approximate the desired style, feedback from the FEA simulation reduces modifications to regions with increased stress. We evaluate the effectiveness of FEA simulation feedback in the augmented stylization process by comparing three stylization control strategies. We also investigate the time efficiency of our approach by comparing three adaptive scheduling strategies. Finally, we demonstrate MechStyle's user interface that allows users to generate stylized and structurally viable 3D models and provide five example applications.", 'abstract_zh': 'Recent developments in生成式AI使创作者能够基于文本提示对3D模型进行风格化。这些方法会改变3D模型的几何结构，一旦制造成型可能导致模型的结构完整性受损。我们提出了MechStyle系统，能够在保持3D打印模型结构完整性的同时对其进行风格化。MechStyle通过结合基于生成式AI的风格化过程和有限元分析（FEA）仿真反馈来实现这一目标。随着风格化过程修改几何结构以接近目标风格，FEA仿真反馈减少了对应力增加区域的修改。我们通过比较三种风格化控制策略评估了FEA仿真反馈在增强风格化过程中的有效性。我们还通过比较三种自适应调度策略研究了该方法的时间效率。最后，我们展示了MechStyle的用户界面，允许用户生成风格化且结构有效的3D模型，并提供了五个示例应用。', 'title_zh': 'MechStyle: 通过机械仿真增强生成式AI以创建具风格且结构可行的3D模型'}
{'arxiv_id': 'arXiv:2509.20524', 'title': 'InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On', 'authors': 'Julien Han, Shuwen Qiu, Qi Li, Xingzi Xu, Mehmet Saygin Seyfioglu, Kavosh Asadi, Karim Bouyarmane', 'link': 'https://arxiv.org/abs/2509.20524', 'abstract': 'We present InstructVTON, an instruction-following interactive virtual try-on system that allows fine-grained and complex styling control of the resulting generation, guided by natural language, on single or multiple garments. A computationally efficient and scalable formulation of virtual try-on formulates the problem as an image-guided or image-conditioned inpainting task. These inpainting-based virtual try-on models commonly use a binary mask to control the generation layout. Producing a mask that yields desirable result is difficult, requires background knowledge, might be model dependent, and in some cases impossible with the masking-based approach (e.g. trying on a long-sleeve shirt with "sleeves rolled up" styling on a person wearing long-sleeve shirt with sleeves down, where the mask will necessarily cover the entire sleeve). InstructVTON leverages Vision Language Models (VLMs) and image segmentation models for automated binary mask generation. These masks are generated based on user-provided images and free-text style instructions. InstructVTON simplifies the end-user experience by removing the necessity of a precisely drawn mask, and by automating execution of multiple rounds of image generation for try-on scenarios that cannot be achieved with masking-based virtual try-on models alone. We show that InstructVTON is interoperable with existing virtual try-on models to achieve state-of-the-art results with styling control.', 'abstract_zh': 'InstructVTON：一种遵循指令的交互式虚拟试穿系统，通过自然语言指导实现细粒度和复杂的设计控制', 'title_zh': '基于 inpainting 的虚拟试穿：最优自动遮罩和自然语言引导的交互式风格控制'}
{'arxiv_id': 'arXiv:2509.20481', 'title': 'Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision', 'authors': 'Jing Li, Oskar Bartosz, Chengyu Wang, Michal Wnuczynski, Dilshan Godaliyadda, Michael Polley', 'link': 'https://arxiv.org/abs/2509.20481', 'abstract': 'The majority of AI models in imaging and vision are customized to perform on specific high-precision task. However, this strategy is inefficient for applications with a series of modular tasks, since each requires a mapping into a disparate latent domain. To address this inefficiency, we proposed a universal Neural Space (NS), where an encoder-decoder framework pre-computes features across vision and imaging tasks. Our encoder learns transformation aware, generalizable representations, which enable multiple downstream AI modules to share the same feature space. This architecture reduces redundancy, improves generalization across domain shift, and establishes a foundation for effecient multi-task vision pipelines. Furthermore, as opposed to larger transformer backbones, our backbone is lightweight and CNN-based, allowing for wider across hardware. We furthur demonstrate that imaging and vision modules, such as demosaicing, denoising, depth estimation and semantic segmentation can be performed efficiently in the NS.', 'abstract_zh': '大多数成像和视觉中的AI模型都是为特定高精度任务量身定制的。然而，对于一系列模块化任务的应用而言，这种策略是低效率的，因为每个任务都需要映射到不同的潜在域。为了解决这一低效率问题，我们提出了一种通用神经空间（NS），其中编码器-解码器框架预计算视觉和成像任务的特征。我们的编码器学习变换感知的一般化表示，使多个下游AI模块能够共享同一特征空间。这种架构减少了冗余，提高了跨域泛化能力，并为高效的多任务视觉管道奠定了基础。此外，与较大的变压器骨干网络相比，我们的骨干网络更轻量级且基于CNN，可以在更广泛的硬件上运行。我们进一步证明，成像和视觉模块，如去马赛克、去噪、深度估计和语义分割，可以在NS中高效运行。', 'title_zh': '共享神经空间：统一先计算特征编码用于多任务和跨域视觉任务'}
