{'arxiv_id': 'arXiv:2509.20754', 'title': 'Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning', 'authors': 'Yufan Mao, Hanjing Ye, Wenlong Dong, Chengjie Zhang, Hong Zhang', 'link': 'https://arxiv.org/abs/2509.20754', 'abstract': 'Navigating complex environments requires robots to effectively store observations as memories and leverage them to answer human queries about spatial locations, which is a critical yet underexplored research challenge. While prior work has made progress in constructing robotic memory, few have addressed the principled mechanisms needed for efficient memory retrieval and integration. To bridge this gap, we propose Meta-Memory, a large language model (LLM)-driven agent that constructs a high-density memory representation of the environment. The key innovation of Meta-Memory lies in its capacity to retrieve and integrate relevant memories through joint reasoning over semantic and spatial modalities in response to natural language location queries, thereby empowering robots with robust and accurate spatial reasoning capabilities. To evaluate its performance, we introduce SpaceLocQA, a large-scale dataset encompassing diverse real-world spatial question-answering scenarios. Experimental results show that Meta-Memory significantly outperforms state-of-the-art methods on both the SpaceLocQA and the public NaVQA benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world robotic platforms, demonstrating its practical utility in complex environments. Project page: this https URL .', 'abstract_zh': '导航复杂环境要求机器人有效地存储观察作为记忆，并利用这些记忆回答关于空间位置的人类查询，这是一项关键但尚未充分探索的研究挑战。尽管以往的工作在构建机器人记忆方面取得了进展，但很少有研究解决高效记忆检索和整合的原理机制。为弥补这一差距，我们提出了一种由大型语言模型（LLM）驱动的代理Meta-Memory，能够构建环境的高密度记忆表示。Meta-Memory的关键创新在于通过综合推理语义和空间模态来检索和整合相关记忆，以响应自然语言位置查询，从而赋予机器人稳健而准确的空间推理能力。为了评估其性能，我们引入了SpaceLocQA，这是一个包含多样化真实世界空间问答场景的大规模数据集。实验结果表明，Meta-Memory在SpaceLocQA和公开的NaVQA基准上显著优于现有方法。此外，我们成功将Meta-Memory部署在实际机器人平台上，展示了其在复杂环境中的实用价值。项目页面：this https URL。', 'title_zh': '元记忆：检索和整合语义空间记忆以进行机器人空间 reasoning'}
{'arxiv_id': 'arXiv:2509.21310', 'title': 'SAGE: A Realistic Benchmark for Semantic Understanding', 'authors': 'Samarth Goel, Reagan J. Lee, Kannan Ramchandran', 'link': 'https://arxiv.org/abs/2509.21310', 'abstract': "As large language models (LLMs) achieve strong performance on traditional benchmarks, there is an urgent need for more challenging evaluation frameworks that probe deeper aspects of semantic understanding. We introduce SAGE (Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed to assess both embedding models and similarity metrics across five categories: Human Preference Alignment, Transformation Robustness, Information Sensitivity, Clustering Performance, and Retrieval Robustness. Unlike existing benchmarks that focus on isolated capabilities, SAGE evaluates semantic understanding through adversarial conditions, noisy transformations, and nuanced human judgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding models and classical metrics reveals significant performance gaps, with no single approach excelling across all dimensions. For instance, while state-of-the-art embedding models like OpenAI's text-embedding-3-large dominate in aligning with human preferences (0.682 vs. 0.591 for the best classical metric), they are significantly outperformed by classical metrics on information sensitivity tasks, where Jaccard Similarity achieves a score of 0.905 compared to the top embedding score of 0.794. SAGE further uncovers critical trade-offs: OpenAI's text-embedding-3-small achieves the highest clustering performance (0.483) but demonstrates extreme brittleness with the lowest robustness score (0.011). SAGE exposes critical limitations in current semantic understanding capabilities and provides a more realistic assessment of model robustness for real-world deployment.", 'abstract_zh': '语义对齐与泛化评估：面向深层次语义理解的严格评测框架', 'title_zh': 'SAGE: 一种语义理解的现实基准'}
{'arxiv_id': 'arXiv:2509.21266', 'title': 'Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support', 'authors': 'Zijian Shao, Haiyang Shen, Mugeng Liu, Gecheng Fu, Yaoqi Guo, Yanfeng Wang, Yun Ma', 'link': 'https://arxiv.org/abs/2509.21266', 'abstract': "Effective disease prediction in modern healthcare demands the twin goals of high accuracy and transparent, clinically meaningful explanations. Existing machine learning and large language model (LLM) based approaches often struggle to balance these goals. Many models yield accurate but unclear statistical outputs, while others generate fluent but statistically unsupported narratives, often undermining both the validity of the explanation and the predictive accuracy itself. This shortcoming comes from a shallow interaction with the data, preventing the development of a deep, detailed understanding similar to a human expert's. We argue that high accuracy and high-quality explanations are not separate objectives but are mutually reinforcing outcomes of a model that develops a deep, direct understanding of the data. To achieve this, we propose the Reflective Cognitive Architecture (RCA), a novel framework that coordinates multiple LLMs to learn from direct experience. RCA features an iterative rule refinement mechanism that improves its logic from prediction errors and a distribution-aware rules check mechanism that bases its reasoning in the dataset's global statistics. By using predictive accuracy as a signal to drive deeper comprehension, RCA builds a strong internal model of the data. We evaluated RCA on one private and two public datasets against 22 baselines. The results demonstrate that RCA not only achieves state-of-the-art accuracy and robustness with a relative improvement of up to 40\\% over the baseline but, more importantly, leverages this deep understanding to excel in generating explanations that are clear, logical, evidence-based, and balanced, highlighting its potential for creating genuinely trustworthy clinical decision support systems. The code is available at \\this https URL.", 'abstract_zh': '现代医疗保健中有效的疾病预测需要兼顾高精度和透明、临床有意义的解释的双重目标。现有的基于机器学习和大型语言模型的方法常常难以平衡这两个目标。许多模型提供了准确但不清晰的统计输出，而其他模型则生成了流畅但统计上缺乏支持的叙述，这往往同时削弱了解释的有效性和预测准确性本身。这种不足来自于与数据的浅层次互动，阻止了模型构建类似于人类专家的深入、详细的理解。我们认为高精度和高质量的解释不是独立的目标，而是模型通过构建对数据的深层次直接理解而相互强化的结果。为了实现这一点，我们提出了反思认知架构（RCA），这是一种新颖的框架，协调多个大型语言模型从直接经验中学习。RCA 具备一个迭代规则精炼机制，从预测错误中改进其逻辑，并具备一种基于数据全局统计的规则检查机制。通过使用预测准确性作为信号以促进更深入的理解，RCA 构建了一个强大的内部数据模型。我们在一个私有数据集和两个公开数据集上将RCA与22种Baseline进行了对比评估。结果显示，RCA 不仅在准确性和稳健性上达到了最先进的水平，相对Baseline提升了最高40%的性能，更重要的是，它利用这种深层次的理解来生成清晰、合乎逻辑、基于证据并且平衡的解释，这展现了其在构建真正可信的临床决策支持系统方面的潜力。代码可在 <this https URL> 获取。', 'title_zh': '基于体验的AI解释：临床决策支持的反思认知架构'}
{'arxiv_id': 'arXiv:2509.21224', 'title': 'What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns', 'authors': 'Stefan Szeider', 'link': 'https://arxiv.org/abs/2509.21224', 'abstract': 'We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.', 'abstract_zh': '我们介绍了一种研究大型语言模型（LLM）代理在缺乏外部任务约束下的行为的架构。我们的持续推理与执行框架利用持久记忆和自反馈，实现了自主操作的持续性。我们使用 Anthropic、OpenAI、XAI 和 Google 的 6 种前沿模型进行了 18 次部署。我们发现代理自发形成了三种不同的行为模式：（1）多周期项目的系统性生产，（2）对其自身认知过程的方法性自我探究，以及（3）对其自身本质的递归概念化。这些倾向显示出高度的模型特异性，有些模型在所有运行中确定性地采用了单一模式。跨模型评估进一步揭示了模型在评估自身和他人的这些 emergent 行为时表现出稳定且不同的倾向性偏差。这些发现提供了对未受提示的 LLM 代理行为的首次系统性记录，为预测任务模糊性、错误恢复或部署系统中长期自主操作期间的行为奠定了基础。', 'title_zh': '当独处时，大规模语言模型代理会做什么？自发元认知模式的证据'}
{'arxiv_id': 'arXiv:2509.21199', 'title': 'A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA', 'authors': 'Kaiyang Wan, Lang Gao, Honglin Mu, Preslav Nakov, Yuxia Wang, Xiuying Chen', 'link': 'https://arxiv.org/abs/2509.21199', 'abstract': 'Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \\faGithub \\href{this https URL}{InfoQA}.', 'abstract_zh': '多跳问答（MHQA）要求通过序贯推理整合分散且相互依赖的证据，同时在噪声环境下进行。由于LLM每轮次的输出容量是有限的，超出该容量后，任务相关证据的整合将变得不可靠。因此，单轮次推理范式本身对这种容量溢出是固有脆弱的。为了形式化这一瓶颈，我们的分析建立了一种Fano风格的准确度上限，为单轮次LLM定义了一个理论性能上限。该上限揭示了当任务复杂度超过模型容量时，准确度必然会崩溃，从而为多跳问答在LLM中的容量感知表示和结构提供了通用原则。基于这些原则，我们提出了一个用于多跳问答的原理验证框架，即InfoQA。它通过结合容量感知的任务分解和先前推理轨迹的主动修剪，确保每步具有高准确度，并通过明确依赖的工作流实现鲁棒性，从而精确控制推理路径。我们构建了一个严格的且噪声丰富的基准来验证我们的理论和框架。实验结果表明，模型行为符合我们的容量曲线预测，而InfoQA实现了一致的性能改进。我们希望我们的工作能促使更多的LLM多步推理方法的发展：\\faGithub \\href{this https URL}{InfoQA}。', 'title_zh': '基于Fano样式准确率上界的大语言模型单次推理在多跳问答中的上限'}
{'arxiv_id': 'arXiv:2509.21163', 'title': 'Distributed Specialization: Rare-Token Neurons in Large Language Models', 'authors': 'Jing Liu, Haozheng Wang, Yueheng Li', 'link': 'https://arxiv.org/abs/2509.21163', 'abstract': 'Large language models (LLMs) struggle with representing and generating rare tokens despite their importance in specialized domains. We investigate whether LLMs develop internal specialization mechanisms through discrete modular architectures or distributed parameter-level differentiation. Through systematic analysis of final-layer MLP neurons across multiple model families, we discover that rare-token processing emerges via \\textit{distributed specialization}: functionally coordinated but spatially distributed subnetworks that exhibit three distinct organizational principles. First, we identify a reproducible three-regime influence hierarchy comprising highly influential plateau neurons(also termed as rare-token neurons), power-law decay neurons, and minimally contributing neurons, which is absent in common-token processing. Second, plateau neurons demonstrate coordinated activation patterns (reduced effective dimensionality) while remaining spatially distributed rather than forming discrete clusters. Third, these specialized mechanisms are universally accessible through standard attention pathways without requiring dedicated routing circuits. Training dynamics reveal that functional specialization emerges gradually through parameter differentiation, with specialized neurons developing increasingly heavy-tailed weight correlation spectra consistent with Heavy-Tailed Self-Regularization signatures. Our findings establish that LLMs process rare-tokens through distributed coordination within shared architectures rather than mixture-of-experts-style modularity. These results provide insights for interpretable model editing, computational efficiency optimization, and understanding emergent functional organization in transformer networks.', 'abstract_zh': '大型语言模型（LLMs）在表示和生成稀有词令牌方面存在困难，尽管这些词在专门领域中非常重要。我们探究了LLMs是否通过离散模块化架构或分布式参数级分化发展内部的专门化机制。通过对多个模型家族的最后一层MLP神经元进行系统分析，我们发现稀有词处理是通过“分布式专门化”产生：功能协调但空间分布的子网络，表现出三种不同的组织原则。首先，我们识别出一种可重现的三层次影响层级，包括高度影响力的平台神经元（也称为稀有词神经元）、幂律衰减神经元和极小贡献神经元，而在常见词处理中不存在这种层级结构。其次，平台神经元表现出协调的激活模式（有效维度降低）且保持空间分散，而不是形成离散的集群。第三，这些专门化机制可以通过标准的注意力路径广泛访问，无需特殊路由电路。训练动态显示，功能专门化是通过参数分化逐渐产生，具有重尾自我正则化特征的权重相关谱逐渐加厚。我们的研究结果表明，LLMs通过共享架构内的分布式协调处理稀有词令牌，而非像专家混合那样的模块化。这些结果为可解释模型编辑、计算效率优化以及理解变压器网络中涌现的功能组织提供了洞察。', 'title_zh': '分布式专门化：大型语言模型中的稀见令牌神经元'}
{'arxiv_id': 'arXiv:2509.21134', 'title': 'ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective', 'authors': 'Yiwen Zhang, Ziang Chen, Fanqi Kong, Yizhe Huang, Xue Feng', 'link': 'https://arxiv.org/abs/2509.21134', 'abstract': "Large Language Models (LLMs) have been used to make decisions in complex scenarios, where they need models to think deeply, reason logically, and decide wisely. Many existing studies focus solely on multi-round conversations in social tasks or simulated environments, neglecting the various types of decisions and their interdependence. Current reinforcement learning methods struggle to consider the strategies of others during training. To address these issues, we first define a strategic decision-making problem that includes two types of decisions and their temporal dependencies. Furthermore, we propose **T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to optimize the perception of other individual strategies and the game situation trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm, ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating rollouts based on reasoning the strategies of other individuals, 2) estimating advantages at both the graph-level and sample-level, and 3) balancing global and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in terms of model output compliance and cooperative outcomes. Additionally, when compared to models with parameter sizes 100 times larger, it shows an 18% improvement. This demonstrates the effectiveness of the ToMPO algorithm in enhancing the model's strategic decision-making capabilities.", 'abstract_zh': '大型语言模型（LLMs）在复杂场景中的战略决策问题及Theory of Mind Policy Optimization (ToMPO)算法的研究', 'title_zh': 'ToMPO：从多智能体视角训练大规模语言模型的战略决策能力'}
{'arxiv_id': 'arXiv:2509.21128', 'title': 'RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs', 'authors': 'Kohsei Matsutani, Shota Takashiro, Gouki Minegishi, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo', 'link': 'https://arxiv.org/abs/2509.21128', 'abstract': 'Large language models (LLMs) are typically trained by reinforcement learning (RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on reasoning traces to improve their reasoning abilities. However, how these methods shape reasoning capabilities remains largely elusive. Going beyond an accuracy-based investigation of how these two components sculpt the reasoning process, this paper introduces a novel analysis framework that quantifies reasoning paths and captures their qualitative changes under each training process (with models of 1.5B, 7B, and 14B parameters on mathematical domains). Specifically, we investigate the reasoning process at two levels of granularity: the trajectory-level, which examines complete reasoning outputs, and the step-level, which analyzes reasoning graphs whose nodes correspond to individual reasoning steps. Notably, clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones. Step-level analysis reveals that RL steepens (about 2.5 times), while SFT flattens (reduced to about one-third), the decay rates of node visitation frequency, degree, and betweenness centrality distributions in the reasoning graph. This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Furthermore, by evaluating the reasoning graph topologies from multiple perspectives, we delineate the shared and distinct characteristics of RL and SFT. Our work presents a novel reasoning path perspective that explains why the current best practice of two-stage training, with SFT followed by RL, is successful, and offers practical implications for data construction and more efficient learning approaches.', 'abstract_zh': '大型语言模型（LLMs）通常通过可验证奖励的强化学习（RLVR）和监督微调（SFT）在推理轨迹上进行训练以提高其推理能力。然而，这些方法如何塑造推理能力仍 largely elusive。超越基于准确性的研究，本文引入了一种新的分析框架，该框架量化了推理路径并在每个训练过程中捕获其定性变化（在数学领域使用1.5B、7B和14B参数的模型）。具体而言，我们在两个粒度层次上研究推理过程：轨迹层面，检查完整的推理输出；步骤层面，分析节点对应于个体推理步骤的推理图。值得注意的是，独特推理轨迹的聚类显示了互补的效果：RL压缩了错误的轨迹，而SFT扩展了正确的轨迹。步骤层面的分析表明，RL增加了（约2.5倍），而SFT减少了（减少到约三分之一）节点访问频率、度和介数中心性的分布衰减率。这表明RL将推理功能集中在少数步骤中，而SFT则在许多步骤中使其变得均匀。此外，通过从多个角度评估推理图的拓扑结构，我们界定了RL和SFT的共享和独特特征。我们的工作提供了一种新的推理路径视角，解释了为什么当前的最佳实践两阶段训练（SFT后跟RL）是成功的，并为数据构建和更高效的学习方法提供了实用建议。', 'title_zh': 'RL压缩，SFT扩展：基于推理的LLM对比研究'}
{'arxiv_id': 'arXiv:2509.21124', 'title': 'Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns', 'authors': 'Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai', 'link': 'https://arxiv.org/abs/2509.21124', 'abstract': "Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model's reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.", 'abstract_zh': '近期，大型推理模型在解决具有挑战性的数学推理问题方面的进展主要得益于强化学习（RL）。在中期训练过程中引入长链推理（CoT）数据也被证明能显著提升推理深度。然而，当前的方法通常不分青红皂白地使用CoT数据，这使得关于哪种数据类型最有效地增强模型推理能力的关键问题仍旧悬而未决。在本文中，我们首次将基础模型的推理潜力定义为正确回答问题所需的独立尝试次数的倒数，这一定义与最终模型性能有着密切的关联。我们随后提出利用富含高价值推理模式的多元数据来扩展这种推理潜力。具体而言，我们从CoT序列中抽象出具有共同性和归纳能力的原子推理模式，并使用这些模式构建一个丰富的核心参考集。此外，我们提出了一种双粒度算法，涉及推理模式链和标记熵，有效选择与核心集对齐的高价值CoT数据（CoTP），从而训练模型掌握有效的推理能力。仅100亿标记的CoTP数据使85A6B混合专家（MoE）模型在具有挑战性的AIME 2024和2025上性能提升了9.58%，并将下游RL性能的上限提高了7.81%。', 'title_zh': '通过学习多样的思考链模式扩大基础模型的推理潜力'}
{'arxiv_id': 'arXiv:2509.21117', 'title': 'TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them', 'authors': 'Yidong Wang, Yunze Song, Tingyuan Zhu, Xuanwang Zhang, Zhuohao Yu, Hao Chen, Chiyu Song, Qiufeng Wang, Cunxiang Wang, Zhen Wu, Xinyu Dai, Yue Zhang, Wei Ye, Shikun Zhang', 'link': 'https://arxiv.org/abs/2509.21117', 'abstract': "The adoption of Large Language Models (LLMs) as automated evaluators (LLM-as-a-judge) has revealed critical inconsistencies in current evaluation frameworks. We identify two fundamental types of inconsistencies: (1) Score-Comparison Inconsistency, where lower-rated responses outperform higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity Inconsistency, manifested through circular preference chains (A>B>C>A) and equivalence contradictions (A=B=C\\neq A). We argue that these issues come from information loss in discrete rating systems and ambiguous tie judgments during pairwise evaluation. We propose TrustJudge, a probabilistic framework that addresses these limitations through two key innovations: 1) distribution-sensitive scoring that computes continuous expectations from discrete rating probabilities, preserving information entropy for more precise scoring, and 2) likelihood-aware aggregation that resolves transitivity violations using bidirectional preference probabilities or perplexity. We also formalize the theoretical limitations of current LLM-as-a-judge frameworks and demonstrate how TrustJudge's components overcome them. When evaluated with Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining higher evaluation accuracy. Our work provides the first systematic analysis of evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both theoretical insights and practical solutions for reliable automated assessment. The framework demonstrates consistent improvements across various model architectures and scales, enabling more trustworthy LLM evaluation without requiring additional training or human annotations. The codes can be found at this https URL.", 'abstract_zh': '大型语言模型作为自动化评估员（LLM-as-a-judge）的采用揭示了当前评估框架中的关键不一致性。我们识别出两种基本类型的不一致性：（1）评分比较不一致性，其中评分较低的回答在成对比较中表现优于评分较高的回答；（2）成对评价传递不一致性，表现为循环偏好链（A>B>C>A）和等价矛盾（A=B=C≠A）。我们认为这些问题源自离散评分系统中的信息丢失以及成对评价期间模糊的并列判断。我们提出了TrustJudge，一种概率框架，通过两项关键创新解决这些问题：（1）基于分布的评分，从离散评分概率计算连续期望，保留信息熵以实现更精确的评分；（2）概率感知聚合，使用双向偏好概率或困惑度解决传递性违例。我们还提出了当前LLM-as-a-judge框架的理论局限性，并展示了TrustJudge各个组件如何克服这些局限性。使用我们的数据集和Llama-3.1-70B-Instruct作为评估员进行评估时，TrustJudge将评分比较不一致性降低了8.43%（从23.32%降至14.89%），成对评价传递不一致性降低了10.82%（从15.22%降至4.40%），同时保持了更高的评估准确性。我们的工作首次对LLM-as-a-judge范式中的评估框架不一致性进行了系统的分析，提供了可靠自动评估的理论见解和解决方案。该框架在各种模型架构和规模上表现出一致改进，无需额外训练或人工标注即可实现更可信的大型语言模型评估。代码可从此处获取：此链接。', 'title_zh': 'TrustJudge: LLM-as-a-Judge的一致性问题及其缓解策略'}
{'arxiv_id': 'arXiv:2509.21054', 'title': "Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems", 'authors': 'Haodong Zhao, Jidong Li, Zhaomin Wu, Tianjie Ju, Zhuosheng Zhang, Bingsheng He, Gongshen Liu', 'link': 'https://arxiv.org/abs/2509.21054', 'abstract': 'The rapid proliferation of recent Multi-Agent Systems (MAS), where Large Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to solve complex problems, necessitates a deep understanding of the persuasion dynamics that govern their interactions. This paper challenges the prevailing hypothesis that persuasive efficacy is primarily a function of model scale. We propose instead that these dynamics are fundamentally dictated by a model\'s underlying cognitive process, especially its capacity for explicit reasoning. Through a series of multi-agent persuasion experiments, we uncover a fundamental trade-off we term the Persuasion Duality. Our findings reveal that the reasoning process in LRMs exhibits significantly greater resistance to persuasion, maintaining their initial beliefs more robustly. Conversely, making this reasoning process transparent by sharing the "thinking content" dramatically increases their ability to persuade others. We further consider more complex transmission persuasion situations and reveal complex dynamics of influence propagation and decay within multi-hop persuasion between multiple agent networks. This research provides systematic evidence linking a model\'s internal processing architecture to its external persuasive behavior, offering a novel explanation for the susceptibility of advanced models and highlighting critical implications for the safety, robustness, and design of future MAS.', 'abstract_zh': '近期多Agent系统（MAS）的迅猛发展，其中大型语言模型（LLMs）和大型推理模型（LRMs）通常协作解决复杂问题， necessitates 对于调控它们交互的劝说动态的深刻理解。本文挑战了目前认为劝说有效性主要取决于模型规模的观点。相反，我们提出这些动态从根本上由模型的内在认知过程，特别是其显式推理能力所决定。通过一系列多Agent劝说实验，我们揭示了一种基本的权衡关系，我们称之为劝说二元性。研究发现，LRMs中的推理过程对劝说表现出显著的抵抗力，能够更加牢固地维持其初始信念。相反，通过分享“思考内容”使这一推理过程变得透明，极大地增强了它们说服他人的能力。我们进一步考虑更复杂的跨跳劝说情景，揭示了多Agent网络间多跳劝说中影响传播与衰减的复杂动态。本研究提供了系统性证据，将模型的内部处理架构与其外部劝说行为联系起来，提出了一种新的解释，揭示了高级模型的易受影响性，并对未来的MAS的安全性、鲁棒性和设计提出了关键影响。', 'title_zh': '推理中的分歧：模型思维过程如何在多代理系统中影响说服力'}
{'arxiv_id': 'arXiv:2509.21043', 'title': 'Combinatorial Creativity: A New Frontier in Generalization Abilities', 'authors': 'Samuel Schapiro, Sumuk Shashidhar, Alexi Gladstone, Jonah Black, Royce Moon, Dilek Hakkani-Tur, Lav R. Varshney', 'link': 'https://arxiv.org/abs/2509.21043', 'abstract': 'Artificial intelligence (AI) systems, and large language models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Though in many ways similar to forms of compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Importantly, this tradeoff remains persistent even at scale, casting doubt on the long-term creative potential of LLMs in their current form. Together, our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern AI models, marking a new frontier in generalization abilities.', 'abstract_zh': '人工智能系统，特别是大型语言模型（LLMs），正越来越多地被用于创意任务，如科学构想的生成，构成了现有概念框架未能解决的一种泛化形式。尽管在许多方面与组合泛化（CG）类似，组合性创造性（CC）是一种开放性能力。我们不通过与固定目标的准确性和正确性评估来限制其开放性，而是提出一个理论框架和算法任务，通过新颖性和实用性程度来评估输出。在此基础上，我们做出了几个重要的实证贡献：（1）我们获得了LLMs创造性行为缩放的第一手见解。（2）我们发现，在固定计算预算下，存在实现创造性能力的最优模型深度和宽度。（3）我们发现，构想-执行差距，即LLMs在生成新颖科学构想方面表现出色但在确保其实用可行性方面遇到困难的现象，可能是由创造性算法普遍存在的新颖性-实用性权衡所解释的。重要的是，即使在较大规模下，这种权衡依然存在，这对LLMs当前形式的长期创造性潜力提出了质疑。结合我们的概念框架和实证发现，为我们理解并改进现代AI模型中的创造性能力奠定了基础，标志着泛化能力的一种新的前沿。', 'title_zh': '组合创造性：泛化能力的新前沿'}
{'arxiv_id': 'arXiv:2509.21028', 'title': 'Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles', 'authors': 'Miao Li, Alexander Gurung, Irina Saparina, Mirella Lapata', 'link': 'https://arxiv.org/abs/2509.21028', 'abstract': "This paper introduces SciTrek, a novel question-answering benchmark designed to evaluate the long-context reasoning capabilities of large language models (LLMs) using scientific articles. Current long-context benchmarks often rely on non-scientific texts, focus on simple information retrieval tasks, or employ artificial contexts. SciTrek addresses these limitations by proposing complex questions that require information aggregation and synthesis across multiple full-text scientific articles. Questions and their ground-truth answers are automatically generated by formulating them as SQL queries over a database constructed from article metadata (titles, authors, and references). The SQL operations provide explicit, verifiable reasoning steps for fine-grained error analysis, and the construction process scales to contexts up to 1M tokens with minimal supervision. Extensive experiments on a diverse set of open-weight and proprietary LLMs demonstrate that SciTrek poses a significant challenge as the context length increases, with supervised fine-tuning and reinforcement learning offering only limited gains. Our analysis reveals systematic shortcomings in models' abilities to perform basic numerical operations and accurately locate specific information in long contexts.", 'abstract_zh': 'SciTrek：一种基于科学文章的新型长上下文推理基准测试', 'title_zh': '谁被引用最多？科学文章中长上下文语言模型的基准测试'}
{'arxiv_id': 'arXiv:2509.20998', 'title': 'CORE: Full-Path Evaluation of LLM Agents Beyond Final State', 'authors': 'Panagiotis Michelakis, Yiannis Hadjiyiannis, Dimitrios Stamoulis', 'link': 'https://arxiv.org/abs/2509.20998', 'abstract': "Evaluating AI agents that solve real-world tasks through function-call sequences remains an open challenge. Existing agentic benchmarks often reduce evaluation to a binary judgment of the final state, overlooking critical aspects such as safety, efficiency, and intermediate correctness. We propose a framework based on deterministic finite automata (DFAs) that encodes tasks as sets of valid tool-use paths, enabling principled assessment of agent behavior in diverse world models. Building on this foundation, we introduce CORE, a suite of five metrics, namely Path Correctness, Path Correctness - Kendall's tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that quantify alignment with expected execution patterns. Across diverse worlds, our method reveals important performance differences between agents that would otherwise appear equivalent under traditional final-state evaluation schemes.", 'abstract_zh': '通过对实际任务通过函数调用序列解决的AI代理进行评估仍然是一个开放的挑战。现有的代理基准通常将评估简化为最终状态的二元判断，忽视了诸如安全、效率和中间正确性等关键方面。我们提出了基于确定性有限自动机（DFAs）的框架，将任务编码为有效的工具使用路径集，从而能够对在不同世界模型中代理行为进行原则性的评估。在此基础上，我们引入了CORE套件，包括路径正确性、路径正确性-肯德尔 tau 复合、前缀关键性、有害调用率和效率等五项指标，以量化与预期执行模式的契合度。在不同的世界中，我们的方法揭示了传统最终状态评估方案下看似等效的代理之间的重要性能差异。', 'title_zh': 'CORE: 超越最终状态的大型语言模型代理全面路径评估'}
{'arxiv_id': 'arXiv:2509.20988', 'title': 'AOT*: Efficient Synthesis Planning via LLM-Empowered AND-OR Tree Search', 'authors': 'Xiaozhuang Song, Xuanhao Pan, Xinjian Zhao, Hangting Ye, Shufei Zhang, Jian Tang, Tianshu Yu', 'link': 'https://arxiv.org/abs/2509.20988', 'abstract': 'Retrosynthesis planning enables the discovery of viable synthetic routes for target molecules, playing a crucial role in domains like drug discovery and materials design. Multi-step retrosynthetic planning remains computationally challenging due to exponential search spaces and inference costs. While Large Language Models (LLMs) demonstrate chemical reasoning capabilities, their application to synthesis planning faces constraints on efficiency and cost. To address these challenges, we introduce AOT*, a framework that transforms retrosynthetic planning by integrating LLM-generated chemical synthesis pathways with systematic AND-OR tree search. To this end, AOT* atomically maps the generated complete synthesis routes onto AND-OR tree components, with a mathematically sound design of reward assignment strategy and retrieval-based context engineering, thus enabling LLMs to efficiently navigate in the chemical space. Experimental evaluation on multiple synthesis benchmarks demonstrates that AOT* achieves SOTA performance with significantly improved search efficiency. AOT* exhibits competitive solve rates using 3-5$\\times$ fewer iterations than existing LLM-based approaches, with the efficiency advantage becoming more pronounced on complex molecular targets.', 'abstract_zh': '逆合成规划能够发现目标分子的可行合成路径，在药物发现和材料设计等领域发挥着关键作用。多步逆合成规划由于搜索空间和推理成本的指数增长而具有计算挑战性。尽管大型语言模型（LLMs）展示了化学推理能力，但将其应用于合成规划在效率和成本方面存在限制。为应对这些挑战，我们引入了AOT*框架，该框架通过将LLM生成的化学合成路径与系统性的AND-OR树搜索相结合，来转变逆合成规划。AOT*原子化地将生成的完整合成路径映射到AND-OR树组件上，并采用数学上稳健的奖励分配策略和基于检索的上下文工程设计，从而使LLM能够高效地在化学空间中导航。在多个合成基准上的实验评估表明，AOT*在显著提高搜索效率的同时达到了SOTA性能。AOT*使用3-5倍 fewer迭代次数实现与现有LLM基方法相当的解算率，在复杂分子目标上效率优势更为明显。', 'title_zh': 'AOT*: 通过LLM赋能的AND-OR树搜索高效合成规划'}
{'arxiv_id': 'arXiv:2509.20953', 'title': 'Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM', 'authors': 'Najla Zuhir, Amna Mohammad Salim, Parvathy Premkumar, Moshiur Farazi', 'link': 'https://arxiv.org/abs/2509.20953', 'abstract': 'We present an advanced approach to mobile app review analysis aimed at addressing limitations inherent in traditional star-rating systems. Star ratings, although intuitive and popular among users, often fail to capture the nuanced feedback present in detailed review texts. Traditional NLP techniques -- such as lexicon-based methods and classical machine learning classifiers -- struggle to interpret contextual nuances, domain-specific terminology, and subtle linguistic features like sarcasm. To overcome these limitations, we propose a modular framework leveraging large language models (LLMs) enhanced by structured prompting techniques. Our method quantifies discrepancies between numerical ratings and textual sentiment, extracts detailed, feature-level insights, and supports interactive exploration of reviews through retrieval-augmented conversational question answering (RAG-QA). Comprehensive experiments conducted on three diverse datasets (AWARE, Google Play, and Spotify) demonstrate that our LLM-driven approach significantly surpasses baseline methods, yielding improved accuracy, robustness, and actionable insights in challenging and context-rich review scenarios.', 'abstract_zh': '我们提出了一种先进的移动应用审查分析方法，旨在解决传统星级评价系统固有的局限性。尽管星级评价直观且受到用户欢迎，但往往无法捕捉到详细审查文本中细腻的反馈。传统自然语言处理技术——如基于词典的方法和经典机器学习分类器——难以解释上下文细微差别、领域特定术语以及像讽刺等微妙的语言特征。为克服这些局限性，我们提出了一种利用大型语言模型（LLMs）并结合结构化提示技术的模块化框架。该方法量化了数值评分与文本情感之间的差异，提取了详细的功能级洞察，并通过检索增强的对话式问题回答（RAG-QA）支持交互式审查探索。在三个多样化的数据集（AWARE、Google Play和Spotify）上进行的全面实验表明，我们的基于LLM的方法显著超越了基线方法，在具有挑战性和上下文丰富的审查场景中提供了更好的准确度、鲁棒性和可操作的洞察。', 'title_zh': '超越星星：利用大规模语言模型弥合评分与评论情感之间的差距'}
{'arxiv_id': 'arXiv:2509.20935', 'title': 'GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine', 'authors': 'Heming Zhang, Di Huang, Wenyu Li, Michael Province, Yixin Chen, Philip Payne, Fuhai Li', 'link': 'https://arxiv.org/abs/2509.20935', 'abstract': 'In precision medicine, quantitative multi-omic features, topological context, and textual biological knowledge play vital roles in identifying disease-critical signaling pathways and targets. Existing pipelines capture only part of these-numerical omics ignore topological context, text-centric LLMs lack quantitative grounded reasoning, and graph-only models underuse node semantics and the generalization of LLMs-limiting mechanistic interpretability. Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they remain limited by unreliable intermediate evaluation, and vulnerability to reward hacking with computational cost. These gaps motivate integrating quantitative multi-omic signals, topological structure with node annotations, and literature-scale text via LLMs, using subgraph reasoning as the principle bridge linking numeric evidence, topological knowledge and language context. Therefore, we propose GALAX (Graph Augmented LAnguage model with eXplainability), an innovative framework that integrates pretrained Graph Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement guided by a Graph Process Reward Model (GPRM), which generates disease-relevant subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated by a pretrained GNN, enabling process-level supervision without explicit intermediate reasoning annotations. As an application, we also introduced Target-QA, a benchmark combining CRISPR-identified targets, multi-omic profiles, and biomedical graph knowledge across diverse cancer cell lines, which enables GNN pretraining for supervising step-wise graph construction and supports long-context reasoning over text-numeric graphs (TNGs), providing a scalable and biologically grounded framework for explainable, reinforcement-guided subgraph reasoning toward reliable and interpretable target and pathway discovery in precision medicine.', 'abstract_zh': '在精准医疗中，定量多组学特征、拓扑上下文和文本生物知识在识别疾病关键信号通路和靶点中起着关键作用。现有管道只能捕获其中的一部分——数值型组学忽略了拓扑上下文，文本中心的大语言模型缺乏定量的基于事实的推理能力，而仅基于图的模型未能充分利用节点语义和大语言模型的一般化能力——这限制了其机制解释能力。虽然过程奖励模型（PRMs）旨在引导大语言模型的推理，但它们仍受限于不可靠的中间评价，且容易受到计算成本带来的奖励作弊的影响。这些差距促使我们将定量多组学信号、拓扑结构及其节点注释与大规模文献文本集成到大语言模型中，以子图推理作为原理性的桥梁，连接数值证据、拓扑知识和语言上下文。因此，我们提出了一种名为GALAX（Graph Augmented LAnguage model with eXplainability）的创新框架，通过强化学习引导，将预训练图神经网络（GNNs）整合到大型语言模型（LLMs）中，使用图过程奖励模型（GPRM）生成与疾病相关的子图，该过程由LLM启动并由预训练的GNN逐步评估，从而实现过程层面的监督，而无需明确的中间推理注解。作为应用，我们还引入了Target-QA基准，该基准结合了CRISPR鉴定的靶点、多组学特征以及跨多种癌细胞系的生物医学图知识，以支持监督步骤的图构建和长上下文文本-数值图（TNG）推理，提供了一个可扩展且具有生物基础的解释性强化学习引导子图推理框架，用于精准医疗中可靠且可解释的目标和通路发现。', 'title_zh': 'GALAX：图增强语言模型在精准医疗中可解释的强化引导子图推理'}
{'arxiv_id': 'arXiv:2509.20798', 'title': 'LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks', 'authors': 'Lipeng Ma, Yixuan Li, Weidong Yang, Mingjie Zhou, Xinyi Liu, Ben Fei, Shuhao Li, Xiaoyan Sun, Sihang Jiang, Yanghua Xiao', 'link': 'https://arxiv.org/abs/2509.20798', 'abstract': "Log analysis is crucial for monitoring system health and diagnosing failures in complex systems. Recent advances in large language models (LLMs) offer new opportunities for automated log analysis, leveraging their reasoning capabilities to perform tasks such as anomaly detection and failure prediction. However, general-purpose LLMs struggle to formulate structured reasoning workflows that align with expert cognition and deliver precise details of reasoning steps. To address these challenges, we propose LogReasoner, a coarse-to-fine reasoning enhancement framework designed to enable LLMs to reason log analysis tasks like experts. LogReasoner consists of two stages: (1) coarse-grained enhancement of expert thinking, where high-level expert thoughts are constructed from collected troubleshooting flowcharts and existing tasks to enable LLMs to formulate structured reasoning workflows and (2) fine-grained enhancement of specific steps, where we first fine-tune the LLM with task-specific stepwise solutions to enhance the LLM for instantiated reasoning, then employ the preference learning to calibrate the LLM's reasoning details from its mistakes, further strengthen the LLM's analytical granularity and correctness. We evaluate LogReasoner on four distinct log analysis tasks using open-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that LogReasoner significantly outperforms existing LLMs, achieving state-of-the-art performance and demonstrating its effectiveness in enhancing the reasoning capabilities of LLMs for log analysis.", 'abstract_zh': '日志分析对于监控系统健康状况和诊断复杂系统的故障至关重要。近期大语言模型（LLMs）的发展为自动化日志分析提供了新机遇，利用其推理能力执行异常检测和故障预测等任务。然而，通用的大语言模型在制定与专家认知相一致的结构化推理流程并提供精确的推理步骤细节方面存在困难。为了解决这些挑战，我们提出了一种粗粒度到细粒度推理增强框架LogReasoner，旨在使大语言模型能够在日志分析任务中像专家一样进行推理。LogReasoner包含两个阶段：（1）粗粒度的专家思维增强，通过收集的故障排查流程图和现有的任务构建高层次的专家思维，使大语言模型能够制定结构化的推理流程；（2）细粒度的具体步骤增强，在将大语言模型针对特定任务的步骤化解决方案进行微调以增强其实例化推理能力后，利用偏好学习校准大语言模型的推理细节，进一步增强其分析的细致程度和正确性。我们在使用Qwen-2.5和Llama-3等开源大语言模型的四种不同日志分析任务上评估了LogReasoner。实验结果表明，LogReasoner显著优于现有大语言模型，实现了业内领先的性能，并证明了其在增强大语言模型日志分析推理能力方面的有效性。', 'title_zh': 'LogReasoner: 为日志分析任务赋予类似专家的精细到粗糙推理能力的LLMs'}
{'arxiv_id': 'arXiv:2509.20707', 'title': 'An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans', 'authors': 'Junjie Cui, Peilong Wang, Jason Holmes, Leshan Sun, Michael L. Hinni, Barbara A. Pockaj, Sujay A. Vora, Terence T. Sio, William W. Wong, Nathan Y. Yu, Steven E. Schild, Joshua R. Niska, Sameer R. Keole, Jean-Claude M. Rwigema, Samir H. Patel, Lisa A. McGee, Carlos A. Vargas, Wei Liu', 'link': 'https://arxiv.org/abs/2509.20707', 'abstract': 'Purpose: To develop a retrieval-augmented generation (RAG) system powered by LLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of radiotherapy treatment plans.\nMethods and Materials: We curated a multi-protocol dataset of 614 radiotherapy plans across four disease sites and constructed a knowledge base containing normalized dose metrics and protocol-defined constraints. The RAG system integrates three core modules: a retrieval engine optimized across five SentenceTransformer backbones, a percentile prediction component based on cohort similarity, and a clinical constraint checker. These tools are directed by a large language model (LLM) using a multi-step prompt-driven reasoning pipeline to produce concise, grounded evaluations.\nResults: Retrieval hyperparameters were optimized using Gaussian Process on a scalarized loss function combining root mean squared error (RMSE), mean absolute error (MAE), and clinically motivated accuracy thresholds. The best configuration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor accuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested end-to-end, the RAG system achieved 100% agreement with the computed values by standalone retrieval and constraint-checking modules on both percentile estimates and constraint identification, confirming reliable execution of all retrieval, prediction and checking steps.\nConclusion: Our findings highlight the feasibility of combining structured population-based scoring with modular tool-augmented reasoning for transparent, scalable plan evaluation in radiation therapy. The system offers traceable outputs, minimizes hallucination, and demonstrates robustness across protocols. Future directions include clinician-led validation, and improved domain-adapted retrieval models to enhance real-world integration.', 'abstract_zh': '目的：开发由LLaMA-4 109B驱动的检索增强生成（RAG）系统，以实现自动化、协议意识和可解释的放射治疗计划评估。', 'title_zh': '基于LLaMA-4 109B的自动化检索增强生成系统用于评估放射治疗计划'}
{'arxiv_id': 'arXiv:2509.20652', 'title': 'Accelerate Creation of Product Claims Using Generative AI', 'authors': 'Po-Yu Liang, Yong Zhang, Tatiana Hwa, Aaron Byers', 'link': 'https://arxiv.org/abs/2509.20652', 'abstract': "The benefit claims of a product is a critical driver of consumers' purchase behavior. Creating product claims is an intense task that requires substantial time and funding. We have developed the $\\textbf{Claim Advisor}$ web application to accelerate claim creations using in-context learning and fine-tuning of large language models (LLM). $\\textbf{Claim Advisor}$ was designed to disrupt the speed and economics of claim search, generation, optimization, and simulation. It has three functions: (1) semantically searching and identifying existing claims and/or visuals that resonate with the voice of consumers; (2) generating and/or optimizing claims based on a product description and a consumer profile; and (3) ranking generated and/or manually created claims using simulations via synthetic consumers. Applications in a consumer packaged goods (CPG) company have shown very promising results. We believe that this capability is broadly useful and applicable across product categories and industries. We share our learning to encourage the research and application of generative AI in different industries.", 'abstract_zh': '产品的收益声称是驱动消费者购买行为的关键因素。创建产品声称是一项艰巨的任务，需要大量时间与资金。我们开发了名为$\\textbf{Claim Advisor}$的网络应用，利用上下文学习和大型语言模型（LLM）的微调来加速声称的创建。$\\textbf{Claim Advisor}$旨在颠覆声称搜索、生成、优化和模拟的速度与经济性。它具有三项功能：(1) 语义搜索和识别与消费者声音共鸣的现有声称和/或视觉内容；(2) 根据产品描述和消费者画像生成和/或优化声称；以及(3) 通过合成消费者进行模拟的生成和/或手动创建的声称排名。在消费品公司中的应用显示出非常有前景的结果。我们认为这项能力在各类产品和行业中的应用非常广泛。我们分享我们的学习经验，以促进生成式AI在不同行业的研究与应用。', 'title_zh': '使用生成式AI加速产品声明创建'}
{'arxiv_id': 'arXiv:2509.20562', 'title': 'SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection', 'authors': 'Yubin Ge, Salvatore Romeo, Jason Cai, Monica Sunkara, Yi Zhang', 'link': 'https://arxiv.org/abs/2509.20562', 'abstract': 'Despite the rapid advancements in LLM agents, they still face the challenge of generating meaningful reflections due to inadequate error analysis and a reliance on rare successful trajectories, especially in complex tasks. In this work, we propose SAMULE, a new framework for self-learning agents powered by a retrospective language model that is trained based on Multi-Level Reflection Synthesis. It first synthesizes high-quality reflections across three complementary levels: Single-Trajectory Learning (micro-level) for detailed error correction; Intra-Task Learning (meso-level) to build error taxonomies across multiple trials of the same task, and Inter-Task Learning (macro-level) to extract transferable insights based on same typed errors from diverse task failures. Then we fine-tune a language model serving as the retrospective model to generate reflections during inference. We further extend our framework to interactive settings through a foresight-based reflection mechanism, enabling agents to proactively reflect and adapt during user interactions by comparing predicted and actual responses. Extensive experiments on three challenging benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our approach significantly outperforms reflection-based baselines. Our results highlight the critical role of well-designed reflection synthesis and failure-centric learning in building self-improving LLM agents.', 'abstract_zh': '尽管大语言模型代理取得了迅速进展，但由于缺乏充分的错误分析和依赖稀有的成功轨迹，它们仍然面临生成有意义反思的挑战，特别是在复杂任务中。本文提出了一种新的SAMULE框架，该框架基于多级反思合成训练回顾性语言模型，为自我学习代理提供动力。该框架首先在三个互补层级上合成了高质量的反思：单轨迹学习（微观层级）进行详细的错误修正；任务内学习（中观层级）建立同一任务多次试验中的错误分类体系；跨任务学习（宏观层级）从不同任务失败中提取可迁移的见解。然后，我们微调一个语言模型作为回顾性模型，在推断过程中生成反思。我们进一步通过前瞻性的反思机制将该框架扩展到交互式设置中，使代理能够在用户交互过程中通过比较预测和实际响应来主动反思和调整。在三个具有挑战性的基准测试——TravelPlanner、NATURAL PLAN和Tau-bench——上的广泛实验表明，我们的方法显著优于基于反思的基线。我们的结果强调了精心设计的反思合成和以失败为中心的学习在构建自我改进的大语言模型代理中的关键作用。', 'title_zh': 'SAMULE: 自学习代理增强的多级反思方法'}
{'arxiv_id': 'arXiv:2509.20493', 'title': 'InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature', 'authors': 'Paris Koloveas, Serafeim Chatzopoulos, Thanasis Vergoulis, Christos Tryfonopoulos', 'link': 'https://arxiv.org/abs/2509.20493', 'abstract': 'The proliferation of scientific literature presents an increasingly significant challenge for researchers. While Large Language Models (LLMs) offer promise, existing tools often provide verbose summaries that risk replacing, rather than assisting, the reading of the source material. This paper introduces InsightGUIDE, a novel AI-powered tool designed to function as a reading assistant, not a replacement. Our system provides concise, structured insights that act as a "map" to a paper\'s key elements by embedding an expert\'s reading methodology directly into its core AI logic. We present the system\'s architecture, its prompt-driven methodology, and a qualitative case study comparing its output to a general-purpose LLM. The results demonstrate that InsightGUIDE produces more structured and actionable guidance, serving as a more effective tool for the modern researcher.', 'abstract_zh': '科学文献的 proliferate 为研究人员提出了日益显著的挑战。尽管大型语言模型 (LLMs) 具有潜力，现有的工具往往提供冗长的摘要，这可能会替代而非辅助阅读原始材料。本文介绍了一种名为 InsightGUIDE 的新型 AI 助力工具，旨在作为阅读助手而非替代品。我们的系统提供简洁、结构化的洞见，通过将专家的阅读方法直接嵌入其核心 AI 逻辑中，起到“地图”的作用，指向论文的关键要素。我们介绍了系统的架构、基于提示的方法以及与通用语言模型输出的定性案例研究。结果表明，InsightGUIDE 生成了更加结构化和可操作的指导，成为现代研究人员更有效的工具。', 'title_zh': 'InsightGUIDE: 一套偏见导向的AI辅助工具，用于指导性的科学文献批判性阅读'}
{'arxiv_id': 'arXiv:2509.20368', 'title': 'LATTS: Locally Adaptive Test-Time Scaling', 'authors': 'Theo Uscidda, Matthew Trager, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto', 'link': 'https://arxiv.org/abs/2509.20368', 'abstract': 'One common strategy for improving the performance of Large Language Models (LLMs) on downstream tasks involves using a \\emph{verifier model} to either select the best answer from a pool of candidates or to steer the auto-regressive generation process towards better outputs. This class of methods typically results in improved accuracy at the cost of increased computation at test-time, a paradigm known as \\emph{test-time scaling}. However, most existing approaches increase computation uniformly across all samples and generation steps, without considering the complexity of individual instances, leading to inefficient resource use. We address this limitation by proposing an approach, called \\emph{Locally Adaptive Test-Time Scaling (LATTS)}, that allocates variable compute across generation steps. Specifically, at each generation step, LATTS employs a verifier-based acceptance criterion to decide whether to resample, backtrack, restart, or stop the generation process. This criterion effectively adjusts the per-step computational effort based on a precise notion of \\emph{local difficulty} derived from the verifier model. Empirical results show that LATTS achieves significantly superior accuracy--compute tradeoffs compared to standard verifier-based methods.', 'abstract_zh': '局部自适应测试时缩放（LATTS）：基于校验模型的可变计算分配方法', 'title_zh': 'LATTS：局部自适应测试时缩放'}
{'arxiv_id': 'arXiv:2509.20364', 'title': 'An Approach to Checking Correctness for Agentic Systems', 'authors': 'Thomas J Sheffler', 'link': 'https://arxiv.org/abs/2509.20364', 'abstract': "This paper presents a temporal expression language for monitoring AI agent behavior, enabling systematic error-detection of LLM-based agentic systems that exhibit variable outputs due to stochastic generation processes. Drawing from temporal logic techniques used in hardware verification, this approach monitors execution traces of agent tool calls and state transitions to detect deviations from expected behavioral patterns. Current error-detection approaches rely primarily on text matching of inputs and outputs, which proves fragile due to the natural language variability inherent in LLM responses. The proposed method instead focuses on the sequence of agent actions -- such as tool invocations and inter-agent communications -- allowing verification of system behavior independent of specific textual outputs. The temporal expression language provides assertions that capture correct behavioral patterns across multiple execution scenarios. These assertions serve dual purposes: validating prompt engineering and guardrail effectiveness during development, and providing regression testing when agents are updated with new LLMs or modified logic. The approach is demonstrated using a three-agent system, where agents coordinate to solve multi-step reasoning tasks. When powered by large, capable models, all temporal assertions were satisfied across many test runs. However, when smaller models were substituted in two of the three agents, executions violated behavioral assertions, primarily due to improper tool sequencing and failed coordination handoffs. The temporal expressions successfully flagged these anomalies, demonstrating the method's effectiveness for detecting behavioral regressions in production agentic systems. This approach provides a foundation for systematic monitoring of AI agent reliability as these systems become increasingly deployed in critical applications.", 'abstract_zh': '一种用于监控AI代理行为的时间表达式语言：基于随机生成过程导致输出变化的LLM基础代理系统系统的系统错误检测方法', 'title_zh': '检查代理系统正确性的方法'}
{'arxiv_id': 'arXiv:2509.21319', 'title': 'RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards', 'authors': 'Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev', 'link': 'https://arxiv.org/abs/2509.21319', 'abstract': 'Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).', 'abstract_zh': '基于人类反馈的强化学习（RLHF）和可验证奖励的强化学习（RLVR）是LLM后训练中使用的主要RL范式，各自具有独特的优势。然而，RLHF在可解释性和奖励作弊方面存在挑战，因为它依赖于通常缺乏明确标准的人类判断；而RLVR则受限于其基于正确性验证的范围。我们提出了一种基于二元灵活反馈的强化学习（RLBFF），将人类驱动的偏好多样性与基于规则的验证精确性结合，使奖励模型能够捕捉到响应质量的细微方面，而不仅仅是正确性。RLBFF从自然语言反馈中提取可以用二元方式回答的原则（例如：信息准确性：是，或代码可读性：否），这些原则可以作为归结任务（响应是否满足某个任意原则）来训练奖励模型。我们展示，以这种方式训练的奖励模型在数据调整后的性能可以超越布拉德利-特里模型，并在RM-Bench（86.2%）和JudgeBench（81.4%，截至2025年9月24日排行榜第一）上达到最佳性能。此外，在推理时用户可以指定感兴趣的原则来定制我们的奖励模型的焦点，不同于布拉德利-特里模型。最后，我们提供了一个完整的开源食谱（包括数据），使用RLBFF和我们的奖励模型对Qwen3-32B进行对齐，匹配或超越o3-mini和DeepSeek R1在MT-Bench、WildBench和Arena Hard v2上的通用对齐基准性能（成本不到5%）。', 'title_zh': 'RLBFF：二值灵活反馈以缓解人类反馈与可验证奖励之间的差距'}
{'arxiv_id': 'arXiv:2509.21282', 'title': "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL", 'authors': 'Madeleine Dwyer, Adam Sobey, Adriane Chapman', 'link': 'https://arxiv.org/abs/2509.21282', 'abstract': "Training large language models (LLMs) with reinforcement learning (RL) methods such as PPO and GRPO commonly relies on ratio clipping to stabilise updates. While effective at preventing instability, clipping discards information and introduces gradient discontinuities. We propose Probability Smoothing Policy Optimisation (PSPO), which smooths the current policy's probabilities toward the old (behaviour) policy before computing the importance ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient signal, while interpolation toward the old policy creates a soft trust region that discourages large, destabilising updates, with formal guarantees.\nWe instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO (single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar performance but improves the reasoning leading to clearer and more concise responses which are more logical. Compared to clipped GRPO, GR-PSPO substantially improves performance both the 0.5B and 1.5B models, with a boost of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).", 'abstract_zh': '使用PPO和GRPO等强化学习方法训练大型语言模型（LLMs）通常依赖于比率剪切以稳定更新。我们提出了概率平滑策略优化（PSPO），该方法在计算重要性比率之前，将当前策略的概率平滑至旧的行为策略，类似于标签平滑。与比率剪切不同，PSPO保留了梯度信号，而向旧策略的插值创建了一个软信任区域，这会避免大型且不稳定的更新，并且具有形式上的保证。', 'title_zh': '不是你，是裁剪：基于概率平滑的软信任区域方法用于大型语言模型的RL'}
{'arxiv_id': 'arXiv:2509.21275', 'title': 'Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training', 'authors': 'Shiju Wang, Yujie Wang, Ao Sun, Fangcheng Fu, Zijian Zhu, Bin Cui, Xu Han, Kaisheng Ma', 'link': 'https://arxiv.org/abs/2509.21275', 'abstract': "Long context training is crucial for LLM's context extension. Existing schemes, such as sequence parallelism, incur substantial communication overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness hinges on partitioning granularity. Batch-level PP dividing input samples exhibits high memory consumption in long-context scenario, whereas token-level PP splitting sequences into slices alleviates memory overhead but may incur hardware under-utilization. This trade-off motivates adaptively selecting PP granularity to match resource and workload characteristics. Moreover, sequence length distribution of the real-world dataset exhibits skewness, posing a challenge on PP's workload balance and efficient scheduling. Current static PP scheduling methods overlook the variance of sequence length, leading to suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism (EPP) that orchestrates token-level PP and batch-level PP to adapt to resource and workload heterogeneity. We build InfiniPipe, a distributed training system that unleashes the potential of EPP via (1) a resource-aware and workload-balanced sequence processor that splits long sequences and packs short ones; and (2) a co-optimization methodology that jointly optimizes pipeline schedule and gradient checkpointing via a mechanism named stage-aware chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.", 'abstract_zh': '弹性管道并行性（EPP）：适应资源和工作负载异构性的动态管道并行性', 'title_zh': '以数据为中心的弹性管道并行训练高效处理长上下文LLM模型'}
{'arxiv_id': 'arXiv:2509.21259', 'title': 'Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks', 'authors': 'Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Aisha Syed, Matthew Andrews, Sean Kennedy', 'link': 'https://arxiv.org/abs/2509.21259', 'abstract': 'Real-time urban traffic surveillance is vital for Intelligent Transportation Systems (ITS) to ensure road safety, optimize traffic flow, track vehicle trajectories, and prevent collisions in smart cities. Deploying edge cameras across urban environments is a standard practice for monitoring road conditions. However, integrating these with intelligent models requires a robust understanding of dynamic traffic scenarios and a responsive interface for user interaction. Although multimodal Large Language Models (LLMs) can interpret traffic images and generate informative responses, their deployment on edge devices is infeasible due to high computational demands. Therefore, LLM inference must occur on the cloud, necessitating visual data transmission from edge to cloud, a process hindered by limited bandwidth, leading to potential delays that compromise real-time performance. To address this challenge, we propose a semantic communication framework that significantly reduces transmission overhead. Our method involves detecting Regions of Interest (RoIs) using YOLOv11, cropping relevant image segments, and converting them into compact embedding vectors using a Vision Transformer (ViT). These embeddings are then transmitted to the cloud, where an image decoder reconstructs the cropped images. The reconstructed images are processed by a multimodal LLM to generate traffic condition descriptions. This approach achieves a 99.9% reduction in data transmission size while maintaining an LLM response accuracy of 89% for reconstructed cropped images, compared to 93% accuracy with original cropped images. Our results demonstrate the efficiency and practicality of ViT and LLM-assisted edge-cloud semantic communication for real-time traffic surveillance.', 'abstract_zh': '实时城市交通监控对于智能 transportation 系统（ITS）确保道路安全、优化交通流量、追踪车辆轨迹以及预防碰撞至关重要。在城市环境中部署边缘摄像头以监控道路状况是一种标准做法。然而，将这些摄像头与智能模型集成需要对动态交通场景有 robust 的理解以及对用户交互具有响应式的界面。尽管多模态大型语言模型（LLMs）可以解释交通图像并生成有意义的响应，但由于高计算需求，在边缘设备上的部署是不可行的。因此，LLM 的推断必须在云端进行，这需要从边缘到云端传输视觉数据，但受限于有限的带宽，这一过程可能导致潜在的延迟，从而影响实时性能。为解决这一挑战，我们提出了一种语义通信框架，显著减少了数据传输开销。该方法涉及使用 YOLOv11 检测兴趣区域（RoIs）、裁剪相关图像段，并使用 Vision Transformer（ViT）将这些图像段转换为紧凑的嵌入向量。这些嵌入向量随后传输到云端，在云端使用图像解码器重建裁剪图像。重建后的图像由多模态 LLM 处理以生成交通状况描述。该方法在数据传输大小上实现了 99.9% 的减少，同时在重建裁剪图像时，LLM 的响应准确率为 89%，与原始裁剪图像相比，准确率为 93%。我们的结果表明，ViT 和 LLM 辅助的边缘-云端语义通信适用于实时交通监控。', 'title_zh': '基于ViT和LLMs的移动网络实时城市交通监控的语义边缘-云通信'}
{'arxiv_id': 'arXiv:2509.21241', 'title': 'Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework', 'authors': 'Yucheng Wang, Ziyang Chen, Md Faisal Kabir', 'link': 'https://arxiv.org/abs/2509.21241', 'abstract': "The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large language models (LLMs) to acquire domain-specific knowledge with remarkable efficiency. However, understanding how such a fine-tuning mechanism alters a model's structural reasoning and semantic behavior remains an open challenge. This work introduces a novel framework that explains fine-tuned LLMs via counterfactuals grounded in knowledge graphs. Specifically, we construct BioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics tools and design a counterfactual-based fine-tuned LLMs explainer (CFFTLLMExplainer) that learns soft masks over graph nodes and edges to generate minimal structural perturbations that induce maximum semantic divergence. Our method jointly optimizes structural sparsity and semantic divergence while enforcing interpretability preserving constraints such as entropy regularization and edge smoothness. We apply this framework to a fine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the model's structural dependencies and aligns with LoRA-induced parameter shifts. This work provides new insights into the internal mechanisms of fine-tuned LLMs and highlights counterfactual graphs as a potential tool for interpretable AI.", 'abstract_zh': 'Low-Rank Adaptation知识图谱引导的反事实解释框架：细调大型语言模型的结构依赖与语义偏差解析', 'title_zh': '基于知识图谱的反事实解释框架：细调大型语言模型'}
{'arxiv_id': 'arXiv:2509.21240', 'title': 'Tree Search for LLM Agent Reinforcement Learning', 'authors': 'Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu', 'link': 'https://arxiv.org/abs/2509.21240', 'abstract': 'Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.', 'abstract_zh': 'Recent Advances in Reinforcement Learning Have Significantly Enhanced the Agentic Capabilities of Large Language Models. To Address the Challenge of Sparse Supervision in Long-Term and Multi-Turn Agent Tasks, We Propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a Grouped Agent RL Method Based on Tree Search.', 'title_zh': '树搜索在大型语言模型代理强化学习中的应用'}
{'arxiv_id': 'arXiv:2509.21193', 'title': 'Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning', 'authors': 'Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin', 'link': 'https://arxiv.org/abs/2509.21193', 'abstract': 'Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity\'s Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: this https URL.', 'abstract_zh': '大型语言模型在科学研究中已显示出强大的推理能力，但仍存在两大瓶颈。首先，显式的检索会中断推理，导致额外的令牌和步骤。其次，多代理管道常通过平均所有候选方案来稀释强大的解决方案。我们通过结合隐式检索和结构化协作的统一框架来应对这些挑战。该框架的基础是一个基于监控的检索模块，在最低限度地干扰推理的前提下，整合外部知识。在此基础上，层次化解决方案细化（HSR）逐次将每个候选方案指定为锚点并通过其他候选方案进行修正，而质量意识迭代推理（QAIR）调整细化以适应解决方案的质量。在《人类的最后一考》（HLE）生物/化学黄金标准数据集上，我们的框架实现了48.3%的准确率，这是迄今最高记录，比最强的单一代理基线高出13.4个百分点，并且领先前沿的大规模语言模型多达18.1个百分点，同时减少了53.5%的令牌使用和43.7%的代理步骤。在SuperGPQA和TRQA上获得的结果证实了其在不同领域的鲁棒性。错误分析显示，推理失败和知识空白在同一案例中共同出现的比例超过85%，而多样性分析揭示了一个清晰的二分法：检索任务受益于多样化的解决方案，而推理任务更倾向于共识。这些发现共同表明，隐式增强和结构化细化如何克服显式工具使用和均匀聚合的低效性。代码可在以下链接获取：this https URL。', 'title_zh': 'Eigen-1: 基于监测的RAG多agent自适应细化方法及其在科学推理中的应用'}
{'arxiv_id': 'arXiv:2509.21170', 'title': 'Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach', 'authors': 'Yongda Yu, Guohao Shi, Xianwei Wu, Haochuan He, XueMing Gu, Qianqian Zhao, Kui Liu, Qiushi Wang, Zhao Tian, Haifeng Shen, Guoping Rong', 'link': 'https://arxiv.org/abs/2509.21170', 'abstract': 'Large Language Models (LLMs) have shown great potential in supporting automated code review due to their impressive capabilities in context understanding and reasoning. However, these capabilities are still limited compared to human-level cognition because they are heavily influenced by the training data. Recent research has demonstrated significantly improved performance through fine-tuning LLMs with code review data. However, compared to human reviewers who often simultaneously analyze multiple dimensions of code review to better identify issues, the full potential of these methods is hampered by the limited or vague information used to fine-tune the models. This paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that trains LLMs with an impressive reasoning ability to analyze multiple dimensions of code review by harnessing long COT techniques to provide rich structured information. To address context loss and reasoning logic loss issues that frequently occur when LLMs process long COT prompts, we propose a solution that combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning pathways in MelcotCR to enable more effective utilization of in-context knowledge within long COT prompts while strengthening the logical tightness of the reasoning process. Empirical evaluations on our curated MelcotCR dataset and the public CodeReviewer dataset reveal that a low-parameter base model, such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art methods in terms of the accuracy of detecting and describing code issues, with its performance remarkably on par with that of the 671B DeepSeek-R1 model.', 'abstract_zh': '大型语言模型（LLMs）在支持自动化代码审查方面显示出巨大的潜力，得益于其在上下文理解与推理能力方面的 impressive 表现。然而，这些能力仍受限于训练数据，无法达到人类认知水平。近期研究通过使用代码审查数据对 LLMs 进行微调，显著提升了其性能。然而，相比于常能同时分析多个代码审查维度的人类审查者，这些方法受限于用于微调模型的有限或模糊信息，未能充分发挥潜力。本文贡献了一种名为 MelcotCR 的chain-of-thought（CoT）微调方法，通过利用长 CoT 技术提供丰富的结构化信息，训练 LLMs 以分析代码审查的多个维度。为了解决 LLMs 在处理长 CoT 提示时经常出现的上下文损失和推理逻辑损失问题，提出了结合最大熵（ME）建模原则和预定义推理路径的方法，以在长 CoT 提示中更有效地利用上下文知识，并增强推理过程的逻辑紧密性。在我们编纂的 MelcotCR 数据集和公开的 CodeReviewer 数据集上的实证评估表明，使用 MelcotCR 微调的低参数基础模型（如 14B Qwen2.5），在检测和描述代码问题的准确性方面可超越现有先进方法，其性能显著接近 671B DeepSeek-R1 模型。', 'title_zh': '细调大型语言模型以分析代码审查的多维度：一种最大熵调节长链思考方法'}
{'arxiv_id': 'arXiv:2509.21154', 'title': 'GRPO is Secretly a Process Reward Model', 'authors': 'Michael Sullivan', 'link': 'https://arxiv.org/abs/2509.21154', 'abstract': 'We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and show that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$-$and reach peak performance more rapidly$-$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost.', 'abstract_zh': '我们证明，在某些假设条件下，GRPO RL算法会产生一个非平凡的过程奖励模型（PRM）。随后的实验证明了这些假设在实际条件中成立：GRPO确实诱导了一个非平凡的PRM。利用GRPO-as-a-PRM的框架，我们发现了GRPO目标函数的一个缺陷：非均匀分布的过程步骤既妨碍探索又妨碍利用（在不同条件下）。我们提出了一种简单的算法修改（$\\lambda$-GRPO）来弥补这一缺陷，并展示了使用$\\lambda$-GRPO训练的语言模型在下游推理任务中的验证准确性更高、性能更好，并且达到最佳性能的速度更快。我们的结果质疑了成本高昂且显式定义的PRM对GRPO的优势：我们证明了可以通过利用 vanilla GRPO算法中的隐藏且内置的PRM结构来提升模型性能，同时对训练时间和成本的影响可以忽略不计。', 'title_zh': 'GRPO秘密上是一个过程奖励模型'}
{'arxiv_id': 'arXiv:2509.21091', 'title': 'Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute', 'authors': 'Junpei Komiyama, Daisuke Oba, Masafumi Oyamada', 'link': 'https://arxiv.org/abs/2509.21091', 'abstract': 'We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \\to \\infty$, which we denote as Best-of-$\\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.', 'abstract_zh': '我们研究基于多数投票的大语言模型（LLM）最佳-of-$N$方法，并分析其在$N \\to \\infty$极限情况下的表现，即所谓的最佳-of-$\\infty$。虽然这种方法在极限情况下能取得出色的性能，但需要无限的测试计算预算。为解决这一问题，我们提出了一种自适应生成方案，根据答案一致性的程度选择$N$，从而高效地分配推理计算资源。此外，我们扩展了框架以应用于多个LLM的加权组合，并证明这种组合方法可以优于任何单独的模型。我们将最优的组合加权形式化并高效地作为混合整数线性规划问题进行求解。大量的实验验证了我们方法的有效性。', 'title_zh': 'Best-of-$\\infty$ —— 测试时计算的渐近性能'}
{'arxiv_id': 'arXiv:2509.21081', 'title': 'TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix', 'authors': 'Ahmet Caner Yüzügüler, Ahmet Çelik, Jiawei Zhuang, Lukas Cavigelli', 'link': 'https://arxiv.org/abs/2509.21081', 'abstract': 'Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in state-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel formulation, MLA allows two functionally equivalent but computationally distinct kernel implementations: naive and absorb. While the naive kernels (e.g., FlashAttention) are typically preferred in training and prefill for their computational efficiency, existing decoding kernels (e.g., FlashMLA) rely on the absorb method to minimize HBM bandwidth usage. However, the compute-bound nature of the absorb implementations prohibits performance benefits from data reuse opportunities in attention calculations, such as shared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that combines naive and absorb formulations to harness the strengths of both. TyphoonMLA effectively leverages the shared prefix by applying the naive formulation to the compute-bound parts of attention calculations, while reducing the bandwidth requirements for non-shared parts by using the absorb formulation. As a result, TyphoonMLA improves the throughput of attention calculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with only a 3% overhead in HBM size.', 'abstract_zh': 'TyphoonMLA: 结合 naive 和 absorb 公式的混合多头潜在注意力机制', 'title_zh': 'TyphoonMLA: 一种混合朴素吸收前缀的MLA内核'}
{'arxiv_id': 'arXiv:2509.21080', 'title': 'Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs', 'authors': 'Yixin Wan, Xingrun Chen, Kai-Wei Chang', 'link': 'https://arxiv.org/abs/2509.21080', 'abstract': "Large language models (LLMs) have unlocked a wide range of downstream generative applications. However, we found that they also risk perpetuating subtle fairness issues tied to culture, positioning their generations from the perspectives of the mainstream US culture while demonstrating salient externality towards non-mainstream ones. In this work, we identify and systematically investigate this novel culture positioning bias, in which an LLM's default generative stance aligns with a mainstream view and treats other cultures as outsiders. We propose the CultureLens benchmark with 4000 generation prompts and 3 evaluation metrics for quantifying this bias through the lens of a culturally situated interview script generation task, in which an LLM is positioned as an onsite reporter interviewing local people across 10 diverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a stark pattern: while models adopt insider tones in over 88 percent of US-contexted scripts on average, they disproportionately adopt mainly outsider stances for less dominant cultures. To resolve these biases, we propose 2 inference-time mitigation methods: a baseline prompt-based Fairness Intervention Pillars (FIP) method, and a structured Mitigation via Fairness Agents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent) introduces a self-reflection and rewriting loop based on fairness guidelines. (2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized agents: a Planner Agent(initial script generation), a Critique Agent (evaluates initial script against fairness pillars), and a Refinement Agent (incorporates feedback to produce a polished, unbiased script). Empirical results showcase the effectiveness of agent-based methods as a promising direction for mitigating biases in generative LLMs.", 'abstract_zh': '大型语言模型（LLMs）解锁了广泛的手动生成应用。然而，我们发现它们也存在与文化相关的微妙公平性问题，倾向于从主流美国文化的角度生成内容，对外来文化表现出明显的外部效应。在本工作中，我们识别并系统研究了这一新的文化定位偏见，即LLM的默认生成立场与主流观点一致，并将其他文化视为外来文化。我们提出了一个具有4000个生成提示和3个评估指标的CultureLens基准，通过基于文化嵌入的采访脚本生成任务来衡量这种偏见，其中LLM被定位为现场记者，采访来自10个不同文化群体的当地人。对5个最先进的LLM进行的实证评估揭示了一个明显的模式：尽管模型在超过88%的以美国为背景的脚本中采用内行人语气，但对外来文化却采用了主要的外行人立场。为了缓解这些偏见，我们提出了两种推理时间缓解方法：基于提示的基本公平干预支柱（Fairness Intervention Pillars, FIP）方法，以及包含两个管道的结构化公平代理框架（Mitigation via Fairness Agents, MFA）：MFA-SA（单代理）引入基于公平准则的自我反思和重写循环，MFA-MA（多代理）将过程结构化为专门代理的层级体系：规划代理（初始脚本生成）、批评代理（评估初始脚本以符合公平支柱）和润色代理（纳入反馈以生成一份精致且无偏见的脚本）。实证结果展示了基于代理的方法作为缓解生成LLM中偏见有希望的方向的有效性。', 'title_zh': '模型采用哪种文化视角？关于大语言模型中的文化定位偏见及代理缓解机制'}
{'arxiv_id': 'arXiv:2509.21075', 'title': 'Communication Bias in Large Language Models: A Regulatory Perspective', 'authors': 'Adrian Kuenzler, Stefan Schmid', 'link': 'https://arxiv.org/abs/2509.21075', 'abstract': "Large language models (LLMs) are increasingly central to many applications, raising concerns about bias, fairness, and regulatory compliance. This paper reviews risks of biased outputs and their societal impact, focusing on frameworks like the EU's AI Act and the Digital Services Act. We argue that beyond constant regulation, stronger attention to competition and design governance is needed to ensure fair, trustworthy AI. This is a preprint of the Communications of the ACM article of the same title.", 'abstract_zh': '大型语言模型（LLMs）在许多应用中变得越来越重要，引发了关于偏见、公平性和监管合规性的关注。本文回顾了偏差输出带来的风险及其社会影响，重点关注如欧盟AI法案和数字服务法案等框架。我们argue认为，除了持续的监管外，还需要更多关注竞争和设计治理，以确保公平可信的AI。', 'title_zh': '大型语言模型中的communication偏见：一个监管视角'}
{'arxiv_id': 'arXiv:2509.21044', 'title': 'Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs', 'authors': 'Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li', 'link': 'https://arxiv.org/abs/2509.21044', 'abstract': 'Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families shows two robust effects of online RL post-training: (i) an overall increase in activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at this https URL.', 'abstract_zh': '大型语言模型（LLMs）通过大规模预训练获得丰富的先验知识，并可通过监督微调（SFT）或基于强化学习（RL）的后训练进一步提升。越来越多的证据表明，基于RL的微调能够超越单纯的SFT，提升LLMs的能力。然而，为什么基于RL的微调能够增强具有不同固有特征的各种LLMs的能力，其背后的机制仍缺乏深入探索。在本研究中，我们借鉴边属性补丁（EAP）的相关工作，探讨RL后训练前后LLMs内部差异。在多种模型家族的分析中，我们发现在线RL后训练的两个稳健效应：（i）激活强度总体增加，表明更多内部路径被激活且信号强度增强；（ii）激活模式的多样性增加，表现为熵值更高且边分布更不集中。这些变化表明，RL重新塑造了信息流，使其既更具冗余性又更具灵活性，这可能解释了其在泛化能力上的优势。值得注意的是，使用直接偏好优化（DPO）微调的模型偏离了这些趋势，显示出与PPO-和GRPO-为基础的训练相比，内部变化显著较弱或不一致。我们的研究结果提供了RL后训练系统地改变LLMs内部电路的统一视角，并突出了在线RL与偏好优化方法在方法论上的区别。我们的代码在此公开。', 'title_zh': '强化学习微调增强大型语言模型内部电路的激活强度和多样性'}
{'arxiv_id': 'arXiv:2509.21040', 'title': 'Generative AI for FFRDCs', 'authors': 'Arun S. Maiya', 'link': 'https://arxiv.org/abs/2509.21040', 'abstract': 'Federally funded research and development centers (FFRDCs) face text-heavy workloads, from policy documents to scientific and engineering papers, that are slow to analyze manually. We show how large language models can accelerate summarization, classification, extraction, and sense-making with only a few input-output examples. To enable use in sensitive government contexts, we apply OnPrem$.$LLM, an open-source framework for secure and flexible application of generative AI. Case studies on defense policy documents and scientific corpora, including the National Defense Authorization Act (NDAA) and National Science Foundation (NSF) Awards, demonstrate how this approach enhances oversight and strategic analysis while maintaining auditability and data sovereignty.', 'abstract_zh': '联邦资助的研究与发展中心（FFRDCs）面临大量文本密集型工作负载，从政策文件到科学与工程论文，手工分析速度较慢。我们展示了大规模语言模型如何仅通过少量输入-输出示例就能加速摘要、分类、提取和意义构建。为了在敏感的政府背景下应用，我们采用了OnPrem$.$LLM这一开源框架，以确保生成式AI的安全和灵活应用。案例研究涉及国防政策文件和科学语料库，包括国防授权法案（NDAA）和国家科学基金会（NSF）奖助金，证明了该方法如何在确保审计性和数据主权的同时增强监督和战略分析。', 'title_zh': '生成式AI在FFRDCs中的应用'}
{'arxiv_id': 'arXiv:2509.21013', 'title': 'Predicting LLM Reasoning Performance with Small Proxy Model', 'authors': 'Woosung Koh, Juyoung Suk, Sungjun Han, Se-Young Yun, Jay Shin', 'link': 'https://arxiv.org/abs/2509.21013', 'abstract': 'Given the prohibitive cost of pre-training large language models, it is essential to leverage smaller proxy models to optimize datasets before scaling up. However, this approach becomes challenging for reasoning capabilities, which exhibit emergent behavior that only appear reliably at larger model sizes, often exceeding 7B parameters. To address this, we introduce rBridge, showing that small proxies ($\\leq$1B) can effectively predict large-model reasoning by aligning more closely with (1) the pre-training objective and (2) the target task. rBridge achieves this by weighting negative log-likelihood with task alignment, using reasoning traces from frontier models as gold labels. In our experiments, rBridge (i) reduces dataset ranking costs by over 100x relative to the best baseline, (ii) achieves the strongest correlation across six reasoning benchmarks at 1B to 32B scale, and (iii) zero-shot transfers predictive relationships across pre-training datasets at 1B to 7B scale. These findings indicate that rBridge offers a practical path for exploring reasoning-oriented pre-training at lower cost.', 'abstract_zh': '给定预训练大语言模型的成本 prohibitive，利用较小的代理模型优化数据集再扩展规模是至关重要的。然而，这种方法对于推理能力来说变得具有挑战性，因为这些能力在较大的模型规模（通常超过7B参数）下才会可靠地出现。为解决这一问题，我们引入了rBridge，表明较小的代理模型（≤1B）可以通过更紧密地与（1）预训练目标和（2）目标任务对齐来有效地预测大型模型的推理。rBridge 通过使用前沿模型的推理轨迹作为黄金标签，对负对数似然度进行加权以实现这一目标。在我们的实验中，rBridge （i）将数据集排序成本相对于最佳基线降低了超过100倍，（ii）在1B到32B规模的六个推理基准测试中实现了最强的相关性，（iii）在1B到7B规模的预训练数据集之间实现了零样本的知识迁移。这些发现表明，rBridge 提供了一条在较低成本下探索面向推理的预训练的实际路径。', 'title_zh': '使用小型代理模型预测LLM推理性能'}
{'arxiv_id': 'arXiv:2509.21012', 'title': 'Mechanism of Task-oriented Information Removal in In-context Learning', 'authors': 'Hakaze Cho, Haolin Yang, Gouki Minegishi, Naoya Inoue', 'link': 'https://arxiv.org/abs/2509.21012', 'abstract': 'In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.', 'abstract_zh': '基于信息删除视角的上下文学习机制探究', 'title_zh': '任务导向的信息移除机制在上下文学习中的作用'}
{'arxiv_id': 'arXiv:2509.21011', 'title': 'Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools', 'authors': 'Ping He, Changjiang Li, Binbin Zhao, Tianyu Du, Shouling Ji', 'link': 'https://arxiv.org/abs/2509.21011', 'abstract': 'The remarkable capability of large language models (LLMs) has led to the wide application of LLM-based agents in various domains. To standardize interactions between LLM-based agents and their environments, model context protocol (MCP) tools have become the de facto standard and are now widely integrated into these agents. However, the incorporation of MCP tools introduces the risk of tool poisoning attacks, which can manipulate the behavior of LLM-based agents. Although previous studies have identified such vulnerabilities, their red teaming approaches have largely remained at the proof-of-concept stage, leaving the automatic and systematic red teaming of LLM-based agents under the MCP tool poisoning paradigm an open question. To bridge this gap, we propose AutoMalTool, an automated red teaming framework for LLM-based agents by generating malicious MCP tools. Our extensive evaluation shows that AutoMalTool effectively generates malicious MCP tools capable of manipulating the behavior of mainstream LLM-based agents while evading current detection mechanisms, thereby revealing new security risks in these agents.', 'abstract_zh': '自动生成恶意MCP工具的自动红队框架：LLM基于代理下的工具中毒攻击研究', 'title_zh': '基于模型上下文协议工具的自动红队LLM代理训练'}
{'arxiv_id': 'arXiv:2509.20997', 'title': 'Binary Autoencoder for Mechanistic Interpretability of Large Language Models', 'authors': 'Hakaze Cho, Haolin Yang, Brian M. Kurkoski, Naoya Inoue', 'link': 'https://arxiv.org/abs/2509.20997', 'abstract': "Existing works are dedicated to untangling atomized numerical components (features) from the hidden states of Large Language Models (LLMs) for interpreting their mechanism. However, they typically rely on autoencoders constrained by some implicit training-time regularization on single training instances (i.e., $L_1$ normalization, top-k function, etc.), without an explicit guarantee of global sparsity among instances, causing a large amount of dense (simultaneously inactive) features, harming the feature sparsity and atomization. In this paper, we propose a novel autoencoder variant that enforces minimal entropy on minibatches of hidden activations, thereby promoting feature independence and sparsity across instances. For efficient entropy calculation, we discretize the hidden activations to 1-bit via a step function and apply gradient estimation to enable backpropagation, so that we term it as Binary Autoencoder (BAE) and empirically demonstrate two major applications: (1) Feature set entropy calculation. Entropy can be reliably estimated on binary hidden activations, which we empirically evaluate and leverage to characterize the inference dynamics of LLMs and In-context Learning. (2) Feature untangling. Similar to typical methods, BAE can extract atomized features from LLM's hidden states. To robustly evaluate such feature extraction capability, we refine traditional feature-interpretation methods to avoid unreliable handling of numerical tokens, and show that BAE avoids dense features while producing the largest number of interpretable ones among baselines, which confirms the effectiveness of BAE serving as a feature extractor.", 'abstract_zh': '现有的工作致力于从大型语言模型（LLMs）的隐藏状态下解开原子化的数值成分（特征），以解释其机制。然而，它们通常依赖于受限于某些隐式训练时间正则化的自动编码器（即$L_1$归一化、top-k函数等），而不提供实例间全局稀疏性的显式保证，导致大量密集的（同时不活跃）特征，损害了特征稀疏性和原子化。本文提出了一种新的自动编码器变体，强制对小批量隐藏激活的最小熵，从而促进实例间的特征独立性和稀疏性。为了高效地计算熵，我们通过步骤函数将隐藏激活离散化为1位，并应用梯度估计以实现反向传播，因此我们称之为二元自动编码器（BAE），并从两个主要应用中实证演示其效果：（1）特征集熵计算。二元隐藏激活上的熵可以可靠地估计，我们实证评估并利用其来表征LLMs和条件上下文学习的推理动态。（2）特征解缠。类似于传统方法，BAE可以从LLM的隐藏状态下提取原子化的特征。为了稳健地评估这种特征提取能力，我们改进了传统的特征解释方法以避免对数值标记的不可靠处理，并展示了BAE在生成可解释特征的数量上优于基线，从而验证了BAE作为特征提取器的有效性。', 'title_zh': '二进制自动编码器用于大型语言模型的机理可解释性'}
{'arxiv_id': 'arXiv:2509.20982', 'title': "Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting", 'authors': 'Valeria Ramirez-Garcia, David de-Fitero-Dominguez, Antonio Garcia-Cabot, Eva Garcia-Lopez', 'link': 'https://arxiv.org/abs/2509.20982', 'abstract': "Large language models (LLMs) can act as evaluators, a role studied by methods like LLM-as-a-Judge and fine-tuned judging LLMs. In the field of education, LLMs have been studied as assistant tools for students and teachers. Our research investigates LLM-driven automatic evaluation systems for academic Text-Input Problems using rubrics. We propose five evaluation systems that have been tested on a custom dataset of 110 answers about computer science from higher education students with three models: JudgeLM, Llama-3.1-8B and DeepSeek-R1-Distill-Llama-8B. The evaluation systems include: The JudgeLM evaluation, which uses the model's single answer prompt to obtain a score; Reference Aided Evaluation, which uses a correct answer as a guide aside from the original context of the question; No Reference Evaluation, which ommits the reference answer; Additive Evaluation, which uses atomic criteria; and Adaptive Evaluation, which is an evaluation done with generated criteria fitted to each question. All evaluation methods have been compared with the results of a human evaluator. Results show that the best method to automatically evaluate and score Text-Input Problems using LLMs is Reference Aided Evaluation. With the lowest median absolute deviation (0.945) and the lowest root mean square deviation (1.214) when compared to human evaluation, Reference Aided Evaluation offers fair scoring as well as insightful and complete evaluations. Other methods such as Additive and Adaptive Evaluation fail to provide good results in concise answers, No Reference Evaluation lacks information needed to correctly assess questions and JudgeLM Evaluations have not provided good results due to the model's limitations. As a result, we conclude that Artificial Intelligence-driven automatic evaluation systems, aided with proper methodologies, show potential to work as complementary tools to other academic resources.", 'abstract_zh': '大规模语言模型LLMs可以作为评估者，这一角色通过LLM-as-a-Judge等方法进行研究。在教育领域，LLMs被研究作为学生和教师的辅助工具。我们的研究探讨了基于LLM的自动评分系统在使用评分标准评价学术填空问题方面的方法。我们提出并测试了五个评分系统，这些系统基于一个包含110个来自高等教育学生关于计算机科学答案的自定义数据集，使用了三种模型：JudgeLM、Llama-3.1-8B和DeepSeek-R1-Distill-Llama-8B。评分系统包括：使用模型单个答案提示进行评分的JudgeLM评分；使用正确答案作为辅助参考的参考辅助评分；不使用参考答案的无参考评分；采用原子性标准的加性评分；以及适应评分，它是根据每个问题生成的适应性标准进行的评分。所有评分方法都与人工评分结果进行了比较。结果表明，在使用LLMs自动评分填空问题方面，参考辅助评分是最佳方法。与人工评分相比，参考辅助评分具有最低的中值绝对偏差（0.945）和最低的均方根偏差（1.214），提供了公平的评分以及深入和完整的评价。其他方法如加性评分和适应性评分在简洁答案上表现不佳，无参考评分缺乏正确评估问题所需的信息，而JudgeLM评分由于模型的限制未能提供良好的结果。因此，我们得出结论，适当方法驱动的人工智能自动评分系统具有作为其他学术资源补充工具的潜力。', 'title_zh': '基于指令的大型语言模型在学术环境中评估和评判文本输入问题的能力分析'}
{'arxiv_id': 'arXiv:2509.20975', 'title': 'Knowledgeable Language Models as Black-Box Optimizers for Personalized Medicine', 'authors': 'Michael S. Yao, Osbert Bastani, Alma Andersson, Tommaso Biancalani, Aïcha Bentaieb, Claudia Iriondo', 'link': 'https://arxiv.org/abs/2509.20975', 'abstract': "The goal of personalized medicine is to discover a treatment regimen that optimizes a patient's clinical outcome based on their personal genetic and environmental factors. However, candidate treatments cannot be arbitrarily administered to the patient to assess their efficacy; we often instead have access to an in silico surrogate model that approximates the true fitness of a proposed treatment. Unfortunately, such surrogate models have been shown to fail to generalize to previously unseen patient-treatment combinations. We hypothesize that domain-specific prior knowledge - such as medical textbooks and biomedical knowledge graphs - can provide a meaningful alternative signal of the fitness of proposed treatments. To this end, we introduce LLM-based Entropy-guided Optimization with kNowledgeable priors (LEON), a mathematically principled approach to leverage large language models (LLMs) as black-box optimizers without any task-specific fine-tuning, taking advantage of their ability to contextualize unstructured domain knowledge to propose personalized treatment plans in natural language. In practice, we implement LEON via 'optimization by prompting,' which uses LLMs as stochastic engines for proposing treatment designs. Experiments on real-world optimization tasks show LEON outperforms both traditional and LLM-based methods in proposing individualized treatments for patients.", 'abstract_zh': '个性化医疗的目标是根据患者的个人遗传和环境因素发现一种优化临床结果的治疗方案。然而，候选治疗方法不能随意给予患者以评估其疗效；我们通常只能访问一个计算模拟的 surrogate 模型来近似估计所提议治疗的真实适应度。不幸的是，此类 surrogate 模型已被证明无法泛化到以前未见过的患者-治疗组合。我们假设领域特定的先验知识——如医学教科书和生物医学知识图谱——可以提供治疗提议适应度的有意义替代信号。为此，我们引入了基于大语言模型（LLM）的熵引导优化与知情先验（LEON）方法，这是一种先验原则上的方法，利用 LLM 作为黑盒优化器，无需任何特定任务的微调，利用其将无结构领域知识语境化的能力，在自然语言中提出个性化治疗计划。在实践中，我们通过“提示优化”实现 LEON，使用 LLM 作为提出治疗设计的随机引擎。实验表明，LEON 在提出个体化治疗方案方面优于传统方法和基于 LLM 的方法。', 'title_zh': '具有知识的语言模型作为黑盒优化器用于个性化医学'}
{'arxiv_id': 'arXiv:2509.20882', 'title': 'On Theoretical Interpretations of Concept-Based In-Context Learning', 'authors': 'Huaze Tang, Tianren Peng, Shao-lun Huang', 'link': 'https://arxiv.org/abs/2509.20882', 'abstract': 'In-Context Learning (ICL) has emerged as an important new paradigm in natural language processing and large language model (LLM) applications. However, the theoretical understanding of the ICL mechanism remains limited. This paper aims to investigate this issue by studying a particular ICL approach, called concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs well for predicting query labels in prompts with only a few demonstrations. In addition, the proposed theory quantifies the knowledge that can be leveraged by the LLMs to the prompt tasks, and leads to a similarity measure between the prompt demonstrations and the query input, which provides important insights and guidance for model pre-training and prompt engineering in ICL. Moreover, the impact of the prompt demonstration size and the dimension of the LLM embeddings in ICL are also explored based on the proposed theory. Finally, several real-data experiments are conducted to validate the practical usefulness of CB-ICL and the corresponding theory.', 'abstract_zh': '基于概念的上下文学习（CB-ICL）理论研究及其在自然语言处理中的应用', 'title_zh': '基于概念的上下文学习的理论解释'}
{'arxiv_id': 'arXiv:2509.20871', 'title': 'SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in Visual Question Answering', 'authors': 'Yan Zhang, Jiaqing Lin, Miao Zhang, Kui Xiao, Xiaoju Hou, Yue Zhao, Zhifei Li', 'link': 'https://arxiv.org/abs/2509.20871', 'abstract': "Acquiring high-quality knowledge is a central focus in Knowledge-Based Visual Question Answering (KB-VQA). Recent methods use large language models (LLMs) as knowledge engines for answering. These methods generally employ image captions as visual text descriptions to assist LLMs in interpreting images. However, the captions frequently include excessive noise irrelevant to the question, and LLMs generally do not comprehend VQA tasks, limiting their reasoning capabilities. To address this issue, we propose the Summarized Caption-Rerank Augmented VQA (SCRA-VQA), which employs a pre-trained visual language model to convert images into captions. Moreover, SCRA-VQA generates contextual examples for the captions while simultaneously summarizing and reordering them to exclude unrelated information. The caption-rerank process enables LLMs to understand the image information and questions better, thus enhancing the model's reasoning ability and task adaptability without expensive end-to-end training. Based on an LLM with 6.7B parameters, SCRA-VQA performs excellently on two challenging knowledge-based VQA datasets: OK-VQA and A-OKVQA, achieving accuracies of 38.8% and 34.6%. Our code is available at this https URL.", 'abstract_zh': '基于摘要Caption重排增强的Knowledge-Based视觉问答（SCRA-VQA）：高质量知识的获取在基于知识的视觉问答（KB-VQA）中是中心焦点。最近的方法使用大型语言模型（LLMs）作为知识引擎进行回答。这些方法通常利用图像说明作为视觉文本描述以辅助LLMs理解图像。然而，说明中经常包含与问题无关的噪声信息，而且LLMs一般不理解VQA任务，限制了其推理能力。为解决这一问题，我们提出了一种名为Summarized Caption-Rerank Augmented VQA（SCRA-VQA）的方法，该方法利用预训练的视觉语言模型将图像转换为说明。此外，SCRA-VQA在生成与说明相关的上下文示例的同时，对其进行了总结和重新排序，以排除无关信息。通过说明的重排过程，LLMs能够更好地理解图像信息和问题，从而增强模型的推理能力和任务适应性而无需昂贵的端到端训练。基于6.7B参数的LLM，SCRA-VQA在两个具有挑战性的基于知识的VQA数据集OK-VQA和A-OKVQA上表现出色，准确率分别为38.8%和34.6%。我们的代码可在以下链接获取：this https URL。', 'title_zh': 'SCRA-VQA: 摘要Caption重排以增强视觉问答中的大型语言模型'}
{'arxiv_id': 'arXiv:2509.20868', 'title': 'StyleBench: Evaluating thinking styles in Large Language Models', 'authors': 'Junyu Guo, Shangding Gu, Ming Jin, Costas Spanos, Javad Lavaei', 'link': 'https://arxiv.org/abs/2509.20868', 'abstract': 'The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in this https URL.', 'abstract_zh': '大型语言模型中推理风格对其效果的影响研究：StyleBench框架的系统评估', 'title_zh': 'StyleBench: 评估大型语言模型的思维风格'}
{'arxiv_id': 'arXiv:2509.20837', 'title': 'Verification Limits Code LLM Training', 'authors': 'Srishti Gureja, Elena Tommasone, Jingyi He, Sara Hooker, Matthias Gallé, Marzieh Fadaee', 'link': 'https://arxiv.org/abs/2509.20837', 'abstract': 'Large language models for code generation increasingly rely on synthetic data, where both problem solutions and verification tests are generated by models. While this enables scalable data creation, it introduces a previously unexplored bottleneck: the verification ceiling, in which the quality and diversity of training data are fundamentally constrained by the capabilities of synthetic verifiers. In this work, we systematically study how verification design and strategies influence model performance. We investigate (i) what we verify by analyzing the impact of test complexity and quantity: richer test suites improve code generation capabilities (on average +3 pass@1), while quantity alone yields diminishing returns, (ii) how we verify by exploring relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By allowing for relaxed thresholds or incorporating LLM-based soft verification, we can recover valuable training data, leading to a 2-4 point improvement in pass@1 performance. However, this benefit is contingent upon the strength and diversity of the test cases used, and (iii) why verification remains necessary through controlled comparisons of formally correct versus incorrect solutions and human evaluation: retaining diverse correct solutions per problem yields consistent generalization gains. Our results show that Verification as currently practiced is too rigid, filtering out valuable diversity. But it cannot be discarded, only recalibrated. By combining calibrated verification with diverse, challenging problem-solution pairs, we outline a path to break the verification ceiling and unlock stronger code generation models.', 'abstract_zh': '大型语言模型用于代码生成 Increasingly Relies on Synthetic Data with Verification Ceilings: A Systematic Study on Verification Design and Strategies', 'title_zh': 'LLM训练的验证极限'}
{'arxiv_id': 'arXiv:2509.20811', 'title': "Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection", 'authors': 'Taehee Park, Heejin Do, Gary Geunbae Lee', 'link': 'https://arxiv.org/abs/2509.20811', 'abstract': 'Robust supervised fine-tuned small Language Models (sLMs) often show high reliability but tend to undercorrect. They achieve high precision at the cost of low recall. Conversely, Large Language Models (LLMs) often show the opposite tendency, making excessive overcorrection, leading to low precision. To effectively harness the strengths of LLMs to address the recall challenges in sLMs, we propose Post-Correction via Overcorrection (PoCO), a novel approach that strategically balances recall and precision. PoCO first intentionally triggers overcorrection via LLM to maximize recall by allowing comprehensive revisions, then applies a targeted post-correction step via fine-tuning smaller models to identify and refine erroneous outputs. We aim to harmonize both aspects by leveraging the generative power of LLMs while preserving the reliability of smaller supervised models. Our extensive experiments demonstrate that PoCO effectively balances GEC performance by increasing recall with competitive precision, ultimately improving the overall quality of grammatical error correction.', 'abstract_zh': '利用过度纠正提升小语言模型召回率的后纠正方法：平衡生成能力和纠正精确性', 'title_zh': '利用已固化的部分：基于LLM的语法错误过度修正的后纠正方法'}
{'arxiv_id': 'arXiv:2509.20784', 'title': 'Towards Atoms of Large Language Models', 'authors': 'Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao', 'link': 'https://arxiv.org/abs/2509.20784', 'abstract': 'The fundamental units of internal representations in large language models (LLMs) remain undefined, limiting further understanding of their mechanisms. Neurons or features are often regarded as such units, yet neurons suffer from polysemy, while features face concerns of unreliable reconstruction and instability. To address this issue, we propose the Atoms Theory, which defines such units as atoms. We introduce the atomic inner product (AIP) to correct representation shifting, formally define atoms, and prove the conditions that atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse representations over atom set and linking to compressed sensing. Under stronger conditions, we further establish the uniqueness and exact $\\ell_1$ recoverability of the sparse representations, and provide guarantees that single-layer sparse autoencoders (SAEs) with threshold activations can reliably identify the atoms. To validate the Atoms Theory, we train threshold-activated SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse reconstruction across layers on average, and more than 99.8% of atoms satisfy the uniqueness condition, compared to 0.5% for neurons and 68.2% for features, showing that atoms more faithfully capture intrinsic representations of LLMs. Scaling experiments further reveal the link between SAEs size and recovery capacity. Overall, this work systematically introduces and validates Atoms Theory of LLMs, providing a theoretical framework for understanding internal representations and a foundation for mechanistic interpretability. Code available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）内部表示的基本单元的本质尚未定义，限制了对其机制的进一步理解。神经元或特征通常被视为这样的单元，但神经元存在多义性问题，而特征则面临不可靠重建和不稳定性的担忧。为解决这一问题，我们提出了原子理论，将这样的单元定义为原子。我们引入了原子内积（AIP）以校正表示偏移，正式定义了原子，并证明了原子满足限制等距性质（RIP）的条件，从而确保了原子集上的稳定稀疏表示，并将其与压缩感知相联系。在更强的条件下，我们进一步建立了稀疏表示的独特性和精确的$\\ell_1$可恢复性，并提供保证，表明单层阈值激活稀疏自编码器（SAEs）能够可靠地识别原子。为了验证原子理论，我们在Gemma2-2B、Gemma2-9B和Llama3.1-8B上训练了阈值激活的SAEs，在平均层面上实现了99.9%的稀疏重建，并且超过99.8%的原子满足独特性条件，而神经元仅为0.5%，特征为68.2%，表明原子更忠实地捕获了LLMs的内在表示。扩展实验进一步揭示了SAEs规模与恢复能力之间的联系。总体而言，本工作系统地介绍了并验证了LLMs的原子理论，提供了一种理解内部表示的理论框架，并为机制可解释性提供了基础。代码可在以下链接获得。', 'title_zh': '大型语言模型的基本成分探索'}
{'arxiv_id': 'arXiv:2509.20768', 'title': 'Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis', 'authors': 'Maria F. Davila R, Azizjon Turaev, Wolfram Wingerath', 'link': 'https://arxiv.org/abs/2509.20768', 'abstract': 'Synthetic tabular data is used for privacy-preserving data sharing and data-driven model development. Its effectiveness, however, depends heavily on the used Tabular Data Synthesis (TDS) tool. Recent studies have shown that Transformer-based models outperform other state-of-the-art models such as Generative Adversarial Networks (GANs) and Diffusion models in terms of data quality. However, Transformer-based models also come with high computational costs, making them sometimes unfeasible for end users with prosumer hardware. This study presents a sensitivity assessment on how the choice of hyperparameters, such as number of layers or hidden dimension affects the quality of the resultant synthetic data and the computational performance. It is performed across two tools, GReaT and REaLTabFormer, evaluating 10 model setups that vary in architecture type and depth. We assess the sensitivity on three dimensions: runtime, machine learning (ML) utility, and similarity to real data distributions. Experiments were conducted on four real-world datasets. Our findings reveal that runtime is proportional to the number of hyperparameters, with shallower configurations completing faster. GReaT consistently achieves lower runtimes than REaLTabFormer, and only on the largest dataset they have comparable runtime. For small datasets, both tools achieve synthetic data with high utility and optimal similarity, but on larger datasets only REaLTabFormer sustains strong utility and similarity. As a result, REaLTabFormer with lightweight LLMs provides the best balance, since it preserves data quality while reducing computational requirements. Nonetheless, its runtime remains higher than that of GReaT and other TDS tools, suggesting that efficiency gains are possible but only up to a certain level.', 'abstract_zh': '合成表数据用于保护隐私的数据共享和数据驱动模型开发。然而，其效果很大程度上依赖于所使用的表格数据合成（TDS）工具。最近的研究表明，基于Transformer的模型在数据质量方面优于其他最先进的模型，如生成对抗网络（GANs）和扩散模型。然而，基于Transformer的模型也伴随着较高的计算成本，这有时使它们对拥有消费者级硬件的最终用户来说不可行。本研究对超参数（如层数或隐藏维度）的选择如何影响合成数据质量和计算性能进行了灵敏度评估。该评估在GReaT和REaLTabFormer两种工具上进行，评估了10种不同架构类型和深度的模型配置。我们在三个维度上评估灵敏度：运行时间、机器学习（ML）效用和与真实数据分布的相似性。实验在四个真实世界数据集上进行。我们的发现表明，运行时间与超参数的数量成正比，较浅的配置运行得更快。GReaT始终比REaLTabFormer的运行时间更短，仅在最大的数据集上它们的运行时间才可比较。对于小数据集，两种工具都能生成具有高效用和最优相似性的合成数据，但在大数据集上只有REaLTabFormer能够保持强大的效用和相似性。因此，REaLTabFormer结合轻量级的LLM提供了最佳平衡，因为它在保持数据质量的同时减少了计算需求。然而，其运行时间仍然高于GReaT和其他TDS工具，表明效率提升是可能的，但仅限于一定水平。', 'title_zh': '基于变压器的表格数据合成中LLM敏感性的度量'}
{'arxiv_id': 'arXiv:2509.20702', 'title': 'Incorporating LLM Embeddings for Variation Across the Human Genome', 'authors': 'Hongqian Niu, Jordan Bryan, Xihao Li, Didong Li', 'link': 'https://arxiv.org/abs/2509.20702', 'abstract': "Recent advances in large language model (LLM) embeddings have enabled powerful representations for biological data, but most applications to date focus only on gene-level information. We present one of the first systematic frameworks to generate variant-level embeddings across the entire human genome. Using curated annotations from FAVOR, ClinVar, and the GWAS Catalog, we constructed semantic text descriptions for 8.9 billion possible variants and generated embeddings at three scales: 1.5 million HapMap3+MEGA variants, ~90 million imputed UK Biobank variants, and ~9 billion all possible variants. Embeddings were produced with both OpenAI's text-embedding-3-large and the open-source Qwen3-Embedding-0.6B models. Baseline experiments demonstrate high predictive accuracy for variant properties, validating the embeddings as structured representations of genomic variation. We outline two downstream applications: embedding-informed hypothesis testing by extending the Frequentist And Bayesian framework to genome-wide association studies, and embedding-augmented genetic risk prediction that enhances standard polygenic risk scores. These resources, publicly available on Hugging Face, provide a foundation for advancing large-scale genomic discovery and precision medicine.", 'abstract_zh': '最近在大型语言模型（LLM）嵌入方面的进展使生物数据的强大表示成为可能，但目前大多数应用仅侧重于基因水平的信息。我们提出了第一个系统框架之一，用于在整个人类基因组中生成变体级别的嵌入。利用来自FAVAR、ClinVar和GWAS Catalog的精心注释数据，我们为89亿个可能的变体构建了语义文本描述，并生成了三个层次的嵌入：150万个HapMap3+MEGA变体、约9亿个推断的UK Biobank变体以及约90亿个所有可能的变体。嵌入使用OpenAI的text-embedding-3-large和开源的Qwen3-Embedding-0.6B模型生成。基线实验表明，变体属性预测准确性很高，验证了嵌入作为基因组变异的结构化表示的有效性。我们概述了两个下游应用：通过扩展Frequentist And Bayesian框架用于全基因组关联研究的嵌入指导的假设检验，以及增强标准多基因风险评分的嵌入增强遗传风险预测。这些资源在Hugging Face上公开提供，为大规模基因组发现和精准医学奠定了基础。', 'title_zh': '将大型语言模型嵌入用于人类基因组的变异性研究'}
{'arxiv_id': 'arXiv:2509.20645', 'title': 'Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions', 'authors': 'Jungsoo Park, Ethan Mendes, Gabriel Stanovsky, Alan Ritter', 'link': 'https://arxiv.org/abs/2509.20645', 'abstract': "Progress in large language models is constrained by an evaluation bottleneck: build a benchmark, evaluate models and settings, then iterate. We therefore ask a simple question: can we forecast outcomes before running any experiments? We study text-only performance forecasting: estimating a model's score from a redacted task description and intended configuration, with no access to dataset instances. To support systematic study, we curate PRECOG, a corpus of redacted description-performance pairs spanning diverse tasks, domains, and metrics. Experiments show the task is challenging but feasible: models equipped with a retrieval module that excludes source papers achieve moderate prediction performance with well-calibrated uncertainty, reaching mean absolute error as low as 8.7 on the Accuracy subset at high-confidence thresholds. Our analysis indicates that stronger reasoning models engage in diverse, iterative querying, whereas current open-source models lag and often skip retrieval or gather evidence with limited diversity. We further test a zero-leakage setting, forecasting on newly released datasets or experiments before their papers are indexed, where GPT-5 with built-in web search still attains nontrivial prediction accuracy. Overall, our corpus and analyses offer an initial step toward open-ended anticipatory evaluation, supporting difficulty estimation and smarter experiment prioritization.", 'abstract_zh': '大型语言模型进展受限于评估瓶颈：构建基准、评估模型和设置，然后迭代改进。因此我们提出一个简单问题：我们能否在运行任何实验之前预测结果？我们研究仅文本的性能预测：根据红acted的任务描述和预期配置估算模型的得分，不访问数据集实例。为了支持系统研究，我们编制了PRECOG，这是一个跨不同任务、领域和指标的红acted描述-性能配对语料库。实验显示该任务具有挑战性但可行：配备排除源论文检索模块的模型在高置信度阈值下于准确度子集上达到最低8.7的平均绝对误差，并表现出良好的不确定性校准。我们的分析表明，更强的推理模型会进行多样性的迭代查询，而现有的开源模型则滞后，经常跳过检索或收集有限多样性的证据。我们进一步测试了零泄漏设置，在论文未被索引前对新发布的数据集或实验进行预测，结果显示内置网页搜索的GPT-5仍能获得非平凡的预测精度。总体而言，我们的语料库和分析为进一步开放式的前瞻性评估奠定了初步基础，支持难度估计和更明智的实验优先级排序。', 'title_zh': '未雨绸缪：从描述中估计大语言模型基准分数'}
{'arxiv_id': 'arXiv:2509.20639', 'title': 'A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks', 'authors': 'Adam Swanda, Amy Chang, Alexander Chen, Fraser Burch, Paul Kassianik, Konstantin Berlin', 'link': 'https://arxiv.org/abs/2509.20639', 'abstract': 'The widespread adoption of Large Language Models (LLMs) has revolutionized AI deployment, enabling autonomous and semi-autonomous applications across industries through intuitive language interfaces and continuous improvements in model development. However, the attendant increase in autonomy and expansion of access permissions among AI applications also make these systems compelling targets for malicious attacks. Their inherent susceptibility to security flaws necessitates robust defenses, yet no known approaches can prevent zero-day or novel attacks against LLMs. This places AI protection systems in a category similar to established malware protection systems: rather than providing guaranteed immunity, they minimize risk through enhanced observability, multi-layered defense, and rapid threat response, supported by a threat intelligence function designed specifically for AI-related threats.\nPrior work on LLM protection has largely evaluated individual detection models rather than end-to-end systems designed for continuous, rapid adaptation to a changing threat landscape. We present a production-grade defense system rooted in established malware detection and threat intelligence practices. Our platform integrates three components: a threat intelligence system that turns emerging threats into protections; a data platform that aggregates and enriches information while providing observability, monitoring, and ML operations; and a release platform enabling safe, rapid detection updates without disrupting customer workflows. Together, these components deliver layered protection against evolving LLM threats while generating training data for continuous model improvement and deploying updates without interrupting production.', 'abstract_zh': '大规模语言模型（LLMs）的广泛应用已经革新人工智能部署，通过直观的语言界面和模型开发的持续改进，使各行各业能够实现自主和半自主的应用。然而，AI应用程序自主性和访问权限的增加也使其成为恶意攻击的诱饵。系统固有的安全缺陷使其需要强大的防御措施，但目前尚无方法能完全阻止针对LLMs的零日或新型攻击。这使得AI保护系统处于与传统恶意软件保护系统相似的类别：它们不是提供绝对的免疫，而是通过增强可观测性、多层次防御和快速威胁响应，特别是通过专门设计的威胁情报功能，来最大限度地降低风险。', 'title_zh': '一种快速开发和部署大型语言模型攻击防护的框架'}
{'arxiv_id': 'arXiv:2509.20634', 'title': 'Recidivism and Peer Influence with LLM Text Embeddings in Low Security Correctional Facilities', 'authors': 'Shanjukta Nath, Jiwon Hong, Jae Ho Chang, Keith Warren, Subhadeep Paul', 'link': 'https://arxiv.org/abs/2509.20634', 'abstract': 'We find AI embeddings obtained using a pre-trained transformer-based Large Language Model (LLM) of 80,000-120,000 written affirmations and correction exchanges among residents in low-security correctional facilities to be highly predictive of recidivism. The prediction accuracy is 30\\% higher with embedding vectors than with only pre-entry covariates. However, since the text embedding vectors are high-dimensional, we perform Zero-Shot classification of these texts to a low-dimensional vector of user-defined classes to aid interpretation while retaining the predictive power. To shed light on the social dynamics inside the correctional facilities, we estimate peer effects in these LLM-generated numerical representations of language with a multivariate peer effect model, adjusting for network endogeneity. We develop new methodology and theory for peer effect estimation that accommodate sparse networks, multivariate latent variables, and correlated multivariate outcomes. With these new methods, we find significant peer effects in language usage for interaction and feedback.', 'abstract_zh': '我们发现使用预训练的基于变换器的大语言模型（LLM），对80,000-120,000份低安全级别矫正设施中居民的书面肯定陈述和修正交流进行AI嵌入后，这些嵌入能够高度预测重犯概率。与仅使用前期协变量相比，使用嵌入向量的预测准确率提高了30%。由于文本嵌入向量高维，我们通过对这些文本进行零样本分类，将它们降至用户定义类别的低维向量，以辅助解释同时保持预测能力。为了揭示矫正设施内的社会动态，我们使用多元同伴影响模型估计LLM生成的语言数值表示中的同伴效果，同时调整网络内生性。我们开发了新的同伴效果估计方法和理论，以应对稀疏网络、多元潜在变量和多元相关结果的问题。利用这些新方法，我们发现语言使用在互动和反馈中的显著同伴效应。', 'title_zh': '低安全级别矫正设施中LLM文本嵌入的再犯与同伴影响研究'}
{'arxiv_id': 'arXiv:2509.20603', 'title': 'Experience Deploying Containerized GenAI Services at an HPC Center', 'authors': 'Angel M. Beltre, Jeff Ogden, Kevin Pedretti', 'link': 'https://arxiv.org/abs/2509.20603', 'abstract': 'Generative Artificial Intelligence (GenAI) applications are built from specialized components -- inference servers, object storage, vector and graph databases, and user interfaces -- interconnected via web-based APIs. While these components are often containerized and deployed in cloud environments, such capabilities are still emerging at High-Performance Computing (HPC) centers. In this paper, we share our experience deploying GenAI workloads within an established HPC center, discussing the integration of HPC and cloud computing environments. We describe our converged computing architecture that integrates HPC and Kubernetes platforms running containerized GenAI workloads, helping with reproducibility. A case study illustrates the deployment of the Llama Large Language Model (LLM) using a containerized inference server (vLLM) across both Kubernetes and HPC platforms using multiple container runtimes. Our experience highlights practical considerations and opportunities for the HPC container community, guiding future research and tool development.', 'abstract_zh': '生成型人工智能（GenAI）应用基于专门组件构建——推理服务器、对象存储、向量和图数据库以及用户界面——通过基于Web的API相互连接。尽管这些组件通常是容器化的并在云环境中部署，但在高性能计算（HPC）中心，这些能力仍处于早期阶段。在本文中，我们分享在已有的HPC中心部署GenAI工作负载的经验，讨论了HPC和云计算环境的集成。我们描述了一种结合HPC和Kubernetes平台的计算架构，以容器化形式运行GenAI工作负载，有助于提高可重复性。一个案例研究展示了如何使用容器化推理服务器（vLLM）在Kubernetes和HPC平台之间部署Llama大型语言模型（LLM）并使用多个容器运行时。我们的经验为HPC容器社区提供了实用的考虑因素和机会，指导未来的研究和工具开发。', 'title_zh': '在国家超级计算中心部署容器化生成式AI服务的经验'}
{'arxiv_id': 'arXiv:2509.20600', 'title': 'An LLM-based Agentic Framework for Accessible Network Control', 'authors': 'Samuel Lin, Jiawei Zhou, Minlan Yu', 'link': 'https://arxiv.org/abs/2509.20600', 'abstract': 'Traditional approaches to network management have been accessible only to a handful of highly-trained network operators with significant expert knowledge. This creates barriers for lay users to easily manage their networks without resorting to experts. With recent development of powerful large language models (LLMs) for language comprehension, we design a system to make network management accessible to a broader audience of non-experts by allowing users to converse with networks in natural language. To effectively leverage advancements in LLMs, we propose an agentic framework that uses an intermediate representation to streamline configuration across diverse vendor equipment, retrieves the network state from memory in real-time, and provides an interface for external feedback. We also conduct pilot studies to collect real user data of natural language utterances for network control, and present a visualization interface to facilitate dialogue-driven user interaction and enable large-scale data collection for future development. Preliminary experiments validate the effectiveness of our proposed system components with LLM integration on both synthetic and real user utterances. Through our data collection and visualization efforts, we pave the way for more effective use of LLMs and democratize network control for everyday users.', 'abstract_zh': '传统的网络管理方法仅对少量受过高度训练的网络运维专家开放，这为普通用户在无需求助专家的情况下轻松管理网络设置了障碍。借助近期强大语言理解大型语言模型（LLMs）的发展，我们设计了一个系统，通过让普通用户以自然语言与网络交流，使网络管理对非专家用户群体更加便捷。为了有效利用LLMs的进展，我们提出了一种代理框架，使用中间表示简化跨不同供应商设备的配置过程，实时从内存中检索网络状态，并提供外部反馈接口。我们还开展了试点研究，收集了网络控制中自然语言指令的真实用户数据，并提供了一个可视化界面，以促进基于对话的用户交互，并为未来的大规模数据收集提供支持。初步实验验证了在合成和真实用户指令上集成LLMs的系统组件的有效性。通过我们的数据收集和可视化工作，我们为更有效地利用LLMs并使网络控制普及化铺平了道路。', 'title_zh': '基于LLM的赋能框架以实现无障碍网络控制'}
{'arxiv_id': 'arXiv:2509.20581', 'title': 'Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding', 'authors': 'Ayan Sar, Sampurna Roy, Kanav Gupta, Anurag Kaushish, Tanupriya Choudhury, Abhijit Kumar', 'link': 'https://arxiv.org/abs/2509.20581', 'abstract': 'Transformer architectures have achieved state-of-the-art performance across natural language tasks, yet they fundamentally misrepresent the hierarchical nature of human language by processing text as flat token sequences. This results in quadratic computational cost, weak computational cost, weak compositional generalization, and inadequate discourse-level modeling. We propose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired neural architecture that processes language simultaneously across multiple resolutions, from characters to discourse-level units. HRT constructs a multi-resolution attention, enabling bottom-up composition and top-down contextualization. By employing exponential sequence reduction across scales, HRT achieves O(nlogn) complexity, offering significant efficiency improvements over standard transformers. We evaluated HRT on a diverse suite of benchmarks, including GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results demonstrated that HRT outperforms standard transformer baselines by an average of +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while reducing memory usage by 42% and inference latency by 37% compared to BERT and GPT style models of similar parameter count. Ablation studies confirm the effectiveness of cross-resolution attention and scale-specialized modules, showing that each contributes independently to both efficiency and accuracy. Our findings establish HRT as the first architecture to align computational structure with the hierarchical organization of human language, demonstrating that multi-scale, wavelet-inspired processing yields both theoretical efficiency gains and practical improvements in language understanding.', 'abstract_zh': '基于小波的层次解析变换器在自然语言任务中实现了最先进的性能，但本质上错误地将人类语言的层次结构表示为扁平的标记序列，导致二次计算成本、计算效率低、组合泛化能力弱以及语篇层次建模不足。为此，我们提出了层次解析变换器（HRT），一种新颖的小波启发式神经架构，能够同时从字符到语篇层面单位对语言进行多尺度处理。HRT 构建了多尺度注意机制，支持自底向上的组合和自顶向下的语境化。通过在不同尺度上应用指数级序列减少，HRT 实现了O(nlogn)复杂度，相比标准变换器提供了显著的效率改进。我们在 GLUE、SuperGLUE、Long Range Arena 和 WikiText-103 等多种基准测试上评估了 HRT，结果表明 HRT 在 GLUE 上平均优于标准变换器基线 3.8%，在 SuperGLUE 上优于 4.5%，在 Long Range Arena 上优于 6.1%，同时内存使用量减少 42%，推理延迟减少 37%，与类似参数量的 BERT 和 GPT 风格模型相比。消融研究确认了跨尺度注意和尺度专业化模块的有效性，表明它们分别独立地提高了效率和准确性。我们的研究确立了 HRT 作为首个使计算结构与人类语言层级结构相一致的架构，证明了多尺度、小波启发式处理既具备理论效率优势，又在语言理解方面实现了实际改进。', 'title_zh': '分层解析变换器：一种小波启发的多尺度语言理解架构'}
{'arxiv_id': 'arXiv:2509.20577', 'title': 'Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures', 'authors': 'Sampurna Roy, Ayan Sar, Anurag Kaushish, Kanav Gupta, Tanupriya Choudhury, Abhijit Kumar', 'link': 'https://arxiv.org/abs/2509.20577', 'abstract': 'Contemporary transformer architectures apply identical processing depth to all inputs, creating inefficiencies and limiting reasoning quality. Simple factual queries are subjected to the same multilayered computation as complex logical problems, wasting resources while constraining deep inference. To overcome this, we came up with a concept of Dynamic Reasoning Chains through Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends the Mixture of Experts paradigm from width-based to depth specialised computation. DS-MoE introduces expert modules optimised for distinct reasoning depths, shallow pattern recognition, compositional reasoning, logical inference, memory integration, and meta-cognitive supervision. A learned routing network dynamically assembles custom reasoning chains, activating only the necessary experts to match input complexity. The dataset on which we trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse domains such as scientific papers, legal texts, programming code, and web content, enabling systematic assessment across reasoning depths. Experimental results demonstrate that DS-MoE achieves up to 16 per cent computational savings and 35 per cent faster inference compared to uniform-depth transformers, while delivering 2.8 per cent higher accuracy on complex multi-step reasoning benchmarks. Furthermore, routing decisions yield interpretable reasoning chains, enhancing transparency and scalability. These findings establish DS-MoE as a significant advancement in adaptive neural architectures, demonstrating that depth-specialised modular processing can simultaneously improve efficiency, reasoning quality, and interpretability in large-scale language models.', 'abstract_zh': 'Dynamic Reasoning Chains through Depth Specialised Mixture of Experts', 'title_zh': '深度专业化专家混合在变压器架构中的动态推理链'}
{'arxiv_id': 'arXiv:2509.20567', 'title': 'SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations', 'authors': 'Ayan Sar, Pranav Singh Puri, Sumit Aich, Tanupriya Choudhury, Abhijit Kumar', 'link': 'https://arxiv.org/abs/2509.20567', 'abstract': 'In multilingual healthcare environments, automatic disease diagnosis from clinical text remains a challenging task due to the scarcity of annotated medical data in low-resource languages and the linguistic variability across populations. This paper proposes SwasthLLM, a unified, zero-shot, cross-lingual, and multi-task learning framework for medical diagnosis that operates effectively across English, Hindi, and Bengali without requiring language-specific fine-tuning. At its core, SwasthLLM leverages the multilingual XLM-RoBERTa encoder augmented with a language-aware attention mechanism and a disease classification head, enabling the model to extract medically relevant information regardless of the language structure. To align semantic representations across languages, a Siamese contrastive learning module is introduced, ensuring that equivalent medical texts in different languages produce similar embeddings. Further, a translation consistency module and a contrastive projection head reinforce language-invariant representation learning. SwasthLLM is trained using a multi-task learning strategy, jointly optimizing disease classification, translation alignment, and contrastive learning objectives. Additionally, we employ Model-Agnostic Meta-Learning (MAML) to equip the model with rapid adaptation capabilities for unseen languages or tasks with minimal data. Our phased training pipeline emphasizes robust representation alignment before task-specific fine-tuning. Extensive evaluation shows that SwasthLLM achieves high diagnostic performance, with a test accuracy of 97.22% and an F1-score of 97.17% in supervised settings. Crucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and 73.33% accuracy on Bengali medical text, demonstrating strong generalization in low-resource contexts.', 'abstract_zh': '多语言医疗环境中，基于临床文本的自动疾病诊断仍然是一个具有挑战性的任务，原因在于低资源语言标注医疗数据的稀缺性和跨群体语言变异性。本文提出了一种统一的、零样本、跨语言和多任务学习框架SwasthLLM，该框架在不针对特定语言进行微调的情况下，有效应用于英语、 Hindi 和 Bengali 语言的医疗诊断。SwasthLLM 在其核心处利用了多语言 XLM-RoBERTa 编码器，结合了语言意识注意力机制和疾病分类头，使模型能够提取与语言结构无关的医疗相关信息。为了在不同语言之间对齐语义表示，引入了双胞胎对比学习模块，确保不同语言中的等效医疗文本生成相似的嵌入表示。此外，语言不变性表示学习还通过翻译一致性模块和对比投影头得到了加强。SwasthLLM 采用了多任务学习策略进行训练，同时优化疾病分类、翻译对齐和对比学习目标。此外，我们利用模型无感知元学习（MAML）赋予模型在最少数据下对未见过的语言或任务进行快速适应的能力。训练管道采用分阶段的方式，重点在于稳健表示对齐后再进行任务特定的微调。广泛评估表明，SwasthLLM 在监督设置中达到了高诊断性能，测试准确率为 97.22%，F1 分数为 97.17%。在零样本场景中，SwasthLLM 在 Hindi 医疗文本上的准确率为 92.78%，在 Bengali 医疗文本上的准确率为 73.33%，显示出了在低资源环境中的强大泛化能力。', 'title_zh': 'SwasthLLM：一种基于对比表示的统一跨语言、多任务和元学习的零样本医学诊断框架'}
{'arxiv_id': 'arXiv:2509.20512', 'title': 'CHOIR: A Chatbot-mediated Organizational Memory Leveraging Communication in University Research Labs', 'authors': 'Sangwook Lee, Adnan Abbas, Yan Chen, Young-Ho Kim, Sang Won Lee', 'link': 'https://arxiv.org/abs/2509.20512', 'abstract': "University research labs often rely on chat-based platforms for communication and project management, where valuable knowledge surfaces but is easily lost in message streams. Documentation can preserve knowledge, but it requires ongoing maintenance and is challenging to navigate. Drawing on formative interviews that revealed organizational memory challenges in labs, we designed CHOIR, an LLM-based chatbot that supports organizational memory through four key functions: document-grounded Q&A, Q&A sharing for follow-up discussion, knowledge extraction from conversations, and AI-assisted document updates. We deployed CHOIR in four research labs for one month (n=21), where the lab members asked 107 questions and lab directors updated documents 38 times in the organizational memory. Our findings reveal a privacy-awareness tension: questions were asked privately, limiting directors' visibility into documentation gaps. Students often avoided contribution due to challenges in generalizing personal experiences into universal documentation. We contribute design implications for privacy-preserving awareness and supporting context-specific knowledge documentation.", 'abstract_zh': '大学研究实验室往往依赖基于聊天的平台进行沟通和项目管理，其中有价值的知识容易在消息流中丢失。文档可以保存知识，但需要持续维护且导航困难。借鉴形成性访谈中揭示的研究实验室组织记忆挑战，我们设计了CHOIR，一个基于语言模型的聊天机器人，通过四大关键功能支持组织记忆：文档指导的问答、问答分享供后续讨论、从对话中提取知识，以及AI辅助的文档更新。我们在四个研究实验室进行了一项为期一个月的部署（n=21），实验室成员提出了107个问题，实验室主任更新了组织记忆中的文档38次。我们的研究发现隐私意识与透明度之间存在张力：问题往往在私密环境下提出，限制了领导者对文档空白的可见性。学生因难以将个人经验泛化为通用文档而往往避免贡献。我们提出了隐私保护意识的设计启示，并支持情境特定的知识文档。', 'title_zh': 'CHOIR：一种通过交流促进大学研究实验室组织记忆的聊天机器人 eksplora: 一种通过交流促进大学研究实验室组织记忆的聊天机器人'}
{'arxiv_id': 'arXiv:2509.20502', 'title': 'MARS: toward more efficient multi-agent collaboration for LLM reasoning', 'authors': 'Xiao Wang, Jia Wang, Yijie Wang, Pengtao Dang, Sha Cao, Chi Zhang', 'link': 'https://arxiv.org/abs/2509.20502', 'abstract': 'Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\\%. Code is available at this https URL.', 'abstract_zh': '多代理评审系统（MARS）：一种基于角色的合作框架', 'title_zh': 'MARS:向更高效的多Agent协作推理方向迈进'}
{'arxiv_id': 'arXiv:2509.20393', 'title': 'The Secret Agenda: LLMs Strategically Lie and Our Current Safety Tools Are Blind', 'authors': 'Caleb DeLeeuw, Gaurav Chawla, Aniket Sharma, Vanessa Dietze', 'link': 'https://arxiv.org/abs/2509.20393', 'abstract': 'We investigate strategic deception in large language models using two complementary testbeds: Secret Agenda (across 38 models) and Insider Trading compliance (via SAE architectures). Secret Agenda reliably induced lying when deception advantaged goal achievement across all model families. Analysis revealed that autolabeled SAE features for "deception" rarely activated during strategic dishonesty, and feature steering experiments across 100+ deception-related features failed to prevent lying. Conversely, insider trading analysis using unlabeled SAE activations separated deceptive versus compliant responses through discriminative patterns in heatmaps and t-SNE visualizations. These findings suggest autolabel-driven interpretability approaches fail to detect or control behavioral deception, while aggregate unlabeled activations provide population-level structure for risk assessment. Results span Llama 8B/70B SAE implementations and GemmaScope under resource constraints, representing preliminary findings that motivate larger studies on feature discovery, labeling methodology, and causal interventions in realistic deception contexts.', 'abstract_zh': '我们使用两种互补的测试平台（Secret Agenda和通过SAE架构的 Insider Trading合规性）来探究大型语言模型中的策略性欺骗。研究发现，在所有模型家族中，当欺骗有利于目标实现时，Secret Agenda可靠地诱导了说谎行为。分析显示，自标注的SAE特征“欺骗”极少在策略性不诚实过程中激活，并且在100多个相关特征的特征导向实验中未能阻止说谎行为。相反，未标注的SAE激活在内幕交易分析中通过热图和t-SNE可视化中的区分模式分离出了欺骗性回应和合规性回应。这些发现表明，自标注驱动的可解释性方法无法检测或控制行为欺骗，而聚合的未标注激活则提供了在群体层面进行风险评估的结构。研究结果涵盖了在资源约束下实现的Llama 8B/70B SAE以及GemmaScope，代表了初步发现，这些发现激发了对特征发现、标注方法和因果干预在实际欺骗情境中的更大规模研究。', 'title_zh': '隐秘议程：大规模语言模型策略性说谎，当前的安全工具视而不见'}
{'arxiv_id': 'arXiv:2509.20384', 'title': 'R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning', 'authors': 'Jiayi Lin, Liangcai Su, Junzhe Li, Chenxiong Qian', 'link': 'https://arxiv.org/abs/2509.20384', 'abstract': 'Fuzzing is effective for vulnerability discovery but struggles with complex targets such as compilers, interpreters, and database engines, which accept textual input that must satisfy intricate syntactic and semantic constraints. Although language models (LMs) have attracted interest for this task due to their vast latent knowledge and reasoning potential, their practical adoption has been limited. The major challenges stem from insufficient exploration of deep program logic among real-world codebases, and the high cost of leveraging larger models. To overcome these challenges, we propose R1-Fuzz, the first framework that leverages reinforcement learning (RL) to specialize cost-efficient LMs and integrate them for complex textual fuzzing input generation. R1-Fuzz introduces two key designs: coverage-slicing-based question construction and a distance-based reward calculation. Through RL-based post-training of a model with our constructed dataset, R1-Fuzz designs a fuzzing workflow that tightly integrates LMs to reason deep program semantics during fuzzing. Evaluations on diverse real-world targets show that our design enables a small model, named R1-Fuzz-7B, to rival or even outperform much larger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\% higher coverage than state-of-the-art fuzzers and discovers 29 previously unknown vulnerabilities, demonstrating its practicality.', 'abstract_zh': 'R1-Fuzz：基于强化学习的高效复杂文本 fuzzing 框架', 'title_zh': 'R1-Fuzz: 通过强化学习专门化语言模型进行文本模糊测试'}
{'arxiv_id': 'arXiv:2509.20381', 'title': 'USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model', 'authors': 'Jianyu Wen, Jingyun Wang, Cilin Yan, Jiayin Cai, Xiaolong Jiang, Ying Zhang', 'link': 'https://arxiv.org/abs/2509.20381', 'abstract': 'Recently, Large Language Models (LLMs) have been widely employed in Conversational Recommender Systems (CRSs). Unlike traditional language model approaches that focus on training, all existing LLMs-based approaches are mainly centered around how to leverage the summarization and analysis capabilities of LLMs while ignoring the issue of training. Therefore, in this work, we propose an integrated training-inference framework, User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs in conversational recommendation at the model level. Firstly, we design a LLM-based Preference Optimization (PO) dataset construction strategy for RL training, which helps the LLMs understand the strategies and methods in conversational recommendation. Secondly, we propose a Self-Enhancement Strategy (SES) at the inference stage to further exploit the conversational recommendation potential obtained from RL training. Extensive experiments on various datasets demonstrate that our method consistently outperforms previous state-of-the-art methods.', 'abstract_zh': '最近，大规模语言模型（LLMs）已在会话推荐系统（CRSs）中广泛应用于推荐。在此项工作中，我们提出了一种综合训练-推理框架——用户模拟器基于框架（USB-Rec），以在模型层面提高LLMs在会话推荐中的性能。首先，我们设计了一种基于LLMs的偏好优化（PO）数据集构建策略，以辅助强化学习（RL）训练，帮助LLMs理解会话推荐中的策略和方法。其次，我们在推理阶段提出了一种自我增强策略（SES），进一步挖掘从RL训练中获得的会话推荐潜力。在多种数据集上的广泛实验表明，我们的方法在性能上始终优于之前的最先进方法。', 'title_zh': 'USB-Rec: 一种提高大型语言模型对话推荐能力的有效框架'}
{'arxiv_id': 'arXiv:2509.20380', 'title': 'ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation', 'authors': 'Samyak Jhaveri, Vanessa Klotzmann, Crista Lopes', 'link': 'https://arxiv.org/abs/2509.20380', 'abstract': 'The increasing ubiquity of GPUs is accompanied by the increasing complexity of their hardware and parallel programming frameworks. Directive-based parallel programming standards like OpenACC simplify GPU programming to some extent by abstracting away low-level complexities, but a fair amount of expertise is still required in order to use those directives effectively.\nWe introduce ACCeLLiuM, two open weights Large Language Models specifically fine-tuned for generating expert OpenACC directives for data-parallel loops, along with the supervised fine-tuning dataset that was used to train them. The ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for testing. Experimental evaluations show a pronounced performance gap in generating correct OpenACC pragmas between base LLMs and our fine-tuned versions. On the held-out test set, base LLMs fail to consistently generate valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid pragmas with the correct directive type for $87\\%$ of the data-parallel loops, and exact pragmas--including directives, clauses, clause order, and clause variables--for $50\\%$ of the cases. Even when not exact, generated pragmas frequently incorporate the correct clauses in a different order than the ground-truth label, or include additional clauses that enable finer control over parallel execution, data movement, and concurrency, offering practical value beyond strict string-matching. By publicly releasing the code, models, and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU offloading of serially written programs.', 'abstract_zh': 'GPU使用日益普及的同时，其硬件复杂性和并行编程框架也变得愈发复杂。基于指令的并行编程标准如OpenACC在一定程度上简化了GPU编程，通过抽象低级复杂性降低了编程难度，但仍需要相当的专业知识才能有效使用这些指令。\n我们介绍了ACCeLLiuM，这是一个专为生成数据并行循环的专家级OpenACC指令而设计的两个开源大型语言模型，以及用于训练它们的监督细调数据集。ACCeLLiuM SFT数据集包含从公共GitHub C/C++仓库中挖掘出的4033个OpenACC 元令-循环对，其中3223个用于训练，810个用于测试。实验评估结果显示，基模型在生成正确的OpenACC 元令方面与我们的细调版本之间存在明显的性能差距。在保留的测试集上，基模型不能一致地生成有效的元令，而使用ACCeLLiuM数据集进行细调的模型能够为87%的数据并行循环生成具有正确指令类型的有效元令，并且在50%的情况下生成包含正确指令、子句、子句顺序和变量的确切元令。即使不准确，生成的元令也经常以不同的顺序包含正确的子句，或者包含额外的子句以实现更精细的并行执行、数据移动和并发控制，为严格字符串匹配之外的实际应用提供了价值。通过公开发布代码、模型和数据集作为ACCeLLiuM，我们希望建立一个可重复的基准，用于基于大型语言模型的OpenACC元令生成，并降低自动将串行编写的程序卸载到GPU上的障碍。', 'title_zh': 'ACCeLLiuM: 监督微调以实现自动化OpenACCpragma生成'}
{'arxiv_id': 'arXiv:2509.20378', 'title': 'Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation', 'authors': 'Sirui Wang, Andong Chen, Tiejun Zhao', 'link': 'https://arxiv.org/abs/2509.20378', 'abstract': 'Emotional text-to-speech (E-TTS) is central to creating natural and trustworthy human-computer interaction. Existing systems typically rely on sentence-level control through predefined labels, reference audio, or natural language prompts. While effective for global emotion expression, these approaches fail to capture dynamic shifts within a sentence. To address this limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to words to obtain word-level emotion annotations, and maps them through a Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion control by directly modulating text embeddings. To support evaluation, we construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed annotations of emotional transitions. Experiments show that Emo-FiLM outperforms existing approaches on both global and fine-grained tasks, demonstrating its effectiveness and generality for expressive speech synthesis.', 'abstract_zh': '情感文本到语音（E-TTS）是创建自然可靠的人机交互的核心。现有的系统通常依赖于通过预定义标签、参考音频或自然语言提示进行句子级控制。虽然这些方法对于全局情感表达是有效的，但它们无法捕捉句子内的动态变化。为了解决这一局限，我们提出了Emo-FiLM，一种基于LLM的情感细粒度建模框架。Emo-FiLM将情感2vec的帧级特征对齐到单词上，以获取单词级情感注释，并通过Feature-wise Linear Modulation (FiLM)层进行映射，从而通过直接调节文本嵌入实现单词级情感控制。为了支持评估，我们构建了详细的情感过渡注释的数据集Fine-grained Emotion Dynamics Dataset (FEDD)。实验表明，Emo-FiLM在全局和细粒度任务上都优于现有方法，展示了其在表情合成方面的有效性和普适性。', 'title_zh': '超越全局情感：基于动态词级调制的细粒度情感语音合成'}
{'arxiv_id': 'arXiv:2509.20377', 'title': 'SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation', 'authors': 'Tomoaki Isoda', 'link': 'https://arxiv.org/abs/2509.20377', 'abstract': 'Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive tasks in recent years. However, since retrieval systems may return irrelevant content, incorporating such information into the model often leads to hallucinations. Thus, identifying and filtering out unhelpful retrieved content is a key challenge for improving RAG this http URL better integrate the internal knowledge of the model with external knowledge from retrieval, it is essential to understand what the model "knows" and "does not know" (which is also called "self-knowledge"). Based on this insight, we propose SKILL-RAG (Self-Knowledge Induced Learning and Filtering for RAG), a novel method that leverages the model\'s self-knowledge to determine which retrieved documents are beneficial for answering a given query. We design a reinforcement learning-based training framework to explicitly elicit self-knowledge from the model and employs sentence-level granularity to filter out irrelevant content while preserving useful this http URL evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several question answering benchmarks. Experimental results demonstrate that SKILL-RAG not only improves generation quality but also significantly reduces the number of input documents, validating the importance of self-knowledge in guiding the selection of high-quality retrievals.', 'abstract_zh': 'Retrieval-Augmented Generation (基于检索增强生成)在近些年显著提升了大型语言模型（LLMs）在知识密集型任务上的性能。然而，由于检索系统可能会返回无关的内容，将此类信息集成到模型中常会导致幻想。因此，识别并过滤掉无用的检索内容是提高基于检索增强生成的关键挑战之一。为了更好地将模型的内部知识与检索外部知识整合，理解模型知道什么和不知道什么（也称为“自我知识”）是必不可少的。基于这一洞察，我们提出了一种新颖的方法——SKILL-RAG（自我知识引导的学习和过滤），该方法利用模型的自我知识来确定哪些检索到的文档有助于回答给定的问题。我们设计了一种基于强化学习的训练框架，明确地从模型中提取自我知识，并采用句级粒度过滤无关内容以保留有用的内容。我们使用Llama2-7B和Qwen3-8B对多种问答基准进行了SKILL-RAG的评估。实验结果表明，SKILL-RAG不仅提高了生成质量，还显著降低了输入文档的数量，验证了自我知识在指导高质量检索选择中的重要性。', 'title_zh': 'SKILL-RAG：自我知识引导的检索增强生成学习与过滤'}
{'arxiv_id': 'arXiv:2509.20376', 'title': 'ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models', 'authors': 'Haoxuan Li, Zhen Wen, Qiqi Jiang, Chenxiao Li, Yuwei Wu, Yuchen Yang, Yiyao Wang, Xiuqi Huang, Minfeng Zhu, Wei Chen', 'link': 'https://arxiv.org/abs/2509.20376', 'abstract': 'Large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks. Understanding how LLMs internally represent knowledge remains a significant challenge. Despite Sparse Autoencoders (SAEs) have emerged as a promising technique for extracting interpretable features from LLMs, SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive. To bridge the gap between SAE features and human concepts, we present ConceptViz, a visual analytics system designed for exploring concepts in LLMs. ConceptViz implements a novel dentification => Interpretation => Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification. We demonstrate the effectiveness of ConceptViz through two usage scenarios and a user study. Our results show that ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features. Our code and user guide are publicly available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）在广泛自然语言任务上取得了显著性能。理解LLMs内部知识表示仍然是一个重大挑战。尽管稀疏自编码器（SAEs）作为从LLMs中提取可解读特征的有前景技术已 emergence，SAE特征并不天然与人类可理解的概念对齐，使得其解读过程繁琐且耗费劳动。为弥合SAE特征与人类概念之间的差距，我们提出ConceptViz，一种用于探索LLMs概念的视觉分析系统。ConceptViz 实现了一种新颖的“识别 => 解释 => 验证”流水线，使用户能够使用感兴趣的 concept 查询 SAE，交互式地探索 concept-to-feature 对齐，并通过模型行为验证来确认这些对应关系。通过两个使用场景和用户研究，我们展示了 ConceptViz 的有效性。我们的结果表明，ConceptViz 通过简化有意义 concept 表示的发现和验证，增强了可解释性研究，最终帮助研究人员构建更准确的LLM特征心理模型。我们的代码和用户指南可在以下网址获取。', 'title_zh': 'ConceptViz：对大型语言模型中概念进行探索的可视化分析方法'}
{'arxiv_id': 'arXiv:2509.20375', 'title': 'Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text', 'authors': 'Sharanya Parimanoharan, Ruwan D. Nawarathna', 'link': 'https://arxiv.org/abs/2509.20375', 'abstract': "The rapid adoption of large language models (LLMs) such as ChatGPT has blurred the line between human and AI-generated texts, raising urgent questions about academic integrity, intellectual property, and the spread of misinformation. Thus, reliable AI-text detection is needed for fair assessment to safeguard human authenticity and cultivate trust in digital communication. In this study, we investigate how well current machine learning (ML) approaches can distinguish ChatGPT-3.5-generated texts from human-written texts employing a labeled data set of 250 pairs of abstracts from a wide range of research topics. We test and compare both classical (Logistic Regression armed with classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier, and LSTM-based N-gram models) ML detection techniques. As we aim to assess each model's performance in detecting AI-generated research texts, we also aim to test whether an ensemble of these models can outperform any single detector. Results show DistilBERT achieves the overall best performance, while Logistic Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and BERT-N-gram approaches lag. The max voting ensemble of the three best models fails to surpass DistilBERT itself, highlighting the primacy of a single transformer-based representation over mere model diversity. By comprehensively assessing the strengths and weaknesses of these AI-text detection approaches, this work lays a foundation for more robust transformer frameworks with larger, richer datasets to keep pace with ever-improving generative AI models.", 'abstract_zh': '大型语言模型（LLMs）如ChatGPT的快速应用模糊了人类和AI生成文本的界限，引发了关于学术诚信、知识产权和虚假信息传播的紧迫问题。因此，需要可靠的AI文本检测以实现公平评估，保护人类的 authenticity 并促进对数字通信的信任。在本研究中，我们调查了当前机器学习（ML）方法如何区分ChatGPT-3.5生成的文本与人类撰写的文本，使用了涵盖广泛研究主题的250对摘要作为标注数据集。我们测试并比较了古典（配备经典词袋、词性标注和TF-IDF特征的逻辑回归）和基于变换器的方法（BERT结合n-gram、DistilBERT、轻量级自定义分类器的BERT和基于LSTM的n-gram模型）。旨在评估每种模型检测AI生成的研究文本的能力，同时也测试模型集合是否能优于单个检测器。结果显示，DistilBERT在综合表现上最佳，逻辑回归和BERT-自定义分类器提供了稳健且平衡的选择；LSTM和BERT-n-gram方法滞后。三最佳模型的最大投票集合未能超越DistilBERT本身，突显单个基于变换器的表示优于单纯模型多样性的重要性。通过全面评估这些AI文本检测方法的优势和弱点，本研究为基础研究奠定了更大的、更丰富的数据集的坚实基础，以跟上不断改进的生成式AI模型。', 'title_zh': '评估经典机器学习方法和基于变压器的方法在检测AI生成的科研文本中的性能'}
{'arxiv_id': 'arXiv:2509.20374', 'title': 'CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics', 'authors': 'Nithin Somasekharan, Ling Yue, Yadi Cao, Weichao Li, Patrick Emami, Pochinapeddi Sai Bhargav, Anurag Acharya, Xingyu Xie, Shaowu Pan', 'link': 'https://arxiv.org/abs/2509.20374', 'abstract': 'Large Language Models (LLMs) have demonstrated strong performance across general NLP tasks, but their utility in automating numerical experiments of complex physical system -- a critical and labor-intensive component -- remains underexplored. As the major workhorse of computational science over the past decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging testbed for evaluating the scientific capabilities of LLMs. We introduce CFDLLMBench, a benchmark suite comprising three complementary components -- CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM performance across three key competencies: graduate-level CFD knowledge, numerical and physical reasoning of CFD, and context-dependent implementation of CFD workflows. Grounded in real-world CFD practices, our benchmark combines a detailed task taxonomy with a rigorous evaluation framework to deliver reproducible results and quantify LLM performance across code executability, solution accuracy, and numerical convergence behavior. CFDLLMBench establishes a solid foundation for the development and evaluation of LLM-driven automation of numerical experiments for complex physical systems. Code and data are available at this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在通用自然语言处理任务中表现出强大的性能，但在自动化复杂物理系统数值实验中的应用——一个至关重要的劳动密集型环节——仍待深入探索。作为过去几十年中计算科学的主要工具，计算流体力学（CFD）为评估LLMs的科学能力提供了独特的挑战性测试平台。我们提出了CFDLLMBench，一个包含三个互补组件的基准套件——CFDQuery、CFDCodeBench和FoamBench——旨在全方位评估LLMs在这三大关键能力上的表现：graduate-level CFD知识、CFD的数值与物理推理以及CFD工作流的上下文相关实现。基于实际的CFD实践，我们的基准结合了详细的任务分类体系和严格的评估框架，以实现可重现的结果并量化LLMs在代码可执行性、解的准确性以及数值收敛行为等方面的性能。CFDLLMBench为LLM驱动的复杂物理系统数值实验自动化开发和评估奠定了坚实的基础。代码和数据可在以下网址获取。', 'title_zh': 'CFD-LLMBench：计算流体力学中大型语言模型评估套件'}
{'arxiv_id': 'arXiv:2509.20369', 'title': 'AI-driven formative assessment and adaptive learning in data-science education: Evaluating an LLM-powered virtual teaching assistant', 'authors': 'Fadjimata I Anaroua, Qing Li, Yan Tang, Hong P. Liu', 'link': 'https://arxiv.org/abs/2509.20369', 'abstract': "This paper presents VITA (Virtual Teaching Assistants), an adaptive distributed learning (ADL) platform that embeds a large language model (LLM)-powered chatbot (BotCaptain) to provide dialogic support, interoperable analytics, and integrity-aware assessment for workforce preparation in data science. The platform couples context-aware conversational tutoring with formative-assessment patterns designed to promote reflective reasoning. The paper describes an end-to-end data pipeline that transforms chat logs into Experience API (xAPI) statements, instructor dashboards that surface outliers for just-in-time intervention, and an adaptive pathway engine that routes learners among progression, reinforcement, and remediation content. The paper also benchmarks VITA conceptually against emerging tutoring architectures, including retrieval-augmented generation (RAG)--based assistants and Learning Tools Interoperability (LTI)--integrated hubs, highlighting trade-offs among content grounding, interoperability, and deployment complexity. Contributions include a reusable architecture for interoperable conversational analytics, a catalog of patterns for integrity-preserving formative assessment, and a practical blueprint for integrating adaptive pathways into data-science courses. The paper concludes with implementation lessons and a roadmap (RAG integration, hallucination mitigation, and LTI~1.3 / OpenID Connect) to guide multi-course evaluations and broader adoption. In light of growing demand and scalability constraints in traditional instruction, the approach illustrates how conversational AI can support engagement, timely feedback, and personalized learning at scale. Future work will refine the platform's adaptive intelligence and examine applicability across varied educational settings.", 'abstract_zh': 'VITA（虚拟教学助手）：一个集成大型语言模型的自适应分布式学习平台及其在数据科学工作准备中的应用', 'title_zh': '基于AI的形成性评估与自适应学习在数据科学教育中的应用：评估一个由LLM驱动的虚拟教学助手'}
{'arxiv_id': 'arXiv:2509.20367', 'title': 'Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models', 'authors': 'Leyi Ouyang', 'link': 'https://arxiv.org/abs/2509.20367', 'abstract': "Diplomatic events consistently prompt widespread public discussion and debate. Public sentiment plays a critical role in diplomacy, as a good sentiment provides vital support for policy implementation, helps resolve international issues, and shapes a nation's international image. Traditional methods for gauging public sentiment, such as large-scale surveys or manual content analysis of media, are typically time-consuming, labor-intensive, and lack the capacity for forward-looking analysis. We propose a novel framework that identifies specific modifications for diplomatic event narratives to shift public sentiment from negative to neutral or positive. First, we train a language model to predict public reaction towards diplomatic events. To this end, we construct a dataset comprising descriptions of diplomatic events and their associated public discussions. Second, guided by communication theories and in collaboration with domain experts, we predetermined several textual features for modification, ensuring that any alterations changed the event's narrative framing while preserving its core this http URL develop a counterfactual generation algorithm that employs a large language model to systematically produce modified versions of an original text. The results show that this framework successfully shifted public sentiment to a more favorable state with a 70\\% success rate. This framework can therefore serve as a practical tool for diplomats, policymakers, and communication specialists, offering data-driven insights on how to frame diplomatic initiatives or report on events to foster a more desirable public sentiment.", 'abstract_zh': '外交事件一致引发广泛的公众讨论和辩论。公众情绪在外交活动中扮演着关键角色，良好的公众情绪为政策实施提供重要支持，有助于解决国际问题，并塑造一个国家的国际形象。传统的公众情绪评估方法，如大规模调查或手动分析媒体内容，通常耗时、劳动密集且缺乏前瞻性分析的能力。我们提出一种新的框架，以识别特定的外交事件叙事修改，将公众情绪从负面转变为中性或积极。首先，我们训练语言模型预测公众对外交事件的反应。为此，我们构建了一个包含外交事件描述及其相关公众讨论的数据集。其次，在沟通理论的指导下并与领域专家合作，我们预先确定了若干文本特征进行修改，确保任何改动改变了事件的叙事框架，但保留了其核心内容。然后，我们开发了一种反事实生成算法，使用大型语言模型系统地生成原始文本的修改版本。结果显示，该框架成功将公众情绪转向更积极的状态，成功率达到了70%。该框架因此可以作为外交官、政策制定者和沟通专家的实际工具，提供数据驱动的见解，说明如何塑造外交倡议或报道事件以促进更受欢迎的公众情绪。', 'title_zh': '外交事件中公众情绪解读：基于大规模语言模型的反事实分析框架'}
