{'arxiv_id': 'arXiv:2510.05070', 'title': 'ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning', 'authors': 'Siheng Zhao, Yanjie Ze, Yue Wang, C. Karen Liu, Pieter Abbeel, Guanya Shi, Rocky Duan', 'link': 'https://arxiv.org/abs/2510.05070', 'abstract': 'Humanoid whole-body loco-manipulation promises transformative capabilities for daily service and warehouse tasks. While recent advances in general motion tracking (GMT) have enabled humanoids to reproduce diverse human motions, these policies lack the precision and object awareness required for loco-manipulation. To this end, we introduce ResMimic, a two-stage residual learning framework for precise and expressive humanoid control from human motion data. First, a GMT policy, trained on large-scale human-only motion, serves as a task-agnostic base for generating human-like whole-body movements. An efficient but precise residual policy is then learned to refine the GMT outputs to improve locomotion and incorporate object interaction. To further facilitate efficient training, we design (i) a point-cloud-based object tracking reward for smoother optimization, (ii) a contact reward that encourages accurate humanoid body-object interactions, and (iii) a curriculum-based virtual object controller to stabilize early training. We evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results show substantial gains in task success, training efficiency, and robustness over strong baselines. Videos are available at this https URL .', 'abstract_zh': 'humanoid全身移动与操作 promise 日常服务与仓储任务的变革能力。虽然近期对通用运动跟踪（GMT）的进展使类人机器人能够再现多种人类动作，但这些策略在移动操作中缺乏所需的精准度和物体意识。为此，我们引入了 ResMimic，这是一种基于人类动作数据的两阶段残差学习框架，以实现精确且表达力强的类人控制。首先，一个在大规模仅人类动作数据上训练的 GMT 策略充当通用任务基础，生成类似人类的全身动作。接着，学习一个高效且精确的残差策略来细化 GMT 输出，改善移动并整合物体交互。为了进一步促进高效训练，我们设计了 (i) 基于点云的目标追踪奖励以使优化更加平滑，(ii) 促进准确的人体-物体交互的接触奖励，以及 (iii) 基于课程的学习虚拟物体控制器以稳定早期训练。我们在仿真和实际的 Unitree G1 类人机器人上评估了 ResMimic 的性能。结果显示，与强大的基线模型相比，在任务成功率、训练效率和稳健性方面取得了显著提升。视频可在以下链接获取：this https URL。', 'title_zh': 'ResMimic: 从通用运动跟踪到类人全身运动操作的残差学习方法'}
{'arxiv_id': 'arXiv:2510.05061', 'title': 'Automaton Constrained Q-Learning', 'authors': 'Anastasios Manganaris, Vittorio Giammarino, Ahmed H. Qureshi', 'link': 'https://arxiv.org/abs/2510.05061', 'abstract': 'Real-world robotic tasks often require agents to achieve sequences of goals while respecting time-varying safety constraints. However, standard Reinforcement Learning (RL) paradigms are fundamentally limited in these settings. A natural approach to these problems is to combine RL with Linear-time Temporal Logic (LTL), a formal language for specifying complex, temporally extended tasks and safety constraints. Yet, existing RL methods for LTL objectives exhibit poor empirical performance in complex and continuous environments. As a result, no scalable methods support both temporally ordered goals and safety simultaneously, making them ill-suited for realistic robotics scenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm that addresses this gap by combining goal-conditioned value learning with automaton-guided reinforcement. ACQL supports most LTL task specifications and leverages their automaton representation to explicitly encode stage-wise goal progression and both stationary and non-stationary safety constraints. We show that ACQL outperforms existing methods across a range of continuous control tasks, including cases where prior methods fail to satisfy either goal-reaching or safety constraints. We further validate its real-world applicability by deploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a cluttered, cabinet-like space with safety constraints. Our results demonstrate that ACQL is a robust and scalable solution for learning robotic behaviors according to rich temporal specifications.', 'abstract_zh': '实时机器人任务通常要求智能体在遵守时间变化的安全约束条件下实现一系列目标。然而，标准强化学习（RL）范式在这类设置中本质上是有限制的。将RL与线性时态逻辑（LTL）结合是一个自然的解决方案，LTL是一种用于描述复杂、时序扩展任务和安全约束的形式语言。然而，现有的针对LTL目标的RL方法在复杂和连续环境中表现出较差的经验性能。因此，目前没有可扩展的方法同时支持时序有序的目标和安全约束，使其不适合现实的机器人场景。我们提出了自动机约束Q-learning（ACQL）算法，该算法通过结合目标条件的价值学习和自动机引导的强化学习填补了这一空白。ACQL支持大多数LTL任务规范，并利用其自动机表示显式地编码阶段式目标进展以及静态和非静态的安全约束。我们展示了ACQL在一系列连续控制任务中优于现有方法，包括先前方法无法满足目标获取或安全约束的情况。我们进一步通过在包含安全约束的杂乱柜式空间中部署6-DOF机器人臂执行目标获取任务来验证其实际应用性。结果表明，ACQL是一种鲁棒且可扩展的解决方案，以用于根据丰富的时序规范学习机器人行为。', 'title_zh': '自动机约束的Q-learning'}
{'arxiv_id': 'arXiv:2510.05057', 'title': 'StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation', 'authors': 'Mingyu Liu, Jiuhe Shu, Hui Chen, Zeju Li, Canyu Zhao, Jiange Yang, Shenyuan Gao, Hao Chen, Chunhua Shen', 'link': 'https://arxiv.org/abs/2510.05057', 'abstract': 'A fundamental challenge in embodied intelligence is developing expressive and compact state representations for efficient world modeling and decision making. However, existing methods often fail to achieve this balance, yielding representations that are either overly redundant or lacking in task-critical information. We propose an unsupervised approach that learns a highly compressed two-token state representation using a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing VLA-based models, improving performance by 14.3% on LIBERO and 30% in real-world task success with minimal inference overhead. More importantly, we find that the difference between these tokens, obtained via latent interpolation, naturally serves as a highly effective latent action, which can be further decoded into executable robot actions. This emergent capability reveals that our representation captures structured dynamics without explicit supervision. We name our method StaMo for its ability to learn generalizable robotic Motion from compact State representation, which is encoded from static images, challenging the prevalent dependence to learning latent action on complex architectures and video data. The resulting latent actions also enhance policy co-training, outperforming prior methods by 10.4% with improved interpretability. Moreover, our approach scales effectively across diverse data sources, including real-world robot data, simulation, and human egocentric video.', 'abstract_zh': '一种体态智能的基本挑战是开发高效的简洁状态表示，以实现高效的世界建模和决策。现有的方法往往难以实现这一平衡，导致状态表示要么冗余过多，要么缺乏关键任务信息。我们提出一种无监督方法，利用轻量级编码器和预训练的扩散变压器（DiT）解码器学习高度压缩的两词元状态表示，利用其强大的生成先验。我们的表示方法高效、可解释，并能无缝集成到现有的基于VLA的模型中，在LIBERO上性能提升14.3%，在现实世界任务成功率上提升30%，且几乎没有任何推理开销。更重要的是，我们发现，通过潜在插值获得的这两个词元之间的差异自然地充当了一个高度有效的潜动作，可以进一步解码为可执行的机器人动作。这一新兴能力揭示了我们的表示方法能够在无需显式监督的情况下捕捉到结构化的动力学。我们将该方法命名为StaMo，强调其从静态图像中学习可泛化的机器人运动能力，挑战了学习潜动作对复杂架构和视频数据的依赖。生成的潜动作还增强了策略协同训练，在解释性增强的情况下优于先前的方法10.4%。此外，我们的方法能够有效地扩展到多种数据源，包括真实世界的机器人数据、仿真数据和人类主观视角视频。', 'title_zh': 'StaMo: 从紧凑状态表示中学习可泛化的机器人运动的无监督学习'}
{'arxiv_id': 'arXiv:2510.05001', 'title': 'Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot', 'authors': 'Aditya Sripada, Abhishek Warrier', 'link': 'https://arxiv.org/abs/2510.05001', 'abstract': "Robotic locomotion research typically draws from biologically inspired leg designs, yet many human-engineered settings can benefit from non-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from Interstellar into a 0.25 m, 0.99 kg research platform with seven actuated degrees of freedom. The film shows two primary gaits: a bipedal-like walk and a high-speed rolling mode. For TARS3D, we build reduced-order models for each, derive closed-form limit-cycle conditions, and validate the predictions on hardware. Experiments confirm that the robot respects its +/-150 degree hip limits, alternates left-right contacts without interference, and maintains an eight-step hybrid limit cycle in rolling mode. Because each telescopic leg provides four contact corners, the rolling gait is modeled as an eight-spoke double rimless wheel. The robot's telescopic leg redundancy implies a far richer gait repertoire than the two limit cycles treated analytically. So, we used deep reinforcement learning (DRL) in simulation to search the unexplored space. We observed that the learned policy can recover the analytic gaits under the right priors and discover novel behaviors as well. Our findings show that TARS3D's fiction-inspired bio-transcending morphology can realize multiple previously unexplored locomotion modes and that further learning-driven search is likely to reveal more. This combination of analytic synthesis and reinforcement learning opens a promising pathway for multimodal robotics.", 'abstract_zh': '基于《星际穿越》中TARS机器人的3D研究：从生物启发的腿设计到非anthropomorphic形态的转化', 'title_zh': '基于TARS启发的机器人第一性原理与RL行走与滚动及其扩展'}
{'arxiv_id': 'arXiv:2510.04991', 'title': 'Efficient Navigation in Unknown Indoor Environments with Vision-Language Models', 'authors': 'D. Schwartz, K. Kondo, J. P. How', 'link': 'https://arxiv.org/abs/2510.04991', 'abstract': 'We present a novel high-level planning framework that leverages vision-language models (VLMs) to improve autonomous navigation in unknown indoor environments with many dead ends. Traditional exploration methods often take inefficient routes due to limited global reasoning and reliance on local heuristics. In contrast, our approach enables a VLM to reason directly about an occupancy map in a zero-shot manner, selecting subgoals that are likely to lead to more efficient paths. At each planning step, we convert a 3D occupancy grid into a partial 2D map of the environment, and generate candidate subgoals. Each subgoal is then evaluated and ranked against other candidates by the model. We integrate this planning scheme into DYNUS \\cite{kondo2025dynus}, a state-of-the-art trajectory planner, and demonstrate improved navigation efficiency in simulation. The VLM infers structural patterns (e.g., rooms, corridors) from incomplete maps and balances the need to make progress toward a goal against the risk of entering unknown space. This reduces common greedy failures (e.g., detouring into small rooms) and achieves about 10\\% shorter paths on average.', 'abstract_zh': '一种利用视觉语言模型改进未知室内环境自主导航的新型高层次规划框架', 'title_zh': '使用视觉语言模型在未知室内环境中高效导航'}
{'arxiv_id': 'arXiv:2510.04898', 'title': 'HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks', 'authors': 'Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson', 'link': 'https://arxiv.org/abs/2510.04898', 'abstract': 'Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\\times$, and accelerates inference speed by $120\\times$. Code is publicly available at this https URL', 'abstract_zh': '基于具有强泛化能力的语言和视觉基础模型并训练大规模机器人数据，在此基础上新兴的Vision-Language-Action (VLA)模型提供了一种学习通用机器人策略的有前途的方法。然而，现有VLA的一个关键缺点是其极高的推理成本。本文提出HyperVLA以解决这一问题。与现有的一体化VLA不同，HyperVLA使用了一种新颖的超网络(HN)-基于的架构，在推理时仅激活一个小的任务特定策略，同时仍保留了在训练过程中容纳多种多任务行为所需的高模型容量。成功训练基于HN的VLA并非易事，因此HyperVLA包含了几种关键技术设计特征，以提高其性能，包括有效利用现有视觉基础模型的先验知识、HN规范化和一种动作生成策略。与一体化VLA相比，HyperVLA在零样本泛化和少量样本适应方面达到了相似甚至更高的成功率，同时显著降低了推理成本。与当前最先进的VLA模型OpenVLA相比，HyperVLA在测试时激活的参数数量减少了90倍，并加速了推理速度120倍。代码已公开于此[链接]。', 'title_zh': 'HyperVLA：通过超网络进行高效的视觉-语言-动作模型推理'}
{'arxiv_id': 'arXiv:2510.04774', 'title': 'Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy', 'authors': 'Weixu Zhu, Marco Dorigo, Mary Katherine Heinrich', 'link': 'https://arxiv.org/abs/2510.04774', 'abstract': 'Our recently introduced self-organizing nervous system (SoNS) provides robot swarms with 1) ease of behavior design and 2) global estimation of the swarm configuration and its collective environment, facilitating the implementation of online automatic code generation for robot swarms. In a demonstration with 6 real robots and simulation trials with >30 robots, we show that when a SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code generated by an external LLM on the fly, completing its mission with an 85% success rate.', 'abstract_zh': '我们最近引入的自组织神经系统（SoNS）为机器人 swarm 提供了 1) 行为设计的便捷性和 2) 全局估计 swarm 配置及其集体环境的功能，从而促进机器人 swarm 在线自动代码生成的实现。在一项使用 6 台真实机器人和超过 30 台机器人的仿真试验中，我们显示当一个增强有 SoNS 的机器人 swarm 受阻时，它可以自动请求并运行外部 LLM 生成的代码，成功完成任务的比例达到 85%。', 'title_zh': '基于LLMs和自组织层次结构的机器人 swarm在线自动代码生成'}
{'arxiv_id': 'arXiv:2510.04696', 'title': 'Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly', 'authors': 'Alexander L. Mitchell, Joe Watson, Ingmar Posner', 'link': 'https://arxiv.org/abs/2510.04696', 'abstract': 'There are many challenges in bimanual assembly, including high-level sequencing, multi-robot coordination, and low-level, contact-rich operations such as component mating. Task and motion planning (TAMP) methods, while effective in this domain, may be prohibitively slow to converge when adapting to disturbances that require new task sequencing and optimisation. These events are common during tight-tolerance assembly, where difficult-to-model dynamics such as friction or deformation require rapid replanning and reattempts. Moreover, defining explicit task sequences for assembly can be cumbersome, limiting flexibility when task replanning is required. To simplify this planning, we introduce a decentralised gradient-based framework that uses a piecewise continuous energy function through the automatic composition of adaptive potential functions. This approach generates sub-goals using only myopic optimisation, rather than long-horizon planning. It demonstrates effectiveness at solving long-horizon tasks due to the structure and adaptivity of the energy function. We show that our approach scales to physical bimanual assembly tasks for constructing tight-tolerance assemblies. In these experiments, we discover that our gradient-based rapid replanning framework generates automatic retries, coordinated motions and autonomous handovers in an emergent fashion.', 'abstract_zh': '双臂装配中的挑战及其分布式梯度基规划方法', 'title_zh': '建立梯度以梯度为基础：双臂机器人装配的去中心化能量函数'}
{'arxiv_id': 'arXiv:2510.04592', 'title': 'MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation', 'authors': 'Yilin Mei, Peng Qiu, Wei Zhang, WenChao Zhang, Wenjie Song', 'link': 'https://arxiv.org/abs/2510.04592', 'abstract': 'Recent advances in robotics have been largely driven by imitation learning, which depends critically on large-scale, high-quality demonstration data. However, collecting such data remains a significant challenge-particularly for mobile manipulators, which must coordinate base locomotion and arm manipulation in high-dimensional, dynamic, and partially observable environments. Consequently, most existing research remains focused on simpler tabletop scenarios, leaving mobile manipulation relatively underexplored. To bridge this gap, we present \\textit{MobRT}, a digital twin-based framework designed to simulate two primary categories of complex, whole-body tasks: interaction with articulated objects (e.g., opening doors and drawers) and mobile-base pick-and-place operations. \\textit{MobRT} autonomously generates diverse and realistic demonstrations through the integration of virtual kinematic control and whole-body motion planning, enabling coherent and physically consistent execution. We evaluate the quality of \\textit{MobRT}-generated data across multiple baseline algorithms, establishing a comprehensive benchmark and demonstrating a strong correlation between task success and the number of generated trajectories. Experiments integrating both simulated and real-world demonstrations confirm that our approach markedly improves policy generalization and performance, achieving robust results in both simulated and real-world environments.', 'abstract_zh': '最近机器人领域的进展很大程度上得益于模仿学习，这依赖于大规模、高质量示范数据。然而，收集这类数据仍然是一个重大挑战，尤其是在移动 manipulator 的情况下，它们必须在高维度、动态且部分可观测的环境中协调底座移动和手臂操作。因此，现有的大多数研究仍然集中在较为简单的桌面场景上，移动 manipulation 仍然相对未被充分探索。为弥补这一缺口，我们提出了 MobRT，一个基于数字孪生的框架，旨在模拟两类复杂的整体任务：与运动物体的交互（例如，开 door 和 drawer）和移动底座抓取-放置操作。MobRT 通过整合虚拟运动控制和整体运动规划，自主生成多样且真实的示范，实现统一且物理一致的执行。我们利用多种基线算法评估 MobRT 生成数据的质量，建立了全面的基准，并展示了任务成功率与生成轨迹数量之间的强烈关联。结合仿真和现实世界示范的实验确认，我们的方法显著提高了策略的泛化能力和性能，在仿真和现实世界环境中均取得了稳健的结果。', 'title_zh': 'MobRT：基于数字孪生的移动 manipulation 可扩展学习框架'}
{'arxiv_id': 'arXiv:2510.04354', 'title': 'Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators', 'authors': "Apurva Badithela, David Snyder, Lihan Zha, Joseph Mikhail, Matthew O'Kelly, Anushri Dixit, Anirudha Majumdar", 'link': 'https://arxiv.org/abs/2510.04354', 'abstract': 'Rapid progress in imitation learning, foundation models, and large-scale datasets has led to robot manipulation policies that generalize to a wide-range of tasks and environments. However, rigorous evaluation of these policies remains a challenge. Typically in practice, robot policies are often evaluated on a small number of hardware trials without any statistical assurances. We present SureSim, a framework to augment large-scale simulation with relatively small-scale real-world testing to provide reliable inferences on the real-world performance of a policy. Our key idea is to formalize the problem of combining real and simulation evaluations as a prediction-powered inference problem, in which a small number of paired real and simulation evaluations are used to rectify bias in large-scale simulation. We then leverage non-asymptotic mean estimation algorithms to provide confidence intervals on mean policy performance. Using physics-based simulation, we evaluate both diffusion policy and multi-task fine-tuned \\(\\pi_0\\) on a joint distribution of objects and initial conditions, and find that our approach saves over \\(20-25\\%\\) of hardware evaluation effort to achieve similar bounds on policy performance.', 'abstract_zh': '快速发展的模仿学习、基础模型和大规模数据集使得机器人的操作策略能够泛化到广泛的任务和环境。然而，这些策略的严格评估仍然是一项挑战。通常在实践中，机器人的策略是在硬件试验上进行评估，但没有任何统计保证。我们提出SureSim框架，通过结合大规模模拟与相对较小规模的实际测试，以提供政策在实际世界性能上的可靠推断。我们的核心思想是将结合实际与模拟评估的问题形式化为一个预测驱动的推断问题，在其中，少量配对的实际和模拟评估被用来纠正大规模模拟中的偏差。然后，我们利用非渐近平均估计算法为平均政策性能提供置信区间。使用基于物理的模拟，我们评估了扩散策略和多任务微调的\\(\\pi_0\\)在对象和初始条件联合分布上的性能，并发现我们的方法在实现相同政策性能边界的情况下节省了超过20-25%的硬件评估努力。', 'title_zh': '基于 Imperfect 模拟器的可信赖且可扩展的机器人策略评估'}
{'arxiv_id': 'arXiv:2510.04353', 'title': 'Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation', 'authors': 'Stephen McCrory, Romeo Orsolino, Dhruv Thanki, Luigi Penco, Robert Griffin', 'link': 'https://arxiv.org/abs/2510.04353', 'abstract': 'Teleoperation is a powerful method to generate reference motions and enable humanoid robots to perform a broad range of tasks. However, teleoperation becomes challenging when using hand contacts and non-coplanar surfaces, often leading to motor torque saturation or loss of stability through slipping. We propose a centroidal stability-based retargeting method that dynamically adjusts contact points and posture during teleoperation to enhance stability in these difficult scenarios. Central to our approach is an efficient analytical calculation of the stability margin gradient. This gradient is used to identify scenarios for which stability is highly sensitive to teleoperation setpoints and inform the local adjustment of these setpoints. We validate the framework in simulation and hardware by teleoperating manipulation tasks on a humanoid, demonstrating increased stability margins. We also demonstrate empirically that higher stability margins correlate with improved impulse resilience and joint torque margin.', 'abstract_zh': '基于质心稳定性的遥操作目标转换方法：在手部接触和非共面表面场景中动态调整接触点和姿态以增强稳定性', 'title_zh': '稳定性意识的人形多接触远程操控调整'}
{'arxiv_id': 'arXiv:2510.04246', 'title': 'ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context', 'authors': 'Huiwon Jang, Sihyun Yu, Heeseung Kwon, Hojin Jeon, Younggyo Seo, Jinwoo Shin', 'link': 'https://arxiv.org/abs/2510.04246', 'abstract': "Leveraging temporal context is crucial for success in partially observable robotic tasks. However, prior work in behavior cloning has demonstrated inconsistent performance gains when using multi-frame observations. In this paper, we introduce ContextVLA, a policy model that robustly improves robotic task performance by effectively leveraging multi-frame observations. Our approach is motivated by the key observation that Vision-Language-Action models (VLA), i.e., policy models built upon a Vision-Language Model (VLM), more effectively utilize multi-frame observations for action generation. This suggests that VLMs' inherent temporal understanding capability enables them to extract more meaningful context from multi-frame observations. However, the high dimensionality of video inputs introduces significant computational overhead, making VLA training and inference inefficient. To address this, ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation. Our experiments show that ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.", 'abstract_zh': '利用时间上下文对于部分可观测机器人任务的成功至关重要。然而，先前的行为克隆工作在使用多帧观察时显示出了不一致的性能提升。在本文中，我们引入了ContextVLA，一种通过有效利用多帧观察来 robustly 提高机器人任务性能的策略模型。我们的方法受到一个关键观察的启发，即视觉-语言-行动模型（VLA），即基于视觉-语言模型（VLM）构建的策略模型，更有效地利用多帧观察来进行行动生成。这表明VLMs内在的时间理解能力使它们能够从多帧观察中提取更有意义的上下文。然而，视频输入的高维度引入了显著的计算 overhead，使得VLA的训练和推理不够高效。为了解决这个问题，ContextVLA 将过去观察压缩为单一上下文令牌，使策略能够高效利用时间上下文进行行动生成。我们的实验表明，ContextVLA 在单帧VLA的基础上一致性地提高了性能，并实现了全多帧训练的好处，但同时减少了训练和推理时间。', 'title_zh': 'ContextVLA：带有 amortized 多帧语境的视觉-语言-行动模型'}
{'arxiv_id': 'arXiv:2510.04234', 'title': 'Flexible Locomotion Learning with Diffusion Model Predictive Control', 'authors': 'Runhan Huang, Haldun Balim, Heng Yang, Yilun Du', 'link': 'https://arxiv.org/abs/2510.04234', 'abstract': 'Legged locomotion demands controllers that are both robust and adaptable, while remaining compatible with task and safety considerations. However, model-free reinforcement learning (RL) methods often yield a fixed policy that can be difficult to adapt to new behaviors at test time. In contrast, Model Predictive Control (MPC) provides a natural approach to flexible behavior synthesis by incorporating different objectives and constraints directly into its optimization process. However, classical MPC relies on accurate dynamics models, which are often difficult to obtain in complex environments and typically require simplifying assumptions. We present Diffusion-MPC, which leverages a learned generative diffusion model as an approximate dynamics prior for planning, enabling flexible test-time adaptation through reward and constraint based optimization. Diffusion-MPC jointly predicts future states and actions; at each reverse step, we incorporate reward planning and impose constraint projection, yielding trajectories that satisfy task objectives while remaining within physical limits. To obtain a planning model that adapts beyond imitation pretraining, we introduce an interactive training algorithm for diffusion based planner: we execute our reward-and-constraint planner in environment, then filter and reweight the collected trajectories by their realized returns before updating the denoiser. Our design enables strong test-time adaptability, allowing the planner to adjust to new reward specifications without retraining. We validate Diffusion-MPC on real world, demonstrating strong locomotion and flexible adaptation.', 'abstract_zh': '基于扩散模型的模型预测控制（Diffusion-MPC）：强鲁棒性与适应性的腿式运动控制', 'title_zh': '基于扩散模型预测控制的灵活运动学习'}
{'arxiv_id': 'arXiv:2510.04190', 'title': 'Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification', 'authors': 'Jian-jie Zheng, Chih-kai Yang, Po-han Chen, Lyn Chao-ling Chen', 'link': 'https://arxiv.org/abs/2510.04190', 'abstract': 'In the study, the social robot act as a patrol to recognize and notify illegal parking in real-time. Dual-model pipeline method and large multimodal model were compared, and the GPT-4o multimodal model was adopted in license plate recognition without preprocessing. For moving smoothly on a flat ground, the robot navigated in a simulated parking lot in the experiments. The robot changes angle view of the camera automatically to capture the images around with the format of license plate number. From the captured images of the robot, the numbers on the plate are recognized through the GPT-4o model, and identifies legality of the numbers. When an illegal parking is detected, the robot sends Line messages to the system manager immediately. The contribution of the work is that a novel multimodal deep learning method has validated with high accuracy in license plate recognition, and a social assistive robot is also provided for solving problems in a real scenario, and can be applied in an indoor parking lot.', 'abstract_zh': '在研究中，社会机器人作为巡逻员实现实时识别和通知非法停车。对比了双模态管道方法和大型多模态模型，并在车牌识别中采用了无需预处理的GPT-4o多模态模型。为了在平坦地面上顺畅移动，机器人在实验中导航于模拟停车场中。机器人自动调整摄像头视角以捕捉带有车牌号格式的图像。通过GPT-4o模型识别机器人捕获的图像中的车牌号码，并判定其合法性。检测到非法停车时，机器人会立即向系统管理员发送Line消息。该工作的贡献在于，一种新颖的多模态深度学习方法在车牌识别中得到了高精度验证，并提供了一种用于解决实际场景问题的社会辅助机器人，可以在室内停车场应用。', 'title_zh': 'Zenbo巡逻：基于多模态深度学习的社会辅助机器人及其在实时非法停车识别与通知中的应用'}
{'arxiv_id': 'arXiv:2510.04076', 'title': 'From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents', 'authors': 'Amin Vahidi-Moghaddam, Sayed Pedram Haeri Boroujeni, Iman Jebellat, Ehsan Jebellat, Niloufar Mehrabi, Zhaojian Li', 'link': 'https://arxiv.org/abs/2510.04076', 'abstract': 'One of the main challenges in modern control applications, particularly in robot and vehicle motion control, is achieving accurate, fast, and safe movement. To address this, optimal control policies have been developed to enforce safety while ensuring high performance. Since basic first-principles models of real systems are often available, model-based controllers are widely used. Model predictive control (MPC) is a leading approach that optimizes performance while explicitly handling safety constraints. However, obtaining accurate models for complex systems is difficult, which motivates data-driven alternatives. ML-based MPC leverages learned models to reduce reliance on hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal policies directly from interaction data. Data-enabled predictive control (DeePC) goes further by bypassing modeling altogether, directly learning safe policies from raw input-output data. Recently, large language model (LLM) agents have also emerged, translating natural language instructions into structured formulations of optimal control problems. Despite these advances, data-driven policies face significant limitations. They often suffer from slow response times, high computational demands, and large memory needs, making them less practical for real-world systems with fast dynamics, limited onboard computing, or strict memory constraints. To address this, various technique, such as reduced-order modeling, function-approximated policy learning, and convex relaxations, have been proposed to reduce computational complexity. In this paper, we present eight such approaches and demonstrate their effectiveness across real-world applications, including robotic arms, soft robots, and vehicle motion control.', 'abstract_zh': '现代控制应用中的一大挑战，尤其是在机器人和车辆运动控制领域，是在实现准确、快速和安全的运动方面的难题。为此，已经开发了最优控制策略，以确保安全性的同时保证高性能。由于实际系统的基础物理模型通常可用，因此基于模型的控制器被广泛使用。模型预测控制（MPC）是一种领先的方法，它在明确处理安全约束的同时优化性能。然而，获得复杂系统的精确模型是困难的，这促进了数据驱动的替代方案。基于机器学习的MPC利用学习到的模型减少对外人工设计动力学的依赖，而强化学习（RL）可以直接从交互数据中学习近最优策略。数据启用预测控制（DeePC）更进一步，完全绕过了建模，直接从原始输入输出数据中学习安全策略。最近，大型语言模型（LLM）代理也出现了，能够将自然语言指令转化为最优控制问题的结构化表述。尽管取得了这些进展，但数据驱动的策略仍然面临着显著的局限性。它们常常遭受响应时间长、计算需求高和内存需求大的问题，使得它们在具有快速动力学、有限机载计算能力或严格内存限制的实际系统中不太实用。为了解决这个问题，已经提出了各种技术，如降阶建模、函数逼近策略学习和凸松弛，以降低计算复杂性。在本文中，我们介绍了这八种方法，并展示了它们在实际应用中的有效性，包括机器人手臂、软体机器人和车辆运动控制。', 'title_zh': '从暗影到光明：迈向跨MPC、DeePC、RL和LLM代理的安全与高效策略学习'}
{'arxiv_id': 'arXiv:2510.04074', 'title': 'Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback', 'authors': 'Chung-Pang Wang, Changwei Chen, Xiao Liang, Soofiyan Atar, Florian Richter, Michael Yip', 'link': 'https://arxiv.org/abs/2510.04074', 'abstract': 'Autonomous surgical systems must adapt to highly dynamic environments where tissue properties and visual cues evolve rapidly. Central to such adaptability is feedback: the ability to sense, interpret, and respond to changes during execution. While feedback mechanisms have been explored in surgical robotics, ranging from tool and tissue tracking to error detection, existing methods remain limited in handling the topological and perceptual challenges of tissue dissection. In this work, we propose a feedback-enabled framework for autonomous tissue dissection that explicitly reasons about topological changes from endoscopic images after each dissection action. This structured feedback guides subsequent actions, enabling the system to localize dissection progress and adapt policies online. To improve the reliability of such feedback, we introduce visibility metrics that quantify tissue exposure and formulate optimal controller designs that actively manipulate tissue to maximize visibility. Finally, we integrate these feedback mechanisms with both planning-based and learning-based dissection methods, and demonstrate experimentally that they significantly enhance autonomy, reduce errors, and improve robustness in complex surgical scenarios.', 'abstract_zh': '自主手术系统必须适应高度动态的环境，其中组织性质和视觉线索会迅速变化。反馈机制对于这种适应性至关重要，包括在执行过程中感知、解释和响应变化的能力。尽管在手术机器人中已经探索了反馈机制，从工具和组织追踪到错误检测，现有方法在处理组织分离过程中的拓扑和知觉挑战方面仍有限制。在本文中，我们提出了一种具有反馈机制的自主组织分离框架，该框架在每次分离操作后明确地从内窥镜图像中推理拓扑变化。这种结构化的反馈引导后续操作，使系统能够定位分离进度并在线调整策略。为了提高反馈的可靠性，我们引入了可视化度量标准，这些标准量化了组织的暴露程度，并制定了主动操纵组织以最大化可视化度量的最优控制器设计。最后，我们将这些反馈机制与基于规划和基于学习的分离方法结合，实验结果表明，它们显著增强了自主性、减少了错误并提高了复杂手术场景中的鲁棒性。', 'title_zh': '反馈很重要：借助视觉和拓扑反馈增强自主分解能力'}
{'arxiv_id': 'arXiv:2510.04041', 'title': 'SITCOM: Scaling Inference-Time COMpute for VLAs', 'authors': 'Ayudh Saxena, Harsh Shah, Sandeep Routray, Rishi Rajesh Shah, Esha Pahwa', 'link': 'https://arxiv.org/abs/2510.04041', 'abstract': 'Learning robust robotic control policies remains a major challenge due to the high cost of collecting labeled data, limited generalization to unseen environments, and difficulties in planning over long horizons. While Vision-Language-Action (VLA) models offer a promising solution by grounding natural language instructions into single-step control commands, they often lack mechanisms for lookahead and struggle with compounding errors in dynamic tasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs (SITCOM), a framework that augments any pretrained VLA with model-based rollouts and reward-based trajectory selection, inspired by Model Predictive Control algorithm. SITCOM leverages a learned dynamics model to simulate multi-step action rollouts to select the best candidate plan for real-world execution, transforming one-shot VLAs into robust long-horizon planners. We develop an efficient transformer-based dynamics model trained on large-scale BridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim gap, and score candidate rollouts using rewards from simulator. Through comprehensive evaluation across multiple tasks and settings in the SIMPLER environment, we demonstrate that SITCOM when combined with a good reward function can significantly improve task completion rate from 48% to 72% using trained dynamics model.', 'abstract_zh': '基于视觉-语言-行动模型的推理时长计算扩展框架（SITCOM）：多步动作模拟与奖励导向轨迹选择', 'title_zh': 'SITCOM: 扩展 VLAs 推理时的计算规模'}
{'arxiv_id': 'arXiv:2510.03910', 'title': 'WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding', 'authors': 'Akhil Padmanabha, Jessie Yuan, Tanisha Mehta, Rajat Kumar Jenamani, Eric Hu, Victoria de León, Anthony Wertz, Janavi Gupta, Ben Dodson, Yunting Yan, Carmel Majidi, Tapomayukh Bhattacharjee, Zackory Erickson', 'link': 'https://arxiv.org/abs/2510.03910', 'abstract': "Millions of people around the world need assistance with feeding. Robotic feeding systems offer the potential to enhance autonomy and quality of life for individuals with impairments and reduce caregiver workload. However, their widespread adoption has been limited by technical challenges such as estimating bite timing, the appropriate moment for the robot to transfer food to a user's mouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with LEarned bite timing, a system that accurately predicts bite timing by leveraging wearable sensor data to be highly reactive to natural user cues such as head movements, chewing, and talking. We train a supervised regression model on bite timing data from 14 participants and incorporate a user-adjustable assertiveness threshold to convert predictions into proceed or stop commands. In a study with 15 participants without motor impairments with the Obi feeding robot, WAFFLE performs statistically on par with or better than baseline methods across measures of feeling of control, robot understanding, and workload, and is preferred by the majority of participants for both individual and social dining. We further demonstrate WAFFLE's generalizability in a study with 2 participants with motor impairments in their home environments using a Kinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling natural, reactive bite timing that generalizes across users, robot hardware, robot positioning, feeding trajectories, foods, and both individual and social dining contexts.", 'abstract_zh': '基于穿戴传感器学习咬食时机的辅助进食系统：WAFFLE', 'title_zh': 'WAFFLE: 一种基于穿戴设备的机器人辅助进食咀嚼时机估计方法'}
{'arxiv_id': 'arXiv:2510.03895', 'title': 'NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation', 'authors': 'Zheng Huang, Mingyu Liu, Xiaoyi Lin, Muzhi Zhu, Canyu Zhao, Zongze Du, Xiaoman Li, Yiduo Jia, Hao Zhong, Hao Chen, Chunhua Shen', 'link': 'https://arxiv.org/abs/2510.03895', 'abstract': "Vision-Language-Action (VLA) models represent a pivotal advance in embodied intelligence, yet they confront critical barriers to real-world deployment, most notably catastrophic forgetting. This issue stems from their overreliance on continuous action sequences or action chunks, which inadvertently create isolated data silos that disrupt knowledge retention across tasks. To tackle these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA) framework: a novel approach that narrows its focus to sparse trajectories, thereby avoiding the catastrophic forgetting associated with dense trajectory fine-tuning. A key innovation of NoTVLA lies in its trajectory planning strategy: instead of centering on the target object's trajectory, it leverages temporal compression and spatial reasoning pruning specifically for the robot end effector's trajectory. Furthermore, training is conducted using these sparse trajectories rather than dense action trajectories, an optimization that delivers remarkable practical advantages with better performance in zero-shot. In multi-task evaluation scenarios, NoTVLA achieves superior performance and generalization compared to pi0 while operating under two critical constraints: it uses over an order of magnitude less computing power than pi0 and requires no wrist-mounted camera. This design ensures that NoTVLA's operational accuracy closely approximates that of single-task expert models. Crucially, it also preserves the model's inherent language capabilities, enabling zero-shot generalization in specific scenarios, supporting unified model deployment across multiple robot platforms, and fostering a degree of generalization even when perceiving tasks from novel perspectives.", 'abstract_zh': '窄轨迹视觉-语言-行动（NoTVLA）框架：克服关键挑战实现泛化性能提升', 'title_zh': 'NoTVLA: 紧凑密集动作轨迹用于通用机器人 manipulation'}
{'arxiv_id': 'arXiv:2510.03776', 'title': 'Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets', 'authors': 'Tiago Rodrigues de Almeida, Yufei Zhu, Andrey Rudenko, Tomasz P. Kucner, Johannes A. Stork, Martin Magnusson, Achim J. Lilienthal', 'link': 'https://arxiv.org/abs/2510.03776', 'abstract': 'Robots and other intelligent systems navigating in complex dynamic environments should predict future actions and intentions of surrounding agents to reach their goals efficiently and avoid collisions. The dynamics of those agents strongly depends on their tasks, roles, or observable labels. Class-conditioned motion prediction is thus an appealing way to reduce forecast uncertainty and get more accurate predictions for heterogeneous agents. However, this is hardly explored in the prior art, especially for mobile robots and in limited data applications. In this paper, we analyse different class-conditioned trajectory prediction methods on two datasets. We propose a set of conditional pattern-based and efficient deep learning-based baselines, and evaluate their performance on robotics and outdoors datasets (THÖR-MAGNI and Stanford Drone Dataset). Our experiments show that all methods improve accuracy in most of the settings when considering class labels. More importantly, we observe that there are significant differences when learning from imbalanced datasets, or in new environments where sufficient data is not available. In particular, we find that deep learning methods perform better on balanced datasets, but in applications with limited data, e.g., cold start of a robot in a new environment, or imbalanced classes, pattern-based methods may be preferable.', 'abstract_zh': '复杂的动态环境中，机器人及其他智能系统应预测周围代理的未来行动和意图，以高效地达到目标并避免碰撞。这些代理的动力学强烈依赖于其任务、角色或可观察标签。基于类别的运动预测是一种减少预测不确定性并为异质代理获得更准确预测的诱人方式。然而，这在先前的研究中很少被探索，特别是在移动机器人及其数据有限的应用中。本文在两个数据集上分析了不同的类别条件轨迹预测方法。我们提出了一组基于条件模式和高效的基于深度学习的基本方法，并在机器人和户外数据集（THÖR-MAGNI和斯坦福无人机数据集）上评估其性能。我们的实验表明，在考虑类别标签的情况下，所有方法在大多数情况下都能提高准确性。更重要的是，我们观察到在不平衡数据集学习或新环境中缺乏足够数据时，学习效果存在显著差异。特别是，我们发现，在平衡数据集上，深度学习方法表现更好，但在数据有限的应用中，例如机器人在新环境中冷启动或类别不平衡时，基于模式的方法可能更为优选。', 'title_zh': '异质性代理的轨迹预测：小规模与不平衡数据集上的性能分析'}
{'arxiv_id': 'arXiv:2510.03706', 'title': 'EmbodiSwap for Zero-Shot Robot Imitation Learning', 'authors': 'Eadom Dessalene, Pavan Mantripragada, Michael Maynord, Yiannis Aloimonos', 'link': 'https://arxiv.org/abs/2510.03706', 'abstract': 'We introduce EmbodiSwap - a method for producing photorealistic synthetic robot overlays over human video. We employ EmbodiSwap for zero-shot imitation learning, bridging the embodiment gap between in-the-wild ego-centric human video and a target robot embodiment. We train a closed-loop robot manipulation policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a visual backbone, repurposing V-JEPA from the domain of video understanding to imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms alternative vision backbones more conventionally used within robotics. In real-world tests, our zero-shot trained V-JEPA model achieves an $82\\%$ success rate, outperforming a few-shot trained $\\pi_0$ network as well as $\\pi_0$ trained over data produced by EmbodiSwap. We release (i) code for generating the synthetic robot overlays which takes as input human videos and an arbitrary robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference code, to facilitate reproducible research and broader adoption.', 'abstract_zh': '我们介绍了EmbodiSwap——一种在人类视频上生成逼真合成机器人叠加的方法。我们使用EmbodiSwap进行零样本模仿学习，填补了野生主观人类视频与目标机器人身体之间的差距。我们通过EmbodiSwap生成的数据对闭环机器人操作策略进行训练。我们创新地将V-JEPA用作视觉骨干，并将其从视频理解领域重新利用到合成机器人视频的模仿学习中。采用V-JEPA优于机器人领域中更常用的传统视觉骨干。在实际测试中，我们的零样本训练的V-JEPA模型实现了82%的成功率，超过了少量样本训练的$\\pi_0$网络以及基于EmbodiSwap生成数据训练的$\\pi_0$网络。我们发布了(i)用于生成合成机器人叠加的代码，该代码以人类视频和任意机器人URDF作为输入生成机器人数据集，(ii)我们在EPIC-Kitchens、HOI4D和Ego4D上合成的机器人数据集，以及(iii)模型检查点和推理代码，以促进可重复研究和更广泛的采用。', 'title_zh': 'EmbodiSwap 用于零样本机器人模仿学习'}
{'arxiv_id': 'arXiv:2510.03677', 'title': 'Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments', 'authors': 'Salim Rezvani, Ammar Jaleel Mahmood, Robin Chhabra', 'link': 'https://arxiv.org/abs/2510.03677', 'abstract': 'Robots with internal visual self-models promise unprecedented adaptability, yet existing autonomous modeling pipelines remain fragile under realistic sensing conditions such as noisy imagery and cluttered backgrounds. This paper presents the first systematic study quantifying how visual degradations--including blur, salt-and-pepper noise, and Gaussian noise--affect robotic self-modeling. Through both simulation and physical experiments, we demonstrate their impact on morphology prediction, trajectory planning, and damage recovery in state-of-the-art pipelines. To overcome these challenges, we introduce a task-aware denoising framework that couples classical restoration with morphology-preserving constraints, ensuring retention of structural cues critical for self-modeling. In addition, we integrate semantic segmentation to robustly isolate robots from cluttered and colorful scenes. Extensive experiments show that our approach restores near-baseline performance across simulated and physical platforms, while existing pipelines degrade significantly. These contributions advance the robustness of visual self-modeling and establish practical foundations for deploying self-aware robots in unpredictable real-world environments.', 'abstract_zh': '具有内部视觉自模型的机器人在现实传感条件下的视觉退化影响及其鲁棒性研究', 'title_zh': '稳健的视觉封装：机器人如何在真实环境中发现自己的身体'}
{'arxiv_id': 'arXiv:2510.03599', 'title': 'Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning', 'authors': 'Shafeef Omar, Majid Khadiv', 'link': 'https://arxiv.org/abs/2510.03599', 'abstract': 'We present a unified framework for multi-task locomotion and manipulation policy learning grounded in a contact-explicit representation. Instead of designing different policies for different tasks, our approach unifies the definition of a task through a sequence of contact goals-desired contact positions, timings, and active end-effectors. This enables leveraging the shared structure across diverse contact-rich tasks, leading to a single policy that can perform a wide range of tasks. In particular, we train a goal-conditioned reinforcement learning (RL) policy to realise given contact plans. We validate our framework on multiple robotic embodiments and tasks: a quadruped performing multiple gaits, a humanoid performing multiple biped and quadrupedal gaits, and a humanoid executing different bimanual object manipulation tasks. Each of these scenarios is controlled by a single policy trained to execute different tasks grounded in contacts, demonstrating versatile and robust behaviours across morphologically distinct systems. Our results show that explicit contact reasoning significantly improves generalisation to unseen scenarios, positioning contact-explicit policy learning as a promising foundation for scalable loco-manipulation.', 'abstract_zh': '我们提出了一种基于接触显式表示的多任务运动与 manipulation 策略学习统一框架。我们通过接触目标（包括期望的接触位置、时间以及主动执行器）的序列来统一任务定义，而不是为不同的任务设计不同的策略。这种方法使得能够利用不同丰富接触任务之间的共享结构，从而产生一个能够执行广泛任务的单一策略。特别是，我们训练了一个基于目标的强化学习（RL）策略以实现给定的接触计划。我们在多种机器人载体和任务上验证了我们的框架：四足机器人执行多种步态，类人机器人执行多种两足和四足步态，以及类人机器人执行不同双臂对象 manipulation 任务。这些情景均由一个单一策略控制，该策略被训练执行基于接触的不同任务，展示了不同形态系统中的通用且稳健的行为。我们的结果表明，明确的接触推理显著提高了对未见情景的泛化能力，将基于接触的策略学习定位为可扩展的运动与 manipulation 的有前途的基础。', 'title_zh': '通过接触学习行动：多任务机器人学习的统一视角'}
{'arxiv_id': 'arXiv:2510.03529', 'title': 'LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy', 'authors': 'Zekai Liang, Xiao Liang, Soofiyan Atar, Sreyan Das, Zoe Chiu, Peihan Zhang, Florian Richter, Shanglei Liu, Michael C. Yip', 'link': 'https://arxiv.org/abs/2510.03529', 'abstract': 'Robotic laparoscopic surgery has gained increasing attention in recent years for its potential to deliver more efficient and precise minimally invasive procedures. However, adoption of surgical robotic platforms remains largely confined to high-resource medical centers, exacerbating healthcare disparities in rural and low-resource regions. To close this gap, a range of solutions has been explored, from remote mentorship to fully remote telesurgery. Yet, the practical deployment of surgical robotic systems to underserved communities remains an unsolved challenge. Humanoid systems offer a promising path toward deployability, as they can directly operate in environments designed for humans without extensive infrastructure modifications -- including operating rooms. In this work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic teleoperation framework. The system leverages an inverse-mapping strategy for manual-wristed laparoscopic instruments that abides to remote center-of-motion constraints, enabling precise hand-to-tool control of off-the-shelf surgical laparoscopic tools without additional setup requirements. A control console equipped with a stereo vision system provides real-time visual feedback. Finally, a comprehensive user study across platforms demonstrates the effectiveness of the proposed framework and provides initial evidence for the feasibility of deploying humanoid robots in laparoscopic procedures.', 'abstract_zh': '基于类人机器人的人体腹腔镜远程操作框架：LapSurgie', 'title_zh': 'lapSurgie: 通过遥操手持腹腔镜进行手术的人形机器人'}
{'arxiv_id': 'arXiv:2510.03460', 'title': 'Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching', 'authors': 'Sibo Tian, Minghui Zheng, Xiao Liang', 'link': 'https://arxiv.org/abs/2510.03460', 'abstract': 'Rapid robot motion generation is critical in Human-Robot Collaboration (HRC) systems, as robots need to respond to dynamic environments in real time by continuously observing their surroundings and replanning their motions to ensure both safe interactions and efficient task execution. Current sampling-based motion planners face challenges in scaling to high-dimensional configuration spaces and often require post-processing to interpolate and smooth the generated paths, resulting in time inefficiency in complex environments. Optimization-based planners, on the other hand, can incorporate multiple constraints and generate smooth trajectories directly, making them potentially more time-efficient. However, optimization-based planners are sensitive to initialization and may get stuck in local minima. In this work, we present a novel learning-based method that utilizes a Flow Matching model conditioned on a single-view point cloud to learn near-optimal solutions for optimization initialization. Our method does not require prior knowledge of the environment, such as obstacle locations and geometries, and can generate feasible trajectories directly from single-view depth camera input. Simulation studies on a UR5e robotic manipulator in cluttered workspaces demonstrate that the proposed generative initializer achieves a high success rate on its own, significantly improves the success rate of trajectory optimization compared with traditional and learning-based benchmark initializers, requires fewer optimization iterations, and exhibits strong generalization to unseen environments.', 'abstract_zh': '基于流匹配的单视角点云引导快速机器人运动生成在人机协作系统中的应用', 'title_zh': '基于点云条件流匹配的暖启动优化驱动运动规划方法在机器人 manipulator 中的应用'}
{'arxiv_id': 'arXiv:2510.03342', 'title': 'Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer', 'authors': "Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, Konstantinos Bousmalis, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, London Chappellet-Volpini, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, David B. D'Ambrosio, Sudeep Dasari, Todor Davchev, Meet Kirankumar Dave, Coline Devin, Norman Di Palo, Tianli Ding, Carl Doersch, Adil Dostmohamed, Yilun Du, Debidatta Dwibedi, Sathish Thoppay Egambaram, Michael Elabd, Tom Erez, Xiaolin Fang, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Ruiqi Gao, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Oliver Groth, Agrim Gupta, Roland Hafner, Steven Hansen, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Alex Hofer, Jasmine Hsu, Lu Huang, Sandy H. Huang, Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Abhishek Jindal, Ryan Julian, Dmitry Kalashnikov, M. Emre Karagozler, Stefani Karp, Matija Kecman, J. Chase Kew, Donnie Kim, Frank Kim, Junkyung Kim, Thomas Kipf, Sean Kirmani, Ksenia Konyushkova, Li Yang Ku, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Tuan Anh Le, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Guy Lever, Jacky Liang, Li-Heng Lin, Fangchen Liu, Shangbang Long, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Andrew Marmon, Sergio Martinez, Assaf Hurwitz Michaely, Niko Milonopoulos, Joss Moore", 'link': 'https://arxiv.org/abs/2510.03342', 'abstract': 'General-purpose robots need a deep understanding of the physical world, advanced reasoning, and general and dexterous control. This report introduces the latest generation of the Gemini Robotics model family: Gemini Robotics 1.5, a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER 1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together three major innovations. First, Gemini Robotics 1.5 features a novel architecture and a Motion Transfer (MT) mechanism, which enables it to learn from heterogeneous, multi-embodiment robot data and makes the VLA more general. Second, Gemini Robotics 1.5 interleaves actions with a multi-level internal reasoning process in natural language. This enables the robot to "think before acting" and notably improves its ability to decompose and execute complex, multi-step tasks, and also makes the robot\'s behavior more interpretable to the user. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for embodied reasoning, i.e., for reasoning capabilities that are critical for robots, such as visual and spatial understanding, task planning, and progress estimation. Together, this family of models takes us a step towards an era of physical agents-enabling robots to perceive, think and then act so they can solve complex multi-step tasks.', 'abstract_zh': '通用机器人需要对物理世界有深入的理解、高级推理能力和通用的灵巧控制。本报告介绍了Gemini Robotics模型家族的最新一代：Gemini Robotics 1.5，这是一种多体态视觉-语言-行动（VLA）模型，以及Gemini Robotics-ER 1.5，这是一种最先进的体态推理（ER）模型。我们结合了三项重大创新。首先，Gemini Robotics 1.5具有新颖的架构和动作转移（MT）机制，使其能够从异构的多体态机器人数据中学习，并使VLA更具通用性。其次，Gemini Robotics 1.5在自然语言中交替进行动作和多层次的内部推理过程。这使机器人能够在行动前进行“思考”，显著提高了其分解和执行复杂多步骤任务的能力，也使机器人的行为更具用户可解释性。第三，Gemini Robotics-ER 1.5在体态推理领域建立了新的最先进的标准，例如对于机器人至关重要的视觉和空间理解、任务规划和进度估计能力。这些模型共同将我们带向一个物理代理的时代，使机器人能够感知、思考然后行动，以解决复杂的多步骤任务。', 'title_zh': 'Gemini Robotics 1.5: 推动通用机器人前沿的高级主体推理、思考与运动转移'}
{'arxiv_id': 'arXiv:2510.04666', 'title': 'Learning a Shape-adaptive Assist-as-needed Rehabilitation Policy from Therapist-informed Input', 'authors': 'Zhimin Hou, Jiacheng Hou, Xiao Chen, Hamid Sadeghian, Tianyu Ren, Sami Haddadin', 'link': 'https://arxiv.org/abs/2510.04666', 'abstract': 'Therapist-in-the-loop robotic rehabilitation has shown great promise in enhancing rehabilitation outcomes by integrating the strengths of therapists and robotic systems. However, its broader adoption remains limited due to insufficient safe interaction and limited adaptation capability. This article proposes a novel telerobotics-mediated framework that enables therapists to intuitively and safely deliver assist-as-needed~(AAN) therapy based on two primary contributions. First, our framework encodes the therapist-informed corrective force into via-points in a latent space, allowing the therapist to provide only minimal assistance while encouraging patient maintaining own motion preferences. Second, a shape-adaptive ANN rehabilitation policy is learned to partially and progressively deform the reference trajectory for movement therapy based on encoded patient motion preferences and therapist-informed via-points. The effectiveness of the proposed shape-adaptive AAN strategy was validated on a telerobotic rehabilitation system using two representative tasks. The results demonstrate its practicality for remote AAN therapy and its superiority over two state-of-the-art methods in reducing corrective force and improving movement smoothness.', 'abstract_zh': '基于治疗师主导的机器人康复循环框架在增强康复效果方面的潜力及其挑战与解决方案：一种新颖的远程康复策略', 'title_zh': '基于治疗师信息的适应形体辅助性康复政策学习'}
{'arxiv_id': 'arXiv:2510.03915', 'title': 'OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications', 'authors': 'Sagar Bharadwaj, Harrison Williams, Luke Wang, Michael Liang, Tao Jin, Srinivasan Seshan, Anthony Rowe', 'link': 'https://arxiv.org/abs/2510.03915', 'abstract': 'World-scale augmented reality (AR) applications need a ubiquitous 6DoF localization backend to anchor content to the real world consistently across devices. Large organizations such as Google and Niantic are 3D scanning outdoor public spaces in order to build their own Visual Positioning Systems (VPS). These centralized VPS solutions fail to meet the needs of many future AR applications -- they do not cover private indoor spaces because of privacy concerns, regulations, and the labor bottleneck of updating and maintaining 3D scans. In this paper, we present OpenFLAME, a federated VPS backend that allows independent organizations to 3D scan and maintain a separate VPS service for their own spaces. This enables access control of indoor 3D scans, distributed maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS services introduces several unique challenges -- coherency of localization results across spaces, quality control of VPS services, selection of the right VPS service for a location, and many others. We introduce the concept of federated image-based localization and provide reference solutions for managing and merging data across maps without sharing private data.', 'abstract_zh': '面向全球的扩展现实（AR）应用需要一个通用的6DoF定位后端，以在不同设备上一致地将内容锚定到现实世界。谷歌和尼安蒂克等大型组织正在扫描户外公共空间以构建自己的视觉定位系统（VPS）。这些集中式的VPS解决方案无法满足未来AR应用的需求——由于隐私担忧、法规以及更新和维护3D扫描的劳动瓶颈，它们无法覆盖私人室内空间。本文介绍了OpenFLAME，一种允许独立组织单独扫描并维护其自身空间VPS服务的联邦VPS后端，这使得室内3D扫描的访问控制、VPS后端的分布式维护以及增加覆盖范围成为可能。VPS服务的分片带来了许多独特的挑战——跨空间的一致性定位结果、VPS服务的质量控制、选择合适的位置VPS服务等。我们引入了联邦图像ベース定位的概念，并提供了管理和合并地图数据而不共享私人数据的参考解决方案。', 'title_zh': '开放FLAME：联邦视觉定位系统，以实现大规模增强现实应用'}
{'arxiv_id': 'arXiv:2510.03896', 'title': 'Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert', 'authors': 'Mingyu Liu, Zheng Huang, Xiaoyi Lin, Muzhi Zhu, Canyu Zhao, Zongze Du, Yating Wang, Haoyi Zhu, Hao Chen, Chunhua Shen', 'link': 'https://arxiv.org/abs/2510.03896', 'abstract': 'Although Vision-Language Models (VLM) have demonstrated impressive planning and reasoning capabilities, translating these abilities into the physical world introduces significant challenges. Conventional Vision-Language-Action (VLA) models, which integrate reasoning and action into a monolithic architecture, generalize poorly because they are constrained by scarce, narrow-domain data. While recent dual-system approaches attempt to decouple "thinking" from "acting", they are often constrained by semantic ambiguities within the action module. This ambiguity makes large-scale, cross-task training infeasible. Consequently, these systems typically necessitate fine-tuning on newly collected data when deployed to novel environments, and the cooperation mechanism between the two systems remains ill-defined. To address these limitations, we introduce, for the first time, a framework centered around a generalizable action expert. Our approach utilizes sparse 3D trajectories as an intermediate representation, effectively bridging the high-level planning capabilities of the VLM with the low-level physical action module. During the planning phase, the VLM is only required to generate coarse 3D waypoints. These waypoints are then processed by our generalizable action expert, which refines them into dense, executable action sequences by sampling real-time point cloud observations of the environment. To promote training efficiency and robust generalization, we introduce a novel "Action Pre-training, Pointcloud Fine-tuning" paradigm. Our method combines the broad generalization capabilities of VLMs in visual understanding and planning with the fine-grained, action-level generalization of action expert.', 'abstract_zh': '虽然视觉语言模型（VLM）展现了令人印象深刻的规划和推理能力，但将其能力转化为物理世界引入了重大挑战。传统的视觉语言动作（VLA）模型由于受限于少量和领域狭窄的数据，在泛化能力上表现不佳。尽管近期的双系统方法试图将“思考”与“动作”解耦，但它们往往受到动作模块内语义歧义的限制，这使得大规模跨任务训练变得不可行。因此，当这些系统部署到新环境中时，通常需要对新收集的数据进行微调，且两个系统的合作机制仍不明确。为解决这些局限性，我们首次提出了一种以可泛化动作专家为中心的框架。我们的方法利用稀疏的3D轨迹作为中间表示，有效连接了VLM的高层规划能力和低层物理动作模块。在规划阶段，VLM仅需生成粗略的3D航点。这些航点随后由我们的可泛化动作专家处理，通过实时环境点云观察来细化生成密集的可执行动作序列。为了提高训练效率和增强泛化能力，我们引入了一种新颖的“动作预训练、点云微调”范式。该方法结合了VLM在视觉理解与规划上的广泛泛化能力，以及动作专家在动作级别上的细粒度泛化能力。', 'title_zh': '桥接思考与行动：释放VLM物理潜力的通用行动专家'}
{'arxiv_id': 'arXiv:2510.03827', 'title': 'LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization', 'authors': 'Xueyang Zhou, Yangming Xu, Guiyao Tie, Yongchao Chen, Guowen Zhang, Duanfeng Chu, Pan Zhou, Lichao Sun', 'link': 'https://arxiv.org/abs/2510.03827', 'abstract': "LIBERO has emerged as a widely adopted benchmark for evaluating Vision-Language-Action (VLA) models; however, its current training and evaluation settings are problematic, often leading to inflated performance estimates and preventing fair model comparison. To address these issues, we introduce LIBERO-PRO, an extended LIBERO benchmark that systematically evaluates model performance under reasonable perturbations across four dimensions: manipulated objects, initial states, task instructions, and environments. Experimental results reveal that, although existing models achieve over 90% accuracy under the standard LIBERO evaluation, their performance collapses to 0.0% under our generalized setting. Crucially, this discrepancy exposes the models' reliance on rote memorization of action sequences and environment layouts from the training set, rather than genuine task understanding or environmental perception. For instance, models persist in executing grasping actions when the target object is replaced with irrelevant items, and their outputs remain unchanged even when given corrupted instructions or even messy tokens. These findings expose the severe flaws in current evaluation practices, and we call on the community to abandon misleading methodologies in favor of robust assessments of model generalization and comprehension. Our code is available at: this https URL.", 'abstract_zh': 'LIBERO-PRO：一种系统评估模型在合理扰动下性能的扩展基准', 'title_zh': 'LIBERO-PRO：超越记忆化 toward 视听行模型的稳健与公平评估'}
{'arxiv_id': 'arXiv:2510.03823', 'title': 'Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning', 'authors': 'Adam Haroon, Tristan Schuler', 'link': 'https://arxiv.org/abs/2510.03823', 'abstract': 'High Altitude Balloons (HABs) can leverage stratospheric wind layers for limited horizontal control, enabling applications in reconnaissance, environmental monitoring, and communications networks. Existing multi-agent HAB coordination approaches use deterministic methods like Voronoi partitioning and extremum seeking control for large global constellations, which perform poorly for smaller teams and localized missions. While single-agent HAB control using reinforcement learning has been demonstrated on HABs, coordinated multi-agent reinforcement learning (MARL) has not yet been investigated. This work presents the first systematic application of multi-agent reinforcement learning (MARL) to HAB coordination for distributed area coverage. We extend our previously developed reinforcement learning simulation environment (RLHAB) to support cooperative multi-agent learning, enabling multiple agents to operate simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area coverage coordination, leveraging Centralized Training with Decentralized Execution to address atmospheric vehicle coordination challenges. Our approach employs specialized observation spaces providing individual state, environmental context, and teammate data, with hierarchical rewards prioritizing coverage while encouraging spatial distribution. We demonstrate that QMIX achieves similar performance to the theoretically optimal geometric deterministic method for distributed area coverage, validating the MARL approach and providing a foundation for more complex autonomous multi-HAB missions where deterministic methods become intractable.', 'abstract_zh': '高空 balloon (HABs) 可利用平流层风层进行有限的水平控制，从而在侦察、环境监测和通信网络等方面应用。现有基于多代理的 HAB 协调方法使用如 Voronoi 分区和极值搜索控制等确定性方法，适用于大规模全球星座，但对较小团队和局部任务表现不佳。虽然已经证明使用强化学习（RL）可以控制单个高空气球，但多代理强化学习（MARL）尚未进行研究。本文首次系统地将多代理强化学习（MARL）应用于高空气球协调，以实现分布式区域覆盖。我们扩展了我们之前开发的强化学习仿真环境（RLHAB），以支持协同多代理学习，使多个代理能够在现实的气象条件下同时运行。我们改编 QMIX 以实现高空气球区域覆盖协调，利用集中训练与分散执行相结合的方法解决大气飞行器协调问题。我们的方法采用了专门的观测空间，提供个体状态、环境背景和队友数据，并通过分层奖励优先考虑覆盖同时鼓励空间分布。我们证明QMIX在分布式区域覆盖方面达到了理论最优的几何确定性方法的相似性能，验证了MARL方法，并为当确定性方法变得无法实现时的更复杂自主多高空气球任务奠定了基础。', 'title_zh': '使用多智能体强化学习的高-altitude气球分布式区域覆盖'}
{'arxiv_id': 'arXiv:2510.03592', 'title': 'Deep Reinforcement Learning for Multi-Agent Coordination', 'authors': 'Kehinde O. Aina, Sehoon Ha', 'link': 'https://arxiv.org/abs/2510.03592', 'abstract': 'We address the challenge of coordinating multiple robots in narrow and confined environments, where congestion and interference often hinder collective task performance. Drawing inspiration from insect colonies, which achieve robust coordination through stigmergy -- modifying and interpreting environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement Learning (S-MADRL) framework that leverages virtual pheromones to model local and social interactions, enabling decentralized emergent coordination without explicit communication. To overcome the convergence and scalability limitations of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum learning, which decomposes complex tasks into progressively harder sub-problems. Simulation results show that our framework achieves the most effective coordination of up to eight agents, where robots self-organize into asymmetric workload distributions that reduce congestion and modulate group performance. This emergent behavior, analogous to strategies observed in nature, demonstrates a scalable solution for decentralized multi-agent coordination in crowded environments with communication constraints.', 'abstract_zh': '基于蚁痕机制的多智能体深度强化学习框架：应对狭窄受限环境中多机器人协调挑战', 'title_zh': '多智能体协调的深度强化学习'}
{'arxiv_id': 'arXiv:2510.04978', 'title': 'Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI', 'authors': 'Kun Xiang, Terry Jingchen Zhang, Yinya Huang, Jixi He, Zirong Liu, Yueling Tang, Ruizhe Zhou, Lijing Luo, Youpeng Wen, Xiuwei Chen, Bingqian Lin, Jianhua Han, Hang Xu, Hanhui Li, Bin Dong, Xiaodan Liang', 'link': 'https://arxiv.org/abs/2510.04978', 'abstract': "The rapid advancement of embodied intelligence and world models has intensified efforts to integrate physical laws into AI systems, yet physical perception and symbolic physics reasoning have developed along separate trajectories without a unified bridging framework. This work provides a comprehensive overview of physical AI, establishing clear distinctions between theoretical physics reasoning and applied physical understanding while systematically examining how physics-grounded methods enhance AI's real-world comprehension across structured symbolic reasoning, embodied systems, and generative models. Through rigorous analysis of recent advances, we advocate for intelligent systems that ground learning in both physical principles and embodied reasoning processes, transcending pattern recognition toward genuine understanding of physical laws. Our synthesis envisions next-generation world models capable of explaining physical phenomena and predicting future states, advancing safe, generalizable, and interpretable AI systems. We maintain a continuously updated resource at this https URL.", 'abstract_zh': '快速发展的具身智能和世界模型推动了将物理法则整合到AI系统中的努力，然而物理感知和符号物理学推理分别独立发展，缺乏一个统一的桥梁框架。本文提供了物理AI的全面概述，明确区分了理论物理推理与应用物理理解，并系统考察了基于物理的方法如何增强AI对结构化符号推理、具身系统和生成模型的现实世界理解。通过严谨分析最近的进展，我们提倡将在物理原则和具身推理过程中扎根的学习，超越模式识别，向着对物理法则真正理解的转变。我们的综合展望了新的世界模型，能够解释物理现象并预测未来状态，推动安全、通用和可解释的AI系统的发展。我们维护了一个持续更新的资源，网址为这个 https URL。', 'title_zh': '感知、推理、建模与交互一致性的研究：物理AI综述'}
{'arxiv_id': 'arXiv:2510.04643', 'title': 'QuantAgents: Towards Multi-agent Financial System via Simulated Trading', 'authors': 'Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, Xiangmin Xu', 'link': 'https://arxiv.org/abs/2510.04643', 'abstract': "In this paper, our objective is to develop a multi-agent financial system that incorporates simulated trading, a technique extensively utilized by financial professionals. While current LLM-based agent models demonstrate competitive performance, they still exhibit significant deviations from real-world fund companies. A critical distinction lies in the agents' reliance on ``post-reflection'', particularly in response to adverse outcomes, but lack a distinctly human capability: long-term prediction of future trends. Therefore, we introduce QuantAgents, a multi-agent system integrating simulated trading, to comprehensively evaluate various investment strategies and market scenarios without assuming actual risks. Specifically, QuantAgents comprises four agents: a simulated trading analyst, a risk control analyst, a market news analyst, and a manager, who collaborate through several meetings. Moreover, our system incentivizes agents to receive feedback on two fronts: performance in real-world markets and predictive accuracy in simulated trading. Extensive experiments demonstrate that our framework excels across all metrics, yielding an overall return of nearly 300% over the three years (this https URL).", 'abstract_zh': '在本文中，我们的目标是开发一个包含模拟交易的多智能体金融系统，这是一种广泛应用于金融专业人士的技术。虽然当前基于大语言模型的智能体模型表现出色，但在某些方面仍与真实世界的投资管理公司存在显著差异。一个关键区别在于智能体依赖于“反省后”的决策，尤其是在面对不利结果时，但缺乏一种独特的人类能力：对未来趋势的长期预测。因此，我们引入了QuantAgents，这是一种结合模拟交易的多智能体系统，用于全面评估各种投资策略和市场情景，而不假设实际风险。具体而言，QuantAgents 包含四个智能体：模拟交易分析师、风险控制分析师、市场新闻分析师和经理，他们通过多次会议协作。此外，我们的系统激励智能体在两个方面接收反馈：在真实市场中的表现和模拟交易中的预测准确性。广泛的实验表明，我们的框架在所有指标上均表现出色，在三年时间里实现了近300%的整体回报率（详见此链接：https://www.crisil.com/research/quantagents-achieves-nearly-300-retur）。', 'title_zh': 'QuantAgents: 向往通过模拟交易构建多agent金融系统'}
{'arxiv_id': 'arXiv:2510.04542', 'title': 'Code World Models for General Game Playing', 'authors': 'Wolfgang Lehrach, Daniel Hennes, Miguel Lazaro-Gredilla, Xinghua Lou, Carter Wendelken, Zun Li, Antoine Dedieu, Jordi Grau-Moya, Marc Lanctot, Atil Iscen, John Schultz, Marcus Chiam, Ian Gemp, Piotr Zielinski, Satinder Singh, Kevin P. Murphy', 'link': 'https://arxiv.org/abs/2510.04542', 'abstract': "Large Language Models (LLMs) reasoning abilities are increasingly being applied to classical board and card games, but the dominant approach -- involving prompting for direct move generation -- has significant drawbacks. It relies on the model's implicit fragile pattern-matching capabilities, leading to frequent illegal moves and strategically shallow play. Here we introduce an alternative approach: We use the LLM to translate natural language rules and game trajectories into a formal, executable world model represented as Python code. This generated model -- comprising functions for state transition, legal move enumeration, and termination checks -- serves as a verifiable simulation engine for high-performance planning algorithms like Monte Carlo tree search (MCTS). In addition, we prompt the LLM to generate heuristic value functions (to make MCTS more efficient), and inference functions (to estimate hidden states in imperfect information games). Our method offers three distinct advantages compared to directly using the LLM as a policy: (1) Verifiability: The generated CWM serves as a formal specification of the game's rules, allowing planners to algorithmically enumerate valid actions and avoid illegal moves, contingent on the correctness of the synthesized model; (2) Strategic Depth: We combine LLM semantic understanding with the deep search power of classical planners; and (3) Generalization: We direct the LLM to focus on the meta-task of data-to-code translation, enabling it to adapt to new games more easily. We evaluate our agent on 10 different games, of which 4 are novel and created for this paper. 5 of the games are fully observed (perfect information), and 5 are partially observed (imperfect information). We find that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10 considered games.", 'abstract_zh': '大型语言模型在经典棋盘和纸牌游戏中的推理能力正在被越来越多地应用，但主导的方法——涉及直接提示生成移动——存在显著缺点。这种方法依赖于模型隐含的脆弱的模式匹配能力，导致频繁出现非法移动和战略性浅薄的玩法。在这里我们介绍了一种替代方法：我们使用大型语言模型将自然语言规则和游戏轨迹翻译成形式化的可执行世界模型，表示为Python代码。生成的模型包括状态转换函数、合法移动枚举和终止检查函数，作为高性能计划算法（如蒙特卡洛树搜索MCTS）的验证性模拟引擎。此外，我们提示大型语言模型生成启发式价值函数（使MCTS更高效）和推理函数（估计不完美信息游戏中的隐藏状态）。与直接将大型语言模型用作策略相比，我们的方法具有三种显著优势：（1）可验证性：生成的形式化世界模型作为游戏规则的形式化规范，让规划者能够算法化地枚举有效操作，前提是生成的模型是正确的；（2）战略深度：结合大型语言模型的语义理解能力和经典规划者的深层搜索能力；（3）通用性：我们将大型语言模型引导关注数据到代码的转换这一元任务，使其更容易适应新游戏。我们在10种不同的游戏中评估了我们的代理，其中4种是新型游戏，专门为本文创建。5种游戏为完全观察游戏（完美信息），5种为部分观察游戏（不完美信息）。我们发现，我们的方法在考虑的10种游戏中有9种超过了或匹配了Gemini 2.5 Pro。', 'title_zh': '代码世界模型在通用游戏-playing中的应用'}
{'arxiv_id': 'arXiv:2510.04532', 'title': 'More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models', 'authors': 'Xurui Song, Shuo Huai, JingJing Jiang, Jiayi Kong, Jun Luo', 'link': 'https://arxiv.org/abs/2510.04532', 'abstract': "Vision-Language Model (VLM) driving agents promise explainable end-to-end autonomy by first producing natural-language reasoning and then predicting trajectory planning. However, whether planning is causally driven by this reasoning remains a critical but unverified assumption. To investigate this, we build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan. Our data generation process converts sensors and annotations into structured inputs and, crucially, separates priors from to-be-reasoned signals, enabling clean information ablations. Using DriveMind, we train representative VLM agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately, indicate a consistent causal disconnect in reasoning-planning: removing ego/navigation priors causes large drops in planning scores, whereas removing CoT produces only minor changes. Attention analysis further shows that planning primarily focuses on priors rather than the CoT. Based on this evidence, we propose the Reasoning-Planning Decoupling Hypothesis, positing that the training-yielded reasoning is an ancillary byproduct rather than a causal mediator. To enable efficient diagnosis, we also introduce a novel, training-free probe that measures an agent's reliance on priors by evaluating its planning robustness against minor input perturbations. In summary, we provide the community with a new dataset and a diagnostic tool to evaluate the causal fidelity of future models.", 'abstract_zh': 'Vision-Language模型驱动代理的推理与规划解耦假设：一个新的大规模驾驶视觉问答数据集及诊断工具探究', 'title_zh': 'beyond Surface-Level Understanding？揭示视觉-语言驾驶模型中的推理-规划差距'}
{'arxiv_id': 'arXiv:2510.04391', 'title': 'Internal World Models as Imagination Networks in Cognitive Agents', 'authors': 'Saurabh Ranjan, Brian Odegaard', 'link': 'https://arxiv.org/abs/2510.04391', 'abstract': 'What is the computational objective of imagination? While classical interpretations suggest imagination is useful for maximizing rewards, recent findings challenge this view. In this study, we propose that imagination serves to access an internal world model (IWM) and use psychological network analysis to explore IWMs in humans and large language models (LLMs). Specifically, we assessed imagination vividness ratings using two questionnaires and constructed imagination networks from these reports. Imagination networks from human groups showed correlations between different centrality measures, including expected influence, strength, and closeness. However, imagination networks from LLMs showed a lack of clustering and lower correlations between centrality measures under different prompts and conversational memory conditions. Together, these results indicate a lack of similarity between IWMs in human and LLM agents. Overall, our study offers a novel method for comparing internally-generated representations in humans and AI, providing insights for developing human-like imagination in artificial intelligence.', 'abstract_zh': '想象的计算目标是什么？经典的解释认为想象有助于最大化奖励，但最近的研究挑战了这一观点。本研究提出，想象服务于访问内部世界模型（IWM）的功能，并利用心理网络分析探索人类和大型语言模型（LLM）的IWM。具体而言，我们使用两个问卷评估想象的生动性，并从这些报告中构建想象网络。人类群体的想象网络在不同的中心性指标之间显示出了相关性，包括预期影响、强度和接近度。然而，在不同的提示和对话记忆条件下，LLM的想象网络缺乏聚类，并且中心性指标之间的相关性较低。这些结果表明，人类和LLM代理的IWM之间缺乏相似性。总的来说，本研究提供了一种新的方法来比较人类和AI内部生成的表征，为发展类似人类的想象提供见解。', 'title_zh': '内部世界模型作为认知代理的想象网络'}
{'arxiv_id': 'arXiv:2510.04371', 'title': 'Speculative Actions: A Lossless Framework for Faster Agentic Systems', 'authors': 'Naimeng Ye, Arnav Ahuja, Georgios Liargkovas, Yunan Lu, Kostis Kaffes, Tianyi Peng', 'link': 'https://arxiv.org/abs/2510.04371', 'abstract': 'Despite growing interest in AI agents across industry and academia, their execution in an environment is often slow, hampering training, evaluation, and deployment. For example, a game of chess between two state-of-the-art agents may take hours. A critical bottleneck is that agent behavior unfolds sequentially: each action requires an API call, and these calls can be time-consuming. Inspired by speculative execution in microprocessors and speculative decoding in LLM inference, we propose speculative actions, a lossless framework for general agentic systems that predicts likely actions using faster models, enabling multiple steps to be executed in parallel. We evaluate this framework across three agentic environments: gaming, e-commerce, web search, and a "lossy" extension for an operating systems environment. In all cases, speculative actions achieve substantial accuracy in next-action prediction (up to 55%), translating into significant reductions in end-to-end latency. Moreover, performance can be further improved through stronger guessing models, top-K action prediction, multi-step speculation, and uncertainty-aware optimization, opening a promising path toward deploying low-latency agentic systems in the real world.', 'abstract_zh': '尽管工业和学术界对AI代理的兴趣日益增长，但它们在环境中的执行往往很慢，阻碍了训练、评估和部署。例如，两台顶尖博弈代理进行一局国际象棋比赛可能需要数小时。一个关键瓶颈在于代理行为是按顺序展开的：每行动都需要一个API调用，这些调用可能耗费大量时间。受微处理器的预测执行和大语言模型推理中的预测解码启发，我们提出了一种无损的预测行为框架，该框架利用更快的模型预测可能出现的动作，使多个步骤能够并行执行。我们在游戏、电子商务、网络搜索以及操作系统环境的“失真”扩展三个代理环境中评估了该框架。在所有情况下，预测行为都能在下一动作预测方面取得显著准确性（最高可达55%），从而显著减少端到端的延迟。此外，通过更强的猜测模型、K项动作预测、多步预测和不确定性感知优化，性能可以进一步提升，为在实际中部署低延迟代理系统开辟了前景。', 'title_zh': '投机行动：一个无损框架以实现更快的自主系统'}
{'arxiv_id': 'arXiv:2510.04284', 'title': 'Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning', 'authors': 'Yunghwei Lai, Kaiming Liu, Ziyue Wang, Weizhi Ma, Yang Liu', 'link': 'https://arxiv.org/abs/2510.04284', 'abstract': "The professionalism of a human doctor in outpatient service depends on two core abilities: the ability to make accurate medical decisions and the medical consultation skill to conduct strategic, empathetic patient inquiry. Existing Large Language Models (LLMs) have achieved remarkable accuracy on medical decision-making benchmarks. However, they often lack the ability to conduct the strategic and empathetic consultation, which is essential for real-world clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor agent trained to master both of the capabilities by ask high-yield questions and conduct strategic multi-turn inquiry to guide decision-making. Our framework introduces three key components: a multi-agent interactive environment, a two-tiered reward architecture that separately optimizes clinical decision-making and communicative inquiry skills, and an experience repository to ground policy learning in high-quality prior trajectories. We evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across multi-facet metrics, such as communication quality, user experience, and task accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source specialized LLMs by a substantial margin with higher parameter efficiency and outperforms powerful proprietary models. Furthermore, the human evaluations show a strong preference for Doctor-R1 to generate human-preferred clinical dialogue, demonstrating the effectiveness of the framework.", 'abstract_zh': '人类门诊医生的专业性取决于两种核心能力：准确的医疗决策能力和战略性、同理心导向的患者咨询技能。现有的大规模语言模型在医疗决策基准测试中已经取得了显著的准确率。然而，它们往往缺乏进行战略性、同理心导向的咨询的能力，这对于实际临床场景是必不可少的。为了解决这一问题，我们提出了一种名为Doctor-R1的AI医生代理，旨在通过提出高收益问题并开展战略性多轮咨询来磨练这两种能力。我们的框架包括三个关键组件：一个多代理交互环境、一个多层奖励架构，分别优化临床决策能力和沟通性咨询技能，并通过高质量的先验轨迹来约束策略学习。我们在OpenAI的HealthBench和MAQuE上评估了Doctor-R1，通过多维度指标评估，如沟通质量、用户体验和任务准确性。令人印象深刻的是，Doctor-R1在参数效率方面超越了现有的开源专业模型，并且在强大的专有模型中表现更佳。此外，人类评估表明，人们更偏好Doctor-R1生成的符合人类偏好的临床对话，这证明了该框架的有效性。', 'title_zh': 'Doctor-R1: 通过经验代理强化学习掌握临床查询能力'}
{'arxiv_id': 'arXiv:2510.04206', 'title': 'AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework', 'authors': 'Hanchen Zhang, Xiao Liu, Bowen Lv, Xueqiao Sun, Bohao Jing, Iat Long Iong, Zhenyu Hou, Zehan Qi, Hanyu Lai, Yifan Xu, Rui Lu, Hongning Wang, Jie Tang, Yuxiao Dong', 'link': 'https://arxiv.org/abs/2510.04206', 'abstract': 'Recent advances in large language models (LLMs) have sparked growing interest in building generalist agents that can learn through online interactions. However, applying reinforcement learning (RL) to train LLM agents in multi-turn, multi-task settings remains challenging due to lack of scalable infrastructure and stable training algorithms. In this work, we present the AgentRL framework for scalable multi-turn, multi-task agentic RL training. On the infrastructure side, AgentRL features a fully-asynchronous generation-training pipeline for efficient multi-turn RL. To support heterogeneous environment development in multi-task RL, we design a unified function-call based API interface, containerized environment development, and a centralized controller. On the algorithm side, we propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training. Experiments show that AgentRL, trained on open LLMs across five agentic tasks, significantly outperforms GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents. Multi-task training with AgentRL matches the best results among all task-specific models. AgentRL is open-sourced at this https URL. The algorithm and framework are adopted in building \\textsc{\\href{this https URL}{AutoGLM}}.', 'abstract_zh': 'Recent advances in大型语言模型(LLMs)近年来在大型语言模型(LLMs)方面的进展激发了构建通过在线交互学习的通用代理的兴趣。然而，在多轮、多任务设置中应用强化学习(RL)训练LLM代理仍然具有挑战性，原因是在可扩展基础设施和稳定训练算法方面存在不足。在本文中，我们提出了AgentRL框架，用于可扩展的多轮、多任务代理型RL训练。在基础设施方面，AgentRL具备完全异步的生成-训练流水线，用于高效多轮RL。为支持多任务RL中的异构环境开发，我们设计了一种统一的功能调用接口API、容器化环境开发和中心控制器。从算法角度来看，我们提出了跨策略采样以鼓励多轮设置中的模型探索，并提出了任务优势归一化以稳定多任务训练。实验显示，使用AgentRL训练的开放LLM代理在五项代理任务上显著优于GPT-5、Clause-Sonnet-4、DeepSeek-R1和其他开源LLM代理。使用AgentRL进行多任务训练达到了所有任务特定模型的最佳结果。AgentRL开源于[this https URL]。该算法和框架被用于构建AutoGLM。', 'title_zh': 'AgentRL: 使用多轮多任务框架扩展代理强化学习'}
{'arxiv_id': 'arXiv:2510.04196', 'title': 'COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability', 'authors': 'Yizhuo Ding, Mingkang Chen, Qiuhua Liu, Fenghua Weng, Wanying Qu, Yue Yang, Yugang Jiang, Zuxuan Wu, Yanwei Fu, Wenqi Shao', 'link': 'https://arxiv.org/abs/2510.04196', 'abstract': 'Large Multimodal Reasoning Models (LMRMs) are moving into real applications, where they must be both useful and safe. Safety is especially challenging in multimodal settings: images and text can be combined to bypass guardrails, and single objective training can cause policy drift that yields over-refusal on benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed reinforcement learning framework that trains reasoning oriented LMRMs under multimodal, multitask, and multiobjective signals, and we release the resulting model, COSMO-R1. Our approach aims to let safety and capability grow together in one stable pipeline rather than competing during alignment. In experiments, COSMO-R1 improves safety while maintaining-and often improving multimodal reasoning and instruction following, shows stronger robustness to multimodal jailbreaks, and reduces unnecessary refusals. The framework also transfers across backbones with consistent gains. Ablations support the design choices, indicating a simple path to advancing safety and general capability together in LMRMs.', 'abstract_zh': '大规模多模态推理模型（LMRMs）在实际应用中既要实用又要安全。特别是在多模态设置中，图像和文本可以结合以绕过安全限制，单一目标训练可能导致政策偏移，从而在良性输入上过度拒绝或在风险输入上不安全地合规。我们提出了COSMO-RL，一种混合强化学习框架，用于在多模态、多任务和多目标信号下训练以推理为导向的LMRMs，并发布了相应的模型COSMO-R1。我们的方法旨在让安全性和能力在单一稳定的流水线中共同成长，而不是在对齐过程中相互竞争。实验结果显示，COSMO-R1在提高安全性的同时，保持并往往增强了多模态推理和指令遵循能力，展示了更强的多模态绕过鲁棒性，并减少了不必要的拒绝。该框架还跨 backbone 获得一致的收益。消融实验支持了设计选择，表明了一条简单路径，可以在LMRMs中同时推进安全性和通用能力。', 'title_zh': 'COSMO-RL：通过联合安全性和稳定性迈向可信赖的LMRMs'}
{'arxiv_id': 'arXiv:2510.03727', 'title': 'Bridging the Gap Between Multimodal Foundation Models and World Models', 'authors': 'Xuehai He', 'link': 'https://arxiv.org/abs/2510.03727', 'abstract': "Humans understand the world through the integration of multiple sensory modalities, enabling them to perceive, reason about, and imagine dynamic physical processes. Inspired by this capability, multimodal foundation models (MFMs) have emerged as powerful tools for multimodal understanding and generation. However, today's MFMs fall short of serving as effective world models. They lack the essential ability such as perform counterfactual reasoning, simulate dynamics, understand the spatiotemporal information, control generated visual outcomes, and perform multifaceted reasoning. We investigates what it takes to bridge the gap between multimodal foundation models and world models. We begin by improving the reasoning capabilities of MFMs through discriminative tasks and equipping MFMs with structured reasoning skills, such as causal inference, counterfactual thinking, and spatiotemporal reasoning, enabling them to go beyond surface correlations and understand deeper relationships within visual and textual data. Next, we explore generative capabilities of multimodal foundation models across both image and video modalities, introducing new frameworks for structured and controllable generation. Our approaches incorporate scene graphs, multimodal conditioning, and multimodal alignment strategies to guide the generation process, ensuring consistency with high-level semantics and fine-grained user intent. We further extend these techniques to controllable 4D generation, enabling interactive, editable, and morphable object synthesis over time and space.", 'abstract_zh': '多模态基础模型向世界模型转变所需的条件：增强推理能力和生成能力以理解深层关系并实现可控的多维生成', 'title_zh': '跨模态基础模型与世界模型的桥梁'}
{'arxiv_id': 'arXiv:2510.04901', 'title': 'Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects', 'authors': 'Jonathan Colaço Carr, Qinyi Sun, Cameron Allen', 'link': 'https://arxiv.org/abs/2510.04901', 'abstract': 'Skills are essential for unlocking higher levels of problem solving. A common approach to discovering these skills is to learn ones that reliably reach different states, thus empowering the agent to control its environment. However, existing skill discovery algorithms often overlook the natural state variables present in many reinforcement learning problems, meaning that the discovered skills lack control of specific state variables. This can significantly hamper exploration efficiency, make skills more challenging to learn with, and lead to negative side effects in downstream tasks when the goal is under-specified. We introduce a general method that enables these skill discovery algorithms to learn focused skills -- skills that target and control specific state variables. Our approach improves state space coverage by a factor of three, unlocks new learning capabilities, and automatically avoids negative side effects in downstream tasks.', 'abstract_zh': '技能对于解锁更高层次的问题解决至关重要。一种常见的技能发现方法是学习那些能可靠地达到不同状态的技能，从而使代理能够控制其环境。然而，现有的技能发现算法往往忽视了许多强化学习问题中自然存在的状态变量，这意味着发现的技能缺乏对特定状态变量的控制能力。这会显著降低探索效率，使技能更难学习，并在目标不明确时导致下游任务中的负面影响。我们提出了一种通用方法，使这些技能发现算法能够学习针对并控制特定状态变量的专注技能。我们的方法通过三倍以上的状态空间覆盖范围提升了学习能力，并且自动避免了下游任务中的负面影响。', 'title_zh': '聚焦技能发现：学习控制特定状态变量并最小化副作用'}
{'arxiv_id': 'arXiv:2510.04868', 'title': 'Model Predictive Control-Guided Reinforcement Learning for Implicit Balancing', 'authors': 'Seyed Soroush Karimi Madahi, Kenneth Bruninx, Bert Claessens, Chris Develder', 'link': 'https://arxiv.org/abs/2510.04868', 'abstract': 'In Europe, profit-seeking balance responsible parties can deviate in real time from their day-ahead nominations to assist transmission system operators in maintaining the supply-demand balance. Model predictive control (MPC) strategies to exploit these implicit balancing strategies capture arbitrage opportunities, but fail to accurately capture the price-formation process in the European imbalance markets and face high computational costs. Model-free reinforcement learning (RL) methods are fast to execute, but require data-intensive training and usually rely on real-time and historical data for decision-making. This paper proposes an MPC-guided RL method that combines the complementary strengths of both MPC and RL. The proposed method can effectively incorporate forecasts into the decision-making process (as in MPC), while maintaining the fast inference capability of RL. The performance of the proposed method is evaluated on the implicit balancing battery control problem using Belgian balancing data from 2023. First, we analyze the performance of the standalone state-of-the-art RL and MPC methods from various angles, to highlight their individual strengths and limitations. Next, we show an arbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and 54.36%, compared to standalone RL and MPC.', 'abstract_zh': '欧洲地区基于模型预测控制的强化学习引导平衡策略研究', 'title_zh': '基于模型预测控制的强化学习隐式平衡方法'}
{'arxiv_id': 'arXiv:2510.04786', 'title': 'Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning', 'authors': 'Jonas Hübotter, Leander Diaz-Bone, Ido Hakimi, Andreas Krause, Moritz Hardt', 'link': 'https://arxiv.org/abs/2510.04786', 'abstract': 'Humans are good at learning on the job: We learn how to solve the tasks we face as we go along. Can a model do the same? We propose an agent that assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and applies reinforcement learning to continue training the model for its target task. The test-time curriculum avoids time-consuming human curation of datasets by automatically selecting the most task-relevant data from a large pool of available training data. Our experiments demonstrate that reinforcement learning on a test-time curriculum consistently improves the model on its target tasks, across a variety of evaluations and models. Notably, on challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that TTC-RL significantly raises the performance ceiling compared to the initial model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to 43%. Our findings show the potential of test-time curricula in extending the test-time scaling paradigm to continual training on thousands of task-relevant experiences during test-time.', 'abstract_zh': '基于测试时动态课程的强化学习：人类可以在工作中学习，那么模型能做到吗？', 'title_zh': '在职学习：针对强化学习的目标测试时序课程'}
{'arxiv_id': 'arXiv:2510.04392', 'title': 'Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards', 'authors': 'Faisal Hamman, Chenyang Zhu, Anoop Kumar, Xujun Peng, Sanghamitra Dutta, Daben Liu, Alfy Samuel', 'link': 'https://arxiv.org/abs/2510.04392', 'abstract': 'RAG systems are increasingly deployed in high-stakes domains where users expect outputs to be consistent across semantically equivalent queries. However, existing systems often exhibit significant inconsistencies due to variability in both the retriever and generator (LLM), undermining trust and reliability. In this work, we focus on information consistency, i.e., the requirement that outputs convey the same core content across semantically equivalent inputs. We introduce a principled evaluation framework that decomposes RAG consistency into retriever-level, generator-level, and end-to-end components, helping identify inconsistency sources. To improve consistency, we propose Paraphrased Set Group Relative Policy Optimization (PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased set to assign group similarity rewards. We leverage PS-GRPO to achieve Information Consistent RAG (Con-RAG), training the generator to produce consistent outputs across paraphrased queries and remain robust to retrieval-induced variability. Because exact reward computation over paraphrase sets is computationally expensive, we also introduce a scalable approximation method that retains effectiveness while enabling efficient, large-scale training. Empirical evaluations across short-form, multi-hop, and long-form QA benchmarks demonstrate that Con-RAG significantly improves both consistency and accuracy over strong baselines, even in the absence of explicit ground-truth supervision. Our work provides practical solutions for evaluating and building reliable RAG systems for safety-critical deployments.', 'abstract_zh': 'RAG系统在高风险领域中的信息一致性评估与提升：Paraphrased Set Group Relative Policy Optimization方法的研究', 'title_zh': '通过组相似性奖励提高检索增强系统的一致性'}
{'arxiv_id': 'arXiv:2510.04390', 'title': 'MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator', 'authors': 'Xuehai He, Shijie Zhou, Thivyanth Venkateswaran, Kaizhi Zheng, Ziyu Wan, Achuta Kadambi, Xin Eric Wang', 'link': 'https://arxiv.org/abs/2510.04390', 'abstract': 'World models that support controllable\nand editable spatiotemporal environments are valuable\nfor robotics, enabling scalable training data, repro ducible evaluation, and flexible task design. While\nrecent text-to-video models generate realistic dynam ics, they are constrained to 2D views and offer limited\ninteraction. We introduce MorphoSim, a language guided framework that generates 4D scenes with\nmulti-view consistency and object-level controls. From\nnatural language instructions, MorphoSim produces\ndynamic environments where objects can be directed,\nrecolored, or removed, and scenes can be observed\nfrom arbitrary viewpoints. The framework integrates\ntrajectory-guided generation with feature field dis tillation, allowing edits to be applied interactively\nwithout full re-generation. Experiments show that Mor phoSim maintains high scene fidelity while enabling\ncontrollability and editability. The code is available\nat this https URL.', 'abstract_zh': '支持可控和可编辑时空环境的世界模型对于机器人学至关重要，能够实现可扩展的训练数据、可重现的评估和灵活的任务设计。尽管最近的文本生成视频模型能够生成逼真的动态效果，但它们局限于2D视角且交互性有限。我们介绍了MorphoSim，这是一种基于语言引导的框架，能够生成具有多视角一致性和对象级控制的4D场景。根据自然语言指令，MorphoSim能够生成动态环境，使物体能够被引导、重新着色或移除，并可以从任意视角观察场景。该框架结合了轨迹引导生成与特征场提炼，允许在无需完全重新生成的情况下进行交互式编辑。实验表明，MorphoSim能够在保持场景保真度的同时实现可控性和编辑性。相关代码可在以下链接获得。', 'title_zh': 'MorphoSim: 一个交互式、可控可编辑的基于语言的4D世界模拟器'}
{'arxiv_id': 'arXiv:2510.04201', 'title': 'World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge', 'authors': 'Moo Hyun Son, Jintaek Oh, Sun Bin Mun, Jaechul Roh, Sehyun Choi', 'link': 'https://arxiv.org/abs/2510.04201', 'abstract': 'While text-to-image (T2I) models can synthesize high-quality images, their performance degrades significantly when prompted with novel or out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We introduce World-To-Image, a novel framework that bridges this gap by empowering T2I generation with agent-driven world knowledge. We design an agent that dynamically searches the web to retrieve images for concepts unknown to the base model. This information is then used to perform multimodal prompt optimization, steering powerful generative backbones toward an accurate synthesis. Critically, our evaluation goes beyond traditional metrics, utilizing modern assessments like LLMGrader and ImageReward to measure true semantic fidelity. Our experiments show that World-To-Image substantially outperforms state-of-the-art methods in both semantic alignment and visual aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated NICE benchmark. Our framework achieves these results with high efficiency in less than three iterations, paving the way for T2I systems that can better reflect the ever-changing real world. Our demo code is available here\\footnote{this https URL}.', 'abstract_zh': 'World-To-Image: 通过代理驱动的世界知识桥接文本到图像生成的性能差距', 'title_zh': '世界到图像：基于代理驱动世界知识的文本到图像生成'}
{'arxiv_id': 'arXiv:2510.04039', 'title': '\\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding', 'authors': 'Bin Lei, Nuo Xu, Ali Payani, Mingyi Hong, Chunhua Liao, Yu Cao, Caiwen Ding', 'link': 'https://arxiv.org/abs/2510.04039', 'abstract': 'Multimodal large language models (MLLMs) have markedly expanded the competence of graphical user-interface (GUI) systems, propelling them beyond controlled simulations into complex, real-world environments across diverse platforms. However, practical usefulness is still bounded by the reliability of visual grounding, i.e., mapping textual references to exact on-screen elements. This limitation prevents the system from accurately performing pointer-level actions such as clicking or dragging. To address it, we introduce GUI-Spotlight -- a model trained for image-grounded reasoning that dynamically invokes multiple specialized tools to iteratively narrow its focus to the relevant region of the screen, thereby substantially improving visual grounding accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only 18.5K training samples achieves 52.8\\% accuracy, surpassing V2P-7B (50.6\\% with 9.6M training samples) and GTA-1-7B (50.1\\% with 1.56M training samples).', 'abstract_zh': '多模态大型语言模型（MLLMs）显著扩展了图形用户界面（GUI）系统的功能，使其从受控仿真扩展到多种平台上的复杂真实环境。然而，其实用性仍受视觉定位可靠性的限制，即文本引用到屏幕元素的精准映射。这一限制阻止系统准确执行指针级操作，如点击或拖拽。为解决这一问题，我们引入了GUI-Spotlight——一种训练用于图像导向推理的模型，能够动态调用多种专门工具以迭代地将注意力聚焦到屏幕的相关区域，从而显著提高视觉定位准确性。在ScreenSpot-Pro基准测试中，仅使用18,500个训练样本的GUI-Spotlight实现了52.8%的准确性，超过了V2P-7B（使用9,600,000个训练样本的50.6%准确性）和GTA-1-7B（使用1,560,000个训练样本的50.1%准确性）。', 'title_zh': 'GUI-Spotlight: 自适应迭代焦点细化方法以增强GUI视觉定位'}
{'arxiv_id': 'arXiv:2510.04020', 'title': 'Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models', 'authors': 'Hao Wu, Yuan Gao, Xingjian Shi, Shuaipeng Li, Fan Xu, Fan Zhang, Zhihong Zhu, Weiyan Wang, Xiao Luo, Kun Wang, Xian Wu, Xiaomeng Huang', 'link': 'https://arxiv.org/abs/2510.04020', 'abstract': 'To address the dual challenges of inherent stochasticity and non-differentiable metrics in physical spatiotemporal forecasting, we propose Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in Model-Based Reinforcement Learning. SFP constructs a novel Generative World Model to simulate diverse, high-fidelity future states, enabling an "imagination-based" environmental simulation. Within this framework, a base forecasting model acts as an agent, guided by a beam search-based planning algorithm that leverages non-differentiable domain metrics as reward signals to explore high-return future sequences. These identified high-reward candidates then serve as pseudo-labels to continuously optimize the agent\'s policy through iterative self-training, significantly reducing prediction error and demonstrating exceptional performance on critical domain metrics like capturing extreme events.', 'abstract_zh': '时空预测作为规划（SFP）：基于模型的强化学习新范式', 'title_zh': '时空预测作为规划：基于生成世界模型的模型导向强化学习方法'}
{'arxiv_id': 'arXiv:2510.03829', 'title': 'A4FN: an Agentic AI Architecture for Autonomous Flying Networks', 'authors': 'André Coelho, Pedro Ribeiro, Helder Fontes, Rui Campos', 'link': 'https://arxiv.org/abs/2510.03829', 'abstract': 'This position paper presents A4FN, an Agentic Artificial Intelligence (AI) architecture for intent-driven automation in Flying Networks (FNs) using Unmanned Aerial Vehicles (UAVs) as access nodes. A4FN leverages Generative AI and Large Language Models (LLMs) to enable real-time, context-aware network control via a distributed agentic system. It comprises two components: the Perception Agent (PA), which semantically interprets multimodal input -- including imagery, audio, and telemetry data -- from UAV-mounted sensors to derive Service Level Specifications (SLSs); and the Decision-and-Action Agent (DAA), which reconfigures the network based on inferred intents. A4FN embodies key properties of Agentic AI, including autonomy, goal-driven reasoning, and continuous perception-action cycles. Designed for mission-critical, infrastructure-limited scenarios such as disaster response, it supports adaptive reconfiguration, dynamic resource management, and interoperability with emerging wireless technologies. The paper details the A4FN architecture, its core innovations, and open research challenges in multi-agent coordination and Agentic AI integration in next-generation FNs.', 'abstract_zh': '本文展示了A4FN，一种基于代理人工智能（AI）的架构，用于在飞行网络（FNs）中通过无人驾驶航空车辆（UAVs）作为接入节点实现意图驱动的自动化。A4FN 利用生成型AI和大型语言模型（LLMs）通过分布式代理系统实现实时、情境感知的网络控制。它包括两个组成部分：感知代理（PA），其通过解释来自UAV载传感器的多模态输入（包括图像、音频和遥测数据）来推导服务级别规范（SLSs）；以及决策与行动代理（DAA），其根据推断出的意图重新配置网络。A4FN 具备代理人工智能的关键特性，包括自主性、目标驱动的推理以及持续的感知-行动循环。该设计适用于如灾害响应等关键任务、基础设施有限的场景，支持适应性重构、动态资源管理并能够与新兴无线技术协同工作。文章详细介绍了A4FN架构、核心创新以及多代理协调和代理人工智能集成在下一代飞行网络中的开放研究挑战。', 'title_zh': 'A4FN：自主飞行网络的主体性AI架构'}
{'arxiv_id': 'arXiv:2510.03699', 'title': 'Dissecting Larval Zebrafish Hunting using Deep Reinforcement Learning Trained RNN Agents', 'authors': 'Raaghav Malik, Satpreet H. Singh, Sonja Johnson-Yu, Nathan Wu, Roy Harpaz, Florian Engert, Kanaka Rajan', 'link': 'https://arxiv.org/abs/2510.03699', 'abstract': 'Larval zebrafish hunting provides a tractable setting to study how ecological and energetic constraints shape adaptive behavior in both biological brains and artificial agents. Here we develop a minimal agent-based model, training recurrent policies with deep reinforcement learning in a bout-based zebrafish simulator. Despite its simplicity, the model reproduces hallmark hunting behaviors -- including eye vergence-linked pursuit, speed modulation, and stereotyped approach trajectories -- that closely match real larval zebrafish. Quantitative trajectory analyses show that pursuit bouts systematically reduce prey angle by roughly half before strike, consistent with measurements. Virtual experiments and parameter sweeps vary ecological and energetic constraints, bout kinematics (coupled vs. uncoupled turns and forward motion), and environmental factors such as food density, food speed, and vergence limits. These manipulations reveal how constraints and environments shape pursuit dynamics, strike success, and abort rates, yielding falsifiable predictions for neuroscience experiments. These sweeps identify a compact set of constraints -- binocular sensing, the coupling of forward speed and turning in bout kinematics, and modest energetic costs on locomotion and vergence -- that are sufficient for zebrafish-like hunting to emerge. Strikingly, these behaviors arise in minimal agents without detailed biomechanics, fluid dynamics, circuit realism, or imitation learning from real zebrafish data. Taken together, this work provides a normative account of zebrafish hunting as the optimal balance between energetic cost and sensory benefit, highlighting the trade-offs that structure vergence and trajectory dynamics. We establish a virtual lab that narrows the experimental search space and generates falsifiable predictions about behavior and neural coding.', 'abstract_zh': '针对生态和能量约束如何塑造生物大脑和人工代理适应性行为的模式鱼胚胎猎食提供了可操作的研究环境：通过基于代理的最小化模型和深度强化学习在回合制模式鱼模拟器中训练循环策略进行探究。', 'title_zh': 'dissecting 幼鱼猎食行为using 深度强化学习训练的RNN代理模型'}
{'arxiv_id': 'arXiv:2510.03578', 'title': 'Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning', 'authors': 'Haoran Li, Chenhan Xiao, Muhao Guo, Yang Weng', 'link': 'https://arxiv.org/abs/2510.03578', 'abstract': 'Learning dynamics is essential for model-based control and Reinforcement Learning in engineering systems, such as robotics and power systems. However, limited system measurements, such as those from low-resolution sensors, demand sample-efficient learning. Symmetry provides a powerful inductive bias by characterizing equivariant relations in system states to improve sample efficiency. While recent methods attempt to discover symmetries from data, they typically assume a single global symmetry group and treat symmetry discovery and dynamic learning as separate tasks, leading to limited expressiveness and error accumulation. In this paper, we propose the Latent Mixture of Symmetries (Latent MoS), an expressive model that captures a mixture of symmetry-governed latent factors from complex dynamical measurements. Latent MoS focuses on dynamic learning while locally and provably preserving the underlying symmetric transformations. To further capture long-term equivariance, we introduce a hierarchical architecture that stacks MoS blocks. Numerical experiments in diverse physical systems demonstrate that Latent MoS outperforms state-of-the-art baselines in interpolation and extrapolation tasks while offering interpretable latent representations suitable for future geometric and safety-critical analyses.', 'abstract_zh': '基于模型的控制和强化学习在工程系统中（如机器人和电力系统）需要学习动力学。然而，受限于有限的系统测量，如低分辨率传感器的数据，要求高效学习。对称性通过表征系统状态下的等变关系来提供强大的归纳偏置，从而提高学习效率。虽然近期方法尝试从数据中发现对称性，但它们通常假设单一全局对称群，并将对称性发现和动态学习视为分离任务，导致表达能力有限和错误积累。本文提出了一种表达性强的模型——隐含混合对称性（Latent Mixture of Symmetries, Latent MoS），该模型从复杂的动态测量中捕捉由对称性控制的潜在因素混合。Latent MoS聚焦于动态学习，同时局部和可证明地保留底层对称变换。为进一步捕捉长期等变性，我们引入了层次架构，将MoS块堆叠起来。在多种物理系统中的数值实验表明，Latent MoS在插值和外推任务中优于最先进的基线方法，同时提供易于未来几何和安全关键分析的可解释潜在表示。', 'title_zh': '潜在对称混合的样本高效动力学习'}
{'arxiv_id': 'arXiv:2510.03351', 'title': 'Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks', 'authors': 'Song Wang, Zhenyu Lei, Zhen Tan, Jundong Li, Javier Rasero, Aiying Zhang, Chirag Agarwal', 'link': 'https://arxiv.org/abs/2510.03351', 'abstract': 'Nearly one in five adolescents currently live with a diagnosed mental or behavioral health condition, such as anxiety, depression, or conduct disorder, underscoring the urgency of developing accurate and interpretable diagnostic tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a powerful lens into large-scale functional connectivity, where brain regions are modeled as nodes and inter-regional synchrony as edges, offering clinically relevant biomarkers for psychiatric disorders. While prior works use graph neural network (GNN) approaches for disorder prediction, they remain complex black-boxes, limiting their reliability and clinical translation. In this work, we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages large language models (LLMs) and neurobiological domain knowledge to automatically generate, filter, and encode interpretable functional connectivity concepts. Each concept is represented as a structured subgraph linking specific brain regions, which are then passed through a concept classifier. Our design ensures predictions through clinically meaningful connectivity patterns, enabling both interpretability and strong predictive performance. Extensive experiments across multiple psychiatric disorder datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform their vanilla counterparts, improving accuracy while providing transparent, clinically aligned explanations. Furthermore, concept analyses highlight disorder-specific connectivity patterns that align with expert knowledge and suggest new hypotheses for future investigation, establishing CONCEPTNEURO as an interpretable, domain-informed framework for psychiatric disorder diagnosis.', 'abstract_zh': '基于概念的神经精神障碍诊断框架：CONCEPTNEURO', 'title_zh': '概念引导的图神经网络在可解释神经精神疾病诊断中的应用'}
{'arxiv_id': 'arXiv:2510.03286', 'title': 'A Biologically Interpretable Cognitive Architecture for Online Structuring of Episodic Memories into Cognitive Maps', 'authors': 'E.A. Dzhivelikian, A.I. Panov', 'link': 'https://arxiv.org/abs/2510.03286', 'abstract': 'Cognitive maps provide a powerful framework for understanding spatial and abstract reasoning in biological and artificial agents. While recent computational models link cognitive maps to hippocampal-entorhinal mechanisms, they often rely on global optimization rules (e.g., backpropagation) that lack biological plausibility. In this work, we propose a novel cognitive architecture for structuring episodic memories into cognitive maps using local, Hebbian-like learning rules, compatible with neural substrate constraints. Our model integrates the Successor Features framework with episodic memories, enabling incremental, online learning through agent-environment interaction. We demonstrate its efficacy in a partially observable grid-world, where the architecture autonomously organizes memories into structured representations without centralized optimization. This work bridges computational neuroscience and AI, offering a biologically grounded approach to cognitive map formation in artificial adaptive agents.', 'abstract_zh': '认知地图提供了一种强大的框架，用于理解生物和人工代理的空间和抽象推理。虽然最近的计算模型将认知地图与海马-Entorhinal机制联系起来，但它们往往依赖于全局优化规则（如反向传播），这些规则缺乏生物可行性。在本工作中，我们提出了一种新型的认知架构，用于使用局部、类似Hebb的学习规则将 episodic 记忆结构化为认知地图，该规则与神经元结构限制兼容。我们的模型将 Successor Features 框架与 episodic 记忆结合起来，通过代理与环境的交互实现增量、在线学习。我们在一个部分可观测的网格世界中展示了其有效性，其中架构在没有集中优化的情况下自主地将记忆组织成结构化表示。本工作将计算神经科学与人工智能相结合，提供了一种基于生物学的方法来形成人工自适应代理的认知地图。', 'title_zh': '一种生物可解释的认知架构：用于在线将 episodic 记忆结构化为认知地图'}
{'arxiv_id': 'arXiv:2510.03265', 'title': 'MindCraft: How Concept Trees Take Shape In Deep Models', 'authors': 'Bowei Tian, Yexiao He, Wanghao Ye, Ziyao Wang, Meng Liu, Ang Li', 'link': 'https://arxiv.org/abs/2510.03265', 'abstract': 'Large-scale foundation models demonstrate strong performance across language, vision, and reasoning tasks. However, how they internally structure and stabilize concepts remains elusive. Inspired by causal inference, we introduce the MindCraft framework built upon Concept Trees. By applying spectral decomposition at each layer and linking principal directions into branching Concept Paths, Concept Trees reconstruct the hierarchical emergence of concepts, revealing exactly when they diverge from shared representations into linearly separable subspaces. Empirical evaluations across diverse scenarios across disciplines, including medical diagnosis, physics reasoning, and political decision-making, show that Concept Trees recover semantic hierarchies, disentangle latent concepts, and can be widely applied across multiple domains. The Concept Tree establishes a widely applicable and powerful framework that enables in-depth analysis of conceptual representations in deep models, marking a significant step forward in the foundation of interpretable AI.', 'abstract_zh': '大规模基础模型在语言、视觉和推理任务上表现出强大性能。然而，它们内部如何构建和稳定概念仍然不清楚。受因果推理启发，我们提出了基于概念树的MindCraft框架。通过在每一层应用谱分解并将主方向连接成分支概念路径，概念树重建了概念的层次涌现过程，揭示了它们从共享表示到线性可分子空间的具体分歧时刻。跨学科的实证评估，包括医学诊断、物理推理和政治决策等领域，表明概念树可以恢复语义层次结构、分离潜在概念，并且可以在多个领域广泛应用。概念树提供了一个广泛适用且强大的框架，使我们能够深入分析深度模型中的概念表示，标志着可解释AI基础的一大进步。', 'title_zh': 'MindCraft：概念树在深度模型中如何形成'}
