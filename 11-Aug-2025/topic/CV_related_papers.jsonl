{'arxiv_id': 'arXiv:2508.06291', 'title': 'Real-Time 3D Vision-Language Embedding Mapping', 'authors': 'Christian Rauch, Björn Ellensohn, Linus Nwankwo, Vedant Dave, Elmar Rueckert', 'link': 'https://arxiv.org/abs/2508.06291', 'abstract': 'A metric-accurate semantic 3D representation is essential for many robotic tasks. This work proposes a simple, yet powerful, way to integrate the 2D embeddings of a Vision-Language Model in a metric-accurate 3D representation at real-time. We combine a local embedding masking strategy, for a more distinct embedding distribution, with a confidence-weighted 3D integration for more reliable 3D embeddings. The resulting metric-accurate embedding representation is task-agnostic and can represent semantic concepts on a global multi-room, as well as on a local object-level. This enables a variety of interactive robotic applications that require the localisation of objects-of-interest via natural language. We evaluate our approach on a variety of real-world sequences and demonstrate that these strategies achieve a more accurate object-of-interest localisation while improving the runtime performance in order to meet our real-time constraints. We further demonstrate the versatility of our approach in a variety of interactive handheld, mobile robotics and manipulation tasks, requiring only raw image data.', 'abstract_zh': '一种度量准确的语义3D表示对于许多机器人任务至关重要。本文提出了一种简单而强大的方法，实现在实时条件下将视觉-语言模型的2D嵌入整合到度量准确的3D表示中。我们结合了局部嵌入掩蔽策略以获得更明显的嵌入分布，并使用信心加权的3D整合以获得更可靠的3D嵌入。所得的度量准确嵌入表示具有任务无关性，能够在全球多房间尺度和局部物体尺度上表示语义概念。这使得可以通过自然语言进行目标物体的局部化的一系列交互式机器人应用成为可能。我们在多种现实世界序列上评估了我们的方法，并展示了这些策略在提高目标物体局部化准确性的同时，提高了运行时性能，以满足我们的实时约束。我们还展示了我们方法在各种交互式手持机器人、移动机器人和操作任务中的通用性，仅需使用原始图像数据即可。', 'title_zh': '实时3D视觉-语言嵌入映射'}
{'arxiv_id': 'arXiv:2508.06207', 'title': 'Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization', 'authors': 'Andrea Dal Prete, Seyram Ofori, Chan Yon Sin, Ashwin Narayan, Francesco Braghin, Marta Gandolla, Haoyong Yu', 'link': 'https://arxiv.org/abs/2508.06207', 'abstract': 'Back exoskeletons can reduce musculoskeletal strain, but their effectiveness depends on support modulation and adaptive control. This study addresses two challenges: defining optimal support strategies and developing adaptive control based on payload estimation. We introduce an optimization space based on muscle activity reduction, perceived discomfort, and user preference, constructing functions to identify optimal strategies. Experiments with 12 subjects revealed optimal operating regions, highlighting the need for dynamic modulation. Based on these insights, we developed a vision-based adaptive control pipeline that estimates payloads in real-time by enhancing exoskeleton contextual understanding, minimising latency and enabling support adaptation within the defined optimisation space. Validation with 12 more subjects showed over 80% accuracy and improvements across all metrics. Compared to static control, adaptive modulation reduced peak back muscle activation by up to 23% while preserving user preference and minimising discomfort. These findings validate the proposed framework and highlight the potential of intelligent, context-aware control in industrial exoskeletons.', 'abstract_zh': '背负外骨骼可以减轻肌骨应力，但其有效性取决于支持调节和自适应控制。本研究解决两大挑战：定义最优支持策略并基于负载估计开发自适应控制。我们基于肌肉活动减少、感知不适和用户偏好引入优化空间，构建函数以识别最优策略。12名受试者的实验揭示了最优操作区域，强调了动态调节的必要性。基于这些洞察，我们开发了一种基于视觉的自适应控制流水线，通过增强外骨骼的上下文理解实时估计负载，减少延迟并使支持适应于定义的优化空间。12名更多受试者的验证显示超过80%的准确率，并在所有指标上有所改善。与静态控制相比，自适应调节可将背部肌肉峰值激活降低高达23%，同时保持用户偏好并最小化不适。这些发现验证了所提出的框架，并突显了智能、上下文感知控制在工业外骨骼中的潜力。', 'title_zh': '基于计算机视觉的自适应控制以优化背部外骨骼性能'}
{'arxiv_id': 'arXiv:2508.06227', 'title': 'Depth Jitter: Seeing through the Depth', 'authors': 'Md Sazidur Rahman, David Cabecinhas, Ricard Marxer', 'link': 'https://arxiv.org/abs/2508.06227', 'abstract': 'Depth information is essential in computer vision, particularly in underwater imaging, robotics, and autonomous navigation. However, conventional augmentation techniques overlook depth aware transformations, limiting model robustness in real world depth variations. In this paper, we introduce Depth-Jitter, a novel depth-based augmentation technique that simulates natural depth variations to improve generalization. Our approach applies adaptive depth offsetting, guided by depth variance thresholds, to generate synthetic depth perturbations while preserving structural integrity. We evaluate Depth-Jitter on two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on model stability under diverse depth conditions. Extensive experiments compare Depth-Jitter against traditional augmentation strategies such as ColorJitter, analyzing performance across varying learning rates, encoders, and loss functions. While Depth-Jitter does not always outperform conventional methods in absolute performance, it consistently enhances model stability and generalization in depth-sensitive environments. These findings highlight the potential of depth-aware augmentation for real-world applications and provide a foundation for further research into depth-based learning strategies. The proposed technique is publicly available to support advancements in depth-aware augmentation. The code is publicly available on \\href{this https URL}{github}.', 'abstract_zh': '深度信息在计算机视觉中的应用对于水下成像、机器人技术和自主导航至关重要。然而，传统的增强技术忽视了深度感知变换，限制了模型在真实世界深度变化中的鲁棒性。本文引入了一种新颖的基于深度的增强技术——Depth-Jitter，以模拟自然的深度变化从而提高模型的泛化能力。我们的方法通过基于深度方差阈值的应用自适应深度偏移，生成合成的深度扰动同时保持结构完整性。我们在两个基准数据集FathomNet和UTDAC2020上评估了Depth-Jitter，展示了其在不同深度条件下的模型稳定性。广泛的经验研究表明，Depth-Jitter在传统增强策略如ColorJitter中表现更好，特别是在深度敏感环境中增强了模型的稳定性和泛化能力。这些发现突显了深度意识增强在实际应用中的潜力，并为深度学习策略的进一步研究奠定了基础。提出的技巧已公开发布，以支持深度意识增强技术的发展。源代码可在GitHub上公开获取。', 'title_zh': '深度抖动：透过深度观察'}
{'arxiv_id': 'arXiv:2508.06453', 'title': 'Text Embedded Swin-UMamba for DeepLesion Segmentation', 'authors': 'Ruida Cheng, Tejas Sudharshan Mathai, Pritam Mukherjee, Benjamin Hou, Qingqing Zhu, Zhiyong Lu, Matthew McAuliffe, Ronald M. Summers', 'link': 'https://arxiv.org/abs/2508.06453', 'abstract': 'Segmentation of lesions on CT enables automatic measurement for clinical assessment of chronic diseases (e.g., lymphoma). Integrating large language models (LLMs) into the lesion segmentation workflow offers the potential to combine imaging features with descriptions of lesion characteristics from the radiology reports. In this study, we investigate the feasibility of integrating text into the Swin-UMamba architecture for the task of lesion segmentation. The publicly available ULS23 DeepLesion dataset was used along with short-form descriptions of the findings from the reports. On the test dataset, a high Dice Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p < 0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by 1.74% and 0.22%, respectively. The dataset and code can be accessed at this https URL', 'abstract_zh': '基于CT的病变分割 enables 自动测量以评估慢性疾病（例如淋巴瘤）的临床评估。将大规模语言模型集成到病变分割工作流程中，有望结合影像特征与放射学报告中关于病变特征的描述。在本研究中，我们探讨了将文本集成到Swin-UMamba架构中以完成病变分割任务的可行性。使用公开可用的ULS23 DeepLesion数据集以及报告中的简短发现描述。在测试数据集上，病变分割获得了82%的高Dice分数和6.58像素的低Hausdorff距离。所提出的Text-Swin-UMamba模型优于先前的方法：在LLM驱动的LanGuideMedSeg模型的基础上提高了37%（p < 0.001），并分别超过基于纯图像的xLSTM-UNet和nnUNet模型1.74%和0.22%。数据集和代码可访问于此网址。', 'title_zh': 'Text嵌入Swin-UMamba用于DeepLesion分割'}
{'arxiv_id': 'arXiv:2508.06407', 'title': 'A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery', 'authors': 'Ch Muhammad Awais, Marco Reggiannini, Davide Moroni, Oktay Karakus', 'link': 'https://arxiv.org/abs/2508.06407', 'abstract': 'High-resolution imagery plays a critical role in improving the performance of visual recognition tasks such as classification, detection, and segmentation. In many domains, including remote sensing and surveillance, low-resolution images can limit the accuracy of automated analysis. To address this, super-resolution (SR) techniques have been widely adopted to attempt to reconstruct high-resolution images from low-resolution inputs. Related traditional approaches focus solely on enhancing image quality based on pixel-level metrics, leaving the relationship between super-resolved image fidelity and downstream classification performance largely underexplored. This raises a key question: can integrating classification objectives directly into the super-resolution process further improve classification accuracy? In this paper, we try to respond to this question by investigating the relationship between super-resolution and classification through the deployment of a specialised algorithmic strategy. We propose a novel methodology that increases the resolution of synthetic aperture radar imagery by optimising loss functions that account for both image quality and classification performance. Our approach improves image quality, as measured by scientifically ascertained image quality indicators, while also enhancing classification accuracy.', 'abstract_zh': '高分辨率成像在提高分类、检测和分割等视觉识别任务性能中扮演着关键角色。在遥感和监控等多个领域，低分辨率图像会限制自动化分析的准确性。为解决这一问题，已经广泛采用了超分辨率（SR）技术，希望通过低分辨率输入重建高分辨率图像。相关传统的SR方法主要专注于基于像素级别的图像质量增强，而对超分辨率图像保真度与下游分类性能之间的关系研究相对较少。这引发了一个关键问题：是否可以将分类目标直接整合到超分辨率过程中以进一步提高分类准确性？在本文中，我们通过部署专门的算法策略来探索超分辨率与分类之间的关系，并提出了一种新颖的方法，通过优化同时考虑图像质量和分类性能的损失函数来提升合成孔径雷达图像的分辨率。我们的方法不仅在科学评价的图像质量指标下改善了图像质量，还在分类准确性上有所提升。', 'title_zh': '一种针对SAR图像中船舶目标的分类意识超分辨率框架'}
{'arxiv_id': 'arXiv:2508.06318', 'title': 'Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection', 'authors': "Giacomo D'Amicantonio, Snehashis Majhi, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, François Bremond, Egor Bondarev", 'link': 'https://arxiv.org/abs/2508.06318', 'abstract': 'Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.', 'abstract_zh': '视频异常检测（VAD）是一个具有挑战性的任务，由于异常事件的多样性以及标注数据的有限性。在弱监督视频异常检测（WSVAD）范式下，训练时仅提供视频级别的标签，而预测在帧级别进行。尽管最新的模型在简单的异常（如爆炸）上表现良好，但在复杂的现实世界事件（如偷窃）上却面临困难。这种困难源于两个关键问题：（1）当前模型无法处理异常类型的多样性，因为它们使用共享模型处理所有类别，忽略了类别特定的特征；（2）弱监督信号缺乏精确的时间信息，限制了捕捉正常事件与异常事件混合下的细微异常模式的能力。为了解决这些挑战，我们提出了一种新的框架——基于高斯点积的专家混合模型（GS-MoE），该框架采用一组专门针对特定异常类型进行捕捉的专家模型。这些专家模型由时间高斯点积损失指导，使模型能够利用时间一致性并增强弱监督信号。高斯点积方法通过关注最有可能包含异常事件的时间段，促使对异常进行更精确和全面的表示。这些专家模型的预测结果通过专家混合机制进行整合，以建模复杂多样的异常模式间的复杂关系。我们的方法在UCF-Crime数据集上达到了91.58%的AUC性能，并在XD-Violence和MSAD数据集上展示了更优异的结果。通过利用类别特定的专家知识和时间指导，GS-MoE为弱监督下的视频异常检测设置了新的基准。', 'title_zh': '混合专家由高斯点引导：一种弱监督视频异常检测的新方法'}
{'arxiv_id': 'arXiv:2508.06287', 'title': 'Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification', 'authors': 'Mobarak Abumohsen, Enrique Costa-Montenegro, Silvia García-Méndez, Amani Yousef Owda, Majdi Owda', 'link': 'https://arxiv.org/abs/2508.06287', 'abstract': 'Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one of the most common causes of death for men and women worldwide. Computed Tomography (CT) images are the most preferred diagnosis method because of their low cost and their faster processing times. Many researchers have proposed various ways of identifying lung cancer using CT images. However, such techniques suffer from significant false positives, leading to low accuracy. The fundamental reason results from employing a small and imbalanced dataset. This paper introduces an innovative approach for LC detection and classification from CT images based on the DenseNet201 model. Our approach comprises several advanced methods such as Focal Loss, data augmentation, and regularization to overcome the imbalanced data issue and overfitting challenge. The findings show the appropriateness of the proposal, attaining a promising performance of 98.95% accuracy.', 'abstract_zh': '肺癌（LC）是全球诊断频率最高的癌症之一，并且是男性和女性最常见的死亡原因之一。计算机断层扫描（CT）图像因其成本低和处理速度快，是最常用的诊断方法。许多研究人员提出了多种利用CT图像识别肺癌的方法。然而，这些技术由于采用小规模和不均衡的数据集而面临显著的假阳性问题，导致准确率较低。根本原因在于使用了小且不均衡的数据集。本文介绍了一种基于DenseNet201模型的创新方法，用于从CT图像检测和分类肺癌。该方法结合了焦点损失、数据增强和正则化等advanced方法，以解决不均衡数据和过拟合问题。研究结果表明该方法的有效性，达到了98.95%的准确率。', 'title_zh': '先进的深度学习技术在肺癌检测与分类中的应用'}
{'arxiv_id': 'arXiv:2508.06170', 'title': 'Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation', 'authors': 'Ojonugwa Oluwafemi Ejiga Peter, Akingbola Oluwapemiisin, Amalahu Chetachi, Adeniran Opeyemi, Fahmi Khalifa, Md Mahmudur Rahman', 'link': 'https://arxiv.org/abs/2508.06170', 'abstract': 'Colonoscopy is a vital tool for the early diagnosis of colorectal cancer, which is one of the main causes of cancer-related mortality globally; hence, it is deemed an essential technique for the prevention and early detection of colorectal cancer. The research introduces a unique multidirectional architectural framework to automate polyp detection within colonoscopy images while helping resolve limited healthcare dataset sizes and annotation complexities. The research implements a comprehensive system that delivers synthetic data generation through Stable Diffusion enhancements together with detection and segmentation algorithms. This detection approach combines Faster R-CNN for initial object localization while the Segment Anything Model (SAM) refines the segmentation masks. The faster R-CNN detection algorithm achieved a recall of 93.08% combined with a precision of 88.97% and an F1 score of 90.98%.SAM is then used to generate the image mask. The research evaluated five state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet, and MANet using ResNet34 as a base model. The results demonstrate the superior performance of FPN with the highest scores of PSNR (7.205893) and SSIM (0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced performance in IoU (64.20%) and Dice score (77.53%).', 'abstract_zh': '结肠镜检查是一种早期诊断结肠直肠癌的关键工具，而结肠直肠癌是全球癌症相关死亡的主要原因之一；因此，它被认为是预防和早期发现结肠直肠癌的essential技术。本研究介绍了一种独特的多方向架构框架，以自动化结肠镜检查图像中的息肉检测，同时帮助解决有限的医疗数据集大小和标注复杂性问题。该研究实现了一个全面的系统，通过Stable Diffusion增强技术生成合成数据，并结合了检测和分割算法。检测方法结合了Faster R-CNN进行初始对象定位，并使用Segment Anything Model (SAM)细化分割掩码。Faster R-CNN检测算法的召回率为93.08%，精确率为88.97%，F1分为90.98%。然后使用SAM生成图像掩码。研究评估了包括U-Net、PSPNet、FPN、LinkNet和MANet在内的五种最先进的分割模型，使用ResNet34作为基础模型。结果表明，FPN表现出色，具有最高的PSNR（7.205893）和SSIM（0.492381）得分，而U-Net在召回率（84.85%）方面表现出色，LinkNet在交并比（IoU，64.20%）和Dice评分（77.53%）方面表现出平衡的性能。', 'title_zh': '基于综合检测与掩码生成的合成数据驱动多架构自动化息肉分割框架'}
{'arxiv_id': 'arXiv:2508.06169', 'title': 'UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting', 'authors': 'Wenpeng Xing, Jie Chen, Zaifeng Yang, Changting Lin, Jianfeng Dong, Chaochao Chen, Xun Zhou, Meng Han', 'link': 'https://arxiv.org/abs/2508.06169', 'abstract': 'Underwater 3D scene reconstruction faces severe challenges from light absorption, scattering, and turbidity, which degrade geometry and color fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF extensions such as SeaThru-NeRF incorporate physics-based models, their MLP reliance limits efficiency and spatial resolution in hazy environments. We introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction. Key innovations include: (1) a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter; and (2) a Physics-Aware Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating Gaussians via uncertainty scoring, ensuring artifact-free geometry. The pipeline operates in training and rendering stages. During training, noisy Gaussians are optimized end-to-end with underwater parameters, guided by PAUP pruning and scattering modeling. In rendering, refined Gaussians produce clean Unattenuated Radiance Images (URIs) free from media effects, while learned physics enable realistic Underwater Images (UWIs) with accurate light transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% reduction in floating artifacts.', 'abstract_zh': 'underwater 3D场景重建面临着严重的挑战，包括光线吸收、散射和浑浊度，这些因素在传统方法如神经辐射场（NeRF）中会降低几何精度和颜色保真度。虽然SeaThru-NeRF等NeRF扩展结合了物理模型，但它们对多层感知器（MLP）的依赖性限制了在浑浊环境中的效率和空间分辨率。我们提出了UW-3DGS，这是一种新的框架，将3D高斯散斑（3DGS）适应于稳健的水下重建。关键创新包括：（1）一种基于体素的空间变异性吸收和后向散射的插件学习水下图像形成模块；（2）一种物理感知不确定性剪枝（PAUP）分支，通过不确定性评分自适应地移除噪声的浮动高斯体素，确保无伪影的几何结构。该管道在训练和渲染阶段运行。在训练阶段，噪声的高斯体素通过PAUP剪枝和散射建模在水下参数的引导下进行端到端的优化。在渲染阶段，优化后的高斯体素产生无介质效应的清洁未衰减辐射图像（URIs），而学习到的物理模型能够生成具有准确光照传输的逼真水下图像（UWIs）。在SeaThru-NeRF和UWBundle数据集上的实验显示了优越的性能，PSNR达到27.604，SSIM达到0.868，LPIPS达到0.104，浮动伪影减少了约65%。', 'title_zh': 'UW-3DGS: 水下3D重建与物理感知高斯点绘制'}
{'arxiv_id': 'arXiv:2508.06136', 'title': 'Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation', 'authors': 'YoungChan Choi, HengFei Wang, YiHua Cheng, Boeun Kim, Hyung Jin Chang, YoungGeun Choi, Sang-Il Choi', 'link': 'https://arxiv.org/abs/2508.06136', 'abstract': 'We propose a novel 3D gaze redirection framework that leverages an explicit 3D eyeball structure. Existing gaze redirection methods are typically based on neural radiance fields, which employ implicit neural representations via volume rendering. Unlike these NeRF-based approaches, where the rotation and translation of 3D representations are not explicitly modeled, we introduce a dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian Splatting (3DGS). Our method generates photorealistic images that faithfully reproduce the desired gaze direction by explicitly rotating and translating the 3D eyeball structure. In addition, we propose an adaptive deformation module that enables the replication of subtle muscle movements around the eyes. Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our framework is capable of generating diverse novel gaze images, achieving superior image quality and gaze estimation accuracy compared to previous state-of-the-art methods.', 'abstract_zh': '我们提出了一种新颖的3D眼球重定向框架，该框架利用显式的3D眼球结构。现有的眼球重定向方法通常基于神经辐射场（Neural Radiance Fields，NeRF），通过体渲染使用隐式的神经表示。与这些基于NeRF的方法不同，在这些方法中3D表示的旋转和平移没有明确建模，我们引入了专门的3D眼球结构，使用3D高斯点积（3DGS）表示眼球。该方法通过明确旋转和平移3D眼球结构生成保真的图像，以忠实再现所需的眼球方向。此外，我们提出了一种自适应变形模块，能够复制眼部周围细微肌肉运动。通过在ETH-XGaze数据集上进行的实验，我们展示了该框架能够生成多样化的新型眼球图像，图像质量以及眼球追踪准确性均优于以前的best-in-class方法。', 'title_zh': 'Roll Your Eyes: 通过明确的3D眼球旋转实现凝视重定向'}
{'arxiv_id': 'arXiv:2508.06038', 'title': 'Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models', 'authors': 'Huanyu Wang, Jushi Kai, Haoli Bai, Lu Hou, Bo Jiang, Ziwei He, Zhouhan Lin', 'link': 'https://arxiv.org/abs/2508.06038', 'abstract': 'Vision-Language Models (VLMs) typically replace the predefined image placeholder token (<image>) in textual instructions with visual features from an image encoder, forming the input to a backbone Large Language Model (LLM). However, the large number of vision tokens significantly increases the context length, leading to high computational overhead and inference latency. While previous efforts mitigate this by selecting only important visual features or leveraging learnable queries to reduce token count, they often compromise performance or introduce substantial extra costs. In response, we propose Fourier-VLM, a simple yet efficient method that compresses visual representations in the frequency domain. Our approach is motivated by the observation that vision features output from the vision encoder exhibit concentrated energy in low-frequency components. Leveraging this, we apply a low-pass filter to the vision features using a two-dimentional Discrete Cosine Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier Transform (FFT) operator with a time complexity of $\\mathcal{O}(n\\log n)$, minimizing the extra computational cost while introducing no additional parameters. Extensive experiments across various image-based benchmarks demonstrate that Fourier-VLM achieves competitive performance with strong generalizability across both LLaVA and Qwen-VL architectures. Crucially, it reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2% compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.', 'abstract_zh': 'Fourier-VLM：频率域中简洁高效的视觉表示压缩方法', 'title_zh': 'Fourier-VLM：在频域压缩视觉令牌以适应大型视觉-语言模型'}
{'arxiv_id': 'arXiv:2508.06021', 'title': 'Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis', 'authors': 'Utku Ozbulak, Michaela Cohrs, Hristo L. Svilenov, Joris Vankerschaver, Wesley De Neve', 'link': 'https://arxiv.org/abs/2508.06021', 'abstract': 'Sub-visible particle analysis using flow imaging microscopy combined with deep learning has proven effective in identifying particle types, enabling the distinction of harmless components such as silicone oil from protein particles. However, the scarcity of available data and severe imbalance between particle types within datasets remain substantial hurdles when applying multi-class classifiers to such problems, often forcing researchers to rely on less effective methods. The aforementioned issue is particularly challenging for particle types that appear unintentionally and in lower numbers, such as silicone oil and air bubbles, as opposed to protein particles, where obtaining large numbers of images through controlled settings is comparatively straightforward. In this work, we develop a state-of-the-art diffusion model to address data imbalance by generating high-fidelity images that can augment training datasets, enabling the effective training of multi-class deep neural networks. We validate this approach by demonstrating that the generated samples closely resemble real particle images in terms of visual quality and structure. To assess the effectiveness of using diffusion-generated images in training datasets, we conduct large-scale experiments on a validation dataset comprising 500,000 protein particle images and demonstrate that this approach improves classification performance with no negligible downside. Finally, to promote open research and reproducibility, we publicly release both our diffusion models and the trained multi-class deep neural network classifiers, along with a straightforward interface for easy integration into future studies, at this https URL.', 'abstract_zh': '使用流影像显微镜结合深度学习的亚显微粒子分析已经证明能够在区分无害成分（如硅油）和蛋白质颗粒方面有效识别粒子类型。然而，可用数据的稀缺性和数据集中粒子类型间的严重失衡是应用多类分类器时的重大障碍，通常迫使研究人员依赖效果较差的方法。这一问题尤其对硅油和气泡等无意出现且数量较少的粒子类型构成挑战，相比之下，获得蛋白质颗粒的大规模图像较为容易。在本工作中，我们开发了一种最先进的扩散模型来解决数据不平衡问题，通过生成高保真图像以扩充训练数据集，从而有效训练多类深度神经网络。我们通过验证生成样本在视觉质量和结构方面与真实粒子图像高度相似来证明这种方法的有效性。为了评估使用扩散生成图像在训练数据集中的效果，我们在包含500,000个蛋白质颗粒图像的验证数据集上进行大规模实验，并证明这种方法能够提升分类性能且无明显缺点。最后，为了促进开放研究和可重复性，我们公开发布了我们的扩散模型、训练好的多类深度神经网络分类器以及简易接口，以便于未来研究中的轻松集成，网址为：https://www.example.com。', 'title_zh': '基于生成AI的图像合成在流动成像显微镜中改进亚显微粒子分类'}
{'arxiv_id': 'arXiv:2508.05950', 'title': 'A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image', 'authors': 'Yanxing Liang, Yinghui Wang, Jinlong Yang, Wei Li', 'link': 'https://arxiv.org/abs/2508.05950', 'abstract': 'The lack of spatial dimensional information remains a challenge in normal estimation from a single image. Recent diffusion-based methods have demonstrated significant potential in 2D-to-3D implicit mapping, they rely on data-driven statistical priors and miss the explicit modeling of light-surface interaction, leading to multi-view normal direction conflicts. Moreover, the discrete sampling mechanism of diffusion models causes gradient discontinuity in differentiable rendering reconstruction modules, preventing 3D geometric errors from being backpropagated to the normal generation network, thereby forcing existing methods to depend on dense normal annotations. This paper proposes SINGAD, a novel Self-supervised framework from a single Image for Normal estimation via 3D GAussian splatting guided Diffusion. By integrating physics-driven light-interaction modeling and a differentiable rendering-based reprojection strategy, our framework directly converts 3D geometric errors into normal optimization signals, solving the challenges of multi-view geometric inconsistency and data dependency. Specifically, the framework constructs a light-interaction-driven 3DGS reparameterization model to generate multi-scale geometric features consistent with light transport principles, ensuring multi-view normal consistency. A cross-domain feature fusion module is designed within a conditional diffusion model, embedding geometric priors to constrain normal generation while maintaining accurate geometric error propagation. Furthermore, a differentiable 3D reprojection loss strategy is introduced for self-supervised optimization that minimizes geometric error between the reconstructed and input image, eliminating dependence on annotated normal datasets. Quantitative evaluations on the Google Scanned Objects dataset demonstrate that our method outperforms state-of-the-art approaches across multiple metrics.', 'abstract_zh': '单张图像引导的3D高斯点扩散Normals估计新框架：自监督3D光互作用引导扩散(SINGAD)', 'title_zh': '基于单张图像的3DGS-扩散自监督框架用于法线估计'}
{'arxiv_id': 'arXiv:2508.05783', 'title': 'Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks', 'authors': 'Mengyu Li, Guoyao Shen, Chad W. Farris, Xin Zhang', 'link': 'https://arxiv.org/abs/2508.05783', 'abstract': 'Machine learning using transformers has shown great potential in medical imaging, but its real-world applicability remains limited due to the scarcity of annotated data. In this study, we propose a practical framework for the few-shot deployment of pretrained MRI transformers in diverse brain imaging tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a large-scale, multi-cohort brain MRI dataset comprising over 31 million slices, we obtain highly transferable latent representations that generalize well across tasks and datasets. For high-level tasks such as classification, a frozen MAE encoder combined with a lightweight linear head achieves state-of-the-art accuracy in MRI sequence identification with minimal supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a hybrid architecture that fuses multiscale CNN features with pretrained MAE embeddings. This model consistently outperforms other strong baselines in both skull stripping and multi-class anatomical segmentation under data-limited conditions. With extensive quantitative and qualitative evaluations, our framework demonstrates efficiency, stability, and scalability, suggesting its suitability for low-resource clinical environments and broader neuroimaging applications.', 'abstract_zh': '使用变压器的机器学习在医学影像领域展示了巨大潜力，但由于标注数据稀缺，其实用性仍受到限制。本研究提出了一种实用框架，用于在多样化的脑部影像任务中进行预训练MRI变压器的一次性部署。通过在包含超过3100万切片的大规模多队列脑MRI数据集上利用Masked Autoencoder (MAE) 的预训练策略，我们获得了高度可移植的潜在表示，这些表示在不同任务和数据集中具有良好的泛化能力。在高层任务如分类方面，冻结的MAE编码器结合轻量级线性头，在MRI序列识别方面在最少监督下达到了最先进的准确性。在低层任务如分割方面，我们提出了MAE-Funet混合架构，该架构将多尺度CNN特征与预训练的MAE嵌入融合。在数据受限条件下，该模型在头骨剥离和多分类解剖学分割方面均优于其他强大基线模型。通过广泛的定量和定性评估，我们的框架展示了高效性、稳定性和可扩展性，表明其适合低资源临床环境和更广泛的神经影像学应用。', 'title_zh': '预训练MRI变换器的少量样本部署在脑成像任务中'}
{'arxiv_id': 'arXiv:2508.05669', 'title': 'Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports', 'authors': 'Jin Khye Tan, En Jun Choong, Ethan Jeremiah Chitty, Yan Pheng Choo, John Hsin Yang Wong, Chern Eu Cheah', 'link': 'https://arxiv.org/abs/2508.05669', 'abstract': "Accurately extracting and representing the structure of tabular data from financial documents remains a critical challenge in document understanding, particularly for regulatory and analytical use cases. This study addresses the complexity of converting financial tables from Malaysian audited financial reports into Markdown format, a task complicated by rotated layouts, multi-level headers, and implicit structural cues. We propose a fine-tuned vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for high-fidelity Markdown generation from document images. Our approach includes a curated dataset of 2,152 image-text pairs with augmentations and a supervised fine-tuning strategy using LoRA. To assess performance, we evaluated our model on 100 out-of-sample tables using a dual framework: a criteria-based LLM-as-a-judge for fine-grained accuracy and our novel Markdown Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based assessment and a 96.53% Markdown TEDS score. This performance significantly surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized reasoning-enabled models. Compared to these self-hosted alternatives, it also significantly reduces inference time. Furthermore, its accuracy exceeds that of widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash. These results demonstrate that domain-specific fine-tuning provides an effective and efficient method to bridge the gap between unstructured financial documents and downstream automation, rivalling much larger and more general models without their computational overhead.", 'abstract_zh': '准确从财务文件中提取和表示表格数据的结构仍然是文档理解中的一个关键挑战，尤其是在监管和分析应用中。本研究致力于解决将马来西亚审计财务报告中的财务表格转换为Markdown格式的复杂性，该任务受到旋转布局、多级表头和隐含结构线索的困扰。我们提出了一种基于Qwen2.5-VL-7B微调的视觉语言模型（VLM），该模型优化了从文档图像生成高质量Markdown的功能。我们的方法包括一个包含2152个图像-文本对的自定义数据集，以及使用LoRA的监督微调策略。为了评估性能，我们使用基于标准的LLM-as-a-judge双框架评估了100个外部样本表格，以及我们新颖的Markdown Tree-Edit-Distance-based Similarity (TEDS) 全局结构保真度度量标准。我们的模型在基于标准的评估中达到了92.20%的整体准确率，在Markdown TEDS评分中达到了96.53%。这一性能显著优于其基模型Qwen2.5-VL-7B、更大规模的VLM和专业的推理增强模型。与这些托管在本地的替代模型相比，它还显著减少了推理时间。此外，其准确率超过了广泛使用的专有模型如OpenAI的GPT-4o和Gemini 2.5 Flash。这些结果表明，针对特定领域的微调提供了一种有效且经济的方法，可以弥合非结构化财务文件和下游自动化之间的差距，而且这与更大且更通用的模型相比，没有其计算开销。', 'title_zh': 'Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports\r\n精调视觉-语言模型以实现马来西亚审计财务报表中财务表格的Markdown转换'}
