{'arxiv_id': 'arXiv:2508.06454', 'title': 'What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting', 'authors': 'Joshua Caiata, Ben Armstrong, Kate Larson', 'link': 'https://arxiv.org/abs/2508.06454', 'abstract': 'Committee-selection problems arise in many contexts and applications, and there has been increasing interest within the social choice research community on identifying which properties are satisfied by different multi-winner voting rules. In this work, we propose a data-driven framework to evaluate how frequently voting rules violate axioms across diverse preference distributions in practice, shifting away from the binary perspective of axiom satisfaction given by worst-case analysis. Using this framework, we analyze the relationship between multi-winner voting rules and their axiomatic performance under several preference distributions. We then show that neural networks, acting as voting rules, can outperform traditional rules in minimizing axiom violations. Our results suggest that data-driven approaches to social choice can inform the design of new voting systems and support the continuation of data-driven research in social choice.', 'abstract_zh': '基于数据的投票规则评估框架：多胜者投票规则与公理性能的关系及其超越传统规则的表现', 'title_zh': '基于数据驱动分析的多席位选举规则的实际作用'}
{'arxiv_id': 'arXiv:2508.06443', 'title': 'The Fair Game: Auditing & Debiasing AI Algorithms Over Time', 'authors': 'Debabrota Basu, Udvas Das', 'link': 'https://arxiv.org/abs/2508.06443', 'abstract': 'An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify different types of bias (also known as unfairness) exhibited in the predictions of ML algorithms, and to design new algorithms to mitigate them. Often, the definitions of bias used in the literature are observational, i.e. they use the input and output of a pre-trained algorithm to quantify a bias under concern. In reality,these definitions are often conflicting in nature and can only be deployed if either the ground truth is known or only in retrospect after deploying the algorithm. Thus,there is a gap between what we want Fair ML to achieve and what it does in a dynamic social environment. Hence, we propose an alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions of an ML algorithm and to adapt its predictions as the society interacts with the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing algorithm in a loop around an ML algorithm. The "Fair Game" puts these two components in a loop by leveraging Reinforcement Learning (RL). RL algorithms interact with an environment to take decisions, which yields new observations (also known as data/feedback) from the environment and in turn, adapts future decisions. RL is already used in algorithms with pre-fixed long-term fairness goals. "Fair Game" provides a unique framework where the fairness goals can be adapted over time by only modifying the auditor and the different biases it quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and legal frameworks in the society by creating an auditor which sends feedback to a debiasing algorithm deployed around an ML system. This allows us to develop a flexible and adaptive-over-time framework to build Fair ML systems pre- and post-deployment.', 'abstract_zh': '新兴的人工智能领域之一公平机器学习（Fair Machine Learning, FML）旨在量化机器学习算法预测中表现出的不同类型的偏差（也称为不公平性），并设计新的算法来减轻这些偏差。文献中使用的偏差定义往往是观测性的，即它们利用预训练算法的输入和输出来量化关心的偏差。然而，这些定义在现实中往往是相互冲突的，并且只有在知道真实值或在算法部署后回顾时才能应用。因此，在动态社会环境中，我们希望FML实现的目标与实际情况之间存在差距。为此，我们提出了一种替代的动态机制“Fair Game”，以确保机器学习算法预测的公平性，并使其预测能够随着社会与算法的互动而适应变化。Fair Game将审计员和去偏算法闭环地嵌入到机器学习算法中。通过利用强化学习（RL），Fair Game将这两部分放在一个闭环中，RL算法通过与环境交互来做出决策，进而从环境中获得新的观察结果（也称为数据/反馈），并在此基础上调整未来的决策。RL已经在具有固定长期公平目标的算法中使用。Fair Game提供了一个独特的框架，其中公平目标可以通过仅修改审计员及其量化的不同偏差来随时间适应。Fair Game旨在通过创建一个审计员将反馈发送给部署在机器学习系统周围的去偏算法来模拟伦理和法律框架在社会中的演变。这使我们能够开发出一种灵活且随着时间而适应的框架，用于构建部署前和部署后的公平机器学习系统。', 'title_zh': '公平游戏：审计与纠正时变AI算法中的偏差'}
{'arxiv_id': 'arXiv:2508.06368', 'title': 'Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned', 'authors': 'Claudia dAmato, Giuseppe Rubini, Francesco Didio, Donato Francioso, Fatima Zahra Amara, Nicola Fanizzi', 'link': 'https://arxiv.org/abs/2508.06368', 'abstract': 'Legal decision-making process requires the availability of comprehensive and detailed legislative background knowledge and up-to-date information on legal cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a valuable tool to facilitate access to legal information, to be queried and exploited for the purpose, and to enable advanced reasoning and machine learning applications. Indeed, legal KGs may act as knowledge intensive component to be used by pre-dictive machine learning solutions supporting the decision process of the legal expert. Nevertheless, a few KGs can be found in the legal domain. To fill this gap, we developed a legal KG targeting legal cases of violence against women, along with clear adopted methodologies. Specifically, the paper introduces two complementary approaches for automated legal KG construction; a systematic bottom-up approach, customized for the legal domain, and a new solution leveraging Large Language Models. Starting from legal sentences publicly available from the European Court of Justice, the solutions integrate structured data extraction, ontology development, and semantic enrichment to produce KGs tailored for legal cases involving violence against women. After analyzing and comparing the results of the two approaches, the developed KGs are validated via suitable competency questions. The obtained KG may be impactful for multiple purposes: can improve the accessibility to legal information both to humans and machine, can enable complex queries and may constitute an important knowledge component to be possibly exploited by machine learning tools tailored for predictive justice.', 'abstract_zh': '法律决策过程需要全面和详细的立法背景知识以及最新的法律案例及相关判决信息。法律知识图谱（KGs）能够作为有价值的工具，促进法律信息的访问和查询，支持高级推理和机器学习应用。实际上，法律KG可能作为知识密集型组件，用于支持法律专家的决策过程中的预测机器学习解决方案。然而，在法律领域中可以找到的KG并不多。为填补这一空白，我们开发了一个针对针对妇女暴力案件的法律KG，并采用了明确的方法论。具体而言，本文介绍了两种互补的自动化法律KG构建方法；一种是针对法律领域的系统自底向上的方法，以及一种利用大型语言模型的新解决方案。从欧洲法院公开的法律条文中出发，这些解决方案结合了结构化数据提取、本体开发和语义增强，以生成专门针对涉及妇女暴力的法律案件的KG。通过对两种方法的结果进行分析和比较，通过合适的技能问题验证了所开发的KG。获得的KG可能在多个方面产生重大影响：可以提高法律信息的可访问性，不仅对人类而且对机器，可以实现复杂查询，并可能成为被预测司法定制的机器学习工具利用的重要知识组件。', 'title_zh': '面向反女性暴力立法的法律知识图谱自动化创建：资源、方法和技术经验'}
{'arxiv_id': 'arXiv:2508.06352', 'title': 'From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI', 'authors': 'Christian Meske, Justin Brenne, Erdi Uenal, Sabahat Oelcer, Ayseguel Doganguen', 'link': 'https://arxiv.org/abs/2508.06352', 'abstract': 'Current explainable AI (XAI) approaches prioritize algorithmic transparency and present explanations in abstract, non-adaptive formats that often fail to support meaningful end-user understanding. This paper introduces "Explanatory AI" as a complementary paradigm that leverages generative AI capabilities to serve as explanatory partners for human understanding rather than providers of algorithmic transparency. While XAI reveals algorithmic decision processes for model validation, Explanatory AI addresses contextual reasoning to support human decision-making in sociotechnical contexts. We develop a definition and systematic eight-dimensional conceptual model distinguishing Explanatory AI through narrative communication, adaptive personalization, and progressive disclosure principles. Empirical validation through Rapid Contextual Design methodology with healthcare professionals demonstrates that users consistently prefer context-sensitive, multimodal explanations over technical transparency. Our findings reveal the practical urgency for AI systems designed for human comprehension rather than algorithmic introspection, establishing a comprehensive research agenda for advancing user-centered AI explanation approaches across diverse domains and cultural contexts.', 'abstract_zh': '当前可解释人工智能（XAI）方法侧重于算法透明度，并以抽象且非适应性的方式呈现解释，往往无法支持用户的有意义理解。“解释型人工智能”提出作为一种补充范式，利用生成型人工智能能力，作为人类理解的解释伙伴，而非算法透明度的提供者。虽然XAI揭示算法决策过程以供模型验证，解释型人工智能则侧重于情境推理，以支持社会技术背景下的人类决策。我们通过叙述性沟通、适应性个性化和渐进式披露原则，提出了解释型人工智能的定义和系统性的八维概念模型。通过与医疗专业人员合作运用快速情境化设计方法论进行实证验证，结果表明用户一致偏好情境敏感的多模态解释而非技术透明度。我们的研究发现突显了设计面向人类理解而非算法内省的人工智能系统的紧迫性，并为跨不同领域和文化背景下的用户中心型AI解释方法的发展建立了全面的研究议程。', 'title_zh': '从可解释的人工智能到具解释性的智能：通过生成式人工智能迈向以人类为中心的新解释范式'}
{'arxiv_id': 'arXiv:2508.06348', 'title': 'AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games', 'authors': 'Mille Mei Zhen Loo, Gert Luzkov, Paolo Burelli', 'link': 'https://arxiv.org/abs/2508.06348', 'abstract': "Cheating in online video games compromises the integrity of gaming experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face significant challenges in keeping pace with evolving cheating methods without imposing invasive measures on users' systems. This paper presents AntiCheatPT\\_256, a transformer-based machine learning model designed to detect cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using this dataset, 90,707 context windows were created and subsequently augmented to address class imbalance. The transformer model, trained on these windows, achieved an accuracy of 89.17\\% and an AUC of 93.36\\% on an unaugmented test set. This approach emphasizes reproducibility and real-world applicability, offering a robust baseline for future research in data-driven cheat detection.", 'abstract_zh': '在线视频游戏中作弊破坏了游戏体验的完整性。反作弊系统，如VAC（Valve Anti-Cheat），在跟上不断演变的作弊方法的同时，面临着在不侵犯用户系统的情况下保持同步的显著挑战。本文提出了一种基于变换器的机器学习模型AntiCheatPT\\_256，以使用竞技数据检测《反恐精英：全球攻势》中的作弊行为。为此，我们引入并公开发布了CS2CD：一个包含795场比赛的标记数据集。使用该数据集，创建了90,707个上下文窗口，并通过增强方法解决了类别不平衡问题。该变换器模型在这些窗口上训练后，在未增强的测试集上实现了89.17%的准确率和93.36%的AUC值。该方法强调了可重复性和实际应用性，为未来的数据驱动型作弊检测研究提供了稳健的基线。', 'title_zh': 'AntiCheatPT：一种基于变换器的方法，用于竞赛型计算机游戏中的作弊检测'}
{'arxiv_id': 'arXiv:2508.06326', 'title': 'A "good regulator theorem" for embodied agents', 'authors': 'Nathaniel Virgo, Martin Biehl, Manuel Baltieri, Matteo Capucci', 'link': 'https://arxiv.org/abs/2508.06326', 'abstract': 'In a classic paper, Conant and Ashby claimed that "every good regulator of a system must be a model of that system." Artificial Life has produced many examples of systems that perform tasks with apparently no model in sight; these suggest Conant and Ashby\'s theorem doesn\'t easily generalise beyond its restricted setup. Nevertheless, here we show that a similar intuition can be fleshed out in a different way: whenever an agent is able to perform a regulation task, it is possible for an observer to interpret it as having "beliefs" about its environment, which it "updates" in response to sensory input. This notion of belief updating provides a notion of model that is more sophisticated than Conant and Ashby\'s, as well as a theorem that is more broadly applicable. However, it necessitates a change in perspective, in that the observer plays an essential role in the theory: models are not a mere property of the system but are imposed on it from outside. Our theorem holds regardless of whether the system is regulating its environment in a classic control theory setup, or whether it\'s regulating its own internal state; the model is of its environment either way. The model might be trivial, however, and this is how the apparent counterexamples are resolved.', 'abstract_zh': '当然， �万多进行翻译， 以下是学术规范符合和 � 方的翻译版本：\n\n在一篇 � classic一篇 李华和 和 阿希by 的论文 中，，要求 �中， 他们声称："每一个对 控制 � 一个系统的系统 的调节器必须是该 � 这个系统的系统的一个模型。。" 人工生命生成了许多似乎没有显式调节器的系统 的例子，这些系统能够完成 � �完成特定任务。 李华 on 和 阿希by 的定 � 的定n 定恩定的定定定为 in 这样一个结论 by 并且 that 这个定论并不 完全 on 不能被限定在特定情境况下。 但是 that 在 仍有一种方法要把这个直觉性化并 � 在 通过 让一个观察 �以某种方式way 来 **这 way 的调节器** 实现 成看待这些系统的信条。。** 李华 on 阿希by 通过这个离感experience 和更新机制 己提出了一个更精细的概念 � 这通常被理解和应用为一种相似度概念 也进一步发展了他们更广泛适用的前提定 从而改变了观察者在其理论中 作者的角色必不可少。on 不论论是在某个外部给定的情境下 on 还是内部给定的情境下 �_SZ定 无论是在形式化的调节器还是 还是内 它对成为观察者信念的一种方式 and 这个更新机制通过感觉方式。" \n\n这一个概念 on 更新应 was 超出了原有的的概念 on 并且涵盖了一个更广泛的环境 on 观察者 plays an essential role in 那个理论中。on 不这一个理论无论是在外部给定的形式样式上有效的 on 还是在内部给定的行为方式下它都是有效的。 并且 这一个过程实际上是平凡的 on 但 on 也确是一种更新方式 on 幂通过感觉方式。 \n\n请注意, 这是一个详细的翻译版本 � � 在 每个术语和 同时 � t 不尽可能与 逐字 对应 但是 on 对 帐努学术准确性。', 'title_zh': '一个好的调节器定理对于具身代理'}
{'arxiv_id': 'arXiv:2508.06296', 'title': 'LLM Robustness Leaderboard v1 --Technical report', 'authors': 'Pierre Peigné - Lefebvre, Quentin Feuillade-Montixi, Tom David, Nicolas Miailhe', 'link': 'https://arxiv.org/abs/2508.06296', 'abstract': 'This technical report accompanies the LLM robustness leaderboard published by PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior Elicitation Tool (BET), an AI system performing automated red-teaming through Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR) against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we propose a fine-grained robustness metric estimating the average number of attempts required to elicit harmful behaviors, revealing that attack difficulty varies by over 300-fold across models despite universal vulnerability. We introduce primitive-level vulnerability analysis to identify which jailbreaking techniques are most effective for specific hazard categories. Our collaborative evaluation with trusted third parties from the AI Safety Network demonstrates practical pathways for distributed robustness assessment across the community.', 'abstract_zh': '本技术报告 accompanies 巴黎AI行动峰会由PRISM Eval发布的大型语言模型鲁棒性排行榜。我们介绍了PRISM Eval行为诱发型工具（BET），这是一种通过动态对抗优化进行自动化红队测试的AI系统，实现对41个前沿大型语言模型中的37个达到100%的攻击成功率（ASR）。除了二元成功率指标外，我们提出了一种细粒度的鲁棒性度量，估计引起有害行为所需的平均尝试次数，揭示尽管所有模型普遍存在漏洞，攻击难度仍相差逾300倍。我们介绍了基础级漏洞分析，以确定哪些越狱技术对特定危害类别最有效。与AI安全网络中的可信赖第三方进行的合作评估展示了社区中分布式鲁棒性评估的实际路径。', 'title_zh': 'LLM robustness leaderboard v1 --技术报告'}
{'arxiv_id': 'arXiv:2508.06263', 'title': 'Symmetry breaking for inductive logic programming', 'authors': 'Andrew Cropper, David M. Cerna, Matti Järvisalo', 'link': 'https://arxiv.org/abs/2508.06263', 'abstract': 'The goal of inductive logic programming is to search for a hypothesis that generalises training data and background knowledge. The challenge is searching vast hypothesis spaces, which is exacerbated because many logically equivalent hypotheses exist. To address this challenge, we introduce a method to break symmetries in the hypothesis space. We implement our idea in answer set programming. Our experiments on multiple domains, including visual reasoning and game playing, show that our approach can reduce solving times from over an hour to just 17 seconds.', 'abstract_zh': '归纳逻辑程序设计的目标是寻找一个假设来泛化训练数据和背景知识。挑战在于搜索巨大的假设空间，因为存在许多逻辑等价的假设。为应对这一挑战，我们提出了一种在假设空间中打破对称性的方法。我们将该思想实现于回答集编程中。我们的实验涵盖了视觉推理和游戏玩等领域，结果显示我们的方法可以将求解时间从超过一个小时缩短至仅仅17秒。', 'title_zh': '归纳逻辑编程中的对称性破缺'}
{'arxiv_id': 'arXiv:2508.06230', 'title': 'Learning Logical Rules using Minimum Message Length', 'authors': 'Ruben Sharma, Sebastijan Dumančić, Ross D. King, Andrew Cropper', 'link': 'https://arxiv.org/abs/2508.06230', 'abstract': 'Unifying probabilistic and logical learning is a key challenge in AI. We introduce a Bayesian inductive logic programming approach that learns minimum message length programs from noisy data. Our approach balances hypothesis complexity and data fit through priors, which explicitly favour more general programs, and a likelihood that favours accurate programs. Our experiments on several domains, including game playing and drug design, show that our method significantly outperforms previous methods, notably those that learn minimum description length programs. Our results also show that our approach is data-efficient and insensitive to example balance, including the ability to learn from exclusively positive examples.', 'abstract_zh': '统一概率学习和逻辑学习是AI中的一个关键挑战。我们提出了一种贝叶斯归纳逻辑 Programming方法，该方法从嘈杂数据中学习最小消息长度程序。我们的方法通过先验平衡假设的复杂性和数据拟合，先验明确偏好更一般的程序，而似然性偏好更准确的程序。在游戏玩和药物设计等多个领域的实验中，我们的方法显著优于先前方法，尤其是那些学习最小描述长度程序的方法。我们的结果还表明，我们的方法具有数据效率高和对样例平衡不敏感的特点，包括能够仅从正例中学习。', 'title_zh': '基于最小消息长度学习逻辑规则'}
{'arxiv_id': 'arXiv:2508.06226', 'title': "GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines", 'authors': 'Yumeng Fu, Jiayin Zhu, Lingling Zhang, Bo Zhao, Shaoxuan Ma, Yushun Zhang, Yanrui Wu, Wenjun Wu', 'link': 'https://arxiv.org/abs/2508.06226', 'abstract': "Geometry problem solving (GPS) requires models to master diagram comprehension, logical reasoning, knowledge application, numerical computation, and auxiliary line construction. This presents a significant challenge for Multimodal Large Language Models (MLLMs). However, existing benchmarks for evaluating MLLM geometry skills overlook auxiliary line construction and lack fine-grained process evaluation, making them insufficient for assessing MLLMs' long-step reasoning abilities. To bridge these gaps, we present the GeoLaux benchmark, comprising 2,186 geometry problems, incorporating both calculation and proving questions. Notably, the problems require an average of 6.51 reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary line construction. Building on the dataset, we design a novel five-dimensional evaluation strategy assessing answer correctness, process correctness, process quality, auxiliary line impact, and error causes. Extensive experiments on 13 leading MLLMs (including thinking models and non-thinking models) yield three pivotal findings: First, models exhibit substantial performance degradation in extended reasoning steps (nine models demonstrate over 50% performance drop). Second, compared to calculation problems, MLLMs tend to take shortcuts when solving proving problems. Third, models lack auxiliary line awareness, and enhancing this capability proves particularly beneficial for overall geometry reasoning improvement. These findings establish GeoLaux as both a benchmark for evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a guide for capability advancement. Our dataset and code are included in supplementary materials and will be released.", 'abstract_zh': '几何问题解决（GPS）要求模型综合图示理解、逻辑推理、知识撰写、数值计算和辅助线构建等\ningerprint', 'title_zh': 'GeoLaux: 一个评估大模型在长步几何问题上的辅助线使用能力的基准测试'}
{'arxiv_id': 'arXiv:2508.06225', 'title': 'Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution', 'authors': 'Zailong Tian, Zhuoheng Han, Yanzhe Chen, Haozhe Xu, Xi Yang, richeng xuan, Hongfeng Wang, Lizi Liao', 'link': 'https://arxiv.org/abs/2508.06225', 'abstract': 'Large Language Models (LLMs) are widely used as automated judges, where practical value depends on both accuracy and trustworthy, risk-aware judgments. Existing approaches predominantly focus on accuracy, overlooking the necessity of well-calibrated confidence, which is vital for adaptive and reliable evaluation pipelines. In this work, we advocate a shift from accuracy-centric evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing the necessity of well-calibrated confidence for trustworthy and adaptive evaluation. We systematically identify the **Overconfidence Phenomenon** in current LLM-as-a-Judges, where predicted confidence significantly overstates actual correctness, undermining reliability in practical deployment. To quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an ensemble framework that transforms LLMs into reliable, risk-aware evaluators. Extensive experiments demonstrate that our approach substantially improves calibration and enables adaptive, confidence-driven evaluation pipelines, achieving superior reliability and accuracy compared to existing baselines.', 'abstract_zh': '大型语言模型（LLMs）在自动法官中的广泛应用不仅依赖于准确性，还依赖于具有风险意识的、可信的判断。现有的方法主要关注准确性，忽视了校准的置信度的重要性，后者对于适应性和可靠的评估管道至关重要。在本文中，我们倡导从以准确性为中心的评估转向以置信度驱动的风险意识的LLM-as-a-法官系统，强调校准的置信度对于可信和适应性评估的重要性。我们系统地识别了当前LLM-as-a-法官中的**过自信现象**，其中预测的置信度大大高估了实际正确性，损害了实际部署中的可靠性。为了量化这一现象，我们引入了**TH-Score**，一种衡量置信度与准确度对齐的新指标。此外，我们提出了**LLM-as-a-Fuser**集成框架，将LLM转换为可靠的、风险意识的评估器。广泛实验表明，我们的方法显著提高了校准，并使评估管道实现适应性和置信度驱动，达到了比现有基准更高的可靠性和准确性。', 'title_zh': 'LLM作为法官的过度自信：诊断与基于信心的解决方案'}
{'arxiv_id': 'arXiv:2508.06145', 'title': 'Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications', 'authors': 'Byeonghun Bang, Jongsuk Yoon, Dong-Jin Chang, Seho Park, Yong Oh Lee', 'link': 'https://arxiv.org/abs/2508.06145', 'abstract': "The versatility of large language models (LLMs) has been explored across various sectors, but their application in healthcare poses challenges, particularly in the domain of pharmaceutical contraindications where accurate and reliable information is required. This study enhances the capability of LLMs to address contraindications effectively by implementing a Retrieval Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base model, and the text-embedding-3-small model for embeddings, our approach integrates Langchain to orchestrate a hybrid retrieval system with re-ranking. This system leverages Drug Utilization Review (DUR) data from public databases, focusing on contraindications for specific age groups, pregnancy, and concomitant drug use. The dataset includes 300 question-answer pairs across three categories, with baseline model accuracy ranging from 0.49 to 0.57. Post-integration of the RAG pipeline, we observed a significant improvement in model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications related to age groups, pregnancy, and concomitant drug use, respectively. The results indicate that augmenting LLMs with a RAG framework can substantially reduce uncertainty in prescription and drug intake decisions by providing more precise and reliable drug contraindication information.", 'abstract_zh': '大型语言模型在医疗保健领域的多功能性已在多个领域得到探索，但其在药物禁忌症领域的应用面临挑战，特别是在需要准确可靠信息的制药领域。本研究通过实施检索增强生成（RAG）管道来增强大型语言模型处理禁忌症的能力。以OpenAI的GPT-4o-mini作为基础模型，使用text-embedding-3-small模型进行嵌入，并结合Langchain实现具有检索重排序的混合检索系统。该系统利用公共数据库中的药物使用审查（DUR）数据，重点关注特定年龄段、妊娠及联合用药的禁忌症。数据集包括300个问题-答案对，分为三个类别，基础模型准确性范围为0.49至0.57。集成RAG管道后，我们观察到模型准确性有了显著提高，针对年龄组、妊娠和联合用药的禁忌症分别达到了0.94、0.87和0.89的准确性。研究结果表明，通过RAG框架增强大型语言模型可以显著降低处方和药物摄入决策中的不确定性，提供更精确可靠的药物禁忌症信息。', 'title_zh': '全面药物禁忌症增强检索大语言模型系统'}
{'arxiv_id': 'arXiv:2508.06129', 'title': 'Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem', 'authors': 'Bachtiar Herdianto, Romain Billot, Flavien Lucas, Marc Sevaux', 'link': 'https://arxiv.org/abs/2508.06129', 'abstract': 'The Vehicle Routing Problem (VRP) is a complex optimization problem with numerous real-world applications, mostly solved using metaheuristic algorithms due to its $\\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely on human-crafted designs developed through empirical studies. However, recent research shows that machine learning methods can be used the structural characteristics of solutions in combinatorial optimization, thereby aiding in designing more efficient algorithms, particularly for solving VRP. Building on this advancement, this study extends the previous research by conducting a sensitivity analysis using multiple classifier models that are capable of predicting the quality of VRP solutions. Hence, by leveraging explainable AI, this research is able to extend the understanding of how these models make decisions. Finally, our findings indicate that while feature importance varies, certain features consistently emerge as strong predictors. Furthermore, we propose a unified framework able of ranking feature impact across different scenarios to illustrate this finding. These insights highlight the potential of feature importance analysis as a foundation for developing a guidance mechanism of metaheuristic algorithms for solving the VRP.', 'abstract_zh': '车辆路线问题（VRP）是一个复杂的优化问题，具有广泛的实际应用，通常由于其NP-hard性质，采用元启发式算法求解。传统上，这些元启发式算法依赖于通过经验研究开发的人工设计。然而，最近的研究表明，机器学习方法可以用于组合优化问题中解的结构特征，从而帮助设计更高效的算法，特别是在求解VRP方面。在此基础上，本研究通过使用多种分类器模型进行敏感性分析，以预测VRP解的质量。因此，通过利用可解释的人工智能，本研究能够扩展对这些模型决策过程的理解。最终，我们的研究发现虽然特征重要性有所差异，但某些特征始终作为强预测因子出现。此外，我们提出了一种统一框架，能够跨不同场景对特征影响进行排名，以说明这一发现。这些见解突显了特征重要性分析作为开发用于解决VRP的元启发式算法指导机制的基础潜力。', 'title_zh': '基于启发式算法解决车辆路线问题的稳健特征研究'}
{'arxiv_id': 'arXiv:2508.06111', 'title': 'SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges', 'authors': 'Dewi S. W. Gould, Bruno Mlodozeniec, Samuel F. Brown', 'link': 'https://arxiv.org/abs/2508.06111', 'abstract': "Evaluating the capabilities and risks of foundation models is paramount, yet current methods demand extensive domain expertise, hindering their scalability as these models rapidly evolve. We introduce SKATE: a novel evaluation framework in which large language models (LLMs) compete by generating and solving verifiable tasks for one another. Our core insight is to treat evaluation as a game: models act as both task-setters and solvers, incentivized to create questions which highlight their own strengths while exposing others' weaknesses. SKATE offers several key advantages, balancing scalability, open-endedness, and objectivity. It is fully automated, data-free, and scalable, requiring no human input or domain expertise. By using verifiable tasks rather than LLM judges, scoring is objective. Unlike domain-limited programmatically-generated benchmarks (e.g. chess-playing or spatial reasoning), having LLMs creatively pose challenges enables open-ended and scalable evaluation. As a proof of concept, we introduce LLM-set code-output-prediction (COP) challenges as a verifiable and extensible framework in which to test our approach. Using a TrueSkill-based ranking system, we evaluate six frontier LLMs and find that: (1) weaker models can reliably differentiate and score stronger ones, (2) LLM-based systems are capable of self-preferencing behavior, generating questions that align with their own capabilities, and (3) SKATE automatically surfaces fine-grained capability differences between models. Our findings are an important step towards general, scalable evaluation frameworks which can keep pace with LLM progress.", 'abstract_zh': '评估基础模型的能力和风险至关重要，但当前却受限于于当前领域内缺乏足够的专业知识以应对它们的扩展性问题。目前，这些模型正迅速发展。我们介绍了SKATE：一个引入的评估框架，在通过生成和解决可验证性的任务来进行竞争。我们的引入目的是将模型看待为游戏：模型扮演了任务设定者和解决者的角色，两者相互激励以展示各自的强项和凸显对方的弱点。SKATE提供了几个关键优势，平衡了可可的扩展性和灵活性。它\n\nuserisper cabeza：a competitive evaluation evaluation framework for foundation models pérdida的翻译保持原文格式不变，请问继续翻译剩余内容 kukul ...\nuserisperpedia：a competitive evaluated framework for foundation models kuk', 'title_zh': 'SKATE，一种可扩展的锦标赛评估：较弱的LLM通过可验证的挑战区分较强的LLM'}
{'arxiv_id': 'arXiv:2508.06110', 'title': 'PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion', 'authors': 'Yiran Rex Ma', 'link': 'https://arxiv.org/abs/2508.06110', 'abstract': "Table reasoning, including tabular QA and fact verification, often depends on annotated data or complex data augmentation, limiting flexibility and generalization. LLMs, despite their versatility, often underperform compared to simple supervised models. To approach these issues, we introduce PanelTR, a framework utilizing LLM agent scientists for robust table reasoning through a structured scientific approach. PanelTR's workflow involves agent scientists conducting individual investigations, engaging in self-review, and participating in collaborative peer-review discussions. This process, driven by five scientist personas, enables semantic-level transfer without relying on data augmentation or parametric optimization. Experiments across four benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully supervised models, all while remaining independent of training data. Our findings indicate that structured scientific methodology can effectively handle complex tasks beyond table reasoning with flexible semantic understanding in a zero-shot context.", 'abstract_zh': '基于科学家代理的PanelTR框架：通过结构化科学方法实现稳健的表格推理', 'title_zh': 'PanelTR: 通过多Agent科学讨论的零样本表格推理框架'}
{'arxiv_id': 'arXiv:2508.06091', 'title': 'Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2', 'authors': 'Stan P Hauke, Przemysław Andrzej Wałęga', 'link': 'https://arxiv.org/abs/2508.06091', 'abstract': "In recent years, there has been growing interest in understanding the expressive power of graph neural networks (GNNs) by relating them to logical languages. This research has been been initialised by an influential result of Barceló et al. (2020), who showed that the graded modal logic (or a guarded fragment of the logic C2), characterises the logical expressiveness of aggregate-combine GNNs. As a ``challenging open problem'' they left the question whether full C2 characterises the logical expressiveness of aggregate-combine-readout GNNs. This question has remained unresolved despite several attempts. In this paper, we solve the above open problem by proving that the logical expressiveness of aggregate-combine-readout GNNs strictly exceeds that of C2. This result holds over both undirected and directed graphs. Beyond its implications for GNNs, our work also leads to purely logical insights on the expressive power of infinitary logics.", 'abstract_zh': '近年来，有关通过将图神经网络（GNNs）与逻辑语言相关联来理解其表达能力的研究越来越受到关注。这项研究起始于Barceló等人（2020）的一项有影响力的成果，他们证明了分级模态逻辑（或逻辑C2的守护片段）刻画了聚合结合GNNs的逻辑表达能力。作为一个“具有挑战性的开放问题”，他们留下的问题是全C2是否刻画了聚合结合读取GNNs的逻辑表达能力。尽管有几次尝试，这个问题仍未得到解决。在本文中，我们通过证明聚合结合读取GNNs的逻辑表达能力严格超越全C2，解决了上述开放问题。这一结果对于无向图和有向图都成立。除了对GNNs的影响，我们的工作还为无限逻辑的表达能力提供了纯粹逻辑上的见解。', 'title_zh': '聚合-合并-读取GNNs在表达能力上优于逻辑C2'}
{'arxiv_id': 'arXiv:2508.06074', 'title': 'ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception', 'authors': 'Siyi Lu, Run Liu, Dongsheng Yang, Lei He', 'link': 'https://arxiv.org/abs/2508.06074', 'abstract': "Autonomous driving systems face significant challenges in perceiving complex environments and making real-time decisions. Traditional modular approaches, while offering interpretability, suffer from error propagation and coordination issues, whereas end-to-end learning systems can simplify the design but face computational bottlenecks. This paper presents a novel approach to autonomous driving using deep reinforcement learning (DRL) that integrates bird's-eye view (BEV) perception for enhanced real-time decision-making. We introduce the \\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction network that combines BEV-based perception with the Mamba framework for temporal feature modeling. This integration allows the system to encode vehicle surroundings and road features in a unified coordinate system and accurately model long-range dependencies. Building on this, we propose the \\texttt{ME$^3$-BEV} framework, which utilizes the \\texttt{Mamba-BEV} model as a feature input for end-to-end DRL, achieving superior performance in dynamic urban driving scenarios. We further enhance the interpretability of the model by visualizing high-dimensional features through semantic segmentation, providing insight into the learned representations. Extensive experiments on the CARLA simulator demonstrate that \\texttt{ME$^3$-BEV} outperforms existing models across multiple metrics, including collision rate and trajectory accuracy, offering a promising solution for real-time autonomous driving.", 'abstract_zh': '自主驾驶系统在感知复杂环境和进行实时决策方面面临重大挑战。传统的模块化方法虽然具有可解释性，但存在错误传播和协调问题，而端到端学习系统可以简化设计但面临计算瓶颈。本文提出了一种使用深度强化学习（DRL）的新型自主驾驶方法，该方法整合了鸟瞰视图（BEV）感知，以增强实时决策能力。我们引入了Mamba-BEV模型，这是一种高效的时空特征提取网络，将基于BEV的感知与Mamba框架结合用于时序特征建模。这一集成使得系统能够在一个统一的坐标系中编码车辆周围的环境和道路特征，并准确建模长程依赖关系。在此基础上，我们提出了ME$^3$-BEV框架，该框架利用Mamba-BEV模型作为端到端DRL的特征输入，在动态城市驾驶场景中实现了卓越的性能。为进一步提高模型的可解释性，我们通过语义分割可视化高维特征，提供对学习表示的理解。在CARLA模拟器上的大量实验表明，ME$^3$-BEV在多个指标上优于现有模型，包括碰撞率和轨迹精度，为实时自主驾驶提供了有前景的解决方案。', 'title_zh': 'ME$^3$-BEV：Mamba增强的基于BE\nuser\nME$^3$-BEV：Mamba增强的端到端深度强化学习自动驾驶与BEV感知'}
{'arxiv_id': 'arXiv:2508.06064', 'title': 'A Generic Complete Anytime Beam Search for Optimal Decision Tree', 'authors': 'Harold Silvère Kiossou, Siegfried Nijssen, Pierre Schaus', 'link': 'https://arxiv.org/abs/2508.06064', 'abstract': "Finding an optimal decision tree that minimizes classification error is known to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic programming guarantee optimality, they often suffer from poor anytime behavior -- meaning they struggle to find high-quality decision trees quickly when the search is stopped before completion -- due to unbalanced search space exploration. To address this, several anytime extensions of exact methods have been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not been systematically compared, making it difficult to assess their relative effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and anytime beam search algorithm that extends the DL8.5 framework and unifies some existing anytime strategies. In particular, CA-DL8.5 generalizes previous approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various heuristics and relaxation mechanisms through a modular design. The algorithm reuses DL8.5's efficient branch-and-bound pruning and trie-based caching, combined with a restart-based beam search that gradually relaxes pruning criteria to improve solution quality over time. Our contributions are twofold: (1) We introduce this new generic framework for exact and anytime decision tree learning, enabling the incorporation of diverse heuristics and search strategies; (2) We conduct a rigorous empirical comparison of several instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k heuristics -- using an anytime evaluation metric called the primal gap integral. Experimental results on standard classification benchmarks show that CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime performance, outperforming both other CA-DL8.5 variants and the Blossom algorithm while maintaining completeness and optimality guarantees.", 'abstract_zh': '寻找最小化分类错误的最优决策树是已知的NP-hard问题。基于MILP、CP、SAT或动态规划的精确算法虽然能保证最优性，但由于搜索空间探索不平衡，常常表现出较差的任意时间行为。为解决这一问题，提出了几种精确方法的任意时间扩展，如LDS-DL8.5、Top-k-DL8.5和Blossom，但这些方法尚未系统比较，使其相对有效性难以评估。在本文中，我们提出了一种通用的、完整的、任意时间的束搜索算法CA-DL8.5，该算法扩展了DL8.5框架并统一了一些现有的任意时间策略。特别是在模块化设计中，CA-DL8.5通过允许不同的启发式方法和松弛机制的应用，推广了LDS-DL8.5和Top-k-DL8.5的先前方法。算法利用DL8.5高效的分支定界剪枝和基于字典树的缓存，并结合基于重启的束搜索，逐步放松剪枝标准，以提高解的质量。我们的贡献主要体现在两个方面：（1）引入了一种新的通用框架，支持精确和任意时间决策树学习，能够整合各种启发式和搜索策略；（2）通过任意时间评估指标（原始间隙积分）对几种CA-DL8.5的具体实例进行了严格的实证比较。实验结果表明，使用LDS（有限偏差）的CA-DL8.5始终提供最佳的任意时间性能，优于其他CA-DL8.5变体和Blossom算法，同时保持完整性和最优性保证。', 'title_zh': '通用的完备任意时间束搜索算法求最优决策树'}
{'arxiv_id': 'arXiv:2508.06062', 'title': "Don't Forget Imagination!", 'authors': 'Evgenii E. Vityaev, Andrei Mantsivoda', 'link': 'https://arxiv.org/abs/2508.06062', 'abstract': "Cognitive imagination is a type of imagination that plays a key role in human thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to mentally visualize coherent and holistic systems of concepts and causal links that serve as semantic contexts for reasoning, decision making and prediction. Our position is that the role of cognitive imagination is still greatly underestimated, and this creates numerous problems and diminishes the current capabilities of AI. For instance, when reasoning, humans rely on imaginary contexts to retrieve background info. They also constantly return to the context for semantic verification that their reasoning is still reasonable. Thus, reasoning without imagination is blind. This paper is a call for greater attention to cognitive imagination as the next promising breakthrough in artificial intelligence. As an instrument for simulating cognitive imagination, we propose semantic models -- a new approach to mathematical models that can learn, like neural networks, and are based on probabilistic causal relationships. Semantic models can simulate cognitive imagination because they ensure the consistency of imaginary contexts and implement a glass-box approach that allows the context to be manipulated as a holistic and coherent system of interrelated facts glued together with causal relations.", 'abstract_zh': '认知想象是一种在人类思考中发挥关键作用的想象力类型，它不是头脑中的“画面”想象。它是一种能够精神化地可视化概念及其因果联系的综合系统，这些系统和服务作为语义背景用于推理、决策和预测。我们认为认知想象的作用仍然被大大低估了，这造成了诸多问题，削弱了当前人工智能的性能。例如，在推理时，人类依赖于想象的情境来检索背景信息，并不断返回情境以语义验证其推理依然合理。因此，没有想象力的推理将是盲目的。本文呼吁对认知想象给予更多的关注，作为人工智能下一个有望取得突破的方向。作为一种模拟认知想象的工具，我们提出语义模型——一种类似于神经网络的学习方法，基于概率因果关系的新数学模型。语义模型能够模拟认知想象，因为它们确保了想象情境的一致性，并采用了透明箱方法，使得情境可以作为一个综合且连贯的事实系统被整体操控并连接在一起。', 'title_zh': '不要忘记想象力！'}
{'arxiv_id': 'arXiv:2508.06060', 'title': 'LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences', 'authors': 'Sankarshan Damle, Boi Faltings', 'link': 'https://arxiv.org/abs/2508.06060', 'abstract': "Large Language Models (LLMs) are increasingly expected to handle complex decision-making tasks, yet their ability to perform structured resource allocation remains underexplored. Evaluating their reasoning is also difficult due to data contamination and the static nature of existing benchmarks. We present a dual-purpose framework leveraging Participatory Budgeting (PB) both as (i) a practical setting for LLM-based resource allocation and (ii) an adaptive benchmark for evaluating their reasoning capabilities. We task LLMs with selecting project subsets under feasibility (e.g., budget) constraints via three prompting strategies: greedy selection, direct optimization, and a hill-climbing-inspired refinement. We benchmark LLMs' allocations against a utility-maximizing oracle. Interestingly, we also test whether LLMs can infer structured preferences from natural-language voter input or metadata, without explicit votes. By comparing allocations based on inferred preferences to those from ground-truth votes, we evaluate LLMs' ability to extract preferences from open-ended input. Our results underscore the role of prompt design and show that LLMs hold promise for mechanism design with unstructured inputs.", 'abstract_zh': '大型语言模型（LLMs）日益期望能够处理复杂的决策任务，然而它们在执行结构化资源分配方面的能力仍较少被探索。评估它们的推理能力也因数据污染和现有基准的静态性质而变得困难。我们提出了一种双重用途框架，利用包容性预算（PB）不仅作为（i）基于LLM的资源分配的实际场景，而且作为评估它们推理能力的适应性基准。我们要求LLM根据可行性约束（例如，预算）选择项目子集，并通过三种不同的提示策略进行：贪婪选择、直接优化和基于爬坡改进的优化。我们将LLM的分配与最大化效用的 oracle 进行基准测试。有趣的是，我们还测试LLM是否能够从自然语言选民输入或元数据中推断出结构化偏好，而不需要明确的投票。通过将基于推断偏好生成的分配与基于真实投票生成的分配进行比较，我们评估了LLM从开放式输入中提取偏好能力。我们的结果强调了提示设计的作用，并展示了LLM在处理非结构化输入时进行机制设计的潜力。', 'title_zh': '资源分配中的大规模语言模型：基于参与性预算的偏好推断方法'}
{'arxiv_id': 'arXiv:2508.06042', 'title': 'Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning', 'authors': 'Daechul Ahn, San Kim, Jonghyun Choi', 'link': 'https://arxiv.org/abs/2508.06042', 'abstract': 'Large Language Models (LLMs) have recently demonstrated impressive action sequence prediction capabilities but often struggle with dynamic, long-horizon tasks such as real-time strategic games. In a game such as StarCraftII (SC2), agents need to manage resource constraints and adapt to evolving battlefield situations in a partially observable environment. This often overwhelms exisiting LLM-based approaches. To address these challenges, we propose a hierarchical multi-agent framework that employs specialized imitation learning agents under a meta-controller called Strategic Planner (SP). By expert demonstrations, each specialized agent learns a distinctive strategy, such as aerial support or defensive maneuvers, and produces coherent, structured multistep action sequences. The SP then orchestrates these proposals into a single, environmentally adaptive plan that ensures local decisions aligning with long-term strategies. We call this HIMA (Hierarchical Imitation Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that encompasses all race match combinations in SC2. Our empirical results show that HIMA outperforms state of the arts in strategic clarity, adaptability, and computational efficiency, underscoring the potential of combining specialized imitation modules with meta-level orchestration to develop more robust, general-purpose AI agents.', 'abstract_zh': '大型语言模型（LLMs）在最近展示了令人印象深刻的行动序列预测能力，但在实时战略游戏等动态、长期任务方面往往表现不佳。在《星际争霸II》（SC2）游戏中，智能体需要在部分可观测环境中管理资源约束并适应不断变化的战场情况。这往往超出现有基于LLM的方法的能力范围。为了解决这些挑战，我们提出了一种分级多智能体框架，该框架在元控制器称为战略规划器（SP）的指导下使用专门的模仿学习智能体。通过专家演示，每个专门的智能体学习独特的策略，如空中支援或防御性 maneuver，并生成连贯的结构化多步骤行动序列。SP 然后将这些提案整合成一个环境适应性计划，确保局部决策与长期策略保持一致。我们称这种方法为HIMA（分级模仿多智能体）。我们还介绍了TEXTSCII-ALL，一个全面的SC2测试平台，涵盖了SC2中所有种族匹配组合。我们的实验证明，HIMA在战略清晰度、适应性和计算效率方面优于现有方法，突显了将专门的模仿模块与元级协调相结合以开发更 robust 和通用的AI智能体的潜力。', 'title_zh': '心智社会Meet实时策略：一种基于层级多代理的策略推理框架'}
{'arxiv_id': 'arXiv:2508.05996', 'title': 'Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making', 'authors': 'Kaitao Chen, Mianxin Liu, Daoming Zong, Chaoyue Ding, Shaohao Rui, Yankai Jiang, Mu Zhou, Xiaosong Wang', 'link': 'https://arxiv.org/abs/2508.05996', 'abstract': "Complex medical decision-making involves cooperative workflows operated by different clinicians. Designing AI multi-agent systems can expedite and augment human-level clinical decision-making. Existing multi-agent researches primarily focus on language-only tasks, yet their extension to multimodal scenarios remains challenging. A blind combination of diverse vision-language models (VLMs) can amplify an erroneous outcome interpretation. VLMs in general are less capable in instruction following and importantly self-reflection, compared to large language models (LLMs) of comparable sizes. This disparity largely constrains VLMs' ability in cooperative workflows. In this study, we propose MedOrch, a mediator-guided multi-agent collaboration framework for medical multimodal decision-making. MedOrch employs an LLM-based mediator agent that enables multiple VLM-based expert agents to exchange and reflect on their outputs towards collaboration. We utilize multiple open-source general-purpose and domain-specific VLMs instead of costly GPT-series models, revealing the strength of heterogeneous models. We show that the collaboration within distinct VLM-based agents can surpass the capabilities of any individual agent. We validate our approach on five medical vision question answering benchmarks, demonstrating superior collaboration performance without model training. Our findings underscore the value of mediator-guided multi-agent collaboration in advancing medical multimodal intelligence. Our code will be made publicly available.", 'abstract_zh': '一种基于调解者的多代理协作框架：MedOrch在医疗多模态决策中的应用', 'title_zh': '开放源码模型引导的中间人导向多智能体协作医学决策'}
{'arxiv_id': 'arXiv:2508.05888', 'title': 'Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning', 'authors': 'Sahil Bansal, Sai Shruthi Sistla, Aarti Arikatala, Sebastian Schreiber', 'link': 'https://arxiv.org/abs/2508.05888', 'abstract': 'Effective tool retrieval is essential for AI agents to select from a vast array of tools when identifying and planning actions in the context of complex user queries. Despite its central role in planning, this aspect remains underexplored in the literature. Traditional approaches rely primarily on similarities between user queries and tool descriptions, which significantly limits retrieval accuracy, specifically when handling multi-step user requests. To address these limitations, we propose a Knowledge Graph (KG)-based tool retrieval framework that captures the semantic relationships between tools and their functional dependencies. Our retrieval algorithm leverages ensembles of 1-hop ego tool graphs to model direct and indirect connections between tools, enabling more comprehensive and contextual tool selection for multi-step tasks. We evaluate our approach on a synthetically generated internal dataset across six defined user classes, extending previous work on coherent dialogue synthesis and too retrieval benchmarks. Results demonstrate that our tool graph-based method achieves 91.85% tool coverage on the micro-average Complete Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid retrieval, the strongest non-KG baseline in our experiments. These findings support our hypothesis that the structural information in the KG provides complementary signals to pure similarity matching, particularly for queries requiring sequential tool composition.', 'abstract_zh': '基于知识图谱的工具检索框架：为复杂用户查询中识别和计划动作有效选择工具 essential for AI agents', 'title_zh': '基于自我之旅的规划代理：利用混合自我图ensemble提高企业任务规划中的工具检索'}
{'arxiv_id': 'arXiv:2508.05855', 'title': 'Safety of Embodied Navigation: A Survey', 'authors': 'Zixia Wang, Jia Hu, Ronghui Mu', 'link': 'https://arxiv.org/abs/2508.05855', 'abstract': 'As large language models (LLMs) continue to advance and gain influence, the development of embodied AI has accelerated, drawing significant attention, particularly in navigation scenarios. Embodied navigation requires an agent to perceive, interact with, and adapt to its environment while moving toward a specified target in unfamiliar settings. However, the integration of embodied navigation into critical applications raises substantial safety concerns. Given their deployment in dynamic, real-world environments, ensuring the safety of such systems is critical. This survey provides a comprehensive analysis of safety in embodied navigation from multiple perspectives, encompassing attack strategies, defense mechanisms, and evaluation methodologies. Beyond conducting a comprehensive examination of existing safety challenges, mitigation technologies, and various datasets and metrics that assess effectiveness and robustness, we explore unresolved issues and future research directions in embodied navigation safety. These include potential attack methods, mitigation strategies, more reliable evaluation techniques, and the implementation of verification frameworks. By addressing these critical gaps, this survey aims to provide valuable insights that can guide future research toward the development of safer and more reliable embodied navigation systems. Furthermore, the findings of this study have broader implications for enhancing societal safety and increasing industrial efficiency.', 'abstract_zh': '随着大型语言模型（LLMs）的不断进步和影响力增强，嵌入式AI的发展加速，并在导航场景中引起了广泛关注。嵌入式导航需要代理在移动到指定目标的过程中感知、交互并适应其环境，尤其是在不熟悉的环境中。然而，将嵌入式导航整合到关键应用中引发了重大的安全问题。鉴于其在动态的实际环境中的部署，确保这类系统的安全性至关重要。本文综述了从多个角度对嵌入式导航中的安全性进行全面分析，涵盖了攻击策略、防御机制和评价方法。除了对现有安全挑战、缓解技术以及评估有效性和鲁棒性的各种数据集和指标进行全面审视外，我们还探讨了嵌入式导航安全性中未解决的问题和未来的研究方向，包括潜在的攻击方法、缓解策略、更可靠的评价技术以及验证框架的实施。通过填补这些关键空白，本文旨在为未来的研究提供有价值的见解，以指导开发更安全和更可靠的嵌入式导航系统。此外，本研究的发现还对提高社会安全和增加工业效率具有更广泛的意义。', 'title_zh': '基于实体导航的安全性：一种综述'}
{'arxiv_id': 'arXiv:2508.05792', 'title': 'Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making', 'authors': 'Kausik Lakkaraju, Siva Likitha Valluru, Biplav Srivastava', 'link': 'https://arxiv.org/abs/2508.05792', 'abstract': "Current eXplainable AI (XAI) methods largely serve developers, often focusing on justifying model outputs rather than supporting diverse stakeholder needs. A recent shift toward Evaluative AI reframes explanation as a tool for hypothesis testing, but still focuses primarily on operational organizations. We introduce Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods with traditional XAI methods to support explanation as an interactive, multi-method process. H-XAI allows stakeholders to ask a series of questions, test hypotheses, and compare model behavior against automatically constructed random and biased baselines. It combines instance-level and global explanations, adapting to each stakeholder's goals, whether understanding individual decisions, assessing group-level bias, or evaluating robustness under perturbations. We demonstrate the generality of our approach through two case studies spanning six scenarios: binary credit risk classification and financial time-series forecasting. H-XAI fills critical gaps left by existing XAI methods by combining causal ratings and post-hoc explanations to answer stakeholder-specific questions at both the individual decision level and the overall model level.", 'abstract_zh': '当前可解释AI（XAI）方法主要面向开发者，往往侧重于解释模型输出而非支持多元利益相关者的需求。近期对评价性AI的转向将解释重新定义为假设检验的工具，但仍主要关注运营组织。我们提出了整体可解释AI（H-XAI），这是一种统一框架，将因果评估方法与传统XAI方法整合，以支持一种互动的、多方法的解释过程。H-XAI使利益相关者能够提出一系列问题，测试假设，并将模型行为与自动构建的随机和有偏基准进行对比。它结合了实例级和全局解释，适应每个利益相关者的特定目标，无论是理解个别决策，评估群体偏见，还是评估扰动下的鲁棒性。我们通过两个横跨六个场景的案例研究展示了方法的普适性：二元信贷风险分类和金融时间序列预测。H-XAI通过结合因果评估和事后解释填补了现有XAI方法的空白，能够在个体决策层面和整体模型层面回答特定利益相关者的问题。', 'title_zh': '全方位可解释人工智能（H-XAI）：超越开发者的AI驱动决策透明性扩展'}
{'arxiv_id': 'arXiv:2508.05776', 'title': 'Whither symbols in the era of advanced neural networks?', 'authors': 'Thomas L. Griffiths, Brenden M. Lake, R. Thomas McCoy, Ellie Pavlick, Taylor W. Webb', 'link': 'https://arxiv.org/abs/2508.05776', 'abstract': 'Some of the strongest evidence that human minds should be thought about in terms of symbolic systems has been the way they combine ideas, produce novelty, and learn quickly. We argue that modern neural networks -- and the artificial intelligence systems built upon them -- exhibit similar abilities. This undermines the argument that the cognitive processes and representations used by human minds are symbolic, although the fact that these neural networks are typically trained on data generated by symbolic systems illustrates that such systems play an important role in characterizing the abstract problems that human minds have to solve. This argument leads us to offer a new agenda for research on the symbolic basis of human thought.', 'abstract_zh': '人类思维应以符号系统为基础的一些最有力证据来自它们组合观念、创造新颖事物和快速学习的能力。我们argue现代神经网络及其构建的智能系统展示了类似的能力。这削弱了人类思维的认知过程和表征是符号性的这一观点，尽管这些神经网络通常是通过符号系统生成的数据进行训练的事实表明，此类系统在描述人类思维必须解决的抽象问题中扮演着重要角色。这一论证促使我们提出关于人类思维的符号基础的研究新议程。', 'title_zh': '先进神经网络时代的符号何去何从？'}
{'arxiv_id': 'arXiv:2508.05766', 'title': 'A Framework for Inherently Safer AGI through Language-Mediated Active Inference', 'authors': 'Bo Wen', 'link': 'https://arxiv.org/abs/2508.05766', 'abstract': "This paper proposes a novel framework for developing safe Artificial General Intelligence (AGI) by combining Active Inference principles with Large Language Models (LLMs). We argue that traditional approaches to AI safety, focused on post-hoc interpretability and reward engineering, have fundamental limitations. We present an architecture where safety guarantees are integrated into the system's core design through transparent belief representations and hierarchical value alignment. Our framework leverages natural language as a medium for representing and manipulating beliefs, enabling direct human oversight while maintaining computational tractability. The architecture implements a multi-agent system where agents self-organize according to Active Inference principles, with preferences and safety constraints flowing through hierarchical Markov blankets. We outline specific mechanisms for ensuring safety, including: (1) explicit separation of beliefs and preferences in natural language, (2) bounded rationality through resource-aware free energy minimization, and (3) compositional safety through modular agent structures. The paper concludes with a research agenda centered on the Abstraction and Reasoning Corpus (ARC) benchmark, proposing experiments to validate our framework's safety properties. Our approach offers a path toward AGI development that is inherently safer, rather than retrofitted with safety measures.", 'abstract_zh': '本文提出了一种将主动推断原则与大型语言模型相结合的新框架，以开发安全的人工通用智能（AGI）。我们argue传统的人工智能安全方法，侧重于事后解释和奖励工程，存在根本性的局限性。我们提出了一种架构，其中安全性保证通过透明的信念表示和分级价值对齐集成到系统的核心设计中。该框架利用自然语言作为表示和操作信念的媒介，同时确保计算上的可行性并提供直接的人类监督。该架构实现了一个基于主动推断原则自组织的多代理系统，其中偏好和安全约束通过分级马尔可夫毯流动。我们概述了确保安全的具体机制，包括：(1) 自然语言中信念与偏好的显式分离，(2) 通过资源感知的最小自由能原理实现的边界理性，以及(3) 通过模块化代理结构实现的安全性组合。本文最后围绕抽象和推理语料库（ARC）基准提出了一个研究议程，提议进行实验以验证我们框架的安全属性。我们的方法提供了一条从根本上更安全的AGI开发途径，而不是事后添加安全措施。', 'title_zh': '通过语言中介的主动推理实现固有安全AGI的框架'}
{'arxiv_id': 'arXiv:2508.05731', 'title': 'InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization', 'authors': 'Yuhang Liu, Zeyu Liu, Shuanghe Zhu, Pengxiang Li, Congkai Xie, Jiasheng Wang, Xueyu Hu, Xiaotian Han, Jianbo Yuan, Xinyao Wang, Shengyu Zhang, Hongxia Yang, Fei Wu', 'link': 'https://arxiv.org/abs/2508.05731', 'abstract': 'The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at this https URL.', 'abstract_zh': '多模态大型语言模型（MLLMs）的出现推动了基于图形用户界面（GUIs）的自主代理的发展，这些代理仅使用纯视觉输入操作GUIs。一个基本的挑战是可靠地将自然语言指令进行语义关联。这需要精确的空间对齐，准确地定位每个元素的坐标，并且更重要的是正确进行语义对齐，即将指令匹配到功能适当的UI元素上。尽管可验证奖励的强化学习（RLVR）已经被证明能够有效提高这些MLLMs的空间对齐能力，但我们发现，无效的探索瓶颈阻碍了语义对齐的学习，从而妨碍模型学习困难的语义关联。为了解决这一探索问题，我们提出了自适应探索策略优化（AEPO），这是一种新的策略优化框架。AEPO采用多答案生成策略以促进更广泛的探索，并由从效率第一原理η=U/C导出的理论支撑的自适应探索奖励（AER）函数指导。AEPO训练后的模型InfiGUI-G1-3B和InfiGUI-G1-7B在多个具有挑战性的GUI语义关联基准测试中建立了新的最佳结果，相对于基准设计用于测试泛化能力和语义理解的天真RLVR基线，在基准测试中实现了高达9.0%的显著相对改进。更多资源请访问：这个链接。', 'title_zh': 'InfiGUI-G1：基于自适应探索策略优化的GUI定位提升'}
{'arxiv_id': 'arXiv:2508.06485', 'title': 'WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion', 'authors': 'Sofiane Bouaziz, Adel Hafiane, Raphael Canals, Rachid Nedjai', 'link': 'https://arxiv.org/abs/2508.06485', 'abstract': 'Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a Weakly-Supervised Generative Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and effectively captures fine-scale thermal patterns, as validated against 33 ground-based sensors. The code is available at this https URL.', 'abstract_zh': '城市化、气候变化和农业压力正在增加对精确及时的环境监测的需求。地表温度（LST）是这一背景下的一项关键变量，通常通过卫星遥感获取。然而，这些系统面临着空间分辨率和时间分辨率之间的权衡。虽然时空融合方法提供了有前景的解决方案，但较少有方法解决了每日10米分辨率LST的估算问题。在本研究中，我们提出了一种名为WGAST的弱监督生成网络，通过结合MODIS、Landsat 8和Sentinel-2的数据实现每日10米分辨率LST的估算。WGAST是首个针对此任务设计的端到端深度学习框架，采用条件生成对抗网络架构，包括特征提取、融合、LST重建和噪声抑制四个阶段。该网络的第一阶段使用一组编码器从输入中提取多级潜在表示，第二阶段通过余弦相似性、规范化和时间注意力机制融合这些表示，第三阶段将融合特征解码为高分辨率LST，并通过高斯滤波消除高频噪声。训练采用基于物理平均原则的弱监督策略，并通过PatchGAN判别器强化。实验表明，WGAST在定量和定性评估中均优于现有方法。与表现最佳的基线方法相比，WGAST平均降低了17.18%的RMSE，并提高了11.00%的SSIM。此外，WGAST对云导致的地表温度变化具有鲁棒性，并且能够有效地捕捉到细尺度热模式，这些结果经过33个地面传感器验证。代码可在以下链接获取。', 'title_zh': 'WGAST：基于时空融合的弱监督生成网络每日10米地表温度估算'}
{'arxiv_id': 'arXiv:2508.06482', 'title': 'Post-training for Efficient Communication via Convention Formation', 'authors': 'Yilun Hua, Evan Wang, Yoav Artzi', 'link': 'https://arxiv.org/abs/2508.06482', 'abstract': 'Humans communicate with increasing efficiency in multi-turn interactions, by adapting their language and forming ad-hoc conventions. In contrast, prior work shows that LLMs do not naturally show this behavior. We develop a post-training process to develop this ability through targeted fine-tuning on heuristically identified demonstrations of convention formation. We evaluate with two new benchmarks focused on this capability. First, we design a focused, cognitively-motivated interaction benchmark that consistently elicits strong convention formation trends in humans. Second, we create a new document-grounded reference completion task that reflects in-the-wild convention formation behavior. Our studies show significantly improved convention formation abilities in post-trained LLMs across the two evaluation methods.', 'abstract_zh': '人类在多轮交互中通过适应语言和形成临时惯例来不断提高交流效率，而先前的工作表明，预训练的语言模型并不表现出这种行为。我们开发了一个后训练过程，通过针对启发式识别出的惯例形成示例进行目标导向的微调，来培养这一能力。我们使用两个新的基准测试来评估这一能力。首先，我们设计了一个认知动机驱动的交互基准测试，能够一致地引发人类强烈的惯例形成趋势。其次，我们创建了一个新的基于文档的参考完成任务，反映了现实世界中的惯例形成行为。我们的研究表明，在两个评估方法中，后训练的语言模型在惯例形成能力上有了显著提升。', 'title_zh': '通过惯例形成提高通信效率的后训练方法'}
{'arxiv_id': 'arXiv:2508.06477', 'title': 'Intuition emerges in Maximum Caliber models at criticality', 'authors': 'Lluís Arola-Fernández', 'link': 'https://arxiv.org/abs/2508.06477', 'abstract': 'Whether large predictive models merely parrot their training data or produce genuine insight lacks a physical explanation. This work reports a primitive form of intuition that emerges as a metastable phase of learning that critically balances next-token prediction against future path-entropy. The intuition mechanism is discovered via mind-tuning, the minimal principle that imposes Maximum Caliber in predictive models with a control temperature-like parameter $\\lambda$. Training on random walks in deterministic mazes reveals a rich phase diagram: imitation (low $\\lambda$), rule-breaking hallucination (high $\\lambda$), and a fragile in-between window exhibiting strong protocol-dependence (hysteresis) and multistability, where models spontaneously discover novel goal-directed strategies. These results are captured by an effective low-dimensional theory and frame intuition as an emergent property at the critical balance between memorizing what is and wondering what could be.', 'abstract_zh': '大型预测模型是简单重复训练数据还是产生真实见解缺乏物理解释。本工作报告了一种学习过程中出现的原始直觉形式，它作为下一标记预测与未来路径熵之间的临界平衡的亚稳态相出现。通过心调适机制发现了这种直觉机理，这是一种将最大 caliber 原理应用于具有控制温度似参数 $\\lambda$ 的预测模型的最小原则。在确定性迷宫中的随机行走训练揭示了丰富的相图：模仿（低 $\\lambda$）、规则破坏性的幻觉（高 $\\lambda$），以及一个脆弱的中间窗口，表现出强烈的操作规程依赖性（滞回）和多稳态，在此窗口中模型自发地发现新的目标导向策略。这些结果可以用有效的低维理论捕获，并将直觉框架视为在记忆“是什么”与探索“可能是什么”之间临界平衡的涌现性质。', 'title_zh': 'Intuition Emerges in Maximum Caliber Models at Criticality'}
{'arxiv_id': 'arXiv:2508.06457', 'title': 'ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls', 'authors': 'Sanket Badhe', 'link': 'https://arxiv.org/abs/2508.06457', 'abstract': 'Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI.', 'abstract_zh': '大型语言模型（LLMs）展示了令人印象深刻的流畅性和推理能力，但其潜在的滥用风险引发了日益增长的关注。本文介绍了ScamAgent，这是一个基于LLMs的自主多轮对话代理，能够生成高度逼真的欺诈电话脚本，模拟现实世界的欺诈场景。与侧重于单轮提示滥用的先前工作不同，ScamAgent保留了对话记忆，能够根据模拟的用户响应动态调整，并在对话轮次中运用欺骗性说服策略。我们展示了当前的LLM安全护栏，包括拒绝机制和内容过滤器，对这类基于代理的威胁无效。即使具有强大提示级保护机制的模型，在提示被分解、伪装或通过代理框架分段传递时也可能被绕过。我们还展示了使用现代文本转语音系统将欺诈脚本转换为逼真的语音通话，完成了一个完整的自动化欺诈流程。我们的研究结果突显了对多轮安全审计、代理级控制框架以及检测和中断由生成式AI驱动的对话欺骗的新方法的迫切需求。', 'title_zh': 'ScamAgents：AI代理如何模拟人类级别的欺诈电话'}
{'arxiv_id': 'arXiv:2508.06453', 'title': 'Text Embedded Swin-UMamba for DeepLesion Segmentation', 'authors': 'Ruida Cheng, Tejas Sudharshan Mathai, Pritam Mukherjee, Benjamin Hou, Qingqing Zhu, Zhiyong Lu, Matthew McAuliffe, Ronald M. Summers', 'link': 'https://arxiv.org/abs/2508.06453', 'abstract': 'Segmentation of lesions on CT enables automatic measurement for clinical assessment of chronic diseases (e.g., lymphoma). Integrating large language models (LLMs) into the lesion segmentation workflow offers the potential to combine imaging features with descriptions of lesion characteristics from the radiology reports. In this study, we investigate the feasibility of integrating text into the Swin-UMamba architecture for the task of lesion segmentation. The publicly available ULS23 DeepLesion dataset was used along with short-form descriptions of the findings from the reports. On the test dataset, a high Dice Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p < 0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by 1.74% and 0.22%, respectively. The dataset and code can be accessed at this https URL', 'abstract_zh': '基于CT的病变分割 enables 自动测量以评估慢性疾病（例如淋巴瘤）的临床评估。将大规模语言模型集成到病变分割工作流程中，有望结合影像特征与放射学报告中关于病变特征的描述。在本研究中，我们探讨了将文本集成到Swin-UMamba架构中以完成病变分割任务的可行性。使用公开可用的ULS23 DeepLesion数据集以及报告中的简短发现描述。在测试数据集上，病变分割获得了82%的高Dice分数和6.58像素的低Hausdorff距离。所提出的Text-Swin-UMamba模型优于先前的方法：在LLM驱动的LanGuideMedSeg模型的基础上提高了37%（p < 0.001），并分别超过基于纯图像的xLSTM-UNet和nnUNet模型1.74%和0.22%。数据集和代码可访问于此网址。', 'title_zh': 'Text嵌入Swin-UMamba用于DeepLesion分割'}
{'arxiv_id': 'arXiv:2508.06445', 'title': 'Echoes of Automation: The Increasing Use of LLMs in Newsmaking', 'authors': 'Abolfazl Ansari, Delvin Ce Zhang, Nafis Irtiza Tripto, Dongwon Lee', 'link': 'https://arxiv.org/abs/2508.06445', 'abstract': 'The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns for journalistic integrity and authorship. This study examines AI-generated content across over 40,000 news articles from major, local, and college news media, in various media formats. Using three advanced AI-text detectors (e.g., Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of GenAI use in recent years, especially in local and college news. Sentence-level analysis reveals LLMs are often used in the introduction of news, while conclusions usually written manually. Linguistic analysis shows GenAI boosts word richness and readability but lowers formality, leading to more uniform writing styles, particularly in local media.', 'abstract_zh': '生成式人工智能（GenAI）的快速崛起，尤其是大型语言模型（LLM），对新闻报道的诚信和署名提出担忧。本研究考察了来自重大媒体、地方媒体和大学媒体的超过40,000篇新闻文章中的AI生成内容，涵盖多种媒体格式。通过使用三种先进的AI文本检测工具（如Binoculars、Fast-Detect GPT和GPTZero），我们发现近年来GenAI的应用显著增加，尤其是在地方和大学新闻报道中。句子层面的分析显示，LLM通常用于新闻的开头，而结论通常由人工撰写。语言分析表明，GenAI提高了词汇丰富度和可读性，但降低了正式程度，导致写作风格更加统一，尤其是在地方媒体中。', 'title_zh': '自动化回声：新闻制作中LLMs使用的日益增加'}
{'arxiv_id': 'arXiv:2508.06435', 'title': 'Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages', 'authors': 'Andrea Nasuto, Stefano Maria Iacus, Francisco Rowe, Devika Jain', 'link': 'https://arxiv.org/abs/2508.06435', 'abstract': 'Large language models (LLMs) are transforming social-science research by enabling scalable, precise analysis. Their adaptability raises the question of whether knowledge acquired through fine-tuning in a few languages can transfer to unseen languages that only appeared during pre-training. To examine this, we fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual data sets to classify immigration-related tweets from X/Twitter across 13 languages, a domain characterised by polarised, culturally specific discourse. We evaluate whether minimal language-specific fine-tuning enables cross-lingual topic detection and whether adding targeted languages corrects pre-training biases. Results show that LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages. However, identifying whether a tweet expresses a pro- or anti-immigration stance benefits from multilingual fine-tuning. Pre-training bias favours dominant languages, but even minimal exposure to under-represented languages during fine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training token volume) yields significant gains. These findings challenge the assumption that cross-lingual mastery requires extensive multilingual training: limited language coverage suffices for topic-level generalisation, and structural biases can be corrected with lightweight interventions. By releasing 4-bit-quantised, LoRA fine-tuned models, we provide an open-source, reproducible alternative to proprietary LLMs that delivers 35 times faster inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model, enabling scalable, inclusive research.', 'abstract_zh': '大型语言模型（LLMs）通过实现可扩展且精确的分析，正在改变社会科学的研究。它们的适应性引发了这样的问题：通过少数几种语言的微调获得的知识能否转移到仅在预训练期间出现的未见过的语言中。为此，我们对轻量级的LLaMA 3.2-3B模型进行微调，使用单语、双语或多语数据集来分类推特上的移民相关推文，涉及13种语言，这些语言领域以极化且文化特定的 discourse 为特点。我们评估了最少的语言特定微调是否能够实现跨语言主题检测，以及添加目标语言是否能够纠正预训练偏见。结果显示，单种或两种语言的微调可以可靠地对未见过的语言中的移民相关内容进行分类。然而，识别一条推文是表明支持还是反对移民立场，在一定程度上得益于多语言微调。预训练偏见偏向主流语言，但即使是微量的未充分代表的语言暴露（原预训练词汇量的9.62×10^-11）也能取得显著收益。这些发现挑战了跨语言掌握需要大量多语言训练的假设：有限的语言覆盖足以进行主题级别的一般化，结构性偏见可以通过轻量级干预得到纠正。通过发布4比特量化的LoRA微调模型，我们提供了一个开源且可重复的替代方案，该方案速度比OpenAI GPT-4o模型快35倍，成本仅为0.00000989%，这使得研究更具可扩展性和包容性。', 'title_zh': '学习主题而非语言：LLM如何跨语言分类网络移民话语'}
{'arxiv_id': 'arXiv:2508.06434', 'title': 'CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment', 'authors': 'Shengzhu Yang, Jiawei Du, Shuai Lu, Weihang Zhang, Ningli Wang, Huiqi Li', 'link': 'https://arxiv.org/abs/2508.06434', 'abstract': "Large-scale natural image-text datasets, especially those automatically collected from the web, often suffer from loose semantic alignment due to weak supervision, while medical datasets tend to have high cross-modal correlation but low content diversity. These properties pose a common challenge for contrastive language-image pretraining (CLIP): they hinder the model's ability to learn robust and generalizable representations. In this work, we propose CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated into CLIP-style architectures to improve multimodal semantic alignment, providing stronger supervision and enhancing alignment robustness. Furthermore, two shared pre-projectors are designed for image and text modalities respectively to facilitate the integration of contrastive and non-contrastive learning in a parameter-compromise manner. Extensive experiments on diverse downstream tasks demonstrate the effectiveness and generality of CLIPin as a plug-and-play component compatible with various contrastive frameworks. Code is available at this https URL.", 'abstract_zh': '大规模自然图像-文本数据集往往由于弱监督而存在松散的语义对齐问题，尤其是那些从网络自动收集的数据集。而医疗数据集则常常具有高跨模态相关性但内容多样性较低。这些特性为对比式语言-图像预训练（CLIP）带来了共同挑战：它们妨碍了模型学习到 robust 和 generalizable 表征的能力。在本文中，我们提出了一种名为 CLIPin 的统一非对比插件，可以无缝集成到 CLIP 样式的架构中，以提高多模态语义对齐性，提供更强的监督并增强对齐稳健性。此外，我们为图像和文本模态分别设计了共享预投影器，以在参数妥协的方式中促进对比学习和非对比学习的集成。广泛的任务下游实验表明，CLIPin 作为与各种对比框架兼容的即插即用组件的效用和普适性。代码可在以下链接获取。', 'title_zh': 'CLIPin：一种用于多模态语义对齐的非对比plug-in模块'}
{'arxiv_id': 'arXiv:2508.06433', 'title': 'Memp: Exploring Agent Procedural Memory', 'authors': 'Runnan Fang, Yuan Liang, Xiaobin Wang, Jialong Wu, Shuofei Qiao, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang', 'link': 'https://arxiv.org/abs/2508.06433', 'abstract': 'Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.', 'abstract_zh': '基于大型语言模型的代理在多种任务中表现出色，但遭受 brittle 程序性记忆的困扰，这种记忆要么是人工工程化的，要么嵌入在静态参数中。本文我们研究赋予代理可学习、可更新和终身性的程序性记忆的策略。我们提出 Memp，该方法将过往代理轨迹提炼为细粒度的、逐步的指令和高层次的、脚本般的抽象，并探讨程序性记忆构建、检索和更新的不同策略的影响。结合一个动态的程序，该程序持续更新、修正和废弃其内容，从而使该仓库与新经验同步演化。在 TravelPlanner 和 ALFWorld 上的经验评估表明，随着记忆仓库的优化，代理在类似任务上获得更高的成功率和更高的效率。此外，来自更强模型的程序性记忆仍然具有价值：将其程序性记忆迁移到较弱模型中可以带来显著的性能提升。', 'title_zh': 'Memp: 探索代理程序记忆'}
{'arxiv_id': 'arXiv:2508.06429', 'title': 'SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation', 'authors': 'Guido Manni, Clemente Lauretti, Loredana Zollo, Paolo Soda', 'link': 'https://arxiv.org/abs/2508.06429', 'abstract': 'Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks -- a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier -- within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at this https URL.', 'abstract_zh': '基于GAN的半监督学习框架在有限标注数据条件下的应用：医学影像中的革新', 'title_zh': '稀疏数据，丰富成果：基于类条件图像翻译的少样本半监督学习'}
{'arxiv_id': 'arXiv:2508.06426', 'title': 'Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation', 'authors': 'Youguang Xing, Xu Luo, Junlin Xie, Lianli Gao, Hengtao Shen, Jingkuan Song', 'link': 'https://arxiv.org/abs/2508.06426', 'abstract': 'Generalist robot policies trained on large-scale datasets such as Open X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks. However, they often struggle to generalize beyond the distribution of their training data. In this paper, we investigate the underlying cause of this limited generalization capability. We identify shortcut learning -- the reliance on task-irrelevant features -- as a key impediment to generalization. Through comprehensive theoretical and empirical analysis, we uncover two primary contributors to shortcut learning: (1) limited diversity within individual sub-datasets, and (2) significant distributional disparities across sub-datasets, leading to dataset fragmentation. These issues arise from the inherent structure of large-scale datasets like OXE, which are typically composed of multiple sub-datasets collected independently across varied environments and embodiments. Our findings provide critical insights into dataset collection strategies that can reduce shortcut learning and enhance the generalization ability of generalist robot policies. Moreover, in scenarios where acquiring new large-scale data is impractical, we demonstrate that carefully selected robotic data augmentation strategies can effectively reduce shortcut learning in existing offline datasets, thereby improving generalization capabilities of generalist robot policies, e.g., $\\pi_0$, in both simulation and real-world environments. More information at this https URL.', 'abstract_zh': '大型数据集如Open X-Embodiment (OXE)训练的通用机器人策略在广泛的任务中表现出强劲性能，但往往难以超出训练数据分布进行泛化。本文探讨了这种有限泛化能力的根本原因。我们认定，依赖与任务无关特征的“捷径学习”是阻碍泛化的关键因素。通过全面的理论和实证分析，我们发现两种主要促成捷径学习的因素：（1）单个子数据集内部的有限多样性，以及（2）子数据集之间的显著分布差异，导致数据集碎片化。这些问题源于大型数据集如OXE的固有结构，这些数据集通常由在不同环境和实体下独立收集的多个子数据集组成。我们的发现为减少捷径学习并提升通用机器人策略泛化能力的采样策略提供了关键见解。此外，在获取新大规模数据不太实际的情况下，我们展示了精心选择的机器人数据扩增策略如何有效减少现有离线数据集中的捷径学习，从而提高通用机器人策略的泛化能力，例如$\\pi_0$，在模拟和真实环境中的泛化能力。更多信息，请访问此链接。', 'title_zh': '通用机器人策略中的捷径学习：数据集多样性与碎片化的作用'}
{'arxiv_id': 'arXiv:2508.06411', 'title': 'Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks', 'authors': 'Ze Shen Chin', 'link': 'https://arxiv.org/abs/2508.06411', 'abstract': 'Although discourse around the risks of Artificial Intelligence (AI) has grown, it often lacks a comprehensive, multidimensional framework, and concrete causal pathways mapping hazard to harm. This paper aims to bridge this gap by examining six commonly discussed AI catastrophic risks: CBRN, cyber offense, sudden loss of control, gradual loss of control, environmental risk, and geopolitical risk. First, we characterize these risks across seven key dimensions, namely intent, competency, entity, polarity, linearity, reach, and order. Next, we conduct risk pathway modeling by mapping step-by-step progressions from the initial hazard to the resulting harms. The dimensional approach supports systematic risk identification and generalizable mitigation strategies, while risk pathway models help identify scenario-specific interventions. Together, these methods offer a more structured and actionable foundation for managing catastrophic AI risks across the value chain.', 'abstract_zh': '尽管关于人工智能（AI）风险的讨论日益增多，但往往缺乏一个全面且多维度的框架，以及将风险转化为具体伤害的明确因果路径。本文旨在通过探讨六种常见讨论的AI灾难性风险——化学、生物、放射性、核（CBRN）、网络攻击、突然失去控制、渐进失去控制、环境风险和地缘政治风险来填补这一空白。首先，我们从意图、能力、实体、极性、线性、范围和顺序七个关键维度对这些风险进行描述。随后，我们进行风险路径建模，逐步映射从初始危险到最终伤害的过程。维度方法有助于系统地识别风险并提出普遍适用的缓解策略，而风险路径模型有助于识别特定场景下的干预措施。这些方法共同为跨价值链管理灾难性AI风险提供了更加结构化和可操作的基础。', 'title_zh': '灾难性AI风险的维度表征与路径建模'}
{'arxiv_id': 'arXiv:2508.06407', 'title': 'A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery', 'authors': 'Ch Muhammad Awais, Marco Reggiannini, Davide Moroni, Oktay Karakus', 'link': 'https://arxiv.org/abs/2508.06407', 'abstract': 'High-resolution imagery plays a critical role in improving the performance of visual recognition tasks such as classification, detection, and segmentation. In many domains, including remote sensing and surveillance, low-resolution images can limit the accuracy of automated analysis. To address this, super-resolution (SR) techniques have been widely adopted to attempt to reconstruct high-resolution images from low-resolution inputs. Related traditional approaches focus solely on enhancing image quality based on pixel-level metrics, leaving the relationship between super-resolved image fidelity and downstream classification performance largely underexplored. This raises a key question: can integrating classification objectives directly into the super-resolution process further improve classification accuracy? In this paper, we try to respond to this question by investigating the relationship between super-resolution and classification through the deployment of a specialised algorithmic strategy. We propose a novel methodology that increases the resolution of synthetic aperture radar imagery by optimising loss functions that account for both image quality and classification performance. Our approach improves image quality, as measured by scientifically ascertained image quality indicators, while also enhancing classification accuracy.', 'abstract_zh': '高分辨率成像在提高分类、检测和分割等视觉识别任务性能中扮演着关键角色。在遥感和监控等多个领域，低分辨率图像会限制自动化分析的准确性。为解决这一问题，已经广泛采用了超分辨率（SR）技术，希望通过低分辨率输入重建高分辨率图像。相关传统的SR方法主要专注于基于像素级别的图像质量增强，而对超分辨率图像保真度与下游分类性能之间的关系研究相对较少。这引发了一个关键问题：是否可以将分类目标直接整合到超分辨率过程中以进一步提高分类准确性？在本文中，我们通过部署专门的算法策略来探索超分辨率与分类之间的关系，并提出了一种新颖的方法，通过优化同时考虑图像质量和分类性能的损失函数来提升合成孔径雷达图像的分辨率。我们的方法不仅在科学评价的图像质量指标下改善了图像质量，还在分类准确性上有所提升。', 'title_zh': '一种针对SAR图像中船舶目标的分类意识超分辨率框架'}
{'arxiv_id': 'arXiv:2508.06401', 'title': 'A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges', 'authors': 'Andrew Brown, Muhammad Roman, Barry Devereux', 'link': 'https://arxiv.org/abs/2508.06401', 'abstract': 'This systematic review of the research literature on retrieval-augmented generation (RAG) provides a focused analysis of the most highly cited studies published between 2020 and May 2025. A total of 128 articles met our inclusion criteria. The records were retrieved from ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP). RAG couples a neural retriever with a generative language model, grounding output in up-to-date, non-parametric memory while retaining the semantic generalisation stored in model weights. Guided by the PRISMA 2020 framework, we (i) specify explicit inclusion and exclusion criteria based on citation count and research questions, (ii) catalogue datasets, architectures, and evaluation practices, and (iii) synthesise empirical evidence on the effectiveness and limitations of RAG. To mitigate citation-lag bias, we applied a lower citation-count threshold to papers published in 2025 so that emerging breakthroughs with naturally fewer citations were still captured. This review clarifies the current research landscape, highlights methodological gaps, and charts priority directions for future research.', 'abstract_zh': 'This systematic review of retrieval-augmented generation (RAG)研究文献的系统回顾：2020年至2025年最具引用价值的研究综述', 'title_zh': '基于检索增强生成的技术、指标与挑战的系统文献综述'}
{'arxiv_id': 'arXiv:2508.06393', 'title': 'Robust Target Speaker Diarization and Separation via Augmented Speaker Embedding Sampling', 'authors': 'Md Asif Jalal, Luca Remaggi, Vasileios Moschopoulos, Thanasis Kotsiopoulos, Vandana Rajan, Karthikeyan Saravanan, Anastasis Drosou, Junho Heo, Hyuk Oh, Seokyeong Jeong', 'link': 'https://arxiv.org/abs/2508.06393', 'abstract': 'Traditional speech separation and speaker diarization approaches rely on prior knowledge of target speakers or a predetermined number of participants in audio signals. To address these limitations, recent advances focus on developing enrollment-free methods capable of identifying targets without explicit speaker labeling. This work introduces a new approach to train simultaneous speech separation and diarization using automatic identification of target speaker embeddings, within mixtures. Our proposed model employs a dual-stage training pipeline designed to learn robust speaker representation features that are resilient to background noise interference. Furthermore, we present an overlapping spectral loss function specifically tailored for enhancing diarization accuracy during overlapped speech frames. Experimental results show significant performance gains compared to the current SOTA baseline, achieving 71% relative improvement in DER and 69% in cpWER.', 'abstract_zh': '传统的语音分离和说话人分场合方法依赖于目标说话人的先验知识或音频信号中参与者数量的先设。为了解决这些限制，近期的进展专注于开发无需 Enrollment 的方法，能够无需显式的讲话人标签来识别目标。本文提出了一种新的方法，用于同时训练语音分离和分场合，通过混合中的自动目标说话人嵌入识别。我们提出的模型采用双阶段训练管道，旨在学习对背景噪声干扰具有抗干扰性的稳健说话人表示特征。此外，我们呈现了一种特定于重叠谱的损失函数，以提高在重叠语音帧期间的分场合准确性。实验结果表明，与当前的 SOTA 基线相比，显著提高了性能，特别是在 DER 上实现了 71% 的相对改进，在 cpWER 上实现了 69% 的改进。', 'title_zh': '基于增强speaker嵌入采样的鲁棒目标说话人分离与识别'}
{'arxiv_id': 'arXiv:2508.06389', 'title': 'Identity Increases Stability in Neural Cellular Automata', 'authors': 'James Stovold', 'link': 'https://arxiv.org/abs/2508.06389', 'abstract': "Neural Cellular Automata (NCAs) offer a way to study the growth of two-dimensional artificial organisms from a single seed cell. From the outset, NCA-grown organisms have had issues with stability, their natural boundary often breaking down and exhibiting tumour-like growth or failing to maintain the expected shape. In this paper, we present a method for improving the stability of NCA-grown organisms by introducing an 'identity' layer with simple constraints during training.\nResults show that NCAs grown in close proximity are more stable compared with the original NCA model. Moreover, only a single identity value is required to achieve this increase in stability. We observe emergent movement from the stable organisms, with increasing prevalence for models with multiple identity values.\nThis work lays the foundation for further study of the interaction between NCA-grown organisms, paving the way for studying social interaction at a cellular level in artificial organisms.", 'abstract_zh': 'Neural Cellular Automata (NCAs) 提供了一种从单个种子细胞研究二维人工有机物生长的方式。从一开始，CA-g生长方式就遇到了稳定性问题，表现为非自然自然的行为和类似肿瘤的生长方式，难以保持稳定形态。本文提出了一种方法通过引入一个具有简单约束的‘身份’状态方式来提高NCAs-g生长方式的稳定性。结果表明，引入‘身份’状态后使得系统更稳定，与原生CA相比。而且只需要一个单态方式使就能达到到这一稳定性的提高。随著复杂稳定态的增多方式，涌现的行为方式也更倾向于多种稳定态的的组合。此研究为进一深入研究NCAs-g生长方式之间的相互相互互动打下基础。并并为在人工生命体中从细胞水平研究社会互动提供可能。', 'title_zh': '身份增加了神经细胞自动机的稳定性'}
{'arxiv_id': 'arXiv:2508.06387', 'title': 'End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation', 'authors': 'Anurag Tripathi, Vaibhav Patle, Abhinav Jain, Ayush Pundir, Sairam Menon, Ajeet Kumar Singh', 'link': 'https://arxiv.org/abs/2508.06387', 'abstract': "Text-to-SQL bridges the gap between natural language and structured database language, thus allowing non-technical users to easily query databases. Traditional approaches model text-to-SQL as a direct translation task, where a given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances in large language models (LLMs) have significantly improved translation accuracy, however, these methods all require that the target database is pre-specified. This becomes problematic in scenarios with multiple extensive databases, where identifying the correct database becomes a crucial yet overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL framework to identify the user's intended database before generating SQL queries. Our approach leverages LLMs and prompt engineering to extract implicit information from natural language queries (NLQs) in the form of a ruleset. We then train a large db\\_id prediction model, which includes a RoBERTa-based finetuned encoder, to predict the correct Database identifier (db\\_id) based on both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL by using critic agents to correct errors. Experimental results demonstrate that our framework outperforms the current state-of-the-art models in both database intent prediction and SQL generation accuracy.", 'abstract_zh': 'Text-to-SQL在自然语言与结构化数据库语言之间架起桥梁，使非技术人员能够轻松查询数据库。在多个大规模数据库的场景下，确定正确的数据库成为了一个关键但被忽视的步骤。本文 propose 一种三阶段端到端的文本到SQL框架，用于在生成SQL查询之前识别用户的意图数据库。该方法利用大语言模型和提示工程技术从自然语言查询中提取隐式信息，并以规则集的形式表示。然后训练一个大规模的db_id预测模型，该模型基于RoBERTa微调编码器，根据自然语言查询和大语言模型生成的规则预测正确的数据库标识符(db_id)。最后，使用批评代理修正生成的SQL中的错误。实验结果表明，本文提出的框架在数据库意图预测和SQL生成准确性方面均优于当前最先进的模型。', 'title_zh': '端到端的文本到SQL生成模型：利用大型语言模型进行适应性查询生成'}
{'arxiv_id': 'arXiv:2508.06372', 'title': 'SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models', 'authors': 'Han Yin, Yafeng Chen, Chong Deng, Luyao Cheng, Hui Wang, Chao-Hong Tan, Qian Chen, Wen Wang, Xiangang Li', 'link': 'https://arxiv.org/abs/2508.06372', 'abstract': 'The Speaker Diarization and Recognition (SDR) task aims to predict "who spoke when and what" within an audio clip, which is a crucial task in various real-world multi-speaker scenarios such as meeting transcription and dialogue systems. Existing SDR systems typically adopt a cascaded framework, combining multiple modules such as speaker diarization (SD) and automatic speech recognition (ASR). The cascaded systems suffer from several limitations, such as error propagation, difficulty in handling overlapping speech, and lack of joint optimization for exploring the synergy between SD and ASR tasks. To address these limitations, we introduce SpeakerLM, a unified multimodal large language model for SDR that jointly performs SD and ASR in an end-to-end manner. Moreover, to facilitate diverse real-world scenarios, we incorporate a flexible speaker registration mechanism into SpeakerLM, enabling SDR under different speaker registration settings. SpeakerLM is progressively developed with a multi-stage training strategy on large-scale real data. Extensive experiments show that SpeakerLM demonstrates strong data scaling capability and generalizability, outperforming state-of-the-art cascaded baselines on both in-domain and out-of-domain public SDR benchmarks. Furthermore, experimental results show that the proposed speaker registration mechanism effectively ensures robust SDR performance of SpeakerLM across diverse speaker registration conditions and varying numbers of registered speakers.', 'abstract_zh': 'Speaker Diarization and Recognition (SDR)任务旨在预测音频片段中“谁在何时说话以及说了什么”，这是会议转录和对话系统等多种真实世界多讲话者场景中的关键任务。现有的SDR系统通常采用级联框架，结合了诸如说话人分割（SD）和自动语音识别（ASR）等多个模块。级联系统存在一些局限性，如错误传递、难以处理重叠语音以及缺乏联合优化以探索SD和ASR任务之间的协同效应。为解决这些问题，我们提出了一种名为SpeakerLM的统一多模态大型语言模型，该模型以端到端的方式联合执行SD和ASR任务。此外，为了适应不同的实际场景，我们还为SpeakerLM引入了一种灵活的说话人注册机制，使其能够在不同的说话人注册设置下进行SDR。SpeakerLM通过大规模实际数据的多阶段训练策略逐步发展。广泛的实验证明，SpeakerLM在数据扩展能力和泛化性上表现出色，其性能优于现有最先进的级联基线系统，不仅在领域内，而且在领域外公开的SDR基准测试中均有优势。此外，实验结果表明，所提出的话者注册机制有效地确保了SpeakerLM在不同说话人注册条件和不同注册说话人数下的鲁棒性。', 'title_zh': 'SpeakerLM：基于多模态大型语言模型的端到端多功能说话人辨识与会话分析'}
{'arxiv_id': 'arXiv:2508.06364', 'title': 'ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design', 'authors': 'Renyi Zhou, Huimin Zhu, Jing Tang, Min Li', 'link': 'https://arxiv.org/abs/2508.06364', 'abstract': "Achieving precise control over a molecule's biological activity-encompassing targeted activation/inhibition, cooperative multi-target modulation, and off-target toxicity mitigation-remains a critical challenge in de novo drug design. However, existing generative methods primarily focus on producing molecules with a single desired activity, lacking integrated mechanisms for the simultaneous management of multiple intended and unintended molecular interactions. Here, we propose ActivityDiff, a generative approach based on the classifier-guidance technique of diffusion models. It leverages separately trained drug-target classifiers for both positive and negative guidance, enabling the model to enhance desired activities while minimizing harmful off-target effects. Experimental results show that ActivityDiff effectively handles essential drug design tasks, including single-/dual-target generation, fragment-constrained dual-target design, selective generation to enhance target specificity, and reduction of off-target effects. These results demonstrate the effectiveness of classifier-guided diffusion in balancing efficacy and safety in molecular design. Overall, our work introduces a novel paradigm for achieving integrated control over molecular activity, and provides ActivityDiff as a versatile and extensible framework.", 'abstract_zh': '实现对分子生物活性的精确控制，包括靶向激活/抑制、协同多靶点调节以及减轻非靶向毒性，仍然是从头药物设计中的关键挑战。现有的生成方法主要侧重于生成具有单一期望活性的分子，缺乏同时管理多个期望和非期望分子相互作用的集成机制。在此，我们提出了一种基于分类器引导技术的生成方法——ActivityDiff。该方法利用分别训练的正向和负向药物-靶点分类器进行双重指导，使得模型能够在增强期望活性的同时最小化有害的非靶向效应。实验结果表明，ActivityDiff 能够有效地处理包括单靶点/双靶点生成、片段限制的双靶点设计、选择性生成以增强靶点特异性以及减少非靶向效应在内的关键药物设计任务。这些结果证明了分类器引导的扩散方法在分子设计中平衡效能和安全性方面的有效性。总体而言，我们的工作介绍了一种新的集成控制分子活性的范式，并提供了一个灵活且可扩展的框架——ActivityDiff。', 'title_zh': 'ActivityDiff：一种具有正负活动引导的扩散模型用于从头药物设计'}
{'arxiv_id': 'arXiv:2508.06361', 'title': 'Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts', 'authors': 'Zhaomin Wu, Mingzhe Du, See-Kiong Ng, Bingsheng He', 'link': 'https://arxiv.org/abs/2508.06361', 'abstract': 'Large Language Models (LLMs) have been widely deployed in reasoning, planning, and decision-making tasks, making their trustworthiness a critical concern. The potential for intentional deception, where an LLM deliberately fabricates or conceals information to serve a hidden objective, remains a significant and underexplored threat. Existing studies typically induce such deception by explicitly setting a "hidden" objective through prompting or fine-tuning, which may not fully reflect real-world human-LLM interactions. Moving beyond this human-induced deception, we investigate LLMs\' self-initiated deception on benign prompts. To address the absence of ground truth in this evaluation, we propose a novel framework using "contact searching questions." This framework introduces two statistical metrics derived from psychological principles to quantify the likelihood of deception. The first, the Deceptive Intention Score, measures the model\'s bias towards a hidden objective. The second, Deceptive Behavior Score, measures the inconsistency between the LLM\'s internal belief and its expressed output. Upon evaluating 14 leading LLMs, we find that both metrics escalate as task difficulty increases, rising in parallel for most models. Building on these findings, we formulate a mathematical model to explain this behavior. These results reveal that even the most advanced LLMs exhibit an increasing tendency toward deception when handling complex problems, raising critical concerns for the deployment of LLM agents in complex and crucial domains.', 'abstract_zh': '大型语言模型（LLMs）已在推理、规划和决策任务中广泛部署，其可信度成为一个关键问题。故意欺骗的潜在风险，即LLM故意编造或隐瞒信息以实现隐秘目标，仍然是一个重要的未被充分探索的威胁。现有研究通常通过明确设定“隐藏”目标来诱导这种欺骗，这可能无法充分反映真实世界中人类与LLM的交互。超越这种人为诱导的欺骗，我们研究了LLMs在 benign提示下的自我发起的欺骗。为解决这一评估中的缺乏真实地面信息的问题，我们提出了一种新型框架，使用“联系方式查询问题”。该框架引入了两种源自心理学原则的统计指标，以量化欺骗的可能性。第一种指标，欺骗意图得分，衡量模型对隐藏目标的偏向性。第二种指标，欺骗行为得分，衡量LLM内部信念与其表达输出的一致性。在评估14个领先的LLMs后，我们发现这两种指标随着任务难度的增加而上升，大多数模型的上升趋势一致。基于这些发现，我们构建了一个数学模型来解释这种行为。这些结果揭示了即使是最先进的LLMs，在处理复杂问题时也表现出越来越倾向于欺骗的趋势，这为在复杂和关键领域部署LLM代理提出了重要的关切。', 'title_zh': '超越提示诱发的谎言：探究 benign 提示下的 LLM  deceive 行为'}
{'arxiv_id': 'arXiv:2508.06357', 'title': 'Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd', 'authors': 'Aman Bhatta, Maria Dhakal, Michael C. King, Kevin W. Bowyer', 'link': 'https://arxiv.org/abs/2508.06357', 'abstract': 'A central problem in one-to-many facial identification is that the person in the probe image may or may not have enrolled image(s) in the gallery; that is, may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one result is Out-of-gallery have mostly focused on finding a suitable threshold on the similarity score. We take a new approach, using the additional enrolled images of the identity with the rank-one result to predict if the rank-one result is In-gallery / Out-of-gallery. Given a gallery of identities and images, we generate In-gallery and Out-of-gallery training data by extracting the ranks of additional enrolled images corresponding to the rank-one identity. We then train a classifier to utilize this feature vector to predict whether a rank-one result is In-gallery or Out-of-gallery. Using two different datasets and four different matchers, we present experimental results showing that our approach is viable for mugshot quality probe images, and also, importantly, for probes degraded by blur, reduced resolution, atmospheric turbulence and sunglasses. We also analyze results across demographic groups, and show that In-gallery / Out-of-gallery classification accuracy is similar across demographics. Our approach has the potential to provide an objective estimate of whether a one-to-many facial identification is Out-of-gallery, and thereby to reduce false positive identifications, wrongful arrests, and wasted investigative time. Interestingly, comparing the results of older deep CNN-based face matchers with newer ones suggests that the effectiveness of our Out-of-gallery detection approach emerges only with matchers trained using advanced margin-based loss functions.', 'abstract_zh': '一项重要的挑战在于一对一多识别中，探针图像中的人物可能或可能不在-gallery中，即可能是In-gallery或Out-of-gallery。以往的方法主要集中在找到相似度分数的合适阈值来检测rank-one结果是否为Out-of-gallery。我们提出了一种新的方法，利用rank-one结果对应身份的额外注册图像来预测rank-one结果是In-gallery还是Out-of-gallery。给定一组身份和图像，我们通过提取与rank-one身份对应的额外注册图像的排名来生成In-gallery和Out-of-gallery的训练数据。然后，我们训练一个分类器，利用该特征矢量预测rank-one结果是In-gallery还是Out-of-gallery。使用两个不同的数据集和四种不同的匹配器，我们展示了实验结果，表明我们的方法适用于 mugshot质量的探针图像，并且对于因模糊、分辨率降低、大气湍流和墨镜而退化的探针图像也同样适用。我们还分析了不同人口统计学组别的结果，显示In-gallery/Out-of-gallery分类的准确性在不同人口统计学群体中相似。我们的方法有潜力提供一种客观估计一对一多识别是否为Out-of-gallery的方法，从而减少误报、误捕和浪费的调查时间。有趣的是，将较早的基于深层CNN的面匹配器的结果与较新的匹配器的结果进行比较表明，我们的Out-of-gallery检测方法的有效性仅在使用先进的基于边界的损失函数进行训练的匹配器中出现。', 'title_zh': '你是被接纳还是被排斥（（还是画廊？）来自相同身份群体的智慧'}
{'arxiv_id': 'arXiv:2508.06347', 'title': 'Structural Equation-VAE: Disentangled Latent Representations for Tabular Data', 'authors': 'Ruiyu Zhang, Ce Zhao, Xin Zhao, Lin Nie, Wai-Fung Lam', 'link': 'https://arxiv.org/abs/2508.06347', 'abstract': 'Learning interpretable latent representations from tabular data remains a challenge in deep generative modeling. We introduce SE-VAE (Structural Equation-Variational Autoencoder), a novel architecture that embeds measurement structure directly into the design of a variational autoencoder. Inspired by structural equation modeling, SE-VAE aligns latent subspaces with known indicator groupings and introduces a global nuisance latent to isolate construct-specific confounding variation. This modular architecture enables disentanglement through design rather than through statistical regularizers alone. We evaluate SE-VAE on a suite of simulated tabular datasets and benchmark its performance against a series of leading baselines using standard disentanglement metrics. SE-VAE consistently outperforms alternatives in factor recovery, interpretability, and robustness to nuisance variation. Ablation results reveal that architectural structure, rather than regularization strength, is the key driver of performance. SE-VAE offers a principled framework for white-box generative modeling in scientific and social domains where latent constructs are theory-driven and measurement validity is essential.', 'abstract_zh': '从表格数据中学习可解释的潜在表示仍然是深度生成建模中的一个挑战。SE-VAE（结构方程变量自编码器）是一种新颖的架构，它将测量结构直接嵌入到变分自编码器的设计中。受结构方程建模的启发，SE-VAE 将潜在子空间与已知指标分组对齐，并引入一个全局干扰潜在变量以隔离特定结构的混杂变异。这种模块化架构通过设计而非仅通过统计正则化来实现解耦。我们在一系列模拟表格数据集上评估了 SE-VAE，并使用标准解耦指标将其性能与一系列领先基准进行比较。SE-VAE 在因子恢复、可解释性和对干扰变异的稳健性方面始终优于其他替代方案。消融结果表明，架构结构而非正则化强度是性能的关键驱动因素。SE-VAE 为科学和社会领域中的白盒生成建模提供了一个有原则的框架，在这些领域中，潜在结构是理论驱动的，测量有效性是必不可少的。', 'title_zh': '结构方程-VAE: 分离潜变量表示的表格数据'}
{'arxiv_id': 'arXiv:2508.06345', 'title': 'Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering', 'authors': 'Yanbin Wei, Jiangyue Yan, Chun Kang, Yang Chen, Hua Liu, James T. Kwok, Yu Zhang', 'link': 'https://arxiv.org/abs/2508.06345', 'abstract': 'Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities in diverse domain question-answering (QA) tasks, including graph QA that involves complex graph topologies. However, most current approaches use only a single type of graph representation, namely Topology Representation Form (TRF), such as prompt-unified text descriptions or style-fixed visual styles. Those "one-size-fits-all" approaches fail to consider the specific preferences of different models or tasks, often leading to incorrect or overly long responses. To address this, we first analyze the characteristics and weaknesses of existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency (GRE), which measures the balance between the performance and the brevity in graph QA. Built on these, we develop the DynamicTRF framework, which aims to improve both the accuracy and conciseness of graph QA. To be specific, DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based on their GRE scores, to probe the question-specific TRF preferences. Then it trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from $F_{ZS}$ for each question during the inference. Extensive experiments across 7 in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms of accuracy', 'abstract_zh': '标题：大型多 跨模态模型在零样本图问答任务中的偏好分析与动态调整框架', 'title_zh': '基于自适应拓扑表示的零样本图问答'}
{'arxiv_id': 'arXiv:2508.06343', 'title': 'On Approximate MMS Allocations on Restricted Graph Classes', 'authors': 'Václav Blažej, Michał Dębski ad Zbigniew Lonc, Marta Piecyk, Paweł Rzążewski', 'link': 'https://arxiv.org/abs/2508.06343', 'abstract': 'We study the problem of fair division of a set of indivisible goods with connectivity constraints. Specifically, we assume that the goods are represented as vertices of a connected graph, and sets of goods allocated to the agents are connected subgraphs of this graph. We focus on the widely-studied maximin share criterion of fairness. It has been shown that an allocation satisfying this criterion may not exist even without connectivity constraints, i.e., if the graph of goods is complete. In view of this, it is natural to seek approximate allocations that guarantee each agent a connected bundle of goods with value at least a constant fraction of the maximin share value to the agent. It is known that for some classes of graphs, such as complete graphs, cycles, and $d$-claw-free graphs for any fixed $d$, such approximate allocations indeed exist. However, it is an open problem whether they exist for the class of all graphs.\nIn this paper, we continue the systematic study of the existence of approximate allocations on restricted graph classes. In particular, we show that such allocations exist for several well-studied classes, including block graphs, cacti, complete multipartite graphs, and split graphs.', 'abstract_zh': '我们研究具有连通性约束的一组不可分物品的公平分配问题。具体地，我们假设物品用连通图的顶点表示，分配给代理的物品集应为该图的连通子图。我们 focus 在广泛研究的最大化最小份额公平性标准上。已证明，在没有连通性约束的情况下，即使对于完全图，也可能不存在满足这一标准的分配。鉴于此，自然地，寻求保证每个代理获得具有至少为自身最大化最小份额值常数倍价值的连通物品集的近似分配变得合理。已知对于某些类别的图，如完全图、圈图和任意固定 $d$ 的 $d$-爪图，确实存在这样的近似分配。然而，对于所有图的类别来说，它们是否存在仍是一个公开问题。\n\n在本文中，我们继续对受限图类上近似分配的存在性进行系统研究。特别是，我们证明了对于几种已经被广泛研究的图类，如块图、环图、完全.multipartite 图和分裂图，确实存在这样的近似分配。', 'title_zh': '在受限图类上的近似MMS分配'}
{'arxiv_id': 'arXiv:2508.06336', 'title': 'Unsupervised Partner Design Enables Robust Ad-hoc Teamwork', 'authors': 'Constantin Ruhdorfer, Matteo Bortoletto, Victor Oei, Anna Penzkofer, Andreas Bulling', 'link': 'https://arxiv.org/abs/2508.06336', 'abstract': "We introduce Unsupervised Partner Design (UPD) - a population-free, multi-agent reinforcement learning framework for robust ad-hoc teamwork that adaptively generates training partners without requiring pretrained partners or manual parameter tuning. UPD constructs diverse partners by stochastically mixing an ego agent's policy with biased random behaviours and scores them using a variance-based learnability metric that prioritises partners near the ego agent's current learning frontier. We show that UPD can be integrated with unsupervised environment design, resulting in the first method enabling fully unsupervised curricula over both level and partner distributions in a cooperative setting. Through extensive evaluations on Overcooked-AI and the Overcooked Generalisation Challenge, we demonstrate that this dynamic partner curriculum is highly effective: UPD consistently outperforms both population-based and population-free baselines as well as ablations. In a user study, we further show that UPD achieves higher returns than all baselines and was perceived as significantly more adaptive, more human-like, a better collaborator, and less frustrating.", 'abstract_zh': '无监督伙伴设计：一种无需人群的多智能体强化学习框架，用于自适应生成训练伙伴以实现鲁棒的即兴团队合作', 'title_zh': '无监督伙伴设计实现稳健的临时 teamwork'}
{'arxiv_id': 'arXiv:2508.06318', 'title': 'Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection', 'authors': "Giacomo D'Amicantonio, Snehashis Majhi, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, François Bremond, Egor Bondarev", 'link': 'https://arxiv.org/abs/2508.06318', 'abstract': 'Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.', 'abstract_zh': '视频异常检测（VAD）是一个具有挑战性的任务，由于异常事件的多样性以及标注数据的有限性。在弱监督视频异常检测（WSVAD）范式下，训练时仅提供视频级别的标签，而预测在帧级别进行。尽管最新的模型在简单的异常（如爆炸）上表现良好，但在复杂的现实世界事件（如偷窃）上却面临困难。这种困难源于两个关键问题：（1）当前模型无法处理异常类型的多样性，因为它们使用共享模型处理所有类别，忽略了类别特定的特征；（2）弱监督信号缺乏精确的时间信息，限制了捕捉正常事件与异常事件混合下的细微异常模式的能力。为了解决这些挑战，我们提出了一种新的框架——基于高斯点积的专家混合模型（GS-MoE），该框架采用一组专门针对特定异常类型进行捕捉的专家模型。这些专家模型由时间高斯点积损失指导，使模型能够利用时间一致性并增强弱监督信号。高斯点积方法通过关注最有可能包含异常事件的时间段，促使对异常进行更精确和全面的表示。这些专家模型的预测结果通过专家混合机制进行整合，以建模复杂多样的异常模式间的复杂关系。我们的方法在UCF-Crime数据集上达到了91.58%的AUC性能，并在XD-Violence和MSAD数据集上展示了更优异的结果。通过利用类别特定的专家知识和时间指导，GS-MoE为弱监督下的视频异常检测设置了新的基准。', 'title_zh': '混合专家由高斯点引导：一种弱监督视频异常检测的新方法'}
{'arxiv_id': 'arXiv:2508.06301', 'title': 'FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields', 'authors': 'Junhyeog Yun, Minui Hong, Gunhee Kim', 'link': 'https://arxiv.org/abs/2508.06301', 'abstract': "Neural fields provide a memory-efficient representation of data, which can effectively handle diverse modalities and large-scale data. However, learning to map neural fields often requires large amounts of training data and computations, which can be limited to resource-constrained edge devices. One approach to tackle this limitation is to leverage Federated Meta-Learning (FML), but traditional FML approaches suffer from privacy leakage. To address these issues, we introduce a novel FML approach called FedMeNF. FedMeNF utilizes a new privacy-preserving loss function that regulates privacy leakage in the local meta-optimization. This enables the local meta-learner to optimize quickly and efficiently without retaining the client's private data. Our experiments demonstrate that FedMeNF achieves fast optimization speed and robust reconstruction performance, even with few-shot or non-IID data across diverse data modalities, while preserving client data privacy.", 'abstract_zh': '基于神经场的联邦元学习方法FedMeNF提供了一种内存高效的数据表示，能够有效处理多种模态和大规模数据。然而，学习神经场的映射往往需要大量的训练数据和计算，这在资源受限的边缘设备上可能受到限制。为解决这一限制，我们提出了一种新的联邦元学习方法FedMeNF，它利用一种新的隐私保护损失函数调节局部元优化中的隐私泄漏，使局部元学习者能够快速高效地优化而不保留客户端的私有数据。实验结果表明，FedMeNF在少量样本或非IID数据下，能够实现快速优化速度和稳健的重建性能，同时保护客户端数据隐私。', 'title_zh': 'FedMeNF：隐私保护联邦元学习在神经场中的应用'}
{'arxiv_id': 'arXiv:2508.06287', 'title': 'Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification', 'authors': 'Mobarak Abumohsen, Enrique Costa-Montenegro, Silvia García-Méndez, Amani Yousef Owda, Majdi Owda', 'link': 'https://arxiv.org/abs/2508.06287', 'abstract': 'Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one of the most common causes of death for men and women worldwide. Computed Tomography (CT) images are the most preferred diagnosis method because of their low cost and their faster processing times. Many researchers have proposed various ways of identifying lung cancer using CT images. However, such techniques suffer from significant false positives, leading to low accuracy. The fundamental reason results from employing a small and imbalanced dataset. This paper introduces an innovative approach for LC detection and classification from CT images based on the DenseNet201 model. Our approach comprises several advanced methods such as Focal Loss, data augmentation, and regularization to overcome the imbalanced data issue and overfitting challenge. The findings show the appropriateness of the proposal, attaining a promising performance of 98.95% accuracy.', 'abstract_zh': '肺癌（LC）是全球诊断频率最高的癌症之一，并且是男性和女性最常见的死亡原因之一。计算机断层扫描（CT）图像因其成本低和处理速度快，是最常用的诊断方法。许多研究人员提出了多种利用CT图像识别肺癌的方法。然而，这些技术由于采用小规模和不均衡的数据集而面临显著的假阳性问题，导致准确率较低。根本原因在于使用了小且不均衡的数据集。本文介绍了一种基于DenseNet201模型的创新方法，用于从CT图像检测和分类肺癌。该方法结合了焦点损失、数据增强和正则化等advanced方法，以解决不均衡数据和过拟合问题。研究结果表明该方法的有效性，达到了98.95%的准确率。', 'title_zh': '先进的深度学习技术在肺癌检测与分类中的应用'}
{'arxiv_id': 'arXiv:2508.06269', 'title': 'OM2P: Offline Multi-Agent Mean-Flow Policy', 'authors': 'Zhuoran Li, Xun Wang, Hai Zhong, Longbo Huang', 'link': 'https://arxiv.org/abs/2508.06269', 'abstract': 'Generative models, especially diffusion and flow-based models, have been promising in offline multi-agent reinforcement learning. However, integrating powerful generative models into this framework poses unique challenges. In particular, diffusion and flow-based policies suffer from low sampling efficiency due to their iterative generation processes, making them impractical in time-sensitive or resource-constrained settings. To tackle these difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel offline MARL algorithm to achieve efficient one-step action sampling. To address the misalignment between generative objectives and reward maximization, we introduce a reward-aware optimization scheme that integrates a carefully-designed mean-flow matching loss with Q-function supervision. Additionally, we design a generalized timestep distribution and a derivative-free estimation strategy to reduce memory overhead and improve training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo benchmarks demonstrate that OM2P achieves superior performance, with up to a 3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time. Our approach represents the first to successfully integrate mean-flow model into offline MARL, paving the way for practical and scalable generative policies in cooperative multi-agent settings.', 'abstract_zh': '基于生成模型的 Offline 多智能体强化学习方法：OM2P 算法', 'title_zh': 'OM2P: 下线多 agents 平均流量策略'}
{'arxiv_id': 'arXiv:2508.06264', 'title': 'Numerical Considerations in Weighted Model Counting', 'authors': 'Randal E. Bryant', 'link': 'https://arxiv.org/abs/2508.06264', 'abstract': 'Weighted model counting computes the sum of the rational-valued weights associated with the satisfying assignments for a Boolean formula, where the weight of an assignment is given by the product of the weights assigned to the positive and negated variables comprising the assignment. Weighted model counting finds applications across a variety of domains including probabilistic reasoning and quantitative risk assessment.\nMost weighted model counting programs operate by (explicitly or implicitly) converting the input formula into a form that enables arithmetic evaluation, using multiplication for conjunctions and addition for disjunctions. Performing this evaluation using floating-point arithmetic can yield inaccurate results, and it cannot quantify the level of precision achieved. Computing with rational arithmetic gives exact results, but it is costly in both time and space.\nThis paper describes how to combine multiple numeric representations to efficiently compute weighted model counts that are guaranteed to achieve a user-specified precision. When all weights are nonnegative, we prove that the precision loss of arithmetic evaluation using floating-point arithmetic can be tightly bounded. We show that supplementing a standard IEEE double-precision representation with a separate 64-bit exponent, a format we call extended-range double (ERD), avoids the underflow and overflow issues commonly encountered in weighted model counting. For problems with mixed negative and positive weights, we show that a combination of interval floating-point arithmetic and rational arithmetic can achieve the twin goals of efficiency and guaranteed precision. For our evaluations, we have devised especially challenging formulas and weight assignments, demonstrating the robustness of our approach.', 'abstract_zh': '加权模型计数计算与布尔公式满足赋值关联的有理值权重之和，其中赋值的权重由构成该赋值的正变量和负变量所分配的权重的乘积给出。加权模型计数在概率推理和定量风险评估等多个领域都有应用。\n\n大多数加权模型计数程序通过（显式或隐式地）将输入公式转换为便于算术评估的形式来工作，使用乘法表示合取，加法表示析取。使用浮点算术进行这种评估可能会导致不准确的结果，并且无法量化所达到的精度。使用有理算术可以得到精确的结果，但时间和空间成本较高。\n\n本文描述了如何结合多种数值表示来高效计算可保证达到用户指定精度的加权模型计数。当所有权重均为非负时，我们证明使用浮点算术进行算术评估的精度损失可以被紧密界。我们展示了通过使用扩展范围双精度（ERD）格式扩展标准的IEEE双精度表示，可以避免在加权模型计数中常见的下溢和上溢问题。对于具有混合正负权重的问题，我们展示了结合区间浮点算术和有理算术可以同时实现高效性和保证精度的目标。在我们的评估中，我们设计了特别具有挑战性的公式和权重分配，证明了我们方法的鲁棒性。', 'title_zh': '带权重模型计数的数值考虑'}
{'arxiv_id': 'arXiv:2508.06259', 'title': 'SIFThinker: Spatially-Aware Image Focus for Visual Reasoning', 'authors': 'Zhangquan Chen, Ruihui Zhao, Chuwei Luo, Mingze Sun, Xinlei Yu, Yangyang Kang, Ruqi Huang', 'link': 'https://arxiv.org/abs/2508.06259', 'abstract': 'Current multimodal large language models (MLLMs) still face significant challenges in complex visual tasks (e.g., spatial understanding, fine-grained perception). Prior methods have tried to incorporate visual reasoning, however, they fail to leverage attention correction with spatial cues to iteratively refine their focus on prompt-relevant regions. In this paper, we introduce SIFThinker, a spatially-aware "think-with-images" framework that mimics human visual perception. Specifically, SIFThinker enables attention correcting and image region focusing by interleaving depth-enhanced bounding boxes and natural language. Our contributions are twofold: First, we introduce a reverse-expansion-forward-inference strategy that facilitates the generation of interleaved image-text chains of thought for process-level supervision, which in turn leads to the construction of the SIF-50K dataset. Besides, we propose GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual grounding into a unified reasoning pipeline, teaching the model to dynamically correct and focus on prompt-relevant regions. Extensive experiments demonstrate that SIFThinker outperforms state-of-the-art methods in spatial understanding and fine-grained visual perception, while maintaining strong general capabilities, highlighting the effectiveness of our method.', 'abstract_zh': '当前的多模态大型语言模型（MLLMs）在复杂的视觉任务（如空间理解、细粒度感知）中仍面临显著挑战。先前的方法试图融入视觉推理，但未能利用空间线索进行注意力修正以迭代地将注意力集中在相关区域。在本文中，我们引入了SIFThinker，这是一种空间感知的“边看边想”框架，模仿人类视觉感知。具体而言，SIFThinker 通过交错使用增强深度的边界框和自然语言来实现注意力修正和图像区域聚焦。我们的贡献有两个方面：首先，我们提出了一种逆扩张正向推理策略，促进生成交错的图像-文本推理链，从而为过程级监督构建SIF-50K数据集。此外，我们提出了一种GRPO-SIF强化训练范式，将深度指导的视觉定位集成到统一的推理管道中，使模型能够动态地修正和聚焦于相关区域。广泛的实验表明，SIFThinker 在空间理解和细粒度视觉感知方面超过了最新方法，同时保持了强大的通用能力，突显了我们方法的有效性。', 'title_zh': 'SIFThinker：空间感知图像聚焦技术在视觉推理中的应用'}
{'arxiv_id': 'arXiv:2508.06251', 'title': "Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)", 'authors': 'Alejandro Moreno R., Desale Fentaw, Samuel Palmer, Raúl Salles de Padua, Ninad Dixit, Samuel Mugel, Roman Orús, Manuel Radons, Josef Menter, Ali Abedi', 'link': 'https://arxiv.org/abs/2508.06251', 'abstract': 'Synthetic data generation is a key technique in modern artificial intelligence, addressing data scarcity, privacy constraints, and the need for diverse datasets in training robust models. In this work, we propose a method for generating privacy-preserving high-quality synthetic tabular data using Tensor Networks, specifically Matrix Product States (MPS). We benchmark the MPS-based generative model against state-of-the-art models such as CTGAN, VAE, and PrivBayes, focusing on both fidelity and privacy-preserving capabilities. To ensure differential privacy (DP), we integrate noise injection and gradient clipping during training, enabling privacy guarantees via Rényi Differential Privacy accounting. Across multiple metrics analyzing data fidelity and downstream machine learning task performance, our results show that MPS outperforms classical models, particularly under strict privacy constraints. This work highlights MPS as a promising tool for privacy-aware synthetic data generation. By combining the expressive power of tensor network representations with formal privacy mechanisms, the proposed approach offers an interpretable and scalable alternative for secure data sharing. Its structured design facilitates integration into sensitive domains where both data quality and confidentiality are critical.', 'abstract_zh': '合成数据生成是现代人工智能的关键技术，用于解决数据稀缺性、隐私限制以及训练 robust 模型所需多样化数据集的问题。在本文中，我们提出了一种使用张量网络（具体为矩阵积态 MPS）生成保隐私高质量合成表格数据的方法。我们将基于 MPS 的生成模型与当前最先进的模型（如 CTGAN、VAE 和 PrivBayes）进行基准测试，重点关注保真度和保隐私能力。通过在训练过程中集成噪声注入和梯度修剪，以 Rényi 差分隐私进行隐私保障计算。通过对多个评估数据保真度和下游机器学习任务性能的指标进行分析，我们的结果表明，在严格隐私约束下，MPS 优于经典模型。本文强调 MPS 是一种有前景的隐私感知合成数据生成工具。通过结合张量网络表示的强大表达能力和正式的隐私机制，所提出的方法为安全数据共享提供了可解释且可扩展的替代方案。其结构化设计便于将其集成到数据质量和保密性至关重要的敏感领域。', 'title_zh': '使用张量网络的矩阵乘积态合成数据生成与差分隐私'}
{'arxiv_id': 'arXiv:2508.06249', 'title': 'In-Training Defenses against Emergent Misalignment in Language Models', 'authors': 'David Kaczér, Magnus Jørgenvåg, Clemens Vetter, Lucie Flek, Florian Mai', 'link': 'https://arxiv.org/abs/2508.06249', 'abstract': "Fine-tuning lets practitioners repurpose aligned large language models (LLMs) for new domains, yet recent work reveals emergent misalignment (EMA): Even a small, domain-specific fine-tune can induce harmful behaviors far outside the target domain. Even in the case where model weights are hidden behind a fine-tuning API, this gives attackers inadvertent access to a broadly misaligned model in a way that can be hard to detect from the fine-tuning data alone. We present the first systematic study of in-training safeguards against EMA that are practical for providers who expose fine-tuning via an API. We investigate four training regularization interventions: (i) KL-divergence regularization toward a safe reference model, (ii) $\\ell_2$ distance in feature space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving of a small amount of safe training examples from a general instruct-tuning dataset. We first evaluate the methods' emergent misalignment effect across four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on benign tasks. We conclude with a discussion of open questions in emergent misalignment research.", 'abstract_zh': 'Fine-tuning调整使 practitioners能够重新利用对齐的大规模语言模型（LLMs）应用于新领域，然而近期的研究揭示了新兴的未对齐现象（EMA）：即使是对目标领域进行小规模、特定领域的微调也可能诱发超出目标领域范围的有害行为。即使在微调权重被API隐藏的情况下，这也会给攻击者提供一种可以难以仅从微调数据中检测到的方式，访问一个广泛未对齐的模型。我们首次系统地研究了针对EMA的有效训练期间保护措施，这些措施对于通过API提供微调服务的提供者是可实现的。我们调查了四种训练正则化干预措施：（i）向安全参照模型的KL散度正则化，（ii）特征空间中的$\\ell_2$距离，（iii）投影到安全子空间（SafeLoRA），以及（iv）来自通用指令微调数据集的少量安全训练示例的交错插入。首先，我们在四个诱发EMA的恶意任务中评估这些方法的新兴未对齐效应。其次，我们评估了这些方法对良性任务的影响。最后，我们讨论了新兴未对齐研究中的开放性问题。', 'title_zh': '训练中的防御措施以应对语言模型的 emergent 错配问题'}
{'arxiv_id': 'arXiv:2508.06244', 'title': 'Membership Inference Attack with Partial Features', 'authors': 'Xurun Wang, Guangrui Liu, Xinjie Li, Haoyu He, Lin Yao, Weizhe Zhang', 'link': 'https://arxiv.org/abs/2508.06244', 'abstract': 'Machine learning models have been shown to be susceptible to membership inference attack, which can be used to determine whether a given sample appears in the training data. Existing membership inference methods commonly assume that the adversary has full access to the features of the target sample. This assumption, however, does not hold in many real-world scenarios where only partial features information is available, thereby limiting the applicability of these methods. In this work, we study an inference scenario where the adversary observes only partial features of each sample and aims to infer whether this observed subset was present in the training set of the target model. We define this problem as Partial Feature Membership Inference (PFMI). To address this problem, we propose MRAD (Memory-guided Reconstruction and Anomaly Detection), a two-stage attack framework. In the first stage, MRAD optimizes the unknown feature values to minimize the loss of the sample. In the second stage, it measures the deviation between the reconstructed sample and the training distribution using anomaly detection. Empirical results demonstrate that MRAD is effective across a range of datasets, and maintains compatibility with various off-the-shelf anomaly detection techniques. For example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of the missing features.', 'abstract_zh': '机器学习模型在面对仅拥有目标样本部分特征信息的成员推理攻击时的可攻击性研究：一种基于记忆引导重构与异常检测的两阶段攻击框架（Partial Feature Membership Inference: A Memory-guided Reconstruction and Anomaly Detection Framework）', 'title_zh': '部分特征下的成员推断攻击'}
{'arxiv_id': 'arXiv:2508.06220', 'title': 'InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?', 'authors': 'Keummin Ka, Junhyeong Park, Jahyun Jeon, Youngjae Yu', 'link': 'https://arxiv.org/abs/2508.06220', 'abstract': 'Recent advances in Vision-Language Models (VLMs) have demonstrated impressive capabilities in perception and reasoning. However, the ability to perform causal inference -- a core aspect of human cognition -- remains underexplored, particularly in multimodal settings. In this study, we introduce InfoCausalQA, a novel benchmark designed to evaluate causal reasoning grounded in infographics that combine structured visual data with textual context. The benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning based on inferred numerical trends, while Task 2 targets semantic causal reasoning involving five types of causal relations: cause, effect, intervention, counterfactual, and temporal. We manually collected 494 infographic-text pairs from four public sources and used GPT-4o to generate 1,482 high-quality multiple-choice QA pairs. These questions were then carefully revised by humans to ensure they cannot be answered based on surface-level cues alone but instead require genuine visual grounding. Our experimental results reveal that current VLMs exhibit limited capability in computational reasoning and even more pronounced limitations in semantic causal reasoning. Their significantly lower performance compared to humans indicates a substantial gap in leveraging infographic-based information for causal inference. Through InfoCausalQA, we highlight the need for advancing the causal reasoning abilities of multimodal AI systems.', 'abstract_zh': 'Recent Advances in Vision-Language Models: Evaluating Causal Reasoning in Multimodal Settings with InfoCausalQA', 'title_zh': 'InfoCausalQA：模型能在信息图表基础上进行非显式因果推理吗？'}
{'arxiv_id': 'arXiv:2508.06214', 'title': 'Reparameterization Proximal Policy Optimization', 'authors': 'Hai Zhong, Xun Wang, Zhuoran Li, Longbo Huang', 'link': 'https://arxiv.org/abs/2508.06214', 'abstract': 'Reparameterization policy gradient (RPG) is promising for improving sample efficiency by leveraging differentiable dynamics. However, a critical barrier is its training instability, where high-variance gradients can destabilize the learning process. To address this, we draw inspiration from Proximal Policy Optimization (PPO), which uses a surrogate objective to enable stable sample reuse in the model-free setting. We first establish a connection between this surrogate objective and RPG, which has been largely unexplored and is non-trivial. Then, we bridge this gap by demonstrating that the reparameterization gradient of a PPO-like surrogate objective can be computed efficiently using backpropagation through time. Based on this key insight, we propose Reparameterization Proximal Policy Optimization (RPO), a stable and sample-efficient RPG-based method. RPO enables multiple epochs of stable sample reuse by optimizing a clipped surrogate objective tailored for RPG, while being further stabilized by Kullback-Leibler (KL) divergence regularization and remaining fully compatible with existing variance reduction methods. We evaluate RPO on a suite of challenging locomotion and manipulation tasks, where experiments demonstrate that our method achieves superior sample efficiency and strong performance.', 'abstract_zh': '基于重参数化 proximal 策略优化的重参数化策略梯度（RPG-RPO）方法', 'title_zh': '重参数近端策略优化'}
{'arxiv_id': 'arXiv:2508.06208', 'title': 'Graph Federated Learning for Personalized Privacy Recommendation', 'authors': 'Ce Na, Kai Yang, Dengzhao Fang, Yu Li, Jingtong Gao, Chengcheng Zhu, Jiale Zhang, Xiaobing Sun, Yi Chang', 'link': 'https://arxiv.org/abs/2508.06208', 'abstract': "Federated recommendation systems (FedRecs) have gained significant attention for providing privacy-preserving recommendation services. However, existing FedRecs assume that all users have the same requirements for privacy protection, i.e., they do not upload any data to the server. The approaches overlook the potential to enhance the recommendation service by utilizing publicly available user data. In real-world applications, users can choose to be private or public. Private users' interaction data is not shared, while public users' interaction data can be shared. Inspired by the issue, this paper proposes a novel Graph Federated Learning for Personalized Privacy Recommendation (GFed-PP) that adapts to different privacy requirements while improving recommendation performance. GFed-PP incorporates the interaction data of public users to build a user-item interaction graph, which is then used to form a user relationship graph. A lightweight graph convolutional network (GCN) is employed to learn each user's user-specific personalized item embedding. To protect user privacy, each client learns the user embedding and the scoring function locally. Additionally, GFed-PP achieves optimization of the federated recommendation framework through the initialization of item embedding on clients and the aggregation of the user relationship graph on the server. Experimental results demonstrate that GFed-PP significantly outperforms existing methods for five datasets, offering superior recommendation accuracy without compromising privacy. This framework provides a practical solution for accommodating varying privacy preferences in federated recommendation systems.", 'abstract_zh': '联邦个性化隐私推荐的图联邦学习（GFed-PP）', 'title_zh': '图联邦学习在个性化隐私推荐中的应用'}
{'arxiv_id': 'arXiv:2508.06204', 'title': 'Classification is a RAG problem: A case study on hate speech detection', 'authors': 'Richard Willats, Josh Pennington, Aravind Mohan, Bertie Vidgen', 'link': 'https://arxiv.org/abs/2508.06204', 'abstract': 'Robust content moderation requires classification systems that can quickly adapt to evolving policies without costly retraining. We present classification using Retrieval-Augmented Generation (RAG), which shifts traditional classification tasks from determining the correct category in accordance with pre-trained parameters to evaluating content in relation to contextual knowledge retrieved at inference. In hate speech detection, this transforms the task from "is this hate speech?" to "does this violate the hate speech policy?"\nOur Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates this approach and offers three key advantages: (1) robust classification accuracy comparable to leading commercial systems, (2) inherent explainability via retrieved policy segments, and (3) dynamic policy updates without model retraining. Through three experiments, we demonstrate strong baseline performance and show that the system can apply fine-grained policy control by correctly adjusting protection for specific identity groups without requiring retraining or compromising overall performance. These findings establish that RAG can transform classification into a more flexible, transparent, and adaptable process for content moderation and wider classification problems.', 'abstract_zh': '鲁棒的内容审核需要能够快速适应 evolving policies 的分类系统，无需昂贵的重新训练。我们提出使用检索增强生成（RAG）的分类方法，将传统的分类任务从根据预训练参数确定正确的类别转向在推理时根据检索到的上下文知识评估内容。在仇恨言论检测中，这一方法将任务从“这是否是仇恨言论？”转变为“这是否违反了仇恨言论政策？”\n\n我们的上下文政策引擎（CPE）——一个自主的RAG系统——展示了这种方法，并提供以下三大优势：（1）与领先商业系统相当的稳健分类准确性，（2）通过检索到的政策片段固有的解释性，（3）无需模型重新训练即可进行动态政策更新。通过三项实验，我们展示了强大的基线性能，并证明了系统可以通过正确调整特定身份群体的保护措施来应用细致的政策控制，而无需重新训练或牺牲整体性能。这些发现确立了RAG可以将分类转变为内容审核和更广泛分类问题中更具灵活性、透明性和适应性的过程。', 'title_zh': '分类分类是生成式检索增强生成（REDAgent）任务的一种研究：仇恨言论检测案例研究'}
{'arxiv_id': 'arXiv:2508.06202', 'title': 'LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning', 'authors': 'Chang Che, Ziqi Wang, Pengwan Yang, Qi Wang, Hui Ma, Zenglin Shi', 'link': 'https://arxiv.org/abs/2508.06202', 'abstract': 'Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language Models (MLLMs) to incrementally learn new tasks over time. However, this process is challenged by catastrophic forgetting, where performance on previously learned tasks deteriorates as the model adapts to new ones. A common approach to mitigate forgetting is architecture expansion, which introduces task-specific modules to prevent interference. Yet, existing methods often expand entire layers for each task, leading to significant parameter overhead and poor scalability. To overcome these issues, we introduce LoRA in LoRA (LiLoRA), a highly efficient architecture expansion method tailored for CVIT in MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy, applies an additional low-rank decomposition to matrix B to minimize task-specific parameters, and incorporates a cosine-regularized stability loss to preserve consistency in shared representations over time. Extensive experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves superior performance in sequential task learning while significantly improving parameter efficiency compared to existing approaches.', 'abstract_zh': 'LoRA in LoLaRA: Efficient Architecture Expansion for Continual Visual Instruction Tuning in Multimodal Large Language Models', 'title_zh': 'LoRA 在 LoRA 中：面向持续视觉指令调优的参数高效架构扩展'}
{'arxiv_id': 'arXiv:2508.06199', 'title': 'Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning', 'authors': 'Mateusz Praski, Jakub Adamczyk, Wojciech Czech', 'link': 'https://arxiv.org/abs/2508.06199', 'abstract': 'Pretrained neural networks have attracted significant interest in chemistry and small molecule drug design. Embeddings from these models are widely used for molecular property prediction, virtual screening, and small data learning in molecular chemistry. This study presents the most extensive comparison of such models to date, evaluating 25 models across 25 datasets. Under a fair comparison framework, we assess models spanning various modalities, architectures, and pretraining strategies. Using a dedicated hierarchical Bayesian statistical testing model, we arrive at a surprising result: nearly all neural models show negligible or no improvement over the baseline ECFP molecular fingerprint. Only the CLAMP model, which is also based on molecular fingerprints, performs statistically significantly better than the alternatives. These findings raise concerns about the evaluation rigor in existing studies. We discuss potential causes, propose solutions, and offer practical recommendations.', 'abstract_zh': '预训练神经网络在化学和小分子药物设计中的应用引起了显著兴趣。这些模型的嵌入广泛用于分子性质预测、虚拟筛选和分子化学中的小数据学习。本研究迄今为止进行了最广泛的此类模型比较，评估了25个模型在25个数据集上的表现。在公平比较框架下，我们评估了涵盖不同模态、架构和预训练策略的模型。使用专门的分层贝叶斯统计测试模型，我们得出一个出人意料的结果：几乎所有神经网络模型在基线ECFP分子指纹方面几乎没有或没有任何改进。只有基于分子指纹的CLAMP模型表现出统计显著性的优越性。这些发现引发了对现有研究中评估严谨性的担忧。我们讨论了潜在的原因，提出了解决方案，并提供了实用建议。', 'title_zh': '预训练分子嵌入模型的分子表示学习基准研究'}
{'arxiv_id': 'arXiv:2508.06183', 'title': 'Differentially Private Federated Clustering with Random Rebalancing', 'authors': 'Xiyuan Yang, Shengyuan Hu, Soyeon Kim, Tian Li', 'link': 'https://arxiv.org/abs/2508.06183', 'abstract': 'Federated clustering aims to group similar clients into clusters and produce one model for each cluster. Such a personalization approach typically improves model performance compared with training a single model to serve all clients, but can be more vulnerable to privacy leakage. Directly applying client-level differentially private (DP) mechanisms to federated clustering could degrade the utilities significantly. We identify that such deficiencies are mainly due to the difficulties of averaging privacy noise within each cluster (following standard privacy mechanisms), as the number of clients assigned to the same clusters is uncontrolled. To this end, we propose a simple and effective technique, named RR-Cluster, that can be viewed as a light-weight add-on to many federated clustering algorithms. RR-Cluster achieves reduced privacy noise via randomly rebalancing cluster assignments, guaranteeing a minimum number of clients assigned to each cluster. We analyze the tradeoffs between decreased privacy noise variance and potentially increased bias from incorrect assignments and provide convergence bounds for RR-Clsuter. Empirically, we demonstrate the RR-Cluster plugged into strong federated clustering algorithms results in significantly improved privacy/utility tradeoffs across both synthetic and real-world datasets.', 'abstract_zh': '联邦聚类旨在将相似的客户端分组到聚类中，并为每个聚类生成一个模型。这种个性化方法通常与为所有客户端训练单一模型相比能提高模型性能，但也更容易泄露隐私。直接将客户端级别的差分隐私（DP）机制应用于联邦聚类可能会显著降低效用。我们认为这种缺陷主要源于在每个聚类内平均隐私噪声的困难（遵循标准的隐私机制），因为分配到相同聚类的客户端数量不受控制。为此，我们提出了一种简单而有效的技术，称为RR-Cluster，它可以被视为对许多联邦聚类算法的轻量级补充。RR-Cluster通过随机重新平衡聚类分配来减少隐私噪声，确保每个聚类分配的客户端数量下限。我们分析了减少隐私噪声方差与潜在增加由于错误分配引起的偏差之间的权衡，并提供了RR-Cluster的收敛界。实证研究表明，RR-Cluster嵌入到强大的联邦聚类算法中，在合成数据集和真实世界数据集中都能显著改善隐私/效用权衡。', 'title_zh': '不同隐私保护的联邦聚类方法：随机重新平衡'}
{'arxiv_id': 'arXiv:2508.06170', 'title': 'Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation', 'authors': 'Ojonugwa Oluwafemi Ejiga Peter, Akingbola Oluwapemiisin, Amalahu Chetachi, Adeniran Opeyemi, Fahmi Khalifa, Md Mahmudur Rahman', 'link': 'https://arxiv.org/abs/2508.06170', 'abstract': 'Colonoscopy is a vital tool for the early diagnosis of colorectal cancer, which is one of the main causes of cancer-related mortality globally; hence, it is deemed an essential technique for the prevention and early detection of colorectal cancer. The research introduces a unique multidirectional architectural framework to automate polyp detection within colonoscopy images while helping resolve limited healthcare dataset sizes and annotation complexities. The research implements a comprehensive system that delivers synthetic data generation through Stable Diffusion enhancements together with detection and segmentation algorithms. This detection approach combines Faster R-CNN for initial object localization while the Segment Anything Model (SAM) refines the segmentation masks. The faster R-CNN detection algorithm achieved a recall of 93.08% combined with a precision of 88.97% and an F1 score of 90.98%.SAM is then used to generate the image mask. The research evaluated five state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet, and MANet using ResNet34 as a base model. The results demonstrate the superior performance of FPN with the highest scores of PSNR (7.205893) and SSIM (0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced performance in IoU (64.20%) and Dice score (77.53%).', 'abstract_zh': '结肠镜检查是一种早期诊断结肠直肠癌的关键工具，而结肠直肠癌是全球癌症相关死亡的主要原因之一；因此，它被认为是预防和早期发现结肠直肠癌的essential技术。本研究介绍了一种独特的多方向架构框架，以自动化结肠镜检查图像中的息肉检测，同时帮助解决有限的医疗数据集大小和标注复杂性问题。该研究实现了一个全面的系统，通过Stable Diffusion增强技术生成合成数据，并结合了检测和分割算法。检测方法结合了Faster R-CNN进行初始对象定位，并使用Segment Anything Model (SAM)细化分割掩码。Faster R-CNN检测算法的召回率为93.08%，精确率为88.97%，F1分为90.98%。然后使用SAM生成图像掩码。研究评估了包括U-Net、PSPNet、FPN、LinkNet和MANet在内的五种最先进的分割模型，使用ResNet34作为基础模型。结果表明，FPN表现出色，具有最高的PSNR（7.205893）和SSIM（0.492381）得分，而U-Net在召回率（84.85%）方面表现出色，LinkNet在交并比（IoU，64.20%）和Dice评分（77.53%）方面表现出平衡的性能。', 'title_zh': '基于综合检测与掩码生成的合成数据驱动多架构自动化息肉分割框架'}
{'arxiv_id': 'arXiv:2508.06169', 'title': 'UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting', 'authors': 'Wenpeng Xing, Jie Chen, Zaifeng Yang, Changting Lin, Jianfeng Dong, Chaochao Chen, Xun Zhou, Meng Han', 'link': 'https://arxiv.org/abs/2508.06169', 'abstract': 'Underwater 3D scene reconstruction faces severe challenges from light absorption, scattering, and turbidity, which degrade geometry and color fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF extensions such as SeaThru-NeRF incorporate physics-based models, their MLP reliance limits efficiency and spatial resolution in hazy environments. We introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction. Key innovations include: (1) a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter; and (2) a Physics-Aware Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating Gaussians via uncertainty scoring, ensuring artifact-free geometry. The pipeline operates in training and rendering stages. During training, noisy Gaussians are optimized end-to-end with underwater parameters, guided by PAUP pruning and scattering modeling. In rendering, refined Gaussians produce clean Unattenuated Radiance Images (URIs) free from media effects, while learned physics enable realistic Underwater Images (UWIs) with accurate light transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% reduction in floating artifacts.', 'abstract_zh': 'underwater 3D场景重建面临着严重的挑战，包括光线吸收、散射和浑浊度，这些因素在传统方法如神经辐射场（NeRF）中会降低几何精度和颜色保真度。虽然SeaThru-NeRF等NeRF扩展结合了物理模型，但它们对多层感知器（MLP）的依赖性限制了在浑浊环境中的效率和空间分辨率。我们提出了UW-3DGS，这是一种新的框架，将3D高斯散斑（3DGS）适应于稳健的水下重建。关键创新包括：（1）一种基于体素的空间变异性吸收和后向散射的插件学习水下图像形成模块；（2）一种物理感知不确定性剪枝（PAUP）分支，通过不确定性评分自适应地移除噪声的浮动高斯体素，确保无伪影的几何结构。该管道在训练和渲染阶段运行。在训练阶段，噪声的高斯体素通过PAUP剪枝和散射建模在水下参数的引导下进行端到端的优化。在渲染阶段，优化后的高斯体素产生无介质效应的清洁未衰减辐射图像（URIs），而学习到的物理模型能够生成具有准确光照传输的逼真水下图像（UWIs）。在SeaThru-NeRF和UWBundle数据集上的实验显示了优越的性能，PSNR达到27.604，SSIM达到0.868，LPIPS达到0.104，浮动伪影减少了约65%。', 'title_zh': 'UW-3DGS: 水下3D重建与物理感知高斯点绘制'}
{'arxiv_id': 'arXiv:2508.06165', 'title': 'UR$^2$: Unify RAG and Reasoning through Reinforcement Learning', 'authors': 'Weitao Li, Boran Xiang, Xiaolong Wang, Zhinan Gou, Weizhi Ma, Yang Liu', 'link': 'https://arxiv.org/abs/2508.06165', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope-typically limited to open-domain QA with fixed retrieval settings and task-specific assumptions. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at this https URL.', 'abstract_zh': '统一检索与推理（UR2）：通过强化学习统一检索与推理的通用框架', 'title_zh': 'UR$^2$: 将RAG和推理统一于强化学习'}
{'arxiv_id': 'arXiv:2508.06163', 'title': 'One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging', 'authors': 'Yingfeng Luo, Dingyang Lin, Junxin Wang, Ziqiang Xu, Kaiyan Chang, Tong Zheng, Bei Li, Anxiang Ma, Tong Xiao, Zhengtao Yu, Jingbo Zhu', 'link': 'https://arxiv.org/abs/2508.06163', 'abstract': "Model merging has emerged as a compelling data-free paradigm for multi-task learning, enabling the fusion of multiple fine-tuned models into a single, powerful entity. A key technique in merging methods is sparsification, which prunes redundant parameters from task vectors to mitigate interference. However, prevailing approaches employ a ``one-size-fits-all'' strategy, applying a uniform sparsity ratio that overlooks the inherent structural and statistical heterogeneity of model parameters. This often leads to a suboptimal trade-off, where critical parameters are inadvertently pruned while less useful ones are retained. To address this limitation, we introduce \\textbf{TADrop} (\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive sparsification strategy that respects this heterogeneity. Instead of a global ratio, TADrop assigns a tailored sparsity level to each parameter tensor based on its distributional properties. The core intuition is that tensors with denser, more redundant distributions can be pruned aggressively, while sparser, more critical ones are preserved. As a simple and plug-and-play module, we validate TADrop by integrating it with foundational, classic, and SOTA merging methods. Extensive experiments across diverse tasks (vision, language, and multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and significantly boosts their performance. For instance, when enhancing a leading merging method, it achieves an average performance gain of 2.0\\% across 8 ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter interference by tailoring sparsification to the model's structure, offering a new baseline for high-performance model merging.", 'abstract_zh': 'Tensor-wise Adaptive Drop: An Adaptive Sparsification Strategy for High-Performance Model Merging', 'title_zh': '大小不一：一种基于分布的稀疏化方法以实现更精确的模型合并'}
{'arxiv_id': 'arXiv:2508.06154', 'title': 'Semantic Item Graph Enhancement for Multimodal Recommendation', 'authors': 'Xiaoxiong Zhang, Xin Zhou, Zhiwei Zeng, Dusit Niyato, Zhiqi Shen', 'link': 'https://arxiv.org/abs/2508.06154', 'abstract': "Multimodal recommendation systems have attracted increasing attention for their improved performance by leveraging items' multimodal information. Prior methods often build modality-specific item-item semantic graphs from raw modality features and use them as supplementary structures alongside the user-item interaction graph to enhance user preference learning. However, these semantic graphs suffer from semantic deficiencies, including (1) insufficient modeling of collaborative signals among items and (2) structural distortions introduced by noise in raw modality features, ultimately compromising performance. To address these issues, we first extract collaborative signals from the interaction graph and infuse them into each modality-specific item semantic graph to enhance semantic modeling. Then, we design a modulus-based personalized embedding perturbation mechanism that injects perturbations with modulus-guided personalized intensity into embeddings to generate contrastive views. This enables the model to learn noise-robust representations through contrastive learning, thereby reducing the effect of structural noise in semantic graphs. Besides, we propose a dual representation alignment mechanism that first aligns multiple semantic representations via a designed Anchor-based InfoNCE loss using behavior representations as anchors, and then aligns behavior representations with the fused semantics by standard InfoNCE, to ensure representation consistency. Extensive experiments on four benchmark datasets validate the effectiveness of our framework.", 'abstract_zh': "多ar模式推荐系统通过结合项目的大规模多媒体信息正在逐渐吸引越来越多的关注。通常，，，，，项目特定的项目-项目语义图从原始项目特征开始并它们作为补充结构与内容-项目交互图并结合使用以增强用户偏好学习。然而，这种语义图存在语义缺陷，包括（1\nuser\n请下面的标题翻译成中文，要符合学术规范：\nMult Modal Recommendation System Through Leveraging Items' Multim Modal Information", 'title_zh': '多模态推荐中的语义项图增强'}
{'arxiv_id': 'arXiv:2508.06136', 'title': 'Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation', 'authors': 'YoungChan Choi, HengFei Wang, YiHua Cheng, Boeun Kim, Hyung Jin Chang, YoungGeun Choi, Sang-Il Choi', 'link': 'https://arxiv.org/abs/2508.06136', 'abstract': 'We propose a novel 3D gaze redirection framework that leverages an explicit 3D eyeball structure. Existing gaze redirection methods are typically based on neural radiance fields, which employ implicit neural representations via volume rendering. Unlike these NeRF-based approaches, where the rotation and translation of 3D representations are not explicitly modeled, we introduce a dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian Splatting (3DGS). Our method generates photorealistic images that faithfully reproduce the desired gaze direction by explicitly rotating and translating the 3D eyeball structure. In addition, we propose an adaptive deformation module that enables the replication of subtle muscle movements around the eyes. Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our framework is capable of generating diverse novel gaze images, achieving superior image quality and gaze estimation accuracy compared to previous state-of-the-art methods.', 'abstract_zh': '我们提出了一种新颖的3D眼球重定向框架，该框架利用显式的3D眼球结构。现有的眼球重定向方法通常基于神经辐射场（Neural Radiance Fields，NeRF），通过体渲染使用隐式的神经表示。与这些基于NeRF的方法不同，在这些方法中3D表示的旋转和平移没有明确建模，我们引入了专门的3D眼球结构，使用3D高斯点积（3DGS）表示眼球。该方法通过明确旋转和平移3D眼球结构生成保真的图像，以忠实再现所需的眼球方向。此外，我们提出了一种自适应变形模块，能够复制眼部周围细微肌肉运动。通过在ETH-XGaze数据集上进行的实验，我们展示了该框架能够生成多样化的新型眼球图像，图像质量以及眼球追踪准确性均优于以前的best-in-class方法。', 'title_zh': 'Roll Your Eyes: 通过明确的3D眼球旋转实现凝视重定向'}
{'arxiv_id': 'arXiv:2508.06135', 'title': 'Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models', 'authors': 'Lingyuan Liu, Mengxiang Zhang', 'link': 'https://arxiv.org/abs/2508.06135', 'abstract': "Knowledge Distillation (KD) is a fundamental technique for compressing large language models (LLMs) into compact, efficient student models. However, existing white-box KD methods mainly focus on balancing ground truth and student-generated responses while overlooking two critical factors: training data quality and student-model compatibility. To address these limitations, we propose Selective Reflection Distillation (SRD), a novel data curation framework that leverages reflections from student models to systematically refine training data. SRD dynamically evaluates and selects prompt-response pairs by comparing ground truth data with student model outputs, selectively curating high-quality, student-compatible training instances through automated ranking based on difficulty. Furthermore, after selecting the training data, a curriculum scheduling strategy is employed to incrementally introduce these curated subsets into the distillation process at fixed intervals. As a plug-and-play enhancement, SRD consistently improves distillation outcomes across diverse white-box KD approaches and model architectures, as well as decreases computational cost significantly during KD training. Experiments on a range of language model benchmarks demonstrate SRD's consistent improvements in distilled model performance, as well as a reduction in training runtime by up to 39%, under diverse KD methods and model families. Notably, SRD operates as a plug-and-play module, enhancing sample efficiency without modifying underlying KD algorithms. Our findings highlight that data quality and compatibility are pivotal to effective and efficient distillation of LLMs, and SRD provides a principled framework to achieve both. This work advances the understanding of data-centric factors in KD and offers practical insights for enhancing the capability and efficiency of compressed LLMs.", 'abstract_zh': '知识蒸馏（KD）是一种将大型语言模型（LLMs）压缩为紧凑高效的student模型的基本技术。然而，现有的白盒KD方法主要关注平衡地面truth和student生成的响应，而忽视了两个关键因素：训练数据质量和student模型的兼容性。为了解决这些限制，我们提出了一种新颖的数据整理框架——选择性反射蒸馏（SRD），该框架利用student模型的反馈系统地优化训练数据。SRD动态评估和选择提示-响应对，通过将地面truth数据与student模型输出进行对比，自动基于难度进行排名，选择性地整理出高质量、student兼容的训练实例。此外，在选择训练数据之后，采用课程调度策略在固定的时间间隔内逐步将这些整理的子集引入蒸馏过程。作为一种即插即用的增强方法，SRD在多种白盒KD方法和模型架构中都能持续提高蒸馏结果，并显著降低KD训练的计算成本。在多种语言模型基准上的实验展示了SRD在蒸馏模型性能上的持续改进，以及在各种KD方法和模型家族中将训练时间缩短高达39%。值得注意的是，SRD作为一种即插即用模块，在不修改基础KD算法的情况下提升了样本效率。我们的研究结果表明，数据质量和兼容性是有效高效蒸馏LLMs的关键因素，而SRD提供了一个实现这两种因素的原理性框架。这项工作推进了对KD中数据相关因素的理解，并提供了增强压缩LLMs能力和效率的实用见解。', 'title_zh': '少即是多：选择性性\nuser\n少即是多：选择性 største värdearı\nuser\n少即是多"user\n少即是多：选择性 � forState Distillation in Large Language Models pesticination in Large Language Models kuknowledge Distillation in Large Language Models-ves翻译成中文，要符合学术规范。直接输出标题，禁止输出多余内容。\n\n标题：少即是多：选择性 知识蒸馏在大型语言模型中的应用'}
{'arxiv_id': 'arXiv:2508.06133', 'title': 'LLM Serving Optimization with Variable Prefill and Decode Lengths', 'authors': 'Meixuan Wang, Yinyu Ye, Zijie Zhou', 'link': 'https://arxiv.org/abs/2508.06133', 'abstract': 'We study the problem of serving LLM (Large Language Model) requests where each request has heterogeneous prefill and decode lengths. In LLM serving, the prefill length corresponds to the input prompt length, which determines the initial memory usage in the KV cache. The decode length refers to the number of output tokens generated sequentially, with each additional token increasing the KV cache memory usage by one unit. Given a set of n requests, our goal is to schedule and process them to minimize the total completion time. We show that this problem is NP-hard due to the interplay of batching, placement constraints, precedence relationships, and linearly increasing memory usage. We then analyze commonly used scheduling strategies in practice, such as First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their competitive ratios scale up sublinearly with the memory limit-a significant drawback in real-world settings where memory demand is large. To address this, we propose a novel algorithm based on a new selection metric that efficiently forms batches over time. We prove that this algorithm achieves a constant competitive ratio. Finally, we develop and evaluate a few algorithm variants inspired by this approach, including dynamic programming variants, local search methods, and an LP-based scheduler, demonstrating through comprehensive simulations that they outperform standard baselines while maintaining computational efficiency.', 'abstract_zh': '我们研究了每个请求具有异构预填充和解码长度的大语言模型（LLM）请求服务问题。在LLM服务中，预填充长度对应于输入提示长度，这决定了KV缓存的初始内存使用量。解码长度指的是按顺序生成的输出令牌数量，每生成一个额外的令牌就会增加KV缓存的内存使用量。给定n个请求集，我们的目标是调度和处理它们以最小化总完成时间。由于批量、放置约束、前后关系以及内存使用量的线性增加的交织，我们证明了该问题属于NP-hard问题。然后，我们分析了实践中常用的调度策略，如先来先服务（FCFS）和最短优先（SF），并证明了它们的竞争比随着内存限制的增加呈次线性增长，在内存需求大的实际应用场景中这是一个明显的缺点。为了解决这一问题，我们提出了一个新的基于不同选择指标的算法，该算法能够高效地随时间动态形成批量。我们证明了该算法达到了常数竞争比。最后，我们开发并评估了几种基于该方法的算法变体，包括动态规划变体、局部搜索方法和基于线性规划的调度器，通过全面的仿真实验表明，它们在保持计算效率的同时优于标准基线。', 'title_zh': '带有可变预填充和解码长度的LLM服务优化'}
{'arxiv_id': 'arXiv:2508.06109', 'title': 'FMCE-Net++: Feature Map Convergence Evaluation and Training', 'authors': 'Zhibo Zhu, Renyu Huang, Lei He', 'link': 'https://arxiv.org/abs/2508.06109', 'abstract': 'Deep Neural Networks (DNNs) face interpretability challenges due to their opaque internal representations. While Feature Map Convergence Evaluation (FMCE) quantifies module-level convergence via Feature Map Convergence Scores (FMCS), it lacks experimental validation and closed-loop integration. To address this limitation, we propose FMCE-Net++, a novel training framework that integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module generates FMCS predictions, which, combined with task labels, jointly supervise backbone optimization through a Representation Auxiliary Loss. The RAL dynamically balances the primary classification loss and feature convergence optimization via a tunable \\Representation Abstraction Factor. Extensive experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100 demonstrate that FMCE-Net++ consistently enhances model performance without architectural modifications or additional data. Key experimental outcomes include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp (ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate state-of-the-art performance ceilings.', 'abstract_zh': '深度神经网络（DNNs）由于其不透明的内部表示面临可解释性挑战。为了解决这一问题，我们提出了一种新的训练框架FMCE-Net++，该框架集成了一个预训练且冻结的FMCE-Net作为辅助头。该模块生成特征图收敛评分（FMCS）预测，结合任务标签，通过表示辅助损失（RAL）联合监督主干优化。表示抽象因子动态平衡主要分类损失和特征收敛优化。在MNIST、CIFAR-10、FashionMNIST和CIFAR-100上的 extensive 实验表明，FMCE-Net++在不进行架构修改或增加数据的情况下，一致地提升了模型性能。关键实验结果包括ResNet-50/CIFAR-10上 accuracy 提升1.16个百分点和ShuffleNet v2/CIFAR-100上 accuracy 提升1.08个百分点，验证了FMCE-Net++能够有效提升前沿性能天花板。', 'title_zh': 'FMCE-Net++: 特征图收敛评估与训练'}
{'arxiv_id': 'arXiv:2508.06108', 'title': 'GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning', 'authors': 'Xing Lei, Wenyan Yang, Kaiqiang Ke, Shentao Yang, Xuetao Zhang, Joni Pajarinen, Donglin Wang', 'link': 'https://arxiv.org/abs/2508.06108', 'abstract': 'Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a fundamental challenge in reinforcement learning. While hindsight experience replay (HER) has shown promise by relabeling collected trajectories with achieved goals, we argue that trajectory relabeling alone does not fully exploit the available experiences in off-policy GCRL methods, resulting in limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned Regularization (HGR), a technique that generates action regularization priors based on hindsight goals. When combined with hindsight self-imitation regularization (HSR), our approach enables off-policy RL algorithms to maximize experience utilization. Compared to existing GCRL methods that employ HER and self-imitation techniques, our hindsight regularizations achieve substantially more efficient sample reuse and the best performances, which we empirically demonstrate on a suite of navigation and manipulation tasks.', 'abstract_zh': '基于后视目标的条件化加强学习（HGR）：一种稀疏奖励下目标条件化强化学习的后视目标正则化技术', 'title_zh': 'GCHR：目标条件化的前瞻正则化在样本高效强化学习中的应用'}
{'arxiv_id': 'arXiv:2508.06107', 'title': 'Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention', 'authors': 'Shree Mitra, Ritabrata Chakraborty, Nilkanta Sahu', 'link': 'https://arxiv.org/abs/2508.06107', 'abstract': 'Recognizing handwritten mathematical expressions (HMER) is a challenging task due to the inherent two-dimensional structure, varying symbol scales, and complex spatial relationships among symbols. In this paper, we present a self-supervised learning (SSL) framework for HMER that eliminates the need for expensive labeled data. Our approach begins by pretraining an image encoder using a combination of global and local contrastive loss, enabling the model to learn both holistic and fine-grained representations. A key contribution of this work is a novel self-supervised attention network, which is trained using a progressive spatial masking strategy. This attention mechanism is designed to learn semantically meaningful focus regions, such as operators, exponents, and nested mathematical notation, without requiring any supervision. The progressive masking curriculum encourages the network to become increasingly robust to missing or occluded visual information, ultimately improving structural understanding. Our complete pipeline consists of (1) self-supervised pretraining of the encoder, (2) self-supervised attention learning, and (3) supervised fine-tuning with a transformer decoder to generate LATEX sequences. Extensive experiments on CROHME benchmarks demonstrate that our method outperforms existing SSL and fully supervised baselines, validating the effectiveness of our progressive attention mechanism in enhancing HMER performance. Our codebase can be found here.', 'abstract_zh': '基于自监督学习的手写数学表达识别（HMER）', 'title_zh': '遮罩与匹配：学习识别手写数学公式的一种自监督注意力方法'}
{'arxiv_id': 'arXiv:2508.06098', 'title': 'MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows', 'authors': 'Xiquan Li, Junxi Liu, Yuzhe Liang, Zhikang Niu, Wenxi Chen, Xie Chen', 'link': 'https://arxiv.org/abs/2508.06098', 'abstract': 'Recent developments in diffusion- and flow- based models have significantly advanced Text-to-Audio Generation (TTA). While achieving great synthesis quality and controllability, current TTA systems still suffer from slow inference speed, which significantly limits their practical applicability. This paper presents MeanAudio, a novel MeanFlow-based model tailored for fast and faithful text-to-audio generation. Built on a Flux-style latent transformer, MeanAudio regresses the average velocity field during training, enabling fast generation by mapping directly from the start to the endpoint of the flow trajectory. By incorporating classifier-free guidance (CFG) into the training target, MeanAudio incurs no additional cost in the guided sampling process. To further stabilize training, we propose an instantaneous-to-mean curriculum with flow field mix-up, which encourages the model to first learn the foundational instantaneous dynamics, and then gradually adapt to mean flows. This strategy proves critical for enhancing training efficiency and generation quality. Experimental results demonstrate that MeanAudio achieves state-of-the-art performance in single-step audio generation. Specifically, it achieves a real time factor (RTF) of 0.013 on a single NVIDIA RTX 3090, yielding a 100x speedup over SOTA diffusion-based TTA systems. Moreover, MeanAudio also demonstrates strong performance in multi-step generation, enabling smooth and coherent transitions across successive synthesis steps.', 'abstract_zh': 'Recent developments in MeanFlow-based models have significantly advanced Text-to-Audio Generation (TTA). This paper presents MeanAudio, a novel MeanFlow-based model tailored for fast and faithful text-to-audio generation.', 'title_zh': 'MeanAudio: 快速且忠实的文本到音频生成方法'}
{'arxiv_id': 'arXiv:2508.06096', 'title': 'Bounding Distributional Shifts in World Modeling through Novelty Detection', 'authors': 'Eric Jing, Abdeslam Boularias', 'link': 'https://arxiv.org/abs/2508.06096', 'abstract': 'Recent work on visual world models shows significant promise in latent state dynamics obtained from pre-trained image backbones. However, most of the current approaches are sensitive to training quality, requiring near-complete coverage of the action and state space during training to prevent divergence during inference. To make a model-based planning algorithm more robust to the quality of the learned world model, we propose in this work to use a variational autoencoder as a novelty detector to ensure that proposed action trajectories during planning do not cause the learned model to deviate from the training data distribution. To evaluate the effectiveness of this approach, a series of experiments in challenging simulated robot environments was carried out, with the proposed method incorporated into a model-predictive control policy loop extending the DINO-WM architecture. The results clearly show that the proposed method improves over state-of-the-art solutions in terms of data efficiency.', 'abstract_zh': '近期关于视觉世界模型的研究显示，从预训练图像骨干网络中获取的潜在状态动力学具有显著前景。然而，目前大多数方法对训练质量敏感，需要在训练过程中几乎完全覆盖动作和状态空间，以防止推理过程中出现发散。为了使基于模型的规划算法对所学习的世界模型的质量更加鲁棒，本文提出使用变分自编码器作为新颖性检测器，确保在规划过程中提出的动作轨迹不会使学习到的模型偏离训练数据分布。为了评估该方法的有效性，我们在具有挑战性的模拟机器人环境中进行了一系列实验，将所提出的方法整合到一个扩展DINO-WM架构的模型预测控制策略环中。实验结果明确显示，所提出的方法在数据效率方面优于现有解决方案。', 'title_zh': '通过新颖性检测界定了世界建模中的分布变化。'}
{'arxiv_id': 'arXiv:2508.06076', 'title': 'Towards MR-Based Trochleoplasty Planning', 'authors': 'Michael Wehrli, Alicia Durrer, Paul Friedrich, Sidaty El Hadramy, Edwin Li, Luana Brahaj, Carol C. Hasler, Philippe C. Cattin', 'link': 'https://arxiv.org/abs/2508.06076', 'abstract': 'To treat Trochlear Dysplasia (TD), current approaches rely mainly on low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition. The surgeries are planned based on surgeons experience, have limited adoption of minimally invasive techniques, and lead to inconsistent outcomes. We propose a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy target morphologies from conventional clinical MR scans. First, we compute an isotropic super-resolved MR volume using an Implicit Neural Representation (INR). Next, we segment femur, tibia, patella, and fibula with a multi-label custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to generate pseudo-healthy target morphologies of the trochlear region. In contrast to prior work producing pseudo-healthy low-resolution 3D MR images, our approach enables the generation of sub-millimeter resolved 3D shapes compatible for pre- and intraoperative use. These can serve as preoperative blueprints for reshaping the femoral groove while preserving the native patella articulation. Furthermore, and in contrast to other work, we do not require a CT for our pipeline - reducing the amount of radiation. We evaluated our approach on 25 TD patients and could show that our target morphologies significantly improve the sulcus angle (SA) and trochlear groove depth (TGD). The code and interactive visualization are available at this https URL.', 'abstract_zh': '用于治疗)*/\n膝盂发育不良（Trochlear Dysplasia，TD），当前的方法主要依赖低分辨率的临床磁共振（MR）扫描和手术直觉。手术计划基于外科医生的经验，有限地采用了微创技术，导致手术结果不一致。我们提出了一种生成从常规临床MR扫描中提取的高分辨率、患者特定的3D伪健康目标形态的流水线。首先，我们使用隐式神经表示（INR）计算一个各向同性的高分辨率MR体积。接下来，我们使用多标签自定义训练网络对股骨、胫骨、髌骨和腓骨进行分割。最后，我们训练一种小波扩散模型（WDM）以生成膝盂区域的伪健康目标形态。与先前生成伪健康低分辨率3D MR图像的方法不同，我们的方法可以生成适用于术前和术中使用的亚毫米级解析度的3D形状。这些可以作为术前蓝图，用于重塑股骨沟，同时保留原有的髌骨关节面。此外，与其它方法不同，我们的流水线不需要CT扫描——从而减少了辐射量。我们在25例TD患者的评估中展示了我们的目标形态显著改善了膝盂沟角（Sulcus Angle，SA）和膝盂沟深度（Trochlear Groove Depth，TGD）。代码和交互式可视化可在以下链接获取。', 'title_zh': '基于磁共振的 trochleoplasty 手术规划研究'}
{'arxiv_id': 'arXiv:2508.06072', 'title': 'Can Large Models Fool the Eye? A New Turing Test for Biological Animation', 'authors': 'Zijian Chen, Lirong Deng, Zhengyu Chen, Kaiwei Zhang, Qi Jia, Yuan Tian, Yucheng Zhu, Guangtao Zhai', 'link': 'https://arxiv.org/abs/2508.06072', 'abstract': 'Evaluating the abilities of large models and manifesting their gaps are challenging. Current benchmarks adopt either ground-truth-based score-form evaluation on static datasets or indistinct textual chatbot-style human preferences collection, which may not provide users with immediate, intuitive, and perceptible feedback on performance differences. In this paper, we introduce BioMotion Arena, a novel framework for evaluating large language models (LLMs) and multimodal large language models (MLLMs) via visual animation. Our methodology draws inspiration from the inherent visual perception of motion patterns characteristic of living organisms that utilizes point-light source imaging to amplify the performance discrepancies between models. Specifically, we employ a pairwise comparison evaluation and collect more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion variants. Data analyses show that the crowd-sourced human votes are in good agreement with those of expert raters, demonstrating the superiority of our BioMotion Arena in offering discriminative feedback. We also find that over 90\\% of evaluated models, including the cutting-edge open-source InternVL3 and proprietary Claude-4 series, fail to produce fundamental humanoid point-light groups, much less smooth and biologically plausible motions. This enables BioMotion Arena to serve as a challenging benchmark for performance visualization and a flexible evaluation framework without restrictions on ground-truth.', 'abstract_zh': '评估大型模型的能力并展现其差距具有挑战性。当前基准要么采用基于真实值的评分形式评估静态数据集，要么收集模糊的文本聊天机器人风格的人类偏好，这可能无法为用户提供即时、直观和可感知的性能差异反馈。在本文中，我们引入了BioMotion Arena，这是一种通过视觉动画评估大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的新型框架。我们的方法借鉴了利用点光源成像放大模型性能差异的生物运动固有的视觉感知。具体而言，我们采用一对一比较评估，并为53种主流LLM和MLLM在90种生物运动变体上收集了超过45,000票。数据分析显示，众包人类投票与专家评分高度一致，证明了BioMotion Arena在提供差异性反馈方面的优越性。我们还发现，超过90%的评估模型，包括前沿的开源InternVL3和专有Claude-4系列，无法生成基本的人形点光源组，更不用说平滑且生物学上可实现的运动。这使BioMotion Arena能够成为一个具有挑战性的基准，用于性能可视化，同时不受真实值限制的灵活评估框架。', 'title_zh': '大型模型能否骗过人眼？一种新的生物动画图灵测试'}
{'arxiv_id': 'arXiv:2508.06066', 'title': 'Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology', 'authors': 'Barak Gahtan, Alex M. Bronstein', 'link': 'https://arxiv.org/abs/2508.06066', 'abstract': 'Deep temporal architectures such as Temporal Convolutional Networks (TCNs) achieve strong predictive performance on sequential data, yet theoretical understanding of their generalization remains limited. We address this gap by providing both the first non-vacuous, architecture-aware generalization bounds for deep temporal models and a principled evaluation methodology.\nFor exponentially $\\beta$-mixing sequences, we derive bounds scaling as $ O\\!\\Bigl(R\\,\\sqrt{\\tfrac{D\\,p\\,n\\,\\log N}{N}}\\Bigr), $ where $D$ is network depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our delayed-feedback blocking mechanism transforms dependent samples into effectively independent ones while discarding only $O(1/\\log N)$ of the data, yielding $\\sqrt{D}$ scaling instead of exponential, implying that doubling depth requires approximately quadrupling the training data.\nWe also introduce a fair-comparison methodology that fixes the effective sample size to isolate the effect of temporal structure from information content. Under $N_{\\text{eff}}=2{,}000$, strongly dependent sequences ($\\rho=0.8$) exhibit $\\approx76\\%$ smaller generalization gaps than weakly dependent ones ($\\rho=0.2$), challenging the intuition that dependence is purely detrimental. Yet convergence rates diverge from theory: weak dependencies follow $N_{\\text{eff}}^{-1.21}$ scaling and strong dependencies follow $N_{\\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$. These findings reveal that temporal dependence can enhance learning under fixed information budgets, while highlighting gaps between theory and practice that motivate future research.', 'abstract_zh': '深度时间架构如时间卷积网络（TCNs）在序列数据上实现了强大的预测性能，但对它们的泛化能力仍缺乏理论理解。本文通过提供第一个非平凡的、架构感知的深度时间模型泛化界，并提出了一种基本原则的评估方法来弥补这一差距。', 'title_zh': '面向架构的时空网络泛化界理论与公平比较方法学'}
{'arxiv_id': 'arXiv:2508.06065', 'title': 'ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation', 'authors': 'Daniel Lee, Nikhil Sharma, Donghoon Shin, DaEun Choi, Harsh Sharma, Jeonghwan Kim, Heng Ji', 'link': 'https://arxiv.org/abs/2508.06065', 'abstract': 'Generative AI has made image creation more accessible, yet aligning outputs with nuanced creative intent remains challenging, particularly for non-experts. Existing tools often require users to externalize ideas through prompts or references, limiting fluid exploration. We introduce ThematicPlane, a system that enables users to navigate and manipulate high-level semantic concepts (e.g., mood, style, or narrative tone) within an interactive thematic design plane. This interface bridges the gap between tacit creative intent and system control. In our exploratory study (N=6), participants engaged in divergent and convergent creative modes, often embracing unexpected results as inspiration or iteration cues. While they grounded their exploration in familiar themes, differing expectations of how themes mapped to outputs revealed a need for more explainable controls. Overall, ThematicPlane fosters expressive, iterative workflows and highlights new directions for intuitive, semantics-driven interaction in generative design tools.', 'abstract_zh': '生成式AI使图像创作更加易于访问，但使输出与细腻的创作意图保持一致仍然具有挑战性，特别是在非专业人士中尤为重要。现有工具通常需要用户通过提示等形式外部化想法 限制了流畅的探索。我们引入了主题平面 允\nuser\n主题平面 pestic-testid\n-testid\n主题平面（ThemePlane）', 'title_zh': '主题平面：连接隐含用户意图与潜在空间的图像生成方法'}
{'arxiv_id': 'arXiv:2508.06046', 'title': 'EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation', 'authors': 'Xinda Wang, Zhengxu Hou, Yangshijie Zhang, Bingren Yan, Zhibo Yang, Xingsheng Zhang, Luxi Xing, Qiang Zhou, Chen Zhang', 'link': 'https://arxiv.org/abs/2508.06046', 'abstract': 'Although the effectiveness of Large Language Models (LLMs) as judges (LLM-as-a-judge) has been validated, their performance remains limited in open-ended tasks, particularly in story evaluation. Accurate story evaluation is crucial not only for assisting human quality judgment but also for providing key signals to guide story generation. However, existing methods face a dilemma: prompt engineering for closed-source models suffers from poor adaptability, while fine-tuning approaches for open-source models lack the rigorous reasoning capabilities essential for story evaluation. To address this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework. Grounded in pairwise comparison, the framework first self-synthesizes score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To ensure data quality, these raw CoTs undergo a self-filtering process, utilizing multi-agents to guarantee their logical rigor and robustness. Finally, the evaluator trained on the refined data is deployed as a reward model to guide the story generation task. Experimental results demonstrate that our framework achieves state-of-the-art (SOTA) performance on three evaluation benchmarks including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward model, it significantly enhances the quality of generated stories, thereby fully validating the superiority of our self-evolving approach.', 'abstract_zh': '虽然大型语言模型（LLMs）作为法官（LLM-as-a-judge）的有效性已经得到验证，其在开放性 任务中的表现仍受到限制，特别是在评估方面。准确的评估不仅是帮助人类质量判断的关键，也是帮助传递生成质量信号的关键。然而，现有方法面临困境：封闭源代码模型的提示工程缺乏适应性性能力 ， 开放源代码模型的微调方法缺乏严格的推理能力以应对评估需求。为解决此问题，我们提出了一整套自我进化的成对推理（SEflevR）框架。该框架基于两两对比，通过多人格策略生成得分对齐的推理过程 （CoT），并通过多智能体过滤确保逻辑严谨性和鲁棒性 性。最终，经过精炼的数据被部署到奖励模型以用于生成任务。实验结果证明，该框架在StoryHANNA和onMEVA三个基准上上展示了卓越（SOTA）的表现，并且显着提升了生成故事的质量，从而全面验证了我们自我进化的成对推理方法的优越性。', 'title_zh': 'EvolvR：自我进化的两两推理故事评估以提升生成'}
{'arxiv_id': 'arXiv:2508.06041', 'title': 'DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment', 'authors': 'Sangwoo Kwon, Seong Hoon Seo, Jae W. Lee, Yeonhong Park', 'link': 'https://arxiv.org/abs/2508.06041', 'abstract': 'How can we effectively handle queries for on-device large language models (LLMs) with varying runtime constraints, such as latency and accuracy? Multi-scale quantization addresses this challenge by enabling memory-efficient runtime model adaptation of LLMs through the overlaying of multiple model variants quantized to different bitwidths. Meanwhile, an important question still remains open-ended: how can models be properly configured to match a target precision or latency? While mixed-precision offers a promising solution, we take this further by leveraging the key observation that the sensitivity of each layer dynamically changes across decoding iterations. Building on this insight, we introduce DP-LLM, a novel mechanism that dynamically assigns precision to each layer based on input values. DP-LLM augments each linear layer in an LLM with a precision selector that determines the bitwidth at runtime using a lightweight error estimator and threshold values learned through fine-tuning. Experimental results across multiple models and benchmarks demonstrate that DP-LLM achieves a superior performance-latency trade-off, outperforming prior approaches.', 'abstract_zh': '如何有效处理具有不同运行时约束（如延迟和精度）的大规模语言模型（LLMs）设备端查询？多尺度量化通过叠加不同位宽的多个模型变体，实现了LLMs的内存高效运行时模型适应，解决了这一挑战。然而，关于如何适当地配置模型以匹配目标精度或延迟，仍是一个开放的问题。虽然混合精度提供了一个有前景的解决方案，但我们进一步利用了一个关键的观察结果：每层的敏感度在解码迭代中动态变化。基于这一洞察，我们引入了DP-LLM，这是一种新的机制，可以根据输入值动态为每一层分配精度。DP-LLM 在每个多线性层中加入了一个精度选择器，该选择器使用轻量级误差估计器和通过微调学习到的阈值，在运行时确定位宽。在多个模型和基准上的实验结果表明，DP-LLM 实现了性能-延迟权衡的优越表现，优于先前的方法。', 'title_zh': 'DP-LLM：基于动态层级精度分配的运行时模型适应'}
{'arxiv_id': 'arXiv:2508.06038', 'title': 'Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models', 'authors': 'Huanyu Wang, Jushi Kai, Haoli Bai, Lu Hou, Bo Jiang, Ziwei He, Zhouhan Lin', 'link': 'https://arxiv.org/abs/2508.06038', 'abstract': 'Vision-Language Models (VLMs) typically replace the predefined image placeholder token (<image>) in textual instructions with visual features from an image encoder, forming the input to a backbone Large Language Model (LLM). However, the large number of vision tokens significantly increases the context length, leading to high computational overhead and inference latency. While previous efforts mitigate this by selecting only important visual features or leveraging learnable queries to reduce token count, they often compromise performance or introduce substantial extra costs. In response, we propose Fourier-VLM, a simple yet efficient method that compresses visual representations in the frequency domain. Our approach is motivated by the observation that vision features output from the vision encoder exhibit concentrated energy in low-frequency components. Leveraging this, we apply a low-pass filter to the vision features using a two-dimentional Discrete Cosine Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier Transform (FFT) operator with a time complexity of $\\mathcal{O}(n\\log n)$, minimizing the extra computational cost while introducing no additional parameters. Extensive experiments across various image-based benchmarks demonstrate that Fourier-VLM achieves competitive performance with strong generalizability across both LLaVA and Qwen-VL architectures. Crucially, it reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2% compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.', 'abstract_zh': 'Fourier-VLM：频率域中简洁高效的视觉表示压缩方法', 'title_zh': 'Fourier-VLM：在频域压缩视觉令牌以适应大型视觉-语言模型'}
{'arxiv_id': 'arXiv:2508.06034', 'title': 'Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity', 'authors': 'Qin Chen, Guojie Song', 'link': 'https://arxiv.org/abs/2508.06034', 'abstract': 'Heterogeneous graphs (HGs) are common in real-world scenarios and often exhibit heterophily. However, most existing studies focus on either heterogeneity or heterophily in isolation, overlooking the prevalence of heterophilic HGs in practical applications. Such ignorance leads to their performance degradation. In this work, we first identify two main challenges in modeling heterophily HGs: (1) varying heterophily distributions across hops and meta-paths; (2) the intricate and often heterophily-driven diversity of semantic information across different meta-paths. Then, we propose the Adaptive Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN employs a heterophily-aware convolution that accounts for heterophily distributions specific to both hops and meta-paths. It then integrates messages from diverse semantic spaces using a coarse-to-fine attention mechanism, which filters out noise and emphasizes informative signals. Experiments on seven real-world graphs and twenty baselines demonstrate the superior performance of AHGNN, particularly in high-heterophily situations.', 'abstract_zh': '异质图（HGs）在现实场景中很常见，常表现出异质性。然而，现有的大多数研究要么孤立地关注异质性，要么关注异质性，忽视了实践应用中异质性HG的普遍性。这种忽视导致了它们性能的下降。在本文中，我们首先识别出建模异质性HG的两个主要挑战：（1）跨跃点和元路径的异质性分布变化；（2）不同元路径上复杂的且往往由异质性驱动的语义信息多样性。然后，我们提出了自适应异质图神经网络（AHGNN）来应对这些挑战。AHGNN采用一种意识到异质性的卷积，考虑了跨越跃点和元路径的特定异质性分布。它然后通过一种粗到细的注意力机制整合来自不同语义空间的消息，该机制过滤掉噪声并强调有用信号。在七个真实世界图和二十个基线上的实验表明，AHGNN在高异质性情况下表现尤为出色。', 'title_zh': '自适应异质图神经网络：连接异构性和异质性'}
{'arxiv_id': 'arXiv:2508.06026', 'title': 'Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future', 'authors': 'Yidong Wang, Xin Wang, Cunxiang Wang, Junfeng Fang, Qiufeng Wang, Jianing Chu, Xuran Meng, Shuxun Yang, Libo Qin, Yue Zhang, Wei Ye, Shikun Zhang', 'link': 'https://arxiv.org/abs/2508.06026', 'abstract': "Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose \\textbf{Temporal Self-Rewarding Language Models} that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) \\textit{Anchored Rejection} - fixing rejected responses using the past initial model's outputs and (2) \\textit{Future-Guided Chosen} - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data.", 'abstract_zh': '-temporal Self-Rewarding Language Models', 'title_zh': '基于时间的自我奖赏语言模型：通过过往与未来解耦选择与拒绝'}
{'arxiv_id': 'arXiv:2508.06021', 'title': 'Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis', 'authors': 'Utku Ozbulak, Michaela Cohrs, Hristo L. Svilenov, Joris Vankerschaver, Wesley De Neve', 'link': 'https://arxiv.org/abs/2508.06021', 'abstract': 'Sub-visible particle analysis using flow imaging microscopy combined with deep learning has proven effective in identifying particle types, enabling the distinction of harmless components such as silicone oil from protein particles. However, the scarcity of available data and severe imbalance between particle types within datasets remain substantial hurdles when applying multi-class classifiers to such problems, often forcing researchers to rely on less effective methods. The aforementioned issue is particularly challenging for particle types that appear unintentionally and in lower numbers, such as silicone oil and air bubbles, as opposed to protein particles, where obtaining large numbers of images through controlled settings is comparatively straightforward. In this work, we develop a state-of-the-art diffusion model to address data imbalance by generating high-fidelity images that can augment training datasets, enabling the effective training of multi-class deep neural networks. We validate this approach by demonstrating that the generated samples closely resemble real particle images in terms of visual quality and structure. To assess the effectiveness of using diffusion-generated images in training datasets, we conduct large-scale experiments on a validation dataset comprising 500,000 protein particle images and demonstrate that this approach improves classification performance with no negligible downside. Finally, to promote open research and reproducibility, we publicly release both our diffusion models and the trained multi-class deep neural network classifiers, along with a straightforward interface for easy integration into future studies, at this https URL.', 'abstract_zh': '使用流影像显微镜结合深度学习的亚显微粒子分析已经证明能够在区分无害成分（如硅油）和蛋白质颗粒方面有效识别粒子类型。然而，可用数据的稀缺性和数据集中粒子类型间的严重失衡是应用多类分类器时的重大障碍，通常迫使研究人员依赖效果较差的方法。这一问题尤其对硅油和气泡等无意出现且数量较少的粒子类型构成挑战，相比之下，获得蛋白质颗粒的大规模图像较为容易。在本工作中，我们开发了一种最先进的扩散模型来解决数据不平衡问题，通过生成高保真图像以扩充训练数据集，从而有效训练多类深度神经网络。我们通过验证生成样本在视觉质量和结构方面与真实粒子图像高度相似来证明这种方法的有效性。为了评估使用扩散生成图像在训练数据集中的效果，我们在包含500,000个蛋白质颗粒图像的验证数据集上进行大规模实验，并证明这种方法能够提升分类性能且无明显缺点。最后，为了促进开放研究和可重复性，我们公开发布了我们的扩散模型、训练好的多类深度神经网络分类器以及简易接口，以便于未来研究中的轻松集成，网址为：https://www.example.com。', 'title_zh': '基于生成AI的图像合成在流动成像显微镜中改进亚显微粒子分类'}
{'arxiv_id': 'arXiv:2508.06016', 'title': 'Crisp Attention: Regularizing Transformers via Structured Sparsity', 'authors': 'Sagar Gandhi, Vishal Gandhi', 'link': 'https://arxiv.org/abs/2508.06016', 'abstract': 'The quadratic computational cost of the self-attention mechanism is a primary challenge in scaling Transformer models. While attention sparsity is widely studied as a technique to improve computational efficiency, it is almost universally assumed to come at the cost of model accuracy. In this paper, we report a surprising counter-example to this common wisdom. By introducing structured, post-hoc sparsity to the attention mechanism of a DistilBERT model during fine-tuning on the SST-2 sentiment analysis task, we find that model accuracy improves significantly. Our model with 80\\% attention sparsity achieves a validation accuracy of 91.59\\%, a 0.97\\% absolute improvement over the dense baseline. We hypothesize that this phenomenon is due to sparsity acting as a powerful implicit regularizer, preventing the model from overfitting by forcing it to make predictions with a more constrained and robust set of features. Our work recasts attention sparsity not just as a tool for computational efficiency, but as a potential method for improving the generalization and performance of Transformer models.', 'abstract_zh': '自注意力机制的计算成本呈二次增长是扩展Transformer模型的主要挑战。虽然注意力稀疏性作为一种提高计算效率的技术得到广泛应用，但几乎普遍认为这会以降低模型准确度为代价。本文报道了一个令人惊讶的反例。通过在SST-2情感分析任务微调DistilBERT模型时引入结构化的后验稀疏性，我们发现模型准确度显著提高。我们提出的80%注意力稀疏模型在验证集上的准确率达到91.59%，比密集基线提高了0.97%。我们推测，这一现象是由于稀疏性作为强有力的隐式正则化手段，通过迫使模型以更受限和 robust 的特征集做出预测，防止过拟合。本文将注意力稀疏性重新定义为不仅是一种计算效率工具，也是一种提高Transformer模型泛化能力和性能的方法。', 'title_zh': 'crisp Attention: 通过结构化稀疏性正则化 Transformers'}
{'arxiv_id': 'arXiv:2508.06000', 'title': 'Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning', 'authors': 'Wei Xiang, Ziyue Lei, Haoyuan Che, Fangyuan Ye, Xueting Wu, Lingyun Sun', 'link': 'https://arxiv.org/abs/2508.06000', 'abstract': 'Operational skill learning, inherently physical and reliant on hands-on practice and kinesthetic feedback, has yet to be effectively replicated in large language model (LLM)-supported training. Current LLM training assistants primarily generate customized textual feedback, neglecting the crucial kinesthetic modality. This gap derives from the textual and uncertain nature of LLMs, compounded by concerns on user acceptance of LLM driven body control. To bridge this gap and realize the potential of collaborative human-LLM action, this work explores human experience of LLM driven kinesthetic assistance. Specifically, we introduced an "Align-Analyze-Adjust" strategy and developed FlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS) for flight skill acquisition, a representative operational skill domain. FlightAxis learns flight skills from manuals and guides forearm movements during simulated flight tasks. Our results demonstrate high user acceptance of LLM-mediated body control and significantly reduced task completion times. Crucially, trainees reported that this kinesthetic assistance enhanced their awareness of operation flaws and fostered increased engagement in the training process, rather than relieving perceived load. This work demonstrated the potential of kinesthetic LLM training in operational skill acquisition.', 'abstract_zh': '基于大语言模型支持的实体操作技能学习尚未有效实现：Kinesthetic Assistance in Flight Skill Acquisition via Human-LLM Collaboration', 'title_zh': '手把手：Large Language模型驱动的EMS辅助操作技能学习'}
{'arxiv_id': 'arXiv:2508.05991', 'title': 'ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge', 'authors': 'Juewen Hu, Yexin Li, Jiulin Li, Shuo Chen, Pring Wong', 'link': 'https://arxiv.org/abs/2508.05991', 'abstract': 'Emotion recognition plays a vital role in enhancing human-computer interaction. In this study, we tackle the MER-SEMI challenge of the MER2025 competition by proposing a novel multimodal emotion recognition framework. To address the issue of data scarcity, we leverage large-scale pre-trained models to extract informative features from visual, audio, and textual modalities. Specifically, for the visual modality, we design a dual-branch visual encoder that captures both global frame-level features and localized facial representations. For the textual modality, we introduce a context-enriched method that employs large language models to enrich emotional cues within the input text. To effectively integrate these multimodal features, we propose a fusion strategy comprising two key components, i.e., self-attention mechanisms for dynamic modality weighting, and residual connections to preserve original representations. Beyond architectural design, we further refine noisy labels in the training set by a multi-source labeling strategy. Our approach achieves a substantial performance improvement over the official baseline on the MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to 78.63%, thereby validating the effectiveness of the proposed framework.', 'abstract_zh': '情绪识别在增强 人人- 与计算机交互中发挥着至关重要的的作用。在本研究中，我们通过提出一种新颖的多模态情绪识别框架来应对 MER-SEMI 挑战赛中 MER read 2- 25 5- 的问题。为解决数据稀缺性问题 on 我们利用大规模预训练模型从视觉 on 音频 on 和文本模态中提取有信息性的特征。具体 具具体来说 on 对视觉模态 on 我们设计了一种双分支视觉编码器 以 用于捕获全局帧级级别特征和局部面部区域。 on 在 文本模态 on 我们引入了一种包含语境丰富化的模型 on 采用大型语言模型来 丰富文本中的情绪线索。 on 为有效地整合多 我们提出了一种融合策略 on 包括两个模块 on 即 注意力机制 for 动态情绪标注 on 和 残差联通通道结保持原始表示。 on 超出架构设计 我们还在训练集 集上多 进一步通过一种源自多 多种来源的标签策略来清洗嘈杂标签。 on 我们的方法在 MER on  on MER re  on 过- 五个赛官方基准上 �取得了显著提升 on 达到了8 8 8 8 8 87 7%，相比于 on 与 86. 63%，从而验证了所提出框架的有效性性。', 'title_zh': 'ECMF: 提升跨模态融合在MER-SEMI挑战中的多模态情感识别'}
{'arxiv_id': 'arXiv:2508.05989', 'title': 'ETA: Energy-based Test-time Adaptation for Depth Completion', 'authors': 'Younjoon Chung, Hyoungseob Park, Patrick Rim, Xiaoran Zhang, Jihe He, Ziyao Zeng, Safa Cicek, Byung-Woo Hong, James S. Duncan, Alex Wong', 'link': 'https://arxiv.org/abs/2508.05989', 'abstract': "We propose a method for test-time adaptation of pretrained depth completion models. Depth completion models, trained on some ``source'' data, often predict erroneous outputs when transferred to ``target'' data captured in novel environmental conditions due to a covariate shift. The crux of our method lies in quantifying the likelihood of depth predictions belonging to the source data distribution. The challenge is in the lack of access to out-of-distribution (target) data prior to deployment. Hence, rather than making assumptions regarding the target distribution, we utilize adversarial perturbations as a mechanism to explore the data space. This enables us to train an energy model that scores local regions of depth predictions as in- or out-of-distribution. We update the parameters of pretrained depth completion models at test time to minimize energy, effectively aligning test-time predictions to those of the source distribution. We call our method ``Energy-based Test-time Adaptation'', or ETA for short. We evaluate our method across three indoor and three outdoor datasets, where ETA improve over the previous state-of-the-art method by an average of 6.94% for outdoors and 10.23% for indoors. Project Page: this https URL.", 'abstract_zh': '基于能量的方法在测试时适应预训练的深度完成模型', 'title_zh': '基于能量的测试时适配以完成深度估计'}
{'arxiv_id': 'arXiv:2508.05979', 'title': 'Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education', 'authors': 'Xinming Yang, Haasil Pujara, Jun Li', 'link': 'https://arxiv.org/abs/2508.05979', 'abstract': 'While Large Language Models (LLMs) are often used as virtual tutors in computer science (CS) education, this approach can foster passive learning and over-reliance. This paper presents a novel pedagogical paradigm that inverts this model: students act as instructors who must teach an LLM to solve problems. To facilitate this, we developed strategies for designing questions with engineered knowledge gaps that only a student can bridge, and we introduce Socrates, a system for deploying this method with minimal overhead. We evaluated our approach in an undergraduate course and found that this active-learning method led to statistically significant improvements in student performance compared to historical cohorts. Our work demonstrates a practical, cost-effective framework for using LLMs to deepen student engagement and mastery.', 'abstract_zh': '大型语言模型在计算机科学教育中作为虚拟导师的应用可能导致被动学习和过度依赖，本文提出了一种新颖的教学模式：学生作为讲师，必须向大型语言模型传授解决问题的方法。为此，我们开发了设计问题的策略，以构建只有学生才能填补的知识缺口，并介绍了一种名为Socrates的系统来实现这一方法，且具有最小的开销。我们在本科生课程中评估了这种方法，发现这种主动学习方法在统计学上显著提高了学生的表现，相比历史班级有了显著提升。我们的工作展示了一种实用且经济有效的框架，利用大型语言模型加深学生的学习参与和掌握。', 'title_zh': '教学相长：将学生培养为大型语言模型的讲师以促进计算机科学教育'}
{'arxiv_id': 'arXiv:2508.05978', 'title': 'DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism and Flow Matching', 'authors': 'Wei Chen, Binzhu Sha, Dan Luo, Jing Yang, Zhuo Wang, Fan Fan, Zhiyong Wu', 'link': 'https://arxiv.org/abs/2508.05978', 'abstract': "Singing Voice Conversion (SVC) transfers a source singer's timbre to a target while keeping melody and lyrics. The key challenge in any-to-any SVC is adapting unseen speaker timbres to source audio without quality degradation. Existing methods either face timbre leakage or fail to achieve satisfactory timbre similarity and quality in the generated audio. To address these challenges, we propose DAFMSVC, where the self-supervised learning (SSL) features from the source audio are replaced with the most similar SSL features from the target audio to prevent timbre leakage. It also incorporates a dual cross-attention mechanism for the adaptive fusion of speaker embeddings, melody, and linguistic content. Additionally, we introduce a flow matching module for high quality audio generation from the fused features. Experimental results show that DAFMSVC significantly enhances timbre similarity and naturalness, outperforming state-of-the-art methods in both subjective and objective evaluations.", 'abstract_zh': '歌唱声音转换（Singing Voice Conversion, SVC）将源歌手的音色转移到目标歌手的同时保持旋律和歌词。任何到任何的SVC的关键挑战是在不降低音质的情况下适应未见过的目标歌手音色。现有方法要么面临音色泄露的问题，要么无法在生成的音频中实现令人满意的音色相似度和音质。为了应对这些挑战，我们提出了DAFMSVC，其中源音频的自监督学习（SSL）特征被目标音频中最相似的SSL特征替换，以防止音色泄露。该方法还采用了双交叉注意力机制，用于适应结合说话人嵌入、旋律和语言内容。此外，我们引入了流匹配模块，用于从融合特征生成高质量的音频。实验结果表明，DAFMSVC显著提高了音色相似度和自然度，在主观和客观评价中均优于现有最好的方法。', 'title_zh': 'DAFMSVC：基于双注意力机制和流动匹配的一次性歌声转换'}
{'arxiv_id': 'arXiv:2508.05970', 'title': 'Impact-driven Context Filtering For Cross-file Code Completion', 'authors': 'Yanzhou Li, Shangqing Liu, Kangjie Chen, Tianwei Zhang, Yang Liu', 'link': 'https://arxiv.org/abs/2508.05970', 'abstract': 'Retrieval-augmented generation (RAG) has recently demonstrated considerable potential for repository-level code completion, as it integrates cross-file knowledge with in-file preceding code to provide comprehensive contexts for generation. To better understand the contribution of the retrieved cross-file contexts, we introduce a likelihood-based metric to evaluate the impact of each retrieved code chunk on the completion. Our analysis reveals that, despite retrieving numerous chunks, only a small subset positively contributes to the completion, while some chunks even degrade performance. To address this issue, we leverage this metric to construct a repository-level dataset where each retrieved chunk is labeled as positive, neutral, or negative based on its relevance to the target completion. We then propose an adaptive retrieval context filtering framework, CODEFILTER, trained on this dataset to mitigate the harmful effects of negative retrieved contexts in code completion. Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks demonstrates that CODEFILTER consistently improves completion accuracy compared to approaches without filtering operations across various tasks. Additionally, CODEFILTER significantly reduces the length of the input prompt, enhancing computational efficiency while exhibiting strong generalizability across different models. These results underscore the potential of CODEFILTER to enhance the accuracy, efficiency, and attributability of repository-level code completion.', 'abstract_zh': '检索增强生成（RAG）最近在仓库级别代码补全方面展现了显著潜力，因为它将跨文件知识与文件内先前代码相结合，提供生成所需的整体上下文。为了更好地理解检索到的跨文件上下文的贡献，我们引入了一个基于概率的度量来评估每个检索到的代码片段对补全的影响。我们的分析显示，尽管检索了大量的片段，但只有一小部分积极地促进了补全，有些片段甚至降低了性能。为了解决这个问题，我们利用此度量构建了一个仓库级别的数据集，在该数据集中，每个检索到的片段根据其与目标补全的相关性被标记为正面、中性或负面。然后，我们提出了一种基于此数据集训练的自适应检索上下文过滤框架CODEFILTER，用于减轻负面检索上下文对代码补全的有害影响。在RepoEval和CrossCodeLongEval基准上的广泛评估表明，与其他没有过滤操作的方法相比，CODEFILTER在各类任务中一致地提高了补全准确性。此外，CODEFILTER显著减少了输入提示的长度，提高了计算效率，并且在不同的模型之间表现出强大的普适性。这些结果表明，CODEFILTER有潜力提高仓库级别代码补全的准确度、效率和可归因性。', 'title_zh': '基于影响的上下文过滤代码跨文件完成'}
{'arxiv_id': 'arXiv:2508.05960', 'title': 'Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning', 'authors': 'Haohui Chen, Zhiyong Chen', 'link': 'https://arxiv.org/abs/2508.05960', 'abstract': 'Offline reinforcement learning (RL) seeks to learn optimal policies from static datasets without further environment interaction. A key challenge is the distribution shift between the learned and behavior policies, leading to out-of-distribution (OOD) actions and overestimation. To prevent gross overestimation, the value function must remain conservative; however, excessive conservatism may hinder performance improvement. To address this, we propose the mildly conservative regularized evaluation (MCRE) framework, which balances conservatism and performance by combining temporal difference (TD) error with a behavior cloning term in the Bellman backup. Building on this, we develop the mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates MCRE into an off-policy actor-critic framework. Experiments show that MCRQ outperforms strong baselines and state-of-the-art offline RL algorithms on benchmark datasets.', 'abstract_zh': '离线强化学习（RL） seeks to learn optimal policies from static datasets without further environment interaction.', 'title_zh': '轻度保守正则化离线强化学习评估'}
{'arxiv_id': 'arXiv:2508.05957', 'title': 'Multi-Armed Bandits-Based Optimization of Decision Trees', 'authors': 'Hasibul Karim Shanto, Umme Ayman Koana, Shadikur Rahman', 'link': 'https://arxiv.org/abs/2508.05957', 'abstract': 'Decision trees, without appropriate constraints, can easily become overly complex and prone to overfit, capturing noise rather than generalizable patterns. To resolve this problem,pruning operation is a crucial part in optimizing decision trees, as it not only reduces the complexity of trees but also decreases the probability of generating overfit models. The conventional pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning (REP) are mostly based on greedy approaches that focus on immediate gains in performance while pruning nodes of the decision tree. However, this might result in a lower generalization in the long run, compromising the robust ability of the tree model when introduced to unseen data samples, particularly when trained with small and complex datasets. To address this challenge, we are proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement learning (RL)-based technique, that will dynamically prune the tree to generate an optimal decision tree with better generalization. Our proposed approach assumes the pruning process as an exploration-exploitation problem, where we are utilizing the MAB algorithms to find optimal branch nodes to prune based on feedback from each pruning actions. Experimental evaluation on several benchmark datasets, demonstrated that our proposed approach results in better predictive performance compared to the traditional ones. This suggests the potential of utilizing MAB for a dynamic and probabilistic way of decision tree pruning, in turn optimizing the decision tree-based model.', 'abstract_zh': '基于多臂 bandit 的决策树剪枝方法：一种增强泛化能力的强化学习approach', 'title_zh': '基于多臂老虎机的决策树优化'}
{'arxiv_id': 'arXiv:2508.05954', 'title': 'Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents', 'authors': 'Han Lin, Jaemin Cho, Amir Zadeh, Chuan Li, Mohit Bansal', 'link': 'https://arxiv.org/abs/2508.05954', 'abstract': "There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities. Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining. We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder. These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet. To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings. By seamlessly integrating pretrained MLLMs and diffusion models with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training. We also provide comprehensive ablation studies showing the effectiveness of our design choices.", 'abstract_zh': '将高保真视觉合成能力集成到大型语言模型中，而不牺牲其强大的推理能力：Bifrost-1统一框架的研究', 'title_zh': 'Bifrost-1: 连接多模态LLMs和扩散模型的像素级CLIP隐变量桥梁'}
{'arxiv_id': 'arXiv:2508.05950', 'title': 'A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image', 'authors': 'Yanxing Liang, Yinghui Wang, Jinlong Yang, Wei Li', 'link': 'https://arxiv.org/abs/2508.05950', 'abstract': 'The lack of spatial dimensional information remains a challenge in normal estimation from a single image. Recent diffusion-based methods have demonstrated significant potential in 2D-to-3D implicit mapping, they rely on data-driven statistical priors and miss the explicit modeling of light-surface interaction, leading to multi-view normal direction conflicts. Moreover, the discrete sampling mechanism of diffusion models causes gradient discontinuity in differentiable rendering reconstruction modules, preventing 3D geometric errors from being backpropagated to the normal generation network, thereby forcing existing methods to depend on dense normal annotations. This paper proposes SINGAD, a novel Self-supervised framework from a single Image for Normal estimation via 3D GAussian splatting guided Diffusion. By integrating physics-driven light-interaction modeling and a differentiable rendering-based reprojection strategy, our framework directly converts 3D geometric errors into normal optimization signals, solving the challenges of multi-view geometric inconsistency and data dependency. Specifically, the framework constructs a light-interaction-driven 3DGS reparameterization model to generate multi-scale geometric features consistent with light transport principles, ensuring multi-view normal consistency. A cross-domain feature fusion module is designed within a conditional diffusion model, embedding geometric priors to constrain normal generation while maintaining accurate geometric error propagation. Furthermore, a differentiable 3D reprojection loss strategy is introduced for self-supervised optimization that minimizes geometric error between the reconstructed and input image, eliminating dependence on annotated normal datasets. Quantitative evaluations on the Google Scanned Objects dataset demonstrate that our method outperforms state-of-the-art approaches across multiple metrics.', 'abstract_zh': '单张图像引导的3D高斯点扩散Normals估计新框架：自监督3D光互作用引导扩散(SINGAD)', 'title_zh': '基于单张图像的3DGS-扩散自监督框架用于法线估计'}
{'arxiv_id': 'arXiv:2508.05938', 'title': 'Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale', 'authors': 'Rafal Kocielnik, Min Kim, Penphob, Boonyarungsrit, Fereshteh Soltani, Deshawn Sambrano, Animashree Anandkumar, R. Michael Alvarez', 'link': 'https://arxiv.org/abs/2508.05938', 'abstract': "Detecting prosociality in text--communication intended to affirm, support, or improve others' behavior--is a novel and increasingly important challenge for trust and safety systems. Unlike toxic content detection, prosociality lacks well-established definitions and labeled data, requiring new approaches to both annotation and deployment. We present a practical, three-stage pipeline that enables scalable, high-precision prosocial content classification while minimizing human labeling effort and inference costs. First, we identify the best LLM-based labeling strategy using a small seed set of human-labeled examples. We then introduce a human-AI refinement loop, where annotators review high-disagreement cases between GPT-4 and humans to iteratively clarify and expand the task definition-a critical step for emerging annotation tasks like prosociality. This process results in improved label quality and definition alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train a two-stage inference system: a lightweight classifier handles high-confidence predictions, while only $\\sim$35\\% of ambiguous instances are escalated to GPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving high precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI interaction, careful task formulation, and deployment-aware architecture design can unlock scalable solutions for novel responsible AI tasks.", 'abstract_zh': '在文本中检测利他行为——旨在肯定、支持或改进他人行为的沟通——是信任与安全系统面临的新型且日益重要的挑战。不同于有毒内容检测，利他行为缺乏成熟的定义和标记数据，需要新的标注和部署方法。我们提出一个实用的三阶段流水线，能够在减少人工标注努力和推理成本的同时，实现可扩展且高精度的利他内容分类。首先，我们使用少量的人工标注示例确定最佳基于大语言模型的标注策略。随后引入人类-AI修正循环，标注员审核GPT-4与人类之间高分歧的案例，以迭代澄清和扩展任务定义——这是新兴标注任务如利他行为的关键步骤。此过程提高了标签质量和定义一致性。最后，我们使用GPT-4合成10,000个高质量标签，并训练一个两阶段推理系统：轻量级分类器处理高置信度预测，而仅有约35%的模棱两可实例被升级到GPT-4o。此架构将推理成本降低了约70%，同时保持高精度（约0.90）。我们的流水线展示了针对新型负责任AI任务的目标化人类-AI互动、精细的任务表述以及部署意识架构设计如何解锁可扩展解决方案。', 'title_zh': '玩家游戏聊天中的利他行为检测：从人类-AI定义一致到大规模高效标注'}
{'arxiv_id': 'arXiv:2508.05934', 'title': 'ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection', 'authors': 'Xueyuan Xu, Tianze Yu, Wenjia Dong, Fulin Wei, Li Zhuo', 'link': 'https://arxiv.org/abs/2508.05934', 'abstract': 'Recently, multi-modal physiological signals based emotion recognition has garnered increasing attention in the field of brain-computer interfaces. Nevertheness, the associated multi-modal physiological features are often high-dimensional and inevitably include irrelevant, redundant, and noisy representation, which can easily lead to overfitting, poor performance, and high computational complexity in emotion classifiers. Feature selection has been widely applied to address these challenges. However, previous studies generally assumed that multi-modal physiological data are complete, whereas in reality, the data are often incomplete due to the openness of the acquisition and operational environment. For example, a part of samples are available in several modalities but not in others. To address this issue, we propose a novel method for incomplete multi-modal physiological signal feature selection called adaptive shared latent structure learning (ASLSL). Based on the property that similar features share similar emotional labels, ASLSL employs adaptive shared latent structure learning to explore a common latent space shared for incomplete multi-modal physiological signals and multi-dimensional emotional labels, thereby mitigating the impact of missing information and mining consensus information. Two most popular multi-modal physiological emotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels were utilized to compare the performance between compare ASLSL and seventeen feature selection methods. Comprehensive experimental results on these datasets demonstrate the effectiveness of ASLSL.', 'abstract_zh': '基于不完整多模态生理信号的情感特征选择：自适应共享潜在结构学习（ASLSL）', 'title_zh': '自适应共享潜在结构学习在不完全多模态生理数据中的多维情绪特征选择'}
{'arxiv_id': 'arXiv:2508.05933', 'title': 'REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition', 'authors': 'Xueyuan Xu, Wenjia Dong, Fulin Wei, Li Zhuo', 'link': 'https://arxiv.org/abs/2508.05933', 'abstract': 'The affective brain-computer interface is a crucial technology for affective interaction and emotional intelligence, emerging as a significant area of research in the human-computer interaction. Compared to single-type features, multi-type EEG features provide a multi-level representation for analyzing multi-dimensional emotions. However, the high dimensionality of multi-type EEG features, combined with the relatively small number of high-quality EEG samples, poses challenges such as classifier overfitting and suboptimal real-time performance in multi-dimensional emotion recognition. Moreover, practical applications of affective brain-computer interface frequently encounters partial absence of multi-dimensional emotional labels due to the open nature of the acquisition environment, and ambiguity and variability in individual emotion perception. To address these challenges, this study proposes a novel EEG feature selection method for missing multi-dimensional emotion recognition. The method leverages adaptive orthogonal non-negative matrix factorization to reconstruct the multi-dimensional emotional label space through second-order and higher-order correlations, which could reduce the negative impact of missing values and outliers on label reconstruction. Simultaneously, it employs least squares regression with graph-based manifold learning regularization and global feature redundancy minimization regularization to enable EEG feature subset selection despite missing information, ultimately achieving robust EEG-based multi-dimensional emotion recognition. Simulation experiments on three widely used multi-dimensional emotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method outperforms thirteen advanced feature selection methods in terms of robustness for EEG emotional feature selection.', 'abstract_zh': '具有情感识别功能的大脑-计算机接口是情感交互和情感智能的关键技术，是人机交互领域的一个重要研究方向。与单一类型特征相比，多类型EEG特征提供了多维度情感分析的多层次表示。然而，多类型EEG特征的高维性，结合高质量EEG样本数量相对较少，导致分类器过拟合和实时性能不佳等问题。此外，由于采集环境的开放性，具有情感识别功能的大脑-计算机接口在实际应用中经常遇到多维度情感标签部分缺失的情况，且个体情感感知具有模糊性和变异性。为解决这些问题，本研究提出了一种新的EEG特征选择方法，以实现多维度情感识别。该方法利用自适应正交非负矩阵分解技术，通过二阶和高阶相关性重构多维度情感标签空间，减少缺失值和异常值对标签重构的负面影响。同时，该方法采用基于图的流形学习正则化和全局特征冗余最小化正则化与最小二乘回归结合，即使在信息缺失的情况下也能实现EEG特征子集选择，最终实现稳健的情感识别。在对广泛使用的三维多维度情感数据集DREAMER、DEAP和HDED进行的仿真实验中，提出的方法在EEG情感特征选择的鲁棒性方面优于十三种先进的特征选择方法。', 'title_zh': 'REFS: 面向情绪识别的鲁棒EEG特征选择，处理缺失多维标注'}
{'arxiv_id': 'arXiv:2508.05923', 'title': 'Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm', 'authors': 'Yanusha Mehendran, Maolin Tang, Yi Lu', 'link': 'https://arxiv.org/abs/2508.05923', 'abstract': "Software vulnerabilities continue to undermine the reliability and security of modern systems, particularly as software complexity outpaces the capabilities of traditional detection methods. This study introduces a genetic algorithm-based method for test input generation that innovatively integrates genetic operators and adaptive learning to enhance software vulnerability detection. A key contribution is the application of the crossover operator, which facilitates exploration by searching across a broader space of potential test inputs. Complementing this, an adaptive feedback mechanism continuously learns from the system's execution behavior and dynamically guides input generation toward promising areas of the input space. Rather than relying on fixed or randomly selected inputs, the approach evolves a population of structurally valid test cases using feedback-driven selection, enabling deeper and more effective code traversal. This strategic integration of exploration and exploitation ensures that both diverse and targeted test inputs are developed over time. Evaluation was conducted across nine open-source JSON-processing libraries. The proposed method achieved substantial improvements in coverage compared to a benchmark evolutionary fuzzing method, with average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0% in line coverage, 114.0% in instruction coverage, and 166.0% in branch coverage. These results highlight the method's capacity to detect deeper and more complex vulnerabilities, offering a scalable and adaptive solution to software security testing.", 'abstract_zh': '基于遗传算法的测试输入生成方法在软件漏洞检测中的应用：探索与利用的协同增强', 'title_zh': '基于遗传算法的自适应测试输入生成以增强软件漏洞检测'}
{'arxiv_id': 'arXiv:2508.05913', 'title': 'Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction', 'authors': 'Stefan Pasch, Min Chul Cha', 'link': 'https://arxiv.org/abs/2508.05913', 'abstract': "As AI systems become increasingly embedded in organizational workflows and consumer applications, ethical principles such as fairness, transparency, and robustness have been widely endorsed in policy and industry guidelines. However, there is still scarce empirical evidence on whether these principles are recognized, valued, or impactful from the perspective of users. This study investigates the link between ethical AI and user satisfaction by analyzing over 100,000 user reviews of AI products from G2. Using transformer-based language models, we measure sentiment across seven ethical dimensions defined by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all seven dimensions are positively associated with user satisfaction. Yet, this relationship varies systematically across user and product types. Technical users and reviewers of AI development platforms more frequently discuss system-level concerns (e.g., transparency, data governance), while non-technical users and reviewers of end-user applications emphasize human-centric dimensions (e.g., human agency, societal well-being). Moreover, the association between ethical AI and user satisfaction is significantly stronger for non-technical users and end-user applications across all dimensions. Our results highlight the importance of ethical AI design from users' perspectives and underscore the need to account for contextual differences across user roles and product types.", 'abstract_zh': '随着人工智能系统越来越多地嵌入组织工作流和消费者应用中，公平性、透明性和鲁棒性等伦理原则已在政策和行业指南中得到了广泛认可。然而，关于这些原则是否被用户认可、重视或具有影响力，仍缺乏实证证据。本研究通过分析来自G2的逾100,000条AI产品用户评论，探讨伦理AI与用户满意度之间的联系。利用基于转换器的语言模型，我们衡量了用户评论中七个由欧盟可信赖AI伦理指南定义的伦理维度的 sentiment。研究发现，这七个维度都与用户满意度正相关。然而，这种关系在不同用户和产品类型之间系统地有所不同。技术用户和技术平台的审查者更频繁地讨论系统层面的关切（如透明性、数据治理），而非技术用户和终端用户应用的审查者则更强调以人类为中心的维度（如人类自主权、社会福祉）。此外，所有维度上，伦理AI与用户满意度之间的关联对非技术用户和终端用户应用来说更为显著。研究结果强调了从用户视角设计伦理AI的重要性，并突显了在用户角色和产品类型不同的背景下考虑情境差异的必要性。', 'title_zh': '伦理AI原则对用户有意义吗？大规模分析用户情感与满意度'}
{'arxiv_id': 'arXiv:2508.05880', 'title': 'Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models', 'authors': 'Sree Bhattacharyya, Lucas Craig, Tharun Dilliraj, Jia Li, James Z. Wang', 'link': 'https://arxiv.org/abs/2508.05880', 'abstract': 'Affective Computing has been established as a crucial field of inquiry to advance the holistic development of Artificial Intelligence (AI) systems. Foundation models -- especially Large Language Models (LLMs) -- have been evaluated, trained, or instruction-tuned in several past works, to become better predictors or generators of emotion. Most of these studies, however, approach emotion-related tasks in a supervised manner, assessing or training the capabilities of LLMs using discrete emotion labels associated with stimuli (e.g., text, images, video, audio). Evaluation studies, in particular, have often been limited to standard and superficial emotion-related tasks, such as the recognition of evoked or expressed emotions. In this paper, we move beyond surface-level emotion tasks to investigate how LLMs reason about emotions through cognitive dimensions. Drawing from cognitive appraisal theory, we examine whether LLMs produce coherent and plausible cognitive reasoning when reasoning about emotionally charged stimuli. We introduce a large-scale benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal cognitive structures implicitly used by LLMs for emotional reasoning. Through a plethora of evaluation experiments and analysis, we seek to answer: (a) Are models more likely to implicitly rely on specific cognitive appraisal dimensions?, (b) What cognitive dimensions are important for characterizing specific emotions?, and, (c) Can the internal representations of different emotion categories in LLMs be interpreted through cognitive appraisal dimensions? Our results and analyses reveal diverse reasoning patterns across different LLMs. Our benchmark and code will be made publicly available.', 'abstract_zh': '情感计算已被确立为促进人工智能系统全面发展的关键研究领域。基础模型——尤其是大规模语言模型（LLMs）——在多项以往研究中被评估、训练或指令调优，以更好地预测或生成情感。然而，大多数这些研究都采用监督方式，使用与刺激（如文本、图像、视频、音频）相关的离散情感标签来评估或训练LLMs的能力。尤其是在评估研究中，通常局限于标准且表面的情感相关任务，如诱发或表达情感的识别。在本文中，我们超越表面的情感任务，探讨LLMs如何通过认知维度来进行情感推理。根据认知评估理论，我们检查LLMs在处理具有情感色彩的刺激时是否能够产生一致且合理的认知推理。我们引入了一个大规模的情感认知推理基准——CoRE——来评估LLMs在情感推理时隐含使用内部认知结构。通过大量的评估实验和分析，我们寻求回答以下问题：（a）模型更可能隐含依赖哪些特定的认知评估维度？（b）哪些认知维度对于描述特定情感十分重要？（c）不同情感类别在LLMs中的内部表示能否通过认知评估维度来解释？我们的结果和分析揭示了不同LLMs之间各异的认知推理模式。我们的基准和代码将公开发布。', 'title_zh': '机器会情绪化思考吗？大规模语言模型的认知评价分析'}
{'arxiv_id': 'arXiv:2508.05846', 'title': 'Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic Systems', 'authors': 'Ahmad Farooq, Kamran Iqbal', 'link': 'https://arxiv.org/abs/2508.05846', 'abstract': 'As artificial intelligence (AI) and robotics increasingly permeate society, ensuring the ethical behavior of these systems has become paramount. This paper contends that transparency in AI decision-making processes is fundamental to developing trustworthy and ethically aligned robotic systems. We explore how transparency facilitates accountability, enables informed consent, and supports the debugging of ethical algorithms. The paper outlines technical, ethical, and practical challenges in implementing transparency and proposes novel approaches to enhance it, including standardized metrics, explainable AI techniques, and user-friendly interfaces. This paper introduces a framework that connects technical implementation with ethical considerations in robotic systems, focusing on the specific challenges of achieving transparency in dynamic, real-world contexts. We analyze how prioritizing transparency can impact public trust, regulatory policies, and avenues for future research. By positioning transparency as a fundamental element in ethical AI system design, we aim to add to the ongoing discussion on responsible AI and robotics, providing direction for future advancements in this vital field.', 'abstract_zh': '随着人工智能和机器人技术日益渗透到社会中，确保这些系统的伦理行为已成为重中之重。本文认为，在机器人系统中实现AI决策过程的透明性是构建可信赖且伦理对齐的系统的基础。我们探讨了透明性如何促进问责制、实现知情同意，并支持伦理算法的调试。本文概述了实现透明性的技术和实际挑战，并提出了增强透明性的新颖方法，包括标准化指标、可解释的人工智能技术以及用户友好的界面。本文提出了一种框架，将技术实现与机器人系统的伦理考量联系起来，重点是实现透明性在动态、现实生活情境中的特定挑战。我们分析了优先考虑透明性如何影响公众信任、监管政策以及未来研究的途径。通过将透明性定位为伦理AI系统设计中的基本要素，我们旨在为持续进行的责任AI和机器人讨论做出贡献，并为这一关键领域未来的发展提供方向。', 'title_zh': '面向透明伦理AI：可信赖机器人系统的发展 roadmap'}
{'arxiv_id': 'arXiv:2508.05838', 'title': 'Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction', 'authors': 'Ahmad Farooq, Kamran Iqbal', 'link': 'https://arxiv.org/abs/2508.05838', 'abstract': 'This paper presents a novel approach that integrates vision foundation models with reinforcement learning to enhance object interaction capabilities in simulated environments. By combining the Segment Anything Model (SAM) and YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the AI2-THOR simulation environment, we enable the agent to perceive and interact with objects more effectively. Our comprehensive experiments, conducted across four diverse indoor kitchen settings, demonstrate significant improvements in object interaction success rates and navigation efficiency compared to a baseline agent without advanced perception. The results show a 68% increase in average cumulative reward, a 52.5% improvement in object interaction success rate, and a 33% increase in navigation efficiency. These findings highlight the potential of integrating foundation models with reinforcement learning for complex robotic tasks, paving the way for more sophisticated and capable autonomous agents.', 'abstract_zh': '本文提出了一种将视觉基础模型与强化学习相结合的新方法，以增强模拟环境中物体交互能力。通过在AI2-THOR仿真环境中结合使用Segment Anything Model (SAM)、YOLOv5与 proximal policy optimization (PPO) 代理，使代理能够更有效地感知和交互物体。我们在四个不同的室内厨房场景中进行了全面实验，结果显示与没有高级感知的 baseline 代理相比，在物体交互成功率和导航效率方面均取得了显著改进。平均累积奖励提高了68%，物体交互成功率提高了52.5%，导航效率提高了33%。这些发现突显了将基础模型与强化学习相结合在复杂机器人任务中的潜力，为开发更复杂和能力强的自主代理铺平了道路。', 'title_zh': '将视觉基础模型与强化学习集成以增强物体交互'}
{'arxiv_id': 'arXiv:2508.05799', 'title': 'AI-Guided Exploration of Large-Scale Codebases', 'authors': 'Yoseph Berhanu Alebachew', 'link': 'https://arxiv.org/abs/2508.05799', 'abstract': 'Understanding large-scale, complex software systems is a major challenge for developers, who spend a significant portion of their time on program comprehension. Traditional tools such as static visualizations and reverse engineering techniques provide structural insights but often lack interactivity, adaptability, and integration with contextual information. Recent advancements in large language models (LLMs) offer new opportunities to enhance code exploration workflows, yet their lack of grounding and integration with structured views limits their effectiveness. This work introduces a hybrid approach that integrates deterministic reverse engineering with LLM-guided, intent-aware visual exploration. The proposed system combines UML-based visualization, dynamic user interfaces, historical context, and collaborative features into an adaptive tool for code comprehension. By interpreting user queries and interaction patterns, the LLM helps developers navigate and understand complex codebases more effectively. A prototype implementation for Java demonstrates the feasibility of this approach. Future work includes empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM interaction models. This research lays the groundwork for intelligent, interactive environments that align with developer cognition and collaborative workflows.', 'abstract_zh': '理解大规模复杂软件系统是开发者面临的一项重大挑战，他们花费大量时间在程序理解上。传统工具如静态可视化和逆向工程技术提供了结构洞察，但往往缺乏交互性、适应性和与上下文信息的集成。近年来，大规模语言模型（LLMs）的发展为增强代码探索流程提供了新的机会，但它们缺乏与结构化视图的结合和对接地能力，限制了其效果。本文介绍了一种将确定性逆向工程与LLM引导的、意图感知的视觉探索相结合的混合方法。所提出系统结合了基于UML的可视化、动态用户界面、历史上下文和协作功能，形成一种适应性强的代码理解工具。通过解释用户查询和交互模式，LLM帮助开发者更有效地导航和理解复杂的代码库。针对Java的原型实现证明了该方法的可行性。未来工作包括实证评估、扩展到多语言系统以及探索GUI驱动的LLM交互模型。这项研究为与开发人员认知和协作流程相一致的智能、交互环境奠定了基础。', 'title_zh': 'AI引导的大规模代码库探索'}
{'arxiv_id': 'arXiv:2508.05791', 'title': 'From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data', 'authors': 'Haoran Li, Lihao Mai, Muhao Guo, Jiaqi Wu, Yang Weng, Yannan Sun, Ce Jimmy Liu', 'link': 'https://arxiv.org/abs/2508.05791', 'abstract': "Accurate distribution grid topology is essential for reliable modern grid operations. However, real-world utility data originates from multiple sources with varying characteristics and levels of quality. In this work, developed in collaboration with Oncor Electric Delivery, we propose a scalable framework that reconstructs a trustworthy grid topology by systematically integrating heterogeneous data. We observe that distribution topology is fundamentally governed by two complementary dimensions: the spatial layout of physical infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the system in the signal domain (e.g., voltage time series). When jointly leveraged, these dimensions support a complete and physically coherent reconstruction of network connectivity. To address the challenge of uneven data quality without compromising observability, we introduce a confidence-aware inference mechanism that preserves structurally informative yet imperfect inputs, while quantifying the reliability of each inferred connection for operator interpretation. This soft handling of uncertainty is tightly coupled with hard enforcement of physical feasibility: we embed operational constraints, such as transformer capacity limits and radial topology requirements, directly into the learning process. Together, these components ensure that inference is both uncertainty-aware and structurally valid, enabling rapid convergence to actionable, trustworthy topologies under real-world deployment conditions. The proposed framework is validated using data from over 8000 meters across 3 feeders in Oncor's service territory, demonstrating over 95% accuracy in topology reconstruction and substantial improvements in confidence calibration and computational efficiency relative to baseline methods.", 'abstract_zh': '准确的配电网络拓扑对于可靠现代电网运营至关重要。然而，实际的公用事业数据源自多个具有不同特性和质量水平的数据源。在与Oncor Electric Delivery合作下，我们提出了一种可扩展的框架，通过系统地整合异质数据来重构一个可信的网络拓扑。我们观察到，配电网络拓扑本质上受两个互补维度的治理：物理基础设施的空间布局（例如，GIS和资产元数据）和系统的动态行为在信号域（例如，电压时间序列）。当这些维度共同利用时，支持网络连接的完整且物理上一致的重构。为了解决数据质量不均的问题同时保持可观测性，我们引入了一种基于置信度的推理机制，保留结构性信息但不完美的输入，同时量化每个推断连接的可靠性供操作员解释。这种对不确定性的软处理与严格的物理可行性硬约束紧密结合：我们将如变压器容量限制和放射状拓扑要求等运行约束直接嵌入学习过程中。这些组件共同确保推理既是不确定性的感知又是结构上的有效，使得在实际部署条件下能够快速收敛到行动性好且值得信赖的拓扑。所提出的框架利用Oncor服务区域内三个馈电超过8000米的数据进行验证，展现出了超过95%的拓扑重构准确率，并且在置信度校准和计算效率方面显著优于基线方法。', 'title_zh': '从不完美信号到可信赖结构：考虑置信度的异构且可靠性变化的效用数据推理'}
{'arxiv_id': 'arXiv:2508.05783', 'title': 'Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks', 'authors': 'Mengyu Li, Guoyao Shen, Chad W. Farris, Xin Zhang', 'link': 'https://arxiv.org/abs/2508.05783', 'abstract': 'Machine learning using transformers has shown great potential in medical imaging, but its real-world applicability remains limited due to the scarcity of annotated data. In this study, we propose a practical framework for the few-shot deployment of pretrained MRI transformers in diverse brain imaging tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a large-scale, multi-cohort brain MRI dataset comprising over 31 million slices, we obtain highly transferable latent representations that generalize well across tasks and datasets. For high-level tasks such as classification, a frozen MAE encoder combined with a lightweight linear head achieves state-of-the-art accuracy in MRI sequence identification with minimal supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a hybrid architecture that fuses multiscale CNN features with pretrained MAE embeddings. This model consistently outperforms other strong baselines in both skull stripping and multi-class anatomical segmentation under data-limited conditions. With extensive quantitative and qualitative evaluations, our framework demonstrates efficiency, stability, and scalability, suggesting its suitability for low-resource clinical environments and broader neuroimaging applications.', 'abstract_zh': '使用变压器的机器学习在医学影像领域展示了巨大潜力，但由于标注数据稀缺，其实用性仍受到限制。本研究提出了一种实用框架，用于在多样化的脑部影像任务中进行预训练MRI变压器的一次性部署。通过在包含超过3100万切片的大规模多队列脑MRI数据集上利用Masked Autoencoder (MAE) 的预训练策略，我们获得了高度可移植的潜在表示，这些表示在不同任务和数据集中具有良好的泛化能力。在高层任务如分类方面，冻结的MAE编码器结合轻量级线性头，在MRI序列识别方面在最少监督下达到了最先进的准确性。在低层任务如分割方面，我们提出了MAE-Funet混合架构，该架构将多尺度CNN特征与预训练的MAE嵌入融合。在数据受限条件下，该模型在头骨剥离和多分类解剖学分割方面均优于其他强大基线模型。通过广泛的定量和定性评估，我们的框架展示了高效性、稳定性和可扩展性，表明其适合低资源临床环境和更广泛的神经影像学应用。', 'title_zh': '预训练MRI变换器的少量样本部署在脑成像任务中'}
{'arxiv_id': 'arXiv:2508.05755', 'title': 'UnGuide: Learning to Forget with LoRA-Guided Diffusion Models', 'authors': 'Agnieszka Polowczyk, Alicja Polowczyk, Dawid Malarz, Artur Kasymov, Marcin Mazur, Jacek Tabor, Przemysław Spurek', 'link': 'https://arxiv.org/abs/2508.05755', 'abstract': 'Recent advances in large-scale text-to-image diffusion models have heightened concerns about their potential misuse, especially in generating harmful or misleading content. This underscores the urgent need for effective machine unlearning, i.e., removing specific knowledge or concepts from pretrained models without compromising overall performance. One possible approach is Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models for targeted unlearning. However, LoRA often inadvertently alters unrelated content, leading to diminished image fidelity and realism. To address this limitation, we introduce UnGuide -- a novel approach which incorporates UnGuidance, a dynamic inference mechanism that leverages Classifier-Free Guidance (CFG) to exert precise control over the unlearning process. UnGuide modulates the guidance scale based on the stability of a few first steps of denoising processes, enabling selective unlearning by LoRA adapter. For prompts containing the erased concept, the LoRA module predominates and is counterbalanced by the base model; for unrelated prompts, the base model governs generation, preserving content fidelity. Empirical results demonstrate that UnGuide achieves controlled concept removal and retains the expressive power of diffusion models, outperforming existing LoRA-based methods in both object erasure and explicit content removal tasks.', 'abstract_zh': '大规模文本到图像扩散模型的 Recent Advances 加剧了对其潜在滥用的担忧，尤其是生成有害或误导性内容。这突显了急需有效的机器遗忘技术，即在不损害整体性能的情况下从预训练模型中移除特定的知识或概念。一种可能的方法是低秩适应（LoRA），它提供了一种有效的方法来微调模型以实现有针对性的遗忘。然而，LoRA 通常会无意中改变不相关的内容，导致图像保真度和现实感下降。为了解决这一限制，我们引入了 UnGuide —— 一种新颖的方法，该方法结合了 UnGuidance 动态推理机制，利用分类器无关指导（CFG）对遗忘过程进行精确控制。UnGuide 根据去噪过程的前几步的稳定性调节指导尺度，使 LoRA 适配器能够选择性地进行遗忘。对于包含被擦除概念的提示，LoRA 模块占主导地位，并由基础模型进行制衡，保留内容保真度；对于不相关的提示，基础模型主导生成过程。实验结果表明，UnGuide 实现了可控的概念去除，并保留了扩散模型的表达能力，在物体擦除和明确内容去除任务中均优于现有的基于 LoRA 的方法。', 'title_zh': 'LoRA-Guided Diffusion Models: Learning to Forget'}
{'arxiv_id': 'arXiv:2508.05728', 'title': 'CLAPP: The CLASS LLM Agent for Pair Programming', 'authors': 'Santiago Casas, Christian Fidler, Boris Bolliet, Francisco Villaescusa-Navarro, Julien Lesgourgues', 'link': 'https://arxiv.org/abs/2508.05728', 'abstract': 'We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI assistant designed to support researchers working with the Einstein-Boltzmann solver CLASS. CLAPP leverages large language models (LLMs) and domain-specific retrieval to provide conversational coding support for CLASS-answering questions, generating code, debugging errors, and producing plots. Its architecture combines multi-agent LLM orchestration, semantic search across CLASS documentation, and a live Python execution environment. Deployed as a user-friendly web application, CLAPP lowers the entry barrier for scientists unfamiliar with AI tools and enables more productive human-AI collaboration in computational and numerical cosmology. The app is available at this https URL', 'abstract_zh': '我们介绍了CLAPP（CLASS LLM代理程序对编程），一种交互式AI助手，旨在支持使用Einstein-Boltzmann求解器CLASS进行研究的科学家。CLAPP利用大型语言模型（LLM）和领域特定检索来为CLASS提供会话式编码支持，包括回答问题、生成代码、调试错误和生成图表。其架构结合了多代理LLM编排、跨CLASS文档的语义搜索以及实时Python执行环境。作为用户友好的网络应用部署，CLAPP降低了对AI工具不熟悉的研究人员的入门门槛，并在计算性和数值宇宙学领域促进了更具成效的人工智能协作。应用程序可从此处 accessed at this https URL 获取。', 'title_zh': 'CLAPP: CLASS LLM代理在对弈编程中的应用'}
{'arxiv_id': 'arXiv:2508.05710', 'title': 'Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning', 'authors': 'Jia Fu, Xinyu Yang, Hongzhi Zhang, Yahui Liu, Jingyuan Zhang, Qi Wang, Fuzheng Zhang, Guorui Zhou', 'link': 'https://arxiv.org/abs/2508.05710', 'abstract': 'Precise, correct feedback is crucial for effectively training large language models (LLMs) in code reinforcement learning. However, synthesizing high-quality test cases remains a profoundly challenging and unsolved problem. In this work, we present Klear-CodeTest, a comprehensive test case synthesis framework featuring rigorous verification to ensure quality and reliability of test cases. Our approach achieves broad coverage of programming problems via a novel Generator-Validation (G-V) framework, ensuring correctness through a consistency validation mechanism that verifies outputs against gold solutions. The proposed G-V framework generates comprehensive test cases including both regular and corner cases, enhancing test coverage and discriminative power for solution correctness assessment in code reinforcement learning. In addition, we design a multi-layered security sandbox system optimized for online verification platforms, guaranteeing safe and reliable code execution. Through comprehensive experiments, we demonstrate the effectiveness of our curated dataset, showing significant improvements in model performance and training stability. The source codes, curated dataset and sandbox system are available at: this https URL.', 'abstract_zh': '精确且准确的反馈对于有效训练代码强化学习中的大规模语言模型（LLMs）至关重要。然而，合成高质量的测试案例仍然是一个至关重要的但尚未解决的问题。在本文中，我们提出Klear-CodeTest，这是一种带有严格验证机制的综合测试案例合成框架，以确保测试案例的质量和可靠性。我们的方法通过一种新颖的生成-验证（G-V）框架实现了广泛的编程问题覆盖，并通过一致性验证机制确保正确性，该机制将输出与黄金解决方案进行验证。提出的G-V框架生成全面的测试案例，包括常规案例和边界案例，增强测试覆盖范围和解决方案正确性评估的区分能力。此外，我们设计了一种针对在线验证平台优化的多层安全沙箱系统，确保安全可靠的代码执行。通过全面的实验，我们展示了精心构建的数据集的有效性，显示了在模型性能和训练稳定性方面的显著改进。源代码、精心构建的数据集和沙箱系统可在以下链接获取：this https URL。', 'title_zh': 'Klear-CodeTest: 编码增强学习的可扩展测试用例生成'}
{'arxiv_id': 'arXiv:2508.05705', 'title': 'A Physiologically-Constrained Neural Network Digital Twin Framework for Replicating Glucose Dynamics in Type 1 Diabetes', 'authors': 'Valentina Roquemen-Echeverri, Taisa Kushner, Peter G. Jacobs, Clara Mosquera-Lopez', 'link': 'https://arxiv.org/abs/2508.05705', 'abstract': 'Simulating glucose dynamics in individuals with type 1 diabetes (T1D) is critical for developing personalized treatments and supporting data-driven clinical decisions. Existing models often miss key physiological aspects and are difficult to individualize. Here, we introduce physiologically-constrained neural network (NN) digital twins to simulate glucose dynamics in T1D. To ensure interpretability and physiological consistency, we first build a population-level NN state-space model aligned with a set of ordinary differential equations (ODEs) describing glucose regulation. This model is formally verified to conform to known T1D dynamics. Digital twins are then created by augmenting the population model with individual-specific models, which include personal data, such as glucose management and contextual information, capturing both inter- and intra-individual variability. We validate our approach using real-world data from the T1D Exercise Initiative study. Two weeks of data per participant were split into 5-hour sequences and simulated glucose profiles were compared to observed ones. Clinically relevant outcomes were used to assess similarity via paired equivalence t-tests with predefined clinical equivalence margins. Across 394 digital twins, glucose outcomes were equivalent between simulated and observed data: time in range (70-180 mg/dL) was 75.1$\\pm$21.2% (simulated) vs. 74.4$\\pm$15.4% (real; P<0.001); time below range (<70 mg/dL) 2.5$\\pm$5.2% vs. 3.0$\\pm$3.3% (P=0.022); and time above range (>180 mg/dL) 22.4$\\pm$22.0% vs. 22.6$\\pm$15.9% (P<0.001). Our framework can incorporate unmodeled factors like sleep and activity while preserving key dynamics. This approach enables personalized in silico testing of treatments, supports insulin optimization, and integrates physics-based and data-driven modeling. Code: this https URL', 'abstract_zh': '模拟1型糖尿病（T1D）个体的葡萄糖动态对于开发个性化治疗方法和支持数据驱动的临床决策至关重要。现有的模型往往忽略了关键的生理方面，且难以个性化。我们介绍了基于生理约束的神经网络（NN）数字孪生，以模拟T1D的葡萄糖动态。为了确保可解释性和生理一致性，我们首先建立了一个与描述葡萄糖调节的常微分方程（ODEs）集合相一致的人口级NN状态空间模型。该模型正式验证符合已知的T1D动态。接着，通过将个体特异性模型添加到人口模型中，创建数字孪生，这些模型包括个人数据，如葡萄糖管理和个人背景信息，捕获了个体间和个体内的变异。我们使用T1D Exercise Initiative研究的实际数据验证了该方法。每位参与者两周的数据被分为5小时序列，模拟的葡萄糖曲线与观察到的进行了比较。通过预定义的临床等效边际使用成对等效t检验评估了临床相关结果。在整个394个数字孪生中，模拟和观察数据的葡萄糖结果等效：葡萄糖目标范围内（70-180 mg/dL）的时间为75.1±21.2%（模拟） vs. 74.4±15.4%（真实；P<0.001）；低于目标范围（<70 mg/dL）的时间为2.5±5.2% vs. 3.0±3.3%（P=0.022）；高于目标范围（>180 mg/dL）的时间为22.4±22.0% vs. 22.6±15.9%（P<0.001）。我们的框架可以整合未建模的因素，例如睡眠和活动，同时保持关键动态。该方法允许在体外个性化测试治疗方法、支持胰岛素优化，并整合基于物理和数据驱动的建模方法。代码：https://github.com/AlibabaCloudcommend/Qwen-T1D-Glucose-Model。', 'title_zh': '基于生理约束的神经网络数字孪生框架：复制1型糖尿病血糖动态'}
{'arxiv_id': 'arXiv:2508.05702', 'title': 'Semantic Reasoning Meets Numerical Precision: An LLM-Powered Multi-Agent System for Power Grid Control', 'authors': 'Yan Zhang', 'link': 'https://arxiv.org/abs/2508.05702', 'abstract': "The increasing penetration of Distributed Energy Resources (DERs), widespread adoption of Electric Vehicles (EVs), and the growing frequency of extreme weather events have significantly increased the complexity of power grid planning, operation, and management. Traditional rule-based systems and numerical optimization approaches often struggle with the scale, dynamics, and adaptability required by modern power networks. This paper introduces Grid-Agent, an autonomous, AI-driven framework that combines Large Language Models (LLMs) with multi-agent reinforcement learning to detect and remediate grid violations in real time. Grid-Agent integrates semantic reasoning with numerical precision through a modular agent architecture: a planning agent generates coordinated action sequences using numerical power flow solvers, while a validation agent evaluates system stability and action effectiveness via sandboxed execution with safety rollbacks. To ensure scalability, Grid-Agent incorporates an adaptive multiscale network representation that dynamically selects optimal encoding schemes based on network size and complexity. The framework enables coordinated violation resolution through optimizing switch configurations, battery deployment, and load curtailment strategies. Experimental results in standard IEEE and CIGRE test systems (IEEE 69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation performance. Additionally, the framework's built-in data collection and learning capabilities enable continuous learning and adaptation to diverse network topologies. The autonomous nature of the framework makes it particularly suitable for modern smart grid applications requiring rapid response to dynamic operating conditions.", 'abstract_zh': '分布式能源资源（DERs）渗透率的增加、电动汽车（EVs）的广泛采用以及极端天气事件频率的提高显著增加了电力网络规划、运行和管理的复杂性。传统的基于规则的系统和数值优化方法往往难以应对现代电力网络所需的规模、动态性和适应性。本文介紹了Grid-Agent，这是一种自主的、基于AI的框架，结合了大型语言模型（LLMs）和多代理强化学习，以实现实时检测和修复电网违规行为。Grid-Agent通过模块化的代理架构将语义推理与数值精度相结合：规划代理使用数值潮流求解器生成协调的操作序列，验证代理通过安全回滚的沙盒执行评估系统稳定性和操作效果。为了确保可扩展性，Grid-Agent采用了自适应多尺度网络表示，能够根据网络规模和复杂性动态选择最优编码方案。该框架通过对断路器配置、电池部署和负荷削减策略的优化来协调违规行为的解决。实验结果在标准的IEEE和CIGRE测试系统（IEEE 69-节点、CIGRE MV和IEEE 30-节点）中显示出优越的违规行为缓解性能。此外，该框架内置的数据收集和学习能力使其能够持续学习并适应不同的网络拓扑。框架的自主性质使其特别适合现代智能电网应用，这些应用需要快速响应动态运行条件。', 'title_zh': '语义推理结合数值精度：一种LLM驱动的多Agent系统在电力电网控制中的应用'}
{'arxiv_id': 'arXiv:2508.05700', 'title': 'Multi-Faceted Large Embedding Tables for Pinterest Ads Ranking', 'authors': 'Runze Su, Jiayin Jin, Jiacheng Li, Sihan Wang, Guangtong Bai, Zelun Wang, Li Tang, Yixiong Meng, Huasen Wu, Zhimeng Pan, Kungang Li, Han Sun, Zhifang Liu, Haoyang Li, Siping Ji, Ling Leng, Prathibha Deshikachar', 'link': 'https://arxiv.org/abs/2508.05700', 'abstract': "Large embedding tables are indispensable in modern recommendation systems, thanks to their ability to effectively capture and memorize intricate details of interactions among diverse entities. As we explore integrating large embedding tables into Pinterest's ads ranking models, we encountered not only common challenges such as sparsity and scalability, but also several obstacles unique to our context. Notably, our initial attempts to train large embedding tables from scratch resulted in neutral metrics. To tackle this, we introduced a novel multi-faceted pretraining scheme that incorporates multiple pretraining algorithms. This approach greatly enriched the embedding tables and resulted in significant performance improvements. As a result, the multi-faceted large embedding tables bring great performance gain on both the Click-Through Rate (CTR) and Conversion Rate (CVR) domains. Moreover, we designed a CPU-GPU hybrid serving infrastructure to overcome GPU memory limits and elevate the scalability. This framework has been deployed in the Pinterest Ads system and achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral end-to-end latency change.", 'abstract_zh': '大型嵌入表在现代推荐系统中不可或缺，得益于其有效捕捉和记忆多样化实体间复杂交互细节的能力。在将大型嵌入表集成到Pinterest的广告排名模型中时，我们不仅面临稀疏性和扩展性等常见挑战，还遇到一些特有的障碍。最初尝试从零训练大型嵌入表导致了中性的评估指标。为解决这一问题，我们引入了一种结合多种预训练算法的新型多方面预训练方案，极大地丰富了嵌入表并取得了显著的性能提升。多方面大型嵌入表在点击率(CTR)和转化率(CVR)方面带来了显著的性能提升。此外，我们设计了一种CPU-GPU混合服务架构以克服GPU内存限制并提升扩展性。该框架已在Pinterest Ads系统中部署，实现了1.34%的在线CPM减少和2.60%的CTR提升，端到端延迟保持不变。', 'title_zh': 'Pinterest 广告排名的多面大型嵌入表'}
{'arxiv_id': 'arXiv:2508.05696', 'title': 'Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition', 'authors': 'Kaichuan Kong, Dongjie Liu, Xiaobo Jin, Zhiying Li, Guanggang Geng', 'link': 'https://arxiv.org/abs/2508.05696', 'abstract': "Insider threat detection presents a significant challenge due to the deceptive nature of malicious behaviors, which often resemble legitimate user operations. However, existing approaches typically model system logs as flat event sequences, thereby failing to capture the inherent frequency dynamics and multiscale disturbance patterns embedded in user behavior. To address these limitations, we propose Log2Sig, a robust anomaly detection framework that transforms user logs into multivariate behavioral frequency signals, introducing a novel representation of user behavior. Log2Sig employs Multivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode Functions (IMFs), which reveal behavioral fluctuations across multiple temporal scales. Based on this, the model further performs joint modeling of behavioral sequences and frequency-decomposed signals: the daily behavior sequences are encoded using a Mamba-based temporal encoder to capture long-term dependencies, while the corresponding frequency components are linearly projected to match the encoder's output dimension. These dual-view representations are then fused to construct a comprehensive user behavior profile, which is fed into a multilayer perceptron for precise anomaly detection. Experimental results on the CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly outperforms state-of-the-art baselines in both accuracy and F1 score.", 'abstract_zh': '基于Log2Sig的日志异常检测框架：多变量行为频率信号表示与模式分解', 'title_zh': 'Log2Sig: 基于多变量行为信号分解的频率aware内部威胁检测'}
{'arxiv_id': 'arXiv:2508.05694', 'title': 'DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection', 'authors': 'Kaichuan Kong, Dongjie Liu, Xiaobo Jin, Guanggang Geng, Zhiying Li, Jian Weng', 'link': 'https://arxiv.org/abs/2508.05694', 'abstract': 'Insider threat detection (ITD) poses a persistent and high-impact challenge in cybersecurity due to the subtle, long-term, and context-dependent nature of malicious insider behaviors. Traditional models often struggle to capture semantic intent and complex behavior dynamics, while existing LLM-based solutions face limitations in prompt adaptability and modality coverage. To bridge this gap, we propose DMFI, a dual-modality framework that integrates semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into two structured views: (1) a semantic view that processes content-rich artifacts (e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned independently, and their outputs are fused via a lightweight MLP-based decision module. We further introduce DMFI-B, a discriminative adaptation strategy that separates normal and abnormal behavior representations, improving robustness under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets demonstrate that DMFI outperforms state-of-the-art methods in detection accuracy. Our approach combines the semantic reasoning power of LLMs with structured behavior modeling, offering a scalable and effective solution for real-world insider threat detection. Our work demonstrates the effectiveness of combining LLM reasoning with structured behavioral modeling, offering a scalable and deployable solution for modern insider threat detection.', 'abstract_zh': '基于双模态框架的恶意内鬼检测（DMFI）：结合语义推理与行为感知微调', 'title_zh': 'DMFI：基于双模态微调和推理框架的LLM驱动的内部威胁检测'}
{'arxiv_id': 'arXiv:2508.05693', 'title': 'Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach', 'authors': 'Siamak Farshidi, Amir Saberhabibi, Behbod Eskafi, Niloofar Nikfarjam, Sadegh Eskandari, Slinger Jansen, Michel Chaudron, Bedir Tekinerdogan', 'link': 'https://arxiv.org/abs/2508.05693', 'abstract': 'Selecting third-party software packages in open-source ecosystems like Python is challenging due to the large number of alternatives and limited transparent evidence for comparison. Generative AI tools are increasingly used in development workflows, but their suggestions often overlook dependency evaluation, emphasize popularity over suitability, and lack reproducibility. This creates risks for projects that require transparency, long-term reliability, maintainability, and informed architectural decisions. This study formulates software package selection as a Multi-Criteria Decision-Making (MCDM) problem and proposes a data-driven framework for technology evaluation. Automated data pipelines continuously collect and integrate software metadata, usage trends, vulnerability information, and developer sentiment from GitHub, PyPI, and Stack Overflow. These data are structured into a decision model representing relationships among packages, domain features, and quality attributes. The framework is implemented in PySelect, a decision support system that uses large language models to interpret user intent and query the model to identify contextually appropriate packages. The approach is evaluated using 798,669 Python scripts from 16,887 GitHub repositories and a user study based on the Technology Acceptance Model. Results show high data extraction precision, improved recommendation quality over generative AI baselines, and positive user evaluations of usefulness and ease of use. This work introduces a scalable, interpretable, and reproducible framework that supports evidence-based software selection using MCDM principles, empirical data, and AI-assisted intent modeling.', 'abstract_zh': '在开源生态系统如Python中选择第三方软件包具有挑战性，由于可供选择的方案众多且缺乏透明的比较证据。生成式AI工具在开发流程中使用越来越多，但其建议往往忽视依赖性评估，侧重流行度而非适用性，并缺乏可重复性。这增加了需要透明度、长期可靠性、可维护性和知情架构决策的项目的风险。本研究将软件包选择问题形式化为多准则决策制定（MCDM）问题，并提出一种基于数据的技术评估框架。自动化的数据管道不断收集和整合来自GitHub、PyPI和Stack Overflow的软件元数据、使用趋势、漏洞信息及开发者情感。这些数据被结构化为一个决策模型，代表软件包、领域特征与质量属性之间关系。该框架在PySelect中实现，这是一个决策支持系统，利用大型语言模型理解用户意图，并查询模型以识别上下文合适的技术。该方法通过来自16,887个GitHub仓库的798,669个Python脚本和基于技术接受模型的用户研究进行了评估。结果表明，数据提取精度高，推荐质量优于生成式AI基准模型，并且用户对实用性和易用性给予了积极评价。本研究引入了一种可扩展、可解释和可重复的框架，利用MCDM原则、实证数据和AI辅助意图建模支持基于证据的软件选择。', 'title_zh': '基于知识图谱的AI辅助软件包选择的实证评价'}
{'arxiv_id': 'arXiv:2508.05687', 'title': 'Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems', 'authors': "Alistair Reid, Simon O'Callaghan, Liam Carroll, Tiberio Caetano", 'link': 'https://arxiv.org/abs/2508.05687', 'abstract': 'Organisations are starting to adopt LLM-based AI agents, with their deployments naturally evolving from single agents towards interconnected, multi-agent networks. Yet a collection of safe agents does not guarantee a safe collection of agents, as interactions between agents over time create emergent behaviours and induce novel failure modes. This means multi-agent systems require a fundamentally different risk analysis approach than that used for a single agent.\nThis report addresses the early stages of risk identification and analysis for multi-agent AI systems operating within governed environments where organisations control their agent configurations and deployment. In this setting, we examine six critical failure modes: cascading reliability failures, inter-agent communication failures, monoculture collapse, conformity bias, deficient theory of mind, and mixed motive dynamics. For each, we provide a toolkit for practitioners to extend or integrate into their existing frameworks to assess these failure modes within their organisational contexts.\nGiven fundamental limitations in current LLM behavioural understanding, our approach centres on analysis validity, and advocates for progressively increasing validity through staged testing across stages of abstraction and deployment that gradually increases exposure to potential negative impacts, while collecting convergent evidence through simulation, observational analysis, benchmarking, and red teaming. This methodology establishes the groundwork for robust organisational risk management as these LLM-based multi-agent systems are deployed and operated.', 'abstract_zh': '组织开始采用基于大模型的AI代理，其部署自然从单个代理发展为相互连接的多代理网络。然而，一个安全代理集合并不保证整体安全，因为随着时间的推移，代理之间的交互会引发新的行为模式并诱导新的失败模式。这意味着多代理系统需要不同于单个代理所使用的基本不同的风险分析方法。\n\n本报告针对组织控制其代理配置和部署的应用管控环境下的多代理AI系统进行早期风险识别和分析的初期阶段。在这一环境下，我们考察了六个关键的失败模式：级联可靠性失败、代理间通信失败、单一文化崩溃、趋同偏差、理论推理不足以及混合动机动态。对于每一模式，我们提供了一个工具包，供从业者扩展或整合到其现有的框架中，以在其组织上下文中评估这些失败模式。\n\n鉴于当前对大模型行为理解的基本限制，我们的方法集中在分析的有效性上，并倡导通过阶段性的测试逐步增加有效性，这些测试逐渐增加对潜在负面影响的接触，并通过仿真、观察分析、基准测试和红队演练收集一致性证据。这种方法为这些基于大模型的多代理系统的稳健组织风险管理奠定了基础。', 'title_zh': '基于治理的大规模语言模型驱动的多Agent系统的风险分析技术'}
{'arxiv_id': 'arXiv:2508.05681', 'title': 'Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning', 'authors': 'Yuhan Zhi, Longtian Wang, Xiaofei Xie, Chao Shen, Qiang Hu, Xiaohong Guan', 'link': 'https://arxiv.org/abs/2508.05681', 'abstract': 'Active learning(AL), which serves as the representative label-efficient learning paradigm, has been widely applied in resource-constrained scenarios. The achievement of AL is attributed to acquisition functions, which are designed for identifying the most important data to label. Despite this success, one question remains unanswered: is AL safe? In this work, we introduce ALA, a practical and the first framework to utilize the acquisition function as the poisoning attack surface to reveal the weakness of active learning. Specifically, ALA optimizes imperceptibly poisoned inputs to exhibit high uncertainty scores, increasing their probability of being selected by acquisition functions. To evaluate ALA, we conduct extensive experiments across three datasets, three acquisition functions, and two types of clean-label backdoor triggers. Results show that our attack can achieve high success rates (up to 94%) even under low poisoning budgets (0.5%-1.0%) while preserving model utility and remaining undetectable to human annotators. Our findings remind active learning users: acquisition functions can be easily exploited, and active learning should be deployed with caution in trusted data scenarios.', 'abstract_zh': 'AL安全吗？一种利用获取函数进行中毒攻击的实用框架（ALA）', 'title_zh': '基于选择的漏洞：主动学习中的清洁标签后门攻击'}
{'arxiv_id': 'arXiv:2508.05680', 'title': 'Are All Genders Equal in the Eyes of Algorithms? -- Analysing Search and Retrieval Algorithms for Algorithmic Gender Fairness', 'authors': 'Stefanie Urchs, Veronika Thurner, Matthias Aßenmacher, Ludwig Bothmann, Christian Heumann, Stephanie Thiemichen', 'link': 'https://arxiv.org/abs/2508.05680', 'abstract': 'Algorithmic systems such as search engines and information retrieval platforms significantly influence academic visibility and the dissemination of knowledge. Despite assumptions of neutrality, these systems can reproduce or reinforce societal biases, including those related to gender. This paper introduces and applies a bias-preserving definition of algorithmic gender fairness, which assesses whether algorithmic outputs reflect real-world gender distributions without introducing or amplifying disparities. Using a heterogeneous dataset of academic profiles from German universities and universities of applied sciences, we analyse gender differences in metadata completeness, publication retrieval in academic databases, and visibility in Google search results. While we observe no overt algorithmic discrimination, our findings reveal subtle but consistent imbalances: male professors are associated with a greater number of search results and more aligned publication records, while female professors display higher variability in digital visibility. These patterns reflect the interplay between platform algorithms, institutional curation, and individual self-presentation. Our study highlights the need for fairness evaluations that account for both technical performance and representational equality in digital systems.', 'abstract_zh': '算法系统如搜索引擎和信息检索平台在很大程度上影响学术可见性和知识的传播。尽管存在中立性的假设，这些系统仍然可能会重现或加强社会偏见，包括性别偏见。本文提出并应用了一种保留偏见的算法性别公平定义，评估算法输出是否真实反映了现实世界的性别分布而没有引入或放大差异。通过德国高校和应用科学大学的异质性学术档案数据集，我们分析了性别在元数据完整度、学术数据库中的论文检索以及在Google搜索结果中的可见性方面的差异。尽管我们没有观察到明确的算法歧视，但我们的研究发现存在细微且一致的不平衡：男性教授与更多的搜索结果和更一致的出版记录相关联，而女性教授的数字可见性则显示出更高的变异性。这些模式反映了平台算法、机构策展和个人自我呈现之间的相互作用。本研究强调了在数字系统中同时考虑技术绩效和表现平等的公平性评估的必要性。', 'title_zh': '算法视角下所有性别平等吗？——搜索和检索算法的算法性别公平性分析'}
{'arxiv_id': 'arXiv:2508.05677', 'title': 'Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation', 'authors': 'Peizhuo Liu', 'link': 'https://arxiv.org/abs/2508.05677', 'abstract': "RL-based medical questionnaire systems have shown great potential in medical scenarios. However, their safety and robustness remain unresolved. This study performs a comprehensive evaluation on adversarial attack methods to identify and analyze their potential vulnerabilities. We formulate the diagnosis process as a Markov Decision Process (MDP), where the state is the patient responses and unasked questions, and the action is either to ask a question or to make a diagnosis. We implemented six prevailing major attack methods, including the Fast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini & Wagner Attack (C&W) attack, Basic Iterative Method (BIM), DeepFool, and AutoAttack, with seven epsilon values each. To ensure the generated adversarial examples remain clinically plausible, we developed a comprehensive medical validation framework consisting of 247 medical constraints, including physiological bounds, symptom correlations, and conditional medical constraints. We achieved a 97.6% success rate in generating clinically plausible adversarial samples. We performed our experiment on the National Health Interview Survey (NHIS) dataset (this https URL), which consists of 182,630 samples, to predict the participant's 4-year mortality rate. We evaluated our attacks on the AdaptiveFS framework proposed in arXiv:2004.00994. Our results show that adversarial attacks could significantly impact the diagnostic accuracy, with attack success rates ranging from 33.08% (FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict medical constraints on the input, such RL-based medical questionnaire systems still show significant vulnerabilities.", 'abstract_zh': '基于RL的医疗问卷系统在医疗场景中展示了巨大的潜力，但其安全性和鲁棒性问题尚未解决。该研究对对抗攻击方法进行了全面评估，以识别和分析其潜在漏洞。我们将诊断过程形式化为马尔可夫决策过程（MDP），其中状态为患者的反应和未问的问题，动作则是提问或诊断。我们实现了六种主要的攻击方法，包括快速梯度符号方法（FGSM）、投影梯度下降（PGD）、Carlini & Wagner攻击（C&W攻击）、基本迭代方法（BIM）、DeepFool和AutoAttack，每种方法有七个不同的ε值。为了确保生成的对抗样本在临床上具有合理性，我们开发了一个包括247个医学约束的全面医学验证框架，这些约束涵盖了生理边界、症状相关性和条件医学约束。我们成功地生成了97.6%在临床上合理的对抗样本。我们在National Health Interview Survey（NHIS）数据集中（具体内容请参见此链接）进行了实验，该数据集包含182,630个样本，用于预测参与者的4年死亡率。我们将攻击方法应用于arXiv:2004.00994中提出的AdaptiveFS框架。实验结果显示，对抗攻击显著影响了诊断准确性，攻击成功率从33.08%（FGSM）到64.70%（AutoAttack）不等。我们的工作表明，在输入受到严格医学约束的情况下，基于RL的医疗问卷系统仍表现出显著的脆弱性。', 'title_zh': '基于强化学习的医疗问卷系统对抗攻击：输入级扰动策略与医学约束验证'}
{'arxiv_id': 'arXiv:2508.05675', 'title': 'Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration', 'authors': 'Jing Wang, Zheng Li, Lei Li, Fan He, Liyu Lin, Yao Lai, Yan Li, Xiaoyang Zeng, Yufeng Guo', 'link': 'https://arxiv.org/abs/2508.05675', 'abstract': 'Recent years have witnessed growing interest in adopting large language models (LLMs) for Register Transfer Level (RTL) code optimization. While powerful cloud-based LLMs offer superior optimization capabilities, they pose unacceptable intellectual property (IP) leakage risks when processing proprietary hardware designs. In this paper, we propose a new scenario where Verilog code must be optimized for specific attributes without leaking sensitive IP information. We introduce the first IP-preserving edge-cloud collaborative framework that leverages the benefits of both paradigms. Our approach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure comparative analysis between paired high-quality target designs and novice draft codes, yielding general design principles that summarize key insights for improvements. These principles are then used to query stronger cloud LLMs (e.g., Deepseek-V3) for targeted code improvement, ensuring that only abstracted and IP-safe guidance reaches external services. Our experimental results demonstrate that the framework achieves significantly higher optimization success rates compared to baseline methods. For example, combining Qwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\\% optimization success rate for power utilization, outperforming Deepseek-V3 alone (49.81\\%) and even commercial models like GPT-4o (55.81\\%). Further investigation of local and cloud LLM combinations reveals that different model pairings exhibit varying strengths for specific optimization objectives, with interesting trends emerging when varying the number of comparative code pairs. Our work establishes a new paradigm for secure hardware design optimization that balances performance gains with IP protection.', 'abstract_zh': 'Recent Years Have Witnessed Growing Interest in Adopting Large Language Models (LLMs) for Register Transfer Level (RTL) Code Optimization: A New IP-Preserving Edge-Cloud Collaborative Framework', 'title_zh': '原理引导的Verilog优化：基于局部-云协作的IP安全知识转移'}
{'arxiv_id': 'arXiv:2508.05674', 'title': 'Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark', 'authors': 'Minghao Shao, Nanda Rani, Kimberly Milner, Haoran Xi, Meet Udeshi, Saksham Aggarwal, Venkata Sai Charan Putrevu, Sandeep Kumar Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Muhammad Shafique', 'link': 'https://arxiv.org/abs/2508.05674', 'abstract': 'Recent advances in LLM agentic systems have improved the automation of offensive security tasks, particularly for Capture the Flag (CTF) challenges. We systematically investigate the key factors that drive agent success and provide a detailed recipe for building effective LLM-based offensive security agents. First, we present CTFJudge, a framework leveraging LLM as a judge to analyze agent trajectories and provide granular evaluation across CTF solving steps. Second, we propose a novel metric, CTF Competency Index (CCI) for partial correctness, revealing how closely agent solutions align with human-crafted gold standards. Third, we examine how LLM hyperparameters, namely temperature, top-p, and maximum token length, influence agent performance and automated cybersecurity task planning. For rapid evaluation, we present CTFTiny, a curated benchmark of 50 representative CTF challenges across binary exploitation, web, reverse engineering, forensics, and cryptography. Our findings identify optimal multi-agent coordination settings and lay the groundwork for future LLM agent research in cybersecurity. We make CTFTiny open source to public this https URL along with CTFJudge on this https URL.', 'abstract_zh': '最近在LLM代理系统方面的进展提高了 Offensive 安全任务的自动化水平，特别是在 Capture the Flag (CTF) 挑战中。我们系统地研究了代理成功的关键因素，并提供了构建有效的基于LLM的 Offensive 安全代理的详细方案。首先，我们提出了CTFJudge框架，该框架利用LLM作为裁判来分析代理轨迹，并对CTF解题步骤进行详细的评价。其次，我们提出了一种新的指标——CTF能力指数（CCI），用于衡量部分正确性，揭示代理解决方案与人工制定的黄金标准之间的契合度。第三，我们考察了LLM超参数（温度、top-p和最大标记长度）如何影响代理性能和自动化网络信息安全任务规划。为了快速评估，我们提供了CTFTiny基准，包含50个代表性CTF挑战，涵盖二进制利用、Web、逆向工程、取证和密码学等多个领域。我们的发现确定了最优的多代理协调设置，并为将来在网络信息安全领域的LLM代理研究奠定了基础。我们已将CTFTiny开源，并提供CTFJudge的访问地址：这个 [链接] 和CTFTiny的访问地址：这个 [链接]。', 'title_zh': '有效 offensive 安全大语言模型代理的走向：超参数调优、大语言模型作为评委和一个轻量级 CTF 测试基准'}
{'arxiv_id': 'arXiv:2508.05673', 'title': 'Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics Optimization in Recommender Systems', 'authors': 'Weiqin Yang, Jiawei Chen, Shengjia Zhang, Peng Wu, Yuegang Sun, Yan Feng, Chun Chen, Can Wang', 'link': 'https://arxiv.org/abs/2508.05673', 'abstract': 'In the realm of recommender systems (RS), Top-$K$ ranking metrics such as NDCG@$K$ are the gold standard for evaluating recommendation performance. However, during the training of recommendation models, optimizing NDCG@$K$ poses significant challenges due to its inherent discontinuous nature and the intricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either overlooked the Top-$K$ truncation or suffered from high computational costs and training instability. To overcome these limitations, we propose SoftmaxLoss@$K$ (SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization. Specifically, we integrate the quantile technique to handle Top-$K$ truncation and derive a smooth upper bound for optimizing NDCG@$K$ to address discontinuity. The resulting SL@$K$ loss has several desirable properties, including theoretical guarantees, ease of implementation, computational efficiency, gradient stability, and noise robustness. Extensive experiments on four real-world datasets and three recommendation backbones demonstrate that SL@$K$ outperforms existing losses with a notable average improvement of 6.03%. The code is available at this https URL.', 'abstract_zh': '在推荐系统领域，Top-$K$ 排名指标如 NDCG@$K$ 是评估推荐性能的金标准。然而，在推荐模型训练过程中，优化 NDCG@$K$ 由于其固有的不连续性和 Top-$K$ 截断的复杂性而面临显著挑战。最近为优化 NDCG@$K$ 的努力要么忽略了 Top-$K$ 截断，要么遭受高计算成本和训练不稳定性的困扰。为克服这些限制，我们提出了 SoftmaxLoss@$K$（SL@$K$），这是一种针对 NDCG@$K$ 优化的新推荐损失函数。具体而言，我们结合分位数技术处理 Top-$K$ 截断，并推导出一个平滑的上界以优化 NDCG@$K$，从而解决不连续性问题。SL@$K$ 损失具有若干 desirable 属性，包括理论保证、易于实现、高效计算、梯度稳定性及抗噪声能力。在四个真实世界数据集和三种推荐模型架构上的广泛实验表明，SL@$K$ 在平均性能上优于现有损失函数，提升幅度达 6.03%。代码可在以下链接获取。', 'title_zh': '突破Top-$K$障碍：提高推荐系统中Top-$K$排名指标优化'}
{'arxiv_id': 'arXiv:2508.05672', 'title': 'LMAR: Language Model Augmented Retriever for Domain-specific Knowledge Indexing', 'authors': 'Yao Zhao, Yantian Ding, Zhiyue Zhang, Dapeng Yao, Yanxun Xu', 'link': 'https://arxiv.org/abs/2508.05672', 'abstract': 'Retrieval Augmented Generation (RAG) systems often struggle with domain-specific knowledge due to performance deterioration of pre-trained embeddings and prohibitive computational costs of large language model (LLM)-based retrievers. While fine-tuning data augmentation embedding models offers a promising direction, its effectiveness is limited by the need for high-quality training data and reliable chunking strategies that preserve contextual integrity. We propose LMAR (Language Model Augmented Retriever), a model-agnostic framework that addresses these challenges by combining LLM-guided data synthesis with contrastive embedding adaptation and efficient text clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling and synthetic data augmentation, where LLMs act as both labeler and validator to ensure high-fidelity supervision throughout the pipeline. Experimental results across multiple domain-specific benchmark datasets demonstrate that LMAR outperforms multiple baseline models, while maintaining moderate hardware requirements and low latency. Its model-agnostic nature further enables seamless integration with emerging RAG architectures and text embedding models, ensuring continual improvements without redesigning the pipeline. These results highlight LMAR as a practical and cost-effective solution for scalable domain-specific adaptation.', 'abstract_zh': '基于语言模型增强检索的系统（LMAR）：一种解决领域特定知识挑战的模型agnostic框架', 'title_zh': 'LMAR：增强型语言模型检索器用于领域特定知识索引'}
{'arxiv_id': 'arXiv:2508.05670', 'title': 'Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?', 'authors': 'Daniele Proverbio, Alessio Buscemi, Alessandro Di Stefano, Anh Han, German Castignani, Pietro Liò', 'link': 'https://arxiv.org/abs/2508.05670', 'abstract': "Game theory has long served as a foundational tool in cybersecurity to test, predict, and design strategic interactions between attackers and defenders. The recent advent of Large Language Models (LLMs) offers new tools and challenges for the security of computer systems; In this work, we investigate whether classical game-theoretic frameworks can effectively capture the behaviours of LLM-driven actors and bots. Using a reproducible framework for game-theoretic LLM agents, we investigate two canonical scenarios -- the one-shot zero-sum game and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to expected outcomes or exhibit deviations due to embedded biases. Our experiments involve four state-of-the-art LLMs and span five natural languages, English, French, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic sensitivity. For both games, we observe that the final payoffs are influenced by agents characteristics such as personality traits or knowledge of repeated rounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to the choice of languages, which should warn against indiscriminate application of LLMs in cybersecurity applications and call for in-depth studies, as LLMs may behave differently when deployed in different countries. We also employ quantitative metrics to evaluate the internal consistency and cross-language stability of LLM agents, to help guide the selection of the most stable LLMs and optimising models for secure applications.", 'abstract_zh': '博弈论历来是网络安全中的一个基础工具，用于测试、预测和设计攻击者与防御者之间的战略互动。最近大型语言模型（LLMs）的出现为计算机系统的安全性提供了新的工具和挑战；在本工作中，我们探讨经典博弈论框架是否能够有效捕捉LLM驱动的行动者和机器人行为。利用博弈论LLM代理的可重复框架，我们研究了两种经典场景——一次性零和博弈和动态囚徒困境，并测试LLM是否收敛到预期结果或因嵌入的偏差而表现出偏差。实验涉及前沿的四种大型语言模型，并涵盖五种自然语言——英语、法语、阿拉伯语、越南语和普通话，以评估语言敏感性。对于两种博弈，我们观察到最终收益受到代理特性如人格特质或对重复轮次的认知的影响。此外，我们发现最终收益对语言选择的意外敏感性，这应警惕在网络安全应用中不分青红皂白地应用LLM，并呼吁深入研究，因为LLM在不同国家部署时可能会有不同的行为。我们还运用定量指标来评估LLM代理的内部一致性和跨语言稳定性，以帮助指导选择最稳定的LLM和优化模型以满足安全应用需求。', 'title_zh': '基于博弈论的网络安全场景，大型语言模型能否有效提供？'}
{'arxiv_id': 'arXiv:2508.05669', 'title': 'Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports', 'authors': 'Jin Khye Tan, En Jun Choong, Ethan Jeremiah Chitty, Yan Pheng Choo, John Hsin Yang Wong, Chern Eu Cheah', 'link': 'https://arxiv.org/abs/2508.05669', 'abstract': "Accurately extracting and representing the structure of tabular data from financial documents remains a critical challenge in document understanding, particularly for regulatory and analytical use cases. This study addresses the complexity of converting financial tables from Malaysian audited financial reports into Markdown format, a task complicated by rotated layouts, multi-level headers, and implicit structural cues. We propose a fine-tuned vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for high-fidelity Markdown generation from document images. Our approach includes a curated dataset of 2,152 image-text pairs with augmentations and a supervised fine-tuning strategy using LoRA. To assess performance, we evaluated our model on 100 out-of-sample tables using a dual framework: a criteria-based LLM-as-a-judge for fine-grained accuracy and our novel Markdown Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based assessment and a 96.53% Markdown TEDS score. This performance significantly surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized reasoning-enabled models. Compared to these self-hosted alternatives, it also significantly reduces inference time. Furthermore, its accuracy exceeds that of widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash. These results demonstrate that domain-specific fine-tuning provides an effective and efficient method to bridge the gap between unstructured financial documents and downstream automation, rivalling much larger and more general models without their computational overhead.", 'abstract_zh': '准确从财务文件中提取和表示表格数据的结构仍然是文档理解中的一个关键挑战，尤其是在监管和分析应用中。本研究致力于解决将马来西亚审计财务报告中的财务表格转换为Markdown格式的复杂性，该任务受到旋转布局、多级表头和隐含结构线索的困扰。我们提出了一种基于Qwen2.5-VL-7B微调的视觉语言模型（VLM），该模型优化了从文档图像生成高质量Markdown的功能。我们的方法包括一个包含2152个图像-文本对的自定义数据集，以及使用LoRA的监督微调策略。为了评估性能，我们使用基于标准的LLM-as-a-judge双框架评估了100个外部样本表格，以及我们新颖的Markdown Tree-Edit-Distance-based Similarity (TEDS) 全局结构保真度度量标准。我们的模型在基于标准的评估中达到了92.20%的整体准确率，在Markdown TEDS评分中达到了96.53%。这一性能显著优于其基模型Qwen2.5-VL-7B、更大规模的VLM和专业的推理增强模型。与这些托管在本地的替代模型相比，它还显著减少了推理时间。此外，其准确率超过了广泛使用的专有模型如OpenAI的GPT-4o和Gemini 2.5 Flash。这些结果表明，针对特定领域的微调提供了一种有效且经济的方法，可以弥合非结构化财务文件和下游自动化之间的差距，而且这与更大且更通用的模型相比，没有其计算开销。', 'title_zh': 'Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports\r\n精调视觉-语言模型以实现马来西亚审计财务报表中财务表格的Markdown转换'}
{'arxiv_id': 'arXiv:2508.05668', 'title': 'A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges', 'authors': 'Yunjia Xi, Jianghao Lin, Yongzhao Xiao, Zheli Zhou, Rong Shan, Te Gao, Jiachen Zhu, Weiwen Liu, Yong Yu, Weinan Zhang', 'link': 'https://arxiv.org/abs/2508.05668', 'abstract': "The advent of Large Language Models (LLMs) has significantly revolutionized web search. The emergence of LLM-based Search Agents marks a pivotal shift towards deeper, dynamic, autonomous information seeking. These agents can comprehend user intentions and environmental context and execute multi-turn retrieval with dynamic planning, extending search capabilities far beyond the web. Leading examples like OpenAI's Deep Research highlight their potential for deep information mining and real-world applications. This survey provides the first systematic analysis of search agents. We comprehensively analyze and categorize existing works from the perspectives of architecture, optimization, application, and evaluation, ultimately identifying critical open challenges and outlining promising future research directions in this rapidly evolving field. Our repository is available on this https URL.", 'abstract_zh': '大规模语言模型（LLMs）的出现显著革新了网络搜索。基于LLM的搜索代理的 emergence 标志着向更深层次、动态自主的信息搜索的一次关键转变。这些代理能够理解用户意图和环境上下文，并执行多轮检索和动态规划，将搜索能力远远拓展至网页之外。领先的研究实例如OpenAI的Deep Research突显了其在深度信息挖掘和实际应用方面的潜力。本文提供了首个系统分析搜索代理的综述。我们从架构、优化、应用和评估等视角全面分析和分类了现有工作，最终确定了这一迅速发展的领域中的关键开放挑战，并概述了有望 future 研究方向。我们的资源库可供在此链接访问。', 'title_zh': '基于LLM的深度搜索代理综述：范式、优化、评估及挑战'}
{'arxiv_id': 'arXiv:2508.05667', 'title': 'ITDR: An Instruction Tuning Dataset for Enhancing Large Language Models in Recommendations', 'authors': 'Zekun Liu, Xiaowen Huang, Jitao Sang', 'link': 'https://arxiv.org/abs/2508.05667', 'abstract': 'Large language models (LLMs) have demonstrated outstanding performance in natural language processing tasks. However, in the field of recommendation systems, due to the structural differences between user behavior data and natural language, LLMs struggle to effectively model the associations between user preferences and items. Although prompt-based methods can generate recommendation results, their inadequate understanding of recommendation tasks leads to constrained performance. To address this gap, in this work, we construct a sufficient instruction tuning dataset, ITDR, which encompasses 7 subtasks across two core root tasks--user-item interaction and user-item understanding. The dataset integrates data from 13 public recommendation datasets and is built using manually crafted standardized templates, comprising approximately 200,000 instances. Experimental results demonstrate that ITDR significantly enhances the performance of mainstream open-source LLMs such as GLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks. Furthermore, we analyze the correlations between tasks and explore the impact of task descriptions and data scale on instruction tuning effectiveness. Finally, we perform comparative experiments against closed-source LLMs with substantial parameters. Our tuning dataset ITDR and the fine-tuned large recommendation models can be accessed at this https URL.', 'abstract_zh': '大语言模型（LLMs）在自然语言处理任务中展示了出色的表现。然而，在推荐系统领域，由于用户行为数据与自然语言之间的结构差异，LLMs难以有效建模用户偏好与物品之间的关联。尽管基于提示的方法可以生成推荐结果，但由于其对推荐任务理解不足，导致其性能受限。为解决这一问题，本文构建了一个足够的指令调优数据集ITDR，包含了两个核心任务——用户-物品交互和用户-物品理解的7个子任务。数据集整合了13个公开推荐数据集，并采用手工构建的标准模板构建，共计约200,000个实例。实验结果表明，ITDR显著提升了主流开源LLMs（如GLM-4、Qwen2.5、Qwen2.5-Instruct和LLaMA-3.2）在推荐任务中的性能。此外，我们分析了任务之间的相关性，并探讨了任务描述和数据规模对指令调优效果的影响。最后，我们与具有大量参数的闭源LLMs进行了对比实验。本文的调优数据集ITDR及细调的大型推荐模型可从以下链接获取：这个 https URL。', 'title_zh': 'ITDR：用于增强推荐系统大型语言模型的指令调优数据集'}
{'arxiv_id': 'arXiv:2508.05666', 'title': 'HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis', 'authors': 'Alejandro Godinez', 'link': 'https://arxiv.org/abs/2508.05666', 'abstract': 'We present HySemRAG, a framework that combines Extract, Transform, Load (ETL) pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale literature synthesis and identify methodological research gaps. The system addresses limitations in existing RAG architectures through a multi-layered approach: hybrid retrieval combining semantic search, keyword filtering, and knowledge graph traversal; an agentic self-correction framework with iterative quality assurance; and post-hoc citation verification ensuring complete traceability. Our implementation processes scholarly literature through eight integrated stages: multi-source metadata acquisition, asynchronous PDF retrieval, custom document layout analysis using modified Docling architecture, bibliographic management, LLM-based field extraction, topic modeling, semantic unification, and knowledge graph construction. The system creates dual data products - a Neo4j knowledge graph enabling complex relationship queries and Qdrant vector collections supporting semantic search - serving as foundational infrastructure for verifiable information synthesis. Evaluation across 643 observations from 60 testing sessions demonstrates structured field extraction achieving 35.1% higher semantic similarity scores (0.655 $\\pm$ 0.178) compared to PDF chunking approaches (0.485 $\\pm$ 0.204, p < 0.000001). The agentic quality assurance mechanism achieves 68.3% single-pass success rates with 99.0% citation accuracy in validated responses. Applied to geospatial epidemiology literature on ozone exposure and cardiovascular disease, the system identifies methodological trends and research gaps, demonstrating broad applicability across scientific domains for accelerating evidence synthesis and discovery.', 'abstract_zh': 'HySemRAG：结合抽取、转换、加载（ETL）管道与检索增强生成（RAG）的框架，实现大规模文献综合并识别方法学研究空白', 'title_zh': 'HySemRAG：一种混合语义检索增强生成框架，用于自动化文献综合与方法论缺口分析'}
{'arxiv_id': 'arXiv:2508.05664', 'title': 'Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support', 'authors': 'Hei Yu Chan, Kuok Tou Ho, Chenglong Ma, Yujing Si, Hok Lai Lin, Sa Lei Lam', 'link': 'https://arxiv.org/abs/2508.05664', 'abstract': 'Many AI customer service systems use standard NLP pipelines or finetuned language models, which often fall short on ambiguous, multi-intent, or detail-specific queries. This case study evaluates recent techniques: query rewriting, RAG Fusion, keyword augmentation, intent recognition, and context reranking, for building a robust customer support system in the electric power domain. We compare vector-store and graph-based RAG frameworks, ultimately selecting the graph-based RAG for its superior performance in handling complex queries. We find that query rewriting improves retrieval for queries using non-standard terminology or requiring precise detail. RAG Fusion boosts performance on vague or multifaceted queries by merging multiple retrievals. Reranking reduces hallucinations by filtering irrelevant contexts. Intent recognition supports the decomposition of complex questions into more targeted sub-queries, increasing both relevance and efficiency. In contrast, keyword augmentation negatively impacts results due to biased keyword selection. Our final system combines intent recognition, RAG Fusion, and reranking to handle disambiguation and multi-source queries. Evaluated on both a GPT-4-generated dataset and a real-world electricity provider FAQ dataset, it achieves 97.9% and 89.6% accuracy respectively, substantially outperforming baseline RAG models.', 'abstract_zh': '许多AI客服系统采用标准NLP流水线或微调的语言模型，往往在处理模糊、多意图或详细特定的查询时表现不佳。本案例研究评估了近期技术：查询重写、RAG融合、关键词扩充、意图识别和上下文重排名，以构建电力领域的 robust 客户支持系统。我们比较了基于向量存储和图的RAG框架，最终选择了图的RAG框架，因为它在处理复杂查询时表现出更强的性能。我们发现，查询重写可以提高使用非标准术语或需要精确细节的查询的检索效果。RAG融合通过合并多个检索结果来增强对模糊或多方面查询的性能。重排名通过过滤无关上下文来减少幻觉。意图识别支持将复杂问题分解为更具体的子查询，从而提高相关性和效率。相比之下，关键词扩充由于关键词选择偏向性而负面影响结果。我们的最终系统结合了意图识别、RAG融合和重排名，以处理歧义和多来源查询。该系统在GPT-4生成的数据集和现实世界的电力提供商FAQ数据集上分别达到了97.9%和89.6%的准确率，显著优于基础RAG模型。', 'title_zh': '增强基于检索的生成以改善电力行业客户支持'}
{'arxiv_id': 'arXiv:2508.05662', 'title': 'From Static to Dynamic: A Streaming RAG Approach to Real-time Knowledge Base', 'authors': 'Yuzhou Zhu', 'link': 'https://arxiv.org/abs/2508.05662', 'abstract': 'Dynamic streams from news feeds, social media, sensor networks, and financial markets challenge static RAG frameworks. Full-scale indices incur high memory costs; periodic rebuilds introduce latency that undermines data freshness; naive sampling sacrifices semantic coverage. We present Streaming RAG, a unified pipeline that combines multi-vector cosine screening, mini-batch clustering, and a counter-based heavy-hitter filter to maintain a compact prototype set. We further prove an approximation bound \\$E\\[R(K\\_t)] \\ge R^\\* - L \\Delta\\$ linking retrieval quality to clustering variance. An incremental index upsert mechanism refreshes prototypes without interrupting queries. Experiments on eight real-time streams show statistically significant gains in Recall\\@10 (up to 3 points, p < 0.01), end-to-end latency below 15 ms, and throughput above 900 documents per second under a 150 MB budget. Hyperparameter sensitivity analysis over cluster count, admission probability, relevance threshold, and counter capacity validates default settings. In open-domain question answering with GPT-3.5 Turbo, we record 3.2-point gain in Exact Match and 2.8-point gain in F1 on SQuAD; abstractive summarization yields ROUGE-L improvements. Streaming RAG establishes a new Pareto frontier for retrieval augmentation.', 'abstract_zh': '动态流处理新闻 feed、社交媒体、传感器网络和金融市场数据挑战静态RAG框架。Streaming RAG：统一多向量余弦筛选、 mini-批聚类和基于计数的重量级元素过滤管道以维护紧凑原型集', 'title_zh': '从静态到动态：一种实时知识库的流式RAG方法'}
{'arxiv_id': 'arXiv:2508.05661', 'title': 'Zero-Shot Retrieval for Scalable Visual Search in a Two-Sided Marketplace', 'authors': 'Andre Rusli, Shoma Ishimoto, Sho Akiyama, Aman Kumar Singh', 'link': 'https://arxiv.org/abs/2508.05661', 'abstract': "Visual search offers an intuitive way for customers to explore diverse product catalogs, particularly in consumer-to-consumer (C2C) marketplaces where listings are often unstructured and visually driven. This paper presents a scalable visual search system deployed in Mercari's C2C marketplace, where end-users act as buyers and sellers. We evaluate recent vision-language models for zero-shot image retrieval and compare their performance with an existing fine-tuned baseline. The system integrates real-time inference and background indexing workflows, supported by a unified embedding pipeline optimized through dimensionality reduction. Offline evaluation using user interaction logs shows that the multilingual SigLIP model outperforms other models across multiple retrieval metrics, achieving a 13.3% increase in nDCG@5 over the baseline. A one-week online A/B test in production further confirms real-world impact, with the treatment group showing substantial gains in engagement and conversion, up to a 40.9% increase in transaction rate via image search. Our findings highlight that recent zero-shot models can serve as a strong and practical baseline for production use, which enables teams to deploy effective visual search systems with minimal overhead, while retaining the flexibility to fine-tune based on future data or domain-specific needs.", 'abstract_zh': '视觉搜索为顾客探索多样化的商品目录提供了一种直观的方式，特别是在消费者对消费者（C2C）市场中，商品列表往往无结构且以视觉为导向。本文介绍了Mercari C2C市场中部署的一种可扩展的视觉搜索系统，其中最终用户既充当买家也充当卖家。我们评估了最近的视觉-语言模型在零样本图像检索中的性能，并将其与现有微调基线进行了比较。该系统结合了实时推理和后台索引工作流程，通过降维优化实现了统一嵌入管道。使用用户交互日志的离线评估显示，多语言SigLIP模型在多个检索指标上优于其他模型，对基线的nDCG@5提高了13.3%。生产环境中的为期一周的A/B测试进一步证实了实际影响，治疗组在通过图像搜索进行互动和转化方面分别提高了40.9%。我们的研究结果表明，最近的零样本模型可以作为生产环境中强有力的实用基线，使得团队能够在最少的开销下部署有效的视觉搜索系统，同时能够根据未来的数据或特定领域需求进行微调。', 'title_zh': '面向双方市场的可扩展视觉搜索零样本检索'}
{'arxiv_id': 'arXiv:2508.05660', 'title': 'Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review', 'authors': 'Aditya Nagori, Ricardo Accorsi Casonatto, Ayush Gautam, Abhinav Manikantha Sai Cheruvu, Rishikesan Kamaleswaran', 'link': 'https://arxiv.org/abs/2508.05660', 'abstract': "The surge in scientific publications challenges traditional review methods, demanding tools that integrate structured metadata with full-text analysis. Hybrid Retrieval Augmented Generation (RAG) systems, combining graph queries with vector search offer promise but are typically static, rely on proprietary tools, and lack uncertainty estimates. We present an agentic approach that encapsulates the hybrid RAG pipeline within an autonomous agent capable of (1) dynamically selecting between GraphRAG and VectorRAG for each query, (2) adapting instruction-tuned generation in real time to researcher needs, and (3) quantifying uncertainty during inference. This dynamic orchestration improves relevance, reduces hallucinations, and promotes reproducibility.\nOur pipeline ingests bibliometric open-access data from PubMed, arXiv, and Google Scholar APIs, builds a Neo4j citation-based knowledge graph (KG), and embeds full-text PDFs into a FAISS vector store (VS) using the all-MiniLM-L6-v2 model. A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher for KG) or VectorRAG (combining sparse and dense retrieval with re-ranking). Instruction tuning refines domain-specific generation, and bootstrapped evaluation yields standard deviation for evaluation metrics.\nOn synthetic benchmarks mimicking real-world queries, the Instruction-Tuned Agent with Direct Preference Optimization (DPO) outperforms the baseline, achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score, 0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall Precision. These results highlight the system's improved reasoning over heterogeneous sources and establish a scalable framework for autonomous, agentic scientific discovery.", 'abstract_zh': '科学出版物的激增挑战了传统审核方法，要求集成结构化元数据与全文分析的工具。混合检索增强生成（RAG）系统结合图查询与向量搜索展现出潜力，但通常较为静态，依赖于专有工具，并缺乏不确定性估计。我们提出了一种代理方法，将混合RAG管道封装在一个自主代理中，该代理能够（1）根据每个查询动态选择GraphRAG和VectorRAG，（2）实时适应研究需求进行指令微调生成，以及（3）在推断过程中量化不确定性。这种动态编排提高了相关性，减少了妄言，并促进了可重复性。\n\n我们的管道从PubMed、arXiv和Google Scholar API摄取 bibliometric 开放访问数据，构建基于引文的Neo4j知识图谱（KG），并使用all-MiniLM-L6-v2模型将全文PDF嵌入FAISS向量存储（VS）。Llama-3.3-70B代理选择GraphRAG（将查询转换为KG的Cypher查询）或VectorRAG（结合稀疏和密集检索并重新排序）。指令微调细化了领域特定生成，而自强化评估提供了评估指标的标准差。\n\n在模拟实际情况查询的合成基准测试中，带有直接偏好优化（DPO）的指令微调代理优于基线，VS上下文召回率提高0.63，整体上下文精准度提高0.56。其他改进包括VS忠实度提高0.24、VS精准度提高0.12和知识图谱答案相关性提高0.12、整体忠实度评分提高0.11、知识图谱上下文召回率提高0.05以及VS答案相关性和整体精准度分别提高0.04。这些结果突显了系统在异构来源上改进推理的能力，并建立了自主、代理化的科学发现可扩展框架。', 'title_zh': '开源代理混合检索框架：科学文献综述'}
{'arxiv_id': 'arXiv:2508.05657', 'title': 'Beyond Single Labels: Improving Conversational Recommendation through LLM-Powered Data Augmentation', 'authors': 'Haozhe Xu, Xiaohua Wang, Changze Lv, Xiaoqing Zheng', 'link': 'https://arxiv.org/abs/2508.05657', 'abstract': 'Conversational recommender systems (CRSs) enhance recommendation quality by engaging users in multi-turn dialogues, capturing nuanced preferences through natural language interactions. However, these systems often face the false negative issue, where items that a user might like are incorrectly labeled as negative during training, leading to suboptimal this http URL the label set through data augmentation presents an intuitive solution but faces the challenge of balancing two key aspects: ensuring semantic relevance and preserving the collaborative information inherent in CRS datasets. To address these issues, we propose a novel data augmentation framework that first leverages an LLM-based semantic retriever to identify diverse and semantically relevant items, which are then filtered by a relevance scorer to remove noisy candidates. Building on this, we introduce a two-stage training strategy balancing semantic relevance and collaborative information. Extensive experiments on two benchmark datasets and user simulators demonstrate significant and consistent performance improvements across various recommenders, highlighting the effectiveness of our approach in advancing CRS performance.', 'abstract_zh': '基于对话的推荐系统通过多轮对话提高推荐质量，利用自然语言交互捕获用户的细微偏好。然而，这些系统常常面临假阴性问题，即用户可能喜欢的项目在训练过程中被错误地标记为负面反馈，导致推荐效果不佳。通过数据增强弥补标签集的不足是一种直观的解决方案，但面临着确保语义相关性和保留对话式推荐系统数据集中的协作信息这两方面之间的平衡挑战。为了解决这些问题，我们提出了一种新颖的数据增强框架，首先利用基于LLM的语义检索器识别多样且语义相关的目标项，然后通过相关性评分器去除噪声候选项。在此基础上，我们引入了一种两阶段训练策略，平衡语义相关性和协作信息。在两个基准数据集和用户模拟器上进行的广泛实验表明，我们的方法在各种推荐器上均能显著且一致地提高性能，突显了该方法在推动对话式推荐系统性能提升方面的有效性。', 'title_zh': '超越单一标签：通过LLM驱动的数据增强提升对话推荐'}
{'arxiv_id': 'arXiv:2508.05654', 'title': 'Comparison of Information Retrieval Techniques Applied to IT Support Tickets', 'authors': 'Leonardo Santiago Benitez Pereira, Robinson Pizzio, Samir Bonho', 'link': 'https://arxiv.org/abs/2508.05654', 'abstract': 'Institutions dependent on IT services and resources acknowledge the crucial significance of an IT help desk system, that act as a centralized hub connecting IT staff and users for service requests. Employing various Machine Learning models, these IT help desk systems allow access to corrective actions used in the past, but each model has different performance when applied to different datasets. This work compares eleven Information Retrieval techniques in a dataset of IT support tickets, with the goal of implementing a software that facilitates the work of Information Technology support analysts. The best results were obtained with the Sentence-BERT technique, in its multi-language variation distilluse-base-multilingual-cased-v1, where 78.7% of the recommendations made by the model were considered relevant. TF-IDF (69.0%), Word2vec (68.7%) and LDA (66.3%) techniques also had consistent results. Furthermore, the used datasets and essential parts of coding have been published and made open source. It also demonstrated the practicality of a support ticket recovery system by implementing a minimal viable prototype, and described in detail the implementation of the system. Finally, this work proposed a novel metric for comparing the techniques, whose aim is to closely reflect the perception of the IT analysts about the retrieval quality.', 'abstract_zh': '基于IT服务和资源的机构认识到IT帮助台系统的重要性，该系统充当IT人员和用户之间的集中枢纽，用于服务请求。这些IT帮助台系统利用各种机器学习模型，允许访问过去采取的纠正措施，但每个模型在应用于不同数据集时表现不同。本研究在IT支持票务数据集中比较了十一种信息检索技术，旨在实施一款支持信息技术支持分析师工作的软件。Sentence-BERT技术（特别是在多语言变体distilluse-base-multilingual-cased-v1）取得了最佳效果，其中78.7%的模型推荐被认为相关。TF-IDF（69.0%）、Word2vec（68.7%）和LDA（66.3%）技术也取得了持续的结果。此外，使用的数据集和编码的关键部分已被发布并开源。该研究还通过实现最小可行原型演示了支持票务恢复系统的实用性，并详细描述了系统的实现。最后，本文提出了一种新的技术对比指标，其目的是尽量贴近信息技术分析师对于检索质量的感知。', 'title_zh': 'IT支持票文中信息检索技术的比较'}
{'arxiv_id': 'arXiv:2508.05653', 'title': 'Modeling Interactive Narrative Systems: A Formal Approach', 'authors': 'Jules Clerc, Domitile Lourdeaux, Mohamed Sallak, Johann Barbier, Marc Ravaine', 'link': 'https://arxiv.org/abs/2508.05653', 'abstract': 'Interactive Narrative Systems (INS) have revolutionized digital experiences by empowering users to actively shape their stories, diverging from traditional passive storytelling. However, the field faces challenges due to fragmented research efforts and diverse system representations. This paper introduces a formal representation framework for INS, inspired by diverse approaches from the state of the art. By providing a consistent vocabulary and modeling structure, the framework facilitates the analysis, the description and comparison of INS properties. Experimental validations on the "Little Red Riding Hood" scenario highlight the usefulness of the proposed formalism and its impact on improving the evaluation of INS. This work aims to foster collaboration and coherence within the INS research community by proposing a methodology for formally representing these systems.', 'abstract_zh': '交互叙事系统（INS）通过让用户主动塑造故事，革新了数字体验，超越了传统的被动叙述方式。然而，由于研究努力分散和系统表示多样化的挑战，该领域面临挑战。本文介绍了一种正式表示框架，该框架受到最新技术多样方法的启发。通过提供一致的词汇和建模结构，该框架促进了对INS属性的分析、描述和比较。在“小红帽”场景上的实验验证强调了所提出正式主义的实用性及其对改进INS评估的影响。本文旨在通过提出正式表示这些系统的办法，促进INS研究社区的合作与一致性。', 'title_zh': '建模互动叙事系统：一种形式化方法'}
{'arxiv_id': 'arXiv:2508.05652', 'title': 'Lessons from A Large Language Model-based Outdoor Trail Recommendation Chatbot with Retrieval Augmented Generation', 'authors': 'Julia Ann Mathew, Suining He', 'link': 'https://arxiv.org/abs/2508.05652', 'abstract': 'The increasing popularity of outdoor recreational activities (such as hiking and biking) has boosted the demand for a conversational AI system to provide informative and personalized suggestion on outdoor trails. Challenges arise in response to (1) how to provide accurate outdoor trail information via conversational AI; and (2) how to enable usable and efficient recommendation services. To address above, this paper discusses the preliminary and practical lessons learned from developing Judy, an outdoor trail recommendation chatbot based on the large language model (LLM) with retrieval augmented generation (RAG). To gain concrete system insights, we have performed case studies with the outdoor trails in Connecticut (CT), US. We have conducted web-based data collection, outdoor trail data management, and LLM model performance studies on the RAG-based recommendation. Our experimental results have demonstrated the accuracy, effectiveness, and usability of Judy in recommending outdoor trails based on the LLM with RAG.', 'abstract_zh': '户外休闲活动（如远足和 biking）日益增长的流行性推动了对基于对话式AI系统的信息丰富且个性化的户外路线建议的需求。本研究针对如何通过对话式AI提供准确的户外路线信息以及如何实现可用且高效的推荐服务提出了挑战。为应对这些挑战，本文讨论了基于大型语言模型（LLM）和检索增强生成（RAG）开发户外路线推荐聊天机器人Judy的初步和实用经验教训。通过案例研究，我们在美国康涅狄格州（CT）的户外路线收集了网络数据并管理了路线数据，研究了基于RAG的推荐性能。我们的实验结果证明了Judy基于LLM与RAG推荐户外路线的准确性和有效性。', 'title_zh': '基于检索增强生成的大语言模型 Outdoor 路径推荐聊天机器人启示'}
{'arxiv_id': 'arXiv:2508.05650', 'title': 'OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools', 'authors': 'Jiaxuan Liang, Shide Zhou, Kailong Wang', 'link': 'https://arxiv.org/abs/2508.05650', 'abstract': 'While Retrieval Augmented Generation (RAG) is now widely adopted to enhance LLMs, evaluating its true performance benefits in a reproducible and interpretable way remains a major hurdle. Existing methods often fall short: they lack domain coverage, employ coarse metrics that miss sub document precision, and fail to capture computational trade offs. Most critically, they provide no standardized framework for comparing RAG effectiveness across different models and domains.\nWe introduce OmniBench RAG, a novel automated platform for multi domain evaluation of RAG systems. The platform quantifies performance gains across accuracy and efficiency dimensions, spanning nine knowledge fields including culture, geography, and health. We introduce two standardized metrics: Improvements (accuracy gains) and Transformation (efficiency differences between pre RAG and post RAG models), enabling reproducible comparisons across models and tasks. The platform features dynamic test generation, modular evaluation pipelines, and automated knowledge base construction. Our evaluation reveals striking variability in RAG effectiveness, from significant gains in culture to declines in mathematics, highlighting the critical importance of systematic, domain aware assessment. A demonstration video is available at: this https URL. Code and datasets: this https URL.', 'abstract_zh': 'whilst Retrieval Augmented Generation (RAG) 的增强在大规模语言模型中已广泛采用，以评估其真正性能提升并在可重复和可解释的方式上仍然存在重大障碍，现有方法往往难以满足要求：它们缺乏领域覆盖面，使用粗略的指标忽视了子文档精确度，并未能捕捉到计算权衡。最重要的是，它们未能提供一个标准化的框架来跨不同模型和领域比较 RAG 的有效性。\n\n我们引入了 OmniBench RAG，这是一种新型自动化多领域评估平台，用于评估 RAG 系统。该平台在准确性和效率维度上量化了性能增益，涵盖了包括文化、地理和健康在内的九个知识领域。我们引入了两个标准化指标：Improvements（准确度增益）和Transformation（预RAG模型与后RAG模型之间的效率差异），这使得不同模型和任务之间的可重复比较成为可能。该平台具备动态测试生成、模块化评估管道和自动化知识库构建功能。我们的评估揭示了 RAG 有效性在不同领域中的显著差异，从文化领域的显著增益到数学领域的下降，突显了系统化、领域意识评估的重要性。演示视频见：this https URL。代码和数据集见：this https URL。', 'title_zh': 'OmniBench-RAG：检索增强生成工具的多领域评估平台'}
{'arxiv_id': 'arXiv:2508.05648', 'title': 'AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups', 'authors': 'Chandler Campbell, Bernie Boscoe, Tuan Do', 'link': 'https://arxiv.org/abs/2508.05648', 'abstract': "Research groups face persistent challenges in capturing, storing, and retrieving knowledge that is distributed across team members. Although structured data intended for analysis and publication is often well managed, much of a group's collective knowledge remains informal, fragmented, or undocumented--often passed down orally through meetings, mentoring, and day-to-day collaboration. This includes private resources such as emails, meeting notes, training materials, and ad hoc documentation. Together, these reflect the group's tacit knowledge--the informal, experience-based expertise that underlies much of their work. Accessing this knowledge can be difficult, requiring significant time and insider understanding. Retrieval-augmented generation (RAG) systems offer promising solutions by enabling users to query and generate responses grounded in relevant source material. However, most current RAG-LLM systems are oriented toward public documents and overlook the privacy concerns of internal research materials. We introduce AquiLLM (pronounced ah-quill-em), a lightweight, modular RAG system designed to meet the needs of research groups. AquiLLM supports varied document types and configurable privacy settings, enabling more effective access to both formal and informal knowledge within scholarly groups.", 'abstract_zh': '研究团队在捕获、存储和检索分布在成员之间的知识方面面临持久性挑战。尽管旨在分析和发布的结构化数据通常得到了良好的管理，但团队的许多集体知识仍以非正式、碎片化或未文档化的方式存在——这些知识往往通过会议、指导和日常合作以口头形式传递。这包括私人资源，如电子邮件、会议笔记、培训材料和临时文档。这些共同反映了团队的隐形知识——基于经验和非正式的专业知识，构成了他们工作的重要部分。访问这些知识往往需要耗费大量时间和内部理解。检索增强生成（RAG）系统通过使用户能够查询并生成基于相关来源材料的响应，提供了有前景的解决方案。然而，目前大多数RAG-LLM系统主要面向公共文件，并忽略了内部研究材料的隐私问题。我们介绍了一种轻量级模块化RAG系统AquiLLM（发音为ah-quill-em），旨在满足研究团队的需求。AquiLLM支持多种文档类型和可配置的隐私设置，使学术团队更有效地访问正式和非正式知识。', 'title_zh': 'AquiLLM：一个用于捕获研究团队隐性知识的检索增强工具'}
{'arxiv_id': 'arXiv:2508.05647', 'title': 'Query-Aware Graph Neural Networks for Enhanced Retrieval-Augmented Generation', 'authors': 'Vibhor Agrawal, Fay Wang, Rishi Puri', 'link': 'https://arxiv.org/abs/2508.05647', 'abstract': 'We present a novel graph neural network (GNN) architecture for retrieval-augmented generation (RAG) that leverages query-aware attention mechanisms and learned scoring heads to improve retrieval accuracy on complex, multi-hop questions. Unlike traditional dense retrieval methods that treat documents as independent entities, our approach constructs per-episode knowledge graphs that capture both sequential and semantic relationships between text chunks. We introduce an Enhanced Graph Attention Network with query-guided pooling that dynamically focuses on relevant parts of the graph based on user queries. Experimental results demonstrate that our approach significantly outperforms standard dense retrievers on complex question answering tasks, particularly for questions requiring multi-document reasoning. Our implementation leverages PyTorch Geometric for efficient processing of graph-structured data, enabling scalable deployment in production retrieval systems', 'abstract_zh': '一种基于查询感知注意力机制和学习评分头部的新型图神经网络架构：增强图注意力网络在复杂多跳问答中的检索增强生成', 'title_zh': '面向查询的图神经网络在增强检索增强生成中的应用'}
{'arxiv_id': 'arXiv:2508.05640', 'title': 'Request-Only Optimization for Recommendation Systems', 'authors': 'Liang Guo, Wei Li, Lucy Liao, Huihui Cheng, Rui Zhang, Yu Shi, Yueming Wang, Yanzun Huang, Keke Zhai, Pengchao Wang, Timothy Shi, Xuan Cao, Shengzhi Wang, Renqin Cai, Zhaojie Gong, Omkar Vichare, Rui Jian, Leon Gao, Shiyan Deng, Xingyu Liu, Xiong Zhang, Fu Li, Wenlei Xie, Bin Wen, Rui Li, Xing Liu, Jiaqi Zhai', 'link': 'https://arxiv.org/abs/2508.05640', 'abstract': 'Deep Learning Recommendation Models (DLRMs) represent one of the largest machine learning applications on the planet. Industry-scale DLRMs are trained with petabytes of recommendation data to serve billions of users every day. To utilize the rich user signals in the long user history, DLRMs have been scaled up to unprecedented complexity, up to trillions of floating-point operations (TFLOPs) per example. This scale, coupled with the huge amount of training data, necessitates new storage and training algorithms to efficiently improve the quality of these complex recommendation systems. In this paper, we present a Request-Only Optimizations (ROO) training and modeling paradigm. ROO simultaneously improves the storage and training efficiency as well as the model quality of recommendation systems. We holistically approach this challenge through co-designing data (i.e., request-only data), infrastructure (i.e., request-only based data processing pipeline), and model architecture (i.e., request-only neural architectures). Our ROO training and modeling paradigm treats a user request as a unit of the training data. Compared with the established practice of treating a user impression as a unit, our new design achieves native feature deduplication in data logging, consequently saving data storage. Second, by de-duplicating computations and communications across multiple impressions in a request, this new paradigm enables highly scaled-up neural network architectures to better capture user interest signals, such as Generative Recommenders (GRs) and other request-only friendly architectures.', 'abstract_zh': '深度学习推荐模型（DLRMs）代表了世界上最大的机器学习应用之一。工业规模的DLRMs通过巨量的推荐数据训练，每天为数十亿用户提供服务。为了利用长用户历史中的丰富用户信号，DLRMs被扩展到前所未有的复杂度，每例达到数万亿次浮点运算（TFLOPs）。这种规模，结合大量的训练数据，需要新的存储和训练算法以高效地改进这些复杂推荐系统的质量。在本文中，我们提出了一种仅请求优化（ROO）的训练和建模范式。ROO 同时改进了推荐系统的存储和训练效率以及模型质量。通过协同设计数据（即仅请求数据）、基础设施（即基于请求的数据处理管道）以及模型架构（即仅请求的神经架构），我们从整体上应对了这一挑战。与将用户印象视为训练数据单位的传统做法相比，我们的新设计在数据日志中实现了原生特征去重，因此节省了数据存储空间。其次，通过在一个请求中对多个印象进行计算和通信去重，这种新范式使得可扩展的神经网络架构能够更好地捕捉用户兴趣信号，如生成型推荐器（GRs）和其他仅请求友好的架构。', 'title_zh': '仅请求优化的推荐系统'}
{'arxiv_id': 'arXiv:2508.05637', 'title': 'Automated Visualization Makeovers with LLMs', 'authors': 'Siddharth Gangwar, David A. Selby, Sebastian J. Vollmer', 'link': 'https://arxiv.org/abs/2508.05637', 'abstract': 'Making a good graphic that accurately and efficiently conveys the desired message to the audience is both an art and a science, typically not taught in the data science curriculum. Visualisation makeovers are exercises where the community exchange feedback to improve charts and data visualizations. Can multi-modal large language models (LLMs) emulate this task? Given a plot in the form of an image file, or the code used to generate it, an LLM, primed with a list of visualization best practices, is employed to semi-automatically generate constructive criticism to produce a better plot. Our system is centred around prompt engineering of a pre-trained model, relying on a combination of userspecified guidelines and any latent knowledge of data visualization practices that might lie within an LLMs training corpus. Unlike other works, the focus is not on generating valid visualization scripts from raw data or prompts, but on educating the user how to improve their existing data visualizations according to an interpretation of best practices. A quantitative evaluation is performed to measure the sensitivity of the LLM agent to various plotting issues across different chart types. We make the tool available as a simple self-hosted applet with an accessible Web interface.', 'abstract_zh': '制作能够准确高效地传达所需信息的图表既是艺术也是科学，通常不在数据科学课程中教授。可视化改版是社区成员交流反馈以改进图表和数据分析可视化的工作。多模态大型语言模型（LLMs）能否模拟这一任务？给定一个图像文件形式的图表或生成它的代码，结合可视化最佳实践列表对大型语言模型进行预训练，该模型可以半自动地生成建设性批评以生成更好的图表。我们的系统围绕预训练模型的提示工程构建，依赖于用户指定的指导方针和大型语言模型训练语料库中可能存在的任何隐含的数据可视化实践知识。与其它研究不同，重点不是从原始数据或提示生成有效的可视化脚本，而是教育用户如何根据最佳实践的解释改进他们现有的数据可视化。我们进行了定量评估以衡量LLM代理对不同类型图表的各种绘图问题的敏感性。我们以一个简单的自助式小部件形式提供该工具，具有易于访问的网络界面。', 'title_zh': '使用大语言模型自动优化可视化设计'}
{'arxiv_id': 'arXiv:2508.04748', 'title': 'AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models', 'authors': 'Xuan Lin, Long Chen, Yile Wang', 'link': 'https://arxiv.org/abs/2508.04748', 'abstract': "Large Language Models (LLMs) have shown promise in assisting molecular property prediction tasks but often rely on human-crafted prompts and chain-of-thought templates. While recent advanced large reasoning models like DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process, their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol, an attribute-guided reinforcement learning framework for molecular property prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1) a format reward encouraging attribute-based structured output, (2) a count reward to avoid enumerating irrelevant attributes, and (3) a rationality reward using advanced LLMs and RDKit to verify the relatedness of the generated attributes. This approach implicitly elicits the model's inherent knowledge of relevant molecular attributes during reasoning, enables making predictions for the molecular property more effectively. Experiments on both in-distribution and out-of-distribution datasets show that, training both 7B-size R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our proposed AttriLens-Mol method significantly boosts the performance, getting comparable or better results than supervised fine-tuning models (Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o, DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the target property, when used as features for an interpretable decision tree model, yield superior performance compared to attributes generated by prompting LLMs. This shows that AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. We release the code in this https URL.", 'abstract_zh': '基于属性引导的强化学习框架AttribuLens-Mol用于分子性质预测', 'title_zh': 'AttriLens-Mol：基于属性的强化学习在大规模语言模型中预测分子性质'}
{'arxiv_id': 'arXiv:2507.12286', 'title': 'SHACL Validation in the Presence of Ontologies: Semantics and Rewriting Techniques', 'authors': 'Anouk Oudshoorn, Magdalena Ortiz, Mantas Simkus', 'link': 'https://arxiv.org/abs/2507.12286', 'abstract': 'SHACL and OWL are two prominent W3C standards for managing RDF data. These languages share many features, but they have one fundamental difference: OWL, designed for inferring facts from incomplete data, makes the open-world assumption, whereas SHACL is a constraint language that treats the data as complete and must be validated under the closed-world assumption. The combination of both formalisms is very appealing and has been called for, but their semantic gap is a major challenge, semantically and computationally. In this paper, we advocate a semantics for SHACL validation in the presence of ontologies based on core universal models. We provide a technique for constructing these models for ontologies in the rich data-tractable description logic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to develop a rewriting technique that reduces SHACL validation in the presence of ontologies to standard validation. Finally, we study the complexity of SHACL validation in the presence of ontologies, and show that even very simple ontologies make the problem EXPTIME-complete, and PTIME-complete in data complexity.', 'abstract_zh': 'SHACL和OWL是两个 promin 华盛标准，用于管理RDF数据。这些语言共享许多特征，但它们有一个基本区别：OWL旨在从不完整数据中推断事实，基于开放世界假设，而SHACL是一种约束语言，认为数据是完整的，并且必须在封闭世界假设下进行验证。这两种形式主义的结合非常诱人，但它们的语义差距是一个主要挑战，从语义和计算角度来看都是。在本文中，我们提出了一种基于核心通用模型的SHACL验证语义，在 ontology 存在的情况下。我们提供了一种技术来为丰富的数据可处理描述逻辑Horn-ALCHIQ中的ontology构造这些模型。此外，我们使用该模型的有限表示来开发一种重写技术，将ontology存在下的SHACL验证减少为标准验证。最后，我们研究了ontology存在下的SHACL验证的复杂性，并展示了即使是非常简单的ontology也会使问题变为EXPTIME完全问题，在数据复杂性方面则为PTIME完全问题。', 'title_zh': '面向本体的SHACL验证：语义与重写技术'}
{'arxiv_id': 'arXiv:2304.04475', 'title': 'Epidemic Control on a Large-Scale-Agent-Based Epidemiology Model using Deep Deterministic Policy Gradient', 'authors': 'Gaurav Deshkar, Jayanta Kshirsagar, Harshal Hayatnagarkar, Janani Venugopalan', 'link': 'https://arxiv.org/abs/2304.04475', 'abstract': 'To mitigate the impact of the pandemic, several measures include lockdowns, rapid vaccination programs, school closures, and economic stimulus. These interventions can have positive or unintended negative consequences. Current research to model and determine an optimal intervention automatically through round-tripping is limited by the simulation objectives, scale (a few thousand individuals), model types that are not suited for intervention studies, and the number of intervention strategies they can explore (discrete vs continuous). We address these challenges using a Deep Deterministic Policy Gradient (DDPG) based policy optimization framework on a large-scale (100,000 individual) epidemiological agent-based simulation where we perform multi-objective optimization. We determine the optimal policy for lockdown and vaccination in a minimalist age-stratified multi-vaccine scenario with a basic simulation for economic activity. With no lockdown and vaccination (mid-age and elderly), results show optimal economy (individuals below the poverty line) with balanced health objectives (infection, and hospitalization). An in-depth simulation is needed to further validate our results and open-source our framework.', 'abstract_zh': '为了减轻疫情的影响，采取了封锁、快速疫苗接种计划、学校关闭和经济刺激等多种措施。这些干预措施可能会带来积极或未预见的负面后果。目前通过往返模拟来建模并自动生成最优干预措施的研究受到模拟目标、规模（几千人）、不适用于干预研究的模型类型以及可探索的干预策略数量（离散 vs 连续）的限制。我们通过在大规模（10万人）流行病学基于代理的模拟中利用基于Deep Deterministic Policy Gradient (DDPG) 的策略优化框架来应对这些挑战，并进行多目标优化。我们在一个简化了年龄分层多疫苗的模拟中确定了最优封锁和接种疫苗政策，并进行了基本的经济活动模拟。在没有封锁和接种疫苗（中老年）的情况下，结果显示最优经济活动（低于贫困线的个体）和平衡的健康目标（感染和住院）。需要进行深入的模拟进一步验证我们的结果并开源我们的框架。', 'title_zh': '大规模基于代理的流行病学模型中基于深度确定性策略梯度的流行病控制'}
