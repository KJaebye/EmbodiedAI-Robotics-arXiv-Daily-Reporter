{'arxiv_id': 'arXiv:2503.16711', 'title': 'Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents', 'authors': 'Mihaela-Larisa Clement, Mónika Farsang, Felix Resch, Radu Grosu', 'link': 'https://arxiv.org/abs/2503.16711', 'abstract': "Autonomous agents that rely purely on perception to make real-time control decisions require efficient and robust architectures. In this work, we demonstrate that augmenting RGB input with depth information significantly enhances our agents' ability to predict steering commands compared to using RGB alone. We benchmark lightweight recurrent controllers that leverage the fused RGB-D features for sequential decision-making. To train our models, we collect high-quality data using a small-scale autonomous car controlled by an expert driver via a physical steering wheel, capturing varying levels of steering difficulty. Our models, trained under diverse configurations, were successfully deployed on real hardware. Specifically, our findings reveal that the early fusion of depth data results in a highly robust controller, which remains effective even with frame drops and increased noise levels, without compromising the network's focus on the task.", 'abstract_zh': '依赖纯感知进行实时控制决策的自主代理需要高效且 robust 的架构。本研究展示了将 RGB 输入与深度信息相结合显著增强了我们的代理预测转向命令的能力，相比仅使用 RGB。我们基于融合的 RGB-D 特征benchmark了轻量级循环控制器，用于序列决策制定。为训练我们的模型，我们使用由专家驾驶员通过物理方向盘控制的小型自主汽车收集高质量数据，捕捉不同的转向难度级别。在多种配置下训练的模型成功部署在实际硬件上。具体而言，我们的研究发现早期融合深度数据生成的控制器具有高度 robust 性，在帧丢失和噪声增加的情况下依然有效，而不影响网络对任务的关注。', 'title_zh': '深度感知至关重要：多模态RGB-D感知在鲁棒自主代理中的应用'}
{'arxiv_id': 'arXiv:2503.16538', 'title': 'Leveraging Vision-Language Models for Open-Vocabulary Instance Segmentation and Tracking', 'authors': 'Bastian Pätzold, Jan Nogga, Sven Behnke', 'link': 'https://arxiv.org/abs/2503.16538', 'abstract': 'This paper introduces a novel approach that leverages the capabilities of vision-language models (VLMs) by integrating them with established approaches for open-vocabulary detection (OVD), instance segmentation, and tracking. We utilize VLM-generated structured descriptions to identify visible object instances, collect application-relevant attributes, and inform an open-vocabulary detector to extract corresponding bounding boxes that are passed to a video segmentation model providing precise segmentation masks and tracking capabilities. Once initialized, this model can then directly extract segmentation masks, allowing processing of image streams in real time with minimal computational overhead. Tracks can be updated online as needed by generating new structured descriptions and corresponding open-vocabulary detections. This combines the descriptive power of VLMs with the grounding capability of OVD and the pixel-level understanding and speed of video segmentation. Our evaluation across datasets and robotics platforms demonstrates the broad applicability of this approach, showcasing its ability to extract task-specific attributes from non-standard objects in dynamic environments.', 'abstract_zh': '本文介绍了一种新颖的方法，通过将视觉语言模型（VLMs）与现有的开放词汇检测（OVD）、实例分割和追踪方法相结合，利用VLM的能力进行可见物体实例的识别、收集应用相关的属性，并指导开放词汇检测器提取相应的边界框，这些边界框随后传递给提供精确分割掩码和追踪能力的视频分割模型。该模型初始化后，可以直接提取分割掩码，实现实时处理图像流并减少计算开销。在线生成新的结构化描述和相应的开放词汇检测结果，可以更新跟踪信息。这种方法结合了VLM的描述能力、OVD的语义 grounding 能力以及视频分割模型的像素级理解和速度。我们在不同数据集和机器人平台上对该方法进行了评估，证明了其在动态环境中的广泛应用能力，特别是在从非标准对象中提取任务特定属性方面的优势。', 'title_zh': '利用视觉语言模型进行开放词汇实例分割和跟踪'}
{'arxiv_id': 'arXiv:2503.16512', 'title': 'Multimodal Sensing and Machine Learning to Compare Printed and Verbal Assembly Instructions Delivered by a Social Robot', 'authors': 'Ruchik Mishra, Laksita Prasanna, Adair Adair, Dan O Popa', 'link': 'https://arxiv.org/abs/2503.16512', 'abstract': 'In this paper, we compare a manual assembly task communicated to workers using both printed and robot-delivered instructions. The comparison was made using physiological signals (blood volume pulse (BVP) and electrodermal activity (EDA)) collected from individuals during an experimental study. In addition, we also collected responses of individuals using the NASA Task Load Index (TLX) survey. Furthermore, we mapped the collected physiological signals to the responses of participants for NASA TLX to predict their workload. For both the classification problems, we compare the performance of Convolutional Neural Networks (CNNs) and Long-Short-Term Memory (LSTM) models. Results show that for our CNN-based approach using multimodal data (both BVP and EDA) gave better results than using just BVP (approx. 8.38% more) and EDA (approx 20.49% more). Our LSTM-based model too had better results when we used multimodal data (approx 8.38% more than just BVP and 6.70% more than just EDA). Overall, CNNs performed better than LSTMs for classifying physiologies for paper vs robot-based instruction by 7.72%. The CNN-based model was able to give better classification results (approximately 17.83% more on an average across all responses of the NASA TLX) within a few minutes of training compared to the LSTM-based models.', 'abstract_zh': '基于印刷与机器人传达指令的人工装配任务生理信号对比研究', 'title_zh': '多模态传感与机器学习比较社交机器人提供的印刷版和口头装配指令'}
{'arxiv_id': 'arXiv:2503.16467', 'title': 'Enhancing Explainability with Multimodal Context Representations for Smarter Robots', 'authors': 'Anargh Viswanath, Lokesh Veeramacheneni, Hendrik Buschmeier', 'link': 'https://arxiv.org/abs/2503.16467', 'abstract': "Artificial Intelligence (AI) has significantly advanced in recent years, driving innovation across various fields, especially in robotics. Even though robots can perform complex tasks with increasing autonomy, challenges remain in ensuring explainability and user-centered design for effective interaction. A key issue in Human-Robot Interaction (HRI) is enabling robots to effectively perceive and reason over multimodal inputs, such as audio and vision, to foster trust and seamless collaboration. In this paper, we propose a generalized and explainable multimodal framework for context representation, designed to improve the fusion of speech and vision modalities. We introduce a use case on assessing 'Relevance' between verbal utterances from the user and visual scene perception of the robot. We present our methodology with a Multimodal Joint Representation module and a Temporal Alignment module, which can allow robots to evaluate relevance by temporally aligning multimodal inputs. Finally, we discuss how the proposed framework for context representation can help with various aspects of explainability in HRI.", 'abstract_zh': '人工智能（AI）在近年来取得了显著进步，驱动着各个领域创新，特别是在机器人技术领域。尽管机器人能够执行日益复杂的任务并实现越来越多的自主性，但在确保可解释性和以用户为中心的设计方面仍然存在挑战，以实现有效的交互。在人机交互（HRI）中，一个关键问题是如何让机器人有效感知和推理多模态输入（如音频和视觉），以促进信任和无缝协作。本文提出了一种泛化且可解释的多模态框架，用于上下文表示，旨在提高语音和视觉模态的融合。我们介绍了一个评估“相关性”的用例，该用例涉及用户口头表述与机器人视觉场景感知之间的相关性。我们提出了多模态联合表示模块和时间对齐模块的方法，以允许机器人通过时间对齐多模态输入来评估相关性。最后，我们讨论了所提出的上下文表示框架如何在HRI中帮助提高解释性。', 'title_zh': '基于多模态上下文表示以提高可解释性让机器人更智能'}
{'arxiv_id': 'arXiv:2503.17116', 'title': 'The CASTLE 2024 Dataset: Advancing the Art of Multimodal Understanding', 'authors': 'Luca Rossetto, Werner Bailer, Duc-Tien Dang-Nguyen, Graham Healy, Björn Þór Jónsson, Onanong Kongmeesub, Hoang-Bao Le, Stevan Rudinac, Klaus Schöffmann, Florian Spiess, Allie Tran, Minh-Triet Tran, Quang-Linh Tran, Cathal Gurrin', 'link': 'https://arxiv.org/abs/2503.17116', 'abstract': 'Egocentric video has seen increased interest in recent years, as it is used in a range of areas. However, most existing datasets are limited to a single perspective. In this paper, we present the CASTLE 2024 dataset, a multimodal collection containing ego- and exo-centric (i.e., first- and third-person perspective) video and audio from 15 time-aligned sources, as well as other sensor streams and auxiliary data. The dataset was recorded by volunteer participants over four days in a fixed location and includes the point of view of 10 participants, with an additional 5 fixed cameras providing an exocentric perspective. The entire dataset contains over 600 hours of UHD video recorded at 50 frames per second. In contrast to other datasets, CASTLE 2024 does not contain any partial censoring, such as blurred faces or distorted audio. The dataset is available via this https URL.', 'abstract_zh': '自视角视频近年来引起了广泛关注，因为它在多个领域中被应用。然而，现有大多数数据集仅限于单一视角。本文介绍了CASTLE 2024数据集，该数据集包含15个时间对齐的视觉和音频来源的多模态集合，以及其他传感器流和辅助数据，来自固定位置的志愿者参与者录制了为期四天的数据，其中包括10名参与者的第一人称和第三人称视角，另有5台固定相机提供第三人称视角。整个数据集包含超过600小时的50帧/秒录制的超高清视频。与现有数据集不同，CASTLE 2024没有包含任何部分遮挡，如模糊的脸部或失真的音频。该数据集可通过此链接访问。', 'title_zh': 'CASTLE 2024 数据集：推动多模态理解的艺术'}
{'arxiv_id': 'arXiv:2503.16683', 'title': 'GAIR: Improving Multimodal Geo-Foundation Model with Geo-Aligned Implicit Representations', 'authors': 'Zeping Liu, Fan Zhang, Junfeng Jiao, Ni Lao, Gengchen Mai', 'link': 'https://arxiv.org/abs/2503.16683', 'abstract': "Advancements in vision and language foundation models have inspired the development of geo-foundation models (GeoFMs), enhancing performance across diverse geospatial tasks. However, many existing GeoFMs primarily focus on overhead remote sensing (RS) data while neglecting other data modalities such as ground-level imagery. A key challenge in multimodal GeoFM development is to explicitly model geospatial relationships across modalities, which enables generalizability across tasks, spatial scales, and temporal contexts. To address these limitations, we propose GAIR, a novel multimodal GeoFM architecture integrating overhead RS data, street view (SV) imagery, and their geolocation metadata. We utilize three factorized neural encoders to project an SV image, its geolocation, and an RS image into the embedding space. The SV image needs to be located within the RS image's spatial footprint but does not need to be at its geographic center. In order to geographically align the SV image and RS image, we propose a novel implicit neural representations (INR) module that learns a continuous RS image representation and looks up the RS embedding at the SV image's geolocation. Next, these geographically aligned SV embedding, RS embedding, and location embedding are trained with contrastive learning objectives from unlabeled data. We evaluate GAIR across 10 geospatial tasks spanning RS image-based, SV image-based, and location embedding-based benchmarks. Experimental results demonstrate that GAIR outperforms state-of-the-art GeoFMs and other strong baselines, highlighting its effectiveness in learning generalizable and transferable geospatial representations.", 'abstract_zh': '视觉和语言基础模型的进步激发了地理基础模型（GeoFMs）的发展，提升了多样化的地理空间任务性能。然而，许多现有的GeoFMs主要集中在高空遥感（RS）数据上，而忽视了地面图像等其他数据模态。多模态GeoFM发展中的一项关键挑战是明确建模跨模态的地理空间关系，这使得任务、空间尺度和时间上下文的泛化能力得以增强。为解决这些限制，我们提出了GAIR，这是一种新颖的多模态GeoFM架构，结合了高空RS数据、街景（SV）图像及其地理定位元数据。我们利用三个因子化的神经编码器将SV图像、其地理定位和RS图像投影到嵌入空间。SV图像需要位于RS图像的地理区域内，但不一定在其地理中心。为了地理对齐SV图像和RS图像，我们提出了一种新颖的隐式神经表示（INR）模块，该模块学习连续的RS图像表示，并在SV图像的地理定位处查找RS嵌入。随后，这些地理对齐的SV嵌入、RS嵌入和位置嵌入是从未标记数据中使用对比学习目标进行训练的。我们在涵盖RS图像基于、SV图像基于和位置嵌入基于的10项地理空间任务基准上评估了GAIR。实验结果表明，GAIR在多项指标上优于最先进的GeoFMs和其他强大的基线，突显了其在学习泛化和可转移地理空间表示方面的有效性。', 'title_zh': 'GAIR: 基于地理对齐隐式表示提高多模态地理基础模型性能'}
{'arxiv_id': 'arXiv:2503.16454', 'title': 'An Audio-Visual Fusion Emotion Generation Model Based on Neuroanatomical Alignment', 'authors': 'Haidong Wang, Qia Shan, JianHua Zhang, PengFei Xiao, Ao Liu', 'link': 'https://arxiv.org/abs/2503.16454', 'abstract': 'In the field of affective computing, traditional methods for generating emotions predominantly rely on deep learning techniques and large-scale emotion datasets. However, deep learning techniques are often complex and difficult to interpret, and standardizing large-scale emotional datasets are difficult and costly to establish. To tackle these challenges, we introduce a novel framework named Audio-Visual Fusion for Brain-like Emotion Learning(AVF-BEL). In contrast to conventional brain-inspired emotion learning methods, this approach improves the audio-visual emotion fusion and generation model through the integration of modular components, thereby enabling more lightweight and interpretable emotion learning and generation processes. The framework simulates the integration of the visual, auditory, and emotional pathways of the brain, optimizes the fusion of emotional features across visual and auditory modalities, and improves upon the traditional Brain Emotional Learning (BEL) model. The experimental results indicate a significant improvement in the similarity of the audio-visual fusion emotion learning generation model compared to single-modality visual and auditory emotion learning and generation model. Ultimately, this aligns with the fundamental phenomenon of heightened emotion generation facilitated by the integrated impact of visual and auditory stimuli. This contribution not only enhances the interpretability and efficiency of affective intelligence but also provides new insights and pathways for advancing affective computing technology. Our source code can be accessed here: this https URL}{this https URL.', 'abstract_zh': '在情感计算领域，传统的用于生成情感的方法主要依赖于深度学习技术和大规模情感数据集。然而，深度学习技术往往复杂且难以解释，建立标准化的大规模情感数据集也极为困难和成本高昂。为解决这些挑战，我们提出了一种名为Audio-Visual Fusion for Brain-like Emotion Learning (AVF-BEL)的新颖框架。与传统的基于大脑的情感学习方法不同，该方法通过模块化组件的整合，增强了听觉和视觉情感融合与生成模型，从而使情感学习和生成过程更加轻量级和易于解释。该框架模拟了大脑的视觉、听觉和情绪通路的整合，优化了视觉和听觉模态之间的情感特征融合，并改进了传统的Brain Emotional Learning (BEL)模型。实验结果表明，与单一模态视觉和听觉情感学习和生成模型相比，听觉-视觉融合情感学习生成模型的相似性有了显著提高。最终，这与视觉和听觉刺激的综合影响引发的情感增强现象保持一致。这一贡献不仅提高了情感智能的可解释性和效率，还为推进情感计算技术提供了新的见解和路径。源代码可在此访问：this https URL this https URL。', 'title_zh': '基于神经解剖对齐的视听融合情感生成模型'}
{'arxiv_id': 'arXiv:2503.16434', 'title': 'Interactive Sketchpad: An Interactive Multimodal System for Collaborative, Visual Problem-Solving', 'authors': 'Steven-Shine Chen, Jimin Lee, Paul Pu Liang', 'link': 'https://arxiv.org/abs/2503.16434', 'abstract': 'Humans have long relied on visual aids like sketches and diagrams to support reasoning and problem-solving. Visual tools, like auxiliary lines in geometry or graphs in calculus, are essential for understanding complex ideas. However, many tutoring systems remain text-based, providing feedback only through natural language. Leveraging recent advances in Large Multimodal Models (LMMs), this paper introduces Interactive Sketchpad, a tutoring system that combines language-based explanations with interactive visualizations to enhance learning. Built on a pre-trained LMM, Interactive Sketchpad is fine-tuned to provide step-by-step guidance in both text and visuals, enabling natural multimodal interaction with the student. Accurate and robust diagrams are generated by incorporating code execution into the reasoning process. User studies conducted on math problems such as geometry, calculus, and trigonometry demonstrate that Interactive Sketchpad leads to improved task comprehension, problem-solving accuracy, and engagement levels, highlighting its potential for transforming educational technologies.', 'abstract_zh': '人类长期以来依靠简图和图表等视觉辅助工具来支持推理和问题解决。视觉工具，如几何学中的辅助线或微积分中的图表，对于理解复杂概念至关重要。然而，许多辅导系统仍然基于文本，仅通过自然语言提供反馈。利用大型多模态模型（LMM）的 recent 进展，本文介绍了互动简图板，这是一种结合基于语言的解释与互动可视化来增强学习的辅导系统。基于预训练的 LMM，互动简图板经过微调，能够同时在文本和可视化方面提供逐步指导，实现与学生的自然多模态交互。通过将代码执行纳入推理过程，生成了准确且稳健的图表。在几何、微积分和三角学等数学问题上的用户研究表明，互动简图板有助于提高任务理解力、问题解决的准确性以及参与度，凸显了其对改造教育技术的潜力。', 'title_zh': '交互式绘图板：一种用于协作性可视化问题解决的多模态交互系统'}
{'arxiv_id': 'arXiv:2503.16432', 'title': 'Multimodal Transformer Models for Turn-taking Prediction: Effects on Conversational Dynamics of Human-Agent Interaction during Cooperative Gameplay', 'authors': 'Young-Ho Bae, Casey C. Bennett', 'link': 'https://arxiv.org/abs/2503.16432', 'abstract': 'This study investigates multimodal turn-taking prediction within human-agent interactions (HAI), particularly focusing on cooperative gaming environments. It comprises both model development and subsequent user study, aiming to refine our understanding and improve conversational dynamics in spoken dialogue systems (SDSs). For the modeling phase, we introduce a novel transformer-based deep learning (DL) model that simultaneously integrates multiple modalities - text, vision, audio, and contextual in-game data to predict turn-taking events in real-time. Our model employs a Crossmodal Transformer architecture to effectively fuse information from these diverse modalities, enabling more comprehensive turn-taking predictions. The model demonstrates superior performance compared to baseline models, achieving 87.3% accuracy and 83.0% macro F1 score. A human user study was then conducted to empirically evaluate the turn-taking DL model in an interactive scenario with a virtual avatar while playing the game "Dont Starve Together", comparing a control condition without turn-taking prediction (n=20) to an experimental condition with our model deployed (n=40). Both conditions included a mix of English and Korean speakers, since turn-taking cues are known to vary by culture. We then analyzed the interaction quality, examining aspects such as utterance counts, interruption frequency, and participant perceptions of the avatar. Results from the user study suggest that our multimodal turn-taking model not only enhances the fluidity and naturalness of human-agent conversations, but also maintains a balanced conversational dynamic without significantly altering dialogue frequency. The study provides in-depth insights into the influence of turn-taking abilities on user perceptions and interaction quality, underscoring the potential for more contextually adaptive and responsive conversational agents.', 'abstract_zh': '本研究探讨了人类-代理交互（HAI）中的多模态轮替预测，重点关注合作游戏环境。该研究涵盖了模型开发和后续用户研究，旨在深化我们对对话式对话系统（SDSs）中对话动力学的理解并予以改进。在建模阶段，我们提出了一种新颖的基于变换器的深度学习（DL）模型，该模型能够同时整合文本、视觉、音频和游戏内上下文等多种模态信息，以实现实时轮替事件预测。该模型采用跨模态变换器架构，有效融合了这些多样模态的信息，从而实现更加全面的轮替预测。与基准模型相比，该模型展现出卓越的性能，准确率为87.3%，宏F1分为83.0%。随后，我们进行了人类使用者研究，在“Dont Starve Together”游戏中与虚拟角色互动，对比了包含轮替预测实验条件（n=40）和无轮替预测控制条件（n=20）的效果。研究对象包括英语和韩语使用者，因为轮替提示会因文化差异而异。我们分析了交互质量，包括话语数量、打断频率以及参与者对虚拟角色的感知等方面。用户研究结果表明，多模态轮替模型不仅提升了人类-代理对话的流畅性和自然性，还维持了平衡的对话动态，未显著改变对话频率。研究为轮替能力对用户体验和交互质量的影响提供了深入洞察，突显了更加上下文相关和响应式对话代理的潜力。', 'title_zh': '多模态 Transformer 模型在合作游戏过程中人类-代理交互对话动力学中的轮流预测效果'}
