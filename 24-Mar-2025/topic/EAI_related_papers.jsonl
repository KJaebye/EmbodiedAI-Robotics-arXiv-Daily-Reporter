{'arxiv_id': 'arXiv:2503.17309', 'title': 'LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language', 'authors': 'Kun Chu, Xufeng Zhao, Cornelius Weber, Stefan Wermter', 'link': 'https://arxiv.org/abs/2503.17309', 'abstract': 'Bimanual robotic manipulation provides significant versatility, but also presents an inherent challenge due to the complexity involved in the spatial and temporal coordination between two hands. Existing works predominantly focus on attaining human-level manipulation skills for robotic hands, yet little attention has been paid to task planning on long-horizon timescales. With their outstanding in-context learning and zero-shot generation abilities, Large Language Models (LLMs) have been applied and grounded in diverse robotic embodiments to facilitate task planning. However, LLMs still suffer from errors in long-horizon reasoning and from hallucinations in complex robotic tasks, lacking a guarantee of logical correctness when generating the plan. Previous works, such as LLM+P, extended LLMs with symbolic planners. However, none have been successfully applied to bimanual robots. New challenges inevitably arise in bimanual manipulation, necessitating not only effective task decomposition but also efficient task allocation. To address these challenges, this paper introduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning and multi-agent planning, automating effective and efficient bimanual task planning. We conduct simulated experiments on various long-horizon manipulation tasks of differing complexity. Our method is built using GPT-4o as the backend, and we compare its performance against plans generated directly by LLMs, including GPT-4o, V3 and also recent strong reasoning models o1 and R1. By analyzing metrics such as planning time, success rate, group debits, and planning-step reduction rate, we demonstrate the superior performance of LLM+MAP, while also providing insights into robotic reasoning. Code is available at this https URL.', 'abstract_zh': '双臂机器人操作提供了显著的灵活性，但同时也由于在空间和时间上协调两个手的复杂性而带来了内在的挑战。现有研究主要集中在使机器人的手达到人类级别的操作技能，但在长时尺度的任务规划方面关注较少。凭借卓越的上下文学习能力和零样本生成能力，大型语言模型（LLMs）已在各种机器人类体中得到应用，以促进任务规划。然而，LLMs 在长时尺度推理中仍存在错误，并且在复杂机器人任务中容易产生幻想，无法保证逻辑正确性。先前的研究，如LLM+P，将LLMs与符号规划结合，但尚未成功应用于双臂机器人。双臂操作带来了新的挑战，不仅需要有效的任务分解，还需要高效的任务分配。为应对这些挑战，本文引入了LLM+MAP，这是一种结合LLM推理与多代理规划的双臂规划框架，实现有效且高效的双臂任务规划。我们在不同复杂性的长时尺度操作任务中进行了模拟实验。我们使用GPT-4o作为后端构建了该方法，并将其性能与直接由LLMs生成的计划进行比较，包括GPT-4o、V3以及最近的强推理模型o1和R1。通过分析规划时间、成功率、群体折扣和规划步长减少率等指标，我们展示了LLM+MAP的优越性能，并为机器人推理提供了见解。代码可访问此处：this https URL。', 'title_zh': 'LLM+MAP: 使用大型语言模型和规划领域定义语言的双臂机器人任务规划'}
{'arxiv_id': 'arXiv:2503.17046', 'title': 'HAPI: A Model for Learning Robot Facial Expressions from Human Preferences', 'authors': "Dongsheng Yang, Qianying Liu, Wataru Sato, Takashi Minato, Chaoran Liu, Shin'ya Nishida", 'link': 'https://arxiv.org/abs/2503.17046', 'abstract': 'Automatic robotic facial expression generation is crucial for human-robot interaction, as handcrafted methods based on fixed joint configurations often yield rigid and unnatural behaviors. Although recent automated techniques reduce the need for manual tuning, they tend to fall short by not adequately bridging the gap between human preferences and model predictions-resulting in a deficiency of nuanced and realistic expressions due to limited degrees of freedom and insufficient perceptual integration. In this work, we propose a novel learning-to-rank framework that leverages human feedback to address this discrepancy and enhanced the expressiveness of robotic faces. Specifically, we conduct pairwise comparison annotations to collect human preference data and develop the Human Affective Pairwise Impressions (HAPI) model, a Siamese RankNet-based approach that refines expression evaluation. Results obtained via Bayesian Optimization and online expression survey on a 35-DOF android platform demonstrate that our approach produces significantly more realistic and socially resonant expressions of Anger, Happiness, and Surprise than those generated by baseline and expert-designed methods. This confirms that our framework effectively bridges the gap between human preferences and model predictions while robustly aligning robotic expression generation with human affective responses.', 'abstract_zh': '自动机器人面部表情生成对于人机交互至关重要，因为基于固定关节配置的手工制作方法往往会产生僵硬和不自然的行为。虽然最近的自动化技术减少了手动调整的需求，但它们往往由于未能充分弥合人类偏好与模型预测之间的差距而有所不足，导致表情细腻和真实性不足，这主要是因为自由度有限和感知整合不足。在本项工作中，我们提出了一种新颖的学习排名框架，利用人类反馈来解决这一差异，并增强了机器人类别表情的表现力。具体而言，我们进行了成对比较注释以收集人类偏好数据，并开发了基于双胞胎排名网络的Human Affective Pairwise Impressions (HAPI) 模型，该模型用于细化表情评估。通过贝叶斯优化和在线表情调查获得的结果表明，与基线方法和专家设计的方法相比，我们的方法生成了显著更加真实和具有社会共鸣效果的愤怒、快乐和惊讶表情。这证明了我们的框架有效弥合了人类偏好与模型预测之间的差距，并且能够稳健地将机器人类别表情生成与人类情感反应对齐。', 'title_zh': 'HAPI：一种从人类偏好学习机器人面部表情的模型'}
{'arxiv_id': 'arXiv:2503.17014', 'title': 'Behavioral Conflict Avoidance Between Humans and Quadruped Robots in Shared Environments', 'authors': 'Shuang Wei, Muhua Zhang, Yun Gan, Deqing Huang, Lei Ma, Chenguang Yang', 'link': 'https://arxiv.org/abs/2503.17014', 'abstract': "Nowadays, robots are increasingly operated in environments shared with humans, where conflicts between human and robot behaviors may compromise safety. This paper presents a proactive behavioral conflict avoidance framework based on the principle of adaptation to trends for quadruped robots that not only ensures the robot's safety but also minimizes interference with human activities. It can proactively avoid potential conflicts with approaching humans or other dynamic objects, whether the robot is stationary or in motion, then swiftly resume its tasks once the conflict subsides. An enhanced approach is proposed to achieve precise human detection and tracking on vibratory robot platform equipped with low-cost hybrid solid-state LiDAR. When potential conflict detected, the robot selects an avoidance point and executes an evasion maneuver before resuming its task. This approach contrasts with conventional methods that remain goal-driven, often resulting in aggressive behaviors, such as forcibly bypassing obstacles and causing conflicts or becoming stuck in deadlock scenarios. The selection of avoidance points is achieved by integrating static and dynamic obstacle to generate a potential field map. The robot then searches for feasible regions within this map and determines the optimal avoidance point using an evaluation function. Experimental results demonstrate that the framework significantly reduces interference with human activities, enhances the safety of both robots and persons.", 'abstract_zh': '基于趋勢適應原理的四足机器人主动行为冲突 avoidance框架：减少对人类活动的干扰并确保安全', 'title_zh': '共享环境中 humans 和四足机器人之间的行为冲突避免'}
{'arxiv_id': 'arXiv:2503.16806', 'title': 'DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation', 'authors': 'Jiangran Lyu, Ziming Li, Xuesong Shi, Chaoyi Xu, Yizhou Wang, He Wang', 'link': 'https://arxiv.org/abs/2503.16806', 'abstract': 'Nonprehensile manipulation is crucial for handling objects that are too thin, large, or otherwise ungraspable in unstructured environments. While conventional planning-based approaches struggle with complex contact modeling, learning-based methods have recently emerged as a promising alternative. However, existing learning-based approaches face two major limitations: they heavily rely on multi-view cameras and precise pose tracking, and they fail to generalize across varying physical conditions, such as changes in object mass and table friction. To address these challenges, we propose the Dynamics-Adaptive World Action Model (DyWA), a novel framework that enhances action learning by jointly predicting future states while adapting to dynamics variations based on historical trajectories. By unifying the modeling of geometry, state, physics, and robot actions, DyWA enables more robust policy learning under partial observability. Compared to baselines, our method improves the success rate by 31.5% using only single-view point cloud observations in the simulation. Furthermore, DyWA achieves an average success rate of 68% in real-world experiments, demonstrating its ability to generalize across diverse object geometries, adapt to varying table friction, and robustness in challenging scenarios such as half-filled water bottles and slippery surfaces.', 'abstract_zh': '非抓取 manipulation 对于处理细长、过大或在非结构化环境中无法抓住的对象至关重要。虽然基于规划的传统方法在复杂的接触建模上遇到困难，但基于学习的方法最近 emerged 作为一种有前途的替代方案。然而，现有的基于学习的方法面临两大主要限制：它们严重依赖多视图相机和精确的姿态跟踪，并且无法在不同的物理条件下进行泛化，例如物体质量的变化和桌面摩擦的变化。为了解决这些挑战，我们提出了动态自适应世界动作模型（DyWA），这是一种新的框架，通过联合预测未来状态并在根据历史轨迹适应动力学变化的基础上增强动作学习。通过统一建模几何、状态、物理和机器人动作，DyWA 在部分可观测条件下实现了更稳健的策略学习。与基线方法相比，仅使用单视角点云观测，我们的方法在模拟实验中将成功率提高了 31.5%。此外，在真实世界实验中，DyWA 达到了 68% 的平均成功率，展示了其在不同几何形状的物体、适应不同的桌面摩擦以及在挑战性场景（如半满的水瓶和滑动表面）中的泛化能力和鲁棒性。', 'title_zh': 'DyWA：动态自适应世界动作模型以实现泛化非拾取操作'}
{'arxiv_id': 'arXiv:2503.16803', 'title': 'BEAC: Imitating Complex Exploration and Task-oriented Behaviors for Invisible Object Nonprehensile Manipulation', 'authors': 'Hirotaka Tahara, Takamitsu Matsubara', 'link': 'https://arxiv.org/abs/2503.16803', 'abstract': "Applying imitation learning (IL) is challenging to nonprehensile manipulation tasks of invisible objects with partial observations, such as excavating buried rocks. The demonstrator must make such complex action decisions as exploring to find the object and task-oriented actions to complete the task while estimating its hidden state, perhaps causing inconsistent action demonstration and high cognitive load problems. For these problems, work in human cognitive science suggests that promoting the use of pre-designed, simple exploration rules for the demonstrator may alleviate the problems of action inconsistency and high cognitive load. Therefore, when performing imitation learning from demonstrations using such exploration rules, it is important to accurately imitate not only the demonstrator's task-oriented behavior but also his/her mode-switching behavior (exploratory or task-oriented behavior) under partial observation. Based on the above considerations, this paper proposes a novel imitation learning framework called Belief Exploration-Action Cloning (BEAC), which has a switching policy structure between a pre-designed exploration policy and a task-oriented action policy trained on the estimated belief states based on past history. In simulation and real robot experiments, we confirmed that our proposed method achieved the best task performance, higher mode and action prediction accuracies, while reducing the cognitive load in the demonstration indicated by a user study.", 'abstract_zh': '基于信念探索-动作克隆的非抓握隐形对象部分观测条件下模仿学习框架', 'title_zh': 'BEAC：模仿复杂探索和任务导向行为实现隐形物体非手持操纵'}
{'arxiv_id': 'arXiv:2503.16715', 'title': 'Ground and Flight Locomotion for Two-Wheeled Drones via Model Predictive Path Integral Control', 'authors': 'Gosuke Kojima, Kohei Honda, Satoshi Nakano, Manabu Yamada', 'link': 'https://arxiv.org/abs/2503.16715', 'abstract': 'This paper presents a novel approach to motion planning for two-wheeled drones that can drive on the ground and fly in the air. Conventional methods for two-wheeled drone motion planning typically rely on gradient-based optimization and assume that obstacle shapes can be approximated by a differentiable form. To overcome this limitation, we propose a motion planning method based on Model Predictive Path Integral (MPPI) control, enabling navigation through arbitrarily shaped obstacles by switching between driving and flight modes. To handle the instability and rapid solution changes caused by mode switching, our proposed method switches the control space and utilizes the auxiliary controller for MPPI. Our simulation results demonstrate that the proposed method enables navigation in unstructured environments and achieves effective obstacle avoidance through mode switching.', 'abstract_zh': '基于Model Predictive Path Integral控制的两轮无人机运动规划新方法', 'title_zh': '两轮无人机的地面和飞行运动控制：基于模型预测路径积分控制方法'}
{'arxiv_id': 'arXiv:2503.16634', 'title': 'A Schwarz-Christoffel Mapping-based Framework for Sim-to-Real Transfer in Autonomous Robot Operations', 'authors': 'Shijie Gao, Nicola Bezzo', 'link': 'https://arxiv.org/abs/2503.16634', 'abstract': 'Despite the remarkable acceleration of robotic development through advanced simulation technology, robotic applications are often subject to performance reductions in real-world deployment due to the inherent discrepancy between simulation and reality, often referred to as the "sim-to-real gap". This gap arises from factors like model inaccuracies, environmental variations, and unexpected disturbances. Similarly, model discrepancies caused by system degradation over time or minor changes in the system\'s configuration also hinder the effectiveness of the developed methodologies. Effectively closing these gaps is critical and remains an open challenge. This work proposes a lightweight conformal mapping framework to transfer control and planning policies from an expert teacher to a degraded less capable learner. The method leverages Schwarz-Christoffel Mapping (SCM) to geometrically map teacher control inputs into the learner\'s command space, ensuring maneuver consistency. To demonstrate its generality, the framework is applied to two representative types of control and planning methods in a path-tracking task: 1) a discretized motion primitives command transfer and 2) a continuous Model Predictive Control (MPC)-based command transfer. The proposed framework is validated through extensive simulations and real-world experiments, demonstrating its effectiveness in reducing the sim-to-real gap by closely transferring teacher commands to the learner robot.', 'abstract_zh': '尽管通过先进仿真技术取得了机器人开发的显著加速，但在实际部署中，由于仿真与现实之间固有的差异，即所谓的“仿真实验到现实世界差距”，机器人的性能往往会降低。这一差距源于模型不准确、环境变化和意外干扰等因素。同样，由于系统退化或系统配置的小幅变化导致的模型差异也阻碍了所开发方法的有效性。有效地缩小这些差距至关重要，但仍是一个开放性的挑战。本文提出了一种轻量级符合映射框架，将专家教师的控制和规划策略转移到退化的较不能力强的学习者。该方法利用Schwarz-Christoffel映射（SCM）将教师的控制输入几何映射到学习者的命令空间，以确保操纵一致。为证明其普适性，该框架被应用于路径跟踪任务中的两种代表性控制和规划方法：1）离散运动基元命令转移；2）连续模型预测控制（MPC）基于的命令转移。所提出框架通过广泛的仿真和实际实验得到验证，展示了其通过密切转移教师命令到学习者机器人以减少仿真实验到现实世界差距的有效性。', 'title_zh': '基于Schwarz-Christoffel变换的从仿真到现实转换框架在自主机器人操作中的应用'}
{'arxiv_id': 'arXiv:2503.16441', 'title': 'Safe and Efficient Social Navigation through Explainable Safety Regions Based on Topological Features', 'authors': 'Victor Toscano-Duran, Sara Narteni, Alberto Carlevaro, Rocio Gonzalez-Diaz, Maurizio Mongelli, Jerome Guzzi', 'link': 'https://arxiv.org/abs/2503.16441', 'abstract': 'The recent adoption of artificial intelligence (AI) in robotics has driven the development of algorithms that enable autonomous systems to adapt to complex social environments. In particular, safe and efficient social navigation is a key challenge, requiring AI not only to avoid collisions and deadlocks but also to interact intuitively and predictably with its surroundings. To date, methods based on probabilistic models and the generation of conformal safety regions have shown promising results in defining safety regions with a controlled margin of error, primarily relying on classification approaches and explicit rules to describe collision-free navigation conditions.\nThis work explores how topological features contribute to explainable safety regions in social navigation. Instead of using behavioral parameters, we leverage topological data analysis to classify and characterize different simulation behaviors. First, we apply global rule-based classification to distinguish between safe (collision-free) and unsafe scenarios based on topological properties. Then, we define safety regions, $S_\\varepsilon$, in the topological feature space, ensuring a maximum classification error of $\\varepsilon$. These regions are built with adjustable SVM classifiers and order statistics, providing robust decision boundaries. Local rules extracted from these regions enhance interpretability, keeping the decision-making process transparent.\nOur approach initially separates simulations with and without collisions, outperforming methods that not incorporate topological features. It offers a deeper understanding of robot interactions within a navigable space. We further refine safety regions to ensure deadlock-free simulations and integrate both aspects to define a compliant simulation space that guarantees safe and efficient navigation.', 'abstract_zh': '近期人工智能在 robotics 中的应用推动了能够适应复杂社交环境的自主系统算法的发展。特别是在确保安全和高效的社交导航方面，这是一大关键挑战，不仅要求 AI 避免碰撞和死锁，还需要与周围环境进行直观和可预测的交互。到目前为止，基于概率模型和生成符合安全区域的方法已经在控制误差范围内定义安全区域方面显示出有前景的结果，主要依赖分类方法和明确规则来描述无碰撞导航条件。\n\n本工作探讨拓扑特征如何贡献于可解释的安全区域在社交导航中的作用。我们不使用行为参数，而是利用拓扑数据分析来对不同的仿真行为进行分类和特征化。首先，我们应用全局规则分类，根据拓扑性质区分安全（无碰撞）和不安全场景。然后，在拓扑特征空间中定义安全区域 $S_\\varepsilon$，确保分类误差的最大值为 $\\varepsilon$。这些区域由可调 SVM 分类器和顺序统计量构建，提供稳健的决策边界。从这些区域中提取的局部规则增强了可解释性，保持决策过程透明。\n\n我们的方法最初将无碰撞和有碰撞的仿真进行了区分，优于未纳入拓扑特征的方法，提供了对机器人在可导航空间中交互的更深层次理解。我们进一步细化安全区域以确保无死锁仿真，并将两者相结合定义一个符合安全和高效导航的仿真空间。', 'title_zh': '基于拓扑特征的可解释安全区域的社会导航安全与高效性'}
{'arxiv_id': 'arXiv:2503.16960', 'title': 'Somatic Safety: An Embodied Approach Towards Safe Human-Robot Interaction', 'authors': 'Steve Benford, Eike Schneiders, Juan Pablo Martinez Avila, Praminda Caleb-Solly, Patrick Robert Brundell, Simon Castle-Green, Feng Zhou, Rachael Garrett, Kristina Höök, Sarah Whatley, Kate Marsh, Paul Tennent', 'link': 'https://arxiv.org/abs/2503.16960', 'abstract': 'As robots enter the messy human world so the vital matter of safety takes on a fresh complexion with physical contact becoming inevitable and even desirable. We report on an artistic-exploration of how dancers, working as part of a multidisciplinary team, engaged in contact improvisation exercises to explore the opportunities and challenges of dancing with cobots. We reveal how they employed their honed bodily senses and physical skills to engage with the robots aesthetically and yet safely, interleaving improvised physical manipulations with reflections to grow their knowledge of how the robots behaved and felt. We introduce somatic safety, a holistic mind-body approach in which safety is learned, felt and enacted through bodily contact with robots in addition to being reasoned about. We conclude that robots need to be better designed for people to hold them and might recognise tacit safety cues among this http URL propose that safety should be learned through iterative bodily experience interleaved with reflection.', 'abstract_zh': '随着机器人进入混乱的人类世界，安全保障的重要性得到了新的体现，物理接触变得不可避免甚至 desirable。我们报道了舞者作为多学科团队的一部分，通过接触即兴练习探索与协作机器人共舞的机会与挑战。我们揭示了他们如何运用敏锐的身体感知和物理技能，以一种既美观又安全的方式与机器人互动，将即兴的身体操作与反思交织在一起，以增进对机器人行为和感受的认识。我们引入了本体安全这一身-心整体的方法，其中安全不仅通过逻辑推理来认知，还需要通过与机器人的身体接触来学习、感受和实践。我们得出结论，机器人需要更好地设计，以便人们能够握住它们，并且可能在其中识别出隐含的安全信号。我们认为，安全性应该通过迭代的身体体验与反思交织来进行学习。', 'title_zh': '体化安全：一种面向安全人机交互的体化方法'}
{'arxiv_id': 'arXiv:2503.16579', 'title': 'World Knowledge from AI Image Generation for Robot Control', 'authors': 'Jonas Krumme, Christoph Zetzsche', 'link': 'https://arxiv.org/abs/2503.16579', 'abstract': "When interacting with the world robots face a number of difficult questions, having to make decisions when given under-specified tasks where they need to make choices, often without clearly defined right and wrong answers. Humans, on the other hand, can often rely on their knowledge and experience to fill in the gaps. For example, the simple task of organizing newly bought produce into the fridge involves deciding where to put each thing individually, how to arrange them together meaningfully, e.g. putting related things together, all while there is no clear right and wrong way to accomplish this task. We could encode all this information on how to do such things explicitly into the robots' knowledge base, but this can quickly become overwhelming, considering the number of potential tasks and circumstances the robot could encounter. However, images of the real world often implicitly encode answers to such questions and can show which configurations of objects are meaningful or are usually used by humans. An image of a full fridge can give a lot of information about how things are usually arranged in relation to each other and the full fridge at large. Modern generative systems are capable of generating plausible images of the real world and can be conditioned on the environment in which the robot operates. Here we investigate the idea of using the implicit knowledge about the world of modern generative AI systems given by their ability to generate convincing images of the real world to solve under-specified tasks.", 'abstract_zh': '当机器人与世界互动时，它们面临许多困难的问题，需要在给定模糊任务的情况下做出决策，往往缺乏明确的正确或错误答案。相比之下，人类经常可以根据他们的知识和经验来填补这些空白。例如，将新购买的食品整理进冰箱这一简单任务涉及个体决定每件物品的位置以及如何有意义地排列它们，例如将相关物品放在一起，然而并没有明确的正确或错误方式来完成这项任务。我们可以将如何完成此类任务的所有信息显式地编码到机器人的知识库中，但这可能会因为机器人可能遇到的任务和情境数量而变得难以管理。但是，真实世界的图像往往隐含地包含了这些问题的答案，并展示了哪些对象配置是有意义的，或者通常被人类使用。一张满载的冰箱图像可以提供很多关于物品如何在彼此间以及与整个满载的冰箱之间通常是如何排列的信息。现代生成系统能够生成现实世界的合理图像，并且可以依据机器人操作的环境进行条件设置。在这里，我们探讨使用现代生成AI系统通过其生成现实世界逼真图像的能力来解决模糊任务的想法。', 'title_zh': 'AI图像生成的 WORLD 知识在机器人控制中的应用'}
{'arxiv_id': 'arXiv:2503.16548', 'title': 'SemanticScanpath: Combining Gaze and Speech for Situated Human-Robot Interaction Using LLMs', 'authors': 'Elisabeth Menendez, Michael Gienger, Santiago Martínez, Carlos Balaguer, Anna Belardinelli', 'link': 'https://arxiv.org/abs/2503.16548', 'abstract': "Large Language Models (LLMs) have substantially improved the conversational capabilities of social robots. Nevertheless, for an intuitive and fluent human-robot interaction, robots should be able to ground the conversation by relating ambiguous or underspecified spoken utterances to the current physical situation and to the intents expressed non verbally by the user, for example by using referential gaze. Here we propose a representation integrating speech and gaze to enable LLMs to obtain higher situated awareness and correctly resolve ambiguous requests. Our approach relies on a text-based semantic translation of the scanpath produced by the user along with the verbal requests and demonstrates LLM's capabilities to reason about gaze behavior, robustly ignoring spurious glances or irrelevant objects. We validate the system across multiple tasks and two scenarios, showing its generality and accuracy, and demonstrate its implementation on a robotic platform, closing the loop from request interpretation to execution.", 'abstract_zh': '大型语言模型（LLMs）大幅增强了社会机器人的对话能力。然而，为了实现直观自然的人机交互，机器人需要通过将模糊或描述不足的语音表达与当前物理环境相关联，并通过参考凝视等非言语方式表达的意图，来实现场景相关的对话理解。在此，我们提出了一种结合语音和凝视的表示方法，以使LLM获得更高的场境意识并正确解析模糊请求。我们的方法依赖于对用户在产生扫描路径的同时提出的口头请求进行基于文本的语义转换，并展示了LLM推理凝视行为的能力，能够稳健地忽略无效凝视或无关对象。我们在多个任务和两种场景下验证了该系统，展示了其通用性和准确性，并通过机器人平台实现了从请求解析到执行的闭环过程。', 'title_zh': '语义扫描路径：结合凝视和语音的基于LLM的 Situated 人机互动'}
{'arxiv_id': 'arXiv:2503.16524', 'title': 'Second-order Theory of Mind for Human Teachers and Robot Learners', 'authors': 'Patrick Callaghan, Reid Simmons, Henny Admoni', 'link': 'https://arxiv.org/abs/2503.16524', 'abstract': "Confusing or otherwise unhelpful learner feedback creates or perpetuates erroneous beliefs that the teacher and learner have of each other, thereby increasing the cognitive burden placed upon the human teacher. For example, the robot's feedback might cause the human to misunderstand what the learner knows about the learning objective or how the learner learns. At the same time -- and in addition to the learning objective -- the learner might misunderstand how the teacher perceives the learner's task knowledge and learning processes. To ease the teaching burden, the learner should provide feedback that accounts for these misunderstandings and elicits efficient teaching from the human. This work endows an AI learner with a Second-order Theory of Mind that models perceived rationality as a source for the erroneous beliefs a teacher and learner may have of one another. It also explores how a learner can ease the teaching burden and improve teacher efficacy if it selects feedback which accounts for its model of the teacher's beliefs about the learner and its learning objective.", 'abstract_zh': '混淆或无用的学习者反馈会创造出或维持教师和学习者彼此之间的错误信念，从而增加对人类教师的认知负担。例如，机器人的反馈可能会导致人类误解学习者对学习目标的理解程度或学习方式。同时，除了学习目标外，学习者可能会误解教师对其任务知识和学习过程的看法。为了减轻教学负担，学习者应提供能够解释这些误解并促使人类进行高效教学的反馈。本文赋予人工智能学习者一种二阶心智理论，该理论将感知理性视为教师和学习者彼此之间可能持有的错误信念的来源。同时探讨了学习者如何通过选择能够反映其对教师关于学习者及其学习目标信念的反馈来减轻教学负担并提高教师的有效性。', 'title_zh': '二阶Theory of Mind对人体教师和机器人学习者的应用'}
{'arxiv_id': 'arXiv:2503.16500', 'title': 'The Impact of VR and 2D Interfaces on Human Feedback in Preference-Based Robot Learning', 'authors': 'Jorge de Heuvel, Daniel Marta, Simon Holk, Iolanda Leite, Maren Bennewitz', 'link': 'https://arxiv.org/abs/2503.16500', 'abstract': 'Aligning robot navigation with human preferences is essential for ensuring comfortable and predictable robot movement in shared spaces, facilitating seamless human-robot coexistence. While preference-based learning methods, such as reinforcement learning from human feedback (RLHF), enable this alignment, the choice of the preference collection interface may influence the process. Traditional 2D interfaces provide structured views but lack spatial depth, whereas immersive VR offers richer perception, potentially affecting preference articulation. This study systematically examines how the interface modality impacts human preference collection and navigation policy alignment. We introduce a novel dataset of 2,325 human preference queries collected through both VR and 2D interfaces, revealing significant differences in user experience, preference consistency, and policy outcomes. Our findings highlight the trade-offs between immersion, perception, and preference reliability, emphasizing the importance of interface selection in preference-based robot learning. The dataset will be publicly released to support future research.', 'abstract_zh': '基于接口模态的人类偏好收集及其对机器人导航策略对齐的影响：一项系统的VR与2D界面比较研究', 'title_zh': '基于偏好的机器人学习中VR和2D界面的人类反馈影响'}
{'arxiv_id': 'arXiv:2503.16499', 'title': 'Stakeholder Perspectives on Whether and How Social Robots Can Support Mediation and Advocacy for Higher Education Students with Disabilities', 'authors': 'Alva Markelius, Julie Bailey, Jenny L. Gibson, Hatice Gunes', 'link': 'https://arxiv.org/abs/2503.16499', 'abstract': 'This paper presents an iterative, participatory, empirical study that examines the potential of using artificial intelligence, such as social robots and large language models, to support mediation and advocacy for students with disabilities in higher education. Drawing on qualitative data from interviews and focus groups conducted with various stakeholders, including disabled students, disabled student representatives, and disability practitioners at the University of Cambridge, this study reports findings relating to understanding the problem space, ideating robotic support and participatory co-design of advocacy support robots. The findings highlight the potential of these technologies in providing signposting and acting as a sounding board or study companion, while also addressing limitations in empathic understanding, trust, equity, and accessibility. We discuss ethical considerations, including intersectional biases, the double empathy problem, and the implications of deploying social robots in contexts shaped by structural inequalities. Finally, we offer a set of recommendations and suggestions for future research, rethinking the notion of corrective technological interventions to tools that empower and amplify self-advocacy.', 'abstract_zh': '本研究呈献了一种迭代的、参与式的、实证的研究方法，探讨使用人工智能，如社会机器人和大型语言模型，来支持高等教育中残疾学生的调解和倡导的可能性。基于与多方利益相关者（包括剑桥大学的残疾学生、残疾学生代表和残疾人从业人员）进行的访谈和焦点小组的定性数据，本研究报告了关于理解问题领域、构思机器人支持以及参与式共同设计倡导支持机器人的发现。这些发现突显了这些技术在提供方向和充当提示板或学习伴侣方面的潜力，同时也指出了同理理解、信任、公平性和可访问性方面存在的局限性。我们讨论了伦理考虑，包括交叉偏见、同理心不足的问题以及在由结构性不平等塑造的背景下部署社会机器人的含义。最后，我们提出了对未来研究的建议和建议，重新思考矫正性技术干预措施的概念，使之成为增强自我倡导的工具。', 'title_zh': '利益相关者视角下的社会机器人在支持高等教育残疾人学生调解和倡导中的作用与方式'}
{'arxiv_id': 'arXiv:2503.16492', 'title': 'FAM-HRI: Foundation-Model Assisted Multi-Modal Human-Robot Interaction Combining Gaze and Speech', 'authors': 'Yuzhi Lai, Shenghai Yuan, Boya Zhang, Benjamin Kiefer, Peizheng Li, Andreas Zell', 'link': 'https://arxiv.org/abs/2503.16492', 'abstract': 'Effective Human-Robot Interaction (HRI) is crucial for enhancing accessibility and usability in real-world robotics applications. However, existing solutions often rely on gestures or language commands, making interaction inefficient and ambiguous, particularly for users with physical impairments. In this paper, we introduce FAM-HRI, an efficient multi-modal framework for human-robot interaction that integrates language and gaze inputs via foundation models. By leveraging lightweight Meta ARIA glasses, our system captures real-time multi-modal signals and utilizes large language models (LLMs) to fuse user intention with scene context, enabling intuitive and precise robot manipulation. Our method accurately determines gaze fixation time interval, reducing noise caused by the gaze dynamic nature. Experimental evaluations demonstrate that FAM-HRI achieves a high success rate in task execution while maintaining a low interaction time, providing a practical solution for individuals with limited physical mobility or motor impairments.', 'abstract_zh': '有效的多模态人机交互（FAM-HRI）对于增强实时机器人应用中的可达性和易用性至关重要。然而，现有的解决方案往往依赖于手势或语言命令，使交互不够高效和明确，特别是在物理受限的用户中。本文提出了一种基于基础模型整合语言和注视输入的高效多模态人机交互框架FAM-HRI。通过利用轻量级的Meta ARIA眼镜，我们的系统捕捉实时多模态信号，并利用大规模语言模型（LLMs）融合用户意图与场景上下文，实现直观精准的机器人操作。该方法准确确定注视固定时间间隔，减少由注视动态特性引起的噪声。实验评估表明，FAM-HRI在执行任务方面具有很高的成功率并维持较低的交互时间，为行动受限或有运动障碍的个体提供了一种实用的解决方案。', 'title_zh': 'FAM-HRI: 基于基础模型的多模态人机交互融合注视与语音'}
{'arxiv_id': 'arXiv:2503.16481', 'title': 'Pedestrians and Robots: A Novel Dataset for Learning Distinct Social Navigation Forces', 'authors': 'Subham Agrawal, Nico Ostermann-Myrau, Nils Dengler, Maren Bennewitz', 'link': 'https://arxiv.org/abs/2503.16481', 'abstract': "The increasing use of robots in human-centric public spaces such as shopping malls, sidewalks, and hospitals, requires understanding of how pedestrians respond to their presence. However, existing research lacks comprehensive datasets that capture the full range of pedestrian behaviors, e.g., including avoidance, neutrality, and attraction in the presence of robots. Such datasets can be used to effectively learn models capable of accurately predicting diverse responses of pedestrians to robot presence, which are crucial for advancing robot navigation strategies and optimizing pedestrian-aware motion planning. In this paper, we address these challenges by collecting a novel dataset of pedestrian motion in two outdoor locations under three distinct conditions, i.e., no robot presence, a stationary robot, and a moving robot. Thus, unlike existing datasets, ours explicitly encapsulates variations in pedestrian behavior across the different robot conditions. Using our dataset, we propose a novel Neural Social Robot Force Model (NSRFM), an extension of the traditional Social Force Model that integrates neural networks and robot-induced forces to better predict pedestrian behavior in the presence of robots. We validate the NSRFM by comparing its generated trajectories on different real-world datasets. Furthermore, we implemented it in simulation to enable the learning and benchmarking of robot navigation strategies based on their impact on pedestrian movement. Our results demonstrate the model's effectiveness in replicating real-world pedestrian reactions and its its utility in developing, evaluating, and benchmarking social robot navigation algorithms.", 'abstract_zh': '机器人在购物中心、人行道和医院等以人为中心的公共空间中的广泛应用要求我们理解行人对其存在响应的方式。但是，现有的研究缺乏能够捕捉行人行为全范围的综合数据集，例如包含避免、中立和吸引等行为。此类数据集可以用于有效地学习能够准确预测行人对机器人存在反应模型，这对于推进机器人导航策略和优化行人aware运动规划至关重要。本文通过在两个室外地点在三种不同的条件下收集行人运动的新数据集来应对这些挑战，即无机器人存在、静止机器人和移动机器人。因此，与现有的数据集不同，我们的数据集明确包含了不同机器人条件下的行人行为变化。利用该数据集，我们提出了一种新型的神经社会机器人力模型（NSRFM），它是传统社会力模型的扩展，结合了神经网络和机器人引起的力以更好地预测机器人的存在下行人的行为。通过将NSRFM在不同的真实世界数据集上生成的轨迹进行对比验证，我们进一步在模拟中实现该模型，以通过其对行人移动的影响来学习和基准测试机器人导航策略。我们的结果表明该模型在复制真实世界行人反应方面的效果以及其在开发、评估和基准测试社会机器人导航算法方面的实用性。', 'title_zh': '行人与机器人：一种学习独特社会导航力的新型数据集'}
{'arxiv_id': 'arXiv:2503.16479', 'title': 'Simulation-based Testing of Foreseeable Misuse by the Driver applicable for Highly Automated Driving', 'authors': 'Milin Patel, Rolf Jung, Yasin Cakir', 'link': 'https://arxiv.org/abs/2503.16479', 'abstract': 'With Highly Automated Driving (HAD), the driver can engage in non-driving-related tasks. In the event of a system failure, the driver is expected to reasonably regain control of the Automated Vehicle (AV). Incorrect system understanding may provoke misuse by the driver and can lead to vehicle-level hazards. ISO 21448, referred to as the standard for Safety of the Intended Functionality (SOTIF), defines misuse as usage of the system by the driver in a way not intended by the system manufacturer. Foreseeable Misuse (FM) implies anticipated system misuse based on the best knowledge about the system design and the driver behaviour. This is the underlying motivation to propose simulation-based testing of FM. The vital challenge is to perform a simulation-based testing for a SOTIF-related misuse scenario. Transverse Guidance Assist System (TGAS) is modelled for HAD. In the context of this publication, TGAS is referred to as the "system," and the driver is the human operator of the system. This publication focuses on implementing the Driver-Vehicle Interface (DVI) that permits the interactions between the driver and the system. The implementation and testing of a derived misuse scenario using the driving simulator ensure reasonable usage of the system by supporting the driver with unambiguous information on system functions and states so that the driver can conveniently perceive, comprehend, and act upon the information.', 'abstract_zh': '基于仿真测试的.Forward Safety of the Intended Functionality 中可预见滥用场景的实施与测试', 'title_zh': '基于仿真测试的可预见驾驶员滥用性分析适用于高度自动驾驶 systems'}
{'arxiv_id': 'arXiv:2503.16476', 'title': 'Injecting Conflict Situations in Autonomous Driving Simulation using CARLA', 'authors': 'Tsvetomila Mihaylova, Stefan Reitmann, Elin A. Topp, Ville Kyrki', 'link': 'https://arxiv.org/abs/2503.16476', 'abstract': 'Simulation of conflict situations for autonomous driving research is crucial for understanding and managing interactions between Automated Vehicles (AVs) and human drivers. This paper presents a set of exemplary conflict scenarios in CARLA that arise in shared autonomy settings, where both AVs and human drivers must navigate complex traffic environments. We explore various conflict situations, focusing on the impact of driver behavior and decision-making processes on overall traffic safety and efficiency. We build a simple extendable toolkit for situation awareness research, in which the implemented conflicts can be demonstrated.', 'abstract_zh': '自主驾驶研究中基于CARLA的冲突情况仿真对于理解和管理自动驾驶车辆（AVs）与人类驾驶员之间的交互至关重要。本文介绍了共享自主驾驶设置下在CARLA中出现的若干典型冲突场景，探讨了驾驶员行为和决策过程对整体交通安全性与效率的影响，并构建了一个简单的可扩展工具箱用于情景意识研究，展示了已实现的冲突情景。', 'title_zh': '在自主驾驶模拟中注入冲突场景的CARLA方法'}
{'arxiv_id': 'arXiv:2503.16475', 'title': 'LLM-Glasses: GenAI-driven Glasses with Haptic Feedback for Navigation of Visually Impaired People', 'authors': 'Issatay Tokmurziyev, Miguel Altamirano Cabrera, Muhammad Haris Khan, Yara Mahmoud, Luis Moreno, Dzmitry Tsetserukou', 'link': 'https://arxiv.org/abs/2503.16475', 'abstract': "We present LLM-Glasses, a wearable navigation system designed to assist visually impaired individuals by combining haptic feedback, YOLO-World object detection, and GPT-4o-driven reasoning. The system delivers real-time tactile guidance via temple-mounted actuators, enabling intuitive and independent navigation. Three user studies were conducted to evaluate its effectiveness: (1) a haptic pattern recognition study achieving an 81.3% average recognition rate across 13 distinct patterns, (2) a VICON-based navigation study in which participants successfully followed predefined paths in open spaces, and (3) an LLM-guided video evaluation demonstrating 91.8% accuracy in open scenarios, 84.6% with static obstacles, and 81.5% with dynamic obstacles. These results demonstrate the system's reliability in controlled environments, with ongoing work focusing on refining its responsiveness and adaptability to diverse real-world scenarios. LLM-Glasses showcases the potential of combining generative AI with haptic interfaces to empower visually impaired individuals with intuitive and effective mobility solutions.", 'abstract_zh': 'LLM-Glasses：一种结合触觉反馈、YOLO-World物体检测和GPT-4驱动推理的可穿戴导航系统', 'title_zh': 'LLM-眼镜：基于GenAI的配备触觉反馈的盲人导航眼镜'}
{'arxiv_id': 'arXiv:2503.16473', 'title': 'PERCY: Personal Emotional Robotic Conversational System', 'authors': 'Zhijin Meng, Mohammed Althubyani, Shengyuan Xie, Imran Razzak, Eduardo B. Sandoval, Mahdi Bamdad, Francisco Cruz', 'link': 'https://arxiv.org/abs/2503.16473', 'abstract': "Traditional rule-based conversational robots, constrained by predefined scripts and static response mappings, fundamentally lack adaptability for personalized, long-term human interaction. While Large Language Models (LLMs) like GPT-4 have revolutionized conversational AI through open-domain capabilities, current social robots implementing LLMs still lack emotional awareness and continuous personalization. This dual limitation hinders their ability to sustain engagement across multiple interaction sessions. We bridge this gap with PERCY (Personal Emotional Robotic Conversational sYstem), a system designed to enable open-domain, multi-turn dialogues by dynamically analyzing users' real-time facial expressions and vocabulary to tailor responses based on their emotional state. Built on a ROS-based multimodal framework, PERCY integrates a fine-tuned GPT-4 reasoning engine, combining textual sentiment analysis with visual emotional cues to accurately assess and respond to user emotions. We evaluated PERCY's performance through various dialogue quality metrics, showing strong coherence, relevance, and diversity. Human evaluations revealed PERCY's superior personalization and comparable naturalness to other models. This work highlights the potential for integrating advanced multimodal perception and personalization in social robot dialogue systems.", 'abstract_zh': '基于高级多模态感知和个性化设计的社交机器人对话系统潜力', 'title_zh': 'PERCY：个性化情感机器人对话系统'}
{'arxiv_id': 'arXiv:2503.16469', 'title': 'Enhancing Human-Robot Interaction in Healthcare: A Study on Nonverbal Communication Cues and Trust Dynamics with NAO Robot Caregivers', 'authors': 'S M Taslim Uddin Raju', 'link': 'https://arxiv.org/abs/2503.16469', 'abstract': "As the population of older adults increases, so will the need for both human and robot care providers. While traditional practices involve hiring human caregivers to serve meals and attend to basic needs, older adults often require continuous companionship and health monitoring. However, hiring human caregivers for this job costs a lot of money. However, using a robot like Nao could be cheaper and still helpful. This study explores the integration of humanoid robots, particularly Nao, in health monitoring and caregiving for older adults. Using a mixed-methods approach with a within-subject factorial design, we investigated the effectiveness of nonverbal communication modalities, including touch, gestures, and LED patterns, in enhancing human-robot interactions. Our results indicate that Nao's touch-based health monitoring was well-received by participants, with positive ratings across various dimensions. LED patterns were perceived as more effective and accurate compared to hand and head gestures. Moreover, longer interactions were associated with higher trust levels and perceived empathy, highlighting the importance of prolonged engagement in fostering trust in human-robot interactions. Despite limitations, our study contributes valuable insights into the potential of humanoid robots to improve health monitoring and caregiving for older adults.", 'abstract_zh': '随着老年人口的增加，对人类和机器人护理提供者的需要也将增加。虽然传统做法是雇佣人类护理人员提供餐饮服务和处理基本需求，但老年人往往还需要持续的陪伴和健康监测。然而，雇佣人类护理人员成本较高。然而，使用像Nao这样的机器人可能是更经济且仍然有效的选择。本研究探讨了将类人机器人，特别是Nao，集成到老年人健康监测和护理中的可能性。采用混合方法并采用被试内因子设计，我们调查了触摸、手势和LED模式等非言语交流模式对增强人机互动的有效性。研究结果表明，Nao基于触摸的健康监测得到了参与者的积极评价，各项指标均表现良好。与手部和头部手势相比，LED模式被认为更有效且更准确。此外，更长的互动时间与更高的信任感和感知到的同理心相关，强调了在培养人机互动中建立信任的重要性。尽管存在局限性，但本研究为类人机器人在改善老年人健康监测和护理方面的潜力提供了有价值的观点。', 'title_zh': '增强医疗领域的人机交互：关于NAO机器人护理助手的非言语沟通线索与信任动态研究'}
{'arxiv_id': 'arXiv:2503.16459', 'title': 'The Realization of Virtual Environments in the Lower Limb Exoskeletal Robot', 'authors': 'Minsu Chang, Doyoung Jeon', 'link': 'https://arxiv.org/abs/2503.16459', 'abstract': "This study proposes the realization of various virtual environments using a lower limb exoskeletal robot for futuristic gait rehabilitation. The proposed method allows the user to feel virtual gravity, buoyancy, and drag while actively walking. The virtual environments include four fluidic conditions: Water, Olive oil, Honey, and Peanut Butter, and four gravitational conditions consisting of the Earth's, Moon's, Mars', and Jupiter's gravity. The control method of the lower limb exoskeletal robot is as follows. First, torque feedback is applied to control the interaction force between the exoskeletal robot and its user. Second, the reference torque is computed in real time with the dynamic equations of the human body and the kinematic data. The eight environments were implemented via the EXOWheel, a wheelchair-integrated lower limb exoskeletal robot. While attaching electromyography sensors and wearing the EXOWheel, eight healthy subjects walked actively under the virtual conditions. Experimental results show that muscular force signals adequately change depending on gravitational, buoyant, and drag effects. Blind tests confirmed that subjects could reliably distinguish all eight virtual environments.", 'abstract_zh': '本研究提出了一种利用下肢外骨骼机器人实现各种虚拟环境以进行未来步态康复的方法。该方法允许用户在主动行走时感受到虚拟的重力、浮力和阻力。虚拟环境包括四种流体条件：水、橄榄油、蜂蜜和花生酱，以及四种重力条件：地球、月球、火星和木星的重力。下肢外骨骼机器人的控制方法如下：首先，应用扭矩反馈来控制外骨骼机器人与其用户的交互力；其次，根据人体动力学方程和运动数据实时计算参考扭矩。这八种环境通过集成轮椅的下肢单元外骨骼机器人（EXOWheel）实现。佩戴EMG传感器并使用EXOWheel，八名健康受试者在虚拟条件下进行了主动行走。实验结果表明，肌肉力信号根据重力、浮力和阻力效应进行了适当变化。盲测结果证实，受试者能够可靠地区分所有八种虚拟环境。', 'title_zh': '下肢外骨骼机器人中虚拟环境的实现'}
{'arxiv_id': 'arXiv:2503.16451', 'title': 'Think-Then-React: Towards Unconstrained Human Action-to-Reaction Generation', 'authors': 'Wenhui Tan, Boyuan Li, Chuhao Jin, Wenbing Huang, Xiting Wang, Ruihua Song', 'link': 'https://arxiv.org/abs/2503.16451', 'abstract': 'Modeling human-like action-to-reaction generation has significant real-world applications, like human-robot interaction and games. Despite recent advancements in single-person motion generation, it is still challenging to well handle action-to-reaction generation, due to the difficulty of directly predicting reaction from action sequence without prompts, and the absence of a unified representation that effectively encodes multi-person motion. To address these challenges, we introduce Think-Then-React (TTR), a large language-model-based framework designed to generate human-like reactions. First, with our fine-grained multimodal training strategy, TTR is capable to unify two processes during inference: a thinking process that explicitly infers action intentions and reasons corresponding reaction description, which serve as semantic prompts, and a reacting process that predicts reactions based on input action and the inferred semantic prompts. Second, to effectively represent multi-person motion in language models, we propose a unified motion tokenizer by decoupling egocentric pose and absolute space features, which effectively represents action and reaction motion with same encoding. Extensive experiments demonstrate that TTR outperforms existing baselines, achieving significant improvements in evaluation metrics, such as reducing FID from 3.988 to 1.942.', 'abstract_zh': '基于大型语言模型的Think-Then-React框架：生成人类反应的挑战与解决方案', 'title_zh': '思考后再反应：迈向无约束的人类动作到反应生成'}
{'arxiv_id': 'arXiv:2503.16449', 'title': 'Mitigating the Uncanny Valley Effect in Hyper-Realistic Robots: A Student-Centered Study on LLM-Driven Conversations', 'authors': 'Hangyeol Kang, Thiago Freitas dos Santos, Maher Ben Moussa, Nadia Magnenat-Thalmann', 'link': 'https://arxiv.org/abs/2503.16449', 'abstract': 'The uncanny valley effect poses a significant challenge in the development and acceptance of hyper-realistic social robots. This study investigates whether advanced conversational capabilities powered by large language models (LLMs) can mitigate this effect in highly anthropomorphic robots. We conducted a user study with 80 participants interacting with Nadine, a hyper-realistic humanoid robot equipped with LLM-driven communication skills. Through pre- and post-interaction surveys, we assessed changes in perceptions of uncanniness, conversational quality, and overall user experience. Our findings reveal that LLM-enhanced interactions significantly reduce feelings of eeriness while fostering more natural and engaging conversations. Additionally, we identify key factors influencing user acceptance, including conversational naturalness, human-likeness, and interestingness. Based on these insights, we propose design recommendations to enhance the appeal and acceptability of hyper-realistic robots in social contexts. This research contributes to the growing field of human-robot interaction by offering empirical evidence on the potential of LLMs to bridge the uncanny valley, with implications for the future development of social robots.', 'abstract_zh': '超现实社会机器人中拟人化效果的挑战：大型语言模型增强对话能力的缓解作用研究', 'title_zh': '缓解超real机器人中的毛骨悚然谷效应：基于学生的LLM驱动对话研究'}
{'arxiv_id': 'arXiv:2503.17338', 'title': 'Capturing Individual Human Preferences with Reward Features', 'authors': 'André Barreto, Vincent Dumoulin, Yiran Mao, Nicolas Perez-Nieves, Bobak Shahriari, Yann Dauphin, Doina Precup, Hugo Larochelle', 'link': 'https://arxiv.org/abs/2503.17338', 'abstract': 'Reinforcement learning from human feedback usually models preferences using a reward model that does not distinguish between people. We argue that this is unlikely to be a good design choice in contexts with high potential for disagreement, like in the training of large language models. We propose a method to specialise a reward model to a person or group of people. Our approach builds on the observation that individual preferences can be captured as a linear combination of a set of general reward features. We show how to learn such features and subsequently use them to quickly adapt the reward model to a specific individual, even if their preferences are not reflected in the training data. We present experiments with large language models comparing the proposed architecture with a non-adaptive reward model and also adaptive counterparts, including models that do in-context personalisation. Depending on how much disagreement there is in the training data, our model either significantly outperforms the baselines or matches their performance with a simpler architecture and more stable training.', 'abstract_zh': '从人类反馈中进行强化学习通常使用一个不区分个体的奖励模型。我们在大型语言模型训练等存在高争议风险的背景下，认为这种设计可能不是一个好的选择。我们提出了一种将奖励模型专用于特定个人或群体的方法。我们基于个体偏好可以表示为一组通用奖励特征线性组合的观察，展示了一种方法来学习这些特征，并利用它们快速适应特定个体的奖励模型，即使个体的偏好在训练数据中未得到反映。我们通过与非自适应奖励模型以及各种自适应模型（包括进行上下文个性化调整的模型）进行大型语言模型实验，展示了该架构的表现。当训练数据中的分歧较大时，我们的模型显著优于基准模型；当分歧较小时，通过更简单的架构和更稳定的训练，我们的模型能够与基准模型取得相似的性能。', 'title_zh': '基于奖励特征捕捉个体人类偏好'}
{'arxiv_id': 'arXiv:2503.16724', 'title': 'Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models', 'authors': 'Zhaoxin Li, Zhang Xi-Jia, Batuhan Altundas, Letian Chen, Rohan Paleja, Matthew Gombolay', 'link': 'https://arxiv.org/abs/2503.16724', 'abstract': "Semantic Interpretability in Reinforcement Learning (RL) enables transparency, accountability, and safer deployment by making the agent's decisions understandable and verifiable. Achieving this, however, requires a feature space composed of human-understandable concepts, which traditionally rely on human specification and fail to generalize to unseen environments. In this work, we introduce Semantically Interpretable Reinforcement Learning with Vision-Language Models Empowered Automation (SILVA), an automated framework that leverages pre-trained vision-language models (VLM) for semantic feature extraction and interpretable tree-based models for policy optimization. SILVA first queries a VLM to identify relevant semantic features for an unseen environment, then extracts these features from the environment. Finally, it trains an Interpretable Control Tree via RL, mapping the extracted features to actions in a transparent and interpretable manner. To address the computational inefficiency of extracting features directly with VLMs, we develop a feature extraction pipeline that generates a dataset for training a lightweight convolutional network, which is subsequently used during RL. By leveraging VLMs to automate tree-based RL, SILVA removes the reliance on human annotation previously required by interpretable models while also overcoming the inability of VLMs alone to generate valid robot policies, enabling semantically interpretable reinforcement learning without human-in-the-loop.", 'abstract_zh': '基于视觉-语言模型赋能自动化的方法的语义可解释强化学习（SILVA）', 'title_zh': '基于视觉-语言模型的强化学习自动语义可解释性研究'}
{'arxiv_id': 'arXiv:2503.16547', 'title': 'Empowering Medical Multi-Agents with Clinical Consultation Flow for Dynamic Diagnosis', 'authors': 'Sihan Wang, Suiyang Jiang, Yibo Gao, Boming Wang, Shangqi Gao, Xiahai Zhuang', 'link': 'https://arxiv.org/abs/2503.16547', 'abstract': 'Traditional AI-based healthcare systems often rely on single-modal data, limiting diagnostic accuracy due to incomplete information. However, recent advancements in foundation models show promising potential for enhancing diagnosis combining multi-modal information. While these models excel in static tasks, they struggle with dynamic diagnosis, failing to manage multi-turn interactions and often making premature diagnostic decisions due to insufficient persistence in information this http URL address this, we propose a multi-agent framework inspired by consultation flow and reinforcement learning (RL) to simulate the entire consultation process, integrating multiple clinical information for effective diagnosis. Our approach incorporates a hierarchical action set, structured from clinic consultation flow and medical textbook, to effectively guide the decision-making process. This strategy improves agent interactions, enabling them to adapt and optimize actions based on the dynamic state. We evaluated our framework on a public dynamic diagnosis benchmark. The proposed framework evidentially improves the baseline methods and achieves state-of-the-art performance compared to existing foundation model-based methods.', 'abstract_zh': '基于传统单模态数据的AI医疗系统限制了诊断准确性，而近年来的基础模型进展显示了通过整合多模态信息增强诊断的潜力。尽管这些模型在静态任务中表现优异，但在动态诊断中却难以应对多轮交互，常因信息不足而过早做出诊断决策。为解决这些问题，我们提出了一种受咨询流程启发并结合强化学习的多代理框架，以模拟完整的咨询过程，整合多种临床信息进行有效诊断。该方法采用层次化动作集，从临床咨询流程和医学教科书结构化而来，有效指导决策过程。此策略增强了代理间的互动，使它们能够根据动态状态进行适应和优化。我们在一个公开的动态诊断基准上评估了该框架。提出的框架显著改进了基线方法，并在与现有基于基础模型的方法相比时达到了最先进的性能。', 'title_zh': '基于临床咨询流程赋能医疗多智能体动态诊断'}
{'arxiv_id': 'arXiv:2503.17125', 'title': 'Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning', 'authors': 'Chan Kim, Seung-Woo Seo, Seong-Woo Kim', 'link': 'https://arxiv.org/abs/2503.17125', 'abstract': 'Deep Reinforcement Learning (DRL) has demonstrated strong performance in robotic control but remains susceptible to out-of-distribution (OOD) states, often resulting in unreliable actions and task failure. While previous methods have focused on minimizing or preventing OOD occurrences, they largely neglect recovery once an agent encounters such states. Although the latest research has attempted to address this by guiding agents back to in-distribution states, their reliance on uncertainty estimation hinders scalability in complex environments. To overcome this limitation, we introduce Language Models for Out-of-Distribution Recovery (LaMOuR), which enables recovery learning without relying on uncertainty estimation. LaMOuR generates dense reward codes that guide the agent back to a state where it can successfully perform its original task, leveraging the capabilities of LVLMs in image description, logical reasoning, and code generation. Experimental results show that LaMOuR substantially enhances recovery efficiency across diverse locomotion tasks and even generalizes effectively to complex environments, including humanoid locomotion and mobile manipulation, where existing methods struggle. The code and supplementary materials are available at \\href{this https URL}{this https URL}.', 'abstract_zh': '基于语言模型的异分布恢复（LaMOuR）：增强机器人控制中的异常状态恢复性能', 'title_zh': '利用语言模型进行强化学习中的分布外恢复'}
{'arxiv_id': 'arXiv:2503.17085', 'title': 'Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics', 'authors': 'J. M. Diederik Kruijssen, Nicholas Emmons', 'link': 'https://arxiv.org/abs/2503.17085', 'abstract': 'Artificial intelligence (AI) systems powered by large language models have become increasingly prevalent in modern society, enabling a wide range of applications through natural language interaction. As AI agents proliferate in our daily lives, their generic and uniform expressiveness presents a significant limitation to their appeal and adoption. Personality expression represents a key prerequisite for creating more human-like and distinctive AI systems. We show that AI models can express deterministic and consistent personalities when instructed using established psychological frameworks, with varying degrees of accuracy depending on model capabilities. We find that more advanced models like GPT-4o and o1 demonstrate the highest accuracy in expressing specified personalities across both Big Five and Myers-Briggs assessments, and further analysis suggests that personality expression emerges from a combination of intelligence and reasoning capabilities. Our results reveal that personality expression operates through holistic reasoning rather than question-by-question optimization, with response-scale metrics showing higher variance than test-scale metrics. Furthermore, we find that model fine-tuning affects communication style independently of personality expression accuracy. These findings establish a foundation for creating AI agents with diverse and consistent personalities, which could significantly enhance human-AI interaction across applications from education to healthcare, while additionally enabling a broader range of more unique AI agents. The ability to quantitatively assess and implement personality expression in AI systems opens new avenues for research into more relatable, trustworthy, and ethically designed AI.', 'abstract_zh': '由大型语言模型驱动的人工智能系统在现代社会中变得越来越普遍，通过自然语言交互实现广泛的应用。随着人工智能代理在日常生活中增多，它们的一般性和统一性表达形式对其吸引力和采用率构成显著限制。个性表达是创造更加人性化和独特的人工智能系统的关键前提。我们展示，在使用现有心理学框架进行指令时，人工智能模型可以表现出确定性和一致性的个性特征，其准确程度根据不同模型的能力而有所不同。我们发现，更先进的模型如GPT-4o和o1在Big Five和Myers-Briggs评估中表现出最高的个性表达准确性，进一步的分析表明，个性表达源自智能和推理能力的结合。我们的研究结果揭示了个性表达通过整体推理而非逐题优化机制发挥作用，响应尺度的度量标准显示出比测试尺度的度量标准更高的变异程度。此外，我们发现模型微调对沟通风格的影响独立于个性表达的准确性。这些发现为创建具有多样化和一致性的个性的人工智能代理奠定了基础，这可以在从教育到医疗保健等应用中显著增强人机交互，并且还可以使更独特的人工智能代理得以实现。在人工智能系统中定量评估和实施个性表达的能力为更加相关、可信和伦理设计的人工智能的更多研究打开了新的途径。', 'title_zh': '确定性人工智能代理心理特征的标准化心理诊断表达'}
{'arxiv_id': 'arXiv:2503.16523', 'title': 'Mind2: Mind-to-Mind Emotional Support System with Bidirectional Cognitive Discourse Analysis', 'authors': 'Shi Yin Hong, Uttamasha Oyshi, Quan Mai, Gibson Nkhata, Susan Gauch', 'link': 'https://arxiv.org/abs/2503.16523', 'abstract': "Emotional support (ES) systems alleviate users' mental distress by generating strategic supportive dialogues based on diverse user situations. However, ES systems are limited in their ability to generate effective ES dialogues that include timely context and interpretability, hindering them from earning public trust. Driven by cognitive models, we propose Mind-to-Mind (Mind2), an ES framework that approaches interpretable ES context modeling for the ES dialogue generation task from a discourse analysis perspective. Specifically, we perform cognitive discourse analysis on ES dialogues according to our dynamic discourse context propagation window, which accommodates evolving context as the conversation between the ES system and user progresses. To enhance interpretability, Mind2 prioritizes details that reflect each speaker's belief about the other speaker with bidirectionality, integrating Theory-of-Mind, physiological expected utility, and cognitive rationality to extract cognitive knowledge from ES conversations. Experimental results support that Mind2 achieves competitive performance versus state-of-the-art ES systems while trained with only 10\\% of the available training data.", 'abstract_zh': '情感支持系统通过生成基于多样化用户情况的策略性支持对话来缓解用户的心理压力。然而，情感支持系统在生成包含及时上下文和可解释性的有效情感支持对话方面能力有限，阻碍了它们获得公众信任。基于认知模型，我们提出Mind-to-Mind（Mind2）框架，该框架从话语分析的角度出发，旨在进行可解释的情感支持上下文建模以生成情感支持对话。具体而言，我们根据动态话语上下文传播窗口对情感支持对话进行认知话语分析，以适应对话过程中逐渐变化的上下文。为了增强可解释性，Mind2优先考虑反映每位对话者对另一方信念的细节，并通过双向方式整合心智理论、生理预期效用和认知理性从情感支持对话中提取认知知识。实验结果表明，Mind2仅使用可用训练数据的10%即可实现与最先进的情感支持系统相当的性能。', 'title_zh': 'Mind2：双向认知话语分析的情感支持系统'}
{'arxiv_id': 'arXiv:2503.16468', 'title': 'Towards properly implementing Theory of Mind in AI systems: An account of four misconceptions', 'authors': 'Ramira van der Meulen, Rineke Verbrugge, Max van Duijn', 'link': 'https://arxiv.org/abs/2503.16468', 'abstract': 'The search for effective collaboration between humans and computer systems is one of the biggest challenges in Artificial Intelligence. One of the more effective mechanisms that humans use to coordinate with one another is theory of mind (ToM). ToM can be described as the ability to `take someone else\'s perspective and make estimations of their beliefs, desires and intentions, in order to make sense of their behaviour and attitudes towards the world\'. If leveraged properly, this skill can be very useful in Human-AI collaboration.\nThis introduces the question how we implement ToM when building an AI system. Humans and AI Systems work quite differently, and ToM is a multifaceted concept, each facet rooted in different research traditions across the cognitive and developmental sciences. We observe that researchers from artificial intelligence and the computing sciences, ourselves included, often have difficulties finding their way in the ToM literature. In this paper, we identify four common misconceptions around ToM that we believe should be taken into account when developing an AI system. We have hyperbolised these misconceptions for the sake of the argument, but add nuance in their discussion.\nThe misconceptions we discuss are:\n(1) "Humans Use a ToM Module, So AI Systems Should As Well".\n(2) "Every Social Interaction Requires (Advanced) ToM".\n(3) "All ToM is the Same".\n(4) "Current Systems Already Have ToM".\nAfter discussing the misconception, we end each section by providing tentative guidelines on how the misconception can be overcome.', 'abstract_zh': '人类与计算机系统之间有效协作的搜索是人工智能领域最大的挑战之一。其中一种更为有效的人类相互协调机制是心理理论（ToM）。ToM 可以描述为“站在他人角度思考，并对其信念、欲望和意图进行估算，从而理解其行为和对世界的态度”的能力。如若合理运用，这一技能对于人机协作大有裨益。\n本论文探讨如何在构建AI系统时实现ToM。人类与AI系统的工作方式大不相同，而ToM是一个多维度的概念，每个维度源于认知科学和发育科学的不同研究传统。我们观察到，来自人工智能和计算科学的研究者，包括我们在内，往往难以在ToM文献中找到方向。在本文中，我们识别出四种关于ToM的常见误解，并认为在开发AI系统时应考虑这些误解。为了论点的需要，我们夸大了这些误解，但在讨论中增添了细微差别。\n我们讨论的误解包括：\n(1) “人类使用ToM模块，因此AI系统也应该有”。\n(2) “每次社交互动都需要（高级）ToM”。\n(3) “所有的ToM都一样”。\n(4) “当前的系统已经具备ToM”。\n在讨论每个误解后，我们为克服这些误解提供了初步指南。', 'title_zh': '关于在AI系统中适当实现理论心智的探讨：四种误解的阐释'}
{'arxiv_id': 'arXiv:2503.16444', 'title': 'Conversational Explanations: Discussing Explainable AI with Non-AI Experts', 'authors': 'Tong Zhang, Mengao Zhang, Wei Yan Low, X. Jessie Yang, Boyang Li', 'link': 'https://arxiv.org/abs/2503.16444', 'abstract': "Explainable AI (XAI) aims to provide insights into the decisions made by AI models. To date, most XAI approaches provide only one-time, static explanations, which cannot cater to users' diverse knowledge levels and information needs. Conversational explanations have been proposed as an effective method to customize XAI explanations. However, building conversational explanation systems is hindered by the scarcity of training data. Training with synthetic data faces two main challenges: lack of data diversity and hallucination in the generated data. To alleviate these issues, we introduce a repetition penalty to promote data diversity and exploit a hallucination detector to filter out untruthful synthetic conversation turns. We conducted both automatic and human evaluations on the proposed system, fEw-shot Multi-round ConvErsational Explanation (EMCEE). For automatic evaluation, EMCEE achieves relative improvements of 81.6% in BLEU and 80.5% in ROUGE compared to the baselines. EMCEE also mitigates the degeneration of data quality caused by training on synthetic data. In human evaluations (N=60), EMCEE outperforms baseline models and the control group in improving users' comprehension, acceptance, trust, and collaboration with static explanations by large margins. Through a fine-grained analysis of model responses, we further demonstrate that training on self-generated synthetic data improves the model's ability to generate more truthful and understandable answers, leading to better user interactions. To the best of our knowledge, this is the first conversational explanation method that can answer free-form user questions following static explanations.", 'abstract_zh': '可解释的人工智能（XAI）旨在提供对AI模型决策的洞察。目前，大多数XAI方法仅提供一次性、静态的解释，无法满足用户多样化的知识水平和信息需求。会话解释已被提议作为一种有效的方法来定制XAI解释。然而，构建会话解释系统受到训练数据稀缺的阻碍。使用合成数据训练面临两个主要挑战：数据多样性不足和生成数据中的幻觉。为了解决这些问题，我们引入了重复惩罚以促进数据多样性，并利用幻觉检测器过滤掉不真实的合成对话轮。我们对提出的系统fEw-shot Multi-round ConvErsational Explanation (EMCEE)进行了自动和人工评估。自动评估结果显示，EMCEE在BLEU上相对提高了81.6%，在ROUGE上相对提高了80.5%，优于基准模型。EMCEE还缓解了由于使用合成数据训练而导致的数据质量退化。在人工评估（N=60）中，EMCEE在提高用户对静态解释的理解、接受度、信任度和合作度方面显著优于基准模型和对照组。通过对模型响应的精细分析，我们进一步证明，使用自我生成的合成数据训练可以提高模型生成更加真实和易于理解的答案的能力，从而改善用户交互。据我们所知，这是首个能够根据静态解释回答自由形式用户问题的会话解释方法。', 'title_zh': '对话式解释：与非AI专家讨论可解释的人工智能'}
