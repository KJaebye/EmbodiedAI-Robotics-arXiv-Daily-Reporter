{'arxiv_id': 'arXiv:2504.13175', 'title': 'Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation', 'authors': 'Sizhe Yang, Wenye Yu, Jia Zeng, Jun Lv, Kerui Ren, Cewu Lu, Dahua Lin, Jiangmiao Pang', 'link': 'https://arxiv.org/abs/2504.13175', 'abstract': 'Visuomotor policies learned from teleoperated demonstrations face challenges such as lengthy data collection, high costs, and limited data diversity. Existing approaches address these issues by augmenting image observations in RGB space or employing Real-to-Sim-to-Real pipelines based on physical simulators. However, the former is constrained to 2D data augmentation, while the latter suffers from imprecise physical simulation caused by inaccurate geometric reconstruction. This paper introduces RoboSplat, a novel method that generates diverse, visually realistic demonstrations by directly manipulating 3D Gaussians. Specifically, we reconstruct the scene through 3D Gaussian Splatting (3DGS), directly edit the reconstructed scene, and augment data across six types of generalization with five techniques: 3D Gaussian replacement for varying object types, scene appearance, and robot embodiments; equivariant transformations for different object poses; visual attribute editing for various lighting conditions; novel view synthesis for new camera perspectives; and 3D content generation for diverse object types. Comprehensive real-world experiments demonstrate that RoboSplat significantly enhances the generalization of visuomotor policies under diverse disturbances. Notably, while policies trained on hundreds of real-world demonstrations with additional 2D data augmentation achieve an average success rate of 57.2%, RoboSplat attains 87.8% in one-shot settings across six types of generalization in the real world.', 'abstract_zh': 'RoboSplat：通过直接操纵3D高斯分布生成多样且视觉上逼真的演示以增强感知运动策略的泛化能力', 'title_zh': '基于高斯点云的新示例生成实现稳健的一次性操作 Manipulation'}
{'arxiv_id': 'arXiv:2504.13165', 'title': 'RUKA: Rethinking the Design of Humanoid Hands with Learning', 'authors': 'Anya Zorin, Irmak Guzey, Billy Yan, Aadhithya Iyer, Lisa Kondrich, Nikhil X. Bhattasali, Lerrel Pinto', 'link': 'https://arxiv.org/abs/2504.13165', 'abstract': "Dexterous manipulation is a fundamental capability for robotic systems, yet progress has been limited by hardware trade-offs between precision, compactness, strength, and affordability. Existing control methods impose compromises on hand designs and applications. However, learning-based approaches present opportunities to rethink these trade-offs, particularly to address challenges with tendon-driven actuation and low-cost materials. This work presents RUKA, a tendon-driven humanoid hand that is compact, affordable, and capable. Made from 3D-printed parts and off-the-shelf components, RUKA has 5 fingers with 15 underactuated degrees of freedom enabling diverse human-like grasps. Its tendon-driven actuation allows powerful grasping in a compact, human-sized form factor. To address control challenges, we learn joint-to-actuator and fingertip-to-actuator models from motion-capture data collected by the MANUS glove, leveraging the hand's morphological accuracy. Extensive evaluations demonstrate RUKA's superior reachability, durability, and strength compared to other robotic hands. Teleoperation tasks further showcase RUKA's dexterous movements. The open-source design and assembly instructions of RUKA, code, and data are available at this https URL.", 'abstract_zh': '灵巧操作是机器人系统的一项基本能力，但由于精确度、紧凑性、力量和成本之间的硬件权衡限制了进展。现有的控制方法在手部设计和应用上做出了妥协。然而，基于学习的方法提供了重新思考这些权衡的可能性，尤其是针对肌腱驱动执行和低成本材料的挑战。本研究介绍了RUKA，一种紧凑、经济且功能强大的肌腱驱动人形手。RUKA采用3D打印部件和现成组件制成，具有5个手指和15个未驱动的自由度，能够实现多种类似人类的握持方式。其肌腱驱动的执行允许在紧凑的人类尺寸形式因子中进行强大的抓取。为了解决控制方面的挑战，我们利用MANUS手套采集的运动捕捉数据学习关节到执行器和指尖到执行器的模型，利用手部的形态准确性。广泛评估表明，RUKA在抓取范围、耐用性和强度方面优于其他机器人手。远程操作任务还展示了RUKA的灵巧动作。RUKA的开源设计和组装说明、代码和数据可在此访问：this https URL。', 'title_zh': 'RUKA: 重新思考类人手的设计与学习'}
{'arxiv_id': 'arXiv:2504.13149', 'title': 'Long Range Navigator (LRN): Extending robot planning horizons beyond metric maps', 'authors': 'Matt Schmittle, Rohan Baijal, Nathan Hatch, Rosario Scalise, Mateo Guaman Castro, Sidharth Talia, Khimya Khetarpal, Byron Boots, Siddhartha Srinivasa', 'link': 'https://arxiv.org/abs/2504.13149', 'abstract': "A robot navigating an outdoor environment with no prior knowledge of the space must rely on its local sensing to perceive its surroundings and plan. This can come in the form of a local metric map or local policy with some fixed horizon. Beyond that, there is a fog of unknown space marked with some fixed cost. A limited planning horizon can often result in myopic decisions leading the robot off course or worse, into very difficult terrain. Ideally, we would like the robot to have full knowledge that can be orders of magnitude larger than a local cost map. In practice, this is intractable due to sparse sensing information and often computationally expensive. In this work, we make a key observation that long-range navigation only necessitates identifying good frontier directions for planning instead of full map knowledge. To this end, we propose Long Range Navigator (LRN), that learns an intermediate affordance representation mapping high-dimensional camera images to `affordable' frontiers for planning, and then optimizing for maximum alignment with the desired goal. LRN notably is trained entirely on unlabeled ego-centric videos making it easy to scale and adapt to new platforms. Through extensive off-road experiments on Spot and a Big Vehicle, we find that augmenting existing navigation stacks with LRN reduces human interventions at test-time and leads to faster decision making indicating the relevance of LRN. this https URL", 'abstract_zh': '一种在无先验环境知识的情况下导航室外环境的机器人必须依赖于局部感知来感知其周围环境并规划路径。这可以表现为局部度量地图或具有固定预测 horizon 的局部策略。超出这一范围，存在未知的“模糊”区域，标记有一定的固定成本。固定的规划 horizon 往往会导致短视的决策，使机器人偏离航线，甚至进入非常困难的地形。理想情况下，我们希望机器人具备全面的知识，这种知识可能是局部成本图的成千上万倍。然而，由于稀疏的感知信息和通常计算上的昂贵，这往往是不可行的。在此工作中，我们关键地观察到，远程导航只需要识别出适合规划的前沿方向，而非全面的地图知识。为此，我们提出了Long Range Navigator (LRN)，它学习一个中间的操作潜能表示，将高维相机图像映射到可用于规划的“可负担”的前沿，并优化与目标的最大对齐度。LRN 特别是通过全未标记的自视点视频进行训练，使其易于扩展并适应新的平台。通过在 Spot 和一辆大型车辆上的离路面实验，我们发现，将 LRN 与现有的导航堆栈结合使用，在测试时减少了人工干预，并导致更快的决策，表明了 LRN 的相关性。', 'title_zh': '长距离导航器（LRN）：将机器人规划视野扩展至超越度量地图之外'}
{'arxiv_id': 'arXiv:2504.13059', 'title': 'RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins', 'authors': 'Yao Mu, Tianxing Chen, Zanxin Chen, Shijia Peng, Zhiqian Lan, Zeyu Gao, Zhixuan Liang, Qiaojun Yu, Yude Zou, Mingkun Xu, Lunkai Lin, Zhiqiang Xie, Mingyu Ding, Ping Luo', 'link': 'https://arxiv.org/abs/2504.13059', 'abstract': 'In the rapidly advancing field of robotics, dual-arm coordination and complex object manipulation are essential capabilities for developing advanced autonomous systems. However, the scarcity of diverse, high-quality demonstration data and real-world-aligned evaluation benchmarks severely limits such development. To address this, we introduce RoboTwin, a generative digital twin framework that uses 3D generative foundation models and large language models to produce diverse expert datasets and provide a real-world-aligned evaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates varied digital twins of objects from single 2D images, generating realistic and interactive scenarios. It also introduces a spatial relation-aware code generation framework that combines object annotations with large language models to break down tasks, determine spatial constraints, and generate precise robotic movement code. Our framework offers a comprehensive benchmark with both simulated and real-world data, enabling standardized evaluation and better alignment between simulated training and real-world performance. We validated our approach using the open-source COBOT Magic Robot platform. Policies pre-trained on RoboTwin-generated data and fine-tuned with limited real-world samples demonstrate significant potential for enhancing dual-arm robotic manipulation systems by improving success rates by over 70% for single-arm tasks and over 40% for dual-arm tasks compared to models trained solely on real-world data.', 'abstract_zh': '机器人领域中，双臂协调与复杂物体操控是发展高级自主系统的关键能力。然而，多样化的高质量示范数据和与现实世界对齐的评估基准的缺乏严重限制了这一发展。为了解决这一问题，我们引入了RoboTwin，这是一种使用3D生成基础模型和大规模语言模型生成多样化专家数据集，并提供双臂机器人任务现实世界对齐评估平台的生成数字孪生框架。具体而言，RoboTwin 从单张2D图像生成物体的多样化数字孪生，生成真实且互动的场景。它还引入了一种空间关系感知的代码生成框架，结合对象标注和大规模语言模型来分解任务、确定空间约束并生成精确的机器人运动代码。我们的框架提供了包含模拟和现实世界数据的综合基准，使得标准化评估和模拟训练与现实世界性能更好的对齐成为可能。我们使用开源的COBOT Magic Robot平台验证了这种方法。基于RoboTwin生成数据预训练并在有限的现实世界样本上微调的策略，在单臂任务上成功率提高了70%以上，在双臂任务上提高了40%以上，相较于仅使用现实世界数据训练的模型具有显著潜力。', 'title_zh': 'RoboTwin: 双臂机器人基准测试与生成式数字孪生'}
{'arxiv_id': 'arXiv:2504.12826', 'title': 'UncAD: Towards Safe End-to-end Autonomous Driving via Online Map Uncertainty', 'authors': 'Pengxuan Yang, Yupeng Zheng, Qichao Zhang, Kefei Zhu, Zebin Xing, Qiao Lin, Yun-Fu Liu, Zhiguo Su, Dongbin Zhao', 'link': 'https://arxiv.org/abs/2504.12826', 'abstract': 'End-to-end autonomous driving aims to produce planning trajectories from raw sensors directly. Currently, most approaches integrate perception, prediction, and planning modules into a fully differentiable network, promising great scalability. However, these methods typically rely on deterministic modeling of online maps in the perception module for guiding or constraining vehicle planning, which may incorporate erroneous perception information and further compromise planning safety. To address this issue, we delve into the importance of online map uncertainty for enhancing autonomous driving safety and propose a novel paradigm named UncAD. Specifically, UncAD first estimates the uncertainty of the online map in the perception module. It then leverages the uncertainty to guide motion prediction and planning modules to produce multi-modal trajectories. Finally, to achieve safer autonomous driving, UncAD proposes an uncertainty-collision-aware planning selection strategy according to the online map uncertainty to evaluate and select the best trajectory. In this study, we incorporate UncAD into various state-of-the-art (SOTA) end-to-end methods. Experiments on the nuScenes dataset show that integrating UncAD, with only a 1.9% increase in parameters, can reduce collision rates by up to 26% and drivable area conflict rate by up to 42%. Codes, pre-trained models, and demo videos can be accessed at this https URL.', 'abstract_zh': '从传感器直接生成自主驾驶规划轨迹的端到端方法：考虑在线地图不确定性以增强自主驾驶安全', 'title_zh': 'UncAD：通过在线地图不确定性实现安全端到端自动驾驶'}
{'arxiv_id': 'arXiv:2504.12755', 'title': 'Trajectory Adaptation using Large Language Models', 'authors': 'Anurag Maurya, Tashmoy Ghosh, Ravi Prakash', 'link': 'https://arxiv.org/abs/2504.12755', 'abstract': 'Adapting robot trajectories based on human instructions as per new situations is essential for achieving more intuitive and scalable human-robot interactions. This work proposes a flexible language-based framework to adapt generic robotic trajectories produced by off-the-shelf motion planners like RRT, A-star, etc, or learned from human demonstrations. We utilize pre-trained LLMs to adapt trajectory waypoints by generating code as a policy for dense robot manipulation, enabling more complex and flexible instructions than current methods. This approach allows us to incorporate a broader range of commands, including numerical inputs. Compared to state-of-the-art feature-based sequence-to-sequence models which require training, our method does not require task-specific training and offers greater interpretability and more effective feedback mechanisms. We validate our approach through simulation experiments on the robotic manipulator, aerial vehicle, and ground robot in the Pybullet and Gazebo simulation environments, demonstrating that LLMs can successfully adapt trajectories to complex human instructions.', 'abstract_zh': '基于人类指令适应机器人轨迹对于实现更直观和可扩展的人机交互至关重要。本工作提出了一种灵活的语言框架，用于适应通用机器人轨迹，这些轨迹由RRT、A-Star等商用运动规划器生成，或从人类演示中学习。我们利用预训练的大语言模型（LLM）生成代码作为策略，以适应轨迹Waypoints，从而实现更复杂的灵活指令，这比现有方法更具优势。该方法允许我们纳入更广泛的命令，包括数值输入。与需要训练的最新基于特征的序列到序列模型相比，我们的方法无需特定任务的训练，提供了更高的可解释性和更有效的反馈机制。我们通过在Pybullet和Gazebo模拟环境中对 manipulator、无人机和地面机器人进行仿真实验，验证了该方法的有效性，展示了大语言模型能够成功适应复杂的人类指令。', 'title_zh': '使用大规模语言模型进行轨迹适应'}
{'arxiv_id': 'arXiv:2504.12702', 'title': 'Embodied Neuromorphic Control Applied on a 7-DOF Robotic Manipulator', 'authors': 'Ziqi Wang, Jingyue Zhao, Jichao Yang, Yaohua Wang, Xun Xiao, Yuan Li, Chao Xiao, Lei Wang', 'link': 'https://arxiv.org/abs/2504.12702', 'abstract': 'The development of artificial intelligence towards real-time interaction with the environment is a key aspect of embodied intelligence and robotics. Inverse dynamics is a fundamental robotics problem, which maps from joint space to torque space of robotic systems. Traditional methods for solving it rely on direct physical modeling of robots which is difficult or even impossible due to nonlinearity and external disturbance. Recently, data-based model-learning algorithms are adopted to address this issue. However, they often require manual parameter tuning and high computational costs. Neuromorphic computing is inherently suitable to process spatiotemporal features in robot motion control at extremely low costs. However, current research is still in its infancy: existing works control only low-degree-of-freedom systems and lack performance quantification and comparison. In this paper, we propose a neuromorphic control framework to control 7 degree-of-freedom robotic manipulators. We use Spiking Neural Network to leverage the spatiotemporal continuity of the motion data to improve control accuracy, and eliminate manual parameters tuning. We validated the algorithm on two robotic platforms, which reduces torque prediction error by at least 60% and performs a target position tracking task successfully. This work advances embodied neuromorphic control by one step forward from proof of concept to applications in complex real-world tasks.', 'abstract_zh': '人工智能向实时环境交互的发展是本体智能和机器人技术的关键aspect。逆动力学是机器人技术中的一个基础问题，它将关节空间映射到机器人系统的扭矩空间。传统的方法依赖于对机器人的直接物理建模，但由于非线性和外部干扰，这通常是困难的或甚至不可能的。近年来，基于数据的模型学习算法被采用来解决这个问题。然而，它们通常需要手动参数调整并且计算成本高。神经形态计算本质上适合以极低的成本处理机器人运动控制中的时空特征。然而，当前的研究仍处于初级阶段：现有的工作只控制低自由度系统，并缺乏性能量化和比较。在本文中，我们提出了一种神经形态控制框架来控制7自由度的机器人 manipulator。我们使用脉冲神经网络利用运动数据的时空连续性以提高控制精度并消除手动参数调整。我们在两个机器人平台上验证了该算法，降低了至少60%的扭矩预测误差并成功执行了目标位置跟踪任务。这项工作将本体神经形态控制从概念证明推进到复杂实际任务的应用中。', 'title_zh': '具身神经形态控制在7-自由度机器人 manipulator 上的应用'}
{'arxiv_id': 'arXiv:2504.12609', 'title': 'Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration', 'authors': 'Tyler Ga Wei Lum, Olivia Y. Lee, C. Karen Liu, Jeannette Bohg', 'link': 'https://arxiv.org/abs/2504.12609', 'abstract': 'Teaching robots dexterous manipulation skills often requires collecting hundreds of demonstrations using wearables or teleoperation, a process that is challenging to scale. Videos of human-object interactions are easier to collect and scale, but leveraging them directly for robot learning is difficult due to the lack of explicit action labels from videos and morphological differences between robot and human hands. We propose Human2Sim2Robot, a novel real-to-sim-to-real framework for training dexterous manipulation policies using only one RGB-D video of a human demonstrating a task. Our method utilizes reinforcement learning (RL) in simulation to cross the human-robot embodiment gap without relying on wearables, teleoperation, or large-scale data collection typically necessary for imitation learning methods. From the demonstration, we extract two task-specific components: (1) the object pose trajectory to define an object-centric, embodiment-agnostic reward function, and (2) the pre-manipulation hand pose to initialize and guide exploration during RL training. We found that these two components are highly effective for learning the desired task, eliminating the need for task-specific reward shaping and tuning. We demonstrate that Human2Sim2Robot outperforms object-aware open-loop trajectory replay by 55% and imitation learning with data augmentation by 68% across grasping, non-prehensile manipulation, and multi-step tasks. Project Site: this https URL', 'abstract_zh': '基于人类演示的实时到模拟再到实时的 Dexterous 操作策略训练框架', 'title_zh': '通过基于一人示范的仿真实验到现实的RL方法跨越人类与机器人 bodilic 缩隙'}
{'arxiv_id': 'arXiv:2504.12441', 'title': 'Learning Transferable Friction Models and LuGre Identification via Physics Informed Neural Networks', 'authors': 'Asutay Ozmen, João P. Hespanha, Katie Byl', 'link': 'https://arxiv.org/abs/2504.12441', 'abstract': 'Accurately modeling friction in robotics remains a core challenge, as robotics simulators like Mujoco and PyBullet use simplified friction models or heuristics to balance computational efficiency with accuracy, where these simplifications and approximations can lead to substantial differences between simulated and physical performance. In this paper, we present a physics-informed friction estimation framework that enables the integration of well-established friction models with learnable components-requiring only minimal, generic measurement data. Our approach enforces physical consistency yet retains the flexibility to adapt to real-world complexities. We demonstrate, on an underactuated and nonlinear system, that the learned friction models, trained solely on small and noisy datasets, accurately simulate dynamic friction properties and reduce the sim-to-real gap. Crucially, we show that our approach enables the learned models to be transferable to systems they are not trained on. This ability to generalize across multiple systems streamlines friction modeling for complex, underactuated tasks, offering a scalable and interpretable path toward bridging the sim-to-real gap in robotics and control.', 'abstract_zh': '准确建模机器人中的摩擦仍然是一个核心挑战，因为像Mujoco和PyBullet这样的机器人模拟器使用简化或启发式的摩擦模型来平衡计算效率与准确性，这些简化和近似可能导致模拟和物理性能之间的巨大差异。在本文中，我们提出了一种物理信息摩擦估计框架，该框架能够将成熟的摩擦模型与可学习组件集成起来，只需使用少量的通用测量数据。我们的方法保证了物理一致性，同时保持了适应现实世界复杂性的灵活性。我们在一个欠驱动且非线性系统上展示了所学的摩擦模型，仅在少量且噪声数据集上进行训练，能够准确模拟动态摩擦性能并减少模拟到现实的差距。关键的是，我们展示了我们的方法使得所学模型能够迁移到未经过训练的系统上。这种跨多个系统泛化的能力简化了复杂欠驱动任务中的摩擦建模，提供了一种可扩展且可解释的途径，以缩小机器人和控制中的模拟到现实差距。', 'title_zh': '基于物理约束神经网络的学习转移摩擦模型及LuGre识别'}
{'arxiv_id': 'arXiv:2504.13111', 'title': 'Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification', 'authors': 'Kumar Manas, Christian Schlauch, Adrian Paschke, Christian Wirth, Nadja Klein', 'link': 'https://arxiv.org/abs/2504.13111', 'abstract': 'Deep learning-based trajectory prediction models have demonstrated promising capabilities in capturing complex interactions. However, their out-of-distribution generalization remains a significant challenge, particularly due to unbalanced data and a lack of enough data and diversity to ensure robustness and calibration. To address this, we propose SHIFT (Spectral Heteroscedastic Informed Forecasting for Trajectories), a novel framework that uniquely combines well-calibrated uncertainty modeling with informative priors derived through automated rule extraction. SHIFT reformulates trajectory prediction as a classification task and employs heteroscedastic spectral-normalized Gaussian processes to effectively disentangle epistemic and aleatoric uncertainties. We learn informative priors from training labels, which are automatically generated from natural language driving rules, such as stop rules and drivability constraints, using a retrieval-augmented generation framework powered by a large language model. Extensive evaluations over the nuScenes dataset, including challenging low-data and cross-location scenarios, demonstrate that SHIFT outperforms state-of-the-art methods, achieving substantial gains in uncertainty calibration and displacement metrics. In particular, our model excels in complex scenarios, such as intersections, where uncertainty is inherently higher. Project page: this https URL.', 'abstract_zh': '基于深度学习的轨迹预测模型在捕捉复杂交互方面展现了 promising 能力，但它们的 out-of-distribution 通用性仍然是一个重大挑战，尤其是由于数据不平衡和缺乏足够的数据和多样性来确保 robustness 和 calibration。为了解决这个问题，我们提出了一种名为 SHIFT（Spectral Heteroscedastic Informed Forecasting for Trajectories）的新颖框架，该框架独特地结合了校准良好的不确定性建模和通过自动规则提取获得的信息先验。SHIFT 将轨迹预测重新定义为分类任务，并采用异方差谱规范化高斯过程来有效分离 epistemic 和 aleatoric 不确定性。我们通过一种使用大型语言模型驱动的检索增强生成框架从训练标签中学习信息先验，这些标签是从自然语言驾驶规则（如停止规则和可行驶性约束）自动生成的。在 nuScenes 数据集上的广泛评估，包括具有挑战性的低数据和跨地点场景，表明 SHIFT 在不确定性校准和位移指标方面优于最先进的方法，特别是在复杂的交叉路口等场景中表现出色。项目页面: this https URL。', 'title_zh': '基于规则正则化异方差深度分类的不确定性意识轨迹预测'}
{'arxiv_id': 'arXiv:2504.12680', 'title': 'Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning', 'authors': 'Baining Zhao, Ziyou Wang, Jianjie Fang, Chen Gao, Fanhang Man, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li, Wenwu Zhu', 'link': 'https://arxiv.org/abs/2504.12680', 'abstract': 'Humans can perceive and reason about spatial relationships from sequential visual observations, such as egocentric video streams. However, how pretrained models acquire such abilities, especially high-level reasoning, remains unclear. This paper introduces Embodied-R, a collaborative framework combining large-scale Vision-Language Models (VLMs) for perception and small-scale Language Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a novel reward system considering think-answer logical consistency, the model achieves slow-thinking capabilities with limited computational resources. After training on only 5k embodied video samples, Embodied-R with a 3B LM matches state-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on both in-distribution and out-of-distribution embodied spatial reasoning tasks. Embodied-R also exhibits emergent thinking patterns such as systematic analysis and contextual integration. We further explore research questions including response length, training on VLM, strategies for reward design, and differences in model generalization after SFT (Supervised Fine-Tuning) and RL training.', 'abstract_zh': '人类可以从连续的视觉观察中感知和推理空间关系，例如第一人称视频流。然而，预训练模型如何获取这些能力，尤其是高级推理能力，尚不明确。本文介绍了一种协作框架Embodied-R，该框架结合大规模视觉-语言模型（VLMs）用于感知，小型语言模型（LMs）用于推理。通过考虑思考-回答逻辑一致性的新型强化学习（RL）奖励系统，模型在有限的计算资源下实现了慢思考能力。在仅使用5000个 embodied 视频样本训练后，Embodied-R 结合一个3B LMs达到了当前最先进的跨模态推理模型（OpenAI-o1，Gemini-2.5-pro）在分布内和分布外 embodied 空间推理任务中的性能。Embodied-R 还展示了系统分析和上下文整合等新兴思考模式。我们进一步探讨了包括响应长度、在VLM上进行训练、奖励设计策略以及SFT（监督微调）和RL训练后模型泛化能力差异等研究问题。', 'title_zh': '具身-R：通过强化学习激活基础模型中具身处境空间推理的协作框架'}
{'arxiv_id': 'arXiv:2504.12714', 'title': 'Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination', 'authors': 'Kunal Jha, Wilka Carvalho, Yancheng Liang, Simon S. Du, Max Kleiman-Weiner, Natasha Jaques', 'link': 'https://arxiv.org/abs/2504.12714', 'abstract': 'Zero-shot coordination (ZSC), the ability to adapt to a new partner in a cooperative task, is a critical component of human-compatible AI. While prior work has focused on training agents to cooperate on a single task, these specialized models do not generalize to new tasks, even if they are highly similar. Here, we study how reinforcement learning on a distribution of environments with a single partner enables learning general cooperative skills that support ZSC with many new partners on many new problems. We introduce two Jax-based, procedural generators that create billions of solvable coordination challenges. We develop a new paradigm called Cross-Environment Cooperation (CEC), and show that it outperforms competitive baselines quantitatively and qualitatively when collaborating with real people. Our findings suggest that learning to collaborate across many unique scenarios encourages agents to develop general norms, which prove effective for collaboration with different partners. Together, our results suggest a new route toward designing generalist cooperative agents capable of interacting with humans without requiring human data.', 'abstract_zh': '零样本协调（ZSC）：在新任务中适应新伙伴的协作能力是人类兼容AI的关键组成部分。以往的工作集中在训练代理在同一任务上协作，但这些专门的模型不适用于新任务，即使新任务非常相似。在这里，我们研究了在包含单一伙伴的环境分布上使用强化学习如何促进学习适用于与许多新伙伴在多种新问题上实现ZSC的一般协作技能。我们引入了基于Jax的两个过程生成器，创建了数亿个可解协调挑战。我们提出了一个新的范式，称为跨环境协作（CEC），并与人类协作时，展示了其在定量和定性上优于竞争基线。我们的发现表明，在多种独特场景中学习协作促使代理发展出一般性规范，这些规范对于与不同伙伴协作证明是有效的。结合我们的结果，这表明了一种新的设计途径，旨在设计能够与人类互动的一般性协作代理，而无需使用人类数据。', 'title_zh': '跨环境协作实现零样本多智能体协调'}
{'arxiv_id': 'arXiv:2504.12511', 'title': 'Multimodal LLM Augmented Reasoning for Interpretable Visual Perception Analysis', 'authors': 'Shravan Chaudhari, Trilokya Akula, Yoon Kim, Tom Blake', 'link': 'https://arxiv.org/abs/2504.12511', 'abstract': 'In this paper, we advance the study of AI-augmented reasoning in the context of Human-Computer Interaction (HCI), psychology and cognitive science, focusing on the critical task of visual perception. Specifically, we investigate the applicability of Multimodal Large Language Models (MLLMs) in this domain. To this end, we leverage established principles and explanations from psychology and cognitive science related to complexity in human visual perception. We use them as guiding principles for the MLLMs to compare and interprete visual content. Our study aims to benchmark MLLMs across various explainability principles relevant to visual perception. Unlike recent approaches that primarily employ advanced deep learning models to predict complexity metrics from visual content, our work does not seek to develop a mere new predictive model. Instead, we propose a novel annotation-free analytical framework to assess utility of MLLMs as cognitive assistants for HCI tasks, using visual perception as a case study. The primary goal is to pave the way for principled study in quantifying and evaluating the interpretability of MLLMs for applications in improving human reasoning capability and uncovering biases in existing perception datasets annotated by humans.', 'abstract_zh': '在人机交互(HCI)、心理学和认知科学的背景下推进人工智能增强推理的研究：基于多模态大型语言模型在视觉感知中的应用', 'title_zh': '多模态LLM增强的可解释视觉感知分析'}
