{'arxiv_id': 'arXiv:2504.13170', 'title': 'A New Semidefinite Relaxation for Linear and Piecewise-Affine Optimal Control with Time Scaling', 'authors': 'Lujie Yang, Tobia Marcucci, Pablo A. Parrilo, Russ Tedrake', 'link': 'https://arxiv.org/abs/2504.13170', 'abstract': 'We introduce a semidefinite relaxation for optimal control of linear systems with time scaling. These problems are inherently nonconvex, since the system dynamics involves bilinear products between the discretization time step and the system state and controls. The proposed relaxation is closely related to the standard second-order semidefinite relaxation for quadratic constraints, but we carefully select a subset of the possible bilinear terms and apply a change of variables to achieve empirically tight relaxations while keeping the computational load light. We further extend our method to handle piecewise-affine (PWA) systems by formulating the PWA optimal-control problem as a shortest-path problem in a graph of convex sets (GCS). In this GCS, different paths represent different mode sequences for the PWA system, and the convex sets model the relaxed dynamics within each mode. By combining a tight convex relaxation of the GCS problem with our semidefinite relaxation with time scaling, we can solve PWA optimal-control problems through a single semidefinite program.', 'abstract_zh': '时间尺度下的线性系统最优控制的半定 Relaxation 方法及其在分段线性 affine 系统中的应用', 'title_zh': '一种新的半定松弛方法，用于具有时间缩放的线性与分段线性最优控制'}
{'arxiv_id': 'arXiv:2504.12813', 'title': 'Approaching Current Challenges in Developing a Software Stack for Fully Autonomous Driving', 'authors': 'Simon Sagmeister, Simon Hoffmann, Tobias Betz, Dominic Ebner, Daniel Esser, Markus Lienkamp', 'link': 'https://arxiv.org/abs/2504.12813', 'abstract': "Autonomous driving is a complex undertaking. A common approach is to break down the driving task into individual subtasks through modularization. These sub-modules are usually developed and published separately. However, if these individually developed algorithms have to be combined again to form a full-stack autonomous driving software, this poses particular challenges. Drawing upon our practical experience in developing the software of TUM Autonomous Motorsport, we have identified and derived these challenges in developing an autonomous driving software stack within a scientific environment. We do not focus on the specific challenges of individual algorithms but on the general difficulties that arise when deploying research algorithms on real-world test vehicles. To overcome these challenges, we introduce strategies that have been effective in our development approach. We additionally provide open-source implementations that enable these concepts on GitHub. As a result, this paper's contributions will simplify future full-stack autonomous driving projects, which are essential for a thorough evaluation of the individual algorithms.", 'abstract_zh': '自主驾驶是一项复杂的任务。一种常见的方法是通过模块化将驾驶任务分解为个体子任务。这些子模块通常分别开发和发布。然而，如果这些单独开发的算法需要重新组合以形成完整的自主驾驶软件堆栈，这将带来特定的挑战。基于我们为TUM Autonomous Motorsport开发软件的实际经验，我们在这篇论文中识别并分析了在科学环境中开发自主驾驶软件堆栈时遇到的一般困难。我们不聚焦于个别算法的具体挑战，而是关注部署研究算法到真实世界测试车辆时出现的一般困难。为了克服这些挑战，我们提出了在开发过程中有效的策略，并在GitHub上提供了开源实现，以促进这些概念的应用。因此，本文的贡献将简化未来的完整堆栈自主驾驶项目，这对于全面评估个别算法至关重要。', 'title_zh': '克服开发全自动驾驶软件栈面临的当前挑战'}
{'arxiv_id': 'arXiv:2504.13150', 'title': 'Readable Twins of Unreadable Models', 'authors': 'Krzysztof Pancerz, Piotr Kulicki, Michał Kalisz, Andrzej Burda, Maciej Stanisławski, Jaromir Sarzyński', 'link': 'https://arxiv.org/abs/2504.13150', 'abstract': 'Creating responsible artificial intelligence (AI) systems is an important issue in contemporary research and development of works on AI. One of the characteristics of responsible AI systems is their explainability. In the paper, we are interested in explainable deep learning (XDL) systems. On the basis of the creation of digital twins of physical objects, we introduce the idea of creating readable twins (in the form of imprecise information flow models) for unreadable deep learning models. The complete procedure for switching from the deep learning model (DLM) to the imprecise information flow model (IIFM) is presented. The proposed approach is illustrated with an example of a deep learning classification model for image recognition of handwritten digits from the MNIST data set.', 'abstract_zh': '负责任的人工智能系统创建是当前人工智能研究与开发中的一个重要议题。负责任的人工智能系统的一个特征是可解释性。在本文中，我们关注可解释的深度学习(XDL)系统。基于物理对象的数字孪生创建，我们提出了创建易于理解的孪生（以不精确信息流模型的形式）的思想，以解释难以理解的深度学习模型。从深度学习模型(DLM)切换到不精确信息流模型(IIFM)的完整过程被详细呈现。所提出的方法通过一个基于MNIST数据集的手写数字图像识别深度学习分类模型示例进行了说明。', 'title_zh': '难读模型的易读双胞胎'}
{'arxiv_id': 'arXiv:2504.13146', 'title': 'Antidistillation Sampling', 'authors': 'Yash Savani, Asher Trockman, Zhili Feng, Avi Schwarzschild, Alexander Robey, Marc Finzi, J. Zico Kolter', 'link': 'https://arxiv.org/abs/2504.13146', 'abstract': "Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. \\emph{Antidistillation sampling} provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see this https URL.", 'abstract_zh': '对抗蒸馏采样通过战略性修改模型的下一-token概率分布，污染推理踪迹，从而显著降低其蒸馏有效性，同时保持模型的实际实用价值。更多信息，请参见：https://<insert_link_here>', 'title_zh': '反浓缩采样'}
{'arxiv_id': 'arXiv:2504.12612', 'title': 'The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance', 'authors': 'Ching-Chun Chang, Isao Echizen', 'link': 'https://arxiv.org/abs/2504.12612', 'abstract': 'Provenance is the chronology of things, resonating with the fundamental pursuit to uncover origins, trace connections, and situate entities within the flow of space and time. As artificial intelligence advances towards autonomous agents capable of interactive collaboration on complex tasks, the provenance of generated content becomes entangled in the interplay of collective creation, where contributions are continuously revised, extended or overwritten. In a multi-agent generative chain, content undergoes successive transformations, often leaving little, if any, trace of prior contributions. In this study, we investigates the problem of tracking multi-agent provenance across the temporal dimension of generation. We propose a chronological system for post hoc attribution of generative history from content alone, without reliance on internal memory states or external meta-information. At its core lies the notion of symbolic chronicles, representing signed and time-stamped records, in a form analogous to the chain of custody in forensic science. The system operates through a feedback loop, whereby each generative timestep updates the chronicle of prior interactions and synchronises it with the synthetic content in the very act of generation. This research seeks to develop an accountable form of collaborative artificial intelligence within evolving cyber ecosystems.', 'abstract_zh': '起源追踪：复杂任务中交互生成内容的时间维度集体创作追溯体系研究', 'title_zh': 'Foundation AI for 多代理溯源的forensics 纪事'}
{'arxiv_id': 'arXiv:2504.12529', 'title': 'Is Trust Correlated With Explainability in AI? A Meta-Analysis', 'authors': 'Zahra Atf, Peter R. Lewis', 'link': 'https://arxiv.org/abs/2504.12529', 'abstract': 'This study critically examines the commonly held assumption that explicability in artificial intelligence (AI) systems inherently boosts user trust. Utilizing a meta-analytical approach, we conducted a comprehensive examination of the existing literature to explore the relationship between AI explainability and trust. Our analysis, incorporating data from 90 studies, reveals a statistically significant but moderate positive correlation between the explainability of AI systems and the trust they engender among users. This indicates that while explainability contributes to building trust, it is not the sole or predominant factor in this equation. In addition to academic contributions to the field of Explainable AI (XAI), this research highlights its broader socio-technical implications, particularly in promoting accountability and fostering user trust in critical domains such as healthcare and justice. By addressing challenges like algorithmic bias and ethical transparency, the study underscores the need for equitable and sustainable AI adoption. Rather than focusing solely on immediate trust, we emphasize the normative importance of fostering authentic and enduring trustworthiness in AI systems.', 'abstract_zh': '本研究批判性地审视了人工智能（AI）系统中可解释性普遍被认为会增强用户信任这一假设。通过元分析的方法，我们对现有文献进行了全面考察，探讨了AI解释性和信任之间的关系。我们的分析结合了90项研究的数据，揭示出AI系统的可解释性与用户信任之间存在统计学上显著但中等程度的正相关关系。这表明虽然可解释性有助于建立信任，但它不是这一方程中的唯一或主要因素。除了为可解释人工智能（XAI）领域的学术贡献外，本研究还强调了其更广泛的社会和技术意义，特别是在医疗保健和司法等关键领域促进问责制和用户信任方面的重要性。通过解决算法偏见和伦理透明度等挑战，研究突显了公平和可持续的AI采用的必要性。本研究强调了培养AI系统中真实持久的信任价值的规范重要性，而不仅仅是即时的信任。', 'title_zh': 'AI中的可解释性与信任相关吗？一项元分析'}
{'arxiv_id': 'arXiv:2504.12497', 'title': 'Heuristic Recognition and Rapid Response to Unfamiliar Events Outside of Agent Design Scope', 'authors': 'Robert E. Wray, Steven J. Jones, John E. Laird', 'link': 'https://arxiv.org/abs/2504.12497', 'abstract': 'Regardless of past learning, an agent in an open world will face unfamiliar situations and events outside of prior experience, existing models, or policies. Further, the agent will sometimes lack relevant knowledge and/or sufficient time to assess the situation, generate and evaluate options, and pursue a robustly considered course of action. How can an agent respond reasonably to situations that are outside of its original design scope? How can it recognize such situations sufficiently quickly and reliably to determine reasonable, adaptive courses of action? We identify key characteristics needed for solutions, evaluate the state-of-the-art by these requirements, and outline a proposed, novel approach that combines domain-general meta-knowledge (in the form of appraisals inspired by human cognition) and metareasoning. It has the potential to provide fast, adaptive responses to unfamiliar situations, more fully meeting the performance characteristics required for open-world, general agents.', 'abstract_zh': '无论过去的学习如何，一个开放世界中的智能体将面临超出其先前经验、现有模型或策略的不熟悉情况和事件。此外，智能体有时会缺乏相关的知识和/或充足的时间来评估情况、产生和评估选项，并采取一个慎重考虑的行动方案。当情况超出了其原始设计范围时，智能体如何做出合理的反应？它如何能够足够快速和可靠地识别这些情况，以确定合理的适应性行动方案？我们识别出所需的关键特征，根据这些要求评估现有的最先进的方法，并提出一种新的结合领域通用元知识（以人类认知启发的评估形式）和元推理的方法。该方法具有提供快速适应性反应以处理不熟悉情况的潜力，更全面地满足开放世界通用智能体所需的表现特征。', 'title_zh': '启发式识别和快速响应超出代理设计范围的陌生事件'}
{'arxiv_id': 'arXiv:2504.12482', 'title': 'Agentic AI Optimisation (AAIO): what it is, how it works, why it matters, and how to deal with it', 'authors': 'Luciano Floridi, Carlotta Buttaboni, Emmie Hine, Jessica Morley, Claudio Novelli, Tyler Schroder', 'link': 'https://arxiv.org/abs/2504.12482', 'abstract': "The emergence of Agentic Artificial Intelligence (AAI) systems capable of independently initiating digital interactions necessitates a new optimisation paradigm designed explicitly for seamless agent-platform interactions. This article introduces Agentic AI Optimisation (AAIO) as an essential methodology for ensuring effective integration between websites and agentic AI systems. Like how Search Engine Optimisation (SEO) has shaped digital content discoverability, AAIO can define interactions between autonomous AI agents and online platforms. By examining the mutual interdependency between website optimisation and agentic AI success, the article highlights the virtuous cycle that AAIO can create. It further explores the governance, ethical, legal, and social implications (GELSI) of AAIO, emphasising the necessity of proactive regulatory frameworks to mitigate potential negative impacts. The article concludes by affirming AAIO's essential role as part of a fundamental digital infrastructure in the era of autonomous digital agents, advocating for equitable and inclusive access to its benefits.", 'abstract_zh': '具备自主发起数字交互能力的代理人工智能（AAI）系统 emergence necessitates a new optimisation paradigm designed explicitly for seamless agent-platform interactions. 这篇文章介绍了代理人工智能优化（AAIO）作为确保网站和代理人工智能系统有效集成的重要方法。类似于搜索引擎优化（SEO）塑造了数字内容的可发现性，AAIO 可以定义自主人工智能代理与在线平台之间的交互。通过分析网站优化与代理人工智能成功之间的互依关系，本文强调了AAIO 可以创造的良性循环。进一步探讨了AAIO 的治理、伦理、法律和社会影响（GELSI），强调了需要前瞻性的监管框架来减轻潜在的负面影响。文章最后肯定了AAIO 在自主数字代理时代作为基础数字基础设施的重要作用，倡导其利益的公平和包容性访问。', 'title_zh': '代理人工智能优化（AAIO）：什么是代理人工智能优化，它如何运作，为何重要，以及如何应对'}
{'arxiv_id': 'arXiv:2504.12417', 'title': 'Interpretable AI-driven Guidelines for Type 2 Diabetes Treatment from Observational Data', 'authors': 'Dewang Kumar Agarwal, Dimitris J. Bertsimas', 'link': 'https://arxiv.org/abs/2504.12417', 'abstract': "Objective: Create precise, structured, data-backed guidelines for type 2 diabetes treatment progression, suitable for clinical adoption.\nResearch Design and Methods: Our training cohort was composed of patient (with type 2 diabetes) visits from Boston Medical Center (BMC) from 1998 to 2014. We divide visits into 4 groups based on the patient's treatment regimen before the visit, and further divide them into subgroups based on the recommended treatment during the visit. Since each subgroup has observational data, which has confounding bias (sicker patients are prescribed more aggressive treatments), we used machine learning and optimization to remove some datapoints so that the remaining data resembles a randomized trial. On each subgroup, we train AI-backed tree-based models to prescribe treatment changes. Once we train these tree models, we manually combine the models for every group to create an end-to-end prescription pipeline for all patients in that group. In this process, we prioritize stepping up to a more aggressive treatment before considering less aggressive options. We tested this pipeline on unseen data from BMC, and an external dataset from Hartford healthcare (type 2 diabetes patient visits from January 2020 to May 2024).\nResults: The median HbA1c reduction achieved by our pipelines is 0.26% more than what the doctors achieved on the unseen BMC patients. For the Hartford cohort, our pipelines were better by 0.13%.\nConclusions: This precise, interpretable, and efficient AI-backed approach to treatment progression in type 2 diabetes is predicted to outperform the current practice and can be deployed to improve patient outcomes.", 'abstract_zh': '研究目标：创建基于精确、结构化和数据支持的2型糖尿病治疗进展指南，适合临床应用。', 'title_zh': '基于观察数据的可解释人工智能驱动的2型糖尿病治疗指南'}
{'arxiv_id': 'arXiv:2504.13173', 'title': "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization", 'authors': 'Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, Vahab Mirrokni', 'link': 'https://arxiv.org/abs/2504.13173', 'abstract': 'Designing efficient and effective architectural backbones has been in the core of research efforts to enhance the capability of foundation models. Inspired by the human cognitive phenomenon of attentional bias-the natural tendency to prioritize certain events or stimuli-we reconceptualize neural architectures, including Transformers, Titans, and modern linear recurrent neural networks as associative memory modules that learn a mapping of keys and values using an internal objective, referred to as attentional bias. Surprisingly, we observed that most existing sequence models leverage either (1) dot-product similarity, or (2) L2 regression objectives as their attentional bias. Going beyond these objectives, we present a set of alternative attentional bias configurations along with their effective approximations to stabilize their training procedure. We then reinterpret forgetting mechanisms in modern deep learning architectures as a form of retention regularization, providing a novel set of forget gates for sequence models. Building upon these insights, we present Miras, a general framework to design deep learning architectures based on four choices of: (i) associative memory architecture, (ii) attentional bias objective, (iii) retention gate, and (iv) memory learning algorithm. We present three novel sequence models-Moneta, Yaad, and Memora-that go beyond the power of existing linear RNNs while maintaining a fast parallelizable training process. Our experiments show different design choices in Miras yield models with varying strengths. For example, certain instances of Miras achieve exceptional performance in special tasks such as language modeling, commonsense reasoning, and recall intensive tasks, even outperforming Transformers and other modern linear recurrent models.', 'abstract_zh': '设计高效的认知偏置架构背骨一直是提升基础模型能力研究的核心。受人类认知现象中注意偏向的启发，我们重新概念化包括变换器、泰坦和现代线性递归神经网络在内的神经架构，将其视为基于内部目标（即注意偏向）学习键值映射的关联记忆模块。令人惊讶的是，我们发现大多数现有序列模型要么使用点积相似性，要么使用L2回归目标作为其注意偏向。超越这些目标，我们提出了几种替代的注意偏向配置及其有效的逼近方法，以稳定训练过程。接着，我们将现代深度学习架构中的遗忘机制重新解释为保持正则化的一种形式，并为序列模型提供了一组新型的遗忘门控。基于这些洞见，我们提出了Miras，一种基于四种选择设计深度学习架构的一般框架：（i）关联记忆架构，（ii）注意偏置目标，（iii）保持门控，和（iv）记忆学习算法。我们介绍了三种新颖的序列模型——Moneta、Yaad和Memora，这些模型超越了现有线性递归神经网络的能力，同时保持了快速并行训练过程。我们的实验表明，Miras中的不同设计选择会导致具有不同优势的模型。例如，Miras的某些实例在特定任务（如语言建模、常识推理和检索密集任务）上取得了卓越的性能，甚至超过了变换器和其他现代线性递归模型。', 'title_zh': '一切皆相连：从测试时记忆、注意力偏差、保持能力到在线优化的探究之旅'}
{'arxiv_id': 'arXiv:2504.13151', 'title': 'MIB: A Mechanistic Interpretability Benchmark', 'authors': 'Aaron Mueller, Atticus Geiger, Sarah Wiegreffe, Dana Arad, Iván Arcuschin, Adam Belfki, Yik Siu Chan, Jaden Fiotto-Kaufman, Tal Haklay, Michael Hanna, Jing Huang, Rohan Gupta, Yaniv Nikankin, Hadas Orgad, Nikhil Prakash, Anja Reusch, Aruna Sankaranarayanan, Shun Shao, Alessandro Stolfo, Martin Tutek, Amir Zur, David Bau, Yonatan Belinkov', 'link': 'https://arxiv.org/abs/2504.13151', 'abstract': 'How can we know whether new mechanistic interpretability methods achieve real improvements? In pursuit of meaningful and lasting evaluation standards, we propose MIB, a benchmark with two tracks spanning four tasks and five models. MIB favors methods that precisely and concisely recover relevant causal pathways or specific causal variables in neural language models. The circuit localization track compares methods that locate the model components - and connections between them - most important for performing a task (e.g., attribution patching or information flow routes). The causal variable localization track compares methods that featurize a hidden vector, e.g., sparse autoencoders (SAEs) or distributed alignment search (DAS), and locate model features for a causal variable relevant to the task. Using MIB, we find that attribution and mask optimization methods perform best on circuit localization. For causal variable localization, we find that the supervised DAS method performs best, while SAE features are not better than neurons, i.e., standard dimensions of hidden vectors. These findings illustrate that MIB enables meaningful comparisons of methods, and increases our confidence that there has been real progress in the field.', 'abstract_zh': '如何知道新的机制可解释性方法是否实现了真正的改进？为了追求有意义且持久的评估标准，我们提出MIB基准，该基准涵盖四个任务和五种模型，分为两条赛道。MIB倾向于那些能够精确且简洁地恢复神经语言模型中相关因果路径或特定因果变量的方法。电路定位赛道比较了在执行特定任务时识别出最重要模型组件及其连接的方法（例如归因补丁或信息流路径）。因果变量定位赛道比较了能够特征化隐藏向量、例如稀疏自编码器（SAEs）或分布式对齐搜索（DAS），并定位与任务相关的因果变量模型特征的方法。使用MIB，我们发现归因和掩码优化方法在电路定位表现最佳。对于因果变量定位，我们发现监督DAS方法表现最佳，而SAE特征并不优于神经元，即隐藏向量的标准维度。这些发现表明，MIB能够进行有意义的方法对比，并增强了我们对领域确实在进步的信心。', 'title_zh': 'MIB: 机制可解释性基准'}
{'arxiv_id': 'arXiv:2504.13128', 'title': 'FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents', 'authors': 'Nandan Thakur, Jimmy Lin, Sam Havens, Michael Carbin, Omar Khattab, Andrew Drozdov', 'link': 'https://arxiv.org/abs/2504.13128', 'abstract': 'We introduce FreshStack, a reusable framework for automatically building information retrieval (IR) evaluation benchmarks from community-asked questions and answers. FreshStack conducts the following steps: (1) automatic corpus collection from code and technical documentation, (2) nugget generation from community-asked questions and answers, and (3) nugget-level support, retrieving documents using a fusion of retrieval techniques and hybrid architectures. We use FreshStack to build five datasets on fast-growing, recent, and niche topics to ensure the tasks are sufficiently challenging. On FreshStack, existing retrieval models, when applied out-of-the-box, significantly underperform oracle approaches on all five topics, denoting plenty of headroom to improve IR quality. In addition, we identify cases where rerankers do not clearly improve first-stage retrieval accuracy (two out of five topics). We hope that FreshStack will facilitate future work toward constructing realistic, scalable, and uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are available at: this https URL.', 'abstract_zh': '我们引入了FreshStack，一个可重复使用的框架，用于从社区提出的问答自动构建信息检索（IR）评价基准。FreshStack包括以下步骤：(1) 从代码和技术文档自动收集语料库，(2) 从社区提出的问答生成关键信息片段，以及(3) 在融合检索技术和混合架构的基础上在关键信息片段层面进行文档检索。我们使用FreshStack构建了五个针对快速增长、近期和专门领域的话题数据集，以确保任务具有足够的挑战性。在FreshStack上，现有的检索模型在所有五个话题上与奥卡姆方法相比显著表现不佳，表明提升信息检索质量的空间很大。此外，我们发现有两个话题中无需重新排序器就能显着提高初步检索准确性。我们希望FreshStack能够促进未来构建现实的、可扩展的和未受污染的IR和RAG评价基准的工作。FreshStack数据集可在以下网址获取：this https URL。', 'title_zh': 'FreshStack: 构建评估技术文档检索的现实基准'}
{'arxiv_id': 'arXiv:2504.13120', 'title': 'Probing and Inducing Combinational Creativity in Vision-Language Models', 'authors': 'Yongqian Peng, Yuxi Ma, Mengmeng Wang, Yuxuan Wang, Yizhou Wang, Chi Zhang, Yixin Zhu, Zilong Zheng', 'link': 'https://arxiv.org/abs/2504.13120', 'abstract': 'The ability to combine existing concepts into novel ideas stands as a fundamental hallmark of human intelligence. Recent advances in Vision-Language Models (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their outputs reflect combinational creativity--defined by M. A. Boden (1998) as synthesizing novel ideas through combining existing concepts--or sophisticated pattern matching of training data. Drawing inspiration from cognitive science, we investigate the combinational creativity of VLMs from the lens of concept blending. We propose the Identification-Explanation-Implication (IEI) framework, which decomposes creative processes into three levels: identifying input spaces, extracting shared attributes, and deriving novel semantic implications. To validate this framework, we curate CreativeMashup, a high-quality dataset of 666 artist-generated visual mashups annotated according to the IEI framework. Through extensive experiments, we demonstrate that in comprehension tasks, best VLMs have surpassed average human performance while falling short of expert-level understanding; in generation tasks, incorporating our IEI framework into the generation pipeline significantly enhances the creative quality of VLMs outputs. Our findings establish both a theoretical foundation for evaluating artificial creativity and practical guidelines for improving creative generation in VLMs.', 'abstract_zh': '具备将现有概念组合成新颖想法的能力是人类智能的基本特征。最近，在视觉-语言模型（VLMs）领域如GPT-4V和DALLE-3的进展引发了对其输出是否反映了组合创造力的辩论——组合创造力被M. A. Boden（1998）定义为通过组合现有概念来合成新颖想法——或仅仅是训练数据中的复杂模式匹配。借鉴认知科学的视角，我们从概念融合的视角探讨了VLMs的组合创造力。我们提出了识别-解释-隐含（IEI）框架，将创造过程分解为三个层面：识别输入空间、提取共享属性以及推导新颖的语义隐含。为了验证这一框架，我们编纂了CreativeMashup数据集，包含666个由艺术家生成的视觉混搭作品，并按照IEI框架进行了注释。通过大量实验，我们证明，在理解任务中，最佳VLMs已经超过了平均水平的人类表现，但在专家级理解方面仍未达到专家水平；在生成任务中，将我们的IEI框架整合到生成流水线中显著提高了VLMs输出的创造性质量。我们的研究结果不仅为评估人工创造力奠定了理论基础，也为提高VLMs的创造性生成提供了实用指南。', 'title_zh': '探究和诱导视觉语言模型的组合创造力'}
{'arxiv_id': 'arXiv:2504.13102', 'title': 'A Multi-task Learning Balanced Attention Convolutional Neural Network Model for Few-shot Underwater Acoustic Target Recognition', 'authors': 'Wei Huang, Shumeng Sun, Junpeng Lu, Zhenpeng Xu, Zhengyang Xiu, Hao Zhang', 'link': 'https://arxiv.org/abs/2504.13102', 'abstract': 'Underwater acoustic target recognition (UATR) is of great significance for the protection of marine diversity and national defense security. The development of deep learning provides new opportunities for UATR, but faces challenges brought by the scarcity of reference samples and complex environmental interference. To address these issues, we proposes a multi-task balanced channel attention convolutional neural network (MT-BCA-CNN). The method integrates a channel attention mechanism with a multi-task learning strategy, constructing a shared feature extractor and multi-task classifiers to jointly optimize target classification and feature reconstruction tasks. The channel attention mechanism dynamically enhances discriminative acoustic features such as harmonic structures while suppressing noise. Experiments on the Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97\\% classification accuracy and 95\\% $F1$-score in 27-class few-shot scenarios, significantly outperforming traditional CNN and ACNN models, as well as popular state-of-the-art UATR methods. Ablation studies confirm the synergistic benefits of multi-task learning and attention mechanisms, while a dynamic weighting adjustment strategy effectively balances task contributions. This work provides an efficient solution for few-shot underwater acoustic recognition, advancing research in marine bioacoustics and sonar signal processing.', 'abstract_zh': '水下声学目标识别（UATR）在保护海洋生物多样性和国家安全方面具有重要意义。深度学习的发展为UATR提供了新机遇，但也面临着参考样本稀缺和复杂环境干扰的挑战。为了应对这些问题，我们提出了一种多任务平衡通道注意卷积神经网络（MT-BCA-CNN）。该方法结合了通道注意机制和多任务学习策略，构建了共享特征提取器和多任务分类器，同时优化目标分类和特征重构任务。通道注意机制动态增强具有辨别力的声学特征，如谐波结构，并抑制噪声。实验表明，MT-BCA-CNN在Watkins海洋生物数据集的27类少样本场景中实现了97%的分类准确率和95%的$F1$分数，明显优于传统CNN和ACNN模型以及现有的UATR方法。消融研究证实了多任务学习和注意力机制的协同效益，而动态权重调整策略有效平衡了任务贡献。这项工作为水下声学少样本识别提供了高效解决方案，促进了海洋生物声学和声纳信号处理的研究。', 'title_zh': '多任务学习平衡注意力卷积神经网络模型在少量样本水下声目标识别中应用'}
{'arxiv_id': 'arXiv:2504.13101', 'title': 'An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research', 'authors': 'Patrik Reizinger, Randall Balestriero, David Klindt, Wieland Brendel', 'link': 'https://arxiv.org/abs/2504.13101', 'abstract': "Self-Supervised Learning (SSL) powers many current AI systems. As research interest and investment grow, the SSL design space continues to expand. The Platonic view of SSL, following the Platonic Representation Hypothesis (PRH), suggests that despite different methods and engineering approaches, all representations converge to the same Platonic ideal. However, this phenomenon lacks precise theoretical explanation. By synthesizing evidence from Identifiability Theory (IT), we show that the PRH can emerge in SSL. However, current IT cannot explain SSL's empirical success. To bridge the gap between theory and practice, we propose expanding IT into what we term Singular Identifiability Theory (SITh), a broader theoretical framework encompassing the entire SSL pipeline. SITh would allow deeper insights into the implicit data assumptions in SSL and advance the field towards learning more interpretable and generalizable representations. We highlight three critical directions for future research: 1) training dynamics and convergence properties of SSL; 2) the impact of finite samples, batch size, and data diversity; and 3) the role of inductive biases in architecture, augmentations, initialization schemes, and optimizers.", 'abstract_zh': '自主学习（SSL）驱动了许多当前的AI系统。随着研究兴趣和投资的增加，SSL的设计空间继续扩展。柏拉图视角下的SSL，遵循柏拉图表示假设（PRH），表明尽管有不同的方法和工程手段，所有表示最终会趋向于同一个柏拉图理想。然而，这一现象缺乏精确的理论解释。通过综合可识别性理论（IT）的证据，我们表明PRH可以在SSL中出现。然而，当前的IT无法解释SSL的经验成功。为了弥合理论与实践之间的差距，我们建议将IT扩展为我们称之为单一可识别性理论（SITh）的更广泛的理论框架，涵盖整个SSL管道。SITh将有助于更深入地了解SSL中的隐含数据假设，并推动该领域朝着学习更具可解释性和泛化能力的表示前进。我们指出了未来研究的三个关键方向：1）SSL的训练动力学和收敛性质；2）有限样本、批次大小和数据多样性的影响；3）架构、增强、初始化方案和优化器中的归纳偏见的作用。', 'title_zh': '基于经验的可辨识性理论将加速自我监督学习研究'}
{'arxiv_id': 'arXiv:2504.13054', 'title': 'Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation', 'authors': 'Yichao Feng, Shuai Zhao, Yueqiu Li, Luwei Xiao, Xiaobao Wu, Anh Tuan Luu', 'link': 'https://arxiv.org/abs/2504.13054', 'abstract': 'Aspect-based summarization aims to generate summaries tailored to specific aspects, addressing the resource constraints and limited generalizability of traditional summarization approaches. Recently, large language models have shown promise in this task without the need for training. However, they rely excessively on prompt engineering and face token limits and hallucination challenges, especially with in-context learning. To address these challenges, in this paper, we propose a novel framework for aspect-based summarization: Self-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely on in-context learning, given an aspect, we employ an embedding-driven retrieval mechanism to identify its relevant text segments. This approach extracts the pertinent content while avoiding unnecessary details, thereby mitigating the challenge of token limits. Moreover, our framework optimizes token usage by deleting unrelated parts of the text and ensuring that the model generates output strictly based on the given aspect. With extensive experiments on benchmark datasets, we demonstrate that our framework not only achieves superior performance but also effectively mitigates the token limitation problem.', 'abstract_zh': '基于方面摘要生成旨在生成针对特定方面定制的摘要，以解决传统摘要方法在资源约束和泛化能力上的局限性。近期，大规模语言模型在该任务上显示出潜力，无需进行专门训练即可完成。然而，它们过度依赖于提示工程技术，并面临标记限制和幻觉挑战，尤其是在上下文学习的情况下。为应对这些挑战，本文提出了一种新的基于方面摘要生成框架：Self-Aspect Retrieval Enhanced Summary Generation。该框架不仅依赖于上下文学习，还采用嵌入驱动的检索机制，根据给定的方面识别其相关文本片段。这种方法提取关键内容的同时避免冗余细节，从而缓解了标记限制挑战。此外，该框架通过删除与给定方面无关的部分来优化标记使用，并确保模型严格基于给定方面生成输出。通过在基准数据集上进行广泛实验，我们证明了该框架不仅能够实现更好的性能，还能有效地缓解标记限制问题。', 'title_zh': '基于自方面检索增强生成的方面级摘要生成'}
{'arxiv_id': 'arXiv:2504.13048', 'title': 'Design Topological Materials by Reinforcement Fine-Tuned Generative Model', 'authors': 'Haosheng Xu, Dongheng Qian, Zhixuan Liu, Yadong Jiang, Jing Wang', 'link': 'https://arxiv.org/abs/2504.13048', 'abstract': "Topological insulators (TIs) and topological crystalline insulators (TCIs) are materials with unconventional electronic properties, making their discovery highly valuable for practical applications. However, such materials, particularly those with a full band gap, remain scarce. Given the limitations of traditional approaches that scan known materials for candidates, we focus on the generation of new topological materials through a generative model. Specifically, we apply reinforcement fine-tuning (ReFT) to a pre-trained generative model, thereby aligning the model's objectives with our material design goals. We demonstrate that ReFT is effective in enhancing the model's ability to generate TIs and TCIs, with minimal compromise on the stability of the generated materials. Using the fine-tuned model, we successfully identify a large number of new topological materials, with Ge$_2$Bi$_2$O$_6$ serving as a representative example--a TI with a full band gap of 0.26 eV, ranking among the largest known in this category.", 'abstract_zh': '拓扑绝缘体（TIs）和拓扑晶体绝缘体（TCIs）是具有非常规电子性质的材料，其发现对于实际应用具有高度的价值。然而，尤其是全带隙的这类材料仍然相对稀缺。鉴于传统方法在已知材料中寻找候选材料的局限性，我们着眼于通过生成模型来生成新的拓扑材料。具体地，我们应用强化微调（ReFT）到预训练的生成模型中，从而使模型的目标与我们的材料设计目标保持一致。我们证明ReFT在增强模型生成TIs和TCIs的能力方面是有效的，并且对生成材料的稳定性影响 minimal。使用微调后的模型，我们成功地识别出大量的新拓扑材料，其中Ge$_2$Bi$_2$O$_6$作为代表性例子——是一种具有0.26 eV全带隙的TIs，位列此类已知的最大值之一。', 'title_zh': '通过强化细调生成模型设计拓扑材料'}
{'arxiv_id': 'arXiv:2504.12977', 'title': "A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology", 'authors': 'Maksim Vishnevskiy', 'link': 'https://arxiv.org/abs/2504.12977', 'abstract': "This paper presents a novel research analytical IT system grounded in Martin Heidegger's Fundamental Ontology, distinguishing between beings (das Seiende) and Being (das Sein). The system employs two modally distinct, descriptively complete languages: a categorical language of beings for processing user inputs and an existential language of Being for internal analysis. These languages are bridged via a phenomenological reduction module, enabling the system to analyze user queries (including questions, answers, and dialogues among IT specialists), identify recursive and self-referential structures, and provide actionable insights in categorical terms. Unlike contemporary systems limited to categorical analysis, this approach leverages Heidegger's phenomenological existential analysis to uncover deeper ontological patterns in query processing, aiding in resolving logical traps in complex interactions, such as metaphor usage in IT contexts. The path to full realization involves formalizing the language of Being by a research team based on Heidegger's Fundamental Ontology; given the existing completeness of the language of beings, this reduces the system's computability to completeness, paving the way for a universal query analysis tool. The paper presents the system's architecture, operational principles, technical implementation, use cases--including a case based on real IT specialist dialogues--comparative evaluation with existing tools, and its advantages and limitations.", 'abstract_zh': '本文基于马丁·海德格尔的基本 ontology，提出了一种新颖的研究分析 IT 系统，区分了存在者（das Seiende）和存在（das Sein）。该系统采用两种模态上不同的、描述性完整语言：一种是关于存在者的分类语言，用于处理用户输入；另一种是关于存在的存在语言，用于内部分析。通过现象学还原模块将这两种语言相连接，使系统能够分析用户查询（包括问题、答案和 IT 专家之间的对话），识别递归和自指结构，并以分类术语提供可操作的洞见。与仅限于分类分析的当代系统不同，该方法利用海德格尔的现象学存在分析来揭示查询处理中的更深层次本体模式，有助于在复杂交互中解决逻辑陷阱，例如在 IT 上文中的比喻使用。实现完整性的路径涉及基于海德格尔的基本 ontology 由研究团队形式化存在的语言；鉴于存在者的语言已具备完整性，这将系统的可计算性简化为完整性，从而为普遍查询分析工具铺平道路。本文介绍了该系统的架构、运作原则、技术实现、应用场景（包括基于真实 IT 专家对话的案例）、现有工具的比较评估，以及其优势和局限性。', 'title_zh': '使用海德格尔的基本本体论分析信息技术系统中用户查询的现象学方法'}
{'arxiv_id': 'arXiv:2504.12891', 'title': 'Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication', 'authors': 'Vicent Briva-Iglesias', 'link': 'https://arxiv.org/abs/2504.12891', 'abstract': 'The rapid evolution of artificial intelligence (AI) has introduced AI agents as a disruptive paradigm across various industries, yet their application in machine translation (MT) remains underexplored. This paper describes and analyses the potential of single- and multi-agent systems for MT, reflecting on how they could enhance multilingual digital communication. While single-agent systems are well-suited for simpler translation tasks, multi-agent systems, which involve multiple specialized AI agents collaborating in a structured manner, may offer a promising solution for complex scenarios requiring high accuracy, domain-specific knowledge, and contextual awareness. To demonstrate the feasibility of multi-agent workflows in MT, we are conducting a pilot study in legal MT. The study employs a multi-agent system involving four specialized AI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and (iv) final editing. Our findings suggest that multi-agent systems may have the potential to significantly improve domain-adaptability and contextual awareness, with superior translation quality to traditional MT or single-agent systems. This paper also sets the stage for future research into multi-agent applications in MT, integration into professional translation workflows, and shares a demo of the system analyzed in the paper.', 'abstract_zh': '人工智能（AI）的快速演进引入了AI代理作为各行各业的游戏规则改变者，但在机器翻译（MT）中的应用仍未被充分探索。本文描述并分析了单个和多代理系统在MT中的潜力，探讨了它们如何提升多语言数字通信的效果。虽然单代理系统适用于简单的翻译任务，但涉及多个专业AI代理以结构化方式协作的多代理系统可能为需要高准确度、领域特定知识和上下文意识的复杂场景提供有力解决方案。为了证明多代理工作流程在MT中的可行性，我们正在进行一项针对法律MT的试点研究。该研究采用了一个涉及四个人工智能代理的多代理系统，分别是（i）翻译，（ii） adequacy审查，（iii）流畅性审查，以及（iv）最终编辑。我们的研究发现表明，多代理系统可能在增强领域适应性和上下文意识方面具有巨大潜力，并且具有比传统MT或单代理系统更高的翻译质量。本文还为多代理应用在MT中的未来研究、整合到专业翻译工作流中奠定了基础，并分享了本文分析的系统演示。', 'title_zh': 'AI代理是否代表了机器翻译的新前沿？单Agent与多Agent系统在多语言数字通信中的挑战与机遇'}
{'arxiv_id': 'arXiv:2504.12841', 'title': 'ALT: A Python Package for Lightweight Feature Representation in Time Series Classification', 'authors': 'Balázs P. Halmos, Balázs Hajós, Vince Á. Molnár, Marcell T. Kurbucz, Antal Jakovác', 'link': 'https://arxiv.org/abs/2504.12841', 'abstract': 'We introduce ALT, an open-source Python package created for efficient and accurate time series classification (TSC). The package implements the adaptive law-based transformation (ALT) algorithm, which transforms raw time series data into a linearly separable feature space using variable-length shifted time windows. This adaptive approach enhances its predecessor, the linear law-based transformation (LLT), by effectively capturing patterns of varying temporal scales. The software is implemented for scalability, interpretability, and ease of use, achieving state-of-the-art performance with minimal computational overhead. Extensive benchmarking on real-world datasets demonstrates the utility of ALT for diverse TSC tasks in physics and related domains.', 'abstract_zh': '我们介绍了ALT，一个用于高效准确时间序列分类的开源Python包。该包实现了基于自适应法则的转换（ALT）算法，通过可变长度时间滑窗将原始时间序列数据转换至线性可分特征空间。这一自适应方法提高了其前身线性法则转换（LLT）算法，有效捕捉不同时间尺度的模式。该软件在可扩展性、可解释性和易用性方面进行了实现，以最小的计算开销达到最先进的性能。在实际数据集上的广泛基准测试展示了ALT在物理及相关领域各种时间序列分类任务中的实用价值。', 'title_zh': 'ALT: 一个轻量级时间序列分类特征表示的Python包'}
{'arxiv_id': 'arXiv:2504.12806', 'title': 'A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks', 'authors': 'Georgios Papadopoulos, Shaltiel Eloul, Yash Satsangi, Jamie Heredge, Niraj Kumar, Chun-Fu Chen, Marco Pistoia', 'link': 'https://arxiv.org/abs/2504.12806', 'abstract': "The loss landscape of Variational Quantum Neural Networks (VQNNs) is characterized by local minima that grow exponentially with increasing qubits. Because of this, it is more challenging to recover information from model gradients during training compared to classical Neural Networks (NNs). In this paper we present a numerical scheme that successfully reconstructs input training, real-world, practical data from trainable VQNNs' gradients. Our scheme is based on gradient inversion that works by combining gradients estimation with the finite difference method and adaptive low-pass filtering. The scheme is further optimized with Kalman filter to obtain efficient convergence. Our experiments show that our algorithm can invert even batch-trained data, given the VQNN model is sufficiently over-parameterized.", 'abstract_zh': 'Variational Quantum Neural Networks的损失景观特征在于随量子比特增加呈指数增长的局部最小值，这使得在训练过程中从模型梯度中恢复输入训练、实际世界的数据比经典神经网络更具挑战性。本文提出一种数值方案，能够成功从可训练的变量子神经网络(VQNN)的梯度中重构输入训练数据和实际世界数据。该方案基于梯度反转，结合梯度估计、有限差分方法和自适应低通滤波。进一步使用卡尔曼滤波器优化以实现高效的收敛。实验表明，当VQNN模型足够过参数化时，该算法即使对批量训练数据也能实现反转。', 'title_zh': '变分量子神经网络中的数值梯度逆向攻击'}
{'arxiv_id': 'arXiv:2504.12803', 'title': 'Enhancing Explainability and Reliable Decision-Making in Particle Swarm Optimization through Communication Topologies', 'authors': 'Nitin Gupta, Indu Bala, Bapi Dutta, Luis Martínez, Anupam Yadav', 'link': 'https://arxiv.org/abs/2504.12803', 'abstract': "Swarm intelligence effectively optimizes complex systems across fields like engineering and healthcare, yet algorithm solutions often suffer from low reliability due to unclear configurations and hyperparameters. This study analyzes Particle Swarm Optimization (PSO), focusing on how different communication topologies Ring, Star, and Von Neumann affect convergence and search behaviors. Using an adapted IOHxplainer , an explainable benchmarking tool, we investigate how these topologies influence information flow, diversity, and convergence speed, clarifying the balance between exploration and exploitation. Through visualization and statistical analysis, the research enhances interpretability of PSO's decisions and provides practical guidelines for choosing suitable topologies for specific optimization tasks. Ultimately, this contributes to making swarm based optimization more transparent, robust, and trustworthy.", 'abstract_zh': 'swarmintelligence在工程和医疗等领域有效地优化复杂系统，但由于配置和超参数不明确，算法解决方案往往可靠性较低。本研究分析了粒子群优化（PSO），关注环形、星形和冯·诺伊曼不同通信拓扑如何影响收敛性和搜索行为。通过使用可解释的基准工具IOHxplainer，研究探讨了这些拓扑如何影响信息流、多样性和收敛速度，澄清了探索与利用之间的平衡。通过可视化和统计分析，研究增强了PSO决策的可解释性，并提供了为特定优化任务选择合适拓扑的具体指导。最终，这有助于使基于群智的优化变得更加透明、稳健和可信。', 'title_zh': '通过通信拓扑提升粒子群优化的可解释性和可靠决策能力'}
{'arxiv_id': 'arXiv:2504.12778', 'title': 'Towards Lossless Token Pruning in Late-Interaction Retrieval Models', 'authors': 'Yuxuan Zong, Benjamin Piwowarski', 'link': 'https://arxiv.org/abs/2504.12778', 'abstract': "Late interaction neural IR models like ColBERT offer a competitive effectiveness-efficiency trade-off across many benchmarks. However, they require a huge memory space to store the contextual representation for all the document tokens. Some works have proposed using either heuristics or statistical-based techniques to prune tokens from each document. This however doesn't guarantee that the removed tokens have no impact on the retrieval score. Our work uses a principled approach to define how to prune tokens without impacting the score between a document and a query. We introduce three regularization losses, that induce a solution with high pruning ratios, as well as two pruning strategies. We study them experimentally (in and out-domain), showing that we can preserve ColBERT's performance while using only 30\\% of the tokens.", 'abstract_zh': 'Late交互神经IR模型如ColBERT在许多基准测试中提供了竞争力的效果-效率折衷。然而，它们需要巨大的内存空间来存储所有文档词元的上下文表示。一些工作提出了使用启发式方法或统计技术从每个文档中修剪词元。但这并不能保证删除的词元不会影响检索得分。我们的工作采用了一个原则性的方法，定义了如何修剪词元而不影响文档和查询之间的得分。我们引入了三种正则化损失，诱导了高修剪比例的解，以及两种修剪策略。我们在实验中研究了它们（领域内和跨领域），展示了可以保留ColBERT的性能同时仅使用30%的词元。', 'title_zh': '向晚交互检索模型中无损令牌剪枝迈进'}
{'arxiv_id': 'arXiv:2504.12777', 'title': 'Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis', 'authors': 'James Rudd-Jones, Mirco Musolesi, María Pérez-Ortiz', 'link': 'https://arxiv.org/abs/2504.12777', 'abstract': 'Climate policy development faces significant challenges due to deep uncertainty, complex system dynamics, and competing stakeholder interests. Climate simulation methods, such as Earth System Models, have become valuable tools for policy exploration. However, their typical use is for evaluating potential polices, rather than directly synthesizing them. The problem can be inverted to optimize for policy pathways, but the traditional optimization approaches often struggle with non-linear dynamics, heterogeneous agents, and comprehensive uncertainty quantification. We propose a framework for augmenting climate simulations with Multi-Agent Reinforcement Learning (MARL) to address these limitations. We identify key challenges at the interface between climate simulations and the application of MARL in the context of policy synthesis, including reward definition, scalability with increasing agents and state spaces, uncertainty propagation across linked systems, and solution validation. Additionally, we discuss challenges in making MARL-derived solutions interpretable and useful for policy-makers. Our framework provides a foundation for more sophisticated climate policy exploration while acknowledging important limitations and areas for future research.', 'abstract_zh': '气候政策制定面临着深不确定性的重大挑战、复杂系统动力学以及利益相关方的矛盾利益。气候模拟方法，如地球系统模型，已成为政策探索的重要工具。然而，它们通常用于评估潜在政策，而非直接综合生成政策。该问题可以反转为优化政策路径，但传统的优化方法往往难以处理非线性动力学、异质代理和全面的不确定性量化。我们提出了一种利用多智能体强化学习（MARL）增强气候模拟的框架，以解决这些局限性。我们在气候模拟与MARL应用于政策综合的接口处识别了几个关键挑战，包括奖励定义、随着代理和状态空间增加的可扩展性、链接系统中的不确定性传播以及解决方案验证。此外，我们讨论了如何使从MARL推导出的解决方案可解释并适用于政策制定者。该框架为更复杂的气候政策探索提供了基础，同时承认了重要的限制和未来研究的领域。', 'title_zh': '多agent强化学习仿真用于环境政策合成'}
{'arxiv_id': 'arXiv:2504.12757', 'title': 'MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System', 'authors': 'Sonu Kumar, Anubhav Girdhar, Ritesh Patil, Divyansh Tripathi', 'link': 'https://arxiv.org/abs/2504.12757', 'abstract': 'As Agentic AI gain mainstream adoption, the industry invests heavily in model capabilities, achieving rapid leaps in reasoning and quality. However, these systems remain largely confined to data silos, and each new integration requires custom logic that is difficult to scale. The Model Context Protocol (MCP) addresses this challenge by defining a universal, open standard for securely connecting AI-based applications (MCP clients) to data sources (MCP servers). However, the flexibility of the MCP introduces new risks, including malicious tool servers and compromised data integrity. We present MCP Guardian, a framework that strengthens MCP-based communication with authentication, rate-limiting, logging, tracing, and Web Application Firewall (WAF) scanning. Through real-world scenarios and empirical testing, we demonstrate how MCP Guardian effectively mitigates attacks and ensures robust oversight with minimal overheads. Our approach fosters secure, scalable data access for AI assistants, underscoring the importance of a defense-in-depth approach that enables safer and more transparent innovation in AI-driven environments.', 'abstract_zh': '随着代理型AI获得主流采用，行业在模型能力上投入大量资源，实现了推理和质量的迅速提升。然而，这些系统仍然主要局限于数据孤岛，每次新的整合都需要定制逻辑，难以规模化。模型上下文协议（MCP）通过定义一种安全连接基于AI的应用程序（MCP客户端）和数据源（MCP服务器）的通用开放标准来应对这一挑战。然而，MCP的灵活性引入了新的风险，包括恶意工具服务器和数据完整性的受损。我们提出了MCP监护，一个通过身份认证、速率限制、日志记录、追踪和Web应用防火墙（WAF）扫描来增强MCP基础通信的框架。通过实际场景和实证测试，我们展示了MCP监护如何有效地缓解攻击，实现最小开销下的全面监督。我们的方法促进了安全、可扩展的AI助手数据访问，强调了多层次防御在AI驱动环境中实现更安全、更透明创新的重要性。', 'title_zh': 'MCP守护者：一种基于MCP的AI系统安全防护层'}
{'arxiv_id': 'arXiv:2504.12740', 'title': 'GPMFS: Global Foundation and Personalized Optimization for Multi-Label Feature Selection', 'authors': 'Yifan Cao, Zhilong Mi, Ziqiao Yin, Binghui Guo, Jin Dong', 'link': 'https://arxiv.org/abs/2504.12740', 'abstract': 'As artificial intelligence methods are increasingly applied to complex task scenarios, high dimensional multi-label learning has emerged as a prominent research focus. At present, the curse of dimensionality remains one of the major bottlenecks in high-dimensional multi-label learning, which can be effectively addressed through multi-label feature selection methods. However, existing multi-label feature selection methods mostly focus on identifying global features shared across all labels, which overlooks personalized characteristics and specific requirements of individual labels. This global-only perspective may limit the ability to capture label-specific discriminative information, thereby affecting overall performance. In this paper, we propose a novel method called GPMFS (Global Foundation and Personalized Optimization for Multi-Label Feature Selection). GPMFS firstly identifies global features by exploiting label correlations, then adaptively supplements each label with a personalized subset of discriminative features using a threshold-controlled strategy. Experiments on multiple real-world datasets demonstrate that GPMFS achieves superior performance while maintaining strong interpretability and robustness. Furthermore, GPMFS provides insights into the label-specific strength across different multi-label datasets, thereby demonstrating the necessity and potential applicability of personalized feature selection approaches.', 'abstract_zh': '基于全局基础与个性化优化的多标签特征选择方法（GPMFS）', 'title_zh': 'GPMFS：全局基础与个性化优化的多标签特征选择'}
{'arxiv_id': 'arXiv:2504.12735', 'title': 'The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems', 'authors': 'Lidong Zhai, Zhijie Qiu, Xizhong Guo, Jiaqi Li', 'link': 'https://arxiv.org/abs/2504.12735', 'abstract': 'This paper proposes the "Academy of Athens" multi-agent seven-layer framework, aimed at systematically addressing challenges in multi-agent systems (MAS) within artificial intelligence (AI) art creation, such as collaboration efficiency, role allocation, environmental adaptation, and task parallelism. The framework divides MAS into seven layers: multi-agent collaboration, single-agent multi-role playing, single-agent multi-scene traversal, single-agent multi-capability incarnation, different single agents using the same large model to achieve the same target agent, single-agent using different large models to achieve the same target agent, and multi-agent synthesis of the same target agent. Through experimental validation in art creation, the framework demonstrates its unique advantages in task collaboration, cross-scene adaptation, and model fusion. This paper further discusses current challenges such as collaboration mechanism optimization, model stability, and system security, proposing future exploration through technologies like meta-learning and federated learning. The framework provides a structured methodology for multi-agent collaboration in AI art creation and promotes innovative applications in the art field.', 'abstract_zh': '基于雅典学院的多代理七层框架：人工智能艺术创作中的多代理系统挑战系统化解决方法', 'title_zh': '雅典学院：多代理系统-seven层架构模型'}
{'arxiv_id': 'arXiv:2504.12721', 'title': 'TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations', 'authors': 'Yihang Lu, Yangyang Xu, Qitao Qing, Xianwei Meng', 'link': 'https://arxiv.org/abs/2504.12721', 'abstract': 'Recent deep learning models for Long-term Time Series Forecasting (LTSF) often emphasize complex, handcrafted designs, while simpler architectures like linear models or MLPs have often outperformed these intricate solutions. In this paper, we revisit and organize the core ideas behind several key techniques, such as redundancy reduction and multi-scale modeling, which are frequently employed in advanced LTSF models. Our goal is to streamline these ideas for more efficient deep learning utilization. To this end, we introduce TimeCapsule, a model built around the principle of high-dimensional information compression that unifies these techniques in a generalized yet simplified framework. Specifically, we model time series as a 3D tensor, incorporating temporal, variate, and level dimensions, and leverage mode production to capture multi-mode dependencies while achieving dimensionality compression. We propose an internal forecast within the compressed representation domain, supported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the learning of predictive representations. Extensive experiments on challenging benchmarks demonstrate the versatility of our method, showing that TimeCapsule can achieve state-of-the-art performance.', 'abstract_zh': '最近的长期时间序列 forecasting 深度学习模型往往强调复杂的手工艺品设计，而简单的架构如线性模型或MLPs常常表现出更优的效果。本文重新审视并组织了几种关键技术的核心思想，如冗余减少和多尺度建模，这些技术经常被高级长期时间序列 forecasting 模型所采用。我们的目标是为了更高效的深度学习应用简化这些思想。为此，我们提出 TimeCapsule 模型，该模型围绕高维信息压缩原则构建，将这些技术统一于一个通用且简化框架中。具体而言，我们将时间序列建模为3D张量，结合时间、变量和尺度维度，并利用模式生成来捕捉多模式依赖关系同时实现维数压缩。我们提出基于联合嵌入预测架构（JEPA）的支持，在压缩表示域内进行内部预测，以监控预测表示的学习。在具有挑战性的基准上的广泛实验表明，我们的方法具有灵活性，TimeCapsule 可以达到最先进的性能。', 'title_zh': 'TimeCapsule：通过压缩预测表示解决长期时间序列预测难题'}
{'arxiv_id': 'arXiv:2504.12673', 'title': 'ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models', 'authors': 'Singon Kim, Gunho Jung, Seong-Whan Lee', 'link': 'https://arxiv.org/abs/2504.12673', 'abstract': 'Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However,retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language modelbased compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy-reducing documents, making it highly useful in real-world scenarios.', 'abstract_zh': '基于抽取的压缩利用较小的语言模型压缩查询相关上下文，降低检索增强生成（RAG）中的计算成本。然而，检索到的文档有时包含与查询无关的信息，或者由于事实错误内容而导致误导，尽管具有高相关性得分。这种行为表明，基于抽取的压缩器更有可能遗漏对正确答案至关重要的信息，尤其是在长上下文中注意力分散的情况下。为了解决这个问题，我们对检索到的文档进行更细致的分类，并提出了抗噪声的基于抽取的压缩（ACoRN），引入了两个新的训练步骤。首先，我们对训练数据集进行离线数据增强，以增强压缩器对两种不同的检索噪声的鲁棒性。其次，由于基于语言模型的压缩器不能充分利用多个检索到的文档中的信息，并表现出位置偏差，我们进行微调以生成以支持正确答案的关键信息为中心的摘要。我们的实验表明，使用ACoRN作为压缩器训练的T5-large，在保持答案字符串的同时提高了EM和F1分数，这可以作为直接证据。ACoRN在包含许多降低准确性的文档的数据集上表现优异，使其在实际场景中非常有用。', 'title_zh': 'ACoRN: 在检索增强语言模型中具有噪声鲁棒性的抽象压缩'}
{'arxiv_id': 'arXiv:2504.12672', 'title': 'Post-processing improves accuracy of Artificial Intelligence weather forecasts', 'authors': 'Belinda Trotta, Robert Johnson, Catherine de Burgh-Day, Debra Hudson, Esteban Abellan, James Canvin, Andrew Kelly, Daniel Mentiplay, Benjamin Owen, Jennifer Whelan', 'link': 'https://arxiv.org/abs/2504.12672', 'abstract': "Artificial Intelligence (AI) weather models are now reaching operational-grade performance for some variables, but like traditional Numerical Weather Prediction (NWP) models, they exhibit systematic biases and reliability issues. We test the application of the Bureau of Meteorology's existing statistical post-processing system, IMPROVER, to ECMWF's deterministic Artificial Intelligence Forecasting System (AIFS), and compare results against post-processed outputs from the ECMWF HRES and ENS models. Without any modification to configuration or processing workflows, post-processing yields comparable accuracy improvements for AIFS as for traditional NWP forecasts, in both expected value and probabilistic outputs. We show that blending AIFS with NWP models improves overall forecast skill, even when AIFS alone is not the most accurate component. These findings show that statistical post-processing methods developed for NWP are directly applicable to AI models, enabling national meteorological centres to incorporate AI forecasts into existing workflows in a low-risk, incremental fashion.", 'abstract_zh': '人工智能天气模型现在在某些变量上达到了运营级性能，但像传统的数值天气预报模型一样，它们也表现出系统偏差和可靠性问题。我们测试了气象局现有统计后处理系统IMPROVER在欧洲中期天气预报中心（ECMWF）确定性人工智能预报系统（AIFS）中的应用，并将结果与ECMWF HRES和ENS模型经过后处理的输出进行比较。在不做任何配置或处理工作流修改的情况下，后处理为AIFS带来了与传统数值天气预报相近的准确度提高，无论是期望值还是概率输出。我们演示了将AIFS与数值天气预报模型结合使用可以提高总体预报技能，即使在单独使用时AIFS并非最准确的成分也是如此。这些发现表明，为数值天气预报开发的统计后处理方法可以直接应用于人工智能模型，使国家气象中心能够以低风险、渐进的方式将人工智能预报整合到现有工作流程中。', 'title_zh': '后处理提高人工神经网络天气预报的准确性'}
{'arxiv_id': 'arXiv:2504.12644', 'title': 'Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification', 'authors': 'Reek Majumder, Mashrur Chowdhury, Sakib Mahmud Khan, Zadid Khan, Fahim Ahmad, Frank Ngeni, Gurcan Comert, Judith Mwakalonge, Dimitra Michalaka', 'link': 'https://arxiv.org/abs/2504.12644', 'abstract': 'Deep learning (DL)-based image classification models are essential for autonomous vehicle (AV) perception modules since incorrect categorization might have severe repercussions. Adversarial attacks are widely studied cyberattacks that can lead DL models to predict inaccurate output, such as incorrectly classified traffic signs by the perception module of an autonomous vehicle. In this study, we create and compare hybrid classical-quantum deep learning (HCQ-DL) models with classical deep learning (C-DL) models to demonstrate robustness against adversarial attacks for perception modules. Before feeding them into the quantum system, we used transfer learning models, alexnet and vgg-16, as feature extractors. We tested over 1000 quantum circuits in our HCQ-DL models for projected gradient descent (PGD), fast gradient sign attack (FGSA), and gradient attack (GA), which are three well-known untargeted adversarial approaches. We evaluated the performance of all models during adversarial attacks and no-attack scenarios. Our HCQ-DL models maintain accuracy above 95\\% during a no-attack scenario and above 91\\% for GA and FGSA attacks, which is higher than C-DL models. During the PGD attack, our alexnet-based HCQ-DL model maintained an accuracy of 85\\% compared to C-DL models that achieved accuracies below 21\\%. Our results highlight that the HCQ-DL models provide improved accuracy for traffic sign classification under adversarial settings compared to their classical counterparts.', 'abstract_zh': '基于深度学习的图像分类模型对于自动驾驶车辆感知模块至关重要，因为错误分类可能导致严重后果。我们创建并比较了混合经典-量子深度学习（HCQ-DL）模型与经典深度学习（C-DL）模型，以展示其在感知模块中对抗敌对攻击的鲁棒性。在将这些模型输入量子系统之前，我们使用了转移学习模型alexnet和vgg-16作为特征提取器。我们在HCQ-DL模型中测试了超过1000个量子电路，针对三种广为人知的无目标敌对攻击方法（投影梯度下降法PGD、快速梯度符号攻击FGSA和梯度攻击GA）。我们在敌对攻击和无攻击情况下评估了所有模型的性能。在无攻击情况下，我们的HCQ-DL模型保持了95%以上的准确性，在FGSA和FGSA攻击下保持了91%以上的准确性，这高于C-DL模型。在PGD攻击中，基于alexnet的HCQ-DL模型保持了85%的准确性，而C-DL模型的准确性低于21%。我们的结果表明，在敌对设置下，HCQ-DL模型在交通标志分类中的准确性优于其经典对手。', 'title_zh': '基于量子计算支持的对抗攻击鲁棒自主车辆感知模块用于交通标志分类'}
{'arxiv_id': 'arXiv:2504.12577', 'title': 'Local Data Quantity-Aware Weighted Averaging for Federated Learning with Dishonest Clients', 'authors': 'Leming Wu, Yaochu Jin, Kuangrong Hao, Han Yu', 'link': 'https://arxiv.org/abs/2504.12577', 'abstract': "Federated learning (FL) enables collaborative training of deep learning models without requiring data to leave local clients, thereby preserving client privacy. The aggregation process on the server plays a critical role in the performance of the resulting FL model. The most commonly used aggregation method is weighted averaging based on the amount of data from each client, which is thought to reflect each client's contribution. However, this method is prone to model bias, as dishonest clients might report inaccurate training data volumes to the server, which is hard to verify. To address this issue, we propose a novel secure \\underline{Fed}erated \\underline{D}ata q\\underline{u}antity-\\underline{a}ware weighted averaging method (FedDua). It enables FL servers to accurately predict the amount of training data from each client based on their local model gradients uploaded. Furthermore, it can be seamlessly integrated into any FL algorithms that involve server-side model aggregation. Extensive experiments on three benchmarking datasets demonstrate that FedDua improves the global model performance by an average of 3.17% compared to four popular FL aggregation methods in the presence of inaccurate client data volume declarations.", 'abstract_zh': '联邦学习中的安全数据数量感知加权平均方法（FedDua）', 'title_zh': '带有不诚实客户端的联邦学习中基于局部数据量感知加权平均的方法'}
{'arxiv_id': 'arXiv:2504.12557', 'title': 'TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback', 'authors': 'Siow Meng Low, Akshat Kumar', 'link': 'https://arxiv.org/abs/2504.12557', 'abstract': "In safe reinforcement learning (RL), auxiliary safety costs are used to align the agent to safe decision making. In practice, safety constraints, including cost functions and budgets, are unknown or hard to specify, as it requires anticipation of all possible unsafe behaviors. We therefore address a general setting where the true safety definition is unknown, and has to be learned from sparsely labeled data. Our key contributions are: first, we design a safety model that performs credit assignment to estimate each decision step's impact on the overall safety using a dataset of diverse trajectories and their corresponding binary safety labels (i.e., whether the corresponding trajectory is safe/unsafe). Second, we illustrate the architecture of our safety model to demonstrate its ability to learn a separate safety score for each timestep. Third, we reformulate the safe RL problem using the proposed safety model and derive an effective algorithm to optimize a safe yet rewarding policy. Finally, our empirical results corroborate our findings and show that this approach is effective in satisfying unknown safety definition, and scalable to various continuous control tasks.", 'abstract_zh': '在安全强化学习中，辅助安全成本用于使智能体进行安全决策。由于安全约束、成本函数和预算在实践中往往是未知的或难以具体化，需要预测所有可能的不安全行为，我们因此提出了一种通用框架，其中真正的安全定义是未知的，并需要从稀疏标注的数据中学习。我们的主要贡献包括：首先，设计了一个安全模型来为每个决策步骤对整体安全的影响进行归因估计，使用多样轨迹及其相应的二元安全标签（即对应轨迹是否安全/不安全）的数据集。其次，展示了安全模型的架构，证明其能够为每个时间步学习一个独立的安全评分。第三，使用提出的安全模型重新表述安全强化学习问题，并推导出一种有效算法来优化安全且具有奖励性的策略。最后，我们的实证结果验证了这些发现，并展示了该方法有效满足未知的安全定义，并适用于各种连续控制任务。', 'title_zh': 'TraCeS: 基于轨迹的安全反馈稀疏信用分配'}
{'arxiv_id': 'arXiv:2504.12552', 'title': 'Privacy-Preserving Operating Room Workflow Analysis using Digital Twins', 'authors': 'Alejandra Perez, Han Zhang, Yu-Chun Ku, Lalithkumar Seenivasan, Roger Soberanis, Jose L. Porras, Richard Day, Jeff Jopling, Peter Najjar, Mathias Unberath', 'link': 'https://arxiv.org/abs/2504.12552', 'abstract': 'Purpose: The operating room (OR) is a complex environment where optimizing workflows is critical to reduce costs and improve patient outcomes. The use of computer vision approaches for the automatic recognition of perioperative events enables identification of bottlenecks for OR optimization. However, privacy concerns limit the use of computer vision for automated event detection from OR videos, which makes privacy-preserving approaches needed for OR workflow analysis. Methods: We propose a two-stage pipeline for privacy-preserving OR video analysis and event detection. In the first stage, we leverage vision foundation models for depth estimation and semantic segmentation to generate de-identified Digital Twins (DT) of the OR from conventional RGB videos. In the second stage, we employ the SafeOR model, a fused two-stream approach that processes segmentation masks and depth maps for OR event detection. We evaluate this method on an internal dataset of 38 simulated surgical trials with five event classes. Results: Our results indicate that this DT-based approach to the OR event detection model achieves performance on par and sometimes even better than raw RGB video-based models on detecting OR events. Conclusion: DTs enable privacy-preserving OR workflow analysis, facilitating the sharing of de-identified data across institutions and they can potentially enhance model generalizability by mitigating domain-specific appearance differences.', 'abstract_zh': '目的：手术室（OR）是一个复杂的工作环境，优化工作流程对于降低成本和改善患者结局至关重要。使用计算机视觉方法自动识别围手术期事件能够识别OR优化中的瓶颈。然而，隐私问题限制了计算机视觉在OR视频自动事件检测中的应用，因此需要隐私保护的方法来分析OR工作流程。方法：我们提出了一种两阶段的隐私保护OR视频分析和事件检测管道。在第一阶段，我们利用视觉基础模型进行深度估计和语义分割，从传统RGB视频中生成匿名的数字孪生（DT）模拟OR。在第二阶段，我们采用了SafeOR模型，这是一种融合了两流的方法，处理分割掩码和深度图进行OR事件检测。我们对该方法在包含38场模拟手术试验的内部数据集上，针对五个事件类进行了评估。结果：结果显示，基于DT的方法在检测OR事件方面与其他基于原始RGB视频的方法具有相当甚至更好的性能。结论：DT使隐私保护的OR工作流程分析成为可能，促进了机构间去识别数据的共享，并有可能通过缓解领域特定外观差异来增强模型的泛化能力。', 'title_zh': '基于数字孪生的隐私保护手术室工作流程分析'}
{'arxiv_id': 'arXiv:2504.12546', 'title': 'Anonymous Public Announcements', 'authors': 'Thomas Ågotnes, Rustam Galimullin, Ken Satoh, Satoshi Tojo', 'link': 'https://arxiv.org/abs/2504.12546', 'abstract': 'We formalise the notion of an \\emph{anonymous public announcement} in the tradition of public announcement logic. Such announcements can be seen as in-between a public announcement from ``the outside" (an announcement of $\\phi$) and a public announcement by one of the agents (an announcement of $K_a\\phi$): we get more information than just $\\phi$, but not (necessarily) about exactly who made it. Even if such an announcement is prima facie anonymous, depending on the background knowledge of the agents it might reveal the identity of the announcer: if I post something on a message board, the information might reveal who I am even if I don\'t sign my name. Furthermore, like in the Russian Cards puzzle, if we assume that the announcer\'s intention was to stay anonymous, that in fact might reveal more information. In this paper we first look at the case when no assumption about intentions are made, in which case the logic with an anonymous public announcement operator is reducible to epistemic logic. We then look at the case when we assume common knowledge of the intention to stay anonymous, which is both more complex and more interesting: in several ways it boils down to the notion of a ``safe" announcement (again, similarly to Russian Cards). Main results include formal expressivity results and axiomatic completeness for key logical languages.', 'abstract_zh': '我们形式化了公佈論文中匿名公佈的概念。这样的公佈可以被视为介于外部公佈（宣布$\\phi$）和一个代理进行的公佈（宣布$K_a\\phi$）之间的一种形式：我们获得的信息不仅包括$\\phi$，但不一定具体知道是谁提供的。即使这样的公佈看似匿名，根据代理的背景知识，它可能会揭示信息发布者的身份：例如，我在留言板上发布信息，即使我没有签名，信息也可能透露出我的身份。此外，类似于俄罗斯扑克牌谜题的情况，如果假设信息发布者意图保持匿名，这实际上可能会泄露更多信息。在本文中，我们首先考虑没有假设意图的情况，在这种情况下，带有匿名公佈操作符的逻辑可以归约为知识逻辑。然后我们考虑假设共同知道意图保持匿名的情况，这既更复杂也更有趣：在多个方面都涉及“安全”公佈的概念（再次类似于俄罗斯扑克牌谜题）。主要结果包括对关键逻辑语言的形式表达能力和公理完全性结果。', 'title_zh': '匿名公共公告'}
{'arxiv_id': 'arXiv:2504.12532', 'title': 'Generalization through variance: how noise shapes inductive biases in diffusion models', 'authors': 'John J. Vastola', 'link': 'https://arxiv.org/abs/2504.12532', 'abstract': "How diffusion models generalize beyond their training set is not known, and is somewhat mysterious given two facts: the optimum of the denoising score matching (DSM) objective usually used to train diffusion models is the score function of the training distribution; and the networks usually used to learn the score function are expressive enough to learn this score to high accuracy. We claim that a certain feature of the DSM objective -- the fact that its target is not the training distribution's score, but a noisy quantity only equal to it in expectation -- strongly impacts whether and to what extent diffusion models generalize. In this paper, we develop a mathematical theory that partly explains this 'generalization through variance' phenomenon. Our theoretical analysis exploits a physics-inspired path integral approach to compute the distributions typically learned by a few paradigmatic under- and overparameterized diffusion models. We find that the distributions diffusion models effectively learn to sample from resemble their training distributions, but with 'gaps' filled in, and that this inductive bias is due to the covariance structure of the noisy target used during training. We also characterize how this inductive bias interacts with feature-related inductive biases.", 'abstract_zh': '扩散模型如何泛化到训练集之外还尚未可知，这基于两点事实显得尤为神秘：用于训练扩散模型的去噪评分匹配（DSM）目标函数的最优值通常是训练分布的评分函数；通常用于学习评分函数的网络具有足够的表达能力，能够高精度地学习这一评分函数。我们主张，DSM目标函数的一项特定特征——其目标不是训练分布的评分函数，而是一个仅在期望意义下等于评分函数的噪声量——强烈影响扩散模型泛化的程度和范围。在本文中，我们发展了部分解释这种“通过方差泛化”现象的数学理论。我们的理论分析利用了一种受物理启发的路径积分方法，计算了几种范型的欠参数化和过参数化扩散模型通常学习到的分布。我们发现，扩散模型有效地学习到的分布类似于其训练分布，但填充了“空缺”，这种归纳偏见是由于训练过程中使用的噪声目标的协方差结构。我们还分析了这种归纳偏见与特征相关归纳偏见的相互作用。', 'title_zh': '通过方差的一般化：噪声如何塑造扩散模型的归纳偏置'}
{'arxiv_id': 'arXiv:2504.12503', 'title': 'Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking Study', 'authors': 'Kaira M. Samuel, Faez Ahmed', 'link': 'https://arxiv.org/abs/2504.12503', 'abstract': 'Engineering problems that apply machine learning often involve computationally intensive methods but rely on limited datasets. As engineering data evolves with new designs and constraints, models must incorporate new knowledge over time. However, high computational costs make retraining models from scratch infeasible. Continual learning (CL) offers a promising solution by enabling models to learn from sequential data while mitigating catastrophic forgetting, where a model forgets previously learned mappings. This work introduces CL to engineering design by benchmarking several CL methods on representative regression tasks. We apply these strategies to five engineering datasets and construct nine new engineering CL benchmarks to evaluate their ability to address forgetting and improve generalization. Preliminary results show that applying existing CL methods to these tasks improves performance over naive baselines. In particular, the Replay strategy achieved performance comparable to retraining in several benchmarks while reducing training time by nearly half, demonstrating its potential for real-world engineering workflows. The code and datasets used in this work will be available at: this https URL.', 'abstract_zh': '工程设计中应用机器学习的问题往往涉及计算密集的方法但依赖于有限的数据集。随着新设计和约束条件的发展，工程数据不断演变，模型必须随着时间 Incorporate 新知识。然而，高昂的计算成本使得从头开始重新训练模型在实际中不可行。连续学习（CL）通过使模型能够在不遗忘之前学得的知识的情况下学习序列数据，提供了一种有前景的解决方案。本文通过在代表性的回归任务上对几种连续学习方法进行基准测试，将连续学习引入工程设计。我们将这些策略应用于五个工程数据集，并构建了九个新的工程连续学习基准，以评估它们在应对遗忘和提高泛化方面的能力。初步结果表明，将现有连续学习方法应用于这些任务能够提高性能，并且特定的重播策略在几个基准中实现了与从头训练相当的性能，同时将训练时间几乎减少了半数，展示了其在真实工程工作流中的潜力。本文所使用的代码和数据集将在此处提供：this https URL。', 'title_zh': '3D工程回归问题的持续学习策略：一项基准研究'}
{'arxiv_id': 'arXiv:2504.12488', 'title': 'Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process', 'authors': 'Mohi Reza, Jeb Thomas-Mitchell, Peter Dushniku, Nathan Laundry, Joseph Jay Williams, Anastasia Kuzminykh', 'link': 'https://arxiv.org/abs/2504.12488', 'abstract': "As generative AI tools like ChatGPT become integral to everyday writing, critical questions arise about how to preserve writers' sense of agency and ownership when using these tools. Yet, a systematic understanding of how AI assistance affects different aspects of the writing process - and how this shapes writers' agency - remains underexplored. To address this gap, we conducted a systematic review of 109 HCI papers using the PRISMA approach. From this literature, we identify four overarching design strategies for AI writing support: structured guidance, guided exploration, active co-writing, and critical feedback - mapped across the four key cognitive processes in writing: planning, translating, reviewing, and monitoring. We complement this analysis with interviews of 15 writers across diverse domains. Our findings reveal that writers' desired levels of AI intervention vary across the writing process: content-focused writers (e.g., academics) prioritize ownership during planning, while form-focused writers (e.g., creatives) value control over translating and reviewing. Writers' preferences are also shaped by contextual goals, values, and notions of originality and authorship. By examining when ownership matters, what writers want to own, and how AI interactions shape agency, we surface both alignment and gaps between research and user needs. Our findings offer actionable design guidance for developing human-centered writing tools for co-writing with AI, on human terms.", 'abstract_zh': '随着生成AI工具如ChatGPT成为日常写作不可或缺的部分，关键问题是如何在使用这些工具时保持写作者的能动性和所有权感。然而，关于AI辅助如何影响写作过程的不同方面以及这对写作者能动性的影响的系统性理解仍显得不足。为填补这一空白，我们采用PRISMA方法系统回顾了109篇人机交互（HCI）论文。从这些文献中，我们识别出四种总体设计策略，用于支持AI写作：结构化指导、引导探索、协作写作和批判性反馈，这些策略映射到写作的四个关键认知过程：规划、翻译、审阅和监控。我们还通过采访来自不同领域的15名写作者，补充了这一分析。研究表明，写作者希望在写作过程中的AI介入程度各不相同：内容导向的写作者（如学术人员）在规划阶段更重视所有权，而形式导向的写作者（如创意人员）在翻译和审阅阶段更重视控制权。写作者的偏好还受到其背景目标、价值观以及原创性和作者身份观念的影响。通过对何时所有权至关重要、写作者希望保留哪些方面以及AI互动如何塑造能动性进行考察，我们揭示了研究与用户需求之间的契合点和差距。我们的研究结果提供了关于如何根据人类需求开发兼顾人类视角的AI辅助写作工具的实用设计指导。', 'title_zh': '基于人类视角与AI共写：在整个写作过程中满足用户需求的研究对接'}
{'arxiv_id': 'arXiv:2504.12476', 'title': 'What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States', 'authors': 'Andreas Jungherr, Adrian Rauchfleisch', 'link': 'https://arxiv.org/abs/2504.12476', 'abstract': 'Recent advances in generative Artificial Intelligence have raised public awareness, shaping expectations and concerns about their societal implications. Central to these debates is the question of AI alignment -- how well AI systems meet public expectations regarding safety, fairness, and social values. However, little is known about what people expect from AI-enabled systems and how these expectations differ across national contexts. We present evidence from two surveys of public preferences for key functional features of AI-enabled systems in Germany (n = 1800) and the United States (n = 1756). We examine support for four types of alignment in AI moderation: accuracy and reliability, safety, bias mitigation, and the promotion of aspirational imaginaries. U.S. respondents report significantly higher AI use and consistently greater support for all alignment features, reflecting broader technological openness and higher societal involvement with AI. In both countries, accuracy and safety enjoy the strongest support, while more normatively charged goals -- like fairness and aspirational imaginaries -- receive more cautious backing, particularly in Germany. We also explore how individual experience with AI, attitudes toward free speech, political ideology, partisan affiliation, and gender shape these preferences. AI use and free speech support explain more variation in Germany. In contrast, U.S. responses show greater attitudinal uniformity, suggesting that higher exposure to AI may consolidate public expectations. These findings contribute to debates on AI governance and cross-national variation in public preferences. More broadly, our study demonstrates the value of empirically grounding AI alignment debates in public attitudes and of explicitly developing normatively grounded expectations into theoretical and policy discussions on the governance of AI-generated content.', 'abstract_zh': 'Recent Advances in Generative Artificial Intelligence Have Raised Public Awareness, Shaping Expectations and Concerns about Their Societal Implications: Evidence from Surveys in Germany and the United States', 'title_zh': '人们期望的人工智能是什么？来自德国和美国的公众意见关于AI内容审核的对齐观点'}
{'arxiv_id': 'arXiv:2504.12463', 'title': 'Dense Backpropagation Improves Training for Sparse Mixture-of-Experts', 'authors': 'Ashwinee Panda, Vatsal Baherwani, Zain Sarwar, Benjamin Therien, Supriyo Chakraborty, Tom Goldstein', 'link': 'https://arxiv.org/abs/2504.12463', 'abstract': 'Mixture of Experts (MoE) pretraining is more scalable than dense Transformer pretraining, because MoEs learn to route inputs to a sparse set of their feedforward parameters. However, this means that MoEs only receive a sparse backward update, leading to training instability and suboptimal performance. We present a lightweight approximation method that gives the MoE router a dense gradient update while continuing to sparsely activate its parameters. Our method, which we refer to as Default MoE, substitutes missing expert activations with default outputs consisting of an exponential moving average of expert outputs previously seen over the course of training. This allows the router to receive signals from every expert for each token, leading to significant improvements in training performance. Our Default MoE outperforms standard TopK routing in a variety of settings without requiring significant computational overhead. Code: this https URL.', 'abstract_zh': 'Mixture of Experts (MoE) 预训练比密集Transformer预训练更具可扩展性，因为MoE学习将输入路由到其前向参数的稀疏子集。然而，这意味着MoE只接收稀疏的反向更新，导致培训不稳定和性能不佳。我们提出了一种轻量级近似方法，该方法为MoE路由器提供密集梯度更新，同时继续稀疏激活其参数。我们称之为Default MoE的方法用默认输出替代缺失的专家激活，该默认输出为训练过程中先前见过的专家输出的指数移动平均值。这使得路由器能够为每个令牌从每个专家接收信号，从而显著提高训练性能。Default MoE在各种设置中优于标准TopK路由，且无需显著增加计算开销。代码：this https URL。', 'title_zh': '稠密反向传播有助于稀疏混合专家模型的训练'}
{'arxiv_id': 'arXiv:2504.12446', 'title': 'Deriving Equivalent Symbol-Based Decision Models from Feedforward Neural Networks', 'authors': 'Sebastian Seidel, Uwe M. Borghoff', 'link': 'https://arxiv.org/abs/2504.12446', 'abstract': 'Artificial intelligence (AI) has emerged as a transformative force across industries, driven by advances in deep learning and natural language processing, and fueled by large-scale data and computing resources. Despite its rapid adoption, the opacity of AI systems poses significant challenges to trust and acceptance.\nThis work explores the intersection of connectionist and symbolic approaches to artificial intelligence, focusing on the derivation of interpretable symbolic models, such as decision trees, from feedforward neural networks (FNNs). Decision trees provide a transparent framework for elucidating the operations of neural networks while preserving their functionality. The derivation is presented in a step-by-step approach and illustrated with several examples. A systematic methodology is proposed to bridge neural and symbolic paradigms by exploiting distributed representations in FNNs to identify symbolic components, including fillers, roles, and their interrelationships. The process traces neuron activation values and input configurations across network layers, mapping activations and their underlying inputs to decision tree edges. The resulting symbolic structures effectively capture FNN decision processes and enable scalability to deeper networks through iterative refinement of subpaths for each hidden layer.\nTo validate the theoretical framework, a prototype was developed using Keras .h5-data and emulating TensorFlow within the Java JDK/JavaFX environment. This prototype demonstrates the feasibility of extracting symbolic representations from neural networks, enhancing trust in AI systems, and promoting accountability.', 'abstract_zh': '人工智能（AI）作为一种颠覆性的力量，在行业中的应用日益广泛，这一进展得益于深度学习和自然语言处理的进步，并依托于大规模的数据和计算资源。尽管AI的采用速度非常快，但其透明度不足给信任和接受带来了重大挑战。\n\n本文探讨了连接主义和符号主义在人工智能领域的交汇，重点关注从前馈神经网络（FNNs）中推导出可解释的符号模型，例如决策树。决策树为阐明神经网络的操作提供了一个透明的框架，同时保留了它们的功能性。该推导采用逐步方法，并通过示例进行说明。本文提出了一种系统的方法，通过利用FNN中的分布式表示来识别符号组件，包括填充项、角色及其相互关系，从而连接神经和符号范式。该过程追踪网络层中神经元激活值和输入配置的变化，将激活及其底层输入映射到决策树的边。由此产生的符号结构有效地捕获了FNN的决策过程，并通过逐层迭代改进子路径实现对更深层网络的扩展应用。\n\n为了验证理论框架，本文在Keras .h5数据和Java JDK/JavaFX环境中仿TensorFlow开发了一个原型。该原型展示了从神经网络中提取符号表示的可行性，增强了对AI系统的信任，并促进了问责制。', 'title_zh': '从前馈神经网络导出等效符号基础决策模型'}
{'arxiv_id': 'arXiv:2504.12436', 'title': 'Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation', 'authors': 'Nairouz Mrabah, Nicolas Richet, Ismail Ben Ayed, Éric Granger', 'link': 'https://arxiv.org/abs/2504.12436', 'abstract': 'Adapting Vision-Language Models (VLMs) to new domains with few labeled samples remains a significant challenge due to severe overfitting and computational constraints. State-of-the-art solutions, such as low-rank reparameterization, mitigate these issues but often struggle with generalization and require extensive hyperparameter tuning. In this paper, a novel Sparse Optimization (SO) framework is proposed. Unlike low-rank approaches that typically constrain updates to a fixed subspace, our SO method leverages high sparsity to dynamically adjust very few parameters. We introduce two key paradigms. First, we advocate for \\textit{local sparsity and global density}, which updates a minimal subset of parameters per iteration while maintaining overall model expressiveness. As a second paradigm, we advocate for \\textit{local randomness and global importance}, which sparsifies the gradient using random selection while pruning the first moment based on importance. This combination significantly mitigates overfitting and ensures stable adaptation in low-data regimes. Extensive experiments on 11 diverse datasets show that SO achieves state-of-the-art few-shot adaptation performance while reducing memory overhead.', 'abstract_zh': '适配新领域的小样本视觉-语言模型的稀疏优化框架', 'title_zh': '稀疏投影优于低秩投影在少量样本适应中的表现'}
{'arxiv_id': 'arXiv:2504.12360', 'title': 'A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version', 'authors': 'Mieczysław A. Kłopotek, Sławomir T. Wierzchoń, Bartłomiej Starosta, Dariusz Czerski, Piotr Borkowski', 'link': 'https://arxiv.org/abs/2504.12360', 'abstract': 'This paper investigates the problem of Graph Spectral Clustering with negative similarities, resulting from document embeddings different from the traditional Term Vector Space (like doc2vec, GloVe, etc.). Solutions for combinatorial Laplacians and normalized Laplacians are discussed. An experimental investigation shows the advantages and disadvantages of 6 different solutions proposed in the literature and in this research. The research demonstrates that GloVe embeddings frequently cause failures of normalized Laplacian based GSC due to negative similarities. Furthermore, application of methods curing similarity negativity leads to accuracy improvement for both combinatorial and normalized Laplacian based GSC. It also leads to applicability for GloVe embeddings of explanation methods developed originally bythe authors for Term Vector Space embeddings.', 'abstract_zh': '基于负相似性的图频域聚类问题研究：从文档嵌入视角探讨梳理性拉普拉斯和规范化拉普拉斯的解决方案及其优劣比较', 'title_zh': '一种处理文本文档可解释图谱聚类中负相似性的方法——扩展版'}
{'arxiv_id': 'arXiv:2504.12358', 'title': 'Towards an AI Observatory for the Nuclear Sector: A tool for anticipatory governance', 'authors': 'Aditi Verma, Elizabeth Williams', 'link': 'https://arxiv.org/abs/2504.12358', 'abstract': 'AI models are rapidly becoming embedded in all aspects of nuclear energy research and work but the safety, security, and safeguards consequences of this embedding are not well understood. In this paper, we call for the creation of an anticipatory system of governance for AI in the nuclear sector as well as the creation of a global AI observatory as a means for operationalizing anticipatory governance. The paper explores the contours of the nuclear AI observatory and an anticipatory system of governance by drawing on work in science and technology studies, public policy, and foresight studies.', 'abstract_zh': 'AI模型在核能研究与工作中迅速嵌入，但其安全、安全性和管控后果尚不明确。本文呼吁在核能 sector 创建一种前瞻性的 AI 治理系统，以及建立一个全球 AI 观测站以实现前瞻治理的落实。文章通过科学与技术研究、公共政策和预见性研究的工作，探讨核能 AI 观测站和前瞻性治理系统的轮廓。', 'title_zh': '面向核能领域的AI观测站：一种前瞻性治理工具'}
{'arxiv_id': 'arXiv:2504.12351', 'title': 'Prototype-Guided Diffusion for Digital Pathology: Achieving Foundation Model Performance with Minimal Clinical Data', 'authors': 'Ekaterina Redekop, Mara Pleasure, Vedrana Ivezic, Zichen Wang, Kimberly Flores, Anthony Sisk, William Speier, Corey Arnold', 'link': 'https://arxiv.org/abs/2504.12351', 'abstract': 'Foundation models in digital pathology use massive datasets to learn useful compact feature representations of complex histology images. However, there is limited transparency into what drives the correlation between dataset size and performance, raising the question of whether simply adding more data to increase performance is always necessary. In this study, we propose a prototype-guided diffusion model to generate high-fidelity synthetic pathology data at scale, enabling large-scale self-supervised learning and reducing reliance on real patient samples while preserving downstream performance. Using guidance from histological prototypes during sampling, our approach ensures biologically and diagnostically meaningful variations in the generated data. We demonstrate that self-supervised features trained on our synthetic dataset achieve competitive performance despite using ~60x-760x less data than models trained on large real-world datasets. Notably, models trained using our synthetic data showed statistically comparable or better performance across multiple evaluation metrics and tasks, even when compared to models trained on orders of magnitude larger datasets. Our hybrid approach, combining synthetic and real data, further enhanced performance, achieving top results in several evaluations. These findings underscore the potential of generative AI to create compelling training data for digital pathology, significantly reducing the reliance on extensive clinical datasets and highlighting the efficiency of our approach.', 'abstract_zh': '基于原型引导的扩散模型生成大规模高保真合成病理数据以实现大规模自监督学习并保持下游性能', 'title_zh': '基于原型引导的扩散方法：在最少临床数据支持下实现基础模型性能'}
{'arxiv_id': 'arXiv:2504.12324', 'title': 'Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction', 'authors': 'Mengying Yuan, Wangzi Xuan, Fei Li', 'link': 'https://arxiv.org/abs/2504.12324', 'abstract': "Natural Language Inference (NLI) is a fundamental task in both natural language processing and information retrieval. While NLI has developed many sub-directions such as sentence-level NLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In this paper, we propose a novel paradigm for CDCL-NLI that extends traditional NLI capabilities to multi-document, multilingual scenarios. To support this task, we construct a high-quality CDCL-NLI dataset including 1,110 instances and spanning 26 languages. To build a baseline for this task, we also propose an innovative method that integrates RST-enhanced graph fusion and interpretability prediction. Our method employs RST (Rhetorical Structure Theory) on RGAT (Relation-aware Graph Attention Network) for cross-document context modeling, coupled with a structure-aware semantic alignment mechanism based on lexical chains for cross-lingual understanding. For NLI interpretability, we develop an EDU-level attribution framework that generates extractive explanations. Extensive experiments demonstrate our approach's superior performance, achieving significant improvements over both traditional NLI models such as DocNLI and R2F, as well as LLMs like Llama3 and GPT-4o. Our work sheds light on the study of NLI and will bring research interest on cross-document cross-lingual context understanding, semantic retrieval and interpretability inference. Our dataset and code are available at \\href{this https URL}{CDCL-NLI-Link for peer review}.", 'abstract_zh': '跨文档跨语言自然语言推理（CDCL-NLI）：一种新的范式', 'title_zh': '跨文档跨语言自然语言推理：基于RST增强图融合与可解释性预测'}
{'arxiv_id': 'arXiv:2504.12319', 'title': 'Specialized text classification: an approach to classifying Open Banking transactions', 'authors': 'Duc Tuyen TA, Wajdi Ben Saad, Ji Young Oh', 'link': 'https://arxiv.org/abs/2504.12319', 'abstract': 'With the introduction of the PSD2 regulation in the EU which established the Open Banking framework, a new window of opportunities has opened for banks and fintechs to explore and enrich Bank transaction descriptions with the aim of building a better understanding of customer behavior, while using this understanding to prevent fraud, reduce risks and offer more competitive and tailored services.\nAnd although the usage of natural language processing models and techniques has seen an incredible progress in various applications and domains over the past few years, custom applications based on domain-specific text corpus remain unaddressed especially in the banking sector.\nIn this paper, we introduce a language-based Open Banking transaction classification system with a focus on the french market and french language text. The system encompasses data collection, labeling, preprocessing, modeling, and evaluation stages. Unlike previous studies that focus on general classification approaches, this system is specifically tailored to address the challenges posed by training a language model with a specialized text corpus (Banking data in the French context). By incorporating language-specific techniques and domain knowledge, the proposed system demonstrates enhanced performance and efficiency compared to generic approaches.', 'abstract_zh': '在欧盟PSD2条例引入开放银行业务框架的背景下，银行和金融科技公司有机会通过丰富银行交易描述来深化客户行为理解，并利用这些理解预防欺诈、降低风险，提供更具竞争力和针对性的服务。虽然过去几年自然语言处理模型和方法在各种应用和领域中取得了惊人的进步，但基于领域特定文本语料库的定制应用特别是在银行业仍被忽视。本文介绍了一个基于语言的法国市场和法语文本的开放银行业务交易分类系统，该系统涵盖了数据收集、标注、预处理、建模和评估阶段。与以往侧重通用分类方法的研究不同，该系统专门针对使用特定领域文本语料库（法国银行数据）训练语言模型所面临的挑战进行设计。通过结合语言特定技术和领域知识，所提出系统在性能和效率上优于通用方法。', 'title_zh': '专门化的文本分类：一种Open Banking交易分类方法'}
