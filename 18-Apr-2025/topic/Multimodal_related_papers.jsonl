{'arxiv_id': 'arXiv:2504.13037', 'title': 'Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond', 'authors': 'Yundi Zhang, Paul Hager, Che Liu, Suprosanna Shit, Chen Chen, Daniel Rueckert, Jiazhen Pan', 'link': 'https://arxiv.org/abs/2504.13037', 'abstract': "Cardiac magnetic resonance imaging is the gold standard for non-invasive cardiac assessment, offering rich spatio-temporal views of the cardiac anatomy and physiology. Patient-level health factors, such as demographics, metabolic, and lifestyle, are known to substantially influence cardiovascular health and disease risk, yet remain uncaptured by CMR alone. To holistically understand cardiac health and to enable the best possible interpretation of an individual's disease risk, CMR and patient-level factors must be jointly exploited within an integrated framework. Recent multi-modal approaches have begun to bridge this gap, yet they often rely on limited spatio-temporal data and focus on isolated clinical tasks, thereby hindering the development of a comprehensive representation for cardiac health evaluation. To overcome these limitations, we introduce ViTa, a step toward foundation models that delivers a comprehensive representation of the heart and a precise interpretation of individual disease risk. Leveraging data from 42,000 UK Biobank participants, ViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling a complete capture of the cardiac cycle. These imaging data are then fused with detailed tabular patient-level factors, enabling context-aware insights. This multi-modal paradigm supports a wide spectrum of downstream tasks, including cardiac phenotype and physiological feature prediction, segmentation, and classification of cardiac and metabolic diseases within a single unified framework. By learning a shared latent representation that bridges rich imaging features and patient context, ViTa moves beyond traditional, task-specific models toward a universal, patient-specific understanding of cardiac health, highlighting its potential to advance clinical utility and scalability in cardiac analysis.", 'abstract_zh': '心脏磁共振成像是非侵入性心脏评估的金标准，能提供丰富的空间-时间心脏解剖和生理视图。患者的健康因素，如人口统计学、代谢和生活方式，已知会对心血管健康和疾病风险产生重大影响，但这些因素目前无法仅通过CMR捕捉。为了全方位理解心脏健康并为个体疾病风险提供最准确的解释，必须在集成框架中同时利用CMR和患者的健康因素。近期的多模态方法已经开始填补这一空白，但它们往往依赖有限的空间-时间数据并专注于孤立的临床任务，限制了全面心脏健康评估表示的发展。为克服这些限制，我们引入了ViTa，这是一个迈向基础模型的步骤，能提供心脏的全面表示并精确解释个体疾病风险。基于来自42,000名UK生物银行参与者的数据，ViTa整合了短轴和长轴视角的3D+T心脏动态序列，能够完整捕捉心脏周期。这些影像数据随后与详细的患者级表格因素融合，提供情境感知的见解。这一多模态范式支持一系列下游任务，包括心脏表型和生理特征预测、心脏和代谢疾病的分割和分类，都在同一统一框架中进行。通过学习将丰富的影像特征和患者情境联系起来的共享潜在表示，ViTa超越了传统、任务特定的模型，朝着所有患者特定的心脏健康理解迈进，突显其在心脏分析中的临床效用和可扩展性的潜力。', 'title_zh': '面向心脏MRI基础模型：全面的视觉-表格式表示以实现整个心脏评估及其他应用'}
{'arxiv_id': 'arXiv:2504.12717', 'title': 'Post-pre-training for Modality Alignment in Vision-Language Foundation Models', 'authors': "Shin'ya Yamaguchi, Dewei Feng, Sekitoshi Kanai, Kazuki Adachi, Daiki Chijiwa", 'link': 'https://arxiv.org/abs/2504.12717', 'abstract': 'Contrastive language image pre-training (CLIP) is an essential component of building modern vision-language foundation models. While CLIP demonstrates remarkable zero-shot performance on downstream tasks, the multi-modal feature spaces still suffer from a modality gap, which is a gap between image and text feature clusters and limits downstream task performance. Although existing works attempt to address the modality gap by modifying pre-training or fine-tuning, they struggle with heavy training costs with large datasets or degradations of zero-shot performance. This paper presents CLIP-Refine, a post-pre-training method for CLIP models at a phase between pre-training and fine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training on small image-text datasets without zero-shot performance degradations. To this end, we introduce two techniques: random feature alignment (RaFA) and hybrid contrastive-distillation (HyCD). RaFA aligns the image and text features to follow a shared prior distribution by minimizing the distance to random reference vectors sampled from the prior. HyCD updates the model with hybrid soft labels generated by combining ground-truth image-text pair labels and outputs from the pre-trained CLIP model. This contributes to achieving both maintaining the past knowledge and learning new knowledge to align features. Our extensive experiments with multiple classification and retrieval tasks show that CLIP-Refine succeeds in mitigating the modality gap and improving the zero-shot performance.', 'abstract_zh': 'CLIP-Refine：预训练后多模态特征空间对齐方法以缓解模态缺口并提升零样本性能', 'title_zh': '视觉-语言基础模型中的模态对齐后预训练'}
{'arxiv_id': 'arXiv:2504.12315', 'title': 'Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models', 'authors': 'Xingguang Ji, Jiakang Wang, Hongzhi Zhang, Jingyuan Zhang, Haonan Zhou, Chenxi Sun, Yahui Liu, Qi Wang, Fuzheng Zhang', 'link': 'https://arxiv.org/abs/2504.12315', 'abstract': 'With the development of Multimodal Large Language Models (MLLMs), numerous outstanding accomplishments have emerged within the open-source community. Due to the complexity of creating and training multimodal data pairs, it is still a computational and time-consuming process to build powerful MLLMs. In this work, we introduce Capybara-OMNI, an MLLM that trains in a lightweight and efficient manner and supports understanding text, image, video, and audio modalities. We present in detail the framework design, the data construction, and the training recipe, to develop an MLLM step-by-step to obtain competitive performance. We also provide exclusive benchmarks utilized in our experiments to show how to properly verify understanding capabilities across different modalities. Results show that by following our guidance, we can efficiently build an MLLM that achieves competitive performance among models of the same scale on various multimodal benchmarks. Additionally, to enhance the multimodal instruction following and conversational capabilities of the model, we further discuss how to train the chat version upon an MLLM understanding model, which is more in line with user habits for tasks like real-time interaction with humans. We publicly disclose the Capybara-OMNI model, along with its chat-based version. The disclosure includes both the model weights, a portion of the training data, and the inference codes, which are made available on GitHub.', 'abstract_zh': '基于轻量高效训练的大规模多模态语言模型Capybara-OMNI及其应用研究', 'title_zh': 'Capybara-OMNI：一种高效的多模态语言模型构建范式'}
