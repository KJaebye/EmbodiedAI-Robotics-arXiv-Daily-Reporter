{'arxiv_id': 'arXiv:2502.19135', 'title': 'A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided Knowledge Base Management', 'authors': 'Enrico Saccon, Ahmet Tikna, Davide De Martini, Edoardo Lamon, Luigi Palopoli, Marco Roveri', 'link': 'https://arxiv.org/abs/2502.19135', 'abstract': 'This paper presents a novel framework, called PLANTOR (PLanning with Natural language for Task-Oriented Robots), that integrates Large Language Models (LLMs) with Prolog-based knowledge management and planning for multi-robot tasks. The system employs a two-phase generation of a robot-oriented knowledge base, ensuring reusability and compositional reasoning, as well as a three-step planning procedure that handles temporal dependencies, resource constraints, and parallel task execution via mixed-integer linear programming. The final plan is converted into a Behaviour Tree for direct use in ROS2. We tested the framework in multi-robot assembly tasks within a block world and an arch-building scenario. Results demonstrate that LLMs can produce accurate knowledge bases with modest human feedback, while Prolog guarantees formal correctness and explainability. This approach underscores the potential of LLM integration for advanced robotics tasks requiring flexible, scalable, and human-understandable planning.', 'abstract_zh': 'PLANTOR：基于自然语言的面向任务机器人规划框架', 'title_zh': '基于LLM辅助知识库管理的多智能体系统时序规划框架'}
{'arxiv_id': 'arXiv:2502.18690', 'title': 'Hybrid Voting-Based Task Assignment in Role-Playing Games', 'authors': 'Daniel Weiner, Raj Korpan', 'link': 'https://arxiv.org/abs/2502.18690', 'abstract': "In role-playing games (RPGs), the level of immersion is critical-especially when an in-game agent conveys tasks, hints, or ideas to the player. For an agent to accurately interpret the player's emotional state and contextual nuances, a foundational level of understanding is required, which can be achieved using a Large Language Model (LLM). Maintaining the LLM's focus across multiple context changes, however, necessitates a more robust approach, such as integrating the LLM with a dedicated task allocation model to guide its performance throughout gameplay. In response to this need, we introduce Voting-Based Task Assignment (VBTA), a framework inspired by human reasoning in task allocation and completion. VBTA assigns capability profiles to agents and task descriptions to tasks, then generates a suitability matrix that quantifies the alignment between an agent's abilities and a task's requirements. Leveraging six distinct voting methods, a pre-trained LLM, and integrating conflict-based search (CBS) for path planning, VBTA efficiently identifies and assigns the most suitable agent to each task. While existing approaches focus on generating individual aspects of gameplay, such as single quests, or combat encounters, our method shows promise when generating both unique combat encounters and narratives because of its generalizable nature.", 'abstract_zh': '在角色扮演游戏（RPG）中，沉浸感的水平至关重要，特别是在游戏代理向玩家传达任务、提示或想法时。为了使代理能够准确理解玩家的情绪状态和语境细微差别，需要具备一定的理解基础，这可以通过使用大规模语言模型（LLM）来实现。然而，维持LLM在多次语境变化中的专注力需要更稳健的方法，例如将LLM与专门的任务分配模型集成，以指导其在整个游戏过程中的表现。为应对这一需求，我们引入了基于投票的任务分配框架（VBTA），该框架受人类任务分配与完成过程中推理的启发。VBTA为代理分配能力配置文件，并为任务分配任务描述，进而生成量化代理能力和任务要求之间契合度的适合性矩阵。利用六种不同的投票方法、预训练的LLM，并结合冲突基于搜索（CBS）进行路径规划，VBTA有效地识别并分配最适合的代理给每个任务。尽管现有方法侧重于生成游戏的单一方面，如单个任务或战斗遭遇，我们的方法因其通用性在生成独特战斗遭遇和叙述时显示出前景。', 'title_zh': '角色扮演游戏中基于混合投票的任务分配'}
{'arxiv_id': 'arXiv:2502.18528', 'title': 'ARACNE: An LLM-Based Autonomous Shell Pentesting Agent', 'authors': 'Tomas Nieponice, Veronica Valeros, Sebastian Garcia', 'link': 'https://arxiv.org/abs/2502.18528', 'abstract': 'We introduce ARACNE, a fully autonomous LLM-based pentesting agent tailored for SSH services that can execute commands on real Linux shell systems. Introduces a new agent architecture with multi-LLM model support. Experiments show that ARACNE can reach a 60\\% success rate against the autonomous defender ShelLM and a 57.58\\% success rate against the Over The Wire Bandit CTF challenges, improving over the state-of-the-art. When winning, the average number of actions taken by the agent to accomplish the goals was less than 5. The results show that the use of multi-LLM is a promising approach to increase accuracy in the actions.', 'abstract_zh': '我们介绍ARACNE：一种针对SSH服务的自主LLM基渗透测试代理，能够执行真实LinuxShell系统的命令。该代理架构支持多LLM模型，并实验证明，ARACNE在对抗自主防守方ShelLM时成功率达到60%，在对抗Over The Wire Bandit CTF挑战时成功率达到57.58%，超越了当前最优方法。当获胜时，代理完成目标所需的平均行动次数少于5次。结果表明，使用多LLM是提高行动准确性的有前途的方法。', 'title_zh': 'ARACNE：基于LLM的自主Shell渗透测试代理'}
{'arxiv_id': 'arXiv:2502.19400', 'title': 'TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding', 'authors': 'Max Ku, Thomas Chong, Jonathan Leung, Krish Shah, Alvin Yu, Wenhu Chen', 'link': 'https://arxiv.org/abs/2502.19400', 'abstract': 'Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.', 'abstract_zh': '理解领域特定的定理往往需要超越基于文本的推理；有效的结构性视觉解释对于深入理解至关重要。尽管大型语言模型在基于文本的定理推理方面表现出色，但它们生成连贯且有教育意义的视觉解释的能力仍然是一个开放的挑战。本工作中，我们引入了TheoremExplainAgent，这是一种使用Manim动画生成长格式定理解释视频（超过5分钟）的代理方法。为了系统地评估多模态定理解释，我们提出了TheoremExplainBench基准，该基准包含来自多个STEM学科的240个定理，以及5个自动评估指标。我们的结果显示，代理规划对于生成详细的长视频是必不可少的，o3-mini代理的成功率为93.8%，总体得分为0.77。然而，我们的定量和定性研究表明，大多数生成的视频在视觉元素布局方面存在较小的问题。此外，多模态解释揭示了基于文本解释无法揭示的更深层次的推理缺陷，突显了多模态解释的重要性。', 'title_zh': 'TheoremExplainAgent: 向量多模态定理解释代理'}
{'arxiv_id': 'arXiv:2502.19295', 'title': 'Complex LLM Planning via Automated Heuristics Discovery', 'authors': 'Hongyi Ling, Shubham Parashar, Sambhav Khurana, Blake Olson, Anwesha Basu, Gaurangi Sinha, Zhengzhong Tu, James Caverlee, Shuiwang Ji', 'link': 'https://arxiv.org/abs/2502.19295', 'abstract': 'We consider enhancing large language models (LLMs) for complex planning tasks. While existing methods allow LLMs to explore intermediate steps to make plans, they either depend on unreliable self-verification or external verifiers to evaluate these steps, which demand significant data and computations. Here, we propose automated heuristics discovery (AutoHD), a novel approach that enables LLMs to explicitly generate heuristic functions to guide inference-time search, allowing accurate evaluation of intermediate states. These heuristic functions are further refined through a heuristic evolution process, improving their robustness and effectiveness. Our proposed method requires no additional model training or fine-tuning, and the explicit definition of heuristic functions generated by the LLMs provides interpretability and insights into the reasoning process. Extensive experiments across diverse benchmarks demonstrate significant gains over multiple baselines, including nearly twice the accuracy on some datasets, establishing our approach as a reliable and interpretable solution for complex planning tasks.', 'abstract_zh': '我们考虑增强大型语言模型（LLMs）以应对复杂的规划任务。现有方法允许LLMs探索中间步骤以制定计划，但这些方法要么依赖于不可靠的自我验证，要么依赖外部验证器来评估这些步骤，这需要大量的数据和计算资源。为此，我们提出了自动启发式发现（AutoHD），这是一种新颖的方法，使LLMs能够明确生成启发式函数以指导推理时的搜索，从而准确评估中间状态。这些启发式函数通过启发式进化过程进一步优化，提高其稳健性和有效性。我们的方法不需要额外的模型训练或微调，由LLMs生成的明确定义的启发式函数提供了可解释性和对推理过程的洞察。在多种基准上的广泛实验表明，与多个基线方法相比取得了显著改进，包括某些数据集上的准确率几乎是基线的两倍，确立了该方法作为解决复杂规划任务的可靠且可解释的解决方案的地位。', 'title_zh': '通过自动启发式发现进行的复杂LLM规划'}
{'arxiv_id': 'arXiv:2502.19091', 'title': 'Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex Tasks Automation', 'authors': 'Humza Sami, Mubashir ul Islam, Samy Charas, Asav Gandhi, Pierre-Emmanuel Gaillardon, Valerio Tenace', 'link': 'https://arxiv.org/abs/2502.19091', 'abstract': 'Recent advancements in Large Language Models (LLMs) have substantially evolved Multi-Agent Systems (MASs) capabilities, enabling systems that not only automate tasks but also leverage near-human reasoning capabilities. To achieve this, LLM-based MASs need to be built around two critical principles: (i) a robust architecture that fully exploits LLM potential for specific tasks -- or related task sets -- and ($ii$) an effective methodology for equipping LLMs with the necessary capabilities to perform tasks and manage information efficiently. It goes without saying that a priori architectural designs can limit the scalability and domain adaptability of a given MAS.\nTo address these challenges, in this paper we introduce Nexus: a lightweight Python framework designed to easily build and manage LLM-based MASs. Nexus introduces the following innovations: (i) a flexible multi-supervisor hierarchy, (ii) a simplified workflow design, and (iii) easy installation and open-source flexibility: Nexus can be installed via pip and is distributed under a permissive open-source license, allowing users to freely modify and extend its capabilities.\nExperimental results demonstrate that architectures built with Nexus exhibit state-of-the-art performance across diverse domains. In coding tasks, Nexus-driven MASs achieve a 99% pass rate on HumanEval and a flawless 100% on VerilogEval-Human, outperforming cutting-edge reasoning language models such as o3-mini and DeepSeek-R1. Moreover, these architectures display robust proficiency in complex reasoning and mathematical problem solving, achieving correct solutions for all randomly selected problems from the MATH dataset. In the realm of multi-objective optimization, Nexus-based architectures successfully address challenging timing closure tasks on designs from the VTR benchmark suite, while guaranteeing, on average, a power saving of nearly 30%.', 'abstract_zh': '最近大型语言模型（LLMs）的进步显著提升了多智能体系统（MASs）的能力，使其不仅能够自动化任务，还能够利用接近人类的推理能力。为了实现这一点，基于LLM的MASs需要遵循两个关键原则：(i)一个稳健的架构，能够充分利用LLM在特定任务或相关任务集上的潜力；和(ii)有效的机制来赋予LLMs执行任务和高效管理信息所需的必要能力。不言而喻，先验的架构设计可能会限制给定MAS的可扩展性和领域适应性。\n\n为了应对这些挑战，本文介绍了Nexus：一个轻量级的Python框架，旨在方便地构建和管理基于LLM的MASs。Nexus引入了以下创新：(i)灵活的多监督器层次结构，(ii)简化的工作流设计，和(iii)易于安装和开源灵活性：Nexus可以通过pip安装，并且分发在一种宽松的开源许可下，允许用户自由修改和扩展其功能。\n\n实验结果表明，使用Nexus构建的架构在多个领域均表现出最先进的性能。在编程任务中，Nexus驱动的MASs在HumanEval上的通过率为99%，在VerilogEval-Human上达到完美的100%，超过了最先进的推理语言模型如o3-mini和DeepSeek-R1。此外，这些架构在复杂的推理和数学问题解决方面表现出强大的能力，能够正确解决数学数据集随机选择的所有问题。在多目标优化领域，基于Nexus的架构成功解决了来自VTR基准套件的设计上的困难的时序闭合问题，同时平均保证了近30%的功耗节省。', 'title_zh': 'Nexus：一种轻量级可扩展的多Agent复杂任务自动化框架'}
{'arxiv_id': 'arXiv:2502.18928', 'title': 'Talking like Piping and Instrumentation Diagrams (P&IDs)', 'authors': 'Achmad Anggawirya Alimin, Dominik P. Goldstein, Lukas Schulze Balhorn, Artur M. Schweidtmann', 'link': 'https://arxiv.org/abs/2502.18928', 'abstract': "We propose a methodology that allows communication with Piping and Instrumentation Diagrams (P&IDs) using natural language. In particular, we represent P&IDs through the DEXPI data model as labeled property graphs and integrate them with Large Language Models (LLMs). The approach consists of three main parts: 1) P&IDs are cast into a graph representation from the DEXPI format using our pyDEXPI Python package. 2) A tool for generating P&ID knowledge graphs from pyDEXPI. 3) Integration of the P&ID knowledge graph to LLMs using graph-based retrieval augmented generation (graph-RAG). This approach allows users to communicate with P&IDs using natural language. It extends LLM's ability to retrieve contextual data from P&IDs and mitigate hallucinations. Leveraging the LLM's large corpus, the model is also able to interpret process information in PIDs, which could help engineers in their daily tasks. In the future, this work will also open up opportunities in the context of other generative Artificial Intelligence (genAI) solutions on P&IDs, and AI-assisted HAZOP studies.", 'abstract_zh': '我们提出了一种方法，使得可以通过自然语言与工艺和仪表图（P&IDs）进行通信。特别地，我们通过DEXPI数据模型将P&IDs表示为标记属性图，并与大型语言模型（LLMs）集成。该方法包括三个主要部分：1）使用我们编写的pyDEXPI Python包将P&IDs转换为从DEXPI格式表示的图表示。2）从pyDEXPI生成P&ID知识图谱的工具。3）使用基于图的检索增强生成（graph-RAG）将P&ID知识图谱集成到LLMs中。该方法使得用户能够使用自然语言与P&IDs进行通信。它扩展了LLMs从P&IDs中检索上下文数据的能力，并减轻了幻觉现象。利用LLMs庞大的语料库，该模型还能够解释PIDs中的工艺信息，这有助于工程师完成日常工作任务。未来，这项工作还将为P&IDs上的其他生成型人工智能（genAI）解决方案以及AI辅助的HAZOP研究开辟机会。', 'title_zh': '模拟管道和仪表图（P&IDs）的表达方式'}
{'arxiv_id': 'arXiv:2502.18873', 'title': 'Multi-LLM Collaborative Search for Complex Problem Solving', 'authors': 'Sen Yang, Yafu Li, Wai Lam, Yu Cheng', 'link': 'https://arxiv.org/abs/2502.18873', 'abstract': "Large language models (LLMs) often struggle with complex reasoning tasks due to their limitations in addressing the vast reasoning space and inherent ambiguities of natural language. We propose the Mixture-of-Search-Agents (MoSA) paradigm, a novel approach leveraging the collective expertise of multiple LLMs to enhance search-based reasoning. MoSA integrates diverse reasoning pathways by combining independent exploration with iterative refinement among LLMs, mitigating the limitations of single-model approaches. Using Monte Carlo Tree Search (MCTS) as a backbone, MoSA enables multiple agents to propose and aggregate reasoning steps, resulting in improved accuracy. Our comprehensive evaluation across four reasoning benchmarks demonstrates MoSA's consistent performance improvements over single-agent and other multi-agent baselines, particularly in complex mathematical and commonsense reasoning tasks.", 'abstract_zh': '大型语言模型（LLMs）常常在复杂推理任务中遇到困难，这是因为它们在处理广阔推理空间和自然语言固有的歧义性方面存在局限。我们提出了一种新的混合搜索代理（MoSA）范式，该范式通过多LLM协同工作来增强基于搜索的推理能力。MoSA 通过结合独立探索与迭代精炼，整合了多条不同的推理路径，从而缓解了单模型方法的局限性。MoSA 以蒙特卡洛树搜索（MCTS）为基础，允许多个代理提出并聚合推理步骤，从而提高了准确性。我们在四个推理基准上的全面评估表明，MoSA 在复杂数学和常识推理任务中相较于单代理和其他多代理基线方法具有一致性性能提升。', 'title_zh': '多大型语言模型协作搜索以解决复杂问题'}
{'arxiv_id': 'arXiv:2502.18836', 'title': 'REALM-Bench: A Real-World Planning Benchmark for LLMs and Multi-Agent Systems', 'authors': 'Longling Geng, Edward Y. Chang', 'link': 'https://arxiv.org/abs/2502.18836', 'abstract': 'This benchmark suite provides a comprehensive evaluation framework for assessing both individual LLMs and multi-agent systems in real-world planning scenarios. The suite encompasses eleven designed problems that progress from basic to highly complex, incorporating key aspects such as multi-agent coordination, inter-agent dependencies, and dynamic environmental disruptions. Each problem can be scaled along three dimensions: the number of parallel planning threads, the complexity of inter-dependencies, and the frequency of unexpected disruptions requiring real-time adaptation. The benchmark includes detailed specifications, evaluation metrics, and baseline implementations using contemporary frameworks like LangGraph, enabling rigorous testing of both single-agent and multi-agent planning capabilities. Through standardized evaluation criteria and scalable complexity, this benchmark aims to drive progress in developing more robust and adaptable AI planning systems for real-world applications.', 'abstract_zh': '这个基准套件提供了一套全面的评估框架，用于评估单个大规模语言模型和多智能体系统在真实世界规划场景中的表现。该套件包含 eleven 个设计问题，从基础逐步进展到高度复杂，并融入了多智能体协调、智能体间依赖关系以及动态环境干扰等关键方面。每个问题可以在三个维度上进行扩展：并行规划线程的数量、相互依赖关系的复杂性，以及需要实时适应的意外干扰的频率。该基准套件包括详细的规范、评估指标以及基于当代框架（如 LangGraph）的基线实现，从而能严格测试单智能体和多智能体规划能力。通过标准化的评估标准和可扩展的复杂性，该基准套件旨在推动开发更具鲁棒性和适应性的 AI 规划系统，应用于实际应用中。', 'title_zh': 'REALM-Bench: 一个面向LLMs和多智能体系统的实际规划基准'}
{'arxiv_id': 'arXiv:2502.18822', 'title': 'Data-Efficient Multi-Agent Spatial Planning with LLMs', 'authors': 'Huangyuan Su, Aaron Walsman, Daniel Garces, Sham Kakade, Stephanie Gil', 'link': 'https://arxiv.org/abs/2502.18822', 'abstract': "In this project, our goal is to determine how to leverage the world-knowledge of pretrained large language models for efficient and robust learning in multiagent decision making. We examine this in a taxi routing and assignment problem where agents must decide how to best pick up passengers in order to minimize overall waiting time. While this problem is situated on a graphical road network, we show that with the proper prompting zero-shot performance is quite strong on this task. Furthermore, with limited fine-tuning along with the one-at-a-time rollout algorithm for look ahead, LLMs can out-compete existing approaches with 50 times fewer environmental interactions. We also explore the benefits of various linguistic prompting approaches and show that including certain easy-to-compute information in the prompt significantly improves performance. Finally, we highlight the LLM's built-in semantic understanding, showing its ability to adapt to environmental factors through simple prompts.", 'abstract_zh': '本项目旨在探讨如何利用预训练大语言模型的世界知识，实现多agent决策制定中的高效和稳健学习。我们通过出租车路线规划和分配问题来检验这一方法，该问题要求agents决定如何最优地接乘客以最小化总体等待时间。尽管该问题基于图形道路网络，我们证明通过适当的提示，零样本性能在这一任务上表现较强。此外，通过有限的微调与单次展开前瞻算法相结合，LLMs可以在与现有方法相当的性能下，通过较少的环境交互次数（减少50倍）实现超越。我们还探讨了不同语言提示方法的益处，并表明在提示中包含某些易于计算的信息显著提高了性能。最后，我们强调了LLMs内置的语义理解能力，展示了其通过简单的提示适应环境因素的能力。', 'title_zh': 'LLMs驱动的数据高效多智能体空间规划'}
{'arxiv_id': 'arXiv:2502.18810', 'title': 'Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal and Redundancy Removal', 'authors': 'Weipeng Jiang, Juan Zhai, Shiqing Ma, Ziyan Lei, Xiaofei Xie, Yige Wang, Chao Shen', 'link': 'https://arxiv.org/abs/2502.18810', 'abstract': 'In recent years, Large Language Models (LLMs) have faced increasing demands to selectively remove sensitive information, protect privacy, and comply with copyright regulations through unlearning, by Machine Unlearning. While evaluating unlearning effectiveness is crucial, existing benchmarks are limited in scale and comprehensiveness, typically containing only a few hundred test cases. We identify two critical challenges in generating holistic audit datasets: ensuring audit adequacy and handling knowledge redundancy between forget and retain dataset. To address these challenges, we propose HANKER, an automated framework for holistic audit dataset generation leveraging knowledge graphs to achieve fine-grained coverage and eliminate redundant knowledge. Applying HANKER to the popular MUSE benchmark, we successfully generated over 69,000 and 111,000 audit cases for the News and Books datasets respectively, identifying thousands of knowledge memorization instances that the previous benchmark failed to detect. Our empirical analysis uncovers how knowledge redundancy significantly skews unlearning effectiveness metrics, with redundant instances artificially inflating the observed memorization measurements ROUGE from 19.7% to 26.1% and Entailment Scores from 32.4% to 35.2%, highlighting the necessity of systematic deduplication for accurate assessment.', 'abstract_zh': '近年来，大型语言模型（LLMs）在通过机器忘却技术选择性地去除敏感信息、保护隐私并遵守版权法规方面面临着不断增加的需求。评估忘却是至关重要的，但现有的基准在规模和全面性方面存在局限，通常仅包含几百个测试案例。我们识别出生成综合审计数据集的两个关键挑战：确保审计充分性和处理忘却集和保留集之间的知识冗余。为应对这些挑战，我们提出了一种名为HANKER的自动化框架，利用知识图谱实现细粒度覆盖并消除冗余知识。将HANKER应用于流行的MUSE基准，我们成功为新闻和书籍数据集分别生成了超过69,000和111,000个审计案例，发现先前基准未能检测到成千上万个知识记忆实例。我们的实证分析揭示了知识冗余如何显著扭曲忘却效果指标，冗余实例使观察到的记忆测量值ROUGE从19.7%提升至26.1%，Entailment Scores从32.4%提升至35.2%，强调了系统去重对于准确评估的必要性。', 'title_zh': '通过知识图谱遍历和冗余移除实现的LLM遗忘数据集生成方法'}
{'arxiv_id': 'arXiv:2502.18744', 'title': 'Like Father, Like Son: Kinship-Aware Preference Mapping (KARMA) for Automatic Alignment in Large Language Models', 'authors': 'Jeesu Jung, Chanjun Park, Sangkeun Jung', 'link': 'https://arxiv.org/abs/2502.18744', 'abstract': 'Recent advancements in Large Language Model (LLM) alignment have sought to mitigate the cost of human annotations by leveraging pretrained models to generate preference data. However, existing methods often compare responses from models with substantially different capabilities, yielding superficial distinctions that fail to provide meaningful guidance on what constitutes a superior response. To address this limitation, we propose Kinship-Aware pReference MApping (KARMA), a novel framework that systematically pairs responses from models with comparable competencies. By constraining preference comparisons to outputs of similar complexity and quality, KARMA enhances the informativeness of preference data and improves the granularity of alignment signals. Empirical evaluations demonstrate that our kinship-aware approach leads to more consistent and interpretable alignment outcomes, ultimately facilitating a more principled and reliable pathway for aligning LLM behavior with human preferences.', 'abstract_zh': 'Recent advancements in Large Language Model (LLM) alignment have sought to mitigate the cost of human annotations by leveraging pretrained models to generate preference data. However, existing methods often compare responses from models with substantially different capabilities, yielding superficial distinctions that fail to provide meaningful guidance on what constitutes a superior response. To address this limitation, we propose Kinship-Aware Preference Mapping (KARMA), a novel framework that systematically pairs responses from models with comparable competencies. By constraining preference comparisons to outputs of similar complexity and quality, KARMA enhances the informativeness of preference data and improves the granularity of alignment signals. Empirical evaluations demonstrate that our kinship-aware approach leads to more consistent and interpretable alignment outcomes, ultimately facilitating a more principled and reliable pathway for aligning LLM behavior with human preferences.\n\n翻译后的标题：\n基于亲和性感知偏好映射的大型语言模型对齐进展', 'title_zh': '父似子：亲缘意识偏好映射（KARMA）在大型语言模型中的自动对齐'}
{'arxiv_id': 'arXiv:2502.18725', 'title': 'Talking to the brain: Using Large Language Models as Proxies to Model Brain Semantic Representation', 'authors': 'Xin Liu, Ziyue Zhang, Jingxin Nie', 'link': 'https://arxiv.org/abs/2502.18725', 'abstract': 'Traditional psychological experiments utilizing naturalistic stimuli face challenges in manual annotation and ecological validity. To address this, we introduce a novel paradigm leveraging multimodal large language models (LLMs) as proxies to extract rich semantic information from naturalistic images through a Visual Question Answering (VQA) strategy for analyzing human visual semantic representation. LLM-derived representations successfully predict established neural activity patterns measured by fMRI (e.g., faces, buildings), validating its feasibility and revealing hierarchical semantic organization across cortical regions. A brain semantic network constructed from LLM-derived representations identifies meaningful clusters reflecting functional and contextual associations. This innovative methodology offers a powerful solution for investigating brain semantic organization with naturalistic stimuli, overcoming limitations of traditional annotation methods and paving the way for more ecologically valid explorations of human cognition.', 'abstract_zh': '利用多模态大型语言模型作为代理通过视觉问答策略从自然图像中提取丰富语义信息，以解决传统自然刺激心理实验的手动标注挑战和生态效度问题：一种基于语言模型衍生表征构建脑语义网络的方法', 'title_zh': '与大脑对话：使用大型语言模型作为代理建模大脑语义表示'}
{'arxiv_id': 'arXiv:2502.18712', 'title': 'TrajLLM: A Modular LLM-Enhanced Agent-Based Framework for Realistic Human Trajectory Simulation', 'authors': 'Chenlu Ju, Jiaxin Liu, Shobhit Sinha, Hao Xue, Flora Salim', 'link': 'https://arxiv.org/abs/2502.18712', 'abstract': "This work leverages Large Language Models (LLMs) to simulate human mobility, addressing challenges like high costs and privacy concerns in traditional models. Our hierarchical framework integrates persona generation, activity selection, and destination prediction, using real-world demographic and psychological data to create realistic movement patterns. Both physical models and language models are employed to explore and demonstrate different methodologies for human mobility simulation. By structuring data with summarization and weighted density metrics, the system ensures scalable memory management while retaining actionable insights. Preliminary results indicate that LLM-driven simulations align with observed real-world patterns, offering scalable, interpretable insights for social problems such as urban planning, traffic management, and public health. The framework's ability to dynamically generate personas and activities enables it to provide adaptable and realistic daily routines. This study demonstrates the transformative potential of LLMs in advancing mobility modeling for societal and urban applications. The source code and interactive demo for our framework are available at this https URL.", 'abstract_zh': '本研究利用大型语言模型（LLMs）模拟人类移动，以应对传统模型中成本高和隐私问题的挑战。该分层框架整合了个性生成、活动选择和目的地预测，并使用现实世界的人口统计和心理数据来创建现实的移动模式。物理模型和语言模型都被应用于探索和展示不同的人类移动模拟方法。通过使用总结和加权密度度量结构化数据，系统确保了可扩展的内存管理，同时保留了可操作的洞察。初步结果显示，由LLM驱动的模拟与观察到的实际模式相符，提供了适用于社会问题如城市规划、交通管理及公共卫生的可扩展且可解释的洞察。该框架能够动态生成个性和活动，使其能够提供可适应且现实的日常安排。本研究展示了LLMs在促进社会和城市应用中的移动建模方面的变革潜力。有关该框架的源代码和交互式演示可在以下链接访问：this https URL。', 'title_zh': 'TrajLLM：一种基于代理的模块化大语言模型增强现实人类轨迹模拟框架'}
{'arxiv_id': 'arXiv:2502.18652', 'title': 'Independent Mobility GPT (IDM-GPT): A Self-Supervised Multi-Agent Large Language Model Framework for Customized Traffic Mobility Analysis Using Machine Learning Models', 'authors': 'Fengze Yang, Xiaoyue Cathy Liu, Lingjiu Lu, Bingzhang Wang, Chenxi', 'link': 'https://arxiv.org/abs/2502.18652', 'abstract': 'With the urbanization process, an increasing number of sensors are being deployed in transportation systems, leading to an explosion of big data. To harness the power of this vast transportation data, various machine learning (ML) and artificial intelligence (AI) methods have been introduced to address numerous transportation challenges. However, these methods often require significant investment in data collection, processing, storage, and the employment of professionals with expertise in transportation and ML. Additionally, privacy issues are a major concern when processing data for real-world traffic control and management. To address these challenges, the research team proposes an innovative Multi-agent framework named Independent Mobility GPT (IDM-GPT) based on large language models (LLMs) for customized traffic analysis, management suggestions, and privacy preservation. IDM-GPT efficiently connects users, transportation databases, and ML models economically. IDM-GPT trains, customizes, and applies various LLM-based AI agents for multiple functions, including user query comprehension, prompts optimization, data analysis, model selection, and performance evaluation and enhancement. With IDM-GPT, users without any background in transportation or ML can efficiently and intuitively obtain data analysis and customized suggestions in near real-time based on their questions. Experimental results demonstrate that IDM-GPT delivers satisfactory performance across multiple traffic-related tasks, providing comprehensive and actionable insights that support effective traffic management and urban mobility improvement.', 'abstract_zh': '基于大规模语言模型的独立移动GPT多agent框架：定制化交通分析与隐私保护', 'title_zh': '独立移动GPT（IDM-GPT）：一种基于自我监督的多agent大型语言模型框架，用于使用机器学习模型进行个性化交通移动分析'}
{'arxiv_id': 'arXiv:2502.18632', 'title': 'Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems', 'authors': 'Zhangqi Duan, Nigel Fernandez, Sri Kanakadandi, Bita Akram, Andrew Lan', 'link': 'https://arxiv.org/abs/2502.18632', 'abstract': 'Knowledge components (KCs) mapped to problems help model student learning, tracking their mastery levels on fine-grained skills thereby facilitating personalized learning and feedback in online learning platforms. However, crafting and tagging KCs to problems, traditionally performed by human domain experts, is highly labor-intensive. We present a fully automated, LLM-based pipeline for KC generation and tagging for open-ended programming problems. We also develop an LLM-based knowledge tracing (KT) framework to leverage these LLM-generated KCs, which we refer to as KCGen-KT. We conduct extensive quantitative and qualitative evaluations validating the effectiveness of KCGen-KT. On a real-world dataset of student code submissions to open-ended programming problems, KCGen-KT outperforms existing KT methods. We investigate the learning curves of generated KCs and show that LLM-generated KCs have a comparable level-of-fit to human-written KCs under the performance factor analysis (PFA) model. We also conduct a human evaluation to show that the KC tagging accuracy of our pipeline is reasonably accurate when compared to that by human domain experts.', 'abstract_zh': '知识组件（KCs）映射到问题有助于建模学生学习，追踪他们在细粒度技能上的掌握水平，从而在在线学习平台中促进个性化学习和反馈。然而，传统上由人类领域专家完成的KCs的构建和标记工作非常耗时。我们提出了一种完全自动化的基于LLM的管道，用于生成和标记开放编程问题的KCs。我们还开发了一种基于LLM的知识追踪（KT）框架，利用这些由LLM生成的KCs，称之为KCGen-KT。我们进行了广泛的定量和定性评估，验证了KCGen-KT的有效性。在真实世界的学生代码提交数据集上，KCGen-KT优于现有KT方法。我们研究了生成KCs的学习曲线，并在性能因素分析（PFA）模型下展示了由LLM生成的KCs与人工编写的KCs具有相当拟合度。此外，我们进行了一项人工评估，表明与领域专家相比，我们管道中的KC标记准确性是合理的。', 'title_zh': '自动化知识组件生成与编程问题知识追踪'}
{'arxiv_id': 'arXiv:2502.18532', 'title': 'CuDIP: Enhancing Theorem Proving in LLMs via Curriculum Learning-based Direct Preference Optimization', 'authors': 'Shuming Shi, Ruobing Zuo, Gaolei He, Jianlin Wang, Chenyang Xu, Zhengfeng Yang', 'link': 'https://arxiv.org/abs/2502.18532', 'abstract': 'Automated theorem proving (ATP) is one of the most challenging mathematical reasoning tasks for Large Language Models (LLMs). Most existing LLM-based ATP methods rely on supervised fine-tuning, which results in a limited alignment between the theorem proving process and human preferences. Direct Preference Optimization (DPO), which aligns LLMs with human preferences, has shown positive effects for certain tasks. However, the lack of high-quality preference data for theorem proving presents a significant challenge. In this paper, we innovatively apply DPO to formal automated theorem proving and introduces a Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP) method. Specifically, we propose a method for constructing preference data which utilizes LLMs and existing theorem proving data to enhance the diversity of the preference data while reducing the reliance on human preference annotations. We then integrate this preference data construction method with curriculum learning to iteratively fine-tune the theorem proving model through DPO. Experimental results on the MiniF2F and ProofNet datasets demonstrate the effectiveness of the proposed method.', 'abstract_zh': 'Automated定理证明中的直接偏好优化 Curriculum Learning导向的迭代定理证明方法（CuDIP）', 'title_zh': 'CuDIP: 基于课程学习直接偏好优化增强的大语言模型定理证明'}
{'arxiv_id': 'arXiv:2502.18531', 'title': 'Enhancing Hepatopathy Clinical Trial Efficiency: A Secure, Large Language Model-Powered Pre-Screening Pipeline', 'authors': 'Xiongbin Gui, Hanlin Lv, Xiao Wang, Longting Lv, Yi Xiao, Lei Wang', 'link': 'https://arxiv.org/abs/2502.18531', 'abstract': "Background: Recruitment for cohorts involving complex liver diseases, such as hepatocellular carcinoma and liver cirrhosis, often requires interpreting semantically complex criteria. Traditional manual screening methods are time-consuming and prone to errors. While AI-powered pre-screening offers potential solutions, challenges remain regarding accuracy, efficiency, and data privacy. Methods: We developed a novel patient pre-screening pipeline that leverages clinical expertise to guide the precise, safe, and efficient application of large language models. The pipeline breaks down complex criteria into a series of composite questions and then employs two strategies to perform semantic question-answering through electronic health records - (1) Pathway A, Anthropomorphized Experts' Chain of Thought strategy, and (2) Pathway B, Preset Stances within an Agent Collaboration strategy, particularly in managing complex clinical reasoning scenarios. The pipeline is evaluated on three key metrics-precision, time consumption, and counterfactual inference - at both the question and criterion levels. Results: Our pipeline achieved high precision (0.921, in criteria level) and efficiency (0.44s per task). Pathway B excelled in complex reasoning, while Pathway A was effective in precise data extraction with faster processing times. Both pathways achieved comparable precision. The pipeline showed promising results in hepatocellular carcinoma (0.878) and cirrhosis trials (0.843). Conclusions: This data-secure and time-efficient pipeline shows high precision in hepatopathy trials, providing promising solutions for streamlining clinical trial workflows. Its efficiency and adaptability make it suitable for improving patient recruitment. And its capability to function in resource-constrained environments further enhances its utility in clinical settings.", 'abstract_zh': '背景：涉及复杂肝脏疾病（如肝细胞癌和肝硬化）的队列研究通常需要解释语义复杂的标准。传统的手动筛选方法耗时且易出错。尽管AI驱动的预筛选提供了潜在解决方案，但在准确性和效率以及数据隐私方面仍存在挑战。方法：我们开发了一种新型患者预筛选流程，利用临床专业知识指导大型语言模型的精确、安全和高效应用。该流程将复杂的标准分解为一系列复合问题，然后采用两种策略通过电子健康记录进行语义问答——（1）路径A，类人专家思维链策略；（2）路径B，代理合作框架内预设立场策略，特别是在处理复杂的临床推理场景时。该流程在问题和标准层面通过三个关键指标——精确度、时间消耗和反事实推理——进行了评估。结果：我们的流程在标准层面实现了高精确度（0.921）和高效率（每任务0.44秒）。路径B在复杂推理方面表现出色，而路径A在精确的数据提取方面更为有效且处理速度更快。两种路径在精确度方面达到相当水平。该流程在肝细胞癌（0.878）和肝硬化试验（0.843）中显示出有希望的结果。结论：这种数据安全、时间高效的流程在肝脏疾病试验中表现出高精确度，为简化临床试验流程提供了有希望的解决方案。其高效性和适应性使其适用于提高患者招募，同时其在资源受限环境中运行的能力进一步增强了其在临床设置中的实用性。', 'title_zh': '提升肝病临床试验效率：一种安全的大语言模型驱动的预筛查流水线'}
{'arxiv_id': 'arXiv:2502.19416', 'title': 'Norm Growth and Stability Challenges in Localized Sequential Knowledge Editing', 'authors': 'Akshat Gupta, Christine Fang, Atahan Ozdemir, Maochuan Lu, Ahmed Alaa, Thomas Hartvigsen, Gopala Anumanchipalli', 'link': 'https://arxiv.org/abs/2502.19416', 'abstract': 'This study investigates the impact of localized updates to large language models (LLMs), specifically in the context of knowledge editing - a task aimed at incorporating or modifying specific facts without altering broader model capabilities. We first show that across different post-training interventions like continuous pre-training, full fine-tuning and LORA-based fine-tuning, the Frobenius norm of the updated matrices always increases. This increasing norm is especially detrimental for localized knowledge editing, where only a subset of matrices are updated in a model . We reveal a consistent phenomenon across various editing techniques, including fine-tuning, hypernetwork-based approaches, and locate-and-edit methods: the norm of the updated matrix invariably increases with successive updates. Such growth disrupts model balance, particularly when isolated matrices are updated while the rest of the model remains static, leading to potential instability and degradation of downstream performance. Upon deeper investigations of the intermediate activation vectors, we find that the norm of internal activations decreases and is accompanied by shifts in the subspaces occupied by these activations, which shows that these activation vectors now occupy completely different regions in the representation space compared to the unedited model. With our paper, we highlight the technical challenges with continuous and localized sequential knowledge editing and their implications for maintaining model stability and utility.', 'abstract_zh': '本研究调查了大型语言模型（LLMs）局部更新对其知识编辑任务的影响，知识编辑旨在 incorporaring 或修改特定事实而不改变模型的 broader 能力。我们首先表明，在不同的后训练干预措施如连续预训练、全量微调和 LORA 基础微调中，更新矩阵的 Frobenius 范数始终增加。这种范数的增加对局部知识编辑尤为不利，在这种编辑中只有模型的一部分矩阵被更新。我们揭示了各种编辑技术（包括微调、超网络方法和查找并编辑方法）中的一致现象：随着更新次数的增加，更新矩阵的范数不可避免地增加。这种增长破坏了模型的平衡，特别是在仅更新孤立矩阵而其余模型保持不变的情况下，可能导致潜在的不稳定性和下游性能退化。通过对中间激活向量的进一步研究，我们发现内部激活的范数减小，并且伴随这些激活所在的子空间发生变化，显示这些激活向量现在在表示空间中占据了完全不同的区域，与未编辑的模型完全不同。通过本文，我们突出显示了连续和局部顺序知识编辑的技术挑战及其对保持模型稳定性和实用性的影响。', 'title_zh': '局部序贯知识编辑中的范式增长与稳定性挑战'}
{'arxiv_id': 'arXiv:2502.19413', 'title': 'Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs', 'authors': 'Christoph Schuhmann, Gollam Rabby, Ameya Prabhu, Tawsif Ahmed, Andreas Hochlehnert, Huu Nguyen, Nick Akinci Heidrich, Ludwig Schmidt, Robert Kaczmarczyk, Sören Auer, Jenia Jitsev, Matthias Bethge', 'link': 'https://arxiv.org/abs/2502.19413', 'abstract': 'Paywalls, licenses and copyright rules often restrict the broad dissemination and reuse of scientific knowledge. We take the position that it is both legally and technically feasible to extract the scientific knowledge in scholarly texts. Current methods, like text embeddings, fail to reliably preserve factual content, and simple paraphrasing may not be legally sound. We urge the community to adopt a new idea: convert scholarly documents into Knowledge Units using LLMs. These units use structured data capturing entities, attributes and relationships without stylistic content. We provide evidence that Knowledge Units: (1) form a legally defensible framework for sharing knowledge from copyrighted research texts, based on legal analyses of German copyright law and U.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from original text, measured by MCQ performance on facts from the original copyrighted text across four research domains. Freeing scientific knowledge from copyright promises transformative benefits for scientific research and education by allowing language models to reuse important facts from copyrighted text. To support this, we share open-source tools for converting research documents into Knowledge Units. Overall, our work posits the feasibility of democratizing access to scientific knowledge while respecting copyright.', 'abstract_zh': '付墙、许可协议和版权规则往往限制了科学知识的广泛传播和再利用。我们认为，从学术文本中提取科学知识在法律和技术上都是可行的。当前的方法，如文本嵌入，无法可靠地保留事实内容，而简单的改写可能不符合法律要求。我们呼吁学术界采纳一个新思路：使用大规模语言模型将学术文档转换为知识单元。这些单元使用结构化数据来捕捉实体、属性和关系，而不包含风格内容。我们提供了证据表明，知识单元：（1）基于德国版权法和美国公平使用原则的法律分析，构成了分享受版权的研究文本知识的合法框架；（2）在四个研究领域内，通过多项选择题测试事实的性能衡量，保留了约95%的原始事实知识。释放科学知识的版权有望对科学研究和教育产生变革性的影响，这将允许语言模型重新使用受版权保护文本的重要事实。为此，我们分享了将研究文档转换为知识单元的开源工具。总体而言，我们的工作提出了在尊重版权的同时使科学知识民主化访问的可能性。', 'title_zh': '项目亚历山大：通过大语言模型解放科学知识的版权束缚'}
{'arxiv_id': 'arXiv:2502.19411', 'title': 'Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs', 'authors': 'Dayu Yang, Tianyang Liu, Daoan Zhang, Antoine Simoulin, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Xin Qian, Grey Yang, Jiebo Luo, Julian McAuley', 'link': 'https://arxiv.org/abs/2502.19411', 'abstract': "In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLM's performance in both areas.", 'abstract_zh': '在大型语言模型中，代码和推理相互增强：代码提供了一个抽象的、模块化的、逻辑驱动的结构，支持推理，而推理将高层次的目标转化为更小的、可执行的步骤，驱动更高级的代码智能。在本研究中，我们考察代码作为增强推理的结构化媒介的作用：它提供了可验证的执行路径，强制进行逻辑分解，并使运行时验证成为可能。我们还探讨了推理改进如何将代码智能从基本的完成转变为高级能力，从而通过计划和调试使模型能够应对复杂的软件工程任务。最后，我们确定了关键挑战，并提出了未来的研究方向，以加强这种协同作用，最终在两个领域提升LLM的性能。', 'title_zh': '代码促进思考，思考驱动代码：基于代码增强的推理与推理驱动的代码智能综述'}
{'arxiv_id': 'arXiv:2502.19410', 'title': 'Less or More: Towards Glanceable Explanations for LLM Recommendations Using Ultra-Small Devices', 'authors': 'Xinru Wang, Mengjie Yu, Hannah Nguyen, Michael Iuzzolino, Tianyi Wang, Peiqi Tang, Natasha Lynova, Co Tran, Ting Zhang, Naveen Sendhilnathan, Hrvoje Benko, Haijun Xia, Tanya Jonker', 'link': 'https://arxiv.org/abs/2502.19410', 'abstract': "Large Language Models (LLMs) have shown remarkable potential in recommending everyday actions as personal AI assistants, while Explainable AI (XAI) techniques are being increasingly utilized to help users understand why a recommendation is given. Personal AI assistants today are often located on ultra-small devices such as smartwatches, which have limited screen space. The verbosity of LLM-generated explanations, however, makes it challenging to deliver glanceable LLM explanations on such ultra-small devices. To address this, we explored 1) spatially structuring an LLM's explanation text using defined contextual components during prompting and 2) presenting temporally adaptive explanations to users based on confidence levels. We conducted a user study to understand how these approaches impacted user experiences when interacting with LLM recommendations and explanations on ultra-small devices. The results showed that structured explanations reduced users' time to action and cognitive load when reading an explanation. Always-on structured explanations increased users' acceptance of AI recommendations. However, users were less satisfied with structured explanations compared to unstructured ones due to their lack of sufficient, readable details. Additionally, adaptively presenting structured explanations was less effective at improving user perceptions of the AI compared to the always-on structured explanations. Together with users' interview feedback, the results led to design implications to be mindful of when personalizing the content and timing of LLM explanations that are displayed on ultra-small devices.", 'abstract_zh': '超小型设备上可解释的大语言模型推荐及解释的研究', 'title_zh': '少一点还是多一点：面向超小型设备的LLM推荐精要解释研究'}
{'arxiv_id': 'arXiv:2502.19363', 'title': 'DataMan: Data Manager for Pre-training Large Language Models', 'authors': 'Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, Junbo Zhao', 'link': 'https://arxiv.org/abs/2502.19363', 'abstract': "The performance emergence of large language models (LLMs) driven by data scaling laws makes the selection of pre-training data increasingly important. However, existing methods rely on limited heuristics and human intuition, lacking comprehensive and clear guidelines. To address this, we are inspired by ``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit its performance. As its pre-training capabilities are related to perplexity (PPL), we derive 14 quality criteria from the causes of text perplexity anomalies and introduce 15 common application domains to support domain mixing. In this paper, we train a Data Manager (DataMan) to learn quality ratings and domain recognition from pointwise rating, and use it to annotate a 447B token pre-training corpus with 14 quality ratings and domain type. Our experiments validate our approach, using DataMan to select 30B tokens to train a 1.3B-parameter language model, demonstrating significant improvements in in-context learning (ICL), perplexity, and instruction-following ability over the state-of-the-art baseline. The best-performing model, based on the Overall Score l=5 surpasses a model trained with 50% more data using uniform sampling. We continue pre-training with high-rated, domain-specific data annotated by DataMan to enhance domain-specific ICL performance and thus verify DataMan's domain mixing ability. Our findings emphasize the importance of quality ranking, the complementary nature of quality criteria, and their low correlation with perplexity, analyzing misalignment between PPL and ICL performance. We also thoroughly analyzed our pre-training dataset, examining its composition, the distribution of quality ratings, and the original document sources.", 'abstract_zh': '大型语言模型（LLM）性能的提升得益于数据规模法则，这使得预训练数据的选择越来越重要。然而，现有方法依赖于有限的启发式方法和人类直觉，缺乏全面和明确的指导方针。为了解决这一问题，我们借鉴了“反向思维”——提示LLM自我识别哪些标准对其性能有益。因为其预训练能力与困惑度（PPL）相关，我们从文本困惑度异常的原因中推导出14个质量标准，并引入了15个常见应用场景以支持领域混合。在本文中，我们训练了一个数据管理器（DataMan），使其从点wise评分中学习质量评级和领域识别，并使用它为一个447B令牌的预训练语料库标注14个质量评级和领域类型。我们的实验验证了该方法的有效性，使用DataMan选择30B令牌训练一个1.3B参数的语言模型，展示了在上下文学习（ICL）、困惑度和指令遵循能力方面相对于最先进的基线的显著改进。基于综合评分l=5的最佳模型超过了使用50%更多数据通过均匀抽样训练的模型。我们继续使用DataMan标注的高评分、领域特定数据进行预训练，以增强特定领域的ICL性能，从而验证了DataMan的领域混合能力。我们的研究强调了质量排名的重要性、质量标准之间的互补性以及它们与困惑度的低相关性，并分析了PPL与ICL性能之间的不一致性。我们还详细分析了预训练数据集的组成、质量评分的分布以及原始文档来源。', 'title_zh': 'DataMan: 大型语言模型预训练的数据管理器'}
{'arxiv_id': 'arXiv:2502.19347', 'title': 'Controlled Diversity: Length-optimized Natural Language Generation', 'authors': 'Diana Marie Schenke, Timo Baumann', 'link': 'https://arxiv.org/abs/2502.19347', 'abstract': "LLMs are not generally able to adjust the length of their outputs based on strict length requirements, a capability that would improve their usefulness in applications that require adherence to diverse user and system requirements. We present an approach to train LLMs to acquire this capability by augmenting existing data and applying existing fine-tuning techniques, which we compare based on the trained models' adherence to the length requirement and overall response quality relative to the baseline model. Our results demonstrate that these techniques can be successfully applied to train LLMs to adhere to length requirements, with the trained models generating texts which better align to the length requirements. Our results indicate that our method may change the response quality when using training data that was not generated by the baseline model. This allows simultaneous alignment to another training objective in certain scenarios, but is undesirable otherwise. Training on a dataset containing the model's own responses eliminates this issue.", 'abstract_zh': 'LLMs通常无法根据严格的长度要求调整其输出长度，这种能力将提高它们在需要遵守多样用户和系统要求的应用中的实用性。我们提出了一种通过扩展现有数据并应用现有微调技术来培训LLMs获得这种能力的方法，并基于训练模型遵守长度要求的程度和整体响应质量与基线模型的比较进行评估。我们的结果表明，这些技术可以成功应用于培训LLMs以遵守长度要求，从而使生成的文本更好地符合长度要求。我们的结果表明，在使用非基线模型生成的数据集上进行训练可能会改变响应质量。这在某些情况下可以同时满足另一个训练目标，但在其他情况下是不希望的。在包含模型自身响应的数据集上进行训练可以解决这一问题。', 'title_zh': '控制多样性：长度优化的自然语言生成'}
{'arxiv_id': 'arXiv:2502.19328', 'title': 'Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems', 'authors': 'Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, Juanzi Li', 'link': 'https://arxiv.org/abs/2502.19328', 'abstract': 'Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (this https URL).', 'abstract_zh': '基于可验证正确性的代理奖励模型', 'title_zh': '代理人奖励建模：结合可验证正确性信号的人类偏好以构建可靠的奖励系统'}
{'arxiv_id': 'arXiv:2502.19320', 'title': "Shh, don't say that! Domain Certification in LLMs", 'authors': 'Cornelius Emde, Alasdair Paren, Preetham Arvind, Maxime Kayser, Tom Rainforth, Thomas Lukasiewicz, Bernard Ghanem, Philip H.S. Torr, Adel Bibi', 'link': 'https://arxiv.org/abs/2502.19320', 'abstract': 'Large language models (LLMs) are often deployed to perform constrained tasks, with narrow domains. For example, customer support bots can be built on top of LLMs, relying on their broad language understanding and capabilities to enhance performance. However, these LLMs are adversarially susceptible, potentially generating outputs outside the intended domain. To formalize, assess, and mitigate this risk, we introduce domain certification; a guarantee that accurately characterizes the out-of-domain behavior of language models. We then propose a simple yet effective approach, which we call VALID that provides adversarial bounds as a certificate. Finally, we evaluate our method across a diverse set of datasets, demonstrating that it yields meaningful certificates, which bound the probability of out-of-domain samples tightly with minimum penalty to refusal behavior.', 'abstract_zh': '大型语言模型（LLMs）常被部署执行受限任务，领域狭窄。例如，可以在LLMs之上构建客服机器人，依赖其广泛的语言理解和能力以提升性能。然而，这些LLMs对抗性易受攻击，可能会生成超出预定领域的输出。为此，我们通过引入领域认证来正式化、评估和缓解这一风险；领域认证是一种保证，准确描述语言模型的越域行为。我们提出了一种简单而有效的方法称为VALID，提供对抗性界值作为证书。我们最终在多样化的数据集上评估了该方法，展示了其能够紧密界定制外样本的概率，同时对拒绝行为的代价最小。', 'title_zh': "Shh, don't say that! LLMs的领域认证"}
{'arxiv_id': 'arXiv:2502.19312', 'title': 'FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users', 'authors': 'Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, Chelsea Finn', 'link': 'https://arxiv.org/abs/2502.19312', 'abstract': 'Effective personalization of LLMs is critical for a broad range of user-interfacing applications such as virtual assistants and content curation. Inspired by the strong in-context learning capabilities of LLMs, we propose Few-Shot Preference Optimization (FSPO), which reframes reward modeling as a meta-learning problem. Under this framework, an LLM learns to quickly adapt to a user via a few labeled preferences from that user, constructing a personalized reward function for them. Additionally, since real-world preference data is scarce and challenging to collect at scale, we propose careful design choices to construct synthetic preference datasets for personalization, generating over 1M synthetic personalized preferences using publicly available LLMs. In particular, to successfully transfer from synthetic data to real users, we find it crucial for the data to exhibit both high diversity and coherent, self-consistent structure. We evaluate FSPO on personalized open-ended generation for up to 1,500 synthetic users across across three domains: movie reviews, pedagogical adaptation based on educational background, and general question answering, along with a controlled human study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in generating responses that are personalized to synthetic users and a 72% winrate with real human users in open-ended question answering.', 'abstract_zh': 'Few-Shot Preference Optimization for Effective Personalization of LLMs', 'title_zh': 'FSPO: 少Shot偏好优化的合成偏好数据在大规模语言模型中的有效个性化'}
{'arxiv_id': 'arXiv:2502.19249', 'title': 'Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases', 'authors': 'Michael Y. Hu, Jackson Petty, Chuan Shi, William Merrill, Tal Linzen', 'link': 'https://arxiv.org/abs/2502.19249', 'abstract': "Pretraining language models on formal languages can improve their acquisition of natural language, but it is unclear which features of the formal language impart an inductive bias that leads to effective transfer. Drawing on insights from linguistics and complexity theory, we hypothesize that effective transfer occurs when the formal language both captures dependency structures in natural language and remains within the computational limitations of the model architecture. Focusing on transformers, we find that formal languages with both these properties enable language models to achieve lower loss on natural language and better linguistic generalization compared to other languages. In fact, pre-pretraining, or training on formal-then-natural language, reduces loss more efficiently than the same amount of natural language. For a 1B-parameter language model trained on roughly 1.6B tokens of natural language, pre-pretraining achieves the same loss and better linguistic generalization with a 33% smaller token budget. We also give mechanistic evidence of cross-task transfer from formal to natural language: attention heads acquired during formal language pretraining remain crucial for the model's performance on syntactic evaluations.", 'abstract_zh': '在正式语言上预训练语言模型可以提高其对自然语言的掌握，但不清楚是正式语言的哪些特性赋予了有效的迁移偏见。从语言学和复杂性理论中汲取灵感，我们假设当正式语言同时捕捉自然语言中的依赖结构并且保持在模型架构的计算限制之内时，有效迁移才会发生。聚焦于变压器模型，我们发现同时具备这两种特性的正式语言可以使语言模型在自然语言上获得更低的损失并展现出更好的语言通用性，与其他语言相比更是如此。事实上，先在正式语言上预训练再在自然语言上训练可以比同等量的自然语言训练更高效地降低损失。对于一个参数量为1B的語言模型，在大约1.6B令牌的自然语言上训练，先进行正式语言预训练再进行自然语言训练可以在减小33%令牌预算的情况下达到相同的损失并展现出更好的语言通用性。我们还提供了从正式语言到自然语言的跨任务迁移的机制性证据：在正式语言预训练过程中获得的注意力头对于模型在句法评估中的性能仍然至关重要。', 'title_zh': '介于电路与乔姆斯基之间：在形式语言上进行预训练赋予了语言偏见'}
{'arxiv_id': 'arXiv:2502.19207', 'title': 'FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge', 'authors': 'Nakyeong Yang, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung', 'link': 'https://arxiv.org/abs/2502.19207', 'abstract': 'Various studies have attempted to remove sensitive or private knowledge from a language model to prevent its unauthorized exposure. However, prior studies have overlooked the complex and interconnected nature of knowledge, where related knowledge must be carefully examined. Specifically, they have failed to evaluate whether an unlearning method faithfully erases interconnected knowledge that should be removed, retaining knowledge that appears relevant but exists in a completely different context. To resolve this problem, we first define a new concept called superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. Based on the definition, we introduce a new benchmark, FaithUn, to analyze and evaluate the faithfulness of unlearning in real-world knowledge QA settings. Furthermore, we propose a novel unlearning method, KLUE, which updates only knowledge-related neurons to achieve faithful unlearning. KLUE identifies knowledge neurons using an explainability method and updates only those neurons using selected unforgotten samples. Experimental results demonstrate that widely-used unlearning methods fail to ensure faithful unlearning, while our method shows significant effectiveness in real-world QA unlearning.', 'abstract_zh': '各种研究尝试通过从语言模型中移除敏感或私人信息来防止其未经授权的暴露。然而，先前的研究忽视了知识的复杂性和相互关联性，其中相关的知识必须仔细审查。具体而言，它们未能评估去学习方法是否忠实删除了应被移除的相关知识，而保留了看似相关但实际上存在于完全不同上下文中的知识。为了解决这一问题，我们首先引入了一个新的概念——表面去学习，指的是去学习方法要么未能删除应被移除的相关知识，要么无意中删除了无关知识。基于这一定义，我们引入了一个新的基准测试——FaithUn，用于分析和评估实际知识问答环境中去学习的忠实性。此外，我们提出了一种新型去学习方法——KLUE，该方法仅更新与知识相关的人工神经元以实现忠实去学习。KLUE 使用可解释性方法识别知识神经元，并仅使用选定的未遗忘样本更新这些神经元。实验结果表明，广泛使用的方法无法确保忠实去学习，而我们的方法在实际问答去学习中显示出显著的效果。', 'title_zh': 'FaithUn: 探究知识互联性以实现语言模型的忠实遗忘'}
{'arxiv_id': 'arXiv:2502.19193', 'title': 'Simulation of Language Evolution under Regulated Social Media Platforms: A Synergistic Approach of Large Language Models and Genetic Algorithms', 'authors': 'Jinyu Cai, Yusei Ishimizu, Mingyue Zhang, Munan Li, Jialong Li, Kenji Tei', 'link': 'https://arxiv.org/abs/2502.19193', 'abstract': 'Social media platforms frequently impose restrictive policies to moderate user content, prompting the emergence of creative evasion language strategies. This paper presents a multi-agent framework based on Large Language Models (LLMs) to simulate the iterative evolution of language strategies under regulatory constraints. In this framework, participant agents, as social media users, continuously evolve their language expression, while supervisory agents emulate platform-level regulation by assessing policy violations. To achieve a more faithful simulation, we employ a dual design of language strategies (constraint and expression) to differentiate conflicting goals and utilize an LLM-driven GA (Genetic Algorithm) for the selection, mutation, and crossover of language strategies. The framework is evaluated using two distinct scenarios: an abstract password game and a realistic simulated illegal pet trade scenario. Experimental results demonstrate that as the number of dialogue rounds increases, both the number of uninterrupted dialogue turns and the accuracy of information transmission improve significantly. Furthermore, a user study with 40 participants validates the real-world relevance of the generated dialogues and strategies. Moreover, ablation studies validate the importance of the GA, emphasizing its contribution to long-term adaptability and improved overall results.', 'abstract_zh': '基于大型语言模型的多代理框架：监管约束下语言策略的迭代演化模拟', 'title_zh': '受规制社交媒体平台上的语言进化模拟：大型语言模型与遗传算法的协同方法'}
{'arxiv_id': 'arXiv:2502.19160', 'title': 'Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models', 'authors': 'Rebekka Görge, Michael Mock, Héctor Allende-Cid', 'link': 'https://arxiv.org/abs/2502.19160', 'abstract': 'Social categories and stereotypes are embedded in language and can introduce data bias into Large Language Models (LLMs). Despite safeguards, these biases often persist in model behavior, potentially leading to representational harm in outputs. While sociolinguistic research provides valuable insights into the formation of stereotypes, NLP approaches for stereotype detection rarely draw on this foundation and often lack objectivity, precision, and interpretability. To fill this gap, in this work we propose a new approach that detects and quantifies the linguistic indicators of stereotypes in a sentence. We derive linguistic indicators from the Social Category and Stereotype Communication (SCSC) framework which indicate strong social category formulation and stereotyping in language, and use them to build a categorization scheme. To automate this approach, we instruct different LLMs using in-context learning to apply the approach to a sentence, where the LLM examines the linguistic properties and provides a basis for a fine-grained assessment. Based on an empirical evaluation of the importance of different linguistic indicators, we learn a scoring function that measures the linguistic indicators of a stereotype. Our annotations of stereotyped sentences show that these indicators are present in these sentences and explain the strength of a stereotype. In terms of model performance, our results show that the models generally perform well in detecting and classifying linguistic indicators of category labels used to denote a category, but sometimes struggle to correctly evaluate the associated behaviors and characteristics. Using more few-shot examples within the prompts, significantly improves performance. Model performance increases with size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that surpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.', 'abstract_zh': '社会类别和刻板印象嵌入语言中，并可能引入大规模语言模型的数据偏差。尽管有保护措施，这些偏差往往仍然存在于模型行为中，可能导致输出表示危害。尽管社会语用学研究为刻板印象的形成提供了有价值的见解，但用于刻板印象检测的NLP方法很少基于这一基础，且往往缺乏客观性、精确性和解释性。为填补这一空白，本文提出了一种新方法，用于检测和量化句子中的刻板印象语言指标。我们从社会类别和刻板印象沟通框架中推导出语言指标，这些指标表明语言中存在强烈的社会类别表述和刻板印象，并据此构建分类方案。为自动化这一方法，我们使用基于上下文学习指令不同的LLM将该方法应用于句子，LLM检查语言属性并提供细粒度评估的基础。根据对不同语言指标重要性的实证评估，我们学习了一种评分函数，用于衡量刻板印象的语言指标。我们的标注结果显示，这些指标存在于这些语句中，解释了刻板印象的强度。在模型性能方面，我们的结果显示，模型通常能够很好地检测和分类标记类别标签所表示类别的语言指标，但在正确评估相关行为和特征方面有时会遇到困难。在提示中使用更多的少样本示例显著提高了性能。随着模型规模的增大，Llama-3.3-70B-Instruct和GPT-4实现了可比较的、超越Mixtral-8x7B-Instruct、GPT-4-mini和Llama-3.1-8B-Instruct的结果。', 'title_zh': '使用大型语言模型检测刻板印象评估的语言指标'}
{'arxiv_id': 'arXiv:2502.19158', 'title': 'When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning', 'authors': 'Yijiang River Dong, Tiancheng Hu, Yinhong Liu, Ahmet Üstün, Nigel Collier', 'link': 'https://arxiv.org/abs/2502.19158', 'abstract': 'While Reinforcement Learning from Human Feedback (RLHF) is widely used to align Large Language Models (LLMs) with human preferences, it typically assumes homogeneous preferences across users, overlooking diverse human values and minority viewpoints. Although personalized preference learning addresses this by tailoring separate preferences for individual users, the field lacks standardized methods to assess its effectiveness. We present a multi-faceted evaluation framework that measures not only performance but also fairness, unintended effects, and adaptability across varying levels of preference divergence. Through extensive experiments comparing eight personalization methods across three preference datasets, we demonstrate that performance differences between methods could reach 36% when users strongly disagree, and personalization can introduce up to 20% safety misalignment. These findings highlight the critical need for holistic evaluation approaches to advance the development of more effective and inclusive preference learning systems.', 'abstract_zh': '基于人类反馈的强化学习（RLHF）在对齐大型语言模型（LLMs）与人类偏好方面广泛应用，但通常假设用户偏好 homogeneous，忽视了多样化的人类价值观和少数观点。尽管个性化偏好学习通过为每位用户量身定制不同的偏好来解决这一问题，但该领域缺乏评估其效果的标准方法。我们提出了一种多维度评估框架，不仅衡量性能，还评估公平性、意外效应和在不同偏好分歧水平下的适应性。通过在三个偏好数据集中比较八种个性化方法的广泛实验，我们发现当用户意见分歧强烈时，方法之间的性能差异可达到 36%，个性化可能导致高达 20% 的安全对齐偏差。这些发现强调了进行全面评估方法的迫切需求，以促进更有效和包容性的偏好学习系统的开发。', 'title_zh': '当个性化遭遇现实：个性化偏好学习的多维度分析'}
{'arxiv_id': 'arXiv:2502.19008', 'title': 'Binary Neural Networks for Large Language Model: A Survey', 'authors': 'Liangdong Liu, Zhitong Zheng, Cong Wang, Tianhuang Su, Zhenyu Yang', 'link': 'https://arxiv.org/abs/2502.19008', 'abstract': 'Large language models (LLMs) have wide applications in the field of natural language processing(NLP), such as GPT-4 and Llama. However, with the exponential growth of model parameter sizes, LLMs bring significant resource overheads. Low-bit quantization, as a key technique, reduces memory usage and computational demands by decreasing the bit-width of model parameters, activations, and gradients. Previous quantization methods for LLMs have largely employed Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ does not require any retraining of the original model, while QAT involves optimizing precision during training to achieve the best quantization parameters. The BitNet team proposed a radically different approach, where quantization is performed from the start of model training, utilizing low-precision binary weights during the training process. This approach has led to the emergence of many binary quantization techniques for large language models. This paper provides a comprehensive review of these binary quantization techniques. Specifically, we will introduce binary quantization techniques in deep neural networks and further explore their application to LLMs, reviewing their various contributions, implementations, and applications.', 'abstract_zh': '大型语言模型（LLMs）在自然语言处理（NLP）领域有着广泛的应用，如GPT-4和Llama。然而，随着模型参数规模的指数级增长，LLMs带来了显著的资源开销。低比特量化作为一种关键技术，通过降低模型参数、激活和梯度的位宽来减少内存使用和计算需求。先前针对LLMs的量化方法主要采用了后训练量化（PTQ）和量化感知训练（QAT）。PTQ不需要重新训练原始模型，而QAT则在训练过程中优化精度以达到最佳的量化参数。BitNet团队提出了一种截然不同的方法，从模型训练之初就开始进行量化，并在训练过程中使用低精度二进制权重。这一方法导致了许多针对大型语言模型的二进制量化技术的出现。本文提供了对这些二进制量化技术的全面综述。特别是在介绍深度神经网络中的二进制量化技术后，我们将进一步探索其在大型语言模型中的应用，综述它们的各种贡献、实现和应用。', 'title_zh': '大规模语言模型中的二值神经网络：一种综述'}
{'arxiv_id': 'arXiv:2502.19002', 'title': 'The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training', 'authors': 'Jinbo Wang, Mingze Wang, Zhanpeng Zhou, Junchi Yan, Weinan E, Lei Wu', 'link': 'https://arxiv.org/abs/2502.19002', 'abstract': "Transformers consist of diverse building blocks, such as embedding layers, normalization layers, self-attention mechanisms, and point-wise feedforward networks. Thus, understanding the differences and interactions among these blocks is important. In this paper, we uncover a clear Sharpness Disparity across these blocks, which emerges early in training and intriguingly persists throughout the training process. Motivated by this finding, we propose Blockwise Learning Rate (LR), a strategy that tailors the LR to each block's sharpness, accelerating large language model (LLM) pre-training. By integrating Blockwise LR into AdamW, we consistently achieve lower terminal loss and nearly $2\\times$ speedup compared to vanilla AdamW. We demonstrate this acceleration across GPT-2 and LLaMA, with model sizes ranging from 0.12B to 1.1B and datasets of OpenWebText and MiniPile. Finally, we incorporate Blockwise LR into Adam-mini (Zhang et al., 2024), a recently proposed memory-efficient variant of Adam, achieving a combined $2\\times$ speedup and $2\\times$ memory saving. These results underscore the potential of exploiting the sharpness disparity to improve LLM training.", 'abstract_zh': 'Transformers由嵌入层、规范化层、自注意力机制和点wise前馈网络等多种构建块组成。因此，理解这些构建块之间的差异及其相互作用至关重要。在本文中，我们揭示了这些构建块之间存在清晰的清晰度差异，这种差异在训练早期出现，并在整个训练过程中持续存在。受这一发现的启发，我们提出了一种块级学习率（Blockwise Learning Rate，BLR）策略，该策略根据每个构建块的清晰度调整学习率，加速大规模语言模型（LLM）的预训练。通过将块级学习率集成到AdamW中，我们一致实现了更低的终端损失，并且速度提高了近两倍。我们在GPT-2和LLaMA上展示了这种加速，模型大小从0.12B到1.1B不等，数据集包括OpenWebText和MiniPile。最后，我们将块级学习率集成到Adam-mini（Zhang et al., 2024）这一近期提出的大规模内存高效变体中，实现了两倍的速度和两倍的内存节省。这些结果突显了利用清晰度差异改进LLM训练的潜力。', 'title_zh': 'Transformer中sharpness disparity原理在语言模型预训练中的加速应用'}
{'arxiv_id': 'arXiv:2502.18980', 'title': 'PEToolLLM: Towards Personalized Tool Learning in Large Language Models', 'authors': 'Qiancheng Xu, Yongqi Li, Heming Xia, Fan Liu, Min Yang, Wenjie Li', 'link': 'https://arxiv.org/abs/2502.18980', 'abstract': "Tool learning has emerged as a promising direction by extending Large Language Models' (LLMs) capabilities with external tools. Existing tool learning studies primarily focus on the general-purpose tool-use capability, which addresses explicit user requirements in instructions. However, they overlook the importance of personalized tool-use capability, leading to an inability to handle implicit user preferences. To address the limitation, we first formulate the task of personalized tool learning, which integrates user's interaction history towards personalized tool usage. To fill the gap of missing benchmarks, we construct PEToolBench, featuring diverse user preferences reflected in interaction history under three distinct personalized settings, and encompassing a wide range of tool-use scenarios. Moreover, we propose a framework PEToolLLaMA to adapt LLMs to the personalized tool learning task, which is trained through supervised fine-tuning and direct preference optimization. Extensive experiments on PEToolBench demonstrate the superiority of PEToolLLaMA over existing LLMs.", 'abstract_zh': '个性化的工具学习通过将大型语言模型的能力与外部工具扩展相结合而崭露头角。现有的工具学习研究主要集中在通用工具使用能力上，这针对了指令中的明确用户需求。然而，它们忽略了个性化工具使用能力的重要性，导致无法处理隐含的用户偏好。为解决这一限制，我们首先定义了个性化工具学习任务，该任务将用户的历史交互整合到个性化工具使用中。为填补缺失的基准，我们构建了PEToolBench，它包含在三种不同个性化设置下反映不同用户偏好的交互历史，涵盖了广泛的工具使用场景。此外，我们提出了一种PEToolLLaMA框架，将大规模语言模型适应个性化工具学习任务，该框架通过监督微调和直接偏好优化进行训练。在PEToolBench上的广泛实验表明，PEToolLLaMA优于现有的大规模语言模型。', 'title_zh': 'PEToolLLM: 面向大型语言模型中个性化工具学习的研究'}
{'arxiv_id': 'arXiv:2502.18978', 'title': 'Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning', 'authors': 'Hongyi Cal, ie Li, Wenzhen Dong', 'link': 'https://arxiv.org/abs/2502.18978', 'abstract': "The effectiveness of instruction fine-tuning for Large Language Models is fundamentally constrained by the quality and efficiency of training datasets. This work introduces Low-Confidence Gold (LCG), a novel filtering framework that employs centroid-based clustering and confidence-guided selection for identifying valuable instruction pairs. Through a semi-supervised approach using a lightweight classifier trained on representative samples, LCG curates high-quality subsets while preserving data diversity. Experimental evaluation demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples achieve superior performance compared to existing methods, with substantial improvements on MT-bench and consistent gains across comprehensive evaluation metrics. The framework's efficacy while maintaining model performance establishes a promising direction for efficient instruction tuning.", 'abstract_zh': '大型语言模型指令微调的效果从根本上受到训练数据集质量和效率的限制。本文引入了一种新颖的过滤框架Low-Confidence Gold (LCG)，该框架采用基于质心的聚类和置信度引导的选择方法来识别有价值的指令对。通过轻量级分类器在代表性样本上进行半监督训练，LCG能够在保持数据多样性的同时精炼高质量的数据子集。实验评估表明，使用LCG过滤的6千样本子集微调的模型在MT-bench等现有方法上表现出更优性能，并在全面的评估指标中实现了显著改进。该框架在保持模型性能的同时有效性的验证为高效的指令微调指明了有前景的方向。', 'title_zh': '低置信度金标准：细化低置信度样本以实现高效的指令调优'}
{'arxiv_id': 'arXiv:2502.18940', 'title': 'MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical Capabilities of LLM Tutors', 'authors': 'Jakub Macina, Nico Daheim, Ido Hakimi, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan', 'link': 'https://arxiv.org/abs/2502.18940', 'abstract': 'Evaluating the pedagogical capabilities of AI-based tutoring models is critical for making guided progress in the field. Yet, we lack a reliable, easy-to-use, and simple-to-run evaluation that reflects the pedagogical abilities of models. To fill this gap, we present MathTutorBench, an open-source benchmark for holistic tutoring model evaluation. MathTutorBench contains a collection of datasets and metrics that broadly cover tutor abilities as defined by learning sciences research in dialog-based teaching. To score the pedagogical quality of open-ended teacher responses, we train a reward model and show it can discriminate expert from novice teacher responses with high accuracy. We evaluate a wide set of closed- and open-weight models on MathTutorBench and find that subject expertise, indicated by solving ability, does not immediately translate to good teaching. Rather, pedagogy and subject expertise appear to form a trade-off that is navigated by the degree of tutoring specialization of the model. Furthermore, tutoring appears to become more challenging in longer dialogs, where simpler questioning strategies begin to fail. We release the benchmark, code, and leaderboard openly to enable rapid benchmarking of future models.', 'abstract_zh': '基于AI的教学辅导模型的教育能力评估对于推动该领域的发展至关重要。然而，我们缺乏一种可靠、易于使用且简便的操作的评估方法来反映模型的教育能力。为填补这一空白，我们提出了MathTutorBench，这是一个开源的全方位教学辅导模型评估基准。MathTutorBench包含了一组涵盖学习科学研究中定义的基于对话教学的辅导能力的数据集和评估指标。为了评估开放型教师回答的质量，我们训练了一个奖励模型，并展示了其能够高效地区分专家与新手教师的回答。我们对多种闭合型和开放型权重模型在MathTutorBench上的表现进行了评估，发现解题能力并不能直接转化为良好的教学效果。教育策略与专业知识似乎形成一种权衡，这种权衡由模型的辅导专业化程度来导航。此外，较长的对话似乎使教学更具挑战性，其中简单的提问策略开始失效。我们公开发布基准、代码和排行榜，以便快速评估未来模型的表现。', 'title_zh': 'MathTutorBench: 一个衡量LLM导师开放式教学能力的标准测试'}
{'arxiv_id': 'arXiv:2502.18935', 'title': 'JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models', 'authors': 'Shuyi Liu, Simiao Cui, Haoran Bu, Yuming Shang, Xi Zhang', 'link': 'https://arxiv.org/abs/2502.18935', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities across various applications, highlighting the urgent need for comprehensive safety evaluations. In particular, the enhanced Chinese language proficiency of LLMs, combined with the unique characteristics and complexity of Chinese expressions, has driven the emergence of Chinese-specific benchmarks for safety assessment. However, these benchmarks generally fall short in effectively exposing LLM safety vulnerabilities. To address the gap, we introduce JailBench, the first comprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in LLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese context. To improve generation efficiency, we employ a novel Automatic Jailbreak Prompt Engineer (AJPE) framework for JailBench construction, which incorporates jailbreak techniques to enhance assessing effectiveness and leverages LLMs to automatically scale up the dataset through context-learning. The proposed JailBench is extensively evaluated over 13 mainstream LLMs and achieves the highest attack success rate against ChatGPT compared to existing Chinese benchmarks, underscoring its efficacy in identifying latent vulnerabilities in LLMs, as well as illustrating the substantial room for improvement in the security and trustworthiness of LLMs within the Chinese context. Our benchmark is publicly available at this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在各种应用中展现了出色的能力，凸显了进行全面安全性评估的紧迫需求。特别地，LLMs增强的中文语言能力，结合中文表达的独特特征和复杂性，推动了专门针对中文的安全评估基准的出现。然而，这些基准在有效揭示LLMs的安全漏洞方面仍存在不足。为填补这一空白，我们引入了JailBench，这是首个全面的中文基准，用于评估LLMs深层次的安全漏洞，具备针对中文语境定制的精细分层安全分类体系。为提高生成效率，我们采用了一种新颖的自动脱牢笼提示工程师（AJPE）框架来构建JailBench，该框架集成了脱牢笼技术以增强评估效果，并利用LLMs通过上下文学习自动扩展数据集。提出的JailBench在13款主流LLMs上进行了广泛评估，并在对抗ChatGPT时取得了最高的攻击成功率，验证了其在识别LLMs潜藏漏洞方面的有效性，并展示了在中文语境下提升LLMs的安全性和可信度的巨大改善空间。我们的基准可从此链接获取： this https URL。', 'title_zh': 'JailBench: 一种全面的中文安全评估基准模型'}
{'arxiv_id': 'arXiv:2502.18915', 'title': 'END: Early Noise Dropping for Efficient and Effective Context Denoising', 'authors': 'Hongye Jin, Pei Chen, Jingfeng Yang, Zhengyang Wang, Meng Jiang, Yifan Gao, Binxuan Huang, Xinyang Zhang, Zheng Li, Tianyi Liu, Huasheng Li, Bing Yin', 'link': 'https://arxiv.org/abs/2502.18915', 'abstract': "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, they are often distracted by irrelevant or noisy context in input sequences that degrades output quality. This problem affects both long- and short-context scenarios, such as retrieval-augmented generation, table question-answering, and in-context learning. We reveal that LLMs can implicitly identify whether input sequences contain useful information at early layers, prior to token generation. Leveraging this insight, we introduce Early Noise Dropping (\\textsc{END}), a novel approach to mitigate this issue without requiring fine-tuning the LLMs. \\textsc{END} segments input sequences into chunks and employs a linear prober on the early layers of LLMs to differentiate between informative and noisy chunks. By discarding noisy chunks early in the process, \\textsc{END} preserves critical information, reduces distraction, and lowers computational overhead. Extensive experiments demonstrate that \\textsc{END} significantly improves both performance and efficiency across different LLMs on multiple evaluation datasets. Furthermore, by investigating LLMs' implicit understanding to the input with the prober, this work also deepens understanding of how LLMs do reasoning with contexts internally.", 'abstract_zh': '大型语言模型（LLMs）在各种自然语言处理任务中展现了出色的表现。然而，它们经常会受到输入序列中无关或噪声信息的干扰，从而降低输出质量。这一问题影响了长上下文和短上下文场景，例如检索增强生成、表格问答和上下文学习。我们揭示，大型语言模型可以在生成标记之前，早期层就已经隐式地识别输入序列中是否包含有用信息。基于这一洞察，我们提出了早期噪声丢弃（\\textsc{END}）这一新颖的方法，无需微调大型语言模型即可缓解这一问题。\\textsc{END} 将输入序列分割成块，并在大型语言模型的早期层使用线性探测器来区分信息块和噪声块。通过早期丢弃噪声块，\\textsc{END} 保留了关键信息、减少了干扰并降低了计算开销。广泛的实验表明，\\textsc{END} 显著提高了不同大型语言模型在多个评估数据集上的性能和效率。此外，通过探究探测器对输入的隐式理解，这项工作进一步加深了对大型语言模型如何在内部进行上下文推理的理解。', 'title_zh': 'END: 早期噪声去除以实现高效有效的上下文去噪'}
{'arxiv_id': 'arXiv:2502.18874', 'title': 'Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework', 'authors': 'Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li', 'link': 'https://arxiv.org/abs/2502.18874', 'abstract': 'Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.', 'abstract_zh': '大型语言模型（LLMs）正被越来越多地用于各种场景下的自动化评估。以往的研究试图对开源LLMs进行微调，以复制强大专有模型（如GPT-4）的评估解释和判断，但这些方法大多仅限于在预定义的一般标准下的文本分析，导致对未见指令的适应性较低，并在评估定量和结构性约束的遵守情况时表现出不稳定。为解决这些局限性，我们提出了一种新颖的评估框架ARJudge，该框架能够自适应地制定评估标准，并结合文本驱动和代码驱动的分析来评估LLM的响应。ARJudge包括两个组件：一个已微调的Analyzer生成多维度的评估分析，以及一个无需微调的Refiner综合并优化所有分析以作出最终判断。我们构建了一个集成评估标准生成任务以及文本驱动和代码驱动分析生成的综合分析语料库来训练Analyzer。研究结果表明，ARJudge在有效性与稳健性方面优于现有微调的评估器。此外，它还强调了多维度评估和代码驱动分析对增强评估能力的重要性。', 'title_zh': '学习多面向评价对齐：一个统一且 robust 的框架'}
{'arxiv_id': 'arXiv:2502.18865', 'title': 'A Theoretical Perspective: How to Prevent Model Collapse in Self-consuming Training Loops', 'authors': 'Shi Fu, Yingjie Wang, Yuzhu Chen, Xinmei Tian, Dacheng Tao', 'link': 'https://arxiv.org/abs/2502.18865', 'abstract': 'High-quality data is essential for training large generative models, yet the vast reservoir of real data available online has become nearly depleted. Consequently, models increasingly generate their own data for further training, forming Self-consuming Training Loops (STLs). However, the empirical results have been strikingly inconsistent: some models degrade or even collapse, while others successfully avoid these failures, leaving a significant gap in theoretical understanding to explain this discrepancy. This paper introduces the intriguing notion of recursive stability and presents the first theoretical generalization analysis, revealing how both model architecture and the proportion between real and synthetic data influence the success of STLs. We further extend this analysis to transformers in in-context learning, showing that even a constant-sized proportion of real data ensures convergence, while also providing insights into optimal synthetic data sizing.', 'abstract_zh': '高质量的数据对于训练大规模生成模型至关重要，然而可供在线获取的真实数据已几乎耗尽。因此，模型越来越多地生成自己的数据以进行进一步训练，形成了自消耗训练循环（STLs）。然而，实证结果极为不一致：一些模型退化甚至崩溃，而另一些模型则成功避免了这些失败，留下了一个重要的理论空白来解释这种差异。本文引入了递归稳定性的有趣概念，并提出了第一个理论泛化分析，揭示了模型架构和真实数据与合成数据比例如何影响STLs的成功。我们进一步将此分析扩展到上下文学习中的转换器，表明即使真实数据的比例保持恒定，也能确保收敛，并提供有关最佳合成数据大小的见解。', 'title_zh': '一个理论视角：如何防止自消耗训练循环中的模型崩溃'}
{'arxiv_id': 'arXiv:2502.18863', 'title': 'Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM', 'authors': 'Junxiao Ma, Jingjing Wang, Jiamin Luo, Peiying Yu, Guodong Zhou', 'link': 'https://arxiv.org/abs/2502.18863', 'abstract': 'Prior studies on Video Anomaly Detection (VAD) mainly focus on detecting whether each video frame is abnormal or not in the video, which largely ignore the structured video semantic information (i.e., what, when, and where does the abnormal event happen). With this in mind, we propose a new chat-paradigm \\textbf{M}ulti-scene Video Abnormal Event Extraction and Localization (M-VAE) task, aiming to extract the abnormal event quadruples (i.e., subject, event type, object, scene) and localize such event. Further, this paper believes that this new task faces two key challenges, i.e., global-local spatial modeling and global-local spatial balancing. To this end, this paper proposes a Global-local Spatial-sensitive Large Language Model (LLM) named Sherlock, i.e., acting like Sherlock Holmes to track down the criminal events, for this M-VAE task. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module and a Spatial Imbalance Regulator (SIR) to address the two challenges respectively. Extensive experiments on our M-VAE instruction dataset show the significant advantages of Sherlock over several advanced Video-LLMs. This justifies the importance of global-local spatial information for the M-VAE task and the effectiveness of Sherlock in capturing such information.', 'abstract_zh': '多场景视频异常事件提取与定位（M-VAE）任务及其全局-局部空间建模与平衡', 'title_zh': 'Sherlock: 向量多场景视频异常事件提取与定位，借助全局-局部空间敏感的大规模语言模型'}
{'arxiv_id': 'arXiv:2502.18862', 'title': 'Investigating Generalization of One-shot LLM Steering Vectors', 'authors': 'Jacob Dunefsky, Arman Cohan', 'link': 'https://arxiv.org/abs/2502.18862', 'abstract': "Steering vectors have emerged as a promising approach for interpreting and controlling LLMs, but current methods typically require large contrastive datasets that are often impractical to construct and may capture spurious correlations. We propose directly optimizing steering vectors through gradient descent on a single training example, and systematically investigate how these vectors generalize. We consider several steering optimization techniques, including multiple novel ones, and find that the resulting vectors effectively mediate safety-relevant behaviors in multiple models. Indeed, in experiments on an alignment-faking model, we are able to optimize one-shot steering vectors that induce harmful behavior on benign examples and whose negations suppress harmful behavior on malign examples. And in experiments on refusal suppression, we demonstrate that one-shot optimized steering vectors can transfer across inputs, yielding a Harmbench attack success rate of 96.9%. Furthermore, to quantitatively assess steering effectiveness in instruction-tuned models, we develop a novel evaluation framework using sequence probabilities from the corresponding base model. With this framework, we analyze how steering vectors modulate an instruction-tuned LLM's ability to recover from outputting false information, and find that this ability derives from the base model. Overall, our findings suggest that optimizing steering vectors on a single example can mediate misaligned behavior in LLMs, and provide a path toward better understanding the relationship between LLM behavior and activation space structure.", 'abstract_zh': '基于单个训练样本优化导向向量以调控和解析大规模语言模型', 'title_zh': '探究单次学习大语言模型导向矢量的泛化能力'}
{'arxiv_id': 'arXiv:2502.18851', 'title': 'Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code', 'authors': 'Jungin Kim, Shinwoo Park, Yo-Sub Han', 'link': 'https://arxiv.org/abs/2502.18851', 'abstract': 'Code watermarking identifies AI-generated code by embedding patterns into the code during generation. Effective watermarking requires meeting two key conditions: the watermark should be reliably detectable, and the code should retain its original functionality. However, existing methods often modify tokens that are critical for program logic, such as keywords in conditional expressions or operators in arithmetic computations. These modifications can cause syntax errors or functional failures, limiting the practical use of watermarking. We present STONE, a method that preserves functional integrity by selectively inserting watermarks only into non-syntax tokens. By excluding tokens essential for code execution, STONE minimizes the risk of functional degradation.\nIn addition, we introduce CWEM, a comprehensive evaluation metric that evaluates watermarking techniques based on correctness, detectability, and naturalness. While correctness and detectability have been widely used, naturalness remains underexplored despite its importance. Unnatural patterns can reveal the presence of a watermark, making it easier for adversaries to remove. We evaluate STONE using CWEM and compare its performance with the state-of-the-art approach. The results show that STONE achieves an average improvement of 7.69% in CWEM across Python, C++, and Java. Our code is available in this https URL.', 'abstract_zh': '代码水印通过在生成过程中嵌入模式来识别AI生成的代码。有效的水印需要满足两个关键条件：水印应该可靠地可被检测到，并且代码应保留其原始功能。然而，现有方法 often 修改关键程序逻辑的标记，如条件表达式中的关键字或算术计算中的操作符。这些修改可能导致语法错误或功能故障，限制了水印技术的实际应用。我们提出了一种名为STONE的方法，通过仅选择性地将水印插入非语法标记来保持功能性完整性。通过排除对于代码执行至关重要的标记，STONE最小化了功能退化的风险。\n此外，我们引入了CWEM，这是一种全面的评估指标，基于正确性、可检测性和自然性来评估水印技术。虽然正确性和可检测性已广为使用，但自然性的重要性虽重要却尚未得到充分探索。不自然的模式可能会揭示水印的存在，使对手更易去除。我们使用CWEM评估了STONE，并将其性能与最先进的方法进行了比较。结果表明，STONE在Python、C++和Java中的CWEM平均提高了7.69%。我们的代码可在以下链接获取：this https URL。', 'title_zh': '不破坏代码的标记：面向检测大模型生成代码的代码水印技术'}
{'arxiv_id': 'arXiv:2502.18848', 'title': 'A Causal Lens for Evaluating Faithfulness Metrics', 'authors': 'Kerem Zaman, Shashank Srivastava', 'link': 'https://arxiv.org/abs/2502.18848', 'abstract': "Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's internal reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, a unified evaluation framework remains absent. To address this gap, we present Causal Diagnosticity, a framework to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of causal diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate a variety of faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that all tested faithfulness metrics often fail to surpass a random baseline. Our work underscores the need for improved metrics and more reliable interpretability methods in LLMs.", 'abstract_zh': '大型语言模型（LLMs）通过自然语言解释为模型可解释性提供了一种替代特征归因方法的途径。然而，尽管这些方法具有合理性，它们可能并不忠实地反映模型的内部推理过程，这对于理解模型的真实决策过程至关重要。虽然已经提出了几种忠实地度量方法，但缺乏一个统一的评估框架。为此，我们提出了因果诊断性框架，用于评估自然语言解释的忠实地度量方法。我们的框架采用了因果诊断性的概念，并使用模型编辑方法生成忠实地度量对。我们的基准包括四个任务：事实核查、类比、物体计数和多跳推理。我们评估了多种忠实地度量方法，包括事后解释和基于推理链的方法。发现所有测试的忠实地度量方法通常不能超过随机基线。我们的工作强调了需要改进的度量方法和更可靠的可解释性方法在LLMs中的需求。', 'title_zh': '一种因果视角下的信实性度量评价框架'}
{'arxiv_id': 'arXiv:2502.18845', 'title': 'Sliding Window Attention Training for Efficient Large Language Models', 'authors': 'Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, Xiangyu Zhao', 'link': 'https://arxiv.org/abs/2502.18845', 'abstract': 'Recent advances in transformer-based Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their quadratic computational complexity concerning sequence length remains a significant bottleneck for processing long documents. As a result, many efforts like sparse attention and state space models have been proposed to improve the efficiency of LLMs over long sequences. Though effective, these approaches compromise the performance or introduce structural complexity. This calls for a simple yet efficient model that preserves the fundamental Transformer architecture. To this end, we introduce SWAT, which enables efficient long-context handling via Sliding Window Attention Training. This paper first attributes the inefficiency of Transformers to the attention sink phenomenon resulting from the high variance of softmax operation. Then, we replace softmax with the sigmoid function and utilize a balanced ALiBi and Rotary Position Embedding for efficient information compression and retention. Experiments demonstrate that SWAT achieves SOTA performance compared with state-of-the-art linear recurrent architectures on eight benchmarks. Code is available at this https URL.', 'abstract_zh': '最近基于Transformer的大语言模型（LLMs）在各种任务中展现了显著的能力。然而，它们与序列长度相关的二次计算复杂性仍然是处理长文档的一个显著瓶颈。为此，人们提出了稀疏注意机制和状态空间模型等方法来提高LLMs在长序列上的效率。尽管这些方法有效，但它们会牺牲性能或增加结构复杂性。因此，需要一种简单且高效的模型来保留基本的Transformer架构。为此，我们提出了SWAT，通过滑动窗口注意力训练实现高效的长上下文处理。本文首先将Transformer的低效归因于softmax操作的高方差引起的注意陷井现象。然后，我们用sigmoid函数替代softmax，并使用平衡的ALiBi和旋转位置嵌入来进行高效的信息压缩和保留。实验结果表明，SWAT在八个基准测试中实现了与最先进的线性递归架构相当甚至更好的性能。代码可在以下链接获取：this https URL。', 'title_zh': '滑动窗口注意力训练高效大型语言模型'}
{'arxiv_id': 'arXiv:2502.18798', 'title': 'ANPMI: Assessing the True Comprehension Capabilities of LLMs for Multiple Choice Questions', 'authors': 'Gyeongje Cho, Yeonkyoung So, Jaejin Lee', 'link': 'https://arxiv.org/abs/2502.18798', 'abstract': "Multiple-choice benchmarks, consisting of various prompts and choices, are among the most widely used methods to assess a language model's natural language understanding capability. Given a specific prompt, we typically compute $P(Choice|Prompt)$ to evaluate how likely a language model is to generate the correct choice compared to incorrect ones. However, we observe that performance measured using this approach reflects not only the model's comprehension of the prompt but also its inherent biases for certain choices regardless of the prompt. This issue makes it challenging to accurately measure a model's natural language understanding, as models may select the answer without fully understanding the prompt. To address this limitation, we propose a novel metric called ANPMI, which normalizes Pointwise Mutual Information (PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the model's natural language understanding by ensuring that it is challenging to answer a question without properly understanding the prompt.", 'abstract_zh': '多种选择基准，由各种提示和选项组成，是最广泛用于评估语言模型自然语言理解能力的方法之一。给定特定的提示，我们通常计算$P(Choice|Prompt)$来评估语言模型生成正确选项而非错误选项的可能性。然而，我们观察到，使用这种方法衡量的性能不仅反映出模型对提示的理解能力，还反映了模型对某些选项的固有偏好。这一问题使得准确衡量模型的自然语言理解能力变得困难，因为模型可能在未完全理解提示的情况下选择答案。为解决这一局限，我们提出了一个新的评估指标ANPMI，该指标通过$-\\log P(Choice)$对点互信息（PMI）进行归一化。ANPMI通过确保没有正确理解提示就难以回答问题，提供了模型自然语言理解能力更为准确的评估。', 'title_zh': 'ANPMI: 评估大型语言模型在多项选择题中真正理解的能力'}
{'arxiv_id': 'arXiv:2502.18791', 'title': 'Seeing the Forest for the Trees: A Large Scale, Continuously Updating Meta-Analysis of Frontier LLMs', 'authors': 'Jungsoo Park, Junmo Kang, Gabriel Stanovsky, Alan Ritter', 'link': 'https://arxiv.org/abs/2502.18791', 'abstract': 'The surge of LLM studies makes synthesizing their findings challenging. Meta-analysis can uncover important trends across studies, but its use is limited by the time-consuming nature of manual data extraction. Our study presents a semi-automated approach for meta-analysis that accelerates data extraction using LLMs. It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset. We conduct a comprehensive meta-analysis of frontier LLMs using an automatically extracted dataset, reducing the effort of paper surveying and data extraction by more than 93\\% compared to manual approaches. We validate our dataset by showing that it reproduces key findings from a recent manual meta-analysis about Chain-of-Thought (CoT), and also uncovers new insights that go beyond it, showing for example that in-context examples benefit multimodal tasks but offer limited gains in mathematical tasks compared to CoT. Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through our scientific artifacts and empirical analysis, we provide novel insights into LLMs while facilitating ongoing meta-analyses of their behavior.', 'abstract_zh': 'LLM研究 surge 促使综合其成果变得更具挑战性：semi-自动化元分析方法加速数据提取的研究', 'title_zh': '从树木中见森林：前沿大语言模型的大型持续更新元分析'}
{'arxiv_id': 'arXiv:2502.18770', 'title': 'Reward Shaping to Mitigate Reward Hacking in RLHF', 'authors': 'Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao', 'link': 'https://arxiv.org/abs/2502.18770', 'abstract': "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to reward hacking, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. While reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests three key design principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid initial growth followed by gradual convergence, and (3) RL reward is best formulated as a function of centered reward. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model itself as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. Code is available at this https URL.", 'abstract_zh': '人类反馈强化学习（RLHF）对于使大型语言模型（LLMs）与人类价值观保持一致是必不可少的。然而，RLHF 易受奖励作弊的影响，即代理利用奖励函数中的缺陷而非学习预期行为，从而降低对齐效果。尽管奖励塑形有助于稳定 RLHF 并部分缓解奖励作弊，但对塑形技术及其基本原理的系统性研究仍存在不足。为了弥补这一差距，我们对常见的奖励塑形方法进行了全面研究。我们的分析提出了三条关键设计原则：（1）RL 奖励应是有限的，（2）RL 从快速初期增长后逐渐收敛受益，（3）RL 奖励最好以中心化奖励的函数形式进行表达。受此见解的启发，我们提出了Preference As Reward (PAR) 新方法，该方法利用奖励模型本身嵌入的潜在偏好作为强化学习的信号。我们在两个基础模型Gemma2-2B和Llama3-8B上，使用Ultrafeedback-Binarized和HH-RLHF两个数据集对PAR进行了评估。实验结果表明PAR在其他奖励塑形方法中表现更优。在AlpacaEval 2.0基准测试中，PAR 的胜率至少比竞争方法高5个百分点。此外，PAR 表现出显著的数据效率，只需单个参考奖励即可实现最优性能，并且即使经过两次完整的训练周期后仍能保持对奖励作弊的鲁棒性。代码可在以下链接获取。', 'title_zh': '通过奖励塑造减轻RLHF中的奖励破解问题'}
{'arxiv_id': 'arXiv:2502.18754', 'title': 'AgentSociety Challenge: Designing LLM Agents for User Modeling and Recommendation on Web Platforms', 'authors': 'Yuwei Yan, Yu Shang, Qingbin Zeng, Yu Li, Keyu Zhao, Zhiheng Zheng, Xuefei Ning, Tianji Wu, Shengen Yan, Yu Wang, Fengli Xu, Yong Li', 'link': 'https://arxiv.org/abs/2502.18754', 'abstract': 'The AgentSociety Challenge is the first competition in the Web Conference that aims to explore the potential of Large Language Model (LLM) agents in modeling user behavior and enhancing recommender systems on web platforms. The Challenge consists of two tracks: the User Modeling Track and the Recommendation Track. Participants are tasked to utilize a combined dataset from Yelp, Amazon, and Goodreads, along with an interactive environment simulator, to develop innovative LLM agents. The Challenge has attracted 295 teams across the globe and received over 1,400 submissions in total over the course of 37 official competition days. The participants have achieved 21.9% and 20.3% performance improvement for Track 1 and Track 2 in the Development Phase, and 9.1% and 15.9% in the Final Phase, representing a significant accomplishment. This paper discusses the detailed designs of the Challenge, analyzes the outcomes, and highlights the most successful LLM agent designs. To support further research and development, we have open-sourced the benchmark environment at this https URL.', 'abstract_zh': 'Web Conference上的AgentSociety挑战是首次探索大型语言模型（LLM）代理在建模用户行为和增强Web平台推荐系统潜力的比赛。该挑战包含两个赛道：用户建模赛道和推荐赛道。参赛者利用来自Yelp、Amazon和Goodreads的联合数据集以及交互环境模拟器，开发创新的LLM代理。该挑战吸引了来自全球的295个团队，并在37个官方比赛日中接收到超过1,400份提交。参赛者在开发阶段和最终阶段分别实现了21.9%和20.3%的任务1性能提升，以及9.1%和15.9%的任务2性能提升，这是一个重要的成就。本文详细讨论了挑战的设计、分析了结果，并强调了最成功的LLM代理设计。为了支持进一步的研究和开发，我们已在https://this.is/开放了基准环境。', 'title_zh': 'AgentSociety挑战：设计面向Web平台的用户建模与推荐的LLM代理'}
{'arxiv_id': 'arXiv:2502.18737', 'title': 'Intent Tagging: Exploring Micro-Prompting Interactions for Supporting Granular Human-GenAI Co-Creation Workflows', 'authors': 'Frederic Gmeiner, Nicolai Marquardt, Michael Bentley, Hugo Romat, Michel Pahud, David Brown, Asta Roseway, Nikolas Martelaro, Kenneth Holstein, Ken Hinckley, Nathalie Riche', 'link': 'https://arxiv.org/abs/2502.18737', 'abstract': "Despite Generative AI (GenAI) systems' potential for enhancing content creation, users often struggle to effectively integrate GenAI into their creative workflows. Core challenges include misalignment of AI-generated content with user intentions (intent elicitation and alignment), user uncertainty around how to best communicate their intents to the AI system (prompt formulation), and insufficient flexibility of AI systems to support diverse creative workflows (workflow flexibility). Motivated by these challenges, we created IntentTagger: a system for slide creation based on the notion of Intent Tags - small, atomic conceptual units that encapsulate user intent - for exploring granular and non-linear micro-prompting interactions for Human-GenAI co-creation workflows. Our user study with 12 participants provides insights into the value of flexibly expressing intent across varying levels of ambiguity, meta-intent elicitation, and the benefits and challenges of intent tag-driven workflows. We conclude by discussing the broader implications of our findings and design considerations for GenAI-supported content creation workflows.", 'abstract_zh': '尽管生成式AI（GenAI）系统在增强内容创建方面具有潜力，用户往往难以有效将GenAI集成到其创作流程中。核心挑战包括生成内容与用户意图不匹配（意图提取和对齐）、用户在如何最好地向AI系统传达其意图方面存在不确定性（提示制定），以及AI系统缺乏支持多样化创作流程的灵活性（工作流程灵活性）。为应对这些挑战，我们开发了基于意图标签（Intent Tags）的概念——小而原子的概念单元，用以概括用户意图——的幻灯片创建系统，以探索细粒度和非线性的微提示互动，以支持人类与GenAI共创作的工作流程。我们的用户研究（包含12名参与者）提供了关于灵活表达意图、在不同模糊度级别上提取元意图的价值，以及基于意图标签驱动的工作流程的优势和挑战的见解。最后，我们讨论了研究发现的更广泛意义，并提出了支持GenAI辅助内容创作工作流程的设计考虑。', 'title_zh': '意图标注：探索微观提示交互以支持细粒度的人机协同创作工作流'}
{'arxiv_id': 'arXiv:2502.18736', 'title': 'AI-Instruments: Embodying Prompts as Instruments to Abstract & Reflect Graphical Interface Commands as General-Purpose Tools', 'authors': 'Nathalie Riche, Anna Offenwanger, Frederic Gmeiner, David Brown, Hugo Romat, Michel Pahud, Nicolai Marquardt, Kori Inkpen, Ken Hinckley', 'link': 'https://arxiv.org/abs/2502.18736', 'abstract': 'Chat-based prompts respond with verbose linear-sequential texts, making it difficult to explore and refine ambiguous intents, back up and reinterpret, or shift directions in creative AI-assisted design work. AI-Instruments instead embody "prompts" as interface objects via three key principles: (1) Reification of user-intent as reusable direct-manipulation instruments; (2) Reflection of multiple interpretations of ambiguous user-intents (Reflection-in-intent) as well as the range of AI-model responses (Reflection-in-response) to inform design "moves" towards a desired result; and (3) Grounding to instantiate an instrument from an example, result, or extrapolation directly from another instrument. Further, AI-Instruments leverage LLM\'s to suggest, vary, and refine new instruments, enabling a system that goes beyond hard-coded functionality by generating its own instrumental controls from content. We demonstrate four technology probes, applied to image generation, and qualitative insights from twelve participants, showing how AI-Instruments address challenges of intent formulation, steering via direct manipulation, and non-linear iterative workflows to reflect and resolve ambiguous intents.', 'abstract_zh': '基于聊天的提示生成冗长的线性文本，这使得在创造性AI辅助设计工作中探索、精炼模糊意图、回溯、重新解释或转向变得困难。相反，AI-Instruments将“提示”作为界面对象，通过三个关键原则实现：（1）用户意图的物化为可重复使用的直接操作工具；（2）通过意图内的反思（Reflection-in-intent）和响应内的反思（Reflection-in-response）反映模糊用户意图的多种解释以及AI模型响应的范围，从而指导设计“操作”以达成预期结果；（3）基于实例、结果或另一工具的推演进行工具的实例化。此外，AI-Instruments利用大语言模型（LLMs）来建议、变化和精炼新的工具，使系统能够自动生成其自身的控制工具，超越了硬编码的功能。我们通过应用图像生成技术探针，并从12名参与者中获得质性见解，展示了AI-Instruments如何解决意图表述、直接操作导向操纵以及非线性迭代工作流程中反映和解决模糊意图的挑战。', 'title_zh': 'AI-Instruments: 将提示拟人化为工具以抽象和反思图形界面命令作为通用工具'}
{'arxiv_id': 'arXiv:2502.18726', 'title': 'Deep-Bench: Deep Learning Benchmark Dataset for Code Generation', 'authors': 'Alireza Daghighfarsoodeh, Chung-Yu Wang, Hamed Taherkhani, Melika Sepidband, Mohammad Abdollahi, Hadi Hemmati, Hung Viet Pham', 'link': 'https://arxiv.org/abs/2502.18726', 'abstract': "Deep learning (DL) has revolutionized areas such as computer vision, natural language processing, and more. However, developing DL systems is challenging due to the complexity of DL workflows. Large Language Models (LLMs), such as GPT, Claude, Llama, Mistral, etc., have emerged as promising tools to assist in DL code generation, offering potential solutions to these challenges. Despite this, existing benchmarks such as DS-1000 are limited, as they primarily focus on small DL code snippets related to pre/post-processing tasks and lack a comprehensive coverage of the full DL pipeline, including different DL phases and input data types.\nTo address this, we introduce DeepBench, a novel benchmark dataset designed for function-level DL code generation. DeepBench categorizes DL problems based on three key aspects: phases such as pre-processing, model construction, and training; tasks, including classification, regression, and recommendation; and input data types such as tabular, image, and text.\nGPT-4o -- the state-of-the-art LLM -- achieved 31% accuracy on DeepBench, significantly lower than its 60% on DS-1000. We observed similar difficulty for other LLMs (e.g., 28% vs. 54% for Claude, 21% vs. 41% for LLaMA, and 15% vs. 20% for Mistral). This result underscores DeepBench's greater complexity. We also construct a taxonomy of issues and bugs found in LLM-generated DL code, which highlights the distinct challenges that LLMs face when generating DL code compared to general code.\nFurthermore, our analysis also reveals substantial performance variations across categories, with differences of up to 7% among phases and 37% among tasks. These disparities suggest that DeepBench offers valuable insights into the LLMs' performance and areas for potential improvement in the DL domain.", 'abstract_zh': '深度学习（DL）已 revolutionized 计算机视觉、自然语言处理等领域。然而，开发 DL 系统因 DL 工作流的复杂性而具有挑战性。大型语言模型（LLMs），如 GPT、Claude、Llama、Mistral 等，作为辅助 DL 代码生成的有希望工具出现，提供了应对这些挑战的潜在解决方案。尽管如此，现有的基准测试，如 DS-1000，仍有限制，因为它们主要关注与预处理/后处理任务相关的少量 DL 代码片段，并且缺乏对整个 DL 管道的综合覆盖，包括不同 DL 阶段和输入数据类型。\n\n为应对这一挑战，我们引入了 DeepBench，一个用于功能级别 DL 代码生成的新型基准数据集。DeepBench 根据三个关键方面对 DL 问题进行分类：阶段，如预处理、模型构建和训练；任务，包括分类、回归和推荐；以及输入数据类型，如表格、图像和文本。\n\nGPT-4o — 目前最先进的 LLM — 在 DeepBench 上的准确率为 31%，远低于其在 DS-1000 上 60% 的准确率。我们观察到其他 LLMs（例如，Claude 的 28% 对比 54%、LLaMA 的 21% 对比 41%、Mistral 的 15% 对比 20%）也存在类似难度。这一结果突显了 DeepBench 的更高复杂度。我们还构建了一个 LLM 生成的 DL 代码中存在的问题和错误的分类体系，这突显了 LLMs 在生成 DL 代码时与其他通用代码相比面临的独特挑战。\n\n此外，我们的分析还揭示了不同类别之间存在显著的性能差异，不同阶段的性能差异可达 7%，而不同任务的性能差异可达 37%。这些差异表明 DeepBench 提供了有关 LLMs 在 DL 领域的表现和潜在改进领域的有价值的见解。', 'title_zh': 'Deep-Bench: 用于代码生成的深度学习基准数据集'}
{'arxiv_id': 'arXiv:2502.18695', 'title': 'Policy-as-Prompt: Rethinking Content Moderation in the Age of Large Language Models', 'authors': 'Konstantina Palla, José Luis Redondo García, Claudia Hauff, Francesco Fabbri, Henrik Lindström, Daniel R. Taber, Andreas Damianou, Mounia Lalmas', 'link': 'https://arxiv.org/abs/2502.18695', 'abstract': 'Content moderation plays a critical role in shaping safe and inclusive online environments, balancing platform standards, user expectations, and regulatory frameworks. Traditionally, this process involves operationalising policies into guidelines, which are then used by downstream human moderators for enforcement, or to further annotate datasets for training machine learning moderation models. However, recent advancements in large language models (LLMs) are transforming this landscape. These models can now interpret policies directly as textual inputs, eliminating the need for extensive data curation. This approach offers unprecedented flexibility, as moderation can be dynamically adjusted through natural language interactions. This paradigm shift raises important questions about how policies are operationalised and the implications for content moderation practices. In this paper, we formalise the emerging policy-as-prompt framework and identify five key challenges across four domains: Technical Implementation (1. translating policy to prompts, 2. sensitivity to prompt structure and formatting), Sociotechnical (3. the risk of technological determinism in policy formation), Organisational (4. evolving roles between policy and machine learning teams), and Governance (5. model governance and accountability). Through analysing these challenges across technical, sociotechnical, organisational, and governance dimensions, we discuss potential mitigation approaches. This research provides actionable insights for practitioners and lays the groundwork for future exploration of scalable and adaptive content moderation systems in digital ecosystems.', 'abstract_zh': '政策-as-提示框架在内容审核中的作用与挑战', 'title_zh': '政策作为提示：在大语言模型时代重新思考内容审核'}
{'arxiv_id': 'arXiv:2502.18635', 'title': 'Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems', 'authors': 'Matthew Barker, Andrew Bell, Evan Thomas, James Carr, Thomas Andrews, Umang Bhatt', 'link': 'https://arxiv.org/abs/2502.18635', 'abstract': 'While Retrieval Augmented Generation (RAG) has emerged as a popular technique for improving Large Language Model (LLM) systems, it introduces a large number of choices, parameters and hyperparameters that must be made or tuned. This includes the LLM, embedding, and ranker models themselves, as well as hyperparameters governing individual RAG components. Yet, collectively optimizing the entire configuration in a RAG or LLM system remains under-explored - especially in multi-objective settings - due to intractably large solution spaces, noisy objective evaluations, and the high cost of evaluations. In this work, we introduce the first approach for multi-objective parameter optimization of cost, latency, safety and alignment over entire LLM and RAG systems. We find that Bayesian optimization methods significantly outperform baseline approaches, obtaining a superior Pareto front on two new RAG benchmark tasks. We conclude our work with important considerations for practitioners who are designing multi-objective RAG systems, highlighting nuances such as how optimal configurations may not generalize across tasks and objectives.', 'abstract_zh': '面向成本、延迟、安全性和对齐的大型语言模型和检索增强生成系统多目标参数优化方法', 'title_zh': '更快、更低成本、更优效果：多目标超参数优化在大语言模型和检索增强生成系统中的应用'}
{'arxiv_id': 'arXiv:2502.18581', 'title': 'Scalable Best-of-N Selection for Large Language Models via Self-Certainty', 'authors': 'Zhewei Kang, Xuandong Zhao, Dawn Song', 'link': 'https://arxiv.org/abs/2502.18581', 'abstract': 'Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size $N$, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at this https URL', 'abstract_zh': '最佳-of-N 选择是通过增加测试时计算量来提高大型语言模型推理性能的关键技术。当前最先进的方法往往采用计算密集型奖励模型来进行响应评估和选择。无奖励替代方法，如自我一致性与普遍自我一致性，处理开放生成任务的能力有限，也很难有效扩展。为了解决这些限制，我们提出了一种新的高效度量标准自我确信，该度量标准利用大型语言模型输出的固有概率分布来估计响应质量，无需外部奖励模型。我们假设，在多个样本中聚合的更高概率分布自我确信与提高的响应准确性相关，因为它反映了更大的生成输出信心。通过在各种推理任务上的广泛实验，我们证明了自我确信:(1) 随样本数量 $N$ 的增加而有效扩展，类似于奖励模型但没有额外的计算开销；(2) 补充逐步推理，超越贪婪解码，进一步提高推理性能；(3) 在传统自我一致性方法表现不佳的开放生成任务中具有良好的泛化能力。我们的发现确立了自我确信作为一种实用且高效的大型语言模型推理能力提升方法的地位。代码可在以下链接获取。', 'title_zh': '大规模语言模型的可扩展最佳选项自信心选择方法'}
{'arxiv_id': 'arXiv:2502.18573', 'title': 'FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models', 'authors': 'Radu Marinescu, Debarun Bhattacharjya, Junkyu Lee, Tigran Tchrakian, Javier Carnerero Cano, Yufang Hou, Elizabeth Daly, Alessandra Pascale', 'link': 'https://arxiv.org/abs/2502.18573', 'abstract': 'Large language models (LLMs) have demonstrated vast capabilities on generative tasks in recent years, yet they struggle with guaranteeing the factual correctness of the generated content. This makes these models unreliable in realistic situations where factually accurate responses are expected. In this paper, we propose FactReasoner, a new factuality assessor that relies on probabilistic reasoning to assess the factuality of a long-form generated response. Specifically, FactReasoner decomposes the response into atomic units, retrieves relevant contexts for them from an external knowledge source, and constructs a joint probability distribution over the atoms and contexts using probabilistic encodings of the logical relationships (entailment, contradiction) between the textual utterances corresponding to the atoms and contexts. FactReasoner then computes the posterior probability of whether atomic units in the response are supported by the retrieved contexts. Our experiments on labeled and unlabeled benchmark datasets demonstrate clearly that FactReasoner improves considerably over state-of-the-art prompt-based approaches in terms of both factual precision and recall.', 'abstract_zh': '大型语言模型（LLMs）在近年来的生成任务中展示了巨大的能力，但它们在保证生成内容的准确性方面存在困难。这使得这些模型在需要事实准确回应的实际情况下不可靠。本文提出了一种新的事实性评估器FactReasoner，该评估器依赖概率推理来评估长文本生成回应的事实性。具体而言，FactReasoner将回应分解为原子单元，从外部知识源检索相关上下文，并使用逻辑关系（蕴含、矛盾）的概率编码构建原子单元和上下文之间的联合概率分布。FactReasoner然后计算所检索上下文支持回应中原子单元的后验概率。我们在标记和未标记的基准数据集上的实验清楚地表明，FactReasoner在事实精确度和召回率方面显著优于最先进的基于提示的方法。', 'title_zh': 'FactReasoner: 大型语言模型长文本事实性评估的概率方法'}
{'arxiv_id': 'arXiv:2502.18545', 'title': 'PII-Bench: Evaluating Query-Aware Privacy Protection Systems', 'authors': 'Hao Shen, Zhouhong Gu, Haokai Hong, Weili Han', 'link': 'https://arxiv.org/abs/2502.18545', 'abstract': 'The widespread adoption of Large Language Models (LLMs) has raised significant privacy concerns regarding the exposure of personally identifiable information (PII) in user prompts. To address this challenge, we propose a query-unrelated PII masking strategy and introduce PII-Bench, the first comprehensive evaluation framework for assessing privacy protection systems. PII-Bench comprises 2,842 test samples across 55 fine-grained PII categories, featuring diverse scenarios from single-subject descriptions to complex multi-party interactions. Each sample is carefully crafted with a user query, context description, and standard answer indicating query-relevant PII. Our empirical evaluation reveals that while current models perform adequately in basic PII detection, they show significant limitations in determining PII query relevance. Even state-of-the-art LLMs struggle with this task, particularly in handling complex multi-subject scenarios, indicating substantial room for improvement in achieving intelligent PII masking.', 'abstract_zh': '大规模语言模型（LLMs）的广泛应用引发了关于用户提示中个人可识别信息（PII）暴露的重大隐私 concern 的担忧。为应对这一挑战，我们提出了一种与查询无关的 PII 遮掩策略，并引入了第一个全面评估框架 PII-Bench，用于评估隐私保护系统。PII-Bench 包含 2,842 个测试样本，涵盖 55 个细粒度的 PII 类别，从单主体描述到复杂的多主体交互，各不相同。每个样本都包含用户查询、上下文描述和标准答案，其中标准答案指出了与查询相关的 PII。我们的实证评估表明，当前模型在基本 PII 检测方面表现良好，但在确定查询相关 PII 方面存在显著局限性。即使最新的 LLM 也在这项任务中遇到困难，尤其是在处理复杂多主体场景方面，这表明在实现智能 PII 遮(masking)方面存在着巨大的改进空间。', 'title_zh': 'PII-Bench: 评估查询感知隐私保护系统'}
{'arxiv_id': 'arXiv:2502.18518', 'title': 'Swallowing the Poison Pills: Insights from Vulnerability Disparity Among LLMs', 'authors': 'Peng Yifeng, Wu Zhizheng, Chen Chen', 'link': 'https://arxiv.org/abs/2502.18518', 'abstract': "Modern large language models (LLMs) exhibit critical vulnerabilities to poison pill attacks: localized data poisoning that alters specific factual knowledge while preserving overall model utility. We systematically demonstrate these attacks exploit inherent architectural properties of LLMs, achieving 54.6% increased retrieval inaccuracy on long-tail knowledge versus dominant topics and up to 25.5% increase retrieval inaccuracy on compressed models versus original architectures. Through controlled mutations (e.g., temporal/spatial/entity alterations) and, our method induces localized memorization deterioration with negligible impact on models' performance on regular standard benchmarks (e.g., <2% performance drop on MMLU/GPQA), leading to potential detection evasion. Our findings suggest: (1) Disproportionate vulnerability in long-tail knowledge may result from reduced parameter redundancy; (2) Model compression may increase attack surfaces, with pruned/distilled models requiring 30% fewer poison samples for equivalent damage; (3) Associative memory enables both spread of collateral damage to related concepts and amplification of damage from simultaneous attack, particularly for dominant topics. These findings raise concerns over current scaling paradigms since attack costs are lowering while defense complexity is rising. Our work establishes poison pills as both a security threat and diagnostic tool, revealing critical security-efficiency trade-offs in language model compression that challenges prevailing safety assumptions.", 'abstract_zh': '现代大型语言模型（LLMs）对毒药攻击表现出关键性漏洞：局部数据污染改变了特定事实知识的同时保留了模型的整体实用性。', 'title_zh': '吞咽毒丸：来自语言模型漏洞差异的洞察'}
{'arxiv_id': 'arXiv:2502.18517', 'title': 'RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via Reward Driven Data Synthesis', 'authors': 'Jianwei Wang, Junyao Yang, Haoran Li, Huiping Zhuang, Cen Chen, Ziqian Zeng', 'link': 'https://arxiv.org/abs/2502.18517', 'abstract': 'The success of large language models (LLMs) has attracted many individuals to fine-tune them for domain-specific tasks by uploading their data. However, in sensitive areas like healthcare and finance, privacy concerns often arise. One promising solution is to sample synthetic data with Differential Privacy (DP) guarantees to replace private data. However, these synthetic data contain significant flawed data, which are considered as noise. Existing solutions typically rely on naive filtering by comparing ROUGE-L scores or embedding similarities, which are ineffective in addressing the noise. To address this issue, we propose RewardDS, a novel privacy-preserving framework that fine-tunes a reward proxy model and uses reward signals to guide the synthetic data generation. Our RewardDS introduces two key modules, Reward Guided Filtering and Self-Optimizing Refinement, to both filter and refine the synthetic data, effectively mitigating the noise. Extensive experiments across medical, financial, and code generation domains demonstrate the effectiveness of our method.', 'abstract_zh': '大规模语言模型（LLMs）的成功吸引了许多人通过上传其数据来针对特定领域任务进行微调。然而，在如医疗和金融等敏感领域，隐私问题经常出现。一种有前景的解决方案是使用具有差分隐私（DP）保证的合成数据来替代私人数据。然而，这些合成数据包含大量被视为噪声的错误数据。现有解决方案通常依赖于简单的基于ROUGE-L分数或嵌入相似性的过滤方法，这些方法在处理噪声方面无效。为此，我们提出了一种名为RewardDS的新颖隐私保护框架，该框架通过训练一个奖励代理模型，并利用奖励信号指导合成数据的生成。RewardDS引入了两个关键模块——奖励引导过滤和自我优化精炼，以过滤和精炼合成数据，有效减少了噪声。我们在医疗、金融和代码生成领域进行的广泛实验证明了该方法的有效性。', 'title_zh': 'RewardDS：通过奖励驱动的数据合成实现大型语言模型的隐私保护微调'}
{'arxiv_id': 'arXiv:2502.18515', 'title': 'A Multi-Agent Framework for Automated Vulnerability Detection and Repair in Solidity and Move Smart Contracts', 'authors': 'Rabimba Karanjai, Sam Blackshear, Lei Xu, Weidong Shi', 'link': 'https://arxiv.org/abs/2502.18515', 'abstract': "The rapid growth of the blockchain ecosystem and the increasing value locked in smart contracts necessitate robust security measures. While languages like Solidity and Move aim to improve smart contract security, vulnerabilities persist. This paper presents Smartify, a novel multi-agent framework leveraging Large Language Models (LLMs) to automatically detect and repair vulnerabilities in Solidity and Move smart contracts. Unlike traditional methods that rely solely on vast pre-training datasets, Smartify employs a team of specialized agents working on different specially fine-tuned LLMs to analyze code based on underlying programming concepts and language-specific security principles. We evaluated Smartify on a dataset for Solidity and a curated dataset for Move, demonstrating its effectiveness in fixing a wide range of vulnerabilities. Our results show that Smartify (Gemma2+codegemma) achieves state-of-the-art performance, surpassing existing LLMs and enhancing general-purpose models' capabilities, such as Llama 3.1. Notably, Smartify can incorporate language-specific knowledge, such as the nuances of Move, without requiring massive language-specific pre-training datasets. This work offers a detailed analysis of various LLMs' performance on smart contract repair, highlighting the strengths of our multi-agent approach and providing a blueprint for developing more secure and reliable decentralized applications in the growing blockchain landscape. We also provide a detailed recipe for extending this to other similar use cases.", 'abstract_zh': '区块链生态系统迅速增长和智能合约中锁定价值的增加 necessitate robust security measures.尽管Solidity和Move等语言旨在提高智能合约安全性，但漏洞仍然存在。本文提出了一种新颖的多智能体框架Smartify，利用大型语言模型（LLMs）自动检测和修复Solidity和Move智能合约中的漏洞。Smartify不同于依赖庞大预训练数据集的传统方法，而是采用专门针对不同语言特定安全原则进行微调的智能体团队，基于底层编程概念分析代码。Smartify在Solidity和Move的专门数据集上进行了评估，证实了其在修复各种漏洞方面的效果。我们的结果显示，Smartify（Gemma2+codegemma）达到最先进的性能，超越了现有LLMs，并增强了通用模型（如Llama 3.1）的能力。尤其是，Smartify可以在不需大规模语言特定预训练数据集的情况下，融入特定语言的知识，如Move的独特之处。本文对各种LLMs在智能合约修复中的性能进行了详细分析，强调了我们多智能体方法的优势，并为开发更加安全可靠的去中心化应用提供了蓝图。我们还提供了将此扩展到其他类似用例的详细方法。', 'title_zh': '基于Solidity和Move智能合约的自动化漏洞检测与修复多Agent框架'}
{'arxiv_id': 'arXiv:2502.18512', 'title': 'FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression', 'authors': 'Jianjian Li, Junquan Fan, Feng Tang, Gang Huang, Shitao Zhu, Songlin Liu, Nian Xie, Wulong Liu, Yong Liao', 'link': 'https://arxiv.org/abs/2502.18512', 'abstract': 'The rapid success of Vision Large Language Models (VLLMs) often depends on the high-resolution images with abundant visual tokens, which hinders training and deployment efficiency. Current training-free visual token compression methods exhibit serious performance degradation in tasks involving high-resolution, text-oriented image understanding and reasoning. In this paper, we propose an efficient visual token compression framework for text-oriented VLLMs in high-resolution scenarios. In particular, we employ a light-weight self-distillation pre-training stage to compress the visual tokens, requiring a limited numbers of image-text pairs and minimal learnable parameters. Afterwards, to mitigate potential performance degradation of token-compressed models, we construct a high-quality post-train stage. To validate the effectiveness of our method, we apply it to an advanced VLLMs, InternVL2. Experimental results show that our approach significantly reduces computational overhead while outperforming the baselines across a range of text-oriented benchmarks. We will release the models and code soon.', 'abstract_zh': 'Vision 大语言模型中的高效视觉令牌压缩框架：针对高分辨率场景下的文本导向模型', 'title_zh': 'FCoT-VL：高效的视觉令牌压缩促进面向文本的大规模跨模态模型发展'}
{'arxiv_id': 'arXiv:2502.18511', 'title': 'ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models', 'authors': 'Xuxu Liu, Siyuan Liang, Mengya Han, Yong Luo, Aishan Liu, Xiantao Cai, Zheng He, Dacheng Tao', 'link': 'https://arxiv.org/abs/2502.18511', 'abstract': 'Generative large language models are crucial in natural language processing, but they are vulnerable to backdoor attacks, where subtle triggers compromise their behavior. Although backdoor attacks against LLMs are constantly emerging, existing benchmarks remain limited in terms of sufficient coverage of attack, metric system integrity, backdoor attack alignment. And existing pre-trained backdoor attacks are idealized in practice due to resource access constraints. Therefore we establish $\\textit{ELBA-Bench}$, a comprehensive and unified framework that allows attackers to inject backdoor through parameter efficient fine-tuning ($\\textit{e.g.,}$ LoRA) or without fine-tuning techniques ($\\textit{e.g.,}$ In-context-learning). $\\textit{ELBA-Bench}$ provides over 1300 experiments encompassing the implementations of 12 attack methods, 18 datasets, and 12 LLMs. Extensive experiments provide new invaluable findings into the strengths and limitations of various attack strategies. For instance, PEFT attack consistently outperform without fine-tuning approaches in classification tasks while showing strong cross-dataset generalization with optimized triggers boosting robustness; Task-relevant backdoor optimization techniques or attack prompts along with clean and adversarial demonstrations can enhance backdoor attack success while preserving model performance on clean samples. Additionally, we introduce a universal toolbox designed for standardized backdoor attack research, with the goal of propelling further progress in this vital area.', 'abstract_zh': '生成式大型语言模型在自然语言处理中至关重要，但它们容易遭受后门攻击，其中微妙的触发器会损害其行为。尽管针对LLMs的后门攻击不断出现，但现有基准在攻击覆盖范围、度量系统完整性和后门攻击对齐方面仍然有限。而且，由于资源访问限制，现有的预训练后门攻击在实践中往往是理想化的。因此，我们建立了ELBA-Bench，这是一个全面统一的框架，允许攻击者通过高效参数微调（如LoRA）或不使用微调技术（如上下文学习）注入后门。ELBA-Bench 包括超过1300个实验，涵盖12种攻击方法、18个数据集和12个LLM的实现。广泛的实验提供了有关各种攻击策略优势和局限性的新发现。例如，在分类任务中，PEFT攻击在没有微调的情况下始终表现出色，并且优化的触发器可以增强鲁棒性从而实现跨数据集的良好泛化；具有任务相关后门优化技术或攻击提示以及干净和对抗性示范的方法可以提高后门攻击的成功率，同时保持模型在干净样本上的性能。此外，我们介绍了一个通用工具箱，旨在促进标准化后门攻击研究，从而推动这一关键领域的进一步进展。', 'title_zh': 'ELBA-Bench: 一种高效的大规模语言模型后门攻击基准'}
{'arxiv_id': 'arXiv:2502.18509', 'title': 'Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents', 'authors': 'Ivoline Ngong, Swanand Kadhe, Hao Wang, Keerthiram Murugesan, Justin D. Weisz, Amit Dhurandhar, Karthikeyan Natesan Ramamurthy', 'link': 'https://arxiv.org/abs/2502.18509', 'abstract': 'Conversational agents are increasingly woven into individuals\' personal lives, yet users often underestimate the privacy risks involved. The moment users share information with these agents (e.g., LLMs), their private information becomes vulnerable to exposure. In this paper, we characterize the notion of contextual privacy for user interactions with LLMs. It aims to minimize privacy risks by ensuring that users (sender) disclose only information that is both relevant and necessary for achieving their intended goals when interacting with LLMs (untrusted receivers). Through a formative design user study, we observe how even "privacy-conscious" users inadvertently reveal sensitive information through indirect disclosures. Based on insights from this study, we propose a locally-deployable framework that operates between users and LLMs, and identifies and reformulates out-of-context information in user prompts. Our evaluation using examples from ShareGPT shows that lightweight models can effectively implement this framework, achieving strong gains in contextual privacy while preserving the user\'s intended interaction goals through different approaches to classify information relevant to the intended goals.', 'abstract_zh': '对话代理越来越多地融入个人生活中，但用户往往低估了其中的隐私风险。当用户向这些代理（例如，大语言模型）共享信息时，其私人信息就会面临泄露的风险。在本文中，我们刻画了用户与大语言模型互动时的上下文隐私概念，旨在通过确保用户仅披露实现其预期目标所必需的相关信息来最小化隐私风险，而这些信息在与不可信接收者（大语言模型）互动时仅需披露。通过形式化设计用户研究，我们观察到即使是“隐私意识强”的用户也会无意中通过间接披露泄露敏感信息。基于该研究的洞察，我们提出了一种本地可部署的框架，在用户和大语言模型之间运作，并识别和重新构思用户提示中的脱节信息。通过对ShareGPT示例的评估显示，轻量级模型可以有效实施该框架，在不同分类信息相关性的方法下，实现强大的上下文隐私增强，同时保持用户预期的互动目标。', 'title_zh': '保护用户免受自身风险：在与对话代理互动时保障上下文隐私'}
{'arxiv_id': 'arXiv:2502.18505', 'title': 'Comprehensive Analysis of Transparency and Accessibility of ChatGPT, DeepSeek, And other SoTA Large Language Models', 'authors': 'Ranjan Sapkota, Shaina Raza, Manoj Karkee', 'link': 'https://arxiv.org/abs/2502.18505', 'abstract': 'Despite increasing discussions on open-source Artificial Intelligence (AI), existing research lacks a discussion on the transparency and accessibility of state-of-the-art (SoTA) Large Language Models (LLMs). The Open Source Initiative (OSI) has recently released its first formal definition of open-source software. This definition, when combined with standard dictionary definitions and the sparse published literature, provide an initial framework to support broader accessibility to AI models such as LLMs, but more work is essential to capture the unique dynamics of openness in AI. In addition, concerns about open-washing, where models claim openness but lack full transparency, has been raised, which limits the reproducibility, bias mitigation, and domain adaptation of these models. In this context, our study critically analyzes SoTA LLMs from the last five years, including ChatGPT, DeepSeek, LLaMA, and others, to assess their adherence to transparency standards and the implications of partial openness. Specifically, we examine transparency and accessibility from two perspectives: open-source vs. open-weight models. Our findings reveal that while some models are labeled as open-source, this does not necessarily mean they are fully open-sourced. Even in the best cases, open-source models often do not report model training data, and code as well as key metrics, such as weight accessibility, and carbon emissions. To the best of our knowledge, this is the first study that systematically examines the transparency and accessibility of over 100 different SoTA LLMs through the dual lens of open-source and open-weight models. The findings open avenues for further research and call for responsible and sustainable AI practices to ensure greater transparency, accountability, and ethical deployment of these models.(DeepSeek transparency, ChatGPT accessibility, open source, DeepSeek open source)', 'abstract_zh': '尽管对开源人工智能（AI）的讨论日益增多，现有研究缺乏对最先进的（SoTA）大型语言模型（LLMs）的透明度和 accessibility 的讨论。开放源代码倡议（OSI）最近发布了其首个正式的开放源代码软件定义。结合标准词典定义和零星的已发表文献，这些定义为支持对如LLMs这样的AI模型更广泛的访问提供了一个初步框架，但还需要更多工作来捕获AI领域开放性的独特动态。此外，对所谓的“开放漂洗”现象的担忧被提出，即模型声称开放但缺乏充分的透明度，这限制了这些模型的可再现性、偏见缓解和领域适应性。在此背景下，我们批判性地分析了过去五年中的SoTA LLMs，包括ChatGPT、DeepSeek、LLaMA等，以评估它们对透明标准的遵守情况及其半开放性的影响。具体而言，我们从开源模型 vs. 开放权重模型的两个角度来考察透明度和可访问性。我们的研究发现，虽然一些模型被标记为开源，但这并不意味着它们是完全开源的。在最佳情况下，开源模型通常也不报告模型训练数据以及代码和关键指标，如权重的可访问性和碳排放。据我们所知，这是首次通过开源和开放权重模型的双重视角系统性地检查超过100个SoTA LLMs的透明度和可访问性。这些发现为进一步研究打开了新的途径，并呼吁负责任和可持续的AI实践，以确保这些模型具有更高的透明度、可问责性和道德利用。', 'title_zh': '全面分析ChatGPT、DeepSeek及其他领先大型语言模型的透明度与可访问性'}
{'arxiv_id': 'arXiv:2502.18504', 'title': 'TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice', 'authors': 'Aman Goel, Xian Carrie Wu, Zhe Wang, Dmitriy Bespalov, Yanjun Qi', 'link': 'https://arxiv.org/abs/2502.18504', 'abstract': 'Jailbreaking large-language models (LLMs) involves testing their robustness against adversarial prompts and evaluating their ability to withstand prompt attacks that could elicit unauthorized or malicious responses. In this paper, we present TurboFuzzLLM, a mutation-based fuzzing technique for efficiently finding a collection of effective jailbreaking templates that, when combined with harmful questions, can lead a target LLM to produce harmful responses through black-box access via user prompts. We describe the limitations of directly applying existing template-based attacking techniques in practice, and present functional and efficiency-focused upgrades we added to mutation-based fuzzing to generate effective jailbreaking templates automatically. TurboFuzzLLM achieves $\\geq$ 95\\% attack success rates (ASR) on public datasets for leading LLMs (including GPT-4o \\& GPT-4 Turbo), shows impressive generalizability to unseen harmful questions, and helps in improving model defenses to prompt attacks.', 'abstract_zh': '大规模语言模型（LLM）的越狱涉及测试其对敌对提示的鲁棒性，并评估其抵御可能引发未经授权或恶意响应的提示攻击的能力。本文提出了一种基于变异的模糊测试技术TurboFuzzLLM，该技术能够高效地发现一批有效的越狱模板，当这些模板与有害问题结合使用时，通过用户的提示可以黑盒方式使目标LLM产生有害响应。我们描述了直接应用现有模板攻击技术的局限性，并展示了我们为变异模糊测试添加的功能性和效率优化，以自动生成有效的越狱模板。TurboFuzzLLM在领先LLM（包括GPT-4o和GPT-4 Turbo）的公开数据集上实现了≥95%的攻击成功率（ASR），展现出对未见过的有害问题的强大泛化能力，并有助于改进模型对提示攻击的防御。', 'title_zh': 'TurboFuzzLLM: 基于突变的 fuzzing 加速用于实践中的大型语言模型逃狱攻击'}
{'arxiv_id': 'arXiv:2502.18499', 'title': 'Mechanistic Understanding of Language Models in Syntactic Code Completion', 'authors': 'Samuel Miller, Daking Rai, Ziyu Yao', 'link': 'https://arxiv.org/abs/2502.18499', 'abstract': "Recently, language models (LMs) have shown impressive proficiency in code generation tasks, especially when fine-tuned on code-specific datasets, commonly known as Code LMs. However, our understanding of the internal decision-making processes of Code LMs, such as how they use their (syntactic or semantic) knowledge, remains limited, which could lead to unintended harm as they are increasingly used in real life. This motivates us to conduct one of the first Mechanistic Interpretability works to understand how Code LMs perform a syntactic completion task, specifically the closing parenthesis task, on the CodeLlama-7b model (Roziere et al. 2023). Our findings reveal that the model requires middle-later layers until it can confidently predict the correct label for the closing parenthesis task. Additionally, we identify that while both multi-head attention (MHA) and feed-forward (FF) sub-layers play essential roles, MHA is particularly crucial. Furthermore, we also discover attention heads that keep track of the number of already closed parentheses precisely but may or may not promote a correct number of closing parentheses that are still missing, leading to a positive or negative impact on the model's performance.", 'abstract_zh': '最近，语言模型在代码生成任务中展现了令人印象深刻的成熟度，尤其是当它们针对代码特定的数据集进行微调时，这类模型通常被称为代码语言模型（Code LMs）。然而，我们对Code LMs内部决策过程的理解仍然有限，这可能导致它们在实际应用中产生不可预见的损害。这促使我们开展了针对Code Llama-7b模型（Roziere et al. 2023）的语法补全任务，进行一项机制可解释性研究，以了解Code LMs如何完成语法补全任务。我们的研究发现表明，模型需要中间层直到后期才能自信地预测闭括号任务的正确标签。此外，我们发现多头注意力（MHA）和前馈（FF）子层都扮演着关键角色，但MHA尤其重要。同时，我们还发现一些跟踪已闭合括号数量的注意力头，但这些头可能促进也可能不促进足够的闭括号，从而对模型性能产生正面或负面的影响。', 'title_zh': '语言模型在句法规则代码补全中的机制理解'}
{'arxiv_id': 'arXiv:2502.18489', 'title': 'LLM4EFFI: Leveraging Large Language Models to Enhance Code Efficiency and Correctness', 'authors': 'Tong Ye, Weigang Huang, Xuhong Zhang, Tengfei Ma, Peiyu Liu, Jianwei Yin, Wenhai Wang', 'link': 'https://arxiv.org/abs/2502.18489', 'abstract': 'Large Language Models (LLMs), particularly Code LLMs, have demonstrated impressive performance in code generation. Current research primarily focuses on the correctness of generated code, while efficiency remains less explored. Recent works have focused on modifying the initial version of the code to improve its efficiency. However, such refinements are limited by the algorithmic design and overall logic of the initial code, resulting in only incremental improvements. In contrast, when human developers write high-quality code, they typically begin by designing several potential solutions at the logical level, evaluating various algorithms and their complexities, and then proceeding to implement and optimize the solution. In this study, we introduce \\tool: \\uline{L}arge \\uline{L}anguage \\uline{M}odel for Code \\uline{Effi}ciency, a novel framework that enables LLMs to generate code that balances both efficiency and correctness. Specifically, \\tool divides the efficiency optimization process into two domains: algorithmic exploration in the logic domain and implementation optimization in the code domain. The correctness of the code is then guaranteed through a synthetic test case refinement process. This approach, which prioritizes efficiency before ensuring correctness, offers a new paradigm for efficient code generation. Experiments demonstrate that \\tool consistently improves both efficiency and correctness, achieving new state-of-the-art performance in code efficiency benchmarks across various LLM backbones.', 'abstract_zh': 'Large Language Models for Code Efficiency: A Framework Balancing Both Correctness and Efficiency', 'title_zh': 'LLM4EFFI：利用大型语言模型提升代码效率与正确性'}
{'arxiv_id': 'arXiv:2502.18482', 'title': 'MixLLM: Dynamic Routing in Mixed Large Language Models', 'authors': 'Xinyuan Wang, Yanchi Liu, Wei Cheng, Xujiang Zhao, Zhengzhang Chen, Wenchao Yu, Yanjie Fu, Haifeng Chen', 'link': 'https://arxiv.org/abs/2502.18482', 'abstract': "Large Language Models (LLMs) exhibit potential artificial generic intelligence recently, however, their usage is costly with high response latency. Given mixed LLMs with their own strengths and weaknesses, LLM routing aims to identify the most suitable model for each query in the stream to maximize response quality and minimize cost and latency. However, the challenges involve: (1) dynamic trade-offs among quality, cost, and latency; (2) enabling continual learning in deployed systems; and (3) navigating a varying (e.g., new LLM addition or old LLM removal) set of LLM candidates over time. To bridge these gaps, we develop MixLLM, a dynamic contextual-bandit-based routing system for query-LLM assignment. Specifically, we first leverage query tags to enhance query embeddings for the routing task. Next, we design lightweight prediction models to estimate the response qualities and costs of queries over LLMs. We then devise a meta-decision maker to choose the query-LLM assignments to best tradeoff response quality, cost, and latency. Finally, the system benefits from continual training, allowing it to adapt to evolving queries and user feedback over time. Our extensive experiments show that MixLLM achieves the best trade-offs in response quality, cost, and latency (97.25% of GPT-4's quality at 24.18% of the cost under the time constraint).", 'abstract_zh': "大规模语言模型（LLMs） recently exhibited potential artificial general intelligence, but their usage is costly with high response latency. Given mixed LLMs with their own strengths and weaknesses, LLM routing aims to identify the most suitable model for each query in the stream to maximize response quality and minimize cost and latency. However, challenges include: (1) dynamic trade-offs among quality, cost, and latency; (2) enabling continual learning in deployed systems; and (3) navigating a varying (e.g., new LLM addition or old LLM removal) set of LLM candidates over time. To bridge these gaps, we develop MixLLM, a dynamic contextual-bandit-based routing system for query-LLM assignment. Specifically, we first leverage query tags to enhance query embeddings for the routing task. Next, we design lightweight prediction models to estimate the response qualities and costs of queries over LLMs. We then devise a meta-decision maker to choose the query-LLM assignments to best tradeoff response quality, cost, and latency. Finally, the system benefits from continual training, allowing it to adapt to evolving queries and user feedback over time. Our extensive experiments show that MixLLM achieves the best trade-offs in response quality, cost, and latency (97.25% of GPT-4's quality at 24.18% of the cost under the time constraint).", 'title_zh': 'MixLLM: 混合大型语言模型的动态路由'}
{'arxiv_id': 'arXiv:2502.18480', 'title': 'QExplorer: Large Language Model Based Query Extraction for Toxic Content Exploration', 'authors': 'Shaola Ren, Li Ke, Longtao Huang, Dehong Gao, Hui Xue', 'link': 'https://arxiv.org/abs/2502.18480', 'abstract': 'Automatically extracting effective queries is challenging in information retrieval, especially in toxic content exploration, as such content is likely to be disguised. With the recent achievements in generative Large Language Model (LLM), we are able to leverage the capabilities of LLMs to extract effective queries for similar content exploration directly. This study proposes QExplorer, an approach of large language model based Query Extraction for toxic content Exploration. The QExplorer approach involves a 2-stage training process: instruction Supervised FineTuning (SFT) and preference alignment using Direct Preference Optimization (DPO), as well as the datasets construction with feedback of search system. To verify the effectiveness of QExplorer, a series of offline and online experiments are conducted on our real-world system. The offline empirical results demonstrate that the performance of our automatic query extraction outperforms that of several LLMs and humans. The online deployment shows a significant increase in the detection of toxic items.', 'abstract_zh': '基于大型语言模型的有毒内容探索查询提取方法QExplorer', 'title_zh': 'QExplorer：基于大型语言模型的有毒内容查询提取'}
{'arxiv_id': 'arXiv:2502.18474', 'title': 'A Contemporary Survey of Large Language Model Assisted Program Analysis', 'authors': 'Jiayimei Wang, Tao Ni, Wei-Bin Lee, Qingchuan Zhao', 'link': 'https://arxiv.org/abs/2502.18474', 'abstract': 'The increasing complexity of software systems has driven significant advancements in program analysis, as traditional methods unable to meet the demands of modern software development. To address these limitations, deep learning techniques, particularly Large Language Models (LLMs), have gained attention due to their context-aware capabilities in code comprehension. Recognizing the potential of LLMs, researchers have extensively explored their application in program analysis since their introduction. Despite existing surveys on LLM applications in cybersecurity, comprehensive reviews specifically addressing their role in program analysis remain scarce. In this survey, we systematically review the application of LLMs in program analysis, categorizing the existing work into static analysis, dynamic analysis, and hybrid approaches. Moreover, by examining and synthesizing recent studies, we identify future directions and challenges in the field. This survey aims to demonstrate the potential of LLMs in advancing program analysis practices and offer actionable insights for security researchers seeking to enhance detection frameworks or develop domain-specific models.', 'abstract_zh': '随着软件系统的日益复杂，程序分析取得了显著进展，传统的分析方法已无法满足现代软件开发的需求。为了解决这些局限性，深度学习技术，尤其是大型语言模型（LLMs），因其在代码理解方面的上下文感知能力而引起了广泛关注。鉴于LLMs的潜力，研究人员在其推出后广泛探索了它们在程序分析中的应用。尽管已有针对LLMs在网络安全中应用的综述，但专门讨论其在程序分析中作用的综合研究仍然较少。在这篇综述中，我们系统地回顾了LLMs在程序分析中的应用，将现有研究归纳为静态分析、动态分析和混合方法。此外，通过对近期研究的分析和综合，我们指出了该领域的未来方向和挑战。本文旨在展示LLMs在推动程序分析实践方面的发展潜力，并为寻求改进检测框架或开发领域特定模型的网络安全研究人员提供可行的见解。', 'title_zh': '大型语言模型辅助程序分析的当代调研'}
{'arxiv_id': 'arXiv:2502.18471', 'title': 'FinBloom: Knowledge Grounding Large Language Model with Real-time Financial Data', 'authors': 'Ankur Sinha, Chaitanya Agarwal, Pekka Malo', 'link': 'https://arxiv.org/abs/2502.18471', 'abstract': 'Large language models (LLMs) excel at generating human-like responses but often struggle with interactive tasks that require access to real-time information. This limitation poses challenges in finance, where models must access up-to-date information, such as recent news or price movements, to support decision-making. To address this, we introduce Financial Agent, a knowledge-grounding approach for LLMs to handle financial queries using real-time text and tabular data. Our contributions are threefold: First, we develop a Financial Context Dataset of over 50,000 financial queries paired with the required context. Second, we train FinBloom 7B, a custom 7 billion parameter LLM, on 14 million financial news articles from Reuters and Deutsche Presse-Agentur, alongside 12 million Securities and Exchange Commission (SEC) filings. Third, we fine-tune FinBloom 7B using the Financial Context Dataset to serve as a Financial Agent. This agent generates relevant financial context, enabling efficient real-time data retrieval to answer user queries. By reducing latency and eliminating the need for users to manually provide accurate data, our approach significantly enhances the capability of LLMs to handle dynamic financial tasks. Our proposed approach makes real-time financial decisions, algorithmic trading and other related tasks streamlined, and is valuable in contexts with high-velocity data flows.', 'abstract_zh': '基于知识 grounding 的大语言模型在处理实时金融查询方面的金融代理方法', 'title_zh': 'FinBloom：基于实时金融数据的大型语言模型知识接地'}
{'arxiv_id': 'arXiv:2502.18468', 'title': 'SOK: Exploring Hallucinations and Security Risks in AI-Assisted Software Development with Insights for LLM Deployment', 'authors': 'Ariful Haque, Sunzida Siddique, Md. Mahfuzur Rahman, Ahmed Rafi Hasan, Laxmi Rani Das, Marufa Kamal, Tasnim Masura, Kishor Datta Gupta', 'link': 'https://arxiv.org/abs/2502.18468', 'abstract': 'The integration of Large Language Models (LLMs) such as GitHub Copilot, ChatGPT, Cursor AI, and Codeium AI into software development has revolutionized the coding landscape, offering significant productivity gains, automation, and enhanced debugging capabilities. These tools have proven invaluable for generating code snippets, refactoring existing code, and providing real-time support to developers. However, their widespread adoption also presents notable challenges, particularly in terms of security vulnerabilities, code quality, and ethical concerns. This paper provides a comprehensive analysis of the benefits and risks associated with AI-powered coding tools, drawing on user feedback, security analyses, and practical use cases. We explore the potential for these tools to replicate insecure coding practices, introduce biases, and generate incorrect or non-sensical code (hallucinations). In addition, we discuss the risks of data leaks, intellectual property violations and the need for robust security measures to mitigate these threats. By comparing the features and performance of these tools, we aim to guide developers in making informed decisions about their use, ensuring that the benefits of AI-assisted coding are maximized while minimizing associated risks.', 'abstract_zh': '大型语言模型（LLMs）如GitHub Copilot、ChatGPT、Cursor AI和Codeium AI在软件开发中的集成已经革新了编程landscape，提供了显著的生产力提升、自动化和增强的调试能力。这些工具在生成代码片段、重构现有代码和为开发者提供实时支持方面证明了其价值。然而，它们的广泛应用也带来了显著的挑战，特别是在安全漏洞、代码质量以及伦理问题方面。本文通过对用户反馈、安全分析和实际案例进行综合分析，探讨了AI助力编程工具的优势与风险。我们研究了这些工具复制不安全编程实践、引入偏见以及生成错误或无意义代码（幻觉）的潜在风险。此外，我们还讨论了数据泄露、知识产权侵犯的风险以及需要采取 robust 安全措施来应对这些威胁。通过比较这些工具的功能和性能，我们旨在指导开发者做出明智的使用决策，确保AI辅助编程的益处最大化，同时最小化相关风险。', 'title_zh': 'SOK：探索AI辅助软件开发中的幻觉和安全风险及对大规模语言模型部署的洞察'}
{'arxiv_id': 'arXiv:2502.18467', 'title': 'ChatGPT vs. DeepSeek: A Comparative Study on AI-Based Code Generation', 'authors': 'Md Motaleb Hossen Manik', 'link': 'https://arxiv.org/abs/2502.18467', 'abstract': "Background: AI-powered code generation, fueled by Large Language Models (LLMs), is revolutionizing software development. Models like OpenAI's Codex and GPT-4, alongside DeepSeek, leverage vast code and natural language datasets. However, ensuring code quality, correctness, and managing complex tasks remains challenging, necessitating thorough evaluation. Methodology: This research compares ChatGPT (version o1) and DeepSeek (version R1) for Python code generation using online judge coding challenges. It evaluates correctness (online judge verdicts, up to three attempts), code quality (Pylint/Flake8), and efficiency (execution time/memory usage). Results: DeepSeek demonstrated higher correctness, particularly on algorithmic tasks, often achieving 'Accepted' on the first attempt. ChatGPT sometimes requires multiple attempts or failures. ChatGPT encountered fewer issues, used comparable or slightly less memory, consumed less execution times and wrote fewer lines of code. Conclusion: DeepSeek exhibited superior correctness in Python code generation, often requiring fewer attempts, suggesting an advantage in algorithmic problem-solving. Both models showed almost similar efficiency in execution time and memory use. Finally, this research provides insights for developers choosing AI coding assistants and informs future AI-driven software development research.", 'abstract_zh': '背景：由大规模语言模型（LLMs）驱动的人工智能代码生成正在革新软件开发。像OpenAI的Codex和GPT-4以及DeepSeek这样的模型利用了大量的代码和自然语言数据集。然而，确保代码质量、正确性并管理复杂任务仍然具有挑战性，需要进行全面评估。方法：本研究比较了ChatGPT（版本o1）和DeepSeek（版本R1）在使用在线判题编程挑战中的Python代码生成。本研究从正确性（在线判题结果，最多三次尝试）、代码质量（Pylint/Flake8）和效率（执行时间/内存使用）三个方面进行了评估。结果：DeepSeek在正确性方面表现更好，尤其是在算法任务方面，经常在第一次尝试就能得到“通过”。ChatGPT有时需要多次尝试或失败。ChatGPT遇到的问题较少，使用的内存与DeepSeek相当或略少，执行时间更短，并编写了更少的代码。结论：DeepSeek在Python代码生成的正确性方面展现出优势，通常需要较少的尝试，表明在算法问题解决方面具有优势。两种模型在执行时间和内存使用方面几乎相似。最终，本研究为选择AI编程助手的开发人员提供了见解，并对未来基于AI的软件开发研究产生了影响。', 'title_zh': 'ChatGPT 与 DeepSeek 的基于 AI 的代码生成 comparative study'}
