{'arxiv_id': 'arXiv:2502.19417', 'title': 'Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models', 'authors': 'Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, Chelsea Finn', 'link': 'https://arxiv.org/abs/2502.19417', 'abstract': 'Generalist robots that can perform a range of different tasks in open-world settings must be able to not only reason about the steps needed to accomplish their goals, but also process complex instructions, prompts, and even feedback during task execution. Intricate instructions (e.g., "Could you make me a vegetarian sandwich?" or "I don\'t like that one") require not just the ability to physically perform the individual steps, but the ability to situate complex commands and feedback in the physical world. In this work, we describe a system that uses vision-language models in a hierarchical structure, first reasoning over complex prompts and user feedback to deduce the most appropriate next step to fulfill the task, and then performing that step with low-level actions. In contrast to direct instruction following methods that can fulfill simple commands ("pick up the cup"), our system can reason through complex prompts and incorporate situated feedback during task execution ("that\'s not trash"). We evaluate our system across three robotic platforms, including single-arm, dual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks such as cleaning messy tables, making sandwiches, and grocery shopping.', 'abstract_zh': '能够在开放环境中执行多种不同任务的通用机器人必须能够不仅推理出完成目标所需的步骤，还能处理复杂指令、提示，甚至在任务执行过程中接收反馈。复杂的指令（例如，“你能为我做一个素食三明治吗？”或“我不喜欢那个”）要求不仅具备执行个体步骤的能力，还具备将复杂命令和反馈置于物理世界中的能力。在本工作中，我们描述了一个层级结构中使用视觉语言模型的系统，首先通过推理复杂的提示和用户反馈来推断出最合适的下一步以完成任务，然后通过低级动作执行该步骤。与可以执行简单命令（如“拿起杯子”）的直接指令跟随方法不同，我们的系统能够通过复杂的提示进行推理，并在任务执行过程中整合位置反馈（如“那不是垃圾”）。我们跨三个机器人平台评估了该系统，包括单臂、双臂以及双臂移动机器人，展示了其处理诸如清理脏桌子、制作三明治和购物等任务的能力。', 'title_zh': 'Hi 机器人：基于层次视觉-语言-动作模型的开放性指令跟随'}
{'arxiv_id': 'arXiv:2502.19374', 'title': 'LiDAR Registration with Visual Foundation Models', 'authors': 'Niclas Vödisch, Giovanni Cioffi, Marco Cannici, Wolfram Burgard, Davide Scaramuzza', 'link': 'https://arxiv.org/abs/2502.19374', 'abstract': 'LiDAR registration is a fundamental task in robotic mapping and localization. A critical component of aligning two point clouds is identifying robust point correspondences using point descriptors. This step becomes particularly challenging in scenarios involving domain shifts, seasonal changes, and variations in point cloud structures. These factors substantially impact both handcrafted and learning-based approaches. In this paper, we address these problems by proposing to use DINOv2 features, obtained from surround-view images, as point descriptors. We demonstrate that coupling these descriptors with traditional registration algorithms, such as RANSAC or ICP, facilitates robust 6DoF alignment of LiDAR scans with 3D maps, even when the map was recorded more than a year before. Although conceptually straightforward, our method substantially outperforms more complex baseline techniques. In contrast to previous learning-based point descriptors, our method does not require domain-specific retraining and is agnostic to the point cloud structure, effectively handling both sparse LiDAR scans and dense 3D maps. We show that leveraging the additional camera data enables our method to outperform the best baseline by +24.8 and +17.3 registration recall on the NCLT and Oxford RobotCar datasets. We publicly release the registration benchmark and the code of our work on this https URL.', 'abstract_zh': '基于DINOv2特征的LiDAR点云配准方法', 'title_zh': '基于视觉基础模型的LiDAR注册'}
{'arxiv_id': 'arXiv:2502.19340', 'title': 'Hybrid Robot Learning for Automatic Robot Motion Planning in Manufacturing', 'authors': 'Siddharth Singh, Tian Yu, Qing Chang, John Karigiannis, Shaopeng Liu', 'link': 'https://arxiv.org/abs/2502.19340', 'abstract': 'Industrial robots are widely used in diverse manufacturing environments. Nonetheless, how to enable robots to automatically plan trajectories for changing tasks presents a considerable challenge. Further complexities arise when robots operate within work cells alongside machines, humans, or other robots. This paper introduces a multi-level hybrid robot motion planning method combining a task space Reinforcement Learning-based Learning from Demonstration (RL-LfD) agent and a joint-space based Deep Reinforcement Learning (DRL) based agent. A higher level agent learns to switch between the two agents to enable feasible and smooth motion. The feasibility is computed by incorporating reachability, joint limits, manipulability, and collision risks of the robot in the given environment. Therefore, the derived hybrid motion planning policy generates a feasible trajectory that adheres to task constraints. The effectiveness of the method is validated through sim ulated robotic scenarios and in a real-world setup.', 'abstract_zh': '工业机器人在多样化制造环境中广泛应用，然而如何使机器人能够自动规划适应变化任务的轨迹仍是一项重大挑战。当机器人在工作单元中与机器、人类或其他机器人协同工作时，这一挑战变得更加复杂。本文提出了一种多层混合机器人运动规划方法，结合了基于任务空间的Reinforcement Learning-基于演示学习（RL-LfD）代理和基于关节空间的深度强化学习（DRL）代理。高层代理学习在两者之间切换，以实现可行且平滑的运动。可行性的计算通过综合考虑给定环境中机器人的可达性、关节极限、操作性和碰撞风险来实现。因此，所提取的混合运动规划策略生成了一条符合任务约束的可行轨迹。该方法的有效性通过模拟的机器人场景和实际应用中的验证得到了证明。', 'title_zh': '制造领域自动机器人运动规划的混合机器人学习方法'}
{'arxiv_id': 'arXiv:2502.19250', 'title': 'ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration', 'authors': 'Minjie Zhu, Yichen Zhu, Jinming Li, Zhongyi Zhou, Junjie Wen, Xiaoyu Liu, Chaomin Shen, Yaxin Peng, Feifei Feng', 'link': 'https://arxiv.org/abs/2502.19250', 'abstract': 'Imitation learning has proven to be highly effective in teaching robots dexterous manipulation skills. However, it typically relies on large amounts of human demonstration data, which limits its scalability and applicability in dynamic, real-world environments. One key challenge in this context is object generalization, where a robot trained to perform a task with one object, such as "hand over the apple," struggles to transfer its skills to a semantically similar but visually different object, such as "hand over the peach." This gap in generalization to new objects beyond those in the same category has yet to be adequately addressed in previous work on end-to-end visuomotor policy learning. In this paper, we present a simple yet effective approach for achieving object generalization through Vision-Language-Action (VLA) models, referred to as \\textbf{ObjectVLA}. Our model enables robots to generalize learned skills to novel objects without requiring explicit human demonstrations for each new target object. By leveraging vision-language pair data, our method provides a lightweight and scalable way to inject knowledge about the target object, establishing an implicit link between the object and the desired action. We evaluate ObjectVLA on a real robotic platform, demonstrating its ability to generalize across 100 novel objects with a 64\\% success rate in selecting objects not seen during training. Furthermore, we propose a more accessible method for enhancing object generalization in VLA models, using a smartphone to capture a few images and fine-tune the pre-trained model. These results highlight the effectiveness of our approach in enabling object-level generalization and reducing the need for extensive human demonstrations, paving the way for more flexible and scalable robotic learning systems.', 'abstract_zh': '模仿学习已被证明在教学徒机器人灵巧操作技能方面非常有效。然而，它通常依赖大量的人类演示数据，这限制了其在动态现实环境中的可扩展性和适用性。在这种背景下，一个关键挑战是对象泛化，即一个被训练执行一项任务（例如“把苹果递过来”）的机器人，在面对语义相似但外观不同的对象（例如“把桃子递过来”）时难以将技能迁移到新的对象上。在先前的端到端视觉-运动策略学习工作中，这一新类别对象的泛化问题尚未得到充分解决。在本文中，我们提出了一种通过视觉-语言-动作（VLA）模型实现对象泛化的简单有效方法，称为**ObjectVLA**。我们的模型使机器人能够在不需要为每个新目标对象提供明确的人类演示数据的情况下，将已学习的技能泛化到新对象上。通过利用视觉-语言配对数据，我们的方法提供了一种轻量级且可扩展的方式来注入目标对象的知识，从而在对象和所需动作之间建立隐式联系。我们在真实的机器人平台上评估了ObjectVLA，结果显示它能够在选择训练期间未见过的100个新对象中成功率达到64%。此外，我们提出了一种更简便的方法来增强VLA模型的对象泛化能力，使用智能手机捕捉几张图像并对预训练模型进行微调。这些结果突显了我们方法在实现对象级别泛化和减少大量人类演示需求方面的有效性，为更灵活和可扩展的机器人学习系统铺平了道路。', 'title_zh': 'ObjectVLA: 无演示的全程开放世界对象操作'}
{'arxiv_id': 'arXiv:2502.19024', 'title': 'Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments', 'authors': 'Zerui Li, Gengze Zhou, Haodong Hong, Yanyan Shao, Wenqi Lyu, Yanyuan Qiao, Qi Wu', 'link': 'https://arxiv.org/abs/2502.19024', 'abstract': 'Vision-and-Language Navigation (VLN) empowers agents to associate time-sequenced visual observations with corresponding instructions to make sequential decisions. However, generalization remains a persistent challenge, particularly when dealing with visually diverse scenes or transitioning from simulated environments to real-world deployment. In this paper, we address the mismatch between human-centric instructions and quadruped robots with a low-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav) approach to mitigate this issue. This work represents the first attempt to highlight the generalization gap in VLN across varying heights of visual observation in realistic robot deployments. Our approach leverages weighted historical observations as enriched spatiotemporal contexts for instruction following, effectively managing feature collisions within cells by assigning appropriate weights to identical features across different viewpoints. This enables low-height robots to overcome challenges such as visual obstructions and perceptual mismatches. Additionally, we transfer the connectivity graph from the HM3D and Gibson datasets as an extra resource to enhance spatial priors and a more comprehensive representation of real-world scenarios, leading to improved performance and generalizability of the waypoint predictor in real-world environments. Extensive experiments demonstrate that our Ground-level Viewpoint Navigation (GVnav) approach significantly improves performance in both simulated environments and real-world deployments with quadruped robots.', 'abstract_zh': '基于地面视角导航（GVNav）的视觉-语言导航（VLN）', 'title_zh': '连续环境中基于视点的视觉-语言导航'}
{'arxiv_id': 'arXiv:2502.18932', 'title': 'SLAM in the Dark: Self-Supervised Learning of Pose, Depth and Loop-Closure from Thermal Images', 'authors': 'Yangfan Xu, Qu Hao, Lilian Zhang, Jun Mao, Xiaofeng He, Wenqi Wu, Changhao Chen', 'link': 'https://arxiv.org/abs/2502.18932', 'abstract': 'Visual SLAM is essential for mobile robots, drone navigation, and VR/AR, but traditional RGB camera systems struggle in low-light conditions, driving interest in thermal SLAM, which excels in such environments. However, thermal imaging faces challenges like low contrast, high noise, and limited large-scale annotated datasets, restricting the use of deep learning in outdoor scenarios. We present DarkSLAM, a noval deep learning-based monocular thermal SLAM system designed for large-scale localization and reconstruction in complex lighting this http URL approach incorporates the Efficient Channel Attention (ECA) mechanism in visual odometry and the Selective Kernel Attention (SKA) mechanism in depth estimation to enhance pose accuracy and mitigate thermal depth degradation. Additionally, the system includes thermal depth-based loop closure detection and pose optimization, ensuring robust performance in low-texture thermal scenes. Extensive outdoor experiments demonstrate that DarkSLAM significantly outperforms existing methods like SC-Sfm-Learner and Shin et al., delivering precise localization and 3D dense mapping even in challenging nighttime environments.', 'abstract_zh': '基于深度学习的暗光SLAM：一种适用于复杂光照条件的大规模定位与重建系统', 'title_zh': '黑暗中SLAM：从热图像中自学姿态、深度和环路闭合'}
{'arxiv_id': 'arXiv:2502.18901', 'title': 'Think on your feet: Seamless Transition between Human-like Locomotion in Response to Changing Commands', 'authors': 'Huaxing Huang, Wenhao Cui, Tonghe Zhang, Shengtao Li, Jinchao Han, Bangyu Qin, Tianchu Zhang, Liang Zheng, Ziyang Tang, Chenxu Hu, Ning Yan, Jiahao Chen, Shipu Zhang, Zheyuan Jiang', 'link': 'https://arxiv.org/abs/2502.18901', 'abstract': 'While it is relatively easier to train humanoid robots to mimic specific locomotion skills, it is more challenging to learn from various motions and adhere to continuously changing commands. These robots must accurately track motion instructions, seamlessly transition between a variety of movements, and master intermediate motions not present in their reference data. In this work, we propose a novel approach that integrates human-like motion transfer with precise velocity tracking by a series of improvements to classical imitation learning. To enhance generalization, we employ the Wasserstein divergence criterion (WGAN-div). Furthermore, a Hybrid Internal Model provides structured estimates of hidden states and velocity to enhance mobile stability and environment adaptability, while a curiosity bonus fosters exploration. Our comprehensive method promises highly human-like locomotion that adapts to varying velocity requirements, direct generalization to unseen motions and multitasking, as well as zero-shot transfer to the simulator and the real world across different terrains. These advancements are validated through simulations across various robot models and extensive real-world experiments.', 'abstract_zh': '一种结合人类运动转移与精确速度跟踪的改进 imitation 学习方法：适应变化速度要求、未见动作的直接泛化及多任务学习的研究', 'title_zh': '随机应变：根据变化的指令实现类人的运动无缝过渡'}
{'arxiv_id': 'arXiv:2502.18760', 'title': 'Learning Autonomy: Off-Road Navigation Enhanced by Human Input', 'authors': 'Akhil Nagariya, Dimitar Filev, Srikanth Saripalli, Gaurav Pandey', 'link': 'https://arxiv.org/abs/2502.18760', 'abstract': 'In the area of autonomous driving, navigating off-road terrains presents a unique set of challenges, from unpredictable surfaces like grass and dirt to unexpected obstacles such as bushes and puddles. In this work, we present a novel learning-based local planner that addresses these challenges by directly capturing human driving nuances from real-world demonstrations using only a monocular camera. The key features of our planner are its ability to navigate in challenging off-road environments with various terrain types and its fast learning capabilities. By utilizing minimal human demonstration data (5-10 mins), it quickly learns to navigate in a wide array of off-road conditions. The local planner significantly reduces the real world data required to learn human driving preferences. This allows the planner to apply learned behaviors to real-world scenarios without the need for manual fine-tuning, demonstrating quick adjustment and adaptability in off-road autonomous driving technology.', 'abstract_zh': '在自主驾驶领域，穿越非沥青路面地形呈现一系列独特挑战，从不可预测的草地和泥土表面到突如其来的灌木丛和水坑等障碍。本文提出了一种新颖的学习型局部规划器，通过仅使用单目摄像头直接从实际场景演示中捕捉人类驾驶的细微之处来应对这些挑战。该规划器的关键特性在于其能够在多种地形类型的复杂非沥青路面上导航，并且具有快速学习能力。通过利用少量的人类演示数据（5-10分钟），它能够迅速学习在各种非沥青路面上的导航技巧。该局部规划器显著减少了学习人类驾驶偏好的所需实际场景数据量。这使得规划器能够在不需要手动微调的情况下，将学习到的行为应用到实际场景中，展示了在非沥青路面自主驾驶技术中的快速调整和适应性。', 'title_zh': '自主学习：增强型离线导航由人类输入辅助'}
{'arxiv_id': 'arXiv:2502.18749', 'title': 'Simulating Safe Bite Transfer in Robot-Assisted Feeding with a Soft Head and Articulated Jaw', 'authors': 'Yi Heng San, Vasanthamaran Ravichandram, J-Anne Yow, Sherwin Stephen Chan, Yifan Wang, Wei Tech Ang', 'link': 'https://arxiv.org/abs/2502.18749', 'abstract': 'Ensuring safe and comfortable bite transfer during robot-assisted feeding is challenging due to the close physical human-robot interaction required. This paper presents a novel approach to modeling physical human-robot interaction in a physics-based simulator (MuJoCo) using soft-body dynamics. We integrate a flexible head model with a rigid skeleton while accounting for internal dynamics, enabling the flexible model to be actuated by the skeleton. Incorporating realistic soft-skin contact dynamics in simulation allows for systematically evaluating bite transfer parameters, such as insertion depth and entry angle, and their impact on user safety and comfort. Our findings suggest that a straight-in-straight-out strategy minimizes forces and enhances user comfort in robot-assisted feeding, assuming a static head. This simulation-based approach offers a safer and more controlled alternative to real-world experimentation. Supplementary videos can be found at: this https URL.', 'abstract_zh': '确保机器人辅助喂食过程中安全舒适的咬合转移具有挑战性，因为需要进行紧密的物理人机交互。本文提出了一种在基于物理的模拟器（MuJoCo）中使用软体动力学建模物理人机交互的新型方法。我们整合了一个可挠头模型和刚性骨架，并考虑了内部动力学，使柔性模型可以由骨架驱动。在仿真中引入现实的软皮肤接触动力学，可以系统地评估咬合转移参数，如插入深度和进入角度，以及它们对用户安全性和舒适性的影响。我们的研究结果表明，在假设头部静止的情况下，直线进出策略可以最小化力并提高机器人辅助喂食的用户舒适度。基于仿真的方法为实际实验提供了更安全、更可控的替代方案。有关补充视频，请参阅：this https URL。', 'title_zh': '基于柔软头部和articulated颚的机器人辅助喂食中安全咬合转移的模拟'}
{'arxiv_id': 'arXiv:2502.18688', 'title': 'Rapidly Built Medical Crash Cart! Lessons Learned and Impacts on High-Stakes Team Collaboration in the Emergency Room', 'authors': 'Angelique Taylor, Tauhid Tanjim, Michael Joseph Sack, Maia Hirsch, Kexin Cheng, Kevin Ching, Jonathan St. George, Thijs Roumen, Malte F. Jung, Hee Rin Lee', 'link': 'https://arxiv.org/abs/2502.18688', 'abstract': 'Designing robots to support high-stakes teamwork in emergency settings presents unique challenges, including seamless integration into fast-paced environments, facilitating effective communication among team members, and adapting to rapidly changing situations. While teleoperated robots have been successfully used in high-stakes domains such as firefighting and space exploration, autonomous robots that aid highs-takes teamwork remain underexplored. To address this gap, we conducted a rapid prototyping process to develop a series of seemingly autonomous robot designed to assist clinical teams in the Emergency Room. We transformed a standard crash cart--which stores medical equipment and emergency supplies into a medical robotic crash cart (MCCR). The MCCR was evaluated through field deployments to assess its impact on team workload and usability, identified taxonomies of failure, and refined the MCCR in collaboration with healthcare professionals. Our work advances the understanding of robot design for high-stakes, time-sensitive settings, providing insights into useful MCCR capabilities and considerations for effective human-robot collaboration. By publicly disseminating our MCCR tutorial, we hope to encourage HRI researchers to explore the design of robots for high-stakes teamwork.', 'abstract_zh': '设计用于紧急情况下高风险团队支持的机器人面临独特挑战，包括无缝融入快节奏环境、促进团队成员间有效沟通以及适应快速变化的情况。虽然远程操作机器人已在灭火和太空探索等高风险领域成功应用，但辅助高风险团队协作的自主机器人仍处于探索阶段。为解决这一缺口，我们通过快速原型设计过程开发了一系列看似自主的机器人，旨在协助急诊室临床团队。我们将标准的急救车转变为医疗机器人急救车（MCCR）。通过实地部署评估MCCR对团队工作负荷的影响和易用性，识别失败模式，并与医疗专业人员合作改进MCCR。我们的工作推进了对高风险、时间敏感环境中机器人设计的理解，提供了有关有用MCCR功能和有效人机协作考虑的见解。通过公开发布MCCR教程，我们希望鼓励HRI研究人员探索高风险团队协作中机器人的设计。', 'title_zh': '快速构建的医疗抢救车！紧急室高 stakes 团队协作中的经验教训及影响'}
{'arxiv_id': 'arXiv:2502.18615', 'title': 'A Distributional Treatment of Real2Sim2Real for Vision-Driven Deformable Linear Object Manipulation', 'authors': 'Georgios Kamaras, Subramanian Ramamoorthy', 'link': 'https://arxiv.org/abs/2502.18615', 'abstract': 'We present an integrated (or end-to-end) framework for the Real2Sim2Real problem of manipulating deformable linear objects (DLOs) based on visual perception. Working with a parameterised set of DLOs, we use likelihood-free inference (LFI) to compute the posterior distributions for the physical parameters using which we can approximately simulate the behaviour of each specific DLO. We use these posteriors for domain randomisation while training, in simulation, object-specific visuomotor policies for a visuomotor DLO reaching task, using model-free reinforcement learning. We demonstrate the utility of this approach by deploying sim-trained DLO manipulation policies in the real world in a zero-shot manner, i.e. without any further fine-tuning. In this context, we evaluate the capacity of a prominent LFI method to perform fine classification over the parametric set of DLOs, using only visual and proprioceptive data obtained in a dynamic manipulation trajectory. We then study the implications of the resulting domain distributions in sim-based policy learning and real-world performance.', 'abstract_zh': '基于视觉感知的柔体线性对象 manipulatiion 的端到端框架：从真实世界到模拟再到现实世界', 'title_zh': '基于视觉驱动的可变形线性物体操纵的分布处理从真实到模拟再到真实的转变'}
{'arxiv_id': 'arXiv:2502.19192', 'title': 'Embodying mechano-fluidic memory in soft machines to program behaviors upon interactions', 'authors': 'Alberto Comoretto, Tanaya Mandke, Johannes T.B. Overvelde', 'link': 'https://arxiv.org/abs/2502.19192', 'abstract': "Soft machines display shape adaptation to external circumstances due to their intrinsic compliance. To achieve increasingly more responsive behaviors upon interactions without relying on centralized computation, embodying memory directly in the machines' structure is crucial. Here, we harness the bistability of elastic shells to alter the fluidic properties of an enclosed cavity, thereby switching between stable frequency states of a locomoting self-oscillating machine. To program these memory states upon interactions, we develop fluidic circuits surrounding the bistable shell, with soft tubes that kink and unkink when externally touched. We implement circuits for both long-term and short-term memory in a soft machine that switches behaviors in response to a human user and that autonomously changes direction after detecting a wall. By harnessing only geometry and elasticity, embodying memory allows physical structures without a central brain to exhibit autonomous feats that are typically reserved for computer-based robotic systems.", 'abstract_zh': '软机器通过其固有的顺应性对外部环境进行形状适应。为了在不依赖集中计算的情况下实现更加响应性的行为，直接在机器的结构中体现记忆至关重要。在这里，我们利用弹性薄壳的双稳定特性改变其包围腔体的流体属性，从而在行进的自振荡机器之间切换稳定频率状态。为了在交互中编程这些记忆状态，我们开发了围绕双稳态壳体的流体电路，其中软管在外部触碰时会弯曲和恢复。我们在一个能够根据人类用户的交互表现出不同行为并在检测到墙壁后自主改变方向的软机器中实现长期和短期记忆的电路。仅仅利用几何和弹性，体现记忆使得没有中央大脑的物理结构能够展示出通常只能由基于计算机的机器人系统实现的自主行为。', 'title_zh': '在软机器中嵌入 mechano-fluidic 记忆以编程交互行为'}
{'arxiv_id': 'arXiv:2502.19171', 'title': 'PlantPal: Leveraging Precision Agriculture Robots to Facilitate Remote Engagement in Urban Gardening', 'authors': 'Albin Zeqiri, Julian Britten, Clara Schramm, Pascal Jansen, Michael Rietzler, Enrico Rukzio', 'link': 'https://arxiv.org/abs/2502.19171', 'abstract': "Urban gardening is widely recognized for its numerous health and environmental benefits. However, the lack of suitable garden spaces, demanding daily schedules and limited gardening expertise present major roadblocks for citizens looking to engage in urban gardening. While prior research has explored smart home solutions to support urban gardeners, these approaches currently do not fully address these practical barriers. In this paper, we present PlantPal, a system that enables the cultivation of garden spaces irrespective of one's location, expertise level, or time constraints. PlantPal enables the shared operation of a precision agriculture robot (PAR) that is equipped with garden tools and a multi-camera system. Insights from a 3-week deployment (N=18) indicate that PlantPal facilitated the integration of gardening tasks into daily routines, fostered a sense of connection with one's field, and provided an engaging experience despite the remote setting. We contribute design considerations for future robot-assisted urban gardening concepts.", 'abstract_zh': '城市园艺因其众多的健康和环境益处而广受认可。然而，缺乏合适的花园空间、严苛的日常安排和有限的园艺技能是市民参与城市园艺的主要障碍。尽管先前的研究探讨了支持城市园艺者的智能家居解决方案，但这些方法目前尚未充分解决这些实际障碍。本文中，我们介绍了PlantPal系统，该系统能够在Regardless of 一个人的位置、技能水平或时间限制的情况下开展园艺活动。PlantPal通过一种配备有园艺工具和多摄像头系统的精确农业机器人（PAR），实现了多个园艺空间的操作共享。为期三周的部署（N=18）表明，PlantPal使得园艺任务能够融入日常生活，强化了与园地的联系，并提供了一种引人入胜的体验，即使在远程情况下也是如此。我们为未来的辅助机器人城市园艺概念提供了设计考虑。', 'title_zh': 'PlantPal: 利用精准农业机器人促进城市园艺的远程参与'}
{'arxiv_id': 'arXiv:2502.19009', 'title': 'Distilling Reinforcement Learning Algorithms for In-Context Model-Based Planning', 'authors': 'Jaehyeon Son, Soochan Lee, Gunhee Kim', 'link': 'https://arxiv.org/abs/2502.19009', 'abstract': 'Recent studies have shown that Transformers can perform in-context reinforcement learning (RL) by imitating existing RL algorithms, enabling sample-efficient adaptation to unseen tasks without parameter updates. However, these models also inherit the suboptimal behaviors of the RL algorithms they imitate. This issue primarily arises due to the gradual update rule employed by those algorithms. Model-based planning offers a promising solution to this limitation by allowing the models to simulate potential outcomes before taking action, providing an additional mechanism to deviate from the suboptimal behavior. Rather than learning a separate dynamics model, we propose Distillation for In-Context Planning (DICP), an in-context model-based RL framework where Transformers simultaneously learn environment dynamics and improve policy in-context. We evaluate DICP across a range of discrete and continuous environments, including Darkroom variants and Meta-World. Our results show that DICP achieves state-of-the-art performance while requiring significantly fewer environment interactions than baselines, which include both model-free counterparts and existing meta-RL methods.', 'abstract_zh': 'Recent Studies Show that Transformers Can Perform In-Context Reinforcement Learning by Imitating Existing RL Algorithms, but They Inherit Suboptimal Behaviors Due to the Gradual Update Rule of Those Algorithms. Model-Based Planning Offers a Promising Solution by Allowing Simulated Outcomes Before Taking Action, and We Propose Distillation for In-Context Planning (DICP) as an In-Context Model-Based RL Framework Where Transformers Learn Environment Dynamics and Improve Policy Simultaneously. Evaluations Across Discrete and Continuous Environments Demonstrate That DICP Achieves State-of-the-Art Performance with Significantly Fewer Environment Interactions Compared to Baselines.', 'title_zh': '基于上下文的模型导向规划的强化学习算法提炼'}
{'arxiv_id': 'arXiv:2502.18641', 'title': 'WhatELSE: Shaping Narrative Spaces at Configurable Level of Abstraction for AI-bridged Interactive Storytelling', 'authors': 'Zhuoran Lu, Qian Zhou, Yi Wang', 'link': 'https://arxiv.org/abs/2502.18641', 'abstract': 'Generative AI significantly enhances player agency in interactive narratives (IN) by enabling just-in-time content generation that adapts to player actions. While delegating generation to AI makes IN more interactive, it becomes challenging for authors to control the space of possible narratives - within which the final story experienced by the player emerges from their interaction with AI. In this paper, we present WhatELSE, an AI-bridged IN authoring system that creates narrative possibility spaces from example stories. WhatELSE provides three views (narrative pivot, outline, and variants) to help authors understand the narrative space and corresponding tools leveraging linguistic abstraction to control the boundaries of the narrative space. Taking innovative LLM-based narrative planning approaches, WhatELSE further unfolds the narrative space into executable game events. Through a user study (N=12) and technical evaluations, we found that WhatELSE enables authors to perceive and edit the narrative space and generates engaging interactive narratives at play-time.', 'abstract_zh': '生成式AI显著增强了交互叙事（IN）中的玩家主动权，通过实现即时内容生成并根据玩家行为进行调整。虽然将生成任务委托给AI使IN更加互动，但作者控制可能叙事空间变得更具挑战性——在这一空间中，最终由玩家与AI互动体验到的故事得以浮现。本文介绍了WhatELSE，这是一种AI桥接的交互叙事作者系统，能够从示例故事中创建叙事可能性空间。WhatELSE 提供三种视图（叙事关键点、大纲和变体）来帮助作者理解叙事空间，并利用语言抽象提供相应工具以控制叙事空间的边界。利用创新的基于大语言模型的叙事规划方法，WhatELSE 进一步将叙事空间展开为可执行的游戏事件。通过用户研究（参与者数量为12）和技术评估，我们发现WhatELSE使作者能够感知和编辑叙事空间，并在游戏过程中生成引人入胜的交互叙事。', 'title_zh': 'WhatELSE：在可配置抽象层次上塑造叙事空间的AI桥梁式互动叙事'}
{'arxiv_id': 'arXiv:2502.18639', 'title': 'Quantum Machine Learning in Precision Medicine and Drug Discovery -- A Game Changer for Tailored Treatments?', 'authors': 'Markus Bertl, Alan Mott, Salvatore Sinno, Bhavika Bhalgamiya', 'link': 'https://arxiv.org/abs/2502.18639', 'abstract': 'The digitization of healthcare presents numerous challenges, including the complexity of biological systems, vast data generation, and the need for personalized treatment plans. Traditional computational methods often fall short, leading to delayed and sometimes ineffective diagnoses and treatments. Quantum Computing (QC) and Quantum Machine Learning (QML) offer transformative advancements with the potential to revolutionize medicine. This paper summarizes areas where QC promises unprecedented computational power, enabling faster, more accurate diagnostics, personalized treatments, and enhanced drug discovery processes. However, integrating quantum technologies into precision medicine also presents challenges, including errors in algorithms and high costs. We show that mathematically-based techniques for specifying, developing, and verifying software (formal methods) can enhance the reliability and correctness of QC. By providing a rigorous mathematical framework, formal methods help to specify, develop, and verify systems with high precision. In genomic data analysis, formal specification languages can precisely (1) define the behavior and properties of quantum algorithms designed to identify genetic markers associated with diseases. Model checking tools can systematically explore all possible states of the algorithm to (2) ensure it behaves correctly under all conditions, while theorem proving techniques provide mathematical (3) proof that the algorithm meets its specified properties, ensuring accuracy and reliability. Additionally, formal optimization techniques can (4) enhance the efficiency and performance of quantum algorithms by reducing resource usage, such as the number of qubits and gate operations. Therefore, we posit that formal methods can significantly contribute to enabling QC to realize its full potential as a game changer in precision medicine.', 'abstract_zh': '医疗领域的数字化转型面临着众多挑战，包括生物系统的复杂性、大量数据的生成以及个性化治疗方案的需求。传统计算方法常常无法满足需求，导致诊断和治疗延误且有时无效。量子计算（QC）和量子机器学习（QML）提供了转变性的进展，有望彻底变革医学。本文总结了QC在提供前所未有的计算能力方面的领域，使诊断更快、更准确，治疗更个性化，并提升药物发现过程。然而，将量子技术整合到精准医疗中也带来了挑战，包括算法错误和高昂的成本。我们表明，基于数学的方法（形式化方法）可以增强QC的可靠性和正确性。通过提供严谨的数学框架，形式化方法有助于精确地指定、开发和验证系统。在基因组数据分析中，形式化规范语言可以精确地定义用于识别与疾病相关的遗传标记的量子算法的行为和属性。模型检查工具可以系统地探索算法的所有可能状态，以确保其在所有条件下正确执行，而定理证明技术则提供数学证明，确保算法满足其指定的属性，保证准确性和可靠性。此外，形式化优化技术可以通过减少资源使用，如量子位和门操作的数量，来增强量子算法的效率和性能。因此，我们提出，形式化方法可以显著贡献于使QC能够在精准医疗领域充分发挥其潜力。', 'title_zh': '量子机器学习在精准医学和药物发现中的应用——个性化治疗的颠覆者？'}
{'arxiv_id': 'arXiv:2502.18586', 'title': 'Autonomous Vision-Guided Resection of Central Airway Obstruction', 'authors': 'M. E. Smith, N. Yilmaz, T. Watts, P. M. Scheikl, J. Ge, A. Deguet, A. Kuntz, A. Krieger', 'link': 'https://arxiv.org/abs/2502.18586', 'abstract': 'Existing tracheal tumor resection methods often lack the precision required for effective airway clearance, and robotic advancements offer new potential for autonomous resection. We present a vision-guided, autonomous approach for palliative resection of tracheal tumors. This system models the tracheal surface with a fifth-degree polynomial to plan tool trajectories, while a custom Faster R-CNN segmentation pipeline identifies the trachea and tumor boundaries. The electrocautery tool angle is optimized using handheld surgical demonstrations, and trajectories are planned to maintain a 1 mm safety clearance from the tracheal surface. We validated the workflow successfully in five consecutive experiments on ex-vivo animal tissue models, successfully clearing the airway obstruction without trachea perforation in all cases (with more than 90% volumetric tumor removal). These results support the feasibility of an autonomous resection platform, paving the way for future developments in minimally-invasive autonomous resection.', 'abstract_zh': '基于视觉引导的自主气管肿瘤切除方法', 'title_zh': '自主视觉导向中央气道阻塞切除术'}
{'arxiv_id': 'arXiv:2502.18548', 'title': 'What is the Alignment Objective of GRPO?', 'authors': 'Milan Vojnovic, Se-Young Yun', 'link': 'https://arxiv.org/abs/2502.18548', 'abstract': 'In this note, we examine the aggregation of preferences achieved by the Group Policy Optimisation (GRPO) algorithm, a reinforcement learning method used to train advanced artificial intelligence models such as DeepSeek-R1-Zero and DeepSeekMath. The GRPO algorithm trains a policy using a reward preference model, which is computed by sampling a set of outputs for a given context, observing the corresponding rewards, and applying shift-and-scale normalisation to these reward values. Additionally, it incorporates a penalty function to discourage deviations from a reference policy.\nWe present a framework that enables us to characterise the stationary policies of the GRPO algorithm. This analysis reveals that the aggregation of preferences differs fundamentally from standard logarithmic pooling, which is implemented by other approaches such as RLHF. The precise form of preference aggregation arises from the way the reward preference model is defined and from the penalty function, which we show to essentially correspond to the reverse Kullback-Leibler (KL) divergence between the aggregation policy and the reference policy.\nInterestingly, we demonstrate that for groups of size two, the reward preference model corresponds to pairwise comparison preferences, similar to those in other alignment methods based on pairwise comparison feedback. We provide explicit characterisations of the aggregate preference for binary questions, for groups of size two, and in the limit of large group size. This provides insights into the dependence of the aggregate preference on parameters such as the regularisation constant and the confidence margin of question answers.\nFinally, we discuss the aggregation of preferences obtained by modifying the GRPO algorithm to use direct KL divergence as the penalty or to use rewards without scale normalisation.', 'abstract_zh': '关于Group Policy Optimisation算法偏好聚合的分析：增强学习方法在高级人工智能模型训练中的应用', 'title_zh': 'GRPO的对齐目标是什么？'}
