# ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects 

**Title (ZH)**: ArtGS：交互式的视觉物理建模与关节对象操纵的3D高斯点绘制方法 

**Authors**: Qiaojun Yu, Xibin Yuan, Yu jiang, Junting Chen, Dongzhe Zheng, Ce Hao, Yang You, Yixing Chen, Yao Mu, Liu Liu, Cewu Lu  

**Link**: [PDF](https://arxiv.org/pdf/2507.02600)  

**Abstract**: Articulated object manipulation remains a critical challenge in robotics due to the complex kinematic constraints and the limited physical reasoning of existing methods. In this work, we introduce ArtGS, a novel framework that extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling for articulated object understanding and interaction. ArtGS begins with multi-view RGB-D reconstruction, followed by reasoning with a vision-language model (VLM) to extract semantic and structural information, particularly the articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS optimizes the parameters of the articulated bones, ensuring physically consistent motion constraints and enhancing the manipulation policy. By leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and closed-loop optimization, ArtGS establishes a new framework for efficient, scalable, and generalizable articulated object modeling and manipulation. Experiments conducted in both simulation and real-world environments demonstrate that ArtGS significantly outperforms previous methods in joint estimation accuracy and manipulation success rates across a variety of articulated objects. Additional images and videos are available on the project website: this https URL 

**Abstract (ZH)**: articulated对象操纵由于复杂的运动约束和现有方法有限的物理推理仍是一项关键挑战。在此工作中，我们引入了ArtGS，这是一种将基于3D Gaussian Splatting (3DGS)扩展并结合视觉-物理建模的新框架，用于articulated对象的理解和互动。ArtGS首先进行多视角RGB-D重建，然后利用视觉语言模型（VLM）进行推理以提取语义和结构信息，特别是articulated骨骼。通过基于动态不同iable的3DGS渲染，ArtGS优化articulated骨骼的参数，确保物理一致的运动约束并提高操纵策略。通过利用动态Gaussian分裂、跨体适应性和闭环优化，ArtGS建立了新的articulated对象建模和操纵的框架。在仿真和真实环境中的实验表明，ArtGS在关节估计准确性和操纵成功率方面显著优于 previous方法，适用于多种articulated对象。有关更多图片和视频，请访问项目网站：this https URL。 

---
# Safe and Socially Aware Multi-Robot Coordination in Multi-Human Social Care Settings 

**Title (ZH)**: 多机器人在多人类社交关怀环境中的安全和社会意识协调 

**Authors**: Ayodeji O. Abioye, Jayati Deshmukh, Athina Georgara, Dominic Price, Tuyen Nguyen, Aleksandra Landowska, Amel Bennaceur, Joel E. Fischer, Sarvapali D. Ramchurn  

**Link**: [PDF](https://arxiv.org/pdf/2507.02521)  

**Abstract**: This research investigates strategies for multi-robot coordination in multi-human environments. It proposes a multi-objective learning-based coordination approach to addressing the problem of path planning, navigation, task scheduling, task allocation, and human-robot interaction in multi-human multi-robot (MHMR) settings. 

**Abstract (ZH)**: 本研究探讨了多机器人在多人类环境中的协调策略，并提出了一种多目标学习为基础的协调方法，以解决路径规划、导航、任务调度、任务分配以及人机交互等问题。 

---
# HAC-LOCO: Learning Hierarchical Active Compliance Control for Quadruped Locomotion under Continuous External Disturbances 

**Title (ZH)**: HAC-LOCO: 学习分层主动顺应控制以应对四足运动中的连续外部干扰 

**Authors**: Xiang Zhou, Xinyu Zhang, Qingrui Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2507.02447)  

**Abstract**: Despite recent remarkable achievements in quadruped control, it remains challenging to ensure robust and compliant locomotion in the presence of unforeseen external disturbances. Existing methods prioritize locomotion robustness over compliance, often leading to stiff, high-frequency motions, and energy inefficiency. This paper, therefore, presents a two-stage hierarchical learning framework that can learn to take active reactions to external force disturbances based on force estimation. In the first stage, a velocity-tracking policy is trained alongside an auto-encoder to distill historical proprioceptive features. A neural network-based estimator is learned through supervised learning, which estimates body velocity and external forces based on proprioceptive measurements. In the second stage, a compliance action module, inspired by impedance control, is learned based on the pre-trained encoder and policy. This module is employed to actively adjust velocity commands in response to external forces based on real-time force estimates. With the compliance action module, a quadruped robot can robustly handle minor disturbances while appropriately yielding to significant forces, thus striking a balance between robustness and compliance. Simulations and real-world experiments have demonstrated that our method has superior performance in terms of robustness, energy efficiency, and safety. Experiment comparison shows that our method outperforms the state-of-the-art RL-based locomotion controllers. Ablation studies are given to show the critical roles of the compliance action module. 

**Abstract (ZH)**: 尽管四足机器人控制领域取得了近期的重大成果，但在面对不可预见的外部干扰时，确保稳健且柔顺的运动仍然具有挑战性。现有方法通常优先考虑运动的稳健性而牺牲柔顺性，这往往导致刚性、高频率的运动以及能量效率低下。因此，本文提出了一种两阶段层次化学习框架，可以在基于力估计的情况下学习主动应对外部力干扰。第一阶段，训练一个速度跟踪策略和一个自动编码器，以提炼历史本体感受信息。通过监督学习，学习一个基于本体感受测量的神经网络估计器，该估计器可根据本体感受测量估计身体速度和外部力。第二阶段，基于预训练的编码器和策略，学习一个受阻抗控制启发的柔顺动作模块。该模块借助实时力估计，被用于根据外部力情况主动调整速度命令。借助柔顺动作模块，四足机器人可以在处理微小干扰的同时适当适应显著的力量，从而在稳健性和柔顺性之间达到平衡。仿真和实际实验表明，本文方法在稳健性、能量效率和安全性方面表现出优越性能，实验对比结果证明本文方法优于最先进的基于强化学习的运动控制器。消融研究展示了柔顺动作模块的关键作用。 

---
# A Vehicle-in-the-Loop Simulator with AI-Powered Digital Twins for Testing Automated Driving Controllers 

**Title (ZH)**: 基于AI驱动的数字孪生的车辆在环仿真测试自动化驾驶控制器系统 

**Authors**: Zengjie Zhang, Giannis Badakis, Michalis Galanis, Adem Bavarşi, Edwin van Hassel, Mohsen Alirezaei, Sofie Haesaert  

**Link**: [PDF](https://arxiv.org/pdf/2507.02313)  

**Abstract**: Simulators are useful tools for testing automated driving controllers. Vehicle-in-the-loop (ViL) tests and digital twins (DTs) are widely used simulation technologies to facilitate the smooth deployment of controllers to physical vehicles. However, conventional ViL tests rely on full-size vehicles, requiring large space and high expenses. Also, physical-model-based DT suffers from the reality gap caused by modeling imprecision. This paper develops a comprehensive and practical simulator for testing automated driving controllers enhanced by scaled physical cars and AI-powered DT models. The scaled cars allow for saving space and expenses of simulation tests. The AI-powered DT models ensure superior simulation fidelity. Moreover, the simulator integrates well with off-the-shelf software and control algorithms, making it easy to extend. We use a filtered control benchmark with formal safety guarantees to showcase the capability of the simulator in validating automated driving controllers. Experimental studies are performed to showcase the efficacy of the simulator, implying its great potential in validating control solutions for autonomous vehicles and intelligent traffic. 

**Abstract (ZH)**: 基于缩比物理车和AI驱动数字孪生的自动驾驶控制器综合实践仿真器 

---
# CoInfra: A Large-Scale Cooperative Infrastructure Perception System and Dataset in Adverse Weather 

**Title (ZH)**: CoInfra: 一种在恶劣天气条件下的大规模协同基础设施感知系统及数据集 

**Authors**: Minghao Ning, Yufeng Yang, Keqi Shu, Shucheng Huang, Jiaming Zhong, Maryam Salehi, Mahdi Rahmani, Yukun Lu, Chen Sun, Aladdin Saleh, Ehsan Hashemi, Amir Khajepour  

**Link**: [PDF](https://arxiv.org/pdf/2507.02245)  

**Abstract**: We present CoInfra, a large-scale cooperative infrastructure perception system and dataset designed to advance robust multi-agent perception under real-world and adverse weather conditions. The CoInfra system includes 14 fully synchronized sensor nodes, each equipped with dual RGB cameras and a LiDAR, deployed across a shared region and operating continuously to capture all traffic participants in real-time. A robust, delay-aware synchronization protocol and a scalable system architecture that supports real-time data fusion, OTA management, and remote monitoring are provided in this paper. On the other hand, the dataset was collected in different weather scenarios, including sunny, rainy, freezing rain, and heavy snow and includes 195k LiDAR frames and 390k camera images from 8 infrastructure nodes that are globally time-aligned and spatially calibrated. Furthermore, comprehensive 3D bounding box annotations for five object classes (i.e., car, bus, truck, person, and bicycle) are provided in both global and individual node frames, along with high-definition maps for contextual understanding. Baseline experiments demonstrate the trade-offs between early and late fusion strategies, the significant benefits of HD map integration are discussed. By openly releasing our dataset, codebase, and system documentation at this https URL, we aim to enable reproducible research and drive progress in infrastructure-supported autonomous driving, particularly in challenging, real-world settings. 

**Abstract (ZH)**: CoInfra：一种用于增强现实世界和恶劣天气条件下多agents感知的大型协作基础设施感知系统及数据集 

---
# RoboBrain 2.0 Technical Report 

**Title (ZH)**: RoboBrain 2.0 技术报告 

**Authors**: BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, Yi Han, Yingbo Tang, Xiangqi Xu, Wei Guo, Yaoxu Lyu, Yijie Xu, Jiayu Shi, Cheng Chi, Mengdi Zhao, Xiaoshuai Hao, Shanyu Rong, Zhengliang Cai, Bolun Zhang, Shuyi Zhang, Huaihai Lyu, Mengfei Du, Lingfeng Zhang, Xi Feng, Xiaodan Liu, Yance Jiao, Chenrui He, Mengsi Lyu, Zhuo Chen, Yulong Ao, Xue Sun, Zheqi He, Jingshu Zheng, Xi Yang, Donghai Shi, Kunchang Xie, Bochao Zhang, Shaokai Nie, Chunlei Men, Yonghua Lin, Zhongyuan Wang, Tiejun Huang, Shanghang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2507.02029)  

**Abstract**: We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. It comes in two variants: a lightweight 7B model and a full-scale 32B model, featuring a heterogeneous architecture with a vision encoder and a language model. Despite its compact size, RoboBrain 2.0 achieves strong performance across a wide spectrum of embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B variant achieves leading results, surpassing prior open-source and proprietary models. In particular, it supports key real-world embodied AI capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent long-horizon planning, and scene graph updating). This report details the model architecture, data construction, multi-stage training strategies, infrastructure and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as a practical step toward building generalist embodied agents. The code, checkpoint and benchmark are available at this https URL. 

**Abstract (ZH)**: RoboBrain 2.0: 我们最新一代的实体化视觉-语言基础模型，设计用于统一物理环境中复杂实体化任务的感知、推理和规划。它有两种变体：一个轻量级的7B模型和一个全规模的32B模型，具有异构架构，包含视觉编码器和语言模型。尽管体积小巧，RoboBrain 2.0 在各种实体化推理任务中的表现依然出色。在空间和时间基准测试中，32B变体取得了领先结果，超过了先前的开源和专有模型。特别是，它支持关键的实体化人工智能能力，包括空间理解（如：功能预测、空间引用、轨迹预测）和时间决策（如：闭环交互、多智能体长时规划和场景图更新）。本报告详细介绍了模型架构、数据构建、多阶段训练策略、基础设施和实际应用。我们希望RoboBrain 2.0 推动实体化人工智能研究，并作为构建通用实体化代理的实用步骤。代码、检查点和基准数据可在以下链接获取。 

---
# Grounding Intelligence in Movement 

**Title (ZH)**: 将智能扎根于运动之中 

**Authors**: Melanie Segado, Felipe Parodi, Jordan K. Matelsky, Michael L. Platt, Eva B. Dyer, Konrad P. Kording  

**Link**: [PDF](https://arxiv.org/pdf/2507.02771)  

**Abstract**: Recent advances in machine learning have dramatically improved our ability to model language, vision, and other high-dimensional data, yet they continue to struggle with one of the most fundamental aspects of biological systems: movement. Across neuroscience, medicine, robotics, and ethology, movement is essential for interpreting behavior, predicting intent, and enabling interaction. Despite its core significance in our intelligence, movement is often treated as an afterthought rather than as a rich and structured modality in its own right. This reflects a deeper fragmentation in how movement data is collected and modeled, often constrained by task-specific goals and domain-specific assumptions. But movement is not domain-bound. It reflects shared physical constraints, conserved morphological structures, and purposeful dynamics that cut across species and settings. We argue that movement should be treated as a primary modeling target for AI. It is inherently structured and grounded in embodiment and physics. This structure, often allowing for compact, lower-dimensional representations (e.g., pose), makes it more interpretable and computationally tractable to model than raw, high-dimensional sensory inputs. Developing models that can learn from and generalize across diverse movement data will not only advance core capabilities in generative modeling and control, but also create a shared foundation for understanding behavior across biological and artificial systems. Movement is not just an outcome, it is a window into how intelligent systems engage with the world. 

**Abstract (ZH)**: 最近机器学习的进展大大提高了我们建模语言、视觉和其他高维数据的能力，但仍难以应对生物学系统最基本的一个方面：运动。在神经科学、医学、机器人学和行为学中，运动对于解释行为、预测意图和促进互动至关重要。尽管运动对我们智能的核心意义重大，但它往往被视为次要问题，而不是一个丰富且结构化的模态。这反映了运动数据收集和建模的更深层次的碎片化，经常受到特定任务目标和特定领域假设的限制。但运动并非局限于特定领域。它反映了共享的物理约束、保守的形态结构以及跨越物种和环境的目的动态。我们主张应将运动视为人工智能的主要建模目标。运动本质上是结构化的，根植于身体和物理之中。这种结构往往使得通过紧凑的、低维度的表示（如姿态）来进行建模更加具可解释性和计算可行性，而不是直接处理原始的高维感官输入。开发可以从多样化的运动数据中学习并泛化的模型不仅将推动生成建模和控制的核心能力的进步，还将为理解生物和人工系统中的行为提供一个共享基础。运动不仅是一个结果，它是智能系统如何与世界互动的一个窗口。 

---
# DexVLG: Dexterous Vision-Language-Grasp Model at Scale 

**Title (ZH)**: DexVLG：大规模 Dexterous Vision-Language-Grasp 模型 

**Authors**: Jiawei He, Danshi Li, Xinqiang Yu, Zekun Qi, Wenyao Zhang, Jiayi Chen, Zhaoxiang Zhang, Zhizheng Zhang, Li Yi, He Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.02747)  

**Abstract**: As large models gain traction, vision-language-action (VLA) systems are enabling robots to tackle increasingly complex tasks. However, limited by the difficulty of data collection, progress has mainly focused on controlling simple gripper end-effectors. There is little research on functional grasping with large models for human-like dexterous hands. In this paper, we introduce DexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction aligned with language instructions using single-view RGBD input. To accomplish this, we generate a dataset of 170 million dexterous grasp poses mapped to semantic parts across 174,000 objects in simulation, paired with detailed part-level captions. This large-scale dataset, named DexGraspNet 3.0, is used to train a VLM and flow-matching-based pose head capable of producing instruction-aligned grasp poses for tabletop objects. To assess DexVLG's performance, we create benchmarks in physics-based simulations and conduct real-world experiments. Extensive testing demonstrates DexVLG's strong zero-shot generalization capabilities-achieving over 76% zero-shot execution success rate and state-of-the-art part-grasp accuracy in simulation-and successful part-aligned grasps on physical objects in real-world scenarios. 

**Abstract (ZH)**: 基于视觉-语言-动作的大规模 Dexterous 抓取预测模型 DexVLG 

---
# StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason 

**Title (ZH)**: StepHint: 多层次逐步提示增强强化学习推理 

**Authors**: Kaiyi Zhang, Ang Lv, Jinpeng Li, Yongbo Wang, Feng Wang, Haoyuan Hu, Rui Yan  

**Link**: [PDF](https://arxiv.org/pdf/2507.02841)  

**Abstract**: Reinforcement learning with verifiable rewards (RLVR) is a promising approach for improving the complex reasoning abilities of large language models (LLMs). However, current RLVR methods face two significant challenges: the near-miss reward problem, where a small mistake can invalidate an otherwise correct reasoning process, greatly hindering training efficiency; and exploration stagnation, where models tend to focus on solutions within their ``comfort zone,'' lacking the motivation to explore potentially more effective alternatives. To address these challenges, we propose StepHint, a novel RLVR algorithm that utilizes multi-level stepwise hints to help models explore the solution space more effectively. StepHint generates valid reasoning chains from stronger models and partitions these chains into reasoning steps using our proposed adaptive partitioning method. The initial few steps are used as hints, and simultaneously, multiple-level hints (each comprising a different number of steps) are provided to the model. This approach directs the model's exploration toward a promising solution subspace while preserving its flexibility for independent exploration. By providing hints, StepHint mitigates the near-miss reward problem, thereby improving training efficiency. Additionally, the external reasoning pathways help the model develop better reasoning abilities, enabling it to move beyond its ``comfort zone'' and mitigate exploration stagnation. StepHint outperforms competitive RLVR enhancement methods across six mathematical benchmarks, while also demonstrating superior generalization and excelling over baselines on out-of-domain benchmarks. 

**Abstract (ZH)**: 可验证回报的强化学习（RLVR）方法：增强大型语言模型的复杂推理能力 

---
# An AI-native experimental laboratory for autonomous biomolecular engineering 

**Title (ZH)**: 基于AI的自主生物分子工程实验实验室 

**Authors**: Mingyu Wu, Zhaoguo Wang, Jiabin Wang, Zhiyuan Dong, Jingkai Yang, Qingting Li, Tianyu Huang, Lei Zhao, Mingqiang Li, Fei Wang, Chunhai Fan, Haibo Chen  

**Link**: [PDF](https://arxiv.org/pdf/2507.02379)  

**Abstract**: Autonomous scientific research, capable of independently conducting complex experiments and serving non-specialists, represents a long-held aspiration. Achieving it requires a fundamental paradigm shift driven by artificial intelligence (AI). While autonomous experimental systems are emerging, they remain confined to areas featuring singular objectives and well-defined, simple experimental workflows, such as chemical synthesis and catalysis. We present an AI-native autonomous laboratory, targeting highly complex scientific experiments for applications like autonomous biomolecular engineering. This system autonomously manages instrumentation, formulates experiment-specific procedures and optimization heuristics, and concurrently serves multiple user requests. Founded on a co-design philosophy of models, experiments, and instruments, the platform supports the co-evolution of AI models and the automation system. This establishes an end-to-end, multi-user autonomous laboratory that handles complex, multi-objective experiments across diverse instrumentation. Our autonomous laboratory supports fundamental nucleic acid functions-including synthesis, transcription, amplification, and sequencing. It also enables applications in fields such as disease diagnostics, drug development, and information storage. Without human intervention, it autonomously optimizes experimental performance to match state-of-the-art results achieved by human scientists. In multi-user scenarios, the platform significantly improves instrument utilization and experimental efficiency. This platform paves the way for advanced biomaterials research to overcome dependencies on experts and resource barriers, establishing a blueprint for science-as-a-service at scale. 

**Abstract (ZH)**: 自主科学研究所，能够独立执行复杂实验并服务于非专业人士，代表着长久以来的一种愿景。实现这一点需要由人工智能（AI）驱动的基本范式转变。虽然自主实验系统正在涌现，但它们仍然局限于具有单一目标和明确简单实验工作流程的领域，如化学合成和催化。我们介绍了一种AI原生自主实验室，旨在应对应用如自主生物分子工程等高度复杂科学实验。该系统自主管理仪器设备，制定特定于实验的程序和优化策略，并同时满足多个用户的需求。该平台基于模型、实验和仪器的协同设计理念，支持AI模型与自动化系统的共同进化。这建立了一个端到端、多用户的自主实验室，能够处理多种仪器设备下的复杂多目标实验。我们的自主实验室支持基础核酸功能，包括合成、转录、扩增和测序。它还能够在疾病诊断、药物开发和信息存储等领域发挥应用。无需人工干预，它可以自主优化实验性能，达到与人类科学家相媲美的成果水平。在多用户场景下，该平台显著提高仪器使用效率和实验效率。该平台为先进生物材料研究铺平了道路，有助于克服对专家的依赖和资源障碍，为大规模的科学即服务建立了蓝图。 

---
# STELLA: Self-Evolving LLM Agent for Biomedical Research 

**Title (ZH)**: STELLA: 自我进化的生物医学研究大语言模型代理 

**Authors**: Ruofan Jin, Zaixi Zhang, Mengdi Wang, Le Cong  

**Link**: [PDF](https://arxiv.org/pdf/2507.02004)  

**Abstract**: The rapid growth of biomedical data, tools, and literature has created a fragmented research landscape that outpaces human expertise. While AI agents offer a solution, they typically rely on static, manually curated toolsets, limiting their ability to adapt and scale. Here, we introduce STELLA, a self-evolving AI agent designed to overcome these limitations. STELLA employs a multi-agent architecture that autonomously improves its own capabilities through two core mechanisms: an evolving Template Library for reasoning strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent automatically discovers and integrates new bioinformatics tools. This allows STELLA to learn from experience. We demonstrate that STELLA achieves state-of-the-art accuracy on a suite of biomedical benchmarks, scoring approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench: DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6 percentage points. More importantly, we show that its performance systematically improves with experience; for instance, its accuracy on the Humanity's Last Exam benchmark almost doubles with increased trials. STELLA represents a significant advance towards AI Agent systems that can learn and grow, dynamically scaling their expertise to accelerate the pace of biomedical discovery. 

**Abstract (ZH)**: 自适应的成长：STELLA——一种自我进化的生物医学AI代理 

---
# Toward a Robust and Generalizable Metamaterial Foundation Model 

**Title (ZH)**: 向着稳健且通用的元材料基础模型 

**Authors**: Namjung Kim, Dongseok Lee, Jongbin Yu, Sung Woong Cho, Dosung Lee, Yesol Park, Youngjoon Hong  

**Link**: [PDF](https://arxiv.org/pdf/2507.02436)  

**Abstract**: Advances in material functionalities drive innovations across various fields, where metamaterials-defined by structure rather than composition-are leading the way. Despite the rise of artificial intelligence (AI)-driven design strategies, their impact is limited by task-specific retraining, poor out-of-distribution(OOD) generalization, and the need for separate models for forward and inverse design. To address these limitations, we introduce the Metamaterial Foundation Model (MetaFO), a Bayesian transformer-based foundation model inspired by large language models. MetaFO learns the underlying mechanics of metamaterials, enabling probabilistic, zero-shot predictions across diverse, unseen combinations of material properties and structural responses. It also excels in nonlinear inverse design, even under OOD conditions. By treating metamaterials as an operator that maps material properties to structural responses, MetaFO uncovers intricate structure-property relationships and significantly expands the design space. This scalable and generalizable framework marks a paradigm shift in AI-driven metamaterial discovery, paving the way for next-generation innovations. 

**Abstract (ZH)**: 材料功能性的进步推动了各个领域的创新，其中通过结构而非组成定义的 metamaterials 处于领先地位。尽管人工智能（AI）驱动的设计策略正在崛起，但它们受到特定任务重训的限制，具有较差的离分布外 (OOD) 通用性，并且需要分别用于正向和逆向设计的单独模型。为了克服这些限制，我们 introduces Metamaterial 基础模型 (MetaFO)，这是一种受大型语言模型启发的贝叶斯变压器基础模型。MetaFO 学习 metamaterials 的内在力学，从而能够跨多种未见过的材料属性和结构响应组合进行概率性的零样本预测。它在非线性逆向设计中也表现出色，即使在 OOD 条件下也是如此。通过将 metamaterials 视为一个操作符，将材料属性映射到结构响应，MetaFO 揭示了复杂的结构-性能关系，并显着扩展了设计空间。这一可扩展且通用的框架标志着人工智能驱动 metamaterial 发现范式的转变，铺平了下一代创新的道路。 

---
# Offline Reinforcement Learning with Penalized Action Noise Injection 

**Title (ZH)**: 带惩罚性动作噪声注入的离线强化学习 

**Authors**: JunHyeok Oh, Byung-Jun Lee  

**Link**: [PDF](https://arxiv.org/pdf/2507.02356)  

**Abstract**: Offline reinforcement learning (RL) optimizes a policy using only a fixed dataset, making it a practical approach in scenarios where interaction with the environment is costly. Due to this limitation, generalization ability is key to improving the performance of offline RL algorithms, as demonstrated by recent successes of offline RL with diffusion models. However, it remains questionable whether such diffusion models are necessary for highly performing offline RL algorithms, given their significant computational requirements during inference. In this paper, we propose Penalized Action Noise Injection (PANI), a method that simply enhances offline learning by utilizing noise-injected actions to cover the entire action space, while penalizing according to the amount of noise injected. This approach is inspired by how diffusion models have worked in offline RL algorithms. We provide a theoretical foundation for this method, showing that offline RL algorithms with such noise-injected actions solve a modified Markov Decision Process (MDP), which we call the noisy action MDP. PANI is compatible with a wide range of existing off-policy and offline RL algorithms, and despite its simplicity, it demonstrates significant performance improvements across various benchmarks. 

**Abstract (ZH)**: 带惩罚的动作噪声注入（PANI）：一种增强离线学习的方法 

---
