{'arxiv_id': 'arXiv:2503.19868', 'title': 'GENIUS: A Generative Framework for Universal Multimodal Search', 'authors': 'Sungyeon Kim, Xinliang Zhu, Xiaofan Lin, Muhammet Bastan, Douglas Gray, Suha Kwak', 'link': 'https://arxiv.org/abs/2503.19868', 'abstract': 'Generative retrieval is an emerging approach in information retrieval that generates identifiers (IDs) of target data based on a query, providing an efficient alternative to traditional embedding-based retrieval methods. However, existing models are task-specific and fall short of embedding-based retrieval in performance. This paper proposes GENIUS, a universal generative retrieval framework supporting diverse tasks across multiple modalities and domains. At its core, GENIUS introduces modality-decoupled semantic quantization, transforming multimodal data into discrete IDs encoding both modality and semantics. Moreover, to enhance generalization, we propose a query augmentation that interpolates between a query and its target, allowing GENIUS to adapt to varied query forms. Evaluated on the M-BEIR benchmark, it surpasses prior generative methods by a clear margin. Unlike embedding-based retrieval, GENIUS consistently maintains high retrieval speed across database size, with competitive performance across multiple benchmarks. With additional re-ranking, GENIUS often achieves results close to those of embedding-based methods while preserving efficiency.', 'abstract_zh': '通用生成检索框架GENIUS：跨模态与跨领域的通用生成检索方法', 'title_zh': 'GENIUS: 生成框架下的通用多模态搜索'}
{'arxiv_id': 'arXiv:2503.19801', 'title': 'SeLIP: Similarity Enhanced Contrastive Language Image Pretraining for Multi-modal Head MRI', 'authors': 'Zhiyang Liu, Dong Yang, Minghao Zhang, Hanyu Sun, Hong Wu, Huiying Wang, Wen Shen, Chao Chai, Shuang Xia', 'link': 'https://arxiv.org/abs/2503.19801', 'abstract': 'Despite that deep learning (DL) methods have presented tremendous potential in many medical image analysis tasks, the practical applications of medical DL models are limited due to the lack of enough data samples with manual annotations. By noting that the clinical radiology examinations are associated with radiology reports that describe the images, we propose to develop a foundation model for multi-model head MRI by using contrastive learning on the images and the corresponding radiology findings. In particular, a contrastive learning framework is proposed, where a mixed syntax and semantic similarity matching metric is integrated to reduce the thirst of extreme large dataset in conventional contrastive learning framework. Our proposed similarity enhanced contrastive language image pretraining (SeLIP) is able to effectively extract more useful features. Experiments revealed that our proposed SeLIP performs well in many downstream tasks including image-text retrieval task, classification task, and image segmentation, which highlights the importance of considering the similarities among texts describing different images in developing medical image foundation models.', 'abstract_zh': '尽管深度学习方法在许多医疗图像分析任务中展示了巨大的潜力，但由于缺乏足够的带有手动标注的数据样本，医疗深度学习模型的实际应用受到限制。鉴于临床放射学检查与放射学报告相关，报告描述了图像内容，我们提出了一种通过对比学习框架开发多模态头部MRI基础模型的方法，该框架结合图像和相应的放射学发现进行训练。特别是，我们提出了一种对比学习框架，其中集成了一种混合语法和语义相似性匹配度量，以降低传统对比学习框架对海量数据的依赖。我们提出的增强相似性对比语言图像预训练（SeLIP）能够有效提取更多有用特征。实验表明，我们的SeLIP在包括图像-文本检索任务、分类任务和图像分割在内的许多下游任务中表现良好，突显了在开发医疗图像基础模型时考虑描述不同图像的文本之间的相似性的重要性。', 'title_zh': 'SeLIP: 增强相似性对比语言图像预训练多模态头部MRI'}
{'arxiv_id': 'arXiv:2503.19706', 'title': 'Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations', 'authors': 'Jungin Park, Jiyoung Lee, Kwanghoon Sohn', 'link': 'https://arxiv.org/abs/2503.19706', 'abstract': 'View-invariant representation learning from egocentric (first-person, ego) and exocentric (third-person, exo) videos is a promising approach toward generalizing video understanding systems across multiple viewpoints. However, this area has been underexplored due to the substantial differences in perspective, motion patterns, and context between ego and exo views. In this paper, we propose a novel masked ego-exo modeling that promotes both causal temporal dynamics and cross-view alignment, called Bootstrap Your Own Views (BYOV), for fine-grained view-invariant video representation learning from unpaired ego-exo videos. We highlight the importance of capturing the compositional nature of human actions as a basis for robust cross-view understanding. Specifically, self-view masking and cross-view masking predictions are designed to learn view-invariant and powerful representations concurrently. Experimental results demonstrate that our BYOV significantly surpasses existing approaches with notable gains across all metrics in four downstream ego-exo video tasks. The code is available at this https URL.', 'abstract_zh': '从第一人称（主观）和第三人称（客观）视频中学习观点不变的表示：一种Bootstrap Your Own Views（BYOV）方法', 'title_zh': '自我引导视点学习：遮蔽自我-环境建模的精细粒度视点不变视频表示'}
{'arxiv_id': 'arXiv:2503.19654', 'title': 'RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models', 'authors': 'Mehdi Moshtaghi, Siavash H. Khajavi, Joni Pajarinen', 'link': 'https://arxiv.org/abs/2503.19654', 'abstract': 'We introduce RGB-Th-Bench, the first benchmark designed to evaluate the ability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs. While VLMs have demonstrated remarkable progress in visual reasoning and multimodal understanding, their evaluation has been predominantly limited to RGB-based benchmarks, leaving a critical gap in assessing their capabilities in infrared vision tasks. Existing visible-infrared datasets are either task-specific or lack high-quality annotations necessary for rigorous model evaluation. To address these limitations, RGB-Th-Bench provides a comprehensive evaluation framework covering 14 distinct skill dimensions, with a total of 1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy metrics: a standard question-level accuracy and a stricter skill-level accuracy, which evaluates model robustness across multiple questions within each skill dimension. This design ensures a thorough assessment of model performance, including resilience to adversarial and hallucinated responses. We conduct extensive evaluations on 19 state-of-the-art VLMs, revealing significant performance gaps in RGB-Thermal understanding. Our results show that even the strongest models struggle with thermal image comprehension, with performance heavily constrained by their RGB-based capabilities. Additionally, the lack of large-scale application-specific and expert-annotated thermal-caption-pair datasets in pre-training is an important reason of the observed performance gap. RGB-Th-Bench highlights the urgent need for further advancements in multimodal learning to bridge the gap between visible and thermal image understanding. The dataset is available through this link, and the evaluation code will also be made publicly available.', 'abstract_zh': 'RGB-Th-Bench: 用于评估视觉语言模型理解RGB-热成像图像对能力的基准', 'title_zh': 'RGB-Th-Bench: 一种用于视觉语言模型的视觉-热成像理解密集基准'}
{'arxiv_id': 'arXiv:2503.19474', 'title': 'A-MESS: Anchor based Multimodal Embedding with Semantic Synchronization for Multimodal Intent Recognition', 'authors': 'Yaomin Shen, Xiaojian Lin, Wei Fan', 'link': 'https://arxiv.org/abs/2503.19474', 'abstract': 'In the domain of multimodal intent recognition (MIR), the objective is to recognize human intent by integrating a variety of modalities, such as language text, body gestures, and tones. However, existing approaches face difficulties adequately capturing the intrinsic connections between the modalities and overlooking the corresponding semantic representations of intent. To address these limitations, we present the Anchor-based Mul- timodal Embedding with Semantic Synchronization (A-MESS) framework. We first design an Anchor-based Multimodal Embed- ding (A-ME) module that employs an anchor-based embedding fusion mechanism to integrate multimodal inputs. Furthermore, we develop a Semantic Synchronization (SS) strategy with the Triplet Contrastive Learning pipeline, which optimizes the pro- cess by synchronizing multimodal representation with label de- scriptions produced by the large language model. Comprehensive experiments indicate that our A-MESS achieves state-of-the-art and provides substantial insight into multimodal representation and downstream tasks.', 'abstract_zh': '基于锚点的多模态嵌入与语义同步框架（A-MESS）', 'title_zh': 'A-MESS：基于锚点的多模态嵌入与语义同步的多模态意图识别'}
{'arxiv_id': 'arXiv:2503.19120', 'title': 'Where is this coming from? Making groundedness count in the evaluation of Document VQA models', 'authors': 'Armineh Nourbakhsh, Siddharth Parekh, Pranav Shetty, Zhao Jin, Sameena Shah, Carolyn Rose', 'link': 'https://arxiv.org/abs/2503.19120', 'abstract': "Document Visual Question Answering (VQA) models have evolved at an impressive rate over the past few years, coming close to or matching human performance on some benchmarks. We argue that common evaluation metrics used by popular benchmarks do not account for the semantic and multimodal groundedness of a model's outputs. As a result, hallucinations and major semantic errors are treated the same way as well-grounded outputs, and the evaluation scores do not reflect the reasoning capabilities of the model. In response, we propose a new evaluation methodology that accounts for the groundedness of predictions with regard to the semantic characteristics of the output as well as the multimodal placement of the output within the input document. Our proposed methodology is parameterized in such a way that users can configure the score according to their preferences. We validate our scoring methodology using human judgment and show its potential impact on existing popular leaderboards. Through extensive analyses, we demonstrate that our proposed method produces scores that are a better indicator of a model's robustness and tends to give higher rewards to better-calibrated answers.", 'abstract_zh': '文档视觉问答（VQA）模型在过去几年中取得了令人印象深刻的进展，已在某些基准上接近或匹配了人类的表现。我们argue指出，广泛使用的大多数基准所采用的评估指标未能考虑模型输出的语义和多模态接地性。因此，幻觉和重大语义错误与充分接地的输出被同等对待，评估分数无法反映出模型的推理能力。为应对这一问题，我们提出了一种新的评估方法，该方法考虑了预测与输出语义特征以及输入文档中多模态位置的相关性。我们提出的方法具有参数化特性，用户可以根据自己的偏好进行配置。我们使用人类判断验证了评分方法，并展示了其在现有流行排行榜上的潜在影响。通过广泛的分析，我们证明了我们提出的方法产生的分数更能够反映模型的稳健性，并倾向于对更好地校准的答案给予更高的评分。', 'title_zh': '来自何处？在文档VQA模型评估中重视 grounding 的重要性'}
{'arxiv_id': 'arXiv:2503.18964', 'title': 'Unifying EEG and Speech for Emotion Recognition: A Two-Step Joint Learning Framework for Handling Missing EEG Data During Inference', 'authors': 'Upasana Tiwari, Rupayan Chakraborty, Sunil Kumar Kopparapu', 'link': 'https://arxiv.org/abs/2503.18964', 'abstract': 'Computer interfaces are advancing towards using multi-modalities to enable better human-computer interactions. The use of automatic emotion recognition (AER) can make the interactions natural and meaningful thereby enhancing the user experience. Though speech is the most direct and intuitive modality for AER, it is not reliable because it can be intentionally faked by humans. On the other hand, physiological modalities like EEG, are more reliable and impossible to fake. However, use of EEG is infeasible for realistic scenarios usage because of the need for specialized recording setup. In this paper, one of our primary aims is to ride on the reliability of the EEG modality to facilitate robust AER on the speech modality. Our approach uses both the modalities during training to reliably identify emotion at the time of inference, even in the absence of the more reliable EEG modality. We propose, a two-step joint multi-modal learning approach (JMML) that exploits both the intra- and inter- modal characteristics to construct emotion embeddings that enrich the performance of AER. In the first step, using JEC-SSL, intra-modal learning is done independently on the individual modalities. This is followed by an inter-modal learning using the proposed extended variant of deep canonically correlated cross-modal autoencoder (E-DCC-CAE). The approach learns the joint properties of both the modalities by mapping them into a common representation space, such that the modalities are maximally correlated. These emotion embeddings, hold properties of both the modalities there by enhancing the performance of ML classifier used for AER. Experimental results show the efficacy of the proposed approach. To best of our knowledge, this is the first attempt to combine speech and EEG with joint multi-modal learning approach for reliable AER.', 'abstract_zh': '计算机界面正朝着使用多模态方向发展，以实现更好的人机交互。自动情绪识别（AER）的使用可以使交互更加自然和有意义，从而提升用户体验。虽然语音是AER中最直接和直观的模态，但由于人类可以故意伪造语音，因此不太可靠。另一方面，生理模态如EEG则更加可靠且无法伪造。然而，由于需要专用的记录设备，EEG在现实场景中的使用不可行。在本文中，我们的一项主要目标是利用EEG模态的可靠性，以增强语音模态上的AER的鲁棒性。我们的方法在训练过程中同时使用这两种模态，即使在没有更可靠的EEG模态的情况下，也能可靠地识别情感。我们提出了一种两步联合多模态学习方法（JMML），该方法利用了模内和模间特征，构建了能够增强AER性能的情绪嵌入。在第一步中，使用JEC-SSL独立地在各个模态上进行模内学习，随后使用提出的扩展深层共变模态自编码器（E-DCC-CAE）的扩展变体进行模间学习。该方法通过将两种模态映射到一个共同的表示空间，学习它们的最大相关性，从而使模态之间的相关性最大化。这些情绪嵌入包含了两种模态的特性，从而增强了用于AER的机器学习分类器的性能。实验结果验证了所提方法的有效性。据我们所知，这是第一次尝试将语音和EEG结合在一起，使用联合多模态学习方法进行可靠的AER。', 'title_zh': '融合脑电和语音的情感识别：一种推理时处理缺失脑电数据的两步联合学习框架'}
