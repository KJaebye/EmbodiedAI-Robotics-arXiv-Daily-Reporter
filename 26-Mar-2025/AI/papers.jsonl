{'arxiv_id': 'arXiv:2503.19815', 'title': 'Thinking agents for zero-shot generalization to qualitatively novel tasks', 'authors': 'Thomas Miconi, Kevin McKee, Yicong Zheng, Jed McCaleb', 'link': 'https://arxiv.org/abs/2503.19815', 'abstract': "Intelligent organisms can solve truly novel problems which they have never encountered before, either in their lifetime or their evolution. An important component of this capacity is the ability to ``think'', that is, to mentally manipulate objects, concepts and behaviors in order to plan and evaluate possible solutions to novel problems, even without environment interaction. To generate problems that are truly qualitatively novel, while still solvable zero-shot (by mental simulation), we use the combinatorial nature of environments: we train the agent while withholding a specific combination of the environment's elements. The novel test task, based on this combination, is thus guaranteed to be truly novel, while still mentally simulable since the agent has been exposed to each individual element (and their pairwise interactions) during training. We propose a method to train agents endowed with world models to make use their mental simulation abilities, by selecting tasks based on the difference between the agent's pre-thinking and post-thinking performance. When tested on the novel, withheld problem, the resulting agent successfully simulated alternative scenarios and used the resulting information to guide its behavior in the actual environment, solving the novel task in a single real-environment trial (zero-shot).", 'abstract_zh': '具有世界模型的智能体可以通过对比预思考和后思考性能来利用其mental simulation能力，以生成真正质性新颖的问题，并在未接触的问题上实现零样本解决。', 'title_zh': '零样本泛化至定性新颖任务的思考代理'}
{'arxiv_id': 'arXiv:2503.19813', 'title': 'Guidelines For The Choice Of The Baseline in XAI Attribution Methods', 'authors': 'Cristian Morasso, Giorgio Dolci, Ilaria Boscolo Galazzo, Sergey M. Plis, Gloria Menegaz', 'link': 'https://arxiv.org/abs/2503.19813', 'abstract': 'Given the broad adoption of artificial intelligence, it is essential to provide evidence that AI models are reliable, trustable, and fair. To this end, the emerging field of eXplainable AI develops techniques to probe such requirements, counterbalancing the hype pushing the pervasiveness of this technology. Among the many facets of this issue, this paper focuses on baseline attribution methods, aiming at deriving a feature attribution map at the network input relying on a "neutral" stimulus usually called "baseline". The choice of the baseline is crucial as it determines the explanation of the network behavior. In this framework, this paper has the twofold goal of shedding light on the implications of the choice of the baseline and providing a simple yet effective method for identifying the best baseline for the task. To achieve this, we propose a decision boundary sampling method, since the baseline, by definition, lies on the decision boundary, which naturally becomes the search domain. Experiments are performed on synthetic examples and validated relying on state-of-the-art methods. Despite being limited to the experimental scope, this contribution is relevant as it offers clear guidelines and a simple proxy for baseline selection, reducing ambiguity and enhancing deep models\' reliability and trust.', 'abstract_zh': '随着人工智能的广泛 adoption，提供证据证明 AI 模型的可靠性、可信赖性和公平性显得尤为必要。为此，可解释 AI 这一新兴领域正在发展出技术，以满足这些要求，从而抵消对该技术普遍性的过度宣传。本文重点关注基线归因方法，旨在通过“中立”刺激（通常称为“基线”）在网络输入上推导出特征归因图。基线的选择至关重要，因为它决定了对网络行为的解释。在此框架下，本文的双重目标是揭示基线选择的影响，并提供一种简单有效的基线选择方法。为实现这一目标，我们提出了一种决策边界采样方法，因为基线被定义为决策边界的一部分，自然成为搜索域。实验在合成样本上进行，并通过最新方法进行验证。尽管仅限于实验范围，但本文的贡献依然重要，因为它提供了清晰的指导方针和简单的基线选择代理，减少了模糊性并增强了深度模型的可靠性和可信赖性。', 'title_zh': 'XAI归因方法中基线选择指南'}
{'arxiv_id': 'arXiv:2503.19809', 'title': 'Simulating Tracking Data to Advance Sports Analytics Research', 'authors': 'David Radke, Kyle Tilbury', 'link': 'https://arxiv.org/abs/2503.19809', 'abstract': 'Advanced analytics have transformed how sports teams operate, particularly in episodic sports like baseball. Their impact on continuous invasion sports, such as soccer and ice hockey, has been limited due to increased game complexity and restricted access to high-resolution game tracking data. In this demo, we present a method to collect and utilize simulated soccer tracking data from the Google Research Football environment to support the development of models designed for continuous tracking data. The data is stored in a schema that is representative of real tracking data and we provide processes that extract high-level features and events. We include examples of established tracking data models to showcase the efficacy of the simulated data. We address the scarcity of publicly available tracking data, providing support for research at the intersection of artificial intelligence and sports analytics.', 'abstract_zh': '高级数据分析已 transforming 运动队的运营模式，特别是在棒球等周期性运动中。这类技术对足球和冰球等持续侵入性运动的影响有限，原因在于比赛复杂度的增加以及获取高分辨率比赛跟踪数据的限制。在本演示中，我们介绍了一种方法，用于从 Google Research Football 环境中收集和利用模拟足球跟踪数据，以支持用于持续跟踪数据的模型开发。数据存储在代表实际跟踪数据模式的结构中，并提供了提取高级特征和事件的过程。我们展示了现有的跟踪数据模型示例，以展示模拟数据的有效性，并解决了公开可用跟踪数据稀缺的问题，从而支持人工智能和运动分析交叉领域的研究。', 'title_zh': '模拟追踪数据以推动体育分析研究'}
{'arxiv_id': 'arXiv:2503.19762', 'title': 'Splitting Answer Set Programs with respect to Intensionality Statements (Extended Version)', 'authors': 'Jorge Fandinno, Yuliya Lierler', 'link': 'https://arxiv.org/abs/2503.19762', 'abstract': 'Splitting a logic program allows us to reduce the task of computing its stable models to similar tasks for its subprograms. This can be used to increase solving performance and prove program correctness. We generalize the conditions under which this technique is applicable, by considering not only dependencies between predicates but also their arguments and context. This allows splitting programs commonly used in practice to which previous results were not applicable.', 'abstract_zh': '将逻辑程序分割以其实现子程序稳定模型的计算减少为目标，可以提高求解性能并证明程序的正确性。我们通过考虑谓词及其参数和上下文之间的依赖关系，推广了该技术可应用的条件，使得更多实践中常用的程序可以被分割处理。', 'title_zh': '基于意向性语句划分答案集程序（扩展版本）'}
{'arxiv_id': 'arXiv:2503.19752', 'title': 'Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect on Human-Like Agenda Generation', 'authors': 'Lewis Newsham, Ryan Hyland, Daniel Prince', 'link': 'https://arxiv.org/abs/2503.19752', 'abstract': "This paper presents SANDMAN, an architecture for cyber deception that leverages Language Agents to emulate convincing human simulacra. Our 'Deceptive Agents' serve as advanced cyber decoys, designed for high-fidelity engagement with attackers by extending the observation period of attack behaviours. Through experimentation, measurement, and analysis, we demonstrate how a prompt schema based on the five-factor model of personality systematically induces distinct 'personalities' in Large Language Models. Our results highlight the feasibility of persona-driven Language Agents for generating diverse, realistic behaviours, ultimately improving cyber deception strategies.", 'abstract_zh': 'SANDMAN：一种利用语言代理模拟说服性人类代理体的网络欺骗架构', 'title_zh': '基于LLM的蜜罐代理中诱导个性：测量对类似人类议程生成的影响'}
{'arxiv_id': 'arXiv:2503.19699', 'title': 'Optimal Path Planning and Cost Minimization for a Drone Delivery System Via Model Predictive Control', 'authors': 'Muhammad Al-Zafar Khan, Jamal Al-Karaki', 'link': 'https://arxiv.org/abs/2503.19699', 'abstract': 'In this study, we formulate the drone delivery problem as a control problem and solve it using Model Predictive Control. Two experiments are performed: The first is on a less challenging grid world environment with lower dimensionality, and the second is with a higher dimensionality and added complexity. The MPC method was benchmarked against three popular Multi-Agent Reinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action Learners (JAL), and Value-Decomposition Networks (VDN). It was shown that the MPC method solved the problem quicker and required fewer optimal numbers of drones to achieve a minimized cost and navigate the optimal path.', 'abstract_zh': '本研究将无人机配送问题表述为控制问题，并使用模型预测控制(MPC)方法求解。进行了两项实验：第一项在较低维度的网格世界环境中进行，第二项则在更高维度并增加了复杂性。MPC方法与三种流行的多智能体强化学习(MARL)方法——独立Q学习(IQL)、联合动作学习者(JAL)和价值分解网络(VDN)——进行了比较。结果显示，MPC方法能够更快地解决问题，并且在实现最小成本和最优路径导航时所需的最优无人机数量较少。', 'title_zh': '基于模型预测控制的无人机配送系统最优路径规划与成本最小化'}
{'arxiv_id': 'arXiv:2503.19602', 'title': 'Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning Large Language Models with Less Overthinking', 'authors': 'Yuyao Ge, Shenghua Liu, Yiwei Wang, Lingrui Mei, Lizhe Chen, Baolong Bi, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2503.19602', 'abstract': 'Recent advances in Large Language Models (LLMs) have introduced Reasoning Large Language Models (RLLMs), which employ extended thinking processes with reflection and self-correction capabilities, demonstrating the effectiveness of test-time scaling. RLLMs exhibit innate Chain-of-Thought (CoT) reasoning capability obtained from training, leading to a natural question: "Is CoT prompting, a popular In-Context Learning (ICL) method for chat LLMs, necessary to enhance the reasoning capability of RLLMs?" In this work, we present the first comprehensive analysis of the impacts of Zero-shot CoT and Few-shot CoT on RLLMs across mathematical reasoning tasks. We examine models ranging from 1.5B to 32B parameters, finding that contrary to concerns, CoT prompting significantly enhances RLLMs\' performance in most scenarios. Our results reveal distinct patterns: large-capacity models show minimal improvement on simple tasks but substantial gains on complex problems, while smaller models exhibit the opposite behavior. Further analysis demonstrates that CoT prompting effectively controls the distribution of the numbers of thinking tokens and reasoning steps, reducing excessive reflections by approximately 90% in some cases. Moreover, attention logits analysis reveals the RLLMs\' overfitting to reflection-related words, which is mitigated by external CoT guidance. Notably, our experiments indicate that for RLLMs, one-shot CoT consistently yields superior performance compared to Few-shot CoT approaches. Our findings provide important insights for optimizing RLLMs\' performance through appropriate prompting strategies.', 'abstract_zh': 'Recent Advances in Large Language Models: The Effect of Chain-of-Thought Prompting on Reasoning Large Language Models', 'title_zh': '天賦的推理不足：_CONTEXT_学习使大型语言模型在较少过度思考的情况下增强推理能力'}
{'arxiv_id': 'arXiv:2503.19584', 'title': 'Multi-agent Application System in Office Collaboration Scenarios', 'authors': 'Songtao Sun, Jingyi Li, Yuanfei Dong, Haoguang Liu, Chenxin Xu, Fuyang Li, Qiang Liu', 'link': 'https://arxiv.org/abs/2503.19584', 'abstract': "This paper introduces a multi-agent application system designed to enhance office collaboration efficiency and work quality. The system integrates artificial intelligence, machine learning, and natural language processing technologies, achieving functionalities such as task allocation, progress monitoring, and information sharing. The agents within the system are capable of providing personalized collaboration support based on team members' needs and incorporate data analysis tools to improve decision-making quality. The paper also proposes an intelligent agent architecture that separates Plan and Solver, and through techniques such as multi-turn query rewriting and business tool retrieval, it enhances the agent's multi-intent and multi-turn dialogue capabilities. Furthermore, the paper details the design of tools and multi-turn dialogue in the context of office collaboration scenarios, and validates the system's effectiveness through experiments and evaluations. Ultimately, the system has demonstrated outstanding performance in real business applications, particularly in query understanding, task planning, and tool calling. Looking forward, the system is expected to play a more significant role in addressing complex interaction issues within dynamic environments and large-scale multi-agent systems.", 'abstract_zh': '一种用于提升办公室协作效率和工作质量的多智能体应用系统及其智能代理架构研究', 'title_zh': '办公协作场景下的多Agent应用系统'}
{'arxiv_id': 'arXiv:2503.19470', 'title': 'ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning', 'authors': 'Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Fan Yang, Zenan Zhou, Weipeng Chen, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen', 'link': 'https://arxiv.org/abs/2503.19470', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.', 'abstract_zh': '大型语言模型(LLMs)在推理方面表现出显著的能力，如OpenAI-o1和DeepSeek-R1的成功所示。然而，将推理与外部搜索过程集成仍然具有挑战性，尤其是在处理需要多个检索步骤的复杂多跳问题时。我们提出了一种名为ReSearch的新型框架，该框架通过强化学习训练LLMs进行推理与搜索的结合，而无需使用任何监督数据来标注推理步骤。我们的方法将搜索操作视为推理链中的核心组件，何时以及如何执行搜索由基于文本的思考引导，而搜索结果随后会影响进一步的推理。我们使用Qwen2.5-7B(-Instruct)和Qwen2.5-32B(-Instruct)模型训练ReSearch，并进行了广泛的实验。尽管仅在一个数据集上进行训练，我们的模型在各种基准上展示了强大的泛化能力。分析表明，ReSearch自然地在强化学习过程中激发了诸如反思和自我纠正等高级推理能力。', 'title_zh': 'ReSearch：通过强化学习学习使用搜索进行推理的LLMs'}
{'arxiv_id': 'arXiv:2503.19326', 'title': 'Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps', 'authors': 'Yu Cui, Bryan Hooi, Yujun Cai, Yiwei Wang', 'link': 'https://arxiv.org/abs/2503.19326', 'abstract': 'Recent reasoning large language models (LLMs) have demonstrated remarkable improvements in mathematical reasoning capabilities through long Chain-of-Thought. The reasoning tokens of these models enable self-correction within reasoning chains, enhancing robustness. This motivates our exploration: how vulnerable are reasoning LLMs to subtle errors in their input reasoning chains? We introduce "Compromising Thought" (CPT), a vulnerability where models presented with reasoning tokens containing manipulated calculation results tend to ignore correct reasoning steps and adopt incorrect results instead. Through systematic evaluation across multiple reasoning LLMs, we design three increasingly explicit prompting methods to measure CPT resistance, revealing that models struggle significantly to identify and correct these manipulations. Notably, contrary to existing research suggesting structural alterations affect model performance more than content modifications, we find that local ending token manipulations have greater impact on reasoning outcomes than structural changes. Moreover, we discover a security vulnerability in DeepSeek-R1 where tampered reasoning tokens can trigger complete reasoning cessation. Our work enhances understanding of reasoning robustness and highlights security considerations for reasoning-intensive applications.', 'abstract_zh': '最近的大型语言模型在通过长链推理过程中展现了数学推理能力的显著提高。这些模型的推理标记能够在其推理链中进行自我纠正，从而提高鲁棒性。这激发了我们探索的问题：推理大型语言模型对其输入推理链中的细微错误有多脆弱？我们引入“妥协的思考”（CPT）这一脆弱性，当模型接收到包含篡改计算结果的推理标记时，它们往往会忽略正确的推理步骤，而采用错误的结果。通过在多个推理大型语言模型上的系统评估，我们设计了三种递增明确的提示方法来衡量CPT的抵抗力，结果显示模型在识别和纠正这些篡改方面面临巨大挑战。值得注意的是，与现有研究认为结构修改比内容修改对模型性能的影响更大的观点相反，我们发现局部结束标记的篡改对推理结果的影响大于结构修改。此外，我们发现DeepSeek-R1中存在一个安全漏洞，篡改的推理标记可以触发完整的推理中断。我们的工作增强了对推理鲁棒性的理解，并强调了推理密集型应用中的安全考虑。', 'title_zh': '过程还是结果？操纵的结尾标记可以使LLMs忽视正确的推理步骤'}
{'arxiv_id': 'arXiv:2503.19302', 'title': 'Observation Adaptation via Annealed Importance Resampling for Partially Observable Markov Decision Processes', 'authors': 'Yunuo Zhang, Baiting Luo, Ayan Mukhopadhyay, Abhishek Dubey', 'link': 'https://arxiv.org/abs/2503.19302', 'abstract': 'Partially observable Markov decision processes (POMDPs) are a general mathematical model for sequential decision-making in stochastic environments under state uncertainty. POMDPs are often solved \\textit{online}, which enables the algorithm to adapt to new information in real time. Online solvers typically use bootstrap particle filters based on importance resampling for updating the belief distribution. Since directly sampling from the ideal state distribution given the latest observation and previous state is infeasible, particle filters approximate the posterior belief distribution by propagating states and adjusting weights through prediction and resampling steps. However, in practice, the importance resampling technique often leads to particle degeneracy and sample impoverishment when the state transition model poorly aligns with the posterior belief distribution, especially when the received observation is highly informative. We propose an approach that constructs a sequence of bridge distributions between the state-transition and optimal distributions through iterative Monte Carlo steps, better accommodating noisy observations in online POMDP solvers. Our algorithm demonstrates significantly superior performance compared to state-of-the-art methods when evaluated across multiple challenging POMDP domains.', 'abstract_zh': '部分可观测马尔可夫决策过程（POMDPs）是用于在具有状态不确定性的情况下随机环境中顺序决策的一个通用数学模型。POMDPs通常在线求解，使算法能够实时适应新信息。在线求解器通常使用基于重要性重采样的粒子滤波器来更新信念分布。由于直接从最新的观测和先前状态的理想状态分布中采样是不现实的，粒子滤波器通过预测和重采样步骤传播状态并调整权重来近似后验信念分布。然而，在实践中，当状态转移模型与后验信念分布不匹配时，重要性重采样技术往往会导致粒子衰减和样本贫乏，尤其是在接收到的观测信息非常丰富的情况下。我们提出了一种方法，通过迭代蒙特卡洛步骤构建从状态转移分布到最优分布的一系列桥梁分布，以更好地适应在线POMDP求解器中的噪声观测。我们的算法在多个具有挑战性的POMDP领域中评估时，表现出显著的优越性能。', 'title_zh': '基于退火重要性重采样的部分可观测马尔可夫决策过程的观测自适应方法'}
{'arxiv_id': 'arXiv:2503.19193', 'title': 'Browsing Lost Unformed Recollections: A Benchmark for Tip-of-the-Tongue Search and Reasoning', 'authors': 'Sky CH-Wang, Darshan Deshpande, Smaranda Muresan, Anand Kannappan, Rebecca Qian', 'link': 'https://arxiv.org/abs/2503.19193', 'abstract': 'We introduce Browsing Lost Unformed Recollections, a tip-of-the-tongue known-item search and reasoning benchmark for general AI assistants. BLUR introduces a set of 573 real-world validated questions that demand searching and reasoning across multi-modal and multilingual inputs, as well as proficient tool use, in order to excel on. Humans easily ace these questions (scoring on average 98%), while the best-performing system scores around 56%. To facilitate progress toward addressing this challenging and aspirational use case for general AI assistants, we release 350 questions through a public leaderboard, retain the answers to 250 of them, and have the rest as a private test set.', 'abstract_zh': '浏览迷失未形记：面向通用人工智能助手的舌尖上的回忆已知项搜索与推理基准', 'title_zh': '找回失落的未形成记忆：舌尖上的搜索与推理基准'}
{'arxiv_id': 'arXiv:2503.19174', 'title': 'AssertionForge: Enhancing Formal Verification Assertion Generation with Structured Representation of Specifications and RTL', 'authors': 'Yunsheng Bai, Ghaith Bany Hamad, Syed Suhaib, Haoxing Ren', 'link': 'https://arxiv.org/abs/2503.19174', 'abstract': 'Generating SystemVerilog Assertions (SVAs) from natural language specifications remains a major challenge in formal verification (FV) due to the inherent ambiguity and incompleteness of specifications. Existing LLM-based approaches, such as AssertLLM, focus on extracting information solely from specification documents, often failing to capture essential internal signal interactions and design details present in the RTL code, leading to incomplete or incorrect assertions. We propose a novel approach that constructs a Knowledge Graph (KG) from both specifications and RTL, using a hardware-specific schema with domain-specific entity and relation types. We create an initial KG from the specification and then systematically fuse it with information extracted from the RTL code, resulting in a unified, comprehensive KG. This combined representation enables a more thorough understanding of the design and allows for a multi-resolution context synthesis process which is designed to extract diverse verification contexts from the KG. Experiments on four designs demonstrate that our method significantly enhances SVA quality over prior methods. This structured representation not only improves FV but also paves the way for future research in tasks like code generation and design understanding.', 'abstract_zh': '从自然语言规范生成SystemVerilog断言（SVAs）在形式验证（FV）中依然面临着因规范固有的模糊性和不完整性带来的重大挑战。现有的基于LLM的方法，如AssertLLM，专注于从规范文档中提取信息，常常未能捕捉RTL代码中存在的关键内部信号交互和设计细节，导致生成的断言不完整或不正确。我们提出了一种新的方法，从规范和RTL代码中构建知识图谱（KG），使用硬件特定的模式和领域特定的实体及关系类型。我们从规范中构建初始KG，然后系统地将其与从RTL代码中提取的信息融合，形成一个统一、全面的KG。这种联合表示使对设计的理解更加深入，并允许进行一个多分辨率上下文合成过程，旨在从KG中提取多样化的验证上下文。实验结果表明，我们的方法在生成SVAs方面相比先前方法有显著提升。这种结构化表示不仅改善了形式验证，还为未来的代码生成和设计理解研究铺平了道路。', 'title_zh': 'AssertionForge：通过结构化规范和RTL表示增强形式验证断言生成'}
{'arxiv_id': 'arXiv:2503.19107', 'title': 'Information-Seeking Decision Strategies Mitigate Risk in Dynamic, Uncertain Environments', 'authors': 'Nicholas W. Barendregt, Joshua I. Gold, Krešimir Josić, Zachary P. Kilpatrick', 'link': 'https://arxiv.org/abs/2503.19107', 'abstract': 'To survive in dynamic and uncertain environments, individuals must develop effective decision strategies that balance information gathering and decision commitment. Models of such strategies often prioritize either optimizing tangible payoffs, like reward rate, or gathering information to support a diversity of (possibly unknown) objectives. However, our understanding of the relative merits of these two approaches remains incomplete, in part because direct comparisons have been limited to idealized, static environments that lack the dynamic complexity of the real world. Here we compared the performance of normative reward- and information-seeking strategies in a dynamic foraging task. Both strategies show similar transitions between exploratory and exploitative behaviors as environmental uncertainty changes. However, we find subtle disparities in the actions they take, resulting in meaningful performance differences: whereas reward-seeking strategies generate slightly more reward on average, information-seeking strategies provide more consistent and predictable outcomes. Our findings support the adaptive value of information-seeking behaviors that can mitigate risk with minimal reward loss.', 'abstract_zh': '在动态和不确定性环境中生存，个体必须发展有效的决策策略，平衡信息收集与决策承诺。在这样的策略模型中，通常更侧重于优化具体的回报，如奖励率，或者收集信息以支持多样性的（可能未知的）目标。然而，这些两种方法的优势之间的相对 merits 我们的理解仍然不完整，部分原因是直接比较仅限于理想化的静态环境，缺乏现实世界的动态复杂性。我们在一个动态觅食任务中比较了规范的回报寻求和信息寻求策略的表现。这两种策略在环境不确定性变化时都表现出类似的探索性和利用性行为的转变。然而，我们发现它们采取的行动存在微妙的差异，导致有意义的表现差异：尽管回报寻求策略平均获得更多的回报，但信息寻求策略提供更为一致和可预测的结果。我们的研究结果支持了信息寻求行为的适应价值，可以在最小化回报损失的情况下降低风险。', 'title_zh': '信息搜寻决策策略在动态不确定环境中降低风险'}
{'arxiv_id': 'arXiv:2503.18984', 'title': 'The Misinterpretable Evidence Conveyed by Arbitrary Codes', 'authors': 'Guido Fioretti', 'link': 'https://arxiv.org/abs/2503.18984', 'abstract': "Evidence Theory is a mathematical framework for handling imprecise reasoning in the context of a judge evaluating testimonies or a detective evaluating cues, rather than a gambler playing games of chance. In comparison to Probability Theory, it is better equipped to deal with ambiguous information and novel possibilities. Furthermore, arrival and evaluation of testimonies implies a communication channel.\nThis paper explores the possibility of employing Evidence Theory to represent arbitrary communication codes between and within living organisms. In this paper, different schemes are explored for living organisms incapable of anticipation, animals sufficiently sophisticated to be capable of extrapolation, and humans capable of reading one other's minds.", 'abstract_zh': '证据理论是一种在法官评估证词或侦探评估线索的背景下处理模棱两可推理的数学框架，而不是赌博者参与机会游戏。与概率理论相比，它更能应对模糊信息和新型可能性。此外，证词的接收和评估意味着存在通信渠道。\n本文探讨将证据理论用于表示生物之间及生物内部任意通信码的可能性。本文研究了适用于缺乏预见能力的生物、能够进行外推的足够复杂动物以及能够读取彼此想法的人类的不同方案。', 'title_zh': '任意编码传达的可误解释证据'}
{'arxiv_id': 'arXiv:2503.18971', 'title': 'LLMs as Planning Modelers: A Survey for Leveraging Large Language Models to Construct Automated Planning Models', 'authors': 'Marcus Tantakoun, Xiaodan Zhu, Christian Muise', 'link': 'https://arxiv.org/abs/2503.18971', 'abstract': 'Large Language Models (LLMs) excel in various natural language tasks but often struggle with long-horizon planning problems requiring structured reasoning. This limitation has drawn interest in integrating neuro-symbolic approaches within the Automated Planning (AP) and Natural Language Processing (NLP) communities. However, identifying optimal AP deployment frameworks can be daunting. This paper aims to provide a timely survey of the current research with an in-depth analysis, positioning LLMs as tools for extracting and refining planning models to support reliable AP planners. By systematically reviewing the current state of research, we highlight methodologies, and identify critical challenges and future directions, hoping to contribute to the joint research on NLP and Automated Planning.', 'abstract_zh': '大型语言模型在各种自然语言任务中表现出色，但在需要结构化推理的长期规划问题上常常表现不佳。这一局限性吸引了自动化规划（AP）和自然语言处理（NLP）社区对神经符号方法的关注。然而，确定最佳AP部署框架颇具挑战性。本文旨在提供一篇及时的综述，并进行深入分析，将大型语言模型定位为提取和精炼规划模型的工具，以支持可靠的自动化规划。通过系统地回顾当前的研究状态，我们强调研究方法，识别关键挑战和未来方向，希望为自然语言处理和自动化规划的联合研究贡献一份力量。', 'title_zh': 'LLMs作为规划模型构建者：利用大型语言模型构建自动化规划模型综述'}
{'arxiv_id': 'arXiv:2503.18968', 'title': 'MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via Reasoning Agentic Workflow', 'authors': 'Ziyue Wang, Junde Wu, Chang Han Low, Yueming Jin', 'link': 'https://arxiv.org/abs/2503.18968', 'abstract': 'Developing reliable AI systems to assist human clinicians in multi-modal medical diagnosis has long been a key objective for researchers. Recently, Multi-modal Large Language Models (MLLMs) have gained significant attention and achieved success across various domains. With strong reasoning capabilities and the ability to perform diverse tasks based on user instructions, they hold great potential for enhancing medical diagnosis. However, directly applying MLLMs to the medical domain still presents challenges. They lack detailed perception of visual inputs, limiting their ability to perform quantitative image analysis, which is crucial for medical diagnostics. Additionally, MLLMs often exhibit hallucinations and inconsistencies in reasoning, whereas clinical diagnoses must adhere strictly to established criteria. To address these challenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system designed to achieve reliable, explainable, and precise medical diagnoses. This is accomplished through a hierarchical workflow: at the task level, knowledge-based reasoning generate reliable diagnostic plans for specific diseases following retrieved clinical criteria. While at the case level, multiple tool agents process multi-modal inputs, analyze different indicators according to the plan, and provide a final diagnosis based on both quantitative and qualitative evidence. Comprehensive experiments on both 2D and 3D medical diagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro, while case studies further highlight its reliability and interpretability. The code is available at this https URL.', 'abstract_zh': '开发可靠的AI系统以辅助人类临床医生进行多模态医疗诊断一直是研究人员的重要目标。最近，多模态大型语言模型（MLLMs）在各个领域取得了显著的进展和成功。凭借强大的推理能力和基于用户指令执行多种任务的能力，它们在增强医疗诊断方面具有巨大的潜力。然而，直接将MLLMs应用于医疗领域仍然存在挑战。它们在理解和分析视觉输入方面缺乏细节，限制了它们执行定量图像分析的能力，这对于医疗诊断至关重要。此外，MLLMs常常表现出幻觉和推理中的不一致性，而临床诊断必须严格遵循既定的标准。为了解决这些挑战，我们提出了基于证据的Reasoning代理系统MedAgent-Pro，旨在实现可靠、可解释和精确的医疗诊断。这通过分层工作流程来实现：在任务层面，基于知识的推理生成遵循检索到的临床标准的具体疾病的可靠诊断计划；而在案例层面，多个工具代理处理多模态输入，根据计划分析不同的指标，并基于定量和定性证据给出最终诊断。在2D和3D医疗诊断任务上的综合实验表明了MedAgent-Pro的优势和有效性，而案例研究进一步突显了其可靠性和可解释性。代码可在以下链接获取。', 'title_zh': 'MedAgent-Pro: 基于多模态证据的医学诊断推理代理工作流'}
{'arxiv_id': 'arXiv:2503.18958', 'title': 'Advancing Deep Learning through Probability Engineering: A Pragmatic Paradigm for Modern AI', 'authors': 'Jianyi Zhang', 'link': 'https://arxiv.org/abs/2503.18958', 'abstract': "Recent years have witnessed the rapid progression of deep learning, pushing us closer to the realization of AGI (Artificial General Intelligence). Probabilistic modeling is critical to many of these advancements, which provides a foundational framework for capturing data distributions. However, as the scale and complexity of AI applications grow, traditional probabilistic modeling faces escalating challenges, such as high-dimensional parameter spaces, heterogeneous data sources, and evolving real-world requirements often render classical approaches insufficiently flexible.\nThis paper proposes a novel concept, Probability Engineering, which treats the already-learned probability distributions within deep learning as engineering artifacts. Rather than merely fitting or inferring distributions, we actively modify and reinforce them to better address the diverse and evolving demands of modern AI. Specifically, Probability Engineering introduces novel techniques and constraints to refine existing probability distributions, improving their robustness, efficiency, adaptability, or trustworthiness.\nWe showcase this paradigm through a series of applications spanning Bayesian deep learning, Edge AI (including federated learning and knowledge distillation), and Generative AI (such as text-to-image generation with diffusion models and high-quality text generation with large language models). These case studies demonstrate how probability distributions once treated as static objects can be engineered to meet the diverse and evolving requirements of large-scale, data-intensive, and trustworthy AI systems. By systematically expanding and strengthening the role of probabilistic modeling, Probability Engineering paves the way for more robust, adaptive, efficient, and trustworthy deep learning solutions in today's fast-growing AI era.", 'abstract_zh': '近年来，深度学习取得了 rapid progression，推动我们向通用人工智能（AGI）的实现更近一步。概率模型对这些进展至关重要，为捕捉数据分布提供了基础框架。然而，随着AI应用规模和复杂性的增加，传统概率模型面临越来越大的挑战，如高维度参数空间、异质数据源以及不断变化的现实世界需求， often render 其经典方法不够灵活。\n\n本文提出了一种新的概念——概率工程，将深度学习中已学习的概率分布视为工程制品。我们不仅限于拟合或推断分布，而是积极修改和强化它们，以更好地应对现代AI的多样性和不断变化的需求。具体来说，概率工程引入了新颖的技术和约束，以改进现有概率分布的稳健性、效率、适应性和可信度。\n\n通过一系列应用，包括贝叶斯深度学习、边缘AI（包括联邦学习和知识蒸馏）以及生成AI（如使用扩散模型的文字转图像生成和大语言模型的高质量文字生成），本文展示了如何将原本视为静态对象的概率分布工程化，以满足大规模、数据密集和可信AI系统的需求。通过系统地扩大和加强概率模型的作用，概率工程为当今快速发展的人工智能时代提供了更稳健、更具适应性、更高效和更可信的深度学习解决方案。', 'title_zh': '通过概率工程推动深度学习：一种面向现代AI的实际范式'}
{'arxiv_id': 'arXiv:2503.18955', 'title': 'Is there a future for AI without representation?', 'authors': 'Vincent C. Müller', 'link': 'https://arxiv.org/abs/2503.18955', 'abstract': "This paper investigates the prospects of AI without representation in general, and the proposals of Rodney Brooks in particular. What turns out to be characteristic of Brooks' proposal is the rejection of central control in intelligent agents; his systems has as much or as little representation as traditional AI. The traditional view that representation is necessary for intelligence presupposes that intelligence requires central control. However, much of recent cognitive science suggests that we should dispose of the image of intelligent agents as central representation processors. If this paradigm shift is achieved, Brooks' proposal for non-centralized cognition without representation appears promising for full-blown intelligent agents - though not for conscious agents and thus not for human-like AI.", 'abstract_zh': '本文探讨了无表征的AI的前景，尤其是Rodney Brooks的提议。Brooks提议的特征在于拒绝智能代理中的中心控制；他的系统可能具有与传统AI同样量级的表征，或者根本不具备表征。传统观点认为表征是智能的必要条件，这暗示智能需要中心控制。然而，近期认知科学的大量研究表明，我们可能应该摒弃智能代理为中心表征处理器的形象。如果这种范式转变得以实现，Brooks提出的无中心化的无表征认知方案可能对完全智能化代理是有前景的——尽管不是对有意识的代理，因此不是对类人AI。', 'title_zh': '没有代表性的AI，未来在哪里？'}
{'arxiv_id': 'arXiv:2503.19900', 'title': 'CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning', 'authors': 'Hao Yu, Zhuokai Zhao, Shen Yan, Lukasz Korycki, Jianyu Wang, Baosheng He, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hanchao Yu', 'link': 'https://arxiv.org/abs/2503.19900', 'abstract': 'The rapid advancement of large vision-language models (LVLMs) has driven significant progress in multimodal tasks, enabling models to interpret, reason, and generate outputs across both visual and textual domains. While excelling in generative tasks, existing LVLMs often face limitations in tasks requiring high-fidelity representation learning, such as generating image or text embeddings for retrieval. Recent work has proposed finetuning LVLMs for representational learning, but the fine-tuned model often loses its generative capabilities due to the representational learning training paradigm. To address this trade-off, we introduce CAFe, a contrastive-autoregressive fine-tuning framework that enhances LVLMs for both representation and generative tasks. By integrating a contrastive objective with autoregressive language modeling, our approach unifies these traditionally separate tasks, achieving state-of-the-art results in both multimodal retrieval and multimodal generative benchmarks, including object hallucination (OH) mitigation. CAFe establishes a novel framework that synergizes embedding and generative functionalities in a single model, setting a foundation for future multimodal models that excel in both retrieval precision and coherent output generation.', 'abstract_zh': '大型多模态视觉-语言模型的快速进步推动了多模态任务的重大进展，使模型能够跨视觉和文本领域进行解释、推理和生成输出。尽管在生成任务上表现出色，现有的大型多模态视觉-语言模型在需要高保真表示学习的任务中，如生成图像或文本嵌入用于检索，常常面临局限。近期的研究提出了为表示学习微调大型多模态视觉-语言模型的方法，但由于表示学习训练范式的影响，微调后的模型往往会失去其生成能力。为了解决这一权衡，我们引入了CAFe，一种对比-自回归微调框架，该框架增强了大型多模态视觉-语言模型在表示和生成任务方面的性能。通过结合对比目标和自回归语言建模，我们的方法统一了传统的分离任务，实现了多模态检索和多模态生成基准的最优结果，包括对象幻觉（OH）缓解。CAFe 确立了一个新的框架，将嵌入功能和生成功能在单一模型中协同工作，为未来在检索精度和连贯输出生成方面都表现出色的多模态模型奠定了基础。', 'title_zh': 'CAFe：对比自回归微调统一表示与生成'}
{'arxiv_id': 'arXiv:2503.19887', 'title': 'A proposal for an incident regime that tracks and counters threats to national security posed by AI systems', 'authors': 'Alejandro Ortega', 'link': 'https://arxiv.org/abs/2503.19887', 'abstract': "Recent progress in AI capabilities has heightened concerns that AI systems could pose a threat to national security, for example, by making it easier for malicious actors to perform cyberattacks on critical national infrastructure, or through loss of control of autonomous AI systems. In parallel, federal legislators in the US have proposed nascent 'AI incident regimes' to identify and counter similar threats. In this paper, we consolidate these two trends and present a proposal for a legally mandated post-deployment AI incident regie that aims to counter potential national security threats from AI systems. We start the paper by introducing the concept of 'security-critical' to describe doctors that pose extreme risks to national security, before arguing that 'security-critical' describes civilian nuclear power, aviation, life science dual-use research of concern, and frontier AI development. We then present in detail our AI incident regime proposal,, justifying each component of the proposal by demonstrating its similarity to US domestic incident regimes in other 'security-critical' sectors. Finally, we sketch a hypothetical scenario where our proposed AI incident regime deals with an AI cyber incident. Our proposed AI incident regime is split into three phases. The first phase revolves around a novel operationalization of what counts as an 'AI incident' and we suggest that AI providers must create a 'national security case' before deploying a frontier AI system. The second and third phases spell out that AI providers should notify a government agency about incidents, and that the government agency should be involved in amending AI providers' security and safety procedures, in order to counter future threats to national security. Our proposal is timely, given ongoing policy interest in the potential national security threats posed by AI systems.", 'abstract_zh': '近期人工智能能力的进展加剧了对人工智能系统可能对国家安全构成威胁的担忧，例如通过使恶意行为者更容易对关键基础设施进行网络攻击，或通过失去对自主人工智能系统的控制。与此同时，美国联邦立法者提出了初步的“人工智能事件制度”来识别和应对类似威胁。本文整合了这两种趋势，并提出了一项旨在应对人工智能系统潜在国家安全威胁的法律强制执行型部署后人工智能事件制度的提案。本文首先引入“安全关键型”的概念来描述对国家安全构成极端风险的医生，然后 argument 说明“安全关键型”也适用于民用核能、航空、生物双用途研究以及前沿人工智能开发。然后，我们详细阐述了我们的人工智能事件制度提案，并通过证明其与美国其他“安全关键型”领域内事件制度相似之处来为每个提案组件提供正当性。最后，我们描绘了一个假设场景，说明我们提议的人工智能事件制度如何处理人工智能网络攻击事件。我们的提议被分成三个阶段。第一阶段围绕着对“人工智能事件”的新颖定义展开，我们建议人工智能提供商在部署前沿人工智能系统前必须创建一个“国家安全案例”。第二和第三阶段规定人工智能提供商必须向政府部门报告事件，并要求政府部门参与修改人工智能提供商的安全和安全程序，以应对未来的国家安全威胁。鉴于对人工智能系统潜在国家安全威胁的持续政策兴趣，我们的提议非常及时。', 'title_zh': '一种追踪并抵御由人工智能系统威胁国家安全的事件机制提案'}
{'arxiv_id': 'arXiv:2503.19885', 'title': 'Dynamics of Structured Complex-Valued Hopfield Neural Networks', 'authors': 'Rama Murthy Garimella, Marcos Eduardo Valle, Guilherme Vieira, Anil Rayala, Dileep Munugoti', 'link': 'https://arxiv.org/abs/2503.19885', 'abstract': 'In this paper, we explore the dynamics of structured complex-valued Hopfield neural networks (CvHNNs), which arise when the synaptic weight matrix possesses specific structural properties. We begin by analyzing CvHNNs with a Hermitian synaptic weight matrix and establish the existence of four-cycle dynamics in CvHNNs with skew-Hermitian weight matrices operating synchronously. Furthermore, we introduce two new classes of complex-valued matrices: braided Hermitian and braided skew-Hermitian matrices. We demonstrate that CvHNNs utilizing these matrix types exhibit cycles of length eight when operating in full parallel update mode. Finally, we conduct extensive computational experiments on synchronous CvHNNs, exploring other synaptic weight matrix structures. The findings provide a comprehensive overview of the dynamics of structured CvHNNs, offering insights that may contribute to developing improved associative memory models when integrated with suitable learning rules.', 'abstract_zh': '本文探讨了结构化复值霍普菲尔德神经网络（CvHNNs）的动力学，这些网络在突触权重矩阵具有特定结构属性时产生。我们首先分析了具有 Hermitian 突触权重矩阵的 CvHNNs，并建立了具有 skew-Hermitian 权重矩阵且同步操作的 CvHNNs 中四循环动力学的存在性。此外，我们引入了两种新的复值矩阵类别：编braided Hermitian 和编braided skew-Hermitian 矩阵。我们证明了这些矩阵类型在全并行更新模式下操作的 CvHNNs 展现出长度为八的循环。最后，我们对同步 CvHNNs 进行了大量计算实验，探索其他突触权重矩阵结构。研究发现提供了结构化 CvHNNs 动力学的全面概述，所提供的见解可能有助于结合适当的联想记忆模型时的发展改进。', 'title_zh': '结构化复值霍普菲尔德神经网络的动力学研究'}
{'arxiv_id': 'arXiv:2503.19868', 'title': 'GENIUS: A Generative Framework for Universal Multimodal Search', 'authors': 'Sungyeon Kim, Xinliang Zhu, Xiaofan Lin, Muhammet Bastan, Douglas Gray, Suha Kwak', 'link': 'https://arxiv.org/abs/2503.19868', 'abstract': 'Generative retrieval is an emerging approach in information retrieval that generates identifiers (IDs) of target data based on a query, providing an efficient alternative to traditional embedding-based retrieval methods. However, existing models are task-specific and fall short of embedding-based retrieval in performance. This paper proposes GENIUS, a universal generative retrieval framework supporting diverse tasks across multiple modalities and domains. At its core, GENIUS introduces modality-decoupled semantic quantization, transforming multimodal data into discrete IDs encoding both modality and semantics. Moreover, to enhance generalization, we propose a query augmentation that interpolates between a query and its target, allowing GENIUS to adapt to varied query forms. Evaluated on the M-BEIR benchmark, it surpasses prior generative methods by a clear margin. Unlike embedding-based retrieval, GENIUS consistently maintains high retrieval speed across database size, with competitive performance across multiple benchmarks. With additional re-ranking, GENIUS often achieves results close to those of embedding-based methods while preserving efficiency.', 'abstract_zh': '通用生成检索框架GENIUS：跨模态与跨领域的通用生成检索方法', 'title_zh': 'GENIUS: 生成框架下的通用多模态搜索'}
{'arxiv_id': 'arXiv:2503.19867', 'title': 'Geometric Meta-Learning via Coupled Ricci Flow: Unifying Knowledge Representation and Quantum Entanglement', 'authors': 'Ming Lei, Christophe Baehr', 'link': 'https://arxiv.org/abs/2503.19867', 'abstract': 'This paper establishes a unified framework integrating geometric flows with deep learning through three fundamental innovations. First, we propose a thermodynamically coupled Ricci flow that dynamically adapts parameter space geometry to loss landscape topology, formally proved to preserve isometric knowledge embedding (Theorem~\\ref{thm:isometric}). Second, we derive explicit phase transition thresholds and critical learning rates (Theorem~\\ref{thm:critical}) through curvature blowup analysis, enabling automated singularity resolution via geometric surgery (Lemma~\\ref{lem:surgery}). Third, we establish an AdS/CFT-type holographic duality (Theorem~\\ref{thm:ads}) between neural networks and conformal field theories, providing entanglement entropy bounds for regularization design. Experiments demonstrate 2.1$\\times$ convergence acceleration and 63\\% topological simplification while maintaining $\\mathcal{O}(N\\log N)$ complexity, outperforming Riemannian baselines by 15.2\\% in few-shot accuracy. Theoretically, we prove exponential stability (Theorem~\\ref{thm:converge}) through a new Lyapunov function combining Perelman entropy with Wasserstein gradient flows, fundamentally advancing geometric deep learning.', 'abstract_zh': '本文通过三项基本原则创新，建立了将几何流与深度学习统一起来的框架：首先，提出了一种热力学耦合 Ricci 流，动态适应参数空间几何以匹配损失景观拓扑，并形式上证明了保持等距知识嵌入（定理~\\ref{thm:isometric}）。其次，通过曲率爆炸分析推导出明确的相转变阈值和关键学习率（定理~\\ref{thm:critical}），使几何外科手术能够自动解决奇异性问题。第三，建立了神经网络与共形场理论之间的 AdS/CFT 类型全息对偶性（定理~\\ref{thm:ads}），为正则化设计提供了纠缠熵界。实验结果表明，该方法在保持 $\\mathcal{O}(N\\log N)$ 复杂度的情况下实现了 2.1 倍的收敛加速和 63% 的拓扑简化，且准确率超越黎曼基线 15.2%。理论上，我们通过将 Perelman 能量与 Wasserstein 梯度流结合的新 Lyapunov 函数证明了指数稳定（定理~\\ref{thm:converge}），从根本上推进了几何深度学习。', 'title_zh': '几何元学习通过耦合里奇流：知识表示与量子纠缠的统一'}
{'arxiv_id': 'arXiv:2503.19848', 'title': 'Guarding against artificial intelligence--hallucinated citations: the case for full-text reference deposit', 'authors': 'Alex Glynn', 'link': 'https://arxiv.org/abs/2503.19848', 'abstract': 'The tendency of generative artificial intelligence (AI) systems to "hallucinate" false information is well-known; AI-generated citations to non-existent sources have made their way into the reference lists of peer-reviewed publications. Here, I propose a solution to this problem, taking inspiration from the Transparency and Openness Promotion (TOP) data sharing guidelines, the clash of generative AI with the American judiciary, and the precedent set by submissions of prior art to the United States Patent and Trademark Office. Journals should require authors to submit the full text of each cited source along with their manuscripts, thereby preventing authors from citing any material whose full text they cannot produce. This solution requires limited additional work on the part of authors or editors while effectively immunizing journals against hallucinated references.', 'abstract_zh': '生成式人工智能系统生成虚假信息的倾向是众所周知的；人工智能生成的引用非-existent来源的文献已进入经过同行评审的出版物的参考文献列表。在此，我提出一种解决这一问题的方法，借鉴了透明度和开放性促进（TOP）数据共享指导原则、生成式人工智能与美国司法体系的冲突以及向美国专利和商标局提交现有技术的先例。期刊应要求作者在提交稿件时一并提交每个引用来源的全文，从而防止作者引用他们无法提供全文的材料。该解决方案仅要求作者或编辑进行少量额外工作，同时有效保护期刊免受虚假参考文献的影响。', 'title_zh': '防范人工智能虚构引文：全文参考文献存档的必要性'}
{'arxiv_id': 'arXiv:2503.19844', 'title': 'A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and Named Entity Recognition for Historical Chinese Sources, 1900-1950', 'authors': 'Zhao Fang, Liang-Chun Wu, Xuening Kong, Spencer Dean Stewart', 'link': 'https://arxiv.org/abs/2503.19844', 'abstract': 'This paper compares large language models (LLMs) and traditional natural language processing (NLP) tools for performing word segmentation, part-of-speech (POS) tagging, and named entity recognition (NER) on Chinese texts from 1900 to 1950. Historical Chinese documents pose challenges for text analysis due to their logographic script, the absence of natural word boundaries, and significant linguistic changes. Using a sample dataset from the Shanghai Library Republican Journal corpus, traditional tools such as Jieba and spaCy are compared to LLMs, including GPT-4o, Claude 3.5, and the GLM series. The results show that LLMs outperform traditional methods in all metrics, albeit at considerably higher computational costs, highlighting a trade-off between accuracy and efficiency. Additionally, LLMs better handle genre-specific challenges such as poetry and temporal variations (i.e., pre-1920 versus post-1920 texts), demonstrating that their contextual learning capabilities can advance NLP approaches to historical texts by reducing the need for domain-specific training data.', 'abstract_zh': '本文将大型语言模型（LLMs）与传统自然语言处理（NLP）工具对比，分析其在1900年至1950年中文文本的字词切分、词性标注和命名实体识别任务中的表现。由于历史中文文献使用表意文字、缺乏自然单词边界且语言经历了显著变化，这些文献对文本分析构成了挑战。利用上海图书馆民国期刊语料库的样本数据，传统工具如Jieba和spaCy与LLMs（包括GPT-4o、Claude 3.5和GLM系列）进行了比较。研究结果表明，尽管计算成本显著增加，LLMs在所有指标上均优于传统方法，揭示了准确性和效率之间的权衡。此外，LLMs在处理如诗歌等特定文体挑战以及时间段变化（即1920年前与1920年后）方面表现更佳，表明其上下文学习能力可以推动NLP方法在历史文本分析中的应用，减少对领域特定训练数据的依赖。', 'title_zh': '1900-1950年历史中文资料中的词分割、词性标注和命名实体识别比较分析'}
{'arxiv_id': 'arXiv:2503.19823', 'title': 'GyralNet Subnetwork Partitioning via Differentiable Spectral Modularity Optimization', 'authors': 'Yan Zhuang, Minheng Chen, Chao Cao, Tong Chen, Jing Zhang, Xiaowei Yu, Yanjun Lyu, Lu Zhang, Tianming Liu, Dajiang Zhu', 'link': 'https://arxiv.org/abs/2503.19823', 'abstract': 'Understanding the structural and functional organization of the human brain requires a detailed examination of cortical folding patterns, among which the three-hinge gyrus (3HG) has been identified as a key structural landmark. GyralNet, a network representation of cortical folding, models 3HGs as nodes and gyral crests as edges, highlighting their role as critical hubs in cortico-cortical connectivity. However, existing methods for analyzing 3HGs face significant challenges, including the sub-voxel scale of 3HGs at typical neuroimaging resolutions, the computational complexity of establishing cross-subject correspondences, and the oversimplification of treating 3HGs as independent nodes without considering their community-level relationships. To address these limitations, we propose a fully differentiable subnetwork partitioning framework that employs a spectral modularity maximization optimization strategy to modularize the organization of 3HGs within GyralNet. By incorporating topological structural similarity and DTI-derived connectivity patterns as attribute features, our approach provides a biologically meaningful representation of cortical organization. Extensive experiments on the Human Connectome Project (HCP) dataset demonstrate that our method effectively partitions GyralNet at the individual level while preserving the community-level consistency of 3HGs across subjects, offering a robust foundation for understanding brain connectivity.', 'abstract_zh': '理解人类大脑的结构和功能组织需要对皮层折叠模式进行详细的检查，其中三铰褶回（3HG）已被识别为关键的结构标志。GyralNet 是一种皮层折叠的网络表示方法，将 3HGs 表示为节点，将皮层褶皱脊表示为边，并突显其在皮层-皮层连接中的关键枢纽作用。然而，现有的 3HGs 分析方法面临着显著挑战，包括在典型神经影像学分辨率下的亚体素尺度、跨个体对应关系的计算复杂性以及将 3HGs 简单视为独立节点而不考虑其社区层级关系的过度简化。为了解决这些限制，我们提出了一种完全可微的子网络分割框架，该框架采用谱模块最大化优化策略来模块化 GyralNet 中 3HGs 的组织。通过结合拓扑结构相似性和来自扩散张量成像的连接模式作为属性特征，我们的方法为皮层组织提供了生物学意义的表示。在人类连接组计划（HCP）数据集上的广泛实验表明，我们的方法有效地在个体水平上分割了 GyralNet，同时在不同个体之间保持了 3HGs 的社区层级一致性，为理解大脑连接提供了一个稳健的基础。', 'title_zh': 'GyralNet 带有可微谱模ularity优化的皮层网络子网络划分'}
{'arxiv_id': 'arXiv:2503.19817', 'title': 'Bitstream Collisions in Neural Image Compression via Adversarial Perturbations', 'authors': 'Jordan Madden, Lhamo Dorje, Xiaohua Li', 'link': 'https://arxiv.org/abs/2503.19817', 'abstract': "Neural image compression (NIC) has emerged as a promising alternative to classical compression techniques, offering improved compression ratios. Despite its progress towards standardization and practical deployment, there has been minimal exploration into it's robustness and security. This study reveals an unexpected vulnerability in NIC - bitstream collisions - where semantically different images produce identical compressed bitstreams. Utilizing a novel whitebox adversarial attack algorithm, this paper demonstrates that adding carefully crafted perturbations to semantically different images can cause their compressed bitstreams to collide exactly. The collision vulnerability poses a threat to the practical usability of NIC, particularly in security-critical applications. The cause of the collision is analyzed, and a simple yet effective mitigation method is presented.", 'abstract_zh': '神经图像压缩的比特流碰撞脆弱性：一种新型白盒 adversarial 攻击揭示的意外漏洞', 'title_zh': '基于 adversarial 永整的神经图像压缩中的比特流碰撞'}
{'arxiv_id': 'arXiv:2503.19804', 'title': 'LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset', 'authors': 'Manjushree Aithal, Rosaura G. VidalMata, Manikandtan Kartha, Gong Chen, Eashan Adhikarla, Lucas N. Kirsten, Zhicheng Fu, Nikhil A. Madhusudhana, Joe Nasti', 'link': 'https://arxiv.org/abs/2503.19804', 'abstract': 'Low-light image enhancement is crucial for a myriad of applications, from night vision and surveillance, to autonomous driving. However, due to the inherent limitations that come in hand with capturing images in low-illumination environments, the task of enhancing such scenes still presents a formidable challenge. To advance research in this field, we introduce our Low Exposure Night Vision (LENVIZ) Dataset, a comprehensive multi-exposure benchmark dataset for low-light image enhancement comprising of over 230K frames showcasing 24K real-world indoor and outdoor, with-and without human, scenes. Captured using 3 different camera sensors, LENVIZ offers a wide range of lighting conditions, noise levels, and scene complexities, making it the largest publicly available up-to 4K resolution benchmark in the field. LENVIZ includes high quality human-generated ground truth, for which each multi-exposure low-light scene has been meticulously curated and edited by expert photographers to ensure optimal image quality. Furthermore, we also conduct a comprehensive analysis of current state-of-the-art low-light image enhancement techniques on our dataset and highlight potential areas of improvement.', 'abstract_zh': '低光照图像增强对于夜间视觉和监视等多种应用至关重要，从夜视和监控到自动驾驶。然而，由于在低光照环境中捕捉图像固有的局限性，增强这类场景依然面临巨大挑战。为推动该领域研究，我们介绍了Low Exposure Night Vision (LENVIZ) 数据集，这是一个包含超过230,000帧、涵盖24,000个真实室内和室外场景（有人或无人）的全面多曝光基准数据集。LENVIZ采用3种不同的相机传感器捕捉，涵盖了广泛的光照条件、噪声水平和场景复杂性，使之成为迄今为止最大的公共可用4K分辨率基准数据集。LENVIZ包括高质量的人工生成的真实 ground truth，每个多曝光低光照场景均由专家摄影师精心策划和编辑，以确保最佳图像质量。此外，我们在该数据集上对当前最先进的低光照图像增强技术进行了全面分析，并指出了改进的潜力。', 'title_zh': 'LENVIZ: 一种高分辨率低曝光夜视基准数据集'}
{'arxiv_id': 'arXiv:2503.19801', 'title': 'SeLIP: Similarity Enhanced Contrastive Language Image Pretraining for Multi-modal Head MRI', 'authors': 'Zhiyang Liu, Dong Yang, Minghao Zhang, Hanyu Sun, Hong Wu, Huiying Wang, Wen Shen, Chao Chai, Shuang Xia', 'link': 'https://arxiv.org/abs/2503.19801', 'abstract': 'Despite that deep learning (DL) methods have presented tremendous potential in many medical image analysis tasks, the practical applications of medical DL models are limited due to the lack of enough data samples with manual annotations. By noting that the clinical radiology examinations are associated with radiology reports that describe the images, we propose to develop a foundation model for multi-model head MRI by using contrastive learning on the images and the corresponding radiology findings. In particular, a contrastive learning framework is proposed, where a mixed syntax and semantic similarity matching metric is integrated to reduce the thirst of extreme large dataset in conventional contrastive learning framework. Our proposed similarity enhanced contrastive language image pretraining (SeLIP) is able to effectively extract more useful features. Experiments revealed that our proposed SeLIP performs well in many downstream tasks including image-text retrieval task, classification task, and image segmentation, which highlights the importance of considering the similarities among texts describing different images in developing medical image foundation models.', 'abstract_zh': '尽管深度学习方法在许多医疗图像分析任务中展示了巨大的潜力，但由于缺乏足够的带有手动标注的数据样本，医疗深度学习模型的实际应用受到限制。鉴于临床放射学检查与放射学报告相关，报告描述了图像内容，我们提出了一种通过对比学习框架开发多模态头部MRI基础模型的方法，该框架结合图像和相应的放射学发现进行训练。特别是，我们提出了一种对比学习框架，其中集成了一种混合语法和语义相似性匹配度量，以降低传统对比学习框架对海量数据的依赖。我们提出的增强相似性对比语言图像预训练（SeLIP）能够有效提取更多有用特征。实验表明，我们的SeLIP在包括图像-文本检索任务、分类任务和图像分割在内的许多下游任务中表现良好，突显了在开发医疗图像基础模型时考虑描述不同图像的文本之间的相似性的重要性。', 'title_zh': 'SeLIP: 增强相似性对比语言图像预训练多模态头部MRI'}
{'arxiv_id': 'arXiv:2503.19794', 'title': 'PAVE: Patching and Adapting Video Large Language Models', 'authors': 'Zhuoming Liu, Yiquan Li, Khoi Duc Nguyen, Yiwu Zhong, Yin Li', 'link': 'https://arxiv.org/abs/2503.19794', 'abstract': 'Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as "patches," which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at this https URL.', 'abstract_zh': '预训练视频大语言模型（Video LLMs）表现出色的推理能力，但将其适应涉及额外模态或数据类型（如音频或3D信息）的新任务仍然具有挑战性。本文提出PAVE，一种灵活的框架，用于通过侧信道信号（如音频、3D提示或多视角视频）将预训练Video LLMs适应下游任务。PAVE引入了称为“补丁”的轻量级适配器，这些补丁在不修改基础模型架构或预训练权重的情况下，添加少量参数和操作。通过这种方式，PAVE可以有效地将预训练的基础模型适应多种下游任务，包括音频-视觉问答、3D推理、多视角视频识别和高帧率视频理解。在这些任务上，PAVE显著增强了基础模型的性能，相对于最先进的任务特定模型，仅有轻微的成本增加，约为0.1%的额外FLOPs和参数。此外，PAVE支持多任务学习，并在不同Video LLMs之间具有良好的泛化能力。代码可在此处访问。', 'title_zh': 'PAVE: 贴图和适应视频大型语言模型'}
{'arxiv_id': 'arXiv:2503.19786', 'title': 'Gemma 3 Technical Report', 'authors': 'Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucińska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini', 'link': 'https://arxiv.org/abs/2503.19786', 'abstract': 'We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.', 'abstract_zh': 'Gemma 3: 一种轻量级开源多模态模型，范围从1亿到27亿参数，并具有视觉理解能力及更长上下文处理能力', 'title_zh': 'Gemma 3 技术报告'}
{'arxiv_id': 'arXiv:2503.19753', 'title': 'A Survey on Event-driven 3D Reconstruction: Development under Different Categories', 'authors': 'Chuanzhi Xu, Haoxian Zhou, Haodong Chen, Vera Chung, Qiang Qu', 'link': 'https://arxiv.org/abs/2503.19753', 'abstract': 'Event cameras have gained increasing attention for 3D reconstruction due to their high temporal resolution, low latency, and high dynamic range. They capture per-pixel brightness changes asynchronously, allowing accurate reconstruction under fast motion and challenging lighting conditions. In this survey, we provide a comprehensive review of event-driven 3D reconstruction methods, including stereo, monocular, and multimodal systems. We further categorize recent developments based on geometric, learning-based, and hybrid approaches. Emerging trends, such as neural radiance fields and 3D Gaussian splatting with event data, are also covered. The related works are structured chronologically to illustrate the innovations and progression within the field. To support future research, we also highlight key research gaps and future research directions in dataset, experiment, evaluation, event representation, etc.', 'abstract_zh': '事件相机由于其高时间分辨率、低延迟和高动态范围，逐渐在三维重建领域获得广泛关注。它们以异步方式捕获每个像素的亮度变化，使得在快速运动和具有挑战性的光照条件下进行精确重建成为可能。在本文综述中，我们提供了事件驱动三维重建方法的综述，包括立体视觉、单目视觉和多模态系统。我们进一步根据几何方法、学习方法和混合方法对近期发展进行了分类。此外，还涵盖了神经辐射场和带有事件数据的3D高斯Splatting等新兴趋势。相关工作的结构化编年史展示了该领域内的创新和发展。为支持未来研究，我们还指出了数据集、实验、评估、事件表示等方面的潜在研究空白和未来研究方向。', 'title_zh': '事件驱动的3D重建综述：不同类别下的发展'}
{'arxiv_id': 'arXiv:2503.19730', 'title': 'CamSAM2: Segment Anything Accurately in Camouflaged Videos', 'authors': 'Yuli Zhou, Guolei Sun, Yawei Li, Yuqian Fu, Luca Benini, Ender Konukoglu', 'link': 'https://arxiv.org/abs/2503.19730', 'abstract': "Video camouflaged object segmentation (VCOS), aiming at segmenting camouflaged objects that seamlessly blend into their environment, is a fundamental vision task with various real-world applications. With the release of SAM2, video segmentation has witnessed significant progress. However, SAM2's capability of segmenting camouflaged videos is suboptimal, especially when given simple prompts such as point and box. To address the problem, we propose Camouflaged SAM2 (CamSAM2), which enhances SAM2's ability to handle camouflaged scenes without modifying SAM2's parameters. Specifically, we introduce a decamouflaged token to provide the flexibility of feature adjustment for VCOS. To make full use of fine-grained and high-resolution features from the current frame and previous frames, we propose implicit object-aware fusion (IOF) and explicit object-aware fusion (EOF) modules, respectively. Object prototype generation (OPG) is introduced to abstract and memorize object prototypes with informative details using high-quality features from previous frames. Extensive experiments are conducted to validate the effectiveness of our approach. While CamSAM2 only adds negligible learnable parameters to SAM2, it substantially outperforms SAM2 on three VCOS datasets, especially achieving 12.2 mDice gains with click prompt on MoCA-Mask and 19.6 mDice gains with mask prompt on SUN-SEG-Hard, with Hiera-T as the backbone. The code will be available at \\href{this https URL}{this http URL}.", 'abstract_zh': '掩蔽视频对象分割 (VCOS): 针对无缝融合环境的掩蔽对象分割，是视觉领域的基础任务，具有多种实际应用。随着SAM2的发布，视频分割取得了显著进展。然而，SAM2在分割掩蔽视频方面的能力不尽如人意，特别是对于简单的提示如点和框。为了解决这一问题，我们提出了一种增强SAM2处理掩蔽场景能力的方法——Camouflaged SAM2 (CamSAM2)，该方法在不修改SAM2参数的情况下提升了其性能。具体而言，我们引入了一种去掩蔽标记以提供VCOS中特征调整的灵活性。为充分利用当前帧和先前帧的细粒度和高分辨率特征，我们提出了隐式对象感知融合 (IOF) 和显式对象感知融合 (EOF) 模块。引入了对象原型生成 (OPG)，通过利用先前帧中高质量特征来抽象并记忆具有信息细节的对象原型。进行了广泛的实验以验证我们方法的有效性。尽管CamSAM2仅为SAM2增加了极少的可学习参数，但在三种VCOS数据集上仍显著优于SAM2，特别是在使用点击提示（MoCA-Mask）时获得了12.2 mDice的提升，在使用掩码提示（SUN-SEG-Hard）时获得了19.6 mDice的提升，以Hiera-T作为骨干网络。代码将在 \\href{this https URL}{this http URL} 公开。', 'title_zh': 'CamSAM2: 在伪装视频中准确分割Anything'}
{'arxiv_id': 'arXiv:2503.19719', 'title': 'On What Depends the Robustness of Multi-source Models to Missing Data in Earth Observation?', 'authors': 'Francisco Mena, Diego Arenas, Miro Miranda, Andreas Dengel', 'link': 'https://arxiv.org/abs/2503.19719', 'abstract': 'In recent years, the development of robust multi-source models has emerged in the Earth Observation (EO) field. These are models that leverage data from diverse sources to improve predictive accuracy when there is missing data. Despite these advancements, the factors influencing the varying effectiveness of such models remain poorly understood. In this study, we evaluate the predictive performance of six state-of-the-art multi-source models in predicting scenarios where either a single data source is missing or only a single source is available. Our analysis reveals that the efficacy of these models is intricately tied to the nature of the task, the complementarity among data sources, and the model design. Surprisingly, we observe instances where the removal of certain data sources leads to improved predictive performance, challenging the assumption that incorporating all available data is always beneficial. These findings prompt critical reflections on model complexity and the necessity of all collected data sources, potentially shaping the way for more streamlined approaches in EO applications.', 'abstract_zh': '近年来，地球观测（EO）领域涌现出了一类鲁棒的多源模型。这些模型利用多来源数据来提高在数据缺失情况下的预测准确性。尽管取得了这些进展，但影响这类模型不同有效性的因素依然知之甚少。在本研究中，我们评估了六种最先进的多源模型在单个数据源缺失或仅有一个数据源可用条件下的预测性能。我们的分析表明，这些模型的有效性紧密依赖于任务特性、数据源间的互补性和模型设计。令人惊讶的是，我们发现去除某些数据源会提高预测性能，这一现象挑战了将所有可用数据纳入模型总是有益的看法。这些发现促使我们对模型复杂性和所有收集数据源的必要性进行深入反思，可能引导地球观测（EO）应用领域的更简洁方法。', 'title_zh': '多源模型在地球观测中对缺失数据的鲁棒性依赖于什么？'}
{'arxiv_id': 'arXiv:2503.19717', 'title': 'Invertible Koopman neural operator for data-driven modeling of partial differential equations', 'authors': 'Yuhong Jin, Andong Cong, Lei Hou, Qiang Gao, Xiangdong Ge, Chonglong Zhu, Yongzhi Feng, Jun Li', 'link': 'https://arxiv.org/abs/2503.19717', 'abstract': "Koopman operator theory is a popular candidate for data-driven modeling because it provides a global linearization representation for nonlinear dynamical systems. However, existing Koopman operator-based methods suffer from shortcomings in constructing the well-behaved observable function and its inverse and are inefficient enough when dealing with partial differential equations (PDEs). To address these issues, this paper proposes the Invertible Koopman Neural Operator (IKNO), a novel data-driven modeling approach inspired by the Koopman operator theory and neural operator. IKNO leverages an Invertible Neural Network to parameterize observable function and its inverse simultaneously under the same learnable parameters, explicitly guaranteeing the reconstruction relation, thus eliminating the dependency on the reconstruction loss, which is an essential improvement over the original Koopman Neural Operator (KNO). The structured linear matrix inspired by the Koopman operator theory is parameterized to learn the evolution of observables' low-frequency modes in the frequency space rather than directly in the observable space, sustaining IKNO is resolution-invariant like other neural operators. Moreover, with preprocessing such as interpolation and dimension expansion, IKNO can be extended to operator learning tasks defined on non-Cartesian domains. We fully support the above claims based on rich numerical and real-world examples and demonstrate the effectiveness of IKNO and superiority over other neural operators.", 'abstract_zh': '可逆Koopman神经算子（IKNO）：一种基于Koopman算子理论和神经算子的新型数据驱动建模方法', 'title_zh': '基于数据驱动的偏微分方程建模的可逆考夫曼神经算子'}
{'arxiv_id': 'arXiv:2503.19712', 'title': 'Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions', 'authors': 'Sanghyuk Kim, Minsik Seo, Namwoo Kang', 'link': 'https://arxiv.org/abs/2503.19712', 'abstract': "This study proposes a neural framework that predicts 3D vehicle collision dynamics by independently modeling global rigid-body motion and local structural deformation. Unlike approaches directly predicting absolute displacement, this method explicitly separates the vehicle's overall translation and rotation from its structural deformation. Two specialized networks form the core of the framework: a quaternion-based Rigid Net for rigid motion and a coordinate-based Deformation Net for local deformation. By independently handling fundamentally distinct physical phenomena, the proposed architecture achieves accurate predictions without requiring separate supervision for each component. The model, trained on only 10% of available simulation data, significantly outperforms baseline models, including single multi-layer perceptron (MLP) and deep operator networks (DeepONet), with prediction errors reduced by up to 83%. Extensive validation demonstrates strong generalization to collision conditions outside the training range, accurately predicting responses even under severe impacts involving extreme velocities and large impact angles. Furthermore, the framework successfully reconstructs high-resolution deformation details from low-resolution inputs without increased computational effort. Consequently, the proposed approach provides an effective, computationally efficient method for rapid and reliable assessment of vehicle safety across complex collision scenarios, substantially reducing the required simulation data and time while preserving prediction fidelity.", 'abstract_zh': '本研究提出了一种神经框架，通过独立建模全局刚体运动和局部结构变形来预测3D车辆碰撞动力学。该方法不同于直接预测绝对位移的方法，明确地将车辆的整体平移和旋转与其结构变形区分开来。该框架的核心由两个专门网络构成：基于四元数的刚体网（Rigid Net）处理刚体运动，基于坐标的具体变形网（Deformation Net）处理局部变形。通过独立处理基本不同的物理现象，所提出的架构能够在不需要为每个组件提供单独监督的情况下实现准确的预测。该模型仅使用可用模拟数据的10%进行训练，比基线模型（包括单层多层感知机和深度算子网络）显著表现出更高的性能，预测误差降低高达83%。广泛的验证结果表明，该框架能够强烈泛化到训练范围外的碰撞条件，即使在极端速度和大撞击角度的严重碰撞中也能准确预测响应。此外，该框架能够从低分辨率输入重构高分辨率变形细节，而不增加计算成本。因此，所提出的方法为复杂碰撞场景中的车辆安全性快速可靠评估提供了一种有效且计算效率高的方法，显著减少了所需的模拟数据和时间，同时保持预测准确性。', 'title_zh': '基于神经场的解耦动力学框架在三维时空预测车辆碰撞中的应用'}
{'arxiv_id': 'arXiv:2503.19711', 'title': 'Writing as a testbed for open ended agents', 'authors': 'Sian Gooding, Lucia Lopez-Rivilla, Edward Grefenstette', 'link': 'https://arxiv.org/abs/2503.19711', 'abstract': 'Open-ended tasks are particularly challenging for LLMs due to the vast solution space, demanding both expansive exploration and adaptable strategies, especially when success lacks a clear, objective definition. Writing, with its vast solution space and subjective evaluation criteria, provides a compelling testbed for studying such problems. In this paper, we investigate the potential of LLMs to act as collaborative co-writers, capable of suggesting and implementing text improvements autonomously. We analyse three prominent LLMs - Gemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action diversity, human alignment, and iterative improvement capabilities impact overall performance. This work establishes a framework for benchmarking autonomous writing agents and, more broadly, highlights fundamental challenges and potential solutions for building systems capable of excelling in diverse open-ended domains.', 'abstract_zh': '开放性任务对大型语言模型构成挑战，由于其庞大的解空间，需要进行广泛探索和 adaptable 策略，尤其是在成功缺乏明确客观定义的情况下。由于其庞大的解空间和主观的评估标准，写作提供了研究此类问题的有效实验平台。在本文中，我们探讨了 LLMs 作为协作型合作者的潜力，能够自主建议并实施文本改进。我们分析了三个突出的 LLMs —— Gemini 1.5 Pro、Claude 3.5 Sonnet 和 GPT-4o，重点关注它们的行为多样性、人类齐心协力以及迭代改进能力如何影响整体性能。这项工作建立了自主写作代理的基准测试框架，并更广泛地突显了构建能够在多样化开放性领域表现出色的系统所面临的根本挑战和潜在解决方案。', 'title_zh': '开放型代理的试验bed：以写作为例'}
{'arxiv_id': 'arXiv:2503.19706', 'title': 'Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations', 'authors': 'Jungin Park, Jiyoung Lee, Kwanghoon Sohn', 'link': 'https://arxiv.org/abs/2503.19706', 'abstract': 'View-invariant representation learning from egocentric (first-person, ego) and exocentric (third-person, exo) videos is a promising approach toward generalizing video understanding systems across multiple viewpoints. However, this area has been underexplored due to the substantial differences in perspective, motion patterns, and context between ego and exo views. In this paper, we propose a novel masked ego-exo modeling that promotes both causal temporal dynamics and cross-view alignment, called Bootstrap Your Own Views (BYOV), for fine-grained view-invariant video representation learning from unpaired ego-exo videos. We highlight the importance of capturing the compositional nature of human actions as a basis for robust cross-view understanding. Specifically, self-view masking and cross-view masking predictions are designed to learn view-invariant and powerful representations concurrently. Experimental results demonstrate that our BYOV significantly surpasses existing approaches with notable gains across all metrics in four downstream ego-exo video tasks. The code is available at this https URL.', 'abstract_zh': '从第一人称（主观）和第三人称（客观）视频中学习观点不变的表示：一种Bootstrap Your Own Views（BYOV）方法', 'title_zh': '自我引导视点学习：遮蔽自我-环境建模的精细粒度视点不变视频表示'}
{'arxiv_id': 'arXiv:2503.19677', 'title': 'Deep Learning for Speech Emotion Recognition: A CNN Approach Utilizing Mel Spectrograms', 'authors': 'Niketa Penumajji', 'link': 'https://arxiv.org/abs/2503.19677', 'abstract': 'This paper explores the application of Convolutional Neural Networks CNNs for classifying emotions in speech through Mel Spectrogram representations of audio files. Traditional methods such as Gaussian Mixture Models and Hidden Markov Models have proven insufficient for practical deployment, prompting a shift towards deep learning techniques. By transforming audio data into a visual format, the CNN model autonomously learns to identify intricate patterns, enhancing classification accuracy. The developed model is integrated into a user-friendly graphical interface, facilitating realtime predictions and potential applications in educational environments. The study aims to advance the understanding of deep learning in speech emotion recognition, assess the models feasibility, and contribute to the integration of technology in learning contexts', 'abstract_zh': '本文探讨了通过梅尔频谱图表示的音频文件应用卷积神经网络CNN进行语音情感分类的应用。传统的高斯混合模型和隐马尔可夫模型已被证明不适合实际部署，推动了向深度学习技术的转变。通过将音频数据转换为可视化格式，CNN模型自主学习识别复杂的模式，从而提高分类准确性。开发的模型被集成到一个用户友好的图形界面中，便于实时预测，并有可能在教育环境中应用。本研究旨在推进对深度学习在语音情感识别中的理解，评估模型的可行性，并促进技术在学习环境中的集成。', 'title_zh': '基于CNN利用梅尔谱图的语音情感识别深度学习方法'}
{'arxiv_id': 'arXiv:2503.19658', 'title': 'BiblioPage: A Dataset of Scanned Title Pages for Bibliographic Metadata Extraction', 'authors': 'Jan Kohút, Martin Dočekal, Michal Hradiš, Marek Vaško', 'link': 'https://arxiv.org/abs/2503.19658', 'abstract': 'Manual digitization of bibliographic metadata is time consuming and labor intensive, especially for historical and real-world archives with highly variable formatting across documents. Despite advances in machine learning, the absence of dedicated datasets for metadata extraction hinders automation. To address this gap, we introduce BiblioPage, a dataset of scanned title pages annotated with structured bibliographic metadata. The dataset consists of approximately 2,000 monograph title pages collected from 14 Czech libraries, spanning a wide range of publication periods, typographic styles, and layout structures. Each title page is annotated with 16 bibliographic attributes, including title, contributors, and publication metadata, along with precise positional information in the form of bounding boxes. To extract structured information from this dataset, we valuated object detection models such as YOLO and DETR combined with transformer-based OCR, achieving a maximum mAP of 52 and an F1 score of 59. Additionally, we assess the performance of various visual large language models, including LlamA 3.2-Vision and GPT-4o, with the best model reaching an F1 score of 67. BiblioPage serves as a real-world benchmark for bibliographic metadata extraction, contributing to document understanding, document question answering, and document information extraction. Dataset and evaluation scripts are availible at: this https URL', 'abstract_zh': '手动数字化 bibliographic 元数据耗时且劳动密集，尤其是在具有高度变体格式的历史和现实档案中。尽管机器学习取得了进展，但由于缺乏专门的元数据提取数据集，自动化受到了阻碍。为解决这一问题，我们引入了 BiblioPage，一个包含扫描标题页并注释有结构化 bibliographic 元数据的数据集。该数据集包含来自 14 个捷克图书馆的约 2,000 个单行标题页，涵盖了广泛的不同出版时期、版式风格和布局结构。每个标题页都注释有 16 个 bibliographic 属性，包括书名、作者和其他出版元数据，以及以边界框形式的精确位置信息。为了从这个数据集中提取结构化信息，我们评估了YOLO和DETR对象检测模型与基于变换器的OCR相结合的方法，实现了最高mAP 52和F1分数 59。此外，我们评估了各种视觉大型语言模型，包括LlamA 3.2-Vision和GPT-4o，最佳模型的F1分数达到67。BiblioPage作为 bibliographic 元数据提取的实际 benchmarks，有助于文档理解、文档问答和文档信息提取。数据集和评估脚本可在以下链接获取：this https URL。', 'title_zh': 'BiblioPage: 一本涵盖扫描标题页的语料库，用于提取文献元数据'}
{'arxiv_id': 'arXiv:2503.19656', 'title': 'Towards Reliable Time Series Forecasting under Future Uncertainty: Ambiguity and Novelty Rejection Mechanisms', 'authors': 'Ninghui Feng, Songning Lai, Xin Zhou, Jiayu Yang, Kunlong Feng, Zhenxiao Yin, Fobao Zhou, Zhangyi Hu, Yutao Yue, Yuxuan Liang, Boyu Wang, Hang Zhao', 'link': 'https://arxiv.org/abs/2503.19656', 'abstract': 'In real-world time series forecasting, uncertainty and lack of reliable evaluation pose significant challenges. Notably, forecasting errors often arise from underfitting in-distribution data and failing to handle out-of-distribution inputs. To enhance model reliability, we introduce a dual rejection mechanism combining ambiguity and novelty rejection. Ambiguity rejection, using prediction error variance, allows the model to abstain under low confidence, assessed through historical error variance analysis without future ground truth. Novelty rejection, employing Variational Autoencoders and Mahalanobis distance, detects deviations from training data. This dual approach improves forecasting reliability in dynamic environments by reducing errors and adapting to data changes, advancing reliability in complex scenarios.', 'abstract_zh': '在实际时间序列预测中，不确定性与可靠评价的缺乏提出重大挑战。值得注意的是，预测误差通常源自于分布内数据的拟合不足以及无法处理分布外输入。为提升模型可靠性，我们引入一种结合模糊性和新颖性拒绝的双重拒绝机制。模糊性拒绝利用预测误差方差，使模型在低置信度下避免做出预测，通过历史误差方差分析而不依赖未来真实值。新颖性拒绝利用变分自编码器和马氏距离检测训练数据之外的偏差。这种双重方法通过减少错误和适应数据变化，在动态环境中提高预测可靠性，推进复杂场景下的可靠性提升。', 'title_zh': '面向未来不确定性的时间序列预测可靠性：含模糊性和新颖性拒绝机制'}
{'arxiv_id': 'arXiv:2503.19654', 'title': 'RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models', 'authors': 'Mehdi Moshtaghi, Siavash H. Khajavi, Joni Pajarinen', 'link': 'https://arxiv.org/abs/2503.19654', 'abstract': 'We introduce RGB-Th-Bench, the first benchmark designed to evaluate the ability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs. While VLMs have demonstrated remarkable progress in visual reasoning and multimodal understanding, their evaluation has been predominantly limited to RGB-based benchmarks, leaving a critical gap in assessing their capabilities in infrared vision tasks. Existing visible-infrared datasets are either task-specific or lack high-quality annotations necessary for rigorous model evaluation. To address these limitations, RGB-Th-Bench provides a comprehensive evaluation framework covering 14 distinct skill dimensions, with a total of 1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy metrics: a standard question-level accuracy and a stricter skill-level accuracy, which evaluates model robustness across multiple questions within each skill dimension. This design ensures a thorough assessment of model performance, including resilience to adversarial and hallucinated responses. We conduct extensive evaluations on 19 state-of-the-art VLMs, revealing significant performance gaps in RGB-Thermal understanding. Our results show that even the strongest models struggle with thermal image comprehension, with performance heavily constrained by their RGB-based capabilities. Additionally, the lack of large-scale application-specific and expert-annotated thermal-caption-pair datasets in pre-training is an important reason of the observed performance gap. RGB-Th-Bench highlights the urgent need for further advancements in multimodal learning to bridge the gap between visible and thermal image understanding. The dataset is available through this link, and the evaluation code will also be made publicly available.', 'abstract_zh': 'RGB-Th-Bench: 用于评估视觉语言模型理解RGB-热成像图像对能力的基准', 'title_zh': 'RGB-Th-Bench: 一种用于视觉语言模型的视觉-热成像理解密集基准'}
{'arxiv_id': 'arXiv:2503.19653', 'title': 'OpenSDI: Spotting Diffusion-Generated Images in the Open World', 'authors': 'Yabin Wang, Zhiwu Huang, Xiaopeng Hong', 'link': 'https://arxiv.org/abs/2503.19653', 'abstract': 'This paper identifies OpenSDI, a challenge for spotting diffusion-generated images in open-world settings. In response to this challenge, we define a new benchmark, the OpenSDI dataset (OpenSDID), which stands out from existing datasets due to its diverse use of large vision-language models that simulate open-world diffusion-based manipulations. Another outstanding feature of OpenSDID is its inclusion of both detection and localization tasks for images manipulated globally and locally by diffusion models. To address the OpenSDI challenge, we propose a Synergizing Pretrained Models (SPM) scheme to build up a mixture of foundation models. This approach exploits a collaboration mechanism with multiple pretrained foundation models to enhance generalization in the OpenSDI context, moving beyond traditional training by synergizing multiple pretrained models through prompting and attending strategies. Building on this scheme, we introduce MaskCLIP, an SPM-based model that aligns Contrastive Language-Image Pre-Training (CLIP) with Masked Autoencoder (MAE). Extensive evaluations on OpenSDID show that MaskCLIP significantly outperforms current state-of-the-art methods for the OpenSDI challenge, achieving remarkable relative improvements of 14.23% in IoU (14.11% in F1) and 2.05% in accuracy (2.38% in F1) compared to the second-best model in localization and detection tasks, respectively. Our dataset and code are available at this https URL.', 'abstract_zh': '本文识别了OpenSDI这一挑战，旨在发现开放世界设置中生成的图像。为应对这一挑战，我们定义了一个新的基准，即OpenSDI数据集（OpenSDID），该数据集因广泛采用了模拟开放世界生成性操纵的大规模视觉-语言模型而脱颖而出。OpenSDID的另一突出特点是包含了由扩散模型全局和局部操纵的图像的检测和定位任务。为应对OpenSDI挑战，我们提出了一种综合预训练模型（SPM）方案，构建基于多种基础模型的混合模型。该方法通过预训练模型之间的协作机制，促进了OpenSDI情境下的泛化能力，超越了传统的单一预训练模型训练方式，通过提示和注意策略实现了多种预训练模型的协同工作。在此基础上，我们引入了MaskCLIP模型，这是一种基于SPM的模型，将对比语言-图像预训练（CLIP）与掩码自编码器（MAE）相结合。在OpenSDID上的广泛评估表明，MaskCLIP在局部和全局操作任务中显著优于当前最先进的方法，分别在IoU（F1为14.11%）和精度（F1为2.38%）上实现了14.23%和2.05%的相对改进。我们的数据集和代码可在以下网址获取。', 'title_zh': 'OpenSDI: 在开放世界中识别扩散生成的图像'}
{'arxiv_id': 'arXiv:2503.19650', 'title': 'HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination Detection', 'authors': 'Maryam Bala, Amina Imam Abubakar, Abdulhamid Abubakar, Abdulkadir Shehu Bichi, Hafsa Kabir Ahmad, Sani Abdullahi Sani, Idris Abdulmumin, Shamsuddeen Hassan Muhamad, Ibrahim Said Ahmad', 'link': 'https://arxiv.org/abs/2503.19650', 'abstract': "This paper presents our findings of the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which focuses on identifying hallucinations and related overgeneration errors in large language models (LLMs). The shared task involves detecting specific text spans that constitute hallucinations in the outputs generated by LLMs in 14 languages. To address this task, we aim to provide a nuanced, model-aware understanding of hallucination occurrences and severity in English. We used natural language inference and fine-tuned a ModernBERT model using a synthetic dataset of 400 samples, achieving an Intersection over Union (IoU) score of 0.032 and a correlation score of 0.422. These results indicate a moderately positive correlation between the model's confidence scores and the actual presence of hallucinations. The IoU score indicates that our model has a relatively low overlap between the predicted hallucination span and the truth annotation. The performance is unsurprising, given the intricate nature of hallucination detection. Hallucinations often manifest subtly, relying on context, making pinpointing their exact boundaries formidable.", 'abstract_zh': '本论文介绍了多语言幻觉与相关过度生成错误共享任务（MU-SHROOM）的研究成果，重点在于识别大型语言模型（LLMs）输出中的幻觉及相关过度生成错误。共享任务涉及检测由14种语言的LLM生成输出中构成的幻觉的具体文本片段。为应对这一任务，我们旨在提供一种细腻且模型感知的幻觉发生及其严重程度的理解，特别是在英语中。我们使用自然语言推理，并使用包含400个样本的合成数据集 fine-tune 了一个 ModernBERT 模型，实现了 Intersection over Union（IoU）得分为0.032和相关系数得分为0.422。这些结果表明，模型的信心得分与实际存在的幻觉之间存在中等程度的正相关。IoU 分数表明，我们的模型在预测的幻觉片段与真实标注之间的重叠程度相对较低。考虑到幻觉检测的复杂性，这一性能结果是不 surprising 的。幻觉往往以微妙的方式表现出来，依赖于上下文，这使得准确界定其确切边界变得困难。', 'title_zh': 'HausaNLP在SemEval-2025任务3中的研究：细粒度模型感知幻觉检测'}
{'arxiv_id': 'arXiv:2503.19649', 'title': 'Recover from Horcrux: A Spectrogram Augmentation Method for Cardiac Feature Monitoring from Radar Signal Components', 'authors': 'Yuanyuan Zhang, Sijie Xiong, Rui Yang, EngGee Lim, Yutao Yue', 'link': 'https://arxiv.org/abs/2503.19649', 'abstract': 'Radar-based wellness monitoring is becoming an effective measurement to provide accurate vital signs in a contactless manner, but data scarcity retards the related research on deep-learning-based methods. Data augmentation is commonly used to enrich the dataset by modifying the existing data, but most augmentation techniques can only couple with classification tasks. To enable the augmentation for regression tasks, this research proposes a spectrogram augmentation method, Horcrux, for radar-based cardiac feature monitoring (e.g., heartbeat detection, electrocardiogram reconstruction) with both classification and regression tasks involved. The proposed method is designed to increase the diversity of input samples while the augmented spectrogram is still faithful to the original ground truth vital sign. In addition, Horcrux proposes to inject zero values in specific areas to enhance the awareness of the deep learning model on subtle cardiac features, improving the performance for the limited dataset. Experimental result shows that Horcrux achieves an overall improvement of 16.20% in cardiac monitoring and has the potential to be extended to other spectrogram-based tasks. The code will be released upon publication.', 'abstract_zh': '基于雷达的健康监测正成为一种有效的非接触式方法，用于提供准确的生命体征测量，但数据稀缺性限制了相关深度学习方法的研究。数据增强常用来通过修改现有数据来丰富数据集，但大多数增强技术只能与分类任务结合使用。为了使增强技术能够应用于回归任务，本研究提出了一种名为Horcrux的谱图增强方法，用于涉及分类和回归任务的雷达基心脏特征监测（如心搏检测、心电图重建）。所提出的方法旨在增加输入样本的多样性，同时增强的谱图仍忠于原始的真实生命体征。此外，Horcrux方法提出在特定区域注入零值，以增强深度学习模型对细微心脏特征的意识，从而改善有限数据集上的性能。实验结果表明，Horcrux在心脏监测上总体提高了16.20%，并有可能扩展到其他谱图基任务中。代码将在发布时公开。', 'title_zh': '从 horcrux 恢复：一种基于雷达信号成分的声谱图增强方法用于心脏特征监测'}
{'arxiv_id': 'arXiv:2503.19647', 'title': 'Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation', 'authors': 'Niccolo Avogaro, Thomas Frick, Mattia Rigotti, Andrea Bartezzaghi, Filip Janicki, Cristiano Malossi, Konrad Schindler, Roy Assaf', 'link': 'https://arxiv.org/abs/2503.19647', 'abstract': 'Large Vision-Language Models (VLMs) are increasingly being regarded as foundation models that can be instructed to solve diverse tasks by prompting, without task-specific training. We examine the seemingly obvious question: how to effectively prompt VLMs for semantic segmentation. To that end, we systematically evaluate the segmentation performance of several recent models guided by either text or visual prompts on the out-of-distribution MESS dataset collection. We introduce a scalable prompting scheme, few-shot prompted semantic segmentation, inspired by open-vocabulary segmentation and few-shot learning. It turns out that VLMs lag far behind specialist models trained for a specific segmentation task, by about 30% on average on the Intersection-over-Union metric. Moreover, we find that text prompts and visual prompts are complementary: each one of the two modes fails on many examples that the other one can solve. Our analysis suggests that being able to anticipate the most effective prompt modality can lead to a 11% improvement in performance. Motivated by our findings, we propose PromptMatcher, a remarkably simple training-free baseline that combines both text and visual prompts, achieving state-of-the-art results outperforming the best text-prompted VLM by 2.5%, and the top visual-prompted VLM by 3.5% on few-shot prompted semantic segmentation.', 'abstract_zh': '大规模视觉-语言模型（VLMs） increasingly被视为可以通过提示进行指令以解决多样任务的基础模型，而不需要针对特定任务进行训练。我们系统地评估了几个 recent 模型在 out-of-distribution MESS 数据集集合中的分割性能，这些模型要么由文本提示引导，要么由视觉提示引导。我们提出了一种可扩展的提示方案，少量 Shot 提示语义分割，受到开源词汇分割和少量 Shot 学习的启发。结果表明，VLMs 在交并比（Intersection-over-Union，IoU）度量标准上的表现比为特定分割任务训练的专业模型落后约 30%。此外，我们发现文本提示和视觉提示是互补的：两种模式中的每一种都会在其他模式可以解决的许多例子上失败。我们的分析表明，能够预见最有效的提示模式可以提高 11% 的性能。受我们发现的启发，我们提出了 PromptMatcher，这一极其简单的无训练基础模型结合了文本和视觉提示，在少量 Shot 提示语义分割中达到了最新成果，分别优于最佳文本提示的 VLM 和顶级视觉提示的 VLM 2.5% 和 3.5%。', 'title_zh': '说还是做？有效提示视觉语言模型进行语义分割'}
{'arxiv_id': 'arXiv:2503.19611', 'title': 'Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation', 'authors': 'Max W. Y. Lam, Yijin Xing, Weiya You, Jingcheng Wu, Zongyu Yin, Fuqiang Jiang, Hangyu Liu, Feng Liu, Xingda Li, Wei-Tsung Lu, Hanyu Chen, Tong Feng, Tianwei Zhao, Chien-Hung Liu, Xuchen Song, Yang Li, Yahui Zhou', 'link': 'https://arxiv.org/abs/2503.19611', 'abstract': 'Autoregressive (AR) models have demonstrated impressive capabilities in generating high-fidelity music. However, the conventional next-token prediction paradigm in AR models does not align with the human creative process in music composition, potentially compromising the musicality of generated samples. To overcome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT) prompting technique tailored for music generation. MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions. By leveraging the contrastive language-audio pretraining (CLAP) model, we establish a chain of "musical thoughts", making MusiCoT scalable and independent of human-labeled data, in contrast to conventional CoT methods. Moreover, MusiCoT allows for in-depth analysis of music structure, such as instrumental arrangements, and supports music referencing -- accepting variable-length audio inputs as optional style references. This innovative approach effectively addresses copying issues, positioning MusiCoT as a vital practical method for music prompting. Our experimental results indicate that MusiCoT consistently achieves superior performance across both objective and subjective metrics, producing music quality that rivals state-of-the-art generation models.\nOur samples are available at this https URL.', 'abstract_zh': '自回归（AR）模型在生成高保真音乐方面展现了出色的能力。然而，AR模型中传统的下一个token预测范式与音乐创作中的人类创意过程不一致，这可能会损害生成样本的音乐性。为克服这一限制，我们提出了MusiCoT，这是一种适用于音乐生成的新型链式思维（CoT）提示技术。MusiCoT使AR模型先概述整体音乐结构，再生成音频token，从而增强生成作品的一致性和创造性。通过利用对比语言-音频预训练（CLAP）模型，我们建立了一条“音乐思维链”，使其具有可扩展性和独立性，不同于传统的CoT方法。此外，MusiCoT支持对音乐结构的深入分析，如乐器编配，并支持音乐参考——可接受变长音频输入作为可选的风格参考。这一创新方法有效解决了复制问题，定位MusiCoT为音乐提示的重要实用方法。实验结果表明，MusiCoT在客观和主观指标上均表现出色，生成的音乐质量与最先进的生成模型不相上下。我们的样本可在此处获得。', 'title_zh': '可分析的音乐思维链提示生成高保真音乐'}
{'arxiv_id': 'arXiv:2503.19607', 'title': 'Enabling Rapid Shared Human-AI Mental Model Alignment via the After-Action Review', 'authors': 'Edward Gu, Ho Chit Siu, Melanie Platt, Isabelle Hurley, Jaime Peña, Rohan Paleja', 'link': 'https://arxiv.org/abs/2503.19607', 'abstract': "In this work, we present two novel contributions toward improving research in human-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and deployment of collaborative AI agents and 2) a tool to allow users to revisit and analyze behaviors within an HMT episode to facilitate shared mental model development. Our browser-based Minecraft testbed allows for rapid testing of collaborative agents in a continuous-space, real-time, partially-observable environment with real humans without cumbersome setup typical to human-AI interaction user studies. As Minecraft has an extensive player base and a rich ecosystem of pre-built AI agents, we hope this contribution can help to facilitate research quickly in the design of new collaborative agents and in understanding different human factors within HMT. Our mental model alignment tool facilitates user-led post-mission analysis by including video displays of first-person perspectives of the team members (i.e., the human and AI) that can be replayed, and a chat interface that leverages GPT-4 to provide answers to various queries regarding the AI's experiences and model details.", 'abstract_zh': '在本工作中，我们提出了两项关于提升人机团队合作（HMT）研究的新贡献：1) 一个基于浏览器的Minecraft测试床，用于加速协作AI代理的测试和部署；2) 一个工具，允许用户回顾和分析HMT情景中的行为，以促进共享心智模型的发展。我们的浏览器-based Minecraft测试床允许在真实的实时、部分可观测环境中快速测试协作代理，同时避免了传统的人机交互用户研究中繁琐的设置。由于Minecraft拥有广泛的玩家基础和丰富的预构建AI代理生态系统，我们希望此贡献能够加速新协作代理的设计研究，并理解HMT中的不同人因因素。我们的认知模型对齐工具通过提供团队成员（即人类和AI）的第一人称视角视频回放以及利用GPT-4的聊天界面来回答关于AI经历和模型细节的各种查询，从而促进用户主导的后任务分析。', 'title_zh': '通过行动回顾实现快速共享人机思维模型对齐'}
{'arxiv_id': 'arXiv:2503.19599', 'title': 'HoarePrompt: Structural Reasoning About Program Correctness in Natural Language', 'authors': 'Dimitrios Stamatios Bouras, Yihan Dai, Tairan Wang, Yingfei Xiong, Sergey Mechtaev', 'link': 'https://arxiv.org/abs/2503.19599', 'abstract': 'While software requirements are often expressed in natural language, verifying the correctness of a program against natural language requirements is a hard and underexplored problem. Large language models (LLMs) are promising candidates for addressing this challenge, however our experience shows that they are ineffective in this task, often failing to detect even straightforward bugs. To address this gap, we introduce HoarePrompt, a novel approach that adapts fundamental ideas from program analysis and verification to natural language artifacts. Drawing inspiration from the strongest postcondition calculus, HoarePrompt employs a systematic, step-by-step process in which an LLM generates natural language descriptions of reachable program states at various points in the code. To manage loops, we propose few-shot-driven k-induction, an adaptation of the k-induction method widely used in model checking. Once program states are described, HoarePrompt leverages the LLM to assess whether the program, annotated with these state descriptions, conforms to the natural language requirements. For evaluating the quality of classifiers of program correctness with respect to natural language requirements, we constructed CoCoClaNeL, a challenging dataset of solutions to programming competition problems. Our experiments show that HoarePrompt improves the MCC by 62% compared to directly using Zero-shot-CoT prompts for correctness classification. Furthermore, HoarePrompt outperforms a classifier that assesses correctness via LLM-based test generation by increasing the MCC by 93%. The inductive reasoning mechanism contributes a 28% boost to MCC, underscoring its effectiveness in managing loops.', 'abstract_zh': '虽然软件需求通常用自然语言表达，但验证程序是否满足自然语言需求是一个困难且未充分探索的问题。大规模语言模型（LLMs）是解决这一挑战的有前景候选者，然而我们的经验表明，在这项任务上它们往往是无效的，经常无法检测甚至是最简单的bug。为了填补这一空白，我们提出了HoarePrompt，这是一种新颖的方法，将程序分析和验证的基本思想应用于自然语言 artifacts。受最强后条件演算的启发，HoarePrompt采用了一种系统而逐步的过程，在此过程中，LLM生成代码中各个点可达程序状态的自然语言描述。为处理循环，我们提出了一种基于少量示例驱动的k-归纳法，这是广为应用于模型检查中的一种方法的适应。在描述了程序状态后，HoarePrompt利用LLM评估包含这些状态描述的程序是否符合自然语言需求。为了评估与自然语言需求相关的程序正确性分类器的质量，我们构建了CoCoClaNeL，这是一个具有挑战性的数据集，包含编程比赛问题的解决方案。实验结果显示，与直接使用Zero-shot-CoT提示进行正确性分类相比，HoarePrompt将MCC提高62%。此外，与通过LLM生成测试进行正确性评估的分类器相比，HoarePrompt将MCC提高了93%。归纳推理机制贡献了28%的MCC增加值，突显了其在处理循环方面的有效性。', 'title_zh': 'HoarePrompt: 在自然语言中关于程序正确性的结构化推理'}
{'arxiv_id': 'arXiv:2503.19585', 'title': 'A Contradiction-Centered Model for the Emergence of Swarm Intelligence', 'authors': 'Wenpin Jiao', 'link': 'https://arxiv.org/abs/2503.19585', 'abstract': 'The phenomenon of emergence of swarm intelligence exists widely in nature and human society. People have been exploring the root cause of emergence of swarm intelligence and trying to establish general theories and models for emergence of swarm intelligence. However, the existing theories or models do not grasp the essence of swarm intelligence, so they lack generality and are difficult to explain various phenomena of emergence of swarm intelligence. In this paper, a contradiction-centered model for the emergence of swarm intelligence is proposed, in which the internal contradictions of individuals determine their behavior and properties, individuals are related and interact within the swarm because of competing and occupying environmental resources, interactions and swarm potential affect the internal contradictions of individuals and their distribution in the swarm, and the swarm intelligence is manifested as the specific distribution of individual contradictions. This model completely explains the conditions, dynamics, pathways, formations and processes of the emergence of swarm intelligence. In order to verify the validity of this model, several swarm intelligence systems are implemented and analyzed in this paper. The experimental results show that the model has good generality and can be used to describe the emergence of various swarm intelligence.', 'abstract_zh': '群体智能涌现现象中心矛盾模型', 'title_zh': '基于矛盾中心的群体智能涌现模型'}
{'arxiv_id': 'arXiv:2503.19564', 'title': 'FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments', 'authors': 'Sree Bhargavi Balija', 'link': 'https://arxiv.org/abs/2503.19564', 'abstract': 'As artificial intelligence systems increasingly operate in Real-world environments, the integration of multi-modal data sources such as vision, language, and audio presents both unprecedented opportunities and critical challenges for achieving trustworthy intelligence. In this paper, we propose a novel framework that unifies federated learning with explainable multi-modal reasoning to ensure trustworthiness in decentralized, dynamic settings. Our approach, called FedMM-X (Federated Multi-Modal Explainable Intelligence), leverages cross-modal consistency checks, client-level interpretability mechanisms, and dynamic trust calibration to address challenges posed by data heterogeneity, modality imbalance, and out-of-distribution generalization. Through rigorous evaluation across federated multi-modal benchmarks involving vision-language tasks, we demonstrate improved performance in both accuracy and interpretability while reducing vulnerabilities to adversarial and spurious correlations. Further, we introduce a novel trust score aggregation method to quantify global model reliability under dynamic client participation. Our findings pave the way toward developing robust, interpretable, and socially responsible AI systems in Real-world environments.', 'abstract_zh': '随着人工智能系统越来越多地在实际环境运行，多模态数据源如视觉、语言和音频的集成为实现可信智能带来了前所未有的机遇和关键挑战。本文提出了一种新的框架，将联邦学习与可解释多模态推理相结合，以确保在去中心化、动态设置中的可信性。我们的方法称为FedMM-X（联邦多模态解释智能），利用跨模态一致性检查、客户端级可解释性机制和动态信任校准，以应对数据异构性、模态失衡和域外泛化的挑战。通过在涉及视觉-语言任务的联邦多模态基准中的严格评估，我们展示了在准确性和可解释性上均有改进，并降低了对对抗性和误分类关系的脆弱性。此外，我们引入了一种新的信任分值聚合方法，以量化动态客户端参与下的全局模型可靠性。我们的研究为在实际环境开发稳健、可解释和负责任的人工智能系统铺平了道路。', 'title_zh': 'FedMM-X：动态环境中文本可信任可解释的联邦多模态学习框架'}
{'arxiv_id': 'arXiv:2503.19551', 'title': 'Scaling Laws of Synthetic Data for Language Models', 'authors': 'Zeyu Qin, Qingxiu Dong, Xingxing Zhang, Li Dong, Xiaolong Huang, Ziyi Yang, Mahmoud Khademi, Dongdong Zhang, Hany Hassan Awadalla, Yi R. Fung, Weizhu Chen, Minhao Cheng, Furu Wei', 'link': 'https://arxiv.org/abs/2503.19551', 'abstract': 'Large language models (LLMs) achieve strong performance across diverse tasks, largely driven by high-quality web data used in pre-training. However, recent studies indicate this data source is rapidly depleting. Synthetic data emerges as a promising alternative, but it remains unclear whether synthetic datasets exhibit predictable scalability comparable to raw pre-training data. In this work, we systematically investigate the scaling laws of synthetic data by introducing SynthLLM, a scalable framework that transforms pre-training corpora into diverse, high-quality synthetic datasets. Our approach achieves this by automatically extracting and recombining high-level concepts across multiple documents using a graph algorithm. Key findings from our extensive mathematical experiments on SynthLLM include: (1) SynthLLM generates synthetic data that reliably adheres to the \\emph{rectified scaling law} across various model sizes; (2) Performance improvements plateau near 300B tokens; and (3) Larger models approach optimal performance with fewer training tokens. For instance, an 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons with existing synthetic data generation and augmentation methods demonstrate that SynthLLM achieves superior performance and scalability. Our findings highlight synthetic data as a scalable and reliable alternative to organic pre-training corpora, offering a viable path toward continued improvement in model performance.', 'abstract_zh': '大型语言模型（LLMs）通过使用高质量的网页数据在预训练中实现了多样化任务的强大性能。然而，近期研究指出现有数据源正迅速枯竭。合成数据作为一种有前景的替代方案出现，但仍不清楚合成数据集是否能像原始预训练数据一样展现出可预测的大规模可扩展性。在本工作中，我们通过引入SynthLLM，一个可扩展的框架，将预训练语料库转换为多样化且高质量的合成数据集，系统地研究了合成数据的可扩展性规律。我们的方法通过使用图算法自动提取和重组多个文档中的高级概念来实现这一点。通过对SynthLLM的大量数学实验，我们得出的关键发现包括：（1）SynthLLM生成的合成数据在不同模型规模下可靠地遵循修正的可扩展性定律；（2）性能改进在约3000亿个标记处达到 Plateau；（3）更大模型在更少的训练标记下接近最佳性能。例如，一个8B模型在1T标记处达到峰值，而一个3B模型则需要4T标记。此外，与现有的合成数据生成和扩充方法的比较表明，SynthLLM在性能和可扩展性方面表现出色。我们的研究结果强调，合成数据作为一种可扩展且可靠的替代方案，在预训练有机语料库之外，提供了继续改进模型性能的可行途径。', 'title_zh': '合成数据规模律对语言模型的影响'}
{'arxiv_id': 'arXiv:2503.19540', 'title': 'FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models', 'authors': 'Dahyun Jung, Seungyoon Lee, Hyeonseok Moon, Chanjun Park, Heuiseok Lim', 'link': 'https://arxiv.org/abs/2503.19540', 'abstract': 'Recent advancements in Large Language Models (LLMs) have significantly enhanced interactions between users and models. These advancements concurrently underscore the need for rigorous safety evaluations due to the manifestation of social biases, which can lead to harmful societal impacts. Despite these concerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs, which can generate biased responses even with simple adversarial instructions. To address this critical gap, we introduce a new benchmark, Fairness Benchmark in LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can sustain fairness even when exposed to prompts constructed to induce bias. To thoroughly evaluate the robustness of LLMs, we integrate prompts that amplify potential biases into the fairness assessment. Comparative experiments between FLEX and existing benchmarks demonstrate that traditional evaluations may underestimate the inherent risks in models. This highlights the need for more stringent LLM evaluation benchmarks to guarantee safety and fairness.', 'abstract_zh': 'Recent Advancements in Large Language Models (LLMs) Have Significantly Enhanced User-Model Interactions but Also Highlight the Need for Rigorous Safety Evaluations, Especially in the Context of FLEX, a New Benchmark for Testing Fairness Under Extreme Scenarios', 'title_zh': 'FLEX：评估大型语言模型公平性鲁棒性的基准'}
{'arxiv_id': 'arXiv:2503.19530', 'title': 'VectorFit : Adaptive Singular & Bias Vector Fine-Tuning of Pre-trained Foundation Models', 'authors': 'Suhas G Hegde, Shilpy Kaur, Aruna Tiwari', 'link': 'https://arxiv.org/abs/2503.19530', 'abstract': 'Popular PEFT methods achieve parameter efficiency by assuming that incremental weight updates are inherently low-rank, which often leads to a performance gap compared to full fine-tuning. While recent methods have attempted to address this limitation, they typically lack sufficient parameter and memory efficiency. We propose VectorFit, an effective and easily deployable approach that adaptively trains the singular vectors and biases of pre-trained weight matrices. We demonstrate that the utilization of structural and transformational characteristics of pre-trained weights enables high-rank updates comparable to those of full fine-tuning. As a result, VectorFit achieves superior performance with 9X less trainable parameters compared to state-of-the-art PEFT methods. Through extensive experiments over 17 datasets spanning diverse language and vision tasks such as natural language understanding and generation, question answering, image classification, and image generation, we exhibit that VectorFit consistently outperforms baselines, even in extremely low-budget scenarios.', 'abstract_zh': 'VectorFit：一种有效的可适应训练预训练权重奇异向量和偏置的方法', 'title_zh': 'VectorFit：自适应奇异矢量与偏差矢量微调的预训练基础模型调优'}
{'arxiv_id': 'arXiv:2503.19510', 'title': 'RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation', 'authors': 'Sheng Wang', 'link': 'https://arxiv.org/abs/2503.19510', 'abstract': 'As robotic technologies advancing towards more complex multimodal interactions and manipulation tasks, the integration of advanced Vision-Language Models (VLMs) has become a key driver in the field. Despite progress with current methods, challenges persist in fusing depth and RGB information within 3D environments and executing tasks guided by linguistic instructions. In response to these challenges, we have enhanced the existing RoboFlamingo framework by introducing RoboFlamingo-Plus, which incorporates depth data into VLMs to significantly improve robotic manipulation performance. Our research achieves a nuanced fusion of RGB and depth information by integrating a pre-trained Vision Transformer (ViT) with a resampling technique, closely aligning this combined data with linguistic cues for superior multimodal understanding. The novelty of RoboFlamingo-Plus lies in its adaptation of inputs for depth data processing, leveraging a pre-trained resampler for depth feature extraction, and employing cross-attention mechanisms for optimal feature integration. These improvements allow RoboFlamingo-Plus to not only deeply understand 3D environments but also easily perform complex, language-guided tasks in challenging settings. Experimental results show that RoboFlamingo-Plus boosts robotic manipulation by 10-20% over current methods, marking a significant advancement. Codes and model weights are public at RoboFlamingo-Plus.', 'abstract_zh': '随着机器人技术向更复杂的多模态交互和操作任务发展，先进的视觉-语言模型（VLMs）的集成已成为关键驱动力。尽管当前方法取得了一定进展，但在3D环境中融合深度和RGB信息以及执行语言指导的任务仍面临挑战。为应对这些挑战，我们通过引入RoboFlamingo-Plus框架增强了现有的RoboFlamingo框架，将深度数据融入VLMs，显著提高了机器人操作性能。我们的研究通过将预训练的视觉变换器（ViT）与重采样技术相结合，实现了RGB和深度信息的精细融合，使这种结合数据更加紧密地与语言提示对齐，从而实现更优越的多模态理解。RoboFlamingo-Plus的创新之处在于其适应深度数据处理输入的方法，利用预训练的重采样器进行深度特征提取，并采用交叉注意力机制进行最佳特征整合。这些改进使RoboFlamingo-Plus不仅能深刻理解3D环境，还能在复杂环境中轻松执行语言指导的任务。实验结果显示，与当前方法相比，RoboFlamingo-Plus在机器人操作性能上提升了10-20%，标志着一个重要的进步。RoboFlamingo-Plus的代码和模型权重已公开。', 'title_zh': 'RoboFlamingo-Plus：视觉语言模型融合深度和RGB感知以增强机器人操作'}
{'arxiv_id': 'arXiv:2503.19502', 'title': 'Towards Long-Range ENSO Prediction with an Explainable Deep Learning Model', 'authors': 'Qi Chen, Yinghao Cui, Guobin Hong, Karumuri Ashok, Yuchun Pu, Xiaogu Zheng, Xuanze Zhang, Wei Zhong, Peng Zhan, Zhonglei Wang', 'link': 'https://arxiv.org/abs/2503.19502', 'abstract': "El Niño-Southern Oscillation (ENSO) is a prominent mode of interannual climate variability with far-reaching global impacts. Its evolution is governed by intricate air-sea interactions, posing significant challenges for long-term prediction. In this study, we introduce CTEFNet, a multivariate deep learning model that synergizes convolutional neural networks and transformers to enhance ENSO forecasting. By integrating multiple oceanic and atmospheric predictors, CTEFNet extends the effective forecast lead time to 20 months while mitigating the impact of the spring predictability barrier, outperforming both dynamical models and state-of-the-art deep learning approaches. Furthermore, CTEFNet offers physically meaningful and statistically significant insights through gradient-based sensitivity analysis, revealing the key precursor signals that govern ENSO dynamics, which align with well-established theories and reveal new insights about inter-basin interactions among the Pacific, Atlantic, and Indian Oceans. The CTEFNet's superior predictive skill and interpretable sensitivity assessments underscore its potential for advancing climate prediction. Our findings highlight the importance of multivariate coupling in ENSO evolution and demonstrate the promise of deep learning in capturing complex climate dynamics with enhanced interpretability.", 'abstract_zh': 'El Niño-Southern Oscillation (ENSO)是一种影响深远的年际气候变异性模式，其演变受复杂的空气-海洋相互作用控制，给长期预测带来重大挑战。在本研究中，我们引入了CTEFNet，这是一种结合卷积神经网络和-transformers的多变量深度学习模型，以增强ENSO预测。通过整合多种海洋和大气预测因子，CTEFNet延长了有效的预报提前期至20个月，减轻了春季可预报性障碍的影响，优于动力模型和现有的深度学习方法。此外，CTEFNet通过基于梯度的敏感性分析提供物理上合理和统计上显著的洞见，揭示了控制ENSO动力学的关键先行信号，这些信号与已建立的理论一致，并揭示了太平洋、大西洋和印度洋之间交互作用的新见解。CTEFNet卓越的预测能力和可解释的敏感性评估证明了其在推进气候预测方面的潜力。我们的研究强调了ENSO演变中多变量耦合的重要性，并展示了深度学习在具有增强解释性的复杂气候动力学建模方面的前景。', 'title_zh': '基于可解释深度学习模型的长-range ENSO预测研究'}
{'arxiv_id': 'arXiv:2503.19501', 'title': 'Pose-Based Fall Detection System: Efficient Monitoring on Standard CPUs', 'authors': 'Vinayak Mali, Saurabh Jaiswal', 'link': 'https://arxiv.org/abs/2503.19501', 'abstract': 'Falls among elderly residents in assisted living homes pose significant health risks, often leading to injuries and a decreased quality of life. Current fall detection solutions typically rely on sensor-based systems that require dedicated hardware, or on video-based models that demand high computational resources and GPUs for real-time processing. In contrast, this paper presents a robust fall detection system that does not require any additional sensors or high-powered hardware. The system uses pose estimation techniques, combined with threshold-based analysis and a voting mechanism, to effectively distinguish between fall and non-fall activities. For pose detection, we leverage MediaPipe, a lightweight and efficient framework that enables real-time processing on standard CPUs with minimal computational overhead. By analyzing motion, body position, and key pose points, the system processes pose features with a 20-frame buffer, minimizing false positives and maintaining high accuracy even in real-world settings. This unobtrusive, resource-efficient approach provides a practical solution for enhancing resident safety in old age homes, without the need for expensive sensors or high-end computational resources.', 'abstract_zh': '入住辅助生活设施的老年人跌倒对其健康构成了显著风险，常导致受伤并降低生活质量。当前的跌倒检测解决方案通常依赖于需要专用硬件的传感器系统，或依赖于需要大量计算资源和GPU进行实时处理的视频模型。与此相反，本文提出了一种无需额外传感器或高性能硬件的稳健跌倒检测系统。该系统结合了姿态估计技术、阈值分析和投票机制，有效地区分跌倒和非跌倒活动。在姿态检测方面，我们利用MediaPipe这一轻量级且高效的框架，能够在标准CPU上进行实时处理，并且具有最小的计算开销。通过分析运动、身体位置和关键姿态点，系统使用20帧缓冲区处理姿态特征，减少了误报，即使在实际环境中也能保持高准确性。这一不显眼、资源高效的方案为提高养老设施居民的安全性提供了实用解决方法，无需昂贵的传感器或高性能计算资源。', 'title_zh': '基于姿态的跌倒检测系统：高效的标准CPU监测'}
{'arxiv_id': 'arXiv:2503.19496', 'title': 'SMT-EX: An Explainable Surrogate Modeling Toolbox for Mixed-Variables Design Exploration', 'authors': 'Mohammad Daffa Robani, Paul Saves, Pramudita Satria Palar, Lavi Rizki Zuhal, oseph Morlier', 'link': 'https://arxiv.org/abs/2503.19496', 'abstract': 'Surrogate models are of high interest for many engineering applications, serving as cheap-to-evaluate time-efficient approximations of black-box functions to help engineers and practitioners make decisions and understand complex systems. As such, the need for explainability methods is rising and many studies have been performed to facilitate knowledge discovery from surrogate models. To respond to these enquiries, this paper introduces SMT-EX, an enhancement of the open-source Python Surrogate Modeling Toolbox (SMT) that integrates explainability techniques into a state-of-the-art surrogate modelling framework. More precisely, SMT-EX includes three key explainability methods: Shapley Additive Explanations, Partial Dependence Plot, and Individual Conditional Expectations. A peculiar explainability dependency of SMT has been developed for such purpose that can be easily activated once the surrogate model is built, offering a user-friendly and efficient tool for swift insight extraction. The effectiveness of SMT-EX is showcased through two test cases. The first case is a 10-variable wing weight problem with purely continuous variables and the second one is a 3-variable mixed-categorical cantilever beam bending problem. Relying on SMT-EX analyses for these problems, we demonstrate its versatility in addressing a diverse range of problem characteristics. SMT-Explainability is freely available on Github: this https URL .', 'abstract_zh': '代理模型在许多工程应用中备受关注，作为黑箱函数的经济高效的近似模型，有助于工程师和实践者做出决策并理解复杂系统。因此，解释方法的需求日益增加，许多研究已经开展以促进从代理模型中发现知识。为应对这些需求，本文介绍了SMT-EX，这是一种增强的开源Python代理建模工具箱（SMT），其集成了一流的代理建模框架中的解释技术。具体而言，SMT-EX 包括三种关键的解释方法：Shapley 加权解释、部分依赖图和个体条件期望。为此目的而开发了一种特有的解释依赖性，可以在构建代理模型后轻松激活，提供了一个用户友好且高效的工具，用于快速获取洞察。通过两个案例展示了SMT-EX的有效性。第一个案例是具有纯连续变量的10变量机翼重量问题，第二个案例是具有混合分类变量的3变量悬臂梁弯折问题。借助SMT-EX 对这些问题的分析，我们展示了其解决各种问题特征的灵活性。SMT-EX 解释模块可在Github上免费获取：this https URL。', 'title_zh': 'SMT-EX：一种用于混合变量设计探索的可解释代理模型工具箱'}
{'arxiv_id': 'arXiv:2503.19474', 'title': 'A-MESS: Anchor based Multimodal Embedding with Semantic Synchronization for Multimodal Intent Recognition', 'authors': 'Yaomin Shen, Xiaojian Lin, Wei Fan', 'link': 'https://arxiv.org/abs/2503.19474', 'abstract': 'In the domain of multimodal intent recognition (MIR), the objective is to recognize human intent by integrating a variety of modalities, such as language text, body gestures, and tones. However, existing approaches face difficulties adequately capturing the intrinsic connections between the modalities and overlooking the corresponding semantic representations of intent. To address these limitations, we present the Anchor-based Mul- timodal Embedding with Semantic Synchronization (A-MESS) framework. We first design an Anchor-based Multimodal Embed- ding (A-ME) module that employs an anchor-based embedding fusion mechanism to integrate multimodal inputs. Furthermore, we develop a Semantic Synchronization (SS) strategy with the Triplet Contrastive Learning pipeline, which optimizes the pro- cess by synchronizing multimodal representation with label de- scriptions produced by the large language model. Comprehensive experiments indicate that our A-MESS achieves state-of-the-art and provides substantial insight into multimodal representation and downstream tasks.', 'abstract_zh': '基于锚点的多模态嵌入与语义同步框架（A-MESS）', 'title_zh': 'A-MESS：基于锚点的多模态嵌入与语义同步的多模态意图识别'}
{'arxiv_id': 'arXiv:2503.19469', 'title': 'Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning', 'authors': 'Fred Philippy, Siwen Guo, Cedric Lothritz, Jacques Klein, Tegawendé F. Bissyandé', 'link': 'https://arxiv.org/abs/2503.19469', 'abstract': 'In NLP, Zero-Shot Classification (ZSC) has become essential for enabling models to classify text into categories unseen during training, particularly in low-resource languages and domains where labeled data is scarce. While pretrained language models (PLMs) have shown promise in ZSC, they often rely on large training datasets or external knowledge, limiting their applicability in multilingual and low-resource scenarios. Recent approaches leveraging natural language prompts reduce the dependence on large training datasets but struggle to effectively incorporate available labeled data from related classification tasks, especially when these datasets originate from different languages or distributions. Moreover, existing prompt-based methods typically rely on manually crafted prompts in a specific language, limiting their adaptability and effectiveness in cross-lingual settings. To address these challenges, we introduce RoSPrompt, a lightweight and data-efficient approach for training soft prompts that enhance cross-lingual ZSC while ensuring robust generalization across data distribution shifts. RoSPrompt is designed for small multilingual PLMs, enabling them to leverage high-resource languages to improve performance in low-resource settings without requiring extensive fine-tuning or high computational costs. We evaluate our approach on multiple multilingual PLMs across datasets covering 106 languages, demonstrating strong cross-lingual transfer performance and robust generalization capabilities over unseen classes.', 'abstract_zh': '零样本分类：面向低资源语言和领域的轻量级和数据高效方法', 'title_zh': '增强小型语言模型在软提示调优下的跨语言泛化零样本分类能力'}
{'arxiv_id': 'arXiv:2503.19455', 'title': 'Data-centric Federated Graph Learning with Large Language Models', 'authors': 'Bo Yan, Zhongjian Zhang, Huabin Sun, Mengmei Zhang, Yang Cao, Chuan Shi', 'link': 'https://arxiv.org/abs/2503.19455', 'abstract': 'In federated graph learning (FGL), a complete graph is divided into multiple subgraphs stored in each client due to privacy concerns, and all clients jointly train a global graph model by only transmitting model parameters. A pain point of FGL is the heterogeneity problem, where nodes or structures present non-IID properties among clients (e.g., different node label distributions), dramatically undermining the convergence and performance of FGL. To address this, existing efforts focus on design strategies at the model level, i.e., they design models to extract common knowledge to mitigate heterogeneity. However, these model-level strategies fail to fundamentally address the heterogeneity problem as the model needs to be designed from scratch when transferring to other tasks. Motivated by large language models (LLMs) having achieved remarkable success, we aim to utilize LLMs to fully understand and augment local text-attributed graphs, to address data heterogeneity at the data level. In this paper, we propose a general framework LLM4FGL that innovatively decomposes the task of LLM for FGL into two sub-tasks theoretically. Specifically, for each client, it first utilizes the LLM to generate missing neighbors and then infers connections between generated nodes and raw nodes. To improve the quality of generated nodes, we design a novel federated generation-and-reflection mechanism for LLMs, without the need to modify the parameters of the LLM but relying solely on the collective feedback from all clients. After neighbor generation, all the clients utilize a pre-trained edge predictor to infer the missing edges. Furthermore, our framework can seamlessly integrate as a plug-in with existing FGL methods. Experiments on three real-world datasets demonstrate the superiority of our method compared to advanced baselines.', 'abstract_zh': '基于大型语言模型的联邦图学习：LLM4FGL', 'title_zh': '基于数据的联邦图学习与大规模语言模型'}
{'arxiv_id': 'arXiv:2503.19449', 'title': 'VecTrans: LLM Transformation Framework for Better Auto-vectorization on High-performance CPU', 'authors': 'Zhongchun Zheng, Long Cheng, Lu Li, Rodrigo C. O. Rocha, Tianyi Liu, Wei Wei, Xianwei Zhang, Yaoqing Gao', 'link': 'https://arxiv.org/abs/2503.19449', 'abstract': "Large language models (LLMs) have demonstrated great capabilities in code generation, yet their effective application in compiler optimizations remains an open challenge due to issues such as hallucinations and a lack of domain-specific reasoning. Vectorization, a crucial optimization for enhancing code performance, often fails because of the compiler's inability to recognize complex code patterns, which commonly require extensive empirical expertise. LLMs, with their ability to capture intricate patterns, thus providing a promising solution to this challenge. This paper presents VecTrans, a novel framework that leverages LLMs to enhance compiler-based code vectorization. VecTrans first employs compiler analysis to identify potentially vectorizable code regions. It then utilizes an LLM to refactor these regions into patterns that are more amenable to the compiler's auto-vectorization. To ensure semantic correctness, VecTrans further integrates a hybrid validation mechanism at the intermediate representation (IR) level. With the above efforts, VecTrans combines the adaptability of LLMs with the precision of compiler vectorization, thereby effectively opening up the vectorization opportunities. Experimental results show that among all 50 TSVC functions unvectorizable by Clang, GCC, and BiShengCompiler, VecTrans successfully vectorizes 23 cases (46%) and achieves an average speedup of 2.02x, greatly surpassing state-of-the-art performance.", 'abstract_zh': '大型语言模型（LLMs）在代码生成方面展现了强大的能力，但在编译器优化中的有效应用由于幻觉问题和缺乏领域特定推理仍是一个开放的挑战。向量化作为提升代码性能的关键优化往往因为编译器无法识别复杂的代码模式而失败，这些模式通常需要大量的经验知识。LLMs 由于能够捕捉复杂的模式，因此为解决这一挑战提供了有前景的解决方案。本文提出了一种名为VecTrans的新框架，利用LLMs增强基于编译器的代码向量化。VecTrans首先使用编译器分析来识别可向量化的目标代码区域。然后利用LLMs将这些区域重构为更符合编译器自动向量化需求的模式。为确保语义正确性，VecTrans进一步在中间表示（IR）级别整合了一种混合验证机制。通过以上努力，VecTrans将LLMs的适应性与编译器向量化精度相结合，从而有效开启了向量化的机会。实验结果表明，在由Clang、GCC和BiShengCompiler无法向量化的50个TSVC函数中，VecTrans成功向量化了23个案例（46%），并实现了2.02倍的平均加速，大大超过了现有最先进的性能。', 'title_zh': 'VecTrans: 高性能CPU上更好的自动向量化LLM转化框架'}
{'arxiv_id': 'arXiv:2503.19426', 'title': 'DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models', 'authors': 'Suyoung Bae, YunSeok Choi, Jee-Hyong Lee', 'link': 'https://arxiv.org/abs/2503.19426', 'abstract': "While Large Language Models (LLMs) excel in zero-shot Question Answering (QA), they tend to expose biases in their internal knowledge when faced with socially sensitive questions, leading to a degradation in performance. Existing zero-shot methods are efficient but fail to consider context and prevent bias propagation in the answers. To address this, we propose DeCAP, a method for debiasing LLMs using Context-Adaptive Prompt Generation. DeCAP leverages a Question Ambiguity Detection to take appropriate debiasing actions based on the context and a Neutral Answer Guidance Generation to suppress the LLMs make objective judgments about the context, minimizing the propagation of bias from their internal knowledge. Our various experiments across eight LLMs show that DeCAP achieves state-of-the-art zero-shot debiased QA performance. This demonstrates DeCAP's efficacy in enhancing the fairness and accuracy of LLMs in diverse QA settings.", 'abstract_zh': '借助上下文自适应提示生成去偏见大型语言模型的方法：DeCAP在零样本问答中的应用', 'title_zh': 'DeCAP：面向大型语言模型零-shot问答去偏见的上下文自适应提示生成'}
{'arxiv_id': 'arXiv:2503.19394', 'title': 'Quantifying Symptom Causality in Clinical Decision Making: An Exploration Using CausaLM', 'authors': 'Mehul Shetty, Connor Jordan', 'link': 'https://arxiv.org/abs/2503.19394', 'abstract': 'Current machine learning approaches to medical diagnosis often rely on correlational patterns between symptoms and diseases, risking misdiagnoses when symptoms are ambiguous or common across multiple conditions. In this work, we move beyond correlation to investigate the causal influence of key symptoms-specifically "chest pain" on diagnostic predictions. Leveraging the CausaLM framework, we generate counterfactual text representations in which target concepts are effectively "forgotten" enabling a principled estimation of the causal effect of that concept on a model\'s predicted disease distribution. By employing Textual Representation-based Average Treatment Effect (TReATE), we quantify how the presence or absence of a symptom shapes the model\'s diagnostic outcomes, and contrast these findings against correlation-based baselines such as CONEXP. Our results offer deeper insight into the decision-making behavior of clinical NLP models and have the potential to inform more trustworthy, interpretable, and causally-grounded decision support tools in medical practice.', 'abstract_zh': '当前医学诊断中的机器学习方法通常依赖于症状与疾病的关联模式，在症状模糊或跨多种条件普遍时存在误诊风险。本研究超越关联性，探讨关键症状（特别是“胸痛”）对诊断预测的因果影响。利用CausaLM框架生成包含目标概念被有效“遗忘”的反事实文本表示，从而对模型预测疾病分布中的因果效应进行原则性估计。通过使用基于文本表示的平均治疗效果（TReATE）方法，我们量化症状的存在或缺失是如何塑造模型诊断结果的，并将这些发现与基于关联性的基准方法（如CONEXP）进行对比。我们的结果为进一步理解临床NLP模型的决策行为提供了深刻的见解，并有可能为医学实践中更可靠、可解释和基于因果关系的决策支持工具提供指导。', 'title_zh': '在临床决策中量化症状因果关系：基于CausaLM的探索'}
{'arxiv_id': 'arXiv:2503.19382', 'title': 'Causal invariant geographic network representations with feature and structural distribution shifts', 'authors': 'Yuhan Wang, Silu He, Qinyao Luo, Hongyuan Yuan, Ling Zhao, Jiawei Zhu, Haifeng Li', 'link': 'https://arxiv.org/abs/2503.19382', 'abstract': 'The existing methods learn geographic network representations through deep graph neural networks (GNNs) based on the i.i.d. assumption. However, the spatial heterogeneity and temporal dynamics of geographic data make the out-of-distribution (OOD) generalisation problem particularly salient. The latter are particularly sensitive to distribution shifts (feature and structural shifts) between testing and training data and are the main causes of the OOD generalisation problem. Spurious correlations are present between invariant and background representations due to selection biases and environmental effects, resulting in the model extremes being more likely to learn background representations. The existing approaches focus on background representation changes that are determined by shifts in the feature distributions of nodes in the training and test data while ignoring changes in the proportional distributions of heterogeneous and homogeneous neighbour nodes, which we refer to as structural distribution shifts. We propose a feature-structure mixed invariant representation learning (FSM-IRL) model that accounts for both feature distribution shifts and structural distribution shifts. To address structural distribution shifts, we introduce a sampling method based on causal attention, encouraging the model to identify nodes possessing strong causal relationships with labels or nodes that are more similar to the target node. Inspired by the Hilbert-Schmidt independence criterion, we implement a reweighting strategy to maximise the orthogonality of the node representations, thereby mitigating the spurious correlations among the node representations and suppressing the learning of background representations. Our experiments demonstrate that FSM-IRL exhibits strong learning capabilities on both geographic and social network datasets in OOD scenarios.', 'abstract_zh': '基于特征-结构混合不变表示学习的地理空间网络异分布泛化方法', 'title_zh': '因果不变地理网络表示：特征和结构分布转移'}
{'arxiv_id': 'arXiv:2503.19373', 'title': 'DeClotH: Decomposable 3D Cloth and Human Body Reconstruction from a Single Image', 'authors': 'Hyeongjin Nam, Donghwan Kim, Jeongtaek Oh, Kyoung Mu Lee', 'link': 'https://arxiv.org/abs/2503.19373', 'abstract': 'Most existing methods of 3D clothed human reconstruction from a single image treat the clothed human as a single object without distinguishing between cloth and human body. In this regard, we present DeClotH, which separately reconstructs 3D cloth and human body from a single image. This task remains largely unexplored due to the extreme occlusion between cloth and the human body, making it challenging to infer accurate geometries and textures. Moreover, while recent 3D human reconstruction methods have achieved impressive results using text-to-image diffusion models, directly applying such an approach to this problem often leads to incorrect guidance, particularly in reconstructing 3D cloth. To address these challenges, we propose two core designs in our framework. First, to alleviate the occlusion issue, we leverage 3D template models of cloth and human body as regularizations, which provide strong geometric priors to prevent erroneous reconstruction by the occlusion. Second, we introduce a cloth diffusion model specifically designed to provide contextual information about cloth appearance, thereby enhancing the reconstruction of 3D cloth. Qualitative and quantitative experiments demonstrate that our proposed approach is highly effective in reconstructing both 3D cloth and the human body. More qualitative results are provided at this https URL.', 'abstract_zh': '单图三维着装人体重构中区分布与人体的独立重建方法', 'title_zh': 'DeClotH：单张图像中可分解的3D衣物和人体重建'}
{'arxiv_id': 'arXiv:2503.19371', 'title': 'Flow to Learn: Flow Matching on Neural Network Parameters', 'authors': 'Daniel Saragih, Deyu Cao, Tejas Balaji, Ashwin Santhosh', 'link': 'https://arxiv.org/abs/2503.19371', 'abstract': 'Foundational language models show a remarkable ability to learn new concepts during inference via context data. However, similar work for images lag behind. To address this challenge, we introduce FLoWN, a flow matching model that learns to generate neural network parameters for different tasks. Our approach models the flow on latent space, while conditioning the process on context data. Experiments verify that FLoWN attains various desiderata for a meta-learning model. In addition, it matches or exceeds baselines on in-distribution tasks, provides better initializations for classifier training, and is performant on out-of-distribution few-shot tasks while having a fine-tuning mechanism to improve performance.', 'abstract_zh': '基于语言模型在推理过程中通过上下文数据学习新概念的能力突出，但类似的工作在图像方面进展缓慢。为解决这一挑战，我们提出了一种流匹配模型FLoWN，该模型学习为不同任务生成神经网络参数。我们的方法在隐空间中建模流，并通过上下文数据进行条件制约。实验验证了FLoWN达到了元学习模型的各项要求，并在同分布任务中匹配或超越基线，为分类器训练提供更好的初始化，并在少量样本的跨分布任务中表现出色，同时具备调优机制以提高性能。', 'title_zh': '流匹配学习：神经网络参数的流匹配'}
{'arxiv_id': 'arXiv:2503.19339', 'title': 'Efficient IoT Intrusion Detection with an Improved Attention-Based CNN-BiLSTM Architecture', 'authors': 'Amna Naeem, Muazzam A. Khan, Nada Alasbali, Jawad Ahmad, Aizaz Ahmad Khattak, Muhammad Shahbaz Khan', 'link': 'https://arxiv.org/abs/2503.19339', 'abstract': "The ever-increasing security vulnerabilities in the Internet-of-Things (IoT) systems require improved threat detection approaches. This paper presents a compact and efficient approach to detect botnet attacks by employing an integrated approach that consists of traffic pattern analysis, temporal support learning, and focused feature extraction. The proposed attention-based model benefits from a hybrid CNN-BiLSTM architecture and achieves 99% classification accuracy in detecting botnet attacks utilizing the N-BaIoT dataset, while maintaining high precision and recall across various scenarios. The proposed model's performance is further validated by key parameters, such as Mathews Correlation Coefficient and Cohen's kappa Correlation Coefficient. The close-to-ideal results for these parameters demonstrate the proposed model's ability to detect botnet attacks accurately and efficiently in practical settings and on unseen data. The proposed model proved to be a powerful defense mechanism for IoT networks to face emerging security challenges.", 'abstract_zh': '不断增加的物联网（IoT）系统中的安全漏洞需要改进的威胁检测方法。本文提出了一种紧凑且高效的方法，通过结合流量模式分析、时间支持学习和聚焦特征提取的集成方法来检测botnet攻击。所提出的基于注意力的模型利用了混合CNN-BiLSTM架构，并在使用N-BaIoT数据集检测botnet攻击时实现了99%的分类准确性，同时在各种场景中保持了高精确度和召回率。所提出的模型的性能通过关键参数，如马修斯相关系数和科恩κ相关系数进一步得到验证。这些参数的接近理想的结果展示了所提出模型在实际设置和未见数据中准确且高效地检测botnet攻击的能力。所提出的模型证明是一种强大的防御机制，用于应对物联网网络面临的新兴安全挑战。', 'title_zh': '基于改进的注意力机制CNN-BiLSTM架构的高效物联网入侵检测'}
{'arxiv_id': 'arXiv:2503.19329', 'title': 'Wavelet-based Global-Local Interaction Network with Cross-Attention for Multi-View Diabetic Retinopathy Detection', 'authors': 'Yongting Hu, Yuxin Lin, Chengliang Liu, Xiaoling Luo, Xiaoyan Dou, Qihao Xu, Yong Xu', 'link': 'https://arxiv.org/abs/2503.19329', 'abstract': 'Multi-view diabetic retinopathy (DR) detection has recently emerged as a promising method to address the issue of incomplete lesions faced by single-view DR. However, it is still challenging due to the variable sizes and scattered locations of lesions. Furthermore, existing multi-view DR methods typically merge multiple views without considering the correlations and redundancies of lesion information across them. Therefore, we propose a novel method to overcome the challenges of difficult lesion information learning and inadequate multi-view fusion. Specifically, we introduce a two-branch network to obtain both local lesion features and their global dependencies. The high-frequency component of the wavelet transform is used to exploit lesion edge information, which is then enhanced by global semantic to facilitate difficult lesion learning. Additionally, we present a cross-view fusion module to improve multi-view fusion and reduce redundancy. Experimental results on large public datasets demonstrate the effectiveness of our method. The code is open sourced on this https URL.', 'abstract_zh': '多视图糖尿病视网膜病变检测 recently emerged as a promising method to address the issue of incomplete lesions faced by single-view diabetic retinopathy. However, it is still challenging due to the variable sizes and scattered locations of lesions. Furthermore, existing multi-view diabetic retinopathy methods typically merge multiple views without considering the correlations and redundancies of lesion information across them. Therefore, we propose a novel method to overcome the challenges of difficult lesion information learning and inadequate multi-view fusion. Specifically, we introduce a two-branch network to obtain both local lesion features and their global dependencies. The high-frequency component of the wavelet transform is used to exploit lesion edge information, which is then enhanced by global semantic to facilitate difficult lesion learning. Additionally, we present a cross-view fusion module to improve multi-view fusion and reduce redundancy. Experimental results on large public datasets demonstrate the effectiveness of our method. The code is open sourced at this https URL.', 'title_zh': '基于小波的全局-局部交互网络与跨注意力机制的多视角糖尿病视网膜病变检测'}
{'arxiv_id': 'arXiv:2503.19328', 'title': 'Substance over Style: Evaluating Proactive Conversational Coaching Agents', 'authors': 'Vidya Srinivas, Xuhai Xu, Xin Liu, Kumar Ayush, Isaac Galatzer-Levy, Shwetak Patel, Daniel McDuff, Tim Althoff', 'link': 'https://arxiv.org/abs/2503.19328', 'abstract': 'While NLP research has made strides in conversational tasks, many approaches focus on single-turn responses with well-defined objectives or evaluation criteria. In contrast, coaching presents unique challenges with initially undefined goals that evolve through multi-turn interactions, subjective evaluation criteria, mixed-initiative dialogue. In this work, we describe and implement five multi-turn coaching agents that exhibit distinct conversational styles, and evaluate them through a user study, collecting first-person feedback on 155 conversations. We find that users highly value core functionality, and that stylistic components in absence of core components are viewed negatively. By comparing user feedback with third-person evaluations from health experts and an LM, we reveal significant misalignment across evaluation approaches. Our findings provide insights into design and evaluation of conversational coaching agents and contribute toward improving human-centered NLP applications.', 'abstract_zh': '尽管自然语言处理研究在对话任务上取得了进展，许多方法侧重于具有明确目标或评估标准的单轮响应。相比之下，辅导任务则面临初始目标不明确，通过多轮交互演变，具有主观评估标准和混合主动对话的独特挑战。在本工作中，我们描述并实现五种具有不同对话风格的多轮辅导代理，并通过用户研究对其进行了评估，收集了关于155次对话的first-person反馈。我们发现用户高度认可核心功能，而缺乏核心功能的样式组件则被视为负面。通过将用户反馈与来自健康专家和语言模型的第三人称评估进行比较，我们揭示了评估方法之间的显著不一致。我们的发现为对话辅导代理的设计和评估提供了见解，并有助于改善以人类为中心的自然语言处理应用。', 'title_zh': '重实质轻形式：评估主动对话教练代理'}
{'arxiv_id': 'arXiv:2503.19311', 'title': 'LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing Image with Longer Text', 'authors': 'Weizhi Chen, Jingbo Chen, Yupeng Deng, Jiansheng Chen, Yuman Feng, Zhihao Xi, Diyou Liu, Kai Li, Yu Meng', 'link': 'https://arxiv.org/abs/2503.19311', 'abstract': 'This study addresses the technical bottlenecks in handling long text and the "hallucination" issue caused by insufficient short text information in remote sensing vision-language foundation models (VLFM). We propose a novel vision-language foundation model, LRSCLIP, and a multimodal dataset, LRS2M. The main contributions are as follows: (1) By integrating multi-source remote sensing data and adopting a large language model labeling strategy, we construct the LRS2M dataset, which contains 2 million image-text pairs, providing both short and long texts for the first time, thus solving the problem of semantic granularity limitations in existing datasets; (2) The design of the LRSCLIP architecture based on Long-CLIP\'s KPS module, which extends CLIP\'s text processing capacity and achieves fine-grained cross-modal feature alignment through a dual-text loss weighting mechanism. Experimental results show that LRSCLIP improves retrieval accuracy by 10\\%-20\\% over the Long-CLIP baseline in the zero-shot long-text cross-modal retrieval task. For the zero-shot short-text cross-modal retrieval task, LRSCLIP achieves improvements over the current best model, GeoRSCLIP, with increases of 0.17\\%, 0.67\\%, and 0.92\\% in Text to Image R@1, Image to Text R@1, and mR on RSITMD, respectively, and 0.04\\%, 2.93\\%, and 1.28\\% on RSICD. In the zero-shot image classification task (average accuracy=75.75\\%) and semantic localization task (Rmi=0.7653), LRSCLIP achieves state-of-the-art performance. These results validate the dual advantages of fine-grained semantic understanding and global feature matching in LRSCLIP. This work provides a new benchmark model and data support for remote sensing multimodal learning. The related code has been open source and is available at this https URL.', 'abstract_zh': '本研究解决远程 sensing 视觉-语言基础模型（VLFM）在处理长文本和技术瓶颈以及由短文本信息不足引起的“幻觉”问题。我们提出了一种新型的视觉-语言基础模型LRSCLIP，以及一个多模态数据集LRS2M。主要贡献包括：（1）通过集成多源遥感数据并采用大规模语言模型标签策略，构建了包含200万图像-文本对的LRS2M数据集，首次提供短文本和长文本，解决了现有数据集在语义粒度限制方面的问题；（2）基于Long-CLIP的KPS模块设计LRSCLIP架构，扩展了CLIP的文字处理能力，并通过双文本损失加权机制实现精细粒度的跨模态特征对齐。实验结果表明，在零样本长文本跨模态检索任务中，LRSCLIP比Long-CLIP基线提高了10%-20%的检索精度。在零样本短文本跨模态检索任务中，LRSCLIP分别在RSITMD和RSICD上比当前最佳模型GeoRSCLIP在Text to Image R@1、Image to Text R@1和mR上提高了0.17%、0.67%、0.92%和0.04%、2.93%、1.28%。在零样本图像分类任务（平均准确率=75.75%）和语义定位任务（Rmi=0.7653）中，LRSCLIP取得了最先进水平。这些结果验证了LRSCLIP在细粒度语义理解和全球特征匹配方面的双重优势。本研究为遥感多模态学习提供了一个新的基准模型和数据支持。相关代码已开源，可在以下网址获取。', 'title_zh': 'LRSCLIP: 一种用于对齐遥感图像与长文本的视觉-语言基础模型'}
{'arxiv_id': 'arXiv:2503.19292', 'title': "Adaptive Wavelet Filters as Practical Texture Feature Amplifiers for Parkinson's Disease Screening in OCT", 'authors': 'Xiaoqing Zhang, Hanfeng Shi, Xiangyu Li, Haili Ye, Tao Xu, Na Li, Yan Hu, Fan Lv, Jiangfan Chen, Jiang Liu', 'link': 'https://arxiv.org/abs/2503.19292', 'abstract': "Parkinson's disease (PD) is a prevalent neurodegenerative disorder globally. The eye's retina is an extension of the brain and has great potential in PD screening. Recent studies have suggested that texture features extracted from retinal layers can be adopted as biomarkers for PD diagnosis under optical coherence tomography (OCT) images. Frequency domain learning techniques can enhance the feature representations of deep neural networks (DNNs) by decomposing frequency components involving rich texture features. Additionally, previous works have not exploited texture features for automated PD screening in OCT. Motivated by the above analysis, we propose a novel Adaptive Wavelet Filter (AWF) that serves as the Practical Texture Feature Amplifier to fully leverage the merits of texture features to boost the PD screening performance of DNNs with the aid of frequency domain learning. Specifically, AWF first enhances texture feature representation diversities via channel mixer, then emphasizes informative texture feature representations with the well-designed adaptive wavelet filtering token mixer. By combining the AWFs with the DNN stem, AWFNet is constructed for automated PD screening. Additionally, we introduce a novel Balanced Confidence (BC) Loss by mining the potential of sample-wise predicted probabilities of all classes and class frequency prior, to further boost the PD screening performance and trustworthiness of AWFNet. The extensive experiments manifest the superiority of our AWFNet and BC over state-of-the-art methods in terms of PD screening performance and trustworthiness.", 'abstract_zh': '帕金森病（PD）是一种全球性的神经退行性疾病。眼睛的视网膜是脑部的延伸，具有在PD筛查中巨大潜力。近期研究表明，可以从光学相干断层扫描（OCT）图像中的视网膜层提取的纹理特征可作为PD诊断的生物标志物。频率域学习技术可以通过分解包含丰富纹理特征的频率成分来增强深度神经网络（DNNs）的特征表示能力。此外，以往的工作尚未利用纹理特征进行OCT自动PD筛查。在上述分析的动机下，我们提出了一种新颖的自适应小波滤波器（AWF），作为实际纹理特征放大器，以充分利用纹理特征的优点，通过频率域学习提升DNNs的PD筛查性能。具体而言，AWF首先通过通道混合器增强纹理特征表示的多样性，然后通过精心设计的自适应小波过滤器令牌混合器强调关键的纹理特征表示。通过将AWFs与DNN主干结合，构建了AWFNet用于自动PD筛查。此外，我们引入了一种新颖的平衡信心（BC）损失，通过挖掘每个类别样本预测概率和类别频率先验的潜力，进一步提升了AWFNet的PD筛查性能和可信度。广泛的实验表明，与最先进的方法相比，我们的AWFNet和BC在PD筛查性能和可信度方面具有优势。', 'title_zh': '自适应小波滤波器作为实用的纹理特征放大器用于OCT帕金森病筛查'}
{'arxiv_id': 'arXiv:2503.19285', 'title': 'No Black Box Anymore: Demystifying Clinical Predictive Modeling with Temporal-Feature Cross Attention Mechanism', 'authors': 'Yubo Li, Xinyu Yao, Rema Padman', 'link': 'https://arxiv.org/abs/2503.19285', 'abstract': 'Despite the outstanding performance of deep learning models in clinical prediction tasks, explainability remains a significant challenge. Inspired by transformer architectures, we introduce the Temporal-Feature Cross Attention Mechanism (TFCAM), a novel deep learning framework designed to capture dynamic interactions among clinical features across time, enhancing both predictive accuracy and interpretability. In an experiment with 1,422 patients with Chronic Kidney Disease, predicting progression to End-Stage Renal Disease, TFCAM outperformed LSTM and RETAIN baselines, achieving an AUROC of 0.95 and an F1-score of 0.69. Beyond performance gains, TFCAM provides multi-level explainability by identifying critical temporal periods, ranking feature importance, and quantifying how features influence each other across time before affecting predictions. Our approach addresses the "black box" limitations of deep learning in healthcare, offering clinicians transparent insights into disease progression mechanisms while maintaining state-of-the-art predictive performance.', 'abstract_zh': '尽管深度学习模型在临床预测任务中表现出色，但解释性仍然是一个重大挑战。受变压器架构启发，我们引入了时间-特征交叉注意力机制（TFCAM），这是一种新型的深度学习框架，旨在捕捉临床特征随时间动态交互，同时提高预测准确性和解释性。在涉及1,422例慢性肾病患者的试验中，预测进展至终末期肾病，TFCAM优于LSTM和RETAIN基准模型，AUROC达到0.95，F1-score为0.69。除了性能提升，TFCAM还通过识别关键时间周期、排名特征重要性以及量化特征如何随时间相互影响从而影响预测，提供了多级解释性。我们的方法解决了深度学习在医疗保健中的“黑盒”限制，为临床医生提供透明的疾病进展机制洞察，同时保持了最先进的预测性能。', 'title_zh': '不再黑箱：时空特征跨注意力机制解析临床预测建模'}
{'arxiv_id': 'arXiv:2503.19281', 'title': "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model", 'authors': 'Feiyang Wang, Xiaomin Yu, Wangyu Wu', 'link': 'https://arxiv.org/abs/2503.19281', 'abstract': "Proving Rubik's Cube theorems at the high level represents a notable milestone in human-level spatial imagination and logic thinking and reasoning. Traditional Rubik's Cube robots, relying on complex vision systems and fixed algorithms, often struggle to adapt to complex and dynamic scenarios. To overcome this limitation, we introduce CubeRobot, a novel vision-language model (VLM) tailored for solving 3x3 Rubik's Cubes, empowering embodied agents with multimodal understanding and execution capabilities. We used the CubeCoT image dataset, which contains multiple-level tasks (43 subtasks in total) that humans are unable to handle, encompassing various cube states. We incorporate a dual-loop VisionCoT architecture and Memory Stream, a paradigm for extracting task-related features from VLM-generated planning queries, thus enabling CubeRobot to independent planning, decision-making, reflection and separate management of high- and low-level Rubik's Cube tasks. Furthermore, in low-level Rubik's Cube restoration tasks, CubeRobot achieved a high accuracy rate of 100%, similar to 100% in medium-level tasks, and achieved an accuracy rate of 80% in high-level tasks.", 'abstract_zh': '在高维度上证明魔方定理代表了人类空间想象和逻辑推理的一个重要里程碑。CubeRobot：一种基于视觉语言模型的新型魔方解决机器人，通过多模态理解和执行能力增强了实体代理，解决了传统魔方机器人在复杂动态场景下的适应性问题。通过使用包含43个超人类处理的多层次任务的CubeCoT图像数据集，CubeRobot能够独立进行高、低层次魔方任务的规划、决策、反思和管理。在低层次魔方恢复任务中，CubeRobot的准确率达到100%，与中层次任务相同，并在高层次任务中达到80%的准确率。', 'title_zh': 'CubeRobot: 通过视觉语言模型在魔方操作中实现语言 grounding'}
{'arxiv_id': 'arXiv:2503.19280', 'title': 'LogicLearner: A Tool for the Guided Practice of Propositional Logic Proofs', 'authors': 'Amogh Inamdar, Uzay Macar, Michel Vazirani, Michael Tarnow, Zarina Mustapha, Natalia Dittren, Sam Sadeh, Nakul Verma, Ansaf Salleb-Aouissi', 'link': 'https://arxiv.org/abs/2503.19280', 'abstract': 'The study of propositional logic -- fundamental to the theory of computing -- is a cornerstone of the undergraduate computer science curriculum. Learning to solve logical proofs requires repeated guided practice, but undergraduate students often lack access to on-demand tutoring in a judgment-free environment. In this work, we highlight the need for guided practice tools in undergraduate mathematics education and outline the desiderata of an effective practice tool. We accordingly develop LogicLearner, a web application for guided logic proof practice. LogicLearner consists of an interface to attempt logic proofs step-by-step and an automated proof solver to generate solutions on the fly, allowing users to request guidance as needed. We pilot LogicLearner as a practice tool in two semesters of an undergraduate discrete mathematics course and receive strongly positive feedback for usability and pedagogical value in student surveys. To the best of our knowledge, LogicLearner is the only learning tool that provides an end-to-end practice environment for logic proofs with immediate, judgment-free feedback.', 'abstract_zh': '命题逻辑的研究——这是计算理论的基础，在本科计算机科学课程中是基石。学习解决逻辑证明需要反复的指导性练习，但本科生通常缺乏一个无评判环境下的即时辅导资源。在此项工作中，我们强调了指导性练习工具在本科数学教育中的重要性，并概述了有效练习工具的理想特征。我们据此开发了LogicLearner，一个在线逻辑证明指导练习的网络应用。LogicLearner包含一个逐步尝试逻辑证明的界面和一个自动生成解决方案的自动化证明解决工具，允许用户在需要时请求指导。我们在一个本科离散数学课程的两个学期中试点LogicLearner作为练习工具，并在学生调查中收到了关于易用性和教学价值的强烈正面反馈。据我们所知，LogicLearner是唯一一个提供即时无评判逻辑证明练习环境的学习工具。', 'title_zh': '逻辑学习器：命题逻辑证明引导练习的工具'}
{'arxiv_id': 'arXiv:2503.19276', 'title': 'Context-Aware Semantic Segmentation: Enhancing Pixel-Level Understanding with Large Language Models for Advanced Vision Applications', 'authors': 'Ben Rahman', 'link': 'https://arxiv.org/abs/2503.19276', 'abstract': 'Semantic segmentation has made significant strides in pixel-level image understanding, yet it remains limited in capturing contextual and semantic relationships between objects. Current models, such as CNN and Transformer-based architectures, excel at identifying pixel-level features but fail to distinguish semantically similar objects (e.g., "doctor" vs. "nurse" in a hospital scene) or understand complex contextual scenarios (e.g., differentiating a running child from a regular pedestrian in autonomous driving). To address these limitations, we proposed a novel Context-Aware Semantic Segmentation framework that integrates Large Language Models (LLMs) with state-of-the-art vision backbones. Our hybrid model leverages the Swin Transformer for robust visual feature extraction and GPT-4 for enriching semantic understanding through text embeddings. A Cross-Attention Mechanism is introduced to align vision and language features, enabling the model to reason about context more effectively. Additionally, Graph Neural Networks (GNNs) are employed to model object relationships within the scene, capturing dependencies that are overlooked by traditional models. Experimental results on benchmark datasets (e.g., COCO, Cityscapes) demonstrate that our approach outperforms the existing methods in both pixel-level accuracy (mIoU) and contextual understanding (mAP). This work bridges the gap between vision and language, paving the path for more intelligent and context-aware vision systems in applications including autonomous driving, medical imaging, and robotics.', 'abstract_zh': '基于上下文的语义分割在像素级图像理解中取得了显著进展，但仍有限制于捕捉物体间的上下文和语义关系。现有的模型，如CNN和基于Transformer的架构，擅长识别像素级特征，但在区分语义相似的对象（例如，医院场景中的“医生”和“护士”）或理解复杂的上下文场景（例如，在自动驾驶中区分奔跑的儿童和普通行人的方面）存在局限性。为了解决这些限制，我们提出了一种新的基于上下文的语义分割框架，该框架将大型语言模型（LLMs）与最先进的视觉骨干网络相结合。我们的混合模型利用Swin Transformer进行稳健的视觉特征提取，并通过文本嵌入丰富语义理解。引入了跨注意力机制以对齐视觉和语言特征，使模型能够更有效地进行上下文推理。此外，使用图神经网络（GNNs）建模场景中的对象关系，捕获传统模型忽略的依赖关系。在基准数据集（如COCO、Cityscapes）上的实验结果表明，我们的方法在像素级准确率（mIoU）和上下文理解（mAP）方面优于现有方法。这项工作填补了视觉和语言之间的鸿沟，为自动驾驶、医学成像和机器人等应用中的更智能和上下文感知视觉系统铺平了道路。', 'title_zh': '基于上下文的语义分割：通过大规模语言模型提升像素级理解以实现先进视觉应用'}
{'arxiv_id': 'arXiv:2503.19267', 'title': 'NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning with Extended Realistic Scenarios', 'authors': 'Songyi Gao, Zuolin Tu, Rong-Jun Qin, Yi-Hao Sun, Xiong-Hui Chen, Yang Yu', 'link': 'https://arxiv.org/abs/2503.19267', 'abstract': 'Offline reinforcement learning (RL) aims to learn from historical data without requiring (costly) access to the environment. To facilitate offline RL research, we previously introduced NeoRL, which highlighted that datasets from real-world tasks are often conservative and limited. With years of experience applying offline RL to various domains, we have identified additional real-world challenges. These include extremely conservative data distributions produced by deployed control systems, delayed action effects caused by high-latency transitions, external factors arising from the uncontrollable variance of transitions, and global safety constraints that are difficult to evaluate during the decision-making process. These challenges are underrepresented in previous benchmarks but frequently occur in real-world tasks. To address this, we constructed the extended Near Real-World Offline RL Benchmark (NeoRL-2), which consists of 7 datasets from 7 simulated tasks along with their corresponding evaluation simulators. Benchmarking results from state-of-the-art offline RL approaches demonstrate that current methods often struggle to outperform the data-collection behavior policy, highlighting the need for more effective methods. We hope NeoRL-2 will accelerate the development of reinforcement learning algorithms for real-world applications. The benchmark project page is available at this https URL.', 'abstract_zh': 'Offline强化学习（RL）旨在通过历史数据学习，而不需要访问环境（成本较高）。为了促进offline RL研究，我们先前引入了NeoRL，并指出现实世界任务的数据集通常保守且有限。基于多年来将offline RL应用于各种领域的经验，我们还发现了额外的现实世界挑战。这些挑战包括由部署的控制系统产生的极端保守的数据分布、由于高延迟过渡导致的动作效应延迟、来自过渡不可控变化的外部因素以及在决策过程中难以评估的全局安全性约束。这些挑战在之前的基准测试中并未充分体现，但在现实世界任务中却经常出现。为此，我们构建了扩展的接近现实世界的offline RL基准（NeoRL-2），其中包括7个从7个模拟任务提取的数据集及其相应的评估模拟器。最新的offline RL方法的基准测试结果表明，当前的方法往往难以超越数据收集行为策略，突显了开发更有效方法的必要性。我们希望NeoRL-2能够加速强化学习算法在现实世界应用中的发展。基准测试项目页面可在以下链接获取。', 'title_zh': 'NeoRL-2: 近真实世界数据集用于扩展现实场景的 Offline Reinforcement Learning'}
{'arxiv_id': 'arXiv:2503.19260', 'title': 'Linguistic Blind Spots of Large Language Models', 'authors': 'Jiali Cheng, Hadi Amiri', 'link': 'https://arxiv.org/abs/2503.19260', 'abstract': 'Large language models (LLMs) are the foundation of many AI applications today. However, despite their remarkable proficiency in generating coherent text, questions linger regarding their ability to perform fine-grained linguistic annotation tasks, such as detecting nouns or verbs, or identifying more complex syntactic structures like clauses in input texts. These tasks require precise syntactic and semantic understanding of input text, and when LLMs underperform on specific linguistic structures, it raises concerns about their reliability for detailed linguistic analysis and whether their (even correct) outputs truly reflect an understanding of the inputs. In this paper, we empirically study the performance of recent LLMs on fine-grained linguistic annotation tasks. Through a series of experiments, we find that recent LLMs show limited efficacy in addressing linguistic queries and often struggle with linguistically complex inputs. We show that the most capable LLM (Llama3-70b) makes notable errors in detecting linguistic structures, such as misidentifying embedded clauses, failing to recognize verb phrases, and confusing complex nominals with clauses. Our results provide insights to inform future advancements in LLM design and development.', 'abstract_zh': '大型语言模型（LLMs）是当今许多AI应用的基础。然而，尽管它们在生成连贯文本方面表现卓越，对于进行细粒度的语言标注任务，如检测名词或动词，或识别输入文本中的更复杂句法结构（如从句）的能力仍然存在疑问。这些任务需要精确的句法和语义理解，当LLMs在特定的语言结构上表现不佳时，这将对其在详细语言分析中的可靠性以及其（即使正确）输出是否真正反映输入的理解能力提出质疑。在本文中，我们通过实证研究分析了近期LLMs在细粒度语言标注任务中的表现。通过一系列实验，我们发现近期的LLMs在处理语言查询时表现出有限的效果，并且在处理复杂语义输入时经常会遇到困难。我们展示了性能最出色的LLM（Llama3-70b）在检测语言结构时会做出显著的错误，例如错误识别嵌入从句、未能识别动词短语以及将复杂的名词结构误认为是从句。我们的研究结果为未来LLM的设计和开发提供了参考。', 'title_zh': '大型语言模型的语义盲点'}
{'arxiv_id': 'arXiv:2503.19223', 'title': 'Face Spoofing Detection using Deep Learning', 'authors': 'Najeebullah, Maaz Salman, Zar Nawab Khan Swati', 'link': 'https://arxiv.org/abs/2503.19223', 'abstract': 'Digital image spoofing has emerged as a significant security threat in biometric authentication systems, particularly those relying on facial recognition. This study evaluates the performance of three vision based models, MobileNetV2, ResNET50, and Vision Transformer, ViT, for spoof detection in image classification, utilizing a dataset of 150,986 images divided into training , 140,002, testing, 10,984, and validation ,39,574, sets. Spoof detection is critical for enhancing the security of image recognition systems, and this research compares the models effectiveness through accuracy, precision, recall, and F1 score metrics. Results reveal that MobileNetV2 outperforms other architectures on the test dataset, achieving an accuracy of 91.59%, precision of 91.72%, recall of 91.59%, and F1 score of 91.58%, compared to ViT 86.54%, 88.28%, 86.54%, and 86.39%, respectively. On the validation dataset, MobileNetV2, and ViT excel, with MobileNetV2 slightly ahead at 97.17% accuracy versus ViT 96.36%. MobileNetV2 demonstrates faster convergence during training and superior generalization to unseen data, despite both models showing signs of overfitting. These findings highlight MobileNetV2 balanced performance and robustness, making it the preferred choice for spoof detection applications where reliability on new data is essential. The study underscores the importance of model selection in security sensitive contexts and suggests MobileNetV2 as a practical solution for real world deployment.', 'abstract_zh': '基于数字图像欺骗的生物特征认证系统中视觉模型的欺骗检测性能研究', 'title_zh': '使用深度学习的面部 spoofing 检测'}
{'arxiv_id': 'arXiv:2503.19217', 'title': 'LLM Benchmarking with LLaMA2: Evaluating Code Development Performance Across Multiple Programming Languages', 'authors': 'Patrick Diehl, Nojoud Nader, Maxim Moraru, Steven R. Brandt', 'link': 'https://arxiv.org/abs/2503.19217', 'abstract': "The rapid evolution of large language models (LLMs) has opened new possibilities for automating various tasks in software development. This paper evaluates the capabilities of the Llama 2-70B model in automating these tasks for scientific applications written in commonly used programming languages. Using representative test problems, we assess the model's capacity to generate code, documentation, and unit tests, as well as its ability to translate existing code between commonly used programming languages. Our comprehensive analysis evaluates the compilation, runtime behavior, and correctness of the generated and translated code. Additionally, we assess the quality of automatically generated code, documentation and unit tests. Our results indicate that while Llama 2-70B frequently generates syntactically correct and functional code for simpler numerical tasks, it encounters substantial difficulties with more complex, parallelized, or distributed computations, requiring considerable manual corrections. We identify key limitations and suggest areas for future improvements to better leverage AI-driven automation in scientific computing workflows.", 'abstract_zh': '大型语言模型（LLMs）的快速进化为软件开发中的各种任务自动化开启了新的可能性。本文评估了Llama 2-70B模型在自动化科学应用中编程语言生成代码、文档和单元测试的能力，以及其在常用编程语言之间翻译现有代码的能力。我们使用代表性测试问题评估模型生成和翻译代码的编译、运行时行为和正确性。此外，我们还评估了自动生成的代码、文档和单元测试的质量。研究结果表明，虽然Llama 2-70B在简单数值任务中经常生成语法正确且功能完整的代码，但在更复杂、并行化或分布式计算任务中遇到重大困难，需要大量人工修正。我们指出了关键限制，并建议未来改进的领域，以更好地利用AI驱动的自动化在科学计算工作流中的应用。', 'title_zh': 'LLM在LLaMA2中的基准测试：跨多种编程语言评估代码开发性能'}
{'arxiv_id': 'arXiv:2503.19213', 'title': 'A Survey of Large Language Model Agents for Question Answering', 'authors': 'Murong Yue', 'link': 'https://arxiv.org/abs/2503.19213', 'abstract': 'This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.', 'abstract_zh': '基于大型语言模型的问答代理发展综述', 'title_zh': '大规模语言模型代理在问答中的应用综述'}
{'arxiv_id': 'arXiv:2503.19212', 'title': 'Continual Reinforcement Learning for HVAC Systems Control: Integrating Hypernetworks and Transfer Learning', 'authors': 'Gautham Udayakumar Bekal, Ahmed Ghareeb, Ashish Pujari', 'link': 'https://arxiv.org/abs/2503.19212', 'abstract': 'Buildings with Heating, Ventilation, and Air Conditioning (HVAC) systems play a crucial role in ensuring indoor comfort and efficiency. While traditionally governed by physics-based models, the emergence of big data has enabled data-driven methods like Deep Reinforcement Learning (DRL). However, Reinforcement Learning (RL)-based techniques often suffer from sample inefficiency and limited generalization, especially across varying HVAC systems. We introduce a model-based reinforcement learning framework that uses a Hypernetwork to continuously learn environment dynamics across tasks with different action spaces. This enables efficient synthetic rollout generation and improved sample usage. Our approach demonstrates strong backward transfer in a continual learning setting after training on a second task, minimal fine-tuning on the first task allows rapid convergence within just 5 episodes and thus outperforming Model Free Reinforcement Learning (MFRL) and effectively mitigating catastrophic forgetting. These findings have significant implications for reducing energy consumption and operational costs in building management, thus supporting global sustainability goals.\nKeywords: Deep Reinforcement Learning, HVAC Systems Control, Hypernetworks, Transfer and Continual Learning, Catastrophic Forgetting', 'abstract_zh': '具有供暖、通风和空调系统的建筑物在确保室内舒适性和效率方面扮演着关键角色。传统上，这类系统受基于物理模型的控制，而大数据的出现使得基于数据驱动的方法，如深度强化学习（DRL）成为可能。然而，基于强化学习（RL）的技术往往存在样本效率低和泛化能力有限的问题，尤其是在不同类型的HVAC系统之间。我们提出了一种基于模型的强化学习框架，利用Hyper网络在具有不同动作空间的任务中持续学习环境动态，这使得合成轨迹生成更加高效并且样本使用更有效。我们的方法在训练于第二个任务后，在连续学习设置中表现出强大的反向迁移能力，仅仅在第一个任务上进行少量微调即可在短短5个episode内实现快速收敛，从而优于基于价值的强化学习（MFRL），并有效防止灾难性遗忘。这些发现对降低建筑管理中的能耗和运营成本具有重要意义，从而支持全球可持续发展目标。\n\n关键词：深度强化学习，HVAC系统控制，Hyper网络，迁移和连续学习，灾难性遗忘。', 'title_zh': '持续强化学习在 HVAC 系统控制中的应用：集成超网络和迁移学习'}
{'arxiv_id': 'arXiv:2503.19206', 'title': 'Overtrained Language Models Are Harder to Fine-Tune', 'authors': 'Jacob Mitchell Springer, Sachin Goyal, Kaiyue Wen, Tanishq Kumar, Xiang Yue, Sadhika Malladi, Graham Neubig, Aditi Raghunathan', 'link': 'https://arxiv.org/abs/2503.19206', 'abstract': 'Large language models are pre-trained on ever-growing token budgets under the assumption that better pre-training performance translates to improved downstream models. In this work, we challenge this assumption and show that extended pre-training can make models harder to fine-tune, leading to degraded final performance. We term this phenomenon catastrophic overtraining. For example, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads to over 2% worse performance on multiple standard LLM benchmarks than its 2.3T token counterpart. Through controlled experiments and theoretical analysis, we show that catastrophic overtraining arises from a systematic increase in the broad sensitivity of pre-trained parameters to modifications, including but not limited to fine-tuning. Our findings call for a critical reassessment of pre-training design that considers the downstream adaptability of the model.', 'abstract_zh': '大型语言模型在不断增长的令牌预算下进行预先训练，假设更好的预先训练性能会转化为下游模型的改进。在本工作中，我们挑战这一假设，并展示延长预先训练会使模型更难调整，导致最终性能下降。我们称这一现象为灾难性过训练。例如，预训练在3T令牌上的指令调整OLMo-1B模型，在多个标准LLM基准测试上的表现比其2.3T令牌版本差2%以上。通过受控实验和理论分析，我们表明灾难性过训练源于预训练参数对修改的系统性敏感度增加，包括但不限于调整。我们的发现呼吁对预训练设计进行重新评估，以考虑模型的下游适应性。', 'title_zh': '过拟合的语言模型更难细调'}
{'arxiv_id': 'arXiv:2503.19201', 'title': 'A Shared Low-Rank Adaptation Approach to Personalized RLHF', 'authors': 'Renpu Liu, Peng Wang, Donghao Li, Cong Shen, Jing Yang', 'link': 'https://arxiv.org/abs/2503.19201', 'abstract': 'Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique for aligning artificial intelligence systems with human values, achieving remarkable success in fine-tuning large language models. However, existing RLHF frameworks often assume that human preferences are relatively homogeneous and can be captured by a single, unified reward model. This assumption overlooks the inherent diversity and heterogeneity across individuals, limiting the adaptability of RLHF to personalized scenarios and risking misalignments that can diminish user satisfaction and trust in AI systems. In this paper, we address these challenges by introducing Low-Rank Adaptation (LoRA) into the personalized RLHF framework. We apply LoRA in the the aggregated parameter space of all personalized reward functions, thereby enabling efficient learning of personalized reward models from potentially limited local datasets. Our approach exploits potential shared structures among the local ground-truth reward models while allowing for individual adaptation, without relying on restrictive assumptions about shared representations as in prior works. We further establish sample complexity guarantees for our method. Theoretical analysis demonstrates the effectiveness of the proposed approach in capturing both shared and individual-specific structures within heterogeneous human preferences, addressing the dual challenge of personalization requirements and practical data constraints. Experimental results on real-world datasets corroborate the efficiency of our algorithm in the personalized RLHF setting.', 'abstract_zh': '基于人类反馈的低秩适应个性化强化学习（Low-Rank Adaptation for Personalized Reinforcement Learning from Human Feedback）', 'title_zh': '一种共享低秩适应方法实现个性化RLHF'}
{'arxiv_id': 'arXiv:2503.19195', 'title': 'Mining-Gym: A Configurable RL Benchmarking Environment for Truck Dispatch Scheduling', 'authors': 'Chayan Banerjee, Kien Nguyen, Clinton Fookes', 'link': 'https://arxiv.org/abs/2503.19195', 'abstract': 'Mining process optimization particularly truck dispatch scheduling is a critical factor in enhancing the efficiency of open pit mining operations However the dynamic and stochastic nature of mining environments characterized by uncertainties such as equipment failures truck maintenance and variable haul cycle times poses significant challenges for traditional optimization methods While Reinforcement Learning RL has shown promise in adaptive decision making for mining logistics its practical deployment requires rigorous evaluation in realistic and customizable simulation environments The lack of standardized benchmarking environments limits fair algorithm comparisons reproducibility and the real world applicability of RL based approaches in open pit mining settings To address this challenge we introduce Mining Gym a configurable open source benchmarking environment designed for training testing and comparing RL algorithms in mining process optimization Built on Discrete Event Simulation DES and seamlessly integrated with the OpenAI Gym interface Mining Gym provides a structured testbed that enables the direct application of advanced RL algorithms from Stable Baselines The framework models key mining specific uncertainties such as equipment failures queue congestion and the stochasticity of mining processes ensuring a realistic and adaptive learning environment Additionally Mining Gym features a graphical user interface GUI for intuitive mine site configuration a comprehensive data logging system a built in KPI dashboard and real time visual representation of the mine site These capabilities facilitate standardized reproducible evaluations across multiple RL strategies and baseline heuristics', 'abstract_zh': '采矿过程优化特别是卡车调度调度是提高露天矿作业效率的关键因素。然而，采矿环境的动态性和随机性，如设备故障、卡车维护和变动的运载周期时间等特点所带来的不确定性，对传统优化方法构成了重大挑战。尽管强化学习（RL）在采矿物流的自适应决策制定方面显示出 promise，但在实际部署中需要在现实的且可定制的仿真环境中进行严格的评估。缺乏标准化的基准测试环境限制了公平的算法比较、可重复性和基于 RL 的方法在露天矿山中的实际应用。为解决这一挑战，我们介绍了一种可配置的开源基准测试环境 Mining Gym，该环境旨在用于采矿过程优化中的 RL 算法的培训、测试和比较。Mining Gym 以离散事件仿真（DES）为基础，并无缝集成了 OpenAI Gym 接口。该框架模拟了关键的采矿特定不确定性，如设备故障、队列拥堵和采矿过程的随机性，确保了一个现实且适应性的学习环境。此外，Mining Gym 还配备了图形用户界面（GUI）用于直观的矿山配置、全面的数据日志系统、内置的 KPI 仪表盘以及实时可视化采矿现场的功能。这些功能使不同 RL 策略和基线启发式方法的标准、可重复评估变得容易。', 'title_zh': 'Mining-Gym：一种用于卡车调度协同的可配置RL基准环境'}
{'arxiv_id': 'arXiv:2503.19176', 'title': 'SoK: How Robust is Audio Watermarking in Generative AI models?', 'authors': 'Yizhu Wen, Ashwin Innuganti, Aaron Bien Ramos, Hanqing Guo, Qiben Yan', 'link': 'https://arxiv.org/abs/2503.19176', 'abstract': 'Audio watermarking is increasingly used to verify the provenance of AI-generated content, enabling applications such as detecting AI-generated speech, protecting music IP, and defending against voice cloning. To be effective, audio watermarks must resist removal attacks that distort signals to evade detection. While many schemes claim robustness, these claims are typically tested in isolation and against a limited set of attacks. A systematic evaluation against diverse removal attacks is lacking, hindering practical deployment. In this paper, we investigate whether recent watermarking schemes that claim robustness can withstand a broad range of removal attacks. First, we introduce a taxonomy covering 22 audio watermarking schemes. Next, we summarize their underlying technologies and potential vulnerabilities. We then present a large-scale empirical study to assess their robustness. To support this, we build an evaluation framework encompassing 22 types of removal attacks (109 configurations) including signal-level, physical-level, and AI-induced distortions. We reproduce 9 watermarking schemes using open-source code, identify 8 new highly effective attacks, and highlight 11 key findings that expose the fundamental limitations of these methods across 3 public datasets. Our results reveal that none of the surveyed schemes can withstand all tested distortions. This evaluation offers a comprehensive view of how current watermarking methods perform under real-world threats. Our demo and code are available at this https URL.', 'abstract_zh': '音频水印在验证AI生成内容的来源中 increasingly used, 使检测AI生成语音、保护音乐IP和防御语音克隆等应用成为可能。为了有效，音频水印必须抵御那些用于规避检测的信号篡改攻击。尽管许多方案声称具有鲁棒性，但这些声明通常是在孤立的情况下并且仅针对有限的攻击进行测试。缺乏针对多样化去除攻击的系统性评估，阻碍了其实用部署。本文研究最近声称具有鲁棒性的水印方案是否能够抵御广泛范围的去除攻击。首先，我们介绍了涵盖22种音频水印方案的分类系统。接着，我们总结了它们的基础技术和潜在脆弱性。然后，我们进行了一项大规模的实证研究来评估它们的鲁棒性。为此，我们构建了一个包含22种去除攻击类型（109种配置）的评估框架，包括信号级、物理级和AI诱导的失真。我们使用开源代码复现了9种水印方案，发现了8种新的高效攻击，并突出了11个关键发现，这些发现揭示了这些方法在3个公开数据集上面临的根本局限性。我们的结果表明，调查的方案都不能抵御所有测试的失真。这项评估为当前水印方法在真实世界威胁下的表现提供了全面视角。相关演示和代码可在以下链接获取。', 'title_zh': 'SoK: 生成式AI模型中的音频数字水印robustness如何？'}
{'arxiv_id': 'arXiv:2503.19152', 'title': 'PSO-UNet: Particle Swarm-Optimized U-Net Framework for Precise Multimodal Brain Tumor Segmentation', 'authors': 'Shoffan Saifullah, Rafał Dreżewski', 'link': 'https://arxiv.org/abs/2503.19152', 'abstract': "Medical image segmentation, particularly for brain tumor analysis, demands precise and computationally efficient models due to the complexity of multimodal MRI datasets and diverse tumor morphologies. This study introduces PSO-UNet, which integrates Particle Swarm Optimization (PSO) with the U-Net architecture for dynamic hyperparameter optimization. Unlike traditional manual tuning or alternative optimization approaches, PSO effectively navigates complex hyperparameter search spaces, explicitly optimizing the number of filters, kernel size, and learning rate. PSO-UNet substantially enhances segmentation performance, achieving Dice Similarity Coefficients (DSC) of 0.9578 and 0.9523 and Intersection over Union (IoU) scores of 0.9194 and 0.9097 on the BraTS 2021 and Figshare datasets, respectively. Moreover, the method reduces computational complexity significantly, utilizing only 7.8 million parameters and executing in approximately 906 seconds, markedly faster than comparable U-Net-based frameworks. These outcomes underscore PSO-UNet's robust generalization capabilities across diverse MRI modalities and tumor classifications, emphasizing its clinical potential and clear advantages over conventional hyperparameter tuning methods. Future research will explore hybrid optimization strategies and validate the framework against other bio-inspired algorithms to enhance its robustness and scalability.", 'abstract_zh': '基于粒子群优化的PSO-UNet在脑肿瘤医学图像分割中的应用：一种精确且计算高效的动态超参数优化方法', 'title_zh': 'PSO-UNet：粒子群优化的U-Net框架用于精确的多模态脑肿瘤分割'}
{'arxiv_id': 'arXiv:2503.19123', 'title': 'Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided Language Modeling', 'authors': 'Haebin Shin, Lei Ji, Xiao Liu, Yeyun Gong', 'link': 'https://arxiv.org/abs/2503.19123', 'abstract': 'Using large teacher models to guide the training of smaller student models has become the prevailing paradigm for efficient and effective learning. However, vocabulary mismatches between teacher and student language models pose significant challenges in language modeling, resulting in divergent token sequences and output distributions. To overcome these limitations, we propose Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel approach that bridges the gap caused by vocabulary mismatch through two key methods: (1) Token-level Lexical Alignment, which aligns token sequences across mismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss of teacher model to guide effective student training. We demonstrate its effectiveness in language modeling with 1B student model using various 7B teacher models with different vocabularies. Notably, with Qwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary with TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to naive continual pretraining. Furthermore, we demonstrate that VocAgnoLM consistently benefits from stronger teacher models, providing a robust solution to vocabulary mismatches in language modeling.', 'abstract_zh': '使用大型教师模型指导小型学生模型的语言模型训练已成为高效和有效的学习主流范式。然而，教师和学生语言模型之间的词汇不匹配给语言建模带来了显著挑战，导致了不同的标记序列和输出分布。为克服这些限制，我们提出了一种新的方法Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM)，该方法通过两种关键方法来弥补词汇不匹配造成的缺口：(1) Token-level Lexical Alignment，这是一种在不匹配的词汇表上对齐标记序列的方法；(2) Teacher Guided Loss，这是一种利用教师模型损失来指导有效学生训练的方法。我们通过使用各种7B参数量的教师模型和1B参数量的学生模型展示了其有效性。值得注意的是，使用与TinyLlama共享约6%词汇的Qwen2.5-Math-Instruct作为教师模型，VocAgnoLM相比盲目连续预训练实现了46%的性能提升。此外，我们还展示了VocAgnoLM从更强的教师模型中获益，提供了一个在语言模型中解决词汇不匹配问题的稳健解决方案。', 'title_zh': '克服词汇 mismatch：无词汇依赖教师引导的语言模型'}
{'arxiv_id': 'arXiv:2503.19120', 'title': 'Where is this coming from? Making groundedness count in the evaluation of Document VQA models', 'authors': 'Armineh Nourbakhsh, Siddharth Parekh, Pranav Shetty, Zhao Jin, Sameena Shah, Carolyn Rose', 'link': 'https://arxiv.org/abs/2503.19120', 'abstract': "Document Visual Question Answering (VQA) models have evolved at an impressive rate over the past few years, coming close to or matching human performance on some benchmarks. We argue that common evaluation metrics used by popular benchmarks do not account for the semantic and multimodal groundedness of a model's outputs. As a result, hallucinations and major semantic errors are treated the same way as well-grounded outputs, and the evaluation scores do not reflect the reasoning capabilities of the model. In response, we propose a new evaluation methodology that accounts for the groundedness of predictions with regard to the semantic characteristics of the output as well as the multimodal placement of the output within the input document. Our proposed methodology is parameterized in such a way that users can configure the score according to their preferences. We validate our scoring methodology using human judgment and show its potential impact on existing popular leaderboards. Through extensive analyses, we demonstrate that our proposed method produces scores that are a better indicator of a model's robustness and tends to give higher rewards to better-calibrated answers.", 'abstract_zh': '文档视觉问答（VQA）模型在过去几年中取得了令人印象深刻的进展，已在某些基准上接近或匹配了人类的表现。我们argue指出，广泛使用的大多数基准所采用的评估指标未能考虑模型输出的语义和多模态接地性。因此，幻觉和重大语义错误与充分接地的输出被同等对待，评估分数无法反映出模型的推理能力。为应对这一问题，我们提出了一种新的评估方法，该方法考虑了预测与输出语义特征以及输入文档中多模态位置的相关性。我们提出的方法具有参数化特性，用户可以根据自己的偏好进行配置。我们使用人类判断验证了评分方法，并展示了其在现有流行排行榜上的潜在影响。通过广泛的分析，我们证明了我们提出的方法产生的分数更能够反映模型的稳健性，并倾向于对更好地校准的答案给予更高的评分。', 'title_zh': '来自何处？在文档VQA模型评估中重视 grounding 的重要性'}
{'arxiv_id': 'arXiv:2503.19100', 'title': 'Anomaly Detection Using Computer Vision: A Comparative Analysis of Class Distinction and Performance Metrics', 'authors': 'Md. Barkat Ullah Tusher, Shartaz Khan Akash, Amirul Islam Showmik', 'link': 'https://arxiv.org/abs/2503.19100', 'abstract': 'This paper showcases an experimental study on anomaly detection using computer vision. The study focuses on class distinction and performance evaluation, combining OpenCV with deep learning techniques while employing a TensorFlow-based convolutional neural network for real-time face recognition and classification. The system effectively distinguishes among three classes: authorized personnel (admin), intruders, and non-human entities. A MobileNetV2-based deep learning model is utilized to optimize real-time performance, ensuring high computational efficiency without compromising accuracy. Extensive dataset preprocessing, including image augmentation and normalization, enhances the models generalization capabilities. Our analysis demonstrates classification accuracies of 90.20% for admin, 98.60% for intruders, and 75.80% for non-human detection, while maintaining an average processing rate of 30 frames per second. The study leverages transfer learning, batch normalization, and Adam optimization to achieve stable and robust learning, and a comparative analysis of class differentiation strategies highlights the impact of feature extraction techniques and training methodologies. The results indicate that advanced feature selection and data augmentation significantly enhance detection performance, particularly in distinguishing human from non-human scenes. As an experimental study, this research provides critical insights into optimizing deep learning-based surveillance systems for high-security environments and improving the accuracy and efficiency of real-time anomaly detection.', 'abstract_zh': '基于计算机视觉的异常检测实验研究：结合OpenCV和深度学习的实时面部识别与分类', 'title_zh': '利用计算机视觉进行异常检测：类区分与性能指标的比较分析'}
{'arxiv_id': 'arXiv:2503.19092', 'title': 'Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation', 'authors': 'Krisztian Balog, Donald Metzler, Zhen Qin', 'link': 'https://arxiv.org/abs/2503.19092', 'abstract': "Large language models (LLMs) are increasingly integral to information retrieval (IR), powering ranking, evaluation, and AI-assisted content creation. This widespread adoption necessitates a critical examination of potential biases arising from the interplay between these LLM-based components. This paper synthesizes existing research and presents novel experiment designs that explore how LLM-based rankers and assistants influence LLM-based judges. We provide the first empirical evidence of LLM judges exhibiting significant bias towards LLM-based rankers. Furthermore, we observe limitations in LLM judges' ability to discern subtle system performance differences. Contrary to some previous findings, our preliminary study does not find evidence of bias against AI-generated content. These results highlight the need for a more holistic view of the LLM-driven information ecosystem. To this end, we offer initial guidelines and a research agenda to ensure the reliable use of LLMs in IR evaluation.", 'abstract_zh': '大型语言模型（LLMs）在信息检索（IR）中的应用日益广泛，驱动排名、评估和AI辅助内容创作。这种广泛应用需要对这些基于LLM组件之间交互可能导致的潜在偏见进行批判性审查。本文综合现有研究，提出了新的实验设计，探讨基于LLM的排序器和助手如何影响基于LLM的评判者。我们提供了关于基于LLM的评判者对基于LLM的排序器表现出显著偏见的首个实证证据。此外，我们观察到基于LLM的评判者在区分细微系统性能差异方面存在局限性。与一些先前的研究结果不同，我们的初步研究未发现基于LLM的评判者对AI生成内容表现出偏见的证据。这些结果强调了对LLM驱动的信息生态系统进行全面审视的必要性。为此，我们提供了初步的指导意见和研究议程，以确保在信息检索评估中可靠地使用LLM。', 'title_zh': '排名者、评委和助手：理解语言模型在信息检索评估中的交互作用'}
{'arxiv_id': 'arXiv:2503.19075', 'title': 'The Case for "Thick Evaluations" of Cultural Representation in AI', 'authors': 'Rida Qadri, Mark Diaz, Ding Wang, Michael Madaio', 'link': 'https://arxiv.org/abs/2503.19075', 'abstract': "Generative AI image models have been increasingly evaluated for their (in)ability to represent non-Western cultures. We argue that these evaluations operate through reductive ideals of representation, abstracted from how people define their own representation and neglecting the inherently interpretive and contextual nature of cultural representation. In contrast to these 'thin' evaluations, we introduce the idea of 'thick evaluations': a more granular, situated, and discursive measurement framework for evaluating representations of social worlds in AI images, steeped in communities' own understandings of representation. We develop this evaluation framework through workshops in South Asia, by studying the 'thick' ways in which people interpret and assign meaning to images of their own cultures. We introduce practices for thicker evaluations of representation that expand the understanding of representation underpinning AI evaluations and by co-constructing metrics with communities, bringing measurement in line with the experiences of communities on the ground.", 'abstract_zh': '生成式AI图像模型越来越被评估其（不）能够代表非西方文化的能力。我们argue提出这些评估依赖于简化了的表现形式理想，这些理想从人们如何定义自己的表现中抽象出来，并忽视了文化表现本质上是解释性和情境性的特征。与这些“薄”评估相比，我们引入了“厚”评估的理念：一种更为精细、情境化和论述性的评价框架，用于评估AI图像中社会世界的表征，植根于社区自身对表征的理解。我们通过在南亚的工作坊开发这一评价框架，研究人们如何“厚”地解释和赋予文化图像意义。我们提出了扩展AI评估中表征理解的“厚”评价实践，并通过与社区共同构建指标，使评价与地面上社区的经验相一致。', 'title_zh': '" Thick 评价" 在人工智能文化表征中的重要性'}
{'arxiv_id': 'arXiv:2503.19074', 'title': 'HingeRLC-GAN: Combating Mode Collapse with Hinge Loss and RLC Regularization', 'authors': 'Osman Goni, Himadri Saha Arka, Mithun Halder, Mir Moynuddin Ahmed Shibly, Swakkhar Shatabda', 'link': 'https://arxiv.org/abs/2503.19074', 'abstract': 'Recent advances in Generative Adversarial Networks (GANs) have demonstrated their capability for producing high-quality images. However, a significant challenge remains mode collapse, which occurs when the generator produces a limited number of data patterns that do not reflect the diversity of the training dataset. This study addresses this issue by proposing a number of architectural changes aimed at increasing the diversity and stability of GAN models. We start by improving the loss function with Wasserstein loss and Gradient Penalty to better capture the full range of data variations. We also investigate various network architectures and conclude that ResNet significantly contributes to increased diversity. Building on these findings, we introduce HingeRLC-GAN, a novel approach that combines RLC Regularization and the Hinge loss function. With a FID Score of 18 and a KID Score of 0.001, our approach outperforms existing methods by effectively balancing training stability and increased diversity.', 'abstract_zh': '最近生成式对抗网络（GANs）的进展展示了其生成高保真图像的能力。然而，模式崩溃这一重要挑战依然存在，当生成器只产生有限的数据模式且未能反映训练数据集的多样性时就会发生这种情况。本研究通过提出多种架构改进措施来应对这一问题，旨在提高GAN模型的多样性和稳定性。我们首先通过使用Wasserstein损失和梯度惩罚改进损失函数，以更好地捕捉数据变异性。我们还探讨了各种网络架构，并得出结论，ResNet显著提高了多样性。在此基础上，我们引入了HingeRLC-GAN，这是一种结合RLC正则化和Hinge损失的新方法。我们的方法在FID得分为18和KID得分为0.001的情况下，通过有效平衡训练稳定性和增加多样性，优于现有方法。', 'title_zh': 'HingeRLC-GAN：基于Hinge损失和RLC正则化的模式崩溃对抗方法'}
{'arxiv_id': 'arXiv:2503.19070', 'title': 'Graph-Level Label-Only Membership Inference Attack against Graph Neural Networks', 'authors': 'Jiazhu Dai, Yubing Lu', 'link': 'https://arxiv.org/abs/2503.19070', 'abstract': "Graph neural networks (GNNs) are widely used for graph-structured data but are vulnerable to membership inference attacks (MIAs) in graph classification tasks, which determine if a graph was part of the training dataset, potentially causing data leakage. Existing MIAs rely on prediction probability vectors, but they become ineffective when only prediction labels are available. We propose a Graph-level Label-Only Membership Inference Attack (GLO-MIA), which is based on the intuition that the target model's predictions on training data are more stable than those on testing data. GLO-MIA generates a set of perturbed graphs for target graph by adding perturbations to its effective features and queries the target model with the perturbed graphs to get their prediction labels, which are then used to calculate robustness score of the target graph. Finally, by comparing the robustness score with a predefined threshold, the membership of the target graph can be inferred correctly with high probability. Our evaluation on three datasets and four GNN models shows that GLO-MIA achieves an attack accuracy of up to 0.825, outperforming baseline work by 8.5% and closely matching the performance of probability-based MIAs, even with only prediction labels.", 'abstract_zh': '基于图级别标签唯一性推理攻击（GLO-MIA）：图神经网络中的会员推理攻击', 'title_zh': '基于图神经网络的图级别标签唯一性成员推理攻击'}
{'arxiv_id': 'arXiv:2503.19068', 'title': 'Minimum Volume Conformal Sets for Multivariate Regression', 'authors': 'Sacha Braun, Liviu Aolaritei, Michael I. Jordan, Francis Bach', 'link': 'https://arxiv.org/abs/2503.19068', 'abstract': 'Conformal prediction provides a principled framework for constructing predictive sets with finite-sample validity. While much of the focus has been on univariate response variables, existing multivariate methods either impose rigid geometric assumptions or rely on flexible but computationally expensive approaches that do not explicitly optimize prediction set volume. We propose an optimization-driven framework based on a novel loss function that directly learns minimum-volume covering sets while ensuring valid coverage. This formulation naturally induces a new nonconformity score for conformal prediction, which adapts to the residual distribution and covariates. Our approach optimizes over prediction sets defined by arbitrary norm balls, including single and multi-norm formulations. Additionally, by jointly optimizing both the predictive model and predictive uncertainty, we obtain prediction sets that are tight, informative, and computationally efficient, as demonstrated in our experiments on real-world datasets.', 'abstract_zh': '构形预测提供了一种原理性的框架，用于构建具有有限样本有效性的预测集。尽管大部分关注集中在一元响应变量上，现有的多元方法要么施加严格的几何假设，要么依赖于灵活但计算成本高昂的方法，这些方法并没有明确优化预测集的体积。我们提出了一种基于新型损失函数的优化驱动框架，该框架可以直接学习最小体积覆盖集，同时确保有效覆盖。该表述自然诱导了一种新的非一致性得分，该得分能够适应残差分布和协变量。我们的方法通过任意范数球定义的预测集进行优化，包括单范数和多范数形式。此外，通过同时优化预测模型和预测不确定性，我们获得了紧致、信息丰富且计算高效的预测集，这一特点已在我们的实证研究中得到验证。', 'title_zh': '多元回归中最小区间套合集'}
{'arxiv_id': 'arXiv:2503.19050', 'title': 'Mist: Efficient Distributed Training of Large Language Models via Memory-Parallelism Co-Optimization', 'authors': 'Zhanda Zhu, Christina Giannoula, Muralidhar Andoorveedu, Qidong Su, Karttikeya Mangalam, Bojian Zheng, Gennady Pekhimenko', 'link': 'https://arxiv.org/abs/2503.19050', 'abstract': 'Various parallelism, such as data, tensor, and pipeline parallelism, along with memory optimizations like activation checkpointing, redundancy elimination, and offloading, have been proposed to accelerate distributed training for Large Language Models. To find the best combination of these techniques, automatic distributed training systems are proposed. However, existing systems only tune a subset of optimizations, due to the lack of overlap awareness, inability to navigate the vast search space, and ignoring the inter-microbatch imbalance, leading to sub-optimal performance. To address these shortcomings, we propose Mist, a memory, overlap, and imbalance-aware automatic distributed training system that comprehensively co-optimizes all memory footprint reduction techniques alongside parallelism. Mist is based on three key ideas: (1) fine-grained overlap-centric scheduling, orchestrating optimizations in an overlapped manner, (2) symbolic-based performance analysis that predicts runtime and memory usage using symbolic expressions for fast tuning, and (3) imbalance-aware hierarchical tuning, decoupling the process into an inter-stage imbalance and overlap aware Mixed Integer Linear Programming problem and an intra-stage Dual-Objective Constrained Optimization problem, and connecting them through Pareto frontier sampling. Our evaluation results show that Mist achieves an average of 1.28$\\times$ (up to 1.73$\\times$) and 1.27$\\times$ (up to 2.04$\\times$) speedup compared to state-of-the-art manual system Megatron-LM and state-of-the-art automatic system Aceso, respectively.', 'abstract_zh': '各种并行技术（如数据并行、张量并行和管道并行）以及内存优化技术（如激活检查点、冗余消除和卸载）被提出以加速大规模语言模型的分布式训练。为了找到这些技术的最佳组合，提出了自动分布式训练系统。然而，现有系统只能调整优化的一部分，因为它们缺乏对重叠意识的考虑、无法导航庞大的搜索空间且忽略了微批处理间的不平衡，从而导致性能不佳。为了解决这些不足，我们提出了一种名为Mist的自动分布式训练系统，该系统综合考虑所有内存占用减少技术和并行技术，并具备重叠、内存占用和不平衡意识。Mist基于三个关键思想：（1）细粒度的重叠为中心调度，以重叠方式协调优化；（2）基于符号的性能分析，使用符号表达式预测运行时间和内存使用情况，以便快速调优；（3）不平衡意识的分级调优，将过程拆分为跨阶段的重叠和不平衡意识的混合整数线性规划问题和跨阶段的双目标约束优化问题，并通过帕累托前沿采样连接它们。我们的评估结果表明，与最先进的手动系统Megatron-LM相比，Mist分别平均实现了1.28倍（最大1.73倍）和1.27倍（最大2.04倍）的加速；与最先进的自动系统Aceso相比，分别实现了1.28倍（最大1.73倍）和1.27倍（最大2.04倍）的加速。', 'title_zh': 'Mist: 通过内存并行性协同优化高效训练大型语言模型'}
{'arxiv_id': 'arXiv:2503.19048', 'title': 'Forecasting Labor Demand: Predicting JOLT Job Openings using Deep Learning Model', 'authors': 'Kyungsu Kim', 'link': 'https://arxiv.org/abs/2503.19048', 'abstract': 'This thesis studies the effectiveness of Long Short Term Memory model in forecasting future Job Openings and Labor Turnover Survey data in the United States. Drawing on multiple economic indicators from various sources, the data are fed directly into LSTM model to predict JOLT job openings in subsequent periods. The performance of the LSTM model is compared with conventional autoregressive approaches, including ARIMA, SARIMA, and Holt-Winters. Findings suggest that the LSTM model outperforms these traditional models in predicting JOLT job openings, as it not only captures the dependent variables trends but also harmonized with key economic factors. These results highlight the potential of deep learning techniques in capturing complex temporal dependencies in economic data, offering valuable insights for policymakers and stakeholders in developing data-driven labor market strategies', 'abstract_zh': '本论文研究长短期记忆模型在预测美国就业空缺和劳动力流动调查数据方面的有效性。利用多种经济指标数据，直接输入LSTM模型以预测后续时期的JOLT就业空缺数据。LSTM模型的性能与传统的自回归方法（包括ARIMA、SARIMA和Holt-Winters）进行比较。研究发现，LSTM模型在预测JOLT就业空缺方面优于传统模型，不仅捕捉了因变量的趋势，还与关键经济因素相协调。这些结果突显了深度学习技术在捕捉经济数据中复杂时间依赖关系的潜力，为政策制定者和利益相关者制定基于数据的劳动力市场策略提供了宝贵见解。', 'title_zh': '劳动力需求预测：基于深度学习模型预测JOLT岗位空缺'}
{'arxiv_id': 'arXiv:2503.19041', 'title': 'LookAhead Tuning: Safer Language Models via Partial Answer Previews', 'authors': 'Kangwei Liu, Mengru Wang, Yujie Luo, Lin Yuan, Mengshu Sun, Ningyu Zhang, Lei Liang, Zhiqiang Zhang, Jun Zhou, Huajun Chen', 'link': 'https://arxiv.org/abs/2503.19041', 'abstract': "Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at this https URL.", 'abstract_zh': '前瞻调优通过预览部分答案前缀来修改训练数据，以保留模型固有的安全机制，从而在不牺牲下游任务的稳健性能的前提下有效维护模型安全。研究表明，前瞻调优是一个可靠且高效的解决方案，用于安全有效地适应大语言模型。代码发布在该网址。', 'title_zh': '拔头而出的调优：通过部分答案预览确保更安全的语言模型'}
{'arxiv_id': 'arXiv:2503.19037', 'title': 'Evolutionary Policy Optimization', 'authors': 'Jianren Wang, Yifan Su, Abhinav Gupta, Deepak Pathak', 'link': 'https://arxiv.org/abs/2503.19037', 'abstract': 'Despite its extreme sample inefficiency, on-policy reinforcement learning has become a fundamental tool in real-world applications. With recent advances in GPU-driven simulation, the ability to collect vast amounts of data for RL training has scaled exponentially. However, studies show that current on-policy methods, such as PPO, fail to fully leverage the benefits of parallelized environments, leading to performance saturation beyond a certain scale. In contrast, Evolutionary Algorithms (EAs) excel at increasing diversity through randomization, making them a natural complement to RL. However, existing EvoRL methods have struggled to gain widespread adoption due to their extreme sample inefficiency. To address these challenges, we introduce Evolutionary Policy Optimization (EPO), a novel policy gradient algorithm that combines the strengths of EA and policy gradients. We show that EPO significantly improves performance across diverse and challenging environments, demonstrating superior scalability with parallelized simulations.', 'abstract_zh': '尽管强化学习的样本效率极低，但在线策略强化学习已成为现实生活应用中的基本工具。随着基于GPU的模拟技术的recent进展，用于RL训练的数据收集能力已呈指数级增长。然而，研究表明，当前的在线策略方法，如PPO，在达到一定规模后无法充分利用并行环境的优势，导致性能饱和。相比之下，进化算法通过随机化增加多样性，使其成为RL的天然补充。然而，现有的EvoRL方法因极低的样本效率而难以广泛采用。为了解决这些挑战，我们引入了进化策略优化（EPO），这是一种结合了进化算法和策略梯度优势的新型策略梯度算法。我们证明EPO在多种复杂环境中显著提高性能，并展示了其在并行模拟中的优越可扩展性。', 'title_zh': '进化策略优化'}
{'arxiv_id': 'arXiv:2503.19007', 'title': 'Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement Learning', 'authors': 'Chak Lam Shek, Pratap Tokekar', 'link': 'https://arxiv.org/abs/2503.19007', 'abstract': 'Large Language Models (LLMs) have shown remarkable promise in reasoning and decision-making, yet their integration with Reinforcement Learning (RL) for complex robotic tasks remains underexplored. In this paper, we propose an LLM-guided hierarchical RL framework, termed LDSC, that leverages LLM-driven subgoal selection and option reuse to enhance sample efficiency, generalization, and multi-task adaptability. Traditional RL methods often suffer from inefficient exploration and high computational cost. Hierarchical RL helps with these challenges, but existing methods often fail to reuse options effectively when faced with new tasks. To address these limitations, we introduce a three-stage framework that uses LLMs for subgoal generation given natural language description of the task, a reusable option learning and selection method, and an action-level policy, enabling more effective decision-making across diverse tasks. By incorporating LLMs for subgoal prediction and policy guidance, our approach improves exploration efficiency and enhances learning performance. On average, LDSC outperforms the baseline by 55.9\\% in average reward, demonstrating its effectiveness in complex RL settings. More details and experiment videos could be found in \\href{this https URL}{this link\\footnote{this https URL}}.', 'abstract_zh': '大型语言模型（LLMs）在推理和决策方面展现了显著的潜力，然而它们与强化学习（RL）在复杂机器人任务中的集成仍处于未探索阶段。本文提出了一种名为LDSC的LLM引导的分层RL框架，该框架通过LLM驱动的子目标选择和选项重用，提高样本效率、泛化能力和多任务适应性。传统RL方法常因探索效率低和高计算成本而受挫。分层RL有助于解决这些问题，但现有方法在面对新任务时往往无法有效重用选项。为了解决这些局限性，我们提出了一种三阶段框架，该框架利用LLM根据任务的自然语言描述生成子目标，一种可重用选项的学习和选择方法，以及一个动作层面的策略，从而在多样化的任务中实现更有效的决策。通过将LLM集成到子目标预测和策略指导中，我们的方法提高了探索效率并增强了学习性能。LDSC在平均奖励方面的平均表现比基线高出55.9%，证明了其在复杂RL环境中的有效性。更多细节和实验视频请参见\\href{this https URL}{此链接\\footnote{this https URL}}。', 'title_zh': '基于LLM引导的语义层次强化学习的选项发现'}
{'arxiv_id': 'arXiv:2503.19006', 'title': 'Computational Thinking with Computer Vision: Developing AI Competency in an Introductory Computer Science Course', 'authors': 'Tahiya Chowdhury', 'link': 'https://arxiv.org/abs/2503.19006', 'abstract': "Developing competency in artificial intelligence is becoming increasingly crucial for computer science (CS) students at all levels of the CS curriculum. However, most previous research focuses on advanced CS courses, as traditional introductory courses provide limited opportunities to develop AI skills and knowledge. This paper introduces an introductory CS course where students learn computational thinking through computer vision, a sub-field of AI, as an application context. The course aims to achieve computational thinking outcomes alongside critical thinking outcomes that expose students to AI approaches and their societal implications. Through experiential activities such as individual projects and reading discussions, our course seeks to balance technical learning and critical thinking goals. Our evaluation, based on pre-and post-course surveys, shows an improved sense of belonging, self-efficacy, and AI ethics awareness among students. The results suggest that an AI-focused context can enhance participation and employability, student-selected projects support self-efficacy, and ethically grounded AI instruction can be effective for interdisciplinary audiences. Students' discussions on reading assignments demonstrated deep engagement with the complex challenges in today's AI landscape. Finally, we share insights on scaling such courses for larger cohorts and improving the learning experience for introductory CS students.", 'abstract_zh': '人工智能领域的技能培养正日益成为计算机科学（CS）学生跨所有CS课程层级的关键能力。然而，大多数先前的研究集中在高级CS课程上，因为传统的入门课程为培养AI技能和知识提供了有限的机会。本文介绍了一门入门级CS课程，在该课程中，学生通过计算机视觉——AI的一个子领域——学习计算思维，以此作为应用背景。该课程旨在通过介绍AI方法及其社会影响来实现计算思维和批判性思维的结果。通过个人项目和阅读讨论等体验性活动，我们的课程寻求在技术学习和批判性思维目标之间取得平衡。基于课前和课后的调查，我们的评估结果显示学生对归属感、自我效能感和AI伦理意识有所增强。研究表明，以AI为重点的背景可以增强参与度和就业能力，学生选择的项目可以支持自我效能感，基于伦理的教学可以有效地服务于跨学科听众。学生在阅读任务上的讨论表明，他们对当今AI领域的复杂挑战有深入的理解。最后，我们分享了扩展此类课程以容纳更大班级规模并改善入门级CS学生学习体验的见解。', 'title_zh': '基于计算机视觉的计算思维：在入门计算机科学课程中培养人工智能能力'}
{'arxiv_id': 'arXiv:2503.19001', 'title': 'DisentTalk: Cross-lingual Talking Face Generation via Semantic Disentangled Diffusion Model', 'authors': 'Kangwei Liu, Junwu Liu, Yun Cao, Jinlin Guo, Xiaowei Yi', 'link': 'https://arxiv.org/abs/2503.19001', 'abstract': 'Recent advances in talking face generation have significantly improved facial animation synthesis. However, existing approaches face fundamental limitations: 3DMM-based methods maintain temporal consistency but lack fine-grained regional control, while Stable Diffusion-based methods enable spatial manipulation but suffer from temporal inconsistencies. The integration of these approaches is hindered by incompatible control mechanisms and semantic entanglement of facial representations. This paper presents DisentTalk, introducing a data-driven semantic disentanglement framework that decomposes 3DMM expression parameters into meaningful subspaces for fine-grained facial control. Building upon this disentangled representation, we develop a hierarchical latent diffusion architecture that operates in 3DMM parameter space, integrating region-aware attention mechanisms to ensure both spatial precision and temporal coherence. To address the scarcity of high-quality Chinese training data, we introduce CHDTF, a Chinese high-definition talking face dataset. Extensive experiments show superior performance over existing methods across multiple metrics, including lip synchronization, expression quality, and temporal consistency. Project Page: this https URL.', 'abstract_zh': '最近在说话人脸生成方面的进展显著提高了面部动画合成的质量。然而，现有方法面临根本性的局限性：基于3DMM的方法保持了时间一致性，但在区域细节控制上不足，而基于Stable Diffusion的方法能够实现空间操控，但在时间一致性上存在问题。这些方法的整合受到不兼容控制机制和面部表示语义纠缠的阻碍。本文提出DisentTalk，介绍了一种数据驱动的语义解纠缠框架，将3DMM表情参数分解为有意义的子空间，以实现精细的面部控制。在此解纠缠表示的基础上，我们开发了一种分层的隐空间扩散架构，该架构在3DMM参数空间中运行，结合区域意识的注意力机制，确保时空一致性。为了解决高质量中文训练数据稀缺的问题，我们引入了CHDTF中文高分辨率说话人脸数据集。广泛实验表明，在多个指标上，包括唇同步、表情质量和时间一致性等方面，该方法优于现有方法。项目页面：这个 https URL。', 'title_zh': 'DisentTalk：语义解耦扩散模型驱动的跨语言表情生成'}
{'arxiv_id': 'arXiv:2503.18998', 'title': 'FACE: Few-shot Adapter with Cross-view Fusion for Cross-subject EEG Emotion Recognition', 'authors': 'Haiqi Liu, C. L. Philip Chen, Tong Zhang', 'link': 'https://arxiv.org/abs/2503.18998', 'abstract': "Cross-subject EEG emotion recognition is challenged by significant inter-subject variability and intricately entangled intra-subject variability. Existing works have primarily addressed these challenges through domain adaptation or generalization strategies. However, they typically require extensive target subject data or demonstrate limited generalization performance to unseen subjects. Recent few-shot learning paradigms attempt to address these limitations but often encounter catastrophic overfitting during subject-specific adaptation with limited samples. This article introduces the few-shot adapter with a cross-view fusion method called FACE for cross-subject EEG emotion recognition, which leverages dynamic multi-view fusion and effective subject-specific adaptation. Specifically, FACE incorporates a cross-view fusion module that dynamically integrates global brain connectivity with localized patterns via subject-specific fusion weights to provide complementary emotional information. Moreover, the few-shot adapter module is proposed to enable rapid adaptation for unseen subjects while reducing overfitting by enhancing adapter structures with meta-learning. Experimental results on three public EEG emotion recognition benchmarks demonstrate FACE's superior generalization performance over state-of-the-art methods. FACE provides a practical solution for cross-subject scenarios with limited labeled data.", 'abstract_zh': '跨被试的EEG情绪识别受到显著的被试间变异性及复杂交织的被试内变异性挑战。现有的工作主要通过领域适应或泛化策略来应对这些挑战，但通常需要大量的目标被试数据或在未见被试上的泛化性能有限。最近的少样本学习范式试图解决这些局限性，但在有限样本下的被试特异性适应中常常遇到灾难性的过拟合。本文介绍了利用跨视角融合方法FACE进行跨被试EEG情绪识别的少样本适配器，该方法利用动态多视角融合和有效的被试特异性适应。具体来说，FACE整合了一个跨视角融合模块，该模块通过被试特异性融合权重动态地将全局脑连接与局部模式结合起来，提供互补的情绪信息。此外，提出了少样本适配器模块，能够通过增强适配器结构以元学习的方式减少过拟合来实现对未见被试的快速适应。在三个公开的EEG情绪识别基准上的实验结果表明，FACE在泛化性能上优于现有方法。FACE为有限标注数据的跨被试场景提供了一个实用的解决方案。', 'title_zh': 'FACE: 少量样本适配器与跨视图融合在跨被试EEG情绪识别中的应用'}
{'arxiv_id': 'arXiv:2503.18996', 'title': 'Enhanced prediction of spine surgery outcomes using advanced machine learning techniques and oversampling methods', 'authors': 'José Alberto Benítez-Andrades, Camino Prada-García, Nicolás Ordás-Reyes, Marta Esteban Blanco, Alicia Merayo, Antonio Serrano-García', 'link': 'https://arxiv.org/abs/2503.18996', 'abstract': 'The study proposes an advanced machine learning approach to predict spine surgery outcomes by incorporating oversampling techniques and grid search optimization. A variety of models including GaussianNB, ComplementNB, KNN, Decision Tree, and optimized versions with RandomOverSampler and SMOTE were tested on a dataset of 244 patients, which included pre-surgical, psychometric, socioeconomic, and analytical variables. The enhanced KNN models achieved up to 76% accuracy and a 67% F1-score, while grid-search optimization further improved performance. The findings underscore the potential of these advanced techniques to aid healthcare professionals in decision-making, with future research needed to refine these models on larger and more diverse datasets.', 'abstract_zh': '该研究提出一种先进的机器学习方法，通过结合过_sampling技术与网格搜索优化以预测脊柱手术结果。该方法在包含预手术、心理测量、社会经济和分析变量的244名患者数据集上测试了多种模型，包括GaussianNB、ComplementNB、KNN、决策树以及使用RandomOverSampler和SMOTE优化的版本。增强的KNN模型达到了最高76%的准确率和67%的F1分数，而网格搜索优化进一步提升了性能。研究结果强调了这些高级技术在辅助医疗专业人员决策方面的潜力，未来的研究需要在更大和更具多样性的数据集上细化这些模型。', 'title_zh': '使用高级机器学习技术和过采样方法增强脊柱手术结果预测'}
{'arxiv_id': 'arXiv:2503.18995', 'title': 'LLMs in the Classroom: Outcomes and Perceptions of Questions Written with the Aid of AI', 'authors': 'Gavin Witsken, Igor Crk, Eren Gultepe', 'link': 'https://arxiv.org/abs/2503.18995', 'abstract': "We randomly deploy questions constructed with and without use of the LLM tool and gauge the ability of the students to correctly answer, as well as their ability to correctly perceive the difference between human-authored and LLM-authored questions. In determining whether the questions written with the aid of ChatGPT were consistent with the instructor's questions and source text, we computed representative vectors of both the human and ChatGPT questions using SBERT and compared cosine similarity to the course textbook. A non-significant Mann-Whitney U test (z = 1.018, p = .309) suggests that students were unable to perceive whether questions were written with or without the aid of ChatGPT. However, student scores on LLM-authored questions were almost 9% lower (z = 2.702, p < .01). This result may indicate that either the AI questions were more difficult or that the students were more familiar with the instructor's style of questions. Overall, the study suggests that while there is potential for using LLM tools to aid in the construction of assessments, care must be taken to ensure that the questions are fair, well-composed, and relevant to the course material.", 'abstract_zh': '我们随机部署使用和未使用LLM工具构建的问题，并评估学生正确作答以及正确区分人类编写和LLM编写问题的能力。通过计算SBERT表示的人类和ChatGPT生成的问题向量与课程教材的余弦相似性，我们确定由ChatGPT辅助编写的问题是否与教师的问题和源文本一致。非显著性的Mann-Whitney U检验（z = 1.018，p = .309）表明，学生无法区分由ChatGPT辅助编写的问题和未辅助编写的问题。然而，由LLM编写的问题的学生得分平均低了几乎9%（z = 2.702，p < .01）。这一结果可能表明，要么AI问题更难，要么学生更熟悉教师的问题风格。总体而言，研究结果表明，在利用LLM工具辅助构建评估时，应注意确保问题公平、结构良好且与课程内容相关。', 'title_zh': 'AI辅助下生成的问题：课堂应用的效果与感知研究'}
{'arxiv_id': 'arXiv:2503.18994', 'title': 'HH4AI: A methodological Framework for AI Human Rights impact assessment under the EUAI ACT', 'authors': "Paolo Ceravolo, Ernesto Damiani, Maria Elisa D'Amico, Bianca de Teffe Erb, Simone Favaro, Nannerel Fiano, Paolo Gambatesa, Simone La Porta, Samira Maghool, Lara Mauri, Niccolo Panigada, Lorenzo Maria Ratto Vaquer, Marta A. Tamborini", 'link': 'https://arxiv.org/abs/2503.18994', 'abstract': 'This paper introduces the HH4AI Methodology, a structured approach to assessing the impact of AI systems on human rights, focusing on compliance with the EU AI Act and addressing technical, ethical, and regulatory challenges. The paper highlights AIs transformative nature, driven by autonomy, data, and goal-oriented design, and how the EU AI Act promotes transparency, accountability, and safety. A key challenge is defining and assessing "high-risk" AI systems across industries, complicated by the lack of universally accepted standards and AIs rapid evolution.\nTo address these challenges, the paper explores the relevance of ISO/IEC and IEEE standards, focusing on risk management, data quality, bias mitigation, and governance. It proposes a Fundamental Rights Impact Assessment (FRIA) methodology, a gate-based framework designed to isolate and assess risks through phases including an AI system overview, a human rights checklist, an impact assessment, and a final output phase. A filtering mechanism tailors the assessment to the system\'s characteristics, targeting areas like accountability, AI literacy, data governance, and transparency.\nThe paper illustrates the FRIA methodology through a fictional case study of an automated healthcare triage service. The structured approach enables systematic filtering, comprehensive risk assessment, and mitigation planning, effectively prioritizing critical risks and providing clear remediation strategies. This promotes better alignment with human rights principles and enhances regulatory compliance.', 'abstract_zh': '本文介绍了HH4AI方法论，这是一种结构化的评估人工智能系统对人权影响的方法，重点关注欧盟AI法案的合规性，并解决技术、伦理和监管挑战。本文强调了人工智能的变革性质，受自主性、数据和以目标为导向的设计驱动，并指出欧盟AI法案如何促进透明度、问责制和安全性。一个关键挑战是定义和评估跨行业的“高风险”人工智能系统，这由于缺乏普遍接受的标准以及人工智能的快速发展而变得复杂。\n为应对这些挑战，本文探讨了ISO/IEC和IEEE标准的相关性，重点关注风险管理、数据质量、偏见缓解和治理。本文提出了一种基本权利影响评估（FRIA）方法论，这是一种基于门控的框架，通过包括人工智能系统概览、人权检查清单、影响评估和最终输出阶段的阶段来隔离和评估风险。过滤机制根据系统的特性进行了定制，以针对问责制、人工智能素养、数据治理和透明度等领域。\n本文通过一个虚构的自动化医疗服务分诊案例研究说明了FRIA方法论。这一结构化方法使系统性的筛选、全面的风险评估和缓解规划成为可能，有效优先考虑关键风险并提供清晰的补救策略，从而更好地与人权原则保持一致，并增强合规性。', 'title_zh': 'HH4AI: EUAI ACT背景下的人工智能人权影响评估方法框架'}
{'arxiv_id': 'arXiv:2503.18991', 'title': 'SRMIR: Shadow Reward Models Based on Introspective Reasoning for LLM Alignment', 'authors': 'Ruoxi Cheng, Shuirong Cao', 'link': 'https://arxiv.org/abs/2503.18991', 'abstract': 'Aligning large language models (LLMs) with human preferences and values is vital for application. However, current alignment methods face three main limitations: (1) reliance on costly human annotation; (2) alignment tax; (3) shallow alignment vulnerable to jailbreak attacks. Additionally, current alignment datasets often suffer from uneven distributions, leading to overrepresentation of some topics and neglect of others. To address these issues, we propose SRMIR (Shadow Reward Models Based on Introspective Reasoning), inspired by shadow models in membership inference attacks. We first construct a balanced safety Chain of Draft (CoD) dataset across $7$ harmful types with structured prompt leveraging the introspective reasoning capabilities of LLMs, then train a set of specialized reward models to guide policy optimization through Group Relative Policy Optimization (GRPO). We apply two strategies, linear combination and categorized approach, to integrate shadow reward models for policy optimization. By comparison, we find that the latter achieves superior alignment despite higher computational costs. Experiments across several LLMs demonstrate SRMIR significantly outperforms existing methods.', 'abstract_zh': '基于内省推理的阴影奖励模型（SRMIR）：解决大型语言模型alignment问题', 'title_zh': 'SRMIR：基于反省推理的影子奖励模型用于LLM对齐'}
{'arxiv_id': 'arXiv:2503.18988', 'title': 'SG-Tailor: Inter-Object Commonsense Relationship Reasoning for Scene Graph Manipulation', 'authors': 'Haoliang Shang, Hanyu Wu, Guangyao Zhai, Boyang Sun, Fangjinhua Wang, Federico Tombari, Marc Pollefeys', 'link': 'https://arxiv.org/abs/2503.18988', 'abstract': "Scene graphs capture complex relationships among objects, serving as strong priors for content generation and manipulation. Yet, reasonably manipulating scene graphs -- whether by adding nodes or modifying edges -- remains a challenging and untouched task. Tasks such as adding a node to the graph or reasoning about a node's relationships with all others are computationally intractable, as even a single edge modification can trigger conflicts due to the intricate interdependencies within the graph. To address these challenges, we introduce SG-Tailor, an autoregressive model that predicts the conflict-free relationship between any two nodes. SG-Tailor not only infers inter-object relationships, including generating commonsense edges for newly added nodes but also resolves conflicts arising from edge modifications to produce coherent, manipulated graphs for downstream tasks. For node addition, the model queries the target node and other nodes from the graph to predict the appropriate relationships. For edge modification, SG-Tailor employs a Cut-And-Stitch strategy to solve the conflicts and globally adjust the graph. Extensive experiments demonstrate that SG-Tailor outperforms competing methods by a large margin and can be seamlessly integrated as a plug-in module for scene generation and robotic manipulation tasks.", 'abstract_zh': '场景图捕获对象之间复杂的相互关系，作为内容生成和操控的强大先验。然而，合理地操控场景图——无论是添加节点还是修改边——仍然是一个具有挑战性和未被充分探讨的任务。诸如向图中添加节点或推断节点与所有其他节点的关系之类的任务在计算上是不可行的，因为即使是单个边的修改也可能由于图内部复杂的相互依赖而引发冲突。为了解决这些挑战，我们引入了SG-Tailor，这是一种自回归模型，用于预测图中任意两个节点之间的无冲突关系。SG-Tailor 不仅推断物体间的相互关系，包括为新添加的节点生成常识边，还能解决边修改引发的冲突，生成连贯的、被操控的图以供下游任务使用。对于节点添加，模型从图中查询目标节点和其他节点来预测适当的相互关系。对于边修改，SG-Tailor 使用剪切和缝合策略来解决冲突并全局调整图。广泛实验表明，SG-Tailor 在性能上大幅优于竞争方法，并可无缝集成到场景生成和机器人操控任务的插件模块中。', 'title_zh': 'SG-Tailor: 不同对象常识关系推理以操纵场景图'}
{'arxiv_id': 'arXiv:2503.18987', 'title': 'Balanced Direction from Multifarious Choices: Arithmetic Meta-Learning for Domain Generalization', 'authors': 'Xiran Wang, Jian Zhang, Lei Qi, Yinghuan Shi', 'link': 'https://arxiv.org/abs/2503.18987', 'abstract': 'Domain generalization is proposed to address distribution shift, arising from statistical disparities between training source and unseen target domains. The widely used first-order meta-learning algorithms demonstrate strong performance for domain generalization by leveraging the gradient matching theory, which aims to establish balanced parameters across source domains to reduce overfitting to any particular domain. However, our analysis reveals that there are actually numerous directions to achieve gradient matching, with current methods representing just one possible path. These methods actually overlook another critical factor that the balanced parameters should be close to the centroid of optimal parameters of each source domain. To address this, we propose a simple yet effective arithmetic meta-learning with arithmetic-weighted gradients. This approach, while adhering to the principles of gradient matching, promotes a more precise balance by estimating the centroid between domain-specific optimal parameters. Experimental results validate the effectiveness of our strategy.', 'abstract_zh': '域泛化用于解决由于训练源域和未见过的目标域之间统计差异导致的分布偏移问题。广泛使用的基于一阶元学习算法通过梯度匹配理论在域泛化任务中展现了强大的性能，该理论旨在通过建立跨源域的平衡参数来减少对任何特定域的过拟合。然而，我们的分析表明，实际上存在许多实现梯度匹配的方向，当前方法仅代表其中一种路径。这些方法实际上忽略了另一个关键因素，即平衡参数应接近每个源域最优参数的质心。为了解决这一问题，我们提出了一个简单有效的算术元学习方法，该方法以算术加权梯度为特征，遵循梯度匹配的原则，通过估计不同源域最优参数的质心来促进更精确的平衡。实验结果验证了该策略的有效性。', 'title_zh': '多方面选择中的平衡方向：领域泛化的算术元学习'}
{'arxiv_id': 'arXiv:2503.18986', 'title': 'SplitFrozen: Split Learning with Device-side Model Frozen for Fine-Tuning LLM on Heterogeneous Resource-Constrained Devices', 'authors': 'Jian Ma, Xinchen Lyu, Jun Jiang, Qimei Cui, Haipeng Yao, Xiaofeng Tao', 'link': 'https://arxiv.org/abs/2503.18986', 'abstract': 'Fine-tuning large language models (LLMs) on private, on-device data can empower tailored personalized AI agents. However, fine-tuning LLMs on resource-constrained edge devices faces significant challenges, including excessive computation overhead, device heterogeneity, and data imbalance. This paper proposes SplitFrozen, a split learning framework that enables efficient LLM fine-tuning by strategically freezing device-side model layers while centralizing parameter-efficient fine-tuning on the server. Our framework partitions LLMs into device-side frozen layers and server-side fine-tuning layers, where heterogeneous resource-constrained devices execute only forward propagation. To minimize server-side training costs, we integrate Low-Rank Adaptation (LoRA) into the server-side layers. A pipeline parallelism strategy further optimizes training efficiency by decoupling device-server computations and leveraging decomposed backward propagation. Experiments on GPT-2 with the MRPC, MNLI-matched, and SST-2 datasets demonstrate that SplitFrozen outperforms FedLoRA and SplitLoRA by 69.4\\% model accuracy under extremely imbalanced data, while reducing up to 86.8\\% device-side computations and 50.2\\% total training time. Experiments also validate the scalability of SplitFrozen on content generation task using Llama-3.2 model on GSM8K dataset.', 'abstract_zh': 'SplitFrozen：一种分拆冻结的高效大语言模型细调框架', 'title_zh': 'SplitFrozen：面向异构资源受限设备的设备端模型冻结分割学习及大模型微调'}
{'arxiv_id': 'arXiv:2503.18985', 'title': 'LoRA Subtraction for Drift-Resistant Space in Exemplar-Free Continual Learning', 'authors': 'Xuan Liu, Xiaobin Chang', 'link': 'https://arxiv.org/abs/2503.18985', 'abstract': 'In continual learning (CL), catastrophic forgetting often arises due to feature drift. This challenge is particularly prominent in the exemplar-free continual learning (EFCL) setting, where samples from previous tasks cannot be retained, making it difficult to preserve prior knowledge. To address this issue, some EFCL methods aim to identify feature spaces that minimize the impact on previous tasks while accommodating new ones. However, they rely on static features or outdated statistics stored from old tasks, which prevents them from capturing the dynamic evolution of the feature space in CL, leading to performance degradation over time. In this paper, we introduce the Drift-Resistant Space (DRS), which effectively handles feature drifts without requiring explicit feature modeling or the storage of previous tasks. A novel parameter-efficient fine-tuning approach called Low-Rank Adaptation Subtraction (LoRA-) is proposed to develop the DRS. This method subtracts the LoRA weights of old tasks from the initial pre-trained weight before processing new task data to establish the DRS for model training. Therefore, LoRA- enhances stability, improves efficiency, and simplifies implementation. Furthermore, stabilizing feature drifts allows for better plasticity by learning with a triplet loss. Our method consistently achieves state-of-the-art results, especially for long task sequences, across multiple datasets.', 'abstract_zh': '在持续学习中，无例集的持续学习（EFCL）设置下特征漂移常导致灾难性遗忘。为此，本文提出了一种称为Drift-Resistant Space（DRS）的方法，无需显式建模特征或存储先前任务的数据，有效处理特征漂移。本文还提出了一种新的参数高效的微调方法——低秩适应减法（LoRA-），通过从初始化的预训练权重中减去老任务的LoRA权重来构建DRS，以适应新任务数据进行模型训练。LoRA-方法提高了稳定性、提高了效率并简化了实现。此外，稳定特征漂移有助于通过三元损失学习提高模型的可塑性。实验结果表明，该方法在多个数据集上，特别是在长任务序列上，达到了最先进的性能。', 'title_zh': 'LoRA子空间减法用于无示例持续学习中的漂移抵抗空间'}
{'arxiv_id': 'arXiv:2503.18983', 'title': 'Confronting Catastrophic Risk: The International Obligation to Regulate Artificial Intelligence', 'authors': 'Bryan Druzin, Anatole Boute, Michael Ramsden', 'link': 'https://arxiv.org/abs/2503.18983', 'abstract': 'While artificial intelligence (AI) holds enormous promise, many experts in the field are warning that there is a non-trivial chance that the development of AI poses an existential threat to humanity. Existing regulatory initiative do not address this threat but merely instead focus on discrete AI-related risks such as consumer safety, cybersecurity, data protection, and privacy. In the absence of regulatory action to address the possible risk of human extinction by AI, the question arises: What legal obligations, if any, does public international law impose on states to regulate its development. Grounded in the precautionary principle, we argue that there exists an international obligation to mitigate the threat of human extinction by AI. Often invoked in relation to environmental regulation and the regulation of potentially harmful technologies, the principle holds that in situations where there is the potential for significant harm, even in the absence of full scientific certainty, preventive measures should not be postponed if delayed action may result in irreversible consequences. We argue that the precautionary principle is a general principle of international law and, therefore, that there is a positive obligation on states under the right to life within international human rights law to proactively take regulatory action to mitigate the potential existential risk of AI. This is significant because, if an international obligation to regulate the development of AI can be established under international law, then the basic legal framework would be in place to address this evolving threat.', 'abstract_zh': '人工智能的发展存在本质性威胁，国际法是否有规制义务', 'title_zh': '应对 catastrophic 风险：国际 regulating 人工智能的义务'}
{'arxiv_id': 'arXiv:2503.18982', 'title': 'Generative Data Imputation for Sparse Learner Performance Data Using Generative Adversarial Imputation Networks', 'authors': 'Liang Zhang, Jionghao Lin, John Sabatini, Diego Zapata-Rivera, Carol Forsyth, Yang Jiang, John Hollander, Xiangen Hu, Arthur C. Graesser', 'link': 'https://arxiv.org/abs/2503.18982', 'abstract': "Learner performance data collected by Intelligent Tutoring Systems (ITSs), such as responses to questions, is essential for modeling and predicting learners' knowledge states. However, missing responses due to skips or incomplete attempts create data sparsity, challenging accurate assessment and personalized instruction. To address this, we propose a generative imputation approach using Generative Adversarial Imputation Networks (GAIN). Our method features a three-dimensional (3D) framework (learners, questions, and attempts), flexibly accommodating various sparsity levels. Enhanced by convolutional neural networks and optimized with a least squares loss function, the GAIN-based method aligns input and output dimensions to question-attempt matrices along the learners' dimension. Extensive experiments using datasets from AutoTutor Adult Reading Comprehension (ARC), ASSISTments, and MATHia demonstrate that our approach significantly outperforms tensor factorization and alternative GAN methods in imputation accuracy across different attempt scenarios. Bayesian Knowledge Tracing (BKT) further validates the effectiveness of the imputed data by estimating learning parameters: initial knowledge (P(L0)), learning rate (P(T)), guess rate (P(G)), and slip rate (P(S)). Results indicate the imputed data enhances model fit and closely mirrors original distributions, capturing underlying learning behaviors reliably. Kullback-Leibler (KL) divergence assessments confirm minimal divergence, showing the imputed data preserves essential learning characteristics effectively. These findings underscore GAIN's capability as a robust imputation tool in ITSs, alleviating data sparsity and supporting adaptive, individualized instruction, ultimately leading to more precise and responsive learner assessments and improved educational outcomes.", 'abstract_zh': '基于生成对抗填充网络的智能辅导系统缺失响应数据填充方法', 'title_zh': '使用生成对抗插补网络的生成型数据插补法处理稀疏学习者绩效数据'}
{'arxiv_id': 'arXiv:2503.18981', 'title': 'FedSKD: Aggregation-free Model-heterogeneous Federated Learning using Multi-dimensional Similarity Knowledge Distillation', 'authors': 'Ziqiao Weng, Weidong Cai, Bo Zhou', 'link': 'https://arxiv.org/abs/2503.18981', 'abstract': "Federated learning (FL) enables privacy-preserving collaborative model training without direct data sharing. Model-heterogeneous FL (MHFL) extends this paradigm by allowing clients to train personalized models with heterogeneous architectures tailored to their computational resources and application-specific needs. However, existing MHFL methods predominantly rely on centralized aggregation, which introduces scalability and efficiency bottlenecks, or impose restrictions requiring partially identical model architectures across clients. While peer-to-peer (P2P) FL removes server dependence, it suffers from model drift and knowledge dilution, limiting its effectiveness in heterogeneous settings. To address these challenges, we propose FedSKD, a novel MHFL framework that facilitates direct knowledge exchange through round-robin model circulation, eliminating the need for centralized aggregation while allowing fully heterogeneous model architectures across clients. FedSKD's key innovation lies in multi-dimensional similarity knowledge distillation, which enables bidirectional cross-client knowledge transfer at batch, pixel/voxel, and region levels for heterogeneous models in FL. This approach mitigates catastrophic forgetting and model drift through progressive reinforcement and distribution alignment while preserving model heterogeneity. Extensive evaluations on fMRI-based autism spectrum disorder diagnosis and skin lesion classification demonstrate that FedSKD outperforms state-of-the-art heterogeneous and homogeneous FL baselines, achieving superior personalization (client-specific accuracy) and generalization (cross-institutional adaptability). These findings underscore FedSKD's potential as a scalable and robust solution for real-world medical federated learning applications.", 'abstract_zh': '联邦学习（FL）在不直接共享数据的情况下实现隐私保护的协作模型训练。异构模型联邦学习（MHFL）通过允许客户端使用针对其计算资源和应用特定需求定制的异构架构训练个性化模型，扩展了这一范式。然而，现有的MHFL方法主要依赖中心化聚合，这引入了可扩展性和效率瓶颈，或者需要客户端之间部分相同的模型架构。虽然对等（P2P）FL去除了对服务器的依赖，但它遭受模型漂移和知识稀释的问题，限制了其在异构环境中的有效性。为了解决这些挑战，我们提出了一种名为FedSKD的新型MHFL框架，该框架通过轮询模型循环直接促进知识交流，无需中心化聚合，同时允许客户端之间具有完全异构的模型架构。FedSKD的核心创新在于多维度相似性知识蒸馏，这使得在批处理、像素/体素和区域级别，异构模型在FL中能够进行双向跨客户端知识传输。这种方法通过渐进强化和分布对齐来缓解灾难性遗忘和模型漂移，同时保持模型异质性。在基于fMRI的自闭症谱系障碍诊断和皮肤病变分类的广泛评估中，FedSKD表现出色，优于最先进的异构和同构FL基线，实现了更好的个性化（客户端特定准确度）和泛化能力（跨机构适应性）。这些发现强调了FedSKD作为适用于实际医疗联邦学习应用的可扩展和稳健解决方案的潜力。', 'title_zh': 'FedSKD：基于多维度相似性知识蒸馏的无聚合模型异构联邦学习'}
{'arxiv_id': 'arXiv:2503.18980', 'title': 'CAE: Repurposing the Critic as an Explorer in Deep Reinforcement Learning', 'authors': 'Yexin Li, Pring Wong, Hanfang Zhang, Shuo Chen, Siyuan Qi', 'link': 'https://arxiv.org/abs/2503.18980', 'abstract': 'Exploration remains a critical challenge in reinforcement learning, as many existing methods either lack theoretical guarantees or fall short of practical effectiveness. In this paper, we introduce CAE, a lightweight algorithm that repurposes the value networks in standard deep RL algorithms to drive exploration without introducing additional parameters. CAE utilizes any linear multi-armed bandit technique and incorporates an appropriate scaling strategy, enabling efficient exploration with provable sub-linear regret bounds and practical stability. Notably, it is simple to implement, requiring only around 10 lines of code. In complex tasks where learning an effective value network proves challenging, we propose CAE+, an extension of CAE that incorporates an auxiliary network. This extension increases the parameter count by less than 1% while maintaining implementation simplicity, adding only about 10 additional lines of code. Experiments on MuJoCo and MiniHack show that both CAE and CAE+ outperform state-of-the-art baselines, bridging the gap between theoretical rigor and practical efficiency.', 'abstract_zh': '探索仍然是强化学习中的一个关键挑战，因为许多现有方法要么缺乏理论保证，要么在实际效果上有所不足。本文介绍了一种轻量级算法CAE，该算法通过重用标准深度强化学习算法中的价值网络来驱动探索，而不引入额外参数。CAE 利用任何线性多臂老虎机技术，并结合适当的缩放策略，实现了可证明的亚线性后悔界和实际稳定性。值得注意的是，CAE 实现非常简单，只需约10行代码。在学习有效价值网络具有挑战性的复杂任务中，我们提出了一种名为CAE+的扩展算法，该扩展算法结合了一个辅助网络。这种扩展将参数数量增加了不到1% ，同时保持实现的简单性，仅增加了约10行代码。实验结果表明，CAE 和 CAE+ 在MuJoCo 和 MiniHack 上均优于最先进的基线方法，实现了理论严谨性和实际效率之间的平衡。', 'title_zh': 'CAE: 将评论者重新利用为探索者在深度强化学习中的应用'}
{'arxiv_id': 'arXiv:2503.18979', 'title': 'Threshold Crossings as Tail Events for Catastrophic AI Risk', 'authors': 'Elija Perrier', 'link': 'https://arxiv.org/abs/2503.18979', 'abstract': "We analyse circumstances in which bifurcation-driven jumps in AI systems with their emergent heavy-tailed outcome distributions. By analysing how a control parameter's random fluctuations near a catastrophic threshold generate extreme outcomes, we demonstrate in what circumstances the probability of a sudden, large-scale, transition aligns closely with the tail probability of the resulting damage distribution. Our results contribute to research in monitoring, mitigation and control of AI systems when seeking to manage potentially catastrophic AI risk.", 'abstract_zh': '我们分析由分叉驱动的AI系统中 Emergent 稀尾结果分布的跃变情况。通过分析控制参数在灾难性阈值附近随机波动如何产生极端结果，我们展示了在何种情况下突然的大规模转变的概率与结果损害分布的尾部概率高度一致。我们的研究结果有助于在管理潜在灾难性AI风险时对AI系统的监控、缓解和控制方面的研究。', 'title_zh': '阈值穿越作为灾难性AI风险的尾事件'}
{'arxiv_id': 'arXiv:2503.18976', 'title': 'Synthetic media and computational capitalism: towards a critical theory of artificial intelligence', 'authors': 'David M. Berry', 'link': 'https://arxiv.org/abs/2503.18976', 'abstract': 'This paper develops a critical theory of artificial intelligence, within a historical constellation where computational systems increasingly generate cultural content that destabilises traditional distinctions between human and machine production. Through this analysis, I introduce the concept of the algorithmic condition, a cultural moment when machine-generated work not only becomes indistinguishable from human creation but actively reshapes our understanding of ideas of authenticity. This transformation, I argue, moves beyond false consciousness towards what I call post-consciousness, where the boundaries between individual and synthetic consciousness become porous. Drawing on critical theory and extending recent work on computational ideology, I develop three key theoretical contributions, first, the concept of the Inversion to describe a new computational turn in algorithmic society; second, automimetric production as a framework for understanding emerging practices of automated value creation; and third, constellational analysis as a methodological approach for mapping the complex interplay of technical systems, cultural forms and political economic structures. Through these contributions, I argue that we need new critical methods capable of addressing both the technical specificity of AI systems and their role in restructuring forms of life under computational capitalism. The paper concludes by suggesting that critical reflexivity is needed to engage with the algorithmic condition without being subsumed by it and that it represents a growing challenge for contemporary critical theory.', 'abstract_zh': '人工智能的批判理论：计算系统生成的文化内容对人类与机器生产传统区别的解稳动', 'title_zh': '合成媒体与计算资本主义：迈向对人工智能的批判性理论'}
{'arxiv_id': 'arXiv:2503.18975', 'title': 'Machine Learning - Driven Materials Discovery: Unlocking Next-Generation Functional Materials - A minireview', 'authors': 'Dilshod Nematov, Mirabbos Hojamberdiev', 'link': 'https://arxiv.org/abs/2503.18975', 'abstract': 'The rapid advancement of machine learning and artificial intelligence (AI)-driven techniques is revolutionizing materials discovery, property prediction, and material design by minimizing human intervention and accelerating scientific progress. This review provides a comprehensive overview of smart, machine learning (ML)-driven approaches, emphasizing their role in predicting material properties, discovering novel compounds, and optimizing material structures. Key methodologies ranging from deep learning, graph neural networks, and Bayesian optimization to automated generative models, such as generative adversarial networks (GANs) and variational autoencoders (VAEs) enable the autonomous design of materials with tailored functionalities. By leveraging AutoML frameworks (e.g., AutoGluon, TPOT, and this http URL), researchers can automate the model selection, hyperparameter tuning, and feature engineering, significantly improving the efficiency of materials informatics. Furthermore, the integration of AI-driven robotic laboratories and high-throughput computing has established a fully automated pipeline for rapid synthesis and experimental validation, drastically reducing the time and cost of material discovery. This review highlights real-world applications of automated ML-driven approaches in predicting mechanical, thermal, electrical, and optical properties of materials, demonstrating successful cases in superconductors, catalysts, photovoltaics, and energy storage systems. We also address key challenges, such as data quality, interpretability, and the integration of AutoML with quantum computing, which are essential for future advancements. Ultimately, the synergy between AI, automated experimentation, and computational modeling transforms the way the materials are discovered, optimized, and designed, paving the way for next-generation innovations in energy, electronics, and nanotechnology.', 'abstract_zh': '机器学习和人工智能驱动方法的快速进步正在变革材料发现、性质预测和材料设计，通过减少人为干预并加速科学研究进程。本文综述了智能、机器学习（ML）驱动的方法，强调了其在预测材料性质、发现新型化合物和优化材料结构方面的作用。从深度学习、图神经网络、贝叶斯优化到自动化生成模型（如生成对抗网络GANs和变分自编码器VAEs），这些方法使能够自主设计具有定制功能的材料。通过利用AutoML框架（如AutoGluon、TPOT等），研究人员可以自动进行模型选择、超参数调优和特征工程，显著提高材料信息化的效率。此外，AI驱动的机器人实验室和高通量计算的整合建立了从快速合成到实验验证的全自动管道，极大地减少了材料发现的时间和成本。本文还强调了自动化ML驱动方法在预测材料的机械、热、电和光学性质方面的实际应用，展示了在超导体、催化剂、光伏和能量存储系统中的成功案例。本文也探讨了数据质量、解释性以及AutoML与量子计算的集成等关键挑战，这些是未来发展的必要条件。最终，AI、自动化实验和计算建模的协同作用改变了材料的发现、优化和设计方式，为能源、电子和纳米技术领域的新一代创新铺平了道路。', 'title_zh': '基于机器学习的材料发现：解锁下一代功能材料——一篇简评'}
{'arxiv_id': 'arXiv:2503.18973', 'title': 'Automated diagnosis of lung diseases using vision transformer: a comparative study on chest x-ray classification', 'authors': 'Muhammad Ahmad, Sardar Usman, Ildar Batyrshin, Muhammad Muzammil, K. Sajid, M. Hasnain, Muhammad Jalal, Grigori Sidorov', 'link': 'https://arxiv.org/abs/2503.18973', 'abstract': 'Background: Lung disease is a significant health issue, particularly in children and elderly individuals. It often results from lung infections and is one of the leading causes of mortality in children. Globally, lung-related diseases claim many lives each year, making early and accurate diagnoses crucial. Radiographs are valuable tools for the diagnosis of such conditions. The most prevalent lung diseases, including pneumonia, asthma, allergies, chronic obstructive pulmonary disease (COPD), bronchitis, emphysema, and lung cancer, represent significant public health challenges. Early prediction of these conditions is critical, as it allows for the identification of risk factors and implementation of preventive measures to reduce the likelihood of disease onset\nMethods: In this study, we utilized a dataset comprising 3,475 chest X-ray images sourced from from Mendeley Data provided by Talukder, M. A. (2023) [14], categorized into three classes: normal, lung opacity, and pneumonia. We applied five pre-trained deep learning models, including CNN, ResNet50, DenseNet, CheXNet, and U-Net, as well as two transfer learning algorithms such as Vision Transformer (ViT) and Shifted Window (Swin) to classify these images. This approach aims to address diagnostic issues in lung abnormalities by reducing reliance on human intervention through automated classification systems. Our analysis was conducted in both binary and multiclass settings. Results: In the binary classification, we focused on distinguishing between normal and viral pneumonia cases, whereas in the multi-class classification, all three classes (normal, lung opacity, and viral pneumonia) were included. Our proposed methodology (ViT) achieved remarkable performance, with accuracy rates of 99% for binary classification and 95.25% for multiclass classification.', 'abstract_zh': '背景：肺部疾病是重要的公共卫生问题，特别是在儿童和老年人中。这些疾病往往源于肺部感染，并且是儿童死亡的主要原因之一。在全球范围内，肺部相关疾病每年夺去许多生命，因此早期和准确的诊断至关重要。胸部X线检查是诊断这些病症的重要工具。包括肺炎、哮喘、过敏、慢性阻塞性肺疾病（COPD）、支气管炎、肺气肿和肺癌在内的最常见肺部疾病构成了重大的公共卫生挑战。对这些疾病的早期预测至关重要，因为它有助于识别风险因素并实施预防措施，以降低疾病发生的可能性。\n\n方法：本研究采用了Talukder, M. A. (2023) [14] 通过Mendeley Data 提供的包含3,475张胸部X光图像的数据集，并将其分为三个类别：正常、肺实变和肺炎。我们应用了包括CNN、ResNet50、DenseNet、CheXNet和U-Net在内的五种预训练深度学习模型，以及包括Vision Transformer (ViT)和Shifted Window (Swin)在内的两种迁移学习算法，以对这些图像进行分类。该方法旨在通过自动化分类系统减少对人力判断的依赖，从而解决肺部异常的诊断问题。我们的分析在二分类和多分类两种场景下进行。\n\n结果：在二分类中，我们集中于区分正常与病毒性肺炎病例；而在多分类中，包括了所有三个类别（正常、肺实变和病毒性肺炎）。我们提出的基于ViT的方法在二分类中的准确率为99%，在多分类中的准确率为95.25%。', 'title_zh': '基于视觉变换器的肺部疾病自动化诊断：胸部X光分类的比较研究'}
{'arxiv_id': 'arXiv:2503.18964', 'title': 'Unifying EEG and Speech for Emotion Recognition: A Two-Step Joint Learning Framework for Handling Missing EEG Data During Inference', 'authors': 'Upasana Tiwari, Rupayan Chakraborty, Sunil Kumar Kopparapu', 'link': 'https://arxiv.org/abs/2503.18964', 'abstract': 'Computer interfaces are advancing towards using multi-modalities to enable better human-computer interactions. The use of automatic emotion recognition (AER) can make the interactions natural and meaningful thereby enhancing the user experience. Though speech is the most direct and intuitive modality for AER, it is not reliable because it can be intentionally faked by humans. On the other hand, physiological modalities like EEG, are more reliable and impossible to fake. However, use of EEG is infeasible for realistic scenarios usage because of the need for specialized recording setup. In this paper, one of our primary aims is to ride on the reliability of the EEG modality to facilitate robust AER on the speech modality. Our approach uses both the modalities during training to reliably identify emotion at the time of inference, even in the absence of the more reliable EEG modality. We propose, a two-step joint multi-modal learning approach (JMML) that exploits both the intra- and inter- modal characteristics to construct emotion embeddings that enrich the performance of AER. In the first step, using JEC-SSL, intra-modal learning is done independently on the individual modalities. This is followed by an inter-modal learning using the proposed extended variant of deep canonically correlated cross-modal autoencoder (E-DCC-CAE). The approach learns the joint properties of both the modalities by mapping them into a common representation space, such that the modalities are maximally correlated. These emotion embeddings, hold properties of both the modalities there by enhancing the performance of ML classifier used for AER. Experimental results show the efficacy of the proposed approach. To best of our knowledge, this is the first attempt to combine speech and EEG with joint multi-modal learning approach for reliable AER.', 'abstract_zh': '计算机界面正朝着使用多模态方向发展，以实现更好的人机交互。自动情绪识别（AER）的使用可以使交互更加自然和有意义，从而提升用户体验。虽然语音是AER中最直接和直观的模态，但由于人类可以故意伪造语音，因此不太可靠。另一方面，生理模态如EEG则更加可靠且无法伪造。然而，由于需要专用的记录设备，EEG在现实场景中的使用不可行。在本文中，我们的一项主要目标是利用EEG模态的可靠性，以增强语音模态上的AER的鲁棒性。我们的方法在训练过程中同时使用这两种模态，即使在没有更可靠的EEG模态的情况下，也能可靠地识别情感。我们提出了一种两步联合多模态学习方法（JMML），该方法利用了模内和模间特征，构建了能够增强AER性能的情绪嵌入。在第一步中，使用JEC-SSL独立地在各个模态上进行模内学习，随后使用提出的扩展深层共变模态自编码器（E-DCC-CAE）的扩展变体进行模间学习。该方法通过将两种模态映射到一个共同的表示空间，学习它们的最大相关性，从而使模态之间的相关性最大化。这些情绪嵌入包含了两种模态的特性，从而增强了用于AER的机器学习分类器的性能。实验结果验证了所提方法的有效性。据我们所知，这是第一次尝试将语音和EEG结合在一起，使用联合多模态学习方法进行可靠的AER。', 'title_zh': '融合脑电和语音的情感识别：一种推理时处理缺失脑电数据的两步联合学习框架'}
{'arxiv_id': 'arXiv:2503.18956', 'title': 'International Agreements on AI Safety: Review and Recommendations for a Conditional AI Safety Treaty', 'authors': 'Rebecca Scholefield, Samuel Martin, Otto Barten', 'link': 'https://arxiv.org/abs/2503.18956', 'abstract': "The malicious use or malfunction of advanced general-purpose AI (GPAI) poses risks that, according to leading experts, could lead to the 'marginalisation or extinction of humanity.' To address these risks, there are an increasing number of proposals for international agreements on AI safety. In this paper, we review recent (2023-) proposals, identifying areas of consensus and disagreement, and drawing on related literature to assess their feasibility. We focus our discussion on risk thresholds, regulations, types of international agreement and five related processes: building scientific consensus, standardisation, auditing, verification and incentivisation.\nBased on this review, we propose a treaty establishing a compute threshold above which development requires rigorous oversight. This treaty would mandate complementary audits of models, information security and governance practices, overseen by an international network of AI Safety Institutes (AISIs) with authority to pause development if risks are unacceptable. Our approach combines immediately implementable measures with a flexible structure that can adapt to ongoing research.", 'abstract_zh': '高级通用人工智能的恶意使用或故障可能给人类带来‘边缘化或灭绝’的风险，根据领先专家的观点。为应对这些风险，国际上关于人工智能安全的协议正逐渐增多。本文回顾了近期（2023年及以后）的提议，识别共识与分歧，并结合相关文献评估其可行性。我们重点关注风险管理阈值、监管措施、国际协议类型以及五项相关过程：建立科学共识、标准化、审计、验证和激励机制。基于这一回顾，我们提出一项条约，设立一个计算阈值，在此阈值之上，开发需接受严格的监管。该条约要求由国际人工智能安全性研究所网络进行互补审计，包括模型、信息安全和治理实践的审计，必要时有权暂停开发。我们的方法结合了可立即实施的措施，并具备根据持续研究进行调整的灵活性。', 'title_zh': '国际人工智能安全协议：审核与建议——条件性人工智能安全条约'}
{'arxiv_id': 'arXiv:2503.17640', 'title': 'On the Hopf-Cole Transform for Control-affine Schrödinger Bridge', 'authors': 'Alexis Teter, Abhishek Halder', 'link': 'https://arxiv.org/abs/2503.17640', 'abstract': 'The purpose of this note is to clarify the importance of the relation $\\boldsymbol{gg}^{\\top}\\propto \\boldsymbol{\\sigma\\sigma}^{\\top}$ in solving control-affine Schrödinger bridge problems via the Hopf-Cole transform, where $\\boldsymbol{g},\\boldsymbol{\\sigma}$ are the control and noise coefficients, respectively. We show that the Hopf-Cole transform applied to the conditions of optimality for generic control-affine Schrödinger bridge problems, i.e., without the assumption $\\boldsymbol{gg}^{\\top}\\propto\\boldsymbol{\\sigma\\sigma}^{\\top}$, gives a pair of forward-backward PDEs that are neither linear nor equation-level decoupled. We explain how the resulting PDEs can be interpreted as nonlinear forward-backward advection-diffusion-reaction equations, where the nonlinearity stem from additional drift and reaction terms involving the gradient of the log-likelihood a.k.a. the score. These additional drift and reaction vanish when $\\boldsymbol{gg}^{\\top}\\propto\\boldsymbol{\\sigma\\sigma}^{\\top}$, and the resulting boundary-coupled system of linear PDEs can then be solved by dynamic Sinkhorn recursions. A key takeaway of our work is that the numerical solution of the generic control-affine Schrödinger bridge requires further algorithmic development, possibly generalizing the dynamic Sinkhorn recursion or otherwise.', 'abstract_zh': '探讨控制 affine 施勞丁格桥梁问题通过霍普夫-科尔变换求解的重要性：$\\boldsymbol{gg}^{\\top}\\propto \\boldsymbol{\\sigma\\sigma}^{\\top}$ 关系的作用', 'title_zh': 'Hopf-Cole 变换在控制拟线性薛定谔桥中的应用'}
