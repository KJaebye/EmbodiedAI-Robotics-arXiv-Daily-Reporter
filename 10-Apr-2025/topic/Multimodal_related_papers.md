# Audio-visual Event Localization on Portrait Mode Short Videos 

**Title (ZH)**: portrait模式短视频中的音视频事件定位 

**Authors**: Wuyang Liu, Yi Chai, Yongpeng Yan, Yanzhen Ren  

**Link**: [PDF](https://arxiv.org/pdf/2504.06884)  

**Abstract**: Audio-visual event localization (AVEL) plays a critical role in multimodal scene understanding. While existing datasets for AVEL predominantly comprise landscape-oriented long videos with clean and simple audio context, short videos have become the primary format of online video content due to the the proliferation of smartphones. Short videos are characterized by portrait-oriented framing and layered audio compositions (e.g., overlapping sound effects, voiceovers, and music), which brings unique challenges unaddressed by conventional methods. To this end, we introduce AVE-PM, the first AVEL dataset specifically designed for portrait mode short videos, comprising 25,335 clips that span 86 fine-grained categories with frame-level annotations. Beyond dataset creation, our empirical analysis shows that state-of-the-art AVEL methods suffer an average 18.66% performance drop during cross-mode evaluation. Further analysis reveals two key challenges of different video formats: 1) spatial bias from portrait-oriented framing introduces distinct domain priors, and 2) noisy audio composition compromise the reliability of audio modality. To address these issues, we investigate optimal preprocessing recipes and the impact of background music for AVEL on portrait mode videos. Experiments show that these methods can still benefit from tailored preprocessing and specialized model design, thus achieving improved performance. This work provides both a foundational benchmark and actionable insights for advancing AVEL research in the era of mobile-centric video content. Dataset and code will be released. 

**Abstract (ZH)**: 音频-视觉事件定位（AVEL）在多模态场景理解中发挥着关键作用。现有用于AVEL的数据集主要包含景观取向的长视频，具有清晰简洁的音频背景，但随着智能手机的普及，短视频已成为在线视频内容的主要格式。短视频特征为 portrait 取向构图和多层次的音频组成（例如，叠加的声音效果、旁白和音乐），这带来了传统方法未曾解决的独特挑战。为此，我们引入了 AVE-PM，这是首个专门针对 portrait 模式短视频的 AVEL 数据集，包含 25,335 个跨越 86 个细粒度类别的剪辑，并附有帧级标注。除数据集创建之外，我们的实证分析表明，最新的 AVEL 方法在跨模式评估中平均性能下降 18.66%。进一步分析揭示了不同视频格式的两个关键挑战：1）portrait 取向构图的空间偏差引入了不同的领域先验，2）嘈杂的音频组成削弱了音频模态的可靠性。为解决这些问题，我们研究了针对 portrait 模式短视频的音频-视觉事件定位的最优预处理方法和背景音乐的影响。实验显示，这些方法仍能从定制预处理和专门模型设计中获益，从而提高性能。本工作为移动为中心的视频内容时代推进 AVEL 研究提供了基础基准和可操作的见解。数据集和代码将公开发布。 

---
# RAVEN: An Agentic Framework for Multimodal Entity Discovery from Large-Scale Video Collections 

**Title (ZH)**: RAVEN：大型视频集合多模态实体发现的代理框架 

**Authors**: Kevin Dela Rosa  

**Link**: [PDF](https://arxiv.org/pdf/2504.06272)  

**Abstract**: We present RAVEN an adaptive AI agent framework designed for multimodal entity discovery and retrieval in large-scale video collections. Synthesizing information across visual, audio, and textual modalities, RAVEN autonomously processes video data to produce structured, actionable representations for downstream tasks. Key contributions include (1) a category understanding step to infer video themes and general-purpose entities, (2) a schema generation mechanism that dynamically defines domain-specific entities and attributes, and (3) a rich entity extraction process that leverages semantic retrieval and schema-guided prompting. RAVEN is designed to be model-agnostic, allowing the integration of different vision-language models (VLMs) and large language models (LLMs) based on application-specific requirements. This flexibility supports diverse applications in personalized search, content discovery, and scalable information retrieval, enabling practical applications across vast datasets. 

**Abstract (ZH)**: 我们提出了RAVEN，一种针对大规模视频集合中多模态实体发现和检索设计的自适应AI代理框架。通过跨视觉、音频和文本模态综合信息，RAVEN自主处理视频数据，生成结构化、可操作的表示以供下游任务使用。主要贡献包括：(1) 一种类别理解步骤，用于推断视频主题和通用实体；(2) 一种模式生成机制，动态定义领域特定实体和属性；(3) 一种丰富的内容提取过程，利用语义检索和模式引导提示。RAVEN设计为模型agnostic，可根据特定应用需求集成不同的视觉-语言模型（VLMs）和大型语言模型（LLMs）。这种灵活性支持个性化搜索、内容发现和可扩展信息检索等多种应用，使得在大规模数据集中实现实际应用成为可能。 

---
