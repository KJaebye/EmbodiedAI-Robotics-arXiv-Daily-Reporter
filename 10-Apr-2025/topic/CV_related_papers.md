# RayFronts: Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration 

**Title (ZH)**: RayFronts：开集语义射线前沿及其在在线场景理解与探索中的应用 

**Authors**: Omar Alama, Avigyan Bhattacharya, Haoyang He, Seungchan Kim, Yuheng Qiu, Wenshan Wang, Cherie Ho, Nikhil Keetha, Sebastian Scherer  

**Link**: [PDF](https://arxiv.org/pdf/2504.06994)  

**Abstract**: Open-set semantic mapping is crucial for open-world robots. Current mapping approaches either are limited by the depth range or only map beyond-range entities in constrained settings, where overall they fail to combine within-range and beyond-range observations. Furthermore, these methods make a trade-off between fine-grained semantics and efficiency. We introduce RayFronts, a unified representation that enables both dense and beyond-range efficient semantic mapping. RayFronts encodes task-agnostic open-set semantics to both in-range voxels and beyond-range rays encoded at map boundaries, empowering the robot to reduce search volumes significantly and make informed decisions both within & beyond sensory range, while running at 8.84 Hz on an Orin AGX. Benchmarking the within-range semantics shows that RayFronts's fine-grained image encoding provides 1.34x zero-shot 3D semantic segmentation performance while improving throughput by 16.5x. Traditionally, online mapping performance is entangled with other system components, complicating evaluation. We propose a planner-agnostic evaluation framework that captures the utility for online beyond-range search and exploration, and show RayFronts reduces search volume 2.2x more efficiently than the closest online baselines. 

**Abstract (ZH)**: 开放集语义映射对于开放世界机器人至关重要。RayFronts：统一表示支持稠密和远距离高效语义映射及其应用壯观分析与评价框架 

---
# Collision avoidance from monocular vision trained with novel view synthesis 

**Title (ZH)**: 基于新颖视角合成训练的单目视觉碰撞避免 

**Authors**: Valentin Tordjman--Levavasseur, Stéphane Caron  

**Link**: [PDF](https://arxiv.org/pdf/2504.06651)  

**Abstract**: Collision avoidance can be checked in explicit environment models such as elevation maps or occupancy grids, yet integrating such models with a locomotion policy requires accurate state estimation. In this work, we consider the question of collision avoidance from an implicit environment model. We use monocular RGB images as inputs and train a collisionavoidance policy from photorealistic images generated by 2D Gaussian splatting. We evaluate the resulting pipeline in realworld experiments under velocity commands that bring the robot on an intercept course with obstacles. Our results suggest that RGB images can be enough to make collision-avoidance decisions, both in the room where training data was collected and in out-of-distribution environments. 

**Abstract (ZH)**: 基于隐式环境模型的碰撞 avoidance 从单目 RGB 图像中学习 avoidance 策略 

---
# Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition 

**Title (ZH)**: 视觉-语言模型准备好应对饮食评估挑战了吗？探索AI驱动食品图像识别的下一个前沿领域 

**Authors**: Sergio Romero-Tapiador, Ruben Tolosana, Blanca Lacruz-Pleguezuelos, Laura Judith Marcos Zambrano, Guadalupe X.Bazán, Isabel Espinosa-Salinas, Julian Fierrez, Javier Ortega-Garcia, Enrique Carrillo de Santa Pau, Aythami Morales  

**Link**: [PDF](https://arxiv.org/pdf/2504.06925)  

**Abstract**: Automatic dietary assessment based on food images remains a challenge, requiring precise food detection, segmentation, and classification. Vision-Language Models (VLMs) offer new possibilities by integrating visual and textual reasoning. In this study, we evaluate six state-of-the-art VLMs (ChatGPT, Gemini, Claude, Moondream, DeepSeek, and LLaVA), analyzing their capabilities in food recognition at different levels. For the experimental framework, we introduce the FoodNExTDB, a unique food image database that contains 9,263 expert-labeled images across 10 categories (e.g., "protein source"), 62 subcategories (e.g., "poultry"), and 9 cooking styles (e.g., "grilled"). In total, FoodNExTDB includes 50k nutritional labels generated by seven experts who manually annotated all images in the database. Also, we propose a novel evaluation metric, Expert-Weighted Recall (EWR), that accounts for the inter-annotator variability. Results show that closed-source models outperform open-source ones, achieving over 90% EWR in recognizing food products in images containing a single product. Despite their potential, current VLMs face challenges in fine-grained food recognition, particularly in distinguishing subtle differences in cooking styles and visually similar food items, which limits their reliability for automatic dietary assessment. The FoodNExTDB database is publicly available at this https URL. 

**Abstract (ZH)**: 基于食品图像的自动膳食评估仍然是一项挑战，需要精确的食品检测、分割和分类。视觉-语言模型（VLMs）通过结合视觉和文本推理提供了新的可能性。在本研究中，我们评估了六种最先进的VLMs（ChatGPT、Gemini、Claude、Moondream、DeepSeek和LLaVA），分析了它们在不同层次上的食品识别能力。在实验框架中，我们引入了FoodNExTDB，这是一个独特的食品图像数据库，包含了9,263张专家标注的图像，涉及10个类别（例如，“蛋白质来源”）、62个子类别（例如，“禽类”）和9种烹饪风格（例如，“烤制”）。FoodNExTDB总共包括由七位专家手动标注数据库中所有图像生成的50,000个营养标签。此外，我们提出了一种新的评估指标——专家加权召回率（EWR），以考虑标注者间的一致性差异。结果显示，闭源模型优于开源模型，在包含单个产品的图像中识别食品产品的EWR超过90%。尽管有这些潜力，当前的VLMs在细粒度食品识别方面仍面临挑战，尤其是在区分烹饪风格的细微差别和视觉相似的食品项目方面，这限制了它们在自动膳食评估中的可靠性。FoodNExTDB数据库可在此处公开访问：this https URL。 

---
# Longitudinal Assessment of Lung Lesion Burden in CT 

**Title (ZH)**: CT纵行评估肺部病灶负荷 

**Authors**: Tejas Sudharshan Mathai, Benjamin Hou, Ronald M. Summers  

**Link**: [PDF](https://arxiv.org/pdf/2504.06924)  

**Abstract**: In the U.S., lung cancer is the second major cause of death. Early detection of suspicious lung nodules is crucial for patient treatment planning, management, and improving outcomes. Many approaches for lung nodule segmentation and volumetric analysis have been proposed, but few have looked at longitudinal changes in total lung tumor burden. In this work, we trained two 3D models (nnUNet) with and without anatomical priors to automatically segment lung lesions and quantified total lesion burden for each patient. The 3D model without priors significantly outperformed ($p < .001$) the model trained with anatomy priors. For detecting clinically significant lesions $>$ 1cm, a precision of 71.3\%, sensitivity of 68.4\%, and F1-score of 69.8\% was achieved. For segmentation, a Dice score of 77.1 $\pm$ 20.3 and Hausdorff distance error of 11.7 $\pm$ 24.1 mm was obtained. The median lesion burden was 6.4 cc (IQR: 2.1, 18.1) and the median volume difference between manual and automated measurements was 0.02 cc (IQR: -2.8, 1.2). Agreements were also evaluated with linear regression and Bland-Altman plots. The proposed approach can produce a personalized evaluation of the total tumor burden for a patient and facilitate interval change tracking over time. 

**Abstract (ZH)**: 在美国，肺癌是第二大死因。早期检测可疑肺结节对于患者的治疗规划、管理和改善预后至关重要。虽然已经提出了许多肺结节分割和容积分析的方法，但很少有研究关注总体肺肿瘤负担的纵向变化。在本研究中，我们训练了两个3D模型（nnUNet），一个是带解剖先验知识的，另一个是不带解剖先验知识的，以自动分割肺部病灶并量化每位患者的整体病灶负担。不带解剖先验知识的3D模型显著优于带解剖先验知识的模型（$p < .001$）。对于检测临床显著的病灶（＞1cm），实现了71.3%的精确率、68.4%的灵敏度和69.8%的F1分数。对于分割，获得了77.1 $\pm$ 20.3的Dice分数和11.7 $\pm$ 24.1 mm的哈斯多夫距离误差。中位病灶负担为6.4 cc（四分位距：2.1, 18.1），手工测量与自动测量的中位体积差异为0.02 cc（四分位距：-2.8, 1.2）。还使用线性回归和Bland-Altman图评估了协议的一致性。所提出的方法可以为每位患者提供个性化的总体肿瘤负担评估，并有助于随时间跟踪间隔变化。 

---
# Leveraging Anatomical Priors for Automated Pancreas Segmentation on Abdominal CT 

**Title (ZH)**: 利用解剖先验信息实现腹部CT胰腺自动分割 

**Authors**: Anisa V. Prasad, Tejas Sudharshan Mathai, Pritam Mukherjee, Jianfei Liu, Ronald M. Summers  

**Link**: [PDF](https://arxiv.org/pdf/2504.06921)  

**Abstract**: An accurate segmentation of the pancreas on CT is crucial to identify pancreatic pathologies and extract imaging-based biomarkers. However, prior research on pancreas segmentation has primarily focused on modifying the segmentation model architecture or utilizing pre- and post-processing techniques. In this article, we investigate the utility of anatomical priors to enhance the segmentation performance of the pancreas. Two 3D full-resolution nnU-Net models were trained, one with 8 refined labels from the public PANORAMA dataset, and another that combined them with labels derived from the public TotalSegmentator (TS) tool. The addition of anatomical priors resulted in a 6\% increase in Dice score ($p < .001$) and a 36.5 mm decrease in Hausdorff distance for pancreas segmentation ($p < .001$). Moreover, the pancreas was always detected when anatomy priors were used, whereas there were 8 instances of failed detections without their use. The use of anatomy priors shows promise for pancreas segmentation and subsequent derivation of imaging biomarkers. 

**Abstract (ZH)**: 基于解剖先验的胰腺分割及其影像生物标志物提取的准确性增强 

---
# MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs 

**Title (ZH)**: MedSegFactory：基于文本引导的医疗图像-掩码对生成 

**Authors**: Jiawei Mao, Yuhan Wang, Yucheng Tang, Daguang Xu, Kang Wang, Yang Yang, Zongwei Zhou, Yuyin Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2504.06897)  

**Abstract**: This paper presents MedSegFactory, a versatile medical synthesis framework that generates high-quality paired medical images and segmentation masks across modalities and tasks. It aims to serve as an unlimited data repository, supplying image-mask pairs to enhance existing segmentation tools. The core of MedSegFactory is a dual-stream diffusion model, where one stream synthesizes medical images and the other generates corresponding segmentation masks. To ensure precise alignment between image-mask pairs, we introduce Joint Cross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic cross-conditioning between streams. This bidirectional interaction allows both representations to guide each other's generation, enhancing consistency between generated pairs. MedSegFactory unlocks on-demand generation of paired medical images and segmentation masks through user-defined prompts that specify the target labels, imaging modalities, anatomical regions, and pathological conditions, facilitating scalable and high-quality data generation. This new paradigm of medical image synthesis enables seamless integration into diverse medical imaging workflows, enhancing both efficiency and accuracy. Extensive experiments show that MedSegFactory generates data of superior quality and usability, achieving competitive or state-of-the-art performance in 2D and 3D segmentation tasks while addressing data scarcity and regulatory constraints. 

**Abstract (ZH)**: 本文介绍了MedSegFactory，一个多功能的医学合成框架，能够跨模态和任务生成高质量的配对医学图像和分割掩码。其目标是作为无限数据仓库，提供图像-掩码配对以增强现有的分割工具。MedSegFactory的核心是一个双流扩散模型，其中一条流生成医学图像，另一条流生成相应的分割掩码。为了确保图像-掩码配对之间的精确对齐，我们引入了联合交叉注意力（JCA），通过流之间的动态交叉条件实现协作去噪范式。这种双向交互使两个表示能够相互引导生成过程，从而增强生成配对的一致性。MedSegFactory通过用户定义的提示解锁按需生成配对医学图像和分割掩码，这些提示指定目标标签、成像模ality、解剖区域和病理条件，促进可扩展和高质量数据生成。这种新的医学图像合成范式能够无缝集成到各种医学成像工作流程中，提高效率和准确性。大量实验表明，MedSegFactory生成的数据质量和可用性更高，在2D和3D分割任务中实现了竞争力或先进水平的表现，同时解决了数据稀缺性和监管约束的问题。 

---
# Compound and Parallel Modes of Tropical Convolutional Neural Networks 

**Title (ZH)**: 热带卷积神经网络的复合与并行模式 

**Authors**: Mingbo Li, Liying Liu, Ye Luo  

**Link**: [PDF](https://arxiv.org/pdf/2504.06881)  

**Abstract**: Convolutional neural networks have become increasingly deep and complex, leading to higher computational costs. While tropical convolutional neural networks (TCNNs) reduce multiplications, they underperform compared to standard CNNs. To address this, we propose two new variants - compound TCNN (cTCNN) and parallel TCNN (pTCNN)-that use combinations of tropical min-plus and max-plus kernels to replace traditional convolution kernels. This reduces multiplications and balances efficiency with performance. Experiments on various datasets show that cTCNN and pTCNN match or exceed the performance of other CNN methods. Combining these with conventional CNNs in deeper architectures also improves performance. We are further exploring simplified TCNN architectures that reduce parameters and multiplications with minimal accuracy loss, aiming for efficient and effective models. 

**Abstract (ZH)**: 卷积神经网络越来越深且复杂，导致更高的计算成本。尽管热带卷积神经网络（TCNNs）减少了乘法次数，但其性能低于标准CNNs。为了解决这个问题，我们提出了两种新的变体——组合TCNN（cTCNN）和并行TCNN（pTCNN），它们使用热带最小加法和最大加法核的组合来替代传统的卷积核。这减少了乘法次数，并平衡了效率与性能。在多种数据集上的实验表明，cTCNN和pTCNN的性能能够匹配甚至超越其他CNN方法。将这些方法与传统的CNN结合使用在更深的架构中也能改进性能。我们进一步研究了简化TCNN架构，以减少参数和乘法次数的同时保持最小的准确率损失，旨在获得高效且有效的模型。 

---
# EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation 

**Title (ZH)**: EIDT-V：利用扩散轨迹中的交叉点进行模型无关的零样本、无需训练的文本到视频生成 

**Authors**: Diljeet Jagpal, Xi Chen, Vinay P. Namboodiri  

**Link**: [PDF](https://arxiv.org/pdf/2504.06861)  

**Abstract**: Zero-shot, training-free, image-based text-to-video generation is an emerging area that aims to generate videos using existing image-based diffusion models. Current methods in this space require specific architectural changes to image generation models, which limit their adaptability and scalability. In contrast to such methods, we provide a model-agnostic approach. We use intersections in diffusion trajectories, working only with the latent values. We could not obtain localized frame-wise coherence and diversity using only the intersection of trajectories. Thus, we instead use a grid-based approach. An in-context trained LLM is used to generate coherent frame-wise prompts; another is used to identify differences between frames. Based on these, we obtain a CLIP-based attention mask that controls the timing of switching the prompts for each grid cell. Earlier switching results in higher variance, while later switching results in more coherence. Therefore, our approach can ensure appropriate control between coherence and variance for the frames. Our approach results in state-of-the-art performance while being more flexible when working with diverse image-generation models. The empirical analysis using quantitative metrics and user studies confirms our model's superior temporal consistency, visual fidelity and user satisfaction, thus providing a novel way to obtain training-free, image-based text-to-video generation. 

**Abstract (ZH)**: 零样本、无需训练的基于图像的文本到视频生成是一个新兴领域，旨在使用现有的图像扩散模型生成视频。现有方法需要对图像生成模型进行特定的架构变化，这限制了它们的适应性和可扩展性。与这些方法不同，我们提供了一种模型无关的方法。我们利用扩散轨迹的交集，仅使用潜在值。我们无法仅通过轨迹交集获得局部帧内一致性与多样性。因此，我们改用基于网格的方法。我们使用上下文训练的语言模型生成一致的帧级提示；另一个模型用于识别帧之间的差异。基于这些信息，我们获得了一个基于CLIP的注意力掩码，用于控制为每个网格单元切换提示的时间。较早切换会导致更高的方差，而较晚切换则会产生更多的一致性。因此，我们的方法可以在确保帧间一致性和方差之间的适当控制方面表现出优势。我们的方法在性能上达到最新水平，并且在与多种图像生成模型合作时更具灵活性。实证分析利用定量指标和用户研究证实了我们模型在时间一致性和视觉保真度方面的优越性以及用户的满意度，从而提供了一种新的方法来实现无需训练的数据驱动的图像到视频生成。 

---
# EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture 

**Title (ZH)**: 改进视觉变换器通过编码器-解码器架构缓解注意力陷阱 

**Authors**: Wenfeng Feng, Guoying Sun  

**Link**: [PDF](https://arxiv.org/pdf/2504.06738)  

**Abstract**: In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel architecture designed to mitigate the attention sink phenomenon observed in Vision Transformer models. Attention sink occurs when an excessive amount of attention is allocated to the [CLS] token, distorting the model's ability to effectively process image patches. To address this, we introduce a layer-aligned encoder-decoder architecture, where the encoder utilizes self-attention to process image patches, while the decoder uses cross-attention to focus on the [CLS] token. Unlike traditional encoder-decoder framework, where the decoder depends solely on high-level encoder representations, EDIT allows the decoder to extract information starting from low-level features, progressively refining the representation layer by layer. EDIT is naturally interpretable demonstrated through sequential attention maps, illustrating the refined, layer-by-layer focus on key image features. Experiments on ImageNet-1k and ImageNet-21k, along with transfer learning tasks, show that EDIT achieves consistent performance improvements over DeiT3 models. These results highlight the effectiveness of EDIT's design in addressing attention sink and improving visual feature extraction. 

**Abstract (ZH)**: 本文提出EDIT（编码器-解码器图像变换器）架构，旨在缓解视觉变换器模型中观察到的注意力陷进现象。注意力陷进发生在过多的注意力集中在[CLS]标记上，从而影响模型有效处理图像 patches 的能力。为了解决这一问题，我们引入了一种层对齐的编码器-解码器架构，其中编码器利用自我注意来处理图像 patches，而解码器则使用交叉注意来聚焦于[CLS]标记。与传统的编码器-解码器框架不同，传统的解码器仅依赖高层编码器表示，而EDIT允许解码器从低层特征开始提取信息，并逐层细化表示。通过序贯注意力图展示了EDIT的自然可解释性，表明其逐层聚焦于关键图像特征。在ImageNet-1k和ImageNet-21k上的实验以及迁移学习任务中，EDIT在DeiT3模型上展示了一致的性能提升，这些结果突显了EDIT设计在解决注意力陷进和提高视觉特征提取方面的有效性。 

---
# Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding 

**Title (ZH)**: 掩码场景建模：在3D场景理解中缩小监督学习与自我监督学习的差距 

**Authors**: Pedro Hermosilla, Christian Stippel, Leon Sick  

**Link**: [PDF](https://arxiv.org/pdf/2504.06719)  

**Abstract**: Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as a weight initialization step for task-specific fine-tuning, limiting their utility for general-purpose feature extraction. This paper addresses this shortcoming by proposing a robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multi-resolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in a linear probing setup. In particular, our model is trained natively in 3D with a novel self-supervised approach based on a Masked Scene Modeling objective, which reconstructs deep features of masked patches in a bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by a large margin. The model and training code can be found at our Github repository (this https URL). 

**Abstract (ZH)**: 自监督学习已通过使在大规模无标注数据集上训练的模型能够提供与标注训练模型性能相当的多功能即用型特征，而重塑了2D计算机视觉。然而，在3D场景理解中，自监督方法通常仅用作任务特定微调的权重初始化步骤，限制了其作为通用特征提取工具的应用。本文通过提出一种专门用于评估自监督特征质量的稳健评估协议来解决这一问题。该协议利用分层模型的多分辨率特征采样来创建丰富的点级表示，这些表示捕获了模型的语义能力，并因此适用于使用线性探针和最近邻方法进行评估。此外，我们引入了第一个在仅使用即用型特征的线性探针设置中与监督模型表现相当的自监督模型。特别是，我们的模型使用一种新颖的自监督方法在3D空间中进行本征训练，该方法基于遮罩场景建模目标，以自底向上方式重建遮罩补丁的深层特征，并特别针对分层3D模型。我们的实验不仅证明了我们的方法达到了与监督模型相当的性能，而且在很大程度上超过了现有的自监督方法。模型和训练代码可在我们的Github仓库中找到（this https URL）。 

---
# Exploring Ordinal Bias in Action Recognition for Instructional Videos 

**Title (ZH)**: 探索指令视频中动作识别的序数偏见 

**Authors**: Joochan Kim, Minjoon Jung, Byoung-Tak Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2504.06580)  

**Abstract**: Action recognition models have achieved promising results in understanding instructional videos. However, they often rely on dominant, dataset-specific action sequences rather than true video comprehension, a problem that we define as ordinal bias. To address this issue, we propose two effective video manipulation methods: Action Masking, which masks frames of frequently co-occurring actions, and Sequence Shuffling, which randomizes the order of action segments. Through comprehensive experiments, we demonstrate that current models exhibit significant performance drops when confronted with nonstandard action sequences, underscoring their vulnerability to ordinal bias. Our findings emphasize the importance of rethinking evaluation strategies and developing models capable of generalizing beyond fixed action patterns in diverse instructional videos. 

**Abstract (ZH)**: 动作识别模型在理解指导视频方面取得了令人瞩目的成果，但它们往往依赖于特定数据集的主导动作序列，而不是真正的视频理解。为解决这一问题，我们提出了两种有效的视频操作方法：动作遮蔽，即遮蔽频繁共现的动作帧；序列打乱，即随机化动作片段顺序。通过全面的实验，我们证明了当前模型在面对非标准动作序列时表现出显著的性能下降，突显了它们对序列表现偏差的脆弱性。我们的研究强调了重新思考评估策略并开发能够超越固定动作模式泛化的模型的重要性。 

---
# Attributes-aware Visual Emotion Representation Learning 

**Title (ZH)**: 属性aware的视觉情绪表示学习 

**Authors**: Rahul Singh Maharjan, Marta Romeo, Angelo Cangelosi  

**Link**: [PDF](https://arxiv.org/pdf/2504.06578)  

**Abstract**: Visual emotion analysis or recognition has gained considerable attention due to the growing interest in understanding how images can convey rich semantics and evoke emotions in human perception. However, visual emotion analysis poses distinctive challenges compared to traditional vision tasks, especially due to the intricate relationship between general visual features and the different affective states they evoke, known as the affective gap. Researchers have used deep representation learning methods to address this challenge of extracting generalized features from entire images. However, most existing methods overlook the importance of specific emotional attributes such as brightness, colorfulness, scene understanding, and facial expressions. Through this paper, we introduce A4Net, a deep representation network to bridge the affective gap by leveraging four key attributes: brightness (Attribute 1), colorfulness (Attribute 2), scene context (Attribute 3), and facial expressions (Attribute 4). By fusing and jointly training all aspects of attribute recognition and visual emotion analysis, A4Net aims to provide a better insight into emotional content in images. Experimental results show the effectiveness of A4Net, showcasing competitive performance compared to state-of-the-art methods across diverse visual emotion datasets. Furthermore, visualizations of activation maps generated by A4Net offer insights into its ability to generalize across different visual emotion datasets. 

**Abstract (ZH)**: 视觉情感分析或识别由于人们日益关注理解图像如何传达丰富语义并引发人类情感而得到了广泛关注。然而，视觉情感分析相对于传统视觉任务来说面临着独特的挑战，尤其是在一般视觉特征与它们所唤起的不同情感状态之间复杂的关系，即情感差距方面。研究人员利用深度表示学习方法来应对从整幅图像中提取通用特征的挑战。然而，现有的大多数方法忽略了亮度、色彩丰富度、场景理解以及面部表情等特定情感属性的重要性。通过本文，我们介绍了A4Net，一种深度表示网络，通过利用四个关键属性（亮度、色彩丰富度、场景上下文以及面部表情）来弥合情感差距。通过融合和联合训练所有属性识别和视觉情感分析的各个方面，A4Net旨在提供对图像中情感内容的更好理解。实验结果表明了A4Net的有效性，在多种视觉情感数据集中展示了与其相比具有竞争力的性能。此外，A4Net生成的激活图可视化展示了其在不同视觉情感数据集上的泛化能力。 

---
# TSP-OCS: A Time-Series Prediction for Optimal Camera Selection in Multi-Viewpoint Surgical Video Analysis 

**Title (ZH)**: TSP-OCS：多视角手术视频分析中的最优相机选择时间序列预测 

**Authors**: Xinyu Liu, Xiaoguang Lin, Xiang Liu, Yong Yang, Hongqian Wang, Qilong Sun  

**Link**: [PDF](https://arxiv.org/pdf/2504.06527)  

**Abstract**: Recording the open surgery process is essential for educational and medical evaluation purposes; however, traditional single-camera methods often face challenges such as occlusions caused by the surgeon's head and body, as well as limitations due to fixed camera angles, which reduce comprehensibility of the video content. This study addresses these limitations by employing a multi-viewpoint camera recording system, capturing the surgical procedure from six different angles to mitigate occlusions. We propose a fully supervised learning-based time series prediction method to choose the best shot sequences from multiple simultaneously recorded video streams, ensuring optimal viewpoints at each moment. Our time series prediction model forecasts future camera selections by extracting and fusing visual and semantic features from surgical videos using pre-trained models. These features are processed by a temporal prediction network with TimeBlocks to capture sequential dependencies. A linear embedding layer reduces dimensionality, and a Softmax classifier selects the optimal camera view based on the highest probability. In our experiments, we created five groups of open thyroidectomy videos, each with simultaneous recordings from six different angles. The results demonstrate that our method achieves competitive accuracy compared to traditional supervised methods, even when predicting over longer time horizons. Furthermore, our approach outperforms state-of-the-art time series prediction techniques on our dataset. This manuscript makes a unique contribution by presenting an innovative framework that advances surgical video analysis techniques, with significant implications for improving surgical education and patient safety. 

**Abstract (ZH)**: 记录开放式手术过程对于教育和医疗评估至关重要；然而，传统的单摄像头方法常常面临由于外科医生头部和身体造成的遮挡问题，以及固定摄像头角度限制，这降低了视频内容的可理解性。本研究通过采用多视角摄像头记录系统来解决这些限制，从六个不同的角度捕捉手术过程以减轻遮挡问题。我们提出了一种基于完全监督学习的时间序列预测方法，从多个同时录制的视频流中选择最佳的镜头序列，确保每个时刻的最佳视角。我们的时间序列预测模型通过预训练模型提取和融合手术视频的视觉和语义特征来预测未来的摄像头选择，并由时间卷积网络捕捉序列依赖性。线性嵌入层降低了维度，Softmax分类器根据最高概率选择最优的摄像头视角。在我们的实验中，我们创建了五组开放式甲状腺切除手术视频，每组视频同时从六个不同的角度进行录制。结果表明，我们的方法在预测长时间范围时仍能达到与传统监督方法相当的准确性。此外，我们的方法在我们的数据集上优于最先进的时间序列预测技术。本文通过提出一种创新框架，推动了手术视频分析技术的发展，对提高手术教育和患者安全具有重要意义。 

---
# Analyzing the Impact of Low-Rank Adaptation for Cross-Domain Few-Shot Object Detection in Aerial Images 

**Title (ZH)**: 低秩适应对跨域少样本目标检测在航空图像中的影响分析 

**Authors**: Hicham Talaoubrid, Anissa Mokraoui, Ismail Ben Ayed, Axel Prouvost, Sonimith Hang, Monit Korn, Rémi Harvey  

**Link**: [PDF](https://arxiv.org/pdf/2504.06330)  

**Abstract**: This paper investigates the application of Low-Rank Adaptation (LoRA) to small models for cross-domain few-shot object detection in aerial images. Originally designed for large-scale models, LoRA helps mitigate overfitting, making it a promising approach for resource-constrained settings. We integrate LoRA into DiffusionDet, and evaluate its performance on the DOTA and DIOR datasets. Our results show that LoRA applied after an initial fine-tuning slightly improves performance in low-shot settings (e.g., 1-shot and 5-shot), while full fine-tuning remains more effective in higher-shot configurations. These findings highlight LoRA's potential for efficient adaptation in aerial object detection, encouraging further research into parameter-efficient fine-tuning strategies for few-shot learning. Our code is available here: this https URL. 

**Abstract (ZH)**: 本文探讨了将低秩适应（LoRA）应用于小模型以在航空图像中进行跨域少量样本目标检测的应用。LoRA originally designed for大规模模型，有助于缓解过拟合现象，使其成为资源受限环境中的一种有前景的方法。我们将LoRA集成到DiffusionDet中，并在DOTA和DIOR数据集上评估其性能。结果显示，LoRA在少量样本设置（例如1-shot和5-shot）中应用于初始微调后能够略微提高性能，而完全微调在高样本设置中仍然更有效。这些发现突显了LoRA在航空目标检测中高效适应的潜力，并鼓励对参数高效微调策略进行进一步研究，以支持少量样本学习。我们的代码可在这里获取：this https URL。 

---
# Temporal-contextual Event Learning for Pedestrian Crossing Intent Prediction 

**Title (ZH)**: 基于时空上下文的行人过街意图预测 

**Authors**: Hongbin Liang, Hezhe Qiao, Wei Huang, Qizhou Wang, Mingsheng Shang, Lin Chen  

**Link**: [PDF](https://arxiv.org/pdf/2504.06292)  

**Abstract**: Ensuring the safety of vulnerable road users through accurate prediction of pedestrian crossing intention (PCI) plays a crucial role in the context of autonomous and assisted driving. Analyzing the set of observation video frames in ego-view has been widely used in most PCI prediction methods to forecast the cross intent. However, they struggle to capture the critical events related to pedestrian behaviour along the temporal dimension due to the high redundancy of the video frames, which results in the sub-optimal performance of PCI prediction. Our research addresses the challenge by introducing a novel approach called \underline{T}emporal-\underline{c}ontextual Event \underline{L}earning (TCL). The TCL is composed of the Temporal Merging Module (TMM), which aims to manage the redundancy by clustering the observed video frames into multiple key temporal events. Then, the Contextual Attention Block (CAB) is employed to adaptively aggregate multiple event features along with visual and non-visual data. By synthesizing the temporal feature extraction and contextual attention on the key information across the critical events, TCL can learn expressive representation for the PCI prediction. Extensive experiments are carried out on three widely adopted datasets, including PIE, JAAD-beh, and JAAD-all. The results show that TCL substantially surpasses the state-of-the-art methods. Our code can be accessed at this https URL. 

**Abstract (ZH)**: 确保通过准确预测行人过街意图（PCI）来保护弱势道路使用者在自动驾驶和辅助驾驶的背景下发挥着关键作用。传统的基于自我视角视频帧集合的方法在大多数PCI预测方法中被广泛使用以预测过街意图，但由于视频帧的高冗余性，它们难以在时间维度上捕捉到与行人行为相关的关键事件，从而导致PCI预测的性能欠佳。我们的研究通过引入一种名为Temporal-Contextual Event Learning (TCL)的新颖方法来应对这一挑战。TCL由Temporal Merging Module (TMM)组成，旨在通过将观测到的视频帧聚类为多个关键时间事件来管理冗余性。然后，采用Contextual Attention Block (CAB)根据视觉和非视觉数据自适应地聚合多个事件特征。通过在关键事件的关键信息上综合时序特征提取和上下文关注，TCL能够学习用于PCI预测的表达性表示。在PIE、JAAD-beh和JAAD-all三个广泛采用的数据集上进行了大量实验，结果显示TCL显著超越了现有最先进的方法。我们的代码可以通过以下链接访问。 

---
