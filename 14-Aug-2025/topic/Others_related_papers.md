# Online Safety under Multiple Constraints and Input Bounds using gatekeeper: Theory and Applications 

**Title (ZH)**: 基于守门人的在线安全：多约束和输入边界下的理论与应用 

**Authors**: Devansh R. Agrawal, Dimitra Panagou  

**Link**: [PDF](https://arxiv.org/pdf/2508.09963)  

**Abstract**: This letter presents an approach to guarantee online safety of a cyber-physical system under multiple state and input constraints. Our proposed framework, called gatekeeper, recursively guarantees the existence of an infinite-horizon trajectory that satisfies all constraints and system dynamics. Such trajectory is constructed using a backup controller, which we define formally in this paper. gatekeeper relies on a small number of verifiable assumptions, and is computationally efficient since it requires optimization over a single scalar variable. We make two primary contributions in this letter. (A) First, we develop the theory of gatekeeper: we derive a sub-optimality bound relative to a full nonlinear trajectory optimization problem, and show how this can be used in runtime to validate performance. This also informs the design of the backup controllers and sets. (B) Second, we demonstrate in detail an application of gatekeeper for multi-agent formation flight, where each Dubins agent must avoid multiple obstacles and weapons engagement zones, both of which are nonlinear, nonconvex constraints. 

**Abstract (ZH)**: 本信提出了一种在多重状态和输入约束下确保网络物理系统在线安全的方法。我们提出的框架称为gatekeeper，递归地保证存在一条满足所有约束和系统动力学的无限时域轨迹。该轨迹通过一个备份控制器构建，我们在这篇论文中对其进行了形式化定义。gatekeeper仅依赖少量可验证假设，并由于只需要对单一标量变量进行优化而具有计算效率。本文的主要贡献包括：(A) 我们发展了gatekeeper的理论：我们推导了相对于完整非线性轨迹优化问题的次优性界，并展示了如何在运行时利用此结果来验证性能。这也有助于指导备份控制器和集合的设计。(B) 我们详细展示了在多agent编队飞行应用中使用gatekeeper，其中每个Dubins agent必须避免多个障碍物和武器交战区，两者均为非线性和非凸约束。 

---
# Safety Perspective on Assisted Lane Changes: Insights from Open-Road, Live-Traffic Experiments 

**Title (ZH)**: 辅助变道的安全视角：基于开放路实况交通实验的见解 

**Authors**: Konstantinos Mattas, Sandor Vass, Gergely Zachar, Junyi Ji, Derek Gloudemans, Davide Maggi, Akos Kriston, Mohamed Brahmi, Maria Christina Galassi, Daniel B Work, Biagio Ciuffo  

**Link**: [PDF](https://arxiv.org/pdf/2508.09233)  

**Abstract**: This study investigates the assisted lane change functionality of five different vehicles equipped with advanced driver assistance systems (ADAS). The goal is to examine novel, under-researched features of commercially available ADAS technologies. The experimental campaign, conducted in the I-24 highway near Nashville, TN, US, collected data on the kinematics and safety margins of assisted lane changes in real-world conditions. The results show that the kinematics of assisted lane changes are consistent for each system, with four out of five vehicles using slower speeds and decelerations than human drivers. However, one system consistently performed more assertive lane changes, completing the maneuver in around 5 seconds. Regarding safety margins, only three vehicles are investigated. Those operated in the US are not restricted by relevant UN regulations, and their designs were found not to adhere to these regulatory requirements. A simulation method used to classify the challenge level for the vehicle receiving the lane change, showing that these systems can force trailing vehicles to decelerate to keep a safe gap. One assisted system was found to have performed a maneuver that posed a hard challenge level for the other vehicle, raising concerns about the safety of these systems in real-world operation. All three vehicles were found to carry out lane changes that induced decelerations to the vehicle in the target lane. Those decelerations could affect traffic flow, inducing traffic shockwaves. 

**Abstract (ZH)**: 本研究调查了五种配备高级驾驶辅助系统（ADAS）的车辆的辅助变道功能。目标是研究商用ADAS技术中新颖且研究不足的特性。实验活动在美国田纳西州纳什维尔附近的I-24高速公路进行，收集了在实际条件下的辅助变道的动力学和安全裕度数据。结果显示，每种系统的辅助变道动力学是一致的，其中四辆汽车使用了比人类驾驶员更慢的速度和减速度。然而，有一种系统在变道时表现得更为积极，在大约5秒内完成了变道动作。关于安全裕度，只有三辆车进行了调查。它们在美国运营，不受相关UN法规的限制，并且设计未遵守这些法规要求。使用仿真方法对接受变道的车辆面临的挑战级别进行了分类，表明这些系统能够迫使跟随车辆减速以保持安全距离。发现一种辅助系统执行了一种对另一辆车构成高挑战级别的操作，这引发了对接收到变道指令的车辆安全性的担忧。所有三辆车在变道时都导致目标车道车辆减速，这种减速可能影响交通流量，引发交通波动。 

---
# Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete 

**Title (ZH)**: 关于正规表达式的知识推理是2EXPTIME完全的 

**Authors**: Avijeet Ghosh, Sujata Ghosh, François Schwarzentruber  

**Link**: [PDF](https://arxiv.org/pdf/2508.09784)  

**Abstract**: Logics for reasoning about knowledge and actions have seen many applications in various domains of multi-agent systems, including epistemic planning. Change of knowledge based on observations about the surroundings forms a key aspect in such planning scenarios. Public Observation Logic (POL) is a variant of public announcement logic for reasoning about knowledge that gets updated based on public observations. Each state in an epistemic (Kripke) model is equipped with a set of expected observations. These states evolve as the expectations get matched with the actual observations. In this work, we prove that the satisfiability problem of $\POL$ is 2EXPTIME-complete. 

**Abstract (ZH)**: 基于观察的知识和行动推理逻辑在多智能体系统的多种领域，包括知识规划中得到了广泛应用。基于公共观察的知识更新形成了这种规划场景中的一个重要方面。公共观察逻辑（POL）是用于推理基于公共观察更新的知识的一种公言公告逻辑的变体。每个知实践（克里普克）模型中的状态配备了一组预期观察。这些状态随着预期与实际观察的匹配而演变。在本文中，我们证明了POL的可满足性问题是2EXPTIME-complete的。 

---
# UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles 

**Title (ZH)**: UbiQTree: 在树集成中的不确定性量化在可解释AI中的应用 

**Authors**: Akshat Dubey, Aleksandar Anžel, Bahar İlgen, Georges Hattab  

**Link**: [PDF](https://arxiv.org/pdf/2508.09639)  

**Abstract**: Explainable Artificial Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP), have become essential tools for interpreting complex ensemble tree-based models, especially in high-stakes domains such as healthcare analytics. However, SHAP values are usually treated as point estimates, which disregards the inherent and ubiquitous uncertainty in predictive models and data. This uncertainty has two primary sources: aleatoric and epistemic. The aleatoric uncertainty, which reflects the irreducible noise in the data. The epistemic uncertainty, which arises from a lack of data. In this work, we propose an approach for decomposing uncertainty in SHAP values into aleatoric, epistemic, and entanglement components. This approach integrates Dempster-Shafer evidence theory and hypothesis sampling via Dirichlet processes over tree ensembles. We validate the method across three real-world use cases with descriptive statistical analyses that provide insight into the nature of epistemic uncertainty embedded in SHAP explanations. The experimentations enable to provide more comprehensive understanding of the reliability and interpretability of SHAP-based attributions. This understanding can guide the development of robust decision-making processes and the refinement of models in high-stakes applications. Through our experiments with multiple datasets, we concluded that features with the highest SHAP values are not necessarily the most stable. This epistemic uncertainty can be reduced through better, more representative data and following appropriate or case-desired model development techniques. Tree-based models, especially bagging, facilitate the effective quantification of epistemic uncertainty. 

**Abstract (ZH)**: 可解释人工智能(XAI)技术，如SHapley Additive exPlanations (SHAP)，已成为解读复杂集成树模型的重要工具，特别是在医疗保健分析等高风险领域。然而，SHAP值通常被视为点估计，这忽略了预测模型和数据中的固有和普遍不确定性。这种不确定性有两个主要来源：aleatoric不确定性，反映了数据中的不可约噪声；epistemic不确定性，源于数据不足。在本文中，我们提出了一种将SHAP值中的不确定性分解为aleatoric、epistemic和纠缠成分的方法。该方法结合了Dempster-Shafer证据理论和基于Dirichlet过程的假设采样，应用于树集成。利用三种实际案例的描述性统计分析验证了该方法，提供了对SHAP解释中嵌入的epistemic不确定性性质的洞察。实验使我们能够更全面地理解基于SHAP的归因的可靠性和可解释性，这些理解可以指导在高风险应用中稳健决策过程的发展和模型的完善。通过多个数据集的实验，我们得出结论，具有最高SHAP值的特征并不一定是最稳定的。可以通过更好地使用代表性数据和遵循适当或特定于案例的模型开发技术来降低epistemic不确定性。基于树的模型，特别是袋装模型，有助于有效地量化epistemic不确定性。 

---
# The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards 

**Title (ZH)**: 奥赛罗AI竞技场：通过有限时间适应未见棋局评估智能系统 

**Authors**: Sundong Kim  

**Link**: [PDF](https://arxiv.org/pdf/2508.09292)  

**Abstract**: The ability to rapidly adapt to novel and unforeseen environmental changes is a cornerstone of artificial general intelligence (AGI), yet it remains a critical blind spot in most existing AI benchmarks. Traditional evaluation largely focuses on optimizing performance within fixed environments, failing to assess systems' flexibility and generalization capabilities when faced with even subtle rule or structural modifications. Addressing this gap, I introduce the Othello AI Arena, a novel benchmark framework designed to evaluate intelligent systems based on their capacity for limited-time adaptation to unseen environments. Our platform poses a meta-learning challenge: participants must develop systems that can analyze the specific configuration and rules of a novel Othello board within a strict time limit (60 seconds) and generate a tailored, high-performing strategy for that unique environment. With this, evaluation of the meta-level intelligence can be separated from the task-level strategy performance. The Arena features a diverse set of game stages, including public stages for development and private stages with structural and rule variations designed to test genuine adaptive and generalization capabilities. Implemented as an accessible web-based platform, the Arena provides real-time visualization, automated evaluation using multi-dimensional metrics, and comprehensive logging for post-hoc analysis. Initial observations from pilot tests and preliminary student engagements highlight fascinating patterns in adaptation approaches, ranging from rapid parameter tuning to rudimentary environmental model learning through simulation. The Othello AI Arena offers a unique educational tool and a valuable research benchmark for fostering and evaluating the crucial skill of rapid, intelligent adaptation in AI systems. 

**Abstract (ZH)**: 基于有限时间内适应未见环境能力的人工通用智能评估框架：愚公棋AI竞技场 

---
# Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning 

**Title (ZH)**: 深度强化学习中知识迁移与初始化的价值函数加速方法 

**Authors**: Soumia Mehimeh  

**Link**: [PDF](https://arxiv.org/pdf/2508.09277)  

**Abstract**: Value function initialization (VFI) is an effective way to achieve a jumpstart in reinforcement learning (RL) by leveraging value estimates from prior tasks. While this approach is well established in tabular settings, extending it to deep reinforcement learning (DRL) poses challenges due to the continuous nature of the state-action space, the noisy approximations of neural networks, and the impracticality of storing all past models for reuse. In this work, we address these challenges and introduce DQInit, a method that adapts value function initialization to DRL. DQInit reuses compact tabular Q-values extracted from previously solved tasks as a transferable knowledge base. It employs a knownness-based mechanism to softly integrate these transferred values into underexplored regions and gradually shift toward the agent's learned estimates, avoiding the limitations of fixed time decay. Our approach offers a novel perspective on knowledge transfer in DRL by relying solely on value estimates rather than policies or demonstrations, effectively combining the strengths of jumpstart RL and policy distillation while mitigating their drawbacks. Experiments across multiple continuous control tasks demonstrate that DQInit consistently improves early learning efficiency, stability, and overall performance compared to standard initialization and existing transfer techniques. 

**Abstract (ZH)**: 基于值函数初始化的深层强化学习方法：DQInit 

---
# VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models 

**Title (ZH)**: VisCodex: 统一的多模态代码生成模型通过融合视觉和编码模型 

**Authors**: Lingjie Jiang, Shaohan Huang, Xun Wu, Yixia Li, Dongdong Zhang, Furu Wei  

**Link**: [PDF](https://arxiv.org/pdf/2508.09945)  

**Abstract**: Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone, while preserving both visual comprehension and advanced coding skills. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions that demand a nuanced understanding of both textual and visual contexts. Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets. 

**Abstract (ZH)**: 多模态大语言模型（MLLMs）在视觉和文本理解的融合方面取得了显著进展。然而，它们从多模态输入生成代码的能力仍然有限。本文引入了VisCodex，这是一种统一框架，可无缝结合视觉和编程语言模型，增强MLLMs的多模态代码生成能力。通过基于任务向量的模型融合技术，我们将最先进的编程大语言模型集成到强大的视觉-语言骨干中，同时保留了视觉理解和高级编程技能。为了支持训练和评估，我们引入了多模态编程数据集（MCD），这个大规模且多样化的数据集包含598,000个样本，包括高质量的HTML代码、图表图像-代码对、图像增强的StackOverflow问答以及算法问题。此外，我们提出了一种新的更具挑战性的基准InfiBench-V，专门用于评估模型在丰富视觉的真实编程问题上的性能，这些问题是需要对文本和视觉上下文有深刻理解的。广泛的实验表明，VisCodex在开源MLLM中达到了最先进的性能，并接近如GPT-4o等 proprietary 模型，突显了我们模型融合策略和新数据集的有效性。 

---
# Residual Reservoir Memory Networks 

**Title (ZH)**: 残差储层记忆网络 

**Authors**: Matteo Pinna, Andrea Ceni, Claudio Gallicchio  

**Link**: [PDF](https://arxiv.org/pdf/2508.09925)  

**Abstract**: We introduce a novel class of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear reservoir, where the latter is based on residual orthogonal connections along the temporal dimension for enhanced long-term propagation of the input. The resulting reservoir state dynamics are studied through the lens of linear stability analysis, and we investigate diverse configurations for the temporal residual connections. The proposed approach is empirically assessed on time-series and pixel-level 1-D classification tasks. Our experimental results highlight the advantages of the proposed approach over other conventional RC models. 

**Abstract (ZH)**: 残差内存网络：基于残差正交连接的训练后递归神经网络 

---
# Rare anomalies require large datasets: About proving the existence of anomalies 

**Title (ZH)**: 稀有异常需要大量数据：关于异常存在性的证明 

**Authors**: Simon Klüttermann, Emmanuel Müller  

**Link**: [PDF](https://arxiv.org/pdf/2508.09894)  

**Abstract**: Detecting whether any anomalies exist within a dataset is crucial for effective anomaly detection, yet it remains surprisingly underexplored in anomaly detection literature. This paper presents a comprehensive study that addresses the fundamental question: When can we conclusively determine that anomalies are present? Through extensive experimentation involving over three million statistical tests across various anomaly detection tasks and algorithms, we identify a relationship between the dataset size, contamination rate, and an algorithm-dependent constant $ \alpha_{\text{algo}} $. Our results demonstrate that, for an unlabeled dataset of size $ N $ and contamination rate $ \nu $, the condition $ N \ge \frac{\alpha_{\text{algo}}}{\nu^2} $ represents a lower bound on the number of samples required to confirm anomaly existence. This threshold implies a limit to how rare anomalies can be before proving their existence becomes infeasible. 

**Abstract (ZH)**: 在数据集中检测是否存在异常对于有效的异常检测至关重要，但这一问题在异常检测文献中却意外地被严重忽视。本文进行了全面研究，探讨了可以确定异常存在的基本条件：当什么情况下我们能够断言异常确实存在？通过涵盖多种异常检测任务和算法的超过三百万次的统计测试，我们发现数据集大小、污染率与算法相关的常数$\alpha_{\text{algo}}$之间存在关系。研究结果表明，对于大小为$N$且污染率为$\nu$的无标签名注数据集，确认异常存在的样本数下限条件为$N \ge \frac{\alpha_{\text{algo}}}{\nu^2}$。这一阈值意味着在证明异常存在之前，异常可以有多罕见是有限制的。 

---
# COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets 

**Title (ZH)**: COME：跨异质超声数据集的协作门控双结构语义学习通用病灶检测 

**Authors**: Lingyu Chen, Yawen Zeng, Yue Wang, Peng Wan, Guo-chen Ning, Hongen Liao, Daoqiang Zhang, Fang Chen  

**Link**: [PDF](https://arxiv.org/pdf/2508.09886)  

**Abstract**: Conventional single-dataset training often fails with new data distributions, especially in ultrasound (US) image analysis due to limited data, acoustic shadows, and speckle noise. Therefore, constructing a universal framework for multi-heterogeneous US datasets is imperative. However, a key challenge arises: how to effectively mitigate inter-dataset interference while preserving dataset-specific discriminative features for robust downstream task? Previous approaches utilize either a single source-specific decoder or a domain adaptation strategy, but these methods experienced a decline in performance when applied to other domains. Considering this, we propose a Universal Collaborative Mixture of Heterogeneous Source-Specific Experts (COME). Specifically, COME establishes dual structure-semantic shared experts that create a universal representation space and then collaborate with source-specific experts to extract discriminative features through providing complementary features. This design enables robust generalization by leveraging cross-datasets experience distributions and providing universal US priors for small-batch or unseen data scenarios. Extensive experiments under three evaluation modes (single-dataset, intra-organ, and inter-organ integration datasets) demonstrate COME's superiority, achieving significant mean AP improvements over state-of-the-art methods. Our project is available at: this https URL. 

**Abstract (ZH)**: 常规单数据集训练在新数据分布面前往往失效，特别是在由于数据有限、声影和 speckle 噪音等因素导致的超声图像分析中。因此，构建适用于多异质超声数据集的通用框架变得至关重要。然而，一个关键挑战随之产生：如何有效地减轻数据集之间的干扰同时保持各数据集特有的判别特征，以实现稳健的下游任务？先前的方法要么使用单一来源特定解码器，要么采用领域适应策略，但这些方法在应用于其他领域时性能下降。考虑到这一点，我们提出了一个通用协作异质来源特定专家混合（Universal Collaborative Mixture of Heterogeneous Source-Specific Experts, COME）。具体而言，COME 建立了双重结构-语义共享专家，创建了一个通用的表示空间，然后与来源特定专家协作，通过提供互补特征来提取判别特征。这种设计通过利用跨数据集的经验分布，利用通用的超声先验信息，实现了稳健的实际应用。在三种评估模式（单数据集、同器官集成数据集和跨器官集成数据集）下进行的广泛实验证明了 COME 的优越性，其在平均精度（mean AP）上显著优于现有方法。我们的项目可在以下链接获取：this https URL。 

---
# STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports 

**Title (ZH)**: STREAM (ChemBio): 一种透明报告AI模型评估的标准 

**Authors**: Tegan McCaslin, Jide Alaga, Samira Nedungadi, Seth Donoughe, Tom Reed, Rishi Bommasani, Chris Painter, Luca Righetti  

**Link**: [PDF](https://arxiv.org/pdf/2508.09853)  

**Abstract**: Evaluations of dangerous AI capabilities are important for managing catastrophic risks. Public transparency into these evaluations - including what they test, how they are conducted, and how their results inform decisions - is crucial for building trust in AI development. We propose STREAM (A Standard for Transparently Reporting Evaluations in AI Model Reports), a standard to improve how model reports disclose evaluation results, initially focusing on chemical and biological (ChemBio) benchmarks. Developed in consultation with 23 experts across government, civil society, academia, and frontier AI companies, this standard is designed to (1) be a practical resource to help AI developers present evaluation results more clearly, and (2) help third parties identify whether model reports provide sufficient detail to assess the rigor of the ChemBio evaluations. We concretely demonstrate our proposed best practices with "gold standard" examples, and also provide a three-page reporting template to enable AI developers to implement our recommendations more easily. 

**Abstract (ZH)**: 危险AI能力的评估对于管理灾难性风险至关重要。公开透明的评估过程——包括测试的内容、实施方式及其结果如何影响决策——对于建立对AI开发的信任至关重要。我们提出STREAM标准（AI模型报告中透明报告评估的标准），旨在改善模型报告中披露评估结果的方式，初始重点关注化学和生物（ChemBio）基准。该标准在政府、民间社会、学术界和前沿AI公司的23位专家的咨询下制定，目的在于（1）作为实用资源帮助AI开发者更清晰地呈现评估结果，（2）帮助第三方识别模型报告是否提供了足够的细节以评估ChemBio评估的严谨性。我们通过“金标准”示例具体展示我们提出的最佳实践，并提供一个三页的报告模板，以便AI开发者更轻松地实施我们的建议。 

---
# PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts 

**Title (ZH)**: Prelude: 一个需具备长段落全局理解和推理能力的基准测试 

**Authors**: Mo Yu, Tsz Ting Chung, Chulun Zhou, Tong Li, Rui Lu, Jiangnan Li, Liyan Xu, Haoshu Lu, Ning Zhang, Jing Li, Jie Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2508.09848)  

**Abstract**: We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning. 

**Abstract (ZH)**: 我们介绍了PRELUDE，一个用于评估长上下文理解的基准，通过判断人物前传故事是否与原书 canonical 教义叙述一致的任务来进行评估。我们的任务对全局理解和深度推理提出了比现有基准更高的要求——因为前传不在原故事中，评估其可信度通常需要寻找和整合只有间接关联的信息。根据实验，88% 的实例需要来自叙述多个部分的证据。实验结果突显了我们任务的挑战性：上下文学习、RAG 和领域内训练与最先进的语言模型以及商业 DeepResearch 服务相比，落后于人类超过 15%。进一步的人类研究发现，模型经常以错误的推理得出正确的答案，导致推理准确性与人类相比存在超过 30% 的差距。这些发现强调了长上下文理解与推理方面的显著改进空间。 

---
# A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems 

**Title (ZH)**: 临床心理健康AI系统中数据集综述 

**Authors**: Aishik Mandal, Prottay Kumar Adhikary, Hiba Arnaout, Iryna Gurevych, Tanmoy Chakraborty  

**Link**: [PDF](https://arxiv.org/pdf/2508.09809)  

**Abstract**: Mental health disorders are rising worldwide. However, the availability of trained clinicians has not scaled proportionally, leaving many people without adequate or timely support. To bridge this gap, recent studies have shown the promise of Artificial Intelligence (AI) to assist mental health diagnosis, monitoring, and intervention. However, the development of efficient, reliable, and ethical AI to assist clinicians is heavily dependent on high-quality clinical training datasets. Despite growing interest in data curation for training clinical AI assistants, existing datasets largely remain scattered, under-documented, and often inaccessible, hindering the reproducibility, comparability, and generalizability of AI models developed for clinical mental health care. In this paper, we present the first comprehensive survey of clinical mental health datasets relevant to the training and development of AI-powered clinical assistants. We categorize these datasets by mental disorders (e.g., depression, schizophrenia), data modalities (e.g., text, speech, physiological signals), task types (e.g., diagnosis prediction, symptom severity estimation, intervention generation), accessibility (public, restricted or private), and sociocultural context (e.g., language and cultural background). Along with these, we also investigate synthetic clinical mental health datasets. Our survey identifies critical gaps such as a lack of longitudinal data, limited cultural and linguistic representation, inconsistent collection and annotation standards, and a lack of modalities in synthetic data. We conclude by outlining key challenges in curating and standardizing future datasets and provide actionable recommendations to facilitate the development of more robust, generalizable, and equitable mental health AI systems. 

**Abstract (ZH)**: 全球范围内心理健康障碍呈上升趋势。然而，受过训练的临床医生的 availability 并未相应增加，导致许多人无法获得足够的及时支持。为了弥合这一差距，近期研究表明，人工智能（AI）在协助心理健康诊断、监测和干预方面具有潜力。然而，开发高效、可靠且符合伦理的AI来协助临床医生，强烈依赖于高质量的临床训练数据集。尽管对训练临床AI助手的数据整理表现出日益浓厚的兴趣，但现有的数据集仍然分散、记录不足且往往难以访问，阻碍了为临床心理健康护理开发的AI模型的可再现性、可比性和泛化性。本文介绍了首个全面的临床心理健康数据集调查，这些数据集与AI驱动的临床助手的训练和发展相关。我们按精神障碍（如抑郁、精神分裂症）、数据模态（如文本、语音、生理信号）、任务类型（如诊断预测、症状严重程度估计、干预生成）、可访问性（公共、受限或私有）以及社会文化背景（如语言和文化背景）对这些数据集进行分类。此外，我们还调查了合成的临床心理健康数据集。我们的调查指出了诸如纵向数据缺乏、文化与语言代表性不足、数据收集和标注标准不一致以及合成数据模态缺乏等关键差距。最后，我们概述了未来数据集整理和标准化的关键挑战，并提供具体的建议以促进开发更稳健、更具泛化性和公平性的心理健康AI系统。 

---
# Explainable Ensemble Learning for Graph-Based Malware Detection 

**Title (ZH)**: 基于图的恶意软件检测的可解释集成学习 

**Authors**: Hossein Shokouhinejad, Roozbeh Razavi-Far, Griffin Higgins, Ali A Ghorbani  

**Link**: [PDF](https://arxiv.org/pdf/2508.09801)  

**Abstract**: Malware detection in modern computing environments demands models that are not only accurate but also interpretable and robust to evasive techniques. Graph neural networks (GNNs) have shown promise in this domain by modeling rich structural dependencies in graph-based program representations such as control flow graphs (CFGs). However, single-model approaches may suffer from limited generalization and lack interpretability, especially in high-stakes security applications. In this paper, we propose a novel stacking ensemble framework for graph-based malware detection and explanation. Our method dynamically extracts CFGs from portable executable (PE) files and encodes their basic blocks through a two-step embedding strategy. A set of diverse GNN base learners, each with a distinct message-passing mechanism, is used to capture complementary behavioral features. Their prediction outputs are aggregated by a meta-learner implemented as an attention-based multilayer perceptron, which both classifies malware instances and quantifies the contribution of each base model. To enhance explainability, we introduce an ensemble-aware post-hoc explanation technique that leverages edge-level importance scores generated by a GNN explainer and fuses them using the learned attention weights. This produces interpretable, model-agnostic explanations aligned with the final ensemble decision. Experimental results demonstrate that our framework improves classification performance while providing insightful interpretations of malware behavior. 

**Abstract (ZH)**: 现代计算环境中恶意软件检测需要既准确又可解释且对规避技术具有鲁棒性的模型。基于图的恶意软件检测和解释的新型堆叠ensemble框架。 

---
# Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations 

**Title (ZH)**: 双伪逆与优化隐藏激活的原型训练 

**Authors**: Mauro Tucci  

**Link**: [PDF](https://arxiv.org/pdf/2508.09787)  

**Abstract**: We present Proto-PINV+H, a fast training paradigm that combines closed-form weight computation with gradient-based optimisation of a small set of synthetic inputs, soft labels, and-crucially-hidden activations. At each iteration we recompute all weight matrices in closed form via two (or more) ridge-regularised pseudo-inverse solves, while updating only the prototypes with Adam. The trainable degrees of freedom are thus shifted from weight space to data/activation space. On MNIST (60k train, 10k test) and Fashion-MNIST (60k train, 10k test), our method reaches 97.8% and 89.3% test accuracy on the official 10k test sets, respectively, in 3.9s--4.5s using approximately 130k trainable parameters and only 250 epochs on an RTX 5060 (16GB). We provide a multi-layer extension (optimised activations at each hidden stage), learnable ridge parameters, optional PCA/PLS projections, and theory linking the condition number of prototype matrices to generalisation. The approach yields favourable accuracy--speed--size trade-offs against ELM, random-feature ridge, and shallow MLPs trained by back-propagation. 

**Abstract (ZH)**: Proto-PINV+H: 一种结合闭形式权重计算和基于梯度的小规模合成输入、软标签及关键隐层层活动的梯度优化的快速训练范式 

---
# Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges 

**Title (ZH)**: 可解释自然语言处理的采用：来自行业和学术界的实践与挑战视角 

**Authors**: Mahdi Dhaini, Tobias Müller, Roksoliana Rabets, Gjergji Kasneci  

**Link**: [PDF](https://arxiv.org/pdf/2508.09786)  

**Abstract**: The field of explainable natural language processing (NLP) has grown rapidly in recent years. The growing opacity of complex models calls for transparency and explanations of their decisions, which is crucial to understand their reasoning and facilitate deployment, especially in high-stakes environments. Despite increasing attention given to explainable NLP, practitioners' perspectives regarding its practical adoption and effectiveness remain underexplored. This paper addresses this research gap by investigating practitioners' experiences with explainability methods, specifically focusing on their motivations for adopting such methods, the techniques employed, satisfaction levels, and the practical challenges encountered in real-world NLP applications. Through a qualitative interview-based study with industry practitioners and complementary interviews with academic researchers, we systematically analyze and compare their perspectives. Our findings reveal conceptual gaps, low satisfaction with current explainability methods, and highlight evaluation challenges. Our findings emphasize the need for clear definitions and user-centric frameworks for better adoption of explainable NLP in practice. 

**Abstract (ZH)**: 可解释自然语言处理（NLP）领域在过去几年中得到了快速发展。随着复杂模型透明度的降低，迫切需要对其决策进行透明化和解释，这对于理解其推理过程以及在高风险环境中促进其部署至关重要。尽管对可解释NLP的关注不断增加，但从业者关于其实际应用和效果的观点仍缺乏探索。本文通过调查从业者在采用可解释性方法方面的体验，具体关注其采用动机、所运用的技术、满意度以及在实际NLP应用中遇到的实际挑战，填补了这一研究空白。通过面向工业从业者进行质性访谈，并辅以学术研究人员的访谈，我们系统地分析和比较了他们的观点。研究发现揭示了概念上的差距，当前可解释性方法的低满意度，以及评估挑战。研究结果强调了需要明确的定义和以用户为中心的框架，以便更好地在实践中采用可解释的NLP。 

---
# Counting Short Trajectories in Elementary Cellular Automata using the Transfer Matrix Method 

**Title (ZH)**: 用转移矩阵方法计数基本细胞自动机中的短轨迹 

**Authors**: Cédric Koller, Barbora Hudcová  

**Link**: [PDF](https://arxiv.org/pdf/2508.09768)  

**Abstract**: Elementary Cellular Automata (ECAs) exhibit diverse behaviours often categorized by Wolfram's qualitative classification. To provide a quantitative basis for understanding these behaviours, we investigate the global dynamics of such automata and we describe a method that allows us to compute the number of all configurations leading to short attractors in a limited number of time steps. This computation yields exact results in the thermodynamic limit (as the CA grid size grows to infinity), and is based on the Transfer Matrix Method (TMM) that we adapt for our purposes. Specifically, given two parameters $(p, c)$ we are able to compute the entropy of all initial configurations converging to an attractor of size $c$ after $p$ time-steps. By calculating such statistics for various ECA rules, we establish a quantitative connection between the entropy and the qualitative Wolfram classification scheme. Class 1 rules rapidly converge to maximal entropy for stationary states ($c=1$) as $p$ increases. Class 2 rules also approach maximal entropy quickly for appropriate cycle lengths $c$, potentially requiring consideration of translations. Class 3 rules exhibit zero or low finite entropy that saturates after a short transient. Class 4 rules show finite positive entropy, similar to some Class 3 rules. This method provides a precise framework for quantifying trajectory statistics, although its exponential computational cost in $p+c$ restricts practical analysis to short trajectories. 

**Abstract (ZH)**: 元胞自动机（ECAs）表现出多样化的行为，常被沃尔夫拉姆定性分类。为理解这些行为提供量化基础，我们研究了此类自动机的全局动力学，并描述了一种方法来计算在有限时间步内导致短吸引子的所有配置数量。该计算在CA网格尺寸趋向无穷大的热力学极限下提供了精确结果，并基于我们为此目的改编的转移矩阵方法（TMM）。具体来说，给定两个参数$(p, c)$，我们能够计算出所有在$p$个时间步后收敛到大小为$c$的吸引子的初始配置的熵。通过为各种ECA规则计算此类统计数据，我们建立了熵与沃尔夫拉姆定性分类方案之间的量化联系。Class 1规则在$p$增加时迅速收敛到稳定态（$c=1$）的最大熵。Class 2规则在合适周期长度$c$下也快速接近最大熵，可能需要考虑平移。Class 3规则显示出零或低的有限熵，在短暂态后饱和。Class 4规则显示出有限正熵，类似于某些Class 3规则。此方法为量化轨迹统计提供了一个精确框架，但由于其在$p+c$方面的指数级计算成本，只适用于短轨迹的实际分析。 

---
# Enhance the machine learning algorithm performance in phishing detection with keyword features 

**Title (ZH)**: 使用关键词特征增强机器学习算法在钓鱼检测中的性能 

**Authors**: Zijiang Yang  

**Link**: [PDF](https://arxiv.org/pdf/2508.09765)  

**Abstract**: Recently, we can observe a significant increase of the phishing attacks in the Internet. In a typical phishing attack, the attacker sets up a malicious website that looks similar to the legitimate website in order to obtain the end-users' information. This may cause the leakage of the sensitive information and the financial loss for the end-users. To avoid such attacks, the early detection of these websites' URLs is vital and necessary. Previous researchers have proposed many machine learning algorithms to distinguish the phishing URLs from the legitimate ones. In this paper, we would like to enhance these machine learning algorithms from the perspective of feature selection. We propose a novel method to incorporate the keyword features with the traditional features. This method is applied on multiple traditional machine learning algorithms and the experimental results have shown this method is useful and effective. On average, this method can reduce the classification error by 30% for the large dataset. Moreover, its enhancement is more significant for the small dataset. In addition, this method extracts the information from the URL and does not rely on the additional information provided by the third-part service. The best result for the machine learning algorithm using our proposed method has achieved the accuracy of 99.68%. 

**Abstract (ZH)**: 近年来，我们观察到互联网上的钓鱼攻击显著增加。在典型的钓鱼攻击中，攻击者设置一个看起来类似于合法网站的恶意网站，以获取终端用户的信息。这可能导致敏感信息泄露和经济损害。为了避免这类攻击，及早检测这些网站的URL至关重要。先前的研究人员提出了许多机器学习算法来区分钓鱼URL和合法URL。在本文中，我们将从特征选择的角度改进这些机器学习算法。我们提出了一种新颖的方法，将关键词特征与传统的特征结合。该方法应用于多种传统机器学习算法，实验结果表明这种方法是有效且有用的。对于大数据集，这种方法可以平均降低分类错误率30%。此外，对于小数据集，其改进更为显著。此外，该方法从URL中提取信息，而不依赖于第三方提供的额外信息。使用我们提出方法的最佳机器学习算法准确率达到99.68%。 

---
# NEUBORN: The Neurodevelopmental Evolution framework Using BiOmechanical RemodelliNg 

**Title (ZH)**: NEUBORN: 基于生物力学重塑的神经发育进化框架 

**Authors**: Nashira Baena, Mariana da Silva, Irina Grigorescu, Aakash Saboo, Saga Masui, Jaques-Donald Tournier, Emma C. Robinson  

**Link**: [PDF](https://arxiv.org/pdf/2508.09757)  

**Abstract**: Understanding individual cortical development is essential for identifying deviations linked to neurodevelopmental disorders. However, current normative modelling frameworks struggle to capture fine-scale anatomical details due to their reliance on modelling data within a population-average reference space. Here, we present a novel framework for learning individual growth trajectories from biomechanically constrained, longitudinal, diffeomorphic image registration, implemented via a hierarchical network architecture. Trained on neonatal MRI data from the Developing Human Connectome Project, the method improves the biological plausibility of warps, generating growth trajectories that better follow population-level trends while generating smoother warps, with fewer negative Jacobians, relative to state-of-the-art baselines. The resulting subject-specific deformations provide interpretable, biologically grounded mappings of development. This framework opens new possibilities for predictive modeling of brain maturation and early identification of malformations of cortical development. 

**Abstract (ZH)**: 理解个体皮层发育对于识别与神经发育障碍相关的偏差至关重要。然而，当前的规范模型框架由于依赖于在群体平均参考空间内建模数据，难以捕捉到细微的解剖细节。在这里，我们提出了一种新的框架，用于从生物力学约束的纵向 diffeomorphic 图像配准中学习个体生长轨迹，通过分层网络架构实现。该方法在开发人类连接组项目提供的新生儿 MRI 数据上进行训练，改进了拉伸的生物学合理性，生成了更能遵循群体水平趋势、更为平滑的拉伸，负面雅可比数更少，相对于先进的基线方法。由此产生的个体特定变形提供了可解释、生物学支持的发育映射。该框架为预测性建模大脑成熟和早期识别皮层发育异常提供了新可能性。 

---
# Anomaly Detection for IoT Global Connectivity 

**Title (ZH)**: 物联网全球连接中的异常检测 

**Authors**: Jesus Omaña Iglesias, Carlos Segura Perales, Stefan Geißler, Diego Perino, Andra Lutu  

**Link**: [PDF](https://arxiv.org/pdf/2508.09660)  

**Abstract**: Internet of Things (IoT) application providers rely on Mobile Network Operators (MNOs) and roaming infrastructures to deliver their services globally. In this complex ecosystem, where the end-to-end communication path traverses multiple entities, it has become increasingly challenging to guarantee communication availability and reliability. Further, most platform operators use a reactive approach to communication issues, responding to user complaints only after incidents have become severe, compromising service quality. This paper presents our experience in the design and deployment of ANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity service of a large global roaming platform. ANCHOR assists engineers by filtering vast amounts of data to identify potential problematic clients (i.e., those with connectivity issues affecting several of their IoT devices), enabling proactive issue resolution before the service is critically impacted. We first describe the IoT service, infrastructure, and network visibility of the IoT connectivity provider we operate. Second, we describe the main challenges and operational requirements for designing an unsupervised anomaly detection solution on this platform. Following these guidelines, we propose different statistical rules, and machine- and deep-learning models for IoT verticals anomaly detection based on passive signaling traffic. We describe the steps we followed working with the operational teams on the design and evaluation of our solution on the operational platform, and report an evaluation on operational IoT customers. 

**Abstract (ZH)**: IoT连接服务提供商依赖移动网络运营商(MNOs)和漫游基础设施在全球范围内交付其服务。在这个复杂的生态系统中，从一端到另一端的通信路径穿越多个实体，确保通信可用性和可靠性变得越来越具有挑战性。此外，大多数平台运营商采用反应式的通信问题处理方式，在问题严重化后再响应用户投诉，影响服务质量。本文介绍了我们在大规模全球漫游平台的IoT连接服务上设计并部署ANCHOR——一种无监督异常检测解决方案的经验。ANCHOR通过过滤大量数据来帮助工程师识别可能存在连接问题的终端客户（即那些影响其多个IoT设备的连接问题），从而在服务受到严重影响之前解决这些问题。首先，我们描述了我们运营的IoT服务、基础设施以及IoT连接提供商的网络可见性。其次，我们描述了在这个平台上设计无监督异常检测解决方案的主要挑战和操作要求。遵循这些指导原则，我们提出了基于被动信令流量的IoT垂直领域异常检测的不同统计规则和机器及深度学习模型。我们描述了与运营团队合作，在运营平台上设计和评估我们解决方案的过程，并在运营IoT客户上进行了评估。 

---
# Demystifying the Role of Rule-based Detection in AI Systems for Windows Malware Detection 

**Title (ZH)**: 揭开基于规则检测在Windows恶意软件检测的AI系统中的作用之谜 

**Authors**: Andrea Ponte, Luca Demetrio, Luca Oneto, Ivan Tesfai Ogbu, Battista Biggio, Fabio Roli  

**Link**: [PDF](https://arxiv.org/pdf/2508.09652)  

**Abstract**: Malware detection increasingly relies on AI systems that integrate signature-based detection with machine learning. However, these components are typically developed and combined in isolation, missing opportunities to reduce data complexity and strengthen defenses against adversarial EXEmples, carefully crafted programs designed to evade detection. Hence, in this work we investigate the influence that signature-based detection exerts on model training, when they are included inside the training pipeline. Specifically, we compare models trained on a comprehensive dataset with an AI system whose machine learning component is trained solely on samples not already flagged by signatures. Our results demonstrate improved robustness to both adversarial EXEmples and temporal data drift, although this comes at the cost of a fixed lower bound on false positives, driven by suboptimal rule selection. We conclude by discussing these limitations and outlining how future research could extend AI-based malware detection to include dynamic analysis, thereby further enhancing system resilience. 

**Abstract (ZH)**: 基于AI系统的签名检测集成与机器学习在恶意软件检测中的应用：研究签名检测对模型训练的影响及未来展望 

---
# A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories 

**Title (ZH)**: 基于细读方法的人工智能生成故事中性别叙事偏见探究 

**Authors**: Daniel Raffini, Agnese Macori, Marco Angelini, Tiziana Catarci  

**Link**: [PDF](https://arxiv.org/pdf/2508.09651)  

**Abstract**: The paper explores the study of gender-based narrative biases in stories generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's character classifications and Freytag's narrative structure. The stories are analyzed through a close reading approach, with particular attention to adherence to the prompt, gender distribution of characters, physical and psychological descriptions, actions, and finally, plot development and character relationships. The results reveal the persistence of biases - especially implicit ones - in the generated stories and highlight the importance of assessing biases at multiple levels using an interpretative approach. 

**Abstract (ZH)**: 该论文探讨了ChatGPT、Gemini和Claude生成的故事中基于性别的叙述偏见研究。启发式问题的设计参考了普罗普的角色分类和费特亚格的叙述结构。通过精细阅读的方法分析故事，特别关注对指令的遵循、角色的性别分布、物理和心理描述、行动以及最终的情节发展和角色关系。研究结果揭示了生成故事中偏见的持续存在，尤其是隐性的偏见，并强调了使用解释性方法在多个层面评估偏见的重要性。 

---
# TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling 

**Title (ZH)**: TimeMKG：融合知识的多元时间序列因果推理 

**Authors**: Yifei Sun, Junming Liu, Ding Wang, Yirong Chen, Xuefeng Yan  

**Link**: [PDF](https://arxiv.org/pdf/2508.09630)  

**Abstract**: Multivariate time series data typically comprises two distinct modalities: variable semantics and sampled numerical observations. Traditional time series models treat variables as anonymous statistical signals, overlooking the rich semantic information embedded in variable names and data descriptions. However, these textual descriptors often encode critical domain knowledge that is essential for robust and interpretable modeling. Here we present TimeMKG, a multimodal causal reasoning framework that elevates time series modeling from low-level signal processing to knowledge informed inference. TimeMKG employs large language models to interpret variable semantics and constructs structured Multivariate Knowledge Graphs that capture inter-variable relationships. A dual-modality encoder separately models the semantic prompts, generated from knowledge graph triplets, and the statistical patterns from historical time series. Cross-modality attention aligns and fuses these representations at the variable level, injecting causal priors into downstream tasks such as forecasting and classification, providing explicit and interpretable priors to guide model reasoning. The experiment in diverse datasets demonstrates that incorporating variable-level knowledge significantly improves both predictive performance and generalization. 

**Abstract (ZH)**: 多元时间序列数据通常包含两类不同的模态：变量的语义描述和采样数值观测。传统的时间序列模型将变量视为匿名的统计信号，忽略了变量名称和描述中蕴含的丰富语义信息。变量的双重文本描述经常编码了关键的先验知识，这对鲁棒且可易于解释的建模至关重要。为此，我们提出了TimeMKG，这是一个多模态因果推理框架，将时间序列建模提升到了基于知识推理的层级。TimeMKG利用大模型来解释变量的语义表征，并构建包含变量间关系的结构化多变量知识图谱。该双重模态编码器分别建模从知识图谱三元组中产生的语义提示和 历史时间序列中的统计模式。跨模态注意力在时间序列级别对这些表征进行对对 �对 同，并 与融合，并注入因果先验，指导下游任务，如预报与分类等工作，提供明确且易解释的先验知识来引导因果推理工作。实验表明，在不同情况下均显著提表了预测性能和跨任务的泛化能力。 

---
# MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography 

**Title (ZH)**: MInDI-3D: 三维迭代深度学习在稀视角锥束计算机断层成像中的应用 

**Authors**: Daniel Barco, Marc Stadelmann, Martin Oswald, Ivo Herzig, Lukas Lichtensteiger, Pascal Paysan, Igor Peterlik, Michal Walczak, Bjoern Menze, Frank-Peter Schilling  

**Link**: [PDF](https://arxiv.org/pdf/2508.09616)  

**Abstract**: We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first 3D conditional diffusion-based model for real-world sparse-view Cone Beam Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation exposure. A key contribution is extending the "InDI" concept from 2D to a full 3D volumetric approach for medical images, implementing an iterative denoising process that refines the CBCT volume directly from sparse-view input. A further contribution is the generation of a large pseudo-CBCT dataset (16,182) from chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We performed a comprehensive evaluation, including quantitative metrics, scalability analysis, generalisation tests, and a clinical assessment by 11 clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10) dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in imaging radiation exposure. We demonstrate its scalability by showing that performance improves with more training data. Importantly, MInDI-3D matches the performance of a 3D U-Net on real-world scans from 16 cancer patients across distortion and task-based metrics. It also generalises to new CBCT scanner geometries. Clinicians rated our model as sufficient for patient positioning across all anatomical sites and found it preserved lung tumour boundaries well. 

**Abstract (ZH)**: MInDI-3D：面向实际稀视角锥束计算机断层扫描伪影去除的3D条件扩散迭代模型 

---
# A Lightweight Learned Cardinality Estimation Model 

**Title (ZH)**: 一种轻量级的学习基数估计模型 

**Authors**: Yaoyu Zhu, Jintao Zhang, Guoliang Li, Jianhua Feng  

**Link**: [PDF](https://arxiv.org/pdf/2508.09602)  

**Abstract**: Cardinality estimation is a fundamental task in database management systems, aiming to predict query results accurately without executing the queries. However, existing techniques either achieve low estimation accuracy or incur high inference latency. Simultaneously achieving high speed and accuracy becomes critical for the cardinality estimation problem. In this paper, we propose a novel data-driven approach called CoDe (Covering with Decompositions) to address this problem. CoDe employs the concept of covering design, which divides the table into multiple smaller, overlapping segments. For each segment, CoDe utilizes tensor decomposition to accurately model its data distribution. Moreover, CoDe introduces innovative algorithms to select the best-fitting distributions for each query, combining them to estimate the final result. By employing multiple models to approximate distributions, CoDe excels in effectively modeling discrete distributions and ensuring computational efficiency. Notably, experimental results show that our method represents a significant advancement in cardinality estimation, achieving state-of-the-art levels of both estimation accuracy and inference efficiency. Across various datasets, CoDe achieves absolute accuracy in estimating more than half of the queries. 

**Abstract (ZH)**: 基数估计是数据库管理系统中的一个基本任务，旨在在不执行查询的情况下准确预测查询结果。然而，现有的技术要么准确度低，要么推断延迟高。同时实现高速和高准确度对于基数估计问题至关重要。在本文中，我们提出了一种名为CoDe（覆盖与分解）的新型数据驱动方法来解决这一问题。CoDe 使用覆盖设计的概念，将表格划分为多个较小的重叠段。对于每个段，CoDe 利用张量分解准确地模拟其数据分布。此外，CoDe 引入了创新的算法来为每个查询选择最适合的分布，并将它们结合以估计最终结果。通过使用多个模型近似分布，CoDe 在有效建模离散分布和确保计算效率方面表现出色。值得注意的是，实验结果表明，我们的方法在基数估计方面取得了重要进展，同时实现了最先进的估计准确度和推断效率。在各个数据集中，CoDe 在超过一半的查询中实现了绝对准确的基数估计。 

---
# Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma 

**Title (ZH)**: 胶质瘤基因型预测的分层脑结构建模 

**Authors**: Haotian Tang, Jianwei Chen, Xinrui Tang, Yunjia Wu, Zhengyang Miao, Chao Li  

**Link**: [PDF](https://arxiv.org/pdf/2508.09593)  

**Abstract**: Isocitrate DeHydrogenase (IDH) mutation status is a crucial biomarker for glioma prognosis. However, current prediction methods are limited by the low availability and noise of functional MRI. Structural and morphological connectomes offer a non-invasive alternative, yet existing approaches often ignore the brain's hierarchical organisation and multiscale interactions. To address this, we propose Hi-SMGNN, a hierarchical framework that integrates structural and morphological connectomes from regional to modular levels. It features a multimodal interaction module with a Siamese network and cross-modal attention, a multiscale feature fusion mechanism for reducing redundancy, and a personalised modular partitioning strategy to enhance individual specificity and interpretability. Experiments on the UCSF-PDGM dataset demonstrate that Hi-SMGNN outperforms baseline and state-of-the-art models, showing improved robustness and effectiveness in IDH mutation prediction. 

**Abstract (ZH)**: Iso Citrate DeHydrogenase (IDH) 突变状态是胶质瘤预后的一个关键生物标志物。然而，当前的预测方法受限于功能性 MRI 的低可用性和噪声。结构和形态连接组提供了一种无创的替代方案，但现有方法往往忽视了大脑的层次组织和多尺度交互。为了解决这一问题，我们提出了 Hi-SMGNN，这是一种从区域级到模块级集成结构和形态连接组的分层框架。该框架配备了具有Siamese网络和跨模态注意力的多模态交互模块、减少冗余的多尺度特征融合机制，以及增强个体特异性和可解释性的个性化模块分区策略。在 UCSF-PDGM 数据集上的实验表明，Hi-SMGNN 在 IDH 突变预测方面优于基线和最新模型，显示了更好的稳健性和有效性。 

---
# Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks 

**Title (ZH)**: 边缘辅助IoV网络中基于能耗约束的分布式多任务联邦微调排名调度 

**Authors**: Bokeng Zheng, Jianqiang Zhong, Jiayi Liu, Xiaoxi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2508.09532)  

**Abstract**: Federated fine-tuning has emerged as a promising approach for adapting foundation models (FMs) to diverse downstream tasks in edge environments. In Internet of Vehicles (IoV) systems, enabling efficient and low-latency multi-task adaptation is particularly challenging due to client mobility, heterogeneous resources, and intermittent connectivity. This paper proposes a hierarchical federated fine-tuning framework that coordinates roadside units (RSUs) and vehicles to support resource-aware and mobility-resilient learning across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we introduce a decentralized, energy-aware rank adaptation mechanism formulated as a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is developed to enable adaptive exploration under per-task energy budgets, achieving provable sublinear regret. To evaluate our method, we construct a large-scale IoV simulator based on real-world trajectories, capturing dynamic participation, RSU handoffs, and communication variability. Extensive experiments show that our approach achieves the best accuracy-efficiency trade-off among all baselines, reducing latency by over 24\% and improving average accuracy by more than 2.5\%. 

**Abstract (ZH)**: 联邦细调框架在物联网车辆系统中的多任务适应 

---
# Verify Distributed Deep Learning Model Implementation Refinement with Iterative Relation Inference 

**Title (ZH)**: 使用迭代关系推理验证分布式深度学习模型实现优化 

**Authors**: Zhanghan Wang, Ding Ding, Hang Zhu, Haibin Lin, Aurojit Panda  

**Link**: [PDF](https://arxiv.org/pdf/2508.09505)  

**Abstract**: Distributed machine learning training and inference is common today because today's large models require more memory and compute than can be provided by a single GPU. Distributed models are generally produced by programmers who take a sequential model specification and apply several distribution strategies to distribute state and computation across GPUs. Unfortunately, bugs can be introduced in the process, and a distributed model implementation's outputs might differ from the sequential model's outputs. In this paper, we describe an approach to statically identify such bugs by checking model refinement, that is, can the sequential model's outputs be reconstructed from the distributed model's outputs? Our approach, implemented in GraphGuard, uses iterative rewriting to prove model refinement. Our approach can scale to today's large models and deployments: we evaluate it using GPT and Llama-3. Further, it provides actionable output that aids in bug localization. 

**Abstract (ZH)**: 分布式机器学习训练与推理如今已非常普遍，因为当前的大模型需要的内存和计算能力超过了单个GPU所能提供的。分布式模型通常是由程序员通过对顺序模型进行一系列分布策略的应用来生成的，以将状态和计算分布在多块GPU上。不幸的是，在这一过程中可能会引入错误，导致分布式模型的输出与顺序模型的输出不同。在本文中，我们描述了一种通过检查模型细化来静态识别此类错误的方法，即分布式模型的输出是否可以从顺序模型的输出重构出来。我们的方法已在GraphGuard中实现，通过迭代重写来证明模型细化。该方法可以扩展到当前的大模型和部署场景：我们使用GPT和Llama-3进行了评估。此外，该方法提供了有助于错误定位的可操作输出。 

---
# From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation 

**Title (ZH)**: 从排名到选择：一种简单而高效的动态段落选择器用于检索增强生成 

**Authors**: Siyuan Meng, Junming Liu, Yirong Chen, Song Mao, Pinlong Cai, Guohang Yan, Botian Shi, Ding Wang  

**Link**: [PDF](https://arxiv.org/pdf/2508.09497)  

**Abstract**: Retrieval-augmented generation (RAG) systems are often bottlenecked by their reranking modules, which typically score passages independently and select a fixed Top-K size. This approach struggles with complex multi-hop queries that require synthesizing evidence across multiple documents, creating a trade-off where small K values omit crucial information and large K values introduce noise. To address this, we introduce the Dynamic Passage Selector (DPS), a novel reranking framework that treats passage selection as a supervised learning problem. Unlike traditional point-wise or list-wise methods, DPS is fine-tuned to capture inter-passage dependencies and dynamically select the most relevant set of passages for generation. As a seamless plug-and-play module, DPS requires no modifications to the standard RAG pipeline. Comprehensive evaluations on five benchmarks show that DPS consistently outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results demonstrate that by enabling adaptive evidence selection, DPS substantially enhances reasoning capabilities in complex RAG scenarios. 

**Abstract (ZH)**: 基于检索的生成（RAG）系统往往受限于其再排序模块，该模块通常独立评分段落并选取固定大小的Top-K。这种方法在处理需要跨多个文档合成证据的复杂多跳查询时存在困难，导致小K值舍弃重要信息而大K值引入噪声。为解决这一问题，我们引入了动态段落选择器（DPS），这是一种新颖的再排序框架，将段落选择视为监督学习问题。与传统的点wise或listwise方法不同，DPS微调以捕捉段落间的依赖性，并动态选择生成所需的相关段落集。作为无缝即插即用模块，DPS无需对标准RAG流程进行任何修改。在五个基准上的全面评估显示，DPS一致地优于最先进的再排序器和微调方法。特别是在具有挑战性的MuSiQue数据集中，DPS分别将F1分数提高了30.06%和15.4%，相较于如Qwen3-reranker和RankingGPT等强大的基线模型。我们的结果表明，通过使证据选择适应变化，DPS在复杂RAG场景中显著增强了推理能力。 

---
# Large-Small Model Collaborative Framework for Federated Continual Learning 

**Title (ZH)**: 大型-小型模型协作框架赋能联邦持续学习 

**Authors**: Hao Yu, Xin Yang, Boyang Fan, Xuemei Cao, Hanlin Gu, Lixin Fan, Qiang Yang  

**Link**: [PDF](https://arxiv.org/pdf/2508.09489)  

**Abstract**: Continual learning (CL) for Foundation Models (FMs) is an essential yet underexplored challenge, especially in Federated Continual Learning (FCL), where each client learns from a private, evolving task stream under strict data and communication constraints. Despite their powerful generalization abilities, FMs often exhibit suboptimal performance on local downstream tasks, as they are unable to utilize private local data. Furthermore, enabling FMs to learn new tasks without forgetting prior knowledge is inherently a challenging problem, primarily due to their immense parameter count and high model complexity. In contrast, small models can be trained locally under resource-constrained conditions and benefit from more mature CL techniques. To bridge the gap between small models and FMs, we propose the first collaborative framework in FCL, where lightweight local models act as a dynamic bridge, continually adapting to new tasks while enhancing the utility of the large model. Two novel components are also included: Small Model Continual Fine-tuning is for preventing small models from temporal forgetting; One-by-One Distillation performs personalized fusion of heterogeneous local knowledge on the server. Experimental results demonstrate its superior performance, even when clients utilize heterogeneous small models. 

**Abstract (ZH)**: Foundation模型在联邦持续学习中轻量级协作框架：面向小型模型与大型模型的动态桥梁 

---
# DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries 

**Title (ZH)**: DeepFeatIoT：统一深度学习、随机化和大规模语言模型特征以增强智能工业中物联网时间序列传感器数据分类 

**Authors**: Muhammad Sakib Khan Inan, Kewen Liao  

**Link**: [PDF](https://arxiv.org/pdf/2508.09468)  

**Abstract**: Internet of Things (IoT) sensors are ubiquitous technologies deployed across smart cities, industrial sites, and healthcare systems. They continuously generate time series data that enable advanced analytics and automation in industries. However, challenges such as the loss or ambiguity of sensor metadata, heterogeneity in data sources, varying sampling frequencies, inconsistent units of measurement, and irregular timestamps make raw IoT time series data difficult to interpret, undermining the effectiveness of smart systems. To address these challenges, we propose a novel deep learning model, DeepFeatIoT, which integrates learned local and global features with non-learned randomized convolutional kernel-based features and features from large language models (LLMs). This straightforward yet unique fusion of diverse learned and non-learned features significantly enhances IoT time series sensor data classification, even in scenarios with limited labeled data. Our model's effectiveness is demonstrated through its consistent and generalized performance across multiple real-world IoT sensor datasets from diverse critical application domains, outperforming state-of-the-art benchmark models. These results highlight DeepFeatIoT's potential to drive significant advancements in IoT analytics and support the development of next-generation smart systems. 

**Abstract (ZH)**: 物联网（IoT）传感器是部署在智慧城市、工业场所和 healthcare 系统中的通用技术。它们持续生成时间序列数据，这些数据使工业中的高级分析和自动化成为可能。然而，传感器元数据丢失或模糊、数据源的异构性、采样频率的差异、测量单位的一致性问题以及不规则的时间戳使得原始 IoT 时间序列数据难以解读，从而削弱了智能系统的效果。为了解决这些挑战，我们提出了一种新颖的深度学习模型——DeepFeatIoT，该模型集成了学习到的局部和全局特征、随机卷积核基特征以及大型语言模型（LLMs）的特征。这种简单而独特的不同学习和非学习特征的融合显著提升了物联网时间序列传感器数据的分类效果，即使在标注数据有限的情况下也是如此。我们的模型通过在多个来自多领域的真实世界 IoT 传感器数据集上的持续且泛化的性能表现，展示了其超越现有先进技术基准模型的效果。这些结果突显了DeepFeatIoT 在推动物联网分析进步方面的潜力，并支持下一代智能系统的开发。 

---
# A Unified Contrastive-Generative Framework for Time Series Classification 

**Title (ZH)**: 统一对比生成框架的时间序列分类 

**Authors**: Ziyu Liu, Azadeh Alavi, Minyi Li, Xiang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2508.09451)  

**Abstract**: Self-supervised learning (SSL) for multivariate time series mainly includes two paradigms: contrastive methods that excel at instance discrimination and generative approaches that model data distributions. While effective individually, their complementary potential remains unexplored. We propose a Contrastive Generative Time series framework (CoGenT), the first framework to unify these paradigms through joint contrastive-generative optimization. CoGenT addresses fundamental limitations of both approaches: it overcomes contrastive learning's sensitivity to high intra-class similarity in temporal data while reducing generative methods' dependence on large datasets. We evaluate CoGenT on six diverse time series datasets. The results show consistent improvements, with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE, respectively. Our analysis reveals that the hybrid objective preserves discriminative power while acquiring generative robustness. These findings establish a foundation for hybrid SSL in temporal domains. We will release the code shortly. 

**Abstract (ZH)**: 自监督学习（SSL）在多变量时间序列中的主要包含两种范式：擅长实例 discrimination 的对比方法和建模数据分布的生成方法。虽然这两种方法单独有效，但它们互补的潜力尚未被探索。我们提出了一种对比生成时间序列框架（CoGenT），这是首次通过联合对比-生成优化将这两种范式统一起来。CoGenT 解决了这两种方法的基本局限性：克服了对比学习在时间数据中高类内相似性上的敏感性，同时减少了生成方法对大规模数据集的依赖。我们在六个不同的时间序列数据集上评估了 CoGenT。结果显示，与独立的 SimCLR 和 MAE 相比，CoGenT 在 F1 值上分别提高了 59.2% 和 14.27%。我们的分析表明，混合目标既保留了区分能力又获得了生成的鲁棒性。这些发现为时间域中的混合 SSL 打下了基础。我们将很快发布代码。 

---
# Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees 

**Title (ZH)**: 隐式超图神经网络：一种具有可证明保证的高阶关系学习稳定框架 

**Authors**: Xiaoyu Li, Guangyu Tang, Jiaojiao Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2508.09427)  

**Abstract**: Many real-world interactions are group-based rather than pairwise such as papers with multiple co-authors and users jointly engaging with items. Hypergraph neural networks have shown great promise at modeling higher-order relations, but their reliance on a fixed number of explicit message-passing layers limits long-range dependency capture and can destabilize training as depth grows. In this work, we introduce Implicit Hypergraph Neural Networks (IHGNN), which bring the implicit equilibrium formulation to hypergraphs: instead of stacking layers, IHGNN computes representations as the solution to a nonlinear fixed-point equation, enabling stable and efficient global propagation across hyperedges without deep architectures. We develop a well-posed training scheme with provable convergence, analyze the oversmoothing conditions and expressivity of the model, and derive a transductive generalization bound on hypergraphs. We further present an implicit-gradient training procedure coupled with a projection-based stabilization strategy. Extensive experiments on citation benchmarks show that IHGNN consistently outperforms strong traditional graph/hypergraph neural network baselines in both accuracy and robustness. Empirically, IHGNN is resilient to random initialization and hyperparameter variation, highlighting its strong generalization and practical value for higher-order relational learning. 

**Abstract (ZH)**: 隐式超图神经网络：面向超图的隐式均衡表示 

---
# Domain-Generalization to Improve Learning in Meta-Learning Algorithms 

**Title (ZH)**: 泛化域提升元学习算法中的学习能力 

**Authors**: Usman Anjum, Chris Stockman, Cat Luong, Justin Zhan  

**Link**: [PDF](https://arxiv.org/pdf/2508.09418)  

**Abstract**: This paper introduces Domain Generalization Sharpness-Aware Minimization Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm designed to generalize across tasks with limited training data. DGS-MAML combines gradient matching with sharpness-aware minimization in a bi-level optimization framework to enhance model adaptability and robustness. We support our method with theoretical analysis using PAC-Bayes and convergence guarantees. Experimental results on benchmark datasets show that DGS-MAML outperforms existing approaches in terms of accuracy and generalization. The proposed method is particularly useful for scenarios requiring few-shot learning and quick adaptation, and the source code is publicly available at GitHub. 

**Abstract (ZH)**: 基于凹度aware最小化的方法不变域泛化元学习模型（DGS-MAML） 

---
# Understanding Dementia Speech Alignment with Diffusion-Based Image Generation 

**Title (ZH)**: 基于扩散驱动的图像生成理解痴呆症言语对
user
基于全局时空注意力的跨模态表达学习研究 

**Authors**: Mansi, Anastasios Lepipas, Dominika Woszczyk, Yiying Guan, Soteris Demetriou  

**Link**: [PDF](https://arxiv.org/pdf/2508.09385)  

**Abstract**: Text-to-image models generate highly realistic images based on natural language descriptions and millions of users use them to create and share images online. While it is expected that such models can align input text and generated image in the same latent space little has been done to understand whether this alignment is possible between pathological speech and generated images. In this work, we examine the ability of such models to align dementia-related speech information with the generated images and develop methods to explain this alignment. Surprisingly, we found that dementia detection is possible from generated images alone achieving 75% accuracy on the ADReSS dataset. We then leverage explainability methods to show which parts of the language contribute to the detection. 

**Abstract (ZH)**: 基于自然语言描述生成高度逼真图像的文本-to-图像模型被数百万用户用于在线创作和分享图像。尽管预计这些模型能够将输入文本和生成图像在相同的潜在空间中对齐，但对病理语言与生成图像间对齐可能性的研究甚少。在本文中，我们探讨了此类模型将与痴呆相关的语言信息与生成图像对齐的能力，并开发了解释这种对齐的方法。令人惊讶的是，我们发现仅从生成的图像中就可以检测痴呆，准确率达到ADReSS数据集的75%。随后，我们利用可解释性方法展示了哪些部分的语言对检测做出了贡献。 

---
# What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation? 

**Title (ZH)**: 从皮肤病变分割的注释者变异性中我们可以学到什么？ 

**Authors**: Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh  

**Link**: [PDF](https://arxiv.org/pdf/2508.09381)  

**Abstract**: Medical image segmentation exhibits intra- and inter-annotator variability due to ambiguous object boundaries, annotator preferences, expertise, and tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated or infiltrative nodules, or irregular borders per the ABCD rule, are particularly prone to disagreement and are often associated with malignancy. In this work, we curate IMA++, the largest multi-annotator skin lesion segmentation dataset, on which we conduct an in-depth study of variability due to annotator, malignancy, tool, and skill factors. We find a statistically significant (p<0.001) association between inter-annotator agreement (IAA), measured using Dice, and the malignancy of skin lesions. We further show that IAA can be accurately predicted directly from dermoscopic images, achieving a mean absolute error of 0.108. Finally, we leverage this association by utilizing IAA as a "soft" clinical feature within a multi-task learning objective, yielding a 4.2% improvement in balanced accuracy averaged across multiple model architectures and across IMA++ and four public dermoscopic datasets. The code is available at this https URL. 

**Abstract (ZH)**: 医学图像分割因注释者之间和内部的一致性差异受到对象边界模糊、注释者偏好、专业水平和工具等因素的影响。边界模糊的病灶，例如刺状或浸润性结节，或者根据ABCD规则具有不规则边界的病灶，特别容易出现分歧，并且经常与恶性病变相关。在本工作中，我们编纂了IMA++，这是最大的多注释者皮肤病变分割数据集，在此基础上我们深入研究了由注释者、恶性病变、工具和技能因素导致的一致性差异。我们发现使用Dice衡量的注释者间一致性（IAA）与皮肤病变的恶性程度之间存在统计学显著（p<0.001）的相关性。进一步地，我们展示了IAA可以从皮肤镜图像直接准确预测，平均绝对误差为0.108。最后，我们利用这种相关性，通过将IAA作为“软”临床特征纳入多任务学习目标，实现了4.2%的均衡准确率提升，这一结果在多个模型架构和IMA++及四个公开的皮肤镜数据集上均有体现。代码可在以下链接获取。 

---
# The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains 

**Title (ZH)**: 人类与AI混合德尔菲模型：一种结构化的框架，用于复杂领域丰富的上下文专家共识 

**Authors**: Cathy Speed, Ahmed A. Metwally  

**Link**: [PDF](https://arxiv.org/pdf/2508.09349)  

**Abstract**: Expert consensus plays a critical role in domains where evidence is complex, conflicting, or insufficient for direct prescription. Traditional methods, such as Delphi studies, consensus conferences, and systematic guideline synthesis, offer structure but face limitations including high panel burden, interpretive oversimplification, and suppression of conditional nuance. These challenges are now exacerbated by information overload, fragmentation of the evidence base, and increasing reliance on publicly available sources that lack expert filtering. This study introduces and evaluates a Human-AI Hybrid Delphi (HAH-Delphi) framework designed to augment expert consensus development by integrating a generative AI model (Gemini 2.5 Pro), small panels of senior human experts, and structured facilitation. The HAH-Delphi was tested in three phases: retrospective replication, prospective comparison, and applied deployment in two applied domains (endurance training and resistance and mixed cardio/strength training). The AI replicated 95% of published expert consensus conclusions in Phase I and showed 95% directional agreement with senior human experts in Phase II, though it lacked experiential and pragmatic nuance. In Phase III, compact panels of six senior experts achieved >90% consensus coverage and reached thematic saturation before the final participant. The AI provided consistent, literature-grounded scaffolding that supported divergence resolution and accelerated saturation. The HAH-Delphi framework offers a flexible, scalable approach for generating high-quality, context-sensitive consensus. Its successful application across health, coaching, and performance science confirms its methodological robustness and supports its use as a foundation for generating conditional, personalised guidance and published consensus frameworks at scale. 

**Abstract (ZH)**: 专家共识在证据复杂、冲突或不足的领域发挥着关键作用。传统的德尔菲研究、共识会议和系统指南综述方法虽提供了结构，但也存在专家负担重、解释简化和条件性细微差异被抑制等局限性。这些挑战如今因信息过载、证据基础碎片化以及对缺乏专家筛选的公共来源的日益依赖而被进一步加剧。本研究介绍并评估了结合生成式AI模型（Gemini 2.5 Pro）、细分的资深人类专家小组和结构化促进的混合人机德尔菲（HAH-Delphi）框架，以增强专家共识的发展。HAH-Delphi框架在三个阶段进行了测试：回顾性复制、前瞻性对比以及在两个应用领域（耐力训练和抗阻及混合有氧/力量训练）的应用部署。在第一阶段，AI复制了95%的已发表专家共识结论，在第二阶段，其与资深人类专家在方向上达成了95%的一致性，尽管缺乏经验性和实用性细微差异。在第三阶段，六名资深专家组成的精炼小组在最终参与者之前实现了>90%的主题饱和。AI提供了持续、基于文献的支撑，促进了分歧的解决并加速了饱和。HAH-Delphi框架提供了一种灵活且可扩展的方法，用于生成高质量、适用性敏感的共识。其在健康、教练和表现科学领域的成功应用证实了其方法论的稳健性，并支持其作为生成条件化和个性化指导及出版共识框架的基础工具的应用。 

---
# Collective dynamics of strategic classification 

**Title (ZH)**: 战略分类的集体动力学 

**Authors**: Marta C. Couto, Flavia Barsotti, Fernando P. Santos  

**Link**: [PDF](https://arxiv.org/pdf/2508.09340)  

**Abstract**: Classification algorithms based on Artificial Intelligence (AI) are nowadays applied in high-stakes decisions in finance, healthcare, criminal justice, or education. Individuals can strategically adapt to the information gathered about classifiers, which in turn may require algorithms to be re-trained. Which collective dynamics will result from users' adaptation and algorithms' retraining? We apply evolutionary game theory to address this question. Our framework provides a mathematically rigorous way of treating the problem of feedback loops between collectives of users and institutions, allowing to test interventions to mitigate the adverse effects of strategic adaptation. As a case study, we consider institutions deploying algorithms for credit lending. We consider several scenarios, each representing different interaction paradigms. When algorithms are not robust against strategic manipulation, we are able to capture previous challenges discussed in the strategic classification literature, whereby users either pay excessive costs to meet the institutions' expectations (leading to high social costs) or game the algorithm (e.g., provide fake information). From this baseline setting, we test the role of improving gaming detection and providing algorithmic recourse. We show that increased detection capabilities reduce social costs and could lead to users' improvement; when perfect classifiers are not feasible (likely to occur in practice), algorithmic recourse can steer the dynamics towards high users' improvement rates. The speed at which the institutions re-adapt to the user's population plays a role in the final outcome. Finally, we explore a scenario where strict institutions provide actionable recourse to their unsuccessful users and observe cycling dynamics so far unnoticed in the literature. 

**Abstract (ZH)**: 基于人工智能的分类算法在金融、医疗、刑事司法或教育领域的高风险决策中得到应用。个体可能会战略性地适应收集到的分类器信息，这反过来可能要求算法重新训练。用户适应和算法重新训练将产生何种集体动态？我们应用进化博弈论来回答这一问题。我们的框架提供了一种数学严谨的方法来处理用户集体与机构之间的反馈回路问题，允许测试减轻策略性适应负面影响的干预措施。作为案例研究，我们考虑机构部署用于信用贷款的算法。我们考虑了几个场景，每个场景代表不同的互动模式。当算法不具备对抗战略性操纵的鲁棒性时，我们能够捕捉战略分类文献中讨论的先前挑战，即用户要么付出过高的成本以满足机构的期望（导致高昂的社会成本），要么利用算法（例如，提供虚假信息）。从这一基线设置出发，我们测试了提升策略性利用检测能力和提供算法回溯的作用。我们表明，增强的检测能力可以降低社会成本，并可能促使用户改进；当完美的分类器不可行（在实践中很可能发生）时，算法回溯可以引导动态走向高用户改进率。机构重新适应用户群体的速度对最终结果起着作用。最后，我们探讨了一个严格的机构向其不成功用户提供可操作回溯的情景，并观察到到目前为止在文献中未注意到的循环动态。 

---
# RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs 

**Title (ZH)**: RicciFlowRec：一种基于金融图中 Ricci 曲率的几何根因推荐系统 

**Authors**: Zhongtian Sun, Anoushka Harit  

**Link**: [PDF](https://arxiv.org/pdf/2508.09334)  

**Abstract**: We propose RicciFlowRec, a geometric recommendation framework that performs root cause attribution via Ricci curvature and flow on dynamic financial graphs. By modelling evolving interactions among stocks, macroeconomic indicators, and news, we quantify local stress using discrete Ricci curvature and trace shock propagation via Ricci flow. Curvature gradients reveal causal substructures, informing a structural risk-aware ranking function. Preliminary results on S\&P~500 data with FinBERT-based sentiment show improved robustness and interpretability under synthetic perturbations. This ongoing work supports curvature-based attribution and early-stage risk-aware ranking, with plans for portfolio optimization and return forecasting. To our knowledge, RicciFlowRec is the first recommender to apply geometric flow-based reasoning in financial decision support. 

**Abstract (ZH)**: 我们提出RicciFlowRec，一种基于 Ricci 曲率和流的几何推荐框架，通过动态金融图上的 Ricci 曲率和流执行根本原因归因。通过建模股票、宏观经济指标和新闻之间 evolving 的交互，我们使用离散 Ricci 曲率定量局部应力，并通过 Ricci 流追踪冲击传播。曲率梯度揭示因果子结构，为结构风险意识排名功能提供信息。基于 FinBERT 的情绪分析在 S&P 500 数据上初步结果显示，在合成扰动下具有改进的鲁棒性和可解释性。这项正在进行的工作支持基于曲率的归因和早期风险意识排名，并计划应用于投资组合优化和收益预测。据我们所知，RicciFlowRec 是首个在金融决策支持中应用几何流推理的推荐系统。 

---
# Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization 

**Title (ZH)**: 突触修剪：对深度学习正则化的一种生物灵感 

**Authors**: Gideon Vos, Liza van Eijk, Zoltan Sarnyai, Mostafa Rahimi Azghadi  

**Link**: [PDF](https://arxiv.org/pdf/2508.09330)  

**Abstract**: Synaptic pruning in biological brains removes weak connections to improve efficiency. In contrast, dropout regularization in artificial neural networks randomly deactivates neurons without considering activity-dependent pruning. We propose a magnitude-based synaptic pruning method that better reflects biology by progressively removing low-importance connections during training. Integrated directly into the training loop as a dropout replacement, our approach computes weight importance from absolute magnitudes across layers and applies a cubic schedule to gradually increase global sparsity. At fixed intervals, pruning masks permanently remove low-importance weights while maintaining gradient flow for active ones, eliminating the need for separate pruning and fine-tuning phases. Experiments on multiple time series forecasting models including RNN, LSTM, and Patch Time Series Transformer across four datasets show consistent gains. Our method ranked best overall, with statistically significant improvements confirmed by Friedman tests (p < 0.01). In financial forecasting, it reduced Mean Absolute Error by up to 20% over models with no or standard dropout, and up to 52% in select transformer models. This dynamic pruning mechanism advances regularization by coupling weight elimination with progressive sparsification, offering easy integration into diverse architectures. Its strong performance, especially in financial time series forecasting, highlights its potential as a practical alternative to conventional dropout techniques. 

**Abstract (ZH)**: 基于幅度的突触剪枝方法通过渐进移除低重要性连接更好地反映生物学原理 

---
# Exact Verification of Graph Neural Networks with Incremental Constraint Solving 

**Title (ZH)**: 增量约束求解实现图神经网络的精确验证 

**Authors**: Minghao Liu, Chia-Hsuan Lu, Marta Kwiatkowska  

**Link**: [PDF](https://arxiv.org/pdf/2508.09320)  

**Abstract**: Graph neural networks (GNNs) are increasingly employed in high-stakes applications, such as fraud detection or healthcare, but are susceptible to adversarial attacks. A number of techniques have been proposed to provide adversarial robustness guarantees, but support for commonly used aggregation functions in message-passing GNNs is still lacking. In this paper, we develop an exact (sound and complete) verification method for GNNs to compute guarantees against attribute and structural perturbations that involve edge addition or deletion, subject to budget constraints. Focusing on node classification tasks, our method employs constraint solving with bound tightening, and iteratively solves a sequence of relaxed constraint satisfaction problems while relying on incremental solving capabilities of solvers to improve efficiency. We implement GNNev, a versatile solver for message-passing neural networks, which supports three aggregation functions, sum, max and mean, with the latter two considered here for the first time. Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its usability and effectiveness, as well as superior performance compared to existing {exact verification} tools on sum-aggregated node classification tasks. 

**Abstract (ZH)**: Graph神经网络（GNNs）在高风险应用中的精确（可靠且完备）验证方法：针对预算约束下的属性和结构扰动保证计算 

---
# TPTP World Infrastructure for Non-classical Logics 

**Title (ZH)**: 非古典逻辑的TPTP世界基础设施 

**Authors**: Alexander Steen, Geoff Sutcliffe  

**Link**: [PDF](https://arxiv.org/pdf/2508.09318)  

**Abstract**: The TPTP World is the well established infrastructure that supports research, development, and deployment of Automated Theorem Proving (ATP) systems. The TPTP World supports a range of classical logics, and since release v9.0.0 has supported non-classical logics. This paper provides a self-contained comprehensive overview of the TPTP World infrastructure for ATP in non-classical logics: the non-classical language extension, problems and solutions, and tool support. A detailed description of use of the infrastructure for quantified normal multi-modal logic is given. 

**Abstract (ZH)**: 非经典逻辑下自动定理证明系统TPTP世界的自包含综合 Overview of the TPTP World Infrastructure for Automated Theorem Proving in Non-classical Logics: Non-classical Language Extension, Problem and Solution Support, and Tool Support, with a Detailed Description for Quantified Normal Modal Logic 

---
# Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation 

**Title (ZH)**: 基于区块链模型验证的去中心化天气预报：分布式机器学习方法 

**Authors**: Rilwan Umar, Aydin Abadi, Basil Aldali, Benito Vincent, Elliot A. J. Hurley, Hotoon Aljazaeri, Jamie Hedley-Cook, Jamie-Lee Bell, Lambert Uwuigbusun, Mujeeb Ahmed, Shishir Nagaraja, Suleiman Sabo, Weaam Alrbeiqi  

**Link**: [PDF](https://arxiv.org/pdf/2508.09299)  

**Abstract**: Weather forecasting plays a vital role in disaster preparedness, agriculture, and resource management, yet current centralized forecasting systems are increasingly strained by security vulnerabilities, limited scalability, and susceptibility to single points of failure. To address these challenges, we propose a decentralized weather forecasting framework that integrates Federated Learning (FL) with blockchain technology. FL enables collaborative model training without exposing sensitive local data; this approach enhances privacy and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures transparent and dependable verification of model updates. To further enhance the system's security, we introduce a reputation-based voting mechanism that assesses the trustworthiness of submitted models while utilizing the Interplanetary File System (IPFS) for efficient off-chain storage. Experimental results demonstrate that our approach not only improves forecasting accuracy but also enhances system resilience and scalability, making it a viable candidate for deployment in real-world, security-critical environments. 

**Abstract (ZH)**: 一种集成联邦学习与区块链的去中心化天气预报框架 

---
# Based AI improves human decision-making but reduces trust 

**Title (ZH)**: 基于AI提升人类决策但降低信任 

**Authors**: Shiyang Lai, Junsol Kim, Nadav Kunievsky, Yujin Potter, James Evans  

**Link**: [PDF](https://arxiv.org/pdf/2508.09297)  

**Abstract**: Current AI systems minimize risk by enforcing ideological neutrality, yet this may introduce automation bias by suppressing cognitive engagement in human decision-making. We conducted randomized trials with 2,500 participants to test whether culturally biased AI enhances human decision-making. Participants interacted with politically diverse GPT-4o variants on information evaluation tasks. Partisan AI assistants enhanced human performance, increased engagement, and reduced evaluative bias compared to non-biased counterparts, with amplified benefits when participants encountered opposing views. These gains carried a trust penalty: participants underappreciated biased AI and overcredited neutral systems. Exposing participants to two AIs whose biases flanked human perspectives closed the perception-performance gap. These findings complicate conventional wisdom about AI neutrality, suggesting that strategic integration of diverse cultural biases may foster improved and resilient human decision-making. 

**Abstract (ZH)**: 当前的AI系统通过维护意识形态中立来最小化风险，但这也可能通过抑制人类决策中的认知参与引入自动化偏差。我们通过随机试验测试了文化偏见的AI是否能增强人类决策能力。参与者在处理信息评估任务时与政治立场不同的GPT-4o变体互动。具有党派偏见的AI助手在人类性能、参与度和评价偏差方面优于非偏见对应版本，并且当参与者遇到对立观点时，这种益处更明显。这些收益伴随着信任成本：参与者低估了带有偏见的AI，而高估了中立系统。向参与者介绍两种偏见互补的AI使感知与表现之间的差距缩小。这些发现令人重新审视关于AI中立性的传统观念，表明战略性地整合多种文化偏见可能促进更优秀和更稳健的人类决策。 

---
# Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative 

**Title (ZH)**: Fake-Mamba：使用双向Mamba替代自注意力的实时语音深度伪造检测 

**Authors**: Xi Xuan, Zimo Zhu, Wenxin Zhang, Yi-Cheng Lin, Tomi Kinnunen  

**Link**: [PDF](https://arxiv.org/pdf/2508.09294)  

**Abstract**: Advances in speech synthesis intensify security threats, motivating real-time deepfake detection research. We investigate whether bidirectional Mamba can serve as a competitive alternative to Self-Attention in detecting synthetic speech. Our solution, Fake-Mamba, integrates an XLSR front-end with bidirectional Mamba to capture both local and global artifacts. Our core innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof 21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and 5.85% EER, respectively, representing substantial relative gains over SOTA models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time inference across utterance lengths, demonstrating strong generalization and practical viability. The code is available at this https URL. 

**Abstract (ZH)**: 语音合成技术的进步加剧了安全威胁，推动了实时深度伪造检测研究。我们探讨Bi-directional Mamba是否能作为Self-Attention的竞争力替代方案用于检测合成语音。我们的解决方案Fake-Mamba结合了XLSR前端和Bi-directional Mamba，以捕捉局部和全局特征。我们的核心创新引入了三种高效的编码器：TransBiMamba、ConBiMamba和PN-BiMamba。利用XLSR丰富的语言表示，PN-BiMamba能有效捕捉合成语音的微妙线索。在ASVspoof 21 LA、21 DF和In-The-Wild基准测试上，Fake-Mamba分别实现了0.97%、1.74%和5.85%的EER，显著优于SOTA模型XLSR-Conformer和XLSR-Mamba。该框架在不同语句长度下保持实时推理，展示了强大的通用性和实际可行性。代码可在此网址获取。 

---
# Ethical Medical Image Synthesis 

**Title (ZH)**: 伦理医学图像合成 

**Authors**: Weina Jin, Ashish Sinha, Kumar Abhishek, Ghassan Hamarneh  

**Link**: [PDF](https://arxiv.org/pdf/2508.09293)  

**Abstract**: The task of ethical Medical Image Synthesis (MISyn) is to ensure that the MISyn techniques are researched and developed ethically throughout their entire lifecycle, which is essential to prevent the negative impacts of MISyn. To address the ever-increasing needs and requirements for ethical practice of MISyn research and development, we first conduct a theoretical analysis that identifies the key properties of ethical MISyn and intrinsic limits of MISyn. We identify that synthetic images lack inherent grounding in real medical phenomena, cannot fully represent the training medical images, and inevitably introduce new distribution shifts and biases.
Ethical risks can arise from not acknowledging the intrinsic limits and weaknesses of synthetic images compared to medical images, with the extreme form manifested as misinformation of MISyn that substitutes synthetic images for medical images without acknowledgment. The resulting ethical harms include eroding trust in the medical imaging dataset environment and causing algorithmic discrimination towards stakeholders and the public.
To facilitate collective efforts towards ethical MISyn within and outside the medical image analysis community, we then propose practical supports for ethical practice in MISyn based on the theoretical analysis, including ethical practice recommendations that adapt the existing technical standards, problem formulation, design, and evaluation practice of MISyn to the ethical challenges; and oversight recommendations to facilitate checks and balances from stakeholders and the public. We also present two case studies that demonstrate how to apply the ethical practice recommendations in practice, and identify gaps between existing practice and the ethical practice recommendations. 

**Abstract (ZH)**: 伦理医学图像合成的任务是在其整个生命周期中确保伦理医学图像合成技术的研究与开发，以防止医学图像合成的负面影响。为了应对医学图像合成研究与开发中的不断增长的伦理实践需求和要求，我们首先进行了理论分析，以识别伦理医学图像合成的关键属性和固有限制。我们发现合成图像缺乏与真实医学现象的内在关联，无法充分代表训练医学图像，并不可避免地引入新的分布偏移和偏差。

由于未能认识到合成图像与医学图像之间的内在限制和弱点，伦理风险可能会产生，极端表现为将合成图像替代医学图像而不加以承认的医学图像合成误导。由此产生的伦理危害包括损害医学影像数据环境的信任，并对相关方和公众造成算法歧视。

为了促进医学图像分析社区内外的集体努力，以实现伦理医学图像合成，我们基于理论分析提出了实用的支持措施，包括伦理实践建议，将医学图像合成的现有技术标准、问题定义、设计和评估实践适应伦理挑战；以及监督建议，促进相关方和公众的监督与平衡。我们还呈现了两个案例研究，说明如何在实践中应用伦理实践建议，并指出了现有实践与伦理实践建议之间的差距。 

---
# Detection of Odor Presence via Deep Neural Networks 

**Title (ZH)**: 基于深度神经网络的气味存在检测 

**Authors**: Matin Hassanloo, Ali Zareh, Mehmet Kemal Özdemir  

**Link**: [PDF](https://arxiv.org/pdf/2508.09264)  

**Abstract**: Odor detection underpins food safety, environmental monitoring, medical diagnostics, and many more fields. The current artificial sensors developed for odor detection struggle with complex mixtures while non-invasive recordings lack reliable single-trial fidelity. To develop a general system for odor detection, in this study we present a preliminary work where we aim to test two hypotheses: (i) that spectral features of local field potentials (LFPs) are sufficient for robust single-trial odor detection and (ii) that signals from the olfactory bulb alone are adequate. To test two hypotheses, we propose an ensemble of complementary one-dimensional convolutional networks (ResCNN and AttentionCNN) that decodes the presence of odor from multichannel olfactory bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score of 81.0%, and an AUC of 0.9247, substantially outperforming previous benchmarks. In addition, the t-SNE visualization confirms that our framework captures biologically significant signatures. These findings establish the feasibility of robust single-trial detection of the presence of odor from extracellular LFPs, as well as demonstrate the potential of deep learning models to provide a deeper understanding of olfactory representations. 

**Abstract (ZH)**: 气味检测是食品安全、环境监测、医学诊断等多个领域的基石。当前为气味检测开发的人工传感器在复杂混合物中表现不佳，而无侵入性的记录方法缺乏可靠的单次试验准确性。为了开发一种通用的气味检测系统，在本研究中我们提出了初步工作，旨在验证两个假设：（i）局部场电位（LFP）的光谱特征足以实现稳健的单次试验气味检测，（ii）仅使用嗅球信号是足够的。为了验证这两个假设，我们提出了一种互补的一维卷积网络集成（ResCNN和AttentionCNN），用于从多通道嗅球LFP中解码气味的存在。在七只清醒小鼠的2,349次试验上进行测试，我们的最终集成模型同时支持了这两个假设，平均准确性达到86.6%，F1分数为81.0%，AUC为0.9247，显著优于之前的基准。此外，t-SNE可视化结果证实了我们框架能够捕获生物学上显著的特征。这些发现表明，可以从体外LFP中实现稳健的单次试验气味检测的可行性，并证明深度学习模型能够为气味表示提供更深入的理解。 

---
# Cross-BCI, A Cross-BCI-Paradigm Classifica-tion Model Towards Universal BCI Applications 

**Title (ZH)**: 跨BCI：一种面向通用BCI应用的跨BCI paradigms分类模型 

**Authors**: Gaojie Zhou, Junhua Li  

**Link**: [PDF](https://arxiv.org/pdf/2508.09242)  

**Abstract**: Classification models used in brain-computer interface (BCI) are usually designed for a single BCI paradigm. This requires the redevelopment of the model when applying it to a new BCI paradigm, resulting in repeated costs and effort. Moreover, less complex deep learning models are desired for practical usage, as well as for deployment on portable devices. In or-der to fill the above gaps, we, in this study, proposed a light-weight and unified decoding model for cross-BCI-paradigm classification. The proposed model starts with a tempo-spatial convolution. It is followed by a multi-scale local feature selec-tion module, aiming to extract local features shared across BCI paradigms and generate weighted features. Finally, a mul-ti-dimensional global feature extraction module is designed, in which multi-dimensional global features are extracted from the weighted features and fused with the weighted features to form high-level feature representations associated with BCI para-digms. The results, evaluated on a mixture of three classical BCI paradigms (i.e., MI, SSVEP, and P300), demon-strate that the proposed model achieves 88.39%, 82.36%, 80.01%, and 0.8092 for accuracy, macro-precision, mac-ro-recall, and macro-F1-score, respectively, significantly out-performing the compared models. This study pro-vides a feasible solution for cross-BCI-paradigm classifica-tion. It lays a technological foundation for de-veloping a new generation of unified decoding systems, paving the way for low-cost and universal practical applications. 

**Abstract (ZH)**: 用于跨脑机接口范式分类的轻量级统一解码模型 

---
# PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research 

**Title (ZH)**: PETLP: 以隐私设计为导向的社会媒体数据处理流水线在人工智能研究中 

**Authors**: Nick Oh, Giorgos D. Vrakas, Siân J. M. Brooke, Sasha Morinière, Toju Duke  

**Link**: [PDF](https://arxiv.org/pdf/2508.09232)  

**Abstract**: Social media data presents AI researchers with overlapping obligations under the GDPR, copyright law, and platform terms -- yet existing frameworks fail to integrate these regulatory domains, leaving researchers without unified guidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and Present), a compliance framework that embeds legal safeguards directly into extended ETL pipelines. Central to PETLP is treating Data Protection Impact Assessments as living documents that evolve from pre-registration through dissemination. Through systematic Reddit analysis, we demonstrate how extraction rights fundamentally differ between qualifying research organisations (who can invoke DSM Article 3 to override platform restrictions) and commercial entities (bound by terms of service), whilst GDPR obligations apply universally. We reveal why true anonymisation remains unachievable for social media data and expose the legal gap between permitted dataset creation and uncertain model distribution. By structuring compliance decisions into practical workflows and simplifying institutional data management plans, PETLP enables researchers to navigate regulatory complexity with confidence, bridging the gap between legal requirements and research practice. 

**Abstract (ZH)**: PETLP（设计隐私-提取、转换、加载和呈现）：一个嵌入法律保障的合规框架 

---
# Beyond Technocratic XAI: The Who, What & How in Explanation Design 

**Title (ZH)**: 超越技术官僚主义的解释设计：谁来解释、解释什么及如何解释 

**Authors**: Ruchira Dhar, Stephanie Brandl, Ninell Oldenburg, Anders Søgaard  

**Link**: [PDF](https://arxiv.org/pdf/2508.09231)  

**Abstract**: The field of Explainable AI (XAI) offers a wide range of techniques for making complex models interpretable. Yet, in practice, generating meaningful explanations is a context-dependent task that requires intentional design choices to ensure accessibility and transparency. This paper reframes explanation as a situated design process -- an approach particularly relevant for practitioners involved in building and deploying explainable systems. Drawing on prior research and principles from design thinking, we propose a three-part framework for explanation design in XAI: asking Who needs the explanation, What they need explained, and How that explanation should be delivered. We also emphasize the need for ethical considerations, including risks of epistemic inequality, reinforcing social inequities, and obscuring accountability and governance. By treating explanation as a sociotechnical design process, this framework encourages a context-aware approach to XAI that supports effective communication and the development of ethically responsible explanations. 

**Abstract (ZH)**: 可解释人工智能（XAI）领域的技术为复杂模型提供了广泛的解释方法。然而，在实践中，生成有意义的解释是一个依赖于具体上下文的任务，需要通过有意的设计选择来确保可访问性和透明度。本文将解释重新框定为一种情境化设计过程——这种方法特别适用于参与构建和部署可解释系统的实践者。基于先前的研究和设计思维原则，我们提出了一种三部分的解释设计框架：确定谁需要解释、确定需要解释什么以及确定如何传递这种解释。我们也强调了伦理考量的重要性，包括知识不平等的风险、强化社会不平等以及掩盖问责制和治理的问题。通过将解释视为一种社会技术设计过程，本框架鼓励一种情境意识的方法，以支持有效的沟通和促进负责任的解释的发展。 

---
# Cowpox: Towards the Immunity of VLM-based Multi-Agent Systems 

**Title (ZH)**: 牛痘：面向基于VLM的多智能体系统的免疫性研究 

**Authors**: Yutong Wu, Jie Zhang, Yiming Li, Chao Zhang, Qing Guo, Nils Lukas, Tianwei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2508.09230)  

**Abstract**: Vision Language Model (VLM)-based agents are stateful, autonomous entities capable of perceiving and interacting with their environments through vision and language. Multi-agent systems comprise specialized agents who collaborate to solve a (complex) task. A core security property is robustness, stating that the system should maintain its integrity under adversarial attacks. However, the design of existing multi-agent systems lacks the robustness consideration, as a successful exploit against one agent can spread and infect other agents to undermine the entire system's assurance. To address this, we propose a new defense approach, Cowpox, to provably enhance the robustness of multi-agent systems. It incorporates a distributed mechanism, which improves the recovery rate of agents by limiting the expected number of infections to other agents. The core idea is to generate and distribute a special cure sample that immunizes an agent against the attack before exposure and helps recover the already infected agents. We demonstrate the effectiveness of Cowpox empirically and provide theoretical robustness guarantees. 

**Abstract (ZH)**: 基于视觉语言模型（VLM）的实体在多智能体系统中的状态感知自主代理能够通过视觉和语言感知和交互其环境。多智能体系统由专门合作解决复杂任务的智能体组成。核心安全属性是鲁棒性，表明系统应在 adversarial 攻击下保持其完整性。然而，现有多智能体系统的设计缺乏鲁棒性考虑，因为对一个智能体的成功攻击可以扩散并感染其他智能体，从而破坏整个系统的保障。为解决这一问题，我们提出了一种新的防御方法 Cowpox，以证明性地增强多智能体系统的鲁棒性。Cowpox Incorporates 分布式机制，通过限制对其他智能体的预期感染数来提高智能体的恢复率。核心思想是生成和分发一种特殊的治愈样本，在暴露前使智能体免疫，并帮助恢复已受感染的智能体。我们通过实验证明了 Cowpox 的有效性，并提供了理论上的鲁棒性保证。 

---
# Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference 

**Title (ZH)**: 基于聚类拓扑的专家放置减少MoE推理中的网络流量 

**Authors**: Danil Sivtsov, Aleksandr Katrutsa, Ivan Oseledets  

**Link**: [PDF](https://arxiv.org/pdf/2508.09229)  

**Abstract**: Efficient deployment of a pre-trained LLM to a cluster with multiple servers is a critical step for providing fast responses to users' queries. The recent success of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy them efficiently, considering their underlying structure. During the inference in MoE LLMs, only a small part of the experts is selected to process a given token. Moreover, in practice, the experts' load is highly imbalanced. For efficient deployment, one has to distribute the model across a large number of servers using a model placement algorithm. Thus, to improve cluster utilization, the model placement algorithm has to take into account the network topology. This work focuses on the efficient topology-aware placement of the pre-trained MoE LLMs in the inference stage. We propose an integer linear program (ILP) that determines the optimal placement of experts, minimizing the expected number of transmissions. Due to the internal structure, this optimization problem can be solved with a standard ILP solver. We demonstrate that ILP-based placement strategy yields lower network traffic than competitors for small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models. 

**Abstract (ZH)**: 预训练MoE大语言模型在多服务器集群中的高效部署及其拓扑感知放置策略 

---
# GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction 

**Title (ZH)**: GSMT：图融合与时空任务纠正多公交轨迹预测 

**Authors**: Fan Ding, Hwa Hui Tew, Junn Yong Loo, Susilawati, LiTong Liu, Fang Yu Leong, Xuewen Luo, Kar Keong Chin, Jia Jun Gan  

**Link**: [PDF](https://arxiv.org/pdf/2508.09227)  

**Abstract**: Accurate trajectory prediction for buses is crucial in intelligent transportation systems, particularly within urban environments. In developing regions where access to multimodal data is limited, relying solely on onboard GPS data remains indispensable despite inherent challenges. To address this problem, we propose GSMT, a hybrid model that integrates a Graph Attention Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and incorporates a task corrector capable of extracting complex behavioral patterns from large-scale trajectory data. The task corrector clusters historical trajectories to identify distinct motion patterns and fine-tunes the predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus information and static station information through embedded hybrid networks to perform trajectory prediction, and applies the task corrector for secondary refinement after the initial predictions are generated. This two-stage approach enables multi-node trajectory prediction among buses operating in dense urban traffic environments under complex conditions. Experiments conducted on a real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method significantly outperforms existing approaches, achieving superior performance in both short-term and long-term trajectory prediction tasks. 

**Abstract (ZH)**: 基于图注意网络和递归神经网络的混合模型在有限多模态数据下城市公交轨迹准确预测 

---
# Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation 

**Title (ZH)**: 基于任务向量的分层自适应网络在测试时的自适应 

**Authors**: Sameer Ambekar, Daniel M. Lang, Julia A. Schnabel  

**Link**: [PDF](https://arxiv.org/pdf/2508.09223)  

**Abstract**: Test-time adaptation allows pretrained models to adjust to incoming data streams, addressing distribution shifts between source and target domains. However, standard methods rely on single-dimensional linear classification layers, which often fail to handle diverse and complex shifts. We propose Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages multiple layers of increasing size for dynamic test-time adaptation. By decomposing the encoder's representation space into such hierarchically organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to adapt to shifts of varying complexity. Our contributions are threefold: First, we propose dynamic layer selection for automatic identification of the optimal layer for adaptation to each test batch. Second, we propose a mechanism that merges weights from the dynamic layer to other layers, ensuring all layers receive target information. Third, we propose linear layer agreement that acts as a gating function, preventing erroneous fine-tuning by adaptation on noisy batches. We rigorously evaluate the performance of Hi-Vec in challenging scenarios and on multiple target datasets, proving its strong capability to advance state-of-the-art methods. Our results show that Hi-Vec improves robustness, addresses uncertainty, and handles limited batch sizes and increased outlier rates. 

**Abstract (ZH)**: 层次自适应网络与任务向量（Hi-Vec）：动态测试时自适应方法 

---
# Understanding Ethical Practices in AI: Insights from a Cross-Role, Cross-Region Survey of AI Development Teams 

**Title (ZH)**: 理解人工智能中的伦理实践：来自跨角色、跨地区的人工智能开发团队调查的洞见 

**Authors**: Wilder Baldwin, Sepideh Ghanavati, Manuel Woersdoerfer  

**Link**: [PDF](https://arxiv.org/pdf/2508.09219)  

**Abstract**: Recent advances in AI applications have raised growing concerns about the need for ethical guidelines and regulations to mitigate the risks posed by these technologies. In this paper, we present a mixed-method survey study - combining statistical and qualitative analyses - to examine the ethical perceptions, practices, and knowledge of individuals involved in various AI development roles. Our survey includes 414 participants from 43 countries, representing roles such as AI managers, analysts, developers, quality assurance professionals, and information security and privacy experts. The results reveal varying degrees of familiarity and experience with AI ethics principles, government initiatives, and risk mitigation strategies across roles, regions, and other demographic factors. Our findings highlight the importance of a collaborative, role-sensitive approach, involving diverse stakeholders in ethical decision-making throughout the AI development lifecycle. We advocate for developing tailored, inclusive solutions to address ethical challenges in AI development, and we propose future research directions and educational strategies to promote ethics-aware AI practices. 

**Abstract (ZH)**: 近年来，AI应用的进展引发了对需要制定伦理指导和规范以减轻这些技术所带来的风险的广泛关注。本文通过结合统计和定性分析的方法，进行了一项混合方法调查研究，以考察不同AI开发角色中个体的伦理感知、实践和知识。调查涵盖了来自43个国家的414名参与者，其角色包括AI经理、分析师、开发者、质量保证专业人员及信息安全和隐私专家。研究结果表明，不同角色、地区和其他人口统计因素之间在AI伦理原则、政府倡议和风险缓解策略的熟悉度和经验上存在差异。研究发现强调了在整个AI开发生命周期中采用协作且角色敏感的方法，涉及多元利益相关者参与伦理决策的重要性。本文提倡为AI开发中的伦理挑战制定量身定制且包容性解决方案，并提出未来研究方向和教育策略，促进伦理意识的增强。 

---
# Deep Generative Models for Discrete Genotype Simulation 

**Title (ZH)**: 深度生成模型在离散基因型模拟中的应用 

**Authors**: Sihan Xie, Thierry Tribout, Didier Boichard, Blaise Hanczar, Julien Chiquet, Eric Barrey  

**Link**: [PDF](https://arxiv.org/pdf/2508.09212)  

**Abstract**: Deep generative models open new avenues for simulating realistic genomic data while preserving privacy and addressing data accessibility constraints. While previous studies have primarily focused on generating gene expression or haplotype data, this study explores generating genotype data in both unconditioned and phenotype-conditioned settings, which is inherently more challenging due to the discrete nature of genotype data. In this work, we developed and evaluated commonly used generative models, including Variational Autoencoders (VAEs), Diffusion Models, and Generative Adversarial Networks (GANs), and proposed adaptation tailored to discrete genotype data. We conducted extensive experiments on large-scale datasets, including all chromosomes from cow and multiple chromosomes from human. Model performance was assessed using a well-established set of metrics drawn from both deep learning and quantitative genetics literature. Our results show that these models can effectively capture genetic patterns and preserve genotype-phenotype association. Our findings provide a comprehensive comparison of these models and offer practical guidelines for future research in genotype simulation. We have made our code publicly available at this https URL. 

**Abstract (ZH)**: 深度生成模型拓宽了模拟现实基因组数据的同时保护隐私和解决数据访问性制约的途径，而先前的研究主要
user
请修改为：深度生成模型为模拟现实基因组数据、同时保护隐私和解决数据访问限制开拓了新途径，先前的研究主要关注生成基因表达和单倍型数据，而本研究探索了生成基因型数据的问题，在由于基因型数据的离散性质，这在不受条件和表型条件下的生成构成了更大的挑战。本研究开发并评估了多种生成模型，包括变分动编码器（扩散模型和生成对抗网络，并提出针对离散基因型数据的适应性。我们使用广泛的已建立的评估方法，包括来自深度学习和数量性遗传学文献的指标，我们的研究结果表明这些模型可以有效地捕捉遗传模式并并 保留基因型--型关联。我们的发现为基因型模拟未来研究提供了全面比较和实践指导。我们已将代码公开在该网址上。。请逐句翻译，禁止输出多余的空行或--型。 

---
# Quantum-Enhanced Generative Adversarial Networks: Comparative Analysis of Classical and Hybrid Quantum-Classical Generative Adversarial Networks 

**Title (ZH)**: 量子增强生成对抗网络：经典与量子经典混合生成对抗网络的比较分析 

**Authors**: Kun Ming Goh  

**Link**: [PDF](https://arxiv.org/pdf/2508.09209)  

**Abstract**: Generative adversarial networks (GANs) have emerged as a powerful paradigm for producing high-fidelity data samples, yet their performance is constrained by the quality of latent representations, typically sampled from classical noise distributions. This study investigates hybrid quantum-classical GANs (HQCGANs) in which a quantum generator, implemented via parameterised quantum circuits, produces latent vectors for a classical discriminator. We evaluate a classical GAN alongside three HQCGAN variants with 3, 5, and 7 qubits, using Qiskit's AerSimulator with realistic noise models to emulate near-term quantum devices. The binary MNIST dataset (digits 0 and 1) is used to align with the low-dimensional latent spaces imposed by current quantum hardware. Models are trained for 150 epochs and assessed with Frechet Inception Distance (FID) and Kernel Inception Distance (KID). Results show that while the classical GAN achieved the best scores, the 7-qubit HQCGAN produced competitive performance, narrowing the gap in later epochs, whereas the 3-qubit model exhibited earlier convergence limitations. Efficiency analysis indicates only moderate training time increases despite quantum sampling overhead. These findings validate the feasibility of noisy quantum circuits as latent priors in GAN architectures, highlighting their potential to enhance generative modelling within the constraints of the noisy intermediate-scale quantum (NISQ) era. 

**Abstract (ZH)**: 基于混合量子-经典生成对抗网络的生成模型研究：量子生成器在经典判别器中的应用 

---
# From Explainable to Explained AI: Ideas for Falsifying and Quantifying Explanations 

**Title (ZH)**: 从可解释到被解释的AI：验证和量化解释的思路 

**Authors**: Yoni Schirris, Eric Marcus, Jonas Teuwen, Hugo Horlings, Efstratios Gavves  

**Link**: [PDF](https://arxiv.org/pdf/2508.09205)  

**Abstract**: Explaining deep learning models is essential for clinical integration of medical image analysis systems. A good explanation highlights if a model depends on spurious features that undermines generalization and harms a subset of patients or, conversely, may present novel biological insights. Although techniques like GradCAM can identify influential features, they are measurement tools that do not themselves form an explanation. We propose a human-machine-VLM interaction system tailored to explaining classifiers in computational pathology, including multi-instance learning for whole-slide images. Our proof of concept comprises (1) an AI-integrated slide viewer to run sliding-window experiments to test claims of an explanation, and (2) quantification of an explanation's predictiveness using general-purpose vision-language models. The results demonstrate that this allows us to qualitatively test claims of explanations and can quantifiably distinguish competing explanations. This offers a practical path from explainable AI to explained AI in digital pathology and beyond. Code and prompts are available at this https URL. 

**Abstract (ZH)**: 深度学习模型的解释对于医学图像分析在临床中的集成是必要的。 

---
# MoQE: Improve Quantization Model performance via Mixture of Quantization Experts 

**Title (ZH)**: MoQE：通过混合量化专家提高量化模型性能 

**Authors**: Jinhao Zhang, Yunquan Zhang, Boyang Zhang, Zeyu Liu, Daning Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2508.09204)  

**Abstract**: Quantization method plays a crucial role in improving model efficiency and reducing deployment costs, enabling the widespread application of deep learning models on resource-constrained devices. However, the quantization process inevitably introduces accuracy degradation. In this paper, we propose Mixture of Quantization Experts( abbr. MoQE), a quantization inference framework based on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the performance of quantization models. MoQE combines multiple quantization variants of one full-precision model as specialized "quantization experts" and dynamically routes input data to the most suitable expert based on its characteristics. MoQE alleviates the performance degradation commonly seen in single quantization models through specialization quantization expert models. We design lightweight, structure-aware router models tailored for both CV and NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText demonstrate that MoQE achieves performance comparable to SOTA quantization model, without incurring significant increases in inference latency. 

**Abstract (ZH)**: 混合量化专家（MoQE）：基于Mixture-of-Experts架构的量化推理框架 

---
# Zero-shot self-supervised learning of single breath-hold magnetic resonance cholangiopancreatography (MRCP) reconstruction 

**Title (ZH)**: 零样本自监督学习在单次屏气磁共振胆胰管成像（MRCP）重建中的应用 

**Authors**: Jinho Kim, Marcel Dominik Nickel, Florian Knoll  

**Link**: [PDF](https://arxiv.org/pdf/2508.09200)  

**Abstract**: Purpose: To investigate the feasibility of applying zero-shot self-supervised learning reconstruction to reduce breath-hold times in magnetic resonance cholangiopancreatography (MRCP). Methods: Breath-hold MRCP was acquired from 11 healthy volunteers on a 3T scanner using an incoherent k-space sampling pattern leading to a breath-hold duration of 14s. We evaluated zero-shot reconstruction of breath-hold MRCP against parallel imaging of respiratory-triggered MRCP acquired in 338s on average and compressed sensing reconstruction of breath-hold MRCP. To address the long computation times of zero-shot trainings, we used a training approach that leverages a pretrained network to reduce backpropagation depth during training. Results: Zero-shot learning reconstruction significantly improved visual image quality compared to compressed sensing reconstruction, particularly in terms of signal-to-noise ratio and ductal delineation, and reached a level of quality comparable to that of successful respiratory-triggered acquisitions with regular breathing patterns. Shallow training provided nearly equivalent reconstruction performance with a training time of 11 minutes in comparison to 271 minutes for a conventional zero-shot training. Conclusion: Zero-shot learning delivers high-fidelity MRCP reconstructions with reduced breath-hold times, and shallow training offers a practical solution for translation to time-constrained clinical workflows. 

**Abstract (ZH)**: 目的：探究零样本自我 supervised 学习重建在减少磁共振胆胰管成像 (MRCP) 呼吸屏气时间中的可行性。方法：从11名健康志愿者中获取呼吸屏气 MRCP 图像，使用无相干 k-空间采样模式在3特斯拉扫描器上进行，呼吸屏气持续时间为14秒。我们将零样本重建的呼吸屏气 MRCP 与平均持续时长为338秒的呼吸触发 MRCP 平行成像及压缩感知重建的呼吸屏气 MRCP 进行评估。为解决零样本训练的长时间问题，我们采用了利用预训练网络的方法，在训练过程中减少反向传播的深度。结果：零样本学习重建在信噪比和胆管勾勒方面显著改善了图像质量，达到了与成功呼吸触发采集相似的水平。浅层训练在11分钟的训练时间内提供了几乎等同的重建性能，而传统零样本训练的训练时间为271分钟。结论：零样本学习可在减少呼吸屏气时间的同时提供高质量的 MRCP 重建，并且浅层训练为时间受限的临床工作流程带来了实用的解决方案。 

---
# $Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation 

**Title (ZH)**: $\Delta$-AttnMask: 基于注意力引导的掩蔽隐藏状态用于高效数据选择和增强 

**Authors**: Jucheng Hu, Suorong Yang, Dongzhan Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2508.09199)  

**Abstract**: Visual Instruction Finetuning (VIF) is pivotal for post-training Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in plain-text large language models, which mainly requires instruction datasets to enable model instruction-following ability, VIF also requires multimodal data to enable joint visual and textual understanding; therefore, it typically requires more data. Consequently, VIF imposes stricter data selection challenges: the method must scale efficiently to handle larger data demands while ensuring the quality of both visual and textual content, as well as their alignment. Despite its critical impact on performance, data selection for VIF remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This data-efficient framework quantifies sample quality through attention-guided masking of the model's hidden states, jointly evaluating image-text pairs without requiring domain labels, auxiliary models, or extra training. By computing loss differences ($\Delta$) between the original states and states masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses sample quality. Experiments across multiple VLMs and datasets show that $\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data, accelerating training by 5x while surpassing full-dataset baselines by +10.1% in overall accuracy. Its model-agnostic and data-agnostic design ensures broad applicability across modalities and architectures. 

**Abstract (ZH)**: 视觉指令微调（VIF）是后训练视觉语言模型（VLMs）的关键。视觉指令微调不仅需要单模态指令数据集来提升模型的指令跟随能力，还要求多模态数据以促进视觉和文本的联合理解。因此，它通常需要更多的数据。这导致视觉指令微调在数据选择上面临更严格的挑战：方法必须高效地扩展以应对更大的数据需求，同时确保视觉和文本内容的质量及其对齐。尽管数据选择对性能有重大影响，但视觉指令微调的数据选择仍然是一个研究不足的领域。在这项工作中，我们提出了$\Delta$-AttnMask。这是一种数据高效的框架，通过注意力引导的模型隐藏状态掩蔽来量化样本质量，可以在不使用领域标签、辅助模型或额外训练的情况下联合评估图像-文本对。通过计算原始状态和使用高注意力区域掩蔽的状态之间的损失差异（$\Delta$），$\Delta$-AttnMask 本质上评估样本质量。在多个视觉语言模型和数据集上的实验显示，$\Delta$-AttnMask 只需使用数据的 20% 就能实现最佳性能，加速训练 5 倍，并在总体准确性上超越全数据基线 +10.1%。其模型无关和数据无关的设计确保了其在不同模态和架构上的广泛应用。 

---
# ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce 

**Title (ZH)**: ADT4券：一种新颖的电子商务中序贯券分发框架 

**Authors**: Li Kong, Bingzhe Wang, Zhou Chen, Suhan Hu, Yuchao Ma, Qi Qi, Suoyuan Song, Bicheng Jin  

**Link**: [PDF](https://arxiv.org/pdf/2508.09198)  

**Abstract**: Coupon distribution is a critical marketing strategy used by online platforms to boost revenue and enhance user engagement. Regrettably, existing coupon distribution strategies fall far short of effectively leveraging the complex sequential interactions between platforms and users. This critical oversight, despite the abundance of e-commerce log data, has precipitated a performance plateau. In this paper, we focus on the scene that the platforms make sequential coupon distribution decision multiple times for various users, with each user interacting with the platform repeatedly. Based on this marketing scenario, we propose a novel marketing framework, named Aligned Decision Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution policy for long-term revenue boosting. ADT4Coupons enables optimized online decision-making in a variety of real-world marketing scenarios. It achieves this by seamlessly integrating three key characteristics, general scenarios, sequential modeling with more comprehensive historical data, and efficient iterative updates within a unified framework. Furthermore, empirical results on real-world industrial dataset, alongside public and synthetic datasets demonstrate the superiority of our framework. 

**Abstract (ZH)**: 在线平台的券分布是一种关键的营销策略，用于提升收入和增强用户参与度。遗憾的是，现有的券分布策略远未充分利用平台与用户之间复杂的序列交互。尽管存在大量的电子商务日志数据，这一关键遗漏导致了性能瓶颈。本文关注平台为不同用户多次进行序列化券分布决策的场景，且每位用户会反复与平台互动。基于这一营销场景，我们提出了一种新颖的营销框架，名为对齐决策变换器用于券（ADT4Coupons），以直接设计长期收入增强的券分布策略。ADT4Coupons能够在多种实际营销场景中实现优化的在线决策。它通过在一个统一框架中无缝整合三大关键特征——通用场景、包含更多历史数据的序列建模以及高效的迭代更新——来实现这一目标。此外，基于真实工业数据集以及公开和合成数据集的实证结果亦证明了该框架的优越性。 

---
# FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation 

**Title (ZH)**: FIVA：联邦逆方差平均在不确定性估计下的通用CT分割 

**Authors**: Asim Ukaye, Numan Saeed, Karthik Nandakumar  

**Link**: [PDF](https://arxiv.org/pdf/2508.09196)  

**Abstract**: Different CT segmentation datasets are typically obtained from different scanners under different capture settings and often provide segmentation labels for a limited and often disjoint set of organs. Using these heterogeneous data effectively while preserving patient privacy can be challenging. This work presents a novel federated learning approach to achieve universal segmentation across diverse abdominal CT datasets by utilizing model uncertainty for aggregation and predictive uncertainty for inference. Our approach leverages the inherent noise in stochastic mini-batch gradient descent to estimate a distribution over the model weights to provide an on-the-go uncertainty over the model parameters at the client level. The parameters are then aggregated at the server using the additional uncertainty information using a Bayesian-inspired inverse-variance aggregation scheme. Furthermore, the proposed method quantifies prediction uncertainty by propagating the uncertainty from the model weights, providing confidence measures essential for clinical decision-making. In line with recent work shown, predictive uncertainty is utilized in the inference stage to improve predictive performance. Experimental evaluations demonstrate the effectiveness of this approach in improving both the quality of federated aggregation and uncertainty-weighted inference compared to previously established baselines. The code for this work is made available at: this https URL 

**Abstract (ZH)**: 不同CT分割数据集通常来自不同的扫描器并在不同的采集设置下获得， often 提供有限且通常不连续的器官分割标签。利用这些异质性数据并同时保护患者隐私是具有挑战性的。本文提出了一种新颖的联邦学习方法，通过利用模型不确定性进行聚合和预测不确定性进行推理，实现跨多样腹部CT数据集的通用分割。该方法利用随机小批量梯度下降固有的噪声来估计模型权重的分布，在客户端层面提供一个实时的模型参数不确定性。然后，服务器利用附加的不确定性信息使用贝叶斯启发的方差加权聚合方案进行参数聚合。此外，本方法通过从模型权重传播不确定性来量化预测不确定性，提供对于临床决策至关重要的信心度量。在推理阶段利用预测不确定性进一步提高预测性能。实验评估表明，与先前建立的基准相比，该方法在提高联邦聚合质量和不确定性加权推理方面是有效的。该工作的代码可在以下链接获得：this https URL。 

---
# Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL 

**Title (ZH)**: 基于过程性内容生成的多目标指令意识表示学习 

**Authors**: Sung-Hyun Kim, In-Chang Baek, Seo-Young Lee, Geum-Hwan Hwang, Kyung-Joong Kim  

**Link**: [PDF](https://arxiv.org/pdf/2508.09193)  

**Abstract**: Recent advancements in generative modeling emphasize the importance of natural language as a highly expressive and accessible modality for controlling content generation. However, existing instructed reinforcement learning for procedural content generation (IPCGRL) method often struggle to leverage the expressive richness of textual input, especially under complex, multi-objective instructions, leading to limited controllability. To address this problem, we propose \textit{MIPCGRL}, a multi-objective representation learning method for instructed content generators, which incorporates sentence embeddings as conditions. MIPCGRL effectively trains a multi-objective embedding space by incorporating multi-label classification and multi-head regression networks. Experimental results show that the proposed method achieves up to a 13.8\% improvement in controllability with multi-objective instructions. The ability to process complex instructions enables more expressive and flexible content generation. 

**Abstract (ZH)**: 近期生成建模的发展强调了自然语言作为高度表达性和 accessible 模态在控制内容生成中的重要性。然而，现有的指令强化学习程序内容生成（IPCGRL）方法通常难以充分利用文本输入的表达丰富性，特别是在复杂、多目标指令下，导致控制能力有限。为了解决这一问题，我们提出了一种名为 MIPCGRL 的多目标表示学习方法，用于指令内容生成器，该方法结合了句子嵌入作为条件。MIPCGRL 通过集成多标签分类和多头回归网络有效地训练多目标嵌入空间。实验结果表明，所提出的方法在多目标指令下的控制能力最高可提高 13.8%。处理复杂指令的能力使内容生成更加表达性和灵活。 

---
# RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System 

**Title (ZH)**: 基于图像的智能交通系统中隐私保护方法：RL-MoE 

**Authors**: Abdolazim Rezaei, Mehdi Sookhak, Mahboobeh Haghparast  

**Link**: [PDF](https://arxiv.org/pdf/2508.09186)  

**Abstract**: The proliferation of AI-powered cameras in Intelligent Transportation Systems (ITS) creates a severe conflict between the need for rich visual data and the fundamental right to privacy. Existing privacy-preserving mechanisms, such as blurring or encryption, are often insufficient, creating an undesirable trade-off where either privacy is compromised against advanced reconstruction attacks or data utility is critically degraded. To resolve this impasse, we propose RL-MoE, a novel framework that transforms sensitive visual data into privacy-preserving textual descriptions, eliminating the need for direct image transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture for nuanced, multi-aspect scene decomposition with a Reinforcement Learning (RL) agent that optimizes the generated text for a dual objective of semantic accuracy and privacy preservation. Extensive experiments demonstrate that RL-MoE provides superior privacy protection, reducing the success rate of replay attacks to just 9.4\% on the CFP-FP dataset, while simultaneously generating richer textual content than baseline methods. Our work provides a practical and scalable solution for building trustworthy AI systems in privacy-sensitive domains, paving the way for more secure smart city and autonomous vehicle networks. 

**Abstract (ZH)**: AI驱动的摄像头在智能交通系统（ITS）中的普及引发了丰富的视觉数据需求与基本隐私权之间的严重冲突。现有的隐私保护机制，如模糊处理或加密，往往不足，导致了隐私泄露与高级重建攻击之间或数据有用性严重下降之间的不良权衡。为了缓解这一矛盾，我们提出了一种新颖的框架RL-MoE，该框架将敏感的视觉数据转换为隐私保护的文本描述，从而消除直接图像传输的需要。RL-MoE的独特之处在于它将混合专家（MoE）架构与强化学习（RL）代理相结合，前者用于精细的、多方面的场景分解，后者则优化生成的文本以实现语义准确性和隐私保护双重目标。广泛实验表明，RL-MoE提供了更好的隐私保护，在CFP-FP数据集上重放攻击成功率降低至9.4%，同时生成了比基线方法更丰富的文本内容。我们的工作为在隐私敏感领域构建可信赖的AI系统提供了实用且可扩展的解决方案，铺平了更加安全的智慧城市和自动驾驶车辆网络的道路。 

---
# A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality 

**Title (ZH)**: 面向增强现实的可解释认知攻击检测的神经符号框架 

**Authors**: Rongqian Chen, Allison Andreyev, Yanming Xiu, Mahdi Imani, Bin Li, Maria Gorlatova, Gang Tan, Tian Lan  

**Link**: [PDF](https://arxiv.org/pdf/2508.09185)  

**Abstract**: Augmented Reality (AR) enriches perception by overlaying virtual elements on the physical world. Due to its growing popularity, cognitive attacks that alter AR content to manipulate users' semantic perception have received increasing attention. Existing detection methods often focus on visual changes, which are restricted to pixel- or image-level processing and lack semantic reasoning capabilities, or they rely on pre-trained vision-language models (VLMs), which function as black-box approaches with limited interpretability. In this paper, we present CADAR, a novel neurosymbolic approach for cognitive attack detection in AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a symbolic perception-graph representation, incorporating prior knowledge, salience weighting, and temporal correlations. The model then enables particle-filter based statistical reasoning -- a sequential Monte Carlo method -- to detect cognitive attacks. Thus, CADAR inherits the adaptability of pre-trained VLM and the interpretability and reasoning rigor of particle filtering. Experiments on an extended AR cognitive attack dataset show accuracy improvements of up to 10.7% over strong baselines on challenging AR attack scenarios, underscoring the promise of neurosymbolic methods for effective and interpretable cognitive attack detection. 

**Abstract (ZH)**: AR中的认知攻击检测的CADAR神经符号方法 

---
# HiSTM: Hierarchical Spatiotemporal Mamba for Cellular Traffic Forecasting 

**Title (ZH)**: HiSTM: 分层时空猎蛛细胞流量预测 

**Authors**: Zineddine Bettouche, Khalid Ali, Andreas Fischer, Andreas Kassler  

**Link**: [PDF](https://arxiv.org/pdf/2508.09184)  

**Abstract**: Cellular traffic forecasting is essential for network planning, resource allocation, or load-balancing traffic across cells. However, accurate forecasting is difficult due to intricate spatial and temporal patterns that exist due to the mobility of users. Existing AI-based traffic forecasting models often trade-off accuracy and computational efficiency. We present Hierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial encoder with a Mamba-based temporal module and attention mechanism. HiSTM employs selective state space methods to capture spatial and temporal patterns in network traffic. In our evaluation, we use a real-world dataset to compare HiSTM against several baselines, showing a 29.4% MAE improvement over the STN baseline while using 94% fewer parameters. We show that the HiSTM generalizes well across different datasets and improves in accuracy over longer time-horizons. 

**Abstract (ZH)**: 基于层次时空机制的蜂窝网络流量预测（Hierarchical SpatioTemporal Mamba (HiSTM)：结合时空编码与注意力机制的蜂窝网络流量预测） 

---
# Quantum-Efficient Reinforcement Learning Solutions for Last-Mile On-Demand Delivery 

**Title (ZH)**: 量子高效强化学习解决方案应用于最后一英里按需配送 

**Authors**: Farzan Moosavi, Bilal Farooq  

**Link**: [PDF](https://arxiv.org/pdf/2508.09183)  

**Abstract**: Quantum computation has demonstrated a promising alternative to solving the NP-hard combinatorial problems. Specifically, when it comes to optimization, classical approaches become intractable to account for large-scale solutions. Specifically, we investigate quantum computing to solve the large-scale Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW). In this regard, a Reinforcement Learning (RL) framework augmented with a Parametrized Quantum Circuit (PQC) is designed to minimize the travel time in a realistic last-mile on-demand delivery. A novel problem-specific encoding quantum circuit with an entangling and variational layer is proposed. Moreover, Proximal Policy Optimization (PPO) and Quantum Singular Value Transformation (QSVT) are designed for comparison through numerical experiments, highlighting the superiority of the proposed method in terms of the scale of the solution and training complexity while incorporating the real-world constraints. 

**Abstract (ZH)**: 量子计算提供了解决NP难组合问题的有希望的替代方案。特别是在优化领域，经典方法对于处理大规模解决方案变得不可行。因此，我们研究了量子计算解决带时间窗的容量受限拾取和配送问题（CPDPTW）的大规模实例。为此，我们设计了一种增强学习（RL）框架，结合参数量子电路（PQC），以最小化现实中的最后一英里按需配送的行驶时间。我们提出了一种新的问题特定编码量子电路，包括纠缠层和变分层。此外，我们通过数值实验设计了接近策略优化（PPO）和量子奇异值变换（QSVT），突出了所提出方法在满足现实世界约束条件下解决方案规模和训练复杂性方面的优越性。 

---
# Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach 

**Title (ZH)**: 非独立同分布数据下联邦学习的长期客户端选择：一种诚实拍卖方法 

**Authors**: Jinghong Tan, Zhian Liu, Kun Guo, Mingxiong Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2508.09181)  

**Abstract**: Federated learning (FL) provides a decentralized framework that enables universal model training through collaborative efforts on mobile nodes, such as smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a mobile client, contributing to the process without uploading local data. This method leverages non-independent and identically distributed (non-IID) training data from different vehicles, influenced by various driving patterns and environmental conditions, which can significantly impact model convergence and accuracy. Although client selection can be a feasible solution for non-IID issues, it faces challenges related to selection metrics. Traditional metrics evaluate client data quality independently per round and require client selection after all clients complete local training, leading to resource wastage from unused training results. In the IoV context, where vehicles have limited connectivity and computational resources, information asymmetry in client selection risks clients submitting false information, potentially making the selection ineffective. To tackle these challenges, we propose a novel Long-term Client-Selection Federated Learning based on Truthful Auction (LCSFLA). This scheme maximizes social welfare with consideration of long-term data quality using a new assessment mechanism and energy costs, and the advised auction mechanism with a deposit requirement incentivizes client participation and ensures information truthfulness. We theoretically prove the incentive compatibility and individual rationality of the advised incentive mechanism. Experimental results on various datasets, including those from IoV scenarios, demonstrate its effectiveness in mitigating performance degradation caused by non-IID data. 

**Abstract (ZH)**: 基于 Truthful Auction 的长期客户端选择联邦学习（LCSFLA） 

---
# scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering 

**Title (ZH)**: scAGC: 学习具有对比引导的自适应细胞图谱-single细胞聚类 

**Authors**: Huifa Li, Jie Fu, Xinlin Zhuang, Haolin Yang, Xinpeng Ling, Tong Cheng, Haochen xue, Imran Razzak, Zhili Chen  

**Link**: [PDF](https://arxiv.org/pdf/2508.09180)  

**Abstract**: Accurate cell type annotation is a crucial step in analyzing single-cell RNA sequencing (scRNA-seq) data, which provides valuable insights into cellular heterogeneity. However, due to the high dimensionality and prevalence of zero elements in scRNA-seq data, traditional clustering methods face significant statistical and computational challenges. While some advanced methods use graph neural networks to model cell-cell relationships, they often depend on static graph structures that are sensitive to noise and fail to capture the long-tailed distribution inherent in single-cell this http URL address these limitations, we propose scAGC, a single-cell clustering method that learns adaptive cell graphs with contrastive guidance. Our approach optimizes feature representations and cell graphs simultaneously in an end-to-end manner. Specifically, we introduce a topology-adaptive graph autoencoder that leverages a differentiable Gumbel-Softmax sampling strategy to dynamically refine the graph structure during training. This adaptive mechanism mitigates the problem of a long-tailed degree distribution by promoting a more balanced neighborhood structure. To model the discrete, over-dispersed, and zero-inflated nature of scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for robust feature reconstruction. Furthermore, a contrastive learning objective is incorporated to regularize the graph learning process and prevent abrupt changes in the graph topology, ensuring stability and enhancing convergence. Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC consistently outperforms other state-of-the-art methods, yielding the best NMI and ARI scores on 9 and 7 datasets, this http URL code is available at Anonymous Github. 

**Abstract (ZH)**: 准确的细胞类型标注是分析单细胞RNA测序（scRNA-seq）数据的关键步骤，它为细胞异质性提供了宝贵见解。然而，由于scRNA-seq数据的高维度和零元素的普遍性，传统聚类方法面临显著的统计和计算挑战。尽管一些先进方法使用图神经网络来建模细胞间的相互关系，但它们往往依赖于敏感于噪声的静态图结构，并未能捕捉到单细胞数据固有的长尾分布。为了解决这些局限性，我们提出了scAGC，一种学习自适应细胞图的单细胞聚类方法，该方法在对比引导下优化特征表示和细胞图。我们的方法以端到端的方式同时优化特征表示和细胞图。具体而言，我们引入了一种拓扑自适应图自动编码器，利用可微的Gumbel-Softmax采样策略，在训练过程中动态优化图结构。这种自适应机制通过促进更平衡的邻域结构解决了长尾度分布的问题。为了建模scRNA-seq数据的离散、过度分散和零充溢特性，我们整合了零充溢负二项式（ZINB）损失以实现稳健的特征重构。此外，我们结合了对比学习目标来规整图学习过程，防止图拓扑的突然变化，从而确保稳定性和加快收敛。在9个真实scRNA-seq数据集上的综合实验中，scAGC始终优于其他最先进的方法，分别在9个和7个数据集上获得最佳的NMI和ARI分数。该项目代码可在Anononyous Github上获取。 

---
# DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic 

**Title (ZH)**: 动态量化训练通过去量化嵌套整数算术实现（DQT） 

**Authors**: Hazem Hesham Yousef Shalby, Fabrizio Pittorino, Francesca Palermo, Diana Trojaniello, Manuel Roveri  

**Link**: [PDF](https://arxiv.org/pdf/2508.09176)  

**Abstract**: The deployment of deep neural networks on resource-constrained devices relies on quantization. While static, uniform quantization applies a fixed bit-width to all inputs, it fails to adapt to their varying complexity. Dynamic, instance-based mixed-precision quantization promises a superior accuracy-efficiency trade-off by allocating higher precision only when needed. However, a critical bottleneck remains: existing methods require a costly dequantize-to-float and requantize-to-integer cycle to change precision, breaking the integer-only hardware paradigm and compromising performance gains. This paper introduces Dynamic Quantization Training (DQT), a novel framework that removes this bottleneck. At the core of DQT is a nested integer representation where lower-precision values are bit-wise embedded within higher-precision ones. This design, coupled with custom integer-only arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost bit-shift operation. This makes DQT the first quantization framework to enable both dequantization-free static mixed-precision of the backbone network, and truly efficient dynamic, instance-based quantization through a lightweight controller that decides at runtime how to quantize each layer. We demonstrate DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1 accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET, 76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this with a bit-width transition cost of only 28.3M simple bit-shift operations, a drastic improvement over the 56.6M costly Multiply-Accumulate (MAC) floating-point operations required by previous dynamic approaches - unlocking a new frontier in efficient, adaptive AI. 

**Abstract (ZH)**: 基于动态量化的深度神经网络在资源受限设备上的部署依赖于量化的应用。虽然静态均匀量化对所有输入应用固定的位宽，但它无法适应输入的不同复杂度。基于实例的动态混合精度量化通过仅在需要时分配更高精度，承诺了更好的准确率-效率 trade-off。然而，一个关键瓶颈仍然存在：现有方法需要一个昂贵的去量化的浮点数和重新量化的整数循环来改变精度，打破了纯整数硬件的范式并损害了性能的提升。本文引入了动态量化训练（DQT）这一新颖框架，消除了这一瓶颈。DQT的核心在于嵌套的整数表示，其中较低精度的值以位级嵌入到较高精度值中。这种设计结合自定义的纯整数算术，通过近乎零成本的位移操作实现了即席的位宽切换。这使DQT成为第一个既能实现不需要去量化的静态混合精度主干网络，又能通过轻量级控制器在运行时决定如何量化每一层，从而实现真正高效的动态、基于实例的量化的量化框架。我们在CIFAR-10上使用ResNet18和ImageNet上使用ResNet50展示了DQT的最新性能。在ImageNet上，我们的4位动态ResNet50达到了77.00%的top-1准确率，相比可比的位操作预算下的领先静态方法（LSQ，76.70%）和动态方法（DQNET，76.94%），均有所提升。最关键的是，DQT仅需28.3百万简单的位移操作的位宽转换成本，相较于之前动态方法所需的56.6百万昂贵的乘累加（MAC）浮点操作，这是一个巨大的改进，开启了高效、自适应AI的新前沿。 

---
# FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective 

**Title (ZH)**: FedMP: 从流形视角解决联邦学习中医疗特征异质性问题 

**Authors**: Zhekai Zhou, Shudong Liu, Zhaokun Zhou, Yang Liu, Qiang Yang, Yuesheng Zhu, Guibo Luo  

**Link**: [PDF](https://arxiv.org/pdf/2508.09174)  

**Abstract**: Federated learning (FL) is a decentralized machine learning paradigm in which multiple clients collaboratively train a shared model without sharing their local private data. However, real-world applications of FL frequently encounter challenges arising from the non-identically and independently distributed (non-IID) local datasets across participating clients, which is particularly pronounced in the field of medical imaging, where shifts in image feature distributions significantly hinder the global model's convergence and performance. To address this challenge, we propose FedMP, a novel method designed to enhance FL under non-IID scenarios. FedMP employs stochastic feature manifold completion to enrich the training space of individual client classifiers, and leverages class-prototypes to guide the alignment of feature manifolds across clients within semantically consistent subspaces, facilitating the construction of more distinct decision boundaries. We validate the effectiveness of FedMP on multiple medical imaging datasets, including those with real-world multi-center distributions, as well as on a multi-domain natural image dataset. The experimental results demonstrate that FedMP outperforms existing FL algorithms. Additionally, we analyze the impact of manifold dimensionality, communication efficiency, and privacy implications of feature exposure in our method. 

**Abstract (ZH)**: 联邦学习（FL）是多个客户端在不共享本地私有数据的情况下协作训练共享模型的一种去中心化机器学习范式。然而，实际应用中的FL经常遇到由参与者客户端之间非同分布（non-IID）本地数据集引起的问题，特别是在医学成像领域，图像特征分布的变化严重影响全局模型的收敛性和性能。为应对这一挑战，我们提出了FedMP，一种在非-IID场景下增强联邦学习的新方法。FedMP利用随机特征流形完成来丰富个体客户端分类器的训练空间，并利用类原型引导客户端间特征流形在语义一致子空间内的对齐，促进决策边界的构建。我们在多个医学成像数据集上验证了FedMP的有效性，包括包含真实多中心分布的数据集，以及多域自然图像数据集。实验结果表明，FedMP在多个数据集上性能优于现有联邦学习算法。此外，我们分析了流形维度、通信效率和特征暴露对隐私的影响。 

---
# webMCP: Efficient AI-Native Client-Side Interaction for Agent-Ready Web Design 

**Title (ZH)**: WebMCP：高效的AI原生客户端交互以支持智能代理就绪的网页设计 

**Authors**: D. Perera  

**Link**: [PDF](https://arxiv.org/pdf/2508.09171)  

**Abstract**: Current AI agents create significant barriers for users by requiring extensive processing to understand web pages, making AI-assisted web interaction slow and expensive. This paper introduces webMCP (Web Machine Context & Procedure), a client-side standard that embeds structured interaction metadata directly into web pages, enabling more efficient human-AI collaboration on existing websites. webMCP transforms how AI agents understand web interfaces by providing explicit mappings between page elements and user actions. Instead of processing entire HTML documents, agents can access pre-structured interaction data, dramatically reducing computational overhead while maintaining task accuracy. A comprehensive evaluation across 1,890 real API calls spanning online shopping, authentication, and content management scenarios demonstrates webMCP reduces processing requirements by 67.6% while maintaining 97.9% task success rates compared to 98.8% for traditional approaches. Users experience significantly lower costs (34-63% reduction) and faster response times across diverse web interactions. Statistical analysis confirms these improvements are highly significant across multiple AI models. An independent WordPress deployment study validates practical applicability, showing consistent improvements across real-world content management workflows. webMCP requires no server-side modifications, making it deployable across millions of existing websites without technical barriers. These results establish webMCP as a viable solution for making AI web assistance more accessible and sustainable, addressing the critical gap between user interaction needs and AI computational requirements in production environments. 

**Abstract (ZH)**: 当前的AI代理通过要求大量处理以理解网页，为用户创建了显著的障碍，使得AI辅助的网页交互变得缓慢且昂贵。本文介绍了一种名为webMCP（Web Machine Context & Procedure）的客户端标准，该标准直接将结构化的交互元数据嵌入到网页中，从而在现有网站上促进更有效的真人-AI协作。webMCP通过提供页面元素与用户操作之间的显式映射，改变了AI代理理解网页界面的方式。相比处理整个HTML文档，代理可以访问预先结构化的交互数据，大大减少了计算开销同时保持任务准确性。在涵盖在线购物、身份验证和内容管理等各种场景的1,890个实际API调用的全面评估中，webMCP将处理需求减少了67.6%，同时保持了97.9%的任务成功率，而传统方法为98.8%。用户在不同类型的网页交互中体验到显著的成本降低（34-63%）和更快的响应时间。统计分析证实这些改进在多种AI模型中具有高度显著性。独立的WordPress部署研究验证了其实用性，展示了在实际内容管理工作流中的一致改进。webMCP无需服务器端修改，使其能够在数百万现有网站上部署而无需技术障碍。这些结果确立了webMCP作为让AI网络辅助更具可持续性和普及性的可行解决方案的地位，弥补了用户交互需求与生产环境中的AI计算要求之间的关键差距。 

---
# Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL) 

**Title (ZH)**: 面向物联网设备的分层可调序列长度的节能随机计算神经网络 

**Authors**: Ziheng Wang, Pedro Reviriego, Farzad Niknia, Zhen Gao, Javier Conde, Shanshan Liu, Fabrizio Lombardi  

**Link**: [PDF](https://arxiv.org/pdf/2508.09163)  

**Abstract**: Stochastic computing (SC) has emerged as an efficient low-power alternative for deploying neural networks (NNs) in resource-limited scenarios, such as the Internet of Things (IoT). By encoding values as serial bitstreams, SC significantly reduces energy dissipation compared to conventional floating-point (FP) designs; however, further improvement of layer-wise mixed-precision implementation for SC remains unexplored. This article introduces Adjustable Sequence Length (ASL), a novel scheme that applies mixed-precision concepts specifically to SC NNs. By introducing an operator-norm-based theoretical model, this article shows that truncation noise can cumulatively propagate through the layers by the estimated amplification factors. An extended sensitivity analysis is presented, using random forest (RF) regression to evaluate multilayer truncation effects and validate the alignment of theoretical predictions with practical network behaviors. To accommodate different application scenarios, this article proposes two truncation strategies (coarse-grained and fine-grained), which apply diverse sequence length configurations at each layer. Evaluations on a pipelined SC MLP synthesized at 32nm demonstrate that ASL can reduce energy and latency overheads by up to over 60% with negligible accuracy loss. It confirms the feasibility of the ASL scheme for IoT applications and highlights the distinct advantages of mixed-precision truncation in SC designs. 

**Abstract (ZH)**: 基于 stochastic 计算 (SC) 的神经网络 (NN) 在物联网 (IoT) 等资源受限场景中的高效低功耗替代方案：Adjustable Sequence Length (ASL) 的探索 

---
# Physics-Guided Memory Network for Building Energy Modeling 

**Title (ZH)**: 基于物理引导的记忆网络 Buildings能耗建模 

**Authors**: Muhammad Umair Danish, Kashif Ali, Kamran Siddiqui, Katarina Grolinger  

**Link**: [PDF](https://arxiv.org/pdf/2508.09161)  

**Abstract**: Accurate energy consumption forecasting is essential for efficient resource management and sustainability in the building sector. Deep learning models are highly successful but struggle with limited historical data and become unusable when historical data are unavailable, such as in newly constructed buildings. On the other hand, physics-based models, such as EnergyPlus, simulate energy consumption without relying on historical data but require extensive building parameter specifications and considerable time to model a building. This paper introduces a Physics-Guided Memory Network (PgMN), a neural network that integrates predictions from deep learning and physics-based models to address their limitations. PgMN comprises a Parallel Projection Layers to process incomplete inputs, a Memory Unit to account for persistent biases, and a Memory Experience Module to optimally extend forecasts beyond their input range and produce output. Theoretical evaluation shows that components of PgMN are mathematically valid for performing their respective tasks. The PgMN was evaluated on short-term energy forecasting at an hourly resolution, critical for operational decision-making in smart grid and smart building systems. Experimental validation shows accuracy and applicability of PgMN in diverse scenarios such as newly constructed buildings, missing data, sparse historical data, and dynamic infrastructure changes. This paper provides a promising solution for energy consumption forecasting in dynamic building environments, enhancing model applicability in scenarios where historical data are limited or unavailable or when physics-based models are inadequate. 

**Abstract (ZH)**: 基于物理约束的内存网络在建筑动态环境中的能源消耗预测 

---
# Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders 

**Title (ZH)**: 同时反馈和未观测混杂因素下的同伴效应估计 

**Authors**: Xiaojing Du, Jiuyong Li, Lin Liu, Debo Cheng, Thuc.Le  

**Link**: [PDF](https://arxiv.org/pdf/2508.09154)  

**Abstract**: Estimating peer causal effects within complex real-world networks such as social networks is challenging, primarily due to simultaneous feedback between peers and unobserved confounders. Existing methods either address unobserved confounders while ignoring the simultaneous feedback, or account for feedback but under restrictive linear assumptions, thus failing to obtain accurate peer effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning framework which leverages I-G transformation (matrix operation) and 2SRI (an instrumental variable or IV technique) to address both simultaneous feedback and unobserved confounding, while accommodating complex, nonlinear and high-dimensional relationships. DIG2RSI first applies the I-G transformation to disentangle mutual peer influences and eliminate the bias due to the simultaneous feedback. To deal with unobserved confounding, we first construct valid IVs from network data. In stage 1 of 2RSI, we train a neural network on these IVs to predict peer exposure, and extract residuals as proxies for the unobserved confounders. In the stage 2, we fit a separate neural network augmented by an adversarial discriminator that incorporates these residuals as a control function and enforces the learned representation to contain no residual confounding signal. The expressive power of deep learning models in capturing complex non-linear relationships and adversarial debiasing enhances the effectiveness of DIG2RSI in eliminating bias from both feedback loops and hidden confounders. We prove consistency of our estimator under standard regularity conditions, ensuring asymptotic recovery of the true peer effect. Empirical results on two semi-synthetic benchmarks and a real-world dataset demonstrate that DIG2RSI outperforms existing approaches. 

**Abstract (ZH)**: 利用I-G变换和双重工具变量的深度学习框架DIG2RSI：同时解决同时反馈和未观测混杂因素以估计同伴效应 

---
# JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis 

**Title (ZH)**: JustDense: 仅使用密集连接代替序列混合进行时间序列分析 

**Authors**: TaekHyun Park, Yongjae Lee, Daesan Park, Dohee Kim, Hyerim Bae  

**Link**: [PDF](https://arxiv.org/pdf/2508.09153)  

**Abstract**: Sequence and channel mixers, the core mechanism in sequence models, have become the de facto standard in time series analysis (TSA). However, recent studies have questioned the necessity of complex sequence mixers, such as attention mechanisms, demonstrating that simpler architectures can achieve comparable or even superior performance. This suggests that the benefits attributed to complex sequencemixers might instead emerge from other architectural or optimization factors. Based on this observation, we pose a central question: Are common sequence mixers necessary for time-series analysis? Therefore, we propose JustDense, an empirical study that systematically replaces sequence mixers in various well-established TSA models with dense layers. Grounded in the MatrixMixer framework, JustDense treats any sequence mixer as a mixing matrix and replaces it with a dense layer. This substitution isolates the mixing operation, enabling a clear theoretical foundation for understanding its role. Therefore, we conducted extensive experiments on 29 benchmarks covering five representative TSA tasks using seven state-of-the-art TSA models to address our research question. The results show that replacing sequence mixers with dense layers yields comparable or even superior performance. In the cases where dedicated sequence mixers still offer benefits, JustDense challenges the assumption that "deeper and more complex architectures are inherently better" in TSA. 

**Abstract (ZH)**: 序列和和和 ChannelD D Mixers:: The D Mechanism in D D Sequential ModelsD: D TheD FactDH StandardD In in in In in in in in in inD TimeD-D SeriesD AnalysisD (TSASDA)D. HasDD BecomeD TheD De D DeD DefD DeD DeDD De DeDD DeD DeDDDDD DeDDDDDDDDDDDDDDDDDDDDDDDDD 最近近 近D 的D � 研究D 对 � 疑D 况D 了D � 复杂 杂 杂 处D 序D 序D � 混 � �激起D �D � 机DD � 例如 的D 必DD � � �layui 重要D 性D，， �DD。D �D。D 重要的DD 的DD 度D。D， 表DD 证DD � � 更D � 为DD 更D 简D 单DD D D 的D 架DD 当D �构DD 凩 在 叫DDD 得 优D �DD � 敯 佳D 的D 性T 性 性 昏 �志愿服务D 的DD 在D 性。 �迟D。D 性DD � � 昺D 。D 这 证DD  衎 际DD � 证DD  是 昷 证实DD D  附D D �D 了D 这D  复D 严DD 利DD 的DD  附DD  机D 度D � 本D 束 � 杜DD � �D 杵D 发DD  自D  的D 主D 杬归 凘 松D 李术D  杜D D �D  的D 影DDD 见D 。D  � � �况DD 迈D  证D  昧 证明DD D  �emicDD 结DD 的DD  隣D 庐DD �D � 机D 术测DD  为D 练D �D 得 �D 代字DD 表D �D D 的D �D � 活动D �D 有 进D 衎 了D  一D 个D 关DD  于D 新D � �D 关D �D � 问 问 问 � 问 � � 问 信DDDD 昑DDD 中D 问 �D 证据D 的DD  证DD，DD 春D 的DD 严D  条D 这D �痘痘D 杰D。  的D  右�D � � 证DD �DDD  为DD 了D 一种DD  新D 的DD 关D D 亊D  DD 亊D 度D 方D 法D 来DD  寜测量D 讘 D 代DD �D 代D D 衴DDD � 的DD  附 只 说D � 安D 当前的 的DD 亊DD 亊D 亊D 的DD  亊D 选择了D  亊D  恁 更D � 优化简化D 的DD  活动D 的D �DD 视角D 的DD  怀D 些DD 的DD  附 亢 �D 亟 � 亊D 亊D 二 必D � 的D D 亣 亟 � � 杶 亊D D 亊D 亁 争 注意D 注D 重视D 注意DD  浓D 浓D DD 浓D 重视D 的DD  争 D 亃D �D �  �性 杵D � 开 是D  亊D 亅D 亍D  �D �D 亠丿 亊D  亢 �D 亐D  亣 D  亐D 亙D D  亱 D  亐DD  亵D 亐DD  D  亐D  亊D D  亡D  亃亀 云D  五D  亖争 云DD  亢D  亦D D  亟D  亐D D。 

---
# 5G Core Fault Detection and Root Cause Analysis using Machine Learning and Generative AI 

**Title (ZH)**: 基于机器学习和生成式AI的5G核心网故障检测与根因分析 

**Authors**: Joseph H. R. Isaac, Harish Saradagam, Nallamothu Pardhasaradhi  

**Link**: [PDF](https://arxiv.org/pdf/2508.09152)  

**Abstract**: With the advent of 5G networks and technologies, ensuring the integrity and performance of packet core traffic is paramount. During network analysis, test files such as Packet Capture (PCAP) files and log files will contain errors if present in the system that must be resolved for better overall network performance, such as connectivity strength and handover quality. Current methods require numerous person-hours to sort out testing results and find the faults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine designed to classify successful and faulty frames in PCAP files, specifically within the 5G packet core. The FA engine analyses network traffic using natural language processing techniques to identify anomalies and inefficiencies, significantly reducing the effort time required and increasing efficiency. The FA Engine also suggests steps to fix the issue using Generative AI via a Large Language Model (LLM) trained on several 5G packet core documents. The engine explains the details of the error from the domain perspective using documents such as the 3GPP standards and user documents regarding the internal conditions of the tests. Test results on the ML models show high classification accuracy on the test dataset when trained with 80-20 splits for the successful and failed PCAP files. Future scopes include extending the AI engine to incorporate 4G network traffic and other forms of network data, such as log text files and multimodal systems. 

**Abstract (ZH)**: 随着5G网络技术的出现，确保包核心流量的完整性和性能至关重要。在网络分析过程中，如果测试文件如Packet Capture (PCAP)文件和日志文件中存在错误，必须解决这些问题以提高整体网络性能，例如连接强度和切换质量。当前方法需要投入大量人工小时来整理测试结果并找出故障。本文提出了一种新颖的AI/ML驱动故障分析（FA）引擎，用于在5G包核心中分类成功的和故障的帧。FA引擎利用自然语言处理技术分析网络流量，以识别异常和低效性，显著减少了所需的工作时间和提高了效率。该FA引擎还通过大型语言模型（LLM）生成AI，基于多个5G包核心文档，提供修复问题的步骤建议。引擎从3GPP标准和用户文档中解释测试内部条件的错误详情。在机器学习模型上的测试结果显示，在80-20分割训练集时，对成功和失败的PCAP文件具有高分类精度。未来的工作包括将AI引擎扩展以纳入4G网络流量和其他形式的网络数据，如日志文本文件和多模态系统。 

---
# Agentic TinyML for Intent-aware Handover in 6G Wireless Networks 

**Title (ZH)**: 代理型TinyML在6G无线网络中意图感知的手柄转移 

**Authors**: Alaa Saleh, Roberto Morabito, Sasu Tarkoma, Anders Lindgren, Susanna Pirttikangas, Lauri Lovén  

**Link**: [PDF](https://arxiv.org/pdf/2508.09147)  

**Abstract**: As 6G networks evolve into increasingly AI-driven, user-centric ecosystems, traditional reactive handover mechanisms demonstrate limitations, especially in mobile edge computing and autonomous agent-based service scenarios. This manuscript introduces WAAN, a cross-layer framework that enables intent-aware and proactive handovers by embedding lightweight TinyML agents as autonomous, negotiation-capable entities across heterogeneous edge nodes that contribute to intent propagation and network adaptation. To ensure continuity across mobility-induced disruptions, WAAN incorporates semi-stable rendezvous points that serve as coordination anchors for context transfer and state preservation. The framework's operational capabilities are demonstrated through a multimodal environmental control case study, highlighting its effectiveness in maintaining user experience under mobility. Finally, the article discusses key challenges and future opportunities associated with the deployment and evolution of WAAN. 

**Abstract (ZH)**: 随着6G网络向日益依赖AI、以用户为中心的生态系统发展，传统的反应式切换机制在移动边缘计算和基于自主代理的服务场景中显示出局限性。本文提出了WAAN，一种跨层框架，通过在网络异构边缘节点中嵌入轻量级TinyML代理作为自主且具备谈判能力的实体，实现意图感知和主动切换。为了确保在移动性中断时的连续性，WAAN整合了半稳定的中继点，作为上下文转移和状态保存的协调锚点。通过多模态环境控制案例研究展示了该框架的操作能力，突显了其在移动性影响下的用户体验保持效果。最后，本文讨论了WAAN部署和演化过程中的关键挑战和未来机遇。 

---
# Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer 

**Title (ZH)**: 基于特征标记变换器的高效实时航空器到港时间预测 

**Authors**: Liping Huang, Yicheng Zhang, Yifang Yin, Sheng Zhang, Yi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2508.09144)  

**Abstract**: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial for arrival management in aviation, particularly for runway sequencing. Given the rapidly changing airspace context, the ETA prediction efficiency is as important as its accuracy in a real-time arrival aircraft management system. In this study, we utilize a feature tokenization-based Transformer model to efficiently predict aircraft ETA. Feature tokenization projects raw inputs to latent spaces, while the multi-head self-attention mechanism in the Transformer captures important aspects of the projections, alleviating the need for complex feature engineering. Moreover, the Transformer's parallel computation capability allows it to handle ETA requests at a high frequency, i.e., 1HZ, which is essential for a real-time arrival management system. The model inputs include raw data, such as aircraft latitude, longitude, ground speed, theta degree for the airport, day and hour from track data, the weather context, and aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA prediction is updated every second. We apply the proposed aircraft ETA prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October 1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers all aircraft within a range of 10NM to 300NM from WSSS. The results show that our proposed method method outperforms the commonly used boosting tree based model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\% of its computing time. Experimental results also indicate that, with 40 aircraft in the airspace at a given timestamp, the ETA inference time is only 51.7 microseconds, making it promising for real-time arrival management systems. 

**Abstract (ZH)**: 基于特征 tokenization 的 Transformer 模型在实时航空器到达时间预测中的应用：以新加坡樟宜机场为例 

---
# Bayesian-Driven Graph Reasoning for Active Radio Map Construction 

**Title (ZH)**: 基于贝叶斯驱动的图推理的主动无线电图构建 

**Authors**: Wenlihan Lu, Shijian Gao, Miaowen Wen, Yuxuan Liang, Chan-Byoung Chae, H. Vincent Poor  

**Link**: [PDF](https://arxiv.org/pdf/2508.09142)  

**Abstract**: With the emergence of the low-altitude economy, radio maps have become essential for ensuring reliable wireless connectivity to aerial platforms. Autonomous aerial agents are commonly deployed for data collection using waypoint-based navigation; however, their limited battery capacity significantly constrains coverage and efficiency. To address this, we propose an uncertainty-aware radio map (URAM) reconstruction framework that explicitly leverages graph-based reasoning tailored for waypoint navigation. Our approach integrates two key deep learning components: (1) a Bayesian neural network that estimates spatial uncertainty in real time, and (2) an attention-based reinforcement learning policy that performs global reasoning over a probabilistic roadmap, using uncertainty estimates to plan informative and energy-efficient trajectories. This graph-based reasoning enables intelligent, non-myopic trajectory planning, guiding agents toward the most informative regions while satisfying safety constraints. Experimental results show that URAM improves reconstruction accuracy by up to 34% over existing baselines. 

**Abstract (ZH)**: 低空经济背景下，无线电图对于确保与空中平台的可靠无线连接变得至关重要。基于航点导航的自主空中代理通常用于数据采集，然而其有限的电池容量显著限制了覆盖范围和效率。为了解决这一问题，我们提出了一种不确定性意识无线电图（URAM）重建框架，该框架明确利用了适用于航点导航的图基推理。我们的方法整合了两个关键的深度学习组件：（1）一个贝叶斯神经网络，实时估计空间不确定性；（2）一种基于注意力的强化学习策略，在概率路网中进行全局推理，并使用不确定性估计来规划具有信息性和节能性的轨迹。这种图基推理使智能、非近视路线规划成为可能，引导代理向最有信息性区域导航，同时满足安全约束。实验结果显示，URAM在重建准确性上较现有基线提高了最多34%。 

---
