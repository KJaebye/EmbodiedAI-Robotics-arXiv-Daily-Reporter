{'arxiv_id': 'arXiv:2508.09976', 'title': 'Masquerade: Learning from In-the-wild Human Videos using Data-Editing', 'authors': 'Marion Lepert, Jiaying Fang, Jeannette Bohg', 'link': 'https://arxiv.org/abs/2508.09976', 'abstract': 'Robot manipulation research still suffers from significant data scarcity: even the largest robot datasets are orders of magnitude smaller and less diverse than those that fueled recent breakthroughs in language and vision. We introduce Masquerade, a method that edits in-the-wild egocentric human videos to bridge the visual embodiment gap between humans and robots and then learns a robot policy with these edited videos. Our pipeline turns each human video into robotized demonstrations by (i) estimating 3-D hand poses, (ii) inpainting the human arms, and (iii) overlaying a rendered bimanual robot that tracks the recovered end-effector trajectories. Pre-training a visual encoder to predict future 2-D robot keypoints on 675K frames of these edited clips, and continuing that auxiliary loss while fine-tuning a diffusion policy head on only 50 robot demonstrations per task, yields policies that generalize significantly better than prior work. On three long-horizon, bimanual kitchen tasks evaluated in three unseen scenes each, Masquerade outperforms baselines by 5-6x. Ablations show that both the robot overlay and co-training are indispensable, and performance scales logarithmically with the amount of edited human video. These results demonstrate that explicitly closing the visual embodiment gap unlocks a vast, readily available source of data from human videos that can be used to improve robot policies.', 'abstract_zh': '机器人操作研究仍受到显著的数据稀缺性困扰：即使最大的机器人数据集也比 recent 语言和视觉领域的突破所依赖的数据小几个数量级且不那么多样化。我们提出了 Masquerade 方法，该方法编辑野外第一人称人类视频以弥合人类与机器人之间的视觉实体差距，然后使用这些编辑后的视频学习一种机器人策略。我们的管道通过以下步骤将每段人类视频转换为机器人化演示：(i) 估计 3D 手部姿势，(ii) 填充人类手臂，(iii) 覆盖渲染的双臂机器人，使其跟踪恢复的末端执行器轨迹。在 675,000 帧这些编辑片段的基础上预训练一个视觉编码器以预测未来 2D 机器人关键点，并在仅使用 50 个机器人演示每任务的情况下微调扩散策略头的同时继续使用该辅助损失，可以比之前的工作获得显著更好的泛化性能。在三个长时距的双臂厨房任务中评估，其中每个任务有三个未见过的场景，Masquerade 的表现比基线高出 5-6 倍。消融实验表明，机器人覆盖和联合训练都是必不可少的，性能随编辑的人类视频量按对数关系增长。这些结果表明，明确地弥合视觉实体差距可以解锁大量现成的数据来源，这些数据可以从人类视频中获取并用于提高机器人策略。', 'title_zh': '欺瞒：利用数据编辑从wild数据中学习'}
{'arxiv_id': 'arXiv:2508.09971', 'title': 'Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model', 'authors': 'Zihan Wang, Nina Mahmoudian', 'link': 'https://arxiv.org/abs/2508.09971', 'abstract': "Vision-driven autonomous river following by Unmanned Aerial Vehicles is critical for applications such as rescue, surveillance, and environmental monitoring, particularly in dense riverine environments where GPS signals are unreliable. We formalize river following as a coverage control problem in which the reward function is submodular, yielding diminishing returns as more unique river segments are visited, thereby framing the task as a Submodular Markov Decision Process. First, we introduce Marginal Gain Advantage Estimation, which refines the reward advantage function by using a sliding window baseline computed from historical episodic returns, thus aligning the advantage estimation with the agent's evolving recognition of action value in non-Markovian settings. Second, we develop a Semantic Dynamics Model based on patchified water semantic masks that provides more interpretable and data-efficient short-term prediction of future observations compared to latent vision dynamics models. Third, we present the Constrained Actor Dynamics Estimator architecture, which integrates the actor, the cost estimator, and SDM for cost advantage estimation to form a model-based SafeRL framework capable of solving partially observable Constrained Submodular Markov Decision Processes. Simulation results demonstrate that MGAE achieves faster convergence and superior performance over traditional critic-based methods like Generalized Advantage Estimation. SDM provides more accurate short-term state predictions that enable the cost estimator to better predict potential violations. Overall, CADE effectively integrates safety regulation into model-based RL, with the Lagrangian approach achieving the soft balance of reward and safety during training, while the safety layer enhances performance during inference by hard action overlay.", 'abstract_zh': '基于视觉的自主河流跟随对于救援、监控和环境监测等应用至关重要，特别是在GPS信号不可靠的密集河流环境中。我们将河流跟随问题形式化为一个子模态覆盖控制问题，其中奖励函数随更多独特河流段的访问而递减，从而将其框架定义为子模态马尔科夫决策过程。首先，我们引入边缘增益优势估计方法，通过使用基于历史 episodic 返回的滑动窗口基线来细化奖励优势函数，从而在非马尔科夫环境中使优势估计与智能体逐渐发展出的动作价值认识保持一致。其次，我们开发了基于补丁化的水语义掩模的语义动态模型，与潜在视觉动力学模型相比，提供了更可解释和数据高效型的短期未来观测预测。第三，我们提出了约束动作动态估计器架构，该架构结合了行为者、成本估计器和语义动态模型（SDM），以形成一个基于模型的安全强化学习（SafeRL）框架，能够解决部分可观测的约束子模态马尔科夫决策过程。模拟结果表明，边缘增益优势估计（MGAE）在收敛速度和性能上均优于传统的基于评论家的方法（如广义优势估计）。语义动态模型提供了更准确的短期状态预测，使成本估计器能够更好地预测潜在的违规行为。总体而言，约束动作动态估计器架构有效地将安全性约束整合进了基于模型的强化学习中，拉格朗日方法在训练过程中实现了奖励和安全的软平衡，而安全性层在推理过程中通过硬动作叠加提升了性能。', 'title_zh': '基于视觉的无人机安全强化学习河流跟踪方法及其语义动力模型'}
{'arxiv_id': 'arXiv:2508.09960', 'title': 'GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation', 'authors': 'Yifei Yao, Chengyuan Luo, Jiaheng Du, Wentao He, Jun-Guo Lu', 'link': 'https://arxiv.org/abs/2508.09960', 'abstract': 'The creation of human-like humanoid robots is hindered by a fundamental fragmentation: data processing and learning algorithms are rarely universal across different robot morphologies. This paper introduces the Generalized Behavior Cloning (GBC) framework, a comprehensive and unified solution designed to solve this end-to-end challenge. GBC establishes a complete pathway from human motion to robot action through three synergistic innovations. First, an adaptive data pipeline leverages a differentiable IK network to automatically retarget any human MoCap data to any humanoid. Building on this foundation, our novel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust, high-fidelity imitation policies. To complete the ecosystem, the entire framework is delivered as an efficient, open-source platform based on Isaac Lab, empowering the community to deploy the full workflow via simple configuration scripts. We validate the power and generality of GBC by training policies on multiple heterogeneous humanoids, demonstrating excellent performance and transfer to novel motions. This work establishes the first practical and unified pathway for creating truly generalized humanoid controllers.', 'abstract_zh': '人类类似人形机器人的创造除受基本碎片化问题的阻碍：不同的机器人形态 rarely 跨数据处理和学习算法通用。本文介绍了通用行为克隆（GBC）框架，这是一个综合性且统一的解决方案，旨在解决这一端到端挑战。GBC 通过三大协同创新建立了一条从人类运动到机器人行动的完整路径。首先，自适应数据管道利用可微逆运动学（IK）网络自动将任何人类运动捕捉（MoCap）数据重新目标化到任何类人机器人。在此基础上，我们的新颖 DAgger-MMPPO 算法及其 MMTransformer 架构学习到鲁棒且高保真度的模仿策略。为了完成生态系统，整个框架基于 Isaac Lab 提供了一个高效且开源的平台，使社区能够通过简单的配置脚本部署完整的流程。我们通过在多个异质类人机器人上训练策略验证了 GBC 的能力和普适性，展示了其在新动作上的出色表现和迁移能力。这项工作建立了创建真正通用类人控制器的第一个实际且统一的途径。', 'title_zh': '通用行为克隆框架：整体人体类人形机器人模仿'}
{'arxiv_id': 'arXiv:2508.09950', 'title': 'PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces', 'authors': 'Bida Ma, Nuo Xu, Chenkun Qi, Xin Liu, Yule Mo, Jinkai Wang, Chunpeng Lu', 'link': 'https://arxiv.org/abs/2508.09950', 'abstract': "The legged locomotion in spatially constrained structures (called crawl spaces) is challenging. In crawl spaces, current exteroceptive locomotion learning methods are limited by large noises and errors of the sensors in possible low visibility conditions, and current proprioceptive locomotion learning methods are difficult in traversing crawl spaces because only ground features are inferred. In this study, a point cloud supervised proprioceptive locomotion reinforcement learning method for legged robots in crawl spaces is proposed. A state estimation network is designed to estimate the robot's surrounding ground and spatial features as well as the robot's collision states using historical proprioceptive sensor data. The point cloud is represented in polar coordinate frame and a point cloud processing method is proposed to efficiently extract the ground and spatial features that are used to supervise the state estimation network learning. Comprehensive reward functions that guide the robot to traverse through crawl spaces after collisions are designed. Experiments demonstrate that, compared to existing methods, our method exhibits more agile locomotion in crawl spaces. This study enhances the ability of legged robots to traverse spatially constrained environments without requiring exteroceptive sensors.", 'abstract_zh': '基于空间受限结构中足式运动的点云监督 proprioceptive 运动强化学习方法', 'title_zh': 'PPL： crawl space中腿式机器人基于点云监督的本体感受性强化学习'}
{'arxiv_id': 'arXiv:2508.09876', 'title': 'A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion', 'authors': 'Xiaowei Tan, Weizhong Jiang, Bi Zhang, Wanxin Chen, Yiwen Zhao, Ning Li, Lianqing Liu, Xingang Zhao', 'link': 'https://arxiv.org/abs/2508.09876', 'abstract': "Exoskeletons have been shown to effectively assist humans during steady locomotion. However, their effects on non-steady locomotion, characterized by nonlinear phase progression within a gait cycle, remain insufficiently explored, particularly across diverse activities. This work presents a shank angle-based control system that enables the exoskeleton to maintain real-time coordination with human gait, even under phase perturbations, while dynamically shaping assistance profiles to match the biological ankle moment patterns across walking, running, stair negotiation tasks. The control system consists of an assistance profile online generation method and a model-based feedforward control method. The assistance profile is formulated as a dual-Gaussian model with the shank angle as the independent variable. Leveraging only IMU measurements, the model parameters are updated online each stride to adapt to inter- and intra-individual biomechanical variability. The profile tracking control employs a human-exoskeleton kinematics and stiffness model as a feedforward component, reducing reliance on historical control data due to the lack of clear and consistent periodicity in non-steady locomotion. Three experiments were conducted using a lightweight soft exoskeleton with multiple subjects. The results validated the effectiveness of each individual method, demonstrated the robustness of the control system against gait perturbations across various activities, and revealed positive biomechanical and physiological responses of human users to the exoskeleton's mechanical assistance.", 'abstract_zh': '基于小腿角的外骨骼控制系统：应对非稳态步行的实时协调与辅助塑造', 'title_zh': '基于足尖角的控制系统使软外骨骼能够辅助人类非稳态运动'}
{'arxiv_id': 'arXiv:2508.09855', 'title': 'Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes', 'authors': 'Yuekun Wu, Yik Lung Pang, Andrea Cavallaro, Changjae Oh', 'link': 'https://arxiv.org/abs/2508.09855', 'abstract': 'Human-robot teaming (HRT) systems often rely on large-scale datasets of human and robot interactions, especially for close-proximity collaboration tasks such as human-robot handovers. Learning robot manipulation policies from raw, real-world image data requires a large number of robot-action trials in the physical environment. Although simulation training offers a cost-effective alternative, the visual domain gap between simulation and robot workspace remains a major limitation. We introduce a method for training HRT policies, focusing on human-to-robot handovers, solely from RGB images without the need for real-robot training or real-robot data collection. The goal is to enable the robot to reliably receive objects from a human with stable grasping while avoiding collisions with the human hand. The proposed policy learner leverages sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes to generate robot demonstrations containing image-action pairs captured with a camera mounted on the robot gripper. As a result, the simulated camera pose changes in the reconstructed scene can be directly translated into gripper pose changes. Experiments in both Gaussian Splatting reconstructed scene and real-world human-to-robot handover experiments demonstrate that our method serves as a new and effective representation for the human-to-robot handover task, contributing to more seamless and robust HRT.', 'abstract_zh': '基于RGB图像训练人机协作政策：专注于人对机器人手递任务', 'title_zh': '向着人机协同的目标：从三维场景学习交接行为'}
{'arxiv_id': 'arXiv:2508.09846', 'title': 'Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation', 'authors': 'Donghoon Baek, Amartya Purushottam, Jason J. Choi, Joao Ramos', 'link': 'https://arxiv.org/abs/2508.09846', 'abstract': "This paper presents an object-aware whole-body bilateral teleoperation framework for wheeled humanoid loco-manipulation. This framework combines whole-body bilateral teleoperation with an online multi-stage object inertial parameter estimation module, which is the core technical contribution of this work. The multi-stage process sequentially integrates a vision-based object size estimator, an initial parameter guess generated by a large vision-language model (VLM), and a decoupled hierarchical sampling strategy. The visual size estimate and VLM prior offer a strong initial guess of the object's inertial parameters, significantly reducing the search space for sampling-based refinement and improving the overall estimation speed. A hierarchical strategy first estimates mass and center of mass, then infers inertia from object size to ensure physically feasible parameters, while a decoupled multi-hypothesis scheme enhances robustness to VLM prior errors. Our estimator operates in parallel with high-fidelity simulation and hardware, enabling real-time online updates. The estimated parameters are then used to update the wheeled humanoid's equilibrium point, allowing the operator to focus more on locomotion and manipulation. This integration improves the haptic force feedback for dynamic synchronization, enabling more dynamic whole-body teleoperation. By compensating for object dynamics using the estimated parameters, the framework also improves manipulation tracking while preserving compliant behavior. We validate the system on a customized wheeled humanoid with a robotic gripper and human-machine interface, demonstrating real-time execution of lifting, delivering, and releasing tasks with a payload weighing approximately one-third of the robot's body weight.", 'abstract_zh': '本文提出了一种针对轮式类人仿生移动与操作的物体感知整体体双边远程操作框架。该框架结合 �整合了整体体双边远程操作与基于视觉的多网格格参数估计的在线阶段过程。该该多阶段过程依次结合由基于视觉的物体感知估计算法、通过大规模视觉-语言模型（VLM）生成的初步参数猜测，以及解耦的分层次抽样策略。视觉估计和 VLM � 前知共同提供了物体惯性性参数的强先验估计，，为此基上了奠定了格步优化基础并提高整体的速度 on。首先的多次层次定义算法估计质量并 以及重心，，然后从质量推到重心推导惯性，，并进而推导出物理上可行的参数。同时，解耦的多多多层次假设方案增强了了对 VLM 基先估计的鲁棒性性。我们的估计算法与高保真度的仿真和硬件并行运行 on，实现实时在线线线线线线线线线线更新。估计的参数 on则通过上更新轮式类人人人人类人人的平衡点以进一步集中在移动和操作上。该集成改善了触觉反馈以动态同步 on从而实现了更动态的整体体远程操作。通过使用所估计算拟物体动力学， on改善了操作追踪同时保留了顺应行为。我们在一架定制的轮式类ononon on类on类人人的设计上面上集带了机器人手爪，并上展示了实时执行提升、输送和放置任务的能力，其中负重约为机器人重量的三分之一。', 'title_zh': '全身体部位双边遥控操作及其多阶段物体参数估计在轮式类人机器人手上操作中的应用'}
{'arxiv_id': 'arXiv:2508.09836', 'title': 'Embodied Tactile Perception of Soft Objects Properties', 'authors': 'Anirvan Dutta, Alexis WM Devillard, Zhihuan Zhang, Xiaoxiao Cheng, Etienne Burdet', 'link': 'https://arxiv.org/abs/2508.09836', 'abstract': 'To enable robots to develop human-like fine manipulation, it is essential to understand how mechanical compliance, multi-modal sensing, and purposeful interaction jointly shape tactile perception. In this study, we use a dedicated modular e-Skin with tunable mechanical compliance and multi-modal sensing (normal, shear forces and vibrations) to systematically investigate how sensing embodiment and interaction strategies influence robotic perception of objects. Leveraging a curated set of soft wave objects with controlled viscoelastic and surface properties, we explore a rich set of palpation primitives-pressing, precession, sliding that vary indentation depth, frequency, and directionality. In addition, we propose the latent filter, an unsupervised, action-conditioned deep state-space model of the sophisticated interaction dynamics and infer causal mechanical properties into a structured latent space. This provides generalizable and in-depth interpretable representation of how embodiment and interaction determine and influence perception. Our investigation demonstrates that multi-modal sensing outperforms uni-modal sensing. It highlights a nuanced interaction between the environment and mechanical properties of e-Skin, which should be examined alongside the interaction by incorporating temporal dynamics.', 'abstract_zh': '使机器人具备类人的精细操作能力，理解机械柔顺性、多模态感知和目的性交互如何共同塑造触觉感知至关重要。在本研究中，我们使用具有可调机械柔顺性和多模态感知（正常力、切力和振动）的专用模块化电子皮肤，系统地研究了感知体态和交互策略如何影响机器人的物体感知。利用一组具有可控粘弹性及表面性质的软波形物体，我们探索了包括按压、旋转变换和滑动等多种丰富的触觉基本操作，这些操作可变化深度、频率和方向性。此外，我们提出了潜在滤波器，这是一种无监督的、基于动作条件的深度状态空间模型，用于推断复杂的交互动力学，并将因果机械特性映射到结构化的潜在空间。这提供了感知体态和交互如何决定和影响感知的一般化和深入解释的表示。我们的研究展示了多模态感知优于单模态感知，并强调了环境与电子皮肤机械性质之间微妙的交互作用，这种交互作用应在考虑时间动态的同时进行研究。', 'title_zh': '软物体质地的 embodied 触觉感知'}
{'arxiv_id': 'arXiv:2508.09797', 'title': 'FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning', 'authors': 'Dongcheng Cao, Jin Zhou, Xian Wang, Shuo Li', 'link': 'https://arxiv.org/abs/2508.09797', 'abstract': 'Agile flight for the quadrotor cable-suspended payload system is a formidable challenge due to its underactuated, highly nonlinear, and hybrid dynamics. Traditional optimization-based methods often struggle with high computational costs and the complexities of cable mode transitions, limiting their real-time applicability and maneuverability exploitation. In this letter, we present FLARE, a reinforcement learning (RL) framework that directly learns agile navigation policy from high-fidelity simulation. Our method is validated across three designed challenging scenarios, notably outperforming a state-of-the-art optimization-based approach by a 3x speedup during gate traversal maneuvers. Furthermore, the learned policies achieve successful zero-shot sim-to-real transfer, demonstrating remarkable agility and safety in real-world experiments, running in real time on an onboard computer.', 'abstract_zh': '基于强化学习的四旋翼悬挂载荷系统的敏捷飞行：FLARE框架', 'title_zh': 'FLARE: 通过强化学习的四旋翼缆挂载荷系统敏捷飞行方法'}
{'arxiv_id': 'arXiv:2508.09700', 'title': 'Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions', 'authors': 'Mahdi Hejrati, Jouni Mattila', 'link': 'https://arxiv.org/abs/2508.09700', 'abstract': 'Teleoperation of beyond-human-scale robotic manipulators (BHSRMs) presents unique challenges that differ fundamentally from conventional human-scale systems. As these platforms gain relevance in industrial domains such as construction, mining, and disaster response, immersive interfaces must be rethought to support scalable, safe, and effective human-robot collaboration. This paper investigates the control, cognitive, and interface-level challenges of immersive teleoperation in BHSRMs, with a focus on ensuring operator safety, minimizing sensorimotor mismatch, and enhancing the sense of embodiment. We analyze design trade-offs in haptic and visual feedback systems, supported by early experimental comparisons of exoskeleton- and joystick-based control setups. Finally, we outline key research directions for developing new evaluation tools, scaling strategies, and human-centered safety models tailored to large-scale robotic telepresence.', 'abstract_zh': '超越人类规模的机器人操作手远程操作：超越人类规模的机器人操作手(BHSRMs)的远程操作提出了与传统人类规模系统根本不同的独特挑战。随着这些平台在建筑、采矿和灾难响应等工业领域中的重要性不断提高，沉浸式界面必须重新设计以支持可扩展、安全和有效的有人-机器人协作。本文探讨了BHSRMs中沉浸式远程操作的控制、认知和界面级挑战，重点关注确保操作员安全、减少感觉运动不匹配以及增强沉浸感。我们分析了触觉和视觉反馈系统的设计权衡，并通过早期的外骨骼和joystick操作设置的实验比较加以支持。最后，我们概述了开发新的评估工具、扩展策略和以人为核心的安全模型的关键研究方向，这些模型适用于大规模机器人远程操作。', 'title_zh': '沉浸式远程操作超人类规模机器人 manipulator 删除器：挑战与未来方向'}
{'arxiv_id': 'arXiv:2508.09621', 'title': 'Interpretable Robot Control via Structured Behavior Trees and Large Language Models', 'authors': 'Ingrid Maéva Chekam, Ines Pastor-Martinez, Ali Tourani, Jose Andres Millan-Romera, Laura Ribeiro, Pedro Miguel Bastos Soares, Holger Voos, Jose Luis Sanchez-Lopez', 'link': 'https://arxiv.org/abs/2508.09621', 'abstract': 'As intelligent robots become more integrated into human environments, there is a growing need for intuitive and reliable Human-Robot Interaction (HRI) interfaces that are adaptable and more natural to interact with. Traditional robot control methods often require users to adapt to interfaces or memorize predefined commands, limiting usability in dynamic, unstructured environments. This paper presents a novel framework that bridges natural language understanding and robotic execution by combining Large Language Models (LLMs) with Behavior Trees. This integration enables robots to interpret natural language instructions given by users and translate them into executable actions by activating domain-specific plugins. The system supports scalable and modular integration, with a primary focus on perception-based functionalities, such as person tracking and hand gesture recognition. To evaluate the system, a series of real-world experiments was conducted across diverse environments. Experimental results demonstrate that the proposed approach is practical in real-world scenarios, with an average cognition-to-execution accuracy of approximately 94%, making a significant contribution to HRI systems and robots. The complete source code of the framework is publicly available at this https URL.', 'abstract_zh': '随着智能机器人越来越多地融入人类环境，需要更加直观可靠、适应性强且交互更加自然的人机交互（HRI）界面。传统机器人控制方法往往要求用户适应界面或记忆预定义命令，限制了其在动态、非结构化环境中的可用性。本文提出了一种新的框架，通过将大型语言模型（LLMs）与行为树相结合，以实现自然语言理解和机器人执行之间的桥梁。该集成使机器人能够理解用户给出的自然语言指令，并通过激活领域特定插件将这些指令转化为可执行的动作。该系统支持可扩展和模块化的集成，主要关注基于感知的功能，如人员跟踪和手势识别。通过在多种环境中的实际实验对该系统进行了评估。实验结果表明，所提出的方法在现实世界场景中是现实可行的，认知到执行的平均准确率约为94%，对人机交互系统和机器人做出了重要贡献。该框架的完整源代码可以在以下网址获取：this https URL。', 'title_zh': 'interpretable 机器人控制通过结构化行为树和大规模语言模型'}
{'arxiv_id': 'arXiv:2508.09606', 'title': 'BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots', 'authors': 'Alejandro Posadas-Nava, Alejandro Carrasco, Richard Linares', 'link': 'https://arxiv.org/abs/2508.09606', 'abstract': "\\textbf{BEAVR} is an open-source, bimanual, multi-embodiment Virtual Reality (VR) teleoperation system for robots, designed to unify real-time control, data recording, and policy learning across heterogeneous robotic platforms. BEAVR enables real-time, dexterous teleoperation using commodity VR hardware, supports modular integration with robots ranging from 7-DoF manipulators to full-body humanoids, and records synchronized multi-modal demonstrations directly in the LeRobot dataset schema. Our system features a zero-copy streaming architecture achieving $\\leq$35\\,ms latency, an asynchronous ``think--act'' control loop for scalable inference, and a flexible network API optimized for real-time, multi-robot operation. We benchmark BEAVR across diverse manipulation tasks and demonstrate its compatibility with leading visuomotor policies such as ACT, DiffusionPolicy, and SmolVLA. All code is publicly available, and datasets are released on Hugging Face\\footnote{Code, datasets, and VR app available at this https URL.", 'abstract_zh': 'BEAVR是一个开源的双臂多躯体虚拟现实(VR)遥控系统，用于机器人，旨在统一异构机器人平台上的实时控制、数据记录和策略学习。BEAVR使用户能够使用商用VR硬件进行实时、灵巧的遥控操作，支持从7自由度 manipulator 到全身类人机器人等多种型号的机器人模块化集成，并直接在LeRobot数据集模式中记录同步的多模态演示。该系统具有零拷贝流式架构，延迟≤35毫秒，采用异步“思考-行动”控制环路进行可扩展推理，以及针对实时多机器人操作优化的灵活网络API。我们在多种操作任务中对BEAVR进行了基准测试，并展示了其与领先的视觉-运动策略（如ACT、DiffusionPolicy和SmolVLA）的兼容性。所有代码均可公开访问，数据集在Hugging Face上发布。', 'title_zh': 'BEAVR: 双手、多身体、Accessible、虚拟现实远程操作机器人系统'}
{'arxiv_id': 'arXiv:2508.09595', 'title': 'HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control', 'authors': 'Michael Fennel, Markus Walker, Dominik Pikos, Uwe D. Hanebeck', 'link': 'https://arxiv.org/abs/2508.09595', 'abstract': 'Research in virtual reality and haptic technologies has consistently aimed to enhance immersion. While advanced head-mounted displays are now commercially available, kinesthetic haptic interfaces still face challenges such as limited workspaces, insufficient degrees of freedom, and kinematics not matching the human arm. In this paper, we present HapticGiant, a novel large-scale kinesthetic haptic interface designed to match the properties of the human arm as closely as possible and to facilitate natural user locomotion while providing full haptic feedback. The interface incorporates a novel admittance-type force control scheme, leveraging hierarchical optimization to render both arbitrary serial kinematic chains and Cartesian admittances. Notably, the proposed control scheme natively accounts for system limitations, including joint and Cartesian constraints, as well as singularities. Experimental results demonstrate the effectiveness of HapticGiant and its control scheme, paving the way for highly immersive virtual reality applications.', 'abstract_zh': '基于虚拟现实和触觉技术的研究一直致力于增强沉浸感。虽然先进的头戴式显示设备已经商业化，但动觉触觉接口仍面临工作空间有限、自由度不足以及运动学与人类手臂不匹配等挑战。在本文中，我们提出了一种名为HapticGiant的新颖大规模动觉触觉接口，旨在尽可能地匹配人类手臂的特性，并促进自然用户移动，同时提供完整的触觉反馈。该接口采用了一种新颖的阻抗型力控制方案，利用分层优化来呈现任意串联系动链和笛卡尔阻抗。值得注意的是，所提出的控制方案自然地考虑了系统的局限性，包括关节和笛卡尔约束以及奇异性。实验结果表明HapticGiant及其控制方案的有效性，为高度沉浸的虚拟现实应用铺平了道路。', 'title_zh': 'HapticGiant：一种具有分层力控制的新型大型触觉力反馈接口'}
{'arxiv_id': 'arXiv:2508.09581', 'title': 'ESCoT: An Enhanced Step-based Coordinate Trajectory Planning Method for Multiple Car-like Robots', 'authors': 'Junkai Jiang, Yihe Chen, Yibin Yang, Ruochen Li, Shaobing Xu, Jianqiang Wang', 'link': 'https://arxiv.org/abs/2508.09581', 'abstract': "Multi-vehicle trajectory planning (MVTP) is one of the key challenges in multi-robot systems (MRSs) and has broad applications across various fields. This paper presents ESCoT, an enhanced step-based coordinate trajectory planning method for multiple car-like robots. ESCoT incorporates two key strategies: collaborative planning for local robot groups and replanning for duplicate configurations. These strategies effectively enhance the performance of step-based MVTP methods. Through extensive experiments, we show that ESCoT 1) in sparse scenarios, significantly improves solution quality compared to baseline step-based method, achieving up to 70% improvement in typical conflict scenarios and 34% in randomly generated scenarios, while maintaining high solving efficiency; and 2) in dense scenarios, outperforms all baseline methods, maintains a success rate of over 50% even in the most challenging configurations. The results demonstrate that ESCoT effectively solves MVTP, further extending the capabilities of step-based methods. Finally, practical robot tests validate the algorithm's applicability in real-world scenarios.", 'abstract_zh': '多车辆轨迹规划（MVTP）是多机器人系统（MRSs）中的一个关键挑战，具有广泛的应用前景。本文提出了一种增强的基于步长的坐标轨迹规划方法ESCoT，用于多个类似车辆的机器人。ESCoT结合了两种关键技术策略：局部机器人组的协作规划和重复配置的重规划。这些策略有效提高了基于步长的多车辆轨迹规划方法的性能。通过广泛的实验，我们展示了ESCoT在稀疏场景中显著提高了解的质量，与基线基于步长的方法相比，在典型冲突场景中提高了70%，在随机生成的场景中提高了34%，同时保持了高解算效率；在密集场景中，ESCoT超过了所有基线方法，在最具挑战性的配置中仍能保持超过50%的成功率。结果表明，ESCoT有效地解决了MVTP问题，进一步扩展了基于步长的方法的能力。最后，实际的机器人测试验证了该算法在现实场景中的适用性。', 'title_zh': 'ESCoT: 一种增强型基于步进的坐标轨迹规划方法用于多个类汽车机器人'}
{'arxiv_id': 'arXiv:2508.09558', 'title': 'CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail', 'authors': 'Jiahui Zuo, Boyang Zhang, Fumin Zhang', 'link': 'https://arxiv.org/abs/2508.09558', 'abstract': 'The manipulation of deformable linear flexures has a wide range of applications in industry, such as cable routing in automotive manufacturing and textile production. Cable routing, as a complex multi-stage robot manipulation scenario, is a challenging task for robot automation. Common parallel two-finger grippers have the risk of over-squeezing and over-tension when grasping and guiding cables. In this paper, a novel eagle-inspired fingernail is designed and mounted on the gripper fingers, which helps with cable grasping on planar surfaces and in-hand cable guiding operations. Then we present a single-grasp end-to-end 3D cable routing framework utilizing the proposed fingernails, instead of the common pick-and-place strategy. Continuous control is achieved to efficiently manipulate cables through vision-based state estimation of task configurations and offline trajectory planning based on motion primitives. We evaluate the effectiveness of the proposed framework with a variety of cables and channel slots, significantly outperforming the pick-and-place manipulation process under equivalent perceptual conditions. Our reconfigurable task setting and the proposed framework provide a reference for future cable routing manipulations in 3D space.', 'abstract_zh': '基于鹰爪启发的单抓取端到端3D电缆走线框架研究', 'title_zh': 'CaRoBio: 基于生物启发的夹指的3D电缆布线'}
{'arxiv_id': 'arXiv:2508.09508', 'title': 'SMART-OC: A Real-time Time-risk Optimal Replanning Algorithm for Dynamic Obstacles and Spatio-temporally Varying Currents', 'authors': 'Reema Raval, Shalabh Gupta', 'link': 'https://arxiv.org/abs/2508.09508', 'abstract': 'Typical marine environments are highly complex with spatio-temporally varying currents and dynamic obstacles, presenting significant challenges to Unmanned Surface Vehicles (USVs) for safe and efficient navigation. Thus, the USVs need to continuously adapt their paths with real-time information to avoid collisions and follow the path of least resistance to the goal via exploiting ocean currents. In this regard, we introduce a novel algorithm, called Self-Morphing Adaptive Replanning Tree for dynamic Obstacles and Currents (SMART-OC), that facilitates real-time time-risk optimal replanning in dynamic environments. SMART-OC integrates the obstacle risks along a path with the time cost to reach the goal to find the time-risk optimal path. The effectiveness of SMART-OC is validated by simulation experiments, which demonstrate that the USV performs fast replannings to avoid dynamic obstacles and exploit ocean currents to successfully reach the goal.', 'abstract_zh': '典型的海洋环境具有时空变化的流场和动态障碍，给无人水面车辆（USVs）的安全高效导航带来了巨大挑战。因此，USVs需要根据实时信息不断调整航路以避免碰撞，并通过利用海洋流场找到通往目标的阻力最小路径。为此，我们提出了一种名为自适应重塑规划树算法以应对动态障碍和流场（SMART-OC）的新算法，该算法在动态环境中实现实时时间风险最优重规划。SMART-OC将路径上的障碍风险与到达目标的时间成本相结合，以找到时间风险最优路径。通过仿真实验验证了SMART-OC的有效性，结果显示USV能够快速调整航路以避开动态障碍并利用海洋流场成功到达目标。', 'title_zh': 'SMART-OC：一种实时时空风险最优再规划算法，用于动态障碍物和时空变化水流'}
{'arxiv_id': 'arXiv:2508.09502', 'title': 'Reactive Model Predictive Contouring Control for Robot Manipulators', 'authors': 'Junheon Yoon, Woo-Jeong Baek, Jaeheung Park', 'link': 'https://arxiv.org/abs/2508.09502', 'abstract': 'This contribution presents a robot path-following framework via Reactive Model Predictive Contouring Control (RMPCC) that successfully avoids obstacles, singularities and self-collisions in dynamic environments at 100 Hz. Many path-following methods rely on the time parametrization, but struggle to handle collision and singularity avoidance while adhering kinematic limits or other constraints. Specifically, the error between the desired path and the actual position can become large when executing evasive maneuvers. Thus, this paper derives a method that parametrizes the reference path by a path parameter and performs the optimization via RMPCC. In particular, Control Barrier Functions (CBFs) are introduced to avoid collisions and singularities in dynamic environments. A Jacobian-based linearization and Gauss-Newton Hessian approximation enable solving the nonlinear RMPCC problem at 100 Hz, outperforming state-of-the-art methods by a factor of 10. Experiments confirm that the framework handles dynamic obstacles in real-world settings with low contouring error and low robot acceleration.', 'abstract_zh': '基于反应模型预测轮廓控制（RMPCC）的机器人路径跟随框架：在动态环境中的高频率避障与避奇异性', 'title_zh': '基于反应模型预测轮廓控制的机器人 manipulator 控制方法'}
{'arxiv_id': 'arXiv:2508.09444', 'title': 'DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation', 'authors': 'Haoxiang Shi, Xiang Deng, Zaijing Li, Gongwei Chen, Yaowei Wang, Liqiang Nie', 'link': 'https://arxiv.org/abs/2508.09444', 'abstract': "Vision-Language Navigation in Continuous Environments (VLN-CE) requires agents to follow natural language instructions through free-form 3D spaces. Existing VLN-CE approaches typically use a two-stage waypoint planning framework, where a high-level waypoint predictor generates the navigable waypoints, and then a navigation planner suggests the intermediate goals in the high-level action space. However, this two-stage decomposition framework suffers from: (1) global sub-optimization due to the proxy objective in each stage, and (2) a performance bottleneck caused by the strong reliance on the quality of the first-stage predicted waypoints. To address these limitations, we propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE policy that unifies the traditional two stages, i.e. waypoint generation and planning, into a single diffusion policy. Notably, DifNav employs a conditional diffusion policy to directly model multi-modal action distributions over future actions in continuous navigation space, eliminating the need for a waypoint predictor while enabling the agent to capture multiple possible instruction-following behaviors. To address the issues of compounding error in imitation learning and enhance spatial reasoning in long-horizon navigation tasks, we employ DAgger for online policy training and expert trajectory augmentation, and use the aggregated data to further fine-tune the policy. This approach significantly improves the policy's robustness and its ability to recover from error states. Extensive experiments on benchmark datasets demonstrate that, even without a waypoint predictor, the proposed method substantially outperforms previous state-of-the-art two-stage waypoint-based models in terms of navigation performance. Our code is available at: this https URL.", 'abstract_zh': '连续环境中的视觉-语言导航（VLN-CE）要求代理遵循自然语言指令通过自由形式的3D空间导航。现有的VLN-CE方法通常采用两阶段的航点规划框架，其中高层航点预测器生成可导航的航点，然后导航规划器在高层动作空间中建议中间目标。然而，这种两阶段分解框架面临以下局限性：（1）由于每个阶段的代理目标而产生的全局次优性，以及（2）性能瓶颈，由于对第一阶段预测航点质量的强烈依赖。为了解决这些问题，我们提出了面向扩散的航向导航（DifNav），这是一种端到端优化的VLN-CE策略，将传统的两个阶段，即航点生成和规划，统一成一个单差分策略。值得注意的是，DifNav 使用条件扩散策略直接建模连续导航空间中未来动作的多模态动作分布，从而消除航点预测器的需要，同时使代理能够捕捉到多种可能的指令遵循行为。为了应对模仿学习中的累积误差问题，并增强长期导航任务中的空间推理能力，我们采用了DAgger进行在线策略训练和专家轨迹增强，并使用汇总数据进一步微调策略。该方法显著提高了策略的鲁棒性及其从错误状态中恢复的能力。基准数据集上的广泛实验表明，即使没有航点预测器，所提出的方法在导航性能上也显著优于之前的基于两阶段航点预测的最新模型。我们的代码可在以下地址获得：this https URL。', 'title_zh': 'DAgger扩散导航：增强扩散策略的DAgger视觉语言导航'}
{'arxiv_id': 'arXiv:2508.09354', 'title': 'CLF-RL: Control Lyapunov Function Guided Reinforcement Learning', 'authors': 'Kejun Li, Zachary Olkin, Yisong Yue, Aaron D. Ames', 'link': 'https://arxiv.org/abs/2508.09354', 'abstract': 'Reinforcement learning (RL) has shown promise in generating robust locomotion policies for bipedal robots, but often suffers from tedious reward design and sensitivity to poorly shaped objectives. In this work, we propose a structured reward shaping framework that leverages model-based trajectory generation and control Lyapunov functions (CLFs) to guide policy learning. We explore two model-based planners for generating reference trajectories: a reduced-order linear inverted pendulum (LIP) model for velocity-conditioned motion planning, and a precomputed gait library based on hybrid zero dynamics (HZD) using full-order dynamics. These planners define desired end-effector and joint trajectories, which are used to construct CLF-based rewards that penalize tracking error and encourage rapid convergence. This formulation provides meaningful intermediate rewards, and is straightforward to implement once a reference is available. Both the reference trajectories and CLF shaping are used only during training, resulting in a lightweight policy at deployment. We validate our method both in simulation and through extensive real-world experiments on a Unitree G1 robot. CLF-RL demonstrates significantly improved robustness relative to the baseline RL policy and better performance than a classic tracking reward RL formulation.', 'abstract_zh': '基于模型的轨迹生成和控制李雅普un夫函数的强化学习奖励塑造框架：提高双足机器人稳健性的方法', 'title_zh': 'CLF-RL: 控制李雅普诺夫函数引导的强化学习'}
{'arxiv_id': 'arXiv:2508.09346', 'title': 'How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy', 'authors': 'Zhenjiang Mao, Mrinall Eashaan Umasudhan, Ivan Ruchkin', 'link': 'https://arxiv.org/abs/2508.09346', 'abstract': 'Autonomous robots that rely on deep neural network controllers pose critical challenges for safety prediction, especially under partial observability and distribution shift. Traditional model-based verification techniques are limited in scalability and require access to low-dimensional state models, while model-free methods often lack reliability guarantees. This paper addresses these limitations by introducing a framework for calibrated safety prediction in end-to-end vision-controlled systems, where neither the state-transition model nor the observation model is accessible. Building on the foundation of world models, we leverage variational autoencoders and recurrent predictors to forecast future latent trajectories from raw image sequences and estimate the probability of satisfying safety properties. We distinguish between monolithic and composite prediction pipelines and introduce a calibration mechanism to quantify prediction confidence. In long-horizon predictions from high-dimensional observations, the forecasted inputs to the safety evaluator can deviate significantly from the training distribution due to compounding prediction errors and changing environmental conditions, leading to miscalibrated risk estimates. To address this, we incorporate unsupervised domain adaptation to ensure robustness of safety evaluation under distribution shift in predictions without requiring manual labels. Our formulation provides theoretical calibration guarantees and supports practical evaluation across long prediction horizons. Experimental results on three benchmarks show that our UDA-equipped evaluators maintain high accuracy and substantially lower false positive rates under distribution shift. Similarly, world model-based composite predictors outperform their monolithic counterparts on long-horizon tasks, and our conformal calibration provides reliable statistical bounds.', 'abstract_zh': '基于深度神经网络控制器的自主机器人在部分可观测性和分布偏移条件下的安全性预测校准框架', 'title_zh': '给定所见保障安全的概率：图像控制自主性的校准预测'}
{'arxiv_id': 'arXiv:2508.09304', 'title': 'Decision-Making-Based Path Planning for Autonomous UAVs: A Survey', 'authors': 'Kelen C. Teixeira Vivaldini, Robert Pěnička, Martin Saska', 'link': 'https://arxiv.org/abs/2508.09304', 'abstract': 'One of the most critical features for the successful operation of autonomous UAVs is the ability to make decisions based on the information acquired from their surroundings. Each UAV must be able to make decisions during the flight in order to deal with uncertainties in its system and the environment, and to further act upon the information being received. Such decisions influence the future behavior of the UAV, which is expressed as the path plan. Thus, decision-making in path planning is an enabling technique for deploying autonomous UAVs in real-world applications. This survey provides an overview of existing studies that use aspects of decision-making in path planning, presenting the research strands for Exploration Path Planning and Informative Path Planning, and focusing on characteristics of how data have been modeled and understood. Finally, we highlight the existing challenges for relevant topics in this field.', 'abstract_zh': '自主自主无人机成功运行的关键特征之一是基于从周围环境获取的信息作出决策的能力。在飞行过程中，每个无人机都必须能够根据其系统和环境中的不确定性进行决策，并进一步处理接收到的信息。这些决策影响无人机未来的行为，即路径规划。因此，路径规划中的决策制定是部署自主无人机在实际应用中的一项使能技术。本文综述了现有利用路径规划中决策制定方面进行研究的工作，概述了探索路径规划和信息性路径规划的研究方向，并集中探讨了数据建模与理解的特点。最后，本文指出了该领域相关主题所面临的现有挑战。', 'title_zh': '基于决策的自主无人机路径规划：一个综述'}
{'arxiv_id': 'arXiv:2508.09963', 'title': 'Online Safety under Multiple Constraints and Input Bounds using gatekeeper: Theory and Applications', 'authors': 'Devansh R. Agrawal, Dimitra Panagou', 'link': 'https://arxiv.org/abs/2508.09963', 'abstract': 'This letter presents an approach to guarantee online safety of a cyber-physical system under multiple state and input constraints. Our proposed framework, called gatekeeper, recursively guarantees the existence of an infinite-horizon trajectory that satisfies all constraints and system dynamics. Such trajectory is constructed using a backup controller, which we define formally in this paper. gatekeeper relies on a small number of verifiable assumptions, and is computationally efficient since it requires optimization over a single scalar variable. We make two primary contributions in this letter. (A) First, we develop the theory of gatekeeper: we derive a sub-optimality bound relative to a full nonlinear trajectory optimization problem, and show how this can be used in runtime to validate performance. This also informs the design of the backup controllers and sets. (B) Second, we demonstrate in detail an application of gatekeeper for multi-agent formation flight, where each Dubins agent must avoid multiple obstacles and weapons engagement zones, both of which are nonlinear, nonconvex constraints.', 'abstract_zh': '本信提出了一种在多重状态和输入约束下确保网络物理系统在线安全的方法。我们提出的框架称为gatekeeper，递归地保证存在一条满足所有约束和系统动力学的无限时域轨迹。该轨迹通过一个备份控制器构建，我们在这篇论文中对其进行了形式化定义。gatekeeper仅依赖少量可验证假设，并由于只需要对单一标量变量进行优化而具有计算效率。本文的主要贡献包括：(A) 我们发展了gatekeeper的理论：我们推导了相对于完整非线性轨迹优化问题的次优性界，并展示了如何在运行时利用此结果来验证性能。这也有助于指导备份控制器和集合的设计。(B) 我们详细展示了在多agent编队飞行应用中使用gatekeeper，其中每个Dubins agent必须避免多个障碍物和武器交战区，两者均为非线性和非凸约束。', 'title_zh': '基于守门人的在线安全：多约束和输入边界下的理论与应用'}
{'arxiv_id': 'arXiv:2508.09908', 'title': 'Collision-Free Bearing-Driven Formation Tracking for Euler-Lagrange Systems', 'authors': 'Haoshu Cheng, Martin Guay, Shimin Wang, Yunhong Che', 'link': 'https://arxiv.org/abs/2508.09908', 'abstract': "In this paper, we investigate the problem of tracking formations driven by bearings for heterogeneous Euler-Lagrange systems with parametric uncertainty in the presence of multiple moving leaders. To estimate the leaders' velocities and accelerations, we first design a distributed observer for the leader system, utilizing a bearing-based localization condition in place of the conventional connectivity assumption. This observer, coupled with an adaptive mechanism, enables the synthesis of a novel distributed control law that guides the formation towards the target formation, without requiring prior knowledge of the system parameters. Furthermore, we establish a sufficient condition, dependent on the initial formation configuration, that ensures collision avoidance throughout the formation evolution. The effectiveness of the proposed approach is demonstrated through a numerical example.", 'abstract_zh': '本文研究了具有参数不确定性且存在多个移动领航者情况下异构Euler-Lagrange系统基于轴承的形成跟踪问题。为估计领航者的速度和加速度，我们首先设计了一个分布式观测器，利用基于轴承的定位条件替代传统的连通性假设。该观测器与自适应机制相结合，使我们能够合成一种新的分布式控制律来引导形成趋向目标形成，而不需先验系统参数知识。此外，我们建立了依赖于初始形成配置的充分条件，确保在整个形成演化过程中避免碰撞。所提出方法的有效性通过数值例子得到了验证。', 'title_zh': '欧拉-拉格朗日系统无碰撞方位驱动构形跟踪'}
{'arxiv_id': 'arXiv:2508.09830', 'title': 'RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians', 'authors': 'Shenxing Wei, Jinxi Li, Yafei Yang, Siyuan Zhou, Bo Yang', 'link': 'https://arxiv.org/abs/2508.09830', 'abstract': 'In this paper, we present a generalizable method for 3D surface reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from RGB images. Unlike existing coordinate-based methods which are often computationally intensive when rendering explicit surfaces, our proposed method, named RayletDF, introduces a new technique called raylet distance field, which aims to directly predict surface points from query rays. Our pipeline consists of three key modules: a raylet feature extractor, a raylet distance field predictor, and a multi-raylet blender. These components work together to extract fine-grained local geometric features, predict raylet distances, and aggregate multiple predictions to reconstruct precise surface points. We extensively evaluate our method on multiple public real-world datasets, demonstrating superior performance in surface reconstruction from point clouds or 3D Gaussians. Most notably, our method achieves exceptional generalization ability, successfully recovering 3D surfaces in a single-forward pass across unseen datasets in testing.', 'abstract_zh': '本研究提出了一种通用方法，利用RGB图像生成的原始点云或预估的3D高斯模型进行3D表面重建。与现有的基于坐标的方法相比，我们的方法在渲染显式表面时计算效率更高，我们提出了一种名为RayletDF的新方法，引入了一种称为射线元距离场的技术，旨在直接从查询射线预测表面点。我们的管道由三个关键模块组成：射线元特征提取器、射线元距离场预测器和多射线元混合器。这些组件共同工作以提取精细的局部几何特征、预测射线元距离，并整合多个预测以重建精确的表面点。我们广泛评估了该方法在多个公开的实际数据集上的表现，展示了在从点云或3D高斯模型进行表面重建方面的优越性能。尤为显著的是，我们的方法具有出色的泛化能力，在测试中能够一次性在未见过的数据集上成功恢复3D表面。', 'title_zh': 'RayletDF：适用于点云或高斯分布的一般化3D表面重建的Raylet距离场'}
{'arxiv_id': 'arXiv:2508.09811', 'title': 'TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos', 'authors': 'Jinxi Li, Ziyang Song, Bo Yang', 'link': 'https://arxiv.org/abs/2508.09811', 'abstract': "In this paper, we aim to model 3D scene geometry, appearance, and physical information just from dynamic multi-view videos in the absence of any human labels. By leveraging physics-informed losses as soft constraints or integrating simple physics models into neural nets, existing works often fail to learn complex motion physics, or doing so requires additional labels such as object types or masks. We propose a new framework named TRACE to model the motion physics of complex dynamic 3D scenes. The key novelty of our method is that, by formulating each 3D point as a rigid particle with size and orientation in space, we directly learn a translation rotation dynamics system for each particle, explicitly estimating a complete set of physical parameters to govern the particle's motion over time. Extensive experiments on three existing dynamic datasets and one newly created challenging synthetic datasets demonstrate the extraordinary performance of our method over baselines in the task of future frame extrapolation. A nice property of our framework is that multiple objects or parts can be easily segmented just by clustering the learned physical parameters.", 'abstract_zh': '本文旨在仅从动态多视角视频中建模3D场景的几何、外观和物理信息，无需任何人体标签。通过利用物理启发式的损失作为软约束，或将简单的物理模型整合到神经网络中，现有工作往往难以学习复杂的运动物理现象，或者需要额外的标签如物体类型或掩码等。我们提出了一种新的框架TRACE，用于建模复杂动态3D场景的运动物理。我们方法的关键新颖之处在于，通过将每个3D点表示为具有大小和空间方向的刚性颗粒，我们直接学习每个颗粒的平移旋转动力学系统，明确估计一组物理参数以控制颗粒的运动。在三个现有动态数据集和一个新创建的具有挑战性的合成数据集上的大量实验表明，我们的方法在未来的帧外推任务中显著优于基线方法。我们框架的一个不错的特点是，通过聚类学习到的物理参数，可以轻松地分割多个物体或部分。', 'title_zh': 'TRACE: 从多视角视频中学习三维高斯物理动力学'}
{'arxiv_id': 'arXiv:2508.09732', 'title': 'Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System', 'authors': 'Romeo Valentin, Sydney M. Katz, Artur B. Carneiro, Don Walker, Mykel J. Kochenderfer', 'link': 'https://arxiv.org/abs/2508.09732', 'abstract': 'Recent advances in data-driven computer vision have enabled robust autonomous navigation capabilities for civil aviation, including automated landing and runway detection. However, ensuring that these systems meet the robustness and safety requirements for aviation applications remains a major challenge. In this work, we present a practical vision-based pipeline for aircraft pose estimation from runway images that represents a step toward the ability to certify these systems for use in safety-critical aviation applications. Our approach features three key innovations: (i) an efficient, flexible neural architecture based on a spatial Soft Argmax operator for probabilistic keypoint regression, supporting diverse vision backbones with real-time inference; (ii) a principled loss function producing calibrated predictive uncertainties, which are evaluated via sharpness and calibration metrics; and (iii) an adaptation of Residual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling runtime detection and rejection of faulty model outputs. We implement and evaluate our pose estimation pipeline on a dataset of runway images. We show that our model outperforms baseline architectures in terms of accuracy while also producing well-calibrated uncertainty estimates with sub-pixel precision that can be used downstream for fault detection.', 'abstract_zh': 'Recent Advances in Data-Driven Computer Vision for Robust Aircraft Pose Estimation from Runway Images: A Step Toward Certification for Safety-Critical Aviation Applications', 'title_zh': '基于实时计算机视觉的着陆系统运行时保证的预测不确定性'}
{'arxiv_id': 'arXiv:2508.09681', 'title': 'Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision', 'authors': 'Gerardo Loza, Junlei Hu, Dominic Jones, Sharib Ali, Pietro Valdastri', 'link': 'https://arxiv.org/abs/2508.09681', 'abstract': "We proposed a novel test-time optimisation (TTO) approach framed by a NeRF-based architecture for long-term 3D point tracking. Most current methods in point tracking struggle to obtain consistent motion or are limited to 2D motion. TTO approaches frame the solution for long-term tracking as optimising a function that aggregates correspondences from other specialised state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose parametrising such a function with our new invertible Neural Radiance Field (InvNeRF) architecture to perform both 2D and 3D tracking in surgical scenarios. Our approach allows us to exploit the advantages of a rendering-based approach by supervising the reprojection of pixel correspondences. It adapts strategies from recent rendering-based methods to obtain a bidirectional deformable-canonical mapping, to efficiently handle a defined workspace, and to guide the rays' density. It also presents our multi-scale HexPlanes for fast inference and a new algorithm for efficient pixel sampling and convergence criteria. We present results in the STIR and SCARE datasets, for evaluating point tracking and testing the integration of kinematic data in our pipeline, respectively. In 2D point tracking, our approach surpasses the precision and accuracy of the TTO state-of-the-art methods by nearly 50% on average precision, while competing with other approaches. In 3D point tracking, this is the first TTO approach, surpassing feed-forward methods while incorporating the benefits of a deformable NeRF-based reconstruction.", 'abstract_zh': '基于NeRF的长时3D点跟踪自优化方法', 'title_zh': 'Surg-InvNeRF: 可逆神经场在手术视觉中的3D跟踪与重建'}
{'arxiv_id': 'arXiv:2508.09625', 'title': 'Plane Detection and Ranking via Model Information Optimization', 'authors': 'Daoxin Zhong, Jun Li, Meng Yee Michael Chuah', 'link': 'https://arxiv.org/abs/2508.09625', 'abstract': 'Plane detection from depth images is a crucial subtask with broad robotic applications, often accomplished by iterative methods such as Random Sample Consensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic guarantees, the ambiguity of its inlier threshold criterion makes it susceptible to false positive plane detections. This issue is particularly prevalent in complex real-world scenes, where the true number of planes is unknown and multiple planes coexist. In this paper, we aim to address this limitation by proposing a generalised framework for plane detection based on model information optimization. Building on previous works, we treat the observed depth readings as discrete random variables, with their probability distributions constrained by the ground truth planes. Various models containing different candidate plane constraints are then generated through repeated random sub-sampling to explain our observations. By incorporating the physics and noise model of the depth sensor, we can calculate the information for each model, and the model with the least information is accepted as the most likely ground truth. This information optimization process serves as an objective mechanism for determining the true number of planes and preventing false positive detections. Additionally, the quality of each detected plane can be ranked by summing the information reduction of inlier points for each plane. We validate these properties through experiments with synthetic data and find that our algorithm estimates plane parameters more accurately compared to the default Open3D RANSAC plane segmentation. Furthermore, we accelerate our algorithm by partitioning the depth map using neural network segmentation, which enhances its ability to generate more realistic plane parameters in real-world data.', 'abstract_zh': '基于模型信息优化的平面检测框架', 'title_zh': '基于模型信息优化的平面检测与排序'}
{'arxiv_id': 'arXiv:2508.09560', 'title': 'WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization', 'authors': 'Jiahao Wen, Hang Yu, Zhedong Zheng', 'link': 'https://arxiv.org/abs/2508.09560', 'abstract': 'Visual geo-localization for drones faces critical degradation under weather perturbations, \\eg, rain and fog, where existing methods struggle with two inherent limitations: 1) Heavy reliance on limited weather categories that constrain generalization, and 2) Suboptimal disentanglement of entangled scene-weather features through pseudo weather categories. We present WeatherPrompt, a multi-modality learning paradigm that establishes weather-invariant representations through fusing the image embedding with the text context. Our framework introduces two key contributions: First, a Training-free Weather Reasoning mechanism that employs off-the-shelf large multi-modality models to synthesize multi-weather textual descriptions through human-like reasoning. It improves the scalability to unseen or complex weather, and could reflect different weather strength. Second, to better disentangle the scene and weather feature, we propose a multi-modality framework with the dynamic gating mechanism driven by the text embedding to adaptively reweight and fuse visual features across modalities. The framework is further optimized by the cross-modal objectives, including image-text contrastive learning and image-text matching, which maps the same scene with different weather conditions closer in the respresentation space. Extensive experiments validate that, under diverse weather conditions, our method achieves competitive recall rates compared to state-of-the-art drone geo-localization methods. Notably, it improves Recall@1 by +13.37\\% under night conditions and by 18.69\\% under fog and snow conditions.', 'abstract_zh': '无人机在遭遇天气干扰（例如雨、雾）时，基于视觉的地理定位面临关键性能下降问题。现有方法面临两大固有限制：1）对有限天气类别过度依赖，限制了泛化能力；2）通过伪天气类别难以有效分离场景与天气特征。为此，我们提出了WeatherPrompt，这是一种多模态学习范式，通过将图像嵌入与文本上下文融合来建立天气不变的表示。该框架包含两个关键贡献：首先，一种无需训练的天气推理机制，利用现成的大规模多模态模型通过类人推理生成多天气文本描述，提高对未见或复杂天气的适应性，并能反映不同的天气强度。其次，为更好地分离场景和天气特征，我们提出了一种多模态框架，该框架通过文本嵌入驱动的动态门控机制自适应调整并融合跨模态视图特征，并通过跨模态目标优化，包括图像-文本对比学习和图像-文本匹配，将具有不同天气条件的同一场景在表示空间中更接近。大量的实验验证表明，在各种天气条件下，我们的方法在与最先进的无人机地理定位方法的召回率上具有竞争力。特别是在夜间条件下，召回率@1提高了13.37%，在雾雪条件下提高了18.69%。', 'title_zh': 'WeatherPrompt：全气象条件无人机视觉地理定位的多模态表示学习'}
{'arxiv_id': 'arXiv:2508.09423', 'title': "Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation", 'authors': 'Badi Li, Ren-jie Lu, Yu Zhou, Jingke Meng, Wei-shi Zheng', 'link': 'https://arxiv.org/abs/2508.09423', 'abstract': 'The Object Goal Navigation (ObjectNav) task challenges agents to locate a specified object in an unseen environment by imagining unobserved regions of the scene. Prior approaches rely on deterministic and discriminative models to complete semantic maps, overlooking the inherent uncertainty in indoor layouts and limiting their ability to generalize to unseen environments. In this work, we propose GOAL, a generative flow-based framework that models the semantic distribution of indoor environments by bridging observed regions with LLM-enriched full-scene semantic maps. During training, spatial priors inferred from large language models (LLMs) are encoded as two-dimensional Gaussian fields and injected into target maps, distilling rich contextual knowledge into the flow model and enabling more generalizable completions. Extensive experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D and Gibson, and shows strong generalization in transfer settings to HM3D. Codes and pretrained models are available at this https URL.', 'abstract_zh': '基于对象目标导航（ObjectNav）任务要求智能体通过想象未观察区域来在未见过的环境中定位指定对象，提出了GOAL，一种生成流基框架，通过将观测区域与大型语言模型（LLM）增强的全场景语义图连接起来，建模室内环境的语义分布。在训练过程中，从大型语言模型推断的空间先验被编码为二维高斯场并注入目标地图中，将丰富的上下文知识融入流模型中，从而实现更具泛化性的完成。广泛实验表明，GOAL在MP3D和Gibson上达到了最先进的性能，并在转移到HM3D的设置中表现出强大的泛化能力。代码和预训练模型可在以下链接获取。', 'title_zh': '蒸馏大语言模型以供流模型前馈，用于对象目标导航中可迁移代理的想象能力'}
{'arxiv_id': 'arXiv:2508.09325', 'title': 'SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning', 'authors': 'Alexandre Brown, Glen Berseth', 'link': 'https://arxiv.org/abs/2508.09325', 'abstract': 'Visual reinforcement learning (RL) is challenging due to the need to learn both perception and actions from high-dimensional inputs and noisy rewards. Although large perception models exist, integrating them effectively into RL for visual generalization and improved sample efficiency remains unclear. We propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment Anything (SAM) for object-centric decomposition and YOLO-World to ground segments semantically via text prompts. It includes a novel transformer-based architecture that supports a dynamic number of segments at each time step and effectively learns which segments to focus on using online RL, without using human labels. By evaluating SegDAC over a challenging visual generalization benchmark using Maniskill3, which covers diverse manipulation tasks under strong visual perturbations, we demonstrate that SegDAC achieves significantly better visual generalization, doubling prior performance on the hardest setting and matching or surpassing prior methods in sample efficiency across all evaluated tasks.', 'abstract_zh': '基于分割的Actor- Critic方法：SegDAC在高维输入和噪声奖励下同时学习感知和动作的视觉强化学习挑战', 'title_zh': 'SegDAC: 基于分割的演员-批评家方法在视觉强化学习中的应用'}
{'arxiv_id': 'arXiv:2508.09233', 'title': 'Safety Perspective on Assisted Lane Changes: Insights from Open-Road, Live-Traffic Experiments', 'authors': 'Konstantinos Mattas, Sandor Vass, Gergely Zachar, Junyi Ji, Derek Gloudemans, Davide Maggi, Akos Kriston, Mohamed Brahmi, Maria Christina Galassi, Daniel B Work, Biagio Ciuffo', 'link': 'https://arxiv.org/abs/2508.09233', 'abstract': 'This study investigates the assisted lane change functionality of five different vehicles equipped with advanced driver assistance systems (ADAS). The goal is to examine novel, under-researched features of commercially available ADAS technologies. The experimental campaign, conducted in the I-24 highway near Nashville, TN, US, collected data on the kinematics and safety margins of assisted lane changes in real-world conditions. The results show that the kinematics of assisted lane changes are consistent for each system, with four out of five vehicles using slower speeds and decelerations than human drivers. However, one system consistently performed more assertive lane changes, completing the maneuver in around 5 seconds. Regarding safety margins, only three vehicles are investigated. Those operated in the US are not restricted by relevant UN regulations, and their designs were found not to adhere to these regulatory requirements. A simulation method used to classify the challenge level for the vehicle receiving the lane change, showing that these systems can force trailing vehicles to decelerate to keep a safe gap. One assisted system was found to have performed a maneuver that posed a hard challenge level for the other vehicle, raising concerns about the safety of these systems in real-world operation. All three vehicles were found to carry out lane changes that induced decelerations to the vehicle in the target lane. Those decelerations could affect traffic flow, inducing traffic shockwaves.', 'abstract_zh': '本研究调查了五种配备高级驾驶辅助系统（ADAS）的车辆的辅助变道功能。目标是研究商用ADAS技术中新颖且研究不足的特性。实验活动在美国田纳西州纳什维尔附近的I-24高速公路进行，收集了在实际条件下的辅助变道的动力学和安全裕度数据。结果显示，每种系统的辅助变道动力学是一致的，其中四辆汽车使用了比人类驾驶员更慢的速度和减速度。然而，有一种系统在变道时表现得更为积极，在大约5秒内完成了变道动作。关于安全裕度，只有三辆车进行了调查。它们在美国运营，不受相关UN法规的限制，并且设计未遵守这些法规要求。使用仿真方法对接受变道的车辆面临的挑战级别进行了分类，表明这些系统能够迫使跟随车辆减速以保持安全距离。发现一种辅助系统执行了一种对另一辆车构成高挑战级别的操作，这引发了对接收到变道指令的车辆安全性的担忧。所有三辆车在变道时都导致目标车道车辆减速，这种减速可能影响交通流量，引发交通波动。', 'title_zh': '辅助变道的安全视角：基于开放路实况交通实验的见解'}
