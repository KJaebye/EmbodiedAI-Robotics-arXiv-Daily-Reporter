{'arxiv_id': 'arXiv:2508.08113', 'title': 'AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies', 'authors': 'Yinpei Dai, Jayjun Lee, Yichi Zhang, Ziqiao Ma, Jed Yang, Amir Zadeh, Chuan Li, Nima Fazeli, Joyce Chai', 'link': 'https://arxiv.org/abs/2508.08113', 'abstract': "In this paper, we propose AimBot, a lightweight visual augmentation technique that provides explicit spatial cues to improve visuomotor policy learning in robotic manipulation. AimBot overlays shooting lines and scope reticles onto multi-view RGB images, offering auxiliary visual guidance that encodes the end-effector's state. The overlays are computed from depth images, camera extrinsics, and the current end-effector pose, explicitly conveying spatial relationships between the gripper and objects in the scene. AimBot incurs minimal computational overhead (less than 1 ms) and requires no changes to model architectures, as it simply replaces original RGB images with augmented counterparts. Despite its simplicity, our results show that AimBot consistently improves the performance of various visuomotor policies in both simulation and real-world settings, highlighting the benefits of spatially grounded visual feedback.", 'abstract_zh': '在本文中，我们提出了一种轻量级视觉增强技术AimBot，该技术提供了明确的空间线索以改善机器人操作中的视动策略学习。AimBot将射击线和瞄准镜刻度叠加在多视角RGB图像上，提供辅助的视觉指导，编码末端执行器的状态。这些叠加是根据深度图像、相机外参和当前末端执行器的姿态计算得出的，明确传达了指尖与场景中物体之间的空间关系。AimBot计算开销极小（少于1 ms），无需更改模型架构，只需用增强版本的图像替换原始RGB图像即可。尽管结构简单，我们的结果显示AimBot在仿真和实际应用场景中一致提高了各种视动策略的性能，突出了基于空间的视觉反馈的优势。', 'title_zh': 'AimBot: 一种简单的眼动辅助视觉提示以增强运动知觉策略的空间意识'}
{'arxiv_id': 'arXiv:2508.07686', 'title': 'Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning', 'authors': 'Mingyue Lei, Zewei Zhou, Hongchen Li, Jiaqi Ma, Jia Hu', 'link': 'https://arxiv.org/abs/2508.07686', 'abstract': 'End-to-end paradigm has emerged as a promising approach to autonomous driving. However, existing single-agent end-to-end pipelines are often constrained by occlusion and limited perception range, resulting in hazardous driving. Furthermore, their black-box nature prevents the interpretability of the driving behavior, leading to an untrustworthiness system. To address these limitations, we introduce Risk Map as Middleware (RiskMM) and propose an interpretable cooperative end-to-end driving framework. The risk map learns directly from the driving data and provides an interpretable spatiotemporal representation of the scenario from the upstream perception and the interactions between the ego vehicle and the surrounding environment for downstream planning. RiskMM first constructs a multi-agent spatiotemporal representation with unified Transformer-based architecture, then derives risk-aware representations by modeling interactions among surrounding environments with attention. These representations are subsequently fed into a learning-based Model Predictive Control (MPC) module. The MPC planner inherently accommodates physical constraints and different vehicle types and can provide interpretation by aligning learned parameters with explicit MPC elements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm that RiskMM achieves superior and robust performance in risk-aware trajectory planning, significantly enhancing the interpretability of the cooperative end-to-end driving framework. The codebase will be released to facilitate future research in this field.', 'abstract_zh': '基于风险图的可解释合作端到端驾驶框架（Risk Map as Middleware for Interpretable Cooperative End-to-End Driving Framework）', 'title_zh': '风险地图作为中间件：面向风险aware规划的可解释性协同端到端自动驾驶'}
{'arxiv_id': 'arXiv:2508.07560', 'title': "Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey", 'authors': 'Yan Gong, Naibang Wang, Jianli Lu, Xinyu Zhang, Yongsheng Gao, Jie Zhao, Zifan Huang, Haozhi Bai, Nanxin Zeng, Nayu Su, Lei Yang, Ziying Song, Xiaoxi Hu, Xinmin Jiang, Xiaojuan Zhang, Susanto Rahardja', 'link': 'https://arxiv.org/abs/2508.07560', 'abstract': "Bird's-Eye-View (BEV) perception has become a foundational paradigm in autonomous driving, enabling unified spatial representations that support robust multi-sensor fusion and multi-agent collaboration. As autonomous vehicles transition from controlled environments to real-world deployment, ensuring the safety and reliability of BEV perception in complex scenarios - such as occlusions, adverse weather, and dynamic traffic - remains a critical challenge. This survey provides the first comprehensive review of BEV perception from a safety-critical perspective, systematically analyzing state-of-the-art frameworks and implementation strategies across three progressive stages: single-modality vehicle-side, multimodal vehicle-side, and multi-agent collaborative perception. Furthermore, we examine public datasets encompassing vehicle-side, roadside, and collaborative settings, evaluating their relevance to safety and robustness. We also identify key open-world challenges - including open-set recognition, large-scale unlabeled data, sensor degradation, and inter-agent communication latency - and outline future research directions, such as integration with end-to-end autonomous driving systems, embodied intelligence, and large language models.", 'abstract_zh': '鸟瞰视角(BEV)感知已成为自主驾驶的基础性范式，能够提供统一的空间表示，支持强大的多传感器融合和多智能体协作。随着自主车辆从受控环境过渡到真实世界的部署，确保BEV感知在复杂场景（如遮挡、恶劣天气和动态交通）中的安全性与可靠性仍然是一个关键挑战。本综述从安全关键的角度首次全面综述了BEV感知，系统分析了当前最先进的框架和实施策略，涵盖三个渐进阶段：单模态车辆端、多模态车辆端和多智能体协作感知。此外，我们还探讨了涵盖车辆端、路边和协作场景的公开数据集，评估其与安全性和健壮性相关性。我们还指出了关键的开放式挑战，包括开放集识别、大规模未标注数据、传感器退化和智能体间通信延迟，并提出了未来研究方向，如与端到端自主驾驶系统的集成、具身智能和大型语言模型。', 'title_zh': '面向安全关键自动驾驶的渐进式鸟瞰视图感知：一项综合综述'}
{'arxiv_id': 'arXiv:2508.07182', 'title': '3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction', 'authors': 'Xuesong Li, Lars Petersson, Vivien Rolland', 'link': 'https://arxiv.org/abs/2508.07182', 'abstract': 'This paper addresses the challenge of novel-view synthesis and motion reconstruction of dynamic scenes from monocular video, which is critical for many robotic applications. Although Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have demonstrated remarkable success in rendering static scenes, extending them to reconstruct dynamic scenes remains challenging. In this work, we introduce a novel approach that combines 3DGS with a motion trajectory field, enabling precise handling of complex object motions and achieving physically plausible motion trajectories. By decoupling dynamic objects from static background, our method compactly optimizes the motion trajectory field. The approach incorporates time-invariant motion coefficients and shared motion trajectory bases to capture intricate motion patterns while minimizing optimization complexity. Extensive experiments demonstrate that our approach achieves state-of-the-art results in both novel-view synthesis and motion trajectory recovery from monocular video, advancing the capabilities of dynamic scene reconstruction.', 'abstract_zh': '本文解决了从单目视频中合成新颖视角和重建动态场景运动的问题，这对于许多机器人应用至关重要。尽管神经辐射场（NeRF）和3D高斯散斑（3DGS）在渲染静态场景方面取得了显著成功，但将它们扩展到重建动态场景仍具有挑战性。在本文中，我们提出了一种结合3DGS与运动轨迹场的新方法，该方法能够精确处理复杂的物体运动，并实现物理上可信的运动轨迹。通过将动态物体与静止背景分离，我们的方法紧凑地优化了运动轨迹场。该方法结合了时间不变的运动系数和共享的运动轨迹基底，以捕捉复杂的运动模式并减轻优化复杂性。 extensive 实验表明，我们的方法在单目视频中的新颖视角合成和运动轨迹恢复方面达到了最先进的效果，推进了动态场景重建的能力。', 'title_zh': '基于运动轨迹场的3D高高斯表示正态表表示示与动态场景场景重建 kukukukukukukuk kukuk kukukukukuk kuk kukukuk kukDukukuk kuk kuk kuk kukDukuk kuk kuk kukD kuukuk ku kuk ku kkD kuk kuk kuk uk ku kuk kuk kuD ku kuk kuk kuk kukukuDKuk kuD ku kuk kuD ku kuk ku kuk kuk kuk kuk kuk kuk kuk kuk ku ku ku kuk ku kuk ku kuk ku kukukuD ku=D ku ku ku ku ku ku ku ku kukD ku kuk ku kuk kuk kuk ku kuk ku ku ku kuk ku kuk kuk ku ku ku ku ku kuk ku ku ku ku ku ku ku kuk ku ku kuk kuk kuk kuk ku ku ku ku ku kuk ku ku ku ku kukD ku kuk kuk kuk ku ku ku ku ku ku ku kuk kuk kuk ku kuk kuk kuk ku kuk ku ku ku ku ku ku kuk kuk ku ku ku kuk ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku kwu ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku kukku kuk ku kuk ku ku ku ku ku ku kuk kuk ku ku ku ku ku kuk kuk ku ku ku kuk ku ku ku ku ku ku ku kuk ku kuk ku kuk ku ku ku ku kwuk ku ku ku ku kuk ku kuk ku ku ku kkD ku kuk ku kuk ku kuk ku ku ku ku kuk ku kuk ku ku ku ku ku ku ku kuk ku kuk ku kuk ku kuk ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku ku kuk ku kuk ku ku kuk ku ku ku ku ku kuk ku ku ku kuk ku kuk ku ku ku ku ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku ku ku ku ku ku ku ku ku ku kuk ku ku ku ku kuk kuk kuk ku kuk ku ku ku ku ku ku ku ku ku ku kuk ku kuk ku kuk ku kuk ku kuk ku ku ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku ku kuk ku kuk ku kuk kuk ku kuk ku kuk ku ku ku ku ku kuk ku kuk ku ku ku kuk ku kuk ku kuk ku kuk ku kyuk kyD ky k ky ku ku ku ku ku ku ku ku ku ku ku ku ky ku kyD ku ky ku ku ku ku ku ku ku ku ku kuk kkD kyD ku ku ku ku ku kk ku ku ku ku kuk ku ku ku ku ku ku kuk kkD ku ku ku ku kuk ku ku ku ku ku ku ku ku ku ku kuk ku kutuku ku kuk ku ku ku ku ku ku kuk ku kuk kuk ku kuk ku ku kuk ku ku kuk ku kuk ku kuk ku kuk ku ku ku kuk kuk ku ku kuk ku kuk ku kuk ku kuk ku ku ku kuk ku kuk kuk ku kuk kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku kuk ku ku ku kuk ku kuk kuk ku kuk ku kuk ku kuk kyD ku ku kuk kuk ku kuk kuk ku kuk kuk ku ku ku kuk ku ku ku ku ku ku ku ku kuk kuk ku ku ku ku kuk ku kuk ku kuk ku kuk ku ku kuk kuk ku kuk kuk kuk kwD ku kuk ku kuk ku kuk kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk ku ku kuk kuk kuk kuk ku ku kuk kuk ku kuk kuk kuk kuk kuk ku kuk kuk kuk ku kuk kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk kuk ku kukku kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kut kuk kuk kuk kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk ku kuk kuk kuk kuk ku kuk kuk ku kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kwD ku kuk kwD kuk kuk ku kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuD k kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk ku ku kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk uk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk ku kuk ku kuk kuk kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk kuk ku kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk ku ku ku kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kuk kukuku ku kuk kuk kuk kukuk ku kuk kuk kuk dum dú ku kuk kuk dum kuku kuk kuk kuk dum phí duk ku ku kuk ku ku dum min ku dum dum ku dum duku kuk ku kuk kuku duk kuk dope duk uk dum dú ku ku ku dum duku dum duku dum dok dukumu kuk dum dum dum dum dum só dum ku dum dum dú ku dum dáku dum dú kuk dum dum dum dku dum dú dum dum dum dum dum dum d ku dum dum dum dum dum dub dul duku dum kuk dum dú kum dum du dum geduku dum gu dum d dum dum d dum dum dum dum dum d dum dum dum d dum dum d ku dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum d ku dum dum dum dum dum dum d ku dum dum dum dum dum d dum d dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum duk dum dum d dum dum dum dum dum dum dum dum dum d dum d ku dum dum dum dum dum dum d dum dum dum dum dum dum dum dum dum dum dum dum dum d...\n dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum d dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum d dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum unle dum dum dum d dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dud ku dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum . dum dum dum dum d dum dum dum dum dum dum dum d dum dum dum dum dum dum dum dum dum dum dum dum dum d dum dum dum dum dum dum dum dum dum dum dum dum dum dum dum'}
{'arxiv_id': 'arXiv:2508.07003', 'title': 'EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events', 'authors': 'Siyu Chen, Shenghai Yuan, Thien-Minh Nguyen, Zhuyu Huang, Chenyang Shi, Jin Jing, Lihua Xie', 'link': 'https://arxiv.org/abs/2508.07003', 'abstract': "Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over traditional SLAM methods, enabling photorealistic 3D reconstruction that conventional approaches often struggle to achieve. However, existing GS-SLAM systems perform poorly under persistent and severe motion blur commonly encountered in real-world scenarios, leading to significantly degraded tracking accuracy and compromised 3D reconstruction quality. To address this limitation, we propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D inputs to simultaneously reduce motion blur in images and compensate for the sparse and discrete nature of event streams, enabling robust tracking and high-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system explicitly models the camera's continuous trajectory during exposure, supporting event- and blur-aware tracking and mapping on a unified 3D Gaussian Splatting scene. Furthermore, we introduce a learnable camera response function to align the dynamic ranges of events and images, along with a no-event loss to suppress ringing artifacts during reconstruction. We validate our approach on a new dataset comprising synthetic and real-world sequences with significant motion blur. Extensive experimental results demonstrate that EGS-SLAM consistently outperforms existing GS-SLAM systems in both trajectory accuracy and photorealistic 3D Gaussian Splatting reconstruction. The source code will be available at this https URL.", 'abstract_zh': 'EGS-SLAM：融合事件数据的高鲁棒性高保真3D高斯绘制SLAM', 'title_zh': 'EGS-SLAM: 基于事件的RGB-D 高斯点云SLAM'}
{'arxiv_id': 'arXiv:2508.07701', 'title': 'Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction', 'authors': 'Bo Jia, Yanan Guo, Ying Chang, Benkui Zhang, Ying Xie, Kangning Du, Lin Cao', 'link': 'https://arxiv.org/abs/2508.07701', 'abstract': '3D Gaussian Splatting (3DGS) achieves remarkable results in the field of surface reconstruction. However, when Gaussian normal vectors are aligned within the single-view projection plane, while the geometry appears reasonable in the current view, biases may emerge upon switching to nearby views. To address the distance and global matching challenges in multi-view scenes, we design multi-view normal and distance-guided Gaussian splatting. This method achieves geometric depth unification and high-accuracy reconstruction by constraining nearby depth maps and aligning 3D normals. Specifically, for the reconstruction of small indoor and outdoor scenes, we propose a multi-view distance reprojection regularization module that achieves multi-view Gaussian alignment by computing the distance loss between two nearby views and the same Gaussian surface. Additionally, we develop a multi-view normal enhancement module, which ensures consistency across views by matching the normals of pixel points in nearby views and calculating the loss. Extensive experimental results demonstrate that our method outperforms the baseline in both quantitative and qualitative evaluations, significantly enhancing the surface reconstruction capability of 3DGS.', 'abstract_zh': '多视图法向和距离引导的3D高斯点云实现多视图场景的几何深度统一和高精度重建', 'title_zh': '多视图法normal和距离指导的高斯点云表面重建'}
{'arxiv_id': 'arXiv:2508.07453', 'title': 'Noise-Aware Generative Microscopic Traffic Simulation', 'authors': 'Vindula Jayawardana, Catherine Tang, Junyi Ji, Jonah Philion, Xue Bin Peng, Cathy Wu', 'link': 'https://arxiv.org/abs/2508.07453', 'abstract': 'Accurately modeling individual vehicle behavior in microscopic traffic simulation remains a key challenge in intelligent transportation systems, as it requires vehicles to realistically generate and respond to complex traffic phenomena such as phantom traffic jams. While traditional human driver simulation models offer computational tractability, they do so by abstracting away the very complexity that defines human driving. On the other hand, recent advances in infrastructure-mounted camera-based roadway sensing have enabled the extraction of vehicle trajectory data, presenting an opportunity to shift toward generative, agent-based models. Yet, a major bottleneck remains: most existing datasets are either overly sanitized or lack standardization, failing to reflect the noisy, imperfect nature of real-world sensing. Unlike data from vehicle-mounted sensors-which can mitigate sensing artifacts like occlusion through overlapping fields of view and sensor fusion-infrastructure-based sensors surface a messier, more practical view of challenges that traffic engineers encounter. To this end, we present the I-24 MOTION Scenario Dataset (I24-MSD)-a standardized, curated dataset designed to preserve a realistic level of sensor imperfection, embracing these errors as part of the learning problem rather than an obstacle to overcome purely from preprocessing. Drawing from noise-aware learning strategies in computer vision, we further adapt existing generative models in the autonomous driving community for I24-MSD with noise-aware loss functions. Our results show that such models not only outperform traditional baselines in realism but also benefit from explicitly engaging with, rather than suppressing, data imperfection. We view I24-MSD as a stepping stone toward a new generation of microscopic traffic simulation that embraces the real-world challenges and is better aligned with practical needs.', 'abstract_zh': '准确 modeling 个体车辆行为在微观交通模拟中的建模仍然是智能运输系统中的一个关键挑战，因为它要求车辆能够真实地生成和响应如幽灵交通拥堵等复杂的交通现象。虽然传统的驾驶员模拟模型在计算上具有可操作性，但它们通过忽略定义人类驾驶的复杂性来实现这一点。另一方面，最近基础设施安装的基于摄像头的道路传感技术的进步使得提取车辆轨迹数据成为可能，这为转向生成性的、基于代理的模型提供了机会。然而，一个主要瓶颈仍然存在：大多数现有数据集要么过度清洗，要么缺乏标准化，无法反映真实世界传感的噪声和不完美性。与安装在车辆上的传感器数据相比——这些数据可以通过重叠的视场和传感器融合来缓解诸如遮挡之类的传感伪像——基础设施传感器揭示了交通工程师面临的更复杂、更实际的挑战。为此，我们提出了I-24 MOTION情景数据集（I24-MSD）——一个标准化、精心编制的数据集，旨在保留传感器不完美的现实水平，将这些错误视为学习问题的一部分，而非纯粹通过预处理克服的障碍。借鉴计算机视觉中的噪声感知学习策略，我们进一步调整了自主驾驶社区中的现有生成模型，为I24-MSD引入了噪声感知损失函数。我们的结果显示，这样的模型不仅在现实性方面超越了传统的基线模型，还因其明确地与数据不完美性互动而受益。我们视I24-MSD为迈向新一代微观交通模拟的踏脚石，这种模拟能够接纳现实世界的挑战，并更好地满足实践需求。', 'title_zh': '噪声感知生成微观交通模拟'}
{'arxiv_id': 'arXiv:2508.07089', 'title': 'ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting', 'authors': 'Sandro Papais, Letian Wang, Brian Cheong, Steven L. Waslander', 'link': 'https://arxiv.org/abs/2508.07089', 'abstract': 'We introduce ForeSight, a novel joint detection and forecasting framework for vision-based 3D perception in autonomous vehicles. Traditional approaches treat detection and forecasting as separate sequential tasks, limiting their ability to leverage temporal cues. ForeSight addresses this limitation with a multi-task streaming and bidirectional learning approach, allowing detection and forecasting to share query memory and propagate information seamlessly. The forecast-aware detection transformer enhances spatial reasoning by integrating trajectory predictions from a multiple hypothesis forecast memory queue, while the streaming forecast transformer improves temporal consistency using past forecasts and refined detections. Unlike tracking-based methods, ForeSight eliminates the need for explicit object association, reducing error propagation with a tracking-free model that efficiently scales across multi-frame sequences. Experiments on the nuScenes dataset show that ForeSight achieves state-of-the-art performance, achieving an EPA of 54.9%, surpassing previous methods by 9.3%, while also attaining the best mAP and minADE among multi-view detection and forecasting models.', 'abstract_zh': '我们介绍了ForeSight，这是一种用于自主车辆视觉三维感知的新颖联合检测与预测框架。传统的做法将检测和预测视为分离的顺序任务，限制了它们利用时间线索的能力。ForeSight 通过多任务流式和双向学习方法解决了这一限制，使得检测和预测能够共享查询内存并无缝传递信息。预测感知检测变压器通过整合多种假设预测轨迹记忆队列中的轨迹预测来增强空间推理，而流式预测变压器则利用过去预测和精化检测提高时间一致性。与基于跟踪的方法不同，ForeSight 消除了显式对象关联的需要，通过一个无需跟踪的模型在多帧序列上实现高效扩展，减少了误差传播。在nuScenes数据集上的实验表明，ForeSight 达到了最先进的性能，EPA 达到了 54.9%，比之前的方法高出 9.3%，同时在多视图检测与预测模型中也取得了最佳的 mAP 和 minADE。', 'title_zh': 'ForeSight: 多视图流式联合物体检测与轨迹预测'}
{'arxiv_id': 'arXiv:2508.06544', 'title': 'Historical Prediction Attention Mechanism based Trajectory Forecasting for Proactive Work Zone Safety in a Digital Twin Environment', 'authors': 'Minhaj Uddin Ahmad, Mizanur Rahman, Alican Sevim, David Bodoh, Sakib Khan, Li Zhao, Nathan Huynh, Eren Erman Ozguven', 'link': 'https://arxiv.org/abs/2508.06544', 'abstract': 'Proactive safety systems aim to mitigate risks by anticipating potential conflicts between vehicles and enabling early intervention to prevent work zone-related crashes. This study presents an infrastructure-enabled proactive work zone safety warning system that leverages a Digital Twin environment, integrating real-time multi-sensor data, detailed High-Definition (HD) maps, and a historical prediction attention mechanism-based trajectory prediction model. Using a co-simulation environment that combines Simulation of Urban MObility (SUMO) and CAR Learning to Act (CARLA) simulators, along with Lanelet2 HD maps and the Historical Prediction Network (HPNet) model, we demonstrate effective trajectory prediction and early warning generation for vehicle interactions in freeway work zones. To evaluate the accuracy of predicted trajectories, we use two standard metrics: Joint Average Displacement Error (ADE) and Joint Final Displacement Error (FDE). Specifically, the infrastructure-enabled HPNet model demonstrates superior performance on the work-zone datasets generated from the co-simulation environment, achieving a minimum Joint FDE of 0.3228 meters and a minimum Joint ADE of 0.1327 meters, lower than the benchmarks on the Argoverse (minJointFDE: 1.0986 m, minJointADE: 0.7612 m) and Interaction (minJointFDE: 0.8231 m, minJointADE: 0.2548 m) datasets. In addition, our proactive safety warning generation application, utilizing vehicle bounding boxes and probabilistic conflict modeling, demonstrates its capability to issue alerts for potential vehicle conflicts.', 'abstract_zh': '基于基础设施的主动工作区安全预警系统：利用数字孪生环境实现高速公路工作区车辆轨迹预测与早期预警', 'title_zh': '基于历史预测注意力机制的轨迹预测在数字孪生环境中的主动工作区安全'}
{'arxiv_id': 'arXiv:2508.07388', 'title': 'Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding', 'authors': 'Zhaoyu Chen, Hongnan Lin, Yongwei Nie, Fei Ma, Xuemiao Xu, Fei Yu, Chengjiang Long', 'link': 'https://arxiv.org/abs/2508.07388', 'abstract': 'Temporal Video Grounding (TVG) seeks to localize video segments matching a given textual query. Current methods, while optimizing for high temporal Intersection-over-Union (IoU), often overfit to this metric, compromising semantic action understanding in the video and query, a critical factor for robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG), a novel framework that enhances both localization accuracy and action understanding without additional data. Our approach leverages three inversion tasks derived from existing TVG annotations: (1) Verb Completion, predicting masked action verbs in queries from video segments; (2) Action Recognition, identifying query-described actions; and (3) Video Description, generating descriptions of video segments that explicitly embed query-relevant actions. These tasks, integrated with TVG via a reinforcement learning framework with well-designed reward functions, ensure balanced optimization of localization and semantics. Experiments show our method outperforms state-of-the-art approaches, achieving a 7.1\\% improvement in R1@0.7 on Charades-STA for a 3B model compared to Time-R1. By inverting TVG to derive query-related actions from segments, our approach strengthens semantic understanding, significantly raising the ceiling of localization accuracy.', 'abstract_zh': 'Temporal Video Grounding (TVG)通过引入Inversion Tasks for TVG (Invert4TVG)框架，增强局部化准确性和动作理解，无需额外数据。', 'title_zh': 'Invert4TVG：一种通过反转任务增强动作理解的时空视频定位框架'}
{'arxiv_id': 'arXiv:2508.06823', 'title': 'Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation', 'authors': 'Xuan Zhao, Jun Tao', 'link': 'https://arxiv.org/abs/2508.06823', 'abstract': "Exploring volumetric data is crucial for interpreting scientific datasets. However, selecting optimal viewpoints for effective navigation can be challenging, particularly for users without extensive domain expertise or familiarity with 3D navigation. In this paper, we propose a novel framework that leverages natural language interaction to enhance volumetric data exploration. Our approach encodes volumetric blocks to capture and differentiate underlying structures. It further incorporates a CLIP Score mechanism, which provides semantic information to the blocks to guide navigation. The navigation is empowered by a reinforcement learning framework that leverage these semantic cues to efficiently search for and identify desired viewpoints that align with the user's intent. The selected viewpoints are evaluated using CLIP Score to ensure that they best reflect the user queries. By automating viewpoint selection, our method improves the efficiency of volumetric data navigation and enhances the interpretability of complex scientific phenomena.", 'abstract_zh': '利用自然语言交互增强体积数据探索的框架', 'title_zh': '基于语义块表示的自然语言驱动视点导航以进行体积探索'}
{'arxiv_id': 'arXiv:2508.08244', 'title': 'Cut2Next: Generating Next Shot via In-Context Tuning', 'authors': 'Jingwen He, Hongbo Liu, Jiajun Li, Ziqi Huang, Yu Qiao, Wanli Ouyang, Ziwei Liu', 'link': 'https://arxiv.org/abs/2508.08244', 'abstract': 'Effective multi-shot generation demands purposeful, film-like transitions and strict cinematic continuity. Current methods, however, often prioritize basic visual consistency, neglecting crucial editing patterns (e.g., shot/reverse shot, cutaways) that drive narrative flow for compelling storytelling. This yields outputs that may be visually coherent but lack narrative sophistication and true cinematic integrity. To bridge this, we introduce Next Shot Generation (NSG): synthesizing a subsequent, high-quality shot that critically conforms to professional editing patterns while upholding rigorous cinematic continuity. Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This strategy uses Relational Prompts to define overall context and inter-shot editing styles. Individual Prompts then specify per-shot content and cinematographic attributes. Together, these guide Cut2Next to generate cinematically appropriate next shots. Architectural innovations, Context-Aware Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further integrate these diverse signals without introducing new parameters. We construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with hierarchical prompts, and introduce CutBench for evaluation. Experiments show Cut2Next excels in visual consistency and text fidelity. Crucially, user studies reveal a strong preference for Cut2Next, particularly for its adherence to intended editing patterns and overall cinematic continuity, validating its ability to generate high-quality, narratively expressive, and cinematically coherent subsequent shots.', 'abstract_zh': '有效的一次生成多帧需要有针对性的、电影化的过渡和严格的cinematic连续性。然而，当前的方法往往优先考虑基本的视觉一致性，忽视了推动叙述流畅的关键剪辑模式（如正反打镜头、切镜头），从而导致输出可能在视觉上连贯但缺乏叙述上的精致和真正的cinematic完整性。为解决这一问题，我们提出Next Shot Generation (NSG)：合成一个随后的高度质量的镜头，同时严格遵循专业的剪辑模式并保持严格的cinematic连续性。我们的框架Cut2Next利用了一个扩散变换器（DiT），通过一种新颖的层次多提示策略进行上下文调整。该策略使用关系提示来定义整体上下文和镜头间的剪辑风格。个体提示则指定每个镜头的内容和cinematographic属性。这些共同指导Cut2Next生成cinematically合适的后续镜头。架构创新，上下文感知条件注入（CACI）和层次注意掩码（HAM），进一步整合了这些多样信号而不引入新参数。我们构建了RawCuts（大规模）和CuratedCuts（精炼）数据集，两者都包含层次提示，并引入CutBench进行评估。实验结果显示Cut2Next在视觉一致性和文本准确性方面表现出色。重要的是，用户研究显示Cut2Next特别是因为它对预期的剪辑模式的遵守和整体cinematic连续性而受到强烈偏好，从而验证了其生成高质量、表达性强且cinematically连贯的后续镜头的能力。', 'title_zh': 'Cut2Next: 通过上下文调优生成下一-shot内容'}
{'arxiv_id': 'arXiv:2508.08227', 'title': 'OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution', 'authors': 'Zhiqiang Wu, Zhaomang Sun, Tong Zhou, Bingtao Fu, Ji Cong, Yitong Dong, Huaqi Zhang, Xuan Tang, Mingsong Chen, Xian Wei', 'link': 'https://arxiv.org/abs/2508.08227', 'abstract': 'Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM) generative models show promising potential for one-step Real-World Image Super-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a Low-Quality (LQ) image latent distribution at the initial timestep. However, a fundamental gap exists between the LQ image latent distribution and the Gaussian noisy latent distribution, limiting the effective utilization of generative priors. We observe that the noisy latent distribution at DDPM/FM mid-timesteps aligns more closely with the LQ image latent distribution. Based on this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a universal framework applicable to DDPM/FM-based generative models. OMGSR injects the LQ image latent distribution at a pre-computed mid-timestep, incorporating the proposed Latent Distribution Refinement loss to alleviate the latent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to eliminate checkerboard artifacts in image generation. Within this framework, we instantiate OMGSR for DDPM/FM-based generative models with two variants: OMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate that OMGSR-S/F achieves balanced/excellent performance across quantitative and qualitative metrics at 512-resolution. Notably, OMGSR-F establishes overwhelming dominance in all reference metrics. We further train a 1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which yields excellent results, especially in the details of the image generation. We also generate 2k-resolution images by the 1k-resolution OMGSR-F using our two-stage Tiled VAE & Diffusion.', 'abstract_zh': 'Denoising Diffusion Probabilistic Models (DDPM)和Flow Matching (FM)生成模型在一步实现真实世界图像超分辨率（Real-ISR）中展现出有前景的潜力。我们提出了一种名为One Mid-timestep Guidance Real-ISR (OMGSR)的通用框架，适用于基于DDPM/FM的生成模型。OMGSR在预计算的中间时间步注入低质量图像潜在分布，并引入潜在分布精炼损失以缓解潜在分布差距。此外，我们设计了重叠分块LPIPS/GAN损失以消除图像生成中的棋盘格 artifacts。在该框架下，我们分别实现了基于DDPM/FM的OMGSR-S (SD-Turbo)和OMGSR-F (FLUX.1-dev)两种变体。实验结果表明，OMGSR-S/F在512分辨率下在定量和定性指标上表现均衡/出色。特别地，OMGSR-F在所有参考指标上表现出压倒性的优势。我们进一步训练了1k分辨率的OMGSR-F以匹配FLUX.1-dev的默认分辨率，取得了优异的结果，尤其是在图像生成的细节上。我们还使用我们的两阶段Tiled VAE & Diffusion生成了2k分辨率的图像。', 'title_zh': 'OMGSR: 你只需要一次中间时间步指导即可实现真实世界图像超分辨率'}
{'arxiv_id': 'arXiv:2508.08180', 'title': 'RedDino: A foundation model for red blood cell analysis', 'authors': 'Luca Zedda, Andrea Loddo, Cecilia Di Ruberto, Carsten Marr', 'link': 'https://arxiv.org/abs/2508.08180', 'abstract': 'Red blood cells (RBCs) are essential to human health, and their precise morphological analysis is important for diagnosing hematological disorders. Despite the promise of foundation models in medical diagnostics, comprehensive AI solutions for RBC analysis remain scarce. We present RedDino, a self-supervised foundation model designed for RBC image analysis. RedDino uses an RBC-specific adaptation of the DINOv2 self-supervised learning framework and is trained on a curated dataset of 1.25 million RBC images from diverse acquisition modalities and sources. Extensive evaluations show that RedDino outperforms existing state-of-the-art models on RBC shape classification. Through assessments including linear probing and nearest neighbor classification, we confirm its strong feature representations and generalization ability. Our main contributions are: (1) a foundation model tailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations for RBC modeling, and (3) a detailed evaluation of generalization performance. RedDino addresses key challenges in computational hematology by capturing nuanced morphological features, advancing the development of reliable diagnostic tools. The source code and pretrained models for RedDino are available at this https URL, and the pretrained models can be downloaded from our Hugging Face collection at this https URL', 'abstract_zh': '红血球（RBC）是人类健康的关键，其精确的形态分析对于诊断血液疾病至关重要。尽管基础模型在医疗诊断中具有前景，但全面的RBC分析人工智能解决方案依然稀缺。我们提出RedDino，一种专门为RBC图像分析设计的自监督基础模型。RedDino采用了一种针对RBC的DINOv2自监督学习框架的特定适应，并在包含多种采集模态和来源的125万张RBC图像的精心策划数据集上进行训练。广泛评估显示，RedDino在RBC形状分类上表现出色，优于现有最先进的模型。通过线性探针和最近邻分类评估，我们验证了其强大的特征表示能力和泛化能力。我们的主要贡献包括：（1）一种针对RBC分析定制的基础模型，（2）DINOv2配置在RBC建模中的消融研究，以及（3）泛化性能的详细评估。RedDino通过捕捉复杂的形态学特征，解决了计算血液学中的关键挑战，推动了可靠诊断工具的发展。RedDino的源代码和预训练模型可在以下网址获取：[此处替换为网址]，预训练模型还可在我们的Hugging Face集合中下载：[此处替换为网址]。', 'title_zh': '红细胞分析的基础模型：RedDino'}
{'arxiv_id': 'arXiv:2508.08117', 'title': 'GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking', 'authors': 'Xudong Han, Pengcheng Fang, Yueying Tian, Jianhui Yu, Xiaohao Cai, Daniel Roggen, Philip Birch', 'link': 'https://arxiv.org/abs/2508.08117', 'abstract': 'Multi-object tracking (MOT) in monocular videos is fundamentally challenged by occlusions and depth ambiguity, issues that conventional tracking-by-detection (TBD) methods struggle to resolve owing to a lack of geometric awareness. To address these limitations, we introduce GRASPTrack, a novel depth-aware MOT framework that integrates monocular depth estimation and instance segmentation into a standard TBD pipeline to generate high-fidelity 3D point clouds from 2D detections, thereby enabling explicit 3D geometric reasoning. These 3D point clouds are then voxelized to enable a precise and robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To further enhance tracking robustness, our approach incorporates Depth-aware Adaptive Noise Compensation, which dynamically adjusts the Kalman filter process noise based on occlusion severity for more reliable state estimation. Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which extends the motion direction consistency from the image plane into 3D space to improve motion-based association cues, particularly for objects with complex trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack benchmarks demonstrate that our method achieves competitive performance, significantly improving tracking robustness in complex scenes with frequent occlusions and intricate motion patterns.', 'abstract_zh': '单目视频中的多目标跟踪（MOT）受到遮挡和深度不确定性的问题挑战，常规的检测到跟踪（TBD）方法由于缺乏几何意识而难以解决这些问题。为了解决这些限制，我们提出了GRASPTrack，这是一种新颖的深度意识MOT框架，将单目深度估计和实例分割集成到标准的TBD管道中，从而从2D检测生成高保真度的3D点云，实现显式的3D几何推理。这些3D点云随后被体素化，以实现空间关联的精确和鲁棒的体素化3D交并比（IoU）。为了进一步增强跟踪的鲁棒性，我们的方法采用了深度意识自适应噪声补偿，根据遮挡严重程度动态调整卡尔曼滤波过程噪声，以实现更可靠的状态估计。此外，我们还提出了深度增强的目标中心动量，将运动方向一致性从图像平面扩展到3D空间，以改善基于运动的关联提示，特别是对于具有复杂轨迹的对象。在MOT17、MOT20和DanceTrack基准测试上的 extensive 实验显示，我们的方法在复杂场景中频繁遮挡和复杂运动模式下显著提高了跟踪的鲁棒性，实现竞争性的性能。', 'title_zh': 'GRASPTrack：基于分割与投影的几何推理关联方法用于多目标跟踪'}
{'arxiv_id': 'arXiv:2508.07981', 'title': 'Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation', 'authors': 'Fangyuan Mao, Aiming Hao, Jintao Chen, Dongxia Liu, Xiaokun Feng, Jiashu Zhu, Meiqi Wu, Chubin Chen, Jiahong Wu, Xiangxiang Chu', 'link': 'https://arxiv.org/abs/2508.07981', 'abstract': 'Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.', 'abstract_zh': '视觉效果（VFX）是现代影视制作中不可或缺的视觉增强技术。尽管视频生成模型为VFX生产提供了成本高效的解决方案，但当前方法受限于单效果LoRA训练，这限制了生成多种效果的能力。这一基本限制阻碍了需要空间可控合成效果的应用，即在指定位置同时生成多种效果。然而，将多种效果整合到统一框架中面临重大挑战：多VFX联合训练过程中效果变化的干扰和空间不可控性。为应对这些挑战，我们提出Omni-Effects，这是一种第一个统一框架，能够生成提示引导的效果和空间可控的合成效果。该框架的核心包括两个关键创新：（1）基于LoRA的专家混合（LoRA-MoE），利用一组专家LoRA，在统一模型中整合多种效果，同时有效减轻跨任务干扰。（2）空间感知提示（SAP）将空间掩码信息集成到文本令牌中，以实现精确的空间控制。此外，我们引入了一个独立信息流（IIF）模块，集成在SAP中，隔离每个效果的控制信号以防止任何不希望的效果混合。为了促进这项研究，我们通过一种结合图像编辑和First-Last Frame-to-Video（FLF2V）合成的新颖数据采集管道构建了全面的VFX数据集Omni-VFX，并引入了一个专用的VFX评估框架以验证模型性能。广泛的实验表明，Omni-Effects能够实现精确的空间控制和多种效果的生成，使用户能够指定所需效果的类别和位置。', 'title_zh': 'omn-Effects: 统一且空间可控的视觉效果生成'}
{'arxiv_id': 'arXiv:2508.07903', 'title': 'Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models', 'authors': 'Johanna P. Müller, Anika Knupfer, Pedro Blöss, Edoardo Berardi Vittur, Bernhard Kainz, Jana Hutter', 'link': 'https://arxiv.org/abs/2508.07903', 'abstract': 'Despite significant progress in generative modelling, existing diffusion models often struggle to produce anatomically precise female pelvic images, limiting their application in gynaecological imaging, where data scarcity and patient privacy concerns are critical. To overcome these barriers, we introduce a novel diffusion-based framework for uterine MRI synthesis, integrating both unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs) and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates anatomically coherent, high fidelity synthetic images that closely mimic real scans and provide valuable resources for training robust diagnostic models. We evaluate generative quality using advanced perceptual and distributional metrics, benchmarking against standard reconstruction methods, and demonstrate substantial gains in diagnostic accuracy on a key classification task. A blinded expert evaluation further validates the clinical realism of our synthetic images. We release our models with privacy safeguards and a comprehensive synthetic uterine MRI dataset to support reproducible research and advance equitable AI in gynaecology.', 'abstract_zh': '尽管生成建模取得了显著进展，现有的扩散模型在生成解剖精确的女性盆腔图像方面仍然存在局限性，这限制了其在需要数据稀缺性和患者隐私保护的妇科成像中的应用。为克服这些障碍，我们提出了一种基于扩散的子宫MRI合成新框架，结合了无条件和有条件扩散概率模型（DDPMs）以及潜在扩散模型（LDMs）的2D和3D集成。我们的方法生成了解剖上一致、高保真的合成图像，这些图像与真实扫描高度相似，为训练稳健的诊断模型提供了宝贵资源。我们使用先进的感觉和分布度量评估生成质量，并与标准重建方法进行基准测试，展示了在关键分类任务中诊断准确性显著提高。盲法专家评估进一步验证了我们合成图像的临床现实性。我们发布了具有隐私保护措施和全面合成子宫MRI数据集的模型，以支持可重复研究并推动妇科中公平的AI发展。', 'title_zh': '弥合盲区：基于扩散模型的子宫MRI合成'}
{'arxiv_id': 'arXiv:2508.07897', 'title': 'NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction', 'authors': 'Tianle Zeng, Junlei Hu, Gerardo Loza Galindo, Sharib Ali, Duygu Sarikaya, Pietro Valdastri, Dominic Jones', 'link': 'https://arxiv.org/abs/2508.07897', 'abstract': 'Computer vision-based technologies significantly enhance surgical automation by advancing tool tracking, detection, and localization. However, Current data-driven approaches are data-voracious, requiring large, high-quality labeled image datasets, which limits their application in surgical data science. Our Work introduces a novel dynamic Gaussian Splatting technique to address the data scarcity in surgical image datasets. We propose a dynamic Gaussian model to represent dynamic surgical scenes, enabling the rendering of surgical instruments from unseen viewpoints and deformations with real tissue backgrounds. We utilize a dynamic training adjustment strategy to address challenges posed by poorly calibrated camera poses from real-world scenarios. Additionally, we propose a method based on dynamic Gaussians for automatically generating annotations for our synthetic data. For evaluation, we constructed a new dataset featuring seven scenes with 14,000 frames of tool and camera motion and tool jaw articulation, with a background of an ex-vivo porcine model. Using this dataset, we synthetically replicate the scene deformation from the ground truth data, allowing direct comparisons of synthetic image quality. Experimental results illustrate that our method generates photo-realistic labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio (29.87). We further evaluate the performance of medical-specific neural networks trained on real and synthetic images using an unseen real-world image dataset. Our results show that the performance of models trained on synthetic images generated by the proposed method outperforms those trained with state-of-the-art standard data augmentation by 10%, leading to an overall improvement in model performances by nearly 15%.', 'abstract_zh': '基于计算机视觉的技术显著提升了手术自动化程度，通过推进工具跟踪、检测和定位。然而，当前的数据驱动方法需要大量的高质量标注图像数据，限制了其在手术数据科学中的应用。我们的工作引入了一种新颖的动态高斯散点图技术以应对手术图像数据集中的数据稀缺问题。我们提出了一个动态高斯模型来表示动态手术场景，使我们能够从未见过的角度和变形中渲染手术工具，并具有真实的组织背景。我们利用一种动态训练调整策略来应对来自真实场景中校准不良的摄像机姿态所带来的挑战。此外，我们提出了一种基于动态高斯分布的方法来自动为我们的合成数据生成注释。为了评估，我们构建了一个新的数据集，包含七个场景，共计14,000帧的工具和摄像机运动以及工具夹子动作，背景为离体猪模型。使用该数据集，我们从真实数据中合成立方形场景变形，允许直接比较合成图像的质量。实验结果表明，我们的方法生成了峰值信噪比（29.87）最高的照片级真实标注图像数据集。进一步用真实和合成图像训练的医学专用神经网络在未见过的真实世界图像数据集上进行性能评估，结果显示，由所提出方法生成的合成图像训练的模型性能比最先进的标准数据增强方法高出10%，总体提高了模型性能近15%。', 'title_zh': 'NeeCo: 基于动态和可变形3D高斯重建的新颖仪器状态图像合成'}
{'arxiv_id': 'arXiv:2508.07852', 'title': 'Vertex Features for Neural Global Illumination', 'authors': 'Rui Su, Honghao Dong, Haojie Jin, Yisong Chen, Guoping Wang, Sheng Li', 'link': 'https://arxiv.org/abs/2508.07852', 'abstract': 'Recent research on learnable neural representations has been widely adopted in the field of 3D scene reconstruction and neural rendering applications. However, traditional feature grid representations often suffer from substantial memory footprint, posing a significant bottleneck for modern parallel computing hardware. In this paper, we present neural vertex features, a generalized formulation of learnable representation for neural rendering tasks involving explicit mesh surfaces. Instead of uniformly distributing neural features throughout 3D space, our method stores learnable features directly at mesh vertices, leveraging the underlying geometry as a compact and structured representation for neural processing. This not only optimizes memory efficiency, but also improves feature representation by aligning compactly with the surface using task-specific geometric priors. We validate our neural representation across diverse neural rendering tasks, with a specific emphasis on neural radiosity. Experimental results demonstrate that our method reduces memory consumption to only one-fifth (or even less) of grid-based representations, while maintaining comparable rendering quality and lowering inference overhead.', 'abstract_zh': '最近关于可学习神经表示的研究在3D场景重建和神经渲染应用领域得到了广泛应用。然而，传统的特征网格表示往往内存占用巨大，成为现代并行计算硬件的重要瓶颈。在本文中，我们提出了神经顶点特征，这是一种适用于涉及显式网格表面的神经渲染任务的一般化可学习表示形式。我们的方法不均匀地在3D空间中分布神经特征，而是直接将可学习特征存储在网格顶点上，利用底层几何结构作为紧凑且结构化的神经处理表示。这不仅优化了内存效率，还通过任务特定的几何先验更好地对齐特征表示。我们在多种神经渲染任务中验证了我们的神经表示，特别强调神经辐射度的研究。实验结果表明，我们的方法将内存消耗减少到网格基表示的五分之一或更少，同时保持类似的渲染质量和降低推理开销。', 'title_zh': '顶点特征在神经全局光照中的应用'}
{'arxiv_id': 'arXiv:2508.07847', 'title': 'Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images', 'authors': 'Shunya Nagashima, Komei Sugiura', 'link': 'https://arxiv.org/abs/2508.07847', 'abstract': 'Accurate, reliable solar flare prediction is crucial for mitigating potential disruptions to critical infrastructure, while predicting solar flares remains a significant challenge. Existing methods based on heuristic physical features often lack representation learning from solar images. On the other hand, end-to-end learning approaches struggle to model long-range temporal dependencies in solar images. In this study, we propose Deep Space Weather Model (Deep SWM), which is based on multiple deep state space models for handling both ten-channel solar images and long-range spatio-temporal dependencies. Deep SWM also features a sparse masked autoencoder, a novel pretraining strategy that employs a two-phase masking approach to preserve crucial regions such as sunspots while compressing spatial information. Furthermore, we built FlareBench, a new public benchmark for solar flare prediction covering a full 11-year solar activity cycle, to validate our method. Our method outperformed baseline methods and even human expert performance on standard metrics in terms of performance and reliability. The project page can be found at this https URL.', 'abstract_zh': '准确可靠的太阳耀斑预测对于缓解关键基础设施潜在的干扰至关重要，然而预测太阳耀斑仍然是一个重大挑战。现有的基于启发式物理特征的方法往往缺乏对太阳图像的表示学习，而端到端学习方法又难以 modeling 太阳图像中的长期时空依赖关系。本研究提出 Deep Space Weather Model (Deep SWM)，该模型基于多个深度状态空间模型，用于处理十通道太阳图像和长期时空依赖关系。Deep SWM 还配备了稀疏掩码自编码器，这是一种新颖的预训练策略，采用两阶段掩码方法保留如日斑等关键区域，同时压缩空间信息。此外，我们构建了 FlareBench，这是一个新的公开基准，涵盖完整的 11 年太阳活动周期，用于验证我们的方法。我们的方法在标准指标上在性能和可靠性方面均超过了基准方法和人类专家的表现。项目页面详见这个 <https://>。', 'title_zh': '深空天气模型：多波长图像的长期太阳flare预测'}
{'arxiv_id': 'arXiv:2508.07819', 'title': 'Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP', 'authors': 'Ke Ma, Jun Long, Hongxiao Fei, Liujie Hua, Yueyi Luo', 'link': 'https://arxiv.org/abs/2508.07819', 'abstract': 'Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of local inductive biases for dense prediction and their reliance on inflexible feature fusion paradigms. We address these limitations through an Architectural Co-Design framework that jointly refines feature representation and cross-modal fusion. Our method integrates a parameter-efficient Convolutional Low-Rank Adaptation (Conv-LoRA) adapter to inject local inductive biases for fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that leverages visual context to adaptively modulate text prompts, enabling a powerful bidirectional fusion. Extensive experiments on diverse industrial and medical benchmarks demonstrate superior accuracy and robustness, validating that this synergistic co-design is critical for robustly adapting foundation models to dense perception tasks.', 'abstract_zh': '预训练视觉-语言模型在零-shot 异常检测中的适应性差距源于它们缺乏局部归纳偏置以及依赖于刚性特征融合范式。我们通过一种架构协同设计框架共同 refinement 特征表示和跨模态融合来解决这些限制。该方法集成了一个参数高效的卷积低秩适应（Conv-LoRA）适配器以注入局部归纳偏置以促进细粒度表示，并引入了一个动态融合网关（DFG），利用视觉上下文自适应调整文本提示，实现强大的双向融合。在多样化的工业和医疗基准上的广泛实验表明，此协同设计能够显著提高准确性和鲁棒性，验证了这种协同设计对于稳健地适应基础模型到密集感知任务的重要性。', 'title_zh': '零样本异常检测中的架构联合设计：CLIP中代表和动态特征融合的解耦'}
{'arxiv_id': 'arXiv:2508.07683', 'title': 'TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding', 'authors': 'Chaohong Guo, Xun Mo, Yongwei Nie, Xuemiao Xu, Chao Xu, Fei Yu, Chengjiang Long', 'link': 'https://arxiv.org/abs/2508.07683', 'abstract': 'Temporal Video Grounding (TVG) aims to precisely localize video segments corresponding to natural language queries, which is a critical capability for long-form video understanding. Although existing reinforcement learning approaches encourage models to generate reasoning chains before predictions, they fail to explicitly constrain the reasoning process to ensure the quality of the final temporal predictions. To address this limitation, we propose Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG), a novel framework that introduces timestamp anchors within the reasoning process to enforce explicit supervision to the thought content. These anchors serve as intermediate verification points. More importantly, we require each reasoning step to produce increasingly accurate temporal estimations, thereby ensuring that the reasoning process contributes meaningfully to the final prediction. To address the challenge of low-probability anchor generation in models (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation training strategy: (1) initial GRPO training to collect 30K high-quality reasoning traces containing multiple timestamp anchors, (2) supervised fine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the SFT-enhanced model. This three-stage training strategy enables robust anchor generation while maintaining reasoning quality. Experiments show that our model achieves state-of-the-art performance while producing interpretable, verifiable reasoning chains with progressively refined temporal estimations.', 'abstract_zh': 'Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG)', 'title_zh': '基于时间戳锚点约束推理的TAR-TVG：增强多模态语言模型的时间视频定位'}
{'arxiv_id': 'arXiv:2508.07630', 'title': 'InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information', 'authors': 'Anirudh Iyengar Kaniyar Narayana Iyengar, Srija Mukhopadhyay, Adnan Qidwai, Shubhankar Singh, Dan Roth, Vivek Gupta', 'link': 'https://arxiv.org/abs/2508.07630', 'abstract': 'We introduce InterChart, a diagnostic benchmark that evaluates how well vision-language models (VLMs) reason across multiple related charts, a task central to real-world applications such as scientific reporting, financial analysis, and public policy dashboards. Unlike prior benchmarks focusing on isolated, visually uniform charts, InterChart challenges models with diverse question types ranging from entity inference and trend correlation to numerical estimation and abstract multi-step reasoning grounded in 2-3 thematically or structurally related charts. We organize the benchmark into three tiers of increasing difficulty: (1) factual reasoning over individual charts, (2) integrative analysis across synthetically aligned chart sets, and (3) semantic inference over visually complex, real-world chart pairs. Our evaluation of state-of-the-art open and closed-source VLMs reveals consistent and steep accuracy declines as chart complexity increases. We find that models perform better when we decompose multi-entity charts into simpler visual units, underscoring their struggles with cross-chart integration. By exposing these systematic limitations, InterChart provides a rigorous framework for advancing multimodal reasoning in complex, multi-visual environments.', 'abstract_zh': '我们介绍了一种名为InterChart的诊断基准，用于评估视觉-语言模型（VLMs）在多个相关图表之间进行推理的能力，这是科学报告、财务分析和公共政策仪表板等真实世界应用的核心任务。InterChart不同于以往专注于孤立且视觉一致的图表的基准，它挑战模型应对从实体推理和趋势关联到基于2-3个主题或结构相关图表的数值估计和抽象多步推理等多样化的提问类型。我们将基准划分为三个逐渐增加难度的层级：（1）在单一图表上的事实推理，（2）在合成对齐的图表集中进行综合分析，（3）在视觉复杂的真实世界图表对上的语义推理。对最先进的开源和闭源VLMs的评估显示，随着图表复杂性的增加，准确率呈现出一致且显著的下降。我们发现，当将多实体图表分解为更简单的视觉单位时，模型的表现更好，这突显了它们在跨图表整合方面的困难。通过揭示这些系统的限制，InterChart为在复杂多视图环境下的多模态推理提供了一个严格的框架。', 'title_zh': 'InterChart: �across分解和分布式图表信息的视觉推理基准测试'}
{'arxiv_id': 'arXiv:2508.07597', 'title': 'ShoulderShot: Generating Over-the-Shoulder Dialogue Videos', 'authors': 'Yuang Zhang, Junqi Cheng, Haoyu Zhao, Jiaxi Gu, Fangyuan Zou, Zenghui Lu, Peng Shu', 'link': 'https://arxiv.org/abs/2508.07597', 'abstract': "Over-the-shoulder dialogue videos are essential in films, short dramas, and advertisements, providing visual variety and enhancing viewers' emotional connection. Despite their importance, such dialogue scenes remain largely underexplored in video generation research. The main challenges include maintaining character consistency across different shots, creating a sense of spatial continuity, and generating long, multi-turn dialogues within limited computational budgets. Here, we present ShoulderShot, a framework that combines dual-shot generation with looping video, enabling extended dialogues while preserving character consistency. Our results demonstrate capabilities that surpass existing methods in terms of shot-reverse-shot layout, spatial continuity, and flexibility in dialogue length, thereby opening up new possibilities for practical dialogue video generation. Videos and comparisons are available at this https URL.", 'abstract_zh': '肩拍对白视频在电影、短剧和广告中至关重要，能提供视觉多样性并增强观众的情感连接。尽管如此，此类对白场景在视频生成研究中仍 largely underexplored。主要挑战包括保持不同镜头中人物的一致性、创造空间连续感以及在有限的计算预算内生成长且多轮的对白。在这里，我们提出了 ShoulderShot 框架，结合双镜头生成与循环视频，能够在保持人物一致性的同时扩展对白。我们的结果显示，该方法在镜头替换布局、空间连续性和对话长度的灵活性方面超越了现有方法，从而为实际对白视频生成开辟了新的可能性。更多视频和比较请参见 this https URL。', 'title_zh': 'ShoulderShot: 生成越肩对话视频'}
{'arxiv_id': 'arXiv:2508.07520', 'title': 'Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI', 'authors': 'Baihan Lin', 'link': 'https://arxiv.org/abs/2508.07520', 'abstract': 'What if the patterns hidden within dialogue reveal more about communication than the words themselves? We introduce Conversational DNA, a novel visual language that treats any dialogue -- whether between humans, between human and AI, or among groups -- as a living system with interpretable structure that can be visualized, compared, and understood. Unlike traditional conversation analysis that reduces rich interaction to statistical summaries, our approach reveals the temporal architecture of dialogue through biological metaphors. Linguistic complexity flows through strand thickness, emotional trajectories cascade through color gradients, conversational relevance forms through connecting elements, and topic coherence maintains structural integrity through helical patterns. Through exploratory analysis of therapeutic conversations and historically significant human-AI dialogues, we demonstrate how this visualization approach reveals interaction patterns that traditional methods miss. Our work contributes a new creative framework for understanding communication that bridges data visualization, human-computer interaction, and the fundamental question of what makes dialogue meaningful in an age where humans increasingly converse with artificial minds.', 'abstract_zh': '对话中隐藏的模式是否揭示了比文字本身更多的沟通信息？我们引入了对话DNA，这是一种新颖的可视化语言，将任何对话——无论是人与人之间、人与AI之间还是群体内部——视为具有可解釋结构的活体系统，可以进行可视化、比较和理解。不同于传统对话分析将丰富的互动简化为统计摘要，我们的方法通过生物隐喻揭示了对话的时间架构。语言复杂性通过线缆厚度流动，情感轨迹通过色彩渐变传递，对话相关性通过连接元素形成，话题连贯性通过螺旋模式保持结构完整性。通过对治疗性对话和历史上重要的人机对话的探索性分析，我们展示了这种可视化方法如何揭示传统方法所忽略的互动模式。我们的工作贡献了一种新的创造框架，用于理解沟通，该框架跨越了数据可视化、人机交互以及在一个日益与人工智能交流的时代，使对话有意义的基本问题的桥接。', 'title_zh': '对话DNA：一种新的视觉语言，用于理解人类与AI的对话结构'}
{'arxiv_id': 'arXiv:2508.07514', 'title': 'From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials', 'authors': 'Artzai Picon, Itziar Eguskiza, Daniel Mugica, Javier Romero, Carlos Javier Jimenez, Eric White, Gabriel Do-Lago-Junqueira, Christian Klukas, Ramon Navarra-Mestre', 'link': 'https://arxiv.org/abs/2508.07514', 'abstract': "Field trials are vital in herbicide research and development to assess effects on crops and weeds under varied conditions. Traditionally, evaluations rely on manual visual assessments, which are time-consuming, labor-intensive, and subjective. Automating species and damage identification is challenging due to subtle visual differences, but it can greatly enhance efficiency and consistency.\nWe present an improved segmentation model combining a general-purpose self-supervised visual model with hierarchical inference based on botanical taxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain using digital and mobile cameras, the model was tested on digital camera data (year 2023) and drone imagery from the United States, Germany, and Spain (year 2024) to evaluate robustness under domain shift. This cross-device evaluation marks a key step in assessing generalization across platforms of the model.\nOur model significantly improved species identification (F1-score: 0.52 to 0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to 0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone images), it maintained strong performance with moderate degradation (species: F1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where earlier models failed.\nThese results confirm the model's robustness and real-world applicability. It is now deployed in BASF's phenotyping pipeline, enabling large-scale, automated crop and weed monitoring across diverse geographies.", 'abstract_zh': '农田试验对于除草剂研究与开发至关重要，用于在不同条件下评估其对作物和杂草的影响。传统评估依赖手动视觉评估，耗时、劳动密集且主观。自动化物种和损伤识别具有挑战性，但由于视觉差异微小，这可以大大提升效率和一致性。\n\n我们提出了一种结合通用自监督视觉模型和基于植物学分类的分层推理的改进分割模型。该模型在2018-2020年德国和西班牙使用数字和移动相机收集的多年度数据集上进行训练，并在2023年的数字相机数据和2024年来自美国、德国和西班牙的无人机影像上进行测试，以评估跨平台稳健性。这一跨设备评估是评价模型跨平台泛化的关键步骤。\n\n该模型在物种识别（F1分数从0.52提高到0.85，R-squared从0.75提高到0.98）和损伤分类（F1分数从0.28提高到0.44，R-squared从0.71提高到0.87）上显著优于先前方法。在跨领域（无人机图像）情况下，它仍能保持较强性能，尽管表现有所降级（物种：F1分数0.60，R-squared 0.80；损伤：F1分数0.41，R-squared 0.62），而早期模型则失败了。\n\n这些结果证实了该模型的稳健性和实际应用潜力。该模型现已部署在巴斯夫的表型分析流水线中，实现跨地域的大规模自动化作物和杂草监测。', 'title_zh': '从田间到无人机：容忍领域漂移的自动多物种和损伤植物语义分割在除草剂试验中的应用'}
{'arxiv_id': 'arXiv:2508.07306', 'title': 'DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices', 'authors': 'Md Zahurul Haquea, Yeahyea Sarker, Muhammed Farhan Sadique Mahi, Syed Jubayer Jaman, Md Robiul Islam', 'link': 'https://arxiv.org/abs/2508.07306', 'abstract': 'Dragon fruit, renowned for its nutritional benefits and economic value, has experienced rising global demand due to its affordability and local availability. As dragon fruit cultivation expands, efficient pre- and post-harvest quality inspection has become essential for improving agricultural productivity and minimizing post-harvest losses. This study presents DragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN) optimized for real-time quality assessment of dragon fruits on mobile devices. We curated a diverse dataset of 13,789 images, integrating self-collected samples with public datasets (dataset from Mendeley Data), and classified them into four categories: fresh, immature, mature, and defective fruits to ensure robust model training. The proposed model achieves an impressive 93.98% accuracy, outperforming existing methods in fruit quality classification. To facilitate practical adoption, we embedded the model into an intuitive mobile application, enabling farmers and agricultural stakeholders to conduct on-device, real-time quality inspections. This research provides an accurate, efficient, and scalable AI-driven solution for dragon fruit quality control, supporting digital agriculture and empowering smallholder farmers with accessible technology. By bridging the gap between research and real-world application, our work advances post-harvest management and promotes sustainable farming practices.', 'abstract_zh': '基于卷积神经网络的轻量级龙果品质检测模型研究', 'title_zh': 'DragonFruitQualityNet：一种适用于移动设备实时龙眼质量检测的轻量级卷积神经网络'}
{'arxiv_id': 'arXiv:2508.07281', 'title': 'Representation Understanding via Activation Maximization', 'authors': 'Hongbo Zhu, Angelo Cangelosi', 'link': 'https://arxiv.org/abs/2508.07281', 'abstract': 'Understanding internal feature representations of deep neural networks (DNNs) is a fundamental step toward model interpretability. Inspired by neuroscience methods that probe biological neurons using visual stimuli, recent deep learning studies have employed Activation Maximization (AM) to synthesize inputs that elicit strong responses from artificial neurons. In this work, we propose a unified feature visualization framework applicable to both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike prior efforts that predominantly focus on the last output-layer neurons in CNNs, we extend feature visualization to intermediate layers as well, offering deeper insights into the hierarchical structure of learned feature representations. Furthermore, we investigate how activation maximization can be leveraged to generate adversarial examples, revealing potential vulnerabilities and decision boundaries of DNNs. Our experiments demonstrate the effectiveness of our approach in both traditional CNNs and modern ViT, highlighting its generalizability and interpretive value.', 'abstract_zh': '理解深度神经网络（DNNs）的内部特征表示是模型可解释性研究中的一个基本步骤。借鉴神经科学中通过视觉刺激探针生物神经元的方法，最近的深度学习研究采用了激活最大化（AM）来合成能够强烈激活人工神经元的输入。在本工作中，我们提出了一种适用于卷积神经网络（CNNs）和视觉变换器（ViTs）的统一特征可视化框架。不同于以往主要聚焦于CNNs的最后一层神经元的努力，我们将特征可视化扩展到中间层，以提供对学习特征表示层次结构的更深入洞察。此外，我们探讨了如何利用激活最大化生成对抗样本，揭示了DNNs的潜在脆弱性和决策边界。我们的实验表明，该方法在传统的CNNs和现代的ViT中都表现出有效性，突显了其普适性和解释价值。', 'title_zh': '通过激活最大化理解表示'}
{'arxiv_id': 'arXiv:2508.07170', 'title': 'Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection', 'authors': 'Yunpeng Shi, Lei Chen, Xiaolu Shen, Yanju Guo', 'link': 'https://arxiv.org/abs/2508.07170', 'abstract': 'In the domain of computer vision, multi-scale feature extraction is vital for tasks such as salient object detection. However, achieving this capability in lightweight networks remains challenging due to the trade-off between efficiency and performance. This paper proposes a novel lightweight multi-scale feature extraction layer, termed the LMF layer, which employs depthwise separable dilated convolutions in a fully connected structure. By integrating multiple LMF layers, we develop LMFNet, a lightweight network tailored for salient object detection. Our approach significantly reduces the number of parameters while maintaining competitive performance. Here, we show that LMFNet achieves state-of-the-art or comparable results on five benchmark datasets with only 0.81M parameters, outperforming several traditional and lightweight models in terms of both efficiency and accuracy. Our work not only addresses the challenge of multi-scale learning in lightweight networks but also demonstrates the potential for broader applications in image processing tasks. The related code files are available at this https URL', 'abstract_zh': '在计算机视觉领域，多尺度特征提取是重要的，对于显著目标检测等任务尤为关键。然而，由于效率与性能之间的权衡，在轻量级网络中实现这一能力仍然具有挑战性。本文提出了一种新颖的轻量级多尺度特征提取层，称为LMF层，该层采用全连接结构中的深度可分离空洞卷积。通过整合多个LMF层，我们开发了LMFNet，这是一种专门为显著目标检测设计的轻量级网络。我们的方法在显著减少参数数量的同时，仍能保持竞争力。结果显示，LMFNet在五个基准数据集上仅使用0.81M参数达到了最先进的或可比拟的结果，并在效率和准确性方面优于多种传统和轻量级模型。我们的工作不仅解决了轻量级网络中的多尺度学习挑战，还展示了其在图像处理任务中的广泛应用潜力。相关代码文件可在以下链接获取：this https URL。', 'title_zh': '轻量级多尺度特征提取的全连接LMF层在显著目标检测中的应用'}
{'arxiv_id': 'arXiv:2508.07165', 'title': 'Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications', 'authors': 'Zelin Qiu, Xi Wang, Zhuoyao Xie, Juan Zhou, Yu Wang, Lingjie Yang, Xinrui Jiang, Juyoung Bae, Moo Hyun Son, Qiang Ye, Dexuan Chen, Rui Zhang, Tao Li, Neeraj Ramesh Mahboobani, Varut Vardhanabhuti, Xiaohui Duan, Yinghua Zhao, Hao Chen', 'link': 'https://arxiv.org/abs/2508.07165', 'abstract': 'Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable versatility, enabling the distinct visualization of different tissue types. Nevertheless, the inherent heterogeneity among MRI sequences poses significant challenges to the generalization capability of deep learning models. These challenges undermine model performance when faced with varying acquisition parameters, thereby severely restricting their clinical utility. In this study, we present PRISM, a foundation model PRe-trained with large-scale multI-Sequence MRI. We collected a total of 64 datasets from both public and private sources, encompassing a wide range of whole-body anatomical structures, with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI scans from 34 datasets (8 public and 26 private) were curated to construct the largest multi-organ multi-sequence MRI pretraining corpus to date. We propose a novel pretraining paradigm that disentangles anatomically invariant features from sequence-specific variations in MRI, while preserving high-level semantic representations. We established a benchmark comprising 44 downstream tasks, including disease diagnosis, image segmentation, registration, progression prediction, and report generation. These tasks were evaluated on 32 public datasets and 5 private cohorts. PRISM consistently outperformed both non-pretrained models and existing foundation models, achieving first-rank results in 39 out of 44 downstream benchmarks with statistical significance improvements. These results underscore its ability to learn robust and generalizable representations across unseen data acquired under diverse MRI protocols. PRISM provides a scalable framework for multi-sequence MRI analysis, thereby enhancing the translational potential of AI in radiology. It delivers consistent performance across diverse imaging protocols, reinforcing its clinical applicability.', 'abstract_zh': '多序列磁共振成像（MRI提供的非凡灵活性使得不同组织类型的独特可视化成为可能。然而，MRI序列固有的异质性给深度学习模型的一般性能带来了显著挑战。这些挑战在面对不同的同时性能表现上削弱了模型，进而严重限制了其临床应用价值。在本文中，我们提出了PRISM，一种大规模多序列MRI预训练的基础模型。我们总共收集了64个数据集，涵盖了广泛的解剖结构，并跨越了不同的MRI序列。其中，34个数据集中包含3,4476个三维MRI扫描（8个公共数据集和26个私人有数据集），我们构建了迄今最大的的多器官多序列MRI训练数据集。我们提出了一种新的预训练范式，将MRI中的解剖不变特征从特定特征中分离出来 保持高层次语义表示。我们建立了一个基准 包含44个下游任务 其中包括疾病诊断 图像分割 注册 痖症预测 和生成。这些任务在3个公共数据集和 11 个私人组群上上e上 评估中 显示出PRISM始终优于预训练模型和 现有基础模型 在44个下游基准中的3个中获得了显着的排名第一。这些结果显示了其在不同未知的情况下对 跨越不同的MRI协议下保持鲁棒性和可扩展性表示的能力。PRISM提供了一个可序列MRI分析的可伸缩框架 从而增强了人工智能在放射学中的转化潜力。PRISM在不同的图像协议中提供了持续的性能 从而加强了其实临床上的应用可行性。', 'title_zh': '大规模多序列预训练在多样化临床应用中的通用MRI分析'}
{'arxiv_id': 'arXiv:2508.07146', 'title': 'Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction', 'authors': 'Yu Liu, Zhijie Liu, Xiao Ren, You-Fu Li, He Kong', 'link': 'https://arxiv.org/abs/2508.07146', 'abstract': 'Predicting pedestrian motion trajectories is critical for the path planning and motion control of autonomous vehicles. Recent diffusion-based models have shown promising results in capturing the inherent stochasticity of pedestrian behavior for trajectory prediction. However, the absence of explicit semantic modelling of pedestrian intent in many diffusion-based methods may result in misinterpreted behaviors and reduced prediction accuracy. To address the above challenges, we propose a diffusion-based pedestrian trajectory prediction framework that incorporates both short-term and long-term motion intentions. Short-term intent is modelled using a residual polar representation, which decouples direction and magnitude to capture fine-grained local motion patterns. Long-term intent is estimated through a learnable, token-based endpoint predictor that generates multiple candidate goals with associated probabilities, enabling multimodal and context-aware intention modelling. Furthermore, we enhance the diffusion process by incorporating adaptive guidance and a residual noise predictor that dynamically refines denoising accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and SDD benchmarks, demonstrating competitive results against state-of-the-art methods.', 'abstract_zh': '基于扩散模型的人行轨迹预测对于自主车辆的路径规划和运动控制至关重要。近期的扩散模型在捕捉人行行为的内在随机性方面表现出有希望的结果。然而，许多扩散模型中缺乏显式的行人意图语义建模可能导致行为误解读和预测准确度降低。为了解决上述挑战，我们提出了一种结合短期和长期运动意图的基于扩散的人行轨迹预测框架。短期意图通过残差极坐标表示建模，以解耦方向和大小来捕捉精细的局部运动模式。长期意图通过可学习的基于令牌的目标端点预测器估计，生成具有关联概率的多个候选目标，从而实现多模态和上下文感知的意图建模。此外，我们通过引入自适应引导和残差噪声预测器增强扩散过程，动态提高去噪准确性。所提出的框架在广泛使用的ETH、UCY和SDD基准上进行评估，显示出与最新方法竞争的性能。', 'title_zh': '意图意识驱动的行人轨迹预测扩散模型'}
{'arxiv_id': 'arXiv:2508.07128', 'title': 'Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays', 'authors': 'Gregory Schuit, Denis Parra, Cecilia Besa', 'link': 'https://arxiv.org/abs/2508.07128', 'abstract': 'Generative image models have achieved remarkable progress in both natural and medical imaging. In the medical context, these techniques offer a potential solution to data scarcity-especially for low-prevalence anomalies that impair the performance of AI-driven diagnostic and segmentation tools. However, questions remain regarding the fidelity and clinical utility of synthetic images, since poor generation quality can undermine model generalizability and trust. In this study, we evaluate the effectiveness of state-of-the-art generative models-Generative Adversarial Networks (GANs) and Diffusion Models (DMs)-for synthesizing chest X-rays conditioned on four abnormalities: Atelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged Cardiac Silhouette (ECS). Using a benchmark composed of real images from the MIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a reader study with three radiologists of varied experience. Participants were asked to distinguish real from synthetic images and assess the consistency between visual features and the target abnormality. Our results show that while DMs generate more visually realistic images overall, GANs can report better accuracy for specific conditions, such as absence of ECS. We further identify visual cues radiologists use to detect synthetic images, offering insights into the perceptual gaps in current models. These findings underscore the complementary strengths of GANs and DMs and point to the need for further refinement to ensure generative models can reliably augment training datasets for AI diagnostic systems.', 'abstract_zh': 'Generative图像模型在自然图像和医学影像领域均取得了显著进展。在医学领域，这些技术为解决数据稀缺问题提供了潜在解决方案，特别是对于影响基于AI的诊断和分割工具性能的低频异常。然而，合成图像的真实性和临床效用仍存在问题，因为生成质量差可能削弱模型的泛化能力和信任度。在这项研究中，我们评估了当前最先进的生成模型—生成对抗网络（GANs）和扩散模型（DMs）—在合成基于四种异常条件的胸部X光片方面的有效性：肺不张（AT）、肺-opacity（LO）、胸腔积液（PE）和心影增大（ECS）。我们使用MIMIC-CXR数据集中的真实图像和GANs、DMs生成的合成图像作为基准，进行了包括三位经验不同的放射科医生在内的读者研究。参与者被要求区分真实图像和合成图像，并评估视觉特征与目标异常的一致性。结果显示，尽管DMs整体生成更具有视觉真实感的图像，但在某些条件下（如心影增大的不存在），GANs表现出更高的准确性。我们进一步识别了放射科医生用于检测合成图像的视觉线索，揭示了当前模型在感知方面的差距。这些发现强调了GANs和DMs互补优势，并指出需要进一步改进以确保生成模型能可靠地增强AI诊断系统的训练数据集。', 'title_zh': 'GANs和扩散模型在生成X射线图像中的感知评估'}
{'arxiv_id': 'arXiv:2508.07102', 'title': 'Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria', 'authors': 'Yang Cao, Yubin Chen, Zhao Song, Jiahao Zhang', 'link': 'https://arxiv.org/abs/2508.07102', 'abstract': 'Generative modelling has seen significant advances through simulation-free paradigms such as Flow Matching, and in particular, the MeanFlow framework, which replaces instantaneous velocity fields with average velocities to enable efficient single-step sampling. In this work, we introduce a theoretical study on Second-Order MeanFlow, a novel extension that incorporates average acceleration fields into the MeanFlow objective. We first establish the feasibility of our approach by proving that the average acceleration satisfies a generalized consistency condition analogous to first-order MeanFlow, thereby supporting stable, one-step sampling and tractable loss functions. We then characterize its expressivity via circuit complexity analysis, showing that under mild assumptions, the Second-Order MeanFlow sampling process can be implemented by uniform threshold circuits within the $\\mathsf{TC}^0$ class. Finally, we derive provably efficient criteria for scalable implementation by leveraging fast approximate attention computations: we prove that attention operations within the Second-Order MeanFlow architecture can be approximated to within $1/\\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results lay the theoretical foundation for high-order flow matching models that combine rich dynamics with practical sampling efficiency.', 'abstract_zh': '无模拟生成建模：Second-Order MeanFlow的理论研究', 'title_zh': '高阶均值流生成模型的可行性、表示能力和可证明高效准则'}
{'arxiv_id': 'arXiv:2508.06982', 'title': 'WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering', 'authors': 'Yixin Zhu, Zuoliang Zhu, Miloš Hašan, Jian Yang, Jin Xie, Beibei Wang', 'link': 'https://arxiv.org/abs/2508.06982', 'abstract': 'Forward and inverse rendering have emerged as key techniques for enabling understanding and reconstruction in the context of autonomous driving (AD). However, complex weather and illumination pose great challenges to this task. The emergence of large diffusion models has shown promise in achieving reasonable results through learning from 2D priors, but these models are difficult to control and lack robustness. In this paper, we introduce WeatherDiffusion, a diffusion-based framework for forward and inverse rendering on AD scenes with various weather and lighting conditions. Our method enables authentic estimation of material properties, scene geometry, and lighting, and further supports controllable weather and illumination editing through the use of predicted intrinsic maps guided by text descriptions. We observe that different intrinsic maps should correspond to different regions of the original image. Based on this observation, we propose Intrinsic map-aware attention (MAA) to enable high-quality inverse rendering. Additionally, we introduce a synthetic dataset (\\ie WeatherSynthetic) and a real-world dataset (\\ie WeatherReal) for forward and inverse rendering on AD scenes with diverse weather and lighting. Extensive experiments show that our WeatherDiffusion outperforms state-of-the-art methods on several benchmarks. Moreover, our method demonstrates significant value in downstream tasks for AD, enhancing the robustness of object detection and image segmentation in challenging weather scenarios.', 'abstract_zh': '基于扩散模型的自动驾驶场景正逆渲染框架：WeatherDiffusion', 'title_zh': 'WeatherDiffusion：受天气指导的渲染模型，用于正向和逆向渲染'}
{'arxiv_id': 'arXiv:2508.06959', 'title': 'Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification', 'authors': 'Qin Xu, Lili Zhu, Xiaoxia Cheng, Bo Jiang', 'link': 'https://arxiv.org/abs/2508.06959', 'abstract': 'The crux of resolving fine-grained visual classification (FGVC) lies in capturing discriminative and class-specific cues that correspond to subtle visual characteristics. Recently, frequency decomposition/transform based approaches have attracted considerable interests since its appearing discriminative cue mining ability. However, the frequency-domain methods are based on fixed basis functions, lacking adaptability to image content and unable to dynamically adjust feature extraction according to the discriminative requirements of different images. To address this, we propose a novel method for FGVC, named Subtle-Cue Oriented Perception Engine (SCOPE), which adaptively enhances the representational capability of low-level details and high-level semantics in the spatial domain, breaking through the limitations of fixed scales in the frequency domain and improving the flexibility of multi-scale fusion. The core of SCOPE lies in two modules: the Subtle Detail Extractor (SDE), which dynamically enhances subtle details such as edges and textures from shallow features, and the Salient Semantic Refiner (SSR), which learns semantically coherent and structure-aware refinement features from the high-level features guided by the enhanced shallow features. The SDE and SSR are cascaded stage-by-stage to progressively combine local details with global semantics. Extensive experiments demonstrate that our method achieves new state-of-the-art on four popular fine-grained image classification benchmarks.', 'abstract_zh': '细粒度视觉分类中关键在于捕捉与微妙视觉特征对应的 discriminative 和类特定线索。近年来，基于频率分解/变换的方法由于其辨别性线索提取能力而引起了广泛关注。然而，频率域方法基于固定的基函数，缺乏对图像内容的适应性，无法根据不同图像的辨别性要求动态调整特征提取。为解决这一问题，我们提出了一种新的细粒度视觉分类方法——细粒度线索导向感知引擎（Subtle-Cue Oriented Perception Engine, SCOPE），该方法在空间域中动态增强低级细节和高级语义的表示能力，突破了固定频率尺度的限制，提高了多尺度融合的灵活性。SCOPE的核心在于两个模块：细粒度细节提取器（Subtle Detail Extractor, SDE），该模块从浅层特征中动态增强边缘和纹理等细微细节；以及显著语义精炼器（Salient Semantic Refiner, SSR），该模块在增强浅层特征的引导下，学习语义一致且结构感知的精炼特征。SDE和SSR逐层级联，逐步将局部细节与全局语义相结合。大量实验表明，我们的方法在四个流行的细粒度图像分类基准上取得了新的最佳性能。', 'title_zh': '超越频率：通过空间分解视角观察细微线索进行细粒度视觉分类'}
{'arxiv_id': 'arXiv:2508.06937', 'title': 'CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing', 'authors': 'Weiyan Xie, Han Gao, Didan Deng, Kaican Li, April Hua Liu, Yongxiang Huang, Nevin L. Zhang', 'link': 'https://arxiv.org/abs/2508.06937', 'abstract': "Recent advances in text-to-image (T2I) models have enabled training-free regional image editing by leveraging the generative priors of foundation models. However, existing methods struggle to balance text adherence in edited regions, context fidelity in unedited areas, and seamless integration of edits. We introduce CannyEdit, a novel training-free framework that addresses these challenges through two key innovations: (1) Selective Canny Control, which masks the structural guidance of Canny ControlNet in user-specified editable regions while strictly preserving details of the source images in unedited areas via inversion-phase ControlNet information retention. This enables precise, text-driven edits without compromising contextual integrity. (2) Dual-Prompt Guidance, which combines local prompts for object-specific edits with a global target prompt to maintain coherent scene interactions. On real-world image editing tasks (addition, replacement, removal), CannyEdit outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent improvement in the balance of text adherence and context fidelity. In terms of editing seamlessness, user studies reveal only 49.2 percent of general users and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited when paired with real images without edits, versus 76.08 to 89.09 percent for competitor methods.", 'abstract_zh': 'Recent Advances in Training-Free Regional Image Editing via CannyEdit：Selective Canny Control and Dual-Prompt Guidance', 'title_zh': 'CannyEdit：选择性Canny控制与双提示指导的无训练图像编辑'}
{'arxiv_id': 'arXiv:2508.06878', 'title': 'NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective', 'authors': 'Maoxun Yuan, Duanni Meng, Ziteng Xi, Tianyi Zhao, Shiji Zhao, Yimian Dai, Xingxing Wei', 'link': 'https://arxiv.org/abs/2508.06878', 'abstract': 'Infrared small target detection and segmentation (IRSTDS) is a critical yet challenging task in defense and civilian applications, owing to the dim, shapeless appearance of targets and severe background clutter. Recent CNN-based methods have achieved promising target perception results, but they only focus on enhancing feature representation to offset the impact of noise, which results in the increased false alarms problem. In this paper, through analyzing the problem from the frequency domain, we pioneer in improving performance from noise suppression perspective and propose a novel noise-suppression feature pyramid network (NS-FPN), which integrates a low-frequency guided feature purification (LFP) module and a spiral-aware feature sampling (SFS) module into the original FPN structure. The LFP module suppresses the noise features by purifying high-frequency components to achieve feature enhancement devoid of noise interference, while the SFS module further adopts spiral sampling to fuse target-relevant features in feature fusion process. Our NS-FPN is designed to be lightweight yet effective and can be easily plugged into existing IRSTDS frameworks. Extensive experiments on the public IRSTDS datasets demonstrate that our method significantly reduces false alarms and achieves superior performance on IRSTDS tasks.', 'abstract_zh': '红外小目标检测与分割（IRSTDS）是国防和民用应用应用中的一项关键但具有挑战性的任务，，，， account 和民用应用应用中（，， 由于目标的昏暗外观和严重的背景杂波。近期近年来，基于卷积神经网络（CNN）的方法在目标感知上取得了显著效果，，但它们仅专注于增强特征以减少噪声的影响，最终导致了增加增加的误报。本研究从噪声抑制的角度分析了该问题，并，并 首次提出了噪声抑制金字塔网络（NS-NPN），该网络结合了低频特征净化（LFP）和螺旋感知采样（SFS）模块。LFP模块通过去除高频分分分分分数来净化噪声特征，以实现在无噪声干扰下的特征增强；SFS模块通过螺旋采样将融合过程中的相关特征融合起来。我们的NS-NPN旨在保持轻轻设计，并且可以很容易地插入现有的IRSTds框架。在设计的irstds数据集上进行了详尽的实验表明，，本方法显著地降低了误报，并在irstds任务上取得了最优效果。', 'title_zh': 'NS-FPN：从噪声抑制视角改进红外小目标检测与分割'}
{'arxiv_id': 'arXiv:2508.06853', 'title': 'AGIC: Attention-Guided Image Captioning to Improve Caption Relevance', 'authors': 'L. D. M. S. Sai Teja, Ashok Urlana, Pruthwik Mishra', 'link': 'https://arxiv.org/abs/2508.06853', 'abstract': 'Despite significant progress in image captioning, generating accurate and descriptive captions remains a long-standing challenge. In this study, we propose Attention-Guided Image Captioning (AGIC), which amplifies salient visual regions directly in the feature space to guide caption generation. We further introduce a hybrid decoding strategy that combines deterministic and probabilistic sampling to balance fluency and diversity. To evaluate AGIC, we conduct extensive experiments on the Flickr8k and Flickr30k datasets. The results show that AGIC matches or surpasses several state-of-the-art models while achieving faster inference. Moreover, AGIC demonstrates strong performance across multiple evaluation metrics, offering a scalable and interpretable solution for image captioning.', 'abstract_zh': '基于注意力引导的图像caption生成（AGIC）：直接在特征空间增强显著视觉区域以指导caption生成', 'title_zh': 'AGIC：注意力引导的图像字幕生成以提高字幕的相关性性\nuser\n纠正上面的翻译，_feed精确一点ankan谢谢。标题：AGIC: Attention-Guided Image Captioning to Improve Caption Relevance。'}
{'arxiv_id': 'arXiv:2508.06632', 'title': 'CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition', 'authors': 'Wenpeng Xing, Jie Chen, Zaifeng Yang, Tiancheng Zhao, Gaolei Li, Changting Lin, Yike Guo, Meng Han', 'link': 'https://arxiv.org/abs/2508.06632', 'abstract': 'Neural Radiance Fields (NeRF) have shown impressive performance in novel view synthesis, but challenges remain in rendering scenes with complex specular reflections and highlights. Existing approaches may produce blurry reflections due to entanglement between lighting and material properties, or encounter optimization instability when relying on physically-based inverse rendering. In this work, we present a neural rendering framework based on dynamic coefficient decomposition, aiming to improve the modeling of view-dependent appearance. Our approach decomposes complex appearance into a shared, static neural basis that encodes intrinsic material properties, and a set of dynamic coefficients generated by a Coefficient Network conditioned on view and illumination. A Dynamic Radiance Integrator then combines these components to synthesize the final radiance. Experimental results on several challenging benchmarks suggest that our method can produce sharper and more realistic specular highlights compared to existing techniques. We hope that this decomposition paradigm can provide a flexible and effective direction for modeling complex appearance in neural scene representations.', 'abstract_zh': '基于动态系数分解的神经渲染框架：改善视依赖外观建模', 'title_zh': 'CoDe-NeRF: 基于动态系数分解的神经渲染'}
{'arxiv_id': 'arXiv:2508.06528', 'title': 'A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition', 'authors': 'Xiuliang Zhang, Tadiwa Elisha Nyamasvisva, Chuntao Liu', 'link': 'https://arxiv.org/abs/2508.06528', 'abstract': 'Video-based behavior recognition is essential in fields such as public safety, intelligent surveillance, and human-computer interaction. Traditional 3D Convolutional Neural Network (3D CNN) effectively capture local spatiotemporal features but struggle with modeling long-range dependencies. Conversely, Transformers excel at learning global contextual information but face challenges with high computational costs. To address these limitations, we propose a hybrid framework combining 3D CNN and Transformer architectures. The 3D CNN module extracts low-level spatiotemporal features, while the Transformer module captures long-range temporal dependencies, with a fusion mechanism integrating both representations. Evaluated on benchmark datasets, the proposed model outperforms traditional 3D CNN and standalone Transformers, achieving higher recognition accuracy with manageable complexity. Ablation studies further validate the complementary strengths of the two modules. This hybrid framework offers an effective and scalable solution for video-based behavior recognition.', 'abstract_zh': '基于视频的行为识别在公共安全、智能监控和人机交互等领域至关重要。为了克服传统3D卷积神经网络（3D CNN）在捕捉长距离依赖性方面的不足以及变换器在全局上下文信息学习上的高计算成本问题，我们提出了一种结合3D CNN和变换器架构的混合框架。该框架中的3D CNN模块提取低层次的时空特征，变换器模块捕捉长距离时间依赖性，并通过融合机制整合两种表示。在基准数据集上的评估表明，所提出模型的识别准确性较高，且具有可管理的复杂度。进一步的消融研究进一步验证了两个模块互补的优势。该混合框架为基于视频的行为识别提供了一种有效且可扩展的解决方案。', 'title_zh': '一种结合3D CNN和Transformer的视频行为识别框架'}
