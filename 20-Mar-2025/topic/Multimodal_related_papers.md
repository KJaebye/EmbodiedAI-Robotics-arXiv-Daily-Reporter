# EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining 

**Title (ZH)**: EgoDTM: 面向3Daware 自我中心视频-语言预训练 

**Authors**: Boshen Xu, Yuting Mei, Xinbi Liu, Sipeng Zheng, Qin Jin  

**Link**: [PDF](https://arxiv.org/pdf/2503.15470)  

**Abstract**: Egocentric video-language pretraining has significantly advanced video representation learning. Humans perceive and interact with a fully 3D world, developing spatial awareness that extends beyond text-based understanding. However, most previous works learn from 1D text or 2D visual cues, such as bounding boxes, which inherently lack 3D understanding. To bridge this gap, we introduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained through large-scale 3D-aware video pretraining and video-text contrastive learning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently learn 3D-awareness from pseudo depth maps generated by depth estimation models. To further facilitate 3D-aware video pretraining, we enrich the original brief captions with hand-object visual cues by organically combining several foundation models. Extensive experiments demonstrate EgoDTM's superior performance across diverse downstream tasks, highlighting its superior 3D-aware visual understanding. Our code will be released at this https URL. 

**Abstract (ZH)**: 自视点视频-语言预训练显著推进了视频表示学习。自视点学习要求人类在三维空间中感知和互动，发展出超越文本理解的空间意识。然而，大多数先前工作主要从一维文本或二维视觉提示，如边界框中学习，这些提示本质上缺乏三维理解。为弥合这一差距，我们提出了EgoDTM，一种结合了轻量级三维意识解码器的自视点深度和文本感知模型，通过大规模三维意识视频预训练和视频-文本对比学习联合训练。为了进一步促进三维意识视频预训练，我们通过有机结合多个基础模型，丰富了原始简要说明，加入了手-物体视觉提示。广泛的实验表明，EgoDTM在多种下游任务中表现出卓越的性能，突显了其优越的三维意识视觉理解能力。我们的代码将在以下链接释放：这个 https URL。 

---
# Visual Position Prompt for MLLM based Visual Grounding 

**Title (ZH)**: 基于视觉定位提示的MLLM视觉 grounding 

**Authors**: Wei Tang, Yanpeng Sun, Qinying Gu, Zechao Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.15426)  

**Abstract**: Although Multimodal Large Language Models (MLLMs) excel at various image-related tasks, they encounter challenges in precisely aligning coordinates with spatial information within images, particularly in position-aware tasks such as visual grounding. This limitation arises from two key factors. First, MLLMs lack explicit spatial references, making it difficult to associate textual descriptions with precise image locations. Second, their feature extraction processes prioritize global context over fine-grained spatial details, leading to weak localization capability. To address this issue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt (VPP) to improve its grounding capability. VPP-LLaVA integrates two complementary mechanisms. The global VPP overlays learnable, axis-like embeddings onto the input image to provide structured spatial cues. The local VPP focuses on fine-grained localization by incorporating position-aware queries, which suggests probable object locations. We also introduce a VPP-SFT dataset with 0.6M samples, consolidating high-quality visual grounding data into a compact format for efficient model training. Training on this dataset with VPP enhances the model's performance, achieving state-of-the-art results on standard grounding benchmarks despite using fewer training samples compared to other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\sim$21M samples). The code and VPP-SFT dataset will be available at this https URL upon acceptance. 

**Abstract (ZH)**: 尽管多模态大型语言模型（MLLMs）在各种图像相关任务中表现出色，但在精确对齐坐标与图像中的空间信息方面仍面临挑战，特别是在位置感知任务（如视觉定位）中尤为明显。这一限制主要源自两个关键因素。首先，MLLMs缺乏明确的空间参考，使得难以将文本描述与精确的图像位置相关联。其次，它们的特征提取过程倾向于提取全局上下文信息，而忽略细粒度的空间细节，导致其定位能力较弱。为解决这一问题，我们提出了VPP-LLaVA，这是一种配备视觉位置提示（VPP）的MLLM，以提高其定位能力。VPP-LLaVA结合了两种互补机制。全局VPP通过在输入图像上叠加可学习的轴向嵌入，提供结构化空间线索。局部VPP则通过引入位置感知查询，专注于细粒度的定位，指出可能的对象位置。我们还引入了一个包含60万样本的VPP-SFT数据集，将高质量的视觉定位数据以紧凑格式整合，以便高效训练模型。在该数据集上进行训练可以提升模型性能，即使使用比其他MLLMs（如MiniGPT-v2）更少的训练样本（远少于210万样本），依然能达到标准定位基准上的先进结果。相关代码和VPP-SFT数据集将在接收后在此网址获取。 

---
# Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for Cross-modal Transfer 

**Title (ZH)**: 利用完美的多模态对齐和高斯假设进行跨模态迁移 

**Authors**: Abhi Kamboj, Minh N. Do  

**Link**: [PDF](https://arxiv.org/pdf/2503.15352)  

**Abstract**: Multimodal alignment aims to construct a joint latent vector space where two modalities representing the same concept map to the same vector. We formulate this as an inverse problem and show that under certain conditions perfect alignment can be achieved. We then address a specific application of alignment referred to as cross-modal transfer. Unsupervised cross-modal transfer aims to leverage a model trained with one modality to perform inference on another modality, without any labeled fine-tuning on the new modality. Assuming that semantic classes are represented as a mixture of Gaussians in the latent space, we show how cross-modal transfer can be performed by projecting the data points from the representation space onto different subspaces representing each modality. Our experiments on synthetic multimodal Gaussian data verify the effectiveness of our perfect alignment and cross-modal transfer method. We hope these findings inspire further exploration of the applications of perfect alignment and the use of Gaussian models for cross-modal learning. 

**Abstract (ZH)**: 多模态对齐旨在构建一个联合隐空间，在该空间中表示相同概念的两种模态映射到同一个向量。我们将这种对齐视为逆问题，并证明在某些条件下可以实现完美的对齐。随后，我们讨论了对齐的特定应用，即跨模态迁移。无监督的跨模态迁移旨在利用一种模态训练的模型在另一种模态上进行推理，而不需要对新模态进行任何标注的微调。假设语义类在隐空间中以高斯混合模型表示，我们展示了如何通过将数据点从表示空间投影到表示每个模态的不同子空间来进行跨模态迁移。我们对合成的多模态高斯数据的实验验证了我们完美对齐和跨模态迁移方法的有效性。我们希望这些发现能激发对完美对齐应用及其在跨模态学习中使用高斯模型的进一步探索。 

---
# Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU 

**Title (ZH)**: 超曲面与欧几里得多元对比学习中的机器忘记：适应MERU的对齐校准 

**Authors**: Àlex Pujol Vidal, Sergio Escalera, Kamal Nasrollahi, Thomas B. Moeslund  

**Link**: [PDF](https://arxiv.org/pdf/2503.15166)  

**Abstract**: Machine unlearning methods have become increasingly important for selective concept removal in large pre-trained models. While recent work has explored unlearning in Euclidean contrastive vision-language models, the effectiveness of concept removal in hyperbolic spaces remains unexplored. This paper investigates machine unlearning in hyperbolic contrastive learning by adapting Alignment Calibration to MERU, a model that embeds images and text in hyperbolic space to better capture semantic hierarchies. Through systematic experiments and ablation studies, we demonstrate that hyperbolic geometry offers distinct advantages for concept removal, achieving near perfect forgetting with reasonable performance on retained concepts, particularly when scaling to multiple concept removal. Our approach introduces hyperbolic-specific components including entailment calibration and norm regularization that leverage the unique properties of hyperbolic space. Comparative analysis with Euclidean models reveals fundamental differences in unlearning dynamics, with hyperbolic unlearning reorganizing the semantic hierarchy while Euclidean approaches merely disconnect cross-modal associations. These findings not only advance machine unlearning techniques but also provide insights into the geometric properties that influence concept representation and removal in multimodal models. Source code available at this https URL 

**Abstract (ZH)**: 机器未学习方法在双曲对比学习中的概念移除研究：通过将对齐校准适应MERU模型探究双曲空间中的概念移除效果 

---
# Shushing! Let's Imagine an Authentic Speech from the Silent Video 

**Title (ZH)**: 静音了！让我们想象一段真实的无声视频中的演讲。 

**Authors**: Jiaxin Ye, Hongming Shan  

**Link**: [PDF](https://arxiv.org/pdf/2503.14928)  

**Abstract**: Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: this https URL. 

**Abstract (ZH)**: 基于视觉引导的语音生成旨在从面部外观或唇部动作生成逼真语音，而不依赖于听觉信号，为电影配音等领域和帮助失声个体提供了显著的应用潜力。虽然近期取得了进展，但现有方法在从视觉线索中统一实现语义、音色和情感韵律的一致性方面仍面临挑战，为此我们提出了Consistent Video-to-Speech (CV2S) 作为增强跨模态一致性的一种扩展任务。为应对新兴挑战，我们引入了ImaginTalk，这是一种新颖的跨模态扩散框架，仅使用视觉输入生成忠实语音，并在离散空间内操作。具体而言，我们提出了一个离散唇部对齐器，从唇部视频中预测离散语音标记以捕捉语义信息，同时错误检测器识别错位的标记，随后通过掩码语言建模与BERT进行修正。为了进一步增强生成语音的表现力，我们开发了一种样式扩散变换器，配备了面部样式适配器，能够在通道和时间维度上自适应地定制身份和韵律动态，并确保与唇部意识语义特征的同步。大量实验表明，ImaginTalk相比现有最先进的基线能够生成更高保真度的语音，具有更准确的语义细节和更具表现力的音色和情感。展示演示请参见我们的项目页面：https://your-project-page-url。 

---
# POSTA: A Go-to Framework for Customized Artistic Poster Generation 

**Title (ZH)**: POSTA：一种定制化艺术海报生成的通用框架 

**Authors**: Haoyu Chen, Xiaojie Xu, Wenbo Li, Jingjing Ren, Tian Ye, Songhua Liu, Ying-Cong Chen, Lei Zhu, Xinchao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2503.14908)  

**Abstract**: Poster design is a critical medium for visual communication. Prior work has explored automatic poster design using deep learning techniques, but these approaches lack text accuracy, user customization, and aesthetic appeal, limiting their applicability in artistic domains such as movies and exhibitions, where both clear content delivery and visual impact are essential. To address these limitations, we present POSTA: a modular framework powered by diffusion models and multimodal large language models (MLLMs) for customized artistic poster generation. The framework consists of three modules. Background Diffusion creates a themed background based on user input. Design MLLM then generates layout and typography elements that align with and complement the background style. Finally, to enhance the poster's aesthetic appeal, ArtText Diffusion applies additional stylization to key text elements. The final result is a visually cohesive and appealing poster, with a fully modular process that allows for complete customization. To train our models, we develop the PosterArt dataset, comprising high-quality artistic posters annotated with layout, typography, and pixel-level stylized text segmentation. Our comprehensive experimental analysis demonstrates POSTA's exceptional controllability and design diversity, outperforming existing models in both text accuracy and aesthetic quality. 

**Abstract (ZH)**: Poster 设计是视觉通信的关键媒介。尽管已有研究利用深度学习技术探索自动 poster 设计，但这些方法缺乏文字准确性、用户个性化和审美吸引力，限制了其在电影和展览等艺术领域中的应用，而在这些领域中，清晰的内容传达和视觉冲击力至关重要。为了克服这些局限性，我们提出了 POSTA：一种基于扩散模型和多模态大型语言模型 (MLLMs) 的模块化框架，用于自定义艺术 poster 生成。该框架由三个模块组成。背景扩散模块根据用户输入创建主题背景。设计 MLLM 然后生成与背景风格相协调和互补的布局和字体元素。最后，为了增强 poster 的审美吸引力，ArtText 扩散模块对关键文字元素进行进一步的风格化处理。最终结果是一个视觉上连贯且有吸引力的 poster，整个过程完全模块化，允许完全的自定义。为了训练我们的模型，我们开发了 PosterArt 数据集，该数据集包含高质量的艺术 poster，并标注了布局、字体和像素级风格化文字分割。我们全面的实验分析表明，POSTA 在文本准确性和审美质量方面均优于现有模型，具备出色的可控性和设计多样性。 

---
# Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives 

**Title (ZH)**: 多模态大语言模型时代的产品描述评价：挑战与未来展望 

**Authors**: Sara Sarto, Marcella Cornia, Rita Cucchiara  

**Link**: [PDF](https://arxiv.org/pdf/2503.14604)  

**Abstract**: The evaluation of machine-generated image captions is a complex and evolving challenge. With the advent of Multimodal Large Language Models (MLLMs), image captioning has become a core task, increasing the need for robust and reliable evaluation metrics. This survey provides a comprehensive overview of advancements in image captioning evaluation, analyzing the evolution, strengths, and limitations of existing metrics. We assess these metrics across multiple dimensions, including correlation with human judgment, ranking accuracy, and sensitivity to hallucinations. Additionally, we explore the challenges posed by the longer and more detailed captions generated by MLLMs and examine the adaptability of current metrics to these stylistic variations. Our analysis highlights some limitations of standard evaluation approaches and suggests promising directions for future research in image captioning assessment. 

**Abstract (ZH)**: 机器生成图像描述的评估是一个复杂且不断演化的挑战。随着多模态大型语言模型（MLLMs）的发展，图像描述已成为一个核心任务，增加了对稳健且可靠的评估指标的需求。本文综述了图像描述评估方面的最新进展，分析了现有指标的发展、优势和局限性。我们从多个维度评估这些指标，包括与人类判断的相关性、排名准确性以及对幻觉的敏感性。此外，我们探讨了MLLMs生成的更长、更详细的描述所提出的挑战，并考察了当前指标对这些文体变化的适应性。我们的分析指出了标准评估方法的一些局限性，并提出了图像描述评估未来研究的有希望的方向。 

---
# Vision-Language Models for Acute Tuberculosis Diagnosis: A Multimodal Approach Combining Imaging and Clinical Data 

**Title (ZH)**: 基于视觉-语言模型的急性 tuberculosis 诊断：结合影像学和临床数据的多模态方法 

**Authors**: Ananya Ganapthy, Praveen Shastry, Naveen Kumarasami, Anandakumar D, Keerthana R, Mounigasri M, Varshinipriya M, Kishore Prasath Venkatesh, Bargava Subramanian, Kalyan Sivasailam  

**Link**: [PDF](https://arxiv.org/pdf/2503.14538)  

**Abstract**: Background: This study introduces a Vision-Language Model (VLM) leveraging SIGLIP and Gemma-3b architectures for automated acute tuberculosis (TB) screening. By integrating chest X-ray images and clinical notes, the model aims to enhance diagnostic accuracy and efficiency, particularly in resource-limited settings.
Methods: The VLM combines visual data from chest X-rays with clinical context to generate detailed, context-aware diagnostic reports. The architecture employs SIGLIP for visual encoding and Gemma-3b for decoding, ensuring effective representation of acute TB-specific pathologies and clinical insights.
Results: Key acute TB pathologies, including consolidation, cavities, and nodules, were detected with high precision (97percent) and recall (96percent). The model demonstrated strong spatial localization capabilities and robustness in distinguishing TB-positive cases, making it a reliable tool for acute TB diagnosis.
Conclusion: The multimodal capability of the VLM reduces reliance on radiologists, providing a scalable solution for acute TB screening. Future work will focus on improving the detection of subtle pathologies and addressing dataset biases to enhance its generalizability and application in diverse global healthcare settings. 

**Abstract (ZH)**: 背景：本研究介绍了利用SIGLIP和Gemma-3b架构的视觉语言模型（VLM），用于自动化急性结核病（TB）筛查。通过整合胸部X光图像和临床笔记，该模型旨在提高诊断的准确性和效率，特别是在资源有限的环境中。

方法：VLM将胸部X光图像的视觉数据与临床上下文结合起来生成详细、情境相关的诊断报告。该架构使用SIGLIP进行视觉编码，使用Gemma-3b进行解码，确保有效表示急性TB特定的病理和临床见解。

结果：研究检测了包括实变、空洞和结节在内的主要急性TB病理，精度为97%，召回率为96%。该模型展示了强大的空间定位能力和在区分TB阳性病例方面的鲁棒性，使其成为急性TB诊断的可靠工具。

结论：VLM的多模态能力减少了对放射科医生的依赖，提供了一个可扩展的解决方案以供急性TB筛查使用。未来工作将重点提高对细微病理的检测能力，并解决数据集偏差问题，以增强其适应性和在全球不同医疗保健环境中的普遍适用性。 

---
# Advancing Chronic Tuberculosis Diagnostics Using Vision-Language Models: A Multi modal Framework for Precision Analysis 

**Title (ZH)**: 使用视觉语言模型推进慢性 tuberculosis 诊断：精确分析的多模态框架 

**Authors**: Praveen Shastry, Sowmya Chowdary Muthulur, Naveen Kumarasami, Anandakumar D, Mounigasri M, Keerthana R, Kishore Prasath Venkatesh, Bargava Subramanian, Kalyan Sivasailam, Revathi Ezhumalai, Abitha Marimuthu  

**Link**: [PDF](https://arxiv.org/pdf/2503.14536)  

**Abstract**: Background This study proposes a Vision-Language Model (VLM) leveraging the SIGLIP encoder and Gemma-3b transformer decoder to enhance automated chronic tuberculosis (TB) screening. By integrating chest X-ray images with clinical data, the model addresses the challenges of manual interpretation, improving diagnostic consistency and accessibility, particularly in resource-constrained settings.
Methods The VLM architecture combines a Vision Transformer (ViT) for visual encoding and a transformer-based text encoder to process clinical context, such as patient histories and treatment records. Cross-modal attention mechanisms align radiographic features with textual information, while the Gemma-3b decoder generates comprehensive diagnostic reports. The model was pre-trained on 5 million paired medical images and texts and fine-tuned using 100,000 chronic TB-specific chest X-rays.
Results The model demonstrated high precision (94 percent) and recall (94 percent) for detecting key chronic TB pathologies, including fibrosis, calcified granulomas, and bronchiectasis. Area Under the Curve (AUC) scores exceeded 0.93, and Intersection over Union (IoU) values were above 0.91, validating its effectiveness in detecting and localizing TB-related abnormalities.
Conclusion The VLM offers a robust and scalable solution for automated chronic TB diagnosis, integrating radiographic and clinical data to deliver actionable and context-aware insights. Future work will address subtle pathologies and dataset biases to enhance the model's generalizability, ensuring equitable performance across diverse populations and healthcare settings. 

**Abstract (ZH)**: 基于SIGLIP编码器和Gemma-3b变压器解码器的视觉语言模型在增强自动化慢性肺结核筛查中的应用 

---
# Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control 

**Title (ZH)**: Cafe-Talk：多模态细粒度和粗粒度控制生成3D交谈人脸动画 

**Authors**: Hejia Chen, Haoxian Zhang, Shoulong Zhang, Xiaoqiang Liu, Sisi Zhuang, Yuan Zhang, Pengfei Wan, Di Zhang, Shuai Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.14517)  

**Abstract**: Speech-driven 3D talking face method should offer both accurate lip synchronization and controllable expressions. Previous methods solely adopt discrete emotion labels to globally control expressions throughout sequences while limiting flexible fine-grained facial control within the spatiotemporal domain. We propose a diffusion-transformer-based 3D talking face generation model, Cafe-Talk, which simultaneously incorporates coarse- and fine-grained multimodal control conditions. Nevertheless, the entanglement of multiple conditions challenges achieving satisfying performance. To disentangle speech audio and fine-grained conditions, we employ a two-stage training pipeline. Specifically, Cafe-Talk is initially trained using only speech audio and coarse-grained conditions. Then, a proposed fine-grained control adapter gradually adds fine-grained instructions represented by action units (AUs), preventing unfavorable speech-lip synchronization. To disentangle coarse- and fine-grained conditions, we design a swap-label training mechanism, which enables the dominance of the fine-grained conditions. We also devise a mask-based CFG technique to regulate the occurrence and intensity of fine-grained control. In addition, a text-based detector is introduced with text-AU alignment to enable natural language user input and further support multimodal control. Extensive experimental results prove that Cafe-Talk achieves state-of-the-art lip synchronization and expressiveness performance and receives wide acceptance in fine-grained control in user studies. Project page: this https URL 

**Abstract (ZH)**: 基于语音驱动的3D Talking Face方法应同时提供精确的唇同步和可控的表情。以往方法仅采用离散的情绪标签对整个序列进行全局控制，而在时空域内限制了灵活的细粒度面部控制。我们提出了一种基于扩散变压器的3D Talking Face生成模型Cafe-Talk，该模型同时融入了粗粒度和细粒度的多模态控制条件。然而，多种条件的交织给实现满意性能带来了挑战。为了解开语音音频和细粒度条件之间的纠缠，我们采用了两阶段训练管道。具体来说，Cafe-Talk首先仅使用语音音频和粗粒度条件进行训练。然后，通过一个提出的细粒度控制适配器逐步添加由动作单位（AUs）表示的细粒度指令，防止不理想的语音-唇同步。为了分离粗粒度和细粒度条件，我们设计了一种换标签训练机制，使细粒度条件占据主导地位。我们还设计了一种基于掩码的CFG技术，以调节细粒度控制的发生和强度。此外，我们引入了一种基于文本的检测器，该检测器与文本-AU对齐，以便接受自然语言用户输入并进一步支持多模态控制。广泛的实验结果证明，Cafe-Talk在唇同步和表现力方面达到了最先进的性能，并在用户研究中获得了广泛接受。项目页面：这个 https URL 

---
