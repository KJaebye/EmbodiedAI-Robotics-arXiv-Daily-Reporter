{'arxiv_id': 'arXiv:2503.14837', 'title': 'SemanticFlow: A Self-Supervised Framework for Joint Scene Flow Prediction and Instance Segmentation in Dynamic Environments', 'authors': 'Yinqi Chen, Meiying Zhang, Qi Hao, Guang Zhou', 'link': 'https://arxiv.org/abs/2503.14837', 'abstract': 'Accurate perception of dynamic traffic scenes is crucial for high-level autonomous driving systems, requiring robust object motion estimation and instance segmentation. However, traditional methods often treat them as separate tasks, leading to suboptimal performance, spatio-temporal inconsistencies, and inefficiency in complex scenarios due to the absence of information sharing. This paper proposes a multi-task SemanticFlow framework to simultaneously predict scene flow and instance segmentation of full-resolution point clouds. The novelty of this work is threefold: 1) developing a coarse-to-fine prediction based multi-task scheme, where an initial coarse segmentation of static backgrounds and dynamic objects is used to provide contextual information for refining motion and semantic information through a shared feature processing module; 2) developing a set of loss functions to enhance the performance of scene flow estimation and instance segmentation, while can help ensure spatial and temporal consistency of both static and dynamic objects within traffic scenes; 3) developing a self-supervised learning scheme, which utilizes coarse segmentation to detect rigid objects and compute their transformation matrices between sequential frames, enabling the generation of self-supervised labels. The proposed framework is validated on the Argoverse and Waymo datasets, demonstrating superior performance in instance segmentation accuracy, scene flow estimation, and computational efficiency, establishing a new benchmark for self-supervised methods in dynamic scene understanding.', 'abstract_zh': '多任务SemanticFlow框架：同时预测全分辨率点云的场景流和实例分割', 'title_zh': 'SemanticFlow：动态环境中超分辨率场景流预测与实例分割的自监督框架'}
{'arxiv_id': 'arXiv:2503.14537', 'title': 'Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive Survey', 'authors': 'Liewen Liao, Weihao Yan, Ming Yang, Songan Zhang', 'link': 'https://arxiv.org/abs/2503.14537', 'abstract': 'Learning-based 3D reconstruction has emerged as a transformative technique in autonomous driving, enabling precise modeling of both dynamic and static environments through advanced neural representations. Despite augmenting perception, 3D reconstruction inspires pioneering solution for vital tasks in the field of autonomous driving, such as scene understanding and closed-loop simulation. Commencing with an examination of input modalities, we investigates the details of 3D reconstruction and conducts a multi-perspective, in-depth analysis of recent advancements. Specifically, we first provide a systematic introduction of preliminaries, including data formats, benchmarks and technical preliminaries of learning-based 3D reconstruction, facilitating instant identification of suitable methods based on hardware configurations and sensor suites. Then, we systematically review learning-based 3D reconstruction methods in autonomous driving, categorizing approaches by subtasks and conducting multi-dimensional analysis and summary to establish a comprehensive technical reference. The development trends and existing challenges is summarized in the context of learning-based 3D reconstruction in autonomous driving. We hope that our review will inspire future researches.', 'abstract_zh': '基于学习的3D重建已成为自主驾驶领域的变革性技术，通过先进的神经表示实现对动态和静态环境的精准建模。虽然增强了感知能力，3D重建为自主驾驶领域的关键任务，如场景理解与闭环模拟，提供了开创性的解决方案。本文首先探讨输入模态，详细介绍3D重建技术，并从多角度进行深入分析。具体而言，我们首先系统地介绍了预备知识，包括数据格式、基准测试和技术预备，方便根据硬件配置和传感器套件即时识别合适的方法。然后，我们系统性地回顾了自主驾驶中的基于学习的3D重建方法，按照子任务进行分类，并进行多维度分析和总结，建立全面的技术参考。在基于学习的3D重建的背景下，总结了发展趋势和现有挑战。我们希望通过此次回顾激发未来的研究。', 'title_zh': '基于学习的自动驾驶中3D重建综述'}
{'arxiv_id': 'arXiv:2503.15417', 'title': 'Temporal Regularization Makes Your Video Generator Stronger', 'authors': 'Harold Haodong Chen, Haojian Huang, Xianfeng Wu, Yexin Liu, Yajing Bai, Wen-Jie Shu, Harry Yang, Ser-Nam Lim', 'link': 'https://arxiv.org/abs/2503.15417', 'abstract': 'Temporal quality is a critical aspect of video generation, as it ensures consistent motion and realistic dynamics across frames. However, achieving high temporal coherence and diversity remains challenging. In this work, we explore temporal augmentation in video generation for the first time, and introduce FluxFlow for initial investigation, a strategy designed to enhance temporal quality. Operating at the data level, FluxFlow applies controlled temporal perturbations without requiring architectural modifications. Extensive experiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow significantly improves temporal coherence and diversity across various video generation models, including U-Net, DiT, and AR-based architectures, while preserving spatial fidelity. These findings highlight the potential of temporal augmentation as a simple yet effective approach to advancing video generation quality.', 'abstract_zh': 'Temporal 增强在视频生成中的探索：FluxFlow 的初步研究及其对时间一致性与多样性的提升', 'title_zh': '时间正则化让你的视频生成器更强 Wrest'}
{'arxiv_id': 'arXiv:2503.15342', 'title': 'TruthLens:A Training-Free Paradigm for DeepFake Detection', 'authors': 'Ritabrata Chakraborty, Rajatsubhra Chakraborty, Ali Khaleghi Rahimian, Thomas MacDougall', 'link': 'https://arxiv.org/abs/2503.15342', 'abstract': 'The proliferation of synthetic images generated by advanced AI models poses significant challenges in identifying and understanding manipulated visual content. Current fake image detection methods predominantly rely on binary classification models that focus on accuracy while often neglecting interpretability, leaving users without clear insights into why an image is deemed real or fake. To bridge this gap, we introduce TruthLens, a novel training-free framework that reimagines deepfake detection as a visual question-answering (VQA) task. TruthLens utilizes state-of-the-art large vision-language models (LVLMs) to observe and describe visual artifacts and combines this with the reasoning capabilities of large language models (LLMs) like GPT-4 to analyze and aggregate evidence into informed decisions. By adopting a multimodal approach, TruthLens seamlessly integrates visual and semantic reasoning to not only classify images as real or fake but also provide interpretable explanations for its decisions. This transparency enhances trust and provides valuable insights into the artifacts that signal synthetic content. Extensive evaluations demonstrate that TruthLens outperforms conventional methods, achieving high accuracy on challenging datasets while maintaining a strong emphasis on explainability. By reframing deepfake detection as a reasoning-driven process, TruthLens establishes a new paradigm in combating synthetic media, combining cutting-edge performance with interpretability to address the growing threats of visual disinformation.', 'abstract_zh': '合成图像生成技术的迅猛发展对识别和理解篡改视觉内容构成了重大挑战。当前的假图像检测方法主要依赖于注重准确性的二分类模型，但往往忽视了可解释性，使得用户无法清晰了解为何某张图被判定为真实或虚假。为弥合这一差距，我们引入了TruthLens，这是一种无需训练的新颖框架，重新构想了深度伪造检测为视觉问答任务（VQA）。TruthLens 利用最先进的大型视觉-语言模型（LVLM）观察和描述视觉特征，并结合大型语言模型（LLMs）如GPT-4的推理能力，分析和汇总证据为明智的决策。通过采用多模态方法，TruthLens 平滑地整合了视觉和语义推理，不仅能够将图像分类为真实或虚假，还能为决策提供可解释的解释。这种透明度提升了信任并提供了关于指示合成内容的特征的关键见解。广泛评估表明，TruthLens 在性能和解释性方面均优于传统方法，能够在具有挑战性的数据集上实现高精度。通过将深度伪造检测重新构想为推理驱动的过程，TruthLens 建立了对抗合成媒体的新范式，结合了顶级性能与解释性，以应对日益严重的视觉虚假信息威胁。', 'title_zh': 'TruthLens：一种无需训练的深度伪造检测范式'}
{'arxiv_id': 'arXiv:2503.15185', 'title': '3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation', 'authors': 'Gyeongrok Oh, Sungjune Kim, Heeju Ko, Hyung-gun Chi, Jinkyu Kim, Dongwook Lee, Daehyun Ji, Sungjoon Choi, Sujin Jang, Sangpil Kim', 'link': 'https://arxiv.org/abs/2503.15185', 'abstract': 'The resolution of voxel queries significantly influences the quality of view transformation in camera-based 3D occupancy prediction. However, computational constraints and the practical necessity for real-time deployment require smaller query resolutions, which inevitably leads to an information loss. Therefore, it is essential to encode and preserve rich visual details within limited query sizes while ensuring a comprehensive representation of 3D occupancy. To this end, we introduce ProtoOcc, a novel occupancy network that leverages prototypes of clustered image segments in view transformation to enhance low-resolution context. In particular, the mapping of 2D prototypes onto 3D voxel queries encodes high-level visual geometries and complements the loss of spatial information from reduced query resolutions. Additionally, we design a multi-perspective decoding strategy to efficiently disentangle the densely compressed visual cues into a high-dimensional 3D occupancy scene. Experimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the effectiveness of the proposed method, showing clear improvements over the baselines. More importantly, ProtoOcc achieves competitive performance against the baselines even with 75\\% reduced voxel resolution.', 'abstract_zh': '基于相机的3D占用率预测中的体素查询分辨率显著影响视图变换的质量。然而，计算约束和实时部署的实际需求要求较小的查询分辨率，这不可避免地会导致信息丢失。因此，在有限的查询尺寸内编码和保留丰富的视觉细节并确保3D占用率的全面表示是至关重要的。为此，我们引入了ProtoOcc，一种新颖的占用网络，通过视图变换中聚类图像片段的原型来增强低分辨率上下文。特别是，将2D原型映射到3D体素查询中，编码高层次的视觉几何结构，补充了由于降低查询分辨率而导致的空间信息损失。另外，我们设计了一种多视角解码策略，以高效地将密集压缩的视觉线索解码为高维3D占用场景。在Occ3D和SemanticKITTI基准上的实验结果证明了所提方法的有效性，显示出在基线方法上的明显改进。更重要的是，即使体素分辨率降低75%，ProtoOcc仍能达到与基线方法竞争力相当的性能。', 'title_zh': '基于原型感知视图变换的低分辨率查询三维占用预测'}
{'arxiv_id': 'arXiv:2503.15126', 'title': 'Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action Segmentation', 'authors': 'Haoyu Ji, Bowen Chen, Weihong Ren, Wenze Huang, Zhihao Yang, Zhiyong Wang, Honghai Liu', 'link': 'https://arxiv.org/abs/2503.15126', 'abstract': 'Skeleton-based Temporal Action Segmentation (STAS) aims to segment and recognize various actions from long, untrimmed sequences of human skeletal movements. Current STAS methods typically employ spatio-temporal modeling to establish dependencies among joints as well as frames, and utilize one-hot encoding with cross-entropy loss for frame-wise classification supervision. However, these methods overlook the intrinsic correlations among joints and actions within skeletal features, leading to a limited understanding of human movements. To address this, we propose a Text-Derived Relational Graph-Enhanced Network (TRG-Net) that leverages prior graphs generated by Large Language Models (LLM) to enhance both modeling and supervision. For modeling, the Dynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived Joint Graphs (TJG) with channel- and frame-level dynamic adaptation to effectively model spatial relations, while integrating spatio-temporal core features during temporal modeling. For supervision, the Absolute-Relative Inter-Class Supervision (ARIS) method employs contrastive learning between action features and text embeddings to regularize the absolute class distributions, and utilizes Text-Derived Action Graphs (TAG) to capture the relative inter-class relationships among action features. Additionally, we propose a Spatial-Aware Enhancement Processing (SAEP) method, which incorporates random joint occlusion and axial rotation to enhance spatial generalization. Performance evaluations on four public datasets demonstrate that TRG-Net achieves state-of-the-art results.', 'abstract_zh': '基于骨架的时间动作分割（STAS）旨在从长未修剪的人体骨架运动序列中分割和识别各种动作。当前的STAS方法通常采用时空建模来建立关节之间的依赖关系以及帧之间的依赖关系，并使用one-hot编码加交叉熵损失对帧级分类监督。然而，这些方法忽略了骨架特征中关节和动作的内在相关性，导致对人类运动的理解有限。为了解决这个问题，我们提出了一种基于文本驱动关系图增强网络（TRG-Net），该网络利用大型语言模型（LLM）生成的先验图来增强建模和监督。在建模方面，动态时空融合建模（DSFM）方法结合了文本驱动关节图（TJG），并采用通道级和帧级动态适应来有效建模空间关系，在时间建模过程中整合时空核心特征。在监督方面，绝对相对跨类监督（ARIS）方法通过对照学习动作特征和文本嵌入来正则化绝对类分布，并利用文本驱动动作图（TAG）捕获动作特征之间的相对跨类关系。此外，我们还提出了一种空间感知增强处理（SAEP）方法，该方法通过随机关节遮挡和轴向旋转来增强空间泛化能力。在四个公开数据集上的性能评估表明，TRG-Net取得了当前最先进的成果。', 'title_zh': '基于文本衍生关系图增强的骨架基动作分割网络'}
{'arxiv_id': 'arXiv:2503.15058', 'title': 'Texture-Aware StarGAN for CT data harmonisation', 'authors': 'Francesco Di Feola, Ludovica Pompilio, Cecilia Assolito, Valerio Guarrasi, Paolo Soda', 'link': 'https://arxiv.org/abs/2503.15058', 'abstract': 'Computed Tomography (CT) plays a pivotal role in medical diagnosis; however, variability across reconstruction kernels hinders data-driven approaches, such as deep learning models, from achieving reliable and generalized performance. To this end, CT data harmonization has emerged as a promising solution to minimize such non-biological variances by standardizing data across different sources or conditions. In this context, Generative Adversarial Networks (GANs) have proved to be a powerful framework for harmonization, framing it as a style-transfer problem. However, GAN-based approaches still face limitations in capturing complex relationships within the images, which are essential for effective harmonization. In this work, we propose a novel texture-aware StarGAN for CT data harmonization, enabling one-to-many translations across different reconstruction kernels. Although the StarGAN model has been successfully applied in other domains, its potential for CT data harmonization remains unexplored. Furthermore, our approach introduces a multi-scale texture loss function that embeds texture information across different spatial and angular scales into the harmonization process, effectively addressing kernel-induced texture variations. We conducted extensive experimentation on a publicly available dataset, utilizing a total of 48667 chest CT slices from 197 patients distributed over three different reconstruction kernels, demonstrating the superiority of our method over the baseline StarGAN.', 'abstract_zh': '计算机断层扫描（CT）在医学诊断中发挥着关键作用；然而，不同重建内核之间的差异阻碍了深度学习等数据驱动方法的可靠性和通用性表现。为此，CT数据谐调作为减轻这种非生物学差异的一种有前途的解决方案应运而生，它通过标准化来自不同来源或条件的数据来最小化这些差异。在这个背景下，生成对抗网络（GANs）已被证明是谐调的一种强大框架，将其视为一种风格迁移问题。然而，基于GAN的方法仍然难以捕捉图像中复杂的相互关系，这对于有效的谐调至关重要。在本文中，我们提出了一种新的纹理感知StarGAN用于CT数据谐调，使其能够在不同的重建内核之间实现一对多的翻译。尽管StarGAN模型已在其他领域取得成功应用，但其在CT数据谐调领域的潜力尚未得到探索。此外，我们的方法引入了一种多尺度纹理损失函数，将不同空间和角度尺度的纹理信息嵌入到谐调过程，有效解决了由内核引起的纹理变化。我们通过在公开可用的数据集上进行广泛的实验，利用197名患者共计48667个胸部CT切片分布在三个不同的重建内核中，展示了我们方法相对于基线StarGAN的优越性。', 'title_zh': '面向纹理的StarGAN在CT数据调和中的应用'}
{'arxiv_id': 'arXiv:2503.15049', 'title': 'HAD-Gen: Human-like and Diverse Driving Behavior Modeling for Controllable Scenario Generation', 'authors': 'Cheng Wang, Lingxin Kong, Massimiliano Tamborski, Stefano V. Albrecht', 'link': 'https://arxiv.org/abs/2503.15049', 'abstract': 'Simulation-based testing has emerged as an essential tool for verifying and validating autonomous vehicles (AVs). However, contemporary methodologies, such as deterministic and imitation learning-based driver models, struggle to capture the variability of human-like driving behavior. Given these challenges, we propose HAD-Gen, a general framework for realistic traffic scenario generation that simulates diverse human-like driving behaviors. The framework first clusters the vehicle trajectory data into different driving styles according to safety features. It then employs maximum entropy inverse reinforcement learning on each of the clusters to learn the reward function corresponding to each driving style. Using these reward functions, the method integrates offline reinforcement learning pre-training and multi-agent reinforcement learning algorithms to obtain general and robust driving policies. Multi-perspective simulation results show that our proposed scenario generation framework can simulate diverse, human-like driving behaviors with strong generalization capability. The proposed framework achieves a 90.96% goal-reaching rate, an off-road rate of 2.08%, and a collision rate of 6.91% in the generalization test, outperforming prior approaches by over 20% in goal-reaching performance. The source code is released at this https URL.', 'abstract_zh': '基于仿真测试的自动驾驶车辆真实交通场景生成框架：HAD-Gen', 'title_zh': 'HAD-Gen: 类人且多样的驾驶行为建模以实现可控场景生成'}
{'arxiv_id': 'arXiv:2503.15008', 'title': 'A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary Learning for Breast Cancer Detection', 'authors': 'Aamir Mehmood, Yue Hu, Saddam Hussain Khan', 'link': 'https://arxiv.org/abs/2503.15008', 'abstract': 'Recent advancements in detecting tumors using deep learning on breast ultrasound images (BUSI) have demonstrated significant success. Deep CNNs and vision-transformers (ViTs) have demonstrated individually promising initial performance. However, challenges related to model complexity and contrast, texture, and tumor morphology variations introduce uncertainties that hinder the effectiveness of current methods. This study introduces a novel hybrid framework, CB-Res-RBCMT, combining customized residual CNNs and new ViT components for detailed BUSI cancer analysis. The proposed RBCMT uses stem convolution blocks with CNN Meet Transformer (CMT) blocks, followed by new Regional and boundary (RB) feature extraction operations for capturing contrast and morphological variations. Moreover, the CMT block incorporates global contextual interactions through multi-head attention, enhancing computational efficiency with a lightweight design. Additionally, the customized inverse residual and stem CNNs within the CMT effectively extract local texture information and handle vanishing gradients. Finally, the new channel-boosted (CB) strategy enriches the feature diversity of the limited dataset by combining the original RBCMT channels with transfer learning-based residual CNN-generated maps. These diverse channels are processed through a spatial attention block for optimal pixel selection, reducing redundancy and improving the discrimination of minor contrast and texture variations. The proposed CB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of 96.42%, and precision of 94.79% on the standard harmonized stringent BUSI dataset, outperforming existing ViT and CNN methods. These results demonstrate the versatility of our integrated CNN-Transformer framework in capturing diverse features and delivering superior performance in BUSI cancer diagnosis.', 'abstract_zh': 'Recent advancements in detecting tumors using deep learning on breast ultrasound images (BUSI) have demonstrated significant success. Deep CNNs and vision-transformers (ViTs) have demonstrated individually promising initial performance. However, challenges related to model complexity and contrast, texture, and tumor morphology variations introduce uncertainties that hinder the effectiveness of current methods. This study introduces a novel hybrid framework, CB-Res-RBCMT, combining customized residual CNNs and new ViT components for detailed BUSI cancer analysis.', 'title_zh': '一种基于信道增强残差CNN-Transformer与区域边界的乳腺癌检测方法'}
{'arxiv_id': 'arXiv:2503.14950', 'title': 'USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and Scene Depth Estimation using Features from a Pre-trained Image Segmentation network', 'authors': 'Joseph Emmanuel DL Dayo, Prospero C. Naval Jr', 'link': 'https://arxiv.org/abs/2503.14950', 'abstract': 'The increasing demand for high-accuracy depth estimation in autonomous driving and augmented reality applications necessitates advanced neural architectures capable of effectively leveraging multiple data modalities. In this context, we introduce the Unified Segmentation Attention Mechanism Network (USAM-Net), a novel convolutional neural network that integrates stereo image inputs with semantic segmentation maps and attention to enhance depth estimation performance. USAM-Net employs a dual-pathway architecture, which combines a pre-trained segmentation model (SAM) and a depth estimation model. The segmentation pathway preprocesses the stereo images to generate semantic masks, which are then concatenated with the stereo images as inputs to the depth estimation pathway. This integration allows the model to focus on important features such as object boundaries and surface textures which are crucial for accurate depth perception. Empirical evaluation on the DrivingStereo dataset demonstrates that USAM-Net achieves superior performance metrics, including a Global Difference (GD) of 3.61\\% and an End-Point Error (EPE) of 0.88, outperforming traditional models such as CFNet, SegStereo, and iResNet. These results underscore the effectiveness of integrating segmentation information into stereo depth estimation tasks, highlighting the potential of USAM-Net in applications demanding high-precision depth data.', 'abstract_zh': '统一分割注意力机制网络（USAM-Net）：结合语义分割和注意力机制的立体深度估计', 'title_zh': 'USAM-Net：一种基于U-Net的网络，结合预训练图像分割网络的特征以改进立体匹配和场景深度估计'}
{'arxiv_id': 'arXiv:2503.14935', 'title': 'FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding', 'authors': 'Chongjun Tu, Lin Zhang, Pengtao Chen, Peng Ye, Xianfang Zeng, Wei Cheng, Gang Yu, Tao Chen', 'link': 'https://arxiv.org/abs/2503.14935', 'abstract': 'Multimodal Large Language Models (MLLMs) have shown remarkable capabilities in video content understanding but still struggle with fine-grained motion comprehension. To comprehensively assess the motion understanding ability of existing MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with structured manual annotations of various motions. Our benchmark includes both close-ended and open-ended tasks. For close-ended evaluation, we carefully design 8,184 multiple-choice question-answer pairs spanning six distinct sub-tasks. For open-ended evaluation, we develop both a novel cost-efficient LLM-free and a GPT-assisted caption assessment method, where the former can enhance benchmarking interpretability and reproducibility. Comprehensive experiments with 21 state-of-the-art MLLMs reveal significant limitations in their ability to comprehend and describe detailed temporal dynamics in video motions. To alleviate this limitation, we further build FAVOR-Train, a dataset consisting of 17,152 videos with fine-grained motion annotations. The results of finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on motion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive assessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train provide valuable tools to the community for developing more powerful video understanding models. Project page: \\href{this https URL}{this https URL}.', 'abstract_zh': '多模态大型语言模型（MLLMs）在视频内容理解方面展现出显著的能力，但仍难以理解细微动作。为全面评估现有MLLMs的动作理解能力，我们引入了FAAVOR-Bench，包含1,776个视频，并附有结构化的手动标注的各种动作。基准包括封闭式和开放式任务。对于封闭式评估，我们精心设计了8,184个多选题-答案对，涵盖六个不同的子任务。对于开放式评估，我们开发了一种成本效益高且无需大型语言模型的方法和一种基于GPT的字幕评估方法，前者可提高基准的可解释性和可再现性。全面实验表明，当前最先进的MLLMs在理解视频动作的详细时序动态方面存在显著局限。为此，我们进一步构建了FAAVOR-Train数据集，包含17,152个细粒度动作标注的视频。基于FAAVOR-Train微调Qwen2.5-VL在TVBench、MotionBench和FAAVOR-Bench上的性能均得到一致提升。全面评估结果表明，提出的FAAVOR-Bench和FAAVOR-Train为开发更强大的视频理解模型提供了有价值的工具。项目页面：\\href{this https URL}{this https URL}。', 'title_zh': 'FAVOR-Bench: 一种全面的细粒度视频运动理解基准'}
{'arxiv_id': 'arXiv:2503.14881', 'title': 'Exploring the Limits of KV Cache Compression in Visual Autoregressive Transformers', 'authors': 'Bo Chen, Xiaoyu Li, Yekun Ke, Yingyu Liang, Zhenmei Shi, Zhao Song', 'link': 'https://arxiv.org/abs/2503.14881', 'abstract': 'A fundamental challenge in Visual Autoregressive models is the substantial memory overhead required during inference to store previously generated representations. Despite various attempts to mitigate this issue through compression techniques, prior works have not explicitly formalized the problem of KV-cache compression in this context. In this work, we take the first step in formally defining the KV-cache compression problem for Visual Autoregressive transformers. We then establish a fundamental negative result, proving that any mechanism for sequential visual token generation under attention-based architectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log n)$, where $n$ is the number of tokens generated and $d$ is the embedding dimensionality. This result demonstrates that achieving truly sub-quadratic memory usage is impossible without additional structural constraints. Our proof is constructed via a reduction from a computational lower bound problem, leveraging randomized embedding techniques inspired by dimensionality reduction principles. Finally, we discuss how sparsity priors on visual representations can influence memory efficiency, presenting both impossibility results and potential directions for mitigating memory overhead.', 'abstract_zh': '视觉自回归模型中的一个基本挑战是在推理过程中存储之前生成的表示所需的大量内存开销。尽管通过压缩技术已尝试减轻这一问题，但先前的工作尚未明确形式化这种上下文中的KV缓存压缩问题。在本文中，我们首次正式定义了视觉自回归变压器的KV缓存压缩问题。然后，我们建立了基本的消极结果，证明在注意力机制架构下，任何顺序生成视觉标记的机制必须至少使用$\\Omega(n^2 d)$内存，其中当$d=\\Omega(\\log n)$时，$n$是生成的标记数量，$d$是嵌入维度。这一结果表明，在没有额外结构约束的情况下实现真正亚二次内存使用是不可能的。我们的证明是通过从计算下界问题的归约构建的，利用基于降维原理的随机嵌入技术。最后，我们讨论了视觉表示的稀疏先验如何影响内存效率，并提出了不可能结果以及减轻内存开销的潜在方向。', 'title_zh': '探索视觉自回归 transformer 中 KV 缓存压缩的极限'}
{'arxiv_id': 'arXiv:2503.14779', 'title': 'Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution', 'authors': 'Akram Khatami-Rizi, Ahmad Mahmoudi-Aznaveh', 'link': 'https://arxiv.org/abs/2503.14779', 'abstract': 'Single Image Super-Resolution (SISR) aims to reconstruct high-resolution (HR) images from low-resolution (LR) inputs. Deep learning, especially Convolutional Neural Networks (CNNs), has advanced SISR. However, increasing network depth increases parameters, and memory usage, and slows training, which is problematic for resource-limited devices. To address this, lightweight models are developed to balance accuracy and efficiency. We propose the Involution & BSConv Multi-Depth Distillation Network (IBMDN), combining Involution & BSConv Multi-Depth Distillation Block (IBMDB) and the Contrast and High-Frequency Attention Block (CHFAB). IBMDB integrates Involution and BSConv to balance computational efficiency and feature extraction. CHFAB enhances high-frequency details for better visual quality. IBMDB is compatible with other SISR architectures and reduces complexity, improving evaluation metrics like PSNR and SSIM. In transformer-based models, IBMDB reduces memory usage while improving feature extraction. In GANs, it enhances perceptual quality, balancing pixel-level accuracy with perceptual details. Our experiments show that the method achieves high accuracy with minimal computational cost. The code is available at GitHub.', 'abstract_zh': '单张图像超分辨率（SISR）旨在从低分辨率（LR）输入重建高分辨率（HR）图像。深度学习，尤其是卷积神经网络（CNNs），推动了SISR的发展。然而，网络深度的增加会导致参数和内存使用量增加，并减慢训练速度，这在资源受限的设备上是一个问题。为解决这个问题，开发了轻量化模型以平衡准确性和效率。我们提出了结合Involution & BSConv多层蒸馏块（IBMDB）和对比与高频注意力块（CHFAB）的Involution & BSConv多层蒸馏网络（IBMDN）。IBMDB将Involution和BSConv结合以平衡计算效率和特征提取。CHFAB增强高频细节以获得更好的视觉质量。IBMDB与其他SISR架构兼容，降低复杂性，改善像PSNR和SSIM这样的评估指标。在基于变压器的模型中，IBMDB在提高特征提取的同时减少内存使用。在生成对抗网络（GANs）中，它增强感知质量，平衡像素级准确性和感知细节。我们的实验表明，该方法以最小的计算成本实现了高精度。代码可在GitHub获得。', 'title_zh': '基于进化的BSConv多深度精炼网络的轻量级图像超分辨率'}
{'arxiv_id': 'arXiv:2503.14716', 'title': 'Construction Site Scaffolding Completeness Detection Based on Mask R-CNN and Hough Transform', 'authors': 'Pei-Hsin Lin, Jacob J. Lin, Shang-Hsien Hsieh', 'link': 'https://arxiv.org/abs/2503.14716', 'abstract': "Construction site scaffolding is essential for many building projects, and ensuring its safety is crucial to prevent accidents. The safety inspector must check the scaffolding's completeness and integrity, where most violations occur. The inspection process includes ensuring all the components are in the right place since workers often compromise safety for convenience and disassemble parts such as cross braces. This paper proposes a deep learning-based approach to detect the scaffolding and its cross braces using computer vision. A scaffold image dataset with annotated labels is used to train a convolutional neural network (CNN) model. With the proposed approach, we can automatically detect the completeness of cross braces from images taken at construction sites, without the need for manual inspection, saving a significant amount of time and labor costs. This non-invasive and efficient solution for detecting scaffolding completeness can help improve safety in construction sites.", 'abstract_zh': '基于深度学习的计算机视觉方法在施工现场检测脚手架及其剪刀撑的研究', 'title_zh': '基于Mask R-CNN和霍夫变换的建筑工地脚手架完整性检测'}
{'arxiv_id': 'arXiv:2503.14640', 'title': 'Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making in Vision Transformer', 'authors': 'Yi Liao, Yongsheng Gao, Weichuan Zhang', 'link': 'https://arxiv.org/abs/2503.14640', 'abstract': 'Various Vision Transformer (ViT) models have been widely used for image recognition tasks. However, existing visual explanation methods can not display the attention flow hidden inside the inner structure of ViT models, which explains how the final attention regions are formed inside a ViT for its decision-making. In this paper, a novel visual explanation approach, Dynamic Accumulated Attention Map (DAAM), is proposed to provide a tool that can visualize, for the first time, the attention flow from the top to the bottom through ViT networks. To this end, a novel decomposition module is proposed to construct and store the spatial feature information by unlocking the [class] token generated by the self-attention module of each ViT block. The module can also obtain the channel importance coefficients by decomposing the classification score for supervised ViT models. Because of the lack of classification score in self-supervised ViT models, we propose dimension-wise importance weights to compute the channel importance coefficients. Such spatial features are linearly combined with the corresponding channel importance coefficients, forming the attention map for each block. The dynamic attention flow is revealed by block-wisely accumulating each attention map. The contribution of this work focuses on visualizing the evolution dynamic of the decision-making attention for any intermediate block inside a ViT model by proposing a novel decomposition module and dimension-wise importance weights. The quantitative and qualitative analysis consistently validate the effectiveness and superior capacity of the proposed DAAM for not only interpreting ViT models with the fully-connected layers as the classifier but also self-supervised ViT models. The code is available at this https URL.', 'abstract_zh': '动态积累注意力图：一种可视化ViT模型决策注意力流的新方法', 'title_zh': '视觉变换器中决策演化解释的动态累积注意力图'}
{'arxiv_id': 'arXiv:2503.14568', 'title': 'Teaching Artificial Intelligence to Perform Rapid, Resolution-Invariant Grain Growth Modeling via Fourier Neural Operator', 'authors': 'Iman Peivaste, Ahmed Makradi, Salim Belouettar', 'link': 'https://arxiv.org/abs/2503.14568', 'abstract': 'Microstructural evolution, particularly grain growth, plays a critical role in shaping the physical, optical, and electronic properties of materials. Traditional phase-field modeling accurately simulates these phenomena but is computationally intensive, especially for large systems and fine spatial resolutions. While machine learning approaches have been employed to accelerate simulations, they often struggle with resolution dependence and generalization across different grain scales. This study introduces a novel approach utilizing Fourier Neural Operator (FNO) to achieve resolution-invariant modeling of microstructure evolution in multi-grain systems. FNO operates in the Fourier space and can inherently handle varying resolutions by learning mappings between function spaces. By integrating FNO with the phase field method, we developed a surrogate model that significantly reduces computational costs while maintaining high accuracy across different spatial scales. We generated a comprehensive dataset from phase-field simulations using the Fan Chen model, capturing grain evolution over time. Data preparation involved creating input-output pairs with a time shift, allowing the model to predict future microstructures based on current and past states. The FNO-based neural network was trained using sequences of microstructures and demonstrated remarkable accuracy in predicting long-term evolution, even for unseen configurations and higher-resolution grids not encountered during training.', 'abstract_zh': '利用傅里叶神经运算子实现多晶体系微观结构演化的一致分辨率建模', 'title_zh': '通过傅里叶神经算子教学人工 Intelligence 进行快速、分辨率无关的晶粒生长建模'}
{'arxiv_id': 'arXiv:2503.14562', 'title': 'Analysis of human visual field information using machine learning methods and assessment of their accuracy', 'authors': 'A.I. Medvedeva, V.V. Bakutkin', 'link': 'https://arxiv.org/abs/2503.14562', 'abstract': 'Subject of research: is the study of methods for analyzing perimetric images for the diagnosis and control of glaucoma diseases. Objects of research: is a dataset collected on the ophthalmological perimeter with the results of various patient pathologies, since the ophthalmological community is acutely aware of the issue of disease control and import substitution. [5]. Purpose of research: is to consider various machine learning methods that can classify glaucoma. This is possible thanks to the classifier built after labeling the dataset. It is able to determine from the image whether the visual fields depicted on it are the results of the impact of glaucoma on the eyes or other visual diseases. Earlier in the work [3], a dataset was described that was collected on the Tomey perimeter. The average age of the examined patients ranged from 30 to 85 years. Methods of research: machine learning methods for classifying image results (stochastic gradient descent, logistic regression, random forest, naive Bayes). Main results of research: the result of the study is computer modeling that can determine from the image whether the result is glaucoma or another disease (binary classification).', 'abstract_zh': '研究主题：基于视网膜周边成像的方法研究，用于青光眼疾病的诊断与控制  \n研究对象：来自眼科学 perimeter 的数据集，包含多种患者病理结果，鉴于眼科学界对疾病控制和进口替代的紧迫需求。  \n研究目的：考虑可用于分类青光眼的各种机器学习方法。这得益于对数据集进行标注后构建的分类器，能够从图像中判断所显示的视野是否为青光眼的影响结果或其他视觉疾病的结果。此前，在文献[3]中描述了一个在 Tomey perimeter 上收集的数据集。受检患者平均年龄在30至85岁之间。  \n研究方法：图像结果分类的机器学习方法（随机梯度下降、逻辑回归、随机森林、朴素贝叶斯）。  \n主要研究结果：研究结果是计算机建模，可以从图像中确定结果是青光眼还是其他疾病（二元分类）。', 'title_zh': '使用机器学习方法分析人类视觉场信息及其准确性评估'}
{'arxiv_id': 'arXiv:2503.14552', 'title': 'Fire and Smoke Datasets in 20 Years: An In-depth Review', 'authors': 'Sayed Pedram Haeri Boroujeni, Niloufar Mehrabi, Fatemeh Afghah, Connor Peter McGrath, Danish Bhatkar, Mithilesh Anil Biradar, Abolfazl Razi', 'link': 'https://arxiv.org/abs/2503.14552', 'abstract': 'Fire and smoke phenomena pose a significant threat to the natural environment, ecosystems, and global economy, as well as human lives and wildlife. In this particular circumstance, there is a demand for more sophisticated and advanced technologies to implement an effective strategy for early detection, real-time monitoring, and minimizing the overall impacts of fires on ecological balance and public safety. Recently, the rapid advancement of Artificial Intelligence (AI) and Computer Vision (CV) frameworks has substantially revolutionized the momentum for developing efficient fire management systems. However, these systems extensively rely on the availability of adequate and high-quality fire and smoke data to create proficient Machine Learning (ML) methods for various tasks, such as detection and monitoring. Although fire and smoke datasets play a critical role in training, evaluating, and testing advanced Deep Learning (DL) models, a comprehensive review of the existing datasets is still unexplored. For this purpose, we provide an in-depth review to systematically analyze and evaluate fire and smoke datasets collected over the past 20 years. We investigate the characteristics of each dataset, including type, size, format, collection methods, and geographical diversities. We also review and highlight the unique features of each dataset, such as imaging modalities (RGB, thermal, infrared) and their applicability for different fire management tasks (classification, segmentation, detection). Furthermore, we summarize the strengths and weaknesses of each dataset and discuss their potential for advancing research and technology in fire management. Ultimately, we conduct extensive experimental analyses across different datasets using several state-of-the-art algorithms, such as ResNet-50, DeepLab-V3, and YoloV8.', 'abstract_zh': '火和烟现象对自然环境、生态系统、全球经济以及人类生活和野生动物构成重大威胁。在这种情况下，需要更加先进和 sophisticated 的技术来实施有效的早期检测、实时监控和减少火灾对生态平衡和公共安全的总体影响策略。近年来，人工智能（AI）和计算机视觉（CV）框架的快速发展极大地推动了高效火灾管理系统的开发。然而，这些系统广泛依赖于充足的高质量火灾和烟雾数据，以便为各种任务（如检测和监控）创建高效的机器学习（ML）方法。尽管火灾和烟雾数据在训练、评估和测试先进深度学习（DL）模型中发挥着关键作用，但现有数据集的全面回顾尚未探索。为此，我们提供了一项深入的回顾，系统地分析和评估过去20年收集的火灾和烟雾数据集。我们调查了每个数据集的特点，包括类型、大小、格式、采集方法以及地理多样性。我们还回顾并突出了每个数据集的独特特点，例如成像模态（RGB、热成像、红外）及其在不同火灾管理任务（分类、分割、检测）中的适用性。此外，我们总结了每个数据集的优点和缺点，并讨论了它们在火灾管理研究和技术进步中的潜在价值。最终，我们使用多个最先进的算法（如ResNet-50、DeepLab-V3和YoloV8）在不同数据集上进行了广泛的实验分析。', 'title_zh': '二十年来火灾与烟雾数据集：深入回顾'}
{'arxiv_id': 'arXiv:2503.14542', 'title': 'AI-Driven Rapid Identification of Bacterial and Fungal Pathogens in Blood Smears of Septic Patients', 'authors': 'Agnieszka Sroka-Oleksiak, Adam Pardyl, Dawid Rymarczyk, Aldona Olechowska-Jarząb, Katarzyna Biegun-Drożdż, Dorota Ochońska, Michał Wronka, Adriana Borowa, Tomasz Gosiewski, Miłosz Adamczyk, Henryk Telega, Bartosz Zieliński, Monika Brzychczy-Włoch', 'link': 'https://arxiv.org/abs/2503.14542', 'abstract': 'Sepsis is a life-threatening condition which requires rapid diagnosis and treatment. Traditional microbiological methods are time-consuming and expensive. In response to these challenges, deep learning algorithms were developed to identify 14 bacteria species and 3 yeast-like fungi from microscopic images of Gram-stained smears of positive blood samples from sepsis patients.\nA total of 16,637 Gram-stained microscopic images were used in the study. The analysis used the Cellpose 3 model for segmentation and Attention-based Deep Multiple Instance Learning for classification. Our model achieved an accuracy of 77.15% for bacteria and 71.39% for fungi, with ROC AUC of 0.97 and 0.88, respectively. The highest values, reaching up to 96.2%, were obtained for Cutibacterium acnes, Enterococcus faecium, Stenotrophomonas maltophilia and Nakaseomyces glabratus. Classification difficulties were observed in closely related species, such as Staphylococcus hominis and Staphylococcus haemolyticus, due to morphological similarity, and within Candida albicans due to high morphotic diversity.\nThe study confirms the potential of our model for microbial classification, but it also indicates the need for further optimisation and expansion of the training data set. In the future, this technology could support microbial diagnosis, reducing diagnostic time and improving the effectiveness of sepsis treatment due to its simplicity and accessibility. Part of the results presented in this publication was covered by a patent application at the European Patent Office EP24461637.1 "A computer implemented method for identifying a microorganism in a blood and a data processing system therefor".', 'abstract_zh': '深度学习算法在Gram染色血样显微镜图像中鉴定14种细菌和3种酵母菌株的研究', 'title_zh': '基于AI的快速识别菌血症患者血液涂片中细菌和真菌病原体方法'}
{'arxiv_id': 'arXiv:2503.14535', 'title': 'Interpretable Unsupervised Joint Denoising and Enhancement for Real-World low-light Scenarios', 'authors': 'Huaqiu Li, Xiaowan Hu, Haoqian Wang', 'link': 'https://arxiv.org/abs/2503.14535', 'abstract': 'Real-world low-light images often suffer from complex degradations such as local overexposure, low brightness, noise, and uneven illumination. Supervised methods tend to overfit to specific scenarios, while unsupervised methods, though better at generalization, struggle to model these degradations due to the lack of reference images. To address this issue, we propose an interpretable, zero-reference joint denoising and low-light enhancement framework tailored for real-world scenarios. Our method derives a training strategy based on paired sub-images with varying illumination and noise levels, grounded in physical imaging principles and retinex theory. Additionally, we leverage the Discrete Cosine Transform (DCT) to perform frequency domain decomposition in the sRGB space, and introduce an implicit-guided hybrid representation strategy that effectively separates intricate compounded degradations. In the backbone network design, we develop retinal decomposition network guided by implicit degradation representation mechanisms. Extensive experiments demonstrate the superiority of our method. Code will be available at this https URL.', 'abstract_zh': '现实世界中的低光照图像常常遭受复杂退化的影响，如局部过曝、低亮度、噪声和不均匀光照。监督方法往往会过度拟合到特定场景中，而无监督方法虽然在泛化能力上更优，但由于缺乏参考图像而在建模这些退化时遇到困难。为解决这一问题，我们提出了一种针对现实世界场景的可解释、无参考联合去噪和低光照增强框架。该方法基于具有不同光照和噪声水平的配对子图像训练策略，并基于物理成像原理和retinex理论。此外，我们利用离散余弦变换（DCT）在sRGB空间中进行频域分解，并引入了一种隐式引导的混合表示策略，有效地分离复杂的复合退化。在网络设计方面，我们提出了由隐式退化表示机制引导的视网膜分解网络。广泛的经验表明，我们的方法具有优越性。代码将在以下链接处提供：this https URL。', 'title_zh': '可解释的无监督联合去噪与增强方法及其在实际低光场景中的应用'}
