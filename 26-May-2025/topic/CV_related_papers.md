# ExoGait-MS: Learning Periodic Dynamics with Multi-Scale Graph Network for Exoskeleton Gait Recognition 

**Title (ZH)**: ExoGait-MS：基于多尺度图网络学习周期动力学的外骨骼步态识别方法 

**Authors**: Lijiang Liu, Junyu Shi, Yong Sun, Zhiyuan Zhang, Jinni Zhou, Shugen Ma, Qiang Nie  

**Link**: [PDF](https://arxiv.org/pdf/2505.18018)  

**Abstract**: Current exoskeleton control methods often face challenges in delivering personalized treatment. Standardized walking gaits can lead to patient discomfort or even injury. Therefore, personalized gait is essential for the effectiveness of exoskeleton robots, as it directly impacts their adaptability, comfort, and rehabilitation outcomes for individual users. To enable personalized treatment in exoskeleton-assisted therapy and related applications, accurate recognition of personal gait is crucial for implementing tailored gait control. The key challenge in gait recognition lies in effectively capturing individual differences in subtle gait features caused by joint synergy, such as step frequency and step length. To tackle this issue, we propose a novel approach, which uses Multi-Scale Global Dense Graph Convolutional Networks (GCN) in the spatial domain to identify latent joint synergy patterns. Moreover, we propose a Gait Non-linear Periodic Dynamics Learning module to effectively capture the periodic characteristics of gait in the temporal domain. To support our individual gait recognition task, we have constructed a comprehensive gait dataset that ensures both completeness and reliability. Our experimental results demonstrate that our method achieves an impressive accuracy of 94.34% on this dataset, surpassing the current state-of-the-art (SOTA) by 3.77%. This advancement underscores the potential of our approach to enhance personalized gait control in exoskeleton-assisted therapy. 

**Abstract (ZH)**: 当前的外骨骼控制方法往往难以提供个性化的治疗。标准化的行走模式可能导致患者不适甚至受伤。因此，个性化的步态对于外骨骼机器人的有效性至关重要，因为它直接影响其适应性、舒适性和个体用户的康复效果。为了在外骨骼辅助治疗及相关应用中实现个性化的治疗，准确识别个人步态至关重要，以便实施定制化的步态控制。步态识别的关键挑战在于有效地捕捉由关节协同作用引起的细微步态特征差异，如步频和步长。为了解决这一问题，我们提出了一种新型方法，该方法在空间域使用多尺度全局密集图形卷积网络（GCN）来识别潜在的关节协同模式。此外，我们提出了一种步态非线性周期动态学习模块，以有效地在时间域捕捉步态的周期特性。为了支持我们的个体步态识别任务，我们构建了一个全面的步态数据集，确保其完整性和可靠性。实验结果表明，我们的方法在该数据集上的准确率达到94.34%，超过当前最先进的方法（SOTA）3.77%。这一进展突显了我们方法在增强外骨骼辅助治疗中个性化步态控制方面的潜力。 

---
# Is Single-View Mesh Reconstruction Ready for Robotics? 

**Title (ZH)**: 单视图网格重建准备好应对机器人技术挑战了吗？ 

**Authors**: Frederik Nolte, Bernhard Schölkopf, Ingmar Posner  

**Link**: [PDF](https://arxiv.org/pdf/2505.17966)  

**Abstract**: This paper evaluates single-view mesh reconstruction models for creating digital twin environments in robot manipulation. Recent advances in computer vision for 3D reconstruction from single viewpoints present a potential breakthrough for efficiently creating virtual replicas of physical environments for robotics contexts. However, their suitability for physics simulations and robotics applications remains unexplored. We establish benchmarking criteria for 3D reconstruction in robotics contexts, including handling typical inputs, producing collision-free and stable reconstructions, managing occlusions, and meeting computational constraints. Our empirical evaluation using realistic robotics datasets shows that despite success on computer vision benchmarks, existing approaches fail to meet robotics-specific requirements. We quantitively examine limitations of single-view reconstruction for practical robotics implementation, in contrast to prior work that focuses on multi-view approaches. Our findings highlight critical gaps between computer vision advances and robotics needs, guiding future research at this intersection. 

**Abstract (ZH)**: 本文评估了单视角网格重建模型在机器人操作中创建数字孪生环境的应用。随着计算机视觉在单视角三维重建方面的 recent 进展，为机器人学上下文高效创建物理环境的虚拟副本提供了潜在突破。然而，现有方法在物理仿真和机器人应用中的适用性尚未得到探索。我们为机器人学上下文的三维重建建立了基准评估标准，包括处理典型输入、生成无碰撞且稳定的重建、管理遮挡以及满足计算约束。使用现实的机器人数据集进行的经验评估表明，尽管在计算机视觉基准测试中取得成功，现有方法仍无法满足机器人特定的要求。我们定量分析了单视角重建在实际机器人实施中的局限性，与之前主要关注多视角方法的研究不同。我们的发现突显了计算机视觉进展与机器人需求之间的关键差距，为该交叉领域的未来研究指明了方向。 

---
# ConvoyNext: A Scalable Testbed Platform for Cooperative Autonomous Vehicle Systems 

**Title (ZH)**: ConvoyNext：一个可扩展的协作自动驾驶车辆系统测试床平台 

**Authors**: Hossein Maghsoumi, Yaser Fallah  

**Link**: [PDF](https://arxiv.org/pdf/2505.17275)  

**Abstract**: The advancement of cooperative autonomous vehicle systems depends heavily on effective coordination between multiple agents, aiming to enhance traffic efficiency, fuel economy, and road safety. Despite these potential benefits, real-world testing of such systems remains a major challenge and is essential for validating control strategies, trajectory modeling methods, and communication robustness across diverse environments. To address this need, we introduce ConvoyNext, a scalable, modular, and extensible platform tailored for the real-world evaluation of cooperative driving behaviors. We demonstrate the capabilities of ConvoyNext through a series of experiments involving convoys of autonomous vehicles navigating complex trajectories. These tests highlight the platform's robustness across heterogeneous vehicle configurations and its effectiveness in assessing convoy behavior under varying communication conditions, including intentional packet loss. Our results validate ConvoyNext as a comprehensive, open-access testbed for advancing research in cooperative autonomous vehicle systems. 

**Abstract (ZH)**: 合作自动驾驶车辆系统的发展高度依赖于多智能体之间的有效协调，旨在提高交通效率、燃油经济性和道路安全。尽管这些潜在好处显著，但在实际环境中的测试仍然是一个重大挑战，对于验证控制策略、轨迹建模方法和通信鲁棒性至关重要。为应对这一需求，我们引入了ConvoyNext平台，这是一个可扩展、模块化和可扩展的平台，专为合作驾驶行为的实际评估而设计。通过涉及自动驾驶车辆编队导航复杂轨迹的一系列实验，我们展示了ConvoyNext的功能。这些测试强调了该平台在异构车辆配置下的鲁棒性，并评估了在不同通信条件下编队行为的有效性，包括有意的包丢失。我们的结果验证了ConvoyNext作为合作自动驾驶车辆系统研究综合性开放式测试平台的有效性。 

---
# Knot So Simple: A Minimalistic Environment for Spatial Reasoning 

**Title (ZH)**: 结不那么简单：一个简洁的空间推理环境 

**Authors**: Zizhao Chen, Yoav Artzi  

**Link**: [PDF](https://arxiv.org/pdf/2505.18028)  

**Abstract**: We propose KnotGym, an interactive environment for complex, spatial reasoning and manipulation. KnotGym includes goal-oriented rope manipulation tasks with varying levels of complexity, all requiring acting from pure image observations. Tasks are defined along a clear and quantifiable axis of complexity based on the number of knot crossings, creating a natural generalization test. KnotGym has a simple observation space, allowing for scalable development, yet it highlights core challenges in integrating acute perception, spatial reasoning, and grounded manipulation. We evaluate methods of different classes, including model-based RL, model-predictive control, and chain-of-thought reasoning, and illustrate the challenges KnotGym presents. KnotGym is available at this https URL. 

**Abstract (ZH)**: 我们提出KnotGym，一个用于复杂空间推理和操作的互动环境。 

---
# MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity 

**Title (ZH)**: MinkUNeXt-SI: 基于点云的空间识别改进，包括球坐标和激光雷达强度 

**Authors**: Judith Vilella-Cantos, Juan José Cabrera, Luis Payá, Mónica Ballesta, David Valiente  

**Link**: [PDF](https://arxiv.org/pdf/2505.17591)  

**Abstract**: In autonomous navigation systems, the solution of the place recognition problem is crucial for their safe functioning. But this is not a trivial solution, since it must be accurate regardless of any changes in the scene, such as seasonal changes and different weather conditions, and it must be generalizable to other environments. This paper presents our method, MinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input data to obtain its spherical coordinates and intensity values normalized within a range of 0 to 1 for each point, and it produces a robust place recognition descriptor. To that end, a deep learning approach that combines Minkowski convolutions and a U-net architecture with skip connections is used. The results of MinkUNeXt-SI demonstrate that this method reaches and surpasses state-of-the-art performance while it also generalizes satisfactorily to other datasets. Additionally, we showcase the capture of a custom dataset and its use in evaluating our solution, which also achieves outstanding results. Both the code of our solution and the runs of our dataset are publicly available for reproducibility purposes. 

**Abstract (ZH)**: 在自主导航系统中，地点识别问题的解决方案对于其安全运行至关重要。但这并非一项简单的任务，因为它必须在场景任何变化（如季节变化和不同天气条件）下保持准确，并且能够泛化到其他环境中。本文提出了一种方法MinkUNeXt-SI，该方法从LiDAR点云出发，预处理输入数据以获得每个点的归一化在0到1范围内的球坐标和强度值，并生成稳健的地点识别描述符。为此，该方法使用结合Minkowski卷积和具有跳接连接的U-net架构的深度学习方法。MinkUNeXt-SI的实验结果表明，该方法不仅达到了最先进的性能，而且还能够在其他数据集中泛化得当。此外，我们展示了自定义数据集的采集及其在评估我们解决方案中的应用，该解决方案同样取得了出色的成果。我们的解决方案代码和数据集运行结果均已公开，以便于可重复性。 

---
# Distance Estimation in Outdoor Driving Environments Using Phase-only Correlation Method with Event Cameras 

**Title (ZH)**: 使用事件摄像头的相位-only相关性方法在户外驾驶环境中进行距离估计 

**Authors**: Masataka Kobayashi, Shintaro Shiba, Quan Kong, Norimasa Kobori, Tsukasa Shimizu, Shan Lu, Takaya Yamazato  

**Link**: [PDF](https://arxiv.org/pdf/2505.17582)  

**Abstract**: With the growing adoption of autonomous driving, the advancement of sensor technology is crucial for ensuring safety and reliable operation. Sensor fusion techniques that combine multiple sensors such as LiDAR, radar, and cameras have proven effective, but the integration of multiple devices increases both hardware complexity and cost. Therefore, developing a single sensor capable of performing multiple roles is highly desirable for cost-efficient and scalable autonomous driving systems.
Event cameras have emerged as a promising solution due to their unique characteristics, including high dynamic range, low latency, and high temporal resolution. These features enable them to perform well in challenging lighting conditions, such as low-light or backlit environments. Moreover, their ability to detect fine-grained motion events makes them suitable for applications like pedestrian detection and vehicle-to-infrastructure communication via visible light.
In this study, we present a method for distance estimation using a monocular event camera and a roadside LED bar. By applying a phase-only correlation technique to the event data, we achieve sub-pixel precision in detecting the spatial shift between two light sources. This enables accurate triangulation-based distance estimation without requiring stereo vision. Field experiments conducted in outdoor driving scenarios demonstrated that the proposed approach achieves over 90% success rate with less than 0.5-meter error for distances ranging from 20 to 60 meters.
Future work includes extending this method to full position estimation by leveraging infrastructure such as smart poles equipped with LEDs, enabling event-camera-based vehicles to determine their own position in real time. This advancement could significantly enhance navigation accuracy, route optimization, and integration into intelligent transportation systems. 

**Abstract (ZH)**: 随着自动驾驶的广泛采用，传感器技术的进步对于确保安全和可靠运行至关重要。结合多种传感器（如LiDAR、雷达和摄像头）的传感器融合技术已被证明是有效的，但多设备的集成增加了硬件复杂性和成本。因此，开发能够执行多重角色的单个传感器对于成本效益和可扩展的自动驾驶系统来说是非常 desirable 的。

事件相机由于其独特的特性，如高动态范围、低延迟和高时间分辨率，而被视为一种有前途的解决方案。这些特性使它们能够在低光照或背光等具有挑战性的光照条件下表现出色。此外，它们检测精细运动事件的能力使其适用于行人检测和基于可见光的车路通信等应用。

在本研究中，我们提出了一种使用单目事件相机和路边LED条形灯进行距离估算的方法。通过将相位唯一相关技术应用于事件数据，我们实现了在检测两个光源之间空间偏移方面的亚像素精度。这使我们能够实现基于三角测量的距离准确估算，而无需立体视觉技术。在户外驾驶场景下的实地实验表明，所提出的方法在20至60米的距离范围内误差小于0.5米的条件下，成功率超过90%。

未来的工作包括利用配备LED的智能路灯等基础设施将此方法扩展到全位置估计，使事件相机为基础的车辆能够实时确定自己的位置。这一进展可以显著提高导航准确性、路线优化，并实现与智能交通系统的集成。 

---
# Lightweight Multispectral Crop-Weed Segmentation for Precision Agriculture 

**Title (ZH)**: 轻量级多光谱作物-杂草分割技术及其在精准农业中的应用 

**Authors**: Zeynep Galymzhankyzy, Eric Martinson  

**Link**: [PDF](https://arxiv.org/pdf/2505.07444)  

**Abstract**: Efficient crop-weed segmentation is critical for site-specific weed control in precision agriculture. Conventional CNN-based methods struggle to generalize and rely on RGB imagery, limiting performance under complex field conditions. To address these challenges, we propose a lightweight transformer-CNN hybrid. It processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using specialized encoders and dynamic modality integration. Evaluated on the WeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of 78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7 million parameters, the model offers high accuracy, computational efficiency, and potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and edge devices, advancing precision weed management. 

**Abstract (ZH)**: 高效的作物-杂草分割对于精准农业中的定点杂草控制至关重要。传统的基于CNN的方法难以泛化且依赖RGB图像，这在复杂田间条件下限制了其性能。为解决这些挑战，我们提出了一种轻量级的Transformer-CNN混合模型。该模型使用专门的编码器处理RGB、近红外（NIR）和红边（RE）波段，并实现动态模态集成。在WeedsGalore数据集上，该模型实现了78.88%的分割准确率（mean IoU），比仅使用RGB的数据模型高出15.8个百分点。该模型仅有870万参数，提供高精度、计算效率，并具有在无人驾驶飞机（UAV）和边缘设备上实时部署的潜力，推动精准杂草管理的发展。 

---
# VideoGameBench: Can Vision-Language Models complete popular video games? 

**Title (ZH)**: VideoGameBench: 视觉-语言模型能完成流行的视频游戏吗？ 

**Authors**: Alex L. Zhang, Thomas L. Griffiths, Karthik R. Narasimhan, Ofir Press  

**Link**: [PDF](https://arxiv.org/pdf/2505.18134)  

**Abstract**: Vision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humans--such as perception, spatial navigation, and memory management--remains understudied. Real video games are crafted to be intuitive for humans to learn and master by leveraging innate inductive biases, making them an ideal testbed for evaluating such capabilities in VLMs. To this end, we introduce VideoGameBench, a benchmark consisting of 10 popular video games from the 1990s that VLMs directly interact with in real-time. VideoGameBench challenges models to complete entire games with access to only raw visual inputs and a high-level description of objectives and controls, a significant departure from existing setups that rely on game-specific scaffolding and auxiliary information. We keep three of the games secret to encourage solutions that generalize to unseen environments. Our experiments show that frontier vision-language models struggle to progress beyond the beginning of each game. We find inference latency to be a major limitation of frontier models in the real-time setting; therefore, we introduce VideoGameBench Lite, a setting where the game pauses while waiting for the LM's next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization of the human skills mentioned above into this benchmark motivates progress in these research directions. 

**Abstract (ZH)**: Vision-language模型在视频游戏中的能力评估：VideoGameBench 

---
# WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions 

**Title (ZH)**: 奇思妙玩：从单张图像和动作生成动态3D场景 

**Authors**: Zizhang Li, Hong-Xing Yu, Wei Liu, Yin Yang, Charles Herrmann, Gordon Wetzstein, Jiajun Wu  

**Link**: [PDF](https://arxiv.org/pdf/2505.18151)  

**Abstract**: WonderPlay is a novel framework integrating physics simulation with video generation for generating action-conditioned dynamic 3D scenes from a single image. While prior works are restricted to rigid body or simple elastic dynamics, WonderPlay features a hybrid generative simulator to synthesize a wide range of 3D dynamics. The hybrid generative simulator first uses a physics solver to simulate coarse 3D dynamics, which subsequently conditions a video generator to produce a video with finer, more realistic motion. The generated video is then used to update the simulated dynamic 3D scene, closing the loop between the physics solver and the video generator. This approach enables intuitive user control to be combined with the accurate dynamics of physics-based simulators and the expressivity of diffusion-based video generators. Experimental results demonstrate that WonderPlay enables users to interact with various scenes of diverse content, including cloth, sand, snow, liquid, smoke, elastic, and rigid bodies -- all using a single image input. Code will be made public. Project website: this https URL 

**Abstract (ZH)**: WonderPlay是一种将物理模拟与视频生成集成的新型框架，用于从单张图片生成动作条件下的动态3D场景。 

---
# Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding 

**Title (ZH)**: 深度视频发现：工具使用辅助的自主搜索在长视频理解中的应用 

**Authors**: Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu  

**Link**: [PDF](https://arxiv.org/pdf/2505.18079)  

**Abstract**: Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code will be released later. 

**Abstract (ZH)**: 长视频理解由于广泛的时空复杂性和在如此扩展的背景下回答问题的难度，提出了显著挑战。尽管大规模语言模型在视频分析能力和处理长上下文方面取得了显著进展，但在处理信息密集型一小时长的视频时仍表现出局限性。为克服这些局限性，我们提出Deep Video Discovery代理，利用分割视频片段上的代理式搜索策略。不同于以往的视频代理手动设计僵化的流程，我们的方法强调代理的自主性。通过在多粒度视频数据库上提供以搜索为中心的工具集，DVD代理利用大规模语言模型的高级推理能力，在当前观察状态规划，战略性地选择工具，制定适当的行动参数，并根据收集到的信息迭代优化其内部推理。我们在多个长视频理解基准上进行了全面评估，展示了整个系统设计的优势。我们的DVD代理在具有挑战性的LVBench数据集上显著超越了先前的工作，取得了SOTA性能。还提供了全面的消融研究和深入的工具分析，为针对长视频理解任务的智能代理的进一步发展提供了见解。代码将在稍后发布。 

---
# FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation 

**Title (ZH)**: FDBPL：基于蒸馏的更快(prompt学习)区域 aware 视觉-语言模型适应 

**Authors**: Zherui Zhang, Jiaxin Wu, Changwei Wang, Rongtao Xu, Longzhao Huang, Wenhao Xu, Wenbo Xu, Li Guo, Shibiao Xu  

**Link**: [PDF](https://arxiv.org/pdf/2505.18053)  

**Abstract**: Prompt learning as a parameter-efficient method that has been widely adopted to adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt design requires domain expertise and iterative optimization, soft-prompt methods rely heavily on task-specific hard labels, limiting their generalization to unseen categories. Recent popular distillation-based prompt learning methods improve generalization by exploiting larger teacher VLMs and unsupervised knowledge transfer, yet their repetitive teacher model online inference sacrifices the inherent training efficiency advantage of prompt learning. In this paper, we propose {\large {\textbf{F}}}aster {\large {\textbf{D}}}istillation-{\large {\textbf{B}}}ased {\large {\textbf{P}}}rompt {\large {\textbf{L}}}earning (\textbf{FDBPL}), which addresses these issues by sharing soft supervision contexts across multiple training stages and implementing accelerated I/O. Furthermore, FDBPL introduces a region-aware prompt learning paradigm with dual positive-negative prompt spaces to fully exploit randomly cropped regions that containing multi-level information. We propose a positive-negative space mutual learning mechanism based on similarity-difference learning, enabling student CLIP models to recognize correct semantics while learning to reject weakly related concepts, thereby improving zero-shot performance. Unlike existing distillation-based prompt learning methods that sacrifice parameter efficiency for generalization, FDBPL maintains dual advantages of parameter efficiency and strong downstream generalization. Comprehensive evaluations across 11 datasets demonstrate superior performance in base-to-new generalization, cross-dataset transfer, and robustness tests, achieving $2.2\times$ faster training speed. 

**Abstract (ZH)**: 更快的蒸馏基于提示学习（Faster Distillation-Based Prompt Learning, FDBPL） 

---
# RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration 

**Title (ZH)**: RestoreVAR：全面图像恢复的视觉自回归生成 

**Authors**: Sudarshan Rajagopalan, Kartik Narayan, Vishal M. Patel  

**Link**: [PDF](https://arxiv.org/pdf/2505.18047)  

**Abstract**: The use of latent diffusion models (LDMs) such as Stable Diffusion has significantly improved the perceptual quality of All-in-One image Restoration (AiOR) methods, while also enhancing their generalization capabilities. However, these LDM-based frameworks suffer from slow inference due to their iterative denoising process, rendering them impractical for time-sensitive applications. To address this, we propose RestoreVAR, a novel generative approach for AiOR that significantly outperforms LDM-based models in restoration performance while achieving over $\mathbf{10\times}$ faster inference. RestoreVAR leverages visual autoregressive modeling (VAR), a recently introduced approach which performs scale-space autoregression for image generation. VAR achieves comparable performance to that of state-of-the-art diffusion transformers with drastically reduced computational costs. To optimally exploit these advantages of VAR for AiOR, we propose architectural modifications and improvements, including intricately designed cross-attention mechanisms and a latent-space refinement module, tailored for the AiOR task. Extensive experiments show that RestoreVAR achieves state-of-the-art performance among generative AiOR methods, while also exhibiting strong generalization capabilities. 

**Abstract (ZH)**: 基于潜扩散模型的全在一ображ restoration 新方法 RestoreVAR：显著提升恢复性能与 inference 速度 

---
# AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models 

**Title (ZH)**: AutoMiSeg: 通过测试时适应基础模型实现自动医学图像分割 

**Authors**: Xingjian Li, Qifeng Wu, Colleen Que, Yiran Ding, Adithya S. Ubaradka, Jianhua Xing, Tianyang Wang, Min Xu  

**Link**: [PDF](https://arxiv.org/pdf/2505.17931)  

**Abstract**: Medical image segmentation is vital for clinical diagnosis, yet current deep learning methods often demand extensive expert effort, i.e., either through annotating large training datasets or providing prompts at inference time for each new case. This paper introduces a zero-shot and automatic segmentation pipeline that combines off-the-shelf vision-language and segmentation foundation models. Given a medical image and a task definition (e.g., "segment the optic disc in an eye fundus image"), our method uses a grounding model to generate an initial bounding box, followed by a visual prompt boosting module that enhance the prompts, which are then processed by a promptable segmentation model to produce the final mask. To address the challenges of domain gap and result verification, we introduce a test-time adaptation framework featuring a set of learnable adaptors that align the medical inputs with foundation model representations. Its hyperparameters are optimized via Bayesian Optimization, guided by a proxy validation model without requiring ground-truth labels. Our pipeline offers an annotation-efficient and scalable solution for zero-shot medical image segmentation across diverse tasks. Our pipeline is evaluated on seven diverse medical imaging datasets and shows promising results. By proper decomposition and test-time adaptation, our fully automatic pipeline performs competitively with weakly-prompted interactive foundation models. 

**Abstract (ZH)**: 基于视觉-语言和分割基础模型的零样本自动医学图像分割管道 

---
# Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention 

**Title (ZH)**: 基于位置增强和多头跨注意力的对象级别跨视图地理定位 

**Authors**: Zheyang Huang, Jagannath Aryal, Saeid Nahavandi, Xuequan Lu, Chee Peng Lim, Lei Wei, Hailing Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2505.17911)  

**Abstract**: Cross-view geo-localization determines the location of a query image, captured by a drone or ground-based camera, by matching it to a geo-referenced satellite image. While traditional approaches focus on image-level localization, many applications, such as search-and-rescue, infrastructure inspection, and precision delivery, demand object-level accuracy. This enables users to prompt a specific object with a single click on a drone image to retrieve precise geo-tagged information of the object. However, variations in viewpoints, timing, and imaging conditions pose significant challenges, especially when identifying visually similar objects in extensive satellite imagery. To address these challenges, we propose an Object-level Cross-view Geo-localization Network (OCGNet). It integrates user-specified click locations using Gaussian Kernel Transfer (GKT) to preserve location information throughout the network. This cue is dually embedded into the feature encoder and feature matching blocks, ensuring robust object-specific localization. Additionally, OCGNet incorporates a Location Enhancement (LE) module and a Multi-Head Cross Attention (MHCA) module to adaptively emphasize object-specific features or expand focus to relevant contextual regions when necessary. OCGNet achieves state-of-the-art performance on a public dataset, CVOGL. It also demonstrates few-shot learning capabilities, effectively generalizing from limited examples, making it suitable for diverse applications (this https URL). 

**Abstract (ZH)**: 跨视角对象级地理定位网络（OCGNet）通过将用户指定的点击位置集成到高斯核转移（GKT）中，来在整个网络中保留位置信息。该提示信号被双渠道嵌入到特征编码器和特征匹配模块中，确保了对象特定的鲁棒定位。此外，OCGNet 还包含一个位置增强（LE）模块和一个多头跨注意力（MHCA）模块，以适应性地强调对象特定的特征或扩展关注到相关的上下文区域。OCGNet 在公开数据集 CVOGL 上取得了最先进的性能，并展示了少量样本学习能力，有效地从少量示例中泛化，使其适用于多种应用（请点击下方链接了解更多信息：https://github.com/XXX/Osubset）。 

---
# DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning 

**Title (ZH)**: DiffusionReward: 通过奖励反馈学习增强 blindness 面部恢复 

**Authors**: Bin Wu, Wei Wang, Yahui Liu, Zixiang Li, Yao Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2505.17910)  

**Abstract**: Reward Feedback Learning (ReFL) has recently shown great potential in aligning model outputs with human preferences across various generative tasks. In this work, we introduce a ReFL framework, named DiffusionReward, to the Blind Face Restoration task for the first time. DiffusionReward effectively overcomes the limitations of diffusion-based methods, which often fail to generate realistic facial details and exhibit poor identity consistency. The core of our framework is the Face Reward Model (FRM), which is trained using carefully annotated data. It provides feedback signals that play a pivotal role in steering the optimization process of the restoration network. In particular, our ReFL framework incorporates a gradient flow into the denoising process of off-the-shelf face restoration methods to guide the update of model parameters. The guiding gradient is collaboratively determined by three aspects: (i) the FRM to ensure the perceptual quality of the restored faces; (ii) a regularization term that functions as a safeguard to preserve generative diversity; and (iii) a structural consistency constraint to maintain facial fidelity. Furthermore, the FRM undergoes dynamic optimization throughout the process. It not only ensures that the restoration network stays precisely aligned with the real face manifold, but also effectively prevents reward hacking. Experiments on synthetic and wild datasets demonstrate that our method outperforms state-of-the-art methods, significantly improving identity consistency and facial details. The source codes, data, and models are available at: this https URL. 

**Abstract (ZH)**: Reward Feedback Learning在盲 Face 修复任务中的应用：一种名为DiffusionReward的框架 

---
# NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling 

**Title (ZH)**: NeuroTrails: 动态稀疏头作为有效ensemble的关键训练方法 

**Authors**: Bram Grooten, Farid Hasanov, Chenxiang Zhang, Qiao Xiao, Boqian Wu, Zahra Atashgahi, Ghada Sokar, Shiwei Liu, Lu Yin, Elena Mocanu, Mykola Pechenizkiy, Decebal Constantin Mocanu  

**Link**: [PDF](https://arxiv.org/pdf/2505.17909)  

**Abstract**: Model ensembles have long been a cornerstone for improving generalization and robustness in deep learning. However, their effectiveness often comes at the cost of substantial computational overhead. To address this issue, state-of-the-art methods aim to replicate ensemble-class performance without requiring multiple independently trained networks. Unfortunately, these algorithms often still demand considerable compute at inference. In response to these limitations, we introduce $\textbf{NeuroTrails}$, a sparse multi-head architecture with dynamically evolving topology. This unexplored model-agnostic training paradigm improves ensemble performance while reducing the required resources. We analyze the underlying reason for its effectiveness and observe that the various neural trails induced by dynamic sparsity attain a $\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays efficacy with convolutional and transformer-based architectures on computer vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4, among many others, demonstrate increased accuracy and stronger robustness in zero-shot generalization, while requiring significantly fewer parameters. 

**Abstract (ZH)**: 神经轨迹：一种动态演化拓扑的稀疏多头架构以提高集成性能并减少资源需求 

---
# RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection 

**Title (ZH)**: RQR3D: 重参数化回归目标以实现BEV基于的3D物体检测 

**Authors**: Ozsel Kilinc, Cem Tarhan  

**Link**: [PDF](https://arxiv.org/pdf/2505.17732)  

**Abstract**: Accurate, fast, and reliable 3D perception is essential for autonomous driving. Recently, bird's-eye view (BEV)-based perception approaches have emerged as superior alternatives to perspective-based solutions, offering enhanced spatial understanding and more natural outputs for planning. Existing BEV-based 3D object detection methods, typically adhering to angle-based representation, directly estimate the size and orientation of rotated bounding boxes. We observe that BEV-based 3D object detection is analogous to aerial oriented object detection, where angle-based methods are recognized for being affected by discontinuities in their loss functions. Drawing inspiration from this domain, we propose Restricted Quadrilateral Representation to define 3D regression targets. RQR3D regresses the smallest horizontal bounding box encapsulating the oriented box, along with the offsets between the corners of these two boxes, thereby transforming the oriented object detection problem into a keypoint regression task. RQR3D is compatible with any 3D object detection approach. We employ RQR3D within an anchor-free single-stage object detection method and introduce an objectness head to address class imbalance problem. Furthermore, we introduce a simplified radar fusion backbone that eliminates the need for voxel grouping and processes the BEV-mapped point cloud with standard 2D convolutions, rather than sparse convolutions. Extensive evaluations on the nuScenes dataset demonstrate that RQR3D achieves state-of-the-art performance in camera-radar 3D object detection, outperforming the previous best method by +4% in NDS and +2.4% in mAP, and significantly reducing the translation and orientation errors, which are crucial for safe autonomous driving. These consistent gains highlight the robustness, precision, and real-world readiness of our approach. 

**Abstract (ZH)**: 准确、快速且可靠的三维感知对于自动驾驶至关重要。基于鸟瞰视角（BEV）的感知方法近年来已成为优于基于视角解决方案的优选方案，提供了增强的空间理解能力和更自然的规划输出。现有的基于BEV的三维物体检测方法通常采用角度基表示法，直接估计旋转边界框的大小和方向。我们观察到基于BEV的三维物体检测类似于航空定向物体检测，其中角度基方法因其损失函数中的不连续性而受到影响。从这一领域中汲取灵感，我们提出了受限四边形表示法来定义三维回归目标。RQR3D回歸包容定向框的最小水平边界框及其两个盒子角点之间的偏移，从而将定向物体检测问题转换为关键点回归任务。RQR3D与任何三维物体检测方法兼容。我们将在无锚点的一阶段物体检测方法中使用RQR3D，并引入物体性头部以解决类别不平衡问题。此外，我们引入了一个简化版的雷达融合骨干网络，它消除了体素分组的需要，并使用标准的二维卷积而不是稀疏卷积处理BEV映射的点云。在nuScenes数据集上的广泛评估表明，RQR3D在相机-雷达三维物体检测中的性能达到最新水平，在NDS和mAP上分别超过之前最佳方法4%和2.4%，显著减少了对于安全自动驾驶至关重要的平移和方向误差。这些一致的改进突显了我们方法的鲁棒性、精确性和实际可用性。 

---
# ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection 

**Title (ZH)**: ViP$^2$-CLIP: 统一对齐的视觉感知提示在零样本异常检测中的应用 

**Authors**: Ziteng Yang, Jingzehua Xu, Yanshu Li, Zepeng Li, Yeqiang Wang, Xinghui Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.17692)  

**Abstract**: Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any target domain training samples, relying solely on external auxiliary data. Existing CLIP-based methods attempt to activate the model's ZSAD potential via handcrafted or static learnable prompts. The former incur high engineering costs and limited semantic coverage, whereas the latter apply identical descriptions across diverse anomaly types, thus fail to adapt to complex variations. Furthermore, since CLIP is originally pretrained on large-scale classification tasks, its anomaly segmentation quality is highly sensitive to the exact wording of class names, severely constraining prompting strategies that depend on class labels. To address these challenges, we introduce ViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception Prompting (ViP-Prompt) mechanism, which fuses global and multi-scale local visual context to adaptively generate fine-grained textual prompts, eliminating manual templates and class-name priors. This design enables our model to focus on precise abnormal regions, making it particularly valuable when category labels are ambiguous or privacy-constrained. Extensive experiments on 15 industrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves state-of-the-art performance and robust cross-domain generalization. 

**Abstract (ZH)**: 基于ViP$^{2}$-CLIP的零样本异常检测 

---
# Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection 

**Title (ZH)**: 用于IVH检测的精确脑超声分割的双注意力残差U-网络 

**Authors**: Dan Yuan, Yi Feng, Ziyun Tang  

**Link**: [PDF](https://arxiv.org/pdf/2505.17683)  

**Abstract**: Intraventricular hemorrhage (IVH) is a severe neurological complication among premature infants, necessitating early and accurate detection from brain ultrasound (US) images to improve clinical outcomes. While recent deep learning methods offer promise for computer-aided diagnosis, challenges remain in capturing both local spatial details and global contextual dependencies critical for segmenting brain anatomies. In this work, we propose an enhanced Residual U-Net architecture incorporating two complementary attention mechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse Attention Layer (SAL). The CBAM improves the model's ability to refine spatial and channel-wise features, while the SAL introduces a dual-branch design, sparse attention filters out low-confidence query-key pairs to suppress noise, and dense attention ensures comprehensive information propagation. Extensive experiments on the Brain US dataset demonstrate that our method achieves state-of-the-art segmentation performance, with a Dice score of 89.04% and IoU of 81.84% for ventricle region segmentation. These results highlight the effectiveness of integrating spatial refinement and attention sparsity for robust brain anatomy detection. Code is available at: this https URL. 

**Abstract (ZH)**: 脑室内出血（IVH）是早产儿严重的神经系统并发症，需要从脑超声图像中进行早期和准确的检测以改善临床结果。虽然近期的深度学习方法为计算机辅助诊断带来了希望，但在分割脑解剖结构时仍面临着捕捉局部空间细节和全局上下文依赖性的挑战。在本文中，我们提出了一种增强的残差U-网架构，结合了两种互补的注意力机制：卷积块注意力模块（CBAM）和稀疏注意力层（SAL）。CBAM提升了模型在细化空间和通道级特征方面的能力，而SAL引入了双分支设计，稀疏注意力过滤掉低置信度的查询-键对以抑制噪声，密集注意力确保了信息的全面传递。在Brain US数据集上的广泛实验表明，我们的方法实现了最先进的分割性能，脑室区域分割的Dice分数为89.04%，IoU为81.84%。这些结果突显了将空间细化和注意力稀疏性集成用于稳健脑解剖结构检测的有效性。代码可在以下链接获取：this https URL。 

---
# EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote Sensing Images with Attention Proxy 

**Title (ZH)**: EMRA-proxy：在注意力代理辅助下增强多类区域语义分割的遥感图像处理 

**Authors**: Yichun Yu, Yuqing Lan, Zhihuan Xing, Xiaoyi Yang, Tingyue Tang, Dan Yu  

**Link**: [PDF](https://arxiv.org/pdf/2505.17665)  

**Abstract**: High-resolution remote sensing (HRRS) image segmentation is challenging due to complex spatial layouts and diverse object appearances. While CNNs excel at capturing local features, they struggle with long-range dependencies, whereas Transformers can model global context but often neglect local details and are computationally this http URL propose a novel approach, Region-Aware Proxy Network (RAPNet), which consists of two components: Contextual Region Attention (CRA) and Global Class Refinement (GCR). Unlike traditional methods that rely on grid-based layouts, RAPNet operates at the region level for more flexible segmentation. The CRA module uses a Transformer to capture region-level contextual dependencies, generating a Semantic Region Mask (SRM). The GCR module learns a global class attention map to refine multi-class information, combining the SRM and attention map for accurate this http URL on three public datasets show that RAPNet outperforms state-of-the-art methods, achieving superior multi-class segmentation accuracy. 

**Abstract (ZH)**: 高分辨率遥感图像分割由于复杂的空间布局和多样的目标外观而具有挑战性。虽然CNN擅长捕获局部特征，但在建模长距离依赖关系方面存在局限性，而Transformer可以建模全局上下文但往往会忽略局部细节且计算成本高。为了解决这一问题，提出了一种新型方法，区域感知代理网络（RAPNet），该方法由两个组件组成：上下文区域注意力（CRA）和全局类别精炼（GCR）。与依赖网格布局的传统方法不同，RAPNet在区域级别操作以实现更具灵活性的分割。CRA模块使用Transformer捕获区域级别的上下文依赖关系，生成语义区域掩码（SRM）。GCR模块学习一个全局类别注意力图以精炼多类别信息，结合SRM和注意力图以实现精确的分割。在三个公开数据集上的实验表明，与当前最先进的方法相比，RAPNet在多类别分割 accuracy 方面表现更优。 

---
# Scaling Image and Video Generation via Test-Time Evolutionary Search 

**Title (ZH)**: 通过测试时进化搜索扩展图像和视频生成 

**Authors**: Haoran He, Jiajun Liang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Ling Pan  

**Link**: [PDF](https://arxiv.org/pdf/2505.17618)  

**Abstract**: As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as a promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains a notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose \textbf{Evo}lutionary \textbf{Search} (EvoSearch), a novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website this https URL. 

**Abstract (ZH)**: 随着模型预训练期间扩展计算（数据和参数）的边际成本持续大幅增加，测试时扩展（TTS）已成为通过在推理时分配额外计算来提高生成模型性能的有前景方向。尽管TTS已经在多个语言任务中展现了显著的成功，但在图像和视频生成模型（基于扩散或流的方法）的测试时扩展行为上仍存在明显的认识差距。尽管近期工作已开始探索视觉任务的推理时策略，但这些方法面临关键限制：局限于特定任务领域、扩展性差或过度优化奖励从而牺牲样本多样性。在本文中，我们提出了一种新颖的、通用且高效的TTS方法——进化搜索（EvoSearch），该方法无需额外训练或模型扩展即可有效提升扩散和流模型在图像和视频生成中的扩展性。EvoSearch将扩散和流模型的测试时扩展重新表述为进化搜索问题，利用生物学进化原理高效探索和优化去噪轨迹。通过结合针对随机微分方程去噪过程精心设计的选择和突变机制，EvoSearch逐代生成更高质量的后代同时保持种群多样性。通过在图像和视频生成任务的扩散和流架构上进行广泛评估，我们展示了我们方法在各个方面都优于现有方法，实现了更高的多样性，并且在未见过的评估指标上具有强大的泛化能力。该项目详情可在以下网址获取：this https URL。 

---
# OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics 

**Title (ZH)**: OrionBench: 信息图中图表和人类可识别对象检测基准 

**Authors**: Jiangning Zhu, Yuxing Zhou, Zheng Wang, Juntao Yao, Yima Gu, Yuhui Yuan, Shixia Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.17473)  

**Abstract**: Given the central role of charts in scientific, business, and communication contexts, enhancing the chart understanding capabilities of vision-language models (VLMs) has become increasingly critical. A key limitation of existing VLMs lies in their inaccurate visual grounding of infographic elements, including charts and human-recognizable objects (HROs) such as icons and images. However, chart understanding often requires identifying relevant elements and reasoning over them. To address this limitation, we introduce OrionBench, a benchmark designed to support the development of accurate object detection models for charts and HROs in infographics. It contains 26,250 real and 78,750 synthetic infographics, with over 6.9 million bounding box annotations. These annotations are created by combining the model-in-the-loop and programmatic methods. We demonstrate the usefulness of OrionBench through three applications: 1) constructing a Thinking-with-Boxes scheme to boost the chart understanding performance of VLMs, 2) comparing existing object detection models, and 3) applying the developed detection model to document layout and UI element detection. 

**Abstract (ZH)**: 基于图表的理解：OrionBench——用于信息图中图表和人类可识别对象的准确目标检测基准 

---
# Graph Mamba for Efficient Whole Slide Image Understanding 

**Title (ZH)**: Graph Mamba用于高效全量组织切片图像理解 

**Authors**: Jiaxuan Lu, Junyan Shi, Yuhui Lin, Fang Yan, Yue Gao, Shaoting Zhang, Xiaosong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.17457)  

**Abstract**: Whole Slide Images (WSIs) in histopathology present a significant challenge for large-scale medical image analysis due to their high resolution, large size, and complex tile relationships. Existing Multiple Instance Learning (MIL) methods, such as Graph Neural Networks (GNNs) and Transformer-based models, face limitations in scalability and computational cost. To bridge this gap, we propose the WSI-GMamba framework, which synergistically combines the relational modeling strengths of GNNs with the efficiency of Mamba, the State Space Model designed for sequence learning. The proposed GMamba block integrates Message Passing, Graph Scanning & Flattening, and feature aggregation via a Bidirectional State Space Model (Bi-SSM), achieving Transformer-level performance with 7* fewer FLOPs. By leveraging the complementary strengths of lightweight GNNs and Mamba, the WSI-GMamba framework delivers a scalable solution for large-scale WSI analysis, offering both high accuracy and computational efficiency for slide-level classification. 

**Abstract (ZH)**: WSI-GMamba框架在组织病理学大尺度医疗图像分析中的应用：结合GNNs的关系建模优势和Mamba的空间模型效率 

---
# Dynamic Manipulation of Deformable Objects in 3D: Simulation, Benchmark and Learning Strategy 

**Title (ZH)**: 三维可变形对象的动态操控：仿真、基准测试与学习策略 

**Authors**: Guanzhou Lan, Yuqi Yang, Anup Teejo Mathew, Feiping Nie, Rong Wang, Xuelong Li, Federico Renda, Bin Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2505.17434)  

**Abstract**: Goal-conditioned dynamic manipulation is inherently challenging due to complex system dynamics and stringent task constraints, particularly in deformable object scenarios characterized by high degrees of freedom and underactuation. Prior methods often simplify the problem to low-speed or 2D settings, limiting their applicability to real-world 3D tasks. In this work, we explore 3D goal-conditioned rope manipulation as a representative challenge. To mitigate data scarcity, we introduce a novel simulation framework and benchmark grounded in reduced-order dynamics, which enables compact state representation and facilitates efficient policy learning. Building on this, we propose Dynamics Informed Diffusion Policy (DIDP), a framework that integrates imitation pretraining with physics-informed test-time adaptation. First, we design a diffusion policy that learns inverse dynamics within the reduced-order space, enabling imitation learning to move beyond naïve data fitting and capture the underlying physical structure. Second, we propose a physics-informed test-time adaptation scheme that imposes kinematic boundary conditions and structured dynamics priors on the diffusion process, ensuring consistency and reliability in manipulation execution. Extensive experiments validate the proposed approach, demonstrating strong performance in terms of accuracy and robustness in the learned policy. 

**Abstract (ZH)**: 基于动力学指导的三维绳索操纵挑战与解决方案：克服数据稀缺性的方法及动力学知情扩散策略 

---
# Wildfire Detection Using Vision Transformer with the Wildfire Dataset 

**Title (ZH)**: 使用视觉变换器进行 wildfires 检测：基于 wildfires 数据集的方法 

**Authors**: Gowtham Raj Vuppari, Navarun Gupta, Ahmed El-Sayed, Xingguo Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2505.17395)  

**Abstract**: The critical need for sophisticated detection techniques has been highlighted by the rising frequency and intensity of wildfires in the US, especially in California. In 2023, wildfires caused 130 deaths nationwide, the highest since 1990. In January 2025, Los Angeles wildfires which included the Palisades and Eaton fires burnt approximately 40,000 acres and 12,000 buildings, and caused loss of human lives. The devastation underscores the urgent need for effective detection and prevention strategies. Deep learning models, such as Vision Transformers (ViTs), can enhance early detection by processing complex image data with high accuracy. However, wildfire detection faces challenges, including the availability of high-quality, real-time data. Wildfires often occur in remote areas with limited sensor coverage, and environmental factors like smoke and cloud cover can hinder detection. Additionally, training deep learning models is computationally expensive, and issues like false positives/negatives and scaling remain concerns. Integrating detection systems with real-time alert mechanisms also poses difficulties. In this work, we used the wildfire dataset consisting of 10.74 GB high-resolution images categorized into 'fire' and 'nofire' classes is used for training the ViT model. To prepare the data, images are resized to 224 x 224 pixels, converted into tensor format, and normalized using ImageNet statistics. 

**Abstract (ZH)**: 美国尤其是加利福尼亚州频发且强度增加的野火凸显了高级检测技术的迫切需求。2023年，野火在全国造成130人死亡，为1990年以来最高。2025年1月，洛杉矶地区的帕利塞德斯和艾顿野火烧毁了约40,000英亩土地和12,000栋建筑，造成了人员伤亡。这些灾害突显了有效检测和预防策略的迫切需求。深度学习模型，如视觉变换器（ViTs），可以通过高精度处理复杂图像数据来提升早期检测能力。然而，野火检测面临挑战，包括高质量实时数据的可用性限制。野火经常发生在传感器覆盖有限的偏远地区，烟雾和云层覆盖等环境因素会妨碍检测。此外，训练深度学习模型计算成本高昂，误报/漏报和扩展性等问题依然存在。将检测系统与实时警报机制结合也颇具挑战性。在这项工作中，我们使用了一个包含10.74 GB高分辨率图像的数据集，这些图像被分为“火灾”和“非火灾”两类用于训练ViT模型。为了准备数据，图像被调整为224 x 224像素，转换为张量格式，并使用ImageNet统计数据进行了标准化。 

---
# Dual-sensing driving detection model 

**Title (ZH)**: 双重传感驾驶检测模型 

**Authors**: Leon C.C.K, Zeng Hui  

**Link**: [PDF](https://arxiv.org/pdf/2505.17392)  

**Abstract**: In this paper, a novel dual-sensing driver fatigue detection method combining computer vision and physiological signal analysis is proposed. The system exploits the complementary advantages of the two sensing modalities and breaks through the limitations of existing single-modality methods. We introduce an innovative architecture that combines real-time facial feature analysis with physiological signal processing, combined with advanced fusion strategies, for robust fatigue detection. The system is designed to run efficiently on existing hardware while maintaining high accuracy and reliability. Through comprehensive experiments, we demonstrate that our method outperforms traditional methods in both controlled environments and real-world conditions, while maintaining high accuracy. The practical applicability of the system has been verified through extensive tests in various driving scenarios and shows great potential in reducing fatigue-related accidents. This study contributes to the field by providing a more reliable, cost-effective, and humane solution for driver fatigue detection. 

**Abstract (ZH)**: 一种结合计算机视觉和生理信号分析的新型双传感驾驶员疲劳检测方法 

---
# EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion 

**Title (ZH)**: EVM-Fusion: 可解释的Vision Mamba 架构与神经算法融合 

**Authors**: Zichuan Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.17367)  

**Abstract**: Medical image classification is critical for clinical decision-making, yet demands for accuracy, interpretability, and generalizability remain challenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba architecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for multi-organ medical image classification. EVM-Fusion leverages a multipath design, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim) modules, operate in parallel with a traditional feature pathway. These diverse features are dynamically integrated via a two-stage fusion process: cross-modal attention followed by the iterative NAF block, which learns an adaptive fusion algorithm. Intrinsic explainability is embedded through path-specific spatial attention, Vim {\Delta}-value maps, traditional feature SE-attention, and cross-modal attention weights. Experiments on a diverse 9-class multi-organ medical image dataset demonstrate EVM-Fusion's strong classification performance, achieving 99.75% test accuracy and provide multi-faceted insights into its decision-making process, highlighting its potential for trustworthy AI in medical diagnostics. 

**Abstract (ZH)**: 可解释的Vision Mamba架构EVM-Fusion在多器官医学图像分类中的应用 

---
# FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems 

**Title (ZH)**: FLEX：基于扩散模型的空间时间物理系统骨干网络 

**Authors**: N. Benjamin Erichson, Vinicius Mikuni, Dongwei Lyu, Yang Gao, Omri Azencot, Soon Hoe Lim, Michael W. Mahoney  

**Link**: [PDF](https://arxiv.org/pdf/2505.17351)  

**Abstract**: We introduce FLEX (FLow EXpert), a backbone architecture for generative modeling of spatio-temporal physical systems using diffusion models. FLEX operates in the residual space rather than on raw data, a modeling choice that we motivate theoretically, showing that it reduces the variance of the velocity field in the diffusion model, which helps stabilize training. FLEX integrates a latent Transformer into a U-Net with standard convolutional ResNet layers and incorporates a redesigned skip connection scheme. This hybrid design enables the model to capture both local spatial detail and long-range dependencies in latent space. To improve spatio-temporal conditioning, FLEX uses a task-specific encoder that processes auxiliary inputs such as coarse or past snapshots. Weak conditioning is applied to the shared encoder via skip connections to promote generalization, while strong conditioning is applied to the decoder through both skip and bottleneck features to ensure reconstruction fidelity. FLEX achieves accurate predictions for super-resolution and forecasting tasks using as few as two reverse diffusion steps. It also produces calibrated uncertainty estimates through sampling. Evaluations on high-resolution 2D turbulence data show that FLEX outperforms strong baselines and generalizes to out-of-distribution settings, including unseen Reynolds numbers, physical observables (e.g., fluid flow velocity fields), and boundary conditions. 

**Abstract (ZH)**: 基于扩散模型的时空物理系统生成建模架构FLEX 

---
# Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering 

**Title (ZH)**: Render-FM：实时高逼真体积渲染的基础模型 

**Authors**: Zhongpai Gao, Meng Zheng, Benjamin Planche, Anwesa Choudhuri, Terrence Chen, Ziyan Wu  

**Link**: [PDF](https://arxiv.org/pdf/2505.17338)  

**Abstract**: Volumetric rendering of Computed Tomography (CT) scans is crucial for visualizing complex 3D anatomical structures in medical imaging. Current high-fidelity approaches, especially neural rendering techniques, require time-consuming per-scene optimization, limiting clinical applicability due to computational demands and poor generalizability. We propose Render-FM, a novel foundation model for direct, real-time volumetric rendering of CT scans. Render-FM employs an encoder-decoder architecture that directly regresses 6D Gaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan optimization through large-scale pre-training on diverse medical data. By integrating robust feature extraction with the expressive power of 6DGS, our approach efficiently generates high-quality, real-time interactive 3D visualizations across diverse clinical CT data. Experiments demonstrate that Render-FM achieves visual fidelity comparable or superior to specialized per-scan methods while drastically reducing preparation time from nearly an hour to seconds for a single inference step. This advancement enables seamless integration into real-time surgical planning and diagnostic workflows. The project page is: this https URL. 

**Abstract (ZH)**: 计算机断层扫描（CT）的体绘制对于医学成像中可视化复杂的三维解剖结构至关重要。当前的高保真方法，尤其是神经渲染技术，需要耗时的逐场景优化，由于计算需求大和缺乏普适性，限制了其临床应用。我们提出Render-FM，这是一种新型的基础模型，用于直接实时渲染CT扫描的体绘制。Render-FM 使用编码器-解码器架构，直接从CT体积中回归6D高斯聚散（6DGS）参数，通过在多样化的医学数据上进行大规模预训练来消除逐扫描优化。通过结合鲁棒的特征提取与6DGS的强大表达能力，我们的方法能够高效地生成高质量的、实时交互的3D可视化结果，适用于多样化的临床CT数据。实验表明，Render-FM 在单次推断步骤中的准备时间从近一小时大幅减少到几秒，同时在视觉保真度方面达到或优于专门的逐扫描方法。这一进步使得无缝集成到实时手术计划和诊断流程中成为可能。项目页面：this https URL。 

---
# Assessing the generalization performance of SAM for ureteroscopy scene understanding 

**Title (ZH)**: 评估SAM在输尿管镜场景理解中的泛化性能 

**Authors**: Martin Villagrana, Francisco Lopez-Tiro, Clement Larose, Gilberto Ochoa-Ruiz, Christian Daul  

**Link**: [PDF](https://arxiv.org/pdf/2505.17210)  

**Abstract**: The segmentation of kidney stones is regarded as a critical preliminary step to enable the identification of urinary stone types through machine- or deep-learning-based approaches. In urology, manual segmentation is considered tedious and impractical due to the typically large scale of image databases and the continuous generation of new data. In this study, the potential of the Segment Anything Model (SAM) -- a state-of-the-art deep learning framework -- is investigated for the automation of kidney stone segmentation. The performance of SAM is evaluated in comparison to traditional models, including U-Net, Residual U-Net, and Attention U-Net, which, despite their efficiency, frequently exhibit limitations in generalizing to unseen datasets. The findings highlight SAM's superior adaptability and efficiency. While SAM achieves comparable performance to U-Net on in-distribution data (Accuracy: 97.68 + 3.04; Dice: 97.78 + 2.47; IoU: 95.76 + 4.18), it demonstrates significantly enhanced generalization capabilities on out-of-distribution data, surpassing all U-Net variants by margins of up to 23 percent. 

**Abstract (ZH)**: 肾结石分割在基于机器学习或深度学习的尿石类型识别中的初步自动化研究：Segment Anything Model在肾结石分割中的潜力及其性能评估 

---
# Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models 

**Title (ZH)**: 合成历史：评估扩散模型中过去的时代图示 

**Authors**: Maria-Teresa De Rosa Palmini, Eva Cetinic  

**Link**: [PDF](https://arxiv.org/pdf/2505.17064)  

**Abstract**: As Text-to-Image (TTI) diffusion models become increasingly influential in content creation, growing attention is being directed toward their societal and cultural implications. While prior research has primarily examined demographic and cultural biases, the ability of these models to accurately represent historical contexts remains largely underexplored. In this work, we present a systematic and reproducible methodology for evaluating how TTI systems depict different historical periods. For this purpose, we introduce the HistVis dataset, a curated collection of 30,000 synthetic images generated by three state-of-the-art diffusion models using carefully designed prompts depicting universal human activities across different historical periods. We evaluate generated imagery across three key aspects: (1) Implicit Stylistic Associations: examining default visual styles associated with specific eras; (2) Historical Consistency: identifying anachronisms such as modern artifacts in pre-modern contexts; and (3) Demographic Representation: comparing generated racial and gender distributions against historically plausible baselines. Our findings reveal systematic inaccuracies in historically themed generated imagery, as TTI models frequently stereotype past eras by incorporating unstated stylistic cues, introduce anachronisms, and fail to reflect plausible demographic patterns. By offering a scalable methodology and benchmark for assessing historical representation in generated imagery, this work provides an initial step toward building more historically accurate and culturally aligned TTI models. 

**Abstract (ZH)**: 随着文本到图像（TTI）扩散模型在内容创建中的影响力日益增强，人们越来越关注其对社会和文化的影响。尽管先前的研究主要关注人口统计和文化偏见，但这些模型如何准确地表现历史背景仍然 largely underexplored。在此项研究中，我们提出了一种系统且可再现的方法来评估 TTI 系统如何描绘不同历史时期。为此，我们引入了 HistVis 数据集，该数据集包含 30,000 张由三种最先进的扩散模型生成的精心设计的合成图像，这些图像描绘了不同历史时期的普遍人类活动。我们从三个关键方面评估生成的图像：（1）隐含的风格关联：检查与特定时期相关的默认视觉风格；（2）历史一致性：识别现代物品在前现代背景中的不合适；（3）人口统计表现：将生成的种族和性别分布与历史可信的基准进行比较。我们的研究发现，在主题上与历史相关的生成图像存在系统性不准确之处，TTI 模型经常通过引入未明确的风格线索、引入历史错误和未能反映可信的人口统计模式来刻板化过去的时代。通过提供一种可扩展的方法和基准来评估生成图像中的历史表现，本项工作为构建更准确的历史和文化对齐的 TTI 模型迈出了第一步。 

---
