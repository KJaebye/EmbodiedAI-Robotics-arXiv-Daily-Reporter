{'arxiv_id': 'arXiv:2505.17862', 'title': 'Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities', 'authors': 'Ziwei Zhou, Rui Wang, Zuxuan Wu', 'link': 'https://arxiv.org/abs/2505.17862', 'abstract': 'Recent Multimodal Large Language Models (MLLMs) achieve promising performance on visual and audio benchmarks independently. However, the ability of these models to process cross-modal information synchronously remains largely unexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual Questioning and Answering benchmark comprising 684 videos of daily life scenarios from diverse sources, rich in both audio and visual information, and featuring 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA Generation Pipeline, which includes automatic annotation, QA generation and QA optimization, significantly improves efficiency for human evaluation and scalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent utilizing open-source Visual Language Model (VLM), Audio Language Model (ALM) and Automatic Speech Recognition (ASR) model to establish a baseline for this benchmark. The results show that current MLLMs still struggle significantly with tasks requiring audio-visual integration, but combining VLMs and ALMs with simple temporal alignment techniques can achieve substantially better performance. Codes and benchmark are available at \\href{this https URL}{this https URL}.', 'abstract_zh': '近期的多模态大型语言模型在视觉和音频基准上取得了令人瞩目的独立性能。然而，这些模型同步处理跨模态信息的能力尚待探索。本文介绍：1) Daily-Omni，一个包含684个日常生活场景视频的多模态问答基准，这些视频来源于多种渠道且富含音频和视觉信息，并包含跨越6个主要任务的1197个多选问答对；2) Daily-Omni 问答生成管道，集成了自动标注、问答生成和问答优化，显著提高了人工评估的效率和基准的可扩展性；3) Daily-Omni-Agent，一个无需训练的代理，利用开源的视觉语言模型（VLM）、音频语言模型（ALM）和自动语音识别（ASR）模型为该基准设立一个基线。结果表明，当前的多模态大型语言模型在需要音频-视觉整合的任务上仍然面临显著挑战，但通过将VLMs和ALMs与简单的时序对齐技术相结合，可以显著提升性能。代码和基准可在\\href{this https URL}{this https URL}获得。', 'title_zh': 'Daily-Omni: 向跨模态Temporal Alignment的音频-视觉推理方向探索'}
{'arxiv_id': 'arXiv:2505.17613', 'title': 'MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation', 'authors': 'Jihan Yao, Yushi Hu, Yujie Yi, Bin Han, Shangbin Feng, Guang Yang, Bingbing Wen, Ranjay Krishna, Lucy Lu Wang, Yulia Tsvetkov, Noah A. Smith, Banghua Zhu', 'link': 'https://arxiv.org/abs/2505.17613', 'abstract': 'Automatically evaluating multimodal generation presents a significant challenge, as automated metrics often struggle to align reliably with human evaluation, especially for complex tasks that involve multiple modalities. To address this, we present MMMG, a comprehensive and human-aligned benchmark for multimodal generation across 4 modality combinations (image, audio, interleaved text and image, interleaved text and audio), with a focus on tasks that present significant challenges for generation models, while still enabling reliable automatic evaluation through a combination of models and programs. MMMG encompasses 49 tasks (including 29 newly developed ones), each with a carefully designed evaluation pipeline, and 937 instructions to systematically assess reasoning, controllability, and other key capabilities of multimodal generation models. Extensive validation demonstrates that MMMG is highly aligned with human evaluation, achieving an average agreement of 94.3%. Benchmarking results on 24 multimodal generation models reveal that even though the state-of-the-art model, GPT Image, achieves 78.3% accuracy for image generation, it falls short on multimodal reasoning and interleaved generation. Furthermore, results suggest considerable headroom for improvement in audio generation, highlighting an important direction for future research.', 'abstract_zh': '自动评估多模态生成呈现重大挑战，因为自动化指标往往难以可靠地与人工评估对齐，尤其是在涉及多个模态的复杂任务中。为解决这一问题，我们提出MMMG，这是一个全面且与人工评估对齐的多模态生成基准，涵盖了4种模态组合（图像、音频、交替文本和图像、交替文本和音频），专注于生成模型面临重大挑战的任务，同时通过模型和程序的结合实现可靠的自动评估。MMMG 包含49项任务（其中包括29项新开发的任务），每项任务都有一个精心设计的评估管道，以及937条指令用于系统地评估多模态生成模型的推理、可控性及其他关键能力。广泛的验证显示，MMMG 高度与人工评估对齐，平均一致性达到94.3%。对24个多模态生成模型的基准测试结果表明，尽管最先进的模型GPT Image 在图像生成上的准确率达到78.3%，但在多模态推理和交替生成方面仍表现不佳。此外，结果还表明在音频生成方面存在改进空间，这指出了未来研究的重要方向。', 'title_zh': 'MMMG：全面可靠的多任务多模态生成评估套件'}
{'arxiv_id': 'arXiv:2505.17436', 'title': 'Scaling Up Biomedical Vision-Language Models: Fine-Tuning, Instruction Tuning, and Multi-Modal Learning', 'authors': 'Cheng Peng, Kai Zhang, Mengxian Lyu, Hongfang Liu, Lichao Sun, Yonghui Wu', 'link': 'https://arxiv.org/abs/2505.17436', 'abstract': 'To advance biomedical vison-language model capabilities through scaling up, fine-tuning, and instruction tuning, develop vision-language models with improved performance in handling long text, explore strategies to efficiently adopt vision language models for diverse multi-modal biomedical tasks, and examine the zero-shot learning performance.\nWe developed two biomedical vision language models, BiomedGPT-Large and BiomedGPT-XLarge, based on an encoder-decoder-based transformer architecture. We fine-tuned the two models on 23 benchmark datasets from 6 multi-modal biomedical tasks including one image-only task (image classification), three language-only tasks (text understanding, text summarization and question answering), and two vision-language tasks (visual question answering and image captioning). We compared the developed scaled models with our previous BiomedGPT-Base model and existing prestigious models reported in the literature. We instruction-tuned the two models using a large-scale multi-modal biomedical instruction-tuning dataset and assessed the zero-shot learning performance and alignment accuracy.', 'abstract_zh': '通过扩展、微调和指令调优提升生物医学视觉语言模型能力，探索高效采用视觉语言模型进行多元生物医学任务的策略，并考察零样本学习性能。发展了两个基于编码器-解码器变换器架构的生物医学视觉语言模型——BiomedGPT-Large和BiomedGPT-XLarge。在包括一项图像仅任务（图像分类）、三项语言仅任务（文本理解、文本摘要和问答）以及两项视觉语言任务（视觉问答和图像字幕）的23个基准数据集上对两个模型进行了微调。将开发的扩展模型与我们之前发布的BiomedGPT-Base模型及文献中报道的杰出模型进行了比较，并使用大规模多元生物医学指令调优数据集对两个模型进行了指令调优，评估了零样本学习性能和对齐准确度。', 'title_zh': '扩大生物医药领域视觉-语言模型的规模：微调、指令调优与多模态学习'}
{'arxiv_id': 'arXiv:2505.17433', 'title': 'MemeReaCon: Probing Contextual Meme Understanding in Large Vision-Language Models', 'authors': 'Zhengyi Zhao, Shubo Zhang, Yuxi Zhang, Yanxi Zhao, Yifan Zhang, Zezhong Wang, Huimin Wang, Yutian Zhao, Bin Liang, Yefeng Zheng, Binyang Li, Kam-Fai Wong, Xian Wu', 'link': 'https://arxiv.org/abs/2505.17433', 'abstract': "Memes have emerged as a popular form of multimodal online communication, where their interpretation heavily depends on the specific context in which they appear. Current approaches predominantly focus on isolated meme analysis, either for harmful content detection or standalone interpretation, overlooking a fundamental challenge: the same meme can express different intents depending on its conversational context. This oversight creates an evaluation gap: although humans intuitively recognize how context shapes meme interpretation, Large Vision Language Models (LVLMs) can hardly understand context-dependent meme intent. To address this critical limitation, we introduce MemeReaCon, a novel benchmark specifically designed to evaluate how LVLMs understand memes in their original context. We collected memes from five different Reddit communities, keeping each meme's image, the post text, and user comments together. We carefully labeled how the text and meme work together, what the poster intended, how the meme is structured, and how the community responded. Our tests with leading LVLMs show a clear weakness: models either fail to interpret critical information in the contexts, or overly focus on visual details while overlooking communicative purpose. MemeReaCon thus serves both as a diagnostic tool exposing current limitations and as a challenging benchmark to drive development toward more sophisticated LVLMs of the context-aware understanding.", 'abstract_zh': 'Memes作为一种流行的多模态在线交流形式，其解释高度依赖于它们出现的具体情境。当前的方法主要侧重于孤立的 meme 分析，主要用于有害内容检测或独立解释，忽视了一个基本挑战：同一个 meme 可以根据其对话情境表达不同的意图。这种忽视造成了评价差距：尽管人类能直观地认识到情境如何塑造 meme 的解释，大型视觉语言模型 (LVLM) 难以理解情境依赖的 meme 意图。为解决这一关键限制，我们引入了 MemeReaCon，这是一种新型基准，专门用于评估 LVLMs 如何在原始情境中理解 memes。我们从五个不同的 Reddit 社区收集 memes，保持每个 meme 的图像、帖子文本和用户评论完整。我们仔细标注了文本和 meme 的互动方式、发布者意图、meme 的结构以及社区的反应。我们对领先 LVLM 的测试表明，模型要么无法解释上下文中的关键信息，要么过度关注视觉细节而忽视交流目的。MemeReaCon 因此既作为诊断工具揭示了当前的局限性，又作为一个具有挑战性的基准，推动开发出更复杂的情境意识 LVLM。', 'title_zh': 'MemeReaCon: 探究大型视觉-语言模型中的情境表情包理解能力'}
{'arxiv_id': 'arXiv:2505.17214', 'title': 'MEDMKG: Benchmarking Medical Knowledge Exploitation with Multimodal Knowledge Graph', 'authors': 'Xiaochen Wang, Yuan Zhong, Lingwei Zhang, Lisong Dai, Ting Wang, Fenglong Ma', 'link': 'https://arxiv.org/abs/2505.17214', 'abstract': 'Medical deep learning models depend heavily on domain-specific knowledge to perform well on knowledge-intensive clinical tasks. Prior work has primarily leveraged unimodal knowledge graphs, such as the Unified Medical Language System (UMLS), to enhance model performance. However, integrating multimodal medical knowledge graphs remains largely underexplored, mainly due to the lack of resources linking imaging data with clinical concepts. To address this gap, we propose MEDMKG, a Medical Multimodal Knowledge Graph that unifies visual and textual medical information through a multi-stage construction pipeline. MEDMKG fuses the rich multimodal data from MIMIC-CXR with the structured clinical knowledge from UMLS, utilizing both rule-based tools and large language models for accurate concept extraction and relationship modeling. To ensure graph quality and compactness, we introduce Neighbor-aware Filtering (NaF), a novel filtering algorithm tailored for multimodal knowledge graphs. We evaluate MEDMKG across three tasks under two experimental settings, benchmarking twenty-four baseline methods and four state-of-the-art vision-language backbones on six datasets. Results show that MEDMKG not only improves performance in downstream medical tasks but also offers a strong foundation for developing adaptive and robust strategies for multimodal knowledge integration in medical artificial intelligence.', 'abstract_zh': '医学多模态知识图谱在知识密集型临床任务中依赖特定领域知识以实现高性能。先前工作主要通过统一医学语言系统(UMLS)等单模态知识图谱来增强模型性能。然而，多模态医学知识图谱的集成尚未充分探索，主要由于缺乏连接影像数据与临床概念的资源。为解决这一问题，我们提出了一种医学多模态知识图谱(MEDMKG)，通过多阶段构建管道统一视觉和文本医学信息。MEDMKG 将 MIMIC-CXR 的丰富多模态数据与 UMLS 的结构化临床知识融合，利用基于规则的工具和大规模语言模型进行准确的概念提取和关系建模。为确保图的质量和紧凑性，我们引入了一种名为邻域感知过滤(NaF)的新过滤算法，专门针对多模态知识图谱。在两种实验设置下，我们在六个数据集上评估了MEDMKG，针对二十四种基准方法和四种最先进的视觉-语言骨干进行对比。结果显示，MEDMKG 不仅在下游医学任务中表现出色，还为开发适应性强且鲁棒的多模态知识集成策略提供了坚实基础。', 'title_zh': 'MEDMKG：多模态知识图谱中的医疗知识 exploitation 评估基准'}
{'arxiv_id': 'arXiv:2505.17796', 'title': 'DetailFusion: A Dual-branch Framework with Detail Enhancement for Composed Image Retrieval', 'authors': 'Yuxin Yang, Yinan Zhou, Yuxin Chen, Ziqi Zhang, Zongyang Ma, Chunfeng Yuan, Bing Li, Lin Song, Jun Gao, Peng Li, Weiming Hu', 'link': 'https://arxiv.org/abs/2505.17796', 'abstract': 'Composed Image Retrieval (CIR) aims to retrieve target images from a gallery based on a reference image and modification text as a combined query. Recent approaches focus on balancing global information from two modalities and encode the query into a unified feature for retrieval. However, due to insufficient attention to fine-grained details, these coarse fusion methods often struggle with handling subtle visual alterations or intricate textual instructions. In this work, we propose DetailFusion, a novel dual-branch framework that effectively coordinates information across global and detailed granularities, thereby enabling detail-enhanced CIR. Our approach leverages atomic detail variation priors derived from an image editing dataset, supplemented by a detail-oriented optimization strategy to develop a Detail-oriented Inference Branch. Furthermore, we design an Adaptive Feature Compositor that dynamically fuses global and detailed features based on fine-grained information of each unique multimodal query. Extensive experiments and ablation analyses not only demonstrate that our method achieves state-of-the-art performance on both CIRR and FashionIQ datasets but also validate the effectiveness and cross-domain adaptability of detail enhancement for CIR.', 'abstract_zh': '基于参考图像和修改文本的综合查询的图像检索（Composed Image Retrieval）旨在根据参考图像和修改文本的联合查询从图片库中检索目标图像。近年来的方法致力于平衡来自两种模态的全局信息，并将查询编码到统一特征中以实现检索。然而，由于对细微细节关注不足，这些粗粒度融合方法在处理微妙的视觉变化或复杂的文本指令时往往表现不佳。在本文中，我们提出了一种新颖的双支路框架DetailFusion，该框架有效地协调了全局和细粒度层级的信息，从而实现细节增强的图像检索（Detail-enhanced CIR）。我们的方法利用从图像编辑数据集中获得的原子细节变化先验，并结合一种细节导向的优化策略来构建细节导向的推理支路。此外，我们设计了自适应特征合成器，该合成器根据每个独特多模态查询的细微信息动态融合全局和细粒度特征。广泛的实验和消融分析不仅证明了我们的方法在CIRR和FashionIQ数据集上达到了最先进的性能，而且还验证了细节增强在图像检索中的有效性和跨域适应性。', 'title_zh': '细节融合：一种用于组合图像检索的细节点和语义点双支框架'}
{'arxiv_id': 'arXiv:2505.17726', 'title': 'Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM', 'authors': 'Donghwan Chi, Hyomin Kim, Yoonjin Oh, Yongjin Kim, Donghoon Lee, Daejin Jo, Jongmin Kim, Junyeob Baek, Sungjin Ahn, Sungwoong Kim', 'link': 'https://arxiv.org/abs/2505.17726', 'abstract': "Recently, multimodal large language models (MLLMs) have emerged as a key approach in achieving artificial general intelligence. In particular, vision-language MLLMs have been developed to generate not only text but also visual outputs from multimodal inputs. This advancement requires efficient image tokens that LLMs can process effectively both in input and output. However, existing image tokenization methods for MLLMs typically capture only global abstract concepts or uniformly segmented image patches, restricting MLLMs' capability to effectively understand or generate detailed visual content, particularly at the object level. To address this limitation, we propose an object-centric visual tokenizer based on Slot Attention specifically for MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and residual vector quantization, our proposed discretized slot tokens can encode local visual details while maintaining high-level semantics, and also align with textual data to be integrated seamlessly within a unified next-token prediction framework of LLMs. The resulting Slot-MLLM demonstrates significant performance improvements over baselines with previous visual tokenizers across various vision-language tasks that entail local detailed comprehension and generation. Notably, this work is the first demonstration of the feasibility of object-centric slot attention performed with MLLMs and in-the-wild natural images.", 'abstract_zh': '近期，多模态大语言模型（MLLMs）已成为实现人工通用智能的关键方法。特别是，视觉语言MLLMs已被开发出来，不仅能生成文本，还能从多模态输入中生成视觉输出。这一进展需要高效的图像令牌，使MLLMs在输入和输出中都能有效处理。然而，现有针对MLLMs的图像 tokenization 方法通常只能捕获全局抽象概念或均匀分割的图像块，限制了MLLMs有效理解或生成详细视觉内容的能力，尤其是在对象层面。为了解决这一局限性，我们提出了一种基于 Slot Attention 的对象中心视觉tokenizer，专门用于MLLMs。特别是，基于Q-Former编码器、扩散解码器和残差矢量量化，我们提出的离散槽令牌能够编码局部视觉细节，同时保持高层次语义，并与文本数据无缝整合，以适应统一的下一个token预测框架。所提出的Slot-MLLM在涉及局部详细理解和生成的各种视觉语言任务上，相对于之前的视觉tokenizer基线模型，表现出了显著的性能提升。值得注意的是，这是首次在MLLMs和真实世界的自然图像中展示对象中心槽注意力的可行性。', 'title_zh': 'Slot-MLLM: 基于对象的视觉词元化多模态大模型'}
{'arxiv_id': 'arXiv:2505.17529', 'title': 'Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding', 'authors': 'Yeongjae Cho, Keonwoo Kim, Taebaek Hwang, Sungzoon Cho', 'link': 'https://arxiv.org/abs/2505.17529', 'abstract': 'Recent advancements in Large Vision-Language Models (LVLMs) have significantly expanded their utility in tasks like image captioning and visual question answering. However, they still struggle with object hallucination, where models generate descriptions that inaccurately reflect the visual content by including nonexistent objects or misrepresenting existing ones. While previous methods, such as data augmentation and training-free approaches, strive to tackle this issue, they still encounter scalability challenges and often depend on additional external modules. In this work, we propose Ensemble Decoding (ED), a novel strategy that splits the input image into sub-images and combines logit distributions by assigning weights through the attention map. Furthermore, we introduce ED adaptive plausibility constraint to calibrate logit distribution and FastED, a variant designed for speed-critical applications. Extensive experiments across hallucination benchmarks demonstrate that our proposed method achieves state-of-the-art performance, validating the effectiveness of our approach.', 'abstract_zh': '近期大规模愿景语言模型（LVLMs）的进展显著扩展了其在图像描述和视觉问答等任务中的应用，但仍面临对象幻觉的问题，即模型生成描述时会包含不存在的对象或错误地描述现有对象。虽然之前的方法，如数据增强和无训练方法，努力解决这一问题，但它们仍然面临可扩展性挑战，并且通常依赖额外的外部模块。在本工作中，我们提出了一种新的 Ensemble Decoding（ED）策略，该策略将输入图像分割成子图像，并通过注意力图分配权重来组合logit分布。此外，我们引入了ED自适应可实现性约束来校准logit分布，并设计了FastED变体以满足速度关键型应用的需求。广泛实验结果表明，我们的方法在幻觉基准测试中达到了最佳性能，验证了该方法的有效性。', 'title_zh': '你在留意我的问题吗？基于注意力导向 ensemble 解码减轻多模态幻觉'}
{'arxiv_id': 'arXiv:2505.17501', 'title': 'RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition', 'authors': 'Yuehan Jin, Xiaoqing Liu, Yiyuan Yang, Zhiwen Yu, Tong Zhang, Kaixiang Yang', 'link': 'https://arxiv.org/abs/2505.17501', 'abstract': 'Multimodal emotion recognition analyzes emotions by combining data from multiple sources. However, real-world noise or sensor failures often cause missing or corrupted data, creating the Incomplete Multimodal Emotion Recognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion Recovery (RoHyDR), a novel framework that performs missing-modality recovery at unimodal, multimodal, feature, and semantic levels. For unimodal representation recovery of missing modalities, RoHyDR exploits a diffusion-based generator to generate distribution-consistent and semantically aligned representations from Gaussian noise, using available modalities as conditioning. For multimodal fusion recovery, we introduce adversarial learning to produce a realistic fused multimodal representation and recover missing semantic content. We further propose a multi-stage optimization strategy that enhances training stability and efficiency. In contrast to previous work, the hybrid diffusion and adversarial learning-based recovery mechanism in RoHyDR allows recovery of missing information in both unimodal representation and multimodal fusion, at both feature and semantic levels, effectively mitigating performance degradation caused by suboptimal optimization. Comprehensive experiments conducted on two widely used multimodal emotion recognition benchmarks demonstrate that our proposed method outperforms state-of-the-art IMER methods, achieving robust recognition performance under various missing-modality scenarios. Our code will be made publicly available upon acceptance.', 'abstract_zh': '多模态情绪识别中的鲁棒混合扩散恢复（RoHyDR）：一种在单模态、多模态、特征和语义级别上进行缺失模态恢复的新框架', 'title_zh': 'RoHyDR：鲁棒混合扩散恢复 incomplete 多模态情感识别'}
{'arxiv_id': 'arXiv:2505.17144', 'title': 'MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models', 'authors': 'Bohan Jin, Shuhan Qi, Kehai Chen, Xinyi Guo, Xuan Wang', 'link': 'https://arxiv.org/abs/2505.17144', 'abstract': "The widespread use of Large Multimodal Models (LMMs) has raised concerns about model toxicity. However, current research mainly focuses on explicit toxicity, with less attention to some more implicit toxicity regarding prejudice and discrimination. To address this limitation, we introduce a subtler type of toxicity named dual-implicit toxicity and a novel toxicity benchmark termed MDIT-Bench: Multimodal Dual-Implicit Toxicity Benchmark. Specifically, we first create the MDIT-Dataset with dual-implicit toxicity using the proposed Multi-stage Human-in-loop In-context Generation method. Based on this dataset, we construct the MDIT-Bench, a benchmark for evaluating the sensitivity of models to dual-implicit toxicity, with 317,638 questions covering 12 categories, 23 subcategories, and 780 topics. MDIT-Bench includes three difficulty levels, and we propose a metric to measure the toxicity gap exhibited by the model across them. In the experiment, we conducted MDIT-Bench on 13 prominent LMMs, and the results show that these LMMs cannot handle dual-implicit toxicity effectively. The model's performance drops significantly in hard level, revealing that these LMMs still contain a significant amount of hidden but activatable toxicity. Data are available at this https URL.", 'abstract_zh': '大规模多模态模型（LMMs）的广泛应用引发了对模型毒性问题的关注。然而，当前研究主要集中在显式毒性上，对于有关偏见和歧视的更隐含的毒性关注较少。为解决这一不足，我们引入了一种更为微妙的毒性类型——双重隐含毒性，并提出了一种新颖的毒性基准——MDIT-Bench：多模态双重隐含毒性基准。具体而言，我们首先通过提出的多阶段人工环路上下文生成方法创建了MDIT-数据集，以涵盖双重隐含毒性。基于此数据集，我们构建了MDIT-Bench，这是一个用于评估模型对双重隐含毒性敏感性的基准，包含317638个问题，覆盖12个类别、23个子类别和780个主题。MDIT-Bench包括三个难度级别，并提出了一种测量模型在不同难度级别上表现出的毒性差距的指标。在实验中，我们对13种 prominent LMMs 进行了MDIT-Bench测试，结果显示这些模型不能有效处理双重隐含毒性。这些模型在高难度级别上的性能显著下降，揭示了这些模型仍然含有相当数量的隐藏但可激活的毒性。相关数据可在此网址获取。', 'title_zh': 'MDIT-Bench: 评估大型多模态模型中的双重隐式毒性'}
