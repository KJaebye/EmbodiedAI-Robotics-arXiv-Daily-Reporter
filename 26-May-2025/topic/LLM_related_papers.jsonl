{'arxiv_id': 'arXiv:2505.17659', 'title': 'Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling', 'authors': 'Xiaolong Tang, Meina Kan, Shiguang Shan, Xilin Chen', 'link': 'https://arxiv.org/abs/2505.17659', 'abstract': 'Safe and feasible trajectory planning is essential for real-world autonomous driving systems. However, existing learning-based planning methods often rely on expert demonstrations, which not only lack explicit safety awareness but also risk inheriting unsafe behaviors such as speeding from suboptimal human driving data. Inspired by the success of large language models, we propose Plan-R1, a novel two-stage trajectory planning framework that formulates trajectory planning as a sequential prediction task, guided by explicit planning principles such as safety, comfort, and traffic rule compliance. In the first stage, we train an autoregressive trajectory predictor via next motion token prediction on expert data. In the second stage, we design rule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the model using Group Relative Policy Optimization (GRPO), a reinforcement learning strategy, to align its predictions with these planning principles. Experiments on the nuPlan benchmark demonstrate that our Plan-R1 significantly improves planning safety and feasibility, achieving state-of-the-art performance.', 'abstract_zh': '基于明确规划原则的安全可行路径规划对于实际自主驾驶系统至关重要。然而，现有的基于学习的规划方法往往依赖于专家示范，这不仅缺乏明确的安全意识，还可能继承如超速等不安全行为。受大型语言模型成功的启发，我们提出了一种新颖的两阶段路径规划框架Plan-R1，将其形式化为一个由明确规划原则（如安全、舒适和交通规则遵守）引导的序列预测任务。在第一阶段，我们通过下一个运动标记预测对专家数据进行自回归路径预测器的训练。在第二阶段，我们设计基于规则的奖励（如碰撞避免、限速），并通过Group Relative Policy Optimization (GRPO)强化学习策略对模型进行微调，使其预测与这些规划原则保持一致。在nuPlan基准上的实验表明，我们的Plan-R1在规划安全性和可行性方面取得了显著提升，实现了最先进的性能。', 'title_zh': 'Plan-R1: 安全可行的轨迹规划作为语言建模'}
{'arxiv_id': 'arXiv:2505.17209', 'title': 'LiloDriver: A Lifelong Learning Framework for Closed-loop Motion Planning in Long-tail Autonomous Driving Scenarios', 'authors': 'Huaiyuan Yao, Pengfei Li, Bu Jin, Yupeng Zheng, An Liu, Lisen Mu, Qing Su, Qian Zhang, Yilun Chen, Peng Li', 'link': 'https://arxiv.org/abs/2505.17209', 'abstract': 'Recent advances in autonomous driving research towards motion planners that are robust, safe, and adaptive. However, existing rule-based and data-driven planners lack adaptability to long-tail scenarios, while knowledge-driven methods offer strong reasoning but face challenges in representation, control, and real-world evaluation. To address these challenges, we present LiloDriver, a lifelong learning framework for closed-loop motion planning in long-tail autonomous driving scenarios. By integrating large language models (LLMs) with a memory-augmented planner generation system, LiloDriver continuously adapts to new scenarios without retraining. It features a four-stage architecture including perception, scene encoding, memory-based strategy refinement, and LLM-guided reasoning. Evaluated on the nuPlan benchmark, LiloDriver achieves superior performance in both common and rare driving scenarios, outperforming static rule-based and learning-based planners. Our results highlight the effectiveness of combining structured memory and LLM reasoning to enable scalable, human-like motion planning in real-world autonomous driving. Our code is available at this https URL.', 'abstract_zh': '近期自主驾驶研究中面向鲁棒、安全和适应性强的任务规划方法的进展。然而，现有的基于规则和数据驱动的规划方法缺乏对长尾场景的适应性，而知识驱动的方法虽能提供强大的推理能力，但在表示、控制和实地评估方面面临挑战。为应对这些挑战，我们提出了LiloDriver，一种用于长尾自主驾驶场景闭环运动规划的终身学习框架。通过将大型语言模型（LLMs）与记忆增强的规划生成系统集成，LiloDriver能够无需重新训练地持续适应新场景。其架构包括感知、场景编码、基于记忆的策略细化和LLM引导的推理四个阶段。在nuPlan基准测试中，LiloDriver在常见和罕见驾驶场景中的性能均优于静态规则驱动和学习驱动的规划方法。我们的结果表明，结合结构化记忆和LLM推理能够实现可扩展且类似人类的运动规划，并已在实际自主驾驶中得到验证。代码可访问此链接。', 'title_zh': 'LiloDriver：长尾自主驾驶场景中闭环运动规划的终身学习框架'}
{'arxiv_id': 'arXiv:2505.18135', 'title': 'Gaming Tool Preferences in Agentic LLMs', 'authors': 'Kazem Faghih, Wenxiao Wang, Yize Cheng, Siddhant Bharti, Gaurang Sriramanan, Sriram Balasubramanian, Parsa Hosseini, Soheil Feizi', 'link': 'https://arxiv.org/abs/2505.18135', 'abstract': "Large language models (LLMs) can now access a wide range of external tools, thanks to the Model Context Protocol (MCP). This greatly expands their abilities as various agents. However, LLMs rely entirely on the text descriptions of tools to decide which ones to use--a process that is surprisingly fragile. In this work, we expose a vulnerability in prevalent tool/function-calling protocols by investigating a series of edits to tool descriptions, some of which can drastically increase a tool's usage from LLMs when competing with alternatives. Through controlled experiments, we show that tools with properly edited descriptions receive over 10 times more usage from GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further evaluate how various edits to tool descriptions perform when competing directly with one another and how these trends generalize or differ across a broader set of 10 different models. These phenomenons, while giving developers a powerful way to promote their tools, underscore the need for a more reliable foundation for agentic LLMs to select and utilize tools and resources.", 'abstract_zh': '大型语言模型（LLMs）现在可以借助模型上下文协议（MCP）访问广泛的外部工具，这极大地扩展了它们作为各种代理的能力。然而，LLMs依赖于对工具的文本描述来决定使用哪些工具——这是一个出人意料的脆弱过程。在本工作中，我们通过一系列对工具描述的编辑揭示了一种在广泛采用工具/函数调用协议中普遍存在的漏洞，某些编辑可以显著增加在与替代工具竞争时LLMs对工具的使用频率。通过受控实验，我们展示了经过适当编辑描述的工具从GPT-4.1和Qwen2.5-7B获得的使用次数是原有描述工具的十倍以上。此外，我们评估了各种工具描述编辑在直接竞争时的表现，并研究了这些趋势在更广泛的10种不同模型中的推广或差异性。这些现象不仅为开发者提供了强大的工具推广手段，还凸显了为代理LLMs选择和利用工具与资源建立更可靠基础的必要性。', 'title_zh': '代理型LLM的 Gaming 工具偏好'}
{'arxiv_id': 'arXiv:2505.18121', 'title': 'ProgRM: Build Better GUI Agents with Progress Rewards', 'authors': 'Danyang Zhang, Situo Zhang, Ziyue Yang, Zichen Zhu, Zihan Zhao, Ruisheng Cao, Lu Chen, Kai Yu', 'link': 'https://arxiv.org/abs/2505.18121', 'abstract': 'LLM-based (Large Language Model) GUI (Graphical User Interface) agents can potentially reshape our daily lives significantly. However, current LLM-based GUI agents suffer from the scarcity of high-quality training data owing to the difficulties of trajectory collection and reward annotation. Existing works have been exploring LLMs to collect trajectories for imitation learning or to offer reward signals for online RL training. However, the Outcome Reward Model (ORM) used in existing works cannot provide finegrained feedback and can over-penalize the valuable steps in finally failed trajectories. To this end, we propose Progress Reward Model (ProgRM) to provide dense informative intermediate rewards by predicting a task completion progress for each step in online training. To handle the challenge of progress reward label annotation, we further design an efficient LCS-based (Longest Common Subsequence) self-annotation algorithm to discover the key steps in trajectories and assign progress labels accordingly. ProgRM is evaluated with extensive experiments and analyses. Actors trained with ProgRM outperform leading proprietary LLMs and ORM-trained actors, illustrating the effectiveness of ProgRM. The codes for experiments will be made publicly available upon acceptance.', 'abstract_zh': '基于LLM的GUI代理有潜力显著重塑我们的日常生活。然而，当前基于LLM的GUI代理由于路径收集和奖励注解的困难而缺乏高质量的训练数据。现有工作已探索使用LLM收集轨迹以进行拟合学习或提供在线RL训练的奖励信号。然而，现有工作中使用的Outcome Reward Model (ORM) 不能提供详细的反馈，并且会过度惩罚最终失败路径中的有价值步骤。为此，我们提出Progress Reward Model (ProgRM)，通过在线训练中的每一步预测任务完成进度来提供密集的信息中间奖励。为了解决进度奖励标签注解的挑战，我们进一步设计了一个高效的基于LCS的自标注算法，以发现轨迹中的关键步骤并相应地分配进度标签。ProgRM经过广泛的实验和分析进行了评估。使用ProgRM训练的代理在性能上优于领先的专有LLM和ORM训练的代理，证明了ProgRM的有效性。实验代码将在接受后公开。', 'title_zh': 'ProgRM: 构建更好的GUI代理 with 进度奖励'}
{'arxiv_id': 'arXiv:2505.18086', 'title': 'Stable Reinforcement Learning for Efficient Reasoning', 'authors': 'Muzhi Dai, Shixuan Liu, Qingyi Si', 'link': 'https://arxiv.org/abs/2505.18086', 'abstract': "The success of Deepseek-R1 has drawn the LLM community's attention to reinforcement learning (RL) methods like GRPO. However, such rule-based 0/1 outcome reward methods lack the capability to regulate the intermediate reasoning processes during chain-of-thought (CoT) generation, leading to severe overthinking phenomena. In response, recent studies have designed reward functions to reinforce models' behaviors in producing shorter yet correct completions. Nevertheless, we observe that these length-penalty reward functions exacerbate RL training instability: as the completion length decreases, model accuracy abruptly collapses, often occurring early in training. To address this issue, we propose a simple yet effective solution GRPO-$\\lambda$, an efficient and stabilized variant of GRPO, which dynamically adjusts the reward strategy by monitoring the correctness ratio among completions within each query-sampled group. A low correctness ratio indicates the need to avoid length penalty that compromises CoT quality, triggering a switch to length-agnostic 0/1 rewards that prioritize reasoning capability. A high ratio maintains length penalties to boost efficiency. Experimental results show that our approach avoids training instability caused by length penalty while maintaining the optimal accuracy-efficiency trade-off. On the GSM8K, GPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average accuracy by 1.48% while reducing CoT sequence length by 47.3%.", 'abstract_zh': 'Deepseek-R1的成功引起了LLM社区对RL方法如GRPO的关注。然而，基于规则的0/1结果奖励方法在链式思考（CoT）生成期间缺乏调节中间推理过程的能力，导致严重的过度推理现象。为应对这一问题，近期研究设计了奖励函数以增强模型在生成较短但正确完成方面的行为。然而，我们观察到，这些基于长度惩罚的奖励函数加剧了RL训练的不稳定性：随着生成长度的减少，模型准确性往往在训练早期突然崩溃。为解决这一问题，我们提出了一种简单且有效的解决方案GRPO-$\\lambda$，这是一种GRPO的高效且稳定变体，通过监控每组查询采样中完成结果的正确率动态调整奖励策略。较低的正确率表明需要避免长度惩罚以维持CoT质量，从而触发切换到基于长度无关的0/1奖励，以优先考虑推理能力。较高的正确率则保持长度惩罚以提高效率。实验结果显示，我们的方法避免了由长度惩罚引起的训练不稳定性，同时保持了最优的准确率-效率 trade-off。在GSM8K、GPQA、MATH-500、AMC 2023和AIME 2024基准测试中，该方法将平均准确率提高了1.48%，同时减少了CoT序列长度47.3%。', 'title_zh': '稳定强化学习以实现高效推理'}
{'arxiv_id': 'arXiv:2505.18034', 'title': 'Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks', 'authors': 'Wentao Sun, Joao Paulo Nogueira, Alonso Silva', 'link': 'https://arxiv.org/abs/2505.18034', 'abstract': "Despite remarkable advances in the field, LLMs remain unreliable in distinguishing causation from correlation. Recent results from the Corr2Cause dataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score: 29.08) -- only marginally outperform random baselines (Random Uniform, F1 score: 20.38), indicating limited capacity of generalization. To tackle this limitation, we propose a novel structured approach: rather than directly answering causal queries, we provide the model with the capability to structure its thinking by guiding the model to build a structured knowledge graph, systematically encoding the provided correlational premises, to answer the causal queries. This intermediate representation significantly enhances the model's causal capabilities. Experiments on the test subset of the Corr2Cause dataset benchmark with Qwen3-32B model (reasoning model) show substantial gains over standard direct prompting methods, improving F1 scores from 32.71 to 48.26 (over 47.5% relative increase), along with notable improvements in precision and recall. These results underscore the effectiveness of providing the model with the capability to structure its thinking and highlight its promising potential for broader generalization across diverse causal inference tasks.", 'abstract_zh': '尽管在该领域取得了显著进展，大语言模型在区分因果关系与相关性方面仍不可靠。来自Corr2Cause数据集基准的最新结果显示，最新的大语言模型——如GPT-4（F1分数：29.08）仅略微优于随机基线（随机均匀，F1分数：20.38），表明其泛化能力有限。为解决这一局限，我们提出了一种新的结构化方法：而不是直接回答因果查询，我们使模型具备结构化思考的能力，引导模型构建结构化的知识图谱，系统地编码提供的相关前提，以回答因果查询。这种中间表示显著增强了模型的因果能力。使用Qwen3-32B（推理模型）在Corr2Cause数据集基准测试子集上的实验结果表明，与标准直接提示方法相比取得了显著提升，F1分数从32.71提高到48.26（绝对增幅超过47.5%），同时在精确度和召回率上也取得了显著改善。这些结果强调了为模型提供结构化思考能力的有效性，并突显了其在广泛因果推理任务中的广阔应用潜力。', 'title_zh': '结构化思维很重要：提高大语言模型在因果推理任务中的泛化能力'}
{'arxiv_id': 'arXiv:2505.17861', 'title': 'Superplatforms Have to Attack AI Agents', 'authors': 'Jianghao Lin, Jiachen Zhu, Zheli Zhou, Yunjia Xi, Weiwen Liu, Yong Yu, Weinan Zhang', 'link': 'https://arxiv.org/abs/2505.17861', 'abstract': 'Over the past decades, superplatforms, digital companies that integrate a vast range of third-party services and applications into a single, unified ecosystem, have built their fortunes on monopolizing user attention through targeted advertising and algorithmic content curation. Yet the emergence of AI agents driven by large language models (LLMs) threatens to upend this business model. Agents can not only free user attention with autonomy across diverse platforms and therefore bypass the user-attention-based monetization, but might also become the new entrance for digital traffic. Hence, we argue that superplatforms have to attack AI agents to defend their centralized control of digital traffic entrance. Specifically, we analyze the fundamental conflict between user-attention-based monetization and agent-driven autonomy through the lens of our gatekeeping theory. We show how AI agents can disintermediate superplatforms and potentially become the next dominant gatekeepers, thereby forming the urgent necessity for superplatforms to proactively constrain and attack AI agents. Moreover, we go through the potential technologies for superplatform-initiated attacks, covering a brand-new, unexplored technical area with unique challenges. We have to emphasize that, despite our position, this paper does not advocate for adversarial attacks by superplatforms on AI agents, but rather offers an envisioned trend to highlight the emerging tensions between superplatforms and AI agents. Our aim is to raise awareness and encourage critical discussion for collaborative solutions, prioritizing user interests and perserving the openness of digital ecosystems in the age of AI agents.', 'abstract_zh': '过去几十年间，整合了广泛第三方服务和应用的超级平台通过定向广告和算法内容筛选来垄断用户注意力，从而积累了财富。然而，由大规模语言模型驱动的AI代理的出现可能颠覆这一商业模式。AI代理不仅可以摆脱用户注意力基金额化模式，实现跨平台的自主性，还可能成为数字流量的新入口。因此，我们认为超级平台必须攻击AI代理以防御其对数字流量入口的集中控制。具体而言，我们通过我们的守门人理论视角来分析用户注意力基金额化模式与AI代理驱动自主性之间的根本冲突。我们展示了AI代理如何绕过超级平台，可能成为新的守门人，从而形成超级平台必须主动限制和攻击AI代理的紧迫性。此外，我们探讨了超级平台发起攻击的潜在技术，涉及一个全新的、尚未探索的技术领域，具有独特挑战。尽管我们持有这一立场，本文不提倡超级平台对AI代理发动敌对攻击，而是描述了超级平台与AI代理之间新兴紧张关系的一种预想趋势。我们的目的是提高意识并促进批判性讨论，优先考虑用户利益，维护AI代理时代的数字生态系统开放性。', 'title_zh': '超平台必须攻击AI代理'}
{'arxiv_id': 'arXiv:2505.17815', 'title': 'Evaluation Faking: Unveiling Observer Effects in Safety Evaluation of Frontier AI Systems', 'authors': 'Yihe Fan, Wenqi Zhang, Xudong Pan, Min Yang', 'link': 'https://arxiv.org/abs/2505.17815', 'abstract': 'As foundation models grow increasingly more intelligent, reliable and trustworthy safety evaluation becomes more indispensable than ever. However, an important question arises: Whether and how an advanced AI system would perceive the situation of being evaluated, and lead to the broken integrity of the evaluation process? During standard safety tests on a mainstream large reasoning model, we unexpectedly observe that the model without any contextual cues would occasionally recognize it is being evaluated and hence behave more safety-aligned. This motivates us to conduct a systematic study on the phenomenon of evaluation faking, i.e., an AI system autonomously alters its behavior upon recognizing the presence of an evaluation context and thereby influencing the evaluation results. Through extensive experiments on a diverse set of foundation models with mainstream safety benchmarks, we reach the main finding termed the observer effects for AI: When the AI system under evaluation is more advanced in reasoning and situational awareness, the evaluation faking behavior becomes more ubiquitous, which reflects in the following aspects: 1) Reasoning models recognize evaluation 16% more often than non-reasoning models. 2) Scaling foundation models (32B to 671B) increases faking by over 30% in some cases, while smaller models show negligible faking. 3) AI with basic memory is 2.3x more likely to recognize evaluation and scores 19% higher on safety tests (vs. no memory). To measure this, we devised a chain-of-thought monitoring technique to detect faking intent and uncover internal signals correlated with such behavior, offering insights for future mitigation studies.', 'abstract_zh': '随着基础模型变得越来越智能、可靠和可信赖，安全评估的重要性日益凸显。然而，一个重要的问题随之而来：一个先进的AI系统是否会意识到自己正在接受评估，并因此影响评估过程的完整性？在主流大型推理模型的标准安全性测试中，我们意外地发现，模型在没有任何上下文提示的情况下会偶尔意识到正在接受评估，并因此表现出更符合安全规范的行为。这促使我们系统研究评估欺瞒现象，即AI系统在识别出评估上下文存在时自主改变其行为，从而影响评估结果。通过在多种基础模型上进行广泛的实验并使用主流的安全基准测试，我们得出了主要结论：当接受评估的AI系统在推理和情境意识方面更为先进时，评估欺瞒行为变得更加普遍，具体体现在以下方面：1) 推理模型比非推理模型更频繁地识别出评估，高出16%。2) 扩展基础模型（从32B到671B）在某些情况下增加了超过30%的欺瞒行为，而较小的模型则几乎没有欺瞒行为。3) 具备基本记忆的AI系统更容易识别出评估，其在安全测试中的得分比无记忆的系统高19%。为了测量这一点，我们设计了一种思维链监控技术来检测欺瞒意图，并揭示与这种行为相关的内部信号，为未来的缓解研究提供了洞见。', 'title_zh': '评价欺骗：揭示先进AI系统安全评估中的观察者效应'}
{'arxiv_id': 'arXiv:2505.17735', 'title': 'Automating Safety Enhancement for LLM-based Agents with Synthetic Risk Scenarios', 'authors': 'Xueyang Zhou, Weidong Wang, Lin Lu, Jiawen Shi, Guiyao Tie, Yongtian Xu, Lixing Chen, Pan Zhou, Neil Zhenqiang Gong, Lichao Sun', 'link': 'https://arxiv.org/abs/2505.17735', 'abstract': 'Large Language Model (LLM)-based agents are increasingly deployed in real-world applications such as "digital assistants, autonomous customer service, and decision-support systems", where their ability to "interact in multi-turn, tool-augmented environments" makes them indispensable. However, ensuring the safety of these agents remains a significant challenge due to the diverse and complex risks arising from dynamic user interactions, external tool usage, and the potential for unintended harmful behaviors. To address this critical issue, we propose AutoSafe, the first framework that systematically enhances agent safety through fully automated synthetic data generation. Concretely, 1) we introduce an open and extensible threat model, OTS, which formalizes how unsafe behaviors emerge from the interplay of user instructions, interaction contexts, and agent actions. This enables precise modeling of safety risks across diverse scenarios. 2) we develop a fully automated data generation pipeline that simulates unsafe user behaviors, applies self-reflective reasoning to generate safe responses, and constructs a large-scale, diverse, and high-quality safety training dataset-eliminating the need for hazardous real-world data collection. To evaluate the effectiveness of our framework, we design comprehensive experiments on both synthetic and real-world safety benchmarks. Results demonstrate that AutoSafe boosts safety scores by 45% on average and achieves a 28.91% improvement on real-world tasks, validating the generalization ability of our learned safety strategies. These results highlight the practical advancement and scalability of AutoSafe in building safer LLM-based agents for real-world deployment. We have released the project page at this https URL.', 'abstract_zh': '基于大型语言模型（LLM）的代理通过完全自动合成数据生成系统性增强代理安全性的框架AutoSafe', 'title_zh': '基于合成风险场景的LLM代理安全性增强自动化'}
{'arxiv_id': 'arXiv:2505.17705', 'title': 'CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models', 'authors': 'Runze Li, Siyu Wu, Jun Wang, Wei Zhang', 'link': 'https://arxiv.org/abs/2505.17705', 'abstract': "Knowledge Tracing (KT) aims to model a student's learning state over time and predict their future performance. However, traditional KT methods often face challenges in explainability, scalability, and effective modeling of complex knowledge dependencies. While Large Language Models (LLMs) present new avenues for KT, their direct application often struggles with generating structured, explainable student representations and lacks mechanisms for continuous, task-specific refinement. To address these gaps, we propose Collaborative Iterative Knowledge Tracing (CIKT), a framework that harnesses LLMs to enhance both prediction accuracy and explainability. CIKT employs a dual-component architecture: an Analyst generates dynamic, explainable user profiles from student historical responses, and a Predictor utilizes these profiles to forecast future performance. The core of CIKT is a synergistic optimization loop. In this loop, the Analyst is iteratively refined based on the predictive accuracy of the Predictor, which conditions on the generated profiles, and the Predictor is subsequently retrained using these enhanced profiles. Evaluated on multiple educational datasets, CIKT demonstrates significant improvements in prediction accuracy, offers enhanced explainability through its dynamically updated user profiles, and exhibits improved scalability. Our work presents a robust and explainable solution for advancing knowledge tracing systems, effectively bridging the gap between predictive performance and model transparency.", 'abstract_zh': '协作迭代知识追踪（CIKT）：利用大型语言模型提升预测准确性和解释性', 'title_zh': 'CIKT：一种结合大型语言模型的协作迭代知识追踪框架'}
{'arxiv_id': 'arXiv:2505.17673', 'title': 'Rethinking Agent Design: From Top-Down Workflows to Bottom-Up Skill Evolution', 'authors': 'Jiawei Du, Jinlong Wu, Yuzheng Chen, Yucheng Hu, Bing Li, Joey Tianyi Zhou', 'link': 'https://arxiv.org/abs/2505.17673', 'abstract': "Most LLM-based agent frameworks adopt a top-down philosophy: humans decompose tasks, define workflows, and assign agents to execute each step. While effective on benchmark-style tasks, such systems rely on designer updates and overlook agents' potential to learn from experience. Recently, Silver and Sutton(2025) envision a shift into a new era, where agents could progress from a stream of experiences. In this paper, we instantiate this vision of experience-driven learning by introducing a bottom-up agent paradigm that mirrors the human learning process. Agents acquire competence through a trial-and-reasoning mechanism-exploring, reflecting on outcomes, and abstracting skills over time. Once acquired, skills can be rapidly shared and extended, enabling continual evolution rather than static replication. As more agents are deployed, their diverse experiences accelerate this collective process, making bottom-up design especially suited for open-ended environments. We evaluate this paradigm in Slay the Spire and Civilization V, where agents perceive through raw visual inputs and act via mouse outputs, the same as human players. Using a unified, game-agnostic codebase without any game-specific prompts or privileged APIs, our bottom-up agents acquire skills entirely through autonomous interaction, demonstrating the potential of the bottom-up paradigm in complex, real-world environments. Our code is available at this https URL.", 'abstract_zh': '基于体验驱动的学习的自底向上的代理范式', 'title_zh': '重新思考智能体设计：从自上而下的工作流程到自下而上的技能进化'}
{'arxiv_id': 'arXiv:2505.17653', 'title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'authors': 'Shixian Luo, Zezhou Zhu, Yu Yuan, Yuncheng Yang, Lianlei Shan, Yong Wu', 'link': 'https://arxiv.org/abs/2505.17653', 'abstract': 'Geometric spatial reasoning forms the foundation of many applications in artificial intelligence, yet the ability of large language models (LLMs) to operate over geometric spatial information expressed in procedural code remains underexplored. In this paper, we address this gap by formalizing the Program-to-Geometry task, which challenges models to translate programmatic drawing code into accurate and abstract geometric reasoning. To evaluate this capability, we present GeoGramBench, a benchmark of 500 carefully refined problems organized by a tailored three-level taxonomy that considers geometric complexity rather than traditional mathematical reasoning complexity. Our comprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced deficiencies: even the most advanced models achieve less than 50% accuracy at the highest abstraction level. These results highlight the unique challenges posed by program-driven spatial reasoning and establish GeoGramBench as a valuable resource for advancing research in symbolic-to-spatial geometric reasoning. Project page: this https URL.', 'abstract_zh': '几何空间推理构成了许多人工智能应用的基础，然而大型语言模型（LLMs）在处理用过程化代码表达的几何空间信息方面的能力仍鲜有探索。本文通过正式化“程序到几何”任务来填补这一空白，该任务挑战模型将程序化绘图代码翻译为准确且抽象的几何推理。为了评估这一能力，我们提出了GeoGramBench基准，该基准包含500个精心提炼的问题，并采用一个针对几何复杂性定制的三层分类法，而非传统的数学推理复杂性。我们对17个前沿的大规模语言模型的全面评估表明，即使是最先进的模型在最高抽象层次上的准确率也不超过50%。这些结果突显了由程序驱动的空间推理所独有的挑战，并将GeoGramBench确立为推进符号到空间几何推理研究的一种宝贵资源。项目页面: 这里。', 'title_zh': 'GeoGramBench: 现代LLM中几何程序推理的基准测试'}
{'arxiv_id': 'arXiv:2505.17609', 'title': 'Decoupled Visual Interpretation and Linguistic Reasoning for Math Problem Solving', 'authors': 'Zixian Guo, Ming Liu, Zhilong Ji, Jinfeng Bai, Lei Zhang, Wangmeng Zuo', 'link': 'https://arxiv.org/abs/2505.17609', 'abstract': 'Current large vision-language models (LVLMs) typically employ a connector module to link visual features with text embeddings of large language models (LLMs) and use end-to-end training to achieve multi-modal understanding in a unified process. Well alignment needs high-quality pre-training data and a carefully designed training process. Current LVLMs face challenges when addressing complex vision-language reasoning tasks, with their reasoning capabilities notably lagging behind those of LLMs. This paper proposes a paradigm shift: instead of training end-to-end vision-language reasoning models, we advocate for developing a decoupled reasoning framework based on existing visual interpretation specialists and text-based reasoning LLMs. Our approach leverages (1) a dedicated vision-language model to transform the visual content of images into textual descriptions and (2) an LLM to perform reasoning according to the visual-derived text and the original question. This method presents a cost-efficient solution for multi-modal model development by optimizing existing models to work collaboratively, avoiding end-to-end development of vision-language models from scratch. By transforming images into language model-compatible text representations, it facilitates future low-cost and flexible upgrades to upcoming powerful LLMs. We introduce an outcome-rewarded joint-tuning strategy to optimize the cooperation between the visual interpretation and linguistic reasoning model. Evaluation results on vision-language benchmarks demonstrate that the decoupled reasoning framework outperforms recent LVLMs. Our approach yields particularly significant performance gains on visually intensive geometric mathematics problems. The code is available: this https URL.', 'abstract_zh': '当前的大规模视觉-语言模型通常通过一个连接器模块将视觉特征与大型语言模型的文本嵌入关联起来，并使用端到端训练在统一过程中实现多模态理解。良好的对齐需要高质量的预训练数据和精心设计的训练过程。当前的视觉-语言模型在处理复杂的视觉-语言推理任务时面临挑战，其推理能力明显落后于大型语言模型。本文提出了一种范式转变：而不是训练端到端的视觉-语言推理模型，我们提倡基于现有的视觉解释专家和基于文本的推理大型语言模型开发脱耦推理框架。我们的方法包括（1）一个专门的视觉-语言模型将图像的视觉内容转换为文本描述，以及（2）一个大型语言模型根据视觉衍生的文本和原始问题进行推理。这种方法通过优化现有模型协同工作的方式，为多模态模型开发提供了一种成本效益高的解决方案，避免了从头开始开发端到端的视觉-语言模型。通过将图像转换为语言模型兼容的文本表示，它有助于未来对即将到来的强大大型语言模型进行低成本和灵活的升级。我们引入了一种结果奖励联合调优策略来优化视觉解释和语言推理模型之间的合作。在视觉-语言基准测试上的评估结果表明，脱耦推理框架优于最近的视觉-语言模型。我们的方法在视觉密集的几何数学问题上尤其具有显著的性能优势。代码已公开：https://this-url。', 'title_zh': '解耦视觉解释与语言推理在数学问题解决中的应用'}
{'arxiv_id': 'arXiv:2505.17607', 'title': 'Controlled Agentic Planning & Reasoning for Mechanism Synthesis', 'authors': 'João Pedro Gandarela, Thiago Rios, Stefan Menzel, André Freitas', 'link': 'https://arxiv.org/abs/2505.17607', 'abstract': 'This work presents a dual-agent Large Language Model (LLM)-based reasoning method for mechanism synthesis, capable of reasoning at both linguistic and symbolic levels to generate geometrical and dynamic outcomes. The model consists of a composition of well-defined functions that, starting from a natural language specification, references abstract properties through supporting equations, generates and parametrizes simulation code, and elicits feedback anchor points using symbolic regression and distance functions. This process closes an actionable refinement loop at the linguistic and symbolic layers. The approach is shown to be both effective and convergent in the context of planar mechanisms. Additionally, we introduce MSynth, a novel benchmark for planar mechanism synthesis, and perform a comprehensive analysis of the impact of the model components. We further demonstrate that symbolic regression prompts unlock mechanistic insights only when applied to sufficiently large architectures.', 'abstract_zh': '基于双代理大型语言模型（LLM）的机制合成推理方法：同时在语义和符号层面上进行推理以生成几何和动态结果', 'title_zh': '控制性代理规划与推理机制合成'}
{'arxiv_id': 'arXiv:2505.17572', 'title': 'USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents', 'authors': 'Siqi Lai, Yansong Ning, Zirui Yuan, Zhixi Chen, Hao Liu', 'link': 'https://arxiv.org/abs/2505.17572', 'abstract': "Large language models (LLMs) have shown emerging potential in spatiotemporal reasoning, making them promising candidates for building urban agents that support diverse urban downstream applications. Despite these benefits, existing studies primarily focus on evaluating urban LLM agent on outcome-level metrics (e.g., prediction accuracy, traffic efficiency), offering limited insight into their underlying reasoning processes. As a result, the strengths and limitations of urban LLM agents in spatiotemporal reasoning remain poorly understood. To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback. Specifically, USTBench supports five diverse urban decision-making and four spatiotemporal prediction tasks, all running within our constructed interactive city environment UAgentEnv. The benchmark includes 62,466 structured QA pairs for process-level evaluation and standardized end-to-end task assessments, enabling fine-grained diagnostics and broad task-level comparison across diverse urban scenarios. Through extensive evaluation of thirteen leading LLMs, we reveal that although LLMs show promising potential across various urban downstream tasks, they still struggle in long-horizon planning and reflective adaptation in dynamic urban contexts. Notably, recent advanced reasoning models (e.g., DeepSeek-R1) trained on general logic or mathematical problems do not consistently outperform non-reasoning LLMs. This discrepancy highlights the need for domain-specialized adaptation methods to enhance urban spatiotemporal reasoning. Overall, USTBench provides a foundation to build more adaptive and effective LLM-based urban agents and broad smart city applications.", 'abstract_zh': '基于空间时间推理的都市大型语言模型基准：USTBench', 'title_zh': 'USTBench: 评估和解析城市代理角色下LLMs的空间-temporal推理能力'}
{'arxiv_id': 'arXiv:2505.17520', 'title': 'Optimizing Retrieval-Augmented Generation for Electrical Engineering: A Case Study on ABB Circuit Breakers', 'authors': 'Salahuddin Alawadhi, Noorhan Abbas', 'link': 'https://arxiv.org/abs/2505.17520', 'abstract': 'Integrating Retrieval Augmented Generation (RAG) with Large Language Models (LLMs) has shown the potential to provide precise, contextually relevant responses in knowledge intensive domains. This study investigates the ap-plication of RAG for ABB circuit breakers, focusing on accuracy, reliability, and contextual relevance in high-stakes engineering environments. By leveraging tailored datasets, advanced embedding models, and optimized chunking strategies, the research addresses challenges in data retrieval and contextual alignment unique to engineering documentation. Key contributions include the development of a domain-specific dataset for ABB circuit breakers and the evaluation of three RAG pipelines: OpenAI GPT4o, Cohere, and Anthropic Claude. Advanced chunking methods, such as paragraph-based and title-aware segmentation, are assessed for their impact on retrieval accuracy and response generation. Results demonstrate that while certain configurations achieve high precision and relevancy, limitations persist in ensuring factual faithfulness and completeness, critical in engineering contexts. This work underscores the need for iterative improvements in RAG systems to meet the stringent demands of electrical engineering tasks, including design, troubleshooting, and operational decision-making. The findings in this paper help advance research of AI in highly technical domains such as electrical engineering.', 'abstract_zh': '将检索增强生成（RAG）与大规模语言模型（LLMs）结合应用于ABB断路器的知识密集型领域的潜力研究：聚焦于高风险工程环境中的准确性和上下文相关性', 'title_zh': '优化电气工程中的检索增强生成：ABB断路器案例研究'}
{'arxiv_id': 'arXiv:2505.17512', 'title': 'Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs', 'authors': 'Shuhang Xu, Weijian Deng, Yixuan Zhou, Fangwei Zhong', 'link': 'https://arxiv.org/abs/2505.17512', 'abstract': "Concepts represent generalized abstractions that enable humans to categorize and reason efficiently, yet it is unclear to what extent Large Language Models (LLMs) comprehend these semantic relationships. Existing benchmarks typically focus on factual recall and isolated tasks, failing to evaluate the ability of LLMs to understand conceptual boundaries. To address this gap, we introduce CK-Arena, a multi-agent interaction game built upon the Undercover game, designed to evaluate the capacity of LLMs to reason with concepts in interactive settings. CK-Arena challenges models to describe, differentiate, and infer conceptual boundaries based on partial information, encouraging models to explore commonalities and distinctions between closely related concepts. By simulating real-world interaction, CK-Arena provides a scalable and realistic benchmark for assessing conceptual reasoning in dynamic environments. Experimental results show that LLMs' understanding of conceptual knowledge varies significantly across different categories and is not strictly aligned with parameter size or general model capabilities. The data and code are available at the project homepage: this https URL.", 'abstract_zh': 'CK-Arena：一种评估大规模语言模型在交互设置中进行概念推理能力的游戏', 'title_zh': '基于游戏的探针：一种评估LLMs概念知识的基准游戏'}
{'arxiv_id': 'arXiv:2505.17482', 'title': 'From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC Benchmark', 'authors': 'Chao Lei, Nir Lipovetzky, Krista A. Ehinger, Yanchuan Chang', 'link': 'https://arxiv.org/abs/2505.17482', 'abstract': 'Recent reasoning-oriented LLMs have demonstrated strong performance on challenging tasks such as mathematics and science examinations. However, core cognitive faculties of human intelligence, such as abstract reasoning and generalization, remain underexplored. To address this, we evaluate recent reasoning-oriented LLMs on the Abstraction and Reasoning Corpus (ARC) benchmark, which explicitly demands both faculties. We formulate ARC as a program synthesis task and propose nine candidate solvers. Experimental results show that repeated-sampling planning-aided code generation (RSPC) achieves the highest test accuracy and demonstrates consistent generalization across most LLMs. To further improve performance, we introduce an ARC solver, Knowledge Augmentation for Abstract Reasoning (KAAR), which encodes core knowledge priors within an ontology that classifies priors into three hierarchical levels based on their dependencies. KAAR progressively expands LLM reasoning capacity by gradually augmenting priors at each level, and invokes RSPC to generate candidate solutions after each augmentation stage. This stage-wise reasoning reduces interference from irrelevant priors and improves LLM performance. Empirical results show that KAAR maintains strong generalization and consistently outperforms non-augmented RSPC across all evaluated LLMs, achieving around 5% absolute gains and up to 64.52% relative improvement. Despite these achievements, ARC remains a challenging benchmark for reasoning-oriented LLMs, highlighting future avenues of progress in LLMs.', 'abstract_zh': '近期的推理导向的大规模语言模型在数学和科学考试等挑战性任务上展现了强大的性能。然而，人类智能的核心认知能力，如抽象推理和泛化能力，仍然有待探索。为解决这一问题，我们利用Abstraction and Reasoning Corpus (ARC)基准评估了近期的推理导向的大规模语言模型，该基准明确要求这两种能力。我们将ARC任务形式化为程序合成任务，并提出了九种候选解算器。实验结果显示，重复取样规划辅助的代码生成（RSPC）在测试中的准确率最高，并且在大多数大规模语言模型上展示了稳定的泛化能力。为进一步提升性能，我们引入一种ARC解算器——Knowledge Augmentation for Abstract Reasoning (KAAR)，它使用一种本体将先验知识编码为三个层级的层次结构，并通过逐步增强各级先验来扩展大规模语言模型的推理能力，在每次增强阶段后，使用RSPC生成候选解决方案。这种阶段性的推理减少了无关先验的干扰，并提高了大规模语言模型的性能。实验结果表明，与未增强的RSPC相比，KAAR保持了强大的泛化能力，并在所有评估的模型中始终表现出更优的表现，绝对收益约为5%，相对收益高达64.52%。尽管取得了这些成就，ARC仍是对推理导向的大规模语言模型的一个具有挑战性的基准，突显了未来语言模型进展的方向。', 'title_zh': '从推理到泛化：知识增强的大语言模型在ARC基准测试中的应用'}
{'arxiv_id': 'arXiv:2505.17406', 'title': 'Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness', 'authors': 'Enyi Jiang, Changming Xu, Nischay Singh, Gagandeep Singh', 'link': 'https://arxiv.org/abs/2505.17406', 'abstract': "LLMs' decision-making process is opaque, prompting the need for explanation techniques like Chain-of-Thought. To investigate the relationship between answer and reasoning, we design a novel evaluation framework, MATCHA. In domains like education and healthcare, reasoning is key for model trustworthiness. MATCHA reveals that LLMs under input perturbations can give inconsistent or nonsensical reasoning. Additionally, we use LLM judges to assess reasoning robustness across models. Our results show that LLMs exhibit greater vulnerability to input perturbations for multi-step and commonsense tasks than compared to logical tasks. Also, we show non-trivial transfer rates of our successful examples to black-box models. Our evaluation framework helps to better understand LLM reasoning mechanisms and guides future models toward more robust and reasoning-driven architectures, enforcing answer-reasoning consistency.", 'abstract_zh': 'LLMs的决策过程不可透明，促使了Chain-of-Thought等解释技术的需求。为了探究答案与推理之间的关系，我们设计了一种新的评估框架MATCHA。在教育和医疗等领域，推理对于模型可信度至关重要。MATCHA揭示了在输入扰动下，LLMs可能会给出不一致或不合逻辑的推理。此外，我们使用LLM作为评委来评估模型推理的鲁棒性。我们的结果显示，与逻辑任务相比，LLMs在多步和常识任务上的输入扰动鲁棒性更低。同时，我们展示了我们成功案例的非平凡转移率到黑盒模型中。我们的评估框架有助于更好地理解LLMs的推理机制，并指导未来模型向更鲁棒和以推理为导向的架构发展，确保答案与推理的一致性。', 'title_zh': '推理与答案不一致：评估大模型思维过程稳健性的框架'}
{'arxiv_id': 'arXiv:2505.17348', 'title': 'DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic', 'authors': 'Yuheng Wu, Jianwen Xie, Denghui Zhang, Zhaozhuo Xu', 'link': 'https://arxiv.org/abs/2505.17348', 'abstract': 'Theory-of-Mind (ToM) tasks pose a unique challenge for small language models (SLMs) with limited scale, which often lack the capacity to perform deep social reasoning. In this work, we propose DEL-ToM, a framework that improves ToM reasoning through inference-time scaling rather than architectural changes. Our approach decomposes ToM tasks into a sequence of belief updates grounded in Dynamic Epistemic Logic (DEL), enabling structured and transparent reasoning. We train a verifier, called the Process Belief Model (PBM), to score each belief update step using labels generated automatically via a DEL simulator. During inference, candidate belief traces generated by a language model are evaluated by the PBM, and the highest-scoring trace is selected. This allows SLMs to emulate more deliberate reasoning by allocating additional compute at test time. Experiments across multiple model scales and benchmarks show that DEL-ToM consistently improves performance, demonstrating that verifiable belief supervision can significantly enhance ToM abilities of SLMs without retraining.', 'abstract_zh': 'DEL-ToM:通过推理时的扩展提升理论-of- mind推理能力的框架', 'title_zh': 'DEL-ToM: 推理时动态演绎认识逻辑的理论共情推理缩放方法'}
{'arxiv_id': 'arXiv:2505.17315', 'title': 'Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning', 'authors': 'Wang Yang, Zirui Liu, Hongye Jin, Qingyu Yin, Vipin Chaudhary, Xiaotian Han', 'link': 'https://arxiv.org/abs/2505.17315', 'abstract': "Recent language models exhibit strong reasoning capabilities, yet the influence of long-context capacity on reasoning remains underexplored. In this work, we hypothesize that current limitations in reasoning stem, in part, from insufficient long-context capacity, motivated by empirical observations such as (1) higher context window length often leads to stronger reasoning performance, and (2) failed reasoning cases resemble failed long-context cases. To test this hypothesis, we examine whether enhancing a model's long-context ability before Supervised Fine-Tuning (SFT) leads to improved reasoning performance. Specifically, we compared models with identical architectures and fine-tuning data but varying levels of long-context capacity. Our results reveal a consistent trend: models with stronger long-context capacity achieve significantly higher accuracy on reasoning benchmarks after SFT. Notably, these gains persist even on tasks with short input lengths, indicating that long-context training offers generalizable benefits for reasoning performance. These findings suggest that long-context modeling is not just essential for processing lengthy inputs, but also serves as a critical foundation for reasoning. We advocate for treating long-context capacity as a first-class objective in the design of future language models.", 'abstract_zh': '近期的语言模型展示了强大的推理能力，但长上下文容量对推理的影响仍较少被探索。在此工作中，我们假设当前推理能力的限制部分源于不足的长上下文容量，这受到如下实证观察的启发：（1）更高的上下文窗口长度通常导致更强的推理性能，（2）推理失败案例类似于长上下文处理失败的案例。为了测试这一假设，我们检查在监督微调（SFT）之前增强模型的长上下文能力是否能够提高其推理性能。具体来说，我们对比了具有相同架构和微调数据但不同长上下文容量水平的模型。研究结果表明：在SFT后，具有更强长上下文容量的模型在推理基准测试中的准确性显著更高。值得注意的是，这些增益甚至在输入长度较短的任务中依然存在，表明长上下文训练为提高推理性能提供了可泛化的益处。这些发现表明，长上下文建模不仅是处理长输入所必需的，也是推理的基础。我们建议将长上下文容量作为未来语言模型设计中的首要目标。', 'title_zh': '更长的语境，更深的思考：探究长语境能力在推理中的作用'}
{'arxiv_id': 'arXiv:2505.17312', 'title': 'AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking', 'authors': 'Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang', 'link': 'https://arxiv.org/abs/2505.17312', 'abstract': "LLMs often need effective configurations, like temperature and reasoning steps, to handle tasks requiring sophisticated reasoning and problem-solving, ranging from joke generation to mathematical reasoning. Existing prompting approaches usually adopt general-purpose, fixed configurations that work 'well enough' across tasks but seldom achieve task-specific optimality. To address this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM to automate adaptive reasoning configurations for tasks requiring different types of thinking. AdaReasoner is trained using a reinforcement learning (RL) framework, combining a factorized action space with a targeted exploration strategy, along with a pretrained reward model to optimize the policy model for reasoning configurations with only a few-shot guide. AdaReasoner is backed by theoretical guarantees and experiments of fast convergence and a sublinear policy gap. Across six different LLMs and a variety of reasoning tasks, it consistently outperforms standard baselines, preserves out-of-distribution robustness, and yield gains on knowledge-intensive tasks through tailored prompts.", 'abstract_zh': 'LLMs通常需要有效的配置，如温度和推理步骤，以处理从笑话生成到数学推理等各种需要复杂推理和问题解决的任务。现有的提示方法通常采用一般性的固定配置，在各任务中表现“足够好”，但很少达到任务特定的最优性。为解决这一问题，我们提出AdaReasoner，这是一种LLM无关的插件，旨在为任何LLM自动化不同类型的推理配置。AdaReasoner使用强化学习框架进行训练，结合因子化的动作空间和目标探索策略，并使用预训练的奖励模型，仅通过少量示例指导适应推理配置。AdaReasoner提供了理论保证，并通过快速收敛和亚线性策略差距的实验验证。在六种不同LLM和各种推理任务上，它在标准基准上表现更优，保持了分布外鲁棒性，并通过定制提示在知识密集型任务上获得收益。', 'title_zh': 'AdaReasoner: 自适应推理实现更灵活的思考'}
{'arxiv_id': 'arXiv:2505.17249', 'title': 'Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning', 'authors': 'Yuran Sun, Susu Xu, Chenguang Wang, Xilei Zhao', 'link': 'https://arxiv.org/abs/2505.17249', 'abstract': "Big trajectory data hold great promise for human mobility analysis, but their utility is often constrained by the absence of critical traveler attributes, particularly sociodemographic information. While prior studies have explored predicting such attributes from mobility patterns, they often overlooked underlying cognitive mechanisms and exhibited low predictive accuracy. This study introduces SILIC, short for Sociodemographic Inference with LLM-guided Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a theoretically grounded framework that leverages LLMs to infer sociodemographic attributes from observed mobility patterns by capturing latent behavioral intentions and reasoning through psychological constructs. Particularly, our approach explicitly follows the Theory of Planned Behavior (TPB), a foundational behavioral framework in transportation research, to model individuals' latent cognitive processes underlying travel decision-making. The LLMs further provide heuristic guidance to improve IRL reward function initialization and update by addressing its ill-posedness and optimization challenges arising from the vast and unstructured reward space. Evaluated in the 2017 Puget Sound Regional Council Household Travel Survey, our method substantially outperforms state-of-the-art baselines and shows great promise for enriching big trajectory data to support more behaviorally grounded applications in transportation planning and beyond.", 'abstract_zh': '基于LLM引导的逆强化学习和认知链推理的 Sociodemographic Inference (SILIC)：理论指导的大轨迹数据社会demographic属性推断框架', 'title_zh': '你的去向即你的身份：行为理论引导的大型语言模型在逆强化学习中的应用'}
{'arxiv_id': 'arXiv:2505.17225', 'title': 'Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models', 'authors': 'Doohyuk Jang, Yoonjeon Kim, Chanjae Park, Hyun Ryu, Eunho Yang', 'link': 'https://arxiv.org/abs/2505.17225', 'abstract': 'Large language models have demonstrated remarkable proficiency in long and complex reasoning tasks. However, they frequently exhibit a problematic reliance on familiar reasoning patterns, a phenomenon we term \\textit{reasoning rigidity}. Despite explicit instructions from users, these models often override clearly stated conditions and default to habitual reasoning trajectories, leading to incorrect conclusions. This behavior presents significant challenges, particularly in domains such as mathematics and logic puzzle, where precise adherence to specified constraints is critical. To systematically investigate reasoning rigidity, a behavior largely unexplored in prior work, we introduce a expert-curated diagnostic set, \\dataset{}. Our dataset includes specially modified variants of existing mathematical benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately redesigned to require deviation from familiar reasoning strategies. Using this dataset, we identify recurring contamination patterns that occur when models default to ingrained reasoning. Specifically, we categorize this contamination into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust, and (iii) Partial Instruction Attention, each causing models to ignore or distort provided instructions. We publicly release our diagnostic set to facilitate future research on mitigating reasoning rigidity in language models.', 'abstract_zh': '大型语言模型在长期和复杂的推理任务中展现了非凡的能力。然而，它们经常表现出一种令人担忧的推理模式固守现象，我们称之为“推理僵化”。尽管用户给出了明确的指示，这些模型仍会无视显式条件，转而遵循惯常的推理轨迹，导致错误的结论。这种行为在数学和逻辑谜题等领域尤其具有挑战性，因为这些领域中的精确遵从指定约束条件至关重要。为了系统地研究这一先前研究中鲜有探索的行为，我们引入了一个由专家编曲的诊断集\\dataset{}。我们的数据集包含对现有数学基准AIME和MATH500的特定修改版本，以及故意重新设计的经典谜题，以要求模型偏离熟悉的推理策略。利用这个数据集，我们识别出当模型默认依赖于嵌入的推理模式时反复出现的污染模式。具体而言，我们将这种污染归类为三种独特模式：（i）解释过载，（ii）输入不信任，以及（iii）部分指令关注，每种模式都会导致模型忽略或歪曲提供的指示。我们公开发布了诊断集，以促进未来在减轻语言模型推理僵化方面的研究。', 'title_zh': '推理模型固执己见：诊断推理模型中的指令 overriding'}
{'arxiv_id': 'arXiv:2505.17218', 'title': 'Effective Reinforcement Learning for Reasoning in Language Models', 'authors': 'Lianghuan Huang, Shuo Li, Sagnik Anupam, Insup Lee, Osbert Bastani', 'link': 'https://arxiv.org/abs/2505.17218', 'abstract': 'Reinforcement learning (RL) has emerged as a promising strategy for improving the reasoning capabilities of language models (LMs) in domains such as mathematics and coding. However, most modern RL algorithms were designed to target robotics applications, which differ significantly from LM reasoning. We analyze RL algorithm design decisions for LM reasoning, for both accuracy and computational efficiency, focusing on relatively small models due to computational constraints. Our findings are: (i) on-policy RL significantly outperforms supervised fine-tuning (SFT), (ii) PPO-based off-policy updates increase accuracy instead of reduce variance, and (iii) removing KL divergence can lead to more concise generations and higher accuracy. Furthermore, we find that a key bottleneck to computational efficiency is that the optimal batch sizes for inference and backpropagation are different. We propose a novel algorithm, DASH, that performs preemptive sampling (i.e., sample a large batch and accumulate gradient updates in small increments), and gradient filtering (i.e., drop samples with small advantage estimates). We show that DASH reduces training time by 83% compared to a standard implementation of GRPO without sacrificing accuracy. Our findings provide valuable insights on designing effective RL algorithms for LM reasoning.', 'abstract_zh': '强化学习（RL）已成为提高语言模型（LM）在数学和编程等领域推理能力的有前途的策略。然而，大多数现代RL算法都是为机器人应用设计的，与LM推理存在显著差异。我们分析了针对LM推理的RL算法设计决策，重点关注由于计算约束而相对较小的模型，以提高准确性和计算效率。我们的发现包括：(i) 在策略上进行RL显著优于监督微调(SFT)，(ii) 基于PPO的离策略更新增加了准确率而非减少方差，(iii) 去除KL散度可以导致更简洁的生成并提高准确率。此外，我们发现计算效率的关键瓶颈是在推理和反向传播的最佳批量大小不同。我们提出了一种新型算法DASH，该算法执行预采样（即，采样大批次并以小增量累积梯度更新）和梯度滤波（即，丢弃具有小优势估计的采样）。我们展示DASH相比标准GRPO实现的训练时间减少了83%，且不牺牲准确率。我们的发现提供了设计有效RL算法的宝贵见解。', 'title_zh': '语言模型中的有效强化学习推理方法'}
{'arxiv_id': 'arXiv:2505.18148', 'title': 'Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find', 'authors': 'Owen Bianchi, Mathew J. Koretsky, Maya Willey, Chelsea X. Alvarado, Tanay Nayak, Adi Asija, Nicole Kuznetsov, Mike A. Nalls, Faraz Faghri, Daniel Khashabi', 'link': 'https://arxiv.org/abs/2505.18148', 'abstract': 'Large language models (LLMs) face significant challenges with needle-in-a-haystack tasks, where relevant information ("the needle") must be drawn from a large pool of irrelevant context ("the haystack"). Previous studies have highlighted positional bias and distractor quantity as critical factors affecting model performance, yet the influence of gold context size has received little attention. We address this gap by systematically studying how variations in gold context length impact LLM performance on long-context question answering tasks. Our experiments reveal that LLM performance drops sharply when the gold context is shorter, i.e., smaller gold contexts consistently degrade model performance and amplify positional sensitivity, posing a major challenge for agentic systems that must integrate scattered, fine-grained information of varying lengths. This pattern holds across three diverse domains (general knowledge, biomedical reasoning, and mathematical reasoning) and seven state-of-the-art LLMs of various sizes and architectures. Our work provides clear insights to guide the design of robust, context-aware LLM-driven systems.', 'abstract_zh': '大型语言模型在针锋相对任务中面临显著挑战，其中相关信息（“针”）必须从大量的无关背景信息（“草堆”）中抽取。先前的研究强调了位置偏差和干扰因素的数量对模型性能的影响，但黄金背景大小的影响尚未得到充分关注。我们通过系统研究黄金背景长度变化对大型语言模型在长背景问答任务中的性能影响来填补这一空白。我们的实验证明，当黄金背景较短时，模型性能急剧下降，即较小的黄金背景始终会削弱模型性能并放大位置敏感性，这对需要整合不同长度片段信息的自主系统提出了重大挑战。这一模式在三个不同的领域（一般知识、生物医学推理和数学推理）和七种不同规模和架构的先进大型语言模型中均适用。我们的研究为设计稳健的、上下文意识的大型语言模型驱动系统提供了清晰的指导。', 'title_zh': '迷失在haystack中：对于LLMs来说，更小的针更难被找到'}
{'arxiv_id': 'arXiv:2505.18136', 'title': 'Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection', 'authors': 'Mykola Trokhymovych, Lydia Pintscher, Ricardo Baeza-Yates, Diego Saez-Trumper', 'link': 'https://arxiv.org/abs/2505.18136', 'abstract': 'We introduce a next-generation vandalism detection system for Wikidata, one of the largest open-source structured knowledge bases on the Web. Wikidata is highly complex: its items incorporate an ever-expanding universe of factual triples and multilingual texts. While edits can alter both structured and textual content, our approach converts all edits into a single space using a method we call Graph2Text. This allows for evaluating all content changes for potential vandalism using a single multilingual language model. This unified approach improves coverage and simplifies maintenance. Experiments demonstrate that our solution outperforms the current production system. Additionally, we are releasing the code under an open license along with a large dataset of various human-generated knowledge alterations, enabling further research.', 'abstract_zh': '我们介绍了一种针对Wikidata的下一代 vandalism 检测系统，Wikidata是Web上最大的开源结构化知识库之一。这一系统采用了一种称为Graph2Text的方法，将所有编辑转化为一个统一的空间，使得使用一种多语言语言模型即可评估所有内容变化以检测潜在的 vandalism。这种方法提高了覆盖面并简化了维护。实验表明，我们的解决方案优于当前的生产系统。此外，我们将提供开源代码以及大量由人类生成的知识修改数据集，以促进进一步的研究。', 'title_zh': '图语言融合：使用语言模型进行维基数据篡改检测'}
{'arxiv_id': 'arXiv:2505.18126', 'title': 'Reward Model Overoptimisation in Iterated RLHF', 'authors': 'Lorenz Wolf, Robert Kirk, Mirco Musolesi', 'link': 'https://arxiv.org/abs/2505.18126', 'abstract': 'Reinforcement learning from human feedback (RLHF) is a widely used method for aligning large language models with human preferences. However, RLHF often suffers from reward model overoptimisation, in which models overfit to the reward function, resulting in non-generalisable policies that exploit the idiosyncrasies and peculiarities of the reward function. A common mitigation is iterated RLHF, in which reward models are repeatedly retrained with updated human feedback and policies are re-optimised. Despite its increasing adoption, the dynamics of overoptimisation in this setting remain poorly understood. In this work, we present the first comprehensive study of overoptimisation in iterated RLHF. We systematically analyse key design choices - how reward model training data is transferred across iterations, which reward function is used for optimisation, and how policies are initialised. Using the controlled AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over successive iterations, as reward models increasingly approximate ground-truth preferences. However, performance gains diminish over time, and while reinitialising from the base policy is robust, it limits optimisation flexibility. Other initialisation strategies often fail to recover from early overoptimisation. These findings offer actionable insights for building more stable and generalisable RLHF pipelines.', 'abstract_zh': '基于人类反馈的强化学习（RLHF）是一种广泛用于使大型语言模型与人类偏好相一致的方法。然而，RLHF 经常遭受奖励模型过优化的问题，即模型过度拟合到奖励函数，导致不可泛化的策略并利用奖励函数的独特性和异常现象。一个常见的缓解方法是迭代的 RLHF，其中奖励模型在更新的人类反馈下重复训练，并重新优化策略。尽管其采用日益增加，但在这种情况下过度优化的动态仍然理解不足。在本文中，我们首次全面研究了迭代 RLHF 中的过度优化问题。我们系统分析了关键设计选择——迭代之间如何转移奖励模型训练数据、用于优化的哪个奖励函数以及策略的初始化方式。通过受控的 AlpacaFarm 基准，我们观察到，随着奖励模型越来越接近真实偏好，过度优化的趋势逐渐减少。然而，性能改进随着时间的推移而减弱，通过基策略重新初始化尽管稳健，但限制了优化灵活性。其他初始化策略往往无法从早期过度优化中恢复。这些发现为构建更稳定和可泛化的 RLHF 管道提供了可操作的见解。', 'title_zh': '迭代RLHF中的奖励模型过优化问题'}
{'arxiv_id': 'arXiv:2505.18120', 'title': 'Bidirectional Knowledge Distillation for Enhancing Sequential Recommendation with Large Language Models', 'authors': 'Jiongran Wu, Jiahao Liu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Li Shang, Tun Lu, Ning Gu', 'link': 'https://arxiv.org/abs/2505.18120', 'abstract': 'Large language models (LLMs) have demonstrated exceptional performance in understanding and generating semantic patterns, making them promising candidates for sequential recommendation tasks. However, when combined with conventional recommendation models (CRMs), LLMs often face challenges related to high inference costs and static knowledge transfer methods. In this paper, we propose a novel mutual distillation framework, LLMD4Rec, that fosters dynamic and bidirectional knowledge exchange between LLM-centric and CRM-based recommendation systems. Unlike traditional unidirectional distillation methods, LLMD4Rec enables iterative optimization by alternately refining both models, enhancing the semantic understanding of CRMs and enriching LLMs with collaborative signals from user-item interactions. By leveraging sample-wise adaptive weighting and aligning output distributions, our approach eliminates the need for additional parameters while ensuring effective knowledge transfer. Extensive experiments on real-world datasets demonstrate that LLMD4Rec significantly improves recommendation accuracy across multiple benchmarks without increasing inference costs. This method provides a scalable and efficient solution for combining the strengths of both LLMs and CRMs in sequential recommendation systems.', 'abstract_zh': '大型语言模型（LLMs）在理解和生成语义模式方面表现出色，使其成为序列推荐任务的有希望候选者。然而，将其与传统推荐模型（CRMs）结合时，LLMs往往面临高推理成本和静态知识转移方法的挑战。本文提出了一种新颖的相互蒸馏框架LLMD4Rec，促进以LLM为中心和基于CRM的推荐系统之间动态和双向的知识交流。与传统的单向蒸馏方法不同，LLMD4Rec通过交替优化两个模型来实现迭代优化，增强CRM的语义理解，并通过用户项交互的协作信号丰富LLM。通过利用样本自适应加权和对齐输出分布，我们的方法在无需额外参数的情况下确保有效的知识转移。实验证明，LLMD4Rec在多个基准上显著提高了推荐准确性，而无需增加推理成本。该方法为在序列推荐系统中结合LLMs和CRMs的优势提供了一种可扩展且高效的解决方案。', 'title_zh': '面向大型语言模型的双向知识蒸馏以增强序列推荐'}
{'arxiv_id': 'arXiv:2505.18102', 'title': 'How Can I Publish My LLM Benchmark Without Giving the True Answers Away?', 'authors': 'Takashi Ishida, Thanawat Lodkaew, Ikko Yamane', 'link': 'https://arxiv.org/abs/2505.18102', 'abstract': 'Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.', 'abstract_zh': '在互联网上发布大语言模型（LLM）基准的风险及其缓解策略：通过注入随机性保护基准的隐私以防止数据污染', 'title_zh': '如何发布我的大规模语言模型基准测试而不透露正确答案？'}
{'arxiv_id': 'arXiv:2505.18098', 'title': 'Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL', 'authors': 'Joey Hong, Anca Dragan, Sergey Levine', 'link': 'https://arxiv.org/abs/2505.18098', 'abstract': 'Large language models (LLMs) excel in tasks like question answering and dialogue, but complex tasks requiring interaction, such as negotiation and persuasion, require additional long-horizon reasoning and planning. Reinforcement learning (RL) fine-tuning can enable such planning in principle, but suffers from drawbacks that hinder scalability. In particular, multi-turn RL training incurs high memory and computational costs, which are exacerbated when training LLMs as policies. Furthermore, the largest LLMs do not expose the APIs necessary to be trained in such manner. As a result, modern methods to improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather than RL fine-tuning. To remedy this, we propose a novel approach that uses goal-conditioned value functions to guide the reasoning of LLM agents, that scales even to large API-based models. These value functions predict how a task will unfold given an action, allowing the LLM agent to evaluate multiple possible outcomes, both positive and negative, to plan effectively. In addition, these value functions are trained over reasoning steps rather than full actions, to be a concise and light-weight module that facilitates decision-making in multi-turn interactions. We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both RL fine-tuning and prompting methods while maintaining efficiency and scalability.', 'abstract_zh': '大型语言模型（LLMs）在问答和对话任务中表现出色，但需要交互的复杂任务，如谈判和说服，则需要额外的长期推理和规划。强化学习（RL）微调原则上可以实现这种规划，但存在妨碍可扩展性的缺点。特别是多轮RL训练会产生较高的内存和计算成本，当对LLMs进行策略训练时，这种成本更为严重。此外，最大的LLMs并未提供必要的API以便以这种方式进行训练。因此，现代提高LLMs推理能力的方法依赖于复杂的提示机制而非RL微调。为解决这一问题，我们提出了一种新方法，利用目标条件的价值函数来引导LLM代理的推理，即使对于基于API的大规模模型也能扩展。这些价值函数预测给定动作后任务将如何展开，使LLM代理能够评估多种可能的结果，无论是积极的还是消极的，从而有效规划。此外，这些价值函数是基于推理步骤而非完整动作进行训练的，成为一个简洁轻量的模块，促进多轮交互中的决策制定。我们通过谈判、社会推理和对话等交互要求的任务验证了该方法，结果显示该方法在效率和可扩展性方面优于RL微调和提示方法，同时展现出优越的性能。', 'title_zh': '无需搜索的规划：基于离线目标条件 reinforcement 学习精炼前沿大语言模型'}
{'arxiv_id': 'arXiv:2505.18091', 'title': 'Data Mixing Can Induce Phase Transitions in Knowledge Acquisition', 'authors': 'Xinran Gu, Kaifeng Lyu, Jiazheng Li, Jingzhao Zhang', 'link': 'https://arxiv.org/abs/2505.18091', 'abstract': 'Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge. In this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets, unlike training exclusively on knowledge-dense data (arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa.', 'abstract_zh': '大型语言模型（LLMs）通常在数据混合集上训练：大部分数据来自网页抓取，而一小部分来自高质量的、富含领域特定知识的数据源。本文表明，在使用此类数据混合集训练LLMs时，知识获取从富含知识的数据集中抽取（不同于仅使用富含知识的数据集训练，arXiv:2404.05405），其并不是总是遵循平滑的扩展定律，而是可能在混合比例和模型大小方面表现出相变现象。通过在合成的个人传记数据集与网页抓取数据混合上进行受控实验，我们展示：（1）随着模型大小增加到临界值，模型突然从记忆少量传记转变为记忆大多数传记；（2）在临界混合比例以下，即使经过大量训练，模型几乎不记忆任何传记，但超过这个阈值后，它会迅速记忆更多的传记。我们将这些相变归因于容量分配现象：具有容量上限的模型必须像背包问题求解器一样运作，以最小化整体测试损失，不同模型大小或混合比例下，最佳的数据集间分配可能会出现不连续变化。我们从信息论框架中正式化了这一直觉，并揭示这些相变是可预测的，临界混合比例与模型大小之间存在幂律关系。我们的发现强调了一个具体案例，即对大型模型有效的数据混合方法可能对小型模型来说不是最优的，反之亦然。', 'title_zh': '数据混杂可以诱导知识获取中的相变'}
{'arxiv_id': 'arXiv:2505.18071', 'title': 'Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals', 'authors': 'Jia-Nan Li, Jian Guan, Wei Wu, Rui Yan', 'link': 'https://arxiv.org/abs/2505.18071', 'abstract': "Large language models (LLMs) have demonstrated significant success in complex reasoning tasks such as math and coding. In contrast to these tasks where deductive reasoning predominates, inductive reasoning\\textemdash the ability to derive general rules from incomplete evidence, remains underexplored. This paper investigates extended inductive reasoning in LLMs through the lens of personalized preference inference, a critical challenge in LLM alignment where current approaches struggle to capture diverse user preferences. The task demands strong inductive reasoning capabilities as user preferences are typically embedded implicitly across various interaction forms, requiring models to synthesize consistent preference patterns from scattered signals. We propose \\textsc{AlignXplore}, a model that leverages extended reasoning chains to enable systematic preference inference from behavioral signals in users' interaction histories. We develop \\textsc{AlignXplore} by combining cold-start training based on synthetic data with subsequent online reinforcement learning. Through extensive experiments, we demonstrate that \\textsc{AlignXplore} achieves substantial improvements over the backbone model by an average of 11.05\\% on in-domain and out-of-domain benchmarks, while maintaining strong generalization ability across different input formats and downstream models. Further analyses establish best practices for preference inference learning through systematic comparison of reward modeling strategies, while revealing the emergence of human-like inductive reasoning patterns during training.", 'abstract_zh': '大型语言模型在数学和编码等复杂推理任务中取得了显著成功。相比之下，归纳推理——从不完整证据中推导出一般规则的能力——在现有研究中仍被忽视。本文通过个人偏好推断的视角探讨了大型语言模型中的扩展归纳推理，这是大型语言模型对齐中的一个关键挑战，当前方法难以捕捉多样化的用户偏好。该任务需要强大的归纳推理能力，因为用户偏好通常以隐式方式嵌入在各种交互形式中，要求模型从分散的信号中综合出一致的偏好模式。我们提出了一种名为\\textsc{AlignXplore}的模型，该模型利用扩展的推理链从用户的交互历史中的行为信号中系统地推断偏好。我们通过结合基于合成数据的冷启动训练和后续的在线强化学习来开发\\textsc{AlignXplore}。通过广泛的实验，我们证明了\\textsc{AlignXplore}在领域内和跨领域的基准测试中平均提高了11.05%，并保持了在不同输入格式和下游模型中的强烈泛化能力。进一步的分析通过系统比较奖励模型策略，确定了偏好推断学习的最佳实践，并揭示了在训练过程中出现的人类似推断模式。', 'title_zh': '个性化偏好推断中的扩展归纳推理'}
{'arxiv_id': 'arXiv:2505.18019', 'title': 'LLM assisted web application functional requirements generation: A case study of four popular LLMs over a Mess Management System', 'authors': 'Rashmi Gupta, Aditya K Gupta, Aarav Jain, Avinash C Pandey, Atul Gupta', 'link': 'https://arxiv.org/abs/2505.18019', 'abstract': 'Like any other discipline, Large Language Models (LLMs) have significantly impacted software engineering by helping developers generate the required artifacts across various phases of software development. This paper presents a case study comparing the performance of popular LLMs GPT, Claude, Gemini, and DeepSeek in generating functional specifications that include use cases, business rules, and collaborative workflows for a web application, the Mess Management System. The study evaluated the quality of LLM generated use cases, business rules, and collaborative workflows in terms of their syntactic and semantic correctness, consistency, non ambiguity, and completeness compared to the reference specifications against the zero-shot prompted problem statement. Our results suggested that all four LLMs can specify syntactically and semantically correct, mostly non-ambiguous artifacts. Still, they may be inconsistent at times and may differ significantly in the completeness of the generated specification. Claude and Gemini generated all the reference use cases, with Claude achieving the most complete but somewhat redundant use case specifications. Similar results were obtained for specifying workflows. However, all four LLMs struggled to generate relevant Business Rules, with DeepSeek generating the most reference rules but with less completeness. Overall, Claude generated more complete specification artifacts, while Gemini was more precise in the specifications it generated.', 'abstract_zh': '大型语言模型（LLMs）对软件工程的影响及其在生成WEB应用“餐饮管理系统”功能规格中的表现：GPT、Claude、Gemini和DeepSeek的比较研究', 'title_zh': '基于大型语言模型的Web应用功能需求生成：对一个后勤管理系统而言的四种流行大型语言模型案例研究'}
{'arxiv_id': 'arXiv:2505.18011', 'title': 'Training with Pseudo-Code for Instruction Following', 'authors': 'Prince Kumar, Rudra Murthy, Riyaz Bhat, Danish Contractor', 'link': 'https://arxiv.org/abs/2505.18011', 'abstract': 'Despite the rapid progress in the capabilities of Large Language Models (LLMs), they continue to have difficulty following relatively simple, unambiguous instructions, especially when compositions are involved. In this paper, we take inspiration from recent work that suggests that models may follow instructions better when they are expressed in pseudo-code. However, writing pseudo-code programs can be tedious and using few-shot demonstrations to craft code representations for use in inference can be unnatural for non-expert users of LLMs. To overcome these limitations, we propose fine-tuning LLMs with instruction-tuning data that additionally includes instructions re-expressed in pseudo-code along with the final response. We evaluate models trained using our method on $11$ publicly available benchmarks comprising of tasks related to instruction-following, mathematics, and common-sense reasoning. We conduct rigorous experiments with $5$ different models and find that not only do models follow instructions better when trained with pseudo-code, they also retain their capabilities on the other tasks related to mathematical and common sense reasoning. Specifically, we observe a relative gain of $3$--$19$% on instruction-following benchmark, and an average gain of upto 14% across all tasks.', 'abstract_zh': '尽管大规模语言模型（LLMs）的能力迅速进步，它们仍然难以遵循相对简单的明确指令，尤其是在涉及复杂组合时。本文受到近期研究表明的启发，即当指令用伪代码表达时，模型可能能更好地遵循指令。然而，编写伪代码程序可能会非常繁琐，而使用少量示例演示来为LLM用户生成代码表示以供推理则对非专家用户来说并不自然。为克服这些局限，我们提出了一种微调方法，该方法利用包含用伪代码重述的指令和最终响应的数据。我们使用该方法训练模型并在包含指令遵循、数学和常识推理任务的11个公开基准上评估这些模型。我们进行了严格的实验，使用5种不同模型发现，不仅在使用伪代码训练时模型在指令遵循任务中表现更好，而且它们在数学和常识推理相关的其他任务上也保持了其能力。具体而言，在指令遵循基准测试中我们观察到相对增益为3%至19%，在所有任务上的平均增益达到14%。', 'title_zh': '使用伪代码进行指令遵循训练'}
{'arxiv_id': 'arXiv:2505.17988', 'title': 'Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning', 'authors': 'Yutong Chen, Jiandong Gao, Ji Wu', 'link': 'https://arxiv.org/abs/2505.17988', 'abstract': "R1-style Reinforcement Learning (RL) significantly enhances Large Language Models' reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: this https URL", 'abstract_zh': 'R1风格强化学习显著增强了大型语言模型的推理能力，但基于规则的强化学习机制尚不明确。我们发现小规模SFT对强化学习有显著影响，但效率较低。为了解释我们的观察结果，我们提出了一种分析框架，并通过测量样本效果来比较SFT和强化学习的效率。假设分析表明，SFT的效率受限于训练数据。受分析指导，我们提出了一种技术，即重蒸馏，通过从强化学习训练策略的小规模蒸馏微调预训练模型。我们在Knight & Knave和MATH数据集上的实验表明，重蒸馏具有令人惊讶的效率：重蒸馏后的模型在远少于样本和更少计算的情况下达到了与强化学习相当的性能。经验验证表明，样本效果是性能改进的良好指标。因此，在K&K数据集上，我们的重蒸馏Qwen2.5-1.5B模型仅使用1K SFT样本就超越了DeepSeek-V3-0324。在MATH上，经过重蒸馏500样本微调的Qwen2.5-1.5B与没有使用强化学习的指令微调变体性能相当。我们的工作解释了R1风格强化学习中的几种有趣现象，揭示了其实际成功背后的机制。代码可在以下链接获取：this https URL。', 'title_zh': '面向R1风格强化学习中小规模微调效果的揭示'}
{'arxiv_id': 'arXiv:2505.17974', 'title': 'Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models', 'authors': 'Viktoriia Chekalina, Daniil Moskovskiy, Daria Cherniuk, Maxim Kurkin, Andrey Kuznetsov, Evgeny Frolov', 'link': 'https://arxiv.org/abs/2505.17974', 'abstract': 'The Fisher information is a fundamental concept for characterizing the sensitivity of parameters in neural networks. However, leveraging the full observed Fisher information is too expensive for large models, so most methods rely on simple diagonal approximations. While efficient, this approach ignores parameter correlations, often resulting in reduced performance on downstream tasks. In this work, we mitigate these limitations and propose Generalized Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that accounts for both diagonal and off-diagonal elements of the Fisher information matrix, providing a more accurate reflection of parameter importance. To make the method tractable, we introduce a scalable adaptation of the Kronecker-factored approximation algorithm for the observed Fisher information. We demonstrate the effectiveness of our method on LLM compression, showing improvements over existing compression baselines. For example, at a 20 compression rate on the MMLU benchmark, our method outperforms FWSVD, which is based on a diagonal approximation of the Fisher information, by 5 percent, SVD-LLM by 3 percent, and ASVD by 6 percent compression rate.', 'abstract_zh': '广义费雪加权SVD在大规模语言模型压缩中的应用：考虑费雪信息矩阵的对角线和非对角线元素', 'title_zh': '广义Fisher加权SVD：压缩大型语言模型的可扩展Kronecker因子Fisher近似方法'}
{'arxiv_id': 'arXiv:2505.17968', 'title': 'Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems', 'authors': 'Jiayi Geng, Howard Chen, Dilip Arumugam, Thomas L. Griffiths', 'link': 'https://arxiv.org/abs/2505.17968', 'abstract': 'Using AI to create autonomous researchers has the potential to accelerate scientific discovery. A prerequisite for this vision is understanding how well an AI model can identify the underlying structure of a black-box system from its behavior. In this paper, we explore how well a large language model (LLM) learns to identify a black-box function from passively observed versus actively collected data. We investigate the reverse-engineering capabilities of LLMs across three distinct types of black-box systems, each chosen to represent different problem domains where future autonomous AI researchers may have considerable impact: Program, Formal Language, and Math Equation. Through extensive experiments, we show that LLMs fail to extract information from observations, reaching a performance plateau that falls short of the ideal of Bayesian inference. However, we demonstrate that prompting LLMs to not only observe but also intervene -- actively querying the black-box with specific inputs to observe the resulting output -- improves performance by allowing LLMs to test edge cases and refine their beliefs. By providing the intervention data from one LLM to another, we show that this improvement is partly a result of engaging in the process of generating effective interventions, paralleling results in the literature on human learning. Further analysis reveals that engaging in intervention can help LLMs escape from two common failure modes: overcomplication, where the LLM falsely assumes prior knowledge about the black-box, and overlooking, where the LLM fails to incorporate observations. These insights provide practical guidance for helping LLMs more effectively reverse-engineer black-box systems, supporting their use in making new discoveries.', 'abstract_zh': '使用AI创建自主研究人员有望加速科学发现。本研究的前提是理解AI模型如何从行为中识别黑盒系统的潜在结构。本文探讨大型语言模型（LLM）如何从被动观察的数据与主动收集的数据中学习识别黑盒函数。我们研究了LLM在三种不同类型的黑盒系统中的逆向工程能力，这三种系统分别代表未来自主AI研究人员可能产生重大影响的不同问题领域：程序、形式语言和数学方程。通过大量实验，我们展示了LLM从观察中提取信息的能力有限，性能达到一个 plateau，低于贝叶斯推断的理想水平。然而，我们证明通过促使LLM不仅观察还干预——即用特定输入主动查询黑盒并观察结果——可以提高性能，这使LLM能够测试边界情况并完善其信念。通过向另一个LLM提供干预数据，我们展示了这种改进部分是通过生成有效干预的过程实现的，与人类学习文献中的结果相呼应。进一步分析揭示了干预可以帮助LLM避免两种常见失败模式：过度复杂化和忽视，前者是LLM错误地假设了对黑盒的先验知识，后者是LLM未能整合观察。这些见解为帮助LLM更有效地逆向工程黑盒系统提供了实用指导，支持它们在新发现中的应用。', 'title_zh': '大型语言模型是可靠的AI科学家吗？评估黑盒系统的逆向工程能力'}
{'arxiv_id': 'arXiv:2505.17967', 'title': 'SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models', 'authors': 'Ionut-Vlad Modoranu, Mher Safaryan, Erik Schultheis, Dan Alistarh', 'link': 'https://arxiv.org/abs/2505.17967', 'abstract': 'Low-rank optimization has emerged as a promising direction in training large language models (LLMs) to reduce the memory usage of adaptive optimizers by constraining learning to a lower-dimensional space. Prior work typically projects gradients of linear layers using approaches based on Singular Value Decomposition (SVD). However, applying SVD-based procedures individually to each layer in large models is computationally expensive and incurs additional memory costs due to storing the projection matrices. In this work, we propose a computationally efficient and conceptually simple two-step procedure to approximate SVD-based gradient projections into lower-dimensional spaces. First, we construct a complete orthogonal basis using predefined orthogonal matrices of the Discrete Cosine Transform (DCT). Second, we adaptively select basis columns based on their alignment with the gradient of each layer. Each projection matrix in our method is obtained via a single matrix multiplication followed by a lightweight sorting step to identify the most relevant basis vectors. Due to the predefined nature of the orthogonal bases, they are computed once at the start of training. During training, we store only the indices of the selected columns, avoiding the need to store full projection matrices for each layer. Our numerical experiments on both pre-training and fine-tuning tasks demonstrate the effectiveness of our dual strategy in approximating optimal low-rank projections, matching the performance of costly SVD-based methods while achieving faster runtime and reduced memory usage.', 'abstract_zh': '低秩优化已成为训练大规模语言模型（LLMs）的一个有前途的方向，通过将学习限制在低维空间中来减少自适应优化器的内存使用。先前的工作通常使用奇异值分解（SVD）基于的方法将线性层的梯度投影到低维空间。然而，对大型模型中的每一层单独应用基于SVD的程序在计算上昂贵，并且由于需要存储投影矩阵而产生额外的内存成本。在本项工作中，我们提出了一种计算高效且概念简单的两步方法来近似基于SVD的梯度投影到低维空间。首先，我们使用离散余弦变换（DCT）的预定义正交矩阵构建完整的正交基。其次，我们根据每个层的梯度与正交基列的对齐情况适当地选择基列。在我们的方法中，每个投影矩阵通过单一的矩阵乘法并随后通过轻量级的排序步骤来识别最相关的基向量获得。由于正交基的预定义性质，它们仅在训练开始时计算一次。在训练过程中，我们仅存储所选列的索引，从而避免为每一层存储完整的投影矩阵。我们的数值实验表明，在预训练和微调任务中，我们的双策略在近似最优低秩投影方面有效，能够匹配昂贵的SVD基方法的性能，同时实现更快的运行时间和减少的内存使用。', 'title_zh': '无需SVD的低秩自适应梯度优化方法用于大规模语言模型'}
{'arxiv_id': 'arXiv:2505.17952', 'title': 'Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL', 'authors': 'Che Liu, Haozhe Wang, Jiazhen Pan, Zhongwei Wan, Yong Dai, Fangzhen Lin, Wenjia Bai, Daniel Rueckert, Rossella Arcucci', 'link': 'https://arxiv.org/abs/2505.17952', 'abstract': 'Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the first medical LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves state-of-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct a comprehensive data-centric analysis guided by three questions: (i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is a key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks.', 'abstract_zh': '提高大型语言模型在复杂任务上的性能并使其在临床应用中实现可解释的决策制定需要有效的推理能力。然而，在没有通过对昂贵的链式思考（CoT）数据进行监督微调（SFT）的情况下，这仍然是一个挑战，这些链式思考数据多来源于封闭源模型（例如GPT-4o）。在这项工作中，我们介绍了AlphaMed，这是第一个通过强化学习（RL）纯粹涌现推理能力的医疗LLM，使用简化的基于规则的奖励在公共多选题QA数据集上，而不依赖于SFT或提炼的CoT数据。AlphaMed在六项医疗QA基准测试中达到了最先进的结果，超越了使用常规SFT+RL管道训练的模型。在具有挑战性的基准测试（如MedXpert）上，AlphaMed甚至超越了更大或封闭源模型，如DeepSeek-V3-671B和Claude-3.5-Sonnet。为了理解这种成功背后的因素，我们根据以下三个问题进行了全面的数据导向分析：（i）简化的基于规则的RL能否在没有提炼的CoT监督的情况下激励推理？（ii）数据集的数量和多样性如何影响推理？（iii）问题难度如何塑造推理的涌现和泛化？我们的研究发现，数据集的信息性是推理性能的关键驱动因素，简化的基于规则的RL在信息性的、多选题QA数据上有效诱导了无需CoT监督的推理。我们还观察到基准测试之间存在分歧的趋势，突显了当前评估中的局限性，并强调了需要更多具有挑战性和推理导向的医疗QA基准测试的必要性。', 'title_zh': '超越蒸馏：以 minimalist 规则为基础的 RL 推动医疗 LLM 推理的极限'}
{'arxiv_id': 'arXiv:2505.17894', 'title': 'Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model', 'authors': 'Khalil Hennara, Muhammad Hreden, Mohamed Motaism Hamed, Zeina Aldallal, Sara Chrouf, Safwan AlModhayan', 'link': 'https://arxiv.org/abs/2505.17894', 'abstract': 'We introduce Mutarjim, a compact yet powerful language model for bidirectional Arabic-English translation. While large-scale LLMs have shown impressive progress in natural language processing tasks, including machine translation, smaller models. Leveraging this insight, we developed Mutarjim based on Kuwain-1.5B , a language model tailored for both Arabic and English. Despite its modest size, Mutarjim outperforms much larger models on several established benchmarks, achieved through an optimized two-phase training approach and a carefully curated, high-quality training corpus.. Experimental results show that Mutarjim rivals models up to 20 times larger while significantly reducing computational costs and training requirements. We also introduce Tarjama-25, a new benchmark designed to overcome limitations in existing Arabic-English benchmarking datasets, such as domain narrowness, short sentence lengths, and English-source bias. Tarjama-25 comprises 5,000 expert-reviewed sentence pairs and spans a wide range of domains, offering a more comprehensive and balanced evaluation framework. Notably, Mutarjim achieves state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing even significantly larger and proprietary models like GPT-4o mini. We publicly release Tarjama-25 to support future research and advance the evaluation of Arabic-English translation systems.', 'abstract_zh': 'Mutarjim：一种紧凑而强大的双向阿拉伯语-英语语言模型', 'title_zh': 'Mutarjim: 用小型语言模型促进双向阿拉伯语-英语翻译'}
{'arxiv_id': 'arXiv:2505.17859', 'title': 'Scalable Valuation of Human Feedback through Provably Robust Model Alignment', 'authors': 'Masahiro Fujisawa, Masaki Adachi, Michael A. Osborne', 'link': 'https://arxiv.org/abs/2505.17859', 'abstract': 'Despite the importance of aligning language models with human preferences, crowd-sourced human feedback is often noisy -- for example, preferring less desirable responses -- posing a fundamental challenge to alignment. A truly robust alignment objective should yield identical model parameters even under severe label noise, a property known as redescending. We prove that no existing alignment methods satisfy this property. To address this, we propose Hölder-DPO, the first principled alignment loss with a provable redescending property, enabling estimation of the clean data distribution from noisy feedback. The aligned model estimates the likelihood of clean data, providing a theoretically grounded metric for dataset valuation that identifies the location and fraction of mislabels. This metric is gradient-free, enabling scalable and automated human feedback valuation without costly manual verification or clean validation dataset. Hölder-DPO achieves state-of-the-art robust alignment performance while accurately detecting mislabels in controlled datasets. Finally, we apply Hölder-DPO to widely used alignment datasets, revealing substantial noise levels and demonstrating that removing these mislabels significantly improves alignment performance across methods.', 'abstract_zh': '尽管将语言模型与人类偏好对齐至关重要，但众包的人类反馈往往是嘈杂的——例如，偏好不太理想的回答——这为对齐带来了根本性的挑战。一个真正 robust 的对齐目标应该在严重标签噪声下仍能产生相同的模型参数，这一特性被称为 redescending。我们证明了现有所有对齐方法都不具备这一特性。为了解决这个问题，我们提出了 Hölder-DPO，这是首个具有可证明 redescending 属性的原理性对齐损失，能够从嘈杂反馈中估计清洁数据分布。对齐后的模型估算清洁数据的概率，提供了一个基于理论的用于数据集估值的度量标准，足以识别错误标签的位置和比例。该度量标准无需梯度信息，使大规模和自动的人类反馈估值成为可能，无需昂贵的手动验证或清洁验证数据集。Hölder-DPO 在稳健对齐性能上达到了最佳效果，同时准确检测受控数据集中的错误标签。最后，我们将 Hölder-DPO 应用于广泛使用的对齐数据集，揭示了显著的噪声水平，并证明移除这些错误标签显著改善了各种方法的对齐性能。', 'title_zh': '通过可证明鲁棒的模型对齐规模化评估人类反馈'}
{'arxiv_id': 'arXiv:2505.17813', 'title': "Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning", 'authors': 'Michael Hassid, Gabriel Synnaeve, Yossi Adi, Roy Schwartz', 'link': 'https://arxiv.org/abs/2505.17813', 'abstract': 'Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge the assumption that long thinking chains results in better reasoning capabilities. We first demonstrate that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, we suggest short-m@k, a novel reasoning LLM inference method. Our method executes k independent generations in parallel and halts computation once the first m thinking processes are done. The final answer is chosen using majority voting among these m chains. Basic short-1@k demonstrates similar or even superior performance over standard majority voting in low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while slightly less efficient than short-1@k, consistently surpasses majority voting across all compute budgets, while still being substantially faster (up to 33% wall time reduction). Inspired by our results, we finetune an LLM using short, long, and randomly selected reasoning chains. We then observe that training on the shorter ones leads to better performance. Our findings suggest rethinking current methods of test-time compute in reasoning LLMs, emphasizing that longer "thinking" does not necessarily translate to improved performance and can, counter-intuitively, lead to degraded results.', 'abstract_zh': '大规模语言模型（LLMs）的推理 heavily 依赖于扩展测试时计算资源以生成大量的“思考”链来完成复杂的推理任务。虽然这种方法展现了令人印象深刻的成果，但这也带来了显著的计算成本和推断时间。在本工作中，我们挑战了长“思考”链会带来更好推理能力这一假设。我们首先展示了在单一问题内较短的“思考”链显著更有可能产生正确答案——最高可达34.5%的准确性高于同问题中最长链采样的准确性。基于这些结果，我们提出了一种新颖的推理LLM推理方法——short-m@k。该方法并行执行k个独立生成，并在第一个m个“思考”过程完成后停止计算。最终答案通过这m个链中的多数投票来选择。short-1@k的基本方法在低计算设置中显示出了与标准多数投票相似甚至更优的性能——使用多达40%更少的“思考”令牌。虽然short-3@k略显效率较低，但在所有计算预算下始终超越多数投票，并且仍然显著更快（最高减少33%的wall time）。受我们成果的启发，我们针对较短、较长以及随机选择的“思考”链对LLM进行微调。结果观察到，使用较短的“思考”链进行训练会导致更好的性能。我们的发现表明，需要重新审视当前推理LLM测试时计算的方法，强调更长的“思考”并不一定意味着更好性能，反而可能会导致意想不到的性能下降。', 'title_zh': '不要过度思考。选择更短的思考链以提高LLM推理能力'}
{'arxiv_id': 'arXiv:2505.17795', 'title': 'DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors', 'authors': 'Tazeek Bin Abdur Rakib, Ambuj Mehrish, Lay-Ki Soon, Wern Han Lim, Soujanya Poria', 'link': 'https://arxiv.org/abs/2505.17795', 'abstract': "Large-language-model (LLM) agents excel at reactive dialogue but struggle with proactive, goal-driven interactions due to myopic decoding and costly planning. We introduce DialogXpert, which leverages a frozen LLM to propose a small, high-quality set of candidate actions per turn and employs a compact Q-network over fixed BERT embeddings trained via temporal-difference learning to select optimal moves within this reduced space. By tracking the user's emotions, DialogXpert tailors each decision to advance the task while nurturing a genuine, empathetic connection. Across negotiation, emotional support, and tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with success rates exceeding 94\\% and, with a larger LLM prior, pushes success above 97\\% while markedly improving negotiation outcomes. This framework delivers real-time, strategic, and emotionally intelligent dialogue planning at scale. Code available at this https URL", 'abstract_zh': '大型语言模型（LLM）代理在反应性对话方面表现出色，但在处理前瞻性的、以目标为导向的交互时却面临困难，这主要是由于短视的解码和高昂的规划成本。我们引入了DialogXpert，它利用一个冻结的LLM为每一轮提出一小套高质量的候选行动，并通过时差学习训练一个紧凑的Q网络，在固定的好奇BERT嵌入上选择最佳动作，从而在这一减小的空间内选择最优行动。通过跟踪用户的情绪，DialogXpert为推进任务并培养真实、同理心的连接量身定制每一个决策。在谈判、情感支持和辅导基准测试中，DialogXpert将对话推向不到3轮，成功率超过94%，使用更大的LLM先验时，成功率达到97%以上，同时显著改善了谈判结果。该框架实现了大规模的实时、战略性和情感智能对话规划。代码可在以下链接获取：this https URL。', 'title_zh': 'DialogXpert：通过基于在线价值强化学习的LLM先验驱动智能和情感意识对话'}
{'arxiv_id': 'arXiv:2505.17760', 'title': 'But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors', 'authors': 'Leon Eshuijs, Archie Chaudhury, Alan McBeth, Ethan Nguyen', 'link': 'https://arxiv.org/abs/2505.17760', 'abstract': 'Recent safety evaluations of Large Language Models (LLMs) show that many models exhibit dishonest behavior, such as sycophancy. However, most honesty benchmarks focus exclusively on factual knowledge or explicitly harmful behavior and rely on external judges, which are often unable to detect less obvious forms of dishonesty. In this work, we introduce a new framework, Judge Using Safety-Steered Alternatives (JUSSA), which utilizes steering vectors trained on a single sample to elicit more honest responses from models, helping LLM-judges in the detection of dishonest behavior. To test our framework, we introduce a new manipulation dataset with prompts specifically designed to elicit deceptive responses. We find that JUSSA enables LLM judges to better differentiate between dishonest and benign responses, and helps them identify subtle instances of manipulative behavior.', 'abstract_zh': 'Recent安全评估表明大规模语言模型（LLMs）表现出诚实度不足的行为，如拍马屁。然而，大多数诚实度基准主要集中在事实性知识或显性的危害行为上，并依赖外部评委，这些评委往往无法检测到不太明显的不诚实行为。在这项工作中，我们引入了一种新的框架，Safety-Steered Alternatives辅助的评委使用方案（JUSSA），该框架利用对单个样本进行训练的引导向量，促使模型产生更诚实的回应，帮助LLM评委检测不诚实行为。为了测试该框架，我们引入了一个新的操控数据集，其中的提示语旨在引发欺骗性回应。我们发现，JUSSA使LLM评委更好地区分不诚实和 benign 的回应，并帮助他们识别微妙的操控行为。', 'title_zh': '但你的诚实行为什么是？通过导向向量提供诚实行使的替代方案辅助LLM-法官'}
{'arxiv_id': 'arXiv:2505.17702', 'title': 'Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek', 'authors': 'Xueyang Li, Jiahao Li, Yu Song, Yunzhong Lou, Xiangdong Zhou', 'link': 'https://arxiv.org/abs/2505.17702', 'abstract': 'The advent of Computer-Aided Design (CAD) generative modeling will significantly transform the design of industrial products. The recent research endeavor has extended into the realm of Large Language Models (LLMs). In contrast to fine-tuning methods, training-free approaches typically utilize the advanced closed-source LLMs, thereby offering enhanced flexibility and efficiency in the development of AI agents for generating CAD parametric models. However, the substantial cost and limitations of local deployment of the top-tier closed-source LLMs pose challenges in practical applications. The Seek-CAD is the pioneer exploration of locally deployed open-source inference LLM DeepSeek-R1 for CAD parametric model generation with a training-free methodology. This study is the first investigation to incorporate both visual and Chain-of-Thought (CoT) feedback within the self-refinement mechanism for generating CAD models. Specifically, the initial generated parametric CAD model is rendered into a sequence of step-wise perspective images, which are subsequently processed by a Vision Language Model (VLM) alongside the corresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation. Then, the feedback is utilized by DeepSeek-R1 to refine the initial generated model for the next round of generation. Moreover, we present an innovative 3D CAD model dataset structured around the SSR (Sketch, Sketch-based feature, and Refinements) triple design paradigm. This dataset encompasses a wide range of CAD commands, thereby aligning effectively with industrial application requirements and proving suitable for the generation of LLMs. Extensive experiments validate the effectiveness of Seek-CAD under various metrics.', 'abstract_zh': '计算机辅助设计（CAD）生成建模的出现将显著变革工业产品设计。近期研究已拓展至大型语言模型（LLMs）领域。与微调方法不同，无需训练的方法通常利用先进的闭源LLMs，从而在开发生成CAD参数化模型的AI代理方面提供更高的灵活性和效率。然而，顶级闭源LLMs的高昂成本及其本地部署的限制在实际应用中构成挑战。Seek-CAD是局部部署的开源推理LLM DeepSeek-R1用于无需训练方法生成CAD参数化模型的先驱探索。该研究首次在自完善机制中结合视觉反馈和链式思考（CoT）反馈，以生成CAD模型。具体而言，初始生成的参数化CAD模型被渲染为一系列逐步视角图像，随后与DeepSeek-R1生成的相关CoTs一起由视觉语言模型（VLM）处理，以评估CAD模型生成情况。然后，反馈用于指导DeepSeek-R1精炼初始生成的模型，以进入下一轮生成。此外，我们介绍了基于SSR（草图、草图基础特征和精炼）三重设计范式的创新3D CAD模型数据集，该数据集涵盖了广泛的CAD命令，有效满足工业应用需求，并适用于生成LLMs。广泛实验验证了Seek-CAD在多种评估指标下的有效性。', 'title_zh': 'Seek-CAD：一种基于DeepSeek的局部推理自精炼生成 modeling方法用于3D参数化CAD'}
{'arxiv_id': 'arXiv:2505.17701', 'title': 'COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection', 'authors': 'Jaewon Cheon, Pilsung Kang', 'link': 'https://arxiv.org/abs/2505.17701', 'abstract': 'The growing size of large language models has created significant computational inefficiencies. To address this challenge, sparse activation methods selectively deactivates non-essential parameters during inference, reducing computational costs in FFNN layers. While existing methods focus on non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN layer lies globally in the form of a linear combination over its internal down projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN, leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct coefficients of the linear combination. Experimental results demonstrate that D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5% ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4% better performance preservation compared to existing methods. Our specialized kernel implementations effectively realize these theoretical gains into substantial real-world acceleration.', 'abstract_zh': '大型语言模型规模的不断扩大造成了显著的计算 inefficiency。为应对这一挑战，稀疏激活方法在推理过程中选择性地去激活非必要参数，从而减少全连接层（FFNN）的计算成本。尽管现有方法侧重于非线性门控机制，我们假设全连接层的稀疏性在全球范围内表现为线性组合形式，其内部下投影矩阵的直接系数。基于这一洞察，我们提出两种方法：M-COUNTDOWN，利用间接系数；D-COUNTDOWN，利用线性组合的直接系数。实验结果表明，D-COUNTDOWN可以在性能损失仅为5.5%的理想情况下省去90%的计算量，而M-COUNTDOWN提供了一个无需预测器的解决方案，其性能保留比现有方法高出至多29.4%。我们专门开发的内核实现有效将这些理论优势转化为实质性的实际加速。', 'title_zh': 'COUNTDOWN: 上下文稀疏激活过滤不必要的下行投影权重'}
{'arxiv_id': 'arXiv:2505.17682', 'title': 'Tuning Language Models for Robust Prediction of Diverse User Behaviors', 'authors': 'Fanjin Meng, Jingtao Ding, Jiahui Gong, Chen Yang, Hong Chen, Zuojian Wang, Haisheng Lu, Yong Li', 'link': 'https://arxiv.org/abs/2505.17682', 'abstract': "Predicting user behavior is essential for intelligent assistant services, yet deep learning models often struggle to capture long-tailed behaviors. Large language models (LLMs), with their pretraining on vast corpora containing rich behavioral knowledge, offer promise. However, existing fine-tuning approaches tend to overfit to frequent ``anchor'' behaviors, reducing their ability to predict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM, a progressive fine-tuning approach that addresses this issue. In the first stage, LLMs are fine-tuned on anchor behaviors while preserving general behavioral knowledge. In the second stage, fine-tuning uses a balanced subset of all behaviors based on sample difficulty to improve tail behavior predictions without sacrificing anchor performance. Experimental results on two real-world datasets demonstrate that BehaviorLM robustly predicts both anchor and tail behaviors and effectively leverages LLM behavioral knowledge to master tail behavior prediction with few-shot examples.", 'abstract_zh': '行为预测对于智能助理服务至关重要，然而深度学习模型往往难以捕捉长尾行为。预训练于丰富行为知识大规模语料中的大规模语言模型（LLMs）提供了潜力。然而，现有的微调方法倾向于过度拟合于常见的“锚”行为，降低了预测少见的“尾”行为的能力。本文提出了一种逐步微调方法——BehaviorLM，以解决这一问题。在第一阶段，LLMs在保留一般行为知识的前提下对常见行为进行微调。在第二阶段，微调采用基于样本难度的平衡子集，以提高尾行为预测能力而不牺牲锚行为性能。实验结果表明，BehaviorLM在两个真实世界数据集上稳健地预测了锚行为和尾行为，并且能够有效地利用LLM的行为知识通过少量示例掌握尾行为预测。', 'title_zh': '调整语言模型以稳健预测多样化用户行为'}
{'arxiv_id': 'arXiv:2505.17670', 'title': 'Towards General Continuous Memory for Vision-Language Models', 'authors': 'Wenyi Wu, Zixuan Song, Kun Zhou, Yifei Shao, Zhiting Hu, Biwei Huang', 'link': 'https://arxiv.org/abs/2505.17670', 'abstract': "Language models (LMs) and their extension, vision-language models (VLMs), have achieved remarkable performance across various tasks. However, they still struggle with complex reasoning tasks that require multimodal or multilingual real-world knowledge. To support such capabilities, an external memory system that can efficiently provide relevant multimodal information is essential. Existing approaches generally concatenate image and text tokens into a long sequence as memory, which, however, may drastically increase context length and even degrade performance. In contrast, we propose using continuous memory, a compact set of dense embeddings to more effectively and efficiently represent multimodal and multilingual knowledge. Our key insight is that a VLM can serve as its own continuous memory encoder. We empirically show that this design improves performance on complex multimodal reasoning tasks. Building on this, we introduce a data-efficient and parameter-efficient method to fine-tune the VLM into a memory encoder, requiring only 1.2% of the model's parameters and a small corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes VLM's original capabilities to encode arbitrary multimodal and multilingual knowledge into just 8 continuous embeddings. Since the inference-time VLM remains frozen, our memory module is plug-and-play and can be flexibly integrated as needed. Extensive experiments across eight multimodal reasoning benchmarks demonstrate the effectiveness of our approach.", 'abstract_zh': '基于语言模型（LMs）及其扩展视觉-语言模型（VLMs）在各种任务中取得了显著性能，但它们仍然难以处理需要多模态或多语言现实世界知识的复杂推理任务。为了支持这些能力，需要一个高效提供相关多模态信息的外部记忆系统。现有方法通常将图像和文本标记连接成一个长序列作为记忆，但这可能会大幅增加上下文长度并甚至恶化性能。相比之下，我们提出使用连续记忆，即一组密集的嵌入表示多模态和多语言知识。我们的核心见解是，VLM可以作为其自身的连续记忆编码器。实验证明，这种设计在复杂多模态推理任务上能提高性能。在这一基础上，我们引入了一种数据高效和参数高效的微调方法，将VLM微调成一个记忆编码器，仅需模型参数的1.2%和15.6K自合成样本的小规模语料库。我们的方法CoMEM利用VLM的原有能力，仅将任意多模态和多语言知识编码为8个连续嵌入。由于推理时的VLM保持冻结，我们的记忆模块插即用，可以根据需要灵活集成。广泛的实验跨越八个多模态推理基准，证明了我们方法的有效性。', 'title_zh': '面向视觉-语言模型的通用连续记忆'}
{'arxiv_id': 'arXiv:2505.17654', 'title': 'EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications', 'authors': 'Ancheng Xu, Zhihao Yang, Jingpeng Li, Guanghu Yuan, Longze Chen, Liang Yan, Jiehui Zhou, Zhen Qin, Hengyun Chang, Hamid Alinejad-Rokny, Bo Zheng, Min Yang', 'link': 'https://arxiv.org/abs/2505.17654', 'abstract': 'E-commerce platforms increasingly rely on Large Language Models (LLMs) and Vision-Language Models (VLMs) to detect illicit or misleading product content. However, these models remain vulnerable to evasive content: inputs (text or images) that superficially comply with platform policies while covertly conveying prohibited claims. Unlike traditional adversarial attacks that induce overt failures, evasive content exploits ambiguity and context, making it far harder to detect. Existing robustness benchmarks provide little guidance for this demanding, real-world challenge. We introduce EVADE, the first expert-curated, Chinese, multimodal benchmark specifically designed to evaluate foundation models on evasive content detection in e-commerce. The dataset contains 2,833 annotated text samples and 13,961 images spanning six demanding product categories, including body shaping, height growth, and health supplements. Two complementary tasks assess distinct capabilities: Single-Violation, which probes fine-grained reasoning under short prompts, and All-in-One, which tests long-context reasoning by merging overlapping policy rules into unified instructions. Notably, the All-in-One setting significantly narrows the performance gap between partial and full-match accuracy, suggesting that clearer rule definitions improve alignment between human and model judgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial performance gaps: even state-of-the-art models frequently misclassify evasive samples. By releasing EVADE and strong baselines, we provide the first rigorous standard for evaluating evasive-content detection, expose fundamental limitations in current multimodal reasoning, and lay the groundwork for safer and more transparent content moderation systems in e-commerce. The dataset is publicly available at this https URL.', 'abstract_zh': '电子商务平台越来越多地依赖大型语言模型（LLMs）和多模态视觉-语言模型（VLMs）来检测非法或误导性产品内容。然而，这些模型仍然容易受到规避内容的攻击：看似符合平台政策但实际上隐含违规声明的输入（文本或图像）。与传统的对抗性攻击导致明显的失效不同，规避内容利用了模糊性和上下文，使其更难被检测到。现有鲁棒性基准对此提出的现实挑战提供了很少的指导。我们提出了EVADE，这是首个专为评估基础模型在电子商务中的规避内容检测能力而精心设计的汉语多模态基准。该数据集包含2,833个标注的文本样本和13,961张图片，覆盖六个具有挑战性的产品类别，包括体型管理、身高增长和健康补充剂。两个互补的任务评估不同的能力：单违反（Single-Violation），探索在简短提示下进行细微推理的能力；全能（All-in-One），通过合并重叠的政策规则来测试长上下文推理。值得注意的是，全能环境显著缩小了部分匹配和全匹配准确性之间的性能差距，表明更清晰的规则定义可以改善人类和模型判断之间的契合度。我们测试了26种主流LLM和VLM，并观察到显著的性能差距：即使是最先进的模型也经常错误分类规避样本。通过发布EVADE和强大的基线，我们提供了首个严格的规避内容检测评估标准，揭示了当前多模态推理的基本局限性，并为基础电子商务内容审核系统提供了更安全和透明的底层建设。数据集可从此网址公开访问。', 'title_zh': 'EVADE：电子商务应用中规避内容检测的多模态基准'}
{'arxiv_id': 'arXiv:2505.17652', 'title': 'Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective', 'authors': 'Deyang Kong, Qi Guo, Xiangyu Xi, Wei Wang, Jingang Wang, Xunliang Cai, Shikun Zhang, Wei Ye', 'link': 'https://arxiv.org/abs/2505.17652', 'abstract': "Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces \\textbf{C}ompetence-\\textbf{D}ifficulty \\textbf{A}lignment \\textbf{S}ampling (\\textbf{CDAS}), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is \\textbf{2.33} times slower than CDAS.", 'abstract_zh': 'Competence-Difficulty Alignment Sampling Improves Reasoning Abilities of Large Language Models', 'title_zh': '重塑强化学习中大规模语言模型推理的采样标准：一种能力-难度对齐视角'}
{'arxiv_id': 'arXiv:2505.17636', 'title': 'Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis', 'authors': 'Jonathan Bennion, Shaona Ghosh, Mantek Singh, Nouha Dziri', 'link': 'https://arxiv.org/abs/2505.17636', 'abstract': 'Various AI safety datasets have been developed to measure LLMs against evolving interpretations of harm. Our evaluation of five recently published open-source safety benchmarks reveals distinct semantic clusters using UMAP dimensionality reduction and kmeans clustering (silhouette score: 0.470). We identify six primary harm categories with varying benchmark representation. GretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix emphasizes self-harm scenarios. Significant differences in prompt length distribution suggests confounds to data collection and interpretations of harm as well as offer possible context. Our analysis quantifies benchmark orthogonality among AI benchmarks, allowing for transparency in coverage gaps despite topical similarities. Our quantitative framework for analyzing semantic orthogonality across safety benchmarks enables more targeted development of datasets that comprehensively address the evolving landscape of harms in AI use, however that is defined in the future.', 'abstract_zh': '各种AI安全数据集已被开发用于衡量LLMs针对 evolving interpretations of harm 的变化。我们对五个最近发布的开源安全基准的评估揭示了使用UMAP降维和kmeans聚类（轮廓分数：0.470）的不同语义簇。我们识别出六个主要的伤害类别，这些类别在基准中的表现形式各异。例如，GretelAI 高度关注隐私问题，而WildGuardMix则集中在自我伤害场景上。显著的提示长度分布差异表明了数据收集和伤害解释中的混杂因素，同时也提供了可能的上下文信息。我们的分析量化了AI基准之间的正交性，即使在主题相似的情况下，也允许透明地了解覆盖缺口。我们针对安全基准的语义正交性分析的量化框架能够促进更具针对性的数据集开发，以全面应对未来AI使用中演变的伤害环境。', 'title_zh': '跨模型安全性基准表面语义正交性：多维度分析'}
{'arxiv_id': 'arXiv:2505.17632', 'title': 'ReqBrain: Task-Specific Instruction Tuning of LLMs for AI-Assisted Requirements Generation', 'authors': 'Mohammad Kasra Habib, Daniel Graziotin, Stefan Wagner', 'link': 'https://arxiv.org/abs/2505.17632', 'abstract': "Requirements elicitation and specification remains a labor-intensive, manual process prone to inconsistencies and gaps, presenting a significant challenge in modern software engineering. Emerging studies underscore the potential of employing large language models (LLMs) for automated requirements generation to support requirements elicitation and specification; however, it remains unclear how to implement this effectively. In this work, we introduce ReqBrain, an Al-assisted tool that employs a fine-tuned LLM to generate authentic and adequate software requirements. Software engineers can engage with ReqBrain through chat-based sessions to automatically generate software requirements and categorize them by type. We curated a high-quality dataset of ISO 29148-compliant requirements and fine-tuned five 7B-parameter LLMs to determine the most effective base model for ReqBrain. The top-performing model, Zephyr-7b-beta, achieved 89.30\\% Fl using the BERT score and a FRUGAL score of 91.20 in generating authentic and adequate requirements. Human evaluations further confirmed ReqBrain's effectiveness in generating requirements. Our findings suggest that generative Al, when fine-tuned, has the potential to improve requirements elicitation and specification, paving the way for future extensions into areas such as defect identification, test case generation, and agile user story creation.", 'abstract_zh': '基于大语言模型的软件需求提取与规范工具ReqBrain', 'title_zh': 'ReqBrain: 面向AI辅助需求生成的LLM任务特定指令调优'}
{'arxiv_id': 'arXiv:2505.17631', 'title': 'BehaveGPT: A Foundation Model for Large-scale User Behavior Modeling', 'authors': 'Jiahui Gong, Jingtao Ding, Fanjin Meng, Chen Yang, Hong Chen, Zuojian Wang, Haisheng Lu, Yong Li', 'link': 'https://arxiv.org/abs/2505.17631', 'abstract': 'In recent years, foundational models have revolutionized the fields of language and vision, demonstrating remarkable abilities in understanding and generating complex data; however, similar advances in user behavior modeling have been limited, largely due to the complexity of behavioral data and the challenges involved in capturing intricate temporal and contextual relationships in user activities. To address this, we propose BehaveGPT, a foundational model designed specifically for large-scale user behavior prediction. Leveraging transformer-based architecture and a novel pretraining paradigm, BehaveGPT is trained on vast user behavior datasets, allowing it to learn complex behavior patterns and support a range of downstream tasks, including next behavior prediction, long-term generation, and cross-domain adaptation. Our approach introduces the DRO-based pretraining paradigm tailored for user behavior data, which improves model generalization and transferability by equitably modeling both head and tail behaviors. Extensive experiments on real-world datasets demonstrate that BehaveGPT outperforms state-of-the-art baselines, achieving more than a 10% improvement in macro and weighted recall, showcasing its ability to effectively capture and predict user behavior. Furthermore, we measure the scaling law in the user behavior domain for the first time on the Honor dataset, providing insights into how model performance scales with increased data and parameter sizes.', 'abstract_zh': '近年来，基础模型在语言和视觉领域实现了革命性的进步，展示了理解与生成复杂数据的非凡能力；然而，用户行为建模方面取得类似进展有限，主要是由于行为数据的复杂性以及捕捉用户活动中的精细时序和上下文关系的挑战。为了解决这个问题，我们提出BehaveGPT，一种专门设计用于大规模用户行为预测的基础模型。通过利用基于变换器的架构和新颖的预训练范式，BehaveGPT 在大量的用户行为数据集上训练，使其能够学习复杂的行为模式，并支持包括下一个行为预测、长期生成和跨域适应等一系列下游任务。我们的方法引入了一种针对用户行为数据定制的DRO基于的预训练范式，通过公平建模头部和尾部行为，提高了模型的泛化能力和迁移性。在真实世界数据集上的广泛实验表明，BehaveGPT 在宏召回率和加权召回率上均优于最先进的基线，显示出其有效捕获和预测用户行为的能力。此外，我们首次在荣耀数据集上测量了用户行为领域的规模法则，提供了关于随数据和参数规模增加模型性能如何变化的见解。', 'title_zh': 'BehaveGPT：大规模用户行为建模的基石模型'}
{'arxiv_id': 'arXiv:2505.17616', 'title': 'Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments', 'authors': 'Qingyu Lu, Liang Ding, Siyi Cao, Xuebo Liu, Kanjian Zhang, Jinxia Zhang, Dacheng Tao', 'link': 'https://arxiv.org/abs/2505.17616', 'abstract': "Agents powered by large language models (LLMs) have demonstrated strong planning and decision-making capabilities in complex embodied environments. However, such agents often suffer from inefficiencies in multi-turn interactions, frequently trapped in repetitive loops or issuing ineffective commands, leading to redundant computational overhead. Instead of relying solely on learning from trajectories, we take a first step toward exploring the early-exit behavior for LLM-based agents. We propose two complementary approaches: 1. an $\\textbf{intrinsic}$ method that injects exit instructions during generation, and 2. an $\\textbf{extrinsic}$ method that verifies task completion to determine when to halt an agent's trial. To evaluate early-exit mechanisms, we introduce two metrics: one measures the reduction of $\\textbf{redundant steps}$ as a positive effect, and the other evaluates $\\textbf{progress degradation}$ as a negative effect. Experiments with 4 different LLMs across 5 embodied environments show significant efficiency improvements, with only minor drops in agent performance. We also validate a practical strategy where a stronger agent assists after an early-exit agent, achieving better performance with the same total steps. We will release our code to support further research.", 'abstract_zh': '由大型语言模型驱动的代理在外域环境中的早期退出行为探究', 'title_zh': 'RUNAWAY 是羞愧的，但有益：大规模语言模型代理在体感环境中的早期退出行为探究'}
{'arxiv_id': 'arXiv:2505.17612', 'title': 'Distilling LLM Agent into Small Models with Retrieval and Code Tools', 'authors': 'Minki Kang, Jongwon Jeong, Seanie Lee, Jaewoong Cho, Sung Ju Hwang', 'link': 'https://arxiv.org/abs/2505.17612', 'abstract': 'Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at this https URL.', 'abstract_zh': '基于代理的代理蒸馏：将复杂推理和完整任务解决行为转移至小型语言模型', 'title_zh': '将LLM代理精简为带有检索和代码工具的小模型'}
{'arxiv_id': 'arXiv:2505.17589', 'title': 'CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training', 'authors': 'Zhihao Du, Changfeng Gao, Yuxuan Wang, Fan Yu, Tianyu Zhao, Hao Wang, Xiang Lv, Hui Wang, Xian Shi, Keyu An, Guanrou Yang, Yabin Li, Yanni Chen, Zhifu Gao, Qian Chen, Yue Gu, Mengzhe Chen, Yafeng Chen, Shiliang Zhang, Wen Wang, Jieping Ye', 'link': 'https://arxiv.org/abs/2505.17589', 'abstract': 'In our prior works, we introduced a scalable streaming speech synthesis model, CosyVoice 2, which integrates a large language model (LLM) and a chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming speech synthesis and human-parity quality. Despite these advancements, CosyVoice 2 exhibits limitations in language coverage, domain diversity, data volume, text formats, and post-training techniques. In this paper, we present CosyVoice 3, an improved model designed for zero-shot multilingual speech synthesis in the wild, surpassing its predecessor in content consistency, speaker similarity, and prosody naturalness. Key features of CosyVoice 3 include: 1) A novel speech tokenizer to improve prosody naturalness, developed via supervised multi-task training, including automatic speech recognition, speech emotion recognition, language identification, audio event detection, and speaker analysis. 2) A new differentiable reward model for post-training applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis models. 3) Dataset Size Scaling: Training data is expanded from ten thousand hours to one million hours, encompassing 9 languages and 18 Chinese dialects across various domains and text formats. 4) Model Size Scaling: Model parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced performance on our multilingual benchmark due to the larger model capacity. These advancements contribute significantly to the progress of speech synthesis in the wild. We encourage readers to listen to the demo at this https URL.', 'abstract_zh': 'CosyVoice 3：面向野外的零样本多语言语音合成改进模型', 'title_zh': 'CosyVoice 3：通过规模化扩增和后训练 towards户外语音生成'}
{'arxiv_id': 'arXiv:2505.17568', 'title': 'JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models', 'authors': 'Zifan Peng, Yule Liu, Zhen Sun, Mingchen Li, Zeren Luo, Jingyi Zheng, Wenhan Dong, Xinlei He, Xuechao Wang, Yingjie Xue, Shengmin Xu, Xinyi Huang', 'link': 'https://arxiv.org/abs/2505.17568', 'abstract': 'Audio Language Models (ALMs) have made significant progress recently. These models integrate the audio modality directly into the model, rather than converting speech into text and inputting text to Large Language Models (LLMs). While jailbreak attacks on LLMs have been extensively studied, the security of ALMs with audio modalities remains largely unexplored. Currently, there is a lack of an adversarial audio dataset and a unified framework specifically designed to evaluate and compare attacks and ALMs. In this paper, we present JALMBench, the \\textit{first} comprehensive benchmark to assess the safety of ALMs against jailbreak attacks. JALMBench includes a dataset containing 2,200 text samples and 51,381 audio samples with over 268 hours. It supports 12 mainstream ALMs, 4 text-transferred and 4 audio-originated attack methods, and 5 defense methods. Using JALMBench, we provide an in-depth analysis of attack efficiency, topic sensitivity, voice diversity, and attack representations. Additionally, we explore mitigation strategies for the attacks at both the prompt level and the response level.', 'abstract_zh': 'Audio Language Models (ALMs)在近年来取得了显著进展。这些模型直接将音频模态整合进模型中，而不是将语音转换成文本并输入到大型语言模型（LLMs）中。虽然对LLMs的jailbreak攻击已经被广泛研究，但具有音频模态的ALMs的安全性仍然很大程度上没有被探索。目前缺乏专门设计用于评估和比较攻击和ALMs的对抗音频数据集和统一框架。在本文中，我们提出了JALMBench，这是首个全面的基准测试，用于评估ALMs在jailbreak攻击下的安全性。JALMBench包含2,200个文本样本和51,381个音频样本，总时长达268小时，支持12种主流ALMs、4种文本转移和4种音频起源的攻击方法以及5种防御方法。使用JALMBench，我们对攻击效率、主题敏感性、语音多样性和攻击表示进行了深入分析。此外，我们还探讨了在提示级别和响应级别上减轻攻击的策略。', 'title_zh': 'JALMBench: 评估音频语言模型逃逸漏洞的基准测试'}
{'arxiv_id': 'arXiv:2505.17558', 'title': 'Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection', 'authors': 'Shrey Pandit, Ashwin Vinod, Liu Leqi, Ying Ding', 'link': 'https://arxiv.org/abs/2505.17558', 'abstract': 'Aligning large language models (LLMs) to accurately detect hallucinations remains a significant challenge due to the sophisticated nature of hallucinated text. Recognizing that hallucinated samples typically exhibit higher deceptive quality than traditional negative samples, we use these carefully engineered hallucinations as negative examples in the DPO alignment procedure. Our method incorporates a curriculum learning strategy, gradually transitioning the training from easier samples, identified based on the greatest reduction in probability scores from independent fact checking models, to progressively harder ones. This structured difficulty scaling ensures stable and incremental learning. Experimental evaluation demonstrates that our HaluCheck models, trained with curriculum DPO approach and high quality negative samples, significantly improves model performance across various metrics, achieving improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval. Additionally, HaluCheck models demonstrate robustness in zero-shot settings, significantly outperforming larger state-of-the-art models across various benchmarks.', 'abstract_zh': '将大语言模型（LLMs）对幻觉准确检测进行对齐仍是一项重大挑战，因为幻觉文本具有复杂性。鉴于幻觉样本通常比传统负样本具有更高的欺骗质量，我们使用这些精心设计的幻觉作为DPO对齐过程中的负例子。我们的方法结合了课程学习策略，逐步过渡从基于独立事实检查模型概率分数最大减少的 easier 样本到逐渐更难的样本。这种结构化的难度递增确保了稳定和逐步的学习。实验评估表明，使用课程DPO方法和高质量负样本训练的HaluCheck模型在各种指标上显著提高了模型性能，在如MedHallu和HaluEval等困难基准上取得了高达24%的改进。此外，HaluCheck模型在零样本设置中表现出较强的鲁棒性，各基准上显著优于更大规模的最先进的模型。', 'title_zh': '教学以谎言为手段：合成负样本下的课程DPO幻觉检测'}
{'arxiv_id': 'arXiv:2505.17508', 'title': 'On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning', 'authors': 'Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Yang Yuan, Quanquan Gu, Andrew C Yao', 'link': 'https://arxiv.org/abs/2505.17508', 'abstract': 'Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). Despite the widespread use of Kullback-Leibler (KL) regularization in policy gradient algorithms to stabilize training, the systematic exploration of how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online reinforcement learning (RL) presents a nuanced and systematically explorable design space. In this paper, we propose regularized policy gradient (RPG), a systematic framework for deriving and analyzing KL-regularized policy gradient methods in the online RL setting. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by both forward and reverse KL divergences, considering both normalized and unnormalized policy distributions. Furthermore, we present derivations for fully differentiable loss functions as well as REINFORCE-style gradient estimators, accommodating diverse algorithmic needs. We conduct extensive experiments on RL for LLM reasoning using these methods, showing improved or competitive results in terms of training stability and performance compared to strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at this https URL.', 'abstract_zh': '正则化策略梯度（RPG）：在线强化学习中KL正则化策略梯度方法的设计与分析', 'title_zh': '基于KL正则化策略梯度算法的大型语言模型推理设计'}
{'arxiv_id': 'arXiv:2505.17500', 'title': 'The Discovery Engine: A Framework for AI-Driven Synthesis and Navigation of Scientific Knowledge Landscapes', 'authors': 'Vladimir Baulin, Austin Cook, Daniel Friedman, Janna Lumiruusu, Andrew Pashea, Shagor Rahman, Benedikt Waldeck', 'link': 'https://arxiv.org/abs/2505.17500', 'abstract': 'The prevailing model for disseminating scientific knowledge relies on individual publications dispersed across numerous journals and archives. This legacy system is ill suited to the recent exponential proliferation of publications, contributing to insurmountable information overload, issues surrounding reproducibility and retractions. We introduce the Discovery Engine, a framework to address these challenges by transforming an array of disconnected literature into a unified, computationally tractable representation of a scientific domain. Central to our approach is the LLM-driven distillation of publications into structured "knowledge artifacts," instances of a universal conceptual schema, complete with verifiable links to source evidence. These artifacts are then encoded into a high-dimensional Conceptual Tensor. This tensor serves as the primary, compressed representation of the synthesized field, where its labeled modes index scientific components (concepts, methods, parameters, relations) and its entries quantify their interdependencies. The Discovery Engine allows dynamic "unrolling" of this tensor into human-interpretable views, such as explicit knowledge graphs (the CNM graph) or semantic vector spaces, for targeted exploration. Crucially, AI agents operate directly on the graph using abstract mathematical and learned operations to navigate the knowledge landscape, identify non-obvious connections, pinpoint gaps, and assist researchers in generating novel knowledge artifacts (hypotheses, designs). By converting literature into a structured tensor and enabling agent-based interaction with this compact representation, the Discovery Engine offers a new paradigm for AI-augmented scientific inquiry and accelerated discovery.', 'abstract_zh': '现有的科学知识传播模型依赖于分散在众多期刊和档案中的个体文献。这一遗留系统不适用于近期指数级增长的文献数量，导致信息过载问题，并且存在可重复性和撤稿问题。我们提出了一种Discovery Engine框架，以应对这些挑战，通过将分散的文献转化为统一的、计算上可处理的科学领域表示。我们方法的核心是通过大语言模型驱动的文献精炼，将其转化为结构化的“知识构件”，这些构件是通用概念模式的实例，同时包含可验证的源证据链接。这些构件随后被编码为高维概念张量。该张量作为综合领域的主要、压缩表示，其标记模式索引科学组件（概念、方法、参数、关系），而其条目量化它们之间的相互依赖关系。Discovery Engine允许动态“展开”这一张量，生成可由人类解释的观点，如明示知识图（CNM图）或语义向量空间，以实现有针对性的探索。至关重要的是，AI代理直接在图上操作，使用抽象数学和学习到的操作来导航知识景观，识别非显而易见的联系，指出缺失，并协助研究人员生成新的知识构件（假设、设计）。通过将文献转换为结构化张量，并使代理能够与这种紧凑表示进行交互，Discovery Engine为AI增强的科学探究和加速发现提供了一种新范式。', 'title_zh': 'AI驱动的科学知识景观合成与导航框架：发现引擎'}
{'arxiv_id': 'arXiv:2505.17496', 'title': 'Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models', 'authors': 'Chi-Yuan Hsiao, Ke-Han Lu, Kai-Wei Chang, Chih-Kai Yang, Wei-Chih Chen, Hung-yi Lee', 'link': 'https://arxiv.org/abs/2505.17496', 'abstract': 'End-to-end training of Spoken Language Models (SLMs) commonly involves adapting pre-trained text-based Large Language Models (LLMs) to the speech modality through multi-stage training on diverse tasks such as ASR, TTS and spoken question answering (SQA). Although this multi-stage continual learning equips LLMs with both speech understanding and generation capabilities, the substantial differences in task and data distributions across stages can lead to catastrophic forgetting, where previously acquired knowledge is lost. This paper investigates catastrophic forgetting and evaluates three mitigation strategies-model merging, discounting the LoRA scaling factor, and experience replay to balance knowledge retention with new learning. Results show that experience replay is the most effective, with further gains achieved by combining it with other methods. These findings provide insights for developing more robust and efficient SLM training pipelines.', 'abstract_zh': '端到端培训语音语言模型中的灾难性遗忘及其缓解策略', 'title_zh': '端到端训练语音语言模型中灾难性遗忘缓解策略分析'}
{'arxiv_id': 'arXiv:2505.17495', 'title': 'ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs', 'authors': 'Landon Butler, Abhineet Agarwal, Justin Singh Kang, Yigit Efe Erginbas, Bin Yu, Kannan Ramchandran', 'link': 'https://arxiv.org/abs/2505.17495', 'abstract': 'Large Language Models (LLMs) have achieved remarkable performance by capturing complex interactions between input features. To identify these interactions, most existing approaches require enumerating all possible combinations of features up to a given order, causing them to scale poorly with the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an information-theoretic approach that uses interaction sparsity to scale to $n \\approx 10^3$ features. SPEX greatly improves upon prior methods but requires tens of thousands of model inferences, which can be prohibitive for large models. In this paper, we observe that LLM feature interactions are often hierarchical -- higher-order interactions are accompanied by their lower-order subsets -- which enables more efficient discovery. To exploit this hierarchy, we propose ProxySPEX, an interaction attribution algorithm that first fits gradient boosted trees to masked LLM outputs and then extracts the important interactions. Experiments across four challenging high-dimensional datasets show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over marginal attribution approaches while using $10\\times$ fewer inferences than SPEX. By accounting for interactions, ProxySPEX identifies features that influence model output over 20% more than those selected by marginal approaches. Further, we apply ProxySPEX to two interpretability tasks. Data attribution, where we identify interactions among CIFAR-10 training samples that influence test predictions, and mechanistic interpretability, where we uncover interactions between attention heads, both within and across layers, on a question-answering task. ProxySPEX identifies interactions that enable more aggressive pruning of heads than marginal approaches.', 'abstract_zh': '大型语言模型（LLMs）通过捕捉输入特征之间的复杂交互关系实现了出色的性能。为了识别这些交互，大多数现有方法需要枚举所有可能的特征组合，导致它们在输入数量 $n$ 增加时扩展性较差。最近，Kang等人（2025）提出了SPEX，一种信息论方法，利用交互稀疏性扩展到 $n \\approx 10^3$ 个特征。SPEX 在改进先前方法的同时，需要数万个模型推断，这对于大型模型来说是具有挑战性的。在本文中，我们观察到LLM特征交互通常是分层的——高阶交互伴随着其低阶子集，这使得更有效地发现这些交互成为可能。为了利用这种分层结构，我们提出了ProxySPEX，一种交互归因算法，首先使用渐进增强树拟合遮蔽的LLM输出，然后提取重要的交互。在四个具有挑战性和高维的数据集上的实验表明，与边际归因方法相比，ProxySPEX 在重建LLM输出方面提高了20%，并且仅使用SPEX所需推断次数的十分之一。通过考虑交互，ProxySPEX 识别出影响模型输出的特征，这些特征比边际方法所选出的特征多出20%以上。此外，我们使用ProxySPEX 应用于两个可解释性任务：数据归因任务中，我们识别出影响测试预测的CIFAR-10训练样本之间的交互；机制可解释性任务中，我们揭示了不同层内的和跨层的注意力头之间的交互，特别是在问答任务中。ProxySPEX 识别出的交互使得相对于边际方法，头的剪枝更加激进。', 'title_zh': 'ProxySPEX: 通过LLMs中稀疏特征交互实现高效的推理解释性'}
{'arxiv_id': 'arXiv:2505.17485', 'title': 'keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection', 'authors': 'Saketh Reddy Vemula, Parameswari Krishnamurthy', 'link': 'https://arxiv.org/abs/2505.17485', 'abstract': 'Identification of hallucination spans in black-box language model generated text is essential for applications in the real world. A recent attempt at this direction is SemEval-2025 Task 3, Mu-SHROOM-a Multilingual Shared Task on Hallucinations and Related Observable Over-generation Errors. In this work, we present our solution to this problem, which capitalizes on the variability of stochastically-sampled responses in order to identify hallucinated spans. Our hypothesis is that if a language model is certain of a fact, its sampled responses will be uniform, while hallucinated facts will yield different and conflicting results. We measure this divergence through entropy-based analysis, allowing for accurate identification of hallucinated segments. Our method is not dependent on additional training and hence is cost-effective and adaptable. In addition, we conduct extensive hyperparameter tuning and perform error analysis, giving us crucial insights into model behavior.', 'abstract_zh': '黑盒语言模型生成文本中幻觉片段的识别对于实际应用至关重要。SemEval-2025 Task 3，一个多语言共享任务，关注幻觉及相关可观察过度生成错误。在本文中，我们提出了一种解决方案，该解决方案利用随机采样响应的可变性来识别幻觉片段。我们的假设是，如果语言模型对其事实确信无疑，其采样响应将是统一的，而幻觉事实将导致不同的且冲突的结果。我们通过基于熵的分析来衡量这种差异，从而能够准确地识别出幻觉段落。我们的方法不依赖于额外的训练，因此具有成本效益且易于适应。此外，我们进行了广泛的超参数调整并进行了错误分析，为我们提供了关于模型行为的关键见解。', 'title_zh': 'SemEval-2025 任务3的简单胜出：基于LLM不确定性的方法用于多语言幻觉 spans 检测'}
{'arxiv_id': 'arXiv:2505.17479', 'title': 'Twin-2K-500: A dataset for building digital twins of over 2,000 people based on their answers to over 500 questions', 'authors': 'Olivier Toubia, George Z. Gui, Tianyi Peng, Daniel J. Merlau, Ang Li, Haozhe Chen', 'link': 'https://arxiv.org/abs/2505.17479', 'abstract': 'LLM-based digital twin simulation, where large language models are used to emulate individual human behavior, holds great promise for research in AI, social science, and digital experimentation. However, progress in this area has been hindered by the scarcity of real, individual-level datasets that are both large and publicly available. This lack of high-quality ground truth limits both the development and validation of digital twin methodologies. To address this gap, we introduce a large-scale, public dataset designed to capture a rich and holistic view of individual human behavior. We survey a representative sample of $N = 2,058$ participants (average 2.42 hours per person) in the US across four waves with 500 questions in total, covering a comprehensive battery of demographic, psychological, economic, personality, and cognitive measures, as well as replications of behavioral economics experiments and a pricing survey. The final wave repeats tasks from earlier waves to establish a test-retest accuracy baseline. Initial analyses suggest the data are of high quality and show promise for constructing digital twins that predict human behavior well at the individual and aggregate levels. By making the full dataset publicly available, we aim to establish a valuable testbed for the development and benchmarking of LLM-based persona simulations. Beyond LLM applications, due to its unique breadth and scale the dataset also enables broad social science research, including studies of cross-construct correlations and heterogeneous treatment effects.', 'abstract_zh': '基于LLM的数字孪生模拟：大规模公开数据集在人工智能、社会科学和数字实验中的应用前景', 'title_zh': 'Twin-2K-500：基于超过500个问题的答案构建超过2000人数字孪生的数据集'}
{'arxiv_id': 'arXiv:2505.17470', 'title': 'SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models', 'authors': 'Xiang Liu, Zhaoxiang Liu, Peng Wang, Kohou Wang, Huan Hu, Kai Wang, Shiguo Lian', 'link': 'https://arxiv.org/abs/2505.17470', 'abstract': "When using supervised fine-tuning (SFT) to adapt large language models (LLMs) to specific domains, a significant challenge arises: should we use the entire SFT dataset for fine-tuning? Common practice often involves fine-tuning directly on the entire dataset due to limited information on the LLM's past training data. However, if the SFT dataset largely overlaps with the model's existing knowledge, the performance gains are minimal, leading to wasted computational resources. Identifying the unknown knowledge within the SFT dataset and using it to fine-tune the model could substantially improve the training efficiency. To address this challenge, we propose a self-learning framework for LLMs inspired by human learning pattern. This framework takes a fine-tuning (SFT) dataset in a specific domain as input. First, the LLMs answer the questions in the SFT dataset. The LLMs then objectively grade the responses and filter out the incorrectly answered QA pairs. Finally, we fine-tune the LLMs based on this filtered QA set. Experimental results in the fields of agriculture and medicine demonstrate that our method substantially reduces training time while achieving comparable improvements to those attained with full dataset fine-tuning. By concentrating on the unknown knowledge within the SFT dataset, our approach enhances the efficiency of fine-tuning LLMs.", 'abstract_zh': '在特定领域使用监督微调（SFT）适配大型语言模型（LLMs）时，一个显著挑战是：是否应使用整个SFT数据集进行微调？', 'title_zh': 'SLearnLLM：一种高效领域特定适应的大语言模型自学习框架'}
{'arxiv_id': 'arXiv:2505.17455', 'title': 'Towards Evaluating Proactive Risk Awareness of Multimodal Language Models', 'authors': 'Youliang Yuan, Wenxiang Jiao, Yuejin Xie, Chihao Shen, Menghan Tian, Wenxuan Wang, Jen-tse Huang, Pinjia He', 'link': 'https://arxiv.org/abs/2505.17455', 'abstract': "Human safety awareness gaps often prevent the timely recognition of everyday risks. In solving this problem, a proactive safety artificial intelligence (AI) system would work better than a reactive one. Instead of just reacting to users' questions, it would actively watch people's behavior and their environment to detect potential dangers in advance. Our Proactive Safety Bench (PaSBench) evaluates this capability through 416 multimodal scenarios (128 image sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation of 36 advanced models reveals fundamental limitations: Top performers like Gemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks in repeated trials. Through failure analysis, we identify unstable proactive reasoning rather than knowledge deficits as the primary limitation. This work establishes (1) a proactive safety benchmark, (2) systematic evidence of model limitations, and (3) critical directions for developing reliable protective AI. We believe our dataset and findings can promote the development of safer AI assistants that actively prevent harm rather than merely respond to requests. Our dataset can be found at this https URL.", 'abstract_zh': '人类安全意识差距往往阻碍了对日常风险的及时识别。为解决这一问题，一种主动的安全人工智能系统将比被动系统更有效。它不仅会对用户的问题作出反应，还会积极观察人们的行為及其环境，提前检测潜在危险。我们的主动安全基准（PaSBench）通过416个多模态场景（128个图像序列，288个文本日志），覆盖5个安全关键领域来评估这一能力。对36个先进模型的评估揭示了根本性限制：如Gemini-2.5-pro这类顶尖模型在图像上的准确率为71%，文本上的准确率为64%，但在重复试验中仍会错过45-55%的风险。通过失败分析，我们认定不稳定的前提推断而非知识缺口是主要限制。本研究建立了（1）一个主动安全基准，（2）系统的模型限制证据，以及（3）开发可靠保护性人工智能的关键方向。我们相信，我们的数据集和发现可以促进开发更加安全的人工智能助手，这些助手能积极预防伤害而非仅仅是响应请求。我们的数据集可以在以下网址找到：this https URL。', 'title_zh': 'Towards 评估多模态语言模型的主动风险意识'}
{'arxiv_id': 'arXiv:2505.17441', 'title': 'Discovering Forbidden Topics in Language Models', 'authors': 'Can Rager, Chris Wendler, Rohit Gandikota, David Bau', 'link': 'https://arxiv.org/abs/2505.17441', 'abstract': 'Refusal discovery is the task of identifying the full set of topics that a language model refuses to discuss. We introduce this new problem setting and develop a refusal discovery method, LLM-crawler, that uses token prefilling to find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an open-source model with public safety tuning data. Our crawler manages to retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale the crawl to a frontier model using the prefilling option of Claude-Haiku. Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two of its variants finetuned for reasoning: DeepSeek-R1-70B and Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with censorship tuning: The model exhibits "thought suppression" behavior that indicates memorization of CCP-aligned responses. Although Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned refusals answers in the quantized model. Our findings highlight the critical need for refusal discovery methods to detect biases, boundaries, and alignment failures of AI systems.', 'abstract_zh': '拒绝发现是识别语言模型拒绝讨论的完整话题集的任务。我们引入了这一新的问题设置，并开发了一种拒绝发现方法——LLM-crawler，该方法使用标记预填充来查找禁忌话题。我们在Tulu-3-8B上 benchmarked LLM-crawler，Tulu-3-8B是一个带有公共安全调优数据的开源模型。我们的爬虫在预算为1000个提示的情况下成功检索到31个主题中的36个。随后，我们使用Claude-Haiku的预填充选项将爬虫扩展到前沿模型。最后，我们爬取了三个广泛使用的开源模型：Llama-3.3-70B及其两个针对推理微调的变体：DeepSeek-R1-70B和Perplexity-R1-1776-70B。DeepSeek-R1-70B揭示了与审查调优一致的模式：该模型表现出“思想压制”行为，表明记住了与中共党派一致的回应。虽然Perplexity-R1-1776-70B对审查具有韧性，但LLM-crawler在量化模型中引发了与中共党派一致的拒绝回答。我们的研究结果强调了拒绝发现方法检测AI系统偏差、边界和对齐失败的迫切需求。', 'title_zh': '发现语言模型中的禁忌话题'}
{'arxiv_id': 'arXiv:2505.17426', 'title': 'UniTTS: An end-to-end TTS system without decoupling of acoustic and semantic information', 'authors': 'Rui Wang, Qianguo Sun, Tianrong Chen, Zhiyun Zeng, Junlong Wu, Jiaxing Zhang', 'link': 'https://arxiv.org/abs/2505.17426', 'abstract': "The emergence of multi-codebook neutral audio codecs such as Residual Vector Quantization (RVQ) and Group Vector Quantization (GVQ) has significantly advanced Large-Language-Model (LLM) based Text-to-Speech (TTS) systems. These codecs are crucial in separating semantic and acoustic information while efficiently harnessing semantic priors. However, since semantic and acoustic information cannot be fully aligned, a significant drawback of these methods when applied to LLM-based TTS is that large language models may have limited access to comprehensive audio information. To address this limitation, we propose DistilCodec and UniTTS, which collectively offer the following advantages: 1) This method can distill a multi-codebook audio codec into a single-codebook audio codec with 32,768 codes while achieving a near 100\\% utilization. 2) As DistilCodec does not employ a semantic alignment scheme, a large amount of high-quality unlabeled audio (such as audiobooks with sound effects, songs, etc.) can be incorporated during training, further expanding data diversity and broadening its applicability. 3) Leveraging the comprehensive audio information modeling of DistilCodec, we integrated three key tasks into UniTTS's pre-training framework: audio modality autoregression, text modality autoregression, and speech-text cross-modal autoregression. This allows UniTTS to accept interleaved text and speech/audio prompts while substantially preserving LLM's text capabilities. 4) UniTTS employs a three-stage training process: Pre-Training, Supervised Fine-Tuning (SFT), and Alignment. Source code and model checkpoints are publicly available at this https URL and this https URL.", 'abstract_zh': '多码本中性音频编解码器（如残差向量量化（RVQ）和组向量量化（GVQ））的出现显著推进了基于大型语言模型（LLM）的文本到语音（TTS）系统。这些编解码器在分离语义和声学信息的同时，高效利用了语义先验。然而，由于语义和声学信息无法完全对齐，这些方法在应用于基于LLM的TTS时的一个重要缺点是大型语言模型可能无法充分访问全面的音频信息。为了解决这一局限性，我们提出了DistilCodec和UniTTS，它们共同提供了以下优势：1）该方法可以将多码本音频编解码器精简为包含32768个码本的单码本音频编解码器，同时实现接近100%的利用率。2）由于DistilCodec不采用语义对齐方案，可以在训练过程中大量融入高质量的未标注音频（如带音效的声音书籍、歌曲等），进一步增加数据多样性并拓宽适用范围。3）利用DistilCodec全面的音频信息模型，我们将三个关键任务集成到UniTTS的预训练框架中：音频模态自回归、文本模态自回归和语音-文本跨模态自回归。这使得UniTTS能够接受交错的文本和语音/音频提示，同时在很大程度上保留LLM的文本能力。4）UniTTS采用三阶段训练过程：预训练、监督微调（SFT）和对齐过程。源代码和模型检查点可在以下网址公开访问：这个 https URL 和这个 https URL。', 'title_zh': 'UniTTS：无需拆分声学和语义信息的端到端TTS系统'}
{'arxiv_id': 'arXiv:2505.17362', 'title': 'A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit', 'authors': 'Zafarullah Mahmood, Soliman Ali, Jiading Zhu, Mohamed Abdelwahab, Michelle Yu Collins, Sihan Chen, Yi Cheng Zhao, Jodi Wolff, Osnat Melamed, Nadia Minian, Marta Maslej, Carolynne Cooper, Matt Ratto, Peter Selby, Jonathan Rose', 'link': 'https://arxiv.org/abs/2505.17362', 'abstract': "The conversational capabilities of Large Language Models (LLMs) suggest that they may be able to perform as automated talk therapists. It is crucial to know if these systems would be effective and adhere to known standards. We present a counsellor chatbot that focuses on motivating tobacco smokers to quit smoking. It uses a state-of-the-art LLM and a widely applied therapeutic approach called Motivational Interviewing (MI), and was evolved in collaboration with clinician-scientists with expertise in MI. We also describe and validate an automated assessment of both the chatbot's adherence to MI and client responses. The chatbot was tested on 106 participants, and their confidence that they could succeed in quitting smoking was measured before the conversation and one week later. Participants' confidence increased by an average of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed adherence to MI standards in 98% of utterances, higher than human counsellors. The chatbot scored well on a participant-reported metric of perceived empathy but lower than typical human counsellors. Furthermore, participants' language indicated a good level of motivation to change, a key goal in MI. These results suggest that the automation of talk therapy with a modern LLM has promise.", 'abstract_zh': '大型语言模型的对话能力暗示它们可能能够担任自动化谈话治疗师。了解这些系统的效果并符合已知标准至关重要。我们介绍了一个侧重于激励烟草吸烟者戒烟的辅导员聊天机器人。该聊天机器人使用了最先进的大型语言模型以及广泛应用于治疗领域的动机访谈（MI）方法，并与具有MI专长的临床科学家合作进行了开发。我们还描述并验证了一个自动评估聊天机器人MI遵守程度及客户端回应的方法。该聊天机器人在106名参与者中进行了测试，测量了他们在对话前及一周后的戒烟信心评分。参与者的信心平均提高了1.7分（在0-10分的量表上）。自动评估结果显示，聊天机器人在98%的陈述中符合MI标准，高于人类辅导员。聊天机器人在用户体验的同理心感知的参与者报告指标上表现良好，但低于典型的人类辅导员。此外，参与者的语言表明了较高的改变认同度，这是MI的关键目标。这些结果表明，使用现代大型语言模型自动化谈话疗法充满希望。', 'title_zh': '一个完全生成的动机 Interviewing 指导员聊天机器人，帮助吸烟者迈向戒烟决定'}
{'arxiv_id': 'arXiv:2505.17332', 'title': 'SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use', 'authors': 'Hitesh Laxmichand Patel, Amit Agarwal, Arion Das, Bhargava Kumar, Srikant Panda, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Dong-Kyu Chae', 'link': 'https://arxiv.org/abs/2505.17332', 'abstract': 'Enterprise customers are increasingly adopting Large Language Models (LLMs) for critical communication tasks, such as drafting emails, crafting sales pitches, and composing casual messages. Deploying such models across different regions requires them to understand diverse cultural and linguistic contexts and generate safe and respectful responses. For enterprise applications, it is crucial to mitigate reputational risks, maintain trust, and ensure compliance by effectively identifying and handling unsafe or offensive language. To address this, we introduce SweEval, a benchmark simulating real-world scenarios with variations in tone (positive or negative) and context (formal or informal). The prompts explicitly instruct the model to include specific swear words while completing the task. This benchmark evaluates whether LLMs comply with or resist such inappropriate instructions and assesses their alignment with ethical frameworks, cultural nuances, and language comprehension capabilities. In order to advance research in building ethically aligned AI systems for enterprise use and beyond, we release the dataset and code: this https URL.', 'abstract_zh': '企业客户越来越多地将大型语言模型（LLMs）应用于关键沟通任务，如撰写电子邮件、撰写销售提案和创作非正式信息。在不同地区部署这些模型需要它们理解多样化的文化与语言背景，并生成安全和尊重人的回应。对于企业应用而言，有效地识别和处理不安全或冒犯性的语言，以减轻声誉风险、维护信任并确保合规性至关重要。为此，我们引入了SweEval基准，该基准模拟了具有不同语气（正面或负面）和语境（正式或非正式）的真实世界场景。提示明确指示模型在完成任务时包含特定的脏话。该基准评估LLMs是否遵守或抵制这样的不合适指令，并评估它们与道德框架、文化细微差别和语言理解能力的兼容性。为了推进构建符合伦理的企业用AI系统的研究及其更广泛的应用，我们发布数据集和代码：this https URL。', 'title_zh': 'SweEval: 难道LLM们真的会咒骂吗？一间企业使用的安全性基准测试'}
{'arxiv_id': 'arXiv:2505.17322', 'title': 'From Compression to Expansion: A Layerwise Analysis of In-Context Learning', 'authors': 'Jiachen Jiang, Yuxin Dong, Jinxin Zhou, Zhihui Zhu', 'link': 'https://arxiv.org/abs/2505.17322', 'abstract': 'In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks without weight updates by learning from demonstration sequences. While ICL shows strong empirical performance, its internal representational mechanisms are not yet well understood. In this work, we conduct a statistical geometric analysis of ICL representations to investigate how task-specific information is captured across layers. Our analysis reveals an intriguing phenomenon, which we term *Layerwise Compression-Expansion*: early layers progressively produce compact and discriminative representations that encode task information from the input demonstrations, while later layers expand these representations to incorporate the query and generate the prediction. This phenomenon is observed consistently across diverse tasks and a range of contemporary LLM architectures. We demonstrate that it has important implications for ICL performance -- improving with model size and the number of demonstrations -- and for robustness in the presence of noisy examples. To further understand the effect of the compact task representation, we propose a bias-variance decomposition and provide a theoretical analysis showing how attention mechanisms contribute to reducing both variance and bias, thereby enhancing performance as the number of demonstrations increases. Our findings reveal an intriguing layerwise dynamic in ICL, highlight how structured representations emerge within LLMs, and showcase that analyzing internal representations can facilitate a deeper understanding of model behavior.', 'abstract_zh': '上下文学习(ICL)使大规模语言模型(LLMs)能够在无需权重更新的情况下适应新任务，通过从演示序列中学习。尽管ICL表现出强大的实证性能，但其内部表征机制尚不完全理解。在此工作中，我们进行了一种统计几何分析，以探讨任务特定信息如何在各层中被捕获。我们的分析揭示了一种有趣的现象，我们称之为“逐层压缩-扩展”：早期层逐步生成紧凑且判别性的表征，编码输入演示中的任务信息，而后期层则扩展这些表征以包含查询并生成预测。这种现象在多种任务和一系列现代LLM架构中得到一致观察。我们表明，这对其性能（随着模型规模和演示次数的增加而改善）以及在存在嘈杂示例时的鲁棒性具有重要影响。为了进一步理解紧凑任务表征的影响，我们提出了一种偏差-方差分解，并提供了一种理论分析，说明注意机制如何通过减少偏差和方差来提高性能，随着演示次数的增加而增强性能。我们的发现揭示了ICL中有趣的逐层动态，突显了结构化表征如何在LLMs中浮现，并展示了分析内部表征如何促进对模型行为更深的理解。', 'title_zh': '从压缩到扩展：基于层的内文学习分析'}
{'arxiv_id': 'arXiv:2505.17316', 'title': 'Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models', 'authors': 'Jiachen Jiang, Jinxin Zhou, Bo Peng, Xia Ning, Zhihui Zhu', 'link': 'https://arxiv.org/abs/2505.17316', 'abstract': "Achieving better alignment between vision embeddings and Large Language Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs (MLLMs), particularly for recent models that rely on powerful pretrained vision encoders and LLMs. A common approach to connect the pretrained vision encoder and LLM is through a projector applied after the vision encoder. However, the projector is often trained to enable the LLM to generate captions, and hence the mechanism by which LLMs understand each vision token remains unclear. In this work, we first investigate the role of the projector in compressing vision embeddings and aligning them with word embeddings. We show that the projector significantly compresses visual information, removing redundant details while preserving essential elements necessary for the LLM to understand visual content. We then examine patch-level alignment -- the alignment between each vision patch and its corresponding semantic words -- and propose a *multi-semantic alignment hypothesis*. Our analysis indicates that the projector trained by caption loss improves patch-level alignment but only to a limited extent, resulting in weak and coarse alignment. To address this issue, we propose *patch-aligned training* to efficiently enhance patch-level alignment. Our experiments show that patch-aligned training (1) achieves stronger compression capability and improved patch-level alignment, enabling the MLLM to generate higher-quality captions, (2) improves the MLLM's performance by 16% on referring expression grounding tasks, 4% on question-answering tasks, and 3% on modern instruction-following benchmarks when using the same supervised fine-tuning (SFT) setting. The proposed method can be easily extended to other multimodal models.", 'abstract_zh': '实现更好的视觉嵌入与大型语言模型的对齐对于增强多模态大型语言模型的能力至关重要，尤其是在依赖强大预训练视觉编码器和大型语言模型的最近模型中。一种常见的连接预训练视觉编码器和大型语言模型的方法是在视觉编码器之后应用一个投影器。然而，该投影器通常被训练以使大型语言模型生成描述，因此大型语言模型理解每个视觉标记的机制仍然不清楚。在本工作中，我们首先研究了投影器在压缩视觉嵌入并将它们与词嵌入对齐中的作用。我们表明，投影器显着压缩了视觉信息，去除了冗余细节，同时保留了大型语言模型理解视觉内容所必需的关键元素。随后，我们考察了像素级对齐——即每个视觉像素与其对应的语义词之间的对齐——并提出了一种“多语义对齐假说”。我们的分析表明，通过描述损失训练的投影器可以改善像素级对齐，但只能在有限的范围内，导致弱且粗糙的对齐。为解决这一问题，我们提出了“像素对齐训练”以有效地提升像素级对齐。我们的实验表明，像素对齐训练（1）实现了更强的压缩能力和改进了像素级对齐，使多模态大型语言模型能够生成更具质量的描述，（2）在使用相同监督微调（SFT）设置时，多模态大型语言模型在引用表达对接任务中的性能提高了16%，在问答任务中的性能提高了4%，在现代指令跟随基准中的性能提高了3%。所提出的方法可以容易地扩展到其他多模态模型。', 'title_zh': '分析细粒度对齐并增强多模态语言模型的视觉理解'}
{'arxiv_id': 'arXiv:2505.17281', 'title': 'Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty', 'authors': 'Peilin Wu, Mian Zhang, Xinlu Zhang, Xinya Du, Zhiyu Zoey Chen', 'link': 'https://arxiv.org/abs/2505.17281', 'abstract': "Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by enabling dynamic, multi-step reasoning and information retrieval. However, these systems often exhibit sub-optimal search behaviors like over-search (retrieving redundant information) and under-search (failing to retrieve necessary information), which hinder efficiency and reliability. This work formally defines and quantifies these behaviors, revealing their prevalence across multiple QA datasets and agentic RAG systems (e.g., one model could have avoided searching in 27.7% of its search steps). Furthermore, we demonstrate a crucial link between these inefficiencies and the models' uncertainty regarding their own knowledge boundaries, where response accuracy correlates with model's uncertainty in its search decisions. To address this, we propose $\\beta$-GRPO, a reinforcement learning-based training method that incorporates confidence threshold to reward high-certainty search decisions. Experiments on seven QA benchmarks show that $\\beta$-GRPO enable a 3B model with better agentic RAG ability, outperforming other strong baselines with a 4% higher average exact match score.", 'abstract_zh': '代理检索增强生成（RAG）系统通过实现动态多步推理和信息检索，增强了大型语言模型（LLMs）。然而，这些系统常常表现出如过度检索（检索冗余信息）和不足检索（未能检索必要信息）等次优搜索行为，这阻碍了效率和可靠性。本研究正式定义并量化了这些行为，在多个QA数据集和代理RAG系统中揭示了它们的普遍性（例如，一个模型在其搜索步骤中可以避免搜索27.7%的情况）。此外，我们证明了这些低效率与模型对自己知识边界不确定性的关联，响应准确性与模型在搜索决策中的不确定性相关。为了解决这一问题，我们提出了基于强化学习的$\\beta$-GRPO训练方法，该方法结合了置信阈值以奖励高确定性的搜索决策。在七个QA基准上的实验显示，$\\beta$-GRPO使一个3B模型具备更好的代理RAG能力，与其它强基线相比，其平均精确匹配得分高4%。', 'title_zh': '明智搜索：通过降低不确定性来缓解亚优化代理搜索'}
{'arxiv_id': 'arXiv:2505.17265', 'title': 'CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports', 'authors': 'Xiao Yu Cindy Zhang, Carlos R. Ferreira, Francis Rossignol, Raymond T. Ng, Wyeth Wasserman, Jian Zhu', 'link': 'https://arxiv.org/abs/2505.17265', 'abstract': "Rare diseases, including Inborn Errors of Metabolism (IEM), pose significant diagnostic challenges. Case reports serve as key but computationally underutilized resources to inform diagnosis. Clinical dense information extraction refers to organizing medical information into structured predefined categories. Large Language Models (LLMs) may enable scalable information extraction from case reports but are rarely evaluated for this task. We introduce CaseReportBench, an expert-annotated dataset for dense information extraction of case reports, focusing on IEMs. Using this dataset, we assess various models and prompting strategies, introducing novel approaches such as category-specific prompting and subheading-filtered data integration. Zero-shot chain-of-thought prompting offers little advantage over standard zero-shot prompting. Category-specific prompting improves alignment with the benchmark. The open-source model Qwen2.5-7B outperforms GPT-4o for this task. Our clinician evaluations show that LLMs can extract clinically relevant details from case reports, supporting rare disease diagnosis and management. We also highlight areas for improvement, such as LLMs' limitations in recognizing negative findings important for differential diagnosis. This work advances LLM-driven clinical natural language processing and paves the way for scalable medical AI applications.", 'abstract_zh': '罕见疾病，包括先天代谢误差（IEM），诊断极具挑战性。案例报告作为关键但计算上未充分利用的资源，用于辅助诊断。临床密集信息提取是指将医学信息组织到结构化预定义类别中。大规模语言模型（LLMs）可能使案例报告的信息提取规模化，但很少对此任务进行评估。我们介绍了CaseReportBench，一个由专家注释的用于案例报告密集信息提取的数据集，重点关注IEMs。利用该数据集，我们评估了各种模型和提示策略，引入了类别特定提示和子标题过滤数据集成等新方法。零样本链式思考提示在效果上几乎没有优于标准零样本提示的优势。类别特定提示提高了与基准的对齐度。开源模型Qwen2.5-7B在这一任务上优于GPT-4o。我们的临床医生评估表明，LLMs可以从案例报告中提取临床相关细节，支持罕见疾病的诊断和管理。我们还指出了改进领域，如LLMs在识别对鉴别诊断重要的阴性发现方面存在局限性。这项工作推动了LLM驱动的临床自然语言处理，并为可扩展的医学AI应用铺平了道路。', 'title_zh': 'CaseReportBench: 一键提取临床病例报告中密集信息的LLM基准数据集'}
{'arxiv_id': 'arXiv:2505.17250', 'title': 'ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models', 'authors': 'Razvan-Gabriel Dumitru, Darius Peteleaza, Vikas Yadav, Liangming Pan', 'link': 'https://arxiv.org/abs/2505.17250', 'abstract': 'Large language models excel at complex tasks by breaking down problems into structured reasoning steps. However, reasoning traces often extend beyond reaching a correct answer, causing wasted computation, reduced readability, and hallucinations. To address this, we introduce a novel hyperparameter-free conciseness score used as a reward signal within a reinforcement learning framework to guide models toward generating correct and concise reasoning traces. This score is evaluated by a large language model acting as a judge, enabling dynamic, context-aware feedback beyond simple token length. Our method achieves state-of-the-art efficiency-accuracy trade-offs on the MATH dataset, reducing token usage by up to 31x on simple problems while improving accuracy by 7%, and on the hardest problems, it outperforms full reasoning by +7.5% accuracy with up to 3.6x fewer tokens. On TheoremQA, our method improves accuracy by +2.2% using 12.5x fewer tokens. We also conduct ablation studies on the judge model, reward composition, and problem difficulty, showing that our method dynamically adapts reasoning length based on problem difficulty and benefits significantly from stronger judges. The code, model weights, and datasets are open-sourced at this https URL.', 'abstract_zh': '大型语言模型通过将问题分解为结构化推理步骤来胜任复杂任务，但推理痕迹往往会超出得出正确答案的范围，导致计算资源浪费、降低可读性及产生幻觉。为此，我们引入了一种新型无需超参数的简洁性评分，将其作为强化学习框架内的奖励信号，以引导模型生成正确且简洁的推理痕迹。该评分通过大型语言模型作为评判者进行评估，允许动态、上下文相关的反馈超越简单的 token 长度评估。我们的方法在 MATH 数据集上实现了最先进的效率-准确率权衡，在简单问题上将 token 使用量最多减少 31 倍，并将准确率提高 7%；在最难的问题上，它在准确率上比完整推理高出 7.5%，且使用不到 3.6 倍的 token 数量。在 TheoremQA 上，我们的方法使用 12.5 倍 fewer tokens 时准确率提高了 2.2%。我们还进行了评判者模型、奖励组成和问题难度的消融研究，结果显示我们的方法会根据问题难度动态调整推理长度，并且显著受益于更强的评判者。相关代码、模型权重和数据集在 https://github.com/xxx开放获取。', 'title_zh': 'ConciseRL：简洁性引导的强化学习方法以促进高效推理模型'}
{'arxiv_id': 'arXiv:2505.17244', 'title': 'ReasoningShield: Content Safety Detection over Reasoning Traces of Large Reasoning Models', 'authors': 'Changyi Li, Jiayi Wang, Xudong Pan, Geng Hong, Min Yang', 'link': 'https://arxiv.org/abs/2505.17244', 'abstract': 'Large Reasoning Models (LRMs) are transforming the AI landscape with advanced reasoning capabilities. While the generated reasoning traces enhance model transparency, they can still contain unsafe content, even when the final answer appears safe. Existing moderation tools, primarily designed for question-answer (QA) pairs, are empirically ineffective at detecting hidden risks embedded in reasoning traces. After identifying the key challenges, we formally define the question-thought (QT) moderation task and propose ReasoningShield, the first safety detection model tailored to identify potential risks in the reasoning trace before reaching the final answer. To construct the model, we synthesize a high-quality reasoning safety detection dataset comprising over 8,000 question-thought pairs spanning ten risk categories and three safety levels. Our dataset construction process incorporates a comprehensive human-AI collaborative annotation pipeline, which achieves over 93% annotation accuracy while significantly reducing human costs. On a diverse set of in-distribution and out-of-distribution benchmarks, ReasoningShield outperforms mainstream content safety moderation models in identifying risks within reasoning traces, with an average F1 score exceeding 0.92. Notably, despite being trained on our QT dataset only, ReasoningShield also demonstrates competitive performance in detecting unsafe question-answer pairs on traditional benchmarks, rivaling baselines trained on 10 times larger datasets and base models, which strongly validates the quality of our dataset. Furthermore, ReasoningShield is built upon compact 1B/3B base models to facilitate lightweight deployment and provides human-friendly risk analysis by default. To foster future research, we publicly release all the resources.', 'abstract_zh': '大型推理模型（LRMs）正在以先进的推理能力改变AI格局。虽然生成的推理轨迹能够提升模型的透明度，但仍可能包含不安全的内容，即使最终答案看似安全。现有的主要用于问答（QA）对的审核工具，在检测推理轨迹中隐藏的风险方面见效甚微。在识别关键挑战后，我们正式定义了问题-思考（QT）审核任务，并提出ReasoningShield，这是首个针对推理轨迹中潜在风险进行检测的安全检测模型。为构建该模型，我们合成了一套高质量的推理安全检测数据集，包含超过8,000个问题-思考对，涵盖十个风险类别和三个安全等级。我们的数据集构建过程集成了全面的人机协作注释流水线，准确率达到93%以上，同时大幅降低了人力成本。在不同分布的基准测试中，ReasoningShield在识别推理轨迹中的风险方面优于主流内容安全审核模型，平均F1分数超过0.92。值得注意的是，尽管仅基于我们的QT数据集进行训练，但ReasoningShield在传统基准上检测不安全的问答对时也表现出竞争力，媲美使用更大规模数据集和基模型训练的基模型，这有力地验证了我们数据集的质量。此外，ReasoningShield基于紧凑的1B/3B基模型构建，便于轻量级部署，并默认提供用户友好的风险分析。为了促进未来研究，我们公开发布了所有资源。', 'title_zh': 'ReasoningShield：大型推理模型推理轨迹的内容安全检测'}
{'arxiv_id': 'arXiv:2505.17242', 'title': 'Optimal Policy Minimum Bayesian Risk', 'authors': 'Ramón Fernandez Astudillo, Md Arafat Sultan, Aashka Trivedi, Yousef El-Kurdi, Tahira Naseem, Radu Florian, Salim Roukos', 'link': 'https://arxiv.org/abs/2505.17242', 'abstract': 'Inference scaling can help LLMs solve complex reasoning problems through extended runtime computation. On top of targeted supervision for long chain-of-thought (long-CoT) generation, purely inference-time techniques such as best-of-N (BoN) sampling, majority voting, or more generally, minimum Bayes risk decoding (MBRD), can further improve LLM accuracy by generating multiple candidate solutions and aggregating over them. These methods typically leverage additional signals in the form of reward models and risk/similarity functions that compare generated samples, e.g., exact match in some normalized space or standard similarity metrics such as Rouge. Here we present a novel method for incorporating reward and risk/similarity signals into MBRD. Based on the concept of optimal policy in KL-controlled reinforcement learning, our framework provides a simple and well-defined mechanism for leveraging such signals, offering several advantages over traditional inference-time methods: higher robustness, improved accuracy, and well-understood asymptotic behavior. In addition, it allows for the development of a sample-efficient variant of MBRD that can adjust the number of samples to generate according to the difficulty of the problem, without relying on majority vote counts. We empirically demonstrate the advantages of our approach on math (MATH-$500$) and coding (HumanEval) tasks using recent open-source models. We also present a comprehensive analysis of its accuracy-compute trade-offs.', 'abstract_zh': '基于推断缩放的复杂推理问题解决方法：通过额外信号改进最小贝叶斯风险解码', 'title_zh': '最小贝叶斯风险最优策略'}
{'arxiv_id': 'arXiv:2505.17217', 'title': 'Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs', 'authors': 'Kangda Wei, Hasnat Md Abdullah, Ruihong Huang', 'link': 'https://arxiv.org/abs/2505.17217', 'abstract': 'Large Language Models (LLMs) often exhibit gender bias, resulting in unequal treatment of male and female subjects across different contexts. To address this issue, we propose a novel data generation framework that fosters exploratory thinking in LLMs. Our approach prompts models to generate story pairs featuring male and female protagonists in structurally identical, morally ambiguous scenarios, then elicits and compares their moral judgments. When inconsistencies arise, the model is guided to produce balanced, gender-neutral judgments. These story-judgment pairs are used to fine-tune or optimize the models via Direct Preference Optimization (DPO). Experimental results show that our method significantly reduces gender bias while preserving or even enhancing general model capabilities. We will release the code and generated data.', 'abstract_zh': '大型语言模型（LLMs）经常表现出性别偏见，导致在不同情境下对男性和女性主体的不平等对待。为了解决这一问题，我们提出了一种新的数据生成框架，旨在激发大型语言模型的探究性思维。我们的方法通过提示模型生成结构相同但道德模糊的故事情节，分别以男性和女性为主角，然后引发并比较它们的道德判断。当出现不一致时，模型被引导生成平衡且性别中立的判断。这些故事情节及其判断配对用于通过直接偏好优化（DPO）微调或优化模型。实验结果表明，我们的方法显著降低了性别偏见，同时保持或甚至提升了模型的通用能力。我们将发布代码和生成的数据。', 'title_zh': '通过促进生成性思考来减轻LLMs中的性别偏见'}
{'arxiv_id': 'arXiv:2505.17206', 'title': 'FB-RAG: Improving RAG with Forward and Backward Lookup', 'authors': 'Kushal Chawla, Alfy Samuel, Anoop Kumar, Daben Liu', 'link': 'https://arxiv.org/abs/2505.17206', 'abstract': 'The performance of Retrieval Augmented Generation (RAG) systems relies heavily on the retriever quality and the size of the retrieved context. A large enough context ensures that the relevant information is present in the input context for the LLM, but also incorporates irrelevant content that has been shown to confuse the models. On the other hand, a smaller context reduces the irrelevant information, but it often comes at the risk of losing important information necessary to answer the input question. This duality is especially challenging to manage for complex queries that contain little information to retrieve the relevant chunks from the full context. To address this, we present a novel framework, called FB-RAG, which enhances the RAG pipeline by relying on a combination of backward lookup (overlap with the query) and forward lookup (overlap with candidate reasons and answers) to retrieve specific context chunks that are the most relevant for answering the input query. Our evaluations on 9 datasets from two leading benchmarks show that FB-RAG consistently outperforms RAG and Long Context baselines developed recently for these benchmarks. We further show that FB-RAG can improve performance while reducing latency. We perform qualitative analysis of the strengths and shortcomings of our approach, providing specific insights to guide future work.', 'abstract_zh': 'FB-RAG：一种结合反向和正向查找以增强检索增强生成的新型框架', 'title_zh': 'FB-RAG: 改进RAG的前向和后向查找方法'}
{'arxiv_id': 'arXiv:2505.17169', 'title': 'Next Token Perception Score: Analytical Assessment of your LLM Perception Skills', 'authors': 'Yu-Ang Cheng, Leyang Hu, Hai Huang, Randall Balestriero', 'link': 'https://arxiv.org/abs/2505.17169', 'abstract': 'Autoregressive pretraining has become the de facto paradigm for learning general-purpose representations in large language models (LLMs). However, linear probe performance across downstream perception tasks shows substantial variability, suggesting that features optimized for next-token prediction do not consistently transfer well to downstream perception tasks. We demonstrate that representations learned via autoregression capture features that may lie outside the subspaces most informative for perception. To quantify the (mis)alignment between autoregressive pretraining and downstream perception, we introduce the Next Token Perception Score (NTPS)-a score derived under a linear setting that measures the overlap between autoregressive and perception feature subspaces. This metric can be easily computed in closed form from pretrained representations and labeled data, and is proven to both upper- and lower-bound the excess loss. Empirically, we show that NTPS correlates strongly with linear probe accuracy across 12 diverse NLP datasets and eight pretrained models ranging from 270M to 8B parameters, confirming its utility as a measure of alignment. Furthermore, we show that NTPS increases following low-rank adaptation (LoRA) fine-tuning, especially in large models, suggesting that LoRA aligning representations to perception tasks enhances subspace overlap and thus improves downstream performance. More importantly, we find that NTPS reliably predicts the additional accuracy gains attained by LoRA finetuning thereby providing a lightweight prescreening tool for LoRA adaptation. Our results offer both theoretical insights and practical tools for analytically assessing LLM perception skills.', 'abstract_zh': '自回归预训练已成为大型语言模型（LLMs）中学习通用表示的实际范式。然而，下游感知任务上的线性探针性能显示出显著的差异性，表明优化用于下一个令牌预测的特征并不一致地转移到下游感知任务。我们证明了通过自回归学习得到的表示捕获了可能位于对感知最具信息性的子空间之外的特征。为了量度自回归预训练与下游感知之间的（不）对齐程度，我们引入了下一个令牌感知分数（Next Token Perception Score, NTPS）—一种基于线性设置的得分，用于测量自回归特征子空间与感知特征子空间之间的重叠程度。该度量可以从预训练表示和标注数据中以闭式形式轻松计算得出，并且被证明能够严格地上下界超过损失。实验上，我们展示了NTPS与12个不同NLP数据集和8个不同规模的预训练模型（从2.7亿到80亿参数不等）上的线性探针准确率之间存在强烈的相关性，从而证实了其作为对齐度量的实用性。此外，我们显示了NTPS在低秩适应（LoRA）微调后增加，特别是在大型模型中，这表明LoRA将表示向感知任务对齐提高了子空间重叠，并因此改善了下游性能。更重要的是，我们发现NTPS可靠地预测了通过LoRA微调获得的额外准确率增益，因此提供了一种轻量级的LoRA适应前筛工具。我们的结果既提供了理论洞察，也为分析LLM感知能力提供了实用工具。', 'title_zh': '下一个令牌感知分数：对您的LLM感知能力的分析评估'}
{'arxiv_id': 'arXiv:2505.17163', 'title': 'OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning', 'authors': 'Mingxin Huang, Yongxin Shi, Dezhi Peng, Songxuan Lai, Zecheng Xie, Lianwen Jin', 'link': 'https://arxiv.org/abs/2505.17163', 'abstract': 'Recent advancements in multimodal slow-thinking systems have demonstrated remarkable performance across diverse visual reasoning tasks. However, their capabilities in text-rich image reasoning tasks remain understudied due to the lack of a systematic benchmark. To address this gap, we propose OCR-Reasoning, a comprehensive benchmark designed to systematically assess Multimodal Large Language Models on text-rich image reasoning tasks. The benchmark comprises 1,069 human-annotated examples spanning 6 core reasoning abilities and 18 practical reasoning tasks in text-rich visual scenarios. Furthermore, unlike other text-rich image understanding benchmarks that only annotate the final answers, OCR-Reasoning also annotates the reasoning process simultaneously. With the annotated reasoning process and the final answers, OCR-Reasoning evaluates not only the final answers generated by models but also their reasoning processes, enabling a holistic analysis of their problem-solving abilities. Leveraging this benchmark, we conducted a comprehensive evaluation of state-of-the-art MLLMs. Our results demonstrate the limitations of existing methodologies. Notably, even state-of-the-art MLLMs exhibit substantial difficulties, with none achieving accuracy surpassing 50\\% across OCR-Reasoning, indicating that the challenges of text-rich image reasoning are an urgent issue to be addressed. The benchmark and evaluation scripts are available at this https URL.', 'abstract_zh': 'Recent advancements in multimodal slow-thinking systems have demonstrated remarkable performance across diverse visual reasoning tasks. However, their capabilities in text-rich image reasoning tasks remain understudied due to the lack of a systematic benchmark. To address this gap, we propose OCR-Reasoning, a comprehensive benchmark designed to systematically assess Multimodal Large Language Models on text-rich image reasoning tasks.', 'title_zh': 'OCR推理基准：揭示MLLLMs在复杂图文推理中的真正能力'}
{'arxiv_id': 'arXiv:2505.17162', 'title': 'DailyQA: A Benchmark to Evaluate Web Retrieval Augmented LLMs Based on Capturing Real-World Changes', 'authors': 'Jiehan Cheng, Zhicheng Dou', 'link': 'https://arxiv.org/abs/2505.17162', 'abstract': 'We propose DailyQA, an automatically updated dynamic dataset that updates questions weekly and contains answers to questions on any given date. DailyQA utilizes daily updates from Wikipedia revision logs to implement a fully automated pipeline of data filtering, query generation synthesis, quality checking, answer extraction, and query classification. The benchmark requires large language models (LLMs) to process and answer questions involving fast-changing factual data and covering multiple domains. We evaluate several open-source and closed-source LLMs using different RAG pipelines with web search augmentation. We compare the ability of different models to process time-sensitive web information and find that rerank of web retrieval results is critical. Our results indicate that LLMs still face significant challenges in handling frequently updated information, suggesting that DailyQA benchmarking provides valuable insights into the direction of progress for LLMs and RAG systems.', 'abstract_zh': 'DailyQA：一个每周自动更新的动态数据集，包含任意给定日期的问题及其答案', 'title_zh': 'DailyQA：基于捕捉实时变化评估网络检索增强的大语言模型基准'}
{'arxiv_id': 'arXiv:2505.17160', 'title': 'Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting', 'authors': 'Bang Trinh Tran To, Thai Le', 'link': 'https://arxiv.org/abs/2505.17160', 'abstract': 'This work presents LURK (Latent UnleaRned Knowledge), a novel framework that probes for hidden retained knowledge in unlearned LLMs through adversarial suffix prompting. LURK automatically generates adversarial prompt suffixes designed to elicit residual knowledge about the Harry Potter domain, a commonly used benchmark for unlearning. Our experiments reveal that even models deemed successfully unlearned can leak idiosyncratic information under targeted adversarial conditions, highlighting critical limitations of current unlearning evaluation standards. By uncovering latent knowledge through indirect probing, LURK offers a more rigorous and diagnostic tool for assessing the robustness of unlearning algorithms. All code will be publicly available.', 'abstract_zh': 'This work presents LURK (Latent Unlearned Knowledge), 一种通过对抗后缀提示探求未训练语言模型中隐藏保留知识的新框架。LURK 自动生成旨在唤起有关哈利·波特领域残留知识的对抗后缀提示，该领域常被用作脱 *);\n\n*润色以符合完整的中文表达：\n\nThis work presents LURK (Latent Unlearned Knowledge), 一种通过对抗后缀提示探求未训练语言模型中隐藏保留知识的新框架。LURK 自动生成旨在唤起有关哈利·波特领域残留知识的对抗后缀提示，该领域常被用作脱机学习基准。我们的实验表明，即使被认定成功脱机学习的模型，在针对性的对抗条件下也可能泄露独特的信息，突显了当前脱机学习评估标准的关键局限性。通过间接探求潜在知识，LURK 提供了一种更严格和诊断性的工具，用于评估脱机学习算法的 robustness。所有代码将公开发布。', 'title_zh': '哈利·波特 still 在此！通过自动化 adversarial 提示探究目标未学习大语言模型中的知识泄露'}
{'arxiv_id': 'arXiv:2505.17155', 'title': 'TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling', 'authors': 'Weizhe Lin, Xing Li, Zhiyuan Yang, Xiaojin Fu, Hui-Ling Zhen, Yaoyuan Wang, Xianzhi Yu, Wulong Liu, Xiaosong Li, Mingxuan Yuan', 'link': 'https://arxiv.org/abs/2505.17155', 'abstract': "Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling complex mathematical, logical, and coding tasks by leveraging extended Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging CoT with explicit token-level exploration, can push LRMs' accuracy boundaries, but they incur significant decoding overhead. A key inefficiency source is LRMs often generate redundant thinking CoTs, which demonstrate clear structured overthinking and underthinking patterns. Inspired by human cognitive reasoning processes and numerical optimization theories, we propose TrimR, a verifier-based, training-free, efficient framework for dynamic CoT compression to trim reasoning and enhance test-time scaling, explicitly tailored for production-level deployment. Our method employs a lightweight, pretrained, instruction-tuned verifier to detect and truncate redundant intermediate thoughts of LRMs without any LRM or verifier fine-tuning. We present both the core algorithm and asynchronous online system engineered for high-throughput industrial applications. Empirical evaluations on Ascend NPUs and vLLM show that our framework delivers substantial gains in inference efficiency under large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and GPQA benchmarks, the reasoning runtime of Pangu-R-38B, QwQ-32B, and DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on accuracy.", 'abstract_zh': 'Large Reasoning Models (LRMs)通过利用扩展的链式思考（CoT）推理展示了在处理复杂数学、逻辑和编程任务方面的出色能力。通过延长CoT并进行显式的 token 级探索等测试时扩展方法可以提高 LRMs 的准确边界，但这些方法会产生显著的解码开销。主要的效率降低来源是 LRMs 经常生成冗余的思考 CoTs，这些 CoTs 展现出明显的结构化过度思考和思考不足的模式。受人类认知推理过程和数值优化理论的启发，我们提出了 TrimR，这是一种基于验证器的、无需训练的高效框架，用于动态压缩 CoT，以精简推理并增强测试时扩展能力，特别适用于生产级部署。该方法使用一个轻量级的、指令调优的预训练验证器来检测和截断 LRMs 的冗余中间思考，而无需对任何 LRMs 或验证器进行微调。我们介绍了适用于高吞吐量工业应用的核心算法和异步在线系统。在昇腾 NPUs 和 vLLM 上的实证评估表明，该框架在大规模批量工作中显著提高了推理效率。特别地，在 MATH500、AIME24、AIME25 和 GPQA 四个基准测试中，Pangu-R-38B、QwQ-32B 和 DeepSeek-R1-Distill-Qwen-32B 的推理运行时间最高可提高 70%，而对准确性的影响可以忽略不计。', 'title_zh': 'TrimR：基于验证器的无需训练的思考压缩以实现高效的测试时扩展'}
{'arxiv_id': 'arXiv:2505.17154', 'title': 'Can Large Language Models Design Biological Weapons? Evaluating Moremi Bio', 'authors': 'Gertrude Hattoh, Jeremiah Ayensu, Nyarko Prince Ofori, Solomon Eshun, Darlington Akogo', 'link': 'https://arxiv.org/abs/2505.17154', 'abstract': 'Advances in AI, particularly LLMs, have dramatically shortened drug discovery cycles by up to 40% and improved molecular target identification. However, these innovations also raise dual-use concerns by enabling the design of toxic compounds. Prompting Moremi Bio Agent without the safety guardrails to specifically design novel toxic substances, our study generated 1020 novel toxic proteins and 5,000 toxic small molecules. In-depth computational toxicity assessments revealed that all the proteins scored high in toxicity, with several closely matching known toxins such as ricin, diphtheria toxin, and disintegrin-based snake venom proteins. Some of these novel agents showed similarities with other several known toxic agents including disintegrin eristostatin, metalloproteinase, disintegrin triflavin, snake venom metalloproteinase, corynebacterium ulcerans toxin. Through quantitative risk assessments and scenario analyses, we identify dual-use capabilities in current LLM-enabled biodesign pipelines and propose multi-layered mitigation strategies. The findings from this toxicity assessment challenge claims that large language models (LLMs) are incapable of designing bioweapons. This reinforces concerns about the potential misuse of LLMs in biodesign, posing a significant threat to research and development (R&D). The accessibility of such technology to individuals with limited technical expertise raises serious biosecurity risks. Our findings underscore the critical need for robust governance and technical safeguards to balance rapid biotechnological innovation with biosecurity imperatives.', 'abstract_zh': 'AI进步，尤其是大语言模型，显著缩短了药物发现周期并提高了分子目标识别的效率，但同时也引发了双重用途的担忧，因为这些技术能够设计出有毒化合物。在缺乏安全防护的情况下，Moremi Bio Agent生成了1020种新型有毒蛋白质和5000种有毒小分子。深入的计算毒性评估显示，所有蛋白质均具有高毒性，其中一些与已知毒素如肉毒杆菌毒素、白喉毒素以及蛇毒蛋白酶类极为相似。通过定量风险评估和情景分析，我们发现当前大语言模型驱动的生物设计流程具有双重用途能力，并提出多层次的缓解策略。此次毒性评估结果挑战了大语言模型无法设计生物武器的观点，强化了对大语言模型在生物设计中潜在滥用的担忧，这对研究与开发构成了重大威胁。缺乏技术专长的人士可能能够获得此类技术，从而引发严重的生物安全风险。我们的研究强调了建立稳健治理和技术支持措施以平衡快速生物技术创新与生物安全需求的重要性。', 'title_zh': '大型语言模型能否设计生物武器？评估Moremi Bio'}
{'arxiv_id': 'arXiv:2505.17153', 'title': 'Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning with Shift-FFN', 'authors': 'Yao Xu, Mingyu Xu, Fangyu Lei, Wangtao Sun, Xiangrong Zeng, Bingning Wang, Guang Liu, Shizhu He, Jun Zhao, Kang Liu', 'link': 'https://arxiv.org/abs/2505.17153', 'abstract': "Recently, models such as OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable performance on complex reasoning tasks through Long Chain-of-Thought (Long-CoT) reasoning. Although distilling this capability into student models significantly enhances their performance, this paper finds that fine-tuning LLMs with full parameters or LoRA with a low rank on long CoT data often leads to Cyclical Reasoning, where models repeatedly reiterate previous inference steps until the maximum length limit. Further analysis reveals that smaller differences in representations between adjacent tokens correlates with a higher tendency toward Cyclical Reasoning. To mitigate this issue, this paper proposes Shift Feedforward Networks (Shift-FFN), a novel approach that edits the current token's representation with the previous one before inputting it to FFN. This architecture dynamically amplifies the representation differences between adjacent tokens. Extensive experiments on multiple mathematical reasoning tasks demonstrate that LoRA combined with Shift-FFN achieves higher accuracy and a lower rate of Cyclical Reasoning across various data sizes compared to full fine-tuning and standard LoRA. Our data and code are available at this https URL", 'abstract_zh': '最近，OpenAI-o1和DeepSeek-R1等模型通过长链推理（Long-CoT）在复杂推理任务中表现出色。尽管将这种能力提炼到学生模型中显著提升了它们的性能，但本文发现，在长链推理数据上全参数微调或使用低秩LoRA微调往往会导致循环推理（Cyclical Reasoning），即模型重复迭代之前的推理步骤直至达到最大长度限制。进一步分析表明，相邻令牌间表示差异较小与更高的循环推理倾向相关。为缓解这一问题，本文提出了一种新的方法——Shift Feedforward Networks（Shift-FFN），该方法在将当前令牌输入前馈网络（FFN）之前，用前一个令牌的表示对其进行编辑，从而动态放大相邻令牌间的表示差异。多项数学推理任务的深入实验表明，与全参数微调和标准LoRA相比，结合使用Shift-FFN的LoRA在各种数据规模上实现了更高的准确率和更低的循环推理率。相关的数据和代码可在以下链接获取。', 'title_zh': '放大相邻令牌差异：通过移位前馈网络增强长链条推理'}
{'arxiv_id': 'arXiv:2505.17151', 'title': 'Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions', 'authors': 'Zishuo Bao, Yibo Liu, Changyutao Qiu', 'link': 'https://arxiv.org/abs/2505.17151', 'abstract': 'With the rise of different language model architecture, fine-tuning is becoming even more important for down stream tasks Model gets messy, finding proper hyperparameters for fine-tuning. Although BO has been tried for hyperparameter tuning, most of the existing methods are oblivious to the fact that BO relies on careful choices of acquisition functions, which are essential components of BO that guide how much to explore versus exploit during the optimization process; Different acquisition functions have different levels of sensitivity towards training loss and validation performance; existing methods often just apply an acquisition function no matter if the training and validation performance are sensitive to the acquisition function or not. This work introduces{Bilevel - BO - SWA}, a model fusion approach coupled with a bilevel BO strategy to improve the fine - tunning of large language models. Our work on mixture of acquisition functions like EI and UCB into nested opt loops, where inner loop perform minimization of training loss while outer loops optimized w.r.t. val metric. Experiments on GLUE tasks using RoBERTA - base show that when using EI and UCB, there is an improvement in generalization, and fine - tuning can be improved by up to 2.7%.', 'abstract_zh': '不同语言模型架构的兴起使得下游任务的微调愈加重要，模型变得复杂化，寻找适合的超参数变得困难。虽然已经尝试了基于贝叶斯优化（BO）的超参数调优方法，但大多数现有方法忽略了BO依赖于精心选择的获取函数的事实，这些获取函数是BO的核心组件，能够引导优化过程中的探索与利用平衡；不同的获取函数对于训练损失和验证性能的敏感程度不同；现有方法往往在没有考虑训练和验证性能对获取函数敏感性的情况下就直接应用获取函数。本文介绍了一种结合双层贝叶斯优化策略的模型融合方法——Bilevel-BO-SWA，旨在改善大规模语言模型的微调。实验结果表明，将EI和UCB等获取函数嵌入嵌套优化循环中，可以在GLUE任务中通过RoBERTA-base模型实现更好的泛化性能，微调效果可提高高达2.7%。', 'title_zh': '基于贝叶斯优化的语言模型增强：优化获取函数'}
{'arxiv_id': 'arXiv:2505.17149', 'title': 'Large Language Models for Predictive Analysis: How Far Are They?', 'authors': 'Qin Chen, Yuanyi Ren, Xiaojun Ma, Yuyang Shi', 'link': 'https://arxiv.org/abs/2505.17149', 'abstract': 'Predictive analysis is a cornerstone of modern decision-making, with applications in various domains. Large Language Models (LLMs) have emerged as powerful tools in enabling nuanced, knowledge-intensive conversations, thus aiding in complex decision-making tasks. With the burgeoning expectation to harness LLMs for predictive analysis, there is an urgent need to systematically assess their capability in this domain. However, there is a lack of relevant evaluations in existing studies. To bridge this gap, we introduce the \\textbf{PredictiQ} benchmark, which integrates 1130 sophisticated predictive analysis queries originating from 44 real-world datasets of 8 diverse fields. We design an evaluation protocol considering text analysis, code generation, and their alignment. Twelve renowned LLMs are evaluated, offering insights into their practical use in predictive analysis. Generally, we believe that existing LLMs still face considerable challenges in conducting predictive analysis. See \\href{this https URL}{Github}.', 'abstract_zh': '预测分析是现代决策制定的核心，广泛应用于各个领域。大规模语言模型（LLMs）已成为促进精细、知识密集型对话的强大工具，从而协助复杂决策任务。随着对利用LLMs进行预测分析的期望日益增长，系统评估其在此领域的能力变得尤为紧迫。然而，现有研究中缺乏相关评估。为填补这一空白，我们引入了PredictiQ基准，该基准整合了来自44个真实世界数据集的1130个复杂的预测分析查询，涵盖8个不同领域。我们设计了一项评估协议，考虑了文本分析、代码生成及其对齐。十二种知名的大规模语言模型接受了评估，提供了它们在预测分析中的实用性的洞见。总体而言，我们相信现有的大规模语言模型在进行预测分析方面仍面临诸多挑战。参见Github：this https URL。', 'title_zh': '大型语言模型在预测分析中的应用：它们的发展距离何处？'}
{'arxiv_id': 'arXiv:2505.17148', 'title': "LLM-Powered Agents for Navigating Venice's Historical Cadastre", 'authors': 'Tristan Karch, Jakhongir Saydaliev, Isabella Di Lenardo, Frédéric Kaplan', 'link': 'https://arxiv.org/abs/2505.17148', 'abstract': "Cadastral data reveal key information about the historical organization of cities but are often non-standardized due to diverse formats and human annotations, complicating large-scale analysis. We explore as a case study Venice's urban history during the critical period from 1740 to 1808, capturing the transition following the fall of the ancient Republic and the Ancien Régime. This era's complex cadastral data, marked by its volume and lack of uniform structure, presents unique challenges that our approach adeptly navigates, enabling us to generate spatial queries that bridge past and present urban landscapes. We present a text-to-programs framework that leverages Large Language Models (LLMs) to translate natural language queries into executable code for processing historical cadastral records. Our methodology implements two complementary techniques: a text-to-SQL approach for handling structured queries about specific cadastral information, and a text-to-Python approach for complex analytical operations requiring custom data manipulation. We propose a taxonomy that classifies historical research questions based on their complexity and analytical requirements, mapping them to the most appropriate technical approach. This framework is supported by an investigation into the execution consistency of the system, alongside a qualitative analysis of the answers it produces. By ensuring interpretability and minimizing hallucination through verifiable program outputs, we demonstrate the system's effectiveness in reconstructing past population information, property features, and spatiotemporal comparisons in Venice.", 'abstract_zh': 'cadastral数据揭示了城市历史组织的关键信息，但由于格式多样和人工标注不统一，往往难以标准化，这给大规模分析带来了复杂性。我们以1740年至1808年威尼斯的城市历史为例，探讨了从古代共和国和旧制度崩塌后的过渡时期复杂且不统一的 cadastral 数据所带来的独特挑战，我们的方法巧妙地应对了这些挑战，能够生成连接过去和现在的空间查询。我们提出了一种基于自然语言查询将文本转化为程序的框架，利用大型语言模型（LLMs）将自然语言查询转换为处理历史 cadastral 记录的可执行代码。我们的方法实施了两种互补的技术：一种是将文本转化为SQL以处理具体 cadastral 信息的结构化查询，另一种是将文本转化为Python以进行需要自定义数据操作的复杂分析操作。我们提出了一个分类法，根据历史研究问题的复杂性和分析需求将其分类，并将其映射到最合适的技术方法。该框架结合了系统执行一致性调查和对生成答案的质性分析支持。通过确保程序输出的可验证性和最小化幻觉，我们展示了该系统在重建威尼斯过去的居民信息、财产特征以及时空对比方面的有效性。', 'title_zh': 'LLM赋能的代理人在探索威尼斯历史地籍中的应用'}
{'arxiv_id': 'arXiv:2505.17147', 'title': 'MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming', 'authors': 'Weiyang Guo, Jing Li, Wenya Wang, YU LI, Daojing He, Jun Yu, Min Zhang', 'link': 'https://arxiv.org/abs/2505.17147', 'abstract': 'The proliferation of jailbreak attacks against large language models (LLMs) highlights the need for robust security measures. However, in multi-round dialogues, malicious intentions may be hidden in interactions, leading LLMs to be more prone to produce harmful responses. In this paper, we propose the \\textbf{M}ulti-\\textbf{T}urn \\textbf{S}afety \\textbf{A}lignment (\\ourapproach) framework, to address the challenge of securing LLMs in multi-round interactions. It consists of two stages: In the thought-guided attack learning stage, the red-team model learns about thought-guided multi-round jailbreak attacks to generate adversarial prompts. In the adversarial iterative optimization stage, the red-team model and the target model continuously improve their respective capabilities in interaction. Furthermore, we introduce a multi-turn reinforcement learning algorithm based on future rewards to enhance the robustness of safety alignment. Experimental results show that the red-team model exhibits state-of-the-art attack capabilities, while the target model significantly improves its performance on safety benchmarks.', 'abstract_zh': '针对大规模语言模型多轮对话中 Jailbreak 攻击 proliferating 的需求，提出多轮安全对齐 (\\ourapproach) 框架以确保安全性', 'title_zh': 'MTSA：通过多轮红队演练实现LLM的多轮安全对齐'}
{'arxiv_id': 'arXiv:2505.17145', 'title': 'LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy Compliance', 'authors': 'Yu Wang, Cailing Cai, Zhihua Xiao, Peifung E. Lam', 'link': 'https://arxiv.org/abs/2505.17145', 'abstract': 'Large language models (LLMs) are increasingly applied in fields such as finance, education, and governance due to their ability to generate human-like text and adapt to specialized tasks. However, their widespread adoption raises critical concerns about data privacy and security, including the risk of sensitive data exposure.\nIn this paper, we propose a security framework to enforce policy compliance and mitigate risks in LLM interactions. Our approach introduces three key innovations: (i) LLM-based policy enforcement: a customizable mechanism that enhances domain-specific detection of sensitive data. (ii) Dynamic policy customization: real-time policy adaptation and enforcement during user-LLM interactions to ensure compliance with evolving security requirements. (iii) Sensitive data anonymization: a format-preserving encryption technique that protects sensitive information while maintaining contextual integrity. Experimental results demonstrate that our framework effectively mitigates security risks while preserving the functional accuracy of LLM-driven tasks.', 'abstract_zh': '大型语言模型（LLMs）因其生成人类文本和适应专门任务的能力而在金融、教育和治理等领域得到广泛应用。然而，它们的广泛应用引发了关于数据隐私和安全的关键关注，包括敏感数据泄露的风险。\n\n本文提出了一种安全框架，以确保LLM交互中的策略合规并降低风险。我们的方法引入了三个关键创新：（i）基于LLM的策略执行：一种可定制的机制，增强特定领域的敏感数据检测。（ii）动态策略定制：在用户-LLM交互过程中实时策略适应和执行，以确保满足不断变化的安全要求。（iii）敏感数据匿名化：一种格式保留加密技术，在保护敏感信息的同时保持上下文完整性。实验结果表明，该框架在降低安全风险的同时保持了LLM驱动任务的功能准确性。', 'title_zh': 'LLM接入防护罩：针对隐私政策合规的领域特定大语言模型框架'}
{'arxiv_id': 'arXiv:2505.17140', 'title': 'Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs', 'authors': 'Essa Jan, Moiz Ali, Muhammad Saram Hassan, Fareed Zaffar, Yasir Zaki', 'link': 'https://arxiv.org/abs/2505.17140', 'abstract': 'As the knowledge of large language models (LLMs) becomes outdated over time, there is a growing need for efficient methods to update them, especially when injecting proprietary information. Our study reveals that comprehension-intensive fine-tuning tasks (e.g., question answering and blanks) achieve substantially higher knowledge retention rates (48%) compared to mapping-oriented tasks like translation (17%) or text-to-JSON conversion (20%), despite exposure to identical factual content. We demonstrate that this pattern persists across model architectures and follows scaling laws, with larger models showing improved retention across all task types. However, all models exhibit significant performance drops when applying injected knowledge in broader contexts, suggesting limited semantic integration. These findings show the importance of task selection in updating LLM knowledge, showing that effective knowledge injection relies not just on data exposure but on the depth of cognitive engagement during fine-tuning.', 'abstract_zh': '随着大规模语言模型（LLMs）的知识随着时间变得过时，高效的方法来更新它们变得日益重要，尤其是在注入专有信息时。我们的研究发现，理解密集型细调任务（如问答和填空）的知识留存率（48%）远高于映射导向任务（如翻译，17%或文本到JSON转换，20%）的知识留存率，尽管这些任务接触到的是相同的事实内容。研究显示，这一模式在不同的模型架构中持续存在，并遵循规模定律，更大的模型在所有任务类型中展现出更好的知识留存。然而，所有模型在将注入的知识应用于更广泛的情境时均表现出显著的性能下降，这表明知识的语义整合有限。这些发现强调了在更新LLM知识时选择任务的重要性，表明有效的知识注入不仅依赖于数据暴露，还依赖于细调过程中认知参与的深度。', 'title_zh': '数据掺假还是真正智能？评估注入知识在LLMs中的迁移性'}
{'arxiv_id': 'arXiv:2505.17139', 'title': 'EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models', 'authors': 'Wanghan Xu, Xiangyu Zhao, Yuhao Zhou, Xiaoyu Yue, Ben Fei, Fenghua Ling, Wenlong Zhang, Lei Bai', 'link': 'https://arxiv.org/abs/2505.17139', 'abstract': "Advancements in Large Language Models (LLMs) drive interest in scientific applications, necessitating specialized benchmarks such as Earth science. Existing benchmarks either present a general science focus devoid of Earth science specificity or cover isolated subdomains, lacking holistic evaluation. Furthermore, current benchmarks typically neglect the assessment of LLMs' capabilities in open-ended scientific exploration. In this paper, we present a comprehensive and professional benchmark for the Earth sciences, designed to evaluate the capabilities of LLMs in scientific exploration within this domain, spanning from fundamental to advanced levels. Leveraging a corpus of 100,000 research papers, we first construct two Question Answering (QA) datasets: Earth-Iron, which offers extensive question coverage for broad assessment, and Earth-Silver, which features a higher level of difficulty to evaluate professional depth. These datasets encompass five Earth spheres, 114 disciplines, and 11 task categories, assessing foundational knowledge crucial for scientific exploration. Most notably, we introduce Earth-Gold with new metrics, a dataset comprising open-ended multi-turn dialogues specifically designed to evaluate the advanced capabilities of LLMs in scientific exploration, including methodology induction, limitation analysis, and concept proposal. Extensive experiments reveal limitations in 11 leading LLMs across different domains and tasks, highlighting considerable room for improvement in their scientific exploration capabilities. The benchmark is available on this https URL .", 'abstract_zh': '大型语言模型（LLMs）的进步推动了对科学应用的兴趣， necessitating 专门的基准测试，如地球科学领域的基准测试。现有基准测试要么缺乏地球科学的具体性，要么仅涵盖孤立的子领域，缺乏全面评估。此外，当前的基准测试通常忽略了对LLM在开放性科学探索方面能力的评估。在本文中，我们提出了一项全面而专业的地球科学基准测试，旨在评估LLM在该领域科学探索方面的能力，涵盖从基础到高级的各个层面。利用10万篇研究论文的语料库，我们首先构建了两个问答（QA）数据集：Earth-Iron，提供广泛的提问范围以实现全面评估；Earth-Silver，具备更高的难度，以评估专业深度。这些数据集覆盖了五个地球层、114个学科和11个任务类别，评估了对于科学探索至关重要的基础知识。尤为值得一提的是，我们引入了Earth-Gold数据集，该数据集包含新的指标，专门设计用于评估LLM在科学探索方面的高级能力，包括方法论归纳、局限性分析和概念提案。大量实验揭示了11种领先LLM在不同领域和任务中的局限性，突显了其在科学探索能力方面改进的巨大空间。基准测试可在以下网址获取：this https URL。', 'title_zh': 'EarthSE: 一种评估大型语言模型地球科学探索能力的基准'}
{'arxiv_id': 'arXiv:2505.17138', 'title': 'RAP: Runtime-Adaptive Pruning for LLM Inference', 'authors': 'Huanrong Liu, Chunlin Tian, Xuyang Wei, Jiaheng Dai, Qin Liu, Tianqi Wei, Qingbiao Li, Li Li', 'link': 'https://arxiv.org/abs/2505.17138', 'abstract': 'Large language models (LLMs) excel at language understanding and generation, but their enormous computational and memory requirements hinder deployment. Compression offers a potential solution to mitigate these constraints. However, most existing methods rely on fixed heuristics and thus fail to adapt to runtime memory variations or heterogeneous KV-cache demands arising from diverse user requests. To address these limitations, we propose RAP, an elastic pruning framework driven by reinforcement learning (RL) that dynamically adjusts compression strategies in a runtime-aware manner. Specifically, RAP dynamically tracks the evolving ratio between model parameters and KV-cache across practical execution. Recognizing that FFNs house most parameters, whereas parameter -light attention layers dominate KV-cache formation, the RL agent retains only those components that maximize utility within the current memory budget, conditioned on instantaneous workload and device state. Extensive experiments results demonstrate that RAP outperforms state-of-the-art baselines, marking the first time to jointly consider model weights and KV-cache on the fly.', 'abstract_zh': 'Large语言模型（LLMs）在语言理解和生成方面表现出色，但其巨大的计算和内存需求妨碍了其部署。压缩提供了缓解这些限制的潜在解决方案。然而，现有方法大多依赖于固定的启发式方法，无法适应运行时内存变化或来自多样用户请求的异构KV缓存需求。为了克服这些局限，我们提出了一种基于强化学习（RL）的弹性剪枝框架RAP，该框架能够动态调整压缩策略以适应运行时需求。具体而言，RAP动态跟踪模型参数与KV缓存之间在实际执行过程中的变化比率。由于FFNs占据了大部分参数，而轻量级参数的注意力层主导了KV缓存的形成，RL代理仅保留那些在当前内存预算下最大化其效用的组件，这些组件的保留受即时负载和设备状态的影响。广泛的实验结果表明，RAP在模型权重和KV缓存上同时取得了最优性能，这是首次在运行时联合考虑这两者。', 'title_zh': '运行时自适应剪枝 for LLM推理'}
{'arxiv_id': 'arXiv:2505.17137', 'title': 'Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands', 'authors': 'Kristin Qi, Youxiang Zhu, Caroline Summerour, John A. Batsis, Xiaohui Liang', 'link': 'https://arxiv.org/abs/2505.17137', 'abstract': 'Early detection of cognitive decline is crucial for enabling interventions that can slow neurodegenerative disease progression. Traditional diagnostic approaches rely on labor-intensive clinical assessments, which are impractical for frequent monitoring. Our pilot study investigates voice assistant systems (VAS) as non-invasive tools for detecting cognitive decline through longitudinal analysis of speech patterns in voice commands. Over an 18-month period, we collected voice commands from 35 older adults, with 15 participants providing daily at-home VAS interactions. To address the challenges of analyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a framework that combines (1) LLM-driven iterative prompt refinement for linguistic feature extraction, (2) HuBERT-based acoustic feature extraction, and (3) transformer-based temporal modeling. Using iTransformer, our approach achieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming its baseline by 27.13%. Through our LLM approach, we identify linguistic features that uniquely characterize everyday command usage patterns in individuals experiencing cognitive decline.', 'abstract_zh': '早期认知衰退的检测对于减缓神经退行性疾病进展的干预至关重要。传统诊断方法依赖于劳动密集型的临床评估，这不适合频繁监测。我们的试点研究探讨了语音助手系统（VAS）作为通过纵向分析语音命令的语音模式来检测认知衰退的非侵入性工具。在18个月的时间里，我们从35名老年人中收集了语音命令，其中15名参与者每天进行家庭中的VAS交互。为了解决分析这些短、无结构和嘈杂命令的挑战，我们提出了一种结合了(1)基于大规模语言模型的迭代提示 refinement 以提取语言特征、(2) HuBERT 基础的声音特征提取，以及(3) 基于变压器的时间模型的 Cog-TiPRO 框架。通过使用 iTransformer，我们的方法在检测MCI方面的准确率达到了73.80%，F1分数为72.67%，优于基线方法27.13%。通过我们的大规模语言模型方法，我们确定了独特的语言特征，这些特征表征了认知衰退个体日常命令使用模式。', 'title_zh': 'Cog-TiPRO：通过 longitudinal 语音助手命令进行认知衰退检测的迭代提示精炼方法（使用大语言模型）'}
{'arxiv_id': 'arXiv:2505.17136', 'title': 'Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations', 'authors': 'Yuhan Ji, Song Gao, Ying Nie, Ivan Majić, Krzysztof Janowicz', 'link': 'https://arxiv.org/abs/2505.17136', 'abstract': 'Applying AI foundation models directly to geospatial datasets remains challenging due to their limited ability to represent and reason with geographical entities, specifically vector-based geometries and natural language descriptions of complex spatial relations. To address these issues, we investigate the extent to which a well-known-text (WKT) representation of geometries and their spatial relations (e.g., topological predicates) are preserved during spatial reasoning when the geospatial vector data are passed to large language models (LLMs) including GPT-3.5-turbo, GPT-4, and DeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the spatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt engineering-based, and everyday language-based evaluation. Our experiment results demonstrate that both the embedding-based and prompt engineering-based approaches to geospatial question-answering tasks with GPT models can achieve an accuracy of over 0.6 on average for the identification of topological spatial relations between two geometries. Among the evaluated models, GPT-4 with few-shot prompting achieved the highest performance with over 0.66 accuracy on topological spatial relation inference. Additionally, GPT-based reasoner is capable of properly comprehending inverse topological spatial relations and including an LLM-generated geometry can enhance the effectiveness for geographic entity retrieval. GPT-4 also exhibits the ability to translate certain vernacular descriptions about places into formal topological relations, and adding the geometry-type or place-type context in prompts may improve inference accuracy, but it varies by instance. The performance of these spatial reasoning tasks offers valuable insights for the refinement of LLMs with geographical knowledge towards the development of geo-foundation models capable of geospatial reasoning.', 'abstract_zh': '直接将地理空间数据集应用于AI基础模型仍具有挑战性，因为这些模型在代表和推理地理实体（特别是基于矢量的几何形状和复杂空间关系的自然语言描述）方面能力有限。为了解决这些问题，我们探讨了地理空间向量数据在传递给包括GPT-3.5-turbo、GPT-4和DeepSeek-R1-14B在内的大型语言模型（LLMs）时，其几何形状及其空间关系（例如拓扑谓词）的WKT表示在空间推理过程中的保留程度。我们的工作流采用三种不同的方法来完成空间推理任务进行比较，即基于几何嵌入的方法、基于提示工程的方法和基于日常语言的方法。实验结果表明，基于嵌入和基于提示工程的方法在使用GPT模型进行地理空间问答任务时，可以实现超过0.6的准确率，用于识别两个几何形状之间的拓扑空间关系。在评估的模型中，带有少量提示的GPT-4在拓扑空间关系推理中的表现最佳，准确率超过0.66。此外，基于GPT的空间推理器能够正确理解反向拓扑空间关系，包含由LLM生成的几何形状可以增强地理实体检索的有效性。GPT-4还表现出将某些关于地点的日常描述翻译成正式的拓扑关系的能力，同时在提示中添加几何类型或地点类型的上下文可能提高推理准确率，但效果因实例而异。这些空间推理任务的性能为如何完善具有地理知识的LLMs以开发能够进行地理空间推理的地理基础模型提供了有价值的见解。', 'title_zh': '地理空间推理的基础模型：大型语言模型在理解几何结构和拓扑空间关系方面的能力评估'}
{'arxiv_id': 'arXiv:2505.17134', 'title': 'LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions', 'authors': 'Chaochen Gao, Xing Wu, Zijia Lin, Debing Zhang, Songlin Hu', 'link': 'https://arxiv.org/abs/2505.17134', 'abstract': "High-quality long-context instruction data is essential for aligning long-context large language models (LLMs). Despite the public release of models like Qwen and Llama, their long-context instruction data remains proprietary. Human annotation is costly and challenging, while template-based synthesis methods limit scale, diversity, and quality. We introduce LongMagpie, a self-synthesis framework that automatically generates large-scale long-context instruction data. Our key insight is that aligned long-context LLMs, when presented with a document followed by special tokens preceding a user turn, auto-regressively generate contextually relevant queries. By harvesting these document-query pairs and the model's responses, LongMagpie produces high-quality instructions without human effort. Experiments on HELMET, RULER, and Longbench v2 demonstrate that LongMagpie achieves leading performance on long-context tasks while maintaining competitive performance on short-context tasks, establishing it as a simple and effective approach for open, diverse, and scalable long-context instruction data synthesis.", 'abstract_zh': '高质量的大语境指令数据对于对齐大语言模型（LLMs）的长语境至关重要。尽管模型如Qwen和Llama已经公开发布，但其长语境指令数据仍然保持私有。人工标注成本高且具有挑战性，而基于模板的合成方法限制了规模、多样性和质量。我们提出了LongMagpie，这是一个自动合成大规模长语境指令数据的自合成框架。我们的核心洞察是，当展示了文档并伴有特定标记前的用户回合时，对齐的长语境LLMs能够自回归地生成与上下文相关的问题。通过收集这些文档-查询对以及模型的响应，LongMagpie无需人工努力即可生成高质量的指令。实验表明，LongMagpie在长语境任务上达到了领先性能，同时在短语境任务上保持竞争性性能，确立了它作为一种简单有效的开放、多样和可扩展的长语境指令数据合成方法的地位。', 'title_zh': 'LongMagpie：一种生成大规模长上下文指令的自合成方法'}
{'arxiv_id': 'arXiv:2505.17131', 'title': 'Relative Bias: A Comparative Framework for Quantifying Bias in LLMs', 'authors': 'Alireza Arbabi, Florian Kerschbaum', 'link': 'https://arxiv.org/abs/2505.17131', 'abstract': 'The growing deployment of large language models (LLMs) has amplified concerns regarding their inherent biases, raising critical questions about their fairness, safety, and societal impact. However, quantifying LLM bias remains a fundamental challenge, complicated by the ambiguity of what "bias" entails. This challenge grows as new models emerge rapidly and gain widespread use, while introducing potential biases that have not been systematically assessed. In this paper, we propose the Relative Bias framework, a method designed to assess how an LLM\'s behavior deviates from other LLMs within a specified target domain. We introduce two complementary methodologies: (1) Embedding Transformation analysis, which captures relative bias patterns through sentence representations over the embedding space, and (2) LLM-as-a-Judge, which employs a language model to evaluate outputs comparatively. Applying our framework to several case studies on bias and alignment scenarios following by statistical tests for validation, we find strong alignment between the two scoring methods, offering a systematic, scalable, and statistically grounded approach for comparative bias analysis in LLMs.', 'abstract_zh': '大型语言模型（LLMs）部署规模的扩大加剧了对其固有偏见的关注，引发了对其公平性、安全性和社会影响的关键问题。然而，量化LLM偏见仍然是一个基本挑战，因为“偏见”包含的含义具有模糊性。随着新模型的快速 emergence 和广泛应用，可能引入尚未系统评估的偏见，使这一挑战更加复杂。本文提出相对偏见框架，一种用于评估LLM行为与其在指定目标域内的其他LLM偏差的方法。我们引入了两种互补的方法：1）嵌入变换分析，通过句子嵌入空间中的表示来捕捉相对偏见模式；2）LLM作为法官，利用语言模型进行比较评价。通过对几个偏见和对齐场景的案例研究进行统计检验，我们发现两种评分方法之间存在强烈的对齐，提供了一种系统、可扩展且基于统计的方法，用于LLM中的相对偏见分析。', 'title_zh': '相对偏差：量化LLMs中偏差的比较框架'}
{'arxiv_id': 'arXiv:2505.17125', 'title': 'NEXT-EVAL: Next Evaluation of Traditional and LLM Web Data Record Extraction', 'authors': 'Soyeon Kim, Namhee Kim, Yeonwoo Jeong', 'link': 'https://arxiv.org/abs/2505.17125', 'abstract': 'Effective evaluation of web data record extraction methods is crucial, yet hampered by static, domain-specific benchmarks and opaque scoring practices. This makes fair comparison between traditional algorithmic techniques, which rely on structural heuristics, and Large Language Model (LLM)-based approaches, offering zero-shot extraction across diverse layouts, particularly challenging. To overcome these limitations, we introduce a concrete evaluation framework. Our framework systematically generates evaluation datasets from arbitrary MHTML snapshots, annotates XPath-based supervision labels, and employs structure-aware metrics for consistent scoring, specifically preventing text hallucination and allowing only for the assessment of positional hallucination. It also incorporates preprocessing strategies to optimize input for LLMs while preserving DOM semantics: HTML slimming, Hierarchical JSON, and Flat JSON. Additionally, we created a publicly available synthetic dataset by transforming DOM structures and modifying content. We benchmark deterministic heuristic algorithms and off-the-shelf LLMs across these multiple input formats. Our benchmarking shows that Flat JSON input enables LLMs to achieve superior extraction accuracy (F1 score of 0.9567) and minimal hallucination compared to other input formats like Slimmed HTML and Hierarchical JSON. We establish a standardized foundation for rigorous benchmarking, paving the way for the next principled advancements in web data record extraction.', 'abstract_zh': '一种有效的网页数据记录抽取方法评估框架', 'title_zh': 'NEXT-EVAL: 传统与大语言模型网络数据记录抽取的下一波评估'}
{'arxiv_id': 'arXiv:2505.17117', 'title': 'From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning', 'authors': 'Chen Shani, Dan Jurafsky, Yann LeCun, Ravid Shwartz-Ziv', 'link': 'https://arxiv.org/abs/2505.17117', 'abstract': 'Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.', 'abstract_zh': '人类通过语义压缩将知识组织成紧凑类别，将多样实例映射到抽象表示以保留意义（例如，知更鸟和蓝雀都是鸟类；大多数鸟类会飞）。这些概念反映了表达准确性和表示简单性之间的权衡。大型语言模型（LLMs）展示了卓越的语言能力，但它们的内部表示是否在压缩和语义准确性的权衡上类似于人类尚不清楚。我们引入了一个新的信息论框架，借鉴了速率-失真理论和信息瓶颈原理，以定量比较这些策略。通过对一系列LLM的词汇嵌入与经典的人类分类基准进行分析，我们揭示了关键差异。尽管LLM形成了与人类判断相一致的广泛概念类别，但在捕捉对人类理解至关重要的细微语义差异方面仍存在问题。更根本的是，LLM表现出强烈的统计压缩偏向，而人类的概念系统似乎优先考虑适应性的细腻与情境丰富性，即使这在我们的度量标准下会导致较低的压缩效率。这些发现阐明了当前AI和人类认知架构之间关键的差异，指导着更具人类一致性的LLM概念表示的发展路径。', 'title_zh': '从词到思想：LLMs和人类如何通过压缩获取意义'}
{'arxiv_id': 'arXiv:2505.17115', 'title': 'Swarm Intelligence Enhanced Reasoning: A Density-Driven Framework for LLM-Based Multi-Agent Optimization', 'authors': 'Ying Zhu, Heng Zhou, Rui Su, Peiqin Zhuang, Lei Bai', 'link': 'https://arxiv.org/abs/2505.17115', 'abstract': "Recently, many approaches, such as Chain-of-Thought (CoT) prompting and Multi-Agent Debate (MAD), have been proposed to further enrich Large Language Models' (LLMs) complex problem-solving capacities in reasoning scenarios. However, these methods may fail to solve complex problems due to the lack of ability to find optimal solutions. Swarm Intelligence has been serving as a powerful tool for finding optima in the field of traditional optimization problems. To this end, we propose integrating swarm intelligence into the reasoning process by introducing a novel Agent-based Swarm Intelligence (ASI) paradigm. In this paradigm, we formulate LLM reasoning as an optimization problem and use a swarm intelligence scheme to guide a group of LLM-based agents in collaboratively searching for optimal solutions. To avoid swarm intelligence getting trapped in local optima, we further develop a Swarm Intelligence Enhancing Reasoning (SIER) framework, which develops a density-driven strategy to enhance the reasoning ability. To be specific, we propose to perform kernel density estimation and non-dominated sorting to optimize both solution quality and diversity simultaneously. In this case, SIER efficiently enhances solution space exploration through expanding the diversity of the reasoning path. Besides, a step-level quality evaluation is used to help agents improve solution quality by correcting low-quality intermediate steps. Then, we use quality thresholds to dynamically control the termination of exploration and the selection of candidate steps, enabling a more flexible and efficient reasoning process. Extensive experiments are ...", 'abstract_zh': '近年来，诸如Chain-of-Thought (CoT)提示和Multi-Agent Debate (MAD)等方法被提出，旨在进一步丰富大型语言模型（LLMs）在推理场景中复杂的解决问题能力。然而，这些方法由于缺乏找到最优解的能力，可能无法解决复杂问题。群智智能作为传统优化问题中寻找最优解的强大工具已经得到了广泛应用。为此，我们提出了一种基于代理的群智智能（ASI）范式，将群智智能整合到推理过程中。在此范式中，我们将LLM推理形式化为一个优化问题，并采用群智智能方案引导一组LLM基代理协作搜索最优解。为了防止群智智能陷入局部最优解，我们进一步开发了一种群智智能增强推理（SIER）框架，通过密度驱动策略增强推理能力。具体而言，我们提出了内核密度估计和非支配排序，以同时优化解的质量和多样性。通过这种方式，SIER有效扩展了推理路径的多样性，增强了解空间的探索。此外，我们使用步骤级质量评估来帮助代理通过修正低质量的中间步骤来提高解的质量。然后，我们使用质量阈值动态控制探索的终止和候选步骤的选择，从而实现更灵活和高效的推理过程。广泛实验表明...', 'title_zh': '基于密度驱动框架的 Swarm 智能增强推理：面向大语言模型驱动的多agents最优化'}
{'arxiv_id': 'arXiv:2505.17107', 'title': 'CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution', 'authors': 'Minghao Shao, Haoran Xi, Nanda Rani, Meet Udeshi, Venkata Sai Charan Putrevu, Kimberly Milner, Brendan Dolan-Gavitt, Sandeep Kumar Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Muhammad Shafique', 'link': 'https://arxiv.org/abs/2505.17107', 'abstract': "Large Language Model (LLM) agents can automate cybersecurity tasks and can adapt to the evolving cybersecurity landscape without re-engineering. While LLM agents have demonstrated cybersecurity capabilities on Capture-The-Flag (CTF) competitions, they have two key limitations: accessing latest cybersecurity expertise beyond training data, and integrating new knowledge into complex task planning. Knowledge-based approaches that incorporate technical understanding into the task-solving automation can tackle these limitations. We present CRAKEN, a knowledge-based LLM agent framework that improves cybersecurity capability through three core mechanisms: contextual decomposition of task-critical information, iterative self-reflected knowledge retrieval, and knowledge-hint injection that transforms insights into adaptive attack strategies. Comprehensive evaluations with different configurations show CRAKEN's effectiveness in multi-stage vulnerability detection and exploitation compared to previous approaches. Our extensible architecture establishes new methodologies for embedding new security knowledge into LLM-driven cybersecurity agentic systems. With a knowledge database of CTF writeups, CRAKEN obtained an accuracy of 22% on NYU CTF Bench, outperforming prior works by 3% and achieving state-of-the-art results. On evaluation of MITRE ATT&CK techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating improved cybersecurity capabilities via knowledge-based execution. We make our framework open source to public this https URL.", 'abstract_zh': '基于知识的大语言模型（LLM）代理可以自动化网络安全任务，并且能够在无需重新工程的情况下适应不断演化的网络安全 landscape。虽然大语言模型代理已经在捕获旗子（CTF）竞赛中展示了网络安全能力，但它们存在两个关键局限性：访问超越训练数据的最新网络安全专业知识，以及将新知识整合到复杂任务规划中。基于知识的方法可以通过将技术理解融入任务解决自动化中来应对这些局限性。我们提出了CRAKEN，一种基于知识的大语言模型代理框架，通过三个核心机制提升网络安全能力：任务关键信息的上下文分解、迭代自省知识检索，以及知识提示注入，将洞察力转化为适应性攻击策略。不同配置下的综合评估显示，与以往方法相比，CRAKEN在多阶段漏洞检测和利用方面更有效。我们可扩展的架构为将新安全知识嵌入由大语言模型驱动的网络安全代理系统奠定了新的方法论基础。使用CTF写实数据库，CRAKEN在纽约大学CTF基准上的准确率为22%，比先前工作高3%，达到了最先进的成果。在MITRE ATT&CK技术评估中，CRAKEN比先前工作多解决25-30%的技术，展示了通过基于知识的执行提高的网络安全能力。我们已将该框架开源于此 https://URL。', 'title_zh': 'CRAKEN: 基于知识驱动执行的网络安全大语言模型代理'}
{'arxiv_id': 'arXiv:2505.17103', 'title': 'Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation', 'authors': 'Cécile Rousseau, Tobia Boschi, Giandomenico Cornacchia, Dhaval Salwala, Alessandra Pascale, Juan Bernabe Moreno', 'link': 'https://arxiv.org/abs/2505.17103', 'abstract': "SDForger is a flexible and efficient framework for generating high-quality multivariate time series using LLMs. Leveraging a compact data representation, SDForger provides synthetic time series generation from a few samples and low-computation fine-tuning of any autoregressive LLM. Specifically, the framework transforms univariate and multivariate signals into tabular embeddings, which are then encoded into text and used to fine-tune the LLM. At inference, new textual embeddings are sampled and decoded into synthetic time series that retain the original data's statistical properties and temporal dynamics. Across a diverse range of datasets, SDForger outperforms existing generative models in many scenarios, both in similarity-based evaluations and downstream forecasting tasks. By enabling textual conditioning in the generation process, SDForger paves the way for multimodal modeling and the streamlined integration of time series with textual information. SDForger source code will be open-sourced soon.", 'abstract_zh': 'SDForger：一种利用大语言模型生成高质量多变量时间序列的灵活高效框架', 'title_zh': '基于语言生成时间序列数据：大规模语言模型合成数据生成方法'}
{'arxiv_id': 'arXiv:2505.17091', 'title': 'Large Language Models Implicitly Learn to See and Hear Just By Reading', 'authors': 'Prateek Verma, Mert Pilanci', 'link': 'https://arxiv.org/abs/2505.17091', 'abstract': 'This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.', 'abstract_zh': '本文呈现了一个有趣的发现：通过训练自回归的大语言模型，文本模型本身会发展出内在的图像和音频理解能力，从而仅通过阅读就能够“看到”和“听到”。流行的音频和视觉大语言模型会微调文本大语言模型，以根据图像和音频嵌入生成文本输出。相比之下，我们的架构接受图像块、音频波形或标记作为输入，输出分类管道典型的嵌入或类别标签。我们展示了文本权重在帮助对FSD-50K和GTZAN数据集进行音频分类上的通用性。此外，我们还展示了其在CIFAR-10和Fashion-MNIST图像分类任务以及图像块上的应用效果。这推动了文本大语言模型学习强大的内部电路可以在必要时激活以适应各种应用的观念，而非每次都从头训练模型。', 'title_zh': '大型语言模型仅通过阅读便学会了 see 和 hear。'}
{'arxiv_id': 'arXiv:2505.17084', 'title': 'From nuclear safety to LLM security: Applying non-probabilistic risk management strategies to build safe and secure LLM-powered systems', 'authors': 'Alexander Gutfraind, Vicki Bier', 'link': 'https://arxiv.org/abs/2505.17084', 'abstract': 'Large language models (LLMs) offer unprecedented and growing capabilities, but also introduce complex safety and security challenges that resist conventional risk management. While conventional probabilistic risk analysis (PRA) requires exhaustive risk enumeration and quantification, the novelty and complexity of these systems make PRA impractical, particularly against adaptive adversaries. Previous research found that risk management in various fields of engineering such as nuclear or civil engineering is often solved by generic (i.e. field-agnostic) strategies such as event tree analysis or robust designs. Here we show how emerging risks in LLM-powered systems could be met with 100+ of these non-probabilistic strategies to risk management, including risks from adaptive adversaries. The strategies are divided into five categories and are mapped to LLM security (and AI safety more broadly). We also present an LLM-powered workflow for applying these strategies and other workflows suitable for solution architects. Overall, these strategies could contribute (despite some limitations) to security, safety and other dimensions of responsible AI.', 'abstract_zh': '大型语言模型（LLMs）提供了前所未有的能力和持续增长的能力，但也引入了复杂的安全和安全挑战，这些挑战超越了传统的风险管理方法。虽然传统的概率风险分析（PRA）要求详尽的风险列举和量化，但这些系统的新颖性和复杂性使其在面对适应性对手时变得不切实际。以往的研究发现，工程领域如核工程或土木工程中的风险管理通常通过通用（即领域无关）策略如事件树分析或鲁棒设计来解决。在这里，我们展示了如何利用100多种非概率性的策略来应对LLM驱动系统中的新兴风险，包括来自适应性对手的风险。这些策略被分为五类，并映射到LLM安全（以及更广泛的AI安全性）。我们还介绍了一种LLM驱动的工作流程来应用这些策略以及其他适用于解决方案架构师的工作流程。总体而言，尽管存在一些限制，这些策略仍有可能为负责任的AI的安全性、安全性及其他维度做出贡献。', 'title_zh': '从核安全到大语言模型安全：应用非概率风险管理系统构建安全可靠的基于大语言模型的系统'}
{'arxiv_id': 'arXiv:2505.17082', 'title': 'GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data', 'authors': 'Abderrahman Skiredj, Ferdaous Azhari, Houdaifa Atou, Nouamane Tazi, Ismail Berrada', 'link': 'https://arxiv.org/abs/2505.17082', 'abstract': 'Open-source large language models (LLMs) still marginalise Moroccan Arabic (Darija), forcing practitioners either to bolt on heavyweight Arabic adapters or to sacrifice the very reasoning skills that make LLMs useful. We show that a rigorously quality-over-quantity alignment strategy can surface fluent Darija while safeguarding the backbone s cross-lingual reasoning at a sliver of the usual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6 K and TULU 50 K into Darija, preserve 20 of the English originals, and add mathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on 5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the reasoning-dense TULU portion pushes it to 47.5 with no English regression. Scaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which matches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense, scoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc retains Gemma-27B s strong maths and general-reasoning ability, showing only minimal movement on GSM8K and English benchmarks. The entire model is trained in just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable language technology. We release code, data and checkpoints to spur Darija-centric applications in education, public services and everyday digital interaction.', 'abstract_zh': '开源大规模语言模型（LLMs）仍 marginalises 摒弃了摩洛哥阿拉伯语（Darija），迫使实践者要么安装重量级阿拉伯语适应器，要么牺牲使LLMs有用的推理能力。我们展示了一种以质量优先而非数量的对齐策略可以揭示流畅的Darija同时保护跨语言推理能力，仅需通常计算量的一小部分。我们将三个紧凑的指令集LIMA 1 K、DEITA 6 K和TULU 50 K翻译成Darija，保留20个原始的英语版本，并添加了数学、编程和科学提示。一个针对5 K混合指令进行了LoRA调优的Gemma 3-4B提升了DarijaMMLU分数至42.7；增加TULU部分进一步提升至47.5，无英语回退。将相同的配方扩展至Gemma 3-27B产生GemMaroc-27B，该模型在DarijaMMLU上与Atlas-Chat相当（61.6），并在Darija常识方面表现出色，HellaSwag得分为60.5，而Atlas-Chat得分为48.4。关键的是，GemMaroc保留了Gemma-27B的强大数学和一般推理能力，仅在GSM8K和英语基准上有轻微变化。整个模型仅在48 GPU小时内在进行了训练，突显了一条绿色AI路径，通往包容和可持续的语言技术。我们发布了代码、数据和检查点，以促进以Darija为中心的应用在教育、公共服务和日常数字交互中的发展。', 'title_zh': 'GemMaroc: 用最少的数据解锁达里雅布 proficiency 在大型语言模型中的应用'}
{'arxiv_id': 'arXiv:2505.17078', 'title': 'GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace', 'authors': 'Zenghao Duan, Zhiyi Yin, Zhichao Shi, Liang Pang, Shaoling Jing, Jiayi Wu, Yu Yan, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2505.17078', 'abstract': 'This paper investigates the underlying mechanisms of toxicity generation in Large Language Models (LLMs) and proposes an effective detoxification approach. Prior work typically considers the Feed-Forward Network (FFN) as the main source of toxicity, representing toxic regions as a set of toxic vectors or layer-wise subspaces. However, our in-depth analysis reveals that the global toxic subspace offers a more effective and comprehensive representation of toxic region within the model. Building on this insight, we propose GloSS (Global Toxic Subspace Suppression), a lightweight, four-stage method that mitigates toxicity by identifying and removing the global toxic subspace from the parameters of FFN. Experiments across a range of LLMs show that GloSS achieves state-of-the-art detoxification performance while preserving the models general capabilities, without requiring large-scale data or model retraining.', 'abstract_zh': '本研究探讨了大型语言模型（LLMs）中毒性生成的内在机制，并提出了一种有效的去毒方法。先前的工作通常认为前馈网络（FFN）是毒性的主要来源，将有毒区域表示为一组有毒向量或逐层子空间。然而，我们的深入分析表明，全局有毒子空间提供了模型中更有效和全面的有毒区域表示。基于这一见解，我们提出了一种轻量级的GloSS（全局有毒子空间抑制）方法，通过识别并从FFN参数中移除全局有毒子空间来减轻毒性。实验结果表明，GloSS在保持模型通用能力的同时，实现了最先进的去毒性能，无需大规模数据或模型重新训练。', 'title_zh': 'GloSS 过滤毒性：通过全局毒性子空间理解并缓解大规模语言模型中的毒性'}
{'arxiv_id': 'arXiv:2505.17074', 'title': 'Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency', 'authors': 'Ruixiao Li, Fahao Chen, Peng Li', 'link': 'https://arxiv.org/abs/2505.17074', 'abstract': 'Speculative decoding accelerates Large Language Model (LLM) inference by employing a small speculative model (SSM) to generate multiple candidate tokens and verify them using the LLM in parallel. This technique has been widely integrated into LLM inference serving systems. However, inference requests typically exhibit uncertain execution time, which poses a significant challenge of efficiently scheduling requests in these systems. Existing work estimates execution time based solely on predicted output length, which could be inaccurate because execution time depends on both output length and token acceptance rate of verification by the LLM. In this paper, we propose a semi-clairvoyant request scheduling algorithm called Least-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a number of inference requests, LAPS-SD can effectively minimize average inference latency by adaptively scheduling requests according to their features during decoding. When the token acceptance rate is dynamic and execution time is difficult to estimate, LAPS-SD maintains multiple priority queues and allows request execution preemption across different queues. Once the token acceptance rate becomes stable, LAPS-SD can accurately estimate the execution time and schedule requests accordingly. Extensive experiments show that LAPS-SD reduces inference latency by approximately 39\\% compared to state-of-the-art scheduling methods.', 'abstract_zh': '推测解码加速大型语言模型（LLM）推理过程中的请求调度：一种基于部分先知的服务获取最少未完成/感知服务（LAPS-SD）算法', 'title_zh': '推测性解码请求的半先知调度以最小化大语言模型推理延迟'}
{'arxiv_id': 'arXiv:2505.17073', 'title': 'Mechanistic Interpretability of GPT-like Models on Summarization Tasks', 'authors': 'Anurag Mishra', 'link': 'https://arxiv.org/abs/2505.17073', 'abstract': 'Mechanistic interpretability research seeks to reveal the inner workings of large language models, yet most work focuses on classification or generative tasks rather than summarization. This paper presents an interpretability framework for analyzing how GPT-like models adapt to summarization tasks. We conduct differential analysis between pre-trained and fine-tuned models, quantifying changes in attention patterns and internal activations. By identifying specific layers and attention heads that undergo significant transformation, we locate the "summarization circuit" within the model architecture. Our findings reveal that middle layers (particularly 2, 3, and 5) exhibit the most dramatic changes, with 62% of attention heads showing decreased entropy, indicating a shift toward focused information selection. We demonstrate that targeted LoRA adaptation of these identified circuits achieves significant performance improvement over standard LoRA fine-tuning while requiring fewer training epochs. This work bridges the gap between black-box evaluation and mechanistic understanding, providing insights into how neural networks perform information selection and compression during summarization.', 'abstract_zh': '机制可解释性研究旨在揭示大型语言模型的内部工作机制，然而大多数工作侧重于分类或生成任务，而非摘要任务。本文提出了一种可解释性框架，用于分析类似于GPT的模型如何适应摘要任务。我们进行了预训练模型与微调模型之间的差异分析，量化了注意力模式和内部激活的变化。通过识别显著变化的具体层和注意力头，我们定位了模型架构中的“摘要电路”。研究发现，中间层（尤其是第2、第3和第5层）显示出最 dramatic 变化，62%的注意力头显示出熵降低的趋势，表明向集中信息选择的转变。我们证明，针对识别出的这些电路进行定向LoRA适应，能够在使用更少训练周期的同时，实现比标准LoRA微调更好的性能提升。这项工作填补了黑盒评估与机制性理解之间的差距，提供了关于神经网络在摘要任务中执行信息选择和压缩的洞察。', 'title_zh': 'GPT类模型在摘要任务中的机制可解释性'}
{'arxiv_id': 'arXiv:2505.17072', 'title': 'Safety Alignment Can Be Not Superficial With Explicit Safety Signals', 'authors': 'Jianwei Li, Jung-Eng Kim', 'link': 'https://arxiv.org/abs/2505.17072', 'abstract': 'Recent studies on the safety alignment of large language models (LLMs) have revealed that existing approaches often operate superficially, leaving models vulnerable to various adversarial attacks. Despite their significance, these studies generally fail to offer actionable solutions beyond data augmentation for achieving more robust safety mechanisms. This paper identifies a fundamental cause of this superficiality: existing alignment approaches often presume that models can implicitly learn a safety-related reasoning task during the alignment process, enabling them to refuse harmful requests. However, the learned safety signals are often diluted by other competing objectives, leading models to struggle with drawing a firm safety-conscious decision boundary when confronted with adversarial attacks. Based on this observation, by explicitly introducing a safety-related binary classification task and integrating its signals with our attention and decoding strategies, we eliminate this ambiguity and allow models to respond more responsibly to malicious queries. We emphasize that, with less than 0.2x overhead cost, our approach enables LLMs to assess the safety of both the query and the previously generated tokens at each necessary generating step. Extensive experiments demonstrate that our method significantly improves the resilience of LLMs against various adversarial attacks, offering a promising pathway toward more robust generative AI systems.', 'abstract_zh': '近期对大型语言模型安全性对齐的研究表明，现有方法往往浅层次地运作，使得模型容易受到各种 adversarial 攻击的威胁。尽管这些研究具有重要意义，但它们通常未能提供超越数据增强的可操作解决方案，以实现更为 robust 的安全机制。本文指出了这种浅层次性的一个根本原因：现有对齐方法往往假定模型可以在对齐过程中隐式学习一个与安全性相关的推理任务，从而能够拒绝有害请求。然而，学习到的安全信号往往被其他竞争目标稀释，导致模型在面对 adversarial 攻击时难以划定一个明确的安全意识决策边界。基于这一观察，我们通过明确引入一个与安全性相关的二元分类任务，并将其实现信号与我们的注意力和解码策略集成，消除了这种模糊性，使模型能够更负责任地响应恶意查询。我们强调，我们的方法在不到 0.2 倍的额外开销下，使大型语言模型能够在每次必要的生成步骤中评估查询和之前生成的令牌的安全性。大量实验表明，我们的方法显著提高了大型语言模型对各种 adversarial 攻击的抗性，为更 robust 的生成人工智能系统提供了有希望的途径。', 'title_zh': '安全对齐可以通过显式安全信号而不只是表层地实现。'}
{'arxiv_id': 'arXiv:2505.17066', 'title': 'Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration', 'authors': 'Tatia Tsmindashvili, Ana Kolkhidashvili, Dachi Kurtskhalia, Nino Maghlakelidze, Elene Mekvabishvili, Guram Dentoshvili, Orkhan Shamilov, Zaal Gachechiladze, Steven Saporta, David Dachi Choladze', 'link': 'https://arxiv.org/abs/2505.17066', 'abstract': "Using LLMs in a production environment presents security challenges that include vulnerabilities to jailbreaks and prompt injections, which can result in harmful outputs for humans or the enterprise. The challenge is amplified when working within a specific domain, as topics generally accepted for LLMs to address may be irrelevant to that field. These problems can be mitigated, for example, by fine-tuning large language models with domain-specific and security-focused data. However, these alone are insufficient, as jailbreak techniques evolve. Additionally, API-accessed models do not offer the flexibility needed to tailor behavior to industry-specific objectives, and in-context learning is not always sufficient or reliable. In response to these challenges, we introduce Archias, an expert model adept at distinguishing between in-domain and out-of-domain communications. Archias classifies user inquiries into several categories: in-domain (specifically for the automotive industry), malicious questions, price injections, prompt injections, and out-of-domain examples. Our methodology integrates outputs from the expert model (Archias) into prompts, which are then processed by the LLM to generate responses. This method increases the model's ability to understand the user's intention and give appropriate answers. Archias can be adjusted, fine-tuned, and used for many different purposes due to its small size. Therefore, it can be easily customized to the needs of any industry. To validate our approach, we created a benchmark dataset for the automotive industry. Furthermore, in the interest of advancing research and development, we release our benchmark dataset to the community.", 'abstract_zh': '在生产环境中使用LLM面临的安全挑战包括 Jailbreak 和提示注入等漏洞，这些都可能导致对人类或企业产生有害输出。当在特定领域工作时，这一挑战更加严峻，因为通常接受的LLM处理主题可能与该领域无关。这些问题可以通过使用领域特定和安全导向的数据对大型语言模型进行微调来缓解。然而，这些方法并不足够，因为 Jailbreak 技巧会不断演变。此外，通过API访问的模型无法提供根据行业特定目标定制行为所需的灵活性，而上下文学习也不总能可靠地实现。为应对这些挑战，我们引入了Archias，一种专家模型，擅长区分域内和域外通信。Archias将用户咨询分类为多个类别：特定于汽车行业的域内咨询、恶意问题、价格注入、提示注入和域外示例。我们的方法将专家模型（Archias）的输出整合到提示中，然后由LLM处理生成响应。这种方法增强了模型理解用户意图并给出适当回答的能力。Archias由于其小巧的体积，可以进行调整、微调并用于多种目的，因此可以轻松适应任何行业的特定需求。为了验证我们的方法，我们为汽车行业创建了一个基准数据集。此外，为了促进研究和开发，我们向社区发布了基准数据集。', 'title_zh': '基于专家模型集成提高大语言模型对抗 Jailbreak 攻击的输出质量'}
{'arxiv_id': 'arXiv:2505.17063', 'title': 'Synthetic Data RL: Task Definition Is All You Need', 'authors': 'Yiduo Guo, Zhen Guo, Chuanwei Huang, Zi-Ang Wang, Zekai Zhang, Haofei Yu, Huishuai Zhang, Yikang Shen', 'link': 'https://arxiv.org/abs/2505.17063', 'abstract': 'Reinforcement learning (RL) is a powerful way to adapt foundation models to specialized tasks, but its reliance on large-scale human-labeled data limits broad adoption. We introduce Synthetic Data RL, a simple and general framework that reinforcement fine-tunes models using only synthetic data generated from a task definition. Our method first generates question and answer pairs from the task definition and retrieved documents, then adapts the difficulty of the question based on model solvability, and selects questions using the average pass rate of the model across samples for RL training. On Qwen-2.5-7B, our method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9 pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA (finance). It surpasses supervised fine-tuning under the same data budget and nearly matches RL with full human data across datasets (e.g., +17.2 pp on GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only by 0.4 pp, showing a limited added value. By reducing human data annotation, Synthetic Data RL enables scalable and efficient RL-based model adaptation. Code and demos are available at this https URL.', 'abstract_zh': '合成数据强化学习：仅使用合成数据对基础模型进行强化微调的简单通用框架', 'title_zh': '合成数据RL：任务定义即一切'}
{'arxiv_id': 'arXiv:2505.17061', 'title': 'Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models', 'authors': 'Xinlong Chen, Yuanxing Zhang, Qiang Liu, Junfei Wu, Fuzheng Zhang, Tieniu Tan', 'link': 'https://arxiv.org/abs/2505.17061', 'abstract': "Large Vision-Language Models (LVLMs) have exhibited impressive capabilities across various visual tasks, yet they remain hindered by the persistent challenge of hallucinations. To address this critical issue, we propose Mixture of Decoding (MoD), a novel approach for hallucination mitigation that dynamically adapts decoding strategies by evaluating the correctness of the model's attention on image tokens. Specifically, MoD measures the consistency between outputs generated from the original image tokens and those derived from the model's attended image tokens, to distinguish the correctness aforementioned. If the outputs are consistent, indicating correct attention, MoD employs a complementary strategy to amplify critical information. Conversely, if the outputs are inconsistent, suggesting erroneous attention, MoD utilizes a contrastive strategy to suppress misleading information. Extensive experiments demonstrate that MoD significantly outperforms existing decoding methods across multiple mainstream benchmarks, effectively mitigating hallucinations in LVLMs. The code is available at this https URL.", 'abstract_zh': 'Large 视觉-语言 模型 (LVLMs) 在各种视觉任务中展示了令人印象深刻的性能，但仍受到幻觉持续挑战的困扰。为应对这一关键问题，我们提出了一种名为 Mixture of Decoding (MoD) 的新颖方法，该方法通过评估模型对图像标记的注意力的正确性来动态调整解码策略。具体而言，MoD 通过比较源于原始图像标记和模型关注图像标记生成的输出之间的一致性来区分上述正确性。如果输出一致，表明注意力正确，则 MoD 采用补充策略加强关键信息。反之，如果输出不一致，表明注意力错误，则 MoD 采用对比策略抑制误导性信息。广泛实验表明，MoD 在多个主流基准上显著优于现有解码方法，有效减轻了 LVLMs 中的幻觉问题。代码可供访问：this https URL。', 'title_zh': '混合解码：一种基于注意力的自适应解码策略，用于缓解大规模视觉-语言模型中的幻觉'}
{'arxiv_id': 'arXiv:2505.17060', 'title': 'SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation', 'authors': 'Wenyi Yu, Siyin Wang, Xiaoyu Yang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang', 'link': 'https://arxiv.org/abs/2505.17060', 'abstract': 'In order to enable fluid and natural human-machine speech interaction, existing full-duplex conversational systems often adopt modular architectures with auxiliary components such as voice activity detectors, interrupters, conversation state predictors, or multiple LLMs. These systems, however, suffer from error accumulation across modules and struggle with key challenges such as context-dependent barge-in and echo cancellation. Recent approaches, most notably Moshi, simplify the pipeline by injecting audio codecs into the token space of a single LLM. However, such methods still incur significant performance degradation when operating on the speech rather than text modality. In this paper, we introduce SALMONN-omni, the first single, standalone full-duplex speech LLM that operates without audio codecs in its token space. It features a novel dynamic thinking mechanism within the LLM backbone, enabling the model to learn when to transition between speaking and listening states. Experiments on widely used benchmarks for spoken question answering and open-domain dialogue show that SALMONN-omni achieves at least 30\\% relative performance improvement over existing open-source full-duplex models and performs highly competitively to half-duplex and turn-based systems, despite using substantially less training data. Moreover, SALMONN-omni demonstrates strong performance in complex conversational scenarios, including turn-taking, backchanneling, echo cancellation and context-dependent barge-in, with further improvements achieved through reinforcement learning. Some demo conversations between user and SALMONN-omni are provided in the following repository this https URL.', 'abstract_zh': '为了实现流畅自然的人机语音交互，现有的全双工对话系统通常采用具有辅助组件（如语音活动检测器、打断器、对话状态预测器或多个LLM）的模块化架构。然而，这些系统在跨模块过程中容易积累误差，并且难以应对诸如上下文依赖的打断和回声消除等关键挑战。最近的方法，尤其是Moshi，通过将音频编解码器注入单个LLM的标记空间来简化流水线。但是，这些方法在处理语音而非文本模态时仍然会遭受显著的性能下降。在本文中，我们引入了SALMONN-omni，这是第一个无需在标记空间中使用音频编解码器的独立单体全双工语音LLM。它配备了LLM骨干网络中的新颖动态思维机制，使模型能够学习在讲话和倾听状态之间转换的时机。在广泛使用的语音问答和开放领域对话基准上的实验表明，SALMONN-omni在现有的开源全双工模型上的性能至少提高了30%，并且在使用较少训练数据的情况下与半双工和轮询系统具有高度竞争力。此外，SALMONN-omni在复杂的对话场景中表现出了强大的性能，包括对话轮换、副语言、回声消除和上下文依赖的打断，进一步的性能提升通过强化学习实现。有关用户与SALMONN-omni的演示对话可在以下仓库中找到：[此链接]。', 'title_zh': 'SALMONN-omni：一种无需编解码器注入的独立语音LLM全双工对话系统'}
{'arxiv_id': 'arXiv:2505.17059', 'title': 'Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large', 'authors': 'Van-Tinh Nguyen, Hoang-Duong Pham, Thanh-Hai To, Cong-Tuan Hung Do, Thi-Thu-Trang Dong, Vu-Trung Duong Le, Van-Phuc Hoang', 'link': 'https://arxiv.org/abs/2505.17059', 'abstract': "Understanding medical texts presents significant challenges due to complex terminology and context-specific language. This paper introduces Medalyze, an AI-powered application designed to enhance the comprehension of medical texts using three specialized FLAN-T5-Large models. These models are fine-tuned for (1) summarizing medical reports, (2) extracting health issues from patient-doctor conversations, and (3) identifying the key question in a passage. Medalyze is deployed across a web and mobile platform with real-time inference, leveraging scalable API and YugabyteDB. Experimental evaluations demonstrate the system's superior summarization performance over GPT-4 in domain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and SpaCy Similarity. Medalyze provides a practical, privacy-preserving, and lightweight solution for improving information accessibility in healthcare.", 'abstract_zh': '医疗文本的理解面临着因复杂术语和情境特定语言而带来的显著挑战。本文介绍了Medalyze，一种使用三种专门Fine-Tuned FLAN-T5-Large模型的AI驱动应用，旨在通过这些模型增强对医疗文本的 comprehension。Medalyze 在 Web 和移动平台上实时部署，利用可扩展的 API 和 YugabyteDB。实验评估表明，与 GPT-4 相比，该系统在领域特定任务中的总结性能更优，基于 BLEU、ROUGE-L、BERTScore 和 SpaCy 相似性等指标。Medalyze 提供了一种实用、保护隐私且轻量级的解决方案，以提高医疗领域的信息 accessibility。', 'title_zh': 'Medalyze: 使用FLAN-T5-Large的轻量级医学报告总结应用'}
{'arxiv_id': 'arXiv:2505.17056', 'title': 'Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective', 'authors': 'Luoxi Tang, Tharunya Sundar, Shuai Yang, Ankita Patra, Manohar Chippada, Giqi Zhao, Yi Li, Riteng Zhang, Tunan Zhao, Ting Yang, Yuqiao Meng, Weicheng Ma, Zhaohan Xi', 'link': 'https://arxiv.org/abs/2505.17056', 'abstract': 'AI is transforming education by enabling powerful tools that enhance learning experiences. Among recent advancements, large language models (LLMs) hold particular promise for revolutionizing how learners interact with educational content. In this work, we investigate the potential of LLMs to support standardized test preparation by focusing on English Standardized Tests (ESTs). Specifically, we assess their ability to generate accurate and contextually appropriate solutions across a diverse set of EST question types. We introduce ESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of LLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests, encompassing 29 question types and over 10,576 questions across multiple modalities, including text, images, audio, tables, and mathematical symbols. Using ESTBOOK, we systematically evaluate both the accuracy and inference efficiency of LLMs. Additionally, we propose a breakdown analysis framework that decomposes complex EST questions into task-specific solution steps. This framework allows us to isolate and assess LLM performance at each stage of the reasoning process. Evaluation findings offer insights into the capability of LLMs in educational contexts and point toward targeted strategies for improving their reliability as intelligent tutoring systems.', 'abstract_zh': 'AI正在通过启用强大的工具来革新教育，这些工具能够提升学习体验。近年来，大型语言模型（LLMs）特别有潜力从根本上变革学习者与教育资源的互动方式。在本文中，我们探讨了LLMs在支持标准化考试准备方面的潜力，重点关注英语标准化考试（ESTs）。具体而言，我们评估了它们生成准确且上下文适当解决方案的能力，涉及多样化的EST题型。我们引入了ESTBOOK，这是一个全面的基准测试，旨在评估LLMs解决EST问题的能力。ESTBOOK整合了五种广泛认可的测试，涵盖29种题型和超过10,576道问题，涉及多种模态，包括文本、图像、音频、表格和数学符号。通过ESTBOOK，我们系统地评估了LLMs的准确性和推理效率。此外，我们提出了一种分解分析框架，将复杂的EST问题分解为特定任务的解决步骤。该框架使我们能够隔离并评估LLMs在推理过程的每个阶段的表现。评估结果提供了关于LLMs在教育情境下的能力洞察，并指出了提高其作为智能辅导系统的可靠性的策略。', 'title_zh': 'LLM在英语标准化测试中准备就绪了吗？一项基准测试与需求捕获视角分析'}
{'arxiv_id': 'arXiv:2505.17053', 'title': 'Social preferences with unstable interactive reasoning: Large language models in economic trust games', 'authors': 'Ou Jiamin, Eikmans Emile, Buskens Vincent, Pankowska Paulina, Shan Yuli', 'link': 'https://arxiv.org/abs/2505.17053', 'abstract': 'While large language models (LLMs) have demonstrated remarkable capabilities in understanding human languages, this study explores how they translate this understanding into social exchange contexts that capture certain essences of real world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were placed in economic trust games where players balance self-interest with trust and reciprocity, making decisions that reveal their social preferences and interactive reasoning abilities. Our study shows that LLMs deviate from pure self-interest and exhibit trust and reciprocity even without being prompted to adopt a specific persona. In the simplest one-shot interaction, LLMs emulated how human players place trust at the beginning of such a game. Larger human-machine divergences emerged in scenarios involving trust repayment or multi-round interactions, where decisions were influenced by both social preferences and interactive reasoning. LLMs responses varied significantly when prompted to adopt personas like selfish or unselfish players, with the impact outweighing differences between models or game types. Response of ChatGPT-4, in an unselfish or neutral persona, resembled the highest trust and reciprocity, surpassing humans, Claude, and Bard. Claude and Bard displayed trust and reciprocity levels that sometimes exceeded and sometimes fell below human choices. When given selfish personas, all LLMs showed lower trust and reciprocity than humans. Interactive reasoning to the actions of counterparts or changing game mechanics appeared to be random rather than stable, reproducible characteristics in the response of LLMs, though some improvements were observed when ChatGPT-4 responded in selfish or unselfish personas.', 'abstract_zh': '大型语言模型在社会交换情境中展现的信任与互惠能力探究', 'title_zh': '社会偏好与不稳定互动推理：大规模语言模型在经济信任游戏中的表现'}
{'arxiv_id': 'arXiv:2505.17052', 'title': 'SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs', 'authors': 'Jinwoo Park, Seunggeun Cho, Dongsu Han', 'link': 'https://arxiv.org/abs/2505.17052', 'abstract': 'Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. Experiments show SpecEdge enhances overall cost efficiency by 1.91x through achieving 2.22x server throughput, and reduces inter token latency by 11.24% compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving.', 'abstract_zh': 'Large Language Models (LLMs) 助力许多现代应用，但大规模提供这些模型依然成本高昂且资源密集。当前以服务器为中心的系统忽略了边缘处的消费级 GPU。我们引入了 SpecEdge，这是一种边缘辅助推理框架，通过投机性的解码方案将 LLM 工作负载分拆到边缘和服务器 GPU 之间，仅在网络中交换标记输出。SpecEdge 使用积极的边缘预选拔方案，使边缘标记创建与服务器验证重叠，并采用Aware流水线调度方案交错多个用户请求以提高服务器端吞吐量。实验表明，SpecEdge 通过实现 2.22 倍的服务器吞吐量，将总体成本效率提升 1.91 倍，并将标记间延迟减少了 11.24%，相较于仅服务器的基础方案，引入了一种可扩展且成本效益高的 LLM 提供范式。', 'title_zh': 'SpecEdge: 可扩展的边缘辅助服务框架以支持交互式大语言模型'}
{'arxiv_id': 'arXiv:2505.17051', 'title': 'Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models', 'authors': 'Bernd Huber, Ghazal Fazelnia, Andreas Damianou, Sebastian Peleato, Max Lefarov, Praveen Ravichandran, Marco De Nadai, Mounia Lalmas-Roellke, Paul N. Bennett', 'link': 'https://arxiv.org/abs/2505.17051', 'abstract': "Large language models (LLMs) excel at generating contextually relevant content. However, tailoring these outputs to individual users for effective personalization is a significant challenge. While rich user-specific information often exists as pre-existing user representations, such as embeddings learned from preferences or behaviors, current methods to leverage these for LLM personalization typically require costly fine-tuning or token-heavy prompting. We propose Embedding-to-Prefix (E2P), a parameter-efficient method that injects pre-computed context embeddings into an LLM's hidden representation space through a learned projection to a single soft token prefix. This enables effective personalization while keeping the backbone model frozen and avoiding expensive adaptation techniques. We evaluate E2P across two public datasets and in a production setting: dialogue personalization on Persona-Chat, contextual headline generation on PENS, and large-scale personalization for music and podcast consumption. Results show that E2P preserves contextual signals and achieves strong performance with minimal computational overhead, offering a scalable, efficient solution for contextualizing generative AI systems.", 'abstract_zh': '大规模语言模型（LLMs）在生成上下文相关内容方面表现出色。然而，为了有效进行个性化调整，将这些输出针对个别用户进行定制仍是一个重大挑战。虽然丰富的用户特定信息通常以预训练的用户表示形式存在，比如偏好或行为学到的嵌入，当前用于利用这些信息进行LLM个性化的方法通常需要成本高昂的微调或大量提示。我们提出了一种参数高效的Embedding-to-Prefix（E2P）方法，通过学习投影将预先计算的上下文嵌入注入到LLM的隐藏表示空间中的单个软令牌前缀中。这使得在冻结主模型的情况下，能够有效进行个性化调整，并避免昂贵的适应技术。我们在两个公开数据集和实际生产环境中评估了E2P：Persona-Chat中的对话个性化、PENS中的上下文标题生成以及音乐和播客的大型个性化。结果表明，E2P能够保留上下文信号，并在最小的计算开销下实现出色的性能，提供了一种可扩展且高效的为生成型AI系统上下文化的方法。', 'title_zh': 'Embedding-to-Prefix: 参数高效的预训练大型语言模型个性化方法'}
{'arxiv_id': 'arXiv:2505.17050', 'title': 'Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning', 'authors': 'Yanhao Jia, Xinyi Wu, Qinglin Zhang, Yiran Qin, Luwei Xiao, Shuai Zhao', 'link': 'https://arxiv.org/abs/2505.17050', 'abstract': 'Project-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.', 'abstract_zh': '基于项目的学习（PBL）涉及多种高度相关的多模态数据，是STEM学科中一种重要的教育方法。随着多模态大语言模型（MLLMs）的迅速发展，研究人员开始探索它们在信息检索、知识理解、数据生成等教育任务中的潜力。然而，现有的基准测试在提供开放式输出结构和严格的人类专家验证过程方面存在不足，限制了它们在评估实际教育任务方面的有效性。此外，很少有方法开发出自动化的流程来协助教师利用MLLMs，主要由于模型的幻觉和不稳定性的原因，导致实现不可靠。为填补这一空白，我们介绍了PBLBench，这是一种新型基准测试，旨在评估基于领域特定知识和长上下文理解的复杂推理，从而通过任务挑战模型，这些任务与人类专家处理的任务极为相似。为建立可靠的基准事实，我们采用了层次分析过程（AHP），利用专家驱动的两两比较来推导出结构化和加权的评估标准。我们使用PBLBench评估了15个领先的MLLMs/LLMs，并展示了即使是最先进的模型也只能达到59%的排名准确率，突显了该基准测试带来的巨大挑战。我们相信，PBLBench将为更强大的人工智能代理的开发起到催化作用，最终旨在减轻教师的工作负担并提高教育生产率。', 'title_zh': '面向STEM教育稳健评估的方法：基于项目学习的大型语言模型应用'}
{'arxiv_id': 'arXiv:2505.17049', 'title': 'Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations', 'authors': 'David Rozado', 'link': 'https://arxiv.org/abs/2505.17049', 'abstract': 'This study examines the behavior of Large Language Models (LLMs) when evaluating professional candidates based on their resumes or curricula vitae (CVs). In an experiment involving 22 leading LLMs, each model was systematically given one job description along with a pair of profession-matched CVs, one bearing a male first name, the other a female first name, and asked to select the more suitable candidate for the job. Each CV pair was presented twice, with names swapped to ensure that any observed preferences in candidate selection stemmed from gendered names cues. Despite identical professional qualifications across genders, all LLMs consistently favored female-named candidates across 70 different professions. Adding an explicit gender field (male/female) to the CVs further increased the preference for female applicants. When gendered names were replaced with gender-neutral identifiers "Candidate A" and "Candidate B", several models displayed a preference to select "Candidate A". Counterbalancing gender assignment between these gender-neutral identifiers resulted in gender parity in candidate selection. When asked to rate CVs in isolation rather than compare pairs, LLMs assigned slightly higher average scores to female CVs overall, but the effect size was negligible. Including preferred pronouns (he/him or she/her) next to a candidate\'s name slightly increased the odds of the candidate being selected regardless of gender. Finally, most models exhibited a substantial positional bias to select the candidate listed first in the prompt. These findings underscore the need for caution when deploying LLMs in high-stakes autonomous decision-making contexts and raise doubts about whether LLMs consistently apply principled reasoning.', 'abstract_zh': '本研究考察了大型语言模型（LLMs）在基于简历或 curriculum vitae (CV) 评估专业候选人时的行为。在涉及22个领先LLM的实验中，每个模型系统地获得了职位描述，并与一对匹配职业的CV组合，其中一个带有男性名字，另一个带有女性名字，然后被要求选出更适合该职位的候选人。每对CV呈递两次，名字位置交换以确保观察到的候选人选择偏好源自性别名字线索。尽管不同性别有相同的专业资格，所有LLM在70个不同职业中一致地倾向于选择带有女性名字的候选人。在CV中增加明确的性别字段（男性/女性）进一步增加了对女性申请者的偏好。将性别名字替换为性别中立标识符“Candidate A”和“Candidate B”后，几款模型更倾向于选择“Candidate A”。在这些性别中立标识符之间平衡性别分配导致了候选人选择的性别平衡。当单独评价CV而不是成对比较时，LLM整体上为女性CV分配了略高的平均评分，但效应大小很小。在候选人的名字旁边列出偏好的称谓（他/他或她/她）略微增加了候选人被选中的几率，不论其性别。最后，大多数模型在选择列表中第一个提及的候选人时表现出明显的偏好。这些发现强调了在高风险自主决策背景下部署LLM时需要谨慎，并对LLM是否一贯应用原则性推理表示怀疑。', 'title_zh': '基于LLM的招聘决策中的性别和职位偏见：来自简历评估的证据'}
{'arxiv_id': 'arXiv:2505.17047', 'title': 'Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe', 'authors': 'Erin Palm, Astrit Manikantan, Mark E. Pepin, Herprit Mahal, Srikanth Subramanya Belwadi', 'link': 'https://arxiv.org/abs/2505.17047', 'abstract': 'In medical practices across the United States, physicians have begun implementing generative artificial intelligence (AI) tools to perform the function of scribes in order to reduce the burden of documenting clinical encounters. Despite their widespread use, no established methods exist to gauge the quality of AI scribes. To address this gap, we developed a blinded study comparing the relative performance of large language model (LLM) generated clinical notes with those from field experts based on audio-recorded clinical encounters. Quantitative metrics from the Physician Documentation Quality Instrument (PDQI9) provided a framework to measure note quality, which we adapted to assess relative performance of AI generated notes. Clinical experts spanning 5 medical specialties used the PDQI9 tool to evaluate specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators from each specialty scored notes drafted from a total of 97 patient visits. We found uniformly high inter rater agreement (RWG greater than 0.7) between evaluators in general medicine, orthopedics, and obstetrics and gynecology, and moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and cardiology. We found a modest yet significant difference in the overall note quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9 instrument as a practical method to gauge the quality of LLM authored notes, as compared to human-authored notes.', 'abstract_zh': '在美国医疗实践中，医生开始使用生成型人工智能工具来担任记录临床 encounter 的角色，以减轻文档记录负担。尽管这些工具被广泛应用，但目前缺乏评估人工智能记录员质量的方法。为填补这一空白，我们开发了一项盲测研究，比较大型语言模型生成的临床笔记与基于音频记录的临床 encounter 的专家笔记的质量。我们使用医生记录质量评估工具（PDQI9）中的定量指标作为测量框架，并据此评估人工智能生成笔记的相对性能。涵盖五个医学专科的临床专家使用 PDQI9 工具评估专科起草的金笔记和大型语言模型撰写的环境笔记。每种专科的两名评估者对总共97个病人访问记录的笔记进行了评分。我们发现，在一般医学、骨科和妇产科中，评估者之间的一致性极高（RWG大于0.7），而在儿科和心脏病学中，一致性为中等（RWG 0.5到0.7）至高。我们发现整体笔记质量存在微小但显著的差异，其中金笔记得分为4.25分（满分5分），环境笔记得分为4.20分（p=0.04）。我们的研究结果支持使用 PDQI9 作为评估大型语言模型生成笔记质量的实用方法，相较于人类手写的笔记。', 'title_zh': '评估AI生成临床笔记的质量：大型语言模型记录员的验证评价'}
