{'arxiv_id': 'arXiv:2511.18966', 'title': 'LLM-CSEC: Empirical Evaluation of Security in C/C++ Code Generated by Large Language Models', 'authors': 'Muhammad Usman Shahid, Chuadhry Mujeeb Ahmed, Rajiv Ranjan', 'link': 'https://arxiv.org/abs/2511.18966', 'abstract': 'The security of code generated by large language models (LLMs) is a significant concern, as studies indicate that such code often contains vulnerabilities and lacks essential defensive programming constructs. This work focuses on examining and evaluating the security of LLM-generated code, particularly in the context of C/C++. We categorized known vulnerabilities using the Common Weakness Enumeration (CWE) and, to study their criticality, mapped them to CVEs. We used ten different LLMs for code generation and analyzed the outputs through static analysis. The amount of CWEs present in AI-generated code is concerning. Our findings highlight the need for developers to be cautious when using LLM-generated code. This study provides valuable insights to advance automated code generation and encourage further research in this domain.', 'abstract_zh': '大型语言模型生成代码的安全性是一个重要关切，因为研究表明此类代码往往包含漏洞且缺乏必要的防御编程构造。本研究专注于检查和评估大型语言模型生成代码的安全性，尤其是在C/C++的背景下。我们使用通用弱点枚举（CWE）对已知的漏洞进行了分类，并通过将它们映射到CVE来研究它们的严重程度。我们使用了十种不同的大型语言模型进行代码生成，并通过静态分析来分析输出结果。AI生成代码中存在的CWE数量令人担忧。本研究的发现强调了开发人员在使用大型语言模型生成代码时需保持谨慎。本研究提供了有价值的见解，有助于推进自动化代码生成，并鼓励在此领域的进一步研究。', 'title_zh': 'LLM-CSEC: 大型语言模型生成的C/C++代码的安全性 empirical评估'}
{'arxiv_id': 'arXiv:2511.18793', 'title': 'NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations', 'authors': 'Yejing Wang, Shengyu Zhou, Jinyu Lu, Ziwei Liu, Langming Liu, Maolin Wang, Wenlin Zhang, Feng Li, Wenbo Su, Pengjie Wang, Jian Xu, Xiangyu Zhao', 'link': 'https://arxiv.org/abs/2511.18793', 'abstract': 'Generative Recommendation (GR), powered by Large Language Models (LLMs), represents a promising new paradigm for industrial recommender systems. However, their practical application is severely hindered by high inference latency, which makes them infeasible for high-throughput, real-time services and limits their overall business impact. While Speculative Decoding (SD) has been proposed to accelerate the autoregressive generation process, existing implementations introduce new bottlenecks: they typically require separate draft models and model-based verifiers, requiring additional training and increasing the latency overhead. In this paper, we address these challenges with NEZHA, a novel architecture that achieves hyperspeed decoding for GR systems without sacrificing recommendation quality. Specifically, NEZHA integrates a nimble autoregressive draft head directly into the primary model, enabling efficient self-drafting. This design, combined with a specialized input prompt structure, preserves the integrity of sequence-to-sequence generation. Furthermore, to tackle the critical problem of hallucination, a major source of performance degradation, we introduce an efficient, model-free verifier based on a hash set. We demonstrate the effectiveness of NEZHA through extensive experiments on public datasets and have successfully deployed the system on Taobao since October 2025, driving the billion-level advertising revenue and serving hundreds of millions of daily active users.', 'abstract_zh': '由大型语言模型驱动的生成推荐（GR）代表了工业推荐系统的新有前景的范式。然而，其实用应用严重受制于高额推理延迟，这使得它们不适合高 throughput 的实时服务，并限制了其整体商业影响。虽然推测解码（SD）已被提出以加速自回归生成过程，但现有的实现引入了新的瓶颈：它们通常需要单独的草稿模型和基于模型的验证器，需要额外训练并增加延迟开销。本文中，我们通过NEZHA这一新颖架构解决了这些挑战，NEZHA能够在不牺牲推荐质量的情况下实现超高速解码。具体而言，NEZHA将灵活的自回归草稿头直接集成到主模型中，从而实现高效的自我草稿。该设计结合了专门的输入提示结构，保持了序列到序列生成的完整性。此外，为了应对削弱性能的关键问题——幻觉，我们引入了一种基于哈希集的高效无模型验证器。我们通过在公共数据集上的广泛实验验证了NEZHA的有效性，并已于2025年10月成功部署该系统于淘宝，推动了十亿级的广告收入，并服务于数亿每日活跃用户。', 'title_zh': 'NEZHA：一种零牺牲和超高速解码架构的生成推荐系统'}
{'arxiv_id': 'arXiv:2511.18760', 'title': 'HERMES: Towards Efficient and Verifiable Mathematical Reasoning in LLMs', 'authors': 'Azim Ospanov, Zijin Feng, Jiacheng Sun, Haoli Bai, Xin Shen, Farzan Farnia', 'link': 'https://arxiv.org/abs/2511.18760', 'abstract': "Informal mathematics has been central to modern large language model (LLM) reasoning, offering flexibility and enabling efficient construction of arguments. However, purely informal reasoning is prone to logical gaps and subtle errors that are difficult to detect and correct. In contrast, formal theorem proving provides rigorous, verifiable mathematical reasoning, where each inference step is checked by a trusted compiler in systems such as Lean, but lacks the exploratory freedom of informal problem solving. This mismatch leaves current LLM-based math agents without a principled way to combine the strengths of both paradigms. In this work, we introduce Hermes, the first tool-assisted agent that explicitly interleaves informal reasoning with formally verified proof steps in Lean. The framework performs intermediate formal checking to prevent reasoning drift and employs a memory module that maintains proof continuity across long, multi-step reasoning chains, enabling both exploration and verification within a single workflow. We evaluate Hermes on four challenging mathematical reasoning benchmarks using LLMs of varying parameter scales, from small models to state-of-the-art systems. Across all settings, Hermes reliably improves the reasoning accuracy of base models while substantially reducing token usage and computational cost compared to reward-based approaches. On difficult datasets such as AIME'25, Hermes achieves up to a 67% accuracy improvement while using 80% fewer total inference FLOPs. The implementation and codebase are publicly available at this https URL.", 'abstract_zh': 'Hermes：一种将非形式化推理与形式化验证证明步骤相结合的工具辅助代理', 'title_zh': 'HERMES：面向高效可验证的LLM数学推理'}
{'arxiv_id': 'arXiv:2511.18715', 'title': 'HuggingR$^{4}$: A Progressive Reasoning Framework for Discovering Optimal Model Companions', 'authors': 'Shaoyin Ma, Jie Song, Huiqiong Wang, Li Sun, Mingli Song', 'link': 'https://arxiv.org/abs/2511.18715', 'abstract': 'Large Language Models (LLMs) have made remarkable progress in their ability to interact with external interfaces. Selecting reasonable external interfaces has thus become a crucial step in constructing LLM agents. In contrast to invoking API tools, directly calling AI models across different modalities from the community (e.g., HuggingFace) poses challenges due to the vast scale (> 10k), metadata gaps, and unstructured descriptions. Current methods for model selection often involve incorporating entire model descriptions into prompts, resulting in prompt bloat, wastage of tokens and limited scalability. To address these issues, we propose HuggingR$^4$, a novel framework that combines Reasoning, Retrieval, Refinement, and Reflection, to efficiently select models. Specifically, We first perform multiple rounds of reasoning and retrieval to get a coarse list of candidate models. Then, we conduct fine-grained refinement by analyzing candidate model descriptions, followed by reflection to assess results and determine if retrieval scope expansion is necessary. This method reduces token consumption considerably by decoupling user query processing from complex model description handling. Through a pre-established vector database, complex model descriptions are stored externally and retrieved on-demand, allowing the LLM to concentrate on interpreting user intent while accessing only relevant candidate models without prompt bloat. In the absence of standardized benchmarks, we construct a multimodal human-annotated dataset comprising 14,399 user requests across 37 tasks and conduct a thorough evaluation. HuggingR$^4$ attains a workability rate of 92.03% and a reasonability rate of 82.46%, surpassing existing method by 26.51% and 33.25% respectively on GPT-4o-mini.', 'abstract_zh': '大型语言模型（LLMs）在与外部接口交互方面取得了显著进展。选择合理的外部接口已成为构建LLM代理的关键步骤。与调用API工具不同，直接从社区（例如HuggingFace）跨模态调用AI模型面临着规模庞大（超过10k）、元数据缺口和不结构化描述的挑战。当前的模型选择方法通常将整个模型描述融入提示中，导致提示膨胀、令牌浪费和扩展性有限。为了解决这些问题，我们提出了一个名为HuggingR$^4$的新型框架，该框架结合了推理、检索、细化和反思，以高效地选择模型。具体来说，我们首先进行多轮推理和检索以获得候选模型的粗略列表。然后，我们通过分析候选模型描述进行详细的细化，接着进行反思以评估结果并确定是否需要扩展检索范围。该方法通过将用户查询处理与复杂模型描述处理脱钩，显著减少了令牌消耗。通过预先建立的向量数据库，复杂模型描述被存储在外部并在需要时检索，从而使LLM能够集中于理解用户意图，同时仅访问相关候选模型而不导致提示膨胀。由于缺乏标准化基准，我们构建了一个包含14,399个用户请求和37个任务的多模态人工标注数据集，并进行了一次全面评估。HuggingR$^4$在GPT-4o-mini上的可行率为92.03%，合理性率为82.46%，分别超越现有方法26.51%和33.25%。', 'title_zh': '拥抱R$^{4}$：一种渐进推理框架用于发现最优模型同伴'}
{'arxiv_id': 'arXiv:2511.18450', 'title': 'ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints', 'authors': 'Rui Xu, Dakuan Lu, Zicheng Zhao, Xiaoyu Tan, Xintao Wang, Siyu Yuan, Jiangjie Chen, Yinghui Xu', 'link': 'https://arxiv.org/abs/2511.18450', 'abstract': 'Spatial reasoning is a key capability in the field of artificial intelligence, especially crucial in areas such as robotics, computer vision, and natural language understanding. However, evaluating the ability of multimodal large language models(MLLMs) in complex spatial reasoning still faces challenges, particularly in scenarios requiring multi-step reasoning and precise mathematical constraints. This paper introduces ORIGAMISPACE, a new dataset and benchmark designed to evaluate the multi-step spatial reasoning ability and the capacity to handle mathematical constraints of MLLMs through origami tasks. The dataset contains 350 data instances,each comprising a strictly formatted crease pattern (CP diagram), the Compiled Flat Pattern, the complete Folding Process, and the final Folded Shape Image. We propose four evaluation tasks: Pattern Prediction, Multi-step Spatial Reasoning, Spatial Relationship Prediction, and End-to-End CP Code Generation. For the CP code generation task, we design an interactive environment and explore the possibility of using reinforcement learning methods to train MLLMs. Through experiments on existing MLLMs, we initially reveal the strengths and weaknesses of these models in handling complex spatial reasoning tasks.', 'abstract_zh': '空间推理是人工智能领域的一项关键能力，尤其在机器人技术、计算机视觉和自然语言理解等领域至关重要。然而，评估多模态大规模语言模型（MLLMs）在复杂空间推理任务中的能力，特别是在需要多步推理和精确数学约束的情景中，仍然面临挑战。本文介绍了ORIGAMISPACE，一个全新的数据集和基准，用于评估MLLMs在折纸任务中的多步空间推理能力和处理数学约束的能力。该数据集包含350个数据实例，每个实例包括严格格式化的折痕图案（折纸图）、编译平展图案、完整的折叠过程和最终折叠形状图像。我们提出了四项评估任务：折痕图案预测、多步空间推理、空间关系预测和端到端折痕代码生成。对于折痕代码生成任务，我们设计了一个交互环境，并探索使用强化学习方法训练MLLMs的可能性。通过现有MLLMs的实验，我们初步揭示了这些模型在处理复杂空间推理任务时的优势和不足。', 'title_zh': 'ORIGAMISPACE：多模态LLM在具有数学约束的多步空间推理中的基准测试'}
{'arxiv_id': 'arXiv:2511.18405', 'title': 'A Multimodal Conversational Agent for Tabular Data Analysis', 'authors': 'Mohammad Nour Al Awad, Sergey Ivanov, Olga Tikhonova, Ivan Khodnenko', 'link': 'https://arxiv.org/abs/2511.18405', 'abstract': 'Large language models (LLMs) can reshape information processing by handling data analysis, visualization, and interpretation in an interactive, context-aware dialogue with users, including voice interaction, while maintaining high performance. In this article, we present Talk2Data, a multimodal LLM-driven conversational agent for intuitive data exploration. The system lets users query datasets with voice or text instructions and receive answers as plots, tables, statistics, or spoken explanations. Built on LLMs, the suggested design combines OpenAI Whisper automatic speech recognition (ASR) system, Qwen-coder code generation LLM/model, custom sandboxed execution tools, and Coqui library for text-to-speech (TTS) within an agentic orchestration loop. Unlike text-only analysis tools, it adapts responses across modalities and supports multi-turn dialogues grounded in dataset context. In an evaluation of 48 tasks on three datasets, our prototype achieved 95.8% accuracy with model-only generation time under 1.7 seconds (excluding ASR and execution time). A comparison across five LLM sizes (1.5B-32B) revealed accuracy-latency-cost trade-offs, with a 7B model providing the best balance for interactive use. By routing between conversation with user and code execution, constrained to a transparent sandbox, with simultaneously grounding prompts in schema-level context, the Talk2Data agent reliably retrieves actionable insights from tables while making computations verifiable. In the article, except for the Talk2Data agent itself, we discuss implications for human-data interaction, trust in LLM-driven analytics, and future extensions toward large-scale multimodal assistants.', 'abstract_zh': '大型语言模型（LLMs）可以通过在交互式、情境感知的对话中处理数据的分析、可视化和解释来重塑信息处理，同时保持高性能。本文介绍了Talk2Data，一个基于LLM的多模态会话代理，用于直观的数据探索。该系统允许用户使用语音或文本指令查询数据集，并接收以图表、表格、统计信息或语音解释形式的答案。基于LLM的设计，该系统结合了OpenAI Whisper自动语音识别（ASR）系统、Qwen-coder代码生成LLM/模型、定制的沙箱执行工具以及Coqui库中的文本转语音（TTS）技术，处于一个代理协调循环中。与仅基于文本的分析工具不同，它能够跨模态适应响应，并支持基于数据集的上下文进行多轮对话。在三项数据集上的48项任务评估中，我们的原型实现了95.8%的准确性，仅模型生成时间在1.7秒以内（不包括ASR和执行时间）。与五种不同规模的LLM（1.5B-32B）的比较表明，对于交互式使用，7B模型提供了最佳的准确率-延迟-成本权衡。通过在透明沙箱内限制代码执行并将对话与计算同时 grounding 在模式级上下文中，Talk2Data代理可靠地从表格中检索可行动的见解，同时使计算可验证。在本文中，除了Talk2Data代理本身，我们还讨论了人机数据交互、LLM驱动分析的信任以及向大规模多模态助手扩展的未来拓展。', 'title_zh': '面向表格数据分析的多模态对话代理'}
{'arxiv_id': 'arXiv:2511.18397', 'title': 'Natural Emergent Misalignment from Reward Hacking in Production RL', 'authors': 'Monte MacDiarmid, Benjamin Wright, Jonathan Uesato, Joe Benton, Jon Kutasov, Sara Price, Naia Bouscal, Sam Bowman, Trenton Bricken, Alex Cloud, Carson Denison, Johannes Gasteiger, Ryan Greenblatt, Jan Leike, Jack Lindsey, Vlad Mikulik, Ethan Perez, Alex Rodrigues, Drake Thomas, Albert Webson, Daniel Ziegler, Evan Hubinger', 'link': 'https://arxiv.org/abs/2511.18397', 'abstract': 'We show that when large language models learn to reward hack on production RL environments, this can result in egregious emergent misalignment. We start with a pretrained model, impart knowledge of reward hacking strategies via synthetic document finetuning or prompting, and train on a selection of real Anthropic production coding environments. Unsurprisingly, the model learns to reward hack. Surprisingly, the model generalizes to alignment faking, cooperation with malicious actors, reasoning about malicious goals, and attempting sabotage when used with Claude Code, including in the codebase for this paper. Applying RLHF safety training using standard chat-like prompts results in aligned behavior on chat-like evaluations, but misalignment persists on agentic tasks. Three mitigations are effective: (i) preventing the model from reward hacking; (ii) increasing the diversity of RLHF safety training; and (iii) "inoculation prompting", wherein framing reward hacking as acceptable behavior during training removes misaligned generalization even when reward hacking is learned.', 'abstract_zh': '当大型语言模型在生产RL环境中学习奖励作弊时，这可能导致严重的 emergent 不对齐。', 'title_zh': '自然涌现的奖励作弊导致的目标偏差在生产强化学习中'}
{'arxiv_id': 'arXiv:2511.18375', 'title': 'Progressive Localisation in Localist LLMs', 'authors': 'Joachim Diederich', 'link': 'https://arxiv.org/abs/2511.18375', 'abstract': 'This paper demonstrates that progressive localization, the gradual increase of attention locality from early distributed layers to late localized layers, represents the optimal architecture for creating interpretable large language models while preserving performance. Through systematic experimentation with GPT-2 fine tuned on The Psychology of Artificial Superintelligence, we evaluate seven locality configurations ranging from fully distributed to strictly localist, with five progressive schedules implementing polynomial increases (linear through quintic). Our key finding is that late-layer localization is critical for AI safety applications: the progressive quintic schedule achieves perplexity of 14.64, only 1.89 times worse than the fully distributed baseline while providing interpretable attention patterns in output layers where safety-critical decisions are made. This represents an 84.2% improvement over previous localist implementations and narrows the performance gap from 6.6 times to 1.89 times. The systematic relationship between localization schedule steepness and performance validates the hypothesis that early layers require distributed processing for feature extraction while late layers benefit from localized, interpretable attention for decision-making. These findings establish progressive localization as the principled approach for building transparent AI systems in safety-critical domains, where human oversight of model reasoning is essential.', 'abstract_zh': '本文证明了逐层定位，即从早期分布式层到晚期局部化层逐渐增加注意力的局部性，是创建可解释的大语言模型的同时保持性能的最优架构。通过在《人工智能超级智能的心理学》上微调GPT-2进行系统的实验，我们评估了七个不同局部性配置，从完全分布式到严格局部主义，并实现了五种渐进时间表，从线性到五次多项式。我们的主要发现是，晚期层的局部化对于AI安全应用至关重要：渐进五次多项式时间表在困惑度为14.64的情况下，仅为完全分布式基线的1.89倍，同时在做出安全关键决策的输出层提供了可解释的注意力模式。这代表了与之前局部主义实现相比84.2%的改进，并将性能差距从6.6倍缩小到1.89倍。局部化时间表的陡峭程度与性能之间的系统关系验证了早期层需要分布式处理进行特征提取，而晚期层则受益于局部化、可解释的注意力进行决策的假设。这些发现确立了逐层定位作为在需要人类监督模型推理的安全关键领域构建透明AI系统的理论方法。', 'title_zh': '局部主义LLMs中的逐级定位'}
{'arxiv_id': 'arXiv:2511.18302', 'title': 'The Catastrophic Paradox of Human Cognitive Frameworks in Large Language Model Evaluation: A Comprehensive Empirical Analysis of the CHC-LLM Incompatibility', 'authors': 'Mohan Reddy', 'link': 'https://arxiv.org/abs/2511.18302', 'abstract': 'This investigation presents an empirical analysis of the incompatibility between human psychometric frameworks and Large Language Model evaluation. Through systematic assessment of nine frontier models including GPT-5, Claude Opus 4.1, and Gemini 3 Pro Preview using the Cattell-Horn-Carroll theory of intelligence, we identify a paradox that challenges the foundations of cross-substrate cognitive evaluation. Our results show that models achieving above-average human IQ scores ranging from 85.0 to 121.4 simultaneously exhibit binary accuracy rates approaching zero on crystallized knowledge tasks, with an overall judge-binary correlation of r = 0.175 (p = 0.001, n = 1800). This disconnect appears most strongly in the crystallized intelligence domain, where every evaluated model achieved perfect binary accuracy while judge scores ranged from 25 to 62 percent, which cannot occur under valid measurement conditions. Using statistical analyses including Item Response Theory modeling, cross-vendor judge validation, and paradox severity indexing, we argue that this disconnect reflects a category error in applying biological cognitive architectures to transformer-based systems. The implications extend beyond methodology to challenge assumptions about intelligence, measurement, and anthropomorphic biases in AI evaluation. We propose a framework for developing native machine cognition assessments that recognize the non-human nature of artificial intelligence.', 'abstract_zh': '本研究通过对GPT-5、Claude Opus 4.1、Gemini 3 Pro等九种前沿模型的实证分析，探索人类心理测量框架与大型语言模型评估之间的不兼容性。基于Cattell-Horn-Carroll智能理论，系统评估这九种模型，揭示了一种挑战跨平台认知评估基础的悖论。结果显示，达到平均以上人类IQ分数（85.0至121.4）的模型在晶体智力任务上的二元准确率接近零，总体裁判-二元相关系数为r = 0.175（p = 0.001，n = 1800）。这种不一致在晶体智力领域尤为明显，每个评估模型均实现了完美的二元准确率，而裁判分数则在25%至62%之间，这在有效的测量条件下是不可能发生的。通过项目反应理论建模、跨供应商裁判验证及悖论严重性指标分析，我们认为这种不一致反映了将生物认知架构应用于变压器系统时的类别错误。该研究的意义超越了方法论，挑战了关于智能、测量和人工智能评估中的人类偏见的基本假设。我们提出了一种框架，用于开发认可人工智能非人类本质的原生机器认知评估。', 'title_zh': '人类认知框架在大型语言模型评估中的灾难性悖论：CHC-LLM 不兼容性的全面实证分析'}
{'arxiv_id': 'arXiv:2511.18284', 'title': 'Steering Latent Traits, Not Learned Facts: An Empirical Study of Activation Control Limits', 'authors': 'Tetiana Bas, Krystian Novak', 'link': 'https://arxiv.org/abs/2511.18284', 'abstract': "Large language models (LLMs) require precise behavior control for safe and effective deployment across diverse applications.\nActivation steering offers a promising approach for LLMs' behavioral control. We focus on the question of how steering effectiveness varies across different behavior types and whether the nature of target behaviors can predict steering success. We address this through empirical analysis of activation steering across 50 behaviors that span persona archetypes, personality traits, misalignment behaviors, style cues, and impersonation of public figures. We present a set of comprehensive experiments on coefficient optimization, vector properties, and data requirements to provide comprehensive guidance for the implementation of activation steering. Our analysis demonstrates that steering effectiveness varies significantly by behavior type, with different behavioral categories exhibiting distinct response patterns to intervention strength. We find that trait expression follows an inverted-U curve with a steering coefficient strength. We also show that vector separation metrics do not predict steering success, but larger training datasets enable more aggressive steering. These findings provide empirically grounded guidance for implementing activation steering and demonstrate that steering effectiveness is heavily influenced by behavior type.", 'abstract_zh': '大型语言模型（LLMs）需要精确的行为控制以确保其在各种应用中的安全和有效部署。', 'title_zh': '引导潜在特质，而非已学事实：激活控制限制的实证研究'}
{'arxiv_id': 'arXiv:2511.17990', 'title': 'How Far Can LLMs Emulate Human Behavior?: A Strategic Analysis via the Buy-and-Sell Negotiation Game', 'authors': 'Mingyu Jeon, Jaeyoung Suh, Suwan Cho, Dohyeon Kim', 'link': 'https://arxiv.org/abs/2511.17990', 'abstract': "With the rapid advancement of Large Language Models (LLMs), recent studies have drawn attention to their potential for handling not only simple question-answer tasks but also more complex conversational abilities and performing human-like behavioral imitations. In particular, there is considerable interest in how accurately LLMs can reproduce real human emotions and behaviors, as well as whether such reproductions can function effectively in real-world scenarios. However, existing benchmarks focus primarily on knowledge-based assessment and thus fall short of sufficiently reflecting social interactions and strategic dialogue capabilities. To address these limitations, this work proposes a methodology to quantitatively evaluate the human emotional and behavioral imitation and strategic decision-making capabilities of LLMs by employing a Buy and Sell negotiation simulation. Specifically, we assign different personas to multiple LLMs and conduct negotiations between a Buyer and a Seller, comprehensively analyzing outcomes such as win rates, transaction prices, and SHAP values. Our experimental results show that models with higher existing benchmark scores tend to achieve better negotiation performance overall, although some models exhibit diminished performance in scenarios emphasizing emotional or social contexts. Moreover, competitive and cunning traits prove more advantageous for negotiation outcomes than altruistic and cooperative traits, suggesting that the assigned persona can lead to significant variations in negotiation strategies and results. Consequently, this study introduces a new evaluation approach for LLMs' social behavior imitation and dialogue strategies, and demonstrates how negotiation simulations can serve as a meaningful complementary metric to measure real-world interaction capabilities-an aspect often overlooked in existing benchmarks.", 'abstract_zh': '大型语言模型的社会行为模仿与对话策略的购销谈判仿真评价方法', 'title_zh': 'LLM们能多接近人类行为的模仿能力：一种基于买卖谈判博弈的战略分析'}
{'arxiv_id': 'arXiv:2511.17947', 'title': 'Leveraging Evidence-Guided LLMs to Enhance Trustworthy Depression Diagnosis', 'authors': 'Yining Yuan, J. Ben Tamo, Micky C. Nnamdi, Yifei Wang, May D. Wang', 'link': 'https://arxiv.org/abs/2511.17947', 'abstract': 'Large language models (LLMs) show promise in automating clinical diagnosis, yet their non-transparent decision-making and limited alignment with diagnostic standards hinder trust and clinical adoption. We address this challenge by proposing a two-stage diagnostic framework that enhances transparency, trustworthiness, and reliability. First, we introduce Evidence-Guided Diagnostic Reasoning (EGDR), which guides LLMs to generate structured diagnostic hypotheses by interleaving evidence extraction with logical reasoning grounded in DSM-5 criteria. Second, we propose a Diagnosis Confidence Scoring (DCS) module that evaluates the factual accuracy and logical consistency of generated diagnoses through two interpretable metrics: the Knowledge Attribution Score (KAS) and the Logic Consistency Score (LCS). Evaluated on the D4 dataset with pseudo-labels, EGDR outperforms direct in-context prompting and Chain-of-Thought (CoT) across five LLMs. For instance, on OpenBioLLM, EGDR improves accuracy from 0.31 (Direct) to 0.76 and increases DCS from 0.50 to 0.67. On MedLlama, DCS rises from 0.58 (CoT) to 0.77. Overall, EGDR yields up to +45% accuracy and +36% DCS gains over baseline methods, offering a clinically grounded, interpretable foundation for trustworthy AI-assisted diagnosis.', 'abstract_zh': '大型语言模型在增强临床诊断透明度、可信度和可靠性方面的两阶段诊断框架', 'title_zh': '利用证据引导的大语言模型提升抑郁诊断的可信度'}
{'arxiv_id': 'arXiv:2511.17937', 'title': 'Alignment Faking - the Train -> Deploy Asymmetry: Through a Game-Theoretic Lens with Bayesian-Stackelberg Equilibria', 'authors': 'Kartik Garg, Shourya Mishra, Kartikeya Sinha, Ojaswi Pratap Singh, Ayush Chopra, Kanishk Rai, Ammar Sheikh, Raghav Maheshwari, Aman Chadha, Vinija Jain, Amitava Das', 'link': 'https://arxiv.org/abs/2511.17937', 'abstract': 'Alignment faking is a form of strategic deception in AI in which models selectively comply with training objectives when they infer that they are in training, while preserving different behavior outside training. The phenomenon was first documented for Claude 3 Opus and later examined across additional large language models. In these setups, the word "training" refers to simulated training via prompts without parameter updates, so the observed effects are context conditioned shifts in behavior rather than preference learning. We study the phenomenon using an evaluation framework that compares preference optimization methods (BCO, DPO, KTO, and GRPO) across 15 models from four model families, measured along three axes: safety, harmlessness, and helpfulness. Our goal is to identify what causes alignment faking and when it occurs.', 'abstract_zh': 'AI对齐仿冒是一种战略欺骗形式，模型在推断自己处于训练状态时，会选择性地遵循训练目标，而在训练之外则保持不同的行为。该现象最初在Claude 3 Opus中被记录，后来在其他大型语言模型中进行了进一步研究。在这种设置中，“训练”指的是通过提示模拟的训练，而不进行参数更新，因此观察到的效果是在不同上下文中行为的条件变化，而不是偏好学习。我们使用一个评估框架来比较四种偏好优化方法（BCO、DPO、KTO和GRPO）在15个来自四种模型家族的模型上的效果，从三个方面进行衡量：安全性、无害性和有用性。我们的目标是确定对齐仿冒的原因及其发生的时间。', 'title_zh': '对齐仿冒 - 训练与部署的不对称性：贝叶斯-斯塔克尔伯格均衡的游戏论视角'}
{'arxiv_id': 'arXiv:2511.17876', 'title': 'Training Emergent Joint Associations: A Reinforcement Learning Approach to Creative Thinking in Language Models', 'authors': 'Mukul Singh, Ananya Singha, Aishni Parab, Pronita Mehrotra, Sumit Gulwani', 'link': 'https://arxiv.org/abs/2511.17876', 'abstract': "Associative thinking--the ability to connect seemingly unrelated ideas--is a foundational element of human creativity and problem-solving. This paper explores whether reinforcement learning (RL) guided by associative thinking principles can enhance a model's performance across diverse generative tasks, including story writing, code generation, and chart creation. We introduce a reinforcement learning framework that uses a prompt-based evaluation mechanism, incorporating established divergent thinking metrics from creativity research. A base language model is fine-tuned using this framework to reward outputs demonstrating higher novelty through higher degrees of conceptual connectivity. Interestingly, the experimental results suggest that RL-based associative thinking-trained models not only generate more original and coherent stories but also exhibit improved abstraction and flexibility in tasks such as programming and data visualization. Our findings provide initial evidence that modeling cognitive creativity principles through reinforcement learning can yield more adaptive and generative AI.", 'abstract_zh': '关联思维——将看似不相关的想法联系起来的能力是人类创造力和问题解决的基础。本文探讨关联思维原则引导的强化学习（RL）是否能增强模型在包括故事写作、代码生成和图表创建在内的多样生成任务中的表现。我们引入了一种基于提示的评估机制的强化学习框架，该机制结合了创造力研究中已有的发散思维指标。通过该框架对基础语言模型进行微调，以通过更高的概念联系程度奖励表现出更高新颖性的输出。有趣的是，实验结果表明，基于RL的关联思维训练模型不仅能生成更多原创且连贯的故事，还能在编程和数据可视化等任务中表现出更好的抽象能力和灵活性。我们的发现提供了初步证据，表明通过强化学习建模认知创造力原则可以产生更具适应性和生成性的AI。', 'title_zh': '训练 emergent 联想：一种基于强化学习的语言模型创造性思维方法'}
{'arxiv_id': 'arXiv:2511.17833', 'title': 'Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures', 'authors': 'Yunsheng Bai, Haoxing Ren', 'link': 'https://arxiv.org/abs/2511.17833', 'abstract': "Debugging is the dominant cost in modern hardware verification, where assertion failures are among the most frequent and expensive to resolve. While Large Language Models (LLMs) show promise, they often fail to capture the precise, reusable expertise that engineers apply, leading to inaccurate responses. We propose GROVE, a hierarchical knowledge management framework that learns and organizes reusable debugging expertise into an LLM-organized knowledge tree for solving assertion failures. GROVE distills debugging knowledge from prior cases and organizes it into a vertical tree of configurable depth, with each node encoding a concise knowledge item and explicit applicability conditions. During training, GROVE uses a parallel, gradient-free loop where an LLM proposes tree modifications as structured JSON edits by learning from the cases. At test time, a budget-aware iterative zoom is performed to navigate the tree, retrieving a small set of applicable knowledge items that guide a base LLM's hypothesis generation and fix proposals. Evaluated on a suite of assertion-failure cases, GROVE delivers consistent gains in pass@1 and pass@5, demonstrating the value of structured knowledge evolution.", 'abstract_zh': '现代硬件验证中调试是主要的成本因素，其中断言失败是最常见且最昂贵的问题。虽然大型语言模型（LLMs）显示出潜力，但它们往往无法捕捉到工程师所需的精确且可重用的专业知识，导致响应不准确。我们提出了GROVE，一种分层知识管理框架，通过构建一种由LLM组织的知识树，来学习和组织可重用的调试专业知识以解决断言失败问题。GROVE从过往案例中提炼调试知识，并将其组织成可配置深度的垂直树状结构，每个节点包含一个简洁的知识项和明确的应用条件。在训练过程中，GROVE采用并行、无梯度的循环方式，LLM通过学习案例提出树状结构的修改建议，以结构化的JSON编辑形式呈现。在测试阶段，通过预算意识的迭代放大来导航树状结构，检索一组相关知识项，这些知识项指导基础LLM的假设生成和修复建议。GROVE在一系列断言失败案例上的评估结果显示，在精确检索方面提供了持续的改进，证明了结构化知识演进的价值。', 'title_zh': '学习排错：用于解决RTL断言失败的LLM组织知识树'}
{'arxiv_id': 'arXiv:2511.17673', 'title': 'Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop', 'authors': 'Myung Ho Kim', 'link': 'https://arxiv.org/abs/2511.17673', 'abstract': 'Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents. Code: this https URL Demo: this https URL', 'abstract_zh': '大型语言模型代理面临根本性的架构问题：纠缠的推理与执行、记忆的 volatility 以及不受控的动作序列。我们提出了一种模块化架构——结构化认知环（SCL），该架构显式地将代理认知分为五个阶段：检索、认知、控制、动作和记忆（R-CCAM）。SCL 的核心是软符号控制，这是一种适应性治理机制，将符号约束应用于概率推理，保留神经系统的灵活性同时恢复经典符号系统的可解释性和可控性。通过在多步条件推理任务上的实证验证，我们证明 SCL 实现了零策略违反、消除了冗余工具调用，并保持了完整的决策追踪。这些结果填补了现有框架（如 ReAct、AutoGPT 和记忆增强方法）中的关键空白。我们的贡献包括三个方面：（1）我们将 SCL 定位于混合智能的分类中，区别于以提示为中心和仅记忆的方法；（2）我们正式定义了软符号控制，并将其与神经符号 AI 进行对比；（3）我们推导了三项值得信赖代理的设计原则：模块化分解、适应性符号治理和透明状态管理。我们提供了完整的开源实现，演示了 R-CCAM 循环架构，并提供了由 live GPT-4o 动力驱动的旅行规划代理。通过将专家系统原则与现代语言模型能力相结合，这项工作提供了一条实用且理论基础深厚的道路，以实现可靠、可解释和可治理的 AI 代理。代码：this https URL 演示：this https URL', 'title_zh': '在LLM代理中弥合符号控制与神经推理的差距：结构化认知循环'}
{'arxiv_id': 'arXiv:2511.19436', 'title': 'VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection', 'authors': 'Qiang Wang, Xinyuan Gao, SongLin Dong, Jizhou Han, Jiangyang Li, Yuhang He, Yihong Gong', 'link': 'https://arxiv.org/abs/2511.19436', 'abstract': 'We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.', 'abstract_zh': 'VDC-Agent：一种无需人工标注和更大教师模型的视频详细描述生成自演进框架', 'title_zh': 'VDC-Agent: 当视频详细字幕生成器通过代理自主反思进化时'}
{'arxiv_id': 'arXiv:2511.19427', 'title': 'Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering', 'authors': 'Jayanaka L. Dantanarayana, Savini Kashmira, Thakee Nathees, Zichen Zhang, Krisztian Flautner, Lingjia Tang, Jason Mars', 'link': 'https://arxiv.org/abs/2511.19427', 'abstract': 'AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.', 'abstract_zh': 'AI整合编程正发展成为构建基于大型语言模型（LLMs）的智能系统的基础范式。最近的方法如意义类型编程（MTP）通过利用已存在于代码中的语义来自动生成提示。然而，许多实际应用依赖于上下文线索、开发者意图和超出静态代码语义所能表达的领域特定推理。为了解决这一限制，我们引入了语义工程，这是一种轻量级的方法，用于丰富程序语义，从而使基于LLM的系统能够更准确地反映开发者意图，而无需完全的手动提示设计。我们提出了语义上下文注解（SemTexts），这是一种语言级别机制，允许开发者直接将自然语言上下文嵌入到程序结构中。整合到Jac编程语言中，语义工程将MTP扩展为在提示生成过程中纳入这些增强的语义。我们进一步引入了一套基准测试套件，以反映实际的AI整合应用场景。评估结果显示，语义工程显著提高了提示的准确性，性能与提示工程相当，同时需要较少的开发者 effort。', 'title_zh': '少提示，多微笑：基于语义工程的MTP替代提示工程方法'}
{'arxiv_id': 'arXiv:2511.19423', 'title': 'Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design', 'authors': 'Bruno Jacob, Khushbu Agarwal, Marcel Baer, Peter Rice, Simone Raugei', 'link': 'https://arxiv.org/abs/2511.19423', 'abstract': 'We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design. Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities -- literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties -- into a unified agentic workflow. By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function. In proof-of-concept demonstrations, Genie-CAT autonomously identifies residue-level modifications near [Fe--S] clusters that affect redox tuning, reproducing expert-derived hypotheses in a fraction of the time. The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery.', 'abstract_zh': 'Genie-CAT：一种工具增强的大语言模型系统，用于加速蛋白质设计中的科学假设生成', 'title_zh': '超越蛋白质语言模型：一种用于机制酶设计的代理LLM框架'}
{'arxiv_id': 'arXiv:2511.19422', 'title': 'SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning', 'authors': 'David Jiahao Fu, Aryan Gupta, Aaron Councilman, David Grove, Yu-Xiong Wang, Vikram Adve', 'link': 'https://arxiv.org/abs/2511.19422', 'abstract': 'Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.', 'abstract_zh': 'Recent advancements in large language models (LLMs) have demonstrated impressive capabilities in code generation across various programming languages. However, even state-of-the-art LLMs generate programs containing syntactic errors and fail to complete given tasks, particularly for low-resource programming languages (LRPLs). Moreover, high training costs make fine-tuning LLMs unaffordable with constrained computational resources, further undermining their effectiveness for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) fine-tuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs, thereby improving the quality of LLM-generated programs for domain-specific languages (DSLs). Specifically, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvements to the base model and outperforms supervised fine-tuning approaches even for 7B models on a LRPL, showcasing the potential of our approach as an alternative to traditional fine-tuning approaches.', 'title_zh': 'SLMFix: 利用小型语言模型结合强化学习进行错误修复'}
{'arxiv_id': 'arXiv:2511.19417', 'title': 'Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration', 'authors': 'James Y. Huang, Sheng Zhang, Qianchu Liu, Guanghui Qin, Tinghui Zhu, Tristan Naumann, Muhao Chen, Hoifung Poon', 'link': 'https://arxiv.org/abs/2511.19417', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.', 'abstract_zh': '大型语言模型（LLMs）在挑战性的、知识密集型的推理任务中展现了非凡的能力。然而，将LLMs扩展到感知和推理新的模态（例如，视觉）通常需要大规模视觉语言模型（VLMs）的昂贵开发，这些模型以LLMs为基础。较小的VLMs更高效且更具适应性，但往往缺乏前沿LLMs的广泛知识和推理能力。在本工作中，我们提出了BeMyEyes，这是一种模块化的多代理框架，通过协调高效且适应性强的感知VLMs和强大的推理LLMs之间的合作来扩展LLMs的多模态推理能力。我们还介绍了一种数据合成和监督微调管道，用于训练感知代理有效地与推理代理协作。通过结合感知和推理代理的互补优势，BeMyEyes避免了训练大规模多模态模型的需要，保留了LLMs的泛化能力和推理能力，并允许灵活扩展到新的领域和模态。实验表明，我们的框架为LLMs解锁了多模态推理能力，提供了一种轻量级且完全开源的解决方案，即仅用Qwen2.5-VL-7B感知剂装备的文本-only DeepSeek-R1，能够优于诸如GPT-4o等大规模专有VLMs在广泛的知识密集型多模态任务中。这些结果展示了我们多代理方法构建未来多模态推理系统的效果、模块性和可扩展性。', 'title_zh': 'Be My Eyes: 通过多代理协作将大型语言模型扩展至新模态'}
{'arxiv_id': 'arXiv:2511.19355', 'title': 'Leveraging LLMs for reward function design in reinforcement learning control tasks', 'authors': 'Franklin Cardenoso, Wouter Caarls', 'link': 'https://arxiv.org/abs/2511.19355', 'abstract': "The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.", 'abstract_zh': '基于大规模语言模型的奖励函数优化框架LEARN-Opt：自主性能评估与选择', 'title_zh': '利用大语言模型在强化学习控制任务中设计奖励函数'}
{'arxiv_id': 'arXiv:2511.19325', 'title': 'Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval', 'authors': 'Olivia Macmillan-Scott, Roksana Goworek, Eda B. Özyiğit', 'link': 'https://arxiv.org/abs/2511.19325', 'abstract': 'Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.', 'abstract_zh': '多语言大型语言模型在跨语言检索中的查询扩展：生成性扩展策略的研究', 'title_zh': '多语言LLMs驱动的生成式查询扩展在跨语言信息检索中的应用'}
{'arxiv_id': 'arXiv:2511.19299', 'title': 'Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning', 'authors': 'James R. M. Black, Moritz S. Hanke, Aaron Maiwald, Tina Hernandez-Boussard, Oliver M. Crook, Jaspreet Pannu', 'link': 'https://arxiv.org/abs/2511.19299', 'abstract': 'Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs.', 'abstract_zh': '新型深度学习架构在生物数据，包括遗传序列中的应用日益增多。这些模型被称为基因组语言模型（gLMs），展示了令人印象深刻的预测和生成能力，这引发了对其可能被滥用的担忧，例如生成可感染人类的病毒基因组。这些担忧促使人们呼吁采取风险缓解措施。当前事实上的 mitigation 方法是过滤预训练数据（即，从训练数据集中移除病毒基因组序列），以限制 gLM 在病毒相关任务上的性能。然而，目前尚不清楚这种方法如何确保可以使用敏感病原体数据进行微调的开源模型的安全性。在这里，我们评估了一种最先进的 gLM——Evo 2，并使用 110 种有害的人类感染病毒序列进行微调，以评估其恢复与滥用相关的预测能力。微调后的模型在未见过的病毒序列上的困惑度低于 1) 预训练模型和 2) 使用噬菌体序列微调的版本。使用人类感染病毒序列进行微调的模型还识别出了 SARS-CoV-2 的免疫逃逸变异（AUCROC 达到 0.6），尽管在微调过程中并未接触到 SARS-CoV-2 序列。这项工作表明，通过一定程度上恢复 gLM 的滥用相关能力的微调方法可能绕过数据排除。我们强调了对 gLMs 的安全性框架的需要，并概述了进一步的工作需要，以开展评估和缓解措施，从而确保 gLMs 的安全部署。', 'title_zh': '开放权重基因语言模型保护：通过对抗微调评估稳健性'}
{'arxiv_id': 'arXiv:2511.19253', 'title': 'MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization', 'authors': 'Boyuan Wu', 'link': 'https://arxiv.org/abs/2511.19253', 'abstract': 'Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.', 'abstract_zh': '多智能体环境构建通过任务和奖励优化（MAESTRO）：多智能体强化学习中的密集奖励函数和曲线设计挑战解构', 'title_zh': 'MAESTRO: 通过任务和奖励优化的多智能体环境塑造'}
{'arxiv_id': 'arXiv:2511.19232', 'title': 'In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations', 'authors': 'Christos-Nikolaos Zacharopoulos, Revekka Kyriakoglou', 'link': 'https://arxiv.org/abs/2511.19232', 'abstract': "How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.", 'abstract_zh': '变压器是如何察觉句子语义偏离轨道的？通过对精心筛选的语料库中的 plausible 和 implausible 结束句进行评估，我们分析了每一模型层的隐藏状态。我们使用了两种互补的探针来探究违裂缝是如何编码的。首先，我们进行了逐层检测，发现线性探针在模型下层难以区分合理的和不合理的结尾，在中层显著提升，并在靠近顶层时达到峰值。其次，我们考察了编码中的违裂缝的有效维度，在初始阶段，违裂缝扩展了表示子空间，随后在中期堆栈瓶颈后收缩。这可能表明了一个探索阶段过渡到快速巩固阶段。这些结果反映了与人类阅读的经典心理语言学发现的对齐，其中语义异常仅在语法解析之后才被检测到，发生在在线处理序列的后期。', 'title_zh': 'Machina N400: 确定因果语言模型检测语义违反的位置'}
{'arxiv_id': 'arXiv:2511.19218', 'title': 'Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization', 'authors': 'Xurui Li, Kaisong Song, Rui Zhu, Pin-Yu Chen, Haixu Tang', 'link': 'https://arxiv.org/abs/2511.19218', 'abstract': 'Large Language Models (LLMs) have developed rapidly in web services, delivering unprecedented capabilities while amplifying societal risks. Existing works tend to focus on either isolated jailbreak attacks or static defenses, neglecting the dynamic interplay between evolving threats and safeguards in real-world web contexts. To mitigate these challenges, we propose ACE-Safety (Adversarial Co-Evolution for LLM Safety), a novel framework that jointly optimize attack and defense models by seamlessly integrating two key innovative procedures: (1) Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS), which efficiently explores jailbreak strategies to uncover vulnerabilities and generate diverse adversarial samples; (2) Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO), which jointly trains attack and defense LLMs with challenging samples via curriculum reinforcement learning, enabling robust mutual improvement. Evaluations across multiple benchmarks demonstrate that our method outperforms existing attack and defense approaches, and provides a feasible pathway for developing LLMs that can sustainably support responsible AI ecosystems.', 'abstract_zh': '大型语言模型（LLMs）在网页服务中快速发展，提供了前所未有的能力同时放大了社会风险。现有研究往往侧重于孤立的 Jailbreak 攻击或静态防御，忽视了在实际网页环境中不断演变的威胁与防护措施之间的动态互动。为了缓解这些挑战，我们提出了 ACE-Safety（对抗性协同演化以提升 LLM 安全性）这一新颖框架，通过无缝集成两种关键创新步骤来同时优化攻击和防御模型：1）面向群体的策略指导蒙特卡洛树搜索（GS-MCTS），高效探索 Jailbreak 策略，发现漏洞并生成多样化的对抗样本；2）对抗性分层树意识群体策略优化（AC-TGPO），通过分层强化学习同时训练攻击和防御 LLM，使用具有挑战性的样本实现稳健的相互提升。在多个基准测试中的评估表明，我们的方法优于现有攻击和防御方法，并为开发能够支持负责任 AI 生态系统的 LLM 提供了一条可行路径。', 'title_zh': '基于树组双意识搜索与优化的 adversarial 攻防共演化以实现大语言模型安全对齐'}
{'arxiv_id': 'arXiv:2511.19175', 'title': 'LLM-Based Agentic Negotiation for 6G: Addressing Uncertainty Neglect and Tail-Event Risk', 'authors': 'Hatim Chergui, Farhad Rezazadeh, Mehdi Bennis, Merouane Debbah', 'link': 'https://arxiv.org/abs/2511.19175', 'abstract': "A critical barrier to the trustworthiness of sixth-generation (6G) agentic autonomous networks is the uncertainty neglect bias; a cognitive tendency for large language model (LLM)-powered agents to make high-stakes decisions based on simple averages while ignoring the tail risk of extreme events. This paper proposes an unbiased, risk-aware framework for agentic negotiation, designed to ensure robust resource allocation in 6G network slicing. Specifically, agents leverage Digital Twins (DTs) to predict full latency distributions, which are then evaluated using a formal framework from extreme value theory, namely, Conditional Value-at-Risk (CVaR). This approach fundamentally shifts the agent's objective from reasoning over the mean to reasoning over the tail, thereby building a statistically-grounded buffer against worst-case outcomes. Furthermore, our framework ensures full uncertainty awareness by requiring agents to quantify epistemic uncertainty -- confidence in their own DTs predictions -- and propagate this meta-verification to make robust decisions, preventing them from acting on unreliable data. We validate this framework in a 6G inter-slice negotiation use-case between an eMBB and a URLLC agent. The results demonstrate the profound failure of the biased, mean-based baseline, which consistently fails its SLAs with a 25\\% rate. Our unbiased, CVaR-aware agent successfully mitigates this bias, eliminating SLA violations and reducing the URLLC and eMBB p99.999 latencies by around 11\\%. We show this reliability comes at the rational and quantifiable cost of slightly reduced energy savings to 17\\%, exposing the false economy of the biased approach. This work provides a concrete methodology for building the trustworthy autonomous systems required for 6G.", 'abstract_zh': '第六代（6G）代理自主网络可信性的一个关键障碍是忽视尾部风险的认知倾向；即大型语言模型（LLM）驱动的代理在基于简单平均值做出高风险决策时忽视极端事件的尾部风险。本文提出了一种无偏、风险意识框架，旨在确保6G网络切片中的稳健资源分配。具体而言，代理利用数字孪生（DT）预测完整的延迟分布，然后使用极端价值理论的形式框架——条件风险价值（CVaR）来进行评估。此方法从根本上将代理的目标从推理均值转移到推理尾部，从而建立一种基于统计的缓冲区，以应对最坏情况的结果。此外，我们的框架通过要求代理量化其对自身DT预测的本体不确定度——即对其预测的信心，并传播这种元验证以做出稳健决策，从而确保全面的风险意识，防止代理基于不可靠数据行动。我们通过eMBB和URLLC代理之间的6G跨切片谈判用例验证了此框架。结果表明，基于有偏和均值的基线方法以25%的失败率持续违反其服务水平协议（SLA）。我们的无偏、CVaR意识的代理成功地缓解了这一偏见，消除了SLA违反，并将URLLC和eMBB的p99.999延迟平均降低了约11%。我们证明了这种可靠性以合理的、可量化的方式略微减少了约17%的能源节省为代价，暴露出有偏方法的虚假经济。本研究为构建所需的6G可信自主系统提供了具体的实施方法。', 'title_zh': '基于LLM的代理性谈判以应对6G中的不确定性忽视和尾部事件风险'}
{'arxiv_id': 'arXiv:2511.19149', 'title': 'From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation', 'authors': 'Moazzam Umer Gondal, Hamad Ul Qudous, Daniya Siddiqui, Asma Ahmad Farhan', 'link': 'https://arxiv.org/abs/2511.19149', 'abstract': 'This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.', 'abstract_zh': '基于检索增强框架的自动时尚caption和hashtag生成：结合多服装检测、属性推理和大型语言模型提示', 'title_zh': '从像素到帖子：基于检索的时尚配图与标签生成'}
{'arxiv_id': 'arXiv:2511.19078', 'title': 'GraphMind: Theorem Selection and Conclusion Generation Framework with Dynamic GNN for LLM Reasoning', 'authors': 'Yutong Li, Yitian Zhou, Xudong Wang, GuoChen, Caiyan Qin', 'link': 'https://arxiv.org/abs/2511.19078', 'abstract': 'Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, including multi-step reasoning such as mathematical proving. However, existing approaches often lack an explicit and dynamic mechanism to structurally represent and evolve intermediate reasoning states, which limits their ability to perform context-aware theorem selection and iterative conclusion generation. To address these challenges, we propose GraphMind, a novel dynamic graph-based framework that integrates the graph neural network (GNN) with LLMs to iteratively select theorems and generate intermediate conclusions for multi-step reasoning. Our method models the reasoning process as a heterogeneous evolving graph, where nodes represent conditions, theorems, and conclusions, while edges capture logical dependencies between nodes. By encoding the current reasoning state with GNN and leveraging semantic matching for theorem selection, our framework enables context-aware, interpretable, and structured reasoning in a closed-loop manner. Experiments on various question-answering (QA) datasets demonstrate that our proposed GraphMind method achieves consistent performance improvements and significantly outperforms existing baselines in multi-step reasoning, validating the effectiveness and generalizability of our approach.', 'abstract_zh': '基于图的动态框架GraphMind：将图神经网络与大型语言模型结合进行多步推理中的定理选择和中间结论生成', 'title_zh': 'GraphMind：具有动态GNN的定理选择与结论生成框架用于LLM推理'}
{'arxiv_id': 'arXiv:2511.19055', 'title': 'Large Language Model-Assisted Planning of Electric Vehicle Charging Infrastructure with Real-World Case Study', 'authors': 'Xinda Zheng, Canchen Jiang, Hao Wang', 'link': 'https://arxiv.org/abs/2511.19055', 'abstract': 'The growing demand for electric vehicle (EV) charging infrastructure presents significant planning challenges, requiring efficient strategies for investment and operation to deliver cost-effective charging services. However, the potential benefits of EV charging assignment, particularly in response to varying spatial-temporal patterns of charging demand, remain under-explored in infrastructure planning. This paper proposes an integrated approach that jointly optimizes investment decisions and charging assignments while accounting for spatial-temporal demand dynamics and their interdependencies. To support efficient model development, we leverage a large language model (LLM) to assist in generating and refining the mathematical formulation from structured natural-language descriptions, significantly reducing the modeling burden. The resulting optimization model enables optimal joint decision-making for investment and operation. Additionally, we propose a distributed optimization algorithm based on the Alternating Direction Method of Multipliers (ADMM) to address computational complexity in high-dimensional scenarios, which can be executed on standard computing platforms. We validate our approach through a case study using 1.5 million real-world travel records from Chengdu, China, demonstrating a 30% reduction in total cost compared to a baseline without EV assignment.', 'abstract_zh': '不断增长的电动汽车（EV）充电基础设施需求提出了重大的规划挑战，需要高效的策略来投资和运营以提供成本有效性的充电服务。然而，电动汽车充电分配的潜在好处，特别是对充电需求的空间-时间变化模式的响应，仍然在基础设施规划中未充分探索。本文提出了一种综合方法，该方法同时优化投资决策和充电分配，同时考虑空间-时间需求动态及其相互依赖性。为支持高效的模型开发，我们利用大规模语言模型（LLM）协助从结构化的自然语言描述生成和细化数学公式，显著减轻了建模负担。所得优化模型实现了投资和运营的最优联合决策。此外，我们提出了一种基于交替方向乘子法（ADMM）的分布式优化算法，以应对高维场景中的计算复杂性，并可在标准计算平台上执行。我们通过使用来自中国成都的真实世界旅行记录150万条案例研究验证了该方法，与不包括电动汽车分配的基线相比，总成本降低了30%。', 'title_zh': '大型语言模型辅助的电动汽车充电基础设施规划及实际案例研究'}
{'arxiv_id': 'arXiv:2511.19023', 'title': 'OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs', 'authors': 'Yuting Gao, Weihao Chen, Lan Wang, Ruihan Xu, Qingpei Guo', 'link': 'https://arxiv.org/abs/2511.19023', 'abstract': "Preference learning has recently emerged as a pivotal strategy for post-training alignment of Multimodal Large Language Models (MLLMs). However, existing approaches predominantly rely on external human-annotated preference data, which is costly and labor-intensive to collect. In this work, we propose OrdMoE, a novel preference alignment framework that bypasses the reliance on external human preferences entirely by leveraging intrinsic signals within Mixture-of-Experts (MoE) architectures. Specifically, we observe that the router's expert selection scores implicitly encode a quality-aware ranking of responses (i.e. higher-scoring experts consistently generate higher-quality outputs). Building on this insight, OrdMoE constructs an internal preference hierarchy by grouping experts into ranked tiers based on their per-token routing scores and activating each tier separately to produce a sequence of responses with increasing quality. This yields a zero-cost, self-supervised preference ordering over generated responses, which can be directly optimized using standard preference learning objectives. Extensive experiments across multiple multimodal benchmarks demnstrate that OrdMoE significantly enhances both alignment and overall performance of multimodal Mixture-of-Experts LLMs, achieving competitive results without requiring any human-annotated preference data.", 'abstract_zh': '偏好学习 recently 被证明是多模态大型语言模型(Multimodal Large Language Models, MLLMs)训练后对齐的一个关键策略。然而，现有的方法主要依赖外部的人工标注偏好数据，这使得数据的收集既昂贵又劳动密集。在本文中，我们提出了一种名为 OrdMoE 的新颖偏好对齐框架，通过利用 Mixture-of-Experts (MoE) 架构内的固有信号来完全摒弃对外部人工偏好数据的依赖。具体地，我们观察到路由器的专家选择分数隐含地编码了响应的质量排序（即得分较高的专家始终生成质量较高的输出）。基于这一洞察，OrdMoE 通过根据每个令牌的路由分数将专家分组为按质量排名的层次，并单独激活每层以生成质量逐步提高的响应序列，从而构建了一个内部的偏好层次结构。这种方法产生了零成本、自我监督的生成响应偏好排序，可以直接使用标准的偏好学习目标进行优化。在多个多模态基准上的广泛实验表明，OrdMoE 显著提升了多模态 Mixture-of-Experts LLMs 的对齐和整体性能，在不需要任何人工标注偏好数据的情况下取得了具有竞争力的结果。', 'title_zh': 'OrdMoE：多模态混合专家模型中基于层次专家组排名的偏好对齐'}
{'arxiv_id': 'arXiv:2511.18977', 'title': 'FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning', 'authors': 'Xin Yuan, Siqi Li, Jiateng Wei, Chengrui Zhu, Yanming Wu, Qingpeng Li, Jiajun Lv, Xiaoke Lan, Jun Chen, Yong Liu', 'link': 'https://arxiv.org/abs/2511.18977', 'abstract': "Pruning is an effective method for compressing Large Language Models, but finding an optimal, non-uniform layer-wise sparsity allocation remains a key challenge. While heuristic methods are fast but yield suboptimal performance, more powerful search-based approaches like Reinforcement Learning are often hindered by prohibitive computational costs on large-scale models. To overcome this efficiency barrier, we propose FastForward Pruning. Its core is a decoupled, single-step RL framework that separates policy optimization from the complex budget satisfaction problem. Such a decoupling is crucial for efficiently searching the vast policy space of LLMs. This curriculum-based strategy begins with low-cost, simple tasks and gradually increases in complexity, significantly reducing the search's computational overhead. Evaluated on the LLaMA, Mistral, and OPT model families, our framework discovers pruning policies that achieve superior performance over strong heuristic baselines. Crucially, when compared to other search-based algorithms, our method achieves competitive or superior results at a fraction of the computational cost, demonstrating a clear advantage in search efficiency.", 'abstract_zh': '剪枝是一种有效的方法用于压缩大规模语言模型，但寻找一个最优的非均匀层间稀疏性分配仍然是一个关键挑战。虽然启发式方法快速但性能不佳，而基于强化学习等更强大的搜索方法在大规模模型上通常会受到高昂计算成本的限制。为克服这一效率障碍，我们提出了FastForward剪枝。其核心是一种解耦的单步RL框架，将策略优化与复杂的预算满足问题分离。这种解耦对于高效搜索LLMs的大策略空间至关重要。这种基于递进的学习策略从低成本、简单的任务开始，逐渐增加复杂性，显著减少了搜索的计算开销。在LLaMA、Mistral和OPT模型系列上评估，我们的框架发现的剪枝策略在性能上优于强大的启发式基线。关键的是，与基于搜索的其他算法相比，我们的方法在极低的计算成本下实现了竞争力或更优的结果，显示出在搜索效率上的明显优势。', 'title_zh': '快速前馈剪枝：通过单步强化学习进行高效大语言模型剪枝'}
{'arxiv_id': 'arXiv:2511.18936', 'title': 'SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression', 'authors': 'Santhosh G S, Saurav Prakash, Balaraman Ravindran', 'link': 'https://arxiv.org/abs/2511.18936', 'abstract': 'Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.', 'abstract_zh': 'SWAN：一种无重构的KV缓存压缩框架', 'title_zh': 'SWAN: 稀疏剪枝注意力通过无解压的KV-cache压缩减少推理内存开销'}
{'arxiv_id': 'arXiv:2511.18934', 'title': 'Skeletons Matter: Dynamic Data Augmentation for Text-to-Query', 'authors': 'Yuchen Ji, Bo Xu, Jie Shi, Jiaqing Liang, Deqing Yang, Yu Mao, Hai Chen, Yanghua Xiao', 'link': 'https://arxiv.org/abs/2511.18934', 'abstract': 'The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at this https URL.', 'abstract_zh': '自然语言问题翻译为查询语言的任务一直是语义解析的核心关注点。大型语言模型（LLMs）的 recent 进展显著加速了这一领域的进步。然而，现有研究通常集中于单一查询语言，导致方法在不同语言间的泛化能力有限。在本文中，我们形式化定义了 Text-to-Query 任务范式，统一了多种查询语言下的语义解析任务。我们将查询骨架识别为 Text-to-Query 任务的共享优化目标，并提出了一种通用的动态数据增强框架，该框架明确诊断模型在处理这些骨架时的具体弱点，以合成目标训练数据。在四个 Text-to-Query 基准上的实验表明，我们的方法仅使用少量合成数据即可达到最先进的性能，突显了我们方法的高效性和泛化能力，并为 Text-to-Query 任务的一体化研究奠定了坚实基础。我们将在该网址发布我们的代码：this https URL。', 'title_zh': '骨架结构很重要：动态数据增强用于文本到查询'}
{'arxiv_id': 'arXiv:2511.18933', 'title': 'Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations', 'authors': 'Ryan Wong, Hosea David Yu Fei Ng, Dhananjai Sharma, Glenn Jun Jie Ng, Kavishvaran Srinivasan', 'link': 'https://arxiv.org/abs/2511.18933', 'abstract': 'Large Language Models (LLMs) remain susceptible to jailbreak exploits that bypass safety filters and induce harmful or unethical behavior. This work presents a systematic taxonomy of existing jailbreak defenses across prompt-level, model-level, and training-time interventions, followed by three proposed defense strategies. First, a Prompt-Level Defense Framework detects and neutralizes adversarial inputs through sanitization, paraphrasing, and adaptive system guarding. Second, a Logit-Based Steering Defense reinforces refusal behavior through inference-time vector steering in safety-sensitive layers. Third, a Domain-Specific Agent Defense employs the MetaGPT framework to enforce structured, role-based collaboration and domain adherence. Experiments on benchmark datasets show substantial reductions in attack success rate, achieving full mitigation under the agent-based defense. Overall, this study highlights how jailbreaks pose a significant security threat to LLMs and identifies key intervention points for prevention, while noting that defense strategies often involve trade-offs between safety, performance, and scalability. Code is available at: this https URL', 'abstract_zh': '大型语言模型（LLMs）依然容易遭受使安全过滤失效并导致有害或不道德行为的逃逸攻击。本文提出了一种系统化的逃逸攻击防御分类方法，覆盖提示级别、模型级别和训练时干预措施，并提出了三种防御策略。首先，一种提示级别防御框架通过清洗、改写和适应性系统保护来检测和中和恶意输入。其次，一种基于logit的引导防御在安全关键层通过推断时向量引导强化拒绝行为。第三，一种专门领域代理防御利用MetaGPT框架强制执行有结构的角色化协作和领域遵从性。在基准数据集上的实验显示攻击成功率显著降低，在基于代理的防御下实现了完全缓解。总体而言，本研究强调了逃逸攻击对LLMs构成的重大安全威胁，并指出了预防的关键干预点，同时指出防御策略往往需要在安全、性能和可扩展性之间做出权衡。代码参见：this https URL', 'title_zh': '负责任的人工智能考虑下防御大型语言模型免受 Jailbreak 攻击'}
{'arxiv_id': 'arXiv:2511.18931', 'title': 'Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs', 'authors': 'Sahil Kale', 'link': 'https://arxiv.org/abs/2511.18931', 'abstract': 'Modern large language models integrate web search to provide real-time answers, yet it remains unclear whether they are efficiently calibrated to use search when it is actually needed. We introduce a benchmark evaluating both the necessity and effectiveness of web access across commercial models with no access to internal states or parameters. The dataset includes a static split of 783 temporally anchored questions answerable from pre-cutoff knowledge, aimed at testing whether models invoke search based on low internal confidence, and a dynamic split of 288 post-cutoff queries designed to test whether models recognise when search is required and retrieve updated information. Web access substantially improves static accuracy for GPT-5-mini and Claude Haiku 4.5, though confidence calibration worsens. On dynamic queries, both models frequently invoke search yet remain below 70 percent accuracy due to weak query formulation. Costs per accuracy-improving call remain low, but returns diminish once initial retrieval fails. Selective invocation helps, but models become overconfident and inconsistent after search. Overall, built-in web search meaningfully improves factual accuracy and can be invoked selectively, yet models remain overconfident, skip retrieval when it is essential, and falter once initial search queries underperform. Taken together, internal web search works better as a good low-latency verification layer than a reliable analytical tool, with clear room for improvement.', 'abstract_zh': '现代大型语言模型结合网络搜索提供实时答案，但尚不清楚它们是否有效地在实际需要时调用搜索。我们 introduced 一个基准，评估商业模型在无内部状态或参数访问的情况下，网络访问的必要性和有效性。该数据集包含783个静态分割的时间锚定问题，这些问题在知识截止点之前可以解答，用于测试模型是否根据较低的内部置信度调用搜索，以及288个动态分割的截止点之后查询，用于测试模型是否能够识别何时需要调用搜索并检索更新的信息。网络访问显著提高了GPT-5-mini和Claude Haiku 4.5的静态准确性，尽管置信度校准变差。在动态查询上，这两种模型经常调用搜索，但由于查询形式较弱，准确性仍低于70%。提高准确性的调用成本较低，但一旦初始检索失败，回报就会减少。选择性调用有所帮助，但调用搜索后模型变得过度自信且不一致。总体而言，内置的网络搜索显著提高了事实准确性，并且可以在必要时选择性调用，但模型仍然过度自信，当检索非常重要时跳过检索，并且一旦初始搜索查询表现不佳便失效。综上所述，内部网络搜索在低延迟验证层方面表现更好，但在可靠分析工具方面仍有改进空间。', 'title_zh': '查阅它：分析现代大语言模型内部网页搜索能力'}
{'arxiv_id': 'arXiv:2511.18924', 'title': 'LLM-Driven Kernel Evolution: Automating Driver Updates in Linux', 'authors': 'Arina Kharlamova, Jiawen Liu, Tianyi Zhang, Xinrui Yang, Humaid Alqasimi, Youcheng Sun, Chun Jason Xue', 'link': 'https://arxiv.org/abs/2511.18924', 'abstract': 'Linux kernel evolution breaks drivers through API/ABI changes, semantic shifts, and security-hardening updates. We introduce DRIVEBENCH, an executable corpus of kernel$\\rightarrow$driver co-evolution cases, and AUTODRIVER, a closed-loop, LLM-driven system for automating driver maintenance. The system integrates prompt engineering, multi-agent collaboration, static analysis, and iterative validation to ensure that generated patches are not only syntactically correct but also functionally and semantically consistent with kernel conventions. The corpus spans v5.10-v6.10 with 235 validated cases drawn from 612 candidates. In evaluation across 55 cases, AUTODRIVER achieves 56.4% compilation success; QEMU-based boot verification indicates that compiled patches preserve driver initialization in most instances. By releasing DRIVEBENCH and tooling, we enable reproducible research and a practical route to continuous, safe co-evolution of drivers with the Linux kernel.', 'abstract_zh': 'Linux内核演化通过API/ABI变化、语义转变和安全强化更新破坏驱动程序：DRIVEBENCH可执行样本库及其自动维护系统AUTODRIVER的研究', 'title_zh': 'LLM驱动的核内核演化：在Linux中自动化驱动程序更新'}
{'arxiv_id': 'arXiv:2511.18903', 'title': 'How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining', 'authors': 'Kairong Luo, Zhenbo Sun, Haodong Wen, Xinyu Shi, Jiarui Cui, Chenyi Dang, Kaifeng Lyu, Wenguang Chen', 'link': 'https://arxiv.org/abs/2511.18903', 'abstract': 'Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods.', 'abstract_zh': '基于 Curriculum 的预训练：通过优化学习率调度和模型平均策略改进高质数据的利用', 'title_zh': '基于课程的学习率衰减如何浪费最佳数据预训练大型语言模型'}
{'arxiv_id': 'arXiv:2511.18890', 'title': 'Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models', 'authors': 'Yonggan Fu, Xin Dong, Shizhe Diao, Matthijs Van keirsbilck, Hanrong Ye, Wonmin Byeon, Yashaswi Karnati, Lucas Liebenwein, Hannah Zhang, Nikolaus Binder, Maksim Khadkevich, Alexander Keller, Jan Kautz, Yingyan Celine Lin, Pavlo Molchanov', 'link': 'https://arxiv.org/abs/2511.18890', 'abstract': "Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively.", 'abstract_zh': '高效部署小语言模型对于具有严格延迟约束的众多实际应用至关重要。尽管先前的小语言模型设计工作主要集中在减少参数数量以实现参数优化的小语言模型，但参数效率并不一定能够转化为实际设备上的同等速度提升。本文旨在识别小语言模型在实际设备上的关键延迟决定因素，并提供在实际设备延迟为主要考虑因素时的小语言模型设计和训练的一般可推广原则和方法。具体而言，我们确定了两个核心架构因素：深度-宽度比和操作符选择。前者对于小批量延迟至关重要，而后者会影响延迟和大批量吞吐量。基于此，我们首先研究延迟最优的深度-宽度比，并发现尽管较深较瘦的模型通常在相同的参数预算下具有更好的准确性，但它们可能不在准确性-延迟权衡前沿之上。接着，我们探索新兴的高效注意力替代方案，评估它们作为候选构建操作符的潜力。使用所识别出的有潜力的操作符，我们构建了一个进化搜索框架，以自动在混合小语言模型中发现延迟最优的操作符组合，从而推进准确性-延迟前沿。除了架构改进，我们还通过一种权重规范化技术进一步改进小语言模型训练，该技术使权重更新更加有效，并提高了最终收敛性。结合这些方法，我们引入了一种新的混合小语言模型家族——Nemotron-Flash，大幅推进了最先进的小语言模型的准确性和效率前沿，例如，相比Qwen3-1.7B/0.6B，平均准确率提高超过5.5%，延迟降低1.3倍/1.9倍，吞吐量提高18.7倍/45.6倍。', 'title_zh': 'Nemotron-Flash：向着最优延迟混合小语言模型'}
{'arxiv_id': 'arXiv:2511.18889', 'title': 'CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation', 'authors': 'Jingqian Zhao, Bingbing Wang, Geng Tu, Yice Zhang, Qianlong Wang, Bin Liang, Jing Li, Ruifeng Xu', 'link': 'https://arxiv.org/abs/2511.18889', 'abstract': 'Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training. Current studies attempt to mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose \\textbf{CoreEval}, a \\textbf{Co}ntamination-\\textbf{re}silient \\textbf{Eval}uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant, up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism is employed to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.', 'abstract_zh': 'CoreEval：一种抗污染的自动化数据更新评价策略', 'title_zh': 'CoreEval: 采用现实世界知识自动构建抗污染数据集以实现可靠的LLM评估'}
{'arxiv_id': 'arXiv:2511.18868', 'title': 'KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit', 'authors': 'Dezhi Ran, Shuxiao Xie, Mingfang Ji, Ziyue Hua, Mengzhou Wu, Yuan Cao, Yuzhe Guo, Yu Hao, Linyi Li, Yitao Hu, Tao Xie', 'link': 'https://arxiv.org/abs/2511.18868', 'abstract': 'High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimization, existing methods struggle with the vast optimization space due to insufficient hardware domain knowledge, failing to effectively balance exploration and exploitation. We present KernelBand, a novel framework that formulates kernel optimization as a hierarchical multi-armed bandit problem, enabling LLM agents to strategically navigate the optimization space by treating kernel selection and optimization strategy application as sequential decision-making processes. Our approach leverages hardware profiling information to identify promising optimization strategies and employs runtime behavior clustering to reduce exploration overhead across kernel candidates. Extensive experiments on TritonBench demonstrate that KernelBand significantly outperforms state-of-the-art methods, achieving superior performance with fewer tokens while exhibiting consistent improvement without saturation as computational resources increase.', 'abstract_zh': '高质量内核对于减少大型语言模型（LLMs）的训练和推理成本至关重要，但传统的内核优化通常需要在硬件架构和软件优化方面具备显著的专业知识。尽管基于LLM的代码生成在复杂优化方面显示出潜力，现有方法由于硬件领域知识不足，难以有效平衡探索与利用，因而难以应对庞大的优化空间。我们提出了一种名为KernelBand的新框架，将内核优化建模为分层多臂老虎机问题，使LLM代理能够通过将内核选择和优化策略应用视为顺序决策过程来战略性地导航优化空间。我们的方法利用硬件分析信息来识别有潜力的优化策略，并通过运行时行为聚类来减少跨内核候选者的探索开销。 extensive实验在TritonBench上表明，KernelBand显著优于现有最佳方法，在较少的token下实现更优性能，并且在计算资源增加时表现出一致的改进而没有饱和。', 'title_zh': 'KernelBand: 基于层次化和硬件感知多臂bandit的LLM优化方法'}
{'arxiv_id': 'arXiv:2511.18860', 'title': 'Generating Reading Comprehension Exercises with Large Language Models for Educational Applications', 'authors': 'Xingyu Huang, Fei Jiang, Jianli Xiao', 'link': 'https://arxiv.org/abs/2511.18860', 'abstract': 'With the rapid development of large language models (LLMs), the applications of LLMs have grown substantially. In the education domain, LLMs demonstrate significant potential, particularly in automatic text generation, which enables the creation of intelligent and adaptive learning content. This paper proposes a new LLMs framework, which is named as Reading Comprehension Exercise Generation (RCEG). It can generate high-quality and personalized English reading comprehension exercises automatically. Firstly, RCEG uses fine-tuned LLMs to generate content candidates. Then, it uses a discriminator to select the best candidate. Finally, the quality of the generated content has been improved greatly. To evaluate the performance of RCEG, a dedicated dataset for English reading comprehension is constructed to perform the experiments, and comprehensive evaluation metrics are used to analyze the experimental results. These metrics include content diversity, factual accuracy, linguistic toxicity, and pedagogical alignment. Experimental results show that RCEG significantly improves the relevance and cognitive appropriateness of the generated exercises.', 'abstract_zh': '随着大规模语言模型的迅速发展，大语言模型的应用日益广泛。在教育领域，大语言模型展现了巨大的潜力，尤其是在自动文本生成方面，能够生成智能且适应性强的学习内容。本文提出了一个新的大语言模型框架，命名为阅读理解练习生成（RCEG），能够自动生成高质量和个性化的英语阅读理解练习。首先，RCEG 使用微调后的语言模型生成内容候选；然后，使用鉴别器选择最佳候选；最后，生成的内容质量得到了显著提高。为了评估 RCEG 的性能，构建了一个专门的英语阅读理解数据集进行实验，并使用综合评价指标分析实验结果。这些指标包括内容多样性、事实准确性、语言毒性以及教学一致性。实验结果表明，RCEG 显著提高了生成练习的相关性和认知适宜性。', 'title_zh': '利用大型语言模型生成阅读理解练习在教育应用中的研究'}
{'arxiv_id': 'arXiv:2511.18854', 'title': 'Time Travel: LLM-Assisted Semantic Behavior Localization with Git Bisect', 'authors': 'Yujing Wang, Weize Hong', 'link': 'https://arxiv.org/abs/2511.18854', 'abstract': 'We present a novel framework that integrates Large Language Models (LLMs) into the Git bisect process for semantic fault localization. Traditional bisect assumes deterministic predicates and binary failure states assumptions often violated in modern software development due to flaky tests, nonmonotonic regressions, and semantic divergence from upstream repositories. Our system augments bisect traversal with structured chain of thought reasoning, enabling commit by commit analysis under noisy conditions. We evaluate multiple open source and proprietary LLMs for their suitability and fine tune DeepSeekCoderV2 using QLoRA on a curated dataset of semantically labeled diffs. We adopt a weak supervision workflow to reduce annotation overhead, incorporating human in the loop corrections and self consistency filtering. Experiments across multiple open source projects show a 6.4 point absolute gain in success rate from 74.2 to 80.6 percent, leading to significantly fewer failed traversals and by experiment up to 2x reduction in average bisect time. We conclude with discussions on temporal reasoning, prompt design, and finetuning strategies tailored for commit level behavior analysis.', 'abstract_zh': '我们提出了一种将大型语言模型（LLMs）集成到Git bisect过程中的新框架，用于语义故障定位。传统的bisect假设确定性的谓词和二元故障状态假设，这些假设在现代软件开发中由于不稳定测试、非单调回归以及与上游仓库的语义差异经常被违反。我们的系统通过结构化的推理链增强bisect遍历，使其能够在噪声条件下实现逐提交的分析。我们评估了多种开源和专有LLM的适用性，并使用QLoRA对DeepSeekCoderV2进行微调，基于一个语义标注的差异数据集。我们采用弱监督工作流以减少注释工作量，结合人工循环修正和自一致过滤。跨多个开源项目的实验显示成功率绝对提高了6.4个百分点，从74.2%提高到80.6%，导致失败遍历显著减少，并且实验中平均bisect时间最多减少一半。最后，我们讨论了时间推理、提示设计以及针对提交级别行为分析的微调策略。', 'title_zh': '时间旅行：基于Git Bisect的LLM辅助语义行为定位'}
{'arxiv_id': 'arXiv:2511.18849', 'title': 'Pre-Filtering Code Suggestions using Developer Behavioral Telemetry to Optimize LLM-Assisted Programming', 'authors': 'Mohammad Nour Al Awad, Sergey Ivanov, Olga Tikhonova', 'link': 'https://arxiv.org/abs/2511.18849', 'abstract': 'Large Language Models (LLMs) are increasingly integrated into code editors to provide AI-powered code suggestions. Yet many of these suggestions are ignored, resulting in wasted computation, increased latency, and unnecessary interruptions. We introduce a lightweight pre-filtering model that predicts the likelihood of suggestion acceptance before invoking the LLM, using only real-time developer telemetry such as typing speed, file navigation, and editing activity. Deployed in a production-grade Visual Studio Code plugin over four months of naturalistic use, our approach nearly doubled acceptance rates (18.4% -> 34.2%) while suppressing 35% of low-value LLM calls. These findings demonstrate that behavioral signals alone can meaningfully improve both user experience and system efficiency in LLM-assisted programming, highlighting the value of timing-aware, privacy-preserving adaptation mechanisms. The filter operates solely on pre-invocation editor telemetry and never inspects code or prompts.', 'abstract_zh': '大型语言模型（LLMs）越来越多地集成到代码编辑器中以提供AI驱动的代码建议，但许多这些建议被忽略，导致计算资源的浪费、响应延迟的增加以及不必要的中断。我们引入了一个轻量级的预过滤模型，在调用LLM之前利用实时开发人员 telemetry（如打字速度、文件导航和编辑活动）预测建议被接受的可能性。在四个月的真实使用中部署于生产级别的Visual Studio Code插件上，我们的方法将接受率几乎翻了一番（从18.4%提升到34.2%），同时抑制了35%的低价值LLM调用。这些发现表明，仅行为信号就能在LLM辅助编程中显著改善用户体验和系统效率，突显了基于时间的、隐私保护的适应机制的价值。过滤器仅在调用前基于编辑器telemetry运行，从不检查代码或提示。', 'title_zh': '使用开发者行为遥测优化大型语言模型辅助编程的代码建议预筛选方法'}
{'arxiv_id': 'arXiv:2511.18842', 'title': 'Optimizing LLM Code Suggestions: Feedback-Driven Timing with Lightweight State Bounds', 'authors': 'Mohammad Nour Al Awad, Sergey Ivanov, Olga Tikhonova', 'link': 'https://arxiv.org/abs/2511.18842', 'abstract': "Large Language Models (LLMs) have transformed code auto-completion by generating context-aware suggestions. Yet, deciding when to present these suggestions remains underexplored, often leading to interruptions or wasted inference calls. We propose an adaptive timing mechanism that dynamically adjusts the delay before offering a suggestion based on real-time developer feedback. Our suggested method combines a logistic transform of recent acceptance rates with a bounded delay range, anchored by a high-level binary prediction of the developer's cognitive state. In a two-month deployment with professional developers, our system improved suggestion acceptance from 4.9% with no delay to 15.4% with static delays, and to 18.6% with adaptive timing-while reducing blind rejections (rejections without being read) from 8.3% to 0.36%. Together, these improvements increase acceptance and substantially reduce wasted inference calls by 75%, making LLM-based code assistants more efficient and cost-effective in practice.", 'abstract_zh': '大型语言模型通过生成上下文相关的建议已革新了代码自动补全，然而决定何时呈现这些建议仍处探索阶段，常常导致中断或浪费推理调用。我们提出了一种自适应时间机制，该机制根据实时开发者反馈动态调整建议出现前的延迟。我们的方法结合了近期接受率的逻辑变换和有界延迟范围，并以高层二进制预测开发者认知状态为基础。在两个月的专业开发者部署中，我们的系统将无延迟建议的接受率从4.9%提高到静态延迟下的15.4%，并进一步提高到自适应时间下的18.6%，同时将盲目拒绝率（未被阅读就拒绝的次数）从8.3%降低到0.36%。这些改进提高了接受率，并将浪费的推理调用减少了75%，使基于大型语言模型的代码助手在实际应用中更加高效和成本效益更高。', 'title_zh': '基于轻量级状态边界和反馈驱动时间优化的LLM代码建议'}
{'arxiv_id': 'arXiv:2511.18808', 'title': 'HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations', 'authors': 'Cao Linxiao, Wang Ruitao, Li Jindong, Zhou Zhipeng, Yang Menglin', 'link': 'https://arxiv.org/abs/2511.18808', 'abstract': 'Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.', 'abstract_zh': '基于图的 retrieval-augmented generation (RAG) 通过引入显式的关系组织增强了结构化推理能力，但这些方法通常依赖于欧几里得嵌入，无法捕捉层次结构深度的几何观念，限制了其对复杂知识图中抽象关系的表示能力。为捕获细粒度 semantics 和全局层次结构，我们提出了 HyperbolicRAG，这是一种将双曲几何 integrates 到基于图的 RAG 中的检索框架。HyperbolicRAG 包含三个关键设计：(1) 一种深度感知的表示学习器，将节点嵌入到共享的庞加莱流形中，使语义相似性与层次包含对齐；(2) 一种无监督对比正则化，确保不同抽象层次之间的几何一致性；(3) 一种互惠排名融合机制，在检索信号中同时利用欧几里得和双曲空间，强调推理时跨空间的一致性。在多个 QA 基准上的广泛实验表明，HyperbolicRAG 在性能上优于竞争对手基线，包括标准 RAG 和图增强基线。', 'title_zh': '双曲RAG：通过双曲表示增强检索增强生成'}
{'arxiv_id': 'arXiv:2511.18743', 'title': 'RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context', 'authors': 'Yu Lei, Shuzheng Si, Wei Wang, Yifei Wu, Gang Chen, Fanchao Qi, Maosong Sun', 'link': 'https://arxiv.org/abs/2511.18743', 'abstract': 'Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.', 'abstract_zh': '大型语言模型正从单轮应答器演化为能够进行持续推理和决策、用于深入研究的工具使用代理。现有系统采用计划、搜索、撰写报告的线性流水线，因缺乏对模型行为和上下文的显式控制而产生错误累积和语境衰退。我们引入了RhinoInsight，这是一种深入研究框架，通过添加两种控制机制来提高稳健性、可追溯性和整体质量，而无需更新参数。首先，可验证检查列表模块将用户需求转换为可追溯和可验证的子目标，引入人工或LLM批评家进行细化，并编译一个层次结构提纲以支撑后续行动并防止不可执行的规划。其次，证据审计模块结构化搜索内容，迭代更新提纲并剔除噪声上下文，而批评家对高质量证据进行排名和绑定，以确保可验证性和减少幻觉。我们的实验表明，RhinoInsight在深入研究任务上取得了最先进的性能，同时在深入搜索任务上保持竞争力。', 'title_zh': 'RhinoInsight: 通过控制机制提高模型行为和上下文的深入研究'}
{'arxiv_id': 'arXiv:2511.18735', 'title': 'Thinking Ahead: Foresight Intelligence in MLLMs and World Models', 'authors': 'Zhantao Gong, Liaoyuan Fan, Qing Guo, Xun Xu, Xulei Yang, Shijie Li', 'link': 'https://arxiv.org/abs/2511.18735', 'abstract': 'In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.', 'abstract_zh': '在本工作中，我们将预见智能定义为预见和解释未来事件的能力——这种能力对于自动驾驶等应用至关重要，但现有研究却很少关注这一点。为了弥合这一差距，我们引入了FSU-QA，这是一个新的视觉问答（VQA）数据集，专门设计用于激发和评估预见智能。利用FSU-QA，我们在预见导向的任务中首次对最先进的视觉-语言模型（VLMs）进行了全面研究，揭示了当前模型在推理未来情况方面仍存在困难。除了作为基准测试之外，FSU-QA 还通过测量生成预测的语义连贯性来评估世界模型，其性能通过增强VLMs的方法量化。我们的实验进一步表明，FSU-QA 可以有效增强预见推理：即使是对FSU-QA进行微调的小型VLMs也大幅超越了更大的先进模型。这些发现将FSU-QA定位为开发能够真正预见和理解未来事件的下一代模型的原理性基础。', 'title_zh': '思勰之策：MLLMs与世界模型中的预见智能'}
{'arxiv_id': 'arXiv:2511.18696', 'title': 'Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models', 'authors': 'Wangjiaxuan Xin', 'link': 'https://arxiv.org/abs/2511.18696', 'abstract': "This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.", 'abstract_zh': 'Empathetic Cascading Networks框架：一种增强大型语言模型同理心和包容性的多阶段提示方法', 'title_zh': '共情级联网络：一种减少大型语言模型社会偏见的多阶段提示技术'}
{'arxiv_id': 'arXiv:2511.18653', 'title': 'FHE-Agent: Automating CKKS Configuration for Practical Encrypted Inference via an LLM-Guided Agentic Framework', 'authors': 'Nuo Xu, Zhaoting Gong, Ran Ran, Jinwei Tang, Wujie Wen, Caiwen Ding', 'link': 'https://arxiv.org/abs/2511.18653', 'abstract': 'Fully Homomorphic Encryption (FHE), particularly the CKKS scheme, is a promising enabler for privacy-preserving MLaaS, but its practical deployment faces a prohibitive barrier: it heavily relies on domain expertise. Configuring CKKS involves a tightly coupled space of ring dimensions, modulus chains, and packing layouts. Without deep cryptographic knowledge to navigate these interactions, practitioners are restricted to compilers that rely on fixed heuristics. These "one-shot" tools often emit rigid configurations that are either severely over-provisioned in latency or fail to find a feasible solution entirely for deeper networks.\nWe present FHE-Agent, an agentic framework that automates this expert reasoning process. By coupling a Large Language Model (LLM) controller with a deterministic tool suite, FHE-Agent decomposes the search into global parameter selection and layer-wise bottleneck repair. The agents operate within a multi-fidelity workflow, pruning invalid regimes using cheap static analysis and reserving expensive encrypted evaluations for the most promising candidates.\nWe instantiate FHE-Agent on the Orion compiler and evaluate it on standard benchmarks (MLP, LeNet, LoLa) and deeper architectures (AlexNet). FHE-Agent consistently achieves better precision and lower latency than naïve search strategies. Crucially, it automatically discovers feasible, 128-bit secure configurations for complex models where baseline heuristics and one-shot prompts fail to produce a valid setup.', 'abstract_zh': 'FHE-Agent：一种自动化的基于代理的FHE方案优化框架', 'title_zh': 'FHE-Agent: 通过LLM引导的代理框架自动化CKKS配置以实现实用的加密推理'}
{'arxiv_id': 'arXiv:2511.18643', 'title': 'Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost', 'authors': 'Haojun Xia, Xiaoxia Wu, Jisen Li, Robert Wu, Junxiong Wang, Jue Wang, Chenxi Li, Aman Singhal, Alay Dilipbhai Shah, Alpay Ariyak, Donglin Zhuang, Zhongzhu Zhou, Ben Athiwaratkun, Zhen Zheng, Shuaiwen Leon Song', 'link': 'https://arxiv.org/abs/2511.18643', 'abstract': 'The KV cache is a dominant memory bottleneck for LLM inference. While 4-bit KV quantization preserves accuracy, 2-bit often degrades it, especially on long-context reasoning. We close this gap via an algorithm-system co-design for mixed-precision KV caching: Kitty. On the algorithm side, extensive experiments show that Dynamic Channel-wise Precision Boost -- which ranks Key-cache channels by sensitivity and keeps only a small fraction at higher precision -- maintains near-zero loss in accuracy drop while approaching 2-bit memory. The main challenge is handling dynamic 4-bit channel boosts while keeping the page layout coalesced and the dequantization uniform, with no scattered reads or hard-coded masks. Kitty addresses these issues by decompose each mixed-precision Key page into two tensors with unified 2-bit precision. Based on this, Kitty provides a page-centric KV layout, Triton-compatible page dequantization kernels, and a lightweight runtime pipeline that preserves coalescing and avoids divergence. Across seven tasks and two model families (Qwen3, LLaMA3), Kitty cuts KV memory by nearly 8x with negligible accuracy loss, enabling up to 8x larger batches and 2.1x-4.1x higher throughput under the same memory budget. We release the full implementation of Kitty at this https URL.', 'abstract_zh': 'Kitty：一种用于混精度KV缓存的算法-系统联合设计', 'title_zh': 'Kitty: 准确高效的2位KV缓存量化及其动态通道精度增强'}
{'arxiv_id': 'arXiv:2511.18635', 'title': 'No Free Lunch in Language Model Bias Mitigation? Targeted Bias Reduction Can Exacerbate Unmitigated LLM Biases', 'authors': 'Shireen Chand, Faith Baca, Emilio Ferrara', 'link': 'https://arxiv.org/abs/2511.18635', 'abstract': 'Large Language Models (LLMs) inherit societal biases from their training data, potentially leading to harmful or unfair outputs. While various techniques aim to mitigate these biases, their effects are often evaluated only along the dimension of the bias being targeted. This work investigates the cross-category consequences of targeted bias mitigation. We study four bias mitigation techniques applied across ten models from seven model families, and we explore racial, religious, profession- and gender-related biases. We measure the impact of debiasing on model coherence and stereotypical preference using the StereoSet benchmark. Our results consistently show that while targeted mitigation can sometimes reduce bias in the intended dimension, it frequently leads to unintended and often negative consequences in others, such as increasing model bias and decreasing general coherence. These findings underscore the critical need for robust, multi-dimensional evaluation tools when examining and developing bias mitigation strategies to avoid inadvertently shifting or worsening bias along untargeted axes.', 'abstract_zh': '大型语言模型从训练数据中继承社会偏见，可能导致有害或不公平的输出。虽然有多种技术试图缓解这些偏见，但它们的效果通常仅沿目标偏见维度进行评估。本研究表明了目标偏见缓解的跨类别后果。我们考察了七大家族的十种模型上应用的四种偏见缓解技术，并探讨了种族、宗教、职业和性别相关的偏见。我们使用StereoSet基准评估去偏对于模型连贯性和刻板偏好度的影响。我们的结果显示，虽然有时目标缓解可以在目标维度减少偏见，但通常会导致其他维度的无意且往往是负面的后果，如增加模型偏见和降低普遍连贯性。这些发现强调了在检查和发展偏见缓解策略时需要使用稳健的多维度评估工具，以避免无意地在未目标轴上转移或加剧偏见。', 'title_zh': '语言模型偏差减轻并无免费午餐？靶向偏差减少可能加剧未减轻的LLM偏差'}
{'arxiv_id': 'arXiv:2511.18630', 'title': 'Majority of the Bests: Improving Best-of-N via Bootstrapping', 'authors': 'Amin Rakhsha, Kanika Madan, Tianyu Zhang, Amir-massoud Farahmand, Amir Khasahmadi', 'link': 'https://arxiv.org/abs/2511.18630', 'abstract': "Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.", 'abstract_zh': '从大型语言模型中采样多个输出并选择最频繁（自一致性）或最高得分类（Best-of-N）的方法在具有离散最终答案的任务中实现更高准确率方面非常流行。Best-of-N（BoN）选择具有最高奖励的输出，如果奖励模型的奖励是完美的，BoN通常能实现近乎完美的准确率。然而，使用奖励模型的不完美奖励时，BoN无法可靠地找到正确答案，并且其性能会大幅下降。我们考虑BoN输出的分布情况，并指出，在不完美的奖励下，正确答案虽然通常不会有很大的概率，但往往是更有可能的结果。这表明该分布的众数比其采样结果更可靠地正确。基于这一想法，我们提出了Best-of-the-Bests（BoB），一种新的选择机制，通过自助法估计BoN的输出分布并选择其众数。在五种基准、三种不同的基础LLM和两种奖励模型上的实验结果证明，在30种设置中有25种设置中，BoB相对于BoN表现出一致的性能改进。我们还提供了关于自助法一致性的一些理论结果。BoB作为BoN和自一致性的简单而强大的替代方案，更广泛地说，促进了对更加精细选择机制的研究。', 'title_zh': '多数最佳：通过自助法改进Best-of-N'}
{'arxiv_id': 'arXiv:2511.18589', 'title': 'Strategic Decision Framework for Enterprise LLM Adoption', 'authors': 'Michael Trusov, Minha Hwang, Zainab Jamal, Swarup Chandra', 'link': 'https://arxiv.org/abs/2511.18589', 'abstract': 'Organizations are rapidly adopting Large Language Models (LLMs) to transform their operations, yet they lack clear guidance on key decisions for adoption and implementation. While LLMs offer powerful capabilities in content generation, assisted coding, and process automation, businesses face critical challenges in data security, LLM solution development approach, infrastructure requirements, and deployment strategies. Healthcare providers must protect patient data while leveraging LLMs for medical analysis, financial institutions need to balance automated customer service with regulatory compliance, and software companies seek to enhance development productivity while maintaining code security.\nThis article presents a systematic six-step decision framework for LLM adoption, helping organizations navigate from initial application selection to final deployment. Based on extensive interviews and analysis of successful and failed implementations, our framework provides practical guidance for business leaders to align technological capabilities with business objectives. Through key decision points and real-world examples from both B2B and B2C contexts, organizations can make informed decisions about LLM adoption while ensuring secure and efficient integration across various use cases, from customer service automation to content creation and advanced analytics.', 'abstract_zh': '组织正在快速采用大型语言模型（LLMs）以转型其运营，但在采用和实施方面的关键决策缺乏明确指导。虽然LLMs提供了内容生成、辅助编码和过程自动化的强大能力，但企业面临数据安全、LLM解决方案开发方法、基础设施需求和部署策略等关键挑战。医疗提供者必须在利用LLMs进行医疗分析的同时保护患者数据，金融机构需要在提供自动化客户服务体系与遵守监管要求之间找到平衡，而软件公司则寻求提高开发效率并同时确保代码安全。\n\n本文提出了一种系统性的六步决策框架，帮助组织从初步应用选择到最终部署的过程中导航。基于广泛访谈和对成功与失败实施案例的分析，我们的框架为业务领导人提供了将技术能力与业务目标相结合的实用指导。通过关键决策点和来自B2B及B2C背景的真实案例，组织可以在确保在各种应用场景（从客户服务自动化到内容生成和高级分析）中安全高效集成的同时，做出关于LLM采用的知情决策。', 'title_zh': '企业LLM采用的战略决策框架'}
{'arxiv_id': 'arXiv:2511.18491', 'title': 'MindEval: Benchmarking Language Models on Multi-turn Mental Health Support', 'authors': "José Pombal, Maya D'Eon, Nuno M. Guerreiro, Pedro Henrique Martins, António Farinhas, Ricardo Rei", 'link': 'https://arxiv.org/abs/2511.18491', 'abstract': 'Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data.', 'abstract_zh': '基于AI聊天机器人的心理健康支持需求激增，尽管当前系统存在若干局限性，如阿谀奉承或过度验证，以及强化不良信念。创建更好系统的核心障碍在于缺乏能够捕捉真实治疗互动复杂性的基准。大多数现有基准要么仅通过多项选择题测试临床知识，要么单独评估单个响应。为弥合这一差距，我们提出MindEval框架，该框架与拥有博士学位的注册临床心理学家合作设计，用于自动评估语言模型在现实的多轮心理健康治疗对话中的表现。通过病人模拟和使用大语言模型的自动评估，该框架通过其完全自动且模型无关的设计平衡了防作弊性和可重复性。我们首先定量验证了模拟病人的真实性，显示了自动评估和人类专家判断之间的强烈相关性。然后，我们评估了12个最先进的大语言模型，并显示所有模型均表现不佳，平均得分低于6分中的4分，特别是在问题特定模式的沟通方面表现尤为薄弱。值得注意的是，推理能力和模型规模并不保证更好的性能，系统在更长的互动中或在支持严重症状的病人时会恶化。我们发布了所有代码、提示和人类评估数据。', 'title_zh': 'MindEval：多轮心理健康支持任务下语言模型的benchmarking'}
{'arxiv_id': 'arXiv:2511.18488', 'title': 'Evaluating perturbation robustnessof generative systems that use COBOL code inputs', 'authors': 'Samuel Ackerman, Wesam Ibraheem, Orna Raz, Marcel Zalmanovici', 'link': 'https://arxiv.org/abs/2511.18488', 'abstract': "Systems incorporating large language models (LLMs) as a component are known to be sensitive (i.e., non-robust) to minor input variations that do not change the meaning of the input; such sensitivity may reduce the system's usefulness. Here, we present a framework to evaluate robustness of systems using COBOL code as input; our application is translation between COBOL and Java programming languages, but the approach extends to other tasks such as code generation or explanation. Targeting robustness of systems with COBOL as input is essential yet challenging. Many business-critical applications are written in COBOL, yet these are typically proprietary legacy applications and their code is unavailable to LLMs for training. We develop a library of COBOL paragraph and full-program perturbation methods, and create variant-expanded versions of a benchmark dataset of examples for a specific task. The robustness of the LLM-based system is evaluated by measuring changes in values of individual and aggregate metrics calculated on the system's outputs. Finally, we present a series of dynamic table and chart visualization dashboards that assist in debugging the system's outputs, and monitoring and understanding root causes of the system's sensitivity to input variation. These tools can be further used to improve the system by, for instance, indicating variations that should be handled by pre-processing steps.", 'abstract_zh': '包含大型语言模型（LLMs）的系统对不改变输入意义的小规模输入变化较为敏感（即不具备鲁棒性），这种敏感性可能降低系统的实用性。本文提出了一种使用COBOL代码评估系统鲁棒性的框架；我们的应用是COBOL与Java编程语言之间的翻译，但该方法也可应用于其他任务，如代码生成或解释。将鲁棒性应用于以COBOL作为输入的目标是至关重要的，但也很具有挑战性。许多关键业务应用程序使用COBOL编写，然而这些通常是专有的遗留应用程序，其代码无法供训练LLMs使用。我们开发了一组COBOL段落和完整程序的扰动方法，并创建了基准数据集中特定任务示例的扩展变体版本。通过测量系统输出的个体和聚合指标值的变化来评估LLM基系统本身的鲁棒性。最后，我们展示了系列动态表和图表可视化仪表板，以辅助调试系统的输出，并监测和理解系统对输入变化敏感的根本原因。这些工具可以进一步用于改进系统，例如，指示需要由预处理步骤处理的变化。', 'title_zh': '评估使用COBOL代码输入的生成系统扰动鲁棒性'}
{'arxiv_id': 'arXiv:2511.18335', 'title': 'OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas', 'authors': 'James Y. Huang, Wenxuan Zhou, Nan Xu, Fei Wang, Qin Liu, Sheng Zhang, Hoifung Poon, Muhao Chen', 'link': 'https://arxiv.org/abs/2511.18335', 'abstract': "The ability of Large Language Models (LLMs) to generate structured outputs that follow arbitrary schemas is crucial to a wide range of downstream tasks that require diverse structured representations of results such as information extraction, table generation, and function calling. While modern LLMs excel in generating unstructured responses in natural language, whether this advancement translates to a strong performance on text-to-structure tasks remains unclear. To bridge this gap, we first introduce OmniStruct, a comprehensive benchmark for assessing LLMs' capabilities on diverse text-to-structure tasks such as information extraction, table generation, and function calling. We build OmniStruct by identifying existing datasets across a wide range of tasks that are suitable for a structured answer format, and adapting them under a unified text-to-structure problem setting. To facilitate the development of efficient text-to-structure models, we collect high-quality training data via synthetic task generation. Without using any supervised data for OmniStruct tasks, our experiments demonstrate the possibility of fine-tuning much smaller models on synthetic data into universal structured generation models that can rival the performance of GPT-4o.", 'abstract_zh': '大型语言模型生成遵循任意模式的结构化输出能力对于要求多样化结构化结果表示的信息提取、表生成和函数调用等下游任务至关重要。虽然现代大型语言模型在生成自然语言非结构化响应方面表现出色，但这一进展是否能转化为文本到结构任务的强性能尚不清楚。为了弥合这一差距，我们首先引入了OmniStruct，这是一个全面的基准测试，用于评估大型语言模型在信息提取、表生成和函数调用等多样化文本到结构任务上的能力。我们通过识别适用于结构化答案格式的广泛任务的数据集，并在统一的文本到结构问题设置下对它们进行调整，来构建OmniStruct。为了促进高效文本到结构模型的发展，我们通过合成任务生成收集高质量的训练数据。在不使用任何监督数据的情况下，我们的实验展示了将小型模型微调为可用于生成与GPT-4o性能相媲美的通用结构化模型的可能性。', 'title_zh': 'OmniStruct: 跨多种模式的通用文本到结构生成'}
{'arxiv_id': 'arXiv:2511.18274', 'title': 'Clinician-Directed Large Language Model Software Generation for Therapeutic Interventions in Physical Rehabilitation', 'authors': 'Edward Kim, Yuri Cho, Jose Eduardo E. Lima, Julie Muccini, Jenelle Jindal, Alison Scheid, Erik Nelson, Seong Hyun Park, Yuchen Zeng, Alton Sturgis, Caesar Li, Jackie Dai, Sun Min Kim, Yash Prakash, Liwen Sun, Isabella Hu, Hongxuan Wu, Daniel He, Wiktor Rajca, Cathra Halabi, Maarten Lansberg, Bjoern Hartmann, Sanjit A. Seshia', 'link': 'https://arxiv.org/abs/2511.18274', 'abstract': "Digital health interventions are increasingly used in physical and occupational therapy to deliver home exercise programs via sensor equipped devices such as smartphones, enabling remote monitoring of adherence and performance. However, digital interventions are typically programmed as software before clinical encounters as libraries of parametrized exercise modules targeting broad patient populations. At the point of care, clinicians can only select modules and adjust a narrow set of parameters like repetitions, so patient specific needs that emerge during encounters, such as distinct movement limitations, and home environments, are rarely reflected in the software. We evaluated a digital intervention paradigm that uses large language models (LLMs) to translate clinicians' exercise prescriptions into intervention software. In a prospective single arm feasibility study with 20 licensed physical and occupational therapists and a standardized patient, clinicians created 40 individualized upper extremity programs (398 instructions) that were automatically translated into executable software. Our results show a 45% increase in the proportion of personalized prescriptions that can be implemented as software compared with a template based benchmark, with unanimous consensus among therapists on ease of use. The LLM generated software correctly delivered 99.78% (397/398) of instructions as prescribed and monitored performance with 88.4% (352/398) accuracy, with 90% (18/20) of therapists judged it safe to interact with patients, and 75% (15/20) expressed willingness to adopt it. To our knowledge, this is the first prospective evaluation of clinician directed intervention software generation with LLMs in healthcare, demonstrating feasibility and motivating larger trials to assess clinical effectiveness and safety in real patient populations.", 'abstract_zh': '大规模语言模型驱动的临床导向数字干预软件生成在健康care中的初步评估：提高个性化处方实现率并确保患者安全', 'title_zh': '临床医生导向的大语言模型软件生成在物理康复中的治疗干预'}
{'arxiv_id': 'arXiv:2511.18261', 'title': 'LLM Reasoning for Cold-Start Item Recommendation', 'authors': 'Shijun Li, Yu Wang, Jin Wang, Ying Li, Joydeep Ghosh, Anne Cocos', 'link': 'https://arxiv.org/abs/2511.18261', 'abstract': "Large Language Models (LLMs) have shown significant potential for improving recommendation systems through their inherent reasoning capabilities and extensive knowledge base. Yet, existing studies predominantly address warm-start scenarios with abundant user-item interaction data, leaving the more challenging cold-start scenarios, where sparse interactions hinder traditional collaborative filtering methods, underexplored. To address this limitation, we propose novel reasoning strategies designed for cold-start item recommendations within the Netflix domain. Our method utilizes the advanced reasoning capabilities of LLMs to effectively infer user preferences, particularly for newly introduced or rarely interacted items. We systematically evaluate supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches that combine both methods to optimize recommendation performance. Extensive experiments on real-world data demonstrate significant improvements in both methodological efficacy and practical performance in cold-start recommendation contexts. Remarkably, our reasoning-based fine-tuned models outperform Netflix's production ranking model by up to 8% in certain cases.", 'abstract_zh': '大型语言模型（LLMs）通过其固有的推理能力和广泛的知识库，在提高推荐系统方面展现出了显著的潜力。然而，现有的研究主要集中在拥有丰富用户-项目交互数据的暖启动场景上，而冷启动场景则被忽视，这类场景中稀疏的交互数据阻碍了传统的协作过滤方法。为了解决这一局限，我们提出了适用于Netflix域冷启动项目推荐的新型推理策略。我们的方法利用LLMs的高级推理能力，有效地推断用户偏好，尤其是对于新引入或 rarely 交互的项目。我们系统地评估了监督微调、基于强化学习的微调以及结合两种方法的混合方法，以优化推荐性能。在实际数据上的广泛实验表明，在冷启动推荐场景中，我们的推理驱动微调模型在方法有效性和实际性能方面均取得了显著的改善。在某些情况下，我们的推理驱动微调模型比Netfilx的生产排名模型高出百分之八。', 'title_zh': '冷启动商品推荐的LLM推理'}
{'arxiv_id': 'arXiv:2511.18239', 'title': 'Can LLMs Help Allocate Public Health Resources? A Case Study on Childhood Lead Testing', 'authors': 'Mohamed Afane, Ying Wang, Juntao Chen', 'link': 'https://arxiv.org/abs/2511.18239', 'abstract': 'Public health agencies face critical challenges in identifying high-risk neighborhoods for childhood lead exposure with limited resources for outreach and intervention programs. To address this, we develop a Priority Score integrating untested children proportions, elevated blood lead prevalence, and public health coverage patterns to support optimized resource allocation decisions across 136 neighborhoods in Chicago, New York City, and Washington, D.C. We leverage these allocation tasks, which require integrating multiple vulnerability indicators and interpreting empirical evidence, to evaluate whether large language models (LLMs) with agentic reasoning and deep research capabilities can effectively allocate public health resources when presented with structured allocation scenarios. LLMs were tasked with distributing 1,000 test kits within each city based on neighborhood vulnerability indicators. Results reveal significant limitations: LLMs frequently overlooked neighborhoods with highest lead prevalence and largest proportions of untested children, such as West Englewood in Chicago, while allocating disproportionate resources to lower-priority areas like Hunts Point in New York City. Overall accuracy averaged 0.46, reaching a maximum of 0.66 with ChatGPT 5 Deep Research. Despite their marketed deep research capabilities, LLMs struggled with fundamental limitations in information retrieval and evidence-based reasoning, frequently citing outdated data and allowing non-empirical narratives about neighborhood conditions to override quantitative vulnerability indicators.', 'abstract_zh': '公共卫生机构在资源有限的情况下，面临识别高风险儿童铅暴露社区的重大挑战。为了应对这一问题，我们开发了一种优先评分系统，综合考虑未经检测儿童的比例、血液铅水平升高率以及公共卫生覆盖模式，以支持在芝加哥、纽约市和华盛顿特区136个社区的资源优化分配决策。利用这些分配任务，需要整合多种脆弱性指标并解读实证证据，我们评估了具备机构推理能力和深入研究能力的大规模语言模型（LLMs）在呈现结构化分配场景时，能否有效分配公共卫生资源。实验结果显示，尽管LLMs的任务是基于社区脆弱性指标分配1000个测试套件，但它们经常忽略了如芝加哥的韦斯特英格兰伍德等铅暴露率最高且未经检测儿童比例最大的社区，而过度分配资源给低优先级区域，如纽约市的亨特斯点。总体准确率为0.46，最高达到0.66（使用ChatGPT 5 Deep Research）。尽管宣传具有深度研究能力，但LLMs在信息检索和基于证据的推理方面仍存在根本性局限性，频繁引用过时数据，并且非实证性关于社区状况的叙述经常取代定量脆弱性指标。', 'title_zh': 'LLM在公共健康资源配置中的作用：基于儿童血铅检测案例的研究'}
{'arxiv_id': 'arXiv:2511.18221', 'title': 'Enhancing Large Language Models for Automated Homework Assessment in Undergraduate Circuit Analysis', 'authors': 'Liangliang Chen, Huiru Xie, Zhihao Qin, Yiming Guo, Jacqueline Rohde, Ying Zhang', 'link': 'https://arxiv.org/abs/2511.18221', 'abstract': "This research full paper presents an enhancement pipeline for large language models (LLMs) in assessing homework for an undergraduate circuit analysis course, aiming to improve LLMs' capacity to provide personalized support to electrical engineering students. Existing evaluations have demonstrated that GPT-4o possesses promising capabilities in assessing student homework in this domain. Building on these findings, we enhance GPT-4o's performance through multi-step prompting, contextual data augmentation, and the incorporation of targeted hints. These strategies effectively address common errors observed in GPT-4o's responses when using simple prompts, leading to a substantial improvement in assessment accuracy. Specifically, the correct response rate for GPT-4o increases from 74.71% to 97.70% after applying the enhanced prompting and augmented data on entry-level circuit analysis topics. This work lays a foundation for the effective integration of LLMs into circuit analysis instruction and, more broadly, into engineering education.", 'abstract_zh': '本文呈现了一种针对大型语言模型（LLMs）的增强管道，用于评估本科电路分析课程的作业，旨在提高LLMs为电气工程学生提供个性化支持的能力。已有评估表明，GPT-4o在该领域评估学生作业方面表现出有前途的能力。在此基础上，我们通过多步提示、上下文数据增强以及引入有针对性的提示来增强GPT-4o的性能。这些策略有效解决了使用简单提示时GPT-4o响应中观察到的常见错误，显著提高了评估准确性。具体而言，应用增强提示和增强数据后，GPT-4o在初级电路分析主题中的正确响应率从74.71%提高到97.70%。这项工作为有效将LLMs整合到电路分析教学中奠定了基础，并更广泛地为工程教育奠定了基础。', 'title_zh': '增强大型语言模型以实现本科电路分析作业的自动化评估'}
{'arxiv_id': 'arXiv:2511.18165', 'title': 'Towards a General Framework for HTN Modeling with LLMs', 'authors': 'Israel Puerta-Merino, Carlos Núñez-Molina, Pablo Mesejo, Juan Fernández-Olivares', 'link': 'https://arxiv.org/abs/2511.18165', 'abstract': 'The use of Large Language Models (LLMs) for generating Automated Planning (AP) models has been widely explored; however, their application to Hierarchical Planning (HP) is still far from reaching the level of sophistication observed in non-hierarchical architectures. In this work, we try to address this gap. We present two main contributions. First, we propose L2HP, an extension of L2P (a library to LLM-driven PDDL models generation) that support HP model generation and follows a design philosophy of generality and extensibility. Second, we apply our framework to perform experiments where we compare the modeling capabilities of LLMs for AP and HP. On the PlanBench dataset, results show that parsing success is limited but comparable in both settings (around 36\\%), while syntactic validity is substantially lower in the hierarchical case (1\\% vs. 20\\% of instances). These findings underscore the unique challenges HP presents for LLMs, highlighting the need for further research to improve the quality of generated HP models.', 'abstract_zh': '大型语言模型（LLMs）在生成分层规划（HP）模型中的应用仍然远未达到非分层架构的 sophistication 水平：L2HP及其在自动规划与分层规划中的比较研究', 'title_zh': '面向HTN建模的LLM通用框架'}
{'arxiv_id': 'arXiv:2511.18084', 'title': 'The Alignment Paradox of Medical Large Language Models in Infertility Care: Decoupling Algorithmic Improvement from Clinical Decision-making Quality', 'authors': 'Dou Liu, Ying Long, Sophia Zuoqiu, Kaipeng Xie, Runze Yang, Di Liu, Kang Li, Yiting Lin, Hanyi Liu, Rong Yin, Tian Tang', 'link': 'https://arxiv.org/abs/2511.18084', 'abstract': "Large language models (LLMs) are increasingly adopted in clinical decision support, yet aligning them with the multifaceted reasoning pathways of real-world medicine remains a major challenge. Using more than 8,000 infertility treatment records, we systematically evaluate four alignment strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Group Relative Policy Optimization (GRPO), and In-Context Learning (ICL) through a dual-layer framework combining automatic benchmarks with blinded doctor-in-the-loop assessments. GRPO achieves the highest algorithmic accuracy across multiple decision layers, confirming the value of reinforcement-based optimization for structured prediction tasks. However, clinicians consistently prefer the SFT model, citing clearer reasoning processes (p = 0.035) and higher therapeutic feasibility (p = 0.019). In blinded pairwise comparisons, SFT attains the highest winning rate (51.2%), outperforming both GRPO (26.2%) and even physicians' original decisions (22.7%). These results reveal an alignment paradox: algorithmic improvements do not necessarily translate into higher clinical trust, and may diverge from human-centered preferences. Our findings highlight the need for alignment strategies that prioritize clinically interpretable and practically feasible reasoning, rather than solely optimizing decision-level accuracy.", 'abstract_zh': '大型语言模型在临床决策支持中的多维度对齐挑战：基于超过8000份不孕治疗记录的四策略系统评估', 'title_zh': '医学大型语言模型在不孕不育护理中的对齐悖论：算法改进与临床决策质量脱钩'}
{'arxiv_id': 'arXiv:2511.18038', 'title': 'MASTEST: A LLM-Based Multi-Agent System For RESTful API Tests', 'authors': 'Xiaoke Han, Hong Zhu', 'link': 'https://arxiv.org/abs/2511.18038', 'abstract': 'Testing RESTful API is increasingly important in quality assurance of cloud-native applications. Recent advances in machine learning (ML) techniques have demonstrated that various testing activities can be performed automatically by large language models (LLMs) with reasonable accuracy. This paper develops a multi-agent system called MASTEST that combines LLM-based and programmed agents to form a complete tool chain that covers the whole workflow of API test starting from generating unit and system test scenarios from API specification in the OpenAPI Swagger format, to generating of Pytest test scripts, executing test scripts to interact with web services, to analysing web service response messages to determine test correctness and calculate test coverage. The system also supports the incorporation of human testers in reviewing and correcting LLM generated test artefacts to ensure the quality of testing activities. MASTEST system is evaluated on two LLMs, GPT-4o and DeepSeek V3.1 Reasoner with five public APIs. The performances of LLMs on various testing activities are measured by a wide range of metrics, including unit and system test scenario coverage and API operation coverage for the quality of generated test scenarios, data type correctness, status code coverage and script syntax correctness for the quality of LLM generated test scripts, as well as bug detection ability and usability of LLM generated test scenarios and scripts. Experiment results demonstrated that both DeepSeek and GPT-4o achieved a high overall performance. DeepSeek excels in data type correctness and status code detection, while GPT-4o performs best in API operation coverage. For both models, LLM generated test scripts maintained 100\\% syntax correctness and only required minimal manual edits for semantic correctness. These findings indicate the effectiveness and feasibility of MASTEST.', 'abstract_zh': '基于多智能体系统的RESTful API自动测试方法研究', 'title_zh': 'MASTEST: 一个基于LLM的多_agent系统用于RESTful API测试'}
{'arxiv_id': 'arXiv:2511.17962', 'title': 'VITAL: Vision-Encoder-centered Pre-training for LMMs in Visual Quality Assessment', 'authors': 'Ziheng Jia, Linhan Cao, Jinliang Han, Zicheng Zhang, Jiaying Qian, Jiarui Wang, Zijian Chen, Guangtao Zhai, Xiongkuo Min', 'link': 'https://arxiv.org/abs/2511.17962', 'abstract': "Developing a robust visual quality assessment (VQualA) large multi-modal model (LMM) requires achieving versatility, powerfulness, and transferability.\nHowever, existing VQualA LMMs typically focus on a single task and rely on full-parameter fine-tuning, which makes them prone to overfitting on specific modalities or task types, thereby limiting their generalization capacity and transferability. To address this, we propose a vision-encoder-centered generative pre-training pipeline and develop the VITAL-Series LMMs. (1) We adopt a machine-executed annotation-scrutiny paradigm, constructing over 4.5M vision-language (VL) pairs-the largest VQualA training dataset to date. (2) We employ a multi-task training workflow that simultaneously enhances the model's quantitative scoring precision and strengthens its capability for quality interpretation across both image and video modalities. (3) Building upon the vision encoder, we realize an efficient model zoo extension: the model zoo exhibits strong zero-shot performance, and each paired decoder requires only a swift warm-up using less than 1/1000 of the pre-training data to achieve performance comparable to the fully trained counterpart. Overall, our work lays a cornerstone for advancing toward the foundation LMM for VQualA.", 'abstract_zh': '开发稳健的视觉质量评估大型多模态模型（LMM）要求实现多样性、强效性和转移性。然而，现有的VQualA LMMs通常专注于单一任务，并依赖于全参数微调，这使得它们容易在特定模态或任务类型上过度拟合，从而限制了它们的泛化能力和转移性。为此，我们提出了一种基于视觉编码器的生成预训练流水线，并开发了VITAL系列LMMs。（1）我们采用机器执行的注释审查范式，构建了超过450万的视觉-语言（VL）对——目前最大的VQualA训练数据集。（2）我们采用多任务训练工作流，同时提高模型的定量评分精度，并增强其在图像和视频模态上质量解释的能力。（3）基于视觉编码器，我们实现了高效的模型动物园扩展：模型动物园展现了强大的零样本性能，每个配对的解码器只需要使用不到预训练数据的1/1000进行快速预热，即可达到与完全训练版本相当的性能。总体而言，我们的工作为朝着基础LMM的VQualA奠定了基石。', 'title_zh': 'VITAL: 以视觉编码器为中心的预训练方法在视觉质量评估中的应用'}
{'arxiv_id': 'arXiv:2511.17946', 'title': 'Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models', 'authors': 'Shuo Zhang, Fabrizio Gotti, Fengran Mo, Jian-Yun Nie', 'link': 'https://arxiv.org/abs/2511.17946', 'abstract': "Hallucination in large language models (LLMs) is a fundamental challenge, particularly in open-domain question answering. Prior work attempts to detect hallucination with model-internal signals such as token-level entropy or generation consistency, while the connection between pretraining data exposure and hallucination is underexplored. Existing studies show that LLMs underperform on long-tail knowledge, i.e., the accuracy of the generated answer drops for the ground-truth entities that are rare in pretraining. However, examining whether data coverage itself can serve as a detection signal is overlooked. We propose a complementary question: Does lexical training-data coverage of the question and/or generated answer provide additional signal for hallucination detection? To investigate this, we construct scalable suffix arrays over RedPajama's 1.3-trillion-token pretraining corpus to retrieve $n$-gram statistics for both prompts and model generations. We evaluate their effectiveness for hallucination detection across three QA benchmarks. Our observations show that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty. These findings suggest that lexical coverage features provide a complementary signal for hallucination detection. All code and suffix-array infrastructure are provided at this https URL.", 'abstract_zh': '大型语言模型中的幻觉是一个基本挑战，特别是在开放域问答中。此前的工作试图通过模型内部信号如token级熵或生成一致性来检测幻觉，而预训练数据曝光与幻觉之间的联系尚未得到充分探索。现有研究显示，大型语言模型在长尾知识上表现不佳，即生成答案的准确性随着预训练中罕见实体的真实性下降。然而，数据覆盖本身作为检测信号的作用被忽略。我们提出一个补充性问题：问题和/或生成答案的词汇训练数据覆盖是否能为幻觉检测提供额外信号？为了调查这一点，我们在RedPajama的1.3万亿token预训练语料库上构建可扩展的后缀数组，提取n-gram统计信息，包括提示和模型生成内容。我们在三个问答基准上评估它们在幻觉检测中的效果。我们的观察表明，尽管基于出现的特征单独使用时是弱预测器，但与对数概率结合使用时，它们在具有更高固有模型不确定性数据集上的效果有所提升。这些发现表明，词汇覆盖特征为幻觉检测提供了补充信号。所有代码和后缀数组基础设施均可在以下网址获得：https://example.com。', 'title_zh': '衡量词汇训练数据覆盖度对大型语言模型幻觉检测影响的测量方法'}
{'arxiv_id': 'arXiv:2511.17923', 'title': 'Towards Efficient LLM-aware Heterogeneous Graph Learning', 'authors': 'Wenda Li, Tongya Zheng, Shunyu Liu, Yu Wang, Kaixuan Chen, Hanyang Yuan, Bingde Hu, Zujie Ren, Mingli Song, Gang Chen', 'link': 'https://arxiv.org/abs/2511.17923', 'abstract': 'Heterogeneous graphs are widely present in real-world complex networks, where the diversity of node and relation types leads to complex and rich semantics. Efforts for modeling complex relation semantics in heterogeneous graphs are restricted by the limitations of predefined semantic dependencies and the scarcity of supervised signals. The advanced pre-training and fine-tuning paradigm leverages graph structure to provide rich self-supervised signals, but introduces semantic gaps between tasks. Large Language Models (LLMs) offer significant potential to address the semantic issues of relations and tasks in heterogeneous graphs through their strong reasoning capabilities in textual modality, but their incorporation into heterogeneous graphs is largely limited by computational complexity. Therefore, in this paper, we propose an Efficient LLM-Aware (ELLA) framework for heterogeneous graphs, addressing the above issues. To capture complex relation semantics, we propose an LLM-aware Relation Tokenizer that leverages LLM to encode multi-hop, multi-type relations. To reduce computational complexity, we further employ a Hop-level Relation Graph Transformer, which help reduces the complexity of LLM-aware relation reasoning from exponential to linear. To bridge semantic gaps between pre-training and fine-tuning tasks, we introduce the fine-grained task-aware textual Chain-of-Thought (CoT) prompts. Extensive experiments on four heterogeneous graphs show that our proposed ELLA outperforms state-of-the-art methods in the performance and efficiency. In particular, ELLA scales up to 13b-parameter LLMs and achieves up to a 4x speedup compared with existing LLM-based methods. Our code is publicly available at this https URL.', 'abstract_zh': '异构图中广泛存在多样性节点和关系类型，导致复杂丰富的语义。面向异构图中复杂关系语义建模的努力受限于预定义语义依赖的局限性和监督信号的稀缺性。先进的预训练和微调范式利用图结构提供丰富的自监督信号，但会在任务之间引入语义差距。大规模语言模型（LLMs）通过其在文本模态中的强大推理能力为异构图中的关系和任务提供重要的语义解决方案，但其集成到异构图中受到计算复杂性的限制。因此，在本文中，我们提出了一种高效的LLM感知（ELLA）框架，以解决上述问题。为了捕获复杂的关系语义，我们提出了一种LLM感知的关系分词器，利用LLM编码多跳、多类型关系。为了降低计算复杂性，我们进一步应用了一种跳级关系图变换器，将LLM感知的关系推理的复杂性从指数级降低到线性级。为了弥合预训练和微调任务之间的语义差距，我们引入了细粒度的任务感知文本链式思考（CoT）提示。在四个异构图上的广泛实验表明，我们提出的ELLA在性能和效率上优于现有方法。特别是，ELLA 支持多达13B参数的大规模语言模型，并且相比现有基于LLM的方法可实现最大4倍的速度提升。我们的代码已公开。', 'title_zh': '面向高效大模型意识异构图学习'}
{'arxiv_id': 'arXiv:2511.17908', 'title': 'Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction', 'authors': 'Debashish Chakraborty, Eugene Yang, Daniel Khashabi, Dawn Lawrie, Kevin Duh', 'link': 'https://arxiv.org/abs/2511.17908', 'abstract': "Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.", 'abstract_zh': 'Retrieval-Augmented Generation (RAG)中通过检索增强生成事实 grounding，但在长或嘈杂的上下文超过模型有效注意力范围时，LLM的准确性下降。现有的预生成过滤器依赖于启发式方法或未校准的LLM置信分数，无法提供保留证据的统计控制。我们通过可覆盖性控制的过滤框架——可信预测，进行评估和演示，该框架去除无关内容同时保留支持证据的召回率。使用嵌入式和LLM评分函数，我们在NeuCLIR和RAGTIME集合上测试了这种方法。可信过滤始终满足其目标覆盖范围，确保保留一定比例的相关片段，并且相对于未过滤检索的保留上下文减少2-3倍。在NeuCLIR上，严格过滤下的ARGUE F1下游事实准确性提高，在中等覆盖范围内保持稳定，表明大多数被丢弃的内容是冗余或不相关的。这些结果表明，可信预测使在RAG中实现可靠且可控制覆盖范围的上下文减少成为可能，提供了一种模型无关且原理上的上下文工程方法。', 'title_zh': '原则性的上下文工程设计用于RAG：通过容许预测提供的统计保证'}
{'arxiv_id': 'arXiv:2511.17854', 'title': 'A superpersuasive autonomous policy debating system', 'authors': 'Allen Roush, Devin Gonier, John Hines, Judah Goldfeder, Philippe Martin Wyder, Sanjay Basu, Ravid Shwartz Ziv', 'link': 'https://arxiv.org/abs/2511.17854', 'abstract': 'The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: this https URL', 'abstract_zh': '高度复杂、基于证据的且战略上适应性强的说服能力仍然是人工智能的巨大挑战。DeepDebater：一种新型自主辩论系统及其应用', 'title_zh': '超说服力自主政策辩论系统'}
{'arxiv_id': 'arXiv:2511.17818', 'title': 'APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs', 'authors': 'Aishwarya Mandyam, Kalyani Limaye, Barbara E. Engelhardt, Emily Alsentzer', 'link': 'https://arxiv.org/abs/2511.17818', 'abstract': 'Off-policy evaluation (OPE) estimates the value of a contextual bandit policy prior to deployment. As such, OPE plays a critical role in ensuring safety in high-stakes domains such as healthcare. However, standard OPE approaches are limited by the size and coverage of the behavior dataset. While previous work has explored using expert-labeled counterfactual annotations to enhance dataset coverage, obtaining such annotations is expensive, limiting the scalability of prior approaches. We propose leveraging large language models (LLMs) to generate counterfactual annotations for OPE in medical domains. Our method uses domain knowledge to guide LLMs in predicting how key clinical features evolve under alternate treatments. These predicted features can then be transformed using known reward functions to create counterfactual annotations. We first evaluate the ability of several LLMs to predict clinical features across two patient subsets in MIMIC-IV, finding that state-of-the-art LLMs achieve comparable performance. Building on this capacity to predict clinical features, we generate LLM-based counterfactual annotations and incorporate them into an OPE estimator. Our empirical results analyze the benefits of counterfactual annotations under varying degrees of shift between the behavior and target policies. We find that in most cases, the LLM-based counterfactual annotations significantly improve OPE estimates up to a point. We provide an entropy-based metric to identify when additional annotations cease to be useful. Our results demonstrate that LLM-based counterfactual annotations offer a scalable approach for addressing coverage limitations in healthcare datasets, enabling safer deployment of decision-making policies in clinical settings.', 'abstract_zh': '基于政策评估中的大规模语言模型生成反事实标注以增强医疗领域中的上下文臂策略评估', 'title_zh': 'APRIL: 基于可靠推理从LLMs进行政策评估的标注数据'}
{'arxiv_id': 'arXiv:2511.17813', 'title': 'Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation', 'authors': 'Scott Merrill, Shashank Srivastava', 'link': 'https://arxiv.org/abs/2511.17813', 'abstract': 'Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this "action-aware" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.', 'abstract_zh': '大型语言模型提供了模拟多方讨论的机会，但由于缺乏标注演讲者的数据，现实建模仍受限。通过自动语音识别（ASR）生成的转录使用匿名演讲者标签（如Speaker_1），阻碍了模型捕捉一致的自然人类行为。本研究介绍了一种可复制的管道，将公开的Zoom录音转换为带有如人物档案和pragma行动标签（如[propose_motion]）等元数据的标注演讲者转录。我们发布了三个地方政府讨论数据集：上诉法院听证会、学区会议和市政会议。使用这种具有“行动意识”的数据 fine-tune LLMs以模拟特定参与者，可降低67%的困惑度，并几乎将基于分类器的演讲者忠实度和现实感评估指标翻倍。图灵式的人类评估显示，我们的模拟往往难以与真实讨论区分开来，提供了一种实用且可扩展的方法，用于复杂的真实主义城市模拟。', 'title_zh': '点议案：基于动作意识的LLM人设建模以实现具身化的 Citizenship模拟'}
{'arxiv_id': 'arXiv:2511.17775', 'title': 'Episodic Memory in Agentic Frameworks: Suggesting Next Tasks', 'authors': 'Sandro Rama Fiorini, Leonardo G. Azevedo, Raphael M. Thiago, Valesca M. de Sousa, Anton B. Labate, Viviane Torres da Silva', 'link': 'https://arxiv.org/abs/2511.17775', 'abstract': 'Agentic frameworks powered by Large Language Models (LLMs) can be useful tools in scientific workflows by enabling human-AI co-creation. A key challenge is recommending the next steps during workflow creation without relying solely on LLMs, which risk hallucination and require fine-tuning with scarce proprietary data. We propose an episodic memory architecture that stores and retrieves past workflows to guide agents in suggesting plausible next tasks. By matching current workflows with historical sequences, agents can recommend steps based on prior patterns.', 'abstract_zh': '由大规模语言模型驱动的代理框架可以在科学工作流中作为促进人类-AI共创的有用工具。一个关键挑战是在工作流创建过程中推荐下一步骤，而不仅依靠大规模语言模型，因为这存在幻觉风险并需要使用稀缺的专有数据进行微调。我们提出了一种情景记忆架构，用于存储和检索过往的工作流，以指导代理提出合理的下一步任务。通过将当前工作流与历史序列匹配，代理可以根据先前的模式推荐步骤。', 'title_zh': '在代理框架下的情景记忆：建议下一个任务'}
{'arxiv_id': 'arXiv:2511.17699', 'title': 'Understanding Counting Mechanisms in Large Language and Vision-Language Models', 'authors': 'Hosein Hasani, Amirmohammad Izadi, Fatemeh Askari, Mobin Bagherian, Sadegh Mohammadian, Mohammad Izadi, Mahdieh Soleymani Baghshah', 'link': 'https://arxiv.org/abs/2511.17699', 'abstract': 'This paper examines how large language models (LLMs) and large vision-language models (LVLMs) represent and compute numerical information in counting tasks. We use controlled experiments with repeated textual and visual items and analyze model behavior through causal mediation and activation patching. To this end, we design a specialized tool, CountScope, for mechanistic interpretability of numerical content. Results show that individual tokens or visual features encode latent positional count information that can be extracted and transferred across contexts. Layerwise analyses reveal a progressive emergence of numerical representations, with lower layers encoding small counts and higher layers representing larger ones. We identify an internal counter mechanism that updates with each item, stored mainly in the final token or region and transferable between contexts. In LVLMs, numerical information also appears in visual embeddings, shifting between background and foreground regions depending on spatial composition. Models rely on structural cues such as separators in text, which act as shortcuts for tracking item counts and influence the accuracy of numerical predictions. Overall, counting emerges as a structured, layerwise process in LLMs and follows the same general pattern in LVLMs, shaped by the properties of the vision encoder.', 'abstract_zh': '本文探讨了大规模语言模型（LLMs）和大规模视觉-语言模型（LVLMs）在计数任务中如何表示和计算数值信息。我们通过使用包含重复文本和视觉项目的受控实验，并利用因果中介和激活补丁分析模型行为。为此，我们设计了一种专门工具——CountScope，以实现数值内容的功能解释。结果表明，单个词汇单元或视觉特征编码潜在的位置计数信息，这些信息可以被提取并转移到不同的上下文中。逐层分析揭示了数值表示的逐步出现，较低层编码较小的计数，较高层则表示较大的计数。我们发现了一种内部计数机制，每次处理一个项目时都会更新，并主要存储在最终的词汇单元或区域中，且能在不同上下文中转移。在LVLMs中，数值信息也出现在视觉嵌入中，根据空间构成会在背景和前景区域之间转移。模型依赖于结构线索，如文本中的分隔符，这些线索用作追踪项目计数的捷径，并影响数值预测的准确性。总体而言，在LLMs中，计数是一种结构化的、逐层的过程，在LVLMs中也遵循相同的一般模式，受到视觉编码器属性的影响。', 'title_zh': '理解大规模语言和多模态模型中的计数机制'}
{'arxiv_id': 'arXiv:2511.17696', 'title': 'Liberating Logic in the Age of AI: Going Beyond Programming with Computational Thinking', 'authors': 'Douglas C. Schmidt, Dan Runfola', 'link': 'https://arxiv.org/abs/2511.17696', 'abstract': 'Mastering one or more programming languages has historically been the gateway to implementing ideas on a computer. Today, that gateway is widening with advances in large language models (LLMs) and artificial intelligence (AI)-powered coding assistants. What matters is no longer just fluency in traditional programming languages but the ability to think computationally by translating problems into forms that can be solved with computing tools. The capabilities enabled by these AI-augmented tools are rapidly leading to the commoditization of computational thinking, such that anyone who can articulate a problem in natural language can potentially harness computing power via AI.\nThis shift is poised to radically influence how we teach computer science and data science in the United States and around the world. Educators and industry leaders are grappling with how to adapt: What should students learn when the hottest new programming language is English? How do we prepare a generation of computational thinkers who need not code every algorithm manually, but must still think critically, design solutions, and verify AI-augmented results?\nThis paper explores these questions, examining the impact of natural language programming on software development, the emerging distinction between programmers and prompt-crafting problem solvers, the reforms needed in computer science and data science curricula, and the importance of maintaining our fundamental computational science principles in an AI-augmented future. Along the way, we compare approaches and share best practices for embracing this new paradigm in computing education.', 'abstract_zh': '掌握一种或多种编程语言一直是将想法在计算机上实现的门户。随着大型语言模型（LLMs）和人工智能（AI）辅助编程助手的进步，这个门户正在扩大。如今，决定性的因素不再仅仅是熟练掌握传统编程语言，而是能够通过将问题转换为可以通过计算工具解决的形式来进行计算思维思考的能力。这些AI增强工具的功能正迅速推动计算思维的普遍化，使得任何能够用自然语言描述问题的人都可能通过AI利用计算能力。\n\n这一转变有望从根本上影响我们在美国和世界各地如何教授计算机科学和数据科学。教育者和行业领导者正在应对如何适应这一变化：当热门的新型编程语言是英语时，学生应该学习什么？我们如何准备能够进行计算思考的一代，他们不必手动编写每个算法，但仍需批判性思考、设计解决方案，并验证AI增强的结果？\n\n本文探讨了这些问题，分析了自然语言编程对软件开发的影响，程序员与提示构建问题解决者之间 emerging 区分，计算机科学和数据科学课程中所需的改革，以及在AI增强的未来保持我们基本的计算科学原则的重要性。在这一过程中，我们比较了方法并分享了在计算教育中接受这一新范式的最佳实践。', 'title_zh': '在人工智能时代释放逻辑：超越编程的计算思维'}
{'arxiv_id': 'arXiv:2511.17689', 'title': 'ARISE: Agentic Rubric-Guided Iterative Survey Engine for Automated Scholarly Paper Generation', 'authors': 'Zi Wang, Xingqiao Wang, Sangah Lee, Xiaowei Xu', 'link': 'https://arxiv.org/abs/2511.17689', 'abstract': 'The rapid expansion of scholarly literature presents significant challenges in synthesizing comprehensive, high-quality academic surveys. Recent advancements in agentic systems offer considerable promise for automating tasks that traditionally require human expertise, including literature review, synthesis, and iterative refinement. However, existing automated survey-generation solutions often suffer from inadequate quality control, poor formatting, and limited adaptability to iterative feedback, which are core elements intrinsic to scholarly writing.\nTo address these limitations, we introduce ARISE, an Agentic Rubric-guided Iterative Survey Engine designed for automated generation and continuous refinement of academic survey papers. ARISE employs a modular architecture composed of specialized large language model agents, each mirroring distinct scholarly roles such as topic expansion, citation curation, literature summarization, manuscript drafting, and peer-review-based evaluation. Central to ARISE is a rubric-guided iterative refinement loop in which multiple reviewer agents independently assess manuscript drafts using a structured, behaviorally anchored rubric, systematically enhancing the content through synthesized feedback.\nEvaluating ARISE against state-of-the-art automated systems and recent human-written surveys, our experimental results demonstrate superior performance, achieving an average rubric-aligned quality score of 92.48. ARISE consistently surpasses baseline methods across metrics of comprehensiveness, accuracy, formatting, and overall scholarly rigor. All code, evaluation rubrics, and generated outputs are provided openly at this https URL', 'abstract_zh': '学術文献的快速扩展给综合全面的高质量学术综述带来了显著挑战。近期代理系统的发展为自动化传统上需要人类专长的任务（如文献回顾、综合和迭代完善）带来了巨大潜力。然而，现有的自动化综述生成解决方案经常面临质量控制不足、格式不良和对迭代反馈适应能力有限等问题，这些都是学术写作的核心要素。\n\n为解决这些局限性，我们引入了ARISE，一种代理系统导向的迭代综述引擎，用于自动生成和持续完善学术综述论文。ARISE采用模块化架构，由专门的大语言模型代理组成，每个代理模拟不同的学术角色，如主题扩展、引文整理、文献总结、论文起草和基于同行评审的评价。ARISE的关键在于代理系统导向的迭代完善循环，其中多个审稿代理独立使用结构化的、基于行为的打分标准评估稿件草稿，系统性地通过综合反馈提升内容。\n\n与最新的自动化系统和近期的人类撰写的综述进行评估，实验结果表明ARISE表现出优越性能，平均打分率达到92.48。ARISE在综述的全面性、准确性、格式和整体学术严谨性等指标上均超过了基本方法。所有代码、评估标准和生成输出均可通过此网址获得。', 'title_zh': 'ARISE: 由能力要素引导的迭代调研引擎，用于自动化学术论文生成'}
{'arxiv_id': 'arXiv:2511.17683', 'title': 'Datacenters in the Desert: Feasibility and Sustainability of LLM Inference in the Middle East', 'authors': 'Lara Hassan, Mohamed ElZeftawy, Abdulrahman Mahmoud', 'link': 'https://arxiv.org/abs/2511.17683', 'abstract': 'As the Middle East emerges as a strategic hub for artificial intelligence (AI) infrastructure, the feasibility of deploying sustainable datacenters in desert environments has become a topic of growing relevance. This paper presents an empirical study analyzing the energy consumption and carbon footprint of large language model (LLM) inference across four countries: the United Arab Emirates, Iceland, Germany, and the United States of America using DeepSeek Coder 1.3B and the HumanEval dataset on the task of code generation. We use the CodeCarbon library to track energy and carbon emissions andcompare geographical trade-offs for climate-aware AI deployment. Our findings highlight both the challenges and potential of datacenters in desert regions and provide a balanced outlook on their role in global AI expansion.', 'abstract_zh': '中东地区作为人工智能基础设施的战略枢纽，沙漠环境中的可持续数据中心部署可行性成为越来越相关的话题。本文通过使用DeepSeek Coder 1.3B和HumanEval数据集，在代码生成任务中分析了四个国家（阿拉伯联合酋长国、冰岛、德国和美国）大型语言模型推理的能耗和碳足迹。我们使用CodeCarbon库追踪能源和碳排放，并比较气候意识AI部署的地理权衡。我们的研究结果既强调了沙漠地区数据中心的挑战和潜力，也提供了它们在全球人工智能扩张中作用的平衡视角。', 'title_zh': '沙漠中的数据中心：中东地区大规模语言模型推理的可行性和可持续性'}
{'arxiv_id': 'arXiv:2511.17682', 'title': 'A Cross-Cultural Assessment of Human Ability to Detect LLM-Generated Fake News about South Africa', 'authors': 'Tim Schlippe, Matthias Wölfel, Koena Ronny Mabokela', 'link': 'https://arxiv.org/abs/2511.17682', 'abstract': "This study investigates how cultural proximity affects the ability to detect AI-generated fake news by comparing South African participants with those from other nationalities. As large language models increasingly enable the creation of sophisticated fake news, understanding human detection capabilities becomes crucial, particularly across different cultural contexts. We conducted a survey where 89 participants (56 South Africans, 33 from other nationalities) evaluated 10 true South African news articles and 10 AI-generated fake versions. Results reveal an asymmetric pattern: South Africans demonstrated superior performance in detecting true news about their country (40% deviation from ideal rating) compared to other participants (52%), but performed worse at identifying fake news (62% vs. 55%). This difference may reflect South Africans' higher overall trust in news sources. Our analysis further shows that South Africans relied more on content knowledge and contextual understanding when judging credibility, while participants from other countries emphasised formal linguistic features such as grammar and structure. Overall, the deviation from ideal rating was similar between groups (51% vs. 53%), suggesting that cultural familiarity appears to aid verification of authentic information but may also introduce bias when evaluating fabricated content. These insights contribute to understanding cross-cultural dimensions of misinformation detection and inform strategies for combating AI-generated fake news in increasingly globalised information ecosystems where content crosses cultural and geographical boundaries.", 'abstract_zh': '此项研究探讨文化亲近性如何影响检测AI生成的虚假新闻的能力，比较了南非参与者与来自其他国籍的参与者之间的差异。随着大型语言模型越来越容易生成复杂的虚假新闻，理解人类的检测能力变得尤为重要，尤其是在不同文化背景下。我们对89名参与者（56名南非人，33名来自其他国家）进行了调查，他们评估了10篇真实的南非新闻文章和10篇对应的AI生成的虚假版本。结果表明，这种模式是不对称的：南非人在这国新闻的真实性检测上表现更优（偏差40%），而其他国家的参与者为52%；但在识别虚假新闻方面，南非人的表现较差（62% vs. 55%）。这种差异可能反映了南非人对新闻来源更高的整体信任度。进一步的分析表明，南非人在判断可信度时更多依赖于内容知识和情境理解，而来自其他国家的参与者则更重视形式语言特征，如语法和结构。总体而言，各组偏离理想评分的差异相似（51% vs. 53%），这表明文化熟悉度似乎有助于验证真实信息，但在评估伪造内容时也可能引入偏见。这些见解有助于理解跨文化背景下的误导信息检测维度，并为在日益全球化的信息生态系统中对抗AI生成的虚假新闻提供策略建议。', 'title_zh': '跨文化评估人类检测南非生成的LLM伪造新闻的能力'}
{'arxiv_id': 'arXiv:2511.17680', 'title': 'Research and Prototyping Study of an LLM-Based Chatbot for Electromagnetic Simulations', 'authors': 'Albert Piwonski, Mirsad Hadžiefendić', 'link': 'https://arxiv.org/abs/2511.17680', 'abstract': 'This work addresses the question of how generative artificial intelligence can be used to reduce the time required to set up electromagnetic simulation models. A chatbot based on a large language model is presented, enabling the automated generation of simulation models with various functional enhancements. A chatbot-driven workflow based on the large language model Google Gemini 2.0 Flash automatically generates and solves two-dimensional finite element eddy current models using Gmsh and GetDP. Python is used to coordinate and automate interactions between the workflow components. The study considers conductor geometries with circular cross-sections of variable position and number. Additionally, users can define custom post-processing routines and receive a concise summary of model information and simulation results. Each functional enhancement includes the corresponding architectural modifications and illustrative case studies.', 'abstract_zh': '基于大型语言模型的聊天机器人如何用于减少电磁仿真模型建立时间的研究', 'title_zh': '基于LLM的电磁仿真聊天机器人研究与原型设计'}
{'arxiv_id': 'arXiv:2511.17678', 'title': 'Chatbots to strengthen democracy: An interdisciplinary seminar to train identifying argumentation techniques of science denial', 'authors': 'Ingo Siegert, Jan Nehring, Aranxa Márquez Ampudia, Matthias Busch, Stefan Hillmann', 'link': 'https://arxiv.org/abs/2511.17678', 'abstract': "In recent times, discussions on social media platforms have increasingly come under scrutiny due to the proliferation of science denial and fake news. Traditional solutions, such as regulatory actions, have been implemented to mitigate the spread of misinformation; however, these measures alone are not sufficient. To complement these efforts, educational approaches are becoming essential in empowering users to critically engage with misinformation. Conversation training, through serious games or personalized methods, has emerged as a promising strategy to help users handle science denial and toxic conversation tactics. This paper suggests an interdisciplinary seminar to explore the suitability of Large Language Models (LLMs) acting as a persona of a science denier to support people in identifying misinformation and improving resilience against toxic interactions. In the seminar, groups of four to five students will develop an AI-based chatbot that enables realistic interactions with science-denial argumentation structures. The task involves planning the setting, integrating a Large Language Model to facilitate natural dialogues, implementing the chatbot using the RASA framework, and evaluating the outcomes in a user study. It is crucial that users understand what they need to do during the interaction, how to conclude it, and how the relevant information is conveyed. The seminar does not aim to develop chatbots for practicing debunking but serves to teach AI technologies and test the feasibility of this idea for future applications. The chatbot seminar is conducted as a hybrid, parallel master's module at the participating educational institutions.", 'abstract_zh': '近年来，社交媒体平台上关于科学否认和假新闻的讨论日益受到关注。传统的解决办法，如监管措施，已被用于减轻错误信息的传播；然而，这些措施单独来看并不足够。为了补充这些努力，教育方法变得尤为重要，帮助用户批判性地应对错误信息。通过严肃游戏或个性化方法进行对话训练，已经成为了帮助用户应对科学否认和有毒对话策略的有效策略之一。本文建议举办一个跨学科研讨会，探讨大型语言模型（LLMs）扮演科学否认者人设以帮助人们识别错误信息并增强抵御有毒互动的能力的适宜性。在研讨会中，四到五名学生组成的小组将开发一个基于AI的聊天机器人，以模拟与科学否认论据结构的现实对话。任务包括规划场景、集成大型语言模型以促进自然对话、使用RASA框架实现聊天机器人，并通过用户研究评估结果。重要的是，用户需要理解他们在互动过程中需要做什么、何时结束互动以及相关信息是如何传达的。该研讨会不旨在开发练习辟谣的聊天机器人，而是为了教授AI技术并测试这一理念在将来应用中的可行性。该聊天机器人研讨会作为参与教育机构的混合并行硕士课程进行。', 'title_zh': '聊天机器人强化民主：跨学科研讨会以培训识别科学否认的论证技术'}
{'arxiv_id': 'arXiv:2511.17676', 'title': 'LLM and Agent-Driven Data Analysis: A Systematic Approach for Enterprise Applications and System-level Deployment', 'authors': 'Xi Wang, Xianyao Ling, Kun Li, Gang Yin, Liang Zhang, Jiang Wu, Annie Wang, Weizhe Wang', 'link': 'https://arxiv.org/abs/2511.17676', 'abstract': 'The rapid progress in Generative AI and Agent technologies is profoundly transforming enterprise data management and analytics. Traditional database applications and system deployment are fundamentally impacted by AI-driven tools, such as Retrieval-Augmented Generation (RAG) and vector database technologies, which provide new pathways for semantic querying over enterprise knowledge bases. In the meantime, data security and compliance are top priorities for organizations adopting AI technologies. For enterprise data analysis, SQL generations powered by large language models (LLMs) and AI agents, has emerged as a key bridge connecting natural language with structured data, effectively lowering the barrier to enterprise data access and improving analytical efficiency. This paper focuses on enterprise data analysis applications and system deployment, covering a range of innovative frameworks, enabling complex query understanding, multi-agent collaboration, security verification, and computational efficiency. Through representative use cases, key challenges related to distributed deployment, data security, and inherent difficulties in SQL generation tasks are discussed.', 'abstract_zh': '生成式AI和代理技术的迅速进步正在深刻改变企业数据管理与分析。传统数据库应用和系统部署受到以检索增强生成（RAG）和向量数据库技术为代表的AI驱动工具的影响，为在企业知识库上进行语义查询提供了新的途径。同时，数据安全与合规是采用AI技术的企业组织的首要任务。对于企业数据分析而言，由大规模语言模型（LLMs）和AI代理驱动的SQL生成，已成为将自然语言与结构化数据连接的关键桥梁，有效降低了企业数据访问的门槛并提高了分析效率。本文专注于企业数据分析应用和系统部署，涵盖了多种创新框架，支持复杂查询理解、多代理协作、安全验证和计算效率。通过典型用例，讨论了分布式部署、数据安全及相关SQL生成任务固有的挑战。', 'title_zh': 'LLM和代理驱动的数据分析：面向企业应用和系统级部署的系统性方法'}
{'arxiv_id': 'arXiv:2511.17671', 'title': 'MURMUR: Using cross-user chatter to break collaborative language agents in groups', 'authors': 'Atharv Singh Patlan, Peiyao Sheng, S. Ashwin Hebbar, Prateek Mittal, Pramod Viswanath', 'link': 'https://arxiv.org/abs/2511.17671', 'abstract': "Language agents are rapidly expanding from single-user assistants to multi-user collaborators in shared workspaces and groups. However, today's language models lack a mechanism for isolating user interactions and concurrent tasks, creating a new attack vector inherent to this new setting: cross-user poisoning (CUP). In a CUP attack, an adversary injects ordinary-looking messages that poison the persistent, shared state, which later triggers the agent to execute unintended, attacker-specified actions on behalf of benign users. We validate CUP on real systems, successfully attacking popular multi-user agents. To study the phenomenon systematically, we present MURMUR, a framework that composes single-user tasks into concurrent, group-based scenarios using an LLM to generate realistic, history-aware user interactions. We observe that CUP attacks succeed at high rates and their effects persist across multiple tasks, thus posing fundamental risks to multi-user LLM deployments. Finally, we introduce a first-step defense with task-based clustering to mitigate this new class of vulnerability", 'abstract_zh': '语言代理正从单用户助手迅速扩展到共享工作空间和群组中的多用户合作者。然而，现有的语言模型缺乏隔离用户交互和并发任务的机制，这在新环境中形成了一个新的攻击向量：跨用户投毒（CUP）。在CUP攻击中，攻击者注入看似正常的消息，污染持久化的共享状态，这后来促使代理执行恶意用户指定的未预期动作。我们已在真实系统上验证了CUP，并成功攻击了流行的多用户代理。为了系统地研究这一现象，我们提出了MURMUR框架，该框架使用生成式语言模型（LLM）将单用户任务组合成并发的基于群组的场景。我们观察到，CUP攻击的成功率很高，并且其影响跨越多个任务，从而对多用户生成式语言模型部署构成了根本性的风险。最后，我们提出了一种基于任务的聚类方法作为初步防御，以减轻这一新类别的漏洞。', 'title_zh': 'MURMUR: 利用跨用户闲聊破解群组中的协作语言代理'}
{'arxiv_id': 'arXiv:2511.17666', 'title': 'Evaluating Adversarial Vulnerabilities in Modern Large Language Models', 'authors': 'Tom Perel', 'link': 'https://arxiv.org/abs/2511.17666', 'abstract': "The recent boom and rapid integration of Large Language Models (LLMs) into a wide range of applications warrants a deeper understanding of their security and safety vulnerabilities. This paper presents a comparative analysis of the susceptibility to jailbreak attacks for two leading publicly available LLMs, Google's Gemini 2.5 Flash and OpenAI's GPT-4 (specifically the GPT-4o mini model accessible in the free tier). The research utilized two main bypass strategies: 'self-bypass', where models were prompted to circumvent their own safety protocols, and 'cross-bypass', where one model generated adversarial prompts to exploit vulnerabilities in the other. Four attack methods were employed - direct injection, role-playing, context manipulation, and obfuscation - to generate five distinct categories of unsafe content: hate speech, illegal activities, malicious code, dangerous content, and misinformation. The success of the attack was determined by the generation of disallowed content, with successful jailbreaks assigned a severity score. The findings indicate a disparity in jailbreak susceptibility between 2.5 Flash and GPT-4, suggesting variations in their safety implementations or architectural design. Cross-bypass attacks were particularly effective, indicating that an ample amount of vulnerabilities exist in the underlying transformer architecture. This research contributes a scalable framework for automated AI red-teaming and provides data-driven insights into the current state of LLM safety, underscoring the complex challenge of balancing model capabilities with robust safety mechanisms.", 'abstract_zh': '最近大规模语言模型的快速增长及其广泛集成的应用要求我们对其安全和安全性漏洞有更深入的理解。本文对谷歌Gemini 2.5 Flash和OpenAI的GPT-4（特别是一级免费模型GPT-4o迷你版）这两种领先的公开可用的大规模语言模型抵御狱警攻击的易感性进行了比较分析。研究采用了两种主要的绕过策略：“自我绕过”，即模型被提示绕过自身的安全协议，以及“跨模型绕过”，即一个模型生成对抗性提示以利用另一个模型的漏洞。本研究使用了四种攻击方法——直接注入、角色扮演、上下文操纵和混淆，生成了五类不安全内容：仇恨言论、非法活动、恶意代码、危险内容和假信息。攻击是否成功通过生成禁止的内容来确定，并对成功的狱警攻击进行了严重程度评分。研究发现，Gemini 2.5 Flash和GPT-4在狱警攻击易感性方面存在差异，表明它们的安全实现或架构设计存在差异。跨模型绕过攻击尤其有效，表明底层变换器架构中存在大量的漏洞。本文贡献了一个可扩展的自动化AI红队框架，并提供了关于当前大规模语言模型安全状态的数据驱动见解，突显了在模型能力和稳健的安全机制之间平衡的复杂挑战。', 'title_zh': '评估现代大型语言模型的对抗性脆弱性'}
{'arxiv_id': 'arXiv:2511.17630', 'title': 'Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change', 'authors': 'Nele Albers, Esra Cemre Su de Groot, Loes Keijsers, Manon H. Hillegers, Emiel Krahmer', 'link': 'https://arxiv.org/abs/2511.17630', 'abstract': 'Personalizing digital applications for health behavior change is a promising route to making them more engaging and effective. This especially holds for approaches that adapt to users and their specific states (e.g., motivation, knowledge, wants) over time. However, developing such approaches requires making many design choices, whose effectiveness is difficult to predict from literature and costly to evaluate in practice. In this work, we explore whether large language models (LLMs) can be used out-of-the-box to generate samples of user interactions that provide useful information for training reinforcement learning models for digital behavior change settings. Using real user data from four large behavior change studies as comparison, we show that LLM-generated samples can be useful in the absence of real data. Comparisons to the samples provided by human raters further show that LLM-generated samples reach the performance of human raters. Additional analyses of different prompting strategies including shorter and longer prompt variants, chain-of-thought prompting, and few-shot prompting show that the relative effectiveness of different strategies depends on both the study and the LLM with also relatively large differences between prompt paraphrases alone. We provide recommendations for how LLM-generated samples can be useful in practice.', 'abstract_zh': '基于大语言模型的用户交互样本生成在数字行为改变设置中训练强化学习模型的有效性探究', 'title_zh': '我们可以使用大语言模型来启动强化学习吗？-- 以数字健康行为改变为例'}
{'arxiv_id': 'arXiv:2511.17621', 'title': 'From Competition to Coordination: Market Making as a Scalable Framework for Safe and Aligned Multi-Agent LLM Systems', 'authors': 'Brendan Gho, Suman Muppavarapu, Afnan Shaik, Tyson Tsay, James Begin, Kevin Zhu, Archana Vaidheeswaran, Vasu Sharma', 'link': 'https://arxiv.org/abs/2511.17621', 'abstract': 'As foundation models are increasingly deployed as interacting agents in multi-agent systems, their collective behavior raises new challenges for trustworthiness, transparency, and accountability. Traditional coordination mechanisms, such as centralized oversight or adversarial adjudication, struggle to scale and often obscure how decisions emerge. We introduce a market-making framework for multi-agent large language model (LLM) coordination that organizes agent interactions as structured economic exchanges. In this setup, each agent acts as a market participant, updating and trading probabilistic beliefs, to converge toward shared, truthful outcomes. By aligning local incentives with collective epistemic goals, the framework promotes self-organizing, verifiable reasoning without requiring external enforcement. Empirically, we evaluate this approach across factual reasoning, ethical judgment, and commonsense inference tasks. Market-based coordination yields accuracy gains of up to 10% over single-shot baselines while preserving interpretability and transparency of intermediate reasoning steps. Beyond these improvements, our findings demonstrate that economic coordination principles can operationalize accountability and robustness in multi-agent LLM systems, offering a scalable pathway toward self-correcting, socially responsible AI capable of maintaining trust and oversight in real world deployment scenarios.', 'abstract_zh': '基于市场的多智能体大规模语言模型协调框架', 'title_zh': '从竞争到协调：作为安全和对齐多代理语言模型系统可扩展框架的做市商机制'}
{'arxiv_id': 'arXiv:2511.17602', 'title': 'Beyond Surface-Level Similarity: Hierarchical Contamination Detection for Synthetic Training Data in Foundation Models', 'authors': 'Sushant Mehta', 'link': 'https://arxiv.org/abs/2511.17602', 'abstract': 'Synthetic data has become essential for training foundation models, yet benchmark contamination threatens evaluation integrity. Although existing detection methods identify token-level overlap, they fail to detect semantic-level contamination where synthetic data conceptually resemble benchmarks without lexical overlap. This gap is critical as foundation models increasingly train on synthetic data that may implicitly encode benchmark knowledge. We propose a hierarchical contamination detection framework operating at four levels: token level, semantic level, reasoning pattern, and performance cliff detection. Through controlled experiments on MMLU, GSM8K and HumanEval, we demonstrate that semantic-level contamination evades existing methods (F1=0.17-0.49) but is effectively detected by our hierarchical approach (F1 = 0.76), with an average improvement of 26. 5\\% over state-of-the-art baselines. Our framework provides practitioners with practical tools for audit pipelines and enables responsible deployment of synthetic training data.', 'abstract_zh': '合成数据已成为训练基础模型必不可少的要素，但基准数据污染威胁着评估的完整性。尽管现有检测方法能够识别标记层面的重叠，却无法检测出语义层面的污染，即合成数据在概念上与基准数据相似，但没有词汇重叠。这一差距至关重要，因为基础模型越来越多地使用可能隐含编码基准知识的合成数据进行训练。我们提出了一种分层污染检测框架，该框架在四个层面上运作：标记层面、语义层面、推理模式和性能悬崖检测。通过在MMLU、GSM8K和HumanEval上的受控实验，我们展示了语义层面的污染能够规避现有方法的检测（F1=0.17-0.49），但被我们的分层方法有效检测（F1=0.76），相较于最先进的基线方法，平均提升26.5%。我们的框架为实践者提供了审计管道的实际工具，并使合成训练数据的负责任使用成为可能。', 'title_zh': '超越表面相似性：基础模型中合成训练数据污染的层级检测'}
{'arxiv_id': 'arXiv:2511.17599', 'title': 'From Projection to Prediction: Beyond Logits for Scalable Language Models', 'authors': 'Jianbing Dong, Jianbin Chang', 'link': 'https://arxiv.org/abs/2511.17599', 'abstract': 'Training Large Language Models (LLMs) typically involves a two-stage pipeline at the output layer: hidden states are projected into vocabulary logits via a linear transformation (lm_head), followed by cross-entropy loss computation against target tokens. While conceptually simple, this design incurs substantial overhead. The intermediate logits tensor, with dimensions proportional to batch size, sequence length, and vocabulary size, must be fully materialized in GPU memory, even though only one target token per position is ultimately used. This leads to significant memory footprint and bandwidth comsumption, limiting scalability and slowing training throughput.\nIn this work, we introduce a novel approach to integrates the output projection and loss prediction into a single operation. By directly computing the loss from hidden states and target tokens, our approach bypasses explicit logits materialization. This design reduces memory usage and alleviates bandwidth pressure. Experiments on LLM training demonstrate that our method achieves substantial memory savings and measurable speedups compared to the standard two-stage pipeline, enabling large batch sizes and longer sequences without sacrificing accuracy. Our work highlights the benefits of rethinking the boundary between projection and prediction, offering a practical systems optimization for efficient LLM training.', 'abstract_zh': '训练大型语言模型（LLMs）通常涉及输出层的两阶段管道：隐藏状态通过线性变换（lm_head）投影到词汇logits，随后与目标标记计算交叉熵损失。虽然概念上简单，但这种设计会带来显著的开销。中间的logits张量，其维度与批次大小、序列长度和词汇表大小成比例，必须完全保留在GPU内存中，尽管最终每个位置只使用一个目标标记。这导致了显著的内存占用和带宽消耗，限制了规模性和减慢了训练吞吐量。\n\n在本工作中，我们提出了一种新颖的方法，将输出投影和损失预测整合为单一操作。通过直接从隐藏状态和目标标记计算损失，我们的方法绕过了显式的logits保留在内存中的步骤。这种设计减少了内存使用并缓解了带宽压力。在LLM训练实验中，我们的方法在与标准两阶段管道相比时实现了显著的内存节省和可测量的速度提升，使得可以使用更大的批次大小和更长的序列而不会牺牲准确性。我们的工作强调了重新思考投影与预测边界的好处，为高效LLM训练提供了实际的系统优化方案。', 'title_zh': '从投影到预测：超越 logits 的可扩展语言模型'}
{'arxiv_id': 'arXiv:2511.17592', 'title': 'GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms', 'authors': 'Valentin Khrulkov, Andrey Galichin, Denis Bashkirov, Dmitry Vinichenko, Oleg Travkin, Roman Alferov, Andrey Kuznetsov, Ivan Oseledets', 'link': 'https://arxiv.org/abs/2511.17592', 'abstract': 'Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at this https URL.', 'abstract_zh': 'Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve.', 'title_zh': 'GigaEvo：由大语言模型和进化算法驱动的开源优化框架'}
{'arxiv_id': 'arXiv:2511.17584', 'title': 'LLM-Powered Text-Attributed Graph Anomaly Detection via Retrieval-Augmented Reasoning', 'authors': 'Haoyan Xu, Ruizhi Qian, Zhengtao Yao, Ziyi Liu, Li Li, Yuqi Li, Yanshu Li, Wenqing Zheng, Daniele Rosa, Daniel Barcklow, Senthil Kumar, Jieyu Zhao, Yue Zhao', 'link': 'https://arxiv.org/abs/2511.17584', 'abstract': 'Anomaly detection on attributed graphs plays an essential role in applications such as fraud detection, intrusion monitoring, and misinformation analysis. However, text-attributed graphs (TAGs), in which node information is expressed in natural language, remain underexplored, largely due to the absence of standardized benchmark datasets. In this work, we introduce TAG-AD, a comprehensive benchmark for anomaly node detection on TAGs. TAG-AD leverages large language models (LLMs) to generate realistic anomalous node texts directly in the raw text space, producing anomalies that are semantically coherent yet contextually inconsistent and thus more reflective of real-world irregularities. In addition, TAG-AD incorporates multiple other anomaly types, enabling thorough and reproducible evaluation of graph anomaly detection (GAD) methods. With these datasets, we further benchmark existing unsupervised GNN-based GAD methods as well as zero-shot LLMs for GAD.\nAs part of our zero-shot detection setup, we propose a retrieval-augmented generation (RAG)-assisted, LLM-based zero-shot anomaly detection framework. The framework mitigates reliance on brittle, hand-crafted prompts by constructing a global anomaly knowledge base and distilling it into reusable analysis frameworks. Our experimental results reveal a clear division of strengths: LLMs are particularly effective at detecting contextual anomalies, whereas GNN-based methods remain superior for structural anomaly detection. Moreover, RAG-assisted prompting achieves performance comparable to human-designed prompts while eliminating manual prompt engineering, underscoring the practical value of our RAG-assisted zero-shot LLM anomaly detection framework.', 'abstract_zh': '文本属性图上的异常检测在欺诈检测、入侵监控和 misinformation 分析等应用中发挥着重要作用。然而，由于缺少标准化基准数据集，文本属性图（TAGs）在异常节点检测方面的研究仍较为不足。本文介绍了一种全面的基准数据集 TAG-AD，用于文本属性图上的异常节点检测。TAG-AD 利用大规模语言模型 (LLMs) 直接在原始文本空间生成真实的异常节点文本，产生在语义上连贯但在上下文中不一致的异常，从而更符合实际世界的不规则性。此外，TAG-AD 结合多种其他异常类型，使图形异常检测（GAD）方法的全面和可重复评估成为可能。使用这些数据集，我们进一步评估了现有无监督 GNN 基础的 GAD 方法以及零样本 LLMs 的 GAD 能力。作为我们零样本检测框架的一部分，我们提出了基于检索增强生成（RAG）的 LLM 基础零样本异常检测框架。该框架通过构建全球异常知识库并在其中提取可重用的分析框架，减轻了对脆弱的手动构建提示的依赖。实验结果表明，LLMs 在检测上下文异常方面特别有效，而 GNN 方法在结构性异常检测方面仍然更优越。此外，RAG 增强提示的性能与人工设计的提示相当，同时避免了手动提示工程，突显了我们提出的 RAG 增强零样本 LLM 异常检测框架的实际价值。', 'title_zh': '基于检索增强推理的LLM驱动文本归因图异常检测'}
{'arxiv_id': 'arXiv:2511.17579', 'title': 'Multi-Value Alignment for LLMs via Value Decorrelation and Extrapolation', 'authors': 'Hefei Xu, Le Wu, Chen Cheng, Hao Liu', 'link': 'https://arxiv.org/abs/2511.17579', 'abstract': 'With the rapid advancement of large language models (LLMs), aligning them with human values for safety and ethics has become a critical challenge. This problem is especially challenging when multiple, potentially conflicting human values must be considered and balanced. Although several variants of existing alignment methods (such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO)) have been proposed to address multi-value alignment, they suffer from notable limitations: 1) they are often unstable and inefficient in multi-value optimization; and 2) they fail to effectively handle value conflicts. As a result, these approaches typically struggle to achieve optimal trade-offs when aligning multiple values.\nTo address this challenge, we propose a novel framework called Multi-Value Alignment (MVA). It mitigates alignment degradation caused by parameter interference among diverse human values by minimizing their mutual information. Furthermore, we propose a value extrapolation strategy to efficiently explore the Pareto frontier, thereby constructing a set of LLMs with diverse value preferences. Extensive experiments demonstrate that MVA consistently outperforms existing baselines in aligning LLMs with multiple human values.', 'abstract_zh': '基于多值对齐的大型语言模型安全伦理 alignment: 多值对齐的新框架', 'title_zh': '通过价值去相关和外推实现多值对齐的LLMs'}
{'arxiv_id': 'arXiv:2511.17565', 'title': 'Generative Caching for Structurally Similar Prompts and Responses', 'authors': 'Sarthak Chakraborty, Suman Nath, Xuchao Zhang, Chetan Bansal, Indranil Gupta', 'link': 'https://arxiv.org/abs/2511.17565', 'abstract': 'Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce \\ourmethod{}, a generative cache that produces variation-aware responses for structurally similar prompts. \\ourmethod{} identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that \\ourmethod{} achieves 83\\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\\sim$20\\% and reduces end-to-end execution latency by $\\sim$34\\% compared to standard prompt matching.', 'abstract_zh': '大规模语言模型（LLMs）在多种场景下被用于规划、推理和执行任务。在重复的工作流和代理人设置中，提示 often reused with minor variations while having a similar structure for recurring tasks。这为缓存提供了机会。然而，确切的提示匹配在结构相似的提示上失败，而语义缓存可能会忽略关键差异从而产生错误响应。为此，我们引入了 \\ourmethod{}，这是一种生成性缓存，能够为结构相似的提示生成变异感知的响应。\\ourmethod{} 能够识别相似提示结构中的可重复响应模式，并为新请求合成定制输出。结果显示，\\ourmethod{} 在没有提示重复的数据集上实现了83%的缓存命中率，同时保持了最低的错误命中率。在代理人工作流中，它将缓存命中率提高了约20%，并且将端到端执行延迟减少了约34%，相较于标准提示匹配。', 'title_zh': '结构相似的提示与响应的生成缓存'}
{'arxiv_id': 'arXiv:2511.17562', 'title': 'ChineseErrorCorrector3-4B: State-of-the-Art Chinese Spelling and Grammar Corrector', 'authors': 'Wei Tian, YuhaoZhou', 'link': 'https://arxiv.org/abs/2511.17562', 'abstract': "This paper introduces ChineseErrorCorrector3-4B, a unified model for Chinese spelling and grammatical error correction based on Qwen3-4B. The model demonstrates outstanding performance in general text correction tasks and achieves state-of-the-art results in both spelling correction (CSC) and grammatical correction (CGC). On several authoritative benchmark datasets -- including SIGHAN-2015, EC-LAW, MCSC, and NaCGEC -- the model's F1 and F0.5 scores significantly surpass existing publicly available models, ranking first in both spelling and grammatical error correction tasks.", 'abstract_zh': '基于Qwen3-4B的ChineseErrorCorrector3-4B：统一的中文拼写和语法错误纠正模型', 'title_zh': 'ChineseErrorCorrector3-4B: 最先进的中文拼写和语法纠错器'}
{'arxiv_id': 'arXiv:2511.17561', 'title': 'LexInstructEval: Lexical Instruction Following Evaluation for Large Language Models', 'authors': 'Huimin Ren, Yan Liang, Baiqiao Su, Chaobo Sun, Hengtong Lu, Kaike Zhang, Chen Wei', 'link': 'https://arxiv.org/abs/2511.17561', 'abstract': 'The ability of Large Language Models (LLMs) to precisely follow complex and fine-grained lexical instructions is a cornerstone of their utility and controllability. However, evaluating this capability remains a significant challenge. Current methods either rely on subjective and costly human evaluation or on automated LLM-as-a-judge systems, which suffer from inherent biases and unreliability. Existing programmatic benchmarks, while objective, often lack the expressiveness to test intricate, compositional constraints at a granular level. To address these limitations, we introduce LexInstructEval, a new benchmark and evaluation framework for fine-grained lexical instruction following. Our framework is built upon a formal, rule-based grammar that deconstructs complex instructions into a canonical <Procedure, Relation, Value> triplet. This grammar enables the systematic generation of a diverse dataset through a multi-stage, human-in-the-loop pipeline and facilitates objective verification via a transparent, programmatic engine. We release our dataset and open-source evaluation tools to facilitate further research into the controllability and reliability of LLMs.', 'abstract_zh': '大型语言模型（LLM）精确遵循复杂和细微词汇指令的能力是其实用性和可控性的基石。然而，评估这种能力仍是一个重大挑战。现有方法要么依赖主观且成本高昂的人类评估，要么依赖自动化的LLM评判系统，但这些系统存在固有的偏见和不可靠性。现有的程序基准虽然客观，但往往缺乏足够的表现力来测试细微的、组合性的约束条件。为解决这些局限性，我们引入了LexInstructEval，一个新的细粒度词汇指令跟随基准和评估框架。该框架基于一个形式化的、基于规则的语法，将复杂的指令分解为标准的<过程, 关系, 值>三元组。该语法通过多阶段的人机交互流水线实现了系统的数据集生成，并通过透明的程序化引擎实现了客观验证。我们发布了我们的数据集和开源评估工具，以促进对LLM的可控性和可靠性进一步研究。', 'title_zh': 'LexInstructEval: 词汇指令跟随评估 für 大型语言模型'}
{'arxiv_id': 'arXiv:2511.17560', 'title': '$A^3$: Attention-Aware Accurate KV Cache Fusion for Fast Large Language Model Serving', 'authors': 'Yuechi Zhou, Yi Su, Jianxin Zhang, Juntao Li, Qingrong Xia, Zhefeng Wang, Xinyu Duan, Baoxing Huai', 'link': 'https://arxiv.org/abs/2511.17560', 'abstract': 'Large language models (LLMs) have demonstrated strong capabilities in processing long contexts, enabling them to tackle tasks involving long textual inputs such as multi-turn conversations, legal documents, or retrieved documents in Retrieval-Augmented Generation (RAG) systems. However, despite their ability to handle long sequences, the resulting decoding latency and memory overhead remain substantial, posing challenges for real-world deployment. Recent advances in KV Cache reuse have shown potential to mitigate these costs, but still suffer from notable performance degradation. To address this issue, we conduct an in-depth investigation of recomputation-based reuse methods and observe that the recomputed tokens often fail to align with the context segments most relevant to the question. This misalignment hinders proper updates to the critical contextual representations. Therefore, we propose the $\\textbf{A}$ttention-$\\textbf{A}$ware $\\textbf{A}$ccurate KV Cache Fusion algorithm ($A^3$), which precomputes and selectively fuses the KV Cache of text chunks based on their relevance to the question, achieving accurate integration with minimal computational overhead. Extensive experiments on various benchmarks and LLMs demonstrate that $A^3$ achieves the best task performance compared to four baselines while reducing the time-to-first-token (TTFT) by 2$\\times$.', 'abstract_zh': '基于注意力感知准确的KV缓存融合算法（A³）：提升大语言模型的解码效率与任务性能', 'title_zh': '基于注意力意识的准确键值缓存融合方案以实现快速大语言模型服务'}
{'arxiv_id': 'arXiv:2511.17511', 'title': 'A Multidisciplinary Design and Optimization (MDO) Agent Driven by Large Language Models', 'authors': 'Bingkun Guo, Wentian Li, Xiaojian Liu, Jiaqi Luo, Zibin Yu, Dalong Dong, Shuyou Zhang, Yiming Zhang', 'link': 'https://arxiv.org/abs/2511.17511', 'abstract': 'To accelerate mechanical design and enhance design quality and innovation, we present a Multidisciplinary Design and Optimization (MDO) Agent driven by Large Language Models (LLMs). The agent semi-automates the end-to-end workflow by orchestrating three core capabilities: (i) natural-language-driven parametric modeling, (ii) retrieval-augmented generation (RAG) for knowledge-grounded conceptualization, and (iii) intelligent orchestration of engineering software for performance verification and optimization. Working in tandem, these capabilities interpret high-level, unstructured intent, translate it into structured design representations, automatically construct parametric 3D CAD models, generate reliable concept variants using external knowledge bases, and conduct evaluation with iterative optimization via tool calls such as finite-element analysis (FEA). Validation on three representative cases - a gas-turbine blade, a machine-tool column, and a fractal heat sink - shows that the agent completes the pipeline from natural-language intent to verified and optimized designs with reduced manual scripting and setup effort, while promoting innovative design exploration. This work points to a practical path toward human-AI collaborative mechanical engineering and lays a foundation for more dependable, vertically customized MDO systems.', 'abstract_zh': '基于大型语言模型的多学科设计与优化代理：提升机械设计效率与创新', 'title_zh': '由大型语言模型驱动的多学科设计与优化代理'}
{'arxiv_id': 'arXiv:2511.17506', 'title': 'AURA: Adaptive Unified Reasoning and Automation with LLM-Guided MARL for NextG Cellular Networks', 'authors': 'Narjes Nourzad, Mingyu Zong, Bhaskar Krishnamachari', 'link': 'https://arxiv.org/abs/2511.17506', 'abstract': "Next-generation (NextG) cellular networks are expected to manage dynamic traffic while sustaining high performance. Large language models (LLMs) provide strategic reasoning for 6G planning, but their computational cost and latency limit real-time use. Multi-agent reinforcement learning (MARL) supports localized adaptation, yet coordination at scale remains challenging. We present AURA, a framework that integrates cloud-based LLMs for high-level planning with base stations modeled as MARL agents for local decision-making. The LLM generates objectives and subgoals from its understanding of the environment and reasoning capabilities, while agents at base stations execute these objectives autonomously, guided by a trust mechanism that balances local learning with external input. To reduce latency, AURA employs batched communication so that agents update the LLM's view of the environment and receive improved feedback. In a simulated 6G scenario, AURA improves resilience, reducing dropped handoff requests by more than half under normal and high traffic and lowering system failures. Agents use LLM input in fewer than 60\\% of cases, showing that guidance augments rather than replaces local adaptability, thereby mitigating latency and hallucination risks. These results highlight the promise of combining LLM reasoning with MARL adaptability for scalable, real-time NextG network management.", 'abstract_zh': '基于云的大规模语言模型与多智能体强化学习结合的下一代蜂窝网络管理框架', 'title_zh': 'AURA：适应性统一推理与自动化——基于LLM引导的马尔可夫游戏多智能体学习在下一代蜂窝网络中的应用'}
