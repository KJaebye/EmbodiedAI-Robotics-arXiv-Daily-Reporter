{'arxiv_id': 'arXiv:2511.18387', 'title': 'Scaling Implicit Fields via Hypernetwork-Driven Multiscale Coordinate Transformations', 'authors': 'Plein Versace', 'link': 'https://arxiv.org/abs/2511.18387', 'abstract': 'Implicit Neural Representations (INRs) have emerged as a powerful paradigm for representing signals such as images, 3D shapes, signed distance fields, and radiance fields. While significant progress has been made in architecture design (e.g., SIREN, FFC, KAN-based INRs) and optimization strategies (meta-learning, amortization, distillation), existing approaches still suffer from two core limitations: (1) a representation bottleneck that forces a single MLP to uniformly model heterogeneous local structures, and (2) limited scalability due to the absence of a hierarchical mechanism that dynamically adapts to signal complexity. This work introduces Hyper-Coordinate Implicit Neural Representations (HC-INR), a new class of INRs that break the representational bottleneck by learning signal-adaptive coordinate transformations using a hypernetwork. HC-INR decomposes the representation task into two components: (i) a learned multiscale coordinate transformation module that warps the input domain into a disentangled latent space, and (ii) a compact implicit field network that models the transformed signal with significantly reduced complexity. The proposed model introduces a hierarchical hypernetwork architecture that conditions coordinate transformations on local signal features, enabling dynamic allocation of representation capacity. We theoretically show that HC-INR strictly increases the upper bound of representable frequency bands while maintaining Lipschitz stability. Extensive experiments across image fitting, shape reconstruction, and neural radiance field approximation demonstrate that HC-INR achieves up to 4 times higher reconstruction fidelity than strong INR baselines while using 30--60\\% fewer parameters.', 'abstract_zh': '超坐标隐式神经表示（HC-INR）：突破表示瓶颈的新型隐式神经表示方法', 'title_zh': '通过超网络驱动的多尺度坐标变换扩展隐式场'}
{'arxiv_id': 'arXiv:2511.17643', 'title': 'Fluid Grey 2: How Well Does Generative Adversarial Network Learn Deeper Topology Structure in Architecture That Matches Images?', 'authors': 'Yayan Qiu, Sean Hanna', 'link': 'https://arxiv.org/abs/2511.17643', 'abstract': 'Taking into account the regional characteristics of intrinsic and extrinsic properties of space is an essential issue in architectural design and urban renewal, which is often achieved step by step using image and graph-based GANs. However, each model nesting and data conversion may cause information loss, and it is necessary to streamline the tools to facilitate architects and users to participate in the design. Therefore, this study hopes to prove that I2I GAN also has the potential to recognize topological relationships autonomously. Therefore, this research proposes a method for quickly detecting the ability of pix2pix to learn topological relationships, which is achieved by adding two Grasshopper-based detection modules before and after GAN. At the same time, quantitative data is provided and its learning process is visualized, and changes in different input modes such as greyscale and RGB affect its learning efficiency. There are two innovations in this paper: 1) It proves that pix2pix can automatically learn spatial topological relationships and apply them to architectural design. 2) It fills the gap in detecting the performance of Image-based Generation GAN from a topological perspective. Moreover, the detection method proposed in this study takes a short time and is simple to operate. The two detection modules can be widely used for customizing image datasets with the same topological structure and for batch detection of topological relationships of images. In the future, this paper may provide a theoretical foundation and data support for the application of architectural design and urban renewal that use GAN to preserve spatial topological characteristics.', 'abstract_zh': '考虑空间内在和外在特性的区域特征在建筑设计和城市更新中的应用离不开逐步使用图像和图基GAN的方法，但每个模型嵌套和数据转换可能会导致信息损失，有必要简化工具以便于建筑师和用户参与设计。因此，本研究希望建立I2I GAN也具有自主识别拓扑关系的潜力。因此，本研究提出了一种快速检测pix2pix学习拓扑关系能力的方法，在GAN前后添加了两个基于Grasshopper的检测模块。同时提供了量化数据并可视化了学习过程，不同输入模式如灰度和RGB对学习效率的影响也进行了分析。本文有两个创新点：1) 证明了pix2pix可以自动学习空间拓扑关系并应用于建筑设计。2) 从拓扑视角填补了基于图像生成GAN性能检测的空白。此外，本研究提出的检测方法速度快且易于操作。两个检测模块可广泛用于定制具有相同拓扑结构的图像数据集以及批量检测图像的拓扑关系。未来，本文可能为使用GAN保留空间拓扑特征的建筑设计和城市更新应用提供理论基础和数据支持。', 'title_zh': 'Fluid Grey 2：生成对抗网络在匹配图像的建筑中学习更深的拓扑结构效果如何？'}
{'arxiv_id': 'arXiv:2511.19401', 'title': 'In-Video Instructions: Visual Signals as Generative Control', 'authors': 'Gongfan Fang, Xinyin Ma, Xinchao Wang', 'link': 'https://arxiv.org/abs/2511.19401', 'abstract': 'Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.', 'abstract_zh': '大规模视频生成模型最近展示了强大的视觉能力，能够预测符合当前观察中的逻辑和物理线索的未来帧。在本文中，我们研究了这些能力能否通过将嵌入在帧中的视觉信号解释为指令来应用于可控的图像到视频生成，我们称这一范式为“视频内指令”(In-Video Instruction)。与基于提示的控制不同，后者提供本质上全局且粗糙的文本描述，In-Video Instruction通过叠加文本、箭头或轨迹等元素直接将用户指导编码到视觉领域中。这使得视觉主体与其预期动作之间能够建立起明确、空间意识化的对应关系，并为不同的对象分配独特的指令。在三个最新生成器，包括Veo 3.1、Kling 2.5和Wan 2.2上的广泛实验显示，视频模型能够可靠地解读并执行此类嵌入在视觉中的指令，尤其是在复杂的多对象场景中。', 'title_zh': '视频中的指令：视觉信号作为生成性控制'}
{'arxiv_id': 'arXiv:2511.19396', 'title': 'Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments', 'authors': 'Jorge Ortigoso-Narro, Jose A. Belloch, Adrian Amor-Martin, Sandra Roger, Maximo Cobos', 'link': 'https://arxiv.org/abs/2511.19396', 'abstract': "Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.", 'abstract_zh': '基于深度学习的目标跟踪和声束形成技术的进步推动了监控、人机交互和机器人技术的新能力。本工作提出了一种集成基于深度学习的跟踪与声束形成嵌入式系统，以实现动态环境中的精确声源定位和定向音频捕获。该方法结合单目深度估计和立体视觉，实现了移动目标的精确三维定位。由MEMS麦克风构建的平面同心圆麦克风阵列提供了一个紧凑、节能的平台，支持在方位角和仰角上进行二维声束扫描。实时跟踪输出持续调整阵列的焦点，使声学响应与目标的位置同步。通过结合学习到的空间意识和动态扫描，该系统在存在多个或移动声源的情况下仍能保持稳健的性能。实验评估表明，该设计在信号与干扰比方面取得了显著提升，适用于视频会议、智能家庭设备和辅助技术。', 'title_zh': '基于设备端深度学习的实时对象跟踪在动态声学环境中的自适应波束形成'}
{'arxiv_id': 'arXiv:2511.19367', 'title': 'An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification', 'authors': 'Saniah Kayenat Chowdhury, Rusab Sarmun, Muhammad E. H. Chowdhury, Sohaib Bassam Zoghoul, Israa Al-Hashimi, Adam Mushtak, Amith Khandakar', 'link': 'https://arxiv.org/abs/2511.19367', 'abstract': 'Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor\'s size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable "black box" manner, our method offers both state-of-the-art performance and transparent decision support.', 'abstract_zh': '精确的肺癌肿瘤分期对于预后和治疗规划至关重要。然而，端到端的深度学习方法在实现这一目标时仍面临挑战，因为这些方法往往忽视了肿瘤-淋巴结-转移系统中至关重要的空间和解剖信息。肿瘤分期依赖于多个定量标准，包括肿瘤大小及其与最近的解剖结构的距离，即便是细微的变化也可能改变分期结果。我们提出了一种以医学为基础的混合管道，通过明确测量肿瘤的大小和距离属性来进行分期，而不是将其视为纯粹的图像分类任务。该方法使用专门的编码-解码网络精确分割肺部及其邻近的解剖结构，包括肺叶、肿瘤、纵隔和膈肌。随后，我们提取必要的肿瘤属性，即测量肿瘤的最大尺寸，并通过分割掩模的定量分析计算肿瘤与邻近解剖结构的距离。最后，我们应用与医疗指南一致的基于规则的肿瘤分期方法。该新型框架在Lung-PET-CT-Dx数据集上进行了评估，表现出色，整体分类准确率为91.36%，T1、T2、T3和T4阶段的F1分数分别为0.93、0.89、0.96和0.90，这些都是前人研究中常常忽略的关键评估指标。据我们所知，这是第一个将显式临床背景融入肿瘤分期分类的研究。与标准的卷积神经网络以不可解释的“黑箱”方式操作不同，我们的方法提供了最先进的性能和透明的决策支持。', 'title_zh': '一种关注解剖结构的混合深度学习框架用于肺癌肿瘤分期分类'}
{'arxiv_id': 'arXiv:2511.19365', 'title': 'DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation', 'authors': 'Zehong Ma, Longhui Wei, Shuai Wang, Shiliang Zhang, Qi Tian', 'link': 'https://arxiv.org/abs/2511.19365', 'abstract': 'Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at this https URL.', 'abstract_zh': '面向频率解耦的像素扩散框架', 'title_zh': 'DeCo: 频率解耦像素扩散用于端到端图像生成'}
{'arxiv_id': 'arXiv:2511.19275', 'title': 'Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization', 'authors': 'Ellie L. Zhang, Duoduo Liao, Callie C. Liao', 'link': 'https://arxiv.org/abs/2511.19275', 'abstract': 'Generation of dynamic, scalable multi-species bird soundscapes remains a significant challenge in computer music and algorithmic sound design. Birdsongs involve rapid frequency-modulated chirps, complex amplitude envelopes, distinctive acoustic patterns, overlapping calls, and dynamic inter-bird interactions, all of which require precise temporal and spatial control in 3D environments. Existing approaches, whether Digital Signal Processing (DSP)-based or data-driven, typically focus only on single species modeling, static call structures, or synthesis directly from recordings, and often suffer from noise, limited flexibility, or large data needs. To address these challenges, we present a novel, fully algorithm-driven framework that generates dynamic multi-species bird soundscapes using DSP-based chirp generation and 3D spatialization, without relying on recordings or training data. Our approach simulates multiple independently-moving birds per species along different moving 3D trajectories, supporting controllable chirp sequences, overlapping choruses, and realistic 3D motion in scalable soundscapes while preserving species-specific acoustic patterns. A visualization interface provides bird trajectories, spectrograms, activity timelines, and sound waves for analytical and creative purposes. Both visual and audio evaluations demonstrate the ability of the system to generate dense, immersive, and ecologically inspired soundscapes, highlighting its potential for computer music, interactive virtual environments, and computational bioacoustics research.', 'abstract_zh': '动态可扩展多物种鸟类声景的生成仍然是计算机音乐和算法声设计中的一个显著挑战。', 'title_zh': '动态多物种鸟类声景生成：基于声学模式和三维空间化的方法'}
{'arxiv_id': 'arXiv:2511.19254', 'title': 'Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation', 'authors': 'Mohamed Rissal Hedna, Sesugh Samuel Nder', 'link': 'https://arxiv.org/abs/2511.19254', 'abstract': 'Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.', 'abstract_zh': '计算机视觉系统在现代物流运营中日益普及，包括对拖车装载量的估算以进行计划、路由和收费。尽管有效，但此类系统可能对物理对抗攻击尤为脆弱，特别是可以打印并放置在内部表面的对抗斑块。在本工作中，我们研究了使用全仿真3D环境对卷积货物装载分类器进行此类攻击的可行性。利用Mitsuba 3进行可微渲染，我们在几何形状、光照和视角变化中优化斑块纹理，并将其与2D合成基线进行比较。我们的实验表明，3D优化的斑块在拒绝服务场景（空到满）中实现了高攻击成功率，成功率高达84.94%。隐蔽攻击（满到空）更具挑战性，但仍达到30.32%。我们分析影响攻击成功率的因素，讨论对自动化物流管道安全性的启示，并强调增强物理鲁棒性的方向。据我们所知，这是首次研究在物理上真实、全仿真的3D场景中对货物装载估算进行对抗斑块攻击的研究。', 'title_zh': '基于差分3D模拟的视觉货物占用估计对抗补丁攻击'}
{'arxiv_id': 'arXiv:2511.19229', 'title': 'Learning Plug-and-play Memory for Guiding Video Diffusion Models', 'authors': 'Selena Song, Ziming Xu, Zijun Zhang, Kun Zhou, Jiaxian Guo, Lianhui Qin, Biwei Huang', 'link': 'https://arxiv.org/abs/2511.19229', 'abstract': 'Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: this https URL.', 'abstract_zh': '基于扩散变换器(DiT)的视频生成模型最近在视觉质量和时间连贯性方面取得了显著成果，但仍频繁违反基本物理定律和常识动态，显示出缺乏显式世界知识。本文探索如何为它们配备即插即用的内存，以注入有益的世界知识。受基于Transformer的大型语言模型中上下文内存的启发，我们进行了实证研究，表明可以通过干预DiT的隐藏状态来引导其行为，并且嵌入空间中的简单低通和高通滤波器自然地分离了低级外观和高级物理/语义线索，使目标指导成为可能。基于这些观察，我们提出了一种可学习的记忆编码器DiT-Mem，由堆叠的3D CNN、低/高通滤波器和自我注意力层组成。编码器将参考视频映射到一个紧凑的记忆令牌集，这些令牌被拼接为DiT自我注意力层内的记忆。在训练过程中，我们冻结了扩散主干网络，并仅优化记忆编码器。这使得在少量训练参数（150M）和10K数据样本的情况下具有相当高效的学习过程，并在推理时实现了即插即用的使用。在先进模型上的广泛实验表明，我们的方法在遵守物理规则和提高视频保真度方面非常有效。我们的代码和数据在此公开发布：this https URL。', 'title_zh': '学习插拔式内存以引导视频扩散模型'}
{'arxiv_id': 'arXiv:2511.19220', 'title': 'Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering', 'authors': 'Federico Felizzi, Olivia Riccomi, Michele Ferramola, Francesco Andrea Causio, Manuel Del Medico, Vittorio De Vita, Lorenzo De Mori, Alessandra Piscitelli Pietro Eric Risuleo, Bianca Destro Castaniti, Antonio Cristiano Alessia Longo, Luigi De Angelis, Mariapia Vassalli, Marcello Di Pumpo', 'link': 'https://arxiv.org/abs/2511.19220', 'abstract': 'Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.', 'abstract_zh': '大型视觉语言模型（VLMs）在医学视觉问答基准测试中取得了显著 performance，但它们对视觉信息的依赖性仍不清楚。我们通过测试四种最先进的模型（Claude Sonnet 4.5、GPT-4o、GPT-5-mini 和 Gemini 2.0 flash exp）来调查当回答意大利医学问题时，前沿 VLMs 是否真正实现了视觉接地。我们使用了 EuropeMedQA 意大利数据集中明确要求图像解释的 60 个问题，在正确医学图像中替换为空白占位符，以测试模型是否真正整合了视觉和文本信息。结果显示了显著的视觉依赖性差异：GPT-4o 展现了最强的视觉接地能力，准确率下降了 27.9 个百分点（从 83.2% [74.6%, 91.7%] 下降到 55.3% [44.1%, 66.6%]），而 GPT-5-mini、Gemini 和 Claude 的准确率仅略微下降了 8.5 个百分点、2.4 个百分点和 5.6 个百分点。对模型生成的推理分析显示，所有模型都提供了对虚构视觉解释的自信解释，这表明各模型在依赖文本捷径与真正的视觉分析之间的不同程度。这些发现突出了模型稳健性中的关键差异，并强调在临床应用前需要进行严格的评估。', 'title_zh': '大型视觉语言模型真的能够扎根于医学图像之中？来自意大利临床视觉问答的证据'}
{'arxiv_id': 'arXiv:2511.19087', 'title': 'EnfoPath: Energy-Informed Analysis of Generative Trajectories in Flow Matching', 'authors': 'Ziyun Li, Ben Dai, Huancheng Hu, Henrik Boström, Soon Hoe Lim', 'link': 'https://arxiv.org/abs/2511.19087', 'abstract': 'Flow-based generative models synthesize data by integrating a learned velocity field from a reference distribution to the target data distribution. Prior work has focused on endpoint metrics (e.g., fidelity, likelihood, perceptual quality) while overlooking a deeper question: what do the sampling trajectories reveal? Motivated by classical mechanics, we introduce kinetic path energy (KPE), a simple yet powerful diagnostic that quantifies the total kinetic effort along each generation path of ODE-based samplers. Through comprehensive experiments on CIFAR-10 and ImageNet-256, we uncover two key phenomena: ({i}) higher KPE predicts stronger semantic quality, indicating that semantically richer samples require greater kinetic effort, and ({ii}) higher KPE inversely correlates with data density, with informative samples residing in sparse, low-density regions. Together, these findings reveal that semantically informative samples naturally reside on the sparse frontier of the data distribution, demanding greater generative effort. Our results suggest that trajectory-level analysis offers a physics-inspired and interpretable framework for understanding generation difficulty and sample characteristics.', 'abstract_zh': '基于流的生成模型通过从参考分布中整合学习到的速度场来合成数据。以往的研究专注于端点度量（如保真度、似然性、感知质量）而忽视了一个更深刻的问题：采样轨迹揭示了什么？受经典力学启发，我们引入了动能路径能量（KPE），这是一种简单而强大的诊断工具，用于量化ODE基采样器生成路径上的总动能。通过在CIFAR-10和ImageNet-256上的全面实验，我们发现了两个关键现象：（i）更高的KPE预测更强的语义质量，表明更丰富的语义样本需要更大的动能努力；（ii）更高的KPE与数据密度呈负相关，信息丰富的样本位于稀疏、低密度区域。这些发现表明，语义上有信息的样本自然位于数据分布的稀疏前沿，需要更大的生成努力。我们的研究结果表明，轨迹层面的分析提供了一个基于物理原理且具有解释性的框架，用于理解生成难度和样本特性。', 'title_zh': 'EnfoPath：能量驱动的生成轨迹流匹配分析'}
{'arxiv_id': 'arXiv:2511.19067', 'title': 'DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling', 'authors': 'Timur Mamedov, Anton Konushin, Vadim Konushin', 'link': 'https://arxiv.org/abs/2511.19067', 'abstract': 'Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.', 'abstract_zh': '可泛化的个人重识别（Re-ID）旨在跨未见过的摄像机和环境识别人个体。不同于现有方法依赖有限的多摄像机标注数据，我们提出了一种新颖的方法DynaMix，该方法有效结合了手动标注的多摄像机数据和大规模伪标注的单摄像机数据。与先前工作不同，DynaMix 通过三个核心组件动态适应训练数据的结构和噪声：（1）一个重新标注模块，实时优化单摄像机个体的伪标签；（2）一个高效质心模块，保持在大规模个体空间下的鲁棒身份表示；（3）一个数据采样模块，精心组合混合数据的小批量以平衡学习复杂性和批内多样性。所有组件均专门设计以在大规模下高效运行，从而在数百万张图像和数十万个体上实现有效的训练。大量实验表明，DynaMix 在可泛化个人重识别方面始终优于现有最佳方法。', 'title_zh': 'DynaMix：通过动态重新标记和混合数据采样的通用行人再识别'}
{'arxiv_id': 'arXiv:2511.19046', 'title': 'MedSAM3: Delving into Segment Anything with Medical Concepts', 'authors': 'Anglin Liu, Rundong Xue, Xu R. Cao, Yifan Shen, Yi Lu, Xiang Li, Qianqian Chen, Jintai Chen', 'link': 'https://arxiv.org/abs/2511.19046', 'abstract': 'Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at this https URL.', 'abstract_zh': '医学图像分割是生物医学发现的基础。现有的方法缺乏普适性，并且需要为新的临床应用进行大量耗时的手动标注。在这里，我们提出了一种名为MedSAM-3的文本可提示医学分割模型，用于医学图像和视频分割。通过使用带有语义概念标签的医学图像对Segment Anything Model (SAM) 3架构进行微调，我们的MedSAM-3实现了医疗提示性概念分割（PCS），允许通过开放词汇的文本描述精准定位解剖结构，而不仅仅是使用几何提示。我们还引入了MedSAM-3代理框架，该框架结合多模态大型语言模型（MLLMs）以在代理参与的循环工作中进行复杂推理和迭代 refinement。在X射线、MRI、超声、CT和视频等多种医学成像模态下的全面实验表明，我们的方法显著优于现有的专科模型和基础模型。我们将在此链接中发布我们的代码和模型：this https URL。', 'title_zh': 'MedSAM3: 探索基于医疗概念的Segment Anything'}
{'arxiv_id': 'arXiv:2511.18989', 'title': 'Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning', 'authors': 'Wassim Benabbas, Mohammed Brahimi, Samir Akhrouf, Bilal Fortas', 'link': 'https://arxiv.org/abs/2511.18989', 'abstract': 'Recent advances in deep learning have enabled significant progress in plant disease classification using leaf images. Much of the existing research in this field has relied on the PlantVillage dataset, which consists of well-centered plant images captured against uniform, uncluttered backgrounds. Although models trained on this dataset achieve high accuracy, they often fail to generalize to real-world field images, such as those submitted by farmers to plant diagnostic systems. This has created a significant gap between published studies and practical application requirements, highlighting the necessity of investigating and addressing this issue. In this study, we investigate whether attention-based architectures and zero-shot learning approaches can bridge the gap between curated academic datasets and real-world agricultural conditions in plant disease classification. We evaluate three model categories: Convolutional Neural Networks (CNNs), Vision Transformers, and Contrastive Language-Image Pre-training (CLIP)-based zero-shot models. While CNNs exhibit limited robustness under domain shift, Vision Transformers demonstrate stronger generalization by capturing global contextual features. Most notably, CLIP models classify diseases directly from natural language descriptions without any task-specific training, offering strong adaptability and interpretability. These findings highlight the potential of zero-shot learning as a practical and scalable domain adaptation strategy for plant health diagnosis in diverse field environments.', 'abstract_zh': 'Recent advances in deep learning have enabled significant progress in plant disease classification using leaf images. However, models trained on the PlantVillage dataset often fail to generalize to real-world field images, highlighting the necessity of investigating this issue. This study investigates whether attention-based architectures and zero-shot learning approaches can bridge the gap between curated academic datasets and real-world agricultural conditions in plant disease classification.', 'title_zh': '重新思考植物疾病诊断：通过视觉变换器和零样本学习弥合学术与实践差距'}
{'arxiv_id': 'arXiv:2511.18919', 'title': 'Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation', 'authors': 'Ruiying Liu, Yuanzhi Liang, Haibin Huang, Tianshu Yu, Chi Zhang', 'link': 'https://arxiv.org/abs/2511.18919', 'abstract': 'Group Relative Policy Optimization (GRPO) has emerged as an effective and lightweight framework for post-training visual generative models. However, its performance is fundamentally limited by the ambiguity of textual visual correspondence: a single prompt may validly describe diverse visual outputs, and a single image or video may support multiple equally correct interpretations. This many to many relationship leads reward models to generate uncertain and weakly discriminative signals, causing GRPO to underutilize reliable feedback and overfit noisy ones. We introduce Bayesian Prior-Guided Optimization (BPGO), a novel extension of GRPO that explicitly models reward uncertainty through a semantic prior anchor. BPGO adaptively modulates optimization trust at two levels: inter-group Bayesian trust allocation emphasizes updates from groups consistent with the prior while down-weighting ambiguous ones, and intra-group prior-anchored renormalization sharpens sample distinctions by expanding confident deviations and compressing uncertain scores. Across both image and video generation tasks, BPGO delivers consistently stronger semantic alignment, enhanced perceptual fidelity, and faster convergence than standard GRPO and recent variants.', 'abstract_zh': 'Bayesian Prior-Guided Optimization (BPGO)：一种新型的引导优化方法，通过语义先验锚点显式建模奖励不确定性', 'title_zh': '学习值得信任的内容：贝叶斯先验引导优化在视觉生成中的应用'}
{'arxiv_id': 'arXiv:2511.18894', 'title': 'MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting', 'authors': 'Chenyu Mu, Guihai Chen, Xun Yang, Erkun Yang, Cheng Deng', 'link': 'https://arxiv.org/abs/2511.18894', 'abstract': "Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.", 'abstract_zh': '基于MetaDCSeg的医疗图像分割：动态学习像素级权重以抑制噪声标注影响并保留可靠标注', 'title_zh': 'MetaDCSeg: 通过元动态中心权重实现的稳健医学图像分割'}
{'arxiv_id': 'arXiv:2511.18856', 'title': 'Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos', 'authors': 'Sana Alamgeer', 'link': 'https://arxiv.org/abs/2511.18856', 'abstract': 'The main goal of the project is to design a new model that predicts regions of interest in 360$^{\\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.', 'abstract_zh': '该项目的主要目标是设计一种新的模型，用于预测360°视频中的感兴趣区域。', 'title_zh': '全景视频中感兴趣区域检测的深度混合模型'}
{'arxiv_id': 'arXiv:2511.18834', 'title': 'FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories', 'authors': 'Lei Ke, Hubery Yin, Gongye Liu, Zhengyao Lv, Jingcai Guo, Chen Li, Wenhan Luo, Yujiu Yang, Jing Lyu', 'link': 'https://arxiv.org/abs/2511.18834', 'abstract': "With the success of flow matching in visual generation, sampling efficiency remains a critical bottleneck for its practical application. Among flow models' accelerating methods, ReFlow has been somehow overlooked although it has theoretical consistency with flow matching. This is primarily due to its suboptimal performance in practical scenarios compared to consistency distillation and score distillation. In this work, we investigate this issue within the ReFlow framework and propose FlowSteer, a method unlocks the potential of ReFlow-based distillation by guiding the student along teacher's authentic generation trajectories. We first identify that Piecewised ReFlow's performance is hampered by a critical distribution mismatch during the training and propose Online Trajectory Alignment(OTA) to resolve it. Then, we introduce a adversarial distillation objective applied directly on the ODE trajectory, improving the student's adherence to the teacher's generation trajectory. Furthermore, we find and fix a previously undiscovered flaw in the widely-used FlowMatchEulerDiscreteScheduler that largely degrades few-step inference quality. Our experiment result on SD3 demonstrates our method's efficacy.", 'abstract_zh': '基于流模型的采样效率仍然是视觉生成中流动匹配应用的关键瓶颈。尽管ReFlow方法在理论上与流动匹配一致，但在实际应用场景中其性能不佳，因此相比一致性蒸馏和评分蒸馏被忽视。本文在ReFlow框架下探讨了这一问题，并提出了FlowSteer方法，通过引导学生沿教师的真实生成轨迹进行学习，释放ReFlow蒸馏的潜力。我们首先识别出分段ReFlow在训练过程中存在关键的分布不匹配问题，并提出在线轨迹对齐（OTA）来解决该问题。然后，我们引入了直接应用于ODE轨迹的对抗蒸馏目标，提高了学生的生成轨迹与教师的一致性。此外，我们发现并修复了一个广泛使用的FlowMatchEulerDiscreteScheduler中存在的缺陷，该缺陷显著降低了短步推理质量。我们的实验结果在SD3上的展示证明了该方法的有效性。', 'title_zh': 'FlowSteer: 引导短步图像合成的真实轨迹'}
{'arxiv_id': 'arXiv:2511.18811', 'title': 'Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache', 'authors': 'Yuqiu Jiang, Xiaozhen Qiao, Tianyu Mei, Haojian Huang, Yifan Chen, Ye Zheng, Zhe Sun', 'link': 'https://arxiv.org/abs/2511.18811', 'abstract': 'Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\\% mAP gain on rare categories and +4.39\\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.', 'abstract_zh': 'Human-Object Interaction (HOI)检测中的自适应多样性缓存模块：缓解长尾偏差的研究', 'title_zh': '通过自适应多样缓存减轻HOI检测中的长尾偏差'}
{'arxiv_id': 'arXiv:2511.18781', 'title': 'A Novel Dual-Stream Framework for dMRI Tractography Streamline Classification with Joint dMRI and fMRI Data', 'authors': "Haotian Yan, Bocheng Guo, Jianzhong He, Nir A. Sochen, Ofer Pasternak, Lauren J O'Donnell, Fan Zhang", 'link': 'https://arxiv.org/abs/2511.18781', 'abstract': "Streamline classification is essential to identify anatomically meaningful white matter tracts from diffusion MRI (dMRI) tractography. However, current streamline classification methods rely primarily on the geometric features of the streamline trajectory, failing to distinguish between functionally distinct fiber tracts with similar pathways. To address this, we introduce a novel dual-stream streamline classification framework that jointly analyzes dMRI and functional MRI (fMRI) data to enhance the functional coherence of tract parcellation. We design a novel network that performs streamline classification using a pretrained backbone model for full streamline trajectories, while augmenting with an auxiliary network that processes fMRI signals from fiber endpoint regions. We demonstrate our method by parcellating the corticospinal tract (CST) into its four somatotopic subdivisions. Experimental results from ablation studies and comparisons with state-of-the-art methods demonstrate our approach's superior performance.", 'abstract_zh': '扩散磁共振成像(dMRI)和功能性磁共振成像(fMRI)数据联合分析的双流束分类框架对于识别具有解剖学意义的白质纤维束至关重要。', 'title_zh': '一种结合弥散张量成像和功能磁共振成像数据的新型双流框架用于DTI跟踪纤维束分类'}
{'arxiv_id': 'arXiv:2511.18775', 'title': 'Rethinking Garment Conditioning in Diffusion-based Virtual Try-On', 'authors': 'Kihyun Na, Jinyoung Choi, Injung Kim', 'link': 'https://arxiv.org/abs/2511.18775', 'abstract': "Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.", 'abstract_zh': '基于上下文特征学习的高效单UNet虚拟试穿模型：Re-CatVTON', 'title_zh': '基于扩散模型的虚拟试穿中服装 conditioning 重新思考'}
{'arxiv_id': 'arXiv:2511.18766', 'title': 'Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment', 'authors': 'Xintao Chen, Xiaohao Xu, Bozhong Zheng, Yun Liu, Yingna Wu', 'link': 'https://arxiv.org/abs/2511.18766', 'abstract': "Unsupervised visual anomaly detection from multi-view images presents a significant challenge: distinguishing genuine defects from benign appearance variations caused by viewpoint changes. Existing methods, often designed for single-view inputs, treat multiple views as a disconnected set of images, leading to inconsistent feature representations and a high false-positive rate. To address this, we introduce ViewSense-AD (VSAD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. At its core is our Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views. We integrate MVAM into a View-Align Latent Diffusion Model (VALDM), enabling progressive and multi-stage alignment during the denoising process. This allows the model to build a coherent and holistic understanding of the object's surface from coarse to fine scales. Furthermore, a lightweight Fusion Refiner Module (FRM) enhances the global consistency of the aligned features, suppressing noise and improving discriminative power. Anomaly detection is performed by comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes. Extensive experiments on the challenging RealIAD and MANTA datasets demonstrate that VSAD sets a new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly proving its robustness to large viewpoint shifts and complex textures.", 'abstract_zh': '基于多视角图像的无监督视觉异常检测：克服实际挑战', 'title_zh': '基于渐进霍夫匹配导向对齐的无监督多视图视觉异常检测'}
{'arxiv_id': 'arXiv:2511.18742', 'title': 'ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion', 'authors': 'Zhenghan Fang, Jian Zheng, Qiaozi Gao, Xiaofeng Gao, Jeremias Sulam', 'link': 'https://arxiv.org/abs/2511.18742', 'abstract': 'Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.', 'abstract_zh': '基于反向离散化的文本到图像扩散模型：ProxT2I及其应用', 'title_zh': 'proxT2I：基于邻近扩散的高效奖励引导文本到图像生成'}
{'arxiv_id': 'arXiv:2511.18734', 'title': "Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion", 'authors': 'Keyang Lu, Sifan Zhou, Hongbin Xu, Gang Xu, Zhifei Yang, Yikai Wang, Zhen Xiao, Jieyi Long, Ming Li', 'link': 'https://arxiv.org/abs/2511.18734', 'abstract': 'Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo\'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo\'City first conceptualize the city through a top-down planning strategy that defines a hierarchical "City-District-Grid" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a "produce-refine-evaluate" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo\'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo\'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.', 'abstract_zh': "基于推理和组合能力的Yo'City: 用户定制且无限拓展的3D城市生成框架", 'title_zh': "Yo'City：基于自我批评扩展的个性化和无界三维真实城市场景生成"}
{'arxiv_id': 'arXiv:2511.18701', 'title': 'ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction', 'authors': 'Mustafa Munir, Harsh Goel, Xiwen Wei, Minkyu Choi, Sahil Shah, Kartikeya Bhardwaj, Paul Whatmough, Sandeep Chinchali, Radu Marculescu', 'link': 'https://arxiv.org/abs/2511.18701', 'abstract': 'Video editing and synthesis often introduce object inconsistencies, such as frame flicker and identity drift that degrade perceptual quality. To address these issues, we introduce ObjectAlign, a novel framework that seamlessly blends perceptual metrics with symbolic reasoning to detect, verify, and correct object-level and temporal inconsistencies in edited video sequences. The novel contributions of ObjectAlign are as follows: First, we propose learnable thresholds for metrics characterizing object consistency (i.e. CLIP-based semantic similarity, LPIPS perceptual distance, histogram correlation, and SAM-derived object-mask IoU). Second, we introduce a neuro-symbolic verifier that combines two components: (a) a formal, SMT-based check that operates on masked object embeddings to provably guarantee that object identity does not drift, and (b) a temporal fidelity check that uses a probabilistic model checker to verify the video\'s formal representation against a temporal logic specification. A frame transition is subsequently deemed "consistent" based on a single logical assertion that requires satisfying both the learned metric thresholds and this unified neuro-symbolic constraint, ensuring both low-level stability and high-level temporal correctness. Finally, for each contiguous block of flagged frames, we propose a neural network based interpolation for adaptive frame repair, dynamically choosing the interpolation depth based on the number of frames to be corrected. This enables reconstruction of the corrupted frames from the last valid and next valid keyframes. Our results show up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to SOTA baselines on the DAVIS and Pexels video datasets.', 'abstract_zh': '视频编辑和合成往往引入对象不一致问题，如帧闪烁和身份漂移，这些都会降低感知质量。为了解决这些问题，我们引入了ObjectAlign，一种新颖的框架，将感知度量与符号推理无缝融合，以检测、验证和纠正编辑视频序列中的对象级和时间不一致。ObjectAlign的 Novel贡献如下：首先，我们提出了表征对象一致性的度量的学习阈值（即基于CLIP的语义相似度、LPIPS感知距离、直方图相关性和SAM衍生的对象掩码IoU）。其次，我们引入了一种神经符号验证器，结合了两个组件：（a）一种基于SMT的形式检查，通过操作掩码对象嵌入以证明对象身份不会漂移；（b）一种基于概率模型检查的时间保真度检查，用于验证视频的正式表示与时间逻辑规范的一致性。随后，一帧过渡被判定为“一致”的标准是一个单一逻辑断言，该断言要求同时满足学习的度量阈值和这种统一的神经符号约束，从而确保低级稳定性和高级时间正确性。最后，对于每个连续的标旗帧块，我们提出了一种基于神经网络的内插方法，以自适应地进行帧修复，根据需要修正的帧数动态选择内插深度。这使得可以从最后一个有效关键帧和下一个有效关键帧重建损坏的帧。我们的结果表明，在DAVIS和Pexels视频数据集中，与现有最佳基线相比，CLIP分数提高了最多1.4点，位移误差减少了最多6.1点。', 'title_zh': 'ObjectAlign: 神经符号对象一致性验证与修正'}
{'arxiv_id': 'arXiv:2511.18676', 'title': 'MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis', 'authors': 'Yongcheng Yao, Yongshuo Zong, Raman Dutt, Yongxin Yang, Sotirios A Tsaftaris, Timothy Hospedales', 'link': 'https://arxiv.org/abs/2511.18676', 'abstract': 'Current vision-language models (VLMs) in medicine are primarily designed for categorical question answering (e.g., "Is this normal or abnormal?") or qualitative descriptive tasks. However, clinical decision-making often relies on quantitative assessments, such as measuring the size of a tumor or the angle of a joint, from which physicians draw their own diagnostic conclusions. This quantitative reasoning capability remains underexplored and poorly supported in existing VLMs. In this work, we introduce MedVision, a large-scale dataset and benchmark specifically designed to evaluate and improve VLMs on quantitative medical image analysis. MedVision spans 22 public datasets covering diverse anatomies and modalities, with 30.8 million image-annotation pairs. We focus on three representative quantitative tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. Our benchmarks show that current off-the-shelf VLMs perform poorly on these tasks. However, with supervised fine-tuning on MedVision, we significantly enhance their performance across detection, T/L estimation, and A/D measurement, demonstrating reduced error rates and improved precision. This work provides a foundation for developing VLMs with robust quantitative reasoning capabilities in medical imaging. Code and data are available at this https URL.', 'abstract_zh': '当前医学中的视觉-语言模型主要设计用于分类问答任务（例如，“这是正常还是异常？”）或定性描述任务。然而，临床决策往往依赖于定量评估，如测量肿瘤大小或关节角度，医生据此作出诊断。现有视觉-语言模型在这一定量推理能力上尚未得到充分利用和充分支持。在本文中，我们介绍了MedVision，这是一个大规模的数据集和基准，专门用于评估和提升视觉-语言模型在定量医学图像分析中的性能。MedVision涵盖了22个公共数据集，包含多种解剖结构和成像模态，共有30.8百万张图像-注释对。我们重点关注三个代表性的定量任务：（1）解剖结构和异常检测，（2）肿瘤/病变大小估计，（3）角度/距离测量。我们的基准测试表明，当前的即用型视觉-语言模型在这三项任务上表现不佳。然而，通过在MedVision上的监督微调，我们显著提升了它们在检测、肿瘤/病变估计和角度/距离测量上的性能，表现出较低的错误率和更高的精确度。本工作为开发具备稳健定量推理能力的视觉-语言模型奠定了基础。代码和数据可在以下链接获取。', 'title_zh': 'MedVision: 医学图像定量分析的数据集和基准'}
{'arxiv_id': 'arXiv:2511.18487', 'title': 'InstructAudio: Unified speech and music generation with natural language instruction', 'authors': 'Chunyu Qiang, Kang Yin, Xiaopeng Wang, Yuzhe Liang, Jiahui Zhao, Ruibo Fu, Tianrui Wang, Cheng Gong, Chen Zhang, Longbiao Wang, Jianwu Dang', 'link': 'https://arxiv.org/abs/2511.18487', 'abstract': 'Text-to-speech (TTS) and text-to-music (TTM) models face significant limitations in instruction-based control. TTS systems usually depend on reference audio for timbre, offer only limited text-level attribute control, and rarely support dialogue generation. TTM systems are constrained by input conditioning requirements that depend on expert knowledge annotations. The high heterogeneity of these input control conditions makes them difficult to joint modeling with speech synthesis. Despite sharing common acoustic modeling characteristics, these two tasks have long been developed independently, leaving open the challenge of achieving unified modeling through natural language instructions. We introduce InstructAudio, a unified framework that enables instruction-based (natural language descriptions) control of acoustic attributes including timbre (gender, age), paralinguistic (emotion, style, accent), and musical (genre, instrument, rhythm, atmosphere). It supports expressive speech, music, and dialogue generation in English and Chinese. The model employs joint and single diffusion transformer layers with a standardized instruction-phoneme input format, trained on 50K hours of speech and 20K hours of music data, enabling multi-task learning and cross-modal alignment. Fig. 1 visualizes performance comparisons with mainstream TTS and TTM models, demonstrating that InstructAudio achieves optimal results on most metrics. To our best knowledge, InstructAudio represents the first instruction-controlled framework unifying speech and music generation. Audio samples are available at: this https URL', 'abstract_zh': '基于指令的文本到语音(TTS)和文本到音乐(TTM)模型在指令控制方面面临显著局限。TTS系统通常依赖参考音频来实现音色控制，仅能提供有限的文字层级属性控制，并很少支持对话生成。TTM系统受限于输入条件要求，这些要求依赖于专家知识标注。这些输入控制条件的高度异质性使它们难以与语音合成联合建模。尽管这些任务共享共同的声学建模特征，但它们长期以来独立开发，留下的挑战是如何通过自然语言指令实现统一建模。我们引入了InstructAudio统一框架，该框架能够基于指令（自然语言描述）控制包括音色（性别、年龄）、副语言（情绪、风格、口音）和音乐（流派、乐器、节奏、氛围）在内的声学属性。该框架支持英语和中文的富有表现力的语音、音乐和对话生成。模型采用联合和单个扩散变换器层，并采用标准化指令-音素输入格式，通过5万小时的语音数据和20万小时的音乐数据训练，实现了多任务学习和跨模态对齐。图1展示了InstructAudio与主流TTS和TTM模型的性能对比，表明InstructAudio在多数指标上取得了最优结果。据我们所知，InstructAudio是第一个统一语音和音乐生成的基于指令的框架。音频样本可在以下链接获取：this https URL。', 'title_zh': 'InstructAudio：统一的自然语言指令驱动的语音与音乐生成'}
{'arxiv_id': 'arXiv:2511.18434', 'title': 'DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation', 'authors': 'Yongkun Du, Pinxuan Chen, Xuye Ying, Zhineng Chen', 'link': 'https://arxiv.org/abs/2511.18434', 'abstract': 'The advent of Multimodal Large Language Models (MLLMs) has unlocked the potential for end-to-end document parsing and translation. However, prevailing benchmarks such as OmniDocBench and DITrans are dominated by pristine scanned or digital-born documents, and thus fail to adequately represent the intricate challenges of real-world capture conditions, such as geometric distortions and photometric variations. To fill this gap, we introduce DocPTBench, a comprehensive benchmark specifically designed for Photographed Document Parsing and Translation. DocPTBench comprises over 1,300 high-resolution photographed documents from multiple domains, includes eight translation scenarios, and provides meticulously human-verified annotations for both parsing and translation. Our experiments demonstrate that transitioning from digital-born to photographed documents results in a substantial performance decline: popular MLLMs exhibit an average accuracy drop of 18% in end-to-end parsing and 12% in translation, while specialized document parsing models show significant average decrease of 25%. This substantial performance gap underscores the unique challenges posed by documents captured in real-world conditions and reveals the limited robustness of existing models. Dataset and code are available at this https URL.', 'abstract_zh': 'Multimodal大型语言模型(Multimodal Large Language Models)的兴起为端到端文档解析和翻译带来了潜力。然而，现有的基准测试，如OmniDocBench和DITrans，主要集中在洁净的扫描或数字生成的文档上，因此无法充分代表真实世界捕获条件下的复杂挑战，如几何畸变和光度变异。为弥补这一空白，我们引入了DocPTBench，一个专门针对拍摄文档解析和翻译设计的综合基准测试。DocPTBench包含了来自多个领域的超过1,300份高分辨率拍摄文档，包含了八种翻译场景，并提供了详尽的人工验证注释，涵盖了解析和翻译两个方面。我们的实验表明，从数字生成文档转向拍摄文档会导致显著的性能下降：流行的Multimodal大型语言模型在端到端解析上的平均准确率下降18%，在翻译上的平均准确率下降12%，而专门的文档解析模型平均下降25%。这一显著的性能差距突显了在现实条件下捕获的文档所带来的独特挑战，并揭示了现有模型的有限鲁棒性。数据集和代码可在以下链接获取：this https URL。', 'title_zh': 'DocPTBench: 评估端到端拍摄文档解析与翻译'}
{'arxiv_id': 'arXiv:2511.18384', 'title': 'NSTR: Neural Spectral Transport Representation for Space-Varying Frequency Fields', 'authors': 'Plein Versace', 'link': 'https://arxiv.org/abs/2511.18384', 'abstract': 'Implicit Neural Representations (INRs) have emerged as a powerful paradigm for representing signals such as images, audio, and 3D scenes. However, existing INR frameworks -- including MLPs with Fourier features, SIREN, and multiresolution hash grids -- implicitly assume a \\textit{global and stationary} spectral basis. This assumption is fundamentally misaligned with real-world signals whose frequency characteristics vary significantly across space, exhibiting local high-frequency textures, smooth regions, and frequency drift phenomena. We propose \\textbf{Neural Spectral Transport Representation (NSTR)}, the first INR framework that \\textbf{explicitly models a spatially varying local frequency field}. NSTR introduces a learnable \\emph{frequency transport equation}, a PDE that governs how local spectral compositions evolve across space. Given a learnable local spectrum field $S(x)$ and a frequency transport network $F_\\theta$ enforcing $\\nabla S(x) \\approx F_\\theta(x, S(x))$, NSTR reconstructs signals by spatially modulating a compact set of global sinusoidal bases. This formulation enables strong local adaptivity and offers a new level of interpretability via visualizing frequency flows. Experiments on 2D image regression, audio reconstruction, and implicit 3D geometry show that NSTR achieves significantly better accuracy-parameter trade-offs than SIREN, Fourier-feature MLPs, and Instant-NGP. NSTR requires fewer global frequencies, converges faster, and naturally explains signal structure through spectral transport fields. We believe NSTR opens a new direction in INR research by introducing explicit modeling of space-varying spectrum.', 'abstract_zh': '神经频谱传输表示（NSTR）：空间变异局部频域的显式建模', 'title_zh': 'NSTR: 基于神经频谱传输表示的空间变化频率场'}
{'arxiv_id': 'arXiv:2511.18326', 'title': 'General vs Domain-Specific CNNs: Understanding Pretraining Effects on Brain MRI Tumor Classification', 'authors': 'Helia Abedini, Saba Rahimi, Reza Vaziri', 'link': 'https://arxiv.org/abs/2511.18326', 'abstract': 'Brain tumor detection from MRI scans plays a crucial role in early diagnosis and treatment planning. Deep convolutional neural networks (CNNs) have demonstrated strong performance in medical imaging tasks, particularly when pretrained on large datasets. However, it remains unclear which type of pretrained model performs better when only a small dataset is available: those trained on domain-specific medical data or those pretrained on large general datasets. In this study, we systematically evaluate three pretrained CNN architectures for brain tumor classification: RadImageNet DenseNet121 with medical-domain pretraining, EfficientNetV2S, and ConvNeXt-Tiny, which are modern general-purpose CNNs. All models were trained and fine-tuned under identical conditions using a limited-size brain MRI dataset to ensure a fair comparison. Our results reveal that ConvNeXt-Tiny achieved the highest accuracy, followed by EfficientNetV2S, while RadImageNet DenseNet121, despite being pretrained on domain-specific medical data, exhibited poor generalization with lower accuracy and higher loss. These findings suggest that domain-specific pretraining may not generalize well under small-data conditions. In contrast, modern, deeper general-purpose CNNs pretrained on large-scale datasets can offer superior transfer learning performance in specialized medical imaging tasks.', 'abstract_zh': '脑肿瘤从MRI扫描检测在早期诊断和治疗规划中扮演着至关重要的角色。深度卷积神经网络（CNNs）在医学影像任务中显示出强大的性能，特别是在使用大规模数据集预训练时。然而，当仅可用少量数据集时，哪种类型的预训练模型表现更好仍不清楚：是基于特定领域医学数据训练的模型还是基于大规模通用数据集预训练的模型。在本研究中，我们系统地评估了三种预训练的CNN架构用于脑肿瘤分类：RadImageNet DenseNet121、EfficientNetV2S 和 ConvNeXt-Tiny，后两者是现代通用CNN。所有模型均在有限大小的脑MRI数据集上使用相同的条件进行训练和微调，以确保公平比较。研究结果表明，ConvNeXt-Tiny 达到了最高的准确率，其次是 EfficientNetV2S，而虽然 RadImageNet DenseNet121 基于特定领域医学数据进行了预训练，但它在泛化性能方面表现较差，准确率较低且损失较大。这些发现表明，在少量数据条件下，特定领域预训练可能无法很好地泛化。相反，基于大规模数据集预训练的现代深通用CNN可以为专门的医学影像任务提供优越的迁移学习性能。', 'title_zh': '通用型与领域专用CNN：理解预训练对脑MRI肿瘤分类的影响'}
{'arxiv_id': 'arXiv:2511.18307', 'title': 'ScriptViT: Vision Transformer-Based Personalized Handwriting Generation', 'authors': 'Sajjan Acharya, Rajendra Baskota', 'link': 'https://arxiv.org/abs/2511.18307', 'abstract': "Styled handwriting generation aims to synthesize handwritten text that looks both realistic and aligned with a specific writer's style. While recent approaches involving GAN, transformer and diffusion-based models have made progress, they often struggle to capture the full spectrum of writer-specific attributes, particularly global stylistic patterns that span long-range spatial dependencies. As a result, capturing subtle writer-specific traits such as consistent slant, curvature or stroke pressure, while keeping the generated text accurate is still an open problem. In this work, we present a unified framework designed to address these limitations. We introduce a Vision Transformer-based style encoder that learns global stylistic patterns from multiple reference images, allowing the model to better represent long-range structural characteristics of handwriting. We then integrate these style cues with the target text using a cross-attention mechanism, enabling the system to produce handwritten images that more faithfully reflect the intended style. To make the process more interpretable, we utilize Salient Stroke Attention Analysis (SSAA), which reveals the stroke-level features the model focuses on during style transfer. Together, these components lead to handwriting synthesis that is not only more stylistically coherent, but also easier to understand and analyze.", 'abstract_zh': '风格化手写文本生成旨在合成既真实又符合特定书写者风格的手写文本。尽管 recent 方法（包括 GAN、变压器和基于扩散的模型）取得了一定进展，但它们往往难以捕捉书写者特异性属性的完整谱系，特别是跨越长距离空间依赖的全局风格模式。因此，在生成文本的同时捕捉细微的书写者特异性特征（如一致的斜度、曲率或笔触压力）仍是一个开放问题。在本文中，我们提出了一种统一框架以解决这些限制。我们引入了一种基于视觉变压器的风格编码器，该编码器可以从多个参考图像中学习全局风格模式，从而使模型更好地表示手写的长期结构特征。然后，我们使用跨注意力机制将这些风格线索与目标文本结合，使系统能够生成更忠实地反映预期风格的手写图像。为了使过程更具可解释性，我们利用了显著笔划注意分析（SSAA），该方法揭示了模型在风格转移过程中关注的笔划级别特征。这些组件共同促进了不仅更具风格连贯性，也更易于理解和分析的手写合成。', 'title_zh': 'ScriptViT: 基于视觉变换器的个性化手写生成'}
{'arxiv_id': 'arXiv:2511.18290', 'title': 'SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes', 'authors': 'Jungho Lee, Minhyeok Lee, Sunghun Yang, Minseok Kang, Sangyoun Lee', 'link': 'https://arxiv.org/abs/2511.18290', 'abstract': '3D reconstruction in large-scale scenes is a fundamental task in 3D perception, but the inherent trade-off between accuracy and computational efficiency remains a significant challenge. Existing methods either prioritize speed and produce low-quality results, or achieve high-quality reconstruction at the cost of slow inference times. In this paper, we propose SwiftVGGT, a training-free method that significantly reduce inference time while preserving high-quality dense 3D reconstruction. To maintain global consistency in large-scale scenes, SwiftVGGT performs loop closure without relying on the external Visual Place Recognition (VPR) model. This removes redundant computation and enables accurate reconstruction over kilometer-scale environments. Furthermore, we propose a simple yet effective point sampling method to align neighboring chunks using a single Sim(3)-based Singular Value Decomposition (SVD) step. This eliminates the need for the Iteratively Reweighted Least Squares (IRLS) optimization commonly used in prior work, leading to substantial speed-ups. We evaluate SwiftVGGT on multiple datasets and show that it achieves state-of-the-art reconstruction quality while requiring only 33% of the inference time of recent VGGT-based large-scale reconstruction approaches.', 'abstract_zh': '大规模场景下的三维重建是三维感知中的一个基础任务，但精确度与计算效率之间的固有权衡仍然是一个重大挑战。现有方法要么优先考虑速度并产生低质量结果，要么在计算效率方面做出牺牲以实现高质量的重建。本文提出了一种无需训练的方法SwiftVGGT，它在保持高质量密集三维重建的同时显著减少了推理时间。为了在大规模场景中保持全局一致性，SwiftVGGT 不依赖于外部的视觉位置识别（VPR）模型来执行环回闭合，从而去除冗余计算并实现千米级环境下的准确重建。此外，我们提出了一种简单有效的点采样方法，使用单步拟三维变换（Sim(3)）奇异值分解（SVD）对齐相邻块，消除了先前工作中常用的逐次重新加权最小二乘法（IRLS）优化，从而显著提高了速度。我们在多个数据集上评估了SwiftVGGT，并展示了其在推理时间仅为最近的大规模重建方法的33%的情况下仍能达到最先进的重建质量。', 'title_zh': 'SwiftVGGT：一种适用于大规模场景的可扩展视觉几何导向变压器'}
{'arxiv_id': 'arXiv:2511.18281', 'title': 'Uni-DAD: Unified Distillation and Adaptation of Diffusion Models for Few-step Few-shot Image Generation', 'authors': 'Yara Bahram, Melodie Desbos, Mohammadhadi Shateri, Eric Granger', 'link': 'https://arxiv.org/abs/2511.18281', 'abstract': "Diffusion models (DMs) produce high-quality images, yet their sampling remains costly when adapted to new domains. Distilled DMs are faster but typically remain confined within their teacher's domain. Thus, fast and high-quality generation for novel domains relies on two-stage training pipelines: Adapt-then-Distill or Distill-then-Adapt. However, both add design complexity and suffer from degraded quality or diversity. We introduce Uni-DAD, a single-stage pipeline that unifies distillation and adaptation of DMs. It couples two signals during training: (i) a dual-domain distribution-matching distillation objective that guides the student toward the distributions of the source teacher and a target teacher, and (ii) a multi-head generative adversarial network (GAN) loss that encourages target realism across multiple feature scales. The source domain distillation preserves diverse source knowledge, while the multi-head GAN stabilizes training and reduces overfitting, especially in few-shot regimes. The inclusion of a target teacher facilitates adaptation to more structurally distant domains. We perform evaluations on a variety of datasets for few-shot image generation (FSIG) and subject-driven personalization (SDP). Uni-DAD delivers higher quality than state-of-the-art (SoTA) adaptation methods even with less than 4 sampling steps, and outperforms two-stage training pipelines in both quality and diversity.", 'abstract_zh': '统一的DM适配与蒸馏单阶段pipeline（Uni-DAD）：面向新颖领域的高效高质生成', 'title_zh': 'Uni-DAD: 统一的扩散模型蒸馏与适应方法用于少量步骤的少样本图像生成'}
{'arxiv_id': 'arXiv:2511.18164', 'title': 'Nested Unfolding Network for Real-World Concealed Object Segmentation', 'authors': 'Chunming He, Rihan Zhang, Dingming Zhang, Fengyang Xiao, Deng-Ping Fan, Sina Farsiu', 'link': 'https://arxiv.org/abs/2511.18164', 'abstract': 'Deep unfolding networks (DUNs) have recently advanced concealed object segmentation (COS) by modeling segmentation as iterative foreground-background separation. However, existing DUN-based methods (RUN) inherently couple background estimation with image restoration, leading to conflicting objectives and requiring pre-defined degradation types, which are unrealistic in real-world scenarios. To address this, we propose the nested unfolding network (NUN), a unified framework for real-world COS. NUN adopts a DUN-in-DUN design, embedding a degradation-resistant unfolding network (DeRUN) within each stage of a segmentation-oriented unfolding network (SODUN). This design decouples restoration from segmentation while allowing mutual refinement. Guided by a vision-language model (VLM), DeRUN dynamically infers degradation semantics and restores high-quality images without explicit priors, whereas SODUN performs reversible estimation to refine foreground and background. Leveraging the multi-stage nature of unfolding, NUN employs image-quality assessment to select the best DeRUN outputs for subsequent stages, naturally introducing a self-consistency loss that enhances robustness. Extensive experiments show that NUN achieves a leading place on both clean and degraded benchmarks. Code will be released.', 'abstract_zh': 'Nested Unfolding Network (NUN)：一种用于实际场景中隐蔽对象分割的统一框架', 'title_zh': '嵌套展开网络用于真实场景中隐藏物体的分割'}
{'arxiv_id': 'arXiv:2511.18152', 'title': 'UnfoldLDM: Deep Unfolding-based Blind Image Restoration with Latent Diffusion Priors', 'authors': 'Chunming He, Rihan Zhang, Zheng Chen, Bowen Yang, CHengyu Fang, Yunlong Lin, Fengyang Xiao, Sina Farsiu', 'link': 'https://arxiv.org/abs/2511.18152', 'abstract': 'Deep unfolding networks (DUNs) combine the interpretability of model-based methods with the learning ability of deep networks, yet remain limited for blind image restoration (BIR). Existing DUNs suffer from: (1) \\textbf{Degradation-specific dependency}, as their optimization frameworks are tied to a known degradation model, making them unsuitable for BIR tasks; and (2) \\textbf{Over-smoothing bias}, resulting from the direct feeding of gradient descent outputs, dominated by low-frequency content, into the proximal term, suppressing fine textures. To overcome these issues, we propose UnfoldLDM to integrate DUNs with latent diffusion model (LDM) for BIR. In each stage, UnfoldLDM employs a multi-granularity degradation-aware (MGDA) module as the gradient descent step. MGDA models BIR as an unknown degradation estimation problem and estimates both the holistic degradation matrix and its decomposed forms, enabling robust degradation removal. For the proximal step, we design a degradation-resistant LDM (DR-LDM) to extract compact degradation-invariant priors from the MGDA output. Guided by this prior, an over-smoothing correction transformer (OCFormer) explicitly recovers high-frequency components and enhances texture details. This unique combination ensures the final result is degradation-free and visually rich. Experiments show that our UnfoldLDM achieves a leading place on various BIR tasks and benefits downstream tasks. Moreover, our design is compatible with existing DUN-based methods, serving as a plug-and-play framework. Code will be released.', 'abstract_zh': 'UnfoldLDM：将深层展开网络与潜在扩散模型结合用于盲图像恢复', 'title_zh': 'UnfoldLDM：基于潜在扩散先验的深度展开盲图像恢复'}
{'arxiv_id': 'arXiv:2511.18055', 'title': 'IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment', 'authors': 'Bowen Qu, Shangkun Sun, Xiaoyu Liang, Wei Gao', 'link': 'https://arxiv.org/abs/2511.18055', 'abstract': "Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not well aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding edited results from different editing methods, and nearly 4,000 samples with corresponding Mean Opinion Scores (MOS) provided by 15 human subjects. Furthermore, we introduce IE-Critic-R1, which, benefiting from Reinforcement Learning from Verifiable Rewards (RLVR), provides more comprehensive and explainable quality assessment for text-driven image editing that aligns with human perception. Extensive experiments demonstrate IE-Critic-R1's superior subjective-alignments on the text-driven image editing task compared with previous metrics. Related data and codes are available to the public.", 'abstract_zh': 'Recent Advances in Text-driven Image Editing: Overcoming Challenges in Evaluation', 'title_zh': 'IE-Critic-R1: 提升基于文本驱动图像编辑的解释性测量以实现人类感知对齐'}
{'arxiv_id': 'arXiv:2511.17929', 'title': 'MambaTAD: When State-Space Models Meet Long-Range Temporal Action Detection', 'authors': 'Hui Lu, Yi Yu, Shijian Lu, Deepu Rajan, Boon Poh Ng, Alex C. Kot, Xudong Jiang', 'link': 'https://arxiv.org/abs/2511.17929', 'abstract': 'Temporal Action Detection (TAD) aims to identify and localize actions by determining their starting and ending frames within untrimmed videos. Recent Structured State-Space Models such as Mamba have demonstrated potential in TAD due to their long-range modeling capability and linear computational complexity. On the other hand, structured state-space models often face two key challenges in TAD, namely, decay of temporal context due to recursive processing and self-element conflict during global visual context modeling, which become more severe while handling long-span action instances. Additionally, traditional methods for TAD struggle with detecting long-span action instances due to a lack of global awareness and inefficient detection heads. This paper presents MambaTAD, a new state-space TAD model that introduces long-range modeling and global feature detection capabilities for accurate temporal action detection. MambaTAD comprises two novel designs that complement each other with superior TAD performance. First, it introduces a Diagonal-Masked Bidirectional State-Space (DMBSS) module which effectively facilitates global feature fusion and temporal action detection. Second, it introduces a global feature fusion head that refines the detection progressively with multi-granularity features and global awareness. In addition, MambaTAD tackles TAD in an end-to-end one-stage manner using a new state-space temporal adapter(SSTA) which reduces network parameters and computation cost with linear complexity. Extensive experiments show that MambaTAD achieves superior TAD performance consistently across multiple public benchmarks.', 'abstract_zh': '基于时间的行动检测（TAD）旨在通过确定不裁剪视频中动作的起始和结束帧来识别和定位动作。最近的结构化状态空间模型如Mamba由于其长距离建模能力和线性计算复杂度，在TAD中展现出了潜在的应用价值。然而，结构化状态空间模型在TAD中通常面临两个关键挑战，即由于递归处理导致的时间上下文衰减，以及在全局视觉上下文建模过程中产生的自我元素冲突，这些问题在处理长时段动作实例时尤为严重。此外，传统的TAD方法由于缺乏全局意识和不高效的检测头，在检测长时段动作实例时存在挑战。本文提出了一种新的状态空间TAD模型MambaTAD，该模型引入了长距离建模和全局特征检测能力，以实现精确的时间动作检测。MambaTAD包含两种互补的设计，能够以卓越的TAD性能相辅相成。首先，它引入了对角遮罩双向状态空间（DMBSS）模块，有效地促进了全局特征融合和时间动作检测。其次，它引入了全局特征融合头，通过多粒度特征和全局意识逐步完善检测。此外，MambaTAD采用了一种新的状态空间时间适配器（SSTA）以端到端一阶段的方式处理TAD，其计算参数和计算成本线性减少。广泛的实验表明，MambaTAD在多个公开基准上始终实现了卓越的TAD性能。', 'title_zh': 'MambaTAD: 当状态空间模型遇到长范围时间动作检测'}
{'arxiv_id': 'arXiv:2511.17881', 'title': 'MGA-VQA: Secure and Interpretable Graph-Augmented Visual Question Answering with Memory-Guided Protection Against Unauthorized Knowledge Use', 'authors': 'Ahmad Mohammadshirazi, Pinaki Prasad Guha Neogi, Dheeraj Kulshrestha, Rajiv Ramnath', 'link': 'https://arxiv.org/abs/2511.17881', 'abstract': 'Document Visual Question Answering (DocVQA) requires models to jointly understand textual semantics, spatial layout, and visual features. Current methods struggle with explicit spatial relationship modeling, inefficiency with high-resolution documents, multi-hop reasoning, and limited interpretability. We propose MGA-VQA, a multi-modal framework that integrates token-level encoding, spatial graph reasoning, memory-augmented inference, and question-guided compression. Unlike prior black-box models, MGA-VQA introduces interpretable graph-based decision pathways and structured memory access for enhanced reasoning transparency. Evaluation across six benchmarks (FUNSD, CORD, SROIE, DocVQA, STE-VQA, and RICO) demonstrates superior accuracy and efficiency, with consistent improvements in both answer prediction and spatial localization.', 'abstract_zh': '文档视觉问答（DocVQA）要求模型联合理解文本语义、空间布局和视觉特征。现有方法在显式的空间关系建模、高分辨率文档处理的效率、多跳推理以及模型可解释性方面存在困难。我们提出了一种多模态框架MGA-VQA，结合了令牌级编码、空间图推理、记忆增强推理和问题引导的压缩。与之前的黑盒模型不同，MGA-VQA 引入了可解释的基于图的决策路径和结构化记忆访问，以增强推理透明度。在六个基准数据集（FUNSD、CORD、SROIE、DocVQA、STE-VQA 和 RICO）上的评估显示了更高的准确性和效率，并且在答案预测和空间定位方面均表现出一致的改进。', 'title_zh': 'MGA-VQA：基于记忆引导保护未经授权知识使用的安全可解释图增强视觉问答'}
{'arxiv_id': 'arXiv:2511.17844', 'title': 'Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation', 'authors': 'Shihan Cheng, Nilesh Kulkarni, David Hyde, Dmitriy Smirnov', 'link': 'https://arxiv.org/abs/2511.17844', 'abstract': 'Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic "real" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.', 'abstract_zh': '细调大规模文本到视频扩散模型以学习新的生成控制（如物理相机参数控制），通常需要难以获取的大量高保真数据。在本文中，我们提出了一种数据高效细调策略，可以从稀疏的低质量合成数据中学习这些控制。我们展示了仅在这种简单数据上进行细调不仅能够实现所需控制，而且在某些情况下甚至优于在照片写实的“真实”数据上进行细调的模型。除了展示这些结果外，我们还提供了一个框架，从直觉和定量的角度解释了这一现象。', 'title_zh': '少即是多：高效的数据适应方法以实现可控的文本到视频生成'}
{'arxiv_id': 'arXiv:2511.17806', 'title': 'REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion', 'authors': 'Ryoma Yataka, Pu Perry Wang, Petros Boufounos, Ryuhei Takahashi', 'link': 'https://arxiv.org/abs/2511.17806', 'abstract': 'Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \\textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset.', 'abstract_zh': '多视图室内雷达感知由于其经济性和低隐私风险引起了关注。现有的方法往往依赖于隐式的跨视图雷达特征关联，如RFMask中的proposal pairing或RETR中的query-to-feature跨注意力，这可能导致特征匹配模糊并在复杂室内场景中检测性能下降。为了解决这些限制，我们提出了一种名为\\textbf{REXO}（多视图雷达目标检测中的3D边界盒扩散）的方法，将DiffusionDet的2D边界盒扩散过程提升到3D雷达空间中。REXO利用这些嘈杂的3D边界盒来指导显式的跨视图雷达特征关联，增强跨视图雷达条件下的去噪过程。通过考虑到人的先验知识即人与地面接触，REXO减少了扩散参数的数量，通过从这一先验确定它们。在两个开源室内雷达数据集上进行评估，我们的方法在HIBER数据集上超越了最先进的方法，AP值提高4.22，在MMVR数据集上AP值提高了11.02。', 'title_zh': 'REXO: 基于3D 方框扩散的室内多视图雷达目标检测'}
{'arxiv_id': 'arXiv:2511.17747', 'title': 'AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations', 'authors': 'Dawid Wolkiewicz, Anastasiya Pechko, Przemysław Spurek, Piotr Syga', 'link': 'https://arxiv.org/abs/2511.17747', 'abstract': "The growing adoption of photorealistic 3D facial avatars, particularly those utilizing efficient 3D Gaussian Splatting representations, introduces new risks of online identity theft, especially in systems that rely on biometric authentication. While effective adversarial masking methods have been developed for 2D images, a significant gap remains in achieving robust, viewpoint-consistent identity protection for dynamic 3D avatars. To address this, we present AEGIS, the first privacy-preserving identity masking framework for 3D Gaussian Avatars that maintains the subject's perceived characteristics. Our method aims to conceal identity-related facial features while preserving the avatar's perceptual realism and functional integrity. AEGIS applies adversarial perturbations to the Gaussian color coefficients, guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry. AEGIS achieves complete de-identification, reducing face retrieval and verification accuracy to 0%, while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB). It also preserves key facial attributes such as age, race, gender, and emotion, demonstrating strong privacy protection with minimal visual distortion.", 'abstract_zh': '三维高斯点云头像的隐私保护身份遮掩框架：AEGIS', 'title_zh': 'AEGIS: 用对抗扰动 preserving 3D面部avatar的隐私'}
{'arxiv_id': 'arXiv:2511.17655', 'title': 'Explainable Deep Learning for Brain Tumor Classification: Comprehensive Benchmarking with Dual Interpretability and Lightweight Deployment', 'authors': 'Md. Mohaiminul Islam, Md. Mofazzal Hossen, Maher Ali Rusho, Nahiyan Nazah Ridita, Zarin Tasnia Shanta, Md. Simanto Haider, Ahmed Faizul Haque Dhrubo, Md. Khurshid Jahan, Mohammad Abdul Qayum', 'link': 'https://arxiv.org/abs/2511.17655', 'abstract': 'Our study provides a full deep learning system for automated classification of brain tumors from MRI images, includes six benchmarked architectures (five ImageNet-pre-trained models (VGG-16, Inception V3, ResNet-50, Inception-ResNet V2, Xception) and a custom built, compact CNN (1.31M params)). The study moves the needle forward in a number of ways, including (1) full standardization of assessment with respect to preprocessing, training sets/protocols (optimizing networks with the AdamW optimizer, CosineAnnealingLR, patiene for early stopping = 7), and metrics to assess performance were identical along all models; (2) a high level of confidence in the localizations based on prior studies as both Grad-CAM and GradientShap explanation were used to establish anatomically important and meaningful attention regions and address the black-box issue; (3) a compact 1.31 million parameter CNN was developed that achieved 96.49% testing accuracy and was 100 times smaller than Inception-ResNet V2 while permitting real-time inference (375ms) on edge devices; (4) full evaluation beyond accuracy reporting based on measures of intersection over union, Hausdorff distance, and precision-recall curves, and confusion matrices across all splits. Inception-ResNet V2 reached state-of-the-art performance, achieving a 99.53% accuracy on testing and obtaining a precision, recall, and F1-score of at least 99.50% dominant performance based on metrics of recent studies. We demonstrated a lightweight model that is suitable to deploy on devices that do not have multi-GPU infrastructure in under-resourced settings. This end-to-end solution considers accuracy, interpretability, and deployability of trustworthy AI to create the framework necessary for performance assessment and deployment within advance and low-resource healthcare systems to an extent that enabled participation at the clinical screening and triage level.', 'abstract_zh': '我们的研究提供了一套完整的深度学习系统，用于从MRI图像自动分类脑肿瘤，包括六种基准架构（五种ImageNet预训练模型（VGG-16、Inception V3、ResNet-50、Inception-ResNet V2、Xception）和一个自定义构建的紧凑型CNN（参数量1.31M））。该研究在多个方面取得了进展，包括（1）在预处理、训练集/协议（使用AdamW优化器、CosineAnnealingLR，早停耐心为7）和评估性能的指标方面实现全面标准化；（2）基于先前研究的高置信度局部化，使用Grad-CAM和GradientShap解释来确定具有解剖学重要性和意义的关注区域，解决黑盒问题；（3）开发了一个紧凑型的参数量为1.31M的CNN，测试准确率达到96.49%，比Inception-ResNet V2小100倍，同时在边缘设备上实现实时推理（375ms）；（4）基于交并比、hausdorff距离以及精确率召回率曲线和混淆矩阵，进行全面评估而不仅仅是报告准确性。Inception-ResNet V2达到了最先进的性能，测试准确率达到99.53%，并基于近期研究的指标实现了精度、召回率和F1分数均值99.50%以上的主导性能。我们展示了适合在多GPU基础设施不足的环境中部署的轻量级模型。该端到端解决方案考虑准确度、可解释性和可部署性，为建立可信赖的人工智能性能评估和部署框架提供了必要条件，能够应用于先进和低资源的医疗保健系统，并允许在临床筛查和分诊水平上参与。', 'title_zh': '可解释的深度学习在脑肿瘤分类中的应用：兼具双重可解释性和轻量部署的全面基准测试'}
{'arxiv_id': 'arXiv:2511.17615', 'title': 'Plug-and-Play Multi-Concept Adaptive Blending for High-Fidelity Text-to-Image Synthesis', 'authors': 'Young-Beom Woo', 'link': 'https://arxiv.org/abs/2511.17615', 'abstract': 'Integrating multiple personalized concepts into a single image has recently become a significant area of focus within Text-to-Image (T2I) generation. However, existing methods often underperform on complex multi-object scenes due to unintended alterations in both personalized and non-personalized regions. This not only fails to preserve the intended prompt structure but also disrupts interactions among regions, leading to semantic inconsistencies. To address this limitation, we introduce plug-and-play multi-concept adaptive blending for high-fidelity text-to-image synthesis (PnP-MIX), an innovative, tuning-free approach designed to seamlessly embed multiple personalized concepts into a single generated image. Our method leverages guided appearance attention to faithfully reflect the intended appearance of each personalized concept. To further enhance compositional fidelity, we present a mask-guided noise mixing strategy that preserves the integrity of non-personalized regions such as the background or unrelated objects while enabling the precise integration of personalized objects. Finally, to mitigate concept leakage, i.e., the inadvertent leakage of personalized concept features into other regions, we propose background dilution++, a novel strategy that effectively reduces such leakage and promotes accurate localization of features within personalized regions. Extensive experimental results demonstrate that PnP-MIX consistently surpasses existing methodologies in both single- and multi-concept personalization scenarios, underscoring its robustness and superior performance without additional model tuning.', 'abstract_zh': '面向高保真文本到图像合成的即插即用多概念自适应融合（PnP-MIX）', 'title_zh': '即插即用多概念自适应融合用于高保真文本到图像合成'}
{'arxiv_id': 'arXiv:2511.17612', 'title': 'Unified Low-Light Traffic Image Enhancement via Multi-Stage Illumination Recovery and Adaptive Noise Suppression', 'authors': 'Siddiqua Namrah', 'link': 'https://arxiv.org/abs/2511.17612', 'abstract': 'Enhancing low-light traffic images is crucial for reliable perception in autonomous driving, intelligent transportation, and urban surveillance systems. Nighttime and dimly lit traffic scenes often suffer from poor visibility due to low illumination, noise, motion blur, non-uniform lighting, and glare from vehicle headlights or street lamps, which hinder tasks such as object detection and scene understanding. To address these challenges, we propose a fully unsupervised multi-stage deep learning framework for low-light traffic image enhancement. The model decomposes images into illumination and reflectance components, progressively refined by three specialized modules: (1) Illumination Adaptation, for global and local brightness correction; (2) Reflectance Restoration, for noise suppression and structural detail recovery using spatial-channel attention; and (3) Over-Exposure Compensation, for reconstructing saturated regions and balancing scene luminance. The network is trained using self-supervised reconstruction, reflectance smoothness, perceptual consistency, and domain-aware regularization losses, eliminating the need for paired ground-truth images. Experiments on general and traffic-specific datasets demonstrate superior performance over state-of-the-art methods in both quantitative metrics (PSNR, SSIM, LPIPS, NIQE) and qualitative visual quality. Our approach enhances visibility, preserves structure, and improves downstream perception reliability in real-world low-light traffic scenarios.', 'abstract_zh': '低光照交通图像增强对于自动驾驶、智能交通和城市监控系统的可靠感知至关重要。为了应对夜间和照明不足的交通场景中存在的低光照、噪声、运动模糊、非均匀照明和车灯或街灯眩光等问题，我们提出了一种全无监督多阶段深度学习框架进行低光照交通图像增强。该模型将图像分解为光照和反射成分，并由三个专门模块逐步精炼：（1）光照适应，用于全局和局部亮度校正；（2）反射恢复，利用空间-通道注意力抑制噪声并恢复结构细节；（3）过度曝光补偿，用于重建饱和区域并平衡场景亮度。网络采用自我监督重构、反射平滑性、感知一致性及域感知正则化损失进行训练，无需配对的真实图像。实验结果表明，与现有方法相比，我们的方法在多个定量指标（PSNR、SSIM、LPIPS、NIQE）和定性视觉质量上表现出更优性能。我们的方法提高了低光照交通场景中的可见度，保留了结构完整性，并在下游感知可靠性方面得到了增强。', 'title_zh': '多阶段 illumination 恢复与自适应噪声抑制的统一线路低光照交通图像增强'}
{'arxiv_id': 'arXiv:2511.17576', 'title': 'Multimodal AI for Body Fat Estimation: Computer Vision and Anthropometry with DEXA Benchmarks', 'authors': 'Rayan Aldajani', 'link': 'https://arxiv.org/abs/2511.17576', 'abstract': 'Tracking body fat percentage is essential for effective weight management, yet gold-standard methods such as DEXA scans remain expensive and inaccessible for most people. This study evaluates the feasibility of artificial intelligence (AI) models as low-cost alternatives using frontal body images and basic anthropometric data. The dataset consists of 535 samples: 253 cases with recorded anthropometric measurements (weight, height, neck, ankle, and wrist) and 282 images obtained via web scraping from Reddit posts with self-reported body fat percentages, including some reported as DEXA-derived by the original posters. Because no public datasets exist for computer-vision-based body fat estimation, this dataset was compiled specifically for this study. Two approaches were developed: (1) ResNet-based image models and (2) regression models using anthropometric measurements. A multimodal fusion framework is also outlined for future expansion once paired datasets become available. The image-based model achieved a Root Mean Square Error (RMSE) of 4.44% and a Coefficient of Determination (R^2) of 0.807. These findings demonstrate that AI-assisted models can offer accessible and low-cost body fat estimates, supporting future consumer applications in health and fitness.', 'abstract_zh': '基于人工智模型的低成像成本身体脂肪百分比估计可行性研究', 'title_zh': '多模态AI在身体脂肪估计中的应用：基于DEXA标准的计算机视觉与人类测量技术'}
{'arxiv_id': 'arXiv:2511.17547', 'title': 'SYNAPSE: Synergizing an Adapter and Finetuning for High-Fidelity EEG Synthesis from a CLIP-Aligned Encoder', 'authors': 'Jeyoung Lee, Hochul Kang', 'link': 'https://arxiv.org/abs/2511.17547', 'abstract': 'Recent progress in diffusion-based generative models has enabled high-quality image synthesis conditioned on diverse modalities. Extending such models to brain signals could deepen our understanding of human perception and mental representations. However,electroencephalography (EEG) presents major challenges for image generation due to high noise, low spatial resolution, and strong inter-subject variability. Existing approaches,such as DreamDiffusion, BrainVis, and GWIT, primarily adapt EEG features to pre-trained Stable Diffusion models using complex alignment or classification pipelines, often resulting in large parameter counts and limited interpretability. We introduce SYNAPSE, a two-stage framework that bridges EEG signal representation learning and high-fidelity image synthesis. In Stage1, a CLIP-aligned EEG autoencoder learns a semantically structured latent representation by combining signal reconstruction and cross-modal alignment objectives. In Stage2, the pretrained encoder is frozen and integrated with a lightweight adaptation of Stable Diffusion, enabling efficient conditioning on EEG features with minimal trainable parameters. Our method achieves a semantically coherent latent space and state-of-the-art perceptual fidelity on the CVPR40 dataset, outperforming prior EEG-to-image models in both reconstruction efficiency and image quality. Quantitative and qualitative analyses demonstrate that SYNAPSE generalizes effectively across subjects, preserving visual semantics even when class-level agreement is reduced. These results suggest that reconstructing what the brain perceives, rather than what it classifies, is key to faithful EEG-based image generation.', 'abstract_zh': '近期基于扩散的生成模型进展使多样模态条件下的高质量图像合成成为可能。将此类模型扩展到脑电图信号可以深化我们对人类知觉和心理表征的理解。然而，由于高噪声、低空间分辨率和强烈的被试间变异性，脑电图（EEG）对图像生成提出了重大挑战。现有的方法，如DreamDiffusion、BrainVis和GWIT，主要通过复杂的对齐或分类管道将EEG特征适应预训练的稳定扩散模型，通常会导致大量参数和有限的可解释性。我们提出SYNAPSE，一种两阶段框架，将EEG信号表示学习与高保真图像合成相结合。在第一阶段，CLIP对齐的EEG自编码器通过结合信号重建和跨模态对齐目标学习具有语义结构的潜在表示。在第二阶段，冻结预训练编码器并与轻量级的稳定扩散适应版本集成，使神经网络能够以最少的可训练参数高效地对EEG特征进行条件化。我们的方法在CVPR40数据集上实现了语义一致的潜在空间和感知保真度的最新成果，在重建效率和图像质量方面均优于先前的EEG到图像模型。定量和定性分析表明，SYNAPSE在被试间具有较强的泛化能力，即使分类一致性降低，也能保持视觉语义。这些结果表明，重建大脑所感知的内容而非分类内容是忠实的EEG图像生成的关键。', 'title_zh': 'SYNAPSE: 结合适配器和微调以实现与CLIP对齐编码器的高保真EEG合成'}
