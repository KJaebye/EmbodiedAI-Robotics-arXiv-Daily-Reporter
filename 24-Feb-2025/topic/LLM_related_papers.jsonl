{'arxiv_id': 'arXiv:2502.15652', 'title': 'Empowering LLMs with Logical Reasoning: A Comprehensive Survey', 'authors': 'Fengxiang Cheng, Haoxuan Li, Fenrong Liu, Robert van Rooij, Kun Zhang, Zhouchen Lin', 'link': 'https://arxiv.org/abs/2502.15652', 'abstract': 'Large language models (LLMs) have achieved remarkable successes on various natural language tasks. However, recent studies have found that there are still significant challenges to the logical reasoning abilities of LLMs. This paper summarizes and categorizes the main challenges into two aspects: (1) Logical question answering, LLMs often fail to generate the correct answer within complex logical problem which requires sophisticated deductive, inductive or abductive reasoning given a collection of premises and constrains. (2) Logical consistency, LLMs are prone to producing responses contradicting themselves across different questions. For example, a state-of-the-art Macaw question-answering LLM answers Yes to both questions Is a magpie a bird? and Does a bird have wings? but answers No to Does a magpie have wings?. To facilitate this research direction, we comprehensively investigate the most cutting-edge methods and propose detailed taxonomies of these methods. Specifically, to accurately answer complex logic questions, previous methods can be categorized based on reliance on external solvers, prompts, pretraining, and fine-tuning. To avoid logical contradictions, we discuss concepts and solutions of various logical consistencies, including implication, negation, transitivity, factuality consistency, and their composites. In addition, we review commonly used benchmark datasets and evaluation metrics, and discuss promising research directions, such as extensions to modal logic to account for uncertainty, and efficient algorithms satisfying multiple logical consistencies simultaneously.', 'abstract_zh': '大型语言模型在各类自然语言任务中取得了显著成果，然而最近的研究发现，这些模型在逻辑推理能力方面仍面临重大挑战。本文从两个方面总结和分类了主要挑战：（1）逻辑问答，大型语言模型在面对需要复杂演绎、归纳或 abduction 推理的逻辑问题集合时，往往不能生成正确的答案。（2）逻辑一致性，大型语言模型容易在同一问题集合中产生自相矛盾的回复。为了促进该研究方向的发展，本文全面调查了最新的方法并提出了这些方法的详细分类。具体而言，为了准确回答复杂的逻辑问题，先前的方法可以根据对外部求解器、提示、预训练和微调的依赖关系进行分类。为了避免逻辑矛盾，本文讨论了各种逻辑一致性的概念及解决方案，包括蕴含、否定、传递性、事实一致性及其复合。此外，本文回顾了常用的标准数据集和评估指标，并讨论了有潜力的研究方向，如扩展到模态逻辑以处理不确定性，并提出同时满足多种逻辑一致性的高效算法。', 'title_zh': '增强大语言模型的逻辑推理能力：一项全面综述'}
{'arxiv_id': 'arXiv:2502.15392', 'title': 'Chitrarth: Bridging Vision and Language for a Billion People', 'authors': 'Shaharukh Khan, Ayush Tarun, Abhinav Ravi, Ali Faraz, Akshat Patidar, Praveen Kumar Pokala, Anagha Bhangare, Raja Kolla, Chandra Khatri, Shubham Agarwal', 'link': 'https://arxiv.org/abs/2502.15392', 'abstract': 'Recent multimodal foundation models are primarily trained on English or high resource European language data, which hinders their applicability to other medium and low-resource languages. To address this limitation, we introduce Chitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model (VLM), specifically targeting the rich linguistic diversity and visual reasoning across 10 prominent Indian languages. Our model effectively integrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM) with a vision module, primarily trained on multilingual image-text data. Furthermore, we also introduce BharatBench, a comprehensive framework for evaluating VLMs across various Indian languages, ultimately contributing to more diverse and effective AI systems. Our model achieves SOTA results for benchmarks across low resource languages while retaining its efficiency in English. Through our research, we aim to set new benchmarks in multilingual-multimodal capabilities, offering substantial improvements over existing models and establishing a foundation to facilitate future advancements in this arena.', 'abstract_zh': '最近的多模态基础模型主要在英语或高资源欧洲语言数据上进行训练，这限制了它们在其他中低资源语言中的适用性。为解决这一局限，我们引入了Chitrarth（Chitra：图像；Artha：意义），一个针对10种 prominent 印地语的包容性视觉-语言模型，专门针对这些语言丰富的语言多样性和视觉推理能力。我们的模型有效结合了一种最先进的多语言大型语言模型（LLM）和一个视觉模块，该模块主要在多语言图像-文本数据上进行训练。此外，我们还提出了BharatBench，一个综合框架，用于评估各种印地语的视觉-语言模型，最终促进了更多样化和有效的AI系统的发展。我们的模型在低资源语言基准测试中取得了最先进的成果，同时保持了其在英语中的效率。通过我们的研究，我们旨在建立多语言-多模态能力的新基准，提供现有模型的重大改进，并奠定未来在此领域发展的基础。', 'title_zh': 'Chitrarth：连接视觉与语言，惠及十亿人群'}
{'arxiv_id': 'arXiv:2502.15359', 'title': 'ARS: Automatic Routing Solver with Large Language Models', 'authors': 'Kai Li, Fei Liu, Zhenkun Wang, Xialiang Tong, Xiongwei Han, Mingxuan Yuan', 'link': 'https://arxiv.org/abs/2502.15359', 'abstract': 'Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of practical constraints, making manual solver design both knowledge-intensive and time-consuming. Although there is increasing interest in automating the design of routing algorithms, existing research has explored only a limited array of VRP variants and fails to adequately address the complex and prevalent constraints encountered in real-world situations. To fill this gap, this paper introduces RoutBench, a benchmark of 1,000 VRP variants derived from 24 attributes, for evaluating the effectiveness of automatic routing solvers in addressing complex constraints. Along with RoutBench, we present the Automatic Routing Solver (ARS), which employs Large Language Model (LLM) agents to enhance a backbone algorithm framework by automatically generating constraint-aware heuristic code, based on problem descriptions and several representative constraints selected from a database. Our experiments show that ARS outperforms state-of-the-art LLM-based methods and commonly used solvers, automatically solving 91.67% of common VRPs and achieving at least a 30% improvement across all benchmarks.', 'abstract_zh': '实际应用场景中的车辆路线问题（VRPs）由多种实际约束构成，使得手动设计求解器既知识密集型又耗时。尽管有越来越多的兴趣在于自动化设计路由算法，现有的研究仅探索了VRP的有限变体，并未能充分应对实际场景中复杂且普遍存在的约束。为了弥补这一不足，本文引入了RoutBench，这是一个由24个属性衍生出的1000个VRP变体基准，用于评估自动路由求解器在应对复杂约束时的有效性。同时，本文还提出了自动路由求解器（ARS），该求解器利用大型语言模型（LLM）代理自动为骨架算法框架生成感知约束的启发式代码，基于问题描述和数据库中选定的几个代表性约束。实验结果表明，ARS优于最先进的基于LLM的方法和常用求解器，在91.67%的常见VRP实例中自动求解，并在所有基准测试中至少达到30%的改进。', 'title_zh': 'ARS: 使用大规模语言模型的自动路由求解器'}
{'arxiv_id': 'arXiv:2502.15677', 'title': 'FLEKE: Federated Locate-then-Edit Knowledge Editing', 'authors': 'Zongkai Zhao, Guozeng Xu, Xiuhua Li, Kaiwen Wei, Jiang Zhong', 'link': 'https://arxiv.org/abs/2502.15677', 'abstract': 'Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating large language models (LLMs) without full retraining. However, existing methods assume a single-user setting and become inefficient in real-world multi-client scenarios, where decentralized organizations (e.g., hospitals, financial institutions) independently update overlapping knowledge, leading to redundant mediator knowledge vector (MKV) computations and privacy concerns. To address these challenges, we introduce Federated Locate-then-Edit Knowledge Editing (FLEKE), a novel task that enables multiple clients to collaboratively perform LEKE while preserving privacy and reducing computational overhead. To achieve this, we propose FedEdit, a two-stage framework that optimizes MKV selection and reuse. In the first stage, clients locally apply LEKE and upload the computed MKVs. In the second stage, rather than relying solely on server-based MKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine similarity, enabling knowledge re-edit and minimizing redundant computations. Experimental results on two benchmark datasets demonstrate that FedEdit retains over 96% of the performance of non-federated LEKE while significantly outperforming a FedAvg-based baseline by approximately twofold. Besides, we find that MEMIT performs more consistently than PMET in the FLEKE task with our FedEdit framework. Our code is available at this https URL.', 'abstract_zh': '联邦定位编辑知识编辑（FLEKE）', 'title_zh': 'FLEKE: 联邦定位编辑知识编辑'}
{'arxiv_id': 'arXiv:2502.15666', 'title': 'Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing', 'authors': 'Shoumik Saha, Soheil Feizi', 'link': 'https://arxiv.org/abs/2502.15666', 'abstract': 'The growing use of large language models (LLMs) for text generation has led to widespread concerns about AI-generated content detection. However, an overlooked challenge is AI-polished text, where human-written content undergoes subtle refinements using AI tools. This raises a critical question: should minimally polished text be classified as AI-generated? Misclassification can lead to false plagiarism accusations and misleading claims about AI prevalence in online content. In this study, we systematically evaluate eleven state-of-the-art AI-text detectors using our AI-Polished-Text Evaluation (APT-Eval) dataset, which contains $11.7K$ samples refined at varying AI-involvement levels. Our findings reveal that detectors frequently misclassify even minimally polished text as AI-generated, struggle to differentiate between degrees of AI involvement, and exhibit biases against older and smaller models. These limitations highlight the urgent need for more nuanced detection methodologies.', 'abstract_zh': '大型语言模型（LLMs）在文本生成中的广泛应用引发了对AI生成内容检测的广泛关切。然而，一个被忽视的挑战是AI润饰文本，即人类撰写的文本通过AI工具进行微妙改进。这引发了关键问题：轻微润饰的文本是否应被视为AI生成？误分类可能导致虚假的剽窃指控和关于在线内容中AI普及程度的误导性说法。在本研究中，我们使用包含按不同AI参与程度 refinement 的11,700个样本的AI润饰文本评估（APT-Eval）数据集，系统评估了十一种最先进的AI文本检测器。我们的发现表明，检测器经常错误地将轻微润饰的文本分类为AI生成，难以区分不同程度的AI参与，并偏向于老款和小型模型。这些局限性突显了更细致的检测方法的迫切需求。', 'title_zh': '几乎人工智能，几乎human：检测AI润色写作的挑战'}
{'arxiv_id': 'arXiv:2502.15639', 'title': 'Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models', 'authors': 'Anirudh Sundar, Sinead Williamson, Katherine Metcalf, Barry-John Theobald, Skyler Seto, Masha Fedzechkina', 'link': 'https://arxiv.org/abs/2502.15639', 'abstract': "Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. A data-efficient alternative to fine-tuning is model interventions -- a method for manipulating model activations to steer generation into the desired direction. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in mLLMs. We identify the neurons to manipulate for a given language and introspect the embedding space of mLLMs pre- and post-manipulation. We show that modifying the mLLM's activations changes its embedding space such that cross-lingual alignment is enhanced. Further, we show that the changes to the embedding space translate into improved downstream performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on cross-lingual retrieval.", 'abstract_zh': '多语言大型语言模型中跨语言表示的一致性是一种期望属性，数据高效的方法：通过干预调整跨语言表示的一致性', 'title_zh': '转向新的嵌入空间：分析多语言模型中模型干预诱导的跨语言对齐'}
{'arxiv_id': 'arXiv:2502.15631', 'title': 'The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer', 'authors': 'Marthe Ballon, Andres Algaba, Vincent Ginis', 'link': 'https://arxiv.org/abs/2502.15631', 'abstract': 'Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies.', 'abstract_zh': '大规模语言模型在数学推理方面取得了显著进展，利用了思维链和测试时计算量扩展。然而，关于推理令牌使用与准确率提升之间的相互作用仍有许多待解答的问题。特别是在比较不同代际模型时，改善性能是源自更长的思维链还是更高效的推理尚不明确。我们系统地分析了在Omni-MATH基准上o1-mini和o3-mini变种的思维链长度，发现o3-mini (m)在不需要更长的思维链的情况下取得了更高的准确率。此外，我们展示了在所有模型和计算设置中，随着思维链的增长，准确率通常会下降，即使控制了问题难度。准确率的下降在能力更强的模型中更为轻微，表明新一代推理模型在测试时计算量的使用上更为有效。最后，我们指出虽然o3-mini (h)相对于o3-mini (m)在准确率上取得轻微提升，但它通过在所有问题上分配更多的推理令牌来实现这一点，即使是o3-mini (m)已经能够解决的问题也是如此。这些发现为模型能力与推理长度之间的关系提供了新的见解，对于效率、扩展性和评估方法具有重要意义。', 'title_zh': '大型语言模型中推理与性能之间的关系——o3（mini）思考更深，而非更久'}
{'arxiv_id': 'arXiv:2502.15618', 'title': 'Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing', 'authors': 'Qi Le, Enmao Diao, Ziyan Wang, Xinran Wang, Jie Ding, Li Yang, Ali Anwar', 'link': 'https://arxiv.org/abs/2502.15618', 'abstract': "We introduce Probe Pruning (PP), a novel framework for online, dynamic, structured pruning of Large Language Models (LLMs) applied in a batch-wise manner. PP leverages the insight that not all samples and tokens contribute equally to the model's output, and probing a small portion of each batch effectively identifies crucial weights, enabling tailored dynamic pruning for different batches. It comprises three main stages: probing, history-informed pruning, and full inference. In the probing stage, PP selects a small yet crucial set of hidden states, based on residual importance, to run a few model layers ahead. During the history-informed pruning stage, PP strategically integrates the probing states with historical states. Subsequently, it structurally prunes weights based on the integrated states and the PP importance score, a metric developed specifically to assess the importance of each weight channel in maintaining performance. In the final stage, full inference is conducted on the remaining weights. A major advantage of PP is its compatibility with existing models, as it operates without requiring additional neural network modules or fine-tuning. Comprehensive evaluations of PP on LLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of FLOPs-can substantially enhance the efficiency of structured pruning of LLMs. For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56 times lower ratio of performance degradation per unit of runtime reduction compared to the state-of-the-art method at a 40% pruning ratio. Our code is available at this https URL.", 'abstract_zh': '探针剪枝 (Probe Pruning)：一种适用于大规模语言模型的在线动态结构剪枝框架', 'title_zh': '探针修剪：通过基于模型的探针进行动态修剪加速LLMs'}
{'arxiv_id': 'arXiv:2502.15609', 'title': 'On the Robustness of Transformers against Context Hijacking for Linear Classification', 'authors': 'Tianle Li, Chenyang Zhang, Xingwu Chen, Yuan Cao, Difan Zou', 'link': 'https://arxiv.org/abs/2502.15609', 'abstract': 'Transformer-based Large Language Models (LLMs) have demonstrated powerful in-context learning capabilities. However, their predictions can be disrupted by factually correct context, a phenomenon known as context hijacking, revealing a significant robustness issue. To understand this phenomenon theoretically, we explore an in-context linear classification problem based on recent advances in linear transformers. In our setup, context tokens are designed as factually correct query-answer pairs, where the queries are similar to the final query but have opposite labels. Then, we develop a general theoretical analysis on the robustness of the linear transformers, which is formulated as a function of the model depth, training context lengths, and number of hijacking context tokens. A key finding is that a well-trained deeper transformer can achieve higher robustness, which aligns with empirical observations. We show that this improvement arises because deeper layers enable more fine-grained optimization steps, effectively mitigating interference from context hijacking. This is also well supported by our numerical experiments. Our findings provide theoretical insights into the benefits of deeper architectures and contribute to enhancing the understanding of transformer architectures.', 'abstract_zh': '基于Transformer的大语言模型（LLMs）展示了强大的上下文学习能力。然而，它们的预测可能会被事实正确的上下文所破坏，这一现象被称为上下文劫持，揭示了一个重要且明显的鲁棒性问题。为了从理论上理解这一现象，我们基于近期线性变压器的发展，探索了一个基于上下文的线性分类问题。在我们的设置中，上下文标记被设计为事实正确的查询-答案对，其中查询类似于最终查询但标签相反。然后，我们发展了一个关于线性变压器鲁棒性的通用理论分析，该分析表示为模型深度、训练上下文长度和劫持上下文标记数量的函数。一个重要发现是，一个训练良好的更深的变压器可以实现更高的鲁棒性，这与实证观察相符。我们展示了这一改进是因为更深的层能够实现更精细的优化步骤，有效地抵消了上下文劫持的干扰。我们的数值实验也很好地支持了这一点。我们的研究结果为更深架构的优势提供了理论见解，并有助于增强对变压器架构的理解。', 'title_zh': '关于Transformer在线性分类中对上下文劫持的鲁棒性'}
{'arxiv_id': 'arXiv:2502.15603', 'title': 'Do Multilingual LLMs Think In English?', 'authors': 'Lisa Schut, Yarin Gal, Sebastian Farquhar', 'link': 'https://arxiv.org/abs/2502.15603', 'abstract': 'Large language models (LLMs) have multilingual capabilities and can solve tasks across various languages. However, we show that current LLMs make key decisions in a representation space closest to English, regardless of their input and output languages. Exploring the internal representations with a logit lens for sentences in French, German, Dutch, and Mandarin, we show that the LLM first emits representations close to English for semantically-loaded words before translating them into the target language. We further show that activation steering in these LLMs is more effective when the steering vectors are computed in English rather than in the language of the inputs and outputs. This suggests that multilingual LLMs perform key reasoning steps in a representation that is heavily shaped by English in a way that is not transparent to system users.', 'abstract_zh': '大型语言模型（LLMs）具有多语言能力，并能在多种语言上完成任务。然而，我们展示了当前LLMs在进行关键决策时往往会接近英语的表示空间，而不考虑其输入和输出的语言。通过对法语、德语、荷兰语和 Mandarin 中的句子进行 logit 视角的内部表示探索，我们发现LLM首先为语义负载单词生成接近英语的表示，然后再将其翻译成目标语言。进一步研究表明，在这些LLM中，当引导向量是在英语而不是输入和输出语言中计算时，激活引导更为有效。这表明多语言LLMs以一种对系统用户不透明的方式，在受英语影响极大的表示空间中执行关键推理步骤。', 'title_zh': '多语言大语言模型是否用英语思考？'}
{'arxiv_id': 'arXiv:2502.15592', 'title': 'Generalizing From Short to Long: Effective Data Synthesis for Long-Context Instruction Tuning', 'authors': 'Wenhao Zhu, Pinzhen Chen, Hanxu Hu, Shujian Huang, Fei Yuan, Jiajun Chen, Alexandra Birch', 'link': 'https://arxiv.org/abs/2502.15592', 'abstract': 'Long-context modelling for large language models (LLMs) has been a key area of recent research because many real world use cases require reasoning over longer inputs such as documents. The focus of research into modelling long context has been on how to model position and there has been little investigation into other important aspects of language modelling such as instruction tuning. Long context training examples are challenging and expensive to create and use. In this paper, we investigate how to design instruction data for the post-training phase of a long context pre-trained model: how much and what type of context is needed for optimal and efficient post-training. Our controlled study reveals that models instruction-tuned on short contexts can effectively generalize to longer ones, while also identifying other critical factors such as instruction difficulty and context composition. Based on these findings, we propose context synthesis, a novel data synthesis framework that leverages off-the-shelf LLMs to generate extended background contexts for high-quality instruction-answer pairs. Experiment results on the document-level benchmark (LongBench) demonstrate that our proposed approach outperforms previous instruction synthesis approaches and comes close to the performance of human-annotated long-context instruction data. The project will be available at: this https URL.', 'abstract_zh': '长上下文建模对于大型语言模型(LLMs)而言是近期研究的关键领域，因为许多实际应用需要在文档等较长输入上进行推理。关于长上下文建模的研究重点在于位置建模，对语言模型中的其他重要方面如指令调优则研究较少。长上下文训练样本的创建和使用既具有挑战性也较为昂贵。在本文中，我们探讨了针对预训练的长上下文模型的后训练阶段如何设计指令数据：如何设计、多少以及什么类型的上下文可以实现最优且高效的后训练。我们的控制研究发现，针对短上下文进行指令调优的模型能够有效泛化到长上下文，同时识别出其他关键因素，如指令难度和上下文组成。基于上述发现，我们提出了一种新颖的数据合成框架——上下文合成，该框架利用现成的LLM生成高质量指令-答案对的扩展背景上下文。在文档级别基准(LongBench)上的实验结果表明，我们提出的方法优于以往的指令合成方法，并且性能接近人工标注的长上下文指令数据。该项目将在此处提供：this https URL。', 'title_zh': '从短-context到长-context的泛化：有效的数据合成用于长-context指令调优'}
{'arxiv_id': 'arXiv:2502.15589', 'title': 'LightThinker: Thinking Step-by-Step Compression', 'authors': 'Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang', 'link': 'https://arxiv.org/abs/2502.15589', 'abstract': 'Large language models (LLMs) have shown remarkable performance in complex reasoning tasks, but their efficiency is hindered by the substantial memory and computational costs associated with generating lengthy tokens. In this paper, we propose LightThinker, a novel method that enables LLMs to dynamically compress intermediate thoughts during reasoning. Inspired by human cognitive processes, LightThinker compresses verbose thought steps into compact representations and discards the original reasoning chains, thereby significantly reducing the number of tokens stored in the context window. This is achieved by training the model on when and how to perform compression through data construction, mapping hidden states to condensed gist tokens, and creating specialized attention masks. Additionally, we introduce the Dependency (Dep) metric to quantify the degree of compression by measuring the reliance on historical tokens during generation. Extensive experiments on four datasets and two models show that LightThinker reduces peak memory usage and inference time, while maintaining competitive accuracy. Our work provides a new direction for improving the efficiency of LLMs in complex reasoning tasks without sacrificing performance. Code will be released at this https URL.', 'abstract_zh': '大语言模型（LLMs）在复杂推理任务中展现了卓越的性能，但生成长序列时的高效性受到大量内存和计算成本的限制。本文提出了一种名为LightThinker的新方法，能够使LLMs在推理过程中动态压缩中间思维。受到人类认知过程的启发，LightThinker将冗长的思维步骤压缩为紧凑表示，并丢弃原始的推理链，从而大幅减少存储在上下文窗口中的令牌数量。这通过在数据构造、将隐藏状态映射到浓缩的主旨令牌以及创建专门的注意力掩码等方式进行训练而实现。此外，我们引入了依赖度（Dep）指标来量化压缩的程度，通过衡量生成过程对历史令牌的依赖性。在四个数据集和两个模型上的实验显示，LightThinker在显著降低峰值内存使用和推理时间的同时，保持了竞争力的准确性。我们的工作为在不牺牲性能的情况下提高LLMs在复杂推理任务中的效率提供了新的方向。代码将在https://github.com/Qwen-Model/LightThinker仓库中发布。', 'title_zh': 'LightThinker: 步步压缩'}
{'arxiv_id': 'arXiv:2502.15568', 'title': 'A Cautionary Tale About "Neutrally" Informative AI Tools Ahead of the 2025 Federal Elections in Germany', 'authors': 'Ina Dormuth, Sven Franke, Marlies Hafer, Tim Katzke, Alexander Marx, Emmanuel Müller, Daniel Neider, Markus Pauly, Jérôme Rutinowski', 'link': 'https://arxiv.org/abs/2502.15568', 'abstract': "In this study, we examine the reliability of AI-based Voting Advice Applications (VAAs) and large language models (LLMs) in providing objective political information. Our analysis is based upon a comparison with party responses to 38 statements of the Wahl-O-Mat, a well-established German online tool that helps inform voters by comparing their views with political party positions. For the LLMs, we identify significant biases. They exhibit a strong alignment (over 75% on average) with left-wing parties and a substantially lower alignment with center-right (smaller 50%) and right-wing parties (around 30%). Furthermore, for the VAAs, intended to objectively inform voters, we found substantial deviations from the parties' stated positions in Wahl-O-Mat: While one VAA deviated in 25% of cases, another VAA showed deviations in more than 50% of cases. For the latter, we even observed that simple prompt injections led to severe hallucinations, including false claims such as non-existent connections between political parties and right-wing extremist ties.", 'abstract_zh': '本研究考察了基于AI的投票建议应用程序（VAAs）和大型语言模型（LLMs）在提供客观政治信息方面的可靠性。', 'title_zh': '关于2025年德国联邦选举前“中性”信息AI工具的警示故事'}
{'arxiv_id': 'arXiv:2502.15543', 'title': 'PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented Generation via Parametric Pruning', 'authors': 'Pengcheng Huang, Zhenghao Liu, Yukun Yan, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, Chenyan Xiong', 'link': 'https://arxiv.org/abs/2502.15543', 'abstract': "Knowledge-Augmented Generation (KAG) has shown great promise in updating the internal memory of Large Language Models (LLMs) by integrating external knowledge. However, KAG inevitably faces knowledge conflicts when the internal memory contradicts external information. Current approaches to mitigating these conflicts mainly focus on improving external knowledge utilization. However, these methods have shown only limited effectiveness in mitigating the knowledge conflict problem, as internal knowledge continues to influence the generation process of LLMs. In this paper, we propose a ParametrIc Pruning-based Knowledge-Augmented Generation (PIP-KAG) approach, which prunes internal knowledge of LLMs and incorporates a plug-and-play adaptation module to help LLMs better leverage external sources. Additionally, we construct the CoConflictQA benchmark based on the hallucination of LLMs to better evaluate contextual faithfulness during answering questions. Experimental results on CoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts and improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by 13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes are available at this https URL.", 'abstract_zh': 'ParametrIc Pruning-based Knowledge-Augmented Generation (PIP-KAG)及其在知识增强生成中的应用：减少知识冲突并提高上下文 fidelity', 'title_zh': 'PIP-KAG：通过参数化剪枝缓解知识增强生成中的知识冲突'}
{'arxiv_id': 'arXiv:2502.15507', 'title': 'Activation Steering in Neural Theorem Provers', 'authors': 'Shashank Kirtania', 'link': 'https://arxiv.org/abs/2502.15507', 'abstract': 'Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language models struggles to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle, we use activation steering to guide LLMs responses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valuable in resource-constrained environments.', 'abstract_zh': '大型语言模型在使用Lean等证明辅助器证明形式定理方面显示出潜力。然而，当前最先进的语言模型在预测证明中的下一步时存在困难，促使实践者使用不同的采样技术来提高LLM的能力。我们观察到，LLM能够预测正确的战术，但在适当排序候选战术的过程中面临挑战，影响整体选择过程。为了克服这一障碍，我们使用激活引导来指导LLM的响应，以在推理时改进生成。我们的结果表明，激活引导为在资源受限环境中增强LLM的定理证明能力提供了一种有希望的轻量级替代方案，特别是专门微调的替代方案。', 'title_zh': '神经定理证明中的激活转向'}
{'arxiv_id': 'arXiv:2502.15487', 'title': 'ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models', 'authors': 'Martina Miliani, Serenna Auriemma, Alessandro Bondielli, Emmanuele Chersoni, Lucia Passaro, Irene Sucameli, Alessandro Lenci', 'link': 'https://arxiv.org/abs/2502.15487', 'abstract': 'Large Language Models (LLMs) are increasingly used in tasks requiring interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely integrates both causal and temporal relations presented in different linguistic orders and explicitly expressed by linguistic connectives. The dataset is enriched with crowdsourced human acceptability ratings. We tested LLMs on ExpliCa through prompting and perplexity-based metrics. We assessed seven commercial and open-source LLMs, revealing that even top models struggle to reach 0.80 accuracy. Interestingly, models tend to confound temporal relations with causal ones, and their performance is also strongly influenced by the linguistic order of the events. Finally, perplexity-based scores and prompting performance are differently affected by model size.', 'abstract_zh': '大型语言模型（LLMs）在需要解释性和推理准确性的任务中日益广泛应用。本文介绍了ExpliCa，一个用于评估LLMs在显式因果推理中的新数据集。ExpliCa独特地整合了以不同语言顺序呈现的因果关系和时间关系，并通过语言连接词明确表达。该数据集包含了众包的人类接受性评分。我们通过提示和困惑度基指标测试了LLMs，并评估了七个商用和开源LLM，结果显示即使顶级模型也难以达到0.80的准确性。有趣的是，模型倾向于混淆时间关系与因果关系，而事件的语言顺序对模型性能也有强烈影响。最后，困惑度基评分和提示性能受模型规模的影响不同。', 'title_zh': 'ExpliCa: 评估大型语言模型中的显式因果推理能力'}
{'arxiv_id': 'arXiv:2502.15485', 'title': 'Enhancing RWKV-based Language Models for Long-Sequence Text Generation', 'authors': 'Xinghan Pan', 'link': 'https://arxiv.org/abs/2502.15485', 'abstract': 'This paper presents an enhanced RWKV-based language generation model designed to improve long-sequence text processing. We propose an adaptive token shift and gating mechanism to better capture long-range dependencies in text generation. Through a series of experiments, we compare the baseline RWKV model with the enhanced model, evaluating performance in terms of forward propagation time, text generation quality, and automatic evaluation metrics such as perplexity, BLEU, and ROUGE. Experimental results show that the enhanced model significantly improves generation quality, especially in BLEU and ROUGE scores, and demonstrates stronger context-capturing ability in long-text generation tasks.', 'abstract_zh': '基于增强RWKV的语言生成模型：提高长序列文本处理能力及长文本生成中的远程依赖捕捉机制', 'title_zh': '基于RWKV的长序列文本生成语言模型的增强方法'}
{'arxiv_id': 'arXiv:2502.15470', 'title': 'PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System', 'authors': 'Yintao He, Haiyu Mao, Christina Giannoula, Mohammad Sadrosadati, Juan Gómez-Luna, Huawei Li, Xiaowei Li, Ying Wang, Onur Mutlu', 'link': 'https://arxiv.org/abs/2502.15470', 'abstract': 'Large language models (LLMs) are widely used for natural language understanding and text generation. An LLM model relies on a time-consuming step called LLM decoding to generate output tokens. Several prior works focus on improving the performance of LLM decoding using parallelism techniques, such as batching and speculative decoding. State-of-the-art LLM decoding has both compute-bound and memory-bound kernels. Some prior works statically identify and map these different kernels to a heterogeneous architecture consisting of both processing-in-memory (PIM) units and computation-centric accelerators. We observe that characteristics of LLM decoding kernels (e.g., whether or not a kernel is memory-bound) can change dynamically due to parameter changes to meet user and/or system demands, making (1) static kernel mapping to PIM units and computation-centric accelerators suboptimal, and (2) one-size-fits-all approach of designing PIM units inefficient due to a large degree of heterogeneity even in memory-bound kernels.\nIn this paper, we aim to accelerate LLM decoding while considering the dynamically changing characteristics of the kernels involved. We propose PAPI (PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that exploits dynamic scheduling of compute-bound or memory-bound kernels to suitable hardware units. PAPI has two key mechanisms: (1) online kernel characterization to dynamically schedule kernels to the most suitable hardware units at runtime and (2) a PIM-enabled heterogeneous computing system that harmoniously orchestrates both computation-centric processing units and hybrid PIM units with different computing capabilities. Our experimental results on three broadly-used LLMs show that PAPI achieves 1.8$\\times$ and 11.1$\\times$ speedups over a state-of-the-art heterogeneous LLM accelerator and a state-of-the-art PIM-only LLM accelerator, respectively.', 'abstract_zh': '基于PIM的大规模语言模型解码加速器：动态调度计算和内存绑定内核', 'title_zh': 'PAPI: 利用计算系统中存内计算技术在大规模语言模型解码中发挥动态并行性优势'}
{'arxiv_id': 'arXiv:2502.15455', 'title': 'R-LoRA: Random Initialization of Multi-Head LoRA for Multi-Task Learning', 'authors': 'Jinda Liu, Yi Chang, Yuan Wu', 'link': 'https://arxiv.org/abs/2502.15455', 'abstract': "Fine-tuning large language models (LLMs) is prohibitively expensive in terms of computational and memory costs. Low-rank Adaptation (LoRA), as one of the most popular parameter-efficient fine-tuning (PEFT) methods, offers a cost-effective alternative by approximating the model changes $\\Delta W \\in \\mathbb{R}^{m \\times n}$ through the product of down-projection matrix $A \\in \\mathbb{R}^{m \\times r}$ and head matrix $B \\in \\mathbb{R}^{r \\times n}$, where $r \\ll \\min(m, n)$. In real-world scenarios, LLMs are fine-tuned on data from multiple domains to perform tasks across various fields, embodying multi-task learning (MTL). LoRA often underperforms in such complex scenarios. To enhance LoRA's capability in multi-task learning, we propose R-LoRA, which incorporates Multi-Head Randomization. Multi-Head Randomization diversifies the head matrices through Multi-Head Random Initialization and Multi-Head Dropout, enabling more efficient learning of task-specific features while maintaining shared knowledge representation. Extensive experiments demonstrate that R-LoRA is better at capturing task-specific knowledge, thereby improving performance in multi-task scenarios. The code is available at this https URL.", 'abstract_zh': '改进LoRA促进多任务学习：基于多头随机化的R-LoRA', 'title_zh': 'R-LoRA: 多任务学习中多头LoRA的随机初始化'}
{'arxiv_id': 'arXiv:2502.15443', 'title': 'When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models', 'authors': 'Weilan Wang, Yu Mao, Dongdong Tang, Hongchao Du, Nan Guan, Chun Jason Xue', 'link': 'https://arxiv.org/abs/2502.15443', 'abstract': 'Large language models (LLMs) exhibit excellent performance in various tasks. However, the memory requirements of LLMs present a great challenge when deploying on memory-limited devices, even for quantized LLMs. This paper introduces a framework to compress LLM after quantization further, achieving about 2.2x compression ratio. A compression-aware quantization is first proposed to enhance model weight compressibility by re-scaling the model parameters before quantization, followed by a pruning method to improve further. Upon this, we notice that decompression can be a bottleneck during practical scenarios. We then give a detailed analysis of the trade-off between memory usage and latency brought by the proposed method. A speed-adaptive method is proposed to overcome it. The experimental results show inference with the compressed model can achieve a 40% reduction in memory size with negligible loss in accuracy and inference speed.', 'abstract_zh': '大型语言模型（LLMs）在各种任务中表现出色。然而，LLMs的内存要求在部署到内存受限的设备上时会带来巨大挑战，即使是量化后的LLMs也是如此。本文提出了一种框架，在量化后进一步压缩LLMs，实现了约2.2倍的压缩比。首先提出了感知压缩的量化方法，通过重新缩放模型参数来增强模型权重的压缩性，随后采用了剪枝方法进一步改进。进一步观察到，在实际场景中解压缩可能会成为瓶颈。因此，我们对所提出方法带来的内存使用与延迟之间的权衡进行了详细分析。提出了一种自适应提速方法来克服这一问题。实验结果表明，使用压缩模型的推理可以帮助减少40%的内存大小，同时几乎不影响准确性和推理速度。', 'title_zh': '当压缩遇到模型压缩：适用于大型语言模型的内存高效双重压缩方法'}
{'arxiv_id': 'arXiv:2502.15435', 'title': 'Single-pass Detection of Jailbreaking Input in Large Language Models', 'authors': 'Leyla Naz Candogan, Yongtao Wu, Elias Abad Rocamora, Grigorios G. Chrysos, Volkan Cevher', 'link': 'https://arxiv.org/abs/2502.15435', 'abstract': 'Defending aligned Large Language Models (LLMs) against jailbreaking attacks is a challenging problem, with existing approaches requiring multiple requests or even queries to auxiliary LLMs, making them computationally heavy. Instead, we focus on detecting jailbreaking input in a single forward pass. Our method, called Single Pass Detection SPD, leverages the information carried by the logits to predict whether the output sentence will be harmful. This allows us to defend in just one forward pass. SPD can not only detect attacks effectively on open-source models, but also minimizes the misclassification of harmless inputs. Furthermore, we show that SPD remains effective even without complete logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a promising approach to efficiently safeguard LLMs against adversarial attacks.', 'abstract_zh': '防御对齐的大语言模型（LLMs）免受牢笼攻击：单向检测方法（SPD）', 'title_zh': '大型语言模型中一次性检测越狱输入的方法'}
{'arxiv_id': 'arXiv:2502.15419', 'title': 'Beyond Translation: LLM-Based Data Generation for Multilingual Fact-Checking', 'authors': 'Yi-Ling Chung, Aurora Cobo, Pablo Serna', 'link': 'https://arxiv.org/abs/2502.15419', 'abstract': 'Robust automatic fact-checking systems have the potential to combat online misinformation at scale. However, most existing research primarily focuses on English. In this paper, we introduce MultiSynFact, the first large-scale multilingual fact-checking dataset containing 2.2M claim-source pairs designed to support Spanish, German, English, and other low-resource languages. Our dataset generation pipeline leverages Large Language Models (LLMs), integrating external knowledge from Wikipedia and incorporating rigorous claim validation steps to ensure data quality. We evaluate the effectiveness of MultiSynFact across multiple models and experimental settings. Additionally, we open-source a user-friendly framework to facilitate further research in multilingual fact-checking and dataset generation.', 'abstract_zh': '多语言事实核查数据集MultiSynFact在大规模打击网络虚假信息方面具有潜力。然而，现有的大部分研究主要集中在英语上。本文介绍了MultiSynFact，这是首个包含220万条断言-来源对的大规模多语言事实核查数据集，旨在支持西班牙语、德语、英语及其他低资源语言。我们的数据集生成管道利用了大型语言模型（LLMs），从Wikipedia中集成外部知识，并结合严格的断言验证步骤以确保数据质量。我们跨多个模型和实验设置评估了MultiSynFact的有效性。此外，我们开源了一个用户友好的框架，以促进多语言事实核查和数据集生成的进一步研究。', 'title_zh': '超越翻译：基于LLM的数据生成在多语言事实核实中的应用'}
{'arxiv_id': 'arXiv:2502.15365', 'title': 'Identifying Features that Shape Perceived Consciousness in Large Language Model-based AI: A Quantitative Study of Human Responses', 'authors': 'Kang Bongsu, Kim Jundong, Yun Tae-Rim, Bae Hyojin, Kim Chang-Eop', 'link': 'https://arxiv.org/abs/2502.15365', 'abstract': "This study quantitively examines which features of AI-generated text lead humans to perceive subjective consciousness in large language model (LLM)-based AI systems. Drawing on 99 passages from conversations with Claude 3 Opus and focusing on eight features -- metacognitive self-reflection, logical reasoning, empathy, emotionality, knowledge, fluency, unexpectedness, and subjective expressiveness -- we conducted a survey with 123 participants. Using regression and clustering analyses, we investigated how these features influence participants' perceptions of AI consciousness. The results reveal that metacognitive self-reflection and the AI's expression of its own emotions significantly increased perceived consciousness, while a heavy emphasis on knowledge reduced it. Participants clustered into seven subgroups, each showing distinct feature-weighting patterns. Additionally, higher prior knowledge of LLMs and more frequent usage of LLM-based chatbots were associated with greater overall likelihood assessments of AI consciousness. This study underscores the multidimensional and individualized nature of perceived AI consciousness and provides a foundation for better understanding the psychosocial implications of human-AI interaction.", 'abstract_zh': '本研究定量分析了哪些特征使人类在基于大规模语言模型（LLM）的AI系统中感知到主观意识。通过聚焦于元认知自我反省、逻辑推理、同理心、情感性、知识、流畅性、意外性以及主观表达性等八种特征，基于与Claude 3 Opus对话的99段文本，对123名参与者进行了调查。通过回归和聚类分析，我们探讨了这些特征如何影响参与者对AI意识的感知。研究结果表明，元认知自我反省和AI表达自身情感显著增加了感知到的意识，而过度强调知识则减少了这一感知。参与者被归类为七个子群，每个子群都有自己独特的特征权重模式。此外，先前对LLM的知识更多以及更频繁使用基于LLM的聊天机器人与更高的总体意识可能性评估相关。本研究强调了感知到的AI意识的多维度和个性化性质，并为更好地理解人机交互的心理社会影响提供了基础。', 'title_zh': '基于大型语言模型的AI中影响感知意识特征的识别：人类响应的定量研究'}
{'arxiv_id': 'arXiv:2502.15361', 'title': 'Evaluating Social Biases in LLM Reasoning', 'authors': 'Xuyang Wu, Jinming Nian, Zhiqiang Tao, Yi Fang', 'link': 'https://arxiv.org/abs/2502.15361', 'abstract': 'In the recent development of AI reasoning, large language models (LLMs) are trained to automatically generate chain-of-thought reasoning steps, which have demonstrated compelling performance on math and coding tasks. However, when bias is mixed within the reasoning process to form strong logical arguments, it could cause even more harmful results and further induce hallucinations. In this paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against their instruction tuned counterparts on the BBQ dataset, and investigated the bias that is elicited out and being amplified through reasoning steps. To the best of our knowledge, this empirical study is the first to assess bias issues in LLM reasoning.', 'abstract_zh': '近年来，在AI推理的发展中，大型语言模型（LLMs）被训练自动生成链式推理步骤，已在数学和编程任务上展示了令人信服的性能。然而，当偏见渗入推理过程并形成强有力的逻辑论证时，它可能会导致更加有害的结果，进一步引发幻觉。在本文中，我们评估了DeepSeek-R1的8B和32B变体与其指令调优版本在BBQ数据集上的表现，并调查了通过推理步骤引发和放大的偏见。据我们所知，这项实证研究是首次评估LLM推理中的偏见问题。', 'title_zh': '评估LLM推理中的社会偏见'}
{'arxiv_id': 'arXiv:2502.15348', 'title': "Constructing a Norm for Children's Scientific Drawing: Distribution Features Based on Semantic Similarity of Large Language Models", 'authors': 'Yi Zhang, Fan Wei, Jingyi Li, Yan Wang, Yanyan Yu, Jianli Chen, Zipo Cai, Xinyu Liu, Wei Wang, Peng Wang, Zhong Wang', 'link': 'https://arxiv.org/abs/2502.15348', 'abstract': "The use of children's drawings to examining their conceptual understanding has been proven to be an effective method, but there are two major problems with previous research: 1. The content of the drawings heavily relies on the task, and the ecological validity of the conclusions is low; 2. The interpretation of drawings relies too much on the subjective feelings of the researchers. To address this issue, this study uses the Large Language Model (LLM) to identify 1420 children's scientific drawings (covering 9 scientific themes/concepts), and uses the word2vec algorithm to calculate their semantic similarity. The study explores whether there are consistent drawing representations for children on the same theme, and attempts to establish a norm for children's scientific drawings, providing a baseline reference for follow-up children's drawing research. The results show that the representation of most drawings has consistency, manifested as most semantic similarity greater than 0.8. At the same time, it was found that the consistency of the representation is independent of the accuracy (of LLM's recognition), indicating the existence of consistency bias. In the subsequent exploration of influencing factors, we used Kendall rank correlation coefficient to investigate the effects of Sample Size, Abstract Degree, and Focus Points on drawings, and used word frequency statistics to explore whether children represented abstract themes/concepts by reproducing what was taught in class.", 'abstract_zh': '使用大型语言模型识别儿童科学绘画及其语义相似性分析：探索同一主题下儿童绘画的一致性并建立儿童科学绘画规范', 'title_zh': '基于大语言模型语义相似性的儿童科学绘画规范构建：分布特征'}
{'arxiv_id': 'arXiv:2502.15334', 'title': 'Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment', 'authors': 'Pedram Zaree, Md Abdullah Al Mamun, Quazi Mishkatul Alam, Yue Dong, Ihsen Alouani, Nael Abu-Ghazaleh', 'link': 'https://arxiv.org/abs/2502.15334', 'abstract': 'Recent research has shown that carefully crafted jailbreak inputs can induce large language models to produce harmful outputs, despite safety measures such as alignment. It is important to anticipate the range of potential Jailbreak attacks to guide effective defenses and accurate assessment of model safety. In this paper, we present a new approach for generating highly effective Jailbreak attacks that manipulate the attention of the model to selectively strengthen or weaken attention among different parts of the prompt. By harnessing attention loss, we develop more effective jailbreak attacks, that are also transferrable. The attacks amplify the success rate of existing Jailbreak algorithms including GCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example, the amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack on Llama2-7B/AdvBench, using less than a third of the generation time).', 'abstract_zh': '最近的研究表明，精心构造的越狱输入可以使对齐等安全措施的大语言模型生成有害输出。为了指导有效的防御和准确评估模型安全，有必要预测潜在的越狱攻击范围。本文提出了一种新的方法来生成高效的越狱攻击，该方法通过操控模型的注意力来选择性地增强或减弱提示不同部分的关注。通过利用注意力损失，我们开发了更有效的可转移的越狱攻击，这些攻击可以提高现有越狱算法（包括GCG、AutoDAN和ReNeLLM）的成功率，同时降低其生成成本（例如，放大后的GCG攻击在Llama2-7B/AdvBench上的成功率为91.2%，而原攻击的成功率为67.9%，且使用的时间不到原攻击的三分之一）。', 'title_zh': '注意力遮蔽：操纵注意力以绕过LLM安全对齐'}
{'arxiv_id': 'arXiv:2502.15304', 'title': 'SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention', 'authors': 'Hong Yankun, Li Xing, Zhen Hui-Ling, Yu Xianzhi, Liu Wulong, Yuan Mingxuan', 'link': 'https://arxiv.org/abs/2502.15304', 'abstract': 'For the efficient inference of Large Language Models (LLMs), the effective compression of key-value (KV) cache is essential. Three main types of KV cache compression techniques, namely sparsity, channel compression, and quantization, have been identified. This study presents SVDq, a Singular Value Decomposition (SVD) - based mixed precision quantization method for K cache. Initially, K cache is transformed into latent channels using SVD basis representations. Since the values in latent channels decay rapidly and become negligible after only a few latent channels, our method then incorporates importance-aware quantization and compression for latent channels. This enables the effective allocation of higher precision to more significant channels. Theoretically, we prove that SVDq results in quantization errors (x0.1 or even lower) that are much lower than those of per-channel key quantization in the original space. Our findings based on RULER and LongBench benchmarks demonstrate that SVDq can achieve an equivalent key cache precision as low as 1.25-bit. When combined with key sparsity, it can reach a key compression ratio of up to 410x for attention computation, all while maintaining comparable model performance. Notably, our method is nearly lossless for LongBench datasets. This indicates that SVDq enables high-precision low-bit quantization, providing a more efficient solution for KV cache compression in LLMs.', 'abstract_zh': '基于SVD的混合精度量化方法SVDq在大型语言模型中有效压缩键值缓存', 'title_zh': 'SVDq：1.25比特和410倍键缓存压缩的大型语言模型注意力机制'}
{'arxiv_id': 'arXiv:2502.15294', 'title': 'Round Attention: A Novel Round-Level Attention Mechanism to Accelerate LLM Inference', 'authors': 'Yaohua Tang, Zhicheng Hu, Kun Cheng, Fan Mo, Qiheng Lv, Hua Wang, Zhi Chen', 'link': 'https://arxiv.org/abs/2502.15294', 'abstract': 'The increasing context window size in large language models (LLMs) has improved their ability to handle complex, long-text tasks. However, as the conversation rounds continue, it is required to store a large amount of KV cache in GPU memory, which significantly affects the efficiency and even availability of the model serving systems. This paper analyzes dialogue data from real users and discovers that the LLM inference manifests a watershed layer, after which the distribution of round-level attention shows notable similarity. We propose Round Attention, a novel round-level attention mechanism that only recalls and computes the KV cache of the most relevant rounds. The experiments show that our method saves 55\\% memory usage without compromising model performance.', 'abstract_zh': '大型语言模型中上下文窗口大小的增加提高了其处理复杂长文本任务的能力，但随着对话轮次的增加，需要在GPU内存中存储大量KV缓存，这显著影响了模型服务系统的效率甚至可用性。本文分析了真实用户的数据，发现LLM推理存在一个分水岭层，在此之后，轮次级别的注意力分布显示出明显的相似性。我们提出了一种新型的轮次级别注意力机制Round Attention，仅召回并计算最相关的轮次的KV缓存。实验结果显示，该方法在不牺牲模型性能的情况下节省了55%的内存使用。', 'title_zh': '圆级attention：一种加速大规模语言模型推理的新型圆级注意力机制'}
{'arxiv_id': 'arXiv:2502.15243', 'title': 'Comparative Analysis of Large Language Models for Context-Aware Code Completion using SAFIM Framework', 'authors': 'Hang Zhang, Yanxin Shen, Lun Wang, Chuanqi Shi, Shaoshuai Du, Yiyi Tao, Yixian Shen', 'link': 'https://arxiv.org/abs/2502.15243', 'abstract': "The advent of Large Language Models (LLMs) has revolutionized code completion, transforming it into a more intelligent and context-aware feature in modern integrated development environments. These advancements have significantly enhanced developers' ability to write efficient and error-free code. This study evaluates the performance of several chat-based LLMs, including Gemini 1.5 Flash, Gemini 1.5 Pro, GPT-4o, GPT-4o-mini, and GPT-4 Turbo, using the Syntax-Aware Fill-in-the-Middle (SAFIM) dataset. This benchmark is specifically designed to assess models' capabilities in syntax-sensitive code generation. Performance metrics, such as cosine similarity with ground-truth completions and latency, were employed to measure both accuracy and efficiency. The findings reveal substantial differences in the models' code completion abilities, offering valuable insights into their respective strengths and weaknesses. This work provides a comparative analysis that underscores the trade-offs between accuracy and speed, establishing a benchmark for future advancements in LLM-based code completion.", 'abstract_zh': '大型语言模型的出现已revolutionized代码完成，将其转变为现代集成开发环境中的更加智能和上下文感知的功能。这些进步显著增强了开发者编写高效无误代码的能力。本研究使用Syntax-Aware Fill-in-the-Middle (SAFIM) 数据集评估了几种基于聊天的大型语言模型，包括Gemini 1.5 Flash、Gemini 1.5 Pro、GPT-4o、GPT-4o-mini和GPT-4 Turbo的表现。该基准特别设计用于评估模型在语法敏感代码生成方面的能力。通过使用与真实完成结果的余弦相似度和延迟等性能指标来衡量准确性和效率。研究发现揭示了模型之间在代码完成能力上的显著差异，提供了它们各自优势和不足的宝贵见解。本研究提供了比较分析，强调了准确性和速度之间的权衡，并建立了基于大型语言模型的代码完成未来进步的基准。', 'title_zh': '基于SAFIM框架的大规模语言模型在上下文感知代码补全方面的比较分析'}
{'arxiv_id': 'arXiv:2502.15226', 'title': 'Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews', 'authors': 'Mengqiao Liu, Tevin Wang, Cassandra A. Cohen, Sarah Li, Chenyan Xiong', 'link': 'https://arxiv.org/abs/2502.15226', 'abstract': 'Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interacted with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, for example, the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our collected chat-and-interview logs will be released.', 'abstract_zh': '哪种大型语言模型（LLM）更好？每个评估都有它的故事，但用户真实如何看待当前的LLM呢？本文介绍了一种基于LLM的访谈者CLUE，它在用户与LLM交互后即时进行用户体验访谈，并自动从大量的访谈日志中收集用户意见的见解。我们进行了一项研究，有数千名用户参与，以了解他们对主流LLM的看法，招募用户首先与目标LLM聊天，然后接受CLUE的访谈。实验证明，CLUE捕捉到了有趣用户意见的例子，如对DeepSeek-R1显示推理过程的两极看法以及对信息新鲜度和多模态的诉求。我们收集的聊天和访谈日志将公开发布。', 'title_zh': '通过大语言模型驱动的即时用户体验访谈理解用户对大型语言模型的意见'}
{'arxiv_id': 'arXiv:2502.15224', 'title': 'Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs', 'authors': 'Tingting Chen, Srinivas Anumasa, Beibei Lin, Vedant Shah, Anirudh Goyal, Dianbo Liu', 'link': 'https://arxiv.org/abs/2502.15224', 'abstract': 'Given the remarkable performance of Large Language Models (LLMs), an important question arises: Can LLMs conduct human-like scientific research and discover new knowledge, and act as an AI scientist? Scientific discovery is an iterative process that demands efficient knowledge updating and encoding. It involves understanding the environment, identifying new hypotheses, and reasoning about actions; however, no standardized benchmark specifically designed for scientific discovery exists for LLM agents. In response to these limitations, we introduce a novel benchmark, \\textit{Auto-Bench}, that encompasses necessary aspects to evaluate LLMs for scientific discovery in both natural and social sciences. Our benchmark is based on the principles of causal graph discovery. It challenges models to uncover hidden structures and make optimal decisions, which includes generating valid justifications. By engaging interactively with an oracle, the models iteratively refine their understanding of underlying interactions, the chemistry and social interactions, through strategic interventions. We evaluate state-of-the-art LLMs, including GPT-4, Gemini, Qwen, Claude, and Llama, and observe a significant performance drop as the problem complexity increases, which suggests an important gap between machine and human intelligence that future development of LLMs need to take into consideration.', 'abstract_zh': '给定大型语言模型（LLMs）的出色表现，一个重要的问题出现了：LLMs能否像人类一样进行科学研究并发现新知识，扮演AI科学家的角色？科学发现是一个迭代过程，需要高效的知识更新和编码。它涉及到理解环境、提出新假设和推理行动；然而，针对科学发现没有专门设计的标准基准供LLM代理使用。为应对这些局限性，我们引入了一个新型基准Auto-Bench，该基准涵盖了评估LLMs在自然和社会科学中进行科学发现所需的关键方面。我们的基准基于因果图发现的原则。它挑战模型揭示隐藏结构并作出最优决策，包括生成有效的论证。通过与 oracle 交互，模型通过战略性干预逐步深化对其潜在交互、化学和社会互动的理解。我们评估了最先进的 LLMs，包括 GPT-4、Gemini、Qwen、Claude 和 Llama，并观察到随着问题复杂性的增加，性能出现显著下降，这表明机器智能与人类智能之间存在重要差距，未来 LLMs 的发展需要考虑这一差距。', 'title_zh': 'Auto-Bench: 一个用于LLM科学研究的自动化基准测试'}
{'arxiv_id': 'arXiv:2502.15217', 'title': 'FormalSpecCpp: A Dataset of C++ Formal Specifications created using LLMs', 'authors': 'Madhurima Chakraborty, Peter Pirkelbauer, Qing Yi', 'link': 'https://arxiv.org/abs/2502.15217', 'abstract': 'FormalSpecCpp is a dataset designed to fill the gap in standardized benchmarks for verifying formal specifications in C++ programs. To the best of our knowledge, this is the first comprehensive collection of C++ programs with well-defined preconditions and postconditions. It provides a structured benchmark for evaluating specification inference tools and testing theaccuracy of generated specifications. Researchers and developers can use this dataset to benchmark specification inference tools,fine-tune Large Language Models (LLMs) for automated specification generation, and analyze the role of formal specifications in improving program verification and automated testing. By making this dataset publicly available, we aim to advance research in program verification, specification inference, and AI-assisted software development. The dataset and the code are available at this https URL.', 'abstract_zh': 'FormalSpecCpp是用于验证C++程序正式规范的标准基准缺口的数据集，到我们所知，这是第一个包含明确预条件和后条件的C++程序的全面集合。它提供了一个结构化的基准，用于评估规范推断工具并测试生成规范的准确性。研究人员和开发人员可以使用此数据集来基准测试规范推断工具、微调大型语言模型（LLMs）以实现自动化规范生成，并分析正式规范在提高程序验证和自动化测试中的作用。通过公开发布此数据集，我们旨在促进程序验证、规范推断和AI辅助软件开发的研究。数据集和代码可在以下网址获取：this https URL。', 'title_zh': 'FormalSpecCpp: 一个由LLM创建的C++形式化规范数据集'}
{'arxiv_id': 'arXiv:2502.15214', 'title': 'The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning', 'authors': 'Sheila Schoepp, Masoud Jafaripour, Yingyue Cao, Tianpei Yang, Fatemeh Abdollahi, Shadan Golestan, Zahin Sufiyan, Osmar R. Zaiane, Matthew E. Taylor', 'link': 'https://arxiv.org/abs/2502.15214', 'abstract': 'Reinforcement learning (RL) has shown impressive results in sequential decision-making tasks. Meanwhile, Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged, exhibiting impressive capabilities in multimodal understanding and reasoning. These advances have led to a surge of research integrating LLMs and VLMs into RL. In this survey, we review representative works in which LLMs and VLMs are used to overcome key challenges in RL, such as lack of prior knowledge, long-horizon planning, and reward design. We present a taxonomy that categorizes these LLM/VLM-assisted RL approaches into three roles: agent, planner, and reward. We conclude by exploring open problems, including grounding, bias mitigation, improved representations, and action advice. By consolidating existing research and identifying future directions, this survey establishes a framework for integrating LLMs and VLMs into RL, advancing approaches that unify natural language and visual understanding with sequential decision-making.', 'abstract_zh': '强化学习（RL）在序列决策任务中展现了令人印象深刻的成果。与此同时，大型语言模型（LLMs）和视觉-语言模型（VLMs）应运而生，并在多模态理解和推理方面表现出色。这些进展推动了将LLMs和VLMs集成到RL中的研究热潮。在这篇综述中，我们回顾了使用LLMs和VLMs解决RL中关键挑战的研究，如缺乏先验知识、长期规划和奖励设计。我们提出了一个分类框架，将这些LLM/VLM辅助的RL方法分为代理、规划者和奖励三个角色。最后，我们探讨了开放式问题，包括态势感知、偏见缓解、改进表示和动作建议。通过整合现有研究并确定未来方向，这篇综述建立了一个框架，以将LLMs和VLMs集成到RL中，推动将自然语言和视觉理解与序列决策相结合的方法的发展。', 'title_zh': 'LLM-和VLM-集成强化学习的发展格局'}
{'arxiv_id': 'arXiv:2502.15210', 'title': 'PairBench: A Systematic Framework for Selecting Reliable Judge VLMs', 'authors': 'Aarash Feizi, Sai Rajeswar, Adriana Romero-Soriano, Reihaneh Rabbany, Spandana Gella, Valentina Zantedeschi, João Monteiro', 'link': 'https://arxiv.org/abs/2502.15210', 'abstract': "As large vision language models (VLMs) are increasingly used as automated evaluators, understanding their ability to effectively compare data pairs as instructed in the prompt becomes essential. To address this, we present PairBench, a low-cost framework that systematically evaluates VLMs as customizable similarity tools across various modalities and scenarios. Through PairBench, we introduce four metrics that represent key desiderata of similarity scores: alignment with human annotations, consistency for data pairs irrespective of their order, smoothness of similarity distributions, and controllability through prompting. Our analysis demonstrates that no model, whether closed- or open-source, is superior on all metrics; the optimal choice depends on an auto evaluator's desired behavior (e.g., a smooth vs. a sharp judge), highlighting risks of widespread adoption of VLMs as evaluators without thorough assessment. For instance, the majority of VLMs struggle with maintaining symmetric similarity scores regardless of order. Additionally, our results show that the performance of VLMs on the metrics in PairBench closely correlates with popular benchmarks, showcasing its predictive power in ranking models.", 'abstract_zh': '作为一种自动化评估工具，大型视觉语言模型(VLMs)越来越多地被用于数据对比，因此理解它们根据提示有效比较数据对的能力变得至关重要。为了解决这一问题，我们提出了PairBench，这是一种低成本框架，系统地评估VLMs作为可定制相似性工具在各种模态和场景下的性能。通过PairBench，我们引入了四个代表相似性评分关键要求的指标：与人类注释的一致性、数据对的顺序无关的一致性、相似性分布的平滑度以及通过提示实现的可控性。我们的分析表明，无论是闭源还是开源模型，在这些指标上都没有绝对的优势；最优选择取决于自动化评估器期望的行为（例如，平滑的评估者与尖锐的评估者），这突显了在广泛采用VLMs作为评估工具时进行全面评估的风险。例如，大多数VLMs在保持顺序无关的相似性得分方面存在困难。此外，我们的结果表明，PairBench中模型在各指标上的性能与流行的基准测试高度相关，展示了其在模型排序方面的预测能力。', 'title_zh': 'PairBench: 一种选择可靠法官大语言模型的系统框架'}
{'arxiv_id': 'arXiv:2502.15197', 'title': 'TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding', 'authors': 'Zhaoxuan Wu, Zijian Zhou, Arun Verma, Alok Prakash, Daniela Rus, Bryan Kian Hsiang Low', 'link': 'https://arxiv.org/abs/2502.15197', 'abstract': 'We propose TETRIS, a novel method that optimizes the total throughput of batch speculative decoding in multi-request settings. Unlike existing methods that optimize for a single request or a group of requests as a whole, TETRIS actively selects the most promising draft tokens (for every request in a batch) to be accepted when verified in parallel, resulting in fewer rejected tokens and hence less wasted computing resources. Such an effective resource utilization to achieve fast inference in large language models (LLMs) is especially important to service providers with limited inference capacity. Compared to baseline speculative decoding, TETRIS yields a consistently higher acceptance rate and more effective utilization of the limited inference capacity. We show theoretically and empirically that TETRIS outperforms baseline speculative decoding and existing methods that dynamically select draft tokens, leading to a more efficient batch inference in LLMs.', 'abstract_zh': 'TETRIS：一种在多请求环境中优化批推测解码总吞吐量的新方法', 'title_zh': 'TETRIS: 批量推测解码的最优草稿令牌选择'}
{'arxiv_id': 'arXiv:2502.15182', 'title': 'LEDD: Large Language Model-Empowered Data Discovery in Data Lakes', 'authors': 'Qi An, Chihua Ying, Yuqing Zhu, Yihao Xu, Manwei Zhang, Jianmin Wang', 'link': 'https://arxiv.org/abs/2502.15182', 'abstract': 'Data discovery in data lakes with ever increasing datasets has long been recognized as a big challenge in the realm of data management, especially for semantic search of and hierarchical global catalog generation of tables. While large language models (LLMs) facilitate the processing of data semantics, challenges remain in architecting an end-to-end system that comprehensively exploits LLMs for the two semantics-related tasks. In this demo, we propose LEDD, an end-to-end system with an extensible architecture that leverages LLMs to provide hierarchical global catalogs with semantic meanings and semantic table search for data lakes. Specifically, LEDD can return semantically related tables based on natural-language specification. These features make LEDD an ideal foundation for downstream tasks such as model training and schema linking for text-to-SQL tasks. LEDD also provides a simple Python interface to facilitate the extension and the replacement of data discovery algorithms.', 'abstract_zh': '数据湖中随不断增加的数据集进行数据发现长期以来被认为是数据管理领域的一个重大挑战，特别是在语义搜索和层次全球目录生成方面。尽管大型语言模型（LLMs）有助于处理数据语义，但在构建一个全面利用LLMs的端到端系统以解决两个语义相关任务方面仍面临挑战。在此次演示中，我们提出LEDD，这是一个具有可扩展架构的端到端系统，利用LLMs提供具有语义含义的层次全球目录和数据湖中的语义表搜索功能。具体来说，LEDD可以根据自然语言规范返回语义相关的表。这些功能使LEDD成为诸如文本到SQL任务的模型训练和模式链接之类的下游任务的理想基础。LEDD还提供了一个简单的Python接口，以方便扩展和替换数据发现算法。', 'title_zh': 'LEDD：大型语言模型赋能的数据湖中数据发现'}
{'arxiv_id': 'arXiv:2502.15155', 'title': 'Extreme Speech Classification in the Era of LLMs: Exploring Open-Source and Proprietary Models', 'authors': 'Sarthak Mahajan, Nimmi Rangaswamy', 'link': 'https://arxiv.org/abs/2502.15155', 'abstract': 'In recent years, widespread internet adoption and the growth in userbase of various social media platforms have led to an increase in the proliferation of extreme speech online. While traditional language models have demonstrated proficiency in distinguishing between neutral text and non-neutral text (i.e. extreme speech), categorizing the diverse types of extreme speech presents significant challenges. The task of extreme speech classification is particularly nuanced, as it requires a deep understanding of socio-cultural contexts to accurately interpret the intent of the language used by the speaker. Even human annotators often disagree on the appropriate classification of such content, emphasizing the complex and subjective nature of this task. The use of human moderators also presents a scaling issue, necessitating the need for automated systems for extreme speech classification. The recent launch of ChatGPT has drawn global attention to the potential applications of Large Language Models (LLMs) across a diverse variety of tasks. Trained on vast and diverse corpora, and demonstrating the ability to effectively capture and encode contextual information, LLMs emerge as highly promising tools for tackling this specific task of extreme speech classification. In this paper, we leverage the Indian subset of the extreme speech dataset from Maronikolakis et al. (2022) to develop an effective classification framework using LLMs. We evaluate open-source Llama models against closed-source OpenAI models, finding that while pre-trained LLMs show moderate efficacy, fine-tuning with domain-specific data significantly enhances performance, highlighting their adaptability to linguistic and contextual nuances. Although GPT-based models outperform Llama models in zero-shot settings, the performance gap disappears after fine-tuning.', 'abstract_zh': '近年来，广泛普及的互联网和各种社交媒体平台用户基数的快速增长导致了在线极端言论的增多。虽然传统语言模型在区分中性和非中性文本（即极端言论）方面表现出色，但对极端言论多样类型的分类仍具挑战性。极端言论分类的任务尤为复杂，因为它要求深刻理解社会文化背景，以准确解读演讲者的语言意图。即使是人类注释员也经常对这类内容的适当分类意见不一，强调了此任务的复杂性和主观性。使用人类审查员也存在扩展问题，因此亟需开发自动化的极端言论分类系统。ChatGPT的近期推出引起了人们对大型语言模型（LLMs）在各种任务中潜在应用的关注。经过大量多样语料库的训练，具备有效捕捉和编码上下文信息能力的LLMs成为应对特定极端言论分类任务的非常有前景的工具。本文利用Maronikolakis等人（2022）的极端言论数据集中的印度子集，结合LLMs开发了有效的分类框架。我们将开源的Llama模型与闭源的OpenAI模型进行对比评估，发现虽预训练的LLMs表现出一定的效果，但通过领域特定数据微调可以显著提高性能，突显了它们对语言和上下文细微差别的适应性。尽管基于GPT的模型在零样本设置中优于Llama模型，但在微调后性能差距消失。', 'title_zh': '在大语言模型时代的情感言词分类：开源与 proprietary 模型探索'}
{'arxiv_id': 'arXiv:2502.15145', 'title': 'Projection Optimization: A General Framework for Multi-Objective and Multi-Group RLHF', 'authors': 'Nuoya Xiong, Aarti Singh', 'link': 'https://arxiv.org/abs/2502.15145', 'abstract': 'Reinforcement Learning with Human Feedback (RLHF) is a widely used fine-tuning approach that aligns machine learning model, particularly Language Model (LM) with human preferences. There are typically multiple objectives driving the preference, hence humans find it easier to express per-objective comparisons rather than a global preference between two choices. %, e.g. compare two papers on their novelty, clarity, correctness, etc. Multi-Objective RLHF (MORLHF) aims to use per-objective preference feedback and achieve Pareto optimality among these objectives by aggregating them into a single unified objective for optimization. However, nearly all prior works rely on linear aggregation, which rules out policies that favor specific objectives such as the worst one. The only existing approach using non-linear aggregation is computationally expensive due to its reward-based nature and the need for retraining whenever the aggregation parameters change. In this work, we address this limitation by transforming the non-linear aggregation maximization problem into a series of sub-problems. Each sub-problem involves only linear aggregation, making it computationally efficient to solve. We further extend our framework to handle multi-group scenarios, where each group has distinct weights for the objectives. Our method enables achieving consensus or maximizing the aggregated objective across all groups. Theoretically, we demonstrate that our algorithmic framework achieves sublinear regret and can be easily adapted to a reward-free algorithm. Empirically, leveraging our theoretical insights, we propose a nearly training-free algorithm once the optimal policies for individual objectives are obtained.', 'abstract_zh': '基于人类反馈的强化学习与多目标优化（Multi-Objective Reinforcement Learning with Human Feedback, MORLHF）', 'title_zh': '投影优化：多目标与多组RLHF的通用框架'}
{'arxiv_id': 'arXiv:2502.15134', 'title': 'Chain-of-Rank: Enhancing Large Language Models for Domain-Specific RAG in Edge Device', 'authors': 'Juntae Lee, Jihwan Bang, Seunghan Yang, Kyuhong Shim, Simyung Chang', 'link': 'https://arxiv.org/abs/2502.15134', 'abstract': 'Retrieval-augmented generation (RAG) with large language models (LLMs) is especially valuable in specialized domains, where precision is critical. To more specialize the LLMs into a target domain, domain-specific RAG has recently been developed by allowing the LLM to access the target domain early via finetuning. The domain-specific RAG makes more sense in resource-constrained environments like edge devices, as they should perform a specific task (e.g. personalization) reliably using only small-scale LLMs. While the domain-specific RAG is well-aligned with edge devices in this respect, it often relies on widely-used reasoning techniques like chain-of-thought (CoT). The reasoning step is useful to understand the given external knowledge, and yet it is computationally expensive and difficult for small-scale LLMs to learn it. Tackling this, we propose the Chain of Rank (CoR) which shifts the focus from intricate lengthy reasoning to simple ranking of the reliability of input external documents. Then, CoR reduces computational complexity while maintaining high accuracy, making it particularly suited for resource-constrained environments. We attain the state-of-the-art (SOTA) results in benchmarks, and analyze its efficacy.', 'abstract_zh': '基于大型语言模型的领域特定检索增强生成（RAG）在专业化领域尤其 valuable，精度至关重要。为了使大型语言模型更专门化于目标领域，最近通过微调允许其早期访问目标领域，开发了领域特定的RAG。在资源受限的环境如边缘设备中，领域特定的RAG更有意义，因为它们应该仅使用小型规模的大型语言模型可靠地完成特定任务（如个性化）。尽管在这一点上领域特定的RAG与边缘设备相契合，但它通常依赖于广泛使用的思想链（CoT）等推理技术。推理步骤有助于理解给定的外部知识，但小型规模的语言模型很难学习它。为解决此问题，我们提出了一种链排名（CoR）方法，将重点从复杂的长推理转移到输入外部文档可靠性的简单排名。然后，CoR降低了计算复杂性同时保持高精度，使其特别适用于资源受限的环境。我们在基准测试中取得了最先进（SOTA）的结果，并分析了其有效性。', 'title_zh': '链排序：增强边缘设备上针对特定领域的语言模型的检索增强生成（RAG）能力'}
{'arxiv_id': 'arXiv:2502.15120', 'title': 'Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning, and Interpretability through Attention Maps', 'authors': 'Yen-Che Hsiao, Abhishek Dutta', 'link': 'https://arxiv.org/abs/2502.15120', 'abstract': 'This study investigates the in-context learning capabilities of various decoder-only transformer-based language models with different model sizes and training data, including GPT2, SmolLM2, OpenELM, TinyLlama, Stable LM, and Gemma 2. We identify a critical parameter threshold (~1.6 billion), beyond which reasoning performance improves significantly in tasks such as commonsense reasoning in multiple-choice question answering and deductive reasoning. Specifically, models above this threshold achieve better success rates in chain-of-thought (CoT) prompting for deductive reasoning tasks, especially those requiring longer reasoning chains, such as proof by contradiction and disjunction elimination. To address limitations in sub-threshold models, we demonstrate that fine-tuning with task-specific exemplars substantially enhances reasoning performance, enabling accurate CoT generation even without additional exemplars in the prompt for tasks with shorter reasoning chains. Finally, our analysis of attention maps reveals that models capable of generating correct CoTs exhibit higher token-level attention scores on subsequent correct tokens and the correct parts of speech, providing interpretability insights into reasoning processes. These findings collectively advance understanding of reasoning capabilities in decoder-only transformer-based models. The code is available at: this https URL.', 'abstract_zh': '本研究探讨了不同模型大小和训练数据的各类解码器基础Transformer语言模型的在上下文学习能力，包括GPT2、SmolLM2、OpenELM、TinyLlama、Stable LM和Gemma 2。我们发现一个关键参数阈值（约16亿），在此之上，推理性能在常识推理和演绎推理等任务中显著提升。具体而言，超过此阈值的模型在演绎推理任务中，尤其是在需要更长推理链的任务（如反证法和析取消去）中的链式思考（CoT）提示时，能实现更高的成功率。为了解决阈值下模型的限制，我们证明了使用特定任务示例进行微调可以显著提高推理性能，即使在较短推理链的任务提示中没有额外示例，也能实现准确的CoT生成。最后，我们对注意力图的分析表明，能够生成正确CoTs的模型在后续正确词语和正确词性上有更高的token级注意力分数，提供了推理过程的可解释性见解。这些发现共同推进了对解码器基础Transformer模型推理能力的理解。代码可供查阅：this https URL。', 'title_zh': '探索语言模型中的推理门槛：通过注意力图揭示扩展、微调和可解释性'}
{'arxiv_id': 'arXiv:2502.15090', 'title': 'Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans', 'authors': 'Masha Fedzechkina, Eleonora Gualdoni, Sinead Williamson, Katherine Metcalf, Skyler Seto, Barry-John Theobald', 'link': 'https://arxiv.org/abs/2502.15090', 'abstract': "Modern large language models (LLMs) achieve impressive performance on some tasks, while exhibiting distinctly non-human-like behaviors on others. This raises the question of how well the LLM's learned representations align with human representations. In this work, we introduce a novel approach to the study of representation alignment: we adopt a method from research on activation steering to identify neurons responsible for specific concepts (e.g., 'cat') and then analyze the corresponding activation patterns. Our findings reveal that LLM representations closely align with human representations inferred from behavioral data. Notably, this alignment surpasses that of word embeddings, which have been center stage in prior work on human and model alignment. Additionally, our approach enables a more granular view of how LLMs represent concepts. Specifically, we show that LLMs organize concepts in a way that reflects hierarchical relationships interpretable to humans (e.g., 'animal'-'dog').", 'abstract_zh': '现代大规模语言模型在一些任务上取得了 impressive 的性能，但在其他任务上表现出明显的非人类行为。这引发了 LLM 学习表示与人类表示之间对齐程度的问题。在本文中，我们引入了一种表示对齐研究的新方法：我们采用激活控制研究中的方法来识别负责特定概念（例如，“猫”）的神经元，然后分析相应的激活模式。我们的发现表明，LLM 的表示与从行为数据推断出的人类表示高度对齐。值得注意的是，这种对齐程度超过了之前工作中中心地位的词嵌入所达到的水平。此外，我们的方法还能够更细致地展现 LLM 如何表示概念。具体而言，我们显示 LLM 以反映可由人类理解的层次关系来组织概念（例如，“动物”-“狗”）。', 'title_zh': '分析神经元，而非嵌入：理解大规模语言模型表示与人类认知的对齐时机和位置'}
{'arxiv_id': 'arXiv:2502.15082', 'title': 'UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning', 'authors': 'Vaidehi Patil, Elias Stengel-Eskin, Mohit Bansal', 'link': 'https://arxiv.org/abs/2502.15082', 'abstract': 'User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or "forgetting" a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model\'s other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the model\'s representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate UPCORE across three standard unlearning methods consistently achieving a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics. We find that UPCORE improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.', 'abstract_zh': '基于保留用途的核心集选择（UPCORE）：一种在遗忘过程中缓解副损伤的通用数据选择框架', 'title_zh': 'UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning'}
{'arxiv_id': 'arXiv:2502.15069', 'title': "Rare Disease Differential Diagnosis with Large Language Models at Scale: From Abdominal Actinomycosis to Wilson's Disease", 'authors': 'Elliot Schumacher, Dhruv Naik, Anitha Kannan', 'link': 'https://arxiv.org/abs/2502.15069', 'abstract': "Large language models (LLMs) have demonstrated impressive capabilities in disease diagnosis. However, their effectiveness in identifying rarer diseases, which are inherently more challenging to diagnose, remains an open question. Rare disease performance is critical with the increasing use of LLMs in healthcare settings. This is especially true if a primary care physician needs to make a rarer prognosis from only a patient conversation so that they can take the appropriate next step. To that end, several clinical decision support systems are designed to support providers in rare disease identification. Yet their utility is limited due to their lack of knowledge of common disorders and difficulty of use.\nIn this paper, we propose RareScale to combine the knowledge LLMs with expert systems. We use jointly use an expert system and LLM to simulate rare disease chats. This data is used to train a rare disease candidate predictor model. Candidates from this smaller model are then used as additional inputs to black-box LLM to make the final differential diagnosis. Thus, RareScale allows for a balance between rare and common diagnoses. We present results on over 575 rare diseases, beginning with Abdominal Actinomycosis and ending with Wilson's Disease. Our approach significantly improves the baseline performance of black-box LLMs by over 17% in Top-5 accuracy. We also find that our candidate generation performance is high (e.g. 88.8% on gpt-4o generated chats).", 'abstract_zh': '大规模语言模型在疾病诊断中展示了令人印象深刻的 capabilities，但在识别更罕见的疾病方面，其有效性仍然存在疑问。随着大规模语言模型在医疗保健环境中使用频率的增加，罕见疾病的性能变得尤为重要。特别是在初级保健医生仅凭与患者的对话就需要做出罕见预后判断的情况下，这一点尤为重要。为此，设计了一些临床决策支持系统以支持罕见疾病识别，但其实用性因缺乏对常见疾病的了解及其使用难度而受到限制。\n\n在本文中，我们提出RareScale结合大规模语言模型和专家系统。我们使用专家系统和大规模语言模型共同模拟罕见疾病对话，生成的数据用于训练罕见疾病候选预测模型。该小型模型的候选者随后被用作黑色盒大规模语言模型的额外输入，以做出最终的鉴别诊断。因此，RareScale 兼顾了罕见和常见疾病的诊断。我们在超过575种罕见疾病的数据上进行了实验，从腹膜炎丝状菌病开始，到威尔逊病结束。我们的方法在Top-5准确性上显著提高了黑色盒大规模语言模型的基线性能，提高了超过17%。我们还发现，我们的候选生成性能很高（例如，在gpt-4生成的对话上达到88.8%）。', 'title_zh': '基于大规模语言模型的罕见病鉴别诊断：从腹腔放线菌病到威尔逊病'}
{'arxiv_id': 'arXiv:2502.15040', 'title': 'Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation', 'authors': 'Yun-Wei Chu, Kai Zhang, Christopher Malon, Martin Renqiang Min', 'link': 'https://arxiv.org/abs/2502.15040', 'abstract': 'Multimodal Large Language Models (MLLMs) have shown impressive performance in vision and text tasks. However, hallucination remains a major challenge, especially in fields like healthcare where details are critical. In this work, we show how MLLMs may be enhanced to support Visual RAG (V-RAG), a retrieval-augmented generation framework that incorporates both text and visual data from retrieved images. On the MIMIC-CXR chest X-ray report generation and Multicare medical image caption generation datasets, we show that Visual RAG improves the accuracy of entity probing, which asks whether a medical entities is grounded by an image. We show that the improvements extend both to frequent and rare entities, the latter of which may have less positive training data. Downstream, we apply V-RAG with entity probing to correct hallucinations and generate more clinically accurate X-ray reports, obtaining a higher RadGraph-F1 score.', 'abstract_zh': '多模态大型语言模型（MLLMs）在视觉和文本任务中展现了令人印象深刻的性能。然而，在细节至关重要的领域如医疗健康中，幻觉仍然是一个主要挑战。在本文中，我们展示了如何通过引入视觉RAG（V-RAG）框架来增强MLLMs，该框架结合了检索到的图像中的文字和视觉数据。在MIMIC-CXR胸部X光报告生成和Multicare医学图像标题生成数据集上，我们证明了视觉RAG能够提高实体探查的准确性，以确定医学实体是否由图像支持。我们还展示了这些改进不仅适用于频繁出现的实体，也适用于训练数据较少的罕见实体。在下游应用中，我们使用V-RAG结合实体探查来纠正幻觉并生成更临床准确的X光报告，从而获得更高的RadGraph-F1评分。', 'title_zh': '使用视觉检索增强生成减少医疗多模态大语言模型的幻觉'}
{'arxiv_id': 'arXiv:2502.15027', 'title': 'InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback', 'authors': 'Henry Hengyuan Zhao, Wenqi Pei, Yifei Tao, Haiyang Mei, Mike Zheng Shou', 'link': 'https://arxiv.org/abs/2502.15027', 'abstract': "Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art LMM (like OpenAI-o1) can correct their results through human feedback less than 50%. Our findings point to the need for methods that can enhance the LMMs' capability to interpret and benefit from feedback.", 'abstract_zh': '现有基准未测试大型多模态模型与人类用户的互动智能，这对于开发通用AI助手至关重要。我们设计了InterFeedback，这是一种可以应用于任何大型多模态模型和数据集的互动框架，以自主评估其这种能力。在此基础上，我们引入了使用MMMU-Pro和MathVerse两个代表性数据集评估互动智能的InterFeedback-Bench，并测试了10种不同的开源大型多模态模型。此外，我们还推出了InterFeedback-Human，这是一个新收集的包含120个案例的数据集，用于手动测试领先模型（如OpenAI-o1和Claude-3.5-Sonnet）的互动性能。我们的评估结果表明，即使是最先进的大型多模态模型（如OpenAI-o1），也有可能通过人类反馈修正其结果不足50%。我们的发现指出了需要改进的方法，以增强大型多模态模型解释和利用反馈的能力。', 'title_zh': 'InterFeedback: 通过人类反馈揭示大型多模态模型的交互智能'}
{'arxiv_id': 'arXiv:2502.15010', 'title': 'Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models', 'authors': 'Mark Russinovich, Ahmed Salem', 'link': 'https://arxiv.org/abs/2502.15010', 'abstract': "Recent copyright agreements between AI companies and content creators have highlighted the need for precise control over language models' ability to reproduce copyrighted content. While existing approaches rely on either complete concept removal through unlearning or simple output filtering, we propose Obliviate, a novel post-training technique that selectively prevents verbatim reproduction of specific text while preserving semantic understanding.\nObliviate operates by selecting tokens within memorized sequences and modifying the model's probability distribution to prevent exact reproduction while maintaining contextual understanding. We evaluate Obliviate on multiple large language models (LLaMA-3.1 8B, LLaMA-3.1-instruct 8B, Qwen-2.5-7B, and Yi-1.5 6B) across both synthetic memorization tasks and organic copyright content. Our results demonstrate that Obliviate achieves orders of magnitude reduction, e.g., 100x, in verbatim memorization while maintaining model performance within 1% of baseline on standard benchmarks (HellaSwag, MMLU, TruthfulQA, and Winogrande). This makes Obliviate particularly suitable for practical deployment scenarios where companies need to efficiently address copyright concerns in pretrained models without compromising their general capabilities.", 'abstract_zh': '近期，人工智能公司与内容创作者之间的版权协议强调了对语言模型复制版权内容能力进行精确控制的必要性。现有方法依赖于完全通过遗忘去除概念或简单的输出过滤，我们提出了一种名为Obliviate的新型后训练技术，该技术能够在保持语义理解的同时，选择性地防止特定文本的精确复制。Obliviate通过在记忆序列中选择词汇并修改模型的概率分布，以防止精确复制同时保持上下文理解。我们在多个大型语言模型（包括LLaMA-3.1 8B、LLaMA-3.1-instruct 8B、Qwen-2.5-7B和Yi-1.5 6B）上对Obliviate进行了评价，涵盖合成记忆任务和有机版权内容。实验结果表明，Obliviate在保持模型性能接近基线（检核街谈、MMLU、TruthfulQA和Winogrande）的情况下，精确记忆减少了数个数量级，例如减少100倍。这使得Obliviate特别适合实际部署场景，公司可以在不牺牲其通用能力的情况下有效解决预训练模型中的版权问题。', 'title_zh': '消隐：高效去记忆化以保护大型语言模型中的知识产权'}
{'arxiv_id': 'arXiv:2502.15007', 'title': 'LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers', 'authors': 'Anton Razzhigaev, Matvey Mikhalchuk, Temurbek Rahmatullaev, Elizaveta Goncharova, Polina Druzhinina, Ivan Oseledets, Andrey Kuznetsov', 'link': 'https://arxiv.org/abs/2502.15007', 'abstract': "We introduce methods to quantify how Large Language Models (LLMs) encode and store contextual information, revealing that tokens often seen as minor (e.g., determiners, punctuation) carry surprisingly high context. Notably, removing these tokens -- especially stopwords, articles, and commas -- consistently degrades performance on MMLU and BABILong-4k, even if removing only irrelevant tokens. Our analysis also shows a strong correlation between contextualization and linearity, where linearity measures how closely the transformation from one layer's embeddings to the next can be approximated by a single linear mapping. These findings underscore the hidden importance of filler tokens in maintaining context. For further exploration, we present LLM-Microscope, an open-source toolkit that assesses token-level nonlinearity, evaluates contextual memory, visualizes intermediate layer contributions (via an adapted Logit Lens), and measures the intrinsic dimensionality of representations. This toolkit illuminates how seemingly trivial tokens can be critical for long-range understanding.", 'abstract_zh': '我们介绍了量化大型语言模型（LLMs）编码和存储上下文信息的方法，揭示了常被视为次要的标记（例如，限定词、标点符号）实际上携带了出乎意料高的上下文信息。值得注意的是，即使仅去除无关标记，尤其是停用词、冠词和逗号，也会一致地降低MMLU和BABILong-4k的性能。我们的分析还显示了上下文化与线性度之间的强相关性，其中线性度衡量从一层嵌入到下一层的转换能被单个线性映射近似刻画的程度。这些发现强调了填充标记在保持上下文方面的重要性。为进一步探索，我们提出了LLM-Microscope，这是一个开源工具包，用于评估标记级别的非线性度、评估上下文记忆、通过调整后的Logit Lens可视化中间层贡献，并测量表示的固有维度。该工具包揭示了看似琐碎的标记对于长距离理解可能是至关重要的。', 'title_zh': 'LLM-Microscope: 探索标点符号在Transformer上下文记忆中的隐藏作用'}
{'arxiv_id': 'arXiv:2502.14975', 'title': 'Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries', 'authors': 'David Noever, Grant Rosario', 'link': 'https://arxiv.org/abs/2502.14975', 'abstract': "We present an open-source benchmark and evaluation framework for assessing emotional boundary handling in Large Language Models (LLMs). Using a dataset of 1156 prompts across six languages, we evaluated three leading LLMs (GPT-4o, Claude-3.5 Sonnet, and Mistral-large) on their ability to maintain appropriate emotional boundaries through pattern-matched response analysis. Our framework quantifies responses across seven key patterns: direct refusal, apology, explanation, deflection, acknowledgment, boundary setting, and emotional awareness. Results demonstrate significant variation in boundary-handling approaches, with Claude-3.5 achieving the highest overall score (8.69/10) and producing longer, more nuanced responses (86.51 words on average). We identified a substantial performance gap between English (average score 25.62) and non-English interactions (< 0.22), with English responses showing markedly higher refusal rates (43.20% vs. < 1% for non-English). Pattern analysis revealed model-specific strategies, such as Mistral's preference for deflection (4.2%) and consistently low empathy scores across all models (< 0.06). Limitations include potential oversimplification through pattern matching, lack of contextual understanding in response analysis, and binary classification of complex emotional responses. Future work should explore more nuanced scoring methods, expand language coverage, and investigate cultural variations in emotional boundary expectations. Our benchmark and methodology provide a foundation for systematic evaluation of LLM emotional intelligence and boundary-setting capabilities.", 'abstract_zh': '一种评估大型语言模型情感边界处理能力的开源基准和评估框架', 'title_zh': '超越否决：量化AI过度拒绝和情感依附边界'}
{'arxiv_id': 'arXiv:2502.14924', 'title': 'A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?', 'authors': 'Ibrahim Alabdulmohsin, Andreas Steiner', 'link': 'https://arxiv.org/abs/2502.14924', 'abstract': "Language exhibits a fractal structure in its information-theoretic complexity (i.e. bits per token), with self-similarity across scales and long-range dependence (LRD). In this work, we investigate whether large language models (LLMs) can replicate such fractal characteristics and identify conditions-such as temperature setting and prompting method-under which they may fail. Moreover, we find that the fractal parameters observed in natural language are contained within a narrow range, whereas those of LLMs' output vary widely, suggesting that fractal parameters might prove helpful in detecting a non-trivial portion of LLM-generated texts. Notably, these findings, and many others reported in this work, are robust to the choice of the architecture; e.g. Gemini 1.0 Pro, Mistral-7B and Gemma-2B. We also release a dataset comprising of over 240,000 articles generated by various LLMs (both pretrained and instruction-tuned) with different decoding temperatures and prompting methods, along with their corresponding human-generated texts. We hope that this work highlights the complex interplay between fractal properties, prompting, and statistical mimicry in LLMs, offering insights for generating, evaluating and detecting synthetic texts.", 'abstract_zh': '语言在其信息论复杂性（即每令牌位数）上表现出分形结构，具有跨尺度的自我相似性和长程依赖性。在本研究中，我们探讨大型语言模型（LLMs）能否重现这种分形特性，并确定在何种条件下它们可能会失败。此外，我们发现自然语言中观察到的分形参数范围狭窄，而LLMs输出的分形参数范围广泛，表明分形参数可能有助于检测LLM生成文本中的较大一部分。值得注意的是，这些发现以及本研究中报道的其他发现对于架构选择是稳健的；例如，Gemini 1.0 Pro、Mistral-7B和Gemma-2B。我们还发布了一个数据集，包含超过240,000篇由不同LLMs（包括预训练和指令调优）在不同解码温度和提示方法下生成的文章及其相应的手工生成文本。我们希望这项工作能够突出分形属性、提示和统计模拟之间复杂的相互作用，为生成、评估和检测合成文本提供见解。', 'title_zh': '两种结构的故事：大型语言模型能否捕捉语言的分形复杂性？'}
{'arxiv_id': 'arXiv:2502.14922', 'title': 'SIFT: Grounding LLM Reasoning in Contexts via Stickers', 'authors': 'Zihao Zeng, Xuyao Huang, Boxiu Li, Zhijie Deng', 'link': 'https://arxiv.org/abs/2502.14922', 'abstract': 'This paper identifies the misinterpretation of the context can be a significant issue during the reasoning process of large language models, spanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones like DeepSeek-R1. For example, in the phrase "10 dollars per kilo," LLMs might not recognize that "per" means "for each," leading to calculation errors. We introduce a novel, post-training approach called **Stick to the Facts (SIFT)** to tackle this. SIFT leverages increasing inference-time compute to ground LLM reasoning in contexts. At the core of SIFT lies the *Sticker*, which is generated by the model itself to explicitly emphasize the key information within the context. Given the curated Sticker, SIFT generates two predictions -- one from the original query and one from the query augmented with the Sticker. If they differ, the Sticker is sequentially refined via *forward* optimization (to better align the extracted facts with the query) and *inverse* generation (to conform with the model\'s inherent tendencies) for more faithful reasoning outcomes. Studies across diverse models (from 3B to 100B+) and benchmarks (e.g., GSM8K, MATH-500) reveal consistent performance improvements. Notably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from 78.33% to **85.67**%, establishing a new state-of-the-art in the open-source community. The code is available at this https URL.', 'abstract_zh': 'This paper identifies the misinterpretation of context as a significant issue during the reasoning process of large language models, spanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones like DeepSeek-R1. For example, in the phrase "10 dollars per kilo," LLMs might not recognize that "per" means "for each," leading to calculation errors. We introduce a novel, post-training approach called **Stick to the Facts (SIFT)** to address this. SIFT leverages increased inference-time compute to ground LLM reasoning in contexts. At the core of SIFT lies the *Sticker*, which is generated by the model itself to explicitly emphasize key information within the context. Given the curated Sticker, SIFT generates two predictions—one from the original query and one from the query augmented with the Sticker. If they differ, the Sticker is sequentially refined via forward optimization (to better align the extracted facts with the query) and inverse generation (to conform with the model\'s inherent tendencies) for more faithful reasoning outcomes. Studies across diverse models (from 3B to 100B+) and benchmarks (e.g., GSM8K, MATH-500) reveal consistent performance improvements. Notably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from 78.33% to **85.67%**, establishing a new state-of-the-art in the open-source community. The code is available at this https URL.', 'title_zh': 'SIFT: 通过贴纸在上下文中约束LLM推理'}
{'arxiv_id': 'arXiv:2502.14911', 'title': 'Batayan: A Filipino NLP benchmark for evaluating Large Language Models', 'authors': 'Jann Railey Montalan, Jimson Paulo Layacan, David Demitri Africa, Richell Isaiah Flores, Michael T. Lopez II, Theresa Denise Magsajo, Anjanette Cayabyab, William Chandra Tjhi', 'link': 'https://arxiv.org/abs/2502.14911', 'abstract': "Recent advances in large language models (LLMs) have demonstrated remarkable capabilities on widely benchmarked high-resource languages; however, linguistic nuances of under-resourced languages remain unexplored. We introduce Batayan, a holistic Filipino benchmark designed to systematically evaluate LLMs across three key natural language processing (NLP) competencies: understanding, reasoning, and generation. Batayan consolidates eight tasks, covering both Tagalog and code-switched Taglish utterances. Our rigorous, native-speaker-driven annotation process ensures fluency and authenticity to the complex morphological and syntactic structures of Filipino, alleviating a pervasive translationese bias in existing Filipino corpora. We report empirical results on a variety of multilingual LLMs, highlighting significant performance gaps that signal the under-representation of Filipino in pretraining corpora, the unique hurdles in modeling Filipino's rich morphology and construction, and the importance of explicit Filipino language support and instruction tuning. Moreover, we discuss the practical challenges encountered in dataset construction and propose principled solutions for building culturally and linguistically-faithful resources in under-represented languages. We also provide a public benchmark and leaderboard as a clear foundation for iterative, community-driven progress in Filipino NLP.", 'abstract_zh': '近期大型语言模型（LLMs）的进步在广泛benchmark的高资源语言上展现了显著的能力；然而，欠资源语言的语义细微差别仍需探索。我们介绍了Batayan，一个综合性的菲律滨语基准，旨在系统性地评估LLMs在自然语言处理（NLP）三大核心能力：理解、推理和生成上的表现。Batayan整合了八个任务，涵盖了塔加洛语及其混用塔古丽夏语表达。我们严格且以母语者驱动的注释过程确保了注释内容在复杂的形态和句法结构上具有流畅性和真实性，从而减轻现有菲律滨语语料库中的普遍翻译倾向偏差。我们在多种多语言LLMs上报告了实证结果，强调了菲律滨语在预训练语料库中的代表性不足、建模菲律滨语丰富形态与构造的独特挑战以及明示菲律滨语语言支持和指令调优的重要性。此外，我们讨论了在数据集构建过程中遇到的实践挑战，并提出了构建文化上和语言上忠实资源的原理性解决方案，以支持欠代表语言的发展。我们还提供了一个公开基准和排行榜，作为菲律滨语NLP持续社区驱动进展的明确基础。', 'title_zh': 'Batayan：一种用于评估大型语言模型的菲律宾自然语言处理基准'}
{'arxiv_id': 'arXiv:2502.14910', 'title': 'EvoP: Robust LLM Inference via Evolutionary Pruning', 'authors': 'Shangyu Wu, Hongchao Du, Ying Xiong, Shuai Chen, Tei-wei Kuo, Nan Guan, Chun Jason Xue', 'link': 'https://arxiv.org/abs/2502.14910', 'abstract': 'Large Language Models (LLMs) have achieved remarkable success in natural language processing tasks, but their massive size and computational demands hinder their deployment in resource-constrained environments. Existing structured pruning methods address this issue by removing redundant structures (e.g., elements, channels, layers) from the model. However, these methods employ a heuristic pruning strategy, which leads to suboptimal performance. Besides, they also ignore the data characteristics when pruning the model.\nTo overcome these limitations, we propose EvoP, an evolutionary pruning framework for robust LLM inference. EvoP first presents a cluster-based calibration dataset sampling (CCDS) strategy for creating a more diverse calibration dataset. EvoP then introduces an evolutionary pruning pattern searching (EPPS) method to find the optimal pruning pattern. Compared to existing structured pruning techniques, EvoP achieves the best performance while maintaining the best efficiency. Experiments across different LLMs and different downstream tasks validate the effectiveness of the proposed EvoP, making it a practical and scalable solution for deploying LLMs in real-world applications.', 'abstract_zh': 'EvoP：一种用于 robust LLM 推断的演化剪枝框架', 'title_zh': 'EvoP：演化裁剪实现 robust LLM 推理'}
{'arxiv_id': 'arXiv:2502.14907', 'title': 'GneissWeb: Preparing High Quality Data for LLMs at Scale', 'authors': 'Hajar Emami Gohari, Swanand Ravindra Kadhe, Syed Yousaf Shah. Constantin Adam, Abdulhamid Adebayo, Praneet Adusumilli, Farhan Ahmed, Nathalie Baracaldo Angel, Santosh Borse, Yuan-Chi Chang, Xuan-Hong Dang, Nirmit Desai, Ravital Eres, Ran Iwamoto, Alexei Karve, Yan Koyfman, Wei-Han Lee, Changchang Liu, Boris Lublinsky, Takuyo Ohko, Pablo Pesce, Maroun Touma, Shiqiang Wang, Shalisha Witherspoon, Herbert Woisetschlager, David Wood, Kun-Lung Wu, Issei Yoshida, Syed Zawad, Petros Zerfos, Yi Zhou, Bishwaranjan Bhattacharjee', 'link': 'https://arxiv.org/abs/2502.14907', 'abstract': "Data quantity and quality play a vital role in determining the performance of Large Language Models (LLMs). High-quality data, in particular, can significantly boost the LLM's ability to generalize on a wide range of downstream tasks. Large pre-training datasets for leading LLMs remain inaccessible to the public, whereas many open datasets are small in size (less than 5 trillion tokens), limiting their suitability for training large models.\nIn this paper, we introduce GneissWeb, a large dataset yielding around 10 trillion tokens that caters to the data quality and quantity requirements of training LLMs. Our GneissWeb recipe that produced the dataset consists of sharded exact sub-string deduplication and a judiciously constructed ensemble of quality filters. GneissWeb achieves a favorable trade-off between data quality and quantity, producing models that outperform models trained on state-of-the-art open large datasets (5+ trillion tokens).\nWe show that models trained using GneissWeb dataset outperform those trained on FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed on a set of 11 commonly used benchmarks (both zero-shot and few-shot) for pre-training dataset evaluation. When the evaluation set is extended to 20 benchmarks (both zero-shot and few-shot), models trained using GneissWeb still achieve a 1.75 percentage points advantage over those trained on FineWeb-V1.1.0.", 'abstract_zh': '大数据量和高质量数据在网络语言模型（LLM）性能中起着至关重要的作用。特别是高质量数据可以显著提升LLM在各种下游任务上的泛化能力。领先的LLM的大规模预训练数据集对公众而言仍然难以获取，而许多开源数据集规模较小（少于5万亿个词元），限制了其用于训练大规模模型的适用性。\n\n在本文中，我们介绍了GneissWeb，一个包含约10万亿个词元的大规模数据集，满足训练LLM的数据质量和数量要求。GneissWeb数据集的生成方法包括分片精确子字符串去重和精心构建的质量过滤器集合。GneissWeb在数据质量和数量之间实现了良好的权衡，生成的模型在使用状态最先进开源大规模数据集（5+万亿个词元）训练的模型中表现出色。\n\n研究表明，使用GneissWeb数据集训练的模型在一套11个常用预训练数据集评估基准（包括零样本和少样本）上的平均得分上，比使用FineWeb-V1.1.0训练的模型高出2.73个百分点。当评估集扩展到20个基准（包括零样本和少样本）时，使用GneissWeb训练的模型仍然比使用FineWeb-V1.1.0训练的模型具有1.75个百分点的优势。', 'title_zh': 'GneissWeb：为大规模生成模型准备高质量数据'}
{'arxiv_id': 'arXiv:2502.14905', 'title': 'Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence', 'authors': 'Bhavik Agarwal, Ishan Joshi, Viktoria Rojkova', 'link': 'https://arxiv.org/abs/2502.14905', 'abstract': 'In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of a 1.5B parameter model through a novel pipeline that combines synthetic reasoning dataset construction with custom reward functions under Group Relative Policy Optimization (GRPO). Specifically, we first perform R1 reinforcement learning on a 20K sample unstructured-to-structured dataset, mirroring the original DeepSeek R1 methods, to establish core reasoning abilities. Subsequently, we performed supervised fine-tuning on a separate 10K reasoning sample dataset, focusing on refining schema adherence for downstream tasks. Despite the relatively modest training scope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO training and 3 hours on 1xA100 for SFT, our model demonstrates robust performance in enforcing schema consistency. We compare our ThinkJSON approach against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1 (Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its effectiveness in real-world applications. Our results underscore the practical utility of a resource-efficient framework for schema-constrained text generation.', 'abstract_zh': '在本论文中，我们通过利用大语言模型的推理能力，解决了在大语言模型生成中严格遵循模式规范的挑战。基于DeepSeek R1强化学习框架，我们的方法通过一种新颖的管道流程来训练一个包含1.5亿参数的模型的结构化推理技能，该流程结合了合成推理数据集构建与定制的奖励函数，并在Group Relative Policy Optimization (GRPO) 下运行。具体来说，我们首先在20K样本的无结构到结构化数据集上进行R1强化学习，模仿原始的DeepSeek R1方法，以建立核心推理能力。随后，我们在一个独立的10K推理样本数据集上进行了监督微调，专注于改进下游任务中的模式遵守能力。尽管训练范围相对有限，GRPO训练大约需要8xH100 GPU集群20小时，SFT训练在1xA100上大约需要3小时，我们的模型在强制执行模式一致性方面显示出稳健的性能。我们将我们的ThinkJSON方法与原始的DeepSeek R1 (671B)、DeepSeek R1的蒸馏版本 (Qwen-1.5B 和 Qwen-7B) 以及Gemini 2.0 Flash (70B) 进行了比较，展示了其在实际应用中的有效性。我们的结果强调了资源高效框架在受限模式文本生成中的实用价值。', 'title_zh': '在JSON中思考：严格遵守LLM架构策略的强化学习方法'}
{'arxiv_id': 'arXiv:2502.14902', 'title': 'PathRAG: Pruning Graph-based Retrieval Augmented Generation with Relational Paths', 'authors': 'Boyu Chen, Zirui Guo, Zidan Yang, Yuluo Chen, Junze Chen, Zhenghao Liu, Chuan Shi, Cheng Yang', 'link': 'https://arxiv.org/abs/2502.14902', 'abstract': 'Retrieval-augmented generation (RAG) improves the response quality of large language models (LLMs) by retrieving knowledge from external databases. Typical RAG approaches split the text database into chunks, organizing them in a flat structure for efficient searches. To better capture the inherent dependencies and structured relationships across the text database, researchers propose to organize textual information into an indexing graph, known asgraph-based RAG. However, we argue that the limitation of current graph-based RAG methods lies in the redundancy of the retrieved information, rather than its insufficiency. Moreover, previous methods use a flat structure to organize retrieved information within the prompts, leading to suboptimal performance. To overcome these limitations, we propose PathRAG, which retrieves key relational paths from the indexing graph, and converts these paths into textual form for prompting LLMs. Specifically, PathRAG effectively reduces redundant information with flow-based pruning, while guiding LLMs to generate more logical and coherent responses with path-based prompting. Experimental results show that PathRAG consistently outperforms state-of-the-art baselines across six datasets and five evaluation dimensions. The code is available at the following link: this https URL', 'abstract_zh': '基于路径的检索增强生成（PathRAG）通过从索引图中检索关键关系路径并将其转换为文本形式来优化大型语言模型的响应质量', 'title_zh': '基于关系路径的图表示检索增强生成剪枝方法'}
{'arxiv_id': 'arXiv:2502.14880', 'title': 'KKA: Improving Vision Anomaly Detection through Anomaly-related Knowledge from Large Language Models', 'authors': 'Dong Chen, Zhengqing Hu, Peiguang Fan, Yueting Zhuang, Yafei Li, Qidong Liu, Xiaoheng Jiang, Mingliang Xu', 'link': 'https://arxiv.org/abs/2502.14880', 'abstract': 'Vision anomaly detection, particularly in unsupervised settings, often struggles to distinguish between normal samples and anomalies due to the wide variability in anomalies. Recently, an increasing number of studies have focused on generating anomalies to help detectors learn more effective boundaries between normal samples and anomalies. However, as the generated anomalies are often derived from random factors, they frequently lack realism. Additionally, randomly generated anomalies typically offer limited support in constructing effective boundaries, as most differ substantially from normal samples and lie far from the boundary. To address these challenges, we propose Key Knowledge Augmentation (KKA), a method that extracts anomaly-related knowledge from large language models (LLMs). More specifically, KKA leverages the extensive prior knowledge of LLMs to generate meaningful anomalies based on normal samples. Then, KKA classifies the generated anomalies as easy anomalies and hard anomalies according to their similarity to normal samples. Easy anomalies exhibit significant differences from normal samples, whereas hard anomalies closely resemble normal samples. KKA iteratively updates the generated anomalies, and gradually increasing the proportion of hard anomalies to enable the detector to learn a more effective boundary. Experimental results show that the proposed method significantly improves the performance of various vision anomaly detectors while maintaining low generation costs. The code for CMG can be found at this https URL.', 'abstract_zh': '基于关键知识增强的视觉异常检测方法', 'title_zh': 'KKA：通过大型语言模型中的异常相关知识改进视觉异常检测'}
