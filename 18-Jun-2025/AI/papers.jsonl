{'arxiv_id': 'arXiv:2506.14755', 'title': 'Optimizing Length Compression in Large Reasoning Models', 'authors': 'Zhengxiang Cheng, Dongping Chen, Mingyang Fu, Tianyi Zhou', 'link': 'https://arxiv.org/abs/2506.14755', 'abstract': 'Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at this https URL.', 'abstract_zh': '大型推理模型中的精炼：一种新的效率原则及其应用', 'title_zh': '在大型推理模型中优化长度压缩'}
{'arxiv_id': 'arXiv:2506.14728', 'title': 'AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes', 'authors': 'Jiahao Qiu, Xinzhe Juan, Yimin Wang, Ling Yang, Xuan Qi, Tongcheng Zhang, Jiacheng Guo, Yifu Lu, Zixin Yao, Hongru Wang, Shilong Liu, Xun Jiang, Liu Leqi, Mengdi Wang', 'link': 'https://arxiv.org/abs/2506.14728', 'abstract': 'While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents.', 'abstract_zh': '基于大型语言模型代理知识蒸馏的研究：AgentDistill框架', 'title_zh': 'AgentDistill：无需训练的通用MCP框代理精简'}
{'arxiv_id': 'arXiv:2506.14570', 'title': 'From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places', 'authors': 'Mohammad Hashemi, Andreas Zufle', 'link': 'https://arxiv.org/abs/2506.14570', 'abstract': 'Capturing human mobility is essential for modeling how people interact with and move through physical spaces, reflecting social behavior, access to resources, and dynamic spatial patterns. To support scalable and transferable analysis across diverse geographies and contexts, there is a need for a generalizable foundation model for spatiotemporal data. While foundation models have transformed language and vision, they remain limited in handling the unique challenges posed by the spatial, temporal, and semantic complexity of mobility data. This vision paper advocates for a new class of spatial foundation models that integrate geolocation semantics with human mobility across multiple scales. Central to our vision is a shift from modeling discrete points of interest to understanding places: dynamic, context-rich regions shaped by human behavior and mobility that may comprise many places of interest. We identify key gaps in adaptability, scalability, and multi-granular reasoning, and propose research directions focused on modeling places and enabling efficient learning. Our goal is to guide the development of scalable, context-aware models for next-generation geospatial intelligence. These models unlock powerful applications ranging from personalized place discovery and logistics optimization to urban planning, ultimately enabling smarter and more responsive spatial decision-making.', 'abstract_zh': '捕获人类移动对于建模人们在物理空间中的互动和移动至关重要，反映了社会行为、资源访问和动态空间模式。为了支持跨多样化地理区域和情境的大规模和可移植分析，需要一个适用于时空数据的基础模型。尽管基础模型已颠覆了语言和视觉领域，但在处理移动数据的空间性、时间和语义复杂性方面仍然有限。本文倡导一种新的空间基础模型类别，将地理位置语义与多尺度的人类移动整合。我们的愿景的核心是从建模离散的兴趣点转向理解地点：由人类行为和移动塑造的动态、富含上下文的区域，这些区域可能包含许多兴趣点。我们识别了可适应性、可扩展性和多粒度推理的关键缺口，并提出了重点在于建模地点和促进高效学习的研究方向。我们的目标是指导下一代地理空间智能的大规模、情境感知模型的发展。这些模型解锁了从个性化地点发现和物流优化到城市规划等一系列强大的应用，最终能够促进更智能和更响应的空间决策。', 'title_zh': '从点到地点：基于理解地点的人类移动驱动的时空基础模型研究'}
{'arxiv_id': 'arXiv:2506.14569', 'title': 'Enhancing Symbolic Machine Learning by Subsymbolic Representations', 'authors': 'Stephen Roth, Lennart Baur, Derian Boer, Stefan Kramer', 'link': 'https://arxiv.org/abs/2506.14569', 'abstract': 'The goal of neuro-symbolic AI is to integrate symbolic and subsymbolic AI approaches, to overcome the limitations of either. Prominent systems include Logic Tensor Networks (LTN) or DeepProbLog, which offer neural predicates and end-to-end learning. The versatility of systems like LTNs and DeepProbLog, however, makes them less efficient in simpler settings, for instance, for discriminative machine learning, in particular in domains with many constants. Therefore, we follow a different approach: We propose to enhance symbolic machine learning schemes by giving them access to neural embeddings. In the present paper, we show this for TILDE and embeddings of constants used by TILDE in similarity predicates. The approach can be fine-tuned by further refining the embeddings depending on the symbolic theory. In experiments in three real-world domain, we show that this simple, yet effective, approach outperforms all other baseline methods in terms of the F1 score. The approach could be useful beyond this setting: Enhancing symbolic learners in this way could be extended to similarities between instances (effectively working like kernels within a logical language), for analogical reasoning, or for propositionalization.', 'abstract_zh': '神经符号AI的目标是整合符号AI和次符号AI的方法，以克服各自局限性。 prominently 系统包括逻辑张量网络（LTN）或DeepProbLog，它们提供了神经谓词并实现了端到端学习。然而，如LTNs和DeepProbLog等系统的灵活性使得它们在简单设置中效率较低，尤其是在许多常量的领域里，尤其是对于有区别的机器学习。因此，我们采用了一种不同的方法：我们建议通过赋予符号机器学习方案访问神经嵌入的能力来增强它们。在本文中，我们展示了这一方法在TILDE及其在相似谓词中使用的常量嵌入中的应用。该方法可以通过进一步细化嵌入来根据符号理论进行调整。在三个真实世界的领域中进行的实验表明，这种方法在F1得分上优于所有基线方法。这种方法可能适用于更广泛的情境：以这种方式增强符号学习器可以扩展到实例之间的相似性（类似于逻辑语言中的核功能）、类比推理或命题化中。', 'title_zh': '通过亚符号表示增强符号机器学习'}
{'arxiv_id': 'arXiv:2506.14568', 'title': 'QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents', 'authors': "Eliott Thomas, Mickael Coustaty, Aurelie Joseph, Gaspar Deloin, Elodie Carel, Vincent Poulain D'Andecy, Jean-Marc Ogier", 'link': 'https://arxiv.org/abs/2506.14568', 'abstract': "Automating table extraction (TE) from business documents is critical for industrial workflows but remains challenging due to sparse annotations and error-prone multi-stage pipelines. While semi-supervised learning (SSL) can leverage unlabeled data, existing methods rely on confidence scores that poorly reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised Table extraction framework designed for business documents. QUEST introduces a novel quality assessment model that evaluates structural and contextual features of extracted tables, trained to predict F1 scores instead of relying on confidence metrics. This quality-aware approach guides pseudo-label selection during iterative SSL training, while diversity measures (DPP, Vendi score, IntDiv) mitigate confirmation bias. Experiments on a proprietary business dataset (1000 annotated + 10000 unannotated documents) show QUEST improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to 6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents), QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by 19% (from 27% to 22%). The framework's interpretable quality assessments and robustness to annotation scarcity make it particularly suited for business documents, where structural consistency and data completeness are paramount.", 'abstract_zh': '基于质量感知的自动表格提取框架：适用于商业文档的QUEST', 'title_zh': 'QUEST：面向质量的半监督商业文档表格提取'}
{'arxiv_id': 'arXiv:2506.14539', 'title': 'Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack', 'authors': 'Daewon Kang, YeongHwan Shin, Doyeon Kim, Kyu-Hwan Jung, Meong Hi Son', 'link': 'https://arxiv.org/abs/2506.14539', 'abstract': "Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelgänger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelgänger method. The experimental results demonstrate that the Doppelgänger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.", 'abstract_zh': '自大型语言模型出现以来，提示工程现在使快速、低 effort 地创建多样化的自主代理成为可能，这些代理已广泛投入使用。然而，这种便利性引发了对底层提示的安全性、鲁棒性和行为一致性的紧迫担忧，同时还提出了防止这些提示被用户滥用的迫切挑战。在本文中，我们提出了一种“双胞胎方法”来展示代理被劫持的风险，从而暴露系统指令和内部信息。接着，我们定义了“对抗转移导致提示对齐崩溃 (PACAT)”级别来评估对此类对抗转移攻击的脆弱性。我们还提出了一种“对抗转移警示 (CAT)”提示来抵御双胞胎方法。实验结果表明，双胞胎方法可以破坏代理的一致性并暴露其内部信息。相比之下，CAT 提示能够有效防御这种对抗攻击。', 'title_zh': '双身人方法：通过基于提示的可转移 adversarial 攻击打破 LLM 代理的角色一致性'}
{'arxiv_id': 'arXiv:2506.14502', 'title': 'Toward Safety-First Human-Like Decision Making for Autonomous Vehicles in Time-Varying Traffic Flow', 'authors': 'Xiao Wang, Junru Yu, Jun Huang, Qiong Wu, Ljubo Vacic, Changyin Sun', 'link': 'https://arxiv.org/abs/2506.14502', 'abstract': "Despite the recent advancements in artificial intelligence technologies have shown great potential in improving transport efficiency and safety, autonomous vehicles(AVs) still face great challenge of driving in time-varying traffic flow, especially in dense and interactive situations. Meanwhile, human have free wills and usually do not make the same decisions even situate in the exactly same scenarios, leading to the data-driven methods suffer from poor migratability and high search cost problems, decreasing the efficiency and effectiveness of the behavior policy. In this research, we propose a safety-first human-like decision-making framework(SF-HLDM) for AVs to drive safely, comfortably, and social compatiblely in effiency. The framework integrates a hierarchical progressive framework, which combines a spatial-temporal attention (S-TA) mechanism for other road users' intention inference, a social compliance estimation module for behavior regulation, and a Deep Evolutionary Reinforcement Learning(DERL) model for expanding the search space efficiently and effectively to make avoidance of falling into the local optimal trap and reduce the risk of overfitting, thus make human-like decisions with interpretability and flexibility. The SF-HLDM framework enables autonomous driving AI agents dynamically adjusts decision parameters to maintain safety margins and adhering to contextually appropriate driving behaviors at the same time.", 'abstract_zh': '基于安全性优先的人类-like 决策框架（SF-HLDM）用于自主车辆的安全、舒适且社会兼容的高效驾驶', 'title_zh': '面向时间变化交通流中安全优先的人类like决策制定的自动驾驶车辆方法研究'}
{'arxiv_id': 'arXiv:2506.14496', 'title': 'LLM-Powered Swarms: A New Frontier or a Conceptual Stretch?', 'authors': 'Muhammad Atta Ur Rahman, Melanie Schranz', 'link': 'https://arxiv.org/abs/2506.14496', 'abstract': "Swarm intelligence traditionally refers to systems of simple, decentralized agents whose local interactions lead to emergent, collective behavior. Recently, the term 'swarm' has been extended to describe AI systems like OpenAI's Swarm, where large language models (LLMs) act as collaborative agents. This paper contrasts traditional swarm algorithms with LLM-driven swarms exploring how decentralization, scalability, and emergence are redefined in modern artificial intelligence (AI). We implement and compare both paradigms using Boids and Ant Colony Optimization (ACO), evaluating latency, resource usage, and behavioral accuracy. The suitability of both cloud-based and local LLMs is assessed for the agent-based use in swarms. Although LLMs offer powerful reasoning and abstraction capabilities, they introduce new constraints in computation and coordination that challenge traditional notions of swarm design. This study highlights the opportunities and limitations of integrating LLMs into swarm systems and discusses the evolving definition of 'swarm' in modern AI research.", 'abstract_zh': 'swarm智能 traditionally refers to 系统中简单且分散的代理通过局部交互产生 emergent 集体行为。最近，术语 ‘swarm’ 已被扩展，以描述 如OpenAI的 Swarm这样的AI系统，其中大型语言模型 (LLMs) 作为协作代理。本文探讨了传统 swarm 算法与由LLM驱动的swarm之间的区别，研究了去中心化、可扩展性和涌现性在现代人工智能 (AI) 中的新定义。我们使用Boids和蚁群优化 (ACO) 实现并比较了这两种范式，评估了延迟、资源使用和行为准确性。评估了基于云和本地的LLM在swarm中基于代理使用的适用性。尽管LLM提供了强大的推理和抽象能力，但它们在计算和协调方面引入了新的约束，挑战了传统swarm设计的概念。本文探讨了将LLM集成到swarm系统中的机会和局限性，并讨论了现代AI研究中 ‘swarm’ 定义的演变。', 'title_zh': 'LLM驱动的集群：一片新天地还是概念 overstretched？'}
{'arxiv_id': 'arXiv:2506.14477', 'title': 'GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in Real-World Anomalies', 'authors': 'Jingqi Yang, Zhilong Song, Jiawei Chen, Mingli Song, Sheng Zhou, linjun sun, Xiaogang Ouyang, Chun Chen, Can Wang', 'link': 'https://arxiv.org/abs/2506.14477', 'abstract': 'The development of high-quality datasets is crucial for benchmarking and advancing research in Graphical User Interface (GUI) agents. Despite their importance, existing datasets are often constructed under idealized conditions, overlooking the diverse anomalies frequently encountered in real-world deployments. To address this limitation, we introduce GUI-Robust, a novel dataset designed for comprehensive GUI agent evaluation, explicitly incorporating seven common types of anomalies observed in everyday GUI interactions. Furthermore, we propose a semi-automated dataset construction paradigm that collects user action sequences from natural interactions via RPA tools and then generate corresponding step and task descriptions for these actions with the assistance of MLLMs. This paradigm significantly reduces annotation time cost by a factor of over 19 times. Finally, we assess state-of-the-art GUI agents using the GUI-Robust dataset, revealing their substantial performance degradation in abnormal scenarios. We anticipate that our work will highlight the importance of robustness in GUI agents and inspires more future research in this direction. The dataset and code are available at this https URL..', 'abstract_zh': '高質量數據集的開發對於 avaliação 和促進圖形用戶界面（GUI）代理的研究至關重要。儘管其重要性，現有數據集往往是根據理想化的條件構建的，忽略了真實部署中頻頻遇到的多樣化的異常。為了解決這一局限，我們介紹了 GUI-Robust，一個新的數據集，旨在全面評價 GUI 代理，並將七種常見的日常 GUI 交互異常Explicitly整合其中。此外，我們提出了一種半自動化的數據集構建框架，通過 RPA 工具收集用戶操作序列，然後利用 MLLMs 給出這些操作對應的操作步驟和任務描述。該框架將注釋時間成本顯著降低超過19倍。最後，我們使用 GUI-Robust 數據集評估現有先進的 GUI 代理，在異常場景中揭示了其顯著Performance下降。我們預期本工作將突出強健性在 GUI 代理中的重要性，並 Inspirer更多的未來研究。數據集和代碼可在以下链接获得：这个链接。', 'title_zh': 'GUI-稳健：一个全面的数据集，用于在实际异常中测试GUI代理的稳健性'}
{'arxiv_id': 'arXiv:2506.14470', 'title': 'AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph Representations on Code Clone Detection', 'authors': 'Zixian Zhang, Takfarinas Saber', 'link': 'https://arxiv.org/abs/2506.14470', 'abstract': 'As one of the most detrimental code smells, code clones significantly increase software maintenance costs and heighten vulnerability risks, making their detection a critical challenge in software engineering. Abstract Syntax Trees (ASTs) dominate deep learning-based code clone detection due to their precise syntactic structure representation, but they inherently lack semantic depth. Recent studies address this by enriching AST-based representations with semantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs (DFGs). However, the effectiveness of various enriched AST-based representations and their compatibility with different graph-based machine learning techniques remains an open question, warranting further investigation to unlock their full potential in addressing the complexities of code clone detection. In this paper, we present a comprehensive empirical study to rigorously evaluate the effectiveness of AST-based hybrid graph representations in Graph Neural Network (GNN)-based code clone detection. We systematically compare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs (FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid representations impact GNNs differently: while AST+CFG+DFG consistently enhances accuracy for convolution- and attention-based models (Graph Convolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST frequently introduces structural complexity that harms performance. Notably, GMN outperforms others even with standard AST representations, highlighting its superior cross-code similarity detection and reducing the need for enriched structures.', 'abstract_zh': '作为最具破坏性的代码气味之一，代码克隆显著增加了软件维护成本并提高了漏洞风险，使其检测成为软件工程中的关键挑战。在基于深度学习的代码克隆检测中，抽象语法树（AST）因其精确的语法结构表示占据主导地位，但也缺乏语义深度。近期研究通过将控制流图（CFG）和数据流图（DFG）等语义图融入基于AST的表示来解决这一问题。然而，各种增强的基于AST的表示及其与不同图机器学习技术的兼容性仍然存在问题，需要进一步研究以充分发挥其在解决代码克隆检测复杂性方面的潜力。在本文中，我们进行了一项全面的经验研究，以严格评估基于图神经网络（GNN）的代码克隆检测中基于AST的混合图表示的有效性。我们系统地比较了各种混合表示（包括CFG、DFG、流增强的AST（FA-AST））在多种GNN架构下的效果。我们的实验揭示了混合表示对GNN的影响不同：虽然AST+CFG+DFG一致地增强卷积和注意力模型（图卷积网络（GCN）、图注意力网络（GAT））的准确性，但FA-AST经常引入结构复杂性从而损害性能。值得注意的是，GMN即使使用标准AST表示也优于其他方法，突显了其优越的跨代码相似性检测能力，从而减少了对复杂结构的需求。', 'title_zh': 'AST增强还是AST过载？混合图表示对代码克隆检测的意外影响'}
{'arxiv_id': 'arXiv:2506.14387', 'title': "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning", 'authors': 'William F. Shen, Xinchi Qiu, Nicola Cancedda, Nicholas D. Lane', 'link': 'https://arxiv.org/abs/2506.14387', 'abstract': "Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.", 'abstract_zh': '现有工作在缓解大规模语言模型（LLM）微调中的灾难性遗忘时，主要关注保留特定数据或任务，而严重忽视了通过安全对齐培养的关键能力的退化，特别是模型忠实地表达无知的能力。本文表明，在常规微调过程中，这种能力会受到显著削弱，导致诸如幻觉等不希望出现的行为。为应对这一新颖且高度实际的问题，我们提出了一种名为SEAT的简单而有效的微调方法，该方法既能保持微调性能，又能保留模型固有的承认自身无知的能力。SEAT结合了两个关键组成部分：（1）稀疏训练以限制激活漂移，以及（2）一种新的实体扰动方法，带有KL散度正则化，旨在对抗知识纠缠。实验结果表明，SEAT在保留无知意识的同时显著优于基线模型，为LLM微调提供了更为 robust 的解决方案。', 'title_zh': '不要胡编乱造：在大模型微调中保留无知意识'}
{'arxiv_id': 'arXiv:2506.14336', 'title': 'AviationLLM: An LLM-based Knowledge System for Aviation Training', 'authors': "Jia'ang Wan, Feng Shen, Fujuan Li, Yanjin Sun, Yan Li, Shiwen Zhang", 'link': 'https://arxiv.org/abs/2506.14336', 'abstract': 'Aviation training is a core link in ensuring flight safety, improving industry efficiency and promoting sustainable development. It not only involves flight simulation but also requires the learning of a great deal of professional aviation theory knowledge. In the existing training system, the knowledge is mainly imparted by the the instructors. However, the number of instructors is limited and the professional answers obtained from the Internet are not accurate enough, resulting in low training efficiency. To address this, we introduced LLM, but the basic pre-trained model cannot provide accurate answers to professional fields, so we fine-tuned it. Traditional Supervised Fine-Tuning (SFT) risk generating superficially plausible but factually incorrect responses due to insufficient data coverage. To address this, we employ Direct Preference Optimization(DPO). This paper proposes Retrieval-Augmented LLM Alignment via Direct Preference Optimization(RALA-DPO). We select open source pre-trained LLM Qwen and adapt it to aviation theory training through DPO-based domain alignment. Simultaneously, to mitigate hallucinations caused by training data biases, knowledge obsolescence, or domain knowledge gaps, we implement Retrieval-Augmented Generation(RAG) technology that combines generative and retrieval models. RALA-DPO effectively retrieves relevant information from external knowledge bases and delivers precise and high-quality responses through the generative model. Experimental results demonstrate that RALA-DPO can improve accuracy in response to professional aviation knowledge. With integrated RAG mechanisms, this system can further improve the accuracy of answers and achieve zero-cost knowledge updates simultaneously.', 'abstract_zh': '航空培训是确保飞行安全、提高行业效率和促进可持续发展的一个核心环节。它不仅涉及飞行模拟，还需要学习大量的专业航空理论知识。在现有的培训体系中，知识主要由教员传授。然而，教员数量有限，从网上获得的专业答案不够准确，导致培训效率低。为了解决这一问题，我们引入了大语言模型（LLM），但是基础预训练模型无法提供专业的准确答案，因此我们对它进行了微调。传统的监督微调（SFT）由于数据覆盖不全，存在生成表面合理但事实错误的回答的风险。为此，我们采用了直接偏好优化（DPO）。本文提出了一种名为基于DPO的检索增强LLM对齐方法（RALA-DPO）。我们选择了开源预训练LLM Qwen，并通过基于DPO的领域对齐将其应用于航空理论培训。同时，为了缓解由于训练数据偏差、知识过时或领域知识空白导致的幻觉，我们实施了结合生成和检索模型的检索增强生成（RAG）技术。RALA-DPO能够从外部知识库中检索相关的信息，并通过生成模型提供准确和高质量的回答。实验结果表明，RALA-DPO能够提高对专业航空知识的回答准确性。通过集成RAG机制，该系统可以进一步提高回答的准确性，并同时实现零成本的知识更新。', 'title_zh': 'AviationLLM：基于LLM的航空培训知识系统'}
{'arxiv_id': 'arXiv:2506.14299', 'title': 'ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems', 'authors': 'Fanzhi Zeng, Siqi Wang, Chuzhao Zhu, Li Li', 'link': 'https://arxiv.org/abs/2506.14299', 'abstract': "How to construct an interpretable autonomous driving decision-making system has become a focal point in academic research. In this study, we propose a novel approach that leverages large language models (LLMs) to generate executable, rule-based decision systems to address this challenge. Specifically, harnessing the strong reasoning and programming capabilities of LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based Decision Systems) framework, which integrates three core modules: the Information Module, the Agents Module, and the Testing Module. The framework operates by first aggregating contextual driving scenario information through the Information Module, then utilizing the Agents Module to generate rule-based driving tactics. These tactics are iteratively refined through continuous interaction with the Testing Module. Extensive experimental evaluations demonstrate that ADRD exhibits superior performance in autonomous driving decision tasks. Compared to traditional reinforcement learning approaches and the most advanced LLM-based methods, ADRD shows significant advantages in terms of interpretability, response speed, and driving performance. These results highlight the framework's ability to achieve comprehensive and accurate understanding of complex driving scenarios, and underscore the promising future of transparent, rule-based decision systems that are easily modifiable and broadly applicable. To the best of our knowledge, this is the first work that integrates large language models with rule-based systems for autonomous driving decision-making, and our findings validate its potential for real-world deployment.", 'abstract_zh': '如何构建可解释的自动驾驶决策系统已成为学术研究的焦点。本研究提出了一种新颖的方法，利用大型语言模型（LLMs）生成可执行的基于规则的决策系统以应对这一挑战。具体而言，借助LLMs的强大推理和编程能力，我们介绍了基于规则驱动的自动驾驶（ADRD：LLM-Driven Autonomous Driving Based on Rule-based Decision Systems）框架，该框架集成三个核心模块：信息模块、代理模块和测试模块。该框架通过信息模块首先聚合上下文驾驶场景信息，然后利用代理模块生成基于规则的驾驶策略，并通过与测试模块的持续交互逐步完善这些策略。广泛的实验评估表明，ADRD在自动驾驶决策任务中表现出优异性能。与传统强化学习方法和最先进的基于LLM的方法相比，ADRD在可解释性、响应速度和驾驶性能方面具有显著优势。这些结果突显了该框架实现复杂驾驶场景全面准确理解的能力，并强调了基于规则、易于修改且广泛适用的透明决策系统具有广阔前景。据我们所知，这是首次将大型语言模型与基于规则的系统集成用于自动驾驶决策的研究，我们的发现验证了其在实际部署中的潜在价值。', 'title_zh': 'ADRN：基于规则驱动决策系统的LLM驱动自动驾驶'}
{'arxiv_id': 'arXiv:2506.14276', 'title': "Don't throw the baby out with the bathwater: How and why deep learning for ARC", 'authors': 'Jack Cole, Mohamed Osman', 'link': 'https://arxiv.org/abs/2506.14276', 'abstract': "The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable challenge for AI systems. Despite the typically low performance on ARC, the deep learning paradigm remains the most effective known strategy for generating skillful (state-of-the-art) neural networks (NN) across varied modalities and tasks in vision, language etc. The deep learning paradigm has proven to be able to train these skillful neural networks and learn the abstractions needed in these diverse domains. Our work doubles down on that and continues to leverage this paradigm by incorporating on-the-fly NN training at test time. We demonstrate that fully committing to deep learning's capacity to acquire novel abstractions yields state-of-the-art performance on ARC. Specifically, we treat both the neural network and the optimizer (rather than just a pre-trained network) as integral components of the inference process, fostering generalization to unseen tasks. Concretely, we propose a methodology for training on ARC, starting from pretrained LLMs, and enhancing their ARC reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment Inference Reverse-Augmentation and Vote (AIRV) as effective test-time techniques. We are the first to propose and show deep learning can be used effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a further 300% boost with TTFT. An early version of this approach secured first place in the 2023 ARCathon competition, while the final version achieved the current best score on the ARC private test-set (58%). Our findings highlight the key ingredients of a robust reasoning system in unfamiliar domains, underscoring the central mechanisms that improve broad perceptual reasoning.", 'abstract_zh': 'ARC-AGI的抽象与推理语料库对AI系统构成了严峻挑战。尽管ARC上的表现通常较低，但深度学习范式仍然是生成各种模态和任务（如视觉、语言等）中表现最先进的神经网络（NN）的最有效已知策略。深度学习范式已经被证明能够训练这些表现先进的神经网络，并学会在这些多样的领域中所需的抽象。我们的工作正是基于这一点，继续利用这一范式，在测试时加入即用即训的NN训练。我们证明了完全投入深度学习获取新颖抽象的能力可以在ARC上达到最先进的性能。具体而言，我们不仅将神经网络，还将优化器（而不仅仅是预训练的网络）视为推理过程中的核心组件，从而促进对未见过的任务的泛化。我们提出了一种针对ARC的训练方法，从预训练的大语言模型开始，增强其ARC推理能力。我们还提出了测试时微调（TTFT）和增强推理逆增强与投票（AIRV）作为有效的测试时技术。这是首次提出并证明深度学习可以有效用于ARC，通过AIRV提高了高达260%的准确性，TTFT进一步提高了300%的准确性。早期版本的这种方法在2023年的ARCathon竞赛中获得了第一名，最终版本在ARC的私有测试集中达到了当前的最佳分数（58%）。我们的发现突显了在不熟悉领域中构建稳健推理系统的关键成分，并强调了提高广泛知觉推理的主要机制。', 'title_zh': '不要抛弃婴儿：关于ARC的深度学习方法及其原因'}
{'arxiv_id': 'arXiv:2506.14246', 'title': 'Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents', 'authors': 'Lingfeng Li, Yunlong Lu, Yongyi Wang, Qifan Zheng, Wenxin Li', 'link': 'https://arxiv.org/abs/2506.14246', 'abstract': "People need to internalize the skills of AI agents to improve their own capabilities. Our paper focuses on Mahjong, a multiplayer game involving imperfect information and requiring effective long-term decision-making amidst randomness and hidden information. Through the efforts of AI researchers, several impressive Mahjong AI agents have already achieved performance levels comparable to those of professional human players; however, these agents are often treated as black boxes from which few insights can be gleaned. This paper introduces Mxplainer, a parameterized search algorithm that can be converted into an equivalent neural network to learn the parameters of black-box agents. Experiments conducted on AI and human player data demonstrate that the learned parameters provide human-understandable insights into these agents' characteristics and play styles. In addition to analyzing the learned parameters, we also showcase how our search-based framework can locally explain the decision-making processes of black-box agents for most Mahjong game states.", 'abstract_zh': '人们需要内化AI代理的技能以提高自身能力。本文专注于麻将这种涉及不完全信息的多人游戏，需要在随机性和隐藏信息中进行有效的长期决策。通过AI研究者的努力，已经有一些麻将AI代理达到了专业人类玩家的水平；然而，这些代理往往被视为黑盒，从中得出的见解有限。本文介绍了一种参数化搜索算法Mxplainer，可以通过转换为等效的神经网络来学习黑盒代理的参数。实验表明，学习到的参数为人类提供了对这些代理特性和游戏风格的可理解见解。除了分析学习到的参数外，我们还展示了基于搜索的框架如何在大多数麻将游戏状态下局部解释黑盒代理的决策过程。', 'title_zh': 'Mxplainer: 通过模仿麻雀代理来解释和学习洞察'}
{'arxiv_id': 'arXiv:2506.14245', 'title': 'Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs', 'authors': 'Xumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu, Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang, Junjie Li, Ziming Miao, Jiang Bian, Mao Yang', 'link': 'https://arxiv.org/abs/2506.14245', 'abstract': 'Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.', 'abstract_zh': '可验证奖励的强化学习（RLVR）正逐渐成为提升大型语言模型（LLMs）推理能力的有前景范式。然而，其有效性的关键悖论使其效果受到质疑：RLVR调优模型往往在解决方案查找的$Pass@K$度量上表现逊于基模型，这引发了一个假设，即RLVR只是以牺牲推理多样性为代价重新加权现有的推理路径。在本工作中，我们通过识别问题的根源——$Pass@K$度量本身是推理的不完善衡量标准，它奖励了可能源自错误或不完整的推理链条的正确最终答案，从而解决了这一矛盾。我们引入了一个更精确的评估指标$CoT$-$Pass@K$，该指标要求推理路径和最终答案都正确。我们提供了新的理论基础，正式化了RLVR与传统RL的独特区别在于其激励逻辑完整性的独特结构。我们的实验证据支持了这一观点：通过$CoT$-$Pass@K$，我们可以观察到RLVR可以激励正确的推理泛化。通过对训练动态的分析，我们发现这种增强的推理能力在训练早期出现，并且能平稳泛化。我们的工作提供了RLVR作用的清晰视角，提供了更可靠的评估方法，并证实了其能够真正推动机器推理进步的潜力。', 'title_zh': '具有可验证奖励的强化学习隐式激励基模型进行正确推理'}
{'arxiv_id': 'arXiv:2506.14239', 'title': 'Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?', 'authors': 'Louis Vervoort, Vitaly Nikolaev', 'link': 'https://arxiv.org/abs/2506.14239', 'abstract': 'We propose a test for abstract causal reasoning in AI, based on scholarship in the philosophy of causation, in particular on the neuron diagrams popularized by D. Lewis. We illustrate the test on advanced Large Language Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already capable of correctly identifying causes in cases that are hotly debated in the literature. In order to assess the results of these LLMs and future dedicated AI, we propose a definition of cause in neuron diagrams with a wider validity than published hitherto, which challenges the widespread view that such a definition is elusive. We submit that these results are an illustration of how future philosophical research might evolve: as an interplay between human and artificial expertise.', 'abstract_zh': '我们提出了一种基于因果哲学研究的AI抽象因果推理测试，特别参考了D.刘易斯普及的神经元图示。我们在先进的大型语言模型（ChatGPT、DeepSeek和Gemini）上进行了测试。令人惊讶的是，这些聊天机器人已经能够正确识别文献中高度争议的因果关系案例。为了评估这些LLM及其未来专门化AI的结果，我们提出了一种比以往发表的更为广泛的在神经元图示中定义因果的概念，挑战了因果定义难以捉摸的普遍观点。我们认为这些结果是未来哲学研究如何演进的一个例证：即人类与人工智能专长之间的互动过程。', 'title_zh': '神经图中原因的揭示，以及在大型语言模型中测试因果推理：哲学未来的瞥见？'}
{'arxiv_id': 'arXiv:2506.14231', 'title': 'ImpReSS: Implicit Recommender System for Support Conversations', 'authors': 'Omri Haller, Yair Meidan, Dudu Mimran, Yuval Elovici, Asaf Shabtai', 'link': 'https://arxiv.org/abs/2506.14231', 'abstract': "Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.", 'abstract_zh': '基于大型语言模型的隐式推荐系统：ImpReSS在客户服务对话中的应用', 'title_zh': '隐式推荐系统支持对话推荐'}
{'arxiv_id': 'arXiv:2506.14224', 'title': 'From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models', 'authors': 'Xinyang Li, Siqi Liu, Bochao Zou, Jiansheng Chen, Huimin Ma', 'link': 'https://arxiv.org/abs/2506.14224', 'abstract': "As large language models evolve, there is growing anticipation that they will emulate human-like Theory of Mind (ToM) to assist with routine tasks. However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven assessment of ToM in multimodal large language models (MLLMs). Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that attention heads in multimodal large models can distinguish cognitive information across perspectives, providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that significantly enhances the model's exhibited ToM by adjusting in the direction of the attention head.", 'abstract_zh': '随着大型语言模型的发展，人们越来越期待它们能够模拟出类似人类的理论思维（ToM）以辅助处理常规任务。然而，现有的机器ToM评估方法主要集中在单模态模型上，并且大多将这些模型视为黑盒，缺乏对其内部机制的解释性探索。为此，本研究采用基于内部机制的方法，对多模态大型语言模型（MLLMs）的ToM进行解释驱动评估。具体而言，我们首先构建了一个多模态ToM测试数据集GridToM，该数据集包含了来自多个视角的多元信念测试任务和感知信息。接着，我们的分析显示，多模态大型模型的注意力头部能够区分不同视角下的认知信息，提供了ToM能力的证据。此外，我们提出了一种轻量级且无需训练的方法，通过调整指向注意力头部的方向显著增强了模型展现的ToM能力。', 'title_zh': '从黑箱到透明思维：评估与增强多模态大语言模型的理论共情能力'}
{'arxiv_id': 'arXiv:2506.14212', 'title': "What's in the Box? Reasoning about Unseen Objects from Multimodal Cues", 'authors': 'Lance Ying, Daniel Xu, Alicia Zhang, Katherine M. Collins, Max H. Siegel, Joshua B. Tenenbaum', 'link': 'https://arxiv.org/abs/2506.14212', 'abstract': "People regularly make inferences about objects in the world that they cannot see by flexibly integrating information from multiple sources: auditory and visual cues, language, and our prior beliefs and knowledge about the scene. How are we able to so flexibly integrate many sources of information to make sense of the world around us, even if we have no direct knowledge? In this work, we propose a neurosymbolic model that uses neural networks to parse open-ended multimodal inputs and then applies a Bayesian model to integrate different sources of information to evaluate different hypotheses. We evaluate our model with a novel object guessing game called ``What's in the Box?'' where humans and models watch a video clip of an experimenter shaking boxes and then try to guess the objects inside the boxes. Through a human experiment, we show that our model correlates strongly with human judgments, whereas unimodal ablated models and large multimodal neural model baselines show poor correlation.", 'abstract_zh': '人们通过灵活整合来自多种来源的信息（听觉和视觉线索、语言以及我们对场景的先验信念和知识）来推断他们无法看到的世界中的物体。我们是如何能够如此灵活地整合许多信息来源来理解我们周围的这个世界，即使我们没有直接的知识？在这项工作中，我们提出了一种神经符号模型，使用神经网络解析开放式的多模态输入，然后应用贝叶斯模型来整合不同来源的信息，评估不同的假说。我们使用一项名为“箱内有何物？”的新颖物体猜测游戏来评估我们的模型，其中人类和模型观看实验员摇动箱子的视频片段，然后尝试猜测箱内的物体。通过一项人类实验，我们证明我们的模型与人类判断有很强的相关性，而单模态删除模型和大型多模态神经基线模型则显示出较差的相关性。', 'title_zh': '盒子里有什么？从多模态线索推理未知物体'}
{'arxiv_id': 'arXiv:2506.14146', 'title': 'Collaborative Editable Model', 'authors': 'Kaiwen Tang, Aitong Wu, Yao Lu, Guangda Sun', 'link': 'https://arxiv.org/abs/2506.14146', 'abstract': 'Vertical-domain large language models (LLMs) play a crucial role in specialized scenarios such as finance, healthcare, and law; however, their training often relies on large-scale annotated data and substantial computational resources, impeding rapid development and continuous iteration. To address these challenges, we introduce the Collaborative Editable Model (CoEM), which constructs a candidate knowledge pool from user-contributed domain snippets, leverages interactive user-model dialogues combined with user ratings and attribution analysis to pinpoint high-value knowledge fragments, and injects these fragments via in-context prompts for lightweight domain adaptation. With high-value knowledge, the LLM can generate more accurate and domain-specific content. In a financial information scenario, we collect 15k feedback from about 120 users and validate CoEM with user ratings to assess the quality of generated insights, demonstrating significant improvements in domain-specific generation while avoiding the time and compute overhead of traditional fine-tuning workflows.', 'abstract_zh': '垂直领域大型语言模型（LLMs）在金融、医疗和法律等专业场景中发挥着重要作用；然而，它们的训练通常依赖大量标注数据和大量的计算资源，阻碍了快速开发和持续迭代。为应对这些挑战，我们介绍了协作可编辑模型（CoEM），该模型从用户的领域片段贡献中构建候选知识池，通过结合用户对话、评级和归因分析来识别高价值知识片段，并通过上下文提示将这些片段注入模型进行轻量级领域适应。借助高价值知识，LLM能够生成更准确且更具领域特定性的内容。在金融信息场景中，我们收集了约120名用户的15000条反馈，并使用用户评级验证CoEM的效果，评估生成洞察的质量，同时避免了传统微调工作流的时间和计算开销，显示出在领域特定生成方面的显著改进。', 'title_zh': '协作可编辑模型'}
{'arxiv_id': 'arXiv:2506.14125', 'title': 'Situational-Constrained Sequential Resources Allocation via Reinforcement Learning', 'authors': 'Libo Zhang, Yang Chen, Toru Takisaka, Kaiqi Zhao, Weidong Li, Jiamou Liu', 'link': 'https://arxiv.org/abs/2506.14125', 'abstract': 'Sequential Resource Allocation with situational constraints presents a significant challenge in real-world applications, where resource demands and priorities are context-dependent. This paper introduces a novel framework, SCRL, to address this problem. We formalize situational constraints as logic implications and develop a new algorithm that dynamically penalizes constraint violations. To handle situational constraints effectively, we propose a probabilistic selection mechanism to overcome limitations of traditional constraint reinforcement learning (CRL) approaches. We evaluate SCRL across two scenarios: medical resource allocation during a pandemic and pesticide distribution in agriculture. Experiments demonstrate that SCRL outperforms existing baselines in satisfying constraints while maintaining high resource efficiency, showcasing its potential for real-world, context-sensitive decision-making tasks.', 'abstract_zh': '随机动态资源分配：情境约束下的序列资源分配提出在实际应用中一个显著挑战，其中资源需求和优先级依赖于具体情境。本文提出了一种新颖的框架SCRL以应对这一问题。我们将情境约束形式化为逻辑蕴含，并开发了一种新的算法以动态惩罚约束违反。为了有效处理情境约束，我们提出了一种概率选择机制来克服传统约束强化学习（CRL）方法的局限性。我们在两种场景下评估了SCRL：灾 pandemic 中的医疗资源分配以及农业中的农药分配。实验结果表明，SCRL 在满足约束条件的同时保持了高资源效率，展示了其在实际情境敏感决策任务中的潜力。', 'title_zh': '基于情景约束的序列资源分配强化学习方法'}
{'arxiv_id': 'arXiv:2506.14092', 'title': 'Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models', 'authors': 'Haonan Yin, Shai Vardi, Vidyanand Choudhary', 'link': 'https://arxiv.org/abs/2506.14092', 'abstract': 'Large language models (LLMs) are increasingly used in decision-support systems across high-stakes domains such as hiring and university admissions, where decisions often involve selecting among competing alternatives. While prior work has noted positional order biases in LLM-driven comparisons, these biases have not been systematically dissected or linked to underlying preference structures. We provide the first comprehensive investigation of positional biases across multiple LLM architectures and domains, uncovering strong and consistent order effects, including a novel centrality bias not previously documented in human or machine decision-making. We also find a quality-dependent shift: when options are high quality, models exhibit primacy bias, but favor latter options when option quality is low. We further identify a previously undocumented bias favoring certain names over others. To distinguish superficial tie-breaking from true distortions of judgment, we introduce a framework that classifies pairwise preferences as robust, fragile, or indifferent. We show that order effects can lead models to select strictly inferior options, and that positional biases are typically stronger than gender biases. These findings suggest that LLMs are not merely inheriting human-like biases, but exhibit distinct failure modes not seen in human decision-making. We propose targeted mitigation strategies, including a novel use of the temperature parameter, to reduce order-driven distortions.', 'abstract_zh': '大型语言模型（LLMs）在高风险领域如招聘和大学录取中的决策支持系统中的位置偏见综合研究：新的中心性偏见与质量依赖性切换，未记录的名称偏好，以及判断扭曲的分类框架', 'title_zh': '易碎的偏好：对大规模语言模型中顺序效应的深入探究'}
{'arxiv_id': 'arXiv:2506.14084', 'title': 'Lightweight Relevance Grader in RAG', 'authors': 'Taehee Jeong', 'link': 'https://arxiv.org/abs/2506.14084', 'abstract': 'Retrieval-Augmented Generation (RAG) addresses limitations of large language models (LLMs) by leveraging a vector database to provide more accurate and up-to-date information. When a user submits a query, RAG executes a vector search to find relevant documents, which are then used to generate a response. However, ensuring the relevance of retrieved documents with a query would be a big challenge. To address this, a secondary model, known as a relevant grader, can be served to verify its relevance. To reduce computational requirements of a relevant grader, a lightweight small language model is preferred. In this work, we finetuned llama-3.2-1b as a relevant grader and achieved a significant increase in precision from 0.1301 to 0.7750. Its precision is comparable to that of llama-3.1-70b. Our code is available at this https URL.', 'abstract_zh': '检索增强生成（RAG）通过利用向量数据库来提供更准确和及时的信息，以克服大规模语言模型（LLMs）的限制。当用户提交查询时，RAG 执行向量搜索以找到相关文档，这些文档随后用于生成响应。然而，确保检索到的文档与查询的相关性将是一个重大挑战。为此，可以部署一个次要模型，即相关性评分器，来验证其相关性。为了减少相关性评分器的计算需求，一个轻量级的小型语言模型更为优选。在此项工作中，我们对 llava-3.2-1b 进行了微调作为相关性评分器，并将精确度从 0.1301 显著提高到 0.7750，其精确度与 llava-3.1-70b 相当。我们的代码可在以下链接获取。', 'title_zh': '轻量级相关性评分器在RAG中'}
{'arxiv_id': 'arXiv:2506.14079', 'title': 'FormGym: Doing Paperwork with Agents', 'authors': 'Matthew Toles, Rattandeep Singh, Isaac Song Zhou Yu', 'link': 'https://arxiv.org/abs/2506.14079', 'abstract': 'Completing paperwork is a challenging and time-consuming problem. Form filling is especially challenging in the pure-image domain without access to OCR, typeset PDF text, or a DOM. For computer agents, it requires multiple abilities, including multi-modal understanding, information retrieval, and tool-use. We present a novel form-filling benchmark consisting of 432 fields spread across 55 documents and 3 tasks, requiring knowledge of 236 features per user. We find that baseline VLAs achieve less than 1% accuracy in most cases, primarily due to poor localization ability. GUI agents also struggle, scoring between 10.6-68.0% despite high cost and latency. Therefore, we also contribute FieldFinder, a tool to assist LLMs in identifying where to place text on a form. With FieldFinder, all models achieve equal or better performance in all six study conditions, with a maximum increase from 2% to 56%.', 'abstract_zh': '填写表格是一项具有挑战性和耗时的问题。在无OCR、无排版PDF文字或无DOM访问权限的纯图像域中，表格填写尤为具有挑战性。对于计算机代理而言，这需要多种能力，包括跨模态理解、信息检索和工具使用。我们提出了一种新型表格填写基准，包含55份文档中的432个字段和3项任务，每个用户需要掌握236个特征。我们发现，基准的多模态视觉语言模型在大多数情况下准确率低于1%，主要原因是定位能力较差。GUI代理也面临挑战，尽管成本和延迟较高，准确率也只能在10.6%-68.0%之间。因此，我们还贡献了FieldFinder工具，以帮助LLM识别人表格填写的文本位置。使用FieldFinder后，所有模型在所有六种研究条件下均实现了同等或更好的性能，性能提升最大可达56%。', 'title_zh': 'FormGym: 用代理进行表格处理'}
{'arxiv_id': 'arXiv:2506.14070', 'title': "Into the Unknown: Applying Inductive Spatial-Semantic Location Embeddings for Predicting Individuals' Mobility Beyond Visited Places", 'authors': 'Xinglei Wang, Tao Cheng, Stephen Law, Zichao Zeng, Ilya Ilyankou, Junyuan Liu, Lu Yin, Weiming Huang, Natchapon Jongwiriyanurak', 'link': 'https://arxiv.org/abs/2506.14070', 'abstract': "Predicting individuals' next locations is a core task in human mobility modelling, with wide-ranging implications for urban planning, transportation, public policy and personalised mobility services. Traditional approaches largely depend on location embeddings learned from historical mobility patterns, limiting their ability to encode explicit spatial information, integrate rich urban semantic context, and accommodate previously unseen locations. To address these challenges, we explore the application of CaLLiPer -- a multimodal representation learning framework that fuses spatial coordinates and semantic features of points of interest through contrastive learning -- for location embedding in individual mobility prediction. CaLLiPer's embeddings are spatially explicit, semantically enriched, and inductive by design, enabling robust prediction performance even in scenarios involving emerging locations. Through extensive experiments on four public mobility datasets under both conventional and inductive settings, we demonstrate that CaLLiPer consistently outperforms strong baselines, particularly excelling in inductive scenarios. Our findings highlight the potential of multimodal, inductive location embeddings to advance the capabilities of human mobility prediction systems. We also release the code and data (this https URL) to foster reproducibility and future research.", 'abstract_zh': '预测个体的下一个位置是人类移动模型中的核心任务，对城市规划、交通、公共政策和个人化移动服务具有广泛的影响。传统方法主要依赖于从历史移动模式中学习的位置嵌入，这限制了它们对显性空间信息的编码能力、对丰富城市语义上下文的整合能力以及处理未见过的位置的能力。为了解决这些挑战，我们探索了CaLLiPer——一种通过对比学习融合兴趣点的空间坐标和语义特征的多模态表示学习框架——在个体移动预测中的应用。CaLLiPer的嵌入具有显性空间性质、富含语义信息，并且具有归纳性设计，即使在新兴位置场景中也能实现稳健的预测性能。通过对四个公开的移动数据集在传统和归纳设置下的广泛实验，我们证明了CaLLiPer在各种情况下都优于强基线，尤其在归纳场景中表现尤为出色。我们的研究结果突显了多模态、归纳性位置嵌入对提升人类移动预测系统能力的潜力。我们还发布了代码和数据（参见链接），以促进可重复性和未来研究。', 'title_zh': '未知之旅：应用归纳空间语义位置嵌入预测个体在访问地点之外的 mobility 行为'}
{'arxiv_id': 'arXiv:2506.14045', 'title': 'Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning', 'authors': 'Martin Klissarov, Akhil Bagaria, Ziyan Luo, George Konidaris, Doina Precup, Marlos C. Machado', 'link': 'https://arxiv.org/abs/2506.14045', 'abstract': 'Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours.', 'abstract_zh': '开发能够在复杂开放环境中超脱探索、规划和学习的智能体是人工智能领域的重大挑战。基于层级强化学习（HRL）通过发现和利用经验流中的时间结构提供了有希望的解决方案。HRL框架的强大吸引力导致了大量的相关文献试图发现有用的时间结构。然而，仍然不清楚如何定义良好的时间结构，以及在哪些问题中识别它会有帮助。本文旨在从决策制定的基本挑战的角度识别HRL的优势，并突出其对智能体性能权衡的影响。通过这些优势，我们概述了HRL中时间结构发现的不同方法家族，包括直接从在线经验学习，利用离线数据集，以及利用大规模语言模型。最后，我们强调了时间结构发现所面临的挑战以及特别适合此类努力的领域。', 'title_zh': '发现时间结构：层次强化学习综述'}
{'arxiv_id': 'arXiv:2506.13990', 'title': 'Machine Mirages: Defining the Undefined', 'authors': 'Hamidou Tembine', 'link': 'https://arxiv.org/abs/2506.13990', 'abstract': 'As multimodal machine intelligence systems started achieving average animal-level and average human-level fluency in many measurable tasks in processing images, language, and sound, they began to exhibit a new class of cognitive aberrations: machine mirages. These include delusion, illusion, confabulation, hallucination, misattribution error, semantic drift, semantic compression, exaggeration, causal inference failure, uncanny valley of perception, bluffing-patter-bullshitting, cognitive stereotypy, pragmatic misunderstanding, hypersignification, semantic reheating-warming, simulated authority effect, fallacious abductive leap, contextual drift, referential hallucination, semiotic Frankenstein effect, calibration failure, spurious correlation, bias amplification, concept drift sensitivity, misclassification under uncertainty, adversarial vulnerability, overfitting, prosodic misclassification, accent bias, turn boundary failure, semantic boundary confusion, noise overfitting, latency-induced decision drift, ambiguity collapse and other forms of error that mimic but do not replicate human or animal fallibility. This article presents some of the errors and argues that these failures must be explicitly defined and systematically assessed. Understanding machine mirages is essential not only for improving machine intelligence reliability but also for constructing a multiscale ethical, co-evolving intelligence ecosystem that respects the diverse forms of life, cognition, and expression it will inevitably touch.', 'abstract_zh': '多模态机器智能系统的机器 mirages 及其认知偏差：定义与评估', 'title_zh': '机器幻象：定义未定义项'}
{'arxiv_id': 'arXiv:2506.13983', 'title': 'SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine', 'authors': 'Adarsh Gupta, Bhabesh Mali, Chandan Karfa', 'link': 'https://arxiv.org/abs/2506.13983', 'abstract': 'Recent advancements in the field of reasoning using Large Language Models (LLMs) have created new possibilities for more complex and automatic Hardware Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the automatic generation of SVAs from industry-level specifications. The proposed framework utilizes a three-stage approach: Stage 1 consists of multi-modal Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine (MCTSr) algorithm for automatic reasoning about SVAs for each signal, and finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA assertions for each signal. The results demonstrated that our framework, SANGAM, can generate a robust set of SVAs, performing better in the evaluation process in comparison to the recent methods.', 'abstract_zh': 'Recent Advancements in Hardware Assertion Generation Using Large Language Models: Introduction of SANGAM, a SystemVerilog Assertion Generation Framework Guided by LLMs', 'title_zh': 'SANGAM: 通过蒙特卡洛树自我完善生成SystemVerilog断言'}
{'arxiv_id': 'arXiv:2506.13980', 'title': 'ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users', 'authors': 'Shahaf David, Yair Meidan, Ido Hersko, Daniel Varnovitzky, Dudu Mimran, Yuval Elovici, Asaf Shabtai', 'link': 'https://arxiv.org/abs/2506.13980', 'abstract': "Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research.", 'abstract_zh': '尽管在对话AI方面取得了显著进展，但由大型语言模型（LLM）驱动的聊天机器人在根据个人用户特征（如技术专长、学习风格和沟通偏好）个性化响应时仍然存在困难。这种缺乏个性化在IT/网络安全（ITSec）等专门知识密集型领域尤为成问题，因为用户的知识水平差异很大。现有的聊天机器人个性化方法主要依赖于静态用户类别或显式自我报告的信息，限制了它们对用户 proficiency 不断变化的感知的适应能力。在本文中，我们提出了一种名为ProfiLLM的新框架，通过聊天机器人交互进行隐式和动态用户画像。该框架包括一个可应用于多种领域的分类体系，以及一种基于LLM的用户画像方法。为了证明ProfiLLM的有效性，我们将其应用于ITSec领域，通过故障排除交互来推断聊天机器人用户的技术水平。具体而言，我们开发了ProfiLLM[ITSec]，即ProfiLLM的ITSec特异化变体，并在263个合成用户中1,760条类人类聊天机器人对话上评估了其性能。结果显示，ProfiLLM[ITSec]能够在单次提示后迅速而准确地推断出ITSec画像，实际得分与预测得分之间的差距减少了55-65%，随后小幅波动并进一步细化。除了评估我们新的隐式和动态画像框架外，我们还提出了一种基于LLM的人设模拟方法、一个ITSec专长的结构化分类体系、我们的代码库以及一个聊天机器人交互数据集，以支持未来的研究。', 'title_zh': 'ProfiLLM：基于LLM的聊天机器人用户隐性 profiling 框架'}
{'arxiv_id': 'arXiv:2506.13920', 'title': 'Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for Explainable Disease Risk Prediction', 'authors': 'Mbithe Nzomo, Deshendran Moodley', 'link': 'https://arxiv.org/abs/2506.13920', 'abstract': 'Multimodal electronic health record (EHR) data is useful for disease risk prediction based on medical domain knowledge. However, general medical knowledge must be adapted to specific healthcare settings and patient populations to achieve practical clinical use. Additionally, risk prediction systems must handle uncertainty from incomplete data and non-deterministic health outcomes while remaining explainable. These challenges can be alleviated by the integration of knowledge graphs (KGs) and Bayesian networks (BNs). We present a novel approach for constructing BNs from ontology-based KGs and multimodal EHR data for explainable disease risk prediction. Through an application use case of atrial fibrillation and real-world EHR data, we demonstrate that the approach balances generalised medical knowledge with patient-specific context, effectively handles uncertainty, is highly explainable, and achieves good predictive performance.', 'abstract_zh': '基于本体的知识图谱和贝叶斯网络构建可解释的多模态电子健康记录疾病风险预测方法', 'title_zh': '知识图谱与贝叶斯网络集成：一种可解释的疾病风险预测混合方法'}
{'arxiv_id': 'arXiv:2506.13917', 'title': 'Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features', 'authors': 'Miguel A. Lago, Ghada Zamzmi, Brandon Eich, Jana G. Delfino', 'link': 'https://arxiv.org/abs/2506.13917', 'abstract': 'Explainability features are intended to provide insight into the internal mechanisms of an AI device, but there is a lack of evaluation techniques for assessing the quality of provided explanations. We propose a framework to assess and report explainable AI features. Our evaluation framework for AI explainability is based on four criteria: 1) Consistency quantifies the variability of explanations to similar inputs, 2) Plausibility estimates how close the explanation is to the ground truth, 3) Fidelity assesses the alignment between the explanation and the model internal mechanisms, and 4) Usefulness evaluates the impact on task performance of the explanation. Finally, we developed a scorecard for AI explainability methods that serves as a complete description and evaluation to accompany this type of algorithm. We describe these four criteria and give examples on how they can be evaluated. As a case study, we use Ablation CAM and Eigen CAM to illustrate the evaluation of explanation heatmaps on the detection of breast lesions on synthetic mammographies. The first three criteria are evaluated for clinically-relevant scenarios. Our proposed framework establishes criteria through which the quality of explanations provided by AI models can be evaluated. We intend for our framework to spark a dialogue regarding the value provided by explainability features and help improve the development and evaluation of AI-based medical devices.', 'abstract_zh': '可解释性特征旨在提供对AI设备内部机制的洞察，但缺乏评估所提供解释质量的技术。我们提出了一种评估和报告可解释AI特征的框架。我们的AI可解释性评估框架基于四个标准：1) 一致性量化对相似输入的解释变异性；2) 可信度估计解释与真实情况的接近程度；3) 忠实度评估解释与模型内部机制的对齐程度；4) 实用性评估解释对任务性能的影响。最后，我们开发了一种AI可解释性方法评分卡，作为此类算法的完整描述和评估。我们描述了这四个标准，并给出如何评估它们的例子。作为案例研究，我们使用Ablation CAM和Eigen CAM来说明在合成乳腺X线摄影中检测乳腺病变时解释热图的评估。前三项标准在临床相关的场景中进行评估。我们提出的框架建立了评估AI模型提供的解释质量的标准。我们希望我们的框架能够激发关于解释性特征价值的对话，并帮助改善基于AI的医疗设备的开发和评估。', 'title_zh': '评估可解释性：一个系统评估和报告可解释AI特征的框架'}
{'arxiv_id': 'arXiv:2506.13841', 'title': 'LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning', 'authors': 'Miho Koda, Yu Zheng, Ruixian Ma, Mingyang Sun, Devesh Pansare, Fabio Duarte, Paolo Santi', 'link': 'https://arxiv.org/abs/2506.13841', 'abstract': "Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at this https URL.", 'abstract_zh': '近年来，特别是通过强化后训练增强的大语言模型（LLMs）在推理能力方面取得了显著进步，如OpenAI o1和DeepSeek-R1模型所示。然而，这些能力主要在数学问题解决和代码生成等领域进行了评估，留下的问题是这些推理技能是否能泛化到复杂的现实世界场景中。在本文中，我们介绍了LocationReasoner基准，用于评估大语言模型在现实世界站点选址场景中的推理能力，该场景要求模型通过处理多样且复杂的时空、环境和物流约束来识别可行位置。该基准包含超过300个精心设计的、难度不一的查询，并配备了一个沙箱环境和基于约束的地理位置搜索的内部工具。 extensive评估显示，最先进的推理模型在现实世界场景中的表现并未显著优于其非推理前身，即使是最新的OpenAI o4模型也在30%的站点选址任务中失败。此外，像ReAct和Reflexion这样的代理策略由于过度推理，往往表现不如直接代码生成提示。鉴于大语言模型在整体和非线性推理方面的关键局限性，我们发布了LocationReasoner，以促进开发能够在现实世界决策任务中进行稳健且基于事实推理的大语言模型和代理。我们的基准代码和数据可在以下网址获取。', 'title_zh': 'LocationReasoner：评估大型语言模型在真实世界选址推理中的性能'}
{'arxiv_id': 'arXiv:2506.13825', 'title': 'The Reflexive Integrated Information Unit: A Differentiable Primitive for Artificial Consciousness', 'authors': "Gnankan Landry Regis N'guessan, Issa Karambal", 'link': 'https://arxiv.org/abs/2506.13825', 'abstract': 'Research on artificial consciousness lacks the equivalent of the perceptron: a small, trainable module that can be copied, benchmarked, and iteratively improved. We introduce the Reflexive Integrated Information Unit (RIIU), a recurrent cell that augments its hidden state $h$ with two additional vectors: (i) a meta-state $\\mu$ that records the cell\'s own causal footprint, and (ii) a broadcast buffer $B$ that exposes that footprint to the rest of the network. A sliding-window covariance and a differentiable Auto-$\\Phi$ surrogate let each RIIU maximize local information integration online. We prove that RIIUs (1) are end-to-end differentiable, (2) compose additively, and (3) perform $\\Phi$-monotone plasticity under gradient ascent. In an eight-way Grid-world, a four-layer RIIU agent restores $>90\\%$ reward within 13 steps after actuator failure, twice as fast as a parameter-matched GRU, while maintaining a non-zero Auto-$\\Phi$ signal. By shrinking "consciousness-like" computation down to unit scale, RIIUs turn a philosophical debate into an empirical mathematical problem.', 'abstract_zh': '人工意识的研究缺乏相当于感知器的小型可训练模块，该模块可以复制、基准测试并迭代改进。我们引入了反射性综合信息单元（RIIU），这是一种递归单元，其隐藏状态 $h$ 增加以太状态 $\\mu$ 和广播缓冲区 $B$ 两个额外向量：（i）太状态 $\\mu$ 记录单元自身的因果足迹，（ii）广播缓冲区 $B$ 将其足迹暴露给网络的其余部分。滑动窗口协方差和可微分的 Auto-$\\Phi$ 替代理使每个 RIIU 在在线本地信息整合方面最大化。我们证明了RIIUs：（1）端到端可微分，（2）可叠加组合，（3）在梯度上升下表现出 $\\Phi$-单调可塑性。在一个八方向网格世界中，四层RIIU代理在执行器故障后13步内恢复了>90%的奖励，这一速度是参数匹配的GRU的两倍，同时保持了非零的Auto-$\\Phi$ 信号。通过将“类似意识”的计算缩小到单位规模，RIIUs 将一场哲学辩论转变为了实证的数学问题。', 'title_zh': '反射整合信息单元：人工意识的可微基础单元'}
{'arxiv_id': 'arXiv:2506.13810', 'title': 'Bridging Pattern-Aware Complexity with NP-Hard Optimization: A Unifying Framework and Empirical Study', 'authors': 'Olivier Saidi', 'link': 'https://arxiv.org/abs/2506.13810', 'abstract': 'NP hard optimization problems like the Traveling Salesman Problem (TSP) defy efficient solutions in the worst case, yet real-world instances often exhibit exploitable patterns. We propose a novel patternaware complexity framework that quantifies and leverages structural regularities e.g., clustering, symmetry to reduce effective computational complexity across domains, including financial forecasting and LLM optimization. With rigorous definitions, theorems, and a meta learning driven solver pipeline, we introduce metrics like Pattern Utilization Efficiency (PUE) and achieve up to 79 percent solution quality gains in TSP benchmarks (22 to 2392 cities). Distinct from theoretical NP hardness, our approach offers a unified, practical lens for pattern-driven efficiency.', 'abstract_zh': 'NP难优化问题如旅行商问题(TSP)在最坏情况下难以高效求解，但实际问题实例往往具有可利用的模式。我们提出了一种新颖的模式感知复杂性框架，该框架量化并利用了结构上的规律性，例如聚类、对称性，以降低跨不同领域的有效计算复杂性，包括金融预测和大语言模型优化。通过严格的定义、定理以及基于元学习的求解器管道，我们引入了模式利用效率(PUE)等度量标准，在TSP基准测试中实现了高达79%的解的质量提升（22至2392个城市）。不同于理论上的NP难性，我们的方法提供了一种统一的、实用的模式驱动效率视角。', 'title_zh': '基于模式感知复杂性的NP难优化问题统一框架与实验研究'}
{'arxiv_id': 'arXiv:2506.13803', 'title': 'Causality in the human niche: lessons for machine learning', 'authors': 'Richard D. Lange, Konrad P. Kording', 'link': 'https://arxiv.org/abs/2506.13803', 'abstract': "Humans interpret the world around them in terms of cause and effect and communicate their understanding of the world to each other in causal terms. These causal aspects of human cognition are thought to underlie humans' ability to generalize and learn efficiently in new domains, an area where current machine learning systems are weak. Building human-like causal competency into machine learning systems may facilitate the construction of effective and interpretable AI. Indeed, the machine learning community has been importing ideas on causality formalized by the Structural Causal Model (SCM) framework, which provides a rigorous formal language for many aspects of causality and has led to significant advances. However, the SCM framework fails to capture some salient aspects of human causal cognition and has likewise not yet led to advances in machine learning in certain critical areas where humans excel. We contend that the problem of causality in the ``human niche'' -- for a social, autonomous, and goal-driven agent sensing and acting in the world in which humans live -- is quite different from the kind of causality captured by SCMs. For example, everyday objects come in similar types that have similar causal properties, and so humans readily generalize knowledge of one type of object (cups) to another related type (bowls) by drawing causal analogies between objects with similar properties, but such analogies are at best awkward to express in SCMs. We explore how such causal capabilities are adaptive in, and motivated by, the human niche. By better appreciating properties of human causal cognition and, crucially, how those properties are adaptive in the niche in which humans live, we hope that future work at the intersection of machine learning and causality will leverage more human-like inductive biases to create more capable, controllable, and interpretable systems.", 'abstract_zh': '人类在关于因果性的认知基础上理解并以因果性术语交流周围的世界，这种认知上的因果性被认为是人类在新领域中泛化和高效学习的能力基础，而当前的机器学习系统在这方面的表现较弱。将人类类似的因果能力构建到机器学习系统中可能有助于构建更有效且可解释的AI。实际上，机器学习社区正在引入通过结构因果模型（SCM）框架正式化的因果性理念，SCM框架为许多因果性方面提供了严谨的形式语言，并取得了显著进展。然而，SCM框架未能捕捉人类因果认知的一些显著方面，也尚未在某些人类表现优异的关键领域推动机器学习的发展。我们认为，在“人类生态位”——一个由感知和行动于其中的社会、自主且目标驱动的智能体所感知和作用的世界——中的因果问题与SCM所捕捉的因果性类型是不同的。例如，日常物体有相似的类型且具有相似的因果属性，因此人类可以很容易地通过在具有相似属性的物体之间构造因果类比来从一种类型的知识（杯子）推广到另一种相关类型的知识（碗），但这类类比在SCM中表达起来最多也显得笨拙。我们探讨了在人类生态位中此类因果能力的适应性和动机。通过更好地理解和感知人类因果认知的属性，特别是这些属性在人类所生活的生态位中的适应性，我们希望未来结合机器学习和因果性的研究能够利用更为人类类似的归纳偏置来创造出更强大、可控且可解释的系统。', 'title_zh': '人类生态位中的因果关系：机器学习的启示'}
{'arxiv_id': 'arXiv:2506.13799', 'title': 'Feedforward Ordering in Neural Connectomes via Feedback Arc Minimization', 'authors': 'Soroush Vahidi', 'link': 'https://arxiv.org/abs/2506.13799', 'abstract': 'We present a suite of scalable algorithms for minimizing feedback arcs in large-scale weighted directed graphs, with the goal of revealing biologically meaningful feedforward structure in neural connectomes. Using the FlyWire Connectome Challenge dataset, we demonstrate the effectiveness of our ranking strategies in maximizing the total weight of forward-pointing edges. Our methods integrate greedy heuristics, gain-aware local refinements, and global structural analysis based on strongly connected components. Experiments show that our best solution improves the forward edge weight over previous top-performing methods. All algorithms are implemented efficiently in Python and validated using cloud-based execution on Google Colab Pro+.', 'abstract_zh': '我们提出了一套可扩展的算法，用于在大规模加权有向图中最小化反馈弧，旨在揭示神经连接组中的生物意义前馈结构。通过FlyWire连接组挑战数据集，我们展示了我们的排名策略在最大化前向指针边的总权重方面的有效性。我们的方法结合了贪心启发式、基于强连通分量的获益感知局部优化以及全局结构分析。实验表明，我们提出的方法在前向边权重方面优于之前表现最佳的方法。所有算法均高效地用Python实现，并在基于云的Google Colab Pro+上进行了验证。', 'title_zh': '神经连接组中前向排序的反馈弧最小化方法'}
{'arxiv_id': 'arXiv:2506.13795', 'title': 'BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection', 'authors': 'Boshen Shi, Yongqing Wang, Fangda Guo, Jiangli Shao, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2506.13795', 'abstract': "Transferring extensive knowledge from relevant social networks has emerged as a promising solution to overcome label scarcity in detecting social bots and other anomalies with GNN-based models. However, effective transfer faces two critical challenges. Firstly, the network heterophily problem, which is caused by bots hiding malicious behaviors via indiscriminately interacting with human users, hinders the model's ability to learn sufficient and accurate bot-related knowledge from source domains. Secondly, single-source transfer might lead to inferior and unstable results, as the source network may embody weak relevance to the task and provide limited knowledge. To address these challenges, we explore multiple source domains and propose a multi-source graph domain adaptation model named \\textit{BotTrans}. We initially leverage the labeling knowledge shared across multiple source networks to establish a cross-source-domain topology with increased network homophily. We then aggregate cross-domain neighbor information to enhance the discriminability of source node embeddings. Subsequently, we integrate the relevance between each source-target pair with model optimization, which facilitates knowledge transfer from source networks that are more relevant to the detection task. Additionally, we propose a refinement strategy to improve detection performance by utilizing semantic knowledge within the target domain. Extensive experiments on real-world datasets demonstrate that \\textit{BotTrans} outperforms the existing state-of-the-art methods, revealing its efficacy in leveraging multi-source knowledge when the target detection task is unlabeled.", 'abstract_zh': '从相关社交网络中转移 extensive 知识以克服基于 GNN 的模型在检测社交机器人和其他异常时面临的标签稀缺问题，已经成为了有前途的解决方案。然而，有效的转移面临着两个关键挑战。首先，由于机器人通过无选择地与用户交互来隐藏恶意行为而引发的网络非同质性问题，阻碍了模型从源领域学习足够的准确的机器人相关知识的能力。其次，单源转移可能导致较差且不稳定的结果，因为源网络可能与任务关联较弱并提供有限的知识。为解决这些挑战，我们探索了多个源领域，并提出了一种名为 \\textit{BotTrans} 的多源图域适应模型。我们最初利用跨多个源网络共享的标签知识，建立具有增加网络同质性的跨源域拓扑结构。然后，我们聚合跨域邻居信息以增强源节点表示的区分性。接着，我们将每对源-目标的相关性与模型优化集成，从而促进更多相关于检测任务的源网络的知识转移。此外，我们提出了一种改进策略，通过利用目标域内的语义知识来提升检测性能。在实际数据集上的大量实验表明，\\textit{BotTrans} 在目标检测任务未标记的情况下，能够更好地利用多源知识，优于现有的先进方法。', 'title_zh': 'BotTrans：多源图域适应的社会机器人检测方法'}
{'arxiv_id': 'arXiv:2506.13793', 'title': 'Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection', 'authors': 'Zongxian Yang, Jiayu Qian, Zegao Peng, Haoyu Zhang, Zhi-An Huang', 'link': 'https://arxiv.org/abs/2506.13793', 'abstract': 'Large reasoning models have recently made significant strides in mathematical and code reasoning, yet their success has not transferred smoothly to the medical domain. While multiple factors contribute to this disparity, a critical issue is the inadequate focus on the quality of intermediate reflection steps, which is particularly crucial in high-stakes medical scenarios. To address this challenge, we propose Med-REFL, a \\underline{\\textbf{Med}}ical \\underline{\\textbf{R}}easoning \\underline{\\textbf{E}}nhancement via self-corrected \\underline{\\textbf{F}}ine-grained ref\\underline{\\textbf{L}}ection. Our method leverages a tree-of-thought approach to decompose medical questions into fine-grained reasoning paths, quantitatively evaluating each step and its subsequent reflections. These assessments enable automatic construction of direct preference optimization data, reducing reliance on expensive expert annotations while guiding models to identify and correct reasoning errors. Experimental results on the MedQA-USMLE benchmark demonstrate Med-REFL achieves consistent improvements, with average gains up to 4.11\\%. Notably, it further boosts the state-of-the-art performance of 7B/8B models by an additional 4.13\\%. Furthermore, Med-REFL exhibits strong generalization capabilities and robustness across several challenging medical question-answering datasets. Our work illustrates that prioritizing reflection quality leads to more accurate and trustworthy reasoning in medical AI applications. Checkpoints, code, and data can be found \\href{this https URL}{here}.', 'abstract_zh': '大型推理模型在数学和代码推理领域取得了显著进展，但在医疗领域的成功未能顺利转移。尽管有多种因素导致这种差异，但关键问题是未能充分关注中间推理步骤的质量，尤其是在高风险医疗场景中这一点尤为重要。为应对这一挑战，我们提出了一种Med-REFL方法，即通过自我校正的细粒度反思增强医学推理。该方法利用树状思维方法将医学问题分解为细粒度的推理路径，定量评估每一步及后续反思。这些评估能够自动构建直接偏好优化数据，减少对昂贵专家注释的依赖，同时指导模型识别并纠正推理错误。Med-REFL在MedQA-USMLE基准上的实验结果表明，该方法实现了一致的改进，平均增益高达4.11%。此外，它还进一步提升了7B/8B模型的最新性能，增益达4.13%。此外，Med-REFL在多个具有挑战性的医学问答数据集中表现出强大的泛化能力和鲁棒性。我们的工作表明，在医学AI应用中优先考虑反思质量可以提高推理的准确性与可靠性。更多信息和资源，请访问此处。', 'title_zh': 'Med-REFL: 医学推理增强 via 自我修正细粒度反思'}
{'arxiv_id': 'arXiv:2506.13792', 'title': 'ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \\& a ML Ensemble on Longitudinal Identity Resolution', 'authors': 'Gonçalo Hora de Carvalho, Lazar S. Popov, Sander Kaatee, Kristinn R. Thórisson, Tangrui Li, Pétur Húni Björnsson, Jilles S. Dibangoye', 'link': 'https://arxiv.org/abs/2506.13792', 'abstract': 'We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.', 'abstract_zh': 'ICE-ID：一种用于历史身份解析的新型基准数据集', 'title_zh': 'ICE-ID: 一种新型历史人口普查数据基准，比较NARS与LLMs及 longitudinal身份解析的机器学习集成方法'}
{'arxiv_id': 'arXiv:2506.13790', 'title': 'The NordDRG AI Benchmark for Large Language Models', 'authors': 'Tapio Pitkäranta', 'link': 'https://arxiv.org/abs/2506.13790', 'abstract': "Large language models (LLMs) are already being piloted for clinical coding and decision support. However, until now, no open benchmark has targeted the hospital funding layer where Diagnosis-Related Groups (DRG) determine reimbursement across many countries. We release NordDRG-AI-Benchmark, the first public test-bed that captures a complete DRG rule set and evaluates an LLM's ability to reason over multilingual diagnosis, procedure, and tariff logic.\nThe benchmark bundles three classes of artefacts: (i) definition tables with 20 interlinked tables covering DRG logic, ICD and NCSP codes, age/sex splits, and country flags; (ii) expert manuals and changelog templates describing real governance workflows; and (iii) a prompt pack of 14 CaseMix tasks that span code lookup, cross-table inference, multilingual terminology, and quality-assurance audits.\nAll artefacts are available at: this https URL\nA baseline demonstration shows that five state-of-the-art LLMs perform very differently on the nine automatically verifiable tasks: o3 (OpenAI) scores 9 out of 9, GPT-4o and o4-mini-high score 7 out of 9, while Gemini 2.5 Pro and Gemini 2.5 Flash solve only 5 out of 9 and 3 out of 9, respectively. These results confirm that NordDRG-AI-Benchmark highlights domain-specific strengths and weaknesses that remain hidden in generic LLM benchmarks, offering a reproducible baseline for research on trustworthy automation in hospital funding.", 'abstract_zh': '大型语言模型（LLMs）已经在临床编码和决策支持中进行了试点。然而，直到现在，还没有公开基准针对医院资金层，其中诊断相关组（DRG）决定许多国家的报销标准。我们发布了NordDRG-AI-Benchmark，这是首个包含完整DRG规则集并评估LLM在多语言诊断、程序和费率逻辑推理能力的公开测试平台。', 'title_zh': 'NordDRG AI基准测试：大型语言模型'}
{'arxiv_id': 'arXiv:2506.13776', 'title': 'Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations', 'authors': 'Kevin L. Wei, Patricia Paskov, Sunishchal Dev, Michael J. Byun, Anka Reuel, Xavier Roberts-Gaal, Rachel Calcott, Evie Coxon, Chinmay Deshpande', 'link': 'https://arxiv.org/abs/2506.13776', 'abstract': 'In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve "super-human" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: this https URL', 'abstract_zh': '在这一立场声明中，我们argue认为，在基础模型评估中的人类基线必须更加严格和透明，以促进人类与AI表现的有意义比较，并为此提出了建议和报告清单。人类表现基线对于机器学习社区、下游用户和决策者理解AI评估至关重要。模型常被声称实现“超人”性能，但现有的基线方法既不够严格也不够详细，无法可靠地衡量和评估性能差异。基于对度量理论和AI评估文献的元评审，我们提出了一种框架，涵盖了设计、执行和报告人类基线的建议。我们将这些建议综合成一份清单，用于系统性地审查115项基础模型评估中的基线（研究），从而识别现有基线方法的不足；这份清单还可以帮助研究人员开展人类基线评估并报告结果。我们希望我们的工作能够推进更严谨的AI评估实践，更好地服务于研究社区和决策者。数据可在以下链接获得：this https URL。', 'title_zh': '严格和透明的人类基线推荐与报告检查表在模型评估中的应用'}
{'arxiv_id': 'arXiv:2506.13774', 'title': 'Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values', 'authors': 'Nell Watson, Ahmed Amer, Evan Harris, Preeti Ravindra, Shujun Zhang', 'link': 'https://arxiv.org/abs/2506.13774', 'abstract': 'Agentic AI systems, possessing capabilities for autonomous planning and action, exhibit immense potential across diverse domains. However, their practical deployment is significantly hampered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the intricate task of providing deep, personalized contextual information without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a \'superego\' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected "Creed Constitutions"-encapsulating diverse rule sets-with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface (this http URL) with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs, achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm\'s harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at this https URL.', 'abstract_zh': '具有自主规划和行动能力的代理人工智能系统在多个领域展现出巨大的潜力。然而，它们的实际部署受到与多样化的人类价值观、复杂的安全需求以及特定合规性要求对齐的挑战。现有的对齐方法在提供深度个性化上下文信息时常常失败，同时又不引起虚构行为或操作效率低下。本文介绍了一种新的解决方案：一个“超我”代理，设计为代理人工智能的个性化监督机制。该系统通过引用用户选择的“信条宪法”（包含多种规则集）来动态指导AI规划，并可根据不可谈判的价值观调整遵守水平。实时合规监督者在执行前验证计划是否符合这些宪法和通用伦理底线。我们呈现了一个功能系统，包括一个演示界面（链接：this http URL）以及一个原型宪法共享门户，并通过模型上下文协议（MCP）成功与第三方模型集成。全面基准评估（HarmBench，AgentHarm）表明，我们的超我代理大幅减少了有害输出，实现了高达98.3%的有害评分减少，并且在有害行为集上的拒绝率接近完美（例如，与Claude Sonnet 4结合时，在AgentHarm的有害行为集上的拒绝率为100%）等领先的大规模语言模型（如Gemini 2.5 Flash和GPT-4o）。这种方法显著简化了个性化AI对齐，使代理系统更可靠地适应个体和文化背景，同时还能显著提高安全性。有关这项研究的概述和示例，请参见链接：this https URL。', 'title_zh': '个性化宪法一致的代理超我：与多样人类价值观对齐的AI安全行为'}
{'arxiv_id': 'arXiv:2506.13773', 'title': 'Representing Time-Continuous Behavior of Cyber-Physical Systems in Knowledge Graphs', 'authors': 'Milapji Singh Gill, Tom Jeleniewski, Felix Gehlhoff, Alexander Fay', 'link': 'https://arxiv.org/abs/2506.13773', 'abstract': "Time-continuous dynamic models are essential for various Cyber-Physical System (CPS) applications. To ensure effective usability in different lifecycle phases, such behavioral information in the form of differential equations must be contextualized and integrated with further CPS information. While knowledge graphs provide a formal description and structuring mechanism for this task, there is a lack of reusable ontological artifacts and methods to reduce manual instantiation effort. Hence, this contribution introduces two artifacts: Firstly, a modular semantic model based on standards is introduced to represent differential equations directly within knowledge graphs and to enrich them semantically. Secondly, a method for efficient knowledge graph generation is presented. A validation of these artifacts was conducted in the domain of aviation maintenance. Results show that differential equations of a complex Electro-Hydraulic Servoactuator can be formally represented in a knowledge graph and be contextualized with other lifecycle data, proving the artifacts' practical applicability.", 'abstract_zh': '时间连续动态模型对于各种网络物理系统（CPS）应用至关重要。为了在不同生命周期阶段确保有效的可用性，这种以微分方程形式的行为信息必须进行上下文化和与进一步的CPS信息集成。虽然知识图谱提供了进行这一任务的正式描述和结构化机制，但仍缺乏可重用的本体 artefact 和减少手动实例化努力的方法。因此，本贡献介绍了两种 artefact：首先，基于标准的模块化语义模型被引入，用于直接在知识图谱中表示微分方程并对其进行语义丰富化；其次，提出了一种高效的知识图谱生成方法。这些 artefact 在航空维修领域进行了验证，结果显示，复杂的电气液压伺服作动器的微分方程可以形式化地表示在知识图谱中，并与其他生命周期数据进行上下文化整合，证明了这些 artefact 的实际适用性。', 'title_zh': '在知识图谱中表示网络物理系统的时间连续行为'}
{'arxiv_id': 'arXiv:2506.13768', 'title': "'Memory States' from Almost Nothing: Representing and Computing in a Non-associative Algebra", 'authors': 'Stefan Reimann', 'link': 'https://arxiv.org/abs/2506.13768', 'abstract': 'This note presents a non-associative algebraic framework for the representation and computation of information items in high-dimensional space. This framework is consistent with the principles of spatial computing and with the empirical findings in cognitive science about memory. Computations are performed through a process of multiplication-like binding and non-associative interference-like bundling. Models that rely on associative bundling typically lose order information, which necessitates the use of auxiliary order structures, such as position markers, to represent sequential information that is important for cognitive tasks. In contrast, the non-associative bundling proposed allows the construction of sparse representations of arbitrarily long sequences that maintain their temporal structure across arbitrary lengths. In this operation, noise is a constituent element of the representation of order information, rather than a means of obscuring it. The non-associative nature of the proposed framework results in the representation of a single sequence by two distinct states. The L-state, generated through left-associative bundling, continuously updates and emphasises a recency effect, while the R-state, formed through right-associative bundling, encodes finite sequences or chunks, capturing a primacy effect. The construction of these states may be associated with activity in the prefrontal cortex in relation to short-term memory and hippocampal encoding in long-term memory, respectively. The accuracy of retrieval is contingent upon a decision-making process that is based on the mutual information between the memory states and the cue. The model is able to replicate the Serial Position Curve, which reflects the empirical recency and primacy effects observed in cognitive experiments.', 'abstract_zh': '一种非结合代数框架下的高维空间中信息项的表示与计算', 'title_zh': '几乎无信息中的记忆状态：在非联想代数中的表示与计算'}
{'arxiv_id': 'arXiv:2506.14767', 'title': 'A Variational Framework for Improving Naturalness in Generative Spoken Language Models', 'authors': 'Li-Wei Chen, Takuya Higuchi, Zakaria Aldeneh, Ahmed Hussen Abdelaziz, Alexander Rudnicky', 'link': 'https://arxiv.org/abs/2506.14767', 'abstract': 'The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at this https URL.', 'abstract_zh': '大型语言模型在文本处理中的成功激励了其在语音建模中的应用。然而，由于语音是连续且复杂的，通常被离散化以进行自回归建模。源自自监督模型的语音令牌（即语义令牌）通常关注语音的语言方面，但忽略了语调信息。因此，基于这些令牌训练的模型可能生成缺乏自然度的语音。现有方法尝试通过向语义令牌中添加音高特征来解决这一问题。但仅靠音高无法全面表示语副语言属性的范围，选择合适的特征需要精细的手动工程。为克服这一问题，我们提出了一种端到端的变分方法，能够自动学习编码这些连续的语音属性以增强语义令牌。该方法消除了手动提取和选择语副语言特征的需要，而且能够根据人类评定产生优选的语音继续。代码、样本和模型可以在以下链接获取。', 'title_zh': '一种变分框架，用于提高生成语音语言模型的自然度'}
{'arxiv_id': 'arXiv:2506.14761', 'title': 'From Bytes to Ideas: Language Modeling with Autoregressive U-Nets', 'authors': 'Mathurin Videau, Badr Youbi Idrissi, Alessandro Leite, Marc Schoenauer, Olivier Teytaud, David Lopez-Paz', 'link': 'https://arxiv.org/abs/2506.14761', 'abstract': 'Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.', 'abstract_zh': 'Tokenization Imposes Fixed Granularity on Input Text, While an Autoregressive U-Net Learns to Embed Its Own Tokens, Offering Multi-Scale Insights and Flexible Prediction Depths', 'title_zh': '从字节到理念：自回归U-网的语言建模'}
{'arxiv_id': 'arXiv:2506.14750', 'title': 'Exploring Speaker Diarization with Mixture of Experts', 'authors': 'Gaobin Yang, Maokui He, Shutong Niu, Ruoyu Wang, Hang Chen, Jun Du', 'link': 'https://arxiv.org/abs/2506.14750', 'abstract': 'In this paper, we propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with a sequence-to-sequence architecture. The system leverages a memory module to enhance speaker embeddings and employs a Seq2Seq framework to efficiently map acoustic features to speaker labels. Additionally, we explore the application of mixture of experts in speaker diarization, and introduce a Shared and Soft Mixture of Experts (SS-MoE) module to further mitigate model bias and enhance performance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE. Experiments on multiple complex acoustic datasets, including CHiME-6, DiPCo, Mixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in robustness and generalization. The proposed methods achieve state-of-the-art results, showcasing their effectiveness in challenging real-world scenarios.', 'abstract_zh': '基于记忆aware多说话人嵌入的序列到序列演讲者聚类系统（NSD-MS2S及其扩展模型NSD-MS2S-SSMoE）：探索专家混合在演讲者聚类中的应用', 'title_zh': '探索专家混合模型在演讲者分离中的应用'}
{'arxiv_id': 'arXiv:2506.14731', 'title': 'Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs', 'authors': 'Ring Team, Bin Hu, Cai Chen, Deng Zhao, Ding Liu, Dingnan Jin, Feng Zhu, Hao Dai, Hongzhi Luan, Jia Guo, Jiaming Liu, Jiewei Wu, Jun Mei, Jun Zhou, Junbo Zhao, Junwu Xiong, Kaihong Zhang, Kuan Xu, Lei Liang, Liang Jiang, Liangcheng Fu, Longfei Zheng, Qiang Gao, Qing Cui, Quan Wan, Shaomian Zheng, Shuaicheng Li, Tongkai Yang, Wang Ren, Xiaodong Yan, Xiaopei Wan, Xiaoyun Feng, Xin Zhao, Xinxing Yang, Xinyu Kong, Xuemin Yang, Yang Li, Yingting Wu, Yongkang Liu, Zhankai Xu, Zhenduo Zhang, Zhenglei Zhou, Zhenyu Huang, Zhiqiang Zhang, Zihao Wang, Zujie Wen', 'link': 'https://arxiv.org/abs/2506.14731', 'abstract': 'We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.', 'abstract_zh': 'Ring-lite：一种基于Mixture-of-Experts架构并通过强化学习优化的大规模语言模型，具备高效的稳健推理能力', 'title_zh': '环形精简：通过C3PO稳定增强学习实现的大规模可扩展推理'}
{'arxiv_id': 'arXiv:2506.14727', 'title': 'Casper: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models', 'authors': 'Huihan Liu, Rutav Shah, Shuijing Liu, Jack Pittenger, Mingyo Seo, Yuchen Cui, Yonatan Bisk, Roberto Martín-Martín, Yuke Zhu', 'link': 'https://arxiv.org/abs/2506.14727', 'abstract': 'Assistive teleoperation, where control is shared between a human and a robot, enables efficient and intuitive human-robot collaboration in diverse and unstructured environments. A central challenge in real-world assistive teleoperation is for the robot to infer a wide range of human intentions from user control inputs and to assist users with correct actions. Existing methods are either confined to simple, predefined scenarios or restricted to task-specific data distributions at training, limiting their support for real-world assistance. We introduce Casper, an assistive teleoperation system that leverages commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time intent inference and flexible skill execution. Casper incorporates an open-world perception module for a generalized understanding of novel objects and scenes, a VLM-powered intent inference mechanism that leverages commonsense reasoning to interpret snippets of teleoperated user input, and a skill library that expands the scope of prior assistive teleoperation systems to support diverse, long-horizon mobile manipulation tasks. Extensive empirical evaluation, including human studies and system ablations, demonstrates that Casper improves task performance, reduces human cognitive load, and achieves higher user satisfaction than direct teleoperation and assistive teleoperation baselines.', 'abstract_zh': '基于常识知识的辅助遥控操作系统', 'title_zh': 'Casper: 基于视觉语言模型推断辅助远程操作的多样化意图'}
{'arxiv_id': 'arXiv:2506.14723', 'title': 'Adaptive Accompaniment with ReaLchords', 'authors': 'Yusong Wu, Tim Cooijmans, Kyle Kastner, Adam Roberts, Ian Simon, Alexander Scarlatos, Chris Donahue, Cassie Tarakajian, Shayegan Omidshafiei, Aaron Courville, Pablo Samuel Castro, Natasha Jaques, Cheng-Zhi Anna Huang', 'link': 'https://arxiv.org/abs/2506.14723', 'abstract': 'Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an \\emph{online} manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an online model pretrained by maximum likelihood, and use reinforcement learning to finetune the model for online use. The finetuning objective leverages both a novel reward model that provides feedback on both harmonic and temporal coherency between melody and chord, and a divergence term that implements a novel type of distillation from a teacher model that can see the future melody. Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment. ReaLchords opens the door to live jamming, as well as simultaneous co-creation in other modalities.', 'abstract_zh': 'Jamming 要求音乐家之间进行协调、预见和合作性创作。当前的音乐生成模型能够产生具有表现力的输出，但无法以在线方式生成，即无法与其他 musician（包括人类或其他 machine）同时进行。我们提出 ReaLchords，这是一种在线生成模型，用于即兴生成和弦伴奏以配合用户旋律。我们通过最大似然预训练一种在线模型，并使用强化学习对其进行微调以适应在线使用。微调目标结合了新颖的奖励模型，该模型提供了关于和声与旋律之间以及时间连贯性的反馈，以及一个发散项，该发散项采用了可以预见到未来旋律的教师模型的新型蒸馏方法。通过定量实验和聆听测试，我们展示了该模型能够很好地适应不熟悉的数据，并生成合适的伴奏。ReaLchords 为现场即兴演奏打开了大门，并且在其他创作模式下也开启了同步创作的可能性。', 'title_zh': '实时和声适应性伴奏'}
{'arxiv_id': 'arXiv:2506.14684', 'title': 'Refining music sample identification with a self-supervised graph neural network', 'authors': 'Aditya Bhattacharjee, Ivan Meresman Higgs, Mark Sandler, Emmanouil Benetos', 'link': 'https://arxiv.org/abs/2506.14684', 'abstract': 'Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under "real world" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.\nIn this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.\nTo enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.', 'abstract_zh': '自动样本识别（ASID）：在基于音频查询的检索领域，这是一种用于检测和识别新音乐作品中重复使用的音频片段的必要但具挑战性的任务。虽然相关任务音频指纹识别已在“真实世界”（噪声、混响）条件下取得了显著进展，但ASID系统在识别经过音乐修改的样本方面仍然遇到困难。因此，能够抵抗常见的音乐制作变换（如时间拉伸、音高变化、效果处理和背景或叠加音乐）的系统是一个重要的开放挑战。\n\n在本工作中，我们提出了一种轻量级且可扩展的编码架构，该架构结合了图神经网络和对比学习框架。与当前最先进的系统相比，我们的模型仅使用了9%的可训练参数，但在性能上达到了可比水平，实现了44.2%的平均平均精度（mAP）。\n\n为了提高检索质量，我们引入了一种两阶段方法，首先进行粗略相似度搜索以选择候选样本，然后使用交叉注意力分类器拒绝无关匹配，并细化检索候选的排名——这一能力在之前模型中是不存在的。此外，由于实际应用中的查询往往较短，我们在本工作中发布针对Sample100数据集的新细粒度注释，并在此基础上对短查询进行系统基准测试。', 'title_zh': '基于自监督图神经网络的音乐样本识别 refinement'}
{'arxiv_id': 'arXiv:2506.14683', 'title': 'Unified Software Engineering agent as AI Software Engineer', 'authors': 'Leonhard Applis, Yuntong Zhang, Shanchao Liang, Nan Jiang, Lin Tan, Abhik Roychoudhury', 'link': 'https://arxiv.org/abs/2506.14683', 'abstract': 'The growth of Large Language Model (LLM) technology has raised expectations for automated coding. However, software engineering is more than coding and is concerned with activities including maintenance and evolution of a project. In this context, the concept of LLM agents has gained traction, which utilize LLMs as reasoning engines to invoke external tools autonomously. But is an LLM agent the same as an AI software engineer? In this paper, we seek to understand this question by developing a Unified Software Engineering agent or USEagent. Unlike existing work which builds specialized agents for specific software tasks such as testing, debugging, and repair, our goal is to build a unified agent which can orchestrate and handle multiple capabilities. This gives the agent the promise of handling complex scenarios in software development such as fixing an incomplete patch, adding new features, or taking over code written by others. We envision USEagent as the first draft of a future AI Software Engineer which can be a team member in future software development teams involving both AI and humans. To evaluate the efficacy of USEagent, we build a Unified Software Engineering bench (USEbench) comprising of myriad tasks such as coding, testing, and patching. USEbench is a judicious mixture of tasks from existing benchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on USEbench consisting of 1,271 repository-level software engineering tasks, USEagent shows improved efficacy compared to existing general agents such as OpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for certain coding tasks, which provides hints on further developing the AI Software Engineer of the future.', 'abstract_zh': '大型语言模型技术的增长提高了自动编码的期望值。然而，软件工程不仅仅是编码，还涉及包括项目维护和演化在内的多种活动。在此背景下，LLM代理的概念受到了关注，这些代理利用大型语言模型作为推理引擎，自主调用外部工具。但LLM代理等同于AI软件工程师吗？在本文中，我们通过开发一个统一的软件工程代理（USEagent）来研究这一问题。不同于现有专为特定软件任务（如测试、调试和修复）构建的专用代理，我们的目标是构建一个能够协调和处理多种能力的统一代理。这给代理带来了在软件开发中处理复杂场景的潜力，如修复不完整的补丁、增加新功能或接手他人编写的代码。我们设想USEagent是未来AI软件工程师的一个初稿，它可以成为未来涉及AI和人类的软件开发团队的一员。为了评估USEagent的有效性，我们构建了一个统一的软件工程基准（USEbench），包括编码、测试和补丁等多种任务，以现有基准（如SWE-bench、SWT-bench和REPOCOD）中的任务为基础。在包含1,271个仓库级别的软件工程任务的USEbench评估中，USEagent相较于现有的通用代理（如OpenHands CodeActAgent）展示了更好的有效性。在某些编码任务上，USEagent仍存在能力上的差距，这为未来开发AI软件工程师提供了一些建议。', 'title_zh': '统一的软件工程代理作为AI软件工程师'}
{'arxiv_id': 'arXiv:2506.14677', 'title': 'Design an Editable Speech-to-Sign-Language Transformer System: A Human-Centered AI Approach', 'authors': 'Yingchao Li', 'link': 'https://arxiv.org/abs/2506.14677', 'abstract': 'This paper presents a human-centered, real-time, user-adaptive speech-to-sign language animation system that integrates Transformer-based motion generation with a transparent, user-editable JSON intermediate layer. The framework overcomes key limitations in prior sign language technologies by enabling direct user inspection and modification of sign segments, thus enhancing naturalness, expressiveness, and user agency. Leveraging a streaming Conformer encoder and autoregressive Transformer-MDN decoder, the system synchronizes spoken input into upper-body and facial motion for 3D avatar rendering. Edits and user ratings feed into a human-in-the-loop optimization loop for continuous improvement. Experiments with 20 deaf signers and 5 interpreters show that the editable interface and participatory feedback significantly improve comprehension, naturalness, usability, and trust, while lowering cognitive load. With sub-20 ms per-frame inference on standard hardware, the system is ready for real-time communication and education. This work illustrates how technical and participatory innovation together enable accessible, explainable, and user-adaptive AI for sign language technology.', 'abstract_zh': '基于人类中心、实时、用户自适应的语音到手语动画系统：Transformer基于的动作生成与透明、用户可编辑的JSON中间层整合', 'title_zh': '设计一种可编辑的语音到手语转换器系统：以人为中心的AI方法'}
{'arxiv_id': 'arXiv:2506.14670', 'title': 'StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery', 'authors': 'Jina Kim, Leeje Jang, Yao-Yi Chiang, Guanyu Wang, Michelle Pasco', 'link': 'https://arxiv.org/abs/2506.14670', 'abstract': "Traditionally, neighborhood studies have employed interviews, surveys, and manual image annotation guided by detailed protocols to identify environmental characteristics, including physical disorder, decay, street safety, and sociocultural symbols, and to examine their impact on developmental and health outcomes. While these methods yield rich insights, they are time-consuming and require intensive expert intervention. Recent technological advances, including vision-language models (VLMs), have begun to automate parts of this process; however, existing efforts are often ad hoc and lack adaptability across research designs and geographic contexts. In this demo paper, we present StreetLens, a human-centered, researcher-configurable workflow that embeds relevant social science expertise in a VLM for scalable neighborhood environmental assessments. StreetLens mimics the process of trained human coders by grounding the analysis in questions derived from established interview protocols, retrieving relevant street view imagery (SVI), and generating a wide spectrum of semantic annotations from objective features (e.g., the number of cars) to subjective perceptions (e.g., the sense of disorder in an image). By enabling researchers to define the VLM's role through domain-informed prompting, StreetLens places domain knowledge at the core of the analysis process. It also supports the integration of prior survey data to enhance robustness and expand the range of characteristics assessed across diverse settings. We provide a Google Colab notebook to make StreetLens accessible and extensible for researchers working with public or custom SVI datasets. StreetLens represents a shift toward flexible, agentic AI systems that work closely with researchers to accelerate and scale neighborhood studies.", 'abstract_zh': '传统邻里研究通常采用访谈、调查和基于详细协议的手动图像标注方法来识别包括物理杂乱、衰败、街道安全和社會文化符号在内的环境特征，并考察其对发展和健康结果的影响。虽然这些方法提供了丰富的见解，但它们耗时且需要大量专家干预。近年来，随着视觉-语言模型（VLMs）等技术的进步，已经开始自动化这一过程的部分环节；然而，现有的努力往往是零散的，缺乏在不同研究设计和地理背景下进行调整的能力。在本文中，我们介绍了一种以人为中心、研究员可配置的工作流程StreetLens，该流程将相关的社会科学研究专长嵌入到VLM中，以实现可扩展的邻里环境评估。StreetLens通过基于经过验证的访谈协议衍生的问题来模拟受过培训的人工编码员的过程，检索相关的街景图像（SVI），并生成从客观特征（如车辆数量）到主观感知（如图像中的杂乱感）的广泛语义注释。通过允许研究者通过领域知识指导的提示来定义VLM的角色，StreetLens将领域知识置于分析过程的核心。它还支持整合先前的调查数据，以增强稳健性并扩展在多种环境下评估的特征范围。我们提供了一个Google Colab笔记本，使研究人员能够轻松访问和扩展公共或自定义街景图像数据集的StreetLens工具。StreetLens代表了一种转变，即将灵活且有自主权的人工智能系统与研究者紧密合作，以加速并扩大邻里研究的规模。', 'title_zh': 'StreetLens: 以人为本的AI代理用于街道视角图像的街区评估'}
{'arxiv_id': 'arXiv:2506.14665', 'title': 'Accurate and scalable exchange-correlation with deep learning', 'authors': 'Giulia Luise, Chin-Wei Huang, Thijs Vogels, Derk P. Kooi, Sebastian Ehlert, Stephanie Lanius, Klaas J. H. Giesbertz, Amir Karton, Deniz Gunceler, Megan Stanley, Wessel P. Bruinsma, Lin Huang, Xinran Wei, José Garrido Torres, Abylay Katbashev, Bálint Máté, Sékou-Oumar Kaba, Roberto Sordillo, Yingrong Chen, David B. Williams-Young, Christopher M. Bishop, Jan Hermann, Rianne van den Berg, Paola Gori-Giorgi', 'link': 'https://arxiv.org/abs/2506.14665', 'abstract': 'Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schrödinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.', 'abstract_zh': '基于深度学习的Skala交换相关泛函：通过直接从数据学习绕过昂贵的手工设计特征实现化学精度', 'title_zh': '深度学习驱动的准确且可扩展的交换相关泛函'}
{'arxiv_id': 'arXiv:2506.14652', 'title': 'Rigor in AI: Doing Rigorous AI Work Requires a Broader, Responsible AI-Informed Conception of Rigor', 'authors': 'Alexandra Olteanu, Su Lin Blodgett, Agathe Balayn, Angelina Wang, Fernando Diaz, Flavio du Pin Calmon, Margaret Mitchell, Michael Ekstrand, Reuben Binns, Solon Barocas', 'link': 'https://arxiv.org/abs/2506.14652', 'abstract': "In AI research and practice, rigor remains largely understood in terms of methodological rigor -- such as whether mathematical, statistical, or computational methods are correctly applied. We argue that this narrow conception of rigor has contributed to the concerns raised by the responsible AI community, including overblown claims about AI capabilities. Our position is that a broader conception of what rigorous AI research and practice should entail is needed. We believe such a conception -- in addition to a more expansive understanding of (1) methodological rigor -- should include aspects related to (2) what background knowledge informs what to work on (epistemic rigor); (3) how disciplinary, community, or personal norms, standards, or beliefs influence the work (normative rigor); (4) how clearly articulated the theoretical constructs under use are (conceptual rigor); (5) what is reported and how (reporting rigor); and (6) how well-supported the inferences from existing evidence are (interpretative rigor). In doing so, we also aim to provide useful language and a framework for much-needed dialogue about the AI community's work by researchers, policymakers, journalists, and other stakeholders.", 'abstract_zh': '在人工智能研究与实践中，严谨性主要仍被理解为方法论严谨性——例如，数学、统计或计算方法是否正确应用。我们认为，这一狭隘的严谨性概念导致了负责任的人工智能社区提出的一些担忧，包括夸大人工智能的能力。我们认为，需要一种更广泛的关于严谨性人工智能研究与实践应包含的内容的概念。我们相信，除了更广泛地理解（1）方法论严谨性——还应包括（2）如何被现有知识指导所做的工作（知识论严谨性）；（3）学科、社区或个人规范、标准或信念如何影响工作（规范性严谨性）；（4）正在使用的基本理论概念是否清晰阐明（概念性严谨性）；（5）报告的内容及其方式（报告严谨性）；以及（6）现有证据得出的推论是否得到充分支持（解释性严谨性）。我们还希望为此提供有用的语汇和框架，以促进研究人员、政策制定者、记者和其他利益相关者非常需要的关于人工智能社区工作的对话。', 'title_zh': 'AI的严谨性：进行严谨的AI工作需要一种更广泛且负责任的AI驱动的严谨性观念'}
{'arxiv_id': 'arXiv:2506.14648', 'title': 'SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning', 'authors': 'Hexian Ni, Tao Lu, Haoyuan Hu, Yinghao Cai, Shuo Wang', 'link': 'https://arxiv.org/abs/2506.14648', 'abstract': 'Preference-based Reinforcement Learning (PbRL) methods provide a solution to avoid reward engineering by learning reward models based on human preferences. However, poor feedback- and sample- efficiency still remain the problems that hinder the application of PbRL. In this paper, we present a novel efficient query selection and preference-guided exploration method, called SENIOR, which could select the meaningful and easy-to-comparison behavior segment pairs to improve human feedback-efficiency and accelerate policy learning with the designed preference-guided intrinsic rewards. Our key idea is twofold: (1) We designed a Motion-Distinction-based Selection scheme (MDS). It selects segment pairs with apparent motion and different directions through kernel density estimation of states, which is more task-related and easy for human preference labeling; (2) We proposed a novel preference-guided exploration method (PGE). It encourages the exploration towards the states with high preference and low visits and continuously guides the agent achieving the valuable samples. The synergy between the two mechanisms could significantly accelerate the progress of reward and policy learning. Our experiments show that SENIOR outperforms other five existing methods in both human feedback-efficiency and policy convergence speed on six complex robot manipulation tasks from simulation and four real-worlds.', 'abstract_zh': '基于偏好强化学习的高效查询选择与偏好引导探索方法：SENIOR', 'title_zh': 'SENIOR：基于偏好的强化学习中高效查询选择与偏好引导探索'}
{'arxiv_id': 'arXiv:2506.14641', 'title': 'Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot', 'authors': 'Xiang Cheng, Chengyan Pan, Minjun Zhao, Deyang Li, Fangchao Liu, Xinyu Zhang, Xiao Zhang, Yong Liu', 'link': 'https://arxiv.org/abs/2506.14641', 'abstract': "In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \\texttt{Qwen2.5-Max} and \\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.", 'abstract_zh': '大型语言模型中上下文学习（ICL）中链式思考（CoT）范例的有效性探究：数学推理能力提升的局限性', 'title_zh': '重访链式思考提示：零样本可能比少样本更强大'}
{'arxiv_id': 'arXiv:2506.14640', 'title': 'Navigating the growing field of research on AI for software testing -- the taxonomy for AI-augmented software testing and an ontology-driven literature survey', 'authors': 'Ina K. Schieferdecker', 'link': 'https://arxiv.org/abs/2506.14640', 'abstract': "In industry, software testing is the primary method to verify and validate the functionality, performance, security, usability, and so on, of software-based systems. Test automation has gained increasing attention in industry over the last decade, following decades of intense research into test automation and model-based testing. However, designing, developing, maintaining and evolving test automation is a considerable effort. Meanwhile, AI's breakthroughs in many engineering fields are opening up new perspectives for software testing, for both manual and automated testing. This paper reviews recent research on AI augmentation in software test automation, from no automation to full automation. It also discusses new forms of testing made possible by AI. Based on this, the newly developed taxonomy, ai4st, is presented and used to classify recent research and identify open research questions.", 'abstract_zh': '在工业领域，软件测试是验证和确认基于软件系统的功能、性能、安全性和易用性等的主要方法。过去十年中，随着对测试自动化和模型驱动测试多年深入研究的进展，软件测试自动化逐渐获得广泛关注。然而，设计、开发、维护和演化测试自动化是一项巨大的努力。同时，AI在许多工程领域的突破为软件测试，无论是手动测试还是自动化测试，开启了新的视角。本文回顾了从无自动化到完全自动化的AI在软件测试自动化中的最新研究，讨论了AI使可能的新测试形式，并基于此，介绍了新的分类框架ai4st，用于分类最新研究并确定开放研究问题。', 'title_zh': '导航不断增长的AI在软件测试领域研究——基于本体的AI增强软件测试分类与文献综述'}
{'arxiv_id': 'arXiv:2506.14634', 'title': "AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation", 'authors': 'Leah von der Heyde, Anna-Carolina Haensch, Bernd Weiß, Jessika Daikeler', 'link': 'https://arxiv.org/abs/2506.14634', 'abstract': "The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.", 'abstract_zh': '近期大语言模型的发展及其更广泛的 accessibility 促进了其在调查研究中的应用讨论，包括对开放问卷答案的分类。由于具备语言能力，大语言模型可能是处理此类问题的有效替代方案，替代耗时的手动编码和监督机器学习模型的预训练。尽管已有研究主要关注英语答案或单一模型的非复杂主题，但尚不清楚其发现是否具有普适性，以及这些分类的质量与传统方法相比如何。在本研究中，我们通过使用德国参与调查原因的数据，探讨不同大语言模型在其他情境下分类开放问卷答案的可行程度，并对比多种尖端大语言模型和提示方法的表现，通过使用人类专家编码来评估其性能。不同大语言模型之间的整体性能差异巨大，只有微调模型才能实现可接受的预测性能。不同提示方法之间的性能差异取决于所使用的大语言模型。此外，大语言模型在不同参与调查原因类别上的分类不平等导致在不进行微调的情况下出现不同的分类分布。我们讨论了这些发现对开放问卷答案编码方法论研究及其实质性分析的影响，以及对处理此类数据的实践者的启示。本研究还强调了研究人员在选择大语言模型时代开放问卷答案分类的自动化方法时需要考虑的诸多权衡。我们的研究为大语言模型在调查研究中高效、准确、可靠的应用条件增添了研究基础。', 'title_zh': '除了调查之外还有什么？使用大型语言模型对关于调查动机的开放式调查回应进行编码'}
{'arxiv_id': 'arXiv:2506.14627', 'title': 'ACM Survey Draft on Formalising Software Requirements with Large Language Models', 'authors': "Arshad Beg, Diarmuid O'Donoghue, Rosemary Monahan", 'link': 'https://arxiv.org/abs/2506.14627', 'abstract': 'This draft is a working document, having a summary of nighty-four (94) papers with additional sections on Traceability of Software Requirements (Section 4), Formal Methods and Its Tools (Section 5), Unifying Theories of Programming (UTP) and Theory of Institutions (Section 6). Please refer to abstract of [7,8]. Key difference of this draft from our recently anticipated ones with similar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:\n[7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted on 18th of March, 2025, it went through the light-weight blind review and accepted for poster presentation. Conference was held on 15th of May, 2025.\n[8] is a nine page paper with additional nine pages of references and summary tables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April, 2025. It went through rigorous review process. The uploaded version on arXiv.org [8] is the improved one of the submission, after addressing the specific suggestions to improve the paper.', 'abstract_zh': '这是一部工作文档，包含了九十四篇论文的摘要，并增设了软件需求可追溯性（第4节）、形式方法及其工具（第5节）、编程统一理论（UTP）与机构理论（第6节）的相关部分。请参见[7,8]的摘要。与我们先前类似标题的预期文件，即AACS 2025 [7]和SAIV 2025 [8]的不同之处在于：', 'title_zh': 'ACM调查草稿：使用大型语言模型形式化软件需求'}
{'arxiv_id': 'arXiv:2506.14625', 'title': 'Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models', 'authors': 'Chenchen Yuan, Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci', 'link': 'https://arxiv.org/abs/2506.14625', 'abstract': "Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.", 'abstract_zh': '大型语言模型在处理复杂多因素道德困境时常常出现分歧。为了应对这些分歧，我们提出了一种框架，将多个大型语言模型的道德判断综合为一个集体形成的道德判断，重新校准与该共识显著偏离的模型。我们的聚合机制融合了连续的道德可接受性评分（超越二元标签），并通过模型可靠性加权。对于错位的模型，我们采取目标向量优化程序微调道德哲学理论的令牌嵌入，同时最小化与共识的JS散度并保留语义完整性。实验表明，我们的方法能够建立稳健的共识并提高个体模型的准确性。这些发现强调了多模型数据驱动的道德对齐的价值及其对更安全、更一致的人工智能系统潜在的促进作用。', 'title_zh': '面向集体道德推理的大型语言模型中概率聚合与目标嵌入优化'}
{'arxiv_id': 'arXiv:2506.14623', 'title': 'Low-code to fight climate change: the Climaborough project', 'authors': 'Aaron Conrardy, Armen Sulejmani, Cindy Guerlain, Daniele Pagani, David Hick, Matteo Satta, Jordi Cabot', 'link': 'https://arxiv.org/abs/2506.14623', 'abstract': "The EU-funded Climaborough project supports European cities to achieve carbon neutrality by 2030. Eleven cities in nine countries will deploy in real conditions products and services fostering climate transition in their local environment. The Climaborough City Platform is being developed to monitor the cities' overall progress towards their climate goals by aggregating historic and real-time data and displaying the results in user-friendly dashboards that will be used by non-technical experts to evaluate the effectiveness of local experimental initiatives, identify those that yield significant impact, and assess the potential consequences of scaling them up to a broader level. In this paper, we explain how we have put in place a low-code/no-code strategy in Climaborough in response to the project's aim to quickly deploy climate dashboards. A low-code strategy is used to accelerate the development of the dashboards. The dashboards embed a no-code philosophy that enables all types of citizen profiles to configure and adapt the dashboard to their specific needs.", 'abstract_zh': '欧盟资助的Climaborough项目支持欧洲城市在2030年前实现碳中和', 'title_zh': '低代码对抗气候变化：Climaborough项目'}
{'arxiv_id': 'arXiv:2506.14596', 'title': 'PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation', 'authors': 'Ming Xu, Xu Zhang', 'link': 'https://arxiv.org/abs/2506.14596', 'abstract': 'Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at this https URL.', 'abstract_zh': '现有的单目三维姿态估计方法主要依赖于关节位置特征，忽视了骨骼内部的内在方向和角度关联，因此在关节遮挡或快速运动变化时往往会生成不合理的人姿。为了解决这些挑战，我们提出了一种PoseGRAF框架。我们首先构建了一种双图卷积结构，分别处理关节图和骨骼图，有效地捕捉它们的局部依赖关系。接着引入了一个跨注意力模块，用于建模骨骼方向和关节特征之间的相互依赖关系。在此基础上，设计了一个动态融合模块，通过利用关节和骨骼之间的关系依赖，自适应地结合两种特征类型。进一步以残差方式引入了一个改进的Transformer编码器，生成最终输出。在Human3.6M和MPI-INF-3DHP数据集上的实验结果表明，我们的方法超过了现有最先进的方法。额外的野外视频评估进一步验证了其泛化能力。代码已在以下网址公开：this https URL。', 'title_zh': 'PoseGRAF：几何强化自适应融合在单目3D人体姿态估计中的应用'}
{'arxiv_id': 'arXiv:2506.14583', 'title': "Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images", 'authors': 'Krishna Sahukara, Zineddine Bettouche, Andreas Fischer', 'link': 'https://arxiv.org/abs/2506.14583', 'abstract': 'Document pages captured by smartphones or scanners often contain tables, yet manual extraction is slow and error-prone. We introduce an automated LaTeX-based pipeline that synthesizes realistic two-column pages with visually diverse table layouts and aligned ground-truth masks. The generated corpus augments the real-world Marmot benchmark and enables a systematic resolution study of TableNet. Training TableNet on our synthetic data achieves a pixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input resolution, and 4.33% with 1024x1024. The best performance on the Marmot benchmark is 9.18% (at 256x256), while cutting manual annotation effort through automation.', 'abstract_zh': '智能手机或扫描仪捕获的文档页面经常包含表格，但手动提取耗时且容易出错。我们介绍了一种基于自动化的LaTeX管道，该管道合成具有多种视觉布局的两列页面，并生成了对齐的地面真实掩模。生成的语料库增强了现实世界的Marmot基准，并使TableNet的系统分辨率研究成为可能。在我们的合成数据上训练TableNet，在256x256输入分辨率下的像素级XOR误差为4.04%，在1024x1024下的误差为4.33%。在Marmot基准上的最佳性能为9.18%（在256x256时），并通过自动化减少了手动注释的工作量。', 'title_zh': '合成数据扩增用于表格检测：重新评估TableNet的性能（使用自动生成的文档图像）'}
{'arxiv_id': 'arXiv:2506.14580', 'title': 'GenerationPrograms: Fine-grained Attribution with Executable Programs', 'authors': 'David Wan, Eran Hirsch, Elias Stengel-Eskin, Ido Dagan, Mohit Bansal', 'link': 'https://arxiv.org/abs/2506.14580', 'abstract': 'Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable "code agent" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program\'s specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.', 'abstract_zh': 'Recent大型语言模型（LLMs）在条件源文本生成方面取得了显著性能，但往往无法正确提供详细的归因，损害了验证性和可信度。此外，现有的归因方法未能解释模型如何及为何利用提供的源文档生成最终响应，限制了模型的可解释性。为克服这些挑战，我们提出了一个模块化生成框架GenerationPrograms，该框架受可执行“代码代理”架构最新进展的启发。与同时生成输出和归因或依赖于事后归因的常规生成方法不同，GenerationPrograms将过程分解为两个不同的阶段：首先，创建由模块化文本操作（如改写、压缩和融合）组成的可执行程序规划，这些操作明确针对查询进行定制；其次，按照程序指定的指令执行这些操作，从而生成最终响应。实证评估表明，GenerationPrograms在两个长文本问答任务和一个多文档摘要任务中，显著提高了文档级和句子级的归因质量。我们进一步证明，GenerationPrograms可以有效作为事后归因方法使用，在恢复准确归因方面优于传统技术。此外，GenerationPrograms生成的可解释程序通过模块级改进实现局部优化，进一步提升总体归因质量。', 'title_zh': '生成程序：可执行程序的细粒度归因'}
{'arxiv_id': 'arXiv:2506.14577', 'title': 'Object-Centric Neuro-Argumentative Learning', 'authors': 'Abdul Rahman Jacob, Avinash Kori, Emanuele De Angelis, Ben Glocker, Maurizio Proietti, Francesca Toni', 'link': 'https://arxiv.org/abs/2506.14577', 'abstract': 'Over the last decade, as we rely more on deep learning technologies to make critical decisions, concerns regarding their safety, reliability and interpretability have emerged. We introduce a novel Neural Argumentative Learning (NAL) architecture that integrates Assumption-Based Argumentation (ABA) with deep learning for image analysis. Our architecture consists of neural and symbolic components. The former segments and encodes images into facts using object-centric learning, while the latter applies ABA learning to develop ABA frameworks enabling predictions with images. Experiments on synthetic data show that the NAL architecture can be competitive with a state-of-the-art alternative.', 'abstract_zh': '近年来，随着我们越来越多地依赖深度学习技术进行关键决策，对其安全、可靠性和可解释性的问题日益凸显。我们提出了一种新颖的神经论辩学习（NAL）架构，将基于假设的论辩（ABA）与深度学习结合用于图像分析。该架构包含神经和符号两个组件。前者使用对象中心学习对图像进行分割并编码为事实，后者应用ABA学习构建ABA框架，以实现基于图像的预测。在合成数据上的实验表明，NAL架构可以与当前最先进的替代方案竞争。', 'title_zh': '对象中心神经论证学习'}
{'arxiv_id': 'arXiv:2506.14574', 'title': 'TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization', 'authors': 'Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, Hengshuang Zhao, Jiaya Jia', 'link': 'https://arxiv.org/abs/2506.14574', 'abstract': 'Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at this https URL.', 'abstract_zh': 'Recent 进展表明，利用细粒度词级奖励模型可以显著提高Proximal Policy Optimization (PPO)在对齐大型语言模型方面的性能，来自人类反馈的强化学习 recent 进展。然而，利用此类词级奖励作为直接偏好优化（DPO）的指导存在挑战，因为DPO被形式化为一个序列级多臂 bandit 问题。为了解决这一挑战，本文将序列级 PPO 分解为一系列词级 proximal 政策优化问题，然后以词级奖励指导的方式重新表述词级 PPO 问题，从而可以推导出闭式最优词级策略和相应的词级奖励。利用获得的奖励和 Bradley-Terry 模型，本文为 DPO 建立了一个带有词级奖励指导的可计算损失函数框架，并提出了一种基于诱导 DPO 奖励的实用奖励指导。这种形式化允许不同词根据各自的奖励展示不同程度与参考策略的偏差。实验结果表明，与 DPO 相比，我们的方法在 MT-Bench 上的胜率提高了 7.5 个百分点，在 AlpacaEval 2 上提高了 6.2 个百分点，在 Arena-Hard 上提高了 4.3 个百分点。代码参见此 https URL。', 'title_zh': 'TGDPO: 利用令牌级奖励指导以提高直接偏好优化效果'}
{'arxiv_id': 'arXiv:2506.14567', 'title': 'Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains', 'authors': 'Emanuel Moss, Elizabeth Watkins, Christopher Persaud, Passant Karunaratne, Dawn Nafus', 'link': 'https://arxiv.org/abs/2506.14567', 'abstract': 'Generative AI tools have become more prevalent in engineering workflows, particularly through chatbots and code assistants. As the perceived accuracy of these tools improves, questions arise about whether and how those who work in high-precision domains might maintain vigilance for errors, and what other aspects of using such tools might trouble their work. This paper analyzes interviews with hardware and software engineers, and their collaborators, who work in integrated circuit design to identify the role accuracy plays in their use of generative AI tools and what other forms of trouble they face in using such tools. The paper inventories these forms of trouble, which are then mapped to elements of generative AI systems, to conclude that controlling the context of interactions between engineers and the generative AI tools is one of the largest challenges they face. The paper concludes with recommendations for mitigating this form of trouble by increasing the ability to control context interactively.', 'abstract_zh': '生成式AI工具在工程工作流中的应用日益普遍，尤其是在聊天机器人和代码助手方面的应用。随着这些工具被认为准确性的提高，人们不禁思考高精度领域工作的从业者如何保持对潜在错误的警惕，以及使用此类工具还会遇到哪些其他问题。本文通过访谈集成电路设计的硬件和软件工程师及其合作者，分析生成式AI工具在他们工作中的作用，以及使用此类工具还会遇到的其他问题。本文列出了这些问题，并将其映射到生成式AI系统的各个要素，得出控制工程师与生成式AI工具交互的上下文是他们面临的一大挑战。文章最后提出建议，通过增强交互式的上下文控制能力来减轻这种问题。', 'title_zh': '控制语境：生成式AI在集成电路设计及其他高精度领域中的应用'}
{'arxiv_id': 'arXiv:2506.14562', 'title': 'AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs', 'authors': 'Di He, Ajay Jaiswal, Songjun Tu, Li Shen, Ganzhao Yuan, Shiwei Liu, Lu Yin', 'link': 'https://arxiv.org/abs/2506.14562', 'abstract': 'Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.', 'abstract_zh': 'AlphaDecay：基于重尾自我正则化理论的模块自适应权重衰减方法', 'title_zh': 'AlphaDecay：模块级权重衰减在LLM中实现重尾平衡'}
{'arxiv_id': 'arXiv:2506.14540', 'title': 'Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs', 'authors': 'Gerardo A. Flores, Alyssa H. Smith, Julia A. Fukuyama, Ashia C. Wilson', 'link': 'https://arxiv.org/abs/2506.14540', 'abstract': 'Machine learning-based decision support systems are increasingly deployed in clinical settings, where probabilistic scoring functions are used to inform and prioritize patient management decisions. However, widely used scoring rules, such as accuracy and AUC-ROC, fail to adequately reflect key clinical priorities, including calibration, robustness to distributional shifts, and sensitivity to asymmetric error costs. In this work, we propose a principled yet practical evaluation framework for selecting calibrated thresholded classifiers that explicitly accounts for the uncertainty in class prevalences and domain-specific cost asymmetries often found in clinical settings. Building on the theory of proper scoring rules, particularly the Schervish representation, we derive an adjusted variant of cross-entropy (log score) that averages cost-weighted performance over clinically relevant ranges of class balance. The resulting evaluation is simple to apply, sensitive to clinical deployment conditions, and designed to prioritize models that are both calibrated and robust to real-world variations.', 'abstract_zh': '基于机器学习的决策支持系统在临床环境中越来越广泛部署，其中概率评分函数用于指导和优先处理患者的管理决策。然而，常用的评分规则，如准确率和AUC-ROC，无法充分反映临床优先考虑的关键因素，包括校准、分布变化的稳健性和不对称错误成本的敏感性。在本文中，我们提出了一种既原则性强又实用的评估框架，用于选择校准的分类器阈值，该框架明确考虑了类流行率的不确定性以及临床环境中常见的领域特定成本不对称性。基于适当评分规则的理论，特别是舒尔维希表示法，我们推导出一种调整后的交叉熵（对数得分）变体，它在临床相关范围内的类平衡上平均成本加权性能。该评估方法简单易行，对临床部署条件敏感，并旨在优先考虑既校准又对现实世界变化具有稳健性的模型。', 'title_zh': '临床优先级对齐的评估：校准、标签转移和错误成本'}
{'arxiv_id': 'arXiv:2506.14535', 'title': 'Automatic Qiskit Code Refactoring Using Large Language Models', 'authors': 'José Manuel Suárez, Luis Mariano Bibbó, Joaquin Bogado, Alejandro Fernandez', 'link': 'https://arxiv.org/abs/2506.14535', 'abstract': 'As quantum software frameworks evolve, developers face increasing challenges in maintaining compatibility with rapidly changing APIs. In this work, we present a novel methodology for refactoring Qiskit code using large language models (LLMs). We begin by extracting a taxonomy of migration scenarios from the different sources of official Qiskit documentation (such as release notes), capturing common patterns such as migration of functionality to different modules and deprecated usage. This taxonomy, along with the original Python source code, is provided as input to an LLM, which is then tasked with identifying instances of migration scenarios in the code and suggesting appropriate refactoring solutions. Our approach is designed to address the context length limitations of current LLMs by structuring the input and reasoning process in a targeted, efficient manner. The results demonstrate that LLMs, when guided by domain-specific migration knowledge, can effectively assist in automating Qiskit code migration. This work contributes both a set of proven prompts and taxonomy for Qiskit code migration from earlier versions to version 0.46 and a methodology to asses the capabilities of LLMs to assist in the migration of quantum code.', 'abstract_zh': '基于大型语言模型的Qiskit代码重构新型方法研究', 'title_zh': '使用大型语言模型进行自动Qiskit代码重构'}
{'arxiv_id': 'arXiv:2506.14534', 'title': 'Complete Characterization for Adjustment in Summary Causal Graphs of Time Series', 'authors': 'Clément Yvernes, Emilie Devijver, Eric Gaussier', 'link': 'https://arxiv.org/abs/2506.14534', 'abstract': 'The identifiability problem for interventions aims at assessing whether the total causal effect can be written with a do-free formula, and thus be estimated from observational data only. We study this problem, considering multiple interventions, in the context of time series when only an abstraction of the true causal graph, in the form of a summary causal graph, is available. We propose in particular both necessary and sufficient conditions for the adjustment criterion, which we show is complete in this setting, and provide a pseudo-linear algorithm to decide whether the query is identifiable or not.', 'abstract_zh': '干预的可识别性问题旨在评估总因果效应是否可以用do-free公式表示，从而仅从观察数据中进行估计。在仅拥有真实因果图的抽象形式（总结因果图）的时间序列背景下，我们研究了这一问题，特别是在考虑多个干预的情况下。我们特别提出了调整准则的必要且充分条件，并表明在该背景下该准则完备。此外，我们提供了一个伪线性算法来决定查询是否可识别。', 'title_zh': '时间序列摘要因果图中调整的完全表征'}
{'arxiv_id': 'arXiv:2506.14530', 'title': 'Sharp Generalization Bounds for Foundation Models with Asymmetric Randomized Low-Rank Adapters', 'authors': 'Anastasis Kratsios, Tin Sum Cheng, Aurelien Lucchi, Haitz Sáez de Ocáriz Borde', 'link': 'https://arxiv.org/abs/2506.14530', 'abstract': "Low-Rank Adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning (PEFT) technique for foundation models. Recent work has highlighted an inherent asymmetry in the initialization of LoRA's low-rank factors, which has been present since its inception and was presumably derived experimentally. This paper focuses on providing a comprehensive theoretical characterization of asymmetric LoRA with frozen random factors. First, while existing research provides upper-bound generalization guarantees based on averages over multiple experiments, the behaviour of a single fine-tuning run with specific random factors remains an open question. We address this by investigating the concentration of the typical LoRA generalization gap around its mean. Our main upper bound reveals a sample complexity of $\\tilde{\\mathcal{O}}\\left(\\frac{\\sqrt{r}}{\\sqrt{N}}\\right)$ with high probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we also determine the fundamental limits in terms of sample efficiency, establishing a matching lower bound of $\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)$. By more closely reflecting the practical scenario of a single fine-tuning run, our findings offer crucial insights into the reliability and practicality of asymmetric LoRA.", 'abstract_zh': 'Asymmetric Low-Rank Adaptation (LoRA) with Frozen Random Factors: A Comprehensive Theoretical Characterization', 'title_zh': '具有不对称随机低秩适配器的 foundation 模型的精确泛化边界'}
{'arxiv_id': 'arXiv:2506.14513', 'title': 'GAMORA: A Gesture Articulated Meta Operative Robotic Arm for Hazardous Material Handling in Containment-Level Environments', 'authors': 'Farha Abdul Wasay, Mohammed Abdul Rahman, Hania Ghouse', 'link': 'https://arxiv.org/abs/2506.14513', 'abstract': "The convergence of robotics and virtual reality (VR) has enabled safer and more efficient workflows in high-risk laboratory settings, particularly virology labs. As biohazard complexity increases, minimizing direct human exposure while maintaining precision becomes essential. We propose GAMORA (Gesture Articulated Meta Operative Robotic Arm), a novel VR-guided robotic system that enables remote execution of hazardous tasks using natural hand gestures. Unlike existing scripted automation or traditional teleoperation, GAMORA integrates the Oculus Quest 2, NVIDIA Jetson Nano, and Robot Operating System (ROS) to provide real-time immersive control, digital twin simulation, and inverse kinematics-based articulation. The system supports VR-based training and simulation while executing precision tasks in physical environments via a 3D-printed robotic arm. Inverse kinematics ensure accurate manipulation for delicate operations such as specimen handling and pipetting. The pipeline includes Unity-based 3D environment construction, real-time motion planning, and hardware-in-the-loop testing. GAMORA achieved a mean positional discrepancy of 2.2 mm (improved from 4 mm), pipetting accuracy within 0.2 mL, and repeatability of 1.2 mm across 50 trials. Integrated object detection via YOLOv8 enhances spatial awareness, while energy-efficient operation (50% reduced power output) ensures sustainable deployment. The system's digital-physical feedback loop enables safe, precise, and repeatable automation of high-risk lab tasks. GAMORA offers a scalable, immersive solution for robotic control and biosafety in biomedical research environments.", 'abstract_zh': '机器人技术和虚拟现实的融合已在高风险实验室环境中，尤其是病毒学实验室中实现了更安全、更高效的 workflows。随着生物危害的复杂性增加，如何在保持精准度的同时最小化直接的人体暴露变得至关重要。我们提出了一种名为GAMORA（Gesture Articulated Meta Operative Robotic Arm）的新型VR指导机器人系统，该系统利用自然手势远程执行危险任务。GAMORA集成了Oculus Quest 2、NVIDIA Jetson Nano和Robot Operating System (ROS)，提供实时沉浸式控制、数字孪生模拟和基于逆运动学的操作。该系统支持基于VR的培训和模拟，并通过3D打印的机械臂在物理环境中执行精准任务。逆运动学确保了复杂操作中的精确操作，如样本处理和移液。管道包括基于Unity的3D环境构建、实时运动规划和硬件在环测试。GAMORA实现了2.2毫米的平均位置偏差（改进前为4毫米）、移液精度在0.2毫升以内，并且在50次试验中重复性为1.2毫米。通过YOLOv8集成对象检测增强了空间意识，而高效的能源操作（功率输出减少50%）确保了可持续部署。该系统的数字-物理反馈回路使高风险实验室任务的安全、精准和可重复自动化成为可能。GAMORA为生物医药研究环境中的机器人控制和生物安全提供了一种可扩展的沉浸式解决方案。', 'title_zh': 'GAMORA：一种用于限制级环境危险材料处理的 gesture articulated meta-operative 机械臂'}
{'arxiv_id': 'arXiv:2506.14472', 'title': 'Leveraging External Factors in Household-Level Electrical Consumption Forecasting using Hypernetworks', 'authors': 'Fabien Bernier, Maxime Cordy, Yves Le Traon', 'link': 'https://arxiv.org/abs/2506.14472', 'abstract': 'Accurate electrical consumption forecasting is crucial for efficient energy management and resource allocation. While traditional time series forecasting relies on historical patterns and temporal dependencies, incorporating external factors -- such as weather indicators -- has shown significant potential for improving prediction accuracy in complex real-world applications. However, the inclusion of these additional features often degrades the performance of global predictive models trained on entire populations, despite improving individual household-level models. To address this challenge, we found that a hypernetwork architecture can effectively leverage external factors to enhance the accuracy of global electrical consumption forecasting models, by specifically adjusting the model weights to each consumer.\nWe collected a comprehensive dataset spanning two years, comprising consumption data from over 6000 luxembourgish households and corresponding external factors such as weather indicators, holidays, and major local events. By comparing various forecasting models, we demonstrate that a hypernetwork approach outperforms existing methods when associated to external factors, reducing forecasting errors and achieving the best accuracy while maintaining the benefits of a global model.', 'abstract_zh': '准确的电能消耗forecasting对于高效的能源管理和资源配置至关重要。虽然传统的时序forecasting依赖历史模式和时间依赖性，但在复杂的实际应用中，将外部因素（如天气指标）纳入其中已显示出显著提高预测精度的潜力。然而，尽管外部因素的加入可以提升单个家庭级别的模型表现，但通常会降低全局预测模型的表现。为应对这一挑战，我们发现，超网络架构能够通过针对性地调整模型权重来有效利用外部因素，从而增强全局电能消耗 forecasting 模型的准确性。通过对比各种 forecasting 模型，我们证明，结合外部因素的超网络方法优于现有方法，能够减少 forecasting 错误，并在保持全局模型优势的同时实现最佳精度。', 'title_zh': '基于超网络在户级电气消费预测中利用外部因素的研究'}
{'arxiv_id': 'arXiv:2506.14464', 'title': 'A Scalable Hybrid Training Approach for Recurrent Spiking Neural Networks', 'authors': 'Maximilian Baronig, Yeganeh Bahariasl, Ozan Özdenizci, Robert Legenstein', 'link': 'https://arxiv.org/abs/2506.14464', 'abstract': 'Recurrent spiking neural networks (RSNNs) can be implemented very efficiently in neuromorphic systems. Nevertheless, training of these models with powerful gradient-based learning algorithms is mostly performed on standard digital hardware using Backpropagation through time (BPTT). However, BPTT has substantial limitations. It does not permit online training and its memory consumption scales linearly with the number of computation steps. In contrast, learning methods using forward propagation of gradients operate in an online manner with a memory consumption independent of the number of time steps. These methods enable SNNs to learn from continuous, infinite-length input sequences. Yet, slow execution speed on conventional hardware as well as inferior performance has hindered their widespread application. In this work, we introduce HYbrid PRopagation (HYPR) that combines the efficiency of parallelization with approximate online forward learning. Our algorithm yields high-throughput online learning through parallelization, paired with constant, i.e., sequence length independent, memory demands. HYPR enables parallelization of parameter update computation over the sub sequences for RSNNs consisting of almost arbitrary non-linear spiking neuron models. We apply HYPR to networks of spiking neurons with oscillatory subthreshold dynamics. We find that this type of neuron model is particularly well trainable by HYPR, resulting in an unprecedentedly low task performance gap between approximate forward gradient learning and BPTT.', 'abstract_zh': '杂合前向传播（HYPR）：结合并行化效率与近似在线前向学习', 'title_zh': '可扩展的混合训练方法用于递归神经脉冲网络'}
{'arxiv_id': 'arXiv:2506.14456', 'title': 'Hamiltonian Formalism for Comparing Quantum and Classical Intelligence', 'authors': 'Elija Perrier', 'link': 'https://arxiv.org/abs/2506.14456', 'abstract': 'The prospect of AGI instantiated on quantum substrates motivates the development of mathematical frameworks that enable direct comparison of their operation in classical and quantum environments. To this end, we introduce a Hamiltonian formalism for describing classical and quantum AGI tasks as a means of contrasting their interaction with the environment. We propose a decomposition of AGI dynamics into Hamiltonian generators for core functions such as induction, reasoning, recursion, learning, measurement, and memory. This formalism aims to contribute to the development of a precise mathematical language for how quantum and classical agents differ via environmental interaction.', 'abstract_zh': '基于量子底层实现的AGI的前景促使我们发展数学框架以直接对比其在经典和量子环境中的操作。为此，我们引入了一种哈密顿 formalism 来描述经典和量子AGI任务，以便对比它们与环境的交互。我们提议将AGI动力学分解为哈密顿生成器，用于核心功能如归纳、推理、递归、学习、测量和记忆。此 formalism 目标是为通过环境交互如何量子和经典代理的不同提供一种精确的数学语言。', 'title_zh': '量子与经典智能比较的哈密顿正则形式分析'}
{'arxiv_id': 'arXiv:2506.14451', 'title': 'Adapting Lightweight Vision Language Models for Radiological Visual Question Answering', 'authors': 'Aditya Shourya, Michel Dumontier, Chang Sun', 'link': 'https://arxiv.org/abs/2506.14451', 'abstract': 'Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.', 'abstract_zh': '近期视觉-语言系统的发展提高了医学影像问答（Radiological VQA）模型的准确性。然而，在模型开发的每个阶段仍存在一些挑战：专家标注图像的限制阻碍了大规模数据采集；医学影像中的复杂和细微特征使得建模工作本身变得困难；缺乏评估努力使得难以识别模型可能失效的情况。在本研究中，我们微调了一个轻量级的3B参数视觉-语言模型用于医学影像问答，证明了适当调整的小模型在开放性和封闭性问题上均能实现稳健的性能。我们提出了一种成本效益高的训练管道，从合成问题-答案对生成到多阶段针对特定医学影像领域数据集的微调。结果显示，尽管我们的模型运行规模远小于LLaVA-Med等先进模型，但在参数规模有限和训练数据规模有限的情况下，仍能取得令人鼓舞的性能。我们引入了一种轻量级的基于显著性的诊断工具，允许领域专家通过显著性分析检查VQA模型的性能并识别潜在的失效模式。', 'title_zh': '适配轻量级视觉语言模型的医学影像视觉问答'}
{'arxiv_id': 'arXiv:2506.14440', 'title': 'Model compression using knowledge distillation with integrated gradients', 'authors': 'David E. Hernandez, Jose Chang, Torbjörn E. M. Nordling', 'link': 'https://arxiv.org/abs/2506.14440', 'abstract': "Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy.", 'abstract_zh': '一种结合集成梯度的知识蒸馏方法在资源受限设备上部署深度学习模型的关键技术', 'title_zh': '使用集成梯度的知识蒸馏模型压缩'}
{'arxiv_id': 'arXiv:2506.14438', 'title': 'sHGCN: Simplified hyperbolic graph convolutional neural networks', 'authors': 'Pol Arévalo, Alexis Molina, Álvaro Ciudad', 'link': 'https://arxiv.org/abs/2506.14438', 'abstract': 'Hyperbolic geometry has emerged as a powerful tool for modeling complex, structured data, particularly where hierarchical or tree-like relationships are present. By enabling embeddings with lower distortion, hyperbolic neural networks offer promising alternatives to Euclidean-based models for capturing intricate data structures. Despite these advantages, they often face performance challenges, particularly in computational efficiency and tasks requiring high precision. In this work, we address these limitations by simplifying key operations within hyperbolic neural networks, achieving notable improvements in both runtime and performance. Our findings demonstrate that streamlined hyperbolic operations can lead to substantial gains in computational speed and predictive accuracy, making hyperbolic neural networks a more viable choice for a broader range of applications.', 'abstract_zh': '双曲几何已成为建模复杂结构数据的强大工具，尤其是在存在层次或树形关系的情况下。通过提供较低失真的嵌入，双曲神经网络为捕捉复杂数据结构提供了欧几里得模型的有前途的替代方案。尽管具有这些优势，它们通常在计算效率和需要高精度的任务方面面临性能挑战。在本文中，我们通过简化双曲神经网络中的关键操作，实现了显著的运行时和性能改进。我们的研究表明，优化的双曲操作可以带来計算速度和预测准确性上的重大提升，使双曲神经网络成为更广泛应用的可行选择。', 'title_zh': 'SHGCN: 简化双曲图卷积神经网络'}
{'arxiv_id': 'arXiv:2506.14434', 'title': 'Unifying Streaming and Non-streaming Zipformer-based ASR', 'authors': 'Bidisha Sharma, Karthik Pandia Durai, Shankar Venkatesan, Jeena J Prakash, Shashi Kumar, Malolan Chetlur, Andreas Stolcke', 'link': 'https://arxiv.org/abs/2506.14434', 'abstract': 'There has been increasing interest in unifying streaming and non-streaming automatic speech recognition (ASR) models to reduce development, training, and deployment costs. We present a unified framework that trains a single end-to-end ASR model for both streaming and non-streaming applications, leveraging future context information. We propose to use dynamic right-context through the chunked attention masking in the training of zipformer-based ASR models. We demonstrate that using right-context is more effective in zipformer models compared to other conformer models due to its multi-scale nature. We analyze the effect of varying the number of right-context frames on accuracy and latency of the streaming ASR models. We use Librispeech and large in-house conversational datasets to train different versions of streaming and non-streaming models and evaluate them in a production grade server-client setup across diverse testsets of different domains. The proposed strategy reduces word error by relative 7.9\\% with a small degradation in user-perceived latency. By adding more right-context frames, we are able to achieve streaming performance close to that of non-streaming models. Our approach also allows flexible control of the latency-accuracy tradeoff according to customers requirements.', 'abstract_zh': '统一处理流式和非流式自动语音识别模型以降低开发、训练和部署成本的研究', 'title_zh': '基于Zipformer的统一流式与非流式ASR'}
{'arxiv_id': 'arXiv:2506.14425', 'title': 'Is Selection All You Need in Differential Evolution?', 'authors': 'Tomofumi Kitamura, Alex Fukunaga', 'link': 'https://arxiv.org/abs/2506.14425', 'abstract': 'Differential Evolution (DE) is a widely used evolutionary algorithm for black-box optimization problems. However, in modern DE implementations, a major challenge lies in the limited population diversity caused by the fixed population size enforced by the generational replacement. Population size is a critical control parameter that significantly affects DE performance. Larger populations inherently contain a more diverse set of individuals, thereby facilitating broader exploration of the search space. Conversely, when the maximum evaluation budgets is constrained, smaller populations focusing on a limited number of promising candidates may be more suitable. Many state-of-the-art DE variants incorporate an archive mechanism, in which a subset of discarded individuals is preserved in an archive during generation replacement and reused in mutation operations. However, maintaining what is essentially a secondary population via an archive introduces additional design considerations, such as policies for insertion, deletion, and appropriate sizing. To address these limitations, we propose a novel DE framework called Unbounded Differential Evolution (UDE), which adds all generated candidates to the population without discarding any individual based on fitness. Unlike conventional DE, which removes inferior individuals during generational replacement, UDE eliminates replacement altogether, along with the associated complexities of archive management and dynamic population sizing. UDE represents a fundamentally new approach to DE, relying solely on selection mechanisms and enabling a more straightforward yet powerful search algorithm.', 'abstract_zh': '无界差分进化（UDE）：一种新的差分进化框架', 'title_zh': '差分进化中仅选择是否足够？'}
{'arxiv_id': 'arXiv:2506.14418', 'title': 'Compositional Attribute Imbalance in Vision Datasets', 'authors': 'Jiayi Chen, Yanbiao Ma, Andi Zhang, Weidong Tang, Wei Dai, Bowei Liu', 'link': 'https://arxiv.org/abs/2506.14418', 'abstract': "Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks.", 'abstract_zh': '视觉属性不平衡是图像分类中一个常见但尚未充分探索的问题，显著影响模型性能和泛化能力。在本工作中，我们首先定义图像的一级和二级属性，然后提出一种基于CLIP的框架构建视觉属性字典，从而实现图像属性的自动化评估。通过对单一属性不平衡和组合属性不平衡的系统性分析，我们揭示了属性稀有性如何影响模型性能。为了应对这些挑战，我们提议根据组合属性的稀有性调整样本的采样概率。这一策略进一步与各种数据增强技术（如CutMix、Fmix和SaliencyMix）集成，以增强模型表示稀有属性的能力。基准数据集上的广泛实验表明，我们的方法有效地缓解了属性不平衡问题，从而提高了深度神经网络的鲁棒性和公平性。我们的研究强调了建模视觉属性分布的重要性，并提供了一种可扩展的长尾图像分类任务解决方案。', 'title_zh': '视觉数据集中的组合属性失衡'}
{'arxiv_id': 'arXiv:2506.14412', 'title': 'RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition', 'authors': 'Tim Cofala, Oleh Astappiev, William Xion, Hailay Teklehaymanot', 'link': 'https://arxiv.org/abs/2506.14412', 'abstract': "Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR 2025 LiveRAG Challenge.", 'abstract_zh': 'Retrieval-Augmented Generation (RAG)增强了大型语言模型（LLMs）的能力，通过将其内部的参数化知识与外部的非参数化来源相结合，旨在提高事实的准确性和减少幻觉。LiveRAG 2025挑战赛探索了RAG解决方案，以最大化DataMorgana的QA配对数据集上的准确性，该数据集由单跳和多跳问题组成。该挑战提供了稀疏的OpenSearch和密集的Pinecone索引，限制模型使用参数不超过10亿的LLM，并且最终答案生成使用Falcon-3-10B。裁判LLM和人类评估者评估提交的答案。在挑战条件下探索不同的检索器组合和RAG解决方案，我们最终的解决方案使用了InstructRAG与Pinecone检索器和BGE重排序器的组合。我们的解决方案取得了准确率为1.13、忠实度为0.55的成绩，在SIGIR 2025 LiveRAG挑战赛中排名第四。', 'title_zh': 'RAGtifier: 评估最新RAG系统中RAG生成方法的SIGIR LiveRAG竞赛zzle'}
{'arxiv_id': 'arXiv:2506.14411', 'title': 'Adaptive Reinforcement Learning for Unobservable Random Delays', 'authors': 'John Wikman, Alexandre Proutiere, David Broman', 'link': 'https://arxiv.org/abs/2506.14411', 'abstract': 'In standard Reinforcement Learning (RL) settings, the interaction between the agent and the environment is typically modeled as a Markov Decision Process (MDP), which assumes that the agent observes the system state instantaneously, selects an action without delay, and executes it immediately. In real-world dynamic environments, such as cyber-physical systems, this assumption often breaks down due to delays in the interaction between the agent and the system. These delays can vary stochastically over time and are typically unobservable, meaning they are unknown when deciding on an action. Existing methods deal with this uncertainty conservatively by assuming a known fixed upper bound on the delay, even if the delay is often much lower. In this work, we introduce the interaction layer, a general framework that enables agents to adaptively and seamlessly handle unobservable and time-varying delays. Specifically, the agent generates a matrix of possible future actions to handle both unpredictable delays and lost action packets sent over networks. Building on this framework, we develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA), which dynamically adjusts to delay patterns. Our method significantly outperforms state-of-the-art approaches across a wide range of locomotion benchmark environments.', 'abstract_zh': '在标准强化学习设置中，智能体与环境的交互通常被建模为马尔可夫决策过程（MDP），假设智能体能够即时观察系统状态、无延迟地选择动作并立即执行。在现实世界的动态环境中，如网络物理系统，这种假设由于交互中的延迟往往会失效。这些延迟会随时间随机变化，通常不可观测，即在决定动作时无法获知。现有方法通过假设已知的固定延迟上限保守地处理这种不确定性，即使延迟往往更低。本文引入了交互层，这是一种通用框架，使智能体能够适应性地处理不可观测的、随时间变化的延迟。具体而言，智能体生成一张可能的未来动作矩阵，以处理不可预测的延迟和网络中传输的动作包丢失。基于此框架，我们开发了一种基于模型的算法——带延迟适应的Actor-Critic（ACDA），该算法能够动态适应延迟模式。我们的方法在一系列动力学基准环境中显著优于现有最先进的方法。', 'title_zh': '自适应强化学习处理不可观测随机延迟'}
{'arxiv_id': 'arXiv:2506.14407', 'title': 'ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge', 'authors': 'Zeinab Sadat Taghavi, Ali Modarressi, Yunpu Ma, Hinrich Schütze', 'link': 'https://arxiv.org/abs/2506.14407', 'abstract': 'Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving "two days ago"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at this http URL.', 'abstract_zh': '检索系统是许多NLP管道的核心，但often依赖于关键词重叠和词义表面相似性等表面级线索。为了超越这些浅层信号评估检索，近期基准引入了重 reasoning 的查询；然而，它们主要将负担转移到查询端处理技术上——如提示或多跳检索——这些技术可以帮助解决复杂性。相比之下，我们提出了ImpliRet基准，将 reasoning 挑战转移到文档端处理：查询简单，但相关性取决于文档中通过时间（例如，“两天前”）、算术和世界知识关系隐含陈述的事实。我们评估了稀疏和密集检索器的各种方法，所有方法在这项任务中都表现不佳：最佳nDCG@10仅为15.07%。我们还测试了长语境模型是否可以克服这一限制。即使只有包括正文档在内的十个文档的短语境，GPT-4.1的得分也只有35.06%，表明文档端 reasoning 仍然是一项挑战。我们的代码可在以下网址获取。', 'title_zh': '隐含事实检索挑战的基准测试'}
{'arxiv_id': 'arXiv:2506.14404', 'title': 'Causally Steered Diffusion for Automated Video Counterfactual Generation', 'authors': 'Nikos Spyrou, Athanasios Vlontzos, Paraskevas Pegios, Thomas Melistas, Nefeli Gkouti, Yannis Panagakis, Giorgos Papanastasiou, Sotirios A. Tsaftaris', 'link': 'https://arxiv.org/abs/2506.14404', 'abstract': 'Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic "what-if" video scenarios in diverse areas such as healthcare and digital media.', 'abstract_zh': '基于视觉语言模型的因果忠实视频生成框架', 'title_zh': '因果引导的扩散模型用于自动视频反事实生成'}
{'arxiv_id': 'arXiv:2506.14399', 'title': 'Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models', 'authors': 'Tian Xia, Fabio De Sousa Ribeiro, Rajat R Rasal, Avinash Kori, Raghav Mehta, Ben Glocker', 'link': 'https://arxiv.org/abs/2506.14399', 'abstract': 'Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.', 'abstract_zh': '去因果干预下的对抗生成图像旨在模拟特定因果干预下的现实视觉结果。扩散模型最近成为这一任务的强大工具，通过分类器无指导（Classifier-Free Guidance，CFG）结合DDIM反向生成。然而，标准的CFG对所有条件变量应用单一全局权重，可能导致身份保真度差和虚假属性变化——这种现象被称为属性放大。为解决这一问题，我们提出去耦合分类器无指导（Decoupled Classifier-Free Guidance，DCFG），这是一种灵活且模型无关的框架，引入了分组条件控制。DCFG 基于属性拆分嵌入策略，将语义输入解耦，使用户可以在定义的属性组上进行选择性指导。对于反事实生成，我们根据因果图将属性分为干预集和不变集，并对每个集应用不同的指导。实验表明，DCFG 提高了干预的准确性，减少了无意中的变化，并增强了可逆性，从而实现更忠实和可解释的反事实图像生成。', 'title_zh': '解耦的分类器免费引导-counter事实扩散模型'}
{'arxiv_id': 'arXiv:2506.14391', 'title': 'HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control', 'authors': 'Yaqiao Zhu, Hongkai Wen, Geyong Min, Man Luo', 'link': 'https://arxiv.org/abs/2506.14391', 'abstract': 'Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers from scalability issues, while decentralized approaches often lack unified objectives, resulting in limited network-level efficiency. In this paper, we propose HiLight, a hierarchical reinforcement learning framework with global adversarial guidance for large-scale TSC. HiLight consists of a high-level Meta-Policy, which partitions the traffic network into subregions and generates sub-goals using a Transformer-LSTM architecture, and a low-level Sub-Policy, which controls individual intersections with global awareness. To improve the alignment between global planning and local execution, we introduce an adversarial training mechanism, where the Meta-Policy generates challenging yet informative sub-goals, and the Sub-Policy learns to surpass these targets, leading to more effective coordination. We evaluate HiLight across both synthetic and real-world benchmarks, and additionally construct a large-scale Manhattan network with diverse traffic conditions, including peak transitions, adverse weather, and holiday surges. Experimental results show that HiLight exhibits significant advantages in large-scale scenarios and remains competitive across standard benchmarks of varying sizes.', 'abstract_zh': '高效的交通信号控制（TSC）对于缓解城市拥堵至关重要，然而现有的强化学习（RL）方法在扩展到大规模网络时难以同时保持全局协调。集中式RL面临可扩展性问题，而分布式方法往往缺乏统一的目标，导致网络级效率有限。在本文中，我们提出了一种名为HiLight的层次化强化学习框架，该框架包含全局对抗指导。HiLight由一个高层元策略和一个低层子策略组成，前者利用Transformer-LSTM架构将交通网络分区并生成子目标，后者具有全局意识，控制个体交叉口。为提高全局规划与局部执行之间的对齐，我们引入了对抗训练机制，其中元策略生成既具有挑战性又提供信息的子目标，子策略学习超越这些目标，从而实现更有效的协调。我们在合成和现实世界的基准上评估了HiLight，并构建了一个包含多样交通条件的大规模曼哈顿网络，包括高峰时段过渡、恶劣天气和假期高峰。实验结果表明，HiLight在大规模场景中表现出显著优势，并且在不同规模的标准基准上仍具有竞争力。', 'title_zh': 'HiLight：一种具有全局对抗性指导的分层强化学习框架，用于大规模交通信号控制'}
{'arxiv_id': 'arXiv:2506.14386', 'title': 'ResNets Are Deeper Than You Think', 'authors': 'Christian H.X. Ali Mehmeti-Göpel, Michael Wand', 'link': 'https://arxiv.org/abs/2506.14386', 'abstract': 'Residual connections remain ubiquitous in modern neural network architectures nearly a decade after their introduction. Their widespread adoption is often credited to their dramatically improved trainability: residual networks train faster, more stably, and achieve higher accuracy than their feedforward counterparts. While numerous techniques, ranging from improved initialization to advanced learning rate schedules, have been proposed to close the performance gap between residual and feedforward networks, this gap has persisted. In this work, we propose an alternative explanation: residual networks do not merely reparameterize feedforward networks, but instead inhabit a different function space. We design a controlled post-training comparison to isolate generalization performance from trainability; we find that variable-depth architectures, similar to ResNets, consistently outperform fixed-depth networks, even when optimization is unlikely to make a difference. These results suggest that residual connections confer performance advantages beyond optimization, pointing instead to a deeper inductive bias aligned with the structure of natural data.', 'abstract_zh': '残差连接在近十年后仍然在现代神经网络架构中无处不在。它们的广泛采用通常归功于显著提高的可训练性：残差网络训练更快、更稳定，并且准确率高于其前向传播 counterparts。虽然已经提出了许多技术，从改进的初始化到先进的学习率调度，以缩小残差网络和前向传播网络之间的性能差距，但这一差距一直存在。在本工作中，我们提出了一种替代解释：残差连接不仅重新参数化前向传播网络，而是存在于不同的函数空间中。我们设计了一个受控的后训练比较以隔离泛化性能与可训练性；我们发现，类似ResNets的可变深度架构始终优于固定深度网络，即使优化不太可能产生影响。这些结果表明，残差连接提供了超越优化的性能优势，而是指向与自然数据结构一致的更深层次的归纳偏置。', 'title_zh': 'ResNets 深于你所想象'}
{'arxiv_id': 'arXiv:2506.14382', 'title': 'DepthSeg: Depth prompting in remote sensing semantic segmentation', 'authors': 'Ning Zhou, Shanxiong Chen, Mingting Zhou, Haigang Sui, Lieyun Hu, Han Li, Li Hua, Qiming Zhou', 'link': 'https://arxiv.org/abs/2506.14382', 'abstract': 'Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.', 'abstract_zh': '深度提示二维遥感语义分割框架（DepthSeg）：缓解光谱混淆和阴影遮挡', 'title_zh': 'DepthSeg: 远景遥感语义分割中的深度提示'}
{'arxiv_id': 'arXiv:2506.14375', 'title': 'IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards', 'authors': 'Muhammad Hamza Yousuf, Jason Li, Sahar Vahdati, Raphael Theilen, Jakob Wittenstein, Jens Lehmann', 'link': 'https://arxiv.org/abs/2506.14375', 'abstract': 'Invasive mechanical ventilation (MV) is a life-sustaining therapy for critically ill patients in the intensive care unit (ICU). However, optimizing its settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for MV control, current stateof-the-art (SOTA) methods struggle with the hybrid (continuous and discrete) nature of MV actions. Discretizing the action space limits available actions due to exponential growth in combinations and introduces distribution shifts that can compromise safety. In this paper, we propose optimizations that build upon prior work in action space reduction to address the challenges of discrete action spaces. We also adapt SOTA offline RL algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby avoiding the pitfalls of discretization. Additionally, we introduce a clinically grounded reward function based on ventilator-free days and physiological targets, which provides a more meaningful optimization objective compared to traditional sparse mortality-based rewards. Our findings demonstrate that AI-assisted MV optimization may enhance patient safety and enable individualized lung support, representing a significant advancement toward intelligent, data-driven critical care solutions.', 'abstract_zh': '侵入性机械通气(MV)是重症监护单元(ICU)中重症患者的生命维持疗法。然而，优化其设置仍然是一个复杂且容易出错的过程，由于患者特定的变异性所致。尽管离线强化学习(OFFLINE RL)在MV控制方面显示出潜力，但当前最先进的(SOTA)方法难以应对MV动作的混合（连续和离散）特性。离散化动作空间会限制可用动作，由于组合数的指数增长而产生，并且引入可能损害安全性的分布偏移。在本文中，我们提出了基于先前动作空间缩减工作的优化方法，以解决离散动作空间的挑战。我们还适应当前最先进的离线RL算法（IQL和EDAC），使其可以直接在混合动作空间上运行，从而避免离散化带来的问题。此外，我们引入了一种基于临床目标（如无通气日数和生理目标）的奖励函数，相比于传统的稀疏死亡率基线奖励，提供了更有意义的优化目标。我们的研究结果表明，人工智能辅助的MV优化可能增强患者安全，并实现个性化的肺支持，代表了智能、数据驱动的重症监护解决方案的重要进步。', 'title_zh': 'IntelliLung: 利用混合动作和临床对齐奖励的离线强化学习提升安全机械通气'}
{'arxiv_id': 'arXiv:2506.14356', 'title': 'EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization', 'authors': 'Xiaoqi Wang, Yi Wang, Lap-Pui Chau', 'link': 'https://arxiv.org/abs/2506.14356', 'abstract': 'Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at this https URL .', 'abstract_zh': '自我中心视频-语言理解既要求高效性也要求精确的空间-时间建模。现有方法面临三个关键挑战：1）多阶段预训练管道引起的 excessive pre-training 成本，2）由于手动分割的 3D 旋转位置嵌入导致的空间-时间编码无效，这阻碍了特征交互，3）软标签多实例检索中的不精确学习目标，忽略了负样本对之间的关联。本文介绍了基于 EVA02 的 EVA02-AT，这是一个专用于自我中心视频理解任务的 EVA02 基础模型系列。EVA02-AT 首先通过单阶段预训练高效地将基于图像的 CLIP 模型转换为统一的视频编码器。其次，我们引入了空间-时间旋转位置嵌入与联合注意相结合的方法，可以有效地在整个隐藏维度上编码空间和时间信息。这种联合编码的空间-时间特征使得模型能够在空间和时间轴之间学习交叉关系，这对于准确建模视频中的运动和交互至关重要。第三，针对多实例视频-语言检索任务，我们引入了对称多相似性（SMS）损失和一种新的训练框架，可以同时提高正样本和负样本的软标签，提供更精确的学习目标。在 Ego4D、EPIC-Kitchens-100 和 Charades-Ego 下的零样本和微调设置下的广泛实验表明，EVA02-AT 在各种自我中心视频-语言任务中的性能达到领先水平，且参数量更少。使用我们 SMS 损失的模型在多实例检索基准上的性能也显著提高。我们的代码和模型可在以下网址公开获取：this https URL。', 'title_zh': 'EVA02-AT：基于空间-时间旋转位置嵌入和对称优化的自视角视频-语言理解'}
{'arxiv_id': 'arXiv:2506.14337', 'title': 'LLM-Powered Intent-Based Categorization of Phishing Emails', 'authors': 'Even Eilertsen, Vasileios Mavroeidis, Gudmund Grov', 'link': 'https://arxiv.org/abs/2506.14337', 'abstract': 'Phishing attacks remain a significant threat to modern cybersecurity, as they successfully deceive both humans and the defense mechanisms intended to protect them. Traditional detection systems primarily focus on email metadata that users cannot see in their inboxes. Additionally, these systems struggle with phishing emails, which experienced users can often identify empirically by the text alone. This paper investigates the practical potential of Large Language Models (LLMs) to detect these emails by focusing on their intent. In addition to the binary classification of phishing emails, the paper introduces an intent-type taxonomy, which is operationalized by the LLMs to classify emails into distinct categories and, therefore, generate actionable threat information. To facilitate our work, we have curated publicly available datasets into a custom dataset containing a mix of legitimate and phishing emails. Our results demonstrate that existing LLMs are capable of detecting and categorizing phishing emails, underscoring their potential in this domain.', 'abstract_zh': '网络钓鱼攻击仍然是现代网络安全的重要威胁，因为它们能够成功地欺骗人类用户和旨在保护它们的防御机制。传统的检测系统主要依赖于用户在收件箱中看不到的邮件元数据。此外，这些系统难以识别经验丰富的用户仅通过邮件文本就能识别的网络钓鱼邮件。本文探讨了大型语言模型（LLMs）在这种情况下检测邮件的实际潜力，重点关注邮件的意图。除了对网络钓鱼邮件进行二元分类外，本文还引入了一种意图类型分类体系，通过LLMs将邮件分类到不同的类别中，从而生成可操作的威胁信息。为了支持我们的工作，我们整理了公开可用的数据集，创建了一个自定义数据集，包含合法和网络钓鱼邮件的混编。我们的结果显示，现有的LLMs能够检测和分类网络钓鱼邮件，突显了它们在该领域的潜力。', 'title_zh': 'LLM 助力基于意图的钓鱼邮件分类'}
{'arxiv_id': 'arXiv:2506.14329', 'title': 'Adjustment for Confounding using Pre-Trained Representations', 'authors': 'Rickmer Schulte, David Rügamer, Thomas Nagler', 'link': 'https://arxiv.org/abs/2506.14329', 'abstract': 'There is growing interest in extending average treatment effect (ATE) estimation to incorporate non-tabular data, such as images and text, which may act as sources of confounding. Neglecting these effects risks biased results and flawed scientific conclusions. However, incorporating non-tabular data necessitates sophisticated feature extractors, often in combination with ideas of transfer learning. In this work, we investigate how latent features from pre-trained neural networks can be leveraged to adjust for sources of confounding. We formalize conditions under which these latent features enable valid adjustment and statistical inference in ATE estimation, demonstrating results along the example of double machine learning. We discuss critical challenges inherent to latent feature learning and downstream parameter estimation arising from the high dimensionality and non-identifiability of representations. Common structural assumptions for obtaining fast convergence rates with additive or sparse linear models are shown to be unrealistic for latent features. We argue, however, that neural networks are largely insensitive to these issues. In particular, we show that neural networks can achieve fast convergence rates by adapting to intrinsic notions of sparsity and dimension of the learning problem.', 'abstract_zh': '非表格数据的潜特征在平均处理效应估计中的应用：转移学习与泛化的挑战', 'title_zh': '使用预训练表示调整混杂因素'}
{'arxiv_id': 'arXiv:2506.14303', 'title': 'orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous Generation of Surgical Images and Ground Truth Labels', 'authors': 'Niran Nataraj, Maina Sogabe, Kenji Kawashima', 'link': 'https://arxiv.org/abs/2506.14303', 'abstract': 'Deep learning in medical imaging faces obstacles: limited data diversity, ethical issues, high acquisition costs, and the need for precise annotations. Bleeding detection and localization during surgery is especially challenging due to the scarcity of high-quality datasets that reflect real surgical scenarios. We propose orGAN, a GAN-based system for generating high-fidelity, annotated surgical images of bleeding. By leveraging small "mimicking organ" datasets, synthetic models that replicate tissue properties and bleeding, our approach reduces ethical concerns and data-collection costs. orGAN builds on StyleGAN with Relational Positional Learning to simulate bleeding events realistically and mark bleeding coordinates. A LaMa-based inpainting module then restores clean, pre-bleed visuals, enabling precise pixel-level annotations. In evaluations, a balanced dataset of orGAN and mimicking-organ images achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy. While our development data lack diverse organ morphologies and contain intraoperative artifacts, orGAN markedly advances ethical, efficient, and cost-effective creation of realistic annotated bleeding datasets, supporting broader integration of AI in surgical practice.', 'abstract_zh': '深度学习在医学影像中面临障碍：数据多样性有限、伦理问题、高获取成本以及精确标注需求。手术中出血检测与定位尤为挑战，由于缺乏能反映实际手术场景的高质量数据集。我们提出orGAN，一种基于GAN的系统，用于生成高保真度、标注过的手术出血图像。通过利用小型“模拟器官”数据集，模拟具有组织属性和出血特征的合成模型，该方法减轻了伦理问题和数据收集成本。orGAN 基于具有关系位置学习的StyleGAN模拟真实的出血事件并在出血位置进行标记。基于LaMa的修补模块随后恢复了出血前的干净视觉效果，实现精确的像素级标注。在评估中，orGAN和模拟器官图像的均衡数据集在手术设置中实现了90%的检测准确率，并在帧级达到了99%的准确率。尽管我们的开发数据缺乏多样化的器官形态并包含术中伪影，但orGAN显著促进了伦理、高效且成本效益高的真实标注出血数据集的创建，支持更广泛地将AI集成到手术实践中。', 'title_zh': 'orGAN：一种同时生成手术图像和ground truth标签的合成数据增强管道'}
{'arxiv_id': 'arXiv:2506.14294', 'title': 'Uncertainty-Driven Radar-Inertial Fusion for Instantaneous 3D Ego-Velocity Estimation', 'authors': 'Prashant Kumar Rai, Elham Kowsari, Nataliya Strokina, Reza Ghabcheloo', 'link': 'https://arxiv.org/abs/2506.14294', 'abstract': "We present a method for estimating ego-velocity in autonomous navigation by integrating high-resolution imaging radar with an inertial measurement unit. The proposed approach addresses the limitations of traditional radar-based ego-motion estimation techniques by employing a neural network to process complex-valued raw radar data and estimate instantaneous linear ego-velocity along with its associated uncertainty. This uncertainty-aware velocity estimate is then integrated with inertial measurement unit data using an Extended Kalman Filter. The filter leverages the network-predicted uncertainty to refine the inertial sensor's noise and bias parameters, improving the overall robustness and accuracy of the ego-motion estimation. We evaluated the proposed method on the publicly available ColoRadar dataset. Our approach achieves significantly lower error compared to the closest publicly available method and also outperforms both instantaneous and scan matching-based techniques.", 'abstract_zh': '我们提出了一种通过结合高分辨率成像雷达和惯性测量单元来估计 ego-速度的方法。该提出的approach通过运用神经网络处理复值原始雷达数据，并估计瞬时线性 ego-速度及其相关的不确定性来解决传统基于雷达的 ego-运动估计方法的局限性。这种不确定性意识的速度估计随后与惯性测量单元的数据结合使用扩展卡尔曼滤波器。滤波器利用网络预测的不确定性来改进惯性传感器的噪声和偏差参数，从而提高整体的鲁棒性和 ego-运动估计的准确性。我们在公开可用的 ColoRadar 数据集上评估了提出的方法。我们的方法在误差上显著低于现有最接近的方法，并且在瞬时速度估计和基于扫描匹配的技术方面都表现出更好的性能。', 'title_zh': '基于不确定性驱动的雷达-惯性融合即时3D ego速度估计'}
{'arxiv_id': 'arXiv:2506.14287', 'title': 'Steering Robots with Inference-Time Interactions', 'authors': 'Yanwei Wang', 'link': 'https://arxiv.org/abs/2506.14287', 'abstract': 'Imitation learning has driven the development of generalist policies capable of autonomously solving multiple tasks. However, when a pretrained policy makes errors during deployment, there are limited mechanisms for users to correct its behavior. While collecting additional data for finetuning can address such issues, doing so for each downstream use case is inefficient at deployment. My research proposes an alternative: keeping pretrained policies frozen as a fixed skill repertoire while allowing user interactions to guide behavior generation toward user preferences at inference time. By making pretrained policies steerable, users can help correct policy errors when the model struggles to generalize-without needing to finetune the policy. Specifically, I propose (1) inference-time steering, which leverages user interactions to switch between discrete skills, and (2) task and motion imitation, which enables user interactions to edit continuous motions while satisfying task constraints defined by discrete symbolic plans. These frameworks correct misaligned policy predictions without requiring additional training, maximizing the utility of pretrained models while achieving inference-time user objectives.', 'abstract_zh': '预训练策略冻结与用户引导行为生成：一种纠正策略错误的替代方法', 'title_zh': '在推理时交互式控制机器人'}
{'arxiv_id': 'arXiv:2506.14280', 'title': 'Improving LoRA with Variational Learning', 'authors': 'Bai Cong, Nico Daheim, Yuesong Shen, Rio Yokota, Mohammad Emtiyaz Khan, Thomas Möllenhoff', 'link': 'https://arxiv.org/abs/2506.14280', 'abstract': 'Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning.', 'abstract_zh': '贝叶斯方法 recently被用于改善LoRA微调，尽管它们提高了校准性，但对其他指标（如准确性）的影响 marginal且有时甚至是有害的。此外，贝叶斯方法还会增加计算开销，并需要额外的技巧才能充分发挥作用。我们通过使用最近提出的一种变分算法IVON来解决这些问题。我们证明IVON易于实现，其成本与AdamW相当，但通过一种简单的后验剪枝技术，它可以显著改善许多指标。我们在大规模LLM（Llama和Qwen系列）上进行了详尽的实验，超越了现有IVON应用的规模。例如，我们在一套常识推理任务上微调了一个Llama-3.2-3B模型，并在准确性上比AdamW提高了1.3%，减少了5.4%的ECE，超过了AdamW和Laplace-LoRA、BLoB等其他最近的贝叶斯方法。总体而言，我们的结果表明，使用IVON的变分学习可以有效改善LoRA微调。', 'title_zh': '基于变分学习提升LoRA'}
{'arxiv_id': 'arXiv:2506.14262', 'title': 'Knowledge Adaptation as Posterior Correction', 'authors': 'Mohammad Emtiyaz Khan', 'link': 'https://arxiv.org/abs/2506.14262', 'abstract': "Adaptation is the holy grail of intelligence, but even the best AI models (like GPT) lack the adaptivity of toddlers. So the question remains: how can machines adapt quickly? Despite a lot of progress on model adaptation to facilitate continual and federated learning, as well as model merging, editing, unlearning, etc., little is known about the mechanisms by which machines can naturally learn to adapt in a similar way as humans and animals. Here, we show that all such adaptation methods can be seen as different ways of `correcting' the approximate posteriors. More accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation. The result is obtained by using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023) where interference created during adaptation is characterized by the natural-gradient mismatch over the past data. We present many examples to demonstrate the use of posterior-correction as a natural mechanism for the machines to learn to adapt quickly.", 'abstract_zh': '适应是智能的圣杯，但即使是最优秀的AI模型（如GPT）也缺乏学龄前儿童的适应性。因此，问题依然存在：机器如何能够快速适应？尽管在模型适应以促进连续学习和联邦学习、模型合并、编辑和遗忘等方面取得了很大进展，但仍不清楚机器是如何自然学习以类似人类和动物的方式适应的机制。在这里，我们显示所有这些适应方法都可以被视为纠正近似后验概率的不同方式。更准确的后验概率导致更小的纠正，进而意味着更快的适应。该结果通过使用Khan和Rue（2023）的贝叶斯学习规则的双重视角获得，其中适应过程中产生的干扰通过过去数据的自然梯度不匹配来表征。我们通过许多例子展示了后验纠正作为一种自然机制，使机器能够快速学习适应。', 'title_zh': '知识适应作为后验修正'}
{'arxiv_id': 'arXiv:2506.14248', 'title': 'Re-Initialization Token Learning for Tool-Augmented Large Language Models', 'authors': 'Chenghao Li, Liu Liu, Baosheng Yu, Jiayan Qiu, Yibing Zhan', 'link': 'https://arxiv.org/abs/2506.14248', 'abstract': "Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.", 'abstract_zh': '大型语言模型在展示出色表现的同时，对于数值推理和计划生成等复杂任务表现出不足。将外部工具，如计算器和数据库，集成到大型语言模型（LLMs）中对于提升问题解决能力至关重要。当前方法通过给每个工具分配一个独特的标记，并通过标记预测的方式（类似词生成）来调用工具，但这种方法未能考虑到工具标记与词标记之间的关系，限制了预训练LLMs的适应性。为解决这一问题，我们提出了一种新的标记学习方法，从初始化的角度将工具标记与现有的词嵌入空间对齐，从而提升模型性能。我们首先基于工具名称或描述构建先验工具标记嵌入，并将其用于初始化和正则化可学习的工具标记嵌入，以此确保学习到的嵌入与词标记空间良好对齐，提高工具调用准确性。我们通过使用GSM8K-XL、FuncQA、KAMEL和VirtualHome数据集来评估方法在数值推理、基于知识的问答和体态计划生成等任务上的性能，并且结果显示，该方法在Cot、React、ICL和ToolkenGPT等近期基线方法上都表现出了明显的改进，表明我们的方法有效地通过相关标记跨领域地增强了LLMs的功能。', 'title_zh': '增强工具支持的大语言模型的重初始化 token 学习'}
{'arxiv_id': 'arXiv:2506.14234', 'title': 'Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team', 'authors': 'Md Tanzib Hosain, Salman Rahman, Md Kishor Morol, Md Rizwan Parvez', 'link': 'https://arxiv.org/abs/2506.14234', 'abstract': "Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at this https URL.", 'abstract_zh': "尽管在复杂推理方面取得了显著进展，当前的大语言模型（LLMs）通常独立运行——处理每个问题时都不积累或整合经验知识。相比之下，诸如奥林匹克或者编程竞赛团队这样的专家解决问题者，会利用丰富的经验织锦：从教练那里吸收指导，从过去的问题中培养直觉，利用工具使用和库功能的知识，根据同伴的专业经验调整策略，在试错过程中不断精炼他们的推理，并学习其他相关问题的知识，即使在比赛期间也是如此。我们提出了一种无需训练的多agents推理框架Xolver，为黑盒LLM配备了持久且不断演化的综合经验记忆。Xolver整合了多种经验模态，包括外部和自我检索、工具使用、协作互动、agent驱动的评估以及迭代精炼。通过在推理时从相关策略、代码片段和抽象推理模式中学习，Xolver避免从头生成解决方案，从而从孤立推理转向经验感知的语言代理。基于开源和专有模型，Xolver在专门的推理代理中表现优异。即使使用轻量级的基础模型（例如QWQ-32B），它也经常超越包括Qwen3-235B、Gemini 2.5 Pro、o3和o4-mini-high在内的高级模型。使用o3-mini-high，它在GSM8K（98.1%）、AIME'24（94.4%）、AIME'25（93.7%）、Math-500（99.8%）和LiveCodeBench-V5（91.6%）上取得新的最佳成绩——强调全面经验学习是通向具备专家级推理能力的通才代理的关键一步。完整代码和数据可在以下链接获得。", 'title_zh': 'Xolver: 多智能体推理结合全方位经验学习犹如奥林匹克团队'}
{'arxiv_id': 'arXiv:2506.14229', 'title': 'HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction', 'authors': 'Changbai Li, Haodong Zhu, Hanlin Chen, Juan Zhang, Tongfei Chen, Shuo Yang, Shuwei Shao, Wenhao Dong, Baochang Zhang', 'link': 'https://arxiv.org/abs/2506.14229', 'abstract': '3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.', 'abstract_zh': '基于层次高斯斑点表示的高效率三维场景重建（Hierarchical Gaussian Splatting for Efficient High-Resolution 3D Scene Reconstruction）', 'title_zh': 'HRGS：基于层次高斯点云的高效高分辨率3D重建'}
{'arxiv_id': 'arXiv:2506.14217', 'title': 'TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift', 'authors': 'Dipesh Tharu Mahato, Rohan Poudel, Pramod Dhungana', 'link': 'https://arxiv.org/abs/2506.14217', 'abstract': 'Deep neural networks often achieve high accuracy, but ensuring their reliability under adversarial and distributional shifts remains a pressing challenge. We propose TriGuard, a unified safety evaluation framework that combines (1) formal robustness verification, (2) attribution entropy to quantify saliency concentration, and (3) a novel Attribution Drift Score measuring explanation stability. TriGuard reveals critical mismatches between model accuracy and interpretability: verified models can still exhibit unstable reasoning, and attribution-based signals provide complementary safety insights beyond adversarial accuracy. Extensive experiments across three datasets and five architectures show how TriGuard uncovers subtle fragilities in neural reasoning. We further demonstrate that entropy-regularized training reduces explanation drift without sacrificing performance. TriGuard advances the frontier in robust, interpretable model evaluation.', 'abstract_zh': '深度神经网络往往能够实现高 accuracy，但确保其在对抗性和分布移变情况下的可靠性仍然是一个迫切的挑战。我们提出了 TriGuard，一个统一的安全评估框架，该框架结合了（1）形式化的鲁棒性验证，（2）归因熵以量化显着性集中度，以及（3）一种新型的归因漂移评分以衡量解释稳定性。TriGuard 暴露了模型 accuracy 和可解释性之间的关键不匹配：经过验证的模型仍然可能出现不稳定的推理，而基于归因的信号提供了超越对抗准确性的补充安全见解。广泛的实验证实在三个数据集中和五个架构上如何借助 TriGuard 揭示神经推理的细微脆弱性。我们进一步证明，使用熵正则化的训练可以减少解释漂移而不牺牲性能。TriGuard 推动了鲁棒性和可解释性模型评估的边界。', 'title_zh': 'TriGuard: 使用归因熵、验证和漂移测试模型安全性'}
{'arxiv_id': 'arXiv:2506.14209', 'title': 'Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in Medical CBCT', 'authors': 'Pengwei Wang', 'link': 'https://arxiv.org/abs/2506.14209', 'abstract': 'Advances in treatment technology now allow for the use of customizable 3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN) of the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of 3D medical images using tools like nnUNet.\nHowever, the scarcity of labeled data in ONJ imaging makes supervised training impractical. This study aims to develop an unsupervised training approach for automatically identifying anomalies in imaging scans.\nWe propose a novel two-stage training pipeline. In the first stage, a VQ-GAN is trained to accurately reconstruct normal subjects. In the second stage, random cube masking and ONJ-specific masking are applied to train a new encoder capable of recovering the data.\nThe proposed method achieves successful segmentation on both simulated and real patient data.\nThis approach provides a fast initial segmentation solution, reducing the burden of manual labeling. Additionally, it has the potential to be directly used for 3D printing when combined with hand-tuned post-processing.', 'abstract_zh': 'Advances 在治疗技术的进步下，现在可以使用定制的 3D 打印水凝胶伤口敷料治疗颌骨放射性坏死 (ORN) 患者。同时，深度学习已经使得使用工具如 nnUNet 精确分割 3D 医学图像成为可能。\n然而，在颌骨放射性坏死 (ONJ) 图像中缺乏标注数据使监督训练变得 impractical。本研究旨在开发一种无监督训练方法，以自动识别成像扫描中的异常。\n我们提出了一种新的两阶段训练管道。在第一阶段，训练一个 VQ-GAN 准确重建正常个体。在第二阶段，应用随机立方体遮罩和 ONJ 特异性遮罩训练一个新编码器，使其能够恢复数据。\n所提出的方法在模拟和真实患者数据上均实现了成功的分割。\n此方法提供了一种快速的初始分割解决方案，减轻了手动标注的负担。此外，当与手工调整的后处理相结合时，它还具有直接用于 3D 打印的潜力。', 'title_zh': '潜在异常检测：掩码VQ-GAN在医学CBCT无监督分割中的应用'}
{'arxiv_id': 'arXiv:2506.14202', 'title': 'DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion', 'authors': 'Makoto Shing, Takuya Akiba', 'link': 'https://arxiv.org/abs/2506.14202', 'abstract': 'Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources.', 'abstract_zh': '使用端到端反向传播训练大型神经网络会导致显著的内存瓶颈，限制了对先进人工智能研究的访问。我们提出了一种新型训练框架 $\\textit{DiffusionBlocks}$，该框架将神经网络块解释为在连续时间扩散过程中执行去噪操作。通过将网络划分为独立训练的块，并基于等累计概率质量优化噪声水平分配，我们的方法在保持与传统反向传播竞争性能的同时实现了显著的内存效率。图像生成和语言建模任务的实验显示，内存减少量与块数成正比，并且性能更优。DiffusionBlocks 为在有限计算资源下 democratize 对大规模神经网络训练的访问提供了一条有前景的道路。', 'title_zh': '基于分数扩散的块级训练生成模型方法'}
{'arxiv_id': 'arXiv:2506.14196', 'title': "Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers", 'authors': 'Jiayue Melissa Shi, Keran Wang, Dong Whi Yoo, Ravi Karkar, Koustuv Saha', 'link': 'https://arxiv.org/abs/2506.14196', 'abstract': "Alzheimer's Disease and Related Dementias (AD/ADRD) are progressive neurodegenerative conditions that impair memory, thought processes, and functioning. Family caregivers of individuals with AD/ADRD face significant mental health challenges due to long-term caregiving responsibilities. Yet, current support systems often overlook the evolving nature of their mental wellbeing needs. Our study examines caregivers' mental wellbeing concerns, focusing on the practices they adopt to manage the burden of caregiving and the technologies they use for support. Through semi-structured interviews with 25 family caregivers of individuals with AD/ADRD, we identified the key causes and effects of mental health challenges, and developed a temporal mapping of how caregivers' mental wellbeing evolves across three distinct stages of the caregiving journey. Additionally, our participants shared insights into improvements for existing mental health technologies, emphasizing the need for accessible, scalable, and personalized solutions that adapt to caregivers' changing needs over time. These findings offer a foundation for designing dynamic, stage-sensitive interventions that holistically support caregivers' mental wellbeing, benefiting both caregivers and care recipients.", 'abstract_zh': '阿尔茨海默病及相关痴呆症的家庭照护者面临长期照护带来的心理健康挑战，现有支持系统往往未能关注他们不断变化的心理健康需求。本研究通过半结构化访谈25名阿尔茨海默病及相关痴呆症患者的家庭照护者，探讨了他们的心理健康问题，分析了他们管理照护负担的实践及使用支持技术的情况，识别了心理健康挑战的主要原因及其影响，并绘制了家庭照护者在照护旅程三个不同阶段心理健康变化的跨时间映射图谱。此外，参与者还提供了关于现有心理健康技术改进建议，强调需要灵活、可扩展且个性化解决方案的迫切需求，以适应照护者不断变化的需求。研究结果为设计动态、阶段敏感的干预措施提供了基础，以全面支持照护者的心理健康，惠及照护者和接受照护者。', 'title_zh': '平衡照护与自我照护：探索阿尔茨海默病和痴呆症照护者的心理健康需求'}
{'arxiv_id': 'arXiv:2506.14177', 'title': "Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages", 'authors': 'Tuan Nguyen, Huy-Dat Tran', 'link': 'https://arxiv.org/abs/2506.14177', 'abstract': 'Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry.', 'abstract_zh': '代码转换（CS）在多语言环境中广泛存在，由于语言复杂性导致标注数据稀缺且成本高，这对ASR构成了挑战。本研究探讨了使用合成CS数据构建CS-ASR的方法。我们提出了一种短语级混合方法来生成模拟自然模式的合成CS数据。利用单语言数据与合成短语混合的CS数据对大规模预训练ASR模型（Whisper、MMS、SeamlessM4T）进行微调。本文重点关注三种资源不足的东南亚语言对：马来-英语（BM-EN）、 Mandarin-马来（ZH-BM）和泰米尔-英语（TA-EN），建立了新的综合性CS-ASR基准，以评估顶级ASR模型的性能。实验结果表明，提出的训练策略在单语言和CS测试中提升了ASR性能，BM-EN表现出最高提升，随后是TA-EN和ZH-BM。这一发现为CS-ASR的发展提供了成本效益高的方法，有利于研究和产业。', 'title_zh': '我们能否在没有实际混杂数据的情况下训练ASR系统以处理混杂语言？以新加坡语言为案例的研究'}
{'arxiv_id': 'arXiv:2506.14175', 'title': 'GRAM: A Generative Foundation Reward Model for Reward Generalization', 'authors': 'Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Qiaozhi He, Murun Yang, Bei Li, Tong Xiao, Chunliang Zhang, Tongran Liu, Jingbo Zhu', 'link': 'https://arxiv.org/abs/2506.14175', 'abstract': 'In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.', 'abstract_zh': '在大规模语言模型对齐中使用标记和未标记数据训练奖励模型的方法探究', 'title_zh': 'GRAM: 一种生成式基础奖励模型用于奖励泛化'}
{'arxiv_id': 'arXiv:2506.14170', 'title': 'A multi-stage augmented multimodal interaction network for fish feeding intensity quantification', 'authors': 'Shulong Zhang, Mingyuan Yao, Jiayin Zhao, Xiao Liu, Haihua Wang', 'link': 'https://arxiv.org/abs/2506.14170', 'abstract': 'In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity.', 'abstract_zh': '多阶段增强多模态交互网络在水产养殖中评估鱼食量的研究', 'title_zh': '多阶段增强多模态交互网络在鱼类摄食强度量化中的应用'}
{'arxiv_id': 'arXiv:2506.14168', 'title': 'VideoMAR: Autoregressive Video Generatio with Continuous Tokens', 'authors': 'Hu Yu, Biao Gong, Hangjie Yuan, DanDan Zheng, Weilong Chai, Jingdong Chen, Kecheng Zheng, Feng Zhao', 'link': 'https://arxiv.org/abs/2506.14168', 'abstract': 'Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \\textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\\%$), training data ($0.5\\%$), and GPU resources ($0.2\\%$).', 'abstract_zh': '基于掩码的自回归模型在连续空间图像生成中展示了令人鼓舞的能力，但其在视频生成中的潜力仍被很大程度上忽视。本文提出VideoMAR，一种简洁高效的仅解码器自回归图像到视频模型，采用连续 token 构建时间帧到帧和空间掩码生成。首先，我们确定时间因果性和空间双向性作为视频自回归模型的基本原则，并提出下一帧扩散损失以结合掩码和视频生成。此外，长序列自回归建模的巨大成本和难度是一个基本但至关重要的问题。为此，我们提出时间从短到长的课程学习和空间逐步分辨率训练，并在推理时采用逐步温度策略以减轻累积误差。此外，VideoMAR 将语言模型的一些独特能力应用到视频生成中。它由于同时进行的时间维度 KV 缓存和空间维度并行生成而具有高效率，并通过3D 旋转嵌入展现时空外推能力。在VBench-I2V基准上，VideoMAR 在参数量（减少9.3%）、训练数据（减少0.5%）和 GPU 资源（减少0.2%）显著减少的情况下，超越了先前的最佳方法（Cosmos I2V）。', 'title_zh': 'VideoMAR：基于连续令牌的自回归视频生成'}
{'arxiv_id': 'arXiv:2506.14159', 'title': 'StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework', 'authors': 'Shayan Talaei, Meijin Li, Kanu Grover, James Kent Hippler, Diyi Yang, Amin Saberi', 'link': 'https://arxiv.org/abs/2506.14159', 'abstract': 'Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative, a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N=28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.', 'abstract_zh': '每一个个体都拥有一份独特且个人化的生活故事，由他们的记忆和经历共同塑造。然而，这些记忆往往零散且难以组织成一个连贯的叙事，这正是自传写作任务的一大挑战。现有的对话式写作助手往往依赖于通用的用户交互和预定义的指南，使得这些系统难以捕捉个人记忆并随着时间的发展构建一个完整的人生历程。我们介绍了StorySage，一个用户驱动的软件系统，旨在满足不同用户群体的需求，支持灵活的对话和结构化的自传写作方法。借助由访谈员、会话记录员、规划师、段落作家和会话协调组成的多智能体框架，我们的系统能够迭代收集用户的记忆，更新其自传，并规划未来的对话。在实验模拟中，StorySage展示了其在多个会话中导航并捕捉用户记忆的能力。用户研究（N=28）表明，相比于基线系统，StorySage能够维持更好的对话流畅性、叙事完整性，并提高用户满意度。总体而言，StorySage既贡献了一种新的自传写作架构，也为多智能体系统如何增强人机创意合作提供了见解。', 'title_zh': 'StorySage：基于多Agent框架的对话式自传写作'}
{'arxiv_id': 'arXiv:2506.14158', 'title': 'S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models', 'authors': 'Tao He, Guang Huang, Yu Yang, Tianshi Xu, Sicheng Zhao, Guiguang Ding, Pengyang Wang, Feng Tian', 'link': 'https://arxiv.org/abs/2506.14158', 'abstract': 'Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.', 'abstract_zh': '具有句法和语义连贯性的推测采样框架（S$^4$C）', 'title_zh': 'S$^4$C：基于语法和语义一致性推测采样的大型语言模型高效推理方法'}
{'arxiv_id': 'arXiv:2506.14144', 'title': 'SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability', 'authors': 'Juho Bai, Inwook Shim', 'link': 'https://arxiv.org/abs/2506.14144', 'abstract': 'Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: this https URL.', 'abstract_zh': '基于场景理解的行人轨迹准确预测方法：SceneAware', 'title_zh': 'SceneAware: 基于场景约束的行人轨迹预测与LLM指导的适宜性分析'}
{'arxiv_id': 'arXiv:2506.14138', 'title': 'NeuroCoreX: An Open-Source FPGA-Based Spiking Neural Network Emulator with On-Chip Learning', 'authors': 'Ashish Gautam, Prasanna Date, Shruti Kulkarni, Robert Patton, Thomas Potok', 'link': 'https://arxiv.org/abs/2506.14138', 'abstract': 'Spiking Neural Networks (SNNs) are computational models inspired by the structure and dynamics of biological neuronal networks. Their event-driven nature enables them to achieve high energy efficiency, particularly when deployed on neuromorphic hardware platforms. Unlike conventional Artificial Neural Networks (ANNs), which primarily rely on layered architectures, SNNs naturally support a wide range of connectivity patterns, from traditional layered structures to small-world graphs characterized by locally dense and globally sparse connections. In this work, we introduce NeuroCoreX, an FPGA-based emulator designed for the flexible co-design and testing of SNNs. NeuroCoreX supports all-to-all connectivity, providing the capability to implement diverse network topologies without architectural restrictions. It features a biologically motivated local learning mechanism based on Spike-Timing-Dependent Plasticity (STDP). The neuron model implemented within NeuroCoreX is the Leaky Integrate-and-Fire (LIF) model, with current-based synapses facilitating spike integration and transmission . A Universal Asynchronous Receiver-Transmitter (UART) interface is provided for programming and configuring the network parameters, including neuron, synapse, and learning rule settings. Users interact with the emulator through a simple Python-based interface, streamlining SNN deployment from model design to hardware execution. NeuroCoreX is released as an open-source framework, aiming to accelerate research and development in energy-efficient, biologically inspired computing.', 'abstract_zh': '基于FPGA的NeuroCoreX：面向SNNs的灵活设计与测试平台', 'title_zh': 'NeuroCoreX：一个基于FPGA的内置学习功能的脉冲神经网络模拟器'}
{'arxiv_id': 'arXiv:2506.14130', 'title': 'KDMOS:Knowledge Distillation for Motion Segmentation', 'authors': 'Chunyu Cao, Jintao Cheng, Zeyu Chen, Linfan Zhan, Rui Fan, Zhijian He, Xiaoyu Tang', 'link': 'https://arxiv.org/abs/2506.14130', 'abstract': "Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at this https URL.", 'abstract_zh': '基于logits的知识蒸馏框架在自主驾驶中的运动对象分割', 'title_zh': 'KDMOS：运动分割的知识蒸馏'}
{'arxiv_id': 'arXiv:2506.14126', 'title': 'Less is More: Undertraining Experts Improves Model Upcycling', 'authors': 'Stefan Horoi, Guy Wolf, Eugene Belilovsky, Gintare Karolina Dziugaite', 'link': 'https://arxiv.org/abs/2506.14126', 'abstract': 'Modern deep learning is increasingly characterized by the use of open-weight foundation models that can be fine-tuned on specialized datasets. This has led to a proliferation of expert models and adapters, often shared via platforms like HuggingFace and AdapterHub. To leverage these resources, numerous model upcycling methods have emerged, enabling the reuse of fine-tuned models in multi-task systems. A natural pipeline has thus formed to harness the benefits of transfer learning and amortize sunk training costs: models are pre-trained on general data, fine-tuned on specific tasks, and then upcycled into more general-purpose systems. A prevailing assumption is that improvements at one stage of this pipeline propagate downstream, leading to gains at subsequent steps. In this work, we challenge that assumption by examining how expert fine-tuning affects model upcycling. We show that long fine-tuning of experts that optimizes for their individual performance leads to degraded merging performance, both for fully fine-tuned and LoRA-adapted models, and to worse downstream results when LoRA adapters are upcycled into MoE layers. We trace this degradation to the memorization of a small set of difficult examples that dominate late fine-tuning steps and are subsequently forgotten during merging. Finally, we demonstrate that a task-dependent aggressive early stopping strategy can significantly improve upcycling performance.', 'abstract_zh': '现代深度学习日益依赖于可针对专门数据集进行微调的开放权重基础模型。这导致了专家模型和适配器的大量涌现，常通过HuggingFace和AdapterHub等平台共享。为了利用这些资源，已经出现了多种模型再利用方法，使微调后的模型能够在多任务系统中重用。因此形成了一条自然的管线，以利用迁移学习的优势并摊薄前期训练成本：模型在通用数据上预训练，在特定任务上进行微调，然后被再利用到更通用的系统中。普遍的假设是，这一管线各阶段的改进会逐渐传递到后续步骤，带来更好的结果。然而，在本工作中，我们通过研究专家微调如何影响模型再利用，挑战了这一假设。我们表明，针对专家个体性能优化进行长时间微调会导致合并性能下降，无论是完全微调的模型还是通过LoRA适配的模型，当LoRA适配器被插入到MoE层时，后续结果也会变差。我们将这种下降归因于难以学习的例子在后期微调中占据主导地位，随后在合并过程中被遗忘。最后，我们证明了依赖任务的激进式早期停止策略能够显著提高模型再利用性能。', 'title_zh': '少即是多：过度训练专家可以提高模型再利用效率'}
{'arxiv_id': 'arXiv:2506.14122', 'title': 'CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs', 'authors': 'Tianming Zhang, Renbo Zhang, Zhengyi Yang, Yunjun Gao, Bin Cao, Jing Fan', 'link': 'https://arxiv.org/abs/2506.14122', 'abstract': 'Temporal Betweenness Centrality (TBC) measures how often a node appears on optimal temporal paths, reflecting its importance in temporal networks. However, exact computation is highly expensive, and real-world TBC distributions are extremely imbalanced. The severe imbalance leads learning-based models to overfit to zero-centrality nodes, resulting in inaccurate TBC predictions and failure to identify truly central nodes. Existing graph neural network (GNN) methods either fail to handle such imbalance or ignore temporal dependencies altogether. To address these issues, we propose a scalable and inductive contrastive learning-based GNN (CLGNN) for accurate and efficient TBC prediction. CLGNN builds an instance graph to preserve path validity and temporal order, then encodes structural and temporal features using dual aggregation, i.e., mean and edge-to-node multi-head attention mechanisms, enhanced by temporal path count and time encodings. A stability-based clustering-guided contrastive module (KContrastNet) is introduced to separate high-, median-, and low-centrality nodes in representation space, mitigating class imbalance, while a regression module (ValueNet) estimates TBC values. CLGNN also supports multiple optimal path definitions to accommodate diverse temporal semantics. Extensive experiments demonstrate the effectiveness and efficiency of CLGNN across diverse benchmarks. CLGNN achieves up to a 663.7~$\\times$ speedup compared to state-of-the-art exact TBC computation methods. It outperforms leading static GNN baselines with up to 31.4~$\\times$ lower MAE and 16.7~$\\times$ higher Spearman correlation, and surpasses state-of-the-art temporal GNNs with up to 5.7~$\\times$ lower MAE and 3.9~$\\times$ higher Spearman correlation.', 'abstract_zh': '基于对比学习的可扩展图神经网络（CLGNN）用于精确高效的Temporal Betweenness Centrality预测', 'title_zh': 'CLGNN：基于对比学习的用于时间图中介中心性预测的图神经网络模型'}
{'arxiv_id': 'arXiv:2506.14113', 'title': 'SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting', 'authors': 'Yitian Zhang, Liheng Ma, Antonios Valkanas, Boris N. Oreshkin, Mark Coates', 'link': 'https://arxiv.org/abs/2506.14113', 'abstract': 'Koopman operator theory provides a framework for nonlinear dynamical system analysis and time-series forecasting by mapping dynamics to a space of real-valued measurement functions, enabling a linear operator representation. Despite the advantage of linearity, the operator is generally infinite-dimensional. Therefore, the objective is to learn measurement functions that yield a tractable finite-dimensional Koopman operator approximation. In this work, we establish a connection between Koopman operator approximation and linear Recurrent Neural Networks (RNNs), which have recently demonstrated remarkable success in sequence modeling. We show that by considering an extended state consisting of lagged observations, we can establish an equivalence between a structured Koopman operator and linear RNN updates. Building on this connection, we present SKOLR, which integrates a learnable spectral decomposition of the input signal with a multilayer perceptron (MLP) as the measurement functions and implements a structured Koopman operator via a highly parallel linear RNN stack. Numerical experiments on various forecasting benchmarks and dynamical systems show that this streamlined, Koopman-theory-based design delivers exceptional performance.', 'abstract_zh': 'Koopman算子理论提供了将非线性动力系统映射到实值测量函数空间的框架，从而实现线性算子表示，用于动力学分析和时间序列预测。尽管线性化有优势，但算子通常是无限维的。因此，目标是学习一类可以产生可处理的有限维Koopman算子近似的测量函数。在本工作中，我们建立了Koopman算子逼近与线性递归神经网络（RNNs）之间的联系，后者在序列建模中取得了显著的成功。我们展示了通过考虑延后观测组成的扩展状态，可以建立结构化Koopman算子与线性RNN更新之间的等价性。基于这一联系，我们提出了SKOLR，它将可学习的输入信号频谱分解与多层感知机（MLP）结合作为测量函数，并通过高度并行的线性RNN堆栈实现结构化Koopman算子。数值实验在各种预测基准和动力学系统上表明，这种基于Koopman理论的简化设计提供了出色的性能。', 'title_zh': 'SKOLR：结构化库默曼算子线性RNN时间序列预测'}
{'arxiv_id': 'arXiv:2506.14111', 'title': 'Essential-Web v1.0: 24T tokens of organized web data', 'authors': 'Essential AI, Andrew Hojel, Michael Pust, Tim Romanski, Yash Vanjani, Ritvik Kapila, Mohit Parmar, Adarsh Chaluvaraju, Alok Tripathy, Anil Thomas, Ashish Tanwer, Darsh J Shah, Ishaan Shah, Karl Stratos, Khoi Nguyen, Kurt Smith, Michael Callahan, Peter Rushton, Philip Monk, Platon Mazarakis, Saad Jamal, Saurabh Srivastava, Somanshu Singla, Ashish Vaswani', 'link': 'https://arxiv.org/abs/2506.14111', 'abstract': 'Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: this https URL', 'abstract_zh': 'Essential-Web v1.0: 一个包含24万亿标记的多类别标注网页数据集', 'title_zh': 'Essential-Web v1.0: 组织化的网络数据24Ttokens'}
{'arxiv_id': 'arXiv:2506.14098', 'title': 'Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks', 'authors': 'Ziyuan Tang, Jie Chen', 'link': 'https://arxiv.org/abs/2506.14098', 'abstract': 'A foundation model like GPT elicits many emergent abilities, owing to the pre-training with broad inclusion of data and the use of the powerful Transformer architecture. While foundation models in natural languages are prevalent, can we build similar models for graphs? This paper describes an approach toward a graph foundation model that is pre-trained with diverse graph datasets by adapting the Transformer backbone. A central challenge toward this end is how a sequence model encodes graphs of varying sizes and from different domains. We propose representing a node as multiple random walks, such that the Transformer can extract node representations from sequences, which in turn form edge and graph representations. We develop a novel context prediction loss for these random walks and theoretically analyze their expressive power in distinguishing neighborhoods and graphs. We also demonstrate the pre-training of our model and its adaptation to downstream tasks, showcasing its potential as a foundation for processing and reasoning with graph-structured data.', 'abstract_zh': '像GPT这样的基础模型由于广域数据的预训练和强大力量的Transformer架构的应用，激发了多种 emergent 能力。虽然自然语言领域中的基础模型十分普遍，我们能否构建类似的图模型？本文描述了一种通过适应Transformer骨干来使用多样化的图数据集进行预训练的方法。这一目标的核心挑战是如何将序列模型编码适用于不同大小和不同领域的图。我们提出将节点表示为多个随机游走，从而使Transformer可以从序列中提取节点表示，进而形成边和图的表示。我们开发了一种新的上下文预测损失函数，并理论分析了这些随机游走在区分邻域和图上的表达能力。我们还展示了该模型的预训练及其对下游任务的适应性，并展示了其作为处理和推理图结构数据的基础的潜力。', 'title_zh': '基于图基础模型的研究：使用随机游走预训练Transformer'}
{'arxiv_id': 'arXiv:2506.14096', 'title': 'Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems', 'authors': 'Sanjeda Akter, Ibne Farabi Shihab, Anuj Sharma', 'link': 'https://arxiv.org/abs/2506.14096', 'abstract': 'The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.', 'abstract_zh': '大型语言模型与计算机视觉的集成正深刻改变图像分割等感知任务。在智能交通系统中，准确的场景理解对于安全和效率至关重要，这一新范式提供了前所未有的能力。本文系统地回顾了大型语言模型增强图像分割这一新兴领域的研究，重点关注其在智能交通系统中的应用、挑战及未来方向。我们根据其提示机制和核心架构对当前方法进行了分类，并highlight了这些创新如何增强自动驾驶、交通监控和基础设施维护中的道路场景理解。最后，我们指出了关键挑战，包括实时性能和安全关键可靠性，并概述了一种以可解释的人本AI为核心的观点，强调这是在下一代交通系统中成功部署该技术的先决条件。', 'title_zh': '基于大型语言模型的图像分割：面向智能交通系统的综述与视角'}
{'arxiv_id': 'arXiv:2506.14086', 'title': 'InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking', 'authors': 'Rahul Seetharaman, Kaustubh D. Dhole, Aman Bansal', 'link': 'https://arxiv.org/abs/2506.14086', 'abstract': "Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.", 'abstract_zh': '基于大型语言模型的InsertRank检索增强器：通过利用BM25评分进一步提高检索性能', 'title_zh': 'InsertRank: LLMs可以基于BM25得分进行列表级重排序以提升性能'}
{'arxiv_id': 'arXiv:2506.14054', 'title': 'Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature', 'authors': 'Joshua Fan, Haodi Xu, Feng Tao, Md Nasim, Marc Grimson, Yiqi Luo, Carla P. Gomes', 'link': 'https://arxiv.org/abs/2506.14054', 'abstract': 'Neural networks are a powerful tool for learning patterns from data. However, they do not respect known scientific laws, nor can they reveal novel scientific insights due to their black-box nature. In contrast, scientific reasoning distills biological or physical principles from observations and controlled experiments, and quantitatively interprets them with process-based models made of mathematical equations. Yet, process-based models rely on numerous free parameters that must be set in an ad-hoc manner, and thus often fit observations poorly in cross-scale predictions. While prior work has embedded process-based models in conventional neural networks, discovering interpretable relationships between parameters in process-based models and input features is still a grand challenge for scientific discovery. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge, further enhancing its interpretability. While the embedded process-based model enforces established scientific knowledge, the encoder reveals new scientific mechanisms and relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.', 'abstract_zh': '科学可解释推理网络：结合可解释神经网络与过程模型的完全透明框架', 'title_zh': '科学可解释推理网络（ScIReN）：揭开自然界的黑箱'}
{'arxiv_id': 'arXiv:2506.14046', 'title': 'Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications', 'authors': 'David Kogan, Max Schumacher, Sam Nguyen, Masanori Suzuki, Melissa Smith, Chloe Sophia Bellows, Jared Bernstein', 'link': 'https://arxiv.org/abs/2506.14046', 'abstract': 'There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.', 'abstract_zh': '有一种需求未被满足，即评估短对话文本的语言难度，特别是在训练和过滤大型语言模型（LLMs）方面。我们引入了Ace-CEFR数据集，该数据集包含经过专家标注的英语对话文本片段及其相应的文本难度级别。我们尝试了多种模型在Ace-CEFR上的表现，包括基于Transformer的模型和LLMs。结果显示，训练于Ace-CEFR的数据集上的模型能够比人类专家更准确地衡量文本难度，并且具有适应生产环境的延迟。最后，我们公开发布Ace-CEFR数据集供研究和开发使用。', 'title_zh': 'Ace-CEFR -- 用于LLM应用的对话文本语言难度自动化评估数据集'}
{'arxiv_id': 'arXiv:2506.14042', 'title': 'Asymptotically Smaller Encodings for Graph Problems and Scheduling', 'authors': 'Bernardo Subercaseaux', 'link': 'https://arxiv.org/abs/2506.14042', 'abstract': 'We show how several graph problems (e.g., vertex-cover, independent-set, $k$-coloring) can be encoded into CNF using only $O(|V|^2 / \\lg |V|)$ many clauses, as opposed to the $\\Omega(|V|^2)$ constraints used by standard encodings. This somewhat surprising result is a simple consequence of a result of Erdős, Chung, and Spencer (1983) about biclique coverings of graphs, and opens theoretical avenues to understand the success of "Bounded Variable Addition\'\' (Manthey, Heule, and Biere, 2012) as a preprocessing tool. Finally, we show a novel encoding for independent sets in some dense interval graphs using only $O(|V| \\lg |V|)$ clauses (the direct encoding uses $\\Omega(|V|^2)$), which we have successfully applied to a string-compression encoding posed by Bannai et al. (2022). As a direct byproduct, we obtain a reduction in the encoding size of a scheduling problem posed by Mayank and Modal (2020) from $O(NMT^2)$ to $O(NMT + M T^2 \\lg T)$, where $N$ is the number of tasks, $T$ the total timespan, and $M$ the number of machines.', 'abstract_zh': '我们展示了如何使用只有 $O(|V|^2 / \\lg |V|)$ 个子句将几个图问题（例如，顶点覆盖、独立集、$k$-着色）编码到CNF中，而不是标准编码使用的 $\\Omega(|V|^2)$ 个约束。这一令人惊讶的结果是Erdős、Chung和Spencer（1983）关于图的biclique覆盖的一个结果的简单推论，并为理解“有界变量增加”（Manthey、Heule和Biere，2012）作为预处理工具的成功开辟了理论途径。最后，我们展示了一种新的编码方法，使用只有 $O(|V| \\lg |V|)$ 个子句为某些稠密区间图编码独立集（直接编码使用 $\\Omega(|V|^2)$ 个子句），并成功应用于Bannai等人（2022）提出的字符串压缩编码。作为直接的结果，我们得到了Mayank和Modal（2020）提出的调度问题编码大小从 $O(NMT^2)$ 减少到 $O(NMT + M T^2 \\lg T)$，其中 $N$ 是任务的数量，$T$ 是总时间段，$M$ 是机器的数量。', 'title_zh': '渐进更小的图问题和调度编码大小'}
{'arxiv_id': 'arXiv:2506.14035', 'title': 'SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement', 'authors': 'Chelsi Jain, Yiran Wu, Yifan Zeng, Jiale Liu, S hengyu Dai, Zhenwen Shao, Qingyun Wu, Huazheng Wang', 'link': 'https://arxiv.org/abs/2506.14035', 'abstract': 'Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at this https URL.', 'abstract_zh': '文档视觉问答（DocVQA）是一项实际且具有挑战性的任务，旨在基于多页文档并参考不同类型的信息（例如图片和表格）提问。为了应对多模态处理，近期方法遵循类似的检索增强生成（RAG）管道，但利用视觉语言模型（VLMs）嵌入模型来嵌入并检索相关页面作为图像，并使用能接受图像作为输入的VLMs生成答案。在本文中，我们介绍了一种轻量级但强大的检索增强框架SimpleDoc，它通过首先通过嵌入相似性检索候选页面，然后根据页面摘要进行过滤和再排序来增强证据页面的收集。一个基于VLM的推理代理反复调用这种双重线索检索器，迭代地将新的页面拉入工作记忆，直到问题得到自信的回答。SimpleDoc在4个DocVQA数据集上的表现优于先前基线，且检索的页面数量大幅减少。我们的代码可在以下网址获得。', 'title_zh': 'SimpleDoc：基于双线索页面检索和迭代精炼的多模态文档理解'}
{'arxiv_id': 'arXiv:2506.14020', 'title': 'Bures-Wasserstein Flow Matching for Graph Generation', 'authors': 'Keyue Jiang, Jiahao Cui, Xiaowen Dong, Laura Toni', 'link': 'https://arxiv.org/abs/2506.14020', 'abstract': 'Graph generation has emerged as a critical task in fields ranging from molecule design to drug discovery. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between a reference distribution and the data distribution. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path assuming that the data lie in Euclidean space. We show that this is suboptimal given the intrinsic non-Euclidean structure and interconnected patterns of graphs, and it poses risks to the sampling convergence. To build a better probability path, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design the probability path for graph generation. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that respects the underlying geometry of graphs and provides smooth velocities in the probability path. The novel framework can be adapted to both continuous and discrete flow-matching algorithms. Experimental evaluations in plain graph generation and 2D/3D molecule generation validate the effectiveness of BWFlow in graph generation with competitive performance, stable training, and guaranteed sampling convergence.', 'abstract_zh': '图生成已在分子设计、药物发现等领域中 emerged 作为一项关键任务。现代方法，特别是扩散和流动模型，通过构建一个在参考分布和数据分布之间进行插值的概率路径，实现了坚实的图生成性能。然而，这些方法通常独立地建模节点和边的演变，并假设数据位于欧几里得空间中，使用线性插值来构建路径。我们证明，在图的内在非欧几里得结构和相互关联模式的情况下，这种建模方法是次优的，并且可能会对采样收敛性带来风险。为了构建更好的概率路径，我们将节点和边的联合演变建模为由马尔可夫随机场（MRF）参数化的连接系统。然后，我们利用MRF对象之间的最优传输位移来设计图生成的概率路径。在此基础上，我们引入了BWFlow，这是一种尊重图底层几何结构的流动匹配框架，并在概率路径中提供平滑的速度。该新型框架可以适应连续和离散的流动匹配算法。在单纯的图生成和2D/3D分子生成实验评估中，BWFlow证明了其在图生成中的有效性，具有竞争力的性能、稳定的训练和有保证的采样收敛性。', 'title_zh': 'Bures-Wasserstein流匹配图生成'}
{'arxiv_id': 'arXiv:2506.14002', 'title': 'Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders', 'authors': 'Siyu Chen, Heejune Sheen, Xuyuan Xiong, Tianhao Wang, Zhuoran Yang', 'link': 'https://arxiv.org/abs/2506.14002', 'abstract': "We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \\highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \\highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.", 'abstract_zh': '我们研究了使用稀疏自编码器（SAEs）从大型语言模型中实现理论上可验证的特征恢复所面临的挑战。现有的SAE训练算法通常缺乏严格的数学保证，并且受到超参数敏感性和不稳定性等实际限制的影响。为了应对这些问题，我们首先提出了一种新的统计框架来解决特征恢复问题，该框架包括一种新的特征可识别性概念，即将多义特征建模为潜在单义概念的稀疏混合。在此框架的基础上，我们引入了一种新的基于“偏差适应”的SAE训练算法。该算法能够自适应调整神经网络的偏差参数以确保适当的激活稀疏性。我们理论证明，在输入数据来自我们提出的统计模型的情况下，该算法可以正确恢复所有单义特征。此外，我们开发了一种改进的经验变体，即分组偏差适应（GBA），并在应用于最多含有15亿参数的大型语言模型时，展示了其相对于基准方法的优越性能。这项工作代表了在提供理论上恢复保证的SAE算法方面的一个基础性步骤，从而通过增强的机制解释提高了更透明和可信赖的AI系统的发展。', 'title_zh': '驯服LLMs中的多义性：通过稀疏自编码器实现可验证的功能恢复'}
{'arxiv_id': 'arXiv:2506.13992', 'title': 'AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science', 'authors': 'An Luo, Xun Xian, Jin Du, Fangqiao Tian, Ganghua Wang, Ming Zhong, Shengchun Zhao, Xuan Bi, Zirui Liu, Jiawei Zhou, Jayanth Srinivasa, Ashish Kundu, Charles Fleming, Mingyi Hong, Jie Ding', 'link': 'https://arxiv.org/abs/2506.13992', 'abstract': "Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.", 'abstract_zh': '大型语言模型（LLMs）已经推进了数据科学工作流程的自动化。然而，尚不清楚它们是否能够像人类数据科学家在实践中那样批判性地利用外部领域知识。为了回答这个问题，我们引入了AssistedDS（辅助数据科学），这是一个基准测试，旨在系统地评估LLMs在表格预测任务中处理领域知识的能力。AssistedDS包括具有明确生成机制的合成数据集和现实世界的Kaggle竞赛，每项任务都附有经过精心筛选的有益和对抗性文档。这些文档提供了针对数据清洗、特征工程和模型选择的领域特定见解。我们评估了最先进的LLMs识别和应用有益与有害领域知识的能力，评估提交的有效性、信息召回率和预测性能。我们的结果显示了三个关键发现：（1）LLMs经常不加批判地采用提供的信息，在引入对抗性内容时显著损害了其预测性能；（2）有益的指导往往不足以抵消对抗性信息的负面影响；（3）在Kaggle数据集中，LLMs常在处理时间序列数据、在不同折中一致地应用特征工程以及正确解释分类变量方面出现错误。这些发现突显示出了当前模型在批判性评估和利用专家知识方面的重要差距，强调了开发更 robust、知识导向的自动化数据科学系统的重要研究方向。', 'title_zh': 'AssistedDS：外域知识辅助LLMs在自动化数据科学中的基准研究'}
{'arxiv_id': 'arXiv:2506.13989', 'title': 'AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering', 'authors': 'Johan Östman, Edvin Callisen, Anton Chen, Kristiina Ausmees, Emanuel Gårdh, Jovan Zamac, Jolanta Goldsteine, Hugo Wefer, Simon Whelan, Markus Reimegård', 'link': 'https://arxiv.org/abs/2506.13989', 'abstract': 'Money laundering enables organized crime by allowing illicit funds to enter the legitimate economy. Although trillions of dollars are laundered each year, only a small fraction is ever uncovered. This stems from a range of factors, including deliberate evasion by launderers, the rarity of confirmed cases, and the limited visibility each financial institution has into the global transaction network. While several synthetic datasets are available, they fail to model the structural and behavioral complexity of real-world money laundering. In particular, they often overlook partial observability, sparse and uncertain labels, strategic behavior, temporal dynamics, class imbalance, and network-level dependencies. To address these limitations, we present AMLGentex, an open-source suite for generating realistic, configurable transaction data and benchmarking detection methods. It enables systematic evaluation of anti-money laundering (AML) systems in a controlled environment that captures key real-world challenges. We demonstrate how the framework can be used to rigorously evaluate methods under conditions that reflect the complexity of practical AML scenarios.', 'abstract_zh': '洗钱通过使非法资金进入合法经济从而助长有组织犯罪。尽管每年有数万亿美元被洗钱，但其中只有小部分能够被发现。这归因于多个因素，包括洗钱者故意规避、确认案例的稀少以及金融机构对全球交易网络的有限可见性。虽然有一些合成数据集可用，但它们无法模拟现实世界洗钱的结构和行为复杂性。特别是，它们通常忽略了部分可观测性、稀疏和不确定的标签、战略行为、时间动态、类别不平衡以及网络层面的依赖性。为解决这些局限性，我们提出AMLGentex，一个开源工具套件，用于生成现实且可配置的交易数据并比较检测方法。它使得在能够捕捉到关键现实挑战的受控环境中系统地评估反洗钱（AML）系统成为可能。我们展示了该框架如何在反映实际AML场景复杂性的条件下严格评估方法。', 'title_zh': 'AMLgentex: 利用数据驱动研究打击洗钱活动'}
{'arxiv_id': 'arXiv:2506.13984', 'title': 'Mirror Descent Using the Tempesta Generalized Multi-parametric Logarithms', 'authors': 'Andrzej Cichocki', 'link': 'https://arxiv.org/abs/2506.13984', 'abstract': 'In this paper, we develop a wide class Mirror Descent (MD) algorithms, which play a key role in machine learning. For this purpose we formulated the constrained optimization problem, in which we exploits the Bregman divergence with the Tempesta multi-parametric deformation logarithm as a link function. This link function called also mirror function defines the mapping between the primal and dual spaces and is associated with a very-wide (in fact, theoretically infinite) class of generalized trace-form entropies. In order to derive novel MD updates, we estimate generalized exponential function, which closely approximates the inverse of the multi-parametric Tempesta generalized logarithm. The shape and properties of the Tempesta logarithm and its inverse-deformed exponential functions can be tuned by several hyperparameters. By learning these hyperparameters, we can adapt to distribution or geometry of training data, and we can adjust them to achieve desired properties of MD algorithms. The concept of applying multi-parametric logarithms allow us to generate a new wide and flexible family of MD and mirror-less MD updates.', 'abstract_zh': '在这篇论文中，我们开发了一类广泛的_mirror descent_ (MD)算法，这些算法在机器学习中扮演着关键角色。为此，我们形式化了一个受约束的优化问题，在该问题中利用Tempesta多元参数变形对数作为链接函数，该链接函数也被称作mirror函数，定义了原始空间和对偶空间之间的映射，并与极其广泛（实际上，理论上是无限广泛）的一类广义迹形式熵相关。为了推导新的MD更新，我们估计了广义指数函数，该函数几乎可以 approximates 多元参数Tempesta广义对数的逆。Tempesta 对数及其逆变形指数函数的形状和属性可以通过多个超参数进行调整。通过学习这些超参数，我们可以适应训练数据的分布或几何结构，并据此调整以实现MD算法所需的各种属性。应用多元参数对数的概念使我们能够生成一个新的广泛而灵活的MD及其无mirror的MD更新族。', 'title_zh': '镜像下降法使用Tempesta广义多参数对数函数'}
{'arxiv_id': 'arXiv:2506.13981', 'title': 'HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting', 'authors': 'Thanh Dan Bui', 'link': 'https://arxiv.org/abs/2506.13981', 'abstract': "High-frequency stock price prediction is challenging due to non-stationarity, noise, and volatility. To tackle these issues, we propose the Hybrid Attentive Ensemble Learning Transformer (HAELT), a deep learning framework combining a ResNet-based noise-mitigation module, temporal self-attention for dynamic focus on relevant history, and a hybrid LSTM-Transformer core that captures both local and long-range dependencies. These components are adaptively ensembled based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set, effectively identifying both upward and downward price movements. This demonstrates HAELT's potential for robust, practical financial forecasting and algorithmic trading.", 'abstract_zh': '高频股票价格预测因非平稳性、噪声和波动性而具有挑战性。为应对这些挑战，我们提出了混合注意力集成学习变压器（HAELT）框架，该框架结合了基于ResNet的噪声减轻模块、时间自注意力以动态聚焦相关历史，以及同时捕捉局部和长程依赖性的混合LSTM-Transformer核心。这些组件基于近期表现进行适应性集成。实验结果表明，HAELT在从2024年1月到2025年5月的每小时Apple Inc. (AAPL)数据上测试集中的F1分数最高，有效地识别了价格的上下行变动。这展示了HAELT在稳健且实际的金融预测和算法交易中的潜力。', 'title_zh': 'HAELT：一种用于高频率股票价格预测的混合注意集成学习变换器框架'}
{'arxiv_id': 'arXiv:2506.13970', 'title': 'Making deep neural networks work for medical audio: representation, compression and domain adaptation', 'authors': 'Charles C Onu', 'link': 'https://arxiv.org/abs/2506.13970', 'abstract': 'This thesis addresses the technical challenges of applying machine learning to understand and interpret medical audio signals. The sounds of our lungs, heart, and voice convey vital information about our health. Yet, in contemporary medicine, these sounds are primarily analyzed through auditory interpretation by experts using devices like stethoscopes. Automated analysis offers the potential to standardize the processing of medical sounds, enable screening in low-resource settings where physicians are scarce, and detect subtle patterns that may elude human perception, thereby facilitating early diagnosis and treatment.\nFocusing on the analysis of infant cry sounds to predict medical conditions, this thesis contributes on four key fronts. First, in low-data settings, we demonstrate that large databases of adult speech can be harnessed through neural transfer learning to develop more accurate and robust models for infant cry analysis. Second, in cost-effective modeling, we introduce an end-to-end model compression approach for recurrent networks using tensor decomposition. Our method requires no post-hoc processing, achieves compression rates of several hundred-fold, and delivers accurate, portable models suitable for resource-constrained devices. Third, we propose novel domain adaptation techniques tailored for audio models and adapt existing methods from computer vision. These approaches address dataset bias and enhance generalization across domains while maintaining strong performance on the original data. Finally, to advance research in this domain, we release a unique, open-source dataset of infant cry sounds, developed in collaboration with clinicians worldwide.\nThis work lays the foundation for recognizing the infant cry as a vital sign and highlights the transformative potential of AI-driven audio monitoring in shaping the future of accessible and affordable healthcare.', 'abstract_zh': '本论文探讨了将机器学习应用于理解和解释医疗音频信号所面临的technical挑战。我们的肺音、心音和语音传达着关于我们健康的重要信息。然而，在当代医学中，这些声音主要通过专家使用听诊器等设备进行听觉分析。自动化分析有可能标准化医疗声音的处理过程，在医生稀缺的低资源环境中实现筛查，并检测人眼可能察觉不到的细微模式，从而促进早期诊断和治疗。\n\n本论文集中于婴儿啼哭声音的分析以预测医疗状况，主要从四个方面进行了贡献。首先，在数据稀缺的环境下，我们通过神经迁移学习利用大量成人语音数据集开发更准确和鲁棒的婴儿啼哭分析模型。其次，在经济高效的建模方面，我们引入了一种用于递归网络的端到端模型压缩方法，使用张量分解。我们的方法无需后处理，实现了几百倍的压缩率，并提供了准确且便于携带的模型，适合资源受限的设备。第三，我们提出了适用于音频模型的新方法，并对计算机视觉中的现有方法进行改编。这些方法解决了数据集偏差问题，增强了跨领域的一般化能力，同时在原始数据上仍保持了强大的性能。最后，为了推动该领域的研究，我们发布了一个独特的开放源代码婴儿啼哭声音数据集，该数据集是与全球临床医生合作开发的。\n\n本研究为识别婴儿啼哭作为生命体征奠定了基础，并突显了基于AI的音频监控在推动可及性和负担得起的医疗保健未来方面的变革潜力。', 'title_zh': '在医疗音频中使深度神经网络发挥作用：表示、压缩和领域适应'}
{'arxiv_id': 'arXiv:2506.13961', 'title': 'Safe Domains of Attraction for Discrete-Time Nonlinear Systems: Characterization and Verifiable Neural Network Estimation', 'authors': 'Mohamed Serry, Haoyu Li, Ruikun Zhou, Huan Zhang, Jun Liu', 'link': 'https://arxiv.org/abs/2506.13961', 'abstract': 'Analysis of nonlinear autonomous systems typically involves estimating domains of attraction, which have been a topic of extensive research interest for decades. Despite that, accurately estimating domains of attraction for nonlinear systems remains a challenging task, where existing methods are conservative or limited to low-dimensional systems. The estimation becomes even more challenging when accounting for state constraints. In this work, we propose a framework to accurately estimate safe (state-constrained) domains of attraction for discrete-time autonomous nonlinear systems. In establishing this framework, we first derive a new Zubov equation, whose solution corresponds to the exact safe domain of attraction. The solution to the aforementioned Zubov equation is shown to be unique and continuous over the whole state space. We then present a physics-informed approach to approximating the solution of the Zubov equation using neural networks. To obtain certifiable estimates of the domain of attraction from the neural network approximate solutions, we propose a verification framework that can be implemented using standard verification tools (e.g., $\\alpha,\\!\\beta$-CROWN and dReal). To illustrate its effectiveness, we demonstrate our approach through numerical examples concerning nonlinear systems with state constraints.', 'abstract_zh': '离散自治非线性系统的安全（状态约束）吸引域精确估计方法', 'title_zh': '离散时间非线性系统的安全吸引域：表征及可验证神经网络估计'}
{'arxiv_id': 'arXiv:2506.13958', 'title': 'Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers', 'authors': 'Leonardo Guiducci, Antonio Rizzo, Giovanna Maria Dimitri', 'link': 'https://arxiv.org/abs/2506.13958', 'abstract': 'Elastic Decision Transformers (EDTs) have proved to be particularly successful in offline reinforcement learning, offering a flexible framework that unifies sequence modeling with decision-making under uncertainty. Recent research has shown that incorporating intrinsic motivation mechanisms into EDTs improves performance across exploration tasks, yet the representational mechanisms underlying these improvements remain unexplored. In this paper, we introduce a systematic post-hoc explainability framework to analyze how intrinsic motivation shapes learned embeddings in EDTs. Through statistical analysis of embedding properties (including covariance structure, vector magnitudes, and orthogonality), we reveal that different intrinsic motivation variants create fundamentally different representational structures. Our analysis demonstrates environment-specific correlation patterns between embedding metrics and performance that explain why intrinsic motivation improves policy learning. These findings show that intrinsic motivation operates beyond simple exploration bonuses, acting as a representational prior that shapes embedding geometry in biologically plausible ways, creating environment-specific organizational structures that facilitate better decision-making.', 'abstract_zh': '弹性决策变换器（EDTs）在离线强化学习中 proven 特别成功，提供了一个统一序列建模与不确定性决策的灵活框架。最近的研究表明，将内在动机机制融入 EDTs 可以提高探索任务中的性能，但这些改进背后的表示机制尚未得到探索。在本文中，我们引入了一个系统的事后可解释性框架来分析内在动机如何塑造 EDTs 中的学习嵌入。通过嵌入属性（包括协方差结构、向量幅度和正交性）的统计分析，我们揭示了不同内在动机变体创建了根本不同的表示结构。我们的分析表明，嵌入指标与性能之间存在环境特定的相关模式，这解释了为何内在动机能够改善策略学习。这些发现表明，内在动机的作用不仅仅局限于简单的探索奖金，而是作为一种表示先验，以生物学上可实现的方式塑造嵌入几何学，创造出环境特定的组织结构，促进更好的决策。', 'title_zh': '可解释的离线强化学习：分析内在动机决策变换器中的表示'}
{'arxiv_id': 'arXiv:2506.13956', 'title': 'ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection', 'authors': 'Shang-Chi Tsai, Seiya Kawano, Angel Garcia Contreras, Koichiro Yoshino, Yun-Nung Chen', 'link': 'https://arxiv.org/abs/2506.13956', 'abstract': "When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.", 'abstract_zh': '在日常人类活动中设计机器人助手法时，增强用户请求与周围环境的视觉提示以提高意图理解至关重要。这一过程被定义为多模态分类任务。然而，收集包含视觉和语言元素的大规模数据集以供模型训练是具有挑战性和耗时的。为解决这一问题，我们在机器人辅助场景中引入了一种新的数据增强框架，涵盖对话和相关环境图像。该方法涉及利用复杂的大型语言模型模拟可能的对话和环境上下文，然后使用稳定的扩散模型生成描绘这些环境的图像。额外生成的数据用于优化最新的多模态模型，使其在有限目标数据的交互中能够更准确地选择适当的动作。基于真实场景收集的数据集的实验结果表明，我们的方法显著提高了机器人的动作选择能力，达到了最先进的性能。', 'title_zh': 'ASMR：使用大型生成模型增强生活场景中的机器人行动反思'}
{'arxiv_id': 'arXiv:2506.13932', 'title': 'How Does LLM Reasoning Work for Code? A Survey and a Call to Action', 'authors': 'Ira Ceka, Saurabh Pujar, Irene Manotas, Gail Kaiser, Baishakhi Ray, Shyam Ramji', 'link': 'https://arxiv.org/abs/2506.13932', 'abstract': 'The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. These advancements have extended into the domain of code, facilitating complex tasks such as code generation, translation, summarization, and repair. However, their utility for real-world deployment in-the-wild has only recently been studied, particularly on software engineering (SWE) tasks such as GitHub issue resolution. In this study, we examine the code reasoning techniques that underlie the ability to perform such tasks, and examine the paradigms used to drive their performance. Our contributions in this paper are: (1) the first dedicated survey on code reasoning for code tasks, highlighting overarching strategies, hybrid and agentic approaches; (2) a taxonomy of various techniques used to drive code reasoning; (3) a comprehensive overview of performance on common benchmarks and a showcase of new, under-explored benchmarks with high potential in SWE; (4) an exploration on how core properties of code can be used to explain different reasoning techniques; and (5) gaps and potentially under-explored areas for future research.', 'abstract_zh': '大型语言模型的崛起在广泛自然语言任务中带来了显著改进，这些进步已延伸至编程领域，促进了代码生成、翻译、摘要和修复等复杂任务。然而，它们在现实世界部署中的实用性仅近期得到了研究，特别是在软件工程任务如GitHub问题解决方面。本文研究了实现这些任务所需的代码推理技术及其背后的范式。本文的主要贡献包括：（1）首份专注于代码任务的代码推理专论，强调总体策略、混合和自主方法；（2）各种驱动代码推理技术的分类法；（3）对常见基准的综合性能概述，以及展示具有高潜力的新未探索基准在软件工程中的应用；（4）探讨代码核心属性如何解释不同推理技术；（5）未来研究中可能未被充分探索的空白领域。', 'title_zh': '大规模语言模型推理工作机制研究：面向代码的一篇综述与呼吁'}
{'arxiv_id': 'arXiv:2506.13925', 'title': 'HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment', 'authors': 'Numair Nadeem, Saeed Anwar, Muhammad Hamza Asad, Abdul Bais', 'link': 'https://arxiv.org/abs/2506.13925', 'abstract': 'Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization.', 'abstract_zh': '半监督语义分割在标签稀缺和领域变化严重的情况下仍具有挑战性。视觉-only方法常常难以泛化，导致类似类别之间的像素分类错误、泛化能力差和边界定位不准确。视觉-语言模型提供了鲁棒且领域不变的语义信息，但缺乏用于密集预测所需的空间定位。我们引入了HierVL，这是一种统一框架，通过将抽象文本嵌入集成到一个针对半监督分割定制的掩码变压器架构中，来弥合这一差距。HierVL 特征包括三个新型组件：层次语义查询生成器，将抽象类别嵌入投影到多尺度查询中，以抑制无关类别并处理类别内的变异性；跨模态空间对齐模块，在稀疏监督下使语义查询与像素特征对齐，以获得更清晰的边界；双查询变压器解码器，融合语义和实例级别查询以防止实例合并。我们还引入了靶向正则化损失，以在整个训练过程中维护视觉-语言对齐，强化语义定位。HierVL 在 COCO（232 个标注图像）、Pascal VOC（92 个标签）、ADE20（158 个标签）和 Cityscapes（100 个标签）四个基准数据集上分别实现了交并比(mean improvement of the intersection over the union)提高 4.4%、3.1%、5.9% 和 1.8%，展示了在 1% 监督下更好的性能。我们的结果表明，语言引导的分割缩小了标签效率差距并解锁了新的细粒度、实例感知泛化水平。', 'title_zh': 'HierVL：基于层次视觉-语言协同的动态文本-空间查询对齐半监督分割'}
{'arxiv_id': 'arXiv:2506.13923', 'title': 'Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models', 'authors': 'Vaskar Nath, Elaine Lau, Anisha Gunjal, Manasi Sharma, Nikhil Baharte, Sean Hendryx', 'link': 'https://arxiv.org/abs/2506.13923', 'abstract': 'We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance through two main means: (1) by compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\\text{Guide}$ - a new class of online training algorithms. $\\text{Guide}$ adaptively incorporates hints into the model\'s context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the "off-policy" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\\text{Guide}$\'s components and theoretically analyze Guide\'s learning efficiency.', 'abstract_zh': '我们研究了通过验证奖励强化学习（RLVR）训练的推理模型如何学习解决新问题的过程。我们发现RLVR通过两种主要方式提升性能：（1）将pass@$k$压缩为pass@1，（2）通过“能力提升”，模型学习解决以前即使在高$k$值下也无法解决的新问题。我们发现虽然能力提升跨越了不同的模型规模，但学习解决新问题主要通过自我精炼驱动。我们在这篇论文中展示了在数学、科学和代码领域的超过50万道推理问题上，从0.5B到72B规模的模型中这些发现。我们进一步表明，可以通过利用自然语言指导来显著提高pass@$k$率，同时仍然要求模型从头推导完整的问题解决链。基于上述洞见，我们推导出了$\\text{Guide}$——一种新的在线训练算法。$\\text{Guide}$自适应地将提示整合到问题的模型上下文中，对于所有展开均为错误的问题，调整“离策”轨迹的重要性采样比，以便在提示不再存在的上下文中优化策略。我们为GRPO和PPO提供了$\\text{Guide}$的变体，并实验证明，对于7B和32B参数模型的Guide-GRPO相较于其原版版本，在数学基准测试中可实现高达4%的宏平均性能提升。我们还包括了精细的消融实验来分析$\\text{Guide}$的各个组件，并对其学习效率进行了理论分析。', 'title_zh': '自适应引导加速推理模型的强化学习'}
{'arxiv_id': 'arXiv:2506.13911', 'title': 'Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization', 'authors': 'Arie Soeteman, Balder ten Cate', 'link': 'https://arxiv.org/abs/2506.13911', 'abstract': 'We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an expressive extension of graph neural networks (GNNs) with hierarchical node individualization, inspired by the Individualization-Refinement paradigm for graph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy of increasingly expressive models that, in the limit, can distinguish graphs up to isomorphism. We provide a logical characterization of HEGNN node classifiers, with and without subgraph restrictions, using graded hybrid logic. This characterization enables us to relate the separating power of HEGNNs to that of higher-order GNNs, GNNs enriched with local homomorphism count features, and color refinement algorithms based on Individualization-Refinement. Our experimental results confirm the practical feasibility of HEGNNs and show benefits in comparison with traditional GNN architectures, both with and without local homomorphism count features.', 'abstract_zh': '我们提出并研究了层次自中心图神经网络（HEGNNs），这是一种受图同构检验中的个体化- refinment范式启发，具有层次节点个体化扩展的图神经网络（GNNs）的表达性扩展。HEGNNs 推广了子图-GNNs，并形成了一种逐步表达性增强的模型层次，在极限情况下，能够区分同构的图。通过使用层次混合逻辑提供HEGNN节点分类器的逻辑特征化，使得我们可以将HEGNN的区分能力与其高阶GNNs、局部同构计数特征增强的GNNs以及基于个体化-refinement的颜色细化算法联系起来。实验结果证实了HEGNNs的实际可行性，并展示了与传统GNN架构及其是否包含局部同构计数特征时相比的优势。', 'title_zh': '层次节点个体化下的图神经网络逻辑表达能力'}
{'arxiv_id': 'arXiv:2506.13910', 'title': 'Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation', 'authors': 'Aritra Dutta, Pushpita Boral, G Suseela', 'link': 'https://arxiv.org/abs/2506.13910', 'abstract': 'The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy.', 'abstract_zh': '全球犯罪率的上升及其对人类和财产造成的重大损失突显了传统监控方法在及时检测各种意外暴力行为方面的局限性。为应对自动暴力检测的迫切需求，我们利用机器学习来检测和分类视频流中的暴力事件。本文介绍了一种全面的暴力检测与分类框架，采用监督学习进行二分类和多分类。检测模型基于3D卷积神经网络，分类模型则利用可分离卷积3D模型进行特征提取，并使用双向LSTM进行时间处理。训练在包含帧级标注的多样化定制数据集上进行，数据集来源包括监控摄像头视频、人类录像、冰球打斗、sohas和wvd数据集等多种平台。此外，还集成了一款搭配raspberry pi的摄像头模块，用于捕获实时视频流，经过ML模型处理。从而在计算资源效率和准确性方面表现出改进。', 'title_zh': '智能图像感知用于犯罪分析：一种增强暴力检测与调查的机器学习方法'}
{'arxiv_id': 'arXiv:2506.13909', 'title': 'Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring', 'authors': 'Xinyuan Tu, Haocheng Zhang, Tao Chengxu, Zuyi Chen', 'link': 'https://arxiv.org/abs/2506.13909', 'abstract': 'Few-shot learning (FSL) has shown promise in vision but remains largely unexplored for \\emph{industrial} time-series data, where annotating every new defect is prohibitively expensive. We present a systematic FSL study on screw-fastening process monitoring, using a 2\\,300-sample multivariate torque dataset that covers 16 uni- and multi-factorial defect types. Beyond benchmarking, we introduce a \\textbf{label-aware episodic sampler} that collapses multi-label sequences into multiple single-label tasks, keeping the output dimensionality fixed while preserving combinatorial label information.\nTwo FSL paradigms are investigated: the metric-based \\emph{Prototypical Network} and the gradient-based \\emph{Model-Agnostic Meta-Learning} (MAML), each paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter transformer \\emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime + Prototypical Network combination achieves a \\textbf{0.944 weighted F1} in the multi-class regime and \\textbf{0.935} in the multi-label regime, outperforming finetuned Moment by up to 5.3\\% while requiring two orders of magnitude fewer parameters and training time. Across all backbones, metric learning consistently surpasses MAML, and our label-aware sampling yields an additional 1.7\\% F1 over traditional class-based sampling.\nThese findings challenge the assumption that large foundation models are always superior: when data are scarce, lightweight CNN architectures augmented with simple metric learning not only converge faster but also generalize better. We release code, data splits and pre-trained weights to foster reproducible research and to catalyze the adoption of FSL in high-value manufacturing inspection.', 'abstract_zh': 'Few-Shot Learning for Industrial Time-Series Data in Screw-Fastening Process Monitoring: A Systematic Study with Label-Aware Episodic Sampling', 'title_zh': '工业时间序列的少量样本学习：以螺栓紧固过程监控为例的比较分析'}
{'arxiv_id': 'arXiv:2506.13904', 'title': 'A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare', 'authors': 'Ivania Donoso-Guzmán, Kristýna Sirka Kacafírková, Maxwell Szymanski, An Jacobs, Denis Parra, Katrien Verbert', 'link': 'https://arxiv.org/abs/2506.13904', 'abstract': 'Despite promising developments in Explainable Artificial Intelligence, the practical value of XAI methods remains under-explored and insufficiently validated in real-world settings. Robust and context-aware evaluation is essential, not only to produce understandable explanations but also to ensure their trustworthiness and usability for intended users, but tends to be overlooked because of no clear guidelines on how to design an evaluation with users.\nThis study addresses this gap with two main goals: (1) to develop a framework of well-defined, atomic properties that characterise the user experience of XAI in healthcare; and (2) to provide clear, context-sensitive guidelines for defining evaluation strategies based on system characteristics.\nWe conducted a systematic review of 82 user studies, sourced from five databases, all situated within healthcare settings and focused on evaluating AI-generated explanations. The analysis was guided by a predefined coding scheme informed by an existing evaluation framework, complemented by inductive codes developed iteratively.\nThe review yields three key contributions: (1) a synthesis of current evaluation practices, highlighting a growing focus on human-centred approaches in healthcare XAI; (2) insights into the interrelations among explanation properties; and (3) an updated framework and a set of actionable guidelines to support interdisciplinary teams in designing and implementing effective evaluation strategies for XAI systems tailored to specific application contexts.', 'abstract_zh': '尽管可解释人工智能取得了令人鼓舞的发展，但在实际应用中，XAI方法的实际价值仍然被低估并且缺乏充分验证。 robust且情境感知的评估至关重要，不仅为了生成可理解的解释，也为了确保这些解释对于预期用户具有可信性和实用性，但由于缺乏明确的设计评估指南，这一问题往往会被人忽视。\n\n本研究旨在填补这一空白，主要目标有两个：（1）开发一套定义明确、基本属性的框架，以描述健康 care 中的 XAI 用户体验；（2）提供基于系统特征的情境敏感指南，以定义评估策略。\n\n我们对82篇用户研究进行了系统回顾，这些研究来自五个数据库，均位于健康 care 设置中，专注于评估 AI 生成的解释。分析基于预定义的编码方案进行，该方案由现有的评估框架启发，并由迭代开发的归纳代码补充。\n\n该回顾研究提供了三个方面的重要贡献：（1）当前评估实践的综合分析，突显了健康 care XAI 中日益关注以人为本的方法；（2）解释属性之间关系的洞察；（3）更新的框架和一套可操作的指南，以支持跨学科团队设计和实施针对特定应用场景的 XAI 系统评估策略。', 'title_zh': '面向用户的解释型人工智能在医疗保健领域中的评价系统的综述'}
{'arxiv_id': 'arXiv:2506.13903', 'title': 'Enhancing interpretability of rule-based classifiers through feature graphs', 'authors': 'Christel Sirocchi, Damiano Verda', 'link': 'https://arxiv.org/abs/2506.13903', 'abstract': "In domains where transparency and trustworthiness are crucial, such as healthcare, rule-based systems are widely used and often preferred over black-box models for decision support systems due to their inherent interpretability. However, as rule-based models grow complex, discerning crucial features, understanding their interactions, and comparing feature contributions across different rule sets becomes challenging. To address this, we propose a comprehensive framework for estimating feature contributions in rule-based systems, introducing a graph-based feature visualisation strategy, a novel feature importance metric agnostic to rule-based predictors, and a distance metric for comparing rule sets based on feature contributions. By experimenting on two clinical datasets and four rule-based methods (decision trees, logic learning machines, association rules, and neural networks with rule extraction), we showcase our method's capability to uncover novel insights on the combined predictive value of clinical features, both at the dataset and class-specific levels. These insights can aid in identifying new risk factors, signature genes, and potential biomarkers, and determining the subset of patient information that should be prioritised to enhance diagnostic accuracy. Comparative analysis of the proposed feature importance score with state-of-the-art methods on 15 public benchmarks demonstrates competitive performance and superior robustness. The method implementation is available on GitHub: this https URL.", 'abstract_zh': '在医疗健康等透明性和可信度至关重要的领域，基于规则的系统广泛使用，并常用于决策支持系统，因其固有的可解释性。然而，随着基于规则的模型变得复杂，区分关键特征、理解它们的交互以及在不同规则集之间比较特征贡献变得挑战重重。为解决这一问题，我们提出了一种全面的框架来估计基于规则系统的特征贡献，引入了一种基于图的特征可视化策略、一种与基于规则的预测器无关的新颖特征重要性度量方法，以及一种基于特征贡献比较规则集的距离度量方法。通过在两个临床数据集和四种基于规则的方法（决策树、逻辑学习机、关联规则、具有规则提取的神经网络）上进行实验，展示本方法可以在数据集级别和类特定级别上揭示临床特征联合预测价值的新洞见。这些洞见有助于识别新的风险因素、特征基因和潜在生物标志物，并确定应优先考虑的患者信息子集以提高诊断准确性。与15个公开基准上的先进方法的比较分析表明，本方法具有竞争力的性能和更高的稳健性。该方法的实现可在GitHub上获得：this https URL。', 'title_zh': '基于特征图增强规则分类器的可解释性'}
{'arxiv_id': 'arXiv:2506.13901', 'title': 'Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations', 'authors': 'Abhilekh Borah, Chhavi Sharma, Danush Khanna, Utkarsh Bhatt, Gurpreet Singh, Hasnat Md Abdullah, Raghav Kaushik Ravi, Vinija Jain, Jyoti Patel, Shubham Singh, Vasu Sharma, Arpita Vats, Rahul Raja, Aman Chadha, Amitava Das', 'link': 'https://arxiv.org/abs/2506.13901', 'abstract': "Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.\nTo address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.\nAdditionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.", 'abstract_zh': '对齐不再是奢侈品，而是必要条件。随着大型语言模型（LLMs）进入高风险领域如教育、医疗、治理和法律，其行为必须可靠地反映人类对齐的价值观和安全约束。然而，当前的评估仍严重依赖于行为代理，如拒绝率、G-Eval评分和毒性分类器，这些方法都存在关键的盲点。对齐的模型往往容易受到 Jailbreak、生成的随机性以及对齐伪装的影响。\n\n为解决这一问题，我们引入了对齐质量指数（AQI）。这是一种新型的几何和提示不变的度量标准，通过分析潜在空间中安全和不安全激活之间的分离程度来实体现评估LLM对齐质量。通过结合Davies-Bouldin评分（DBS）、邓安指数（DI）、谢伊-比利指数（XBI）和卡林斯基-哈拉布扎指数（CHI）等多种度量标准，AQI捕捉聚类质量，以检测隐藏的对齐偏差和Jailbreak风险，即使在输出看似合规的情况下也是如此。AQI还可作为对齐伪装的早期预警信号，提供一种在不了解行为背景的情况下进行安全审计的稳健、解码不变工具。\n\n此外，我们提出LITMUS数据集以在这些具有挑战性的条件下促进稳健评估。LITMUS在不同模型（DPO、GRPO和RLHF条件）下的实证测试表明，AQI与外部评委评分相关，并能揭示拒绝率指标未能揭示的漏洞。我们将我们的实现公开，以促进该领域的进一步研究。', 'title_zh': 'AQI对齐质量指数：超越拒绝样本：基于潜在几何、聚类发散和层wise聚合表示的内在对齐诊断指数'}
{'arxiv_id': 'arXiv:2506.13900', 'title': 'Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models', 'authors': 'Marouane Il Idrissi, Agathe Fernandes Machado, Arthur Charpentier', 'link': 'https://arxiv.org/abs/2506.13900', 'abstract': 'Cooperative game theory has become a cornerstone of post-hoc interpretability in machine learning, largely through the use of Shapley values. Yet, despite their widespread adoption, Shapley-based methods often rest on axiomatic justifications whose relevance to feature attribution remains debatable. In this paper, we revisit cooperative game theory from an interpretability perspective and argue for a broader and more principled use of its tools. We highlight two general families of efficient allocations, the Weber and Harsanyi sets, that extend beyond Shapley values and offer richer interpretative flexibility. We present an accessible overview of these allocation schemes, clarify the distinction between value functions and aggregation rules, and introduce a three-step blueprint for constructing reliable and theoretically-grounded feature attributions. Our goal is to move beyond fixed axioms and provide the XAI community with a coherent framework to design attribution methods that are both meaningful and robust to shifting methodological trends.', 'abstract_zh': '合作博弈论已成为机器学习事后解释的基石，很大程度上得益于舍甫琴值的应用。尽管如此，Shapley基方法往往依赖于对解释特征相关性存疑的公理化论据。在本文中，我们从解释性的角度重新审视合作博弈论，并倡导更广泛和更为原则地使用其工具。我们强调了两种高效的分配方案，韦伯集和哈萨尼集，这些方案超越了Shapley值，提供了更丰富的解释灵活性。我们提供了这些分配方案的可访问概述，阐明了价值函数与聚合规则之间的区别，并引入了三个步骤的蓝图，用于构建可靠且理论依据充分的特征 Attribution方法。我们的目标是超越固定的公理，为可解释人工智能社区提供一个连贯的框架，以设计既具有意义又能够抵抗方法学趋势变化的方法。', 'title_zh': '超越Shapley值：合作博弈在机器学习模型解释中的应用'}
{'arxiv_id': 'arXiv:2506.13892', 'title': 'Scaling Algorithm Distillation for Continuous Control with Mamba', 'authors': 'Samuel Beaussant, Mehdi Mounsif', 'link': 'https://arxiv.org/abs/2506.13892', 'abstract': "Algorithm Distillation (AD) was recently proposed as a new approach to perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic training histories autoregressively with a causal transformer model. However, due to practical limitations induced by the attention mechanism, experiments were bottlenecked by the transformer's quadratic complexity and limited to simple discrete environments with short time horizons. In this work, we propose leveraging the recently proposed Selective Structured State Space Sequence (S6) models, which achieved state-of-the-art (SOTA) performance on long-range sequence modeling while scaling linearly in sequence length. Through four complex and continuous Meta Reinforcement Learning environments, we demonstrate the overall superiority of Mamba, a model built with S6 layers, over a transformer model for AD. Additionally, we show that scaling AD to very long contexts can improve ICRL performance and make it competitive even with a SOTA online meta RL baseline.", 'abstract_zh': '基于S6模型的算法蒸馏在长时序元强化学习中的应用研究', 'title_zh': 'Mamba连续控制中算法蒸馏缩放研究'}
{'arxiv_id': 'arXiv:2506.13886', 'title': 'Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles', 'authors': 'Antara Raaghavi Bhattacharya, Isabel Papadimitriou, Kathryn Davidson, David Alvarez-Melis', 'link': 'https://arxiv.org/abs/2506.13886', 'abstract': 'Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.', 'abstract_zh': '不同语言中数值系统的构建和组合方式多样，尽管人类能够一致地适应这种多样性，大型语言模型在涉及跨语言数值系统的语言数学问题上却存在困难，而人类能够成功地解决这类问题。我们通过一系列实验解开语言和数学在数字中的作用，发现除非数学操作明确使用已知符号（如“+”、“×”等）标记，模型无法始终解决问题。进一步的消融实验探讨了数字构建和组合的个别参数如何影响性能。尽管人类利用对数字的语义理解推断隐含的组成结构，大型语言模型似乎缺乏这种隐含数字结构的概念。我们得出结论，从人类规模数据中的隐含模式灵活推断组合规则的能力仍然是当前推理模型面临的公开挑战。', 'title_zh': '使用多语言数字谜题探究语言和数学推理在语言模型中的交互作用'}
{'arxiv_id': 'arXiv:2506.13862', 'title': 'StaQ it! Growing neural networks for Policy Mirror Descent', 'authors': 'Alena Shilova, Alex Davey, Brahim Driss, Riad Akrour', 'link': 'https://arxiv.org/abs/2506.13862', 'abstract': 'In Reinforcement Learning (RL), regularization has emerged as a popular tool both in theory and practice, typically based either on an entropy bonus or a Kullback-Leibler divergence that constrains successive policies. In practice, these approaches have been shown to improve exploration, robustness and stability, giving rise to popular Deep RL algorithms such as SAC and TRPO. Policy Mirror Descent (PMD) is a theoretical framework that solves this general regularized policy optimization problem, however the closed-form solution involves the sum of all past Q-functions, which is intractable in practice. We propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions in memory, and show that for finite and large enough $M$, a convergent algorithm can be derived, introducing no error in the policy update, unlike prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong theoretical guarantees and is competitive with deep RL baselines, while exhibiting less performance oscillation, paving the way for fully stable deep RL algorithms and providing a testbed for experimentation with Policy Mirror Descent.', 'abstract_zh': '在强化学习（RL）中，正则化已成为理论和实践中的一个热门工具，通常基于熵奖励或约束后续策略的库兹堡-莱布勒散度。实验证明，这些方法可以提高探索性、稳健性和稳定性，催生了如SAC和TRPO等流行的深度RL算法。策略镜像梯度下降（PMD）是一种理论框架，用于解决这一一般正则化策略优化问题，但由于闭式解涉及所有过去的Q函数之和，实际中难以实现。我们提出并分析了一种仅在内存中保留最后M个Q函数的PMD类算法，证明对于有限和足够大的M值，可以导出收敛算法，且不会在策略更新中引入误差，不同于先前的深度RL PMD实现。StaQ算法具备强大的理论保证，性能与深度RL基线相当，且性能波动较少，为完全稳定的深度RL算法铺平了道路，并为策略镜像梯度下降的实验提供了平台。', 'title_zh': 'StaQ它！用于策略镜像梯度的神经网络增长方法'}
{'arxiv_id': 'arXiv:2506.13846', 'title': 'Fake it till You Make it: Reward Modeling as Discriminative Prediction', 'authors': 'Runtao Liu, Jiahao Zhan, Yingqing He, Chen Wei, Alan Yuille, Qifeng Chen', 'link': 'https://arxiv.org/abs/2506.13846', 'abstract': "An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).", 'abstract_zh': '一种有效的奖励模型在视觉生成模型的后训练增强中发挥着关键作用。然而，当前的奖励建模方法由于依赖广泛的用户标注偏好数据或精心构建的质量维度而面临着实施复杂性的问题，这些质量维度往往不完整且工程强度高。受生成对抗网络（GANs）中对抗训练的启发，本文提出了一种高效的奖励建模框架GAN-RM，该框架消除了手动偏好标注和显式质量维度的工程。我们的方法通过辨别一小组代表性且未配对的目标样本（称为偏好代理数据）与模型生成的普通输出来进行奖励模型的训练，仅需几百个目标样本。全面的实验结果表明，GAN-RM 在多个关键应用中均表现出有效性，包括测试时缩放（作为 Best-of-N 样本筛选实现）、后训练方法如监督微调（SFT）和直接偏好优化（DPO）。', 'title_zh': '假作真时真亦假：奖励模型作为辨别性预测'}
{'arxiv_id': 'arXiv:2506.13845', 'title': "Students' Reliance on AI in Higher Education: Identifying Contributing Factors", 'authors': 'Griffin Pitts, Neha Rani, Weedguet Mildort, Eva-Marie Cook', 'link': 'https://arxiv.org/abs/2506.13845', 'abstract': "The increasing availability and use of artificial intelligence (AI) tools in educational settings has raised concerns about students' overreliance on these technologies. Overreliance occurs when individuals accept incorrect AI-generated recommendations, often without critical evaluation, leading to flawed problem solutions and undermining learning outcomes. This study investigates potential factors contributing to patterns of AI reliance among undergraduate students, examining not only overreliance but also appropriate reliance (correctly accepting helpful and rejecting harmful recommendations) and underreliance (incorrectly rejecting helpful recommendations). Our approach combined pre- and post-surveys with a controlled experimental task where participants solved programming problems with an AI assistant that provided both accurate and deliberately incorrect suggestions, allowing direct observation of students' reliance patterns when faced with varying AI reliability. We find that appropriate reliance is significantly related to students' programming self-efficacy, programming literacy, and need for cognition, while showing negative correlations with post-task trust and satisfaction. Overreliance showed significant correlations with post-task trust and satisfaction with the AI assistant. Underreliance was negatively correlated with programming literacy, programming self-efficacy, and need for cognition. Overall, the findings provide insights for developing targeted interventions that promote appropriate reliance on AI tools, with implications for the integration of AI in curriculum and educational technologies.", 'abstract_zh': '人工智能工具在教育领域的日益可用性和应用增加了学生过度依赖这些技术的担忧。过度依赖发生在个人接受错误的人工智能生成建议的情况下，通常未经批判性评价，导致问题解决方案缺陷并损害学习成果。本研究调查了本科生中人工智能依赖模式的潜在因素，不仅考察了过度依赖，还考察了适当的依赖（正确接受有益建议并拒绝有害建议）和不足的依赖（错误拒绝有益建议），并结合前测和后测问卷以及一项控制实验任务，其中参与者在人工智能助手提供的准确和故意错误建议的帮助下解决编程问题，从而直接观察学生在面对不同的人工智能可靠性时的依赖模式。研究发现，适当的依赖与学生的编程自我效能感、编程素养和探究需要显著相关，而与后任务信任和满意度呈负相关。过度依赖与后任务信任和对人工智能助手的满意度显著相关。不足的依赖与编程素养、编程自我效能感和探究需要呈负相关。总体而言，研究结果提供了有关开发旨在促进适当依赖人工智能工具的干预措施的见解，这些干预措施对人工智能在课程和教育技术中的整合具有重要意义。', 'title_zh': '高等教育中学生对AI的依赖：探究影响因素'}
{'arxiv_id': 'arXiv:2506.13838', 'title': 'Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy', 'authors': 'Lorena Poenaru-Olaru, June Sallou, Luis Cruz, Jan Rellermeyer, Arie van Deursen', 'link': 'https://arxiv.org/abs/2506.13838', 'abstract': 'The reliability of machine learning (ML) software systems is heavily influenced by changes in data over time. For that reason, ML systems require regular maintenance, typically based on model retraining. However, retraining requires significant computational demand, which makes it energy-intensive and raises concerns about its environmental impact. To understand which retraining techniques should be considered when designing sustainable ML applications, in this work, we study the energy consumption of common retraining techniques. Since the accuracy of ML systems is also essential, we compare retraining techniques in terms of both energy efficiency and accuracy. We showcase that retraining with only the most recent data, compared to all available data, reduces energy consumption by up to 25\\%, being a sustainable alternative to the status quo. Furthermore, our findings show that retraining a model only when there is evidence that updates are necessary, rather than on a fixed schedule, can reduce energy consumption by up to 40\\%, provided a reliable data change detector is in place. Our findings pave the way for better recommendations for ML practitioners, guiding them toward more energy-efficient retraining techniques when designing sustainable ML software systems.', 'abstract_zh': '机器学习软件系统的可靠性受随时间变化的数据影响显著。因此，机器学习系统需要定期维护，通常基于模型重训练。然而，重新训练需要大量的计算资源，使其能源消耗密集并引发环境影响的担忧。为了理解在设计可持续机器学习应用时应考虑哪些重训练技术，本研究研究了常见重训练技术的能源消耗。鉴于机器学习系统准确性的重要性，我们在能源效率和准确性两方面比较了重训练技术的效果。研究表明，与使用全部可用数据相比，仅使用最新数据进行重训练可将能源消耗减少高达25%，是现状的一种可持续替代方案。此外，我们的研究发现，在有证据表明更新必要时才重新训练模型，而不是按照固定的时间表进行，可以将能源消耗减少高达40%，前提是有可靠的 数据变化检测器。我们的研究为机器学习从业者提供了更好的建议，引导他们在设计可持续机器学习软件系统时采用更节能的重训练技术。', 'title_zh': '可持续机器学习重训练：优化能源效率而不牺牲准确性'}
{'arxiv_id': 'arXiv:2506.13836', 'title': 'Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study', 'authors': 'Dang Viet Anh Nguyen, Carlos Lima Azevedo, Tomer Toledo, Filipe Rodrigues', 'link': 'https://arxiv.org/abs/2506.13836', 'abstract': "Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a promising approach for improving urban mobility. However, its robustness under real-world disruptions such as traffic incidents remains largely underexplored. In this study, we introduce T-REX, an open-source, SUMO-based simulation framework for training and evaluating RL-TSC methods under dynamic, incident scenarios. T-REX models realistic network-level performance considering drivers' probabilistic rerouting, speed adaptation, and contextual lane-changing, enabling the simulation of congestion propagation under incidents. To assess robustness, we propose a suite of metrics that extend beyond conventional traffic efficiency measures. Through extensive experiments across synthetic and real-world networks, we showcase T-REX for the evaluation of several state-of-the-art RL-TSC methods under multiple real-world deployment paradigms. Our findings show that while independent value-based and decentralized pressure-based methods offer fast convergence and generalization in stable traffic conditions and homogeneous networks, their performance degrades sharply under incident-driven distribution shifts. In contrast, hierarchical coordination methods tend to offer more stable and adaptable performance in large-scale, irregular networks, benefiting from their structured decision-making architecture. However, this comes with the trade-off of slower convergence and higher training complexity. These findings highlight the need for robustness-aware design and evaluation in RL-TSC research. T-REX contributes to this effort by providing an open, standardized and reproducible platform for benchmarking RL methods under dynamic and disruptive traffic scenarios.", 'abstract_zh': '基于强化学习的交通信号控制（RL-TSC）已成为提高城市交通流动性的有前途的方法。然而，其在如交通事件等现实世界中断情况下的鲁棒性研究仍处于起步阶段。在本研究中，我们引入了T-REX，一个开放源代码的基于SUMO的仿真框架，用于在动态和事件场景下训练和评估RL-TSC方法。T-REX考虑驾驶者的概率 rerouting、速度适应和情境车道变换，建模网络层面的真实性能，从而在事件情况下模拟拥堵传播。为了评估鲁棒性，我们提出了一套超越传统交通效率度量的指标。通过在合成和真实世界网络上的广泛实验，我们展示了T-REX在多种真实世界部署范式下评估多项先进RL-TSC方法的能力。我们的研究结果表明，虽然独立的价值基方法和分散的压力基方法在稳定交通条件和同质网络中表现出快速收敛和泛化能力，但在事件驱动的分布变化下其性能急剧下降。相比之下，分层协调方法在大型不规则网络中更能提供稳定和适应性更强的性能，得益于它们的结构化决策架构。然而，这也伴随着收敛速度较慢和更高的训练复杂性。这些发现凸显了在RL-TSC研究中鲁棒性意识设计和评估的必要性。T-REX通过提供一个开放、标准化和可重复的平台来基准测试在动态和扰动交通场景下的RL方法，为此做出了贡献。', 'title_zh': '基于强化学习的交通信号控制在遭遇事件情况下的鲁棒性：一项 comparative study'}
{'arxiv_id': 'arXiv:2506.13834', 'title': 'Evolvable Conditional Diffusion', 'authors': 'Zhao Wei, Chin Chun Ooi, Abhishek Gupta, Jian Cheng Wong, Pao-Hsiung Chiu, Sheares Xue Wen Toh, Yew-Soon Ong', 'link': 'https://arxiv.org/abs/2506.13834', 'abstract': 'This paper presents an evolvable conditional diffusion method such that black-box, non-differentiable multi-physics models, as are common in domains like computational fluid dynamics and electromagnetics, can be effectively used for guiding the generative process to facilitate autonomous scientific discovery. We formulate the guidance as an optimization problem where one optimizes for a desired fitness function through updates to the descriptive statistic for the denoising distribution, and derive an evolution-guided approach from first principles through the lens of probabilistic evolution. Interestingly, the final derived update algorithm is analogous to the update as per common gradient-based guided diffusion models, but without ever having to compute any derivatives. We validate our proposed evolvable diffusion algorithm in two AI for Science scenarios: the automated design of fluidic topology and meta-surface. Results demonstrate that this method effectively generates designs that better satisfy specific optimization objectives without reliance on differentiable proxies, providing an effective means of guidance-based diffusion that can capitalize on the wealth of black-box, non-differentiable multi-physics numerical models common across Science.', 'abstract_zh': '本文提出了一种可进化条件扩散方法，使得像计算流体力学和电磁学这样的领域中常见的黑盒非可微多物理模型能够有效地引导生成过程，以促进自主科学发现。我们将引导问题形式化为一个优化问题，通过更新去噪分布的描述统计量来优化期望的适应度函数，并从概率进化的视角，通过基本原则导出了一个进化引导的方法。有趣的是，最终导出的更新算法类似于常见的基于梯度引导扩散模型的更新，但无需计算任何导数。我们在两个AI for Science场景中验证了我们提出的可进化扩散算法：流体拓扑自动化设计和元表面。实验结果表明，该方法能够有效地生成能更好地满足特定优化目标的设计，而无需依赖可微近似，从而提供了一种有效的基于引导的扩散方法，能够利用跨科学领域常见的黑盒非可微多物理数值模型。', 'title_zh': '可演化条件扩散'}
{'arxiv_id': 'arXiv:2506.13833', 'title': 'A Survey on World Models Grounded in Acoustic Physical Information', 'authors': 'Xiaoliang Chen, Le Chang, Xin Yu, Yunhe Huang, Xianling Tu', 'link': 'https://arxiv.org/abs/2506.13833', 'abstract': 'This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal "intuitive physics" engine through sound.', 'abstract_zh': '本调查为基于声学物理信息的基础之上新兴的世界模型领域提供了全面 overview，并探讨了利用声信号进行高保真环境感知、因果物理推理及动态事件预测性模拟的理论基础、核心方法论框架及近期技术进步。调查解释了声信号作为物理事件机械波动能量的直接载体，如何编码有关材料属性、内部几何结构及复杂交互动态的丰富潜在信息。具体而言，该调查通过解释基本物理法则如何在声信号中编码物理信息来奠定理论基础，随后回顾了核心方法论支柱，包括物理感知神经网络（PINNs）、生成模型及自监督多模态学习框架。此外，调查详细说明了声学世界模型在机器人技术、自动驾驶、医疗保健及金融领域的重大应用。最后，该调查系统地概述了相关的重要技术与伦理挑战，并提出了一条通往强大、因果、不确定性意识及负责任的声学智能的具体研究方向，以促进基于声音的体现性主动声学智能的研究路径，使AI系统能够通过声音构建内部“直观物理学”引擎。', 'title_zh': '基于声学物理信息的世界模型综述'}
{'arxiv_id': 'arXiv:2506.13832', 'title': 'FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development via Automatic Evaluation', 'authors': 'Hongda Zhu, Yiwen Zhang, Bing Zhao, Jingzhe Ding, Siyao Liu, Tong Liu, Dandan Wang, Yanan Liu, Zhaojian Li', 'link': 'https://arxiv.org/abs/2506.13832', 'abstract': 'Large Language Models (LLMs) have made significant strides in front-end code generation. However, existing benchmarks exhibit several critical limitations: many tasks are overly simplistic, test cases often lack rigor, and end-to-end validation is absent. These issues hinder the accurate assessment of model performance. To address these challenges, we present FrontendBench, a benchmark co-developed by humans and LLMs. FrontendBench categorizes tasks based on code functionality and incorporates interactive test scenarios, enabling a more comprehensive and practical evaluation of front-end code generation capabilities. The benchmark comprises 148 meticulously crafted prompt-test case pairs spanning five levels of web components, from basic UI elements to complex interactive features. Each task reflects realistic front-end development challenges. Furthermore, we introduce an automatic evaluation framework that executes generated code within a sandbox environment and assesses outcomes using predefined test scripts. This framework achieves a 90.54% agreement rate with expert human evaluations, demonstrating high reliability. We benchmark several state-of-the-art LLMs on FrontendBench and observe substantial performance disparities in handling real-world front-end tasks. These results highlight FrontendBench as a reliable and scalable benchmark, supporting consistent multimodal evaluation and providing a robust foundation for future research in front-end code generation. Our data and code will be released soon.', 'abstract_zh': '大规模语言模型（LLMs）在前端代码生成方面取得了显著进展。然而，现有的基准测试存在几个关键限制：许多任务过于简单，测试案例通常缺乏严谨性，缺乏端到端验证。这些问题阻碍了对模型性能的准确评估。为解决这些挑战，我们提出了由人类和LLMs共同开发的FrontendBench基准测试。FrontendBench根据代码功能对任务进行分类，并结合了交互式测试场景，从而实现对前端代码生成能力更全面和实用的评估。该基准测试包含148个精心编制的提示-测试案例配对，涵盖五个级别的Web组件，从基本的UI元素到复杂的交互功能。每个任务都反映了实际的前端开发挑战。此外，我们介绍了一种自动评估框架，在沙箱环境中执行生成的代码，并使用预定义的测试脚本评估结果。该框架与专家人工评估的共识率达到了90.54%，显示出很高的可靠性。我们在FrontendBench上对几个最先进的LLMs进行了基准测试，发现它们在处理实际前端任务方面的性能存在显著差异。这些结果突显了FrontendBench作为可靠且可扩展基准测试的价值，支持一致的多模态评估，并为未来前端代码生成的研究提供了坚实的基础。我们的数据和代码即将发布。', 'title_zh': 'FrontendBench：一种通过自动评估来评估LLM在前端开发能力的基准测试'}
{'arxiv_id': 'arXiv:2506.13831', 'title': 'Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation', 'authors': 'Jitian Zhao, Chenghui Li, Frederic Sala, Karl Rohe', 'link': 'https://arxiv.org/abs/2506.13831', 'abstract': "Concept-based approaches, which aim to identify human-understandable concepts within a model's internal representations, are a promising method for interpreting embeddings from deep neural network models, such as CLIP. While these approaches help explain model behavior, current methods lack statistical rigor, making it challenging to validate identified concepts and compare different techniques. To address this challenge, we introduce a hypothesis testing framework that quantifies rotation-sensitive structures within the CLIP embedding space. Once such structures are identified, we propose a post-hoc concept decomposition method. Unlike existing approaches, it offers theoretical guarantees that discovered concepts represent robust, reproducible patterns (rather than method-specific artifacts) and outperforms other techniques in terms of reconstruction error. Empirically, we demonstrate that our concept-based decomposition algorithm effectively balances reconstruction accuracy with concept interpretability and helps mitigate spurious cues in data. Applied to a popular spurious correlation dataset, our method yields a 22.6% increase in worst-group accuracy after removing spurious background concepts.", 'abstract_zh': '基于概念的方法通过识别模型内部表示中的人类可理解概念，是一种阐释深度神经网络模型（如CLIP）嵌入的有前景方法。尽管这些方法有助于解释模型行为，但现有方法缺乏统计严谨性，使得验证识别的概念和比较不同技术变得困难。为应对这一挑战，我们提出了一种假设检验框架，用于量化CLIP嵌入空间中的旋转敏感结构。一旦识别出这些结构，我们提出了一种事后概念分解方法。与现有方法不同，该方法提供了理论保证，表明发现的概念代表了稳健且可重复的模式（而非方法特定的伪影），在重构误差方面也优于其他技术。实证研究显示，我们的基于概念的分解算法在提升重构准确性和概念可解释性方面取得了平衡，并有助于减轻数据中的伪线索。应用于一个流行的伪相关数据集后，我们的方法在移除伪背景概念后，最差组的准确性提高了22.6%。', 'title_zh': 'CLIP嵌入结构的量化：概念解释的统计框架'}
{'arxiv_id': 'arXiv:2506.13827', 'title': 'Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing', 'authors': 'Zhuoying Li, Zhu Xu, Yuxin Peng, Yang Liu', 'link': 'https://arxiv.org/abs/2506.13827', 'abstract': 'Instruction-based image editing, which aims to modify the image faithfully according to the instruction while preserving irrelevant content unchanged, has made significant progress. However, there still lacks a comprehensive metric for assessing the editing quality. Existing metrics either require high human evaluation costs, which hinder large-scale evaluation, or are adapted from other tasks and lose task-specific concerns, failing to comprehensively evaluate both instruction-based modification and preservation of irrelevant regions, resulting in biased evaluation. To tackle this, we introduce a new metric called Balancing Preservation and Modification (BPM), tailored for instruction-based image editing by explicitly disentangling the image into editing-relevant and irrelevant regions for specific consideration. We first identify and locate editing-relevant regions, followed by a two-tier process to assess editing quality: Region-Aware Judge evaluates whether the position and size of the edited region align with the instruction, and Semantic-Aware Judge further assesses the instruction content compliance within editing-relevant regions as well as content preservation within irrelevant regions, yielding comprehensive and interpretable quality assessment. Moreover, the editing-relevant region localization in BPM can be integrated into image editing approaches to improve editing quality, demonstrating its broad applicability. We verify the effectiveness of the BPM metric on comprehensive instruction-editing data, and the results show the highest alignment with human evaluation compared to existing metrics, indicating its efficacy. Code is available at: this https URL', 'abstract_zh': '基于指令的图像编辑评价指标：平衡保留与修改（BP&M）', 'title_zh': '平衡保存与修改：一种基于区域和语义的指令驱动图像编辑度量标准'}
{'arxiv_id': 'arXiv:2506.13824', 'title': 'MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios', 'authors': 'Jinyang Huang, Xiachong Feng, Qiguang Chen, Hanjie Zhao, Zihui Cheng, Jiesong Bai, Jingxuan Zhou, Min Li, Libo Qin', 'link': 'https://arxiv.org/abs/2506.13824', 'abstract': 'Code debugging is a crucial task in software engineering, which attracts increasing attention. While remarkable success has been made in the era of large language models (LLMs), current research still focuses on the simple no-library or single-library setting, ignoring the complex multi-library scenario in real-world applications. To address this limitation, we make the first attempt to introduce MLDebugging (Multi-Library Debugging), a comprehensive benchmark designed to assess debugging challenges within multi-library Python code. Specifically, MLDebugging encompasses 126 distinct Python libraries, covering a wide range of multi-library code issues, categorized into seven distinct types. Furthermore, we conduct a thorough evaluation of MLDebugging using both mainstream open-source and closed-source LLMs and highlight that current LLMs still struggle to correctly perform code debugging across multi-library scenarios. We hope this work can uncover the potential of LLMs in multi-library debugging scenario and offer insights for future research.', 'abstract_zh': '多库调试是软件工程中的关键任务，吸引了越来越多的关注。尽管在大规模语言模型（LLMs）时代取得了显著进展，当前研究仍主要集中在无库或单库设置上，忽视了真实世界应用中的复杂多库场景。为解决这一局限，我们首次尝试引入MLDebugging（多库调试）这一综合基准，用于评估多库Python代码中的调试挑战。具体而言，MLDebugging 包含126个不同的Python库，涵盖了广泛的多库代码问题，并按七大类进行分类。此外，我们使用主流的开源和闭源LLM对MLDebugging进行了全面评估，指出当前的LLM仍然难以在多库场景中正确执行代码调试。我们希望这项工作能揭示LLMs在多库调试场景中的潜力，并为未来的研究提供见解。', 'title_zh': 'MLDebugging: 面向跨多库场景的代码调试基准测试'}
{'arxiv_id': 'arXiv:2506.13820', 'title': 'Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge', 'authors': 'Shraddha Surana, Ashwin Srinivasan, Michael Bain', 'link': 'https://arxiv.org/abs/2506.13820', 'abstract': "The IPARC Challenge, inspired by ARC, provides controlled program synthesis tasks over synthetic images to evaluate automatic program construction, focusing on sequence, selection, and iteration. This set of 600 tasks has resisted automated solutions. This paper presents a structured inductive programming approach with LLMs that successfully solves tasks across all IPARC categories. The controlled nature of IPARC reveals insights into LLM-based code generation, including the importance of prior structuring, LLMs' ability to aid structuring (requiring human refinement), the need to freeze correct code, the efficiency of code reuse, and how LLM-generated code can spark human creativity. These findings suggest valuable mechanisms for human-LLM collaboration in tackling complex program synthesis.", 'abstract_zh': 'IPARC挑战赛：受ARC启发，提供基于合成图像的受控程序合成任务，以评估自动程序构建能力，重点关注序列、选择和迭代。这套包含600个任务的挑战题库难以被自动化解决。本文提出了一种基于LLM的结构化归纳编程方法，成功解决了所有IPARC类别的任务。IPARC的受控性质揭示了基于LLM的代码生成中的见解，包括先前结构化的重要性、LLM辅助结构化（需要人类细化）、正确代码冻结的需求、代码重用的效率，以及LLM生成代码激发人类创造力的方式。这些发现表明了人类与LLM协作解决复杂程序合成问题的关键机制。', 'title_zh': '使用大语言模型进行结构化程序合成：IPARC挑战赛的研究结果与见解'}
{'arxiv_id': 'arXiv:2506.13817', 'title': 'DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models', 'authors': 'Saleem A. Al Dajani, Abel Sanchez, John R. Williams', 'link': 'https://arxiv.org/abs/2506.13817', 'abstract': 'Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable of downstream tasks such as cell-typing and perturbation prediction. As data volume grows, these models may surpass human performance in labeling, paving the way for reliable inference in large-scale perturbation screens. This application demonstrates domain-specific innovation in health monitoring and diagnostics, aligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.', 'abstract_zh': '生成式AI基础模型为处理结构化生物数据提供了变革性的潜力，特别是在单细胞RNA测序领域，其中数据集正迅速扩大至数十亿细胞。我们提出了结合自主基础模型和实时网络搜索自动标注实验数据的方法，实现了高达82.5%的准确率。这通过提高注释 throughput，消除了手动编目和人为错误的关键瓶颈，从而在监督学习中处理结构化组学数据。我们的方法使开发虚拟细胞基础模型成为可能，这些模型能够执行下游任务，如细胞分类和扰动预测。随着数据量的增长，这些模型可能在注释方面超越人类表现，为大规模扰动筛选提供可靠推理。该应用展示了健康监测和诊断领域的特定领域创新，与人类细胞图谱和人类肿瘤图谱网络等努力相一致。', 'title_zh': 'DeepSeq：通过网络搜索增强的代理生成人工智能基础模型单细胞RNA测序数据标注高通量方法'}
{'arxiv_id': 'arXiv:2506.13811', 'title': 'Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study', 'authors': 'Sompote Youwai, David Phim, Vianne Gayl Murcia, Rianne Clair Onas', 'link': 'https://arxiv.org/abs/2506.13811', 'abstract': 'This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.', 'abstract_zh': '基于路由器的多Agent系统在智能任务分类和专家选择中的土木工程基础设计自动化研究', 'title_zh': '基于大型语言模型的路由器多智能体架构在基础设计自动化中的潜在应用：一项任务分类与专家遴选研究'}
{'arxiv_id': 'arXiv:2506.13809', 'title': 'Analysis and Optimization of Probabilities of Beneficial Mutation and Crossover Recombination in a Hamming Space', 'authors': 'Roman V. Belavkin', 'link': 'https://arxiv.org/abs/2506.13809', 'abstract': "Inspired by Fisher's geometric approach to study beneficial mutations, we analyse probabilities of beneficial mutation and crossover recombination of strings in a general Hamming space with arbitrary finite alphabet. Mutations and recombinations that reduce the distance to an optimum are considered as beneficial. Geometric and combinatorial analysis is used to derive closed-form expressions for transition probabilities between spheres around an optimum giving a complete description of Markov evolution of distances from an optimum over multiple generations. This paves the way for optimization of parameters of mutation and recombination operators. Here we derive optimality conditions for mutation and recombination radii maximizing the probabilities of mutation and crossover into the optimum. The analysis highlights important differences between these evolutionary operators. While mutation can potentially reach any part of the search space, the probability of beneficial mutation decreases with distance to an optimum, and the optimal mutation radius or rate should also decrease resulting in a slow-down of evolution near the optimum. Crossover recombination, on the other hand, acts in a subspace of the search space defined by the current population of strings. However, probabilities of beneficial and deleterious crossover are balanced, and their characteristics, such as variance, are translation invariant in a Hamming space, suggesting that recombination may complement mutation and boost the rate of evolution near the optimum.", 'abstract_zh': '受Fishers几何方法研究有益突变的启发，我们分析了一般哈明空间中任意有限字母字符串的有益突变和交叉重组的概率。减少到最优值距离的突变和重组被视为有益。通过几何和组合分析，我们推导出最优值周围球体内转换概率的闭式表达式，完整描述了多个世代中距离最优值的马尔可夫演化过程。这为优化突变和重组操作的参数铺平了道路。我们推导了使突变和交叉重组到最优值的概率最大化的突变和重组半径的最优性条件。分析突出了这些进化操作之间的重要差异。虽然突变有可能到达搜索空间的任何部分，但接近最优值时有益突变的概率会降低，因此最优突变半径或速率也应减少，导致进化在接近最优值时减慢。另一方面，交叉重组作用于由当前字符串群体定义的搜索空间的一个子空间。然而，有益和有害交叉重组的概率是平衡的，它们在哈明空间中的特征，如方差，是平移不变的，这表明重组可能补充突变，并促进在接近最优值时的进化速率。', 'title_zh': 'Hamming空间中有利突变和交叉重组概率的分析与优化'}
{'arxiv_id': 'arXiv:2506.13807', 'title': 'BraTS orchestrator : Democratizing and Disseminating state-of-the-art brain tumor image analysis', 'authors': 'Florian Kofler, Marcel Rosier, Mehdi Astaraki, Ujjwal Baid, Hendrik Möller, Josef A. Buchner, Felix Steinbauer, Eva Oswald, Ezequiel de la Rosa, Ivan Ezhov, Constantin von See, Jan Kirschke, Anton Schmick, Sarthak Pati, Akis Linardos, Carla Pitarch, Sanyukta Adap, Jeffrey Rudie, Maria Correia de Verdier, Rachit Saluja, Evan Calabrese, Dominic LaBella, Mariam Aboian, Ahmed W. Moawad, Nazanin Maleki, Udunna Anazodo, Maruf Adewole, Marius George Linguraru, Anahita Fathi Kazerooni, Zhifan Jiang, Gian Marco Conte, Hongwei Li, Juan Eugenio Iglesias, Spyridon Bakas, Benedikt Wiestler, Marie Piraud, Bjoern Menze', 'link': 'https://arxiv.org/abs/2506.13807', 'abstract': 'The Brain Tumor Segmentation (BraTS) cluster of challenges has significantly advanced brain tumor image analysis by providing large, curated datasets and addressing clinically relevant tasks. However, despite its success and popularity, algorithms and models developed through BraTS have seen limited adoption in both scientific and clinical communities. To accelerate their dissemination, we introduce BraTS orchestrator, an open-source Python package that provides seamless access to state-of-the-art segmentation and synthesis algorithms for diverse brain tumors from the BraTS challenge ecosystem. Available on GitHub (this https URL), the package features intuitive tutorials designed for users with minimal programming experience, enabling both researchers and clinicians to easily deploy winning BraTS algorithms for inference. By abstracting the complexities of modern deep learning, BraTS orchestrator democratizes access to the specialized knowledge developed within the BraTS community, making these advances readily available to broader neuro-radiology and neuro-oncology audiences.', 'abstract_zh': 'BraTS挑战集群在脑肿瘤分割方面显著推动了脑肿瘤图像分析的进步，通过提供大规模的数据集和解决临床相关任务。尽管BraTS在成功和受欢迎程度方面取得了这些成就，但通过BraTS开发的算法和模型在科学和临床社区中的应用仍然有限。为了加速其推广，我们介绍了BraTS orchestrator，这是一个开源的Python包，提供了无缝访问BraTS挑战生态系统中多种脑肿瘤先进分割和合成算法的途径。该包可在GitHub（此链接请点击：https://github.com/）上获得，包含针对编程经验有限的用户设计的直观教程，使研究人员和临床医生能够轻松部署BraTS比赛中的获胜算法进行推断。通过抽象现代深度学习的复杂性，BraTS orchestrator使BraTS社区开发的专业知识普及化，使这些进展能够被更广泛的神经放射学和神经肿瘤学受众方便地使用。', 'title_zh': 'BraTS协调者：普及和传播最先进的脑肿瘤图像分析技术'}
{'arxiv_id': 'arXiv:2506.13805', 'title': 'Dr. GPT Will See You Now, but Should It? Exploring the Benefits and Harms of Large Language Models in Medical Diagnosis using Crowdsourced Clinical Cases', 'authors': 'Bonam Mingole, Aditya Majumdar, Firdaus Ahmed Choudhury, Jennifer L. Kraschnewski, Shyam S. Sundar, Amulya Yadav', 'link': 'https://arxiv.org/abs/2506.13805', 'abstract': 'The proliferation of Large Language Models (LLMs) in high-stakes applications such as medical (self-)diagnosis and preliminary triage raises significant ethical and practical concerns about the effectiveness, appropriateness, and possible harmfulness of the use of these technologies for health-related concerns and queries. Some prior work has considered the effectiveness of LLMs in answering expert-written health queries/prompts, questions from medical examination banks, or queries based on pre-existing clinical cases. Unfortunately, these existing studies completely ignore an in-the-wild evaluation of the effectiveness of LLMs in answering everyday health concerns and queries typically asked by general users, which corresponds to the more prevalent use case for LLMs. To address this research gap, this paper presents the findings from a university-level competition that leveraged a novel, crowdsourced approach for evaluating the effectiveness of LLMs in answering everyday health queries. Over the course of a week, a total of 34 participants prompted four publicly accessible LLMs with 212 real (or imagined) health concerns, and the LLM generated responses were evaluated by a team of nine board-certified physicians. At a high level, our findings indicate that on average, 76% of the 212 LLM responses were deemed to be accurate by physicians. Further, with the help of medical professionals, we investigated whether RAG versions of these LLMs (powered with a comprehensive medical knowledge base) can improve the quality of responses generated by LLMs. Finally, we also derive qualitative insights to explain our quantitative findings by conducting interviews with seven medical professionals who were shown all the prompts in our competition. This paper aims to provide a more grounded understanding of how LLMs perform in real-world everyday health communication.', 'abstract_zh': '大型语言模型在高 stakes 应用中的普及，如医学（自我）诊断和初步分类，引起了对其在与健康相关问题和查询使用中的有效性和适当性的伦理和实践关注，以及潜在的有害性。尽管已有工作考虑了大型语言模型回答专家撰写的健康查询/提示、医学考试题库中的问题或基于现有临床案例的查询的有效性，但这些现有研究完全忽视了对大型语言模型在回答普通用户通常提出的日常健康问题和查询的有效性的在野生环境中的评估，这对应了大型语言模型更常见的使用场景。为了填补这一研究缺口，本文介绍了采用新颖的众包方法评估大型语言模型在回答日常健康查询方面有效性的一场大学级竞赛的研究成果。在为期一周的时间内，共有34名参与者向四种可访问的大型语言模型提出了212个真实的（或虚构的）健康问题，由一组九名认证的内科医生团队评估了大型语言模型生成的回复。总体而言，我们的研究发现，内科医生认为76%的212个大型语言模型的回复是准确的。此外，在医疗专业人员的帮助下，我们研究了具备全面医学知识库支持的检索增强（RAG）版本的大型语言模型是否能提高其生成回复的质量。最后，我们通过对七名在我们的竞赛中看到所有提示的医疗专业人员进行访谈，获得了定性的洞见来解释我们的定量发现。本文旨在提供对大型语言模型在现实世界日常健康沟通中表现的更为现实的理解。', 'title_zh': 'Dr. GPT 现在会见到你，但是应该吗？——基于 crowdsourced 临床案例探讨大型语言模型在医疗诊断中的利弊'}
{'arxiv_id': 'arXiv:2506.13804', 'title': 'Instruction and Solution Probabilities as Heuristics for Inductive Programming', 'authors': 'Edward McDaid, Sarah McDaid', 'link': 'https://arxiv.org/abs/2506.13804', 'abstract': 'Instruction subsets (ISs) are heuristics that can shrink the size of the inductive programming (IP) search space by tens of orders of magnitude. Here, we extend the IS approach by introducing instruction and solution probabilities as additional heuristics. Instruction probability reflects the expectation of an instruction occurring in a solution, based on the frequency of instruction occurrence in a large code sample. The solution probability for a partial or complete program is simply the product of all constituent instruction probabilities, including duplicates. We treat the minimum solution probabilities observed in code sample program units of different sizes as solution probability thresholds. These thresholds are used to prune the search space as partial solutions are constructed, thereby eliminating any branches containing unlikely combinations of instructions. The new approach has been evaluated using a large sample of human code. We tested two formulations of instruction probability: one based on instruction occurrence across the entire code sample and another that measured the distribution separately for each IS. Our results show that both variants produce substantial further reductions in the IP search space size of up to tens of orders of magnitude, depending on solution size. In combination with IS, reductions of over 100 orders of magnitude can be achieved. We also carried out cross-validation testing to show that the heuristics should work effectively with unseen code. The approach is described and the results and some ideas for future work are discussed.', 'abstract_zh': '启发式指令子集（IS）可以通过减少归纳编程（IP）搜索空间的规模来缩小数十个数量级。在此基础上，我们通过引入指令和解决方案概率作为额外的启发式方法来扩展IS方法。指令概率反映了指令在大型代码样本中出现频率的期望值，作为其在解决方案中出现的预期。解决方案概率是所有组成部分指令概率的乘积，包括重复的指令。我们将不同大小的代码样本程序单元中观察到的最小解决方案概率视为解决方案概率阈值。这些阈值用于在构建部分解决方案时修剪搜索空间，从而排除任何包含不太可能出现的指令组合的分支。我们使用大量的人类代码样本进行了新方法的评估。我们测试了两种指令概率的公式：一种基于代码样本中指令的总体出现频率，另一种则分别测量每个指令子集的分布。结果显示，这两种变体都能进一步减少高达数十个数量级的IP搜索空间大小，具体取决于解决方案的大小。与指令子集结合使用时，可以实现超过100个数量级的减少。我们还进行了交叉验证测试，以表明这些启发式方法应能有效应用于未见过的代码。我们将描述该方法并讨论实验结果和一些未来研究的想法。', 'title_zh': '指令和解决方案概率作为归纳编程的启发式方法'}
{'arxiv_id': 'arXiv:2506.13800', 'title': 'Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework', 'authors': 'Abul Ehtesham, Aditi Singh, Saket Kumar', 'link': 'https://arxiv.org/abs/2506.13800', 'abstract': 'Enhancing clinical decision support (CDS), reducing documentation burdens, and improving patient health literacy remain persistent challenges in digital health. This paper presents an open-source, agent-based framework that integrates Large Language Models (LLMs) with HL7 FHIR data via the Model Context Protocol (MCP) for dynamic extraction and reasoning over electronic health records (EHRs). Built on the established MCP-FHIR implementation, the framework enables declarative access to diverse FHIR resources through JSON-based configurations, supporting real-time summarization, interpretation, and personalized communication across multiple user personas, including clinicians, caregivers, and patients. To ensure privacy and reproducibility, the framework is evaluated using synthetic EHR data from the SMART Health IT sandbox (this https URL), which conforms to the FHIR R4 standard. Unlike traditional approaches that rely on hardcoded retrieval and static workflows, the proposed method delivers scalable, explainable, and interoperable AI-powered EHR applications. The agentic architecture further supports multiple FHIR formats, laying a robust foundation for advancing personalized digital health solutions.', 'abstract_zh': '增强临床决策支持（CDS），减轻文档负担，提高患者健康素养仍然是数字健康领域持续的挑战。本文提出了一种基于代理的开源框架，通过Model Context Protocol (MCP) 集成大规模语言模型（LLMs）与HL7 FHIR数据，实现电子健康记录（EHR）的动态提取和推理。该框架建立在现有的MCP-FHIR实现基础上，通过JSON配置实现声明式的访问多种FHIR资源，支持跨多个用户角色（包括临床医生、护理人员和患者）的实时总结、解释和个人化沟通。为确保隐私和可重复性，该框架使用SMART Health IT sandbox提供的符合FHIR R4标准的合成EHR数据进行评估。与依赖硬编码检索和静态工作流的传统方法不同，所提出的方法可提供可扩展、可解释且互操作的基于AI的EHR应用程序。该代理架构进一步支持多种FHIR格式，为推进个性化数字健康解决方案奠定了坚实的基础。', 'title_zh': '通过LLM和模型上下文协议增强临床决策支持和EHR Insights：一个开源MCP-FHIR框架'}
{'arxiv_id': 'arXiv:2506.13798', 'title': 'Contemporary AI foundation models increase biological weapons risk', 'authors': 'Roger Brent, T. Greg McKelvey Jr', 'link': 'https://arxiv.org/abs/2506.13798', 'abstract': 'The rapid advancement of artificial intelligence has raised concerns about its potential to facilitate biological weapons development. We argue existing safety assessments of contemporary foundation AI models underestimate this risk, largely due to flawed assumptions and inadequate evaluation methods. First, assessments mistakenly assume biological weapons development requires tacit knowledge, or skills gained through hands-on experience that cannot be easily verbalized. Second, they rely on imperfect benchmarks that overlook how AI can uplift both nonexperts and already-skilled individuals. To challenge the tacit knowledge assumption, we examine cases where individuals without formal expertise, including a 2011 Norwegian ultranationalist who synthesized explosives, successfully carried out complex technical tasks. We also review efforts to document pathogen construction processes, highlighting how such tasks can be conveyed in text. We identify "elements of success" for biological weapons development that large language models can describe in words, including steps such as acquiring materials and performing technical procedures. Applying this framework, we find that advanced AI models Llama 3.1 405B, ChatGPT-4o, and Claude 3.5 Sonnet can accurately guide users through the recovery of live poliovirus from commercially obtained synthetic DNA, challenging recent claims that current models pose minimal biosecurity risk. We advocate for improved benchmarks, while acknowledging the window for meaningful implementation may have already closed.', 'abstract_zh': '人工智能的迅猛发展引发了对其可能促进生物武器开发潜在风险的关注。我们argue现有对当代基础人工智能模型的安全评估低估了这一风险，主要是由于错误的假设和不充分的评估方法。首先，评估错误地假设生物武器开发需要默会知识，即通过实际操作获得但难以口头表达的技能。其次，它们依赖于不完善的基准，忽视了人工智能如何提升非专家和已有技能者的水平。为挑战默会知识的假设，我们考察了没有正式专业知识的个体，如2011年挪威极右分子合成爆炸物的案例，分析他们成功完成复杂技术任务的情况。我们还审查了病原体构建过程的记录，强调这些任务可以通过文本传达。我们发现在文字中能够描述生物武器开发成功的“要素”，包括获取材料和技术操作步骤等。应用这一框架，我们发现最先进的AI模型Llama 3.1 405B、ChatGPT-4o和Claude 3.5 Sonnet能够准确引导用户从商业合成DNA中恢复活病毒，挑战了近期关于当前模型对生物安全风险较小的观点。我们呼吁改进基准，同时承认实施有意义的改进的机会可能已不再存在。', 'title_zh': '当代AI基础模型增加了生物武器风险'}
{'arxiv_id': 'arXiv:2506.13796', 'title': 'ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries', 'authors': 'Zhou Chen, Xiao Wang, Yuanhong Liao, Ming Lin, Yuqi Bai', 'link': 'https://arxiv.org/abs/2506.13796', 'abstract': 'As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.', 'abstract_zh': '随着全球气候变暖问题日益严峻，气候科学研究的需求持续增长。以大型语言模型（LLMs）为代表的自然语言处理技术在气候变暖研究中得到了广泛应用，为决策者和公众提供了必要的信息支持。一些研究通过构建气候变暖相关的指令数据并调优LLMs，提高了模型在相关任务上的性能。然而，当前研究在高效生成大量高精度气候变暖指令数据方面仍显不足，限制了气候变暖LLMs的进一步发展。本研究介绍了一种自动构建指令数据的方法，该方法利用文档中的事实和背景知识生成指令，并通过网络爬取和收集种子指令来增强指令数据的多样性。使用该方法构建了一个名为ClimateChat-Corpus的气候变暖指令数据集，用于调优开源LLMs，最终得到一个名为ClimateChat的LLM。评估结果显示，ClimateChat在气候变暖问答任务上的性能显著提升。此外，我们还评估了不同基础模型和指令数据对LLMs性能的影响，证明了其能够适应广泛的气候变暖科学研究任务，强调了选择合适的基础模型进行指令调优的重要性。本研究为构建气候变暖指令数据和训练特定于气候变暖的LLMs提供了有价值的参考和实证支持。', 'title_zh': 'ClimateChat：设计数据与方法以调优大语言模型以回答气候变化查询'}
{'arxiv_id': 'arXiv:2506.13787', 'title': 'Analysis of Anonymous User Interaction Relationships and Prediction of Advertising Feedback Based on Graph Neural Network', 'authors': 'Yanjun Dai, Haoyang Feng, Yuan Gao', 'link': 'https://arxiv.org/abs/2506.13787', 'abstract': "While online advertising is highly dependent on implicit interaction networks of anonymous users for engagement inference, and for the selection and optimization of delivery strategies, existing graph models seldom can capture the multi-scale temporal, semantic and higher-order dependency features of these interaction networks, thus it's hard to describe the complicated patterns of the anonymous behavior. In this paper, we propose Decoupled Temporal-Hierarchical Graph Neural Network (DTH-GNN), which achieves three main contributions. Above all, we introduce temporal edge decomposition, which divides each interaction into three types of channels: short-term burst, diurnal cycle and long-range memory, and conducts feature extraction using the convolution kernel of parallel dilated residuals; Furthermore, our model builds a hierarchical heterogeneous aggregation, where user-user, user-advertisement, advertisement-advertisement subgraphs are combined through the meta-path conditional Transformer encoder, where the noise structure is dynamically tamped down via the synergy of cross-channel self-attention and gating relationship selector. Thirdly, the contrast regularity of feedback perception is formulated, the consistency of various time slices is maximized, the entropy of control exposure information with dual-view target is maximized, the global prototype of dual-momentum queue distillation is presented, and the strategy gradient layer with light weight is combined with delaying transformation signal to fine-tune the node representation for benefit-oriented. The AUC of DTH-GNN improved by 8.2% and the logarithmic loss improved by 5.7% in comparison with the best baseline model.", 'abstract_zh': '解耦时序层次图神经网络（DTH-GNN）及其在匿名用户行为模式识别中的应用', 'title_zh': '基于图神经网络的匿名用户交互关系分析及广告反馈预测'}
{'arxiv_id': 'arXiv:2506.13786', 'title': 'Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction', 'authors': 'Vuong M. Ngo, Tran Quang Vinh, Patricia Kearney, Mark Roantree', 'link': 'https://arxiv.org/abs/2506.13786', 'abstract': 'Diabetes is a chronic metabolic disease characterized by elevated blood glucose levels, leading to complications like heart disease, kidney failure, and nerve damage. Accurate state-level predictions are vital for effective healthcare planning and targeted interventions, but in many cases, data for necessary analyses are incomplete. This study begins with a data engineering process to integrate diabetes-related datasets from 2011 to 2021 to create a comprehensive feature set. We then introduce an enhanced bagging ensemble regression model (EBMBag+) for time series forecasting to predict diabetes prevalence across U.S. cities. Several baseline models, including SVMReg, BDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our EBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved the best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an R2 of 0.9.', 'abstract_zh': '糖尿病是一种以高血糖为特征的慢性代谢疾病，会导致心脏病、肾衰竭和神经损伤等并发症。准确的州级预测对于有效的医疗规划和有针对性的干预至关重要，但在许多情况下，用于必要分析的数据是不完整的。本研究首先进行数据工程，整合2011年至2021年的糖尿病相关数据集，创建综合特征集。然后，我们引入了增强袋装集成回归模型(EBMBag+)进行时间序列预测，以预测美国城市糖尿病患病率。我们将EBMBag+算法与多个基线模型进行比较，包括SVMReg、BDTree、LSBoost、NN、LSTM和ERMBag。实验结果表明，EBMBag+取得了最佳性能，平均绝对误差（MAE）为0.41，均方误差（RMSE）为0.53，均绝对百分比误差（MAPE）为4.01，R²值为0.9。', 'title_zh': '基于数据集成的袋装聚类回归增强方法在时间序列糖尿病预测中的应用'}
{'arxiv_id': 'arXiv:2506.13782', 'title': 'XGraphRAG: Interactive Visual Analysis for Graph-based Retrieval-Augmented Generation', 'authors': 'Ke Wang, Bo Pan, Yingchaojie Feng, Yuwei Wu, Jieyi Chen, Minfeng Zhu, Wei Chen', 'link': 'https://arxiv.org/abs/2506.13782', 'abstract': "Graph-based Retrieval-Augmented Generation (RAG) has shown great capability in enhancing Large Language Model (LLM)'s answer with an external knowledge base. Compared to traditional RAG, it introduces a graph as an intermediate representation to capture better structured relational knowledge in the corpus, elevating the precision and comprehensiveness of generation results. However, developers usually face challenges in analyzing the effectiveness of GraphRAG on their dataset due to GraphRAG's complex information processing pipeline and the overwhelming amount of LLM invocations involved during graph construction and query, which limits GraphRAG interpretability and accessibility. This research proposes a visual analysis framework that helps RAG developers identify critical recalls of GraphRAG and trace these recalls through the GraphRAG pipeline. Based on this framework, we develop XGraphRAG, a prototype system incorporating a set of interactive visualizations to facilitate users' analysis process, boosting failure cases collection and improvement opportunities identification. Our evaluation demonstrates the effectiveness and usability of our approach. Our work is open-sourced and available at this https URL.", 'abstract_zh': '基于图的检索增强生成（GraphRAG）在通过外部知识库增强大型语言模型（LLM）的回答方面展现了巨大的能力。与传统的RAG相比，它引入了一种图作为中间表示，以更好地捕捉语料库中的结构化关系知识，提升生成结果的精确度和全面性。然而，由于图RAG复杂的信息处理管道以及在图构建和查询过程中涉及的大量大型语言模型调用，开发者通常面临分析图RAG在他们数据集上的效果的挑战，这限制了图RAG的可解释性和可访问性。本研究提出了一种可视化分析框架，帮助RAG开发者识别图RAG的关键召回并追踪这些召回通过图RAG管道的过程。基于此框架，我们开发了XGraphRAG原型系统，该系统包含一系列交互式可视化工具，以促进用户分析过程，增强故障案例收集和改进机会的识别。我们的评估证明了该方法的有效性和实用性。我们的工作已开源，并可从此链接获取：this https URL。', 'title_zh': 'XGraphRAG：基于图检索增强生成的交互式可视化分析'}
{'arxiv_id': 'arXiv:2506.13781', 'title': 'Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment', 'authors': 'Pablo Ariño Fernández, Carlos Quesada González', 'link': 'https://arxiv.org/abs/2506.13781', 'abstract': "The job shop scheduling problem is an NP-hard combinatorial optimization problem relevant to manufacturing and timetabling. Traditional approaches use priority dispatching rules based on simple heuristics. Recent work has attempted to replace these with deep learning models, particularly graph neural networks (GNNs), that learn to assign priorities from data. However, training such models requires customizing numerous factors: graph representation, node features, action space, and reward functions. The lack of modular libraries for experimentation makes this research time-consuming. This work introduces JobShopLib, a modular library that allows customizing these factors and creating new components with its reinforcement learning environment. We trained several dispatchers through imitation learning to demonstrate the environment's utility. One model outperformed various graph-based dispatchers using only individual operation features, highlighting the importance of feature customization. Our GNN model achieved near state-of-the-art results on large-scale problems. These results suggest significant room for improvement in developing such models. JobShopLib provides the necessary tools for future experimentation.", 'abstract_zh': '作业车间调度问题是与制造和排程相关的NP难组合优化问题。传统的方法基于简单的启发式规则使用优先级调度规则。最近的研究尝试用深度学习模型，尤其是图神经网络（GNNs），来替代这些方法，让模型从数据中学习优先级分配。然而，训练这样的模型需要定制众多因素：图表示、节点特征、操作空间和奖励函数。缺乏模块化库使得这一研究耗时。本工作介绍了一个模块化库JobShopLib，它允许定制这些因素，并使用其强化学习环境创建新组件。通过模仿学习训练了几种调度器以展示该环境的实用性。一个模型仅使用单个操作特征就优于各种基于图的调度器，强调了特征定制的重要性。我们的GNN模型在大规模问题上达到了接近最先进的结果。这些结果表明在开发此类模型方面有巨大的改进空间。JobShopLib为未来的实验提供了必要的工具。', 'title_zh': '使用图神经网络解决作业车间调度问题：可定制的强化学习环境'}
{'arxiv_id': 'arXiv:2506.13780', 'title': 'Hidden Bias in the Machine: Stereotypes in Text-to-Image Models', 'authors': 'Sedat Porikli, Vedat Porikli', 'link': 'https://arxiv.org/abs/2506.13780', 'abstract': 'Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.', 'abstract_zh': '文本到图像（T2I）模型已变革了视觉内容创建，从自然语言提示生成高度逼真的图像。然而，人们对其可能复制和放大现有社会偏见的潜在风险依然存有担忧。为探讨这些问题，我们汇集了一个涵盖主题类别如职业、特质、行为、意识形态、情绪、家庭角色、地点描述、宗教信仰和生活事件的多样化提示集。对于每一种160个独特的主题，我们创作了多种提示变体以反映广泛的意义和视角。使用Stable Diffusion 1.5（基于UNet）和Flux-1（基于DiT）模型并采用原始检查点，我们在一致的设置下生成了超过16,000张图像。此外，我们从Google图片搜索中收集了8,000张对比图像。所有输出均经过筛选，排除了抽象、失真或无意义的结果。我们的分析揭示了生成图像中性别、种族、年龄、体型和其他以人类为中心的因素方面的显著差异。这些差异往往反映了并加强了社会叙事中嵌入的有害刻板印象。我们讨论了这些发现的意义，并强调需要更多包容性的数据集和开发实践，以促进生成视觉系统的公平性。', 'title_zh': '机器中的隐性偏见：文本到图像模型中的刻板印象'}
{'arxiv_id': 'arXiv:2506.13778', 'title': 'Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning', 'authors': 'Anvi Alex Eponon, Moein Shahiki-Tash, Ildar Batyrshin, Christian E. Maldonado-Sifuentes, Grigori Sidorov, Alexander Gelbukh', 'link': 'https://arxiv.org/abs/2506.13778', 'abstract': 'This study presents a question-based knowledge encoding approach that improves retrieval-augmented generation (RAG) systems without requiring fine-tuning or traditional chunking. We encode textual content using generated questions that span the lexical and semantic space, creating targeted retrieval cues combined with a custom syntactic reranking method.\nIn single-hop retrieval over 109 scientific papers, our approach achieves a Recall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We also introduce "paper-cards", concise paper summaries under 300 characters, which enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified technical queries.\nFor multihop tasks, our reranking method reaches an F1 score of 0.52 with LLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking and fine-tuned baselines which score 0.328 and 0.412 respectively.\nThis method eliminates fine-tuning requirements, reduces retrieval latency, enables intuitive question-driven knowledge access, and decreases vector storage demands by 80%, positioning it as a scalable and efficient RAG alternative.', 'abstract_zh': '基于问题的知識編碼方法：改進 Retrieval-Augmented Generation 系統而不需微調或傳統分塊', 'title_zh': '基于问题生成的知识压缩：无需微调增强多跳文档检索'}
{'arxiv_id': 'arXiv:2506.13777', 'title': 'A Survey of Physics-Informed AI for Complex Urban Systems', 'authors': 'En Xu, Huandong Wang, Yunke Zhang, Sibo Li, Yinzhou Tang, Zhilun Zhou, Yuming Lin, Yuan Yuan, Xiaochen Fan, Jingtao Ding, Yong Li', 'link': 'https://arxiv.org/abs/2506.13777', 'abstract': 'Urban systems are typical examples of complex systems, where the integration of physics-based modeling with artificial intelligence (AI) presents a promising paradigm for enhancing predictive accuracy, interpretability, and decision-making. In this context, AI excels at capturing complex, nonlinear relationships, while physics-based models ensure consistency with real-world laws and provide interpretable insights. We provide a comprehensive review of physics-informed AI methods in urban applications. The proposed taxonomy categorizes existing approaches into three paradigms - Physics-Integrated AI, Physics-AI Hybrid Ensemble, and AI-Integrated Physics - and further details seven representative methods. This classification clarifies the varying degrees and directions of physics-AI integration, guiding the selection and development of appropriate methods based on application needs and data availability. We systematically examine their applications across eight key urban domains: energy, environment, economy, transportation, information, public services, emergency management, and the urban system as a whole. Our analysis highlights how these methodologies leverage physical laws and data-driven models to address urban challenges, enhancing system reliability, efficiency, and adaptability. By synthesizing existing methodologies and their urban applications, we identify critical gaps and outline future research directions, paving the way toward next-generation intelligent urban system modeling.', 'abstract_zh': '城市系统是典型的复杂系统，在此基础上将基于物理的建模与人工智能相结合，为提高预测准确性、可解释性和决策制定提供了有前景的范式。在这种背景下，人工智能在捕捉复杂非线性关系方面表现出色，而基于物理的模型则确保与现实世界的规律保持一致并提供可解释的洞察。我们对城市应用中的物理信息人工智能方法进行了全面综述。提出的分类方案将现有方法分为三种范式——物理集成AI、物理-AI混合集成、AI集成物理，并进一步详细介绍了七种代表性方法。这种分类明确了物理-AI集成的差异程度和方向，指导根据应用需求和数据可用性选择和开发适当的方法。我们系统地考察了这些方法在八个关键城市领域的应用：能源、环境、经济、交通、信息、公共服务、应急管理以及城市系统整体。我们的分析突显了这些方法如何利用物理定律和数据驱动的模型来解决城市挑战，增强系统的可靠性和适应性。通过对现有方法及其城市应用的综合研究，我们识别出关键的缺口，并勾画出未来研究方向，为下一代智能城市系统建模指明了方向。', 'title_zh': '物理学知情的人工智能在复杂城市系统中的调研'}
{'arxiv_id': 'arXiv:2506.13772', 'title': 'MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs', 'authors': 'Zhenyan Lu, Daliang Xu, Dongqi Cai, Zexi Li, Wei Liu, Fangming Liu, Shangguang Wang, Mengwei Xu', 'link': 'https://arxiv.org/abs/2506.13772', 'abstract': 'Large language models (LLMs) are deployed on mobile devices to power killer applications such as intelligent assistants. LLMs pre-trained on general corpora often hallucinate when handling personalized or unseen queries, leading to incorrect or outdated responses. Knowledge editing addresses this by identifying and adjusting a small crucial portion of model weights, without compromising the general knowledge. However, prior knowledge editing methods are impractical to run on local devices due to the resource-heavy backpropagation (BP) needed for updates. We present MobiEdit, the first mobile knowledge editing framework that enables efficient LLM personalization on commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces full-precision BP with quantized forward-only gradient estimation, thus compatible with the energy-efficient mobile neural processing units (NPUs). MobiEdit replaces full-precision backpropagation with quantized forward-only gradient estimation, making it compatible with energy-efficient mobile NPUs. To further improve gradient estimation efficiency, we introduce two optimizations: an early stoping mechanism that adaptively terminates editing upon success and a prefix cache that reuses computation across steps. Our approach enables real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile devices with 7.6$\\times$ less memory, 14.7 $\\times$ less energy and 3.6$\\times$ less latency compared to previous knowledge editing methods.', 'abstract_zh': '大规模语言模型（LLMs）部署在移动设备上以驱动智能助手等杀手级应用。预训练于通用语料库的大规模语言模型在处理个人化或未见过的查询时经常会发生幻觉，导致不正确或过时的响应。知识编辑通过识别并调整模型权重中的一个小而关键的部分来解决这一问题，而不牺牲一般知识。然而，先前的知识编辑方法由于需要进行更新的计算强度大的反向传播（BP），在本地设备上运行不实际。我们提出了MobiEdit，这是首个能够在商用现货（COTS）移动设备上实现高效大规模语言模型个性化的移动知识编辑框架。MobiEdit用量化前向传播梯度估计取代了全精度的反向传播，使其与节能的移动神经处理单元（NPUs）兼容。为了进一步提高梯度估计效率，我们引入了两种优化：一个适应性终止机制，可在成功后自动终止编辑，以及前缀缓存，可在步骤之间重用计算。我们的方法使MobiEdit能够在COTS移动设备上对一个3亿参数的模型（Qwen2.5-3B-Instruct）进行实时编辑，与之前的知识编辑方法相比，实现7.6倍的内存节省、14.7倍的能效提升和3.6倍的延迟减少。', 'title_zh': 'MobiEdit: 资源高效的知识编辑用于个性化本地部署的语言模型'}
{'arxiv_id': 'arXiv:2506.13771', 'title': 'LittleBit: Ultra Low-Bit Quantization via Latent Factorization', 'authors': 'Banseok Lee, Dongkyu Kim, Youngcheon You, Youngmin Kim', 'link': 'https://arxiv.org/abs/2506.13771', 'abstract': "Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.", 'abstract_zh': '小比特：一种用于极端大规模语言模型压缩的新方法', 'title_zh': 'LittleBit: 超低比特量化通过潜在因子分解'}
{'arxiv_id': 'arXiv:2506.13769', 'title': 'Non-planar Object Detection and Identification by Features Matching and Triangulation Growth', 'authors': 'Filippo Leveni', 'link': 'https://arxiv.org/abs/2506.13769', 'abstract': 'Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.', 'abstract_zh': '基于特征的模板匹配与识别：通过增量特征匹配迭代分组方法', 'title_zh': '非平面物体检测与识别：基于特征匹配和三角化生长'}
