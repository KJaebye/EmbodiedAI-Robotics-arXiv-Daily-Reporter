{'arxiv_id': 'arXiv:2506.13986', 'title': 'Diffusion-based Inverse Observation Model for Artificial Skin', 'authors': 'Ante Maric, Julius Jankowski, Giammarco Caroleo, Alessandro Albini, Perla Maiolino, Sylvain Calinon', 'link': 'https://arxiv.org/abs/2506.13986', 'abstract': 'Contact-based estimation of object pose is challenging due to discontinuities and ambiguous observations that can correspond to multiple possible system states. This multimodality makes it difficult to efficiently sample valid hypotheses while respecting contact constraints. Diffusion models can learn to generate samples from such multimodal probability distributions through denoising algorithms. We leverage these probabilistic modeling capabilities to learn an inverse observation model conditioned on tactile measurements acquired from a distributed artificial skin. We present simulated experiments demonstrating efficient sampling of contact hypotheses for object pose estimation through touch.', 'abstract_zh': '基于接触的姿态估计由于观测的不连续性和含义模糊性而具有挑战性，这些观测可能对应于多个可能的系统状态。这种多模态性使得在尊重接触约束的前提下高效地采样有效假设变得困难。扩散模型可以通过去噪算法学习生成此类多模态概率分布的样本。我们利用这些概率建模能力，基于从分布式人工皮肤获取的触觉测量学习一个条件逆观测模型。我们展示了模拟实验，通过触觉高效采样接触假设以估计物体姿态。', 'title_zh': '基于扩散的逆观察模型 for 人工皮肤'}
{'arxiv_id': 'arXiv:2506.14212', 'title': "What's in the Box? Reasoning about Unseen Objects from Multimodal Cues", 'authors': 'Lance Ying, Daniel Xu, Alicia Zhang, Katherine M. Collins, Max H. Siegel, Joshua B. Tenenbaum', 'link': 'https://arxiv.org/abs/2506.14212', 'abstract': "People regularly make inferences about objects in the world that they cannot see by flexibly integrating information from multiple sources: auditory and visual cues, language, and our prior beliefs and knowledge about the scene. How are we able to so flexibly integrate many sources of information to make sense of the world around us, even if we have no direct knowledge? In this work, we propose a neurosymbolic model that uses neural networks to parse open-ended multimodal inputs and then applies a Bayesian model to integrate different sources of information to evaluate different hypotheses. We evaluate our model with a novel object guessing game called ``What's in the Box?'' where humans and models watch a video clip of an experimenter shaking boxes and then try to guess the objects inside the boxes. Through a human experiment, we show that our model correlates strongly with human judgments, whereas unimodal ablated models and large multimodal neural model baselines show poor correlation.", 'abstract_zh': '人们通过灵活整合来自多种来源的信息（听觉和视觉线索、语言以及我们对场景的先验信念和知识）来推断他们无法看到的世界中的物体。我们是如何能够如此灵活地整合许多信息来源来理解我们周围的这个世界，即使我们没有直接的知识？在这项工作中，我们提出了一种神经符号模型，使用神经网络解析开放式的多模态输入，然后应用贝叶斯模型来整合不同来源的信息，评估不同的假说。我们使用一项名为“箱内有何物？”的新颖物体猜测游戏来评估我们的模型，其中人类和模型观看实验员摇动箱子的视频片段，然后尝试猜测箱内的物体。通过一项人类实验，我们证明我们的模型与人类判断有很强的相关性，而单模态删除模型和大型多模态神经基线模型则显示出较差的相关性。', 'title_zh': '盒子里有什么？从多模态线索推理未知物体'}
{'arxiv_id': 'arXiv:2506.14170', 'title': 'A multi-stage augmented multimodal interaction network for fish feeding intensity quantification', 'authors': 'Shulong Zhang, Mingyuan Yao, Jiayin Zhao, Xiao Liu, Haihua Wang', 'link': 'https://arxiv.org/abs/2506.14170', 'abstract': 'In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity.', 'abstract_zh': '多阶段增强多模态交互网络在水产养殖中评估鱼食量的研究', 'title_zh': '多阶段增强多模态交互网络在鱼类摄食强度量化中的应用'}
{'arxiv_id': 'arXiv:2506.14035', 'title': 'SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement', 'authors': 'Chelsi Jain, Yiran Wu, Yifan Zeng, Jiale Liu, S hengyu Dai, Zhenwen Shao, Qingyun Wu, Huazheng Wang', 'link': 'https://arxiv.org/abs/2506.14035', 'abstract': 'Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at this https URL.', 'abstract_zh': '文档视觉问答（DocVQA）是一项实际且具有挑战性的任务，旨在基于多页文档并参考不同类型的信息（例如图片和表格）提问。为了应对多模态处理，近期方法遵循类似的检索增强生成（RAG）管道，但利用视觉语言模型（VLMs）嵌入模型来嵌入并检索相关页面作为图像，并使用能接受图像作为输入的VLMs生成答案。在本文中，我们介绍了一种轻量级但强大的检索增强框架SimpleDoc，它通过首先通过嵌入相似性检索候选页面，然后根据页面摘要进行过滤和再排序来增强证据页面的收集。一个基于VLM的推理代理反复调用这种双重线索检索器，迭代地将新的页面拉入工作记忆，直到问题得到自信的回答。SimpleDoc在4个DocVQA数据集上的表现优于先前基线，且检索的页面数量大幅减少。我们的代码可在以下网址获得。', 'title_zh': 'SimpleDoc：基于双线索页面检索和迭代精炼的多模态文档理解'}
{'arxiv_id': 'arXiv:2506.13780', 'title': 'Hidden Bias in the Machine: Stereotypes in Text-to-Image Models', 'authors': 'Sedat Porikli, Vedat Porikli', 'link': 'https://arxiv.org/abs/2506.13780', 'abstract': 'Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.', 'abstract_zh': '文本到图像（T2I）模型已变革了视觉内容创建，从自然语言提示生成高度逼真的图像。然而，人们对其可能复制和放大现有社会偏见的潜在风险依然存有担忧。为探讨这些问题，我们汇集了一个涵盖主题类别如职业、特质、行为、意识形态、情绪、家庭角色、地点描述、宗教信仰和生活事件的多样化提示集。对于每一种160个独特的主题，我们创作了多种提示变体以反映广泛的意义和视角。使用Stable Diffusion 1.5（基于UNet）和Flux-1（基于DiT）模型并采用原始检查点，我们在一致的设置下生成了超过16,000张图像。此外，我们从Google图片搜索中收集了8,000张对比图像。所有输出均经过筛选，排除了抽象、失真或无意义的结果。我们的分析揭示了生成图像中性别、种族、年龄、体型和其他以人类为中心的因素方面的显著差异。这些差异往往反映了并加强了社会叙事中嵌入的有害刻板印象。我们讨论了这些发现的意义，并强调需要更多包容性的数据集和开发实践，以促进生成视觉系统的公平性。', 'title_zh': '机器中的隐性偏见：文本到图像模型中的刻板印象'}
